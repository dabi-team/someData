1
2
0
2

r
a

M
3

]
L
M

.
t
a
t
s
[

1
v
5
6
1
2
0
.
3
0
1
2
:
v
i
X
r
a

Parsimonious Inference

Parsimonious Inference

Jed A. Duersch
Thomas A. Catanach
Sandia National Laboratories
Livermore, CA 94550, United States

jaduers@sandia.gov
tacatan@sandia.gov

Abstract

Bayesian inference provides a uniquely rigorous approach to obtain principled justiﬁcation
for uncertainty in predictions, yet it is diﬃcult to articulate suitably general prior belief in
the machine learning context, where computational architectures are pure abstractions sub-
ject to frequent modiﬁcations by practitioners attempting to improve results. Parsimonious
inference is an information-theoretic formulation of inference over arbitrary architectures
that formalizes Occam’s Razor; we prefer simple and suﬃcient explanations. Our universal
hyperprior assigns plausibility to prior descriptions, encoded as sequences of symbols, by
expanding on the core relationships between program length, Kolmogorov complexity, and
Solomonoﬀ’s algorithmic probability. We then cast learning as information minimization
over our composite change in belief when an architecture is speciﬁed, training data are
observed, and model parameters are inferred. By distinguishing model complexity from
prediction information, our framework also quantiﬁes the phenomenon of memorization.

Although our theory is general, it is most critical when datasets are limited, e.g. small or
skewed. We develop novel algorithms for polynomial regression and random forests that
are suitable for such data, as demonstrated by our experiments. Our approaches combine
eﬃcient encodings with prudent sampling strategies to construct predictive ensembles with-
out cross-validation, thus addressing a fundamental challenge in how to eﬃciently obtain
predictions from data.

Keywords: Bayesian inference, information, prior belief, Kolmogorov complexity, Solomonoﬀ
probability

1. Introduction

We began this investigation desiring to understand the relationship between prior belief
and the resulting uncertainty in predictions obtained from inference in the hope that new
insights would provide a sound basis to improve prediction credibility in machine learn-
ing. The mathematical and epistemological foundations of rational belief, from which the
laws of probability and Bayesian inference are derived as an extended logic from binary
propositional logic (Cox, 1946), lead us to assert the central role of Bayesian inference in
obtaining rigorous justiﬁcation for uncertainty in predictions. Although this foundation of
reason holds generally, it is critical when we need to learn robust predictions from limited
datasets. Yet, applying Bayesian inference within the machine learning context requires
addressing a fundamental challenge:
inference requires prior belief. When the amount of
evidence contained within a dataset regarding a phenomenon of interest is extremely lim-
ited, specifying prior belief is not merely an inconvenience; it is the dominant source of
uncertainty in predictions. Examples of such data limitations include having few observa-

1

 
 
 
 
 
 
J. A. Duersch and T. A. Catanach

tions, noisy measurements, skewed or highly imbalanced labels of interest, or even a degree
of mislabeling in the data.

When predictive models integrate well-understood physical principles, they are often
accompanied by physically plausible parameter ranges that provide a strong basis for prior
belief. Likewise, canonical priors are acceptable for simple approximations with relatively
few unconstrained parameters in comparison to the size of the dataset intended for inference.
In contrast, the
Kass and Wasserman (1996) give a thorough survey of related work.
machine learning paradigm seeks to instrument arbitrary algorithms with high parameter
dimensionality. A typical architecture may have tens of thousands, or perhaps millions, of
free parameters. In this setting, the sensitivity of predictions to an arbitrary choice of prior
belief may be unacceptable for applications of consequence (Owhadi et al., 2015).

1.1 Our Contributions

Expanding on the work of Solomonoﬀ (1964a,b, 2009), Kolmogorov (1965), Rissanen (1983,
1984), and Hutter (2007), we develop a theoretical framework that assigns plausibility to
arbitrary inference architectures. Just as Solomonoﬀ derives algorithmic probability from
program length, the minimum number of bits needed to encode a program for a speciﬁed
Universal Turing Machine (UTM), we show how a modest generalization yields a universal
hyperprior over symbolic encodings of ordinary priors. We may regard an ordinary prior,
that which is typically used in Bayesian inference, as a restricted state of belief from a
general universe of potential explanatory models. Within our framework, every choice of
computational architecture, and associated prior over model parameters, is just a restriction
of prior belief. Our hyperprior provides a means to measure and control the complexity of
such choices.

We show how our theory of information (Duersch and Catanach, 2020), Theorem 1,
allows us to derive a training objective from the information that is created when we select
a prior representation, observe the training data, and either infer the posterior distribution
or construct a variational approximation of it. Zhang et al. (2018) provide a thorough survey
of recent work on variational inference. Our main result, Theorem 2, clariﬁes how we may
understand learning as an information optimization problem. Our parsimony objective
separates into three components:

• Encoding information contained within a symbolic description of prior belief;

• Model information gained through inference using evidence;

• Predictive information gained regarding the observed labels from plausible models.

In our derivation, the ﬁrst two terms appear with negative signs and the third with a
positive sign, revealing how our theory suppresses complexity as an intrinsic tradeoﬀ against
increased agreement with observed labels. The ﬁrst component guards against excessive
complexity in our description of prior belief and the second guards against priors that are
poorly suited to our data. In contrast, the third component promotes agreement between
resulting predictions and the data. The main distinction between the second and third
components is that model information is measured in the space of explanations, whereas
predictive information measures the result of applying plausible models to our data.

2

Parsimonious Inference

We review work on universal priors over integers, corresponding binary representations,
and show how a simple integer encoding approximates the scaling invariance of Jeﬀreys
prior. We then demonstrate this theory with two learning prototypes.

Our ﬁrst algorithm casts polynomial regression within this framework, predicting a dis-
tribution over continuous outcomes from a continuous input. By setting the maximum
polynomial degree to be much higher than the data merits, standard machine learning
training strategies are susceptible to memorization, as we demonstrate by applying gradi-
ent based training with leave-one-out cross-validation. In contrast, our prototype discovers
much simpler models from the same high-degree basis. Moreover, when we aggregate pre-
dictions over an ensemble of polynomial representations, our prototype demonstrates the
natural increase in uncertainty we intuitively associate with extrapolation.

Our second algorithm samples ensembles of decision trees that are constructed using
the parsimonious inference objective. These models aim to predict discrete labels through
a sequence of partitions on continuous feature coordinates. Our random forest prototype
demonstrates the ability to learn credible prediction uncertainty from extremely small and
heavily skewed datasets, which we contrast with a standard decision tree model and boot-
strap aggregation. Both of these algorithms achieve superior prediction uncertainty through
Bayesian inference from prior belief that is derived to both quantify and naturally suppress
complexity over arbitrary explanations.

Although our basic hyperprior is subject to a choice of interpreter—which need not
be Turing-complete, but must transform valid codes into coherent probability distributions
over predictive models—we go on to show that to be consistent with this theory, there exists
a unique hyperprior over an ensemble of Turing-complete interpreters, Theorem 3.

1.2 Organization

Section 2 begins with a discussion and illustration of the severe inadequacies of traditional
machine learning training approaches that depend on cross-validation. We then brieﬂy
review the critical connections between scientiﬁc principles, rational belief, and Bayesian
inference, which provide a sound theory to obtain rigorously justiﬁed uncertainty in predic-
tions. When placed in the machine learning context, however, we explore how principled
justiﬁcation for prior belief over abstract models, as well as our unavoidable disregard for an
inﬁnite number of alternative models, remains a critical challenge. Further, we summarize
how our theory of information is derived to satisfy key properties that allow us to relate
the various forms of complexity that follow in the parsimonious inference objective.

Section 3 continues with our main contributions, including a discussion of generalized
description length, a coherent complexity hyperprior, and the principles of minimum in-
formation and maximum entropy. These notions culminate in the parsimonious inference
objective, providing a suitable framework to understand and control model complexity over
arbitrary learning architectures. We also show how this objective allows us to quantify mem-
orization. Our theory allows us to apply these concepts within a wide variety of approaches
to solve learning problems, including variational inference techniques.

Section 4 examines implementation details within our prototype algorithms, including

eﬃcient encodings and training strategies for polynomial regression and decision trees.

3

J. A. Duersch and T. A. Catanach

Section 5 concludes with a discussion of how we may consistently compare multiple in-
terpreters, a pathway to frame and address computability `a priori, our theory’s relationship
to other work, and a summary of our ﬁndings.

2. Background

In order to clarify how we may improve trust in machine learning predictions, we must
begin with the origin of trust in science. The epistemological foundation of the scientiﬁc
method shares a fundamental connection with Bayesian inference and determines how we
may optimally account for evidence to learn plausible explanations. Bayesian theory alone,
however, does not provide a complete learning framework when we employ high-parameter
families, such as most machine learning architectures. Thus, we also review Solomonoﬀ’s
and Kolmogorov’s notions of complexity as a means to promote simplicity in learned models.
To motivate the need for this discussion, we begin by illustrating the severe deﬁciencies of
standard machine learning training practices when they are applied to small datasets.

2.1 Memorization

The term memorization is often conﬂated with overtraining, but we distinguish these terms
as follows. Overtraining is characterized by degradation in prediction quality on unseen data
that occurs after an initial stage of improvements. In contrast, memorization refers more
generally to any predictive algorithm that exhibits unjustiﬁable conﬁdence, or low prediction
uncertainty, in the training dataset labels that were used to adjust model parameters.
Section 3.3 provides rigorous analysis to justify this view. Conﬂating these terms leads to
an incorrect picture of the problem; to avoid memorization, we must merely halt training
at the correct moment.

Machine learning algorithms are typically trained using some variation of stochastic
gradient descent (Robbins and Monro, 1951). When applied to overparameterized models,
traditional optimization strategies are subject to overtraining. Cross-validation (Allen,
1974) attempts to prevent overtraining by monitoring predictions on a holdout dataset, but
we show how this method still fails to prevent memorization on small datasets. The same
strategy is also used to tune hyperparameters, such as such as regularization weights and
learning-rate schedules.

The obvious diﬃculty presented by cross-validation is the inherent tradeoﬀ between
using as much data as possible to train parameters, but also having a reliable estima-
tor for prediction quality. For limited data, standard practices apply some form of k-
fold cross-validation (Hastie et al., 2009). One forms k distinct partitions of the dataset,
trains k models respectively, and aggregates predictions by averaging. Leave-one-out cross-
validation uses the same number of partitions as datapoints. Each partition reserves only
one observation to estimate the best model over each training trajectory.

Figure 1 demonstrates these techniques using polynomial regression, ﬁtting 20th degree
polynomials with only 12 points. Retaining more polynomial coeﬃcients than training
points allows us to observe how standard training fails when data are limited. The top-left
shows an example of a single model trained by holding out one point for validation, shown
in green. The bottom-left shows average predictions over 12 such models. Yet, suppose we
could train with all 12 points while remaining highly conﬁdent that we will halt training

4

Parsimonious Inference

Figure 1: Illustration of standard training shortcomings. Top-left: optimum obtained from hold-
ing out the green point. Bottom-left: mean predictions over all single-holdout op-
tima. Top-middle:
idealized training on all original data and 1000 extra validation
points from ground truth. Bottom-middle: mean predictions over 12 random starts,
N (θi | µ = 0, σ = 0.2). Removing the tradeoﬀ between holdout and training data does
not prevent complexities in predictions. Top-right: optimal model discovered using our
theoretical framework and prototype algorithm. Bottom-right: our aggregate accounts
for many plausible models, improving robustness and demonstrating natural extrapola-
tion uncertainty.

at the correct moment. This ideal is demonstrated as a thought experiment in the middle
column of Figure 1 by sampling 1000 extra data from the generative process, the ground
truth mechanism that creates observations. We see that eliminating the tradeoﬀ between
training and validation would not prevent artifacts from developing that conﬁdently hew to
scant observations, memorization.

Typically, one would also use cross-validation to tune the optimal polynomial degree,
which would certainly constrain complexity somewhat. The purpose of this experiment,
however, is to show that na¨ıve training may never even explore low-complexity models,
especially in high dimensions. For most high-parameter families, such as neural networks,
there is no feasible hierarchy of bases that would be analogous to limiting the polynomial
degree. For example, if we wished to constrain model parameters to a ﬁxed sparsity pattern,
the number of patterns to test would grow exponentially in the number of nonzero elements.
This experiment demonstrates how neither of the competing cross-validation objectives
address the core problem with learning from limited data. Memorization is often framed
in terms of a bias-variance tradeoﬀ; predictions should avoid ﬂuctuating rapidly, but also
remain ﬂexible enough to extract predictive patterns. In our theoretical framework, how-
ever, memorization is more comprehensively and rigorously understood as unparsimonious

5

J. A. Duersch and T. A. Catanach

model complexity, i.e. increases in model information that are not justiﬁed by only small
improvements to training predictions.

Regularization strategies attempt to address this heuristically by penalizing excessive
freedom in learning parameters, for example attaching an (cid:96)1 or (cid:96)2 norm to the training
objective. While many of these approaches can be equivalently cast as choices of prior belief,
they lack a unifying principle that would illuminate and resolve choices of regularization
shape and weight. One must, again, resort to hyperparameter tuning via cross-validation,
thus failing to address the core challenge: to eﬃciently learn from limited data.

2.2 Scientiﬁc Reasoning and Bayesian Inference

In order to reiterate the concrete relationship between Bayesian inference and scientiﬁc
reasoning, we review the epistemological foundations of reason at the center of the scientiﬁc
method. These foundations bear decisive consequences regarding the valid forms of analysis
we may pursue in order to obtain rational predictions. At its core, the scientiﬁc method relies
on coherent mathematical models of observable phenomena that have been informed over
centuries of physical measurements. Within the ﬁeld of epistemology, this is the naturalist
view of rational belief (Brandt, 1985).
It holds that validity is ultimately derived from
consistency, which can be understood in three key components:

1. Rational beliefs must be logical, avoiding internal contradictions;

2. Rational beliefs must be empirical, accounting for all available evidence;

3. Rational beliefs must be predictive, continually reassessing validity by how well pre-

dictions agree with new observations.

The third point is really nothing more than a restatement of the second point, placing
emphasis on the evolving nature of rational beliefs as new data become available. The
critical signiﬁcance of the ﬁrst point is that it provides a path to elevate the second and
third points to a rigorous extended logic: Bayesian inference.

Building on the rich body of work by many scholars—including Ramsey (2016, original
1926), De Finetti (1937), and Jeﬀreys (1998, original 1939)—Cox (1946) shows that for a
mathematical framework analyzing degrees of truth, belief as an extended logic, to be con-
sistent with binary propositional logic, that formalism must satisfy the laws of probability:

1. Probability is nonnegative.

2. Only impossibility has probability zero.

3. Only certainty has maximum probability, normalized to one.

4. To revise the degree of credibility we assign to a model upon reviewing empirical

evidence, we must apply Bayes’ theorem.

Consequently, the Bayesian paradigm provides a uniquely rigorous approach to quantify
uncertainty in predictions derived through inductive reasoning. Therefore, the only logically
correct path to quantify and suppress memorization in learning must be cast within the
Bayesian perspective.

6

Parsimonious Inference

Analysis proceeds with a probability distribution called the prior p(θ) that quantiﬁes
our lack of information, or initial uncertainty, in plausible explanatory models. Here, θ is
any speciﬁc parameter state within a model class, or computational architecture. When
we need to emphasize the prior’s dependence on a model class, as well as the shape of
the parameter distribution within that class, we will write the prior as p(θ | ψ), where a
description ψ, or hyperparameter sequence, provides such details. We will examine how ψ
plays a key role regarding model complexity in detail in Section 3. The empirical data are
expressed as a set of ordered pairs D = {(xi, yi) | i ∈ [n]} that have been sampled from the
generative process g(x, y). Features xi are used to predict labels yi from the architecture
paired with θ. We write the predicted distribution over all potential labels as p(yi | xi, θ).
If the ordered pairs in D represent independent samples from the underlying process, the
likelihood is evaluated as p(D | θ) = (cid:81)
i∈[n] p(yi | xi, θ), which expresses the probability of
observing D if a hypothetical explanation θ held. Then, we update our beliefs according
In our picture, we hold that having θ alone is suﬃcient to evaluate
to Bayes’ theorem.
predictions. When we explicate the role of prior descriptions ψ, that means inference can
be written as

p(θ | D, ψ) =

p(D | θ)p(θ | ψ)
p(D | ψ)

where p(D | ψ) =

(cid:90)

dθ p(D | θ)p(θ | ψ)

is the model-class evidence. If we have a hyperprior p(ψ) over potential descriptions, we
can also infer the hyperposterior

p(ψ | D) =

p(D | ψ)p(ψ)
p(D)

where p(D) =

(cid:90)

dψ p(D | ψ)p(ψ).

The central point of inference is that it does not attempt to identify a single explanation
matching the data, as with stochastic gradient optimization and cross-validation. Rather,
inference naturally adheres to the Epicurean principle—we should retain multiple explana-
tions according to their respective degrees of plausibility—within a coherent mathematical
framework. As a distribution, the posterior is meaningful in a way that a single model is
not; it allows us to update our beliefs consistently as new evidence emerges. We obtain ra-
tional predictions by evaluating the posterior predictive integral, or even the hyperposterior
predictive integral, respectively constructed as

p(y | x, D, ψ) =

(cid:90)

dθ p(y | x, θ)p(θ | D, ψ)

and

p(y | x, D) =

(cid:90)

dψ p(y | x, D, ψ)p(ψ | D).

The resulting predictions meet the exigent standard of rational belief for meaningful uncer-
tainty quantiﬁcation.

2.3 The Universal Scope of Prior Belief

The pervasive objection to the Bayesian paradigm is the lack of clear provenance for prior
In addition to objections based on subjectivity, translating our intuitive beliefs
belief.
into distributions can be diﬃcult. This problem is exacerbated in the domain of machine

7

J. A. Duersch and T. A. Catanach

learning, where computational models are abstract and driven only by practical utility,
rather than well-understood physical principles. The premise of machine learning is that we
do not need to integrate expert knowledge and specialized scientiﬁc theory into algorithms
to obtain useful predictions from data, which eludes the traditional view of priors, that we
must express our beliefs.

Prediction sensitivity to prior belief is most apparent when the number of parameters
If we have n
approaches or exceeds the size of our dataset, as illustrated in Figure 2.
observations and k > n diﬀerentiable parameters, then every point in parameter space
must have at least k − n perturbable dimensions in which the likelihood gradient is zero.
Because the likelihood remains constant as we move through these dimensions, each point
lives within a (k − n)-dimensional submanifold wherein prior belief entirely determines the
structure of posterior belief. Thus, within these submanifolds, the contribution of parameter
uncertainty to prediction uncertainty is not aﬀected by evidence. Clearly, we cannot be
satisﬁed with meeting only the bare conditions for technically rational belief; we require
concrete philosophical justiﬁcation for prior belief.

Figure 2: Illustration of prediction sensitivity to prior belief. The ﬁrst row uses Chebyshev polyno-
mial bases and the second uses standard bases. The third row exacerbates the problem
with basis-dependence by using a polynomial that already memorized the data for the
ﬁrst basis function, followed by the Chebyshev basis. All priors are normal, N (θ | 0, I).
The choice of basis clearly matters; inference alone does not prevent the development of
artifacts we associate with memorization.

8

Parsimonious Inference

In order to appreciate the solution, we must grasp the full severity of the problem by
framing it in the most arduous scope. We can deﬁne the model universe as the set containing
every computational architecture that could produce coherent predictions over y from x.
By considering inference over the model universe, we see that every architectural design
decision is equivalent to a choice of support for prior belief, i.e. the subdomain in which
prior belief is nonzero. Using model-class descriptions ψ to capture potential choices of
prior belief allows us to subsume these complications by investigating the correct form of a
hyperprior p(ψ).

This picture also illuminates a second signiﬁcant challenge in the machine learning
setting, the problem of dimensionality in high-parameter families. Even if we obtain an
attractive hyperprior, we can always construct increasingly complicated architectures. It
is not possible to explore all of them. Occam’s Razor provides a compelling path to a
solution; explanations should not exhibit more complexity than what is required to explain
the evidence. We conclude that a comprehensive hyperprior must compute and suppress
a rigorous formulation of complexity. Moreover, our learning framework must justify dis-
regarding inﬁnite dimensions from inference and simultaneously address how to feasibly
construct or approximate restricted posteriors.

2.4 Controlling Complexity

Bayesian theorists have had a persistent interest in articulating core principles for con-
structing priors over abstract models, particularly within the objectivist Bayesian philoso-
phy. Examples include maximum entropy priors (Jaynes, 1957; Good, 1963) and Jeﬀrey’s
priors (Jeﬀreys, 1946). Other approaches use information criteria to determine a suitable
number of parameters, such as the Akaike Information Criterion (AIC) (Akaike, 1974) and
Bayesian Information Criterion (BIC) (Schwarz, 1978).
In contrast to these approaches
that explicitly compare model classes, in which a parameter is either present or absent,
Automatic Relevance Determination (ARD) (MacKay, 1995; Neal, 2012) takes a softer ap-
proach. ARD uses a hyperprior to express uncertain relevance of diﬀerent parameters and
features in a model.
It postulates that most model parameters should be close to zero
because only a limited number of features are relevant for prediction. Through inference,
relevant model parameters can be identiﬁed automatically. Having been speciﬁcally devel-
oped for neural networks, ARD provides an important perspective to understand complete
theories of learning. We will discuss the relationship between ARD priors and our theory
in Section 5.4.

Perhaps the most principled approach to a universal prior is Solomonoﬀ’s work on al-
gorithmic probability (Solomonoﬀ, 1960, 1964a,b, 2009). Solomonoﬀ derives a prior over
all possible programs based upon their lengths using binary encodings subject to an opti-
mal UTM. Hutter (2007), reviewing central principles of reason, goes further to show how
Solomonoﬀ’s framework solves important philosophical problems in the Bayesian setting,
including predictive and decision-theoretic performance bounds under the assumption that
the generative process is a program. Potapov et al. (2012) also discuss Solomonoﬀ’s algo-
rithmic probability, emphasizing the importance of retaining many alternative models to
not only learn robust predictions, but also maintain adaptability in decision making.

9

J. A. Duersch and T. A. Catanach

Kolmogorov’s work on mapping complexity (Kolmogorov, 1965) is closely related and
we will examine it in detail in Section 3.1. Rissanen’s work on universal priors and Min-
imum Description Length (MDL) (Rissanen, 1983, 1984) is also related. We examine his
universal prior on integers in Section 4.1 and the relationship between our theory and MDL
in Section 5.5. The key advantage of Solomonoﬀ’s approach is that it applies generally
to any model we can program, thus eliminating artiﬁcial constraints on computational ar-
chitectures. Solomonoﬀ does not separate the model θ from the model class ψ, since any
model from any model class can be expressed as a program. As this encoding-length based
prior provides a strong base for our work, we provide a detailed discussion in Section 3.2.
Information-theoretic formulations of complexity can be traced back to Shannon (1948)
and the concept of entropy as a measure of the uncertainty associated with sequences of
discrete symbols that may be transmitted over a communication channel.
In order to
rigorously understand how information in our datasets relates to Bayesian inference and
encoding complexity, we developed a theory of information (Duersch and Catanach, 2020)
rooted in understanding information as an expectation over rational belief.

Given an arbitrary latent variable z, we would like to measure the information gained
by shifting belief between hypothetical states, i.e. from q0(z) to q1(z). We require this
measurement to be taken relative to a third state of belief, r(z), which we hold to be valid.
The precise reasoning by which validity of r(z) is derived is an important epistemological
question. For our purposes, r(z) will either be rational belief, expressing our present un-
derstanding of the actual state of aﬀairs, or a choice, representing a hypothetical state of
aﬀairs following a decision.

Rational belief is deﬁned as the posterior distribution resulting from inference, which
reserves some nonnegative probability for every outcome that is plausible. In contrast, we
regard a choice as a restriction on the support of belief, eﬀectively conﬁning probability
to any distribution that we can describe. This occurs when we must adhere to a course of
action from a set of mutually incompatible options. The following postulates and Theorem 1
summarize key results.

1. Information gained by changing belief from q0(z) to q1(z) is quantiﬁed as an expec-

tation over a third state r(z), called the view of expectation.

2. Information is additive over independent belief processes.

3. If belief does not change then no information is gained, regardless of the view of

expectation.

4. Information gained from any normalized prior state of belief q0(z) to an updated

state of belief r(z) in the view of r(z) must be nonnegative.

Theorem 1 Information as a rational measure of change in belief. Information,
measured in bits, satisfying these postulates must take the form

Ir(z)[ q1(z) (cid:107) q0(z) ] =

(cid:90)

dz r(z) log2

(cid:19)

(cid:18) q1(z)
q0(z)

bits.

10

Parsimonious Inference

When the view of expectation is the same as the target belief, we recover the Kullback-

Leibler divergence (Kullback and Leibler, 1951)

D[ r(z) (cid:107) q(z) ] = Ir(z)[ r(z) (cid:107) q(z) ] .

The entropy of a distribution p(z) over discrete outcomes z ∈ {zi | i ∈ [n]} is equivalent to
the expected information gained upon realization in the view of the realization

S[ p(z) ] =

n
(cid:88)

i=1

p(zi) log2

(cid:18) 1

(cid:19)

p(zi)

bits.

Our theory allows us to relate and analyze changes in belief regarding our data, model
parameters, and hyperparameters within a uniﬁed framework. Appendix A provides selected
corollaries of Theorem 1 for reference.

Our main result, Theorem 2, builds on this work to show how memorization may be
quantiﬁed, Corollary 6, and prevented. Simple changes to the model structure that beneﬁt
multiple predictions are parsimonious, worthwhile investments. In contrast, memorization
as a wasteful transfer of information from the space of predictions to the space of expla-
nations. As we must inevitably solve feasible approximations of the posterior predictive
integral in order to obtain practical predictions, our theory provides additional beneﬁt by
allowing us to analyze posterior approximations within the same formalism.

3. Complexity and Parsimony

We present our theoretical learning framework in three parts. First, we analyze a modest
generalization of Kolmogorov’s notion of program length to sequences of symbols drawn
from arbitrary alphabets that may be conditioned on previously realized symbols. This
simpliﬁes our ability to assign complexity to arbitrary descriptions of prior belief. Second,
we use description length to derive a hyperprior that extends Solomonoﬀ’s formulation
of algorithmic probability to general inference architectures. Third, we cast learning as
an information minimization principle. We show how learning balances the two forms of
information contained within models, due to both prior descriptions and inference, against
the information the models provide about our dataset. Not only does this formalism allow
us to analyze the utility of potential restrictions of prior belief, we also recover variational
inference optimization from the same principle.

3.1 Program Length and Kolmogorov Complexity

Kolmogorov’s discussion of complexity begins with a countable set of objects A = {a} that
are indexed with binary sequences. For Kolmogorov, an object a is a program and the
length of the program (cid:96)(a) is taken to be the number of binary digits in a corresponding
binary sequence ψ(a). Given a domain of program inputs X and a codomain of outputs Y,
a programming method ϕ(·, ·) accepts the program a and an input x ∈ X and returns an
output y = ϕ(a, x) ∈ Y. The Kolmogorov complexity of an ordered pair (x, y) ∈ X × Y is
the length of the shortest program that is capable of reproducing the pair

Kϕ(x, y) = min
a∈B

(cid:96)(a) where B = {a | y = ϕ(a, x)} ⊂ A.

11

J. A. Duersch and T. A. Catanach

An ordered pair may be understood to enforce multiple function values or even the entire
mapping that deﬁnes a function. Further, Kolmogorov’s framework easily captures the
complexity of a singleton y by taking an empty input, x = ∅.

Because Turing-complete programming methods can simulate one another, the shortest
length of a program in a new language is bound from above by that of the source lan-
guage plus a constant; the new shortest program cannot be longer than simply attaching a
simulator to the source version. Thus, the length of an eﬃcient simulator for the original
programming method provides the bounding constant oﬀset.

The descriptions of interest to us, however, may not admit perfectly eﬃcient binary
codes. For example, we may wish to represent the outcome of rolling of a balanced six-
sided die, having an approximate entropy of 2.585 bits. Rather than solving for an optimal
binary encoding (Huﬀman, 1952), yielding an expected length of approximately 2.667 bits,
it is convenient to extend the notion of the length to ﬁnite sequences of symbols drawn
from multiple alphabets. In this case, using an alphabet with six symbols would allow the
encoding to exactly achieve the entropy limit. See Section 4.3 for another example in which
this extension supports eﬃcient descriptions of feature domain partitions.

Let a description be composed of a sequence of symbols represented as ψ = (si)n
i=1.
When we wish to draw attention to the role of a sequence as an encoding of an object, we
write ψ(a). For our purposes, these objects do not necessarily need to be programs. Rather,
they are simply descriptions of belief, p(θ | ψ). For each i ∈ [n], a symbol si is selected
from an alphabet Σi. We emphasize that each alphabet is allowed to depend on previously
realized symbols in the sequence so that the sequence of alphabets is not ﬁxed. To avoid
cumbersome notation and excessive indexing variables, we express subsequences as (s)j
1 and
leave the natural indexing (si)j
i=1 implied. Let (s)0
1 indicate the empty subsequence. We
also leave the conditional dependence of each alphabet on previous symbols implied so that
(cid:3). As with Kolmogorov, the length of an
(cid:2)(s)i−1
we may simply write Σi rather than Σi
1
object is derived from a sequence, (cid:96)(a) = (cid:96)(ψ(a)).

If we treat each symbol in an encoding ψ(a) as a discrete random variable, we have

p(ψ(a)) =

n
(cid:89)

i=1

p(si | (s)i−1

1

).

The entropy corresponding to each potential symbol, or the information we expect to gain
upon realization, is the maximum if and only if the probability of each symbol is uniform
over its alphabet, p(si | (s)i−1

) = 1

|Σi| . That is,

1

(cid:88)

si∈Σi

p(si | (s)i−1

1

) log2

(cid:18)

(cid:19)

1
p(si | (s)i−1

1

)

≤ log2(|Σi|).

Our construction of generalized length in Deﬁnition 1 invokes the principle of maximum
entropy to remove restrictions on the kinds of codes we can consider, while recovering
Kolmogorov’s length when all symbols are binary digits.

Deﬁnition 1 Generalized Length as Maximum Entropy Encoding. The general-
ized length of an arbitrary sequence ψ is the upper bound on entropy of the corresponding

12

Parsimonious Inference

sequence of alphabets from which each symbol is drawn,

(cid:96)(ψ) =

n
(cid:88)

i=1

log2(|Σi|) bits.

Corollary 1 shows that this length saturates the information lower bound in Shannon’s
source coding theorem (Shannon, 1948). We provide all proofs in Appendix B. Further-
more, when we develop an eﬃcient encoding for the kinds of objects we would like to use,
Corollary 2 allows us to naturally derive the probability of an object from the encoding.

Corollary 1 Generalized Length Lower Bound. Given a set of objects A and proba-
bilities p(a) for all a ∈ A, the expected generalized length of an object is bound from below
by the entropy

Ep(a) (cid:96)(ψ(a)) ≥ Ep(a) log2

(cid:18) 1

p(a)

(cid:19)

.

Corollary 2 Probability from Length. Given a set of objects A and a maximum entropy
encoding with p(a) = p(ψ(a)) for all a ∈ A, the generalized length satisﬁes

p(a) = 2−(cid:96)(a).

3.2 Algorithmic Probability

Five years before Kolmogorov published his work on mapping complexity, Solomonoﬀ artic-
ulated the foundations for inductive inference and algorithmic probability. He was speciﬁ-
cally interested in programs capable of reproducing a binary sequence y, i.e. the subset of
programs B = {a | y = ϕ(a, ∅)} ⊂ A, and he derived the probabilistic contribution of each
program to plausible continuations of the sequence

p(a | y) ∝

(cid:40)

2−(cid:96)(ψ(a)) a ∈ B
a /∈ B
0

where, as with Kolmogorov’s picture, length corresponds to an optimal binary encoding,
subject to an optimal UTM. That is, the UTM for which the optimal binary encoding
is shortest. We understand his result as Bayesian inference wherein Corollary 2 provides
prior belief and a program has unit likelihood if it halts and reproduces the sequence.
Otherwise, the likelihood is zero. It follows that the Kolmogorov complexity is simply the
length of the Maximum A Posteriori (MAP) estimator in the same picture. As such, the
Kolmogorov complexity is the minimum amount of information that is possible to gain by
restricting belief to a discrete program that is capable of reproducing a desired ordered
pair. If, however, we allow distributions of belief over many programs, so that q(a) ≥ 0 for
any a ∈ B, Corollary 3 shows that Solomonoﬀ’s algorithmic probability is the minimizer of
information gain, improving beyond the Kolmogorov complexity.

Corollary 3 Information Optimality of Solomonoﬀ Programs. If we measure the
change in belief from all possible programs according to Corollary 2 to any distribution

13

J. A. Duersch and T. A. Catanach

q(a) that restricts belief to programs capable of reproducing an input-output pair (x, y), the
minimizer

q∗(a) = argmin

q(a)

D[ q(a) (cid:107) p(a) ]

subject to q(a) = 0 ∀ a /∈ B = {a | y = ϕ(a, x)} ,

is uniquely given by

q∗(a) =

2−(cid:96)(ψ(a))
p(B)

∀ a ∈ B where p(B) =

2−(cid:96)(ψ(a)).

(cid:88)

a∈B

Solomonoﬀ’s picture is even more general than it ﬁrst appears. There is no need to
conﬁne our attention to binary programs that reproduce binary sequences. We may apply
the same framework to any algorithm that generates coherent probabilities on a given
dataset, p(y | x, a). Since any algorithm we write is ultimately still a program, this induces
universal prior belief over arbitrary predictive algorithms. Again, such a prior depends on
the choice of UTM or programming method. Then, Bayesian inference yields rational belief
as a posterior distribution over all such algorithms.

Yet, this approach is fundamentally diﬃcult because it requires us to eﬃciently explore
the posterior over suitable programs via their discrete sequences. Since it is not always
possible to anticipate how a program will respond to given inputs in ﬁnite time, we arrive
at the problem of uncomputability. Moreover, even if we discover seemingly high posterior
programs, we cannot guarantee that our sample adequately approximates the posterior
predictive integral. We discuss this problem and a potential solution further in Section 5.3.
Rather than restricting our attention to programs, we would like to allow more gen-
eral inference architectures. As indicated earlier, we accomplish this by relaxing ψ to be
merely a description of prior belief. Doing so requires an interpreter, which translates valid
sequences into coherent distributions over valid models, p(θ | ψ), and then computes coher-
ent likelihoods from speciﬁc models, p(y | x, θ). If the interpreter is Turing complete, then
a description could still be a complete program, but we can also consider interpreters that
merely require speciﬁcation of a few hyperparameters of a distribution that is well-suited
to our data source.

Not only are short descriptions easier to discover, letting our data drive updates in
belief through inference is often substantially more information-eﬃcient than accounting
for a full program that computes the equivalent result. In this picture, we lose the ability of
interpreters to simulate one another, but we gain access to simpler encodings that may be
much easier to propose and evaluate. That said, the choice of interpreter is an important
problem. We revisit this issue in Section 5.1.

We remark that subjective prior beliefs may be expressible by allowing symbol proba-
bilities to be nonuniform over relevant alphabets. In this view, just as generalized length is
the limit of expected information gained by realization from an encoding, the corresponding
probability in Corollary 2 may be regarded as the limit of subjective priors within a given
encoding. Subjective prior beliefs are also reﬂected by the choice of interpreter.

The parsimonious hyperprior over corresponding model classes from Corollary 2, p(ψ) =
2−(cid:96)(ψ), is enough to complete the Bayesian framework with well-founded justiﬁcation for
how we arrive at prior belief over arbitrary architectures. When we perform Bayesian

14

Parsimonious Inference

inference from a parsimonious prior or hyperprior, we call the result parsimonious rational
belief. To achieve computational feasibility, however, we still need to investigate principled
restrictions of belief.

3.3 The Principle of Information Minimization

We develop this paradigm in order to promote computational feasibility while retaining
well-founded theoretical justiﬁcation for resulting predictions. As alluded to in Corollary 3,
we can cast learning as an information minimization problem over our total change in belief
due to observing the training data D, selecting one or more model classes ψ, and solving for
distributions over models θ within each class. The principle of minimum information (Evans,
1969), based on the closely related principle of maximum entropy (Jaynes, 1957), intuitively
states that driving the information gained upon viewing the training data to be as low as
possible, we obtain better predictions. If a dataset contains a highly predictive pattern,
then once that pattern is known we can obtain strong predictions. As a consequence, the
information gained by observing new labels drops. In contrast, the information gained by
observing new labels will remain high when there is no discernable pattern. We derive
Theorem 2 from a rigorous formulation of the minimum information principle using our
work regarding information as a rational measure of change in belief and show how this
information objective can be manipulated into three terms that provide insight into how
we may understand and control complexity during learning as a constrained optimization
problem.

Theorem 2 Parsimonious Inference Optimization. Let our training dataset be rep-
resented as an ordered pair (x, y). A model θ computes coherent probabilities over potential
labels y from features x as p(y | x, θ). Shorthand p(y | θ) leaves dependence on x implied.
A description of the model class is represented by a sequence ψ, which restricts prior belief
to p(θ | ψ). The parsimonious hyperprior over potential sequences induced by generalized
length is p(ψ) = 2−(cid:96)(ψ). Our joint belief in labels, models, and prior encodings is given by
p(y, θ, ψ) = p(y | θ)p(θ | ψ)p(ψ). Viewing the realized labels ˇy from the dataset changes
rational belief to r(y | ˇy), a distribution assigning full probability to the observed outcomes.
Let any potential choice of belief over descriptions after viewing the data be q(ψ). Like-
wise, within a model class ψ, an arbitrary distribution over models is q(θ | ψ). The total
information gained is given by the Kullback-Leibler divergence

D[ r(y | ˇy)q(θ | ψ)q(ψ) (cid:107) p(y | θ)p(θ | ψ)p(ψ) ] .

The minimizer of this objective is equivalent to the maximizer of the following parsimony
objective, expressed in three parts

ω[q(θ | ψ), q(ψ)] = Eq(θ|ψ)q(ψ) Ir(y|ˇy)[ p(y | θ) (cid:107) p(y | θ0) ]

(prediction information)

− Eq(ψ) D[ q(θ | ψ) (cid:107) p(θ | ψ) ]
− Eq(ψ) (cid:96)(ψ) + S[ q(ψ) ] ,

−(inference information)

−(description information)

where θ0 anchors the predictive information measurement to any ﬁxed baseline.

15

J. A. Duersch and T. A. Catanach

The notation r(y | ˇy) is intended to emphasize that our rational belief in plausible labels
is totally restricted to what we have observed after viewing the evidence. In contrast, the
arguments to the optimization objective, q(θ | ψ) and q(ψ), do not depend on the data
until optimization constrains them.

We hold that the view of expectation taken in Theorem 2 is valid because it represents
the known labels and the actual distributions that will be used in practice to compute
predictions. Further, this construction of information, rather than the reversed divergence
D[ p(y, θ, ψ) (cid:107) r(y | ˇy)q(θ | ψ)q(ψ) ], is necessary to avoid multiple inﬁnities; for each de-
scription ψ with q(ψ) = 0, the reversed divergence is inﬁnite. Moreover, we cannot avoid
eliminating an inﬁnite number of such cases from consideration.

The ﬁrst term, prediction information, is the expected information gained about training
data resulting from our belief in explanations. Both secondary terms, inference information
and description information, account for model complexity. Anchoring predictive informa-
tion to any ﬁxed model p(y | θ0) allows us to coherently interpret label information as that
which is gained relative to θ0. Any ﬁxed predictive distribution suﬃces, including the prior
predictive

(cid:90)

p(y | x) =

dψ dθ p(y | x, θ)p(θ | ψ)p(ψ).

However, because the prior predictive may be diﬃcult (or impossible) to compute, it is
much simpler to use a na¨ıve model θ0. If we disregard the role of ψ and only account for
prediction information and inference information from prior belief p(θ), i.e. from the ﬁrst
two terms of the parsimonious inference objective, then we recover a form of the Bayesian
Occam’s Razor (MacKay, 1992),

Ir(y|ˇy)[ p(y) (cid:107) p(y | θ0) ] = log2

= Ep(θ|ˇy) log2

(cid:19)

(cid:18) p(ˇy | θ)
p(ˇy | θ0)

(cid:18) p(ˇy)

(cid:19)

p(ˇy | θ0)

− D[ p(θ | ˇy) (cid:107) p(θ) ] ,

provided we use the exact posterior p(θ | ˇy) in place of q(θ | ψ). Otherwise, we recover a
variational inference objective that is equivalent to maximizing the Evidence Lower Bound
(ELBO). The Bayesian Occam’s Razor also reveals the tradeoﬀ between the information
that explanations provide about our data and the complexity of those explanations, but it
leaves the provenance of p(θ) unaddressed.

The description information terms show how our theory subsumes the Principle of Max-
imum Entropy. If we were to disregard the critical role that the parsimonious hyperprior
plays in controlling complexity, i.e. dropping expected length, we would be left with an
optimization objective that drives increases in entropy within our chosen distribution of
descriptions. Doing so, however, would mean that long and complicated descriptions would
be just as plausible as short and simple descriptions, as long as they are equally capable
of explaining the data. Correctly accounting for description length completes a rigorous
formulation of Occam’s Razor.

Critically, Corollaries 4 and 5 show that unconstrained optimization recovers, and is
therefore consistent with, Bayesian inference and parsimonious rational belief. As demon-
strated in Section 4.3, some prior beliefs facilitate exact inference and easily allow us to
take q(θ | ψ) = p(θ | ˇy, ψ).

16

Parsimonious Inference

Corollary 4 Optimality of Inference. Given a single description ψ specifying prior
belief p(θ | ψ), the conditionally optimal distribution over models,

q∗(θ | ψ) = argmax
q(θ|ψ)

Eq(θ|ψ) Ir(y|ˇy)[ p(y | θ) (cid:107) p(y | θ0) ] − D[ q(θ | ψ) (cid:107) p(θ | ψ) ] ,

is the posterior distribution q∗(θ | ψ) = p(θ | ˇy, ψ).

Corollary 5 Optimality of Hyper Inference. Applying the optimizer from Corollary 4
to the objective in Theorem 2 produces the second optimization problem

q∗(ψ) = argmax

q(ψ)

Eq(ψ) log2

(cid:19)

(cid:18) p(ˇy | ψ)
p(ˇy | θ0)

− D[ q(ψ) (cid:107) p(ψ) ] .

The optimizer is the hyperposterior distribution, q∗(ψ) = p(ψ | ˇy).

Yet, unconstrained optimization of q(ψ), subject to a Turing-complete interpreter,
Instead, we
would need to explore unlimited varieties of programs and model classes.
can restrict the support of prior belief and posterior approximations to a feasible set,
F = {q(θ | ψ)q(ψ)}, and Theorem 2 still provides a consistent framework to evaluate
and compare the utility of such restrictions.

The parsimony objective also allows us to understand and quantify memorization of
training data in Corollary 6 as a bound on the increase in model complexity that is required
to achieve increased agreement between predictions and our training data. This bound also
holds when we restrict F.

Corollary 6 Quantifying Memorization. We can write the combined model complexity
terms as χ[q(θ, ψ)] = D[ q(θ, ψ) (cid:107) p(θ, ψ) ] and let q∗(θ, ψ) be the constrained optimizer of
the parsimony objective, restricted to a given feasible set F = {q(θ, ψ)}. Let the optimal
predictions be written as

(cid:90)

q∗(y) =

dψ dθ p(y | θ)q∗(θ, ψ).

Every feasible alternative q(θ, ψ) must satisfy

χ[q(θ, ψ)] − χ[q∗(θ, ψ)] ≥ Eq(θ,ψ) Ir(y|ˇy)[ p(y | θ) (cid:107) q∗(y) ] ,

showing that any increased agreement with training data can only be achieved by a still
greater increase in model complexity.

Section 4.2 includes a visualization of this complexity tradeoﬀ, Figure 4 corresponding
to potential polynomial representations for a regression model. As our experiments demon-
strate, restricting our attention to classes of simple descriptions provides a tractable means
to discover models and control complexity.

17

J. A. Duersch and T. A. Catanach

4. Implementation

The parsimony objective acts on opportunities for compression to reduce the complexity
of our belief over models through both the description of prior belief and the information
gained due to inference. While there are many ways to encode the concepts that we need
to articulate prior belief, compression is only possible if the interpreter admits a range of
code lengths. Consequently, it is important to review some eﬃcient encodings, capable of
expressing increasing degrees of speciﬁcity with longer codes, that are needed by our proto-
type implementations. Then we discuss our algorithms for polynomial regression followed
by decision trees.

4.1 Useful Encodings

Sometimes we need to identify one of multiple states without any principle that would al-
low us to break the symmetry among potential outcomes. For example, our decision tree
algorithm requires a feature dimension to be speciﬁed from n possibilities. Laplace’s prin-
ciple of insuﬃcient reason indicates that our encoding should not break symmetry among
hypothetical permutations of the features. We can easily handle this case by representing
each state with a single symbol from an alphabet of n possibilities.

As the cardinality of the set increases, however, the information provided by realizing
a symbol increases logarithmically. Thus, this approach cannot hold when we have count-
ably inﬁnite sets, such as with integers or rational numbers, or information would diverge.
Instead, we must break symmetry with either some notion of magnitude, some notion of
precision, or both.

4.1.1 Nonnegative integers

Rissanen’s universal prior over integers (Rissanen, 1983) can be derived by counting out-
comes over binary sequences of increasing length. Provided the sequence length is known,
any nonnegative integer z can be encoded with (cid:98)log2(z + 1)(cid:99) binary digits, as shown in
Table 1. Yet, the sequence length is also a nonnegative integer, thus a recursive encoding
of arbitrary nonnegative integers will have length approaching

log∗

2(z) =(cid:98)log2(z + 1)(cid:99) + (cid:98)log2 ((cid:98)log2(z + 1)(cid:99) + 1)(cid:99)

+ (cid:98)log2 ((cid:98)log2 ((cid:98)log2(z + 1)(cid:99) + 1)(cid:99) + 1)(cid:99) + · · · .

Sequence
z

0

0
1

1
2

00
3

01
4

10
5

11
6

000
7

001
8

· · ·
· · ·

Table 1: Enumeration of binary sequences of increasing length.

The Elias γ coding (Elias, 1975) simply represents the sequence length with a negated
unary preﬁx. Elias δ codes add one recursion, thus representing the sequence length with
a γ code. Elias ω codes allow for arbitrary recursions by building up positive integers that
either represent the length of the next sequence or the ﬁnal outcome. Decoding begins with
the initial value N = 1. If the next bit is 0, then N is the ﬁnal value. Otherwise, the leading

18

Parsimonious Inference

1 followed by N bits encodes the updated value of N . The process repeats until the next
segment has a leading 0. We can take z = N − 1 for nonnegative integers.

In practice, however, we do not need representations for arbitrarily large integers and
we can obtain more eﬃcient codes with a limited maximum representation. For example,
we can start with a single bit to represent the length of the subsequent code segment and
iterate representations from Table 1 a predetermined number of times. If we know how many
recursions are needed to represent a maximum integer, we obtain a code that approximates
Rissanen’s universal prior. Table 2 shows how the ﬁrst few Rissanen codes are formed. This
encoding becomes very eﬃcient for large integers, but the number of length recursions must
be set high enough.

ψ0
z0
ψ1
z1
ψ2
z2

0
0

0

0

1
1
0
1
0
1

1
1
0
1
1
2

1
1
1
2
00
3

1
1
1
2
01
4

1
1
1
2
10
5

1
1
1
2
11
6

Table 2: The ﬁrst sequence ψ0 has an implied length of 1 bit. The represented outcome z0 indicates
the length of ψ1 and so on. Rissaneni codes are formed by concatenation (ψ0, ψ1, . . . , ψi).
With three length recursions, numbers 0 through 126 are compressed to use between 1
and 9 bits.

We can also obtain good compression by using a single symbol to indicate the length
of the remaining sequence. Length-symbol codes also approximate the scaling invariance
of Jeﬀrey’s prior. See Section 5.4 for further discussion of this property. Table 3 provides
a comparison. Although we show codes with a 2-bit length symbol for easy comparison
to other binary codes, the length symbol does not need to have a binary representation in
general.

z
Unary
Elias γ
Elias δ
Elias ω
Rissanen1
Rissanen2
Rissanen3
Length-symbol

0
0
1
1
0
0
0
0
00

1
10
010
0100
100
10
100
1000
010

2
110
011
0101
110
11
101
1001
011

3
1110
00100
01100
101000
n.r.
1100
10100
1000

4
11110
00101
01101
101010
n.r.
1101
10101
1001

5
111110
00110
01110
101100
n.r.
1110
10110
1010

6
1111110
00111
01111
101110
n.r.
1111
10111
1011

7
11111110
0001000
00100000
1110000
n.r.
n.r.
1100000
11000

Table 3: Nonnegative integers with unary codes, Elias codes, Rissanen codes, and a 2-bit length-

symbol code. Integers that have no representation are indicated by n.r.

19

J. A. Duersch and T. A. Catanach

4.1.2 Binary Fractions

It will also be useful to represent a dense distribution of fractions on the open unit interval
q ∈ (0, 1), thus allowing us to approximate any real number to arbitrary precision by a
variety of potential transformations. Binary fractions, with a denominator that is an integer
power of 2 and a numerator that is odd, provide such a set with a convenient encoding.
These fractions can be written as

q =

2i − 1
2z+1

where

i ∈ [2z]

and z ∈ Z≥0.

If we desire all fractions of a speciﬁc precision, corresponding to a ﬁxed z, to have
the same encoding length, then the numerator may be regarded as a single symbol with 2z
outcomes or z bits. We must also represent the precision z with one of the integer encodings
above. As with the length-symbol codes, we can also represent z with a single symbol that
indicates the number of numerator bits to read. Table 4 shows some examples.

We can then translate and scale q to represent an angle on the real Riemann circle, the
corresponding real numbers are r = tan(π(q−1/2)), but other choices are also possible, such
2 erf −1(2q − 1). Multiplying
as inverting the normal cumulative distribution function r =
the result by some σ > 0 would set any desirable scale of outcomes.

√

q
Code

1/2
00

1/4
010

3/4
011

1/8
1000

3/8
1001

5/8
1010

7/8
1011

1/16
11000

· · ·
· · ·

Table 4: Leading binary fractions on the open unit interval with a 2-bit encoding of z.

4.2 Polynomial Regression

Our regression prototype directly encodes a polynomial with a description of coeﬃcients
and captures uncertainty with a hyperposterior ensemble. Since any nth degree polynomial
can be written as a linear combination of n + 1 basis functions of ascending degree, we
ﬁrst need to identify the degree of polynomial. Our experiments show that a length-symbol
encoding serves this purpose well. Coeﬃcients are represented in the Chebyshev basis.
Since critical points equioscillate in this basis, the corresponding polynomial coeﬃcients are
interpretable as the length scales of oscillation. Because we expect all n + 1 coeﬃcients to
take nontrivial values, the encoding reserves a variable-length segment for each coeﬃcient,
rather than attempting to use a sparse encoding. Still, natural sparsity will result from
binary fractional codes, representing angles on the Riemann circle, after transforming them
to the corresponding real numbers.

Algorithm 1 samples the hyperposterior using a nonreversible sequence of reversible
samples over all representable polynomial coeﬃcients. We regard each sample of poly-
nomial coeﬃcients as a Dirac delta distribution concentrated at a single polynomial. This
is equivalent to treating the complexity hyperprior as an ordinary prior over polynomial
descriptions. Because our encoding length is most sensitive to the degree, our sampler
proposes all joint perturbations of the leading nonzero and each other coeﬃcient in a ran-
domly permuted order. All other coeﬃcients are held ﬁxed in each proposal set. For each
coeﬃcient, the sampler considers all binary fractions with z ≤ 4.

20

Parsimonious Inference

Algorithm 1 Parsimonious Polynomial Regression Gibb’s Sampler
Require: Vectors x and y provide abscissas and ordinates, respectively, with y scaled so that the

intrinsic stochasticity of the process is σ = 1. Generate n samples from polynomials using at

most b Chebyshev basis functions.

Ensure: Ψ = {ψ} is an ensemble of hyperposterior polynomial descriptions ψi ∼ p(ψ | X, y).
1: function ParsimoniousRegression(x, y, n, b)

2:

3:

4:

5:

6:

7:

8:

9:

10:

11:

Initialize ψ to the zero polynomial

for each sample iteration i = 1, 2, . . . , n do

Generate a random permutation of the basis functions.

for each permuted coeﬃcient j = 1, 2, . . . , b do

Identify the leading nonzero coeﬃcient k in ψ.

Form tensor product of all representable perturbations over both j and k.

Update ψ by sampling the hyperposterior, restricted to these perturbations.

end for

Add ψ to the hyperposterior ensemble Ψ.

end for

12: end function

Figure 3 compares the complexity suppression of leave-one-out cross-validation with our
results from Algorithm 1 using 20th degree polynomials. In order to provide a fair compar-
ison, we generate 21 full leave-one-out ensembles, corresponding to each polynomial degree,
and then select the ensemble with the best average over holdout log-likelihoods. Thus,
both approaches explore the same model families, wherein all coeﬃcients above a certain
degree are zero. We see that, if the hyperparameter search covers a range of dimensions, the
standard approach achieves some, albeit limited, success in identifying relatively low com-
plexity ensembles. We also tested unary, Elias γ, and suﬃcient Rissanen codes for both the
polynomial degree and corresponding binary fractions using the data in the second row of
Figure 3. Speciﬁcally, we used Rissanen3 codes for the polynomial degree and Rissanen2 for
the binary fraction precision. The resulting aggregate parsimony objectives are compared
in Table 5.

Polynomial encoding Unary Elias γ Rissanen Length-symbol
104.8

Parsimony objective (bits)

103.0

104.5

101.9

Table 5: Length-symbol codes give the optimal parsimony objective for this dataset.

Figure 4 demonstrates memorization by applying Algorithm 1 to the same data in the
second row of Figure 3, but replacing the hyperprior with a uniform distribution, eﬀectively
sampling the likelihood by disregarding generalized length. Plotting prediction information
against model complexity (left) for both ensembles shows that the likelihood ensemble has
much higher complexity. If we constrain feasible beliefs to only single samples from either
of the ensembles, then the MAP deﬁnes the complexity tradeoﬀ limit in Corollary 6, as

21

J. A. Duersch and T. A. Catanach

Figure 3: Regression experiments comparing leave-one-out cross validation (left column), the hyper-
MAP (middle column), and the hyperposterior aggregated over 50 samples (right col-
umn). All data come from the same ground truth. The hyper-MAP is consistently
simpler than the leave-one-out aggregate. The hyperposterior aggregate naturally cap-
tures extrapolation risk, increasing uncertainty as we deviate from data. More data allow
modest increases in complexity to reduce uncertainty.

Figure 4: Memorization demonstration with regression. We plot prediction information against
model complexity (left) for models obtained from Algorithm 1 (left cluster) and likeli-
hood samples from the same algorithm by disregarding generalized length (right cluster).
Likelihood samples have much longer descriptions, some achieving better agreement with
the data (memorization domain). Neither the MLE (middle) nor the likelihood ensemble
(right) perform well.

well as the corresponding memorization domain in which models may gain increased agree-
ment with the data at the cost of an even greater increase in complexity. Memorization is
worst at the Maximum Likelihood Estimator (MLE), found within the likelihood ensemble.

22

Parsimonious Inference

The corresponding predictions (middle) closely ﬁt the data. Predictions from the likeli-
hood ensemble (right) show increased uncertainty, but still violate Occam’s Razor, thus
underscoring the central role of generalized length in suppressing memorization.

4.3 Decision Trees

Decision trees predict discrete classiﬁcations, labels, by evaluating a sequence of binary
decisions. Each case in our training dataset is represented by both a feature vector x, with
k components that are each comparable to a threshold, and an enumerated label y ∈ [(cid:96)],
where (cid:96) is the number of labels. Evaluation begins at the root note, representing the axis-
aligned bounding box of potential features. The node speciﬁes a feature dimension and
a comparison threshold serving to partition the feature domain into two components, the
left and right child nodes. The comparison outcome indicates membership and the process
iterates so that a sequence of binary decisions ﬁlters each case through a series of increasingly
restrictive partitions, each of which is intended to simplify the classiﬁcation problem. This
ﬁltration terminates at a leaf node that speciﬁes either a single label or, more generally,
probabilities over all labels.

Decision trees are trained using a recursive process that also begins at the root node.
We take the set of training cases that are members of a given node and we must either
construct a branch structure or halt splitting and ﬁnalize label probabilities. The conven-
tional procedure evaluates every potential splitting outcome with some utility function and
then chooses the optimizer. While a wide variety of utility functions are used in practice,
a standard information-theoretic approach maximizes the reduction in entropy due to the
splitting.

Let cy represent the count of training cases with the label y that fall within a given node
domain. If the node were a leaf, the frequentist approach to predicting label probabilities
would use the sample mean

µy =

cy
c

where

c =

(cid:96)
(cid:88)

y=1

cy.

We denote the corresponding variables for hypothetical left and right child nodes using
superscripts, e.g. c(L)
, respectively. The reduction in entropy associated with
a potential splitting, weighted by the fraction of cases that appear within the respective
domains of each child, is

and c(R)

y

y

∆S =

c(L)
c

(cid:96)
(cid:88)

y=1

µ(L)
y

log2(µ(L)

y

) +

c(R)
c

(cid:96)
(cid:88)

y=1

µ(R)
y

log2(µ(R)

y

) −

(cid:96)
(cid:88)

y=1

µy log2(µy).

The splitting that maximizes ∆S is accepted and the resulting child nodes are trained
recursively until no further reduction is possible, i.e. when a node contains only cases of a
single label.

Bootstrap aggregation constructs an ensemble of decision trees, a random forest, by
resampling the dataset. This consists of forming a new dataset, the same size as the
original, by sampling the original dataset uniformly with replacement. Predictions are then
aggregated by taking the average over the ensemble.

23

J. A. Duersch and T. A. Catanach

Our approach encodes each decision tree as a state of prior belief on the corresponding
partition of feature coordinates. We describe the partition recursively using an encoding
for each node in the binary search tree. Each node begins by specifying whether it is a leaf
or branch with 1 bit. If the node is a branch, a symbol from k possibilities gives the feature
dimension used in the comparison, thus contributing log2(k) bits to the generalized length.
Since we can translate and scale features in the given dimension to the unit interval [0, 1],
we can represent any splitting threshold as a binary fraction on the open unit interval. Note
that it is never useful to split at either 0 or 1. The encoding continues by describing the
left and right children.

If the node is a leaf, its feature partition has an independent prior, a ﬂat Dirichlet
distribution, over the simplex of all coherent label probabilities. Let θ represent a vector of
label probabilities within a single leaf. We can perform exact inference using label counts
cy to recover Laplace’s rule of succession

Ep(θ|ˇy) [θy] =

cy + 1
c + (cid:96)

,

which is the posterior-predictive distribution for labels of new data that land within this
leaf.

Our parsimonious decision trees are generated using a recursive process that begins by
calling Algorithm 2 on the full training dataset to form the root node. The remaining
structure is generated by sampling both a feature dimension and threshold or by halting to
form a leaf node that infers label probabilities as above. If the structure splits, then data
are partitioned accordingly, and the process repeats on the left and right children.

If we wanted to sample each splitting from the exact posterior, we would need to
marginalize over all elaborations to the node structure that could follow. Since the num-
ber of such structures grows exponentially with depth, this would not be computationally
feasible. Instead, we must sample an approximate posterior by assuming children will be
leaf nodes, as in the standard approach. Unfortunately, this approximation can generate
over-attractive splitting domains, thus making it diﬃcult to sample high posterior alterna-
tives. To mitigate this problem, we anneal the likelihood in the posterior approximation
to increase sample diversity. In our experiments, we used an annealing schedule that disre-
gards the likelihood for the ﬁrst two branch depths, α = [0, 0, 1, 1, . . .]. The hyperposterior
predictive integral can then be corrected using importance weighting over the ensemble of
decision trees. For each tree t with a description ψt, we need the probabilities of both the
composition of sampled proposals s(ψt) and the posterior p(ψt | X, y), up to the unknown
normalization. This gives importance weights

wt ∝

p(ψt | ˇy)
s(ψt)

so that

(cid:88)

t

wt = 1.

Because we are performing exact inference, we could skip the following information

analysis and compute the log likelihood directly from label counts

log2 (p(ˇy | ψ)) = log2

(cid:18) Γ((cid:96))

(cid:19)

Γ(c + (cid:96))

+

(cid:96)
(cid:88)

y=1

log2(Γ(cy + 1)).

24

Parsimonious Inference

Algorithm 2 Parsimonious Node Construction for Decision Trees
Require: X is a matrix of features and y is the corresponding vector of labels for data in this

node’s partition. We sample an approximate annealed hyperposterior, controlled by an annealing

schedule α and recursion depth d.

Ensure: Sample a node description ψ with proposal probability s(ψ) and unnormalized hyperpos-

terior p(ψ | X, y).

1: function [ψ, s(ψ), p(ψ | X, y)] = ParsimonyNode(X, y, α, d)

2:

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

Let ψ0 describe this node as a leaf.
Compute the likelihood p(y | ψ0) from a ﬂat Dirichlet prior over all label probabilities.
Enumerate every possible feature domain splitting for this node as i ∈ [n].

for each splitting i = 1, 2, . . . , n do

Let ψi describe this node as a branch.
Partion data with the splitting, (X (L), y(L)) and (X (R), y(R)).

Approximate the likelihood assuming both children are independent leaf nodes

p(y | ψi) ≈ p(y(L) | ψi)p(y(R) | ψi).

end for
Sample ˇψ from s(ψi) ∝ p(y | ψi)αd p(ψi) over i = 0, 1, . . . , n.
if ˇψ is a branch node then

Partion data accordingly as (X (L), y(L)) and (X (R), y(R)).

Recursively construct left and right child nodes as

(cid:104)

(cid:105)
ψ(L), s(ψ(L)), p(ψ(L) | X (L), y(L))
(cid:105)
ψ(R), s(ψ(R)), p(ψ(R) | X (R), y(R))

(cid:104)

= ParsimonyNode(X (L), y(L), α, d + 1)

and

= ParsimonyNode(X (R), y(R), α, d + 1).

Concatenate descriptions, ψ =
Compose sample probabilities, s(ψ) = s( ˇψ)s(ψ(L))s(ψ(R)).
Compose the hyperposterior

.

(cid:104) ˇψ, ψ(L), ψ(R)(cid:105)

p(ψ | X, y) = p(ψ(L) | X (L), y(L))p(ψ(R) | X (R), y(R))p( ˇψ).

else ( ˇψ is the leaf node)

Set ψ = ˇψ and s(ψ) = s( ˇψ).
Compute the unnormalized hyperposterior, p(ψ | X, y) = p(y | ˇψ)p( ˇψ).

end if

21: end function

25

J. A. Duersch and T. A. Catanach

However, we can use this opportunity to demonstrate how the analysis would proceed
if a variational approximation were used by simply replacing p(θ | ˇy) below with any
distribution from a feasible set, q(θ) ∈ F.

The amount of information due to change in model belief is

D[ p(θ | ˇy) (cid:107) p(θ) ]

=

1
log(2)



log

(cid:19)

(cid:18) Γ(c + (cid:96))
Γ((cid:96))

− c(cid:122)(c + (cid:96)) +

(cid:96)
(cid:88)

y=1

cy(cid:122)(cy + 1) − log(Γ(cy + 1))





where (cid:122)(x) = d
about labels from a uniform prior is

dx log(Γ(x)) is the digamma function. The prediction information gained

Ep(θ|ˇy) Ir(y|ˇy)[ p(y | θ) (cid:107) p(y | θ0) ] =


c log((cid:96)) − c(cid:122)(c + (cid:96)) +

1
log(2)

(cid:96)
(cid:88)

y=1



cy(cid:122)(cy + 1)

 .

Subtracting inference information from predictive information recovers the log likelihood
up to an additive constant.

Our ﬁrst set of decision tree experiments, Figure 5, examines learning from a generative
process that cleanly partitions the data into regions containing a single label. Row one

Figure 5: These ﬁrst decision tree experiments use a highly skewed generative process with well-
separated label domains. Conventional decision trees and random forests, columns 1 and
3, obtain highly conﬁdent predictions despite having few data, Our parsimonious trees
and hyperposterior aggregates, columns 2 and 4, only gradually reduce uncertainty and
demonstrate better extrapolation uncertainty.

26

Parsimonious Inference

compares models that learn from a single sample. Rows two and three learn from 25 and
100 samples, respectively. The leading two columns compare a conventional decision trees
with parsimonious decision trees. The last two columns compare bootstrap aggregation
with our hyperposterior aggregates. Every aggregate ensemble contains 1000 decision tree
samples.

This is a highly skewed generative process; even with 25 samples, the second row still
has no realizations of a blue label. Yet, the parsimonious aggregate predictions in both
rows 1 and 2 naturally increase uncertainty as the prediction domain deviates from the
training data. In contrast, the conventional approach obtains absolute certainty in regions
that lack data. Blue labels ﬁnally appear in the third row, showing how the parsimonious
forest reacts to skewed data. This generative process is incapable of generating data in the
oﬀ-diagonal regions, but without any way of knowing that, we should be highly skeptical of
certainty in the absence of evidence.

The second set of experiments, Figure 6, examines a generative process that mixes labels.
There is nonzero probability of generating a point of either label at any location, but red
labels are more likely to appear on the left and blue on the right. We observe that typical
decision trees are more complicated than their parsimonious counterparts, as expected.
In contrast, parsimonious
Moreover, complexity increases rapidly as our dataset grows.
decision trees increase complexity gradually. The typical approach also yields conﬁdent
artifacts in the vicinity of few data, whereas parsimonious trees and parsimonious forests
only gradually reduce uncertainty.

Figure 6: Our second decision tree experiments investigate a generative process with smooth mixing
of labels. Conventional approaches increase complexity rapidly with more data, yielding
predictive artifacts that hew to few points. In contrast, parsimonious trees remain simple,
only gaining conﬁdence with suﬃcient evidence.

27

J. A. Duersch and T. A. Catanach

5. Discussion

Because our formulation of complexity accounts for information from arbitrary descriptions,
it already contains the functionality needed to address a wide variety of challenges. First,
we examine how to include other sources of prior belief in this framework and how to extend
it to prior belief over multiple interpreters. Second, we discuss how changing the scope of
descriptions to account for symbols generated and communicated by elementary operations
during the evaluation of predictions provides a mechanism to prefer fast algorithms. Third,
we explore how other Bayesian hyperpriors relate to description complexity. Fourth, we
compare the non-Bayesian treatment of probability in Rissanen’s Minimum Description
Length to our approach. Finally, we oﬀer our concluding remarks.

5.1 Comparing and Inferring Interpreters

We can easily compare interpreters within the same theoretical framework by identifying a
common language, by which we indicate an interpreter that is also Turing-complete. Let
Φ = {ϕi | i ∈ [n]} represent an ensemble of interpreters written in the common language
ϕ∗. If ϕi is capable of producing a state a from an encoding ψi(a) and the interpreter
itself is encoded by ψ∗(ϕi) within the common language, then applying the same hyperprior
gives

p(a, ϕi) = p(a | ψi)p(ψi) = 2−(cid:96)(ψi(a))−(cid:96)(ψ∗(ϕi)).

This is equivalent to simply prepending each state encoding with that of the relevant in-
terpreter, ψ∗(a, ϕi) = [ψ∗(ϕi), ψi(a)], and regarding the common language as the shared
interpreter of all such codes.

Although this approach merely shifts the burden of how we derive interpreter validity to
another level of abstraction, we can still obtain practical insight into credible interpreters
with this view. Nefarious interpreters, such as the third basis in Figure 2, eﬀectively transfer
complexity from an otherwise long encoding, subject to a simple interpreter, to a short
encoding, subject to a long interpreter. Yet, when we shift the derivation of plausibility to
a common language, the excessive complexity becomes visible in both cases.

This approach also provides a formulation for grammar discovery through inference. If
we have several datasets and associated learning problems that should be explainable within
a common language, then we can infer the structure of an eﬃcient interpreter. An interpreter
that represents common functions among the diﬀerent learning problems eﬃciently will be
more likely than one that solves a single problem well by hiding complex functions with
shortcuts.

Ultimately, however, deriving interpreter validity from a common language alone cre-
ates a problem of inﬁnite regress; the question remains of how we may derive plausibility
among several languages. Although we hold that this question does not impede practical
applications, it can be answered by appealing to consistency through simulation. Theo-
rem 3 presents a unique universal prior, consistent with our theoretical framework, for any
ensemble of languages. The proof immediately follows.

Theorem 3 Universality of Consistent Belief in Turing-Complete Ensembles.
Given an ensemble of Turing-complete interpreters, Φ = {ϕi | i ∈ [n]}, we may consider an

28

Parsimonious Inference

arbitrary state of prior belief, p(ϕi) over i ∈ [n], and apply Corollary 2 to obtain the joint
probability of one interpreter simulating another

p(ψi, ψj) = p(ψi | ψj)p(ψj)

for all

i, j ∈ [n].

There exists a unique prior for which the marginalized simulation probability recovers the
prior, thus consistently accounting for simulation complexity within prior belief.

Proof of Theorem 3. A simulator ψij is a code sequence that may be prepended to any
(cid:9) include all simulators
valid code for ϕi and allow it to run on ϕj. Let the set Ψij = (cid:8)ψij
for language pairs indexed i, j ∈ [n]. Applying Corollary 2 yields the hyperprior transition
matrix, expressed elementwise by row i and column j as

p(ϕi | ϕj) =



(cid:88)





−1

(cid:88)

2−(cid:96)(ψkj )



(cid:88)

2−(cid:96)(ψij ),

k

ψkj ∈Ψkj

ψij ∈Ψij

where each column is normalized over the languages in the ensemble. A consistent prior
must satisfy p(ϕi) = (cid:80)
j p(ϕi | ϕj)p(ϕj) for all i ∈ [n]. Because all Turing-complete lan-
guages can simulate one another, Ψij is nonempty for all pairs. Existence and uniqueness
immediately follow by applying the Perron–Frobenius Theorem; every square matrix with
positive entries has a unique largest eigenvalue and the paired eigenvector may be con-
structed to have positive entries. Since the transition matrix maps every normalized state
to another normalized state, that eigenvalue must be 1 and no other eigenvectors may be
coherent probabilities.

This result shifts remaining subjectivity to the set of interpreters we are willing to
consider for comparison. In practice, the shortest simulator in each set, say ˇψij ∈ Ψij, will
dominate the corresponding matrix element. Thus, a more practical approximation of the
hyperprior transition matrix is

p(ϕi | ϕj) ≈

(cid:32)

(cid:88)

2−(cid:96)( ˇψkj )

(cid:33)−1

2−(cid:96)( ˇψij ).

k

Note 2−(cid:96)( ˇψii) = 1 for all i ∈ [n], since the shortest self-simulator is trivial in each language,
ˇψii = ∅. We ﬁnd this result intuitive because it suppresses interpreters that require excessive
complexity to simulate. In the absence of such a computation, we may only conclude that
we should prefer interpreters that appear to be simple.

5.2 Integrating Additional Beliefs

Although this work is motivated by the diﬃculty of expressing prior belief over abstract
models, when we have access to additional information that could constrain prior beliefs,
that information may be impactful. Therefore, we should be able to integrate other prior
beliefs within the general complexity framework. Let our complexity-based prior belief be
denoted as p(a | C) = 2−(cid:96)(ψ(a)). If we also have other prior beliefs, p(a | B), we can form

29

J. A. Duersch and T. A. Catanach

the composite prior p(a | B, C). For example, B may express physical laws or previously
observed data. One approach would be to use an interpreter that implicitly embeds B
within viable encodings so that p(a | B, C) = 2−(cid:96)(ψB(a)). Alternatively, if we assume that
belief derived from B is conditionally independent of our complexity-based belief C, then
we have

p(a | B, C) =

p(B | a, C)p(a | C)
p(B | C)

=

p(B | a)p(a | C)
p(B | C)

=

p(a | B)p(B)p(a | C)
p(a)p(B | C)

∝ p(a | B)p(a | C),

where p(a) must be a constant for all a since both B and C have been constructed to
capture all our beliefs. Thus, the composite prior is easily formed up to a constant of
proportionality.

5.3 The Imperative of Utility

It is not useful to consider models that, in order to provide a substantial contribution to
predictions, would require more evidence than we anticipate having. Likewise, it is not useful
to consider models that would require either more computational energy, communication
capacity, or time to evaluate than we can aﬀord. Practical models must be discoverable,
and predictions must be computable. The Kolmogorov complexity is well-known to be
uncomputable, thus raising a natural concern that generalizing prior belief to arbitrary
descriptions only exacerbates the problem. Yet, the primary purpose of Theorem 2 is
to show how information theory allows us to restrict our attention to feasible manifolds
of belief, while simultaneously allowing us to compare outcomes from diﬀerent choices of
restriction. Because long descriptions are already exponentially suppressed `a priori, the
information we generate by refusing to consider long descriptions becomes small as the
descriptions we drop become long.

Even so, it is instructive to examine uncomputability more careful as it motivates future
directions. Suppose we had an oracle Ω that would determine whether or not a program a
is capable of reproducing a mapping (x, y) in a ﬁnite amount of time

Ω(ϕ, a, x, y) =

(cid:40)

true y = ϕ(a, x)
false

otherwise.

The existence of such an oracle would allow us to determine the Kolmogorov complexity by
brute force, generating and checking programs in order of increasing length until the oracle
returns true. Further, it would be a trivial matter to write another brute force subroutine
to identify the ﬁrst sequence y with Kolmogorov complexity above an arbitrarily high limit
Kϕ(∅, y) > τ . By setting τ to exceed the combined lengths of the oracle and brute force
subroutines, we would have succeeded in writing a program that contradicts the Kolmogorov
complexity.

The core problem with this thought experiment is the arbitrarily large amount of mem-
ory and elementary operations that would be required to run the program. Disregarding the
halting problem, the brute force search would need to generate full programs in memory,

30

Parsimonious Inference

while only incurring the cost of encoding a counter. We may conclude that problems asso-
ciated with computability will be alleviated if we simply include memory operations, every
symbol generated or transmitted between slow and fast levels of memory, in the deﬁnition
of model evaluation length. Lempel and Ziv (1976) present a related framework to measure
sequence production complexity as the minimum number of steps required to build a se-
quence from a production process to construct a hierarchy of subsequences. Speidel (2008)
provides additional discussion of recent work by Titchener (1998).

Speed priors (Schmidhuber, 2002) and related work by Filan et al. (2016) develop these
approaches to articulate prior beliefs that prefer eﬃcient algorithms in the context of binary
UTMs. In this view, prior belief becomes an expression for the degree of utility considering a
model would contribute to obtaining feasible predictions. Building on these approaches will
allow us to restrict our attention to models that can be evaluated with limited resources. For
example, randomized algorithms such as Randomized QR with Column Pivoting (RQRCP)
(Duersch and Gu, 2020) would gain plausibility by having reduced slow communication
bottlenecks. In order for machine learning to be capable of providing discoverable, compu-
tationally feasible, and useful models, we cannot avoid limiting our attention accordingly.

5.4 Relationship to other Bayesian Methods

Our hyperprior provides a principled foundation to derive results that are similar in function
to several well-known methods for speciﬁc Bayesian inference problems. Notable compar-
isons include sparsity inducing priors, like Automatic Relevance Determination, for regres-
sion problems with continuous coeﬃcients. The ARD prior is a hyperprior over parameters
which is intended to identify critical parameters and drive remaining parameters towards
zero. The ARD prior is implemented as:

p(θ) = N (θ | 0, σ)

where we need to specify p(σ). In the original work introducing the ARD prior, p(σ) is a
gamma distribution in the precision τ = σ−2 with a small shape parameter. This closely
corresponds to the improper Jeﬀrey’s prior p(σ) ∝ 1
σ , often used in practice for unknown
scalar covariances because it is scaling invariant. If we partition the potential values of σ
into intervals 0 < a < b < ∞, where a and b are any positive real numbers, the cumulative
probability diverges for values less than a and values greater than b, thus dominating over
the ﬁnite contribution within [a, b]. It follows that sampling the Jeﬀrey’s prior would yield
outcomes either very close to zero or diverging towards inﬁnity. Within this formulation, if θ
has little relevance to the likelihood, then probability is maximized when θ and σ approach
zero. Otherwise, a large enough σ will be found to allow θ to take moderate nonzero values
with the Jeﬀrey’s prior introducing only a slight penalty as σ increases. Therefore, it can be
interpreted as making a binary choice between very large or very small σ. More generally,
a gamma distribution allows, indeed requires, the relative probably of these two outcomes
to be tuned.

If we uniformly discretize the possible values of σ as σi = iσ1 and assign them proba-
up to a maximum value i ∈ [M ], we can equate this prior

bilities according to p(σi) ∝ 1
σi
with the complexity prior p(σi) = 2−(cid:96)(σi) to obtain

(cid:96)(σi) = log2(σi) + c = log2(i) + log2(σ1) + c = log2(i) + (cid:96)(σ1),

31

J. A. Duersch and T. A. Catanach

where the constant c gives the normalization. Since the complexity of σi increases loga-
rithmically in i, we see that Jeﬀrey’s prior is the continuous limit of the number of bits
required to express an integer multiple of σ1. We may interpret the ﬁxed oﬀset, (cid:96)(σ1), as
the contribution of a single symbol that determines the number of bits to read.

While sparsity is a useful notion of complexity for many problems, it is not universal.
Sparsity either regards a continuous parameter as either complex (nonzero) or not complex
(zero). While sparsity-inducing priors, like ARD, can compel continuous parameters to zero
if they do not provide enough beneﬁt to predictions, they have no aﬀordance to suppress
other forms of complexity. For example, there is no compelling notion of sparsity within the
construction of decision trees. Moreover, when we need to encode constants within prior
descriptions, our theory supports consistent distinctions in complexity among potential
constants.

5.5 Relationship to Minimum Description Length

Rissanen’s Minimum Description Length (MDL) shares many similarities with our theory,
but it is not motivated by the philosophical foundations of reason that drive the Bayesian
paradigm. Rather, MDL views inference as ﬁnding an optimal compressed representation
of a dataset and probability as a way of developing eﬃcient codes. MDL representations
contain both the model used to construct an eﬃcient code and the compressed form of the
data that follows. The length of the data representation (cid:96)(D) is the sum of the number of
bits needed to describe the model (cid:96)(a) and the number of bits needed to describe the residual
data (cid:96)(D | a). In its simplest form, the inference problem for identifying a hypothesis or
program a ∈ A is

(cid:96)(D) = min
a∈A

(cid:96)(a) + (cid:96)(D | a),

requiring a speciﬁc discretization and encoding for a hypothesis space A. To address the
arbitrary task of designing a hypothesis space encoding, MDL proposes a minimax opti-
mization over universal codes, minimizing the worst-case regret associated with arbitrary
data. This simple form of MDL can also be reﬁned to compare and optimize hypothesis
classes instead of individual hypotheses, which corresponds to the Bayesian model-class
selection problem. Gr¨unwald (2007) provide an in-depth exposition.

While there is some similarity between our approach and that of MDL, our theory is
driven by a comprehensive treatment of information, and a consistent derivation of prior
belief from complexity. MDL is also consistent with some objectivist Bayesian formulations,
such as Jeﬀrey’s priors, however the philosophical motivation is quite diﬀerent. Although
optimizing MDL encoding length drives at a notion of simplicity, it is not framed within an
extended logic to update beliefs from prior or intermediate results, subjective or otherwise.
Likewise, Minimum Message Length (MML) (Wallace and Boulton, 1968; Boulton and
Wallace, 1975; Wallace and Freeman, 1987) is a Bayesian framework that is similar to
MDL. Instead of optimizing hypothesis encodings to minimize worst-case regret, MML
minimizes expected code length, which depends on a subjective prior over the attributes
a code describes. We assert that a consistent treatment of both information and prior
complexity is critical in the abstract setting of machine learning.

32

Parsimonious Inference

Further, we highlight the deeper understanding of optimal representations that our the-
ory provides compared to MDL and MML. Optimization is fundamentally inconsistent with
Bayesian probability theory; inference compels posterior belief from a prior state and the
result expresses our rational belief in possible models. Yet, we recognize that a choice must
be made to simplify this process so that problems can be solved on machines with ﬁnite
resources. This is why we must distinguish rational choice from rational belief. Rational
choices are informed by rational belief, but also require a utility function. Building on
Bernardo’s work (Bernardo, 1979), Corollaries 4, 5, 10 and 11 show the variety of circum-
stances in which information is a proper utility function that serves to guide well-posed
optimization for rational choices. The rational choice becomes the representation we use
to approximate posterior belief for future predictions. While a rational choice could be a
single model, as in MDL and MML, other representations, such as the ensembles in our
experiments, have greater utility and provide better prediction uncertainty quantiﬁcation.

5.6 Summary and Conclusion

We proposed Parsimonious Inference, a complete theory of learning based on an information-
theoretic formulation of Bayesian inference that quantiﬁes and suppresses a general notion
of explanatory complexity. We showed how our information-theoretic objective allows us to
understand the relationship between model complexity and increased agreement between
predictions and data labels.

Within the Bayesian perspective, once the prior, the likelihood, and the data are spec-
iﬁed, the posterior inexorably follows. Yet, when we consider the inﬁnite varieties of algo-
rithms that may be developed in machine learning, we ﬁnd that any universal prior that
reserves some degree of plausibility for an arbitrary algorithm becomes uncomputable in
practice. Our framework allows us to resolve the imperative of utility by quantifying the
value of a choice, wherein we only consider a feasible set of prior beliefs and posterior ap-
proximations. By accounting for model complexity from ﬁrst principles, we can evaluate
the utility of such restrictions within a single framework to obtain well-justiﬁed predictions
within a practical computational budget.

A central aspect of our framework is the distinction between the intrinsic meaning of
a potential state of belief and an eﬃcient encoding of that state. Encoding complexity
provides a critical missing component that is needed to measure the complexity of arbitrary
inference architectures and naturally associate complexity with plausibility. Our formula-
tion of generalized length allows us to assign length to a wide variety of codes, beyond
binary codes that are typically associated with program length. We examined some ele-
mentary codes to express integers and fractions on the open interval, which can be mapped
to a broad class of numbers that may prove useful to represent prior beliefs.

We showed how feasibility-constrained optimizers satisfy quantiﬁable memorization bounds

in comparison to models that may produce better adherence to training data, but at the
cost of increased description length, increased inference information, or information gener-
ated by an approximating distribution proposed to generate predictions. Our experimental
results show how our hyperposterior ensembles avoid developing artifacts that artiﬁcially
hew to seen data within the predictive structure. Moreover, accounting for multiple expla-
nations by hyperposterior sampling allows us to compute extrapolation uncertainty from

33

J. A. Duersch and T. A. Catanach

ﬁrst principles as the input domain deviates from past observations. These experimental
results demonstrate how our theory allows us to obtain predictions from extremely small
datasets without cross-validation.

Our theory solves critical challenges in understanding how to eﬃciently learn from data,
obtain well-grounded justiﬁcation for uncertainty in predictions, and anticipate extrapola-
tion regimes where additional data would prove most beneﬁcial, thus opening a new domain
of predictive capabilities. This work also provides a principled foundation to address the
challenge of feasible learning in the face of high dimensionality.

Acknowledgements

We would like to extend our earnest appreciation to Jaideep Ray, Justin Jacobs, and Philip
Kegelmeyer for several helpful discussions on this topic. We also acknowledge and appreciate
a conversation with Andrew Charman that clariﬁed our view of the distinction between
rational belief and the inherently restrictive nature of a choice. We sincerely appreciate
reviewer feedback that helped us improve this work.
Funding: This work was funded, in part, by the U.S. Department of Energy.

Sandia National Laboratories is a multimission laboratory managed and operated by
National Technology and Engineering Solutions of Sandia, LLC., a wholly owned subsidiary
of Honeywell International, Inc., for the U.S. Department of Energy’s National Nuclear
Security Administration under contract DE-NA-0003525. This paper describes objective
technical results and analysis. Any subjective views or opinions that might be expressed in
the paper do not necessarily represent the views of the U.S. Department of Energy or the
United States Government.

Appendix A. Selected Information Corollaries

We provide proofs of Theorem 1 and the following corollaries in our previous work (Duersch
and Catanach, 2020).

Corollary 7 Chain rule of conditional dependence. Information associated with joint
variables decomposes as

Ir(z1,z2)[ q1(z1, z2) (cid:107) q0(z1, z2) ] = Ir(z1)[ q1(z1) (cid:107) q0(z1) ]

+ Er(z1) Ir(z2|z1)[ q1(z2 | z1) (cid:107) q0(z2 | z1) ]

where r(z1, z2) = r(z2 | z1)r(z1) and q(z1, z2) = q(z2 | z1)q(z1).

Corollary 8 Additivity over belief sequences. Information gained over a sequence of
belief updates is additive within the same view. Given initial belief q0(z), intermediate states
q1(z) and q2(z), and the view r(z) we have

Ir(z)[ q2(z) (cid:107) q0(z) ] = Ir(z)[ q2(z) (cid:107) q1(z) ] + Ir(z)[ q1(z) (cid:107) q0(z) ] .

Corollary 9 Antisymmetry. Information from q1(z) to q0(z) is the negative of infor-
mation from q0(z) to q1(z)

Ir(z)[ q0(z) (cid:107) q1(z) ] = −Ir(z)[ q1(z) (cid:107) q0(z) ] .

34

Parsimonious Inference

Corollary 10 Information is a proper utility function. Taking the rational view
p(z | x) over the latent variable z conditioned upon an experimental outcome x, the infor-
mation Ip(z|x)[ q(z) (cid:107) p(z) ] from prior belief p(z) to reported belief q(z) is a proper utility
function. That is, the unique optimizer recovers rational belief

q∗(z) = argmax

q(z)

Ip(z|x)[ q(z) (cid:107) p(z) ] = p(z | x).

Corollary 11 Proper perturbation response. Let q1(z) be measurably distinct from
the view r(z) and Ir(z)[ q1(z) (cid:107) q0(z) ] be ﬁnite. Let the perturbation η(z) preserve normal-
ization and drive belief toward r(z) on all measurable subsets. It follows

lim
ε→0

∂
∂ε

Ir(z)[ q1(z) + εη(z) (cid:107) q0(z) ] > 0.

Appendix B. Proofs

Proof of Corollary 1. The primary complication is that each alphabet Σj+1 depends
on previously realized symbols (s)j
1. We proceed by induction. The induction hypothesis
regarding a partial sequence (s)j

1 for j < n is

E

p((s)j
1)

(cid:34) j

(cid:88)

(cid:35)
log2 (|Σi|)

i=1

≥ E

p((s)j
1)

(cid:34) j

(cid:88)

i=1

(cid:18)

log2

1
p(si | (s)i−1

1

)

(cid:19)(cid:35)

.

The base case associated with the ﬁrst symbol s1 easily follows from Jensen’s inequality as

Ep(s1) log2 (|Σ1|) = log2 (|Σ1|) = log2







1



(cid:88)

s1∈Σ1

(cid:88)

≥

s1∈Σ1

p(s1) log2

(cid:18) 1

(cid:19)

p(s1)

= Ep(s1) log2

(cid:18) 1

p(s1)

(cid:19)

.

The induction step is given by applying Jensen’s inequality and the induction hypothesis

log2 (|Σi|) + E

p(sj+1|(s)j

(cid:35)
1) [log2 (|Σj+1|)]

(cid:35)
log2 (|Σi|) + log2 (|Σj+1|)

(cid:18)

log2

(cid:19)

1
p(si | (s)i−1

1

)

+ E

p(sj+1|(s)j
1)

(cid:32)

(cid:34)
log2

1
p(sj+1 | (s)j
1)

(cid:33)(cid:35)(cid:35)

(cid:34)j+1
(cid:88)

(cid:35)
log2 (|Σi|)

)

E

p((s)j+1

1

= E

p((s)j
1)

= E

p((s)j
1)

≥ E

p((s)j
1)

= E

p((s)j+1

1

i=1
(cid:34) j

(cid:88)

i=1
(cid:34) j

(cid:88)

i=1
(cid:34) j

(cid:88)

i=1
(cid:34)j+1
(cid:88)

)

i=1

(cid:18)

log2

1
p(si | (s)i−1

1

)

(cid:19)(cid:35)

.

35

J. A. Duersch and T. A. Catanach

The claim follows by noting that p(a) = p((s)n

1 ) = (cid:81)n

i=1 p(si | (s)i−1

1

).

Proof of Corollary 2.
encoding also maximizes entropy, then

If the encoding is consistent with object probability, and the

p(a) =

n
(cid:89)

i=1

p(si | (s)i−1

1

) =

n
(cid:89)

i=1

1
|Σi|

= 2−(cid:96)(a).

Proof of Corollary 2, recursive consistency alternative. We also obtain the same
prior by combining a counting argument with consistent prior belief in length descriptions.
In order to distinguish a complete short sequence from merely a subsequence of a longer de-
scription, we need some indication of the complete sequence length. The following argument
holds for descriptions ψ that are capable of being partitioned into a component ψl that de-
termines the sequence length and the unconstrained complement ψc so that ψ = (ψl, ψc)
and (cid:96)(ψ) = (cid:96)(ψl) + (cid:96)(ψc). Once ψl is known, we can easily identify ψc, regardless of
its content. Yet, the same problem arises with knowing when we have a complete length
description ψl, which can be resolved with recursive partitions ψl = (ψll, ψlc) and so on.
For the description to be ﬁnite, this recursion must end implicitly with only one possible
outcome ψl...ll = ∅.

We deﬁne (cid:96)(ψc) so that the complement allows for 2(cid:96)(ψc) outcomes. Binary sequences
provide useful intuition for this construction. If we believe all descriptions of a given length
have the same prior probability, then

(cid:90)

1 =

dψc p(ψc | (cid:96)(ψ)) = 2(cid:96)(ψc)p(ψc | (cid:96)(ψ))

so that p(ψc | (cid:96)(ψ)) = 2(cid:96)(ψ)−(cid:96)(ψl).

Thus, p(ψ) = 2(cid:96)(ψ)−(cid:96)(ψl)p(ψl). But if the length description satisﬁes a consistent prior to
determine our belief in the length of the sequence, we must also have p(ψl) = 2(cid:96)(ψl)−(cid:96)(ψll)p(ψll).
This gives

p(ψ) = 2(cid:96)(ψ)−(cid:96)(ψll)p(ψll).

Since the recursion ends with only one outcome, we have (cid:96)(ψl...ll) = 0 and p(ψl...ll) = 1.
Thus, (cid:96)(ψ) = (cid:96)(ψc) + (cid:96)(ψlc) + · · · + (cid:96)(ψl...lc) and p(ψ) = 2−(cid:96)(ψ).

Proof of Corollary 3. Let q∗(a) be the optimizer. We express arbitrary inﬁnitesimal
belief perturbations in the vicinity of the optimizer as q(a) = q∗(a) + εη(a) where ε is a
scalar diﬀerential element and η(a) is an arbitrary perturbation, so long as q∗(a) > 0, so
that

q(a) =

(cid:88)

a∈B

(cid:88)

a∈B

q∗(a) = 1

and

η(a) = 0.

(cid:88)

a∈B

36

Parsimonious Inference

The information gained from prior belief to q(a) is

D[ q(a) (cid:107) p(a) ] =

q(a) log2

(cid:18) q(a)

2−(cid:96)(ψ(a))

(cid:19)

.

(cid:88)

a∈B

Diﬀerentiating with respect to ε, evaluating at ε = 0, and applying the variational principle
yields

0 =

(cid:20) ∂
∂ε

(cid:21)
Iq(a)[ q(a) (cid:107) p(a) ]

ε=0

=

(cid:88)

a∈B

(cid:18)

η(a)

log2

(cid:18) q∗(a)

(cid:19)

2−(cid:96)(ψ(a))

(cid:19)

+ 1

.

As this must hold for arbitrary η(a), the factor in parenthesis must be constant, provided
q∗(a) > 0. Solving for q∗(a) shows that the stated distribution is the unique critical point.
It remains to show that this distribution achieves the global minimum. Applying Jensen’s
inequality to the information gained from any feasible state gives

D[ q(a) (cid:107) p(a) ] = −

(cid:88)

a∈B

q(a) log2

(cid:33)

(cid:32)

2−(cid:96)(ψ(a))
q(a)

≥ − log2

(cid:32)

(cid:88)

a∈B

(cid:33)

2−(cid:96)(ψ(a))

= log2

(cid:18) 1

(cid:19)

p(B)

= D[ q∗(a) (cid:107) p(a) ] .

Proof of Theorem 2. The chain rule of conditional dependence, Corollary 7, allows us
to express the information gained as

D[ r(y | ˇy)q(θ | ψ)q(ψ) (cid:107) p(y | θ)p(θ | ψ)p(ψ) ]
= Ir(y|ˇy)q(θ|ψ)q(ψ)[ r(y | ˇy)q(θ | ψ)q(ψ) (cid:107) p(y | θ)p(θ | ψ)p(ψ) ]
= Iq(ψ)[ q(ψ) (cid:107) p(ψ) ] + Eq(ψ) Iq(θ|ψ)[ q(θ | ψ) (cid:107) p(θ | ψ) ]

+ Eq(θ|ψ)q(ψ) Ir(y|ˇy)[ r(y | ˇy) (cid:107) p(y | θ) ] .

Combining properties of additivity over belief sequences, Corollary 8, with antisymmetry,
Corollary 9, we express the argument of expectation in the last term as

Ir(y|ˇy)[ r(y | ˇy) (cid:107) p(y | θ) ]
= Ir(y|ˇy)[ r(y | ˇy) (cid:107) p(y | θ0) ] − Ir(y|ˇy)[ p(y | θ) (cid:107) p(y | θ0) ]

where θ0 is any ﬁxed model that serves as a convenient baseline for measuring predic-
tive information with regard to training labels. Once the data are observed, the term
Ir(y|ˇy)[ r(y | ˇy) (cid:107) p(y | θ0) ] is a constant, independent of choices q(θ | ψ)q(ψ). Thus we
have

ω = D[ r(y | ˇy) (cid:107) p(y | θ0) ] − D[ r(y | ˇy)q(θ | ψ)q(ψ) (cid:107) p(y | θ)p(θ | ψ)p(ψ) ]
= Eq(θ|ψ)q(ψ) Ir(y|ˇy)[ p(y | θ) (cid:107) p(y | θ0) ] − Eq(ψ) D[ q(θ | ψ) (cid:107) p(θ | ψ) ]

− D[ q(ψ) (cid:107) p(ψ) ] .

37

J. A. Duersch and T. A. Catanach

We apply Corollary 2 as a prior over descriptions and unpack the KL divergence to obtain

− D[ q(ψ) (cid:107) p(ψ) ] =

(cid:90)

dψ q(ψ) log2

(cid:33)

(cid:32)

2−(cid:96)(ψ)
q(ψ)

= S[ q(ψ) ] − Eq(ψ) (cid:96)(ψ).

Proof of Corollary 4. We unpack deﬁnitions, combine both terms, and apply Bayes’
theorem to obtain

(cid:90)

=

dθ q(θ | ψ) log2

Eq(θ|ψ) Ir(y|ˇy)[ p(y | θ) (cid:107) p(y | θ0) ] − D[ q(θ | ψ) (cid:107) p(θ | ψ) ]
(cid:18) p(ˇy | θ)p(θ | ψ)
p(ˇy | θ0)q(θ | ψ)
(cid:18) p(θ | ˇy, ψ)p(ˇy | ψ)
p(ˇy | θ0)q(θ | ψ)

=

(cid:19)

(cid:19)

(cid:90)

dθ q(θ | ψ) log2
(cid:19)
(cid:18) p(ˇy | ψ)
p(ˇy | θ0)

= log2

− D[ q(θ | ψ) (cid:107) p(θ | ˇy, ψ) ] .

The second term is the negative Kullback-Leibler divergence, i.e. nonpositive. Therefore
the objective is maximized when the second term vanishes, which occurs if and only if
q∗(θ | ψ) = p(θ | ˇy, ψ).

Proof of Corollary 5. As in Corollary 4, we unpack deﬁnitions, combine both terms, and
apply Bayes’ theorem to obtain

Eq(ψ) log2

(cid:19)

(cid:18) p(ˇy | ψ)
p(ˇy | θ0)

− D[ q(ψ) (cid:107) p(ψ) ] =

(cid:88)

q(ψ) log2

(cid:18) p(ˇy | ψ)p(ψ)
p(ˇy | θ0)q(ψ)

(cid:19)

(cid:88)

=

ψ

q(ψ) log2

(cid:18) p(ψ | ˇy)p(ˇy)
p(ˇy | θ0)q(ψ)

(cid:19)

= log2

ψ
(cid:18) p(ˇy)

(cid:19)

p(ˇy | θ0)

− D[ q(ψ) (cid:107) p(ψ | ˇy) ] .

The objective is maximized if and only if the second term vanishes, thus q∗(ψ) = p(ψ | ˇy).

Proof of Corollary 6. Applying Jensen’s inequality to expected prediction information
from the optimizer gives

Ir(y|ˇy)[ q∗(y) (cid:107) p(y | θ0) ] − χ[q∗(θ, ψ)]
≥ Eq∗(θ,ψ) Ir(y|ˇy)[ p(y | θ) (cid:107) p(y | θ0) ] − χ[q∗(θ, ψ)]
≥ Eq(θ,ψ) Ir(y|ˇy)[ p(y | θ) (cid:107) p(y | θ0) ] − χ[q(θ, ψ)].

Since Ir(y|ˇy)[ q∗(y) (cid:107) p(y | θ0) ] = Eq(θ,ψ) Ir(y|ˇy)[ q∗(y) (cid:107) p(y | θ0) ], we can apply antisymme-
try and additivity within the expectations to arrive at the stated result.

38

Parsimonious Inference

References

H. Akaike. A new look at the statistical model identiﬁcation.

IEEE Transactions on
Automatic Control, 19(6):716–723, dec 1974. doi: 10.1109/tac.1974.1100705. URL https:
//doi.org/10.1109/tac.1974.1100705.

D. M. Allen. The relationship between variable selection and data agumentation and a

method for prediction. technometrics, 16(1):125–127, 1974.

J. M. Bernardo. Expected information as expected utility. The Annals of Statistics, pages

686–690, 1979.

D. Boulton and C. S. Wallace. An information measure for single link classiﬁcation. The

Computer Journal, 18(3):236–238, 1975.

R. B. Brandt. The concept of rational belief. The Monist, 68(1):3–23, 1985.

R. T. Cox. Probability, frequency and reasonable expectation. American journal of physics,

14(1):1–13, 1946.

B. De Finetti. La pr´evision: ses lois logiques, ses sources subjectives. In Annales de l’institut

Henri Poincar´e, volume 7, pages 1–68, 1937.

J. A. Duersch and T. A. Catanach. Generalizing information to the evolution of rational

belief. Entropy, 22(1):108, 2020.

J. A. Duersch and M. Gu. Randomized projection for rank-revealing matrix factorizations

and low-rank approximations. SIAM Review, 62(3):661–682, 2020.

P. Elias. Universal codeword sets and representations of the integers. IEEE transactions

on information theory, 21(2):194–203, 1975.

R. A. Evans. The principle of minimum information. IEEE Transactions on Reliability, 18

(3):87–90, 1969.

D. Filan, J. Leike, and M. Hutter. Loss bounds and time complexity for speed priors. In

Artiﬁcial Intelligence and Statistics, pages 1394–1402, 2016.

I. J. Good. Maximum entropy for hypothesis formulation, especially for multidimensional

contingency tables. The Annals of Mathematical Statistics, 34(3):911–934, 1963.

P. D. Gr¨unwald. The minimum description length principle. MIT press, 2007.

T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning: data mining,

inference, and prediction. Springer Science & Business Media, 2009.

D. A. Huﬀman. A method for the construction of minimum-redundancy codes. Proceedings

of the IRE, 40(9):1098–1101, 1952.

M. Hutter. On universal prediction and bayesian conﬁrmation. Theoretical Computer

Science, 384(1):33–48, 2007.

39

J. A. Duersch and T. A. Catanach

E. T. Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620,

1957.

H. Jeﬀreys. An invariant form for the prior probability in estimation problems. Proceedings
of the Royal Society of London. Series A. Mathematical and Physical Sciences, 186(1007):
453–461, 1946.

H. Jeﬀreys. The theory of probability. OUP Oxford, 1998.

R. E. Kass and L. Wasserman. Formal rules for selecting prior distributions: A review and
annotated bibliography. Journal of the American Statistical Association, 435:1343–1370,
1996.

A. N. Kolmogorov. Three approaches to the quantitative deﬁnition of information. Problems

of information transmission, 1(1):1–7, 1965.

S. Kullback and R. A. Leibler. On information and suﬃciency. The Annals of Mathematical

Statistics, 22(1):79–86, Mar. 1951. doi: 10.1214/aoms/1177729694.

A. Lempel and J. Ziv. On the complexity of ﬁnite sequences.

IEEE Transactions on

information theory, 22(1):75–81, 1976.

D. J. MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute of

Technology, 1992.

D. J. MacKay. Probable networks and plausible predictions—a review of practical bayesian
methods for supervised neural networks. Network: computation in neural systems, 6(3):
469–505, 1995.

R. M. Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business

Media, 2012.

H. Owhadi, C. Scovel, and T. Sullivan. On the brittleness of bayesian inference. SIAM
Review, 57(4):566–582, Jan. 2015. doi: 10.1137/130938633. URL https://doi.org/10.
1137/130938633.

A. Potapov, A. Svitenkov, and Y. Vinogradov. Diﬀerences between kolmogorov complex-
In International Conference on

ity and solomonoﬀ probability: consequences for agi.
Artiﬁcial General Intelligence, pages 252–261. Springer, 2012.

F. P. Ramsey. Truth and probability. In Readings in Formal Epistemology, pages 21–45.

Springer, 2016.

J. J. Rissanen. A universal prior for integers and estimation by minimum description length.

The Annals of statistics, pages 416–431, 1983.

J. J. Rissanen. Universal coding, information, prediction, and estimation. IEEE Transac-

tions on Information theory, 30(4):629–636, 1984.

H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical

statistics, pages 400–407, 1951.

40

Parsimonious Inference

J. Schmidhuber. The speed prior: a new simplicity measure yielding near-optimal com-
putable predictions. In International conference on computational learning theory, pages
216–228. Springer, 2002.

G. Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6(2):461–
464, Mar. 1978. doi: 10.1214/aos/1176344136. URL https://doi.org/10.1214/aos/
1176344136.

C. E. Shannon. A mathematical theory of communication. Bell System Technical Journal,

27(3):379–423, July 1948. doi: 10.1002/j.1538-7305.1948.tb01338.x.

R. J. Solomonoﬀ. A preliminary report on a general theory of inductive inference. United

States Air Force, Oﬃce of Scientiﬁc Research, 1960.

R. J. Solomonoﬀ. A formal theory of inductive inference. part i. Information and control,

7(1):1–22, 1964a.

R. J. Solomonoﬀ. A formal theory of inductive inference. part ii. Information and control,

7(2):224–254, 1964b.

R. J. Solomonoﬀ. Algorithmic probability: Theory and applications. In Information theory

and statistical learning, pages 1–23. Springer, 2009.

U. Speidel. On the bounds of the titchener t-complexity. In 2008 6th International Sympo-
sium on Communication Systems, Networks and Digital Signal Processing, pages 321–325.
IEEE, 2008.

M. Titchener. Deterministic computation of string complexity, information and entropy. In

Inter. Symp. On Inform. Theory, Aug. 16-21, 1998, Boston, 1998.

C. S. Wallace and D. M. Boulton. An information measure for classiﬁcation. The Computer

Journal, 11(2):185–194, 1968.

C. S. Wallace and P. R. Freeman. Estimation and inference by compact coding. Journal of

the Royal Statistical Society: Series B (Methodological), 49(3):240–252, 1987.

C. Zhang, J. Butepage, H. Kjellstrom, and S. Mandt. Advances in variational inference.

IEEE transactions on pattern analysis and machine intelligence, 2018.

41

