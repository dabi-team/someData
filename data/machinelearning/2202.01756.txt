2
2
0
2

b
e
F
3

]

C
O
.
h
t
a
m

[

1
v
6
5
7
1
0
.
2
0
2
2
:
v
i
X
r
a

On the Convergence of Inexact Predictor-Corrector Methods for
Linear Programming

Gregory Dexter∗

Agniva Chowdhury†‡

Haim Avron§

Petros Drineas∗

Abstract

Interior point methods (IPMs) are a common approach for solving linear programs (LPs)
with strong theoretical guarantees and solid empirical performance. The time complexity of
these methods is dominated by the cost of solving a linear system of equations at each iteration.
In common applications of linear programming, particularly in machine learning and scientiﬁc
computing, the size of this linear system can become prohibitively large, requiring the use
of iterative solvers, which provide an approximate solution to the linear system. However,
approximately solving the linear system at each iteration of an IPM invalidates the theoretical
guarantees of common IPM analyses. To remedy this, we theoretically and empirically analyze
(slightly modiﬁed) predictor-corrector IPMs when using approximate linear solvers: our approach
guarantees that, when certain conditions are satisﬁed, the number of IPM iterations does not
increase and that the ﬁnal solution remains feasible. We also provide practical instantiations of
approximate linear solvers that satisfy these conditions for special classes of constraint matrices
using randomized linear algebra.

1

Introduction

Linear programming is a ubiquitous problem appearing across applied mathematics and computer
science, with extensive applications in both theory and practice. Modern machine learning applica-
tions of linear programming include ‘1-regularized SVMs [36], basis pursuit (BP) [33], sparse inverse
covariance matrix estimation (SICE) [35], the nonnegative matrix factorization (NMF) [25], MAP
inference [22], compressed sensing [12], and adversarial deep learning [31]. The central importance
of this problem has resulted in substantial research on provably accurate algorithms for linear
programming, at the same time, practically eﬃcient algorithms are critically needed.

The two major families of algorithms used to solve linear programs are simplex methods and interior
point methods (IPMs), with combinations of the two (e.g., IPMs used in the early stages and simplex
methods used once approximately optimal solutions have been reached) being useful in practice
[32]. Predictor-corrector methods, a special type of IPMs, have been particularly useful in solving
linear programs accurately and are perhaps the most successful example of theoretically provable
yet practically eﬃcient approaches for linear programs.

More precisely, consider a linear program (LP) of the following (standard) form. Let A ∈ Rm×n be

∗Department of Computer Science, Purdue University, West Lafayette, IN, USA, {gdexter,pdrineas}@purdue.edu.
†Computer Science and Mathematics Division, Oak Ridge National Laboratory, TN, USA, chowdhurya@ornl.gov.
‡This work was done when the author was a graduate student at Purdue University.
§School of Mathematical Sciences, Tel Aviv University, Tel Aviv, Israel, haimav@tauex.tau.ac.il.

1

 
 
 
 
 
 
the constraint matrix and x ∈ Rn be the free variable:

min cT x, subject to Ax = b, x ≥ 0.

The associated dual problem is

max bT y, subject to AT y + s = c, s ≥ 0,

(1)

(2)

where y ∈ Rm is the dual variable and s ∈ Rn is the slack variable. The ﬁrst (weakly) polynomial
time algorithm for linear programming is the ellipsoid method, developed by Khachiyan in 1979 [18].
While the ellipsoid method was deemed to be ineﬃcient in practice, it provided inspiration for
the ﬁrst IPM, developed by Karmarkar in 1984 [17]. Karmarkar’s initial work was followed by
an explosion of research on IPMs that led to numerous algorithms with various theory-practice
tradeoﬀs.

Predictor-corrector IPMs achieve nearly optimal theoretical guarantees, while being commonly used
in popular linear optimization packages [27, 1]. More precisely, predictor-corrector algorithms are
primal-dual path-following IPMs. They compute a sequence of iterates (xk, yk, sk) within the primal-
dual polytope of feasible solutions which approach an optimal solution of the LP. Path-following
IPMs require that the iterates within the polytope remain near the so-called central path of the
polytope, which results in faster convergence. In each iterate, updates are computed by solving the
normal equations, namely a system of linear equations of the following form:

AD2AT ∆y = p.

(3)

In the above equation, p is a vector (see eqn. (7) for the exact deﬁnition) and D2 = XS−1, where
X is the diagonal matrix whose entries are the xi and S is the diagonal matrix whose entries are
the si. To analyze the computational complexity of predictor-corrector methods, one ﬁrst computes
the number of outer iterations, namely the number of iterations in the IPM algorithm required to
converge to an approximately optimal solution. Then, one analyzes the time required to compute
each of the iterates by solving the linear system of eqn. (3).

Standard approaches analyzing the rate of convergence and the time complexity of predictor-corrector
methods (and other IPMs) typically assume that eqn. (3) is solved exactly at each iteration. However,
this assumption becomes untenable for large-scale problems and inexact iterative solvers are nearly
universally used in practice. The resulting methods are often called inexact predictor-corrector IPMs.
Theoretically understanding the behavior of inexact linear equation solvers when combined with
IPMs is highly non-trivial. Indeed, an early, provably accurate, approach combining inexact solvers
with short step IPMs (a diﬀerent, less practical, class of IPMs) appeared in the work of Daitch and
Spielman [11], which argued that eqn. (3) can be solved in near linear time when the constraint
matrix AD2AT is symmetric and diagonally dominant. This allowed fast approximate solutions to
problems such as generalized maximum ﬂow [11]. More recently, the literature survey by Gondzio
[13] highlighted that pairing IPMs with iterative linear solvers is the way forward towards solving
large-scale LPs that arise in machine learning applications. See Section 1.2 for a detailed discussion
of relevant prior work on LP solvers.

1.1 Our contributions

In this paper, we prove that a (slightly modiﬁed) predictor-corrector IPM can tolerate errors in
solving the linear system of eqn. (3) at each outer iteration without sacriﬁcing the feasibility of the

2

derived solution and without increasing the number of outer iterations of predictor-corrector IPMs.
Our proposed inexact predictor-corrector IPM (Algorithm 1 in Section 4) starts with a feasible
point and converges to an (cid:15)-optimal exactly feasible solution in O(
(cid:15) ) outer iterations, where
µ0 is the duality measure at the starting point1, while approximately solving the system of linear
equations of eqn. (3).

n log µ0

√

Approximately solving the linear system of eqn. (3) is problematic for two reasons: ﬁrst, it invalidates
known analyses of classical predictor-corrector IPMs and, second, it results in infeasible iterates,
even when the IPM starts from a feasible point. To address these issues in theory and in practice, we
introduce an error-adjustment vector, similar to the work of Monteiro and O’Neal [24], Chowdhury
et al. [8]. The error adjustment vector is our only modiﬁcation to the classical predictor-corrector
IPM and it allows us to return provably accurate, feasible solutions, without any increase in the
outer iteration complexity of predictor-corrector IPMs.

More precisely, let ∆˜y be an approximate solution for the linear system of eqn. (3) and let v ∈ Rn be
an error adjustment vector (more on this vector v later). Let these two vectors satisfy the following
conditions:

AD2AT ∆˜y = p + AS−1v and kvk2 < Θ((cid:15)).

(4)

In words, the above conditions simply state that the approximate solution ∆˜y is an exact solution to
a slightly modiﬁed system of normal equations, where the vector p has been replaced by the vector
p + AS−1v. The two norm of the vector v must be relatively small, namely less than Θ((cid:15)), where (cid:15)
is the target accuracy of the (overall) IPM solver. We emphasize that the error-adjustment vector
v ∈ Rn is user-controlled, as long as it satisﬁes the above conditions. Then, these conditions are
suﬃcient to guarantee that our predictor-corrector IPM (see Algorithm 1 in Section 4) converges to
a solution (x∗, y∗, s∗) with a duality measure µ∗ < (cid:15) in O(
(cid:15) ) outer iterations. Importantly,
the ﬁnal solution is exactly (and not approximately) feasible.

n log µ0

√

A few additional remarks are necessary to better understand our results for the error-adjusted
inexact predictor-corrector IPM. First, we note that our method achieves the best-known outer
iteration complexity for predictor-corrector IPMs. Second, the error tolerance of the approximate
linear equation solver does not directly depend on n and is constant if (cid:15) is constant. Third, there
are many potential constructions of v which fulﬁll the above guarantees. This raises the problem of
ﬁnding eﬃcient constructions of v for an inexact linear solver, as this has signiﬁcant impact on the
eﬃciency of the method.

To address the last point, we exhibit eﬃcient, practical methods to compute ∆˜y and v by adapting
the preconditioned conjugate gradient (PCG) algorithm for constraint matrices A that are short-
and-fat, tall-and-thin, or even have exact low-rank (see Section 4.1 and Appendix D for details).
More precisely, we show that using PCG, we can compute ∆˜y and v in O (cid:0)log nµ
(cid:1) inner iterations,
(cid:15)
where each inner iteration is simply a matrix-vector product. It is notable that this inner iteration
complexity does not depend on the spectrum or the condition number of the input matrix AD2AT .
This is particularly important since the condition number of this matrix changes over iterations
and might increase signiﬁcantly as the outer iterations of the predictor-corrector IPM approach the
optimal solution.

Our second contribution in this paper is a novel analysis of the classical, inexact predictor-corrector

1The duality measure µ = 1

n xT s quantiﬁes how close a primal-dual point (x, y, s) is to optimality at a certain

iteration.

3

IPMs in the special setting where the ﬁnal solution is allowed to be only approximately feasible.
More precisely, assume that ∆˜y is an approximate solution to the linear system of eqn. (3) that
satisﬁes the following two conditions:

k∆˜y − (AD2AT )−1pkAD2AT ≤ δ

and kAD2AT ∆˜y − pk2 ≤ δ.

(5)

Here δ is the error tolerance of the solver and we note that the ﬁrst condition guarantees that the
exact and the approximate solutions are close with respect to the energy norm, while the second
condition guarantees that the two-norm of the residual error of the solver is small. (See Section 2
for notation.) We provide a novel analysis of the standard predictor-corrector method described
in Wright [32] when approximate solvers that satisfy the above conditions are used. Theorem 1
proves that these conditions suﬃce in order to prove that the standard predictor-corrector algorithm
(see Algorithm 2 in Appendix B) converges to a solution (x∗, y∗, s∗) with duality measure µ∗ such
that kAx∗ − bk2 < (cid:15) and µ∗ < (cid:15) in O(
(cid:15) ) outer iterations. Assuming that µ0 and (cid:15) are
constant, the accuracy parameter δ is set to Θ (1/√

n) at all iterations of the algorithm.

n log µ0

√

√

n log µ0

A few remarks are necessary to better understand the second result. First, the outer iteration
complexity is essentially equivalent to the “optimal” O(
(cid:15) ) iteration complexity of the exact
predictor-corrector IPM methods and exhibits linear convergence in the accuracy parameter (cid:15).
n,
Second, the accuracy parameter δ for the approximate solver is, generally, proportional to 1/
which is similar to the condition of Daitch and Spielman [11]. It is worth noting that our proof
collapses if the error bound exceeds this threshold, and an interesting open problem is whether
this condition is necessary for predictor-corrector IPMs. Third, the ﬁnal solution vector is only
approximately (and not exactly) feasible, satisfying kAx∗ − bk2 < (cid:15) for the accuracy parameter2 (cid:15).
Fourth, for the same family of matrices as in our previous approach (tall-and-thin, short-and-fat, or
exact low-rank k (cid:28) min{m, n}), we can again show that by using PCG solvers, we can eﬃciently
compute an approximate solution ∆˜y in O
iterations of the preconditioned solver,
where σmax(AD) is the largest singular value of the matrix AD. We emphasize that, unlike our
previous approach that uses the error-adjustment vector v, the convergence of the standard inexact
predictor-corrector IPMs depends logarithmically on properties of the input matrix AD at each
iteration.

log σmax(AD) nµ

√

(cid:16)

(cid:17)

δ

Our two contributions exhibit a trade-oﬀ between algorithmic simplicity and theoretical guarantees.
On one hand, the standard predictor-corrector IPM can be used without modiﬁcations with an
iterative linear solver, but will not return a feasible solution and will need higher solver accuracy
that depends on the largest singular value of AD. Alternatively, the predictor-corrector method
can be slightly modiﬁed to use an error-adjustment vector with the added beneﬁts of obtaining an
exactly feasible solution and removing dependence of the inner iteration complexity on the largest
singular value of AD.

We conclude by noting that our proof techniques are ﬂexible and can be extended to analyze
long-step and short-step IPMs, which are, however, less interesting in practice.

1.2 Related Work

Due to the central importance of linear programming in computer science, there exists a large body
of work on LPs and IPMs speciﬁcally. We refer the reader to the 2012 survey of Gondzio [13] for

2For notational simplicity, we use the same accuracy parameter (cid:15) for both the duality measure and the approximate

feasibility of the ﬁnal solution vector. Our analysis can be easily extended to use diﬀerent accuracy parameters.

4

more information on the broader state of IPMs, as we focus on literature that is most closely related
to our work. Recall that our main focus in this paper is a theoretical analysis of the outer iteration
complexity of inexact predictor-corrector IPMs with and without a correction vector that guarantees
an exactly feasible solution.

Since the 1950s, there has been continual eﬀort in the theoretical computer science community to
develop new LP solvers with improved worst-case asymptotic time complexity. Presently, there is no
single fastest LP solver over all typical regimes of LPs. The work of Lee and Sidford [19] provides an
IPM which requires eO(prank(A) log 1
(cid:15) ) outer iterations and eO(1) linear system solves at each outer
iterations. The recent works of Cohen et al. [10], Song and Yu [28] have a total time complexity
of O∗(nω log n
(cid:15) ) where ω ∼ 2.37 is the current best-known exponent of matrix multiplication. The
work of van den Brand et al. [30] provides the theoretically fastest solver when A is tall and dense,
√
n) and the total time complexity is eO(mn + n3).
in which case the outer iteration complexity is eO(
All three of these works provide short-step IPMs, which have been found to converge slowly in
practice. Additionally, these works leverage techniques such as fast matrix multiplication and inverse
maintenance, which, due to numerical instability and large constant factors, are generally ineﬀective
in practice. Our algorithms do not depend on any of these techniques. We instead focus on the
predictor-corrector method, which is highly eﬀective in practice, yet still has strong theoretical
guarantees, with an outer iteration complexity of O(
(cid:15) ) and one linear system solve per outer
iteration. Our method can be fully implemented in less than 150 lines of code using well-established
numerical techniques such as preconditioned conjugate gradient descent, as we show in Appendix D.
The time complexity of this inexact linear system solver is eO(nnz(A) + k3), where k is the rank of
A.

n log µ0

√

In our work, we analyze the prototypical predictor-corrector algorithm described in [32]. One
variant of this method, given by Mehrotra [21], is considered the industry-standard approach to
solving LPs and is perhaps the most common IPM used in linear programming packages [27, 1]. We
do note that Mehotra’s algorithm (unlike the standard predictor-corrector IPMs) does not come
with provable accuracy guarantees. Development of new predictor-corrector variants along with
theoretical analyses is ongoing. Examples include the variant of Mehrotra’s predictor-corrector IPM
given by Salahi et al. [26] or the analysis by Almeida and Teixeira [1] of a predictor-corrector method
speciﬁcally suited for LPs arising in transportation problems [5]. Other recent works includes the
paper by Schork and Gondzio [27] on empirically evaluating Mehrotra’s algorithm when using
preconditioned conjugate gradient descent to solve the normal equations at each step. The work
of Yang and Yamashita [34] provides an infeasible predictor-corrector method with O(n log 1
(cid:15) ) outer
iteration complexity and empirically demonstrates its competitiveness with existing methods. The
importance of predictor-corrector methods motivates us to develop a better theoretical understanding
of their convergence properties when using inexact linear solvers.

Multiple works have analyzed the impact of using inexact linear solvers within IPM algorithms, with
early examples being [7] and [23]. One method which is relevant to our work is that of Monteiro
and O’Neal [24] which guaranteed the convergence of a long-step IPM by correcting the error of
the inexact solver using a correction vector v as we describe in eqn. (4). This idea was further
developed by Chowdhury et al. [8], which introduced a more eﬃcient construction of v. Another
example of such works is Daitch and Spielman [11], which gives a short-step IPM alongside an
inexact Laplacian system solver to solve the generalized max-ﬂow problem. However, the analysis of
their inexact short-step IPM does not seem to be directly applicable to solving general LPs where
the constraint matrix is not Laplacian. We improve over these prior works by analyzing the inexact
predictor-corrector method using two diﬀerent approaches and we make minimal assumptions of the

5

linear system solver and LP.

We further note that recently various ﬁrst-order methods (with proper enhancements) are also
being explored to identify high-quality solutions to large-scale LPs quickly [6, 20, 2]. However,
most of these endeavors are based on the combinations of existing heuristics and do not come with
theoretical guarantees.

2 Background

2.1 Notation

For any natural number n, let [n] = {1, 2, ...n}. Bold capital letters denote matrices (e.g, A); bold
lower case letters denote vectors (e.g., x); and the i-th element of vector x is written as xi. Let In
denote the n × n identity matrix; let 1m and 0m denote length m vectors of all ones and zeroes
respectively. We deﬁne the norm of a vector kxkp to be its well-known ‘p norm and the norm of
a matrix kAkp to be the induced ‘p norm, i.e. kAkp = maxkxkp=1 kAxkp. We also use the energy
xT Mx, where x is a vector and M is a symmetric positive deﬁnite matrix. We
norm kxkM =
denote the Hadamard (element-wise) product of two vectors u, v as u ◦ v. Finally, we denote the
Moore-Penrose pseudoinverse of a matrix A as A†.

√

2.2 Background

Interior point methods using an exact linear solver iteratively converge towards a primal-dual
solution (x∗, y∗, s∗), which optimally solves the primal and dual LPs of eqns. (1, 2). The direction
of each iterative step is determined by solving the so-called normal equations:

AD2AT ∆y = −σµAS−11n + Ax,
∆s = −AT ∆y,
∆x = −x + σµS−11m − D2∆s.

(6a)

(6b)

(6c)

In the above, σ ∈ [0, 1] is the centering parameter, which controls the tradeoﬀ between progressing
towards the optimal solution and staying near the central path. Let p be equal to the right-hand-side
of eqn. (6a), i.e.,

p = −σµAS−11n + Ax.

(7)

Path-following IPM algorithms ensure that the iterates remain suﬃciently far from the boundary of
the convex polytope representing the feasible set of the primal and dual LPs and near the central
path. In this paper, we use the ‘2 neighborhood N2(θ) deﬁned as follows.

N2(θ) = (cid:8)(x, y, s) ∈ R2n+m : kx ◦ s − µ1nk2 ≤ θµ, (x, s) > 0(cid:9).

(8)

The step size of the outer iterations will need to be dynamically determined to ensure that the
iterates remain in the appropriate neighborhood. The following notation compactly describes the
next iterate after a step of size α:

x(α) = x + α∆x, y(α) = y + α∆y, s(α) = s + α∆s, and µ(α) = (x + α∆x)T (s + α∆s)/n.

6

The following identities hold for the exact steps determined by eqn. (6) above; see [32] for details:

∆xT ∆s = 0,
sT ∆x + xT ∆s = −nµ + nσµ,
µ(α) = (1 − α + ασ)µ.

(9)

(10)

(11)

We will also use the following bound on k∆x ◦ ∆sk2 to argue that the iterative steps remain in the
neighborhood deﬁned by eqn. (8); see Lemma 5.4 in [32] for a proof:

(x, y, s) ∈ N2(θ) ⇒ k∆x ◦ ∆sk2 ≤

θ2 + n(1 − σ)2
23/2(1 − θ)

µ.

(12)

3 Overview of our approach and proofs

Inexactly solving the normal equations when determining the step direction in predictor-corrector
IPMs adds new diﬃculties to the convergence analysis such methods. Identities that are critical in
analyzing the the exact methods, such as ∆xT ∆s = 0, no longer hold. Another source of diﬃculty
is that, even when starting from a feasible initial point, the iterates will become infeasible due to
the error incurred by the solvers. We can handle this infeasibility in two diﬀerent ways. First, we
can assume bounds on the maximum error of the solver at each step, which would guarantee that
the ﬁnal solution is (cid:15)-feasible, i.e. kAx∗ − bk2 ≤ (cid:15). The second approach, previously introduced
in [24], is to adjust the error in each step to ensure that the next iterate is feasible. We analyze
both approaches in our work.

The following equation block designates the step of inexactly solving the normal equations, where
the vector f is the error residual incurred by solving for ∆˜y using an inexact linear solver:

AD2AT ∆˜y = −σµAS−11n + Ax − f ,

∆˜s = −AT ∆˜y,
∆˜x = −x + σµS−11n − D2∆˜s.

(13a)

(13b)

(13c)

The next equation block deals with the case of an error-adjusted approximate step. The idea behind
this error-adjustment is the construction of a vector v with small norm such that the error vector
f is exactly equal to −AS−1v. We then correct the primal step ∆˜x by subtracting S−1v, which
guarantees that A∆˜x is equal to zero:

AD2AT ∆˜y = −σµAS−11n + Ax + AS−1v,

∆˜s = −AT ∆˜y,
∆˜x = −x + σµS−11n − D2∆˜s − S−1v.

(14a)

(14b)

(14c)

Note that both the inexact and error-adjusted normal equations maintain dual feasibility of the
iterate when starting from any dual feasible starting point, i.e., AT y + s = c.

In order to derive our theoretical bounds, we ﬁrst analyzed the uncorrected inexact predictor-
corrector IPM, which is the original predictor-corrector IPM using an approximate solver denoted
Solve. In the interest of space, we delegate the presentation and analysis of this algorithm to the
Appendix (see Appendix B and Algorithm 2). The uncorrected inexact predictor-corrector IPM

7

takes two steps (a predictor step and a corrector step) in each outer iteration. Starting from a point
in N2(0.25), the algorithm takes a predictor step with centering parameter σ = 0 and a dynamically
chosen step size α, such that the iterate remains in N2(0.5). The algorithm then takes a corrector
step with centering parameter σ = 1 and step size α = 1, which returns the iterate back to N2(0.25).
The predictor step results in a multiplicative decrease in the duality measure, and the corrector
step sets up the next predictor step, while only resulting in a slight additive increase in the duality
measure. Our main result for Algorithm 2 in Appendix B is given by the following theorem.

Theorem 1. Let (cid:15) > 0 be a tolerance parameter and (x0, y0, s0) ∈ N2(0.25) with duality measure
µ0 be a feasible starting point. Then, Algorithm 2 (see Appendix B) converges to a dual-feasible
n log µ0
point (x∗, y∗, s∗) with duality measure µ∗ such that kAx∗ − bk2 < (cid:15) and µ∗ < 2(cid:15) in O(
(cid:15) )
outer iterations, where the approximate linear solver Solve is called twice in each outer iteration
(cid:15)C0
with error tolerance parameter δ(cid:15),n = min{
n log µ0/(cid:15) } and the constant C0 is deﬁned in Lemma
B.4.

√
(cid:15)
26 ,

√

√

2

Next, in Section 4, we present and analyze our main contribution, the error-adjusted predictor-
corrector method (Algorithm 1), which uses a linear solver3 Solvev to compute an approximate
solution ∆˜y and an error-adjustment vector v that satisfy eqns. (14). The main result of Section 4
is the following theorem.

Theorem 2. Let (cid:15) > 0 be a tolerance parameter and (x0, y0, s0) ∈ N2(0.25) with duality measure µ0
be a feasible starting point. Then, Algorithm 1 converges to a primal-dual feasible point (x∗, y∗, s∗)
√
with duality measure µ∗ such that µ∗ < 2(cid:15) in O(
(cid:15) ) outer iterations, where Solvev is called
twice in each outer iteration with error tolerance (cid:15)/27.

n log µ0

The convergence analysis of both inexact predictor-corrector algorithms shares the same overall
structure, which we now outline. First, we upper bound k∆˜x ◦ ∆˜sk2, a technical result that will be
needed in upcoming steps. Second, we derive a bound for the left-hand side of the N2 neighborhood
condition (kx ◦ s − µ1nk2) after step size α. This bound depends on k∆˜x ◦ ∆˜sk2 and the error of
the linear solver. Third, we ﬁnd a value for the step size α that depends on k∆˜x ◦ ∆˜sk2, which
keeps the next iterate in the appropriate neighborhood, N2(0.5). Fourth, we lower bound the step
size α using the upper bound on k∆˜x ◦ ∆˜sk2. Fifth, we use the lower bound on α from the previous
step to lower bound the multiplicative decrease in the duality measure after the predictor step.
Finally, we prove that the corrector step with step size α = 1 returns the iterate to N2(0.25) by
using the inequality from the second step and then bound the resulting additive increase in the
duality measure.

(cid:16)

√

For both predictor-corrector algorithms, the above structure provides a guaranteed decrease in
δ(cid:15),n
the duality measure over a single step of the form ˜µ1 ≤
n , where C0 ∈ (0, 1),
n], and δ(cid:15),n > 0 is the tolerance parameter for the corresponding linear solver. We can
C1 ∈ [0, C0/
use this relation to conclude (using standard arguments) that each algorithm converges to a point
(x∗, y∗, s∗) with duality measure µ∗ < 2(cid:15). We note that the proof of the inexact predictor-corrector
IPM without using error-adjustment is simpler, partly because the duality measure during its inexact
predictor-corrector step is always higher than the duality measure during the exact step. This is
not the case for the inexact predictor-corrector IPM with error-adjustment, which needs extra care
in bounding the duality gap decrease in each iteration.

1 − C0√
n

µ0 + C1

(cid:17)

In Section 4.1 (see also Appendix D), we show how the approximate linear solver Solvev can be

3(∆˜y, v) = Solvev(A, p, δ) takes three inputs: the input matrix A, the response vector p, and the target accuracy

(or tolerance) δ and returns an approximate solution ∆˜y and the error-adjustment vector v.

8

eﬃciently instantiated when the constraint matrix A has exact low rank (which includes as special
cases tall-and-thin and short-and-fat matrices), by using a preconditioned conjugate gradient (PCG)
method.

4 Error-adjusted Inexact Predictor-Corrector IPMs

In this section, we introduce an algorithm that we will call error-adjusted inexact predictor-corrector
IPM (Algorithm 1). This algorithm uses an inexact linear solver Solvev, which returns an
approximate solution ∆˜y and a correction vector v that satisﬁes the conditions of eqn. (4). This
correction vector guarantees that the ﬁnal solution will be exactly feasible and, as discussed in
Section 1, Algorithm 1 can tolerate larger errors for the inexact solver. We follow the proof sketch
of Section 3 to prove convergence guarantees and time complexity for Algorithm 1. We will assume
that the matrix A has full row rank, ie., rank(A) = m ≤ n; see Appendix D.3 for extensions
alleviating this constraint.

Algorithm 1 Error-adjusted Inexact Predictor-corrector

Input: A ∈ Rm×n, initial feasible point (x0, y0, s0) ∈ N2(0.25); IPM tolerance (cid:15) > 0.
Initialize: k ← 0;
while µk > 2(cid:15) do

(c) Set α = min

Predictor Step (σ = 0):
(a) Compute (∆˜y, v) = Solvev(AD2AT , Ax, (cid:15)/27).
(b) Compute ∆˜x and ∆˜s via eqn. (14).
1/2, (µ/16k∆˜x◦∆˜sk2)1/2o
n
(d) Compute (xk, yk, sk) = (xk, yk, sk) + α(∆˜xk, ∆˜yk, ∆˜sk).
Corrector Step (α = 1, σ = 1):
(e) Compute (∆˜y, v) = Solvev(AD2AT , −µAS−11n + Ax, (cid:15)/27).
(f) Compute ∆˜x and ∆˜s via eqn. (14).
(g) Compute (xk+1, yk+1, sk+1) = (xk, yk, sk) + (∆˜xk, ∆˜yk, ∆˜sk).
(h) k ← k + 1.

end while

We proceed by expressing the diﬀerence of the exact vs. approximate solutions, using eqn. (6) vs.
eqn. (14):

∆y − ∆˜y = −(AD2AT )−1AS−1v,
∆s − ∆˜s = −AT (∆y − ∆˜y)

= AT (AD2AT )−1AS−1v,
∆x − ∆˜x = −D2(∆s − ∆˜s) + S−1v

= −D2AT (AD2AT )−1AS−1v + S−1v.

(15)

(16)

(17)

We prove that Algorithm 1 converges to a point (x∗, y∗, s∗), such that µ∗ < 2(cid:15), Ax∗ = b, and
AT y∗ + s∗ = c in O(
n log µ0/(cid:15)) outer iterations. First, we start with a technical result to bound
k∆˜x ◦ ∆˜sk2. (All proofs are delegated to Appendix C.)

√

Lemma 4.1. Let (x, y, s) ∈ N2(θ) and let (∆˜x, ∆˜y, ∆˜s) be the step calculated from the inexact

9

normal equations without error-adjustment (see eqn. (14)). Then,

k∆˜x ◦ ∆˜sk2 ≤

θ2 + n(1 − σ)2
23/2(1 − θ)

µ + 3

s

(θ2 + n(1 − σ)2)µ
(1 − θ)

k(XS)−1/2vk2 + 2k(XS)−1/2vk2
2.

This inequality represents a key technical contribution of our results. The proofs of Monteiro and
O’Neal [24], Chowdhury et al. [8] on the convergence of a long-step IPM cannot be readily extended
to the predictor-corrector method, as the predictor-corrector algorithm requires ﬁner control over
deviations from the central path due to using the ‘2-neighborhood. However, this more restrictive
neighborhood allows it to achieve better outer iteration complexity. Observe that in the corrector
step, when σ = 1, our bound does not directly scale with n in this case, in contrast to Lemma 16 in
[8] and Lemma 3.7 in [24].

We can use the previous inequality to bound the deviation of the iterate from the central path after
a step of size α.

Lemma 4.2. If α ∈ [0, 1], then

k˜x(α) ◦ ˜s(α) − ˜µ(α)1nk2 ≤ (1 − α)kx ◦ s − µ1nk2 + α2k∆˜x ◦ ∆˜sk2 + 2αkvk2.

Given the previous bound, we can then derive a step size α which guarantees that the iterate
remains in N2(0.5) after the predictor step.

Lemma 4.3. If (x, y, s) ∈ N2(0.25), α = min
predictor step (˜x(α), ˜y(α), ˜s(α)) ∈ N2(0.5).

1/2, (µ/16k∆˜x◦∆˜sk2)1/2o
n

, and kvk2 ≤ µ/32, then the

We then show that the predictor step with step size α as given in the above lemma guarantees a
multiplicative decrease in the duality gap. Recall that σ = 0 in the predictor step when solving the
normal equations.

Lemma 4.4. If (x, y, s) ∈ N2(0.25), α = min
, and kvk2 ≤ µ/32, then the
predictor step (˜x(α), ˜y(α), ˜s(α)) remains in N2(0.5) and there exists a constant C0 ∈ (0, 1) such
that,

1/2, (µ/16k∆˜x◦∆˜sk2)1/2o
n

˜µ(α)
µ

≤ 1 −

C0√
n

.

After the previous lemma, we have shown that the predictor step results in a multiplicative decrease
in the duality gap, while keeping the next iterate in the neighborhood N2(0.5). We then show that
the corrector step returns the iterate to the N2(0.25) neighborhood, while increasing the duality
gap by a small additive amount.

Lemma 4.5. Let (x, y, s) ∈ N2(0.5) and kvk2 ≤ µ/27. Then, the corrector step (˜x(1), ˜y(1), ˜s(1)) ∈
N2(0.25) and |˜µ(1) − µ| ≤ 1√

n kvk2.

Overall, the structure of the proof approach for the inexact predictor-corrector method (Appendix
B) is similar to the proof structure shown here. However, proving the individual lemmas for the
error-adjusted algorithm requires slightly more work, since the correction step adds an additional
adjustment to the iterates in each step, which must be accounted for.

In comparison to the proof of the standard predictor-corrector method found in Wright [32], the
general idea and organization of the lemmas are shared, however, accounting for the error in each

10

step can lead to unwieldy and complicated formulas. An important part of our proof that partially
alleviates this problem is to generalize the inequalities used in Wright [32] to depend smoothly on
the error of the inexact linear system solve, so we recover the Statement of each lemma when the
error is zero. This allows us to appropriately set the linear system solver precision to give up small
factors in the tightness of our result in each lemma, which can then be oﬀset by increasing the
precision of the linear system solver. By doing so, we ensure that the added proof complexity for
the inexact predictor-corrector method is locally resolved in each lemma. As a result, our proofs are
conceptually simpler than prior work proving the convergence of inexact IPMs, such as Chowdhury
et al. [8] and Daitch and Spielman [11].

4.1 Implementing Solvev

We demonstrate that Solvev can be eﬀectively implemented using a preconditioned conjugate
gradient (PCG) method that also constructs the correction vector v that satisﬁes the conditions
of eqn. (4). By employing a randomized preconditioner, the resulting PCG method guarantees an
exponential decrease in the energy norm of the residual. Here we sketch our approach, and we
provide details in Appendix D.

Let AD = UΣVT be the thin SVD representation with V ∈ Rm×n and W ∈ Rn×w be an oblivious
sparse sketching matrix which satisﬁes, for some accuracy parameter ζ ∈ (0, 1/2):

kVWWT VT − Imk2 ≤

ζ
2

,

(18)

with probability at least 1 − η. The work of Cohen et al. [9] shows how to construct such a matrix
W fulﬁlling this guarantee with sketch size w = O(m/ζ2 · log m/η) and O(1/ζ · log m/η) non-zero entries
per row. Next, we use the above sketching matrix to deﬁne

Q = ADWWT DAT .

We note that Q does not need to be explicitly constructed, since we will only use the inverse
of its square root Q−1/2 (see Algorithm 3 in Appendix D for details). More speciﬁcally, since
W has log m/η non-zero entries per row and D is a diagonal matrix, ADW can be computed in
O(nnz(A) · log m/η) time. Then, computing Q−1/2 via the SVD of ADW takes O(m3 log m/η) time.
The overall time complexity to compute Q−1/2 is O(nnz(A) · log m/η + m3 log m/η).

Next, we prove that the vector ˜zt returned by PCG (Algorithm 3, Appendix D) fulﬁlls the following
inequality with probability at least 1 − η:

kQ−1/2(AD2AT )Q−1/2˜zt − Q−1/2pk2 ≤ ζ tkQ−1/2pk2,

for the aforementioned error-parameter ζ. Given W (the sketching matrix used to construct the
preconditioner), we proceed to construct the error-adjustment vector v as follows:

v = (XS)1/2W(ADW)†(AD2AT ∆˜y − p),

where ∆˜y = ˜zt after t = O (cid:0)log nµ
(cid:1) iterations. The additional time needed to compute the error-
(cid:15)
adjustment vector v is negligible, since it only adds matrix vector products, using quantities that have
already been computed and are available to the algorithm. Combining randomized preconditioning,
conjugate gradients, and our proposed construction of the error-adjustment vector v is theoretically
and practically eﬃcient for short-and-fat, tall-and-thin, and exact low-rank matrices. It can also

11

Figure 1: This graph demonstrates the linear re-
lationship between the number of iterations and
√
n, as predicted by Theorem 2. The line shows
the median number of iterations and the intervals
designate the 10% and 90% quantiles out of 60
repetitions. Other parameters are m = 20; (cid:15) = 0.1;
and solver tolerance 0.001.

Figure 2: This graph demonstrates the linear re-
lationship between the number of iterations and
log(1/(cid:15)), as predicted by Theorem 2. The line
shows the median number of iterations and the
conﬁdence intervals designate the 10% and 90%
quantiles out of 60 repetitions. Other parameters
are m = 30; n = 70; and solver tolerance equal to
(cid:15).

take advantage of any sparsity in the input matrix, since both our preconditioner construction and
conjugate gradient methods leverage input sparsity.

4.2 Empirical validation of Theorem 2

√

We experimentally validate the predictions of Theorem 2 on synthetic data. Speciﬁcally, we observe
n and log(1/(cid:15)), when the
the predicted linear relationship between the number of iterations vs.
precision of Solvev is set to O((cid:15)). To generate the synthetic LPs, we sample a constraint matrix
A ∈ Rm×n, the initial primal variable x0 ∈ Rn, and the initial dual variable y0 ∈ Rm, where each
entry is sampled uniformly over an appropriate interval. From these points, we can choose an initial
slack variable such that kx0 ◦ s0 − µ01nk2 < (0.25)µ0. The initial primal-dual point along with the
constraint matrix completely describes the LP, since the initial point is assumed to be primal-dual
feasible. We implement Solvev to ﬁnd the primal-dual step and correction vector v by uniformly
sampling a vector v ∈ Rn fulﬁlling kvk2 = O((cid:15)) and then solving for the corresponding step in
eqn. (14a) exactly; see Figures 1 and 2. We also test our preconditioned gradient descent method
for ﬁnding the primal-dual step, and error-adjustment vector v in Appendix E and we ﬁnd that the
performance of our approach is comparable to the exact method shown here.

5 Conclusions

We present and analyze an inexact predictor-corrector IPM algorithm that uses preconditioned
inexact solvers to accelerate each iteration of the IPM, without increasing the number of iterations
or sacriﬁcing the feasibility of the returned solution. In future work, it is of interest to extend
this framework to design fast and scalable algorithms for more general convex problems, such as
semideﬁnite programming.

12

Acknowledgements

GD, AC, and PD were partially supported by NSF 10001390, NSF 10001415, and DOE 14000600.
HA was partially supported by BSF grant 2017698.

References

[1] Regina Almeida and Arilton Teixeira. On the convergence of a predictor-corrector variant

algorithm. Top, 23(2):401–418, 2015.

[2] David Applegate, Mateo Diaz Diaz, Oliver Hinder, Haihao Lu, Miles Lubin, Brendan
O’Donoghue, and Warren Schudy. Practical large-scale linear programming using primal-
dual hybrid gradient. In Advances in Neural Information Processing Systems, 2021.

[3] Haim Avron and Sivan Toledo. Randomized algorithms for estimating the trace of an implicit
symmetric positive semi-deﬁnite matrix. Journal of the ACM (JACM), 58(2):1–34, 2011.

[4] Richard Barrett, Michael Berry, Tony F Chan, James Demmel, June Donato, Jack Dongarra,
Victor Eijkhout, Roldan Pozo, Charles Romine, and Henk Van der Vorst. Templates for the
solution of linear systems: building blocks for iterative methods. SIAM, 1994.

[5] F Bastos and J Paixão. Interior-point approaches to the transportation and assignment problems

on microcomputers. Investigação Operacional, 13(1):3–15, 1993.

[6] Kinjal Basu, Amol Ghoting, Rahul Mazumder, and Yao Pan. ECLIPSE: An extreme-scale
linear program solver for web-applications. In International Conference on Machine Learning,
pages 704–714, 2020.

[7] Stefania Bellavia. Inexact interior-point method. Journal of Optimization Theory and Applica-

tions, 96(1):109–121, 1998.

[8] Agniva Chowdhury, Palma London, Haim Avron, and Petros Drineas. Faster randomized infea-
sible interior point methods for tall/wide linear programs. In Advances in Neural Information
Processing Systems, volume 33, pages 8704–8715, 2020.

[9] Michael B. Cohen, Jelani Nelson, and David P. Woodruﬀ. Optimal approximate matrix product
In 43rd International Colloquium on Automata, Languages, and

in terms of stable rank.
Programming, pages 11:1–11:14, 2016.

[10] Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix

multiplication time. Journal of the ACM (JACM), 68(1):1–39, 2021.

[11] Samuel I Daitch and Daniel A Spielman. Faster approximate lossy generalized ﬂow via interior
point algorithms. In Proceedings of the fortieth annual ACM symposium on Theory of computing,
pages 451–460, 2008.

[12] David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):

1289–1306, 2006.

[13] Jacek Gondzio. Interior point methods 25 years later. European Journal of Operational Research,

218(3):587–601, 2012.

[14] Martin H. Gutknecht. Software for numerical linear algebra, volume 2. ETH Zurich, 2008.
URL http://www.sam.math.ethz.ch/~mhg/unt/SWNLA/itmethSWNLA08.pdf.

13

[15] Martin H Gutknecht and Stefan Röllin. The Chebyshev iteration revisited. Parallel Computing,

28(2):263–283, 2002.

[16] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review,
53(2):217–288, 2011.

[17] Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In Proceedings

of the 16th Annual ACM Symposium on Theory of Computing, pages 302–311, 1984.

[18] Leonid Genrikhovich Khachiyan. A polynomial algorithm in linear programming. In Doklady

Akademii Nauk, volume 244, pages 1093–1096. Russian Academy of Sciences, 1979.

[19] Yin Tat Lee and Aaron Sidford. Solving linear programs with

arXiv preprint arXiv:1910.08033, 2019.

√

rank linear system solves.

[20] Tianyi Lin, Shiqian Ma, Yinyu Ye, and Shuzhong Zhang. An ADMM-based interior-point
method for large-scale linear programming. Optimization Methods and Software, 36(2-3):
389–424, 2021.

[21] Sanjay Mehrotra. On the implementation of a primal-dual interior point method. SIAM Journal

on optimization, 2(4):575–601, 1992.

[22] Ofer Meshi and Amir Globerson. An alternating direction method for dual MAP LP relaxation.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,
pages 470–483. Springer, 2011.

[23] Shinji Mizuno and Florian Jarre. Global and polynomial-time convergence of an infeasible-
interior-point algorithm using inexact computation. Mathematical Programming, 84(1), 1999.

[24] Renato DC Monteiro and Jerome W O’Neal. Convergence analysis of a long-step primal-dual
infeasible interior-point lp algorithm based on iterative linear solvers. Georgia Institute of
Technology, 2003.

[25] Ben Recht, Christopher Re, Joel Tropp, and Victor Bittorf. Factoring nonnegative matrices
with linear programs. In Advances in Neural Information Processing Systems, pages 1214–1222,
2012.

[26] Maziar Salahi, Jiming Peng, and Tamás Terlaky. On mehrotra-type predictor-corrector algo-

rithms. SIAM Journal on Optimization, 18(4):1377–1397, 2008.

[27] Lukas Schork and Jacek Gondzio. Implementation of an interior point method with basis

preconditioning. Mathematical Programming Computation, 12(4):603–635, 2020.

[28] Zhao Song and Zheng Yu. Oblivious sketching-based central path method for linear programming.

In International Conference on Machine Learning, pages 9835–9847, 2021.

[29] Shashanka Ubaru and Yousef Saad. Fast methods for estimating the numerical rank of large
matrices. In International Conference on Machine Learning, pages 468–477. PMLR, 2016.

[30] Jan van den Brand, Yin Tat Lee, Aaron Sidford, and Zhao Song. Solving tall dense linear
programs in nearly linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium
on Theory of Computing, pages 775–788, 2020.

[31] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex
outer adversarial polytope. In International Conference on Machine Learning, pages 5286–5295.

14

PMLR, 2018.

[32] Stephen J Wright. Primal-dual interior-point methods. SIAM, 1997.

[33] Junfeng Yang and Yin Zhang. Alternating direction algorithms for ‘1-problems in compressive

sensing. SIAM Journal on Scientiﬁc Computing, 33(1):250–278, 2011.

[34] Yaguang Yang and Makoto Yamashita. An arc-search O(nL) infeasible-interior-point algorithm

for linear programming. Optimization Letters, 12(4):781–798, 2018.

[35] Ming Yuan. High dimensional inverse covariance matrix estimation via linear programming.

Journal of Machine Learning Research, 11(Aug):2261–2286, 2010.

[36] Ji Zhu, Saharon Rosset, Robert Tibshirani, and Trevor J. Hastie. 1-norm support vector

machines. In Advances in Neural Information Processing Systems, pages 49–56, 2004.

A Additional Proofs

Lemma A.1. If the duality measure of the IPM decreases with the relation,

µ1 ≤

(cid:18)

1 −

(cid:19)

C0√
n

µ0 + C1(cid:15)

for C0 ∈ (0, 1), C1 ∈ [0,

C0√
n

),

for all µ0 ≥ 2(cid:15), then the IPM algorithm converges to a point (x∗, y∗, s∗) with duality measure µ∗ < 2(cid:15)
in

log µ0

√

(cid:15) outer iterations.

n
C0

Proof. Each algorithm terminates when µk < 2(cid:15) indicating convergence has been reached. Therefore,
we assume that at each iteration µk ≥ 2(cid:15).

Although the starting point is assumed to be feasible. This fact can be ignored in the convergence
analysis. The constraint matrix of the linear program is assumed to be full rank and the system is
undetermined. This implies that for all (x0, y0, s0) there exists a vector b so that the starting point
is feasible. Since the duality measure does not depend on b, the decrease in the duality gap occurs
whether or not the starting point is feasible.

Given this, we can deﬁne a recurrence relation T (k) such that µk ≤ T (k), where,

T (k) =

(cid:18)

1 −

(cid:19)

C0
np

T (k − 1) + C1(cid:15)

T (0) = µ0.

(cid:16)

(cid:17)

1 − C0√
If we deﬁne ξ =
n
The solution to the recurrence relation is.

, then we have a recurrence relation of the form T (k) = ξT (k − 1) + C1(cid:15).

T (k) =

(cid:15)C1(1 − ξk)
1 − ξ

+ ξkµ0

+ ξkµ0

≤

(cid:15)C1
C0/np
≤ (cid:15) + ξkµ0.

Therefore, µk ≤ T (k) ≤ 2(cid:15) if ξkµ0 ≤ (cid:15). We can prove the outer iteration complexity using a standard

15

argument by substituting back in

(cid:16)

β > −1. We prove that µk < 2(cid:15) if k ≥

(cid:17)

1 − C0√
n
√
n
log µ0
(cid:15) .
C0
√

for ξ and using the identity log(1 + β) ≤ β for all

log

µ0
(cid:15)

≤ log

(cid:15)
µ0

k ≥

n
C0
−kC0√
n
(cid:18)

1 −

⇒

⇒ k log

C0√
n
(cid:19)k

(cid:19)

≤ log

(cid:15)
µ0

µ0 ≤ (cid:15)

(cid:18)

1 −

C0√
n

Therefore, the IPM algorithm is guaranteed to converge in

√

n
C0

log µ0

(cid:15) outer iterations.

Lemma A.2. Let u, v ∈ Rn.

ku ◦ vk2 ≤ kuk2kvk2

Proof.

ku ◦ vk2

2 =

n
X

i=1

(uivi)2 =

n
X

i=1

i v2
u2

i ≤

  n
X

u2
i

!   n
X

i=1

i=1

!

v2
i

= kuk2

2kvk2
2.

Lemma A.3. If M is an m × n matrix of full row rank and x is an arbitrary vector in the row
space of M, then kMxk2 ≥ σm(M)kxk2.

Proof. Let M = UΣVT where U, Σ ∈ Rm×m and V ∈ Rn×m constitute the thin-SVD of M. Since
U is an orthonormal matrix, multiplying a vector by U does not change the ‘2-norm. Therefore, we
have the following with y = VT x:

kMxk2

2 = kΣVT xk2
2 = kUΣVT xk2
2
m
X

= kΣyk2

2 =

σi(M)2y2
i

≥ σm(M)2

i=1
m
X

i ≥ σm(M)2kyk2
y2
2,

i=1

The vectors y and x have the same norm since yT y = xT VVT x = xT x, and the columns of V are
2 ≥ σm(M)2kxk2
orthonormal. Therefore, kMxk2
2.

Lemma A.4. (Simpliﬁcation of Lemma 12 in [8]) If (x, y, s) ∈ N2(θ), then,

kQ−1/2pk2 ≤ σ

s

2nµ
1 − θ

+ p2nµ.

16

Proof. By Lemma 7 in [8], if the condition given by eqn. (32) is fulﬁlled by sketching matrix W,
then kQ−1/2AD2AT Q−1/2 − Imk2 ≤ ζ. Since ζ ∈ (0, 1), this imples kQ−1/2AD2AT Q−1/2k2 ≤ 2
and so kQ−1/2ADk2 ≤
2. Recall that p = −σµAS−11n + Ax. We can split the terms of Q−1/2p
by the triangle inequality and then bound these terms separately below. We begin with the ﬁrst
term:

√

kσµQ−1/2AS−11nk2 = σµkQ−1/2AD(XS)−1/21nk2

≤ σµkQ−1/2ADk2k(XS)−1/21nk2
≤

2σµk(XS)−1/21nk2

√

√

√

≤

≤

2σµ

2σµ

r n

min xisi

s n

(1 − θ)µ

s

= σ

2nµ
1 − θ

.

Next, we bound the second term:

kQ−1/2Axk2 = kQ−1/2ADD−1xk2

= kQ−1/2AD(S1/2X−1/2)X1nk2
= kQ−1/2AD(SX)−1/21nk2
≤ kQ−1/2ADk2k(SX)−1/21nk2

√
2

≤

v
u
u
t

n
X

i=1

xisi = p2nµ.

By adding the bounds together, we conclude that:

kQ−1/2pk2 ≤ σ

s

2nµ
1 − θ

+ p2nµ.

B Inexact Predictor-Corrector IPM

In this section, we analyze the inexact predictor-corrector method (Algorithm 2) using an inexact
linear solver Solve. We follow the proof outline given in Section 3 to prove the convergence
guarantees and time complexity of Algorithm 2. As is common in predictor-corrector IPMs (see [32]),
we will assume that the matrix A has full row rank, ie., rank(A) = m ≤ n. See Appendix D.3 for
extensions alleviating this constraint.

17

Algorithm 2 Inexact Predictor-Corrector without correction

Input: A ∈ Rm×n, initial feasible point (x0, y0, s0) ∈ N2(0.25); IPM tolerance (cid:15) > 0, linear
solver tolerance δ(cid:15),n > 0.
Initialize: k ← 0;
while µk > 2(cid:15) do

(c) Set α = min

Predictor Step (σ = 0):
(a) Compute ∆˜y = Solve(AD2AT , Ax, δ(cid:15),n).
(b) Compute ∆˜x and ∆˜s using eqn. (13).
1/2, (µ/16(k∆˜x◦∆˜sk2))1/2o
n
(d) Compute (xk, yk, sk) = (xk, yk, sk) + α(∆˜xk, ∆˜yk, ∆˜sk).
Corrector Step (α = 1, σ = 1):
(e) Compute ∆˜y = Solve(AD2AT , −µAS−11n + Ax, δ(cid:15),n).
(f) Compute ∆˜x and ∆˜s using eqn. (13).
(g) Compute (xk+1, yk+1, sk+1) = (xk, yk, sk) + (∆˜xk, ∆˜yk, ∆˜sk).
(h) k ← k + 1.

end while

We will repeatedly express the inexact step as a function of the exact step. We start by expressing
the diﬀerence of the steps computed by the exact normal equations versus the inexact normal
equations, i.e. eqns. (6) versus eqns. (13):

∆y − ∆˜y = (AD2AT )−1f ,
∆s − ∆˜s = −AT (∆y − ∆˜y) = −AT (AD2AT )−1f ,
∆x − ∆˜x = −D2(∆s − ∆˜s) = D2AT (AD2AT )−1f .

(19)

(20)

(21)

We will prove that Algorithm 2 converges to a point (x∗, y∗, s∗) satisfying µ∗ < 2(cid:15) and kAx∗−bk2 < (cid:15)
in O(

n log µ0/(cid:15)) outer iterations. First, we start with a technical result to bound k∆˜x ◦ ∆˜sk2.

√

Lemma B.1. Let (x, y, s) ∈ N2(θ) and let (∆˜x, ∆˜y, ∆˜s) denote step calculated from the inexact
normal equations (see eqn. (13)). Then

k∆˜x ◦ ∆˜sk2 ≤

θ2 + n(1 − σ)2
23/2(1 − θ)

µ + 2

s

(θ2 + n(1 − σ)2)µ
(1 − θ)

k(AD)†f k2 + k(AD)†f k2
2.

Proof. We start by expressing the inexact step as the diﬀerence from the corresponding exact step,
using eqns. (20) and (21). This will allow us to leverage results for exact predictor-corrector IPMs
in our proof:

k∆˜x ◦ ∆˜sk2 = k[∆x − (∆x − ∆˜x)] ◦ [∆s − (∆s − ∆˜s)]k2

≤ k∆x ◦ ∆sk2 + k∆x ◦ (AT (AD2AT )−1f ) − ∆s ◦ (D2AT (AD2AT )−1f )k2
+ k(AT (AD2AT )−1f ) ◦ (D2AT (AD2AT )−1f )k2.

We will bound each of the three terms in the last inequality separately. Let B1 = k∆x ◦ ∆sk2;

18

B2 = k∆x ◦ (AT (AD2AT )−1f ) − ∆s ◦ (D2AT (AD2AT )−1f )k2; and
B3 = k(AT (AD2AT )−1f ) ◦ (D2AT (AD2AT )−1f )k2. Using eqn. (12), we get a bound on B1:

B1 ≤

θ2 + n(1 − σ)2
23/2(1 − θ)

µ.

Next, we bound B2. First, we rearrange B2 using properties of the Hadamard product (see Lemma
A.2) and Moore-Penrose pseudoinverse:

B2 = k∆x ◦ (AT (AD2AT )−1f ) − ∆s ◦ (D2AT (AD2AT )−1f )k2

= k[D−1∆x] ◦ [DAT (AD2AT )−1f ] − [D∆s] ◦ [DAT (AD2AT )−1f ]k2
= k[D−1∆x − D∆s] ◦ [DAT (AD2AT )−1f ]k2
≤ kD−1∆x − D∆sk2k(AD)†f k2.

(22)

We now bound kD−1∆x − D∆sk2 by using the deﬁnitions of ∆y, ∆s and ∆x given in eqn. (6).
Thus,

D−1∆x − D∆s = D−1(−x + σµS−11n − D2∆s) − D∆s

= D−1(−x + σµS−11n) − 2D∆s
= D−1(−x + σµS−11n) − 2(AD)†A(−x + σµS−11n)
= (D−1 − 2(AD)†A)(−x + σµS−11n)
= (D−1 − 2(AD)†A)DD−1(−X1n + σµS−11n)
= (I − 2(AD)†AD)[(XS)−1/2(−XS1n + σµ1n)].

At this point, we have shown that

B2 ≤ kI − 2(AD)†ADk2k(XS)−1/2(−XS1n + σµ1n)k2k(AD)†f k2.

(23)

We can now use the fact that the two-norm of I − 2(AD)†AD = I − (AD)†AD − (AD)†AD is
upper bounded by two, since I − (AD)†AD and (AD)†AD are both projection matrices and have
two-norm at most one. Next, we bound the middle term, namely k(XS)−1/2(−XSe + σµ1n)k2; the
proof will use the fact that (x, y, s) ∈ N2(θ), which implies that kx ◦ s − µ1nk2 ≤ θµ:

19

k(XS)−1/2(−XS1n + σµ1n)k2

2 =

n
X

(−xisi + σµ)2
xisi

≤

kx ◦ s − σµ1nk2
2
mini xisi

i=1
kx ◦ s − σµ1nk2
2
(1 − θ)µ

≤

k(x ◦ s − µ1n) + (1 − σ)µ1nk2
2
(1 − θ)µ

[(x ◦ s − µ1n) + (1 − σ)µ1n]T [(x ◦ s − µ1n) + (1 − σ)µ1n]
(1 − θ)µ

kx ◦ s − µ1nk2

2 + 2(1 − σ)µ1T

n (x ◦ s − µ1n) + (1 − σ)2µ2n

(1 − θ)µ

kx ◦ s − µ1nk2

2 + 2(1 − σ)µ(nµ − nµ) + (1 − σ)2µ2n

θ2µ2 + (1 − σ)2µ2n
(1 − θ)µ

(1 − θ)µ
(θ2 + (1 − σ)2n)µ
(1 − θ)

≤

.

≤

≤

≤

≤

≤

Inserting the bounds for kI − 2(AD)†ADk2 and k(XS)−1/2(−XS1n + σµ1n)k2 into the previous
bound for B2 gives:

s

B2 ≤ 2

(θ2 + (1 − σ)2n)µ
(1 − θ)

k(AD)†f k2.

Finally, we bound B3 using properties of the Hadamard product (see Lemma A.2):

B3 = k[AT (AD2AT )−1f ] ◦ [D2AT (AD2AT )−1f ]k2
= k[DAT (AD2AT )−1f ] ◦ [DAT (AD2AT )−1f ]k2
= kDAT (AD2AT )−1f k2
2
= k(AD)T ((AD)(AD)T )−1f k2
2
= k(AD)†f k2
2.

(24)

Adding the bounds on B1, B2, and B3 gives the ﬁnal inequality:

k∆˜x ◦ ∆˜sk2 ≤

θ2 + n(1 − σ)2
23/2(1 − θ)

µ + 2

s

(θ2 + n(1 − σ)2)µ
(1 − θ)

k(AD)†f k2 + k(AD)†f k2
2.

The next lemma will bound the value of k˜x(α) ◦ ˜s(α) − ˜µ(α)1nk2, which will allow us to later show
that the iterates remain in the correct neighborhood for a given step size.

Lemma B.2. If α ∈ [0, 1], then

k˜x(α) ◦ ˜s(α) − ˜µ(α)1nk2 ≤ (1 − α)kx ◦ s − µ1nk2 + α2k∆˜x ◦ ∆˜sk2 + α2k(AD)†f k2
2.

Proof. First, we derive an expression for the diﬀerence in the duality measure between the exact

20

and inexact step, (µ(α) − ˜µ(α)). We begin by expanding the deﬁnition of n(µ(α) − ˜µ(α)).

n(µ(α) − ˜µ(α)) = (x + α∆x)T (s + α∆s) − (x + α∆˜x)T (s + α∆˜s)

= (xT s + α∆xT s + xT α∆s + α2∆xT ∆s) − (xT s + α∆˜xT s + xT α∆˜s + α2∆˜xT ∆˜s)
= α(∆x − ∆˜x)T s + αxT (∆s − ∆˜s) + α2(∆x − ∆˜x)T (∆s − ∆˜s)

We next substitute the diﬀerences between the exact and inexact steps given by eqns. (20, 21). We
do this in two parts for clearer exposition.

α(∆x − ∆˜x)T s + αxT (∆s − ∆˜s) = α[D2AT (AD2AT )−1f ]T s + αxT [−AT (AD2AT )−1f ]

= αf T (AD2AT )−1AD2s − αxT AT (AD2AT )−1f
= αf T (AD2AT )−1AXS−1s − αxT AT (AD2AT )−1f
= αf T (AD2AT )−1Ax − αxT AT (AD2AT )−1f
= 0.

Therefore, substituting this result into the previous set of equations, we get the following.

n(µ(α) − ˜µ(α)) = α2(∆x − ∆˜x)T (∆s − ∆˜s)

= α2[D2AT (AD2AT )−1f ]T [−AT (AD2AT )−1f ]
= −α2f T (AD2AT )−1(AD2AT )(AD2AT )−1f
= −α2f T (AD2AT )−1f ⇒

µ(α) − ˜µ(α) =

−α2
n

k(AD)†f k2
2,

(25)

where the ﬁnal line follows from properties of the pseudoinverse. Next, we prove two identities that
will be useful. For the ﬁrst identity, we expand the term xi(α)si(α) and insert eqn. (11) to cancel
terms:

xi(α)si(α) − µ(α) = xisi + α(xi∆si + ∆xisi) + α2∆xi∆si − (1 − α(1 − σ))µ

= (1 − α)xisi + ασµ + α2∆xi∆si − (1 − α + ασ)µ
= (1 − α)(xisi − µ) + α2∆xi∆si.

(26)

For the second identity, we expand terms and substitute eqns. (20) and (21) for ∆s − ∆˜s and
∆x − ∆˜x, respectively:

xi(α)si(α) − ˜x(α)i˜s(α)i
= xisi + α(xi∆si + ∆xisi) + α2∆xi∆si − [xisi + α(xi∆˜si + ∆˜xisi) + α2∆˜xi∆˜si]
= α[xi(∆si − ∆˜si) + si(∆xi − ∆˜xi)] + α2(∆xi∆si − ∆˜xi∆˜si)
= α[x ◦ (−AT (AD2AT )−1f ) + s ◦ (D2AT (AD2AT )−1f )]i + α2(∆xi∆si − ∆˜xi∆˜si)
= α[x ◦ (−AT (AD2AT )−1f ) + x ◦ (AT (AD2AT )−1f )]i + α2(∆xi∆si − ∆˜xi∆˜si)
= α2(∆xi∆si − ∆˜xi∆˜si).

(27)

21

using the above two identities, we expand and rearrange ˜x(α)i˜s(α)i − ˜µ(α) to get:

˜x(α)i˜s(α)i − ˜µ(α) = x(α)is(α)i − (x(α)is(α)i − ˜x(α)i˜s(α)i) − [µ(α) − (µ(α) − ˜µ(α))]
= [x(α)is(α)i − µ(α)] − (x(α)is(α)i − ˜x(α)i˜s(α)i) + (µ(α) − ˜µ(α))
= [(1 − α)(xisi − µ) + α2∆xi∆si] − (x(α)is(α)i − ˜x(α)i˜s(α)i) + (µ(α) − ˜µ(α))
= (1 − α)(xisi − µ) + α2∆xi∆si − α2(∆xi∆si − ∆˜xi∆˜si) + (µ(α) − ˜µ(α))
= (1 − α)(xisi − µ) + α2∆˜xi∆˜si + (µ(α) − ˜µ(α)).

Taking vector norms of the above element-wise equality and substituting eqn. (25) for µ(α) − ˜µ(α),
we conclude,

k˜x(α) ◦ ˜s(α) − ˜µ(α)1nk2 ≤ (1 − α)kx ◦ s − µ1nk2 + α2k∆˜x ◦ ∆˜sk2 + k(µ(α) − ˜µ(α))1nk2

≤ (1 − α)kx ◦ s − µ1nk2 + α2k∆˜x ◦ ∆˜sk2 + α2k(AD)†f k2
2.

Next, we prove that the predictor stays in the slightly enlarged neighborhood N2(0.5) when starting
from the smaller neighborhood N2(0.25). We will later show that the corrector step guarantees that
the iterate “returns” to the “correct” N2(0.25) neighborhood. Recall that σ = 0 in the predictor
step to get the following lemma.

Lemma B.3. If (x, y, s) ∈ N2(0.25),

(

α = min

1/2,

(cid:18)

µ
16(k∆˜x ◦ ∆˜sk2)

(cid:19)1/2)

,

k(AD)†f k2 ≤

√

µ
2

,

then the predictor step (˜x(α), ˜y(α), ˜s(α)) ∈ N2(0.5).

Proof. We begin with the inequality of Lemma B.2. Recall that kx ◦ s − µ1nk2 ≤ µ/4 since
(x, y, s) ∈ N (0.25):

k˜x(α) ◦ ˜s(α) − ˜µ(α)1nk2 ≤ (1 − α)kx ◦ s − µ1nk2 + α2k∆˜x ◦ ∆˜sk2 + α2k(AD)†f k2
2

≤ (1 − α)kx ◦ s − µ1nk2 +

µ(k∆˜x ◦ ∆˜sk2)
16(k∆˜x ◦ ∆˜sk2

+

1
22

µ
4

(1 − α)µ
4
(1 − α)µ
4

+

+

1
8(1 − α)
(1 − α)µ
4

(1 − α)µ ≤

˜µ(α).

≤

≤

≤

1
2

1
2

(1 − α)µ

(since (x, y, s) ∈ N2(0.25))

(since α ≤ 1/2)

The last line follows from eqn. (11), which gives µ(α) = (1 − α)µ for σ = 0. We also know
from eqn. (25) that ˜µ(α) ≥ µ(α). Therefore, k˜x(α) ◦ ˜s(α) − ˜µ(α)1nk2 ≤ 1/2 ˜µ(α). Now, we must
show that the condition (˜x(α), ˜s(α)) > 0 is fulﬁlled. First, by eqns. (11, 25), we have ˜µ(α) =
[1−α(1−σ)]µ+ α2
2, which shows ˜µ(α0) > 0 for all positive step sizes α0 ≤ α. From the ﬁrst
part of this proof, we have that ˜xi(α)˜si(α) ≥ 1/2˜µ(α). We conclude that (˜x(α), ˜y(α), ˜s(α)) ∈ N2(0.5).

n k(AD)†f k2

22

The next lemma shows that the step size given in the previous lemma will result in a multiplicative
decrease in the duality gap during the predictor step.

Lemma B.4. If (x, y, s) ∈ N2(0.25),

(

α = min

1/2,

(cid:18)

µ
16(k∆˜x ◦ ∆˜sk2)

(cid:19)1/2)

,

and

k(AD)†f k2 ≤

√

µ
8

,

then the predictor step (˜x(α), ˜y(α), ˜s(α)) ∈ N2(0.5) and there exists a constant C0 ∈ (0, 1) such that,

˜µ(α)
µ

≤ 1 −

C0√
n

.

We note that in the above lemma (˜x(α), ˜y(α), ˜s(α)) is the output after the predictor step only,
before the corrector step is applied.

Proof. To lower bound the decrease in the duality gap at each step, we ﬁrst ﬁnd a lower bound
for the step size α, using the upper bound for k∆˜x ◦ ∆˜sk2. By Lemma B.1, we have the following
inequality:

k∆˜x ◦ ∆˜sk2 ≤

θ2 + n(1 − σ)2
23/2(1 − θ)

µ + 2

s

(θ2 + n(1 − σ)2)µ
(1 − θ)

k(AD)†f k2 + k(AD)†f k2
2.

We can simplify this inequality by substituting k(AD)†f k2 ≤

√

µ/8, θ = 0.25, and σ = 0 to get:

k∆˜x ◦ ∆˜sk2 ≤

≤

(0.25)2 + n(1 − 0)2
23/2(1 − 0.25)
s

1/16 + n
23/2(3/4)

µ + 2

(1/16 + n)µ
3/4

√

µ
8

+

µ
64

s

µ + 2

(0.252 + n(1 − 0)2)µ
(1 − 0.25)

√

µ
8

+

µ
64

≤ nµ

s

1/16 + 1
23/2(3/4)

+ 2

(1/16 + 1)
3/4

1
8

+

1
64

!

≤ 0.82 · nµ.

We can now insert the upper bounds for k∆˜x ◦ ∆˜sk2 and k(AD)†f k2 to lower bound α:

(cid:18)

(cid:18)

(cid:18)

(

α = min

1/2,

(

≥ min

1/2,

(

≥ min

1/2,

q

≥

0.14/n.

µ
8(k∆˜x ◦ ∆˜sk2 + k(AD)†f k2
2)

(cid:19)1/2)

µ
8(0.82)nµ + µ/8

(cid:19)1/2)

(cid:19)1/2)

1
n(6.56 + 1/8)

We proceed by using eqns. (11) and (25) to upper bound ˜µ(α). We substitute the upper bound for
k(AD)†f k2 as well as the upper and lower bounds for α as needed to derive a worst case bound.

23

 
Recall that the step size α is upper bounded by 1/2 (by deﬁnition):

˜µ(α) ≤ µ(α) − [µ(α) − ˜µ(α)] ≤ (1 − α)µ +

α2
n

k(AD)†f k2
2

≤ (1 − α) µ +

≤

1 −

!

r 0.14
n

µ +

α2µ
64n
!

" 

≤

1 −

r 0.14
n

1
4 · 64

√

n

#

"

µ ≤

1 −

+

µ
4 · 64n
√

0.14 − (1/256)
n

√

#

µ.

Then, if we let C0 =

√

0.14 − (1/256), we get

˜µ(α)
µ

≤ 1 −

C0√
n

,

with C0 ∈ (0, 1).

So far, We have shown that the predictor step results in a multiplicative decrease in the duality gap
while keeping the next iterate in the neighborhood N2(0.5). We now show that the corrector step
returns the iterate to the N2(0.25) neighborhood while increasing the duality gap by only a small
additive amount.

Lemma B.5. Let (x, y, s) ∈ N2(0.5) and k(AD)†f k2 ≤
N2(0.25) and |˜µ(1) − µ| ≤ k(AD)†f k2

2/n.

√

µ/26. Then, the corrector step (˜x(1), ˜y(1), ˜s(1)) ∈

Proof. First, we show that taking a step with step size α = 1 and centering parameter σ = 1 from
a point in N2(0.5) “returns” that point to the smaller neighborhood N2(0.25). We start with the
inequality given by Lemma B.2 with α = 1 and then substitute the bound for k∆˜x ◦ ∆˜sk2 from
Lemma B.1 to get:

k˜x(1) ◦ ˜s(1) − ˜µ(1)k2 ≤ k∆˜x ◦ ∆˜sk2 + k(AD)†f k2
2

≤

θ2 + n(1 − σ)2
23/2(1 − θ)

µ + 2

s

(θ2 + n(1 − σ)2)µ
(1 − θ)

k(AD)†f k2 + 2k(AD)†f k2
2.

Next, we use k(AD)†f k2 ≤

√

µ/26, θ = 0.5, and σ = 1:

k˜x(1) ◦ ˜s(1) − ˜µ(1)k2 ≤

(0.5)2 + n(1 − 1)2
23/2(1 − 0.5)

µ + 2

s

((0.5)2 + n(1 − 1)2)µ
(1 − 0.5)

√
µ
26 +

2µ
212

µ + 2

≤

≤

(1/4)
23/2(1/2)
µ
211/2

µ
29/2

+

+

√
µ
26 +

2µ
212

s

(1/4)µ
(1/2)
1
4

µ
211 ≤

µ.

Again, ˜µ(α) ≥ µ(α) which implies that k˜x(1) ◦ ˜s(1) − ˜µ(1)k2 ≤ 1/4 ˜µ(1), so we can conclude that
(˜x(1), ˜y(1), ˜s(1)) ∈ N2(0.25).

Finally, we show that the duality gap increases only slightly. In the exact case, the duality gap does
not increase at all when σ = 1, i.e. µ(1) = µ, as can be seen in eqn. (11). Therefore, by looking at

24

 
the diﬀerence between the exact and inexact duality gaps, i.e. eqn. (25), we get

|˜µ(1) − µ(1)| ≤

k(AD)†f k2
2
n

⇒ |˜µ(1) − µ| ≤

k(AD)†f k2
2
n

.

Finally, we combine the results of the previous lemmas with a standard convergence argument to show
the overall correctness and convergence rate of Algorithm 2, namely the inexact Predictor-Corrector
IPM, thus proving Theorem 1.

Proof. (of Theorem 1) We introduce the following notation to more easily discuss the steps of Algo-
rithm 2. Let (∆˜xp, ∆˜yp, ∆˜sp) denote the predictor step computed by steps (a-b) and (∆˜xc, ∆˜yc, ∆˜sc)
denote the corrector step computed by steps (e-f).

Algorithm 2 ﬁrst computes the predictor step from ∆˜yp = Solve(AD2AT , Ax, δ(cid:15),n) and then
computes the corrector step from ∆˜yc = Solve(AD2AT , −µAS−11n + Ax, δ(cid:15),n). Let fp and fc
denote the error vectors incurred when solving for the predictor and corrector steps, respectively.
Guarantee (i) of Solve (see eqn. (5)) allows us to bound the term k(AD)†f k2 as follows:

k∆˜y − (AD2AT )−1(−σµAS−11n + Ax)kAD2AT ≤ δ(cid:15),n
⇒ k∆˜y − ∆ykAD2AT ≤ δ(cid:15),n
⇒ k∆˜y − ∆ykAD2AT ≤ δ(cid:15),n
⇒ k(AD2AT )−1f kAD2AT ≤ δ(cid:15),n
⇒ k(AD)†f k2 ≤ δ(cid:15),n.

(28)

Thus, we have the following bounds, using the value of δ(cid:15),n in Theorem 1:

k(AD)†fpk2, k(AD)†fck2 ≤

√

(cid:15)/26.

If
Algorithm 2 sets the step size α for the predictor step to the value given by Lemma B.3.
(x0, y0, s0) ∈ N2(0.25) and µ0 ≥ 2(cid:15), then Lemma B.4 guarantees that the predictor step will reduce
the duality gap by a multiplicative factor of 1 − C0/√
n, while keeping the iterate in the neighborhood
N2(0.5). Lemma B.5 then guarantees that the ensuing corrector step will return the iterate to the
neighborhood N2(0.25), while increasing the duality measure by at most δ(cid:15),n/n. Therefore, a single
iteration of Algorithm 2, starting from a point (x0, y0, s0) ∈ N2(0.25) such that µ0 ≥ 2(cid:15) guarantees:

µ1 ≤

(cid:18)

1 −

(cid:19)

C0√
n

µ0 +

√

2

(cid:15)C0

n log µ0/(cid:15)

.

This fulﬁlls the conditions of Lemma A.1 with C0 ∈ (0, 1) and C1 ≤

we conclude that Algorithm 2 converges after

√

n
C0

log µ0

2
(cid:15) outer iterations iterations.

√

C0

n log µ0/(cid:15) ≤ C0/√

n. Therefore,

We now prove that the ﬁnal iterate of Algorithm 2, which we denote by (x∗, y∗, s∗), is (cid:15)-feasible. In a
single iteration, the primal variable changes by ∆˜x = ∆˜xp + ∆˜xc. Let f = fp + fc and recall that by
the second guarantee of Solve, kAD2AT ∆˜y − pk2 ≤ δ(cid:15),n. This is equivalent to kfpk2, kfck2 ≤ δ(cid:15),n
for all iterations of Algorithm 2. We proceed by showing that the change in the primal residual at

25

the k-th iteration is exactly fk:

kAxk − Axk−1k2 = kA(xk−1 + ∆˜xk−1) − Axk−1k2 = kA∆˜xk−1k2

= kA(∆xk−1 − ∆˜xk−1)k2 = kA(D2AT (AD2AT )−1fk−1)k2 = kfk−1k2.

We can use the bound kfik2 ≤ 2δ(cid:15),n to bound the primal residual at the k-th iteration.

kb − Axkk2 = kb − Ax1 + (Ax2 − Ax1) + ... + (Axk − Axk−1)k2

= k(Ax0 − Ax1) + (Ax2 − Ax1) + ... + (Axk − Axk−1)k2
≤ kAx0 − Ax1k2 + kAx2 − Ax1k + ... + kAxk − Axk−1k2
≤ kf0k2 + kf1k2 + ... + kfk−1k2 ≤ 2kδ(cid:15),n.

√

We previously concluded in this proof by Lemma A.1 that the Algorithm 2 will converge after
(cid:15)C0
n log µ0/(cid:15) . Therefore, we conclude
k =
that kAx∗ − bk2 ≤ (cid:15), i.e. the solution is (cid:15)-primal feasible.

iterations. By the conditions of Theorem 1, δ(cid:15),n <

log µ0
(cid:15)

n
C0

√

2

C Error-Adjusted Predictor-Corrector IPM

Lemma C.1. Let (x, y, s) ∈ N2(θ) and let (∆˜x, ∆˜y, ∆˜s) be the step calculated from the inexact
normal equations with error-adjustment (see eqn. (14)). Then,

k∆˜x ◦ ∆˜sk2 ≤

θ2 + n(1 − σ)2
23/2(1 − θ)

µ + 3

s

(θ2 + n(1 − σ)2)µ
(1 − θ)

k(XS)−1/2vk2 + 2k(XS)−1/2vk2
2.

Proof. This proof has a similar structure to Lemma B.1. However, there are additional steps needed
due to the correction vector and we will use several of our previous bounds throughout the proof.
First, we start by expressing ∆˜x ◦ ∆˜s as a function of the exact step and then we substitute the
diﬀerence between the exact and error-adjusted steps (eqns. (16) and (17)):

k∆˜x ◦ ∆˜sk2 = k[∆x − (∆x − ∆˜x)] ◦ [∆s − (∆s − ∆˜s)]k2

≤ k∆x ◦ ∆sk2 + k∆x ◦ (AT (AD2AT )−1AS−1v)
− ∆s ◦ (D2AT (AD2AT )−1AS−1v − S−1v)k2
+ k(D2AT (AD2AT )−1AS−1v − S−1v) ◦ (AT (AD2AT )−1AS−1v)k2.

We will bound each of these three terms in the last inequality separately. Let B1 = k∆x ◦ ∆sk2;
B2 = k∆x ◦ (AT (AD2AT )−1AS−1v) − ∆s ◦ (D2AT (AD2AT )−1AS−1v − S−1v)k2; and
B3 = k(D2AT (AD2AT )−1AS−1v − S−1v) ◦ (AT (AD2AT )−1AS−1v)k2. Using eqn. (12), we get a
bound on B1:

B1 ≤

θ2 + n(1 − σ)2
23/2(1 − θ)

µ.

26

Next, we bound B2 by splitting it into two parts:

B2 = k∆x ◦ (AT (AD2AT )−1AS−1v) − ∆s ◦ (D2AT (AD2AT )−1AS−1v − S−1v)k2

= k∆x ◦ (AT (AD2AT )−1AS−1v) − ∆s ◦ D2AT (AD2AT )−1AS−1v + ∆s ◦ S−1vk2
≤ k∆x ◦ (AT (AD2AT )−1AS−1v) − ∆s ◦ D2AT (AD2AT )−1AS−1vk2 + k∆s ◦ S−1vk2.

2 = k∆x ◦ (AT (AD2AT )−1AS−1v) − ∆s ◦ D2AT (AD2AT )−1AS−1vk2 and B(b)

Let B(a)
S−1vk2. Notice that B(a)
2
predictor-corrector proof without error-adjustment of Section B by setting f = AS−1v:

2 = k∆s ◦
can be bounded in the same way as B2 was bounded in eqn. (23) in the

B(a)
2 ≤ kI − 2(AD)†ADk2k(XS)−1/2(−XSe + σµ1n)k2k(AD)†AS−1vk2.

(29)

Furthermore, the ﬁrst two terms of the above inequality were already bounded in the predictor-
corrector proof without error-adjustment:

kI − 2(AD)†ADk2 ≤ 2

and k(XS)−1/2(−XSe + σµ1n)k2 ≤

s

(θ2 + (1 − σ)2n)µ
(1 − θ)

.

Next, we note that

k(AD)†AS−1vk2 = k(AD)†AD(XS)−1/2vk2 ≤ k(AD)†ADk2k(XS)−1/2vk2 ≤ k(XS)−1/2vk2.

Substituting the above three inequalities into eqn. (29) we get:

s

B(a)

2 ≤ 2

(θ2 + (1 − σ)2n)µ
(1 − θ)

k(XS)−1/2vk2.

We now bound B(b)
2 . Using the deﬁnition of ∆˜s (eqn. (16)), as well as k(AD)†ADk2 ≤ 1 and the
bound on k(XS)−1/2(−XS1n + σµ1n)k2, we get. Again, we apply Lemma A.2 to go from the fourth
to the ﬁfth line.

B(b)
2 = kS−1v ◦ ∆sk2

= kS−1v ◦ AT (AD2AT )−1(σµAS−11n − Ax)k2
= k(XS)−1/2v ◦ DAT (AD2AT )−1(σµAS−11n − Ax)k2
= k(XS)−1/2v ◦ (AD)†(σµAS−11n − Ax)k2
≤ k(XS)−1/2vk2k(AD)†(σµAS−11n − Ax)k2
≤ k(XS)−1/2vk2k(AD)†AD(σµ(XS)−1/21n − (XS)1/21n)k2
≤ k(XS)−1/2vk2k(AD)†ADk2k(XS)−1/2(−XS1n + σµ1n)k2

s

≤

(θ2 + n(1 − σ)2)µ
(1 − θ)

k(XS)−1/2vk2.

We can now use the bounds on B(a)

2

and B(b)
2

to obtain the desired bound on B2:

B2 ≤ B(a)

2 + B(b)

2 ≤ 3

s

(θ2 + n(1 − σ)2)µ
(1 − θ)

k(XS)−1/2vk2.

27

Finally, we bound B3. We distribute the terms in B3 and split the norm into two components using
the triangle inequality:

B3 = k(D2AT (AD2AT )−1AS−1v + S−1v) ◦ (AT (AD2AT )−1AS−1v)k2

= k(D2AT (AD2AT )−1AS−1v ◦ (AT (AD2AT )−1AS−1v + S−1v ◦ (AT (AD2AT )−1AS−1vk2
≤ k(D2AT (AD2AT )−1AS−1v ◦ AT (AD2AT )−1AS−1vk2 + kS−1v ◦ AT (AD2AT )−1AS−1vk2.

3 = k(D2AT (AD2AT )−1AS−1v◦AT (AD2AT )−1AS−1vk2 and B(b)

3 = kS−1v◦AT (AD2AT )−1AS−1vk2.

following similar ideas to the derivation of the bound of B3 in the predictor-

Let B(a)
We ﬁrst bound B(a)
corrector proof without error-adjustment:

3

B(a)
3 = k(D2AT (AD2AT )−1AS−1v ◦ AT (AD2AT )−1AS−1vk2
= k(DAT (AD2AT )−1AS−1v ◦ DAT (AD2AT )−1AS−1vk2
= k(AD)†AS−1vk2

2 ≤ k(XS)−1/2vk2
2.

We also bound B(b)
3 :

B(b)
3 = kS−1v ◦ AT (AD2AT )−1AS−1vk2 = k(XS)−1/2v ◦ DAT (AD2AT )−1AS−1vk2

≤ k(XS)−1/2vk∞k(XS)−1/2vk2 ≤ k(XS)−1/2vk2
2.

Combining the above two inequalities, we get the overall bound for B3:

kB3k2 ≤ B(a)

3 + B(b)

3 ≤ 2k(XS)−1/2vk2
2.

Finally, summing up all bounds for B1, B2, and B3 gives our ﬁnal inequality:

k∆˜x ◦ ∆˜sk2 ≤

θ2 + n(1 − σ)2
23/2(1 − θ)

µ + 3

s

(θ2 + n(1 − σ)2)µ
(1 − θ)

k(XS)−1/2vk2 + 2k(XS)−1/2vk2
2.

Recall that a point (x, y, s) is in the neighborhood N2(θ) if kx ◦ s − µ1nk2 ≤ µθ. We bound the left
hand side of this condition after a step of size α is taken.

Lemma C.2. If α ∈ [0, 1], then

k˜x(α) ◦ ˜s(α) − ˜µ(α)1nk2 ≤ (1 − α)kx ◦ s − µ1nk2 + α2k∆˜x ◦ ∆˜sk2 + 2αkvk2.

Proof. We start by expanding the expression for ˜x(α) ◦ ˜s(α):

˜x(α) ◦ ˜s(α) = (x + α∆˜x) ◦ (s + α∆˜s)

= x ◦ s + α(x ◦ ∆˜s + s ◦ ∆˜x) + α2∆˜x ◦ ∆˜s
= x ◦ s + α(x ◦ ∆˜s + s ◦ (−x + σµS−11n − D2∆˜s − S−1v)) + α2∆˜x ◦ ∆˜s
= x ◦ s + α(−x ◦ s + σµ1n − v) + α2∆˜x ◦ ∆˜s
= (1 − α)x ◦ s + ασµ1n − αv + α2∆˜x ◦ ∆˜s.

Left-multiplying the ﬁnal expression by the vector 1T

n and dividing by n, gives an expression for

28

˜µ(α). Notice that ∆˜xT ∆˜s = −∆˜xT AT ∆˜y = 0 by substituting the deﬁnition of the ∆˜s without
error-adjustment and using the fact that Ax = b at each step:

˜µ(α) =

1
n

n [(1 − α)x ◦ s + ασµ1n − αv + α2∆˜x ◦ ∆˜s]
1T

= [1 − α(1 − σ)]µ − α/n vT 1n.

(30)

For simplicity of exposition, we ﬁrst look at individual elements of the vector ˜x(α) ◦ ˜s(α) − ˜µ(α):

˜xi(α)˜si(α) − ˜µ(α) = (1 − α)xisi + ασµ − αvi + α2∆˜xi∆˜si − µ + αµ − ασµ +

= (1 − α)(xisi − µ) + α2∆˜xi∆˜si − α

vi −

!

.

vT 1n
n

αvT 1n
n

We bound the norm of the last summand as follows:

1
n

|vT 1n| ≤

1
n

kvk2k1nk2 ≤

1
√
n

kvk2 ⇒

(cid:13)
(cid:13)
(cid:13)v −

(vT 1n)
n

1n

(cid:13)
(cid:13)
(cid:13)2

≤ kvk2 +

1
√
n

kvk2 ≤ 2kvk2.

Therefore, we can conclude that,

k˜x(α) ◦ ˜s(α) − ˜µ(α)1nk2 ≤ (1 − α)kx ◦ s − µ1nk2 + α2k∆˜x ◦ ∆˜sk2 + 2αkvk2.

Given the previous bound, we can now derive a step size α which guarantees that the iterate remains
in N2(0.5) after the predictor step.

Lemma C.3. If (x, y, s) ∈ N2(0.25), α = min
predictor step (˜x(α), ˜y(α), ˜s(α)) ∈ N2(0.5).

1/2, (µ/16k∆˜x◦∆˜sk2)1/2o
n

, and kvk2 ≤ µ/32, then the

Proof. Our starting point is the bound of Lemma C.2. By deﬁnition, α is upper-bounded by both
1/2 and the term depending on µ:

k˜x(α) ◦ ˜s(α) − ˜µ(α)1nk2 ≤ (1 − α)kx ◦ s − µ1nk2 + α2k∆˜x ◦ ∆˜sk2 + 2αkvk2

k∆˜x ◦ ∆˜sk2 +

2
2

kvk2

≤

≤

≤

≤

≤

≤

(1 − α)µ
4
(1 − α)µ
4
(1 − α)µ
4
(1 − α)µ
4

+

+

+

+

+

µ
16k∆˜x ◦ ∆˜sk2
µ
µ
32
16
µ
µ
8
32
(1 − α)µ
8(1 − α)

µ
32

−

−

(1 − α)µ −

µ
32

˜µ(α).

1
2
1
2

(30), which states ˜µ(α) = [1 − α(1 − σ)]µ − α/n vT 1n. By
The last step follows from eqn.
applying the Cauchy-Schwarz inequality to vT 1n as done previously and σ = 0, we obtain ˜µ(α) ≥

29

 
n kvk2, which allows us to conclude that k˜x(α) ◦ ˜s(α) − ˜µ(α)1nk2 ≤ 1
(1 − α)µ − α√
2 ˜µ(α). Now,
we must show that the condition (˜x(α), ˜s(α)) > 0 is fulﬁlled. First, by eqns. (11, 30), we have
˜µ(α) = [1 − α(1 − σ)]µ − α/n vT 1n, which shows ˜µ(α0) > 0 for all positive step sizes α0 ≤ α. From the
ﬁrst part of this proof, we have that ˜xi(α)˜si(α) ≥ 1/2˜µ(α). We conclude that (˜x(α), ˜y(α), ˜s(α)) ∈
N2(0.5).

We now show that the predictor step with step size α as given in the above lemma guarantees a
multiplicative decrease in the duality gap. Recall that σ = 0 in the predictor step when solving the
normal equations.

Lemma C.4. If (x, y, s) ∈ N2(0.25), α = min
, and kvk2 ≤ µ/32, then the
predictor step (˜x(α), ˜y(α), ˜s(α)) remains in N2(0.5) and there exists a constant C0 ∈ (0, 1) such
that,

1/2, (µ/16k∆˜x◦∆˜sk2)1/2o
n

˜µ(α)
µ

≤ 1 −

C0√
n

.

Proof. Lemma C.3 already shows that this value of α ensures that the next iterate remains in
N2(0.5). Therefore, we just need to prove the multiplicative decrease in the duality measure.
Towards that end, we will again need to lower bound the step size α, starting from the upper bound
for k∆˜x ◦ ∆˜sk2. By Lemma C.1, we get the following inequality:

k∆˜x ◦ ∆˜sk2 ≤

θ2 + n(1 − σ)2
23/2(1 − θ)

µ + 3

s

(θ2 + n(1 − σ)2)µ
(1 − θ)

k(XS)−1/2vk2 + 2k(XS)−1/2vk2
2.

We now derive a bound on k(XS)−1/2vk2 using the bound on kvk2. We use the fact that for
(x, y, s) ∈ N2(θ), xisi ≥ (1 − θ)µ to get:4

k(XS)−1/2vk2 ≤ k(XS)−1/2k2kvk2 ≤

√

1
min xisi

kvk2 ≤

1
p(1/4)µ

µ
9

≤

√

µ

2

9

.

Next, we simplify the inequality from Lemma C.1 by substituting k(XS)−1/2vk2 ≤ 2
and σ = 0 to get

√
µ
9 , θ = 0.25,

k∆˜x ◦ ∆˜sk2 ≤

(0.25)2 + n(1 − 0)2
23/2(1 − 0.25)

µ + 3

s

((0.25)2 + n(1 − 0)2)µ
(1 − 0.25)

√

2

µ

9

+

2 · 22µ
92

≤

(1/16) + n
23/2(3/4)

µ + 3

s

(1/16) + n
(3/4)

2µ
9

+

23µ
92

≤ nµ

(1/16) + 1
23/2(3/4)

+ 3

s

(1/16) + 1
(3/4)

2
9

+

23
92

!

≤ 1.4 · nµ.

4The inequality xisi ≥ (1 − θ)µ follows from the deﬁnition of N2(θ).

30

 
The above upper bound can now be used to lower-bound α:

α = min

≥ min

(

(

(cid:18)

(cid:18)

1
2

1
2

,

,

(cid:19)1/2)

µ
16k∆˜x ◦ ∆˜sk2

(cid:19)1/2)

µ
16nµ(1.4)

≥

0.2
√
n

.

Eqn. (30) states that ˜µ(α) = [1 − α(1 − σ)]µ − α/n vT 1n. Combining it with our upper and lower
bounds for α we can bound the decrease in the duality measure ˜µ as follows:

˜µ(α) = [1 − α(1 − σ)]µ −

≤ [1 − α(1 − σ)]µ +

α
vT 1n
n
α
√
n

kvk2

(By Cauchy-Schwarz)

µ
n · 9

(cid:18)

1 −

(cid:18)

(cid:18)

1 −

1 −

≤

≤

≤

(cid:19)

0.2
√
n

µ +

√

2
(cid:19)
0.2 − 1/18
√
n
(cid:19)

0.14
√
n

µ.

µ

(31)

We have shown that the predictor step results in a multiplicative decrease in the duality gap, while
keeping the next iterate in the neighborhood N2(0.5). We now show that the corrector step returns
the iterate to the N2(0.25) neighborhood, while increasing the duality gap by a small additive
amount.

Lemma C.5. Let (x, y, s) ∈ N2(0.5) and kvk2 ≤ µ/27. Then, the corrector step (˜x(1), ˜y(1), ˜s(1)) ∈
N2(0.25) and |˜µ(1) − µ| ≤ 1√

n kvk2.

Proof. We start by simplifying the inequality of Lemma C.1 for the corrector step. Recall:

k∆˜x ◦ ∆˜sk2 ≤

θ2 + n(1 − σ)2
23/2(1 − θ)

µ + 3

s

(θ2 + n(1 − σ)2)µ
(1 − θ)

k(XS)−1/2vk2 + 2k(XS)−1/2vk2
2.

We bound k(XS)−1/2vk2 using the bound on kvk2 from the condition of the lemma:
√
µ
26 .

k(XS)−1/2vk2 ≤ k(XS)−1/2k2kvk2 ≤

1
p(1/2)µ

1
min xisi

µ
27 ≤

kvk2 ≤

√

We then simplify the inequality from Lemma C.1 by substituting k(XS)−1/2vk2 ≤
σ = 1:

√
µ
26 , θ = 0.5, and

k∆˜x ◦ ∆˜sk2 ≤

(0.5)2 + n(1 − 1)2
23/2(1 − 0.5)

µ + 3

s

((0.5)2 + n(1 − 1)2)µ
(1 − 0.5)

√
µ
26 + 2

µ
214 ≤

µ
25/2

+

3µ
213/2

+

µ
213 .

31

Next, we show that taking a step with step size α = 1 and centering parameter σ = 1 from a point
in the “larger” neighborhood N2(0.5) returns the iterate to the “smaller” neighborhood N2(0.25).
We start from the result of Lemma C.2 with α = 1:

k˜x(1) ◦ ˜s(1) − ˜µ(1)k2 ≤ (1 − α)kx ◦ s − µ1nk2 + α2k∆˜x ◦ ∆˜sk2 + 2αkvk2

≤

+

+

≤ k∆˜x ◦ ∆˜sk2 + 2kvk2
3µ
213/2
3µ
213/2
˜µ
4

µ
25/2
µ
25/2
µ
4

µ
213 +
µ
213 +

µ
27 ≤

≤

≤

+

+

−

.

2µ
27
2µ
27 +

µ
27 −

µ
27

The last step follows from ˜µ(α) ≥ µ − α√

n kvk2, which can be derived from eqn. (30).

This implies that the corrector step will return the iterate to the neighborhood N2(0.25).
Finally, by eqn. (30), we know that ˜µ(α) = [1 − α(1 − σ)]µ − α/nvT 1n. Substituting α = 1 and
σ = 1 allows us to bound ˜µ(1) − µ:

˜µ(1) = µ −

1
n

vT 1n ⇒ ˜µ(1) − µ =

−1
n

vT 1n ⇒ ˜µ(1) − µ ≤

√

1
n

nkvk2 ⇒ ˜µ(1) − µ ≤

1
√
n

kvk2.

We are now ready to combine the results of the previous lemmas to show the overall correctness
and convergence rate of Algorithm 1, the error-adjusted inexact predictor-corrector IPM.

Proof. (of Theorem 2) By the guarantees of Solvev, we know that the error-adjusted normal
equations are solved for a given σ and kvk2 < (cid:15)/27 < µ/27 at each iteration. First, Lemma C.3
guarantees that the intermediate point computed at step (d) of Algorithm 1 remains in the
neighborhood N2(0.5). Lemma C.4 guarantees that the predictor step decreases the duality measure
of the iterate by at least a multiplicative factor of the form (1 − C0/√

n) for some constant C0.

Next, Lemma C.5 ensures that the corrector step of Algorithm 1 returns the iterate to the neigh-
borhood N2(0.25), while increasing the duality measure by at most 1/√
n. Therefore,
a single iteration of Algorithm 1 starting from a point (x0, y0, s0) ∈ N2(0.25) such that µ0 ≥ 2(cid:15)
guarantees the following inequality:

nkvk2 ≤ δ(cid:15),n/√

µ1 ≤

(cid:18)

1 −

(cid:19)

C0√
n

µ0 +

(cid:15)
√

.

n

27

This fulﬁlls the conditions of Lemma A.1 with C0 ∈ (0, 1) and C1 ≤ 1
n ≤ C0/
√
27
n log µ0/(cid:15)) iterations.
Therefore, we conclude that Algorithm 1 converges in O(

√

√

n (see eqn. (31)).

Finally, we prove that the ﬁnal iterate is primal-feasible, i.e. kAx∗ − bk2 = 0. By eqn. (17), at
each step of Algorithm 1, ∆x − ∆˜x = −D2AT (AD2AT )−1AS−1v + S−1v. We left multiply this
expression by A to get

A∆x − A∆˜x = −AD2AT (AD2AT )−1AS−1v + AS−1v = 0m.

32

This implies that the change in the primal-residual of the error-adjusted algorithm is the same as
the change in the primal-residual of the exact algorithm at every iteration. Therefore, since the
exact algorithm returns a primal-feasible solution, the error-adjusted algorithm does as well.

D Implementing Solve and Solvev using randomized linear alge-

bra

We now discuss how to implement the solvers that are needed in our inexact predictor-corrector
IPMs, with and without the correction vector, using standard preconditioned solvers, such as the
preconditioned conjugate gradient (PCG) method. We use well-known sketching-based approaches
to construct the preconditioner, leveraging results from the randomized linear algebra literature.

We ﬁrst focus on (full row-rank) constraint matrices A ∈ Rm×n that are short-and-fat, ie., m (cid:28) n.
Clearly such matrices have rank m (cid:28) n. In Section D.3 below we will discuss how to reduce general
LP problems with exact low-rank constraint matrices to this setting. Moreover, in Appendix D.2,
we also discuss how to handle the tall-and-thin constraint matrices. Towards that end, consider the
LP of eqn. (1) with an input matrix A ∈ Rm×n and m (cid:28) n. First, we prove that the preconditioned
conjugate gradient (PCG) method of Algorithm 3 (see also [8]) can fulﬁll the requirements of Solve
iterations and the guarantees of Solvev in O (cid:0)log nµ
in O
δ

log σmax(AD)nµ

(cid:1) iterations.

(cid:16)

(cid:17)

δ

Let AD = UΣVT be the thin SVD representation and W ∈ Rn×w be an oblivious sparse sketching
matrix which satisﬁes:5

kVWWT VT − Imk2 ≤

,

(32)

ζ
2

with probability at least 1 − η. The work of Cohen et al. [9] shows how to construct such a matrix
W fulﬁlling this guarantee with sketch size w = O(m/ζ2 · log m/η) and O(1/ζ · log m/η) non-zero entries
per row. One possible construction is to uniformly sample s = O(1/ζ · log m/η) entries per row of
W without replacement and set each of the selected entries to ±1/s independently and uniformly
randomly. Next, we use the above sketching matrix to deﬁne Q = ADWWT DAT ; we note that Q
is not explicitly constructed in Algorithm 3. Then, with probability at least 1 − η, the vector ˜zt
computed by Algorithm 3 fulﬁlls the following inequality (see Equation 7 in [8]).

kQ−1/2(AD2AT )Q−1/2˜zt − Q−1/2pk2 ≤ ζ tkQ−1/2pk2,

ζ ∈ (0, 1).

(33)

Algorithm 3 Preconditioned Conjugate Gradient (Algorithm 1 in [8])

Input: AD ∈ Rm×n with m (cid:28) n, p ∈ Rm, failure probability η, iteration count t;
1. Compute ADW and its SVD, where W ∈ Rn×w fulﬁlls eqn. (32) with r = m. Let UQ ∈ Rm×m
1/2
Q ∈ Rm×m be the matrix of its singular values;
be the matrix of its left singular vectors and let Σ
2. Compute Q−1/2 = UQΣ−1/2
3. Initialize ˜z0 ← 0m and run standard CG on Q−1/2AD2AT Q−1/2˜z = Q−1/2p for t iterations;
Output: return ∆˜y = Q−1/2˜zt;

Q UT
Q;

5Let kAk2

F = P

i,j A2

ij = tr(AT A) denote the (square of the) Frobenius norm of matrix A.

33

Recall that the function Solve is deﬁned to have the following guarantees:

∆˜y = Solve(AD2AT , p, δ) ⇒ k∆˜y − (AD2AT )−1pkAD2AT ≤ δ and kAD2AT ∆˜y − pk2 ≤ δ.

The next lemma shows that Algorithm 3 fulﬁlls the conditions of Solve.

Lemma D.1. If Algorithm 3 is used to compute ∆˜y = Solve(AD2AT , p, δ), (x, y, s) ∈ N2(θ),
and t = O

, then, with probability at least 1 − η, ∆˜y satisﬁes

log σmax(AD)nµ

(cid:16)

(cid:17)

δ

k∆˜y − (AD2AT )−1pkAD2AT ≤ δ and kAD2AT ∆˜y − pk2 ≤ δ.

Proof. We start by bounding k∆˜y − (AD2AT )−1pkAD2AT , where ∆˜y = Q−1/2˜zt, using the guar-
i (Q−1/2AD) ≤
antee given by eqn. (33) and Lemma 2 of [8], which guarantees (1 + ζ/2)−1 ≤ σ2
(1 − ζ/2)−1 for all i = 1 . . . m, when W fulﬁlls eqn. (32):

kQ−1/2(AD2AT )Q−1/2z(t) − Q−1/2pk2 = kQ−1/2(AD2AT )(Q−1/2z(t) − (AD2AT )−1p)k2

≥

≥

1
p1 + ζ/2
1
p1 + ζ/2

kDAT (Q−1/2z(t) − (AD2AT )−1p)k2

k(Q−1/2z(t) − (AD2AT )−1p)k(AD2AT ).

The ﬁrst step is justiﬁed by Lemma A.3, since the column space of DAT is the row space of
Q−1/2AD. We show in Lemma A.4 that kQ−1/2pk2 ≤ C
nµ for some constant C depending only
on σ and θ. Combining this bound with eqn. (33) gives:

√

ζ t (1 + ζ/2)1/2 C

√

nµ ≥ k(Q−1/2z(t) − (AD2AT )−1p)k(AD2AT ).

(34)

This implies that k∆˜y − (AD2AT )−1pkAD2AT ≤ δ after t = O (cid:0)log nµ
kAD2AT ∆˜y − pk2 using the guarantee of eqn. (33):

δ

(cid:1) iterations. Next, we bound

kQ−1/2(AD2AT )Q−1/2z(t) − Q−1/2pk2 ≥ σmin(Q−1/2)kAD2AT Q−1/2z(t) − pk2
⇒ ζ tσmax(Q1/2)kQ−1/2pk2 ≥ kAD2AT Q−1/2z(t) − pk2
nµ ≥ kAD2AT Q−1/2z(t) − pk2.

⇒ ζ tσmax(Q1/2)C

√

Again, the ﬁrst step follows from Lemma A.3. Since ζ ∈ (0, 1), we conclude that kAD2AT ∆˜y−pk2 <
δ after t = O(cid:0) log σmax(AD)nµ
(cid:1) iterations. Therefore, both guarantees of Solve can be achieved
with probability at least 1 − η in t = O(cid:0) log σmax(AD)nµ

(cid:1) iterations.

δ

δ

Satisfying eqn. (33). Exploiting the properties of the preconditioner Q−1/2, [8] showed how to
satisfy eqn. (33) using popular solvers beyond conjugate gradient. Such solvers include steepest
descent and Richardson iteration. We could do the same in our work and prove similar results
for, say, the Chebyshev iteration [4, 14, 15]. Indeed, the preconditioner Q−1/2 can be combined
with Theorem 1.6.2 of [14] to satisfy eqn. (33). Chebyshev iteration avoids the computation
of the inner products which is typically needed for CG or other inexact methods. As a result,
Chebyshev iteration oﬀers several advantages in a parallel environment as it does not need to
evaluate communication-intensive inner products for computing the recurrence parameters.

34

D.1 Computing the error-adjustment vector v for Algorithm 1

In this section we discuss how to eﬃciently compute the correction vector v for our “corrected”
inexact predictor-corrector IPM. Recall eqn. (14): the correction vector must satisfy AS−1v =
(AD2AT )(∆˜y − ∆y). One possible construction of such a vector v is the following:

v = (XS)1/2W(ADW)†(AD2AT ∆˜y − p).

(35)

Notice that this vector can be constructed eﬃciently from quantities already computed in Algorithm 3.
Left-multiplying by AS−1 immediately proves that this construction for v satisﬁes AS−1v =
(AD2AT )(∆˜y − ∆y), with probability at least 1 − η. We now prove the following lemma:

Lemma D.2. Let ∆˜y be computed by Algorithm 3 and let the correction vector v be computed by
eqn. (35). Then, kvk2 ≤ δ after t = O (cid:0)log nµ
δ

(cid:1) iterations.

Proof. Lemma 5 from [8] (using our notation) guarantees that:

kvk2 ≤ p3nµkQ−1/2(AD2AT )Q−1/2z(t) − Q−1/2pk2.

Using eqn. (34),

kvk2 ≤ p3nµkQ−1/2(AD2AT )Q−1/2z(t) − Q−1/2pk2

≤ p3nµ(ζ t(1 + ζ/2)1/2Cn

√

µ).

Again, since ζ ∈ (0, 1), we can conclude that kvk2 ≤ δ after t = O (cid:0)log nµ
δ
with probability at least 1 − η.

(cid:1) iterations of Algorithm 3

D.2 Constraint matrices with m (cid:29) n and rank(A) = n

So far, we only focused on constraint matrices that have full row-rank and are wide i.e., m (cid:28) n. By
considering the dual problem, our methods also address constraint matrices that are tall-and-thin
and have full column rank i.e., m (cid:29) n. Let A ∈ Rm×n be the constraint matrix with m (cid:29) n and
rank(A) = n such that the primal LP is given by

min cTx, subject to Ax = b, x ≥ 0.

The associated dual problem is,

max bTy, subject to ATy + s = c, s ≥ 0,

(36)

(37)

Note that the dual variable y is a free variable i.e. it can have both non-negative and non-positive
entries. However, we can always rewrite y as the diﬀerence between two non-negative vectors.
Therefore, let y = y+ − y−, where both y+, y− ≥ 0. Now, if we rewrite eqn. (37) in terms of y+
and y− and change max bT y to min −bT y, it becomes

min −bTy, subject to ATy+ − ATy− + s = c, and y+, y−s ≥ 0

(38)

35

Now, we can express eqn. (38) as

min ¯bT¯y, subject to ¯A¯y = c, ¯y ≥ 0 ,

(39)

where ¯A =

h

AT −AT

i

In

∈ Rn×(2m+n), ¯b =


 ∈ R2m+n, and ¯y =












−b
b
0n


 ∈ R2m+n.


y+
y−
s

Note that ¯A is short-and-fat as 2m + n (cid:29) n and it also has full row-rank. Therefore, eqn. (39) can
be solved using our framework.

D.3 A generalization to low-rank constraint matrices

We will now discuss how to apply randomized preconditioners and iterative solvers to LPs where
A can be any m × n matrix with rank(A) = k (cid:28) min{m, n}, which we assume to be known6. In
addition, we further emphasize that we also assume the set of primal-dual solutions of the LP is
non-empty i.e. there exists at least one feasible point.
First, we brieﬂy discuss the approximate SVD “proto-algorithm” of [16] that will be instrumental
in translating the low-rank LP into our sketching-based framework. The single-iteration “proto-
algorithm” of [16] returns a matrix Z ∈ Rm×(‘+2) with ZTZ = I(‘+2) (‘ ≤ k) such that for some
constant ε0 ≥ 0, the following inequality holds with high probability:7

kA − ZZTAk2 ≤ (1 + (cid:15)0)kA − A‘k2 ,

(40)

where A‘ is the best ‘-rank approximation of A. The computation of Z is dominated by the cost
of multiplying A by a vector and thus, can be computed in O(‘ · nnz(A)) time. By taking ‘ = k,
we have A = Ak which makes the right hand side of eqn. (40) equal to zero. Therefore, letting
eA = ZZTA directly yields A = eA.

Now, as we already have A = eA from eqn. (40) with ‘ = k,

argmin
Ax=b, x≥0

cTx = argmin
eAx=b, x≥0

cTx .

Now, the matrix Z is orthogonal, so it has full column-rank. Therefore, when multiplying from the
left, it keeps the same rank. So rank(ZTA) = rank(ZZTA) = rank( eA) = k.
Next, let x is a feasible point, then

b = eAx = ZZTAx = ZZTb .

(41)

Let F1 = {x : eAx = b, x ≥ 0} and F2 = {x : ZTAx = ZTb, x ≥ 0} be two sets. If u ∈ F1, then

ZTAu = (ZTZ)ZTAu = ZT(ZZTA)u = ZT eAu = ZTb,

i.e. u ∈ F2 .

The last equality above holds as u ∈ F1. Therefore, F1 ⊆ F2. Now, we need to prove F2 ⊆ F1.
For this, let u ∈ F2. Then, eAx = Z(ZTAu) = ZZTb = b, where the last equality follows from
the feasible region induced by ( eA, b) is identical to the
eqn. (41). Therefore, we have F1 = F2 i.e.

6When k is not known in advance, one can eﬃciently estimate it using trace estimation techniques [3, 29].
7Here, (cid:15)0 = 9

‘ + p · pmin{m, n}. We set p = 2 (the minimal allowed value), which suﬃces for our purposes,

√

since the matrix A = A‘ has exact low-rank.

36

feasible region induced by (ZTA, ZTb). Therefore, the LP min

eAx=b, x≥0 cTx can be restated as

min cTx , subject to ZTAx = ZTb , x ≥ 0 .

(42)

Note that we have already shown rank(ZTA) = k (which is (cid:28) n). However, ZTA ∈ R(k+2)×m does
not have full-rank. Therefore, we can use Gaussian elimination to get the k linearly independent
rows of ZTA in O(nk2) time and solve eqn. (42) using our framework. See Section D.4 for the
running time of our algorithms for low-rank constraint matrices.

D.4 Running times for Algorithm 3, Solve, and SolveV

Finally, we discuss the running times of Algorithm 3, Solve, and SolveV.

Lemma D.3. Algorithm 3 called with input matrix AD ∈ Rm×n, failure probability η and iteration
count t has a total time complexity O (cid:0)nnz(A) · log m/η + m3 log m/η + mt + nnz(A) · t(cid:1).

Proof. First, recall that W has log m/η non-zero entries per row and D is a diagonal matrix.
Therefore, ADW can be computed in O(nnz(A) · log m/η) time. Then, computing Q−1/2 via the
SVD of ADW takes O(m3 log m/η) time. We conclude that the overall time complexity to compute
Q−1/2 is O(nnz(A) · log m/η + m3 log m/η).

After computing the preconditioner, each inner iteration requires multiplying ˜z with Q−1/2AD2AT =
Q−1/2(AD)(AD)T . Multiplying a vector by (AD)(AD)T takes O(nnz(A)) time and multiplying a
vector by Q−1/2 takes O(m) time. Therefore, the overall time complexity of Algorithm 3 is

(cid:17)
(cid:16)
nnz(A) · log m/η + m3 log m/η + mt + nnz(A) · t

.

O

We can then immediately derive the time complexity of Solve by combining Lemma D.3 and
Lemma D.1. We conclude that Solve can be implemented by Algorithm 3 with probability at least
1 − η in time

(cid:18)

O

nnz(A) · log m/η + m3 log m/η + m log

σmax(AD)nµ
δ

+ nnz(A) · log

σmax(AD)nµ
δ

(cid:19)

.

(43)

We can similarly derive the time complexity of implementing Solvev by combining Lemma D.3
and Lemma D.2. Observe from eqn. (35) that computing v does not aﬀect the time complexity,
since it is a single matrix-vector product using values already computed by Algorithm 3, except
pre-multiplying the vector (ADW)†(AD2AT ∆˜y − p) by W that takes time O(nnz(A). log m/η)
(assuming nnz(A) ≥ n), which is dominated by the cost of computing (ADW)†. Therefore,
Algorithm 3 combined with eqn. (35) can implement Solvev in time,

(cid:18)

O

nnz(A) · log m/η + m3 log m/η + m log

nµ
δ

+ nnz(A) · log

(cid:19)

.

nµ
δ

(44)

Recall that η is the failure probability of the algorithm.

We note that it is straightforward to obtain an overall time complexity for Algorithms 2 and 1 using
the above results by setting η = O (1/√

n log(µ0/(cid:15))) and applying the union bound.

37

Running time for Low-rank constraint matrices. In Section D.3, the approximate SVD takes
O(k · nnz(A)) time, computing ZTA takes another O(k · nnz(A)) time, and performing the Gaussian
elimination to get k linearly independent rows of ZTA takes O(nk2) time. Therefore, overall it takes
O(k2 ·nnz(A)) time to preprocess the data (assuming nnz(A) ≥ n). Now, Solve can be implemented
time and similarly,
in O
δ
Solvev can be implemented in O (cid:0)nnz(A) · log k/η + k3 log k/η + k log nµ

nnz(A) · log k/η + k3 log k/η + k log σmax(AD)nµ

+ nnz(A) · log σmax(AD)nµ

(cid:1) time.

(cid:16)

(cid:17)

δ

δ + nnz(A) · log nµ

δ

E Experiments

We experimentally validated the key predictions of our results. First, we measure the number of
iterations needed for Algorithm 1 to converge in relation to the number of variables n, and the
precision of the ﬁnal solution (cid:15).

E.1 Generating the random LP

To construct a random LP, we ﬁrst sample x0 ∈ Rn, y0 ∈ Rm, and A ∈ Rm×n, where the entries
of x0 are sampled uniformly from [0, 10] and the entries of y0 and A are sampled uniformly from
[−10, 10]. We then set s0 ∈ Rn by [s0]i = 20 · [x0]−1
0 x0 = 20 and
i
ks0 ◦ x0 − µ01nk2 = 0. The generated constraint matrix A and initial primal-dual point (x0, y0, s0)
along with the assumption that the initial point is primal-dual feasible is enough information to
exactly describe the linear program.

. This guarantees that µ0 = 1/n · sT

E.2 Testing Algorithm 1

We ﬁrst test the predictions of Theorem 2 under a simple instantiation of Solvev. To implement
Solvev, we sample a random vector v ∈ Rn randomly from the unit sphere and rescale it so
that AS−1v = δ, where δ is the accuracy parameter of Solvev. We then use a standard linear
system solver to solve the perturbed system given by Equation 4. Note that this instantiation of
Solvev would not be useful in practice, but it is nevertheless useful to test whether the outer
iteration complexity of Theorem 2 holds empirically. Figures 1 and 2 summarize our results on the
relationship between the number of outer iterations versus n and (cid:15).

We ﬁnd that, in all displayed experiments, primal infeasibility is around 10−10 and does not change
substantially with n or (cid:15). We conclude that the error-adjustment eﬀectively keeps the iterates
primal-feasible, modulo minor numerical errors.

E.3 Testing an iterative instantiation of Solvev (Algorithm 3)

We repeat the above two experiments while using the iterative linear system solver described in
Section D. We note that the iterative solver only requires a few number of iterations (< 20) in the
parameter regime we test. We avoid a more in-depth analysis of the PCG iteration complexity,
as this was already performed in [8]. Overall, we ﬁnd that there is no notable diﬀerence between
using the perturbed Solvev method or the iterative instantiation. Results of our experiments are
summarized in Figures 3 and 4 below.

38

Figure 3: This graph demonstrates that the linear
relationship between the number of iterations and
√
n continues to hold when using the iterative in-
stantiation of Solvev. The line shows the median
number of iterations and the intervals designate
the 10% and 90% quantiles out of 60 repetitions.
Other parameters are m = 20, (cid:15) = 0.1, δ = 0.001,
and (sketch size) w = 60.

Figure 4: This graph demonstrates that the lin-
ear relationship between the number of iterations
and log(1/(cid:15)) continues to hold when using the it-
erative instantiation of Solvev. The line shows
the median number of iterations and the intervals
designate the 10% and 90% quantiles out of 60
repetitions. Other parameters are m = 30, n = 70,
δ((cid:15)) = (cid:15), and (sketch size) w = 60.

39

