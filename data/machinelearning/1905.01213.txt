Matlab vs. OpenCV:
A Comparative Study of Different Machine Learning Algorithms

(cid:73)

Ahmed A. Elsayeda, Waleed A. Yousefb,∗

aB.Sc., Senior Data Scientist, TeraData, Egypt.
bPh.D., Computer Science Department, Faculty of Computers and Information, Helwan University, Egypt.
Human Computer Interaction Laboratory (HCI Lab.), Egypt.

9
1
0
2

g
u
A
4
1

]

G
L
.
s
c
[

4
v
3
1
2
1
0
.
5
0
9
1
:
v
i
X
r
a

Abstract

Scientiﬁc Computing relies on executing computer algorithms coded in some programming languages. Given a particular
available hardware, algorithms speed is a crucial factor. There are many scientiﬁc computing environments used to code
such algorithms. Matlab is one of the most tremendously successful and widespread scientiﬁc computing environments
that is rich of toolboxes, libraries, and data visualization tools. OpenCV is a (C++)-based library written primarily for Com-
puter Vision and its related areas. This paper presents a comparative study using 20 different real datasets to compare the
speed of Matlab and OpenCV for some Machine Learning algorithms. Although Matlab is more convenient in developing
and data presentation, OpenCV is much faster in execution, where the speed ratio reaches more than 80 in some cases.
The best of two worlds can be achieved by exploring using Matlab or similar environments to select the most successful
algorithm; then, implementing the selected algorithm using OpenCV or similar environments to gain a speed factor.

Keywords: Matlab, OpenCV, Machine Learning.

1. Introduction

1.1. Background

Scientiﬁc Computing relies on executing computer algorithms written in some programming languages. Given a par-
ticular available hardware, algorithms speed is a crucial factor. There are many scientiﬁc computing environments used
to code such algorithms. Matlab is a high-level programming language for scientiﬁc computing (MATLAB, 2011). It is
an array-based programming language, where an array is the basic data element. Matlab has an extensive scientiﬁc li-
brary and toolboxes across different areas of science, in addition to its data visualization capabilities and functionalities.
OpenCV (Bradski, 2000) on the other hand, developed by Intel and now supported by Willow Garage (WillowGarage, 2008),
is a free library release under BSD license. It aims at providing a well optimized, well tested, and open-source (C++)-based
implementation for computer vision algorithms.

Code developing, algorithm implementation, data presentation, and other scientiﬁc computing activities are much
easier and more convenient in Matlab or similar environments. However, algorithms developed in native languages, e.g.,
C++, execute much faster. In research projects that deliver ﬁnal products, in particular machine learning and model selec-
tion based projects, one usually tries several models and run hundreds, or even thousands, of experiments before settling
on the ﬁnal model. It is much more convenient and time saving then to leverage an environment with a rich toolboxes and
libraries as Matlab. However, the ﬁnal selected model has to execute fast for satisfactory performance. This may require
recoding the selected model anew using a native language, e.g., C++.

1.2. Motivation

The motivation behind the present article was a real life project for developing a Computer Aided Detection (CAD) to
detect breast cancer in digital mammograms (Yousef et al., 2010; Abdel Razek et al., 2013, 2012; Yousef, 2017). Using Matlab
was extremely efﬁcient in trying dozens of machine learning and image processing algorithms. When the project reached
the point of deploying the selected models to a ﬁnal product it was necessary to write some of those selected algorithms in
plain C to gain a speed factor. Moreover, sometimes it was necessary in the developing phase, before deployment, to code
some of the machine learning algorithms in plain C, to be able to run thousands of experiments in reasonable time.

We spent signiﬁcant amount of time during the past years developing rigorous methods for classiﬁer assessment in-
cluding nonparametric estimation from one dataset (Yousef et al., 2004; Yousef, 2019a), assessment in terms of Partial Area

(cid:73)

This manuscript was initially composed in 2011 as part of a research pursued that time. This paper is currently under consideration in Pattern

Recognition Letters.

∗

Corresponding Author
Email addresses: AhmedAnis03@gmail.com (Ahmed A. Elsayed), wyousef@GWU.edu, wyousef@fci.helwan.edu.eg (Waleed A. Yousef)

1

 
 
 
 
 
 
Data sets

n

p

Connectionist Bench
Breast Tissue
Blood Transfusion
Glass Identiﬁcation
Haberman’s Survival
Image segmentation
Iris
MAGIC Gamma
shuttle
Letter Recognition
Arcene
Madelon
Pen Based Recog.
SPECT Heart
SPECTF Heart
Arcene Test
Madelon Test
Pen Based Recog. Test
SPECT Heart Test
SPECTTF Heart Test

208
106
748
214
306
210
150
19020
43500
20000
100
2000
7494
80
80
167
1800
3489
187
187

60
9
4
9
3
19
4
10
9
17
10000
500
16
22
44
10000
500
16
22
44

C

2
4
2
6
2
7
3
2
7
26
2
2
9
2
2
2
2
9
2
2

D1
D2
D3
D4
D5
D6
D7
D8
D9
D10
D11
D12
D13
D14
D15
D16
D17
D18
D19
D20

Table 1: 20 UCI real datasets used to compare performance; n, p, C are #observations, #features, and #Classes respectively.

Under the ROC Curve (PAUC) (Yousef, 2013), uncertainty estimation of the performance (Yousef et al., 2005), uncertainty
estimation from two independent sets (Yousef et al., 2006; Chen et al., 2012a), uncertainty estimation using cross validation
estimator (Yousef, 2019b,c; Yousef and Chen, 2009), and the best practices for classiﬁer design and assessment Chen et al.
(2012b); Shi et al. (2010); Yousef and Kundu (2014); Yousef (2019d), among many others.

However, we learned that all of these successful methodologies may be very difﬁcult to apply if the learning algorithm
is computationally very slow, the matter that compromises their utility. Therefore, it is not only important to have a fast
algorithm for its own sake but also for the sake of accuracy estimation and assessment, which is mandatory for sound
conclusion and results. Yet, the speed of these learning algorithms is not determined only by their design but also by their
computing environment.

In retrospect, from the introduction above, the present article complements our theoretical work on assessment and
provides a very practical comparative study between Matlab and OpenCV using 20 real datasets, to compare their execution
times for different machine learning algorithms. It is impressive that in some cases OpenCV is 80 times faster than Matlab.

1.3. Manuscript Roadmap

In Section 2 we describe the settings of this study including hardware, software, datasets, and the machine learning

algorithms used for comparison. Section 3 presents the results and discussion.

2. Methods

In this section we introduce the environment used in conducting our comparative study. We describe the hardware,

software, datasets, and the machine learning algorithms used as a basis for comparison.

We ran all experiments on an Intel core 2 duo P7450 machine, with 3GB RAM, and Ubuntu 11.04 32-bit Operating
System. We used Matlab version 7.12.0.635 (R2011a), and OpenCV C++ version 2.1 without TBB library (Intel, 2011). This
is to ensure that we use sequential processing as Matlab, to establish similar environment conditions. Code was compiled
using gcc compiler version 4.5.2.

We used 20 different real datasets, obtained from UCI Machine Learning Repository (Newman and Asuncion, 2007). All

datasets have multivariate quantitative attributes with no missing values. Table 1 summarizes these datasets.

Below, we list the machine learning algorithms used as a basis of comparison. Full description of those algorithms is
not the scope of the present article and can be found elsewhere in the literature (e.g., Hastie et al., 2009). However, we
emphasize how we call these algorithms in each environment (Matlab and OpenCV) to establish common settings for
comparison. It is important to note that some options available in Matlab are not available in OpenCV. However, we set
the options and call these algorithms in both environment in a way that makes sure they both conform and have common
settings; (see Appendix 5.)

2

2.1. Classiﬁcation and Regression Trees (CART)

CART (Breiman et al., 1984) is a nonparametric technique that can select the most important variables to predict the
output. A tree is constructed from some questions that recursively split the learning sample into parts till it hits a stopping
condition.

We use the classiﬁcation tree by disabling pruning, surrogate, cross validation options, so that each leaf node can hold
one observation. It is clear that this is an overﬁtting training; however, we are not interested in building a predictive model
here. Rather, we are interested in comparing the execution speed. The impurity measurement used is GINI index. In
Matlab, we set the merging leaves option mergeleaves to off and in OpenCV we set the maximum depth per tree as max
integer value for signed numbers.

2.2. Naive Bayes

It stems from Bayes’ theorem, by assuming that features are independent. Naive Bayes is a very simple classiﬁer; how-
ever, it outperforms more sophisticated classiﬁers in some problems. It is more efﬁcient when the dimensionality of the
features is high.

There is no additional properties to set for Naive Bayes. We just pass the datasets to both Matlab and OpenCV.

2.3. Boosting

Boosting (Freund et al., 2004) is one of the most efﬁcient classiﬁcation algorithms over the past few years. It is an
algorithm that combine weak classiﬁers to produce a strong one. It was originally designed for classiﬁcation problems but
it extends to cover regression problems as well; see Hastie et al. (2009); Duda et al. (2001).

We used a CART, with 10 observations per node and GINI index for impurity measurement, as a weak classiﬁer. The
algorithm used for boosting is AdaBoostM1, which is one of the most popular boosting algorithms. Number of ensemble
learning cycles is set to 100.

2.4. Random Forest

Random Forest (Breiman, 2001) is an ensemble classiﬁer, comprised of many classiﬁcation trees built on different boot-
strap replications from the original dataset (Hastie et al., 2009). To classify an object, each tree gives a vote for a class; the
object is classiﬁed as the majority vote of the trees.

We used 20 trees in the forest, each tree with minimum of ten observations per node. In both environments, we set
(p), where p is the
calculating variable importance to on, and the number of splitting features m to the default value
number of dimensions. In Matlab, the trees grow without pruning and we set mergeleaves to off. In OpenCV we set the
maximum depth per tree as max integer value for signed numbers.

(cid:112)

2.5. K-Nearest Neighbor (KNN)

KNN is one of the simplest, yet most efﬁcient, classiﬁers. It classiﬁes the object based on the majority vote of the closest

observations (neighbors).

The number of neighbors K is set to one. We used KD search tree as our searching algorithm. The distance measure is

Euclidean Distance.

3. Results and Conclusion

To compare the speed of Matlab and OpenCV for a particular machine learning algorithm, we run that algorithm on one
of the datasets in Figure 1, using the settings and calling parameters of Appendix 5. For each dataset, we run the algorithm
1000 times and take the average of the execution times. Averaging over 1000 experiments is more than necessary since
convergence is reached after few hundreds. Figure 1 is a plot for the log of the execution times ratio between Matlab and
OpenCV. Each bar represents an algorithm tested on a dataset. Due to limitation of memory size and CPU performance,
not all algorithms are tested on all datasets.

It is obvious that OpenCV outperforms Matlab across all experiments (with a speed factor up to 80 in some cases),
except for the KNN algorithm only on Pen Based recognition dataset. It seems that the reason is a combination of number
of dimensionality, sample size, and the use of training set. For instance, KNN on datasets D16 and D17 produced a log
time ratio of 0.8 and 0.9 respectively (see Figure). However, when we used only 16 dimensions out of the 10,000 (or 500)
dimensions of D16 (or D17, the ratio was reduced to 0.5 (or 0.06), which means that Matlab became closer to OpenCV.
However, two other factors affect these results: the training set and the number of observations. Full understanding for the
relative behavior of Matlab and OpenCV in KNN requires more future investigation.

Matlab is a tremendously successful scientiﬁc computing environment that helps in developing code in an easy and
lucid way. However, when the execution time is an important factor one may need to sacriﬁce development convenience
and write algorithms in a more native way, e.g., by using C, C++, OpenCV, or any similar programming languages.

It is always helpful to distinguish between two purposes of scientiﬁc computing: research and product development.
For research, one may rely heavily on Matlab, or any similar library-based environment, to experiment new approaches

3

Figure 1: The Log of the execution time ratio (log(T1/T2)) between Matlab and OpenCV for different Machine Learning algorithms. The X-axis is real
dataset label as illustrated in Table 1.

or algorithms. Its rich library and toolboxes, along with data analysis and visualization capabilities, help in rapid research
and experimentation and saves the time of implementation from scratch. On the other hand, one of the major important
features of a ﬁnal product is its execution speed that can be achieved by implementation in a native programming language.
A prudent trade off between both approaches is achieved by experimenting using a rich library-based environment; then
after settling on a ﬁnal algorithm or method one can hard-code and deploy it using a more native language.

4. Acknowledgement

The ﬁrst author is grateful to Dr. Ayman Attia, of Helwan University, for fruitful discussions and Omar Elshenawy for

technical assistance and early manuscript review.

5. Appendix: Parameters and Functions Call

5.1. Classiﬁcation trees

Matlab

1

c l a s s r e g t r e e ( Data , Target ,

’method ’ ,

’ c l a s s i f i c a t i o n ’ ,

’ minparent ’ , 1 ,

’ prune ’ ,

’ o f f ’ ,

’ mergeleaves ’ ,

’ o f f ’ ) ;

OpenCV

1

2

3

CvDTreeParams Params = CvDTreeParams ( Int_Max , 1 , 0 , false , 100 , 0 , false , false , NULL ) ;
CvDTree * C_Tree = new CvDTree ;
C_Tree - > train ( Data , CV_ROW_SAMPLE , responses , NULL , NULL , var_type , NULL , Params ) ;

5.2. Naive Bayes

Matlab

1 NaiveBayes . f i t ( Data , Target ) ;

OpenCV

1

2

C v N o r m a l B a y e s C l a s s i f i e r * Bays = new C v N o r m a l B a y e s C l a s s i f i e r ;
Bays - > train ( data , responses , 0 , 0) ;

5.3. Boosting

Matlab

1

2

Tree = C l a s s i f i c a t i o n T r e e . template ( ’ MinParent ’ , 10) ;
ens = fitensemble ( Data , Target ,

’AdaBoostM1 ’ , 100 , Tree ,

’ type ’ ,

’ c l a s s i f i c a t i o n ’ ) ;

OpenCV

4

1

2

3

4

5

6

7

8

9

CvBoost * Boost ;
CvBoostParams Params = CvBoostParams ( Boost - > REAL , 100 , 0 , Int_Max , false , NULL ) ;
Params . max_ categories =100;
Params . mi n _sa mple_count =10;
Params . r e gr e s s i o n _ a c c u r a cy =0;
Params . spli t_criteria = Boost - > GINI ;
Params . t r u n c a t e _ p r u n e d _ t r e e = false ;
Params . use_1se_rule = false ;
Boost - > train ( data , CV_ROW_SAMPLE , responses , 0 , 0 , var_type , 0 , Params , false ) ;

5.4. Random Forest
Matlab

1

TreeBagger ( 2 0 , Data , Target ,

’OOBVarImp ’ ,

’on ’ ,

’Method ’ ,

’ c l a s s i f i c a t i o n ’ ,

’ mergeleaves ’ ,

’ o f f ’ ,

’ prune ’ ,

’ o f f ’ ,

’

NVarToSample ’ ,

’ A l l ’ ) ;

OpenCV

1

2

3

4

CvRTrees * RTrees ;
CvRTParams Params ;
Params = CvRTParams ( int_Max , 10 , 0 , false , 100 , 0 , true , 0 , 20 , 0 , CV_TERMCRIT_ITER ) ;
RTrees - > train ( Data , CV_ROW_SAMPLE , Target , 0 , 0 , var_type , 0 , Params ) ;

5.5. KNN
Matlab

1

2

Tree = createns ( Data ,
IDX = knnsearch ( Tree , TestData ,

’NSMethod ’ ,

’ kdtree ’ ) ;

’K ’ , 1) ;

OpenCV

1

2

3

4

5

6

7

8

CvKNearest * Knn = new CvKNearest ;
Knn - > train ( data , responses , 0 , false , 1) ;
CvMat * Sample = cvCreateMat (1 , Data - > cols , CV_32F ) ;
for ( j =0; j < Data - > rows ; j ++) {

for ( k =0; k < Data - > cols ; k ++)

cvmSet ( Sample , 0 , k , cvmGet ( Data , j , k ) ) ;

Knn - > find_nearest ( TempData , 1 , Results , 0 , 0 , 0) ;

}

References

Abdel Razek, N.M., Yousef, W.A., Mustafa, W.A., 2012. Microcalciﬁcation detection with and without prototype cad system (libcad): a comparative study,
in: European Society of Radiology, ECR 2012 / C-1063. pp. 1–15. URL: https://doi.org/10.1594/ecr2012/C-1063, doi:10.1594/ecr2012/
C-1063.

Abdel Razek, N.M., Yousef, W.A., Mustafa, W.A., 2013. Microcalciﬁcation Detection With and Without Cad System (LIBCAD): a Comparative study. The

Egyptian Journal of Radiology and Nuclear Medicine 44, 397–404.

Bradski, G., 2000. The OpenCV Library. Dr. Dobb’s Journal of Software Tools .
Breiman, L., 2001.
1010933404324.

Random forests. Machine Learning 45, 5–32. URL: https://doi.org/10.1023/a:1010933404324, doi:10.1023/a:

Breiman, L., Friedman, J., Olshen, R., Stone, C., 1984. Classiﬁcation and regression trees. The Wadsworth statistics/probability series, Wadsworth Inter-

national Group, Belmont, Calif.

Chen, W., Gallas, B.D., Yousef, W.A., 2012a. Classiﬁer Variability: Accounting for Training and testing. Pattern Recognition 45, 2661–2671. URL: https:

//doi.org/10.1016/j.patcog.2011.12.024, doi:10.1016/j.patcog.2011.12.024.

Chen, W., Yousef, W.A., Gallas, B.D., Hsu, E.R., Lababidi, S., Tang, R., Pennello, G.A., Symmans, W.F., Pusztai, L., 2012b. Uncertainty Estimation With a Finite
Dataset in the Assessment of Classiﬁcation models. Computational Statistics & Data Analysis 56, 1016–1027. doi:10.1016/j.csda.2011.05.024.

Duda, R.O., Hart, P.E., Stork, D.G., 2001. Pattern classiﬁcation. 2nd ed., Wiley, New York.
Freund, Y., Iyer, R., Schapire, R.E., Singer, Y., 2004. An Efﬁcient Boosting Algorithm for Combining preferences. Journal of Machine Learning Research 4,

933–969.

Hastie, T., Tibshirani, R., Friedman, J.H., 2009. The elements of statistical learning: data mining, inference, and prediction. 2nd ed., Springer, New York.
Intel, 2011. IntelÂ˝o threading building blocks. URL: http://threadingbuildingblocks.org/.
MATLAB, 2011. version 7.12.0.635 (R2011a). The MathWorks Inc., Natick, Massachusetts.
Newman, D.J., Asuncion, A., 2007. UCI Machine Learning Repository. University of California, Irvine, Dept. of Information and Computer Sciences .
Shi, L., Campbell, G., Jones, W.D., Campagne, F., Wen, Z., Walker, S.J., Su, Z., Chu, T.M., Goodsaid, F.M., Pusztai, L., Shaughnessy Jr., J.D., Oberthuer, A.,
Thomas, R.S., Paules, R.S., Fielden, M., Barlogie, B., Chen, W., Du, P., Fischer, M., Furlanello, C., Gallas, B.D., Ge, X., Megherbi, D.B., Symmans, W.F.,
Wang, M.D., Zhang, J., Bitter, H., Brors, B., Bushel, P.R., Bylesjo, M., Chen, M., Cheng, J., Chou, J., Davison, T.S., Delorenzi, M., Deng, Y., Devanarayan,
V., Dix, D.J., Dopazo, J., Dorff, K.C., Elloumi, F., Fan, J., Fan, S., Fan, X., Fang, H., Gonzaludo, N., Hess, K.R., Hong, H., Huan, J., Irizarry, R.A., Judson, R.,
Juraeva, D., Lababidi, S., Lambert, C.G., Li, L., Li, Y., Li, Z., Lin, S.M., Liu, G., Lobenhofer, E.K., Luo, J., Luo, W., McCall, M.N., Nikolsky, Y., Pennello, G.A.,
Perkins, R.G., Philip, R., Popovici, V., Price, N.D., Qian, F., Scherer, A., Shi, T., Shi, W., Sung, J., Thierry-Mieg, D., Thierry-Mieg, J., Thodima, V., Trygg,
J., Vishnuvajjala, L., Wang, S.J., Wu, J., Wu, Y., Xie, Q., Yousef, W.A., Zhang, L., Zhang, X., Zhong, S., Zhou, Y., Zhu, S., Arasappan, D., Bao, W., Lucas,
A.B., Berthold, F., Brennan, R.J., Buness, A., Catalano, J.G., Chang, C., Chen, R., Cheng, Y., Cui, J., Czika, W., Demichelis, F., Deng, X., Dosymbekov, D.,
Eils, R., Feng, Y., Fostel, J., Fulmer-Smentek, S., Fuscoe, J.C., Gatto, L., Ge, W., Goldstein, D.R., Guo, L., Halbert, D.N., Han, J., Harris, S.C., Hatzis, C.,
Herman, D., Huang, J., Jensen, R.V., Jiang, R., Johnson, C.D., Jurman, G., Kahlert, Y., Khuder, S.A., Kohl, M., Li, J., Li, M., Li, Q.Z., Li, S., Liu, J., Liu, Y., Liu,
Z., Meng, L., Madera, M., Martinez-Murillo, F., Medina, I., Meehan, J., Miclaus, K., Mofﬁtt, R.A., Montaner, D., Mukherjee, P., Mulligan, G.J., Neville, P.,
Nikolskaya, T., Ning, B., Page, G.P., Parker, J., Parry, R.M., Peng, X., Peterson, R.L., Phan, J.H., Quanz, B., Ren, Y., Riccadonna, S., Roter, A.H., Samuelson,
F.W., Schumacher, M.M., Shambaugh, J.D., Shi, Q., Shippy, R., Si, S., Smalter, A., Sotiriou, C., Soukup, M., Staedtler, F., Steiner, G., Stokes, T.H., Sun,
Q., Tan, P.Y., Tang, R., Tezak, Z., Thorn, B., Tsyganova, M., Turpaz, Y., Vega, S.C., Visintainer, R., von Frese, J., Wang, C., Wang, E., Wang, J., Wang, W.,
Westermann, F., Willey, J.C., Woods, M., Wu, S., Xiao, N., Xu, J., Xu, L., Yang, L., Zeng, X., Zhang, M., Zhao, C., Puri, R.K., Scherf, U., Tong, W., Wolﬁnger,
R.D., 2010. The microarray quality control (maqc)-ii study of common practices for the development and validation of microarray-based predictive
models. Nat Biotechnol 28, 827–838.

5

WillowGarage, 2008 URL: http://www.willowgarage.com/.
Yousef, W.A., 2013. Assessing Classiﬁers in Terms of the Partial Area Under the Roc curve. Computational Statistics & Data Analysis 64, 51–70. URL:

https://doi.org/10.1016/j.csda.2013.02.032.

Yousef, W.A., 2017. Method and system for image analysis to detect cancer. URL: http://appft.uspto.gov/netacgi/nph-Parser?Sect1=

PTO1&Sect2=HITOFF&d=PG01&p=1&u=%2Fnetahtml%2FPTO%2Fsrchnum.html&r=1&f=G&l=50&s1=%2220190019291%22.PGNR.&OS=DN/
20190019291&RS=DN/20190019291. patent pending, US 62/531,219.

Yousef, W.A., 2019a. AUC: nonparametric estiamtors and their smoothness. arXiv preprint arXiv:1907.12851 .
Yousef, W.A., 2019b.
arXiv:1908.00325 .

Estimating the standard error of cross-validation-based estimators of classiﬁcation rules performance.

arXiv preprint

Yousef, W.A., 2019c. A leisurely look at versions and variants of the cross validation estimator. arXiv preprint arXiv:1907.13413 .
Yousef, W.A., 2019d. Prudence when assuming normality: an advice for machine learning practitioners. arXiv preprint arXiv:1907.12852 .
Yousef, W.A., Chen, W., 2009. Estimating Cross-Validation Variability, in: Proceedings of the 2009 Joint Statistical Meeting, Section on Statistics in Epi-

demiology., pp. 3318–3326.

Yousef, W.a., Kundu, S., 2014. Learning Algorithms May Perform Worse With Increasing Training Set Size: Algorithm-Data incompatibility. Computational

Statistics & Data Analysis 74, 181–197. URL: https://doi.org/10.1016/j.csda.2013.05.021, doi:10.1016/j.csda.2013.05.021.

Yousef, W.a., Mustafa, W.a., Ali, A.a., Abdelrazek, N.a., Farrag, A.M., 2010. On Detecting Abnormalities in Digital mammography. 2010 IEEE 39th Applied
Imagery Pattern Recognition Workshop (AIPR) , 1–7URL: https://doi.org/10.1109/AIPR.2010.5759684, doi:10.1109/AIPR.2010.5759684.
Yousef, W.A., Wagner, R.F., Loew, M.H., 2004. Comparison of Non-Parametric Methods for Assessing Classiﬁer Performance in Terms of {ROC} Parameters,

in: Applied Imagery Pattern Recognition Workshop, 2004. Proceedings. 33rd; IEEE Computer Society, pp. 190–195.

Yousef, W.A., Wagner, R.F., Loew, M.H., 2005. Estimating the Uncertainty in the Estimated Mean Area Under the {ROC} Curve of a Classiﬁer. Pattern

Recognition Letters 26, 2600–2610.

Yousef, W.A., Wagner, R.F., Loew, M.H., 2006. Assessing Classiﬁers From Two Independent Data Sets Using {ROC} Analysis: a Nonparametric Approach.

Pattern Analysis and Machine Intelligence, IEEE Transactions on 28, 1809–1817.

6

