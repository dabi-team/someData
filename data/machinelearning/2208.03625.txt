Noname manuscript No.
(will be inserted by the editor)

Parabolic Relaxation for Quadratically-constrained
Quadratic Programming – Part II: Theoretical and
Computational Results

Ramtin Madani · Mersedeh Ashraphijuo ·
Mohsen Kheirandishfard · Alper Atamt ¨urk

2
2
0
2

g
u
A
7

]

C
O
.
h
t
a
m

[

1
v
5
2
6
3
0
.
8
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract In the ﬁrst part of this work [32], we introduce a convex parabolic re-
laxation for quadratically-constrained quadratic programs, along with a sequential
penalized parabolic relaxation algorithm to recover near-optimal feasible solutions.
In this second part, we show that starting from a feasible solution or a near-feasible
solution satisfying certain regularity conditions, the sequential penalized parabolic
relaxation algorithm convergences to a point which satisﬁes Karush–Kuhn–Tucker
optimality conditions. Next, we present numerical experiments on benchmark non-
convex QCQP problems as well as large-scale instances of system identiﬁcation prob-
lem demonstrating the efﬁciency of the proposed approach.

Keywords Parabolic relaxation · Convex relaxation · Quadratically-constrained
quadratic programming · Non-convex optimization

PACS 87.55.de

Mathematics Subject Classiﬁcation (2010) 65K05 · 90-08 · 90C26 · 90C22

This work is in part supported by the NSF Award 1809454. Alper Atamt¨urk is supported, in part, by grant
FA9550-10-1-0168 from the Ofﬁce of the Assistant Secretary of Defense for Research & Engineering and
the NSF Award 1807260.

Ramtin Madani · Mersedeh Ashraphijuo
Department of Electrical Engineering, University of Texas Arlington
E-mail: ramtin.madani@uta.edu, mersedeh.ashraphijuo@uta.edu

Mohsen Kheirandishfard
Cognitiv
E-mail: mohsen.kheirandishfard@gmail.com

Alper Atamt¨urk
Department of Industrial Engineering & Operations Research, University of California Berkeley
E-mail: atamturk@berkeley.edu

 
 
 
 
 
 
2

1 Introduction

Ramtin Madani et al.

As discussed in the ﬁrst part of this work, [32], we study the problem of minimizing
a quadratic objective function over a feasible set that is also deﬁned by quadratic
functions, i.e., quadratically-constrained quadratic programming (QCQP). A wide
variety of convex relaxations and search methods have been proposed in the literature
to tackle the class of QCQP optimization problems. The most notable among them is
the semideﬁnite programming (SDP) relaxation.

Scalability is the main challenge to the real-world applications of SDP. Recently,
there have been a signiﬁcant attention towards addressing this issue [33]. One ap-
proach for improvement is developing methods to exploit problem-speciﬁc structures,
e.g., sparsity and symmetry. For certain structures, there are techniques that allow ex-
pressing a large semideﬁnite constraint as a set of smaller semideﬁnite constraints,
with increased computational efﬁciency [1, 22]. Such results are leveraged in opti-
mization and control theory problems to improve efﬁciency [18, 19, 26, 34]. By ex-
ploiting the underlying sparsity properties of the problem, and using decomposition,
[55, 56] propose scalable algorithms to obtain structured feedback gains to stabilize
large-scale systems. Also, the sparsity structure is exploited for polynomial optimiza-
tion problems in [29, 50]. Another common structure in applications of SDP is sym-
metry. To this end, the method of symmetry reduction proposed in [11, 21, 40, 47].
Similar to symmetry reduction, facial reduction leverages a degeneracy structure ad
proposed in [7, 12, 28, 38, 40].

Another approach is developing methods for producing low-rank solutions. In
some cases where there is indeed a low-rank optimal solution to the problem [5, 6,
30, 37], and in other cases acceptable low-rank feasible points to the problem are suf-
ﬁcient [10, 27, 43]. One direction includes the Burer-Monteiro algorithm [9], quasi-
Newton algorithms for unconstrained optimization problems [31], Riemannian opti-
mization [8, 24], and coordinate descent [13, 46]. Another direction includes Frank-
Wolfe based methods [17] such as Hazan’s algorithm [23] and the numerical method
in [53].

First-order methods scale to signiﬁcantly larger problem sizes, while trading off
the accuracy of resulting output, e.g., Alternating Direction Method of Multipliers
(ADMM) [36], and the augmented Lagrangian-based methods [52, 54].

And ﬁnally, another approach that is most relevant to our work is a set of con-
servative algorithms that lead to guaranteed feasible points which can be suboptimal,
as opposed to the ones which produce solutions that may violate some constraints.
They are special-purpose algorithms leveraging domain knowledge associated with
the target application, e.g., in neural networks [16, 41, 42], or general-purpose ones
that apply broadly across application domains, e.g., DSOS and SDSOS algorithms
[3] and their adaptive improvements where a sequence of linear programs (LPs) or
second order cone programs (SOCPs) are solved, e.g., by iterative change of basis [2]
or column generation [4].

In the ﬁrst part of this work, we introduce parabolic relaxation, an alternative
convex relaxation and discuss its basic properties. In this second part, our ﬁrst result
states that given a feasible initial point satisfying the linear independence constraint
qualiﬁcation (LICQ) condition, the penalized relaxation produces a unique solution

Title Suppressed Due to Excessive Length

3

which is feasible for the original QCQP and its objective value is not worse than
that of the initial point. Our second result states that if the initial point is infeasi-
ble, but sufﬁciently close to the feasible set, and satisﬁes a generalized LICQ condi-
tion, then there exists a unique optimal solution to the penalized convex relaxation
problem which is feasible for the original QCQP. Motivated by these results on con-
structing feasible points, we propose a sequential procedure for solving non-convex
QCQPs with convergence guarantees to a point which satisﬁes Karush–Kuhn–Tucker
(KKT) optimality conditions. Finally, we demonstrate its performance on benchmark
instances from QPLIB library [20] as well as on large-scale system identiﬁcation
problems.

1.1 Notations

For a given vector a and a matrix A, the symbols ai and Aij, respectively, indicate
the ith element of a and the (i, j)th element of A. The symbols R, Rn, and Rn×m
denote the sets of real scalars, real vectors of size n, and real matrices of size n × m,
respectively. The set of n × n real symmetric matrices is shown by Sn. The nota-
tions Dn and S+
n , respectively, represent the sets of diagonally-dominant and positive
semideﬁnite members of Sn. The n × n identity matrix is denoted by I n. The origins
of Rn and Rn×m are denoted by 0n and 0n×m, respectively, while 1n denotes the
all one n-dimensional vector. For a pair of n × n symmetric matrices (A, B), the
notation A (cid:23) B means that A − B is positive semideﬁnite, whereas A (cid:31) B means
that A − B is positive deﬁnite. Given a matrix A ∈ Rm×n, the notation σmin(A)
represents the minimum singular value of A. The symbols (cid:104)· , ·(cid:105) and (cid:107) · (cid:107)F denote
the Frobenius inner product and norm of matrices, respectively. Kronecker product
of matrices is represented by ⊗. The notations (cid:107) · (cid:107)1 and (cid:107) · (cid:107)2 denote the 1- and
2-norms of vectors/matrices, respectively. The superscript (·)(cid:62) and the symbol tr{·}
represent the transpose and trace operators, respectively. vec{·} denotes the vector-
ization operator. The notation | · | represents either the absolute value operator or
cardinality of a set, depending on the context. For every x ∈ Rn, the notation [x]
represents an n × n diagonal matrix with the elements of x. The notations ∇f (a)
and ∇2f (a), respectively, represent the gradient and Hessian of the function f at a
point a. The boundary of a subset S of a topological space X is deﬁned as the set of
points in the closure of S not belonging to the interior of S.

2 Theoretical Results

This section formally states the results on the objective penalization and the conver-
gence of the proposed sequential algorithm. We ﬁrst need to deﬁne the notions of
feasibility distance, quasi-binding constraints, generalized LICQ, singularity func-
tion, and pencil norm.

4

2.1 Preliminaries

Ramtin Madani et al.

As discussed in [32], we consider the problem of ﬁnding a matrix Y ∈ Rn×m that
minimizes a quadratic objective function subject to a set of equality constraints E and
inequality constraints I, i.e.,

minimize
Y ∈Rn×m

subject to

q0 (Y )

qk (Y ) = 0,
qk (Y ) ≤ 0,

(1a)

(1b)

(1c)

k ∈ E,

k ∈ I,

where each function qk : Rn×m → R is deﬁned as qk (Y ) (cid:44) tr{Y (cid:62)Ak Y } +
2 tr{B(cid:62)
k Y } + ck , and {Ak ∈ Sn, Bk ∈ Rn×m, ck ∈ R}k∈{0}∪E∪I are given
matrices/scalars. We propose the following penalized convex relaxation for problem
(1a) – (1c):

minimize
Y ∈Rn×m
X∈Sn

subject to

¯q0 (Y, X) + η × tr{X − 2 ˇY Y (cid:62) + ˇY ˇY (cid:62)}

¯qk (Y, X) = 0,
¯qk (Y, X) ≤ 0,
Xii + Xjj − 2Xij ≥ (cid:107)(ei − ej)(cid:62)Y (cid:107)2
2,
Xii + Xjj + 2Xij ≥ (cid:107)(ei + ej)(cid:62)Y (cid:107)2
2,

k ∈ E,

k ∈ I,

i, j ∈ N ,

i, j ∈ N .

(2a)

(2b)

(2c)

(2d)

(2e)

where ¯qk (Y, X) (cid:44) tr{Ak X} + 2 tr{B(cid:62)
k Y } + ck for each k ∈ {0} ∪ E ∪ I,
ˇY ∈ Rn×m is an initial point, and η > 0 is a constant. One can adopt a sequential
framework to obtain feasible and near optimal points for problem (1a) – (1c). This
procedure is detailed in Algorithm 1.

The following deﬁnition offers a distance measure from the feasible set of this

problem.

Deﬁnition 1 (Feasibility Distance) Denote F ⊆ Rn×m as the feasible set for prob-
lem (1a) – (1c). The feasibility distance function dF : Rn×m → R is deﬁned as:

dF ( ˇY ) (cid:44) min{(cid:107)Y − ˇY (cid:107)F | Y ∈ F}.

(3)

Given a feasible point Y ∈ F, one can deﬁne binding constraints in order to form
the Jacobian matrix at point Y . The next deﬁnition extends this to points that are not
necessarily feasible.

Deﬁnition 2 (Quasi-binding Constraints) For every ˇY ∈ Rn×m, deﬁne the set of
quasi-binding constraints as:

B ˇY

(cid:44) E ∪ {k ∈ I (cid:12)

(cid:12) ˜qk( ˇY ) ≥ 0},

(4)

Title Suppressed Due to Excessive Length

5

Algorithm 1 Sequential Penalized Parabolic Relaxation.
Input: Y (0) ∈ Rn×m, η > 0 and l := 0
1: repeat
2:
3:

∗

l := l + 1
solve the penalized convex problem (2a) – (2e) with ˇY = Y (l−1) to obtain
X(cid:1)
Y (l) :=

(cid:0) ∗
Y ,

∗
Y
4:
5: until stopping criterion is met.
Output: Y (l)

where, for each k ∈ {0} ∪ E ∪ I, the expanded function ˜qk : Rn×m → R is deﬁned
as:

˜qk( ˇY ) (cid:44) qk( ˇY ) + (cid:107)∇qk( ˇY )(cid:107)F dF ( ˇY ) + (cid:107)Ak(cid:107)2 dF ( ˇY )2.
(5)
Observe that if ˇY is feasible, then B ˇY is the set of binding constraints and ˜qk( ˇY ) =
qk( ˇY ), for every k ∈ {0} ∪ E ∪ I.

The next deﬁnition offers a generalization for the notion of LICQ regularity as

well as singularity of any given point in Rn×m.
Deﬁnition 3 (Generalized LICQ) For every ˇY ∈ Rn×m and Q (cid:44) {k1, k2, . . . , k|C|} ⊆
E ∪ I, deﬁne the Jacobian matrix as:
(cid:104)
vec(cid:8)∇qk1

( ˇY )(cid:9), . . . , vec(cid:8)∇qk|C|
The point ˇY is said to satisfy GLICQ condition if the rows of JB ˇY
( ˇY ) are linearly
independent, where B ˇY is the set of quasi-binding constraints for ˇY . Moreover, the
singularity function s : Rn×m → R is deﬁned as:

( ˇY )(cid:9), vec(cid:8)∇qk2

( ˇY )(cid:9)(cid:105)(cid:62)

JQ( ˇY ) (cid:44)

(6)

.

s( ˇY ) (cid:44)

(cid:8)JB ˇY

( ˇY )(cid:9),

(cid:26)σmin
0,

|B ˇY | ≤ n,
otherwise,

(7)

where σmin denotes the smallest singular value operator.
In general, it is computationally hard to calculate the exact distance to F and to verify
GLICQ as a consequence. However, local search methods can be used in practice to
ﬁnd a local solution for (3), resulting in upper bounds on the distance to F. The next
deﬁnition introduces the notion of matrix pencil corresponding to problem (1a) – (1c)
as a measure for the intensity of constraint matrices.
Deﬁnition 4 (Pencil Norm) For every Q ⊆ E ∪ I, deﬁne the matrix function pQ :
R|Q| → Sn as:

pQ(τ ) (cid:44) (cid:88)

τkAk.

k∈Q

Moreover, for every k ≥ 0 deﬁne the pencil norm ρk as:

ρk (cid:44) max

(cid:110)

(cid:107)pE∪I(τ )(cid:107)k

(cid:111)
(cid:12)
(cid:12) τ ∈ R|E∪I| ∧ (cid:107)τ (cid:107)2 = 1

,

which is upperbounded by (cid:80)

k∈E∪I (cid:107)Ak(cid:107)k .

(8)

(9)

6

Ramtin Madani et al.

2.2 Statement of Theorems

The next theorem states that if η is sufﬁciently large, then penalization preserves the
feasibility of the initial point.

Theorem 1 Let ˇY ∈ F be an LICQ feasible point for problem (1a) – (1c). If

η > (cid:107)A0(cid:107)1 + (cid:107)A0(cid:107)2 +

2(2ρ1 + ρ2)(cid:107)∇q0( ˇY )(cid:107)F
s( ˇY )

,

η > (cid:107)A0(cid:107)2 + (cid:107)∇q0( ˇY )(cid:107)F

(cid:32)(cid:115)

(cid:107)Ak(cid:107)2
|qk( ˇY )|

+

(cid:107)∇qk( ˇY )(cid:107)F
|qk( ˇY )|

(cid:33)

(10a)

,

∀k ∈ I \ B ˇY ,

(10b)

where B ˇY is the set of binding constraints for ˇY , then the convex problem (2a) – (2e)
has a unique solution (

X) that satisﬁes

Y ) ≤ q0( ˇY ).

Y (cid:62) and q0(

∗
X =

∗
Y ,

∗
Y

∗

∗

∗

Proof Refer to Section 2.4 for the proof.

(cid:117)(cid:116)

The next theorem states that if ˇY is not feasible but close to F, then penalization

results in a feasible point as well.

Theorem 2 Let ˇY ∈ Rn×m be a GLICQ point for problem (1a) – (1c) that satisﬁes:

s( ˇY ) > 2(ρ1 + ρ2)dF ( ˇY ).

If

η > (cid:107)A0(cid:107)1 + (cid:107)A0(cid:107)2 +

2ρ1(cid:107)A0(cid:107)1 dF ( ˇY ) + 2(2ρ1 + ρ2)((cid:107)∇q0( ˇY )(cid:107)F + (cid:107)A0(cid:107)2 dF ( ˇY ))
s( ˇY ) − 2(ρ1 + ρ2)dF ( ˇY )
η > (cid:107)A0(cid:107)2 + (cid:0)(cid:107)∇q0( ˇY )(cid:107)F + (cid:107)A0(cid:107)2 dF ( ˇY )(cid:1) ×

,

(11)

(12a)

(cid:32)(cid:115)

(cid:107)Ak(cid:107)2
|˜qk( ˇY )|

+

(cid:107)∇qk( ˇY )(cid:107)F + 2(cid:107)Ak(cid:107)2 dF ( ˇY )
|˜qk( ˇY )|

(cid:33)

,

∀k ∈ I \ B ˇY ,

(12b)

where B ˇY is the set of quasi-binding constraints for ˇY and functions {˜qk}k∈I are
deﬁned by equation (5), then the convex problem (2a) – (2e) has a unique solution
∗
Y ,
(

X) that satisﬁes

Y ) ≤ ˜q0( ˇY ).

Y (cid:62) and q0(

∗
X =

∗
Y

∗

∗

∗

Proof Refer to Section 2.4 for the proof.

(cid:117)(cid:116)

The next theorem offers a convergence guarantee for Algorithm 1.

Theorem 3 Let ˇF (cid:44) (cid:8)Y ∈ F | q0(Y ) ≤ ˇq(cid:9) denote an epigraph of problem (1a) –
(1c) such that s(Y 1)−1(cid:107)∇q0(Y 2)(cid:107)F is bounded for every Y 1, Y 2 ∈ ˇF. If Y 0 ∈ ˇF,
and

η > (cid:107)A(cid:107)1 + (cid:107)A(cid:107)2 + 3ρ1

maxY ∈ ˇF {(cid:107)∇q0(Y )(cid:107)F}
minY ∈ ˇF {s(Y )}

,

(13)

Title Suppressed Due to Excessive Length

7

∗

Y (l))(cid:62). Moreover, the sequence {

X (l) =
then every member of the sequence generated by Algorithm 1 satisﬁes
∗
Y (l)(
l=0 converges to a point that satisﬁes
the Karush–Kuhn–Tucker optimality conditions for problem (1a) – (1c), with non-
increasing objective values .

Y (l)}∞

∗

∗

Proof Refer to Section 2.4 for the proofs.

(cid:117)(cid:116)

2.3 Nesterov’s Acceleration

In this section, we explore the possibility of applying Nesterov’s acceleration scheme
to Algorithm 1 as a heuristic approach. As delineated in Algorithm (2), the point
(cid:0) ∗
X (l)(cid:1) denotes the solution to problem (2a) – (2e) with initial point ˇY = ˇY (l)
Y (l),
and penalty term ηl , where:

∗

ˇY (l) := (1 − λl)

Y (l−1) + λl ˇY (l−1),

∗

(14)

and the values {λl ∈ [0, 1)}∞
step. Due to non-convexity of F, even if
belong to F and Theorem 1 cannot be used to ensure the feasibility of

l=1 are determined dynamically at each
Y (l−1) is feasible, the point ˇY (l) may not

l=1 and {ηl > 0}∞

Y (l).

∗

∗

Despite this issue, we show that if

Y (l−1) is feasible and satisﬁes LICQ, then
it is possible to select the values ηl and λl in step 3 of Algorithm (2) such that the
Y (l) ∈ F. To this end,
conditions of Theorem 2 are satisﬁed, which guarantees that
assume that:

∗

∗

ηl > (cid:107)A0(cid:107)1 + (cid:107)A0(cid:107)2 + 2(2ρ1 + ρ2)

ηl > (cid:107)A0(cid:107)2 + (cid:107)∇q0(

∗

Y (l−1))(cid:107)F×

∗

(cid:107)∇q0(
s(

Y (l−1))(cid:107)F
Y (l−1))

∗

,

(15a)

(cid:32)(cid:115)

(cid:107)Ak(cid:107)2
Y (l−1))|

∗

|qk(

+

∗

(cid:107)∇qk(
|qk(

Y (l−1))(cid:107)F
Y (l−1))|

∗

(cid:33)

,

∀k ∈ I \ B ∗

Y (l−1) ,

(15b)

Algorithm 2 Accelerated Sequential Heuristic
Input: ˇY (0) ∈ Rn×m, λ0 := 1 and (cid:96) := 0
1: repeat
2:

(cid:96) := (cid:96) + 1
select η(cid:96) > 0 and λl ∈ [0, 1)
ˇY ((cid:96)) := (1 − λ(cid:96))
solve the penalized convex problem (2a) – (2e) with ˇY = ˇY ((cid:96)) and η = η(cid:96)

Y ((cid:96)−1) + λ(cid:96) ˇY ((cid:96)−1)

∗

3:

4:

5:

to obtain (cid:0) ∗

Y ((cid:96)),
6: until stopping criterion is met.

X ((cid:96))(cid:1)

∗

Output:

∗

Y ((cid:96))

8

Now, since

Ramtin Madani et al.

dF

(cid:0) ˇY (l)(cid:1) = dF

(cid:0) ∗
Y (l−1) + λl( ˇY (l−1) −

∗

Y (l−1))(cid:1) ≤ λl(cid:107) ˇY (l−1) −

∗

Y (l−1)(cid:107)F, (16)

if λl is sufﬁciently small, then ˇY (l) can get arbitrarily close to
consequence, we have:

∗

Y (l−1) and, as a

B ˇY (l) ⊆ B ∗
s(cid:0) ˇY (l)(cid:1) > 4ρ dF

Y (l−1),

(cid:0) ˇY (l)(cid:1),

ηl > (cid:107)A0(cid:107)1 + (cid:107)A0(cid:107)2 +

2ρ1(cid:107)A0(cid:107)1 dF

(cid:0) ˇY (l)(cid:1) + 2(2ρ1 + ρ2)(cid:2)(cid:107)∇q0

(cid:0) ˇY (l)(cid:1)(cid:107)F + (cid:107)A0(cid:107)2 dF

(cid:0) ˇY (l)(cid:1)(cid:3)

,

ηl > (cid:107)A0(cid:107)2 +

(cid:16)

(cid:107)∇q0

s(cid:0) ˇY (l)(cid:1) − 2(ρ1 + ρ2)dF
(cid:0) ˇY (l)(cid:1)(cid:107)F + (cid:107)A0(cid:107)2 dF

(cid:0) ˇY (l)(cid:1)(cid:17)

(cid:0) ˇY (l)(cid:1)

(cid:32)(cid:115)

(cid:107)Ak(cid:107)2
(cid:0) ˇY (l)(cid:1)|

|˜qk

+

(cid:107)∇qk

(cid:0) ˇY (l)(cid:1)(cid:107)F +2(cid:107)Ak(cid:107)2 dF
(cid:0) ˇY (l)(cid:1)|

|˜qk

×
(cid:0) ˇY (l)(cid:1)

(cid:33)

,

∀k ∈ I \B ˇY (l) ,

where B ˇY (l) is the set of quasi-binding constraints for ˇY (l). Therefore, according to
Y (l) ∈ F. This way, one may ensure the feasibility of the se-
Theorem 2, we have
∗
Y (l)}∞
quence {
l=1. We leave the convergence and theoretical analysis of this heuristic
for future work.

∗

2.4 Proofs

In order to prove Theorems 1, 2, and 3, we consider the following auxiliary optimiza-
tion problem:

minimize
Y ∈Rn×m

subject to

q0 (Y ) + η(cid:107)Y − ˇY (cid:107)2

F

qk (Y ) = 0,
qk (Y ) ≤ 0,

(18a)

(18b)

(18c)

k ∈ E,

k ∈ I.

Observe that the convex problem (2a) – (2e) is a relaxation of (18a) – (18c) and this
is the motivation for introducing (18a) – (18c).

The next two lemmas show that by increasing the penalty term η, the optimal
Y can get as close to the initial point ˇY as dF ( ˇY ). This lemma will later be

∗

solution
used to show that

∗

Y can inherit GLICQ property from ˇY .

Lemma 1 The following inequality holds for any k ∈ {0} ∪ E ∪ I and arbitrary
Y1, Y2 ∈ Rn×m:

|qk(Y1) − qk(Y2)| ≤ (cid:107)Ak(cid:107)2 (cid:107)Y1 − Y2(cid:107)2
|(cid:107)∇qk(Y1)(cid:107)F − (cid:107)∇qk(Y2)(cid:107)F| ≤ 2(cid:107)Ak(cid:107)2 (cid:107)Y1 − Y2(cid:107)F.

F + (cid:107)∇qk(Y2)(cid:107)F(cid:107)Y1 − Y2(cid:107)F,

(19a)

(19b)

Title Suppressed Due to Excessive Length

Proof The proof of (19b) is straightforward. Additionally:

|qk(Y1)−qk(Y2)| = |(cid:104)Ak , (Y1 − Y2)(Y1 − Y2)(cid:62)(cid:105)+2(cid:104)Ak Y2 +Bk , Y1 − Y2(cid:105)|

≤ |(cid:104)Ak , (Y1 − Y2)(Y1 − Y2)(cid:62)(cid:105)|+2|(cid:104)Ak Y2 +Bk , Y1 − Y2(cid:105)|
≤ (cid:107)Ak(cid:107)2 (cid:107)Y1 − Y2(cid:107)2
≤ (cid:107)Ak(cid:107)2 (cid:107)Y1 − Y2(cid:107)2

F + 2(cid:107)Ak Y2 + Bk (cid:107)F(cid:107)Y1 − Y2(cid:107)F
F + (cid:107)∇qk(Y2)(cid:107)F(cid:107)Y1 − Y2(cid:107)F,

which proves (19a).

9

(20a)

(20b)

(20c)

(20d)

(cid:117)(cid:116)

Lemma 2 If η > (cid:107)A0(cid:107)2 , then every optimal solution
satisﬁes:

∗

Y of problem (18a) – (18b)

∗

0 ≤ (cid:107)

Y − ˇY (cid:107)F − dF ( ˇY ) ≤

(cid:107)A0(cid:107)2 dF ( ˇY ) + (cid:107)∇q0( ˇY )(cid:107)F
η − (cid:107)A0(cid:107)2

,

(21)

where dF is deﬁned by equation (3).

Proof According to Deﬁnition 1, the distance between an arbitrary point ˇY and any
point in F is greater than or equal to dF ( ˇY ). This implies that (cid:107)
Y − ˇY (cid:107)F − dF ( ˇY )
is lower bounded by zero. To prove the upper bound, let ¯Y be an arbitrary member of
{Y ∈ F | (cid:107)Y −
Y is an optimal solution to problem (18a) –
(18b), one can write:

Y (cid:107)F = dF (

Y )}. Since

∗

∗

∗

∗

−(cid:107)∇q0( ˇY )(cid:107)F(cid:107)

∗

∗

∗

Y − ˇY (cid:107)2
F

∗
Y ) + η(cid:107)

Y − ˇY (cid:107)F + (cid:0)η − (cid:107)A0(cid:107)2

(cid:1)(cid:107)
≤ − q0( ˇY ) + q0(
Y − ˇY (cid:107)2
F
≤ − q0( ˇY ) + q0( ¯Y ) + η(cid:107) ¯Y − ˇY (cid:107)2
F
≤(cid:107)∇q0( ˇY )(cid:107)F(cid:107) ¯Y − ˇY (cid:107)F + (cid:0)η + (cid:107)A0(cid:107)2
=(cid:107)∇q0( ˇY )(cid:107)FdF ( ˇY ) + (cid:0)η + (cid:107)A0(cid:107)2

(cid:1)(cid:107) ¯Y − ˇY (cid:107)2

F

(cid:1)dF ( ˇY )2,

(22a)

(22b)

(22c)

(22d)

(22e)

where (22b) and (22d) are concluded form Lemma 1. Hence:

− (cid:107)∇q0( ˇY )(cid:107)F
≤ 2(cid:107)A0(cid:107)2 dF ( ˇY )2,

(cid:0)(cid:107)

∗

Y − ˇY (cid:107)F + dF ( ˇY )(cid:1) + (cid:0)η − (cid:107)A0(cid:107)2

(cid:1)(cid:0)(cid:107)

and

∗

Y − ˇY (cid:107)2

F − dF ( ˇY )2(cid:1)
(23)

(cid:16)

∗

Y − ˇY (cid:107)F + dF ( ˇY )

(cid:107)
≤ 2(cid:107)A0(cid:107)2 dF ( ˇY )2.

(cid:17) (cid:104)(cid:16)
(cid:107)

∗

Y − ˇY (cid:107)F − dF ( ˇY )

(cid:17) (cid:0)η − (cid:107)A0(cid:107)2

(cid:1) − (cid:107)∇q0( ˇY )(cid:107)F

(cid:105)

(24)

Now due to feasibility of

∗

Y we have dF ( ˇY ) ≤ (cid:107)

Y − ˇY (cid:107)F which implies:

∗

10

(cid:16)

Ramtin Madani et al.

∗

Y − ˇY (cid:107)F − dF ( ˇY )
(cid:107)

(cid:1) − (cid:107)∇q0( ˇY )(cid:107)F

(cid:17) (cid:0)η − (cid:107)A0(cid:107)2
2dF ( ˇY )
Y − ˇY (cid:107)F + dF ( ˇY )
(cid:107)

∗

≤ (cid:107)A0(cid:107)2 dF ( ˇY ) ×

≤ (cid:107)A0(cid:107)2 dF ( ˇY ),

(25)

which concludes the right side of (21).

(cid:117)(cid:116)

The next lemma shows that if ˇY satisﬁes GLICQ and η is sufﬁciently large, then

∗

Y satisﬁes GLICQ as well.

Lemma 3 Let ˇY ∈ Rn×m be a GLICQ point that satisﬁes s( ˇY ) > 2ρ dF ( ˇY ) and
assume that:

η > (cid:107)A0(cid:107)2 + 2ρ2 ×
η > (cid:107)A0(cid:107)2 + (cid:0)(cid:107)∇q0( ˇY )(cid:107)F + (cid:107)A0(cid:107)2 dF ( ˇY )(cid:1) ×

(cid:107)∇q0( ˇY )(cid:107)F + (cid:107)A0(cid:107)2 dF ( ˇY )
s( ˇY ) − 2ρ2 dF ( ˇY )

,

(cid:32)(cid:115)

(cid:107)Ak(cid:107)2
|˜qk( ˇY )|

+

(cid:107)∇qk( ˇY )(cid:107)F + 2(cid:107)Ak(cid:107)2 dF ( ˇY )
|˜qk( ˇY )|

(cid:33)

,

(26a)

∀k ∈ I \ B ˇY , (26b)

where B ˇY denotes the set of quasi-binding constraints for ˇY . Then, every solution
of problem (18a) – (18b) is LICQ and satisﬁes:

∗
Y

∗

Y ) ≥ s( ˇY ) − 2ρ2 (cid:107) ˇY −

∗

Y (cid:107)F,

s(

(27)

where functions {˜qk}k∈I are deﬁned in (5) and B ˇY denotes the set of quasi-binding
constraints for ˇY .

Proof We ﬁrst need to show that B ∗
Y

denotes the set of quasi-
Y . Let k ∈ I \ B ˇY . Then, according to assumption (26b) and

⊆ B ˇY , where B ∗
Y

∗

binding constraints for
Lemma 2, we have:

(cid:107) ˇY −

∗

Y (cid:107)F−dF ( ˇY ) ≤

(cid:107)A0(cid:107)2 dF ( ˇY ) + (cid:107)∇q0( ˇY )(cid:107)F
η − (cid:107)A0(cid:107)2
|˜qk( ˇY )|

(cid:113)

<

(cid:107)∇qk( ˇY )(cid:107)F + 2(cid:107)Ak(cid:107)2 dF ( ˇY ) +

(cid:107)Ak(cid:107)2 |˜qk( ˇY )|

(28a)

,

(28b)

Title Suppressed Due to Excessive Length

11

where the second line is concluded by substituting the lower bound for η. Hence:

qk(

∗

Y ) ≤qk( ˇY ) + (cid:107)∇qk( ˇY )(cid:107)F(cid:107) ˇY −

∗

Y (cid:107)F + (cid:107)Ak(cid:107)2 (cid:107) ˇY −

∗

Y (cid:107)2
F

≤qk( ˇY )+(cid:107)∇qk( ˇY )(cid:107)F

(cid:18)

dF ( ˇY ) +

(cid:107)A0(cid:107)2 dF ( ˇY )+(cid:107)∇q0( ˇY )(cid:107)F
η − (cid:107)A0(cid:107)2
(cid:19)2

(cid:19)

+(cid:107)Ak(cid:107)2

(cid:18)

dF ( ˇY )+

(cid:107)A0(cid:107)2 dF ( ˇY ) + (cid:107)∇q0( ˇY )(cid:107)F
η − (cid:107)A0(cid:107)2

=˜qk( ˇY )+

(cid:107)A0(cid:107)2 dF ( ˇY ) + (cid:107)∇q0( ˇY )(cid:107)F
η − (cid:107)A0(cid:107)2

(cid:18)

×

(cid:107)∇qk( ˇY )(cid:107)F + 2(cid:107)Ak(cid:107)2 dF ( ˇY ) + (cid:107)Ak(cid:107)2

(cid:107)A0(cid:107)2 dF ( ˇY ) + (cid:107)∇q0( ˇY )(cid:107)F
η − (cid:107)A0(cid:107)2

(cid:19)

<˜qk( ˇY )+

|˜qk( ˇY )|

(cid:107)∇qk( ˇY )(cid:107)F +2(cid:107)Ak(cid:107)2 dF ( ˇY )+

(cid:113)


(cid:107)∇qk( ˇY )(cid:107)F +2(cid:107)Ak(cid:107)2 dF ( ˇY )+

×

=˜qk( ˇY ) + |˜qk( ˇY )| = 0,

(cid:107)Ak(cid:107)2 |˜qk( ˇY )|


(cid:107)Ak(cid:107)2 |˜qk( ˇY )|
(cid:113)
(cid:107)Ak(cid:107)2 |˜qk( ˇY )|



(29)

which implies that k /∈ B ∗
Y

, and therefore B ∗
Y

⊆ B ˇY .

Finally, the LICQ regularity of

proven as follows:

∗

∗

Y and the lower bound (27) on s(

Y ) can be

s(

∗

Y

Y ) = min(cid:8)(cid:107)JB ∗
≥ min(cid:8)(cid:107)JB ˇY
≥ min(cid:8)(cid:107)JB ˇY
≥ min(cid:8)(cid:107)JB ˇY

∗

∗

(

Y )(cid:62)ξ(cid:107)2 | (cid:107)ξ(cid:107)2 = 1(cid:9)
Y )(cid:62)ν(cid:107)2 | (cid:107)ν(cid:107)2 = 1(cid:9)
(
( ˇY )(cid:62)ν(cid:107)2 − (cid:107)[JB ˇY
( ˇY )(cid:62)ν(cid:107)2 | (cid:107)ν(cid:107)2 = 1(cid:9)
(ν)( ˇY −

(30a)

(30b)
Y )](cid:62)ν(cid:107)2 | (cid:107)ν(cid:107)2 = 1(cid:9) (30c)

∗

(

( ˇY ) − JB ˇY

− 2 max(cid:8)(cid:107)pB ˇY
≥ s( ˇY ) − 2 max(cid:8)(cid:107)pB ˇY
≥ s( ˇY ) − 2ρ2 × (cid:107) ˇY −

∗

Y )(cid:107)F | (cid:107)ν(cid:107)2 = 1(cid:9)
(ν)(cid:107)2 | (cid:107)ν(cid:107)2 = 1(cid:9)(cid:107) ˇY −
Y (cid:107)F

∗

∗

Y (cid:107)F

(30d)

(30e)

(30f)

≥ s( ˇY ) − 2ρ2 ×

(cid:18)

dF ( ˇY ) +

(cid:107)A0(cid:107)2 dF ( ˇY ) + (cid:107)∇q0( ˇY )(cid:107)F
η − (cid:107)A0(cid:107)2

(cid:19)

> 0,

(30g)

where the last inequality is resulted from (26a).

(cid:117)(cid:116)

The next lemma offers a bound on the dual optimal solution of (18a) – (18c),

which will be used for proving the theorems.

12

Ramtin Madani et al.

∗

Lemma 4 Let
a vector of Lagrange multipliers ∗τ ∈ R|B ∗

Y

Y be an LICQ optimal solution to problem (18a) – (18b). There exists

(cid:107)η−1 ∗τ (cid:107)2 ≤

2(cid:107)

∗

Y that satisﬁes:

∗

| associated with
Y − ˇY (cid:107)F + η−1(cid:107)∇q0(
s(

∗
Y )

∗

Y )(cid:107)F

.

(31)

Proof Due to LICQ, there exists a vector of Lagrange multipliers ∗τ ∈ R|B ∗
ated with

Y

∗

Y that satisﬁes KKT stationarity condition:
Y − ˇY ) + (A0

Y + B0 ) +

∗τk(Ak

(cid:88)

∗

∗

η(

∗

Y + Bk ) = 0n×m,

(32)

| associ-

k∈B ∗
Y

and, therefore:

2vec{η(

∗

Y − ˇY ) + (A0

Since JB ∗

Y

(

∗

Y ) is full-rank, we have:

∗

Y + B0)} + JB ∗

Y

∗

Y )(cid:62) ∗τ = 0nm.

(

∗τ = −2J +
B ∗
Y

Hence:

∗

Y )(cid:62)vec{η(
(

∗

Y − ˇY ) + (A0

∗

Y + B0)}.

(cid:107)η−1 ∗τ (cid:107)2 ≤ 2(cid:107)J +
B ∗
Y

(

∗

Y )(cid:107)2 × (cid:107)vec{(

∗

Y − ˇY ) + η−1(A0

∗

Y + B0)}(cid:107)2

∗

∗

= 2s(
∗

≤ s(

Y )−1(cid:107)(
Y )−1(cid:0)2(cid:107)

Y − ˇY ) + η−1(A0
Y − ˇY (cid:107)F + η−1(cid:107)∇q0(

Y + B0)(cid:107)F
(cid:1),

Y )(cid:107)F

∗

∗

∗

which proves the lemma.

(33)

(34)

(35a)

(35b)

(35c)

(cid:117)(cid:116)

The next lemma offers a sufﬁcient condition on the success of penalization.

∗

Lemma 5 Let
problem (18a) – (18b). If the matrix:

Y and ∗τ ∈ R|B ∗

Y

| be a pair of primal and dual optimal solutions for

∗

Λ (cid:44) ηI n + A0 + pB ∗

( ∗τ ),

Y

is diagonally-dominant, then (
– (2e), where

X (cid:44) ∗
Y

Y (cid:62).

∗

∗

∗
Y ,

∗

X) is the unique optimal solution for problem (2a)

∗

Proof Since
Λ is diagonally-dominant, it can serve as a Lagrange multiplier matrix
corresponding to the constraints (2d) – (2e). To prove that (
Λ) are
primal and dual optimal solutions, it sufﬁces to verify the following KKT conditions:

X) and ( ∗τ ,

∗
Y ,

∗

∗

Stationarity with respect to X:

ηI n + A0 + pB ∗

Y

Stationarity with respect to Y :

∗

∗
Λ

Y − η ˇY + B0 +

( ∗τ ) −
(cid:88)

∗

Λ = 0n×n,

(36a)

∗τkBk = 0n×m,

k∈B ∗
Y

Primal feasibility:

Complementary slackness:

∗
Y ,

¯qk(

∗
X) = 0,

k ∈ B ∗
Y

,

∗
Λ,
(cid:104)

∗
X −

∗
Y

∗

Y (cid:62)(cid:105) = 0.

(36b)

(36c)

(36d)

ρ1 × (cid:107)η−1 ∗τ (cid:107)2
1 − η−1(cid:107)A0(cid:107)1

≤

≤

≤

=

ρ1
1 − η−1(cid:107)A0(cid:107)2

ρ1
1 − η−1(cid:107)A0(cid:107)1
ρ1
1 − η−1(cid:107)A0(cid:107)1

×

×

×

=

ρ1
1 − η−1(cid:107)A0(cid:107)1

×

Title Suppressed Due to Excessive Length

13

Λ. Additionally,
Stationarity with respect to X is followed from the deﬁnition of
Y for problem
(36b) is followed from the deﬁnition of
(18a) – (18b). Finally, the remaining conditions (36c) and (36d) can be concluded
(cid:117)(cid:116)
from the deﬁnition of

Λ, as well as the optimality of

Y for (18a) – (18b).

X and feasibility of

∗

∗

∗

∗

∗

∗

Proof (Theorem 2) Let
cording to Lemma 3, the point
variables ∗τ ∈ R|B ∗
constraints for

Y

∗

Y be an optimal solution for problem (18a) – (18b). Ac-
Y is LICQ. As a result, there exists a vector of dual
∈ E ∪ I is the set of binding

| corresponding to

∗

∗

Y , where B ∗
Y

Y . Now, according to Lemmas 1, 2, 3, and 4, we have:

∗

2(cid:107)

∗

Y )(cid:107)F

×

ρ1
1 − η−1(cid:107)A0(cid:107)1

Y − ˇY (cid:107)F + η−1(cid:107)∇q0(
s(

∗

2(1 + η−1(cid:107)A0(cid:107)2 )(cid:107)

∗
Y )
Y − ˇY (cid:107)F + η−1(cid:107)∇q0( ˇY )(cid:107)F
Y (cid:107)F
2(1 + η−1(cid:107)A0(cid:107)2 ) dF ( ˇY )+η−1(cid:107)∇q0( ˇY )(cid:107)F

s( ˇY ) − 2ρ2 × (cid:107) ˇY −

∗

1−η−1(cid:107)A0(cid:107)2
s( ˇY ) − 2ρ2 × dF ( ˇY )+η−1(cid:107)∇q0( ˇY )(cid:107)F

1−η−1(cid:107)A0(cid:107)2

+ η−1(cid:107)∇q0( ˇY )(cid:107)F

2(1 + η−1(cid:107)A0(cid:107)2 )(dF ( ˇY ) + η−1(cid:107)∇q0( ˇY )(cid:107)F) + η−1(1 − η−1(cid:107)A0(cid:107)2 )(cid:107)∇q0( ˇY )(cid:107)F
(1 − η−1(cid:107)A0(cid:107)2 )s( ˇY ) − 2ρ2(dF ( ˇY ) + η−1(cid:107)∇q0( ˇY )(cid:107)F)

2dF ( ˇY ) + (cid:0)2(cid:107)A0(cid:107)2 dF ( ˇY ) + 3(cid:107)∇q0( ˇY )(cid:107)F

(cid:1) η−1 + (cid:107)A0(cid:107)2 (cid:107)∇q0( ˇY )(cid:107)Fη−2

s( ˇY ) − 2ρ2dF ( ˇY ) − (cid:0)(cid:107)A0(cid:107)2 s( ˇY ) + 2ρ2(cid:107)∇q0( ˇY )(cid:107)F

(cid:1)η−1

≤

2ρ1dF ( ˇY ) + 2ρ1

(cid:0)(cid:107)A0(cid:107)2 dF ( ˇY ) + 2(cid:107)∇q0( ˇY )(cid:107)F

(cid:1) η−1

(cid:0)s( ˇY ) − 2ρ2dF ( ˇY )(cid:1)(cid:0)1 − η−1(cid:107)A0(cid:107)1

(cid:1) − (cid:0)(cid:107)A0(cid:107)2 s( ˇY ) + 2ρ2(cid:107)∇q0( ˇY )(cid:107)F

≤ 1

and, the last inequality is equivalent to (12a). Hence:

(cid:1)η−1
(37)

η − (cid:107)A0(cid:107)1 − ρ1 × (cid:107) ∗τ (cid:107)2 > 0,

(38)

which implies that the matrix ηI n + A0 + pB ∗

cording to Lemma 5, the point (
(2a) – (2e).

∗
Y ,

∗
Y

( ∗τ ) is diagonally-dominant and ac-
Y (cid:62)) is the unique optimal solution for problem

Y

∗

14

Ramtin Madani et al.

Additionally, let ¯Y be an arbitrary member of {Y ∈ F | (cid:107)Y − ˇY (cid:107)F = dF ( ˇY )}.

Due to optimality of

Y , we have:

∗

∗

q0(

Y )+η × dF ( ˇY )2

∗

∗
Y ) + η(cid:107)

Y − ˇY (cid:107)2
≤ q0(
F
≤ q0( ¯Y ) + η(cid:107) ¯Y − ˇY (cid:107)2
F
≤ q0( ˇY ) + (cid:107)∇q0( ˇY )(cid:107)F(cid:107) ¯Y − ˇY (cid:107)F + (cid:0)η + (cid:107)A0(cid:107)2
= q0( ˇY ) + (cid:107)∇q0( ˇY )(cid:107)FdF ( ˇY ) + (cid:0)η + (cid:107)A0(cid:107)2
= ˜q0( ˇY ) + η × dF ( ˇY )2,

(cid:1)(cid:107) ¯Y − ˇY (cid:107)2

(cid:1)dF ( ˇY )2

(39a)

(39b)

F (39c)
(39d)

(39e)

(cid:117)(cid:116)

which concludes q0(

∗

Y ) ≤ ˜q0( ˇY ).

Proof (Theorem 1) The proof is a consequence of Theorem 2 with dF ( ˇY ) = 0. (cid:117)(cid:116)

Lemma 6 Deﬁne hη : F → F as the function mapping any initial point ˇY of
problem (18a) – (18c) to its primal solution. Consider an arbitrary ˇY ∈ F. If the
primal solution

Y is LICQ and the dual point ∗τ ∈ R|B ∗

| satisﬁes:

Y

∗

ηIn + A0 + p ∗
B
then, hη is continuous at point ˇY , where B ∗
Y
constraints for

Y .

∗

( ∗τ ) (cid:31) 0,

(40)

⊆ E ∪ I denotes the set of binding

Proof Due to LICQ regularity of
ditions are satisﬁed for (

Y , ∗τ ):

∗

∗

Y , the KKT stationarity and primal feasibility con-

(cid:16)

ηI n + A0 + p ∗
B

( ∗τ )

(cid:17) ∗

Y + B0 +

(cid:88)

k∈B ∗
Y

∗τkBk = η ˇY ,

∗
Y ) = 0,

qk(

(41a)

k ∈ B ∗
Y

. (41b)

The Jacobian matrix of the above equations is:

(cid:16)


2


( ∗τ )

ηIn + A0 + p ∗
B
Y )(cid:62)
J ∗
(
B

∗

(cid:17)

⊗ Im J ∗
B

∗
Y )
(

0
∗
B|×|
|

∗
B|



 ,

which is non-singular due to non-singularity J ∗
B

∗

Y ) and assumption (40).
(

(42)

(cid:117)(cid:116)

Proof (Theorem 3) Let (cid:8)Y (l)(cid:9)∞
Assume by induction that Y (l) ∈ ˇF and let
(18a) – (18c) for ˇY = Y (l). According to the optimality of

l=0 denote the sequence generated by Algorithm 1.
Y be an optimal solution for problem
Y and feasibility of Y (l):

∗

∗

∗
Y ) + η(cid:107)

∗

q0(

Y − Y (l)(cid:107)2

F ≤ q0(Y (l)) + η(cid:107)Y (l) − Y (l)(cid:107)2
F,

(43)

which concludes q0(

Y ) ≤ q0(Y (l)), and therefore

∗

∗

Y ∈ ˇF.

Title Suppressed Due to Excessive Length

15

Due to the assumption minY ∈ ˇF {s(Y )} > 0, the point

there exists a vector of dual variables ∗τ ∈ R|B ∗
E ∪ I is the set of binding constraints for
well as the assumption (13), we have:

Y

∗

| corresponding to

∗

Y is LICQ. As a result,
⊆

Y , where B ∗
Y

∗

Y . Now, according to Lemmas 2 and 4, as

ρ1 × (cid:107)η−1 ∗τ (cid:107)2
1 − η−1(cid:107)A0(cid:107)1

≤

ρ1
1 − η−1(cid:107)A0(cid:107)1

×

∗

2(cid:107)

Y − Y (l)(cid:107)F + η−1(cid:107)∇q0(
s(

∗

Y )(cid:107)F

∗
Y )
∗

2 × η−1(cid:107)∇q0(Y (l))(cid:107)F

+ η−1(cid:107)∇q0(

Y )(cid:107)F

×

s(

∗
Y )

1−η−1(cid:107)A0(cid:107)2

(cid:0)1 − η−1(cid:107)A0(cid:107)1

ρ1
1 − η−1(cid:107)A0(cid:107)1
ρ1 η−1(cid:0)2(cid:107)∇q0(Y (l))(cid:107)F + (1 − η−1(cid:107)A0(cid:107)2 )(cid:107)∇q0(
∗
Y )
ρ1 η−1(cid:0)2(cid:107)∇q0(Y (l))(cid:107)F + (1 − η−1(cid:107)A0(cid:107)2 )(cid:107)∇q0(
(cid:0)1 − η−1(cid:107)A0(cid:107)1 − η−1(cid:107)A0(cid:107)2
η−1ρ1
1 − η−1(cid:107)A0(cid:107)1 − η−1(cid:107)A0(cid:107)2

(cid:1)(cid:0)1 − η−1(cid:107)A0(cid:107)2

(cid:1)s(

(cid:1)s(

×

∗
Y )
2(cid:107)∇q0(Y (l))(cid:107)F + (cid:107)∇q0(
s(
maxY ∈ ˇF {(cid:107)∇q0(Y )(cid:107)F}
minY ∈ ˇF {s(Y )}

∗
Y )

≤ 1.

3ρ1
η − (cid:107)A0(cid:107)1 − (cid:107)A0(cid:107)2

×

∗

Y )(cid:107)F

(cid:1)

∗

Y )(cid:107)F

(cid:1)

≤

≤

≤

≤

≤

∗

Y )(cid:107)F

Hence:

η − (cid:107)A0(cid:107)1 − ρ1 × (cid:107) ∗τ (cid:107)2 > 0,

(44a)

(44b)

(44c)

(44d)

(44e)

(44f)

(45)

which implies that the matrix ηI n + A0 + pB ∗

( ∗τ ) is diagonally-dominant and ac-
Y (cid:62)) is the unique optimal solution for problem

Y

∗

∗
Y ,

∗
Y

cording to Lemma 5, the point (
(2a) – (2e), which implies that Y (l+1) =

∗

Y .

On the other hand, according to (43), the sequence (cid:8)q0(Y (l))(cid:9)∞

l=0 is non-increasing

and convergent. Additionally, we have:

(cid:107)Y (l+1) − Y (l)(cid:107)2

F ≤ η−1 (cid:16)

(cid:17)
q0(Y (l)) − q0(Y (l+1))

,

(46)

which means that (cid:8)Y (l)(cid:9)∞
of ˇF.

l=0 is convergent to a point Y (∞) ∈ ˇF due to compactness

According to Lemma (6), we have hη

(cid:0)Y (∞)(cid:1) = Y (∞) (See Lemma 6 for the
deﬁnition of hη). Now, due to LICQ regularity of Y (∞), there exists a vector of
dual variables τ (∞) ∈ R|B(∞)| for which the following KKT stationarity and primal
feasibility conditions are satisﬁed:
(cid:0)τ (∞)(cid:1)(cid:17)

Y (∞) + B0 +

(47a)

(cid:88)

(cid:16)

A0 + p ∗
B

k∈B

Y (∞)
qk

τ (∞)
k Bk = 0n×m,
(cid:0)Y (∞)(cid:1) = 0,

k ∈ BY (∞),

(47b)

16

Ramtin Madani et al.

where B(∞) denotes the set of binding constraints for Y (∞). Finally, it can be easily
observed that (47a) – (47b) are equivalent to KKT optimality conditions for the orig-
(cid:117)(cid:116)
inal non-convex problem (1a) – (1c) which completes the proof.

3 Computational Results

In this section, three case studies are given to demonstrate some possible applica-
tions and to evaluate the merits of the proposed parabolic relaxation. In Section 3.1,
we compute lower bounds offered by the parabolic relaxation for benchmark cases
from the library of quadratic programming instances (QPLIB) [20], and compare
them to that of SOCP relaxation. In Section 3.2, we evaluate the ability of penalized
parabolic relaxation in ﬁnding feasible points for non-convex QCQPs with continu-
ous variables. In Section 3.3, we discuss the problem of identifying linear dynamical
systems based on limited snapshots from sample trajectories. All of the experiments
are performed on a desktop computer with a 12-core 3.0GHz CPU and 256GB RAM.
MOSEK v8.1 [35] is used through MATLAB 2017a to solve the resulting convex
relaxations.

3.1 Bounds for Non-convex QCQP

This case study is concerned with assessing the tightness of parabolic relaxation in
comparison with SOCP relaxation. To this end, we perform experiments on the non-
convex QCQP problems from QPLIB. The size, number of constraints, and optimal
cost for each instance are reported in Table 1. The following valid inequalities are
imposed on all convex relaxations:

Xkk − (xlb
Xkk − (xub
Xkk − (xlb

k + xub
k + xub
k + xlb

k )xk + xlb
k )xk + xub
k )xk + xlb

k xub
k xub
k xlb

k ≤ 0,
k ≥ 0,
k ≥ 0,

∀k ∈ N ,

∀k ∈ N ,

∀k ∈ N ,

(48a)

(48b)

(48c)

where xlb, xub ∈ Rn are given lower and upper bounds on x and N (cid:44) {1, . . . , n}.
Lower bounds from parabolic and SOCP relaxations and their gaps from the optimal-
ity are reported in Table 1 where:

GAP(%) (cid:44) 100 ×

q0(xQPLIB) − q0( ∗x,

|q0(xQPLIB)|

∗
X)

,

(49)

and xQPLIB is the optimal point provided by QPLIB, and ( ∗x,
X) represent the out-
come of convex relaxation. As shown by the table, for all of the cases except 0018
and 0343, the parabolic relaxation outperforms SOCP by an average of 572%. For
the case 0018, both relaxations are unbounded.

∗

Title Suppressed Due to Excessive Length

17

Table 1: Comparison of lower bounds from the SOCP and the parabolic relaxations
for QPLIB instances.

Inst

0018
0343
0911
0975
1055
1143
1157
1353
1423
1437
1451
1493
1507
1535
1619
1661
1675
1703
1745
1773
1886
1913
1922
1931
1940
1967

Cons Cons

Total Quad Total Optimal
Var
50
50
50
50
40
40
40
50
40
50
60
40
30
60
50
60
60
60
50
60
50
48
30
40
48
50

Cost
-6.386
-6.386
-32.148
-37.854
-33.037
-57.247
-10.948
-7.714
-14.967
-7.789
-87.576
-43.160
-8.301
-11.586
-9.217
-15.955
-75.669
-132.802
-72.377
-14.642
-78.672
-52.108
-35.951
-55.709
-38.310
-107.581

1
1
50
10
20
24
9
6
24
11
66
5
33
66
30
13
13
36
55
7
50
48
60
40
96
75

0
0
50
10
20
20
1
1
20
1
60
1
30
60
25
1
1
30
50
1
50
48
60
40
96
75

SOCP

Parabolic

LB
-
-226.334
-299.520
-295.158
-393.235
-600.990
-37.239
-73.476
-92.610
-67.007
-793.469
-407.538
-55.646
-122.272
-135.057
-114.224
-670.112
-1197.274
-378.953
-169.958
-627.305
-289.660
-216.084
-390.254
-283.950
-1214.746

GAP(%)
-
3444.22
831.71
679.74
1090.29
949.82
240.14
852.49
518.74
760.25
806.03
844.24
570.32
955.33
1365.26
615.92
785.59
801.55
423.58
1060.77
697.37
455.88
501.06
600.52
641.19
1029.14

t(s)

-
1.56
0.63
0.63
0.55
0.61
0.72
0.64
0.56
0.75
0.70
0.58
0.59
0.61
0.67
0.70
0.66
0.66
0.61
0.64
0.72
0.67
0.63
0.66
0.70
0.73

LB
-
-234.706
-76.525
-78.384
-94.630
-179.463
-20.431
-23.557
-31.901
-28.147
-227.164
-142.863
-16.726
-40.866
-31.556
-46.364
-198.723
-411.862
-138.833
-49.329
-163.551
-82.897
-62.914
-103.182
-69.374
-306.963

GAP(%)
-
3575.32
138.04
107.07
186.44
213.49
86.61
205.37
113.14
261.36
159.39
231.01
101.49
252.71
242.35
190.60
162.62
210.13
91.82
236.90
107.89
59.09
75.00
85.22
81.08
185.33

t(s)

-
0.63
0.58
0.56
0.59
0.63
0.61
0.63
0.58
0.64
0.70
0.56
0.56
0.66
0.55
0.66
0.63
0.67
0.61
0.69
0.67
0.63
0.69
0.59
0.61
0.67

3.2 Feasible Point Recovery

This case study is concerned with the recovery of feasible points from inexact parabolic
relaxations using Algorithm 1. Table 2 demonstrates a comparison between this ap-
proach and GUROBI 9.0. Let ( ∗x,
X) denote the optimal solution of the convex re-
laxation (2a) – (2e). We use the point ˇx = ∗x as the initial point of the algorithm.

∗

The penalty parameter η is chosen via bisection as the smallest number of the
X = ∗x ∗x(cid:62) during the ﬁrst ten rounds, where α ∈
form α × 10β, which results in a
{1, 2, 5} and β is an integer. In all of the experiments, the value of η remained static
throughout Algorithm 1. Denote the sequence of penalized SDP solutions obtained
by Algorithm 1 as:

∗

(x(1), X (1)), (x(2), X (2)), (x(3), X (3)),

. . . .

The smallest i such that:

tr{X (i) − x(i)(x(i))(cid:62)} < 10−7,

(50)

18

Ramtin Madani et al.

Table 2: Comparison between feasible points offered by penalized parabolic relax-
ations and GUROBI 9.0.

Inst

0018
0343
0911
0975
1055
1143
1157
1353
1423
1437
1451
1493
1507
1535
1619
1661
1675
1703
1745
1773
1886
1913
1922
1931
1940
1967

Optimal
Cost
-6.386
-6.386
-32.148
-37.854
-33.037
-57.247
-10.948
-7.714
-14.967
-7.789
-87.576
-43.160
-8.301
-11.586
-9.217
-15.955
-75.669
-132.802
-72.377
-14.642
-78.672
-52.108
-35.951
-55.709
-38.310
-107.581

η

5e+2
5e+2
1e+1
1e+1
2e+1
5e+1
5e+0
5e+0
5e+0
5e+0
5e+1
5e+1
5e+0
5e+0
5e+0
5e+0
2e+1
5e+1
2e+1
5e+0
2e+1
1e+1
1e+1
2e+1
2e+1
5e+1

ifeas
1
1
4
1
1
1
1
1
1
1
2
1
1
2
1
1
1
2
1
1
1
9
2
1
1
2

Penalized Parabolic
tstop(s)
120.19
129.65
27.99
13.04
6.65
57.23
3.01
22.61
10.49
16.38
62.92
22.57
17.50
68.76
17.36
16.47
9.65
22.23
14.03
29.75
13.27
11.86
10.21
27.54
24.08
32.86

istop
286
322
63
31
65
143
25
52
26
37
142
58
41
123
42
38
23
54
33
72
33
27
25
66
55
75

UB
-6.377
-6.031
-31.713
-36.433
-32.775
-55.511
-10.942
-7.708
-14.687
-7.787
-87.357
-41.831
-8.292
-11.521
-9.212
-15.664
-75.539
-132.474
-71.805
-14.177
-78.601
-51.888
-35.448
-54.290
-38.264
-106.861

GAP(%)
0.13
5.56
1.35
3.75
0.79
3.03
0.06
0.08
1.87
0.03
0.25
3.08
0.11
0.56
0.06
1.83
0.17
0.25
0.79
3.18
0.09
0.42
1.40
2.55
0.12
0.67

GUROBI

UB
-5.939
-5.939
0.000
-19.643
-6.496
-46.370
Inf
-7.357
Inf
Inf
Inf
-40.379
-5.104
Inf
Inf
Inf
Inf
Inf
Inf
Inf
-77.185
-50.847
-35.561
-55.345
-37.426
0.000

GAP(%)
7.00
7.00
100.00
48.11
80.34
19.00
Inf
4.63
Inf
Inf
Inf
6.44
38.52
Inf
Inf
Inf
Inf
Inf
Inf
Inf
1.89
2.42
1.08
0.65
2.31
100.00

is denoted by ifeas, i.e., it is the number of rounds that Algorithm 1 needs to attain a
tight penalization. Moreover, the smallest i such that:

q0(x(i−1)) − q0(x(i))
|q0(x(i))|

≤ 10−4,

(51)

is denoted by istop, and UB (cid:44) q0(x(istop)). The following formula is used to calculate
the ﬁnal percentage gaps from the optimal costs reported by the QPLIB library:

GAP(%) = 100 ×

qstop
0 − q0(xQPLIB)
|q0(xQPLIB)|

.

(52)

Moreover, t(s) denotes the cumulative solver time in seconds for the istop rounds. Our
results are compared with GUROBI 9.0 by ﬁxing the maximum solver time equal to
the cumulative solver time spent by Algorithm 1. The resulting lower bounds, upper
bounds and GAPs (from equation (52)) are reported in Table 2.

As demonstrated by Figures 1 and 2, Algorithm 1 outperforms GUROBI for the
majority of cases. However, for cases 1922, 1931, 0975, and 1423, GUROBI ulti-
mately outperforms Algorithm 1 and this is shown by Figure 3.

Title Suppressed Due to Excessive Length

19

Fig. 1: Bounds offered by GUROBI 9.0 and the penalized parabolic relaxation for QPLIB cases.

0100200300400500600Time-10-8-6-4-2Objective Value0018GUROBI LBGUROBI UBOptimal ValueParabolic UB0100200300400500600Time-10-8-6-4-2Objective Value0343GUROBI LBGUROBI UBOptimal ValueParabolic UB0100200300400500600Time-60-40-200Objective Value0911GUROBI LBGUROBI UBOptimal ValueParabolic UB0100200300400500600Time-60-40-200Objective Value1055GUROBI LBGUROBI UBOptimal ValueParabolic UB0100200300400500600Time-100-80-60-40-200Objective Value1143GUROBI LBGUROBI UBOptimal ValueParabolic UB01020304050Time-12-11.5-11-10.5-10Objective Value1157GUROBI LBGUROBI UBOptimal ValueParabolic UB050100150200250300Time-9-8.5-8-7.5-7Objective Value1353GUROBI LBGUROBI UBOptimal ValueParabolic UB0100200300400500600Time-10-9-8-7Objective Value1437GUROBI LBGUROBI UBOptimal ValueParabolic UB0100200300400500600Time-2000200400Objective Value1451GUROBI LBGUROBI UBOptimal ValueParabolic UB050100150200Time-50-45-40-35Objective Value1493GUROBI LBGUROBI UBOptimal ValueParabolic UB050100150200250300Time-10-9-8-7-6-5Objective Value1507GUROBI LBGUROBI UBOptimal ValueParabolic UB050100150200Time050100Objective Value1535GUROBI LBGUROBI UBOptimal ValueParabolic UB20

Ramtin Madani et al.

Fig. 2: Bounds offered by GUROBI 9.0 and the penalized parabolic relaxation for QPLIB cases.

3.3 Identiﬁcation of Linear Dynamical Systems

Following [51] and [14], this case study is concerned with the problem of identify-
ing the parameters of linear dynamical systems given limited observations of state
vectors. Optimization is an important tool for problems involving dynamical systems
such as the identiﬁcation of transfer functions and control synthesis [15, 25, 44, 48,
49]. One of these computationally-hard problems is data-driven system identiﬁcation

0100200300400500600Time-15-10-5Objective Value1619GUROBI LBGUROBI UBOptimal ValueParabolic UB0100200300400500600Time-25-20-15-10Objective Value1661GUROBI LBGUROBI UBOptimal ValueParabolic UB050100150200250300Time-90-80-70-60Objective Value1675GUROBI LBGUROBI UBOptimal ValueParabolic UB050100150200Time-2000200400600800Objective Value1703GUROBI LBGUROBI UBOptimal ValueParabolic UB0100200300400500Time-80-78-76-74-72-70Objective Value1745GUROBI LBGUROBI UBOptimal ValueParabolic UB020406080100Time-35-30-25-20-15-10-5Objective Value1773GUROBI LBGUROBI UBOptimal ValueParabolic UB051015202530Time-80-60-40-200Objective Value1886GUROBI LBGUROBI UBOptimal ValueParabolic UB0510152025303540Time-80-60-40-200Objective Value1913GUROBI LBGUROBI UBOptimal ValueParabolic UB050100150Time-60-50-40-30-20-10Objective Value1940GUROBI LBGUROBI UBOptimal ValueParabolic UB020406080100Time-200-150-100-500Objective Value1967GUROBI LBGUROBI UBOptimal ValueParabolic UBTitle Suppressed Due to Excessive Length

21

Fig. 3: Bounds offered by GUROBI 9.0 and the penalized parabolic relaxation for QPLIB cases.

(without intrusive means) which has been widely studied in the literature of control
[39, 45]. In this case study, we cast system identiﬁcation as a non-convex QCQP
and evaluate ability of the proposed parabolic relaxation in solving very large-scale
instances of this problem. We consider identiﬁcation problems with over 10,000 vari-
ables for which:

– SDP relaxation is intractable.
– Both variants of SOCP relaxation as well as S (2+m)

n,m

fail to produce feasible points

due to the absence of comprehensive boundary property (CBP). 1

– Parabolic relaxation is consistently successful in recovering the ground truth fea-

sible point.

To formulate the problem, consider a discrete-time linear system described by the

following dynamical equations:

x[t + 1] = Ax[t] + Bu[t],

t ∈ T ,

(53)

throughout a time horizon T = {1, 2, . . . , |T |}, where:

– {x[t] ∈ Rn}t∈T are the state vectors that are known at times t ∈ K, where K ⊆ T

and unknown at times t ∈ U (cid:44) T \ K.

– {u[t] ∈ Rm}t∈T are the known control command vectors that are enforced exter-

nally to regulate the behavior of system.

– A ∈ Rn×n and B ∈ Rn×m are ﬁxed unknown matrices.

Our goal is to determine the pair of ground truth matrices (Atrue, Btrue), given a
sample trajectory of the control commands {u[t] ∈ Rm}t∈T and the incomplete state
vectors {x[t] ∈ Rn}t∈K. This problem amounts to the following optimization with

1 S(2+m)
n,m

and CBP are deﬁned in [32].

051015202530Time-50-40-30-20Objective Value1922GUROBI LBGUROBI UBOptimal ValueParabolic UB0100200300400500Time-80-70-60-50-40Objective Value1931GUROBI LBGUROBI UBOptimal ValueParabolic UB020406080100120Time-60-40-200Objective Value0975GUROBI LBGUROBI UBOptimal ValueParabolic UB05101520253035Time-20-18-16-14-12-10Objective Value1423GUROBI LBGUROBI UBOptimal ValueParabolic UB22

Ramtin Madani et al.

non-convex quadratic constraints:

{x[t], z[t] ∈ Rn}t∈U , A ∈ Rn×n, B ∈ Rn×m

ﬁnd
subject to x[t + 1] = Ax[t] + Bu[t],
x[t + 1] = Ax[t]
+ Bu[t],
(cid:124) (cid:123)(cid:122) (cid:125)
non-convex

(54a)

(54b)

(54c)

t ∈ K,

t ∈ U.

Observe that the constraint (54b) is linear because x[t] is known for every t ∈ K,
whereas the constraint (54c) is non-convex due to the bilinear term Ax[t] because
when t ∈ U, both x[t] and A are unknown. Problem (54a) – (54c) can be cast in the
lifted form with respect to the matrix variable:

Y (cid:44)

(cid:104)

A(cid:62) (cid:2)x[t](cid:3)

(cid:105)(cid:62)

t∈U

∈ R(n+|U |)×n,

(55)

which is composed of the matrix A and all of the unknown state vectors:

ﬁnd

{x[t] ∈ Rn}t∈U , A ∈ Rn×n, B ∈ Rn×m,

¯A ∈ Sn,

¯X ∈ S|U |

subject to x[t + 1] = Ax[t] + Bu[t],

(cid:34) ¯A (cid:2)x[t + 1] − Bu[t](cid:3)
∗

¯X

t∈U

(cid:35)

=

(cid:34)

A
(cid:2)x[t](cid:3)(cid:62)

t∈U

(56a)

(56b)

t ∈ K,

(cid:35)

(cid:104)
A(cid:62) (cid:2)x[t](cid:3)

t∈U

(cid:105)

,

t ∈ U,

(56c)

(cid:2)x[t](cid:3)

where ¯A ∈ Sn and ¯X ∈ S|U | account for AA(cid:62) and (cid:2)x[t](cid:3)(cid:62)
t∈U , respec-
tively. Transformation of “=” to “(cid:23)” results in an SDP relaxation of this problem.
However, this relaxation can be intractable because of the dense bipartite clique
(cid:2)x[t + 1] − Bu[t](cid:3)
t∈U . Alternatively, parabolic relaxation allows us to omit all of
the off-diagonal elements of ¯A and ¯X from the lifted formulation. Hence, in order to
formulate parabolic relaxation of problem (54a) – (54b) we only need to introduce a
modest number of auxiliary variables:
– Deﬁne ¯a = diag{ ¯A} ∈ Rn as the variable whose k-th element represents (cid:107)A(cid:62)ek(cid:107)2
2,

t∈U

where {ek}n

k=1 is the standard base in Rn.

– Deﬁne ¯x = diag{ ¯X} ∈ R|U | as the variable whose elements represent (cid:107)x[t](cid:107)2

2 for

all t ∈ U,

Then, problem (54a) – (54b) can be relaxed as:

ﬁnd

{x[t] ∈ Rn}t∈U , A ∈ Rn×n, B ∈ Rn×m,

¯a ∈ Rn,

¯x ∈ R|U |

(cid:0)x[t + 1] − Bu[t](cid:1) ≥ (cid:107)A(cid:62)ek + x[t](cid:107)2
(cid:0)x[t + 1] − Bu[t](cid:1) ≥ (cid:107)A(cid:62)ek − x[t](cid:107)2

subject to x[t + 1] = Ax[t] + Bu[t],
¯ak + ¯xt + 2 e(cid:62)
k
¯ak + ¯xt − 2 e(cid:62)
k
¯xt ≥ (cid:107)x[t](cid:107)2
2,
¯ak ≥ (cid:107)A(cid:62)ek(cid:107)2
2,

t ∈ K,

(57a)

(57b)

2, t ∈ U, k ∈ {1, . . . , n}, (57c)
2, t ∈ U, k ∈ {1, . . . , n}, (57d)
(57e)

t ∈ U,

k ∈ {1, . . . , n}.

(57f)

Title Suppressed Due to Excessive Length

23

In this case study, we consider system identiﬁcation problems with n = 16, m =
14, |T | = 801 and K = {1, 5, 9, . . . , 801}, i.e., one in every four state vectors is
assumed known. The resulting QCQP variable x is 10, 096-dimensional. The ground
truth matrices are chosen as follows:

– Every element of A − In is uniformly chosen from the interval [−0.25, +0.25]
– Elements of B ∈ R16×14 have zero-mean standard normal distribution.
– Elements of x[1] are uniformly chosen from the interval [0.5, 1.5].
– For every t ∈ T , we have u[t] = Fx[t] + w[t], where the elements of w[t] have
independent Gaussian distribution with zero mean and standard deviation 0.1.
Additionally, F = −(Im + B(cid:62)P)−1B(cid:62)PA is an optimal LQR controller with P
representing the unique positive-deﬁnite solution to the Riccati equation:

A(cid:62)PA + I n − P = A(cid:62)PB(I n + B(cid:62)PB)−1B(cid:62)PA.

(58)

We generate 15 random systems and solve the resulting problem, using the sequential
Algorithm 1 with the initial point ˇA = I n and ˇx[t] = 0n for every t ∈ U. Figure 4
illustrates the convergence of the error function:

1
n

(cid:107)A − Atrue(cid:107)F +

1
√
nm

(cid:107)B − Btrue(cid:107)F,

(59)

for the 15 instances. Each round of the penalized parabolic is solved in less than 15
seconds. Due to the bipartite n × |U| clique in (56c), SDP relaxation is intractable
even when sparsity is exploited. Moreover, due to the absence of CBP, neither of
SOCP relaxations variants in [32, Section 5] converge to the true system matrices.

Fig. 4: The performance of the sequential Algorithm 1 for system identiﬁcation.

4 Conclusions

In the ﬁrst this work [32], we proposed a computationally efﬁcient convex relax-
ation, named parabolic relaxation, to ﬁnd near optimal solutions for quadratically-
constrained quadratic programming. In cases where the relaxation is not exact, a pe-
nalized relaxation is developed to steer the relaxation towards a feasible solution. To

010203040506070Rounds10-610-410-2100Error24

Ramtin Madani et al.

improve the quality of the recovered solution, we propose a sequential scheme that
starts from an arbitrary point (feasible or infeasible) and solves a sequence of penal-
ized problems to recover a feasible near optimal solution. We prove the convergence
of the sequential procedure to a KTT point. The experimental results show the effec-
tiveness of our proposed approach on benchmark QCQP cases as well as the problem
of identifying linear dynamical systems.

5 Acknowledgments

We are grateful to GAMS Development Corporation for providing us with unre-
stricted access to a full set of solvers throughout the project.

References

1. Agler J, Helton W, McCullough S, Rodman L (1988) Positive semideﬁnite matri-
ces with a given sparsity pattern. Linear Algebra and its Applications 107:101–
149

2. Ahmadi AA, Hall G (2017) Sum of squares basis pursuit with linear and second
order cone programming. Algebraic and geometric methods in discrete mathe-
matics 685:27–53

3. Ahmadi AA, Majumdar A (2019) DSOS and SDSOS optimization: more
tractable alternatives to sum of squares and semideﬁnite optimization. SIAM
Journal on Applied Algebra and Geometry 3(2):193–230

4. Ahmadi AA, Dash S, Hall G (2017) Optimization over structured subsets of posi-
tive semideﬁnite matrices via column generation. Discrete Optimization 24:129–
151

5. Atamt¨urk A, G´omez A (2019) Rank-one convexiﬁcation for sparse regression.

arXiv preprint arXiv:190110334

6. Barvinok AI (1995) Problems of distance geometry and convex properties of

quadratic maps. Discrete & Computational Geometry 13(2):189–202

7. Borwein J, Wolkowicz H (1981) Regularizing the abstract convex program. Jour-

nal of Mathematical Analysis and Applications 83(2):495–530

8. Boumal N, Voroninski V, Bandeira A (2016) The non-convex Burer-Monteiro
approach works on smooth semideﬁnite programs. Advances in Neural Informa-
tion Processing Systems 29

9. Burer S, Monteiro RD (2003) A nonlinear programming algorithm for solving
semideﬁnite programs via low-rank factorization. Mathematical Programming
95(2):329–357

10. d’Aspremont A, El Ghaoui L, Jordan MI, Lanckriet GR (2007) A direct formula-
tion for sparse PCA using semideﬁnite programming. SIAM Review 49(3):434–
448

11. De Klerk E (2010) Exploiting special structure in semideﬁnite programming:
A survey of theory and applications. European Journal of Operational Research
201(1):1–10

Title Suppressed Due to Excessive Length

25

12. Drusvyatskiy D, Wolkowicz H, et al. (2017) The many faces of degeneracy in
conic optimization. Foundations and Trends® in Optimization 3(2):77–170
13. Erdogdu MA, Ozdaglar A, Parrilo PA, Vanli ND (2021) Convergence rate of
block-coordinate maximization Burer-Monteiro method for solving large SDPs.
Mathematical Programming pp 1–39

14. Fattahi S, Sojoudi S (2018) Data-driven sparse system identiﬁcation. In: 56th An-
nual Allerton Conference on Communication, Control, and Computing (Aller-
ton), IEEE

15. Fattahi S, Fazelnia G, Lavaei J, Arcak M (2018) Transformation of optimal cen-
tralized controllers into near-globally optimal static distributed controllers. IEEE
Transactions on Automatic Control 64(1):66–80

16. Fazlyab M, Morari M, Pappas GJ (2020) Safety veriﬁcation and robustness anal-
ysis of neural networks via quadratic constraints and semideﬁnite programming.
IEEE Transactions on Automatic Control

17. Frank M, Wolfe P (1956) An algorithm for quadratic programming. Naval re-

search logistics quarterly 3(1-2):95–110

18. Fujisawa K, Kim S, Kojima M, Okamoto Y, Yamashita M (2009) User’s manual
for SparseCoLO: Conversion methods for sparse conic-form linear optimization
problems. Report B-453, Dept of Math and Comp Sci Japan, Tech Rep pp 152–
8552

19. Fukuda M, Kojima M, Murota K, Nakata K (2001) Exploiting sparsity in
semideﬁnite programming via matrix completion I: General framework. SIAM
Journal on Optimization 11(3):647–674

20. Furini F, Traversi E, Belotti P, Frangioni A, Gleixner A, Gould N, Liberti L,
Lodi A, Misener R, Mittelmann H, Sahinidis N, Vigerske S, Wiegele A (2019)
QPLIB: A library of quadratic programming instances. Mathematical Program-
ming Computations 11:237–310, URL http://qplib.zib.de/

21. Gatermann K, Parrilo PA (2004) Symmetry groups, semideﬁnite programs, and

sums of squares. Journal of Pure and Applied Algebra 192(1-3):95–128

22. Grone R, Johnson CR, S´a EM, Wolkowicz H (1984) Positive deﬁnite comple-
tions of partial Hermitian matrices. Linear algebra and its applications 58:109–
124

23. Hazan E (2008) Sparse approximate solutions to semideﬁnite programs. In: Latin

American symposium on theoretical informatics, Springer, pp 306–316

24. Journ´ee M, Bach F, Absil PA, Sepulchre R (2010) Low-rank optimization
on the cone of positive semideﬁnite matrices. SIAM Journal on Optimization
20(5):2327–2351

25. Kheirandishfard M, Zohrizadeh F, Adil M, Madani R (2018) Convex relaxation
of bilinear matrix inequalities part ii: Applications to optimal control synthesis.
In: 2018 IEEE Conference on Decision and Control (CDC), IEEE, pp 75–82
26. Kim S, Kojima M, Mevissen M, Yamashita M (2011) Exploiting sparsity in linear
and nonlinear matrix inequalities via positive semideﬁnite matrix completion.
Mathematical programming 129(1):33–68

27. Kulis B, Surendran AC, Platt JC (2007) Fast low-rank semideﬁnite programming
for embedding and clustering. In: Artiﬁcial Intelligence and Statistics, PMLR, pp
235–242

26

Ramtin Madani et al.

28. Kungurtsev V, Marecek J (2020) A two-step pre-processing for semideﬁnite pro-
gramming. In: 2020 59th IEEE Conference on Decision and Control (CDC),
IEEE, pp 384–389

29. Lasserre J, Toh KC, Yang S (2017) A bounded degree SOS hierarchy for polyno-
mial optimization. EURO Journal on Computational Optimization 5(1-2):87–117
30. Lemon A, So AMC, Ye Y (2016) Low-rank semideﬁnite programming: Theory
and applications. Foundations and Trends® in Optimization 2(1-2):1–156
31. Liu DC, Nocedal J (1989) On the limited memory BFGS method for large scale

optimization. Mathematical programming 45(1):503–528

32. Madani R, Ashraphijuo M, Kheirandishfard M, Atamt¨urk A (2022) Parabolic re-
laxation for quadratically-constrained quadratic programming – part I. Submitted
to Mathematical Programming

33. Majumdar A, Hall G, Ahmadi AA (2020) Recent scalability improvements for
semideﬁnite programming with applications in machine learning, control, and
robotics. Annual Review of Control, Robotics, and Autonomous Systems 3:331–
360

34. Mason RP, Papachristodoulou A (2014) Chordal sparsity, decomposing SDPs
and the Lyapunov equation. In: 2014 American control conference, IEEE, pp
531–537

35. MOSEK A (2017) The MOSEK optimization toolbox for MATLAB manual.
Version 8.1. URL http://docs.mosek.com/8.1/toolbox/index.
html

36. O’donoghue B, Chu E, Parikh N, Boyd S (2016) Conic optimization via operator
splitting and homogeneous self-dual embedding. Journal of Optimization Theory
and Applications 169(3):1042–1068

37. Pataki G (1998) On the rank of extreme matrices in semideﬁnite programs and
the multiplicity of optimal eigenvalues. Mathematics of Operations Research
23(2):339–358

38. Pataki G (2013) Strong duality in conic linear programming: facial reduction
and extended duals. In: Computational and Analytical Mathematics, Springer,
pp 613–634

39. Pereira J, Ibrahimi M, Montanari A (2010) Learning networks of stochastic dif-
ferential equations. In: Advances in Neural Information Processing Systems, pp
172–180

40. Permenter FN (2017) Reduction methods in semideﬁnite and conic optimization.

PhD thesis, Massachusetts Institute of Technology

41. Qin C, Dvijotham KD, O’Donoghue B, Bunel R, Stanforth R, Gowal S, Uesato
J, Swirszcz G, Kohli P (2018) Veriﬁcation of non-linear speciﬁcations for neural
networks. In: International Conference on Learning Representations

42. Raghunathan A, Steinhardt J, Liang PS (2018) Semideﬁnite relaxations for cer-
tifying robustness to adversarial examples. Advances in Neural Information Pro-
cessing Systems 31

43. Recht B, Fazel M, Parrilo PA (2010) Guaranteed minimum-rank solutions of
linear matrix equations via nuclear norm minimization. SIAM review 52(3):471–
501

Title Suppressed Due to Excessive Length

27

44. Rotkowitz M, Lall S (2005) A characterization of convex problems in decentral-

ized control. IEEE transactions on Automatic Control 50(12):1984–1996

45. Sarkar T, Rakhlin A (2018) How fast can linear dynamical systems be learned?

arXiv preprint arXiv:181201251

46. Sun Y, Vandenberghe L (2015) Decomposition methods for sparse matrix near-
ness problems. SIAM Journal on Matrix Analysis and Applications 36(4):1691–
1717

47. Vallentin F (2009) Symmetry in semideﬁnite programs. Linear Algebra and its

Applications 430(1):360–369

48. Wang YS, Matni N, Doyle JC (2018) Separable and localized system-level
synthesis for large-scale systems. IEEE Transactions on Automatic Control
63(12):4234–4249

49. Wang YS, Matni N, Doyle JC (2019) A system level approach to controller syn-

thesis. IEEE Transactions on Automatic Control

50. Weisser T, Lasserre JB, Toh KC (2018) Sparse-BSOS: a bounded degree SOS
hierarchy for large scale polynomial optimization with sparsity. Mathematical
Programming Computation 10(1):1–32

51. Yadav A, Altun T, Madani R, Davoudi A (2020) Macromodeling of electric ma-

chines from ab initio models. IEEE Transactions on Energy Conversion

52. Yang L, Sun D, Toh KC (2015) SDPNAL+: a majorized semismooth Newton-CG
augmented lagrangian method for semideﬁnite programming with nonnegative
constraints. Mathematical Programming Computation 7(3):331–366

53. Yurtsever A, Udell M, Tropp J, Cevher V (2017) Sketchy decisions: Convex
low-rank matrix optimization with optimal storage. In: Artiﬁcial intelligence and
statistics, PMLR, pp 1188–1196

54. Zhao XY, Sun D, Toh KC (2010) A Newton-CG augmented Lagrangian method
for semideﬁnite programming. SIAM Journal on Optimization 20(4):1737–1765
55. Zheng Y, Mason RP, Papachristodoulou A (2016) A chordal decomposition ap-
proach to scalable design of structured feedback gains over directed graphs. In:
2016 IEEE 55th conference on decision and control (CDC), IEEE, pp 6909–6914
56. Zheng Y, Mason RP, Papachristodoulou A (2017) Scalable design of structured
controllers using chordal decomposition. IEEE Transactions on Automatic Con-
trol 63(3):752–767

