2
2
0
2

y
a
M
7
2

]

G
L
.
s
c
[

1
v
2
3
8
3
1
.
5
0
2
2
:
v
i
X
r
a

Counterfactual Analysis in Dynamic Models:
Copulas and Bounds

Martin B. Haugh
Imperial College
m.haugh@imperial.ac.uk

Raghav Singal
Dartmouth College
singal@dartmouth.edu

Abstract

We provide an explicit model of the causal mechanism in a structural causal model
(SCM) with the goal of estimating counterfactual quantities of interest (CQIs).
We propose some standard dependence structures, i.e. copulas, as bases cases
for the causal mechanism. While these base cases can be used to construct more
interesting copulas, there are uncountably many copulas in general and so we
formulate optimization problems for bounding the CQIs. As our ultimate goal
is counterfactual reasoning in dynamic models which may have latent-states,
we show by way of example that ﬁltering / smoothing / sampling methods for
these models can be integrated with our modeling of the causal mechanism.
Speciﬁcally, we consider the “cheating-at-the-casino” application of a hidden
Markov model and use linear programming (LP) to construct lower and upper
bounds on the casino’s winnings due to cheating. These bounds are considerably
tighter when we constrain the copulas in the LPs to be time-independent. We
can characterize the entire space of SCMs obeying counterfactual stability (CS),
and we use it to negatively answer the open question of Oberst and Sontag [18]
regarding the uniqueness of the Gumbel-max mechanism for modeling CS. Our
work has applications in epidemiology and legal reasoning, and more generally
in counterfactual off-policy evaluation, a topic of increasing interest in the
reinforcement learning community.

History. This version: May 26, 2022

1

Introduction

It’s been discovered that a casino has been cheating at one of its games over a period of several
weeks. Instead of using a fair die, it has been found to adopt a (hidden) Markov policy whereby it
occasionally swaps out the fair die for a loaded die. The earnings of the casino in this game over the
time period in question are known and a court is now asking how much of these earnings were due to
cheating? Or consider someone who has recently died from cancer. The exact progression of her
disease is unknown. What is known, however, is that over a period of time prior to her diagnosis,
her insurance company adopted a strategy of denying her regular scans for this type of cancer even
though these scans should have been covered by her policy. Had these scans gone ahead, the cancer
may have been found early and the patient’s life saved. Now a court wants to know the probability
that her life would have been saved had the routine scans been permitted?

Answering these questions requires a counterfactual analysis as they require us to imagine a world
where a certain policy and outcome took place given that a different policy and outcome were actually
observed. Such questions belong on the third rung of Pearl’s so-called ladder of causation [20] and
require a causal mechanism to answer them. Our goal in this paper is to provide a simple framework
for answering these kinds of questions in dynamic latent-state Markov models. In particular, we can
summarize our contributions as follows:

 
 
 
 
 
 
(i) We propose a structural causal model (SCM) which explicitly recognizes that modeling the
full range of counterfactual possibilities at each node in the SCM requires specifying a joint
probability distribution over counterfactual random variables at that node.

(ii) Specifying the aforementioned probability distribution amounts to specifying the dependence
structure or copula of the counterfactual random variables. We therefore introduce the language
of copulas to this kind of analysis and describe some well known copulas as benchmarks.

(iii) Because uncountably many copulas can be consistent with observational data, in general we
can only hope to provide bounds on the counterfactual quantities of interest (CQI’s), e.g. casino
earnings due to cheating, or the probability the patient would have survived had scans been
permitted. We therefore formulate optimization problems to construct lower and upper bounds
on the CQI’s. Because our problems are dynamic in nature, we impose an easily justiﬁable time
consistency requirement on the copulas and this results in considerably tighter bounds.

(iv) Our modeling approach can easily handle constraints on the causal mechanism. For example,
we show that the recently proposed counterfactual stability (CS) property [18] can be modeled
(at least in our application) via linear constraints in our optimization problems. We also resolve
some open questions of [18] regarding (a) the sensitivity of the CQI to the choice of SCM and
(b) whether other approaches (beyond the use of the Gumbel-max “trick” they propose) can be
found to generate counterfactuals while satisfying the CS property.

(v) We show by way of example that ﬁltering / smoothing / sampling methods for dynamic latent-
state models can be integrated with our modeling of the causal mechanism. Speciﬁcally,
we consider the “cheating-at-the-casino” hidden Markov model (HMM) application and use
linear programming (LP) to construct lower and upper bounds on the casino’s winnings due
to cheating. To the best of our knowledge, we are the ﬁrst to provide bounds on a CQI in a
dynamic latent-state model.

One potential weakness of our approach is that we assume the model structure is known. This
assumption is necessary to perform the ﬁltering and smoothing steps that calculate the distribution
of the latent-states conditional on the observations. Without this step we cannot compute bounds
on the CQI’s. This is not a major limitation, however, as there are many interesting and successful
applications, e.g. [3, 4], where the model structure (though perhaps not the model parameters) is
assumed to be known. (If the model parameters are not known they can easily be estimated given
sufﬁcient data.) Moreover, if our focus is not on bounds but rather on the investigation of interesting
causal mechanisms then our framework would still be useful for counterfactual off-policy evaluation
irrespective of whether or not latent variables are present.

There are two streams of work that are closely related to our work. The ﬁrst relates to work by
researchers including Pearl and colleagues, e.g. [2, 22, 11, 6, 19, 16], on constructing bounds on
CQI’s. These papers focus on static models and generally consider binary variables. They also use
LP techniques to construct bounds and as these problems are much smaller they have been able to
develop analytic expressions for these bounds. The tightness of these bounds then depends on whether
observational and / or experimental data are available and whether structural assumptions such as
monotonicity are imposed. With the exception of [16], these papers do not assume the structure
of the graph is known. In contrast, we do assume the graph structure is known but only so as to
perform the ﬁltering and smoothing steps for computing the hidden-state distribution conditional on
the observations. We also explicitly discuss the dependence structure, i.e. copulas, and identify some
interesting copulas as base cases for our analysis.

A second stream of related research [5, 13, 18, 23] is more recent and concerns counterfactual
off-policy evaluation for dynamic models, usually in a reinforcement learning context. These papers
recognize the need to model the causal mechanism but they generally ﬁx a single causal mechanism
[5] or invoke a possibly desirable property such as CS [18, 23] and then implicitly ﬁx the causal
mechanism via the Gumbel-max trick. [13] also aims to extend the Gumbel-max approach but their
choice of causal mechanism is one that minimizes the variance when estimating CQI’s. In summary,
none of these approaches explicitly account for all of the possible causal mechanisms, i.e. joint
distributions or copulas, and therefore they don’t consider the construction of lower and upper bounds
on the CQI’s.

2

2 Preliminaries: Structural Causal Models

We ﬁrst review structural causal models (SCMs) in the context of directed acyclic graphs (DAGs),
e.g. [19]. Each node in the DAG represents a random variable. The random variables are divided
into two types: endogenous variables Vi which are modeled explicitly, and exogenous variables
which are not modeled explicitly and represent background noise. Each endogenous variable Vi has a
(potentially empty) set of endogenous parent variables pai, an exogenous parent variable Ui, and an
associated function fi which uniquely determines Vi as a function of pai and Ui, i.e. Vi = fi(pai, Ui).
Throughout the paper we will make the following assumption.
Assumption 1. The exogenous variables are independent.

Assumption 1 is consistent with the assumption of no unobserved confounding in our SCMs. This
is important for dynamic latent state models where we need to know the structure of the graph for
ﬁltering, smoothing and sampling. Figure 1a displays a simple example of an SCM. This SCM is
intended to model the outcome VO of a surgical procedure on a patient. This outcome depends on
the hospital VH where it is performed, the surgeon VS who performs it and an exogenous variable
UO representing some background noise. The endogenous variables are therefore VO, VH and VS.
There are also exogenous variables UH and US which essentially model the random mechanism
determining the hospital where the procedure is performed and the surgeon who performs it. None
of the exogenous variables have parents and because of Assumption 1, each exogenous variable
has a single outbound arrow to the single endogenous variable with which it is associated. (This
also implies that VH and VS are independent so that the possible surgeons work across all possible
hospitals and knowing the hospital where the procedure is performed tells us nothing regarding the
identity of the surgeon who performs it.) Finally, the joint distribution of the endogenous variables is
completely determined by the marginal distribution of the exogenous variables.

We remark it is quite common and wlog when dealing with association and intervention questions, i.e.
questions from the ﬁrst two rungs, to take the exogenous variables to be U (0, 1) random variables.
This follows because we can use the inverse-transform method from Monte-Carlo simulation (e.g.
[9]) to generate each Vi with the correct probability distribution conditional on pai. We refer to [19]
for further details on SCMs.

UH

US

UH

VH

UO

VS

VH

UnO
O

US

VS

VO

(a)

VO

(b)

Figure 1: (a) A simple SCM modeling the outcome VO of a surgical procedure on a patient. The
outcome depends on the hospital VH where it is performed, the surgeon VS who performs it and
an exogenous variable UO. (b) An SCM for the same problem but now explicitly recognizing (via
boldface font) that UnO

O is a vector of nO U (0, 1) random variables.

2.1 Counterfactual Reasoning

Suppose there are three possible hospitals, two possible surgeons and two possible outcomes (“Good”,
“Bad’) for the surgical procedure. We observe the ﬁrst surgeon performed the procedure in the ﬁrst
hospital and that the outcome was “Bad”. A natural question to ask is why this outcome occurred?
Was it because of the hospital or surgeon or some combination of the two? Or perhaps it was just
“bad luck”? More speciﬁcally, we might be interested in answering the counterfactual question:

“Would the outcome have been “Good” if the procedure was performed by surgeon # 2 in hospital # 3
given that it was actually performed by surgeon # 1 in hospital # 1 and the outcome was “Bad”?

It’s well known how to answer a question like this in an SCM and there are three steps required:

1. Abduction. Use the evidence (hospital # 1, surgeon # 1 and outcome “Bad”) to infer the

conditional distribution of UO.

3

2. Action. Modify the SCM by removing the arrows from UH to VH and from US to VS and

replace them with the appropriate functions VH = 3 and VS = 2.

3. Prediction. Use the modiﬁed model and inferred distribution of UO to predict VO.

These steps are widely understood - e.g. see [19] - but in our opinion there seems to be some confusion
over modeling the exogenous variables to reﬂect the full range of possible causal mechanisms. This
is perhaps not too surprising as most causal questions focus on the ﬁrst two rungs (“Association” and
“Intervention”) of Pearl’s “ladder of causation”. While modeling the distribution of the exogenous
variables is clearly important for answering association and intervention questions, we need to go
further than this when answering counterfactual questions which are represented by the third rung
on Pearl’s ladder. In particular, we need to model the full causal mechanism and we will speak of
exogenous random vectors rather than exogenous random variables.

2.2 The Causal Mechanism

Let ni denote the number of possible settings of pai with the understanding that if Vi has no parents
then ni = 1. Referring to our surgical procedure example in Figure 1a for example, VO has 2
endogenous parents, VH and VS, which have 3 and 2 possible values, respectively. Therefore paO
has nO = 3 × 2 = 6 possible settings. In general, each possible setting of pai requires its own
separate U (0, 1) random variable. This is most easily seen by recognizing that VO | (paO = s)
deﬁnes a distinct random variable for each s = 1, . . . , nO. Since nO = 6 in our example, there are
six distinct random variables that must be modeled. Moreover, these six random variables might be
independent, or they might display positive or negative dependence. One way to handle this is to
associate each VO | (paO = s) with a distinct U (0, 1) random variable. The dependence structure
among these uniforms is then what determines the dependence structure among the elements of
{Vi | (pai = s)}ni
s=1. Returning to our general SCM notation, we can therefore write

Vi = fi(pai, Uni
i )
ni(cid:88)

=

fi,s(Ui,s) 1{pai=s}

(1)

s=1

for an arbitrary endogenous node Vi where Uni
:= [Ui,1 · · · Ui,ni] is an ni × 1 vector of U (0, 1)
i
random variables, pai = s represents (via a slight abuse of notation) the event that pai take on their
sth possible setting, and 1{·} denotes the indicator function. The representation (1) allows us to
model {Vi | (pai = s)}ni
s=1 and capture any dependence structure among these ni random variables
by specifying the joint multivariate distribution of Uni
i are
i
known (they are all U (0, 1)), specifying the multivariate distribution of Uni
i amounts to specifying
its dependence structure or copula.
For example, if the Ui,s’s are mutually independent and pai = s(cid:48) was observed for some s(cid:48), then the
abduction step will infer the distribution of Ui,s(cid:48) conditional on the observed data. This will tell us
nothing about the Ui,s’s for s (cid:54)= s(cid:48), however. Alternatively, if Ui,1 = Ui,2 = · · · = Ui,ni then this
models perfect positive dependency and inferring the conditional distribution of Ui,s(cid:48) in the abduction
step amounts to simultaneously inferring the conditional distribution of all the Ui,s’s.

. Since the univariate marginals of Uni

These considerations lead to a deﬁnition of the structural causal model where we explicitly account
for this mechanism.
Deﬁnition 1. A structural causal model (SCM) over endogenous random variables V =
(V1, . . . , Vm) is given by a DAG G over nodes V, exogenous random vectors U = (Un1
1 , . . . , Unm
m )
and functions f1, . . . , fm such that Vi = fi(pai, Uni
i ) where pai ⊂ V are the parents of Vi in G.
Each Uni
is an ni × 1 vector of U (0, 1) random variables with joint distribution Ci. Finally each fi
i
can be represented as in (1).
Remark 1. Because of our ultimate focus on dynamic latent space models (see §4) we will continue
to invoke Assumption 1 throughout the paper. In the context of Deﬁnition 1, this means we can assume
that the Uni

i ’s in our SCM are independent, i.e. Uni

is independent of Unj
j

for i (cid:54)= j.

i

Figure 1b displays the SCM for our surgical procedure example. The only difference between the
SCM in Figure 1a and Figure 1b is that in the latter ﬁgure we use UnO
O rather than UO to represent
the exogenous noise associated with VO. Since neither VH nor VS have parents we can continue to

4

use UH and US to represent their exogenous noise variables. (To be consistent with Deﬁnition 1, we
should use UnH

S but since nH = nS = 1 we lose nothing in sticking with UH and US.)

H and UnS

We emphasize that for causal questions related to association and intervention there is no need to
introduce Uni
i ’s and Deﬁnition 1. This is because only counterfactual queries require the abduction
step. That said, in many applications it seems that a single U (0, 1) random variable Ui is assumed for
all counterfactual queries. This amounts to assuming Ui,1 = Ui,2 = · · · = Ui,ni which, as mentioned
earlier, leads to extreme positive dependency among the counterfactuals. While we haven’t seen an
SCM deﬁned explicitly via vectors of U (0, 1) random variables as in Deﬁnition 1, it should come
as no surprise to those who work with counterfactuals and so we make no claims here regarding
originality. Indeed we wouldn’t be surprised if other researchers had also represented an SCM via (1)
and Deﬁnition 1. For example, rather than introducing Uni
i ’s as we do above, [2] introduce so-called
response function variables. Ultimately these response-function variables achieve the same goal as
our representation in (1) although the approach seems quite difﬁcult to follow.

2.3 An Alternative Deﬁnition of the SCM

i

To simplify our notation we deﬁne V (s)
to denote the random variable Vi | (pai = s). While
Deﬁnition 1 and the Uni
i ’s are useful from a conceptual point of view, it will be more convenient
to work with an alternative deﬁnition of the SCM. This is because in discrete-state space models
there will be inﬁnitely many joint distributions of Uni
that all lead to the same joint distribution
i
of {V (s)
i }ni
s=1 does not uniquely identify the
joint distribution of Uni
. This is a consequence of Sklar’s Theorem from the theory of copulas
i
and is discussed in further detail in Appendix A. We will therefore take a more direct approach by
directly modeling the joint distribution of {V (s)
s=1. This leads to an alternative (though essentially
equivalent) and more practically useful deﬁnition of an SCM.

s=1. In other words, the joint distribution of {V (s)

i }ni

i }ni

Deﬁnition 2. A structural causal model (SCM) over endogenous random variables V =
(V1, . . . , Vm) is given by a DAG G over nodes V. Each endogenous node Vi is associated with
an ni-dimensional cumulative distribution function (CDF) Π(i) which is the joint CDF of {V (s)
i }ni
where ni is the number of possible settings of pai, the parents of Vi in G.

s=1

As with Deﬁnition 1, we haven’t seen an SCM deﬁned explicitly via the joint CDF’s of the {V (s)
s=1.
But again, we make no claims of originality here since a representation such as this would be obvious
to many. Indeed Pearl and his colleagues [2, 16, 19, 22], for example, work implicitly with such a
representation when computing bounds on CQI’s although they typically have static models with
binary variables in mind. (In our our cheating-at-the-casino application in §4, we will use this
representation at all the emission nodes of the HMM and use it to bound the expected winnings of the
casino due to cheating.)

i }ni

Dropping the superscript i, we refer to Π as the counterfactual joint CDF of node V , i.e. the joint
CDF of {V (s)}n
s=1, and we use π for the corresponding probability mass function (PMF). That is,

Π1...n(v1, . . . , vn) = P(V (1) ≤ v1, . . . , V (n) ≤ vn)
π1...n(v1, . . . , vn) = P(V (1) = v1, . . . , V (n) = vn).

Note that we can only ever observe one of these random variables at a time.

Before proceeding we make two observations:

1. We always assume the univariate marginal distributions of Π are known. This is a conse-
quence of knowing the data-generating mechanism. For example, in our surgical procedure
application we could easily estimate P(VO = Good | VH = 1, VS = 2) given sufﬁcient data
from procedures performed by similar surgeons at similar hospitals and on similar patients.
But in general we assume we don’t know the joint distribution {VO | ((VH , VS) = s)}6
s=1.
In our casino application of §4, this amounts to knowing each of the initial, transition and
emission distributions in the HMM. They can all be estimated given sufﬁcient data.

2. As noted previously, a key step in estimating a counterfactual is the abduction step. In our
SCM setting, this step can be executed as follows. If we observe pa(V ) = s(cid:48) and V (s(cid:48)) = h

5

then it is a simple exercise in probability to update Π (or π). For example,

(cid:16)

(cid:17)
v1, . . . , vs(cid:48)−1, vs(cid:48)+1, . . . , vn) | V (s(cid:48)) = h

P

=

π1...n(v1, . . . , vs(cid:48)−1, h, vs(cid:48)+1, . . . , vn)
πs(cid:48)(h)

where πs(cid:48) is the marginal PMF of Vs(cid:48) which is easily computed from π. More generally, we
may only have a distribution for pa(V ) and / or V conditional on a sequence of observations.
We can then use further conditioning arguments to compute π(cid:48) (the distribution of {V (s)}n
s=1
conditional on the observations) in terms of π. We do this in our HMM application in §4.

While we don’t know the counterfactual distribution in general, we may be able to use domain speciﬁc
knowledge to specify it or constrain it in some way. This is the subject of §3.

3 Modeling the Counterfactual Joint Distribution of V

We begin in §3.1 by identifying some important dependence structures or copulas that form interesting
base cases. Then in §3.2 we discuss how bounds on counterfactual quantities might be computed.
When properties like counterfactual stability [18] are imposed, then tighter bounds are obtained.

3.1 Specifying a Dependence Structure / Copula

Because the univariate marginal distributions of Π are known, we only need to specify the dependence
structure or copula of Π. We consider three important base cases:

Independence.
distribution by ΠI and it models independence among the V (s)’s.

In this case Π1...n(v1, . . . , vn) = Π1(v1) × · · · × Πn(vn). We denote this joint

In this case Π1...n(v1, . . . , vn) = min{Π1(v1), . . . , Πn(vn)}. We denote this
Comonotonicity.
joint distribution by ΠP and it represents the case of extreme positive dependence among the V (s)’s.

Countermonotonicity. This only applies in the n = 2-dimensional case and then Π12(v1, v2) =
max{0, Π1(v1) + Π2(v2) − 1}. We denote this joint distribution by ΠN and it represents the case
of extreme negative dependence among V (1) and V (2).

An explanation for these distributions as well as the interpretation we attached to them is in Appendix
A. Note that all three base cases identify the joint distribution Π in terms of the known marginals.

Of course there are uncountably many copulas beyond these three copulas including, for example,
parametric copulas such as the Gaussian, t and Archimedean copulas. Any of these copulas could be
used to specify Π. It is also easily seen that convex combinations of copulas are copulas. For example
we could take Π = λΠI + (1 − λ)ΠP for some λ ∈ [0, 1] and use this to represent a modeler’s
uncertainty regarding whether ΠI or ΠP is more appropriate in a given application.

Finally, we note that these copulas can be combined together to form copulas in higher dimensions.
For example in an n = 7-dimensional setting, it’s possible that the copula of (V (1), V (2)) is the coun-
termonotonic copula, that of (V (3), V (4), V (5)) is the independence copula and that of (V (6), V (7))
is the comonotonic copula. Of course this doesn’t uniquely specify the full 7-dimensional copula
but it does highlight how domain speciﬁc knowledge might be used to reduce the space of feasible
copulas and obtain tighter bounds for the counterfactual quantities of interest as we now discuss.

3.2 Computing Bounds

Let a denote the number of possible values of the node V . Then specifying the n-dimensional
counterfactual joint CDF Π of V amounts to selecting a point on the an-dimensional simplex, i.e.
the set of non-negative vectors in Ran
that sum to 1. In our setting the selection of such a point is
constrained by knowing the univariate marginals. Other domain-speciﬁc information may also be
available. For example, in the surgical procedure example, it may be reasonable to assume that the
performances of the surgeons are mutually independent. Considerations such as this would help to
constrain Π or equivalently (since the marginals are known) the copula.

6

If we can’t uniquely identify the appropriate copula then we can formulate two optimization problems.
The ﬁrst problem maximizes the CQI while the second minimizes it. Both are optimization problems
over the an-dimensional simplex. If the objective is linear (as will be the case in our cheating-casino
application) then solving these bounds amounts to solving a linear program. If additional information
or constraints on the copula are linear, then the feasible set for these optimization problems will be
linear. For example, [18] propose counterfactual stability (CS) as being a desirable property for
counterfactuals. Loosely speaking, the CS property can be stated as follows. Suppose we observe an
outcome V = v under some policy I. Then the counterfactual outcome of V under an interventional
policy I (cid:48) cannot be V = v(cid:48) (for v(cid:48) (cid:54)= v) when P(cid:48)(V = v)/P(V = v) ≥ P(cid:48)(V = v(cid:48))/P(V = v(cid:48))
where P and P(cid:48) denote probabilities under the policies I and I (cid:48), respectively. In words, CS states
that if V = v was observed and this outcome becomes relatively more likely than V = v(cid:48) under
the intervention, then the counterfactual value of V under the intervention cannot be v(cid:48). Whether
or not imposing CS is appropriate will depend on the application but as we shall see in §4 we can
easily impose it via linear constraints in our cheating-casino application. Moreover, it will follow
immediately that there are inﬁnitely many copulas in that application that satisfy CS and that picking
one of them and generating counterfactuals from the resulting joint CDF is straightforward.

Time Consistency.
In dynamic models where the state transitions etc. are homogeneous, i.e. time-
independent, it is very natural to also assume that the copulas are time independent. We refer to this
property as time consistency. For example, in the HMM of §4, we will need to work with a copula at
each emission node of the graph. It seems reasonable to insist that this copula is time consistent. As
we shall see, this results in much tighter bounds on the CQI.

Non-Linear Optimization Problems. While our cheating-casino application only requires the
solution of LPs to computer lower and upper bounds on the CQI, in general these problems will be
non-linear albeit over a linear constraint set. For example, if we want to compute the probability the
patient would have survived if she had been permitted to take the regular scans, then bounding this
probability will require the solution of non-linear and (one suspects) generally non-convex problems.
These kinds of problems (where we optimize over a joint distribution subject to knowing the marginals)
are sometimes called Fréchet problems and arise frequently, for example, in quantitative ﬁnance, e.g.
[15]. For example, we may know the so-called value-at-risk of several individual portfolios but want
to compute the worst-case value-at-risk when all of the portfolios are combined.

4 An Application: Cheating at the Casino

The framework developed above is general and we demonstrate its power via the dishonest casino
[8, 12] application of a HMM. This application is well-known and often used to teach HMMs in
machine learning courses [1, 7]. We discuss the HMM in §4.1, the CQI in §4.2, and results in §4.3.

4.1 A Dynamic Latent Space Model

The dishonest casino is a discrete-time HMM with T periods; see Figure 2. The hidden state
Ht ∈ {1, 2} for t ∈ [T ] denotes whether the casino is using a fair die (Ht = 1) or a loaded die
(Ht = 2). Under a fair die, each of the six outcomes (Ot ∈ {1, . . . , 6}) is equally likely whereas
under a loaded die, higher outcomes are more likely. The casino wins a reward of wi ∈ R if the die
rolls i ∈ {1, . . . , 6} and wi is increasing in i so the casino expects to win more under a loaded die.

H1

O1

H2

O2

. . .

HT

OT

Figure 2: Dishonest casino. The states H1:T are hidden while the observations O1:T are observable.

We assume the data generating process, i.e. the initial state distribution p := [ph]h∈{1,2}, the
transition distribution matrix Q := [qhh(cid:48)]h,h(cid:48)∈{1,2}, and the emission distribution matrix E :=
[ehi]h∈{1,2},i∈{1,...,6}, are all known. More details on (p, Q, E) are provided in Appendix B.1.

7

4.2 Expected Winnings Attributable to Cheating (EWAC)
We observed a path o1:T of die rolls which implies the casino won wobs := (cid:80)
t wot. We now want to
compute the expected winnings (of the casino) attributable to cheating or EWAC. Put another way,
how much would the casino have won if they had been forced to use the fair die given the sequence of
observed winnings? To answer this question we need the distribution of the counterfactual path (cid:101)O1:T
where we use a “tilde” to denote counterfactuals. To do this we ﬁrst condition on o1:T , i.e. abduction,
and then set H1:T = 1, which is the action step. We then use (cid:101)O1:T to predict the counterfactual
winnings (cid:80)

, which is random since (cid:101)O1:T is so. Formally, we deﬁne

t w

(cid:101)Ot

EWAC := wobs −

(cid:88)

t

E[w

(cid:101)Ot

].

(2)

t = i, O(2)

t = j) be the joint counterfactual PMF where O(h)

Our task here is counterfactual off-policy evaluation where the counterfactual policy is “no cheating
ever” and the data comes from the policy of “occasional cheating according to the HMM”. Let
π(i, j) = P(O(1)
:= Ot | (Ht = h).
We invoked time consistency and so we have the same counterfactual joint distribution at each
observation node. (Note that we don’t have to worry about the joint counterfactual distribution at
the hidden-state nodes because the action step simply sets all of these nodes to 1.) We also deﬁne
gt(h) := P(Ht = h | o1:T ), which is easily computed via standard ﬁltering and smoothing algorithms
[3]. We now characterize EWAC in Proposition 1 and we prove it in Appendix B.2.
Proposition 1 (Characterization). For any time-consistent π, EWAC satisﬁes

t

EWAC(π) = wobs −

T
(cid:88)

t=1

(cid:40)

gt(1) × wot + gt(2) ×

6
(cid:88)

i=1

wi

π(i, ot)
e2,ot

(cid:41)

.

(3)

Not surprisingly, EWAC depends on π which is the only unknown in our setup. As discussed in
§3.1, one possibility here is to specify π via a copula such as the independence, comonotonic or
countermonotonic copulas (see Appendix B.3) but the justiﬁcation for doing this is likely to be
lacking. Instead, we can simply obtain bounds on the EWAC. For an upper bound (UB), we maximize
EWAC as in (3) with π as a matrix of decision variables. The feasible set F ensures the joint PMF π
satisﬁes the marginal constraints so that F = {π ≥ 0 : (cid:80)
i π(i, j) = e2j ∀j}.
Since (3) is linear in π, we obtain the following LP: EWACub := maxπ∈F EWAC(π). A lower
bound EWAClb is obtained by solving EWAClb := minπ∈F EWAC(π). Note that π is constrained
to be time-consistent as discussed in §3.2.

j π(i, j) = e1i ∀i, (cid:80)

Counterfactual Stability (CS). Our framework allows us to be robust to the copula choice. We
contrast this with [18], where one speciﬁes a property of interest (CS) and implicitly derives via
the Gumbel-max mechanism a generally non-unique SCM that satisﬁes it. In fact, as discussed in
§3.2, our framework provides a systematic way to capture CS by simply adding CS constraints to the
optimization. We can therefore characterize the entire space of SCMs that obey CS by modeling CS
in our LP’s via the following linear constraints: π(j, i) = 0 for all i (cid:54)= j such that e1i/e2i ≥ e1j/e2j.
cs the CS bounds, and by EWAC∗
(See Appendix B.4 for details.) Denoting by EWACub
the EWAC under the true (unknown) copula, we obtain the following inequalities.
cs ≤ EWACub
Proposition 2. EWAClb ≤ EWAC∗ ≤ EWACub and EWAClb ≤ EWAClb

cs and EWAClb

cs ≤ EWACub.

That is, EWAC∗ always lies between EWAClb and EWACub, and CS results in tighter bounds but the
CS bounds may not be “legitimate” bounds on EWAC∗ if the “true” copula does not satisfy CS.

4.3 Numerical Results

We parameterize (p, Q) via a scalar η ∈ [0, 1], which quantiﬁes the extent of cheating by the casino.
Recall that EWAC is deﬁned w.r.t. an observed path o1:T and as such, we analyze two paths, Path # 1
and Path # 2. On Path # 1 the sequence of observations happened to have an average of 3.5 which is
what would be expected if a fair die had always been used. On Path # 2 the sequence of observations
had an average of approx. 4.2. For each path, we consider all values of η ∈ {0.001, 0.002, . . . , 0.999}
with η = 0 and η = 1 denoting “always cheating” and ’“never cheating”, respectively. Further details
are provided in Appendix B.5.

8

(a) Path # 1

(b) Path # 2

Figure 3: EWAC results. In Figure 3a, the UB, UB (CS), and comonotonic curves coincide (the
highest curve in the ﬁgure), as do the LB and countermonotonic curves (the lowest curve).

Figure 3 displays the EWAC’s corresponding to seven different copulas - ΠI, ΠP and ΠN from §3.1,
and the lower and upper bound copulas with and without CS constraints. In addition, we also plot a
very naive estimate of EWAC which completely ignores the information in the observations, i.e. we
fail to execute the abduction step. We note that imposing CS can result in a signiﬁcant tightening
of the bounds. For example, when η = 0.2 in Figure 3a, the (LB, UB) interval is approx. [−10, 20]
whereas the CS interval is much tighter at approx. [12, 19]. We also note that every point in the CS
interval corresponds to a copula that satisﬁes the CS property. This resolves the open question of
[18] regarding whether there are other causal mechanisms beyond the Gumbel-max mechanism that
satisfy CS. In fact, our framework characterizes all SCMs consistent with CS.

As noted earlier, the counterfactual winnings are random because the counterfactual path (cid:101)O1:T is
random. Given this randomness, we focused on the expected WAC but it is possible to understand the
distribution of WAC by sampling (cid:101)O1:T via standard HMM sampling algorithms such as the FFBS
algorithm, e.g. [3]. (Details and related results are provided in Appendix B.7.) This highlights the
ﬂexibility of our framework. The ability to simulate counterfactual paths is an important aspect of
our framework and would be required in general to compute CQI’s in other applications. Finally,
we imposed time-consistency on all the copulas in Figure 3; see also Proposition 1. Imposing time
consistency is very natural and results in much tighter bounds. If we don’t impose time-consistency,
then we obtain larger LPs that are nonetheless still very easy to solve. We explore this further in
Appendix B.8. Finally, a considerably more detailed discussion of Figure 3 and some intuition for the
patterns in the ﬁgure is provided in Appendix B.6.

5 Conclusions

We have provided a framework for conducting counterfactual off-policy evaluation that considers all
possible causal mechanisms. We have shown by way of example that this framework allows us to
compute lower and upper bounds on CQI’s in dynamic models with possibly latent states. To the best
of our knowledge we are the ﬁrst to do this. We also show how the CS property can be handled in our
approach and how there are (inﬁnitely) many causal mechanisms in general that are consistent with
CS. When CS is justiﬁed, it can lead to much tighter bounds on the CQI.

There are many possible directions for future research. For example, it would be worthwhile
considering other applications where bounds on the CQI can only be obtained by formulating and
solving more challenging optimization problems than LPs. For another direction, recall from §3.2
that the size of the optimization problems grows exponentially in n, the number of settings of the
parent nodes. These optimization problems might be challenging to solve in more complex models
but it may still be possible to either solve or compute good bounds on their optimal solutions. Finally,
it would also be interesting to consider other applications where domain speciﬁc knowledge allows
us to impose tight constraints on the copulas.

9

00.20.40.60.81Fairnessparameter-30-20-100102030EWACNaiveUB / LBUB / LB (CS)IndependenceComonotonicCountermonotonic00.20.40.60.81Fairnessparameter05101520253035EWACReferences

[1] Aarti Singh. 10-601 (CMU), 2011. URL https://www.cs.cmu.edu/~aarti/Class/

10601/slides/HMM_11_1_2011.pdf.

[2] Alexander Balke and Judea Pearl. Counterfactual probabilities: Computational methods,
bounds and applications. In Ramon Lopez de Mantaras and David Poole, editors, Uncertainty
Proceedings 1994, pages 46–54. Morgan Kaufmann, San Francisco (CA), 1994.

[3] David Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press, 2012.

[4] Dimitri P. Bertsekas. Dynamic Programming and Optimal Control: 1. Athena Scientiﬁc, 2017.

[5] Lars Buesing, Theophane Weber, Yori Zwols, Nicolas Heess, Sebastien Racaniere, Arthur Guez,
and Jean-Baptiste Lespiau. Woulda, coulda, shoulda: Counterfactually-guided policy search. In
International Conference on Learning Representations, 2019.

[6] Zhihong Cai, Manabu Kuroki, Judea Pearl, and Jin Tian. Bounds on direct effects in the presence

of confounded intermediate variables. Biometrics, 64(3):695–701, 2008.

[7] Doug Downey. EECS 349 (Northwestern), 2010. URL https://users.cs.northwestern.

edu/~ddowney/courses/349_Fall2010/lectures/hmmsML.pdf.

[8] Richard Durbin, Sean R Eddy, Anders Krogh, and Graeme Mitchison. Biological sequence
analysis: Probabilistic models of proteins and nucleic acids. Cambridge university press, 1998.

[9] Peter Glynn and Søren Asmussen. Stochastic Simulation: Algorithms and Analysis. Springer,

2007.

[10] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2022. URL https://www.

gurobi.com.

[11] S. Kaufman, J.S. Kaufman, R.F. MacLenose, S. Greenland, and C. Poole. Improved estimation
of controlled direct effects in the presence of unmeasured confounding of intermediate variables.
Statistics in Medicine, 25:1683–1702, 2005.

[12] Lin Himmelmann. Dishonest Casino, 2022. URL https://search.r-project.org/CRAN/

refmans/HMM/html/dishonestCasino.html.

[13] Guy Lorberbom, Daniel D. Johnson, Chris J Maddison, Daniel Tarlow, and Tamir Hazan. Learn-
ing generalized gumbel-max causal mechanisms. In M. Ranzato, A. Beygelzimer, Y. Dauphin,
P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing
Systems, volume 34, pages 26792–26803. Curran Associates, Inc., 2021.

[14] MATLAB. Version 9.10.0 (R2021b). The MathWorks Inc., Natick, Massachusetts, 2021.

[15] Alexander J. McNeil, Rüdiger Frey, and Paul Embrechts. Quantitative Risk Management:

Concepts, Techniques and Tools. Princeton University Press, 2 edition, 2015.

[16] Scott Mueller, Ang Li, and Judea Pearl. Causes of effects: Learning individual responses from

population data. arXiv, 2021.

[17] R.B. Nelsen. An Introduction to Copulas. Springer, 2 edition, 2006.

[18] Michael Oberst and David Sontag. Counterfactual off-policy evaluation with Gumbel-max
structural causal models. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceed-
ings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of
Machine Learning Research, pages 4881–4890. PMLR, 09–15 Jun 2019.

[19] Judea Pearl. Causality. Cambridge University Press, 2 edition, 2009.

[20] Judea Pearl and Dana Mackenzie. The Book of Why. Penguin Books, 2018.

[21] Abe Sklar. Fonctions de répartition à n dimensions et leurs marges. Publ. Inst. Statist. Univ.

Paris, 8:229–231, 1959.

10

[22] Jin Tian and Judea Pearl. Probabilities of causation: Bounds and identiﬁcation. Annals of

Mathematics and Artiﬁcial Intelligence, 8:287–313, 2000.

[23] Stratis Tsirtsis, Abir De, and Manuel Rodriguez. Counterfactual explanations in sequential
decision making under uncertainty. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and
J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,
pages 30127–30139. Curran Associates, Inc., 2021.

11

Appendices

A A Brief Introduction to Copulas

Copulas are functions that enable us to separate the marginal distributions from the dependency
structure of a given multivariate distribution. They are particularly useful in applications where the
marginal distributions are known (either from domain speciﬁc knowledge or because there is sufﬁcient
marginal data) but a joint distribution with these known marginals is required. This situation arises in
many applications - for example in insurance and ﬁnance. In ﬁnance for example, the market prices of
options on individual securities or indices can be used to compute the so-called risk-neutral (marginal)
distributions for these securities. But if one is pricing an option on a basket of individual securities
then the joint risk-neutral distribution is required. A similar situation arises with credit-default swaps
(CDS). The market prices of CDS’s can be used to infer the marginal risk-neutral probability of a
company declaring bankruptcy by a certain date. But a collateralized debt obligation (CDO) depends
on the joint risk-neutral distribution of the underlying companies going bankrupt. In our application
in this paper, we know the marginal distribution of each random variable in {V (s)
s=1. (Recall that
V (s)
:= Vi | (pai = s).) Indeed these marginal distributions can be estimated from data. But the
i
joint distribution must be speciﬁed in order to compute counterfactuals.

i }ni

In each of these cases one needs to work with a joint distribution with ﬁxed or pre-speciﬁed marginal
distributions. Copulas and Sklar’s Theorem (see below) can be very helpful in these situations. We
only brieﬂy review some of the main results from the theory of copulas here but [17] can be consulted
for an introduction to the topic. [15] also contains a nice introduction in the context of ﬁnancial risk
management.
Deﬁnition 3. A d-dimensional copula, C : [0, 1]d : → [0, 1] is a cumulative distribution function
with uniform marginals.

We write C(u) = C(u1, . . . , ud) for a generic copula. It follows immediately from Deﬁnition 3 that
C(u1, . . . , ud) is non-decreasing in each argument and that C(1, . . . , 1, ui, 1, . . . , 1) = ui. It is also
easy to conﬁrm that C(1, u1, . . . , ud−1) is a (d − 1)-dimensional copula and, more generally, that all
k-dimensional marginals with 2 ≤ k ≤ d are copulas. The most important result from the theory of
copulas is Sklar’s Theorem [21].
Theorem 1 (Sklar 1959). Consider a d-dimensional CDF Π with marginals Π1, . . . , Πd. Then
there exists a copula C such that

Π(x1, . . . , xd) = C (Π1(x1), . . . , Πd(xd))

(4)

for all xi ∈ [−∞, ∞] and i = 1, . . . , d.

If Πi is continuous for all i = 1, . . . , d, then C is unique; otherwise C is uniquely determined only
on Ran(Π1) × · · · × Ran(Πd) where Ran(Πi) denotes the range of the CDF Πi.

Conversely, consider a copula C and univariate CDF’s Π1, . . . , Πd. Then Π as deﬁned in (4) is a
multivariate CDF with marginals Π1, . . . , Πd.

A particularly important aspect of Sklar’s Theorem in the context of this paper is that C is only
uniquely determined on Ran(Π1) × · · · × Ran(Πd). Because we are interested in applications
with discrete state-spaces this implies that there will be many copulas that lead to the same joint
distribution Π. It is for this reason that we prefer to work with our second deﬁnition of SCM (recall
Deﬁnition 2), i.e. the deﬁnition that works directly with the joint distribution of {V (s)
The following important result was derived independently by Fréchet and Hoeffding and provides
lower and upper bounds on copulas.
Theorem 2 (The Fréchet-Hoeffding Bounds). Consider a copula C(u) = C(u1, . . . , ud). Then

i }ni

s=1.

(cid:40)

max

1 − d +

d
(cid:88)

i=1

(cid:41)

ui, 0

≤ C(u) ≤ min{u1, . . . , ud}.

Three very important copulas are the comonotonic, countermonotonic (only when d = 2) and
independence copulas which model extreme positive dependency, extreme negative dependency and
(not surprisingly) independence. They are deﬁned as follows.

1

Comonotonic Copula. The comonotonic copula is given by
C P(u) := min{u1, . . . , ud}
which coincides with the Fréchet-Hoeffding upper bound. It corresponds to the case of extreme posi-
tive dependence. For example, let U = (U1, . . . , Ud) with U1 = U2 = · · · = Ud ∼ U (0, 1). Then
clearly min{u1, . . . , ud} = Π(u1, . . . , ud) but by Sklar’s Theorem F (u1, . . . , ud) = C(u1, . . . , ud)
and so C(u1, . . . , ud) = min{u1, . . . , ud}.

(5)

Countermonotonic Copula. The countermonotonic copula is a 2-dimensional copula given by
C N(u) := max{u1 + u2 − 1, 0}
which coincides with the Fréchet-Hoeffding lower bound when d = 2. It corresponds to the case of
extreme negative dependence. It is easy to check that (6) is the joint distribution of (U, 1 − U ) where
U ∼ U (0, 1). (The Fréchet-Hoeffding lower bound is only tight when d = 2. This is analogous to
the fact that while a pairwise correlation can lie anywhere in [−1, 1], the average pairwise correlation
of d random variables is bounded below by −1/(d − 1).)

(6)

Independence Copula. The independence copula satisﬁes

C I(u) :=

d
(cid:89)

i=1

ui

and it is easy to conﬁrm using Sklar’s Theorem that random variables are independent if and only if
their copula is the independence copula.

A well known and important result regarding copulas is that they are invariant under monotonic
transformations.
Proposition 3 (Invariance Under Monotonic Transformations). Suppose the random variables
X1, . . . , Xd have continuous marginals and copula CX . Let Ti : R → R, for i = 1, . . . , d be strictly
increasing functions. Then the dependence structure of the random variables

is also given by the copula CX .

Y1 := T1(X1), . . . , Yd := Td(Xd)

This leads immediately to the following result.
Proposition 4. Let X1, . . . , Xd be random variables with continuous marginals and suppose Xi =
Ti(X1) for i = 2, . . . , d where T2, . . . , Td are strictly increasing transformations. Then X1, . . . , Xd
have the comonotonic copula.

Proof. Apply the invariance under monotonic transformations proposition and observe that the
copula of (X1, X1, . . . , X1) is the comonotonic copula.

B Further Details on the Dishonest Casino Application

B.1 HMM Primitives

A standard HMM (without the causal mechanism) has three primitives, namely the initial state
distribution and the transition and emission matrices. We deﬁne them as follows for our dishonest
casino application:

1. The initial state distribution is denoted by p := (p1, p2) where ph := P(H1 = h) for

h ∈ {1, 2}.

2. The transition matrix is denoted by Q := [qhh(cid:48)]h,h(cid:48)∈{1,2} where qhh(cid:48) := P(Ht+1 = h(cid:48) |

Ht = h) for h, h(cid:48) ∈ {1, 2} and t ∈ [T − 1].

3. The emission matrix is denoted by E := [ehi]h∈{1,2},i∈{1,...,6} where ehi := P(Ot = i |
Ht = h) for h ∈ {1, 2}, i ∈ {1, . . . , 6}, and t ∈ [T ]. If the hidden state is 1 (fair), then
each outcome is equally likely, i.e., e1i = 1/6 for all i.

2

The transition and emission matrices are time-independent although this is easily relaxed. We
assume these matrices and initial distribution are known. (If this were not the case then it would
be straightforward to estimate them, e.g. via the EM / Baum-Welch algorithm, [3], given sufﬁcient
observational data.)

B.2 Proof of Proposition 1

Proposition 1 (Characterization). For any time-consistent π, EWAC satisﬁes

EWAC(π) = wobs −

T
(cid:88)

t=1

(cid:40)

gt(1) × wot + gt(2) ×

6
(cid:88)

i=1

wi

π(i, ot)
e2,ot

(cid:41)

.

Proof. We recall from (2) that

EWAC := wobs −

T
(cid:88)

t=1

E[w

(cid:101)Ot

].

(3)

(2)

E[w

Note that our deﬁnition of the (cid:101)Ot’s from §4.2 entailed conditioning on the realized observation
sequence o1:T . Making this explicit, we obtain
| o1:T ]
| o1:T , Ht = 1]
(cid:125)

] = E[w
= E [w
(cid:124)

| o1:T , Ht = 2]
(cid:125)

+ E[w
(cid:124)

P(Ht = 2 | o1:T )
(cid:125)
(cid:123)(cid:122)
(cid:124)
=gt(2)

P(Ht = 1 | o1:T )
(cid:123)(cid:122)
(cid:125)
(cid:124)
=gt(1)

(7)

(cid:101)Ot

(cid:101)Ot

(cid:101)Ot

(cid:101)Ot

(cid:123)(cid:122)
=:((cid:63))

(cid:123)(cid:122)
=wot

(cid:101)Ot

That E[w
| o1:T , Ht = 1] = wot follows because Ht = 1 denotes the fair die was used at time t in
which case (cid:101)Ot = ot. Since gt(h) = P(Ht = h | o1:T ) is easily computed using standard ﬁltering
and smoothing algorithms [3], the only term in (7) that remains to compute is the term labelled ((cid:63)).
Deﬁning O(h)

t = Ot | (Ht = h), we obtain

((cid:63)) := E[w

(cid:101)Ot

| o1:T , Ht = 2] = E[wO(1)

t

| O(2)

t = ot]

=

=

=

6
(cid:88)

i=1

6
(cid:88)

i=1

6
(cid:88)

i=1

wi × P(O(1)

t = i | O(2)

t = ot)

wi ×

P(O(1)

t = i, O(2)
P(O(2)
t = ot)

t = ot)

wi ×

π(i, ot)
e2,ot

(8)

where π(i, j) = P(O(1)
(2), (7), and (8) together, we obtain (3).

t = i, O(2)

t = j) is the counterfactual joint PMF as deﬁned in §2.3. Putting

B.3 Characterization of EWAC Under the Benchmark Copulas

We now provide a characterization of EWAC for each of the independence, comonotonic and
countermonotonic copulas.
Proposition 5 (Characterization Under Copulas). Let EWACI, EWACP and EWACN denote the
EWAC for each of the independence, comonotonic and countermonotonic copulas, respectively. Then

EWACI = wobs −

EWACP = wobs −

EWACN = wobs −

(cid:40)

T
(cid:88)

(cid:40)

(cid:40)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

gt(1)wot + gt(2)

gt(1)wot + gt(2)

gt(1)wot + gt(2)

(cid:41)

wie1i

(cid:41)

(cid:41)

wi

πP(i, ot)
e2,ot

wi

πN(i, ot)
e2,ot

6
(cid:88)

i=1

6
(cid:88)

i=1

6
(cid:88)

i=1

(independence)

(comonotonic)

(countermonotonic)

3

where for all (i, j)

πP(i, j) :=

πN(i, j) :=

1
(cid:88)

1
(cid:88)

(cid:96)=0

1
(cid:88)

(cid:96)(cid:48)=0
1
(cid:88)

(cid:96)=0

(cid:96)(cid:48)=0

(−1)(cid:96)+(cid:96)(cid:48)

min{Π1(i − (cid:96)), Π2(j − (cid:96)(cid:48))}

(−1)(cid:96)+(cid:96)(cid:48)

(Π1(i − (cid:96)) + Π2(j − (cid:96)(cid:48)) − 1)+,

and for h ∈ {1, 2}, Πh(·) is the CDF of O(h)
all i.

t

, i.e., Πh(i) = P(Ot ≤ i | Ht = h) = (cid:80)

j≤i ehj for

Proof. It follows from (3) that we simply need to characterize π(i, ot) under each of the three copulas.
For the independence copula, we have

πI(i, j) = P(O(1)

t = i, O(2)

t = j) = P(O(1)

t = i) × P(O(2)

t = j)

and we obtain EWACI. For the comonotonic and countermonotonic copulas, the following general
fact proves useful:

= e1i × e2j

π(i, j) = P(O(1)
= P(O(1)

t = i, O(2)
t ≤ i, O(2)
−P(O(1)

t = j)
t ≤ j) − P(O(1)

t ≤ i, O(2)

t < j) + P(O(1)

t < i, O(2)

t ≤ j)
t < i, O(2)

t < j)

= Π(i, j) − Π(i − 1, j) − Π(i, j − 1) + Π(i − 1, j − 1)

1
(cid:88)

1
(cid:88)

=

(−1)(cid:96)+(cid:96)(cid:48)

Π(i − (cid:96), j − (cid:96)(cid:48))

(9)

where Π(i, j) = P(O(1)
P(O(h)

(cid:96)=0

(cid:96)(cid:48)=0
t ≤ i, O(2)

t ≤ i) for all (h, i), the comonotonic joint CDF ΠP satisﬁes

t ≤ j) is the joint counterfactual CDF. Recalling that Πh(i) =

t ≤ i, O(2)
ΠP(i, j) = P(O(1)
t ≤ j)
= P(Π−1
1 (U ) ≤ i, Π−1
= P(U ≤ Π1(i), U ≤ Π2(j))
= P(U ≤ min{Π1(i), Π2(j)})
= min{Π1(i), Π2(j)}
where comonotonicity is invoked in (10a) with U ∼ Uniform(0, 1). Substituting (10b) into (9) yields

2 (U ) ≤ j)

(10b)

(10a)

πP(i, j) =

1
(cid:88)

1
(cid:88)

(−1)(cid:96)+(cid:96)(cid:48)

min{Π1(i − (cid:96)), Π2(j − (cid:96)(cid:48))}

(11)

(cid:96)(cid:48)=0
for all (i, j). Finally, the countermonotonic joint counterfactual CDF satisﬁes

(cid:96)=0

ΠN(i, j) = P(O(1) ≤ i, O(2) ≤ j)
1 (U ) ≤ i, Π−1
= P(Π−1
= P(U ≤ Π1(i), 1 − U ≤ Π2(j))
= P(U ≤ Π1(i), U ≥ 1 − Π2(j))
= P(U ∈ [1 − Π2(j), Π1(i)])
= (Π1(i) + Π2(j) − 1)+,
(12c)
where countermonotonicity is invoked in (12a) with U ∼ Uniform(0, 1), and (·)+ := max{·, 0}.
Substituting (12b) into (9) yields

2 (1 − U ) ≤ j)

(12b)

(12a)

πN(i, j) =

1
(cid:88)

1
(cid:88)

(−1)(cid:96)+(cid:96)(cid:48)

(Π1(i − (cid:96)) + Π2(j − (cid:96)(cid:48)) − 1)+

(13)

for all (i, j).

(cid:96)=0

(cid:96)(cid:48)=0

4

B.4 Modeling Counterfactual Stability via Linear Constraints

In the context of the dishonest casino HMM, counterfactual stability (CS) [18] can be expressed as
follows. Recall that ehi = P(O(h) = i) denotes the probability that observation equals i given the
hidden state equals h. Now suppose that for arbitrary observations (i, j) such that i (cid:54)= j, we have

e1i
e21

≥

e1j
e2j

.

Then CS requires P(O(1) = j | O(2) = i) = 0 where we recall that O(h)
t = Ot | (Ht = h). That is,
if i was observed and this outcome becomes relatively more likely than j under the intervention, then
the counterfactual observation under the intervention cannot be j. (Recall that Ht = 1 and Ht = 2
denote the events that the fair and loaded die were used, respectively, at time t, and the intervention
in question is that the casino employs a policy of always using the fair die.) Since

P(O(1) = j | O(2) = i) =

P(O(1) = j, O(2) = i)
P(O(2) = i)

=

π(j, i)
e2j

,

we can model the CS property via following linear constraints:

π(j, i) = 0 for all (i, j) such that i (cid:54)= j and

e1i
e21

≥

e1j
e2j

.

(14)

B.5 Setup of the Numerical Experiments

In our numerical experiments of §4, we assume T = 30 periods and introduce a parameter η ∈ [0, 1],
which quantiﬁes the degree of fairness in the HMM policy adopted by the casino. In particular, the
initial state and transition distributions are given by

p = (η, 1 − η)
(cid:20)η
η

1 − η
1 − η

Q =

(cid:21)

.

That is, the initial state, i.e. die, is fair w.p. η and the next state is the same as the current state w.p. η
as well. The emission distributions under the fair and loaded die obey

[e1i]i ∝ (1, . . . , 1)
[e2i]i ∝ (1, . . . , 6).

Finally, we set the casino’s winnings to be wi := i for all i. Under this setup, the CS constraints (14)
t > O(2)
are simply π(i, j) = 0 for all i > j, i.e., P(O(1)
Recall that EWAC is deﬁned w.r.t. an observed path o1:T . The two paths we considered in §4.3 were
deﬁned as follows. Path # 1 equals

) = 0 for all t.

t

with an average of 3.5. Path # 2 equals

(1, 2, 3, 4, 5, 6, . . . , 1, 2, 3, 4, 5, 6),

(1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 1, 2, 3, 4, 4, 5, 5, 5, 5),

with an average of approximately 4.2. The ﬁrst path therefore represents an unlucky path from
the perspective of the casino adopted the cheating policy and yet only received a total payout that
would have been average for a policy where it always used the fair die. The second path is more
consistent with what might be expected under the cheating policy. For each path, we experiment with
η ∈ {0.001, 0.002, . . . , 0.999} so lower values of η correspond to casino using the loaded die (and
therefore cheating) more frequently.

We coded in MATLAB [14] and used gurobi [10] to solve the LPs. Our computations took a total of
less than 10 minutes on a 3.8 GHz 8-Core Intel Core i7 processor with 16 GB 2667 MHz DDR4
memory.

5

B.6 Further Discussion of the Numerical Results

In this appendix, we provide additional discussion of the EWAC results from §4.3. Recall that the
naive estimate of EWAC does not condition on the observed sequence of die rolls and therefore
completely ignores the abduction step. It simply calculates what the expected winnings would have
been if the casino were to use a fair die on a fresh sequence of T die rolls and subtracts this from the
observed winnings. To interpret naive EWAC, consider Path # 1 which corresponds to Figure 3a.
Path # 1 is one where wobs = 105 which happens to be the expected winnings under complete fairness
(30 × 3.5). Accordingly, the naive estimate of EWAC is 0 on this path. In contrast, wobs = 125 for
Path # 2 and so the naive estimate of EWAC on this path is 125 − 105 = 20. As we can see from
Figure 3b (which corresponds to Path # 2), the naive estimate of EWAC can lie outside the interval
[LB, UB]. This just serves to emphasize that there is no causal mechanism that is consistent with the
naive estimate. Moreover, the naive estimate does not depend on η and is therefore constant in
Figures 3a and 3b.
We now turn to EWACI, the EWAC under the independence copula. For Path # 1, we can see from
Figure 3a that EWACI starts at 0 (η = 0) and ends at 0 (η = 1), with a peak in between. This behavior
can be explained via the EWACI characterization from Proposition 5 in Appendix B.3. In particular,
we have

EWACI = wobs −

= wobs −

(cid:88)

t
(cid:88)

t

(cid:40)

gt(1)wot + gt(2)

(cid:41)

(cid:88)

i

wie1i

{gt(1) × ot + gt(2) × 3.5}

(15)

t wot = (cid:80)

since e1i = 1/6 for all i and because we assumed wi = i. When η = 0, the casino always
uses the loaded die and hence gt(1) = 0 and gt(2) = 1 for all t. Since wobs = 105 on this path
we obtain EWACI = 0. Similarly, when η = 1, the casino always uses the fair die and hence
gt(1) = 1 and gt(2) = 0 for all t. In this case, (15) again yields EWACI = 0 (since by deﬁnition
wobs = (cid:80)
t ot). For intermediate values of η, periods t with a high value of ot will
typically a have a higher value of gt(2) than periods with lower values of ot. This is because the
ﬁltering / smoothing algorithm will generally infer that the loaded die is more likely to have been used
when high die rolls, i.e. values of ot, are observed. Referring to (15), this implies that more weight
is placed on the 3.5 term than on the ot term when ot is high. Since the ot’s have an average value
of 3.5 and (cid:80)
t ot = wobs, this explains the peaked behavior of EWACI in Figure 3a for intermediate
values of η.
In the case of Path # 2 and Figure 3b, EWACI begins at 20 (when η = 0) and monotonically decreases
to 0 at η = 1. This behavior can again be explained via (15). For example, when η = 0, gt(1) = 0 and
gt(2) = 1 for all t and this implies EWACI = wobs − (cid:80)
t 3.5 = 125 − 30 × 3.5 = 20. When η = 1,
gt(1) = 1 and gt(2) = 0 for all t and this implies EWACI = wobs − (cid:80)
The observation that all EWACs converge to 0 as η → 1 can be explained via the general EWAC
expression from Proposition 1:

t wot = wobs − wobs = 0.

EWAC(π) = wobs −

(cid:40)

gt(1) × wot + gt(2) ×

(cid:88)

wi

i

π(i, ot)
e2,ot

(cid:41)

.

(cid:88)

t

When η = 1 (casino never uses the loaded die), we have gt(1) = 1 and gt(2) = 0 for all t. Hence
EWAC(π) = wobs − (cid:80)
t wot = wobs − wobs = 0 regardless of the causal mechanism / counterfactual
joint distribution π.

We also observe that an EWAC can be negative. In Figure 3a, for example, the countermonotonic
EWAC, i.e. EWACN, is negative for small values of η. We can explain this using the EWACN
characterization in Proposition 5 but instead we will provide an intuitive explanation. As discussed
earlier, when η = 0, we have gt(1) = 0 and gt(2) = 1 for all t. This reﬂects the posterior certainty
that the casino used a loaded die in every period. This in turn implies the casino should have made
signiﬁcantly more than 105 in expectation. However, Path # 1 corresponds to wobs = 105, which
implies the casino experienced a streak of “bad luck” despite always using the loaded die. The
countermonotonic copula ﬂips the “bad luck” into “good luck” (compare the U and 1 − U in (12a)),

6

and results in counterfactual winnings of over 105. Subtracting these counterfactual winnings from
wobs = 105, we obtain a negative EWACN. The same logic applies to non-zero but low values of η
with Path # 1. Of course, if the countermonotonic curve is below 0, the LB has to be below 0 since
by deﬁnition, LB is a lower bound (for all feasible copulas), and we clearly see this behavior for Path
# 1 in Figure 3a.
In all of our results, we observe that EWACI lies between EWACP and EWACN. Under Path # 1,
EWACP coincides with the UB copula and EWACN coincides with the LB copula. As demonstrated by
Path # 2 and Figure 3b, this is not true in general, however, but they may serve as good approximations
to the [LB, UB] range. Furthermore, we observe that EWACP obeys the CS property under both paths
(since it always lies between the corresponding bounds) but EWACI and EWACN can violate the CS
property, e.g., Path # 1. Finally, we observe that even for a given path, the ordering among EWACI,
EWACP and EWACN can vary with η. This is clear from Figure 3b.

B.7 Distribution of Winnings Attributable to Cheating (WAC)

The results in §4.3 focused on the expected WAC but it’s also possible to estimate the distribution of
WAC. Towards this end, consider an arbitrary counterfactual joint PMF π. This could be πI, πP or
πN, or the PMF corresponding to any of the four bounds. Our goal is to understand the distribution of
WAC under the given π (conditional of course on the observed path o1:T ). To do so, we ﬁrst generate
B posterior samples of the hidden path via the FFBS algorithm [3]: [h1:T (b)]B
b=1. “Posterior” here
corresponds to conditioning on o1:T . Second, for each sampled path b ∈ [B], we loop over each
hidden state ht(b) for t ∈ [T ]. If the hidden state equals fair (ht(b) = 1), then the counterfactual
observation equals the one we observed, i.e. (cid:101)Ot(b) = ot. Otherwise ht(b) = 2 and we use the joint
distribution π conditioned on the observation ot to sample (cid:101)Ot(b). We therefore capture both sources
of uncertainty, namely (1) the hidden state uncertainty via the FFBS algorithm and (2) the uncertainty
in the counterfactual observation. Note that the ﬁrst source of uncertainty does not depend on the
causal mechanism, i.e. π, but the second step does.

In Figure 4 we display histograms of the WAC for four pairs of π for a value of η = 0.5 and for same
two paths that were considered earlier. To map Figure 4 to Figure 3, note the average corresponding
to each histogram in Figure 4 should match the value reported in Figure 3 (for η = 0.5). For
example, in Figure 4a, the comonotonic histogram has an average around 12, which matches to the
comonotonic value for η = 0.5 in Figure 3a. Under both the paths, the comonotonic histogram
lies to the right of 0, and indeed we can easily show that WACP ≥ 0 w.p. 1. We observe that the
comonotonic and countermonotonic histograms are very similar to the UB and LB histograms,
suggesting they might be able to serve as approximations to the bounds in other applications when
the bounds are difﬁcult to compute. (Of course one would need to provide some application-speciﬁc
justiﬁcation for making such an approximation.)

(a) Path # 1

(b) Path # 2

Figure 4: WAC distribution for η = 0.5.

7

B.8 Relaxing Time Consistency

As discussed in §3.2, it seems very reasonable to impose time consistency on the underlying joint
PMF π, and we did so when computing bounds in our §4 numerics. Doing so shrinks the feasibility
space of π in the underlying linear program (LP) and this leads to tighter bounds. To understand how
much we gain (in terms of tightness of the bounds) by imposing time consistency, we now allow for
time-varying copula / joint counterfactual distributions [πt]t in the LP.
Recall the original lower bound LP from §4.2 was obtained by solving

where

EWAC(π) = wobs −

EWAC(π),

min
π∈F

(cid:40)

gt(1) × wot + gt(2) ×

T
(cid:88)

t=1

(cid:41)

6
(cid:88)

i=1

wi

π(i, ot)
e2,ot

and F = {π ≥ 0 :

(cid:88)

j

π(i, j) = e1i ∀i,

(cid:88)

i

π(i, j) = e2j ∀j}.

We can relax the time-consistency constraint by simply allowing π to be time-varying. This leads to
the following LP:

min
π1:T

wobs −

(cid:40)

T
(cid:88)

t=1

gt(1) × wot + gt(2) ×

(cid:41)

6
(cid:88)

i=1

wi

πt(i, ot)
e2,ot

(16a)

s.t. πt ∈ F ∀t ∈ [T ]

(16b)
We can then solve (16) and the corresponding maximization problem to obtain lower and upper
bounds on EWAC when we don’t impose time-consistency. We do so for the same numerical
setup of §4.3 and the results (analogous to Figure 3) are shown in Figure 5, with “time-varying”
denoting the bounds obtained when we relax time consistency. Clearly, there is a lot of value in
imposing time-consistency. (We did not include the naive, independence, comonotonic, and
countermonotonic curves to avoid cluttering the ﬁgures.)

(a) Path # 1

(b) Path # 2

Figure 5: EWAC results with time-varying copula. In Figure 5a, the UB and UB (CS) curves coincide.
The UB / LB and UB / LB (CS) curves are the same as in Figure 3.

We also note that the LP in (16) possesses a special structure which allows us to characterize its
solution analytically. First, observe that it is separable w.r.t. t. We can then simplify the resulting
period t objective (by omitting constants and scaling factors), and obtain the following LP for the
period t PMF πt:

max
πt∈F

6
(cid:88)

i=1

wiπt(i, ot).

(17)

(The minimization changes to a maximization due to the negative sign.) Not only is (17) lower
dimensional than (16), its optimal solution exhibits the following closed-form:

(cid:40)

(cid:41)

πt(k, ot)

∀i.

(18)

πt(i, ot) = min

e1i, e2,ot −

8

(cid:88)

k>i

00.20.40.60.81Fairnessparameter-100-50050100EWACUB / LB (time-varying)UB / LBUB / LB (CS)00.20.40.60.81Fairnessparameter-100-50050100EWACTo see why (18) holds, we suppose ot = 2 for illustration and then consider the following 6-by-6
matrix of decision variables:

πt(1, 1) πt(1, 2) πt(1, 3) πt(1, 4) πt(1, 5) πt(1, 6)
πt(2, 1) πt(2, 2) πt(2, 3) πt(2, 4) πt(2, 5) πt(2, 6)
πt(3, 1) πt(3, 2) πt(3, 3) πt(3, 4) πt(3, 5) πt(3, 6)
πt(4, 1) πt(4, 2) πt(4, 3) πt(4, 4) πt(4, 5) πt(4, 6)
πt(5, 1) πt(5, 2) πt(5, 3) πt(5, 4) πt(5, 5) πt(5, 6)
πt(6, 1) πt(6, 2) πt(6, 3) πt(6, 4) πt(6, 5) πt(6, 6)

e21

e22

e23

e24

e25

e26

e11
e12
e13
e14
e15
e16

As dictated by F, each row i needs to sum to e1i and each column j to e2j. Since (17) maximizes
(cid:80)
i wiπt(i, 2) (recall ot = 2), we focus on the second column which is highlighted in blue. Given
that w1 < . . . < w6, it is optimal to set πt(6, 2) to be as big as possible and to move up greedily:

πt(6, 2) = min{e16, e22}
πt(5, 2) = min (cid:8)e15, e22 − πt(6, 2)(cid:9)
πt(4, 2) = min (cid:8)e14, e22 − πt(6, 2) − πt(5, 2)(cid:9)

...

(cid:40)

πt(1, 2) = min

e11, e22 −

(cid:41)

πt(i, 2)

.

(cid:88)

i>1

Our expression in (18) simply generalizes this pattern. Note that the πt variables in other columns
are “free” and can be set arbitrarily as long as the constraints in F are satisﬁed (since they do not
appear in the objective). One can characterize the upper bound solution similarly by proceeding in
the reverse order, i.e., from row 1 to row 6.

9

