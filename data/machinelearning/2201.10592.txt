Noname manuscript No.
(will be inserted by the editor)

DebtFree: Minimizing Labeling Cost in
Self-Admitted Technical Debt Identiﬁcation using
Semi-Supervised Learning

Huy Tu · Tim Menzies

Received: date / Accepted: date

2
2
0
2

n
a
J

5
2

]
E
S
.
s
c
[

1
v
2
9
5
0
1
.
1
0
2
2
:
v
i
X
r
a

Abstract Keeping track of and managing Self-Admitted Technical Debts
(SATDs) is important for maintaining a healthy software project. Current
active-learning SATD recognition tool involves manual inspection of 24% of
the test comments on average to reach 90% of the recall. Among all the test
comments, about 5% are SATDs. The human experts are then required to read
almost a quintuple of the SATD comments which indicates the ineﬃciency of
the tool. Plus, human experts are still prone to error: 95% of the false-positive
labels from previous work were actually true positives.

To solve the above problems, we propose DebtFree, a two-mode framework
based on unsupervised learning for identifying SATDs. In mode1, when the ex-
isting training data is unlabeled, DebtFree starts with an unsupervised learner
to automatically pseudo-label the programming comments in the training data.
In contrasts, in mode2 where labels are available with the corresponding train-
ing data, DebtFree starts with a pre-processor that identiﬁes the highly prone
SATDs from the test dataset. Then, our machine learning model is employed
to assist human experts in manually identifying the remaining SATDs. Our
experiments on 10 software projects show that both models yield statistically
signiﬁcant improvement in eﬀectiveness over the state-of-the-art automated
and semi-automated models. Speciﬁcally, DebtFree can reduce the labeling
eﬀort by 99% in mode1 (unlabeled training data), and up to 63% in mode2
(labeled training data) while improving the current active learner’s F1 rela-
tively to almost 100%.

Keywords Technical Debt · Semi-Supervised Learning · Unsupervised
Learning · Labeling Eﬀort

H. Tu and T. Menzies
Department of Computer Science,
North Carolina State University,
Raleigh, USA
E-mail: hqtu@ncsu.edu and timm@ieee.org

 
 
 
 
 
 
2

1 Introduction

Huy Tu, Tim Menzies

When developers rush out code, that code often contains technical debt (TD),
i.e. decisions that must later be repaid with further work. As the initial step
towards understanding and resolving TDs, many research [25, 55, 84] ﬁrst
detecting the intentionally documented (via comments) TD, i.e., self-admitted
TD (SATD). SATDs are crucial to identify as they (1) are diﬀused in the
codebase [84]; (2) can survive long-term (more than 1,000 commits) [5]; and
(3) complicate maintainability of the software [22, 73] State-of-the-art (SOTA)
works have identiﬁed SATDs automatically [55] or semi-automatically [84].

However, models that recognize TD must be learned from labeled data.
Generating such labels can be extremely slow and expensive. For instance,
Tu et al. [67] reported that manually labeling 22, 500+ commits required 175
person-hours, including cross-checking. Due to the labor-intensive nature of
the process, researchers often reuse datasets labeled from previous studies. For
instance, Lo et al., Yang et al., and Xia et al. certiﬁed their methods using data
generated by Kamei et al. [31, 78, 79, 81]. While this practice allows researchers
to rapidly test new methods, it leaves the possibility for any labeling mistake
to propagate to other related works. In fact, before reusing Maldonaldo et al.’s
data [37] to identify SATDs, Yu et al. [84] discovered that more than 98% of
the false positives were actually true positives, casting doubt on related work
using the original dataset. Hence, it is timely to ask:

Can we reduce the labeling eﬀort associated with building models for
technical debt?
An unsupervised learning technique that learns patterns from unlabeled data
is a promising direction in SATD identiﬁcation. However, without supervision,
the technique alone can be ineﬀective. As illustrated in Figure 1, our approach
is to ﬁrst demonstrate that previous methods can be extended or integrated
with unsupervised learning to greatly reduce the labeling eﬀort while eﬀec-
tively recognizing SATDs. This proposed method, called DebtFree, includes
the combination of three separate approaches in a novel manner:
1. Pseudo-Labeling: This step is required if the training data does not have
any labels to start with. First, we frugally pseudo-labels the training data
with unsupervised learning, i.e., identifying hidden patterns in data in order
to map unlabeled examples in two groups. Intuitively, the more complex the
data instance [65], the more likely that the comment is describing a SATD.
CLA by Nam et al.[47] is an example of an unsupervised classiﬁer that
recognizes “complex” examples (those with many values above the median).
The intuition here is well documented [44, 65, 14, 26].

2. Filtering: This step is optional. We identify early and remove instances

from the test dataset that are likely to be SATDs.

3. Active Learning: This step is always required. We train on some labeled
data and then guide the human experts to manually ﬁnd the comments
that are most likely to contain SATDs. It is critical to assess whether the
labeled data is insightful enough to guide the human experts for the entire
labeling process. If not, we propose Falcon, a new active learning policy to
take advantage of such data while still ensuring eﬀectiveness.

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

3

Fig. 1: Workﬂows of DebtFree = Pseudo-Labeling (via Unsupervised Learn-
ing, i.e., CLA [47]) + Filtering (via CLA [47] or Jitterbug’s Easy [84]) + Active
Learning (via Emblem [67], Jitterbug’s Hard [84], or this study’s Falcon). Step
1 is required if there is no access to the training data’s labels. Step 2 is op-
tional while Step 3 is required at all times. The gray arrows indicate diﬀerent
conﬁgurations of the method that will be investigated for this study.

In this work, we aim to better data generation associated with building mod-
els for SATDs identiﬁcation by reducing the labeling eﬀort come from man-
ual method [67] and improving the labeling quality of fully automated meth-
ods [84]. Moreover, our investigation also showed that the eﬀort-aware method
we propose, DebtFree, also performs statistically similar or even better than
two SOTA works [84, 55]. To understand and validate this end-to-end method,
DebtFree, we investigate the following research questions:

RQ1: How well can the state-of-the-art unsupervised learning
method identify SATDs? We investigate variants that stem from Nam et
al.’s unsupervised learning CLA method. As these methods leverage on hidden
patterns within the data, we compare them to the pattern-based SOTA for
identifying SATDs, also by Yu et al. [84].

Result:
From our exploration of various unsupervised learners, the original CLA
by Nam et al. performs the best. Moreover, CLA performs similarly to
the SOTA pattern-based approach, Easy [84], without having access to
the data’s labels (100% less eﬀort).

RQ2: How can the state-of-the-art active learning framework be
combined with the state-of-the-art unsupervised learner? From RQ1,
unsupervised learning methods are promising but not optimal. Hence, we study
diﬀerent combinations by incorporating a chosen unsupervised learner with
several SOTA active learning frameworks in SE.

4

Huy Tu, Tim Menzies

Result:
With the eﬀort-aware theme, we investigate diﬀerent combinations of
active learners and CLA across two settings of the training data, either
with having 1-no access and 2-access to the labels to propose DebtFree.
In setting 1, or DebtFree(0), the best combination is Pseudo-Labeling
(via CLA) with our proposed active learner, Falcon. In setting 2, or
DebtFree(100), the best combination is Filtering (via CLA) with the
SOTA active learner for SATDs identiﬁcation, Hard.

RQ3: How does the proposed DebtFree perform against state-of-
the-art models in identifying SATDs? After ﬁnalizing two combinations
from RQ2 to propose DebtFree(0)/(100), it is essential to assess their useful-
ness by comparing against the SOTA models for SATDs identiﬁcation.

Result:
When comparing against the SOTA semi-supervised learning work by
Yu et al. [84] and the SOTA supervised learning work (with deep learn-
ing) by Ren et al. [55], our proposed method DebtFree outperforms
them signiﬁcantly. First, DebtFree(100) performs similarly to Ren et al.
[55]’s work and better than Yu et al. [84]’s work while reducing the la-
beling cost by 2.5 times. Second, DebtFree(0) performs similarly to
Ren et al. [55]’s work without having access to the training data’s labels
and outperforms Yu et al. [84]’s work while expending 99% less eﬀort.

Our contributions to the ﬁeld of software analytic are:

1. This work is the ﬁrst to assess the usage of unsupervised learning to reduce

the cost of labels labeling in identifying SATDs.

2. In the low-resource setting (training data with no label), our unsupervised
methods outperform the prior SOTA models while requiring less knowledge
(prior work used 100% labeled data while we get by with very little) [55, 84].
Counting the training data, we can reduce 99% of the number of examples
that have to be labeled. This is the largest reduction ever reported in the
eﬀort required to commission a SATD identiﬁcation model.

3. In the high-resource setting (training data with labels), we propose an im-
provement to the two-step Jitterbug technique [84] by replacing the pattern-
based approach with an unsupervised learner to help reduce the commis-
sioning eﬀort of labeling on new data by 62.5% (5/8).

4. Our proposed active learning scheme, Falcon, outperforms both SOTA deep
learning method [55] and SOTA two-step method [84] across both low-
resource and high-resource settings.

5. Nearly all the prior work on unsupervised learning focus on defect predic-
tion [21, 47, 49, 75, 76, 77, 80, 81, 87, 88]. The performance of our framework
suggests that many more domains in software analytics could beneﬁt from
unsupervised learning.

6. To better support other researchers our scripts and data are on-line at

https://github.com/HuyTu7/DebtFree.

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

5

The rest of this paper is structured as follows. Section 2 discusses the moti-
vation, background and related works. Section 3 describes our methodology.
Section 4 focuses on our experimental design, while section 5 analyzes the
results. Section 6 and 7 discuss our short-comings and directions for future
work, respectively.

2 Motivation and Background

2.1 On the merits of studying Technical Debt and SATDs

Technical Debts (TDs) are introduced in the software when developers make
decisions based on short-term beneﬁts instead of long-term stability. TDs can
accumulate interest similar to ﬁnancial debts if they are not resolved in a
timely manner. In 2012, after interviewing 35 software developers from di-
verse projects in diﬀerent companies, varying both in size and type, Lim
et al. [34] found developers generate technical debts due to factors like in-
creased workload, unrealistic deadline in projects, lack of knowledge, boredom,
peer-pressure among developers, unawareness or short-term business goals of
stakeholders, and reuse of legacy, third-party, or open-source code. After ob-
serving ﬁve large-scale projects and companies in two studies, Wehaibi et al.
[73] and Martini and Bosch [42] found that the number of technical debts in
a project may be very low (only 3% on average). However, those TDs con-
taminate other parts of a software system and create a signiﬁcant amount of
defects in the future. Fixing such technical debts is more diﬃcult than regular
defects, often twice the cost if not resolving immediately [24]). The Software
Improvement Group study by Nugroho et al. [50] oﬀers a cost estimate of TD
accumulation: a regular mid-level project owes $857, 500 in TD and resolving
TD has a Return On Investment of 15% in seven years. Yet, limited success
has been achieved despite a large body of research on identifying TD as part of
Code Smells using static code analysis [20, 39, 40, 41, 86]. Static code analysis
has a high rate of false alarms while imposing complex and heavy structures
for identifying TD [2, 23, 63, 64].

EMF

SATD comments

Table 1: Examples of SATD comments.

Project
Apache Ant // cannot remove underscores due to

Therefore, several re-
searchers proposed to
target self-admitted tech-
nical debt identiﬁcation
as the ﬁrst step since
they are often inten-
tionally documented or
“self-admitted” (via source
code comments) by the
developers. Some examples of SATDs within the data are shown in Table 1.
In summary, identifying and resolving SATDs have several beneﬁts:
– Removing SATDs early reduces the maintenance cost of a software project.
As reported by Wehaibi et al. [73] and Wang et al. [71], these SATDs have

JFreeChart // do we need to update the crosshair values?
JMeter
SQuirrel
ArgoUML

protected visibility >:(
// TODO Binary incompatibility;
an old override must override putAll.

// Can be null (not sure why)
// is this right???
// Why does the next part not work?

6

Huy Tu, Tim Menzies

Fig. 2: Nine stages of the machine learning workﬂow from a case study at
Microsoft by Zimmermann et al. [4]. Some stages are data-oriented (e.g., data
collection, cleaning, and labeling) and others are model-oriented (e.g., model
requirements, feature engineering, model training, evaluation, deployment and
monitoring).

negative implications on the software development process, in particular by
making it more diﬃcult to change in the future.

– With SATDs elimination, software projects have better evolvability trajec-

tory for accelerating new functionalities addition and integration.

– We can leverage those easily found SATDs as cheap training data for recog-
nizing TDs [84]. SATDs are the documents of TDs that have been “admit-
ted” by the developers, so they are not a speciﬁc type of TDs. SATDs cover
diﬀerent types of TDs such as code, defect, and requirement debts by Bavota
et al.’s categorization [5]. In other words, as long as the document refers
to some aspect of technical debt it is treated as SATD. According to the
recent TDs categorization study, Fucci et al. [22] showed how SATDs are
mapped across 10 categories, e.g., poor implementation choices, partially
implemented, functional issues, etc.

2.2 Methods for Identiﬁcation of Technical Debt

One of the goals of industrial analytics is that new conclusions can be quickly
obtained from new data just by applying data mining algorithms. As shown
in Figure 2, there are at least nine separate stages that must be completed
before that goal be reached [4]. Each of these stages oﬀers unique and separate
challenges, each of which deserves extensive attention. Many of these steps have
been extensively studied in the literature [15, 16, 17, 30, 35, 37, 38, 53, 84, 85].
However, the labeling work of step 4 has been receiving scant attention. In
literature, there are several approaches for executing the labeling process:
1. Manual labeling;
2. Crowd sourcing;
3. Reuse of labels;
4. Automatic labeling;
5. Active learning (which is a special kind of semi-supervised learning)

All of these approaches have their drawbacks; e.g. they are error-prone
or will not scale. In response to these shortcomings, this study will take two
directions:
– First, we will try a label-free approach using a combination of pure unsuper-
vised learning techniques to pseudo-label the data, and subsequently active
learning, i.e., DebtFree(0);

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

7

– If the label-free approach fails, then we will try a hybrid of an active-learning
approach, called DebtFree(100), which starts with the help of unsupervised
learning to ﬁrst ﬁlter out the highly technical-debt prone comments before
the incrementally learning on all of the SATDs.

2.2.1 Manual labeling

In manual labeling, a team of (e.g.) graduate students assigns labels then (a)
cross-checks their work via say, a Kappa statistic; then (b) use some skilled
third person to resolve any labeling disagreements [36, 66, 67].

Manual labeling can be very slow. Tu et al. recently studied a corpus of
678 Github projects [67, 66]. A random selection of 10 projects from that cor-
pus had 22, 500 commits, which took 175 hours to manually label the commits
buggy, non-buggy (time includes cross-checking). That is, in a hypothetical sit-
uation of manual labeling 500 projects (with each project has 5,000 commits)
would have required 90 weeks of work.

2.2.2 Crowd Sourcing

Tu et al. [67] oﬀers a cost estimate of what resources would be required to
sub-contract that eﬀort to dozens of crowd sourced workers via tools like Me-
chanical Turk (MT). Applying best practices in crowd sourcing [10], assuming
(a) at least USA minimum ages [60]; and (b) our university taking a 50% over-
head tax on grants; then crowd sourcing the labeling of the issues from 500
projects would require $320,000 of grant reserve.

2.2.3 Reusing Labels

Because manual labeling can be time-consuming, crowd sourcing too expen-
sive, and micro-labeling error-prone, researchers often reuse labels from previ-
ous studies [78, 79, 81]. This approach is unsatisfactory for two reasons. One,
when exploring a new domain, there may be no relevant, pre-existing labels to
reuse. Two, reusing labels might propagate unsatisfactory label instances for
future work. For example, the widely cited NASA datasets in defect prediction
were found to have dubious quality [52, 59] in 2013 but has been utilized since
then. Speciﬁcally, Yu et al. [84] were exploring self-admitted technical debt
and found that their classiﬁers had an alarming high false positive rate. But
when they manually checked the labels of their data taken from a prior study
by Maldonado and Shihab [37], they found that over 98% of the reused false-
positive labels were incorrect. Table 2 shows some example comments whose
labels were updated in Yu et al. [84]’s study.

2.2.4 Automatic labeling

If labels cannot be generated manually or reused from other papers, using
automatic labeling processes is an attractive alternative. For example, defect

8

Huy Tu, Tim Menzies

Table 2: Examples of diﬀerent labels from the original datasets curated by Mal-
donado and Shihab [37] and the updated datasets by Yu et al. [84]

Project

Comment Text

Apache
Ant

ArgoUML

JFreeChart

JRuby

Columba

//TODO Test on other versions of weblogic
//TODO add more attributes to the task, to take
care of all jspc options //TODO Test on Unix
// skip backup files. This is actually a
workaround for the cpp generator, which always
creates backup files (it’s a bug).
// FIXME: we’ve cloned the chart, but the
dataset(s) aren’t cloned and we should do that
// All errors to sysread should be
SystemCallErrors, but on a closed stream Ruby
returns an IOError. Java throws same exception
for all errors so we resort to this hack...
// FIXME r.setPos();

Original
Label [37]
no

Yu et al.’s
Label [84]
yes

no

no

no

no

yes

yes

yes

yes

prediction papers [8, 28, 31, 32, 45, 48, 56] can label a commit as “bug-ﬁxing”
when the commit text contains certain keywords (e.g. ”bug”, “ﬁx”, “wrong”,
“error”, “fail”, etc [67]). Vasilescu et al. [68, 69] noted that these keywords
are used in a somewhat ad hoc manner (researchers peek at a few results,
then tinker with regular expressions that combine these keywords). Tu et al.
[67] had found that these simplistic keyword approaches can introduce many
errors, perhaps due to the specialization of the project nature or the ad-hoc
nature of their creation [68].

R-3c

Again, TDs are often “self-admitted” by developers in code comments [53]
as shown in Table 1 in order to signal other developers that the corresponding
TD that will be resolved for better results. In 2014, after
code has
studying four large-scale open-source software projects, Potdar and Shihab [53]
concluded that developers may intentionally leave traces of TDs (i.e., SATDs)
in their comments, such as “hack, ﬁxme, is problematic, this isn’t very solid,
probably a bug, hope everything will work, ﬁx this crap”). These comments tend
to make SATDs much easier to ﬁnd. Identifying and tracking SATDs have three
important beneﬁts as indicated §2.1. There are two prominent approaches to
automatically identify SATDs:

1. Pattern-based approaches [15, 16, 17, 37, 53] consist of three steps:
(1) manually inspect code comments and label each one as SATD or non-
SATD, (2) manually analyze the labeled items and summarize patterns for
SATDs, e.g., if a comment has keywords like “hack, ﬁxme, probably a bug”,
then it has a high chance of being related to a SATD, (3) apply the summarized
patterns to unlabeled comments to identify SATDs. Instead, Yu et al. [84]
proposed Easy as the SOTA pattern-based method to automatically identify
20-90% of SATDs by ﬁnding patterns associated with high precision (close to
100%).
Limitation of the SOTA Pattern-based approach: this approach does need ex-
tensively labeled training data to ﬁnd patterns that are associated with SATDs
because it relies on precision. As mentioned above, generating that data re-
quires intensive labor and expensive cost. Moreover, this method can still miss
up to 80% of SATDs.

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

9

2. Machine Learning approaches [30, 35, 38, 85] involve models work-
ing in the supervised learning manner, which are trained on labeled SATD
datasets to learn the underlying rules of comments admitting TDs. For exam-
ple, Tan et al. [61, 62] analyzed source code comments using natural language
processing to understand programming rules and documentations and indi-
cates comment quality and inconsistency. In 2017, Maldonado et al. [38] suc-
cessfully identiﬁed two types of SATD in 10 open-source projects (average 63%
F1 Score) using Natural Language Processing (Max Entropy Stanford Classi-
ﬁer) using only 23% training data. Huang et al. [35] introduced a Multinomial
Naive Bayes sub-classiﬁer for each training dataset using information gain as
feature selection then combine those sub-classiﬁers with boosting technique to
achieve an average of 73% F1 scores [30]. A recent IDE for Eclipse was also re-
leased using this technique for identifying SATD in Java projects [35]. Recently,
some studies explore diﬀerent feature engineering for identifying SATDs, e.g.
Wattanakriengkrai et al. [72] applied N-gram IDF as features, and Flisar and
Podgorelec [19] explored how feature selection with word embedding can help
the prediction. The latest progress are from Wang et al. [71]’s HATD and Ren
et al. [55]’s tuned CNN utilized a deep convolutional neural network to achieve
a higher F1 score than all the previous solutions. The HATD paper asserts that
their algorithm defeats CNN but, after much eﬀort, we could not reproduce
that result1. These machine learning models can be a good indicator for which
comments are more likely to be related to SATDs.
Limitation of the SOTA Machine Learning approach: deep learners often re-
quire having access to a substantial amount of labeled data which is not al-
ways available, especially in new domains (e.g., the success of open-source
projects). With precision ranging from 60% to 85%, it is not reliable to fully
automate the process. Human experts are then required to verify every deci-
sion the machine learning model made and thus costs a large amount of time
and labor.

2.2.5 Semi-supervised Learning

Finally, another approach is to only label a representative sample of the data,
build a classiﬁer from that sample, then use that classiﬁer to label the remain-
ing data [74]. To ﬁnd that representative example, an unsupervised learner (e.g.
associations rule learner), a clustering algorithm, or an instance selection algo-
rithm is used to ﬁnd repeated patterns in the data [32]. Then a human oracle
is asked to label one exemplar from each pattern. More sophisticated versions
of this scheme include active learners, where an AI tool advances ahead of the
human to fetch the most informative next sample to be labeled [33, 58]. If
humans agree to ﬁrst label only the most informative examples, then active
learners can be used to produce better models more eﬃciently by reducing the
number of examples that humans have to label.

1 We found that there is no reproduction package published with HATD. We tried con-

tacting the authors of that paper, without success.

10

Huy Tu, Tim Menzies

The more general term for active learning is semi-supervised learning. Both
terms mean “do what you can with a small sample of the labels” while active
learning adds a feedback loop that checks new labels one at a time with an
oracle. Moreover, semi-supervised learning relies on partially labeled data and
mostly unlabeled data.

Since 2012, active learning approaches have received scarce attention in
SE [33, 67, 83, 84]. Initially, active learning seems to be a promising method for
addressing the cost of label checking and generating: for self-admitted technical
debt, only 24% on a median of the training corpus had to be labeled [84]; using
active learning, eﬀort estimation for N projects only needed labels on 11% of
those projects [33]; further, while seeking 95% of the vulnerabilities in 28,750
Mozilla Firefox C and C++ source code ﬁles, humans only had to inspect 30%
of the code [83]. However, active learning still produces disappointing results.
For example, it is still a daunting task to “only” label 5% to 10% of the projects
in the 1,857,423 projects in RepoReapers [46] or the 9.6 million links explored
by Hata et al. [27]. Although it might be justiﬁed for very mission-critical
projects, consider the Firefox study [83] which required the human eﬀort of
inspecting 28,750 (total source code ﬁles) x 30% = 8,625 source code ﬁles
to identify 95% of the vulnerabilities. This is beyond the resources of most
analysts.

Several two-step frameworks were proposed for the active learning ap-
proach. Yu et al. [84] proposed Jitterbug to identify SATDs: (1) identify pat-
terns for the “easy to ﬁnd” SATDs (20-90% of all SATDs) with close to 100%
precision and automatically classify comments with the patterns as SATDs
(without human veriﬁcation), (2) apply machine learning techniques to guide
human experts to ﬁnd the remaining “hard to ﬁnd” SATDs with least num-
ber of comments read. Interestingly, Guo et al. [25] utilized a similar idea but
using only four keywords (“ﬁxme, todo, hack, xxx”) to identify the “easy to
ﬁnd” SATDs and applied supervised learning models to incrementally ﬁnd the
remaining “hard to ﬁnd” SATDs.
Limitation of the SOTA Active Learning approach:
– Costly in real-world: both steps (pattern-based method and machine learn-
ing) require the training data to be labeled in order to proceed which can be
costly in the real world, especially in a new domain. More importantly, Jit-
terbug’s active learning strategy relies on the ﬁrst step of the pattern-based
method in order to reach the target recall. Thus, without the labeled train-
ing data, Jitterbug’s guarantee in reaching the user-speciﬁed recall would
be almost impossible.

– Diﬃcult for Active Learning: the ﬁrst step of the pattern-based approach
identiﬁed up to 90% of the SATDs, but this makes it diﬃcult for the active
learning strategy to ﬁnd the rest of 10% SATDs. This can increase human
eﬀort to review the labels. This will be conﬁrmed later in §6.
Hence, in this SATDs identiﬁcation work, we aim to reduce the labeling
cost of both SOTA works including the SOTA semi-supervised learner from
Yu et al. [84] and the SOTA supervised learner from Ren et al. [55]. In the
process of developing such a method, our investigation shows our proposed

11
DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL
Table 3: Diﬀerences between two SOTAs of Ren et al. [55] and Yu et al. [84]
against our work, DebtFree. Again, the ﬁrst two steps of DebtFree are optional
as they will be investigated later.

Ren et al. [55]

Yu et al. [84]

DebtFree

Learning
Type

Supervised

Semi-Supervised

Semi-Supervised

1. Trains the deep learn-
ing model CNN on (N −
1) labeled datasets.

Core
Process

2. Uses the trained CNN
model to identify SATDs
on the target i dataset

out

1. Filtering
easy
SATDs in the target i
dataset with the pattern
recognizer, Easy, to learn
patterns with
higher
than 80% precision on
(N − 1) labeled datasets.

(N − 1)

2. Trains the RF model
on
labeled
datasets and incremen-
tally update the model
in the active
learning
manner.

the

(Optional) Pseudo-
1.
labels
(N − 1)
datasets with the un-
learner,
supervised
CLA [47].

2.
(Optional) Filtering
out easy SATDs in the
target i dataset with the
pattern recognizer (e.g.,
Easy or CLA) on (N − 1)
pseudo-labeled/labeled
datasets.

3. Trains the RF model
on (N − 1) pseudo-
labeled/labeled datasets
and incrementally update
the model in the active
learning manner.

- Use all 100% labels from
(N − 1) datasets.

- Use all 100% labels from
(N − 1) datasets.

- Use 0% labels from (N −
1) datasets with step 1.

Labeling
Eﬀort

- No labeling done on the
target i dataset.

- On average,
23% on the
dataset.

labeling
i

target

- On average,
11% on the
dataset.

labeling
i

target

method, DebtFree, also performs statistically similar or even better. The
diﬀerences between our approach and their are described in Table 3. The
investigation explores the usefulness of unsupervised learning through a mix
of approaches: (1) unsupervised learning in low-resource setting (unlabeled
data) can frugally pseudo-label the training data, (2) an unsupervised learner
acts as a preprocessor to ﬁlter out SATDs without relying on data labels in
high-resource settings (labeled data), and (3) a tuned active learning strategy
to specialize the learning on the target dataset by ﬁltering out the training
datasets when there is no beneﬁt from learning on them anymore.

In order to overcome the previously documented limitations in identifying
SATDs (i.e., the previous supervised learning and active learning methods are
expensive), unsupervised learning is a promising direction, but not competent
enough. To address this literature gap, it is an opportune time to propose
the DebtFree framework, which is based on the integration of unsupervised
learning and active learning. DebtFree investigates the combination of three
approaches including pseudo-labeling, ﬁltering, and active learning with diﬀer-
ent candidates for each approach. DebtFree’s conﬁgurations are formulated
after exploring two scenarios: training data labels are known and training data
labels are unknown. In the case of low-resource (training data with no labels),
we will pick DebtFree(0). On the other hand, given resources (training data
with labels), DebtFree(100) is employed instead.

12

Huy Tu, Tim Menzies

3 Methodology

3.1 General Framework

Our proposed DebtFree is an end-to-end solution that labels the data, ex-
tends the data corpus, and identiﬁes SATDs in a semi-supervised learning
approach. DebtFree is comprised of two settings DebtFree(0) and Debt-
Free(100).

3.2 DebtFree(0)

When there is no access to the labels of the training data, our study shows
that the ﬁltering step is not needed here and the best combination for Debt-
Free(0) consists of two steps: unsupervised learning with CLA [47] to cheaply
pseudo-labels the training data, and then our proposed active learning strat-
egy, Falcon, to incrementally update and learn to identify the SATDs on the
test data.

3.2.1 Pseudo-labeling via CLA/CLAFI

In the SOTA literature and comparative study of unsupervised models in de-
fect prediction, CLA starts with two steps of (1) Clustering the instances and
(2) LAbeling those instances accordingly to the cluster. In the low resource
setting with no labels available, we can label/predict all instances. CLAFI
is an extension of CLA which is a full-stack framework that also include (3)
Features selection and (4) Instances selection. Both CLA and CLAFI were ﬁrst
proposed by Nam and Kim [47] in the domain of defect prediction. The intu-
ition of such methods is based on the defect proneness tendency that is often
found in defect prediction research, that is the higher complexity is associated
with the proneness of the defects [14, 26, 44, 47, 54, 65]. Put simply, there is
a tendency where the problematic instance’s feature values are higher than
the non-problematic ones. For instance, Hassan et al. [26] predicted defects
using the entropy (or complexity) of code changes (the more complex changes
to a ﬁle, the higher the chance the ﬁle will contain faults). This tendency
and CLA’s/CLAFI’s eﬀectiveness were conﬁrmed via the recent literature and
comparative study of 40 unsupervised models in defect prediction across 27
datasets and three types of features. They found CLA’s/CLAFI’s performance
is superior to other unsupervised methods while similar to supervised learning
approaches. Moreover, Tu et al. [65] recently applied this intuition in develop-
ing their method to further the SOTA work for static analysis and issue close
time prediction. Therefore, this study investigates and ﬁnds that the hypoth-
esized tendency is also applicable in SATDs data but only eﬀective for the
semi-automated method but not the fully automated one. CLA is preferred
over CLAFI but this study examines both before choosing one over the other.

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

13

Before CLA or CLAFI, DebtFree(0) extracts features from each comment
candidate as L2-normalized terms (the square root of the sum of the squared
vector) terms with the TF-IDF2 scores (after stop word removal).
Clustering:
1. Find the median of feature F1, F2, ..., Fn (median(Fi)) across the whole

dataset.

2. For each data instance Xi, go through each feature value of the respective
data instance to count the time when the feature Fi > median(Fi) as Ki.
Labeling: label the instance Xi as SATD if Ki > median(K), else label it as
non-TD.
Feature Selection: Calculate the violation score per feature, called metric in
the original proposal of Nam et al. [47]. The process is done on both the train
and the test dataset.
1. For each Fi, go through all instances of Xj, a violation happens when Fi at

Xj is higher than the median(Ki) but Yj = 1 and vice-versa.

2. Sum all the violations per feature across the whole dataset and sort it in

ascending order.

3. Select the feature with the lowest violation score, if multiple of them have

the same score then pick all of them.

Instance Selection:
1. With the selected features, go through each instance Xi and check if the
respective Fj values violated the proneness assumption then remove that
instance Xi.

2. If the dataset does not have instances with both classes at the end then

pick the next minimum violation score to select metrics.

3. This process is only done on the training dataset.

After selecting features with the minimum violation scores and removing
the instances that violated the technical-debt proneness tendency, a practi-
tioner can train any machine learners on the preprocessed training data to
identify the SATDs from the unlabeled/test dataset. For this step, we picked
Random Forest which is also Jitterbug’s choice of learner after being compared
across other ones [84].

3.2.2 Active Learning via Falcon

The data that do not share the technical-debt proneness tendency is being
continuously learned through human and AI partnership, named Falcon as
the proposed active learning strategy from this study. First, a classiﬁcation
model (for SATD or non-SATD comments) is trained on the training dataset.
When reading the new comments, Falcon initially uses uncertainty sampling
to quickly build the model, then switches to certainty sampling to greedily
ﬁnd technical debt comments. The machine learner (RF) uses this feedback

2 For token t, its tf-idf score: Tﬁdf (t) = (cid:80)

or document d, Tﬁdf (t, d) = wt

d × (log

(cid:80)

token t appears in document d.

d∈D Tﬁdf (t, d), in which for token t in comment
|D|
i is the number of times
d∈D sgn(wt
d)

+ 1) where wt

14

Huy Tu, Tim Menzies

from human to learn their models incrementally. These trained model then
sorts the stream of comments such that humans read the most informative
ones ﬁrst (and the comments are sorted again each time a human oﬀers a new
label for a comment). After the found SATDs reach a speciﬁc threshold then
Falcon drops the training data and incrementally updates it with the target
data. This is similar to the separation action of the Falcon rocket to drop the
thruster for boosting up the rocket’s speed after reaching a required height.
More speciﬁcally, Falcon executes as follows:

Step 1 Feature Extraction: Given a set of comments candidates, Falcon
extracts features from each candidate as L2-normalized terms with
the highest TF-IDF. Initialize the set of labeled data points as L ← ∅
and the set of labeled positive data points as LB ← ∅.

Step 2 Bootstrap Learning: Falcon utilizes the labeled training dataset to
train a machine learning model, i.e., Random Forest (Yu et al.’s choice
of learner [84]).

Step 3 Initial Sampling: Falcon starts by randomly sampling unlabeled
candidate studies until humans declare that they see N1 = 1 technical-
debt examples.

Step 4 Uncertainty Sampling: Then, as human assessors oﬀer labels, one
example at a time, Falcon trains and updates with weighting to con-
trol query with uncertainty sampling, until N2 = 10 technical-debt
examples are found. Here, diﬀerent weights are assigned to each class
(WB = 1/|LB|, WN = 1/|LN |).

Step 5 Certainty Sampling: Next, Falcon trains further using certainty
sampling and Wallace’s “aggressive undersampling” [70] that culls
majority class examples closest to the decision boundary.

Step 6 Training Data Separation: Falcon drops training data after ﬁnding
more than 10% of estimated SATDs then the model is retrained on
the reviewed target data.

Step 7 Early Stopping: Falcon stops training when it is estimated that

N3 = 90% of the SATDs have been found.

To generate the N4 estimate, whenever the RF model is retrained, Falcon
makes temporary “guesses” about the unlabeled examples (by running those
examples through the classiﬁer). To turn these guesses into an estimate of the
remaining technical-debt comments, Falcon:

1. Builds a fast and simple model, e.g., Logistic Regression, using the guesses.
Faster feedback cycle to update the model and for the users to make deci-
sions

2. Using that regression model, Hard makes new guesses on the remaining

unlabeled examples.

3. Loops back to step1 until the new guesses are the same as the guesses in

the previous loop.

4. Uses this logistic regression model to estimate the remaining number of

positive examples in the data.

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

15

The reader will note that there are many speciﬁc engineering decisions
built into the above design (e.g. the values {N1 = 1, N2 = 10, N3 = 90%}).
Those decisions were initially made by Yu et al. [82] after exploring 32 diﬀerent
kinds of active learners. Those were later adopted by the SOTA active learning
framework Jitterbug [84] for SATD identiﬁcation.

3.3 DebtFree (100)

DebtFree(100) is inspired by the SOTA’s Jitterbug framework in the high-
resource setting, i.e., have access to labeled training data. Instead of a pattern-
based approach with a active learning strategy, we propose to replace the
pattern-based approach with an unsupervised learner to ﬁlter out the highly
technical-debt prone comments. Then, the state-of-the-art active learning ap-
proach (i.e., Hard [84]) guides the human experts to ﬁrst start with training on
training data labeled by the experts and then identify all of SATD comments
on new coming data.

3.3.1 Filtering via CLA

Here, SATDs from the test data can be early ﬁltered out with CLA. However,
instead of picking thresholdK = median(K) to label the training and test
data, we iterate thresholdK = percentile(allK, i) for i from 50% to 95% per-
centile. Then, the best thresholdK is the one with the highest precision to iden-
tify the SATDs within the training set. The test set with Ki > best thresholdK
are labeled as SATDs. Then, instances that meet the tendency in both test and
training datasets are removed. The intuition is similar to Yu et al. [84] is that
the “easier” SATDs can ﬁrst be easily identiﬁed where easier here means they
share the tendency of the higher complexity is associated with the proneness
of the technical-debts.

Table 4: Diﬀerences between the active learning strategies for this study: EM-
BLEM [67], HARD [84], and our proposed approach, i.e., FALCON.

start with learning on existing
training datasets.

drop the training data and re-
train the model on the re-
viewed test data after at-
taining a certain threshold of
SATDs

Emblem[67] Hard[84] Falcon

(cid:53)

(cid:53)

(cid:51)

(cid:53)

(cid:51)

(cid:51)

16

Huy Tu, Tim Menzies

(a) Emblem

(b) Hard

(c) Falcon

Fig. 3: The visual analogy of rocket launching for the comparison of Emblem,
Hard, and Falcon processes.

3.3.2 Active Learning via Hard

The active learning strategy here is Hard from Yu et al. [84]’s Jitterbug which
is similar to Falcon (as discussed previously in §3.2.2) except it does not ﬁlter
out training data at any point in the learning process. Another variation for
comparison also includes Tu et al.’s Emblem [67]. Emblem only uses one hun-
dred randomly sampled labeled test data to start the active learning strategy.
For Emblem, we suspected that due to the randomness in sampling, in the
ﬁrst one hundred instances the performance will be unstable. In summary, the
diﬀerences and similarities are documented in Table 4 and the performance of
all three methods will be compared in RQ2.1. For a more intuitive compar-
ison, Figure 3 shows the rocket launching process in each method. Emblem
is cheap with no need for the booster (labeled training data) but diﬃcult to
reach a substantial height (acquiring SATDs). Hard is eﬀective but expensive
to carry the whole booster all the way till the end. Falcon is a hybrid that uses
the booster upon launch to boost the rocket’s speed but is discarded once the
rocket reaches a certain height.

4 Experimental Design

4.1 Data

To validate this study’s hypothesis and the proposed methods’ eﬀectiveness, we
use the dataset from Maldonado et al. [37] which has been corrected by Yu et
al. [84]. The dataset includes ten open-source projects collected from Github:
Apache-Ant-1.7.0, Apache-Jmeter-2.10, Hibernate-Distribution3.3.2.GA, Ar-

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

17

Table 5: Dataset Details

Domain

Comments

Project

Apache
Ant
JMeter

ArgoUML

Columba

EMF

Hibernate

JEdit

Release
/ Year
/
1.7.0
2006
2.10
2013
-

/

/

/

/

/

1.4
2007
2.4.1
2008
3.3.2
2009
4.2
2004

JFreeChart 1.0.19 /

/

2014
1.4.0
2009
-

JRuby

SQuirrel
SUM
MEDIAN

Automating
Build
Testing

Dia-

UML
gram
Email Client

Model
Framework
Object Map-
ping Tool
Java
Editor
Java Frame-
work
Ruby for Java

Text

Database

Original
SATDs [37]
131 (3.2%)

Corrected
SATDs [84]
135 (3.3%)

374 (4.6%)

416 (5.2%)

1413 (15%)

1630 (17.3%)

204 (3.2%)

220 (3.4%)

104 (2.4%)

119 (2.7%)

472 (16%)

493 (17%)

4098

8057

9452

6468

4390

2968

10322

256 (2.5%)

259 (2.5%)

4408

4897

7215
62275
5683

209 (4.7%)

247 (5.6%)

622 (12.7%)

665 (13.4%)

286 (4%)

313 (4.3%)

4071 (6.5%) 4497 (7.2%)
271 (4.8%)

286 (5%)

goUML, Columba-1.4-src, EMF-2.4.1, jEdit-4.2, jFreeChart-1.0.19, jRuby-1.4.0,
SQL12. The provided dataset contains project names, classiﬁcation type (if
any) with actual comments.

Table 5 lists the varying statistics of the comments found in these 10
projects. As only a small ratio of the source code comments describe SATDs, it
would be time-consuming to label all comments manually. Thus, Maldonado
and Shihab developed 5 ﬁltering heuristics to eliminate comments that are
unlikely to be classiﬁed as comments [37]: (1) remove license comments/auto-
generated comments, (2) remove commented source code, (3) remove Javadoc
commented source comments, (4) group multiple single-line comments to-
gether, and (5) removing duplicate comments. In the end, the number of com-
ments that require manual annotation was signiﬁcantly reduced from 259,229
to 62,275 (reducing the data by 75%).

Furthermore in their work [37], two humans then manually classiﬁed each
comment according to the six diﬀerent types of SATD mentioned by Alves
et al. [3] if they contained any SATD at all, else marked them W IT HOU T
CLASSIF ICAT ION . Note that, we do not use the ﬁne-grained SATD cate-
gories proposed by Maldonado et al. [38], rather we focus on a binary problem
of instances being a SATD or not. Our study is concerned speciﬁcally with
identifying if a problem is SATD (e.g., DEF ECT , IM P LEM EN T AT ION ,
DESIGN , etc) or not (W IT HOU T CLASSIF ICAT ION ). Simply, as long
as the code comment refers to some aspect of technical debt it is treated as
SATD. Similarly to previous work [84, 55], we have changed the ﬁnal label
into a binary problem by deﬁning WITHOUT CLASSIFICATION as no and

18

Huy Tu, Tim Menzies

the rest of the categories as yes. Stratiﬁed sampling of the dataset is applied
to check personal bias, revealing with a 99% conﬁdence interval of 5%. A third
human veriﬁed the agreement between the two using stratiﬁed sampling and
reported a high level of agreement (Cohen’s Kapp [12] coeﬃcient of +0.81).
The signiﬁcantly high level of agreement indicates that the dataset is unbiased
and reliable.

On the contrary, Yu et al. [84] inspected this dataset for biases, and found
that 98% of the false positives were wrongly labeled. The diﬀerences between
the original and corrected SATDs count are reported in the last two columns
of the Table 5. Speciﬁcally, Maldonado et al. [37] missed more than 10% of
the total SATDs. This discrepancy highlights the importance of validating
prior research’s conclusions, data, and methodologies should one employ them
in their work, a process that might add signiﬁcant overhead to the amount of
required eﬀort. Fortunately, active learning oﬀers feasible remedial venues. Our
proposed method DebtFree also established new state of the art that outshined
the state-of-the-art active learning method for technical debt, Jitterbug.

4.2 Data Miners

There are several data miner options for the active learning part of the Debt-
Free(100) or supervised learning part of the DebtFree(0). For this study, we
only test simple and fast learners since the active learning model is updated/re-
trained frequently while practitioners appreciate quick feedback loop to im-
prove software,code, and technical-debts. Such learners include:

Logistic Regression: a statistical method that uses a logistic function
to model a binary dependent variable [29]. A standard logistic function is a
common “S” shape with Equation 1:

p(x) =

1
1 + e−(β0+β1x)

(1)

where p(x) ∈ [0, 1] for all t. Fitting on the training data, logistic regression
looks for the best parameter β to classify input data x into two target classes
{0, 1}. LR is used for estimating SATDs in DebtFree(100)’s active learning.
Random Forest: an ensemble learning method that constructs a multi-
tude of decision trees, each time with diﬀerent subsets of the data rows R and
columns C 3. Each decision tree is recursively built to ﬁnd the features that
yields the most reduction in entropy (higher entropy indicates lower ability to
draw conclusions in the paritioned data) [7]. Test data is then passed across all
N trees. Conclusions are determined by a majority vote across all the trees [6].
Holistically, RF is based on bagging (bootstrap aggregation) which averages
the results over multiple trees from sub-samples (reducing variance). Both
methods are popular in Machine Learning, and are implemented in the open-
source toolkit Scikit-learn [51]. Random Forest is used for classifying SATD
comments in DebtFree(0), DebtFree(100), and Yu et al.’s Jitterbug [84].

3 Speciﬁcally, using log2 C of the columns, selected at random.

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

19

4.3 Experimental Process

Experiments are conducted on the SATD dataset with 10 projects described
in §4.1. Each time, one project is selected as a target project (with labels
unknown) and the rest 9 datasets are treated as source projects. This study
addresses the following research questions:

RQ1: How well can the state-of-the-art unsupervised learning

method identify SATDs?

RQ2: How can the state-of-the-art active learning framework be

combined with the state-of-the-art unsupervised learner?

RQ3: How does the proposed DebtFree perform against state-of-

the-art models in identifying SATDs?

Recall our study investigates the combination of three approaches including
Pseudo-labeling, Filtering, and Active Learning. In this big data era, there are
available datasets easily curated from the web (e.g., Github) with no labels and
we can harvest valuable insights from them without labeling the data. The ﬁrst
RQ demonstrates that unsupervised learning by itself is not eﬀective but it is
useful for understanding some hidden patterns and early ﬁltering out SATDs.
In order to extend both the unsupervised learning approach and the active
learning approach, the second RQ experiments diﬀerent ways of integrating
both frameworks in two settings with RQ2.1 where there is no access to the
labels of the training data and RQ2.2 where there is access to the labels. For
RQ2.1, we test diﬀerent candidates for all three steps whereas in RQ2.2 we
only focus on candidates for the ﬁltering step and the active learning step.
With RQ2.1, we can simulate a real world situation of having data with no
labels by hiding the labels of the training data. During the active learning
step across the experiments, the oracles are queried for the target project, the
ground truth labels are applied to label the queried comments to simulate the
human-in-the-loop labeling process.

From RQ2, we ﬁnalize the best combination for two settings from RQ2 as
DebtFree(0) and DebtFree(100). In RQ3, we assess DebtFree(0)/(100)’s
usefulness by comparing them against the SOTA semi-supervised learning
work by Yu et al. [84] (i.e., Jitterbug) and the SOTA supervised learning
work by Ren et al. [55] (i.e., CNN). We recycle the available implementations
of their approaches instead of re-implementing them ourselves to generate the
results. Therefore, we are more conﬁdent that our conclusions or insights would
align with the original work.

4.4 Statistical Testing

We employ Cohen’d eﬀect size test to determine which results are similar by
calculating medium step2. We take guidance from Sawilowsky [57] at al.’s
work to determine the value of d to be used. That paper asserts that “small”
and “medium” eﬀects can be measured using d = 0.2 and d = 0.5, respectively.
We analyze this data looking for diﬀerences larger than d = (0.5 + 0.2)/2 =
0.35. This d is higher than the Jitterbug’s d = 0.2, ensuring the diﬀerences in

20

Huy Tu, Tim Menzies

the results are statistically signiﬁcant, and consequently higher conﬁdence in
the eﬀectiveness of the proposed methods:

M ediumstep2 = 0.35 · StdDev(All results)

(2)

5 Results

This section will provide details on the experiments and results for answering
the research questions listed above.

RQ1: How well can the state-of-the-art Unsupervised Learning

method identify SATDs?

In this experiment, we compare the performance of the following ﬁve treat-

ments:
– CLA: The unsupervised learner described in §3.2.1, which clusters the test
data instances based on the features’ median by marking the ones with
features higher than the median as SATDs and vice-versa.

– CLAFI+RF: A combination of CLAFI (§3.2.1) and the data miner RF
(§4.2). It started with pseudo-labeling the unlabeled training data with
CLA. Then the data is preprocessed by removing the features (for nine
train and one target datasets) and instances (only on the nine training
datasets) that violated the assumption from CLA. Then we pick Random
Forest as the learner choice from SOTA Yu et al.’s Jitterbug work’s data
miner of choice for supervised learning.

– CLA+RF: This is similar to the previous step except no features and

instances preprocessing step.

– Easy: The pattern-based approach, ﬁrst step of Jitterbug. For each pattern
p, ﬁnd score = P rec(p)4 · P (p) = T P (p)4/P (p)3 where P (p) is the number
of comments p (positives) and T P (p) is the number of SATD comments
containing p (true positives). We iteratively select the pattern with the
highest score, then removes comments containing that pattern from both
train and test data until the selected pattern has lower than 80% precision
on the training data. Then, stop and evaluate.

– Easy+CLA: First, the pattern-based approach Easy ﬁlters out test data
comments with patterns that have more than 80% precision on training
data and then passed to CLA for identiﬁed the rest of SATDs.
These pattern-based approaches can be seen as two groups: unsupervised
learners (i.e., CLA variants) and supervised learners (i.e., Easy and Easy+CLA).
In term of the eﬀectiveness of these methods, it is recommended to evaluate
multiple metrics so we employ Recall and APFD.

Recall measures the ability to identify the SATDs, Recall = T P/(T P +
F N ). T P is the number of true positives (SATD comments predicted as
SATDs), T N is the number of true negatives (non-SATD comments predicted
as non-SATDs), F P is the number of false positives (non-SATD comments pre-
dicted as SATDs), and F N is the number of false negatives (SATD comments
predicted as non-SATDs).

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

21

Table 6: Comparison between CLA, CLAFI+RF, CLA+RF, Easy, and
Easy+CLA are made in terms of Recall and APFD (the higher the bet-
ter). Medians and IQRs (delta between 75th percentile and 25th percentile,
lower the better) are calculated for easy comparisons. CLA, CLA+RF, and
CLAFI+RF do not have access to training data’s labels while Easy and
Easy+CLA do. Here, the darker cells show top rank while the lighter cells
show rank two where the diﬀerence between two ranks is statistically signiﬁ-
cant by being higher than M reported in the left most column.

Treatment

l
e
r
r
i
u
Q
S

CLA
CLAFI+RF
CLA+RF

65
53
81
58
Easy+CLA 93
73
CLA
74
CLAFI+RF
74
CLA+RF
57
Easy+CLA 92

Recall

(M = 8%) Easy

APFD

(M = 6%) Easy

r
e
t
e

M
J

72
31
75
77
96
73
63
74
76
94

F
M
E

55
30
77
41
82
61
58
64
41
81

t
n
A
e
h
c
a
p
A
67
46
91
27
91
78
64
73
27
82

L
M
U
o
g
r
A

67
47
85
90
95
72
70
53
83
96

e
t
a
n
r
e
b
i
H

62
42
84
75
95
65
57
62
71
93

t
i
d
E
J

79
39
76
22
98
91
76
80
22
93

t
r
a
h
C
e
e
r
F
J
55
25
78
55
71
61
57
64
54
80

a
b
m
u
l
o
C

64
23
61
88
98
70
67
64
87
98

y
b
u
R
J

65
23
82
90
98
72
68
68
85
98

n
a
i
d
e
M

65
35
80
67
95
72
66
66
64
93

R
Q

I

5
17
8
47
7
8
10
9
42
14

In order to take eﬀort into consideration for the performance of the model,
we also employ APFD. APFD calculates the area under the curve of the recall-
cost curve whereas other metrics (e.g. precision, recall, F1, G1) only evaluate
a single point of the curve. APFD ranges from 0.0 to 1.0, with higher values
indicating that higher recall could be achieved at a lower cost, and thus, more
advantageous. An APFD value of 0.5 can be achieved by randomly select the
next item each time.

– Recall = |{SATDs}∩{human veriﬁed comments}|

|{SATDs}|

– Cost = |{human veriﬁed comments}|

|{comments}|

Cautionary note: Menzies et al. [43] warned that precision can be mis-
leading for imbalanced data sets like those studied here (e.g. Table 5 reports
that the median of target class is 5%). Hence, we will not report precision
results and will not place much weight on F1. Table 6 shows the results of the
experiment. The Cohen’d eﬀect size test is applied to determine which results
are similar by calculating medium step2 or M across Recall and APFD.

On each target project in Table 6, the darker cells show top rank while
the lighter cells show rank two. Diﬀerent colors indicate the statistically sig-
niﬁcant diﬀerences at least equal or larger than the best result(s) subtracts
the medium step2 (M ). Observations from Table 6 include:
– Surprisingly, there was no improvements from the data preprocessing (met-
rics and instances selection) and supervised learning (RF) step after CLA.
– CLA performs similarly to Easy in the recall, betters in APFD. In another
word, CLA performs better than Easy without access to the train labels.

22

Huy Tu, Tim Menzies

– CLA+RF performs better than Easy in recall and APFD. CLA+RF per-

forms better than Easy without access to the training data’s labels.

– Notably, CLA+RF performs similarly to CLA, wins in Recall but loses in

APFD so CLA’s performance is more balanced.

– Easy+CLA performs the best. This indicates additional eﬀectiveness of
CLA as in bettering the performance of Easy. It is notable that Easy re-
lies on training data’s labels (i.e., supervised learning) in order to identify
SATDs when variants of CLA do not.
The results conﬁrm the eﬀectiveness of unsupervised learning (1) by itself
or with supervised learning in the case of no labels available, and (2) as a
post-processor after the SOTA pattern-based approach. There is a technical-
debt proneness tendency within the data to identify SATDs. Moreover, this
positive eﬀect also comes with the beneﬁt of not needing access to the labels.
The negative results from preprocessing the data (metrics and instances se-
lection) then supervised learning show that the selected metrics and instances
are not fully representative or relevant to identify SATDs on new data. Simply,
the technical-debt proneness tendencies that exist inside the training datasets
cannot be transferred completely to the target test dataset. Therefore, it might
not be eﬀective to learn from the training data at all.

At the same time, in case there are resources available for labeling the
training data, Easy is still a useful pattern-based approach to automatically
identify “easy to ﬁnd” SATDs. Then, CLA can be employed to identify the
rest of SATDs. The combination of Easy+CLA should be the state-of-the-art
automatic method for identifying SATDs. It is simple without any deployment
of a machine learning model for the traditional supervised learning approach.

The ﬁndings of this RQ is that:

Result:
From our exploration of various unsupervised learners, the original CLA
by Nam et al. performs the best. Moreover, CLA performs similarly to
the SOTA pattern-based approach, Easy [84], without having access to
the data’s labels (100% less eﬀort).

RQ2: How can the state-of-the-art active learning framework be

combined with the state-of-the-art unsupervised learner?

In the situation where business users want to reach a speciﬁc amount of
SATDs, our previously discussed automatic methods would not be able to
guarantee such recall. For this demand, an active learning approach is more
suitable [84]. However, the SOTA active learner [84] requires the training data
to be labeled which is very expensive why the RQ1’s unsupervised learning
method is eﬃcient but not eﬀective enough. Therefore, this RQ explores how
to integrate both methods in order to minimize the cost of labeling while
improving the eﬀectiveness in identifying SATDs in both settings of training
data labels unknown versus known.

RQ2.1: How to ﬁnd SATDs with no access to labeled training data?

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

23

In the low-resource setting (unlabeled training data), unsupervised learning
can help guess or pseudo-label the training data with no cost or the experts
can label the ﬁrst 100 instances or 1% of the test data before applying the
active learning strategy. The following methods are tested in this experiment:
– Emblem: without utilizing the training data, we start with random 100 test
data instances to incrementally update the model to guide the developers
to ﬁnd SATDs.

– Pseudo-labeling (via CLA) + Filtering (via CLA) + Hard: First, apply
unsupervised learning to pseudo-label the training data that highly ﬁts the
hypothesized technical-debts proneness tendency with zero human eﬀort.
Second, automatically identify the SATDs in test data via the same unsu-
pervised learner. Then, a active learning strategy (as explained in §3.2.2)
with the data miner RF (referenced in §4.2) to incrementally acquire infor-
mation and update the model in identifying the SATDs that do not ﬁt such
a tendency.

– Pseudo-labeling (via CLA) + Hard: This is synonymous with the previ-
ous one except skipping the ﬁltering via CLA step and simply apply Hard
to incrementally learn and identify SATDs on both frugally pseudo-labeled
data and new data.

– Pseudo-labeling (via CLA) + Filtering (via CLA) + Falcon: First, ap-
ply unsupervised learning to pseudo-label the training data based on the
hypothesized technical-debts proneness tendency. Then, the same method
is employed to ﬁlter out the highly technical-debt prone SATDs in the test
data. RF model is applied to continuously train on the pseudo-labeled data
and the rest of the test/target data until the found SATDs are 10% of
the estimated SATDs. Finally, the model will discard the cheaply labeled
training data while retraining the model on the reviewed test data so far
and continuously until the found SATDs are more than the user-speciﬁed
threshold (i.e. 90%).

– Pseudo-labeling (via CLA) + Falcon: This is synonymous to the previous

one except no early ﬁltering via CLA.
Beside APFD, we will also evaluate the methods in F1, G1, and the la-
beling cost. It is recommended to evaluate with metrics that aggregate mul-
tiple metrics like F1 and G1. F1 is a harmonic mean of recall and precision,
P recision = T P/(T P + F P ). G1 is a harmonic mean of recall and false-alarm
rate, F AR = F P/(T P + T N ):

F 1 =

2 · Precision · Recall
Precision + Recall

G1 =

2 · Recall · (1 − FAR)
Recall + (1 − FAR)

(3)

(4)

Table 7 reports the comparison between all the previously discussed meth-
ods across four metrics: F1, G1, APFD, and the labeling Cost. F1, G1, and
the labeling cost are measured for all methods aiming to ﬁnd 90% of the rest
SATDs. Except for the labeling cost, the higher the values of the other metrics
the better. The labeling cost is for labeling the test data labels. Pseudo-labeling
and Filtering are abbreviated to P and F. Similarly to RQ1, we also employ

24

Huy Tu, Tim Menzies

Table 7: Comparison between Emblem [67], P+Hard, P+F+Hard,
P+Falcon, and P+F+Falcon where P is Pseudo-Labeling and F is Fil-
tering. P and F are done via CLA. The comparison is made in terms of F1
score, G-score, APFD, and cost for identifying SATDs in the low-resource la-
beled data setting. For F1 score, G-score, and reviewing cost, both methods
target at ﬁnding 90% of the SATDs with its estimator. Medians and IQRs
(delta between 75th and 25th percentile, lower the better) are calculated for
easy comparisons. Here, the light red cells show best performing methods
where the diﬀerence between them are higher than M reported in the left
most column.

Treatment

Emblem[67]
P+Hard
P+F+Hard
(M = 8%) P+Falcon

F1

P+F+Falcon
Emblem[67]
P+Hard
P+F+Hard
(M = 6%) P+Falcon

G1

P+F+Falcon
Emblem[67]
P+Hard
P+F+Hard

APFD

(M = 3%) P+Falcon

P+F+Falcon
Emblem [67]
P+Hard
P+F+Hard

Cost

(M = 4%) P+Falcon

P+F+Falcon

l
e
r
r
i
u
Q
S

61
28
47
50
30
83
82
83
89
79
92
77
86
90
91
6
20
5
10
10

r
e
t
e

M
J

49
40
59
68
49
88
87
83
90
86
92
81
91
93
92
13
17
3
7
9

t
n
A
e
h
c
a
p
A
8
17
26
29
22
14
77
83
80
76
82
78
86
84
86
3
25
12
14
11

L
M
U
o
g
r
A

93
70
88
88
76
95
87
91
94
88
90
81
89
88
92
17
25
11
19
13

e
t
a
n
r
e
b
i
H

75
47
79
73
64
82
77
86
88
85
84
67
77
83
87
15
38
12
31
20

t
i
d
E
J

7
32
38
41
35
75
87
74
83
72
88
90
95
93
93
65
10
3
6
4

t
r
a
h
C
e
e
r
F
J
30
21
39
46
39
40
52
78
74
78
87
63
75
69
69
4
15
10
11
12

a
b
m
u
l
o
C

66
36
53
63
41
82
88
91
94
84
95
81
90
93
92
4
13
3
7
9

F
M
E

9
12
19
22
17
20
75
87
77
75
77
66
79
81
83
4
32
19
14
13

y
b
u
R
J

81
52
86
83
73
87
79
95
94
92
90
77
85
89
91
13
24
11
17
16

n
a
i
d
e
M

55
34
50
57
40
82
81
85
89
82
89
78
86
89
91
10
22
11
13
12

R
Q

I

66
26
41
32
29
47
10
8
14
10
6
14
11
10
6
11
10
9
10
4

Cohen’d medium eﬀect size test [57] to determine the best treatment(s) in
each target project. From the results, we can see:
– F1: P+Falcon performs the best (9/10) while P+F+Hard performs the

second-best (7/10).

– G1: P+Falcon outperforms the rest (9/10) while P+F+Hard’s performance

is placed at second.

– APFD: P+F+Falcon’s performance is the best (9/10) while P+Falcon and

Emblem perform similarly as the second best (7/10).

– Cost: Emblem’s and P+F+Hard’s processes are the most eﬃcient (7/10).
P+F+Hard always performs better than P+Hard across all four metrics.
However, the same eﬀect is not observed in P+Falcon and P+F+Falcon due to
Falcon losing some insights of frugally pseudo-labeled data after dropping the
training data at a certain threshold and the ﬁltering step further reduce the
insights (i.e, highly technical-debt prone comments) for Falcon to learn. How-
ever, P+Falcon outperforms all methods, except slightly lost to P+F+Falcon
in G1, and makes the system experts read 3% more on average. Falcon’s ef-

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

25

fectiveness overwrites the necessity of ﬁltering. Moreover, P+Falcon’s perfor-
mance is better than P+Hard indicating the eﬀectiveness of Falcon over Hard.
Overall, in the situation of having no access to the labeled training data,

the eﬀectiveness of Pseudo-labeling (via CLA) + Falcon demonstrates that:
– Unsupervised learning can cheaply and quickly guess the labels of the train-
ing data to provide insights for bootstrapping the active learning strategy.
However, those frugally pseudo-labeled data do not have enough insights
to help Hard guide the experts eﬃciently to identify the SATDs the whole
way up to 90%.

– Emblem alone is cost-eﬃcient but it does not eﬀectively identify the SATDs.
– A hybrid of both is more preferable as Falcon would drop training data
with guessed labels after the found SATDs are at least 10% of the estimated
SATD% so there are enough insights to help Falcon guide the experts to
eﬃciently identify the rest of SATDs.

– The best conﬁguration for a “label-free” process including a labeler to utilize
the unlabeled data for bootstrapping and the tuned active learning strategy
is labeling via CLA and Falcon. Hence, DebtFree(0) = Pseudo-labeling
(via CLA) + Falcon as shown in Figure 4.
RQ2.2: How to ﬁnd SATDs with having access to labeled training data?
For this research question, we are interested in the best conﬁgurations of
DebtFree(100) to take advantage of an abundant amount of labeled data
from previous research work [37, 84]. The comparison is made through the
F1 score, G1 score, APFD, and cost metrics. The methods are similar to the
previous RQ but without the labeling step:
– Filtering (via CLA) + Hard: First, apply unsupervised learning to learn
the hypothesized technical-debts proneness tendency on the humanly la-
beled training data then automatically identify the SATDs in test data
with zero human eﬀort. Second, an active learning strategy adopted from

Fig. 4: Workﬂows of DebtFree(0) = Pseudo-Labeling (via Unsupervised
Learning, i.e., CLA [47]) + Active Learning (via Falcon).

26

Huy Tu, Tim Menzies

Table 8: Comparison between Hard, F+Hard, Falcon, and F+Falcon in
terms of F1 score, G-score, APFD, and cost in identifying SATDs in the high-
resource labeled data setting. Filtering-F are done via CLA. For F1 score,
G-score, and reviewing cost, both methods target at ﬁnding 90% of the “hard
to ﬁnd” SATDs with its estimator. Medians and IQRs (delta between 75th
and 25th percentile, lower the better) are calculated for easy comparisons.
Here, the light red cells show best performing methods where the diﬀerence
between them are higher than M reported in the left most column.

Treatment

Hard
F+Hard
(M = 8%) Falcon

F1

G1
(M = 2%)

APFD
(M = 2%)

Cost
(M = 2%)

F+Falcon
Hard
F+Hard
Falcon
F+Falcon
Hard
F+Hard
Falcon
F+Falcon
Hard
F+Hard
Falcon
F+Falcon

l
e
r
r
i
u
Q
S

48
34
55
34
93
90
86
88
95
98
94
97
13
8
9
8

r
e
t
e

M
J

59
52
70
45
92
91
90
89
95
96
95
97
11
7
7
7

F
M
E

28
23
28
18
88
86
83
81
95
97
91
94
14
5
11
12

t
n
A
e
h
c
a
p
A
25
24
44
24
87
85
78
82
91
96
90
92
20
12
9
7

L
M
U
o
g
r
A

94
82
93
84
98
95
96
94
91
97
91
96
18
11
12
17

e
t
a
n
r
e
b
i
H

82
66
73
66
89
86
90
88
89
94
89
92
17
10
20
24

t
i
d
E
J

29
44
50
30
93
92
88
91
95
96
94
98
14
9
3
6

t
r
a
h
C
e
e
r
F
J
53
51
48
34
82
80
80
78
90
89
83
76
10
7
9
11

a
b
m
u
l
o
C

84
73
54
35
97
98
96
93
98
98
97
98
4
5
8
9

y
b
u
R
J

92
80
82
70
96
94
94
92
92
96
92
96
14
10
14
17

n
a
i
d
e
M

56
52
55
35
93
91
89
89
94
96
92
96
14
9
9
10

R
Q

I

55
29
25
36
8
8
11
10
4
1
4
5
6
3
4
10

the SOTA work [84] (§3.3.2) with the data miner RF (§4.2) to incrementally
acquire information and update the model in identifying the SATDs that
do not ﬁt such tendency.

– Hard (§3.3.2): This is synonymous with the previous one except no ﬁltering

via CLA in the beginning.

– Filtering (via CLA) + Falcon: The ﬁltering step is the same as above.
Then, an RF model is applied to continuously learn on the unﬁltered train-
ing data and the rest of the unﬁltered test/target data until the found
SATDs are 10% of the estimated SATDs. Finally, the model will discard
the labeled training data while retraining the model on the reviewed test
data so far and continuously until the found SATDs are more than the
user-speciﬁed threshold (i.e. 90%).

– Falcon (§3.2.2): This method is synonymous with the previous one except

for no ﬁltering step in the beginning.
The results for this RQ are reported in Table 8. Similarly, F1, G1, and
the labeling cost are measured for all methods aiming to ﬁnd 90% of the rest
SATDs. Except for the labeling cost, the higher the values of the other metrics
the better. The labeling cost is for labeling the test data labels. Filtering is ab-
breviated to F. Similarly to RQ1, we also employ Cohen’d medium eﬀect size
test [13, 57] to determine the best treatment to determine the best treatments
in each target project. From the results, it is observed that:

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

27

Fig. 5: Workﬂows of DebtFree(100) = Filtering (via Unsupervised Learning,
i.e., CLA [47]) + Active Learning (via Hard [84]).

– F1: Hard’s and Falcon’s performance are similarly as the best ones (7/10).
– G1: Hard outshines the rest over all the projects while F+Hard performs

as the second best (7/10).

– APFD: F+Hard outperforms the rest on ten out of ten projects and F+Falcon

comes in as second place (7/10).

– Cost: F+Hard is the most eﬃcient in eight out of ten projects and then

Falcon (6/10).

Falcon still outperforms F+Falcon across F1, G1, and cost while losing
on APFD. However, Hard outshines Falcon here, wins in cost, draws in F1,
and loses in G1 and APFD. This indicates that when there is an abundant
amount of labeled data, discarding them after reaching a certain threshold is
not as eﬀective as just active learning on the rest since there is enough insights
from the labels. Hard performs similarly to F+Hard as the best ones across all
methods by winning on F1 and G1 while losing on APFD and cost. However,
the beneﬁt of ﬁltering here is it helps stabilize Hard’s performance as Hard by
itself has a high variance across four metrics. As mentioned previously RQ1,
F1 and G1 scores are only single points of the curve while APFD calculates
the area under the recall-cost curve. In the theme of eﬀort-aware, the best
conﬁgurations of DebtFree(100) include Filtering (via CLA) with Hard as
shown in Figure 5.

From this RQ, we conclude that:

28

Huy Tu, Tim Menzies

Result:
With the eﬀort-aware theme, we investigate diﬀerent combinations of
active learners and CLA across two settings of the training data, either
with having 1-no access and 2-access to the labels to propose DebtFree.
In setting 1, or DebtFree(0), the best combination is Pseudo-Labeling
(via CLA) with our proposed active learner, Falcon. In setting 2, or
DebtFree(100), the best combination is Filtering (via CLA) with the
SOTA active learner for SATDs identiﬁcation, Hard.

RQ3: How does the proposed DebtFree perform against state-of-

the-art models in identifying SATDs?

For this research question, we are interested in the overall performance of
DebtFree by comparing (1) “label-free” DebtFree(0), (2) the latest state-of-
the-art deep convolutional neural network-based approach [55], (3) two-step
SOTA Jitterbug approach [84], and (4) DebtFree(100). The comparison is
made via F1 score, G1 score, APFD, and cost metrics:

– DebtFree(0): First, apply unsupervised learning to label the unlabeled
training data based on the hypothesized technical-debts proneness tendency.
Finally, Falcon is applied to continuously learn and identify the rest of
SATDs.

– Jitterbug [84]: First apply pattern-based approach (i.e, Easy, described in
§5’s RQ1) to automatically identify the “easy to ﬁnd” SATDs, then apply
a active learning strategy (i.e., Hard, described in §3.3.2) with RF as the
learner of choice to guide humans in identifying the “hard to ﬁnd” SATDs.
– CNN: deep learning solution that is based on a convolutional neural net-
work structure with word2vec features and hyperparameter tuning to clas-
sify each comment into SATD or non-SATD [55].

– DebtFree(100): First, apply CLA as an unsupervised learner to ﬁlter out
the test data that are most likely to be technical debts. Then, the SOTA
active learning strategy of Hard is applied to incrementally identify the rest
SATDs.

The results for this RQ are reported in Table 9. Similarly, F1, G1, and
the labeling cost are measured for all methods aiming to ﬁnd 90% of the
rest SATDs. Except for the labeling cost, the higher the values of the other
metrics the better. The labeling cost is for labeling the test data labels. Similar
to RQ1, we also employ Cohen’d medium eﬀect size test [13] to determine the
best treatment to determine the best treatments in each target project. From
the results, it is observed that:

– F1: CNN outperforms the rest of the methods in eight out of ten projects

and then DebtFree(0)’s performance is placed at second.

– G1: DebtFree(100) outshines the rest in nine projects and then Jitterbug

comes in second place.

– APFD: DebtFree(100) performs similarly as Jitterbug as the best ones in

nine out of ten projects.

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

29

Table 9: Comparison between DebtFree(0), CNN, Jitterbug, and Debt-
Free(100) in terms of F1 score, G-score, APFD, and cost. For F1 score, G-
score, and reviewing cost, both methods target at ﬁnding 90% of the “hard to
ﬁnd” SATDs with its estimator. Medians and IQRs (delta between 75th and
25th percentile, lower the better) are calculated for easy comparisons. Here,
the darker cells show top rank while the lighter cells show rank two where
the diﬀerence between two ranks is statistically signiﬁcant by being higher
than M reported in the left most column.

Treatment

DebtFree(0)
CNN [55]
Jitterbug [84]
DebtFree(100)
DebtFree(0)
CNN [55]
Jitterbug [84]
DebtFree(100)
DebtFree(0)
CNN [55]
Jitterbug [84]
DebtFree(100)
DebtFree(0)
CNN [55]
Jitterbug [84]
DebtFree(100)

l
e
r
r
i
u
Q
S

50
60
24
34
89
87
91
90
90
69
95
98
10
0
18
8

r
e
t
e

M
J

68
62
55
52
90
91
85
91
93
83
99
96
7
0
35
7

F
M
E

22
51
61
23
77
83
88
86
81
58
95
97
14
0
28
5

t
n
A
e
h
c
a
p
A
29
45
17
24
80
84
85
85
84
66
93
96
14
0
24
12

L
M
U
o
g
r
A

88
82
21
82
94
92
93
95
88
87
92
97
19
0
12
11

e
t
a
n
r
e
b
i
H

73
78
34
66
88
90
88
86
83
79
94
94
31
0
25
10

t
i
d
E
J

41
52
23
44
83
79
91
92
93
64
100
96
6
0
17
9

t
r
a
h
C
e
e
r
F
J
46
51
75
51
74
76
93
80
69
81
99
89
11
0
19
7

a
b
m
u
l
o
C

63
76
47
73
94
98
90
98
93
89
95
98
7
0
23
5

y
b
u
R
J

83
87
23
80
94
95
88
94
89
88
94
96
17
0
27
10

n
a
i
d
e
M

57
61
29
52
89
89
89
90
89
80
95
96
13
0
24
9

R
Q

I

32
27
32
29
14
33
3
8
10
21
5
1
10
0
8
3

F1

(M = 8%)

G1

(M = 2%)

APFD
(M = 4%)

Cost

(M = 4%)

– Cost: CNN is the most eﬃcient since it is modeled as the traditional su-

pervised approach (i.e., not touching the test/target data).
DebtFree(100) performs similarly to the SOTA supervised learning ap-
proach of CNN [55], wins in APFD, draws in G1, and loses in F1 and cost.
However, DebtFree(100)’s performance exceeds the SOTA two-step Jitterbug
approach [84] across three metrics (F1, G1, and cost) while performing sim-
ilarly in APFD. In particular, DebtFree(100) saves the labeling cost 250%
from Jitterbug on average (except in ArgoUML projects).

DebtFree(0) outperforms the SOTA two-step Jitterbug approach [84]
while performing similarly to the SOTA supervised learning approach of CNN
[55]. DebtFree(0) wins CNN in APFD, draws in G1, and loses in F1 and
cost. DebtFree(0) wins Jitterbug in F1 and cost, draws in G1, and loses in
APFD. Speciﬁcally, DebtFree(0) saves the labeling cost almost double than
Jitterbug on average (except in ArgoUML and Hibernate project). However,
DebtFree(0) loses to DebtFree(100) across four metrics. In general, both
of these SOTA work’s and DebtFree(100)’s eﬀectiveness rely on the labeled
training data, without them, they might not exist. On the other hand, Debt-
Free(0) does not require the training data to be labeled, and requiring no
labels from the training data is already a win in itself. Speciﬁcally, out of the
total ten datasets, there are nine labeled training datasets (90%) so CNN needs

30

Huy Tu, Tim Menzies

90% labeled data while Jitterbug needs 92% on average while DebtFree(0)
only need 1% of the data to be labeled (99% cheaper).

Notably, Falcon having access to labeled training data actually surpasses
CNN on three metrics (F1, G1, and APFD) while losing in cost. Moreover,
Falcon performs better than Jitterbug in F1 and cost, draws in G1, and loses
in APFD. Speciﬁcally, Falcon saves the labeling cost 240% from Jitterbug on
average (except in ArgoUML project). Hence, Falcon is more balanced than
Filtering (via CLA) and Hard in term of eﬀectiveness over both methods.

On a side note, active learning without the training data, Emblem, per-
forms similarly to Jitterbug (with access to labeled training data and a pattern-
based approach) as Emblem wins in F1 and Cost but loses in G1 and APFD.
Moreover, Emblem loses to Jitterbug by a small margin for G1 (median at 82
versus 89) versus APFD (median at 89 versus 95) but wins over by a larger
margin for F1 (median at 55 versus 29) and Cost (median at 13 versus 24).
This restates how labeled training data and insights from them are not com-
pletely transferred to the learning of the continuous strategy which contradicts
the previous SOTA’s conclusion.

Altogether, DebtFree’s eﬀectiveness can be concluded that:

Result:
When comparing against the SOTA semi-supervised learning work by
Yu et al. [84] and the SOTA supervised learning work (with deep learn-
ing) by Ren et al. [55], our proposed method DebtFree outperforms
them signiﬁcantly. First, DebtFree(100) performs similarly to Ren et al.
[55]’s work and better than Yu et al. [84]’s work while reducing the la-
beling cost by 2.5 times. Second, DebtFree(0) performs similarly to
Ren et al. [55]’s work without having access to the training data’s labels
and outperforms Yu et al. [84]’s work while being 99% less eﬀort.

6 Threats of Validity

There are several validity threats [18] to the design of this study. Any con-
clusion made from this work must be considered with the following issues in
mind:

Conclusion validity focuses on the signiﬁcance of the treatment. To en-
hance conclusion validity, we ran experiments on 10 diﬀerent target projects
and found that our proposed method always performed better than the state-
of-the-art approaches. More importantly, we applied a wider step (d = 0.35)
for the statistical testing of Cohen’d (described in RQ1 of §5) than the state-
of-the-art work [84] (d = 0.2) so the observed eﬀects are validated with more
conﬁdence. In addition, we have considered generalization issues of single eval-
uation metrics (e.g., recall and precision) and instead evaluate our methods
on metrics that aggregate multiple metrics like F1 and G1 while being more
eﬀort-aware (APFD and cost). As future work, we plan to test the proposed

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

31

Table 10: The left subtable showed TP/P rate after Easy and CLA while the
right subtable are the median ratios of estimated SATDs over actual SATDs
per iteration of the active learning strategies of Emblem, DebtFree(0)’s-
D(0)’s Falcon, DebtFree(100)’s-D(100)’s Hard, and Jitterbug’s-J’s Hard [84].
The median and IQR are also reported for ease of comparison.

(a) Percentage of SATDs being iden-
tiﬁed by the Easy and CLA in each
project.

Datasets
SQuirrel
JMeter
EMF
Apache Ant
ArgoUML
Hibernate
JEdit
JFreeChart
Columba
JRuby
Median
IQR

Easy CLA

58
77
41
27
90
76
22
55
88
90
67
47

47
22
25
42
45
31
35
13
18
27
29
13

(b) Median ratio of estimated SATDs over ac-
tual SATDs across all iterations per project
by each active learning method. The higher
the number than 100%, the more the approach
overestimate and vice-versa.

Datasets

Emblem D(0)’s D(100)’s

J’s

SQuirrel
JMeter
EMF
Apache Ant
ArgoUML
Hibernate
JEdit
JFreeChart
Columba
JRuby
Median
IQR

81
94
13
7
125
74
122
34
60
72
73
60

Falcon Hard Hard
153
69
429
83
76
75
23
100
1195
58
362
85
148
85
128
81
592
86
826
84
258
84
464
4

88
104
101
60
93
97
71
60
98
103
95
30

methods with additional analyses that are endorsed within SE literature (e.g.,
P-opt20 [67]) or general ML literature (e.g., MCC [11]).

Finally, in order to understand why our proposed method outshines the
others, we oﬀer a deeper analysis via Table 10. The SOTA active learning
Jitterbug starts with the ﬁltering step (Easy) before active learning (Hard)
on the rest. Recall §2.2.5, one of our hypothesized Jitterbug’s shortcomings is
Jitterbug’s Easy can ﬁnd up to 90% of the SATDs can make it diﬃcult for Hard
to follow up and ﬁnd the rest of 10% SATDs. Hence, Table 10.a reported the
percentage of SATDs being found via the ﬁltering step (Easy [84] or CLA [47])
and Table 9.b reported the median ratio of estimated SATDs 0over actual
SATDs across all iterations per project for Jitterbug’s Hard [84] (after Easy),
DebtFree(0)’s Falcon, DebtFree(100)’s Hard (after CLA), and Emblem [67].
The hypothesis here is that the higher the SATDs being discovered by the
early ﬁltering step, the harder it is to ﬁnd the rest of SATDs, and the higher
the overestimation is done by the consequent active learning strategy. This
forces the human experts to review more comments than necessary, which
results in more cost and eﬀort. For instance, in JRuby, Easy ﬁnds 90% of the
SATDs so Jitterbug’s Hard has a diﬃcult time ﬁnding the rest 10% and ends
up overestimating the rest SATDs on average by 8.3 times. Jitterbug’s Hard
always overestimates (up to 12 times than the actual SATDs) than the rest of
the methods except in the case of Apache Ant and EMF. Meanwhile, CLA only
ﬁnds up to 29% of the SATDs on average so the estimation of DebtFree(100)’s
Hard is only 16% away of the actual SATDs and more balanced (i.e, lowest
in IQR). With no prior ﬁltering step, Emblem tends to underestimate by 27%

32

Huy Tu, Tim Menzies

but DebtFree(0)’s Falcon has the closest estimation to the actual SATDs ,
i.e., 95%.

Internal validity focuses on how sure we can be that the treatment caused
the outcome. To enhance internal validity, we heavily constrained our experi-
ments to the same dataset, with the same settings, except for the treatments
being compared.

Construct validity focuses on the relation between the theory behind the
experiment and the observation. To enhance construct validity, we compared
solutions with and without our strategies in Table 7 and 8 while showing that
both components (unsupervised learning with CLA and active learning of
Falcon/Hard) and in both settings (low-resource labels versus labels abun-
dant) to improve the overall performance. However, we only showed that with
our setting of featurization and default parameters of random forest learn-
ers. The performance can get even better by tuning the parameters, variety of
scalers, and diﬀerent data preprocessors (e.g., synthetic minority over-sampling
or SMOTE that is known to help with unbalanced datasets [1, 9]). We plan
to explore these in our future work.

External validity concerns how widely our conclusions can be applied.
In order to test the generalizability of our approach, we always kept a project
as the holdout test set and never used any information from it in training.

7 Conclusion and Future Work

Managing Self-Admitted Technical Debts are important to maintaining a healthy
software project. The current automated solutions do not have satisfactory
precision and recall in identifying SATDs to fully automate the process. More-
over, the learning requires the training data to be labeled, which is not always
available because of high cost and labor as the case discussed in §2.1. We
showed that there is a “technical-debt proneness tendency” in the data where
SATDs are associated with higher complexity of the data. In order to reduce
the label famine and human eﬀort, a half-automated two-mode framework was
proposed, called DebtFree. If there is a lack of labeled data, DebtFree(0)
ﬁrst pseudo-labels the training data’s labels using an unsupervised learning
method that is based on “technical-debt proneness tendency”. When there
are abundant labeled training data, DebtFree(100) applies the same unsu-
pervised learner to ﬁnd the best tendency on the training data to ﬁlter out
the highly prone SATDs from the test data. Then, an active learning model
iteratively trains and update on both historically labeled data and new human-
labeled ones while guiding the human experts to target the most likely SATDs
according to the model’s ranking. Our proposed active learning method (i.e,
Falcon) is the best one for DebtFree(0) while Yu et al.’s Hard [84] is the best
one for DebtFree(100). The process can be repeated till the estimated recall
from the model reaches the predeﬁned target recall. Our ﬁndings include:
1. When combining the previous SOTA’s pattern-based approach (i.e., Easy)
and a simple unsupervised learning (i.e., CLA), the performance was higher

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

33

than Easy alone without any human eﬀort. Therefore, Easy+CLA should
be considered as the new simple baseline method for identifying SATDs.
2. In case of low-resource data, a combination method of CLA to pseudo-label
the training data and a novel active learning strategy (i.e., Falcon) surpassed
both the SOTA semi-automated method Jitterbug [84] and the SOTA deep
learning automated method CNN [55]. This serves as a proof-of-concept of
how unsupervised learning can cheaply label the training data to bootstrap
the active learning of a machine learning model to identify SATDs on the
test data.

3. In case of having access to the labeled data, a combination method of CLA
to ﬁlter out the highly prone SATDs and Hard performed similarly to Ren
et al.’s CNN [55] while outperforming Yu et al. [84] (5/8 cheaper).

4. Falcon is our novel active learning method for its eﬀectiveness in identifying
SATDs by frugally pseudo-labeling data and also when having access to the
training data labels (as Falcon without ﬁltering performed better than both
SOTA methods, in RQ3).

5. The success of those methods for technical debt suggests that there could
be many more domains in software analytics that could beneﬁt from un-
supervised learning. As mentioned above, those beneﬁts include the ability
to commission new models, faster, with much less time consuming and less
error-prone labeling of examples.

6. Overall, our proposed super learning method with DebtFree is the most

eﬀective and eﬃcient in identifying SATDs.
That said, DebtFree still suﬀers from the validity threats discussed in §6.
To further reduce those threats and to move forward with this research, we
propose the following future work:
– Apply hyper-parameter tuning on data preprocessing and model conﬁgu-
ration to see if our current conclusions still hold and whether tuning can
further improve the performance.

– Explore more complex patterns (other than just single word patterns Easy

has explored).

– Survey more advanced feature engineering in the active learning strategy
for ﬁnding the rest of SATDs. For example, explore N-gram patterns [72]
and word embeddings with deep neural networks [19].

– Explore other sampling techniques to help with unbalanced class data (one

of the key characteristics for SATDs [55]).

– Test whether replacing the random forest model in DebtFree with a deep

learning model (i.e., CNN [55]) will further improve its performance.

– Try diﬀerent settings of labeling and ﬁltering (via unsupervised learning)
combine with a deep learning model (i.e., CNN [55]) to automatically iden-
tify the SATDs.

– Survey other unsupervised learning methods for frugally pseudo-labeling

and ﬁltering data.

– Extend the work to label debt in other artifacts where technical debt may
be presented (e.g. issue trackers, documentation) and other software engi-
neering domains (e.g., security, issue close times, static warning analysis,

34

Huy Tu, Tim Menzies

etc) and compare it with other state-of-the-art methods which continue to
appear.

Acknowledgements

This work was partially funded by an NSF CISE Grant #1931425.

References

1. A. Agrawal and T. Menzies. Is” better data” better than” better data

miners”? In ICSE, 2018.

2. K. Ali and O. Lhot´ak. Application-only call graph construction.

In

ECOOP, 2012.

3. N. SR Alves, L. F Ribeiro, V. Caires, T. S Mendes, and R. O Sp´ınola.
Towards an ontology of terms on technical debt. In TechDebt, 2014.
4. S. Amershi, A. Begel, C. Bird, R. DeLine, H. Gall, E. Kamar, N. Nagap-
pan, B. Nushi, and T. Zimmermann. Software engineering for machine
learning: A case study. In ICSE, 2019.

5. G. Bavota and B. Russo. A large-scale empirical study on self-admitted

technical debt. In MSR, 2016.

6. L Breiman. Random Forests. Machine learning, 2001.
7. L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation

and regression trees. Cytometry, 1987.

8. G. Catolino. Just-in-time bug prediction in mobile applications: The do-

main matters! In MOBILESoft, 2017.

9. N. V Chawla, K. W Bowyer, L. O Hall, and W P. Kegelmeyer. Smote:

synthetic minority over-sampling technique. JAIR, 2002.

10. D. Chen, K. T Stolee, and T. Menzies. Replication can improve prior

results: A github study of pull request acceptance. In ICPC, 2019.

11. D. Chicco and G. Jurman. The advantages of the matthews correlation
coeﬃcient (mcc) over f1 score and accuracy in binary classiﬁcation evalu-
ation. BMC Genomics, 2020.

12. J. Cohen. Weighted kappa: Nominal scale agreement provision for scaled

disagreement or partial credit. Psychological Bulletin, 1968.

13. P. R. Cohen. Empirical Methods for Artiﬁcial Intelligence. MIT Press,

1995.

14. M. D’Ambros, M. Lanza, and R. Robbes. Evaluating defect prediction
approaches: a benchmark and an extensive comparison. EMSE, 2012.
15. M. A. de Freitas Farias, M. G. de Mendon¸ca Neto, A. B. da Silva, and
R. O. Sp´ınola. A contextualized vocabulary model for identifying technical
debt on code comments. In MTD, 2015.

16. M. A. de Freitas Farias, J. A. Santos, M. Kalinowski, M. Mendon¸ca, and
R. O. Sp´ınola. Investigating the identiﬁcation of technical debt through
code comment analysis. In ICEIS, 2016.

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

35

17. M. A. de Freitas Farias, M. G. de Mendon¸ca Neto, M. Kalinowski, and
R. O. Sp´ınola. Identifying self-admitted technical debt through code com-
ment analysis with a contextualized vocabulary. IST, 2020.

18. R. Feldt and A. Magazinius. Validity threats in empirical software engi-

neering research-an initial survey. In SEKE, 2010.

19. J. Flisar and V. Podgorelec. Identiﬁcation of self-admitted technical debt
using enhanced feature selection based on word embedding. IEEE Access,
2019.

20. F. A. Fontana, V. Ferme, and S. Spinelli. Investigating the impact of code

smells debt on quality code evaluation. In MTD, 2012.

21. W. Fu and T. Menzies. Revisiting unsupervised learning for defect pre-

diction. In FSE, 2017.

22. G. Fucci, N. Cassee, F. Zampetti, N. Novielli, A. Serebrenik, and
M. Di Penta. Waiting around or job half-done? sentiment in self-admitted
technical debt. In MSR, 2021.

23. J. Graf. Speeding up context-, object-and ﬁeld-sensitive sdg generation.

In SCAM, 2010.

24. Y. Guo, C. Seaman, R. Gomes, A. Cavalcanti, G. Tonin, F. QB Da Silva,
A. LM Santos, and C. Siebra. Tracking technical debt—an exploratory
case study. In ICSME, 2011.

25. Z. Guo, S. Liu, J. Liu, Y. Li, L. Chen, H. Lu, Y. Zhou, and B. Xu. Mat:
A simple yet strong baseline for identifying self-admitted technical debt,
2019.

26. A. E. Hassan. Predicting faults using the complexity of code changes. In

ICSE, 2009.

27. H. Hata, C. Treude, R. G. Kula, and T. Ishio. 9.6 million links in source

code comments: Purpose, evolution, and decay. In ICSE, 2019.

28. A. Hindle, D. M. German, and R. Holt. What do large commits tell us?:

A taxonomical study of large commits. MSR, 2008.

29. D. W Hosmer Jr, S. Lemeshow, and R. X Sturdivant. Applied logistic

regression. 2013.

30. Q. Huang, E. Shihab, X. Xia, D. Lo, and S. Li. Identifying self-admitted
technical debt in open source projects using text mining. EMSE, 2018.
31. Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi. A large-scale empirical study of just-in-time quality
assurance. TSE, 2013.

32. S. Kim, E. J. Whitehead, Jr., and Y. Zhang. Classifying software changes:

Clean or buggy? IEEE Trans SE, 2008.

33. E. Kocaguneli, T. Menzies, J. Keung, D. Cok, and R. Madachy. Active
learning and eﬀort estimation: Finding the essential content of software
eﬀort estimation data. TSE, 2012.

34. E. Lim, N. Taksande, and C. Seaman. A balancing act: What software
practitioners have to say about technical debt. IEEE Software, 2012.
35. Z. Liu, Q. Huang, X. Xia, E. Shihab, D. Lo, and S. Li. Satd detector: a
text-mining-based self-admitted technical debt detection tool. In ICSE,
2018.

36

Huy Tu, Tim Menzies

36. R. R Lutz and I. C. Mikulski. Empirical analysis of safety-critical anoma-

lies during operations. TSE, 2004.

37. E. da S Maldonado and E. Shihab. Detecting and quantifying diﬀerent

types of self-admitted technical debt. In MTD, 2015.

38. E. da S. Maldonado, E. Shihab, and N. Tsantalis. Using natural language
processing to automatically detect self-admitted technical debt. TSE,
2017.

39. R. Marinescu. Detection strategies: Metrics-based rules for detecting de-

sign ﬂaws. In ICSME, 2004.

40. R. Marinescu. Assessing technical debt by identifying design ﬂaws in
software systems. IBM Journal of Research and Development, 2012.
41. R. Marinescu, G. Ganea, and I. Verebi. Incode: Continuous quality as-

sessment and improvement. In CSMR, 2010.

42. A. Martini and J. Bosch. The danger of architectural technical debt:

Contagious debt and vicious circles. In 12th ICSA, 2015.

43. T. Menzies, A. Dekhtyar, J. Distefano, and J. Greenwald. Problems with
precision: A response to ”comments on ’data mining static code attributes
to learn defect predictors’”. TSE, 2007.

44. T. Menzies, J. Greenwald, and A. Frank. Data mining static code at-

tributes to learn defect predictors. TSE, Jan 2007.

45. A. Mockus and L. Votta. Identifying reasons for software changes using

historic databases. In ICPC, 2000.

46. N. Munaiah, S. Kroh, C. Cabrey, and M. Nagappan. Curating github for

engineered software projects. EMSE, 2017.

47. J. Nam and S. Kim. Clami: Defect prediction on unlabeled datasets. In

ASE, 2015.

48. M. Nayrolles and A. Hamou-Lhadj. Clever: Combining code metrics with
clone detection for just-in-time fault prevention and resolution in large
industrial projects. In MSR, 2018.

49. C. Ni, X. Xia, D. Lo, X. Chen, and Q. Gu. Revisiting supervised and un-
supervised methods for eﬀort-aware cross-project defect prediction. TSE,
2020.

50. A. Nugroho, J. Visser, and T. Kuipers. An empirical model of technical

debt and interest. In MTD, 2011.

51. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-
learn: Machine learning in python. JMLR, 2011.

52. J. Petri´c, D. Bowes, T. Hall, B. Christianson, and N. Baddoo. The jinx

on the nasa software defect data sets. In EASE, 2016.

53. A. Potdar and E. Shihab. An exploratory study on self-admitted technical

debt. In ICSME, 2014.

54. F. Rahman and P. Devanbu. How, and why, process metrics are better.

In ICSE, 2013.

55. X. Ren, Z. Xing, X. Xia, D. Lo, X. Wang, and J. Grundy. Neural network-
based detection of self-admitted technical debt: From performance to ex-
plainability. TOSEM, 2019.

DebtFree: Minimizing Labeling Cost in SATD Identiﬁcation using SSL

37

56. C. Rosen, B. Grawi, and E. Shihab. Commit guru: Analytics and risk

prediction of software commits. ESEC/FSE 2015, 2015.
57. S. Sawilowsky. New eﬀect size rules of thumb. JMASM, 2009.
58. B. Settles. Active learning literature survey. 2009.
59. M. Shepperd, Q. Song, Z. Sun, and C. Mair. Data quality: Some comments

on the nasa software defect datasets. TSE, 2013.

60. M S. Silberman, B. Tomlinson, R. LaPlante, J. Ross, L. Irani, and A. Zal-
divar. Responsible research with crowds: pay crowdworkers at least mini-
mum wage. Communications of the ACM, 2018.

61. L. Tan, D. Yuan, G. Krishna, and Y. Zhou. /* icomment: Bugs or bad

comments?*. In OSR, 2007.

62. S. H. Tan, D. Marinov, L. Tan, and G. T Leavens. @ tcomment: Testing
javadoc comments to detect comment-code inconsistencies. In ICST, 2012.
63. N. Tsantalis and A. Chatzigeorgiou. Identiﬁcation of extract method refac-

toring opportunities for the decomposition of methods. JSS, 2011.

64. N. Tsantalis, D. Mazinanian, and G. P. Krishnan. Assessing the refactora-

bility of software clones. TSE, 2015.

65. H. Tu and T. Menzies. Frugal: Unlocking ssl for software analytics. In

ASE, 2021.

66. Huy Tu, Rishabh Agrawal, and Tim Menzies. The changing nature of

computational science software, 2020.

67. Huy Tu, Zhe Yu, and Tim Menzies. Better data labelling with emblem

(and how that impacts defect prediction). TSE, 2020.
68. B. Vasilescu. Personnel communication at fse’18, 2018.
69. B. Vasilescu, Y. Yu, H. Wang, P. Devanbu, and V. Filkov. Quality and
productivity outcomes relating to continuous integration in github.
In
FSE, 2015.

70. B. C Wallace, T. A Trikalinos, J. Lau, C. Brodley, and C. H Schmid. Semi-
automated screening of biomedical citations for systematic reviews. BMC
bioinformatics, 2010.

71. X. Wang, J. Liu, L. Li, X. Chen, X. Liu, and H. Wu. Detecting and explain-
ing self-admitted technical debts with attention-based neural networks. In
ASE, 2020.

72. S. Wattanakriengkrai, N.

Sintoplertchaikul,
M. Choetkiertikul, C. Ragkhitwetsagul, T. Sunetnanta, H. Hata,
and K. Matsumoto. Automatic classifying self-admitted technical debt
using n-gram idf. In APSEC, 2019.

Srisermphoak,

S.

73. S. Wehaibi, E. Shihab, and L. Guerrouj. Examining the impact of self-

admitted technical debt on software quality. In SANER, 2016.

74. I Witten, Eibe Frank, M Hall, and C Pal. : Data mining: practical machine

learning tools and techniques. elsevier inc. 2017.

75. Z. Xu, L. Li, M. Yan, J. Liu, X. Luo, J. Grundy, Y. Zhang, and X. Zhang. A
comprehensive comparative study of clustering-based unsupervised defect
prediction models. JSS, 2021.

76. M. Yan, Y. Fang, D. Lo, X. Xia, and X. Zhang. File-level defect prediction:

Unsupervised vs. supervised models. In ESEM, 2017.

38

Huy Tu, Tim Menzies

77. J. Yang and H. Qian. Defect prediction on unlabeled datasets by using

unsupervised clustering. In HPCC/SmartCity/DSS, 2016.

78. X. Yang, D. Lo, X. Xia, and J. Sun. Tlel: A two-layer ensemble learning

approach for just-in-time defect prediction. IST, 2017.

79. Xinli Yang, David Lo, Xin Xia, Yun Zhang, and Jianling Sun. Deep learn-
ing for just-in-time defect prediction. In QRS, pages 17–26. IEEE, 2015.
80. Y. Yang, Y. Zhou, J. Liu, Y. Zhao, H. Lu, L. Xu, B. Xu, and H. Leung.
Eﬀort-aware just-in-time defect prediction: Simple unsupervised models
could be better than supervised models. In 24th SIGSOFT FSE, 2016.
81. Y. Yang, Y. Zhou, J. Liu, Y. Zhao, H. Lu, L. Xu, B. Xu, and H. Leung.
Eﬀort-aware just-in-time defect prediction: Simple unsupervised models
could be better than supervised models. FSE, 2016.

82. Z. Yu, N. A. Kraft, and T. Menzies. Finding better active learners for

faster literature reviews. EMSE, 2018.

83. Z. Yu, C. Theisen, L. Williams, and T. Menzies. Improving vulnerability

inspection eﬃciency using active learning. TSE, 2019.

84. Z. Yu, F. M. Fahid, H. Tu, and T. Menzies.

Identifying self-admitted

technical debts with jitterbug: A two-step approach. TSE, 2020.

85. F. Zampetti, A. Serebrenik, and M. Di Penta. Automatically learning
patterns for self-admitted technical debt removal. In SANER, 2019.
86. N. Zazworka, R. O Sp´ınola, A. Vetro, F. Shull, and C. Seaman. A case

study on eﬀectively identifying technical debt. In EASE, 2013.

87. F. Zhang, Q. Zheng, Y. Zou, and A. E Hassan. Cross-project defect pre-
diction using a connectivity-based unsupervised classiﬁer. In ICSE, 2016.
88. Y. Zhou, Y. Yang, H. Lu, L. Chen, Y. Li, Y. Zhao, J. Qian, and B. Xu.
How far we have progressed in the journey? an examination of cross-project
defect prediction. TOSEM, 2018.

