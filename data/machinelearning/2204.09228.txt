Tight Last-Iterate Convergence of the Extragradient and the
Optimistic Gradient Descent-Ascent Algorithm for
Constrained Monotone Variational Inequalities

Yang Cai*†
Yale University
yang.cai@yale.edu

Argyris Oikonomou*†
Yale University
argyris.oikonomou@yale.edu

Weiqiang Zheng†
Yale University
weiqiang.zheng@yale.edu

May 17, 2022

Abstract

The monotone variational inequality is a central problem in mathematical programming that
uniﬁes and generalizes many important settings such as smooth convex optimization, two-
player zero-sum games, convex-concave saddle point problems, etc. The extragradient algo-
rithm by Korpelevich [1976] and the optimistic gradient descent-ascent algorithm by Popov [1980]
are arguably the two most classical and popular methods for solving monotone variational in-
equalities. Despite their long histories, the following major problem remains open. What is the
last-iterate convergence rate of the extragradient algorithm or the optimistic gradient descent-ascent al-
gorithm for monotone and Lipschitz variational inequalities with constraints? We resolve this
open problem by showing that both the extragradient algorithm and the optimistic gradient
last-iterate convergence rate for arbitrary convex
descent-ascent algorithm have a tight O
feasible sets, which matches the lower bound by Golowich et al. [2020a,b]. Our rate is measured
in terms of the standard gap function. At the core of our results lies a non-standard performance
measure – the tangent residual, which can be viewed as an adaptation of the norm of the operator
that takes the local constraints into account. We use the tangent residual (or a slight variation
of the tangent residual) as the the potential function in our analysis of the extragradient algo-
rithm (or the optimistic gradient descent-ascent algorithm) and prove that it is non-increasing
between two consecutive iterates.

(cid:16) 1√

(cid:17)

T

2
2
0
2

y
a
M
6
1

]

C
O
.
h
t
a
m

[

3
v
8
2
2
9
0
.
4
0
2
2
:
v
i
X
r
a

*Supported by a Sloan Foundation Research Fellowship and the NSF Award CCF-1942583 (CAREER).
†Part of this work was done while the author was visiting the Simons Institute for the Theory of Computing.

 
 
 
 
 
 
Contents

1

Introduction
1.1 Our Performance Measure: the Tangent Residual . . . . . . . . . . . . . . . . . . . .
1.2
1.3 Related Work .

.
Sum-of-Squares based Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

.

.

.

2 Preliminaries

3 The Tangent Residual and Its Properties

4 Best-Iterate Convergence of EG with Constant Step Size

1
3
4
4

7

9

11

5 Last-Iterate Convergence of EG with Constant Step Size

12
5.1 Warm Up: Unconstrained Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5.2 Last-Iterate Convergence of EG with Arbitrary Convex Constraints . . . . . . . . . . 15

6 Last-Iterate Convergence of OGDA with Constant Step Size

20
6.1 Warm Up: Unconstrained Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
6.2 Last-Iterate Convergence of OGDA with Arbitrary Convex Constraints . . . . . . . . 21

A Additional Preliminaries

B Missing Proofs from Section 4

C Missing Proofs from Section 5.1

26

31

33

D Non-Monotonicity of Several Standard Performance Measures

33
D.1 Non-Monotonicity of the Natural Residual and its Variants . . . . . . . . . . . . . . . 34
D.2 Non-Monotonicity of the Gap Functions and its Variant . . . . . . . . . . . . . . . . . 35

E Optimistic Gradient Descent Ascent Algorithm

35
E.1 Best-Iterate Convergence of OGDA with Constant Step Size . . . . . . . . . . . . . . 36
E.2 Best-Iterate of Φ
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
E.3 Monotonicity of the Potential
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
E.4 Combining Everything .

.

.

.

.

.

.

k

F Agnostic to Lemma 6 Proof for Monotonicity of Tangent Residual of EG

43

1 Introduction

The monotone variational inequality (VI) problem plays a crucial role in mathematical programming,
providing a unifying setting for the study of optimization and equilibrium problems. It also serves
as a computational framework for numerous important applications in ﬁelds such as Economics,
Engineering, and Finance [Facchinei and Pang, 2007]. Monotone VIs have been studied since the
1960s [Hartman and Stampacchia, 1966, Browder, 1965, Lions and Stampacchia, 1967, Brezis and
Sibony, 1968, Sibony, 1970]. Formally, a monotone VI is speciﬁed by a closed convex set Z ⊆ Rn
and a monotone operator F : Z → Rn,1 with the goal of ﬁnding a z∗ ∈ Z such that

(cid:104)F(z∗), z∗ − z(cid:105) ≤ 0 ∀z ∈ Z.

(1)

We further assume the operator F to be Lipschitz, which is a natural assumption that is satisﬁed
in most applications and is also made in the majority of algorithmic works concerning monotone
VIs. An important special case of the monotone and Lipschitz VI is the convex-concave saddle
point problem:

min
x∈X

max
y∈Y

f (x, y),

(2)

where X and Y are closed convex sets in Rn, and f (·, ·) is smooth, convex in x, and concave in
y.2 Besides its central importance in Game Theory, Convex Optimization, and Online Learning,
the convex-concave saddle point problem has recently received a lot of attention from the machine
learning community due to several novel applications such as the generative adversarial networks
(GANS) (e.g., [Goodfellow et al., 2014, Arjovsky et al., 2017]), adversarial examples (e.g., [Madry
et al., 2018]), robust optimization (e.g., [Ben-Tal et al., 2009]), and reinforcement learning (e.g., [Du
et al., 2017, Dai et al., 2018]).

The extragradient (EG) algorithm by Korpelevich [1976] and the optimistic gradient descent-
ascent (OGDA) algorithm by Popov [1980] are arguably the two most classical and popular
methods for solving Lipschitz and monotone VIs. Interestingly, a fundamental property of these
two simple and natural algorithms remained elusive despite their long histories. Namely, the
last-iterates of both algorithms are only known to asymptotically converge to a solution of the
monotone and Lipschitz VI,3 but no upper bounds on the rate of convergence had been provided
for general settings. Motivated by this gap of our understanding, the following question has been
posed as an open question in several recent works [Wei et al., 2021b, Golowich et al., 2020b,a,
Hsieh et al., 2019].

What is the last-iterate convergence rate of the extragradient algorithm and the optimistic gradient
descent-ascent algorithm for monotone and Lipschitz variational inequalities with constraints?

1F is monotone if (cid:104)F(z) − F(z(cid:48)), z − z(cid:48)(cid:105) ≥ 0 for all z, z(cid:48) ∈ Z.
2If we set F(x, y) =

(cid:19)

(cid:18) ∇x f (x, y)
−∇y f (x, y)

and Z = X × Y, then (i) F(x, y) is a monotone and Lipschitz operator, and (ii)

the set of saddle points coincide with the solutions of the monotone VI for operator F and domain Z.

3The last-iterate asymptotic convergence of EG can be found in Korpelevich [1976] and Facchinei and Pang [2007],

and the last-iterate asymptotic convergence of OGDA can be found in Popov [1980] and Hsieh et al. [2019].

1

We resolve this open problem by providing the tight last-iterate convergence rate of EG and
OGDA under arbitrary convex constraints.
Indeed, the same problem has not been answered
even for two-player zero-sum games, arguably one of the most basic monotone and Lipschitz
VIs.4 Prior to our work, the only setting where an upper bound on the rate of convergence exists
for either EG or OGDA for solving general Lipschitz and monotone VIs is when the problem is
unconstrained, i.e., Z = Rn, [Golowich et al., 2020b, Gorbunov et al., 2021, Golowich et al., 2020a].

Main Result: For any monotone and Lipschitz variational inequality problem with an arbitrary
convex constraint set Z, both EG and OGDA with constant step size achieve
a tight last-iterate convergence of O
in terms of the standard convergence
T
measures – the gap function (Deﬁnition 1) and the tangent residual (Deﬁnition 3).a
See Theorem 3 for the formal statement. We further show that the tangent residual
is an upper bound of the natural residual (Lemma 1), so our result also implies a
tight last-iterate convergence rate of O( 1√
T

) for the natural residual.

(cid:16) 1√

(cid:17)

aIn the unconstrained setting, the tangent residual is simply the (cid:96)2-norm of F. In the constrained setting,

the tangent residual is the (cid:96)2-norm of F’s projection to the tangent cone.

Our upper bounds in terms of the gap function and the natural residual match the lower bounds
of Golowich et al. [2020b,a], that is, they match in all of the following terms: T, the Lipschitz
constant of F, and the distance between the starting point z0 and the solution z∗.

To the best of our knowledge, our result is the ﬁrst to provide a last-iterate convergence rate for
solving monotone and Lipschitz VIs using any algorithm that belongs to the general class known
as p-stationary canonical linear iterative algorithms (p-SCLI) [Arjevani and Shamir, 2016], which con-
tains the EG, OGDA, and other well-known algorithms. Although often viewed as an approx-
imation to EG, OGDA has an additional feature compared to EG, i.e., it is a no-regret learning
algorithm (see e.g., [Rakhlin and Sridharan, 2013]). A nice implication of our result for OGDA is
that for smooth and monotone games (see Appendix A for the deﬁnition), players can each play
a no-regret learning algorithm, i.e., OGDA, with constant learning rate, and the overall player
behavior exhibits O( 1√
) last-iterate convergence rate to a Nash equilibrium in terms of the gap
T
function.

Why Last-Iterate Convergence? Both the EG and OGDA algorithms are known to have average-
iterate convergence. In particular, the average of the iterates of the algorithm converges at a rate of
O(1/T) [Nemirovski, 2004, Auslender and Teboulle, 2005, Tseng, 2008, Monteiro and Svaiter, 2010,
Mokhtari et al., 2020, Hsieh et al., 2019]. Nonetheless, there are several important reasons to study
last-iterate convergence. First, not only is last-iterate convergence theoretically stronger and more
appealing, it is also the only type of convergence that describes the trajectory of an algorithm.
As demonstrated by Mertikopoulos et al. [2018], the trajectory of an algorithm may be cycling
around in the space perpetually while still converges in the average-iterate sense. In game theory,

4A two-player zero-sum game can be speciﬁed by its payoff matrix A ∈ R(cid:96)×m. It is a special case of the convex-
concave saddle point problem, where X = ∆(cid:96) , Y = ∆m (∆k denotes the k-dimensional simplex), and the function
f (x, y) = x(cid:62) Ay.

2

we often view these algorithms as models of agents’ behavior in a system/game. Thus, only last-
iterate convergence provides a description of the evolution of the system. Additionally, EG and
OGDA have been successfully applied to improve the training dynamics in GANs, as the training
of GANs can be formulated as a saddle point problem [Daskalakis et al., 2018, Yadav et al., 2018,
Liang and Stokes, 2019, Gidel et al., 2019a,b, Chavdarova et al., 2019]. On the one hand, in this
formulation of GANs, the objective function f is usually non-convex and non-concave, making
existing theoretical guarantees for the average iterate inapplicable. On the other hand, the last
iterate typically has good performance in practice. Thus, it is crucial to develop machinery that
allows us to analyze the behavior of the last iterate of these algorithms.

1.1 Our Performance Measure: the Tangent Residual

A major challenge we face for establishing the last-iterate convergence for EG or OGDA in the
constrained setting is the choice of the convergence measure. For simplicity, we focus on our
choice of the performance measure for EG, as our performance measure for OGDA is similar and
inspired by our performance measure for EG. In the unconstrained case, the central performance
measure for EG is the norm of the operator. The key component in both [Golowich et al., 2020b]
and [Gorbunov et al., 2021] is to establish that the norm of the operator at the last iterate (also the
T-th iterate) is upper bounded by O( 1√
) last-iterate convergence rate for
T
the gap function.

), which implies a O( 1√
T

In the constrained setting, the norm of the operator is a poor choice to measure convergence,
as it can be far away from 0 even in the limit, and is hence insufﬁcient to guarantee convergence
in terms of the gap function. A standard generalization of the norm of the operator in the con-
strained setting is the natural residual (Deﬁnition 4), which takes the constraints into account and
is guaranteed to converge to 0 in the limit. Unfortunately, we observe that the natural residual
is not monotonically decreasing even in basic bilinear games (see Appendix D), making it difﬁcult
to directly analyze. Similar non-monotonicity has been observed for several other natural perfor-
mance measures such as the norm of the operator mapping introduced in [Diakonikolas, 2020]
and the gap function, leaving all these performance measures unsuitable. See more discussion
about these performance measures in Section 3 and Appendix D.

We choose a non-standard performance measure: the tangent residual, which can be viewed
as the norm of the operator projected to the tangent cone of the current iterate (Deﬁnition 3). To the
best of our knowledge, this performance measure has not been used in the study of EG or OGDA.
The tangent residual plays a crucial role in our analyses for both algorithms. Unlike the aforemen-
tioned performance measures, we show that the tangent residual is monotonically decreasing and has
a last-iterate convergence rate of O( 1√
) for EG. For OGDA, we prove that a small modiﬁcation of
T
the tangent residual is monotonically decreasing, which implies that the tangent residual has a
last-iterate convergence rate of O( 1√
). Using the convergence rate of the tangent residual, we can
T
easily derive the last-iterate convergence rate of other classical performance measures such as the
natural residual or the gap function. However, we suspect these rates can be challenging to obtain
directly.

3

1.2 Sum-of-Squares based Analysis

We provide a quick overview on how we establish the monotonicity of the tangent residual of the
EG algorithm. We ﬁrst introduce the concept of sum-of-squares programming.

Sum-of-Squares (SOS) Programming. Suppose we want to prove that a polynomial p(x) ∈
R[x1, . . . , xn] is non-negative over a semialgebraic set S = {x ∈ Rn : gi(x) ≤ 0, ∀i ∈ [m]},
where each gi(x) is also a polynomial. One way is to construct a certiﬁcate of non-negativity, for
example, by providing a set of nonnegative coefﬁcients {ai}i∈[m] ∈ Rm
≥0 such that p(x) + ∑i∈[m] ai ·
gi(x) is a sum-of-squares polynomial, that is, a polynomial that can be expressed as the sum of
squares of further polynomials. Surprisingly, if p(x) is indeed non-negative over S, a certiﬁcate
of non-negativity always exists as guaranteed by a foundational result in real algebraic geometry
– the Krivine-Stengle Positivestellensatz [Krivine, 1964, Stengle, 1974], a generalization of Artin’s
resolution of Hilbert’s 17th problem [Artin, 1927]. Note that, it is sometimes necessary to allow
more sophisticated forms of certiﬁcates than in the example above, e.g., replacing each coefﬁcient
ai with a SOS polynomial si(x), etc. The complexity of a certiﬁcate is parametrized by the highest
degree of the polynomial involved. The SOS programming consists of a hierarchy of algorithms,
where the d-th hierarchy is an algorithm that searches for a certiﬁcate of non-negativity up to degree
2d based on semideﬁnite programming.

We mainly discuss the analysis of EG here, as the analysis of OGDA is similar and also based
on SOS programming. At the core of our analysis of the EG algorithm lies the monotonicity of the
squared tangent residual, which can be formulated as the non-negativity of a degree-4 polynomial
in the iterates.5 Our original proof directly applies SOS programming to certify the non-negativity
of this degree-4 polynomial. The certiﬁcate is rather complex and involves a polynomial identity
of a degree-8 polynomial in 27 variables, which we discover by solving a degree-8 SOS program.
Interested readers can ﬁnd the proof in Appendix F. In this version, we include a simpliﬁed proof.
By introducing auxiliary vectors that are not part of the update rule of EG, we provide an equivalent
formulation of the squared tangent residual (Lemma 6) that is a degree-2 polynomial, which al-
lows us to prove the monotonicity of the squared tangent residual using a degree-2 SOS program.
The proof can be found in Section 5.2.

For OGDA, we are not able to show that the squared tangent residual is monotone. Inspired by
the adaptive potential proof in [Golowich et al., 2020a], we suspect that some extra correction term
is needed to construct the potential function. Instead of trying to devise such a correction term
manually, we manage to directly ﬁnd one by searching over a family of performance measures
using SOS programming. The search we perform is heuristic but might be helpful to discover
potential functions in other problems. See Section 6 for a more detailed discussion.

1.3 Related Work

Last-Iterate Convergence Rate for EG-like Algorithms in the Unconstrained Setting.
Golowich et al. [2020b,a] show a lower bound of Ω( 1√
) for solving bilinear games using any p-
T

5The tangent residual is not a polynomial, but the squared tangent residual is a degree-4 polynomial

4

SCLI algorithms, which include EG and OGDA. For EG, Golowich et al. [2020b] show an matching
upper bound under an additional second-order smoothness condition. Gorbunov et al. [2021] im-
prove the result and show that the same upper bound holds without the second-order smoothness
condition. For OGDA, Golowich et al. [2020a] provides a matching upper bound under the same
second-order smoothness condition. These upper bounds hold for all smooth and Lipschitz VIs.
With the additional assumption that the operator F is cocoercive, Lin et al. [2020] show a O( 1√
)
T
convergence rate for online gradient descent. If we further assume that either F is strongly mono-
tone in VI or the payoff matrix A in a bilinear game has all singular values bounded away from 0,
linear convergence rate is known for EG, OGDA, and several of their variants [Daskalakis et al.,
2018, Gidel et al., 2019a, Liang and Stokes, 2019, Mokhtari et al., 2020, Peng et al., 2020, Zhang and
Yu, 2020].

Last-Iterate Convergence Rate for EG-like Algorithms in the Constrained Setting. The results
for the constrained setting are sparser. If the operator F is strongly monotone, we know that EG
and some of its variants have linear convergence rate [Tseng, 1995, Malitsky, 2015]. Several papers
establish the asymptotic convergence, i.e., converge in the limit, of the optimistic multiplicative
weight updates in constrained convex-concave saddle point problems [Daskalakis and Panageas,
2019, Lei et al., 2021]. Finally, a recent paper by Wei et al. [2021b] provides a linear rate convergence
of OGDA for bilinear games when the domain is a polytope. They show that there is a problem de-
pendent constant 0 < c < 1 that depends on the payoff matrix of the game as well as the constraint
set, so that the error shrinks by a 1 − c factor. However, c may be arbitrarily close to 0, even if we
assume the corresponding operator to be L-Lipschitz. As a result, their convergence rate is slower
than ours when T is not comparable to 1
c , which may be exponentially large in the dimension n,
though their rate will eventually catch up. Overall, their “instance-speciﬁc” bound is incompara-
ble and complements the worst-case view taken in this paper, where we want to derive the worst-
case convergence rate for all VIs with monotone and L-Lipschitz operator F. Our result is the ﬁrst
last-iterate convergence rate in this worst-case view and matches the lower bound by Golowich
et al. [2020b,a].

Other Algorithms and Performance Measures. Other than the gap function, one can also mea-
sure the convergence using the norm of the operator if the setting is unconstrained, or the nat-
ural residual (Deﬁnition 4) or similar notions if the setting is constrained. In the unconstrained
setting, Kim [2021], Yoon and Ryu [2021], and Lee and Kim [2021] provide algorithms that ob-
tain O( 1
T ) convergence rate in terms of the norm of the operator, which is shown to be optimal
by Yoon and Ryu [2021] for Lipschitz and monotone VIs. In the constrained setting, Diakoniko-
las [2020] shows the same O( 1
T ) convergence rate under the extra assumption that the operator is
cocoercive and loses an additional logarithmic factor when the operator is only monotone. Our
result implies a O( 1√
) last-iterate convergence rate in terms of the natural residual for both EG
T
and OGDA. From an optimization point of view, i.e., the goal is to solve a Lipschitz and mono-
tone VI, we should choose one of the above faster algorithms over EG or OGDA. However, one of
our main motivation is game theoretic, that is, we would like to view simple algorithms such as
OGDA and EG as models of agents’ behavior and understand the speed for the overall behavior

5

to converge to a Nash equilibrium. From this game-theoretic view point, we believe understand-
ing the last-iterate convergence rate of simple algorithms such as EG and OGDA is an important
question.

Computer-Aided Proofs. A powerful computer-aided proof framework – the performance esti-
mation problem (PEP) technique (e.g., [Drori and Teboulle, 2014, Taylor et al., 2017b]) is widely
applied to analyze ﬁrst-order iterative methods. Indeed, the last-iterate convergence rate of EG in
the unconstrained setting by Gorbunov et al. [2021] is obtained via the PEP technique. Although
the PEP framework can handle projections [Taylor et al., 2017a, Ryu et al., 2020, Goujaud et al.,
2022, Dragomir et al., 2021], the main challenge for applying it to the constrained setting is that,
the PEP framework requires the performance measures to be polynomials of degree 2 or less (see
e.g., [Taylor et al., 2017a]).6 In fact, solving the PEP is equivalent to solving a degree-2 SOS pro-
gram, which can be viewed as the dual of the PEP [Tan et al., 2021]. In the unconstrained setting,
the performance measure is a degree-2 polynomial – the squared norm of the operator, and that
is why one can either use the PEP (as in [Gorbunov et al., 2021]) or a degree-2 SOS to certify its
monotonicity (Theorem 1). In the constrained setting, we use the squared tangent residual to mea-
sure the algorithm’s progress, which in our original formulation is a degree 4 polynomial, making
the PEP framework not directly applicable.7 As the SOS approach can accommodate polynomial
objectives and constraints of any degree, we could directly apply it to certify the monotonicity of
the tangent residual in the constrained setting, although the resulting proof is complex. With the
new formulation of the squared tangent residual (Lemma 6), we manage to simplify our proof
and derive it using a degree-2 SOS program. It is also not hard to see that one can apply the PEP
framework on the new formulation of the squared tangent residual, and the resulting program is
the dual program of our degree-2 SOS program. We believe an interesting future direction is to
understand whether there are natural settings in optimization where degree-2 SOS programs are
provably insufﬁcient and higher degree SOS programs are necessary.

Lessard et al. [2016] analyze ﬁrst-order iterative algorithms for convex optimization using a
technique inspired by the stability analysis from control theory. They model ﬁrst-order iterative
algorithms using discrete-time dynamical systems and search over quadratic potential functions
that satisfy a set of Integral Quadratic Constraints (IQC). Zhang et al. [2021] extend the IQC frame-
work to study smooth and strongly monotone VIs in the unconstrained setting.

SOS Programming and Analysis of Iterative Methods. SOS programming has been employed
in the design and analysis of algorithms in convex optimization. To the best of our knowledge,
these results only concern minimization of smooth and strongly-convex functions in the uncon-
strained setting. Fazlyab et al. [2018] propose a framework to search the optimal parameters of the
algorithm, e.g., step size. They use SOS programming to search over quadratic potential functions
and parameters of the algorithm with the goal of optimizing the exponential decay rate of the

6More speciﬁcally, the PEP framework requires the performance measure as well as the constraints to be linear in
(i) the function values at the iterates and (ii) the Gram matrix of a set of vectors consisting of the iterates and their
gradients.

7The tangent residual is the square root of a rational function and can only be even harder to handle.

6

potential function. Tan et al. [2021] proposes to use SOS programming to study the convergence
rates of ﬁrst-order methods in unconstrained convex optimization.

Simultaneous Result on Last-Iterate Convergence of OGDA in the Unconstrained Setting.
Shortly after we obtained the last-iterate convergence rate for OGDA in the constrained setting,
we learned in early March, 2022 from private communication that Eduard Gorbunov, Gauthier
Gidel, and Adrien Taylor had been working on the same problem. At the time of the communica-
tion, they could obtain the same last-iterate convergence rate for OGDA in the unconstrained case
using a different method based on PEP.

2 Preliminaries

We consider the Euclidean Space (Rn, (cid:107) · (cid:107)), where (cid:107) · (cid:107) is the (cid:96)2 norm and (cid:104)·, ·(cid:105) denotes inner
product on Rn. We use z[i] to denote the i-th coordinate of z ∈ Rn and ei to denote the unit vector
such that ei[j] := 1[i = j], the dimension of ei is going to be clear from context.

Variational Inequality.
variational inequality problem is deﬁned as follows: ﬁnd z∗ ∈ Z such that

Given a closed convex set Z ⊆ Rn and an operator F : Z → Rn, a

(cid:104)F(z∗), z∗ − z(cid:105) ≤ 0 ∀z ∈ Z.

(3)

We say F is monotone if (cid:104)F(z) − F(z(cid:48)), z − z(cid:48)(cid:105) ≥ 0, for all z, z(cid:48) ∈ Z, and is L-Lipschitz if,

(cid:107)F(z) − F(z(cid:48))(cid:107) ≤ L(cid:107)z − z(cid:48)(cid:107) for all z, z(cid:48) ∈ Z.

Remark 1. One sufﬁcient condition for such a z∗ to exist is when the set Z is bounded, but there are also
other sufﬁcient conditions that apply to unbounded Z. See [Facchinei and Pang, 2007] for more details.
Throughout this paper, we only consider monotone VIs that have a solution.

Deﬁnition 1 (Gap Function). A standard way to measure the performance of z ∈ Z is by its gap function
deﬁned as GAPZ,F,D(z) = maxz(cid:48)∈Z ∩B(z,D) (cid:104)F(z), z − z(cid:48)(cid:105), where D > 0 is a ﬁxed parameter and B(z, D)
is a ball with radius D centered at z.8 When Z, F and D are clear from context, we omit the subscripts and
write the gap function at z as GAP(z).

The Extragradient Algorithm. Let zk be the k-th iterate of the Extragradient (EG) Algorithm. The
update rule of EG is as follows:

zk+ 1

2

= ΠZ [zk − ηF(zk)] = arg min
z∈Z

zk+1 = ΠZ

(cid:104)

(cid:105)

zk − ηF(zk+ 1

2

)

= arg min
z∈Z

(cid:107)z − (zk − ηF(zk)) (cid:107),
(cid:16)

zk − ηF(zk+ 1

2

(cid:13)
(cid:13)
(cid:13)z −

(cid:17)(cid:13)
(cid:13)
(cid:13) .

)

(4)

(5)

8Sometimes the gap function is deﬁned to allow z(cid:48) to take value in Z ∩ B(z∗, (cid:107)z∗ − z0(cid:107)), where z0 is the starting
point of the EG algorithm, and z∗ is the solution that the last iterate of the algorithm converges to. Due to Lemma 3,
(cid:107)zk − z∗(cid:107) ≤ (cid:107)z0 − z∗(cid:107) for every k, so B(zk, 2(cid:107)z∗ − z0(cid:107)) contains B(z∗, (cid:107)z∗ − z0(cid:107)).

7

The Optimistic Gradient Descent-Ascent Algorithm. Let zk and wk be the k-th iterate of the
Optimistic Gradient Descent Ascent Method (OGDA) method. Let z0, w0 be arbitrary starting
points in Z. The update rule is as follows:

wk+1 = ΠZ [zk − ηF(wk)] = arg min
z∈Z
zk+1 = ΠZ [zk − ηF(wk+1)] = arg min
z∈Z

(cid:107)z − (zk − ηF(wk)) (cid:107)

(cid:107)z − (zk − ηF(wk+1))(cid:107)

(6)

(7)

Note that the OGDA method only requires T queries to the operator at {wk}0≤k≤T−1, while
EG requires 2T queries to the operator. Additionally, OGDA is a more natural algorithm in multi-
agent online learning settings [Cesa-Bianchi and Lugosi, 2006, Shalev-Shwartz et al., 2012], as
players play according to the strategy proﬁle wk and receive gradient feedback F(wk) to compute
zk and wk+1, while EG requires players to play every half step zk and zk+ 1
to get gradient feedback.
Finally, as we mentioned before, OGDA is a no-regret algorithm while EG is not.

2

In Section 4 and 5, we present the analysis of the EG algorithm and provide a detailed descrip-
tion about how to use SOS programming to derive the proof. The analysis of the OGDA algorithm
is a simple extension of our analysis to the EG algorithm. We formally state the results of OGDA
in Section 6 and postpone the detailed analysis of OGDA in Section E.

Sum-of-Squares (SOS) Polynomials. Let x be a set of variables. We denote the set of real poly-
nomials in x as R[x]. We say that polynomial p(x) ∈ R[x] is an SOS polynomial if there exist
polynomials {qi(x) ∈ R[x]}i∈[M] such that p(x) = ∑i∈[M] qi(x)2. We denote the set of SOS polyno-
mials in x as SOS[x]. Note that any SOS polynomial is non-negative.

In Figure 1 we present a generic formulation of a degree-d SOS program. The
SOS Programs.
SOS program takes three kinds of input, a polynomial g(x), sets of polynomials {gi(x)}i∈[M] and
{hi(x)}i∈[N]. Each polynomial in {g(x)} ∪ {gi(x)}i∈[M] ∪ {hi(x)}i∈[N] has degree of at most d. The
SOS program searches for an SOS polynomial in the set of polynomials Σ = {g(x) + ∑i∈[M] pi(x) ·
gi(x) + ∑i∈[N] qi(x) · hi(x)}, where {pi(x)}i∈[M] and {qi(x)}i∈[N] are polynomials in x. More pre-
cisely for each i ∈ [M], pi(x) is an SOS polynomial with degree at most d − deg(gi(x)). For each
i ∈ [N], qi(x) is a (not necessarily SOS) polynomial with degree at most d − deg(gi(x)). Note
that any polynomial in set Σ is at most degree d. In our applications, we choose {gi(x)}i∈[M] to
be non-positive polynomials and {hi(x)}i∈[N] to be polynomials that are equal to 0. Any feasible
solution to the program certiﬁes the non-negativity of g(x).

Roadmap of the Paper.
In Section 3, we introduce our new performance measure – the tangent
residual and prove some of its properties. In Section 4, we show that the EG algorithm enjoys best-
iterate convergence. In Section 5, we strengthen the convergence guarantee for the EG algorithm
and obtain the tight last-iterate convergence rate by showing that the tangent residual (Deﬁni-
tion 3) is non-increasing across the iterations of the EG algorithm. The last-iterate convergence
rate for the tangent residual also implies a last-iterate convergence rate for the gap function (Deﬁ-
nition 1) and the natural residual (deﬁnition 4) as shown in Lemma 2 and Lemma 1. In Section 6,

8

Input Fixed Polynomials.

• Polynomial g(x)
• Polynomial gi(x) ∈ R[x] for all i ∈ [M].
• Polynomial hi(x) ∈ R[x] for all i ∈ [N].

Decision Variables of the SOS Program:

• pi(x) ∈ SOS[x] is an SOS polynomial with degree at most d − deg (gi), for all i ∈ [M].
• qi(x) ∈ R[x] is a polynomial with degree at most d − deg (hi) , for all i ∈ [N].

Constraints of the SOS Program:

g(x) + ∑
i∈[M]

pi(x) · gi(x) + ∑
i∈[N]

qi(x) · hi(x) ∈ SOS[x]

Figure 1: Generic degree d SOS program.

we further prove the tight last-iterate convergence rate for the OGDA algorithm. The analysis
of the OGDA algorithm follows the same steps as in the analysis of the EG algorithm, and we
postpone most of the details in Appendix E.

3 The Tangent Residual and Its Properties

We formally introduce our performance measure the tangent residual . As discussed in Section 1.1,
many standard and natural performance measures, i.e., the natural residual, (cid:107)zk − zk+1/2(cid:107),9
(cid:107)zk − zk+1(cid:107), maxz∈Z (cid:104)F(z), zk − z(cid:105) and maxz∈Z (cid:104)F(zk), zk − z(cid:105), are unfortunately non-decreasing
for EG. See Appendix D for numerical examples.

Deﬁnition 2 (Unit Normal Cone). Given a closed convex set Z ⊆ Rn and a point z ∈ Z, we denote
by NZ (z) = (cid:104)v ∈ Rn : (cid:104)v, z(cid:48) − z(cid:105) ≤ 0, ∀z(cid:48) ∈ Z (cid:105) the normal cone of Z at point z and by (cid:98)NZ (z) = {v ∈
NZ (z) : (cid:107)v(cid:107) ≤ 1} the intersection of the unit ball with the the normal cone of Z at z. Note that (cid:98)NZ (z) is
nonempty and compact for any z ∈ Z, as (0, . . . , 0) ∈ (cid:98)NZ (z).

Deﬁnition 3 (Tangent Residual). Given an operator F : Z → Rn and a closed convex set Z, let
TZ (z) := {z(cid:48) ∈ Rn : (cid:104)z(cid:48), a(cid:105) ≤ 0, ∀a ∈ CZ (z)} be the tangent cone of z, and deﬁne JZ (z) := {z} + TZ (z).
The tangent residual of F at z ∈ Z is deﬁned as rtan
JZ (z)[z − F(z)] − z(cid:107). An equivalent deﬁ-
nition is rtan

(F,Z )(z) := (cid:107)Π
(cid:104)a, F(z)(cid:105)2.

(F,Z )(z) := (cid:114)(cid:107)F(z)(cid:107)2 − max a∈ (cid:98)NZ (z),

(cid:104)F(z),a(cid:105)≤0

Remark 2. We show the equivalence of the two deﬁnitions of tangent residual in Lemma 7. For the rest of
the paper, we may use either of the two equivalent deﬁnitions depending on which one is more convenient.

9(cid:107)zk − zk+1/2(cid:107) is proportional to the norm of the operator mapping introduced in [Diakonikolas, 2020].

9

When the convex set Z and the operator F are clear from context, we are going to omit the
subscript and denote the unit normal cone as (cid:98)N(z) = (cid:98)NZ (z) and the tangent residual as rtan(z) =
rtan
(F,Z )(z). Although the deﬁnition is slightly technical, one can think of the tangent residual as the
norm of another operator (cid:98)F, which is F projected to all directions that are not “blocked” by the
boundary of Z if one takes an inﬁnitesimally small step (cid:101) · F(z), which is the same as projecting F
to JZ (z). Intuitively, if the tangent residual is small, then the next iterate will not be far away from
the current one.

Next, we formally deﬁne the natural residual associated with the instance formally stated in

Deﬁnition 4, and show how it is related to the tangent residual.

Deﬁnition 4. Consider an instance I of the variational inequality problem on convex set Z ⊆ Rn and
monotone operator F : Z → Rn. For z ∈ Z, the natural map and natural residual associated with I is
deﬁned as follows

K (z) = z − ΠZ (z − F(z)),
Fnat

(F,Z )(z) = (cid:13)
rnat

(cid:13)Fnat

K (z)(cid:13)
(cid:13).

Given an instance of the monotone VI constrained on convex set Z ⊆ Rn and operator F :
Z → Rn, point z∗ is a solution of the monotone VI iff rnat
(F,Z )(z∗) = 0. In Lemma 1, we show that
the tangent residual upper bounds the the natural residual. See Figure 2 for illustration of how
the tangent residual relates to the natural residual.

Lemma 1. Consider an instance I of the variational inequality problem on convex set Z ⊆ Rn and
monotone operator F : Z → Rn. For any z ∈ Z, rtan

(F,Z ) ≥ rnat

(F,Z )(z).

Proof. Let w = ΠZ (z − F(z)) and a1 = z − F(z) − w. Observe that

(cid:107)F(z)(cid:107)2 = (cid:107)z − w(cid:107)2 + (cid:107)a1(cid:107)2 − 2(cid:104)z − w, a1(cid:105).

Since rnat

(F,Z )(z)2 = (cid:107)z − w(cid:107)2 and (cid:104)z − w, a1(cid:105) ≤ 0, we have rnat

(cid:104)

(F,Z )(z)2 ≤ (cid:107)F(z)(cid:107)2 − (cid:107)a1(cid:107)2.
(cid:105)
− z(cid:107)

2

JZ (z)

z − F(z)

According to Lemma 7, rtan(z)2 = (cid:107)Π

, where JZ (z) := z + TZ (z) and
TZ (z) = {z(cid:48) ∈ Rn : (cid:104)z(cid:48), a(cid:105) ≤ 0, ∀a ∈ CZ (z)} is the tangent cone of z. Since JZ(z) is
= 0, and
a cone with origin z, we have
(cid:13)
(cid:105)
(cid:13)
JZ (z)
(cid:13)
(cid:13)
(cid:13)z − F(z) − Π
(cid:13)

JZ (z)
(cid:13)
(cid:13)z − F(z) − Π
(cid:13)

, z − Π
JZ (z)
(cid:105)(cid:13)
2
(cid:13)
(cid:13)

. As Z ⊆ JZ (z), (cid:107)a1(cid:107)2 ≥

= (cid:107)F(z)(cid:107)2 −
2

z − F(z)
(cid:104)

, which implies that

z − F(z) − Π

z − F(z)

z − F(z)

z − F(z)

z − F(z)

− z
(cid:104)

(cid:105)(cid:13)
(cid:13)
(cid:13)

JZ (z)

JZ (z)

(cid:13)
(cid:13)
(cid:13)

(cid:105)(cid:69)

Π

(cid:68)

(cid:104)

(cid:104)

(cid:105)

(cid:104)

2

rnat
(F,Z )(z)2 ≤

(cid:104)

(cid:13)
(cid:13)
(cid:13)

Π

JZ (z)

z − F(z)

(cid:105)

− z

2

(cid:13)
(cid:13)
(cid:13)

= rtan(z)2.

Due to the above lemma, an upper bound of the tangent residual is also an upper bound of
the natural residual. We show in Theorem 2 the monotonicity of the tangent residual of the EG
updates, which is the technical core of our analysis and implies the O( 1√
) convergence rate of the
T

10

Figure 2: Illustration of the tangent residual and the natural residual. The blue line represents
the tangent residual and the red line represents the natural residual. It is clear that the tangent
residual upper bounds the natural residual.

tangent residual. As a result, we also show that the natural residual has a O( 1√
) convergence rate.
T
One may be tempted to directly use the natural residual as the convergence measure. However,
from our numerical experiments, the natural residual of the EG updates is not monotone, and we
believe that it is very challenging to directly establish the convergence rate for the natural residue
without using the tangent residual as a proxy.

In the next lemma, we argue why a small tangent residual implies a small gap function, hence

an approximate solution of the variational inequality. The proof is postponed to Appendix A.

Lemma 2. [Adapted from the proof of Theorem 10 in [Golowich et al., 2020b].] Given a closed convex set
Z ∈ Rn, an operator F : Z → Rn and z ∈ Z, we have

GAPZ,F,D(z) := max

z(cid:48)∈Z ∩B(z,D)

(cid:10)F(z), z − z(cid:48)(cid:11) ≤ D · rtan

(F,Z )(z).

If we have a convex-concave function f (z) : Z → R such that z = (x, y), Z = X × Y where X and
, then the duality gap at z with respect to X (cid:48) and

(cid:19)

Y are closed convex sets, let F(x, y) =
Y (cid:48) is dgX (cid:48),Y (cid:48)
diameters of X (cid:48) and Y (cid:48) are both upper bounded by D.10

(z) := maxy(cid:48)∈Y (cid:48) f (x, y(cid:48)) − minx(cid:48)∈X (cid:48) f (x(cid:48), y) ≤ D

f

(cid:18) ∇x f (x, y)
−∇y f (x, y)

√

2 · rtan

(F,Z )(z), if z ∈ X (cid:48) × Y (cid:48) and the

4 Best-Iterate Convergence of EG with Constant Step Size

En route to establish the last-iterate convergence of the EG algorithm, we ﬁrst show a weaker
guarantee known as the best-iterate convergence. Lemma 3 implies that after running EG for
T steps, there exists an iteration t∗ ∈ [T] where (cid:107)zt∗ − zt∗+ 1
T ). The proof can be found
in [Korpelevich, 1976] and [Facchinei and Pang, 2007] (included in Appendix B for completeness).

(cid:107)2 ≤ O( 1

2

10When X and Y are bounded, we choose X (cid:48) = X and Y (cid:48) = Y, otherwise the convention is to choose X (cid:48) and Y (cid:48) to

be X ∩ B(x∗, 2(cid:107)x0 − x∗(cid:107)) and Y ∩ B(y∗, 2(cid:107)y0 − y∗(cid:107)) respectively, where (x∗, y∗) is a saddle point.

11

!+#!(!)!!−#(!)&Lemma 3 ([Korpelevich, 1976, Facchinei and Pang, 2007]). Let Z be a closed convex set in Rn, F(·) be
a monotone and L-Lipschitz operator mapping from Z to Rn. For any solution z∗ of the monotone VI, that
is, (cid:104)F(z∗), z∗ − z(cid:105) ≤ 0 for all z ∈ Z. For all k,

(cid:107)zk − z∗(cid:107)2 ≥ (cid:107)zk+1 − z∗(cid:107)2 + (1 − η2L2)(cid:107)zk − zk+ 1

2

(cid:107)2.

(8)

In Lemma 4, we relate (cid:107)zk − zk+ 1

(cid:107) with the tangent residual at zk+1, and derive the best-iterate
convergence guarantee in terms of the tangent residual in Lemma 5. The proofs of Lemma 4 and 5
are postponed to Appendix B.
Lemma 4. For all k, rtan(zk+1) ≤ (cid:0)1 + ηL + (ηL)2(cid:1) ||zk−zk+1/2||

.

2

η

In lemma 5, we argue the tangent residual has a best-iterate convergence with rate O( 1√
T

).

Lemma 5. Let Z be a closed convex set in Rn, F(·) be a monotone and L-Lipschitz operator mapping from
Z to Rn. Suppose the step size of the EG algorithm η ∈ (0, 1
L ), then for any solution z∗ of the monotone VI
and any integer T > 0, there exists t∗ ∈ [T] such that:

(cid:13)
(cid:13)
(cid:13)zt∗ − zt∗+ 1

2

(cid:13)
(cid:13)
(cid:13)

2

≤

1
T

(cid:107)z0 − z∗(cid:107)2
1 − (ηL)2 ,

AND

rtan(zt∗+1) ≤

1 + ηL + (ηL)2
η

1
√
T

(cid:107)z0 − z∗(cid:107)
(cid:112)
1 − (ηL)2

.

5 Last-Iterate Convergence of EG with Constant Step Size

In this section, we show that the last-iterate convergence rate is O( 1√
). In particular, we prove
T
that the tangent residual is non-increasing, which, in combination with Lemma 5, implies the last-
iterate convergence rate of EG. To establish the monotonicity of the tangent residual, we combine
SOS programming with the low-dimensionality of the EG update rule. To better illustrate our
approach, we ﬁrst prove the result in the unconstrained setting (Section 5.1), then show how to
generalize it to the constrained setting (Section 5.2).

5.1 Warm Up: Unconstrained Case

As a warm-up, we consider the unconstrained setting where Z = Rn. Although the last-iterate
convergence rate is known in the unconstrained setting due to [Golowich et al., 2020b, Gorbunov
et al., 2021], we provide a simpler proof that also permits a larger step size. Our analysis holds for
any step size η ∈ (0, 1

L ), while the previous analysis requires η ≤ 1√
2L

[Gorbunov et al., 2021].

Let zk be the k-th iterate of the EG method. In Theorem 1, we show that the tangent residual is
monotone in the unconstrained setting.11 Our approach is to apply SOS programming to search
for a certiﬁcate of non-negativity for (cid:107)F(zk)(cid:107)2 − (cid:107)F(zk+1)(cid:107)2 for every k, over the semialgebraic set

11In the unconstrained setting, the tangent residual is simply the norm of the operator rtan

(F,Rn)(z) = (cid:107)F(z)(cid:107).

12

deﬁned by the following polynomial constraints in variables {zi[(cid:96)], ηF(zi)[(cid:96)]}i∈{k,k+ 1

2 ,k+1},(cid:96)∈[n]:

[(cid:96)] − zk[(cid:96)] + ηF(zk)[(cid:96)] = 0,

2

zk+ 1
(cid:13)ηF(zi) − ηF(zj)(cid:13)
(cid:13)
(cid:13)

2 − (ηL)2(cid:13)

(cid:13)zi − zj

zk+1[(cid:96)] − zk[(cid:96)] + ηF(zk+ 1
(cid:13)
(cid:13)

∀i, j ∈ {k, k +

2 ≤ 0,

2

)[(cid:96)] = 0, ∀(cid:96) ∈ [n],

(EG Update)

1
2

, k + 1},

(Lipschitzness)

(cid:10)ηF(zi) − ηF(zj), zj − zi

(cid:11) ≤ 0,

∀i, j ∈ {k, k +

1
2

, k + 1}.

(Monotonicity)

We always multiply F with η in the constraints as it will be convenient later. We use K to denote
the set {k, k + 1
2 , k + 1}. To obtain a certiﬁcate of non-negativity, we apply SOS programming
to search for a degree-2 SOS proof. More speciﬁcally, we want to ﬁnd non-negative coefﬁcients
((cid:96))
2 (w) in R[w] for each (cid:96) ∈ [n], where
{λ∗
w := {zi[(cid:96)], ηF(zi)[(cid:96)]}i∈K,(cid:96)∈[n], such that the following is an SOS polynomial:

i,j}i>j,i,j∈K and degree-1 polynomials γ

((cid:96))
1 (w) and γ

i,j, µ∗

(cid:107)ηF(zk)(cid:107)2 − (cid:107)ηF(zk+1)(cid:107)2 + ∑

i>j and i,j∈K

λ∗
i,j ·

(cid:16)(cid:13)
(cid:13)ηF(zi) − ηF(zj)(cid:13)
(cid:13)

2 − (ηL)2(cid:13)

(cid:13)zi − zj

(cid:13)
(cid:13)

2(cid:17)

+ ∑

i>j and i,j∈K

i,j · (cid:10)ηF(zi) − ηF(zj)), zj − zi
µ∗

(cid:11) + ∑
(cid:96)∈[n]

((cid:96))
1 (w)(zk+ 1
γ

2

[(cid:96)] − zk[(cid:96)] + ηF(zk)[(cid:96)])

+ ∑
(cid:96)∈[n]

((cid:96))
2 (w)(zk+1[(cid:96)] − zk[(cid:96)] + ηF(zk+ 1
γ

2

)[(cid:96)]).

(9)

Due to constraints satisﬁed by the EG iterates, the non-negativity of Expression (9) clearly im-
plies that (cid:107)F(zk)(cid:107)2 − (cid:107)F(zk+1)(cid:107)2 is non-negative. However, Expression (9) is in fact an inﬁnite
family of polynomials rather than a single one. Expression (9) corresponds to a different polyno-
mial for every integer n. To directly search for the solution, we would need to solve an inﬁnitely
large SOS program, which is clearly infeasible. By exploring the symmetry in Expression (9), we
show that it sufﬁces to solve a constant size SOS program. Let us ﬁrst expand Expression (9) as
follows:
∑
(cid:96)∈[n]

(ηF(zk)[(cid:96)])2 − (ηF(zk+1)[(cid:96)])2 + ∑

(cid:16)(cid:0)ηF(zi)[(cid:96)] − ηF(zj)[(cid:96)](cid:1)2 − (ηL)2(cid:0)zi[(cid:96)] − zj[(cid:96)](cid:1)2(cid:17)

i>j and i,j∈K

λ∗
i,j

(cid:16)

+ ∑

i>j and i,j∈K

µ∗
i,j

(cid:0)ηF(zi)[(cid:96)] − ηF(zj)[(cid:96)])(cid:1)(cid:0)zj[(cid:96)] − zi[(cid:96)](cid:1) + γ

((cid:96))
1 (w)(zk+ 1

2

+ γ

((cid:96))
2 (w)(zk+1[(cid:96)] − zk[(cid:96)] + ηF(zk+ 1

2

)[(cid:96)])

(cid:17)

.

[(cid:96)] − zk[(cid:96)] + ηF(zk)[(cid:96)])

(10)

What we will argue next is that, due to the symmetry across coordinates, it sufﬁces to directly
search for a single SOS proof that shows that each of the n summands in Expression (10) is an
SOS polynomial. More speciﬁcally, we make use of the following two key properties. (i) For any
(cid:96), (cid:96)(cid:48) ∈ [n], the (cid:96)-th summand and (cid:96)(cid:48)-th summand are identical subject to a change of variable;12 (ii)

12Simply replace {zi[(cid:96)]}i∈K and {ηF(zi)[(cid:96)]}i∈K with {zi[(cid:96)(cid:48)]}i∈K and {ηF(zi)[(cid:96)(cid:48)]}i∈K.

13

the (cid:96)-th summand only depends on the coordinate (cid:96), i.e., variables in {zi[(cid:96)], ηF(zi)[(cid:96)]}i∈K and does
not involve any other coordinates.13 We solve the following SOS program, whose solution can be
((cid:96))
used to construct {λ∗
2 (w)}(cid:96)∈[n] so that each of the summands in
Expression (10) is an SOS polynomial.

i,j}i>j,i,j∈K and {γ

((cid:96))
1 (w), γ

i,j, µ∗

Input Fixed Polynomials. We use x to denote (x0, x1, x2) and y to denote (y0, y1, y2). Interpret xi as zk+ i
and yi as ηF(zk+ i
coordinate (cid:96). gL
monotonicity constraints.

[(cid:96)]
)[(cid:96)] for 0 ≤ i ≤ 2. Observe that h1(x, y) and h2(x, y) come from the EG update rule on
i,j(x, y) come from the (cid:96)-th coordinate’s contribution in the Lipschitzness and

i,j(x, y) and gm

2

2

• h1(x, y) := x1 − x0 + y0 and h2(x, y) := x2 − x0 + y1.
• gL
• gm

i,j(x, y) := (yi − yj)2 − C · (xi − xj)2 for any 0 ≤ j < i ≤ 2.a
i,j(x, y) := (yi − yj)(xj − xi) for any 0 ≤ j < i ≤ 2.

Decision Variables of the SOS Program:

i,j ≥ 0, and pm

• pL
i,j ≥ 0, for all 0 ≤ j < i ≤ 2.
• q1(x, y) and q2(x, y) are two degree 1 polynomials in R[x, y].

Constraints of the SOS Program:

s.t.

0 − y2
y2

2 + ∑

2≥i>j≥0

i,j · gL
pL

i,j(x, y) + ∑

2≥i>j≥0

i,j · gm
pm

i,j(x, y)

(11)

+q1(x, y) · h1(x, y) + q2(x, y) · h2(x, y) ∈ SOS[x, y].

aC represents (ηL)2. Larger C corresponds to a larger step size and makes the SOS program harder to satisfy.
Through binary search, we ﬁnd that the largest possible value of C is 1 while maintaining the feasibility of the SOS
program.

Figure 3: Our SOS program in the unconstrained setting.

The proof of the following theorem is based on a feasible solution to the SOS program in

Figure 3.

Theorem 1. Let F : Rn → Rn be a monotone and L-Lipschitz operator. Then for any k ∈ N, the EG
algorithm with step size η ∈ (0, 1

L ) satisﬁes (cid:107)F(zk)(cid:107)2 ≥ (cid:107)F(zk+1)(cid:107)2.

Proof. Since F is monotone and L-Lipschitz, we have

and

(cid:104)F(zk+1) − F(zk), zk − zk+1(cid:105) ≤ 0

(cid:13)
(cid:13)
(cid:13)F(zk+ 1

2

) − F(zk+1)

2

(cid:13)
(cid:13)
(cid:13)

− L2(cid:13)
(cid:13)
(cid:13)zk+ 1

2

− zk+1

2

(cid:13)
(cid:13)
(cid:13)

≤ 0.

13We mainly care about the polynomials arise from the constraints. Although γ

other coordinates, we show that it sufﬁces to consider polynomials in {zi[(cid:96)], ηF(zi)[(cid:96)]}i∈K.

((cid:96))
1 (w) and γ

((cid:96))
2 (w) could depend on

14

We simplify them using the update rule of EG and ηL < 1. In particular, we replace zk − zk+1 with
ηF(zk+ 1

− zk+1 with ηF(zk+ 1

) and zk+ 1

) − ηF(zk).

2

2

2

(cid:68)

F(zk+1) − F(zk), F(zk+ 1

2

(cid:69)

)

≤ 0,

(cid:13)
(cid:13)
(cid:13)F(zk+ 1

2

) − F(zk+1)

2

(cid:13)
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)
(cid:13)F(zk+ 1

2

) − F(zk)

2

(cid:13)
(cid:13)
(cid:13)

≤ 0.

(12)

(13)

In Proposition 1 at Appendix C we verify the following identity.

(cid:107)F(zk)(cid:107)2 − (cid:107)F(zk+1)(cid:107)2 + 2 · LHS of Inequality(12) + LHS of Inequality(13) = 0.

Thus, (cid:107)F(zk)(cid:107)2 − (cid:107)F(zk+1)(cid:107)2 ≥ 0.

Corollary 1 is implied by combing Lemma 2, Lemma 5, Theorem 1 and the fact that η ∈ (0, 1

L ).
Corollary 1. Let F(·) : Rn → Rn be a monotone and L-Lipshitz operator and z∗ ∈ Rn be a solution to
the variational inequality. For any T ≥ 1, let zT be T-th iterate of the EG algorithm with constant step size
η ∈ (0, 1

L ), then GAPRn,F,D(zT) ≤ 1√
T

3D(cid:107)z0−z∗(cid:107)
√
1−(ηL)2
η

.

5.2 Last-Iterate Convergence of EG with Arbitrary Convex Constraints

We establish the last-iterate convergence rate of the EG algorithm in the constrained setting in
this section. The plan is similar to the one in Section 5.1. First, we use the assistance of SOS
programming to prove the monotonicity of the tangent residual (Theorem 2), then combine it
with the best-iterate convergence guarantee from Lemma 5 to derive the last-iterate convergence
rate (Theorem 3).

Due to the constraints, proving the monotonicity of the tangent residual becomes much more
challenging. The tangent residual in the constrained setting ( Deﬁnition 3) is signiﬁcantly more
complex than its counterpart in the unconstrained setting. In Lemma 6, we introduce an auxiliary
point c(z) for every point z that can be used to simpliﬁed the tangent residual.

Lemma 6. Let Z ⊆ Rn be a closed convex set and F : Z → R be an operator. For any z ∈ Z, denote
c(z) := Π

the projection of −F(z) on the normal cone N(z). Then we have

− F(z)

(cid:104)

(cid:105)

N(z)

• rtan(z) = (cid:107)F(z) + c(z)(cid:107),

• (cid:104)F(z) + c(z), c(z)(cid:105) = 0,

• (cid:104)F(z) + c(z), a(cid:105) ≥ 0, ∀a ∈ N(z).

Proof. According to the deﬁnition of c(z), rtan(z) = (cid:107)F(z) + c(z)(cid:107) follows from Lemma 7. Since
c(z) = Π

, we know that for all a ∈ N(z),

− F(z)

(cid:105)

(cid:104)

N(z)

(cid:104)−F(z) − c(z), a − c(z)(cid:105) ≤ 0.

(14)

15

Note that c(z) ∈ N(z) and N(z) is a cone. By substituting a = 0 and a = 2 · c(z) in (14), we get

Therefore, for all a ∈ N(z), we have

(cid:104)−F(z) − c(z), c(z)(cid:105) = 0.

(cid:104)−F(z) − c(z), a(cid:105) = (cid:104)−F(z) − c(z), a − c(z)(cid:105) ≤ 0.

Next, we need to decide over which semialgebraic set that we want to certify the non-
negativity of rtan(zk)2 − rtan(zk+1)2. Naturally, we would like to use all constraints of Z, but there
might be arbitrarily many of them. In the next paragraph, we argue how to reduce the number of
constraints.

2

2

(cid:105)

(cid:104)

N(zk+1)

− F(zk+1)

Reducing the Number of Constraints. Suppose we are not given the description of Z ⊆ Rn,
and we only observe one iteration of the EG algorithm. In other words, we know zk, zk+ 1
, and
zk+1, as well as F(zk), F(zk+ 1
), and F(zk+1). To express the squared tangent residual at zk and
the squared tangent residual at zk+1, let us also assume that the vector ck = Π
and
, and according to Lemma 6, we have rtan(zk)2 = (cid:107)F(zk) + ck(cid:107)2, and
ck+1 = Π
rtan(zk+1)2 = (cid:107)F(zk+1) + ck+1(cid:107)2. Our plan is to derive a set of inequalities that must be satisﬁed
by these vectors. From this limited information, what can we learn about Z? We can conclude
that Z must lie in the intersection of the following halfspaces: (a) (cid:104)ck, z(cid:105) ≤ (cid:104)ck, zk(cid:105). This is true
because ck ∈ N(zk). (b) (cid:104)ak+ 1
). This is true
). (c) (cid:104)ak+1, z(cid:105) ≥ (cid:104)ak+1, zk+1(cid:105), where ak+1 =
because zk+ 1
−(zk − ηF(zk+ 1
)), so −ak+1 ∈ N(zk+1).
See Figure 4 for illustration. Additionally, due to our deﬁnition of ck and ck+1 and Lemma 6, we
know that (d) (cid:104)ηF(zi) + ηci, ηci(cid:105) = 0 for i ∈ {k, k + 1}, and (e) (cid:104)ηF(zk+1) + ηck+1, ak+1(cid:105) ≤ 0 as
−ak+1 ∈ N(zk+1).

) − zk+1). This is true because zk+1 = ΠZ (zk − ηF(zk+ 1

= ΠZ (zk − ηF(zk)), so −ak+ 1

= −(zk − ηF(zk) − zk+ 1

(cid:105), where ak+ 1

, z(cid:105) ≥ (cid:104)ak+ 1

∈ N(zk+ 1

− F(zk)

, zk+ 1

N(zk)

(cid:104)

(cid:105)

2

2

2

2

2

2

2

2

2

2

Clearly, for any Z, the inequalities in (a) to (e) must hold, though there might be other inequal-
ities that are also true. Our goal is to prove that the tangent residual is non-increasing even if only
inequalities (a) to (e) hold. If we can do so, then we prove that tangent residual is non-increasing
for an arbitrary Z.

Formulation as SOS program. Similar to the unconstrained case, our plan is to search for a
certiﬁcate of non-negativity of the following expression

(cid:107)F(zk) + ck(cid:107)2 − (cid:107)F(zk+1) + ck+1(cid:107)2

(15)

16

Figure 4: Reducing the number of constraints.

over the semialgebraic set deﬁned by the following polynomial constraints in variables
(cid:110)

(cid:111)

{zi[(cid:96)], ηF(zi)[(cid:96)]}i∈{k,k+ 1

2 ,k+1} ∪ {ci[(cid:96)]}i∈k,k+1

(cid:13)ηF(zi) − ηF(zj)(cid:13)
(cid:13)
(cid:13)

2 − (ηL)2(cid:13)

(cid:13)zi − zj

(cid:13)
(cid:13)

2 ≤ 0,

(cid:10)ηF(zi) − ηF(zj), zj − zi

(cid:11) ≤ 0,

(cid:10)ai, zi − zj

(cid:11) ≤ 0,

(cid:11) ≤ 0,

(cid:10)ηci, zj − zi
(cid:104)ηF(zi) + ηci, ηci(cid:105) = 0,
(cid:104)ηF(zk+1) + ηck+1, ak+1(cid:105) ≤ 0,

(cid:96)∈[n]

∀i ∈ {k +

1
2

1
2
1
2
1
2

∀i, j ∈ {k, k +

∀i, j ∈ {k, k +

, k + 1},

, k + 1},

(Lipschitzness)

(Monotonicity)

, k + 1}, j ∈ {k, k +

∀i ∈ {k, k + 1}, j ∈ {k, k +

, k + 1},

(−ai ∈ N(zi))

1
2

, k + 1},

(ci ∈ N(zi))

∀i ∈ {k, k + 1},

(Lemma 6)
(Lemma 6).

Similar to Section 5, we multiply the operators, ck, and ck+1 with η for convenience. Fortunately,
the dimensional-dependent Expression (15) and semialgebraic set are symmetric across coordi-
nates, and more speciﬁcally, satisfy the two key properties in the unconstrained case – Property (i)
and (ii). Hence, we can represent all of the coordinates (cid:96) ≥ 1 with one coordinate in the SOS pro-
gram, and we can form a constant size SOS program to search for a certiﬁcate of non-negativity
for Expression (15) as shown in Figure 5.

In Theorem 2, we establish the monotonicity of the tangent residual. Our proof is based on the

solution to the degree-2 SOS program concerning polynomials in 8 variables (Figure 5).

Theorem 2. Let Z ⊆ Rn be a closed convex set and F : Z → Rn be a monotone and L-Lipschitz operator.
For any step size η ∈ (0, 1
L ) and any zk ∈ Z, the EG method update satisﬁes rtan

(F,Z )(zk) ≥ rtan

(F,Z )(zk+1).

17

(cid:104)ck,z(cid:105)=(cid:104)ck,zk(cid:105)(cid:68)ak+12,z(cid:69)=(cid:68)ak+12,zk+12(cid:69)−ak+12ck−ak+1(cid:104)ak+1,z(cid:105)=(cid:104)ak+1,zk+1(cid:105)zkzk−ηF(zk)zk+1zk−ηF(cid:16)zk+12(cid:17)zk+12Zzk+1−ηF(zk+1)N(zk+1)zk+1−ηck+12

Input Fixed Polynomials. We use x to denote (x0, x1, x2), y to denote (y0, y1, y2) and w to denote (w0, w2).
)[(cid:96)] for 0 ≤ i ≤ 2, w0 as ηck[(cid:96)] and w2 as ηck+1[(cid:96)]. Let b1 =
[(cid:96)] and yi as ηF(zk+ i
Interpret xi as zk+ i
−(x0 − y0 − x1) and b2 = −(x0 − y1 − x2).
i,j(x, y, w) and gm
Origin of Constraints. gL
Lipschitzness and monotonicity constraints. Similarly, gb
ordinate contribution of fact that −ai and ci are in the normal cone of zi. Finally, hw
comes from the (cid:96)-th coordinate contribution due to the inequalities of Lemma 6.

i,j(x, y, w) come from the (cid:96)-th coordinate’s contribution in the
i,j(x, y, w) come from the (cid:96)-th co-
i (x, y, w) and gr(x, y, w)

i,j(x, y, w) and gw

2

• gL
• gm

i,j(x, y, w) := (yi − yj)2 − C · (xi − xj)2 for any 0 ≤ j < i ≤ 2.a
i,j(x, y, w) := (yi − yj)(xj − xi) for any 0 ≤ j < i ≤ 2.
i,j(x, y, w) := bi · (xi − xj) for any i ∈ {1, 2}, 0 ≤ j ≤ 2.
i,j(x, y, w) := wi · (xj − xi) for any i ∈ {0, 2}, 0 ≤ j ≤ 2.

• gb
• gw
• gr(x, y, w) := (y2 + w2) · b2.
• hw

i (x, y, w) := (yi + wi) · wi for any i ∈ {0, 2}.

Decision Variables of the SOS Program:

i,j ≥ 0, for all 0 ≤ j < i ≤ 2.

i,j ≥ 0, and pm
i,j ≥ 0, for any i ∈ {1, 2}, 0 ≤ j ≤ 2.
i,j ≥ 0, for any i ∈ {0, 2}, 0 ≤ j ≤ 2.

• pL

• pb
• pw
• pr ≥ 0.
• qw
0 , qw

2 ∈ R.

Constraints of the SOS Program:

s.t.

(y0 + w0)2 − (y2 + w2)2 + ∑

i,j · gL
pL

i,j(x, y, w) + ∑

i,j · gm
pm

i,j(x, y, w)

+ ∑i∈{1,2},2≥j≥0 pb
+pr · gr(x, y, w) + ∑i∈{0,2} qw
i

i,j · gb

2≥i>j≥0
i,j(x, y, w) + ∑i∈{0,2},2≥j≥0 pw
i (x, y, w)

· hw

2≥i>j≥0
i,j · gw

i,j(x, y, w)

∈ SOS[x, y, w]

(16)

aC represents (ηL)2.

Figure 5: Our SOS program in the constrained setting.

Proof. Let ck = Π

NZ (zk)(−F(zk)) and ck+1 = Π

NZ (zk+1)(−F(zk+1)). By Lemma 6 we have

η2rtan(zk)2 − η2rtan(zk+1)2 = (cid:107)ηF(zk) + ηck(cid:107)2 − (cid:107)ηF(zk+1) + ηck+1(cid:107)2

Combining the monotonicity and L-Lipschitzness of F with the fact that L ≤ 1

η , we have

(−1) ·

(cid:18)(cid:13)
(cid:13)
(cid:13)zk+ 1

2

− zk+1

2

(cid:13)
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)
(cid:13)ηF(zk+ 1

2

) − ηF(zk+1)

2(cid:19)

(cid:13)
(cid:13)
(cid:13)

≤ 0,

(−2) · (cid:104)ηF(zk+1) − ηF(zk), zk+1 − zk)(cid:105) ≤ 0, .

18

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

(26)

Since zk+ 1
ηF(zk) − zk+ 1

2

2

= ΠZ (zk − ηF(zk)) and zk+1 = ΠZ
∈ N(zk+ 1

zk − ηF(zk+ 1
) − zk+1 ∈ N(zk+1), which further implies

)

2

, we can infer that zk −

(cid:16)

(cid:17)

2

) and zk − ηF(zk+ 1
(cid:68)

2

(−2) ·

(cid:68)

(cid:68)

(−2) ·

(−2) ·

zk − ηF(zk) − zk+ 1

, zk+ 1

2

2

zk − ηF(zk+ 1

2

) − zk+1, zk+1 − zk
(cid:69)

ηck, zk − zk+ 1

2

≤ 0, .

(cid:69)

− zk+1
(cid:69)

≤ 0,

≤ 0,

Since zk − ηF(zk+ 1

2

) − zk+1 ∈ N(zk+1) and ck+1 = Π

NZ (zk+1)(−F(zk+1)), by Lemma 6 we have

(cid:68)

(−2) ·

ηck+1 + ηF(zk+1), zk − ηF(zk+ 1
(−2) · (cid:104)ηck+1 + ηF(zk+1), −ηck+1(cid:105) = 0.

2

) − zk+1

(cid:69)

≤ 0,

MATLAB code for the veriﬁcation of the following identity can be found at this link.

Expression (17) + LHS of Inequality (18) + LHS of Inequality (19)

+LHS of Inequality (20) + LHS of Inequality (21) + LHS of Inequality (22)
+LHS of Inequality (23) + LHS of Inequality (24)
=(cid:107)ηF(zk) + ηck − zk + zk+ 1
+(cid:107)ηF(zk+ 1

) + ηck+1 − zk + zk+1(cid:107)2 ≥ 0,

(cid:107)2

2

2

which concludes the proof.

Theorem 3. Let Z ⊆ Rn be a closed convex set, F(·) : Z → Rn be a monotone and L-Lipschitz operator
and z∗ ∈ Z be the solution to the variational inequality. Then for any T ≥ 1, zT produced by EG with any
constant step size η ∈ (0, 1

L ) satisﬁes

• GAP(zT) ≤ 1√
T

3D||z0−z∗||
√
1−(ηL)2
η

,

• rnat(zT) ≤ rtan(zT) ≤ 1√
T

3||z0−z∗||
√
1−(ηL)2

.

η

L ). Choosing η to be 1

2L and D = O((cid:107)z0 − z∗(cid:107)), then GAP(zT) = O( D2 L√

Theorem 3 is implied by combing Lemma 1, Lemma 2, Lemma 5, Theorem 2 and the fact
) matching

that η ∈ (0, 1
the Ω( D2 L√
) lower bound for EG, OGDA, and more generally all p-SCLI algorithms [Golowich
T
et al., 2020b,a] in terms of the dependence on D, L, and T. Additionally, rnat
),
and rtan
), so our upper bounds for both the natural residual and tangent resid-
ual also match the Ω( DL√
) lower bounds with respect to natural residual and tangent residual
T
for EG [Golowich et al., 2020b]. This is because both the natural residual and tangent residual
are equivalent to the norm of the operator, and [Golowich et al., 2020b] shows that in the uncon-
strained setting (cid:107)F(zT)(cid:107) = Ω( DL√
T

Z,F,D(zT) = O( DL√

Z,F,D(zT) = O( DL√

).

T

T

T

19

6 Last-Iterate Convergence of OGDA with Constant Step Size

In this section, we show that the OGDA algorithm with constant step size η ∈ (0, 1
)
last-iterate convergence rate with respect to the tangent residual or the gap function. The analysis
of the OGDA algorithm follows the same steps as in the analysis of the EG algorithm. Compared
to the EG algorithm, the last iterate convergence of OGDA follows by builds on the monotonicity
and best-iterate convergence of the following potential function

2L ) has O( 1√

T

Φ

k = (cid:107)F(zk) − F(wk)(cid:107)2 + rtan(zk)2.
(27)
The potential function can be thought of as the tangent residual (rtan(zk)2) and an extra correc-
tion term (cid:107)F(zk) − F(wk)(cid:107)2. The potential function is discovered directly through SOS program-
ming. The SOS program was formulated by searching over linear combinations of ||F(zk)||2 −
||F(zk+1)||2,||F(wk)||2 − ||F(wk+1)||2,(cid:104)F(zk), F(wk)(cid:105) − (cid:104)F(zk+1), F(wk+1)(cid:105) and rtan(zk) − rtan(zk+1),
under (i) the constraint that the linear combination is non-increasing,14 and (ii) the constraints in-
duced by properties of the operator F(·), the update rule of OGDA and the set Z (See Figure 5 for
a demonstration of the induced constraints of EG algorithm for solving a monotone VI over con-
vex constraints). We then use the linear combination output by the SOS program as the potential
function in our analysis. We believe our heuristic for ﬁnding a potential function could be useful
in other settings. In general, one can ﬁrst choose a collection of basis functions that may be part
of a potential function, then use SOS programming to search over all linear combinations of the
basis functions subject to the constraint that the linear combination is non-negative to discover the
potential function. We postpone all technical details to Appendix E.

6.1 Warm Up: Unconstrained Case

k is non-
) last-iterate convergence rate for OGDA with respect to the

In this section, we show that in the unconstrained setting the potential function Φ
increasing, which implies the O( 1√
T
tangent residual, the natural residual, and the gap function.
Theorem 4. Let F : Rn → Rn be a monotone and L-Lipschitz operator. Then for any k ∈ N, the OGDA
2L ) satisﬁes (cid:107)F(zk) − F(wk)(cid:107)2 + (cid:107)F(zk)(cid:107)2 ≥ (cid:107)F(zk+1) − F(wk+1)(cid:107)2 +
algorithm with step size η ∈ (0, 1
(cid:107)F(zk+1)(cid:107)2.
Proof. Since F is monotone and L-Lipschitz, we have (cid:104)F(zk+1) − F(zk), zk − zk+1(cid:105) ≤ 0 and
(cid:107)F(wk+1) − F(zk+1)(cid:107)2 − L2(cid:107)wk+1 − zk+1(cid:107)2 ≤ 0. We simplify them using the update rule of
OGDA and η2L2 < 1
In particular, we replace zk − zk+1 by ηF(wk+1) and wk+1 − zk+1 with
4 .
ηF(wk+1) − ηF(wk).

(cid:104)F(zk+1) − F(zk), F(wk+1)(cid:105) ≤ 0,
(cid:107)F(wk+1) − F(zk+1)(cid:107)2 −

1
4

(cid:107)F(wk+1) − F(wk)(cid:107)2 ≤ 0.

(28)

(29)

14To avoid ﬁnding the trivial linear combination, i.e., all coefﬁcients equal to 0, we also use the objective function in
the SOS program to encourage a non-trivial solution if one exists by, for example, maximizing the sum of the coefﬁcients
of the linear combination.

20

MATLAB code for the veriﬁcation of the following identity can be found at this link.

(cid:107)F(zk) − F(wk)(cid:107)2 + (cid:107)F(zk)(cid:107)2 − (cid:107)F(zk+1) − F(wk+1)(cid:107)2 − (cid:107)F(zk+1)(cid:107)2
+ 2 · LHS of Inequality(28) + 2 · LHS of Inequality(29)

=

1
2

(cid:107)F(wk) + F(wk+1) − 2F(zk))(cid:107)2.

(30)

Thus, (cid:107)F(zk) − F(wk)(cid:107)2 + (cid:107)F(zk)(cid:107)2 ≥ (cid:107)F(zk+1) − F(wk+1)(cid:107)2 + (cid:107)F(zk+1)(cid:107)2.

The following theorem is a combination of Corollary 2, Theorem 4, Lemma 12, Lemma 1 and

Lemma 2.
Theorem 5. Let F : Rn → R be a monotone and L-Lipschitz operator. Let z0 = w0 ∈ Rn
be arbitrary starting point and {zk, wk}k≥0 be the iterates of the OGDA algorithm with any step
(4 + 6η4L4)(cid:107)z0 − z∗(cid:107)2 + (16η2L2 + 6η4L4)(cid:107)w0 − z0(cid:107)2 =
size η ∈ (0, 1
O(max{(cid:107)z0 − z∗(cid:107), (cid:107)w0 − z0(cid:107)}). Then for any T ≥ 1,

2L ). Denote D0

:=

(cid:113)

• GAPZ,F,D(zT) ≤ 1√
T

·

√

DD0
1−4(ηL)2)

.

η

• rnat

Z,F,D(zT) ≤ rtan

Z,F,D(zT) ≤ 1√
T

√

·

η

D0

1−4(ηL)2

.

• GAPZ,F,D(wT+1) ≤ 1√
T

·

√

2(2+ηL)·DD0
√
1−4(ηL)2
η

.

• rnat

Z,F,D(wT+1) ≤ rtan

Z,F,D(wT+1) ≤ 1√
T

√
2(2+ηL)·D0
√
1−4(ηL)2

η

.

·

6.2 Last-Iterate Convergence of OGDA with Arbitrary Convex Constraints

In this section, we formally state the last-iterate convergence of OGDA algorithm with respect to
the gap function, the natural residual and the tangent residual in the constrained setting. All the
details are postponed to Appendix E.

Theorem 6. Let Z ⊆ Rn be a closed convex set and F : Z → R be a monotone and L-Lipschitz oper-
ator. Let z0, w0 ∈ Z be arbitrary starting point and {zk, wk}k≥0 be the iterates of the OGDA algorithm
(4 + 6η4L4)(cid:107)z0 − z∗(cid:107)2 + (16η2L2 + 6η4L4)(cid:107)w0 − z0(cid:107)2 =
with any step size η ∈ (0, 1
O(max{(cid:107)z0 − z∗(cid:107), (cid:107)w0 − z0(cid:107)}). Then for any T ≥ 1,
√

2L ). Let D0 :=

DD0

(cid:113)

• GAPZ,F,D(zT) ≤

.

η·

T·(1−4·(ηL)2)

• rnat

Z,F,D(zT) ≤ rtan

Z,F,D(zT) ≤

√

η·

D0

T·(1−4·(ηL)2)

.

• GAPZ,F,D(wT+1) ≤

√
2(2+ηL)·D·D0
√
T·(1−4·(ηL)2)

η·

.

21

• rnat

Z,F,D(wT+1) ≤ rtan

Z,F,D(wT+1) ≤

√
√

2(2+ηL)D0
T·(1−4·(ηL)2)

.

η·

Setting D = max{(cid:107)z0 − z∗(cid:107), (cid:107)w0 − z0(cid:107)}, and η =

), which matches the lower bound of Ω( D2 L√
T

1
√
2L

2

, we have GAPZ,F,D(zT) (or
) by Golowich et al. [2020a].

GAPZ,F,D(wT)) = O( D2 L√
T

References

Yossi Arjevani and Ohad Shamir. On the iteration complexity of oblivious ﬁrst-order optimization

algorithms. In International Conference on Machine Learning, 2016.

Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein Generative Adversarial Net-

works. In Proceedings of the 34th International Conference on Machine Learning, July 2017.

Emil Artin. ¨Uber die zerlegung deﬁniter funktionen in quadrate. In Abhandlungen aus dem mathe-
matischen Seminar der Universit¨at Hamburg, volume 5, pages 100–115. Springer, 1927. Issue: 1.

Alfred Auslender and Marc Teboulle. Interior projection-like methods for monotone variational

inequalities. Mathematical programming, 104(1):39–68, 2005. Publisher: Springer.

Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust Optimization. Princeton

University Press, December 2009.

Ha¨ım Brezis and Mo¨ıse Sibony. M´ethodes d’approximation et d’it´eration pour les op´erateurs

monotones. Archive for Rational Mechanics and Analysis, 28(1):59–82, January 1968.

Felix E. Browder. Nonlinear monotone operators and convex sets in Banach spaces. Bulletin of the

American Mathematical Society, 71(5):780–785, 1965.

Yang Cai and Constantinos Daskalakis. On Minmax Theorems for Multiplayer Games. In Proceed-
ings of the 2011 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), January 2011.

Yang Cai, Ozan Candogan, Constantinos Daskalakis, and Christos Papadimitriou. Zero-Sum Poly-
matrix Games: A Generalization of Minmax. Mathematics of Operations Research, 41(2):648–655,
May 2016.

Yang Cai, Argyris Oikonomou, and Weiqiang Zheng. Tight last-iterate convergence of the extra-
gradient and the optimistic gradient descent-ascent algorithm for constrained monotone varia-
tional inequalities (version 2), 2022. URL https://arxiv.org/abs/2204.09228v2.

Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University

Press, 2006.

Tatjana Chavdarova, Gauthier Gidel, Franc¸ois Fleuret, and Simon Lacoste-Julien. Reducing noise
in GAN training with variance reduced extragradient. In Advances in Neural Information Process-
ing Systems, volume 32, 2019.

22

Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed:
Convergent reinforcement learning with nonlinear function approximation. In International Con-
ference on Machine Learning, 2018.

Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and
constrained min-max optimization. In 10th Innovations in Theoretical Computer Science Conference,
volume 124, pages 27:1–27:18, 2019.

Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with

optimism. In 6th International Conference on Learning Representations, 2018.

Jelena Diakonikolas. Halpern iteration for near-optimal and parameter-free monotone inclusion

and strong solutions to variational inequalities. In Conference on Learning Theory, 2020.

Radu-Alexandru Dragomir, Adrien B. Taylor, Alexandre d’Aspremont, and J´er ˆome Bolte. Optimal
complexity and certiﬁcation of bregman ﬁrst-order methods. Mathematical Programming, 2021.

Yoel Drori and Marc Teboulle. Performance of ﬁrst-order methods for smooth convex minimiza-

tion: a novel approach. Mathematical Programming, 145(1):451–482, 2014.

Simon S. Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduc-

tion methods for policy evaluation. In International Conference on Machine Learning, 2017.

Eyal Even-Dar, Yishay Mansour, and Uri Nadav. On the convergence of regret minimization dy-
In Proceedings of the forty-ﬁrst annual ACM symposium on Theory of

namics in concave games.
computing, pages 523–532, 2009.

Francisco Facchinei and Jong-Shi Pang. Finite-dimensional variational inequalities and complementar-

ity problems. Springer Science & Business Media, 2007.

Mahyar Fazlyab, Manfred Morari, and Victor M Preciado. Design of ﬁrst-order optimization algo-
rithms via sum-of-squares programming. In 2018 IEEE Conference on Decision and Control (CDC),
pages 4445–4452. IEEE, 2018.

Gauthier Gidel, Hugo Berard, Ga¨etan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A vari-
ational inequality perspective on generative adversarial networks. In 7th International Conference
on Learning Representations, 2019a.

Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, R´emi Le Priol, Gabriel Huang,
Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dy-
namics. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, 2019b.

Noah Golowich, Sarath Pattathil, and Constantinos Daskalakis. Tight last-iterate convergence
rates for no-regret learning in multi-player games. In Annual Conference on Neural Information
Processing Systems, 2020a.

23

Noah Golowich, Sarath Pattathil, Constantinos Daskalakis, and Asuman E. Ozdaglar. Last iterate
is slower than averaged iterate in smooth convex-concave saddle point problems. In Conference
on Learning Theory, 2020b.

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Annual Conference on
Neural Information Processing Systems, 2014.

Eduard Gorbunov, Nicolas Loizou, and Gauthier Gidel. Extragradient method: O(1/K) last-iterate
convergence for monotone variational inequalities and connections with cocoercivity. CoRR,
abs/2110.04261, 2021. URL https://arxiv.org/abs/2110.04261.

Baptiste Goujaud, C´eline Moucer, Franc¸ois Glineur, Julien M. Hendrickx, Adrien B. Taylor, and
Aymeric Dieuleveut. Pepit: computer-assisted worst-case analyses of ﬁrst-order optimization
methods in python. CoRR, abs/2201.04040, 2022. URL https://arxiv.org/abs/2201.04040.

Philip Hartman and Guido Stampacchia. On some non-linear elliptic differential-functional equa-

tions. Acta Mathematica, 115:271–310, January 1966.

Yu-Guan Hsieh, Franck Iutzeler, J´er ˆome Malick, and Panayotis Mertikopoulos. On the conver-
gence of single-call stochastic extra-gradient methods. In Annual Conference on Neural Informa-
tion Processing Systems, 2019.

Donghwan Kim. Accelerated proximal point method for maximally monotone operators. Mathe-

matical Programming, 190(1):57–87, November 2021.

G. M. Korpelevich. The extragradient method for ﬁnding saddle points and other problems. Mate-

con, 12:747–756, 1976.

Jean-Louis Krivine. Anneaux pr´eordonn´es. Journal d’analyse math´ematique, 12:p. 307–326, 1964.

Sucheol Lee and Donghwan Kim. Fast extra gradient methods for smooth structured nonconvex-
nonconcave minimax problems. In Annual Conference on Neural Information Processing Systems,
2021.

Qi Lei, Sai Ganesh Nagarajan, Ioannis Panageas, and Xiao Wang. Last iterate convergence in no-
regret learning: constrained min-max optimization for convex-concave landscapes. In The 24th
International Conference on Artiﬁcial Intelligence and Statistics, 2021.

Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and Design of Optimization Al-
gorithms via Integral Quadratic Constraints. SIAM Journal on Optimization, 26(1):57–95, January
2016.

Tengyuan Liang and James Stokes.

Interaction matters: A note on non-asymptotic local con-
vergence of generative adversarial networks. In The 22nd International Conference on Artiﬁcial
Intelligence and Statistics, 2019.

24

Tianyi Lin, Zhengyuan Zhou, Panayotis Mertikopoulos, and Michael I. Jordan. Finite-time last-
In Proceedings of the 37th International

iterate convergence for multi-agent learning in games.
Conference on Machine Learning, 2020.

Jacques-Louis Lions and Guido Stampacchia. Variational inequalities. Communications on pure and

applied mathematics, 20(3):493–519, 1967.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In 6th International Conference on
Learning Representations, 2018.

Yu Malitsky. Projected Reﬂected Gradient Methods for Monotone Variational Inequalities. SIAM

Journal on Optimization, 25(1):502–520, January 2015.

Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in adversar-
In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on

ial regularized learning.
Discrete Algorithms, pages 2703–2717, 2018.

Aryan Mokhtari, Asuman E. Ozdaglar, and Sarath Pattathil. A uniﬁed analysis of extra-gradient
and optimistic gradient methods for saddle point problems: Proximal point approach. In The
23rd International Conference on Artiﬁcial Intelligence and Statistics, 2020.

Renato DC Monteiro and Benar Fux Svaiter. On the Complexity of the Hybrid Proximal Extra-
gradient Method for the Iterates and the Ergodic Mean. SIAM Journal on Optimization, 20(6):
2755–2787, January 2010.

Arkadi Nemirovski. Prox-method with rate of convergence O (1/t) for variational inequalities
with Lipschitz continuous monotone operators and smooth convex-concave saddle point prob-
lems. SIAM Journal on Optimization, 15(1):229–251, 2004.

Wei Peng, Yu-Hong Dai, Hui Zhang, and Lizhi Cheng. Training GANs with centripetal accelera-

tion. Optimization Methods and Software, 35(5):955–973, 2020.

Leonid Denisovich Popov. A modiﬁcation of the Arrow-Hurwicz method for search of saddle

points. Mathematical notes of the Academy of Sciences of the USSR, 28(5):845–848, 1980.

Alexander Rakhlin and Karthik Sridharan. Online Learning with Predictable Sequences. In Con-

ference on Learning Theory, June 2013.

Ernest K. Ryu, Adrien B. Taylor, Carolina Bergeling, and Pontus Giselsson. Operator splitting per-
formance estimation: Tight contraction factors and optimal parameter selection. SIAM Journal
on Optimization, 30(3):2251–2271, 2020.

Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and

Trends® in Machine Learning, 4(2):107–194, 2012.

Mo¨ıse Sibony. M´ethodes it´eratives pour les ´equations et in´equations aux d´eriv´ees partielles non

lin´eaires de type monotone. CALCOLO, 7(1):65–183, March 1970.

25

Gilbert Stengle. A nullstellensatz and a positivstellensatz in semialgebraic geometry. Mathematis-

che Annalen, 207(2):87–97, June 1974.

Sandra SY Tan, Antonios Varvitsiotis, and Vincent YF Tan. Analysis of optimization algorithms

via sum-of-squares. Journal of Optimization Theory and Applications, 190(1):56–81, 2021.

Adrien B. Taylor, Julien M. Hendrickx, and Franc¸ois Glineur. Exact Worst-case Performance of
First-order Methods for Composite Convex Optimization. SIAM Journal on Optimization, 27(3):
1283–1313, January 2017a.

Adrien B. Taylor, Julien M. Hendrickx, and Franc¸ois Glineur. Performance estimation toolbox
(PESTO): automated worst-case analysis of ﬁrst-order optimization methods. In 2017 IEEE 56th
Annual Conference on Decision and Control (CDC), 2017b.

Paul Tseng. On linear convergence of iterative methods for the variational inequality problem.

Journal of Computational and Applied Mathematics, 60(1):237–252, June 1995. ISSN 0377-0427.

Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. submit-

ted to SIAM Journal on Optimization, 2(3), 2008.

Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence
of decentralized optimistic gradient descent/ascent in inﬁnite-horizon competitive markov
games. In Conference on Learning Theory, 2021a.

Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate conver-
gence in constrained saddle-point optimization. In 9th International Conference on Learning Rep-
resentations, 2021b.

Abhay Kumar Yadav, Sohil Shah, Zheng Xu, David W. Jacobs, and Tom Goldstein. Stabilizing
adversarial nets with prediction methods. In 6th International Conference on Learning Representa-
tions, 2018.

Taeho Yoon and Ernest K. Ryu. Accelerated algorithms for smooth convex-concave minimax prob-
lems with o(1/kˆ2) rate on squared gradient norm. In Proceedings of the 38th International Confer-
ence on Machine Learning, 2021.

Guodong Zhang, Xuchan Bao, Laurent Lessard, and Roger B. Grosse. A uniﬁed analysis of ﬁrst-
order methods for smooth games via integral quadratic constraints. Journal of Machine Learning
Research, 22:103:1–103:39, 2021.

Guojun Zhang and Yaoliang Yu. Convergence of gradient methods on bilinear zero-sum games.

In 8th International Conference on Learning Representations, 2020.

A Additional Preliminaries

For z ∈ Rn and D > 0, we use B(z, D) = {z(cid:48) ∈ Rn : (cid:107)z(cid:48) − z(cid:107) ≤ R} to denote the ball of radius D,
centered at z.

26

Min-Max Saddle Points. A special case of the variational inequality problem is the constrained
min-max problem minx∈X maxy∈Y f (x, y), where X and Y are closed convex sets in Rn, and f (·, ·)
(cid:19)

is smooth, convex in x, and concave in y. It is well known that if one set F(x, y) =

(cid:18) ∇x f (x, y)
−∇y f (x, y)

,

then F(x, y) is a monotone and Lipschitz operator [Facchinei and Pang, 2007].

Equilibria of Monotone Games. Monotone games are a large class of multi-player games
that include many common and well-studied class of games such as bilinear games, λ-cocoercive
games [Lin et al., 2020], zero-sum polymatrix games [Cai and Daskalakis, 2011, Cai et al., 2016],
and zero-sum socially-concave games [Even-Dar et al., 2009]. Besides, the min-max saddle point
problem is a special case of two-player monotone games. We include the deﬁnition of monotone
games here and remind readers that ﬁnding a Nash Equilibrium of a monotone game is exactly
the same as ﬁnding a solution to a monotone variational inequality.

A continuous game G is denoted as (N , (Xi)i∈[N], ( fi)i∈[N]) where there are N players N =
{1, · · · , N}. Player i ∈ N chooses action from a closed convex set Xi ∈ Rni such that X :=
Π
i∈N Xi ∈ Rn and wants to minimize its cost function fi : X → R. For each player i, we denote
x−i the vector of actions of all the other players. A Nash Equilibrium of game G is an action proﬁle
i ∈ Xi. Let F(x) = (∇xi fi(x), · · · , ∇xN fN(x)) ∈ Rn.
i, x∗
x∗ ∈ X such that fi(x∗) ≤ fi(x(cid:48)
We say G is monotone if (cid:104)F(x) − F(x(cid:48)), x − x(cid:48)(cid:105) ≥ 0 for any x, x(cid:48) ∈ X .

−i) for any x(cid:48)

In Lemma 7 we present several equivalent formulations of the tangent residual.

Lemma 7. Let Z be a closed convex set and F : Z → Rn be an operator. Denote NZ (z) the normal cone
of z and JZ (z) := {z} + TZ (z), where TZ (z) = {z(cid:48) ∈ Rn : (cid:104)z(cid:48), a(cid:105) ≤ 0, ∀a ∈ NZ (z)} is the tangent cone
of z. Then all of the following quantities are equivalent:

1.

(cid:114)(cid:107)F(z)(cid:107)2 − max a∈ (cid:98)NZ (z),

(cid:104)F(z),a(cid:105)≤0

(cid:104)F(z), a(cid:105)2

2. min a∈ (cid:98)NZ (z),
(cid:104)F(z),a(cid:105)≤0

(cid:107)F(z) − (cid:104)F(z), a(cid:105) · a(cid:107)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

Π

Π

TZ (z)

(cid:104)

JZ (z)

(cid:104)

− F(z)

(cid:105)(cid:13)
(cid:13)
(cid:13)

z − F(z)

(cid:13)
(cid:13)−F(z) − Π
(cid:13)

NZ (z)

(cid:105)

(cid:104)

− z

(cid:13)
(cid:13)
(cid:13)

− F(z)

(cid:105)(cid:13)
(cid:13)
(cid:13)

3.

4.

5.

6. min

a∈NZ (z)

(cid:107)F(z) + a(cid:107)

Proof. (quantity 1 = quantity 2). Observe that

min
a∈ (cid:98)NZ (z),
(cid:104)F(z),a(cid:105)≤0

(cid:107)F(z) − (cid:104)F(z), a(cid:105) · a(cid:107)2 = (cid:107)F(z)(cid:107)2 − max
a∈ (cid:98)NZ (z),
(cid:104)F(z),a(cid:105)≤0

(cid:104)F(z), a(cid:105)2 ·

(cid:16)

2 − (cid:107)a(cid:107)2(cid:17)

.

27

max a∈ (cid:98)NZ (z),
(cid:104)F(z),a(cid:105)≤0

Therefore,

it

is

enough

to

show that max a∈ (cid:98)NZ (z),
(cid:104)F(z),a(cid:105)≤0

(cid:104)F(z), a(cid:105)2 · (2 − (cid:107)a(cid:107)2)

=

(cid:104)F(z), a(cid:105)2.

If (cid:98)NZ (z) = {(0, . . . , 0)}, then the equality holds trivially. Now we

assume that {(0, . . . , 0)} (cid:40) (cid:98)NZ (z) and consider any a ∈ (cid:98)NZ (z)\(0, . . . , 0). Let c ∈
. By
Deﬁnition 2, (cid:107)a(cid:107) ≤ 1, which implies that c · a ∈ (cid:98)NZ (z). We try to maximize the following objective

1, 1
(cid:107)a(cid:107)

(cid:104)

(cid:105)

(cid:104)F(z), c · a(cid:105)2 ·

(cid:16)

2 − c2(cid:107)a(cid:107)2(cid:17)

=

(cid:104)F(z), a(cid:105)2
(cid:107)a(cid:107)2

· c2(cid:107)a(cid:107)2 ·

(cid:16)

2 − c2(cid:107)a(cid:107)2(cid:17)

.

One can easily verify that function c2(cid:107)a(cid:107)2 · (2 − c2(cid:107)a(cid:107)2) is maximized when c2(cid:107)a(cid:107)2 = 1 ⇔ c =

1

(cid:107)a(cid:107) . Thus when {(0, . . . , 0)} (cid:40) (cid:98)NZ (z),
(cid:16)

(cid:104)F(z), a(cid:105)2 ·

2 − (cid:107)a(cid:107)2(cid:17)

max
a∈ (cid:98)NZ (z),
(cid:104)F(z),a(cid:105)≤0

(cid:104)F(z), a(cid:105)2 ·

(cid:16)

2 − (cid:107)a(cid:107)2(cid:17)

(cid:104)F(z), a(cid:105)2

= max

a∈ (cid:98)NZ (z),
(cid:104)F(z),a(cid:105)≤0,
(cid:107)a(cid:107)=1

= max

a∈ (cid:98)NZ (z),
(cid:104)F(z),a(cid:105)≤0,
(cid:107)a(cid:107)=1

(cid:104)F(z), a(cid:105)2,

= max

a∈ (cid:98)NZ (z),
(cid:104)F(z),a(cid:105)≤0

which concludes the proof.
(quantity 3 = quantity 4). By deﬁnition, JZ (z) = {z} + TZ (z). Thus we have

(cid:104)

(cid:13)
(cid:13)
(cid:13)

Π

JZ (z)

z − F(z)

(cid:105)

− z

(cid:13)
(cid:13)
(cid:13) =

(cid:13)
(cid:13)
(cid:13)

Π

TZ (z)

(cid:104)

− F(z)

(cid:105)(cid:13)
(cid:13)
(cid:13).

(quantity 4 = quantity 5). By deﬁnition, the tangent cone TZ (z) is the polar cone of the normal
cone NZ (z). Since NZ (z) is a closed convex cone, by Moreau’s decomposition theorem, we have
for any vector x ∈ Rn,

x = Π

NZ (z)(x) + Π

TZ (z)(x),

(cid:68)

Π

NZ (z)(x), Π

TZ (z)(x)

(cid:69)

= 0.

Thus it is clear that we have

(cid:104)

(cid:13)
(cid:13)
(cid:13)

Π

JZ (z)

z − F(z)

(cid:105)

− z

(cid:13)
(cid:13)
(cid:13) =

=

(quantity 5 = quantity 6). Denote a∗ := Π

NZ (z)

− F(z)

(cid:105)(cid:13)
(cid:13)
(cid:13)
(cid:104)

(cid:104)

Π

(cid:13)
(cid:13)
TZ (z)
(cid:13)
(cid:13)
(cid:13)−F(z) − Π
(cid:13)
(cid:104)

(cid:105)

− F(z)

− F(z)

(cid:105)(cid:13)
(cid:13)
(cid:13).

NZ (z)

. By deﬁnition of projection, we have

a∗ = argmin
a∈NZ (z)

(cid:107)F(z) + a(cid:107)2.

28

Thus

(cid:13)
(cid:13)−F(z) − Π
(cid:13)

NZ (z)

(cid:104)

− F(z)

2

(cid:105)(cid:13)
(cid:13)
(cid:13)

= (cid:107)F(z) + a∗(cid:107)2 = min
a∈NZ (z)

(cid:107)F(z) + a(cid:107)2.

(quantity 6 = quantity 2). For any ﬁx non-zero a ∈ NZ (z), (i) if (cid:104)F(z), a(cid:105) > 0, then (cid:107)F(z) + a(cid:107)2 ≥
(cid:107)F(z)(cid:107)2, and (ii) if (cid:104)F(z), a(cid:105) ≤ 0, (cid:107)F(z) + a(cid:107)2 ≥ (cid:107)F(z) − (cid:104)F(z),

a

(cid:107)F(z) + r · a(cid:107)2 =

min
r≥0

F(z) −

(cid:28)

F(z),

(cid:13)
(cid:13)
(cid:13)
(cid:13)

a
(cid:107)a(cid:107)

a
(cid:107)a(cid:107) (cid:105) ·
(cid:29)

·

(cid:107)a(cid:107) (cid:107)2, as
(cid:13)
(cid:13)
(cid:13)
(cid:13)

a
(cid:107)a(cid:107)

2

.

Hence,

(cid:107)F(z) + a(cid:107)2 = min

(cid:13)F(z) − (cid:10)F(z), a(cid:48)(cid:11) · a(cid:48)(cid:13)
(cid:13)
(cid:13)

2

min
a∈NZ (z)

a(cid:48)∈ (cid:98)NZ (z)
(cid:104)F(z),a(cid:48)(cid:105)≤0
The ﬁrst equality is because for any a ∈ NZ (z), there exists a(cid:48) ∈ (cid:98)NZ (z) so that (cid:107)F(z) + a(cid:107)2 ≥
(cid:107)F(z) − (cid:104)F(z), a(cid:48)(cid:105)(cid:107)2.

In the following Lemma, we show a useful property of the tangent residual that we use repeat-

edly.
Lemma 8. Let Z ⊆ Rn be a closed convex set and F : Z → R be an operator. Let η > 0 and z1, z2, z3 ∈ Z
be three points such that z1 = ΠZ [z2 − ηF(z3)], then we have

rtan(z1) ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

z2 − z1
η

+ F(z1) − F(z3)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

Proof. If z1 = z2 − ηF(z3), then the lemma holds since

rtan(z1) ≤ (cid:107)F(z1)(cid:107) = (cid:107)F(z3) + F(z1) − F(z3)(cid:107) =

(cid:13)
(cid:13)
(cid:13)
(cid:13)

z2 − z1
η

+ F(z1) − F(z3)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

For the rest of the proof, we assume that z1 (cid:54)= z2 − ηF(z3). Since z2 − ηF(z3) − z1 ∈ N(z1), we
have

(cid:104)z2 − ηF(z3) − z1, z − z1(cid:105) ≤ 0,

∀z ∈ Z.

Deﬁne a := z2−ηF(z3)−z1

(cid:107)z2−ηF(z3)−z1(cid:107) . We thus know a ∈ (cid:98)N(z1). Let
(cid:40) F(z1)−(cid:104)a,F(z1)(cid:105)·a
(cid:107)F(z1)−(cid:104)a,F(z1)(cid:105)·a(cid:107)
(0, . . . , 0)

a⊥ :=

if (cid:107)F(z1) − (cid:104)a, F(z1)(cid:105) · a(cid:107) (cid:54)= 0,
otherwise.

Observe that (cid:104)a, a(cid:105) = 1, (cid:104)a⊥, a(cid:105) = 0 and F(z1) = (cid:104)a, F(z1)(cid:105)a + (cid:104)a⊥, F(z1)(cid:105)a⊥. Thus

0 = (cid:104)a⊥, a(cid:105) = (cid:104)a⊥, z2 − ηF(z3) − z1(cid:105)

⇔(cid:104)a⊥, F(z3)(cid:105) =

(cid:104)a⊥, z2 − z1(cid:105)
η

.

29

(31)

Moreover, the fact that (cid:104)a, z1 − z2 + ηF(z3)(cid:105) ≤ 0 implies (cid:104)a, F(z3)(cid:105) ≤ (cid:104)a,z2−z1(cid:105)
plies that

η

, which further im-

(cid:104)a, F(z1)(cid:105) = (cid:104)a, F(z3) + F(z1) − F(z3)(cid:105) ≤

(cid:28)

a,

z2 − z1
η

+ F(z1) − F(z3)

(cid:29)

.

(32)

Combining the deﬁnition of rtan(z1), Equation (31), and Equation (32) we have

rtan(z1)2 ≤ ||F(z1)||2 − (cid:104)a, F(z1)(cid:105)2 · 1[(cid:104)a, F(z1)(cid:105) ≤ 0]

= (cid:104)a⊥, F(z1)(cid:105)2 + (cid:104)a, F(z1)(cid:105)2 · 1[(cid:104)a, F(z1)(cid:105) > 0]
= (cid:104)a⊥, F(z3) + F(z1) − F(z3)(cid:105)2 + (cid:104)a, F(z1)(cid:105)2 · 1[(cid:104)a, F(z1)(cid:105) > 0]

≤

≤

(cid:28)

a⊥,

z2 − z1
η

+ F(z1) − F(z3)

(cid:29)2

(cid:28)

a,

+

z2 − z1
η

+ F(z1) − F(z3)

(cid:29)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

z2 − z1
η

+ F(z1) − F(z3)

2

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Proof of Lemma 2: If (cid:104)a, F(z)(cid:105) ≥ 0 for all a ∈ (cid:98)N(z), then we have rtan(z) = (cid:107)F(z)(cid:107). Thus for any
z(cid:48) ∈ Z, by Cauchy-Schwarz inequality, we have

(cid:104)F(z), z − z(cid:48)(cid:105) ≤ (cid:107)F(z)(cid:107)(cid:107)z − z(cid:48)(cid:107) ≤ D · rtan(z).

Otherwise there exists a ∈ (cid:98)N(z) such that (cid:107)a(cid:107) = 1, (cid:104)a, F(z)(cid:105) < 0 and rtan(z) =
(cid:113)

(cid:107)F(z)(cid:107)2 − (cid:104)a, F(z)(cid:105)2 = (cid:107)F(z) − (cid:104)a, F(z)(cid:105)a(cid:107). Then for any z(cid:48) ∈ Z, we have

(cid:10)F(z), z − z(cid:48)(cid:11) = (cid:10)F(z) − (cid:104)a, F(z)(cid:105)a, z − z(cid:48)(cid:11) + (cid:104)a, F(z)(cid:105) · (cid:10)a, z − z(cid:48)(cid:11)

≤ (cid:10)F(z) − (cid:104)a, F(z)(cid:105)a, z − z(cid:48)(cid:11)
≤ (cid:107)F(z) − (cid:104)a, F(z)(cid:105)a(cid:107)(cid:107)z − z(cid:48)(cid:107)
≤ D · rtan(z),

where we use (cid:104)a, F(z)(cid:105) < 0 and (cid:104)a, z − z(cid:48)(cid:105) ≥ 0 in the ﬁrst inequality and Cauchy-Schwarz inequal-
ity in the second inequality.

If Z (cid:48) = X (cid:48) × Y (cid:48) and F(x, y) =

for a convex-concave function f then

(cid:18) ∇x f (x, y)
−∇y f (x, y)

(cid:19)

dgX (cid:48),Y (cid:48)
f

(z) = max
y(cid:48)∈Y (cid:48)
= max
y(cid:48)∈Y (cid:48)
≤ max
y(cid:48)∈Y (cid:48)
= max
z(cid:48)∈Z (cid:48)
√

f (x(cid:48), y)

f (x, y(cid:48)) − min
x(cid:48)∈X (cid:48)
( f (x, y(cid:48)) − f (x, y)) − min
x(cid:48)∈X (cid:48)
(cid:10)∇y f (x, y), y(cid:48) − y(cid:11) + max
x(cid:48)∈X (cid:48)
(cid:10)F(z), z − z(cid:48)(cid:11),

( f (x(cid:48), y) − f (x, y)),
(cid:10)∇x f (x, y), x − x(cid:48)(cid:11),

≤ D

2 · rtan(z),

30

where we use the fact that f is a convex-concave function in the ﬁrst inequality and (cid:107)z − z(cid:48)(cid:107) =
(cid:113)
(cid:4)

(cid:107)x − x(cid:48)(cid:107)2 + (cid:107)y − y(cid:48)(cid:107)2 ≤

2D in the second inequality.

√

B Missing Proofs from Section 4

Proof of Lemma 3: By Pythagorean inequality,

(cid:107)zk+1 − z∗(cid:107)2 ≤ (cid:107)zk − ηF(zk+ 1

) − z∗(cid:107)2 − (cid:107)zk − ηF(zk+ 1
= (cid:107)zk − z∗(cid:107)2 − (cid:107)zk − zk+1(cid:107)2 + 2η(cid:104)F(zk+ 1
= (cid:107)zk − z∗(cid:107)2 − (cid:107)zk − zk+1(cid:107)2 + 2η(cid:104)F(zk+ 1

) − zk+1(cid:107)2
), z∗ − zk+1(cid:105)
), z∗ − zk+ 1

2

2

2

2

(cid:105) + 2η(cid:104)F(zk+ 1

2

2

), zk+ 1

2

− zk+1(cid:105).

(33)

We ﬁrst use monotonicity of F(·) to argue that (cid:104)F(zk+ 1

2

), z∗ − zk+ 1

2

(cid:105) ≤ 0.

Fact 1. For all z ∈ Z, (cid:104)F(z), z∗ − z(cid:105) ≤ 0.

Proof.

0 ≤ (cid:104)F(z∗) − F(z), z∗ − z(cid:105)

= (cid:104)F(z∗), z∗ − z(cid:105) − (cid:104)F(z), z∗ − z(cid:105)
≤ −(cid:104)F(z), z∗ − z(cid:105)

(monotonicity of F(·))

(optimality of z∗ and z ∈ Z )

We can simplify Equation (33) using Fact 1:

(cid:107)zk+1 − z∗(cid:107)2 ≤ (cid:107)zk − z∗(cid:107)2 − (cid:107)zk − zk+1(cid:107)2 + 2η(cid:104)F(zk+ 1
= (cid:107)zk − z∗(cid:107)2 − (cid:107)zk − zk+ 1
= (cid:107)zk − z∗(cid:107)2 − (cid:107)zk − zk+ 1

2

2

2

), zk+ 1

− zk+1(cid:105)
− zk+1(cid:107)2 − 2(cid:104)zk − ηF(zk+ 1
− zk+1(cid:107)2
− zk+1(cid:105) − 2(cid:104)ηF(zk) − ηF(zk+ 1
− zk+1(cid:107)2 − 2η(cid:104)F(zk) − F(zk+ 1

2

2

2

2

(cid:107)2 − (cid:107)zk+ 1
(cid:107)2 − (cid:107)zk+ 1
, zk+ 1
(cid:107)2 − (cid:107)zk+ 1

2

2

2

2

) − zk+ 1

, zk+ 1

2

− zk+1(cid:105)

2

), zk+ 1
), zk+ 1

2

2

2

− zk+1(cid:105)

− zk+1(cid:105)

− 2(cid:104)zk − ηF(zk) − zk+ 1

≤ (cid:107)zk − z∗(cid:107)2 − (cid:107)zk − zk+ 1

2

The last inequality is because (cid:104)zk − ηF(zk) − zk+ 1
fact that zk+ 1

= ΠZ [zk − ηF(zk)] and zk+1 ∈ Z.

2

Finally, since F(·) is L-Lipschitz, we know that

2

, zk+ 1

2

− zk+1(cid:105) ≥ 0, which follows from the that

−(cid:104)F(zk) − F(zk+ 1

2

), zk+ 1

2

− zk+1(cid:105) ≤ (cid:107)F(zk) − F(zk+ 1

2

)(cid:107) · (cid:107)zk+ 1

2

− zk+1(cid:107) ≤ L(cid:107)zk − zk+ 1

2

(cid:107) · (cid:107)zk+ 1

2

− zk+1(cid:107).

31

So we can further simplify the inequality as follows:

(cid:107)zk+1 − z∗(cid:107)2 ≤(cid:107)zk − z∗(cid:107)2 − (cid:107)zk − zk+ 1
(cid:107)2 − (cid:107)zk+ 1
≤(cid:107)zk − z∗(cid:107)2 − (cid:107)zk − zk+ 1
(cid:107)2 − (cid:107)zk+ 1
≤(cid:107)zk − z∗(cid:107)2 − (1 − η2L2)(cid:107)zk − zk+ 1

2

2

2

2

2

− zk+1(cid:107)2 − 2η(cid:104)F(zk) − F(zk+ 1
− zk+1(cid:107)2 + 2ηL(cid:107)zk − zk+ 1
(cid:107)2

2

2

), zk+ 1

2

− zk+1(cid:105)

(cid:107) · (cid:107)zk+ 1

2

− zk+1(cid:107)

Hence,

(cid:107)zk − z∗(cid:107)2 ≥ (cid:107)zk+1 − z∗(cid:107)2 + (1 − η2L2)(cid:107)zk − zk+ 1

2

(cid:107)2.

Proof of Lemma 4: We need the following fact for our proof.

Fact 2. (cid:107)zk+ 1

2

− zk+1(cid:107) ≤ ηL(cid:107)zk − zk+ 1

2

(cid:107). Moreover, when ηL < 1, (cid:107)zk+ 1

2

(cid:4)

.

− zk+1(cid:107) ≤ (cid:107)zk−zk+1(cid:107)
(cid:105)

1−ηL

(cid:104)

Proof. Recall that zk+ 1
. By the non-
expansiveness of the projection operator and the L-Lipschitzness of operator F, we have that
(cid:107)zk+ 1

) − F(zk))(cid:107) ≤ ηL(cid:107)zk − zk+ 1

= ΠZ [zk − ηF(zk)] and zk+1 = ΠZ

− zk+1(cid:107) ≤ (cid:107)η(F(zk+ 1

zk − ηF(zk+ 1

(cid:107).

)

2

2

2

2

2

Finally, by the triangle inequality

(cid:107)zk − zk+1(cid:107) ≥

(cid:13)
(cid:13)
(cid:13)zk − zk+ 1

2

(cid:13)
(cid:13)
(cid:13) −

(cid:13)
(cid:13)
(cid:13)zk+ 1

2

− zk+1

(cid:13)
(cid:13)
(cid:13) ≥ (1 − ηL)

(cid:13)
(cid:13)
(cid:13)zk − zk+ 1

2

(cid:13)
(cid:13)
(cid:13).

Now we prove Lemma 4. By the L-Lipschitzness of operator F we have

(cid:107)F(zk+1) − F(zk+ 1

2

)(cid:107) ≤ L(cid:107)zk+1 − zk+ 1

2

(cid:107) ≤ ηL2(cid:107)zk − zk+ 1

2

(cid:107).

(34)

Recall that zk+1 = ΠZ

(cid:104)

zk − ηF(zk+ 1

2

(cid:105)

)

. Using Lemma 8, we have

rtan(zk+1) ≤

≤

≤

≤

(cid:13)
zk − zk+1
(cid:13)
(cid:13)
(cid:13)
η
(cid:107)zk − zk+1(cid:107)
η

+ F(zk+1) − F(zk+ 1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

)

2

+ (cid:107)F(zk+1) − F(zk+ 1

2

)(cid:107)

(cid:107)zk − zk+1(cid:107) + (ηL)2(cid:107)zk − zk+ 1
η
|| + ||zk+ 1

||zk − zk+ 1

2

2

2

(cid:107)

− zk+1|| + (ηL)2||zk − zk+ 1

||

2

≤ (cid:0)1 + ηL + (ηL)2(cid:1)

η

||zk − zk+ 1
η

2

||

.

32

The second and the fourth inequality follow from the triangle inequality. The third inequality
follows from Equation (34). In the ﬁnal inequality we use ||zk+ 1
|| by
(cid:4)
Fact 2.
Proof of Lemma 5: By Lemma 3 we have

− zk+1|| ≤ ηL||zk − zk+ 1

2

2

(cid:107)z0 − z∗(cid:107)2 ≥ (cid:107)zT+1 − z∗(cid:107)2 + (1 − η2L2)

T
∑
k=0

(cid:107)zk − zk+ 1

2

(cid:107)2 ≥ (1 − η2L2)

T
∑
k=0

(cid:107)zk − zk+ 1

2

(cid:107)2

Thus there exists a t∗ ∈ [T] such that (cid:107)zt∗ − zt∗+ 1
Lemma 4.

2

(cid:107)2 ≤ (cid:107)z0−z∗(cid:107)2

T(1−η2 L2) . We conclude the proof by applying
(cid:4)

C Missing Proofs from Section 5.1

Proposition 1.

(cid:107)F(zk)(cid:107)2 − (cid:107)F(zk+1)(cid:107)2 + 2 ·
(cid:18)(cid:13)
(cid:13)
(cid:13)F(zk+ 1

F(zk+1) − F(zk), F(zk+ 1
2
(cid:13)
(cid:13)
(cid:13)F(zk+ 1

) − F(zk+1)

(cid:13)
(cid:13)
(cid:13)

+

−

)

2

2

) − F(zk)

2(cid:19)

(cid:13)
(cid:13)
(cid:13)

= 0.

2

(cid:69)

Proof. Expanding the LHS of the equation in the statement we can verify that

(cid:107)F(zk)(cid:107)2 − (cid:107)F(zk+1)(cid:107)2 + 2 ·
(cid:13)
(cid:13)
(cid:13)F(zk+ 1
(cid:13)
(cid:13)
(cid:13)F(zk+ 1

−

+

2

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

)

)

F(zk+1), F(zk+ 1
(cid:68)
2

2

− 2 ·

2

+ 2 ·

(cid:68)

(cid:69)

)

− 2 ·

(cid:68)

(cid:69)

)

F(zk), F(zk+ 1
(cid:69)

2

)

F(zk+1), F(zk+ 1
2
(cid:69)

F(zk), F(zk+ 1

)

2

+ (cid:107)F(zk+1)(cid:107)2

− (cid:107)F(zk)(cid:107)2 = 0.

(cid:68)

(cid:68)

D Non-Monotonicity of Several Standard Performance Measures

We conduct numerical experiments by trying to ﬁnd saddle points in constrained bilinear games
using EG, and veriﬁed that the following performance measures are not monotone: the (squared)
natural residual, (cid:107)zk − zk+ 1

(cid:107)2, (cid:107)zk − zk+1(cid:107)2, maxz∈Z (cid:104)F(z), zk − z(cid:105), maxz∈Z (cid:104)F(zk), zk − z(cid:105).

All of our counterexamples are constructed by trying to ﬁnd a saddle point in bilinear games

2

of the following form:

min
x∈X

max
y∈Y

x(cid:62) Ay − b(cid:62)x − c(cid:62)y

(35)

where X , Y ⊆ R2, A is a 2 × 2 matrix and b, c are 2-dimensional column vectors. All of the
instances of the bilinear game considered in this section have X , Y = [0, 10]2. We denote by

33

Z = X × Y and by F(x, y) =

(cid:18) Ay − b

(cid:19)

−A(cid:62)x + c

: Z → Rn. We remind readers that ﬁnding a saddle

point of bilinear game (35), is equivalent to solving the monotone VI with operator F(z) on set Z.

D.1 Non-Monotonicity of the Natural Residual and its Variants

Performance Measure: Natural Residual. Let A =

(cid:21)

(cid:20)1 2
1 1

, b = c =

(cid:21)

(cid:20)1
1

. Running

the EG method on the corresponding VI problem with step-size η = 0.1 starting at z0 =
(0.3108455, 0.4825575, 0.4621875, 0.5768655)T has the following trajectory:

z1 = (0.24923465, 0.47967569, 0.43497808, 0.57458145)T,
z2 = (0.19396855, 0.48164918, 0.40193211, 0.56061753)T.

Thus we have

rnat(z0)2 =0.15170013184049996,
rnat(z1)2 =0.13617654362050116,
rnat(z2)2 =0.16125792556139756.

It is clear that the natural residual is not monotone.

Performance Measure: (cid:107)zk − zk+ 1

2

akonikolas, 2020] is exactly 1

η · (cid:107)zk − zk+ 1

2

(cid:107). Let A =

(cid:107)2. Note that the norm of the operator mapping deﬁned in [Di-
(cid:21)

(cid:21)

(cid:20)0.50676631 0.15042569
0.46897595 0.96748026

, b = c =

(cid:20)1
1

.

Running the EG method on the corresponding VI problem with step-size η = 0.1 starting at
z0 = (2.35037432, 0.00333996, 1.70547279, 0.71065999)T has the following trajectory:

=(2.35325656, 0, 1.72473848, 0.64633879)T,
z 1
2
z1 =(2.35324779, 0, 1.72472791, 0.64605901)T,
=(2.35612601, 0, 1.74398258, 0.58145791)T
z2 =(2.35612201, 0, 1.74412844, 0.5815012)T,
=(2.35898819, 0, 1.76352876, 0.51694333)T.

2

z1+ 1

z2+ 1

2

Thus we have

(cid:13)
(cid:13)
(cid:13)z0 − z 1

2

(cid:13)
(cid:13)
(cid:13)z1 − z1+ 1
(cid:13)
(cid:13)
(cid:13)z2 − z2+ 1

2

2

2

2

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=0.00452784581555656,

=0.004552329544896258,

=0.004552306444552208.

It is clear that the (cid:107)zk − zk+ 1

2

(cid:107)2 is not monotone.

34

Performance Measure: (cid:107)zk − zk+1(cid:107)2. Let A =
ning the EG method on the corresponding VI problem with step-size η = 0.1 starting at z0 =
(2.37003485, 0, 1.84327237, 0.25934775)T has the following trajectory:

, b = c =

. Run-

(cid:20)0.50676631 0.15042569
0.46897595 0.96748026

(cid:21)

(cid:21)

(cid:20)1
1

z1 =(2.37267186, 0, 1.86351397, 0.1950396)T,
z2 =(2.37524308, 0, 1.88388624, 0.13077023)T,
z3 =(2.37774149, 0.00426125, 1.90438549, 0.06653856)T.

Thus we have

(cid:107)z0 − z1(cid:107)2 =0.004552214685275266,
(cid:107)z1 − z2(cid:107)2 =0.004552191904998012,
(cid:107)z2 − z3(cid:107)2 =0.004570327450598002.

It is clear that the (cid:107)zk − zk+1(cid:107)2 is not monotone.

D.2 Non-Monotonicity of the Gap Functions and its Variant

Performance Measure:
(cid:20)−0.21025101

0.22360196
0.40667685 −0.2922158

(cid:21)

Function
(cid:20)0
(cid:21)
0

Gap

and maxz∈Z (cid:104)F(z), zk − z(cid:105). Let A

=

, b = c =

. One can easily verify that (cid:104)F(z), zk − z(cid:105) =

(cid:104)F(zk), zk − z(cid:105), which further implies that maxz∈Z (cid:104)F(z), zk − z(cid:105) = maxz∈Z (cid:104)F(zk), zk − z(cid:105) =
GAP(zk), which implies that non-monotonicity of the gap function implies non-monotonicity of
maxz∈Z (cid:104)F(z), zk − z(cid:105). Running the EG method on the corresponding VI problem with step-size
η = 0.1 starting at z0 = (0.53095379, 0.29084076, 0.62132986, 0.49440498) has the following
trajectory:

z1 = (0.53290086, 0.28009156, 0.62151204, 0.4981395)T,
z2 = (0.5347502, 0.26947398, 0.62122195, 0.50222691)T.

One can easily verify that

GAP(z0) =0.6046398415472187,
GAP(z1) =0.58462873354003214,
GAP(z2) =0.5914026255469654.

It is clear that the duality gap is not monotone.

E Optimistic Gradient Descent Ascent Algorithm

Let Z ⊆ Rn be a closed convex set and F : Z → R be an operator. Let zk and wk be the k-th iterate
of the Optimistic Gradient Descent Ascent algorithm (OGDA) algorithm. Let z0, w0 be arbitrary

35

point in Z and {zk, wk}k≥0 be the iterated of the OGDA algorithm. The update rule for any k ≥ 0
is as follows:

wk+1 = ΠZ [zk − ηF(wk)] = arg min
z∈Z
zk+1 = ΠZ [zk − ηF(wk+1)] = arg min
z∈Z

(cid:107)z − (zk − ηF(wk)) (cid:107)

(cid:107)z − (zk − ηF(wk+1))(cid:107)

(36)

We prove last-iterate convergence for OGDA with respect to the gap function, natural residual
and tangent residual in Theorem 8 at Section E.4. The last-iterate convergence proof for OGDA is
a simple extension of the proof for EG. The last-iterate convergence for the performance measures
we mentioned follow from the last-iterate convergence of the following monotonically decreasing
potential function:

Φ

k = (cid:107)F(zk) − F(wk)(cid:107)2 + rtan(zk)2

(37)

In Section E.1 we show that OGDA enjoys last-iterate convergence with respect to the quantity
(cid:107)zk − wk+1(cid:107) [Wei et al., 2021a, Hsieh et al., 2019] and in Section E.2 we show how to upper bound
the potential function Φ
k by the best-iterate. In Section E.3 we show that the potential function Φ
k
is monotonically decreasing across iterates and ﬁnally in Section E.4 we show how to translate the
last-iterate convergence with respect to the potential function Φ
k to last-iterate convergence of the
performance measures of interest.

E.1 Best-Iterate Convergence of OGDA with Constant Step Size

Best-iterate convergence guarantees for OGDA are known [Wei et al., 2021a] and can easily be
derived by Hsieh et al. [2019]. We include the proof here for completeness.

Lemma 9. Let Z ⊆ Rn be a closed convex set, F : Z → R be a monotone and L-Lipschitz operator, and z∗
be a saddle point. Let z0, w0 ∈ Z be arbitrary starting points and {zk, wk}k≥0 be the iterates of the OGDA
algorithm with any step size η ∈ (0, 1

2L ). Then for all T ≥ 1,

T
∑
k=0

(cid:107)zk − wk+1(cid:107)2 ≤

1 − 2η2L2
1 − 4η2L2

(cid:107)z0 − z∗(cid:107)2 +

2η2L2
1 − 4η2L2

(cid:107)w0 − z0(cid:107)2.

(38)

Proof of Lemma 9:
(cid:107)wk − wk+1(cid:107)2 to the weighted sum of {(cid:107)zt − wt+1(cid:107)2}0≤t≤k.

In order to upper bound ∑T

k=0 (cid:107)wk − wk+1(cid:107)2, we ﬁrst relate the quantity

Lemma 10. For all k ≥ 0,

(cid:107)wk − wk+1(cid:107)2 ≤ 2(2η2L2)k(cid:107)w0 − z0(cid:107)2 +

k
∑
t=0

2(2η2L2)t(cid:107)zk−t − wk+1−t(cid:107)2.

(39)

Moreover, for all T ≥ 0,

T
∑
k=0

(cid:107)wk − wk+1(cid:107)2 ≤

(cid:32)

2
1 − 2η2L2

(cid:107)w0 − z0(cid:107)2 +

(cid:33)

(cid:107)zk − wk+1(cid:107)2

.

T
∑
k=0

(40)

36

Proof. We ﬁrst prove Equation (39) by induction. Note that for all k ≥ 0, we have

(cid:107)wk − wk+1(cid:107)2 = (cid:107)wk − zk + zk − wk+1(cid:107)2

≤ 2(cid:107)wk − zk(cid:107)2 + 2(cid:107)zk − wk+1(cid:107)2.

(41)

The inequality follows from the fact that (a + b)2 ≤ 2a2 + 2b2. Thus Equation (39) holds for the
base case k = 0. For the sake of induction, we assume that Equation (39) holds for some k − 1 ≥ 0.
Using the update rule of OGDA, the non-expansiveness of the projection operator, and the L-
Lipschitzness of F, for all k ≥ 1 we have

(cid:107)wk − zk(cid:107)2 ≤ η2(cid:107)F(wk−1) − F(wk)(cid:107)2 ≤ η2L2(cid:107)wk−1 − wk(cid:107)2.

(42)

Combining Equation (41), Equation (42), and the induction assumption, we have

(cid:107)wk − wk+1(cid:107)2 ≤ 2(cid:107)wk − zk(cid:107)2 + 2(cid:107)zk − wk+1(cid:107)2

≤ 2η2L2(cid:107)wk−1 − wk(cid:107)2 + 2(cid:107)zk − wk+1(cid:107)2
k−1
∑
t=0

2(2η2L2)k−1(cid:107)w0 − z0(cid:107)2 +

≤ 2η2L2

(cid:32)

2(2η2L2)t(cid:107)zk−1−t − wk−t(cid:107)2

+ 2(cid:107)zk − wk+1(cid:107)2

(cid:33)

= 2(2η2L2)k(cid:107)w0 − z0(cid:107)2 +

= 2(2η2L2)k(cid:107)w0 − z0(cid:107)2 +

k
∑
t=1
k
∑
t=0

2(2η2L2)t(cid:107)zk−t − wk+1−t(cid:107)2 + 2(cid:107)zk − wk+1(cid:107)2

2(2η2L2)t(cid:107)zk−t − wk+1−t(cid:107)2.

This completes the proof of Equation (39).

Summing Equation (39) with k = 0, 1, · · · , T, we have

T
∑
k=0

(cid:107)wk − wk+1(cid:107)2 ≤

=

≤

T
∑
k=0
T
∑
k=0

2(2η2L2)k(cid:107)w0 − z0(cid:107)2 +

2(2η2L2)k(cid:107)w0 − z0(cid:107)2 +

T
∑
k=0
T
∑
k=0

k
∑
t=0
(cid:32)T−k
∑
t=0

2(2η2L2)t(cid:107)zk−t − wk+1−t(cid:107)2

(cid:33)

2(2η2L2)t

· (cid:107)zk − wk+1(cid:107)2

(cid:33)

2
1 − 2η2L2

(cid:32)

(cid:107)w0 − z0(cid:107)2 +

T
∑
k=0

(cid:107)zk − wk+1(cid:107)2

.

This completes the proof of Equation (40).

Back to the proof of Lemma 9. For all k ≥ 0, we have

(cid:107)zk+1 − z∗(cid:107)2 = (cid:107)zk+1 − zk + zk − z∗(cid:107)2

= (cid:107)zk − z∗(cid:107)2 + (cid:107)zk+1 − zk(cid:107)2 + 2(cid:104)zk+1 − zk, zk − z∗(cid:105)
= (cid:107)zk − z∗(cid:107)2 − (cid:107)zk+1 − zk(cid:107)2 + 2(cid:104)zk+1 − zk, zk+1 − z∗(cid:105)
≤ (cid:107)zk − z∗(cid:107)2 − (cid:107)zk+1 − zk(cid:107)2 − 2η(cid:104)F(wk+1), zk+1 − z∗(cid:105).

(43)

37

The last
ΠZ [zk − ηF(wk+1)].

inequality follows

from (cid:104)zk+1 − zk + ηF(wk+1), zk+1 − z∗(cid:105) ≤ 0 as zk+1 =

Similarly, for all k ≥ 0, we have

(cid:107)zk+1 − wk+1(cid:107)2 = (cid:107)zk+1 − zk + zk − wk+1(cid:107)2

= (cid:107)zk+1 − zk(cid:107)2 + (cid:107)zk − wk+1(cid:107)2 + 2(cid:104)zk − wk+1, zk+1 − zk(cid:105)
= (cid:107)zk+1 − zk(cid:107)2 − (cid:107)zk − wk+1(cid:107)2 + 2(cid:104)zk − wk+1, zk+1 − wk+1(cid:105)
≤ (cid:107)zk+1 − zk(cid:107)2 − (cid:107)zk − wk+1(cid:107)2 + 2η(cid:104)F(wk), zk+1 − wk+1(cid:105).

(44)

The last
ΠZ [zk − ηF(wk)].

inequality follows from (cid:104)zk − ηF(wk) − wk+1, zk+1 − wk+1(cid:105) ≤ 0 as wk+1 =

We can further simplify Equation (43) using Fact 1:

(cid:107)zk+1 − z∗(cid:107)2 ≤ (cid:107)zk − z∗(cid:107)2 − (cid:107)zk+1 − zk(cid:107)2 − 2η(cid:104)F(wk+1), zk+1 − z∗(cid:105)

= (cid:107)zk − z∗(cid:107)2 − (cid:107)zk+1 − zk(cid:107)2 − 2η(cid:104)F(wk+1), zk+1 − wk+1(cid:105) + 2η(cid:104)F(wk+1), z∗ − wk+1(cid:105)
≤ (cid:107)zk − z∗(cid:107)2 − (cid:107)zk+1 − zk(cid:107)2 − 2η(cid:104)F(wk+1), zk+1 − wk+1(cid:105).
(45)

Summing Equation (44) and Equation (45), we get

(cid:107)zk+1 − z∗(cid:107)2 ≤ (cid:107)zk − z∗(cid:107)2 − (cid:107)zk − wk+1(cid:107)2 − (cid:107)zk+1 − wk+1(cid:107)2 + 2η(cid:104)F(wk) − F(wk+1), zk+1 − wk+1(cid:105)

≤ (cid:107)zk − z∗(cid:107)2 − (cid:107)zk − wk+1(cid:107)2 − (cid:107)zk+1 − wk+1(cid:107)2 + 2η(cid:107)F(wk) − F(wk+1)(cid:107)(cid:107)zk+1 − wk+1(cid:107)
≤ (cid:107)zk − z∗(cid:107)2 − (cid:107)zk − wk+1(cid:107)2 − (cid:107)zk+1 − wk+1(cid:107)2 + 2ηL(cid:107)wk − wk+1(cid:107)(cid:107)zk+1 − wk+1(cid:107)
≤ (cid:107)zk − z∗(cid:107)2 − (cid:107)zk − wk+1(cid:107)2 + η2L2(cid:107)wk − wk+1(cid:107)2,

(46)

where we use Cauchy-Schwarz inequality in the second inequality and L-Lipschitzness of F(·) in
the third inequality. In the last inequality, we optimize the quadratic function in (cid:107)zk+1 − wk+1(cid:107).

Summing Equation (46) for k = 0, 1, · · · , T and using Lemma 10, we get

(cid:107)zT+1 − z∗(cid:107)2 ≤ (cid:107)z0 − z∗(cid:107)2 −

≤ (cid:107)z0 − z∗(cid:107)2 −

T
∑
k=0
T
∑
k=0

T
∑
k=0
2η2L2
1 − 2η2L2

(cid:107)zk − wk+1(cid:107)2 +

(cid:107)zk − wk+1(cid:107)2 + η2L2

(cid:107)wk − wk+1(cid:107)2

(cid:32)

(cid:107)w0 − z0(cid:107)2 +

(cid:33)

(cid:107)zk − wk+1(cid:107)2

T
∑
k=0

(Lemma 10)

= (cid:107)z0 − z∗(cid:107)2 −

1 − 4η2L2
1 − 2η2L2

T
∑
k=0

(cid:107)zk − wk+1(cid:107)2 +

2η2L2
1 − 2η2L2

(cid:107)w0 − z0(cid:107)2.

Since η2L2 < 1

4 , we complete the proof by rearranging the above inequality.

(cid:4)

38

E.2 Best-Iterate of Φ
In this section, we use Lemma 9 to show that there exists t∗ ∈ [T] such that Φt∗ = O( 1

k

T ).

Lemma 11. Let Z ⊆ Rn be a closed convex set, F : Z → R be a monotone and L-Lipschitz operator,
and z∗ be a saddle point. Let z0, w0 ∈ Z be arbitrary starting point and {zk, wk}k≥0 be the iterates of the
OGDA algorithm with any step size η ∈ (0, 1

2L ). Then for all T ≥ 1,

T
∑
k=1

(cid:16)

(cid:107)ηF(zk) − ηF(wk)(cid:107)2 + η2rtan(zk)2(cid:17)

≤

4 + 6η4L4
1 − 4η2L2

(cid:107)z0 − z∗(cid:107)2 +

16η2L2 + 6η4L4
1 − 4η2L2

(cid:107)w0 − z0(cid:107)2.

Moreover, when w0 = z0

T
∑
k=1

(cid:16)

(cid:107)ηF(zk) − ηF(wk)(cid:107)2 + η2rtan(zk)2(cid:17)

≤

4 + 6η4L4
1 − 4η2L2

(cid:107)z0 − z∗(cid:107)2.

Proof of Lemma 11: For all k ≥ 1, we have

(cid:107)ηF(zk) − ηF(wk)(cid:107)2 ≤ η2L2(cid:107)zk − wk(cid:107)2

≤ η4L4(cid:107)wk−1 − wk(cid:107)2.

(L-Lipschitzness of F)

(Equation (42))

Using Lemma 8 with the fact that zk = ΠZ [zk−1 − ηF(wk)], we have for all k ≥ 1,

η2rtan(zk)2 ≤ (cid:107)zk−1 − zk + ηF(zk) − ηF(wk)(cid:107)2

≤ 2(cid:107)zk−1 − zk(cid:107)2 + 2η2(cid:107)F(zk) − F(wk)(cid:107)2
≤ 2(cid:107)zk−1 − wk + wk − zk(cid:107)2 + 2η2L2(cid:107)wk − zk(cid:107)2
≤ 4(cid:107)zk−1 − wk(cid:107)2 + (4 + 2η2L2)(cid:107)wk − zk(cid:107)2
≤ 4(cid:107)zk−1 − wk(cid:107)2 + (4 + 2η2L2)η2L2(cid:107)wk−1 − wk(cid:107)2.

(L-Lipschitzness of F)

(Equation (42))

Summing the above inequalities with k = 1, · · · , T and using Lemma 9 and Lemma 10, we

39

have

T
∑
k=1

(cid:16)

(cid:107)ηF(zk) − ηF(wk)(cid:107)2 + η2rtan(zk)2(cid:17)

≤ 4

T−1
∑
k=0

(cid:107)zk − wk+1(cid:107)2 + (4 + 3η2L2)η2L2

(cid:107)w0 − z0(cid:107)2 +

(cid:18)

4 +

≤

≤

=

2(4 + 3η2L2)η2L2
1 − 2η2L2
2(4 + 3η2L2)η2L2
1 − 2η2L2
(cid:18) 4 − 8η2L2
1 − 4η2L2
16η2L2 + 6η4L4
1 − 4η2L2

+

+

(cid:107)w0 − z0(cid:107)2 +

2(4 + 3η2L2)η2L2
1 − 4η2L2

(cid:18) 8η2L2

+

1 − 4η2L2
(cid:19)

(cid:107)z0 − z∗(cid:107)2

(cid:107)w0 − z0(cid:107)2 +

4 + 6η4L4
1 − 4η2L2

(cid:107)z0 − z∗(cid:107)2,

(cid:107)wk − wk+1(cid:107)2

T−1
∑
k=0
2(4 + 3η2L2)η2L2
1 − 2η2L2

(cid:107)zk − wk+1(cid:107)2

(cid:19) T−1
∑
k=0
4(4 + 3η2L2)η4L4
(1 − 2η2L2) · (1 − 4η2L2)

(cid:19)

(cid:107)w0 − z0(cid:107)2

which concludes the proof.

(cid:4)

Corollary 2. Let Z ⊆ Rn be a closed convex set, F : Z → R be a monotone and L-Lipschitz operator,
and z∗ be a saddle point. Let z0, w0 ∈ Z be arbitrary starting point and {zk, wk}k≥0 be the iterates of the
OGDA algorithm with any step size η ∈ (0, 1

2L ). Then for all T ≥ 1, there exists t∗ ∈ [T] such that

(cid:107)ηF(zt∗ ) − ηF(wt∗ )(cid:107)2 + η2rtan(zt∗ )2 ≤

1
T

4 + 6η4L4
1 − 4η2L2

(cid:107)z0 − z∗(cid:107)2 +

1
T

16η2L2 + 6η4L4
1 − 4η2L2

(cid:107)w0 − z0(cid:107)2.

Moreover, when w0 = z0

(cid:107)ηF(zt∗ ) − ηF(wt∗ )(cid:107)2 + η2rtan(zt∗ )2 ≤

1
T

4 + 6η4L4
1 − 4η2L2

(cid:107)z0 − z∗(cid:107)2.

E.3 Monotonicity of the Potential

In this section we show that the potential function Φ
k is monotonically decreasing across iterates
of OGDA. We only include the simpliﬁed proof discovered using a degree 2 SOS program. The
original proof is based on a higher degree SOS program and can be found in an earlier version of
this paper [Cai et al., 2022] and at this link.

Theorem 7. Let Z ⊆ Rn be a closed convex set and F : Z → R be a monotone and L-Lipschitz operator.
Then for any zk, wk ∈ Z, the OGDA algorithm with any step size η ∈ (0, 1
2L ) produces wk+1, zk+1 ∈ Z
that satisfy (cid:107)F(zk) − F(wk)(cid:107)2 + rtan(zk)2 ≥ (cid:107)F(zk+1) − F(wk+1)(cid:107)2 + rtan(zk+1)2.

40

Proof. Let ck = Π

NZ (zk)(−F(zk)) and ck+1 = Π
η2rtan(zk)2 + η2(cid:107)F(zk) − F(wk)(cid:107)2 −
= (cid:107)ηF(zk) + ηck(cid:107)2 + η2(cid:107)F(zk) − F(wk)(cid:107)2

NZ (zk+1)(−F(zk+1)). Lemma 6 implies that
η2rtan(zk+1)2 + (cid:107)F(zk+1) − F(wk+1)(cid:107)2(cid:17)
(cid:16)

(cid:16)

(cid:107)ηF(zk+1) + ηck+1(cid:107)2 + (cid:107)F(zk+1) − F(wk+1)(cid:107)2(cid:17)

−

Since F is monotone and L-Lipschitz, and η ∈ (0, 1

2L ), we have

(−2) · ((cid:104)ηF(zk+1) − ηF(zk), zk+1 − zk(cid:105)) ≤ 0,

(cid:107)zk+1 − wk+1(cid:107)2 − (cid:107)ηF(zk+1) − ηF(wk+1)(cid:107)2

≤ 0.

(cid:19)

(−2) ·

(cid:18) 1
4

(47)

(48)

(49)

(cid:104)

(cid:105)

(cid:104)

(cid:105)

Since wk+1 = ΠZ
wk+1 ∈ N(wk+1) and zk − ηF(wk+1) − zk+1 ∈ N(zk+1). Thus we have

and zk+1 = ΠZ

zk − ηF(wk+1)

zk − ηF(wk)

, we have that zk − ηF(wk) −

(−1) · (cid:104)zk − ηF(wk) − wk+1, wk+1 − zk+1(cid:105) ≤ 0,
(−2) · (cid:104)zk − ηF(wk+1) − zk+1, zk+1 − zk(cid:105) ≤ 0.

Since c(zk) ∈ N(zk), we have that

(−1) · (cid:104)ηc(zk), zk − wk+1(cid:105) ≤ 0,
(−1) · (cid:104)ηc(zk), zk − zk+1(cid:105) ≤ 0.

(50)

(51)

(52)

(53)

According to Lemma 6 and the fact
Π

N(zk+1)(−F(zk+1)) we have

that zk − ηF(wk+1) − zk+1 ∈ N(zk+1),

ck+1 ∈

(−2) · (cid:104)ηc(zk+1) + ηF(zk+1), zk − ηF(wk+1) − zk+1(cid:105) ≤ 0,
(−2) · (cid:104)ηc(zk+1) + ηF(zk+1), −c(zk+1)(cid:105) = 0, .

(54)

(55)

MATLAB code for the veriﬁcation of the following identity can be found at this link.

Expression (47) + LHS of Inequality (48) + LHS of Inequality (49) + LHS of Inequality (50)

+ LHS of Inequality (51) + LHS of Inequality (52) + LHS of Inequality (53)
+ LHS of Inequality (55) + LHS of Inequality (54)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

wk+1 − zk+1
2

ηF(zk) + ηc(zk) − zk +

wk+1 + zk+1
2

+ ηF(wk) − ηF(zk)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

2

=

+

+ (cid:107)zk − ηF(wk+1) − zk+1 − ηc(zk+1)(cid:107)2
≥ 0.

Thus, (cid:107)F(zk) − F(wk)(cid:107)2 + rtan(zk)2 ≥ (cid:107)F(zk+1) − F(wk+1)(cid:107)2 + rtan(zk+1)2.

41

(56)

(57)

(58)

E.4 Combining Everything
In this section, we combine the results of the previous sections and show that ΦT = O (cid:0) 1
show the last-iterate convergence rate for performance measures of iterest.

T

(cid:1) and we

Lemma 12. Let Z ⊆ Rn be a closed convex set and F : Z → R be a monotone and L-Lipschitz operator.
Let z0, w0 ∈ Z be arbitrary starting point and {zk, wk}k≥0 be the iterates of the OGDA algorithm with any
step size η ∈ (0, 1

2L ). Then for any k ≥ 0,

rtan
(F,Z )(wk+1) ≤

√

(cid:113)

2(2 + ηL)

(F,Z )(zk)2 + (cid:107)F(wk) − F(zk)(cid:107)2.
rtan

Proof. Since wk+1 = ΠZ [zk − F(wk)], by using Lemma 8 we have

rtan
(F,Z )(wk+1) ≤

≤

≤

(cid:13)
zk − wk+1
(cid:13)
(cid:13)
(cid:13)
η
(cid:13)
zk − wk+1
(cid:13)
(cid:13)
(cid:13)
η
1 + ηL
η

(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ F(wk+1) − F(wk)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ (cid:107)F(wk) − F(zk)(cid:107) + (cid:107)F(zk) − F(wk+1)(cid:107)

(cid:107)zk − wk+1(cid:107) + (cid:107)F(wk) − F(zk)(cid:107).

(L-Lipschitzness of F)

Using Lemma 1 and the non-expansiveness of the projection mapping, we have

(cid:107)zk − wk+1(cid:107) ≤ (cid:107)zk − ΠZ [zk − ηF(zk)](cid:107) + (cid:107)ΠZ [zk − ηF(zk)] − wk+1(cid:107)
(ηF,Z )(zk) + (cid:107)ΠZ [zk − ηF(zk)] − ΠZ [zk − ηF(wk)](cid:107)
(ηF,Z )(zk) + η(cid:107)F(zk) − F(wk)(cid:107)
(F,Z )(zk) + η(cid:107)F(zk) − F(wk)(cid:107).

= rnat
≤ rtan
= ηrtan

Combing the above two inequalities, we have

(F,Z )(wk+1) ≤ (1 + ηL)rtan
rtan

√

≤

2(2 + ηL)

(F,Z )(zk) + (2 + ηL)(cid:107)F(wk) − F(zk)(cid:107)
(F,Z )(zk)2 + (cid:107)F(wk) − F(zk)(cid:107)2.
rtan

(cid:113)

(a + b ≤

√

√

2

a2 + b2)

Combining Corollary 2, Theorem 7, Lemma 12, Lemma 1 and Lemma 2 we get O( 1√
T

) last-
iterate convergence in terms of the tangent residual, natural residual and gap function for both zT
and wT+1. The result is formally stated in Theorem 8.

Theorem 8. Let Z ⊆ Rn be a closed convex set and F : Z → R be a monotone and L-Lipschitz oper-
ator. Let z0, w0 ∈ Z be arbitrary starting point and {zk, wk}k≥0 be the iterates of the OGDA algorithm
(4 + 6η4L4)(cid:107)z0 − z∗(cid:107)2 + (16η2L2 + 6η4L4)(cid:107)w0 − z0(cid:107)2 =
with any step size η ∈ (0, 1
O(max{(cid:107)z0 − z∗(cid:107), (cid:107)w0 − z0(cid:107)}). Then for any T ≥ 1,

2L ). Let D0 :=

(cid:113)

42

• GAPZ,F,D(zT) ≤ 1√
T

·

√

DD0
1−4·(ηL)2

.

η

• rnat

Z,F,D(zT) ≤ rtan

Z,F,D(zT) ≤ 1√
T

√

·

η·

D0

1−4·(ηL)2)

.

• GAPZ,F,D(wT+1) ≤ 1√
T

·

√

2(2+ηL)·D·D0
√
1−4·(ηL)2
η·

.

• rnat

Z,F,D(wT+1) ≤ rtan

Z,F,D(wT+1) ≤ 1√
T

√
2(2+ηL)D0
√
1−4·(ηL)2

η·

.

·

F Agnostic to Lemma 6 Proof for Monotonicity of Tangent Residual of

EG

Let a = max a∈ (cid:98)NZ (z),
(cid:104)F(z),a(cid:105)≤0

(cid:104)a, F(z). By Deﬁnition 3, a natural formulation of the squared tangent residual

is rtan(z)2 = (cid:107)F(z)(cid:107)2 − (cid:104)a, F(z)(cid:105)2, which is a degree-4 formulation of the tangent residual with
respect to our {a, F(z)}. In this section, we show how to prove monotonicity of tangent residual
of EG with arbitrary convex constraints, while being agnostic to the degree two formulation of
tangent residual as shown in Lemma 6 with the use of a higher-degree SOS program.

2

Reducing the Number of Constraints. Our reasoning behind reducing the number of con-
straints follows similar to the corresponding paragraph in Section 5.2 with minor modiﬁcations
that we list here for completeness. Suppose we are not given the description of Z, and we only
observe one iteration of the EG algorithm. In other words, we know zk, zk+ 1
, and zk+1, as well
as F(zk), F(zk+ 1
), and F(zk+1). To compute the squared tangent residual at zk, let us also as-
(F,Z )(zk)2 = (cid:107)F(zk)(cid:107)2 − (cid:104)F(zk), −ak(cid:105)2. From this
sume that the unit vector −ak ∈ (cid:98)N(zk) satisﬁes rtan
limited information, what can we learn about Z? We can conclude that Z must lie in the inter-
section of the following halfspaces: (a) (cid:104)ak, z(cid:105) ≥ bk, where bk = (cid:104)ak, zk(cid:105). This is true because
−ak ∈ (cid:98)N(zk). (b) (cid:104)ak+ 1
is true because zk+ 1

=
= ΠZ (zk − ηF(zk)), so (cid:104)zk+ 1
)
)(cid:107) and bk+1 = (cid:104)ak+1, zk+1(cid:105). This is true because

(cid:107)z
− zk + ηF(zk), z − zk+ 1

−zk+ηF(zk)
−zk+ηF(zk)(cid:107) and bk+ 1

(cid:105) ≥ 0 for all z ∈ Z. (c)

, where ak+ 1

, z(cid:105) ≥ bk+ 1

zk+1−zk+ηF(z
(cid:107)zk+1−zk+ηF(z

= (cid:104)ak+ 1

(cid:105). This

, zk+ 1

k+ 1
2
k+ 1
2

z

2

2

2

2

2

2

2

2

2

2

k+ 1
2
k+ 1
2

(cid:104)ak+1, z(cid:105) ≥ bk+1, where ak+1 =
zk+1 = ΠZ (zk − ηF(zk+ 1

2

)), so (cid:104)zk+1 − zk + ηF(zk+ 1

), z − zk+1(cid:105) ≥ 0 for all z ∈ Z.

2

The “hardest instance” of Z that is consistent with our knowledge of zk, zk+ 1

, and zk+1 is
when Z is exactly the intersection of these three halfspaces. In such case, the squared tangent
residual of zk+1 is (cid:107)F(zk+1)(cid:107)2 − (cid:104)F(zk+1), ak+1(cid:105)2 · 1[(cid:104)F(zk+1), ak+1(cid:105) ≥ 0], and it is an upper bound of
rtan
(F,Z )(zk+1)2 for any other consistent Z. Our goal is to prove the tangent residual is non-increasing
even in the ”hardest case”, that is, to prove the non-negativity of

2

(cid:107)F(zk)(cid:107)2 − (cid:104)F(zk), ak(cid:105)2 −

(cid:16)

(cid:107)F(zk+1)(cid:107)2 − (cid:104)F(zk+1), ak+1(cid:105)2 · 1[(cid:104)F(zk+1), ak+1(cid:105) ≥ 0]

(cid:17)

(59)

43

2 , k + 1} involved, we can choose a new basis, so that ak+1 = (1, 0, . . . , 0), ak+ 1

Low-dimensionality of an EG Update. As there are only three hyperplanes (cid:104)ai, z(cid:105) ≥ bi for
i ∈ {k, k + 1
=
− zk + ηF(zk) are co-directed, and
(θ1, θ2, 0, . . . , 0), and ak = (σ1, σ2, σ3, 0, . . . , 0). As ak+ 1
ak+1 and zk+1 − zk + ηF(zk+ 1
) are co-directed, an important property of this change of basis is that the
EG update from zk to zk+1 is unconstrained in all coordinates (cid:96) ≥ 4. More speciﬁcally,

and zk+ 1

2

2

2

2

zk+ 1

2

[(cid:96)] − zk[(cid:96)] + ηF(zk)[(cid:96)] = 0,

zk+1[(cid:96)] − zk[(cid:96)] + ηF(zk+ 1

2

)[(cid:96)] = 0, ∀(cid:96) ≥ 4.

Hence, we can represent all of the coordinates (cid:96) ≥ 4 with one coordinate in the SOS program
similar to the unconstrained case. We still need to keep the ﬁrst three dimensions, but now we
only face a problem in dimension 4 rather than in dimension n, and we can form a constant size
SOS program to search for a certiﬁcate of non-negativity for Expression (59).

In Lemma 13, we further simplify the instance that we need to consider.

In particular, we
argue that it is w.l.o.g. to assume that (1) ak+1, ak+ 1
, and ak are linear independent and (2) the
= bk+1 = 0. Both assumption (1)
intersection of the three halfspaces forms a cone, i.e., bk = bk+ 1
and (2) reduce the number of variables we need to consider in the SOS program, so a low degree
SOS proof is more likely to exist. To maximally reduce the number of variables, we only included
the minimal number of constraints that sufﬁce to derive an SOS proof.

2

2

Lemma 13 (Simpliﬁcation Procedure). Let I be a variational inequality problem for a closed convex set
Z ⊆ Rn and a monotone and L-Lipschitz operator F : Z → Rn. Suppose the EG algorithm has a constant
2 )-th iteration as deﬁned
step size η. Let zk be the k-th iteration of the EG algorithm, zk+ 1
in (4), and zk+1 be the (k + 1)-th iteration as deﬁned in (5).

be the (k + 1

2

Then either rtan

(F,Z )(zk) ≥ rtan

(F,Z )(zk+1), or there exist vectors ak, ak+ 1
), F(zk+1) ∈ RN with N ≤ n + 5 that satisfy the following conditions.

2

F(zk+ 1

2

, ak+1,zk,zk+ 1

2

,zk+1, F(zk),

1. ak = (β1, β2, 1, 0, . . . , 0), ak+ 1

2

= (α, 1, 0, . . . , 0), and ak+1 = (1, 0, . . . , 0) for some α, β1, β2 ∈ R.

2. (cid:107)F(zk) − (cid:104)F(zk),ak(cid:105)·ak

(cid:107)ak(cid:107)2

2

(cid:107)

− (cid:107)F(zk+1) − (cid:104)F(zk+1),ak+1(cid:105)·ak+1

(cid:107)ak+1(cid:107)2

1[(cid:104)F(zk+1), ak+1(cid:105) ≥ 0](cid:107)

2

< 0.

3. Additionally, (cid:104)ai, zj(cid:105) ≥ 0 and (cid:104)ai, zi(cid:105) = 0 for all i, j ∈ {k, k + 1

− zk +
ηF(zk) are co-directed, i.e., they are colinear and have the same direction, and ak+1 and zk+1 − zk +
ηF(zk+ 1

2 , k + 1}. ak+ 1

) are co-directed.

and zk+ 1

2

2

2

4.

2

(cid:13)
(cid:13)
(cid:13)F(zk+1) − F(zk+ 1

≤L2(cid:13)
(cid:13)
(cid:13)
(cid:13)
)
(cid:13)zk+1 − zk+ 1
(cid:13)
(cid:10)F(zk+1) − F(zk), zk+1 − zk
(cid:11) ≥0
(cid:10)ak, F(zk)(cid:11) ≥0.

2

2

2

(cid:13)
(cid:13)
(cid:13)

(60)

(61)

(62)

Proof of Lemma 13:
to work with terms
rtan(zk)2, rtan(zk+1)2 rather than rtan(zk), rtan(zk+1). Since rtan(zk), rtan(zk+1) ≥ 0, then rtan(zk) −

it will be more convenient

For our proof,

44

rtan(zk+1) ≥ 0 iff rtan(zk)2 − rtan(zk+1)2 ≥ 0. For the rest of the proof, we refer to the property that
ak = (β1, β2, 1, 0, . . . , 0), ak+ 1

= (α, 1, 0, . . . , 0), ak+1 = (1, 0, . . . , 0) as the form property.

2

= ΠZ [zk − ηF(zk)] and zk+1 =

Recall that the k-th update of EG is as follows zk+ 1
(cid:104)
zk − ηF(zk+ 1

. We deﬁne the following vectors:

(cid:105)

)

2

ΠZ

2

−ak ∈ argmin
a∈ (cid:98)NZ (zk),
(cid:104)F(zk),a(cid:105)≤0

(cid:107)F(zk) − (cid:104)F(zk), a(cid:105) · a(cid:107)2,

2

= zk+ 1

− zk + ηF(zk),
ak+ 1
ak+1 = zk+1 − zk + ηF(zk+ 1

2

2

).

(63)

(64)

(65)

, and ak+1 satisfy (i) the form property, and (ii) (cid:104)ak, zk(cid:105) =
(cid:105) = (cid:104)ak+1, zk+1(cid:105) = 0. We use this simple case as the basis of our construction, and will

2

, zk+ 1

For now, let us assume that ak, ak+ 1
(cid:104)ak+ 1
remove these assumptions later.
= ak+ 1

2

2

We set ak = ak, ak+ 1
) = F(zk+ 1

, zk+1 = zk+1, F(zk) = F(zk),
) and F(zk+1) = F(zk+1). We ﬁrst argue that Property 4 holds. Since F(·) is
F(zk+ 1
monotone and L-Lipschitz, Inequality (60) and (61) are satisﬁed. In addition, Inequality (62) is
satisﬁed due to the deﬁnition of ak.

, ak+1 = ak+1, zk = zk, zk+ 1

= zk+ 1

2

2

2

2

2

2

Next, we show that property 3 holds. By the deﬁnition of ak+ 1
− zk + ηF(zk) (or zk+1 − zk + ηF(zk+ 1

ak+1) and zk+ 1

2

2

2

)) are co-directed. As −ak ∈ (cid:98)NZ (zk),

(or ak+1), it is clear that ak+ 1

2

(or

(cid:104)ak, z(cid:105) ≥(cid:104)ak, zk(cid:105) = 0,

z ∈ {zk, zk+ 1

2

, zk+1}.

(66)

According to the update rule of the EG algorithm (Equation (4) and (5)) , Equation (64), and Equa-
tion (65), we know that for all z ∈ Z,

(cid:68)

ak+ 1

2

, z − zk+ 1

2

(cid:69)

(cid:68)

zk+ 1

2

− zk + ηF(zk), z − zk+ 1

2

(cid:69)

≥ 0,

(cid:104)ak+1, z − zk+1(cid:105) =

zk+1 − zk + ηF(zk+ 1

2

), z − zk+1

(cid:69)

≥ 0,

=
(cid:68)

(67)

(68)

which implies that for any i ∈ {k + 1

2 , k + 1}, j ∈ {k, k + 1
Finally, we verify Property 2. By Equation (68), − ak+1

2 , k + 1}, (cid:104)ai, zj(cid:105) ≥ (cid:104)ai, zi(cid:105) = 0.

(cid:107)ak+1(cid:107) ∈ (cid:98)N(zk+1), which in combination with

Lemma 7 implies

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F(zk+1) −

(cid:104)F(zk+1), ak+1(cid:105) · ak+1
(cid:107)ak+1(cid:107)2

1[(cid:10)F(zk+1), ak+1

(cid:11) ≥ 0]

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:107)F(zk+1) − (cid:104)F(zk+1), a(cid:105) · a(cid:107)2

≥ min

a∈ (cid:98)NZ (zk+1),
(cid:104)F(zk+1),a(cid:105)≤0
=rtan(zk+1)2.

45

According to Lemma 7 and Equation (63), we know that rtan(zk)2 = (cid:107)F(zk) − (cid:104)F(zk), ak(cid:105) · ak(cid:107)2 =
(cid:107)F(zk) − (cid:104)F(zk),ak(cid:105)·ak

. If rtan(zk)2 − rtan(zk+1)2 < 0, then

(cid:107)

2

(cid:107)ak(cid:107)2

0 > rtan(zk)2 − rtan(zk+1)2

≥

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F(zk) −

(cid:104)F(zk), ak(cid:105) · ak
(cid:107)ak(cid:107)2

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F(zk+1) −

(cid:104)F(zk+1), ak+1(cid:105) · ak+1
(cid:107)ak+1(cid:107)2

1[(cid:10)F(zk+1), ak+1

(cid:11) ≥ 0]

2

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(69)

Observe that when vectors ak and ak+1 satisfy the form property, then (cid:107)ak(cid:107) = 1, and (cid:107)ak+1(cid:107) ≥ 1
and Equation (69) is well-deﬁned.

2

, zk+ 1

This completes the proof for the case, where (i) vectors ak, ak+ 1

erty, and (ii) (cid:104)ak, zk(cid:105) = (cid:104)ak+ 1
tions. We ﬁrst show how to modify the construction so that for any ak, ak+ 1
, ˆzk+1, (cid:98)F( ˆzk), (cid:98)F( ˆzk+ 1
vectors ˆak, ˆak+ 1
erties, (a)(cid:104) ˆak, ˆzk(cid:105) = (cid:104) ˆak+ 1
pendent. In our ﬁnal step, we choose a proper basis to construct vectors ak, ak+ 1
F(zk),F(zk+ 1

, and ak+1 satisfy the form prop-
(cid:105) = (cid:104)ak+1, zk+1(cid:105) = 0. Our next step is to remove the assump-
, ak+1, we can construct
), (cid:98)F( ˆzk+1) ∈ Rn+5, that satisfy, among other prop-
, ˆak+1 are linear inde-
,zk+1,
, ak+1,zk,zk+ 1
), F(zk+1) ∈ Rn+5 that satisfy all four properties in the statement of Lemma 13.

(cid:105) = (cid:104) ˆak+1, ˆzk+1(cid:105) = 0, and (b) vectors ˆak, ˆak+ 1

, ˆak+1, ˆzk, ˆzk+ 1

, ˆzk+ 1

2

2

2

2

2

2

2

2

2

2

2

2

We now present the construction of vectors ˆak, ˆak+ 1

), (cid:98)F( ˆzk+1).
High-levelly speaking, we introduce ﬁve dummy dimensions. The purpose of the ﬁrst dummy
dimension is to ensure property (a). We use the remaining four dummy dimensions to ensure that
the newly created vectors ˆak, ˆak+ 1
and ˆak+1 are linearly independent satisfying property (b). More
speciﬁcally for parameters (cid:96), (cid:101) > 0 that we determine later, we deﬁne ˆzk, ˆzk+ 1
),
(cid:98)F( ˆzk+1), ˆak, ˆak+ 1

, ˆzk+1, (cid:98)F( ˆzk), (cid:98)F( ˆzk+ 1

, ˆzk+1, (cid:98)F( ˆzk), (cid:98)F( ˆzk+ 1

, ˆak+1 as follows

, ˆak+1, ˆzk, ˆzk+ 1

2

2

2

2

2

2

2

∀i ∈ {k, k +

1
2

, k + 1}

ˆzi := (−(cid:101)−1, 0, 0, 0, 0, zi)
(cid:101)
η

(cid:98)F( ˆzk) := (

ak+ 1

, zk+ 1

(cid:68)

·

2

2

(cid:69)

, 0,

(cid:101)
η

, 0,

(cid:98)F( ˆzk+ 1

2

) := (

(cid:98)F( ˆzk+1) := (

(cid:101)
η
(cid:101)
η

· (cid:104)ak+1, zk+1(cid:105), 0, 0,

· (cid:104)ak+1, zk+1(cid:105), 0, 0,

(cid:101)
η
(cid:101)
η

(cid:96)(cid:101)
η
(cid:96)(cid:101)
η
(cid:96)(cid:101)
η

,

,

, F(zk))

, F(zk+ 1

2

))

, F(zk+1))

ˆak := ((cid:101) · (cid:104)ak, zk(cid:105), (cid:101), 0, 0, (cid:96)(cid:101), ak)

(cid:68)

(cid:69)

2

:= ((cid:101) ·

, 0, (cid:101), 0, (cid:96)(cid:101), ak+ 1
, zk+ 1
ˆak+ 1
ˆak+1 := ((cid:101) · (cid:104)ak+1, zk+1(cid:105), 0, 0, (cid:101), (cid:96)(cid:101), ak+1)

ak+ 1

2

2

2

)

(70)

(71)

(72)

(73)

(74)

(75)

(76)

Clearly, ˆak, ˆak+ 1
and ˆzk+ 1

and ˆak+ 1

2

2

2

, ˆak+1 are linear independent, ˆak+1 and ˆzk+1 − ˆzk + η (cid:98)F( ˆzk+ 1
) are co-directed,
− ˆzk + η (cid:98)F( ˆzk) are co-directed. Note that the following inequalites hold. By

2

46

Equation (63)-(65), it is clear that −ak ∈ (cid:98)N(zk) ⊆ N(zk), −ak+ 1
which further implies

2

∈ N(zk+ 1

2

) and −ak+1 ∈ N(zk+1),

(cid:104) ˆai, ˆzj(cid:105) = (cid:104)ai, zj(cid:105) − (cid:104)ai, zi(cid:105) ≥ 0,

(cid:104) ˆai, ˆzi(cid:105) = 0,
(cid:13)
(cid:13)
(cid:13) (cid:98)F( ˆzk+1) − (cid:98)F( ˆzk+ 1
(cid:68)
(cid:98)F( ˆzk+1) − (cid:98)F( ˆzk), ˆzk+1 − ˆzk

(cid:13)
(cid:13)
(cid:13)

=

)

2

2

, k + 1}

∀i ∈ {k, k +

∀i, j ∈ {k, k +

1
2
1
, k + 1}
2
≤ L2(cid:13)
(cid:13)
(cid:13)zk+1 − zk+ 1
= (cid:104)F(zk+1) − F(zk), zk+1 − zk(cid:105) ≥ 0
(cid:16)

(cid:13)
(cid:13)
(cid:13)F(zk+1) − F(zk+ 1

(cid:13)
(cid:13)
(cid:13)

(cid:69)

)

2

2

2

2

(cid:13)
(cid:13)
(cid:13)

=

(cid:13)
(cid:13)
(cid:13) ˆzk+1 − ˆzk+ 1

2

(cid:13)
(cid:13)
(cid:13)

2

(77)

(78)

(79)

(80)

Moreover, (cid:104) ˆak, (cid:98)F( ˆzk)(cid:105) = (cid:101)2
η ·
ciently large so that (cid:104)ak, zk(cid:105)(cid:104)ak+ 1

2

(cid:105) + (cid:96)2(cid:17)
(cid:105) + (cid:96)2 ≥ 0. Hence, for our choice of (cid:96),

, zk+ 1

2

2

+ (cid:104)ak, F(zk)(cid:105). We choose (cid:96) to be sufﬁ-

(cid:104)ak, zk(cid:105)(cid:104)ak+ 1
, zk+ 1
(cid:68)

(cid:69)

2

ˆak, (cid:98)F( ˆzk)

≥ (cid:104)ak, F(zk)(cid:105) ≥ 0.

(81)

We deﬁne function

(cid:98)H( ˆzk) :=

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:98)F( ˆzk) −

(cid:98)F( ˆzk) −

· ˆak

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:104) (cid:98)F( ˆzk), ˆak(cid:105)
(cid:107) ˆak(cid:107)2
(cid:16)

(cid:101)2
η ·

(cid:105) + (cid:96)2(cid:17)

(cid:104)ak, zk(cid:105)(cid:104)ak+ 1

, zk+ 1
(cid:101)2((cid:104)ak, zk(cid:105)2 + 1 + (cid:96)2) + (cid:107)ak(cid:107)2

2

2

+ (cid:104)ak, F(zk)(cid:105)

· ˆak

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

(cid:18)

(cid:101)2
η ·

Deﬁne f ((cid:101)) :=

(cid:19)

(cid:104)ak,zk(cid:105)(cid:104)a

,z

k+ 1
2

k+ 1
2
(cid:101)2((cid:104)ak,zk(cid:105)2+1+(cid:96)2)+(cid:107)ak(cid:107)2

(cid:105)+(cid:96)2

+(cid:104)ak,F(zk)(cid:105)

. We can simplify (cid:98)H( ˆzk) to be

(cid:101)2 ·

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:32) (cid:104)ak+ 1

2

(cid:105)

, zk+ 1
η

2

− f ((cid:101))(cid:104)ak, zk(cid:105), − f ((cid:101)),

1
η

, 0,

(cid:96)
η

− f ((cid:101))(cid:96)

2

(cid:33)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ (cid:107)F(zk) − f ((cid:101)) · ak(cid:107)2.

When ak = (0, . . . , 0), rtan(zk)2 = (cid:107)F(zk)(cid:107)2 (Equation (63)) and f ((cid:101)) =
0. Therefore, lim(cid:101)→0+ (cid:98)H( ˆzk) = (cid:107)F(zk)(cid:107)2 = rtan(zk)2, when ak = (0, . . . , 0). When ak
2
(Equation (63)) and lim(cid:101)→0+ f ((cid:101)) = (cid:104)ak,F(zk)(cid:105)
(0, . . . , 0), rtan(zk)2 = (cid:107)F(zk) − (cid:104)ak,F(zk)(cid:105)
(cid:107)ak(cid:107)2
(cid:107)ak(cid:107)2
lim(cid:101)→0+ (cid:98)H( ˆzk) = (cid:107)F(zk) − (cid:104)ak,F(zk)(cid:105)
· ak(cid:107)
(cid:107)ak(cid:107)2
Similarly, we deﬁne function

η(1+(cid:96)2) for any (cid:101) >
(cid:54)=

= rtan(zk)2.

· ak(cid:107)

, so

2

(cid:96)2

(cid:98)H( ˆzk+1) :=

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:98)F( ˆzk+1) −

(cid:98)F( ˆzk+1) −

1

(cid:69)

(cid:104)(cid:68)

(cid:98)F( ˆzk+1), ˆak+1

(cid:104) (cid:98)F( ˆzk+1), ˆak+1(cid:105) · ˆak+1
(cid:107) ˆak+1(cid:107)2
(cid:104)ak+1, zk+1(cid:105)2 + 1 + (cid:96)2(cid:17)
(cid:101)2((cid:104)ak+1, zk+1(cid:105)2 + 1 + (cid:96)2) + (cid:107)ak+1(cid:107)2

(cid:101)2
η ·

(cid:16)

+ (cid:104)F(zk+1), ak+1(cid:105)

≥ 0

(cid:105)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:104)(cid:68)

· ˆak+1

1

(cid:98)F( ˆzk+1), ˆak+1

(cid:69)

(cid:105)

≥ 0

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

47

Deﬁne g((cid:101)) :=

(cid:101)2
η ·((cid:104)ak+1,zk+1(cid:105)2+1+(cid:96)2)+(cid:104)F(zk+1),ak+1(cid:105)
(cid:101)2((cid:104)ak+1,zk+1(cid:105)2+1+(cid:96)2)+(cid:107)ak+1(cid:107)2

, and we can simplify (cid:98)H( ˆzk+1) to be

(cid:101)2

(cid:18) 1
η

(cid:104)(cid:68)

− g((cid:101))1

(cid:98)F( ˆzk+1), ˆak+1

(cid:105)(cid:19)2

(cid:69)

≥ 0

· (cid:107)((cid:104)ak+1, zk+1(cid:105), 0, 0, 1, (cid:96))(cid:107)2

+

(cid:13)
(cid:13)
(cid:13)F(zk+1) − g((cid:101)) · ak+1

1

(cid:104)(cid:68)

(cid:98)F( ˆzk+1), ˆak+1

(cid:69)

≥ 0

2

(cid:105)(cid:13)
(cid:13)
(cid:13)

.

When ak+1 = (0, . . . , 0), we have g((cid:101)) = 1

(cid:107)F(zk+1)(cid:107)2 ≥ rtan(zk+1)2. When ak+1 (cid:54)= (0, . . . , 0), we have lim(cid:101)→0+ g((cid:101)) = (cid:104)F(zk+1),ak+1(cid:105)
lim(cid:101)→0+ 1

= 1 [(cid:104)F(zk+1), ak+1(cid:105) ≥ 0],15 hence

(cid:104) (cid:98)F( ˆzk+1), ˆak+1(cid:105) ≥ 0

(cid:107)ak+1(cid:107)2

(cid:105)

(cid:104)

η and (cid:104) (cid:98)F( ˆzk+1), ˆak+1(cid:105) ≥ 0, so (cid:98)H( ˆzk+1) =
and

(cid:101)→0+ (cid:98)H( ˆzk+1) =
lim

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F(zk+1) −

(cid:104)F(zk+1), ak+1(cid:105)
(cid:107)ak+1(cid:107)2

· ak+1

1 [(cid:104)F(zk+1), ak+1(cid:105) ≥ 0]

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≥ rtan(zk+1)2.

The last inequality is because

−ak+1
(cid:107)ak+1(cid:107) ∈ (cid:98)NZ (zk+1) and Lemma 7.

If rtan(zk)2 − rtan(zk+1)2 < 0, then

(cid:16)

lim
(cid:101)→0+

(cid:98)H( ˆzk) − (cid:98)H( ˆzk+1)

(cid:17)

≤ rtan(zk)2 − rtan(zk+1)2 < 0.

2

), (cid:98)F( ˆzk+1).

Thus, we can choose a sufﬁciently small (cid:101) so that (cid:98)H( ˆzk) − (cid:98)H( ˆzk+1) < 0. Together with Inequal-
ities (77)-(81), we can satisfy all properties excluding the form property using ˆzk, ˆzk+ 1
, ˆzk+1, ˆak,
ˆak+ 1

, ˆak+1, (cid:98)F( ˆzk), (cid:98)F( ˆzk+ 1
2
Now we show how to make the vectors also satisfy the form property. We perform a change
and ˆak+1 only depend on the ﬁrst three coordinates. We use the
of basis so that vectors ˆak, ˆak+ 1
Gram–Schmidt process to generate a basis, where vectors ˆak, ˆak+ 1
, ˆak+1 all lie in the span of the
ﬁrst three vector of the new basis. More formally, let N = n + 5 and {bi}i∈[N] be a sequence of
orthonormal vectors produced by the Gram-Schmidt process on ordered input ˆak+1, ˆak+ 1
, ˆak and
{ei}i∈[N]. Let Q be the N × N matrix, where the i-th row of Q is vector bi. Observe that any vector
z ∈ RN written in the basis {ei}i∈[N] can be represented by the basis {bi}i∈[N] with coefﬁcients
Q · z.

2

2

2

2

, zk+1 = Q · ˆzk+1,
Let ak = Q · ˆak, ak+ 1
F(zk) = Q · (cid:98)F( ˆzk), F(zk+ 1
) and F(zk+1) = Q · (cid:98)F( ˆzk+1). Note that ak is the coefﬁcients
of ˆak written in the basis {bi}i∈[N] and similar reasoning holds for the rest of the deﬁned vectors.
Clearly, for any ˆz, ˆz(cid:48) ∈ RN, if we deﬁne z = Q · ˆz and z(cid:48) = Q · ˆz(cid:48), then we have

, ak+1 = Q · ˆak+1, zk = Q · ˆzk, zk+ 1

= Q · ˆak+ 1
) = Q · (cid:98)F( ˆzk+ 1

= Q · ˆzk+ 1

2

2

2

2

2

2

15This is because (cid:104) (cid:98)F( ˆzk+1), ˆak+1(cid:105) is never smaller than (cid:104)F(zk+1), ak+1(cid:105), and the function 1[x ≥ 0] is right continuous.

(cid:104)z, z(cid:48)(cid:105) = (cid:104) ˆz, ˆz(cid:48)(cid:105).

(82)

48

Combining Equation (82) and the fact that all properties but the form property hold for vectors ˆak,
) and (cid:98)F( ˆzk+1), we conclude that the same properties also
ˆak+ 1
, ˆak+1, ˆzk, ˆzk+ 1
, zk+1, F(zk), F(zk+ 1
hold for vectors ak, ak+ 1

, ˆzk+1, (cid:98)F( ˆzk), (cid:98)F( ˆzk+ 1
, ak+1, zk, zk+ 1

) and F(zk+1).

2

2

2

2

2

2

∈ Span(b1, b2), ˆak ∈ Span(b1, b2, b3), (cid:104) ˆak+1, b1(cid:105) > 0, (cid:104) ˆak+ 1

Finally, by properties of the Gram-Schmidt process, the order of the vector in its input,
and ˆak+1 are linearly independent, we have ˆak+1 ∈ Span(b1),
, b2(cid:105) > 0 and (cid:104) ˆak, b3(cid:105) > 0. Thus
= (α, c, 0, . . . , 0) and ak+1 = (d, 0, . . . , 0), where β1, β2, α ∈ R and
and ak+1, we can make them satisfy the form property.
(cid:4)

and the fact that vectors ˆak, ˆak+ 1
ˆak+ 1
ak = (β1, β2, b, 0, . . . , 0), ak+ 1
b, c, d > 0. By properly scaling ak, ak+ 1
This completes the proof.

2

2

2

2

2

In Theorem 9, we establish the monotonicity of the tangent residual. Our proof is based on the

solution to the degree-8 SOS program concerning polynomials in 27 variables.
Theorem 9. Let Z ⊆ Rn be a closed convex set and F : Z → Rn be a monotone and L-Lipschitz operator.
For any step size η ∈ (0, 1
L ) and any zk ∈ Z, the EG method update satisﬁes rtan
Proof of Theorem 9: Assume towards contradiction that rtan(zk+1) > rtan(zk), using Lemma 13
there exists numbers α, β1, β2 ∈ R and vectors ak, ak+ 1
),
F(zk+1) ∈ RN where N = n + 5 that satisfy the properties in the statement of Lemma 13 and

, zk+1, F(zk), F(zk+ 1

(F,Z )(zk) ≥ rtan

, ak+1, zk, zk+ 1

(F,Z )(zk+1).

2

2

2

0 >

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F(zk) −

(cid:104)F(zk), ak(cid:105)
(cid:107)ak(cid:107)2

ak

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F(zk+1) −

(cid:104)F(zk+1), ak+1(cid:105)ak+1
(cid:107)ak+1(cid:107)2

1[(cid:10)F(zk+1), ak+1

(cid:11) ≥ 0]

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= (cid:107)F(zk)(cid:107)2 − (cid:107)F(zk+1)(cid:107)2 −

(β1F(zk)[1] + β2F(zk)[2] + F(zk)[3])2
1 + β2
β2

2 + 1

+ F(zk+1)[1]21[F(zk+1)[1] ≥ 0],

where we use the fact that ak = (β1, β2, 1, 0, . . . , 0) and ak+1 = (1, 0, . . . , 0).

We use TARGET to denote

(cid:107)F(zk)(cid:107)2 − (cid:107)F(zk+1)(cid:107)2 −

(β1F(zk)[1] + β2F(zk)[2] + F(zk)[3])2
1 + β2
β2

2 + 1

+ F(zk+1)[1]21[F(zk+1)[1] ≥ 0],

and our goal is to show that TARGET is non-negative, and thus reach a contradiction.

Our plan is to show that we can obtain a sum of quotients of SOS polynomials by adding non-
positive terms to TARGET, which implies the non-negativity of TARGET. We add the non-positive
terms in a few steps.

Combining Lemma 13 and the fact that η > 0 and (ηL)2 ≤ 1, we derive the following two

inequalities:

(cid:10)ηF(zk+1) − ηF(zk), zk − zk+1
(cid:11) ≤ 0,
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)zk+1 − zk+ 1
(cid:13)

≤ 0.

(cid:13)
(cid:13)
(cid:13)

−

)

2

2

(cid:13)
(cid:13)
(cid:13)ηF(zk+1) − ηF(zk+ 1

2

2

(83)

(84)

Equipped with these two inequalities, it is clear that

η2 · TARGET ≥ η2 · TARGET + 2 · LHS of Inequality (83) + LHS of Inequality (84).

(85)

49

Therefore, it is sufﬁcient to show that the RHS of Inequality (85) is non-negative.

We take advantage of the sparsity of vectors ak, ak+ 1

, ak+1 by considering the following parti-
tion of [N]. We deﬁne P1 = {1, 2, 3} and P2 = {4, 5, · · · , N}. For any vector z ∈ RN and j ∈ {1, 2},
we deﬁne pj(z) ∈ Rn to be the vector such that pj(z)[i] = z[i] for i ∈ Pj and pj(z)[i] = 0 otherwise.
We divide the RHS of Inequality (85) using the partition to Expression (86) and Expression (87):

2

η2(cid:13)

2 − η2(cid:13)

(cid:13)p2(F(zk))(cid:13)
(cid:13)
+ η2(cid:13)
(cid:13)
(cid:13)p2(F(zk+1)) − p2(F(zk+ 1

(cid:13)p2(F(zk+1))(cid:13)
(cid:13)

))

2 + 2η(cid:10)p2(F(zk+1)) − p2(F(zk)), p2(zk) − p2(zk+1)(cid:11)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)p2(zk+1) − p2(zk+ 1

(cid:13)
(cid:13)
(cid:13)

−

)

2

2

,

2

2

(86)

and

η2(cid:13)

2 − η2(cid:13)

(cid:13)p1(F(zk))(cid:13)
(cid:13)
+ η2(cid:13)
(cid:13)
(cid:13)p1(F(zk+1)) − p1(F(zk+ 1

(cid:13)p1(F(zk+1))(cid:13)
(cid:13)

))

2 + 2η(cid:10)p1(F(zk+1)) − p1(F(zk)), p1(zk) − p1(zk+1)(cid:11)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)p1(zk+1) − p1(zk+ 1

(cid:13)
(cid:13)
(cid:13)

−

)

2

2

2

2

−

η2
1 + β2
β2

2 + 1

(cid:0)β1F(zk)[1] + β2F(zk)[2] + F(zk)[3](cid:1)2 + η2F(zk+1)[1]21[F(zk+1)[1] ≥ 0].

(87)

First we show that the Expression (86) is non-negative. According to Lemma 13, ak+ 1
− zk + ηF(zk) are co-directed, and ak+1 and zk+1 − zk + ηF(zk+ 1
[i] = ak+1[i] = 0 and thus we have

and
) are co-directed. For any

zk+ 1
i ∈ P2, we know ak+ 1

2

2

2

2

(cid:68)

(cid:69)

(cid:68)

(cid:69)

0 =

=

ei, ak+ 1

[i] = zk[i] − ηF(zk+1)[i],
0 = (cid:104)ei, ak+1(cid:105) = (cid:10)ei, zk+1 − zk + ηF(zk+1)(cid:11) ⇔ zk+1[i] = zk[i] − ηF(zk+1)[i],

− zk + ηF(zk+1)

⇔ zk+ 1

ei, zk+ 1

2

2

2

which implies that

p2(zk+ 1
) = p2(zk) − η · p2(F(zk)),
p2(zk+1) = p2(zk) − η · p2(F(zk+ 1

2

2

)).

Intuitively, one can think of the coordinates in P2 as the ones where the EG update is uncon-
strained. With the two new equalities, it is easy to verify that Expression (86) is always 0.

η2(cid:13)

2 − η2(cid:13)

(cid:13)p2(F(zk+1))(cid:13)
(cid:13)

(cid:13)p2(F(zk))(cid:13)
2 + 2η(cid:10)p2(F(zk+1)) − p2(F(zk)), p2(zk) − p2(zk+1)(cid:11)
(cid:13)
+ η2(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
))
(cid:13)p2(F(zk+1)) − p2(F(zk+ 1
−
(cid:13)p2(zk+1) − p2(zk+ 1
(cid:13)
2 + 2η2(cid:68)
(cid:13)p2(F(zk))(cid:13)
(cid:13)p2(F(zk+1))(cid:13)
(cid:13)
(cid:13)
− η2(cid:13)
(cid:13)
+ η2(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)p2(F(zk) − p2(F(zk+ 1
(cid:13)p2(F(zk+1)) − p2(F(zk+ 1
(cid:13)

(cid:13)
(cid:13)
(cid:13)
p2(F(zk+1)) − p2(F(zk)), p2(F(zk+ 1

2 − η2(cid:13)

(cid:13)
(cid:13)
(cid:13)

))

))

))

(cid:69)

)

2

2

2

2

2

2

2

2

2

=η2(cid:13)

=0.

50

We now turn our attention to Expression (87) and show that it is non-negative. The analysis
is more challenging here. We introduce the following six non-positive expressions, multiply each
of them with a carefully chosen coefﬁcient, then add them together with Expression (87). We
ﬁnally verify that the sum is a sum of quotients of SOS polynomials implying the non-negativity
of Expression (87). We believe it will be extremely challenging if not impossible for human beings
to discover these non-positive expressions and their associated coefﬁcients manually to complete
this proof. We instead harness the power of the SOS programming to overcome the difﬁculty and
make the discovery.

We ﬁrst present the six non-positive expressions.

(cid:16)(cid:16)

(cid:16)(cid:16)

zk+ 1

2

[1]

zk+1[2]

zk+ 1

2

− zk + ηF(zk)

zk+ 1

2

− zk + ηF(zk)

(cid:17)

[1] − α

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:17)

[2]

= 0,

(cid:17)

(cid:17)

2

zk+ 1

− zk + ηF(zk)

= 0,
(cid:0)α(cid:0)zk − ηF(zk)(cid:1)[1] + (cid:0)zk − ηF(zk)(cid:1)[2](cid:1)(αzk+1[1] + zk+1[2]) ≤ 0,
≤ 0,

− zk + ηF(zk)

[1] − α

zk+ 1

[2]

[3]

(cid:17)

2

−η(cid:0)β1F(zk)[1] + β2F(zk)[2] + F(zk)[3](cid:1)(cid:16)

β1zk+ 1

2

[1] + β2zk+ 1
(cid:16)

2

[2] + zk+ 1

2

zk[1]

zk[1] − ηF(zk+ 1

)[1]

2

(cid:17)

(cid:17)

≤ 0,

≤ 0.

−ηF(zk+1)[1]1

(cid:104)

F(zk+1)[1] ≤ 0

(cid:105)(cid:16)

zk[1] − ηF(zk+ 1

2

)[1]

(88)

(89)

(90)

(91)

(92)

(93)

Equation (88) and Equation (89)
(cid:105) = 0 and that ak+ 1
(cid:104)(1, −α, 0, · · · , 0), ak+ 1
2
(cid:68)

2

follow from the combination of
− zk + ηF(zk) are co-directed:
and zk+ 1

2

the fact

that

(1, −α, 0, · · · , 0), zk+ 1
(cid:17)

2

− zk + ηF(zk)

zk+ 1

2

− zk + ηF(zk)
(cid:16)

[1] − α

zk+ 1

2

(cid:16)

⇔

− zk + ηF(zk)

(cid:17)

[2] = 0.

(cid:69)

= 0

Note that the LHS of Inequality (90) is equal to (cid:104)ak+ 1

, zk − ηF(zk)(cid:105) · (cid:104)ak+ 1

2

guarantees that (cid:104)ak+ 1
(cid:104)ak+ 1

, zk+ 1

2

2

2

(cid:105) = 0, we have that

, zk+1(cid:105) ≥ 0. Since ak+ 1

and zk − ηF(zk) − zk+ 1

2

2

, zk+1(cid:105). Lemma 13
are oppositely directed, and

2

(cid:68)

0 ≥

ak+ 1

2

, zk − ηF(zk) − zk+ 1

2

(cid:69)

(cid:68)

=

ak+ 1

2

, zk − ηF(zk)

(cid:69)

.

Hence, (cid:104)ak+ 1

, zk − ηF(zk)(cid:105) · (cid:104)ak+ 1

, zk+1(cid:105) ≤ 0.

2

2

Observe that the LHS of Inequality (91) is equal to −η(cid:104)ak, F(zk)(cid:105) · (cid:104)ak, zk+ 1
(cid:105) ≥ 0.

antees that (cid:104)ak, F(zk)(cid:105) ≥ 0 and (cid:104)ak, zk+ 1

2

2

(cid:105). Lemma 13 guar-

Finally, we argue Inequality (92) and Inequality (93). Note that the LHS of Inequality (92)
) − zk+1 are oppositely

)(cid:105). Since ak+1 and zk − ηF(zk+ 1

2

is equal to (cid:104)ak+1, zk(cid:105) · (cid:104)ak+1, zk − ηF(zk+ 1
directed, and (cid:104)ak+1, zk+1(cid:105) = 0, we have that

2

(cid:68)

0 ≥

ak+1, zk − ηF(zk+ 1

2

) − zk+1

(cid:69)

(cid:68)

=

ak+1, zk − ηF(zk+ 1

2

(cid:69)

.

)

51

Clearly, −F(zk+1)1[F(zk+1)[1] ≤ 0] is non-negative and zk[1] = (cid:104)ak+1, zk(cid:105) is also non-negative due
to Lemma 13. Thus Inequality (92) and Inequality (93) hold.

Our next step is to show that the following is non-negative.

Expression (87) + 2 × (LHS of Equation (88) + LHS of Inequality (92) + LHS of Inequality (93))

+

2α
1 + α2

× LHS of Equation (89) +

2
1 + α2

× LHS of Inequality (90) +

2
1 + β2
1 + β2
2

× LHS of Inequality (91)

We ﬁrst simplify Expression (94), using the following relationship between the variables.

zk[3] = −β1zk[1] − β2zk[2],

2

2

2

[1],

[2] = −αzk+ 1
[3] = zk[3] − ηF(zk)[3],

zk+ 1
zk+ 1
zk+1[1] = 0,
zk+1[2] = zk[2] − ηF(zk+ 1
zk+1[3] = zk[3] − ηF(zk+ 1

2

)[2],

)[3].

2

(94)

(95)

(96)

(97)

(98)

(99)

(100)

Equation (95), Equation (96), and Equation (98) follows by (cid:104)ai, zi(cid:105) = 0 for i ∈ {k, k + 1
2 , k + 1} due
to Lemma 13. We know that (i) (cid:104)ak+ 1
, e3(cid:105) = (cid:104)ak+1, e2(cid:105) = (cid:104)ak+1, e3(cid:105) = 0 by the deﬁnition of ak+ 1
) are
and ak+1, and (ii) ak+ 1
co-directed by Lemma 13, thus Equation (97), Equation (99), and Equation (100) follow from the
combination of (i) and (ii).

− zk + ηF(zk) are co-directed, ak+1 and zk+1 − zk + ηF(zk+ 1

and zk+ 1

2

2

2

2

2

We simplify Expression (94) by substituting zk[3], zk+ 1

[2], zk+ 1

2

[3], zk+1[1], zk+1[2], and zk+1[3]

2

using Equations (95)-(100).

Expression (87) is equal to the sum of the following three parts.
The ﬁrst part is

η2(cid:13)

(cid:13)p1(F(zk))(cid:13)
(cid:13)

2 − η2(cid:13)

(cid:13)p1(F(zk+1))(cid:13)
(cid:13)

2 −

η2
1 + β2
β2

2 + 1

(cid:0)β1F(zk)[1] + β2F(zk)[2] + F(zk)[3](cid:1)2

=

+ η2F(zk+1)[1]21[F(zk+1)[1] ≥ 0]
3
∑
i=1
+ η2F(zk+1)[1]21[F(zk+1)[1] ≥ 0].

(cid:0)η2F(zk)[i]2 − η2F(zk+1)[i]2(cid:1) −

η2
1 + β2
β2

2 + 1

The second part is

(cid:0)β1F(zk)[1] + β2F(zk)[2] + F(zk)[3](cid:1)2

2η(cid:10)p1(F(zk+1)) − p1(F(zk)), p1(zk) − p1(zk+1)(cid:11)
= 2ηzk[1](cid:0)F(zk+1)[1] − F(zk)[1](cid:1) + 2η2F(zk+ 1

2

+ 2η2F(zk+ 1

2

)[3](cid:0)F(zk+1)[3] − F(zk)[3](cid:1).

52

)[2](cid:0)F(zk+1)[2] − F(zk)[2](cid:1)

(101)

(102)

The third part is

η2(cid:13)
(cid:13)
(cid:13)p1(F(zk+1)) − p1(F(zk+ 1

2

))

2

(cid:13)
(cid:13)
(cid:13)

η2(cid:16)

=

3
∑
i=1
(cid:16)

−

F(zk+1)[i] − F(zk+ 1

2

)[i]

ηF(zk)[3] − ηF(zk+ 1

2

(cid:17)2

.

)[3]

−

(cid:17)2

(cid:13)
(cid:13)
(cid:13)p1(zk+1) − p1(zk+ 1

2

2

(cid:13)
(cid:13)
(cid:13)

)

− zk+ 1

2

[1]2 −

(cid:16)

zk[2] − ηF(zk+ 1

2

)[2] + αzk+ 1

2

[1]

(cid:17)2

(103)

(cid:17)

.

(104)

2× LHS of Equation (88) is equal to
(cid:16)

2zk+ 1

2

[1]

(zk+ 1
(cid:16)

2

− zk + ηF(zk))[1] − α(zk+ 1

2

− zk + ηF(zk))[2]

(cid:17)

= 2zk+ 1

2

[1]

(zk+ 1

2

− zk + ηF(zk))[1] − α(−αzk+ 1

2

[1] − zk[2] + ηF(zk)[2])

2α
1+α2 × LHS of Equation (89) is equal to

2α
1 + α2 zk+1[2]
2α
=
1 + α2

(cid:16)

(cid:16)

(zk+ 1

2

− zk + ηF(zk))[1] − α(zk+ 1

2

− zk + ηF(zk))[2]

(cid:17)

zk[2] − ηF(zk+ 1

2

(cid:17)(cid:16)

)[2]

(zk+ 1

2

− zk + ηF(zk))[1] − α

(cid:16)

−αzk+ 1

2

[1] − zk[2] + ηF(zk)[2]

(cid:17)(cid:17)

.

(105)

2
1+α2 × LHS of Inequality (90) is equal to

2
(cid:0)α(cid:0)zk − ηF(zk)(cid:1)[1] + (cid:0)zk − ηF(zk)(cid:1)[2](cid:1)(αzk+1[1] + zk+1[2])
1 + α2
2
1 + α2

(cid:0)α(cid:0)zk − ηF(zk)(cid:1)[1] + (cid:0)zk − ηF(zk)(cid:1)[2](cid:1)(cid:16)

zk[2] − ηF(zk+ 1

=

2

(cid:17)

.

)[2]

(106)

2
1+β2
1+β2
2

× LHS of Inequality (91) is equal to

(cid:0)β1ηF(zk)[1] + β2ηF(zk)[2] + ηF(zk)[3](cid:1)(cid:16)

β1zk+ 1

2

[1] + β2zk+ 1

2

[2] + zk+ 1

2

[3]

(cid:17)

(cid:0)β1ηF(zk)[1] + β2ηF(zk)[2] + ηF(zk)[3](cid:1)(cid:16)

(β1 − αβ2)zk+ 1

2

[1] + zk[3] − ηF(zk)[3]

(cid:17)

−

= −

= −

2
1 + β2
1 + β2
2
2
1 + β2
1 + β2
2
2
1 + β2
1 + β2
2
(cid:16)
(β1 − αβ2)zk+ 1

·

2

(cid:0)ηβ1F(zk)[1] + ηβ2F(zk)[2] + ηF(zk)[3](cid:1)

[1] − β1zk[1] − β2zk[2] − ηF(zk)[3]

(cid:17)

.

2× LHS of Inequality (92) is equal to

(cid:16)

2zk[1]

zk[1] − ηF(zk+ 1

2

(cid:17)

.

)[1]

53

(107)

(108)

2× LHS of Inequality (93) is equal to

−2ηF(zk+1)[1]1

(cid:104)

F(zk+1)[1] ≤ 0

(cid:105)(cid:16)

zk[1] − ηF(zk+ 1

2

(cid:17)

.

)[1]

(109)

After the substitution, we need to argue that the sum of Expression (101) to (109) is a sum of

quotients of SOS polynomials, which we prove by establishing the following identity.

Expression (101) + Expression (102) + Expression (103) + Expression (104) + Expression (105)

+ Expression (106) + Expression (107) + Expression (108) + Expression (109)

(cid:16)

=

zk[1] − ηF(zk+ 1

2

)[1] + ηF(zk+1)[1] · 1[F(zk+1)[1] ≥ 0]

(cid:17)2

zk[1] − ηF(zk)[1] − zk+ 1
1 + β2

1 + β2
2

2

(cid:17)2

[1]

ηF(zk)[3] + β1zk[1] + β2zk[2] + (αβ2 − β1)zk+ 1
1 + β2

2

(cid:17)2

[1]

1 + β2
2
(cid:17)2
[1]

zk[2] − ηF(zk)[2] + αzk+ 1
1 + β2

1 + β2
2

2

β1(zk[2] − ηF(zk)[2] + αzk+ 1

2

[1]) − β2(zk[1] − ηF(zk)[1] − zk+ 1
1 + β2

2

1 + β2
2

(cid:16)

(cid:16)

(cid:16)

(cid:16)

+

+

+

+

≥0.

(110)

(111)

(112)

(113)

(114)

(cid:17)2

[1])

The identity for the case where F(zk+1)[1] ≥ 0 is veriﬁed at Appendix F in the second
version of this paper on arXiv, which can be found at this link. Observe that only Expres-
sion (101), Expression (109) and Term (110) depend on the sign of F(zk+1)[1]. It is sufﬁcient for
us to verify the case where F(zk+1)[1] ≥ 0, as when F(zk+1)[1] < 0, we only need to subtract
η2F(zk+1)[1]2 + 2ηF(zk+1)[1](zk[1] − ηF(zk+ 1
)[1]) from both the LHS and the RHS,16 and the iden-
tity still holds.

2

Hence, Expression (87) is non-negative. Combining with the non-negativity of Expression (86),
(cid:4)

we conclude that TARGET is non-negative. This completes the proof.

16Notice

that

(zk[1] − ηF(zk+ 1

2

)[1] + ηF(zk+1)[1])2

=

(zk[1] − ηF(zk+ 1

2

)[1])2 + η2F(zk+1)[1]2 +

2ηF(zk+1)[1](zk[1] − ηF(zk+ 1

2

)[1]).

54

