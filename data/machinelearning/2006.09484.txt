0
2
0
2

n
u
J

6
1

]

G
L
.
s
c
[

1
v
4
8
4
9
0
.
6
0
0
2
:
v
i
X
r
a

Partial Policy Iteration
for L1-Robust Markov Decision Processes

Chin Pang Ho
City University of Hong Kong

Marek Petrik
University of New Hampshire

Wolfram Wiesemann
Imperial College London

clint.ho@cityu.edu.hk

mpetrik@cs.unh.edu

ww@imperial.ac.uk

Abstract

Robust Markov decision processes (MDPs) allow to compute reliable solutions for dynamic
decision problems whose evolution is modeled by rewards and partially-known transition
probabilities. Unfortunately, accounting for uncertainty in the transition probabilities sig-
niﬁcantly increases the computational complexity of solving robust MDPs, which severely
limits their scalability. This paper describes new eﬃcient algorithms for solving the com-
mon class of robust MDPs with s- and sa-rectangular ambiguity sets deﬁned by weighted
L1 norms. We propose partial policy iteration, a new, eﬃcient, ﬂexible, and general pol-
icy iteration scheme for robust MDPs. We also propose fast methods for computing the
robust Bellman operator in quasi-linear time, nearly matching the linear complexity the
non-robust Bellman operator. Our experimental results indicate that the proposed meth-
ods are many orders of magnitude faster than the state-of-the-art approach which uses
linear programming solvers combined with a robust value iteration.

1. Introduction

Markov decision processes (MDPs) provide a versatile methodology for modeling and solving
dynamic decision problems under uncertainty (Puterman, 2005). Unfortunately, however,
MDP solutions can be very sensitive to estimation errors in the transition probabilities
and rewards. This is of particular worry in reinforcement learning applications, where the
model is ﬁt to data and therefore inherently uncertain. Robust MDPs (RMDPs) do not
assume that the transition probabilities are known precisely but instead allow them to take
on any value from a given ambiguity set or uncertainty set (Xu and Mannor, 2006; Mannor
et al., 2012; Hanasusanto and Kuhn, 2013; Tamar et al., 2014; Delgado et al., 2016). With
appropriately chosen ambiguity sets, RMDP solutions are often much less sensitive to model
errors (Xu and Mannor, 2009; Petrik, 2012; Petrik et al., 2016).

Most of the RMDP literature assumes rectangular ambiguity sets that constrain the errors
in the transition probabilities independently for each state (Iyengar, 2005; Nilim and El
Ghaoui, 2005; Le Tallec, 2007; Kaufman and Schaefer, 2013; Wiesemann et al., 2013).
This assumption is crucial to retain many of the desired structural features of MDPs. In
particular, the robust return of an RMDP with a rectangular ambiguity set is maximized by

1

 
 
 
 
 
 
a stationary policy, and the optimal value function satisﬁes a robust variant of the Bellman
optimality equation. Rectangularity also ensures that an optimal policy can be computed
in polynomial time by robust versions of the classical value or policy iteration (Iyengar,
2005; Hansen et al., 2013).

A particularly popular class of rectangular ambiguity sets is deﬁned by bounding the L1-
distance of any plausible transition probabilities from a nominal distribution (Iyengar, 2005;
Strehl et al., 2009; Jaksch et al., 2010; Petrik and Subramanian, 2014; Taleghan et al., 2015;
Petrik et al., 2016). Such ambiguity sets can be readily constructed from samples (Weiss-
man et al., 2003; Behzadian et al., 2019), and their polyhedral structure implies that the
worst transition probabilities can be computed by the solution of linear programs (LPs).
Unfortunately, even for the speciﬁc class of L1-ambiguity sets, an LP has to be solved for
each state and each step of the value or policy iteration. Generic LP algorithms have a
worst-case complexity that is approximately quartic in the number of states (Vanderbei,
1998), and they thus become prohibitively expensive for RMDPs with many states.

In this paper, we propose a new framework for solving RMDPs. Our framework applies to
both sa-rectangular ambiguity sets, where adversarial nature observes the agent’s actions
before choosing the worst plausible transition probabilities (Iyengar, 2005; Nilim and El
Ghaoui, 2005), and s-rectangular ambiguity sets, where nature must commit to a realiza-
tion of the transition probabilities before observing the agent’s actions (Le Tallec, 2007;
Wiesemann et al., 2013). We achieve a signiﬁcant theoretical and practical acceleration
over the robust value and policy iteration by reducing the number of iterations needed to
compute an optimal policy and by reducing the computational complexity of each iteration.
The overall speedup of our framework allows us to solve RMDPs with L1-ambiguity sets in
a time complexity that is similar to that of classical MDPs. Our framework comprises of
three components, each of which represents a novel contribution.

Our ﬁrst contribution is partial policy iteration (PPI), which generalizes the classical mod-
iﬁed policy iteration to RMDPs. PPI resembles the robust modiﬁed policy iteration (Kauf-
man and Schaefer, 2013), which has been proposed for sa-rectangular ambiguity sets. In
contrast to the robust modiﬁed policy iteration, however, PPI applies to both sa-rectangular
and s-rectangular ambiguity sets, and it is guaranteed to converge at the same linear rate
as robust value and robust policy iteration. In our experimental results, PPI outperforms
robust value iteration by several orders of magnitude.

Our second contribution is a fast algorithm for computing the robust Bellman operator for
sa-rectangular weighted L1-ambiguity sets. Our algorithm employs the homotopy contin-
it starts with a singleton ambiguity set for which the
uation strategy (Vanderbei, 1998):
worst transition probabilities can be trivially identiﬁed, and it subsequently traces the most
adverse transition probabilities as the size of the ambiguity set increases. The time com-
plexity of our homotopy method is quasi-linear in the number of states and actions, which
is signiﬁcantly faster than the quartic worst-case complexity of generic LP solvers.

Our third contribution is a fast algorithm for computing the robust Bellman operator for
s-rectangular weighted L1-ambiguity sets. While often less conservative and hence more
appropriate in practice, s-rectangular ambiguity sets are computationally challenging since
the agent’s optimal policy can be randomized (Wiesemann et al., 2013). We propose a

2

bisection approach to decompose the s-rectangular Bellman computation into a series of
sa-rectangular Bellman computations. When our bisection method is combined with our
homotopy method, its time complexity is quasi-linear in the number of states and actions,
compared again to the quartic complexity of generic LP solvers.

Put together, our contributions comprise a complete framework that can be used to solve
RMDPs eﬃciently. Besides being faster than solving LPs directly, our framework does not
require an expensive black-box commercial optimization package such as CPLEX, Gurobi,
or Mosek. A well-tested and documented implementation of the methods described in this
paper is available at https://github.com/marekpetrik/craam2.

Compared to an earlier conference version of this work (Ho et al., 2018), the present paper
introduces PPI, it improves the bisection method to work with PPI, it provides extensive
and simpler proofs, and it reports more complete and thorough experimental results.

The remainder of the paper is organized as follows. We summarize relevant prior work
in Section 2 and subsequently review basic properties of RMDPs in Section 3. Section 4
describes our partial policy iteration (PPI), Section 5 develops the homotopy method for
sa-rectangular ambiguity sets, and Section 6 is devoted to the bisection method for s-
rectangular ambiguity sets. Section 7 compares our algorithms with the solution of RMDPs
via Gurobi, a leading commercial LP solver, and we oﬀer concluding remarks in Section 8.

Notation. Regular lowercase letters (such as p) denote scalars, boldface lowercase letters
(such as p) denote vectors, and boldface uppercase letters (such as X) denote matrices.
Indexed values are printed in bold if they are vectors and in regular font if they are scalars.
That is, pi refers to the i-th element of a vector p, whereas pi is the i-th vector of a sequence
of vectors. An expression in parentheses indexed by a set of natural numbers, such as ppiqiPZ
for Z “ t1, . . . , ku, denotes the vector pp1, p2, . . . , pkq. Similarly, if each pi is a vector, then
i as a row. The expression ppiqj P R represents
P “ ppiqiPZ is a matrix with each vector pT
the element in i-th row and j-th column. Calligraphic letters and uppercase Greek letters
(such as X and Ξ) are reserved for sets. The symbols 1 and 0 denote vectors of all ones
and all zeros, respectively, of the size appropriate to their context. The symbol I denotes
the identity matrix of the appropriate size. The probability simplex in RS
` is denoted as
(
. The set R represents real numbers and the set R` represents
∆S “
` | 1Tp “ 1
non-negative real numbers.

p P RS

(cid:32)

2. Related Work

We review relevant prior work that aims at (i) reducing the number of iterations needed to
compute an optimal RMDP policy, as well as (ii) reducing the computational complexity
of each iteration. We also survey algorithms for related machine learning problems.

The standard approach for computing an optimal RMDP policy is robust value iteration,
which is a variant of the classical value iteration for non-robust MDPs that iteratively applies
the robust Bellman operator to an increasingly accurate approximation of the optimal robust
value function (Givan et al., 2000; Iyengar, 2005; Le Tallec, 2007; Wiesemann et al., 2013).
Robust value iteration is easy to implement and versatile, and it converges linearly with a
rate of γ, the discount factor of the RMDP.

3

Unfortunately, robust value iteration requires many iterations and thus performs poorly
when the discount factor of the RMDP approaches 1. To alleviate this issue, robust policy
iteration alternates between robust policy evaluation steps that determine the robust value
function for a ﬁxed policy and policy improvement steps that select the optimal greedy
policy for the current estimate of the robust value function (Iyengar, 2005; Hansen et al.,
2013). While the theoretical convergence rate guarantee for the robust policy iteration
matches that for the robust value iteration, its practical performance tends to be superior
for discount factors close to 1. However, unlike the classical policy iteration for non-robust
MDPs, which solves a system of linear equations in each policy evaluation step, robust policy
iteration solves a large LP in each robust policy evaluation step. This restricts robust policy
iteration to small RMDPs.

Modiﬁed policy iteration, also known as optimistic policy iteration, tends to signiﬁcantly
outperform both value and policy iteration on non-robust MDPs (Puterman, 2005). Modi-
ﬁed policy iteration adopts the same strategy as policy iteration, but it merely approximates
the value function in each policy evaluation step by executing a small number of value iter-
ations. Generalizing the modiﬁed policy iteration to RMDPs is not straightforward. There
were several early attempts to develop a robust modiﬁed policy iteration (Satia and Lave,
1973; White and Eldeib, 1994), but their convergence guarantees are in doubt (Kaufman
and Schaefer, 2013). The challenge is that the alternating maximization (in the policy
improvement step) and minimization (in the policy evaluation step) may lead to inﬁnite cy-
cles in the presence of approximation errors. Several natural robust policy iteration variants
have been shown to loop inﬁnitely on some inputs (Condon, 1993).

To the best of our knowledge, robust modiﬁed policy iteration (RMPI) is the ﬁrst gener-
alization of the classical modiﬁed policy iteration to RMDPs with provable convergence
guarantees (Kaufman and Schaefer, 2013). RMPI alternates between robust policy evalua-
tion steps and policy improvement steps. The robust policy evaluation steps approximate
the robust value function of a ﬁxed policy by executing a small number of value iterations,
and the policy improvement steps select the optimal greedy policy for the current estimate
of the robust value function. Our partial policy iteration (PPI) improves on RMPI in sev-
eral respects. RMPI only applies to sa-rectangular problems in which there exist optimal
deterministic policies, while PPI also applies to s-rectangular problems in which all optimal
policies may be randomized. Also, RMPI relies on a value iteration to partially evalu-
ate a ﬁxed policy, whereas PPI can evaluate the ﬁxed policy more eﬃciently using other
schemes such as policy or modiﬁed policy iteration. Finally, PPI enjoys a guaranteed linear
convergence rate of γ.

Apart from variants of the robust value and the robust (modiﬁed) policy iteration, eﬀorts
have been undertaken to eﬃciently evaluate the robust Bellman operator for structured
classes of ambiguity sets. While this evaluation amounts to the solution of a convex opti-
mization problem for generic convex ambiguity sets and reduces to the solution of an LP for
polyhedral ambiguity sets, the resulting polynomial runtime guarantees are insuﬃcient due
to the large number of evaluations required. Quasi-linear time algorithms for computing
Bellman updates for RMDPs with unweighted sa-rectangular L1-ambiguity sets have been
proposed by Iyengar (2005) and Petrik and Subramanian (2014). Similar algorithms have
been used to guide the exploration of MDPs (Strehl et al., 2009; Taleghan et al., 2015). In

4

contrast, our algorithm for sa-rectangular ambiguity sets applies to both unweighted and
weighted L1-ambiguity sets, where the latter ones have been shown to provide superior
robustness guarantees (Behzadian et al., 2019). The extension to weighted norms requires
a surprisingly large change to the algorithm. Quasi-linear time algorithms have also been
proposed for sa-rectangular L8-ambiguity sets (Givan et al., 2000), L2-ambiguity sets (Iyen-
gar, 2005) and KL-ambiguity sets (Iyengar, 2005; Nilim and El Ghaoui, 2005). We are not
aware of any previous specialized algorithms for s-rectangular ambiguity sets, which are
signiﬁcantly more challenging as all optimal policies may be randomized, and it is therefore
not possible to compute the worst transition probabilities independently for each action.

Our algorithm for computing the robust Bellman operator over an sa-rectangular ambigu-
ity set resembles LARS, a homotopy method for solving the LASSO problem (Drori and
Donoho, 2006; Hastie et al., 2009; Murphy, 2012). It also resembles methods for computing
fast projections onto the L1-ball (Duchi et al., 2008; Thai et al., 2015) and the weighted
L1-ball (van den Berg and Friedlander, 2011). In contrast to those works, our algorithm
optimizes a linear function (instead of a more general quadratic one) over the intersection
of the (weighted) L1-ball and the probability simplex (as opposed to the entire L1-ball).

Our algorithm for computing the robust Bellman operator for s-rectangular ambiguity sets
employs a bisection method. This is a common optimization technique for solving low-
dimensional problems. We are not aware of works that use bisection to solve s-rectangular
RMDPs or similar machine learning problems. However, a bisection method has been
previously used to solve sa-rectangular RMDPs with KL-ambiguity sets (Nilim and El
Ghaoui, 2005). That bisection method, however, has a diﬀerent motivation, solves a diﬀerent
problem, and bisects on diﬀerent problem parameters.

Throughout this paper, we focus on RMDPs with sa-rectangular or s-rectangular ambiguity
sets but note that several more-general classes have been proposed recently (Mannor et al.,
2012, 2016; Goyal and Grand-Clement, 2018). These k-rectangular and r-rectangular sets
have tangible advantages, but also introduce additional computational complications.

3. Robust Markov Decision Processes

This section surveys RMDPs and their basic properties. We cover both sa-rectangular and
s-rectangular ambiguity sets but limit the discussion to norm-constrained ambiguity sets.
An MDP pS, A, p0, p, r, γq is described by a state set S “ t1, . . . , Su and an action set
A “ t1, . . . , Au. The initial state is selected randomly according to the distribution p0 P ∆S.
When the MDP is in state s P S, taking the action a P A results in a stochastic transition
to a new state s1 P S according to the distribution ps,a P ∆S with a reward of rs,a,s1 P R.
We condense the transition probabilities ps,a to the transition function p “ pps,aqsPS,aPA P
p∆SqSˆA which can also be also interpreted as a function p : S ˆ A Ñ ∆S. Similarly, we
condense the rewards to vectors rs,a “ prs,a,s1qs1PS P RS and r “ prs,aqsPS,aPA. The discount
factor is γ P p0, 1q.
A (stationary) randomized policy π “ pπsqsPS, πs P ∆A for all s P S, is a function that
prescribes to take an action a P A with the probability πs,a whenever the MDP is in a state
s P S. We use Π “ p∆AqS to denote the set of all randomized stationary policies.

5

For a given policy π P Π, an MDP becomes a Markov reward process, which is a Markov
chain with the S ˆ S transition matrix P pπq “ ppspπqqsPS and the rewards rpπq “
prspπqqsPS P RS where

ÿ

pspπq “

πs,a ¨ ps,a

and rspπq “

aPA

aPA

ÿ

πs,a ¨ pT

s,ars,a ,

and pspπq P ∆S and rspπq P R. The total expected discounted reward of this Markov
reward process is

«

8ÿ

E

t“0

ﬀ

γt ¨ rSt,At,St`1

“ pT

0 pI ´ γ ¨ P pπqq´1rpπq .

Here, the initial random state S0 is distributed according to p0, the subsequent random
states S1, S2, . . . are distributed according to ppπq, and the random actions A0, A1, . . . are
distributed according to π. The value function of this Markov reward process is vpπ, pq “
pI ´ γ ¨ P pπqq´1rpπq. For each state s P S, vspπ, pq describes the total expected discounted
reward once the Markov reward process enters s. It is well-known that the total expected
discounted reward of an MDP is optimized by a deterministic policy π satisfying πs,a P t0, 1u
for each s P S and a P A (Puterman, 2005).

RMDPs generalize MDPs in that they account for the uncertainty in the transition function
p. More speciﬁcally, the RMDP pS, A, p0, P, r, γq assumes that the transition function p
is chosen adversarially from an ambiguity set (or uncertainty set) of plausible values P Ď
p∆SqSˆA (Hanasusanto and Kuhn, 2013; Wiesemann et al., 2013; Petrik and Subramanian,
2014; Petrik et al., 2016; Petrik and Russell, 2019). The objective is to compute a policy
π P Π that maximizes the return, or the expected sum of discounted rewards, under the
worst-case transition function from P:

max
πPΠ

min
pPP

pT

0 vpπ, pq .

(1)

The maximization in (1) represents the objective of the agent, while the minimization can
be interpreted as the objective of adversarial nature. To ensure that the minimum exists,
we assume throughout the paper that the set P is compact.

The optimal policies in RMDPs are history-dependent, stochastic and NP-hard to compute
even when restricted to be stationary (Iyengar, 2005; Wiesemann et al., 2013). However,
the problem (1) is tractable for some broad classes of ambiguity sets P. The most common
such class are the sa-rectangular ambiguity sets, which are deﬁned as Cartesian products of
sets Ps,a Ď ∆S for each state s and action a (Iyengar, 2005; Nilim and El Ghaoui, 2005; Le
Tallec, 2007):

!
p P p∆SqSˆA | ps,a P Ps,a @s P S, a P A

)

P “

.

(2)

Since each probability vector ps,a belongs to a separate set Ps,a, adversarial nature can select
the worst transition probabilities independently for each state and action. This amounts to
nature being able to observe the agent’s action prior to choosing the transition probabilities.
Similar to non-robust MDPs, there always exists an optimal deterministic stationary policy
in sa-rectangular RMDPs (Iyengar, 2005; Nilim and El Ghaoui, 2005).

6

In this paper, we study sa-rectangular ambiguity sets that constitute weighted L1-balls
around some nominal transition probabilities ¯ps,a P ∆S:

(cid:32)

Ps,a “

p P ∆S | }p ´ ¯ps,a}1,ws,a ď κs,a

(

Here, the weights ws,a P RS
radius κs,a P R` of the ball is called the budget, and the weighted L1-norm is deﬁned as

` are assumed to be strictly positive: ws,a ą 0, s P S, a P A. The

nÿ

}x}1,w “

wi |xi| .

i“1

Various L1-norm ambiguity sets have been applied to a broad range of RMDPs (Iyengar,
2005; Petrik and Subramanian, 2014; Petrik et al., 2016; Behzadian et al., 2019; Russel et al.,
2019; Derman et al., 2019) and have also been used to guide exploration in MDPs (Strehl
et al., 2009; Jaksch et al., 2010; Taleghan et al., 2015).

Similarly to MDPs, the robust value function vπ “ minpPP vpπ, pq of an sa-rectangular
RMDP for a policy π P Π can be computed using the robust Bellman policy update Lπ :
RS Ñ RS. For sa-rectangular RMDPs constrained by the L1-norm, the operator Lπ is
deﬁned for each state s P S as

πs,a ¨ min
pPPs,a

pTprs,a ` γ ¨ vq

˙

ˆ

ˆ

pLπvqs “

“

ÿ

aPA
ÿ

aPA

!
pTprs,a ` γ ¨ vq | }p ´ ¯ps,a}1,ws,a ď κs,a

)˙

.

(3)

πs,a ¨ min
pP∆S

The robust value function is the unique solution to vπ “ Lπvπ (Iyengar, 2005). To compute
the optimal value function, we use the sa-rectangular robust Bellman optimality operator
L : RS Ñ RS deﬁned as

pLvqs “ max
aPA

min
pPPs,a

“ max
aPA

min
pP∆S

pTprs,a ` γ ¨ vq
!
pTprs,a ` γ ¨ vq | }p ´ ¯ps,a}1,ws,a ď κs,a

)

.

(4)

Let π‹ P Π be an optimal robust policy which solves (1). Then the optimal robust value
function v‹ “ vπ‹ is the unique vector that satisﬁes v‹ “ Lv‹ (Iyengar, 2005; Wiesemann
et al., 2013).
Note that the p P ∆S in the equations above represents a probability vector rather than the
transition function p P p∆SqSˆA. To prevent confusion between the two in the remainder
of the paper, we specify the dimensions of p whenever it is not obvious from its context.

As mentioned above, sa-rectangular sets assume that nature can observe the agent’s action
when choosing the robust transition probabilities. This assumption grants nature too much
power and often results in overly conservative policies (Le Tallec, 2007; Wiesemann et al.,
2013). S-rectangular ambiguity sets partially alleviate this issue while preserving the com-
putational tractability of sa-rectangular sets. They are deﬁned as Cartesian products of
sets Ps Ď p∆SqA for each state s (as opposed to state-action pairs earlier):

(cid:32)

(
p P p∆SqSˆA | pps,aqaPA P Ps @s P S

P “

(5)

7

Since the probability vectors ps,a, a P A, for the same state s are subjected to the joint
constraints captured by Ps, adversarial nature can no longer select the worst transition prob-
abilities independently for each state and action. The presence of these joint constraints
amounts to nature choosing the transition probabilities while only observing the state and
not the agent’s action (but observing the agent’s policy). In contrast to non-robust MDPs
and sa-rectangular RMDPs, s-rectangular RMDPs are optimized by randomized policies
in general (Le Tallec, 2007; Wiesemann et al., 2013). As before, we restrict our atten-
tion to s-rectangular ambiguity sets deﬁned in terms of L1-balls around nominal transition
probabilities:

+

#

Ps “

p P p∆SqA |

ÿ

(cid:107)pa ´ ¯ps,a(cid:107)1,ws,a ď κs

aPA

In contrast to the earlier sa-rectangular ambiguity set, nature is now restricted by a single
budget κs P R` for all transition probabilities pps,aqaPA relating to a state s P S. We note
that although sa-rectangular ambiguity sets are a special case of s-rectangular ambiguity
sets in general, this is not true for our particular classes of L1-ball ambiguity sets.
The s-rectangular robust Bellman policy update Lπ : RS Ñ RS is deﬁned as

pLπvqs “ min
pPPs

ÿ

´
πs,a ¨ pT

¯
a prs,a ` γ ¨ vq

aPA

#

ÿ

“ min

pPp∆S qA

aPA

πs,a ¨ pT

a prs,a ` γ ¨ vq |

ÿ

aPA

+

(6)

(cid:107)pa ´ ¯ps,a(cid:107)1,ws,a ď κs

.

As in the sa-rectangular case, the robust value function is the unique solution to vπ “ Lπvπ
(Wiesemann et al., 2013). The s-rectangular robust Bellman optimality operator L : RS Ñ
RS is deﬁned as

pLvqs “ max
dP∆A

min
pPPs

aPA

a prs,a ` γ ¨ vq

ÿ

da ¨ pT
#

ÿ

“ max
dP∆A

min
pPp∆S qA

aPA

da ¨ pT

a prs,a ` γ ¨ vq |

+

(7)

(cid:107)pa ´ ¯ps,a(cid:107)1,ws,a ď κs

.

ÿ

aPA

The optimal robust value function v‹ “ vπ‹ in an s-rectangular RMDP is also the unique
vector that satisﬁes v‹ “ Lv‹ (Iyengar, 2005; Wiesemann et al., 2013). We use the same
symbols Lπ and L for sa-rectangular and s-rectangular ambiguity sets; their meaning will
be clear from the context.

4. Partial Policy Iteration

In this section, we describe and analyze a new iterative method for solving RMDPs with
sa-rectangular or s-rectangular ambiguity sets which we call Partial Policy Iteration (PPI).
It resembles standard policy iteration; it evaluates policies only partially before improving
them. PPI is the ﬁrst policy iteration method that provably converges to the optimal
solution for s-rectangular RMDPs. We ﬁrst describe and analyze PPI and then compare it
with existing robust policy iteration algorithms.

8

Algorithm 1: Partial Policy Iteration (PPI)

Input: Tolerances (cid:15)1, (cid:15)2, . . . such that (cid:15)k`1 ă γ(cid:15)k and desired precision δ
Output: Policy πk such that }vπk ´ v‹}8 ď δ
k Ð 0, v0 Ð an arbitrary initial value function ;
repeat

k Ð k ` 1;
// Policy improvement
Compute ˜vk Ð Lvk´1 and choose greedy πk such that Lπk vk´1 “ ˜vk;
// Policy evaluation
Solve MDP in Def. 1 to get vk such that }Lπk vk ´ vk}8 ď p1 ´ γq (cid:15)k ;

until }Lvk ´ vk}8 ă 1´γ
return πk

2 δ;

Algorithm 1 provides an outline of PPI. The algorithm follows the familiar pattern of
interleaving approximate policy evaluation with policy improvement and thus resembles
the modiﬁed policy iteration (also known as optimistic policy iteration) for classical, non-
robust MDPs (Bertsekas and Shreve, 1978; Puterman, 2005). In contrast to classical policy
iteration, which always evaluates incumbent policies precisely, PPI approximates policy
evaluation. This is fast and suﬃcient, particularly when evaluating highly suboptimal
policies.

Notice that by employing the robust Bellman optimality operator L, the policy improvement
step in Algorithm 1 selects the updated greedy policy πk in view of the worst transition
function from the ambiguity set. Although the robust Bellman optimality operator L re-
quires more computational eﬀort than its non-robust counterpart, it is necessary as several
variants of PPI that employ a non-robust Bellman optimality operator have been shown to
fail to converge to the optimal solution (Condon, 1993).

The policy evaluation step in Algorithm 1 is performed by approximately solving a robust
policy evaluation MDP deﬁned as follows.

Deﬁnition 1. For an s-rectangular RMDP pS, A, p0, P, r, γq and a ﬁxed policy π P Π, we
deﬁne the robust policy evaluation MDP pS, ¯A, p0, ¯p, ¯r, γq as follows. The continuous state-
dependent action sets ¯Apsq, s P S, represent nature’s choice of the transition probabilities
and are deﬁned as ¯Apsq “ Ps. Thus, nature’s decisions are of the form α “ pαaqaPA P p∆SqA
with αa P ∆S, a P A. The transition function ¯p and the rewards ¯r are deﬁned as

ÿ

¯ps,α “

πs,a ¨ αa

and ¯rs,α “ ´

aPA

ÿ

aPA

πs,a ¨ αT

a rs,a ,

where ¯ps,α P ∆S and ¯rs,α P R. All other parameters of the robust policy evaluation
MDP coincide with those of the RMDP. Moreover, for sa-rectangular RMDPs we replace
¯Apsq “ Ps with ¯Apsq “ ˆaPAPs,a.

We emphasize that although the robust policy evaluation MDP in Deﬁnition 1 computes
the robust value function of the policy π, it is, nevertheless a regular non-robust MDP.

9

Indeed, although the robust policy evaluation MDP has an inﬁnite action space, its optimal
value function exists since the Assumptions 6.0.1–6.0.4 of Puterman (2005) are satisﬁed.
Moreover, since the rewards ¯r are continuous (in fact, linear) in α and the sets ¯Apsq are
compact by construction of P, there also exists an optimal deterministic stationary policy
by Theorem 6.2.7 of Puterman (2005) and the extreme value theorem. When the action
sets ¯Apsq are polyhedral, the greedy action for each state can be computed readily from an
LP, and the MDP can be solved using any standard MDP algorithm. Section 6.3 describes
a new algorithm that computes greedy actions in quasi-linear time, which is much faster
than the time required by generic LP solvers.

The next proposition shows that the optimal solution to the robust policy evaluation MDP
from Deﬁnition 1 indeed corresponds to the robust value function vπ of the policy π.
Proposition 1. For an RMDP pS, A, p0, P, r, γq and a policy π P Π, the optimal value
function ¯v‹ of the associated robust policy evaluation MDP satisﬁes ¯v‹ “ ´vπ.

Proof. Let ¯L be the Bellman operator for the robust policy evaluation MDP. To prove the
result, we ﬁrst argue that ¯Lv “ ´pLπp´vqq for every v P RS. Indeed, Deﬁnition 1 and
basic algebraic manipulations reveal that

p¯Lvqs “ max
αP ¯Apsq

¯rs,α ` γ ¨ ¯pT

s,αv

˜

´
ÿ

ÿ
aPA

aPA

“ max
αPPs

“ max
αPPs

“ ´min
αPPs

¸

ÿ

πs,a ¨ αT

a rs,a

` γ ¨

aPA
πs,a ¨ αT

a p´rs,a ` γ ¨ vq

¸

T

πs,a ¨ αa

v

(from Deﬁnition 1)

˜

ÿ

aPA

πs,a ¨ αT

a prs,a ` γ ¨ p´vqq “ p´Lπp´vqqs .

Let ¯v‹ “ ¯L¯v‹ be the ﬁxed point of ¯L, whose existence and uniqueness is guaranteed by the
Banach ﬁxed-point theorem since ¯L is a contraction under the L8-norm. Substituting ¯v‹
into the identity above then gives

¯v‹ “ ¯L¯v‹ “ ´Lπp´¯v‹q ùñ ´¯v‹ “ Lπp´¯v‹q ,

which shows that ´¯v‹ is the unique ﬁxed point of Lπ since this operator is also an L8-
(cid:4)
contraction (see Proposition 6 in Appendix A).

The robust policy evaluation MDP can be solved by value iteration, (modiﬁed) policy
iteration, linear programming, or another suitable method. We describe in Section 6.3
an eﬃcient algorithm for calculating Lπk . The accuracy requirement }Lπk vk ´ vk}8 ď
p1 ´ γq (cid:15)k in Algorithm 1 can be used as the stopping criterion in the employed method.
As we show next, this condition guarantees that }vk ´ vπk }8 ď (cid:15)k, that is, vk is an (cid:15)k-
approximation to the robust value function of πk.

Proposition 2. Consider any value function vk and any policy πk greedy for vk, that is,
Lπk vk “ Lvk. The robust value function vπk of πk can then be bounded as follows.

}vπk ´ vk}8 ď

}Lπk vk ´ vk}8

1
1 ´ γ

10

Proof. The statement follows immediately from Corollary 4 in Appendix A if we set π “ πk
(cid:4)
and v “ vk.

Algorithm 1 terminates once the condition }Lvk ´ vk}8 ă 1´γ
2 δ is met. Note that this
condition can be veriﬁed using the computations from the current iteration and thus does
not require a new application of the Bellman optimality operator. As the next proposition
shows, this termination criterion guarantees that the computed policy πk is within δ of the
optimal policy.

Proposition 3. Consider any value function vk and any policy πk greedy for vk. If v‹ is
the optimal robust value function, then

}v‹ ´ vπk }8 ď

2
1 ´ γ

}Lvk ´ vk}8 ,

where vπk the robust value function of πk.

The statement of Proposition 3 parallels the well-known properties of approximate value
functions for classical, non-robust MDPs (Williams and Baird, 1993).

Proof of Proposition 3. Using the triangle inequality of vector norms, we see that

}v‹ ´ vπk }8 ď }v‹ ´ vk}8 ` }vk ´ vπk }8 .

Using Corollary 4 in Appendix A with v “ vk, the ﬁrst term }v‹ ´ vk}8 can be bounded
from above as follows.
1
1 ´ γ
The second term }vk ´ vπk }8 above can be bounded using Proposition 2 and the fact that
Lπk vk “ Lvk, which holds since πk is greedy for vk:

}v‹ ´ vk}8 ď

}Lvk ´ vk}8

}vk ´ vπk }8 ď

1
1 ´ γ

}Lvk ´ vk}8

The result then follows by combining the two bounds.

(cid:4)

We are now ready to show that PPI converges linearly with a rate of at most γ to the
optimal robust value function. This is no worse than the convergence rate of the robust
value iteration. The result mirrors similar results for classical, non-robust MDPs. Regular
policy iteration is not known to converge at a faster rate than value iteration even though
it is strongly polynomial (Puterman, 2005; Post and Ye, 2015; Hansen et al., 2013).

Theorem 1. Consider c ą 1 such that (cid:15)k`1 ď γc (cid:15)k for all k in Algorithm 1. Then the
optimality gap of the policy πk`1 computed in each iteration k ě 1 is bounded from above
by

ˆ

›
›v‹ ´ vπk`1

›
›

8 ď γk

}v‹ ´ vπ1}8 `

11

2 (cid:15)1
p1 ´ γc´1qp1 ´ γq

˙

.

Theorem 1 requires the sequence of acceptable evaluation errors (cid:15)k to decrease faster than
the discount factor γ. As one would expect, the theorem shows that smaller values of (cid:15)k lead
to a faster convergence in terms of the number of iterations. On the other hand, smaller (cid:15)k
values also imply that each individual iteration is computationally more expensive.

The proof of Theorem 1 follows an approach similar to the convergence proofs of policy
iteration (Puterman and Brumelle, 1979; Puterman, 2005), modiﬁed policy iteration (Put-
erman and Shin, 1978; Puterman, 2005) and robust modiﬁed policy iteration (Kaufman and
Schaefer, 2013). The proofs for (modiﬁed) policy iteration start by assuming that the initial
value function v0 satisﬁes v0 ď v‹; the policy updates and evaluations then increase vk as
fast as value iteration while preserving vk ď wk for some wk satisfying limk“8 wk “ v‹.
The incomplete policy evaluation in RMDPs may result in vk ě v‹, which precludes the
use of the modiﬁed policy iteration proof strategy. The convergence proof for RMPI inverts
the argument by starting with v0 ě v‹ and decreasing vk while preserving vk ě wk. This
property, however, is only guaranteed to hold when the policy evaluation step is performed
using value iteration. PPI, on the other hand, makes no assumptions on how the policy
evaluation step is performed. Its approximate value functions vk may not satisfy vk ď v‹,
and the decreasing approximation errors (cid:15)k guarantee improvements in vπk that are suﬃ-
ciently close to those of robust policy iteration. A key challenge is that vk ‰ vπk , which
implies that the incumbent policies πk can actually become worse in the short run.

Proof of Theorem 1. We ﬁrst show that the robust value function of policy πk`1 is at least
as good as that of πk with a tolerance that depends on (cid:15)k. Using this result, we then
prove that in each iteration k, the optimality gap of the determined policy πk shrinks by
the factor γ, again with a tolerance that depends on (cid:15)k. In the third and ﬁnal step, we
recursively apply our bound on the optimality gap of the policies π1, π2, . . . to obtain the
stated convergence rate.

We remind the reader that for each iteration k of Algorithm 1, vk denotes the approximate
robust value function of the incumbent policy πk, whereas vπk denotes the precise robust
value function of πk. We abbreviate the robust Bellman policy update Lπk by Lk. Moreover,
we denote by π‹ the optimal policy with robust value function v‹. The proof uses several
properties of robust Bellman operators that are summarized in Appendix A.

As for the ﬁrst step, recall that the policy evaluation step of PPI computes a value function
vk that approximates the robust value function vπk within a certain tolerance:

Combining this bound with Proposition 2 yields }vπk ´ vk}8 ď (cid:15)k, which is equivalent to

}Lkvk ´ vk}8 ď p1 ´ γq (cid:15)k .

vπk ě vk ´ (cid:15)k ¨ 1
vk ě vπk ´ (cid:15)k ¨ 1 .

(8)

(9)

12

We use this bound to bound Lk`1vπk from below as follows:

from (9) and Proposition 7

Lk`1vπk ě Lk`1pvk ´ (cid:15)k1q
ě Lk`1vk ´ γ(cid:15)k1
ě Lkvk ´ γ(cid:15)k1
ě Lkpvπk ´ (cid:15)k1q ´ γ(cid:15)k1 from (8) and Proposition 7
ě Lkvπk ´ 2γ(cid:15)k1
ě vπk ´ 2γ(cid:15)k1

from Lemma 4
because vπk “ Lkvπk

from Lemma 4
Lk`1 is greedy to vk

(10)

This lower bound on Lk`1vπk readily translates into the following lower bound on vπk`1:

vπk`1 ´ vπk “ Lk`1vπk`1 ´ vπk

“ pLk`1vπk`1 ´ Lk`1vπk q ` pLk`1vπk ´ vπk q
ě γP pvπk`1 ´ vπk q ` pLk`1vπk ´ vπk q
ě γP pvπk`1 ´ vπk q ´ 2γ(cid:15)k1

from vπk`1 “ Lk`1vπk`1
add 0

from Lemma 5

from (10)

Here, P is the stochastic matrix deﬁned in Lemma 5. Basic algebraic manipulations show
that the inequality above further simpliﬁes to

pI ´ γP qpvπk`1 ´ vπk q ě ´2γ(cid:15)k1 .

Recall that for any stochastic matrix P , the inverse pI ´ γP q´1 exists, is monotone, and
satisﬁes pI ´ γP q´11 “ p1 ´ γq´11, which can all be seen from its von Neumann series
expansion. Using these properties, the lower bound on vπk`1 simpliﬁes to

vπk`1 ě vπk ´

2 γ (cid:15)k
1 ´ γ

1 ,

(11)

which concludes the ﬁrst step.

To prove the second step, note that the policy improvement step of PPI reduces the opti-
mality gap of policy πk as follows:

v‹ ´ vπk`1 “ v‹ ´ Lk`1vπk`1

“ pv‹ ´ Lk`1vπk q ´ pLk`1vπk`1 ´ Lk`1vπk q
ď pv‹ ´ Lk`1vπk q ´ γ ¨ P pvπk`1 ´ vπk q

from the deﬁnition of vπk`1
subtract 0

for some P from Lemma 5

1

from (11) and P 1 “ 1

ď pv‹ ´ Lk`1vπk q `
ˆ

2γ2(cid:15)k
1 ´ γ

ď pv‹ ´ Lk`1vkq `
ˆ

γ(cid:15)k `

ď pv‹ ´ Lπ‹vkq `

γ(cid:15)k `
ˆ

ď pv‹ ´ Lπ‹vπk q `

2γ(cid:15)k `

˙

2γ2(cid:15)k
1 ´ γ
˙
2γ2(cid:15)k
1 ´ γ

2γ2(cid:15)k
1 ´ γ

1

1
˙

from (10)

Lk`1 is greedy to vk

1

from (9)

“ pLπ‹v‹ ´ Lπ‹vπk q `

2γ(cid:15)k
1 ´ γ

1

from v‹ “ Lπ‹v‹

13

Corollary 3 shows that v‹ ě vπk`1, which allows us to apply the L8-norm operator on both
sides of the inequality above. Using the contraction property of the robust Bellman policy
update (see Proposition 6), the bound above implies that

›
›v‹ ´ vπk`1

›
›

8 ď }Lπ‹v‹ ´ Lπ‹vπk }8 `

2γ(cid:15)k
1 ´ γ

ď γ }v‹ ´ vπk }8 `

2γ(cid:15)k
1 ´ γ

,

(12)

which concludes the second step.

To prove the second step, we recursively apply the inequality (12) to bound the overall
optimality gap of policy πk`1 as follows:

›
›v‹ ´ vπk`1

›
›
8 ď γ }v‹ ´ vπk }8 `
›
›
8 `

›
›v‹ ´ vπk´1

ď γ2

2γ(cid:15)k
1 ´ γ

2γ(cid:15)k
1 ´ γ

`

2γ2(cid:15)k´1
1 ´ γ

ď . . .

ď γk }v‹ ´ vπ1}8 `

2
1 ´ γ

k´1ÿ

j“0

(cid:15)j`1γk´j .

The postulated choice (cid:15)j ď γc(cid:15)j´1 ď γ2c(cid:15)j´2 ď . . . ď γpj´1qc(cid:15)1 with c ą 1 implies that

k´1ÿ

j“0

(cid:15)j`1γk´j ď (cid:15)1

k´1ÿ

j“0

γjcγk´j “ γk(cid:15)1

k´1ÿ

j“0

γjpc´1q ď γk

(cid:15)1
1 ´ γc´1 .

The result follows by substituting the value of the geometric series in the bound above. (cid:4)

PPI improves on several existing algorithms for RMDPs. To the best of our knowledge,
the only method that has been shown to solve s-rectangular RMDPs is the robust value
iteration (Wiesemann et al., 2013). Robust value iteration is simple and versatile, but it may
be ineﬃcient because it employs the computationally intensive robust Bellman optimality
operator L both to evaluate and to improve the incumbent policy. In contrast, PPI only
relies on L to improve the incumbent policy πk, whereas the robust value function of πk is
evaluated (approximately) using the more eﬃcient robust Bellman policy update Lπk . In
addition to robust value iteration, several methods proposed for sa-rectangular RMDPs can
potentially be generalized to s-rectangular problems.

Robust Modiﬁed Policy Iteration (RMPI) (Kaufman and Schaefer, 2013) is the algorithm
for sa-rectangular RMDPs that is most similar to PPI. RMPI can be cast as a special
case of PPI in which the policy evaluation step is solved by value iteration rather than
by an arbitrary MDP solver. Value iteration can be signiﬁcantly slower than (modiﬁed)
policy iteration in this context due to the complexity of computing Lπk . RMPI also does
not reduce the approximation error (cid:15)k in the policy evaluations but instead runs a ﬁxed
number of value iterations. The decreasing tolerances (cid:15)k of PPI are key to guaranteeing its
convergence rate; a comparable convergence rate is not known for RMPI.

Robust policy iteration (Iyengar, 2005; Hansen et al., 2013) is also similar to PPI, but it
has only been proposed in the context of sa-rectangular RMDPs. The main diﬀerence to

14

PPI is that the policy evaluation step in robust policy iteration is performed exactly with
the tolerance (cid:15)k “ 0 for all iterations k, which can be done by solving a large LP (Iyengar,
2005). Although this approach is elegant and simple to implement, our experimental results
show that it does not scale to even moderately-sized problems.

PPI is general and works for sa-rectangular and s-rectangular RMDPs whose robust Bellman
operators L and Lπ can be computed eﬃciently. In the next two sections we show that, in
fact, the robust Bellman optimality and update operators can be computed eﬃciently for
sa-rectangular and s-rectangular ambiguity sets deﬁned by bounds on the L1-norm.

5. Computing the Bellman Operator: SA-Rectangular Sets

In this section, we develop an eﬃcient homotopy algorithm to compute the sa-rectangular
robust Bellman optimality operator L deﬁned in (4). Our algorithm computes the inner
minimization over p P Ps,a in (4); to compute Lv for some v P RS, we simply execute our
algorithm for each action a P A and select the maximum of the obtained objective values.
To simplify the notation, we ﬁx a state s P S and an action a P A throughout this section
and drop the associated subscripts whenever the context is unambiguous (for example, we
use ¯p instead of ¯ps,a). We also ﬁx a value function v throughout this section.

Our algorithm uses the idea of homotopy continuation (Vanderbei, 1998) to solve the fol-
lowing parametric optimization problem q : R` Ñ R, which is parameterized by ξ:

!
pTz | }p ´ ¯p}1,w ď ξ

)

qpξq “ min
pP∆S

(13)

Here, we use the abbreviation z “ rs,a ` γ ¨ v. Note that ξ plays the role of the budget κs,a
in our sa-rectangular uncertainty set Ps,a, and that qpκs,aq computes the inner minimization
over p P Ps,a in (4). Our homotopy method achieves its eﬃciency by computing qpξq for
ξ “ 0 and subsequently for all ξ P p0, κs,as instead of computing qpκs,aq directly (Asif and
Romberg, 2009; Garrigues and El Ghaoui, 2009). The problem qp0q is easy since the only
feasible solution is p “ ¯p, and thus qp0q “ ¯pTz. We then trace an optimal solution p‹pξq as
ξ increases, until we reach ξ “ κs,a. Our homotopy algorithm is fast because the optimal
solution can be traced eﬃciently when ξ is increased. As we show below, qpξq is piecewise
aﬃne with at most S2 pieces (or S pieces, if all components of w are equal), and exactly
two elements of p‹pξq change when ξ increases.

By construction, qpξq varies with ξ only when ξ is small enough so that the constraint
}p ´ ¯p}1,w ď ξ in (13) is binding at optimality. To avoid case distinctions for the trivial
case when }p ´ ¯p}1,w ă ξ at optimality and qpξq is constant, we assume in the remainder
of this section that ξ is small enough. Our homotopy algorithm treats large ξ identically to
the largest ξ for which the constraint is binding at optimality.

In the remainder of this section, we ﬁrst investigate the structure of basic feasible solutions
to the problem (13) in Section 5.1. We then exploit this structure to develop our homotopy
method in Section 5.2, and we conclude with a complexity analysis in Section 5.3.

15

i P . . . Ñ

pi ´ ¯pi ď li
¯pi ´ pi ď li
pi ě 0

NB UB LB EB
(cid:88)
¨
(cid:88) (cid:88)
¨
¨

(cid:88)
¨
¨

¨
¨
¨

sNB
¨
¨
(cid:88)

sUB
sLB
(cid:88)
¨
(cid:88)
¨
(cid:88) (cid:88)

Table 1: Possible subsets of active constraints in (15). Check marks indicate active con-

straints that are included in the basis B for each index i “ 1, . . . , S.

5.1 Properties of the Parametric Optimization Problem qpξq

Our homotopy method employs the following LP formulation of problem (13):

zTp

qpξq “ min
p,lPRS
subject to p ´ ¯p ď l
¯p ´ p ď l
p ě 0
1Tp “ 1, wTl “ ξ

(14)

Note that l ě 0 is enforced implicitly. The standard approach is to solve (14) using a generic
LP algorithm. This is, unfortunately, too slow to be practical as our empirical results show.

Implementing a homotopy method in the context of a linear program, such as (14), is
especially convenient since qpξq and p‹pξq are piecewise aﬃne in ξ (Vanderbei, 1998). Indeed,
the optimal p‹pξq is aﬃne in ξ for each optimal basis in (14), and a breakpoint (or a “knot”)
occurs whenever the currently optimal basis becomes infeasible for a particular ξ. This
argument also shows that qpξq is piecewise aﬃne. Our homotopy method starts with ξ “ 0
and traces an optimal basis in (14) while increasing ξ. The key to its eﬃciency is the special
structure of the relevant bases to problem (14), which we describe next.

Each basis B in the linear program (14) is fully characterized by 2S linearly independent
(inequality and/or equality) constraints that are active, see for example Deﬁnition 2.9 of
Bertsimas and Tsitsiklis (1997). Remember that an active constraint is satisﬁed with equal-
ity, but not every constraint that is satisﬁed as equality has to be active in a given basis B.
To analyze the structure of a basis B, we note that the components pi and li of any feasible
solution pp, lq to (14) must satisfy the following three inequality constraints:

pi ´ ¯pi ď li, ¯pi ´ pi ď li, pi ě 0 .

(15)

Since the three constraints in (15) contain only two variables pi and li, they must be linearly
dependent. Thus, for every i “ 1, . . . , S, at most two out of the three constraints in (15)
can be active. Table 1 enumerates the seven possible subsets of active constraints (15)
for any given component i “ 1, . . . , S. Here, the letters N, U, L and E mnemonize the
cases where none of the constraints is active, only the upper bound or the lower bound
on ¯pi is active and where both bounds are simultaneously active and hence pi equals ¯pi.
Moreover, we have three cases where in addition to the constraints indicated by N, U, L,

16

Figure 1: Example evolution of p‹pξq for a uniform (left) and a non-uniform weight vector
w (right). Point markers indicate breakpoints where the optimal bases change.

the nonnegativity constraint pi ě 0 is active; those cases are distinguished by adding a bar
to the aforementioned letters. By construction, the sets in Table 1 are mutually exclusive
and jointly exhaustive, that is, they partition the index set 1, . . . , S.

In addition to the inequality constraints (15), a basis B may include one or both of the
equality constraints from (14). The set QB Ď t1, 2u indicates which of these equality
constraints are included in the basis B. Together with the sets from Table 1, QB uniquely
identiﬁes any basis B. The 2S linearly independent active constraints involving the 2S
decision variables uniquely specify a solution pp, lq for a given basis B as

pi ´ ¯pi “ li
¯pi ´ pi “ li
pi “ 0
1Tp “ 1
wTl “ ξ

sUB
sLB
sLB

@i P UB Y EB Y
@i P LB Y EB Y
sUB Y
sNB Y
@i P
if 1 P QB
if 2 P QB .

(16)

We use pBpξq to denote the solution p to (16) and deﬁne qBpξq “ zTpBpξq for any ξ. The
vector pBpξq may be feasible in (14) only for some values of ξ.

Before we formally characterize the properties of the optimal bases for diﬀerent values of
ξ, we illustrate the parametric behavior of p‹pξq, which is an optimizer to (14) that our
homotopy algorithm chooses. Note that this optimizer is not necessarily unique. As ξ
changes, the values of exactly two components of p‹pξq change. Since the components of
p‹pξq must sum to 1, one component pj increases and another component pi decreases. We
say that pi is a donor as it donates some of its probability mass to the receiver pj. The
examples below illustrate the speciﬁc paths traced by p‹pξq and illustrate the complications
that arise from using non-uniform weights w.

Example 1 (Uniform Weights). Consider the function qpξq in (13) for an RMDP with
4 states, z “ p4, 3, 2, 1qJ, ¯p “ p0.2, 0.3, 0.4, 0.1qJ and w “ 1. Figure 1 (left) depicts the
evolution of p‹pξq as a function of ξ. Component p4 is the receiver for all values of ξ, and
the donors are the components p1, p2 and p3. We show in Section 5.3 that for uniform
weights w, the component with the smallest value of z is always the sole receiver.

17

lllll0.000.250.500.751.000.00.51.01.52.0Size of ambiguity set: xTransition probability: pi*Indexl1234llllll0.000.250.500.751.000123Size of ambiguity set: xTransition probability: pi*Indexl1234Example 2 (Non-Uniform Weights). Consider the function qpξq in (13) for an RMDP
with 4 states, z “ p2.9, 0.9, 1.5, 0.0qJ, ¯p “ p0.2, 0.3, 0.3, 0.2qJ and w “ p1, 1, 2, 2qJ. Fig-
ure 1 (right) depicts the evolution of p‹pξq as a function of ξ. The donor-receiver pairs are
p1, 2q, p2, 4q p3, 4q and again p2, 4q. In particular, several components can serve as receivers
for diﬀerent values of ξ when w is non-uniform. Also, the same component can serve as a
donor more than once.

In the remainder of this subsection, we show that for any basis B to (14) that is of interest
for our homotopy method, at most two components of pBpξq vary with ξ. To this end, we
bound the sizes of the sets from Table 1.

Lemma 1. Any basis B to (14) satisﬁes |UB| ` |LB| ` |

sNB| ` 2|NB| “ |QB| ď 2.

Proof. The statement follows from a counting argument. Since the sets listed in Table 1
partition the index set 1, . . . , S, their cardinalities must sum to S:

|NB| ` |UB| ` |LB| ` |EB| ` |

sNB| ` |

sUB| ` |

sLB| “ S.

(17)

Each index i “ 1, . . . , S contributes between zero and two active constraints to the basis.
sUB contributes 2 constraints.
For example, i P NB contributes no constraint, whereas i P
The requirement that B contains exactly 2S linearly independent constraints translates to

0 ¨ |NB| ` 1 ¨ |UB| ` 1 ¨ |LB| ` 2 ¨ |EB| ` 1 ¨ |

sNB| ` 2 ¨ |

sUB| ` 2 ¨ |

sLB| ` |QB| “ 2S .

(18)

Subtracting two times (17) from (18), we get

´2 ¨ |NB| ´ |UB| ´ |LB| ´ |

sNB| ` |QB| “ 0 .

The result then follows by performing elementary algebra.

(cid:4)

We next show that for any basis B feasible in the problem (14) for a given ξ, the elements
in UB and LB act as donor-receiver pairs.

Proposition 4. Consider some ξ ą 0 and a basis B to problem (14) that is feasible in a
neighborhood of ξ. Then the derivatives 9p “ d
(C1) If UB “ tiu and LB “ tju, i ‰ j, then:

dξ pBpξq and 9q “ d

dξ qBpξq satisfy:

9q “

zi ´ zj
wi ` wj

,

9pi “

1
wi ` wj

,

9pj “ ´

1
wi ` wj

.

(C2) If UB “ ti, ju, i ‰ j and wi ‰ wj, and LB “ H, then:

9q “

zi ´ zj
wi ´ wj

,

9pi “

1
wi ´ wj

,

9pj “ ´

1
wi ´ wj

.

The derivatives 9p and 9q of all other types of feasible bases to problem (14) are zero.

The derivative 9p shows that in a basis of class (C1), i is the receiver and j is the donor. In
a basis of class (C2), on the other hand, an inspection of 9p reveals that i is the receiver and
j is the donor whenever wi ą wj, and the reverse situation occurs when wi ă wj.

18

sU Y

sN Y

Proof of Proposition 4. In this proof, we consider a ﬁxed basis B and thus drop the subscript
B to reduce clutter. We also denote by xD the subvector of x P RS formed by the elements
xi, i P D, whose indices are contained in the set D Ď S.
sL implies ppBpξqqi “ 0 for every ξ and thus 9pi “ 0. Likewise,
Note that i P
i P E implies that ppBpξqqi “ ¯pi for every ξ and thus 9pi “ 0 as well. Hence,
9pi ‰ 0 is only
possible if i P U Y L Y N. Since at least two components of pBpξq need to change as we
vary ξ, we can restrict ourselves to bases B that satisfy |U| ` |L| ` |N| ě 2. Since Lemma 1
furthermore shows that |U| ` |L| ` 2|N| ď 2, we only need to consider three cases in the
following: (C1) |U| “ |L| “ 1 and |N| “ 0; (C2) |U| “ 2 and |L| “ |N| “ 0; and (C3)
|L| “ 2 and |U| “ |N| “ 0. For each of these cases, we denote by p and l the unique vectors
that satisfy the active constraints (16) for the basis B.

Table 1 implies the following useful equality that any p must satisfy.

1 “ 1Tp “ 1TpN ` 1TpU ` 1TpL ` 1TpE ` 1Tp sN ` 1TpsU ` 1TpsL

“ 1TpN ` 1TpU ` 1TpL ` 1T ¯pE

(19)

Case (C1); U “ tiu, L “ tju, i ‰ j, and N “ H: In this case, equation (19) implies that
pi ` pj “ 1 ´ 1T ¯pE and thus 9pi ` 9pj “ 0. We also have

wTl “ wT

UlU ` wT

LlL ` wT

NlN ` wT
“ wili ` wjlj ` wT
“ wili ` wjlj ´ wT
“ wippi ´ ¯piq ` wjp¯pj ´ pjq ´ wT

E lE ` wT
sU ¯psU ` wT

sUlsU ` wT
sL ¯psL

E lE ` wT
sLlsL

sU ¯psU ` wT

sL ¯psL ,

sNl sN ` wT

sUlsU ` wT

sLlsL

where the second identity follows from the fact that N “ H, U “ tiu and L “ tju by
assumption, as well as sN “ H due to Lemma 1. The third identity holds since the active
constraints in E, sU and sL imply that lE “ 0, lsU “ ´ ¯psU and lsL “ ¯psL, respectively. The last
identity, ﬁnally, is due to the fact that pi ´ ¯pi “ li since i P U and ¯pj ´ pj “ lj since j P L.
Since any feasible basis B satisﬁes that wTl “ ξ, we thus obtain that

wippi ´ ¯piq ` wjp¯pj ´ pjq “ ξ ` wT

sU ¯psU ´ wT

sL ¯psL

ùñ
ðñ
ðñ

wi 9pi ´ wj 9pj “ 1
wi 9pi ` wj 9pi “ 1

9pi “ 1

wi`wj

taking d{dξ on both sides
from 9pi ` 9pj “ 0

.

The expressions for 9pj and 9q follow from 9pi ` 9pj “ 0 and elementary algebra, respectively.
Case (C2); U “ ti, ju, i ‰ j, and L “ N “ H: Similar steps as in case (C1) show that

wippi ´ ¯piq ` wjppj ´ ¯pjq “ ξ ` wT

sU ¯psU ´ wT

sL ¯psL ,

9pj and 9q. Note that if wi “ wj in the
which in turn yields the desired expressions for 9pi,
equation above, then the left hand side’s derivative with respect to ξ is zero, and we obtain
a contradiction. This allows us to assume that wi ‰ wj in case (C2).
Case (C3); L “ ti, ju, i ‰ j, and U “ N “ H: Note that pL ď ¯pL since lL satisﬁes both
lL ě 0 and lL “ ¯pL ´ pL. Since (19) implies that 1Tp “ 1TpL ` 1T ¯pE “ 1, however, we
(cid:4)
conclude that pL “ ¯pL, that is, we must have 9p “ 0 and 9q “ 0.

19

5.2 Homotopy Algorithm

Algorithm 2: Homotopy method to compute qpξq.

Input: LP parameters z, w and ¯p
Output: Breakpoints pξtqt“0,...T `1 and values pqtqt“0,...T `1, deﬁning the function q
Initialize ξ0 Ð 0, p0 Ð ¯p and q0 Ð qpξ0q “ pT

0 z ;

// Derivatives 9q for bases of (14) (see Proposition 4)
for i “ 1 . . . S do

for j “ 1 . . . S satisfying i ‰ j do

Case C1 (UB “ tiu and LB “ tju): αi,j Ð pzi ´ zjq{pwi ` wjq ;
Case C2 (UB “ ti, ju): βi,j Ð pzi ´ zjq{pwi ´ wjq if wi ‰ wj ;

end

end

// Sort derivatives and map to bases (see Proposition 4)
Store pαi,j, C1q, i ‰ j and αi,j ă 0, and pβi,j, C2q, i ‰ j and βi,j ă 0, in a list D ;
Sort the list D in ascending order of the ﬁrst element ;
Construct bases B1, . . . , BT from D “ pd1, . . . , dT q as:

#

Bm “

pUB “ tiu, LB “ tjuq
pUB “ ti, ju, LB “ Hq

if dm “ pαi,j, C1q ,
if dm “ pβi,j, C2q ;

// Trace optimal pBpξq with increasing ξ
for l “ 1 . . . T do

if Bl infeasible for ξl´1 then

Set ξl Ð ξl´1, pl Ð pl´1 and ql Ð ql´1 ;
continue;

end
Compute 9p, 9q according to the cases (C1) and (C2) from Proposition 4 ;
Compute maximum ∆ξ for which Bl remains feasible: ∆ξ Ð

maxt∆ξ ě 0 | ppl´1qj ` ∆ξ ¨ 9pj ě 0u
maxt∆ξ ě 0 | ppl´1qj ` ∆ξ ¨ 9pj ě ¯pju
maxt∆ξ ě 0 | ppl´1qi ` ∆ξ ¨ 9pi ě ¯piu

if dl “ pαi,j, C1q ,
if dl “ pβi,j, C2q and wi ą wj ,
if dl “ pβi,j, C2q and wi ă wj ;

$
’&

’%

Set ξl Ð ξl´1 ` ∆ξ, pl Ð pl´1 ` ∆ξ ¨ 9p and ql Ð ql´1 ` ∆ξ ¨ 9q ;

end
Set ξT `1 Ð 8 and qT `1 Ð qT ;
return Breakpoints pξtqt“0,...T `1 and values pqtqt“0,...T `1.

We are now ready to describe our homotopy method, which is presented in Algorithm 2.
The algorithm starts at ξ0 “ 0 with the optimal solution p0 “ ¯p achieving the objective
value q0 “ pJ
0 z. The algorithm subsequently traces each optimal basis as ξ increases, until
the basis becomes infeasible and is replaced with the next basis. Since the function qpξq is
convex, it is suﬃcient to consider bases that have a derivative 9q that is no smaller than ones

20

traced previously. Note that a basis of class (C1) satisﬁes UB “ tiu and LB “ tju for some
receiver i P S and some donor j P S, j ‰ i, and this basis is feasible at p “ p‹pξq, ξ ě 0,
only if pi P r¯pi, 1s and pj P r0, ¯pjs (see Proposition 4). Likewise, a basis of class (C2) satisﬁes
UB “ ti, ju, i ‰ j, and LB “ H, and it is feasible at p “ p‹pξq, ξ ě 0, only if pi P r¯pi, 1s and
pj P r¯pj, 1s. In a basis of class (C2), i is the receiver and j is the donor whenever wi ą wj,
and the reverse situation occurs when wi ă wj. To simplify the exposition, we assume that
all bases in Algorithm 2 have pairwise diﬀerent slopes 9q, which can always be achieved by
applying a suﬃciently small perturbation to w and/or z. Our implementation accounts for
ﬂoating-point errors by using a queue to store and examine the feasibility of all bases that
are withing some small (cid:15) of the last 9q.

Algorithm 2 generates the entire solution path of qpξq. If the goal is to compute the function
q for a particular value of ξ, then we can terminate the algorithm once the for loop over l
has reached this value. In contrast, our bisection method for s-rectangular ambiguity sets
(described in the next section) requires the entire solution path to compute robust Bellman
policy updates. We also note that Algorithm 2 records all vectors p1, . . . pT . This is done
for ease of exposition; for practical implementations, it is suﬃcient to only store the current
iterate pl and update the two components that change in the for loop over l.

The following theorem proves the correctness of our homotopy algorithm. It shows that the
function q is a piecewise aﬃne function deﬁned by the output of Algorithm 2.

Theorem 2. Let pξtqt“0,...,T `1 and pqtqt“0,...,T `1 be the output of Algorithm 2. Then, qpξq
is a piecewise aﬃne function with breakpoints ξl that satisﬁes qpξtq “ qt for t “ 0, . . . , T ` 1.

We prove the statement by contradiction. Since each point ql returned by Algorithm 2
corresponds to the objective value of a feasible solution to problem (14) at ξ “ ξl, the
output generated by Algorithm 2 provides an upper bound on qpξq. Assume to the contrary
that the output does not coincide point-wise with the function qpξq. In that case, there
must be a value of ξ at which the homotopy method disregards a feasible basis that has
a strictly smaller derivative than the one selected. This, however, contradicts the way in
which bases are selected by the algorithm.

Proof of Theorem 2. For ξ ď ξT , the piecewise aﬃne function computed by Algorithm 2 is

gpξq “ min

αP∆T `1

#

Tÿ

t“0

αt qt

|

Tÿ

t“0

+

αt ξt “ ξ

.

To prove the statement, we show that gpξq “ qpξq for all ξ P r0, ξT s. Note that gpξq ě qpξq
for all ξ P r0, ξT s by construction since our algorithm only considers feasible bases. Also,
from the construction of g, we have that qpξ0q “ gpξ0q for the initial point.

To see that gpξq ď qpξq, we need to show that Algorithm 2 does not skip any relevant
bases. To this end, assume to the contrary that there exists a ξ1 P pξ0, ξT s such that
qpξ1q ă gpξ1q. Without loss of generality, there exists a value ξ1 such that that qpξq “ gpξq
for all breakpoints ξ ď ξ1 of q; this can always be achieved by choosing a suﬃciently small
value of ξ1 where q and g diﬀer. Let ξl be the largest element in tξt
| t “ 0, . . . , T u such

21

that ξl ă ξ1, that is, we have ξl ă ξ1 ď ξl`1. Such ξl exists because ξ1 ą ξ0 and qpξ0q “ gpξ0q.
Let Bl be the basis chosen by Algorithm 2 for the line segment connecting ξl and ξl`1. We
then observe that

9qpξ1q “

qpξ1q ´ ql
ξ1 ´ ξl

ă

gpξ1q ´ ql
ξ1 ´ ξl

“

ql`1 ´ ql
ξl`1 ´ ξl

“ 9gpξ1q ,

where the ﬁrst identity follows from our choice of ξ1, the inequality directly follows from
qpξ1q ă gpξ1q, and the last two identities hold since Bl is selected by Algorithm 2 for the line
segment connecting ξl and ξl`1. However, by Lemma 1 and Proposition 4, Bl is the basis
with the minimal slope between ξl and ξl`1, and it thus satisﬁes

ql`1 ´ ql
ξl`1 ´ ξl

ď 9qpξq ,

which contradicts the strict inequality above. The correctness of the last value ξT `1 “ 8,
(cid:4)
ﬁnally, follows since q is constant for large ξ as the constraint wTl “ ξ is inactive.

5.3 Complexity Analysis

A naive implementation of Algorithm 2 has a computational complexity of OpS2 log Sq be-
cause it sorts all pairs of indexes pi, jq P S ˆ S according to their derivatives 9q. Although
this already constitutes a signiﬁcant improvement over the theoretical OpS4.5q complexity
of solving (14) using a generic LP solver, we observed numerically that the naive imple-
mentation performs on par with state-of-the-art LP solvers. In this section, we describe a
simple structural property of the parametric problem (14) that allows us to dramatically
speed up Algorithm 2.
Our improvement is based on the observation that a component i P S cannot be a receiver
in an optimal basis if there exists another component j that has both a smaller objective
coeﬃcient zj and weight wj. We call such components i dominated, and any dominated
receivers can be eliminated from further consideration without aﬀecting the correctness of
Algorithm 2.

Proposition 5. Consider a component i P S such that there is another component j P S
satisfying pzj, wjq ď pzi, wiq as well as pzj, wjq ‰ pzi, wiq. Then for any basis B in which i
acts as receiver, Algorithm 2 selects the stepsize ∆ξ “ 0.

Proof. Assume to the contrary that in iteration l, the basis Bl contains i as receiver and
Algorithm 2 selects a stepsize ∆ξ ą 0. Consider pξl´1, pl´1, ql´1q, the parameters at the
beginning of iteration l, as well as pξl, pl, qlq, the parameters at the end of iteration l. To
simplify the exposition, we denote in this proof by 1i, i “ 1, . . . , S, the i-th unit basis vector
in RS.
Let k P S be the donor in iteration l. Note that k ‰ j as otherwise 9q ě 0, which would
contradict the construction of the list D. Deﬁne δ via pl “ pl´1 ` δr1i ´ 1ks, and note
that δ ą 0 since ∆ξ ą 0. We claim that the alternative parameter setting pξ1
l, q1
lq with
l “ pl´1 ` δr1j ´ 1ks, ξ1
p1
lq ď pξl, qlq and

l ´ ¯p(cid:107)1,w and q1

l satisﬁes pξ1

l “ zJp1

l “ (cid:107)p1

l, p1

l, q1

22

l, q1

pξ1
lq ‰ pξl, qlq. Since this would correspond to a line segment with a steeper decrease
than the one constructed by Algorithm 2, this contradicts the optimality of Algorithm 2
proved in Theorem 2. To see that pξ1

lq ď pξl, qlq, note that

l, q1
l ´ ¯p(cid:13)

(cid:13)
(cid:13)p1

ξ1
l “

(cid:13)1,w ď (cid:107)pl ´ ¯p(cid:107)1,w “ ξl

since wj ď wi and pi ě ¯pi (otherwise, i could not be a receiver). Likewise, we have

l “ zJp1
q1

l ď zJpl “ ql

since zj ď zi. Finally, since pwi, ziq ‰ pwj, zjq, at least one of the previous two inequalities
(cid:4)
must be strict, which implies that pξl, pl, qlq is not optimal, a contradiction.

One readily veriﬁes that if there are two potential receivers i and j satisfying wi “ wj
and zi “ zj, either one of the receivers can be removed from further consideration without
aﬀecting the correctness of Algorithm 2. We thus arrive at Algorithm 3, which constructs
a minimal set of receivers to be considered by Algorithm 2 in time OpS log Sq.

Algorithm 3: Identify non-dominated receivers i P S.

Input: Objective coeﬃcients zi and weights wi for all components i P S
Sort the elements zi and wi in non-decreasing order of zi; break ties in
non-decreasing order of wi ;
Initialize the set of possible receivers as R Ð t1u ;
for i “ 2 . . . S do

if wi ă mintwk | k P Ru then
Update R Ð R Y tiu ;

end

end
return Possible receivers mapped back to their original positions in R

Proposition 5 immediately implies that for a uniform w, only i P S with a minimal compo-
nent zi can serve as a receiver, and our homotopy method can be adapted to run in time
OpS log Sq. More generally, if there are C diﬀerent weight values, then we need to consider
at most one receiver for each of the C values. The following corollary summarizes this fact.

Corollary 1. If |twi
OpCS log CSq and produce an output of length T ď CS.

| i P Su| “ C, then Algorithms 2 and 3 can be adapted to run in time

6. Computing the Bellman Operator: S-Rectangular Sets

We now develop a bisection scheme to compute the s-rectangular robust Bellman optimality
operator L deﬁned in (7). Our bisection scheme builds on the homotopy method for the
sa-rectangular Bellman optimality operator described in the previous section.

The remainder of the section is structured as follows. We ﬁrst describe the bisection scheme
for computing L in Section 6.1. Our method does not directly compute the greedy policy

23

Figure 2: Visualization of the s-rectangular Bellman update with the response functions

q1, q2, q3 for 3 actions.

required for our PPI from Section 4 but computes the optimal values of some dual variables
instead. Section 6.2 describes how to extract the optimal greedy policy from these dual
variables. Since our bisection scheme for computing L cannot be used to compute the
s-rectangular robust Bellman policy update Lπ for a ﬁxed policy π P Π, we describe a
diﬀerent bisection technique for computing Lπ in Section 6.3. We use this technique to
solve the robust policy evaluation MDP deﬁned in Section 4.

6.1 Bisection Scheme for Robust Bellman Optimality Operator

To simplify the notation, we ﬁx a state s P S throughout this section and drop the associated
subscripts whenever the context is unambiguous.
In particular, we denote the nominal
transition probabilities under action a as ¯pa P ∆S, the rewards under action a as ra P RS,
the L1-norm weight vector as wa P RS, and the budget of ambiguity as κ. We also ﬁx a
value function v throughout this section. We then aim to solve the optimization problem

#

ÿ

aPA

max
dP∆A

min
ξPRA
`

da ¨ qapξaq |

+

ξa ď κ

,

ÿ

aPA

(20)

where qapξq is deﬁned in (13). Note that problem (20) exhibits a very speciﬁc structure:
It has a single constraint, and the function qa is piecewise aﬃne with at most S2 pieces.
We will use this structure to derive an eﬃcient solution scheme that outperforms the naive
solution of (20) via a standard LP solver.

Our bisection scheme employs the following reformulation of (20):

#

min
uPR

u |

ÿ

aPA

+

q´1
a puq ď κ

,

24

(21)

llllllluuuuuuuuuuuuuuuuuuux3x3x3x3x3x3x3x3x3x3x3x3x3x3x3x3x3x3x3x2x2x2x2x2x2x2x2x2x2x2x2x2x2x2x2x2x2x2x1x1x1x1x1x1x1x1x1x1x1x1x1x1x1x1x1x1x1−101230123Allocated ambiquity budget: xaRobust Q−function: q(xa)Functionlq1q2q3where the inverse functions q´1

a are deﬁned as

q´1
a puq “ min
pP∆S

)
!
}p ´ ¯pa}1,wa | pTz ď u

@a P A.

(22)

ř

aPA ξa ď κ for ξa “ }pa ´ ¯pa}1,wa. In problem (22), q´1

Before we formally show that (20) and (21) are indeed equivalent, we discuss the intuition
that underlies the formulation (21). In problem (20), the adversarial nature chooses the
transition probabilities pa, a P A, to minimize value of
a zq while adhering to
ř
a puq
the ambiguity budget via
can be interpreted as the minimum ambiguity budget }p ´ ¯pa}1,wa assigned to the action
a P A that allows nature to ensure that taking an action a results in a robust value pTz not
exceeding u. Any value of u that is feasible in (21) thus implies that within the speciﬁed
overall ambiguity budget of κ, nature can ensure that every action a P A results in a robust
value not exceeding u. Minimizing u in (21) thus determines the transition probabilities
that lead to the lowest robust value under any policy, which is the same as computing the
robust Bellman optimality operator (20).

aPA da ¨ ppT

Example 3. Figure 2 shows an example with 3 actions and the corresponding q-functions
q1, q2, q3. To achieve the robust value of u depicted in the ﬁgure, the smallest action-wise
budgets ξa that guarantee qpξaq ď u, i “ 1, 2, 3, are indicated at ξ1, ξ2 and ξ3, resulting in
an overall budget of κ “ ξ1 ` ξ2 ` ξ3.

We are now ready to state the main result of this section.

Theorem 3. The optimal objective values of (20) and (21) coincide.

Theorem 3 relies on the following auxiliary result, which we state ﬁrst.

Lemma 2. The functions qa and q´1

a are convex in ξ and u, respectively.

Proof. The convexity of qa is immediate from the LP formulation (14). The convexity of
(cid:4)
q´1
a

can be shown in the same way by linearizing the objective function in (22).

Proof of Theorem 3. Since the functions qa, a P A, are convex (see Lemma 2), we can
exchange the maximization and minimization operators in (20) to obtain
˜

¸

+

#

ÿ

ÿ

min
ξPRA
`

max
dP∆A

aPA

da ¨ qapξaq

|

ξa ď κ

.

aPA

Since the inner maximization is linear in d, it is optimized at an extreme point of ∆A. This
allows us to re-express the optimization problem as

#

min
ξPRA
`

max
aPA

pqapξaqq |

+

ξa ď κ

.

ÿ

aPA

We can linearize the objective function in this problem by introducing the epigraphical
variable u P R:

#

+

ÿ

min
uPR

min
ξPRA
`

u |

aPA

ξa ď κ, u ě max
aPA

rqapξaqs

.

(23)

25

It can be readily seen that for a ﬁxed u in the outer minimization, there is an optimal ξ in
the inner minimization that minimizes each ξa individually while satisfying qapξaq ď u for
all a P A. Deﬁne ga as the a-th component of this optimal ξ:

gapuq “ min
ξaPR`

tξa | qapξaq ď uu.

(24)

We show that gapuq “ q´1

a puq. To see this, we substitute qa in (24) to get:
)

gapuq “ min
ξaPR`

min
paP∆S

a za ď u, }pa ´ ¯pa}1,wa ď ξa

.

!
ξa | pT

The identity ga “ q´1
a
must satisfy ξ‹
problem (23) shows that the optimization problem (20) is indeed equivalent to (21).

a in the equation above
. Finally, substituting the deﬁnition of ga in (24) into the
(cid:4)

then follows by realizing that the optimal ξ‹

a “ }pa ´ ¯pa}1,wa

Algorithm 4: Bisection scheme for the robust Bellman optimality operator (7)

Input: Desired precision (cid:15), functions q´1

a , a P A

umin: maximum known u for which (21) is infeasible,
umax: minimum known u for which (21) is feasible

Output: ˆu such that |u‹ ´ ˆu| ď (cid:15), where u‹ is optimal in (21)
while umax ´ umin ą 2 (cid:15) do

Split interval rumin, umaxs in half: u Ð pumin ` umaxq{2;
Calculate the budget required to achieve the mid point u: s Ð
if s ď κ then

ř

u is feasible: update the feasible upper bound: umax Ð u;

aPA q´1

a puq ;

else

u is infeasible: update the infeasible lower bound: umin Ð u;

end

end
return pumin ` umaxq{2;

The bisection scheme for solving problem (21) is outlined in Algorithm 4. Bisection is a
natural and eﬃcient approach for solving the one-dimensional optimization problem. This
algorithm is simple and works well in practice, but it can be further improved by leveraging
a , a P A, are piecewise aﬃne. In fact, Algorithm 4 only solves
the fact that the functions q´1
problem (21) to (cid:15)-optimality, and it requires the choice of a suitable precision (cid:15).

t qt“0,...,Ta`1, Ta ď S2, of each function qa, a P A. Then each inverse function q´1
a

We outline how to adapt Algorithm 4 to determine the optimal solution to problem (21)
in quasi-linear time independent of the precision (cid:15); please see Appendix B for details.
Recall that Algorithm 2 computes the breakpoints pξa
t qt“0,...,Ta`1, and objective values
pqa
is
also piecewise aﬃne with breakpoints pqa
t qt“0,...,Ta`1, and corresponding function values
ξa
t “ q´1
Ta`1.) We now
t , a P A, to a single list K in ascending order. We then execute
combine all breakpoints qa
a variant of Algorithm 4 in which both umin and umax are always set to some breakpoints

t q. (Care needs to be taken to deﬁne q´1

a puq “ 8 for u ă qa

a pqa

26

from K. Instead of choosing the midpoint u Ð pumin ` umaxq{2 in each iteration of the
bisection, we choose the median breakpoint between umin and umax. We stop once umin and
umax are consecutive breakpoints in K, in which case the optimal solution of (21) can be
computed by basic algebra.

The details of Algorithm 4 are described in Appendix B which implies the following com-
plexity statement.

Theorem 4. The combined computational complexity of Algorithms 2 and 5 is OpS2A log SA`
A log S log SAq.

Because each execution of Algorithm 5 requires that Algorithm 2 is executed to produce
its inputs, Theorem 4 states the joint complexity of the two algorithms. Using reasoning
similar to Corollary 1, the bound in Theorem 4 can be tightened as follows.

Corollary 2. If |twi
in time OpCSA log CSA ` A log CS log CSAq.

| i P Su| “ C, then Algorithms 2 and 5 can be adapted to run jointly

We emphasize that general (interior-point) algorithms for the linear programming formu-
lation of the robust Bellman optimality operator has the theoretical worst-case complexity
of OpS4.5A4.5q; see Appendix C.

6.2 Recovering the Greedy Policy

Since Algorithm 4 only computes the value of the robust Bellman optimality operator L and
not an optimal greedy policy d‹ achieving this value, it cannot be used in PPI or related
robust policy iteration methods (Iyengar, 2005; Kaufman and Schaefer, 2013) as is. This
section describes how to compute an optimal solution d‹ to problem (20) from the output
of Algorithm 4. We again ﬁx a state s P S and drop the associated subscripts whenever the
context is unambiguous. We also ﬁx a value function v throughout this section. Finally, we
assume that κ ą 0; the limiting case κ “ 0 is trivial since the robust Bellman optimality
operator then reduces to the nominal Bellman optimality operator.
Recall that Algorithm 4 computes the optimal solution u‹ P R to problem (21), which
thanks to Theorem 3 equals the optimal value of problem (20). We therefore have

#

u‹ “ max
dP∆A

min
ξPRA
`
#

“ min
ξPRA
`

max
dP∆A

ÿ

da ¨ qapξaq |

aPA
ÿ

aPA

da ¨ qapξaq |

ÿ

aPA
ÿ

aPA

+

ξa ď κ

+

ξa ď κ

,

(25)

where the second equality follows from the classical Minimax theorem. To compute an
optimal d‹ from u‹, we ﬁrst use the deﬁnition (22) of q´1

to compute ξ‹ deﬁned as

a

a “ q´1
ξ‹

a pu‹q

@a P A .

(26)

Intuitively, the components ξ‹
a of this vector represent the action-wise uncertainty budgets
required to ensure that no greedy policy achieves a robust value that exceeds u‹. The set

27

Cpξ‹q “ ta P A | qapξ‹
aq “ u‹u of all actions achieving the optimal robust value plays an
important role in the construction of an optimal greedy policy d‹. To this end, the following
result collects important properties of ξ‹ and Cpξ‹q.

Lemma 3. The vector ξ‹ deﬁned in (26) is optimal in (25). Moreover, Cpξ‹q ‰ H and

(i) qapξ‹

aq “ u‹ for all a P Cpξ‹q;

(ii) ξ‹

a “ 0 and qapξ‹

aq “ ¯pJ

a z ď u‹ for all a P AzCpξ‹q.

Proof. We ﬁrst show that Cpξ‹q ‰ H. To this end, we note that for all a P A, we have

qapξ‹

aq “ qapq´1

a pu‹qq “ min
p1P∆S

!

pT
1 z | }p1 ´ ¯pa}1,wa ď min
p2P∆S

!
}p2 ´ ¯pa}1,wa | pT

2 z ď u‹

))

2qTz ď u‹.

2 to
aq ď
Imagine now that Cpξ‹q “ H. This implies, by the previous argument,
aq ă u‹ for all a P A. In that case, u‹ would not be optimal in (21) which is a

by the deﬁnitions of qa and q´1
in (13) and (22), respectively. Any optimal solution p‹
a
the inner minimization is also feasible in the outer minimization, and therefore qapξ‹
pp‹
that qapξ‹
contradiction and therefore Cpξ‹q ‰ H.
We next argue that ξ‹ is optimal in (25). To see that ξ‹ is feasible in (25), we ﬁx any
optimal solution ¯ξ P RA in (25). By construction, this solution satisﬁes qap ¯ξaq ď u‹ for all
a P A, and the deﬁnition of qa in (13) implies that there are pa P ∆S, a P A, such that
a z ď u‹ and }pa ´ ¯pa}1,wa ď ¯ξa. The deﬁnition of q´1
pT
in (22) implies that each pa is
a
feasible in q´1

a is bounded from above by ¯ξa, and we observe that
ÿ

a pu‹q. Thus, each ξ‹

ÿ

ξ‹
a ď

¯ξa ď κ .

aPA

aPA

a “ q´1

aq ď u‹ for all a P A.

a also implies that ξ‹

a pu‹q ě 0, ξ‹ is indeed feasible in (25).

a “ 0 for a P AzCpξ‹q, assume to the contrary that ξ‹

Since the deﬁnition of q´1
The optimality of ξ‹ in (25) then follows from the fact that qapξ‹
The statement that qapξ‹
Cpξ‹q. To see that ξ‹
a P AzCpξ‹q. Since qapξ‹
and }p‹
This implies, however, that there is (cid:15) ą 0 such that p‹
achieves a lower objective value than p‹
thus conclude that ξ‹
all a P AzCpξ‹q as well. The fact that qapξ‹
been shown in the ﬁrst paragraph of this proof.

aq “ u‹ for all a P Cpξ‹q follows immediately from the deﬁnition of
a ą 0 for some
aqTz ă u‹
a ´ ¯pa}1,wa ą 0 as well.
aq is feasible in (22) and
a in (22). We
aq “ ¯pJ
a z for
aq ď u‹ for all a P AzCpξ‹q, ﬁnally, has already
(cid:4)

a, which contradicts the optimality of p‹
a “ 0 for a P AzCpξ‹q. This immediately implies that qapξ‹

a P ∆S optimal in (22) satisfying pp‹

a ą 0, we have }p‹
a ` (cid:15) ¨ p ¯pa ´ p‹

a. At the same time, since ξ‹

aq ă u‹, there is p‹

a ´ ¯pa}1,wa ď ξ‹

The construction of d‹ P ∆A relies on the slopes of qa, which are piecewise constant but
discontinuous at the breakpoints of qa. However, the functions qa are convex by Lemma 2,
and therefore their subdiﬀerentials Bqapξaq exist for all ξa ě 0. Using these subdiﬀerentials,
we construct optimal action probabilities d‹ P ∆A from ξ‹ as follows.

28

(i) If 0 P Bq¯apξ‹

¯aq for some ¯a P Cpξ‹q, deﬁne d‹ as

#

d‹
a “

1
0

if a “ ¯a
otherwise

@a P A .

(27a)

(ii) If 0 R Bq¯apξ‹

¯aq for all a P Cpξ‹q, deﬁne d‹ as
#

d‹
a “

eař

a1PA ea1

with ea “

´ 1
fa
0

if a P Cpξ‹q
otherwise

@a P A ,

(27b)

where fa can be any element from Bqapξ‹

aq, a P A.

The choice of d‹ may not be unique as there may be multiple ¯a P Cpξ‹q that satisfy the ﬁrst
condition, and the choice of fa P Bqapξ‹
aq in the second condition may not be unique either.

Theorem 5. Any vector d‹ satisfying (27a) or (27b) is optimal in problem (20). Moreover,
for ξ‹ deﬁned in (26), pd‹, ξ‹q is a saddle point in (20).

Proof. One readily veriﬁes that d‹ satisfying (27a) is contained in ∆A. To see that d‹ P ∆A
for d‹ satisfying (27b), we note that Cpξ‹q is non-empty due to Lemma 3 and that fa ă 0
and thus ea ą 0 since qa is non-increasing. To see that d‹ satisfying (27a) or (27b) is
optimal in (20), we show that it achieves the optimal objective value u‹, that is, that

#

ÿ

aPA

min
ξPRA
`

d‹
a ¨ qapξaq |

+

ξa ď κ

ě u‹ .

ÿ

aPA

(28)

Observe that u‹ is indeed achieved for ξ “ ξ‹ since

ÿ

ÿ

a ¨ qapξ‹
d‹

aq “

a ¨ qapξ‹
d‹

aq “

aPA

aPCpξ‹q

ÿ

aPCpξ‹q

a ¨ u‹ “ u‹ .
d‹

a “ 0 for a R Cpξ‹q, the second equality follows from

Here, the ﬁrst equality holds since d‹
the deﬁnition of Cpξ‹q, and the third equality follows from d‹ P ∆A.
To establish the inequality (28), we show that ξ‹ is optimal in (28). This also proves that
pd‹, ξ‹q is a saddle point of problem (20). We denote by Bξpf qrξ‹s the subdiﬀerential of a
convex function f with respect to ξ, evaluated at ξ “ ξ‹. The KKT conditions for non-
diﬀerentiable convex programs (see, for example, Theorem 28.3 of Rockafellar 1970), which
are suﬃcient for the optimality of ξ‹ in the minimization on the left-hand side of (28),
require the existence of a scalar λ‹ ě 0 and a vector α‹ P RA

` such that

0 P Bξ

˜

ÿ

aPA

λ‹ ¨

d‹
a ¨ qapξaq ´ λ‹
¸
˜

ÿ

κ ´

aPA

˜

¸

ÿ

ÿ

κ ´

ξa

´

¸

α‹

a ¨ ξa

rξ‹s

[Stationarity]

ξ‹
a

“ 0, α‹

a ¨ ξ‹

aPA

aPA
a “ 0 @a P A

[Compl. Slackness]

The stationarity condition simpliﬁes using the chain rule to

0 P d‹

a ¨ Bqapξ‹

aq ` λ‹ ´ α‹
a

@a P A .

(29)

29

If d‹ satisﬁes (27a), then both (29) and complementary slackness are satisﬁed for λ‹ “ 0
and α‹ “ 0. On the other hand, if d‹ satisﬁes (27b), we set

λ‹ “

1ř

aPCpξ‹q ea

,

a “ 0 @a P Cpξ‹q,
α‹

a “ λ‹ @a P AzCpξ‹q ,
α‹

where ea is deﬁned in (27b). This solution satisﬁes λ‹ ě 0 and α ě 0 because fa ď 0
and therefore ea ě 0. This solution satisﬁes (29), and Lemma 3 implies that the second
complementary slackness condition is satisﬁed as well. To see that the ﬁrst complementary
a “ κ under the conditions of (27b).
slackness condition is satisﬁed, we argue that
aPA ξ‹
aq are closed
Assume to the contrary that
for all a P Cpξ‹q (see Theorem 23.4 of Rockafellar 1970), we have

a ă κ. Since 0 R Bqapξ‹

aq and the sets Bqapξ‹

aPA ξ‹

ř

ř

D ¯βa ą 0 such that

qapξ‹

a ` βaq ă qapξaq @βa P p0, ¯βaq

for all a P Cpξ‹q. We can thus marginally increase each component ξ‹
a, a P Cpξ‹q, to obtain
a new solution to problem (25) that is feasible and that achieves a strictly lower objective
value than u‹. This, however, contradicts the optimality of u‹. We thus conclude that
ř
(cid:4)
a “ κ, that is, the ﬁrst complementary slackness condition is satisﬁed as well.

aPA ξ‹

aq and q´1

The values ξ‹ and d‹ can be computed in time OpA log Sq since they rely on the quantities
a pu‹q that have been computed previously by Algorithm 2 and Algorithm 4,
qapξ‹
respectively. The worst-case transition probabilities can also be retrieved from the mini-
mizers of qa deﬁned in (13) since, as Theorem 5 implies, ξ‹ is optimal in the minimization
problem in (20).

6.3 Bisection Scheme for Robust Bellman Policy Update

Recall that the robust policy evaluation MDP pS, ¯A, p0, ¯p, ¯r, γq deﬁned in Section 4 has
continuous action sets ¯Apsq “ Ps, s P S, and the transition function ¯p and the rewards ¯r
deﬁned as

ÿ

ÿ

¯ps,α “

πs,a ¨ αa

and ¯rs,α “ ´

aPA

aPA

πs,a ¨ αT

a rs,a .

To solve this MDP via value iteration or (modiﬁed) policy iteration, we must compute the
Bellman optimality operator L deﬁned as
!
¯rs,α ` γ ¨ ¯pT

)

s,αv

pLvqs “ max
αPPs

#

+

“ max

αPp∆S qA

“ ´ min

αPp∆S qA

aPA
#

ÿ

aPA

ÿ

πs,a ¨ αT

a pγv ´ rs,aq |

ÿ

aPA

πs,a ¨ αT

a prs,a ´ γvq |

ÿ

aPA

(cid:107)αa ´ ¯ps,a(cid:107)1,ws,a ď κs

+

(cid:107)αa ´ ¯ps,a(cid:107)1,ws,a ď κs

.

The continuous action space in this MDP makes it impossible to compute Lv by simply
enumerating the actions. The non-robust Bellman operator could be solved as a linear

30

program, but this suﬀers from the same computational limitations its application to the
robust Bellman operator described earlier.

Using similar ideas as in Section 6.1, we can re-express the minimization problem as

#

ÿ

aPA

min
ξPRA
`

πs,a ¨ qs,apξaq |

+

ξa ď κs

,

ÿ

aPA

(30)

where we use z “ rs,a ´ γv in our deﬁnition of the functions qs,a.

At the ﬁrst glance, problem (30) seems to be a special case of problem (20) from Section 6.1,
and one may posit that it can be solved using Algorithm 4. Unfortunately, this is not the
case: In problem (30), the policy π is ﬁxed and may be randomized, whereas Algorithm 4
takes advantage of the fact that d can be assumed to be deterministic once the maximization
and minimization are swapped in (20).

Problem (30) can still be solved eﬃciently by taking advantage of the fact that it only
contains a single resource constraint on ξ and that the functions qs,a are piecewise aﬃne
and convex. To see this, note that the Lagrangian of (30) is

#

ÿ

aPA

max
λPR`

min
ξPRA
`

pπs,a ¨ qs,apξaqq ` λ ¨ 1Tξ ´ λ κs

,

+

where the use of strong duality is justiﬁed since (30) can be reformulated as a linear program
that is feasible by construction. The minimization can now be decomposed by actions:

#

ÿ

max
λPR`

min
ξaPR`

tπs,a ¨ qs,apξaq ` λξau ´ λ κs

+

loooooooooooooooooooooooooomoooooooooooooooooooooooooon

aPA

“upλq

The inner minimization problems over ξa, a P A, are convex, and they can be solved
exactly by bisection since the involved functions qs,a are piecewise aﬃne. Likewise, the
maximization over λ can be solved exactly by bisection since u is concave and piecewise
aﬃne. Note that the optimal value of λ is bounded from below by 0 and from above by the
maximum derivative of any qs,a, a P A.

7. Numerical Evaluation

We now compare the runtimes of PPI (Algorithm 1) combined with the homotopy method
(Algorithm 2) and the bisection method (Algorithm 4) with the runtime of a naive ap-
proach that combines the robust value iteration with a computation of the robust Bellman
optimality operator L using a general LP solver. We use Gurobi 9.0, a state-of-the-art
commercial optimization package. All algorithms were implemented in C++, parallelized
using the OpenMP library, and used the Eigen library to perform linear algebra oper-
ations. The algorithms were compiled with GCC 9.3 and executed on an AMD Ryzen
9 3900X CPU with 64GB RAM. The source code of the implementation is available at
http://github.com/marekpetrik/craam2.

31

7.1 Experimental Setup

Our experiments involve two problems from diﬀerent domains with a fundamentally diﬀerent
structure. The two domains are the inventory management problem (Zipkin, 2000; Porteus,
2002) and the cart-pole problem (Lagoudakis and Parr, 2003). The inventory management
problem has many actions and dense transition probabilities. The cart-pole problem, on
the other hand, has only two actions and sparse transition probabilities. More actions and
dense transition probabilities make for much more challenging computation of the Bellman
update compared to policy evaluation.

Next, we give a high-level description of both problems as well as our parameter choice.
Because the two domains serve simply as benchmark problems and their full description
would be lengthy, we only outline their motivation, construction, and properties. To facil-
itate the reproducibility of the domains, the full source code, which was used to generate
them, is available at http://github.com/marekpetrik/PPI_paper. The repository also
contains CSV ﬁles with the precise speciﬁcation of the RMDPs being solved.

In our inventory management problem, a retailer orders, stores and sells a single product
over an inﬁnite time horizon. Any orders submitted in time period t will be fulﬁlled at the
beginning of time period t ` 1, and orders are subject to deterministic ﬁxed and variable
costs. Any items held in inventory incur deterministic per-period holding costs, and the
inventory capacity is limited. The per-unit sales price is deterministic, but the per-period
demand is stochastic. All accrued demand in time period t is satisﬁed up to the available in-
ventory. Any remaining unsatisﬁed demand is backlogged at a per-unit backlogging penalty
up to a given limit. The states and actions of our MDP represent the inventory levels and
the order quantities in any given time period, respectively. The stochastic demands drive
the stochastic state transitions. The rewards are the sales revenue minus the purchase costs
in each period.

In our experiments, we set the ﬁxed and variable ordering costs to 5.99 and 1.0, respectively.
The inventory holding and backlogging costs are 0.1 and 0.15, respectively. We vary the
inventory capacity I to study the impact of the problem’s size on the runtimes, while the
backlog limit is I{3. We also impose an upper limit of I{2 on each order. The corresponding
MDP thus has I ` I{3 “ 4{3 ¨ I states and I{2 actions. Note that due to the inventory
capacity limits, not all actions are available at every state. The unit sales price is 1.6. The
demand in each period follows a Normal distribution with a mean of I{2 and a standard
deviation of I{5 and is rounded to the closest integer. We use a discount factor of 0.995.

In our cart-pole problem, a pole has to be balanced upright on top of a cart that moves
along a single dimension. At any point in time, the state of the system is described by
four continuous quantities: the cart’s position and velocity, as well as the pole’s angle and
angular velocity. To balance the pole, one can apply a force to the cart from the left or from
the right. The resulting MDP thus accommodates a 4-dimensional continuous state space
and two actions. Several diﬀerent implementations of this problem can be found in the
literature; in the following, we employ the deterministic implementation from the OpenAI
Gym. Again, we use a discount factor of 0.995.

Since the state space of our cart-pole problem is continuous, we discretize it to be amenable
to our solution methods. The discretization follows a standard procedure in which random

32

samples from the domain are subsampled to represent the discretized state space. The
transitions are then estimated from samples that are closest to each state. In other words,
the probability of transitioning from a discretized state s to another discretized state s1 is
proportional to the number of sampled transitions that originate near s and end up near s1.
The discretized transition probabilities are no longer deterministic, even though the original
problem transitions are.

The ambiguity sets are modiﬁed slightly in this section to ensure a more realistic evaluation.
Assuming that the robust transition can be positive to any state of the RMDP can lead to
overly conservative policies. To obtain less conservative policies, we restrict our ambiguity
sets Ps,a and Ps from Section 3 to probability distributions that are absolutely continuous
with respect to the nominal distributions ¯ps,a. Our sa-rectangular ambiguity sets Ps,a thus
become

P

T

(

(cid:32)

Ps,a “

p P ∆S | }p ´ ¯ps,a}1,ws,a ď κs,a, ps1 ď

¯ps,a,s1

@s1 P S

,

and we use a similar construction for our s-rectangular ambiguity sets Ps. We set the
ambiguity budget to κs,a “ 0.2 and κs “ 1.0 in the sa-rectangular and s-rectangular version
of our inventory management problem, respectively, and we set κs,a “ κs “ 0.1 in our cart-
pole problem. Anecdotally, the impact of the ambiguity budget on the runtimes is negligible.
We report separate results for uniform weights ws,a “ 1 and non-uniform weights ws,a that
are derived from the value function v. In the latter case, we follow the suggestions of Russel
et al. (2019) and choose weights pws,aqs1 that are proportional to |vs1 ´ 1Tv{S|. All weights
ws,a are normalized so that their values are contained in r0, 1s. Note that the simultaneous
scaling of ws,a and κs,a does not aﬀect the solution.

Recall that the policy evaluation step in PPI can be accomplished by any MDP solution
method. In our inventory management problem, whose instances have up to 1, 000 states,
we use policy iteration and solve the arising systems of linear equations via the LU de-
composition of the Eigen library (Puterman, 2005). This approach does not scale well to
MDPs with S " 1, 000 states as the policy iteration manipulates matrices of dimension
S ˆ S. Therefore, in our cart-pole problem, whose instances have 1, 000 or more states,
we use modiﬁed policy iteration (Puterman, 2005) instead. We compare the performance
of our algorithms to the robust value iteration as well as the robust modiﬁed policy it-
eration (RMPI) of Kaufman and Schaefer (2013). Recall that in contrast to PPI, RMPI
evaluates robust policies through a ﬁxed number of value iteration steps. Since the im-
pact of the number of value iteration steps on the overall performance of RMPI is not
well understood, we ﬁx this number to 1, 000 throughout our experiments. Finally, we set
(cid:15)k`1 “ mintγ2(cid:15)k, 0.5{p1´γq¨}Lπk vk ´ vk}8u in Algorithm 1, which satisﬁes the convergence
condition in Theorem 1.

7.2 Results and Discussion

Table 2 reports the runtimes required by our homotopy method (Algorithm 2), our bisection
method (Algorithm 4) and Gurobi (LP Solver) to compute 200 steps of the robust Bellman
optimality operator L across all states s P S. We ﬁxed the number of Bellman evaluations
in this experiment to clearly separate the speedups achieved by a quicker evaluation of the
Bellman operator itself, studied in this experiment, from the speedups obtained by using PPI

33

Problem Ambiguity States

LP Solver

Algorithm 2

LP Solver

Algorithm 4

SA-rectangular

S-rectangular

Inventory Uniform
Inventory Weighted
Inventory Uniform
Inventory Weighted
Inventory Uniform
Inventory Weighted

Cart-pole Uniform
Cart-pole Weighted
Cart-pole Uniform
Cart-pole Weighted
Cart-pole Uniform
Cart-pole Weighted

100
100
500
500

13.96
13.85
583.20
440.35
1,000 ą 10,000.00
4,071.47
1,000

24.67
0.02
21.36
0.75
1,715.94
0.36
655.00
20.69
20.00 ą 10,000.00
3,752.21
109.27

1,000
1,000
2,000
2,000
4,000
4,000

9.50
12.70
12.81
12.04
23.39
19.96

0.18
1.93
1.90
2.03
1.91
2.05

19.85
32.80
13.33
13.08
23.29
21.16

0.06
0.86
19.65
36.24
51.97
163.32

1.94
1.90
1.88
1.95
1.76
2.14

Table 2: Runtime (in seconds) required by diﬀerent algorithms to compute 200 steps of the

robust Bellman optimality operator.

in place of value iteration, studied in the next experiment. The computations are parallelized
over all available threads via OpenMP using Jacobi-style value iteration (Puterman, 2005).
By construction, all algorithms identify the same optimal solutions in each application of
the Bellman operator. The computations were terminated after 10, 000 seconds.

There are several important observations we can make from the results in Table 2. First of
all, that our algorithms outperform Gurobi by an order of magnitude for weighted ambiguity
sets and by two orders of magnitude for uniform (unweighted) ambiguity sets, independent
of the type of rectangularity. This impressive performance is because the inventory manage-
ment problem has many actions, which makes computing the Bellman operator particularly
challenging. The computation time also reﬂects that homotopy and bisection methods have
quasi-linear time complexities when used with uniform L1 norms.
It is remarkable that
even with the simple cart-pole problem our algorithms are about 10 to 20 times faster than
a state-of-the-art LP solver. Notably, even moderately-sized RMDPs may be practically
intractable to general LP solvers.

S-rectangular instances of such problems are particularly challenging for LP solvers as they
have to solve a single, monolithic LP across all actions. Perhaps surprisingly, our algorithms
also outperform Gurobi in the simple cart-pole problem by an order of magnitude. In fact,
the table reveals that even moderately-sized RMDPs may be practically intractable when
solved with generic LP solvers.

Table 3 reports the runtimes required by the parallelized versions of the robust value itera-
tion (VI), the robust modiﬁed policy iteration (RMPI) and our partial policy iteration (PPI)
to solve our inventory management and cart-pole problems to approximate optimality. To
this end, we choose a precision of δ “ 40 (that is, }Lπk vk ´ vk}8 ď 0.1), as deﬁned in Algo-

34

SA-rectangular

S-rectangular

Problem Ambiguity

States

VI

RMPI PPI

VI

PPI

Inventory Uniform
Inventory Weighted
Inventory Uniform
Inventory Weighted
Inventory Uniform
Inventory Weighted

Cart-pole Uniform
Cart-pole Weighted
Cart-pole Uniform
Cart-pole Weighted
Cart-pole Uniform
Cart-pole Weighted

100
100
500
500
1,000
1,000

1,000
1,000
10,000
10,000
20,000
20,000

0.12
10.28
1.39
140.53
8.65
393.90

0.03
0.25
0.32
1.72
0.44
6.37

0.03
0.94
0.06
5.69
0.23
14.36

0.06
0.17
0.26
1.13
0.54
3.22

0.01
0.14
0.14
2.11
0.59
6.90

0.03
0.04
0.13
0.21
0.29
0.62

3.52
15.02
24.69
276.63
217.90
519.21

0.80
0.98
8.40
13.43
16.24
28.50

0.15
1.02
2.71
16.76
13.98
163.18

0.15
0.28
1.06
3.52
2.40
9.30

Table 3: Runtime (in seconds) required by diﬀerent algorithms to compute an approxi-

mately optimal robust value function.

rithm 1, for our inventory management problem, as well as a smaller precision of δ “ 4 (that
is, }Lπk vk ´ vk}8 ď 0.01) for our cart-pole problem, to account for the smaller rewards in
this problem. All algorithms use the homotopy (Algorithm 2) and the bisection method
(Algorithm 4) to compute the robust Bellman optimality operator. Note that RMPI is
only applicable to sa-rectangular ambiguity sets. The computations were terminated after
10, 000 seconds.

There are also several important observations we can make from the results in Table 3. As
one would expect, PPI in RMDPs behaves similarly to policy iteration in MDPs. It out-
performs value iteration in essentially all benchmarks, being almost up to 100 times faster,
but the margin varies signiﬁcantly. The improvement margin depends on the relative com-
plexity of policy improvements and evaluations. In the sa-rectangular cart-pole problem,
for example, the policy improvement step is relatively cheap, and thus the beneﬁt of em-
ploying a policy evaluation is small. The situation is reversed in the s-rectangular inventory
management problem, in which the policy improvement step is very time-consuming. PPI
outperforms the robust value iteration most signiﬁcantly in the sa-rectangular inventory
management problem since the policy evaluation step is much cheaper than the policy im-
provement step due to the large number of available actions. RMPI’s performance, on the
other hand, is more varied: while it sometimes outperforms the other methods, it is usu-
ally dominated by at least one of the competing algorithms. We attribute this fact to the
ineﬃcient value iteration that is employed in the robust policy evaluation step of RMPI.
It is important to emphasize that PPI has the same theoretical convergence rate as the
robust value iteration, and thus its performance relative to the robust value iteration and
RMPI will depend on the speciﬁc problem instance and as well as the employed parameter
settings.

35

In conclusion, our empirical results show that our proposed combination of PPI and the
homotopy or bisection method achieves a speedup of up to four orders of magnitude for both
sa-rectangular and s-rectangular ambiguity sets when compared with the state-of-the-art
solution approach that combines a robust value iteration with a computation of the robust
Bellman operator via a commercial LP solver. Since our methods scale more favorably with
the size of the problem, their advantage is likely to only increase with larger problems that
what we considered here.

8. Conclusion

We proposed three new algorithms to solve robust MDPs over L1-ball uncertainty sets.
Our homotopy algorithm computes the robust Bellman operator over sa-rectangular L1-ball
uncertainty sets in quasi-linear time and is thus almost as eﬃcient as computing the nominal,
non-robust Bellman operator. Our bisection scheme utilizes the homotopy algorithm to
compute the robust Bellman operator over s-rectangular L1-ball uncertainty sets, again in
quasi-linear time. Both algorithms can be combined with PPI, which generalizes the highly
eﬃcient modiﬁed policy iteration scheme to robust MDPs. Our numerical results show
signiﬁcant speedups of up to four orders of magnitude over a leading LP solver for both
sa-rectangular and s-rectangular ambiguity sets.

Our research opens up several promising avenues for future research. First, our homo-
topy method sorts the bases of problem (14) in quasi-linear time. This step could also be
implemented in linear time using a variant of the quickselect algorithm, which has led to
improvements in a similar context (Condat, 2016). Second, we believe that the techniques
presented here can be adapted to other uncertainty sets, such as L8- and L2-balls around
the nominal transition probabilities or uncertainty sets based on φ-divergences. Both the
eﬃcient implementation of the resulting algorithms as well as the empirical comparison of
diﬀerent uncertainty sets on practical problem instances would be of interest. Finally, it
is important to study how our methods generalize to robust value function approximation
methods (Tamar et al., 2014).

Acknowledgments

We thank Bruno Scherrer for pointing out the connections between policy iteration and
algorithms for solving zero-sum games and Stephen Becker for insightful comments. This
work was supported by the National Science Foundation under Grants No. IIS-1717368
and IIS-1815275, by the Engineering and Physical Sciences Research Council under Grant
No. EP/R045518/1, and by the Start-Up Grant scheme of the City University of Hong Kong.
Any opinions, ﬁndings, and conclusions or recommendations are those of the authors and
do not necessarily reﬂect the views of the funding bodies.

References

M. S. Asif and J. Romberg. Dantzig selector homotopy with dynamic measurements. In

IS&T/SPIE Computational Imaging, 2009.

36

B. Behzadian, R. Russel, and M. Petrik. High-Conﬁdence Policy Optimization: Reshaping

Ambiguity Sets in Robust MDPs. Technical report, Arxiv, 2019.

D. Bertsekas and S. Shreve. Stochastic optimal control: The discrete time case. 1978.

D. P. Bertsekas. Abstract Dynamic Programming. 2013.

D. Bertsimas and J. N. Tsitsiklis. Introduction to Linear Optimization. 1997.

L. Condat. Fast projection onto the Simplex and the l1 Ball. Mathematical Programming,

158(1-2):575–585, 2016.

A. Condon. On algorithms for simple stochastic games. Advances in Computational Com-
plexity Theory, DIMACS Series in Discrete Mathematics and Theoretical Computer Sci-
ence, 13:51–71, 1993.

K. V. Delgado, L. N. De Barros, D. B. Dias, and S. Sanner. Real-time dynamic programming
for Markov decision processes with imprecise probabilities. Artiﬁcial Intelligence, 230:
192–223, 2016.

E. Derman, D. Mankowitz, T. Mann, and S. Mannor. A Bayesian Approach to Robust

Reinforcement Learning. Technical report, 2019.

I. Drori and D. Donoho. Solution of l1 Minimization Problems by LARS/Homotopy Meth-

ods. In Acoustics, Speech and Signal Processing (ICASSP), 2006.

J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Eﬃcient projections onto the
l1-ball for learning in high dimensions. In International Conference of Machine Learning
(ICML), 2008.

P. J. Garrigues and L. El Ghaoui. An Homotopy Algorithm for the Lasso with Online
In Advances in Neural Information Processing Systems (NIPS), pages

Observations.
489–496, 2009.

R. Givan, S. Leach, and T. Dean. Bounded-parameter Markov decision processes. Artiﬁcial

Intelligence, 122(1):71–109, 2000.

V. Goyal and J. Grand-Clement. Robust Markov Decision Process: Beyond Rectangularity.

Technical report, 2018.

G. Hanasusanto and D. Kuhn. Robust Data-Driven Dynamic Programming. In Advances

in Neural Information Processing Systems (NIPS), 2013.

T. Hansen, P. Miltersen, and U. Zwick. Strategy iteration is strongly polynomial for 2-
player turn-based stochastic games with a constant discount factor. Journal of the ACM
(JACM), 60(1):1–16, 2013.

T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. 2nd

edition, 2009.

37

C. P. Ho, M. Petrik, and W. Wiesemann. Fast Bellman Updates for Robust MDPs. In

International Conference on Machine Learning (ICML), pages 1979–1988, 2018.

G. N. Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):

257–280, 2005.

T. Jaksch, R. Ortner, and P. Auer. Near-optimal Regret Bounds for Reinforcement Learn-

ing. Journal of Machine Learning Research, 11(1):1563–1600, 2010.

D. L. Kaufman and A. J. Schaefer. Robust modiﬁed policy iteration. INFORMS Journal

on Computing, 25(3):396–410, 2013.

M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning

Research, 4:1107–1149, 2003.

Y. Le Tallec. Robust, Risk-Sensitive, and Data-driven Control of Markov Decision Processes.

PhD thesis, MIT, 2007.

S. Mannor, O. Mebel, and H. Xu. Lightning does not strike twice: Robust MDPs with
coupled uncertainty. In International Conference on Machine Learning (ICML), 2012.

S. Mannor, O. Mebel, and H. Xu. Robust MDPs with k-rectangular uncertainty. Mathe-

matics of Operations Research, 41(4):1484–1509, 2016.

K. Murphy. Machine Learning: A Probabilistic Perspective. 2012.

A. Nilim and L. El Ghaoui. Robust control of Markov decision processes with uncertain

transition matrices. Operations Research, 53(5):780–798, 2005.

M. Petrik. Approximate dynamic programming by minimizing distributionally robust

bounds. In International Conference of Machine Learning (ICML), 2012.

M. Petrik and R. H. Russell. Beyond Conﬁdence Regions: Tight Bayesian Ambiguity Sets

for Robust MDPs. Technical report, 2019.

M. Petrik and D. Subramanian. RAAM : The beneﬁts of robustness in approximating
aggregated MDPs in reinforcement learning. In Neural Information Processing Systems
(NIPS), 2014.

M. Petrik, Mohammad Ghavamzadeh, and Y. Chow. Safe Policy Improvement by Mini-
mizing Robust Baseline Regret. In Advances in Neural Information Processing Systems
(NIPS), 2016.

E. L. Porteus. Foundations of Stochastic Inventory Theory. Stanford Business Books, 2002.

I. Post and Y. Ye. The simplex method is strongly polynomial for deterministic Markov

decision processes. Mathematics of Operations Research, 40(4):859–868, 2015.

M. Puterman and M. Shin. Modiﬁed policy iteration algorithms for discounted Markov

decision problems. Management Science, 24(11):1127–1137, 1978.

38

M. L. Puterman. Markov decision processes: Discrete stochastic dynamic programming.

2005.

M. L. Puterman and S. L. Brumelle. On the convergence of policy iteration in stationary

dynamic programming. Mathematics of Operations Research, 4(1):60–69, 1979.

R. T. Rockafellar. Convex Analysis, 1970.

R. Russel, B. Behzadian, and M. Petrik. Optimizing Norm-bounded Weighted Ambiguity
Sets for Robust MDPs. Technical Report NeurIPS Workshop on Safe and Robust Decision
Making, 2019.

J. Satia and R. Lave. Markovian decision processes with uncertain transition probabilities.

Operations Research, 21:728–740, 1973.

A. L. Strehl, L. Li, and M. Littman. Reinforcement learning in ﬁnite MDPs: PAC analysis.

Journal of Machine Learning Research, 10:2413–2444, 2009.

M. A. Taleghan, T. G. Dietterich, M. Crowley, K. Hall, and H. J. Albers. PAC Optimal
MDP Planning with Application to Invasive Species Management. Journal of Machine
Learning Research, 16:3877–3903, 2015.

A. Tamar, S. Mannor, and H. Xu. Scaling up Robust MDPs Using Function Approximation.

In International Conference of Machine Learning (ICML), 2014.

J. Thai, C. Wu, A. Pozdnukhov, and A. Bayen. Projected sub-gradient with l1or simplex
constraints via isotonic regression. In IEEE Conference on Decision and Control (CDC),
pages 2031–2036, 2015.

E. van den Berg and M. P. Friedlander. Sparse Optimization with Least-Squares Con-

straints. SIAM Journal on Optimization, 21(4):1201–1229, 2011.

R. J. Vanderbei. Linear Programming: Foundations and Extensions, volume 49. Springer,

2nd edition, 1998.

T. Weissman, E. Ordentlich, G. Seroussi, S. Verdu, and M. J. Weinberger. Inequalities for

the L1 deviation of the empirical distribution. 2003.

C. White and H. Eldeib. Markov decision processes with imprecise transition probabilities.

Operations Research, 42(4):739–749, 1994.

W. Wiesemann, D. Kuhn, and B. Rustem. Robust Markov decision processes. Mathematics

of Operations Research, 38(1):153–183, 2013.

R. J. R. Williams and L. C. L. Baird. Tight performance bounds on greedy policies based
In Yale Workshop on Adaptive and Learning Systems.

on imperfect value functions.
Northeastern University, 1993.

H. Xu and S. Mannor. The robustness-performance tradeoﬀ in Markov decision processes.

In Advances in Neural Information Processing Systems (NIPS), 2006.

39

H. Xu and S. Mannor. Parametric regret in uncertain Markov decision processes. In IEEE

Conference on Decision and Control (CDC), pages 3606–3613, 2009.

P. H. Zipkin. Foundations of Inventory Management. 2000.

Appendix A. Properties of Robust Bellman Operator

We prove several fundamental properties of the robust Bellman policy update Lπ and the
robust Bellman optimality operator L over s-rectangular and sa-rectangular ambiguity sets.

Proposition 6. For both s-rectangular and sa-rectangular ambiguity sets, the robust Bell-
man policy update Lπ and the robust Bellman optimality operator L are γ-contractions
under the L8-norm, that is

}Lπx ´ Lπy}8 ď γ }x ´ y}8

and

}Lx ´ Ly}8 ď γ }x ´ y}8 .

The equations Lπv “ v and Lv “ v have the unique solutions vπ and v‹, respectively.

Proof. See Theorem 3.2 of Iyengar (2005) for sa-rectangular sets and Theorem 4 of Wiese-
(cid:4)
mann et al. (2013) for s-rectangular sets.

Proposition 7. For both s-rectangular and sa-rectangular ambiguity sets, the robust Bell-
man policy update Lπ and the robust Bellman optimality operator L are monotone:

Lπx ě Lπy

and

Lx ě Ly

@x ě y .

Proof. We show the statement for s-rectangular ambiguity sets; the proof of sa-rectangular
uncertainty sets is analogous. Consider π P Π as well as x, y P RS such that x ě y and
deﬁne

ÿ

Fspp, xq “

aPA

πs,a ¨ pT

a prs,a ` γ ¨ xq .

The monotonicity of the robust Bellman policy update Lπ follows from the fact that

pLπxqs “ min
pPPs

Fspp, xq “ Fspp‹, xq ě Fspp‹, yq

(a)
ě pLπyqs

@s P S ,

where p‹ P arg minpPPs Fspp, xq. The inequality (a) holds because Fspp‹, ¨q is monotone
since p‹ ě 0.

To prove the monotonicity of the robust Bellman optimality operator L, consider again
some x and y with x ě y and let π‹ be the greedy policy satisfying Ly “ Lπ‹y. We then
have that

pLyqs “ pLπ‹yqs ď pLπ‹xqs ď pLxqs,

where the inequalities follow from the (previously shown) monotonicity of Lπ‹ and the fact
(cid:4)
that pLxqs “ pmaxπPΠ Lπxqs ě pLπ‹xqs.

40

Proposition 6 and 7 further imply the following two properties of Lπ and L.

Corollary 3. For both s-rectangular and sa-rectangular ambiguity sets, the robust Bellman
policy update Lπ and the robust Bellman optimality operator L satisfy v‹ ě vπ for each
π P Π.

Proof. The corollary follows from the monotonicity (Proposition 7) and contraction prop-
erties (Proposition 6) of L and Lπ using standard arguments. See, for example, Proposition
(cid:4)
2.1.2 in Bertsekas (2013).

Corollary 4. For both s-rectangular and sa-rectangular ambiguity sets, the robust Bellman
policy update Lπ and the robust Bellman optimality operator L satisfy for any v P RS that

}v‹ ´ v}8 ď

1
1 ´ γ

}Lv ´ v}8 and

}vπ ´ v}8 ď

1
1 ´ γ

}Lπv ´ v}8 .

Proof. The corollary follows from the monotonicity (Proposition 7) and contraction prop-
erties (Proposition 6) of L and Lπ using standard arguments. See, for example, Proposition
(cid:4)
2.1.1 in Bertsekas (2013).

We next show that both Lπ and L are invariant when adding a constant to the value
function.

Lemma 4. For both s-rectangular and sa-rectangular ambiguity sets, the robust Bellman
policy update Lπ and the robust Bellman optimality operator L are translation invariant for
each π P Π:

Lπpv ` (cid:15) ¨ 1q “ Lπv ` γ(cid:15) ¨ 1 and Lpv ` (cid:15) ¨ 1q “ Lv ` γ(cid:15) ¨ 1

@v P RS, @(cid:15) P R

Proof. We show the statement for s-rectangular ambiguity sets; the proof of sa-rectangular
uncertainty sets is analogous. Fixing π P Π, v P RS and (cid:15) P R, we have

ÿ

pLπpv ` (cid:15)1qqs “ min
pPPs

“ min
pPPs

aPA
ÿ

aPA

πs,a ¨ pT

a prs,a ` γ ¨ rv ` (cid:15) ¨ 1sq

a prs,a ` γ ¨ vq ` γ(cid:15)q

πs,a ¨ ppT
ÿ

“ γ(cid:15) ` min
pPPs

aPA

πs,a ¨ pT

a prs,a ` γ ¨ vq ,

where the ﬁrst identity holds by deﬁnition of Lπ, the second is due to the fact that pT
since Ps Ď p∆SqA, and the third follows from the fact that
To see that Lpv ` (cid:15) ¨ 1q “ Lv ` γ(cid:15) ¨ 1, we note that

aPA πs,a “ 1.

ř

a 1 “ 1

Lpv ` (cid:15) ¨ 1q “ Lπ1pv ` (cid:15) ¨ 1q “ Lπ1v ` γ(cid:15) ¨ 1 ď Lv ` γ(cid:15) ¨ 1 ,

where π1 P Π is the greedy policy that satisﬁes Lπ1pv ` (cid:15) ¨ 1q “ Lpv ` (cid:15) ¨ 1q, as well as

Lv ` γ(cid:15) ¨ 1 “ Lπ2v ` γ(cid:15) ¨ 1 “ Lπ2pv ` (cid:15) ¨ 1q ď Lpv ` (cid:15) ¨ 1q ,

where π2 P Π is the greedy policy that satisﬁes Lπ2v “ Lv.

(cid:4)

41

Our last result in this section shows that the diﬀerence between applying the robust Bellman
policy update Lπ to two value functions can be bounded from below by a linear function.

Lemma 5. For both s-rectangular and sa-rectangular ambiguity sets, there exists a stochas-
tic matrix P such that the robust Bellman policy update Lπ satisﬁes

for each π P Π and x, y P RS.

Lπx ´ Lπy ě γ ¨ P px ´ yq ,

Proof. We show the statement for s-rectangular ambiguity sets; the proof of sa-rectangular
uncertainty sets is analogous. We have that

pLπx ´ Lπyqs “ min
pPPs

ě min
pPPs

“ min
pPPs

#

ÿ

#

aPA
ÿ

#

aPA
ÿ

aPA

+

#

ÿ

´ min
pPPs

πs,a ¨ pT

a prs,a ` γ ¨ xq

´
πs,a ¨ pT

¯
a prs,a ` γ ¨ xq

´

+

ÿ

aPA

πs,a ¨ γ ¨ pT

a px ´ yq

.

+

πs,a ¨ pT

a prs,a ` γ ¨ yq
+

aPA

´
πs,a ¨ pT

¯
a prs,a ` γ ¨ yq

The result follows by constructing the stochastic matrix P such that its s-th row is
pT

a where pa is the optimizer in the last minimization above.

ř

aPA πs,a¨
(cid:4)

Appendix B. Bisection Algorithm with Quasi-Linear Time Complexity

Ta`1. We use this data as input for our revised bisection scheme in Algorithm 5.

We adapt Algorithm 4 to determine the optimal solution to problem (21) in quasi-linear time
without dependence on any precision (cid:15). Recall that Algorithm 2 computes the breakpoints
t qt, t “ 0, . . . , Ta `1, Ta ď S2, of each function
t qt, t “ 0, . . . , Ta `1 and objective values pqa
pξa
qa, a P A. Moreover, each inverse function q´1
is also piecewise aﬃne with breakpoints pqa
t qt,
a
t “ 0, . . . , Ta ` 1 and corresponding function values ξa
a puq “ 8 for
u ă qa
t , t “ 0, . . . Ta ` 1 and a P A, of the inverse
Algorithm 5 ﬁrst combines all breakpoints qa
a , a P A, to a single list K in ascending order. It then bisects on the indices of
functions q´1
these breakpoints. The result is a breakpoint pair pkmin, kmaxq satisfying kmax “ kmin ` 1
‰
a have a
a pˆqkmaxq
as well as κ P
breakpoint between ˆqkmin and ˆqkmax, ﬁnding the optimal solution u‹ to problem (7) then
reduces to solving a single linear equation in one unknown, which is done in the last part
of Algorithm 5.

. Since none of the functions q´1

t q, as well as q´1

a pˆqkminq,

t “ q´1

aPA q´1

aPA q´1

a pqa

“ř

ř

The complexity of Algorithm 5 is dominated by the merging of the sorted lists pqa
t qt“0,...Ta`1,
a P A, as well as the computation of s inside the while-loop. Merging A sorted lists, each
of size less than or equal to CS, can be achieved in time OpCSA log Aq. However, each
one of these lists needs to be also sorted in Algorithm 2 giving the overall complexity of
OpCSA log CSAq. Then, computing q´1
a at a given point can be achieved in time Oplog CSq,

42

Algorithm 5: Quasi-linear time bisection scheme for solving (7)

t qt“0,...,Ta`1, of all functions qa, a P A

Input: Breakpoints pqa
Output: The optimal solution u‹ to the problem (21)
Combine qa
order, omitting any duplicates ;

t , t “ 0, . . . , Ta and a P A, to a single list K “ pˆq1, . . . , ˆqKq in ascending

// Bisection search to find the optimal line segment pkmin, kmaxq
kmin Ð 1; kmax Ð K ;
while kmax ´ kmin ą 1 do

Split tkmin, . . . , kmaxu in half: k Ð roundppkmin ` kmaxq{2q ;
Calculate the budget required to achieve u “ ˆqk: s Ð
if s ď κ then

ř

aPA q´1

a pˆqkq ;

u “ ˆqk is feasible: update the feasible upper bound: kmax Ð k ;

else

u “ ˆqk is infeasible: update the infeasible lower bound: kmin Ð k ;

end

end

are affine on pˆqkmin, ˆqkmaxq

umax Ð ˆqkmax ;

ř

a
ř

// All q´1
umin Ð ˆqkmin;
a puminq; smax Ð
smin Ð
α Ð pκ ´ sminq{psmax ´ sminq ;
u‹ Ð p1 ´ αq ¨ umin ` α ¨ umax;
return u‹

aPA q´1

aPA q´1

a pumaxq ;

so that s in an individual iteration of the while-loop can be computed in time OpA log CSq.
Since the while-loop is executed Oplog CSAq many times, computing s has an overall com-
plexity of OpA log CS log CSAq. We thus conclude that Algorithm 5 has a complexity of
OpCSA log A ` A log CS log CSAq.

Appendix C. Computing the Bellman Operator via Linear Programming

In this section we present an LP formulation for the robust s-rectangular Bellman optimality
operator L deﬁned in (7):

pLvqs “ max
dP∆A

min
pPp∆S qA

#

ÿ

aPA

da ¨ pT

a za |

ÿ

aPA

+

(cid:107)pa ´ ¯ps,a(cid:107)1,ws,a ď κs

43

Here, we use za “ rs,a`γ¨v in the objective function. Employing an epigraph reformulation,
the inner minimization problem can be re-expressed as the following linear program:

min
pPRAˆS ,θPRAˆS

subject to

ÿ

da ¨ zT

a pa

aPA
1Tpa “ 1
pa ´ ¯pa ě ´θa
¯pa ´ pa ě ´θa
wT

ÿ

´

a θa ě ´κ

@a P A
@a P A
@a P A

rxas
ryn
a s
ryp
as
rλs

aPA
p ě 0,

θ ě 0

For ease of exposition, we have added the dual variables corresponding to each constraint
in brackets. This linear program is feasible by construction, which implies that its optimal
value coincides with the optimal value of its dual. We can thus dualize this linear program
and combine it with the outer maximization to obtain the following linear programming
reformulation of the the robust s-rectangular Bellman optimality operator L:

max
dPRA,xPRA, λPR
ypPRSˆA,ynPRSˆA
subject to

ÿ

´

¯

xa ` ¯pT

a ryn

a ´ yp
as

´ κ ¨ λ

a ` yn

aPA
1Td “ 1, d ě 0
´yp
yp
a ` yn
yp ě 0 yn ě 0
λ ě 0

a ` x ¨ 1 ď daza

a ´ λ ¨ wa ď 0

@a P A
@a P A

This problem has OpSAq variables and an input bitlength of OpSAq. As such, its theoretical
runtime complexity is OpS4.5A4.5q.

44

