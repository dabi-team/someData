9
1
0
2

g
u
A
7
2

]

G
L
.
s
c
[

2
v
4
6
3
9
0
.
8
0
9
1
:
v
i
X
r
a

Adversarial Edit Attacks for Tree Data

Benjamin Paaßen

CITEC Center of Excellence, Bielefeld University∗

Preprint of the IDEAL 2019 paper Paaßen [2019] as provided by the authors.

Abstract

Many machine learning models can be attacked with adversarial examples, i.e. in-
puts close to correctly classiﬁed examples that are classiﬁed incorrectly. However, most
research on adversarial attacks to date is limited to vectorial data, in particular image
data. In this contribution, we extend the ﬁeld by introducing adversarial edit attacks
for tree-structured data with potential applications in medicine and automated program
analysis. Our approach solely relies on the tree edit distance and a logarithmic number
of black-box queries to the attacked classiﬁer without any need for gradient information.
We evaluate our approach on two programming and two biomedical data sets and
show that many established tree classiﬁers, like tree-kernel-SVMs and recursive neural
networks, can be attacked eﬀectively.

1 Introduction

In recent years, multiple papers have demonstrated that machine learning classiﬁers can be
fooled by adversarial examples, i.e. an example x′ that is close to a correctly classiﬁed data
point x, but is classiﬁed incorrectly [Akhtar and Mian, 2018, Madry et al., 2018]. The threat
of such attacks is not to be underestimated, especially in security-critical applications such as
medicine or autonomous driving, where adversarial examples could lead to misdiagnoses or
crashes [Eykholt et al., 2018].

Despite this serious threat to all classiﬁcation models, existing research has almost ex-
clusively focused on image data [Akhtar and Mian, 2018, Madry et al., 2018], with the no-
table exceptions of a few contributions on audio data [Carlini and Wagner, 2018], text data
[Ebrahimi et al., 2018], and graph data [Dai et al., 2018, Zügner et al., 2018]. In particular,
no adversarial attack approach has yet been developed for tree data, such as syntax trees
of computer programs or biomedical molecules. Furthermore, all attack approaches for non-
image data to date rely on knowledge about the classiﬁer architecture and/or gradient, which
may not always be available [Madry et al., 2018].

In this paper, we address both issues by introducing adversarial edit attacks, a novel
black-box attack scheme for tree data. In particular, we propose to select for a point x a
neighboring point with a diﬀerent label y, compute the tree edits necessary to change x into
y, and applying the minimum number of edits which still change the classiﬁer output.

Our paper is structured as follows. We ﬁrst introduce background and related work
on adversarial examples, then introduce our adversarial attack method, and ﬁnally evaluate
our method by attacking seven diﬀerent tree classiﬁers on four tree data sets, two from the
programming domain and two from the biomedical domain.

∗Support by the Bielefeld Young Researchers Fund is gratefully acknowledged.

1

 
 
 
 
 
 
Preprint of Paaßen [2019] provided by the authors.

2

2 Related Work

Following Szegedy et al. [Szegedy et al., 2014], we deﬁne an adversarial example for some
data point x ∈ X and a classiﬁer f : X → {1, . . . , L} and a target label ℓ ∈ {1, . . . , L} as the
solution z to the following optimization problem

min
z∈X ,s.t.f (z)=ℓ

d(z, x)2,

(1)

where d is a distance on the data space X . In other words, z is the closest data point to x
which is still classiﬁed as ℓ. For image data, the distance d(z, x) is often so small that z and
x look exactly the same to human observers [Szegedy et al., 2014].

Note that Problem 1 is hard to solve because X is typically high dimensional and the
constraint f (z) = ℓ is discrete. Accordingly, the problem has been addressed with heuristic
approaches, such as the fast gradient sign method [Goodfellow et al., 2015], which changes
x along the sign of the gradient of the classiﬁer loss; or Carlini-Wagner attacks, which
incorporate the discrete label constraint as a diﬀerentiable term in the objective function
[Carlini and Wagner, 2017]. We call these methods white-box because they all rely on knowl-
edge of the architecture and/or gradient ∇zf (z) of the classiﬁer.
In contrast, there also
exist black-box attack methods, which only need to query f itself, such as one-pixel at-
tacks, which are based on evolutionary optimization instead of gradient-based optimization
[Akhtar and Mian, 2018, Su et al., 2017].

In the realm of non-image data, prior research has exclusively focused on white-box attacks
for speciﬁc data types and/or models. In particular, [Carlini and Wagner, 2018] consider audio
ﬁles, relying on decibels and the CTC loss as measure of distance; [Ebrahimi et al., 2018]
attack text data by inferring single character replacements that increase the classiﬁcation
loss; and [Dai et al., 2018, Zügner et al., 2018] attack graph data by inferring edge deletions
or insertions which fool a graph convolutional neural network model.

Our own approach is related to [Carlini and Wagner, 2018], in that we rely on an align-
ment between two inputs to construct adversarial examples, and to [Ebrahimi et al., 2018], in
that we consider discrete node-level changes, i.e. node deletions, replacements, or insertions.
However, in contrast to these prior works, our approach is black-box instead of white-box and
works in tree data as well as sequence data.

3 Method

To develop an adversarial attack scheme for tree data, we face two challenges. First, Problem 1
requires a distance function d for trees. Second, we need a method to apply small changes to
a tree x in order to construct an adversarial tree z. We can address both challenges with the
tree edit distance, which is deﬁned as the minimum number of node deletions, replacements,
or insertions needed to change a tree into another [Zhang and Shasha, 1989] and thus provides
both a distance and a change model.

Formally, we deﬁne a tree over some ﬁnite alphabet A recursively as an expression T =
x(T1, . . . , Tm), where x ∈ A and where T1, . . . , Tm is a (possibly empty) list of trees over A.
We denote the set of all trees over A as T (A). As an example, a(), a(b), and a(b(a, a), a)
are both trees over the alphabet A = {a, b}. We deﬁne the size of a tree T = x(T1, . . . , Tm)
recursively as |T | := 1 + P

m
c=1 |Tc|.

Next, we deﬁne a tree edit δ over alphabet A as a function δ : T (A) → T (A). In more
detail, we consider node deletions deli, replacements repi,a, and insertions insi,c,C,a, which
respectively delete the ith node in the input tree and move its children up to the parent,

Preprint of Paaßen [2019] provided by the authors.

3

a

del→2

a

repf→4

a

ins2,1,g→1

a

b

e

edc

fdc

f

eg

dc

d

Figure 1: An illustration of the eﬀect of the tree edit script ¯δ = del2, rep4,f, ins1,2,1,g on the
tree (a(b(c,d), e)). We ﬁrst delete the second node of the tree, then replace the fourth node
with an f, and ﬁnally insert a g as second child of the ﬁrst node, using the former second child
as grandchild.

z2 X

y

x

z1 ×

Figure 2: Two adversarial attack attempts, one random (z1) and one backtracing attack (z2).
z1 is constructed by moving randomly in the space of possible trees until the label changes.
z2 is constructed by moving along the connecting line to the closest neighbor with diﬀerent
label y until the label changes. z1 is not counted as successful, because it is closer to y than
to x, whereas z2 is counted as successful. The background pattern indicates the predicted
label of the classiﬁer.

relabel the ith node in the input tree with symbol a ∈ A, and insert a new node with label a
as cth child of node i, moving former children c, . . . , c + C down. Figure 1 displays the eﬀects
of each edit type.

We deﬁne an edit script as a sequence ¯δ = δ1, . . . , δn of tree edits δj and we deﬁne the
application of ¯δ to a tree T recursively as ¯δ(T ) := (δ2, . . . , δn)(cid:0)δ1(T )(cid:1). Figure 1 displays an
example edit script.

Finally, we deﬁne the tree edit distance d(x, y) as the length of the shortest script which
transforms x into y, i.e. d(x, y) := min¯δ:¯δ(x)=y |¯δ|. This tree edit distance can be computed
eﬃciently via dynamic programming in O(|x|2 · |y|2) [Zhang and Shasha, 1989]. We note that
several variations of the tree edit distance with other edit models exist, which are readily
compatible with our approach [Bille, 2005, Paaßen et al., 2018]. For brevity, we focus on the
classic tree edit distance in this paper.

Random baseline attack: The concept of tree edits yields a baseline attack approach for
trees. Starting from a tree x with label f (x), we apply random tree edits, yielding another
tree z, until f (z) 6= f (x). To make this more eﬃcient, we double the number of edits in
each iteration until f (z) 6= f (x), yielding an edit script ¯δ = δ1, . . . , δn, and then use binary
search to identify the shortest preﬁx ¯δj := δ1, . . . , δj such that f (cid:0)(δ1, . . . , δj)(x)(cid:1) 6= f (x). This
reduced the number of queries to O(log(n)).

Note that this random attack scheme may ﬁnd solutions z which are far away from x,
thus limiting the plausibility as adversarial examples. To account for such cases, we restrict
Problem 1 further and impose that z only counts as a solution if z is still closer to x than to
any point y which is correctly classiﬁed and has a diﬀerent label than x (refer to Figure 2).

Another drawback of our random baseline is that it can not guarantee results after a

Preprint of Paaßen [2019] provided by the authors.

4

ﬁxed amount of edits because we may not yet have explored enough trees to have crossed
the classiﬁcation boundary. We address this limitation with our proposed attack method,
backtracing attacks.

Backtracing attack: For any two trees x and y, we can compute a co-optimal edit script
¯δ with ¯δ(x) = y and |¯δ| = d(x, y) in O(|x| · |y| · (|x| + |y|)) via a technique called backtracing
[Paaßen, 2018, refer to Algorithm 6 and Theorem 16]. This forms the basis for our proposed
attack. In particular, we select for a starting tree x the closest neighbor y with the target
label ℓ, i.e. f (y) = ℓ. Then, we use backtracing to compute the shortest script ¯δ from x to
y. This script is guaranteed to change the label at some point. We then apply binary search
to identify the shortest preﬁx of ¯δ which still changes the label (refer to Figure 2). Refer to
Algorithm 1 for the details of the algorithm.

Algorithm 1 A targeted adversarial edit algorithm which transforms the input tree x to
move it closer to a reference tree y with the desired target label ℓ. The backtracing algorithm
for the tree edit distance ted-backtrace is described in [Paaßen, 2018].
1: function targeted(A tree x, a classiﬁer f , and a reference tree y with f (y) = ℓ.)
2:

δ1, . . . , δn ← ted-backtrace(x, y). lo ← 1. hi ← n.
while lo < hi do

3:

4:
5:

6:

7:

8:

9:

10:

2 · (lo + hi)⌋. z ← (δ1, . . . , δj)(x).

j ← ⌊ 1
if f (z) 6= ℓ then
lo ← j + 1.

else

hi ← j.

end if
end while
return (δ1, . . . , δhi)(x)

11:
12: end function

Note that we can upper-bound the length of ¯δ by |x| + |y|, because at worst we delete x
entirely and then insert y entirely. Accordingly, our attack ﬁnishes after at most O(cid:0) log(|x| +
|y|)(cid:1) steps/queries to f . Finally, because y is the closest tree with label ℓ to x, our attack
is guaranteed to yield a successful adversarial example if our preﬁx is shorter than half of ¯δ,
because then d(x, z) = |pref ix| < 1
2 (cid:0)d(x, z) + d(z, y)(cid:1), which implies that
d(x, z) < d(z, y). In other words, we are guaranteed to ﬁnd a solution to problem 1, in the
sense that our our label is guaranteed to change to ℓ, and that our solution is closest to x
along the shortest script ¯δ towards y.

2 d(x, y) = 1

2 |¯δ| = 1

4 Experiments

In our evaluation, we attack seven diﬀerent tree classiﬁers on four data sets. As outcome
measures, we consider the success rate, i.e. the fraction of test data points for which the
attack could generate a successful adversarial example according to the deﬁnition in Figure 2;
and the distance ratio d(z, x)/d(z, y), i.e. how much closer z is to x compared to other points y
with the same label as z. To avoid excessive computation times, we abort random adversarial
attacks that have not succeeded after 100 tree edits. Accordingly, the distance ratio is not
available for random attacks that have been aborted, yielding some n.a. entries in our results
(Table 1).

Preprint of Paaßen [2019] provided by the authors.

5

Our experimental hypotheses are that backtracing attacks succeed more often than random
attacks due to their targeted nature (H1), but that random attacks have lower distance ratios
(H2), because they have a larger search space from which to select close adversarials.

Datasets: We perform our evaluation on four tree classiﬁcation data sets from [Gallicchio and Micheli,
2013, Paaßen et al., 2018], in particular MiniPalindrome and Sorting as data sets of Java pro-
grams, as well as Cystic and Leukemia from the biomedical domain. The number of trees in
each data set are 48, 64, 160, and 442 respectively. The latter three data sets are (imbalanced)
binary classiﬁcation problems, the ﬁrst is a six-class problem. We perform all experiments in
a crossvalidation with 6, 8, 10, and 10 folds for the respective data sets, following the protocol
of [Paaßen et al., 2018].

Classiﬁers: On each data set, we train seven diﬀerent classiﬁers, namely ﬁve support
vector machines (SVM) with diﬀerent kernels and two recursive neural network types. As
the ﬁrst two kernels, we consider the double centering kernel (linear ; [Gisbrecht and Schleif,
2015]) based on the tree edit distance, and the radial basis function kernel (RBF ) k(x, y) =
2 · d(x, y)2/σ2(cid:1), for which we optimize the bandwidth parameter σ ∈ R+ in a nested
exp (cid:0) − 1
crossvalidation in the range {0.5, 1, 2} · ¯d, where ¯d is the average tree edit distance in the data
set. We ensure positive semi-deﬁniteness for these kernels via the clip eigenvalue correction
[Gisbrecht and Schleif, 2015]. Further, we consider three tree kernels, namely the subtree ker-
nel (ST ), which counts the number of shared proper subtrees, the subset tree kernel (SST ),
which counts the number of shared subset trees, and the partial tree kernel (PT ), which
counts the number of shared partial trees [Aiolli et al., 2011]. All three kernels have a de-
cay hyper-parameter λ, which regulates the inﬂuence of larger subtrees. We optimize this
hyper-parameter in a nested crossvalidation for each kernel in the range {0.001, 0.01, 0.1}.
For all SVM instances, we also optimized the regularization hyper-parameter C in the range
{0.1, 1, 10, 100}.

As neural network variations, we ﬁrst consider recursive neural networks (Rec; [Sperduti and Starita,

m

1997]), which map a tree x(T1, . . . , Tm) to a vector by means of the recursive function
i=1 G(Ti) + ~bx(cid:1), where sigm(a) := 1/(1 + exp(−a)) is
G(x(T1, . . . , Tm)) := sigm(cid:0)W x · P
the logistic function and W x ∈ Rn×n as well as ~bx ∈ Rn for all x ∈ A are the parameters of
the model. We classify a tree by means of another linear layer with one output for each of
the L classes, i.e. f (T ) := argmaxℓ[V · G(T ) + ~c]ℓ, where V ∈ RL×n and ~c ∈ RL are param-
eters of the model and [~v]ℓ denotes the ℓth entry of vector ~v. We trained the network using
the crossentropy loss and Adam[Kingma and Ba, 2015] as optimizer until the training loss
dropped below 0.01. Note that the number of embedding dimensions n is a hyper-parameter
of the model, which we ﬁxed here to n = 10 as this was suﬃcient to achieve the desired
training loss. Finally, we consider tree echo state networks (TES ; [Gallicchio and Micheli,
2013]), which have the same architecture as recursive neural networks, but where the recur-
sive weight matrices W x ∈ Rn×n and the bias vectors ~bx ∈ Rn remain untrained after random
initialization. Only the output parameters V and ~c are trained via simple linear regression.
The scaling of the recursive weight matrices and n are hyper-parameters of the model, which
we optimized in a nested crossvalidation via grid search in the ranges {0.7, 0.9, 1, 1.5, 2} and
{10, 50, 100} respectively.

As implementations, we use the scikit-learn version of SVM, the edist package for the
tree edit distance and its backtracing1, the ptk toolbox2 for the ST, SST, and PT kernels
[Aiolli et al., 2011], a custom implementation of recursive neural networks using pytorch[Paszke et al.,

1https://gitlab.ub.uni-bielefeld.de/bpaassen/python-edit-distances
2http://joedsm.altervista.org/pythontreekernels.htm

Preprint of Paaßen [2019] provided by the authors.

6

2017], and a custom implementation of tree echo state networks3. We perform all experiments
on a consumer grade laptop with an Intel i7 CPU.

Results and Discussion: Table 1 displays the mean classiﬁcation error ± standard devia-
tion in crossvalidation, as well as the success rates and the distance ratios for random attacks
and backtracing attacks for all data sets and all classiﬁers.

We evaluate our results statistically by aggregating all crossvalidation folds across data
sets and comparing success rates and distance rations between in a a one-sided Wilcoxon
sign-rank test with Bonferroni correction. We observe that backtracing attacks have higher
success rates for the linear and RBF kernel SVM (p < 10−5), slightly higher rates for the ST
and SST kernels (p < 0.05), indistinguishable success for the PT kernel, and lower success
rates for the recursive and tree echo state networks (p < 0.01). This generally supports our
hypothesis that backtracing attacks have higher success rates (H1), except for both neural
network models. This is especially pronounced for Cystic and Leukemia data sets, where
random attacks against SVM models always failed.

Regarding H2, we observe that random attacks achieve lower distance ratios for the ST,
SST, and PT kernels (p < 0.01), and much lower ratios for recursive neural nets and tree echo
state nets (p < 10−5). For the linear and RBF kernel, the distance ratios are statistically
indistinguishable. This supports H2.

5 Conclusion

In this contribution, we have introduced a novel adversarial attack strategy for tree data based
on tree edits in one random and one backtracing variation. We observe that backtracing at-
tacks achieve more consistent and reliable success across data sets and classiﬁers compared to
the random baseline. Only for recursive neural networks are random attacks more successful.
We also observe that the search space for backtracing attacks may be too constrained because
random attacks generally ﬁnd adversarials that are closer to the original sample. Future
research could therefore consider alternative search spaces, e.g. based on semantic consider-
ations. Most importantly, our research highlights the need for defense mechanisms against
adversarial attacks for tree classiﬁers, especially neural network models.

References

Fabio Aiolli, Giovanni Da San Martino, and Alessandro Sperduti. Extending tree kernels
with topological information. In Timo Honkela, Wlodzislaw Duch, Mark A. Girolami, and
Samuel Kaski, editors, Proceedings of the 21st International Conference on Artiﬁcial Neural
Networks (ICANN 2011), pages 142–149, 2011. doi:10.1007/978-3-642-21735-7_18.

Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer
vision: A survey. IEEE Access, 6:14410–14430, 2018. doi:10.1109/ACCESS.2018.2807385.

Philip Bille. A survey on tree edit distance and related problems. Theoretical Computer

Science, 337(1):217 – 239, 2005. doi:10.1016/j.tcs.2004.12.030.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In
Proceedings of the 2017 IEEE Symposium on Security and Privacy (SP 2017), pages 39–57,
2017. doi:10.1109/SP.2017.49.

3All implementations and experiments are available at https://gitlab.ub.uni-bielefeld.de/bpaassen/adversarial-edit-attacks

Preprint of Paaßen [2019] provided by the authors.

7

Table 1: The unattacked classiﬁcation accuracy (higher is better), attack success rate (higher
is better), and distance ratio d(z, x)/d(z, y) between the adversarial example z, the original
point x, and the closest point y to z with the same label (lower is better) for all classiﬁers
and all data sets. Classiﬁers and data sets are listed as rows, attack schemes as columns. All
values are averaged across crossvalidation folds and listed ± standard deviation. The highest
success rate and lowest distance ratio in each column is highlighted via bold print.
If all
attacks failed, results are listed as n.a.

classiﬁer

no attack
accuracy

random

backtracing

success rate

dist. ratio

success rate

dist. ratio

0.09 ± 0.09
0.96 ± 0.06
1.00 ± 0.00
0.06 ± 0.06
0.88 ± 0.07 0.86 ± 0.08 0.29 ± 0.05
0.96 ± 0.06 0.78 ± 0.15 0.36 ± 0.08
0.96 ± 0.06 0.80 ± 0.07 0.35 ± 0.10
0.85 ± 0.13
0.72 ± 0.14
0.92 ± 0.06 0.95 ± 0.07 0.08 ± 0.03

MiniPalindrome
0.24 ± 0.07 0.52 ± 0.15
0.27 ± 0.21 0.52 ± 0.17
0.72 ± 0.10
0.54 ± 0.11
0.54 ± 0.11
0.17 ± 0.05 0.79 ± 0.08
0.71 ± 0.09

Sorting

0.02 ± 0.04
0.94 ± 0.06
0.94 ± 0.06
0.18 ± 0.14
0.81 ± 0.16 0.65 ± 0.09 0.20 ± 0.05
0.42 ± 0.17
0.89 ± 0.10
0.88 ± 0.12
0.42 ± 0.14
0.87 ± 0.01 0.64 ± 0.20 0.44 ± 0.07
0.70 ± 0.15 0.84 ± 0.11 0.21 ± 0.08

0.86 ± 0.00 0.44 ± 0.16
0.57 ± 0.07 0.42 ± 0.16
0.61 ± 0.17
0.50 ± 0.14 0.49 ± 0.17
0.52 ± 0.17 0.50 ± 0.14
0.26 ± 0.17
0.20 ± 0.16

2.68 ± 3.54
1.44 ± 0.51
0.93 ± 0.15
1.91 ± 1.19
1.91 ± 1.19
1.26 ± 0.41
1.57 ± 0.81

1.55 ± 0.49
1.64 ± 0.47
3.01 ± 1.91
1.67 ± 0.52
1.69 ± 0.87
1.87 ± 0.59
2.40 ± 0.88

Cystic

0.00 ± 0.00
0.72 ± 0.09
0.00 ± 0.00
0.74 ± 0.09
0.00 ± 0.00
0.75 ± 0.10
0.00 ± 0.00
0.72 ± 0.09
0.74 ± 0.08
0.00 ± 0.00
0.76 ± 0.11 0.46 ± 0.14 0.77 ± 0.09
0.71 ± 0.11 0.63 ± 0.11 0.62 ± 0.11

n.a.
n.a.
n.a.
n.a.
n.a.

0.14 ± 0.07 1.71 ± 0.65
0.22 ± 0.13 1.68 ± 0.56
0.49 ± 0.23 0.86 ± 0.24
0.34 ± 0.16 1.25 ± 0.32
0.35 ± 0.13 1.26 ± 0.44
1.45 ± 1.28
0.33 ± 0.10
1.23 ± 0.28
0.36 ± 0.17

Leukemia

0.00 ± 0.00
0.92 ± 0.04
0.00 ± 0.00
0.95 ± 0.03
0.00 ± 0.00
0.92 ± 0.03
0.00 ± 0.00
0.95 ± 0.03
0.95 ± 0.02
0.00 ± 0.00
0.93 ± 0.03 0.41 ± 0.07 0.73 ± 0.07
0.88 ± 0.02 0.69 ± 0.11 0.53 ± 0.04

n.a.
n.a.
n.a.
n.a.
n.a.

0.27 ± 0.17 3.20 ± 1.77
0.20 ± 0.08 2.88 ± 1.65
0.21 ± 0.09 2.64 ± 0.51
0.19 ± 0.10 2.57 ± 0.65
0.20 ± 0.10 2.54 ± 0.56
2.43 ± 0.64
0.24 ± 0.10
2.49 ± 1.11
0.36 ± 0.16

linear
RBF
ST
SST
PT
Rec
TES

linear
RBF
ST
SST
PT
Rec
TES

linear
RBF
ST
SST
PT
Rec
TES

linear
RBF
ST
SST
PT
Rec
TES

Preprint of Paaßen [2019] provided by the authors.

8

Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on
speech-to-text. In Gabriela Ciocarlie, editor, Proceedings of the 2018 IEEE Security and
Privacy Workshops (SPW 2018), pages 1–7, 2018. doi:10.1109/SPW.2018.00009. URL
https://arxiv.org/abs/1801.01944.

Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial
attack on graph structured data. In Jennifer Dy and Andreas Krause, editors, Proceedings
of the 35th International Conference on Machine Learning (ICML 2018), pages 1115–1124.
PMLR, 2018. URL http://proceedings.mlr.press/v80/dai18b.html.

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. HotFlip: White-box adversarial ex-
amples for text classiﬁcation. In Claire Cardie, Iryna Gurevych, and Yosuke Miyao, editors,
Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics
(ACL 2018), pages 31–36, 2018. URL https://www.aclweb.org/anthology/P18-2006.

Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul
Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning
visual classiﬁcation. In Michael Brown, Bryan Morse, Shmuel Peleg, David Forsyth, Ivan
Laptev, Deva Ramanan, and Aude Oliva, editors, Proceedings of the 31st IEEE Conference
on Computer Vision and Pattern Recognition (CVPR 2018), pages 1625–1634, 2018. URL
https://arxiv.org/abs/1707.08945.

Claudio Gallicchio and Alessio Micheli. Tree echo state networks. Neurocomputing, 101:319 –

337, 2013. doi:10.1016/j.neucom.2012.08.017.

Andrej Gisbrecht and Frank-Michael Schleif. Metric and non-metric proximity transformations
at linear costs. Neurocomputing, 167:643–657, 2015. doi:10.1016/j.neucom.2015.04.017.
URL https://arxiv.org/abs/1411.1646.

Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing ad-
versarial examples.
In Yoshua Bengio and Yann LeCunn, editors, Proceedings of the
3rd International Conference on Learning Representations (ICLR 2015), 2015. URL
http://arxiv.org/abs/1412.6572.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCunn, editors, Proceedings of the 3rd International Conference on
Learning Representations (ICLR 2015), 2015. URL http://arxiv.org/abs/1412.6980.

Towards deep learning models

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
In
the 6th
URL

Vladu.
Yoshua Bengio, Yann LeCunn, and Tara Sainath, editors, Proceedings of
International Conference on Learning Representations (ICLR 2018), 2018.
https://openreview.net/forum?id=rJzIBfZAb.

to adversarial attacks.

resistant

Benjamin Paaßen. Revisiting the tree edit distance and its backtracing: A tutorial. CoRR,

abs/1805.06869, 2018. URL http://arxiv.org/abs/1805.06869.

Benjamin Paaßen. Adversarial edit attacks for tree data. In Hujun Yin, David Camacho, and
Peter Tino, editors, Proceedings of the 20th International Conference on Intelligent Data
Engineering and Automated Learning (IDEAL 2019), 2019. accepted.

Benjamin Paaßen, Claudio Gallicchio, Alessio Micheli, and Barbara Hammer. Tree Edit Dis-
tance Learning via Adaptive Symbol Embeddings. In Jennifer Dy and Andreas Krause, ed-
itors, Proceedings of the 35th International Conference on Machine Learning (ICML 2018),

Preprint of Paaßen [2019] provided by the authors.

9

volume 80 of Proceedings of Machine Learning Research, pages 3973–3982, 2018. URL
http://proceedings.mlr.press/v80/paassen18a.html.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
Auto-
DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
In Alex Wiltschko, Bart Van Merriënboer, and
matic diﬀerentiation in pytorch.
Pascal Lamblin, editors, Proceedings of
the Interna-
tional Conference on Neural Information Processing Systems (NIPS 2017), 2017. URL
https://openreview.net/forum?id=BJJsrmfCZ.

the 2nd Autodiﬀ Workshop at

Alessandro Sperduti and Antonina Starita.

Supervised neural networks for the classi-
IEEE Transactions on Neural Networks, 8(3):714–735, 1997.

ﬁcation of structures.
doi:10.1109/72.572108.

Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep
neural networks. CoRR, abs/1710.08864, 2017. URL http://arxiv.org/abs/1710.08864.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Good-
fellow, and Rob Fergus. Intriguing properties of neural networks. In Yoshua Bengio and
Yann LeCunn, editors, Proceedings of the 2nd International Conference on Learning Rep-
resentations (ICLR 2014), 2014. URL http://arxiv.org/abs/1312.6199.

Kaizhong Zhang and Dennis Shasha. Simple fast algorithms for the editing distance be-
tween trees and related problems. SIAM Journal on Computing, 18(6):1245–1262, 1989.
doi:10.1137/0218082.

Daniel Zügner, Amir Akbarnejad, and Stephan Günnemann. Adversarial attacks on neu-
In Proceedings of the 24th ACM International Confer-
ral networks for graph data.
ence on Knowledge Discovery & Data Mining (SIGKDD 2018), pages 2847–2856, 2018.
doi:10.1145/3219819.3220078. URL https://arxiv.org/abs/1805.07984.

