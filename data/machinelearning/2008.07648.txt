1
2
0
2

n
u
J

1
1

]

G
L
.
s
c
[

2
v
8
4
6
7
0
.
8
0
0
2
:
v
i
X
r
a

Nonparametric Learning of Two-Layer ReLU
Residual Units

Zhunxuan Wang∗
Amazon
London EC2A 2FA, United Kingdom
wzhunxua@amazon.com

Linyun He
Pennsylvania State University
State College, PA 16801, United States
ljh5602@psu.edu

Chunchuan Lyu
University of Edinburgh
Edinburgh EH8 9AB, United Kingdom
chunchuan.lv@gmail.com

Shay B. Cohen
University of Edinburgh
Edinburgh EH8 9AB, United Kingdom
scohen@inf.ed.ac.uk

Abstract

We describe an algorithm that learns two-layer residual units with rectiﬁed linear
unit (ReLU) activation: suppose the input x is from a distribution with support
space Rd and the ground-truth generative model is such a residual unit, given by

y = B∗

(A∗x)+ + x
i
h

,

∈

∈

∈
d and for c

Rd, [c+]i = max

Rm×d is full-rank with m

Rd×d is a nonnegative full-rank ma-
where ground-truth network parameters A∗
trix and B∗
.
0, ci}
≥
We design layer-wise objectives as functionals whose analytic minimizers express
the exact ground-truth network in terms of its parameters and nonlinearities. Fol-
lowing this objective landscape, learning residual units from ﬁnite samples can be
formulated using convex optimization of a nonparametric function: for each layer,
we ﬁrst formulate the corresponding empirical risk minimization (ERM) as a posi-
tive semi-deﬁnite quadratic program (QP), then we show the solution space of the
QP can be equivalently determined by a set of linear inequalities, which can then
be efﬁciently solved by linear programming (LP). We further prove the statistical
strong consistency of our algorithm, and demonstrate the robustness and sample
efﬁciency of our algorithm by experiments.

{

1 Introduction

Neural networks have achieved remarkable success in various ﬁelds such as computer vision
[LeCun et al., 1998, Krizhevsky et al., 2012, He et al., 2016a] and natural language processing [Kim,
2014, Sutskever et al., 2014]. This success is largely due to the strong expressive power of neural
networks [Bengio and Delalleau, 2011], where nonlinear activation units, such as rectiﬁed linear
units (ReLU) [Nair and Hinton, 2010] and hyperbolic tangents (tanh) play a vital role to ensure
the large learning capacity of the networks [Maas et al., 2013]. Meanwhile, the nonlinearity of neu-
ral networks makes them signiﬁcantly more difﬁcult to train than linear models [Livni et al., 2014].
Therefore, with the development of neural network applications, ﬁnding efﬁcient algorithms with
provable properties to train such nontrivial neural networks has become an important and a rela-
tively new goal.

∗Work mostly done at University of Edinburgh.

Preprint. Under review.

 
 
 
 
 
 
Residual networks, or ResNets [He et al., 2016a], are a class of deep neural networks that adopt
skip connections to feed values between nonadjacent layers, where skipped layers may contain non-
linearities in between. Without loss of expressivity, ResNets avoid the vanishing gradient problem
by directly passing gradient information from previous layers to current layers where otherwise gra-
dients might vanish without skipping. In practice, ResNets have shown strong learning efﬁciency
in several tasks, e.g. achieving at least 93% test accuracy on CIFAR-10 classiﬁcation, lowering
single-crop error to 20.1% on the 1000-class ImageNet dataset [Russakovsky et al., 2015, He et al.,
2016b].

Common ResNets are often aggregated by many repeated shallow networks, where
each network acts as a minimal unit with such skip propagation, named a residual unit
[He et al., 2016b]. Given the ﬂexibility and simplicity of residual units, much theoreti-
cal work has been devoted to study them and develop training algorithms for them in a
way that sidesteps from the standard backpropagation regime and provides guarantees
on the quality of estimation (see subsection 1.1). In this paper, we propose algorithms
that can learn a general class of single-skip two-layer residual units with ReLU activa-
tion as shown on the right by equation: y = B
, where for a scalar c,
c+ = max
(for a vector, this maximization is applied coordinate-wise), x is a ran-
{
Rm×d
dom vector as the network input with support space Rd, and A
are weight matrices of layer 1 and layer 2, respectively.

(Ax)+ + x
i

Rd×d and B

0, c

∈

∈

}

h

A

ReLU

B

Compared to previous work [Ge et al., 2018, 2017, Zhang et al., 2018, Wu et al., 2019, Tian,
2017, Du et al., 2017, Brutzkus and Globerson, 2017, Soltanolkotabi, 2017, Li and Yuan, 2017,
Zhong et al., 2017], the introduction of residual connections simpliﬁes the recovery of the network
parameters by removing the permutation and scaling invariance. However, a naive mean square error
minimization for estimating the parameters remains nonconvex. Unlike most previous work, we do
not assume a speciﬁc input distribution or that it is symmetric [Ge et al., 2018, Du and Goel, 2018].

We show that under regularity conditions on the weights of the residual unit, the problem of
learning it can be formulated through quadratic programming (QP). We use nonparametric estima-
tion [Guntuboyina et al., 2018] to estimate the ReLU function values in the networks. We further
rewrite our constructed quadratic programs to linear programs (LPs). The LP formulation is simpler
to optimize and has the same solution space as the QP for the network parameters.

1.1 Related Work

Provable learning of neural networks has been a recent topic of research. Arora et al. [2014] recover
a multi-layer generative network with sparse connection and Livni et al. [2014] study the learning
of multi-layer neural networks with polynomial activation. Goel et al. [2018] learns a one-layer
convolution network with a perceptron-like update rule. They prove the correctness of an iterative
algorithm for exact recovery of the target network. Rabusseau et al. [2019] describe a spectral al-
gorithm for two-layer linear networks. Recent work connected optimization and two-layer network
learning [Ergen and Pilanci, 2021, Sahiner et al., 2020, Ergen and Pilanci, 2020, Pilanci and Ergen,
2020] and showed how to optimize deep networks layer by layer [Belilovsky et al., 2019].

For learning a one-layer ReLU network, Wu et al. [2019] optimize the norm and direction of neural
network weight vectors separately, and Zhang et al. [2018] use gradient descent with a speciﬁc ini-
tialization. For learning a two-layer ReLU network, Ge et al. [2017] redesign the optimization land-
scape and Ge et al. [2018] use a moment-based method Janzamin et al. [2015]. Many others have
studied ReLU networks in various settings [Tian, 2017, Du et al., 2017, Brutzkus and Globerson,
2017, Soltanolkotabi, 2017, Li and Yuan, 2017, Zhong et al., 2017, Goel and Klivans, 2017].

The study of ReLU networks with two hidden layers has also been gaining atten-
tion [Goel and Klivans, 2017, Allen-Zhu et al., 2019], with a focus on PAC learning [Valiant, 1984].
In relation to our work, Allen-Zhu and Li [2019] examined the PAC-learnable function of a speciﬁc
three-layer neural network with residual connections. Their work differs from ours in two aspects.
First, their learnable functions include a smaller (than the student network) three-layer residual net-
work. Second, the assumptions they make on their three-layer model are rather different than ours.

In relation to nonparametric estimation, Guntuboyina et al. [2018] treat the ﬁnal output of a shape-
restricted regressor as a parameter, placing some restrictions on the type of function that can be es-

2

timated (such as convexity). They provide solutions for estimation with isotonic regression [Brunk,
1955, Ayer et al., 1955, van Eeden, 1956], convex regression [Seijo et al., 2011], shape-restricted
additive models [Meyer, 2013, Chen and Samworth, 2016] and shape-restricted single index mod-
els [Kakade et al., 2011, Kuchibhotla et al., 2017].

1.2 Main Results

We design quadratic objective functionals with linear bounded feasible domain that take network
parameters and functions as variables to estimate the ground-truth network parameters and nonlin-
earities. The values of the objectives are moments over the input distribution. Thm. 1.1 summarizes
the landscapes of the objectives.

Theorem 1.1 (objective landscape, informal). Suppose a ground-truth residual unit has nondegen-
erate weights in both layers and nonnegative weights in layer 1. Then there exist quadratic func-
tionals deﬁned in linear-constrained domains whose minimizers a) are unique, and are the exact
ground-truth, or b) are not unique, but can be adjusted to the exact ground-truth.

In practice, the exact moments over unobserved models cannot be accessed. We can only con-
struct the empirical risk minimization (ERM) of the moment-valued objectives by generated sam-
ples. With functions as variables in moment-valued objectives being optimized nonparametrically,
the empirical objectives become quadratic functions with linear constraints (a QP). We further show
the convexity of our QP which guarantees its solvability in polynomial time w.r.t. sample size and
dimension. With the access to the exact solution to our convex programs, the strong consistency of
our network learner is guaranteed:

Theorem 1.2 (strong consistency, informal). Suppose samples are generated by a ground-truth
residual unit that has nondegenerate weights in both layers and nonnegative weights in layer 1.
Then there exists an algorithm that learns a network a.s.
the exact ground-truth as sample size
−−→
grows.

Roadmap: Assume A and B are student network weights of layer 1 and 2. We ﬁrst give a warm-up
vanilla linear regression approach only knowing A is in Rd×d and B is in Rm×d (section 3). We
then move to the details of our nonparametric learning for layer 2 (section 4), and similarly, layer
1 (section 5). We formalize Thm. 1.1, showing how nonparametric learning allows us to select the
values of A and B from reduced spaces that are seeded by A∗ and B∗, under which LR is faster
than vanilla LR. In section 6, we describe the strong consistency of our methods for respective layers,
formalizing Thm. 1.2 using the continuous mapping theorem [Mann and Wald, 1943].

2 Preliminaries

We describe the notation used in this paper, introduce the model and its underlying assumptions, and
state conditions that simplify the problem but can be removed without loss of learnability.

2.1 Notation

∈

∈

Rd×d and B∗

Rm. We
The ReLU residual units we use take as input a vector x
Rm×d to denote the ground-truth network parameters for layer 1 and
use A∗
2, respectively. We use circumﬂex to denote predicted terms (e.g. an empirical objective function
ˆf , estimated layer 1 weights ˆA). We use n to denote the number of i.i.d. samples available for the
to denote the i-th sample drawn. For an integer k, We deﬁne [k] to be
training algorithm,
, and e(j) as the standard basis vector with a 1 at position j. All scalar-based operators
1, 2, . . . , k
{
are element wise in the case of vectors or matrices unless speciﬁed otherwise. We use x and y to
refer to the input and output vectors, respectively, as random variables.

Rd and return a vector y

x(i), y(i)

∈

∈

{

}

}

Linear Regression: By linear regression (LR), we are referring in this paper to the problem of
estimating L for unbiased model y = Lx. The estimation is done by minimizing the empirical risk
ˆR (L) = 1
– using linear least squares (LLS). Its solution has a closed
2n
form which we use as known. We refer the reader to Hamilton [1994] for more information.

Lx(i)

y(i)

i∈[n]

−

2

P

(cid:13)
(cid:13)

(cid:13)
(cid:13)

3

2.2 Models, Assumptions and Conditions

Following previous work [Livni et al., 2014], we assume a given neural network structure speciﬁes
a hypothesis class that contains all the networks conforming to this structure. Learning such a class
means to use training samples to ﬁnd a set of weights such that the neural networks predictions gen-
eralize well to unseen samples, where both the training and the unseen samples are drawn from an
unobserved ground-truth distribution. In this paper, the hypothesis class is given by ReLU residual
units and we assume it has sufﬁcient expressive power to ﬁt the ground-truth model. More specif-
ically, we discuss the realizable case of learning, in which the ground-truth model is set to be a
residual unit taken from the hypothesis class with the form:

y = B∗

(A∗x)+ + x
i

h

,

(1)

and is used to draw samples for learning. Unlike other multi-layer ReLU-afﬁne models that do not
apply skip connections, we cannot permute the weight matrices of the residual unit and retain the
same function because of skip-adding x. This helps us circumvent issues of identiﬁability2, and
allows us to precisely estimate the ground-truth weight matrices A∗ and B∗.

Our general approach for residual unit layer 2 learns a scaled ground-truth weight matrix that also
minimizes the layer 2 objective. The existence of such scaled equivalence of our layer 2 approach
comes from what is deﬁned below.
Deﬁnition 2.1 (component-wise scale transformation). A matrix A
transformation w.r.t. the j-th component if (Aj,:)⊤ = Aj,j ·
Additionally, estimation is more complex when the layer 2 weights B∗ is nonsquare. For simplicity
of our algorithm presentation for layer 2, we stick to Cond. 2.1 in the following sections3.
Condition 2.1 (layer 2 objective minimizer unique). A∗ is not a scale transformation w.r.t. any
components and B∗ is a square matrix, i.e. m = d.

Rd×d is said to be a scale

e(j).

∈

3 Warm-Up: Vanilla Linear Regression

Consider a ground-truth two-layer residual unit. If we assume that the inputs only contain vectors
with negative entries, i.e. x < 0, the effect of the ReLU function in the residual unit then disappears
because of the nonnegativity of A∗. The residual unit turns into linear model y = B∗x. Thus, direct
LR on samples with negative inputs can learn the exact ground-truth layer 2 parameter B∗ with at
least d samples, formulating the LR as a solvable full-rank linear equation system.

·

On the contrary, if the inputs only contain vectors with positive entries, i.e. x > 0, all the neurons in
the hidden layer are then activated by the ReLU and the nonlinearity is eliminated. The residual unit
in this case turns into y = B∗ (A∗ + Id) x. Taking what left-multiplies x as a single weight matrix
D∗, it is also a linear model. Direct LR on at least d samples with positive inputs by the residual
unit can learn the exact D∗. Since we have the access to the exact B∗, solving the exact A∗ is to
solve a full-rank linear equation system B∗

˜A = D∗, where the unique solution ˜A = A∗ + Id.

While simple, this vanilla LR approach requires a large number of redundant samples, since sam-
pled inputs usually have a small proportion of fully negative/positive vectors. Taking random input
vectors i.i.d. with respect to each entry as an example, the probability of sampling a vector with all
negative entries is pd
−, where p− is the probability of sampling a negative vector entry, then the ex-
pected number of samples to get one negative vector is 1/pd
+ samples
are expected for a positive vector. Besides, each LR step in this approach requires d such samples
respectively to make the linear equation system full-rank, which implies the expected sample size
. For other common random vectors like Gaussian samples,
to be d-exponential d
the proportions of fully negative/positive vectors in sampled inputs are also expected to decrease
exponentially as d grows, as high-dimensional random vectors like Gaussian are essentially concen-
trated uniformly in a sphere [Johnstone, 2006]. Technical and experimental details about sample
size expectations and the vanilla LR algorithm are further discussed in Appendix C.

−. Denoting p+ similarly, 1/pd

− + 1/pd
−

1/pd

(cid:1)

(cid:0)

·

2Here, we are referring to the ability of identifying the true model parameters with inﬁnite samples.
3In Appendix D, we show that B∗ estimation remains solvable without satisfying Cond. 2.1.

4

4 Nonparametric Learning: Layer 2

We present how we learn a residual unit layer 2 under Cond. 2.1 (estimating B∗): We ﬁrst design
an objective functional with the arguments being a matrix and a function. The objective uses expec-
tation of a loss over the true distribution generating the data and is uniquely minimized by [B∗]−1
and a rectiﬁer function (ReLU). We follow to formulate its ERM by nonparametric estimation as a
standard convex QP, further simpliﬁed as an LP that has the same capability as the QP to learn layer
2.

4.1 Objective Design and Landscape

Consider the formulation of a residual unit as in Eq. 1. It is possible to rewrite the model as equation:
C ∗y = (A∗x)+ + x, where on both sides of the equal sign is the output of the hidden neuron with
skip addition, and C ∗B∗ = Id. We aim to estimate the inverse of B∗ by matrix variable C and the
(A∗x)+ by a function variable h. The objective is formulated as risk functional
nonlinearity x
by the L2 error between values respectively computed by C and h

7→

G2 (C, h) =

Ex

1
2

h (x) + x

Cy

2
k

−

k
h

i

,

(2)

∈

Rd×m, the domain of h is the nonnegative4 continuous5 Rd

≥0 in shorthand. This objective is quadratic because the forward mapping x

where the estimator C
space, written as C0
7→
h(x) + x and the backward mapping y
Cy are both linear w.r.t. C and h, and the two are
linearly combined in a L2 norm. The objective by Eq. 2 is minimized by the ground-truth, i.e. C ∗
(A∗x)+, is one of its minimizers. However, it is not simple to describe other variable
and x
values that minimize the objective if any exist. We give that under Cond. 2.1, the minimizer of G2
is unique to be the exact ground-truth in the given domain.
Theorem 4.1 (objective minimizer, layer 2). Deﬁne G2(C, h) as Eq. 2, where C
Then under Cond. 2.1, G2(C, h) reaches its zero minimum iff C = [B∗]−1 and h : x

C0
≥0.
∈
(A∗x)+.

Rd function

Rd×m, h

→

7→

7→

∈

7→

Technical details are in Appendix E, where we use Lem. 4.2 (in subsection 4.3) to prove a more
general theorem that does not require Cond. 2.1 and is sufﬁcient for Thm. 4.1. In the next subsection,
we construct the ERM of G2 and present our convex QP formulation.

4.2 ERM with Nonparametric Estimation is Convex QP

Consider the second layer objective (Eq. 2) with nonnegative continuous function space as the do-
main of h. We follow Vapnik [1992] and deﬁne its standard empirical risk functional:

ˆG2(C, h) =

1
2n

h(x(i)) + x(i)

−

Cy(i)

2

.

(3)

Xi∈[n] (cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

C0

∈

The function variable h
≥0 can be optimized either parametrically or nonparametrically. If
we parameterize h, the nonlinearity w.r.t. its parameters would make ˆG2 lose its quadratic form.
Instead, we estimate h nonparametrically: for each sample input x(i), we introduce variables ξ(i)
that estimates mapped values by h. This avoids introducing nonlinearity to the objective and keeps
ˆG2 quadratic. On the other hand, the domain of h, i.e. nonnegative continuous function space, turns
into a set of linear inequalities as constraints when optimizing nonparametrically. In this sense,
learning the second layer of the residual unit can be formulated as the following QP:

ˆGNPE
2

min
C, Ξ

(C, Ξ) :=

1
2n

ξ(i) + x(i)

Cy(i)

2

, s.t. ξ(i)

−

0,

i

∀

∈

≥

[n].

(4)

Xi∈[n] (cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

4Setting h as nonnegative ensures that a) only ReLU nonlinearity minimizes G2 (see Thm. 4.1), and b) h’s

nonparametric estimator linearly constrained (explained in subsection 4.2).

5If h is not a continuous function, only a null set of discontinuities is possible to make G2 reach zero
as its minimum. Setting h as continuous simpliﬁes our theoretical results that still strictly support empirical
discussion.

5

where Ξ =

ξ(i)

n
i=1 is the nonparametric estimator of x

{

}

(A∗x)+.

7→

{

}

∈

C0

ξ(i)

Nonparametric Estimation Validation: It is guaranteed that a solution to the ERM with nonpara-
metric estimation, i.e. the QP, is sufﬁcient to minimize the standard empirical risk functional (Eq. 3).
n
More speciﬁcally, assuming C and Ξ =
i=1 are a solution to layer 2 QP (Eq. 4), it is clear that
≥0 such that h(x(i)) = ξ(i) minimize the empirical risk functional Eq. 3. Conversely,
C and h
a minimizer of the standard empirical risk Eq. 3, C and h, corresponds to a solution to layer 2 QP as
we set ξ(i) = h(x(i)). Therefore, minimizing ˆG2 and solving layer 2 QP are empirically equivalent.
Convexity: The convexity of the QP: Eq. 4 is also guaranteed. First, the constraints are linear. In
[n], the L2 norm wraps linearity w.r.t. C and ξ(i). Such
addition, for each sample with index i
formulation ensures the quadratic coefﬁcient matrix to be positive semideﬁnite . Thus, with the sum
of convex functions still being convex, the QP objective (Eq. 4) becomes convex. Even without the
knowledge of how samples are generated, this QP would be a convex program. Strict proofs are in
Appendix F.

∈

4.3 LP Simpliﬁcation

x

x

∈

≥

−

≥

−

7→

−
x.

2 k

Cy

Cy

−
Cy

h (x) + x

Rd and its

0 holds for all x

0 holds for any x

Rd, then C and h : x

2. If there is a feasi-
Consider single-sample error written as g2 (C, h; x, y) = 1
k
x always minimize
ble C such that Cy
−
g2 as zero, and thereby minimize the layer 2 objective (Eq. 2). Thus, we obtain a condition that is
equivalent to G2 reaching minimum in Thm. 4.1 and avoids randomness (see Lem. 4.2).
Lemma 4.2. G2(C, h) reaches its zero minimum iff Cy
corresponding residual unit output y, and h : x

x where C complies with Cy

7→
The pointwise satisfaction of the inequality in Lem. 4.2 describes the solution space for the mini-
mization of G2. The sufﬁciency of satisfying this inequality to minimize G2 is directly obtained by
assigning h : x
0 for all x in the support space
Rd. The necessity of satisfying this inequality comes from its contraposition: If a C violates the
inequality, there must be a nonnull set of x that yield the violation due to the continuity of Cy
x.
In this sense, the resulting G2 value becomes nonzero.
Empirically speaking, we cannot solve an inequality that pointwisely holds w.r.t. the support space
if we only observe ﬁnite samples. We can only estimate C by solving the inequality that pointwisely
holds w.r.t. each sample. Following this, we formulate such estimation as to ﬁnd a feasible point in
the space deﬁned by a set of linear inequalities, each of which corresponds to a sample: Cy(i)
−
x(i)
0. Each point in the feasibility deﬁned by the inequalities has a one-to-one correspondence
in the solution space to layer 2 QP (Eq. 4): C
}i∈[n]). The set of inequalities
can be solved by a standard LP with a constant objective and constraints being the inequalities, i.e.

Cy(i)

x(i)

(C,

Cy

↔

7→

≥

−

−

−

≥

−

∈

x

{

const, s.t. Cy(i)

x(i)

0,

i

[n].

min
C

−
With the one-to-one correspondences, LP: Eq. 5 and QP: Eq. 4 have equivalent solution spaces.
Moreover, LP: Eq. 5 is also a convex program since both objective and constraints of a standard LP
are linear. Alg. 1 summarizes the layer 2 estimator: Simply solve the QP/LP6 and return the inverse
(A∗x)+ estimate. Regardless of time complexity,
of ˆC as layer 2 weights estimate and ˆΞ as x
7→
QP and LP in Alg. 1 work equivalently since their solution spaces are equivalent to each other.

≥

∈

∀

(5)

As per this section, nonparametric learning directly ﬁnds a unique layer 2 estimate under Cond. 2.1.
In the general case without Cond. 2.1 (discussed in Appendix D), nonparametric learning essentially
reduces the possible values of B from Rm×d to a B∗ scale-equivalent matrix space, where LR uses
sampled data much more efﬁciently than vanilla LR on layer 2 (section 3).

5 Nonparametric Learning: Layer 1

With layer 2 learned, outputs by hidden neurons become observable. The two-layer problem is
thereby reduced to single-layer. Consider a ground-truth single-layer model: h = (A∗x)+. To

6We use CVX [Grant and Boyd, 2014, 2008] that calls SDPT3 [Toh et al., 1999] (a free solver under GPLv3

license) and solves our convex QP/LP in polynomial time. See Appendix B for technical details.

6

Algorithm 1 Learn a ReLU residual unit, layer
2.
1: Input:
Eq. 1.

n
i=1, samples drawn by

(x(i), y(i))
}

{

2: Output: ˆB, ˆΞ, a layer 2 and x

estimate.

(A∗x)+

7→

3: Go to line 4 if QP, line 5 if LP.
4: Solve QP: Eq. 4 and obtain a ˆGNPE
mizer, denoted by ˆC, ˆΞ. Go to line 6.

2 mini-

5: Solve LP: Eq. 5 and obtain a minimizer ˆC,

then assign ˆξ(i)
6: return ˆC −1, ˆΞ.

←

ˆCy(i)

x(i).

−

Algorithm 2 Learn a ReLU residual unit, layer
1.

{

n
i=1, layer 1 samples.

(x(i), h(i))
1: Input:
}
2: Output: ˆA, a layer 1 estimate.
3: Solve QP: Eq. 7 or LP: Eq. 8 and obtain a
1 minimizer, denoted by ˆA, ˆΦ. { ˆΦ is no

ˆGNPE
longer needed.}
4: for all j
[d] do
∈
h(i)
ˆkj ←
j , ˆAj,:x(i)
LR
5:
j >0
{
ˆAj,:/ˆkj. {Rescale ˆA.}
ˆAj,: ←
6:
7: end for
8: return ˆA.

}h(i)

.

(
−

7→

G1 (A, r) =

construct a learning objective for this model, we rewrite the model as a nonlinearity plus a linear
A∗x)+ + A∗x, where on both sides of the equal sign is the output of layer
mapping by A∗: h = (
1, and the nonlinearity is x

A∗x)+. The objective of layer 1 is formulated as:

−

Ex

1
2

k
h

r(x) + Ax

2

h

k

−

i

,

(6)

∈

Rd×d is of layer 1 weights estimator, the domain of r is also C0

≥0. The minimizer A
where A
of the risk G1 falls into a matrix space such that for any matrix A in the space, each row of A is a
scaled-down version of the same row of A∗ without changing the direction (Thm. 5.1).
Theorem 5.1 (objective minimizer space, layer 1). Deﬁne G1(A, r) as Eq. 6, where A
r
0

≥0. Then G1(A, r) reaches its zero minimum iff for each j

Rd×d,
∈
[d], Aj,: = kjA∗
j,: where

1, and r : x

(A∗x)+

diag(k)

A∗x.

∈

C0
kj ≤

∈
≤

7→

−

·

+

≤

j,:x

k
≤
0
kj ≤
≤
diag(k)
·

The scale equivalence in the solution space is derived from a ReLU inequality: (x)+
0

1. Let the j-th row of A be a scaled-down version of A∗, i.e. Aj,: = kjA∗
A∗
1. According to the inequality, we have
A∗x. Thus, r minimizing G1 in Thm. 5.1 lies in feasibility C0

j,:x, which indicates (A∗x)+
≥0 when A = diag(k)

(cid:0)
Due to the existence of scale equivalence, we must compute the scale factor to obtain the ground-
+ ob-
truth weights A∗. The scale factor kj is sufﬁciently obtainable with (Aj,:x)+ and
A∗
j,:x
servable: Conditioned on nonnegative ReLU input, we have a linear model Aj,:x = kjA∗
j,:x where
(cid:1)
Aj,:x and A∗
j,:x are observed. Thm. 5.2 summarizes layer 1 scale factor property, allowing us to
correct a minimizer of G1 to the ground-truth weights A∗ by computing a scalar for each row.
Theorem 5.2 (scale factor, layer 1). Assume A is a minimizer of G1. Then for any j
(Aj,:x)+ /

+ is always equal to the scale factor kj given that (Aj,:x)+ > 0.

kx where
j,: where

kjA∗

≥
A∗.

A∗

[d],

≥

≥

∈

(cid:1)

(cid:0)

·

j,:x

≥

−

(cid:1)
Ax

(cid:0)
The randomness elimination for G1 follows the same pattern as layer 2. If there is a feasible A such
that (A∗x)+
Rd, such A is a solution to our objective by Eq. 6. We
0 holds for all x
can also have the following proposition that avoids randomness.
Lemma 5.3. G1(A, r) reaches its zero minimum iff h
Ax
Ax.
corresponding hidden output h, and r : x

−
We now turn into empirical discussion. Similar to layer 2, we formulate layer 1 QP by its empirical
objective with nonparametric estimation and linear constraints representing the nonnegativity of r:

0 holds for any x

Rd and its

7→

≥

−

h

∈

∈

ˆGNPE
1

min
A, Φ

(A, Φ) :=

1
2n

φ(i) + Ax(i)

2

h(i)

, s.t. φ(i)

0,

i

∀

∈

≥

[n].

(7)

A∗x where k refers
where Φ =
to the scaling equivalence. Similarly, the solution space of QP: Eq. 7 can be represented by a set of

n
i=1 is the function estimator of x

diag(k)

φ(i)

7→

−

{

}

·

Xi∈[n] (cid:13)
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)
(cid:13)
(A∗x)+

7

linear inequalities as constraints of the following efﬁciently solvable LP

const, s.t. h(i)

Ax(i)

0,

i

[n].

min
A

−
Alg. 2 describes how a layer 1 is learned: First, a G1 minimizer estimate ˆA is obtained by solving
the QP/LP. Then for the j-th row, the scale factor kj is estimated by running LR on h(i)
j and ˆAj,:x(i)
s.t. h(i)
j > 0 to correct ˆA, as Aj,:x = kjhj is an unbiased linear model given that hj > 0. By
nonparamatric learning, the value space of A is reduced from Rd×d to
,
1
0
|
where LR uses sampled data much more efﬁciently than vanilla LR layer 1 (section 3).

kj ≤

diag(k)

A∗

≥

≤

∈

∀

{

}

·

(8)

6 Full Algorithm and Analysis

7→

a.s.
−−→

The full algorithm concatenates Alg. 1 and 2 layerwisely, with observations of input/output by the
(A∗x)+ by Alg. 1. b) Estimates
ground-truth network: a) Estimates layer 2 and nonlinearity: x
layer 1 by running Alg. 2 on input samples and the nonlinearity estimate. It has provable guarantees.
For empirical analysis, we use ˆCn to denote the estimation of C ∗ from n random samples. Similar
notations are applied to other estimations. First of all, our methods solving respective layers, Alg. 1
and 2, are strongly consistent if involved convex QPs/LPs can be exactly solved.
C ∗ and ˆBn
Lemma 6.1 (layer 2 strong consistency). Under Cond. 2.1, ˆCn

.
→ ∞
For ˆCn: In Appendix G, we prove its more general a.s. convergence without satisfying Cond. 2.1,
where the solution space to layer 2 objective is a noncompact continuous set where all the elements
are scaling equivalences. We use Hausdorff distance [Rockafellar and Wets, 2009] as metric and
prove that the empirical solution space a.s. converges to the theoretical solution space. Then the
more general a.s. convergence holds with the strong consistency of layer 2 scale factor estimator
where we use LR to estimate the scale factors.
For ˆBn: By the continuous mapping theorem [Mann and Wald, 1943], we directly propagate the
strong consistency of C ∗ estimator to its inverse B∗’s estimator.
According to full algorithm description, layer 1 estimation uses ˆCny(i)
x(i) as the outputs where
[n]. Thus, the strong consistency of the hidden neuron estimator is also guaranteed by the
i
continuous mapping theorem. Following the proof sketch of the ˆCn a.s. convergence, we obtain the
strong consistency of layer 1 estimator (See Lem. 6.2).
Lemma 6.2 (layer 1 strong consistency). ˆAn

a.s.
−−→

B∗, n

A∗, n

−

∈

a.s.
−−→

.
→ ∞

By the continuous mapping theorem, the strong consistency of the full algorithm (Thm. 6.3, formal
version of Thm. 1.2), which is commonly deﬁned by a loss function that is continuous on network
weights, is implied by the strong consistency of network weights estimators (Lem. 6.1 and 6.2). See
Appendix G for proofs of strong consistency discussions in this section.
Theorem 6.3 (strong consistency, formal). Deﬁne L as L2 output loss. L( ˆAn, ˆBn)

0, n

a.s.
−−→

.
→ ∞

7 Experiments

We provide an experimental analysis to demonstrate the effectiveness and robustness of our approach
2 ,
in comparison to stochastic gradient descent (SGD) on L2 output loss: L(A, B) = 1
2
where we parameterize the output prediction by ˆy = B
. Our proposed methods
outperform SGD in terms of sample efﬁciency and robustness to different network weights and
noise strengths, which indicates bad optimization landscape of L2 output loss for ReLU residual
units.
Setup: The ground-truth weights are generated through i.i.d. folded standard Gaussian7 and standard
Gaussian for layer 1 and 2 respectively, i.e. A∗ i.i.d.
(0, 1). The input distri-
0.9, 1.1).
0.1, 1) –
bution is set to be an i.i.d. zero mean Gaussian-uniform equal mixture
(
−

(Ax)+ + x
i
h

(0, 1), B∗ i.i.d.

∼ N
(
−
N

∼ |N |

ˆy
k

Ex

−

U

y

k

7A folded Gaussian is the absolute value of a Gaussian, with p.d.f. p(|x|) where x ∼ N , denoted as |N |.

We use folded Gaussian to ensure layer 1 weights to be nonnegative.

8

SGD

8 

10

12

14

16

18

20

22

Ours

8 

10

12

14

16

18

20

22

160 192 224 256 288 320 352 384

160 192 224 256 288 320 352 384

0.6

0.5

0.4

0.3

0.2

0.1

Table 1: Means / Standard deviations of net-
work estimate errors for different network
weights. Values are computed from the process
of learning 128 different ground-truth networks
with d = 16. 512 training samples are drawn
for each learning trial.

Figure 1: Output errors by SGD and our
method for different dimensions and sample
sizes. For the same d, we ﬁx the ground-truth
network as sample size grows.

Layer 1

Layer 2

Output

Mean

Std Mean

Std Mean

Std

SGD 0.715
Ours
0.039

0.090
0.008

1.203
0

≈

0.134
0

≈

0.431
0.055

0.038
0.008

0.5

0.4

0.3

0.2

0.1

0

0

SGD
LP
QP

0.2

0.4

0.6

0.8

1

1

0.8

0.6

0.4

0.2

0

0

SGD
LP
QP

0.2

0.4

0.6

0.8

1

1.4

1.2

1

0.8

0.6

0.4

0.2

0

0

SGD
LP
QP

0.2

0.4

0.6

0.8

1

Figure 2: Respective errors of layer 1, 2 and outputs for different label noise strengths by SGD,
LP and QP. We ﬁx the ground-truth weights with d = 10 and only the noise strength varies. 512
training samples are drawn for each learning trial.

SGD is conducted on mini-batch empirical losses of L(A, B) with batch size 32 for 256 epochs in
each learning trial. We apply time-based learning rate decay η = η0/ (1 + γ
T ) with initial rate
η0 = 10−3 and decay rate γ = 10−5, where T is the epoch number. The above hyperparameters
are tuned to outperform other hyperparameters in learning ReLU residual units in terms of output
errors.

·

Evaluation: We use relative errors to measure the accuracy of our vector/matrix estimates: For a
network with weights A and B and its teacher network with weights A∗ and B∗, a) layer 1 error
refers to
] by test
data. Due to the equivalence between the solution spaces of our QP and LP without label noise, we
choose LP in noiseless experiments, named as “ours”. Besides, to reduce variance, the results of
learning the same ground-truths are computed as means across 16 trials.

, similar to layer 2. b) output error refers to ˆE [

A∗
k

y
k

ˆy
k

A∗

A

−

−

y

k

k

k

k

k

/

/

Sample Efﬁciency: Consider Fig. 1. We consider there the variation in the output prediction errors
(warmer color, larger error) as a function of d, the dimension of the input and the number of samples
the learning algorithm is using. We compare SGD against our algorithm. We observe that empiri-
cally our approach to the estimation of the neural network is more sample efﬁcient. For SGD, the
estimation is relatively easy with only up to 10 dimensions. As expected, once the dimension grows,
sample size required for the same error level as our method is larger. Still, overall, our method is
capable of learning robustly with small sample sizes and more efﬁciently than SGD even for larger
sample sizes.

Network Weight Robustness: This experiment aims to verify whether our method can genuinely
learn a broader class of residual units. In Tab. 1, our method shows a light-tailed distribution with
nearly zero means and standard deviations for layer 1, 2, and output errors across various ground-
truth networks, whereas SGD performs much less robustly in the same sense. Our method shows
strong robustness to network weight changes, indicating its applicability across the whole hypothesis
class.

Noise Robustness: Fig. 2 veriﬁes the robustness of our methods for residual units with output
noise. Samples are generated by a ground-truth residual unit with output noise being i.i.d. zero-mean

9

Gaussian in different strengths (i.e. standard deviations). We conduct both QP and LP because in
noisy setting the two approaches are not equivalent w.r.t. solution space8. First, SGD always gives
larger errors than our methods, even though it is hardly affected by tuning the noise strength. For
QP/LP, all the layer 1, 2 and output errors grow almost linearly as noise strength increases, indicating
that both QP/LP learn the optima robustly with output noise, where QP slightly outperforms LP.

8 Conclusion

In this paper, we address the problem of learning a general class of two-layer residual units and
propose an algorithm based on landscape design and convex optimization: First, minimizers of our
objective functionals can express the exact ground-truth network. Then, the corresponding ERM
with nonparametric function estimation can be solved by convex QP/LP, which indicates polynomial-
time solvability w.r.t. sample size and dimension. Moreover, our algorithms solving both layers and
the whole networks are strongly consistent, with very weak conditions on input distributions.

Our work opens the door to a variety of open problems to explore. We provide a strong consistency
result without assuming a particular input distribution. It would be interesting to explore the sam-
ple complexity of our algorithm with stronger assumption on input distribution. Another extension
d, nondegeneration, non-
could be disposing of the limitations on ground-truth weights, e.g. m
negativity of A∗. Besides, our algorithm solves ReLU by nonparametric estimation through convex
optimization. This might provide an inspiration to solving learning problems with other nonlineari-
ties using nonparametric methods that improve or change the optimization landscape.

≥

References

Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document

recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In Advances in neural information processing systems, pages 1097–1105, 2012.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016a.

Yoon Kim. Convolutional neural networks for sentence classiﬁcation. arXiv preprint arXiv:1408.5882, 2014.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.

In

Advances in neural information processing systems, pages 3104–3112, 2014.

Yoshua Bengio and Olivier Delalleau. On the expressive power of deep architectures. In International Confer-

ence on Algorithmic Learning Theory, pages 18–36. Springer, 2011.

Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In Proceed-

ings of the 27th international conference on machine learning (ICML-10), pages 807–814, 2010.

Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural network acoustic

models. In ICML Workshop on Deep Learning for Audio, Speech and Language Processing, 2013.

Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efﬁciency of training neural networks.

In Advances in neural information processing systems, pages 855–863, 2014.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. Inter-
national journal of computer vision, 115(3):211–252, 2015.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In

European conference on computer vision, pages 630–645. Springer, 2016b.

Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang. Learning two-layer neural networks with symmetric

inputs. arXiv preprint arXiv:1810.06793, 2018.

Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design.

arXiv preprint arXiv:1711.00501, 2017.

8See Appendix A for noisy model discussion.

10

Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer ReLU networks via

gradient descent. ArXiv, abs/1806.07808, 2018.

Shanshan Wu, Alexandros G. Dimakis, and Sujay Sanghavi. Learning distributions generated by one-layer

ReLU networks. In NeurIPS, 2019.

Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its applica-

tions in convergence and critical point analysis. In ICML, 2017.

Simon S. Du, Jason D. Lee, Yuandong Tian, Barnabás Póczos, and Amrendra Kumar Singh. Gradient descent

learns one-hidden-layer cnn: Don’t be afraid of spurious local minima. ArXiv, abs/1712.00779, 2017.

Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs.

ArXiv, abs/1702.07966, 2017.

Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. In NIPS, 2017.

Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activation. ArXiv,

abs/1705.09886, 2017.

Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery guarantees for one-

hidden-layer neural networks. In ICML, 2017.

Simon S. Du and Surbhi Goel.

Improved learning of one-hidden-layer convolutional neural networks with

overlaps. ArXiv, abs/1805.07798, 2018.

Adityanand Guntuboyina, Bodhisattva Sen, et al. Nonparametric shape-restricted regression. Statistical Sci-

ence, 33(4):568–594, 2018.

Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep represen-

tations. In International Conference on Machine Learning, pages 584–592, 2014.

Surbhi Goel, Adam R. Klivans, and Raghu Meka. Learning one convolutional layer with overlapping patches.

ArXiv, abs/1802.02547, 2018.

Guillaume Rabusseau, Tianyu Li, and Doina Precup. Connecting weighted automata and recurrent neural
In The 22nd International Conference on Artiﬁcial Intelligence and

networks through spectral learning.
Statistics, pages 1630–1639. PMLR, 2019.

Tolga Ergen and Mert Pilanci. Implicit convex regularizers of cnn architectures: Convex optimization of two-
In International Conference on Learning Representations

and three-layer networks in polynomial time.
(ICLR), 2021.

Arda Sahiner, Tolga Ergen, John Pauly, and Mert Pilanci. Vector-output ReLU neural network problems are
copositive programs: Convex analysis of two layer networks and polynomial-time algorithms. arXiv preprint
arXiv:2012.13329, 2020.

Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality. arXiv

preprint arXiv:2002.09773, 2020.

Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time convex op-
timization formulations for two-layer networks. In International Conference on Machine Learning, pages
7695–7705. PMLR, 2020.

Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can scale to ima-

genet. In International conference on machine learning, pages 583–593. PMLR, 2019.

Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed

training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.

Surbhi Goel and Adam R. Klivans. Learning neural networks with two nonlinear layers in polynomial time. In

COLT, 2017.

Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural
networks, going beyond two layers. In Advances in neural information processing systems, pages 6155–
6166, 2019.

Leslie G. Valiant. A theory of the learnable. Commun. ACM, 27:1134–1142, 1984.

11

Zeyuan Allen-Zhu and Yuanzhi Li. What can ResNet learn efﬁciently, going beyond kernels? In Advances in

Neural Information Processing Systems, pages 9015–9025, 2019.

Hugh D Brunk. Maximum likelihood estimates of monotone parameters. The Annals of Mathematical Statistics,

pages 607–616, 1955.

Miriam Ayer, H Daniel Brunk, George M Ewing, William T Reid, and Edward Silverman. An empirical
distribution function for sampling with incomplete information. The annals of mathematical statistics, pages
641–647, 1955.

Constance van Eeden. Maximum likelihood estimation of ordered probabilities, 2. Stichting Mathematisch

Centrum. Statistische Afdeling, (S 196/56), 1956.

Emilio Seijo, Bodhisattva Sen, et al. Nonparametric least squares estimation of a multivariate convex regression

function. The Annals of Statistics, 39(3):1633–1657, 2011.

Mary C. Meyer. A simple new algorithm for quadratic programming with applications in statistics. Communi-

cations in Statistics - Simulation and Computation, 42:1126–1139, 2013.

Yining Chen and Richard J Samworth. Generalized additive and index models with shape constraints. Journal

of the Royal Statistical Society: Series B (Statistical Methodology), 78(4):729–754, 2016.

Sham M. Kakade, Adam Tauman Kalai, Varun Kanade, and Ohad Shamir. Efﬁcient learning of generalized

linear and single index models with isotonic regression. In NIPS, 2011.

Arun K Kuchibhotla, Rohit K Patra, and Bodhisattva Sen. Efﬁcient estimation in convex single index models.

Preprint. Available at, 2017.

Henry B Mann and Abraham Wald. On stochastic limit and order relationships. The Annals of Mathematical

Statistics, 14(3):217–226, 1943.

James Douglas Hamilton. Time series analysis, volume 2. Princeton university press Princeton, NJ, 1994.

Iain M Johnstone. High dimensional statistical inference and random matrices. arXiv preprint math/0611589,

2006.

Vladimir Vapnik. Principles of risk minimization for learning theory.

In Advances in neural information

processing systems, pages 831–838, 1992.

Michael Grant and Stephen Boyd. CVX: Matlab software for disciplined convex programming, version 2.1.

http://cvxr.com/cvx, March 2014.

Michael Grant and Stephen Boyd.

In
V. Blondel, S. Boyd, and H. Kimura, editors, Recent Advances in Learning and Control, Lec-
ture Notes in Control and Information Sciences, pages 95–110. Springer-Verlag Limited, 2008.
http://stanford.edu/~boyd/graph_dcp.html.

Graph implementations for nonsmooth convex programs.

Kim-Chuan Toh, Michael J Todd, and Reha H Tütüncü. Sdpt3—a matlab software package for semideﬁnite

programming, version 1.3. Optimization methods and software, 11(1-4):545–581, 1999.

R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer Science & Business

Media, 2009.

Mikhail K Kozlov, Sergei P Tarasov, and Leonid G Khachiyan. The polynomial solvability of convex quadratic

programming. USSR Computational Mathematics and Mathematical Physics, 20(5):223–228, 1980.

Katta G Murty. Computational complexity of parametric linear programming. Mathematical programming, 19

(1):213–219, 1980.

Florian Jarre. On the convergence of the method of analytic centers when applied to convex quadratic programs.

Mathematical Programming, 49(1-3):341–358, 1990.

Yin Zhang, Richard A Tapia, and John E Dennis, Jr. On the superlinear and quadratic convergence of primal-
dual interior point linear programming algorithms. SIAM Journal on Optimization, 2(2):304–324, 1992.

Yinyu Ye and Edison Tse. An extension of karmarkar’s projective algorithm for convex quadratic programming.

Mathematical programming, 44(1-3):157–179, 1989.

Renato DC Monteiro and Ilan Adler. Interior path following primal-dual algorithms. part ii: Convex quadratic

programming. Mathematical Programming, 44(1-3):43–66, 1989.

12

Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In Proceedings of the six-

teenth annual ACM symposium on Theory of computing, pages 302–311, 1984.

M Émile Borel. Les probabilités dénombrables et leurs applications arithmétiques. Rendiconti del Circolo

Matematico di Palermo (1884-1940), 27(1):247–271, 1909.

Pranab K Sen and Julio M Singer. Large sample methods in statistics: an introduction with applications,

volume 25. CRC press, 1994.

J Farkas. Ober die theorie der einfachen ungleichungen. J. Reine Angew. Math, 124:1–24, 1902.

David G Luenberger. Optimization by vector space methods. John Wiley & Sons, 1997.

Robert I Jennrich. Asymptotic properties of non-linear least squares estimators. The Annals of Mathematical

Statistics, 40(2):633–643, 1969.

Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy´nski. Lectures on stochastic programming: mod-

eling and theory. SIAM, 2014.

Appendices

We include an outline of the appendices:

• Appendix A: A brief discussion of our learning algorithms in a noisy context, referring to
the LP slack variable technique that is used in our experiments with noise in the main paper.
Also, this section provides an insight to potential future direction of this work.

• Appendix B: A detailed explanation of the computational complexity and the methods

through which the QPs/LPs in this paper are solved.

• Appendix C: A formal result and an empirical validation of the example given in the main

paper in the context of the vanilla linear regression method (section 3).

• Appendix D: A generalization of layer 2 learning to the case where Cond. 2.1 is not satis-

ﬁed.

• Appendix E: Proofs regarding the minimizers of the objective functions we use.
• Appendix F: Proofs that justify the convexity of our QPs.
• Appendix G: Proofs that show our estimation algorithm is strongly consistent.

A Discussion: Noisy Model

Here, we discuss our learning methods in the noisy case. We introduce output noise to our model,
namely

(A∗x)+ + x
i
h
where the label noise z
E [z] = 0. In addition z and x are statistically independent.

y = B∗

∈

Rd is an i.i.d. random vector with respect to each component, satisfying

+ z,

(9)

7→

(A∗x)+) = Ez[

Taking layer 2 as an example, our original objective functional does not reach zero by substituting
2] = σ2 Tr C ∗C ∗⊤ where σ is the noise
the ground-truth: G2(C ∗, x
k
strength. However, if layer 2 is well conditioned, the ground-truth will still assign the objective a
value close to zero. In this sense, with G2’s continuity, the ground-truth can approximately minimize
G2, which validates our QP approach in learning noisy residual units.
Our original LP (Eq. 5) fails to give feasible solutions due to possible violation of the inequality
x = (A∗x)+ + C ∗z is not necessarily an entrywise nonnegative vector
C ∗y
because the term C ∗z might have negative entries. So we introduce slack variables ζ(i) to soften
the constraints:

0, since C ∗y

C ∗z
k

−

−

≥

x

min
C,Z

1
n

Xi∈[n]
s.t. Cy(i)

ζ(i),

1⊤

·

−

x(i)

≥ −

ζ(i), ζ(i)

0.

≥

13

(10)

(11)

Algorithm 3 Learn a ReLU residual unit by LR.

n
i=1, n samples drawn by Eq. 1.

{

(x(i), y(i))
1: Input:
}
2: Output: ˆA, ˆB, estimated weight matrices.
3: ˆB
∈ {
4: ˆD
∈ {
5: Solve full-rank linear equation system ˆB
6: return ˜A

(x(i), y(i))
}
(x(i), y(i))
}

⌊n/2⌋
i=1
|
n
i=⌊n/2⌋+1 |

(x, y)
(x, y)

LR
{
LR
{

←
←

Id, ˆB.

·

x < 0

.
}
x > 0
˜A = ˆD

−

. {the estimation of B∗ (A∗ + Id)}

}

+ C ∗z(i) < 0, then its L1 norm would be
For a sample (x(i), y(i)) with noise z(i), if
added to the objective. This method remedies violations to the inequality C∗y
0. With access
≥
to a sufﬁciently large sample and with the “stability” assumption, our solution ˆC would be close to
C ∗ since large deviations seldom rise and their penalties are diluted in the objective.

A∗x(i)

−

x

(cid:0)

(cid:1)

+

B Discussion: Solving Convex Programs

The theoretical foundation of solving convex QP and LP has been driven to maturity in terms of
computational complexity [Kozlov et al., 1980, Murty, 1980] and convergence analysis [Jarre, 1990,
Zhang et al., 1992]. The time complexity for a convex QP/LP is analyzed in terms of the number
of scalar variables N and the number of bits L in the input [Ye and Tse, 1989]. For example, under
Cond. 2.1, N = d2 + nd for the QP because the matrix variables have d2 scalars and the function
estimators have nd scalars. Similarly we have N = d2 for the noiseless LP and N = d2 + nd for the
noisy LP. It is guaranteed that primal-dual interior point methods can solve the convex QP/LP in a
(N 2.5) arithmetic
polynomial nummber of iterations
operations from the Cholesky decomposition for the needed matrix inversion [Monteiro and Adler,
(N 3L)
1989, Karmarkar, 1984]. This indicates that our QPs/LPs are guaranteed to be solvable in
arithmetic operations, which is generally an

(√N L), where each iteration costs at worst

(poly(n, m, d, L)) complexity.

O

O

O

To solve convex QPs/LPs in this paper, we use CVX, a commonly used package for specifying and
solving convex programs [Grant and Boyd, 2014, 2008]. For both QPs and LPs, CVX calls a solver,
SDPT3 [Toh et al., 1999], that is speciﬁed for semideﬁnite-quadratic-linear programming and applies
interior-point methods with the computational complexity mentioned above. Experimentally, SDPT3
indeed speciﬁes and solves our programs fast and makes our numerical results robust and stable.

O

C Vanilla Linear Regression: More Details

Alg. 3 gives the full list of steps of learning two-layer residual units by LR, where we ﬁrst split the
drawn samples into two halves for the respective two key steps, then for both halves we ﬁlter negative
and positive vectors, respectively. By running LR on both ﬁltered sets we obtain an estimation of
the ground-truth network.

In the following, we formalize this intuition and describe the exponential sample complexity of this
approach under entry-wise i.i.d. setting as an example of the inefﬁciency of this method.

Theorem C.1 (exponential sample complexity, vanilla LR). Assume the input vectors are i.i.d. with
respect to each component. Then Alg. 3 learns a neural network from a ground-truth residual unit
with at least

expected number of samples.

2d+1

d

O

·

(cid:0)

(cid:1)

∈

Proof. For each j
[d], let Pj be the marginal probability of xj being positive, i.e. Pj = P (xj >
0) > 0. Thus, the probability that a sample is positive P (x > 0) =
j∈[d] Pj . Then the expected
j∈[d] Pj. Similarly, the expected
number of sampling trials to obtain d positive samples is d/
Q
P (xj = 0)]. Let n
number of sampling trials to obtain d negative samples is d/
j∈[d] [1
Q
be a random natural number s.t. with n samples the network is learned. The expected number of
Q

Pj −

−

14

Table 2: Learning success rate of vanilla LR on residual units with input x i.i.d.
(0, 1). The
success rates shows an exponentially decreasing trend with the same number of samples. The sample
sizes to achieve close to the same rate grows exponentially as the number of dimensions grows
linearly.

∼ N

n

d

4

6

8

10

12

1e1

1e2

5e2

1e3

5e3

1e4

5e4

1e5

0.002

0.137

0

0

0

0

0

0

0

0

0.999

0.041

1

0.614

1

1

0

0

0

0

0

0

0.579

0

0

1

1

0.998

0.002

1

1

1

1

1

1

1

1

0

0.001

0.322

samples that guarantees successful learning is the sum of the two expectations

1
j∈[d] Pj

+

1
Pj −

j∈[d] [1

−

P (xj = 0)] )

Q

1
j∈[d] Pj

+

Q

1
j∈[d] (1

Pj ) #

−

(

"

E[n] = d

d

d

d

≥

≥

≥

= d

Q
2

Yj∈[d]

2

Yj∈[d]
2d+1.

·

·

·

1
Q
Pj (1

Pj)

−
1

p
(Pj + 1

Pj) /2

−

Thus E[n]

(d

·

≥ O

2d+1).

With an exponential sample complexity, the time complexity of the LR approach is thereby also
exponential because ﬁltering through all the samples costs time with the same complexity as the
number of samples, even though the LR itself only costs polynomial time in the number of samples.
To summarize, this vanilla LR approach learns exact ground-truth residual units but with exponential
complexity in terms of both computational cost and sample size. While this approach to learning a
residual unit such as ours is simple and intuitive, we aim to improve on this approach, making full
use of all samples available.

C.1 Experiments: Sample Efﬁciency

We present the results of the vanilla LR learning the residual units. As discussed in section 3, LR re-
quires full-rank linear systems parameterized by samples to learn the exact ground-truth parameters.
However, with degenerate linear systems, LR is completely incapable of learning the parameters.
Therefore, there exists a hard threshold for the number of samples required by LR that makes the
linear equation system full-rank. With such dichotomic property on LR learning, we take the learn-
ing success rate among 1000 trials as the metric to evaluate the performance of LR with different
sample sizes and number of dimensions.

Tab. 2 shows the learning success rates with zero-mean Gaussian inputs. For each ﬁxed number of
dimensions, it appears there is a hard threshold that switches the learnability of LR. The exponential
sample complexity is also reﬂected there as the number of dimensions grows linearly. Tab. 3 shows
the rates with input mean non-zero, where an overall decline in success rates happens. This obser-
vation is explainable because the bottleneck of LR learning is the lower between the probability of
sampling a positive and a negative vector. A positive mean reduces the probability of the latter, and
thereby increases the sample size required.

15

Table 3: Learning success rate of vanilla LR on residual units with input x i.i.d.
(0.1, 1). With
the non-zero Gaussian mean, the success rates show an overall decline compared with the rates
shown in Tab. 2 for zero-mean Gaussian inputs.

∼ N

n

d

4

6

8

10

12

1e1

1e2

5e2

1e3

5e3

1e4

5e4

1e5

0.001

0.122

0

0

0

0

0

0

0

0

0.996

0.030

1

0.346

1

1

1

1

0

0

0

0

0

0

0.116

0.792

0

0

0

0

1

1

1

0.619

1

1

1

1

0

0.001

D Generalization of Layer 2 Learning

In this appendix, we discuss how layer 2 is learned without satisfying Cond. 2.1. It is a fact that
A∗ being a scale transformation w.r.t. some components causes scaling equivalence to our layer 2
objective functional minimizers just as in layer 1. To be more speciﬁc, we give a general version
of Thm. 4.1 that handles the case without Cond. 2.1 and describes the scaling equivalence of G2
minimizers. See Thm. D.1 for a formal description.

Theorem D.1 (objective minimizer space, layer 2, general). Let G2 be 1
2
as a functional, where C
diag(k) where for each j
1
kj ≤
1+A∗

∈
∈
1, if A∗ is a scale transformation w.r.t. the j-th component.

a)
b) kj = 1, if A∗ is not a scale transformation w.r.t. the j-th component.

Rd×m, h
[d],

j,j ≤

k
h

Ex

C0

∈

≥0. Then G2 (C, h) reaches its zero minimum iff CB∗ =
i

h (x) + x

Cy

−

2

k

As in layer 1, the scaling equivalence in layer 2 can also be obtained by assigning A∗ as a scale
transformation w.r.t. some component and using the properties of the ReLU nonlinearity. See
subsection E.1 for a detailed explanation of Thm. D.1. To obtain the scale factors k and correct
a scale-equivalent G2 minimizer C to the ground-truth, we observe linear models parameterized by
[d], [Cy]j = kj xj given that xj < 0. See Thm. D.2 and its
the scale factors, where for each j
∈
proof for justiﬁcations of the linear models that are used to compute k.

Theorem D.2 (scale factor, layer 2, general). Assume C is a minimizer of G1 in the context of
Thm. D.1. Then for any j

[d], the following three propositions are equivalent:

∈

a) A∗ is a scale transformation w.r.t. the j-th component.
b) [Cy]j /xj is a constant cn
c) [Cy]j /xj is a constant cp

j given that xj < 0.
j given that xj > 0.

Besides, if one of the above three propositions is true, then kj = cn
j .

Proof. For j

∈

[d], we ﬁrst prove a) =

⇒

b), c): From

⊤

A∗
j,:

= A∗

j,j ·

e(j) we have

⊤

kje(j)

(cid:1)

(cid:0)
(A∗x)+ + x
i
h

[Cy]j
xj

=

=

=

(cid:2)

kj

h(cid:0)
kj,
kj

(cid:26)

xj
+

(cid:3)
A∗
j,:x
xj
(cid:1)

A∗

j,j + 1

,

xj < 0
xj > 0

.

(cid:0)

(cid:1)

16

+ xj

kj

=

i

A∗

j,jxj
xj

h(cid:0)

+

+ xj

(cid:1)

i

i=1, samples drawn by Eq. 1; ˆC, a ˆGNPE

2 minimizer.

{

2 minimizer.

Algorithm 4 Rescale a ˆGNPE
1: Parameters: εtol > 0, LR objective tolerance.
n
(x(i), y(i))
2: Input:
}
3: Output: ˆk, a layer 2 scale factor estimate.
4: for each j
ˆkj ←
if the LR objective optimal ˆRj( ˆkj) > εtol then
(cid:3)

[d] do
x(i)
j ,

ˆCy(i)

∈
LR

x(i)
j <0

n

o

5:

(cid:2)

.

j

6:
ˆkj ←
7:
end if
8:
9: end for
10: return ˆk.

1.

Algorithm 5 Learn a ReLU residual unit layer 2.

{

(A∗x)+ estimate.

n
i=1, samples drawn by Eq. 1.

(x(i), y(i))
1: Input:
}
2: Output: ˆB, ˆΞ, a layer 2 and x
7→
3: Go to line 4 if QP, line 5 if LP.
4: Solve QP: Eq. 4 and obtain a ˆGNPE
2 minimizer, denoted by ˆC, ˆΞ. Go to line 6.
5: Solve LP: Eq. 5 and obtain a minimizer ˆC, then assign ˆξ(i)
6: Run Alg. 4 on
7: ˆB

}i∈[n], ˆC and obtain ˆk.

(x(i), y(i))

ˆCy(i), y(i)

diag−1(ˆk)

ˆCy(i)

LR

←

−

{

.

x(i) for each i

[n].

∈

diag−1(k)

ˆξ(i) + x(i)

i∈[n]

o
x(i) for each i

←

8: ˆξ(i)

n
←
of (A∗x)+.}
9: return ˆB, ˆΞ.

·

(cid:2)

−

(cid:3)

[n]. {Correct ˆΞ to the function estimation

∈

Then we prove

a) =

⇒ ¬

b),

¬

c):

¬

a) =

⇒ ∃

j′

∈

¬

[d] and j′

= j s.t. A∗

j,j′

= 0. Recall

[Cy]j
xj

kj

=

A∗

j,:x
xj
(cid:1)

h(cid:0)

+

+ xj

.

i

The value of xj′ affects the value of [Cy]j /xj because A∗
have supp p(xj′
when given both xj < 0 and xj > 0 because xj′ can be any real number.

= 0. In fact, from supp p(x) = Rd we
j,j′
xj) = R, indicating that the value of [Cy]j /xj can never be kept as a constant

|

With the exact derivations above, we are able to obtain a left inverse of B∗, namely C, that satisﬁes
CB∗ = Id. Consider Eq. 1 left multiplied by B∗C

B∗Cy = y.

(12)

Eq. 12 is also a noiseless unbiased linear model where Cy and y are observable, and thereby B∗ is
computable due to the easy solvability of its linearity.

Alg. 5 describes how a residual unit second layer is learned without satisfying Cond. 2.1: We ﬁrst
solve the QP/LP and obtain a scaling equivalence to an estimated left inverse of B∗ and the function
estimate, namely ˆC and ˆΞ. Then we compute the scale factor estimate ˆk by running Alg. 4, where
for each component index j
[d], we ﬁrst use a tolerance parameter as a threshold to gate whether
ground-truth layer 1 is a scale transformation, and if so, we run LR to estimate the model [Cy]j =
kjxj, otherwise the scale factor is directly assigned by 1. Upon correcting ˆC and ˆΞ by ˆk, we obtain
a layer 2 estimate ˆB by running LR to estimate the linear model Eq. 12. The strong consistency of
our results in this appendix is justiﬁed in Appendix G.

∈

17

6
6
6
E Exact Derivation of Objective Functional Minimizers

E.1 Layer 2

Lem. 4.2 is proved as follows.

0, random vector h(x) + x

Cy is always a zero vector,

−

= ”: Since h(x) = Cy

x
Proof. “
−
which implies G2(C, h) = 0. Hence “
2

⇐

”: Since r.v.

h(x) + x

Cy

“ =

≥
⇐

= ” holds.

0 we have

k

−
G2(C, h) = 0 =

≥

k
λ

⇒

x

Rd

∈

(cid:16)n
where λ is the Lebesgue measure on Rd.

Rd,
Proof by contradiction: Assume that
corresponding network output. Let f (x) = (Cy
x
ǫ =

f (x′) > 0,

B(x; δ) i.e.

δ > 0,

x′

∈

∃

(cid:12)
(cid:12)
(cid:12)

⇒

−

h(x) + x
k

−

Cy

2 > 0
k

o(cid:17)

= 0

(13)

[d], s.t. (Cy′

x′)i < 0, where y′ is the
i
∃
∈
x)i which is continuous on Rd. Therefore for
−
x′
k

< δ,

−

∃
f (x)

−

|
[h(x) + x

x
∀
∈
f (x′)
|
Cy]i > 0 =

< ǫ =

⇒

−

k
2f (x′) < f (x) < 0 =

h(x) + x

Cy

k

−

⇒ k

=

⇒

Cy]i > 0

[x
⇒
2 > 0.

−

−
πd/2
Γ (d/2 + 1)

Since λ (B(x; δ)) =

δd > 0 and B(x; δ) is a subset of the measured set in

Eq. 13, we have a contradiction. Thus Cy

x

0 holds pointwisely in Rd,

x

must be a null set. With h’s continuity, h must be x

−

≥

indicating
x.
Cy

−

7→

x

−

≥

(cid:9)

0 holds for any x

∈

Rd and its corresponding output y only if CB∗ is a

= Cy
−
” holds.

|

x

∈

h(x)

Rd
Therefore “ =
(cid:8)
Lemma E.1. Cy
diagonal matrix.

⇒

Proof. Let D = CB∗. We rewrite Cy

x

0 as

D

For further use, we substitute x by
−
still holds. Added by Eq. 14 we have

−
≥
(A∗x)+ + x
h
x and the resulting inequality D

0.

−

≥

x

i

A∗x)+

−

(
−

h

x

+ x

i

(14)

0

≥

(15)

D

A∗

k,: ·

x

(cid:0)(cid:12)
(cid:12)

0.

d×1 ≥
(cid:1)

(cid:12)
(cid:12)

Proof by contradiction: Assume D is not diagonal, then
following two cases:

i

= j, such that Di,j 6

= 0. Consider the

∃

a) Di,j > 0. We take the i-th row of Eq. 14 as follows

d

Di,k

A∗

k,: ·

+

x

+ xk

Xk=1
Let x−j = 0 and xj < 0. Then

h(cid:0)
d
k=1 Di,k

(cid:1)
A∗

k,: ·
b) Di,j < 0. We take the i-th row of Eq. 15 as follows

(cid:20)(cid:16)

P

i
+

x

(cid:17)

xi.

≥

(16)

+ xk

= Di,j ·

(cid:21)

xj < 0 =

⇒ ⊥

.

Let x = (A∗)−1 v, where v = e(j). Then

d

Xk=1

Di,k

A∗

k,: ·

x

0.

≥

(17)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

d
k=1 Di,k

P

(cid:12)
(cid:12)
(cid:12)

A∗

k,: ·

x

(cid:12)
(cid:12)
(cid:12)

= Di,j < 0 =

⇒ ⊥

.

With Lem. 4.2 and Lem. E.1, we prove Thm. D.1 as follows.

18

6
6
Proof. With Lem. 4.2, we only need to prove

x

∀

∈

Rd, Cy

x

0

≥

−

⇐⇒

CB∗ = diag(k).

“

= ”: We have

⇐

Cy

−

x = CB∗

For the i-th row of Eq. 18, consider the following two cases:

(A∗x)+ + x
h

i

x = diag(k)

−

(A∗x)+ + x
h

i

x.

−

(18)

a) A∗

x = Ai,ixi, i.e. A∗ is a scale transformation w.r.t. the i-th row. With

i,: ·
we have

1
1+A∗

i,i ≤

ki ≤

1

[Cy

−

x]i = ki

A∗

i,ixi

+

+ xi

xi = ki

+

A∗

i,ixi

+ (ki −

1) xi.

0, [Cy
For xi ≥
For xi < 0, [Cy
x

h(cid:0)
(cid:1)
kiA∗
x]i =
1
i,i + ki −
x]i = (ki −
0.
1) xi ≥
(cid:0)
= Ai,ixi. With ki = 1 we have

−
−

i

(cid:1)

b) A∗

i,: ·

−
xi ≥

(cid:0)

(cid:1)

0.

[Cy

−

x]i = ki

A∗

i,:x

+

+ xi

h(cid:0)

(cid:1)

xi =

A∗

i,:x

+

0.

≥

(cid:0)

(cid:1)

−

i

Hence “

= ” holds.

⇐

“ =

⇒

”: Let D = CB∗. With Lem. E.1, D is diagonal. Consider the following two cases:
a) A∗

x = Ai,ixi. The i-th inequality can be written as

i,: ·

[Cy

−

x]i = Di,i

A∗

i,ixi

= Di,i

h(cid:0)
A∗

i,ixi

+
(cid:1)

+

+ xi

xi

−
i
+ (Di,i −
Rd which contradicts with Eq. 19 in the

1) xi ≥

(19)

0.

(cid:0)

Proof by contradiction: we need to ﬁnd x
following three cases:
a) If Di,i ≤
0, then, xi > 0 =
b) If Di,i > 1, then, xi < 0 =
c) If 0 < Di,i < 1
1+A∗
i,i

Di,i
.

⇒
⇒ ⊥
∃

, then

(cid:1)
∈

A∗

i,ixi

+

+ (Di,i −

1) xi < 0 =

⇒ ⊥

.

(cid:0)
a > 0, s.t. Di,i =

(cid:1)

1
1+A∗

i,i+a . Letting xi > 0, we have

Di,i

A∗

i,ixi

+

+ (Di,i −

1) xi =

+

A∗
i,ixi
1 + A∗
(cid:0)

i,i + a − (cid:0)

(cid:1)

i,i + a

A∗
1 + A∗

xi
i,i + a
(cid:1)

< 0 =

⇒ ⊥

.

(cid:0)

Hence
x

1
1+A∗
i,i ≤
= Ai,ixi, i.e.

b) A∗

i,: ·

(cid:1)
Di,i ≤
∃

1.
= i, s.t. A∗
j

i,j > 0. The i-th inequality can be written as

[Cy

−

x]i = Di,i

= Di,i

A∗

i,:x
+
(cid:1)

h(cid:0)
A∗

i,:x

+

+ xi

xi

−
i
+ (Di,i −
Rd which contradicts with Eq. 20 in the

1) xi ≥

(20)

0.

(cid:0)

Proof by contradiction: we need to ﬁnd x
following three cases:
a) If Di,i ≤
b) If Di,i > 1, then, xi < 0
c) If 0 < Di,i < 1, then, xi > 0

0, then, xi > 0 =

Di,i
⇒
x−i ≤
∧

A∗
0 =
(cid:0)
xj ≤ −

∧

(cid:1)
∈

+

i,:x
.
⇒ ⊥
(cid:1)
∗
A
i,i
xi ∧
A∗
i,j

Hence Di,i = 1.

+ (Di,i −

1) xi < 0 =

⇒ ⊥

.

xk ≤

0 =

⇒ ⊥

, where k

= i, j.

Hence D = CB∗ = diag(k), and thereby “ =

⇒

” holds.

E.2 Layer 1

The proof of Lem. 5.3 is similar to that of Lem. 4.2 because the two lemmas follow the same idea,
which is to link objective functional minimization with always-hold inequalities. With Lem. 5.3 ,
we prove Thm. 5.1 as follows.

19

6
6
6
6
Proof. With Lem. 5.3, we only need to prove
[d], Ai,: = kiA∗
i,:, where 0
+
a∗⊤x

≤
a⊤x

i.e.

Rd,

x

ki ≤
0
≥

∈

∀
= ”: The case where a∗ = 0 is clear. If a∗ is not a zero vector and a = ka∗, we have

⇐⇒

−

≤

≤

(cid:17)

(cid:16)

a = ka∗, where 0

k

1.

x

Rd, (A∗x)+

Ax

0

∈
−
1. This is equivalent to what it is for a single row,

⇐⇒ ∀

≥

∈

∀

i

“

⇐

a) If a∗⊤x

≥

b) If a∗⊤x < 0,

0,

a∗⊤x

+

−

−

(cid:16)

(cid:16)

+

(cid:17)

a∗⊤x

(cid:17)

Hence “

= ” holds.

⇐

a⊤x = a∗⊤x

ka∗⊤x = (1

a⊤x =

−

−
ka∗⊤x

0.

≥

k) a∗⊤x

0.

≥

−

”: If a∗ = 0, a must be a zero vector, otherwise let x = a, then

“ =
not a zero vector, consider two cases below:

⇒

a⊤x < 0 =

⇒ ⊥

−

. If a∗ is

a) If a

= ka∗ where k

0. Let x = ka∗
2 (cos θ
(1

k
kak ·
1) < 0 ,

≥
a∗
k
a∗
(cid:27)
k
where θ denotes the angle between a and a∗.
b) If a = ka∗ where k > 1, then let x = a∗ we have

a∗⊤x =
a

cos θ) > 0

a⊤x =

k
k k

=

−

−

−

a

k

⇒

a∗, then x is not a zero vector, and

+

a∗⊤x

(cid:16)

(cid:17)

−

a⊤x < 0 =

⇒ ⊥

+

a∗⊤x

a⊤x = (1

k)

a∗

k

−

−

2 < 0 =
k

1 if a∗ is not zero, and thereby “ =

(cid:17)

.

⇒ ⊥
” holds.

⇒

Hence a = ka∗ where 0

(cid:16)
k
≤

≤

F QP Convexity

The LPs in the main paper are trivially convex. So in this appendix, we only justify the convexity of
our QPs: We ﬁrst prove the convexity of single-sample objectives, then the convexity of the empirical
objectives with nonparametric estimation, i.e. ˆGNPE
is obtained by the convexity of convex
function summations.
Lemma F.1. Suppose f (u) = 1

2 where u is a real matrix. Then f is convex w.r.t. u.

and ˆGNPE

T u

2

1

2 k

b
k

−

Lem. F.1 is easily obtained since the Hessian f ′′(T ) = T ⊤T is positive semideﬁnite. In the fol-
lowing, we demonstrate and justify the convexity of the QPs of both layers by rewriting their single-
sample objectives into the formulation of f and summing them without loss of convexity.
Theorem F.2. QP: Eq. 4, and QP: Eq. 7 are convex optimization.

Proof. First of all, constraints of both QPs are trivially linear and convex. Thus, we only need to
justify the convexity of the two empirical objectives, ˆGNPE
(see Eq. 7 and 4). Consider
2, which, in the
the single-sample version of ˆGNPE
formulation of f , can be rewritten with

1
2
(A, φ; x, h) = 1

, namely gNPE

and ˆGNPE

φ + Ax

h
k

2 k

−

1

1

x⊤

x⊤

. . .

T = 




which guarantees the convexity of gNPE
1
2 k

2, we have
k

ξ + x

Cy

−

1

1

. . .



, u =

, b = h

(21)

x⊤


1



1 w.r.t. A and φ by Lem. F.1. For gNPE

2

(C, ξ; x, y) =

A⊤
1,:
A⊤
2,:
...
A⊤
d,:
φ



















T = 





y⊤

−

y⊤

−

. . .

1

1

. . .

y⊤

−

20

C ⊤
1,:
C ⊤
2,:
...
C ⊤
m,:
ξ





















, u =


1




, b =

x

−

(22)

6
which guarantees the convexity of gNPE
Taking layer 1 as an example, by deﬁnition we have

2 w.r.t. C and ξ by Lem. F.1. Now we consider the summation.

ˆGNPE
1

(A, Φ) =

gNPE
1

(A, φ(i); x(i), h(i)).

(23)

Xi∈[n]
[n], equivalently, we take Φ as a variable instead of φ(i) in gNPE

For each i
∈
determining the value of gNPE
the sum ˆGNPE

1

1

. In this sense, gNPE

1

is convex w.r.t. A and Φ. Similarly, ˆGNPE

2

is convex w.r.t. A and Φ for each i
is convex w.r.t. C and Ξ.

∈

, but with only φ(i)

1

Φ
∈
[n]. Thus,

G Strong Consistency

In this appendix, we justify the strong consistency of our estimators for the residual unit layer 1/2
learning and the whole network.

According to Thm. D.1 and Thm. 5.1, the solutions to our objective functionals are continuous
sets. Besides, there are theoretical intermediate results that are also represented as continuous sets,
e.g. possible left-inverse matrices for B∗ in the results for layer 2. Thus, to analyze the consistency
of our learning algorithm, we deﬁne distances between sets, so that the convergence of sets can be
well deﬁned. Further point convergence results, i.e. layer 1/2 estimator strong consistency, are based
on the set convergence we deﬁne.
Deﬁnition G.1 (deviation). Let A
(M, d). The deviation of set A from the set B, denoted by D(A, B), is
D(A, B) = sup
a∈A

M be two non-empty sets from a metric space

d(a, B) = sup
a∈A

M and B

d(a, b),

inf
b∈B

(24)

⊆

⊆

where sup and inf represent supremum and inﬁmum, respectively.
Deﬁnition G.2 (Hausdorff distance). Let A
space (M, d). The Hausdorff distance between A and B, denoted by DH(A, B), is

M and B

⊆

⊆

M be two non-empty sets from a metric

{
Remark. In the following, we use the Frobenius norm to deﬁne the distance between matrices,
i.e. d(X, Y ) =

Y

DH(A, B) = max

D(A, B), D(B, A)
}

.

(25)

X
k

−

kF.

G.1 Layer 2

In this subsection, we prove the strong consistency of layer 2 estimator.

G.1.1 Objective Minimizer Space Estimator

For simplicity of notation, we use ˆSn to denote our layer 2 QP/LP solution space by n random
samples9 as a random set

and S∗ to denote the value space of C in Lem. 4.2 that minimizes layer 2 objective functional

ˆSn :=

C

{

∈

Rd×m : Cy(i)

x(i)

−

0,

i

∀

∈

≥

[n]

}

(26)

{

x

∈

C

S∗ :=

Rd×m : Cy

Rd and its corresponding output y
0,
}
For further use, we name ˆPn as the set of n sampled inputs that deﬁne ˆSn, i.e. ˆPn :=
}i∈[n]
where each x(i) is the same random variable in Eq. 26. By the deﬁnitions above, we describe the
strong consistency of our QP/LP as Lem. G.1.
Lemma G.1 (QP/LP strong consistency, layer 2). DH(ˆSn, S∗)

0 as n

x(i)

(27)

≥

−

∈

x

∀

{

.

a.s.
−−→

.
→ ∞

Proof. First we prove that DH(ˆSn, S∗)
0 only
if CB∗ = diag(k). We inherit the notation as deﬁning D = CB∗. Theorem D.1 is based on
Lem. E.1, and we prove them by raising points that show contradiction, i.e. violate the inequality
that pointwisely holds:

. Recall Thm. D.1, Cy

0 as n

→ ∞

−

≥

x

p
−→

9Here, we take samples as random variables for empirical analysis.

21

a) In the proof of Lem. E.1, we use d points:

e(i), for i

∈
−
< xi, so that Di,j (i

[d], to make

= j) cannot be positive; and an-

+

d
k=1 Di,k

A∗

x

+ xk

k,: ·
P
other d points: (A∗)−1e(i), to show

(cid:20)(cid:16)

(cid:17)

(cid:21)

cannot be negative. For each
ities, we know there exists a neighborhood of

−

A∗

d
k=1 Di,k

= j)
e(i) we use here, since the violations follow strict inequal-
(cid:12)
(cid:12)
Ni,
(cid:12)

< 0, so that Di,j (i

e(i), Ni = N(

e(i)), such that

k,: ·

P

(cid:12)
(cid:12)
(cid:12)

z

x

−

−

∀

∈

+ zk

< zi. We can similarly ﬁnd such neighborhood of each

d
k=1 Di,k

A∗

k,: ·

(cid:20)(cid:16)

+

z

(cid:17)

(cid:21)

(A∗)−1e(i) that the strict inequality holds within the neighborhood respectively. We index
P
them as Nd+1 to N2d.

b) In the proof of Thm. D.1, we further construct d points: for each i

xk ≤
x]i < 0, and eliminates the possibility of 0 < Di,i < 1 when A∗

[d], we take a point x
such that xi > 0
= i, j. This counterexample shows
[Cy
i,: is not a scale
transformation. We can similarly ﬁnd neighborhood of each point and index them as N2d+1
to N3d. Note that we omit some cases in the proof of Thm. D.1, because the ﬁrst 2d points
are sufﬁcient to use in those cases to show contradiction.

xj ≤ −

0, where k

xi ∧

∗
A
i,i
A∗
i,j

−

∈

∧

In the sampling procedure, if we sample at least one point in each neighborhood Ni, Thm. D.1
assures the solution we get ˆCn would lie in the true optimal set S∗. The probability that the sampling
procedure “omits” any of the neighborhoods is

(28)

(29)

P

ˆPn

N1 =

(cid:16)

\

or ˆPn

N2 =

∅

∅

or . . . or ˆPn

N3d =

\

\

∅

(cid:17)

≤

≤

Since the measure on each neighborhood P (Ni) =

p(x) > 0,

x∈Ni

3d

P

ˆPn

i=1
X
3d[1

(cid:16)

\

min
i∈[3d]

−

Ni =

∅
(cid:17)
P (Ni)]n

P

N1 =

ˆPn
∅
(cid:16)
Here we obtain DH(ˆSn, S∗)
Eq. 28

\

or ˆPn

N2 =

∅

\
0 as n

p
−→

→ ∞

R
or . . . or ˆPn

N3d =

∅

→

0, as n

.
→ ∞

(cid:17)
. Now we take the inﬁnite sum over the both sides of

\

P

ˆPn
(cid:16)

\

N1 =

∅

or ˆPn

N2 =

∅

or . . . or ˆPn

N3d =

Xn∈[∞]

3d

≤

1

min
i∈[3d]

−

P (Ni)
(cid:21)

= 3d

(cid:20)

Xn∈[∞] (cid:20)

\
n

\

1
mini∈[3d] P (Ni) −

∅

(cid:17)

1

< +

(cid:21)

.
∞

By the Borel-Cantelli lemma [Borel, 1909], DH(ˆSn, S∗)

a.s.
−−→

0 as n

.
→ ∞

G.1.2 Scale Factor Estimator

To avoid ambiguity, we use nsf to denote the number of samples used in Alg. 4. The samples pairs
nsf
i=1. Without loss of generality, the following discussion focuses on some ﬁxed
are
{
index j
j < 0

[d]. In Alg. 4, we plug in our estimator ˆCn and use LR to estimate kj given that x(i)

(x(i), y(i))
}

∈

knsf ( ˆCn) = arg min

k

1
2nsf

[ ˆCny(i)]j −
Xi∈[nsf] (cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

kx(i)
j

2

=

i∈[nsf] x(i)

j [ ˆCny(i)]j
x(i)
j

2

P

i∈[nsf]

.

(30)

We ﬁrst give the strong consistency of layer 2 scale factor estimator for as nsf
Lem. G.2.
Lemma G.2 (scale factor estimator strong consistency, layer 2). Suppose A∗ is a scale transforma-
tion w.r.t. the j-th component, and knsf ( ˆCn) is the nsf-sample estimator of kj via LR: Eq. 30 given

(cid:16)
→ ∞

(cid:17)
, as described in

P

22

6
6
6
that x(i)

j < 0. Deﬁne sets

Unsf,n :=

{

knsf (C) : C

ˆSn}

∈

, and U∗ :=

1
1 + A∗
j,j

"

, 1

.

#

(31)

Then lim

nsf→∞

lim
n→∞

DH(Unsf,n, U∗)

a.s.
== 0.

Proof. Following our notation, Thm. D.1 and Thm. D.2 ensure that if A∗ is a scale transformation
w.r.t. the j-th component, for any C belonging to the true optimal set S∗, knsf(C)
1
.
, 1
1+A∗
∈
j,j
Z+. Note that
And the “iff” statement strengthens that U∗ =
i
since S∗
ˆCn ∈

Unsf,n. We only need to prove D(Unsf,n, U∗)

ˆSn, we have U∗
S∗,

⊂
ˆSn and

knsf (C) : C

for any nsf

.
→ ∞

∈
0 as n

a.s.
−−→

S∗

C

⊂

∈

∈

∀

∀

}

{

h

knsf( ˆCn)

−

1

knsf(C)
(cid:12)
(cid:12)
(cid:12)
2 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
2 

x(i)
j
(cid:16)
1

(cid:17)

x(i)
j

i∈[nsf]

(cid:16)
1

(cid:17)

x(i)
j

(cid:16)

(cid:17)
x(i)
j

i∈[nsf]

i∈[nsf]

(cid:12)
(cid:12)
(cid:12)
=

≤

≤

=

P

P

P

P

Then

i∈[nsf]

Xi∈[nsf]

x(i)
j

(e(j))⊤

(cid:20)

ˆCn −

(cid:16)

C

B∗

A∗x(i)

(cid:17)

(cid:20)(cid:16)

(cid:17)

+

+ x(i)

(cid:21)(cid:21)

x(i)
j

(e(j))⊤

ˆCn −
(cid:16)

Xi∈[nsf] (cid:12)
(cid:12)
(cid:12)



(cid:12)
(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)

x(i)
j

2 

ˆCn −

C

B∗

F k

C

B∗

A∗x(i)

+

(cid:17)

2
(cid:13)
(cid:13)
(cid:13)
A∗
kF (
k

(cid:16)(cid:13)
(cid:13)
(cid:13)
kF + 1)

2
(cid:13)
(cid:13)
(cid:13)
x(i)

(cid:13)
(cid:13)
(cid:13)

2



(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)


B∗

Xi∈[nsf] (cid:12)
(cid:12)
(cid:12)
kF (
k
x(i)
j

A∗

i∈[nsf]

(cid:12)
(cid:13)
(cid:12)
(cid:13)
(cid:12)
(cid:13)
kF + 1)

2

(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
x(i)

(cid:16)

(cid:17)

h(cid:12)
(cid:12)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12)
P

2

i

(cid:13)
(cid:13)

ˆCn −
(cid:13)
(cid:13)
(cid:13)

C

.

F

(cid:13)
(cid:13)
(cid:13)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
x(i)

(cid:17)





2
(cid:13)
(cid:13)
(cid:13)

knsf( ˆCn)

sup
ˆCn∈ˆSn

inf
C∈S∗

i∈[nsf]

≤ P

which implies

x(i)
j

(cid:12)
(cid:12)
(cid:12)

h(cid:12)
(cid:12)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12)
P

B∗

−

knsf(C)
(cid:12)
(cid:12)
(cid:12)
kF + 1)

kF (

A∗
k
x(i)
j

2

i∈[nsf]

(cid:16)

(cid:17)

x(i)

(cid:13)
(cid:13)

2

(cid:13)
(cid:13)

i

sup
ˆCn∈ˆSn

inf
C∈S∗

C

ˆCn −
(cid:13)
(cid:13)
(cid:13)

F

(cid:13)
(cid:13)
(cid:13)

(32)

(33)

D(Unsf,n, U)

≤ P

i∈[nsf]

x(i)
j

h(cid:12)
(cid:12)
(cid:12)

B∗

A∗

kF (
k
x(i)
j

kF + 1)

2

x(i)

(cid:13)
(cid:13)

2

i

(cid:13)
(cid:13)

D(ˆSn, S∗).

(34)

i∈[nsf]

(cid:17)
limit over both sides of Eq. 34. With the strong law of large numbers10 we have

(cid:16)

k

(cid:12)
(cid:12)
(cid:12)
P

Take the nsf

→ ∞

lim
nsf→∞

D(Unsf,n, U)

≤

E

x(i)
j

h(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

B∗

k

A∗

kF (
k
x(i)
E
j

kF + 1)

2

x(i)

(cid:13)
(cid:13)

2

i

(cid:13)
(cid:13)

D(ˆSn, S∗), w.p. 1.

(35)

Since D(ˆSn, S∗)

a.s.
−−→

0 as n

→ ∞

, we have D(Unsf,n, U∗)

h

i

a.s.
−−→

0 as n

→ ∞

then nsf

.
→ ∞

10Here we assume the Kolmogorov’s strong law assumption on moments [Sen and Singer, 1994] is met as

commonly done in empirical analysis.

23

G.1.3 Layer 2 Weights Estimator

In Alg. 5, we solve B via LR. Let ˆz(i) = diag−1(ˆk)
Rd, where ˆk is obtained through
Alg. 4 with input ˆCn. Assume we are using sample size of nw to do the LR. The optimization
problem is

ˆCny(i)

∈

·

min
B

y(i)

Xi∈[nw] (cid:13)
(cid:13)
(cid:13)

−

2

Bˆz(i)

(cid:13)
(cid:13)
(cid:13)

Now we present the strong consistency of layer 2 estimator, as described in Thm. G.3.

Theorem G.3 (strong consistency, layer 2). Suppose ˆBnsf is the solution to Eq. 36. Then ˆBnw
B∗ as n, nsf, nw

.
→ ∞

(36)

a.s.
−−→

Proof. Let β denote vec B (ﬂattening B into a vector), then Bˆz(i) =
operation

Im. Here the
denotes the Kronecker product. Then we can deﬁne an equivalent optimization problem

ˆz(i)

⊗

⊤

(cid:0)

(cid:1)

min
β

1
2nw

y(i)

Xi∈[nw] (cid:13)
(cid:13)
(cid:13)
(cid:13)

⊤

ˆz(i)

(cid:20)(cid:16)

(cid:17)

−

⊗

Im

β

(cid:21)

2

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Take the derivatives of β, we obtain

2

−

⊤

ˆz(i)

Xi∈[nw] "(cid:20)(cid:16)

(cid:17)

Im

⊗

⊤

y(i)

(cid:21)

(cid:18)

⊤

ˆz(i)

(cid:17)

−

(cid:20)(cid:16)

⊗

Im

β

(cid:21)

(cid:19)#

= 0

Then the optimal solution ˆβnw of this optimization can be written in closed form:

(37)

(38)

ˆβnw =


Xi∈[nw] (cid:20)(cid:16)


⊤

ˆz(i)

(cid:17)

Im

⊗

⊤

⊤

ˆz(i)

(cid:21)

(cid:20)(cid:16)

(cid:17)

−1

Im

⊗

−1

(cid:21)





⊤

y(i)

⊤

ˆz(i)

(cid:17)

Im

⊗

(cid:21)









Xi∈[nw] (cid:20)(cid:16)

ˆz(i)





Xi∈[nw] h

Im

⊗

⊤

ˆz(i)

i (cid:20)(cid:16)

(cid:17)

Im

⊗

−1

(cid:21)





⊤

ˆz(i)

ˆz(i)

(cid:16)

(cid:17)

⊗

(cid:21)


Xi∈[nw] (cid:20)


⊤

ˆz(i)

ˆz(i)

Xi∈[nw]

(cid:16)

(cid:17)













⊗

Im

Im






−1





Xi∈[nw]

ˆz(i)









Xi∈[nw]

⊗

Im

Im


y(i)





y(i)





ˆz(i)

⊗

Im

y(i)

i





Xi∈[nw] h




ˆz(i)

⊗

⊗

=

=

=

ε > 0,

N such that

We inherit the notation from the last two subsections. By Lem. G.1, d( ˆCn, S∗)
Thus
w.p. 1. Then by Lem. G.2,
knsf (Cn)
have
(cid:12)
(cid:12)
(cid:12)

.
→ ∞
ε
≤
Nsf, we
Kε w.p.1. For simplicity, we omit the under-script nsf of knsf in the

ε w.p. 1, i.e.
∀
K > 0, for ε that is small enough,

S∗ s.t. d( ˆCn, Cn)
nsf

Cn ∈
∃
Nsf such that
∃

∀
∃
knsf ( ˆCn)

N , d( ˆCn, S∗)

0 as n

∃
≤

≥

≤

≥

−

(cid:12)
(cid:12)
(cid:12)

n

∀

a.s.
−−→

24

following discussion. In fact,

ˆz(i)
j −

z(i)
j

=

(cid:12)
(cid:12)
(cid:12)
=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
+

≤

1

≤

1

Then

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
1
(cid:12)
k( ˆCn)k(Cn) (cid:20)
1
k( ˆCn)k(Cn) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
k( ˆCn)k(Cn) (cid:12)
(cid:12)
(cid:12)
2
1 + A∗
(cid:12)
j,j
(cid:12)
1 + A∗
Kε
(cid:1)
(cid:0)
j,j
−
2
1 + A∗
(cid:0)
j,j
1 + A∗
Kε
(cid:1)
(cid:0)
j,j
−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)

1
k( ˆCn)

e(i)

⊤

ˆCny(i)

(cid:16)

(cid:17)

1
k(Cn)

−

(cid:16)

⊤

e(i)

Cny(i)

(cid:17)
⊤

Cn

y(i)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Cny(i)
(cid:12)

(cid:21)
⊤

(cid:16)
k( ˆCn)

(cid:17)

e(i)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Cny(i)

⊤

k(Cn)

e(i)

(cid:16)

(cid:17)

k(Cn)

e(i)

⊤

⊤

(cid:16)

(cid:17)

ˆCn −

k( ˆCn)

e(i)

(cid:16)

(cid:17)

k(Cn)

e(i)

ˆCny(i)

−

k(Cn)

e(i)

⊤

Cny(i)

−

(cid:16)

y(i)

(cid:16)(cid:13)
(cid:13)
(cid:13)
y(i)

(cid:13)
(cid:13)
(cid:13)

(cid:16)(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:1)

(cid:1)

(cid:16)
k( ˆCn)

−

(cid:17)
ˆCn −
(cid:13)
(cid:13)
(cid:13)
+ dK (
k

Cn

+

F

(cid:13)
(cid:12)
(cid:13)
(cid:12)
(cid:13)
(cid:12)
A∗
kF + 1)

x(i)

(cid:13)
(cid:13)
(cid:13)

(cid:17)

(cid:13)
(cid:13)
(cid:13)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
CnB∗
k

(cid:17)

k(Cn)
(cid:12)
(cid:12)
(cid:12)

ε

A∗

kF (
k

kF + 1)

x(i)

(cid:13)
(cid:13)
(cid:13)

(cid:17)

(cid:13)
(cid:13)
(cid:13)
(39)

−

z(i)

z(i)

⊤

(cid:17)

⊤

(cid:16)

z(i)

F
(cid:21)(cid:13)
(cid:13)
(cid:13)
z(i)
(cid:13)

(cid:16)
ˆz(i)

(cid:17)

⊤

−

(cid:17)

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)

⊤

(cid:16)

(cid:17)

F
(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ z(i)

ˆz(i)

⊤

(cid:17)
⊤

(cid:16)
ˆz(i)

F

−

(cid:20)(cid:16)
+ 2

z(i)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
is also bounded by

(cid:17)
z(i)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
−

ˆz(i)
(cid:13)
(cid:13)
(cid:13)

z(i)

(cid:20)(cid:16)

(cid:13)
(cid:13)
(cid:13)

⊤

ˆz(i)

ˆz(i)

z(i)

z(i)

⊤

(cid:13)
(cid:13)
(cid:13)
=
(cid:13)

≤

≤

≤

(cid:16)
ˆz(i)

(cid:16)
ˆz(i)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
h
(cid:13)
(cid:13)
(cid:13)
(cid:13)
h
(cid:13)
(cid:13)
(cid:13)
Xj∈[d] (cid:16)
ˆz(i)

ˆz(i)

ˆz(i)

−
⊤

(cid:17)
ˆz(i)

(cid:16)
z(i)

(cid:17)
ˆz(i)

−

(cid:17)
z(i)

(cid:16)
⊤

ˆz(i)

−

−

i (cid:16)

z(i)

ˆz(i)

F

(cid:13)
(cid:13)
(cid:13)
z(i)
(cid:13)

(cid:17)

−

F
(cid:13)
(cid:13)
⊤
(cid:13)
(cid:13)
(cid:17)
+

⊤

i h
z(i)
j

2

+ 2

i
z(i)

ˆz(i)
j −

(cid:17)

⊤

−

z(i)

z(i)

(cid:0)

(cid:1)
ˆz(i)

ˆz(i)

(cid:13)
(cid:13)
(cid:13)
⊤

F

(cid:13)
(cid:13)
(cid:13)
⊤

(cid:0)

(cid:1)

(cid:13)
(cid:13)
Xi∈[nw]
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Denote
expression we have ˆP and ˆQ. Hence,

i∈[nw] z(i)

z(i)

i

(cid:0)

(cid:1)

hP

⊤

(cid:13)
(cid:13)
as P and
(cid:13)

It follows that
can prove

and

(cid:13)
(cid:13)
(cid:13)

β∗

ˆβnw −
(cid:13)
(cid:13)
(cid:13)

F

(cid:13)
(cid:13)
(cid:13)

=

≤

=

(ε). With similar techniques we

O

(cid:16)

(cid:17)

−

Xi∈[nw]

⊤

z(i)

z(i)

(cid:16)

(cid:17)

ˆz(i)

Im −

z(i)

Im

⊗

⊗

F ≤ O

(ε)

≤ O

(40)

(41)

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(ε)

i∈[nw] z(i) as Q. Substitute z(i) with ˆz(i) in the above

(cid:13)
(cid:13)
(cid:13)

P

Im

y(i)

i

(cid:17)

Im

y(i)

−1

ˆQ

i
−1

(cid:16)h

ˆQ

⊗

⊗

(cid:16)h
−1

i
Im

[Q

(cid:17)
i
Im] y(i)

i
−1

(cid:16)

⊗

ˆQ

−

Q

(cid:17)

⊗

Im

⊗

Im

⊗

ˆP

⊗

Im

⊗

h(cid:16)

⊗

F (cid:13)
(cid:13)
i
(cid:13)
(cid:13)
Im] y(i)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

ˆP

ˆP

(cid:13)
h
(cid:13)
(cid:13)
(cid:13)
(cid:13)
h
(cid:13)
(cid:13)
+
(cid:13)

(cid:13)
h
(cid:13)
(cid:13)
ˆP
(cid:13)
(cid:13)
h
(cid:13)
(cid:13)
[Q
(cid:13)
(cid:13)
(cid:13)
(cid:13)

25

Im]−1

[Q

Im] y(i)

⊗

(cid:16)
−1

Im

[Q

Im] y(i)

(cid:16)

i
Im]−1

[Q

Im] y(i)

⊗

⊗

−

−

[P

⊗

ˆP

h

⊗

[P

⊗

−

(cid:17)
Im

y(i)

i

F

(cid:13)
(cid:13)
(cid:13)

(cid:16)
ˆP

+

(cid:13)
h
(cid:13)
(cid:13)
(cid:13)

Im

⊗

−1

i

F
(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:17)(cid:13)
(cid:13)
(cid:13)
[P
(cid:13)

−

⊗

Im]−1

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)

In the ﬁrst part, by triangle inequality,

−1

Im

ˆP

⊗

(cid:13)
i
h
(cid:13)
(cid:13)
So we only need to prove
(cid:13)

ˆP

⊗

−1

−1

Im

i
[P

−

⊗

F ≤
(cid:13)
(cid:13)
(cid:13)
ˆP
(cid:13)

⊗

(cid:13)
h
(cid:13)
(cid:13)
Im
(cid:13)

i

(cid:13)
h
(cid:13)
(cid:13)
(cid:13)
(P + ∆P )−1 = P −1

Denote ˆP
By simple calculation we have

−

P = ∆P . From Eq. 39, we know every entry of ∆P

[P

−

⊗

Im]−1

Im]−1

+

[P

Im]−1

(42)

F

⊗

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(ε) to claim
(cid:13)

(cid:13)
(cid:13)
ˆβnw −
(cid:13)
β∗
(cid:13)
Im can be bounded by
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

F ≤ O

O

(ε).

(ε).

⊗

F ≤ O
(cid:13)
(cid:13)
(cid:13)
(cid:13)

P −1∆P P −1 +

(ε2).

O

−

P −1

−

ˆP −1 = P −1∆P P −1 +

(ε2) =

O

(ε).

O

(43)

(44)

Then we have

G.2 Layer 1

In this subsection, we justify the strong consistency of layer 1 objective functional minimizer esti-
mator in detail, i.e. the layer 1 QP/LP solution space. We will omit the detailed proof of Alg. 2 line 4
to 7 strong consistency since it is similar to the proof of Lem. G.2. Besides, we also omit the strong
(A∗x)+ function estimator because it can be directly obtained by Lem. G.1
consistency of the x
7→
and G.2 and the continuous mapping theorem.

We use a new optimization problem equivalent to the optimization of G1. Before that, we ﬁrst deﬁne
the equivalence between two optimization problems as follows.
Deﬁnition G.3. Let opt1 and opt2 be two optimization problems, with f1, f2 as the respective
objective functions. Then opt1 and opt2 are said to be equivalent if given a feasible solution
to opt1, namely x1, a feasible solution to opt2 is uniquely corresponded, namely x2, such that
f1(x1) = f2(x2), and vice versa.

The new optimization problem and its equivalence to the optimization of G1 is described in
Lem. G.4.
Lemma G.4. The optimization of G1 (Eq. 6) is equivalent to

f (A) =

Ex

(Ax

min
A

−

1
2

h)+

2

.

(45)

(cid:20)(cid:13)
(cid:13)
(cid:13)
Proof. To see this, suppose A1 is one optimal solution to Eq. 45, then we can construct r1(x) =
(A1x

h)+ so that G1 (A1, r1) = f (A1) and the optimality implies

(cid:13)
(cid:13)
(cid:13)

(cid:21)

−

min
A, r

G1 (A, r)

min
A

≤

f (A).

On the other hand, suppose (A2, r2) is an optimum of G1. Let r3(x) = (h
and h be the corresponding hidden output, if [h
otherwise [r3(x) + A2x
that we know

A2x]j ≥
−
[h2 (x) + A2x

j = [A2x

−
0, then [r3(x) + A2x

A2x)+, then

Rd
∈
∀
h]j = 0,
j since h2 is nonnegative. So

h]2

h]2

h]2

≤

−

−

−

x

G1(A, r) = G1(A2, r2) = G1(A2, r3) = f (A2)

−
min
A, r

From the optimality, we further have

min
A, r

G1(A, r)

min
A

≥

f (A)

From the simple calculation above, we can see that one optimal solution to Eq. 45 has a one-to-one
correspondence to an optimal solution to G1.

Similarly, the empirical version of the two problems are equivalent, which indicates their consistency
of the empirical estimation being equivalent. In the following, we justify the strong consistency of
empirical Eq. 45 instead of G1

ˆfn (A) =

min
A

1
2n

Ax(i)

h(i)

−

2

+

.

(cid:17)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xi∈[n] (cid:13)
(cid:16)
(cid:13)
(cid:13)
(cid:13)
26

(46)

·

{

0

A∗

diag(k)

as the true optimal solution set, and ˆTn as
Denote T∗ :=
∈
the optimal solution set corresponding to the n-sample problem. In the following, we justify four
conditions in a row that hold for f to derive the strong consistency of its optimal solution estimator.
Lemma G.5. Let ˆTn′ be the layer 1 QP/LP solution space by n′ samples. Then there exists a
compact set C determined by A∗, namely C(A∗), s.t. ˆTn′

C(A∗) w.p. 1 as n

kj ≤

1, j

[d]

≤

}

|

⊂

.
→ ∞

[d], let a∗ be the l-th row of A∗, and ˆal be the l-th row of ˆAn′. We’d like ﬁrst to prove

Proof.
l
∈
∀
that the set

ˆTl

n′ =

{

al : al is the l-th row of A, where A

ˆTn′

}

∈

is compact w.p.1.
Suppose n′ > d, and among the n′ samples, we classify them into two folds. To avoid ambiguity, let
[q]; and v(j) be the points such that (a∗)⊤v(j) < 0,
u(i) be the points such that (a∗)⊤u(i) > 0, i
0,
j

q]. From the analysis of Thm. 5.1, we have (a∗)⊤u(i)

[q] and ˆa⊤v(j)

ˆa⊤u(i),

[n

∈
q]. It follows that we can rewrite ˆTl

≥
n′ as a polyhedron

i
∀

∈

∈
j
∀

∈

−
[n

−

ˆTl

n′ =

a

{

∈

Rd : a⊤u(i)

(a∗)⊤u(i), a⊤v(j)

0

}

≤

≤

(48)

We are going to show the polyhedron ˆTl
d
n′ , such that
(˜a + λd)⊤ u(i) = ˜a⊤u(i) + λd⊤u(i)

= 0 and ˜a

Rd, d

ˆTl

∈

∈

∃

∀

n′ is bounded by contradiction. If it is not bounded, then
λ > 0, ˜a + λd

ˆTl

(a∗)⊤ u(i)

⇐⇒

≤

(a∗)⊤ u(i)

−

≤

˜a⊤u(i) (49)

∈

n′ . Then
λd⊤u(i)

(47)

≤

similarly,

−

(˜a + λd)⊤ v(i) = ˜a⊤v(j) + λd⊤v(j)

From the deﬁnition, λ can be arbitrarily big, then d⊤u(i)
[n

q].

0,

≤

0

[q], and d⊤v(j)

(50)

0,

j
∀

≤

∈

≤
i
∀

∈

u(i)

Since we know span
{
(Otherwise if d⊤u(i) = 0 for all i
assumption that, ˆTl
w.p. 1

= Rd w.p. 1, then there

) < 0, w.p. 1.
= Rd or d = 0.) Under our
n′ is not bounded, we know the following system (w.r.t x) has a feasible solution

some i∗ such that d⊤u(i

[q], then either span

u(i)

} 6

∈

∃

}

{

∗

x

U
V

−
−
⊤

(cid:21)
and

(cid:20)
u(i)

∗

0, x⊤u(i

) < 0

≥

(I)

⊤

v(j)

respectively. By Farkas’ lemma [Farkas, 1902],

where every row of U and V is
the system

−
is not feasible (w.p. 1). We claim that u(i
second system actually has a feasible solution and thus it raises the contradiction.

) lies in the conic hull of

−

≥

−

·

∗

(cid:0)

[

(cid:1)
U ⊤,

(cid:0)
V ⊤]

(cid:1)
x = u(i

∗

), x

0

(II)
v(j)’s w.p. 1. So that the

Denote the conic hull as



) /

∈
= 0, such that b⊤u(i

Now suppose u(i
b

∗

H =

t

∈

Rd : t =

λj

v(j)

−

, λj ≥

0 for

j
∀

∈

[n

−

q]

Xj∈[n−q]

(cid:17)
H, by the supporting hyperplane theorem [Luenberger, 1997],
)



(cid:16)

H. Then by deﬁnition,

b⊤t for

∗




b

∃

∈

Rd,

−
Denote the hyperplane J =

∈ {
t : b⊤t

≤
∗
b⊤u(i

)

, then

∗

b⊤u(i

, for

)

}

j
∀

∈

[n

−

q]

≤

v(j)

{

t
∀
∈
t : b⊤t

≤
v(j)

}
J, for

∗

u(i

H

) /
∈

P

P

j
∀
Since we have here a geometric sequence, we know its inﬁnite sum is bounded. By Borel-Cantelli
H w.p. 1. Then system II is feasible w.p. 1. So that
lemma [Borel, 1909], we conclude that u(i
ˆTl

= P

[n

−

−

−

≤

q]

∈

∈

∈

∈

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:17)

(cid:16)

)

∗

v(j)

J

n′ is compact w.p. 1.

n−q

27

6
6
Now we prove that there exists a compact set C(A∗), s.t. ˆTn′
we focus on the analysis of one row. As discussed above, ˆTl
of all u(i) sampled in estimating ˆTn′ , and ˆW2
ˆTn′′ , similarly deﬁne sample point sets ˆW1
conic hull of ˆW1
we’d omit it here.

C(A∗) w.p. 1 as n
. Similarly,
→ ∞
n′ is compact w.p. 1. Let ˆW1
n′ be the set
n′ be the set of all v(j) sampled. Now for another set
n′ , u(i) lies in the
u(i)
H w.p. 1, so
)

ˆW1
n′′ and ˆW2
n′′ . Actually, this part of proof is much like the way we prove u(i

n′′ . We claim that

⊂

∈

∈

∀

∗

a

ˆTn′′ , let x(1) and x(2) be two different points in ˆW1
λ1 ≥
∀

0 and λ2 ≥

n′′ . Then a⊤(λ1x(1) + λ2x(2))
0. This simple calculation reveals a⊤u(i)

n′ (from the claim we made). This implies that ˆTn′′

≤
≤
C(A∗) w.p. 1 as

∈

∀
(a∗)⊤(λ1x(1) + λ2x(2)) for
(a∗)⊤u(i) for
, too.
n

u(i)

ˆW1

∈

∀

⊂

→ ∞

Lemma G.6. The minimizer space of f (A), i.e. T∗ =
contained in C(A∗).
Lemma G.7. f (A) is ﬁnite valued and continuous on C(A∗).

{

diag(k)

A∗

·

0

kj ≤

≤

|

1, j

, is

[d]

}

∈

→ ∞

2

Lem. G.6 and G.7 are easily obtained by the formulation of f (see Eq. 45) and Lem. G.5.
Lemma G.8 (uniform a.s. convergence). ˆfn(A)

, uniformly in A

f (A) as n

a.s.
−−→

C(A∗).

∈

Proof. Name single-sample objective g(x, A) = 1
2
is guaranteed by the uniform law of large numbers [Jennrich, 1969]:
(cid:13)
(cid:13)
(cid:13)

(Ax

(cid:13)
(cid:13)
(cid:13)

−

h)+

a) By Lem. G.5, C(A∗) is a compact set.
b) g is continuous w.r.t. A by its formulation and measurable over x at each A
c) In fact,

C(A∗).

∈

. The uniform a.s. convergence

g(x, A) =

(Ax

1
2
(cid:13)
Ax
(cid:13)
(cid:13)
A
k

2

h)+

−
2 +
k
kF +

(cid:13)
2
A∗x
(cid:13)
k
k
(cid:13)
A∗
kF)

k

≤ k
(

x
k

2 .
k

≤
C(A∗) is in a compact set,

Since A

∈

g(x, A)

sup
A∈C(A∗) k

A

kF +

≤ "

A∗

k

kF

# k

x

2 .
k

(51)

Thus the dominating function exists11.

By Lem. G.5, G.6, G.7 and G.8, all of the conditions are satisﬁed in [Shapiro et al., 2014, Thm. 5.3].
Thus, we have the strong consistency of layer 1 objective optima estimator as described in Lem. G.9.
Lemma G.9 (QP/LP strong consistency, layer 1). DH(ˆTn′ , T∗)

0 as n

a.s.
−−→

.
→ ∞

Similar to Lem. G.2, we have the strong consistency of the layer 1 scale factor estimator as described
in Lem. G.10.
Lemma G.10 (scale factor estimator strong consistency, layer 1). Let kn′
estimator of kj via LR: Alg. 2 line 5. given that h(i)

( ˆCn) be the n′

sf-sample

j > 0. Deﬁne sets

sf

Vn′

sf,n :=

Then lim
n′

sf→∞

lim
n′→∞

DH(Vn′

sf,n′, V∗)

kn′

(A) : A

sf

{
a.s.
== 0.

ˆTn′

}

∈

, and V∗ := [0, 1] .

(52)

Remark. In case kj = 0, suppose the algorithm ﬁnds a solution over a continuous distribution with
[0, 1] as support and the probability that it ﬁnds a solution with scale factor 0 is 0.

2
11Here, we assume E[kxk

] < +∞ as commonly done in empirical analysis.

28

With Thm. 5.2 and the continuous mapping theorem, the strong consistency of layer 1 estimation is
guaranteed.
Theorem G.11 (strong consistency, layer 1). Suppose ˆAn′ is scaled by ˆkn′
n′, n′

. Then ˆAn′

a.s.
−−→

A∗ as

sf

.
sf → ∞

By Thm. G.3 and G.11, Thm. 6.3 is guaranteed by the continuous mapping theorem.

29

