0
2
0
2

b
e
F
9
2

]

C
D
.
s
c
[

1
v
0
9
2
0
0
.
3
0
0
2
:
v
i
X
r
a

Enumerating Hardware–Software Splits with Program Rewriting

Gus Smith, Zachary Tatlock, and Luis Ceze
University of Washington

1

Introduction

2 Overview of Solution

The Instruction Set Architecture (ISA) has long been
an essential abstraction in the computing world. De-
scribing complex hardware behavior using a manage-
able set of instructions has largely decoupled com-
puter architects from compiler engineers, allowing
the ﬁelds to make signiﬁcant progress independently.
During the reign of Moore’s Law, this decoupled ad-
vancement allowed software to beneﬁt from regular
improvements in hardware. But now, as Moore’s
Law fails to hold, researchers have been increasingly
searching for innovations which cross the ISA bound-
ary and challenge the assumption of a rigid software–
hardware border.

Especially interesting is the idea of software–
hardware codesign: the process of designing software
and hardware in lockstep, rather than solidifying the
ISA and designing in isolation. A core problem in
software–hardware codesign is in the sheer size of
the design space. Without a set ISA to constrain
the software–hardware interface, the design space ex-
plodes to all possible combinations of hardware and
software, with all possible interfaces between them.

This work presents a strategy for managing the
massive hardware–software design space within the
constrained domain of machine learning inference
workloads and accelerators. We ﬁrst propose En-
gineIR, a new language for representing machine
learning hardware and software in a single program.
Then, using equality graphs—a data structure from
the compilers literature—we suggest a method for ef-
ﬁciently enumerating the design space by performing
rewrites over our representation.

Within the scope of this work, we view hardware-
accelerated machine learning inference workloads as
being comprised of three distinct hardware or soft-
ware components. First, at their base, the workloads
are built on calls to ﬁxed size kernels, such as a 16×16
matrix multiplication; a machine learning accelera-
tor accelerates workloads by providing ﬁnely tuned
hardware engines implementing these kernels. Sec-
ond, software schedules expand ﬁxed-size kernel calls
to take arbitrary-sized input by using loops or par-
allelism to call kernels multiple times. Finally, some
concept of storage hardware carries intermediate val-
ues between kernel invocations.

To explore hardware–software splits, we begin with
ML workloads written in Relay, which is the interme-
diate representation used by the TVM compiler[1].
Relay represents a machine learning workload as a
series of kernel calls, but does not make explicit the
underlying hardware and software components de-
scribed above.

From Relay, we lower to EngineIR, a Relay-like lan-
guage which fully reiﬁes the hardware engines, hard-
ware storage buﬀers, and software schedules under-
lying Relay programs. EngineIR engines represent
underlying computational hardware. An engine is
declared with a set of parameters (H, W, C, and K,
in Figure 1) corresponding to the parameters of the
underlying hardware design. Each usage of a Relay
operator will be converted to a call to an EngineIR
engine instantiation with concrete parameters. Dur-
ing this lowering process, a software schedule will also
be created, implementing the kernel using the under-
lying hardware engine. Similarly, each converted call
will be given an explicit storage buﬀer for its out-

1

 
 
 
 
 
 
engine conv2d<uint H, uint W,
uint C, uint K>

(Tensor data, Tensor kernel);

// %0 = relay.nn.conv2d(data, kernel);
for i in ...

for j in ...

buffer<...>[..i..j..] =

conv2d<16, 16, 3, 3>

(data[..i..j..], kernel);

Figure 1: An engine declaration, plus a concrete in-
stantiation of the engine wrapped in a software sched-
ule of nested for loops.

Figure 2: Example equality graph rewrites.

put. Figure 1 shows an engine declaration for a con-
volution engine, including parameters for the input
size (height, width, channels, and kernel size, respec-
tively). Additionally, it shows a Relay nn.conv2d
call being reiﬁed into a software schedule over a con-
crete engine and concrete storage.

Once a workload is lowered from Relay to En-
gineIR, we enumerate the space of
functionally
equivalent hardware–software designs using equality
graphs[2] (e-graphs). E-graphs are able to represent
an exponential number of equivalent programs eﬃ-
ciently; this key feature is what makes our hardware–
software search space manageable. To enumerate the
search space, we ﬁrst transform the EngineIR pro-
gram into its equivalent e-graph. We then perform
rewrites over the e-graph. These rewrites encode
transformations which alter the hardware–software
split of a program, but preserve functional equiva-
lence. Figure 2 shows two such rewrites. The pro-
gram is a single call to a 128-bit wide ReLU, a com-
mon machine learning kernel. Initially, the e-graph

2

has a single node, representing a single usage of a
128-bit wide ReLU hardware engine. Rewrite 1 en-
codes the knowledge that we can change the size of
ReLU units in hardware by adding a software sched-
ule which loops over the unit. The e-graph is ex-
panded with a new loop-based program; the dotted
line indicates that the new program is equivalent to
the old. Rewrite 2 encodes the knowledge that we
can parallelize a software for loop by instantiating
more hardware. By incorporating a large body of
such rewrites, and running them for a number of it-
erations, the e-graph will expand to include an expo-
nential number of equivalent hardware–software pro-
grams. When running an entire workload through a
series of rewrites, we expect to see a wide range of de-
sign points represented. For example, we should see
designs which instantiate an engine for every kernel
invocation, alongside designs which use complex soft-
ware schedules and very little hardware. From here,
we can attempt to extract promising design candi-
dates; however, the extraction procedure is out of
the scope of this early work.

3 Evaluation Methodology
The goal of this work is to provide a strategy for
enumerating a massive design space. As such, the
evaluation will focus on our ability to generate a di-
verse set of useful designs. A diverse set of designs
should include many design points which diﬀer signif-
icantly from each other. However, this does not help
in design space exploration if the designs themselves
are not worth exploring. Thus, the set of designs
should also include many useful design points; that
is, designs which could turn into eﬃcient hardware.

4 Related Work
There are many recent examples of automated ma-
chine learning hardware generation, such as [3], which
demonstrates compilation from TensorFlow to FP-
GAs. Their solution to the hardware–software split
is to instantiate one engine for each type of kernel
in the workload. While this produces competent de-
signs, the goal of this work is to allow for the easy
enumeration and exploration of more complex (but
potentially more proﬁtable) splits.

References

[1] Jared Roesch, Steven Lyubomirsky, Logan We-
ber, Josh Pollock, Marisa Kirisame, Tianqi Chen,
and Zachary Tatlock. Relay: a new ir for machine
learning frameworks. Proceedings of the 2nd ACM
SIGPLAN International Workshop on Machine
Learning and Programming Languages - MAPL
2018, 2018.

[2] Charles Gregory Nelson. Techniques for program

veriﬁcation.

[3] Stefan Hadjis and Kunle Olukotun. Tensorﬂow
to cloud fpgas: Tradeoﬀs for accelerating deep
neural networks. In 2019 29th International Con-
ference on Field Programmable Logic and Appli-
cations (FPL), pages 360–366. IEEE, 2019.

3

