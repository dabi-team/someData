Born-Again Tree Ensembles

Thibaut Vidal 1 Toni Pacheco 1 Maximilian Schiffer 2

0
2
0
2

g
u
A
7
2

]

G
L
.
s
c
[

3
v
2
3
1
1
1
.
3
0
0
2
:
v
i
X
r
a

Abstract
The use of machine learning algorithms in ﬁnance,
medicine, and criminal justice can deeply impact
human lives. As a consequence, research into in-
terpretable machine learning has rapidly grown
in an attempt to better control and ﬁx possible
sources of mistakes and biases. Tree ensembles
offer a good prediction quality in various domains,
but the concurrent use of multiple trees reduces
the interpretability of the ensemble. Against this
background, we study born-again tree ensembles,
i.e., the process of constructing a single decision
tree of minimum size that reproduces the exact
same behavior as a given tree ensemble in its en-
tire feature space. To ﬁnd such a tree, we develop
a dynamic-programming based algorithm that ex-
ploits sophisticated pruning and bounding rules
to reduce the number of recursive calls. This algo-
rithm generates optimal born-again trees for many
datasets of practical interest, leading to classiﬁers
which are typically simpler and more interpretable
without any other form of compromise.

1. Introduction

Tree ensembles constitute a core technique for prediction
and classiﬁcation tasks. Random forests (Breiman, 2001)
and boosted trees (Friedman, 2001) have been used in vari-
ous application ﬁelds, e.g., in medicine for recurrence risk
prediction and image classiﬁcation, in criminal justice for
custody decisions, or in ﬁnance for credit risk evaluation.
Although tree ensembles offer a high prediction quality,
distorted predictions in high-stakes decisions can be exceed-
ingly harmful. Here, interpretable machine learning models
are essential to understand potential distortions and biases.
Research in this domain has signiﬁcantly increased (Mur-
doch et al., 2019) with numerous works focusing on the
construction of optimal sparse trees (Hu et al., 2019) or on

1Department of Computer Science, Pontiﬁcal Catholic Univer-
sity of Rio de Janeiro (PUC-Rio), Rio de Janeiro, Brazil. 2TUM
School of Management, Technical University of Munich, Munich,
Germany. Correspondence to: Thibaut Vidal <vidalt@inf.puc-
rio.br>.

the interpretability of neural networks (Zhang et al., 2018;
Melis & Jaakkola, 2018).

Currently, there exists a trade-off between the interpretabil-
ity and the performance of tree (ensemble) classiﬁers. Sin-
gle decision trees (e.g., those produced by CART) are well-
known for their interpretability, whereas tree ensembles and
gradient boosting approaches allow for high prediction qual-
ity but are generally more opaque and redundant. Against
this background, we study born-again tree ensembles in a
similar notion as born-again trees (see, Breiman & Shang,
1996), and search for a simpler classiﬁer that faithfully re-
produces the behavior of a tree ensemble.

Formally, let (X, y) = {xi, yi}n
i=1 be a training set in
which each xi ∈ Rp is a p-dimensional numerical feature
vector, and each yi ∈ N is its associated class. Each sample
of this training set has been independently drawn from an
unknown distribution (X , Y). Based on this training set, a
tree ensemble T learns a function FT : X → Y that pre-
dicts yi for each xi drawn from X . With this notation, we
state Problem 1, which is the core of our studies.

Problem 1 (Born-again tree ensemble) Given a tree en-
semble T , we search for a decision tree T of minimal size
that is faithful to T , i.e., such that FT (x) = FT (x) for all
x ∈ Rp.

We note that the condition FT (x) = FT (x) applies to the
entire feature space. Indeed, our goal is to faithfully re-
produce the decision function of the tree ensemble for all
possible inputs in X . In other words, we are looking for
a new representation of the same classiﬁer. Problem 1 de-
pends on the deﬁnition of a size metric. In this study, we
refer to the size of a tree either as its depth (D) or its number
of leaves (L). Additionally, we study a hierarchical objective
(DL) which optimizes depth in priority and then the num-
ber of leaves. For brevity, we detail the methodology for
the depth objective (D) in the main paper. The supplemen-
tary material contains the algorithmic adaptations needed to
cover the other objectives, rigorous proofs for all theorems,
as well as additional illustrations and experimental results.

Theorem 1 states the computational complexity of Prob-
lem 1.

 
 
 
 
 
 
Born-Again Tree Ensembles

Theorem 1 Problem 1 is NP-hard when optimizing depth,
number of leaves, or any hierarchy of these two objectives.

This result uses a direct reduction from 3-SAT. Actually, the
same proof shows that the sole fact of verifying the faithful-
ness of a solution is NP-hard. In this work, we show that
despite this intractability result, Problem 1 can be solved
to proven optimality for various datasets of practical inter-
est, and that the solution of this problem permits signiﬁcant
advances regarding tree ensemble simpliﬁcation, interpreta-
tion, and analysis.

1.1. State of the Art

Our work relates to the ﬁeld of interpretable machine learn-
ing, especially thinning tree ensembles and optimal decision
tree construction. We review these ﬁelds concisely and refer
to Guidotti et al. (2018), Murdoch et al. (2019) and Rudin
(2019) for surveys and discussions on interpretable machine
learning, as well as to Rokach (2016) for an overview on
general work on decision forests.

Thinning tree ensembles has been studied from different
perspectives and divides in two different streams, i) classical
thinning of a tree ensemble by removing some weak learners
from the original ensemble and ii) replacing a tree ensemble
by a simpler classiﬁer, e.g., a single decision tree.

Early works on thinning focused on ﬁnding reduced en-
sembles which yield a prediction quality comparable to the
full ensemble (Margineantu & Dietterich, 1997). Finding
such reduced ensembles has been proven to be NP-hard (Ta-
mon & Xiang, 2000) and in some cases reduced ensembles
may even outperform the full ensemble (Zhou et al., 2002).
While early works proposed a static thinning, dynamic thin-
ning algorithms that store the full ensemble but dynamically
query only a subset of the trees have been investigated by
Hern´andez-Lobato et al. (2009), Park & Furnkranz (2012),
and Mart´ınez-Mu˜noz et al. (2008). For a detailed discus-
sion on this stream of research we refer to Rokach (2016),
who discusses the development of ranking-based methods
(see, e.g., Prodromidis et al., 1999; Caruana et al., 2004;
Banﬁeld et al., 2005; Hu et al., 2007; Partalas et al., 2010;
Rokach, 2009; Zhang & Wang, 2009) and search-based
methods (see, e.g., Prodromidis & Stolfo, 2001; Windeatt
& Ardeshir, 2001; Zhou et al., 2002; Zhou & Tang, 2003;
Rokach et al., 2006; Zhang et al., 2006).

In their seminal work about born-again trees, Breiman &
Shang (1996) were the ﬁrst to introduce a thinning problem
that aimed at replacing a tree ensemble by a newly con-
structed simpler classiﬁer. Here, they used a tree ensemble
to create a data set which is then used to build a born-again
tree with a prediction accuracy close to the accuracy of
the tree ensemble. Ensuing work followed three different
concepts. Meinshausen (2010) introduced the concept of

node harvesting, i.e., reducing the number of decision nodes
to generate an interpretable tree. Recent works along this
line used tree space prototypes to sparsen a tree (Tan et al.,
2016) or rectiﬁed decision trees that use hard and soft labels
(Bai et al., 2019). Friedman & Popescu (2008) followed
a different concept and proposed a linear model to extract
rules from a tree ensemble, which can then be used to re-
built a single tree. Similarly, Sirikulviriya & Sinthupinyo
(2011) focused on deducing rules from a random forest,
while Hara & Hayashi (2016) focused on rule extraction
from tree ensembles via bayesian model selection, and Mol-
las et al. (2019) used a local-based, path-oriented similarity
metric to select rules from a tree ensemble. Recently, some
works focused on directly extracting a single tree from a
tree ensemble based on stabilized but yet heuristic splitting
criteria (Zhou & Hooker, 2016), genetic algorithms (Van-
dewiele et al., 2017), or by actively sampling training points
(Bastani et al., 2017a;b). All of these works focus on the
creation of sparse decision trees that remain interpretable
but can be used to replace a tree ensemble while securing a
similar prediction performance. However, these approaches
do not guarantee faithfulness, such that the new classiﬁer
is not guaranteed to retain the same decision function and
prediction performance.

In the ﬁeld of neural networks, related studies were done
on model compression (Buciluˇa et al., 2006). The proposed
approaches often use knowledge distillation, i.e., using a
high-capacity teacher to train a compact student with similar
knowledge (see, e.g., Hinton et al., 2015). Recent works
focused on creating soft decision trees from a neural net-
work (Frosst & Hinton, 2017), decomposing the gradient in
knowledge distillation (Furlanello et al., 2018), deriving a
class of models for self-explanatory neural networks (Melis
& Jaakkola, 2018), or speciﬁed knowledge representations
in high conv-layers for interpretable convolutional neural
networks (Zhang et al., 2018). Focusing on feed-forward
neural networks, Frankle & Carbin (2018) proposed pruning
techniques that identify subnetworks which perform close to
the original network. Clark et al. (2019) studied born-again
multi task networks for natural language processing, while
Kisamori & Yamazaki (2019) focused on synthesizing an
interpretable simulation model from a neural network. As
neural networks are highly non-linear and even less trans-
parent than tree ensembles, all of these approaches remain
predominantly heuristic and faithfulness is typically not
achievable.

Optimal decision trees. Since the 1990’s, some works fo-
cused on constructing decision trees based on mathematical
programming techniques. Bennett (1992) used linear pro-
gramming to construct trees with linear combination splits
and showed that this technique performs better than conven-
tional univariate split algorithms. Bennett & Blue (1996)

Born-Again Tree Ensembles

focused on building global optimal decision trees to avoid
overﬁtting, while Nijssen & Fromont (2007) presented an
exact algorithm to build a decision tree for speciﬁc depth, ac-
curacy, and leaf requirements. Recently, Bertsimas & Dunn
(2017) presented a mixed integer programming formulation
to construct optimal classiﬁcation trees. On a similar note,
G¨unl¨uk et al. (2018) presented an integer programming ap-
proach for optimal decision trees with categorical data, and
Verwer & Zhang (2019) presented a binary linear program
for optimal decision trees. Hu et al. (2019) presented a
scalable algorithm for optimal sparse binary decision trees.
While all these works show that decision trees are in general
amenable to be built with optimization techniques, none of
these works focused on constructing born-again trees that
match the accuracy of a given tree ensemble.

Summary. Thinning problems have been studied for both
tree ensembles and neural networks in order to derive inter-
pretable classiﬁers that show a similar performance than the
aforementioned algorithms. However, all of these works em-
bed heuristic construction techniques or an approximative
objective, such that the resulting classiﬁers do not guarantee
a behavior and prediction performance equal to the original
tree ensemble or neural network. These approaches appear
to be plausible for born-again neural networks, as neural net-
works have highly non-linear structures that cannot be easily
captured in an optimization approach. In contrast, work in
the ﬁeld of building optimal decision trees showed that the
construction of decision trees is generally amenable for op-
timization based approaches. Nevertheless, these works
focused so far on constructing sparse or optimal trees that
outperform heuristically created trees, such that the question
whether one could construct an optimal decision tree that
serves as a born-again tree ensemble remains open. Answer-
ing this question and discussing some of its implications is
the focus of our study.

1.2. Contributions

With this work, we revive the concept of born-again tree
ensembles and aim to construct a single —minimum-size—
tree that faithfully reproduces the decision function of the
original tree ensemble. More speciﬁcally, our contribution is
fourfold. First, we formally deﬁne the problem of construct-
ing optimal born-again tree ensembles and prove that this
problem is NP-hard. Second, we highlight several properties
of this problem and of the resulting born-again tree. These
ﬁndings allow us to develop a dynamic-programing based
algorithm that solves this problem efﬁciently and constructs
an optimal born-again tree out of a tree ensemble. Third, we
discuss speciﬁc pruning strategies for the born-again tree
that allow to reduce redundancies that cannot be identiﬁed
in the original tree ensemble. Fourth, besides providing
theoretical guarantees, we present numerical studies which

allow to analyze the characteristics of the born-again trees
in terms of interpretability and accuracy. Further, these stud-
ies show that our algorithm is amenable to a wide range of
real-world data sets.

We believe that our results and the developed algorithms
open a new perspective in the ﬁeld of interpretable machine
learning. With this approach, one can construct simple
classiﬁers that bear all characteristics of a tree ensemble.
Besides interpretability gains, this approach casts a new light
on tree ensembles and highlights new structural properties.

2. Fundamentals

In this section, we introduce some fundamental deﬁnitions.
Afterwards, we discuss a worst-case bound on the depth of
an optimal born-again tree.

Tree ensemble. We deﬁne a tree ensemble T as a set of
trees t ∈ T with weights wt. For any sample x, the tree
ensemble returns the majority vote of its trees: FT (x) =
WEIGHTED-MAJORITY{(Ft(x), wt)}t∈T (ties are broken
in favor of the smaller index).

Cells. Let Hj be the set of all split levels (i.e., hyper-
planes) extracted from the trees for each feature j. We
can partition the feature space Rp into cells SELEM =
{1, . . . , |H1| + 1} × · · · × {1, . . . , |Hp| + 1} such that each
cell z = (z1, . . . , zp) ∈ SELEM represents the box contained
between the (zj − 1)th and zth
j hyperplanes for each feature
j ∈ {1, . . . , p}. Cells such that zj = 1 (or zj = |Hj| + 1)
extend from −∞ (or to ∞, respectively) along dimension j.
We note that the decision function of the tree ensemble
FT (z) is constant in the interior of each cell z, allowing us
to exclusively use the hyperplanes of {Hj}d
j=1 to construct
an optimal born-again tree.

Regions. We deﬁne a region of the feature space as a
pair (zL, zR) ∈ S 2
ELEM such that zL ≤ zR. Region (zL, zR)
encloses all cells z such that zL ≤ z ≤ zR. Let SREGIONS
be the set of all regions. An optimal born-again tree T
for a region (zL, zR) is a tree of minimal size such that
FT (x) = FT (x) within this region.

Figure 1. Example of a cell, region and splitting hyperplane

 Hyperplane levels  for feature 1 CELL REGION Hyperplane levels for  feature 2 ● ● ● ● ● ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ● ● ● ● ● ● ● ● ●  POSSIBLE  SPLITTING HYPERPLANE zL zR Born-Again Tree Ensembles

Figure 1 depicts a cell and a region on a two-dimensional
feature space. We also provide a more extensive example of
the born-again tree generation process in the supplementary
material. The number of cells and regions increases rapidly
with the number of hyperplanes and features, formally:

|SELEM| =

p
(cid:89)

(|Hj| + 1)

j=1

|SREGION| =

p
(cid:89)

j=1

(|Hj| + 1)(|Hj| + 2)
2

.

(1)

(2)

Moreover, Theorem 2 gives initial bounds on the size of the
born-again decision tree.

Theorem 2 The depth of an optimal born-again tree T
satisﬁes Φ(T ) ≤ (cid:80)
t∈T Φ(t), where Φ(t) represents the
depth of a tree t. This bound is tight.

This bound corresponds to a worst case behavior which is
usually attained only on purposely-designed pathological
cases. As highlighted in our computational experiments,
the average tree depth remains generally lower than this
analytical worst case. Beyond interpretability beneﬁts, the
tree depth represents the number of sequential operations
(hyperplane comparisons) needed to determine the class of a
given sample during the test stage. Therefore, an optimized
born-again tree is not only more interpretable, but it also
requires less test effort, with useful applications for clas-
siﬁcation in embarked systems, typically occurring within
limited time and processing budgets.

3. Methodology

In this section, we introduce a dynamic programming (DP)
algorithm which optimally solves Problem 1 for many data
sets of practical interest. Let Φ(zL, zR) be the depth of an
optimal born-again decision tree for a region (zL, zR) ∈
SREGION. We can then limit the search to optimal born-again
trees whose left and right sub-trees represent optimal born-
again trees for the respective sub-regions. Hence, we can
recursively decompose a larger problem into subproblems
using

Φ(zL, zR) =



0 if ID(zL, zR)
(cid:40)

min
1≤j≤p



min
z L
j ≤l<z R
j

(cid:110)

1 + max{Φ(zL, zR

jl), Φ(zL

jl, zR)}

(3)

(cid:41)

(cid:111)

,

in which ID(zL, zR) takes value TRUE if and only if all cells
z such that zL ≤ z ≤ zR admit the same weighted majority
class. In this equation, zR
j ) represents
the “top right” corner of the left region obtained in the

jl = zR + ej(l − z R

j ) is the “bottom
subdivision, and zL
jl = zL + ej(l + 1 − z L
left” corner of the right region obtained in the subdivision.

While Equation (3) bears the main rationale of our algo-
rithm, it suffers in its basic state from two main weaknesses
that prevent its translation into an efﬁcient algorithm: ﬁrstly,
each veriﬁcation of the ﬁrst condition (i.e., the base case) re-
quires evaluating whether ID(zL, zR) is true and possibly re-
quires the evaluation of the majority class on an exponential
number of cells if done brute force. Secondly, the recursive
call considers all possible hyperplanes within the region to
ﬁnd the minimum over j ∈ {1, . . . , p} and z L
j ≤ l < z R
j .
In the following, we propose strategies to mitigate both
drawbacks.

To avoid the evaluation of ID(zL, zR) by inspection, we
integrate this evaluation within the recursion to proﬁt from
the memory structures of the DP algorithm. With these
changes, the recursion becomes:

Φ(zL, zR) =
(cid:40)

min
j

min
j ≤l<z R
z L
j

(cid:8)1jl(zL, zR) + max{Φ(zL, zR

jl), Φ(zL

(4)

(cid:41)
jl, zR)}(cid:9)

where 1jl(zL, zR) =




0

if Φ(zL, zR
jl) = Φ(zL
and FT (zL) = FT (zR);

jl, zR) = 0



1 otherwise.

To limit the number of recursive calls, we can ﬁlter out for
each dimension j any hyperplane l ∈ {1, . . . , |Hj|} such
that FT (z) = FT (z + ej) for all z such that zj = l, and
exploit two additional properties of the problem.

Theorem 3 Let (zL, zR) and (¯zL, ¯zR) be two regions such
that zL ≤ ¯zL ≤ ¯zR ≤ zR, then Φ(¯zL, ¯zR) ≤ Φ(zL, zR).

Theorem 3 follows from the fact that any feasible tree satis-
fying FT (x) = FT (x) on a region (zL, zR) also satisﬁes this
condition for any subregion (¯zL, ¯zR). Therefore, Φ(¯zL, ¯zR)
constitutes a lower bound of Φ(zL, zR). Combining this
bound with Equation (4), we get

max{Φ(zL, zR

jl), Φ(zL

jl, zR)}

≤ Φ(zL, zR)
≤ 1jl(zL, zR) + max{Φ(zL, zR

jl), Φ(zL

jl, zR)}

for each j ∈ {1, . . . , p} and z L

j ≤ l < z R
j .

This result will be fundamental to use bounding techniques
and therefore save numerous recursions during the DP algo-
rithm. With Theorem 4, we can further reduce the number
of candidates in each recursion.

Theorem 4 Let j ∈ {1, . . . , p} and l ∈ {z L

j , . . . , z R

j − 1}.

Born-Again Tree Ensembles

• If Φ(zL, zR

jl) ≥ Φ(zL

jl, zR) then ∀l(cid:48) > l

1jl(zL, zR) + max{Φ(zL, zR
≤ 1jl(cid:48)(zL, zR) + max{Φ(zL, zR

jl), Φ(zL
jl(cid:48)), Φ(zL

jl, zR)}
jl(cid:48), zR)}

• If Φ(zL, zR

jl) ≤ Φ(zL

jl, zR) then ∀l(cid:48) < l

1jl(zL, zR) + max{Φ(zL, zR
≤ 1jl(cid:48)(zL, zR) + max{Φ(zL, zR

jl), Φ(zL
jl(cid:48)), Φ(zL

jl, zR)}
jl(cid:48), zR)}.

jl) ≥ Φ(zL
jl) ≤ Φ(zL

Based on Theorem 4, we can discard all hyperplane levels
l(cid:48) > l in Equation (4) if Φ(zL, zR
jl, zR). The
same argument holds when Φ(zL, zR
jl, zR) with
l(cid:48) < l. We note that the two cases of Theorem 4 are not
mutually exclusive. No other recursive call is needed for
the considered feature when an equality occurs. Otherwise,
at least one case holds, allowing us to search the range
j − 1} in Equation (4) by binary search with
j , . . . , z R
l ∈ {z L
j − z L
only O(log(z R

j )) subproblem calls.

General algorithm structure. The DP algorithm pre-
sented in Algorithm 1 capitalizes upon all the aforemen-
It is initially launched on the region
tioned properties.
representing the complete feature space, by calling BORN-
AGAIN(zL, zR) with zL = (1, . . . , 1)(cid:124) and zR = (|H1| +
1, . . . , |Hp| + 1)(cid:124).

Firstly, the algorithm checks whether it attained a base case
in which the region (zL, zR) is restricted to a single cell
(Line 1). If this is the case, it returns an optimal depth
of zero corresponding to a single leaf, otherwise it tests
whether the result of the current subproblem deﬁned by
region (zL, zR) is not yet in the DP memory (Line 2). If this
is the case, it directly returns the known result.

Past these conditions, the algorithm starts enumerating pos-
sible splits and opening recursions to ﬁnd the minimum of
Equation (4). By Theorem 4 and the related discussions,
it can use a binary search for each feature to save many
possible evaluations (Lines 9 and 10). By Theorem 3, the
exploitation of lower and upper bounds on the optimal solu-
tion value (Lines 7, 9, 20, and 21) allows to stop the iterative
search whenever no improving solution can exist. Finally,
the special case of Lines 13 and 14 covers the case in which
jl, zR) = 0 and FT (zL) = FT (zR), corre-
Φ(zL, zR
sponding to a homogeneous region in which all cells have
the same majority class. As usual in DP approaches, our
algorithm memorizes the solutions of sub-problems and
reuses them in future calls (Lines 15, 17, and 26).

jl) = Φ(zL

We observe that this algorithm maintains the optimal solu-
tion of each subproblem in memory, but not the solution
itself in order to reduce memory consumption. Retrieving
the solution after completing the DP can be done with a sim-
ple inspection of the ﬁnal states and solutions, as detailed in
the supplementary material.

The maximum number of possible regions is |SREGION| =
(cid:81)
(|Hj |+1)(|Hj |+2)
(Equation 2) and each call to BORN-
j
2
AGAIN takes up to O((cid:80)
j log |Hj|) elementary operations
due to Theorem 4, leading to a worst-case complexity of
O(|SREGION| (cid:80)
j log |Hj|) time for the overall recursive al-
gorithm. Such an exponential complexity is expectable for
an NP-hard problem. Still, as observed in our experiments,
the number of regions explored with the bounding strategies
is much smaller in practice than the theoretical worst case.

Algorithm 1 BORN-AGAIN(zL, zR)

l ← (cid:98)(LOW + UP)/2(cid:99)
j ))
Φ1 ← BORN-AGAIN(zL, zR + ej(l − z R
Φ2 ← BORN-AGAIN(zL + ej(l + 1 − z L
j ), zR)
if (Φ1 = 0) and (Φ2 = 0) then
if f (zL, T ) = f (zR, T ) then

MEMORIZE((zL, zR), 0) and return 0

(LOW, UP) ← (z L
j )
j , z R
while LOW < UP and LB < UB do

1: if (zL = zR) return 0
2: if (zL, zR) exists in memory then
return MEMORY(zL, zR)
3:
4: end if
5: UB ← ∞
6: LB ← 0
7: for j = 1 to p and LB < UB do
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25: end for
26: MEMORIZE((zL, zR), UB) and return UB

end if
UB ← min{UB, 1 + max{Φ1, Φ2}}
LB ← max{LB, max{Φ1, Φ2}}
if (Φ1 ≥ Φ2) then UP ← l
if (Φ1 ≤ Φ2) then LOW ← l + 1

end while

end if

else

MEMORIZE((zL, zR), 1) and return 1

4. Computational Experiments

The goal of our computational experiments is fourfold:

1. Evaluating the computational performance of the pro-
posed DP algorithm as a function of the data set charac-
teristics, e.g., the size metric in use, the number of trees
in the original ensemble, and the number of samples
and features in the datasets.

2. Studying the structure and complexity of the born-

again trees for different size metrics.

3. Measuring the impact of a simple pruning strategy

applied on the resulting born-again trees.

4. Proposing and evaluating a fast heuristic algorithm to

ﬁnd faithful born-again trees.

Born-Again Tree Ensembles

The DP algorithm was implemented in C++ and compiled
with GCC 9.2.0 using ﬂag -O3, whereas the original ran-
dom forests were generated in Python (using scikit-learn
v0.22.1). All our experiments were run on a single thread
of an Intel(R) Xeon(R) CPU E5-2620v4 2.10GHz, with
128GB of available RAM, running CentOS v7.7. In the
remainder of this section, we discuss the preparation of
the data and then describe each experiment. Detailed com-
putational results, data, and source codes are available in
the supplementary material and at the following address:
https://github.com/vidalt/BA-Trees.

4.1. Data Preparation

We focus on a set of six datasets from the UCI machine
learning repository [UCI] and from previous work by Smith
et al. (1988) [SmithEtAl] and Hu et al. (2019) [HuEtAl]
for which using random forests (with ten trees) showed
a signiﬁcant improvement upon stand-alone CART. The
characteristics of these datasets are summarized in Table 1:
number of samples n, number of features p, number of
classes K and class distribution CD. To obtain discrete
numerical features, we used one-hot encoding on categorical
data and binned continuous features into ten ordinal scales.
Then, we generated training and test samples for all data
sets using a ten-fold cross validation. Finally, for each fold
and each dataset, we generated a random forest composed
of ten trees with a maximum depth of three (i.e., eight leaves
at most), considering p/2 random candidate features at each
split. This random forest constitutes the input to our DP
algorithm.

Data set

Table 1. Characteristics of the data sets
CD

p K

n

BC: Breast-Cancer
CP: COMPAS
FI: FICO
HT: HTRU2
PD: Pima-Diabetes
SE: Seeds

683
6907
10459
17898
768
210

9
12
17
8
8
7

2
2
2
2
2
3

65-35
54-46
52-48
91-9
65-35
33-33-33

Src.

UCI
HuEtAl
HuEtAl
UCI
SmithEtAl
UCI

4.2. Computational Effort

In a ﬁrst analysis, we evaluate the computational time of
Algorithm 1 for different data sets and size metrics. Figure 2
reports the results of this experiment as a box-whisker plot,
in which each box corresponds to ten runs (one for each
fold) and the whiskers extend to 1.5 times the interquartile
range. Any sample beyond this limit is reported as outlier
and noted with a “◦”. D denotes a depth-minimization objec-
tive, whereas L refers to the minimization of the number of
leaves, and DL refers to the hierarchical objective which pri-
oritizes the smallest depth, and then the smallest number of
leaves. As can be seen, constructing a born-again tree with

Figure 2. Computational times to construct a born-again tree from
a random forest with 10 trees and depth 3, for each objective
(D/L/DL) and data set

objective D yields signiﬁcantly lower computational times
compared to using objectives L and DL. Indeed, the binary
search technique resulting from Theorem 4 only applies to
objective D, leading to a reduced number of recursive calls
in this case compared to the other algorithms.

In our second analysis, we focus on the FICO case
and randomly extract subsets of samples and features to
produce smaller data sets. We then measure the com-
putational effort of Algorithm 1 for metric D (depth
optimization) as a function of the number of features
(p ∈ {2, 3, 5, 7, 10, 12, 15, 17}),
the number of samples
(n ∈ {250, 500, 750, 1000, 2500, 5000, 7500, 10459}), and
the number of trees in the original random forest (T ∈
{3, 5, 7, 10, 12, 15, 17, 20}). Figure 3 reports the results of
this experiment. Each boxplot corresponds to ten runs, one
for each fold.

We observe that the computational time of the DP algorithm
is strongly driven by the number of features, with an expo-
nential growth relative to this parameter. This result is in
line with the complexity analysis of Section 3. The number
of trees inﬂuences the computational time signiﬁcantly less.
Surprisingly, the computational effort of the algorithm actu-
ally decreases with the number of samples. This is due to
the fact that with more sample information, the decisions
of the individual trees of the random forest are less varied,
leading to fewer distinct hyperplanes and therefore to fewer
possible states in the DP.

4.3. Complexity of the Born-Again Trees

We now analyze the depth and number of leaves of the born-
again trees for different objective functions and datasets in
Table 2.

As can be seen, the different objectives can signiﬁcantly

●●●●●●●●●●●●●BCCPFIHTPDSE0.010.11101001000DLDL(cid:38)(cid:51)(cid:56)(cid:3)(cid:55)(cid:76)(cid:80)(cid:72)(cid:3)(cid:11)(cid:86)(cid:12)(cid:50)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:76)(cid:89)(cid:72)(cid:29)Born-Again Tree Ensembles

Figure 3. Growth of the computational time (in milliseconds) of Algorithm 1 as a function of the number of samples, features and trees. In
each experiment, the parameters which are not under scrutiny are ﬁxed to their baseline values of n = 2.5 × 103, p = 10 and T = 10.

Table 2. Depth and number of leaves of the born-again trees

D

L

DL

Data set Depth # Leaves Depth # Leaves Depth # Leaves

BC
CP
FI
HT
PD
SE

Avg.

12.5
8.9
8.6
6.0
9.6
10.2

9.3

2279.4
119.9
71.3
20.2
460.1
450.9

567.0

18.0
8.9
8.6
6.3
15.0
13.8

11.8

890.1
37.1
39.2
11.9
169.7
214.6

227.1

12.5
8.9
8.6
6.0
9.6
10.2

9.3

1042.3
37.1
39.2
12.0
206.7
261.0

266.4

inﬂuence the outcome of the algorithm. For several data sets,
the optimal depth of the born-again tree is reached with any
objective, as an indirect consequence of the minimization of
the number of leaves. In other cases, however, prioritizing
the minimization of the number of leaves may generate 50%
deeper trees for some data sets (e.g., PD). The hierarchical
objective DL succeeds in combining the beneﬁts of both
objectives. It generates a tree with minimum depth and with
a number of leaves which is usually close to the optimal one
from objective L.

4.4. Post-Pruned Born-Again Trees

Per deﬁnition, the born-again tree reproduces the same ex-
act behavior as the majority class of the original ensemble
classiﬁer on all regions of the feature space X . Yet, some
regions of X may not contain any training sample, either
due to data scarcity or simply due to incompatible feature
values (e.g., “sex = MALE” and “pregnancy = TRUE”). These
regions may also have non-homogeneous majority classes
from the tree ensemble viewpoint due to the combinations of
decisions from multiple trees. The born-again tree, however,
is agnostic to this situation and imitates the original classiﬁ-

cation within all the regions, leading to some splits which
are mere artifacts of the ensemble’s behavior but never used
for classiﬁcation.

To circumvent this issue, we suggest to apply a simple post-
pruning step to eliminate inexpressive tree sub-regions. We
therefore verify, from bottom to top, whether both sides of
each split contain at least one training sample. Any split
which does not fulﬁll this condition is pruned and replaced
by the child node of the branch that contains samples. The
complete generation process, from the original random for-
est to the pruned born-again tree is illustrated in Figure 4. In
this simple example, it is noteworthy that the born-again tree
uses an optimal split at the root node which is different from
all root splits in the ensemble. We also clearly observe the
role of the post-pruning step, which contributes to eliminate
a signiﬁcant part of the tree.

To observe the impact of the post-pruning on a larger range
of datasets, Table 3 reports the total number of leaves of the
random forests, as well as the average depth and number of
leaves of the born-again trees before and after post-pruning.
As previously, the results are averaged over the ten folds.
As can be seen, post-pruning signiﬁcantly reduces the size
of the born-again trees, leading to a ﬁnal number of leaves
which is, on average, smaller than the total number of leaves
in the original tree ensemble. This indicates a signiﬁcant
gain of simplicity and interpretability.

However, post-pruning could cause a difference of behavior
between the original tree ensemble classiﬁer and the ﬁnal
pruned born-again tree. To evaluate whether this ﬁltering
had any signiﬁcant impact on the classiﬁcation performance
of the born-again tree, we ﬁnally compare the out-of-sample
accuracy (Acc.) and F1 score of the three classiﬁers in
Table 4.

●●●●●0.250.50.7512.557.510.5051015(cid:55)(cid:11)(cid:80)(cid:86)(cid:12)(cid:49)(cid:88)(cid:80)(cid:69)(cid:72)(cid:85)(cid:3)(cid:82)(cid:73)(cid:3)(cid:54)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)(cid:3)(cid:81)(cid:3)(cid:11)(cid:91)(cid:20)(cid:19)(cid:19)(cid:19)(cid:12)●235710121517050100150200250300(cid:49)(cid:88)(cid:80)(cid:69)(cid:72)(cid:85)(cid:3)(cid:82)(cid:73)(cid:3)(cid:41)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:86)(cid:3)(cid:83)(cid:55)(cid:11)(cid:80)(cid:86)(cid:12)●●●●●●3571012151720024681012(cid:55)(cid:11)(cid:80)(cid:86)(cid:12)(cid:49)(cid:88)(cid:80)(cid:69)(cid:72)(cid:85)(cid:3)(cid:82)(cid:73)(cid:3)(cid:55)(cid:85)(cid:72)(cid:72)(cid:86)(cid:3)(cid:55)Born-Again Tree Ensembles

Figure 4. Example of a post-pruned born-again tree on the Seeds data set

Table 3. Comparison of depth and number of leaves

Data set

Random Forest
# Leaves

Born-Again

BA + Pruning

Depth # Leaves Depth # Leaves

BC
CP
FI
HT
PD
SE

Avg.

61.1
46.7
47.3
42.6
53.7
55.7

51.2

12.5 2279.4
119.9
8.9
71.3
8.6
20.2
6.0
460.1
9.6
450.9
10.2

9.3

567.0

9.1
7.0
6.5
5.1
9.4
7.5

7.4

35.9
31.2
15.8
13.2
79.0
21.5

32.8

Table 4. Accuracy and F1 score comparison

Random Forest

Data set Acc.

F1

Born-Again
F1

Acc.

BA + Pruning
Acc.

F1

BC
CP
FI
HT
PD
SE

0.953
0.660
0.697
0.977
0.746
0.790

0.949
0.650
0.690
0.909
0.692
0.479

0.953
0.660
0.697
0.977
0.746
0.790

0.949
0.650
0.690
0.909
0.692
0.479

0.946
0.660
0.697
0.977
0.750
0.790

0.941
0.650
0.690
0.909
0.700
0.481

Avg.

0.804

0.728

0.804

0.728

0.803

0.729

First of all, the results of Table 4 conﬁrm the faithfulness of
our algorithm, as they verify that the prediction quality of
the random forests and the born-again tree ensembles are
identical. This was expected per deﬁnition of Problem 1.
Furthermore, only marginal differences were observed be-
tween the out-of-sample performance of the born-again tree
with pruning and the other classiﬁers. For the considered
datasets, pruning contributed to eliminate inexpressive re-
gions of the tree without much impact on classiﬁcation
performance.

4.5. Heuristic Born-Again Trees

As Problem 1 is NP-hard, the computational time of our
algorithm will eventually increase exponentially with the
number of features (see Figure 3). This is due to the increas-
ing number of recursions, and to the challenge of testing
homogeneity for regions without exploring all cells. Indeed,
even proving that a given region is homogeneous (i.e., that it
contains cells of the same class) remains NP-hard, although
it is solvable in practice using integer programming tech-
niques. Accordingly, we take a ﬁrst step towards scalable
heuristic algorithms in the following. We therefore explain
how our born-again tree construction algorithm can be mod-
iﬁed to preserve the faithfulness guarantee while achieving
only a heuristic solution in terms of size.

We made the following adaptations to Algorithm 1 to de-
rive its heuristic counterpart. For each considered region

k-groove ≤ 5.75comp ≤ 0.86True1False20k-length ≤ 5.5202asym ≤ 4.97k-groove ≤ 5.75comp ≤ 0.86Truek-width ≤ 3.48False20area ≤ 13.37001asym ≤ 2.251k-groove ≤ 5.75area ≤ 12.62True False asym ≤ 2.2502k-groove ≤ 4.8701k-width ≤ 3.4801k-length ≤ 6.061Initial Tree Ensemble:k-groove ≤ 5.75comp ≤ 0.86Truek-length ≤ 6.06False20area ≤ 13.3702k-groove ≤ 4.870area ≤ 12.62k-length ≤ 5.52002k-groove ≤ 4.87asym ≤ 4.970area ≤ 12.6201asym ≤ 2.251k-width ≤ 3.481Born-again tree:  POST-PRUNINGk-groove ≤ 5.75comp ≤ 0.86Truek-length ≤ 6.06False20k-length ≤ 5.5202asym ≤ 4.970area ≤ 12.6201asym ≤ 2.251k-width ≤ 3.481Born-Again Tree Ensembles

(zL, zR), we proceed as follows.

1. Instead of evaluating all possible splits and opening
recursions, we randomly select Nc = 1000 cells in the
region and pick the splitting hyperplane that maximizes
the information gain.

2. If all these cells belong to the same class, we rely
on an integer programming solver to prove whether
the region is homogeneous or not.
If the region is
homogeneous, we deﬁne a leaf. Otherwise, we have
detected a violating cell, and continue splitting until all
regions are homogeneous to guarantee faithfulness.

With these adaptations, the heuristic algorithm ﬁnds born-
again trees that are guaranteed to be faithful but not neces-
sarily minimal in size. Table 5 compares the computational
time of the optimal born-again tree algorithm using objec-
tive D “TD(s)”, objective L “TL(s)” with that of the heuristic
algorithm “TH(s)”. It also reports the percentage gap of
the heuristic tree depth “GapD(%)” and number of leaves
“GapL(%)” relative to the optimal solution values of each
objective.

Table 5. Computational time and optimality gap of the heuristic
born-again tree algorithm

TL(s)

TH(s)

GapD(%)

GapL(%)

Data set

BC
CP
FI
HT
PD
SE

TD(s)

39.09
0.01
0.05
0.01
0.91
0.37

1381.45
0.10
0.23
0.01
17.95
5.96

Avg.

6.74

234.28

1.14
0.04
0.03
0.01
0.19
0.24

0.28

44.80
0.00
0.00
8.33
44.79
37.25

22.53

48.37
4.85
1.79
10.92
25.63
29.03

20.10

As visible in these experiments, the CPU time of the heuris-
tic algorithm is signiﬁcantly smaller than that of the optimal
method, at the expense of an increase in tree depth and
number of leaves, by 22.53% and 20.10% on average, re-
spectively. To test the limits of this heuristic approach, we
also veriﬁed that it could run in a reasonable amount of time
(faster than a minute) on larger datasets such as Ionosphere,
Spambase, and Miniboone (the latter with over 130,000
samples and 50 features).

5. Conclusions

In this paper, we introduced an efﬁcient algorithm to trans-
form a random forest into a single, smallest possible, deci-
sion tree. Our algorithm is optimal, and provably returns a
single tree with the same decision function as the original
tree ensemble. In brief, we obtain a different representation
of the same classiﬁer, which helps us to analyze random
forests from a different angle. Interestingly, when investigat-
ing the structure of the results, we observed that born-again

decision trees contain many inexpressive regions designed
to faithfully reproduce the behavior of the original ensemble,
but which do not contribute to effectively classify samples.
It remains an interesting research question to properly un-
derstand the purpose of these regions and their contribution
to the generalization capabilities of random forests. In a
ﬁrst simple experiment, we attempted to apply post-pruning
on the resulting tree. Based on our experiments on six struc-
turally different datasets, we observed that this pruning does
not diminish the quality of the predictions but signiﬁcantly
simpliﬁes the born-again trees. Overall, the ﬁnal pruned
trees represent simple, interpretable, and high-performance
classiﬁers, which can be useful for a variety of application
areas.

As a perspective for future work, we recommend to progress
further on solution techniques for the born-again tree en-
sembles problem, proposing new optimal algorithms to ef-
fectively handle larger datasets as well as fast and accu-
rate heuristics. Heuristic upper bounds can also be jointly
exploited with mathematical programming techniques to
eliminate candidate hyperplanes and recursions. Another
interesting research line concerns the combination of the
dynamic programming algorithm for the construction of
the born-again tree with active pruning during construction,
leading to a different deﬁnition of the recursion and to differ-
ent base-case evaluations. Finally, we recommend to pursue
the investigation of the structural properties of tree ensem-
bles in light of this new born-again tree representation.

Acknowledgements

The authors gratefully thank the editors and referees for
their insightful recommendations, as well as Simone Bar-
bosa, Quentin Cappart, and Artur Pessoa for rich scientiﬁc
discussions. This research has been partially supported by
CAPES, CNPq [grant number 308528/2018-2] and FAPERJ
[grant number E-26/202.790/2019] in Brazil.

References

Bai, J., Li, Y., Li, J., Jiang, Y., and Xia, S. Rectiﬁed de-
cision trees: Towards interpretability, compression and
empirical soundness. arXiv preprint arXiv:1903.05965,
2019.

Banﬁeld, R. E., Hall, L. O., Bowyer, K. W., and Kegelmeyer,
W. P. Ensemble diversity measures and their application
to thinning. Information Fusion, 6(1):49–62, 2005.

Bastani, O., Kim, C., and Bastani, H.

Interpretability
via model extraction. arXiv preprint arXiv:1706.09773,
2017a.

Bastani, O., Kim, C., and Bastani, H.

Interpreting

Born-Again Tree Ensembles

blackbox models via model extraction. arXiv preprint
arXiv:1705.08504, 2017b.

Bennett, K. Decision tree construction via linear program-
ming. In Proceedings of the 4th Midwest Artiﬁcial Intelli-
gence and Cognitive Science Society Conference, Utica,
Illinois, 1992.

Bennett, K. and Blue, J. Optimal decision trees. Technical

report, Rensselaer Polytechnique Institute, 1996.

Bertsimas, D. and Dunn, J. Optimal classiﬁcation trees.

Machine Learning, 106(7):1039–1082, 2017.

Breiman, L. Random forests. Machine Learning, 45(1):

5–32, 2001.

Breiman, L. and Shang, N. Born again trees. Technical

report, University of California Berkeley, 1996.

Buciluˇa, C., Caruana, R., and Niculescu-Mizil, A. Model
compression. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, 2006.

Caruana, R., Niculescu-Mizil, A., Crew, G., and Ksikes,
A. Ensemble selection from libraries of models.
In
Proceedings of the twenty-ﬁrst International Conference
on Machine Learning, pp. 18, 2004.

Clark, K., Luong, M.-T., Khandelwal, U., Manning, C. D.,
and Le, Q. V. Bam! born-again multi-task networks
arXiv preprint
for natural language understanding.
arXiv:1907.04829, 2019.

Frankle, J. and Carbin, M. The lottery ticket hypothesis:
Finding sparse, trainable neural networks. arXiv preprint
arXiv:1803.03635, 2018.

Friedman, J. Greedy function approximation: A gradient
boosting machine. Annals of Statistics, 29(5):1189–1232,
2001.

Friedman, J. H. and Popescu, B. E. Predictive learning via
rule ensembles. The Annals of Applied Statistics, 2(3):
916–954, 2008.

Frosst, N. and Hinton, G. Distilling a neural network into
a soft decision tree. arXiv preprint arXiv:1711.09784,
2017.

Furlanello, T., Lipton, Z. C., Tschannen, M., Itti, L., and
Anandkumar, A. Born again neural networks. arXiv
preprint arXiv:1805.04770, 2018.

G¨unl¨uk, O., Kalagnanam, J., Menickelly, M., and Schein-
berg, K. Optimal decision trees for categorical data via
integer programming. arXiv preprint arXiv:1612.03225,
2018.

Hara, S. and Hayashi, K. Making tree ensembles inter-
pretable: A bayesian model selection approach. arXiv
preprint arXiv:1606.09066, 2016.

Hern´andez-Lobato, D., Martinez-Muoz, G., and Su´arez,
A. Statistical instance-based pruning in ensembles of
independent classiﬁers. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 31(2):364–369, 2009.

Hinton, G., Vinyals, O., and Dean, J.
the knowledge in a neural network.
arXiv:1503.02531, 2015.

Distilling
arXiv preprint

Hu, Q., Yu, D., Xie, Z., and Li, X. Eros: Ensemble rough
subspaces. Pattern recognition, 40(12):3728–3739, 2007.

Hu, X., Rudin, C., and Seltzer, M. Optimal sparse decision
In Advances in Neural Information Processing

trees.
Systems, 2019.

Kisamori, K. and Yamazaki, K. Model bridging: To inter-
pretable simulation model from neural network. arXiv
preprint arXiv:1906.09391, 2019.

Margineantu, D. and Dietterich, T. Pruning adaptive boost-
ing. In Proceedings of the Fourteenth International Con-
ference Machine Learning, 1997.

Mart´ınez-Mu˜noz, G., Hern´andez-Lobato, D., and Su´arez,
A. An analysis of ensemble pruning techniques based
on ordered aggregation. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 31(2):245–259, 2008.

Meinshausen, N. Node harvest. The Annals of Applied

Statistics, pp. 2049–2072, 2010.

Melis, D. A. and Jaakkola, T. Towards robust interpretability
In Advances in

with self-explaining neural networks.
Neural Information Processing Systems, 2018.

Mollas, I., Tsoumakas, G., and Bassiliades, N. Lionforests:
Local interpretation of random forests through path selec-
tion. arXiv preprint arXiv:1911.08780, 2019.

Murdoch, W., Singh, C., Kumbier, K., Abassi-Asl, R.,
Interpretable machine learning: deﬁ-
arXiv preprint

and Yu, B.
nitions, methods, and applications.
arXiv:1901.04592v1, 2019.

Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Gian-
notti, F., and Pedreschi, D. A survey of methods for
explaining black box models. ACM Computing Surveys
(CSUR), 51(5):1–42, 2018.

Nijssen, S. and Fromont, E. Mining optimal decision trees
from itemset lattices. In Proceedings of the 13th ACM
SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, 2007.

Born-Again Tree Ensembles

Verwer, S. and Zhang, Y. Learning optimal classiﬁcation
trees using a binary linear program formulation. In Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelli-
gence, 2019.

Windeatt, T. and Ardeshir, G. An empirical comparison
of pruning methods for ensemble classiﬁers. In Interna-
tional Symposium on Intelligent Data Analysis, 2001.

Zhang, H. and Wang, M. Search for the smallest random

forest. Statistics and its Interface, 2(3):381, 2009.

Zhang, Q., Nian Wu, Y., and Zhu, S.-C. Interpretable con-
volutional neural networks. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
2018.

Zhang, Y., Burer, S., and Street, W. N. Ensemble pruning via
semi-deﬁnite programming. Journal of Machine Learning
Research, 7(Jul):1315–1338, 2006.

Zhou, Y. and Hooker, G. Interpreting models via single tree
approximation. arXiv preprint arXiv:1610.09036, 2016.

Zhou, Z., Wu, J., and Tang, W. Ensembling neural networks:
many could be better than all. Artiﬁcial Intelligence, 137:
239–263, 2002.

Zhou, Z.-H. and Tang, W. Selective ensemble of decision
trees. In International Workshop on Rough Sets, Fuzzy
Sets, Data Mining, and Granular-Soft Computing, 2003.

Park, S. and Furnkranz, J. Efﬁcient prediction algorithms
for binary decomposition techniques. Data Mining and
Knowledge Discovery, 24(1):40–77, 2012.

Partalas, I., Tsoumakas, G., and Vlahavas, I. An ensem-
ble uncertainty aware measure for directed hill climbing
ensemble pruning. Machine Learning, 81(3):257–282,
2010.

Prodromidis, A. L. and Stolfo, S. J. Cost complexity-based
pruning of ensemble classiﬁers. Knowledge and Informa-
tion Systems, 3(4):449–469, 2001.

Prodromidis, A. L., Stolfo, S. J., and Chan, P. K. Effective
and efﬁcient pruning of metaclassiﬁers in a distributed
data mining system. Knowledge Discovery and Data
Mining Journal, 32, 1999.

Rokach, L. Collective-agreement-based pruning of ensem-
bles. Computational Statistics & Data Analysis, 53(4):
1015–1026, 2009.

Rokach, L. Decision forest: Twenty years of research.

Information Fusion, 27:111–125, 2016.

Rokach, L., Maimon, O., and Arbel, R. Selective votingget-
ting more for less in sensor fusion. International Journal
of Pattern Recognition and Artiﬁcial Intelligence, 20(03):
329–350, 2006.

Rudin, C. Stop explaining black box machine learning
models for high stakes decisions and use interpretable
models instead. Nature Machine Intelligence, 1(5):206–
215, 2019.

Sirikulviriya, N. and Sinthupinyo, S. Integration of rules
from a random forest. In International Conference on In-
formation and Electronics Engineering, volume 6, 2011.

Smith, J., Everhart, J., Dickson, W., Knowler, W., and Jo-
hannes, R. Using the ADAP learning algorithm to fore-
cast the onset of diabetes mellitus. In Proceedings of the
Annual Symposium on Computer Applications in Medical
Care, pp. 261–265. IEEE Computer Society Press, 1988.

Tamon, C. and Xiang, J. On the boosting pruning prob-
lem. In Proceedings of the 11th European Conference on
Machine Learning, 2000.

Tan, H. F., Hooker, G., and Wells, M. T. Tree space pro-
totypes: Another look at making tree ensembles inter-
pretable. arXiv preprint arXiv:1611.07115, 2016.

Vandewiele, G., Lannoye, K., Janssens, O., Ongenae, F.,
De Turck, F., and Van Hoecke, S. A genetic algorithm for
interpretable model extraction from decision tree ensem-
bles. In Paciﬁc-Asia Conference on Knowledge Discovery
and Data Mining. Springer, 2017.

Supplementary Material – Proofs

Born-Again Tree Ensembles

Proof of Theorem 1. We show the NP-hardness of the born-again tree ensemble problem by reduction from 3-SAT. Let P
be a propositional logic formula presented in conjunctive normal form with three literals per clause. For example, consider
P = (x1 ∨ x2 ∨ x3) ∧ (¬x1 ∨ ¬x2 ∨ x4) ∧ (x1 ∨ ¬x3 ∨ ¬x4). 3-SAT aims to determine whether there exist literal values
xi ∈ {TRUE, FALSE} in such a way that P is true. From a 3-SAT instance with k clauses and l literals, we construct an
instance of the born-again tree ensemble problem with 2k − 1 trees of equal weight as follows:

• As illustrated in Figure 5, the ﬁrst k trees (t1, . . . , tk) represent the clauses. Each of these trees is complete and has a
depth of three, with eight leaves representing the possible combinations of values of the three literals. As a consequence
of this construction, seven of the leaves predict class TRUE, and the last leaf predicts class FALSE.

• The last k − 1 trees contain only a single leaf as root node predicting FALSE.

Finding the optimal born-again decision tree for this input leads to one of the two following outcomes:

• If the born-again decision tree contains only one leaf predicting class FALSE, then 3-SAT for P is FALSE.
• Otherwise 3-SAT for P is TRUE.

Indeed, in the ﬁrst case, if the born-again tree only contains a single FALSE region (and since it is faithful to the behavior
of the original tree ensemble) there exists no input sample for which TRUE represents the majority class for the 2k − 1
trees. As such, the ﬁrst k trees cannot jointly predict TRUE for any input and 3-SAT is FALSE. In the second case, either
the optimal born-again decision tree contains a single leaf (root node) of class TRUE, or it contains multiple leaves among
which at least one leaf predicts TRUE (otherwise the born-again tree would not be optimal). In both situations, there exists a
sample for which the majority class of the tree ensemble is TRUE and therefore for which all of the ﬁrst k trees necessarily
return TRUE, such that 3-SAT is TRUE. This argument holds for any objective involving the minimization of a monotonic
size metric, i.e., a metric for which the size of a tree does not decrease upon addition of a node. This includes in particular,
the depth, the number of leaves, and the hierarchical objectives involving these two metrics.

Figure 5. Trees representing the 3-SAT clauses for P = (x1 ∨ x2 ∨ x3) ∧ (¬x1 ∨ ¬x2 ∨ x4) ∧ (x1 ∨ ¬x3 ∨ ¬x4)

Proof of Theorem 2. Consider a tree ensemble T = {t1, . . . , t|T |}. We construct a sequence of decision trees starting
with T1 = t1 by iteratively appending a copy of tree tk for k ∈ {2, . . . , |T |} in place of each leaf of the tree Tk−1 to
form Tk. This leads to a born-again tree T|T | of depth (cid:80)
i Φ(ti). Each leaf of this tree represents a region of the feature
space over which the predicted class of the trees t ∈ T is constant, such that the ensemble behavior on this region is
faithfully represented by a single class. With this construction, tree T|T | faithfully reproduces the behavior of the original tree
ensemble. Since the optimal born-again tree T has a depth no greater than that of T|T |, we conclude that Φ(T ) ≤ (cid:80)
i Φ(ti).
Moreover, we prove that this bound is tight, i.e., it is attained for a family of tree ensembles with an arbitrary number of
trees. To this end, we consider the feature space X = Rd and the following 2d − 1 trees with equal weight:

• For i ∈ {1, . . . , d}, tree ti contains a single internal node representing the split xi ≤ 0, leading to a leaf node predicting

class 0 when the splitting condition is satisﬁed, and to a leaf node predicting class 1 otherwise.

• The remaining d − 1 trees contain a single leaf at the root node predicting class 1.

In the resulting tree ensemble, class 0 represents the majority if and only if xi ≤ 0 for all i ∈ {1, . . . , d}. To be faithful to
the original tree ensemble, the optimal born-again decision tree must verify that xi ≤ 0 for all i ∈ {1, . . . , d} to declare a
sample as part of class 0. This requires at least d comparisons. The depth of the born-again decision tree needed to make

x1 TRUE FALSE x2 x2 x3 x3 x3 x3 TRUE ETRUE ETRUE ETRUE ETRUE ETRUE (cid:38)(cid:4)(cid:62)(cid:94)(cid:28) ¬x1 TRUE FALSE ¬x2 ¬x2 x4 x4 x4 x4 TRUE ETRUE ETRUE ETRUE ETRUE ETRUE (cid:38)(cid:4)(cid:62)(cid:94)(cid:28) ¬x2 TRUE FALSE ¬x3 ¬x3 ¬x4 ¬x4 ¬x4 ¬x4 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE (cid:38)(cid:4)(cid:62)(cid:94)(cid:28) these tests is Φ(T ) = d = (cid:80)

i Φ(ti).

Born-Again Tree Ensembles

Proof of Theorem 3. Any tree T satisfying FT (x) = FT (x) on a region (zL, zR) also satisﬁes this condition for any
subregion (¯zL, ¯zR). Therefore, every feasible solution (tree) of the born-again tree ensemble problem for region (zL, zR)
is feasible for the subproblem restricted to (¯zL, ¯zR). As a consequence, the optimal solution value Φ(¯zL, ¯zR) for the
subproblem is smaller or equal than the optimal solution value Φ(zL, zR) of the original problem.

Proof of Theorem 4. We will use the extended notation 1jl(zL, zR) to denote 1jl. Firstly, we observe that 1jl(zL, zR) =
1jl(cid:48)(zL, zR) for all l and l(cid:48). Indeed, regardless of l and j,

(Φ(zL, zR

jl) = Φ(zL

jl, zR) = 0 and FT (zL) = FT (zR)) ⇔ Φ(zL, zR) = 0.

Next, we observe that Φ(zL, zR
Φ(zL, zR

jl) ≥ Φ(zL

jl, zR), then Φ(zL, zR

jl) ≤ Φ(zL, zR

jl(cid:48)) and Φ(zL

jl(cid:48), zR) ≤ Φ(zL

jl, zR) for all l(cid:48) > l follows from Theorem 3. If

jl(cid:48)) ≥ Φ(zL

jl(cid:48), zR) follows from the two previous inequalities and:

max{Φ(zL, zR

jl), Φ(zL

jl, zR)} = Φ(zL, zR

jl) ≤ Φ(zL, zR

jl(cid:48)) = max{Φ(zL, zR

jl(cid:48)), Φ(zL

jl(cid:48), zR)}.

Analogously, we observe that based on Theorem 3 Φ(zL
jl(cid:48), zR) ≥ Φ(zL, zR
jl, zR), then Φ(zL
If Φ(zL, zR

jl) ≤ Φ(zL

jl, zR) ≤ Φ(zL
jl(cid:48)) follows from the two previous inequalities and:

jl(cid:48), zR) and Φ(zL, zR

jl(cid:48)) ≤ Φ(zL, zR

jl) for all l(cid:48) < l holds.

max{Φ(zL, zR

jl), Φ(zL

jl, zR)} = Φ(zL

jl, zR) ≤ Φ(zL

jl(cid:48), zR) = max{Φ(zL, zR

jl(cid:48)), Φ(zL

jl(cid:48), zR)}.

Combining these results with the ﬁrst observation, we obtain in both cases that:

1jl(zL, zR) + max{Φ(zL, zR

jl), Φ(zL

jl, zR)} ≤ 1jl(cid:48)(zL, zR) + max{Φ(zL, zR

jl(cid:48)), Φ(zL

jl(cid:48), zR)}.

Supplementary Material – Pseudo-Codes for Objectives D and DL

Our solution approach is applicable to different tree size metrics, though the binary search argument resulting from
Theorem 4 is only applicable for depth minimization. We considered three possible objectives.

• (D) Depth minimization;
• (L) Minimization of the number of leaves;
• (DL) Depth minimization as primary objective, and then number of leaves as a secondary objective.

The dynamic programming algorithm for depth minimization (D) is described in the main body of the paper. Algo-
rithms 1 and 2 detail the implementation of the dynamic programming algorithm for objectives L and DL, respectively.
To maintain a similar structure and use the same solution extraction procedure in Section 5, these two algorithms focus
on minimizing the number of splits rather than the number of leaves, given that these quantities are proportional and only
differ by one unit in a proper binary tree. Moreover, the hierarchical objective DL is transformed into a weighted sum by
associating a large cost of M for each depth increment, and 1 for each split. This allows to store each dynamic programming
result as a single value and reduces memory usage.

The main differences with the algorithm for objective D occur in the loop of Line 8, which consists for L and DL in an
enumeration instead of a binary search. The objective calculations are also naturally different. As seen in Line 20, the new
number of splits is calculated as 1 + Φ1 + Φ2 for objective L (i.e., the sum of the splits from the subtrees plus one). When
using the hierarchical objective DL, we obtain the depth and number of splits from the subproblems as (cid:98)Φi/M(cid:99) and Φi%M,
respectively, and use these values to obtain the new objective. Our implementation of these algorithms (in C++) is available
at the following address: https://github.com/vidalt/BA-Trees.

Supplementary Material – Solution Extraction

To reduce computational time and memory consumption, our dynamic programming algorithms only store the optimal
objective value of the subproblems. To extract the complete solution, we exploit the following conditions to recursively
retrieve the optimal splits from the DP states. For a split to belong to the optimal solution:

Born-Again Tree Ensembles

Algorithm 2 BORN-AGAIN-L(zL, zR)

Algorithm 3 BORN-AGAIN-DL(zL, zR)

j ))
j ), zR)

for l = z L

j − 1 and LB < UB do

Φ1 ← BORN-AGAIN-L(zL, zR + ej(l − z R
Φ2 ← BORN-AGAIN-L(zL + ej(l + 1 − z L
if (Φ1 = 0) and (Φ2 = 0) then
if f (zL, T ) = f (zR, T ) then

1: if (zL = zR) return 0
2: if (zL, zR) exists in memory then
return MEMORY(zL, zR)
3:
4: end if
5: UB ← ∞
6: LB ← 0
7: for j = 1 to p and LB < UB do
j to z R
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23: end for
24: MEMORIZE((zL, zR), UB) and return UB

UB ← min{UB, 1 + Φ1 + Φ2}
LB ← max{LB, max{Φ1, Φ2}}

MEMORIZE((zL, zR), 1) and return 1

MEMORIZE((zL, zR), 0) and return 0

end for

end if

end if

else

for l = z L

j − 1 and LB < UB do

Φ1 ← BORN-AGAIN-DL(zL, zR + ej(l − z R
Φ2 ← BORN-AGAIN-DL(zL + ej(l + 1 − z L
if (Φ1 = 0) and (Φ2 = 0) then
if f (zL, T ) = f (zR, T ) then

1: if (zL = zR) return 0
2: if (zL, zR) exists in memory then
return MEMORY(zL, zR)
3:
4: end if
5: UB ← ∞
6: LB ← 0
7: for j = 1 to p and LB < UB do
j to z R
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23: end for
24: MEMORIZE((zL, zR), UB) and return UB

end if
DEPTH ← 1 + max{(cid:98)Φ1/M(cid:99), (cid:98)Φ2/M(cid:99)}
SPLITS ← 1 + Φ1%M + Φ2%M
UB ← min{UB, M × DEPTH + SPLITS}
LB ← max{LB, max{Φ1, Φ2}}

MEMORIZE((zL, zR), 0) and return 0

end for

end if

else

MEMORIZE((zL, zR),M+1) and return M+1

j ))
j ), zR)

1. Both subproblems should exist in the dynamic programming memory.
2. The objective value calculated from the subproblems should match the known optimal value for the considered region.

These conditions lead to Algorithm 3, which reports the optimal tree in DFS order.

Supplementary Material – Detailed Results

In this section, we report additional computational results which did not ﬁt in the main paper due to space limitations.
Tables 6 to 8 extend the results of Tables 2 and 3 in the main paper. They report for each objective the depth and number of
leaves of the different classiﬁers, as well as their minimum and maximum values achieved over the ten runs (one for each
training/test pair).

Table 6. Complexity of the different classiﬁers – Considering objective D

Random Forest
#Leaves
Avg. Min Max

Born Again Tree
Depth
Avg. Min Max

Data set

BC
CP
FI
HT
PD
SE

Overall

61.1
46.7
47.3
42.6
53.7
55.7

51.2

57
40
40
36
45
51

36

68
55
52
49
63
60

68

12.5
8.9
8.6
6.0
9.6
10.2

9.3

11
7
3
2
7
9

2

13
11
13
7
12
11

13

#Leaves
Avg. Min Max

2279.4 541
23
119.9
5
71.3
3
20.2
101
460.1
159
450.9

567.0

3

4091
347
269
38
1688
793

4091

Born Again Tree + Pruning
Depth
Avg. Min Max

#Leaves
Avg. Min Max

9.1
7.0
6.5
5.1
9.4
7.5

7.4

8
4
3
2
7
6

2

11
9
9
6
12
8

12

35.9
31.2
15.8
13.2
79.0
21.5

26
10
4
3
53
16

32.8

3

44
50
27
22
143
31

143

Finally, Tables 9 to 11 extend the results of Table 4 in the main paper. They report for each objective the average accuracy and
F1 scores of the different classiﬁers, as well as the associated standard deviations over the ten runs on different training/test
pairs.

Born-Again Tree Ensembles

for j = 1 to p do
j to z R
for l = z L

EXPORT a leaf with class MAJORITY-CLASS(zL)
return

j − 1 do
j ))
Φ1 ← MEMORY(zL, zR + ej(l − z R
j , zR)
Φ2 ← MEMORY(zL + ej(l + 1 − z L
if ΦOPT = CALCULATE-OBJECTIVE(Φ1, Φ2) then

Algorithm 4 EXTRACT-OPTIMAL-SOLUTION(zL, zR, ΦOPT)
1: if ΦOPT = 0 then
2:
3:
4: else
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end if

EXTRACT-OPTIMAL-SOLUTION(zL, zR + ej(l − z R
EXTRACT-OPTIMAL-SOLUTION(zL + ej(l + 1 − z L
EXPORT a split on feature j with level z L
j
return

end if
end for

end for

j ), Φ1)
j ), zR, Φ2)

Table 7. Complexity of the different classiﬁers – Considering objective L

Random Forest
#Leaves
Avg. Min Max

Born Again Tree
Depth
Avg. Min Max

Data set

BC
CP
FI
HT
PD
SE

Overall

61.1
46.7
47.3
42.6
53.7
55.7

51.2

57
40
40
36
45
51

36

68
55
52
49
63
60

68

18.0
8.9
8.6
6.3
15.0
13.8

17
7
3
2
12
12

11.8

2

20
11
13
8
19
16

20

#Leaves
Avg. Min Max

890.1 321
10
37.1
4
39.2
11.9
3
169.7 50
214.6 60

227.1 3

1717
105
107
19
345
361

1717

Born Again Tree + Pruning
Depth
Avg. Min Max

#Leaves
Avg. Min Max

9.0
6.5
6.3
4.3
11.0
7.7

7.5

7
3
3
2
8
6

2

11
8
8
6
17
9

17

23.1
11.4
12.0
6.4
30.7
14.2

17
4
4
3
20
9

16.3

3

32
21
20
9
42
19

42

Table 8. Complexity of the different classiﬁers – Considering objective DL

Random Forest
#Leaves
Avg. Min Max

Born Again Tree
Depth
Avg. Min Max

Data set

BC
CP
FI
HT
PD
SE

Overall

61.1
46.7
47.3
42.6
53.7
55.7

51.2

57
40
40
36
45
51

36

68
55
52
49
63
60

68

12.5
8.9
8.6
6.0
9.6
10.2

9.3

11
7
3
2
7
9

2

13
11
13
7
12
11

13

#Leaves
Avg. Min Max

1042.3 386
10
37.1
4
39.2
3
12.0
70
206.7
65
261.0

266.4

3

2067
105
107
19
387
495

2067

Born Again Tree + Pruning
Depth
Avg. Min Max

#Leaves
Avg. Min Max

8.9
6.5
6.3
4.6
8.9
7.4

7.1

8
3
3
2
7
6

2

10
8
8
6
11
9

11

27.7
11.4
12.0
6.5
42.1
17.0

18
4
4
3
28
12

19.5

3

39
21
20
10
62
24

62

Born-Again Tree Ensembles

Table 9. Accuracy of the different classiﬁers – Considering objective D

Random Forest
Acc.
Avg.

Std.

0.953 0.040
0.660 0.022
0.697 0.049
0.977 0.009
0.746 0.062
0.790 0.201

F1
Avg.

Std.

0.949 0.040
0.650 0.024
0.690 0.049
0.909 0.044
0.692 0.065
0.479 0.207

Born Again Tree
Acc.
Avg.

Std.

F1
Avg.

Std.

0.953 0.040
0.660 0.022
0.697 0.049
0.977 0.009
0.746 0.062
0.790 0.201

0.949 0.040
0.650 0.024
0.690 0.049
0.909 0.044
0.692 0.065
0.479 0.207

Data set

BC
CP
FI
HT
PD
SE

Born Again Tree + Pruning
Acc.
Avg.

F1
Avg.

Std.

Std.

0.946 0.047
0.660 0.022
0.697 0.049
0.977 0.009
0.750 0.067
0.790 0.196

0.941 0.046
0.650 0.024
0.690 0.049
0.909 0.044
0.700 0.069
0.481 0.208

Avg.

0.804 0.064

0.728 0.072

0.804 0.064

0.728 0.072

0.803 0.065

0.729 0.073

Table 10. Accuracy of the different classiﬁers – Considering objective L

Random Forest
Acc.
Avg.

Std.

0.953 0.040
0.660 0.022
0.697 0.049
0.977 0.009
0.746 0.062
0.790 0.201

F1
Avg.

Std.

0.949 0.040
0.650 0.024
0.690 0.049
0.909 0.044
0.692 0.065
0.479 0.207

Born Again Tree
Acc.
Avg.

Std.

F1
Avg.

Std.

0.953 0.040
0.660 0.022
0.697 0.049
0.977 0.009
0.746 0.062
0.790 0.201

0.949 0.040
0.650 0.024
0.690 0.049
0.909 0.044
0.692 0.065
0.479 0.207

Data set

BC
CP
FI
HT
PD
SE

Born Again Tree + Pruning
Acc.
Avg.

F1
Avg.

Std.

Std.

0.943 0.052
0.660 0.022
0.697 0.049
0.977 0.009
0.751 0.064
0.790 0.193

0.938 0.053
0.650 0.024
0.690 0.049
0.909 0.044
0.698 0.068
0.479 0.207

Avg.

0.804 0.064

0.728 0.072

0.804 0.064

0.728 0.072

0.803 0.065

0.727 0.074

Table 11. Accuracy of the different classiﬁers – Considering objective DL

Random Forest
Acc.
Avg.

Std.

0.953 0.040
0.660 0.022
0.697 0.049
0.977 0.009
0.746 0.062
0.790 0.201

F1
Avg.

Std.

0.949 0.040
0.650 0.024
0.690 0.049
0.909 0.044
0.692 0.065
0.479 0.207

Born Again Tree
Acc.
Avg.

Std.

F1
Avg.

Std.

0.953 0.040
0.660 0.022
0.697 0.049
0.977 0.009
0.746 0.062
0.790 0.201

0.949 0.040
0.650 0.024
0.690 0.049
0.909 0.044
0.692 0.065
0.479 0.207

Data set

BC
CP
FI
HT
PD
SE

Born Again Tree + Pruning
Acc.
Avg.

F1
Avg.

Std.

Std.

0.941 0.051
0.660 0.022
0.697 0.049
0.977 0.009
0.747 0.069
0.781 0.195

0.935 0.049
0.650 0.024
0.690 0.049
0.909 0.044
0.693 0.076
0.477 0.210

Avg.

0.804 0.064

0.728 0.072

0.804 0.064

0.728 0.072

0.801 0.066

0.726 0.075

Supplementary Material – Born-Again Tree Illustration

Born-Again Tree Ensembles

Finally, Figure 6 illustrates the born-again tree ensemble problem on a simple example with an original ensemble composed
of three trees. All cells and the corresponding majority classes are represented. There are two classes, depicted by a • and a
◦ sign, respectively.

Figure 6. Cells and born-again tree on a simple example

 x2 ≤ 4 x1 ≤ 7 x1 ≤ 2 ○ ● ○ ● x1 ≤ 2 x2 ≤ 2 x2 ≤ 4 ○ ● ○ ● x2 ≤ 2 x1 ≤ 7 x1 ≤ 4 ○ ● ○ ● TRUE FALSE TRUE FALSE TRUE FALSE ○ ○ ○ ○ ○ ○ ● ● ● ● ● ● MAJORITY CLASS TREE ENSEMBLE ● ● ● ● ● ○ ○ ○ ○ ○ ○ ○ MAP OF ALL CELLS DYNAMIC PROGRAM ● ● ○ ● ● ● ○ ○ ○ x2 ≤ 4 x1 ≤ 4 x1 ≤ 2 ○ ● TRUE FALSE BORN-AGAIN TREE x2 x1 7 4 2 2 4 x2 ≤ 4 x1 ≤ 7 ● ○ ● ○ 