1
2
0
2

r
a

M
0
1

]

G
L
.
s
c
[

1
v
7
5
8
5
0
.
3
0
1
2
:
v
i
X
r
a

Fast block-coordinate Frank-Wolfe algorithm for semi-relaxed
optimal transport

Takumi Fukunaga ∗

Hiroyuki Kasai †

March 11, 2021

Abstract

Optimal transport (OT), which provides a distance between two probability distributions by
considering their spatial locations, has been applied to widely diverse applications. Computing an
OT problem requires solution of linear programming with tight mass-conservation constraints.
This requirement hinders its application to large-scale problems. To alleviate this issue, the
recently proposed relaxed-OT approach uses a faster algorithm by relaxing such constraints.
Its eﬀectiveness for practical applications has been demonstrated. Nevertheless, it still exhibits
slow convergence. To this end, addressing a convex semi-relaxed OT, we propose a fast block-
coordinate Frank-Wolfe (BCFW) algorithm, which gives sparse solutions. Speciﬁcally, we provide
their upper bounds of the worst convergence iterations, and equivalence between the linearization
duality gap and the Lagrangian duality gap. Three fast variants of the proposed BCFW are
also proposed. Numerical evaluations in color transfer problem demonstrate that the proposed
algorithms outperform state-of-the-art algorithms across diﬀerent settings.

1 Introduction

The Optimal transport (OT) problem seeks an optimal transport plan or transport matrix by solving
the total minimum transport cost from sources to destinations. This calculation requires source
mass conservation from one source to targets, and versa, which are represented in formulation as a
transport polytope. The OT problem can express the distance between two probability distributions,
which is known as Wasserstein distance [1]. Consequently, this problem has been applied to widely
diverse machine learning problems such as adversarial risk [2], inference with aggregate data [3],
graph optimal transport [4, 5], domain adaptation [6], multi-view learning [7], and clustering [8].
Among the OT problem formulations, the Kantorovich formulation is represented as convex linear
programming (LP) [9]. Thereby, many dedicated solvers such as an interior-point method and a
network-ﬂow method can obtain the solutions. It is, nevertheless challenging to solve large-scale
problems eﬃciently because its computational cost increases cubically in terms of the data size.

To alleviate this diﬃculty, the Sinkhorn algorithm [10], an entropy-regularized approach, works
eﬀectively on the OT problem, which is faster and which enables a parallel implementation. This
computation includes a diﬀerentiable and unconstrained convex optimization. For that reason, it
is easier to solve. In addition, the resultant OT distance is applicable to many machine learning
problems by virtue of its diﬀerentiability. Furthermore, addressing its numerical unsuitability and

∗Department of Communications and Computer Engineering, School of Fundamental Science and Engineering,

WASEDA University, 3-4-1 Okubo, Shinjuku-ku, Tokyo 169-8555, Japan (e-mail: f takumi1997@suou.waseda.jp)

†Department of Communications and Computer Engineering, School of Fundamental Science and Engineering,

WASEDA University, 3-4-1 Okubo, Shinjuku-ku, Tokyo 169-8555, Japan (e-mail: hiroyuki.kasai@waseda.jp)

1

 
 
 
 
 
 
non-robustness against small values of the regularizer, stabler variants have also been developed,
but they are adversely aﬀected by their slow convergences [11]. To reduce the runtime, a greedy
algorithm of the Sinkhorn algorithm, the Greenkhorn algorithm [12] and its accelerated variant
[13] have been proposed.
It should be noted that these approaches produce a dense transport
matrix because the entropy term is always positive. Along another avenue of development, a
smooth-regularized approach exploits strong convexity and Lipschitz continuity [14], where adding
smooth terms onto the objective function enables harnessing of gradient-based approaches and dual
formulations. One distinguishing feature is that regularization with the squared Euclidean norm
obtains sparser solutions than the entropy-regularized approaches. Recent studies have exploited
smoothness to the curse of dimensionality. Speciﬁcally, methods such as the smooth and strongly
convex Brenier potentials [15] and the Gaussian-smoothed OT [16] have been described in reports
of the relevant literature.

Most of the previously described works have attempted to add regularizers onto the objective
function. Some works address the fact that the tight mass-conservation constraint in the OT
problem does not work well in some applications where weights and mass need not be preserved.
For this particular problem, a constraint-relaxed approach has been proposed recently by loosening
such strict constraints. This approach has gained great success for applications such as color transfer
[17] and multi-label learning [18]. However, it still exhibits a slow convergence property.

Envisioning the development of a faster solver producing sparser solutions in the OT problem,
and particularly addressing its convex semi-relaxed formulation, this paper is the ﬁrst to present
a block-coordinate Frank-Wolfe (BCFW) algorithm with theoretical analysis. The FW algorithm
(a.k.a. the conditional gradient method) is a class of linear convex programming methods calling
a linear optimization oracle [19]. The key advantage of this algorithm is that its projection-free
property is generally more eﬃcient than projection operations when the dimension of the data
is large. Thus, the FW algorithm is one of the most popular approaches in the OT problem
[20, 21, 22, 23]. In addition, the output solutions of the FW algorithm can be sparse, which are
beneﬁcial in many applications. However, because this algorithm must call linear oracle for all
columns of the transport matrix at every iteration, its computational burden is problematic when
the matrix size is extremely large. Hence, we further combine a coordinate descent approach, which
selects one column randomly every iteration, resulting in much smaller computation cost, and also in
achieving faster convergence [24]. The block coordinate approach is also discussed in the literature
of the OT problem [25, 26]. Although this approach has already been discussed in the literature
for various problems [27], its concrete convergence for the relaxed OT problem remains unclear.
Therefore, this paper can oﬀer several important theoretical contributions.

• Our convergence analysis yields an upper-bound of the curvature constant without relying on
an oracle, as described in an earlier paper by [27]. Then, we directly exploit a variable block
on the semi-relaxed domain and give iteration complexities for ǫ-optimality with FW and
BCFW algorithms for the semi-relaxed OT problem.

• Our analysis of the duality gap reveals that the linearization duality gap, a special case of the
Fenchel duality gap, is equivalent to the Lagrangian duality gap. We derive the Lagrangian
dual for the semi-relaxed OT problem. We prove this equivalence. This linearization duality
gap certiﬁes the quantity of the current approximation for monitoring the convergence. This
point can be exploited for the stopping criterion in our proposed algorithms.

• This paper proposes three fast variants of the proposed BCFW, i.e., the BCFW algorithms
with pairwise-steps and away-step, and gap-adaptive sampling. For the latter, a convergence
analysis is also provided.

2

• Numerical evaluations on the color transfer problem gives detailed analysis of the proposed
BCFW, and show the eﬀectiveness of the proposed BCFW in the semi-relaxed OT problem.

The paper is organized as explained hereinafter. Section 2 presents preliminary descriptions of
optimal transport, (semi-)relaxed optimal transport, and the block-coordinate Frank-Wolfe (BCFW)
algorithm. Section 3 presents details of our proposed BCFW algorithm for the semi-relaxed optimal
transport problem. The theoretical analysis for the convergence, duality gap, and computational
complexity are also provided. In Section 5. we discuss three fast variants of the proposed BCFW
algorithm with away-steps, pairwise-steps, and gap-adaptive sampling. Finally, in Section 6, numer-
ical comparisons with existing methods are provided with results suggesting superior performance
of the proposed BCFW algorithms. The proposed BCFW codes are implemented in MATLAB.
Concrete proofs of theorems, additional numerical results and source codes are provided as supple-
mentary materials.

2 Preliminary and related work

×

×

n denotes the set of m

Herein, Rn denotes n-dimensional Euclidean space. Also, Rn
+ denotes the set of vectors in which all
n matrices and Rm
elements are non-negative. Rm
n
denotes the set of
×
×
+
m
n matrices in which all elements are non-negative. We present vectors as bold lower-case letters
a, b, c, . . . and matrices as bold-face upper-case letters A, B, C, . . . . The i-th element of a and the
element at the (i, j) position of A are represented respectively as ai and Ai,j. When a matrix A
is denoted as (a1, . . . , an), ai represents the i-th column vector of A. ei is the canonical standard
Rn is the n-dimensional vector in
unit vector, of which the i-th element is 1. Others are zero. 1n
.
i ai = 1
which all the elements are one. The probability simplex is denoted as ∆m =
}
δa is the delta function at the vector a.
is the Euclidean dot-product between vectors. For two
= tr(AT B) is the Frobenius dot-product. We denote
matrices of the same size A and B,
the set

,
·i
h·
A, B
i
h

by [n].

Rm :

a
{

P

∈

∈

1, . . . , n
{

}

2.1 Optimal transport problem

The OT problem derives from the Monge problem, which seeks an optimal mapping between two
probability distributions as ν =

n
i=1 biδyi given as

m
i=1 aiδxi, µ =

P

min
T

subject to

m

P

d(xi, T (xi))

i=1
X
bj =

ai,

j

∀

∈

[m],

Xi:T (xi)=yj

where d(
) is the cost function between two points. Both mapping and constraints are discrete,
,
·
·
thus, the Monge problem is diﬃcult to solve directly. To this diﬃculty, Kantorovich proposed a
formulation by which the constraints are continuous [9]. Concretely, given a cost matrix C, the
problem is deﬁned as

where the domain

(a, b) is deﬁned as

U

T

min
∈U

(a,b) h

T, C

,
i

(a, b) =

T
{

∈

U

n

Rm
+

×

: T1n = a, TT 1m = b

.
}

3

(1)

(2)

(a, b) requires the mass-conservation constraints or the marginal constraints between
This domain
two probabilities a and b. The obtained optimal transport matrix T∗ brings powerful distances
between distributions deﬁned as

U

p(ν, µ) =

T∗, C
h

i

W

1
p ,

which is called the p-th order Wasserstein distance [28]. Especially, when p = 1, the distance is
equivalent to the Earth Mover Distance (EMD) [29]. Many problems appearing in machine learning
and statistical learning are deﬁnable in the OT problem. Interested readers are referred to [1] for a
more comprehensive survey.

2.2 Relaxed optimal transport

As discussed in Section 1, solving large-scale linear programming problems is challenging in terms
of the computational costs of obtaining solutions [30]. Furthermore, the strict mass-conservation
constraints might cause dreadful degradation of performance in some application. For example,
Ferradans et al.
reported that tight mass conservation does not reﬂect the color diﬀerence be-
tween images in a color transfer problem [20]. This subsection introduces two categories of relaxed
formulations of the OT problems.

Domain constraint relaxation. One approach is to relax the domain constraint [20]. Ferradans
et al. propose allowing each point of X to be transported to multiple points of Y and vice versa.
This is deﬁned as

T, C

min
T
∈S

κ h

,
i

where a relaxed domain

κ is deﬁned as

S

κ =

S

T
{

∈

n

Rn
+

×

: kX1n

T1n

≤

≤

KX 1n, ky1n

TT 1n

≤

≤

KY 1n, 1T

n T1n = M

,
}

and constants (kX , KX , kY , KY , M ) are hyper-parameters. This method enables the transport ma-
trix to increase or decrease the mass between two points. A noteworthy point is that the relaxed
domain retains the linear constraints as the original. For that reason, existing solvers of linear
programming are applicable. Rabin et al. extend it to propose the relaxed weighted OT, which
loosens the column constraints [17]. There also exist other relaxed formulations considering only
T1n = a or TT 1m = b as

min
T1n = a h

T, C

i

or

min
TT 1m=b h

T, C

.
i

These optimal solutions are summation of minimum costs of each row or column vector. Therefore,
they are solvable faster than linear programming. In practice, this method is useful for document
classiﬁcation [31].
Its extended formulation has recently been developed in the context of style
transfer [32, 33]. They attempt to deﬁne the relaxed earth mover distance (REMD) as the maximum
of above formulations, and combine it with neural networks.

Regularized constraint relaxation. In another line of attempts, the penalty of the domains
deﬁned in (2) is added to the objective function [14]. The relaxation of the marginal constraints is
eﬀective when only partial transport is allowed. Relaxing both marginal constraints in (2) yields
the following relaxed formulation as

T, C

min
T
≥

0 h

1
2

+

i

Φ(T1n, a) +

1
2

Φ(TT 1m, b),

where Φ(x, y) is a smooth divergence measure function.

4

We also have an alternative formulation, which relaxes one of the two constraints in (2). This

is a semi-relaxed problem, deﬁned as the following.

min
0,TT 1m=b h

T, C

i

T

≥

+ Φ(T1n, a).

(3)

−

1n

κ
k

This setting is useful in color transfer. Rabin et al. also propose the weighted regularization
term
k1 and the relaxed weighted OT so that the ratio of the source image becomes close
to that of the reference image [17]. Benamou proposes a similar formulation, which is solvable
using the augmented Lagrangian [34]. Ferradans et al. propose a regularized and relaxed problem
particularly addressing both color transfer and barycenter [20]. They use the proximal splitting
method and the coordinate descent method. Moreover, using the Kullback–Leibler (KL) divergence
as Φ(x, y), a multi-label prediction problem is proposed, which is solved by using a Sinkhorn-like
algorithm because of the entropy-regularized term [18]. However, the KL divergence is not unstable
because of divergence at zero [11]. Furthermore, some relaxed methods address cardinality-penalized
problems. Instead of cardinality of solutions, Carli et al. approximate them by exploiting the rank
regularization, sum-of-norm relaxation, and maximum norm relaxation for eﬀective clustering [35].

2.3 Block-coordinate Frank-Wolfe algorithm

The Frank-Wolfe (FW) algorithm is a constraint convex optimization method. It is known to be a
linear approximation algorithm that uses conditional gradient [19]. Although the FW algorithm is
known to converge to optimal solutions at a sublinear rate, its projection-free property is preferred
in the case where the convex constraint is simple and the feasible point can be found easily. More
speciﬁcally, at every iteration, the feasible point s is found ﬁrst by minimizing the linearization of
. To ﬁnd the feasible point s, we solve the following subproblem.
f over the convex feasible set

M

s = arg min
s′

∈M

s′,
h

f (x(k))
.
i

∇

(4)

.
M

In that equation, x(k) represents the k-th current point. Because the domain
is the convex set
and the objective is linear for s, it is possible to solve (4) using linear programming. Finally, the
γ)x(k) + γs, where γ is
next iterate x(k+1) is obtainable by a convex combination as x(k+1) = (1
the stepsize. Consequently, the generated iterates can be maintained inside the feasible set
if
the initial point x(0) is in

M

M

−

One shortcoming of the FW algorithm is that solving the minimization problem must be per-
can be block-separable as the Cartesian
formed at each iteration. To address this issue, if domain
Rm over n
1, then we can perform a single cheaper
product
=
update of only
. In this line of algorithms, the block-coordinate
M
Frank-Wolfe (BCFW) algorithm has been proposed, for example, in the structural SVM problem
[27] and in the MAP inference [36]. This algorithm is applicable to the constrained convex problem
of the form

(1)
× · · · × M
(i) instead of on an entire of

M
≥

M
M

× M

M

(n)

⊂

(2)

min
(2)

(1)

f (x).

(n)

x

∈M

×M
×···×M
(i) is convex, with m =

n
We assume that each factor
i=1 mi. We solve the subproblem on the
factor which is selected randomly. As a result, the BCFW algorithm can be implemented in cheaper
iteration. When n = 1, this algorithm is reduced to the FW algorithm.

M

P

5

3 Block-coordinate Frank-Wolfe algorithm for semi-relaxed

optimal transport problem

The present paper particularly addresses the semi-relaxed problem of (3) with Φ(x, y) = 1
x
because it is not only smooth but also convex. The problem of interest is formally deﬁned as

2λ k

y

2
2
k

−

min
0,
T
TT 1m=b (cid:26)
≥

f (T) :=

T, C
h

i

+

1
2λ k

T1n

a

2
2
k

−

,

(cid:27)

where λ is a relaxation parameter. The domain is transformed into

where bi∆m represents the simplex of the summation bi.

= b1∆m

b2∆m

×

M

× · · · ×

bn∆m,

3.1 Algorithm description

(5)

(6)

After describing Frank-Wolfe (FW) algorithm, we elaborate on a block-coordinate Frank-Wolfe
(BCFW) algorithm for the semi-relaxed optimal transport problem.

Frank-Wolfe (FW) algorithm. We ﬁrst consider the FW algorithm for this problem, and then
propose a faster block-coordinate Frank-Wolfe algorithm. The gradient

Rmn is given as

f (T)

∇

∈

f (T) =

∇







+

1
λ

c1
...
ci
...
cn

a

−



a

,

T1n
...
T1n
...
T1n

−
















Rm represents the gradient on the i-th variable block bi∆m.

















−

a

fi(T) := ci + 1/λ
where
−
The subproblem (4) is equivalent to

(T1n

∇

·

a)

∈

si = biej = bi arg min

ek,

∆m,k

ek∈

[m]h
∈

if (T(k))
,
i

∇

(7)

∈

[m] and ej is the extreme point on probability simplex [37]. In other words, we just ﬁnd
where j
the index of the minimal elements of the gradient of the variable blocks. The computational cost
of the subproblem (7) is greatly improved. The detailed computational cost analysis is described in
Section 3.2.3.

After ﬁnding the points S = (s1, s2, . . . , sn)

n, we search an optimal stepsize γ. One classi-
cal way in the FW algorithm is a decay stepsize (DEC), where γ = 2/(k+2) with the iteration number
γ)x + γs),
k. A line-search algorithm can be also applicable. Concretely, we solve minγ
and calculate γ directly since the objective of the semi-relaxed problem is quadratic. As for the
stopping criterion, we monitor the duality gap g(T) that will be deﬁned in Theorem 3.3 in Section
3.2.2, and stop the algorithm when g(T) < ǫ, where ǫ is an approximation precision parameter.

[0,1] f ((1
∈

−

∈

×

Rm

Block-coordinate Frank-Wolfe (BCFW) algorithm. We now propose an application of the
block-coordinate Frank-Wolfe algorithm to the semi-relaxed problem considering that the feasible set
can be separable as the cartesian product. The procedure of Algorithm 1 most closely resembles
M
that of the FW algorithm, but they are slightly diﬀerent. It is necessary to solve the subproblem on

6

Algorithm 1 Block-coordinate Frank-Wolfe (BCFW) for semi-relaxed OT
Require: T(0) = (t(0)
1: for k = 0 . . . K do
2:

1 , . . . , t(0)
n )

[n] randomly

× · · · ×

bn∆m

b1∆m

∈

Select index i
Compute si = bi arg min

∈

Compute stepsize γ as

∆m,k

ek∈

[m] h
∈

3:

4:

ek,

if (T(k))
i

∇

5:

Update t(k+1)

j

j

∀

∈

[n] as

γ =






t(k+1)
j

=

6: end for

,

t(k)
j
(1

−

(

γLS,
2n
k + 2n

(line

search in (8))

−

,

(decay rule)

γ)tk

i + γsi,

(for j
= i)
(otherwise)

the variable block selected randomly at every iteration. More concretely, the subproblem is identical
to (7), but we solve the subproblem only for the i-th column, which is selected randomly. Then, all
the other columns of T remain the same. Regarding the stepsize calculation, we use the formula
γ = 2n/(k + 2n), which is necessary for the convergence guarantee, as shown in Theorem 3.2.
Similarly to the FW algorithm, an exact line-search (ELS) algorithm can be used. Nevertheless,
the optimal stepsize in the BCFW algorithm diﬀers from that of the FW algorithm (A.1) because
the BCFW algorithm only requires the updated column vector on the variable block. Consequently,
the optimal stepsize γLS is calculated as

λ

t(k)
i −
h

si, ci

γLS =

i

t(k)
+
i −
h
t(k)
si
i −
k

si, T(k)1n
2
k

a
i

,

−

(8)

−

where ti is the i
th column of T, and si is the solution of the i-th subproblem in (7). As we will
discuss in Theorem 3.3, the duality gap can be used for the stopping criterion, and in a practical
implementation, we monitor the value of the duality gap because the subproblem is solved at every
iteration. It is noteworthy that, in the BCFW algorithm, calculating the value of the duality gap
from the solution of the value of (7) is impossible because the solutions of the subproblems on all
the variable blocks are needed. Therefore, calculating the duality gap, if attempted every iteration,
engenders huge increases of runtime, consequently resulting in loss of the beneﬁt of the cheaper
iteration complexity in BCFW. Consequently, in our practical implementation, we monitor the
duality gap every n iterations, of which period is equal to that of the FW algorithm.

Lastly, we consider two rules for choosing (sampling) a column at each iteration: the uniform
[n], of which con-
random order and the random permutation. The former randomly selects i
vergence analysis is given in Section 3.2.1. The latter runs a cyclic order on a permuted index, for
example (1
when n = 3. Those algorithms are,
1
hereinafter, denoted, respectively, as BCFW-U and BCFW-P. Another sampling strategy using the
duality gap is discussed in Section 4.2, which is called BCFW-GA.

→ · · ·

→

→

→

→

→

→

→

→

(3

(2

3)

3)

1)

∈

2

2

7

6
3.2 Theoretical results

This section explains convergence analysis of the FW and BCFW algorithms proposed in the pre-
ceding subsection. We then discuss the relation between the linearization duality gap as a special
case of the Fenchel duality gap and Lagrange duality gap. This discussion provides equivalence
between them in this semi-relaxed OT problem. Finally, we also summarize their computational
complexity.

3.2.1 Convergence analysis

We analyze theoretically the worst convergence iteration of the FW and BCFW algorithms. The
result for the FW algorithm is provided in the supplementary material. The result for the proposed
BCFW algorithm is given below. We ﬁrst deﬁne the curvature constant C ⊗f as follows:

Deﬁnition 3.1 (Curvature constant for cartesian product [27]). When a domain
of the cartesian product

(n), its curvature constant is deﬁned as C ⊗f

has a structure
:=

M

(2)

(1)

M

× M

× · · · × M

n

i=1 C (i)

f , where C (i)

f

is

P

C (i)
f

2
γ2 (f (Y)

−

f (T)

yi −

− h

ti,

if (T)
),
i

∇

:=
T

sup
, si
∈ M
∈ M
γ
[0, 1],
∈
Y = T + γ(s[i] −

(i),

t[i])

where x[i] refers to the zero-padding of xi.

Then, we have the convergence of BCFW:

≤

C
k

k+2n . Then, we have E[f (T(j))]

Theorem 3.2. Let T∗ is the optimal solution of the semi-relaxed OT problem in (5). Consider
Algorithm 1 under the initial point of T as T(0) = (b1e1, . . . , bie1, . . . , bne1) with a decay stepsize
rule k = 2n
f (T∗)
f (T∗),
−
4
λ . Additionally, given an approximation precision
and where C ⊗f is the curvature constant with
( n
λǫ ) for its convergence.
constant ǫ, if
Otherwise, it requires the additional number of 2nh0

≤
2
λ , Algorithm 1 requires at most the number of

2n
k+2n (C ⊗f + h0), where h0 = f (T(0))

ǫ ≤
For its proof, we ﬁrst bound the curvature constant C ⊗f by taking into account the twice diﬀer-
entiability of f (T) and the simplex structure. g(T(0)) is also upper-bounded from
and the
assumption of T(0). Finally, we derive the upper-bound of the complexity. It should be noted that
λ . The full proof is given in the supplementary
some additional iterations are needed when
materiel.

k∞ ≤

C
k

C
k

C
k
ǫ

> 2

k∞

k∞

2n(

O

−

∞)

.

k

3.2.2 Linearization duality gap and stopping criterion

The linearization duality is a special case of the Fenchel duality in the FW algorithm [27, 38], and
its duality gap at the points x is given as

g(x) = max
∈M

s′

x
h

−

s′,

f (x)
i

∇

=

x
h

s,

f (x)
,
i

∇

−

M

is convex. Note that adding f (x) onto the linearization duality is equivalent to the Wolf
where
duality [37]. For the semi-relaxed OT problem, we speciﬁcally give the equivalence between the
linearization gap, denoted as g(T), and the Lagrangian duality gap as shown below.

8

Theorem 3.3. Consider the semi-relaxed problem in (5). The linearization duality gap is provided
as

1
λ h
where S is the solution of the subproblem (7). Then, the linearization duality gap g(T) is equivalent
to the Lagrangian duality gap of the semi-relaxed problem.

S1n, T1n

g(T) =

T
h

T1n

S, C

,
i

−

−

−

+

a

i

The full proof is given in the supplementary materiel, but its proof sketch is the following: The

dual problem is ﬁrst derived as

max
T

f (T)

n

−

ti,
h
i=1
X

if (T)
i

∇

+

n

i=1
X

(
bi max
∇
[m]
j
∈

if (T))j.

Then, we consider the Lagrangian duality gap gL(T) as the diﬀerence between the objective and
dual objective of the semi-relaxed problem. Finally, we show that gL(T) is equal to g(T) deﬁned
in this theorem. From this theorem, we can use the function g(T) as both the linearization duality
gap and the Lagrangian duality gap. Therefore, g(T) is suitable for the stopping criterion of the
algorithms.

3.2.3 Computational complexity analysis of baseline algorithm of BCFW

O

Next we analyze the subproblem of the semi-relaxed OT problem. The subproblem is solvable
((mn)3 log(mn)) because the
using linear programming. However, the computational cost is
transport matrix T is vectorized as the mn-dimension for linear programming. The column vector
of the transport matrix T is independent of other column vectors of the semi-relaxed OT problem.
Therefore, because it is possible to solve the subproblem on n variable blocks, the computational
(nm3 log m). The subproblem (7) in the
complexity of the subproblem (4) can be reduced to
proposed FW algorithm is deﬁned on the Cartesian product of the probability simplex. Therefore,
it is equivalent to the problem (7) [38]. As a result, the computational complexities of linear
programming
(m), which speeds up the time. The BCFW algorithm
requires only one variable block selected randomly at every iteration, whereas the FW algorithm
must solve the n variable block. The BCFW algorithm has the same convergence as that of the FW
algorithm. As a result, the computational complexities of the BCFW algorithm are more improved
than those of the FW algorithm. We further analyze of the computational complexities of fast
variants of the proposed BCFW algorithm in Section 4.3.

(m3 log m) are reduced to

O

O

O

4 Fast variants of BCFW

4.1 Variants of pairwise-steps (BCAFW) and away-steps (BCPFW)

As discussed in the previous section, the Frank Wolfe (FW) algorithm exhibits sublinear, thus several
improvements have been investigated to accelerate this rate [39]. Among them, this subsection
addresses and follows a strategy that replaces the FW direction with diﬀerent directions, which
are called the pairwise-steps [39] and the away-steps [40]. These modiﬁcations achieve a linear rate
without the strongly convexity of the objective [41]. More speciﬁcally, we exploit the away-steps and
the pairwise-steps in block-coordinate method [42, 43]. We denote the BCFWs with the away-steps
and with the pairwise-steps as BCAFW and BCPFW, respectively.

The BCFW algorithm generates convex combinational points from atoms in each variable block.
Thus, there might exist select non-desirable atoms, and this leads to sublinear convergence rates of

9

the FW and BCFW algorithms. To avoid this situation, the away-steps and pairwise-steps have
been proposed. They remove unnecessary atoms from an active set on each variable block. By
following the work [42], this paper combines the proposed BCFW algorithm with the away-steps
and the pairwise-steps, and attempts to improve the rate of convergence.

Let

i be the active set on the i-th (i

S

i =

S

ej
{

∈

∈

[n]) variable block, which is deﬁned as

∆m : αej > 0, j

,
[n]
}

∈

(9)

where αej is the coeﬃcient of the j-th extreme point ej. This is because each variable block in the
semi-relaxed problem is the probability simplex, and the extreme points is in
. We
}
then consider a new subproblem in order to remove the atoms, which is deﬁned as

e1, e2, . . . , en
{

vi = arg max
v′

i

∈S

v′, ci +
h

1
λ

(T1n

a)
.
i

−

.
}
⊂ {
and the Away direction dAway =
vi, respectively, we select the one reducing the objective function value more. Then, we ﬁnd
γ)x + γs). This stepsize γLS is calculated by replacing

This problem can be solved in the same way as the subproblem (7) because
Deﬁning two directions, i.e., the FW direction dFW = si
t(k)
i −
the stepsize γ satisfying minγ
t(k)
i −

[0,γmax] f ((1
∈
si in (8) with d, which is given by

e1, e2, . . . , en

t(k)
i

i
S

−

−

λ

d, ci
h

i

+

γLS =

−

d, T(k)1n
h
2
d
k
k

a
i

,

−

(10)

but

(k)
i

S

where d can be dFW or dAway. Then we must update not only the selected column vector t(k)
also the selected active set

.

i

We similarly consider the block-coordinate Pairwise Frank-Wolfe (BCPFW), of which procedure
is similar to that of BCAFW. The main diﬀerence is the updating direction. While the BCAFW
algorithm combines the current point t(k)
i with the direction, the BCPFW only uses two atoms si
and vi. We set the direction dPair = si
vi and γmax = αvi. This operation develops the movement
between only two atoms and improves the convergence rate.

−

The overall algorithms of BCAFW and BCFPFW are summarized in Algorithms A.2 and A.3,

respectively.

4.2 A variant of adaptive sampling (BCFW-GA)

This subsection, furthermore, focuses on another approach to fasten the convergence speed of the
proposed algorithm, which is an adaptive sampling scheme that is popular approach in block-
coordinate methods [44, 45, 46, 47]. This paper particularly addresses the approach considering
the duality gap, and denotes the method as the BCFW algorithm with gap-adaptive sampling
(BCFW-GA).

4.2.1 Algorithm description

The BCFW algorithm operates on the block-separable domain as in (6), and updates one single
column that is randomly selected. Therefore, the convergence rate of such coordinate-descent-based
algorithms heavily depends on its sampling method, i.e., sampling probability distribution over the
coordinates (columns). While the BCFW-U and BCFW-P algorithms proposed earlier use a uniform
sampling, a new variant of the BCFW algorithm in this section attempt to faster the convergence

10

by exploiting the weighted distributions that are generated by the duality gap in each column, i.e.,
column-wise duality gap. In the literature, many eﬀorts have been done in this particular direction
for this decade. This paper speciﬁcally follows the same line of the researches of [42, 45] because
they address the duality gap whereas others mainly focus on the Lipschitz constants of the gradients
[44, 45, 46, 47].

The main idea behind our proposed approach is as follows: The columns with larger duality gaps
admit higher improvement to the objective function value, thus, such columns should be sampled
In this way, we try to make more signiﬁcant progress than the uniform-sampling
more often.
method. For this purpose, after update of ti, the proposed BCFW-GA updates the duality gap for
each column. Here, note that g(T) is given as

g(T) =

S, C
i

−

+

T
h

n

1
λ h

T1n

S1n, T1n

a

i

−

−
n

si)T ci +

(ti

−

=

i=1
X

where gi(T) is given by

1
λ h

i=1
X

(ti

si), T1n

a
i

−

=

−

gi(T),

n

i=1
X

gi(T) =

ti
h

−

si, ci

+

i

1
λ h

ti

−

si, T1n

a

.
i

−

[n].

i

∀

∈

(11)

Therefore, updating the column-wise duality gap gi(T) every iteration, we select an index i at
random in proportion to the probability generated from (g1(T), g2(T), . . . , gn(T)).

−

In the meantime, the update of gi(T) apparently depends on T. Hence, every time one single ti
is updated, we need to re-calculate gi(T) of all other (n
1) columns to obtain its correct probability.
Nevertheless, this is intractable, and wastes the beneﬁt of the block coordinate approach. Therefore,
n iterations, we periodically update gi(T) of all the columns to obtain
in practice, at every M
their exact values. This update is speciﬁcally called the global update in this paper, and the loop
of this global update is called an outer iteration. In contract to the outer iteration, the update of
single gi(T) within the cycle of the global update is called an inner iteration. Within the global
update period, i.e., the inner iteration, we store the calculated gi(T) for each i-th column, and do
not perform the global update for the other columns. For the update of gj(T) of the j-th column
= i), we utilize the stored latest (but outdated) gj(T). Hence, we expect that, when M is
(j
reasonably small, the convergence can be achieved, otherwise not.

×

The overall algorithm of BCFW-GA is summarized in Algorithm A.4.

4.2.2 Convergence analysis of BCFW-GA

We give a convergence analysis of BCFW-GA, which is a straightforward extension to the semi-
relaxed OT problem from that of the structured SVM problem in [42].

Theorem 4.1. (Total complexity analysis of BCFW-GA (Algorithm A.4)) Let T∗ is the optimal
solution of the semi-relaxed OT problem. Consider Algorithm A.4 under the initial point of T
as T(0) = (b1e1, . . . , bnen) with a decay stepsize rule γ = 2n
2
λ , Algotihm
( 8n
λ , it requires
ǫλ ) at best and
A.4 requires
O
C
ǫλ + 8
ǫλ√n ) at best and
k

k+2n . Then, if
( 8n√n
ǫλ ) at worst. On the other hand, if
O
C
( 2n
k
ǫ

ǫλ ) at worst.

k∞ ≤
> 2

C
k
C
k

+ 8n√n

( 2n

k∞

∞

∞

k

k

O

O

The followings are remarks.

11

6
Remark 4.2. (Comparison to Theorem 3.2 in BCFW)

BCFW has

2n
k + 2n ·

ǫ

≥

4
λ

+

4
λ

(cid:19)

(cid:18)

k

≥

⇐⇒

8n
λǫ

+

8n
λǫ −

2n

approx.
⇐⇒

k

≥

8n
λǫ

+

8n
λǫ

=

16n
λǫ

.

)
(
∗∗
|{z}

Comparing this to the result of BCFW-GA, we conﬁrm that the term (**) of BCFW is replaced
ǫλ√n in (A.10) of BCFW-GA. Therefore, we ﬁnd that BCFW-GA can reduce the complexity
with
of BCFW roughly by half in the best case. It is, however, that, in the worst case, it increases by

8

( 8n√n

ǫλ ).

O

As for the the worst case, we have a similar discussion as [42] below.

Remark 4.3. Recall that the BCFW algorithm has E[f (T(k))]
2n
k+2n (C ⊗f + h0) in The-
orem 3.2, and BCFW-GA has E[f (T(k))]
2n
f (T∗)
k+2n (C ⊗f χ⊗ + h0) as in (A.9) in Theorem
E.3. Comparing two inequalities, BCFW-GA has the additional coeﬃcient χ⊗. Therefore, we have

f (T∗)

−

≤

≤

−

√n
χ(g:(x(k)))3 ≤

1

⇐⇒

χ(g:(x(k)))

1
6 .

n

≥

4.3 Computational complexity analysis of fast variants of BCFW

The algorithmic diﬀerence of BCAFW and BCPFW against the baseline BCFW is the additional
procedures of the away-steps and the pairwise-steps. These steps have little eﬀect on the complexity.
, the complexity of the away-steps and that of pairwise-steps is
Deﬁning the cardinality of
|
) because their procedure is equivalent to the subproblem (7). Therefore, the total complexity
|
m,

O
of those algorithms is
and thus, the computational costs of BCAFW and BCPFW are approximately

) at every iteration. Moreover its cardinality satisﬁes 1
|
(n).

(
i
|S

(m +

≤ |S

i as

i
|S

i
|S

| ≤

O

S

i

The complexity of BCFW-GA is worse than that of BCFW because it needs to adaptively
re-compute the probability based on the column-wise duality gap gi(T) at every inner or outer
iteration. We ﬁrst consider only the update of the total duality gap at every outer iteration, i.e.,
epoch. Sampling needs to compute the cumulative sum of its column-wise duality gap, and its
computational cost is
(n(m + n)) at
every epoch, including the calculation of the column-wise duality gap. When the total duality
gap is updated at every iteration, its computational cost is actually equal to those of updating
the column-wise duality gap. Therefore the computation at each outer iteration requires the total
(n(m + n)). In [48, 49], the computation of cumulative sums is improved by use of
complexity
It computes the cumulative sums and the update of
tree structure which is built in
(log n). Therefore, it allows us to compute gap sampling in
the i-th column-wise duality gap in

(n). Therefore, the total complexity of gap sampling is

(n log n).

O

O

O

O

O

(n log n + nm). But, we do not explore this structure in the numerical evaluations.

O

O

5 Numerical evaluations

This section evaluates the performances of the proposed BCFW algorithm and the fast variants
of BCFW. We ﬁrst evaluate convergence behaviors of the proposed baseline algorithm of BCFW
discussed in Section 3. Then, the comparison evaluations among the fast variants of BCFW proposed
in Section 4 are performed. Finally, we evaluate the color-transferred images visually using the
baseline algorithm of BCFW. It should be noted that, hereinafter, this section uses BCFW-U for

12

the baseline algorithm of BCFW unless otherwise stated. We denote the optimal transport matrix of
the unregularized and unrelaxed linear programming in (1) and the obtained matrix of (5) as T∗LP and
T, respectively. The evaluation metrics are deﬁned as explained below. (i) objective function value:
TT 1m
b
a
f (T), (ii) duality gap value: g(T), (iii) marginal constraint error: ec =
+
,
k
−
k
−
k
T∗LPk
T∗LPk
T
(iv) sparsity: the ratio of zero elements in T, (v) transport matrix error: em =
,
/
k
k
and (vi) value error: ev =
. The algorithms are initialized from the
/
|h
i|
same initialization point T(0), of which ﬁrst row is set b. The algorithms are stopped when the
iteration count reaches 1000 epochs unless otherwise stated. We selected the relaxation parameters
λ =
from our preliminary evaluations. All the experiments are
executed on a 3.7 GHz Intel Core i5 PC with 64 GB RAM. Finally, this experiment uses two public
domain images, which are source image “Gangshan District” by Boris Smokrovic, and reference
image, and “Minesota landscape arboretum” by Shannon Kunkle.

T1n
k

T∗LP, C

T∗LP, C

10, 10−

10−
{

9, 10−

7, 10−

8, 10−

T, C

i − h

−

|h

i|

11

}

5.1 Conﬁgurations for color transfer problem

This experiment addresses the OT-based color transfer problem [50], which is an eﬀective application
of the semi-relaxed formulation. We mainly follow the conﬁgurations introduced in [14]. Given two
images that have three dimensions of RGB, we ﬁrst extract image features of the images. The
algorithm of the feature extraction uses the k-means algorithm for image quantization. We used
the litekmeans package. After executing k-means with a predeﬁned number of classes, all the pixels
in the image are assigned into each class. Averaging all the pixel values assigned in each class
yields weight vectors, i.e., centroids. By following the procedure, we obtain m color centroids
x1, x2, . . . , xm
∆m by counting of the assigned
∈
R3 and b
pixels for m classes. Similarly, n color centroids y1, y2, . . . , yn ∈
∆n are obtained.
∈
n
i=1 biδyi. The cost
Finally, the empirical distributions are obtained as α =
n
matrix C in (1) is calculated as Ci,j =
×
by solving the optimization problem using these values, the new i-th color centroid ˆxi is calculated
using the following projection operator:

m
i=1 aiδxi, β =
yjk2. After obtaining a transport matrix T

R3. Additionally, we obtain a color histogram a

xi
k

Rm
+

P

P

−

∈

∈

ˆxi = arg min
x

R3

∈

n

j=1
X

Ti,j

x
k

yjk2 =

−

n
j=1 Ti,jyj
n
j=1 Ti,j

.

P

(12)

Finally, we recover a new color-transformed image by substituting ˆxi into xi.

P

5.2 Evaluations of baseline BCFW

5.2.1 Approximation error and convergence behavior

This subsection evaluates the empirical approximation errors, and the convergence behaviors. The
comparison algorithms are the projected gradient descent (PGD) method and the fast iterative
shrinkage-thresholding algorithm (FISTA) [51] for the semi-relaxed OT problem deﬁned in (5).
As for our proposed algorithms, we used the decay stepsize rule (DEC), i.e., γ = 2/(k + 2) and
γ = 2n/(k + 2n) in FW and BCFW, respectively. The exact line-search stepsize rule (ELS) γLS is
also used. The corresponding FW and BCFW algorithms are denoted as FW-DEC and FW-ELS,
and BCFW-U-DEC and BCFW-U-ELS, respectively.

Comparison across diﬀerent λ. Figure 1 shows comparison results of approximation errors
across diﬀerent λs. From Figure 1(a) and (b), the objective value and the duality gap indicate the
7 because the larger λs cause smaller relaxation term, thus both values
smallest values when λ = 10−
become smaller. Among the algorithm, BCFWs and FISTA give the smallest objective values as

13

108

106

104

n
o
i
t
c
n
u
f

e
v
i
t
c
e
j
b
O

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

108

106

104

102

p
a
g
y
t
i
l
a
u
D

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS

10-2

10-4

10-6

r
o
r
r
e

i

t
n
a
r
t
s
n
o
c

l

i

a
n
g
a
M

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

102

10-11

10-10

10-9

10-8

10-7

100

10-11

10-10

10-9

10-8

10-8

10-11

10-7

tt

10-10

10-9

10-8

10-7

(a) objective value : f (T)

(b) duality gap : g(T)

(c) marginal constraint error : ec

1

0.8

y
t
i
s
r
a
p
S

0.6

0.4

0.2

1.15

1.1

1.05

1

0.95

r
o
r
r
e

x
i
r
t
a
m

t
r
o
p
s
a
r
T

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

0.7

0.6

0.5

0.4

0.3

0.2

0.1

r
o
r
r
e

e
u
l
a
V

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

0
10-11

10-10

10-9

10-8

10-7

0.9
10-11

10-10

10-9

10-8

10-7

0
10-11

10-10

10-9

10-8

10-7

(d) sparsity

(e) matrix error : eM

(f) value error : ev

500

400

]
c
e
s
[

e
m
T

i

300

200

100

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

0
10-11

10-10

10-9

10-8

10-7

(g) computational time

Figure 1: Evaluations on diﬀerent relaxation parameters λ =

10−
{

7, 10−

8, 10−

9, 10−

10, 10−

11

.
}

shown in (a), and BCFW-U-ELS yields the best duality gap as in (b). Regarding the marginal
constraint error ec in (c), both the FW algorithms give the worst performances across all λs. Both
the FW and BCFW algorithms yield stably sparser solutions across diﬀerent λs, and both the BCFW
7 as seen in (d). Similarly, (e) demonstrates
algorithms give the sparsest solutions when λ = 10−
7 give the best transport matrix error em across all the algorithms. From
that the cases with λ = 10−
7 because the
(f), the value errors ev in all the algorithms indicate the smallest values when λ = 10−
smaller λs cause bigger relaxation term, i.e., the second term in (5). Speciﬁcally, both the BCFW-
U-ELS algorithms give the best results. Finally, we can see in (g) that our proposed FW-DEC and
BCFW-U-DEC are roughly 4–5 times faster than PGD and FISTA. Also, both the FW-ELS and
BCFW-U-ELS with the exact line-search rule are slower than those with the decay stepsize rule.
Overall, the proposed BCFW-U-DEC/ELS algorithms stably outperform all the other algorithms
across all λs, and we ﬁnd that they particularly give the best performances when λ = 10−

7.

14

 
 
 
 
 
 
 
 
7, we discuss convergence per-
Convergence behavior. Addressing one speciﬁc case with λ = 10−
formances of the approximation errors. Figure 2 shows the results. The objective value and duality
gap in Figures 2(a)-(d) reveal that the BCFW-U algorithms are faster than the FW algorithm. In
terms of iteration, the exact line-search stepsize rules are superior to the decay stepsize rules in
both the FW and BCFW-U algorithms, but the superiorities of the exact line-search become di-
minished in terms of computational time. Regarding the marginal constraint error ec in (e), FISTA
gives the best performance thanks to the expensive orthogonal projection on the simplex, but both
the BCFW algorithms give similar performances. As for the transport matrix error em in (g) and
(h), the BCFW-U algorithm with the decay stepsize rule gives the best performance. Lastly, it is
understandable that the proposed algorithms stably generate sparser solutions due to the algorithm
architecture.

400

350

300

250

t
s
o
C

200

150

100

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

400

350

300

250

t
s
o
C

200

150

100

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

108

106

104

102

p
a
g
y
t
i
l
a
u
D

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS

0

200

400

600

800

1000

0

100

Iteration

200

300

Time [sec]

400

500

100

0

(a) objective value : f (T)

(b) objective value (time): f (T)

200

400

600

800

1000

Iteration
(c) duality gap : g(T)

108

106

104

102

p
a
g
y
t
i
l
a
u
D

100

0

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS

100

i

r
o
r
r
e
t
n
a
r
t
s
n
o
c

10-1

10-2

l

i

a
n
g
a
M

10-3

10-4

0

50

100

150
Time

200

250

300

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

0.9
0.8
0.7
0.6
0.5

0.4

0.3

0.2

y
t
i
s
r
a
p
S

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

200

400

600

800

1000

0

200

400

600

800

1000

Iteration

(d) duality gap (time) : g(T)

(e) marginal constraint error : ec

1.1

1.05

1

0.95

r
o
r
r
e

x
i
r
t
a
m

t
r
o
p
s
a
r
T

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

1.1

1.05

1

0.95

r
o
r
r
e

x
i
r
t
a
m

t
r
o
p
s
a
r
T

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

0

200

400

600

800

1000

0

100

200

300

400

500

Iteration

Time [sec]

Iteration

(f) sparsity

100

r
o
r
r
e

e
u
l
a
V

10-2

10-4

10-6

0

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

200

400

600

800

1000

Iteration

(g) matrix error : em

(h) matrix error (time) : em

(i) value error : ev

Figure 2: Evaluations on convergence (λ = 10−

7).

15

 
 
 
 
 
 
 
 
 
Table 1: Comparison to smoothed dual and smoothed semi-dual algorithms

algorithms

BCFW-U-DEC

BCFW-U-ELS

Smoothed Dual

Smoothed SemiDual

λ, γ

11

9

7

11

9

7

10−
10−
10−
10−
10−
10−

2

10−
10+0
10+2
2
10−
10+0
10+2

em

ec

time [s]

1.00e+00
9.92e0-01
9.28e–01,

1.13e+00
1.11e+00
9.33e–01,

4.65e+02
4.67e–01
1.24e+00

1.07e+00
1.00e+00
5.82e–01

3.54e–05,
3.75e–05,
3.50e-04

4.61e–05
6.30e–05
3.54e-04

1.25e+00
1.25e+00
3.04e–02

1.24e–02
1.23e+00
2.60e–03

4.58e–01
1.29e+02
1.34e+02

2.68e+02
2.66e+02
2.71e+02

4.75e+02
3.76e+02
3.83e+02

1.08e+03
1.07e+03
1.09e+03

5.2.2 Comparison with smoothed dual algorithms

We further compared with the smoothed dual algorithm (Smoothed Dual) and its semi-dual variant
(Smoothed SemiDual) [14]. Their cost functions are actually diﬀerent from (5), thus, we cannot
directly compare them with ours. Nevertheless, the transport matrix error em and the marginal
constraint error ec would yield valuable insights. Additionally, we measured computational times for
rough comparisons although the author’s code1 is written in Python whereas ours are MATLAB.
The squared 2-norm is used for the smoothed algorithms, and their regularization parameter γ
, which is inside the the range of the original paper. We used the
are set as γ =
}
11, 10−
relaxation parameter λ =
for BCFW-U, which has the same range of orders
10−
{
as γ. Table 1 summarizes em, ec and the computational time. From Table 1, the lowest em is
obtained by Smoothed SemiDual. This is because the smoothed methods regularize the original
LP problem, and tend to output a similar T as TLP. However, our proposed algorithms can give
stabler em, and smaller marginal constraint errors ec. More importantly, it should be emphasized
that our proposed algorithms are extremely faster than the smoothed algorithms although the
Smooth Dual and SemiDual algorithms call internally the L-BFGS-B solver of scipy library, which
is a widely-used, reliable and fast solver.

2, 100, 102

10−
{

9, 10−

}

7

5.3 Evaluations of fast variants of BCFW

5.3.1 Evaluations on BCAFW and BCPFW

This subsection evaluates the performance improvements by two fast variants discussed in Section
4.1, i.e., the BCFW with away-steps (BCAFW) and the BCFW with pairwise-steps (BCPFW).
The experimental conﬁgurations are the same as in Section 5.2. Addressing the exact line-search
7, we discuss the convergence performances of the approximation
stepsize rule (ELS) and λ = 10−
errors. Figure 3 shows the results. The objective value and duality gap in Figures 3(a)-(d) reveal
that both the two fast variants BCAFW and BCPFW yield faster convergence in terms of iteration.

1https://github.com/mblondel/smooth-ot/

16

BCPFW gives much better performances in terms of computational time, as well. As for the matrix
error em and the value error ev, both the two faster variants are worse than the baseline BCFW.
However, these two metrics are evaluated in terms of the solution TLP, i.e., the solution of the
non-relaxed linear programming problem. Thus, this degradation is caused by the diﬀerence of the
two objective functions.

t
s
o
C

100

98

96

94

92

90

88

102

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

100

98

96

94

92

90

88

t
s
o
C

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

102

p
a
g
y
t
i
l
a
u
D

101

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

0

200

400

600

800

1000

0

100

Iteration

200

300

Time [sec]

400

500

100

0

(a) objective value : f (T)

(b) objective value (time): f (T)

200

400

600

800

1000

Iteration
(c) duality gap : g(T)

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

p
a
g
y
t
i
l
a
u
D

101

100

0

100

200

300

400

500

Time

10-4

10
9

8

7

6

5

4

r
o
r
r
e

t
n
i
a
r
t
s
n
o
c

i

l
a
n
g
a
M

3

0

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

y
t
i
s
r
a
p
S

0.995

0.99

0.985

0.98

0.975

200

400

600

800

1000

0

200

400

600

800

1000

Iteration

(d) duality gap (time) : g(T)

(e) marginal constraint error : ec

1.5

1.4

1.3

1.2

1.1

1

r
o
r
r
e

x
i
r
t
a
m

t
r
o
p
s
a
r
T

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

1.5

1.4

1.3

1.2

1.1

1

r
o
r
r
e
x
i
r
t
a
m

t
r
o
p
s
a
r
T

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

100

r
o
r
r
e
e
u
a
V

l

10-2

10-4

Iteration

(f) sparsity

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

0

200

400

600

800

1000

0

100

Iteration

200

300

Time [sec]

400

500

10-6

0

200

400

600

800

1000

Iteration

(g) matrix error : em

(h) matrix error (time) : em

(i) value error : ev

Figure 3: Evaluations on convergence of fast variants of BCFW (BCAFW and BCPFW).

5.3.2 Evaluations on BCFW-GA

This subsections evaluates the eﬀectiveness of the proposed gap-adaptive sampling (BCFW-GA).
We denote the BCFW-GA with diﬀerent M as “BCFW-GAD-
-Mx”. We also consider
DEC,ELS
}
{
the BCFW-GA that does not execute the internal update of the column-wise duality gap gi(T),
but updates them globally at every outer iteration, i.e., n internal iterations. This is denoted
”. The baseline method is BCFW-U. Figure 4 shows the results, of
DEC,ELS
as “BCFW-GAS-
}
{

17

 
 
 
 
 
 
 
 
 
which x-axis is from 0 to 200 for detailed considerations by addressing the beginning of the (outer)
iteration. We set λ = 10−

7.

As for the case with the decay stepsize rule in (a), BCFW-GAD-DEC-M1 give the fastest
convergence in the objective value, and gives more stable dual gap than other BCFW-GAD-DEC-
Mx. Also, BCFW-GAS-DEC yields the wort result. However, with respect to computational time,
this superiority vanishes, and the uniform uniform-sampling BCFW-U-DEC yields the smallest
objective function value.

∈

The gap-adaptive sampling strategy relies on the column-wise duality gap gi(T) for i

[n].
Therefore, when these variances are large with truly correct estimations, we expect that we can
make more signiﬁcant progress by sampling more often the columns with larger sub-optimality
gap. On the other hand, if the variances are small, the performance of the gap-adaptive sampling
method is similar to that of the uniform-sampling method. From this consideration, we present the
variances of the column-wise duality gap gi(T) in each outer iteration in (iv). In order to investigate
the behavior of the variances in more detail, the range of x-axis is set the ﬁrst 50 iterations. From
(iv), the variances of BCFW-GAD-ELS-M1 becomes quickly much smaller than that of BCFW-U.
This reveals that the adaptive-sampling method can reduce drastically the sub-optimality gap at
the beginning of iterations. As for the BCFW-GAD-ELS-M
methods, their variances drop
5,10,20
}
{
to zero values very quickly during M period within a few (1 or 2) iterations, and then go up larger
values at the next M cycle. From these results, after a few iterations, these methods start to sample
nearly uniformly, where any decrease of the sub-optimality gap cannot be gained. Here, it should
be emphasized that these variances are not true ones because the column-wise duality gaps are
outdated and contain accumulated errors until the next global update is performed. Consequently,
the BCFW with the gap-adaptive sampling with the global update is eﬀective for the reduction
of the duality gap with respect to iteration number. However, as pointed out above, it pays a
high price for expensive computation for updating the column-wise duality gap and the weighted
sampling.

When the case with the exact line-search stepsize rule in (b), the observations are slightly
diﬀerent from those above. From (i) and (ii) in (b), the BCFW-GAD-ELS-M1 does not improve any
performances of the uniform-sampling BCFW-U-ELS, rather it is inferior to the uniform sampling
algorithm. Furthermore, BCFW-GAS-ELS, which does not update the column-wise duality gap
in the inner iteration, gives the best performances, but those gains are fairly small. From (iv), its
variance of gi(T) is much smaller than others. In fact, it is not zero, but, is monotonically decreasing.
Therefore, it is understandable that BCFW-GAS-ELS adaptively selects the columns with larger
column-wise duality gap gi(T), and eﬀectively reduces the total duality gap g(T). We also see
similar observations in the BCAFW and BCPFW algorithms in (c) and (d), respectively.
It is,
however, that the improvements cannot be seen with respect to computational time. Consequently,
the exact line-search stepsize rule is eﬀective, and the eﬀectiveness of the gap-adaptive sampling is
fairly limited.

5.4 Evaluations on color-transformed images

This section speciﬁcally evaluates the impact of the relaxation parameter λ in the color transfer
problem. Then, we discuss a suitable setting to obtain the images that are visually natural.

5.4.1 Synthetic image data

In this section, two synthetic images are created, which are the source image (a) and the reference
image (b) as shown in Figure 5. These two images contain three colors, where n = m = 3, and

18

120

115

110

105

100

t
s
o
C

95

90

0

120

115

110

105

100

t
s
o
C

95

90

0

120

115

110

105

100

t
s
o
C

95

90

0

BCFW-U-DEC
BCFW-GAS-DEC
BCFW-GAD-DEC-M1
BCFW-GAD-DEC-M5
BCFW-GAD-DEC-M10
BCFW-GAD-DEC-M20

120

115

110

105

100

95

90

t
s
o
C

BCFW-U-DEC
BCFW-GAS-DEC
BCFW-GAD-DEC-M1
BCFW-GAD-DEC-M5
BCFW-GAD-DEC-M10
BCFW-GAD-DEC-M20

104

103

102

101

p
a
g
y
t
i
l
a
u
D

BCFW-U-DEC
BCFW-GAS-DEC
BCFW-GAD-DEC-M1
BCFW-GAD-DEC-M5
BCFW-GAD-DEC-M10
BCFW-GAD-DEC-M20

10-3

BCFW-U-DEC
BCFW-GAS-DEC
BCFW-GAD-DEC-M1
BCFW-GAD-DEC-M5
BCFW-GAD-DEC-M10
BCFW-GAD-DEC-M20

5

4

3

2

1

p
a
g
y
t
i
l
a
u
d
n
m
u
o
c

l

f
o
e
c
n
a
i
r
a
V

50

100
Iteration

150

200

0

10

20

30
Time [sec]

40

50

60

0

50

100
Iteration

150

200

0

0

10

20

30

Iteration

40

50

(i) objective value : f (T)

(ii) objective value (time)

(iii) duality gap: g(T)

(iv) variance of gi(T)

(a) BCFW-U-DEC and BCFW-GA-DEC

BCFW-U-ELS
BCFW-GAS-ELS
BCFW-GAD-ELS-M1
BCFW-GAD-ELS-M5
BCFW-GAD-ELS-M10
BCFW-GAD-ELS-M20

120

115

110

105

100

95

90

t
s
o
C

BCFW-U-ELS
BCFW-GAS-ELS
BCFW-GAD-ELS-M1
BCFW-GAD-ELS-M5
BCFW-GAD-ELS-M10
BCFW-GAD-ELS-M20

104

103

102

p
a
g
y
t
i
l

a
u
D

101

BCFW-U-ELS
BCFW-GAS-ELS
BCFW-GAD-ELS-M1
BCFW-GAD-ELS-M5
BCFW-GAD-ELS-M10
BCFW-GAD-ELS-M20

10-3

5

4

3

2

1

p
a
g
y
t
i
l
a
u
d
n
m
u
o
c
f
o
e
c
n
a

l

i
r
a
V

BCFW-U-ELS
BCFW-GAS-ELS
BCFW-GAD-ELS-M1
BCFW-GAD-ELS-M5
BCFW-GAD-ELS-M10
BCFW-GAD-ELS-M20

50

100
Iteration

150

200

0

20

40
60
Time [sec]

80

100

0

50

100
Iteration

150

200

0

0

10

20

30

40

50

Iteration

(i) objective value : f (T)

(ii) objective value (time)

(iii) duality gap: g(T)

(iv) variances of gi(T)

(b) BCFW-U-ELS and BCFW-GA-ELS

120

115

110

105

100

t
s
o
C

95

90

0

BCAFW-U-ELS
BCAFW-GAS-ELS
BCAFW-GAD-ELS-M1
BCAFW-GAD-ELS-M5
BCAFW-GAD-ELS-M10
BCAFW-GAD-ELS-M20

50

100
Iteration

150

200

120

115

110

105

100

t
s
o
C

95

90

0

BCAFW-U-ELS
BCAFW-GAS-ELS
BCAFW-GAD-ELS-M1
BCAFW-GAD-ELS-M5
BCAFW-GAD-ELS-M10
BCAFW-GAD-ELS-M20

104

103

102

p
a
g
y
t
i
l

a
u
D

101

BCAFW-U-ELS
BCAFW-GAS-ELS
BCAFW-GAD-ELS-M1
BCAFW-GAD-ELS-M5
BCAFW-GAD-ELS-M10
BCAFW-GAD-ELS-M20

10-3

5

4

3

2

1

p
a
g
y
t
i
l

a
u
d
n
m
u
o
c

l

f
o
e
c
n
a

i
r
a
V

BCAFW-U-ELS
BCAFW-GAS-ELS
BCAFW-GAD-ELS-M1
BCAFW-GAD-ELS-M5
BCAFW-GAD-ELS-M10
BCAFW-GAD-ELS-M20

50

100

Time [sec]

150

0

50

100
Iteration

150

200

0

0

10

20

30

40

50

Iteration

(i) objective value : f (T)

(ii) objective value (time)

(iii) duality gap: g(T)

(iv) variances of gi(T)

(c) BCAFW-U-ELS and BCAFW-GA-ELS

BCPFW-U-ELS
BCPFW-GAS-ELS
BCPFW-GAD-ELS-M1
BCPFW-GAD-ELS-M5
BCPFW-GAD-ELS-M10
BCPFW-GAD-ELS-M20

120

115

110

105

100

95

90

t
s
o
C

BCPFW-U-ELS
BCPFW-GAS-ELS
BCPFW-GAD-ELS-M1
BCPFW-GAD-ELS-M5
BCPFW-GAD-ELS-M10
BCPFW-GAD-ELS-M20

104

103

102

101

p
a
g
y
t
i
l
a
u
D

BCPFW-U-ELS
BCPFW-GAS-ELS
BCPFW-GAD-ELS-M1
BCPFW-GAD-ELS-M5
BCPFW-GAD-ELS-M10
BCPFW-GAD-ELS-M20

10-3

5

4

3

2

1

p
a
g
y
t
i
l
a
u
d
n
m
u
o
c

l

f
o
e
c
n
a
i
r
a
V

BCPFW-U-ELS
BCPFW-GAS-ELS
BCPFW-GAD-ELS-M1
BCPFW-GAD-ELS-M5
BCPFW-GAD-ELS-M10
BCPFW-GAD-ELS-M20

50

100
Iteration

150

200

0

20

40

60
Time [sec]

80

100

120

0

50

100
Iteration

150

200

0

0

10

20

30

40

50

Iteration

(i) objective value : f (T)

(ii) objective value (time)

(iii) duality gap: g(T)

(iv) variances of gi(T)

(d) BCPFW-U-ELS and BCPFW-GA-ELS

Figure 4: Evaluations on duality-adaptive sampling.

19

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
≈

(0.1, 0.3, 0.6)T and b

(0.6, 0.3, 0.1)T . In this setting, BCFW-U does not necessarily sample
a
all the columns at every outer iteration due to the very extremely small m and n. Thus, this poses
diﬃculties to analyze the transitions of the color-transferred images and the transport matrices.
Therefore, we used BCFW-P-DEC insted, which runs a cyclic order on a permuted index at every
outer iteration, i.e., epoch. We compare two cases λ =

≈

3

6, 10−

10−
{

.
}

(a) source

(b) reference

(c) color-transferred image by LP

Figure 5: Source and reference synthetic images, and color-transferred image by LP (n = m = 3).

i

i

a

−

−

2λ k

+ 1

T1n

T, C
h

T, C
h

of the objective function f (T) :=

6) in Figure 6. At the beginning of the iterations, the
Small relaxation parameter (λ = 10−
2
a
2 is mostly ignored in terms
ﬁrst term
k
of optimization as shown in (d), and has little impact on the solution because of small λ. During
this phase, the cost matrix C does not have any impact, and the optimization process proceeds to
reduce the second term 1
2
2. This can be veriﬁed in (d), which presents the objective
k
function value and the norm of gradient of these two separated terms. As seen in (d), it takes a
few hundred of iterations to decrease the function value of the second term. Therefore, if the initial
value of T(0) satisﬁes the constraint of b, i.e., TT 1m = b, the optimization process tends to operate
in the vertical direction of T, i.e., the row direction, in order to satisfy the given distribution of a.
As a result, the ratio of the elements in each row vector moves closer to that of b. More speciﬁcally,
the i-th row vector of T, i.e., (Ti,1, Ti,2, . . . , Ti,n) approaches aibT . In fact, the ratio of (Ti,1, Ti,2, Ti,3)
of the i-th (i

in (b).

T1n

2λ k

) row of T is close to 6 : 3 : 1 at k =
1, 2, 3
}

Here, recalling that the projection operator as in (12), we understand that the new centroids ˆxi
of the i-th image depends on only Ti,j(
[n]), more precisely, the ratio of Ti,j. It should be also
noted that the centroid of the source image does not have any impact on creating ˆxi. Therefore,
because the ratios of all the row vectors get similar each other, the obtained new centroid ˆxi(i
[n])
[n]. This can be veriﬁed that the images at k = 140 in ﬁgure (a)
mostly resemble across all i
consist of a (nearly) single (brown) color. Indeed, this brown color is the b-weighted mixed color of
the three reference colors in Figure 5(b). Furthermore, ﬁgure (c) shows the heat-map of a row-wise
normalized transport matrix, of which each row is normalized such that its sum is equal to 1. As
seen in (c), all the rows at each column at k = 140 have mostly the same colors with similar values,
and these colors are nearly identical to those of b.

140, 1000
}
{

∈ {

∈

∈

∈

∀

j

However, when the iteration proceeds and when 1

2
a
2 relatively decreases compared to
k
is gaining its inﬂuence on the optimization process. Then, the
the ﬁrst term, the ﬁrst term
process proceeds towards the similar result of linear programming (LP). This can be veriﬁed the
image at k = 5

104 in (a).

T, C
h

T1n

2λ k

−

i

3) in Figure 7. This case starts to reduce the ﬁrst
Large relaxation parameter (λ = 10−
and second term simultaneously from the beginning as shown in (d) and (e), and proceeds directly
towards a close solution of LP. From (d), the objective function value and the norm of gradient for
the second term go down quickly, and the values of the ﬁrst term get dominant after only a few

×

20

k = 1

k = 140

k = 103

k = 5 × 104

(a) transition of color-transferred images

T(1)=


0.085 0.075 0.000
0.000 0.220 0.000
0.510 0.000 0.100





0.060 0.034 0.014
0.170 0.093 0.034
0.360 0.170 0.057

T(140)=


(b) transition of transport matrices T(k)

T(103)=


0.055 0.041 0.011
0.190 0.078 0.033
0.360 0.180 0.060







T(5·105)=


0.007 0.100 0.000
0.290 0.000 0.001
0.300 0.300 0.100





heat-map of b

heat-map of b

heat-map of b

heat-map of b

k = 1

k = 140

k = 103

k = 5 × 104

(c) heat-map of row-wise normalized transport matrices of T(k) with b

l

e
u
a
v
n
o
c
n
u
f

i

e
v
i
t
c
e
j
b
O

i

t
n
e
d
a
r
g
f
o
m
r
o
N

Iteration

Iteration

(d) objective value

(e) norm of gradient

Figure 6: Transition of color-transferred images, transport matrices, normalized transport matrices,
6).
objective value, and norm of gradient (λ = 10−

21

 
 
 
 
k = 1

k = 10

k = 30

k = 103

(a) transition of color-transferred images

T(1)=


0.085 0.075 0.000
0.000 0.220 0.000
0.510 0.000 0.100





0.076 0.018 0.000
0.320 0.006 0.022
0.200 0.280 0.083

T(10)=

(b) transition of transport matrices T(k)

T(30)=


0.001 0.080 0.000
0.340 0.000 0.010
0.250 0220 0.095









T(103 )=


0.000 0.072 0.000
0.350 0.000 0.000
0.250 0.230 0.100





heat-map of b

heat-map of b

heat-map of b

heat-map of b

k = 1

k = 10

k = 30

k = 103

(c) heat-map of row-wise normalized transport matrices of T(k) and b

l

e
u
a
v
n
o
c
n
u
f

i

e
v
i
t
c
e
j
b
O

i

t
n
e
d
a
r
g
f
o
m
r
o
N

Iteration

Iteration

(d) objective value

(e) norm of gradient

Figure 7: Transition of color-transferred images, transport matrices, normalized transport matrices,
3).
objective value, and norm of gradient (λ = 10−

22

 
 
 
 
iterations. We see that the image at k = 1000 is quite similar to that of LP in Figure 5(c). Also,
6. This is due to
this result reveals that its convergence is much faster than the case with λ = 10−
that the BCFW algorithm requires a large number of iterations when small λ because of the large
penalty term.

5.4.2 Real-world image data

We continue to discuss the impact of λ on the color-transferred image using real-world images. The
two images used here are shown in Figures 8(a) and (b), respectively. The color-transferred image
generated by linear programming (LP) for the original OT problem is also shown in (c).

(a) source

(b) reference

(c) color-transferred image by LP

Figure 8: Source and reference real-world images, and color-transferred image by LP.

We evaluated two sets, i.e., m = n = 32 and m = n = 256. The baseline BCFW with the decay
stepsize rule, BCFW-U-DEC, is used. The results of the former and the latter are shown in Figures
9 and 10, respectively.

9 in Figure 9(a), the same situation
Small size m = n = 32 in Figure 9. As for the case λ = 10−
happens at k = 3300 as the previous experiment using synthetic images. This single color is close
to the b-weighed averaged color of the centroids of the reference image. Next, at k = 105, the
single color is scattered and a naturally color-transferred images are produced. However, when the
optimization process proceeds further, it can be seen that the image at k = 106 contains artiﬁcial
gray or blue pixels in the background. Figure 9(b) illustrates the heat-map of the obtained transport
matrices at the k-th iteration, T(k). As the result of T(3000) at k = 3000, the matrix contains similar
values along each column, in other words, the vertical blocks with similar colors can be recognized.
However, this vertical structure disappears as the iteration proceeds. On the other hand, when
9. In addition, we cannot see
λ = 10−
such vertical structures and the single-colored image through the entire of iterations.

6, the convergence is extremely faster than that of λ = 10−

Large size m = n = 256 in Figure 10. Although the heat-map of the normalized transferred
matrices as in Figure 9(b) cannot be illustrated due to too small value of each pixels, we ﬁnd the
same situation in the color-transferred images as the case with m = n = 32. Additionally, the
6, 104) looks quite similar to that of LP, which
ﬁnally obtained image at k = 200 when (λ, k) = (10−
6
is shown in Figure 8(c). Therefore, it is reasonably understandable that the weight by λ = 10−
has similar impacts on the constraint of T1n = a in the non-relaxed original OT problem.

5.4.3 Discussion

From these observations, when the relaxation parameter λ is quite large, BCFW for the semi-
relaxed OT problem converges faster. However, it pays a high price on dreadful artifacts on the

23

k = 1

k = 3300

k = 105

k = 106

(a) transition of color-transferred images (λ = 10−9)

k = 106
k = 1
(b) transition of heat-map of row-wise normalized transport matrices T(k) (λ = 10−9)

k = 3000

k = 105

k = 1

k = 100

k = 103

k = 104

(c) transition of color-transferred images (λ = 10−6)

k = 104
k = 1
(d) transition of heat-map of row-wise normalized transport matrices T(k) (λ = 10−6)

k = 103

k = 100

Figure 9: Transition of color-transferred images and heat-map of row-wise normalized transport
matrices (m = n = 32).

24

k = 1

k = 200

k = 104

k = 105

(a) transition of color-transferred images (λ = 10−9)

e
u
l
a
v
n
o
i
c
n
u
f

e
v
i
t
c
e
b
O

j

t
n
e
i
d
a
r
g
f
o
m
r
o
N

Iteration

Iteration

(b) objective value (λ = 10−9)

(c) norm of gradient (λ = 10−9)

k = 1

k = 10

k = 103

k = 104

(d) transition of color-transferred images (λ = 10−6)

e
u
l
a
v
n
o
i
c
n
u
f

e
v
i
t
c
e
j
b
O

t
n
e
i
d
a
r
g
f
o
m
r
o
N

Iteration

Iteration

(e) function value (λ = 10−6)

(g) norm of gradient (λ = 10−6)

Figure 10: Transition of color-transferred images and objective value and norm of gradient (m =
n = 256).

25

 
 
 
 
 
 
 
 
color-transferred images. On the other hand, when using relatively small λ, BCFW can produce
a natural image and avoids heavy artifacts because BCFW for the semi-relaxed OT problem does
not necessarily transfer all the probability mass of the source image to the reference while keeping
its solution closer to the original one. Nevertheless, it should be emphasized that it admits a slow
convergence rate, and, more importantly, we will get similar images as LP if we further proceed its
optimization process. Thus, it is perforable to terminate before its complete convergence.

6 Conclusion

We have proposed in this paper faster block-coordinate Frank-Wolfe algorithms for a convex semi-
relaxed optimal transport problem. The main contributions are that we proved their upper-bounds
of the worst convergence iterations, and the equivalence between the linearization duality gap and
the Lagrangian duality gap. Three variants have been also proposed. Numerical evaluations demon-
strated that the proposed algorithms outperform state-of-the-art algorithms across diﬀerent settings.

26

Supplementary

This supplementary material presents the details of the proposed algorithms, the complete proof
of the convergence analysis in the main material, additional theoretical results, and additional
experiments. The structure is as follows:

• Section A:

The complete algorithms of FW, BCAFW, BCPFW and BCFW-GA.

• Sections B, C, D and E:

The complete proof of the convergence analysis in the main material, and additional theoretical
results.

• Section F:

Additional experiments for the main material.

It should be ﬁrst noted that, throughout this supplementary material, we dare to include a
( 8
λǫ ) as in Theorem B.2, to discuss

constant factor in the order of total complexity, for example,
and compare obtained results in detail.

O

A Algorithms

This section summarizes the algorithms of FW, BCAFW, BCPFW and BCFW-GA in Algorithms
A.1, A.2, A.3, and A.4, respectively.

Algorithm A.1 Frank-Wolfe (FW) algorithm for semi-relaxed OT
Require: T(0)
b1∆m
1: for k = 0 . . . K do
2:

for i = 0 . . . n do

× · · · ×

bn∆m

∈

3:

4:

5:

6:

7:
8:

ek,

if (T(k))
i

∇

[m] h
∈

Compute si = bi arg min

∆m,k

ek∈

end for
if g(T(k))
break

≤

ǫ then

end if
Compute stepsize γ as

γ =

γLS,
2
k + 2

,




(line

−

search in (A.1))

(decay rule)

Update T(k+1) = (1

9:
10: end for


γ)T(k) + γS

−

In the algorithm, the optimal stepsize γLS by the line-search is given as

γLS =

λ

T(k)
h

−

S, C

T(k)1n
F +
i
h
T(k)1n
k

−

−
S1n

S1n, T(k)1n
2
k

a

i

.

−

(A.1)

27

Algorithm A.2 Block-Coordinate Away-step Frank-Wolfe (BCAFW) for semi-relaxed OT
Require: T(0) = (t(0)
1: for k = 0 . . . K do
2:

2 , . . . , t(0)
n )

1 , t(0)

e1}
,
{

[n] randomly

× · · · ×

bn∆m

b1∆m

b2∆m

i =

[n].

×

∈

∈

S

∀

i

⊲ Line 2 in Algorithm 1

if (T(k))
i

∇

e,

f (T(k)), dAwayi

then

3:

4:

5:

6:

7:
8:

9:

10:

11:

12:

13:

∈

Select index i
Compute si
Set dFW = si
Compute vi = bi arg max

t(k)
i

−

ej

i,j

[m]h
∈

Set dAway = t(k)
if

∈S
vi
i −
f (T(k)), dFWi ≥ h−∇

h−∇
d = dFW, γmax = 1

else

d = dAway, γmax =

1

αvi

αvi

−

end if
Compute stepsize γ = γLS using d
Update t(k+1)
j
j
(k+1)
Update
i

[n]
∆m : α(t+1)

∀
=

e

∈
e
{

∈

S

14:
15: end for

⊲ (10)
⊲ Line 5 in Algorithm 1

> 0
}

Algorithm A.3 Block-Coordinate Pairwise step Frank-Wolfe (BCPFW) for semi-relaxed OT
Require: T(0) = (t(0)
1: for k = 0 . . . K do
2:

2 , . . . , t(0)
n )

1 , t(0)

e1}
,
{

[n] randomly

× · · · ×

bn∆m

b1∆m

b2∆m

i =

[n].

×

∈

∈

S

∀

i

3:

4:

5:

6:

7:

∈

Select index i
Compute si
Compute vi
Set d = dPair = si
Compute stepsize γ = γLS
Update t(k+1)
j
(k+1)
Update
i

[n]

−

∈

∀

j

8:
9: end for

S

vi and γmax = αvi

⊲ Line 2 in Algorithm 1
⊲ Line 5 in Algorithm A.2

⊲ Line 12 in Algorithm A.2
⊲ Line 5 in Algorithm 1

⊲ Line 14 in Algorithm A.2

28

Algorithm A.4 Block-coordinate Frank-Wolfe with gap-adaptive sampling (BCFW-GA) for semi-
relaxed OT
Require: T(0) = (t(0)
is a large constant.
1: for k = 0 . . . K do
2:

bn∆m, gi(T) = C,

2 , . . . , t(0)
n )

[n], where C

1 , t(0)

× · · · ×

b1∆m

b2∆m

×

∈

∈

∀

i

∈

[n] randomly with probability

Select index i
Compute si
Compute stepsize γ
Update t(k+1)
Update and store the duality gap for the i-th column gi(T) in (11).
if k satisﬁes the global update condition then

gi(T)

[n]

∝

∈

∀

j

,

j

⊲ Line 3 in Algorithm 1
⊲ Line 4 in Algorithm 1
⊲ Line 5 in Algorithm 1

for i = 1 . . . n do
Compute si
Update and store the duality gap for the i-th column gi(T) in (11).

⊲ Line 3 in Algorithm 1

end for

3:

4:

5:

6:

7:

8:

9:

10:

11:

end if

12:
13: end for

B Convergence analysis of the proposed FW algorithm for

semi-relaxed OT problem

We ﬁrst deﬁne the curvature constant Cf for the semi-relaxed OT problem.

Deﬁnition B.1 (Curvature constant of FW algorithm for semi-relaxed OT problem).

The curvature constant on the domain

of the semi-relaxed OT problem is deﬁned as

Cf :=

sup

T, S
,
∈ M
[0, 1],
γ
∈
Y = T + γ(S

−

M
2
γ2 (f (Y)
T)

f (T)

Y

− h

T,

f (T)
),
i

∇

−

−

where

M

represents b1∆m

b2∆m

×

× · · · ×

bn∆m.

We then drive the convergence analysis of the FW algorithm.

Theorem B.2. Let T∗ is the optimal solution of the semi-relaxed OT problem. Consider Algorithm
k+2. Then, we have f (T(k))
A.1 with a decay stepsize rule γ = 2
1, where Cf
4
the curvature constant with Cf
λ . Additionally, given a constant ǫ, this means that Algorithm
A.1 requires at most

2Cf
k+2 for k

( 8
λǫ ) for its convergence.

f (T∗)

−

≤

≥

≤

O

To prove the theorem above, we ﬁrst state the following theorem in the classical FW algorithm.

Theorem B.3 (Convergence analysis for a general FW algorithm [27, 38]).

The curvature constant on the domain

Cf :=

sup

x, s
γ

,
∈ M
[0, 1],
∈
y = x + γ(s

−

x)

M
2
γ2 (f (y)

is deﬁned as

f (x)

y

− h

x,

f (x)
),
i

∇

−

−

where y is equal to x + γ(s
−
Frank-Wolfe algorithm satisﬁes

x) for γ

∈

f (x(k))

[0, 1]. Then, for each k

≥

1, the iterate x(k) of the

f (x∗)

−

2Cf
k + 2

,

≤

29

(A.2)

where x∗ is the optimal solution.

We also give the following lemma.

Lemma B.4. The upper-bound of squared ℓ2-norm
m
i=1 xi satisﬁes

Proof. The square of

x
k

2
2 is 1 on a domain ∆m.
k

P

x satisﬁes

m
i=1 xi = 1 and

P

P

where we note xixj

0.

This complete the proof.

≥

m

2

m

!

i=1
X
i is equal to

i=1
X
n
i=1 x2
2
2 ≤
k

x
k

xi

=

x2
i + 2

xixj.

i<j
X1
≤
≤

m

x
k

2
2. Hence, squared ℓ2-norm is bounded as
k

12

2

−

i<j
X1
≤
≤

m

xixj

1,

≤

We now provide the complete proof of Theorem B.2.

Proof. Since the objective of the semi-relaxed problem is twice diﬀerentiable, the Taylor expansion
at γ = 0 of f (T + γ(S

T)) is

−

f (T + γ(S

−

T)) = f (T) + γ(S

T)T

∇

−

f (T) +

1
2

γ2(T

S)T

∇

−

2f (T′)(S

T),

−

(A.3)

where T′ satisﬁes T′

[T, T + γ(S

T)] [37].

∈

−

Substituting (A.3) to (B.1) gives the following formula transformation.

Cf =

≤

≤

sup

T, S
,
∈ M
γ
[0, 1],
∈
Y = T + γ(S

sup

T, S
,
∈ M
[0, 1],
γ
∈
Y = T + γ(S

−

−

sup

T, S
,
∈ M
[T, T + γ(S

T′

∈

For arbitrary X

∈

2
γ2 (f (Y)

−

f (T)

Y

− h

T,

f (T)
)
i

∇

−

2
γ2 (γ(S

−

T)T

∇

f (T) +

1
2

γ2(S

T)T

∇

−

2f (T′)(S

T)

−

γ(S

− h

T),

f (T)
)
i

∇

−

T)

T)

T)T

(S

−

∇

2f (T′)(S

T).

−

T)]

−
n, the Hessian matrix of f (X) is

Rm

×

2f (X) =

∇

Em
...
Em

1
λ 



· · ·
...

Em
...
Em



(
∈

Rmn

mn),

×

(A.4)

where Em is the identity matrix of size m
stacks all column vectors of a matrix X = (x1, x2, . . . , xn)
n )T
column vector (xT

· · ·
m. Here, we denote a vectorization operation that
Rm, to make a

Rm
Rmn as Vec(X). Then, the Hessian matrix in (A.4) satisﬁes

n, where xi

2 , . . . , xT

1 , xT

×

∈

∈

×




∈

Vec(X)T

∇

2f (X)Vec(X) = (xT

1 , xT

2 , . . . , xT

n ) 




Em
...
Em

· · ·
...

· · ·

Em
...
Em

x1
x2
...
xn




















=

X1n
k

2
k

≥

0.

30

 
Thus, the Hessian matrix (A.4) is positive semi-deﬁnite. From this fact, the curvature constant Cf
is obtained as

Cf

≤

sup

T,S

∈M

1
λ k

S1n

T1n

2
k

−

(A.5)

where

= b1∆m

b2∆m

×

M

× · · · ×

bn∆m. Consequently, the inequality (A.5) is represented as

Cf

≤

≤

sup

T,S

∈M

4
λ

si

1
λ k

S1n

T1n

2
k

≤

2

−

n

1
λ k

2S1n

sup
S

∈M

2
k

2

sup
i
bi,
∀

∈

∈

[n]  

si
k

k!

≤

i=1
X

4
λ  

n

i=1
X

bi

≤

!

4
λ

12

≤

4
λ

,

4
λ

si

≤

sup
i
bi,
∀

∈

[n] k
∈

n

i=1
X

si

2
k

where the ﬁfth inequality used Lemma B.4.

Finally, given a constant ǫ, considering (A.2) in Theorem B.3, the FW algorithm for the

semi-relaxed OT problem converges if it satisﬁes the following inequality.

2Cf
k + 2 ⇐⇒

ǫ

≥

2
k + 2 ·

4
λ ⇐⇒

ǫ

≥

k + 2

2

·

≥

4
ǫλ

.

Hence, the FW algorithm in Algorithm A.1 requires at most

This completes the proof.

( 8
ǫλ ) iterations.

O

C Proof of Theorem 3.2

It is known that the convergence analysis of a general BCFW algorithm is presented as below.

Theorem C.1 (Convergence analysis of a general BCFW algorithm [27]).

For each k

≥

1, the iterates xk of BCFW Algorithm satisﬁes

E[f (x(k))]

f (x∗)

−

2n
k + 2n

≤

(C ⊗f + h0),

(A.6)

where x∗ is the optimal solution, and h0 is f (x(0))

f (x∗). Here, C ⊗f :=

−

n

i=1 C (i)

f , where

C (i)
f

:=

sup
x∈M,si∈Mi,

γ

[0,1],

y=x+γ(s[i]−x[i])

∈

2
γ2 (f (y)

−

f (x)

yi −

− h

xi,

∇

P
if (x)
),
i

and x[i] refers to the zero-padding of xi.

The following gives the complete proof of Theorem 3.2 in the main material.

Proof. We ﬁrst bound the block curvature constant C ⊗f in Deﬁnition 3.1.

When f is twice diﬀerentiable, we can transform the block curvature constant as well as the

curvature constant of FW (A.5). Then, C (i)
f

satisﬁes

C (i)

f ≤

sup

ti,si

bi∆m

∈

1
λ k

si

ti

2.
k

−

31

This can be rearranged by Lemma B.4 as

C (i)

f ≤

sup

ti,si

bi∆m

∈

1
λ k

si

ti

2
k

−

≤

si

sup
bi∆m

∈

1
λ k

2si

2
k

≤

4
λ

s
sup
bi∆m k
∈

si

2
k

≤

4
λ

b2
i .

Consequently, the curvature constant C ⊗f of the entire domain satisﬁes

C ⊗f =

n

i=1
X

C (i)

f ≤

n

i=1
X

4b2
i
λ ≤

4
λ

,

(A.7)

because bi is in the probability simplex and we used Lemma B.4 .

We now prove the upper-bound of the linearization duality gap g(T) at the initial point k = 0 by
considering the discussion above. We deﬁne the initial matrix T(0) as T(0) = (b1e1, . . . , bie1, . . . , bne1),
Rm is one of the extreme points and the j-th element is 1 otherwise 0. h0 satisﬁes
where ej
∈
h0 = f (T(0))

g(T(0)). Thus, we bound g(T(0)) as the following.

f (T∗)

−
g(T(0)) =

≤

S,

T(0)
−
h
t(0)
1 −
...



∇
s1





























n

si

t(0)
i −
...

sn

si

t(0)
sn
n
−
b1e1 −
s1
...
bie1 −
...
bne1 −
b1e1
...
bie1
...
bne1








biC1i, +



·



f (T(0))
i
c1
...
ci
...
cn



·


















+

1
λ






















c1
...
ci
...
cn

+















·

















c1
...
ci
...
cn
n










1
λ





+

1
λ















bie1, e1 −
h
i=1
X
n

n



1
λ








b1e1 −
...
bie1 −
...
bne1 −
1
a
λ

i −

s1

si

t(0)
1 −
...

t(0)
i −
...

t(0)
sn
n
−
b1e1 −
...
bie1 −
...
bne1 −
s1











s1

·













si

sn

T(0)1n
...
T(0)1n
...
T(0)1n
e1 −
...
e1 −
...
e1 −
a











−

−

−
a

a

a

a

a

a













·















e1 −
...
e1 −
...
e1 −
a

i





si

·

sn
n















si, e1 −
h
i=1
X

n

a

a











bi

e1, a
h

i −

i=1
X

1
λ h

si, e1i

n

+

i=1
X

1
λ h

si, a

i

i=1
X
n

i=1
X
n

i=1
X
n

i=1
X

1
λ

bi

−

n

i=1
X
si, a
h
i=1
X

i

biC1,i +

biC1,i +

biC1,i +

1
λ

1
λ

1
λ

i=1
X
1
λ

+

+

1
λ

.

=

=

≤

=

=

≤

≤

b is in the probability simplex and its extreme points are e1, . . . , en. Therefore,
C1,1, . . . , C1,i, . . . , C1,n)

C

.

≤ k

k∞

32

(A.8)

n
i=1 biC1,i

≤

P

2

If

C
k

8
λ holds. Given an approximation precision constant ǫ, considering
(A.6) in Theorem C.1, the BCFW algorithm for the semi-relaxed OT problem converges if it
satisﬁes the following inequality.

λ , C ⊗f + h0 ≤

k∞ ≤

2n
k + 2n ·

8
λ ⇐⇒

ǫ

≥

16n
λǫ −

k

≥

2n

16n
λǫ

.

k

≥

⇐⇒

Hence, BCFW requires at most

On the other hand, If

C
k

k∞

BCFW requires additional iterations
This derives the desired results.

O
> 2

( 16n
λǫ ) iterations.
λ , h0 satisﬁes h0 + C ⊗f ≤
).
O

C
k
ǫ

( 2n

∞

k

h0 + 4

λ ≤ k

C

+ 2

λ + 4

λ . Thereby,

k∞

D Proof of Theorem 3.3

This section gives the complete proof of Theorem 3.3.

Proof. The proof strategy follows that of [37]. We ﬁrst consider the Lagrangian dual. The La-
grangian function L(T, Λ, µ) is deﬁned as

n

m

L(T, Λ, µ) = f (T)

Λ, T

− h

+

i

µi

i=1
X





j=1
X

tj,i

bi

,



−



0 and µ are dual variables. Then, the Lagrangian dual function is

where Λ = (λ1, λ2, . . . , λn)
deﬁned as

≥

g(Λ, µ) = inf
T

L(T, Λ, µ).

The Lagrangian function L is also convex because of convexity of f . The gradient of Lagrangian at
T is provided as

TL(T, Λ, µ) =

∇

f (T)

∇

−

(λ1, λ2, . . . , λn)T + (µ11m, . . . µn1m)T .

Considering the case that the gradient (A.9) is equal to 0, Λ satisﬁes λi =
tuting Λ into the Lagrangian dual function yeilds

∇

if (T) + µi1m. Substi-

g(T, µ) = f (T)

n

−

h∇

i=1
X

if (T), ti

n

i −

i=1
X

µibi.

Thus, the Lagrangian dual problem, denoted as w(T), is deﬁned as

w(T)

:= max
0
Λ

≥

g(Λ, µ).

This is written as

T

max
Rm×n,µ

Rn

∈

∈
subject to

f (T)

µi

−

≤

n

−

ti,
h
i=1
X
(
min
∇
[m]
j
∈

if (T)

i −

∇

if (T))j,

n

µibi

i=1
X
i
∀

∈

[n].

33

Denoting

µi(T) as minj

[m](
∈

∇

−

if (T))j, the Lagrangian dual problem is derived as

T

max
Rm×n
∈

f (T)

n

−

ti,
h
i=1
X

if (T)
i

∇

+

n

i=1
X

(
bi min
∇
[m]
j
∈

if (T))j.

Now, denoting the Lagrangian gap as gL(T), the Lagrange duality gap is represented as

gL(T) = f (T)

w(T)

=

=

n

−
ti,
h
i=1
X
n

∇

if (T)

i −

n

i=1
X
n

(
bi min
∇
[m]
j
∈

if (T))j

ti,
h
i=1
X
n

∇

if (T)

i −

si

i=1
X

si,

min
∈

bi∆mh

if (T)
i

∇

=

si

max
i=1
∈
X
T
= max
h
S′
∈M
= g(T).

−

bi∆mh

ti

si,

if (T)
i

∇

−

S,

f (T)
i

∇

As seen, we conﬁrm that the Lagrange duality gL(T) is equal to the mineralization duality gap
g(T).

This completes the proof.

E Proof of Theorem 4.1

We give a convergence analysis Theorem 4.1 of BCFW-GA, which is a straightforward extension
to the semi-relaxed OT problem from that of the structured SVM problem in [42]). For this purpose,
the following is given:

Deﬁnition E.1 (Deﬁnition 5 in [42]). The non-uniformity measure χ(x) of a vector x
deﬁned as χ(x) =
x.

n is
is the probability vector obtained by normalizing

1 + n2Var[p], where p = x
x
1
k
k

∈

R+

p

In addition, the non-uniformity measure holds as follows.

Lemma E.2. (Lemma 6 in [42] and its remark) For x
fore, the non-uniformity measure χ(x) satisﬁes χ(x) is in [1, √n].

∈

Rn

+, we have

k2 = χ(x)
√n k

x

x
k

k1. There-

Then, the following theorem is given by extending Theorem 2 in [42].

Theorem E.3. (Convergence analysis of Algorithm A.4 [42]) Let T∗ be the optimal solution of the
)T , where C (i)
semi-relaxed OT problem. Consider Algorithm A.4. Let C (:)
f
f
is deﬁned in Deﬁnition 3.1. Also, denote g:(T(k)) as (g1, . . . , gn)T as deﬁned in (11). Then, for
each k

1, the iterates of T(k) of Algorithm A.4 satisﬁes

f , . . . , C (n)

be (C (1)

, C (2)

f

f

≥

E[f (T(k))]

f (T∗)

−

2n
k + 2n

≤

(C ⊗f χ⊗ + h0),

(A.9)

where the constant χ⊗ is an upper bound on E[

]. Here, the expectation is taken with

respect to the random selection of the blocks at the k-th iteration of the algorithm.

χ(C (:)
f )
χ(g:(T(k)))3

34

Proof. The proof is identical to that of Theorem 2 in [42]. Thus, we omit it.

We are ready to prove Theorem 4.1 of Algorithm A.4 using Theorem E.3 and Lemma E.2.

Proof. The proof mostly follows that of Theorem 3.2.
g(T(0))
4

We ﬁrst recall C ⊗f ≤

also note the range of χ(x) for x
corresponds to the uniform case whereas χ(x)

≈
Then, considering χ⊗ is an upper bound on E[

λ in (A.7), and h0 ≤
∈

+ 2
+ as in Lemma E.2, i.e., 1

λ in (A.8). Furthermore, we
√n. χ(x) = 1
√n happens when the distribution is non-uniform.

χ(x)

≤ k

k∞

Rn

C

≤

≤

], we have the best case below when

χ(C (:)
f )
χ(g:(T(k)))3
√n for the non-uniform case as

χ(C (:)

f ) = 1 and χ(g:(T(k)))

≈

χ⊗

≤

1
(√n)3 .

On the other hand, we have the worst case below when χ(C (:)
uniform case

f ) = √n and χ(g:(T(k))) = 1 for the

Therefore, if

C
k

k∞ ≤

2
λ , we have

χ⊗

≤

√n.

C ⊗f χ⊗ + h0 =

4
λ

1
(√n)3 +

2
λ

+

2
λ

=

4
λ

1
n√n

(cid:18)

(cid:19)

(cid:18)

+ 1

,

(cid:19)

and

C ⊗f χ⊗ + h0 =

4
λ

√n +

2
λ

+

2
λ

=

4
λ

(√n + 1).

(cid:18)

(cid:19)

Consequently, from (A.9), Algorithm A.4 satisﬁes in the best case

2n
k + 2n ·

4
λ

ǫ

≥

1
n√n

(cid:18)

+ 1

(cid:19)

k

≥

⇐⇒

8n
λǫ

+

8

ǫλ√n −

2n

approx.
⇐⇒

k

≥

8n
λǫ

+

8
ǫλ√n

approx.
⇐⇒

k

≥

8n
λǫ

,(A.10)

)
(
∗
| {z }

k

+

8n
λǫ

8n√n
λǫ
≥
+ 2
λ . Therefore, we have

approx.
⇐⇒

k

8n√n
λǫ

.

≥

and also has in the worst case
4
λ

2n
k + 2n ·

(√n + 1)

⇐⇒

≥

ǫ

k

On the other hand, if

C
k

k∞

and

8n√n

8n
λǫ

+

≥
> 2
λ , h0 is bounded on

ǫλ −

2n

approx.
⇐⇒

C ⊗f χ⊗ + h0 =

4
λ

C ⊗f χ⊗ + h0 =

k∞

C
k
1
(√n)3 +

C
k

k∞

+

(cid:18)

4
λ

√n +

C
k

k∞

+

2
λ

2
λ

,

(cid:19)

.

(cid:19)
Similarly, from (A.10), Algorithm A.4 also satisﬁes in the best case

(cid:18)

and also has in the worst case

This completes the proof.

4n
ǫλ

+

2n

C
k
ǫ

k∞

k

≥

8n√n
ǫλ

+

2n

C
k
ǫ

k

≥

k∞

.

35

F Additional numerical evaluations

This section additionally presents numerical results. We used two publicly available images “Graﬁti”
by Jon Ander and “Rainbow Bridge National Monument Utah” by Bernard Sprangg. All the
conﬁgurations of the experiments are the same as those in Section 5.

108

n
o
i
t
c
n
u
f

e
v
i
t
c
e
j
b
O

106

104

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

1010

108

106

104

102

p
a
g
y
t
i
l
a
u
D

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS

10-2

10-4

10-6

r
o
r
r
e

i

t
n
a
r
t
s
n
o
c

l

i

a
n
g
a
M

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

102

10-11

10-10

10-9

10-8

10-7

100

10-11

10-10

10-9

10-8

10-7

10-8

10-11

10-10

10-9

10-8

10-7

(a) objective value : f (T)

(b) duality gap : g(T)

(c) marginal constraint error : ec

1

0.8

0.6

0.4

0.2

y
t
i
s
r
a
p
S

1.2

1.15

1.1

1.05

1

0.95

r
o
r
r
e

x
i
r
t
a
m

t
r
o
p
s
a
r
T

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

1

0.8

0.6

0.4

0.2

r
o
r
r
e

e
u
l
a
V

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

0
10-11

10-10

10-9

10-8

10-7

0.9
10-11

10-10

10-9

10-8

10-7

0
10-11

10-10

10-9

10-8

10-7

(d) sparsity

(e) matrix error : eM

(f) value error : ev

500

400

300

200

100

]
c
e
s
[

e
m
T

i

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

0
10-11

10-10

10-9

10-8

10-7

(g) computational time

Figure A.1: Evaluations on diﬀerent relaxation parameters λ (corresponding to Figure 1).

36

 
 
 
 
 
 
 
 
400

350

300

250

t
s
o
C

200

150

100

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

400

350

300

250

t
s
o
C

200

150

100

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

108

106

104

102

p
a
g
y
t
i
l
a
u
D

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS

0

200

400

600

800

1000

0

100

Iteration

200

300

Time [sec]

400

500

100

0

(a) objective value : f (T)

(b) objective value (time): f (T)

200

400

600

800

1000

Iteration
(c) duality gap : g(T)

108

106

104

102

p
a
g
y
t
i
l
a
u
D

100

0

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS

100

r
o
r
r
e

i

t
n
a
r
t
s
n
o
c

10-1

10-2

l

i

a
n
g
a
M

10-3

10-4

0

50

100

150
Time

200

250

300

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

100

y
t
i

s
r
a
p
S

10-1

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

200

400

600

800

1000

0

200

400

600

800

1000

Iteration

(d) duality gap (time) : g(T)

(e) marginal constraint error : ec

1.7

1.6

1.5

1.4

1.3

1.2

1.1

1

r
o
r
r
e

x

i
r
t
a
m

t
r
o
p
s
a
r
T

0.9

0

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

200

400

600

800

1000

Iteration

1.7

1.6

1.5

1.4

1.3

1.2

1.1

1

r
o
r
r
e
x

i
r
t
a
m

t
r
o
p
s
a
r
T

0.9

0

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

100

200

300

400

500

Time [sec]

Iteration

(f) sparsity

100

r
o
r
r
e
e
u
a
V

l

10-2

10-4

10-6

0

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

200

400

600

800

1000

Iteration

(g) matrix error : em

(h) matrix error (time) : em

(i) value error : ev

Figure A.2: Evaluations on convergence (λ = 10−

7) (corresponding to Figure 2).

37

 
 
 
 
 
 
 
 
 
t
s
o
C

100

98

96

94

92

90

88

86

84

82

102

p
a
g
y
t
i
l
a
u
D

101

100

0

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

t
s
o
C

100

98

96

94

92

90

88

86

84

82

0

200

400

600

800

1000

Iteration

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

0

100

200

300
Time [sec]

400

500

600

(a) objective value : f (T)

(b) objective value (time): f (T)

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

10-4

10
9

8

7

6

5

4

r
o
r
r
e
t
n
i
a
r
t
s
n
o
c

i

l
a
n
g
a
M

100

200

300
Time

400

500

600

3

0

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

200

400

600

800

1000

Iteration
(c) duality gap : g(T)

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

102

p
a
g
y
t
i
l
a
u
D

101

100

0

y
t
i

s
r
a
p
S

0.995

0.99

0.985

0.98

0.975

200

400

600

800

1000

0

200

400

600

800

1000

Iteration

(d) duality gap (time) : g(T)

(e) marginal constraint error : ec

1.5

1.4

1.3

1.2

1.1

1

r
o
r
r
e

x

i
r
t
a
m

t
r
o
p
s
a
r
T

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

1.5

1.4

1.3

1.2

1.1

1

r
o
r
r
e
x

i
r
t
a
m

t
r
o
p
s
a
r
T

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

100

r
o
r
r
e
e
u
a
V

l

10-2

10-4

10-6

0

200

400

600

800

1000

0

100

200

Iteration

300
Time [sec]

400

500

600

10-8

0

Iteration

(f) sparsity

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

200

400

600

800

1000

Iteration

(g) matrix error : em

(h) matrix error (time) : em

(i) value error : ev

Figure A.3: Evaluations on convergence of away-steps and pairwise-steps algorithms (corresponding
to Figure 3).

38

 
 
 
 
 
 
 
 
 
120

115

110

105

100

95

90

85

t
s
o
C

BCFW-U-DEC
BCFW-GAS-DEC
BCFW-GAD-DEC-M1
BCFW-GAD-DEC-M5
BCFW-GAD-DEC-M10
BCFW-GAD-DEC-M20

120

115

110

105

100

95

90

85

t
s
o
C

BCFW-U-DEC
BCFW-GAS-DEC
BCFW-GAD-DEC-M1
BCFW-GAD-DEC-M5
BCFW-GAD-DEC-M10
BCFW-GAD-DEC-M20

104

103

102

101

p
a
g
y
t
i
l
a
u
D

BCFW-U-DEC
BCFW-GAS-DEC
BCFW-GAD-DEC-M1
BCFW-GAD-DEC-M5
BCFW-GAD-DEC-M10
BCFW-GAD-DEC-M20

10-4

2

BCFW-U-DEC
BCFW-GAS-DEC
BCFW-GAD-DEC-M1
BCFW-GAD-DEC-M5
BCFW-GAD-DEC-M10
BCFW-GAD-DEC-M20

p
a
g
y
t
i
l

l

a
u
d
n
m
u
o
c
f
o
e
c
n
a
i
r
a
V

1.5

1

0.5

0

50

100
Iteration

150

200

0

10

20

30
Time [sec]

40

50

60

0

50

100
Iteration

150

200

0

0

10

20

30

Iteration

40

50

(i) objective value : f (T)

(ii) objective value (time)

(iii) duality gap: g(T)

(iv) variance of gi(T)

(a) BCFW-U-DEC and BCFW-GA-DEC

120

115

110

105

100

95

90

85

t
s
o
C

BCFW-U-ELS
BCFW-GAS-ELS
BCFW-GAD-ELS-M1
BCFW-GAD-ELS-M5
BCFW-GAD-ELS-M10
BCFW-GAD-ELS-M20

120

115

110

105

100

95

90

85

t
s
o
C

BCFW-U-ELS
BCFW-GAS-ELS
BCFW-GAD-ELS-M1
BCFW-GAD-ELS-M5
BCFW-GAD-ELS-M10
BCFW-GAD-ELS-M20

104

103

102

p
a
g
y
t
i
l

a
u
D

101

BCFW-U-ELS
BCFW-GAS-ELS
BCFW-GAD-ELS-M1
BCFW-GAD-ELS-M5
BCFW-GAD-ELS-M10
BCFW-GAD-ELS-M20

0

50

100
Iteration

150

200

0

20

40
60
Time [sec]

80

100

0

50

100
Iteration

150

200

10-4

2

p
a
g
y
t
i
l

l

a
u
d
n
m
u
o
c
f
o
e
c
n
a
i
r
a
V

1.5

1

0.5

0

0

BCFW-U-ELS
BCFW-GAS-ELS
BCFW-GAD-ELS-M1
BCFW-GAD-ELS-M5
BCFW-GAD-ELS-M10
BCFW-GAD-ELS-M20

10

20

30

40

50

Iteration

(i) objective value : f (T)

(ii) objective value (time)

(iii) duality gap: g(T)

(iv) variances of gi(T)

(b) BCFW-U-ELS and BCFW-GA-ELS

120

115

110

105

100

95

90

85

t
s
o
C

BCAFW-U-ELS
BCAFW-GAS-ELS
BCAFW-GAD-ELS-M1
BCAFW-GAD-ELS-M5
BCAFW-GAD-ELS-M10
BCAFW-GAD-ELS-M20

120

115

110

105

100

95

90

85

t
s
o
C

BCAFW-U-ELS
BCAFW-GAS-ELS
BCAFW-GAD-ELS-M1
BCAFW-GAD-ELS-M5
BCAFW-GAD-ELS-M10
BCAFW-GAD-ELS-M20

104

103

102

p
a
g
y
t
i
l

a
u
D

101

BCAFW-U-ELS
BCAFW-GAS-ELS
BCAFW-GAD-ELS-M1
BCAFW-GAD-ELS-M5
BCAFW-GAD-ELS-M10
BCAFW-GAD-ELS-M20

0

50

100
Iteration

150

200

0

50

100

Time [sec]

150

0

50

100
Iteration

150

200

10-4

2

p
a
g
y
t
i
l
a
u
d
n
m
u
o
c

l

f
o
e
c
n
a
i
r
a
V

1.5

1

0.5

0

0

BCAFW-U-ELS
BCAFW-GAS-ELS
BCAFW-GAD-ELS-M1
BCAFW-GAD-ELS-M5
BCAFW-GAD-ELS-M10
BCAFW-GAD-ELS-M20

10

20

30

40

50

Iteration

(i) objective value : f (T)

(ii) objective value (time)

(iii) duality gap: g(T)

(iv) variances of gi(T)

(c) BCAFW-U-ELS and BCAFW-GA-ELS

120

115

110

105

100

95

90

85

t
s
o
C

BCPFW-U-ELS
BCPFW-GAS-ELS
BCPFW-GAD-ELS-M1
BCPFW-GAD-ELS-M5
BCPFW-GAD-ELS-M10
BCPFW-GAD-ELS-M20

120

115

110

105

100

95

90

85

t
s
o
C

BCPFW-U-ELS
BCPFW-GAS-ELS
BCPFW-GAD-ELS-M1
BCPFW-GAD-ELS-M5
BCPFW-GAD-ELS-M10
BCPFW-GAD-ELS-M20

104

103

102

101

p
a
g
y
t
i
l
a
u
D

BCPFW-U-ELS
BCPFW-GAS-ELS
BCPFW-GAD-ELS-M1
BCPFW-GAD-ELS-M5
BCPFW-GAD-ELS-M10
BCPFW-GAD-ELS-M20

0

50

100
Iteration

150

200

0

20

40

60
Time [sec]

80

100

120

0

50

100
Iteration

150

200

10-4

2

1.5

1

0.5

BCPFW-U-ELS
BCPFW-GAS-ELS
BCPFW-GAD-ELS-M1
BCPFW-GAD-ELS-M5
BCPFW-GAD-ELS-M10
BCPFW-GAD-ELS-M20

p
a
g
y
t
i
l

l

a
u
d
n
m
u
o
c
f
o
e
c
n
a

i
r
a
V

0

0

10

20

30

40

50

Iteration

(i) objective value : f (T)

(ii) objective value (time)

(iii) duality gap: g(T)

(iv) variances of gi(T)

(d) BCPFW-U-ELS and BCPFW-GA-ELS

Figure A.4: Evaluations on duality-adaptive sampling (corresponding to Figure 4).

39

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
References

[1] G. Peyre and M. Cuturi. Computational optimal transport. Foundations and Trends in Ma-

chine Learning, 11(5-6):355–607, 2019.

[2] M. S. Pydi and V. Jog. Adversarial risk via optimal transport and optimal couplings. In ICML,

2020.

[3] R. Singh, I. Haasler, Q. Zhang, J. Karlsson, and Y. Chen. Inference with aggregate data: An

optimal transport approach. arXiv preprint: arXiv:2003.13933, 2020.

[4] L. Chen, Z. Gan, Y. Cheng, L. Li, L. Carin, and J. Liu. Graph optimal transport for cross-

domain alignment. In ICML, 2020.

[5] J. Huang, Z. Fang, and H. Kasai. LCS graph kernel based on Wasserstein distance in longest

common subsequence metric space. arXiv preprint arXiv:2012.03612, 2020.

[6] R. Ievgen, C. Nicolas, F. R´emi, and T. Devis. Optimal transport for multi-source domain

adaptation under target shift. In AISTATS, 2019.

[7] H. Kasai. Multi-view Wasserstein discriminant analysis with entropic regularized Wasserstein

distance. In ICASSP, 2020.

[8] T. Fukunaga and Kasai. H. Wasserstein k-means with sparse simplex projection. In ICPR,

2020.

[9] L. Kantorovich. On the transfer of masses (in russian). Doklady Akademii Nauk, 37(2):227–229,

1942.

[10] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transportation distances.

In NIPS, 2013.

[11] L. Chizat, G. Peyr´e, B. Schmitzer, and F.-X. Vialard. Scaling algorithms for unbalanced

optimal transport problems. Mathematics of computation, 87:2563–2609, 2018.

[12] J. Altschuler, J. Weed, and P. Rigollet. Near-linear time approximation algorithms for optimal

transport via sinkhorn iteration. In NIPS, 2017.

[13] T. Lin, N. Ho, and M. Jordan. On eﬃcient optimal transport: An analysis of greedy and

accelerated mirror descent algorithms. In ICML, 2020.

[14] M. Blondel, V. Seguy, and A. Rolet. Smooth and sparse optimal transport. In AISTATS, 2018.

[15] F.-P. Paty, A. d’Aspremont, and M. Cuturi. Regularity as regularization: Smooth and strongly

convex brenier potentials in optimal transport. In AISTATS, 2020.

[16] Z. Goldfeld and K. Greenewald. Gaussian-smoothed optimal transport: Metric structure and

statistical eﬃciency. In AISTATS, 2020.

[17] J. Rabin, S. Ferradans, and N. Papadakis. Adaptive color transfer with relaxed optimal trans-

port. ICIP, 2014.

[18] C. Frogner, C. Zhang, H. Mobahi, M. Araya-Polo, and T. Poggio. Learning with a Wasserstein

loss. In NIPS, 2015.

40

[19] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logstics

Quarterly, 3:95–110, 1956.

[20] S. Ferradans, N. Papadakis, G. Peyr´e, and J.-F. Aujol. Regularized discrete optimal transport.

SIAM Journal on Imaging Sciences, 7(3):1853–1882, 2013.

[21] A. Rakotomamonjy, R. Flamary, and N. Courty. Generalized conditional gradient: analysis of

convergence and applications. arXiv preprint arXiv:1510.06567, 2015.

[22] N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain
IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(9):1853–

adaptation.
1865, 2017.

[23] F.-P. Paty and M. Cuturi. Subspace robust Wasserstein distances. In ICML, 2019.

[24] S. J. Wright. Coordinate descent algorithms. Mathematical Programming, 151:3–34, 2015.

[25] M. Perrot, N. Courty, R. Flamary, and A. Habrard. Mapping estimation for discrete optimal

transport. In NIPS, 2016.

[26] I. Redko, T. Vayer, R. Flamary, and N. Courty. Co-optimal transport. In NeurIPS, 2020.

[27] S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-coordinate Frank-Wolfe

optimization for structural SVMs. In ICML, 2013.

[28] C. Villani. Optimal transport: Old and new. Springer, 2008.

[29] E. Levina and P. J. Bickel. The earth mover’s distance is the mallows distance: Some insights

from statistics. In ICCV, 2001.

[30] N. Bonneel, M. Panne, S. Paris, and W. Heidrich. Displacement interpolation using Lagrangian

mass transport. ACM Transactions on Graphics, 30(6):1–12, 2011.

[31] M. Kusner, Y. Sun, N. Kolkin, and K. Weinberger. From word embeddings to document

distances. In ICML, 2015.

[32] N. Kolkin, J. Salavon, and G. Shakhnarovich. Style transfer by relaxed optimal transport and

self-similarity. In CVPR, 2019.

[33] T. Qiu, B. Ni, Z. Liu, and X. Chen. Fast optimal transport artistic style transfer. In MultiMedia

Modeling, 2021.

[34] J.-D. Benamou. Numerical resolution of an ”unbalanced” mass transport problem. ESAIM:

M2AN, 37(5):851–868, 2003.

[35] P. C. Francesca, L. Ning, and T. T. Georgiou. Convex clustering via optimal mass transport.

arXiv preprint: arXiv:1307.5459, 2013.

[36] P. Swoboda and V. Kolmogorov. Map inference via block-coordinate frank-wolfe algorithm. In

CVPR, 2019.

[37] K. L. Clarkson. Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm. ACM

Transactions on Algorithms, 6(4), 2010.

[38] M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In ICML, 2013.

41

[39] P. Wolfe. Integer and nonlinear programming. Amsterdam : North-Holland Pub. Co., 1970.

[40] B. F. Mitchell, V. F. Dem’yanov, and V. N. Malozemov. Finding the point of a polyhedron

closest to the origin. SIAM Journal on Control, 12(1):19–26, 1974.

[41] S. Lacoste-Julien and M. Jaggi. On the global linear convergence of Frank-Wolfe optimization

variants. In NIPS, 2015.

[42] A. Osokin, J.-B. Alayrac, I. Lukasewitz, P. Dokania, and S. Lacoste-Julien. Minding the gaps

for block Frank-Wolfe optimization of structured SVMs. In ICML, 2016.

[43] V. Franc. FASOLE: Fast algorithm for structured output learning. In ECML PKDD, 2014.

[44] Y. Nesterov. Eﬃciency of coordinate descent methods on huge-scale optimization problems.

SIAM Journal on Optimization, 22(2):341–362, 2012.

[45] D. Perekrestenko, V. Cevher, and M. Jaggi. Faster coordinate descent via adaptive importance

sampling. In AISTATS, 2017.

[46] D. Needell, N. Srebro, and R. Ward. Stochastic gradient descent, weighted sampling, and the

randomized kaczmarz algorithm. In NIPS, 2014.

[47] P. Zhao and T. Zhang. Stochastic optimization with importance sampling. In ICML, 2015.

[48] Y. Nesterov. Subgradient methods for huge-scale optimization problems. Mathematical Pro-

gramming, 146(1):275–296, 2014.

[49] S. Shalev-Shwartz and Y. Wexler. Minimizing the maximal loss: How and Why?

arXiv

preprint: arXiv:1602.01690, 2016.

[50] F. Pitie and A. Kokaram. The linear monge-kantorovitch linear colour mapping for example-

based colour transfer. In 4th European Conference on Visual Media Production, 2007.

[51] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences, 2(1):182–202, 2009.

42

