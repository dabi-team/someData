1
2
0
2

t
c
O
4
1

]

G
L
.
s
c
[

2
v
9
0
2
6
0
.
0
1
1
2
:
v
i
X
r
a

A Brief Introduction to Automatic Diﬀerentiation for
Machine Learning

Davan Harrison
University of California, Santa Cruz

June 9, 2019

Abstract

Machine learning, and neural network models in particular, have been improving
the state of the art performance on many artiﬁcial intelligence related tasks. Neural
network models are typically implemented using frameworks that perform gradient
based optimization methods to ﬁt a model to a dataset. These frameworks use a tech-
nique of calculating derivatives called automatic diﬀerentiation (AD) which removes
the burden of performing derivative calculations from the model designer. In this re-
port we describe AD, its motivations, and diﬀerent implementation approaches. We
brieﬂy describe dataﬂow programming as it relates to AD. Lastly, we present example
programs that are implemented with Tensorﬂow and PyTorch, which are two commonly
used AD frameworks.

1

Introduction

Deep learning is modeling technique which involves constructing large and complicated neu-
ral network models to perform tasks in a variety of artiﬁcial intelligence related ﬁelds, such
as Computer Vision and Natural Language Processing. A neural network model uses com-
plicated mathematical expressions that operate on matrix or tensor variables. Typically,
neural network models are ﬁt to a dataset using gradient based optimization methods that
involve repeated computation of derivatives. In the recent years a number of neural network
modeling frameworks have been developed, such as Theano [3], Tensorﬂow [1], and PyTorch
[5]. These frameworks employ automatic diﬀerentiation (AD), which is a method of com-
puting derivatives of numeric functions that are deﬁned programmatically. AD allows for
eﬃcient and accurate evaluation of derivatives while at the same time removing the burden
of performing derivative calculations from the model designer.

In this report we discuss AD and its diﬀerent forms. Also, we will discuss dataﬂow
programming which is a programming paradigm relevant to AD. Lastly, we will discuss the
Tensorﬂow [1] and PyTorch [5] machine learning frameworks, and present example programs
that are implemented using these frameworks.

1

 
 
 
 
 
 
Z := A × B + C

W := Z + 4
Y := Z2 − (3Z + B)

(b) written statement.

(a) dataﬂow graph.

Figure 1: An example graphical representation of a written arithmetic calculation.

2 Dataﬂow Programming

Dataﬂow programming is a programming paradigm where programs are represented as di-
rected graphs [7, 6]. The dataﬂow programming paradigm has a concept called an executable
block [6]. Thus, under this paradigm, the execution model gets represented as a graph where
executable blocks are the nodes, and the edges between nodes represent data dependencies.
The dataﬂow graph of an entire program will consist of at least one source node, at least one
end node, and a set of data processing nodes. All the nodes are connected by directed data
dependency edges. The nodes of the graph are primitive instructions such as arithmetic or
comparison operations, and can have multiple data inputs (directed edges ﬂowing toward
a node) and outputs (directed edges ﬂowing out of a node)[4]. Once all of a node’s input
edges contain data, the node becomes ﬁreable, and will be executed at some future time [4].
The node becomes dormant (waiting to become ﬁreable) once the computation is ﬁnished,
and the node’s result gets passed on by its outward edges to other nodes that depend on
the result. Figure 1 shows an example sequential program given in [7, 6] and its dataﬂow
representation.

The beneﬁt of the dataﬂow paradigm is that it easily supports concurrent execution of
instructions because in the cases that multiple nodes become ﬁreable at the same time, then
they can all be executed in parallel.

Many AD frameworks utilize the dataﬂow programming paradigm to construct programs
in the form of computation graphs that can be operated on using the backpropogation algo-
rithm.

2

3 Automatic Diﬀerentiation

In this section we will discuss AD and its motivation, and then describe the forward and
backward modes of AD. The section will close with a brief discussion of two commonly used
implementations of AD: operator overloading (OO), and source transformation (ST) [8].

3.1 Why AD?

Machine learning frequently involves computing and evaluating derivatives in order to opti-
mize an objective function. There are four methods of programmatically computing deriva-
tives: computing them by hand and coding them up, approximating derivatives with numeri-
cal diﬀerentiation, symbolic diﬀerentiation as in Maple or Mathematica, and lastly automatic
diﬀerentiation. In a machine learning setting, AD is preferred over the other three methods
due to a number of limiting factors. Manual diﬀerentiation is time consuming and prone
to error, while numerical diﬀerentiation is inaccurate due to truncation and rounding errors
[2]. Symbolic diﬀerentiation suﬀers from ”expression swell” and imposes restrictions on the
programmer, such as limiting use of branching, loops, and recursion [2].

AD is diﬀerent from numerical and symbolic diﬀerentiation but it has some overlap. AD is
partly symbolic and partly numerical because it uses symbolic diﬀerentiation rules to produce
numerical values of derivatives[2]. An AD system computes derivatives by accumulating
values produced in the process of executing program instructions. Then numerical derivative
evaluations are produced, as opposed to derivative expressions. The strengths of AD over
the other methods is that it allows for accurate derivative computations, is computationally
eﬃcient, and requires minimal code changes in order to be used [2].

3.2 Modes of AD

When AD is applied to a program, the program is augmented so that it also computes
derivatives alongside its standard computations. This is achieved by decomposing a program
into its primitive operations, (e.g., binary arithmetic operations, transcendental functions,
etc.) that have known derivatives [2, 8]. Then the derivative of the original program structure
is calculated by using the chain rule from calculus to combine the derivatives of the program’s
primitive constituents. This process can be represented as an evaluation trace 1 of a program
[2].

AD has two modes, forward mode (left to right) and reverse mode (right to left), that
In forward
each correspond to the direction in which the chain rule is evaluated [2, 8].
mode, intermediate computations of the original program and computations of derivatives
are calculated alongside each other using a single monolithic pass over the program, or by
making multiple passes (one pass for each input variable). Forward mode has constant
memory requirements and computation complexity depends on the number of inputs [8].

1also known as a Wangert list

3

Figure 2: Computation graph for y = f (x1, x2) = ln(x1) + x1x2 − sin(x2).

In reverse mode, the chain rule is evaluated in the reverse order of the original program,
sometimes called the primal program. This is achieved by constructing a second program
called an adjoint program which has a control ﬂow in reverse order to that of the original
program [8]. Program execution is broken into two separate phases. In the ﬁrst phase, the
primal program is executed. Then, in the second phase, the adjoint program is executed
to calculate the gradients by starting with the output of the primal program, continuing
through the intermediate variables, and ending with the primal program’s inputs[8]. All the
primal program’s intermediate variables need to be saved until the adjoint program com-
pletes it execution because they are used for computing the gradients. Therefore, reverse
mode has memory requirements that grow with the number of intermediate variables, and
has computation complexity that grows with the number of outputs [8]. In machine learn-
ing, reverse mode AD is more frequently used due to the fact that many tasks in machine
learning have few inputs and involve gradient based optimization of a scalar error value,
which cause it to be more eﬃcient than forward mode AD. Reverse mode AD corresponds
to the backpropagation algorithm [2, 8].

Consider this example computation whose computation graph is shown in Figure 2:

y = f (x1, x2) = ln(x1) + x1x2 − sin(x2) ,

(1)

and is an example taken from [8]. Table 1 shows an evaluation trace for Equation 1 when
using reverse mode AD. In Figure 2 and Table 1, vi for 1 ≤ i represent intermediate variables,
and vi for i ≤ 0 represent input variables. The output variable is represented by y.

3.3 Operator overloading

In the operator overloading (OO) approach to AD, the adjoint program is dynamically
constructed upon execution of the primal program. The deﬁnitions of primitive operations
of the host programming language are overloaded so that they perform additional tasks that
facilitate the evaluation trace [8]. As each operation of the primal program is executed,
the inputs and results are saved on a ”tape” in order to preserve intermediate variables for
later use by the adjoint program. Often, the tape takes the form of a graph that resembles
the computation graphs used in dataﬂow programming. After the forward execution of the

4

Forward Primal Trace

Reverse Adjoint Trace

v−1 = x1
v0 = x2

= 2
= 5

v1 = ln(v−1) = ln(2)
v2 = v−1 × v0 = 2 × 5

v3 = sin(v0)
v4 = v1 + v2

= sin(5)
= 0.693 + 10

v5 = v4 − v3

= 10.693 + 0.959

y = v5

= 11.652

∂v1
∂v−1

−1

x(cid:48)
1 = v(cid:48)
2 = v(cid:48)
x(cid:48)
0
v(cid:48)
−1 = v(cid:48)
−1 + v(cid:48)
1
∂v2
v(cid:48)
0 = v(cid:48)
0 + v(cid:48)
2
∂v0
∂v2
−1 = v(cid:48)
v(cid:48)
2
∂v−1
∂v3
v(cid:48)
0 = v(cid:48)
3
∂v0
∂v4
v(cid:48)
2 = v(cid:48)
4
∂v2
∂v4
1 = v(cid:48)
v(cid:48)
4
∂v1
∂v5
3 = v(cid:48)
v(cid:48)
5
∂v3
∂v5
4 = v(cid:48)
v(cid:48)
5
∂v4
5 = y(cid:48)
v(cid:48)

= 5.5
= 1.716

= 5

−1 + v(cid:48)
0 + v(cid:48)
2 × v0

1/v−1 = 5.5
2 × v−1 = 1.716

= v(cid:48)
= v(cid:48)
= v(cid:48)
= v(cid:48)3 × cos(v0) = −0.284
= v(cid:48)
= v(cid:48)
= v(cid:48)
= v(cid:48)
= 1

4 × 1
4 × 1
5 × (−1)
5 × 1

= 1
= 1
= −1
= 1

Table 1: Evaluation trace of y = f (x1, x2) = ln(x1) + x1x2 − sin(x2) with x1 = 2 and x2 = 5
using revers mode AD.

program, calculation of derivatives is achieved by processing the tape in the reverse direction
[8].

OO has several strengths and weaknesses. The backpropogation algorithm is simple to
implement using the OO method [8]. Also, it supports ease of use by allowing users to
implement models as a regular program in the host language, thereby facilitating arbitrary
control ﬂow statements and promoting intuitive debugging [2]. On the other hand, the OO
method incurs a runtime cost because the adjoint program is constructed at each execution
of the primal program. Furthermore, it does not allow for static optimization of the adjoint
program.

3.4 Source Transformation

In OO, the adjoint program is dynamically constructed at each execution and is deﬁned as
an implicit property of the primal program, but this is not case in the source transformation
approach (ST). In ST, the user uses a domain speciﬁc mini language to deﬁne a computation
graph that explicitly speciﬁes both the primal and adjoint programs [2]. Then, during
execution, the framework interprets the program with diﬀerent inputs while keeping the
computation graph ﬁxed. Similar to OO, a tape can be used in the ST approach to save the
intermediate variables during execution, or the intermediate variables can be saved directly
on the computation graph.

A beneﬁt of the ST approach is that it only builds the adjoint program once, and it allows
for ahead of time optimization of the computation graph structure since the adjoint program
is known during compilation [8, 2]. Furthermore, with ST, the framework is aware of the
entire computation graph structure ahead of time, which makes it easier to take advantage

5

(b) computation graph.

(a) Python code.

Figure 3: A PyTorch program that implements Equation 1 and the computation graph
produced by PyTorch while executing the program.

of the parallelization opportunities provided by computation graphs and the dataﬂow pro-
gramming paradigm. On the down side, this method can impose limitations on the user,
such as limiting use of loops, recursion, and higher order functions [8]. In addition, programs
using ST can be diﬃcult to debug and have an un-intuitive control ﬂow [2].

4 AD in PyTorch

The PyTorch framework uses the OO approach to implement AD [5]. PyTorch is a library
for the Python programming language and allows users to use any Python features they want
[5]. Primitive instructions are overloaded so that they perform an evaluation trace and save
intermediate variables, but the framework limits its use of tapes. Instead, the intermediate
results record only the subset of the computation graph that relates to their computation
which supports parallel execution and allows sections of the computation graph to be quickly
freed from memory once they are no longer needed [5]. Even though PyTorch is a Python
library, most of it is implemented in C++ to improve run-time eﬃciency. Figure 3 shows a
program that uses the PyTorch framework to implement the computation of Equation 1, as
well the computation graph that is created during the execution of the program.

5 AD in Tensorﬂow

The Tensorﬂow framework[1] follows the source transformation approach where the user
explicitly constructs a computation graph. A Tensorﬂow program that implements the com-
putations of Equation 1 is shown in Figure 4. Note that the computation graph is explicitly

6

(b) computation graph.
variables are named with Const.

Input

(a) Python code.

Figure 4: A Tensorﬂow program that implements Equation 1 and a graphical representation
of the computation graph constructed by Tensorﬂow.

referenced. Nodes are added to the graph by accessing it as a global variable as part of the
Tensorﬂow instructions.

Tensorﬂow programs get compiled using its XLA2 linear algebra compiler which optimizes
computations. Unlike PyTorch, Tensorﬂow is language-independent. So computation graphs
can be implemented in Python and compiled and saved using an intermediate representation.
Then they can be loaded and run by a Tensorﬂow framework of a diﬀerent language, such
as C++, or Javascript.

6 Conclusion

In this report we describe automatic diﬀerentiation and its motivations. We discuss the
forward and reverse modes of AD, and focus on the reverse mode because it is typically used
in machine learning. Two approaches to reverse mode AD are described. Then, we present
examples of similar programs implemented using two AD frameworks, namely, Tensorﬂow
and PyTorch. Along the way we brieﬂy discuss the dataﬂow programming paradigm which
is heavily used by many AD frameworks, including Tensorﬂow and PyTorch.

2www.tensorflow.org/xla

7

References

[1] Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
Ghemawat, S., Irving, G., Isard, M., et al. Tensorﬂow: A system for large-scale
In 12th {USENIX} Symposium on Operating Systems Design and
machine learning.
Implementation ({OSDI} 16) (2016), pp. 265–283.

[2] Baydin, A. G., Pearlmutter, B. A., Radul, A. A., and Siskind, J. M. Au-
tomatic diﬀerentiation in machine learning: a survey. Journal of Marchine Learning
Research 18 (2018), 1–43.

[3] Bergstra, J., Bastien, F., Breuleux, O., Lamblin, P., Pascanu, R., Delal-
leau, O., Desjardins, G., Warde-Farley, D., Goodfellow, I., Bergeron,
A., et al. Theano: Deep learning on gpus with python. In NIPS 2011, BigLearning
Workshop, Granada, Spain (2011), vol. 3, Citeseer, pp. 1–48.

[4] Johnston, W. M., Hanna, J., and Millar, R. J. Advances in dataﬂow program-

ming languages. ACM computing surveys (CSUR) 36, 1 (2004), 1–34.

[5] Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z.,
Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. Automatic diﬀerentiation
in pytorch. In NIPS 2017 Autodiﬀ Workshop: The Future of Gradient-based Machine
Learning Software and Techniques, Long Beach, CA, US, December 9, 2017 (2017).

[6] Sousa, T. B. Dataﬂow programming concept, languages and applications. In Doctoral

Symposium on Informatics Engineering (2012), vol. 130.

[7] Sutherland, W. R. The on-line graphical speciﬁcation of computer procedures. PhD

thesis, Massachusetts Institute of Technology, 1966.

[8] van Merri¨enboer, B., Breuleux, O., Bergeron, A., and Lamblin, P. Auto-
matic diﬀerentiation in ml: Where we are and where we should be going. In Advances
in neural information processing systems (2018), pp. 8757–8767.

8

