Co-training for Policy Learning

Jialin Song†

Ravi Lanka‡
† California Institute of Technology
‡Jet Propulsion Laboratory, California Institute of Technology

Yisong Yue†

Masahiro Ono‡

9
1
0
2

l
u
J

3

]

G
L
.
s
c
[

1
v
4
8
4
4
0
.
7
0
9
1
:
v
i
X
r
a

Abstract

We study the problem of learning sequential
decision-making policies in settings with mul-
tiple state-action representations. Such set-
tings naturally arise in many domains, such
as planning (e.g., multiple integer program-
ming formulations) and various combinato-
rial optimization problems (e.g.,
those with
both integer programming and graph-based
formulations).
Inspired by the classical co-
training framework for classiﬁcation, we study
the problem of co-training for policy learn-
ing. We present sufﬁcient conditions under
which learning from two views can improve
upon learning from a single view alone. Moti-
vated by these theoretical insights, we present
a meta-algorithm for co-training for sequen-
tial decision making. Our framework is com-
patible with both reinforcement learning and
imitation learning. We validate the effec-
tiveness of our approach across a wide range
of tasks, including discrete/continuous control
and combinatorial optimization.

1

INTRODUCTION

Conventional wisdom in problem-solving suggests that
there is more than one way to look at a problem. For
sequential decision making problems, such as those in
reinforcement learning and imitation learning, one can
often utilize multiple different state-action representa-
tions to characterize the same problem. A canonical
application example is learning solvers for hard opti-
mization problems such as combinatorial optimization
[23, 41, 15, 56, 4]. It is well-known in the operations
research community that many combinatorial optimiza-
tion problems have multiple formulations. For exam-
ple, the maximum cut problem admits a quadratic integer

program as well as a linear integer program formulation
[7, 18]. Another example is the traveling salesman prob-
lem, which admits multiple integer programming formu-
lations [47, 45]. One can also formulate many problems
using a graph-based representation (see Figure 1). Be-
yond learning combinatorial optimization solvers, other
examples with multiple state-action representations in-
clude robotic applications with multiple sensing modal-
ities such as third-person view demonstrations [57] and
multilingual machine translation [27].

In the context of policy learning, one natural question
is how different state-action formulations impact learn-
ing and, more importantly, how learning can make use
of multiple formulations. This is related to the co-
training problem [6], where different feature representa-
tions of the same problem enable more effective learning
than using only a single representation [64, 33]. While
co-training has received much attention in classiﬁcation
tasks, little effort has been made to apply it to sequen-
tial decision making problems. One issue that arises
in the sequential case is that some settings have com-
pletely separate state-action representations while others
can share the action space.

In this paper, we propose CoPiEr (co-training for policy
learning), a meta-framework for policy co-training that
can incorporate both reinforcement learning and imita-
tion learning as subroutines. Our approach is based on
a novel theoretical result that integrates and extends re-
sults from PAC analysis for co-training [16] and general
policy learning with demonstrations [29]. To the best of
our knowledge, we are the ﬁrst to formally extend the
co-training framework to policy learning.

Our contributions can be summarized as:

•

We present a formal theoretical framework for pol-
icy co-training. Our results include: 1) a general
theoretical characterization of policy improvement,
and 2) a specialized analysis in the shared-action
setting to explicitly quantify the performance gap

 
 
 
 
 
 
1

max

(cid:88)5

−

i=1

xi,

2

3

5

4

≥

subject to:
x1 + x2
x2 + x3
x3 + x4
x3 + x5
x4 + x5
xi

≥

≥

≥

≥
0, 1
}

1,

1,

1,

1,

1,

,

1,

· · ·

∈ {

∈ {

i
∀

, 5
}
Figure 1: Two ways to encode minimum vertex cover
(MVC) problems. Left: policies learn to operate directly
on the graph view to ﬁnd the minimal cover set [30].
Right: we express MVC as an integer linear program,
then polices learn to traverse the resulting combinatorial
search space, i.e., learn to branch-and-bound [23, 56].

•

•

(i.e., regret) versus the optimal policy. These theo-
retical characterizations shed light on rigorous algo-
rithm design for policy learning that can appropri-
ately exploit multiple state-action representations.

We present CoPiEr (co-training for policy learn-
ing), a meta-framework for policy co-training. We
specialize CoPiEr in two ways: 1) a general mech-
anism for policies operating on different represen-
tations to provide demonstrations to each other, and
2) a more granular approach to sharing demonstra-
tions in the shared-action setting.

We empirically evaluate on problems in combi-
natorial optimization and discrete/continuous con-
trol. We validate our theoretical characterizations
to identify when co-training can improve on single-
view policy learning. We further showcase the prac-
ticality of our approach for the combinatorial opti-
mization setting, by demonstrating superior perfor-
mance compared to a wide range of strong learning-
based benchmarks as well as commercial solvers
such as Gurobi.

2 RELATED WORK

Co-training Our work is inspired by the classical co-
training framework for classiﬁcation [6], which utilizes
two different feature representations, or views, to effec-
tively use unlabeled data to improve the classiﬁcation
accuracy. Subsequent extensions of co-training includes
co-EM [44] and co-regularization [55]. Co-training has
been widely used in natural language processing [64, 31],
clustering [33, 40], domain adaptation [11] and game

playing [34]. For policy learning, some related ideas
have been explored where multiple estimators of the
value or critic function are trained together [67, 63].

In addition to the empirical successes, several previous
works also establish theoretical properties of co-training
[6, 3, 16, 65]. Common assumptions in these analyses
include: 1) each view is sufﬁcient for learning a good
classiﬁer on its own, and 2) conditional independence of
the features given the labels. Recently, there are works
considering weakened assumptions, such as allowing for
weak dependencies between the two views [5], or relax-
ing the sufﬁciency condition [66].

Policy Learning for Sequential Decision Making Se-
quential decision making pertains to tasks where the pol-
icy performs a series of actions in a stateful environment.
A popular framework to characterize the interaction be-
tween the agent and the environment is a Markov Deci-
sion Process (MDP). There are two main approaches for
policy learning in MDPs: reinforcement learning and im-
itation learning. For both reinforcement learning and im-
itation learning, we show that co-training on two views
can provide improved exploration in the former and sur-
rogate demonstrations in the latter, in both cases leading
to superior performance.

Reinforcement learning uses the observed environmen-
tal rewards to perform policy optimization. Recent works
include Q-Learning approaches such as deep Q-networks
[42], as well as policy gradient approaches such as
DDPG [38], TRPO [52] and PPO [53]. Despite its suc-
cessful applications to a wide variety of tasks including
playing games [42, 54], robotics [37, 32] and combina-
torial optimization [15, 41], high sample complexity and
unstable learning pose signiﬁcant challenges in practice
[24], often causing learning to be unreliable.

Imitation learning uses demonstrations (from an expert)
as the primary learning signal. One popular class of algo-
rithms is reduction-based [17, 49, 51, 50, 10], which gen-
erates cost-sensitive supervised examples from demon-
strations. Other approaches include estimating the ex-
pert’s cost-to go [58], inverse reinforcement learning
[1, 26, 68], and behavioral cloning [61]. One major lim-
itation of imitation learning is the reliance on demon-
strations. One solution is to combine imitation and rein-
forcement learning [36, 29, 12, 43] to learn from fewer
or coarser demonstrations.

3 BACKGROUND & PRELIMINARIES

Markov Decision Process with Two State Representa-
tions. A Markov decision process (MDP) is deﬁned by
T ). Let
a tuple (
denote the state space,

, r, γ,

,

,
A

P

S

S

S

(s(cid:48)

|

S

P

the action space,

s, a) the (probabilistic) state
A
dynamics, r(s, a) the reward function, γ the discount
factor and (optinal)
T a set of terminal states where
the decision process ends. We consider both stochas-
tic and deterministic MDPs. An MDP with two views
A,
A = (
A,
A
T ) and
can be written as
A
S
S
B
B, rB, γB,
T ). To connect the two
M
S
views, we make the following assumption about the abil-
ity to translate trajectories between the two views.

A, rA, γA,

B = (

M
P

B,

B,

A

P

S

M

n ),

, sA

0 , sA

0 , aA

A,
Assumption 1. For a (complete) trajectory in
τ A = (sA
1 , aA
there is a function
1 ,
· · ·
fA→B that maps τ A to its corresponding (complete) tra-
B: fA→B(τ A) = τ B =
jectory τ B in the other view
(sB
m). The rewards for τ A and τ B
1 , aB
1 ,
are the same under their respective reward functions,
i.e., (cid:80)n−1
i ) = (cid:80)m−1
j ). Simi-
larly, there is a function fB→A that maps trajectories
in

A which preserves the total rewards.

· · ·
i , aA

j=0 rB(sB

i=0 rA(sA

j , aB

0 , aB

0 , sB

B to

, sB

M

M

M

Note that in Assumption 1, the length of τ A and τ B can
be different because of different state and action spaces.

Combinatorial Optimization Example. Minimum
vertex cover (MVC) is a combinatorial optimization
problem deﬁned over a graph G = (V, E). A cover set is
E is incident to
a subset U
⊂
at least one v
U . The objective is to ﬁnd a U with the
minimal cardinality. For the graph in Figure 1, a minimal
cover set is

V such that every edge e

∈

∈

.

2, 3, 4
}

{

There are two natural ways to represent an MVC problem
as an MDP. The ﬁrst is graph-based [15] with the action
space as V and the state space as sequences of vertices in
V representing partial solutions. The deterministic tran-
sition function is the obvious choice of adding a vertex to
the current partial solution. The rewards are -1 for each
selected vertex. A terminal state is reached if the selected
vertices form a cover.

The second way is to formulate an integer linear program
(ILP) that encodes an MVC problem:

max

(cid:88)

−

v∈V

xv,

subject to :
xu + xv
xv

≥
0, 1
}

∈ {

1,

e = (u, v)

,

∀
v
∀

V.

∈

E,

∈

We can then use branch-and-bound [35] to solve this ILP,
which represents the optimization problem as a search
tree, and explores different areas of a search tree through
a sequence of branching operations. The MDP states
then represent current search tree, and the actions corre-
spond to which node to explore next. The deterministic
transition function is the obvious choice of adding a new

node into the search tree. The reward is 0 if an action
does not lead to a feasible solution and is the objective
value of the feasible solution minus the best incumbent
objective if an action leads to a node with a better feasible
solution. A terminal state is a search tree which contains
an optimal solution or reaches a limit on the number of
nodes to explore.

The relationship between solutions in the two formula-
tions are clear. For a graph G = (V, E), a feasible solu-
tion to the ILP corresponds to a vertex cover by select-
ing all the vertices v
V with xv = 1 in the solution.
This correspondence ensures the existence of mappings
between two representations that satisfy Assumption 1.

∈

Note that, despite the deterministic dynamics, solving
MVC and other combinatorial optimization problems
can be extremely challenging due to the very large state
Indeed, policy learning for combinatorial opti-
space.
mization is a topic of active research [30, 23, 56, 41, 4].

Policy Learning. We consider policy learning over a
distribution of MDPs. For instance, there can be a distri-
bution of MVC problems. Formally, we have a distribu-
tion
).
of MDPs that we can sample from (i.e.,
For a policy π, we deﬁne the following terms:

M ∼ D

D

η(π,

(cid:88)n−1
) = Eτ ∼π[
M
i=0
J(π) = EM∼D[η(π,
(cid:88)n−1
i=0
(cid:88)n−1
i=0

Vπ(s) = Eτ ∼π[

Qπ(s, a) = Eτ ∼π[

γir(si, ai)],

)],

M
γir(si, ai)
|
γir(si, ai)
|

s0 = s, a0 = a],

s0 = s],

Aπ(s, a) = Qπ(s, a)

Vπ(s),

−

M

with η being the expected cumulative reward of an indi-
, J the overall objective, Q the Q func-
vidual MDP
tion, V the value function and A the advantage function.
The performance of two policies can be related via the
advantage function [52, 28]: η(π(cid:48),
) +
Eτ ∼π(cid:48)[(cid:80)n−1
i=0 γiAπ(si, ai)]. Based on Theorem 1 below,
we can rewrite the ﬁnal term with the occupancy mea-
sure, ρπ(s, a) = P(π(s) = a) (cid:80)∞

) = η(π,

M

M

i=0 γiP(si = s
π).
|

Theorem 1. (Theorem 2 of [60]). For any policy π,
it is the only policy that has its corresponding occu-
pancy measure ρπ, i.e., there is a one-to-one mapping
between policies and occupancy measures. Speciﬁcally,
P(π(s) = a) = ρπ(s,a)

(cid:80)

a(cid:48) ρπ(s,a(cid:48)) .

With
(cid:80)∞

slight
i=0 γiP(si = s
|

notation

abuse,

=
π) to be the state visitation dis-

deﬁne ρπ(s)

tribution. In policy iteration, we aim to maximize:

Eτ ∼π(cid:48)[

(cid:88)n−1
i=0

γiAπ(si, ai)],

(cid:88)n−1
i=0
(cid:88)n−1
i=0

=

≈

Esi∼ρπ(cid:48) (s)[Eai∼π(cid:48)(si)[γiAπ(si, ai)]],

Esi∼ρπ(s)[Eai∼π(cid:48)(si)[γiAπ(si, ai)]].

This is done instead of taking an expectation over ρπ(cid:48)(s)
which has a complicated dependency on a yet unknown
policy π(cid:48). Policy gradient methods tend to use the ap-
proximation by using ρπ which depends on the current
policy. We deﬁne the approximate objective as:

ηπ(π(cid:48),

)
M

= η(π,

) +

M

(cid:88)n−1
i=0

Esi∼ρπ(s)[Eai∼π(cid:48)(si)[γiAπ(si, ai)]],

and its associated expectation over
EM∼D[ηπ(π(cid:48),
)].

M

as Jπ(π(cid:48)) =

D

4 A THEORY OF POLICY

CO-TRAINING

In this section, we provide two theoretical characteriza-
tions of policy co-training. These characterizations high-
light a trade-off in sharing information between different
views, and motivates the design of our CoPiEr algorithm
presented in Section 5.

We restrict our analysis to inﬁnite horizon MDPs, and
thus require a strict discount factor γ < 1. We show
in our experiments that our CoPiEr algorithm performs
well even in ﬁnite horizon MDPs with γ = 1. Due to
space constraints, we defer all proofs to the appendix.

We present two theoretical analyses with different types
of guarantees:

•

•

Section 4.1 quantiﬁes the policy improvement in
terms of policy advantages and differences, and
caters to policy gradient approaches.

Section 4.2 quantiﬁes the performance gap with
respect to an optimal policy in terms of policy dis-
agreements, which is a stronger guarantee than pol-
icy improvement. This analysis is restricted to the
shared action space setting, and caters to learning
reduction approaches.

4.1 GENERAL CASE: POLICY IMPROVEMENT

WITH DEMONSTRATIONS

For an MDP
M ∼ D
two policies with different views ηA(πA,
A) > ηB(πB,
If ηA(πA,
ηB(πB,

, consider the rewards of
A) and
B), πA

B).

M

M

M
M

performs better than πB on this instance , and we could
use the translated trajectory of πA as a demonstration for
πB. Even when J(πA) > J(πB), because J is com-
, πB can still outperform πA
puted in expectation over
on some MDPs. Thus it is possible for the exchange of
demonstrations to go in both directions.

D

D

, supp(
supp(

into two
D
2 such that the sup-
2) and
supp(

Formally, we can partition the distribution
(unnormalized) parts
port of
supp(
supp(

1 and
D
) = supp(
D
2) =
D
A)
M
2), η(πB,

D
∪
, where for an MDP
∅
η(πB,
M
B) > η(πA,

≥
M ∈
M
struction, we can quantify the performance gap as:
Deﬁnition 1.

D
1)
∩
D
1), η(πA,
D
supp(

M ∈
B) and for an MDP
A). By con-

M

1)

D

D

δ1 = EM∼D1[η(πA,
δ2 = EM∼D2[η(πB,

A)
B)

M

M

η(πB,
η(πA,

−

−

B)]
0,
≥
A)] > 0.

M

M

We can now state our ﬁrst result on policy improvement.
Theorem 2. (Extension of Theorem 1 in [29]) Deﬁne:

D = EM∼D[maxs DKL(πA(s)
π(cid:48)A(s))],
αA
(cid:107)
= EM∼D2[maxs DJS(πB(s)
βB
πA(s))],
D2
(cid:107)
D = EM∼D[maxs DKL(πB(s)
αB
π(cid:48)B(s))],
(cid:107)
= EM∼D1[maxs DJS(πA(s)
πB(s))],
βA
D1
(cid:107)
(cid:15)B
= maxM∈supp(D2) maxs,a
D2
(cid:15)A
D = maxM∈supp(D) maxs,a
(cid:15)A
D1
(cid:15)B
D = maxM∈supp(D) maxs,a

AπB (s, a)
|
|
,
AπA(s, a)
|
AπA (s, a)
|
|
.
AπB (s, a)
|

|
= maxM∈supp(D1) maxs,a

,

|

,

Here DKL & DJS denote the Kullback-Leibler and
Jensen-Shannon divergence respectively. Then we have:

J(π(cid:48)A) ≥ JπA (π(cid:48)A) −

J(π(cid:48)B) ≥ JπB (π(cid:48)B) −

2γA(4βB

D2 + αA

D(cid:15)A
D)

2γB(4βA

D1 + αB

D(cid:15)B
D)

D2 (cid:15)B
(1 − γA)2
D1 (cid:15)A
(1 − γB)2

+ δ2,

+ δ1.

Compared to conventional analyses on policy improve-
ment, the new key terms that determine how much the
policy improves are the β’s and δ’s. The β’s, which quan-
tify the maximal divergence between πA and πB, hinders
improvement, while the δ’s contribute positively. If the
net contribution is positive, then the policy improvement
bound is larger than that of conventional single view pol-
icy gradient. This insight motivates co-training algo-
rithms that explicitly aim to minimize the β’s.

One technicality is how to compute DJS(πA(s)
πB(s))
(cid:107)
given that the state and action spaces for the two repre-
sentations might be different. Proposition 1 ensures that
we can measure the Jensen-Shannon divergence between
two policies with different MDP representations.

πA

sA
0

aA
0

πA

sA
1

aA
1

aA

sA

sB

aB

s0

a0

s1

a1

a∗

πB

sB
0

aB
0

πB

sB
1

aB
1

Figure 3: Graphical model encodes the conditional inde-
pendence model.

Figure 2: Co-training with shared action space.

A

B

Proposition

1. For
an MDP

representations
of
satisfying
M
maxs DJS(πB(s)
quantities
the
πB(s)) are well-deﬁned.
maxs DJS(πA(s)
(cid:107)
and βA
D1

Minimizing βB
is not straightforward since the
D2
trajectory mappings between the views can be very com-
plicated. We present practical algorithms in Section 5.

M
Assumption
πA(s))
(cid:107)

and
1,
and

4.2 SPECIAL CASE: PERFORMANCE GAP
FROM OPTIMAL POLICY IN SHARED
ACTION SETTING

A

A =

We now analyze the special case where the action spaces
B. Figure
of the two views are the same, i.e.,
2 depicts the learning interaction between πA and πB.
For each state s, we can directly compare actions chosen
by the two policies since the action space is the same.
This insight leads to a stronger analysis result where we
can bound the gap between a co-trained policy with an
optimal policy. The approach we take resembles learning
reduction analyses for interactive imitation learning.

A

For this analysis we focus on discrete action spaces with
k actions, deterministic learned policies, and a determin-
istic optimal policy (which is guaranteed to exist [48]).
We reduce policy learning to classiﬁcation: for a given
state s, the task of identifying the optimal action π∗(s) is
a classiﬁcation problem. We build upon the PAC gener-
alization bound results in [16] and show that under As-
sumption 2, optimizing a measure of disagreements be-
tween the two policies leads to effective learning of π∗.
Assumption 2. For a state s, its two representations sA
and sB are conditionally independent given the optimal
action π∗(s).

This assumption is common in analyses of co-training for
classiﬁcation [6, 16]. Although this assumption is typi-
cally violated in practice [44], our empirical evaluation
still demonstrates strong performance.

Assumption 2 corresponds to a graphical model describ-
ing the relationship between optimal actions and the state
representations (Figure 3). The intuition is that, when we
do not know a∗ = π∗(s), we should maximize the agree-

≥

ment between aA = πA(sA) and aB = πB(sB). By the
data-processing inequality in information theory [13], we
know that I(aA; a∗)
I(aA; aB). In practice, this means
that if aA and aB agree a lot, they must reveal substantial
information about what a∗ is. We formalize this intuition
and obtain an upper bound on the classiﬁcation error rate,
which enables quantifying the performance gap. Notice
that if we do not have any information from π∗, the best
we can hope for is to learn a mapping that matches π∗
up to some permutation of the action labels [16]. Thus
we assume we have enough state-action pairs from π∗ so
that we can recover the permutation. In practice this is
satisﬁed as we demonstrate in Section 6.1.

Formally, we connect the performance gap between a
learned policy and an optimal policy with an empirical
estimation on the disagreement in action choices among
m
τ A
i=1 be sampled trajec-
two co-trained policies. Let
i }
{
tories from πA and
m
fA→B(τ A
i )
i=1 be the mapped tra-
}
i=1, let N (aA = i)
m
fA→B(τ A
i )
jectories in
}
be the number of times action i is chosen by πA and
N = (cid:80)k
i=1 N (aA = i) be the total number of actions
in one trajectory set. Let N (aB = i) be the number
of times action i is chosen by πB when going through
i=1 and N (aA = i, aB = i)
m
fA→B(τ A
i )
the states in
}
{
record when both actions agree on i.

{
B. In

M

{

|

|

π

We also require a measure of model complexity, as is
common in PAC style analysis. We use
to denote the
number of bits needed to represent π. We can now state
our second main result quantifying the performance gap
with respect to an optimal policy:
Theorem 3. If Assumption 2 holds for
and a
deterministic optimal policy π∗. Let πA and πB be two
deterministic policies for the two representations.
Deﬁne:

M ∼ D

ˆP(aA = i

ˆP(aA

= i

|

|

aB = i) =

aB = i) =

N (aA = i, aB = i)
N (aB = i)

N (aA

= i, aB = i)

N (aB = i)

,

,

(cid:15)i(πA, πB, σ) =

(cid:115)

πA
ln 2(
|

) + ln (2k/σ)

πB
+
|
|
2N (aB = i)

|

,

ζi(πA, πB, σ) = ˆP(aA = i

aB = i)

|
= i

ˆP(aA

−

aB = i)

|

−

2(cid:15)i(πA, πB, σ),

(cid:54)
(cid:54)
(cid:54)
bi(πA, πB, σ) =

(ˆP(aA

1
ζi(πA, πB, δ)
+ (cid:15)i(πA, πB, σ)),
= π∗(s)),

aB = i)

= i

|

(cid:96)(s, π) = 1(π(s)
(cid:15)A = Es∼ρπA [(cid:96)(s, πA)].

Then with probability 1

σ:

−

(cid:15)A
A)

≤

maxj∈{1,··· ,k} bj(πA, πB, σ),
η(π∗,
uT (cid:15)A,

η(πA,

M

)
M
where T is the time horizon and u is the largest one-step
deviation loss compared with π∗.

−

≥

To obtain a small performance gap compared to π∗, one
must minimize (cid:15)A, which measures the disagreement be-
tween πA and π∗. However, we cannot directly esti-
mate this quantity since we only have limited sample
trajectories from π∗. Alternatively, we can minimize an
upper bound, maxj∈{1,··· ,k} bj(πA, πB, δ), which mea-
sures the maximum disagreement on actions between
πA and πB and, importantly, can be estimated via sam-
ples.
In Section 5.2, we design an algorithm that ap-
proximately minimizes this bound. The advantage of two
views over a single view enables us to establish an upper
bound on (cid:15)A, which is otherwise unmeasureable.

5 THE CoPiEr ALGORITHM

We now present practical algorithms motivated by the
theoretical insights from Section 4. We start with a meta-
algorithm named CoPiEr (Algorithm 1), whose impor-
tant subroutines are EXCHANGE and UPDATE. We
provide two concrete instantiations for the general case
and the special case with a shared action space.

Algorithm 1 CoPiEr (Co-training for Policy Learning)

1: Input: A distribution

of MDPs, two policies

πA, πB, mapping functions fA→B, fB→A

D

B

M

M

2: repeat
3:
4:
5:

Sample
M ∼ D
Run πA on
M
Run πB on
M
B
τ (cid:48)
τ (cid:48)
,
j
i
} ←
{
}
{
UPDATE(πA,
πA
←
UPDATE(πB,
πB
8:
←
9: until Convergence

, form

A,
A to generate trajectories
B to generate trajectories
,

τ A
EXCHANGE(
i }
{
A
τ (cid:48)
τ A
)
,
j
i }
}
{
{
B
τ (cid:48)
τ B
)
,
j
i }
}
{
{

7:

6:

A

τ A
m
i }
i=1
{
n
τ B
j }
j=1
{
τ B
)
j }
{

5.1 GENERAL CASE

m
i=1 and

M
M
M
fA→B(τ A
i )

Algorithm 2 EXCHANGE: General Case
n
τ B
j }
j=1
{
(cid:80)m
A) = 1
i=1 r(τ A
i )
m
(cid:80)n
B) = 1
j=1 r(τ B
j )
n
B) then
m
i=1
}

τ A
1: Input: Trajectories
i }
{
2: Compute estimate ˆη(πA,
3: Compute estimate ˆη(πB,
A) > ˆη(πB,
4: if ˆη(πA,
τ A→B
5:
i
{
τ B→A
6:
j
{
7: else
τ A→B
8:
i
{
τ B→A
9:
j
{
10: end if
11: return

n
fB→A(τ B
j )
j=1
}

} ← ∅
} ← {

} ← {
} ← ∅

M

τ A→B
i

,
}

τ B→A
j
{

}

{

Algorithm 3 UPDATE
1: Input: Current policy π, sampled trajectories from

a

{

(cid:80)m

n
j=1

τi
π,
2: Form
(cid:40)

m
i=1 and demonstrations
}
loss
i=1 r(τi) + λC(π,

τ (cid:48)
j}
{
(π)
=
function
n
τ (cid:48)
j=1), RL with IL loss
j}
−
{
n
τ (cid:48)
λC(π,
j=1), IL loss only
j}
{
π
3: Update π
←

∇L

(π)

−

L

α

the relative quality of the two policies from sampled tra-
jectories (Lines 2-4 in Algorithm 2). Then we use the
trajectories from the better policy as demonstrations for
the worse policy on this MDP. This mirrors the theoreti-
cal insight presented in Section 4, where based on which
sub-distribution an MDP is sampled from, the relative
quality of the two policies is different.

For UPDATE, we can form a loss function that is derived
from either imitation learning or reinforcement learning.
Recall that we aim to optimize the β terms in Theorem 2,
however it is infeasible to directly optimize them. So we
consider a surrogate loss C (Line 2 of Algorithm 3) that
measures the policy difference. In practice, we typically
use behavior cloning loss as the surrogate.

5.2 SPECIAL CASE: SHARED ACTION SPACE

For the special case with a shared action space, we can
collect more informative feedback beyond the trajectory
level. Instead, we collect interactive state-level feedback,
as is popular in imitation learning algorithms such as
DAgger [51] and related approaches [58, 17, 49, 56, 23].
Speciﬁcally, we can use Algorithms 4 & 5 to exchange
actions in a state-coupled manner. This process is de-
picted in Figure 2, where πA’s visited states, sA
0 and sA
1 ,
1 , resulting in receiving πB’s
are mapped to sB
0 and aB
actions, aB

0 and sB
1 , in the exchange.

Algorithm 2 covers the general case for exchanging tra-
jectories generated by the two policies. First we estimate

Unlike the general case where information exchange is
asymmetric, as Theorem 3 indicates, we aim to minimize

(cid:54)
(cid:54)
Algorithm 4 EXCHANGE: Special Case
n
τ B
1: Input: Trajectories
j=1
j }
{
fB→A(τ B
2: DA→B = INTERACTIVE(
j=1, πA)
n
j )
}
{
i=1, πB)
m
3: DB→A = INTERACTIVE(
fA→B(τ A
i )
}
{
4: return DA→B, DB→A

m
i=1 and

τ A
i }

{

Algorithm 5 INTERACTIVE
m
i=1, query policy π
}

τi

{

∅
←

1 to m do
for each state s

1: Input: Trajectories
2: D =
3: for i
4:
5:
6:
7: end for
8: return D

D
←
end for

∪ {

D

τi do
(s, π(s))

∈

}

policy disagreement. Both policies are simultaneously
optimizing this objective, which requires both directions
of information exchange (Lines 2-3 in Algorithm 4). The
update step (Algorithm 3) is the same as the general case.

6 EXPERIMENTS

We now present empirical results on both the special and
general cases of CoPiEr. We demonstrate the gener-
ality of our approach by applying three distinct combi-
nations of policy co-training: reinforcement learning on
both views (Section 6.1), reinforcement learning on one
view and imitation learning on the other (Section 6.2),
and imitation learning on both views (Section 6.3). Fur-
thermore, our experiments on combinatorial optimiza-
tion (Sections 6.2 & 6.3) demonstrate signiﬁcant im-
provements over strong learning-based baselines as well
as commercial solvers, and thus showcase the practical-
ity of our approach. More details about the experiment
setup can be found in the appendix.

6.1 DISCRETE & CONTINUOUS CONTROL:

SPECIAL CASE WITH RL+RL

Setup. We conduct experiments on discrete and contin-
uous control tasks with OpenAI Gym [8] and Mujoco
physical engine [62]. We use the garage repository [19]
to run reinforcement learning for both views.

for policy optimization.

Policy Class. We use a feed-forward neural network with
two hidden layers (64 & 32 units) and tanh activations as
the policy class. For discrete actions, π(s) outputs a soft-
max distribution. For continuous actions, π(s) outputs a
(multivariate) Gaussian. For policy update, we use Pol-
icy Gradient [59] with a linear baseline function [21] and
deﬁne the loss function C in Algorithm 3 to be the KL-
divergence between output action distributions.

Methods Compared. We compare with single view pol-
icy gradient, labelled as “A (PG)” and “B (PG)”, and
with a policy trained on the union of the two views
but test on two views separately, labelled as “A (All)”
and “B (All)”. We also establish an upper bound on
performance by training a model without view splitting
(“A+B”). Each method uses the same total number of
samples (i.e., CoPiEr uses half per view).

Results. Figure 4 shows the results. CoPiEr is able
to converge to better or comparable solutions in almost
all cases except for view A in Hopper. The poor per-
formance in Hopper could be due to the disagreement
between the two policies not shrinking enough to make
Theorem 3 meaningful. As a comparison, at end of the
training, the average KL-divergence for the two policies
is about 2 for Hopper, compared with 0.23 for Swimmer
and 0.008 for Acrobot. One possible cause for such large
disagreement is that the two views have signiﬁcance dif-
ferences in difﬁculty for learning, which is the case for
Hopper by noticing A (PG) and B (PG) have a difference
in returns of about 190.

6.2 MINIMUM VERTEX COVER: GENERAL

CASE WITH RL+IL

Setup. We now consider the challenging combinatorial
optimization problem of minimum vertex cover (MVC).
We use 150 randomly generated Erd˝os-R´enyi [20] graph
instances for each scale, with scales ranging
100-200,
{
200-300, 300-400, 400-500
vertices. For training, we
}
use 75 instances, which we partition into 15 labeled and
60 unlabeled instances. We use the best solution found
by Gurobi within 1 hour as the expert solution for the la-
beled set to bootstrap imitation learning. For each scale,
we use 30 held-out graph instances for validation, and
we report the performance on 45 test graph instances.

Two Views and Features. For each environment, states
are represented by feature vectors, typically capturing lo-
cation, velocity and acceleration. We create two views by
removing different subsets of features from the complete
feature set. Note that both views have the same underly-
ing action space as the original MDP, so it is the special
case covered in Section 5.2. We use interactive feedback

Views and Features. The two views are the graphs
themselves and integer linear programs constructed from
the graphs. For the graph view, we use DQN-based re-
inforcement learning [15] to learn a sequential vertex se-
lection policy. We use structure2vec [14] to com-
pute graph embeddings to use as state representations.
For the ILP, we use imitation learning [23] to learn node

(a) Acrobot Swing-up. A denotes re-
moving the ﬁrst coordinate in the state
vector and B removing the second co-
ordinate.

(b) Swimmer. A denotes removing
all even index coordinates in the state
vector and B removing all odd index
ones.

(c) Hopper. A denotes removing all
even index coordinates in the state
vector and B removing all odd index
ones.

Figure 4: Discrete & continuous control tasks. Experiment results are across 5 random seeded runs. Shaded area
indicates

1 standard deviation.

±

two views, we use DQN [42] with supervised losses [25]
to learn to imitate better demonstrations from the ILP
view. For all our experiments, we determined the reg-
ularizer for the supervised losses and other parameters
through cross-validation on the smallest scale (100-200
vertices). The graph view models are pre-trained with
the labeled set using behavior cloning. We use the same
number of training iterations for all the methods.

For the ILP view, our policy class consists of a node
ranking model that prioritizes which node to visit next.
We use RankNet [9] as the ranking model, instantiated
using a 2-layer neural network with ReLU as activation
functions. We implement our approach for the ILP view
within the SCIP [2] integer programming framework.

Methods Compared. At test time, when a new graph
is given, we run both policies and return the better so-
lution. We term this practical version “CoPiEr Final”
and measure other policies’ performance against it. We
compare with single view learning baselines. For the
graph view, we compare with RL-based policy learning
over graphs [15], labelled as “Graph (RL)”. And for
the ILP view, we compare with imitation learning [23]
“ILP (DAgger)”, retrospective imitation [56] “ILP (Ret-
rospective Imitation)” and a commercial solver Gurobi
[22]. We combine “Graph (RL)” and “ILP (DAgger)” as
non-CoPiEr (Final) by returning the better solution of the
two. We also show the performance of the two policies
in CoPiEr as standalone policies instead of combining
them, labelled “Graph (CoPiEr)” and “ILP (CoPiEr)”.
ILP methods are limited by the same node budget in
branch-and-bound trees.

Results.
Figure 5 shows the results. We see that
CoPiEr Final outperforms all baselines as well as

Figure 5: Comparison of CoPiEr with other learning-
based baselines and a commercial solver, Gurobi. The
y-axis measure relative gaps of various methods com-
pared with CoPiEr Final. CoPiEr Final outperforms all
the baselines. Notably, the gaps are signiﬁcant because
getting optimizing over large graphs is very challenging.

selection policy for branch-and-bound search. A node
selection policy determines which node to explore next
in the current branch-and-bound search tree. We use
node-speciﬁc features (e.g., LP relaxation lower bound
and objective value) and tree-speciﬁc features (e.g., in-
tegrality gap, and global lower and upper bounds) as our
state representations. Vertex selection in graphs and node
selection in branch-and-bound are different. So we use
the general case algorithm in Section 5.1.

Policy Class. For the graph view, our policy class is sim-
ilar to [15]. In order to perform end-to-end learning of
the parameters with labeled data exchanged between the

020406080100Iterations−450−400−350−300−250−200−150−100AverageReturnAcrobotA(CoPiEr)A(PG)A(All)B(CoPiEr)B(PG)B(All)A+B0255075100125150175200Iterations−250255075100125AverageReturnSwimmerA(CoPiEr)A(PG)A(All)B(CoPiEr)B(PG)B(All)A+B0100200300400500Iterations050100150200250300350AverageReturnHopperA(CoPiEr)A(PG)A(All)B(CoPiEr)B(PG)B(All)A+BGurobi. Interestingly, it also performs much better than
either standalone CoPiEr policies, which suggests that
Graph (CoPiEr) is better for some instances while ILP
(CoPiEr) is better on others. This ﬁnding validates com-
bining the two views to maximize the beneﬁts from both.
For the exact numbers on the ﬁnal performance, please
refer to Appendix 8.4.

6.3 RISK-AWARE PATH PLANNING: GENERAL

CASE WITH IL+IL

Setup. We ﬁnally consider a practical application of risk-
aware path planning [46]. Given a start point, a goal
point, a set of polygonal obstacles, and an upper bound
of the probability of failure (risk bound), we must ﬁnd a
path, represented by a sequence of way points, that min-
imizes cost while limiting the probability of collision to
within the risk bound. Details on the data generation can
be found in the Appendix 8.3. We report the perfor-
mance evaluations on 50 test instances.

Views and Features. This problem can be formulated
into a mixed integer linear program (MILP) as well as
a quadratically constrained quadratic program (QCQP),
both of which can be solved using branch-and-bound
[35, 39]. For each view, we learn a node selection pol-
icy for branch-and-bound via imitation learning. Feature
representations are similar to ILP view in MVC experi-
ment (Section 6.2). For the QCQP view, we use the state
variables bounds along the trace for each node from the
root in the branch and bound tree as an additional feature.
Although the search framework is the same, because of
the different nature of the optimization problem formu-
lations, the state and action space are incompatible, and
so we use the general case of CoPiEr. A pictorial repre-
sentation of the two views is presented in Appendix 8.2.

Policy Class. The policy class for both MILP and QCQP
views is similar to that of ILP view in MVC (Section
6.2), and we learn node ranking models.

Methods Compared. Similar to MVC experiment, we
compare other methods with “CoPiEr Final” which re-
turns the better solution of the two. We use single view
learning baselines, speciﬁcally those based on imitation
learning [23], “QCQP (DAgger)” and “MILP(DAgger)”,
and on retrospective imitation [56], “QCQP (Retrospec-
tive Imitation)” and “MILP (Retrospective Imitation)”.
Two versions of non-CoPiEr Final are presented, based
on DAgger and Retrospective Imitation, respectively.
Gurobi is also used to solve MILPs but it is not able to
solve the QCQPs because they are non-convex.

Results. Figure 6 shows the results. Like in MVC,
we again see that CoPiEr Final outperforms baselines
as well as Gurobi. We also observe a similar beneﬁt of

Figure 6: Comparison of CoPiEr with other learning-
based baselines and a commercial solver, Gurobi. The y-
axis measure relative gaps of various methods compared
with CoPiEr Final. CoPiEr Final outperforms all the
baselines. Notably, the scale of problems as measured
by the number of integer variables far exceed previous
state-of-the-art method [56].

aggregating both policies. The effectiveness of CoPiEr
enables solving much larger problems than considered in
previous work [56] (560 vs 1512 binary variables).

7 CONCLUSION & FUTURE WORK

We have presented CoPiEr (Co-training for Policy
Learning), a general framework for policy learning for
sequential decision making tasks with two representa-
tions. Our theoretical analyses and algorithm design
cover both the general case as well as a special case
with shared action spaces. Our approach is compatible
with both reinforcement learning and imitation learning
as subroutines. We evaluated on a variety of settings,
including control and combinatorial optimization. Our
results on showcase the generality of our framework and
signiﬁcant improvements over numerous baselines.

There are many interesting directions for future work.
On the theory front, directions include weakening as-
sumptions such as conditional independence, or extend-
ing to more than two views. On the application front, al-
gorithms such as CoPiEr can potentially improve perfor-
mance in a wide range of robotic and other autonomous
systems that utilize different sensors and image data.

Acknowledgments. The work was funded in part by
NSF awards #1637598 & #1645832, and support from
Raytheon and Northrop Grumman. This research was
also conducted in part at the Jet Propulsion Lab, Cali-
fornia Insitute of Technology under a contract with the
National Aeronautics and Space Administration.

References

[1] Pieter Abbeel and Andrew Y Ng. Apprenticeship learn-
ing via inverse reinforcement learning. In International
Conference on Machine Learning, 2004.

[2] Tobias Achterberg. SCIP : solving constraint integer pro-
grams. Mathematical Programming Computation, 2009.

[3] Maria-Florina Balcan, Avrim Blum, and Ke Yang. Co-
training and expansion: Towards bridging theory and
practice. In Neural information processing systems, 2005.

[4] Mislav Balunovic, Pavol Bielik, and Martin Vechev.
In Neural Information

Learning to solve smt formulas.
Processing Systems, 2018.

[5] Avrim Blum and Yishay Mansour. Efﬁcient co-training of
linear separators under weak dependence. In Conference
on Learning Theory, 2017.

[6] Avrim Blum and Tom Mitchell. Combining labeled and
unlabeled data with co-training. In Conference on Learn-
ing Theory, 1998.

[7] Endre Boros and Peter L Hammer. The max-cut prob-
lem and quadratic 0–1 optimization; polyhedral aspects,
relaxations and bounds. Annals of Operations Research,
1991.

[8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas
Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. Openai gym. arXiv, 2016.

[9] Chris Burges, Erin Renshaw, and Matt Deeds. Learning to
rank using gradient descent. In International conference
on Machine learning, 1998.

[10] Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal,
Hal Daume, and John Langford. Learning to search bet-
ter than your teacher. In International Conference on Ma-
chine Learning, 2015.

[11] Minmin Chen, Kilian Q Weinberger, and John Blitzer.
Co-training for domain adaptation. In Neural information
processing systems, 2011.

[12] Ching-An Cheng, Xinyan Yan, Nolan Wagener, and By-
ron Boots. Fast policy learning through imitation and re-
inforcement. In Conference on Uncertainty in Artiﬁcial
Intelligence, 2018.

[13] Thomas M Cover and Joy A Thomas. Elements of infor-

mation theory. John Wiley & Sons, 2012.

[14] Hanjun Dai, Bo Dai, and Le Song. Discriminative Em-
beddings of Latent Variable Models for Structured Data.
In International Conference on Machine Learning, pages
1–23, 2016.

[15] Hanjun Dai, Elias B Khalil, Yuyu Zhang, Bistra Dilkina,
and Le Song. Learning combinatorial optimization al-
gorithms over graphs. In Neural Information Processing
Systems, 2017.

[16] Sanjoy Dasgupta, Michael L Littman, and David A
McAllester. Pac generalization bounds for co-training.
In Neural information processing systems, 2002.

[17] Hal Daum´e, John Langford, and Daniel Marcu. Search-
based structured prediction. Machine learning, 2009.

[18] Wenceslas Fernandez de la Vega and Claire Kenyon-
Mathieu. Linear programming relaxations of maxcut. In
ACM-SIAM symposium on Discrete algorithms, 2007.

[19] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and
Pieter Abbeel. Benchmarking deep reinforcement learn-
ing for continuous control. In International Conference
on Machine Learning, 2016.

[20] Paul Erd˝os and Alfr´ed R´enyi. On the evolution of random

graphs. Publ. Math. Inst. Hung. Acad. Sci, 1960.

[21] Evan Greensmith, Peter L Bartlett, and Jonathan Baxter.
Variance reduction techniques for gradient estimates in
reinforcement learning. Journal of Machine Learning Re-
search, 2004.

[22] LLC Gurobi Optimization. Gurobi optimizer reference

manual, 2018.

[23] He He, Hal Daume III, and Jason M Eisner. Learning to
search in branch and bound algorithms. In Neural infor-
mation processing systems, 2014.

[24] Peter Henderson, Riashat Islam, Philip Bachman, Joelle
Pineau, Doina Precup, and David Meger. Deep reinforce-
ment learning that matters. In AAAI Conference on Arti-
ﬁcial Intelligence, 2018.

[25] Todd Hester, Olivier Pietquin, Marc Lanctot, Tom Schaul,
Dan Horgan, John Quan, Andrew Sendonaris, Ian Os-
band, Gabriel Dulac-arnold, John Agapiou, and Joel Z
Leibo. Deep Q-Learning from Demonstrations. In AAAI
Conference on Artiﬁcial Intelligence, 2018.

[26] Jonathan Ho and Stefano Ermon. Generative adversar-
ial imitation learning. In Neural Information Processing
Systems, 2016.

[27] Melvin Johnson, Mike Schuster, Quoc V Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fer-
nanda Vi´egas, Martin Wattenberg, Greg Corrado, et al.
Googles multilingual neural machine translation system:
Enabling zero-shot translation. Transactions of the Asso-
ciation for Computational Linguistics, 2017.

[28] Sham Kakade and John Langford. Approximately opti-
mal approximate reinforcement learning. In International
Conference on Machine Learning, 2002.

[29] Bingyi Kang, Zequn Jie, and Jiashi Feng. Policy opti-
mization with demonstrations. In International Confer-
ence on Machine Learning, 2018.

[30] Elias Boutros Khalil, Pierre Le Bodic, Le Song, George L
Nemhauser, and Bistra N Dilkina. Learning to branch
in mixed integer programming. In AAAI Conference on
Artiﬁcial Intelligence, 2016.

[31] Svetlana Kiritchenko and Stan Matwin. Email classiﬁ-
cation with co-training. In Conference of the Center for
Advanced Studies on Collaborative Research, 2011.

[32] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforce-
ment learning in robotics: A survey. The International
Journal of Robotics Research, 2013.

[33] Abhishek Kumar and Hal Daum´e. A co-training approach
for multi-view spectral clustering. In International Con-
ference on Machine Learning, 2011.

[34] Guillaume Lample and Devendra Singh Chaplot. Play-
ing fps games with deep reinforcement learning. In AAAI
Conference on Artiﬁcial Intelligence, 2017.

[35] Ailsa H Land and Alison G Doig. An automatic method
In 50
for solving discrete programming problems.
Years of Integer Programming 1958-2008, pages 105–
132. Springer, 2010.

[36] Hoang Le, Nan Jiang, Alekh Agarwal, Miroslav Dudik,
Yisong Yue, and Hal Daum´e. Hierarchical imitation and
In International Conference on
reinforcement learning.
Machine Learning, 2018.

[52] John Schulman, Sergey Levine, Philipp Moritz, Michael
Jordan, and Pieter Abbeel. Trust region policy optimiza-
tion. In International Conference on Machine Learning,
2015.

[37] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter
Abbeel. End-to-end training of deep visuomotor policies.
The Journal of Machine Learning Research, 2016.
[38] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel,
Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and
Daan Wierstra. Continuous control with deep reinforce-
ment learning. In International Conference on Learning
Representations, 2016.

[39] Jeff Linderoth. A simplicial branch-and-bound algorithm
for solving quadratically constrained quadratic programs.
Mathematical programming, 2005.

[40] Jialu Liu, Chi Wang, Jing Gao, and Jiawei Han. Multi-
view clustering via joint nonnegative matrix factorization.
In SIAM International Conference on Data Mining, 2013.
[41] Azalia Mirhoseini, Hieu Pham, Quoc V Le, Benoit
Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar,
Mohammad Norouzi, Samy Bengio, and Jeff Dean. De-
vice placement optimization with reinforcement learning.
In International Conference on Machine Learning, 2017.
[42] Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller. Playing atari with deep reinforcement
learning. arXiv, 2013.

[43] Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wo-
jciech Zaremba, and Pieter Abbeel. Overcoming explo-
ration in reinforcement learning with demonstrations. In
International Conference on Robotics and Automation,
2018.

[44] Kamal Nigam and Rayid Ghani. Analyzing the effective-
ness and applicability of co-training. In ACM Conference
on Information and knowledge Management, 2000.

[45] Temel

¨Oncan, ˙I Kuban Altınel, and Gilbert Laporte.
A comparative analysis of several asymmetric traveling
salesman problem formulations. Computers & Opera-
tions Research, 2009.

[46] Masahiro Ono and Brian C Williams. An efﬁcient motion
planning algorithm for stochastic dynamic systems with
constraints on probability of failure. In AAAI Conference
on Artiﬁcial Intelligence, 2008.

[47] AJ Orman and HP Williams. A survey of different in-
teger programming formulations of the travelling sales-
man problem. In Optimisation, econometric and ﬁnancial
analysis. Springer, 2007.

[48] Martin L Puterman. Markov decision processes: discrete
stochastic dynamic programming. John Wiley & Sons,
2014.

[49] St´ephane Ross and Drew Bagnell. Efﬁcient reductions for
imitation learning. In International Conference on Artiﬁ-
cial Intelligence and Statistics, 2010.

[50] Stephane Ross and J Andrew Bagnell. Reinforcement
and imitation learning via interactive no-regret learning.
arXiv, 2014.

[53] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv, 2017.

[54] David Silver, Aja Huang, Chris J Maddison, Arthur
Guez, Laurent Sifre, George Van Den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Panneer-
shelvam, Marc Lanctot, et al. Mastering the game of go
with deep neural networks and tree search. Nature, 2016.

[55] Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin. A
co-regularization approach to semi-supervised learning
with multiple views. In ICML workshop on learning with
multiple views, 2005.

[56] Jialin Song, Ravi Lanka, Albert Zhao, Aadyot Bhatnagar,
Yisong Yue, and Masahiro Ono. Learning to search via
retrospective imitation. arXiv, 2018.

[57] Bradly C Stadie, Pieter Abbeel, and Ilya Sutskever. Third-

person imitation learning. arXiv, 2017.

[58] Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron
Boots, and J Andrew Bagnell. Deeply aggrevated: Dif-
ferentiable imitation learning for sequential prediction. In
International Conference on Machine Learning, 2017.

[59] Richard S Sutton, David A McAllester, Satinder P Singh,
and Yishay Mansour. Policy gradient methods for rein-
forcement learning with function approximation. In Neu-
ral information processing systems, 2000.

[60] Umar Syed, Michael Bowling, and Robert E Schapire.
Apprenticeship learning using linear programming. In In-
ternational Conference on Machine Learning, 2008.

[61] Umar Syed and Robert E Schapire. A game-theoretic ap-
proach to apprenticeship learning. In Neural information
processing systems, 2008.

[62] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mu-
joco: A physics engine for model-based control. In Inter-
national Conference on Intelligent Robots and Systems,
2012.

[63] Hado Van Hasselt, Arthur Guez, and David Silver. Deep
reinforcement learning with double q-learning. In AAAI
Conference on Artiﬁcial Intelligence, 2016.

[64] Xiaojun Wan. Co-training for cross-lingual sentiment
classiﬁcation. In Joint conference of ACL and IJCNLP.
Association for Computational Linguistics, 2009.

[65] Wei Wang and Zhi-Hua Zhou. A new analysis of co-
training. In International Conference on Machine Learn-
ing, 2010.

[66] Wei Wang and Zhi-Hua Zhou. Co-training with insufﬁ-
In Asian conference on machine learning,

cient views.
2013.

[67] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt,
Marc Lanctot, and Nando Freitas. Dueling network archi-
tectures for deep reinforcement learning. In International
Conference on Machine Learning, 2016.

[51] St´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A
reduction of imitation learning and structured prediction
to no-regret online learning. In International Conference
on Artiﬁcial Intelligence and Statistics, 2011.

[68] Brian Ziebart, Andrew Maas, J Andrew Bagnell, and
Anind Dey. Maximum entropy inverse reinforcement
learning. In AAAI Conference on Artiﬁcial Intelligence,
2008.

8 APPENDIX

8.1 PROOFS

Proof for Proposition 1:

M

M

with two representations

Proof. We show that maxs DJS(πB(s)
πA(s)) is well-
(cid:107)
A
deﬁned for an MDP
B. From Theorem 1, we know the distribution
and
π(s) can be written with respect to its occupancy mea-
sure ρπ. It is sufﬁcient to show that we can map occu-
pancy measures of πA and πB to a common MDP. By
the deﬁnition of an occupancy measure,

M

ρπ(s, a) = P(π(s) = a)

(cid:88)∞
i=0
= Eτ =(s0,a0,··· ,sn)∼π[

γiP(si = s
|
(cid:88)n

π)

γi1((si, ai) = (s, a))]

i=0

that is to say, the occupancy measure is the expected dis-
counted count of a state-action pair to appear in all pos-
sible trajectories. Since we have trajectory mappings be-
B, we can convert an occupancy mea-
tween
B by mapping each trajectory
sure in
and perform the count in the new MDP representation.
Formally, the occupancy measure ρB
B can
be mapped to an occupancy measure in

A and
M
A to one in

M
M

M

M

πB of πB in
A by
M

ρA
πB (s, a)
= E

τ B ∼πB ,
fB→A(τ B )=(s0,a0,··· ,sn)

[

(cid:88)n

i=0

γi1((si, ai) = (s, a))]

we

M

can

from
this,
πA(s)) using any s in

compute
Following
DJS(πB(s)
A. And the
(cid:107)
maximum is deﬁned. In the deﬁnition, there is a choice
B or
whether to map πA’s occupancy measure to
A. Though both approaches lead to a valid
πB’s to
deﬁnition, we use the deﬁnition that for DJS(
), we
always map the representation in the ﬁrst argument to
that of the second argument. It is preferable to the other
one because in Theorem 2, we want to optimize

M

M

·(cid:107)·

J(π(cid:48)A)

≥

JπA (π(cid:48)A)

−

by optimizing

2γA(4βB
D2
(1

(cid:15)B
D2
γA)2

+ αA

D(cid:15)A
D)

−

+ δ2

βB
D2

= EM∼D2 [maxs DJS(πB(s)

πA(s))]
(cid:107)
w.r.t. πA. If
usually via computing the gradient of βB
D2
B, the gradient will
we use fA→B to map from
M
involve a complex composition of fA→B and πA, which
is undesirable.

A to

M

To prove Theorem 2, we need to use a policy improve-
ment result for a single MDP (a modiﬁed version of The-
orem 1 in [29]).

, an expert policy
Theorem 4. Assume for an MDP
πE have a higer advantage of over a policy π with a
margin, i.e., η(πE,
η(π,

δ Deﬁne

M

)
M

−

)
M

≥

α = maxs DKL(π(cid:48)(s)
π(s))
(cid:107)
β = maxs DJS(π(cid:48)(s)
πE(s))
(cid:107)
(cid:15)πE = maxs,a
(cid:15)π = maxs,a

AπE (s, a)
|
|
Aπ(s, a)

|

|

then η(π(cid:48),

)
M

≥

ηπ(π(cid:48),

)
M

−

2γ(4β(cid:15)πE +α(cid:15)π)
(1−γ)2

+ δ

≥

Proof. The only difference from the original theorem is
that the original assumes EaE ∼πE (s),a∼π(s)[Aπ(s, aE)
−
δ(cid:48) > 0 for every state s. It is a stronger
Aπ(s, a)]
assumption which is not needed in their analysis. No-
tice that the advantage of a policy over itself is zero, i.e.,
Ea∼π(s)[Aπ(s, a)] = 0 for every s, so the margin as-
sumption simpliﬁes to EaE ∼πE (s)[Aπ(s, aE)]
By the policy advantage formula,

δ(cid:48).

≥

η(πE,

)
M

−

η(π,

M

(cid:88)∞
i=0

γiAπ(si, ai)]

(cid:88)∞
i=0

γiAπ(si, ai)]

Eai∼πE (si)[
[δ(cid:48) (cid:88)∞

γi]

i=0

) = Eτ ∼πE [
= Esi∼ρπE
Esi∼ρπE
δ(cid:48)

≥

=

γ

1

−

So an assumption on per-state advantage translates to a
overall advantage. Thus we can make this weaker as-
sumption which is also more intuitive and the original
statement still holds with a different δ term.

Proof of Theorem 2:

Proof. Theorem 2 is a distributional extension to the the-
orem above. For

2, let δM = η(πB,

B)

M ∼ D

M

−

η(πA,

A).

M

J(π(cid:48)A)
= EM∼D[η(π(cid:48)A,
= EM∼D1 [η(π(cid:48)A,
EM∼D1 [η(π(cid:48)A,

≥
EM∼D2 [ηπA (π(cid:48)A,

M

M

M

A)]
A)] + EM∼D2[η(π(cid:48)A,
A)]+

A)]

M

A)

M

−

2γA(4β(cid:15)πB + α(cid:15)πA)

γA)2

(1
−
2γAα(cid:15)πA
γA)2 ]+
(1

−
2γA(4β(cid:15)πB + α(cid:15)πA)

−

EM∼D1 [ηπA((π(cid:48)A,

≥

M

A)

EM∼D2 [ηπA (π(cid:48)A,

A)

M

−

= EM∼D[ηπA (π(cid:48)A,

A)]

EM∼D2 [

2γA
(1

·
−

JπA(π(cid:48)A)

−

≥

M

−
4β(cid:15)πB
γA)2 ] + EM∼D2 [δM]
D(cid:15)A
2γA(4βB
(cid:15)B
D)
D2
D2
γA)2
(1

+ αA

−

+ δM]

(1

−
EM∼D[

γA)2
2γAα(cid:15)πA
γA)2 ]
(1
−

−

The derivation for J(π(cid:48)B) is the same.

−

1 loss, (cid:96)(s, π) = 1(π(s)

Finally, we provide the proof for Theorem 3. We ﬁrst
quantify the performance gap between a policy π and an
optimal policy π∗. For a policy that is able to achieve (cid:15)
= π∗(s)), measured against
0
π∗’s action choices under its own state distributions, then
we can bound the performance gap. Let Qπ(cid:48)
t (s, π) denote
the t-step cost of executing π in initial state s and then
following policy π(cid:48)
Theorem 5. (Theorem 2.2 from [51], adpated to our
notations) Let π be such taht Es∼ρπ [(cid:96)(s, π)] = (cid:15), and
Qπ∗
T −t+1(s, π∗)
1, 2,
{
Thus the important quantity to measure is (cid:15), and by mea-
suring the disagreements between two policies in two
views, we can upper bound (cid:15). The result is originally
stated in the context of classiﬁcation, and the above theo-
rem justiﬁes the learning reduction approach of reducing
policy learning to classiﬁcation.

Qπ∗
, then η(π,
}

u for all action a, t

T −t+1(s, a)

≤
η(π∗,

)
M

uT (cid:15).

M

· · ·

, T

−

−

≥

∈

)

Theorem 6. (Corollary 5 in [16] applied to full clas-
siﬁers) Using the deﬁnitions in Theorem 3, with prob-
ability 1
σ over the choice of a sample set N , for
all pairs of classiﬁers h1, h2 such that for all i we have
ζi(h1, h2, σ) > 0 and bi(h1, h2, σ)

−

1.

≤

maxj∈{1,··· ,k} bj(h1, h2, σ)

(cid:15)

≤

Proof. The only change from the original proof is that
instead of a partial classiﬁer which can output
, we
consider a full classiﬁer. Then we could eliminate the

⊥

+ δM]

Figure 7: Two views for Risk-Aware Path Planning. On
the left, the obstacle is enclosed by a polytope (MILP
view) and on the right the obstacle is enclosed by an el-
lipse (QCQP view).

estimates for P(h1
) and the error introduced by
converting a partial classiﬁer to a full classiﬁer via ran-
dom labelling when the output is

=

⊥

.

⊥

+ δ2

Proof of Theorem 3:

Proof. For the bound for πA, we are measuring (cid:15)A on its
sampled paths. Then directly apply Theorem 6 gives an
upper bound on (cid:15)A. Apply Theorem 5 gives the result of
Theorem 3.

8.2 PICTORIAL REPRESENTATION OF THE
TWO-VIEWS IN RISK-AWARE PATH
PLANNING:

We present a pictorial representation of the two differ-
ent views used in the experiments in Fig 7. In the MILP
view, the constraint space is represented using additional
auxiliary binary variables to choose the active side of
the polytope, whereas in the QCQP view, the constraint
space can be encoded in a quadratic constraint.

8.3 RISK-AWARE PLANNING DATASET

GENERATION:

y

x

≤

≤

≤

≤

1, 0

We generate 150 obstacle maps. Each map contains 10
rectangle obstacles, with the center of each obstacle cho-
sen from a uniform random distribution over the space
0
1. The side length of each ob-
stacle was chosen from a uniform distribution in range
[0.01, 0.02] and the orientation was chosen from a uni-
form distribution between 0◦ and 360◦. In order to avoid
trivial infeasible maps, any obstacles centered close to
the destination are removed. For MILP view, we directly
use the randomly generated rectangles for deﬁning the
constraint space. However, for the QCQP view, we en-
close the rectangle obstacles with a circle for deﬁning the
quadratic constraint.

(cid:54)
(cid:54)
8.4 DISCRETE/CONTINUOUS CONTROL

RESULTS IN TABULAR FORM

Acrobot

Swimmer

Hopper

A (CoPiEr)
A (PG)
A (All)
B (CoPiEr)
B (PG)
B (All)
A + B

86.44
169.57
252.42
88.48
257.16
251.74
86.42

−
−
−
−
−
−
−

±
±
±
±
±
±
±

10.80
10.48
8.73
15.13
10.93
9.65
3.48

106.35
109.09
100.36
104.16
103.48
96.74
108.71

23.11
21.58
22.37
19.32
21.89
19.57
5.03

±
±
±
±
±
±
±

217.83
278.66
49.39
168.88
89.34
22.59
346.53

30.03
32.87
10.35
18.21
4.89
5.55
5.91

±
±
±
±
±
±
±

