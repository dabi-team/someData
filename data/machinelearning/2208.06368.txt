2
2
0
2

t
c
O
7
1

]
L
M

.
t
a
t
s
[

2
v
8
6
3
6
0
.
8
0
2
2
:
v
i
X
r
a

MARKOV OBSERVATION MODELS

BY MICHAEL A. KOURITZIN1,a,

1Department of Mathematical and Statistical Sciences, University of Alberta, amichaelk@ualberta.ca

Herein, the Hidden Markov Model is expanded to allow for Markov
chain observations. In particular, the observations are assumed to be a Markov
chain whose one step transition probabilities depend upon the hidden Markov
chain. An Expectation-Maximization analog to the Baum-Welch algorithm is
developed for this more general model to estimate the transition probabil-
ities for both the hidden state and for the observations as well as to esti-
mate the probabilities for the initial joint hidden-state-observation distribu-
tion. A believe state or ﬁlter recursion to track the hidden state then arises
from the calculations of this Expection-Maximization algorithm. A dynamic-
programming analog to the Viterbi algorithm is also developed to estimate
the most likely sequence of hidden states given the sequence of observations.

1. Introduction. Hidden Markov models (HMMs) were introduced in a series of papers
by Baum and collaborators [1], [2]. Traditional HMMs have enjoyed termendous success in
applications like computational ﬁnance [28], single-molecule kinetic analysis [27], animal
tracking [32], forcasting commodity futures [12] and protein folding [34]. In HMMs the
unobservable hidden states X are a discrete-time Markov chain and the observations process
Y is some distorted, corrupted partial information or measurement of the current state of X
satisfying the condition

P (cid:0)Yn ∈ A(cid:12)
These probabilities, P (cid:0)Yn ∈ A(cid:12)

(cid:12)Xn

(cid:12)Xn, Xn−1, ..., X1

(cid:1) = P (cid:0)Yn ∈ A(cid:12)

(cid:12)Xn

(cid:1) .

(cid:1), are called the emission probabilities.

This type of observation modeling can be limiting. Consider observations Y consisting of
daily stock price and volume that are based upon a hidden (bullish/bearish type) market state
X. If there was really just an emission probability, the prior day’s price and volume would be
completely forgotten and a new one would be chosen randomly only depending solely upon
the market bull/bear state. Clearly, this is not what happens. The next day’s price and volume
is related to the prior day’s in some way. Perhaps, prices are held in a range by recent earnings
or volume is elevated for several days due to some company news. Indeed, the autoregressive
HMM (AR-HMM) was been introduced because the (original) HMM does not allow for an
observation to depend upon a past observation. For the AR-HMM the observations take the
structure:

0

p

+ β(Xn)
1

Yn = β(Xn)

Yn−1 + · · · + β(Xn)

Yn−p + εn,

(1)
where {εn}∞
n=1 are a (usually zero-mean Gaussian) i.i.d. sequence of random variables and
the autoregressive coefﬁcients are functions of the current hidden state Xn. Most critically,
one might view this AR-HMM as a linear, Gaussian partial patch to the HMM deﬁciency
and expect a more general, useful theory. Still, the AR-HMM has experienced tremendous
success in applications like speech recognition (see [5]), diagnosing blood infections (see
[33]) and the study of climate patterns (see [39]). Finally, the most general model that truly
incorporates (possibly non-linear) dependencies of Yn on past values of Y is referred to

MSC2020 subject classiﬁcations: Primary 62M05, 62M20; secondary 60J10, 60J22.
Keywords and phrases: Markov Observation Models, Hidden Markov Model, Baum-Welch Algorithm,

Expectation-Maximization, Viterbi Algorithm, Filtering.
1

 
 
 
 
 
 
2

as Markov-switching models or sometimes Markov jump systems. These are very general
models that are particularly important in ﬁnancial applications. However, as mentioned in
[6] the analyses of Markov-switching models can be far more intricate than those of HMM
due to the fact that the properties of the observed process are not directly controlled by those
of the hidden chain. We will at least establish Baum-Welch-like and Viterbi-like algorithms
for estimating (initial and transition) probabilities and the most likely sequence from the
observed data for such systems in the discrete setting. We refer to our models as Markov
Observation Models (MOM) because we set them in one particular Markov only form and
do not tie them to any ﬁnancial (or other) application.

Perhaps, the most important goals of HMM are calibrating the model, real-time believe
state propagation, i.e. ﬁltering, and decoding the whole hidden sequence from the observation
sequence. The ﬁrst problem is solved mathematically in the HMM setting by the Baum-Welch
re-estimation algorithm, which is an application of the Expectation-Maximization (EM) al-
gorithm, predating the EM algorithm. The ﬁltering problem is also solved effectively using
a recursive algorithm that is similar to part of the Baum-Welch algorithm. In practice, there
can be numeric problems like a multitude of local maxima to trap the Baum-Welch algorithm
or inefﬁcient matrix operations when the state size is large but the hidden state resides in a
small subset most of the time. In these cases, it can be adviseable to use particle ﬁlters or
other alternative methods, which are not the subject of this note (see instead [6] for more
information). The forward and backward propagation probabilities of the Baum-Welch al-
gorithm also tend to get very small over time. While satisfactory results can sometimes be
obtained by (often logarithmic) rescaling, this is still a severe problem limiting the use of
the Baum-Welch algorithm (see more explanation within). Our raw algorithms for the more
general Markov observation models will also share these difﬁculties but as a secondary con-
tribution we will explain how to avoid this small number problem when we give our ﬁnal
pseudocode so our EM algorithm will truly apply to many big data problems.

The optimal complete-observation sequence decoding problem in the HMM case is solved
by the Viterbi algorithm (see [37], [30]), which is a dynamic programming type algorithm.
Given the sequence of observations {Yi}N
i=1 and the model probabilities, the Viterbi algo-
rithm returns the most likely hidden state sequence {X ∗
i=1. The Viterbi algorithm is a
forward-backward algorithm like the Baum-Welch algorithm and hence computer efﬁcient
but not real time. The most natural applications of the Viterbi algorithm are perhaps speech
recognition [30] and text recognition [31]. We develop a Markov observation model general-
ization to the Viterbi algorithm and explain how to handle the small number problem in this
algorithm as well.

i }N

The HMM can be thought of as a nonlinear generalization of the earlier Kalman ﬁlter
(see [17], [18]). Nonlinear ﬁltering theory is another related generalization of the Kalman ﬁl-
ter and has many cellebrated successes like the Fujisaki-Kallianpur-Kunita and the Duncan-
Mortensen-Zakai equations (see e.g. [40], [15], [23] for some of the original work and [22],
[24] for some of the more recent general results). The hidden state, called signal in nonlinear
ﬁltering theory, can be a general Markov process model and live in a general state space but
there is no universal EM algorithm for identifying the model like the Baum-Welch algorithm
nor dynamic programming algorithm for identifying a most likely hidden state path like the
Viterbi algorithm. Rather the goals are usually to compute ﬁlters, predictors and smoothers,
for which there are no exact closed form solutions, except in isolated cases (see [20]), and
approximations have to be used. Like HMM, nonlinear ﬁltering has enjoyed widespread ap-
plication. For instance, the subﬁeld of nonlinear particle ﬁltering, also known as sequential
Monte Carlo, has a number of powerful algorithms (see [29], [13], [21], [8]) and has been
applied to numerous problems in areas like bioinformatics [16], economics and mathematical
ﬁnance [9], intracellular movement [26], fault detection [11], pharmacokinetics [4] and many

MARKOV OBSERVATION MODELS

3

other ﬁelds. Still, like HMM, the observations in nonlinear ﬁlter models are largely limited to
distorted, corrupted, partial observations of the signal with very few limited exceptions like
[10].

The purpose of this note is to promote a class of Markov Observation Models (MOM)
that will be shown to subsume the HMM and AR-HMM models in the next section. MOM is
also very different than the models considered in non-linear ﬁltering. Hence, to the author’s
knowledge, MOM represents a practically important class of models to analyze and apply
to real world problems. Both the Baum-Welch and the Viterbi algorithms will be extended
to these MOM models as, together with the model itself, the main contributions. A real-time
ﬁltering recursion is also extended. It should be noted that our EM and dynamic programming
generalizations of the Baum-Welch and Viterbi algorithms include new methods for handling
an unseen ﬁrst observation that is not even part of the HMM model. Finally, the small number
problem encountered in HMM and the raw MOM algorithms is resolved.

The layout of this note is as follows. In the next section, we give our model as well as our
main notation. In Section 3, we apply EM techniques to derive an analog to the Baum-Welch
algorithm for identifying the system (probability) parameters. In particular, joint recursive
formulas for the hidden state transition probabilities, observation transition probabilities and
the initial joint hidden-observation state distribution are derived. Section 4 translates these
formula into a pseudocode implementation of our EM algorithm. More calculations and ex-
planations are included to explain how we avoid the small number problem often encountered
in HMM. Section 5 is devoted to connecting the limit points of the EM type algorithm to the
maxima of the conditional likelihood given the observations. Section 6 contains our real-time
ﬁlter process recursion and our forward-backward most likely hidden sequence detection.
Speciﬁcally, it contains our dynamic programming analog to the Viterbi algorithm for MOM
as well as its derivation and pseudocode implementation. Finally, Section 7 features a sim-
ple application of our (Baum-Welch-like) EM and our (Viterbi-like) dynamic programming
algorithms on real bitcoin data to detect uptrends.

2. Model. Suppose N is some positive integer (representing the ﬁnal time) and O is
some discrete observation space. In our model, like HMM, the hidden state is a homogeneous
Markov chain X on some discrete (ﬁnite or countable) state space E with one step transition
probabilities denoted by px→x(cid:48) for x, x(cid:48) ∈ E. However, in contrast to HMM, we allow self
dependence in the observations. (This is illustrated by right arrows between the Y ’s in Figure
1 below.) In particular, given the hidden state {Xi}N
i=0, we take the observations to be a
(conditional) Markov chain Y with transitions probabilities

(2)

(cid:16)

P

Yn+1 = y

(cid:12)
(cid:12){Xi = xi}N
(cid:12)

i=0, Yn = yn

(cid:17)

= qyn→y(xn+1) ∀x0, ..., xN ∈ E; y, yn ∈ O

that do not affect the hidden state transitions in the sense

(3)

P (Xn+1 = x(cid:48)(cid:12)
(cid:12)Xn = x, {Xi}i<n, {Yj}j≤n) = px→x(cid:48), ∀x, x(cid:48) ∈ E, n ∈ N0
(cid:12)

still. This means that

(4)

(cid:16)

P

Yn+1 = y

(cid:12)
(cid:12){Xi}N
(cid:12)

i=0, Yn

(cid:17)

(cid:16)

= P

Yn+1 = y

(cid:12)
(cid:12)
(cid:12)Xn+1, Yn

(cid:17)

, ∀y ∈ O

i.e. that the new observation only depends upon the new hidden state (as well as the past

observation), and also that the hidden state, observation pair

is jointly Markov (in

(cid:19)

(cid:18) X
Y

addition to the hidden state itself being Markov) with joint one step transition probabilities
(cid:17)

(cid:16)

P

Xn+1 = x, Yn+1 = y

= pxn→x qyn→y(x) ∀x, xn ∈ E; y, yn ∈ O.

(cid:12)
(cid:12)
(cid:12)Xn = xn, Yn = yn

4

prior X

prior Y

X0

Y0

X1

X2

X3

XN

Y1

Y2

Y3

YN

Obs 1
shaded values: not observed;
unshaded: observed;

Obs 2

Obs 3

Obs N

X0, Y0: not part of normal HMM
X0, X1, Y0: Estimated together in Viterbi

FIG 1. Markov Observation Model Structure

The joint Markov property then implies that

(cid:16)

P

Xn+1 = x, Yn+1 = y

(cid:12)
(cid:12)
(cid:12)X1 = x1, Y1 = y1, X2 = x2, Y2 = y2, ..., Xn = xn, Yn = yn

(cid:17)

=

pxn→xqyn→y (x) .

Notice that this generalizes the emisson probability to

P (cid:0)Yn ∈ A(cid:12)

(cid:12)Xn, Xn−1, ..., X1; Yn−1, ..., Y1

(cid:1) = P (cid:0)Yn ∈ A(cid:12)

(cid:12)Yn−1, Xn

(cid:1) = qYn−1→Yn (Xn)







so MOM generalizes HMM by just taking qYn−1→y (Xn) = bXn(y), a state dependent proba-
bility mass function. To see that MOM generalizes AR-HMM, we re-write (1) as
β(Xn)
3
0
0
. . .
0

· · · β(Xn)
p
0
· · ·
· · ·
0
...
· · · 1

β(Xn)
0
0
0
...
0

β(Xn)
1
1
0
...
0

β(Xn)
2
0
1

























































+ εn

(5)

=

+











0

0

,

(cid:124)

(cid:125)

(cid:124)

(cid:125)

Yn
Yn−1
Yn−2
...
Yn−p+1
(cid:123)(cid:122)
Yn

Yn−1
Yn−2
Yn−3
...
Yn−p
(cid:123)(cid:122)
Yn−1

which, given the hidden state Xn, gives an explicit formula for Yn in terms of only Yn−1 and
some independent noise εn. Hence, {Yn} is obviously conditionally Markov and {(Xn, Yn)}
is a MOM.

A subtly that arises with our Markov Observation Model (MOM) over HMM is that we
need an enlarged initial distribution since we have a Y0 that is not observed (see Figure 1).
Rather, we think of starting up the observation process at time 1 even though there were
observations to be had prior to this time. Further, since we generally do not know the model
parameters, we need means to estimate this initial distribution

P (X0 ∈ dx0, Y0 ∈ dy0) = µ (dx0, dy0).

2.1. Key Notation.

• We will use the shorthand notation P (Y1, ..., Yn) for P (Y1 = y1, ..., Yn = yn) |y1=Y1,...,yn=Yn.
• αk

(cid:12)
n(x) = P k(Xn = x, Y1, ..., Yn) and βk
(cid:12)Xn = x, Yn) (both de-
ﬁned differently when n = 0 below) are probabilities computed using the current estimates
pk
x→x(cid:48), qk
n(x)
will be key variables in the forward respectively backward propagation step of our raw
Baum-Welch-like EM algorithm for estimating the transition and initial probabilities. For
notational ease, we will drop the fact P depends on k hereafter.

y→y(cid:48)(x) and µk(x, y) of the transition and initial probabilities. αk

n(x) = P k(Yn+1, ..., YN

n(x) and βk

MARKOV OBSERVATION MODELS

5

• The ﬁlter πk

n(x) = P (Xn = x(cid:12)

n(x) P (Y1,...,Yn)

(cid:12)Y1, ..., Yn) and χk
P (Y1,...,YN −1) are used in our
reﬁned Baum-Welch-like algorithm to replace αk
n of the raw algorithm in
order to solve the small number problem discussed below. Whereas αk
n(x)βk
n(ξ) is often
the product of two tiny unequally sized factors, πk
n are scaled to always be man-
n(x)χk
ageable factors. Yet, πk
n satisfy nice forward
and backward recursions so they are efﬁcient to compute and our reﬁned EM algorithm for
MOM is efﬁcient and avoids the small number problem.

n(x) = βk
n respectively βk

P (Y1,...,YN −1) and both πk

n(ξ) = αk

n and χk

n and χk

n(x)βk

n(ξ)

• δn(x) =

max
y0;x0,x1,...,xn−1

P (Y0 = y0; X0 = x0, X1 = x1, ..., Xn−1 = xn−1; Xn = x; Y1, ..., Yn)

is the key internal function in our Viterbi-like dynamic programming algorithm for deter-
mining the most likely sequence of hidden states. δn also suffers from the small number
problem as it tends to get ridiculously small as n increases. However, since there is only
one factor it is easy to scale and scaling each δn does not affect Viterbi-like algorithm, we
can replace δn with a properly scaled version γn below.

3. Probability Estimation via EM algorithm.

In this section, we develop a recursive
expectation-maximum algorithm that can be used to create convergent estimates for the tran-
sition and initial probabilities of our MOM models. We leave the theoretical justiﬁcation of
convergence to Section 5.

The main goal of developing an EM algorithm would be to ﬁnd px→x(cid:48) for all x, x(cid:48) ∈ E,
qy→y(cid:48)(x) for all y, y(cid:48) ∈ O, x ∈ E and µ(x, y) for all x ∈ E, y ∈ O. Noting every time step is
considered to be a transition in a discrete-time Markov chain, we would ideally set:

(6)

(7)

px→x(cid:48) =

qy→y(cid:48)(x) =

transitions x to x(cid:48)
occurrences of x
transitions y to y(cid:48) when x is true
occurrences of y when x is true

.

Here, ‘when x is true’ means when the hidden state is in state x. However, we can never see
x nor x(cid:48) in MOM from our data so we must estimate when they are true. Hence, we replace
the above with

(8)

px→x(cid:48) =

Expected transitions x to x(cid:48)
Expected occurrences of x

=

N
(cid:80)
n=1

P (Xn−1 = x, Xn = x(cid:48)(cid:12)
(cid:12)
(cid:12)Y1, ..., YN )

N
(cid:80)
n=1

P (Xn−1 = x

(cid:12)
(cid:12)
(cid:12)Y1, ..., YN )

(9)

qy→y(cid:48)(x) =

Expected transitions y to y(cid:48) when x is true
Expected occurrences of y when x is true

1Y1=y(cid:48)P (Y0 = y, X1 = x

=

P (Y0 = y, X1 = x

(cid:12)
(cid:12)
(cid:12)Y1, ..., YN ) +
(cid:12)
(cid:12)
(cid:12)Y1, ..., YN ) +

N
(cid:80)
n=2
N
(cid:80)
n=2

1Yn−1=y,Yn=y(cid:48)P (Xn = x

(cid:12)
(cid:12)
(cid:12)Y1, ..., YN )

(cid:12)
(cid:12)
(cid:12)Y1, ..., YN )
1Yn−1=yP (Xn = x

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)Y1, ..., YN ), P (Xn = x
(cid:12)Y1, ..., YN ) for
which means we must compute P (Y0 = y, X1 = x
all 0 ≤ n ≤ N and P (Xn−1 = x, Xn = x(cid:48)(cid:12)
(cid:12)
(cid:12)Y1, ..., YN ) for all 1 ≤ n ≤ N to get these two

6

transition probability estimates. However, let

(10)

and

(11)

(cid:26) α0 (x, y) = P (Y0 = y, X0 = x)

αn (x) = P (Y1, ..., Yn, Xn = x) , 1 ≤ n ≤ N





β0 (x1, y) = P

βn (xn+1) = P

βN −1 (xN ) = P

(cid:16)

(cid:16)

(cid:16)

Y1, ..., YN

(cid:12)
(cid:12)
(cid:12)X1 = x1, Y0 = y

(cid:17)

(cid:12)
(cid:12)
(cid:12)Xn+1 = xn+1, Yn
(cid:17)

Yn+1, ..., YN

YN

(cid:12)
(cid:12)
(cid:12)XN = xN , YN −1

= qYN −1→YN (xN )

(cid:17)

, ∀0 < n < N − 1

.

Notice we include an extra variable y in α0, β0. This is because we do not see the ﬁrst obser-
vation Y0 so we have to consider all possibilities and treat it like another hidden state. Now,
by Bayes’ rule, (11) and (10)

(12)

P (Y0 = y, X1 = x

(cid:12)
(cid:12)
(cid:12)Y1, ..., YN )

=

=

P (Y1, ..., YN

(cid:12)
(cid:12)X1 = x, Y0 = y)P (X1 = x, Y0 = y)

P (Y1, ..., YN )

β0(x, y) (cid:80)
x0
(cid:80)
ξ αN (ξ)

px0→xα0(x0, y)

.

Next, by the Markov property and (10)

(13)

P (Xn−1 = x, Xn = x(cid:48)(cid:12)
(cid:12)
(cid:12)Y1, ..., YN )
P (Xn−1 = x, Xn = x(cid:48), Y1, ..., YN )
P (Y1, ..., YN )

αn−1(x)P (Xn = x(cid:48), Yn, ..., YN

(cid:12)
(cid:12)
(cid:12)Xn−1 = x, Y1, ..., Yn−1)

P (Y1, ..., YN )

αn−1(x)P (Xn = x(cid:48), Yn, ..., YN

(cid:12)
(cid:12)
(cid:12)Xn−1 = x, Yn−1)

P (Y1, ..., YN )

=

=

=

so by (3,4,11,10)

(14)

=

=

=

=

P (Xn−1 = x, Xn = x(cid:48)(cid:12)
(cid:12)
(cid:12)Y1, ..., YN )
αn−1(x)P (Xn = x(cid:48), Yn, ..., YN , Xn−1 = x, Yn−1)P (Xn = x(cid:48), Xn−1 = x, Yn−1)
P (Y1, ..., YN )P (Xn = x(cid:48), Xn−1 = x, Yn−1)P (Xn−1 = x, Yn−1)
(cid:12)Xn = x(cid:48), Xn−1 = x, Yn−1)P (Xn = x(cid:48)(cid:12)
(cid:12)

αn−1(x)P (Yn, ..., YN

(cid:12)Xn−1 = x, Yn−1)

αn−1(x)P (Yn, ..., YN

(cid:12)Xn = x(cid:48), Yn−1)P (Xn = x(cid:48)(cid:12)
(cid:12)

(cid:12)Xn−1 = x)

P (Y1, ..., YN )

P (Y1, ..., YN )

αn−1(x)βn−1(x(cid:48))px→x(cid:48)
ξ αN (ξ)

(cid:80)

for n = 2, 3, ..., N .

MARKOV OBSERVATION MODELS

7

REMARK 3.1. The Baum-Welch algorithm for regular HMM also constructs the joint

conditional probability in (14). In the HMM case, the numerator in (14) looks like

P (Xn−1 = x, Xn = x(cid:48), Y1, ..., YN )=

αn−1
(cid:125)(cid:124)

(cid:122)
(cid:123)
P (Xn−1 = x, Y1, ..., Yn−1) P (Xn = x(cid:48)|Xn−1 = x)

∗ P (Yn+1, ..., YN |Xn = x(cid:48))
(cid:123)(cid:122)
(cid:125)
their βn−1

(cid:124)

P (Yn|Xn = x(cid:48)),

which works well when the observations are conditionally independent. However, this multi-
plication rule does not apply in our more general Markov observations case. Moreover, there
is no conditional independence so

P (Xn = x(cid:48), Yn, ..., YN |Xn−1 = x, Yn−1)
(cid:54)= P (Xn = x(cid:48)|Xn−1 = x, Yn−1)P (Yn, ..., YN |Xn−1 = x, Yn−1).

Our new strategy is to deﬁne

βn−1(x(cid:48)) = P (Yn, ..., YN |Xn = x(cid:48), Yn−1)

and note that

βn−1(x(cid:48)) = P (Yn, ..., YN |Xn = x(cid:48), Xn−1 = x, Yn−1)

for all x. Surprisingly, with such modest changes, the algorithms making HMM such a pow-
erful tool translate to the more general MOM models.

It follows from (14) that

(15)

(cid:12)
(cid:12)
P (Xn = x
(cid:12)Y1, ..., YN ) = αn(x)

(cid:88)

xn+1

βn(xn+1)px→xn+1
ξ αN (ξ)

(cid:80)

for n = 1, 2, ..., N − 1 and

(16)

P (Xn = x

(cid:12)
(cid:12)
(cid:12)Y1, ..., YN ) = βn−1 (x)

(cid:88)

xn−1

pxn−1→xαn−1(xn−1)

αN (ξ)

(cid:80)
ξ

(17)

for n = 2, 3, ..., N . Similarly to (13,14), one has that
P (X0 = x, X1 = x(cid:48)(cid:12)
(cid:12)
(cid:12)Y1, ..., YN )
(cid:80)
y P (X0 = x, Y0 = y)P (X1 = x(cid:48); Y1, ..., YN

=

=

and so

(18)

P (Y1, ..., YN )

(cid:80)

y α0(x, y)px→x(cid:48)β0(x(cid:48), y)
ξ αN (ξ)

(cid:80)

P (X0 = x

(cid:12)
(cid:12)
(cid:12)Y1, ..., YN ) =

(cid:88)

x(cid:48)

(cid:80)

y α0(x, y)px→x(cid:48)β0(x(cid:48), y)
αN (ξ)

.

(cid:80)
ξ

(cid:12)
(cid:12)X0 = x, Y0 = y)

αn and βn are computed recursively below using the prior estimates of px→x(cid:48) , qy→y(cid:48) (x) and
µ.

8

Recalling that there are prior observations that we do not see, we must also estimate an ini-
tial joint distribution for an initial hidden state and observation. An expectation-maximization
argument for the initial distribution leads one to the assignment

(19)

µ(x, y)= P (X0 = x, Y0 = y

(cid:12)
(cid:12)
(cid:12)Y1, ..., YN )

P (Y1, ..., YN

(cid:12)
(cid:12)
(cid:12)X0 = x, Y0 = y)P (X0 = x, Y0 = y)

P (Y1, ..., YN )

=

for all x ∈ E, y ∈ O, which is Bayes’ rule.

Expectation-maximization algorithms use these types of formula and prior estimates to
produce better estimates. We take estimates for px→x(cid:48), qy→y(cid:48) (x) and µ(x, y) and get new
estimates for these quantities iteratively using (8), (17), (14), (18) and (15):

(20)

p(cid:48)
x→x(cid:48) =

α0(x, y)px→x(cid:48)β0(x(cid:48), y) +

(cid:80)
y

N −1
(cid:80)
n=1

αn(x)px→x(cid:48)βn(x(cid:48))

(cid:80)
y

(cid:80)
x1

α0(x, y)px→x1β0(x1, y) +

N −1
(cid:80)
n=1

(cid:80)
xn+1

px→xn+1βn (xn+1) αn(x)

,

then using (9), (12,16)
(21)

1Y1=y(cid:48)β0(x, y) (cid:80)

pξ→xα0 (ξ, y) +

ξ

β0(x, y) (cid:80)
ξ

pξ→xα0 (ξ, y) +

N −1
(cid:80)
n=1

N −1
(cid:80)
n=1

1Yn=y,Yn+1=y(cid:48)βn (x) (cid:80)

αn(ξ)pξ→x

ξ

,

1Yn=yβn (x) (cid:80)

αn(ξ)pξ→x

ξ

P (Y1, .., YN

(cid:80)
x1

(cid:12)
(cid:12)
(cid:12)X1 = x1, X0 = x, Y0 = y)P (X1 = x1

(cid:12)
(cid:12)
(cid:12)X0 = x, Y0 = y)µ(x, y)

P (Y1, .., YN )

(cid:80)
x1

=

β0(x1, y)px→x1µ(x, y)

αN (ξ)

(cid:80)
ξ

.

REMARK 3.2.

1) Different iterations of px→ξ, µ(x, y) will be used on the left and right
hand sides of (20,22). The new estimates on the left are denoted p(cid:48)
x→ξ, µ(cid:48)(x, y). Moreover, αN
also depends on (the earlier iteration of) µ so the equation is not linear. It should be thought
of as a Bayes’ rule with the µ on the right being a prior (to incorporating the observations
with the current set of parameters) and the one on the left being a posterior.
2) Setting a px→ξ = 0 or µ(x, y) = 0 will result in it staying zero for all updates. This effec-
tively removes this parameter from the EM optimization update and should be avoided unless
it is known that one of these should be 0.
3) If there is no successive observations with Yn = y and Yn+1 = y(cid:48) in the actual observation
sequence, then all new estimates q(cid:48)
y→y(cid:48) (x) will either be set to 0 or close to it. They might not
be exactly zero due to the ﬁrst term in the numerator of (21) where we could have an estimate
of Y0 = y and an observed Y1 = y(cid:48).

Naturally, our solution degenerates to the Baum-Welch algorithm in the HMM case. How-
ever, the extra Markov component of MOM complicates this algorithm and its derivation. We

q(cid:48)
y→y(cid:48) (x) =

and using (19)

(22)

µ(cid:48)(x, y)=

MARKOV OBSERVATION MODELS

9

start with α, which is the most similar to HMM. Here, we have by the joint Markov property
and (10) that:

(23)

αn (x)

= P (Y1, ..., Yn, Xn = x)

=

=

(cid:88)

xn−1

(cid:88)

xn−1

P (Y1, ..., Yn, Xn−1 = xn−1, Xn = x)

P (Y1, ..., Yn−1, Xn−1 = xn−1) P (Xn = x, Yn

(cid:12)
(cid:12)
(cid:12)Y1, ..., Yn−1, Xn−1 = xn−1)

= qYn−1→Yn (x)

(cid:88)

xn−1

αn−1(xn−1)pxn−1→x ,

which can be solved forward for n = 2, 3, ..., N − 1, N , starting at

α1 (x1) =

(cid:88)

(cid:88)

x0

y0

µ(x0, y0) px0→x1 qy0→Y1 (x1) .

Recall α0 = µ is assigned differently.

Our iterative estimates for px→x(cid:48), qy→y(cid:48)(x) and µ(x, y) also rely on the second (backward)
recursion for βn. It also follows from the Markov property, our transition probabilities and
(3, 4) that:

(24)

βn (x)=P

(cid:16)

Yn+1, ..., YN

(cid:16)

=P

Yn+2, ..., YN

(cid:16)
Yn+2, ..., YN

=P

(cid:17)

(cid:12)
(cid:12)
(cid:12)Xn+1 = x, Yn
(cid:12)
(cid:12)
(cid:12)Xn+1 = x, Yn+1, Yn
(cid:12)
(cid:12)
(cid:12)Xn+1 = x, Yn+1

(cid:17)

(cid:16)

(cid:17)

P

Yn+1

(cid:12)
(cid:12)
(cid:12)Xn+1 = x, Yn

(cid:17)

qYn→Yn+1 (x)

(cid:88)

(cid:16)
Yn+2, ..., YN

P

=

x(cid:48)∈E

(cid:12)
(cid:17)
(cid:12)Xn+2 = x(cid:48), Xn+1 = x, Yn+1
(cid:12)

∗P

Xn+2 = x(cid:48)(cid:12)
(cid:17)
(cid:16)
(cid:12)
(cid:12)Xn+1 = x, Yn+1
βn+1(x(cid:48))px→x(cid:48)qYn→Yn+1 (x) ,

(cid:88)

=

x(cid:48)

qYn→Yn+1 (x)

which can be solved backward for n = N − 2, N − 3, ..., 3, 2, 1, 0, starting from
(cid:12)
(cid:12)
(cid:12)XN = x, YN −1) = qYN −1→YN (x).

βN −1 (x) = P (YN

It is worth noting that when we use βn with n = 0 we will have Y0 = y is some ﬁxed value
of interest not the missed observation that we never see and we use the notation β0(x0, y0).
We only see Y1, Y2, ..., YN .

We now have everything required for our algorithm, which is given in Algorithm 1 in

Section 4.

To be able to show convergence in Section 5, we need to track when parameters could
become 0. The following lemma follows immediately from (23), (24), induction and the
fact that (cid:80)
px→x(cid:48) = 1. Any sensible initialization of our EM algorithm would ensure the
x(cid:48)

condition qYn→Yn+1(x) > 0 holds.

LEMMA 3.3.

Suppose qYn→Yn+1(x) > 0 for all x ∈ E and n ∈ {1, ..., N − 1}. Then,

10

(cid:80)
x(cid:48)
1. p(cid:48)

1. βm(x) > 0 for all x ∈ E and m ∈ {1, ..., N − 1}.
2. β0(x, y) > 0 for any x ∈ E, y ∈ O such that qy→Y1(x) > 0.
3. αm(x) > 0 for all x ∈ E and m ∈ {1, ..., N } if both

µ(x0, y0)qy0→Y1(x) > 0 for all x, x0 ∈ E.

(cid:80)
x(cid:48)

px(cid:48)→x > 0 and (cid:80)
y0

4. α0(x, y) > 0 if µ(x, y) > 0.

Notice the condtion (cid:80)
x(cid:48)
at least one other state while (cid:80)
y0

px(cid:48)→x > 0 for all x says that any hidden state can be reached from

µ(x0, y0)qy0→Y1(x) > 0 for all x, x0 ensures that all the initial

hidden states are meaningful. The following result is the key to ensuring that our non-zero
parameters stay non-zero. It follows from the prior lemma as well as (20,21,22,24).

LEMMA 3.4.
px(cid:48)→x > 0 for all x ∈ E and (cid:80)
y0

Suppose N ≥ 2, qYn→Yn+1(x) > 0 for all x ∈ E and n ∈ {1, ..., N − 1},

µ(x0, y0)qy0→Y1(x) > 0 for all x, x0 ∈ E. Then,

x→x(cid:48) > 0 if and only if px→x(cid:48) > 0 for any x, x(cid:48).

2. q(cid:48)

y→y(cid:48)(x) > 0 for all x ∈ E if either
for all ξ ∈ E.

N −1
(cid:80)
n=1

1Yn=y,Yn+1=y(cid:48) > 0 or µ(ξ, y)1Y1=y(cid:48)qy→Y1(ξ) > 0

3. µ(cid:48)(x, y) > 0 if µ(x, y) > 0 and qy→Y1(ξ) > 0 for all ξ ∈ E. µ(cid:48)(x, y) = 0 if qy→Y1(ξ) = 0

for all ξ ∈ E.

x→x(cid:48) , q1

y→y(cid:48)(x), µ1(x, y); and uses the formula for p(cid:48)
x→x(cid:48), q2
y→y(cid:48)(x), µ3(x, y); etc. It is important to know that our estimates {pk

The algorithm; given explicitly in Section 4; starts with initial estimates of all px→x(cid:48),
y→y(cid:48)(x),
y→y(cid:48)(x), µ2(x, y);
x→x(cid:48), qk

qy→y(cid:48)(x), µ(x, y); say p1
µ(cid:48)(x, y) to reﬁne these estimates successively to the next estimates p2
p3
x→x(cid:48), q3
are getting better as k → ∞. Lemma 3.4 will be used in some cases to ensure that an initially
positive parameter stays positive as k increases, which important in our proofs to follow.

x→x(cid:48), q(cid:48)

y→y(cid:48)(x), µk(x, y)}

4. EM Algorithm and Small Number Problem. The raw algorithm that we have con-

sidered hitherto computes αn and βn recursively. By their deﬁnitions,

(25)

(26)

αn(x)=P (Y1, ..., Yn, Xn = x)

βn(x)=P (Yn+1, ..., YN

(cid:12)
(cid:12)
(cid:12)Xn+1 = x, Yn)

both can get extremely small when N is large. In this case, α1(x) would be a reasonable
number as it is just a probability of the event {Y1 = Y1, X0 = x}. However, β1(x) would be a
conditional probability of an exact occurrence of Y2, ..., YN , which would usually be extraor-
dinarily small. Conversely, αN (x) would usually be extraordinarily small and βN (x) may be
a reasonable number. In between, the product αn(x)βn(x) would usually be extraordinarily
small. The unfortunate side-effect of this is that our px→x(cid:48) (and q) calculations are basically
going to result in zero over zero most of the time when a computer is employed. We need a
ﬁx.

This small number problem is resolved by using the ﬁlter instead of α. Observe that the

ﬁlter

πn(x) = P (Xn = x|Y1, ..., Yn) =

αn(x)

αn(ξ)

(cid:80)
ξ

MARKOV OBSERVATION MODELS

11

is a (conditional) probability of a single event regardless of n. Hence, it does not necessarily
get extraordinarily small. However, scaling αn in a manner depending upon n means we
will have to scale βn as well in a counteracting way. The idea is to note that αn(x)βn(x)
appear together in computing the p(cid:48)
y→y(cid:48)(x) in such a way that we can divide
every αn(x)βn(x) by the same small number without changing the values of the p’s and q’s.
Speciﬁcally, we replace

x→x(cid:48) and q(cid:48)

αn(x)βn(x) ⇒

αn(x)βn(x)
a1a2 · · · aN −1

= πn(x)χn(x), ∀n ∈ {1, ..., N − 1},

where πn(x) is the ﬁlter and χn(x) = βn(x)
. a1, ..., aN are normalizing constants and
αn(x, y)βn(x, y) is scaled similarly. Using (23,24), one ﬁnds the recursions for π and χ are:
(cid:88)

an+1···aN −1

ρn (x) = qYn−1→Yn (x)

πn−1(xn−1)pxn−1→x ,

(27)

πn (x) =

ρn (x)
an

, an =

xn−1

(cid:88)

xn

ρn(xn),

which can be solved forward for n = 2, 3, ..., N − 1, N , starting at

(cid:80)
x0

(cid:80)
y0

π1 (x) =

µ(x0, y0) px0→x qy0→Y1 (x1)

a1

(cid:88)

(cid:88)

(cid:88)

, a1 =

x1

x0

y0

µ(x0, y0) px0→x1 qy0→Y1 (x1) .

Like β, χ is a backward recursion starting from

χN −1 (x) = P (YN

(cid:12)
(cid:12)
(cid:12)XN = x, YN −1) = qYN −1→YN (x)

and then continuing as

(28)

χn (x)=

qYn→Yn+1 (x)
an+1

(cid:88)

x(cid:48)

χn+1(x(cid:48))px→x(cid:48),

which can be solved backward for n = N − 2, N − 3, ..., 3, 2, 1.

Finally, the n = 0 value for π and χ become
χ1(x(cid:48))
a1

χ0 (x, y)=

(cid:88)

(29)

x(cid:48)

px→x(cid:48)qy→Y1 (x) ,

(30)

π0 (x, y)=α0 (x, y) = µ (x, y) .

The adjusted, non-raw algorithm is given in Algorithm 1

Note: In the three probability (p, q, µ) update steps of Algorithm 1, it usually better from
numeric and performance perspectives to compute the numerators and then use the facts
that they must be probability mass functions to properly normalize rather than use the full
equation as given.

5. Convergence of Probabilities.

In this section, we establish the convergence proper-
y→y(cid:48)(x), µk(x, y)} that we
ties of the transition probabilities and initial distribution {pk
derived in Section 3. Our method adapts the ideas of Baum et. al. [3], Liporace [25] and Wu
[38] to our setting.

x→x(cid:48), qk

We think of the transition probabilities and initial distribution as parameters, and let Θ
denote all of the non-zero transition and initial distribution probabilities in p, q, µ. Let e =
|E| and o = |O| be the cardinalities of the hidden and observation spaces. Then, the whole

// Characterize MOM models

12

Algorithm 1: EM algorithm for MOM

Data: Observation sequence: Y1, ..., YN
Input: Initial Estimates: {px→x(cid:48) }, {qy→y(cid:48) (x)}, {µ(x, y)}
Output: Final Estimates: {px→x(cid:48) }, {qy→y(cid:48) (x)}, {µ(x, y)}

/* Initalization.

1 while p, q, and µ have not converged do

/* Forward propagation.
π0 (x, y) = µ(x, y) ∀x ∈ E, y ∈ O;
(cid:88)
ρ1 (x) =

(cid:88)

µ(x0, y0) px0→x qy0→Y1 (x) ∀x ∈ E;

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

(cid:88)

xn−1∈E

πn−1(xn−1)pxn−1→x ∀x ∈ E.

y0∈O

x0∈E
x ρ1 (x)
ρ1(x)
.
a1

a1 = (cid:80)
π1 (x) =
for n = 2, 3, ..., N do

ρn (x) = qYn−1→Yn (x)

an = (cid:80)
πn (x) =

x ρn (x).
ρn(x)
.
an
/* Backward propagation.
χN −1 (x) = qYN −1→YN
for n = N − 2, N − 3, ..., 1 do

(x) ∀x ∈ E.

χn (x) =

qYn→Yn+1(x)
an+1

χ0 (x, y) =

qy→Y1(x)
a1

(cid:80)
x(cid:48)∈E

/* Probability Update.

χn+1(x(cid:48))px→x(cid:48) ∀x ∈ E.

(cid:80)
x(cid:48)∈E
χ1(x(cid:48))px→x(cid:48) ∀x ∈ E, y ∈ O.

pξ→x

(cid:80)
ξ

(cid:34)
1Y1=y(cid:48) χ0(x, y)π0 (ξ, y) +

(cid:35)
1Yn=y,Yn+1=y(cid:48) χn(x) πn(ξ)

N −1
(cid:80)
n=1

(cid:34)
χ0(x, y)π0 (ξ, y) +

pξ→x

(cid:80)
ξ

N −1
(cid:80)
n=1

1Yn=yχn(x) πn(ξ)

(cid:35)

µ (x, y) (cid:80)
(cid:80)

(cid:34)

x1
θ µ (ξ, θ) (cid:80)
(cid:80)
y
(cid:34)

px→x(cid:48)

χ0(x1, y)px→x1

x1

χ0(x1, θ)pξ→x1
N −1
(cid:80)
n=1

π0(x, y)χ0(x(cid:48), y) +

px→x1

π0(x, y)χ0(x1, y) +

(cid:80)
y

∀x ∈ E; y ∈ O.

(cid:35)

πn(x)χn(x(cid:48))

N −1
(cid:80)
n=1

χn (x1) πn(x)

(cid:35) ∀x, x(cid:48) ∈ E.

qy→y(cid:48) (x) =

∀x ∈ E; y, y(cid:48) ∈ O.

µ (x, y) =

(cid:80)

ξ

px→x(cid:48) =

(cid:80)
x1

*/

*/

*/

*/

parameter space has cardinality d(cid:48) = e2 + e ∗ o2 + e ∗ o for the px→x(cid:48) plus qy→y(cid:48)(x) plus
µ(x, y) and lives on [0, 1]d(cid:48) . However, we are removing the values that will be set to zero and
adding sum to one constraints to consider a constrained optimization problem on (0, ∞)d
for some d ≤ d(cid:48). Removing these zero possibilities gives us necessary regularity for our re-
estimation procedure. However, it was not enough to just remove them at the beginning. We
had to ensure that zero parameters did not creep in during our interations or else we will
be doing such things as taking logarithms of 0. Lemma 3.4 suggests a strategy for initially
assigning estimates so zeros will not occur in later estimates in the case that the value of Y1
also appears later in the observation sequence.

MARKOV OBSERVATION MODELS

13

1. Pick initial estimate {p1

x→x(cid:48)} such that (cid:80)
state can be reached from somewhere. From above we know p1
(cid:80)
x

x→x(cid:48) > 0 for all x(cid:48).
pk

x

x→x(cid:48) > 0 for all x(cid:48). This says that any hidden
p1

x→x(cid:48) → pk

x→x(cid:48) for all k so

2. Pick q1

y→y(cid:48)(x) > 0 for all x ∈ E if and only if

3. Pick µ1(x, y) > 0 if and only if q1

y→Y1

picked in the previous step to make this decision.

N −1
(cid:80)
n=1

1Yn=y,Yn+1=y(cid:48) > 0.

(x) > 0. Here you are using the values you just

This will produce an example of a zero separating sequence in the case the value of Y1 is
repeated as at least one Yn with n > 1.

DEFINITION 5.1. A sequence of estimates {pk, qk, µk} is zero separating if:
x→x(cid:48) > 0 iff pk
y→y(cid:48)(x) > 0 iff qk

1. p1
2. q1
3. µ1(x, y) > 0 iff µk(x, y) > 0 for all k = 1, 2, 3, ....

y→y(cid:48)(x) > 0 for all k = 1, 2, 3, ..., and

x→x(cid:48) > 0 for all k = 1, 2, 3, ...,

Here, iff stands for if and only if.

This means that we can potentially optimize over p, q, µ that we initially do not set to zero.
Henceforth, we factor the zero p, µ, q out of Θ, consider Θ ⊂ (0, ∞)d with d ≤ d(cid:48) and deﬁne
the parameterized mass functions

(31)

py0,y1,...,yN (x;Θ)

= px0→x1qy0→y1(x1)px1→x2qy1→y2(x2) · · · pxN −1→xN qyN −1→yN (xN)µ(x0, y0)

in terms of the non-zero values only. The observable likelihood

(32)

PY1,...,YN (Θ)=

(cid:88)

(cid:88)

py0,Y1,...,YN (x0, x1, ..., xN ; Θ)

x0,x1,...,xN

y0

is not changed by removing the zero values of p, µ, q and this removal allows us to deﬁne the
re-estimation function

(33)

QY1,...,YN (Θ, Θ(cid:48)) =

(cid:88)

(cid:88)

x0,...,xN

y0

py0,Y1,...,YN (x0, ..., xN ; Θ) ln py0,Y1,...,YN (x0, ..., xN ; Θ(cid:48)).

Note: Here and in the sequel, the summation in P, Q above are only over the non-zero com-
binations. We would not include an xi, xi+1 pair where pxi→xi+1 = 0 nor an x0, y0 pair where
µ(x0, y0) = 0. Hence, our parameter space is

Γ = {Θ ∈ (0, ∞)d :

(cid:88)

x(cid:48)

px→x(cid:48) = 1,

(cid:88)

y(cid:48)

qy→y(cid:48)(x) = 1 ∀x,

Later, we will consider the extended parameter space

K = {Θ ∈ [0, 1]d :

(cid:88)

x(cid:48)

px→x(cid:48) = 1,

(cid:88)

y(cid:48)

qy→y(cid:48)(x) = 1 ∀x,

µ(x, y) = 1}.

µ(x, y) = 1}

(cid:88)

x,y

(cid:88)

x,y

as limit points. Note: In both Γ and K, Θ is only over the px→x(cid:48) , qy→y(cid:48)(x) and µ(x, y) that
are not just set to 0 (before limits).

14

Then, equating Y0 with y0 to ease notation, one has that

(34)

Q(Θ, Θ(cid:48))=

(cid:88)

(cid:88)

(cid:34) N
(cid:89)

pxn−1→xnqYn−1→Yn(xn)

µ(x0, y0)

(cid:35)

y0

n=1

x0,...,xN
(cid:34) N
(cid:88)

(cid:8)ln p(cid:48)

xm−1→xm

+ ln q(cid:48)

Ym−1→Ym

(xm)(cid:9) + ln µ(cid:48)(x0, y0)

(cid:35)

.

m=1

The re-estimation function will be used to interpret the EM algorithm we derived earlier. We
impose the following condition to ensure everything is well deﬁned.

(Zero) The EM estimates are zero separating.

The following result that is motivated by Theorem 3 of Liporace [25].

THEOREM 5.2.

Suppose (Zero) holds. The expectation-maximization solutions (20, 21,
22) derived in Section 3 are the unique critical point of the re-estimation function Θ(cid:48) →
Q(Θ, Θ(cid:48)), subject to Θ(cid:48) forming probability mass functions. This critical point is a maximum
taking value in (0, 1]d for d explained above.

We consider it as an optimization problem over the open set (0, ∞)d but with the con-

straint that we have mass functions so the values have to be in the set (0, 1]d.

PROOF. One has by (34) as well as the constraint (cid:80)

x(cid:48) p(cid:48)

x→x(cid:48) = 1 that the maximum must

satisfy

(35)

0=

∂
∂p(cid:48)

x→x(cid:48)




Q(Θ, Θ(cid:48)) − λ





(cid:88)



ξ

p(cid:48)
x→ξ − 1










(cid:88)

(cid:88)

(cid:34) N
(cid:89)

=

x0,...,xN

y0

n=1

pxn−1→xnqYn−1→Yn(xn)

(cid:35) N
(cid:88)

1xm−1=x1xm=x(cid:48)

p(cid:48)
x→x(cid:48)

µ(x0, y0) − λ

m=1
x→x(cid:48), summing over x(cid:48) and then using the

where λ is a Lagrange multiplier. Multiplying by p(cid:48)
Markov property as well as the argument in (14,15), one has that

(36)

λ=

N
(cid:88)

(cid:88)

(cid:88)

(cid:34) N
(cid:89)

m=1

x0,...,xN

y0

n=1

pxn−1→xnqYn−1→Yn(xn)

1xm−1=x µ(x0, y0)

(cid:35)

=

N
(cid:88)

m=1

P (Xm−1 = x, Y1, ..., YN )

(cid:88)

(cid:88)

=

y

x1

β0(x1, y)px→x1α0(x, y) +

N
(cid:88)

(cid:88)

m=2

xm

βm−1(xm)px→xmαm−1(x).

Substituting (36) into (35), one has by the Markov property that

(37)

p(cid:48)
x→x(cid:48) =

(cid:88)

(cid:88)

(cid:34) N
(cid:89)

x0,...,xN

y0

n=1

pxn−1→xnqYn−1→Yn(xn)

(cid:35) N
(cid:88)

m=1

1xm−1=x1xm=x(cid:48)
λ

µ(x0, y0)

MARKOV OBSERVATION MODELS

15

N
(cid:80)
m=1

P (Xm−1 = x, Xm = x(cid:48), Y1, ..., YN )

(cid:80)
y

(cid:80)
x1

β0(x1, y)px→x1α0(x, y) +

N
(cid:80)
m=2

(cid:80)
xm

βm−1(xm)px→xmαm−1(x)

β0(x(cid:48), y)px→x(cid:48)α0(x, y) +

βm−1(x(cid:48))px→x(cid:48)αm−1(x)

(cid:80)
y

(cid:80)
x1

β0(x1, y)px→x1α0(x, y) +

βm−1(xm)px→xmαm−1(x)

.

N
(cid:80)
m=2
N
(cid:80)
m=2

(cid:80)
xm

=

=

(cid:80)
y

Clearly, the value on the far right of (37) is in (0, 1] (since we assumed px→x(cid:48) > 0). Similarly,

(38)

0=

∂
y→y(cid:48)(x)

∂q(cid:48)

(cid:40)

Q(Θ, Θ(cid:48)) − λ

(cid:32)

(cid:88)

θ∈O

(cid:33)(cid:41)

q(cid:48)
y→θ(x) − 1

(cid:88)

(cid:88)

(cid:34) N
(cid:89)

=

x0,...,xN

y0

n=1

pxn−1→xnqYn−1→Yn(xn)

(cid:35) N
(cid:88)

1Ym−1=y1Ym=y(cid:48)1Xm=x
q(cid:48)
y→y(cid:48)(x)

µ(x0, y0) − λ,

m=1
y→y(cid:48)(x), summing over y(cid:48) and then using

where λ is a Lagrange multiplier. Multiplying by q(cid:48)
the Markov property as well as the argument in (14,16), one has that

(39)

λ=

N
(cid:88)

(cid:88)

(cid:88)

(cid:34) N
(cid:89)

m=1

x0,...,xN

y0

n=1

pxn−1→xnqYn−1→Yn(xn)

1Ym−1=y1xm=xµ(x0, y0)

(cid:35)

=P (Y0 = y, X1 = x, Y1, ..., YN ) +

N
(cid:88)

m=2

1Ym−1=yP (Xm = x, Y1, ..., YN )

=β0(x, y)

(cid:88)

x0

px0→xµ(x0, y) +

N −1
(cid:88)

n=1

1Yn=yβn(x)

(cid:88)

xn

pxn→xαn(xn).

Substituting (39) into (38), one has that

(40)

q(cid:48)
y→y(cid:48)(x)

=

=

P (Y0 = y, X1 = x, Y1 = y(cid:48), Y2, ..., YN ) +

N
(cid:80)
m=2

1Ym−1=y,Ym=y(cid:48)P (Xm = x, Y1, ..., YN )

β0(x, y) (cid:80)
x0

px0→xµ(x0, y) +

N −1
(cid:80)
n=1

1Yn=yβn(x) (cid:80)
xn

pxn→xαn(xn)

1Y1=y(cid:48)β0(x, y) (cid:80)
x0
β0(x, y) (cid:80)
x0

px0→xµ(x0, y) +

px0→xµ(x0, y) +

N −1
(cid:80)
n=1
N −1
(cid:80)
n=1

1Yn=y1Yn+1=y(cid:48)βn(x) (cid:80)
xn

pxn→xαn(xn)

.

1Yn=yβn(x) (cid:80)
xn

pxn→xαn(xn)

Finally, for a maximum one also requires


(41)

0=

Q(Θ, Θ(cid:48)) − λ



(cid:88)

µ(cid:48)(ξ, θ) − 1










∂
∂µ(cid:48)(x, y)






(cid:88)

(cid:88)

(cid:34) N
(cid:89)

=

x0,...,xN

y0

n=1

ξ∈E,θ∈O

(cid:35)

pxn−1→xnqYn−1→Yn(xn)

1x0=x1y0=y
µ(cid:48)(x, y)

µ(x0, y0) − λ,

16

where λ is a Lagrange multiplier. Multiplying by µ(cid:48)(x, y) and summing over x, y, one has
that

(42)

λ =

(cid:88)

(cid:88)

(cid:34) N
(cid:89)

x0,...,xN

y0

n=1

pxn−1→xnqYn−1→Yn(xn)

µ(x0, y0)

(cid:35)

=P (Y1, ..., YN )

(cid:88)

=

αN (ξ).

ξ

Substituting (42) into (41), one has by (3,4) that

(cid:80)
x0,...,xN

(cid:80)
y0

(cid:20) N
(cid:81)
n=1

(43)

µ(cid:48)(x, y)=

(cid:21)
1x0=x1y0=yµ(x0, y0)
pxn−1→xnqYn−1→Yn(xn)

αN (ξ)

(cid:80)
ξ

=

P (X0 = x, Y0 = y, Y1, ..., YN )

αN (ξ)

(cid:80)
ξ

(cid:80)
x1

=

(cid:80)
x1

=

P (Y1, ..., YN |X1 = x1, X0 = x, Y0 = y)P (X1 = x1|X0 = x, Y0 = y)µ(x, y)

αN (ξ)

(cid:80)
ξ

β0(x1, y)px→x1µ(x, y)

αN (ξ)

(cid:80)
ξ

.

If we were to sum the numerator on the far right of (43), then upon substitution of β we would
get P (Y1, ..., YN ), which matches the denominator. Hence, µ(cid:48)(x, y) ∈ [0, 1] like the other new
estimates. Now, we have established that the EM algorithm of Section 3 corresponds to the
unique critical point of Θ(cid:48) → Q(Θ, Θ(cid:48)). Moreover, all mixed partial derivative of Q in the
components of Θ(cid:48) are 0, while

(44)

(45)

and

(46)

∂2QY1,Y2,...,YN (Θ, Θ(cid:48))

∂p(cid:48)

2

x→x(cid:48)
(cid:34) N
(cid:89)

(cid:88)

=−

y0;x0,...,xN

n=1

pxn−1→xnqYn−1→Yn(xn)

(cid:35) N
(cid:88)

1xm−1=x,xm=x(cid:48)

m=1

2

p(cid:48)
x→x(cid:48)

µ(x0, y0)

∂q(cid:48)

∂2QY1,Y2,...,YN (Θ, Θ(cid:48))
y→y(cid:48)(x)2
(cid:34) N
(cid:89)

(cid:88)

=−

pxn−1→xnqYn−1→Yn(xn)

y0;x0,...,xN

n=1

m=1

(cid:35) N
(cid:88)

1Ym−1=y,Ym=y(cid:48),xm=x
y→y(cid:48)(x)2
q(cid:48)

µ(x0, y0)

∂2QY1,Y2,...,YN (Θ, Θ(cid:48))
∂µ(cid:48)(x, y)2
(cid:34) N
(cid:89)

(cid:88)

=−

pxn−1→xnqYn−1→Yn(xn)

y0;x0,...,xN

n=1

m=1

(cid:35) N
(cid:88)

1y0=y,x0=x
µ(cid:48)(x, y)2 µ(x0, y0).

MARKOV OBSERVATION MODELS

17

Hence, the Hessian matrix is diagonal with negative values along its axis and the critical point
is a maximum.

The upshot of this result is that, if the EM algorithm produces parameters {Θk} ⊂ Γ, then
Q(Θk, Θk+1) ≥ Q(Θk, Θk). Now, we have the following result, based upon Theorem 2.1 of
Baum et. al. [3], that establishes the observable likelihood is also increasing i.e. P (Θk+1) ≥
P (Θk).

LEMMA 5.3.

Suppose (Zero) holds. Q(Θ, Θ(cid:48)) ≥ Q(Θ, Θ) implies P (Θ(cid:48)) ≥ P (Θ). More-

over, Q(Θ, Θ(cid:48)) > Q(Θ, Θ) implies P (Θ(cid:48)) > P (Θ).

PROOF. ln(t) for t > 0 has convex inverse exp(t). Hence, by Jensen’s inequality

(47)

Q(Θ, Θ(cid:48)) − Q(Θ, Θ)
P (Θ)

(cid:34)

= ln exp

(cid:88)

(cid:88)

ln

(cid:18) py0,Y1,...,YN (x0, x1, ..., xN ; Θ(cid:48))
py0,Y1,...,YN (x0, x1, ..., xN ; Θ)

(cid:19) py0,Y1,...,YN (x0, x1, ..., xN ; Θ)
P (Θ)

(cid:35)

x0,x1,...,xN

(cid:80)
x0,x1,...,xN

(cid:80)
y0







≤ ln

y0
py0,Y1,...,YN (x0, x1, ..., xN ; Θ) py0,Y1,...,YN (x0,x1,...,xN ;Θ(cid:48))
py0,Y1,...,YN (x0,x1,...,xN ;Θ)

P (Θ)







= ln

(cid:19)

(cid:18) P (Θ(cid:48))
P (Θ)

and the result follows.

The stationary points of P and Q are also related.

LEMMA 5.4.

Suppose (Zero) holds. A point Θ ∈ Γ is a critical point of P (Θ) if and only
if it is a ﬁxed point of the re-estimation function, i.e. Q(Θ; Θ) = maxΘ(cid:48) Q(Θ; Θ(cid:48)) since Q is
differentiable on (0, ∞)d in Θ(cid:48).

PROOF. The following derivatives are equal:

(48)

∂PY1,...,YN (Θ)
∂px→x(cid:48)

(cid:34) N
(cid:89)

(cid:88)

(cid:88)

=

pxn−1→xnqYn−1→Yn(xn)

y0

n=1

x0,...,xN
∂QY1,Y2,...,YN (Θ, Θ(cid:48))

=

∂p(cid:48)

x→x(cid:48)

(cid:12)
(cid:12)
(cid:12)Θ(cid:48)=Θ

,

(cid:35) N
(cid:88)

m=1

1xm−1=x,xm=x(cid:48)
pxm−1→xm

µ(x0, y0)

which are deﬁned since px→x(cid:48) (cid:54)= 0. Similarly,

(49)

∂PY1,...,YN (Θ)
∂qy→y(cid:48)(x)

(cid:88)

(cid:88)

(cid:34) N
(cid:89)

=

pxn−1→xnqYn−1→Yn(xn)

(cid:35) N
(cid:88)

y0

n=1

x0,...,xN
∂QY1,Y2,...,YN (Θ, Θ(cid:48))
y→y(cid:48)(x)

∂q(cid:48)

=

m=1

(cid:12)
(cid:12)
(cid:12)Θ(cid:48)=Θ

1Ym−1=y,Ym=y(cid:48),xm=x
qYm−1→Ym(x)

µ(x0, y0)

18

and

(50)

∂PY1,...,YN (Θ)
∂µ(x, y)

=

=

(cid:88)

(cid:88)

(cid:34) N
(cid:89)

pxn−1→xnqYn−1→Yn(xn)

1(x0,y0)=(x,y)

(cid:35)

y0

n=1

x0,...,xN
∂QY1,Y2,...,YN (Θ, Θ(cid:48))
∂µ(cid:48)(x, y)

(cid:12)
(cid:12)
(cid:12)Θ(cid:48)=Θ

.

We can rewrite (37,40,43) in recursive form with the values of α and β substituted in to

ﬁnd that

Θk+1 = M (Θk),

where M is a continuous function. Moreover, P : K → [0, 1] is continuous and satisﬁes
P (Θk) ≤ P (M (Θk)) from above. Now, we have established everything we need for the
following result, which follows from the proof of Theorem 1 of [38].

THEOREM 5.5.

k=1 is relatively compact, all its limit
points (in K) are stationary points of P , producing the same likelihood P (Θ∗) say, and
P (Θk) converges monotonically to P (Θ∗).

Suppose (Zero) holds. Then, {Θk}∞

[38] has several interesting results in the context of general EM algorithms to guarantee
convergence to local or global maxima under certain conditions. However, the point of this
note is to introduce a new model and algorithms with just enough theory to justify the algo-
rithms. Hence, we do not consider theory under any special cases here but rather refer the
reader to Wu [38].

6. Viterbi algorithm. Like for HMM, the ﬁlter for MOM can be computed in real time

(once the parameters are known)

πn (x) = P

(cid:16)

Xn = x

(cid:12)
(cid:12)
(cid:12)Y1, ..., Yn

(cid:17)

, ∀x ∈ E.

So, we can just compute αn (x) and then just normalize i.e.

(51)

πn (x) =

αn (x)

αn (ξ)

(cid:80)
ξ

and π(A) =

π(x).

(cid:88)

x∈A

This provides our tracking estimate of the hidden state given the observations. Prediction can
then be done by running the Kolmogorov forward equation starting from this estimate.

We can compute the most likely single hidden state values x+

n of the hidden state Xn,
given the back observations Y1, ..., Yn by ﬁnding the values that maximize x → P (Xn =
(cid:12)
(cid:12)
(cid:12)Y1, ..., Yn). However, the Viterbi algorithm is used in HMM to ﬁnd the most likely whole
x
sequence of hidden state given the complete sequence of observations. This is particularly
important in problems like decoding or recognition but is still useful in a widerange of appli-
cation. It is a dynamic programming type algorithm.

As there is a EM analog to the Baum-Welch algorithm for our MOM models, it is natural
to wonder if there is a dynamic programming analog to the Viterbi algorithm for our MOM
models. The answer is yes and it is more similar to the Viterbi algorithm than our MOM
EM algorithm is to the Baum-Welch algorithm. There are three small variants that one can

MARKOV OBSERVATION MODELS

19

consider: ﬁnding the most likely sequence including both the initial hidden state X0 and the
unseen observation Y0, including just the hidden state X0 or neither. We consider doing both
X0 and Y0 here. (The others are basically the same, starting with a marginal of our initial
distribution given in our algorithm here.)

We deﬁne a sequence of functions δ0,1(y, x0, x1), {δn(x)}N
0, x∗

and a sequence of estimates {y∗
algorithm, Algorithm 2 below. Then, we show the algorithm works by noting

n=2, the maximum functions,
N }, the most likely sequence, within our Viterbi

1, ..., x∗

0, x∗

(52)

δn(x) =

max
y0;x0,x1,...,xn−1

P (Y0 = y0; X0 = x0, ..., Xn−1 = xn−1; Xn = x; Y1, ..., Yn),

for all n, x and establishing that {y∗

0, x∗

0, ..., x∗

N } satisﬁes x∗

N = arg max

δN (x) and

δN (x∗

N ) = P (Y0 = y∗

0; X0 = x∗

0, X1 = x∗

1, ..., XN −1 = x∗

x
N −1; XN = x∗

N ; Y1, ..., YN ).

Algorithm 2: Viterbi algorithm for MOM

Input: Observation sequence: Y1, ..., YN
Output: Most likely Hidden state sequence: P ∗; y∗
0; x∗
Data: Probabilities {px→x(cid:48) }, {qy→y(cid:48) (x)}, {µ(x, y)}

0, x∗

1, ..., x∗
N

// Distinguish MOM models

1 δ0,1(y0, x0, x1) = µ(x0, y0)px0→x1 qy0→Y1 (x1), ∀y0, x0, x1

// Initialize joint

distribution.
/* Substitute Marginal δ1(x1) = (cid:80)
y0,x0

2 δ2(x2) =

max
y0∈O;x0,x1∈E

(cid:2)δ0,1(y0, x0, x1)px1→x2

δ0,1(y0, x0, x1) if want x∗
(cid:3) qY1→Y2 (x2)

1, ..., x∗

N .

/* Replace δn with normalized γn given below to avoid small number

problem.
3 ψ2(x2) = arg max

y0∈O;x0,x1∈E

(cid:2)δ0,1(y0, x0, x1)px1→x2

(cid:3)

/* Now propagate maximums, keeping track where they occur.

*/

*/

*/

4 for n:=3 to N do
5

δn(xn) = max

6

xn−1∈E
ψn(xn) = arg max
xn−1∈E

(cid:2)δn−1(xn−1)pxn−1→xn
(cid:2)δn−1(xn−1)pxn−1→xn

(cid:3) qYn−1→Yn (xn)
(cid:3)

/* Termination

7 P ∗ = max
xN ∈E
8 x∗
N = arg max
xN ∈E

[δN (xN )]

[δN (xN )]

/* Path Back Tracking

9 for n:=N-1 down to 2 do
n = ψn+1(x∗
x∗
n+1)
10
1) = ψ2(x∗
0, x∗
0; x∗
2)

11 (y∗

// Maximums

// Maximum Locations

*/

*/

6.1. Dynamic Programming Explanation.

δ2(x) as deﬁned in Algorithm 2 veriﬁably sat-
isﬁes (52) by simple substitution. Next, assume δn−1(x) satisﬁes (52) for some n ≥ 3. Then,
by the algorithm and the Markov property:

(53)

δn(xn)

= max
xn−1∈E

(cid:2)δn−1(xn−1)pxn−1→xn

(cid:3) qYn−1→Yn(xn)

20

(cid:20)

=max
xn−1

max
y0;x0,...,xn−2

(cid:21)
qYn−1→Yn(xn)
P (Y0 = y0; X0 = x0, ..., Xn−1 = xn−1; Y1, ..., Yn−1)pxn−1→xn

=

P (Y0 = y0; X0 = x0, ..., Xn−2 = xn−2, Xn−1 = xn−1; Xn = xn; Y1, ..., Yn),

max
y0;x0,...,xn−2,xn−1
and (52) follows for all n by induction. Next, it follows from the algorithm that x∗
arg max
x

N =
δN (x). Finally, we have by the Path Back Tracking part of the algorithm as well

as induction that
δN (x∗

(54)

N )=δN −1(x∗

N −1)px∗

N −1→x∗
N

qYN −1→YN (x∗

N )

=δ0,1(y∗

0; x∗

0, x∗
1)

N
(cid:89)

px∗

n−1→x∗

nqYn−1→Yn(x∗
n)

=P (Y0 = y∗

0, X1 = x∗
and the most likely sequence is established.

n=2
0; X0 = x∗

1, ..., XN −1 = x∗

N −1; XN = x∗

N ; Y1, ..., YN )

REMARK 6.1. Our Viterbi dynamic programming algorithm can be thought of as a direct
generalization of the orginal Viterbi algorithm for HMM. Indeed, we need only let qy→y(cid:48)(x) =
bx(y(cid:48)) for some probability mass function (depending upon x) bx to recover the normal HMM
and the normal Viterbi algorithm. Then, we would drop the consideration of the most likely
starting point (y∗

0) and be back to the original setting.

0, x∗

6.2. Small Number Problem. The Viterbi-type algorithm also suffers from extraordinar-
ily small, shrinking numbers. Indeed, since we have multiple events in both X and Y , the
numbers will shrink faster than our EM algorithm in n. On the other hand, we are not taking
ratios and it is easier to scale this algorithm than the EM algorithm. Still, one might wonder
if we can handle the small number problem for our dynamic programming algorithm in a
similar manner as we did for our EM algorithm.

While we did not adjust Algorithm 2, this algorithm can be adjusted for small numbers.

The idea is similar to that used in the EM algorithm. Simply replace δ2 with

υ2(x2) =

max
y0∈O;x0,x1∈E

[δ0,1(y0, x0, x1)px1→x2] qY1→Y2(x2)

γ2(x2) =

υ2(x2)
a2

, a2 =

(cid:88)

ξ

υ2(ξ)

and δn, n ≥ 3 with γn, where

υn(xn) = max
xn−1∈E

(cid:2)γn−1(xn−1)pxn−1→xn

(cid:3) qYn−1→Yn(xn)

γn(xn) =

υ(xn)
an

, an =

(cid:88)

ξ

υn(ξ).

Then, replace δn with γn everywhere else in the algorithm. Of course, the maximum se-
quence likelihood P ∗ would have be scaled down by multiplying by the product of the an’s.
Otherwise, the algorithm would remain the same.

MARKOV OBSERVATION MODELS

21

7. Bitcoin Example. To establish the applicability of our model and algorithms to real-
world big data problems, we include a simple, illustrative MOM model example application.
Bitcoin is a highly volatile digital currency that can be traded by various means. Further, hold-
ing Bitcoin during uptrends has proven to be a superlative investment, while holding it during
other periods has been extremely risky and painful. Therefore, it is of interest to see if our
MOM model algorithms might be able to isolate uptrend periods and provide a two-hidden-
state Markov Observation Model that matches historical data reasonably well. Accordingly,
we applied our (Baum-Welch-like) EM and (Viterbi-like) dynamic programming algorithms
to identify and demonstrate a (hidden) regime-change model for daily Bitcoin closing prices
from Sept 1, 2018 until Sept 1, 2022. Our purpose herein was solely to establish the veracity
of our algorithms and model on real data. Future ﬁnancial studies should be conducted on
best use practices.

The price varied (rather dramatically) from a low of 3235.76 USD on Dec. 15, 2018 to
a high of 67566.83 USD on Nov. 8, 2021 over our four year period of interest. Instead of
raw prices, we took our observations Y be the natural logarithm of prices, which ranged
from 8.0823 to 11.12 (see the continuous orange line in Figure 2), and divided those into
b = 25 equal-sized bins. For example, bin 0 consisted of log prices with the range 8.08 to
8.2016 corresponding to actual prices $3230 to $3646.79 while the last bin, bin 24, consisted
of log prices with the range 11.0005 to 11.122 corresponding to actual prices $59904.09 to
$67643.06 all in US dollars. Since our example was only for demonstration purposes we just
chose to have s = 2 hidden states 0 and 1.

7.1. Initialization.

It is well known that the Baum-Welch algorithm for HMM will get
stuck at a local maximum. This should be even more true for our EM algorithm of our MOM
model as we have even more to estimate. (MOM has a larger initial distribution and more
complex Markov transitions probabilities compared to HMM’s single state initial distribution
and emission probabilities.) Therefore, it makes sense to start the algorithm with an idea of
the solution that we seek. Most importantly, we want to differentiate the hidden states so
we plan that state 1 will represent an uptrend and state 0 will represent everything else and
initialize accordingly. However, since we want our algorithm to ﬁnd a variety of uptrends,
we will allow some inconsistencies in our initial set up that will force the algorithm to make
signiﬁcant changes. Also, we recognize that it is the algorithm that decides what the hidden
states are. While we suggesting state 1 will be an uptrend, the algorithm, by the time it has
ﬁnished, may have decided 1 represents something completely different like high volatility
say.

Our ﬁrst step was to use the data to come up with initial qy→y(cid:48)(1) for uptrend observation
transitions and qy→y(cid:48)(0) other observation transitions. Accordingly, we made a somewhat
arbtrarily decision about when Bitcoin might be in an uptrend. In particular, we decided,
based on a brief glimpse at the graph, to say it was in an uptrend from the low on December
15, 2018 until the high on July 3, 2019, then again from the low on March 12, 2020 until the
high on April 15, 2021, and ﬁnally from the low on July 20, 2021 until the high on November
8, 2021. Otherwise, it was not in an uptrend. This amounts to six changes over the 1461 days
in these four years.

REMARK 7.1. Naturally, there were down and up days for both hidden states. Also, the
very ﬁrst price was excluded as this is our Y0 price that we would not see in practice. Finally,
we need to emphasize that we expect that the bin size effect was rather huge. We used 25
bins, which is extremely crude, and an arbitrary 4 year period with no sign of numerical
issues. Also, the amount of data was very uneven over the bins, which we simply ignored,
but it certainly hampered algorithm performance. A larger, ﬁner study by more experienced
computer programmers is deﬁnitely recommended.

22

Blue High: Most likely uptrend; Blue Low: Anything but an uptrend.

FIG 2. BitCoin Uptrend Detection - Three Uptrends

To create our initial observation transitions, we initalized our b × b matrices {qy→y(cid:48)(0)}
and {qy→y(cid:48)(1)} to zero. (Here, y and y(cid:48) refer to either bin number or rounded log price
through a one-to-one mapping.) Starting from n = 1 and going through to n = N − 1 we
added 1 to qYn→Yn+1(1) if we were in an uptrend and otherwise 1 to qYn→Yn+1(0). Then, we
normalized both matrices so that the non-zero rows added to one.

To initialize µ(x, y), we ﬁrst set it all to zero. Next, we went through n ∈ {1, ..., N − 1}
if Yn+1 was equal to Y1 then we added qYn→Y1(x) to µ(x, Yn) for x = 0, 1. Finally, we
normalized µ(x, y) so it summed to 1.

We set the stopping criterion to be extremely tight, making sure that the p’s and µ’s were
essentially done changing. (The q’s will also be done in this case so there is little need to
check this bulky matrix.) The initialization of the p will be varied and explained in the results.

7.2. Results. Our ﬁrst goal was to see if the algorithms would return the three uptrends

that were supplied. To do this, we initialized p as follows:

(55)

(cid:20) p0→0 p0→1
p1→0 p1→1

(cid:21)

(cid:20) 0.997 0.003
0.003 0.997

(cid:21)

.

=

This means that it should switch states every 333 days on average, which is roughly consistent
with my initial take of three uptrends given above.

After k = 11 iterations, the EM algorithm converged and the combined result of both
algorithms is displayed in Figure 2. While it had the same number of uptrends as I supplied
it rejected my suggestions. In particular, it shifted my ﬁrst uptrend, split my second and
discarded my third. I believe that the algorithm’s uptrends are at least as good as my initial
ones. The ﬁnal p matrix in this case was

(56)

(cid:20) p0→0 p0→1
p1→0 p1→1

(cid:21)

(cid:20) 0.99643132 0.00356868
0.00302665 0.99697335

(cid:21)

.

=

MARKOV OBSERVATION MODELS

23

Blue High: Most likely uptrend; Blue Low: Anything but an uptrend.

FIG 3. BitCoin Uptrend Detection - Short Uptrends

From an investor’s perspective shorter, steeper uptrends might be more desirable. Hence,
we investigated the possibility of ﬁnding more, shorter uptrends without retraining the q
matrices. Instead, we merely changed the initial p matrix to
(cid:21)
(cid:20) 0.90 0.10
0.01 0.99

(cid:20) p0→0 p0→1
p1→0 p1→1

(57)

=

(cid:21)

,

which initially makes all changes more likely. However, it sets the initial expected time in an
uptrend to just ten days initially. After k = 43 iterations, the EM algorithm converged and the
combined result of both algorithms is displayed in Figure 3. Compared to the earlier result
the uptrends were split and shrunk. In addition, a new uptrend was added in the later part
of the data stream. It is very interesting that it did a decent job of ﬁnding a different type of
uptrend without any new training, but rather just a different p matrix initialization. The ﬁnal
p matrix was:

(58)

(cid:20) p0→0 p0→1
p1→0 p1→1

(cid:21)

(cid:20) 0.98329893 0.01670107
0.0127778 0.9872222

(cid:21)

.

=

The EM algorithm spent those 43 iterations making signﬁcant changes. In particular, the ﬁnal
p matrix and graph suggests a near equal time in uptrends as not. However, this is somewhat
out of our control. We supply the data, the number of hidden states and some initial estimates
and then tell the EM algorithm to give us the locally optimal model, whatever that may be.
In both cases the result exceeded our expectations.

[1] Baum, L. E. and Petrie, T. (1966). Statistical Inference for Probabilistic Functions of Finite State Markov

Chains. The Annals of Mathematical Statistics. 37 (6): 1554-1563. doi:10.1214/aoms/1177699147.

REFERENCES

24

[2] Baum, L. E. and Eagon, J. A. (1967). An inequality with applications to statistical estimation for probabilistic
functions of Markov processes and to a model for ecology. Bulletin of the American Mathematical
Society. 73 (3): 360. doi:10.1090/S0002-9904-1967-11751-8. Zbl 0157.11101.

[3] Baum, L. E., Petrie, T., Soules, G. and Weiss, N. (1970). A Maximization Technique Occurring in Statistical
Analysis of Probabilistic Functions in Markov Chains. The Annals of Mathematical Statistics, 41, 164-
171. http://dx.doi.org/10.1214/aoms/1177697196.

[4] Bonate, P: Pharmacokinetic-Pharmacodynamic Modeling and Simulation. Berlin: Springer; 2011
[5] Bryan, J. D. and Levinson, S. E. (2015). Autoregressive Hidden Markov Model and the Speech Signal. Pro-

cedia Computer Science 61 328-333.

[6] Cappé, O., Moulines, E. and Rydén, T. Inference in Hidden Markov Models. Springer, Berlin 2007.
[7] Chopin, N. (2004). Central Limit Theorem for Sequential Monte Carlo Methods and its Application to

Bayesian Inference. The Annals of Statistics 32 (6), 2385–2411.

[8] Chopin, N. and Papaspiliopoulos, O. An Introduction to Sequential Monte Carlo. Springer Nature, Switzer-

land AG 2020. doi: 10.1007/978-3-030-47845-2.

[9] Creal, D. (2012). "A Survey of Sequential Monte Carlo Methods for Economics and Finance". Econometric

Reviews. 31 (2). doi:10.1080/07474938.2011.607333.

[10] Crisan, D., Kouritzin, M. A. and Xiong, J. (2009). Nonlinear ﬁltering with signal dependent observation
noise. Electronic Journal of Probability, 14 1863-1883. https://doi.org/10.1214/EJP.v14-687
[11] D’Amato, E., Notaro, I., Nardi, V. A., Scordamaglia, V. (2021). "A Particle Filtering Approach for Fault De-
tection and Isolation of UAV IMU Sensors: Design, Implementation and Sensitivity Analysis". Sensors.
21 (9). doi:10.3390/s21093066

[12] Date, P., Mamon, R., Tenyakov, A. (2013). Filtering and forecasting commodity futures prices under an

HMM framework. Energy Economics, 40, 1001-1013. https://doi.org/10.1016/j.eneco.2013.05.016.

[13] Del Moral, P., Kouritzin, M.A., and Miclo, L. (2001). On a class of discrete generation interacting particle

systems. Electronic Journal of Probability 6 : Paper No. 16, 26 p.

[14] Elfring J, Torta E, van de Molengraft R. (2021). Particle Filters: A Hands-On Tutorial. Sensors (Basel) 21

(2):438. doi: 10.3390/s21020438.

[15] Fujisaki, M., Kallianpur, G. and Kunita, H. (1972). Stochastic differential equations for the nonlinear ﬁlter-

ing problem. Osaka J. Math. 9, 19–40.

[16] Hajiramezanali, E.; Imani, M.; Braga-Neto, U.; Qian, X.; Dougherty, E. R. (2019). "Scalable optimal
Bayesian classiﬁcation of single-cell trajectories under regulatory model uncertainty". BMC Genomics
20 (Suppl 6): 435. doi:10.1186/s12864-019-5720-3.

[17] Kalman, R. E. (1960). "A New Approach to Linear Filtering and Prediction Problems". Journal of Basic

Engineering. 82: 35-45. doi:10.1115/1.3662552.

[18] Kalman, R. E., and Bucy, R. S. (1961). "New Results in Linear Filtering and Prediction Theory." ASME. J.

Basic Eng. 83(1): 95-108. https://doi.org/10.1115/1.3658902.

[19] Kloek, T.; van Dijk, H. K. (1978). "Bayesian Estimates of Equation System Parameters: An Application of

Integration by Monte Carlo". Econometrica. 46 (1): 1-19. doi:10.2307/1913641

[20] Kouritzin, M. A. (1998). On exact ﬁlters for continuous signals with discrete observations, IEEE Transac-

tions on Automatic Control, vol. 43, no. 5, pp. 709-715, doi: 10.1109/9.668842.

[21] Kouritzin, M. A. (2017). Residual and Stratiﬁed Branching Particle Filters, Computational Statistics and

Data Analysis 111, pp. 145-165. doi: 10.1016/j.csda.2017.02.003.

[22] Kouritzin, M.A. and Long, H. (2008), "On extending classical ﬁltering equations", Statistics and Probability

Letters. 78 3195-3202, doi: 10.1016/j.spl.2008.06.005.

[23] Kurtz, T.G. and Ocone, D.L. (1988). Unique characterization of conditional distributions in nonlinear ﬁlter-

ing. Ann. Probab. 16, 80–107.

[24] Kurtz, T.G. and Nappo G. (2010). The Filtered Martingale Problem. in The Oxford Handbook of Nonlinear

Filtering, Oxford University Press.

[25] Liporace, L. A. (1982). Maximum likelihood estimation for multivariate observations of Markov sources.

IEEE Trans. Inf. Theory 28(5): 729-734.

[26] Maroulas, V. and Nebenführ, A. (2015). Tracking Rapid Intracellular Movements: A Bayesian Random Set

Approach. The Annals of Applied Statistics 9 (2): 926-949. doi: 10.1214/15-AOAS819.

[27] Nicolai, C. (2013). Solving ion channel kinetics with the QuB software. Biophysical Reviews and Letters 8

(3n04): 191-211. doi:10.1142/S1793048013300053

[28] Petropoulos, A., Chatzis, S. P. and Xanthopoulos, S. (2016). "A novel corporate credit rating sys-
tem based on Student’s-t hidden Markov models". Expert Systems with Applications. 53: 87-105.
doi:10.1016/j.eswa.2016.01.015

[29] Pitt, M.K.; Shephard, N. (1999). "Filtering Via Simulation: Auxiliary Particle Filters". Journal of the Amer-

ican Statistical Association. 94 (446): 590-591. doi:10.2307/2670179.

MARKOV OBSERVATION MODELS

25

[30] Rabiner, L.R. (1989). "A tutorial on hidden Markov models and selected applications in speech recognition".
Proceedings of the IEEE 77 (2): 257–286. CiteSeerX 10.1.1.381.3454. doi:10.1109/5.18626.
[31] Shinghal, R. and Toussaint, G.T. (1979). "Experiments in text recognition with the modiﬁed Viterbi algo-
rithm," IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-l 184-193.
[32] Sidrow, E., Heckman, N., Fortune, S. M., Trites, A. W., Murphy, I., and Auger-Méthé, M. (2022). Modelling
multi-scale, state-switching functional data with hidden Markov models. Canadian Journal of Statistics,
50(1), 327-356.

[33] Stanculescu, I., Williams, C. K. I., and Freer, Y. (2014). Autoregressive Hidden Markov Models for the
Early Detection of Neonatal Sepsis. IEEE Journal of Biomedical and Health Informatics 18(5):1560-
1570. DOI: 10.1109/JBHI.2013.2294692

[34] Stigler, J., Ziegler, F., Gieseke, A., Gebhardt, J. C. M. and Rief, M. (2011). The Complex Folding Net-
work of Single Calmodulin Molecules. Science. 334 (6055): 512-516. Bibcode:2011Sci...334..512S.
doi:10.1126/science.1207598

[35] van Dijk, H. K.; Kloek, T. (1984). Experiments with some alternatives for simple importance sampling in
Monte Carlo integration. In Bernardo, J. M.; DeGroot, M. H.; Lindley, D. V.; Smith, A. F. M. (eds.).
Bayesian Statistics. Vol. II. Amsterdam: North Holland. ISBN 0-444-87746-0.

[36] Van Leeuwen, P.J., Künsch, H.R., Nerger, L., Potthast, R., Reich, S. (2019). Particle ﬁlters for high-
dimensional geoscience applications: A review. Q. J. R. Meteorol Soc. 145: 2335–2365. doi:
10.1002/qj.3551.

[37] Viterbi, A. J. (1967). "Error bounds for convolutional codes and an asymptotically optimum decoding algo-

rithm". IEEE Transactions on Information Theory. 13 (2): 260-269. doi:10.1109/TIT.1967.1054010.

[38] Wu, C.F.J. (1983). "On the Convergence Properties of the EM Algorithm," Ann. Statist. 11(1): 95-103.
[39] Xuan, T. (2004) Autoregressive Hidden Markov Model with Application in an El Nino Study. MSc. Thesis,

University of Saskatchewan, Saskatoon.

[40] Zakai, M. (1969). On the optimal ﬁltering of diffusion processes. Z. Wahrsch. Verw. Gebiete 11, 230–243.

