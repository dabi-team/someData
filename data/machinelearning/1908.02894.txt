1
2
0
2

r
p
A
5
2

]

G
L
.
s
c
[

4
v
4
9
8
2
0
.
8
0
9
1
:
v
i
X
r
a

How much data is suﬃcient to learn high-performing algorithms?
Generalization guarantees for data-driven algorithm design

Maria-Florina Balcan
Carnegie Mellon University
ninamf@cs.cmu.edu

Dan DeBlasio
University of Texas at El Paso
dfdeblasio@utep.edu

Travis Dick
University of Pennsylvania
tbd@seas.upenn.edu

Ellen Vitercik
Carnegie Mellon University
vitercik@cs.cmu.edu

Carl Kingsford
Carnegie Mellon University
Ocean Genomics, Inc.
carlk@cs.cmu.edu

Tuomas Sandholm
Carnegie Mellon University
Optimized Markets, Inc.
Strategic Machine, Inc.
Strategy Robot, Inc.
sandholm@cs.cmu.edu

April 27, 2021

Abstract

Algorithms often have tunable parameters that impact performance metrics such as run-
time and solution quality. For many algorithms used in practice, no parameter settings admit
meaningful worst-case bounds, so the parameters are made available for the user to tune. Al-
ternatively, parameters may be tuned implicitly within the proof of a worst-case approximation
ratio or runtime bound. Worst-case instances, however, may be rare or nonexistent in practice.
A growing body of research has demonstrated that data-driven algorithm design can lead to sig-
niﬁcant improvements in performance. This approach uses a training set of problem instances
sampled from an unknown, application-speciﬁc distribution and returns a parameter setting
with strong average performance on the training set.

We provide a broadly applicable theory for deriving generalization guarantees that bound the
diﬀerence between the algorithm’s average performance over the training set and its expected
performance on the unknown distribution. Our results apply no matter how the parameters are
tuned, be it via an automated or manual approach. The challenge is that for many types of algo-
rithms, performance is a volatile function of the parameters: slightly perturbing the parameters
can cause a large change in behavior. Prior research [e.g., 7, 8, 10, 48] has proved generalization
bounds by employing case-by-case analyses of greedy algorithms, clustering algorithms, integer
programming algorithms, and selling mechanisms. We uncover a unifying structure which we
use to prove extremely general guarantees, yet we recover the bounds from prior research. Our
guarantees, which are tight up to logarithmic factors in the worst case, apply whenever an algo-
rithm’s performance is a piecewise-constant, -linear, or—more generally—piecewise-structured
function of its parameters. Our theory also implies novel bounds for voting mechanisms and
dynamic programming algorithms from computational biology.

1

Introduction

Algorithms often have tunable parameters that impact performance metrics such as runtime, so-
lution quality, and memory usage. These parameters may be set explicitly, as is often the case in
applied disciplines. For example, integer programming solvers expose over one hundred parame-
ters for the user to tune. There may not be parameter settings that admit meaningful worst-case

1

 
 
 
 
 
 
bounds, but after careful parameter tuning, these algorithms can quickly ﬁnd solutions to computa-
tionally challenging problems. However, applied approaches to parameter tuning have rarely come
with provable guarantees. Alternatively, an algorithm’s parameters may be set implicitly, as is of-
ten the case in theoretical computer science: a proof may implicitly optimize over a parameterized
family of algorithms in order to guarantee a worst-case approximation factor or runtime bound.
Worst-case bounds, however, can be overly pessimistic in practice. A growing body of research
(surveyed in a book chapter by Balcan [4]) has demonstrated the power of data-driven algorithm
design, where machine learning is used to ﬁnd parameter settings that work particularly well on
problems from the application domain at hand.

We present a broadly applicable theory for proving generalization guarantees in the context of
data-driven algorithm design. We adopt a natural learning-theoretic model of data-driven algorithm
design introduced by Gupta and Roughgarden [48]. As in the applied literature on automated algo-
rithm conﬁguration [e.g., 53, 55, 58, 66, 87, 106, 107], we assume there is an unknown, application-
speciﬁc distribution over the algorithm’s input instances. A learning procedure receives a training
set sampled from this distribution and returns a parameter setting—or conﬁguration—with strong
average performance over the training set. If the training set is too small, this conﬁguration may
have poor expected performance. Generalization guarantees bound the diﬀerence between aver-
age performance over the training set and actual expected performance. Our guarantees apply no
matter how the parameters are optimized, via an algorithmic search as in automated algorithm
conﬁguration [e.g., 24, 87, 106, 107], or manually as in experimental algorithmics [e.g., 15, 57, 73].
Across many types of algorithms—for example, combinatorial algorithms, integer programs,
and dynamic programs—the algorithm’s performance is a volatile function of its parameters. This
is a key challenge that distinguishes our results from prior research on generalization guarantees.
For well-understood functions in machine learning theory, there is generally a simple connection
between a function’s parameters and the value of the function. Meanwhile, slightly perturbing an
algorithm’s parameters can cause signiﬁcant changes in its behavior and performance. To provide
generalization bounds, we uncover structure that governs these volatile performance functions.

The structure we discover depends on the relationship between primal and dual functions [3].
To derive generalization bounds, a common strategy is to calculate the intrinsic complexity of a
function class U which we refer to as the primal class. Every function uρ ∈ U is deﬁned by a
parameter setting ρ ∈ Rd and uρ(x) ∈ R measures the performance of the algorithm parameterized
by ρ given the input x. We measure intrinsic complexity using the classic notion of pseudo-
dimension [83]. This is a challenging task because the domain of every function in U is a set of
problem instances, so there are no obvious notions of Lipschitz continuity or smoothness on which
we can rely. Instead, we use structure exhibited by the dual class U ∗. Every dual function u∗
x ∈ U ∗
is deﬁned by a problem instance x and measures the algorithm’s performance as a function of its
parameters given x as input. The dual functions have a simple, Euclidean domain Rd and we
demonstrate that they have ample structure which we can use to bound the pseudo-dimension of
U .

1.1 Our contributions

Our results apply to any parameterized algorithm with dual functions that exhibit a clear-cut, ubiq-
uitous structural property: the duals are piecewise constant, piecewise linear, or—more broadly—
piecewise structured. The parameter space decomposes into a small number of regions such that
within each region, the algorithm’s performance is “well behaved.” As an example, Figure 1 illus-
trates a piecewise-structured function of two parameters ρ[1] and ρ[2]. There are two functions g(1)
and g(2) that deﬁne a partition of the parameter space and four constant functions that deﬁne the

2

Figure 1: A piecewise-constant function over R2

≥0 with linear boundary functions g(1) and g(2).

function value on each subset from this partition.

More formally, the dual class U ∗ is (F , G, k)-piecewise decomposable if for every problem in-
stance, there are at most k boundary functions from a set G (for example, the set of linear sepa-
rators) that partition the parameter space into regions such that within each region, algorithmic
performance is deﬁned by a function from a set F (for example, the set of constant functions). We
bound the pseudo-dimension of U in terms of the pseudo- and VC-dimensions of the dual classes F ∗
and G∗, denoted Pdim (F ∗) and VCdim (G∗). This yields our main theorem: if [0, H] is the range
of the functions in U , then with probability 1 − δ over the draw of N training instances, for any
parameter setting, the diﬀerence between the algorithm’s average performance over the training
set and its expected performance is ˜O
. Speciﬁcally,
we prove that Pdim(U ) = ˜O (Pdim (F ∗) + VCdim (G∗) ln k) and that this bound is tight up to log
factors. The classes F and G are often so well structured that bounding Pdim (F ∗) and VCdim (G∗)
is straightforward.

(cid:0)Pdim (F ∗) + VCdim (G∗) ln k + ln 1
δ

(cid:113) 1
N

(cid:1)(cid:17)

H

(cid:16)

This is the most broadly applicable generalization bound for data-driven algorithm design in
the distributional learning model that applies to arbitrary input distributions. A nascent line
of research [7–11, 48] provides generalization bounds for a selection of parameterized algorithms.
Unlike the results in this paper, those papers analyze each algorithm individually, case by case.
Our approach recovers those bounds, implying guarantees for conﬁguring greedy algorithms [48],
clustering algorithms [7], and integer programming algorithms [7, 8], as well as mechanism design for
revenue maximization [10]. We also derive novel generalization bounds for computational biology
algorithms and voting mechanisms.

Proof insights. At a high level, we prove this guarantee by counting the number of parameter
settings with signiﬁcantly diﬀerent performance over any set S of problem instances. To do so, we
ﬁrst count the number of regions induced by the |S|k boundary functions that correspond to these
problem instances. This step subtly depends not on the VC-dimension of the class of boundary
functions G, but rather on VCdim (G∗). These |S|k boundary functions partition the parameter
space into regions where across all instances x in S, the dual functions u∗
x are simultaneously
structured. Within any one region, we use the pseudo-dimension of the dual class F ∗ to count the
number of parameter settings in that region with signiﬁcantly diﬀerent performance. We aggregate
these bounds over all regions to bound the pseudo-dimension of U .

Parameterized dynamic programming algorithms from computational biology. Our re-
sults imply bounds for a variety of computational biology algorithms that are used in practice. We
analyze parameterized sequence alignment algorithms [36, 45, 50, 81, 82] as well as RNA folding
algorithms [80], which predict how an input RNA strand would naturally fold, oﬀering insight into
the molecule’s function. We also provide guarantees for algorithms that predict topologically asso-

3

ciating domains in DNA sequences [37], which shed light on how DNA wraps into three-dimensional
structures that inﬂuence genome function.

Parameterized voting mechanisms. A mechanism is a special type of algorithm designed to
help a set of agents come to a collective decision. For example, a town’s residents may want to
build a public resource such as a park, pool, or skating rink, and a mechanism would help them
decide which to build. We analyze neutral aﬃne maximizers [74, 78, 86], a well-studied family of
parameterized mechanisms. The parameters can be tuned to maximize social welfare, which is the
sum of the agents’ values for the mechanism’s outcome.

1.2 Additional related research

A growing body of theoretical research investigates how machine learning can be incorporated in
the process of algorithm design [2, 7–12, 17, 20, 31, 32, 39, 48, 54, 61, 62, 72, 75, 85, 102–104]. A
chapter by Balcan [4] provides a survey. We highlight a few of the papers that are most related to
ours below.

1.2.1 Prior research

Runtime optimization with provable guarantees. Kleinberg et al. [61, 62] and Weisz et al.
[103, 104] provide conﬁguration procedures with provable guarantees when the goal is to minimize
runtime. In contrast, our bounds apply to arbitrary performance metrics, such as solution quality as
well as runtime. Also, their procedures are designed for the case where the set of parameter settings
is ﬁnite (although they can still oﬀer some guarantees when the parameter space is inﬁnite by ﬁrst
sampling a ﬁnite set of parameter settings and then running the conﬁguration procedure; Balcan
et al. [8, 13] study what kinds of guarantees discretization approaches can and cannot provide). In
contrast, our guarantees apply immediately to inﬁnite parameter spaces. Finally, unlike our results,
the guarantees from this prior research are not conﬁguration-procedure-agnostic: they apply only
to the speciﬁc procedures that are proposed.

Learning-augmented algorithms. A related line of research has designed algorithms that re-
place some steps of a classic worst-case algorithm with a machine-learned oracle that makes pre-
dictions about structural aspects of the input [54, 72, 75, 85]. If the prediction is accurate, the
algorithm’s performance (for example, its error or runtime) is superior to the original worst-case
algorithm, and if the prediction is incorrect, the algorithm performs as well as that worst-case al-
gorithm. Though similar, our approach to data-driven algorithm design is diﬀerent because we are
not attempting to learn structural aspects of the input; rather, we are optimizing the algorithm’s
parameters directly using the training set. Moreover, we can also compete with the best-known
worst-case algorithm by including it in the algorithm class over which we optimize. Just adding that
one extra algorithm—however diﬀerent—does not increase our sample complexity bounds. That
best-in-the-worst-case algorithm does not have to be a special case of the parameterized algorithm.

Dispersion. Prior research by Balcan, Dick, and Vitercik [9] as well as concurrent research
by Balcan, Dick, and Pegden [12] provides provable guarantees for algorithm conﬁguration, with a
particular focus on online learning and privacy-preserving algorithm conﬁguration. These tasks are
impossible in the worst case, so these papers identify a property of the dual functions under which
online and private conﬁguration are possible. This condition is dispersion, which, roughly speaking,
requires that the discontinuities of the dual functions are not too concentrated in any ball. Online

4

learning guarantees imply sample complexity guarantees due to online-to-batch conversion, and
Balcan et al. [9] also provide sample complexity guarantees based on dispersion using Rademacher
complexity.

To prove that dispersion holds, one typically needs to show that under the distribution over
problem instances, the dual functions’ discontinuities do not concentrate. This argument is typically
made by assuming that the distribution is suﬃciently nice or—when applicable—by appealing to the
random nature of the parameterized algorithm. Thus, for arbitrary distributions and deterministic
In contrast, our results hold even when the
algorithms, dispersion does not necessarily hold.
discontinuities concentrate, and thus are applicable to a broader set of problems in the distributional
learning model. In other words, the results from this paper cannot be recovered using the techniques
of Balcan et al. [9, 12].

1.2.2 Concurrent and subsequent research

Subsequently to the appearance of the original version of this paper in 2019 [16], an extensive
body of research has developed that studies the use of machine learning in the context of algorithm
design, as we highlight below.

Learning-augmented algorithms. The literature on learning-augmented algorithms (summa-
rized in the previous section) has continued to ﬂourish in subsequent research [26, 27, 31, 32, 56, 65,
65, 102]. Some of these papers make explicit connections to the types of parameter optimization
problems we study in this paper, such as research by Lavastida et al. [65], who study online ﬂow
allocation and makespan minimization problems. They formulate the machine-learned predictions
as a set of parameters and study the sample complexity of learning a good parameter setting. An
interesting direction for future research is to investigate which other problems from this literature
can be formulated as parameter optimization algorithms, and whether the techniques in this paper
can be used to derive tighter or novel guarantees.

Sample complexity bounds for data-driven algorithm design. Chawla et al. [20] study
a data-driven algorithm design problem for the Pandora’s box problem, where there is a set of
alternatives with costs drawn from an unknown distribution. A search algorithm observes the
alternatives’ costs one-by-one, eventually stopping and selecting one alternative. The authors show
how to learn an algorithm that minimizes the selected alternative’s expected cost, plus the number
of alternatives the algorithm examines. The primary contributions of that paper are 1) identifying
a ﬁnite subset of algorithms that compete with the optimal algorithm, and 2) showing how to
eﬃciently optimize over that ﬁnite subset of algorithms. Since the authors prove that they only
need to optimize over a ﬁnite subset of algorithms, the sample complexity of this approach follows
from a Hoeﬀding and union bound.

Blum et al. [17] study a data-driven approach to learning a nearly optimal cooling schedule
for the simulated annealing algorithm. They provide upper and lower sample complexity bounds,
with their upper bound following from a careful covering number argument. We leave as an open
question whether our techniques can be combined with theirs to match their sample complexity
√
lower bound of ˜Ω ( 3

m), where m is the cooling schedule length.

Machine learning for combinatorial optimization. A growing body of applied research has
developed machine learning approaches to discrete optimization, largely with the aim of improving
classic optimization algorithms such as branch-and-bound [30, 35, 38, 63, 84, 91, 93–95, 101, 108].

5

For example, Chmiela et al. [21] present data-driven approaches to scheduling heuristics in branch-
and-bound, and they leave as an open question whether the techniques in this paper can be used
to provide provable guarantees.

2 Notation and problem statement

We study algorithms parameterized by a set P ⊆ Rd. As a concrete example, parameterized
algorithms are often used for sequence alignment [45]. There are many features of an alignment
one might wish to optimize, such as the number of matches, mismatches, or indels (deﬁned in
Section 4.1). A parameterized objective function is deﬁned by weighting these features. As another
example, hierarchical clustering algorithms often use linkage routines such as single, complete, and
average linkage. Parameters can be used to interpolate between these three classic procedures [7],
which can be outperformed with a careful parameter tuning [11].

We use X to denote the set of problem instances the algorithm takes as input. We measure
the performance of the algorithm parameterized by ρ = (ρ[1], . . . , ρ[d]) ∈ Rd via a utility function
uρ : X → [0, H], with U = {uρ : ρ ∈ P} denoting the set of all such functions. We assume there is
an unknown, application-speciﬁc distribution D over X .

Our goal is to ﬁnd a parameter vector in P with high performance in expectation over the
distribution D. We provide generalization guarantees for this problem. Given a training set of
problem instances S sampled from D, a generalization guarantee bounds the diﬀerence—for any
choice of the parameters ρ—between the average performance of the algorithm over S and its
expected performance.

Speciﬁcally, our main technical contribution is a bound on the pseudo-dimension [83] of the set
U . For any arbitrary set of functions H that map an abstract domain Y to R, the pseudo-dimension
of H, denoted Pdim(H), is the size of the largest set {y1, . . . , yN } ⊆ Y such that for some set of
targets z1, . . . , zN ∈ R,

|{(sign (h (y1) − z1) , . . . , sign (h (yN ) − zN )) | h ∈ H}| = 2N .

(1)

Classic results from learning theory [83] translate pseudo-dimension bounds into generalization
guarantees. For example, suppose [0, H] is the range of the functions in H. For any δ ∈ (0, 1) and
any distribution D over Y, with probability 1 − δ over the draw of S ∼ DN , for all functions h ∈ H,
the diﬀerence between the average value of h over S and its expected value is bounded as follows:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

(cid:88)

y∈S

h(y) − E
y∼D

(cid:12)
(cid:12)
(cid:12)
[h(y)]
(cid:12)
(cid:12)
(cid:12)

(cid:32)

(cid:115)

= O

H

1
N

(cid:18)

Pdim(H) + ln

(cid:19)(cid:33)

.

1
δ

(2)

When H is a set of binary-valued functions that map Y to {0, 1}, the pseudo-dimension of H is
more commonly referred to as the VC-dimension of H [97], denoted VCdim(H).

3 Generalization guarantees for data-driven algorithm design

In data-driven algorithm design, there are two closely related function classes. First, for each
parameter setting ρ ∈ P, uρ : X → R measures performance as a function of the input x ∈
X . Similarly, for each input x, there is a function ux : P → R deﬁned as ux(ρ) = uρ(x) that
measures performance as a function of the parameter vector ρ. The set {ux | x ∈ X } is equivalent
to Assouad’s notion of the dual class [3].

6

Figure 2: Boundary functions partitioning R2. The arrows indicate on which side of each function
g(i)(ρ) = 0 and on which side g(i)(ρ) = 1. For example, g(1) (ρ1) = 1, g(1) (ρ2) = 1, and g(1) (ρ3) =
0.

Deﬁnition 3.1 (Dual class [3]). For any domain Y and set of functions H ⊆ RY , the dual class of
H is deﬁned as H∗ = (cid:8)h∗
y ∈ H∗ ﬁxes an
input y ∈ Y and maps each function h ∈ H to h(y). We refer to the class H as the primal class.

y : H → R | y ∈ Y(cid:9) where h∗

y(h) = h(y). Each function h∗

The set of functions {ux | x ∈ X } is equivalent to the dual class U ∗ = {u∗

x : U → [0, H] | x ∈ X }
in the sense that for every parameter vector ρ ∈ P and every problem instance x ∈ X , ux(ρ) =
u∗
x (uρ).

Many combinatorial algorithms share a clear-cut, useful structure: for each instance x ∈ X ,
the function ux is piecewise structured. For example, each function ux might be piecewise constant
with a small number of pieces. Given the equivalence of the functions {ux | x ∈ X } and the dual
class U ∗, the dual class exhibits this piecewise structure as well. We use this structure to bound
the pseudo-dimension of the primal class U .

Intuitively, a function h : Y → R is piecewise structured if we can partition the domain Y
into subsets Y1, . . . , YM so that when we restrict h to one piece Yi, h equals some well-structured
function f : Y → R.
In other words, for all y ∈ Yi, h(y) = f (y). We deﬁne the partition
Y1, . . . , YM using boundary functions g(1), . . . , g(k) : Y → {0, 1}. Each function g(i) divides the
domain Y into two sets: the points it labels 0 and the points it labels 1. Figure 2 illustrates a
partition of R2 by boundary functions. Together, the k boundary functions partition the domain Y
into at most 2k regions, each one corresponding to a bit vector b ∈ {0, 1}k that describes on which
side of each boundary the region belongs. For each region, we specify a piece function fb : Y → R
that deﬁnes the function values of h restricted to that region. Figure 1 shows an example of a
piecewise-structured function with two boundary functions and four piece functions.

For many parameterized algorithms, every function in the dual class is piecewise structured.
Moreover, across dual functions, the boundary functions come from a single, ﬁxed class, as do the
piece functions. For example, the boundary functions might always be halfspace indicator functions,
while the piece functions might always be linear functions. The following deﬁnition captures this
structure.

Deﬁnition 3.2. A function class H ⊆ RY that maps a domain Y to R is (F , G, k)-piecewise
decomposable for a class G ⊆ {0, 1}Y of boundary functions and a class F ⊆ RY of piece functions
if the following holds: for every h ∈ H, there are k boundary functions g(1), . . . , g(k) ∈ G and a
piece function fb ∈ F for each bit vector b ∈ {0, 1}k such that for all y ∈ Y, h(y) = fby (y) where
by = (cid:0)g(1)(y), . . . , g(k)(y)(cid:1) ∈ {0, 1}k.

Our main theorem shows that whenever the dual class U ∗ is (F , G, k)-piecewise decomposable,
we can bound the pseudo-dimension of U in terms of the VC-dimension of G∗ and the pseudo-
dimension of F ∗. Later, we show that for many common classes F and G, we can easily bound the
complexity of their duals.

7

Theorem 3.3. Suppose that the dual function class U ∗ is (F , G, k)-piecewise decomposable with
boundary functions G ⊆ {0, 1}U and piece functions F ⊆ RU . The pseudo-dimension of U is
bounded as follows:

Pdim(U ) = O ((Pdim(F ∗) + VCdim(G∗)) ln (Pdim(F ∗) + VCdim(G∗)) + VCdim(G∗) ln k) .

To help make the proof of Theorem 3.3 succinct, we extract a key insight in the following
lemma. Given a set of functions H that map a domain Y to {0, 1}, Lemma 3.4 bounds the number
of binary vectors

(h1(y), . . . , hN (y))

(3)

we can obtain for any N functions h1, . . . , hN ∈ H as we vary the input y ∈ Y. Pictorially, if we
partition R2 using the functions g(1), g(2), and g(3) from Figure 2 for example, Lemma 3.4 bounds
the number of regions in the partition. This bound depends not on the VC-dimension of the class H,
but rather on that of its dual H∗. We use a classic lemma by Sauer [90] to prove Lemma 3.4. Sauer’s
lemma [90] bounds the number of binary vectors of the form (h (y1) , . . . , h (yN )) we can obtain for
any N elements y1, . . . , yN ∈ Y as we vary the function h ∈ H by (eN )VCdim(H). Therefore, it does
not immediately imply a bound on the number of vectors from Equation (3). In order to apply
Sauer’s lemma, we must transition to the dual class.

Lemma 3.4. Let H be a set of functions that map a domain Y to {0, 1}. For any functions
h1, . . . , hN ∈ H, the number of binary vectors (h1(y), . . . , hN (y)) obtained by varying the input
y ∈ Y is bounded as follows:

|{(h1(y), . . . , hN (y)) | y ∈ Y}| ≤ (eN )VCdim(H∗).

(4)

Proof. We rewrite the left-hand-side of Equation (4) as
ﬁx N inputs and vary the function h∗

y (h1) , . . . , h∗
y, the lemma statement follows from Sauer’s lemma [90].

(cid:12) . Since we

y (hN )(cid:1) (cid:12)

(cid:12) y ∈ Y(cid:9)(cid:12)

(cid:12)
(cid:12)

(cid:8)(cid:0)h∗

We now prove Theorem 3.3.

Proof of Theorem 3.3. Fix an arbitrary set of problem instances x1, . . . , xN ∈ X and targets
z1, . . . , zN ∈ R. We bound the number of ways that U can label the problem instances x1, . . . , xN
with respect to the target thresholds z1, . . . , zN ∈ R. In other words, as per Equation (1), we bound
the size of the set







(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

sign (uρ (x1) − z1)
...
sign (uρ (xN ) − zN )






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ρ ∈ P


(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

=







(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

sign (cid:0)u∗

x1 (uρ) − z1

...

sign (cid:0)u∗

xN (uρ) − zN

(cid:1)






(cid:1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ρ ∈ P


(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(5)

by (ekN )VCdim(G∗)(eN )Pdim(F ∗). Then solving for the largest N such that

2N ≤ (ekN )VCdim(G∗)(eN )Pdim(F ∗)

gives a bound on the pseudo-dimension of U . Our bound on Equation (5) has two main steps:

1. In Claim 3.5, we show that there are M < (ekN )VCdim(G∗) subsets P1, . . . , PM partitioning
the parameter space P such that within any one subset, the dual functions u∗
x1, . . . , u∗
xN
are simultaneously structured. In particular, for each subset Pj, there exist piece functions
f1, . . . , fN ∈ F such that u∗
xi (uρ) = fi (uρ) for all parameter settings ρ ∈ Pj and i ∈ [N ].
This is the partition of P induced by aggregating all of the boundary functions corresponding
to the dual functions u∗

x1, . . . , u∗

xN .

8

2. We then show that for any region Pj of the partition, as we vary the parameter vector ρ ∈ Pj,
uρ can label the problem instances x1, . . . , xN in at most (eN )Pdim(F ∗) ways with respect to
the target thresholds z1, . . . , zN . It follows that the total number of ways that U can label
the problem instances x1, . . . , xN is bounded by (ekN )VCdim(G∗)(eN )Pdim(F ∗).

We now prove the ﬁrst claim.

Claim 3.5. There are M < (ekN )VCdim(G∗) subsets P1, . . . , PM partitioning the parameter space P
such that within any one subset, the dual functions u∗
are simultaneously structured. In
particular, for each subset Pj, there exist piece functions f1, . . . , fN ∈ F such that u∗
xi (uρ) = fi (uρ)
for all parameter settings ρ ∈ Pj and i ∈ [N ].

x1, . . . , u∗
xN

(cid:111)

(1)
i

(k)
i

x1, . . . , u∗
xN

Proof of Claim 3.5. Let u∗
∈ U ∗ be the dual functions corresponding to the problem
instances x1, . . . , xN . Since U ∗ is (F , G, k)-piecewise decomposable, we know that for each function
(k)
i ∈ G ⊆ {0, 1}U that deﬁne its piecewise decomposi-
u∗
xi, there are k boundary functions g
(cid:110)
tion. Let ˆG = (cid:83)N
(1)
be the union of these boundary functions across all i ∈ [N ]. For
g
i=1
i
ease of notation, we relabel the functions in ˆG, calling them g1, . . . , gkN . Let M be the total number
of kN -dimensional vectors we can obtain by applying the functions in ˆG ⊆ {0, 1}U to elements of
U :

, . . . , g

, . . . , g

M :=

g1 (uρ)
...
gkN (uρ)
By Lemma 3.4, M < (ekN )VCdim(G∗) . Let b1, . . . , bM be the binary vectors in the set from Equa-
tion (6). For each i ∈ [M ], let Pj = {ρ | (g1 (uρ) , . . . , gkN (uρ)) = bj} . By construction, for each set
Pj, the values of all the boundary functions g1 (uρ) , . . . , gkN (uρ) are constant as we vary ρ ∈ Pj.
Therefore, there is a ﬁxed set of piece functions f1, . . . , fN ∈ F so that u∗
xi (uρ) = fi (uρ) for all
parameter vectors ρ ∈ Pj and indices i ∈ [N ]. Therefore, the claim holds.


 : ρ ∈ P


(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)


(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)




(6)





.

Claim 3.5 and Equation (5) imply that for every subset Pj of the partition,







(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

sign (uρ (x1) − z1)
...
sign (uρ (xN ) − zN )






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ρ ∈ Pj


(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=











sign (f1 (uρ) − z1)
...
sign (fN (uρ) − zN )






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ρ ∈ Pj


(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

.

(7)

Lemma 3.4 implies that Equation (7) is bounded by (eN )Pdim(F ∗).

In other words, for any
region Pj of the partition, as we vary the parameter vector ρ ∈ Pj, uρ can label the problem
instances x1, . . . , xN in at most (eN )Pdim(F ∗) ways with respect to the target thresholds z1, . . . , zN .
Because there are M < (ekN )VCdim(G∗) regions Pj of the partition, we can conclude that U can label
the problem instances x1, . . . , xN in at most (ekN )VCdim(G∗)(eN )Pdim(F ∗) distinct ways relative to
the targets z1, . . . , zN . In other words, Equation (5) is bounded by (ekN )VCdim(G∗)(eN )Pdim(F ∗).
On the other hand, if U shatters the problem instances x1, . . . , xN , then the number of distinct
labelings must be 2N . Therefore, the pseduo-dimension of U is at most the largest value of N such
that 2N ≤ (ekN )VCdim(G∗)(eN )Pdim(F ∗), which implies that

N = O ((Pdim(F ∗) + VCdim(G∗)) ln (Pdim(F ∗) + VCdim(G∗)) + VCdim(G∗) ln k) ,

as claimed.

We prove several lower bounds which show that Theorem 3.3 is tight up to logarithmic factors.

9

(a) Constant
(zero oscillations).

function

(b) Linear function (one
oscillation).

(c) Inverse-quadratic function of the form
h(ρ) = a

ρ2 + bρ + c (two oscillations).

Figure 3: Each solid line is a function with bounded oscillations and each dotted line is an arbitrary
threshold. Many parameterized algorithms have piecewise-structured duals with piece functions
from these families.

Theorem 3.6. The following lower bounds hold:

1. There is a parameterized sequence alignment algorithm with Pdim(U ) = Ω(log n) for some n ≥
1. Its dual class U ∗ is (F , G, n)-piecewise decomposable for classes F and G with Pdim (F ∗) =
VCdim (G∗) = 1.

2. There is a parameterized voting mechanism with Pdim(U ) = Ω(n) for some n ≥ 1. Its dual
class U ∗ is (F , G, 2)-piecewise decomposable for classes F and G with Pdim (F ∗) = 1 and
VCdim (G∗) = n.

Proof. In Theorem 4.3, we prove the result for sequence alignment, in which case n is the maximum
length of the sequences, F is the set of constant functions, and G is the set of threshold functions.
In Theorem 5.2, we prove the result for voting mechanisms, in which case n is the number of
agents that participate in the mechanism, F is the set of constant functions, and G is the set of
homogeneous linear separators in Rn.

Applications of our main theorem to representative function classes

We now instantiate Theorem 3.3 in a general setting inspired by data-driven algorithm design.

One-dimensional functions with a bounded number of oscillations. Let U = {uρ | ρ ∈ R}
be a set of utility functions deﬁned over a single-dimensional parameter space. We often ﬁnd that
the dual functions are piecewise constant, linear, or polynomial. More generally, the dual functions
are piecewise structured with piece functions that oscillate a ﬁxed number of times.
In other
words, the dual class U ∗ is (F , G, k)-piecewise decomposable where the boundary functions G are
thresholds and the piece functions F oscillate a bounded number of times, as formalized below.

Deﬁnition 3.7. A function h : R → R has at most B oscillations if for every z ∈ R, the function
ρ (cid:55)→ I{h(ρ)≥z} is piecewise constant with at most B discontinuities.

Figure 3 illustrates three common types of functions with bounded oscillations. In the following
lemma, we prove that if H is a class of functions that map R to R, each of which has at most B
oscillations, then Pdim(H∗) = O(ln B).

Lemma 3.8. Let H be a class of functions mapping R to R, each of which has at most B oscilla-
tions. Then Pdim(H∗) = O(ln B).

Proof. Suppose that Pdim (H∗) = N . Then there exist functions h1, . . . , hN ∈ H and witnesses
z1, . . . , zN ∈ R such that for every subset T ⊆ [N ], there exists a parameter setting ρ ∈ R such that
ρ (hi) ≥ zi if and only if i ∈ T . We can simplify notation as follows: since h (ρ) = h∗
h∗
ρ (h) for every

10

function h ∈ H, we have that for every subset T ⊆ [N ], there exists a parameter setting ρ ∈ R such
that hi (ρ) ≥ zi if and only if i ∈ T . Let P ∗ be the set of 2N parameter settings corresponding to
each subset T ⊆ [N ]. By deﬁnition, these parameter settings induce 2N distinct binary vectors as
follows:







(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

I{h1(ρ)≥z1}
...
I{hN (ρ)≥zN }


 : ρ ∈ P ∗


(cid:12)

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

= 2N .

On the other hand, since each function hi has at most B oscillations, we can partition R into
M ≤ BN + 1 intervals I1, . . . , IM such that for every interval Ij and every i ∈ [N ], the function
ρ (cid:55)→ I{hi(ρ)≥zi} is constant across the interval Ij. Therefore, at most one parameter setting ρ ∈ P ∗
can fall within a single interval Ij. Otherwise, if ρ, ρ(cid:48) ∈ Ij ∩ P ∗, then






I{h1(ρ)≥z1}
...
I{hN (ρ)≥zN }






 =




I{h1(ρ(cid:48))≥z1}
...
I{hN (ρ(cid:48))≥zN }




 ,

which is a contradiction. As a result, 2N ≤ BN + 1. The lemma then follows from Lemma A.1.

Lemma 3.8 implies the following pseudo-dimension bound when the dual function class U ∗ is
(F , G, k)-piecewise decomposable, where the boundary functions G are thresholds and the piece
functions F oscillate a bounded number of times.
Lemma 3.9. Let U = {uρ | ρ ∈ R} be a set of utility functions and suppose the dual class U ∗ is
(F , G, k)-decomposable, where the boundary functions G = {ga | a ∈ R} are thresholds ga : uρ (cid:55)→
I{a≤ρ}. Suppose for each f ∈ F , the function ρ (cid:55)→ f (uρ) has at most B oscillations. Then
Pdim(U ) = O((ln B) ln(k ln B)).
Proof. First, we claim that VCdim (G∗) = 1. For a contradiction, suppose G∗ can shatter two
functions ga, gb ∈ G∗, where a < b. There must be a parameter setting ρ ∈ R where g∗
uρ (ga) =
ga (uρ) = I{a≤ρ} = 0 and g∗
uρ (gb) = gb (uρ) = I{b≤ρ} = 1. Therefore, b ≤ ρ < a, which is a
contradiction, so VCdim (G∗) = 1.

Next, we claim that Pdim (F ∗) = O(ln B). For each function f ∈ F , let hf : R → R be
deﬁned as hf (ρ) = f (uρ). By assumption, each function hf has at most B oscillations. Let
H = {hf | f ∈ F } and let N = Pdim (H∗). By Lemma 3.8, we know that N = O(ln B). We claim
that Pdim(H∗) ≥ Pdim(F ∗). For a contradiction, suppose the class F ∗ can shatter N + 1 functions
f1, . . . , fN +1 using witnesses z1, . . . , zN +1 ∈ R. By deﬁnition, this means that


(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)
(cid:12)

I(cid:110)

(cid:111)

I(cid:110)

f ∗
uρ (f1)≥z1
...

f ∗
uρ (fN +1)≥zN +1








(cid:111)

: ρ ∈ P

(cid:12)

(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

= 2N +1.

I



{h∗

uρ(f ) = f (uρ) = hf (ρ) = h∗

For any function f ∈ F and any parameter setting ρ ∈ R, f ∗
Therefore,

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)


(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)
(cid:12)
which contradicts the fact that Pdim(H∗) = N . Therefore, Pdim(F ∗) ≤ N = O(ln B). The
corollary then follows from Theorem 3.3.

ρ(hf1)≥z1}
...

f ∗
uρ (f1)≥z1
...

(cid:12)

(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)


(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

f ∗
uρ (fN +1)≥zN +1

= 2N +1,

: ρ ∈ P

: ρ ∈ P

ρ(hf ).






hfN +1

≥zN +1









I(cid:110)

I(cid:110)

I(cid:110)





=

h∗
ρ

(cid:111)

(cid:111)

(cid:111)

(cid:17)

(cid:16)

11

Multi-dimensional piecewise-linear functions.
In many data-driven algorithm design prob-
lems, we ﬁnd that the boundary functions correspond to halfspace thresholds and the piece functions
correspond to constant or linear functions. We handle this case in the following lemma.
Lemma 3.10. Let U = (cid:8)uρ | ρ ∈ P ⊆ Rd(cid:9) be a class of utility functions deﬁned over a d-dimensional
parameter space. Suppose the dual class U ∗ is (F , G, k)-piecewise decomposable, where the bound-
ary functions G = (cid:8)fa,θ : U → {0, 1} | a ∈ Rd, θ ∈ R(cid:9) are halfspace indicator functions ga,θ : uρ (cid:55)→
I{a·ρ≤θ} and the piece functions F = (cid:8)fa,θ : U → R | a ∈ Rd, θ ∈ R(cid:9) are linear functions fa,θ :
uρ (cid:55)→ a · ρ + θ. Then Pdim(U ) = O(d ln(dk)).

The proof of this lemma follows from classic VC- and pseudo-dimension bounds for linear

functions and can be found in Appendix B.

4 Parameterized computational biology algorithms

We study algorithms that are used in practice for three biological problems: sequence alignment,
RNA folding, and predicting topologically associated domains in DNA. In these applications, there
are two unifying similarities. First, algorithmic performance is measured in terms of the distance
between the algorithm’s output and a ground-truth solution. In most cases, this solution is dis-
covered using laboratory experimentation, so it is only available for the instances in the training
set. Second, these algorithms use dynamic programming to maximize parameterized objective
functions. This objective function represents a surrogate optimization criterion for the dynamic
programming algorithm, whereas utility measures how well the algorithm’s output resembles the
ground truth. There may be multiple solutions that maximize this objective function, which we
call co-optimal. Although co-optimal solutions have the same objective function value, they may
have diﬀerent utilities. To handle tie-breaking, we assume that in any region of the parameter
space where the set of co-optimal solutions is ﬁxed, the algorithm’s output is also ﬁxed, which is
typically true in practice.

4.1 Sequence alignment

4.1.1 Global pairwise sequence alignment

In pairwise sequence alignment, the goal is to line up strings in order to identify regions of simi-
larity. In biology, for example, these similar regions indicate functional, structural, or evolutionary
relationships between the sequences. Formally, let Σ be an alphabet and let S1, S2 ∈ Σn be two
sequences. A sequence alignment is a pair of sequences τ1, τ2 ∈ (Σ ∪ {−})∗ such that |τ1| = |τ2|,
del (τ1) = S1, and del (τ2) = S2, where del is a function that deletes every −, or gap character.
There are many features of an alignment that one might wish to optimize, such as the number
of matches (τ1[i] = τ2[i]), mismatches (τ1[i] (cid:54)= τ2[i]), indels (τ1[i] = − or τ2[i] = −), and gaps
(maximal sequences of consecutive gap characters in τ ∈ {τ1, τ2}). We denote these features using
functions (cid:96)1, . . . , (cid:96)d that map pairs of sequences (S1, S2) and alignments L to R.

A common dynamic programming algorithm Aρ [45, 100] returns the alignment L that maxi-

mizes the objective function

ρ[1] · (cid:96)1 (S1, S2, L) + · · · + ρ[d] · (cid:96)d (S1, S2, L) ,

(8)

where ρ ∈ Rd is a parameter vector. We denote the output alignment as Aρ (S1, S2). As Gus-
ﬁeld, Balasubramanian, and Naor [50] wrote, “there is considerable disagreement among molecular
biologists about the correct choice” of a parameter setting ρ.

12

We assume that there is a utility function that characterizes an alignment’s quality, denoted
u(S1, S2, L) ∈ R. For example, u(S1, S2, L) might measure the distance between L and a “ground
truth” alignment of S1 and S2 [89]. We then deﬁne uρ (S1, S2) = u (S1, S2, Aρ (S1, S2)) to be the
utility of the alignment returned by the algorithm Aρ.

In the following lemma, we prove that the set of utility functions uρ has piecewise-structured

dual functions.
Lemma 4.1. Let U be the set of functions U = (cid:8)uρ : (S1, S2) (cid:55)→ u (S1, S2, Aρ (S1, S2)) | ρ ∈ Rd(cid:9)
that map sequence pairs S1, S2 ∈ Σn to R. The dual class U ∗ is (cid:0)F , G, 4nn4n+2(cid:1)-piecewise de-
composable, where F = {fc : U → R | c ∈ R} consists of constant functions fc : uρ (cid:55)→ c and
G = (cid:8)ga : U → {0, 1} | a ∈ Rd(cid:9) consists of halfspace indicator functions ga : uρ (cid:55)→ I{a·ρ<0}.

Proof. Fix a sequence pair S1 and S2. Let L be the set of alignments the algorithm returns as
we range over all parameter vectors ρ ∈ Rd.
In
Lemma C.1, we prove that |L| ≤ 2nn2n+1. For any alignment L ∈ L, the algorithm Aρ will return
L if and only if

In other words, L = (cid:8)Aρ(S1, S2) | ρ ∈ Rd(cid:9).

ρ[1] · (cid:96)1 (S1, S2, L) + · · · + ρ[d] · (cid:96)d (S1, S2, L) > ρ[1] · (cid:96)1

(cid:0)S1, S2, L(cid:48)(cid:1) + · · · + ρ[d] · (cid:96)d

(cid:0)S1, S2, L(cid:48)(cid:1) (9)

2

(cid:1) ≤ 4nn4n+2 hyperplanes such that
for all L(cid:48) ∈ L\{L}. Therefore, there is a set H of at most (cid:0)2nn2n+1
across all parameter vectors ρ in a single connected component of Rd\H, the output of the algorithm
parameterized by ρ, Aρ(S1, S2), is ﬁxed. (As is standard, Rd \H indicates set removal.) This means
that for any connected component R of Rd \H, there exists a real value cR such that uρ(S1, S2) = cR
for all ρ ∈ R. By deﬁnition of the dual, this means that u∗

S1,S2 (uρ) = uρ (S1, S2) = cR as well.

We now use this structure to show that the dual class U ∗ is (cid:0)F , G, 4nn4n+2(cid:1)-piecewise decom-
posable, as per Deﬁnition 3.2. Recall that G = (cid:8)ga : U → {0, 1} | a ∈ Rd(cid:9) consists of halfspace
indicator functions ga : uρ (cid:55)→ I{a·ρ<0} and F = {fc : U → R | c ∈ R} consists of constant functions
fc : uρ (cid:55)→ c. For each pair of alignments L, L(cid:48) ∈ L, let g(L,L(cid:48)) ∈ G correspond to the halfspace
represented in Equation (9). Order these k := (cid:0)|L|
(cid:1) functions arbitrarily as g(1), . . . , g(k). Every
2
connected component R of Rd \ H corresponds to a sign pattern of the k hyperplanes. For a given
region R, let bR ∈ {0, 1}k be the corresponding sign pattern. Deﬁne the function f (bR) ∈ F as
f (bR) = fcR, so f (bR) (uρ) = cR for all ρ ∈ Rd. Meanwhile, for every vector b not corresponding to
a sign pattern of the k hyperplanes, let f (b) = f0, so f (b) (uρ) = 0 for all ρ ∈ Rd. In this way, for
every ρ ∈ Rd,

u∗
S1,S2 (uρ) =

(cid:88)

{g(i)(uρ)=b[i],∀i∈[k]}f (b)(uρ),
I

as desired.

b∈{0,1}k

Lemmas 3.10 and 4.1 imply that Pdim(U ) = O(nd ln n + d ln d). In Appendix C, we also prove
tighter guarantees for a structured subclass of algorithms [45, 100].
In that case, d = 4 and
(cid:96)1 (S1, S2, L) is the number of matches in the alignment, (cid:96)2 (S1, S2, L) is the number of mismatches,
(cid:96)3 (S1, S2, L) is the number of indels, and (cid:96)4 (S1, S2, L) is the number of gaps. Building on prior
research [36, 50, 81], we show (Lemma 4.2) that the dual class U ∗ is (cid:0)F , G, O (cid:0)n3(cid:1)(cid:1)-piecewise
decomposable with F and G deﬁned as in Lemma 4.1. This implies a pseudo-dimension bound of
O(ln n), which is signiﬁcantly tighter than that of Lemma 4.1. We also prove that this pseudo-
dimension bound is tight with a lower bound of Ω(ln n) (Theorem 4.3). Moreover, in Appendix 4.1.3,
we provide guarantees for algorithms that align more than two sequences.

13

4.1.2 Tighter guarantees for a structured algorithm subclass: the aﬃne-gap model

A line of prior work [36, 50, 81, 82] analyzed a speciﬁc instantiation of the objective function
(8) where d = 3.
In this case, we can obtain a pseudo-dimension bound of O(ln n), which is
exponentially better than the bound implied by Lemma 4.1. Given a pair of sequences S1, S2 ∈ Σn,
the dynamic programming algorithm Aρ returns the alignment L maximizes the objective function

mt(S1, S2, L) − ρ[1] · ms(S1, S2, L) − ρ[2] · id(S1, S2, L) − ρ[3] · gp(S1, S2, L),

where mt(S1, S2, L) equals the number of matches, ms(S1, S2, L) is the number of mismatches,
id(S1, S2, L) is the number of indels, gp(S1, S2, L) is the number of gaps, and ρ = (ρ[1], ρ[2], ρ[3]) ∈
R3 is a parameter vector. We denote the output alignment as Aρ (S1, S2). This is known as
the aﬃne-gap scoring model. We exploit speciﬁc structure exhibited by this algorithm family to
obtain the exponential pseudo-dimension improvement. This useful structure guarantees that for
any pair of sequences S1 and S2, there are only O (cid:0)n3/2(cid:1) diﬀerent alignments the algorithm family
(cid:8)Aρ | ρ ∈ R3(cid:9) might produce as we range over parameter vectors [36, 50, 81]. This bound is
exponentially smaller than our generic bound of 4nn4n+2 from Lemma C.1.

Lemma 4.2. Let U be the set of functions

U = {uρ : (S1, S2) (cid:55)→ u (S1, S2, Aρ (S1, S2)) | ρ ∈ R≥0}
that map sequence pairs S1, S2 ∈ Σn to R. The dual class U ∗ is (cid:0)F , G, O (cid:0)n3(cid:1)(cid:1)-piecewise de-
composable, where F = {fc : U → R | c ∈ R} consists of constant functions fc : uρ (cid:55)→ c
and where G = {ga : U → {0, 1} | a ∈ R} consists of halfspace indicator functions ga : uρ (cid:55)→
I{a[1]ρ[1]+a[2]ρ[2]+a[3]ρ[3]<a[4]}.
Proof. Fix a sequence pair S1 and S2. Let L be the set of alignments the algorithm returns as we
range over all parameter vectors ρ ∈ R3. In other words, L = {Aρ(S1, S2) | ρ ∈ R3}. From prior
research [36, 50, 81], we know that |L| = O (cid:0)n3/2(cid:1). For any alignment L ∈ L, the algorithm Aρ
will return L if and only if

mt(S1, S2, L) − ρ[1] · ms(S1, S2, L) − ρ[2] · id(S1, S2, L) − ρ[3] · gp(S1, S2, L)
> mt(S1, S2, L(cid:48)) − ρ[1] · ms(S1, S2, L(cid:48)) − ρ[2] · id(S1, S2, L(cid:48)) − ρ[3] · gp(S1, S2, L(cid:48))
for all L(cid:48) ∈ L \ {L}. Therefore, there is a set H of at most O (cid:0)n3(cid:1) hyperplanes such that across
all parameter vectors ρ in a single connected component of R3 \ H, the output of the algorithm
parameterized by ρ, Aρ(S1, S2), is ﬁxed. The proof now follows by the exact same logic as that of
Lemma 4.1.

Lemmas 3.10 and 4.2 imply that Pdim(U ) = O(ln n). We also prove that this pseudo-dimension
bound is tight up to constant factors. In this lower bound proof, our utility function u is the Q
score between a given alignment L of two sequences (S1, S2) and the ground-truth alignment L∗
(the Q score is also known as the SPS score in the case of multiple sequence alignment [28]). The
Q score between L and the ground-truth alignment L∗ is the fraction of aligned letter pairs in L∗
that are correctly reproduced in L. For example, the following alignment L has a Q score of 2
3
because it correctly aligns the two pairs of Cs, but not the pair of Gs:
(cid:21)
(cid:20)G A T C C
A G - C C

(cid:20)- G A T C C
A G - - C C

L∗ =

L =

(cid:21)

.

We use the notation u (S1, S2, L) ∈ [0, 1] to denote the Q score between L and the ground-truth
alignment of S1 and S2. The full proof of the following theorem is in Appendix C.

14

Theorem 4.3. There exists a set (cid:8)Aρ | ρ ∈ R3
phabet Σ such that the set of functions U = (cid:8)uρ : (S1, S2) (cid:55)→ u (S1, S2, Aρ (S1, S2)) | ρ ∈ R3
which map sequence pairs S1, S2 ∈ ∪n
Ω(log n).

(cid:9) of co-optimal-constant algorithms and an al-
(cid:9) ,
i=1Σi of length at most n to [0, 1], has a pseudo-dimension of

≥0

≥0

Proof sketch. In this proof sketch, we illustrate the way in which two sequences pairs can be shat-
tered, and then describe how the proof can be generalized to Θ(log n) sequence pairs.

Setup. Our setup consists of the following three elements: the alphabet, the two sequence pairs
(cid:16)
, and ground-truth alignments of these pairs. We detail these elements

and

S

(cid:17)

(cid:16)

(cid:17)

(2)
(2)
1 , S
2

(1)
(1)
S
1 , S
2
below:

1. Our alphabet consists of twelve characters: {ai, bi, ci, di}3

i=1.

2. The two sequence pairs are comprised of three subsequence pairs:

(cid:16)

(3)
(3)
1 , t
t
2

(cid:17)

, where

(cid:16)

(1)
(1)
1 , t
t
2

(cid:16)

(cid:17)

,

(2)
(2)
1 , t
t
2

(cid:17)

, and

(1)
1 = a1b1d1
t
(1)
2 = b1c1d1
t

,

(2)
1 = a2a2b2d2
t
(2)
2 = b2c2c2d2
t

,

and

(3)
1 = a3a3a3b3d3
(3)
2 = b3c3c3c3d3

t
t

.

(10)

We deﬁne the two sequence pairs as
(2)
1 t
(2)
2 t

(3)
1 = a1b1d1a2a2b2d2a3a3a3b3d3
(3)
2 = b1c1d1b2c2c2d2b3c3c3c3d3

(1)
1 = t
(1)
2 = t

(1)
1 t
(1)
2 t

S
S

and

S
S

(2)
1 = t
(2)
2 = t

(2)
1 = a2a2b2d2
(2)
2 = b2c2c2d2

.

3. Finally, we deﬁne ground-truth alignments of the sequence pairs

We deﬁne the ground-truth alignment of

(cid:16)

S

(1)
(1)
1 , S
2

(cid:17)

to be

(cid:16)

S

(1)
(1)
1 , S
2

(cid:17)

and

(cid:16)

S

(2)
(2)
1 , S
2

(cid:17)

.

a1 b1
-
b1

-
c1 d1

d1 a2 a2 b2

-

-

-

-
b2 c2 c2 d2 b3

d2 a3 a3 a3 b3
-
-

-

-

-
d3
c3 c3 c3 d3

-

.

(11)

The most important properties of this alignment are that the dj characters are always match-
ing and the bj characters alternate between matching and not matching. Similarly, we deﬁne
the ground-truth alignment of the pair

to be

S

(cid:17)

(cid:16)

(2)
(2)
1 , S
2

a2 a2 b2
-
-

d2
b2 c2 c2 d2

-

-

.

Shattering. We now show that these two sequence pairs can be shattered. A key step is proving
that the functions u(0,ρ[2],0)


have the following form:

and u(0,ρ[2],0)

(1)
(1)
1 , S
2

(2)
(2)
1 , S
2

S

S

(cid:17)

(cid:16)

(cid:16)

(cid:17)

u(0,ρ[2],0)

(cid:16)

S

(1)
(1)
1 , S
2

(cid:17)

=




4
6
5
6
4
6
5
6

if ρ[2] ≤ 1
6
if 1
if 1
if ρ[2] > 1
2

6 < ρ[2] ≤ 1
4 < ρ[2] ≤ 1

4

2

and u(0,ρ[2],0)

(cid:16)

S

(2)
(2)
1 , S
2

(cid:17)

=

(cid:40)
1
1
2

if ρ[2] ≤ 1
4
if ρ[2] > 1
4

.

(12)

(cid:16)

(cid:17)

(1)
(1)
1 , S
2

S

The form of u(0,ρ[2],0)
that the two sequence pairs are shattered by the parameter settings (0, 0, 0), (cid:0)0, 1
(0, 1, 0) with the witnesses z1 = z2 = 3
to 0 and the indel parameter ρ[2] takes the values (cid:8)0, 1

is illustrated by Figure 4. It is then straightforward to verify
3 , 0(cid:1), and
4 . In other words, the mismatch and gap parameters are set

5 , 0(cid:1), (cid:0)0, 1

5 , 1

3 , 1(cid:9).

15

(cid:16)

(cid:17)

(1)
(1)
1 , S
2

S

as a function of the indel parameter ρ[2]. When ρ[2] ≤ 1
6 ,
Figure 4: The form of u(0,ρ[2],0)
the algorithm returns the bottom alignment. When 1
4 , the algorithm returns the
alignment that is second to the bottom. When 1
2 , the algorithm returns the alignment
that is second to the top. Finally, when ρ[2] > 1
2 , the algorithm returns the top alignment.
The purple characters denote which characters are correctly aligned according to the ground-truth
alignment (Equation (11)).

6 < ρ[2] ≤ 1

4 < ρ[2] ≤ 1

16

Proof sketch of Equation (12). The full proof that Equation (12) holds follows the following
high-level reasoning:

1. First, we prove that under the algorithm’s output alignment, the dj characters will always
Intuitively, this is because the algorithm’s objective function will always be

be matching.
(j)
maximized when each subsequence t
1
2. Second, we prove that the characters bj will be matched if and only if ρ[2] ≤ 1

2j . Intuitively,
this is because in order to match these characters, we must pay with 2j indels. Since the
(1)
objective function is mt
, the 1 match will be worth
2 , L
the 2j indels if and only if 1 ≥ 2jρ[2].

is aligned with t

− ρ[2] · id

(1)
2 , L

(1)
1 , S

(1)
1 , S

(j)
2 .

S

S

(cid:16)

(cid:17)

(cid:16)

(cid:17)

These two properties in conjunction mean that when ρ[2] > 1
2 , none of the bj characters are
matched, so the characters that are correctly aligned (as per the ground-truth alignment (Equa-
tion (11))) in the algorithm’s output are (a1, b1), (d1, d1), (d2, d2), (a3, b3), and (d3, d3), as illus-
trated by purple in the top alignment of Figure 4. Since there are a total of 6 aligned letters in the
= 5
ground-truth alignment, we have that the Q score is 5
6 , or in other words, u(0,ρ[2],0)
6 .
(cid:3), the indel penalty ρ[2] is suﬃciently small
When ρ[2] shifts to the next-smallest interval (cid:0) 1
4 , 1
that the b1 characters will align. Thus we lose the correct alignment (a1, b1), and the Q score
(cid:3), the b2 characters
drops to 4
will align, which is correct under the ground-truth alignment (Equation (11)). Thus the Q score
increases back to 5
6 , we lose the correct alignment
(a3, b3) in favor of the alignment of the b3 characters, so the Q score falls to 4
6 . In this way, we
from Equation (12). A parallel argument proves the form
prove the form of u(0,ρ[2],0)

6 . Similarly, if we decrease ρ[2] to the next-smallest interval (cid:0) 1

6 . Finally, by the same logic, when ρ[2] ≤ 1

(1)
(1)
1 , S
2

6 , 1

(1)
(1)
1 , S
2

S

S

(cid:17)

(cid:16)

(cid:17)

(cid:16)

2

4

of u(0,ρ[2],0)

(cid:16)

S

(2)
(2)
1 , S
2

(cid:17)

.

chosen k = Θ (

Generalization to shattering Θ(log n) sequence pairs. This proof intuition naturally gener-
(j)
a la
alizes to Θ(log n) sequence pairs of length O(n) by expanding the number of subsequences t
i
for a carefully-
Equation (10). In essence, if we deﬁne S

√

(1)
1 = t
n), then we can force u(0,ρ[2],0)
(4)
(2)
2 t
2

(k−1)
· · · t
1

(k)
(1)
(2)
· · · t
1 and S
1 t
1
(cid:17)
(cid:16)
(1)
(1)
1 , S
S
2
(k−1)
2

(2)
2 = t

· · · t

(4)
1

(1)
2 = t

(2)
(1)
2 t
2

· · · t
to oscillate O(n) times. Similarly, if we
(cid:17)

(cid:16)

(k)
2

deﬁne S
to oscillate half as many times, and so on. This construction allows us to shatter Θ(log n) se-
quences.

, then we can force u(0,ρ[2],0)

and S

S

(2)
1 = t

(2)
1 t

(1)
(1)
1 , S
2

4.1.3 Progressive multiple sequence alignment

The multiple sequence alignment problem is a generalization of the pairwise alignment problem
introduced in Section 4.1. Let Σ be an abstract alphabet and let S1, . . . , Sκ ∈ Σn be a collection of
sequences in Σ of length n. A multiple sequence alignment is a collection of sequences τ1, . . . , τκ ∈
(Σ ∪ {−})∗ such that the following hold:

1. The aligned sequences are the same length: |τ1| = |τ2| = · · · = |τκ|.

2. Removing the gap characters from τi gives Si: for all i ∈ [κ], del(τi) = Si.

3. For every position in the alignment, at least one of the aligned sequences has a non-gap
character. In other words, for every position i ∈ [|τ1|], there exists a sequence τj such that
τj[i] (cid:54)= −.

17

The extension from pairwise to multiple sequence alignment is computationally challenging:
all common formulations of the problem are NP-complete [59, 99]. As a result, heuristics have
been developed to ﬁnd good but possibly sub-optimal alignments. The most common heuristic
approach is called progressive multiple sequence alignment. It leverages eﬃcient pairwise alignment
algorithms to heuristically align multiple sequences [34].

The input to a progressive multiple sequence alignment algorithm is a collection of sequences
S1, . . . , Sκ together with a binary guide tree G with κ leaves1. The tree indicates how the original
alignment should be decomposed into a hierarchy of subproblems, each of which can be heuristically
solved using pairwise alignment. The leaves of the guide tree correspond to the input sequences
S1, . . . , Sκ.

While there are many formalizations of the progressive alignment method, for the sake of anal-
ysis we will focus on “partial consensus” described by Higgins and Sharp [51]. Here, we provide a
high-level description of the algorithm and in Appendix C.2, we include more detailed pseudo-code.
At a high level, the algorithm recursively constructs an alignment in two stages: ﬁrst, it creates a
consensus sequence for each node in the guide tree using a pairwise alignment algorithm, and then
it propagates the node-level alignments to the leaves by inserting additional gap characters.

v1 and σ(cid:48)

In a bit more detail, for each node v in the tree, we construct an alignment L(cid:48)

v of the consensus
sequences of its children as well as a consensus sequence σ(cid:48)
v ∈ Σ∗. Since each leaf corresponds to
a single input sequence, it has a trivial alignment and the consensus sequence is just the input
sequence itself. For an internal node v with children c1 and c2, we use a pairwise alignment
algorithm to construct an alignment of the consensus strings σ(cid:48)
v2. Finally, we deﬁne the
consensus sequence of the node v to be the string σv ∈ Σ∗ such that σv[i] is the most-frequent
non-gap character in the ith position in the alignment L(cid:48)
v. By deﬁning the consensus sequence in
this way, we can represent all of the sub-alignments of the leaves of the subtree rooted at v as a
single sequence which can be aligned using existing methods. We obtain a full multiple sequence
alignment by iteratively replacing each consensus sequence by the pairwise alignment it represents,
adding gap columns to the sub-alignments when necessary. Once we add a gap to a sequence, we
never remove it: “once a gap, always a gap.”

The family (cid:8)Aρ | ρ ∈ Rd(cid:9) of parameterized pairwise alignment algorithms introduced in Sec-
tion 4.1 induces a parameterized family of progressive multiple sequence alignment algorithms
(cid:8)Mρ | ρ ∈ Rd(cid:9).
In particular, the algorithm Mρ takes as input a collection of input sequences
S1, . . . , Sκ ∈ Σn and a guide tree G, and it outputs a multiple-sequence alignment L by applying
the pairwise alignment algorithm Aρ at each node of the guide tree. We assume that there is a
utility function that characterizes an alignment’s quality, denoted u (S1, . . . , Sκ, L) ∈ [0, 1]. We
then deﬁne uρ (S1, . . . , Sκ, G) = u (S1, . . . , Sκ, Mρ (S1, . . . , Sκ, G)) to be the utility of the alignment
returned by the algorithm Mρ. The proof of the following lemma is in Appendix C.2. It follows by
the same logic as Lemma 4.1 for pairwise sequence alignment, inductively over the guide tree.

Lemma 4.4. Let G be a guide tree of depth η and let U be the set of functions

U =

(cid:110)
uρ : (S1, . . . , Sκ, G) (cid:55)→ u(cid:0)S1, . . . , Sκ, Mρ(S1, . . . , Sκ, G)(cid:1) | ρ ∈ Rd(cid:111)

.

The dual class U ∗ is
(cid:18)

F , G,

(cid:16)

4nκ (nκ)4nκ+2(cid:17)2dη

4dη+1(cid:19)

-piecewise decomposable,

1The problem of constructing the guide tree is also an algorithmic task, often tackled via hierarchical clustering,

but we are agnostic to that pre-processing step.

18

where G = (cid:8)ga,θ : U → {0, 1} | a ∈ Rd, θ ∈ R(cid:9) consists of halfspace indicator functions ga,θ : uρ (cid:55)→
I{a·ρ≤θ} and F = {fc : U → R | c ∈ R} consists of constant functions fc : uρ (cid:55)→ c.

This lemma together with Lemma 3.10 implies that Pdim(U ) = O (cid:0)dη+1nκ ln(nκ) + dη+2(cid:1).
Therefore, the pseudo-dimension grows only linearly in n and quadratically in κ in the aﬃne-gap
model (d = 3) when the guide tree is balanced (η ≤ log κ).

4.2 RNA folding

RNA molecules have many essential roles including protein coding and enzymatic functions [52].
RNA is assembled as a chain of bases denoted A, U, C, and G. It is often found as a single strand
folded onto itself with non-adjacent bases physically bound together. RNA folding algorithms
infer the way strands would naturally fold, shedding light on their functions. Given a sequence
S ∈ {A, U, C, G}n, we represent a folding by a set of pairs φ ⊂ [n] × [n]. If (i, j) ∈ φ, then the ith
and jth bases of S bind together. Typically, the bases A and U bind together, as do C and G. Other
matchings are likely less stable. We assume that the foldings do not contain any pseudoknots, which
are pairs (i, j), (i(cid:48), j(cid:48)) that cross with i < i(cid:48) < j < j(cid:48).

A well-studied algorithm returns a folding that maximizes a parameterized objective func-
tion [80]. At a high level, this objective function trades oﬀ between global properties of the folding
(the number of binding pairs |φ|) and local properties (the likelihood that bases would appear close
together in the folding). Speciﬁcally, the algorithm Aρ uses dynamic programming to return the
folding Aρ(S) that maximizes

ρ |φ| + (1 − ρ)

(cid:88)

(i,j)∈φ

MS[i],S[j],S[i−1],S[j+1]I{(i−1,j+1)∈φ},

(13)

where ρ ∈ [0, 1] is a parameter and MS[i],S[j],S[i−1],S[j+1] ∈ R is a score for having neighboring pairs
of the letters (S[i], S[j]) and (S[i − 1], S[j + 1]). These scores help identify stable sub-structures.

We assume there is a utility function that characterizes a folding’s quality, denoted u(S, φ).
For example, u(S, φ) might measure the fraction of pairs shared between φ and a “ground-truth”
folding, obtained via expensive computation or laboratory experiments.
Lemma 4.5. Let U be the set of functions U = {uρ : S (cid:55)→ u (S, Aρ (S)) | ρ ∈ R}. The dual class
U ∗ is (cid:0)F , G, n2(cid:1)-piecewise decomposable, where G = {ga : U → {0, 1} | a ∈ R} consists of threshold
functions ga : uρ (cid:55)→ I{ρ<a} and F = {fc : U → R | c ∈ R} consists of constant functions fc : uρ (cid:55)→ c.
Proof. Fix a sequence S. Let Φ be the set of alignments that the algorithm returns as we range
over all parameters ρ ∈ R. In other words, Φ = {Aρ(S) | ρ ∈ [0, 1]}. We know that every folding
has length at most n/2. For any k ∈ {0, . . . , n/2}, let φk be the folding of length k that maximizes
the right-hand-side of Equation (13):

φk = argmaxφ:|φ|=k

(cid:88)

(i,j)∈φ

MS[i],S[j],S[i−1],S[j+1]I{(i−1,j+1)∈φ}.

The folding the algorithm returns will always be one of (cid:8)φ0, . . . , φn/2

(cid:9), so |Φ| ≤ n

2 + 1.

Fix an arbitrary folding φ ∈ Φ. We know that φ will be the folding returned by the algorithm

Aρ(S) if and only if

ρ |φ| + (1 − ρ)

(cid:88)

MS[i],S[j],S[i−1],S[j+1]I{(i−1,j+1)∈φ}

≥ ρ

(cid:12)φ(cid:48)(cid:12)
(cid:12)

(cid:12) + (1 − ρ)

(i,j)∈φ
(cid:88)

MS[i],S[j],S[i−1],S[j+1]I{(i−1,j+1)∈φ(cid:48)}

(14)

(i,j)∈φ(cid:48)

19

(cid:1) ≤ n2
for all φ(cid:48) ∈ Φ \ {φ}. Since these functions are linear in ρ, this means there is a set of T ≤ (cid:0)|Φ|
2
intervals [ρ1, ρ2), [ρ2, ρ3), . . . , [ρT , ρT +1] with ρ1 := 0 < ρ2 < · · · < ρT < 1 := ρT +1 such that for
any one interval I, across all ρ ∈ I, Aρ(S) is ﬁxed. This means that for any one interval [ρi, ρi+1),
there exists a real value ci such that uρ(S) = ci for all ρ ∈ [ρi, ρi+1). By deﬁnition of the dual, this
means that u∗

We now use this structure to show that the dual class U ∗ is (cid:0)F , G, n2(cid:1)-piecewise decomposable,
as per Deﬁnition 3.2. Recall that G = {ga : U → {0, 1} | a ∈ R} consists of threshold functions
ga : uρ (cid:55)→ I{ρ<a} and F = {fc : U → R | c ∈ R} consists of constant functions fc : uρ (cid:55)→ c. We claim
that there exists a function f (b) ∈ F for every vector b ∈ {0, 1}T such that for every ρ ∈ [0, 1],

S(uρ) = uρ(S) = ci as well.

u∗
S(uρ) =

(cid:88)

b∈{0,1}T

{gρi (uρ)=b[i],∀i∈[T ]}f (b)(uρ).
I

(15)

To see why, suppose ρ ∈ [ρi, ρi+1) for some i ∈ [T ]. Then gρj (uρ) = I{ρ≤ρj } = 1 for all j ≥ i + 1
and gρj (uρ) = I{ρ≤ρj } = 0 for all j ≤ i. Let bi ∈ {0, 1}T be the vector that has only 0’s in its ﬁrst i
coordinates and all 1’s in its remaining T − i coordinates. For all i ∈ [T ], we deﬁne f (bi) = fci, so
f (bi) (uρ) = ci for all ρ ∈ [0, 1]. For any other b, we set f (b) = f0, so f (b) (uρ) = 0 for all ρ ∈ [0, 1].
Therefore, Equation (15) holds.

Since constant functions have zero oscillations, Lemmas 3.9 and 4.5 imply that Pdim(U ) =

O (ln n) .

4.3 Prediction of topologically associating domains

Inside a cell, the linear DNA of the genome wraps into three-dimensional structures that in-
ﬂuence genome function. Some regions of the genome are closer than others and thereby in-
teract more. Topologically associating domains (TADs) are contiguous segments of the genome
that fold into compact regions. More formally, given the genome length n, a TAD set is a set
T = {(i1, j1), . . . , (it, jt)} ⊂ [n] × [n] such that i1 < j1 < i2 < j2 < · · · < it < jt. If (i, j) ∈ T , the
bases within the corresponding substring physically interact more frequently with each other than
with other bases. Disrupting TAD boundaries can aﬀect the expression of nearby genes, which can
trigger diseases such as congenital malformations and cancer [71].

The contact frequency of any two genome locations, denoted by a matrix M ∈ Rn×n, can be
measured via experiments [67]. A dynamic programming algorithm Aρ introduced by Filippova
et al. [37] returns the TAD set Aρ(M ) that maximizes

where ρ ≥ 0 is a parameter,

(cid:88)

(i,j)∈T

sρ(i, j) − µρ(j − i),

sρ(i, j) =

1
(j − i)ρ

(cid:88)

Mpq

i≤p<q≤j

(16)

is the scaled density of the subgraph induced by the interactions between genomic loci i and j, and

µρ(d) =

1
n − d

n−d−1
(cid:88)

t=0

sρ(t, t + d)

is the mean value of sρ over all sub-matrices of length d along the diagonal of M . We note that unlike
the sequence alignment and RNA folding algorithms, the parameter ρ appears in the exponent of
the objective function.

20

We assume there is a utility function that characterizes the quality of a TAD set T , denoted
u(M, T ) ∈ R. For example, u(M, T ) might measure the fraction of TADs in T that are in the
correct location with respect to a ground-truth TAD set.

(cid:16)

F , G, 2n24n2(cid:17)

Lemma 4.6. Let U be the set of functions U = {uρ : M (cid:55)→ u (M, Aρ (M )) | ρ ∈ R}. The dual class
-piecewise decomposable, where G = {ga : U → {0, 1} | a ∈ R} consists of
U ∗ is
threshold functions ga : uρ (cid:55)→ I{ρ<a} and F = {fc : U → R | c ∈ R} consists of constant functions
fc : uρ (cid:55)→ c.

Proof. Fix a matrix M . We begin by rewriting Equation (16) as follows:

Aρ(M ) = argmax
T ⊂[n]×[n]





1
(j − i)ρ

(cid:88)

(i,j)∈T







(cid:88)

Muv

 −

i≤u<v≤j

1
n − j + i

n−j+i
(cid:88)

t=0

1
(j − i)ρ

= argmax

= argmax

(cid:88)

(i,j)∈T
(cid:88)

(i,j)∈T

1
(j − i)ρ











(cid:88)

Muv

 −

i≤u<v≤j

1
n − j + i

cij
(j − i)ρ ,

n−j+i
(cid:88)

(cid:88)

t=0

t≤p<q≤t+j−i

(cid:88)



Mpq



t≤p<q≤t+j−i



Mpq



where



cij =



(cid:88)

i≤u<v≤j



Muv

 −

1
n − j + i

n−j+i
(cid:88)

(cid:88)

Mpq

t=0

t≤p<q≤t+j−i

is a constant that does not depend on ρ.

Let T be the set of TAD sets that the algorithm returns as we range over all parameters ρ ≥ 0.
In other words, T = {Aρ(M ) | ρ ∈ R≥0}. Since each TAD set is a subset of [n] × [n], |T | ≤ 2n2
.
For any TAD set T ∈ T , the algorithm Aρ will return T if and only if

(cid:88)

(i,j)∈T

cij
(j − i)ρ >

(cid:88)

(i(cid:48),j(cid:48))∈T (cid:48)

ci(cid:48)j(cid:48)
(j(cid:48) − i(cid:48))ρ

for all T (cid:48) ∈ T \ {T }. This means that as we range ρ over the positive reals, the TAD set returned
by algorithm Aρ(M ) will only change when

(cid:88)

(i,j)∈T

cij
(j − i)ρ −

(cid:88)

(i(cid:48),j(cid:48))∈T (cid:48)

ci(cid:48)j(cid:48)

(j(cid:48) − i(cid:48))ρ = 0

(17)

for some T, T (cid:48) ∈ T . As a result of Rolle’s Theorem (Corollary A.3), we know that Equation (17)
has at most |T | + |T (cid:48)| ≤ 2n2 solutions. This means there are t ≤ 2n2(cid:0)|T |
intervals
2
[ρ1, ρ2) , [ρ2, ρ3) , . . . , [ρt, ρt+1) with ρ1 := 0 < ρ2 < · · · < ρt < ∞ := ρt+1 that partition R≥0 such
that across all ρ within any one interval [ρi, ρi+1), the TAD set returned by algorithm Aρ(M ) is
ﬁxed. Therefore, there exists a real value ci such that uρ(M ) = ci for all ρ ∈ [ρi, ρi+1). By deﬁnition
of the dual, this means that u∗

(cid:1) ≤ 2n24n2

M (uρ) = uρ(M ) = ci as well.

We now use this structure to show that the dual class U ∗ is

-piecewise decom-
posable, as per Deﬁnition 3.2. Recall that G = {ga : U → {0, 1} | a ∈ R} consists of threshold
functions ga : uρ (cid:55)→ I{ρ<a} and F = {fc : U → R | c ∈ R} consists of constant functions fc : uρ (cid:55)→ c.

(cid:16)

F , G, 2n24n2(cid:17)

21

We claim that there exists a function f (b) ∈ F for every vector b ∈ {0, 1}t such that for every
ρ ≥ 0,

u∗
M (uρ) =

(cid:88)

b∈{0,1}t

{gρi (uρ)=b[i],∀i∈[t]}f (b)(uρ).
I

(18)

To see why, suppose ρ ∈ [ρi, ρi+1) for some i ∈ [t]. Then gρj (uρ) = I{ρ≤ρj } = 1 for all j ≥ i + 1
and gρj (uρ) = I{ρ≤ρj } = 0 for all j ≤ i. Let bi ∈ {0, 1}t be the vector that has only 0’s in its ﬁrst
i coordinates and all 1’s in its remaining t − i coordinates. For all i ∈ [t], we deﬁne f (bi) = fci, so
f (bi) (uρ) = ci for all ρ ∈ [0, 1]. For any other b, we set f (b) = f0, so f (b) (uρ) = 0 for all ρ ∈ [0, 1].
Therefore, Equation (18) holds.

Since constant functions have zero oscillations, Lemmas 3.9 and 4.6 imply that Pdim(U ) =

O (cid:0)n2(cid:1) .

5 Parameterized voting mechanisms

A large body of research in economics studies how to design protocols—or mechanisms—that
help groups of agents come to collective decisions. For example, when children inherit an estate,
how should they divide the property? When a jointly-owned company is dissolved, which partner
should buy the others out? There is no one protocol that best answers these questions; the optimal
mechanism depends on the setting at hand.

We study a family of mechanisms called neutral aﬃne maximizers (NAMs) [74, 78, 86]. A
NAM takes as input a set of agents’ reported values for each possible outcome and returns one
of those outcomes. A NAM can thus be thought of as an algorithm that the agents use to arrive
at a single outcome. NAMs are incentive compatible, which means that each agent is incentivized
to report his values truthfully. In order to satisfy incentive compatibility, each agent may have to
make a payment. NAMs are also budget-balanced which means that the aggregated payments are
redistributed among the agents.

Formally, we study a setting where there is a set of m alternatives and a set of n agents. Each
agent i has a value vi(j) ∈ R for each alternative j ∈ [m]. We denote all of his values as vi ∈ Rm
and all n agents’ values as v = (v1, . . . , vn) ∈ Rnm. In this case, the unknown distribution D is
over vectors v ∈ Rnm.

(cid:80)n

A NAM is deﬁned by n parameters (one per agent) ρ = (ρ[1], . . . , ρ[n]) ∈ Rn

≥0 such that at
least one agent is assigned a weight of zero. There is a social choice function ψρ : Rnm → [m]
which uses the values v ∈ Rnm to choose an alternative ψρ(v) ∈ [m].
In particular, ψρ(v) =
argmaxj∈[m]
i=1 ρ[i]vi(j) maximizes the agents’ weighted values. Each agent i with zero weight
ρ[i] = 0 is called a sink agent because his values do not inﬂuence the outcome. For every agent
who is not a sink agent (ρ[i] (cid:54)= 0), their payment is deﬁned as in the weighted version of the
classic Vickrey-Clarke-Groves mechanism [22, 46, 98]. To achieve budget balance, these payments
are given to the sink agent(s). More formally, let j∗ = ψρ(v) and for each agent i, let j−i =
argmaxj∈[m]

i(cid:48)(cid:54)=i ρ[i(cid:48)]vi(cid:48)(j). The payment function is deﬁned as

(cid:80)

i(cid:48)(cid:54)=i ρ[i(cid:48)]vi(cid:48) (j∗) − (cid:80)

pi(v) =

i(cid:48)(cid:54)=i pi(cid:48)(v)

(cid:16)(cid:80)


1

ρ[i]
− (cid:80)

0

(cid:17)
i(cid:48)(cid:54)=i ρ[i(cid:48)]vi(cid:48) (j−i)

if ρ[i] (cid:54)= 0
if i = min {i(cid:48) : ρ[i(cid:48)] = 0}
otherwise.

We aim to optimize the expected social welfare Ev∼D [(cid:80)n

ψρ(v), so we deﬁne the utility function uρ(v) = (cid:80)n

i=1 vi (ψρ(v)).

i=1 vi (ψρ(v))] of the NAM’s outcome

22

≥0, {i | ρ[i] = 0} (cid:54)= ∅(cid:9). The dual class
Lemma 5.1. Let U be the set of functions U = (cid:8)uρ | ρ ∈ Rn
U ∗ is (cid:0)F , G, m2(cid:1)-piecewise decomposable, where G = {ga : U → {0, 1} | a ∈ Rn} consists of
halfspace indicators ga : uρ (cid:55)→ I{ρ·a≤0} and F = {fc : U → R | c ∈ R} consists of constant
functions fc : uρ (cid:55)→ c.

Proof. Fix a valuation vector v ∈ Rnm. We know that for any two alternatives j, j(cid:48) ∈ [m], the
alternative j would be selected over j(cid:48) so long as

n
(cid:88)

i=1

ρ[i]vi(j) >

n
(cid:88)

i=1

ρ[i]vi

(cid:0)j(cid:48)(cid:1) .

(19)

v (uρ) = uρ(v) = cR as well.

(cid:1) hyperplanes such that across all parameter vectors ρ in a single
Therefore, there is a set H of (cid:0)m
2
connected component of Rn \ H, the outcome of the NAM deﬁned by ρ is ﬁxed. When the outcome
of the NAM is ﬁxed, the social welfare is ﬁxed as well. This means that for a single connected
component R of Rn \ H, there exists a real value cR such that uρ(v) = cR for all ρ ∈ R. By
deﬁnition of the dual, this means that u∗

We now use this structure to show that the dual class U ∗ is (cid:0)F , G, m2(cid:1)-piecewise decomposable,
as per Deﬁnition 3.2. Recall that G = {ga : U → {0, 1} | a ∈ Rn} consists of halfspace indicator
functions ga : uρ (cid:55)→ I{a·ρ<0} and F = {fc : U → R | c ∈ R} consists of constant functions
fc : uρ (cid:55)→ c. For each pair of alternatives j, j(cid:48) ∈ L, let g(j,j(cid:48)) ∈ G correspond to the halfspace
(cid:1) functions arbitrarily as g(1), . . . , g(k). Every
represented in Equation (19). Order these k := (cid:0)m
2
connected component R of Rn \ H corresponds to a sign pattern of the k hyperplanes. For a given
region R, let bR ∈ {0, 1}k be the corresponding sign pattern. Deﬁne the function f (bR) ∈ F as
f (bR) = fcR, so f (bR) (uρ) = cR for all ρ ∈ Rn. Meanwhile, for every vector b not corresponding to
a sign pattern of the k hyperplanes, let f (b) = f0, so f (b) (uρ) = 0 for all ρ ∈ Rn. In this way, for
every ρ ∈ Rn,

u∗
v (uρ) =

(cid:88)

I

{g(i)(uρ)=b[i],∀i∈[k]}f (b)(uρ),

as desired.

b∈{0,1}k

Theorem 3.3 and Lemma 5.1 imply that the pseudo-dimension of U is O(n ln m). Next, we prove
2 , which means that our pseudo-dimension upper bound

that the pseudo-dimension of U is at least n
is tight up to log factors.
Theorem 5.2. Let U be the set of functions U = (cid:8)uρ | ρ ∈ Rn
n
2 .

≥0, {ρ[i] | i = 0} (cid:54)= ∅(cid:9). Then Pdim(U ) ≥

Proof. Let the number of alternatives m = 2 and without loss of generality, suppose that n is even.
To prove this theorem, we will identify a set of N = n
2 valuation vectors v(1), . . . , v(N ) that are
shattered by the set U of social welfare functions.

Let (cid:15) be an arbitrary number in (cid:0)0, 1
2

(cid:1). For each (cid:96) ∈ [N ], deﬁne agent i’s values for the ﬁrst and

second alternatives under the (cid:96)th valuation vector v((cid:96))—namely, v

((cid:96))
i (1) and v

((cid:96))
i (2)—as follows:

((cid:96))
i (1) =

v

(cid:40)

1 if (cid:96) = i
0 otherwise

and v

((cid:96))
i (2) =

(cid:40)
(cid:15)
0

if (cid:96) = n
2 + i
otherwise.

23

For example, if there are n = 6 agents, then across the N = n
the agents’ values for the ﬁrst alternative are deﬁned as

2 = 3 valuation vectors v(1), v(2), v(3),


(1)
1 (1)
v
(2)

1 (1)
v

(3)
1 (1)
v

· · ·
· · ·
· · ·

v
v
v

(1)
6 (1)
(2)
6 (1)
(3)
6 (1)




 =





1 0 0 0 0 0
0 1 0 0 0 0
0 0 1 0 0 0





and their values for the second alternative are deﬁned as


v

v

v

(1)
1 (2)
(2)
1 (2)
(3)
1 (2)

· · ·
· · ·
· · ·




 =

(1)
v
6 (2)
(2)
v
6 (2)
(3)
6 (2)
v





0 0 0 (cid:15) 0 0
0 0 0 0 (cid:15) 0
0 0 0 0 0 (cid:15)



 .

Let b ∈ {0, 1}N be an arbitrary bit vector. We will construct a NAM parameter vector ρ such
that for any (cid:96) ∈ [N ], if b(cid:96) = 0, then the outcome of the NAM given bids v((cid:96)) will be the second
(cid:0)v((cid:96))(cid:1) = (cid:15) because there is always exactly one agent who has a value of (cid:15) for the
alternative, so uρ
second alternative, and every other agent has a value of 0. Meanwhile, if b(cid:96) = 0, then the outcome
(cid:0)v((cid:96))(cid:1) = 1 because there is always
of the NAM given bids v((cid:96)) will be the ﬁrst alternative, so uρ
exactly one agent who has a value of 1 for the ﬁrst alternative, and every other agent has a value
of 0. To do so, when b(cid:96) = 0, ρ must ignore the values of agent (cid:96) in favor of the values of agent
2 + (cid:96). After all, under v((cid:96)), agent (cid:96) has a value of 1 for the ﬁrst alternative and agent n
n
2 + (cid:96) has
a value of (cid:15) for the second alternative, and all other values are 0. By a similar argument, when
b(cid:96) = 1, ρ must ignore the values of agent n
2 + (cid:96) in favor of the values of agent (cid:96). Speciﬁcally, we
2 + (cid:96)(cid:3) = 1 and if
deﬁne ρ ∈ {0, 1}n as follows: for all (cid:96) ∈ [N ] = (cid:2) n
2
b(cid:96) = 1, then ρ[(cid:96)] = 1 and ρ (cid:2) n

2 + (cid:96)(cid:3) = 0. All other entries of ρ are set to 0.

(cid:3), if b(cid:96) = 0, then ρ[(cid:96)] = 0 and ρ (cid:2) n

We claim that if b(cid:96) = 0, then uρ
(cid:96) (1) = ρ[(cid:96)] = 0. Meanwhile, (cid:80)n
((cid:96))

ρ[(cid:96)]v

i=1 ρ[i]v

((cid:96))

i (2) = ρ (cid:2) n

2 + (cid:96)(cid:3) v

(cid:0)v((cid:96))(cid:1) = (cid:15). To see why, we know that (cid:80)n

((cid:96))
n

((cid:96))
i (1) =
2 +(cid:96)(1) = (cid:15). Therefore, the outcome
(cid:0)v((cid:96))(cid:1) = (cid:15).

i=1 ρ[i]v

of the NAM is alternative 2. The social welfare of this alternative is (cid:15), so uρ

Next, we claim that if b(cid:96) = 1, then uρ
(cid:96) (1) = ρ[(cid:96)] = 1. Meanwhile, (cid:80)n
((cid:96))

i=1 ρ[i]v

ρ[(cid:96)]v

(cid:0)v((cid:96))(cid:1) = 1. To see why, we know that (cid:80)n
((cid:96))
n

((cid:96))
i (1) =
2 +(cid:96)(1) = 0. Therefore, the outcome

i (2) = ρ (cid:2) n

2 + (cid:96)(cid:3) v

i=1 ρ[i]v

((cid:96))

(cid:0)v((cid:96))(cid:1) = 1.

of the NAM is alternative 1. The social welfare of this alternative is 1, so uρ

We conclude that the valuation vectors v(1), . . . , v(N ) that are shattered by the set U of social

welfare functions with witnesses z(1) = · · · = z(N ) = 1
2 .

Theorem 5.2 implies that the pseudo-dimension upper bound from Lemma 5.1 is tight up to

logarithmic factors.

6 Subsumption of prior research on generalization guarantees

Theorem 3.3 also recovers existing guarantees for data-driven algorithm design.
In all of these
cases, Theorem 3.3 implies generalization guarantees that match the existing bounds, but in many
cases, our approach provides a more succinct proof.

1. In Section 6.1, we analyze several parameterized clustering algorithms [7], which have piecewise-
constant dual functions. These algorithms ﬁrst run a linkage routine which builds a hierar-
chical tree of clusters. The parameters interpolate between the popular single, average, and
complete linkage. The linkage routine is followed by a dynamic programming procedure that
returns a clustering corresponding to a pruning of the hierarchical tree.

24

2. Balcan et al. [11] study a family of linkage-based clustering algorithms where the parame-
ters control the distance metric used for clustering in addition to the linkage routine. The
algorithm family has two sets of parameters. The ﬁrst set of parameters interpolate be-
tween linkage algorithms, while the second set interpolate between distance metrics. The
dual functions are piecewise-constant with quadratic boundary functions. We recover their
generalization bounds in Section 6.1.2.

3. In Section 6.2, we analyze several integer programming algorithms, which have piecewise-
constant and piecewise-inverse-quadratic dual functions (as in Figure 3c). The ﬁrst is branch-
and-bound, which is used by commercial solvers such as CPLEX. Branch-and-bound always
ﬁnds an optimal solution and its parameters control runtime and memory usage. We also
study semideﬁnite programming approximation algorithms for integer quadratic program-
ming. We analyze a parameterized algorithm introduced by Feige and Langberg [33] which
includes the Goemans-Williamson algorithm [41] as a special case. We recover previous gen-
eralization bounds in both settings [7, 8].

4. Gupta and Roughgarden [48] introduced parameterized greedy algorithms for the knapsack
and maximum weight independent set problems, which we show have piecewise-constant dual
functions. We recover their generalization bounds in Section 6.3.

5. We provide generalization bounds for parameterized selling mechanisms when the goal is to
maximize revenue, which have piecewise-linear dual functions (as in Figure 3b). A long line of
research has studied revenue maximization via machine learning [5, 19, 23, 25, 29, 43, 44, 47,
68, 69, 76, 77, 88]. In Section 6.4, we recover Balcan, Sandholm, and Vitercik’s generalization
bounds [10] which apply to a variety of pricing, auction, and lottery mechanisms. They proved
new bounds for mechanism classes not previously studied in the sample-based mechanism
design literature and matched or improved over the best known guarantees for many classes.

6.1 Clustering algorithms

A clustering instance is made up of a set points V from a data domain X and a distance metric
d : X × X → R≥0. The goal is to split up the points into groups, or “clusters,” so that within
each group, distances are minimized and between each group, distances are maximized. Typically,
a clustering’s quality is quantiﬁed by some objective function. Classic choices include the k-means,
k-median, or k-center objective functions. Unfortunately, ﬁnding the clustering that minimizes any
one of these objectives is NP-hard. Clustering algorithms have uses in data science, computational
biology [79], and many other ﬁelds.

Balcan et al. [7, 11] analyze agglomerative clustering algorithms. This type of algorithm requires
a merge function c(A, B; d) → R≥0, deﬁning the distances between point sets A, B ⊆ V . The al-
gorithm constructs a cluster tree. This tree starts with n leaf nodes, each containing a point from
V . Over a series of rounds, the algorithm merges the sets with minimum distance according to c.
The tree is complete when there is one node remaining, which consists of the set V . The children
of each internal node consist of the two sets merged to create the node. There are several common
merge function c: mina∈A,b∈B d(a, b) (single-linkage),
a∈A,b∈B d(a, b) (average-linkage), and
maxa∈A,b∈B d(a, b) (complete-linkage). Following the linkage procedure, there is a dynamic pro-
gramming step. This steps ﬁnds the tree pruning that minimizes an objective function, such as the
k-means, -median, or -center objectives.

1
|A|·|B|

(cid:80)

To evaluate the quality of a clustering, we assume access to a utility function u : T → [−1, 1]
where T is the set of all cluster trees over the data domain X . For example, u (T ) might measure

25

the distance between the ground truth clustering and the optimal k-means pruning of the cluster
tree T ∈ T .

In Section 6.1.1, we present results for learning merge functions and in Section 6.1.2, we present
results for learning distance functions in addition to merge functions. The latter set of results apply
to a special subclass of merge functions called two-point-based (as we describe in Section 6.1.2),
and thus do not subsume the results in Section 6.1.1, but do apply to the more general problem of
learning a distance function in addition to a merge function.

6.1.1 Learning merge functions

Balcan, Nagarajan, Vitercik, and White [7] study several families of merge functions:

(cid:40)

C1 =

c1,ρ : (A, B; d) (cid:55)→

(cid:18)

min
u∈A,v∈B

(d(u, v))ρ + max

u∈A,v∈B

(d(u, v))ρ

ρ ∈ R ∪ {∞, −∞}

,

(cid:41)

(cid:19)1/ρ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:26)

C2 =

c2,ρ : (A, B; d) (cid:55)→ ρ min

u∈A,v∈B

C3 =






c3,ρ : (A, B; d) (cid:55)→






1
|A||B|

(cid:88)

u∈A,v∈B

(cid:27)

ρ ∈ [0, 1]

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)






u∈A,v∈B
1/ρ (cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)



(d(u, v))ρ

ρ ∈ R ∪ {∞, −∞}

.

d(u, v) + (1 − ρ) max

d(u, v)

The classes C1 and C2 interpolate between single- (c1,−∞ and c2,1) and complete-linkage (c1,∞

and c2,0). The class C3 includes as special cases average-, complete-, and single-linkage.

For each class i ∈ {1, 2, 3} and each parameter ρ, let Ai,ρ be the algorithm that takes as input

a clustering instance (V, d) and returns a cluster tree Ai,ρ(V, d) ∈ T .

Balcan et al. [7] prove the following useful structure about the classes C1 and C2:

Lemma 6.1 ([7]). Let (V, d) be an arbitrary clustering instance over n points. There is a partition
of R into k ≤ n8 intervals I1, . . . , Ik such that for any interval Ij and any two parameters ρ, ρ(cid:48) ∈ Ij,
the sequences of merges the agglomerative clustering algorithm makes using the merge functions
c1,ρ and c1,ρ(cid:48) are identical. The same holds for the set of merge functions C2.

This structure immediately implies that the corresponding class of utility functions has a

piecewise-structured dual class.

Corollary 6.2. Let U be the set of functions

U = {uρ : (V, d) (cid:55)→ u (A1,ρ(V, d)) | ρ ∈ R ∪ {−∞, ∞}}

mapping clustering instances (V, d) to [−1, 1]. The dual class U ∗ is (F , G, n8)-piecewise decompos-
able, where G = {ga : U → {0, 1} | a ∈ R} consists of threshold functions ga : uρ (cid:55)→ I{ρ<a} and
F = {fc : U → R | c ∈ R} consists of constant functions fc : uρ (cid:55)→ c. The same holds when U is
deﬁned according to merge functions in C2 as U = {uρ : (V, d) (cid:55)→ u (A2,ρ(V, d)) | ρ ∈ [0, 1]} .

Lemma 3.9 and Corollary 6.2 imply the following pseudo-dimension bound.

Corollary 6.3. Let U be the set of functions

U = {uρ : (V, d) (cid:55)→ u (A1,ρ(V, d)) | ρ ∈ R ∪ {−∞, ∞}}

mapping clustering instances (V, d) to [−1, 1]. Then Pdim(U ) = O(ln n). The same holds when U
is deﬁned according to merge functions in C2 as U = {uρ : (V, d) (cid:55)→ u (A2,ρ(V, d)) | ρ ∈ [0, 1]} .

26

Balcan et al. [7] prove a similar guarantee for the more complicated class C3.

Lemma 6.4 ([7]). Let (V, d) be an arbitrary clustering instance over n points. There is a partition
of R into k ≤ n232n intervals I1, . . . , Ik such that for any interval Ij and any two parameters
ρ, ρ(cid:48) ∈ Ij, the sequences of merges the agglomerative clustering algorithm makes using the merge
functions c3,ρ and c3,ρ(cid:48) are identical.

Again, this structure immediately implies that the corresponding class of utility functions has

a piecewise-structured dual class.

Corollary 6.5. Let U be the set of functions

U = {uρ : (V, d) (cid:55)→ u (A3,ρ(V, d)) | ρ ∈ R ∪ {−∞, ∞}}

mapping clustering instances (V, d) to [−1, 1]. The dual class U ∗ is (cid:0)F , G, n232n(cid:1)-piecewise decom-
posable, where G = {ga : U → {0, 1} | a ∈ R} consists of threshold functions ga : uρ (cid:55)→ I{ρ<a} and
F = {fc : U → R | c ∈ R} consists of constant functions fc : uρ (cid:55)→ c.

Lemma 3.9 and Corollary 6.5 imply the following pseudo-dimension bound.

Corollary 6.6. Let U be the set of functions

U = {uρ : (V, d) (cid:55)→ u (A3,ρ(V, d)) | ρ ∈ R ∪ {−∞, ∞}}

mapping clustering instances (V, d) to [−1, 1]. Then Pdim(U ) = O(n).

Corollaries 6.3 and 6.6 match the pseudo-dimension guarantees that Balcan et al. [7] prove.

6.1.2 Learning merge functions and distance functions

Balcan, Dick, and Lang [11] extend the clustering generalization bounds of Balcan, Nagarajan,
Vitercik, and White [7] to the case of learning both a distance metric and a merge function. They
introduce a family of linkage-based clustering algorithms that simultaneously interpolate between
a collection of base metrics d1, . . . , dL and base merge functions c1, . . . , cL. The algorithm family is
parameterized by ρ = (α, β) ∈ ∆L(cid:48) ×∆L, where α and β are mixing weights for the merge functions
and metrics, respectively. The algorithm with parameters ρ = (α, β) starts with each point in a
cluster of its own and repeatedly merges the pair of clusters A and B minimizing cα(A, B; dβ),
where

cα(A, B; d) =

αi · ci(A, B; d)

and

dβ(a, b) =

βi · di(a, b).

L(cid:48)
(cid:88)

L
(cid:88)

i=1

i=1

We use the notation Aρ to denote the algorithm that takes as input a clustering instance (V, d)
and returns a cluster tree Aρ(V, d) ∈ T using the merge function cα(A, B; dβ), where ρ = (α, β).
When analyzing this algorithm family, Balcan et al. [11] prove that the following piecewise-
structure holds when all of the merge functions are two-point-based, which roughly requires that
for any pair of clusters A and B, there exist points a ∈ A and b ∈ B such that c(A, B; d) = d(a, b).
Single- and complete-linkage are two-point-based, but average-linkage is not.

quadratic
Lemma 6.7 ([11]). For any clustering instance V , there exists a collection of O
boundary functions that partition the (L + L(cid:48))-dimensional parameter space into regions where the
algorithm’s output is constant on each region in the partition.

(cid:16)

|V |4L(cid:48)(cid:17)

27

This lemma immediately implies that the corresponding class of utility functions has a piecewise-

structured dual class.

Corollary 6.8. Let U be the set of functions U = {uρ : (V, d) (cid:55)→ u (Aρ(V, d)) | ρ ∈ ∆L(cid:48) × ∆L}
mapping clustering instances (V, d) to [−1, 1]. The dual class U ∗ is
-piecewise
decomposable, where F is the set of constant functions and G is the set of quadratic functions
deﬁned on ∆L(cid:48) × ∆L.

|V |4L(cid:48)(cid:17)(cid:17)

F , G, O

(cid:16)

(cid:16)

Using the fact that VCdim (G∗) = O((L + L(cid:48))2), we obtain the following pseudo-dimension

bound.

Corollary 6.9. Let U be the set of functions U = {uρ : (V, d) (cid:55)→ u (Aρ(V, d)) | ρ ∈ ∆L(cid:48) × ∆L}
mapping clustering instances (V, d) to [−1, 1]. Then

Pdim(U ) = O

(cid:16)(cid:0)L + L(cid:48)(cid:1)2

log (cid:0)L + L(cid:48)(cid:1) + (cid:0)L + L(cid:48)(cid:1)2

L(cid:48) log(n)

(cid:17)

.

This matches the generalization bound that Balcan et al. [11] prove.

6.2 Integer programming

Several papers [7, 8] study data-driven algorithm design for both integer linear and integer quadratic
programming, as we describe below.

Integer linear programming.
In the context of integer linear programming, Balcan et al. [8]
focus on branch-and-bound (B&B) [64], an algorithm for solving mixed integer linear programs
(MILPs). A MILP is deﬁned by a matrix A ∈ Rm×n, a vector b ∈ Rm, a vector c ∈ Rn, and a set
of indices I ⊆ [n]. The goal is to ﬁnd a vector x ∈ Rn such that c · x is maximized, Ax ≤ b, and
for every index i ∈ I, xi is constrained to be binary: xi ∈ {0, 1}.

Branch-and-bound builds a search tree to solve an input MILP Q. At the root of the search
tree is the original MILP Q. At each round, the algorithm chooses a leaf of the search tree, which
represents an MILP Q(cid:48). It does so using a node selection policy; common choices include depth-
and best-ﬁrst search. Then, it chooses an index i ∈ I using a variable selection policy. It next
branches on xi: it sets the left child of Q(cid:48) to be that same integer program, but with the additional
constraint that xi = 0, and it sets the right child of Q(cid:48) to be that same integer program, but with
the additional constraint that xi = 1. The algorithm fathoms a leaf, which means that it never
will branch on that leaf, if it can guarantee that the optimal solution does not lie along that path.
The algorithm terminates when it has fathomed every leaf. At that point, we can guarantee that
the best solution to Q found so far is optimal. See the paper by Balcan et al. [8] for more details.
Balcan et al. [8] study mixed integer linear programs (MILPs) where the goal is to maximize an
objective function c(cid:62)x subject to the constraints that Ax ≤ b and that some of the components of
x are contained in {0, 1}. Given a MILP Q, we use the notation ˘xQ = (˘xQ[1], . . . ˘xQ[n]) to denote
an optimal solution to the MILP’s LP relaxation. We denote the optimal objective value to the
MILP’s LP relaxation as ˘cQ, which means that ˘cQ = c(cid:62) ˘xQ.

Branch-and-bound systematically partitions the feasible set in order to ﬁnd an optimal solution,
organizing the partition as a tree. At the root of this tree is the original integer program. Each child
represents the simpliﬁed integer program obtained by partitioning the feasible set of the problem
contained in the parent node. The algorithm prunes a branch if the corresponding subproblem is
infeasible or its optimal solution cannot be better than the best one discovered so far. Oftentimes,
branch-and-bound partitions the feasible set by adding a constraint. For example, if the feasible

28

set is characterized by the constraints Ax ≤ b and x ∈ {0, 1}n, the algorithm partition the feasible
set into one subset where Ax ≤ b, x1 = 0, and x2, . . . , xn ∈ {0, 1}, and another where Ax ≤ b,
x1 = 1, and x2, . . . , xn ∈ {0, 1}. In this case, we say that the algorithm branches on x1.

Balcan et al. [8] show how to learn variable selection policies. Speciﬁcally, they study score-based

variable selection policies, deﬁned below.

Deﬁnition 6.10 (Score-based variable selection policy [8]). Let score be a deterministic function
that takes as input a partial search tree T , a leaf Q of that tree, and an index i, and returns a
real value score(T , Q, i) ∈ R. For a leaf Q of a tree T , let NT ,Q be the set of variables that have
not yet been branched on along the path from the root of T to Q. A score-based variable selection
policy selects the variable argmaxxi∈NT ,Q

{score(T , Q, i)} to branch on at the node Q.

This type of variable selection policy is widely used [1, 40, 70]. See the paper by Balcan et al.

[8] for examples.

Given d arbitrary scoring rules score1, . . . , scored, Balcan et al. [8] provide guidance for learn-
ing a linear combination ρ[1]score1 + · · · + ρ[d]scored that leads to small expected tree sizes. They
assume that all aspects of the tree search algorithm except the variable selection policy, such as
the node selection policy, are ﬁxed. In their analysis, they prove the following lemma.

Lemma 6.11 ([8]). Let score1, . . . , scored be d arbitrary scoring rules and let Q be an arbitrary
MILP over n binary variables. Suppose we limit B&B to producing search trees of size τ . There is
a set H of at most n2(τ +1) hyperplanes such that for any connected component R of [0, 1]d \ H ,
the search tree B&B builds using the scoring rule ρ[1]score1 + · · · + ρ[d]scored is invariant across
all (ρ[1], . . . , ρ[d]) ∈ R.

This piecewise structure immediately implies the following guarantee.

Corollary 6.12. Let score1, . . . , scored be d arbitrary scoring rules and let Q be an arbitrary
MILP over n binary variables. Suppose we limit B&B to producing search trees of size τ . For
each parameter vector ρ = (ρ[1], . . . , ρ[d]) ∈ [0, 1]d, let uρ(Q) be the size of the tree, divided by
τ , that B&B builds using the scoring rule ρ[1]score1 + · · · + ρ[d]scored given Q as input. Let
U be the set of functions U = (cid:8)uρ | ρ ∈ [0, 1]d(cid:9) mapping MILPs to [0, 1]. The dual class U ∗ is
(cid:0)F , G, n2(τ +1)(cid:1)-piecewise decomposable, where G = {ga,θ : U → {0, 1} | a ∈ Rd, θ ∈ R} consists
of halfspace indicator functions ga,θ : uρ (cid:55)→ I{ρ·a≤θ} and F = {fc : U → R | c ∈ R} consists of
constant functions fc : uρ (cid:55)→ c.

Corollary 6.12 and Lemma 3.10 imply the following pseudo-dimension bound.

Corollary 6.13. Let score1, . . . , scored be d arbitrary scoring rules and let Q be an arbitrary
MILP over n binary variables. Suppose we limit B&B to producing search trees of size τ . For each
parameter vector ρ = (ρ[1], . . . , ρ[d]) ∈ [0, 1]d, let uρ(Q) be the size of the tree, divided by τ , that
B&B builds using the scoring rule ρ[1]score1 +· · ·+ρ[d]scored given Q as input. Let U be the set of
functions U = (cid:8)uρ | ρ ∈ [0, 1]d(cid:9) mapping MILPs to [0, 1]. Then Pdim(U ) = O(d(τ ln(n) + ln(d))).

Corollary 6.13 matches the pseudo-dimension guarantee that Balcan et al. [8] prove.

Integer quadratic programming. A diverse array of NP-hard problems, including max-2SAT,
max-cut, and correlation clustering, can be characterized as integer quadratic programs (IQPs). An
IQP is represented by a matrix A ∈ Rn×n. The goal is to ﬁnd a set X = {x1, . . . , xn} ∈ {−1, 1}n

29

Algorithm 1 SDP rounding algorithm with rounding function r
Input: Matrix A ∈ Rn×n.
1: Draw a random vector Z from Z , the n-dimensional Gaussian distribution.
2: Solve the SDP (20) for the optimal embedding U = {u1, . . . , un}.
3: Compute set of fractional assignments r((cid:104)Z, u1(cid:105)), . . . , r((cid:104)Z, un(cid:105)).
4: For all i ∈ [n], set xi to 1 with probability 1

2 + 1

2 · r ((cid:104)Z, ui(cid:105)) and −1 with probability 1

2 − 1
2 ·

r ((cid:104)Z, ui(cid:105)).
Output: x1, . . . , xn.

maximizing (cid:80)
relaxation:

i,j∈[n] aijxixj. The most-studied IQP approximation algorithms operate via an SDP

maximize

(cid:88)

i,j∈[n]

aij(cid:104)ui, uj(cid:105)

subject to ui ∈ Sn−1.

(20)

The approximation algorithm must transform, or “round,” the unit vectors into a binary assignment
of the variables x1, . . . , xn. In the seminal GW algorithm [41], the algorithm projects the unit vectors
onto a random vector Z, which it draws from the n-dimensional Gaussian distribution, which we
denote using Z. If (cid:104)ui, Z(cid:105) > 0, it sets xi = 1. Otherwise, it sets xi = −1.

The GW algorithm’s approximation ratio can sometimes be improved if the algorithm prob-
In the ﬁnal step, the algorithm can use any rounding
abilistically assigns the binary variables.
function r : R → [−1, 1] to set xi = 1 with probability 1
2 · r ((cid:104)Z, ui(cid:105)) and xi = −1 with proba-
bility 1
2 · r ((cid:104)Z, ui(cid:105)). See Algorithm 1 for the pseudocode. Algorithm 1 is known as a Random
Projection, Randomized Rounding (RPR2) algorithm, so named by the seminal work of Feige and
Langberg [33].

2 + 1

2 − 1

Balcan et al. [7] analyze s-linear rounding functions [33] φs : R → [−1, 1], parameterized by

s > 0, deﬁned as follows:

φs(y) =




−1
y/s

1

if y < −s
if − s ≤ y ≤ s
if y > s.

The goal is to learn a parameter s such that in expectation, (cid:80)

i,j∈[n] aijxixj is maximized.
The expectation is over several sources of randomness: ﬁrst, the distribution D over matrices A;
second, the vector Z; and third, the assignment of x1, . . . , xn. This ﬁnal assignment depends on
the parameter s, the matrix A, and the vector Z. Balcan et al. [7] refer to this value as the true
utility of the parameter s. Note that the distribution over matrices, which deﬁnes the algorithm’s
input, is unknown and external to the algorithm, whereas the Gaussian distribution over vectors
as well as the distribution deﬁning the variable assignment are internal to the algorithm.

The distribution over matrices is unknown, so we cannot know any parameter’s true utility.
Therefore, to learn a good parameter s, we must use samples. Balcan et al. [7] suggest drawing
samples from two sources of randomness: the distributions over vectors and matrices.
In other
words, they suggest drawing a set of samples S = (cid:8)(cid:0)A(1), Z(1)(cid:1) , . . . , (cid:0)A(m), Z(m)(cid:1)(cid:9) ∼ (D × Z )m .
Given these samples, Balcan et al. [7] deﬁne a parameter’s empirical utility to be the expected
objective value of the solution Algorithm 1 returns given input A, using the vector Z and φs in
Step 3, on average over all (A, Z) ∈ S. Generally speaking, Balcan et al. [7] suggest sampling the
ﬁrst two randomness sources in order to isolate the third randomness source. They argue that this
third source of randomness has an expectation that is simple to analyze. Using pseudo-dimension,
they prove that every parameter s, its empirical and true utilities converge.

30

A bit more formally, Balcan et al. [7] use the notation p(i,Z,A,s) to denote the distribution that
the binary value xi is drawn from when Algorithm 1 is given A as input and uses the rounding
function r = φs and the hyperplane Z in Step 3. Using this notation, the parameter s has a true
utility of





E
A,Z∼D×Z



E
xi∼p(i,Z,A,s)

(cid:88)



i,j

aijxixj



 .2


We also use the notation us(A, Z) to denote the expected objective value of the solution Algorithm 1
returns given input A, using the vector Z and φs in Step 3. The expectation is over the ﬁnal
(cid:105)
assignment of each variable xi. Speciﬁcally, us(A, Z) = Exi∼p(i,Z,A,s)
. By deﬁnition, a
parameter’s true utility equals EA,Z∼D×Z [us(A, Z)]. Given a set (cid:0)A(1), Z(1)(cid:1) , . . . , (cid:0)A(m), Z(m)(cid:1) ∼
D × Z , a parameter’s empirical utility is 1
i=1 us
m
Both we and Balcan et al. [7] bound the pseudo-dimension of the function class U = {us : s > 0}.
Balcan et al. [7] prove that the functions in U are piecewise structured: roughly speaking, for a
ﬁxed matrix A and vector Z, each function in U is a piecewise, inverse-quadratic function of the
parameter s. To present this lemma, we use the following notation: given a tuple (A, Z), let
uA,Z : R → R be deﬁned such that uA,Z(s) = us (A, Z).

(cid:0)A(i), Z(i)(cid:1).

i,j aijxixj

(cid:104)(cid:80)

(cid:80)m

Lemma 6.14 ([7]). For any matrix A and vector Z, the function uA,Z : R>0 → R is made up of
s + c for some a, b, c ∈ R. Moreover, if the border
n + 1 piecewise components of the form a
between two components falls at some s ∈ R>0, then it must be that s = |(cid:104)ui, Z(cid:105)| for some ui in
the optimal SDP embedding of A.

s2 + b

This piecewise structure immediately implies the following corollary about the dual class U ∗.

Corollary 6.15. Let U be the set of functions U = {us : s > 0}. The dual class U ∗ is (F , G, n)-
piecewise decomposable, where G = {ga : U → {0, 1} | a ∈ R} consists of threshold functions
ga : us (cid:55)→ I{s≤a} and F = {fa,b,c : U → R | a, b, c ∈ R} consists of inverse-quadratic functions
fa,b,c : us (cid:55)→ a

s2 + b

s + c.

Lemma 3.9 and Corollary 6.15 imply the following pseudo-dimension bound.

Corollary 6.16. Let U be the set of functions U = {us : s > 0}. The pseudo-dimension of U is at
most O(ln n).

Corollary 6.16 matches the pseudo-dimension bound that Balcan et al. [7] prove.

6.3 Greedy algorithms

Gupta and Roughgarden [48] provide pseudo-dimension bounds for greedy algorithm conﬁguration,
analyzing two canonical combinatorial problems: the maximum weight independent set problem
and the knapsack problem. We recover their bounds in both cases.

2We, like Balcan et al. [7], use the abbreviated notation

(cid:34)

E
A,Z∼D×Z

E
xi∼p(i,Z,A,s)

(cid:34)

(cid:88)

i,j

(cid:35)(cid:35)

(cid:34)

aijxixj

=

E
A,Z∼D×Z

E
x1∼p(1,Z,A,s),...,xn∼p(n,Z,A,s)

(cid:35)(cid:35)

aijxixj

.

(cid:34)

(cid:88)

i,j

31

Maximum weight independent set (MWIS)
In the MWIS problem, the input is a graph G
with a weight w (v) ∈ R≥0 per vertex v. The objective is to ﬁnd a maximum-weight (or independent)
set of non-adjacent vertices. On each iteration, the classic greedy algorithm adds the vertex v that
maximizes w (v) / (1 + deg (v)) to the set. It then removes v and its neighbors from the graph. Given
a parameter ρ ≥ 0, Gupta and Roughgarden [48] propose the greedy heuristic w (v) / (1 + deg (v))ρ.
In this context, the utility function uρ(G, w) equals the weight of the vertices in the set returned
by the algorithm parameterized by ρ. Gupta and Roughgarden [48] implicitly prove the following
lemma about each function uρ (made explicit in work by Balcan et al. [9]). To present this lemma,
we use the following notation: let uG,w : R → R be deﬁned such that uG,w(ρ) = uρ (G, w).

Lemma 6.17 ([48]). For any weighted graph (G, w), the function uG,w : R → R is piecewise
constant with at most n4 discontinuities.

This structure immediately implies that the function class U = {uρ : ρ > 0} has a piecewise-

structured dual class.

Corollary 6.18. Let U be the set of functions U = {uρ : ρ > 0}. The dual class U ∗ is (F , G, n4)-
piecewise decomposable, where G = {ga : U → {0, 1} | a ∈ R} consists of threshold functions
ga : uρ (cid:55)→ I{ρ<a} and F = {fc : U → R | c ∈ R} consists of constant functions fc : uρ (cid:55)→ c.

Lemma 3.9 and Corollary 6.18 imply the following pseudo-dimension bound.

Corollary 6.19. Let U be the set of functions U = {uρ : ρ > 0}. The pseudo-dimension of U is
O(ln n).

This matches the pseudo-dimension bound by Gupta and Roughgarden [48].

Knapsack. Moving to the classic knapsack problem, the input is a knapsack capacity C and a
set of n items i each with a value νi and a size si. The goal is to determine a set I ⊆ {1, . . . , n}
with maximium total value (cid:80)
i∈I si ≤ C. Gupta and Roughgarden [48] suggest
the family of algorithms parameterized by ρ > 0 where each algorithm returns the better of the
following two solutions:

i∈I νi such that (cid:80)

• Greedily pack items in order of nonincreasing value νi subject to feasibility.
• Greedily pack items in order of νi/sρ

i subject to feasibility.

It is well-known that the algorithm with ρ = 1 achieves a 2-approximation. We use the notation
uρ (ν, s, C) to denote the total value of the items returned by the algorithm parameterized by ρ
given input (ν, s, C).

Gupta and Roughgarden [48] implicitly prove the following fact about the functions uρ (made
explicit in work by Balcan et al. [9]). To present this lemma, we use the following notation: given
a tuple (ν, s, C), let uν,s,C : R → R be deﬁned such that uν,s,C(ρ) = uρ (ν, s, C).

Lemma 6.20 ([48]). For any tuple (ν, s, C), the function uν,s,C : R → R is piecewise constant
with at most n2 discontinuities.

This structure immediately implies that the function class U = {uρ : ρ > 0} has a piecewise-

structured dual class.

Corollary 6.21. Let U be the set of functions U = {uρ : ρ > 0}. The dual class U ∗ is (F , G, n2)-
piecewise decomposable, where G = {ga : U → {0, 1} | a ∈ R} consists of threshold functions
ga : uρ (cid:55)→ I{ρ<a} and F = {fc : U → R | c ∈ R} consists of constant functions fc : uρ (cid:55)→ c.

32

Lemma 3.9 and Corollary 6.21 imply the following pseudo-dimension bound.

Corollary 6.22. Let U be the set of functions U = {uρ : ρ > 0}. The pseudo-dimension of U is
O(ln n).

This matches the pseudo-dimension bound by Gupta and Roughgarden [48].

6.4 Revenue maximization

The design of revenue-maximizing multi-item mechanisms is a notoriously challenging problem.
Remarkably, the revenue-maximizing mechanism is not known even when there are just two items
for sale. In this setting, the mechanism designer’s goal is to ﬁeld a mechanism with high expected
revenue on the distribution over agents’ values. A line of research has provided generalization
guarantees for mechanism design in the context of revenue maximization [6, 19, 23, 25, 29, 43, 44, 47,
76, 77]. These papers focus on sales settings: there is a seller, not included among the agents, who
will use a mechanism to allocate a set of goods among the agents. The agents submit bids describing
their values for the goods for sale. The mechanism determines which agents receive which items and
how much the agents pay. The seller’s revenue is the sum of the agents’ payments. The mechanism
designer’s goal is to select a mechanism that maximizes the revenue. In contrast to the mechanisms
we analyze in Section 5, these papers study mechanisms that are not necessarily budget-balanced.
Speciﬁcally, under every mechanism they study, the sum of the agents’ payments—the revenue—is
at least zero. As in Section 5, all of the mechanisms they analyze are incentive compatible.

We study the problem of selling m heterogeneous goods to n buyers. They denote a bundle
of goods as a subset b ⊆ [m]. Each buyer j ∈ [n] has a valuation function vj : 2[m] → R over
In this setting, the set X of problem instances consists of n-tuples of buyer
bundles of goods.
values v = (v1, . . . , vn). As in Section 5, selling mechanisms deﬁned by an allocation function and
a set of payment functions. Every auction in the classes they study is incentive compatible, so they
assume that the bids equal the bidders’ valuations. An allocation function ψ : X → (cid:0)2[m](cid:1)n
maps
the values v ∈ X to a division of the goods (b1, . . . , bn) ∈ (cid:0)2[m](cid:1)n
, where bi ⊆ [m] is the set of goods
buyer i receives. For each agent i ∈ [n], there is a payment function pi : X → R which maps values
v ∈ X to a payment pi(v) ∈ R≥0 that agent i must make.

We recover generalization guarantees proved by Balcan et al. [10] which apply to a variety of
widely-studied parameterized mechanism classes, including posted-price mechanisms, multi-part
tariﬀs, second-price auctions with reserves, aﬃne maximizer auctions, virtual valuations combina-
torial auctions mixed-bundling auctions, and randomized mechanisms. They provided new bounds
for mechanism classes not previously studied in the sample-based mechanism design literature and
matched or improved over the best known guarantees for many classes. They proved these guar-
antees by uncovering structure shared by all of these mechanisms: for any set of buyers’ values,
revenue is a piecewise-linear function of the mechanism’s parameters. This structure is captured
by our deﬁnition of piecewise decomposability.

Each of these mechanism classes is parameterized by a d-dimensional vector ρ ∈ P ⊆ Rd for
some d ≥ 1. For example, when d = m, ρ might be a vector of prices for each of the items. The
revenue of a mechanism is the sum of the agents’ payments. Given a mechanism parameterized by
a vector ρ ∈ Rd, we denote the revenue as uρ : X → R, where uρ(v) = (cid:80)n

i=1 pi(v).

Balcan et al. [10] provide psuedo-dimension bounds for any mechanism class that is delineable.
To deﬁne this notion, for any ﬁxed valuation vector v ∈ X , we use the notation uv(ρ) to denote
revenue as a function of the mechanism’s parameters.

Deﬁnition 6.23 ((d, t)-delineable [10]). A mechanism class is (d, t)-delineable if:

33

1. The class consists of mechanisms parameterized by vectors ρ from a set P ⊆ Rd; and

2. For any v ∈ X , there is a set H of t hyperplanes such that for any connected component P (cid:48)

of P \ H, the function uv (ρ) is linear over P (cid:48).

Delineability naturally translates to decomposability, as we formalize below.

Lemma 6.24. Let U be a set of revenue functions corresponding to a (d, t)-delineable mechanism
class. The dual class U ∗ is (F , G, t)-piecewise decomposable, where G = {ga,θ : U → {0, 1} | a ∈
Rd, θ ∈ R} consists of halfspace indicator functions ga,θ : uρ (cid:55)→ I{ρ·a≤θ} and F = {fa,θ : U → R |
a ∈ Rd, θ ∈ R} consists of linear functions fa,θ : uρ (cid:55)→ ρ · a + θ.

Lemmas 3.10 and 6.24 imply the following bound.

Corollary 6.25. Let U be a set of revenue functions corresponding to a (d, t)-delineable mechanism
class. The pseudo-dimension of U is at most O(d ln(td)).

Corollary 6.25 matches the pseudo-dimension bound that Balcan et al. [10] prove.

7 Experiments

We complement our theoretical guarantees with experiments in several settings to help illustrate
our bounds. Our experiments are in the contexts of both new and previously-studied domains:
tuning parameters of sequence alignment algorithms in computational biology, tuning parameters
of sales mechanisms so as to maximize revenue, and tuning parameters of voting mechanisms so as
to maximize social good. We summarize two observations from the experiments that help illustrate
the theoretical message of this paper.

Observation 1. First, using sequence alignment data (Section 7.1), we demonstrate that with a
ﬁnite number of samples, an algorithm’s average performance over the samples provides a tight
estimate of its expected performance on unseen instances.

Observation 2. Second, our experiments empirically illustrate that two algorithm families for the
same computational problem may require markedly diﬀerent training set sizes to avoid overﬁtting, a
fact that has also been explored in prior theoretical research [7, 10]. This shows that it is crucial to
bound an algorithm family’s intrinsic complexity to provide accurate guarantees, and in this paper
we provide tools for doing so. Our experiments here are in the contexts of mechanism design for
revenue maximization (Section 7.2.1) and mechanism design for voting (Section 7.2.2), both using
real-world data. In both of these settings, we analyze two natural, practical mechanism families
where one of the families is intrinsically simple and the other is intrinsically complex. When we use
Theorem 3.3 to select enough samples to ensure that overﬁtting does not occur for the simple class,
we indeed empirically observe overﬁtting when optimizing over the complex class. The complex
class requires more samples to avoid overﬁtting. In revenue-maximizing mechanism design, it was
known that there exists a separation between the intrinsic complexities of the two corresponding
mechanism families [10], though this was not known in voting mechanism design. In both of these
settings, despite surface-level similarities between the complex and simple mechanism families, there
is a pronounced diﬀerence in their intrinsic complexities, as illustrated by these experiments.

7.1 Sequence alignment experiments

Changing the alignment parameter can alter the accuracy of the produced alignments. Figure 5
shows the regions of the gap-open/gap-extension penalty plane divided into regions such that each

34

Figure 5: Parameter space decomposition for a single example.

region corresponds to a diﬀerent computed alignment. The regions in the ﬁgure are produced
using the XPARAL software of Gusﬁeld and Stelling [49], with using the BLOSUM62 amino acid
replacement matrix, the scores in each region were computed using Robert Edgar’s qscore package3.
The alignment sequences are a single pairwise alignment from the data set described below.

To illustrate Observation 1, we use the IPA tool [60] to learn optimal parameter choices for
a given set of example pairwise sequence alignments. We used 861 protein multiple sequence
alignment benchmarks that had been previously been used in DeBlasio and Kececioglu [24], which
split these benchmarks into 12 cross-validation folds that evenly distributed the “diﬃculty” of
an alignment (the accuracy of the alignment produced using aligner defaults parameter choice).
All pairwise alignments were extracted from each multiple sequence alignment. We then took
randomized increasing sized subsets of the pairwise sequence alignments from each training set
and found the optimal parameter choices for aﬃne gap costs and alphabet-dependent substitution
costs. These parameters were then given to the Opal aligner [v3.1b, 105] to realign the pairwise
alignments in the associated test sets.

Figure 6 shows the impact of increasing the number of training examples used to optimize
parameter choices. As the number of training examples increases, the optimized parameter choice
is less able to ﬁt the training data exactly and thus the training accuracy decreases. For the

3http://drive5.com/qscore/

35

0246810Gap Extension Penalty051015202530Gap Open Penalty0.00.20.40.60.81.0AccuracyFigure 6: Pairwise sequence alignment experiments showing the average accuracy on training and
test examples using parameter choices optimized for various training set sizes.

same reason the parameter choices are more general and the test accuracy increases. The test
and training accuracies are roughly equal when the training set size is close to 1000 examples and
remains equal for larger training sets. The test accuracy is actually slightly higher and this is likely
due to the training subset not representing the distribution of inputs as well as the full test set due
to the randomization being on all of the alignments rather than across diﬃculty, as was done to
create the cross-validation separations.

7.2 Mechanism design experiments

In this section, we provide experiments analyzing the estimation error of several mechanism classes.
We introduced the notion of estimation error in Section 2, but review it brieﬂy here. For a class
of mechanisms parameterized by vectors ρ (such as the class of neutral aﬃne maximizers from
Section 5), let uρ(v) ∈ [0, H] be the utility of the mechanism parameterized by ρ when the agents’
values are deﬁned by the vector v. For example, this utility might equal its social welfare or
revenue. Given a set of N samples S, the mechanism class’s estimation error equals the maximum
diﬀerence, across all parameter vectors ρ, between the average utility of the mechanism over the
samples 1
v∈S uρ(v) and its expected utility Ev∼D [uρ(v)]. We know that with high probability,
N
the estimation error is bounded by ˜O
, where d is the pseudo-dimension of the mechanism
class. In other words, for all parameter vectors ρ,

d/N

(cid:80)

(cid:112)

H

(cid:17)

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

(cid:88)

v∈S

uρ(v) − E
v∼D

(cid:12)
(cid:12)
(cid:12)
[uρ(v)]
(cid:12)
(cid:12)

= ˜O

(cid:32)

(cid:114)

H

(cid:33)

.

d
N

Said another way, ˜O
samples are suﬃcient to ensure that with high probability, for every
parameter vector ρ, the diﬀerence between the average and expected utility of the mechanism
parameterized by ρ is at most (cid:15).

(cid:17)

(cid:16) H 2d
(cid:15)2

7.2.1 Revenue maximization for selling mechanisms

We provide experiments that illustrate our guarantees in the context of revenue maximization
(which we introduced in Section 6.4). The type of mechanisms we analyze in our experiments

36

0500100015002000TrainingSetSize0.550.600.650.70AccuracyTestingTrainingare second-price auctions with reserves, one of the most well-studied mechanism classes in eco-
nomics [98]. In this setting, there is one item for sale and a set of n agents who are interested in
buying that item. This mechanism family is deﬁned by a parameter vector ρ ∈ Rn
≥0, where each
entry ρ[i] is the reserve price for agent i. The agents report their values for the item to the auc-
tioneer. If the highest bidder—say, agent i∗—reports a bid that is larger than her reserve ρ [i∗], she
wins the item and pays the maximum of the second-highest bid and her reserve ρ [i∗]. Otherwise,
no agent wins the item. The second-price auction (SPA) is called anonymous if every agent has
the same reserve price: ρ[1] = ρ[2] = · · · = ρ[n]. Otherwise, the SPA is called non-anonymous.
Like neutral aﬃne maximizers, SPAs are incentive compatible, so we assume the agents’ bids equal
their true values. We refer to the class of non-anonymous SPAs as AN and the class of anonymous
SPAs as AA.

The class of non-anonymous SPAs AN is more complex than the class of anonymous SPAs AA,
and thus the sample complexity of the former is higher than the latter. However, non-anonymous
SPAs can provide much higher revenue than anonymous SPAs. We illustrate this tradeoﬀ in our
experiments, which helps exemplify Observation 2.

In a bit more detail, the pseudo-dimension of the class of non-anonymous SPAs AN is ˜O(n), an
upper bound proved in prior research [10, 77] which can be recovered using the techniques in this
paper, as we summarize in Section 6.4. What’s more, Balcan et al. [10] proved a pseudo-dimension
lower bound of Ω(n) for this class. Meanwhile, the pseudo-dimension of the class of anonymous
SPAs AA is O(1). This upper bound was proved in prior research by Morgenstern and Roughgarden
[77] and Balcan et al. [10], and it follows from the general theory presented in this paper, as we
formalize below in Lemma 7.1. In our experiments, we exhibit a distribution over agents’ values
under which the following properties hold:

1. The true estimation error of the set of non-anonymous SPAs AN is larger than our upper

bound on the worst-case estimation error of the set of anonymous SPAs AA.

2. The expected revenue of the optimal non-anonymous SPA is signiﬁcantly larger than the
expected revenue of the optimal anonymous SPA: the former is 0.38 and the latter is 0.57.

These two points imply that even though it may be tempting to optimize over the class of non-
anonymous SPAs AN in order to obtain higher expected revenue, the training set must be large
enough to ensure the empirically-optimal mechanism has nearly optimal expected revenue.

The distribution we analyze is over the Jester Collaborative Filtering Dataset [42], which consists
of ratings from tens of thousands of users of one hundred jokes—in this example, the jokes could be
proxies for comedians, and the agents could be venues who are bidding for the opportunity to host
the comedians. Since the auctions we analyze in these experiments are for a single item, we run
experiments using agents’ values for a single joke, which we select to have a roughly equal number
of agents with high values as with low values (we describe this selection in greater detail below).
Figure 7 illustrates the outcome of our experiments. The orange dashed line is our upper bound on
the estimation error of the class of anonymous SPAs AA, which equals
δ , with
δ = 0.01. This upper bound has been presented in prior research [10, 77], and we recover it using the
general theory presented in this paper, as we demonstrate below in Lemma 7.1. The blue solid line
is the true estimation error4 of the class of non-anonymous SPAs AN over the Jester dataset, which
we calculate via the following experiment. For a selection of training set sizes N ∈ [2000, 12000],
we draw N sample agent values, with the number of agents equal to 10,612 (as we explain below).

N ln(eN )+

2N ln 1

(cid:113) 1

(cid:113) 4

4Sample complexity and estimation error bounds for SPAs have been studied in prior research [10, 25, 77], and

our guarantees match the bounds provided in that literature.

37

(cid:113) 4

Figure 7: Revenue maximization experiments. We vary the size of the training set, N , along
the x-axis. The orange dashed line is our theoretical upper bound on the estimation error of the
class of anonymous SPAs AA,
2N ln 100, which follows from our pseudo-dimension
bound (Lemma 7.1). The blue solid line lower bounds the true estimation error of the class of
non-anonymous SPAs AN over the Jester dataset. For several choices of N ∈ [2000, 10000], we
compute this lower bound by drawing a set of N training instances, ﬁnding a mechanism in AN
with high average revenue over the training set, and calculating the mechanism’s estimation error
(the diﬀerence between its average revenue and expected revenue). For scale, estimation error is a
quantity in the range [0, 1].

N ln(eN ) +

(cid:113) 1

We then ﬁnd a vector of non-anonymous reserves with maximum average revenue over the samples
but low expected revenue on the true distribution, as we describe in greater detail below. We
plot this diﬀerence between average and expected revenue, averaged over 100 trials of the same
procedure. As these experiments illustrate, there is a tradeoﬀ between the sample complexity and
revenue guarantees of these two classes, and it is crucial to calculate a class’s intrinsic complexity
to provide accurate guarantees. We now explain the details of these experiments.

Distribution over agents’ values. We use the Jester Collaborative Filtering Dataset [42], which
consists of ratings from 24,983 users of 100 jokes. The users’ ratings are in the range [−10, 10],
so we normalize their values to lie in the interval [0, 1]. We select a joke which has at least 5000
(normalized) ratings in the interval (cid:2) 1
(cid:3) and at least 5000 (normalized) ratings in the interval
4 , 1(cid:3) (speciﬁcally, Joke #22). There is a total of 5334 ratings of the ﬁrst type and 5278 ratings
(cid:2) 3
of the second type. Let W = {w1, . . . , w10,612} be all 10,612 values. For every i ∈ [10612], we
deﬁne the following valuation vector v(i): agent i’s value v
equals wi and for all other agents
(i)
j = 0. We deﬁne the distribution D over agents’ values to be the uniform distribution over
j (cid:54)= i, v
v(1), . . . , v(10,612).

4 , 1

(i)
i

2

Empirically-optimal non-anonymous reserve vectors with poor generalization. Given
a set of samples S ∼ DN , let p (cid:0)v(1)(cid:1) , . . . , p (cid:0)v(10,612)(cid:1) deﬁne the empirical distribution over S (that
is, p (cid:0)v(i)(cid:1) equals the number of times v(i) appears in S divided by N ). Then for any non-anonymous

38

reserve vector ρ ∈ Rn, the average revenue over the samples is

1
N

10,612
(cid:88)

i=1

(cid:16)

v(i)(cid:17)

p

ρ[i]1{wi≥ρ[i]}.

(21)

This is because for every vector v(i), the only agent with a non-zero value is agent i, whose value
is wi. Therefore, the highest bidder is agent i, who wins the item if wi ≥ ρ[i], and pays reserve
ρ[i], which is the maximum of ρ[i] and the second-highest bid. As is clear from Equation (21),
if v(i) ∈ S, an empirically-optimal reserve vector ˆρ (which maximizes average revenue over the
samples) will set ˆρi = wi, and if v(i) (cid:54)∈ S, ˆρi can be set arbitrarily, because it has no eﬀect on the
average revenue over the samples. In our experiments, for all v(i) (cid:54)∈ S, we set ˆρi = 3
4 . The intuition
(cid:3) will not win the item, and therefore will drag
is that those agents i with v(i) (cid:54)∈ S and wi ∈ (cid:2) 1
down expected revenue.

4 , 1

2

In Figure 7, the orange dashed line is the diﬀerence between the average value of ˆρ over S and
its expected value, which we calculate via the following experiment. For a selection of training
set sizes N ∈ [2000, 12000], we draw the set S ∼ DN . We then construct the reserve vector ˆρ as
described above. We plot the diﬀerence between the average value of ˆρ over S and its expected
value, averaged over 100 trials of the same procedure.

Estimation error of anonymous SPAs. We now use our general theory to bound the estima-
tion error of the class of anonymous SPAs AA so that we can plot the blue solid line in Figure 7.
This pseudo-dimension upper bound has been presented in prior research [10, 77], but here we
show that it can be recovered using this paper’s techniques. We use Lemma 3.8 to obtain a slightly
tighter pseudo-dimension bound (up to constant factors) than that of Corollary 6.25.

Given a reserve price ρ ≥ 0 and valuation vector v ∈ Rn, let uρ(v) ∈ [0, 1] be the revenue of the
anonymous second-price auction with a reserve price of ρ when the bids equal v. Using Lemma 3.8,
we prove that the pseudo-dimension of this class of revenue functions is at most 2.

Lemma 7.1. The pseudo-dimension of the class AA = {uρ : ρ ≥ 0} is at most 2.

Proof. Given a vector v of bids, let v(1) be the highest bid in v and let v(2) be the second-highest
bid. Under the anonymous SPA, the highest bidder wins the item if v(1) ≥ ρ and pays max (cid:8)v(2), ρ(cid:9).
Therefore uρ(v) = max (cid:8)v(2), ρ(cid:9) 1{v(1)≥ρ}. By deﬁnition of the dual function,

v (uρ) = max (cid:8)v(2), ρ(cid:9) 1{v(1)≥ρ}.
u∗

The dual function is thus an increasing function of ρ in the interval (cid:2)0, v(1)
(cid:3) and is equal to zero in
the interval (cid:0)v(1), ∞(cid:1). Therefore, the function has at most two oscillations (as in Deﬁnition 3.7).
By Lemma 3.8, the pseudo-dimension of AA is at most the largest integer D such that 2D ≤ 2D +1,
which equals 2. Therefore, the theorem statement holds.

By Equation (2), with probability 1 − δ over the draw of S ∼ DN , for any reserve ρ ≥ 0,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

(cid:88)

v∈S

uρ(v) − E
v∼D

(cid:12)
(cid:12)
(cid:12)
[uρ(v)]
(cid:12)
(cid:12)

(cid:114)

4
N

≤

ln(eN ) +

(cid:114)

1
2N

ln

1
δ

.

(22)

In Figure 7, the blue solid line equals the right-hand-side of Equation (22) with δ = 0.01 as a
function of N .

39

Discussion.

In summary, this section exempliﬁes a distribution over agents’ values where:

1. The true estimation error of the set of non-anonymous SPAs (the orange dashed line in
Figure 7) is larger than our upper bound on the worst-case estimation error of the set of
anonymous SPAs (the blue solid line in Figure 7).

2. The expected revenue of the optimal non-anonymous SPA is signiﬁcantly larger than the
expected revenue of the optimal anonymous SPA: the former is 0.38 and the latter is 0.57.

Therefore, there is a tradeoﬀ between the sample complexity and revenue guarantees of these two
classes.

7.2.2 Social welfare maximization for voting mechanisms

In this section, we provide similar experiments as those in the previous section, but in the context
of neutral aﬃne maximizers (NAMs), which we deﬁned in Section 5. We present a simple subset
of neutral aﬃne maximizers (NAMs) with a small estimation error upper bound. We then experi-
mentally demonstrate that the true estimation error of the class of NAMs is larger than this simple
subset’s estimation error. Therefore, it is crucial to calculate a class’s intrinsic complexity in order
to provide accurate guarantees. These experiments further illustrate Observation 2.

Our simple set of mechanisms is deﬁned as follows. One agent is selected to be the sink agent
(a sink agent i has the weight ρ[i] = 0), and every other agent’s weight is set to 1. In other words,
this class is deﬁned by the set of all vectors ρ ∈ {0, 1}n where exactly one component of ρ is equal
to zero. We use the notation A0 to denote this simple class, AN AM to denote the set of all NAMs
and uρ(v) to denote the social welfare of the NAM parameterized by ρ given the valuation vector
v.

Since there are n NAMs in A0, a Hoeﬀding and union bound tells us that for any distribution
D, with probability 0.99 over the draw of N valuation vectors S ∼ DN , for all n parameter vectors
ρ,

(cid:12)
(cid:12)
(cid:12)
E
(cid:12)
v∼D
(cid:12)

[uρ(v)] −

1
N

(cid:88)

v∈S

uρ(v)

(cid:114)

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ln(200n)
2N

.

(23)

This is the orange dashed line in Figure 8. Meanwhile, as we prove in Section 5, the pseudo-
dimension of the class of all NAMs AN AM is ˜Θ(n), so it is a more complex set of mechanisms
than A0. To experimentally compute the lower bound on the true estimation error of the class of
all NAMs AN AM , we identify a distribution such that the class has high estimation error, as we
describe below.

Distribution. As in the previous section, we use the Jester Collaborative Filtering Dataset [42]
to deﬁne our distribution. This dataset consists of ratings from 24,983 users of 100 jokes. The users’
ratings are in the range [−10, 10], so we normalize their ratings to lie in the interval [−0.5, 0.5].
We begin by selecting two jokes (jokes #7 and #15) such that—at a high level—a large number
of agents either like the ﬁrst joke a lot and do not like the second joke, or do not like the ﬁrst joke
and like the second joke a medium amount. We explain the intuition behind this choice below.
Speciﬁcally, we split the agents into two groups: in the ﬁrst group, the agents rated joke 1 at least
0.35 and rated joke 2 at most 0, and in the second group, the agents rated joke 1 at most 0 and
rated joke 2 between 0 and 0.15. We call the set of ratings corresponding to the ﬁrst group A1 ⊆ R2
and those corresponding to the second group A2 ⊆ R2. The set A1 has size 870 and A2 has size
1677.

40

(cid:113) ln(200n)
2N

Figure 8: Neutral aﬃne maximizer experiments. We vary the size of the training set, N , along the
x-axis. The orange dashed line is our upper bound on the estimation error of the simple subset of
NAMs A0,
(Equation (23)). The blue solid line lower bounds the true estimation error of
the entire class of NAMs AN AM over the Jester dataset. For several choices of N ∈ [100, 600], we
compute this lower bound by drawing a set of N training instances, ﬁnding a mechanism in AN AM
with high average social welfare over the training set, and calculating the mechanism’s estimation
error (the diﬀerence between its average social welfare and expected social welfare). For scale,
estimation error is a quantity in the range [0, 1].

We use A1 and A2 to deﬁne a distribution D over the valuations of n = 1000 agents for two
jokes. The support of D consists of 500 valuation vectors v(1), . . . , v(500) ∈ R2×1000. For i ∈ [500],
v(i) is deﬁned as follows. The values of agent i for the two jokes,
, are chosen
uniformly at random from A1 and the values of agent 500 + i are chosen uniformly at random from
A2. Every other agent i has a value of v

(i)
i (1), v

(i)
i (2)

(cid:16)

(cid:17)

v

(i)
i (2) = 0.

(i)
i (1) = v

Parameter vector with poor estimation error. Given a set of samples S ⊆ (cid:8)v(1), . . . , v(500)(cid:9),
we deﬁne a parameter vector ρ ∈ {0, 1}1000 with high estimation error as follows: for all i ∈ [500],

ρ[i] =

(cid:40)

1 if v(i) ∈ S
0 otherwise

and ρ[500 + i] =

(cid:40)
0
1

if v(i) (cid:54)∈ S
otherwise.

(24)

Intuitively, this parameter vector5 has high estimation error for the following reason. Suppose
v(i) ∈ S. The only agents with nonzero values in v(i) are agent i and agent 500 + i. Since v(i) ∈ S,
ρ[i] = 1 and ρ[500 + i] = 0. Therefore, agent i’s favorite joke is selected. Since agent i’s values are
from the set A1, they have a value of at least 0.35 for joke 1 and a value of at most 0 for joke 2.
Therefore, joke 1 will be the selected joke. Meanwhile, by the same reasoning for every v(i) (cid:54)∈ S, if

5Although this parameter vector has high average social welfare over the samples, it may set multiple agents to be
sink agents, which may be wasteful. We leave the problem of ﬁnding a parameter vector with high estimation error
and only a single sink agent to future research.

41

we run the NAM deﬁned by ρ, joke 2 will be the selected joke. In expectation over D, joke 1 has a
signiﬁcantly higher social welfare than joke 2. Therefore, the NAM deﬁned by ρ will have a high
average social welfare over the samples in S but a low expected social welfare, which means it will
have high estimation error. We illustrate this intuition in our experiments.

Experimental procedure. We repeat the following experiment 100 times. For various choices
of N ∈ [600], we draw a set of samples S ∼ DN , compute the parameter vector ρ deﬁned by
Equation (24), and compute the diﬀerence between the average social welfare of ρ over S and its
expected social welfare. We plot the diﬀerence averaged over all 100 runs.

Discussion. These experiments demonstrate that although the simple and complex NAM families
AN AM and A0 are artiﬁcially similar (they are both deﬁned by the m agent weights), the complex
family AN AM requires far more samples to avoid overﬁtting than the simple family A0. This
illustrates the importance of using our pseudo-dimension bounds to provide accurate guarantees.

8 Conclusions

We provided a general sample complexity theorem for learning high-performing algorithm conﬁg-
urations. Our bound applies whenever a parameterized algorithm’s performance is a piecewise-
structured function of its parameters: for any ﬁxed problem instance, boundary functions partition
the parameters into regions where performance is a well-structured function. We proved this guar-
antee by exploiting intricate connections between primal function classes (measuring the algorithm’s
performance as a function of its input) and dual function classes (measuring the algorithm’s perfor-
mance on a ﬁxed input as a function of its parameters). We demonstrated that many parameterized
algorithms exhibit this structure and thus our main theorem implies sample complexity guarantees
for a broad array of algorithms and application domains.

A great direction for future research is to build on these ideas for the sake of learning a portfolio
of conﬁgurations, rather than a single high-performing conﬁguration. At runtime, machine learning
is used to determine which conﬁguration in the portfolio to employ for the given input. Gupta and
Roughgarden [48] and Balcan et al. [14] have provided initial results in this direction, but a general
theory of portfolio-based algorithm conﬁguration is yet to be developed.

Acknowledgments. This research is funded in part by the Gordon and Betty Moore Founda-
tion’s Data-Driven Discovery Initiative (GBMF4554 to C.K.), the US National Institutes of Health
(R01GM122935 to C.K.), the US National Science Foundation (a Graduate Research Fellowship
to E.V. and grants IIS-1901403 to M.B. and T.S., IIS-1618714, CCF-1535967, CCF-1910321, and
SES-1919453 to M.B., IIS-1718457, IIS-1617590, and CCF-1733556 to T.S., and DBI-1937540 to
C.K.), the US Army Research Oﬃce (W911NF-17-1-0082 and W911NF2010081 to T.S.), the De-
fense Advanced Research Projects Agency under cooperative agreement HR00112020003 to M.B.,
an AWS Machine Learning Research Award to M.B., an Amazon Research Award to M.B., a Mi-
crosoft Research Faculty Fellowship to M.B., a Bloomberg Research Grant to M.B., a fellowship
from Carnegie Mellon University’s Center for Machine Learning and Health to E.V., and by the
generosity of Eric and Wendy Schmidt by recommendation of the Schmidt Futures program.

42

References

[1] Tobias Achterberg. SCIP: solving constraint integer programs. Mathematical Programming

Computation, 1(1):1–41, 2009.

[2] Daniel Alabi, Adam Tauman Kalai, Katrina Ligett, Cameron Musco, Christos Tzamos, and
Ellen Vitercik. Learning to prune: Speeding up repeated computations. In Conference on
Learning Theory (COLT), 2019.

[3] Patrick Assouad. Densit´e et dimension. Annales de l’Institut Fourier, 33(3):233–282, 1983.

[4] Maria-Florina Balcan. Data-driven algorithm design. In Tim Roughgarden, editor, Beyond

Worst Case Analysis of Algorithms. Cambridge University Press, 2020.

[5] Maria-Florina Balcan, Avrim Blum, Jason D Hartline, and Yishay Mansour. Mechanism
In Proceedings of the Annual Symposium on Foundations of

design via machine learning.
Computer Science (FOCS), pages 605–614, 2005.

[6] Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. Sample complexity of auto-
mated mechanism design. In Proceedings of the Annual Conference on Neural Information
Processing Systems (NeurIPS), 2016.

[7] Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. Learning-
theoretic foundations of algorithm conﬁguration for combinatorial partitioning problems.
Conference on Learning Theory (COLT), 2017.

[8] Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to

branch. International Conference on Machine Learning (ICML), 2018.

[9] Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm
design, online learning, and private optimization. In Proceedings of the Annual Symposium
on Foundations of Computer Science (FOCS), 2018.

[10] Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. A general theory of sample
complexity for multi-item proﬁt maximization. In Proceedings of the ACM Conference on
Economics and Computation (EC), 2018. Extended abstract. Full version available on arXiv
with the same title.

[11] Maria-Florina Balcan, Travis Dick, and Manuel Lang. Learning to link. In Proceedings of the

International Conference on Learning Representations (ICLR), 2020.

[12] Maria-Florina Balcan, Travis Dick, and Wesley Pegden. Semi-bandit optimization in the
dispersed setting. In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), 2020.

[13] Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. Learning to optimize com-
putational resources: Frugal training with generalization guarantees. AAAI Conference on
Artiﬁcial Intelligence (AAAI), 2020.

[14] Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. Generalization in portfolio-
based algorithm selection. In AAAI Conference on Artiﬁcial Intelligence (AAAI), 2021.

43

[15] Jon Louis Bentley, David S Johnson, Frank Thomson Leighton, Catherine C McGeoch, and
Lyle A McGeoch. Some unexpected expected behavior results for bin packing. In Proceedings
of the Annual Symposium on Theory of Computing (STOC), pages 279–288, 1984.

[16] Dimitris Bertsimas and Vassilis Digalakis Jr. Maria-ﬂorina balcan and dan f. deblasio and
arXiv preprint

travis dick and carl kingsford and tuomas sandholm and ellen vitercik.
arXiv:1908.02894, 2019.

[17] Avrim Blum, Chen Dan, and Saeed Seddighin. Learning complexity of simulated annealing.
In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2021.

[18] Robert Creighton Buck. Partition of space. The American Mathematical Monthly, 50:541–

544, 1943. ISSN 0002-9890.

[19] Yang Cai and Constantinos Daskalakis. Learning multi-item auctions with (or without)
In Proceedings of the Annual Symposium on Foundations of Computer Science

samples.
(FOCS), 2017.

[20] Shuchi Chawla, Evangelia Gergatsouli, Yifeng Teng, Christos Tzamos, and Ruimin Zhang.
Pandora’s box with correlations: Learning and approximation. In Proceedings of the Annual
Symposium on Foundations of Computer Science (FOCS), 2020.

[21] Antonia Chmiela, Elias B Khalil, Ambros Gleixner, Andrea Lodi, and Sebastian Pokutta.
Learning to schedule heuristics in branch-and-bound. arXiv preprint arXiv:2103.10294, 2021.

[22] Ed H. Clarke. Multipart pricing of public goods. Public Choice, 11:17–33, 1971.

[23] Richard Cole and Tim Roughgarden. The sample complexity of revenue maximization. In

Proceedings of the Annual Symposium on Theory of Computing (STOC), 2014.

[24] Dan DeBlasio and John D Kececioglu. Parameter Advising for Multiple Sequence Alignment.

Springer, 2018.

[25] Nikhil R Devanur, Zhiyi Huang, and Christos-Alexandros Psomas. The sample complexity
of auctions with side information. In Proceedings of the Annual Symposium on Theory of
Computing (STOC), 2016.

[26] Paul D¨utting, Silvio Lattanzi, Renato Paes Leme, and Sergei Vassilvitskii. Secretaries with

advice. arXiv preprint arXiv:2011.06726, 2020.

[27] Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal Wag-
ner. Learning-based support estimation in sublinear time. In Proceedings of the International
Conference on Learning Representations (ICLR), 2021.

[28] Robert C Edgar. Quality measures for protein alignment benchmarks. Nucleic acids research,

38(7):2145–2153, 2010.

[29] Edith Elkind. Designing and learning optimal ﬁnite support auctions. In Annual ACM-SIAM

Symposium on Discrete Algorithms (SODA), 2007.

[30] Marc Etheve, Zacharie Al`es, Cˆome Bissuel, Olivier Juan, and Saﬁa Kedad-Sidhoum. Rein-
forcement learning for variable selection in a branch and bound algorithm. pages 176–185.
Springer, 2020.

44

[31] ´Etienne Bamas, Andreas Maggiori, Lars Rohwedder, and Ola Svensson. Learning augmented
energy minimization via speed scaling. In Proceedings of the Annual Conference on Neural
Information Processing Systems (NeurIPS), 2020.

[32] ´Etienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learn-
ing augmented algorithms. In Proceedings of the Annual Conference on Neural Information
Processing Systems (NeurIPS), 2020.

[33] Uriel Feige and Michael Langberg. The RPR2 rounding technique for semideﬁnite programs.

Journal of Algorithms, 60(1):1–23, 2006.

[34] Da-Fei Feng and Russell F Doolittle. Progressive sequence alignment as a prerequisite to

correct phylogenetic trees. Journal of Molecular Evolution, 25(4):351–360, 1987.

[35] Aaron Ferber, Bryan Wilder, Bistra Dilkina, and Milind Tambe. MIPaaL: Mixed integer
program as a layer. In AAAI Conference on Artiﬁcial Intelligence (AAAI), volume 34, pages
1504–1511, 2020.

[36] David Fern´andez-Baca, Timo Sepp¨al¨ainen, and Giora Slutzki. Parametric multiple sequence
alignment and phylogeny construction. Journal of Discrete Algorithms, 2(2):271–287, 2004.

[37] Darya Filippova, Rob Patro, Geet Duggal, and Carl Kingsford. Identiﬁcation of alternative
topological domains in chromatin. Algorithms for Molecular Biology, 9:14, May 2014.

[38] Nikolaus Furian, Michael O’Sullivan, Cameron Walker, and Eranda C¸ ela. A machine learning-
based branch and price algorithm for a sampled vehicle routing problem. OR Spectrum, pages
1–40, 2021.

[39] Vikas Garg and Adam Kalai. Supervising unsupervised learning. In Proceedings of the Annual

Conference on Neural Information Processing Systems (NeurIPS). 2018.

[40] Andrew Gilpin and Tuomas Sandholm.

Information-theoretic approaches to branching in

search. Discrete Optimization, 8(2):147–159, 2011. Early version in IJCAI-07.

[41] Michel X Goemans and David P Williamson. Improved approximation algorithms for max-
imum cut and satisﬁability problems using semideﬁnite programming. Journal of the ACM
(JACM), 42(6):1115–1145, 1995.

[42] Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Eigentaste: A constant

time collaborative ﬁltering algorithm. Information Retrieval, 4(2):133–151, 2001.

[43] Yannai A Gonczarowski and Noam Nisan. Eﬃcient empirical revenue maximization in single-
In Proceedings of the Annual Symposium on Theory of

parameter auction environments.
Computing (STOC), pages 856–868, 2017.

[44] Yannai A Gonczarowski and S Matthew Weinberg. The sample complexity of up-to-ε multi-

dimensional revenue maximization. Journal of the ACM, 68(3):1–28, 2021.

[45] Osamu Gotoh. An improved algorithm for matching biological sequences. Journal of Molec-

ular Biology, 162(3):705 – 708, 1982. ISSN 0022-2836.

[46] Theodore Groves. Incentives in teams. Econometrica, 41:617–631, 1973.

45

[47] Chenghao Guo, Zhiyi Huang, and Xinzhi Zhang. Settling the sample complexity of single-
parameter revenue maximization. Proceedings of the Annual Symposium on Theory of Com-
puting (STOC), 2019.

[48] Rishi Gupta and Tim Roughgarden. A PAC approach to application-speciﬁc algorithm se-

lection. SIAM Journal on Computing, 46(3):992–1017, 2017.

[49] Dan Gusﬁeld and Paul Stelling. Parametric and inverse-parametric sequence alignment with

xparal. In Methods in enzymology, volume 266, pages 481–494. Elsevier, 1996.

[50] Dan Gusﬁeld, Krishnan Balasubramanian, and Dalit Naor. Parametric optimization of se-

quence alignment. Algorithmica, 12(4-5):312–326, 1994.

[51] Desmond G Higgins and Paul M Sharp. Clustal: a package for performing multiple sequence

alignment on a microcomputer. Gene, 73(1):237–244, 1988.

[52] Robert W. Holley, Jean Apgar, George A. Everett, James T. Madison, Mark Marquisee,
Susan H. Merrill, John Robert Penswick, and Ada Zamir. Structure of a ribonucleic acid.
Science, 147(3664):1462–1465, 1965.

[53] Eric Horvitz, Yongshao Ruan, Carla Gomez, Henry Kautz, Bart Selman, and Max Chicker-
ing. A Bayesian approach to tackling hard computational problems. In Proceedings of the
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2001.

[54] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estima-
tion algorithms. In Proceedings of the International Conference on Learning Representations
(ICLR), 2019.

[55] Frank Hutter, Holger Hoos, Kevin Leyton-Brown, and Thomas St¨utzle. ParamILS: An auto-
matic algorithm conﬁguration framework. Journal of Artiﬁcial Intelligence Research, 36(1):
267–306, 2009. ISSN 1076-9757.

[56] Piotr Indyk, Frederik Mallmann-Trenn, Slobodan Mitrovi´c, and Ronitt Rubinfeld. Online

page migration with ml advice. arXiv preprint arXiv:2006.05028, 2020.

[57] Raj Iyer, David Karger, Hariharan Rahul, and Mikkel Thorup. An experimental study of
polylogarithmic, fully dynamic, connectivity algorithms. ACM Journal of Experimental Al-
gorithmics, 6:4–es, December 2002. ISSN 1084-6654.

[58] Serdar Kadioglu, Yuri Malitsky, Meinolf Sellmann, and Kevin Tierney. ISAC-instance-speciﬁc
algorithm conﬁguration. In Proceedings of the European Conference on Artiﬁcial Intelligence
(ECAI), 2010.

[59] John D Kececioglu and Dean Starrett. Aligning alignments exactly. In Proceedings of the
Annual International Conference on Computational Molecular Biology, RECOMB, volume 8,
pages 85–96, 2004.

[60] Eagu Kim and John Kececioglu. Inverse sequence alignment from partial examples. Proceed-

ings of the International Workshop on Algorithms in Bioinformatics, pages 359–370, 2007.

[61] Robert Kleinberg, Kevin Leyton-Brown, and Brendan Lucier. Eﬃciency through procrastina-
tion: Approximately optimal algorithm conﬁguration with runtime guarantees. In Proceedings
of the International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2017.

46

[62] Robert Kleinberg, Kevin Leyton-Brown, Brendan Lucier, and Devon Graham. Procrastinat-
ing with conﬁdence: Near-optimal, anytime, adaptive algorithm conﬁguration. Proceedings
of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2019.

[63] James Kotary, Ferdinando Fioretto, Pascal Van Hentenryck, and Bryan Wilder. End-to-end

constrained optimization learning: A survey. arXiv preprint arXiv:2103.16378, 2021.

[64] Ailsa H Land and Alison G Doig. An automatic method of solving discrete programming

problems. Econometrica: Journal of the Econometric Society, pages 497–520, 1960.

[65] Thomas Lavastida, Benjamin Moseley, R Ravi, and Chenyang Xu.

Learnable and
instance-robust predictions for online matching, ﬂows and load balancing. arXiv preprint
arXiv:2011.11743, 2020.

[66] Kevin Leyton-Brown, Eugene Nudelman, and Yoav Shoham. Empirical hardness models:
Methodology and a case study on combinatorial auctions. Journal of the ACM, 56(4):1–52,
2009. ISSN 0004-5411.

[67] Erez Lieberman-Aiden, Nynke L. van Berkum, Louise Williams, Maxim Imakaev, Tobias
Ragoczy, Agnes Telling, Ido Amit, Bryan R. Lajoie, Peter J. Sabo, Michael O. Dorschner,
Richard Sandstrom, Bradley Bernstein, M. A. Bender, Mark Groudine, Andreas Gnirke, John
Stamatoyannopoulos, Leonid A. Mirny, Eric S. Lander, and Job Dekker. Comprehensive
mapping of long-range interactions reveals folding principles of the human genome. Science,
326(5950):289–293, 2009. ISSN 0036-8075. doi: 10.1126/science.1181369.

[68] Anton Likhodedov and Tuomas Sandholm. Methods for boosting revenue in combinatorial
auctions. In Proceedings of the National Conference on Artiﬁcial Intelligence (AAAI), pages
232–237, San Jose, CA, 2004.

[69] Anton Likhodedov and Tuomas Sandholm. Approximating revenue-maximizing combinato-
rial auctions. In Proceedings of the National Conference on Artiﬁcial Intelligence (AAAI),
Pittsburgh, PA, 2005.

[70] Jeﬀ Linderoth and Martin Savelsbergh. A computational study of search strategies for mixed

integer programming. INFORMS Journal of Computing, 11:173–187, 1999.

[71] Dar´ıo G Lupi´a˜nez, Malte Spielmann, and Stefan Mundlos. Breaking TADs: how alterations

of chromatin domains result in disease. Trends in Genetics, 32(4):225–237, 2016.

[72] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice.

In International Conference on Machine Learning (ICML), 2018.

[73] Catherine C McGeoch. A guide to experimental algorithmics. Cambridge University Press,

2012.

[74] Debasis Mishra and Arunava Sen. Roberts’ theorem with neutrality: A social welfare ordering

approach. Games and Economic Behavior, 75(1):283–298, 2012.

[75] Michael Mitzenmacher. A model for learned bloom ﬁlters and optimizing by sandwiching. In
Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS),
pages 464–473, 2018.

47

[76] Mehryar Mohri and Andr´es Mu˜noz. Learning theory and algorithms for revenue optimization
In International Conference on Machine Learning

in second price auctions with reserve.
(ICML), 2014.

[77] Jamie Morgenstern and Tim Roughgarden. Learning simple auctions.

In Conference on

Learning Theory (COLT), 2016.

[78] Swaprava Nath and Tuomas Sandholm. Eﬃciency and budget balance in general quasi-linear

domains. Games and Economic Behavior, 113:673 – 693, 2019.

[79] Saket Navlakha, James White, Niranjan Nagarajan, Mihai Pop, and Carl Kingsford. Finding
biologically accurate clusterings in hierarchical tree decompositions using the variation of
information. In Annual International Conference on Research in Computational Molecular
Biology, volume 5541, pages 400–417. Springer, 2009.

[80] Ruth Nussinov and Ann B Jacobson. Fast algorithm for predicting the secondary structure
of single-stranded RNA. Proceedings of the National Academy of Sciences, 77(11):6309–6313,
1980.

[81] Lior Pachter and Bernd Sturmfels. Parametric inference for biological sequence analysis.
Proceedings of the National Academy of Sciences, 101(46):16138–16143, 2004. doi: 10.1073/
pnas.0406011101.

[82] Lior Pachter and Bernd Sturmfels. Tropical geometry of statistical models. Proceedings of the

National Academy of Sciences, 101(46):16132–16137, 2004. doi: 10.1073/pnas.0406010101.

[83] David Pollard. Convergence of Stochastic Processes. Springer, 1984.

[84] Antoine Prouvost, Justin Dumouchelle, Lara Scavuzzo, Maxime Gasse, Didier Ch´etelat, and
Andrea Lodi. Ecole: A gym-like library for machine learning in combinatorial optimization
solvers. arXiv preprint arXiv:2011.06069, 2020.

[85] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ML pre-
dictions. In Proceedings of the Annual Conference on Neural Information Processing Systems
(NeurIPS), pages 9661–9670, 2018.

[86] Kevin Roberts. The characterization of implementable social choice rules. In J-J Laﬀont,
editor, Aggregation and Revelation of Preferences. North-Holland Publishing Company, 1979.

[87] Tuomas Sandholm. Very-large-scale generalized combinatorial multi-attribute auctions:
In Zvika Neeman, Alvin Roth, and Nir

Lessons from conducting $60 billion of sourcing.
Vulkan, editors, Handbook of Market Design. Oxford University Press, 2013.

[88] Tuomas Sandholm and Anton Likhodedov. Automated design of revenue-maximizing com-
binatorial auctions. Operations Research, 63(5):1000–1025, 2015. Special issue on Computa-
tional Economics. Subsumes and extends over a AAAI-05 paper and a AAAI-04 paper.

[89] J. Michael Sauder, Jonathan W. Arthur, and Roland L. Dunbrack Jr. Large-scale comparison
of protein sequence alignment algorithms with structure alignments. Proteins: Structure,
Function, and Bioinformatics, 40(1):6–22, 2000.

[90] Norbert Sauer. On the density of families of sets. Journal of Combinatorial Theory, Series

A, 13(1):145–147, 1972.

48

[91] Daniel Selsam and Nikolaj Bjørner. Guiding high-performance SAT solvers with unsat-core
predictions. In International Conference on Theory and Applications of Satisﬁability Testing,
pages 336–353. Springer, 2019.

[92] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to

algorithms. Cambridge University Press, 2014.

[93] Yifei Shen, Yuanming Shi, Jun Zhang, and Khaled B Letaief. Lorm: Learning to optimize
for resource management in wireless networks with few training samples. IEEE Transactions
on Wireless Communications, 19(1):665–679, 2019.

[94] Jialin Song, Ravi Lanka, Yisong Yue, and Bistra Dilkina. A general large neighborhood search
framework for solving integer programs. In Proceedings of the Annual Conference on Neural
Information Processing Systems (NeurIPS), 2020.

[95] Yunhao Tang, Shipra Agrawal, and Yuri Faenza. Reinforcement learning for integer program-
ming: Learning to cut. In International Conference on Machine Learning (ICML), 2020.

[96] Timo Tossavainen. On the zeros of ﬁnite sums of exponential functions. Australian Mathe-

matical Society Gazette, 33(1):47–50, 2006.

[97] Vladimir Vapnik and Alexey Chervonenkis. On the uniform convergence of relative frequencies
of events to their probabilities. Theory of Probability and its Applications, 16(2):264–280,
1971.

[98] William Vickrey. Counterspeculation, auctions, and competitive sealed tenders. Journal of

Finance, 16:8–37, 1961.

[99] Lusheng Wang and Tao Jiang. On the complexity of multiple sequence alignment. Journal

of Computational Biology, 1(4):337–348, 1994.

[100] Michael S Waterman, Temple F Smith, and William A Beyer. Some biological sequence

metrics. Advances in Mathematics, 20(3):367–387, 1976.

[101] Hugues Wattez, Fr´ed´eric Koriche, Christophe Lecoutre, Anastasia Paparrizou, and S´ebastien
Tabary. Learning variable ordering heuristics with multi-armed bandits and restarts. 2020.

[102] Alexander Wei and Fred Zhang. Optimal robustness-consistency trade-oﬀs for learning-
augmented online algorithms. In Proceedings of the Annual Conference on Neural Information
Processing Systems (NeurIPS), 2020.

[103] Gell´ert Weisz, Andr´as Gy¨orgy, and Csaba Szepesv´ari. LeapsAndBounds: A method for
In International Conference on Machine

approximately optimal algorithm conﬁguration.
Learning (ICML), 2018.

[104] Gell´ert Weisz, Andr´as Gy¨orgy, and Csaba Szepesv´ari. CapsAndRuns: An improved method
for approximately optimal algorithm conﬁguration. In International Conference on Machine
Learning (ICML), 2019.

[105] Travis J. Wheeler and John D. Kececioglu. Multiple alignment by aligning alignments. Bioin-

formatics, 23(13):i559–i568, 07 2007.

[106] Lin Xu, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. SATzilla: portfolio-based
algorithm selection for SAT. Journal of Artiﬁcial Intelligence Research, 32(1):565–606, 2008.

49

[107] Lin Xu, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Hydra-MIP: Automated
algorithm conﬁguration and selection for mixed integer programming. In RCRA workshop on
Experimental Evaluation of Algorithms for Solving Problems with Combinatorial Explosion
at the International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2011.

[108] Giulia Zarpellon, Jason Jo, Andrea Lodi, and Yoshua Bengio. Parameterizing branch-and-
bound search trees to learn branching policies. In AAAI Conference on Artiﬁcial Intelligence
(AAAI), 2021.

A Helpful lemmas

Lemma A.1 (Shalev-Shwartz and Ben-David [92]). Let a ≥ 1 and b > 0. If y < a ln y + b, then
y < 4a ln(2a) + 2b.

The following is a corollary of Rolle’s theorem that we use in the proof of Lemma 4.6.

Lemma A.2 (Tossavainen [96]). Let h be a polynomial-exponential sum of the form h(x) =
(cid:80)t

i , where bi > 0 and ai ∈ R. The number of roots of h is upper bounded by t.

i=1 aibx

Corollary A.3. Let h be a polynomial-exponential sum of the form

h(x) =

t
(cid:88)

i=1

ai
bx
i

,

where bi > 0 and ai ∈ R. The number of roots of h is upper bounded by t.
Proof. Note that (cid:80)t

= 0 if and only if

i=1

ai
bx
i





n
(cid:89)



bx
i



j=1

t
(cid:88)

i=1

ai
bx
i

=

n
(cid:88)

i=1





x

ai



(cid:89)

j(cid:54)=i

bi



= 0.

Therefore, the corollary follows from Lemma A.2.

B Additional details about our general theorem

Lemma 3.10. Let U = (cid:8)uρ | ρ ∈ P ⊆ Rd(cid:9) be a class of utility functions deﬁned over a d-dimensional
parameter space. Suppose the dual class U ∗ is (F , G, k)-piecewise decomposable, where the bound-
ary functions G = (cid:8)fa,θ : U → {0, 1} | a ∈ Rd, θ ∈ R(cid:9) are halfspace indicator functions ga,θ : uρ (cid:55)→
I{a·ρ≤θ} and the piece functions F = (cid:8)fa,θ : U → R | a ∈ Rd, θ ∈ R(cid:9) are linear functions fa,θ :
uρ (cid:55)→ a · ρ + θ. Then Pdim(U ) = O(d ln(dk)).

uρ for all ρ ∈ P where g∗

Proof. First, we prove that the VC-dimension of the dual class G∗ is at most d + 1. The dual class
uρ (ga,θ) = I{a·ρ≤θ}. Let ˆG = (cid:8)ˆgρ : Rd+1 → {0, 1}(cid:9)
G∗ consists of functions g∗
be the class of halfspace thresholds ˆgρ : (a, θ) (cid:55)→ I{a·ρ≤θ}. It is well-known that VCdim
≤ d+1,
which we prove means that VCdim (G∗) ≤ d + 1. For a contradiction, suppose G∗ can shatter d + 2
functions ga1,θ1, . . . , gad+2,θd+2 ∈ G. Then for every subset T ⊆ [d + 2], there exists a parameter
vector ρT such that ai · ρT ≤ θi if and only if i ∈ T . This means that ˆG can shatter the tuples

(cid:17)
(cid:16) ˆG

50

(a1, θ1) , . . . , (ad+2, θd+2) as well, which contradicts the fact that VCdim
VCdim (G∗) ≤ d + 1.

(cid:17)
(cid:16) ˆG

≤ d + 1. Therefore,

(cid:17)

(cid:111)

(cid:110) ˆfρ : Rd+1 → R
(cid:16) ˆF

d + 1. The dual class F ∗ consists of functions f ∗
ˆF =

By a similar argument, we prove that the pseudo-dimension of the dual class F ∗ is at most
uρ for all ρ ∈ P where f ∗
uρ (fa,θ) = a · ρ + θ. Let
be the class of linear functions ˆfρ : (a, θ) (cid:55)→ a · ρ + θ. It is well-known that
≤ d + 1, which we prove means that Pdim (F ∗) ≤ d + 1. For a contradiction, suppose
Pdim
F ∗ can shatter d + 2 functions fa1,θ1, . . . , fad+2,θd+2 ∈ F . Then there exist witnesses z1, . . . , zd+2
such that for every subset T ⊆ [d + 2], there exists a parameter vector ρT such that ai · ρT + θi ≤ zi
if and only if i ∈ T . This means that ˆF can shatter the tuples (a1, θ1) , . . . , (ad+2, θd+2) as well,
which contradicts the fact that Pdim

≤ d + 1. Therefore, Pdim (F ∗) ≤ d + 1.

(cid:16) ˆF

(cid:17)

The lemma statement now follows from Theorem 3.3.

C Additional details about sequence alignment (Section 4.1)

Lemma C.1. Fix a pair of sequences S1, S2 ∈ Σn. There are at most 2nn2n+1 alignments of S1
and S2.

Proof. For any alignment (τ1, τ2), we know that |τ1| = |τ2| and for all i ∈ [|τ1|], if τ1[i] = −, then
τ2[i] (cid:54)= − and vice versa. This means that τ1 and τ2 have the same number of gaps. To prove the
upper bound, we count the number of alignments (τ1, τ2) where τ1 and τ2 each have exactly i gaps.
(cid:1) choices for the sequence τ1. Given a sequence τ1, we can only pair a gap in τ2 with
There are (cid:0)n+i
i
a non-gap in τ1. Since there are i gaps in τ2 and n non-gaps in τ1, there are (cid:0)n
(cid:1) choices for the
sequence τ2 once τ1 is ﬁxed. This means that there are (cid:0)n+i
(cid:1) ≤ 2nn2n alignments (τ1, τ2) where
τ1 and τ2 each have exactly i gaps. Summing over i ∈ [n], the total number of alignments is at
most 2nn2n+1.
Theorem 4.3. There exists a set (cid:8)Aρ | ρ ∈ R3
phabet Σ such that the set of functions U = (cid:8)uρ : (S1, S2) (cid:55)→ u (S1, S2, Aρ (S1, S2)) | ρ ∈ R3
which map sequence pairs S1, S2 ∈ ∪n
Ω(log n).

(cid:9) of co-optimal-constant algorithms and an al-
(cid:9) ,
i=1Σi of length at most n to [0, 1], has a pseudo-dimension of

(cid:1)(cid:0)n
i

≥0

≥0

i

i

Proof. To prove this theorem, we identify:

1. An alphabet Σ,

2. A set of N = Θ(log n) sequence pairs

(cid:16)

S

(1)
(1)
1 , S
2

(cid:17)

, . . . ,

(cid:16)

(N )
S
1

(N )
, S
2

(cid:17)

∈ ∪n

i=1Σi × Σi,

3. A ground-truth alignment L

(i)
∗

for each sequence pair

(cid:16)

S

(i)
(i)
1 , S
2

(cid:17)

, and

4. A set of N witnesses z1, . . . , zN ∈ R such that for any subset T ⊆ [N ], there exists an indel
< zi and if i (cid:54)∈ [T ], then

penalty parameter ρ[T ] such that if i ∈ [T ], then u0,ρ[T ],0

(i)
(i)
1 , S
2

S

(cid:17)

(cid:16)

u0,ρ[T ],0

(cid:16)

S

(i)
(i)
1 , S
2

(cid:17)

≥ zi.

We now describe each of these four elements in turn.

51

(cid:106)
The alphabet Σ. Let k = 2
we denote as {ai, bi, ci, di}k
i=1.

log

√

(cid:107)

n/2

√

− 1 = Θ(

n). The alphabet Σ consists of 4k characters6

The set of N = Θ(log n) sequence pairs. These N sequence pairs are deﬁned by a set of k
subsequence pairs
is deﬁned as follows:

∈ Σ∗ × Σ∗. Each pair

, . . . ,

(cid:16)

(cid:17)

(cid:17)

(cid:17)

(cid:16)

(cid:16)

t

t

(k)
(k)
1 , t
t
2

(1)
(1)
1 , t
2

(i)
(i)
1 , t
2

• The subsequence t

(i)
1 begins with i ais followed by bidi. For example, t

(3)
1 = a3a3a3b3d3.

• The subsequence t
(3)
2 = b3c3c3c3d3.
t

(i)
2 begins with 1 bi, followed by i cis, followed by 1 di. For example,

Therefore, t

(i)
1 and t

(i)
2 are both of length i + 2.

We use these subsequence pairs to deﬁne a set of N = log(k + 1) = Θ(log n) sequence pairs. The
is the concatenation of all subsequences

(1)
is deﬁned as follows: S
1

(1)
(1)
1 , S
2

S

(cid:16)

(cid:17)

ﬁrst sequence pair,
(k)
(1)
1 , . . . , t
t
1

and S

(1)
2

is the concatenation of all subsequences t

(1)
2 , . . . , t

(k)
2 :

S

(1)
1 = t

(1)
1 t

(3)
(2)
1 t
1

(k)
· · · t
1

and

S

(1)
2 = t

(1)
2 t

(3)
(2)
2 t
2

· · · t

(k)
2 .

(2)
Next, S
1

and S

(2)
2

are the concatenation of every 2nd subsequence:

S

(2)
1 = t

(2)
1 t

(4)
1 t

(6)
1

(k−1)
· · · t
1

and

S

(2)
2 = t

(2)
2 t

(6)
(4)
2 t
2

(k−1)
· · · t
2

.

(3)
Similarly, S
1

(3)
and S
2

are the concatenation of every 4th subsequence:

S

(3)
1 = t

(4)
1 t

(12)
(8)
1 t
1

(k−3)
· · · t
1

and

S

(3)
2 = t

(4)
2 t

(12)
(8)
2 t
2

(k−3)
· · · t
2

.

Generally speaking, S

(i)
1 and S

(i)

2 are the concatenation of every (cid:0)2i−1(cid:1)th

subsequence:

(2i−1)
(i)
1 = t
1

S

(2·2i−1)
t
1

(3·2i−1)
1

t

(k+1−2i−1)
· · · t
1

and

(i)
2 = t

(2i−1)
2

(2·2i−1)
2

(3·2i−1)
t
2

t

(k+1−2i−1)
· · · t
2

.

S

To explain the index of the last subsequence of every pair, since k + 1 is a power of two, we know
that k − 1 is divisible by 2, k − 3 is divisible by 4, and more generally, k + 1 − 2i−1 is divisible by
2i−1.

We claim that there are a total of N = log(k + 1) such sequence pairs. To see why, note that
(1)
and S
consists of k subsequences. Each sequence in the
2
subsequences. More generally, each sequence in the ith

each sequence in the ﬁrst pair S
second pair S

(1)
1
consists of k−1
2

(2)
1

and S

(2)
2
(k+1−2i−1)
and S
2

(k+1−2i−1)
1

pair S
of only one subsequence, so k+1−2i−1

consists of k+1−2i−1

2i−1

subsequences. The ﬁnal pair will will consist

2i−1 = 1, or in other words i = log(k + 1).

We also claim that each sequence has length at most n. This is because the longest sequence
(i)
j , these two sequences are of

. By deﬁnition of the subsequences t

(1)
(1)
1 , S
2

S

(cid:17)

(cid:16)

2 k(k + 5) ≤ n. Therefore, all N sequence pairs are of length at most n.

pair is the ﬁrst,
length (cid:80)k

i=1(i + 2) = 1

6To simplify the proof, we use this alphabet of size 4k, but we believe it is possible to adapt this proof to handle

the case where there are only 4 characters in the alphabet.

52

(cid:106)
Example C.2. Suppose that n = 128. Then k = 2
three sequence pairs have the following form7:

√

(cid:107)

n/2

log

− 1 = 7 and N = log(k + 1) = 3. The

S(1)
1 = a1b1d1a2a2b2d2a3a3a3b3d3a4a4a4a4b4d4a5a5a5a5a5b5d5a6a6a6a6a6a6b6d6a7a7a7a7a7a7a7b7d7
S(1)
2 = b1c1d1b2c2c2d2b3c3c3c3d3b4c4c4c4c4d4b5c5c5c5c5c5d5b6c6c6c6c6c6c6d6b7c7c7c7c7c7c7c7d7
S(2)
1 = a2a2b2d2a4a4a4a4b4d4a6a6a6a6a6a6b6d6
S(2)
2 = b2c2c2d2b4c4c4c4c4d4b6c6c6c6c6c6c6d6
S(3)
1 = a4a4a4a4b4d4
S(3)
2 = b4c4c4c4c4d4

A ground-truth alignment for every sequence pair. To deﬁne a ground-truth alignment for
all N sequence pairs, we ﬁrst deﬁne two alignments per subsequence pair
. The resulting
ground-truth alignments will be a concatenation of these alignments. The ﬁrst alignment, which
(i)
1 begins with i ais, followed by 1 bi, followed by
we denote as

, is deﬁned as follows: h
(i)
2 begins with i gap characters, followed by 1 bi, followed by i

(i)
(i)
1 , t
t
2

(i)
(i)
1 , h
2

(cid:17)

(cid:16)

(cid:16)

(cid:17)

h

i gap characters, followed by 1 di; h
cis, followed by 1 di. For example,

(3)
1 = a3 a3 a3 b3
h
(3)
-
2 = -
h

-

-

-
d3
b3 c3 c3 c3 d3

-

.

The second alignment, which we denote as

(cid:16)

(i)
(i)
1 , (cid:96)
2

(cid:96)

(cid:17)

followed by 1 bi, followed by i gap characters, followed by 1 di; (cid:96)
gap characters, followed by i cis, followed by 1 di. For example,

, is deﬁned as follows: (cid:96)
(i)
2 begins with 1 bi, followed by i

(i)
1 begins with i ais,

(3)
1 = a3 a3 a3 b3
(cid:96)
(3)
-
-
2 = b3
(cid:96)

-

-

-

-
d3
c3 c3 c3 d3

.

We now use these 2k alignments to deﬁne a ground-truth alignment L
(i)
(i)
1 , S
2

. Beginning with the ﬁrst pair

(1)
(1)
1 , S
2

, where

S

(cid:17)

(cid:16)

(cid:17)

(i)
∗ per sequence pair

(cid:16)

S

S

(1)
1 = t

(1)
1 t

(2)
1 t

(3)
1

· · · t

(k)
1

and

S

(1)
2 = t

(1)
2 t

(2)
2 t

(3)
2

· · · t

(k)
2 ,

(1)
we deﬁne the alignment of S
1

be (cid:96)

(1)
2 h

(2)
2 (cid:96)

(4)
(3)
2 h
2

· · · (cid:96)

(k)
· · · (cid:96)
1
(cid:16)
(k)
S
2 . Moving on to the second pair

to be (cid:96)

(1)
1 h

(3)
1 h

(2)
1 (cid:96)

(4)
1

(1)
and we deﬁne the alignment of S
2
(2)
(2)
1 , S
2

, where

(cid:17)

S

(2)
1 = t

(2)
1 t

(6)
(4)
1 t
1

· · · t

(k−1)
1

and

S

(2)
2 = t

(2)
2 t

(4)
2 t

(6)
2

(k−1)
· · · t
2

,

(2)
(4)
we deﬁne the alignment of S
1 (cid:96)
1
(k−1)
(8)
(6)
· · · (cid:96)
. Generally speaking, each pair
2 h
2
2

(8)
(6)
1 h
1

to be (cid:96)

to be (cid:96)

(2)
1 h

(2)
2 h

(k−1)
1
(cid:16)

(4)
2 (cid:96)

· · · (cid:96)

(cid:17)

S

(i)
(i)
1 , S
2

, where

and we deﬁne the alignment of S

to

(1)
2

S

(i)
1 = t

(2i−1)
1

(2·2i−1)
1

(3·2i−1)
t
1

t

(k−2i−1+1)
· · · t
1

and

(2i−1)
(i)
2 = t
2

(2·2i−1)
t
2

(3·2i−1)
2

t

S

· · · t

(k−2i−1+1)
2

,

7The maximum length of these six strings is 42, which is smaller than 128, as required.

53

(a) An initial alignment where the dj characters are not aligned.

(b) An alignment where the gap characters in Figure 9a are shifted such that the dj characters are aligned.
The objective function value of both alignments is the same.

Figure 9: Illustration of Claim C.3: we can assume that each dj character in S
in S

(i)
2 .

(i)
1

is matched to dj

is made up of k+1
(i)
odd. We deﬁne the alignment of S
1
(j(cid:48))
of type h
1
(2i−1)
is (cid:96)
1

(4·2i−1)
h
1

(2·2i−1)
h
1

(3·2i−1)
1

(i)
S
1

(cid:96)

2i−1 − 1 subsequences. Since k + 1 is a power of two, this number of subsequences is
(j)
1 and alignments
, beginning and ending with alignments of the ﬁrst type. Speciﬁcally, the alignment

to alternate between alignments of type (cid:96)

· · · (cid:96)

(k−2i−1+1)
1

(i)
. Similarly, we deﬁne the alignment of S
2

to be

(2i−1)
2

(cid:96)

(2·2i−1)
2

(3·2i−1)
(cid:96)
2

(4·2i−1)
2

h

h

· · · (cid:96)

(k−2i−1+1)
2

.

The N witnesses. We deﬁne the N values that witness the shattering of these N sequence pairs
to be z1 = z2 = · · · = zN = 3
4 .

Shattering the N sequence pairs. Our goal is to show that for any subset T ⊆ [N ], there
exists an indel penalty parameter ρ[T ] such that if i ∈ [T ], then u0,ρ[T ],0
4 and if

(i)
(i)
1 , S
2

< 3

S

(cid:17)

(cid:16)

≥ 3

4 . To prove this, we will use two helpful claims, Claims C.3 and

i (cid:54)∈ [T ], then u0,ρ[T ],0
C.4.

(cid:16)

S

(i)
(i)
1 , S
2

(cid:17)

Claim C.3. For any pair

(cid:16)

S

(i)
(i)
1 , S
2

(cid:17)

and indel parameter ρ[2] ≥ 0, there exists an alignment

L ∈ argmaxL(cid:48)mt

(cid:16)

S

(i)
1 , S

(i)

2 , L(cid:48)(cid:17)

− ρ[2] · id

(cid:16)

S

(i)
1 , S

(i)

2 , L(cid:48)(cid:17)

such that each dj character in S

(i)
1

is matched to dj in S

(i)
2 .
(cid:16)

(cid:16)

S

S

(i)

(i)

(i)
1

(i)
1 , S

(i)
1 , S

2 , L(cid:48)(cid:17)

be an alignment such that

2 , L(cid:48)(cid:17)
is not matched to dj in S

− ρ[2] · id
Proof. Let L0 ∈ argmaxL(cid:48)mt
(i)
some dj character in S
2 . Denote the alignment L0 as (τ1, τ2). Let
j ∈ Z be the smallest integer such that for some indices (cid:96)1 (cid:54)= (cid:96)2, τ1[(cid:96)1] = dj and τ2[(cid:96)2] = dj. Next,
let (cid:96)0 be the maximum index smaller than (cid:96)1 and (cid:96)2 such that τ1[(cid:96)0] = dj(cid:48) for some j(cid:48)
(cid:54)= j. We
illustrate (cid:96)0, (cid:96)1, and (cid:96)2 in Figure 9a. By deﬁnition of j, we know that τ1[(cid:96)0] = τ2[(cid:96)0] = dj(cid:48). We
also know there is at least one gap character in {τ1[i] : (cid:96)0 + 1 ≤ i ≤ (cid:96)1} ∪ {τ2[i] : (cid:96)0 + 1 ≤ i ≤ (cid:96)2}
because otherwise, the dj characters would be aligned in L0. Moreover, we know there is at most
one match among these elements between characters other than dj (namely, between the character
bj). If we rearrange all of these gap characters so that they fall directly after dj in both sequences,
as in Figure 9b, then we may lose the match between the character bj, but we will gain the match

54

between the character dj. Moreover, the number of indels remains the same, and all matches in
the remainder of the alignment will remain unchanged. Therefore, this rearranged alignment has
at least as high an objective function value as L0, so the claim holds.

Based on this claim, we will assume, without loss of generality, that for any pair

indel parameter ρ[2] ≥ 0, under the alignment L = A0,ρ[2],0
(i)
1 are matched to dj in S

A0,ρ[2],0, all dj characters in S

(i)
2 .

(cid:16)

S

(i)
(i)
1 , S
2

(cid:17)

(cid:16)

S

(i)
(i)
1 , S
2

(cid:17)

and

returned by the algorithm

Claim C.4. Suppose that the character bj is in S
(cid:17)
if and only if ρ[2] ≤ 1
2j .
in L = A0,ρ[2],0

(i)
(i)
1 , S
2

S

(cid:16)

(i)
1 and S

(i)
2 . The bj characters will be matched

Proof. Since all dj characters are matched in L, in order to match bj, it is necessary to add exactly
2j gap characters: all 2j aj and cj characters must be matched with gap characters. Under the
(cid:16)
objective function mt
S
S
, this one match will be worth the
2jρ[2] penalty if and only if 1 ≥ 2jρ[2], as claimed.

2 , L(cid:48)(cid:17)

2 , L(cid:48)(cid:17)

− ρ[2] · id

(i)
1 , S

(i)
1 , S

(i)

(i)

(cid:16)

We now use Claims C.3 and C.4 to prove that we can shatter the N sequence pairs
(cid:16)

(cid:17)

. . . ,

S

(N )
1

(N )
, S
2

.

(cid:16)

S

(1)
(1)
1 , S
2

(cid:17)

,

Claim C.5. There are k+1

2i−1 − 1 thresholds

1
2(k+1)−2i <

1
2(k+1)−2·2i <

as ρ[2] ranges from 0 to 1, when ρ[2] crosses one of these thresholds, u0,ρ[2],0
from above 3
(cid:16)

4 , or vice versa, beginning with u0,0,0

(i)
(i)
1 , S
2

S

(cid:17)

(cid:17)

(cid:16)

u0,1,0

S

(i)
(i)
1 , S
2

4 to below 3
> 3
4 .

1

2(k+1)−3·2i < · · · < 1
2i such that
(cid:17)
(cid:16)
(i)
(i)
switches
1 , S
2

S

< 3

4 and ending with

Proof. Recall that

(2i−1)
(i)
1 = t
1

S

(2·2i−1)
t
1

(3·2i−1)
t
1

(k−2i−1+1)
· · · t
1

and

(2i−1)
(i)
2 = t
2

(2·2i−1)
t
2

(3·2i−1)
2

t

S

· · · t

(k−2i−1+1)
2

,

(i)
so in S
1
alignment of S
is

and S
(i)
1

(i)
2 , the bj characters are b2i−1, b2·2i−1, b3·2i−1, . . . , bk−2i−1+1. Also, the reference
(k−2i−1+1)
(i)
· · · (cid:96)
is (cid:96)
2
1

and the reference alignment of S

(3·2i−1)
(cid:96)
1

(4·2i−1)
1

(2·2i−1)
1

(2i−1)
1

h

h

(2i−1)
2

(cid:96)

(2·2i−1)
2

(3·2i−1)
(cid:96)
2

(4·2i−1)
h
2

(k−2i−1+1)
· · · (cid:96)
2

.

h

We know that when the indel penalty ρ[2] is equal to zero, all dj characters will be aligned, as
will all bj characters. This means we will correctly align all dj characters and we will correctly
align all bj characters in the
pairs, but we will incorrectly align the bj characters in
(cid:17)

(j)
(j)
1 , h
2
(cid:16)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

h

the

(j)
(j)
1 , (cid:96)
2

(cid:96)

pairs. The number of
(cid:16)

(cid:17)

(j)
(j)
1 , (cid:96)
the number of
2
number of matches equals the following:

pairs is k+1

(cid:96)

h

(j)
(j)
1 , h
2

pairs in this reference alignment is k+1

2i − 1 and
2i . Therefore, the utility of the alignment that maximizes the

u0,0,0

(cid:16)

S

(i)
(i)
1 , S
2

(cid:17)

=

k+1

2i−1 − 1 + k+1
k+1
2i−2 − 2

2i − 1

=

3(k + 1) − 2i+1
4(k + 1) − 2i+1 <

3
4

,

where the ﬁnal inequality holds because 2i+1 ≤ 2(k + 1) < 3(k + 1).

55

Next, suppose we increase ρ[2] to lie in the interval

1
2(k+1)−2i ,
. Since it is no longer
the case that ρ[2] ≤
2(k−2i−1+1) , we know that the bk−2i−1+1 characters will no longer be matched,
and thus we will correctly align this character according to the reference alignment. This means we
(cid:17)
will correctly align all dj characters and we will correctly align all bj characters in the

1
2(k+1)−2·2i

(cid:16)

h

1

(j)
(j)
1 , h
2

(cid:16)

(cid:105)

pairs, but we will incorrectly align all but one of the bj characters in the
the utility of the alignment that maximizes mt(S1, S2, L) − ρ[2] · id(S1, S2, L) is

(cid:96)

(cid:16)

(cid:17)

(j)
(j)
1 , (cid:96)
2

pairs. Therefore,

u0,ρ[2],0

(cid:16)

S

(i)
(i)
1 , S
2

(cid:17)

=

k+1

2i−1 − 1 + k+1
2i
k+1
2i−2 − 2

=

3(k + 1) − 2i
4(k + 1) − 2i+1 >

3
4

,

where the ﬁnal inequality holds because 2i+1 ≤ 2(k + 1) < 4(k + 1).

1

(cid:16)

Next, suppose we increase ρ[2] to lie in the interval

(cid:105)
1
2(k+1)−2·2i ,
. Since it is no longer
the case that ρ[2] ≤
2(k−2·2i−1+1) , we know that the bk−2·2i−1+1 characters will no longer be matched,
and thus we will incorrectly align this character according to the reference alignment. This means
we will correctly align all dj characters and we will correctly align the bj characters in all but one of
(cid:17)
the
pairs. Therefore, the utility of the alignment that maximizes mt(S1, S2, L) − ρ[2] · id(S1, S2, L) is

pairs, but we will incorrectly align all but one of the bj characters in the

1
2(k+1)−3·2i

(j)
(j)
1 , h
h
2

(j)
(j)
1 , (cid:96)
2

(cid:16)

(cid:16)

(cid:17)

(cid:96)

u0,ρ[2],0

(cid:16)

S

(i)
(i)
1 , S
2

(cid:17)

=

k+1

2i−1 − 1 + k+1
2i
k+1
2i−2 − 2

>

3
4

.

In a similar fashion, every time ρ[2] crosses one of the thresholds
2i , the utility will shift from above 3

2(k+1)−3·2i < · · · < 1

1

4 to below or vice versa, as claimed.

1

2(k+1)−2i <

1

2(k+1)−2·2i <

ness 3

The above claim demonstrates that the N sequence pairs are shattered, each with the wit-
where

(cid:17)

(cid:16)

1
2(k+1)−j2i ,

1
2(k+1)−(j+1)2i

4 . After all, for every i ∈ {2, . . . , N } and every interval
is uniformly above or below 3

(cid:16)

(cid:17)

4 , there exists a subpartition of this interval into

(i)
(i)
u0,ρ[2],0
S
1 , S
2
the two intervals
(cid:18)

1
2(k + 1) − j2i ,

1
2(k + 1) − (2j + 1)2i−1

(cid:19)

and

(cid:18)

1
2(k + 1) − (2j + 1)2i−1 ,
(cid:17)

1
2(k + 1) − (j + 1)2i

(cid:19)

such that in the ﬁrst interval, u0,ρ[2],0
3
4 . Therefore, for any subset T ⊆ [N ], there exists an indel penalty parameter ρ[T ] such that if
4 and if i (cid:54)∈ [T ], then u0,ρ[T ],0
i ∈ [T ], then u0,ρ[T ],0

4 and in the second, u0,ρ[2],0

(i)
(i)
1 , S
2

(i)
(i)
1 , S
2

> 3
4 .

< 3

S

S

S

(cid:17)

(cid:17)

(cid:16)

(cid:16)

(i−1)
S
1

(i−1)
, S
2

< 3

(cid:16)

(i−1)
1

(i−1)
, S
2

(cid:17)

>

(cid:16)

C.1 Tighter guarantees for a structured algorithm subclass: sequence align-

ment using hidden Markov models

While we focused on the aﬃne gap model in the previous section, which was inspired by the results
in Gusﬁeld, Balasubramanian, and Naor [50], the result in Pachter and Sturmfels [81] helps to
provide uniform convergence guarantees for any alignment scoring function that can be modeled as
a hidden Markov model (HMM). A bound on the number of parameter choices that emit distinct
sets of co-optimal alignments in that work is found by taking an algebraic view of the alignment
HMM with d tunable parameters. In fact, the bounds provided can be used to provide guarantees
for many types of HMMs.

56

Lemma C.6. Let (cid:8)Aρ | ρ ∈ Rd(cid:9) be a set of co-optimal-constant algorithms and let u be a utility
function mapping tuples (S1, S2, L) of sequence pairs and alignments to the interval [0, 1]. Let U
be the set of functions U = {uρ : (S1, S2) (cid:55)→ u (S1, S2, Aρ (S1, S2)) | ρ ∈ R} mapping sequence pairs
S1, S2 ∈ Σn to [0, 1]. For some constant c1 > 0, the dual class U ∗ is (cid:0)F , G, c2
1n2d(d−1)/(d+1)(cid:1)-
piecewise decomposable, where G = {ga : U → {0, 1} | a ∈ Rd+1} consists of halfspace indicator
functions ga : uρ (cid:55)→ I{a1ρ[1]+...+adρ[d]<ad+1} and F = {fc : U → R | c ∈ R} consists of constant
functions fc : uρ (cid:55)→ c.

S1,S2 : U → R from the dual class
Proof. Fix a sequence pair S1 and S2 and consider the function u∗
S1,S2(uρ) = uρ(S1, S2). Consider the set of alignments LS1,S2 = {Aρ(S1, S2) | ρ ∈ Rd}.
U ∗, where u∗
There are at most O (cid:0)nd(d−1)/(d+1)(cid:1) sets of co-optimal solutions as we range ρ over Rd [81]. The
remainder of the proof is analogous to that for Lemma 4.2.

Finally the results of Lemma C.6 imply the following pseudo-dimension bound.

Corollary C.7. Let (cid:8)Aρ | ρ ∈ Rd(cid:9) be a set of co-optimal-constant algorithms and let u be a utility
function mapping tuples (S1, S2, L) to [0, H]. Let U be the set of functions
uρ : (S1, S2) (cid:55)→ u (S1, S2, Aρ (S1, S2)) | ρ ∈ Rd(cid:111)
(cid:110)

U =

mapping sequence pairs S1, S2 ∈ Σn to [0, 1]. Then Pdim(U ) = O (cid:0)d2 ln n(cid:1) .

C.2 Additional details about progressive multiple sequence alignment (Sec-

tion 4.1.3)

Deﬁnition C.8. Let (τ1, τ2) be a sequence alignment. The consensus sequence of this alignment
is the sequence σ ∈ Σ∗ where σ[j] is the most-frequent non-gap character in the jth position in the
alignment (breaking ties in a ﬁxed but arbitrary way). For example, the consensus sequence of the
alignment

(cid:21)
(cid:20)A T - C
G - C C

is ATCC when ties are broken in favor of A over G.

Figure 10 illustrates an example of this algorithm in action, and corresponds to the psuedo-code
given in Algorithm 2. The ﬁrst loop matches with Figure 10a, the second and third match with
Figure 10b.

Lemma 4.4. Let G be a guide tree of depth η and let U be the set of functions

U =

(cid:110)
uρ : (S1, . . . , Sκ, G) (cid:55)→ u(cid:0)S1, . . . , Sκ, Mρ(S1, . . . , Sκ, G)(cid:1) | ρ ∈ Rd(cid:111)

.

The dual class U ∗ is
(cid:18)

F , G,

(cid:16)

4nκ (nκ)4nκ+2(cid:17)2dη

4dη+1(cid:19)

-piecewise decomposable,

where G = (cid:8)ga,θ : U → {0, 1} | a ∈ Rd, θ ∈ R(cid:9) consists of halfspace indicator functions ga,θ : uρ (cid:55)→
I{a·ρ≤θ} and F = {fc : U → R | c ∈ R} consists of constant functions fc : uρ (cid:55)→ c.

57

Algorithm 2 Progressive Alignment Algorithm ProgressiveAlignment
Input: Binary guide tree G, pairwise sequence alignment algorithm Aρ

Let v1, . . . , vm be an ordering of the nodes in G from deepest to shallowest, with nodes of the
same depth ordered arbitrarily
for i ∈ {1, . . . , m} do
if vi is a leaf then

(cid:46) Compute the consensus sequences

Set σ(cid:48)

vi to be the leaf’s sequence

else

Let c1 and c2 be the children of vi
Compute the pairwise alignment L(cid:48)
Set σ(cid:48)
set σvm = σ(cid:48)
for i ∈ {m, . . . , 1} do

vi = Aρ
vi to be the consensus sequence of L(cid:48)

vm

(cid:1)

c1, σ(cid:48)
c2

(cid:0)σ(cid:48)
vi (as in Deﬁnition C.8)

(cid:46) note vm is the root of G
(cid:46) Compute the alignment sequences

2) be the alignment sequences computed at vi

if vi is not a leaf then
vi = (τ (cid:48)

1, τ (cid:48)

Let L(cid:48)
Let c1 and c2 be the children of vi
Set σc1 = σc2 = “”
Set k = 0
for j ∈ [|σvi|] do

if σvi[j] = ‘-’ then

Append ‘-’ to the end of both σc1 and σc2

else

Append τ (cid:48)
Append τ (cid:48)
Increment k by 1

1[k] to the end of σc1
2[k] to the end of σc2

for i ∈ {1, . . . , m} do

if vi is a leaf representing sequence Sj then

Set τj = σvi

(cid:46) Construct the ﬁnal alignment

58

(a) A guide tree

(b) Constructing the alignment using the guide tree

Figure 10: This ﬁgure illustrates an example of the progressive sequence alignment algorithm in
action. Figure 10a depicts a completed guide tree. The ﬁve input sequences are represented by the
leaves. Each internal leaf, depicts an alignment of the (consensus) sequences contained in the leaf’s
children. Each internal leaf other than the root also contains the consensus sequence corresponding
to that alignment. Figure 10b illustrates how to extract an alignment of the ﬁve input strings (as
well as the consensus strings) from Figure 10a.

Proof. A key step in the proof of Lemma 4.1 for pairwise alignment shows that for any pair of
sequences S1, S2 ∈ Σn, we can ﬁnd a set H of 4nn4n+2 hyperplanes such that for any pair ρ and ρ(cid:48)
belonging to the same connected component of Rd \ H, we have Aρ(S1, S2) = Aρ(cid:48)(S1, S2). We use
this result to prove the following claim.

Claim C.9. For each node v in the guide tree, there is a set Hv of hyperplanes where for any
connected component R of Rd \ Hv, the alignment and consensus sequence computed by Mρ is ﬁxed
across all ρ ∈ R. Moreover, the size of Hv is bounded as follows:

|Hv| ≤ (cid:96)dheight(v) (cid:16)

(cid:96)4d(cid:17)(dheight(v)−1)/(d−1)

,

where (cid:96) := 4nκ(nκ)4nκ+2.

Before we prove Claim C.9, we remark that the longest consensus sequence computed for any
node v of the guide tree has length at most nκ, which is a bound on the sum of the lengths of the
input sequences.

Proof of Claim C.9. We prove this claim by induction on the guide tree G. The base case corre-
sponds to the leaves of G. On each leaf, the alignment and consensus sequence constructed by Mρ
is constant for all ρ ∈ Rd, since there is only one string to align (i.e., the input string placed at
that leaf). Therefore, the claim holds for the leaves of G. Moving to an internal node v, suppose
that the inductive hypothesis holds for its children v1 and v2. Assume without loss of generality
that height (v1) ≥ height (v2), so that height (v) = height (v1) + 1. Let Hv1 and Hv2 be the sets of
hyperplanes corresponding to the children v1 and v2. By the inductive hypothesis, these sets are
each of size at most

s := (cid:96)dheight(v1) (cid:16)

(cid:96)4d(cid:17)(dheight(v1)−1)/(d−1)

Letting H = Hv1 ∪ Hv2, we are guaranteed that for every connected component of Rd \ H, the
alignment and consensus string computed by Mρ for both children v1 and v2 is constant. Based

59

on work by Buck [18], we know that there are at most (2s + 1)d ≤ (3s)d connected components of
Rd \H. For each region, by the same argument as in the proof of Lemma 4.1, there are an additional
(cid:96) hyperplanes that partition the region into subregions where the outcome of the pairwise merge
at node v is constant. Therefore, there is a set Hv of at most

(cid:96)(3s)d + 2s ≤ (cid:96)(4s)d

(cid:18)
4(cid:96)dheight(v1) (cid:16)

(cid:96)4d(cid:17)(dheight(v1)−1)/(d−1)(cid:19)d

= (cid:96)

= (cid:96)dheight(v1)+1 (cid:16)
= (cid:96)dheight(v1)+1 (cid:16)
= (cid:96)dheight(v) (cid:16)

(cid:96)4d(cid:17)(dheight(v1)+1−d)/(d−1)+1
(cid:96)4d(cid:17)(dheight(v1)+1−1)/(d−1)

(cid:96)4d(cid:17)(dheight(v)−1)/(d−1)

hyperplanes where for every connected component of Rd \ H, the alignment and consensus string
computed by Mρ at v is invariant.

Applying Claim C.9 to the root of the guide tree, the function ρ (cid:55)→ Mρ(S1, . . . , Sκ, G) is

piecewise constant with (cid:96)dheight(G) (cid:0)(cid:96)4d(cid:1)(dheight(G)−1)/(d−1)
then follows from the following chain of inequalities:

linear boundary functions. The lemma

(cid:96)dheight(G) (cid:16)

(cid:96)4d(cid:17)(dheight(G)−1)/(d−1)

(cid:96)4d(cid:17)dheight(G)
4dheight(G)+1

≤ (cid:96)dheight(G) (cid:16)
= (cid:96)2dheight(G)
= (cid:0)4nκ(nκ)4nκ+2(cid:1)2dheight(G)
4nκ (nκ)4nκ+2(cid:17)2dheight(G)
4nκ (nκ)4nκ+2(cid:17)2dη

4dη+1

≤

=

(cid:16)

(cid:16)

.

4dheight(G)+1

4dheight(G)+1

60

