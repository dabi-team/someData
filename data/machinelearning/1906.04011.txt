June 5, 2019 

Visual Backpropagation 

Roy S. Freedman1 

Abstract 
We  show  how  a  declarative  functional  programming  specification  of  backpropagation  yields  a 
visual  and  transparent  implementation  within  spreadsheets.    We  call  our  method  “Visual 
Backpropagation.”  This  backpropagation  implementation  exploits  array  worksheet  formulas, 
manual calculation, and has a sequential order of computation similar to the processing of a systolic 
array.   The implementation uses no hidden macros nor user-defined functions; there are no loops, 
assignment statements, or links to any procedural programs written in conventional languages.  As 
an illustration, we compare a Visual Backpropagation solution to a Tensorflow (Python) solution 
on a standard regression problem.  

Keywords:  machine  learning,  neural  networks,  backpropagation,  functional  programming, 
spreadsheets, Excel  

Introduction  

1 
Neural  networks  are  nonlinear  formulas  that  approximate  functions.    Backpropagation  [1]  is  a 
machine learning algorithm that creates these approximation formulas iteratively.  The iterative 
approach has been likened to finding the top of a mountain in a mountain range: at every iteration, 
we take a step in the direction of the steepest slope.  We stop taking steps when the approximation 
is “good enough.” Mathematicians call this approach “gradient search.” 
Backpropagation is a proven and popular machine learning technique since its development over 
thirty  years  ago.    According  to  LeCun  et  al  [2],  backpropagation  is  popular  because  it  is 
“conceptionally simple, computationally efficient, and because it often works.”  Bishop [3, p.246] 
concurs:  “One of the most important aspects of backpropagation is its computational efficiency.”  
Backpropagation  has  been  programmed  in  C,  C++,  Fortran,  Java,  Julia,  Python,  R,  Visual 
Basic, and other procedural (imperative) languages. Procedural programs implementations rely on 
constructs like loops and assignment statements.  Programs typically result in text that looks quite 
different  and  more  complicated  than  the  actual  mathematical  specification.    (My  favorite  C 
implementation of backpropagation is the 17 pages of C code in Pao [4, Appendix A]).  In general, 
these programs are not for neophytes. 
  Getting a backpropagation-based machine learning experiment to actually work (meaning – in 
a technical sense – to converge to a meaningful solution) has been described as more than an art 
than a science [2].  The reason for this is the number of design and parameter choices that need to 
be made.  These choices include specifying the  nature of the approximating function (topology, 
activation functions); specifying properties of the iteration (learning rates); specifying techniques 
that assist convergence (data scaling); specifying how the answer will be evaluated (data selection 
for  in-sample  training  and  out-sample  tuning  and  cross-validation);  and,  finally,  specifying  the 

1  Roy  S.  Freedman  is  with  Inductive  Solutions,  Inc.,  New  York,  NY  10280  and  with  the  Department  of 
Finance and Risk Engineering, New York University Tandon School of Engineering, Brooklyn NY 11201.  
Email: roy@inductive.com.  

 
 
 
 
                                                      
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

criteria  used  to  determine  when  to  stop  (when  the  approximation  is  “good  enough”  based  on 
determining average errors). 

Current  backpropagation  implementations  –  especially  those  used  for  academic  and 
professional  education  –  rely  on  open  source  software  languages  and  libraries.    All  design  and 
parameter choices need to be programmed.  Some implementations graft a front-end user interface 
– such as a spreadsheet – to a hidden backpropagation back-end engine.  The interface manages 
data collection and parameter selection via an Application Programming Interface, Dynamic Link 
Library, or other inter-process communication scheme.  Etheridge and Brooks list a number of such 
vendors in their 1994 paper [5].   

The motivation of this paper is to explain the purpose and use of backpropagation to students 
and professionals in finance by using spreadsheets as a transparent backpropagation engine.  The 
target audience includes non-experts in machine learning and artificial intelligence; yet, as finance 
professionals,  they  are  familiar  with  linear  regression  and  spreadsheets.    We  show  how  the 
backpropagation  algorithm  can  be  implemented  in  Excel  spreadsheets  using  Excel  worksheet 
functions like array and matrix multiplication.  We call our method “Visual Backpropagation.”  We 
use “pure Excel” – there are no dynamic link libraries to C, C++, C#, Java, Python, Visual Basic 
for Applications (VBA), or any other language.  There are no macros no user-defined functions and 
no  subroutines;  there  are  no  loops,  assignment  statements,  or  any  procedural  programming 
constructs.   

In Section 2 we first review the array-based and spreadsheet-influenced notation used in Visual 
Backpropagation.  We also show how neural network nonlinear formulas generalize the formulas 
used in linear regression.  Section 3 reviews the semantics of spreadsheet computation – especially 
computation  under  the  “manual  mode  of  calculation”  and  specifies  our  visual  backpropagation 
formulas.  Here we use a simple problem (the nonlinearly separable “exclusive or” together with 
“and”) as an example for a 2-hidden layer learning system (2-2-2-1 topology).  (Additional details 
associated with manual calculation are in the Appendix.)  Section 4 shows Visual Backpropagation 
solving a practical problem (the Auto Mpg problem found in the UCI Machine Learning repository 
[6]).  This example is used in the Tensorflow regression tutorial [7]).  Finally, Section 5 summarizes 
our work and offers some proposals for future work and extensions.   

Since our approach is visual, we provide many figures showing screen images that illustrate 
the Visual Backpropagation method.  We preface figure captions by section number. The actual 
working  spreadsheets  (content  covered  by  the  Creative  Commons  Attribution  3.0  License,  and 
formulas by the Apache 2.0 License) are posted on a website [8].   

Why spreadsheets?   
Spreadsheets are the lingua franca of the computing world of accounting, economics, finance, and 
basic  brute-force  data  collecting  for  data  sets  of  less  than  a  million  records.    For  most  people, 
spreadsheets are easy to learn and easy to use.  There are dozens of spreadsheet implementations 
(some  are  open  source)  but  even  after  several  decades,  Microsoft  Excel  is  the  most  popular 
spreadsheet platform.  Walsh [9] summarizes seven reasons why at least half a billion people use 
Excel.    Despite  this,  the  spreadsheet  programming  paradigm  has  been  ignored  by  academic 
computer  science  and  many  programming  professionals:    “Real  programmers  don't  use 
spreadsheets” [10].   One reason is that  

The neglect of spreadsheets in the programming language literature might be caused by 
an aversion to a language created by students who had the bad taste of using their ideas 
to earn money instead of starting a Computing Science career that might have led to a 
Turing  award.  I  don't  think,  however,  that  computer  language  specialists  harbor  such 
spiteful  thoughts,  and  therefore,  I  suggest  that  spreadsheets  are  intrinsically 
uninteresting. Presumably, many others who have looked at spreadsheets have come to 
2 

 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

the same conclusion, and such pedestrian results have never been seen fit for publication 
(or at least, they have not been accepted by journal or conference referees)[10]. 

Nevertheless, the same author admits 

In spreadsheets the ideas from two powerful languages, LISP and APL are combined. The 
acceptance of spreadsheets by non-programmers shows that functional programming and the 
use of arrays as the only data structure are concepts that are easy to understand [10]. 

The  academic  disdain  for  spreadsheets  continues.    Indeed,  Fouhey  and  Maturana  [11]  posted  a 
“joke paper” that mocks a fake use of Excel in connection with machine learning.  But Walsh [9] 
concludes: 

…mention Excel to techies and it’s often dismissed with a sniff.  However, somewhat “like 
the love that dare not speak its name”, the vast majority of users in the business world use 
Excel and practically every system has a button that says “Give it to me in Excel”. 

This situation is changing.  In 2014, in a serious and rigorous academic monograph of spreadsheets, 
Sestoff  [12]  showed  that  spreadsheets  really  are  dynamically  typed  functional  programs.  
Functional  programs  are  declarative:  these  programs  do  not  rely  on  procedural  constructs.  
Declarative  functional  programs  look  like  mathematical  specifications.    In  a  sense,  spreadsheet 
programming consists of (1) selecting and naming ranges of cells, and (2) entering (or re-entering) 
values  and  formulas in  cells.    Program  testing  consists  of  tracing  the  propagation  of  values  via 
formula computation from cell to cell or region to region.  Spreadsheet programming declaratively 
specifies  names  and  computational  expressions.    In  contrast,  programming  in  conventional 
imperative languages specifies a schedule of control flow with procedural statements. 

2  Array Notations for Backpropagation 
This section specifies our notation for backpropagation in the context of linear algebra (i.e., matrix 
and  tensor  operations)  and  show  how  these  operations  have  an  equivalence  to  Excel  array 
operations.  We do not derive the backpropagation algorithm; there are plenty of nice derivations, 
in particular, see Bishop [3] or Pao [4]; Efron and Hastie [13, p. 356-9] use a vector notation almost 
similar to ours.  We re-specify the backpropagation algorithm in a form suitable for spreadsheet 
formula computation.   

We assume the reader is familiar with basic spreadsheet concepts:   

  How spreadsheets reference cells, and how spreadsheets define named cell regions; 
  How formulas, array formulas, and built-in functions are inserted into cells;  
  How formula-derived values propagate computation across cells; 
  How computation is achieved via calculation options (automatic vs. manual). 

2.1  Review of Array Operations 
Array operations originate from the mathematics of vectors, matrices, and tensors.  We show how 
the many operations on these mathematical objects are modeled by Excel array operations.  We use 
our  formalism  to  map  backpropagation  into  a  tensor  form  and  ultimately  to  Excel  array 
equivalences – as copy-and-paste array formulas.    

We first start with a notation for naming.  Spreadsheet arrays are rectangular regions of cells 
(or a single cell).  Denote ranges by bold type.  Here is a formal definition of two arrays: one has 

3 

 
 
 
 
 
 
 
 
 
  
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

three values in its rows and the other has two values in its columns (sometimes called a row array 
and column array, respectively): 

 and 

 . 

In Excel spreadsheet notation, instead of bold type  for naming arrays, we use plain type (frequently 
with an underscore to make sure our name is not confused with a row or a column location; for 
example, the name “a2” denotes the cell in the second row and first column of a worksheet).  Many 
spreadsheets have a Define Name command that binds names to spreadsheet arrays.  For example, 
the Define Name in Excel defines array a_ via 

This defines array a_ as a place on the worksheet. 

Extend these definitions to names of components by specifying values.  We use “:=” to denote 
“evaluates to” (for names and formulas) and “=” to denote “the same as” (when checking equality 
of  expressions).    The  individual  cells  in  an  array  are  called  components  (indicated  in  non-bold 
type).  The standard convention is to use subscripts to denote row-column locations.  For example,   
is the component in the first row and second column; 
first column: 

 is the component in the second row and 

 and 

   . 

Excel does this with the OFFSET worksheet function: 

=OFFSET(a_, 0,1,1,1)  := 

To avoid ambiguity, the definition includes the number of rows and the number of columns via 
.  Frequently we call the number of rows (of a column array) or number of columns 

(of a row array) the dimension. 

The transpose of an array is an operation that switches the rows and columns: 

4 

:(13)a:(21)b12a111213:(13):aaaa1121:(21):bbb12a()rowcolumn111112131213:(13)::(31)::TTTaaaaaaaa 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

The transpose operations changes an n-dimensional row vector to an n-dimensional column vector 

(and  vice  versa!).    In  notational  shorthand: 

  .    Excel  does  this  with  the 

TRANSPOSE worksheet function.  

We  can  specify  a  new  column  array  by  appending  a  row to  an  existing  column  array.    For 
 that appends a row below the last 

 is new column vector 

example, an extension of vector 
row of 

.  Suppose this row has value 1.  Specify this extension by:   

Note that there is nothing special about 1: the added value can be any number. Similarly, denote a 
restriction of column array 
 by removing the last column; the restriction of a row array removes 
the last row.  For example: 

    . 

In this sense, the 
 specification becomes an operator for a restriction.  (In linear 
algebra, single column arrays are called vectors; restrictions form a subspace of the parent vector 
space.) 

Given row array 

 and column array 

 , the tensor (or outer) product 

creates the following rectangular array: 

Note that an array of one row and three columns combined with an array of two rows and one 
column yields an array of two rows and three columns.  The tensor product of an n-dimensional 
row array with an m-dimensional column array is an array with m-rows and n-columns: a notational 
shorthand is 

.  A rectangular array is also called a matrix. 

 cell region 

In Excel, the tensor product is supported by Excel array multiplication.  For example, let’s first 
 as C4:E4 
use the Define Name command (on the Formulas tab) to define the 
  as G4:G5.  Populate sample values {1,2,3} and {4,5} into these regions.  
and 
 region I4:K5 (via <control-shift-enter>). Excel inserts 
Enter array formula 
 (unfortunately these brackets do not 
the curly brackets to show it is an array formula 
show when printed).  Array multiplication yields the correct answer for tensor product (see Figure 
2.1). 

 cell region 

 in the 

5 

(1):(1):Tnnb*bb11112121:(21)*:(31):::111bbbbbba1112:(13):(12)::(12):*:(21):aaaabb()rowcolumn:(13)a:(21)bab111111111213111213212121131111111211132111211221:(23):::bbbaaaaaabbbabababababababbbb(1):(1):():nmmn(13)a_(21)b_=a_*b_(23)={a_*b_} 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Figure 2.1.  Tensor product of two arrays: 
Array 

is defined as C4:E4 and array 

 corresponds to 

.   

as G4:G5. Top: Values.  Bottom: Formulas. 

Mathematical  notations  are  more  forgiving  than  programming  languages.    Let’s  extend  the 
mathematical notation and introduce an Excel-like spreadsheet notation for specifying arrays.  The 
following specifies the location of arrays 

 and 

 : 

Given a name, define a restriction: 

Here is a notation for defining array formulas.  An example: the (un-named) region I4:K5 contains 
the array (tensor) product of 

 and 

 : 

Backpropagation uses other array operations.  Matrix addition for two matrices having the same 

number of rows and columns is just array addition in Excel.  Mathematically: 

 and   

  then 

  . 

Excel implements the Hadamard product 
and columns as array multiplication: 

 for two arrays having the same number of rows 

6 

=a_*b_aba_b_a_b_a_:(1×3)//C4:E4b_:(2×1)//G4:G5a_:(1×2)//C4:D4a_b_      :(2×3)//I4:K5   :=   a_*b_111213212223:wwwwwww111213212223:vvvvvvv111112121313212122222323:wvwwwvwvwvwvwvwv 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

We define matrix addition and Hadamard product for restrictions as well: for example: 

  . 

 and 

  . 

How does an Excel spreadsheet tell the difference between Hadamard product and tensor 
product?  By context: Hadamard product 
number of rows and columns. 

  is defined for two arrays having the same 

Another  important  operation  used  throughout  the  neural  network  and  machine  learning 
literature is applying a real-valued function to every component in an array.  Examples of real-
valued functions that are built-in Excel worksheet functions are 
, 
.    For  example,  given  array 
and  the  Heaviside  step  function 

,

 defined above, define 

  as: 

For 

: 

Applying a functions to an array is called a map in the functional programming literature: this is 
supported  with  spreadsheet  array  formulas.    See  Figure  2.2  for  Excel  examples.    Note  that  a 
functional map is a special case of a vector-valued function, which specifies, in general, a function 
for each component.  An example of a vector-valued function is 

(The vector-valued function is an array so we represent it in bold type.) The functional maps of 
real-valued  functions  that  are  used  in  backpropagation  are  called  “activation  functions.”  Some 
typical activation functions (and their first derivatives) are shown in the table in Figure 2.3.  See 
[14] for a nice table of activation functions.   

7 

111213111213111112121313212223212223212122222323:wwwvvvwvwvwvwwwvvvwvwvwvwv21111112212121:(21):bbbbbbbb1112131112131111121221222321222321212222:(22)::(22):wwwvvvwvwvwwwvvvwvwvwv=a_*b_()exp()fxx()tanh()fxx()():(0,1,0)fxuxifx:(23)wfw111213212223()()():(23)::(23):(23):()()()fwfwfwfffwfwfwww()tanh()fxx112131tanh():(31):tanh:(31):tanh()tanh()TTafaaaa11112221331()exp():(31)():tanh()()(0,1,0)TTTTfaafafifaafaaa 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Figure  2.2.    Spreadsheet  showing  values  and  formulas  for  array  addition 

,  Hadamard 

product 

 , array function mapping 

 .  Array 

  is specified with Excel Define 

Name command as B11:D12; array 

 is specified with Excel Define Name command as F11:H12.   

Note:   

    corresponds  to   

; 

    corresponds  to 

  ;     

corresponds to 

 . Top: Values.  Bottom: Formulas. 

Function: Math/Excel 

Name 

Derivative 

Hyperbolic tangent 

Logistic sigmoid 

Identity function 
(ID) 

Rectified Linear Unit 
(RELU) 

Figure 2.3.  Some activation functions. 

The Heaviside unit step function helps specify the “rectified linear unit” or RELU function.   
Note that that the derivative of the Heaviside function is not defined for  

. 

8 

w_+_vw_*_vexp(v_)w_v_=w_+v_wv=w_*v_wv=exp(v_)exp()v():tanh()fzz=tanh(z)2'()1()fzfz():1/(1exp())fzz=1/(1+exp(z))'()()*(1())fzfzfz():fzz=z'()1fz():()fzzuz=if(z>0,z,0)'()()fzuz=if(z>0,1,0)0z 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Backpropagation uses matrix multiplication.  To review: given 

  array 

  and  

array 

, the matrix product 

 is the  array defined by 

Matrix  product  is  supported  in  Excel  by  the  MMULT  worksheet  function.    In  general,  given 
 array with each 
.   

component given by a sum of products.  A shorthand for this rule is 

, the matrix product 

  array 

 array 

  and  

 is an 

We combine matrix multiply with the 

 operator for a restriction.  For example: 

In Excel, restrictions supported by selecting a partial range for the formula.  See Figure 2.4. 

Figure 2.4.  Matrix multiplication for arrays 

 and 

(these arrays were specified in Figures 2.1 

and 2.3.)  Note the three different restrictions for the matrix  multiplication.   The “usual” result  is 

  ;  however  we  also  show  the 

  and 

restrictions.    Note:   

corresponds to

. 

9 

(23)w(31)bwb111112131111122113312121222321112221233131:bwwwwbwbwbbwwwwbwbwbbwb()nmw(1)mbwb(1)n:():(1):(1)nmmn()rowcolumn111213111111122113212223212111222123:(21):0111wwwbwbwbwwwwbwbwbww_a_(31)(21)(11)=MMult(w_1,x)1wx 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

2.2  Generalizing Linear Regression  
Neural network nonlinear formulas generalize the formulas used in linear regression.  Let’s first 
review some linear regression formulas.  Given the following arrays: 

; 

; 

;  

a linear formula that relates 

 to 

is: 

Mathematicians call this linear formula an affine transformation: it transforms 

 vectors to 

 vectors.  (This formula generalizes the equation for a line:

. )  

Here is an alternative representation.  Recall that the result of the matrix multiply 

  .    Extend the 

  array 

  is a 
  to  rectangular 

column  array  via  rule
array 

  with an extra column whose values are the column vector 

: 

,

. 

 with 

 is sometimes called a weight matrix.  Now define column vector 

 as an extension of 

Array 
: 

 with 

. 

Then an alternative representation of a linear formula is: 

In this representation, a linear formula that relates 
with 
given a set of array pairs: 

:  we save a matrix addition operation by subsuming 

 to 

 is just the matrix multiply of 

 in 

.  Next, suppose we are 

Or equivalently, 

10 

1:(1):nxnxx1:(1):moutmoutout1:(1):mbmbb:()mnAxout:(1):moutAxb(1)n(1)myaxbAx:():(1):(1)mnnmmnAwb:((1))::((1))mnmnwAb,1imiwb1,2,...,inwinpx1:((1)1)::11nxnxxinp11ninp:::1xoutwinpAbAxbinpoutwinpbw(1),(1),(2),(2),,(),(),(),()ssSSxtargxtargxtargxtarg(1),(1),,(),()SSinptarginptarg 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

with 

  VISUAL BACKPROPAGATION 

 and 

Each  column  array  pair  is  a  called  a  sample  record  or  a  training  record.    For  some  arbitrary 

 array  

, compute the output as a linear formula: 

 for 

 . 

The linear regression problem is:  Finds a linear formula (array 

 – computes a vector 
the sample input vectors 
the corresponding target vector 

) that – given an input vector 
  that is optimal in the following sense: if the input vector is one of 
 should be “approximately equal to” 

,  then the output vector 
.  In other words: 

The error for each sample record is 

For a perfect approximation, the error for each sample should be zero.  In practice this is rarely the 
case due to improper observation, “noise,” or computational faults.  The same inputs could even 
produce  different  targets  at  different  times:  this  can  be  due  to  accounting  errors  or  due  to  a 
misspecification that mistakenly neglected “hidden” inputs.  In any case, a common criteria is find 
  that minimizes an average error over all samples.  Because of this, the output might not 
the 

be an exact match: it could correspond to an average of the corresponding targets. 

Here are three methods that find 

: 

Method I.  Randomly guess many values for every component of array 
guess by 
average error formula computes the mean square error over the sample: 

 – denote each random 
 – and with this guess matrix, compute the average errors over the entire sample.  One 

 for each guess 

. 

If we find this error “good enough” then stop the procedure and use the matrix  that had the smallest 
 ; otherwise continue guessing (viz, in machine learning: 
average error as “the solution” for 

11 

1()()():((1)1)::1()1nxsssnxsxinp1()():(1):()mtargssmtargstarg(1)mnw1()():(1)::()()moutssmsoutsoutwinp1,2,,sSwinpout()sinpout()starg():()()sssoutwinptarg()()sstargoutoptwoptwww211err:()(;)SsssStargoutwwoptw 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

“training”).  It can be shown using results from probability (Monte Carlo theory), that if we keep 
guessing long enough (and the initial guess is not an unlucky one), we will closely approximate the 
  that minimizes err over all possible guesses.  (Mathematically: the weights converge 
optimal 
or  are  “trained”  to  approximate  the  optimum  solution.)    Method  I  is  iterative:  it  requires  many 
repetitions (iterations) of the same steps in order to achieve an answer.  Sometimes the process of 
iteration is called “training” or “learning.” 

and then compute the error of sample number r: 

.  Next, we pick an 
Method II.  Guess just one starting value for every component of array 
arbitrary record in the sample (either in order, shuffled order, or completely at random).  Call this 
sample  record  number  r.      Using  the  current  array 
to 
compute 
.  Maintain a 
running average of errors over the entire sample. If we find the running average of the errors “good 
enough,” then stop.  Otherwise, perform a tensor product of this error with the transpose of the 
 ).  Multiply this by a small positive number 
input.  (Recall : 
 to create a 
(traditionally called 
new updated array.  The result is the new weight matrix:  

 – eta – usually less than one).  Add this to the current array 

and  the  selected  record  r,  use 

 ;   

Repeat the  process:  set 

and  pick  another  sample.    Recompute  the  error  with this 
current array 
.  If we the running average of the errors is “good enough,” then stop.  Otherwise 
continue the iteration.  This method, due to Widrow [15] (dating from 1959) is sometimes called 
the delta rule.  It can be shown (using differential calculus) that the updated iterates will closely 
 that minimizes err over all possible guesses – provided that we wait 
approximate the optimal 

long enough and the initial guess is not an unlucky one.  Method II is also iterative: at each iteration, 
we are mathematically taking a small step along the gradient – the direction of steepest decrease of 
the errors (visualize the error surface as a bowl).  Backpropagation is a generalization of Method 
II.  

Method III.  This method (called the classical method of least squares) is at least 200 years old.   
It is the basis of Excel’s LINEST and TREND formulas.  The first step is to consolidate all inputs 
and targets in their own arrays: 

 has, as columns, all input samples 

Array 
has, as columns, all corresponding target samples 
differential calculus), the optimal 

 that minimizes err over all possible guesses is: 

;  array 

 .  It can be shown (using 

12 

optwww()rinp()rout()()rrtargout(1(1)):(1):((1)):nmmnwnew:()()()Tssswwinptargoutnew:wwwoptw:((1)):(1):(1)1(2):(1)1():(1)1nSnnSnINPUTSinpinpinp:():(1):1(2):1():1mSmmSmTARGETStargtargtargINPUTS():(1)1sninpTARGETS():1smtargoptw 
 
 
 
  
 
 
 
 
 
 
 
 
  
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

This solution uses the matrix inverse operation: for matrix 
Matrix inverse is a worksheet function supported by Excel.  The copy-and-paste array formula for 
the regression weights is 

 its inverse is denoted by 

  .  

=TRANSPOSE (   MMULT( MINVERSE( MMULT(INPUTS,TRANSPOSE(INPUTS))),  
MMULT(INPUTS,TRANSPOSE(TARGETS) )  )  ) 

The advantage of Method III is that it is not an iterative approximation: it is exact (as long as the 
data is “well-conditioned” in the numerical analysis sense).   Figure 2.5 shows a simple spreadsheet 
implementation of a regression problem. 

Figure 2.5.  Simple regression problem: create a formula from (x1,x2) that computes for output 
(targ1, targ2).  Left: Problem specification: inputs and target values (Samples); INPUTS array; 
TARGETS array; optimum linear regression weights via Method III. Right: computed outputs.   
The formula for the optimal weights in cells C18:E19 is  

=TRANSPOSE(   MMULT(MINVERSE ( MMULT( C9:F11,TRANSPOSE( C9:F11))),  
MMULT(C9:F11,TRANSPOSE(C14:F15) )  )  )  . 

13 

1opt:((1))::((1)):((1)):((1)):()TTTmnnSSnnSSmwINPUTSINPUTSINPUTSTARGETSA1A 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Note that the sum of the squared errors are 1 (for targ1) and 0.25 (for targ2). That these errors 
are not zero (or even small) is due to the nature of the problem.  The first target is the “exclusive 
or” function (returns 1 if only one input is 1 but not both; otherwise return 0); the second is the 
“and” function (returns 1 if both inputs are 1; otherwise return 0).   We can show that no linear 
formula exists that exactly computes these two functions (they are not “linearly separable”). 

Can we make the regression more powerful in that we can make better approximations than the 
linear one?  One way is to simply include more inputs (and arrange these inputs to be functions of 
other inputs).  This is the functional link approach advocated by Pao [4].  Another way is to consider 
making the linear formula into a network of linear formulas: here, the output of one linear formula 
is the input to a second linear formula.  Let’s see how this works for a network with the first output 
feeding  the  second  output  and  the  second  output  feeding  the  third  output  (a  “three-layered” 
network): 

; 

; 

;   

It looks like we have complicated things.  But we really have not.  Since 

This implies that this three-layer linear network can be reduced to a single-layer standard linear 
regression.  The single layer has matrix 

and column vector 

 given by: 

 and 

Linear formulas of linear formulas are linear formulas.  However, consider a simple extension of 
the linear world to the nonlinear world.  Select any three activation functions (see Figure 2.3) as a 
map.  Such a three-layered (nonlinear) network is specified by: 

Here it again looks like we have complicated things.  And we really have, since nonlinear 
functions of nonlinear functions do not in general reduce to known simpler forms: 

14 

1111:(1):moutAxb1111:();:(1)mnmAb22212:(1):moutAoutb22122:();:(1)mmmAb3323::(1):moutoutAoutb323:();:(1)mmmAb323321233211233212123321213223:::::outAoutbAAoutbbAAAxbbbAAAxAbbbAAAxAbAAbbAb321:AAAA213223:bAbAAbb11111:(1):mfoutAxb222212:(1):mfoutAoutb33323:(1):mfoutoutAoutb 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

For linear networks additional complexity makes no difference.  Nonlinearity is where complexity 
creates an emergent property: nonlinear neural network regression frequently outperforms linear 
regression: the average errors for nonlinear neural networks are smaller than the linear regression 
case.  They can also solve problems that are not linearly separable. 

2.3  Nonlinear Formulas for Feed-Forward Networks 
Specify an integer 
): we need to account 
for q different weight matrices and q different output column vectors.  As in the linear case, the 
training data set consists of a set of records of input-outputs.  Suppose we are given a weight matrix  
  from the inputs, weights, and 

 (usually called the number of layers, where  

; we compute the first output

activation function 

 .  The specifications are: 

; 

; 

Vector 

 has 

  rows; the restriction is 

.  Array  

 specifies a first layer.  Continue the accounting by computing a second output from the first 

output, with a second weight matrix 

 and second activation function 

 : 

In general 

;  

;  

(We can set

.)  Form the final output vector 

 where 

: 

15 

33233322123332211123:::ffffffoutAoutbAAoutbbAAAxbbbq2,3,...q11:((1))mnw11:((1)1)mout1f1:(1):((1)1):11nxnnxxinp11:((1))mnw11111():(1):((1)1):1fmmwinpout1out1(1)m11111:(1)():(1)mfmoutwinp1out221:((1))mmw2f221:(1)mmw221222():(1):((1)1):1fmmwoutout1:((1))hhhmmw1():(1):((1)1)1hhhhhhfmmwoutout0:outinp:(1)qmout:qmm1:(1)():(1)qqqqqqmfmoutoutwout 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Note the final output is not extended to an extra dimension. We specify this computational topology 
is the number of sample inputs and 
via dash notation: 

.  Here, 

is the number of outputs.   For 

then the 

values refer to hidden layers: 

refers to the 

output dimension (“number of neurons”) in the first hidden layer, 

 is the number of neurons in 

hidden layer h, and so on.  The final output vector 

depends on 

,

 ,…,

.  All 

these must be previously computed in order of dependency.  This arrangement specifies a “feed-
forward network.”  Consequently, given an input vector and a sequence of weight matrices, we 
compute  the  output  by  forward-propagating  the  subsequent  outputs  as  inputs  to  the  next  layer.  
Knowledge of input 

 are sufficient to determine 

, and functions 

,  weights 

.   

Figure 2.6 shows a 2-3-2 network in our representation.  We can visualize the network topology 
by using the Excel Trace Dependents utility.  Any change in the values in cells B5:B7 propagates 
to the final outputs in cells K5:K6.  The array specifications are: 

;   

;   

Figure 2.6. A 2-3-2 feedforward topology for that solves the regression problem 
specified in Figure 2.5.  Top:  Worksheet values view.  Middle:  Recreation of the 
neural network view via Excel’s Trace Dependents utility.  Bottom:  Formula view. 

16 

121hqqnmmmmmnm2qhm1mhmout1qout2qout1outinphwhfout1:((21)1):0//5:71BBinp1:(3(21))//5:7CEw11tanh():(31):((31)1)://5:81FFwinpout2:(2(31))//5:6GJw221:(21):tanh():(21)//5:6KKoutoutwout 
 
 
   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

2.4  Formulas for Backpropagation 
For linear formulas, the squared error surface for the weights involves squaring the linear outputs.  
The  resultant  error  surface  is  a  quadratic  formula  of  the  outputs:  it  corresponds  to  a  mult-
dimensional bowl. We pick a point and take iterative steps in the direction of the steepest downward 
slope.  Slopes of quadratics are easy to compute: that is how Method II works in finding optimal 
weights.   

For  nonlinear formulas  based  on  the functional  mapping  of  activation  functions,  we  need a 

 and its first derivative. (Recall the first derivative of a function 

representation of both
).  Backpropagation requires 
corresponds to the slope of the tangent of the function at the point 
first derivative of our activation functions to implement gradient descent.  At layer h, the derivative 
function is evaluated at column array 

: 

 . 

Figure 2.7 shows array expressions for these derivatives applied to to array 
that  the  result  uses  Hadamard  product 
complicated, in a spreadsheet this reduces to array multiplication. 

.  Note 
.    Even  though  this  looks  mathematically  very 

Function: Math/Excel 

Name 

Derivative 

Hyperbolic tangent 

Logistic sigmoid 

Identity function 

Rectified Linear Unit (RELU) 

Figure 2.7.  Derivatives of the activation functions listed in Figure 2.3 evaluated at column array 
.  Note that  denotes the h-dimensional unity column vector with all columns 

assigned to 1. 

The identity function 

 is actually the activation function for the linear formula used 
in Method II.  As we observed, a multiple-layer linear network does not improve matters over linear 
regression.  On the other hand, the other nonlinear functions listed in Figure 2.3 and 2.7 induce an 
error surface that generally is not a simple bowl: in general it is a bowl with humps and valleys.  
This means that a steepest descent gradient search on the error surface is not guaranteed to find an 
absolute smallest error.  No matter: we stop when the error is “good enough”; otherwise restart with 
another guess. 

17 

()fz'()fzz1hhzwout1'():(1)hhhhfmzwoutz1hhzwout1():(1)hhhhhfmwoutout1'():(1)hhhhfmzwoutz()tanh()fzz=tanh(z)1tanh():(1)hhhhmwoutout():(1)hhhm1outout()1/(1exp())fzz=1/(1+exp(z))11/(1exp()):(1)hhhhmwoutout:(1)hhhmout1out()fzz=z1:(1)hhhhmwoutout:(1)hm1()()fzzuz=if(z>0,z,0)11()():(1)hhhhhumwoutwout():(1)hhumout1hhzwout1()fzz 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

In backpropagation, we compute a series of column arrays 

activation function error gradients (slopes).  For 
per output (or what is the same: one delta per weight matrix). 

(called deltas) that are related to 
layers we need to account for  deltas, one delta 

We proceed backwards, from the final output layer.  Define the delta as the array difference 
(the  “error”)  multiplied  (via  Hadamard  product)  by  the  derivative  (slope)  of  the 

activation function.  The result is: 

For other layers, 

is computed from the previously computed

: errors computed previously 

are “back-propagated” from “outer” layers to “inner” layers.  So for 

: 

Note that the matrix multiplication of the transpose of the weight matrix involves a restriction.  The 

weight 

matrix  multiplied  by  delta 

should yield a column vector specified by 

; we take the restriction 

.  For 

example: consider the tanh activation function.  At final output layer: 

For 

Figure 2.8 shows the target and last two layers  for a 2-2-2-2 network using tanh.  Here 

;

.   

  . 

Figure 2.8.  Computation of deltas  and 

.  Top: values; bottom: formulas.  Note: 

. 

18 

δqqtargout1:(1)::(1):'():(1):qqqqqqqmmfmzwoutδ=δtargoutzhδ1hδ1,2,,2,1hqq111:(1)::(1)'():(1);1,,1hhThhhhhhhmmfmhqzwoutδwδz111:((1)):(1)Thhhhhmmmmw11:(1)hhmδ(1)1hm1hm:(1)::(1):qhhqmmδtargout1outout1,2,,2,1hqq11:(1)::(1):(1)Thhhhhhhhmmmδwδ1outout3q1232;2;2;2nmmmδ2δ2322:;:Tδtargout1outoutδwδ1outout 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

We use the deltas to update the weights iteratively.  As in Method II, let 

 (eta) be a small real 

number: usually 

.  We need to account for a set of q such etas:

, one for 

each weight matrix.  Given a set of q weight matrices 

 that already have values 

(they all might initially have random values).  Then given an input sample, we compute the output 
.  We use the errors between 
(and all intermediary outputs) and compare it to the target 
target  and  output to  update  the  weights  using the tensor  product 
of  the  current  delta  and the 
previous output (which is actually the input to the current output).  The backpropagation formula 
is: 

Note that the arrays are dimensionally consistent: we have specifications  

 and 

As we showed in the previously, the tensor product of an n-dimensional row vector with an m-
dimensional column vector is a matrix with m-rows and n-columns: in our case we have  

This matrix has the same rows and columns as 
weights in last two layers for the 2-2-2-2 network. 

.  Figure 2.9 shows the array 

.   

Figure 2.9.  Backpropagation updates.    
Top:  Formula View for 

.  Bottom:  Formula View for 

19 

011,...,,,hq1,...,...,hqwwwouttarg111111::;:TqqqqqThhhhhThqwwoutδwwoutδwwinpδ111:((1)1):1(1)Thhhmmout:(1)hhmδ11(1(1):_(1):_(1):_hhhhmmmm1:((1))hhhmmw3w2w 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Note:  we can use the same 

 for all layers, so that 

.  Or we can use an 

individual 
For layer h: 

for each cell based on Hadamard multiplication.  In this case 

 becomes a matrix.  

Learning becomes 

Note that the formulas for the outputs, deltas, and weights are a functional specification for the 
backpropagation  algorithm:  formulas  are  presented  declaratively:  there  are  no  procedural 
programming  structures.    However:  before  rushing  to  copy  these  formulas  into  an  Excel 
spreadsheet we must note that these formulas will not work!  We did not specify how the inputs are 
selected for the algorithm; nor did we specify the initial states of the weight arrays.  We are close, 
but  not  close  enough  for  a  working  implementation.    In  order  to  do  this  we  need  to  review 
spreadsheet computational semantics. 

3  Visualizing Visual Back Propagation 
The backpropagation formulas are formulas are recursive (“circular” in spreadsheet terminology).  
To  resolve  circular  formulas  we  require  an  understanding  of  the  Manual  Mode  of  Calculation.  
Essentially,  spreadsheet  computation  in  Manual  Mode  is  very  similar  to  the  computation  of  a 
systolic array: a parallel architecture used for signal processing: 

A systolic array architecture is a subset of a data-flow architecture. It is made up of a 
number of identical cells, each cell locally connected to its nearest neighbor. The cells 
are usually arranged in a definite geometric pattern, corresponding to the tessellations of 
the Euclidean plane. The cells that are on the boundary of the pattern are able to interact 
with the outside world, At a given clock pulse, data enters and exits each cell; entering 
data is processed and stored so it can be output at the next pulse. Computational power 
is  thus  identified  with  the  speed  of  input  and  output:  a  wavefront  of  computation  is 
propagated in the array with a throughput proportional to the input/output bandwidth. 
This pulsing behavior is what gives this architecture its name. [16]. 

This  description  sounds  like  the  computational  dynamics  of  a  spreadsheet  in  Manual  Mode.  
Manual  mode  is  a calculation  option  that  – in  Excel  –  is specified  with  the  Calculation  Option 
dialog.  Some of the major features of Manual Calculation in Excel that are exploited by Visual 
Backpropagation are reviewed in the Appendix.  (Unfortunately, most spreadsheet systems do not 
support circular formula resolution in Manual Mode.)  The basic concept is: calculation in Manual 
Mode proceeds in a systolic style from left-to-right and then top-down [17].  Microsoft calls this 
“Row Major Order” calculation [18].  We now put all this together.   

20 

1...hqη1:((1))hhhmmη111111::;:TqqqqqThhhhhThqwwηoutδwwηoutδwwηinpδ 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

3.1  Spreadsheet Regions for Visual Backpropagation 
We need two regions to embed the backpropagation formulas: we call them Region A and 
Region B.  Region A – always arranged above Region B – incorporates the initialization 
of random weights.  We sometimes call Region A the predictor region and region B the 
corrector region (these terms are inspired from certain algorithms in numerical analysis).  
Both regions incorporate named arrays for the targets, inputs, outputs, and, and deltas. 
For Region A define the following arrays.  The array for the inputs: 

  . 

Next, define the weights and outputs: starting from the input layer: for 

outputs: 

…. 

…. 

; 

;  

;  

;  

…. 

…. 

Represent the deltas starting from the output layer: for 

Define similar arrays for Region B: 

;  

; 

;  

;  

;  

21 

, 

;  

;  

((1)1)ninpA:1,2,,,,hhq11:((1))mnwA11:((1)1)moutA221:(1)mmwA22:((1)1)moutA1:((1))hhhmmwA:((1)1)hhmoutA1,..,1hq1:((1))qqqmmwA:(1)qqmoutAoutA,,,2,1hqh:(1)::(1)qqqmmδA=δA:(1)hhmδA22:(1)mδA12:(1)mδA((1)1)ninpB:11:((1))mnwB11:((1)1)moutB221:(1)mmwB22:((1)1)moutB1:((1))hhhmmwB:((1)1)hhmoutB1,..,1hq1:((1))qqqmmwB:(1)qqmoutBoutB:(1)::(1)qqqmmδB=δB:(1)hhmδB1,,1hq22:(1)mδB12:(1)mδB 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

To  provide  for  initialization:  define  a  parameter 
gradient search.  We assume there is a function called 
example, in Excel, we can choose 
between 0 and 1; more commonly we choose 
decimal number between -1 and 1.  The choice is a design option. 

that  is  set  to  0  for  initialization  and  1  for 
that assigns initial values.  For 
to be  =rand() – this returns a random decimal number 
to be 2*rand() -1 – this returns a random 

For Region A, define and specify (order is significant) the following formulas: 

1A. Define and specify a sample record of inputs and targets as a column vector dependent on 
spreadsheet iteration.  The inputs and corresponding targets are selected by formulas (involving 
the OFFSET worksheet function) from a named spreadsheet region (This is discussed in the 
Appendix).   

2A. Define and specify the weights and outputs as array formulas in row major order from left 
to right.  For the first (input) layer

:  

In general, for 

 (where 

): 

For last (output) layer 

Note that the weight updates for Region A depend on results in Region B.  

3A. Define and specify the deltas as array formulas in row major order from lefty to right: 
For last layer 

For 

Note that the updates for the outputs and deltas in Region A depend on results in Region A.  

22 

ru()random()random()random1h1111111111:((1)):0,(),():(1):((1)1):1TmnifrurandomfmmwAwBinpBδBwAinpAoutA1,..,1hq0nm111:((1)):0,(),():(1):((1)1):1ThhhhhhhhhhhhhmmifrurandomfmmwAwBoutBδBwAoutAoutAhq1111:((1)):0,(),:(1):():(1)TqqqqqqqqqqqqmmifrurandommfmwAwBoutBδBoutAoutAwAoutAhq1:(1)::(1):':(1)qqqqqqqmmfmzwAoutAδA=δAtargAoutAz1,,1hq111:(1)::(1)';1,,1hhThhhhhhmmfhqzwAoutAδAwAδAz 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

We similarly specify computation for Region B as follows: 

1B. Define and specify a sample record of inputs and targets as a column vector dependent on 
spreadsheet iteration.  The inputs and corresponding targets are selected by formulas (involving 
the OFFSET worksheet function) from a named spreadsheet region (This is discussed in the 
Appendix).   

2B. Define and specify the weights and outputs as array formulas in row major order from left 
to right.  For the first (input) layer

:  

For 

For output 

Note that the weight updates for Region B depend on results in Region A.  

3B. Define and specify the deltas as array formulas in row major order from left to right: 

For 

Note that the updates for the outputs and deltas in Region B depend on results in Region B.  

Region  A  incorporates  initialization  when  the  spreadsheet  name
;  Region  B  always 
incorporates the values in Region A for weight updating.  By left-right top-down semantics, these 
,  Region  A  always  incorporates  the 
Region  A  values  are  available.    Conversely,  when 
already computed values in Region B for weight updating: by left-right top-down semantics, these 
Region B values are available (see the Appendix for examples).  

In practice, first initialize the set of weights in Region A by  setting iteration to 1 (on Excel 
;  and  pressing  Shift-F9  calculates  the 
Options:  Formulas:  Calculation  Options);  setting
active worksheet (only!) and propagates the weights with random values.  These random weights 
are  matrix  multiplied  with  the  selected  input  record  to  compute  and  propagate  values  to  the 

23 

1h1111111111:((1)):():(1):((1)1):1TmnfmmwBwAinpAδAwBinpBoutB1,..,1hq111:((1)):():(1):((1)1):1ThhhhhhhhhhhhhmmfmmwBwAoutAδAwBoutBoutB1111:((1))::(1):():(1)TqqqqqqqqqqqqmmmfmwBwAoutAδAoutBoutBwBoutB1:(1)::(1):':(1)qqqqqqqmmfmzwBoutBδB=δBtargBoutBz1,,1hq111:(1)::(1)';1,,1hhThhhhhhmmfhqzwBoutBδBwBδBz:0ru:1ru:0ru 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

corresponding dependent outputs.  After the last output is computed we compute the deltas.  The 
initial  weights  are  propagated  to  Region  B.      After  initialization,  “learning”  occurs  by  setting 
iteration to a larger number – typically a multiple of the total number of record samples.  After 
setting
;  and  pressing  Shift-F9,  the  spreadsheet  calculates  all  formulas  in  the  active 
worksheet and propagates the weights, outputs, and delta values associated with backpropagation 
gradient  search.    After  one  iteration  Excel  uses  these  backpropagation  weights  and  inputs  to 
compute and propagate gradient search values to the corresponding dependent outputs and deltas 
across Region A, then to Region B, and back to Region A.  The backpropagation stops when the 
iteration limit is reached.   

The  following  shows  a  simple  example  of  the  regression  problem  of  Figure  2.5  (example 
spreadsheets are posted in [8]).  As observed in Section 2, this problem is not linearly separable.  
We  used  Visual  Backpropagation  to  implement  a  2-2-2-2  network  with  tanh  as  the  activation 
function for all layers.  After about 1000 iterations (250 times through all samples, or 250 “epochs”) 
the computed weights are 

The output values compared to the targets are: 

Clearly  the  nonlinear  network  results  in  a  much  better  approximation  than  the  naïve  linear 
regression with the same inputs: the sum of the squared errors are less than 0.005 (targ1) and 0.0013 
(targ2).  Adding  a  third  input  to  the  linear  regression  –  for  example,  the  nonlinear  combination 
x1*x2 – results in an even better (exact) approximation than the nonlinear network with two inputs.  
But is this addition biased?  (The extra input is actually targ2). 

Let’s inspect the spreadsheet that computes this Visual Backpropagation [8].  First, note that 
the  worksheet  names  in  bold  are  defined  by  the  DEFINE  NAME  (or  Name  Manager)  utility.  
Circular formula semantics are reviewed in the Appendix. 

Figure 3.1 shows the initial state after the definition of array region names and insertion of 
array formulas.  The first formula in cell K5 is an iteration counter (a circular formula), used for 
information purposes.  Cells K6:K7 define two numbers that help select two sample records.  For 
4 samples, these numbers range between 0 and 3.  These are also circular formulas that use the 
MOD function.  The training sample selected for an iteration weight update for Region A is in 
E11:H11; for Region B the sample is in E12:H12.  Both ranges use the OFFSET function to select 
the values from the Training Data in E6:H9.   Cell K11 defines the learning rate eta: here, set to 0.1 
for  all  layers.    Cell  K14  defines  the  ru  parameter:  when  set  to  zero,  weights  are  randomized; 
otherwise, weights are updated according to the backpropagation update rule. 

24 

:1ru 
 
 
 
 
 
  
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Region A and Region B are presented in the thick box surrounding B16:U27.  This sheet shows 
the initialization so ru = 0 and iteration is set to 1.  We proceed in left-to-right top-down semantics. 

In Region A: 

The targets and inputs inpA and targA are transposed from the values in E11:H11.   
Set weights w_1A to random values.   
Compute the first layer output out_1A with inpA, w_1A and a tanh function map.   
Set weights w_2A to random values.   
Compute the second layer output out_2A with out_1A, w_2A and a tanh function map.  
Set weights w_3A to random values.   
Compute the last (third) layer output outA with out_2A, w_3A and a tanh function map.   
Compute delta delA for the last (third) output with targA and outA.   
Compute delta del_2A for the second output with delA, w_3A  and out_2A.   
Compute delta del_1A for the first output with del_2A, w_2A  and out_1A.   

In Region B: 

The targets and inputs inpA and targA are transposed from the values in E11:H11.   
Update weights w_1B via backpropagation formulas with w_1A, eta, inpA, and delA.   
Compute the first layer output out_1B with inpB, w_1B and a tanh function map.   
Update weights w_2B via backpropagation formulas with w_2A, eta, out_1A, and del_2A.   
Compute the second layer output out_2A with out_1A, w_2A and a tanh function map.  
Update weights w_3B via backpropagation formulas with w_3A, eta, out_2A, and delA.   
Compute the last (third) layer output outB with out_2B, w_3B and a tanh function map.   
Compute delta delB for the last (third) output with targB and outB.   
Compute delta del_2B for the second output with delB, w_3B  and out_2B.   
Compute delta del_1B for the first output with del_2B, w_2B  and out_1B.   

Figure 3.2 shows this sheet with ru = 1 and iteration set to 1000.  The worksheet shows the state 
after  learning  with  1002  iterations  (about  250  epochs).    We  proceed  in  left-to-right  top-down 
semantics. 

In Region A: (Changes from above indicatesd with *) 
The targets and inputs inpA and targA are transposed from the values in E11:H11.   
*Update weights w_1A via backpropagation formulas with w_1B, eta, inpB, and delB.   
Compute the first layer output out_1A with inpA, w_1A and a tanh function map.   
*Update weights w_2A via backpropagation formulas with w_2B, eta, out_1B, and del_2B.   
Compute the second layer output out_2A with out_1A, w_2A and a tanh function map.  
*Update weights w_3A via backpropagation formulas with w_3B, eta, out_2B, and delB.   
Compute the last (third) layer output outA with out_2A, w_3A and a tanh function map.   
Compute delta delA for the last (third) output with targA and outA.   
Compute delta del_2A for the second output with delA, w_3A  and out_2A.   
Compute delta del_1A for the first output with del_2A, w_2A  and out_1A.   

In Region B: 
Semantics of computation identical to above with ru=0. 

25 

 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Figure 3.1.  Visual Backpropagation initiation.  Random weight selection (ru=0).    

26 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Figure 3.2.  Visual Backpropagation after 1000 iterations (ru=1).    

27 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Figure 3.3 shows the Data Selection Formulas.  The OFFSET formula array returns the record 
from TrData that is displaced itc rows down from the top for Region A,and displaced itcp1 rows 
down from the top for Region B.  Data is selected at each iteration.  

Figure  3.4  shows  the  formulas  in  the  target,  input,  first  layer  weights  and  first  outputs  for 
Regions A and Region B.  Note that these are basically a cut-and-paste of the formulas described 
above in Section 3.1. 

Figure 3.5 shows the formulas in the second layer weights and second outputs in both Regions. 

Figure 3.3.  Visual Backpropagation data selection formulas. 

Figure 3.4.  Formulas for target, input, layer 1 weights and first outputs (Regions A and Region B).  

Figure 3.5.  Formulas for layer 2 weights and second outputs (Regions A and Region B).  

28 

 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Figure  3.6  shows  the  formulas  in  the  third  layer  weights  and  third  (final)  outputs  in  both 

Regions. 

Figure 3.7 shows the formulas for the three deltas for Regions A and Region B.  The formulas 
in R31:S32 show the computation of the 4-period (corresponding to the number of records in the 
sample data set) exponential moving average of the absolute errors.  We can use this to determine 
if the errors are converging to zero as calculation proceeds. 

Figure 3.6.  Formulas for layer 3 weights and third (final) outputs (Regions A and Region B).  

Figure 3.7.  Formulas: Output delta, Second Layer Output Delta, and First Level Output Delta. 
Exponential Moving average of output errors (Regions A and Region B).  

29 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

4  Visual Backpropagation on a Practical Problem   

We show Visual Backpropagation solving the “Auto MPG” problem found in the UCI Machine 
Learning repository [6].  This example includes a data set a table of 398 sample records listing 
inputs (such as Horsepower, Weight, Model Year) and a single target: the miles per gallon (MPG).  
This example is used in the Tensorflow regression tutorial [7]; there, the topology is 9-64-64-1.  
The Tensorflow model has 9 inputs, 64 outputs in the first layer, 64 outputs in the second layer, 
and 1 output for the approximated MPG (miles per gallon).  RELU activation functions are used 
for the middle two layers and a softmax activation [14] is used for output. Learning rate of 0.001 
is used for all layers.   

In the Visual Backpropagation model we use a slightly different model in order to investigate 

two questions:  

(1) Do the large number of outputs make a difference?   
To  investigate  this  we  configure  a  Visual  Backpropagation  spreadsheet  with  9-50-30-1 
topology; there are 40% fewer weights to learn (2061 vs 4865).  We use the same inputs and the 
same learning rates.   

(2) Does softmax make a difference? 
Following Øland et al’s critique of softmax activation [19], we use the identity function as the 
activation function on the last output layer and still keep the RELU activation functions on the first 
two layers. 

We follow the same procedure as in the Tensorflow tutorial using heuristics recommended by 

Lecun et al [2]: 

1.  Validate data: make sure that all input and target values are numbers and otherwise “make 
sense”  for  back  propagation.    This task  is easy  for  spreadsheets.   There  are  392  purely 
numerical records. 

2.  Divide  data  into  an  in-Sample  region  (for  training)  and  an  out-Sample  region  (for 
evaluation or tuning).  The Tensorflow example uses 80% of the data for training (314 
records);  we  use  a  larger  set  of  360  samples  (90%). The remainder  (78 records  and  32 
records respectively) are used for out-sample analysis.  Data selection tasks are easy for 
spreadsheets. 

3.  Scale data using values from the in-Sample Region.  Scaled data improves backpropagation 
convergence.  We use the z-score scaling (subtract the mean from each value and divide 
by  the  standard  deviation)  used  in  the  Tensorflow  example.      This  task  is  easy  for 
spreadsheets. 

4.  Train with backpropagation.   
5.  Investigate errors associated with the in-Sample region.  This task is easy for spreadsheets. 
6.  Cross-Validation:  Investigate errors  associated with the out-Sample region.  For tuning, 

this could involve further application of backpropagation. 

We discuss each step in turn.  Our Visual Backpropagation spreadsheets are available at [8]. 

4.1  Validate Data and Select in- and out-Sample Subsets 
We read the  data  into  car-mpg-scaling.xlsx  (see Tab:  orig-data).    Column  C  contains  the target 
mpg;  columns  D  through  L  contain  the  data.    In  column  N  we  add  a  computed  field  called 
IsNumber:  the values are the array formulas 

= IF(ISNUMBER(SUM(ABS(C7:L7))),"Yes","No") // M7 

30 

 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

We copy this array formula down through all rows.  The simple formula uses the ISNUMBER 
worksheet function to determine if the sum of the absolute values in the preceding columns is a 
number: these records are valid; if the formula returns “NO” then there are non-numeric values 
somewhere in the record.  We use this field with the Advanced Filter utility to extract all valid 
records from the data set as shown in Figure 4.1. 

Figure 4.1.  Formulas: Output delta, Second Layer Output Delta, and First Level Output Delta. 
Exponential Moving average of output errors (Regions A and Region B).  

4.2  Validate Data and Select in- and out-Sample Subsets 
From 398 records, 392 have only numbers (by inspection some have text like “?”).  Select 360 
records for in-Sample Training with backpropagation (rationale: 360 is a nice number).  Note that 
more rigorous cross-validation scenarios can involve several out-Sample regions. 

4.3  Scale data 
Given a columns of field values create a rescaled value
inputs and targets).  Linear scales are simplest and invertible: 
units:  
Sample data.  In linear range scaling, suppose 
over the in-Sample data.  To scale between 

.  Train on these rescaled values: (all 
; and in the original 
.   Note that to avoid bias, the scale parameters must be derived from the in-
in the original units 

, then  

and 

and 

 and 

values  are  between  -1  and  1.    Another  linear  scaling  requires  sample 

Typically  the  rescaled 
average 

 and sample standard deviation 

.  The “normalized z-score” scaling is 

so 

and 

. 

Figure 4.2 shows the values computed for scaling for the Auto MPG problem. 

31 

xyyx()/xymin()Axmax()Bxmin()aymax()bybaBABaABBAy()/yx1// 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Figure 4.2.  Values for scaling validated data. 

4.4  Training with Visual Backpropagation 
For spreadsheet-based data, it is easy to see how linear regression performs.  Using Excel’s built-
in functions LINEST and TREND we can determine weights, in-Sample errors, and out-Sample 
errors.  (Note that LINEST outputs are ordered rightmost value first.) For Auto MPG, the average 
absolute in-Sample error in original units is 2.44 mpg; the average absolute out-Sample error in 
original units is 3.06 mpg.  These values should be viewed as a baseline: we expect the nonlinear 
networks to outperform the standard linear values.  The regression weights are shown in Figure 5.3.   

Figure 4.3.  Linear Regression weights for Auto MPG.   

Note that the most significant linear factors (those with the largest magnitude of weights) are car 
weight and car year.  This is intuitively appealing since weight adversely affects mileage; we also 
expect newer cars to be more efficient. 

Setting up the 9-50-30-1 nonlinear network is straightforward: in our case it is a modification 
of the 2-2-2-2 network we used in the example shown in Figure 3.1 and Figure 3.2.  First copy the 
scaled in-Sample  data (360  records) to a  visual  backpropagation  worksheet.   Then  define  array 
regions  for  Region  A  and  Region  B.    Then  copy  and  paste  the  array  formulas.    To  make  the 
worksheet manageable, format the worksheet by hiding or displaying rows or columns.  The final 
results after training are in vbp_car-mpg-RELU-9-50-30-1-optim.xlsx [8].  For example, Figure 4.4 
shows the two training regions displaying weight rows 1,9,10,31,51 and columns 1 and 10 for the 
first layer weights; 1 and 51 for the second layer weights; and 1 and 31 for the third layer weights 
(see  column  G  and  row  374  for  the  row  and  column  indices).    Figure  4.5  shows  the  network 
topology with the Excel Trace Dependents utility.  Figure 4.6 shows all weights formatted via “red-
yellow-green” color scales in conditional formatting; this shows visually the highest and lowest 
weights.  

On a 64-bit 2.7 GHz laptop, training speed is about 30 seconds per 100 epochs (360 samples).   

An  exponential  moving  average  of  the  errors  was  computed  on  the  training  sheet  to  give  an 
indication of convergence: see cell DD485 on Figure 4.4.  The Appendix (Section A.3) reviews 
some details on iterative (recursive) representations of exponential moving averages. 

32 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Figure 4.4.  Region A and Region B for Auto MPG [8] with hidden rows and columns. 

Figure 4.5.  Network topology for Auto MPG via the Excel Trace Dependents utility. 

33 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Figure 4.6.  The 3 weight regions for Auto MPG (formatted with red-yellow-green color scale) . 

Investigate errors associated with the in-Sample region 

4.5 
The charts in Figure 4.7 compare the results of our 9-50-30-1 network after about 80 epochs with 
the results from linear regression (Method III in Section 2.2).  Both charts show that the 9-50-30-1 
offers improvement over regression.  Would more training or would different initial weights yield 
better results?  Once the network is set up it is easy to try to answer this by restarting the iterations.  
We can easily change the learning parameters or activation functions as well in order to “tune” the 
9-50-30-1 network.  

Absolute Error: in-Sample

Absolute Error: out-Sample

12

10

8

6

4

2

0

12

10

8

6

4

2

0

1

37

73

109 145 181 217 253 289 325

1

6

11

16

21

26

31

Regression

VBOptimal

Regression

VBOptimal

Figure 4.7.  Absolute Errors: 9-50-30-1 (RELU-RELU-Id; 80 epochs) vs. 9-1 (linear regression). 
The horizontal axis shows the sample number.  Left: in Sample; Right: out-Sample. 

34 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

4.6  Cross-Validation: Investigate errors associated with the out-Sample region 
Cross-validation has two meanings.   In one interpretation, cross-validation provides estimates of 
the true model error.   It answers the question: how well does a model predict?  In some sense, 
looking at the one or more sets of out-sample data can provides an indication of predictability.   
The second interpretation of cross-validation provides a way to perform model tuning: the selection 
of  model  parameters  that  improve  predictability.    This  interpretation  seems  to  be  prevalent  in 
machine learning. 

Here is an example of the second interpretation.  Suppose at the end of each training epoch we 
use the weights to look at the out-sample errors.  If the current out-sample error is smaller than the 
out-sample  error  of  the  previous  training  epoch,  then  save  these  weights;  otherwise  keep  the 
previous weights.  If we continue this procedure epoch-by-epoch, we should get a set of weights 
with the lowest out-sample error.  Figure 4.8 shows a chart of the results and the “cross-validation 
procedure” written in VBA (whose only use is to control computation and save the results).   

out-Sample Convergence
Absolute Error vs. Epochs

Linear Regression

6

5

4
r
o
r
r
3
E
s
b
A
2

1

0

Sub lrn() 
  For n = 1 To 100 
   Worksheets("Training").Calculate 
   Worksheets("out-Sample").Calculate 
   Range("F135").Offset(n, 0).Value = n 
   Range("G135").Offset(n,1).Value = Range("U132").Value 
   If Range("U132").Value < Range("W132").Value Then 
      Range("W132").Value = Range("U132").Value 
      Range("X132").Value = n 
    '' save workbook 
      ActiveWorkbook.Save 
   End If 
 Next n 
End Sub 

1

22

43

Epoch

64

85

Figure 4.8.  The use of cross-validation in model selection. 

As the number of epochs increase, the average absolute out-sample errors gradually decrease.  We 
also show the improvement our  9-50-30-1 network over the average absolute out-sample errors 
over linear regression.  Figure 4.9 summarizes the cross-validation findings for three models for 
the Auto MPG problem.  

35 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Model 

Linear Regression 
9-1 

Avg. Absolute Error 
(mpg):  
in-Sample 

Avg. Absolute 
Error (mpg):  
out-Sample 

2.44 

3.06 

Visual Backpropagation 
9-50-30-1 

1.47  (epoch 84) 
     0.878 (epoch 1000) 

2.36 (epoch 84) 
  2.35 (epoch 118) 

Tensorflow [7] 
9-64-64-1 

    0.997  (epoch 995) 

1.95 (?) 

Figure 4.9.  Model Comparison: 9-1 vs. 9-50-30-1 vs. 9-64-64-1 

Note  that  the  “best”  average  absolute  in-sample  and  out-sample  errors  for  the  two  nonlinear 
networks were obtained from weights that were tuned with the out-sample data.  Many authors [20-
22]  have  said  that  using  single  cross-validation  procedure  is  for  model  tuning  and  for  error 
assessment and estimation is problematic.  At best the results are biased and at worst it can lead to 
under-estimation of the true error.    The selection bias can be reduced if we set up another set of 
out-sample cross-validation data.  (Further discussion of cross-validation is beyond the scope of 
this paper.) 

5  Conclusions  

Visual Backpropagation embeds a functional programming specification of the backpropagation 
algorithm within spreadsheets.  This spreadsheet implementation is convenient and understandable 
for non-experts in machine learning and artificial intelligence.  It is suitable for solving problems 
that are represented in spreadsheets. 

One consequence of Visual Backpropagation spreadsheets is that they are pure formula-based 
computations.  These spreadsheets can be readily shared.  No software, programs, or libraries need 
to be downloaded and installed, called, or accessed.  There are no security concerns or trust settings 
involving special macros: there are no macros.   

A  second  consequence  is  that  the  backpropagation  implementation  is  reasonably  fast  for 
interpretive  languages:  we  know  that  built-in  Excel  functions  are  faster  than  the  corresponding 
programs in VBA [12], and there are some indications that VBA is faster than Python (see e.g. [23-
24]).   Moreover, at least with Excel, we can easily enable multiprocessing speedup with (select 
“multi-threaded  calculation”  under  “Advanced  Options”).    For  this  laptop,  the  option  “Use  all 
processors on this computer” was selected. 

36 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Acknowledgements 
I  wish  to  acknowledge  Dr.  R.  Down,  S.  Pennebaker,  Dr.  F.  Hilf  and  W.P.  Stahl  for  their 
encouragement and helpful conversations.  

References 
[1]  Rumelhart,  D.  E.,  J.  L.  McClelland,  and  the  PDP  Research  Group  (Eds.)  (1986).  Parallel 
Distributed  Processing:  Explorations  in  the  Microstructure  of  Cognition,  Volume  1: 
Foundations. MIT Press. 

[2]  LeCun, Y., Bottou, L., Orr, G., & Muller, K. (2012).  Efficient backprop. In Neural networks: 

Tricks of the trade. Springer, Berlin, Heidelberg, pp. 9-48. 

[3]  Bishop,  C.  (2006).    Pattern  Recognition  and  Machine  Learning  (Information  Science  and 

Statistics) Springer-Verlag. 

[4]  Pao,  Y-H  (1989).    Adaptive  Pattern  Recognition  and  Neural  Networks.  Reading:  Addison 

Wesley. 

[5]  Etheridge,  H.  L.,  &  Brooks,  R.  C.  (1994).  Neural  networks:  A  new  technology.  The  CPA 

Journal, 64(3).   

[6]  Dua, D. and Graff, C. (2019). UCI Machine Learning Repository. Irvine, CA: University of 

California, School of Information and Computer Science.   
URL: https://archive.ics.uci.edu/ml/datasets/Auto+MPG 
[7]  Regression: predict fuel efficiency.  TensorFlow Tutorial.     

URL: https://www.tensorflow.org/tutorials/keras/basic_regression  

[8]  Visual Backpropagation Examples.  URL:  https://www.inductive.com/visualbackprop  
[9]  Walsh, A.  (2017).  Seven reasons why Excel is still used by half a billion people worldwide.  

Irish Tech News.   URL: https://irishtechnews.ie/seven-reasons-why-  
excel-is-still-used-by-half-a-billion-people-worldwide/ 

[10]  Casimir, R. J. (1992). Real programmers don't use spreadsheets.  ACM Sigplan Notices, 27(6), 

10-16. 

[11]  Fouhey, D. &  Maturana, D. (2016).  Deep Spreadsheets with ExcelNet.  . 

URL: http://web.eecs.umich.edu/~fouhey/#fun 

[12]  Sestoft, P. (2014).  Spreadsheet Implementation Technology: Basics and Extensions.  MIT 

Press.  ISBN: 9780262526647.   

[13]  Efron,  B.,  and  Hastie,  T.    (2016)  Computer  Age  Statistical  Inference.  Vol.  5.    Cambridge 

University Press. 

[14]  Wikipedia  contributors.  (2019,  February  22).  Activation  function.  In  Wikipedia,  The  Free 

Encyclopedia. URL: https://en.wikipedia.org/wiki/Activation_function    

[15]  B. Widrow (1959).  Adaptive Sampled-Data Systems – A Statistical Theory of Adaptation.  

IRE WESCON Convention Record, 4:74-85.   
URL: http://www-isl.stanford.edu/~widrow/papers/c1959adaptivesampled.pdf  

[16]  Freedman, R.S.  (1984). Logic Programming of Systolic Arrays.  Chapter 28 in VLSI Signal 
Processing, Cappello, P. et al (eds).  IEEE Press, New York 1984.  ISBN 0-87942-186-X. 

[17]  Williams, C.  (2014). Excel Calculation Methods.  Decision Models, Inc.   

URL: http://www.decisionmodels.com/calcsecretsc.htm 

[18]  Excel performance: Improving calculation performance.  Microsoft Office Developer Center, 
10/05/2017.          URL:  https://docs.microsoft.com/en-us/office/vba/excel/concepts/excel-
performance/excel-improving-calcuation-performance  

37 

 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

[19]  Øland,  A.,  Bansal,  A.,  Dannenberg,  R.  B.,  &  Raj,  B.  (2017).  Be  careful  what  you 
backpropagate:  A  case  for  linear  output  activations  &  gradient  boosting.  arXiv  preprint 
arXiv:1707.04199. 

[20]  Hastie, T., R. Tibshirani, and J. Friedman (2011). The Elements of Statistical Learning. Data 

Mining, Inference, and Prediction. 2nd ed. , New York: Springer. 

[21]  Neunhoeffer, M., & Sternberg, S. (2019). How Cross-Validation Can Go Wrong and What to 

Do About It. Political Analysis, 27(1), 101-106. doi:10.1017/pan.2018.39. 

[22]  Cawley,  G.,  and  Talbot,  N.  (2010).    On  over-fitting  in  model  selection  and  subsequent 
selection bias in performance evaluation. Journal of Machine Learning Research, pp. 2079-
2107. 

[23]  Is Python faster than Visual Basic? Quora, (2017).  URL: https://www.quora.com/Is-Python-

faster-than-Visual-Basic 

[24]  Python loop slower than Excel VBA?  Stack Overflow (2017).   

URL: https://stackoverflow.com/questions/30045508/python-loop-slower-than-excel-vba  

[25]  Wikipedia  contributors.  (2019,  January  24).  Moving  average.  In  Wikipedia,  The  Free 

Encyclopedia.  URL: https://en.wikipedia.org/wiki/Moving_average 

[26]  Gilchrist, W. (1976).  Statistical forecasting. Vol. 322. London: Wiley. 

38 

 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

APPENDIX:  Computational Semantics of Manual Calculation 

We  review  some  of  the  major  features  of  Manual  Calculation  that  are  exploited  in  Visual 

Backpropagation. 

Figure A.1 shows the Excel Calculation Options Dialog.  For Visual Backpropagation:  Set the 
Workbook  Calculation  to  Manual.    Uncheck  Recalculate  before  saving.    Enable  iterative 
calculation.    When  setting  up  regions:  set  Maximum  Iterations  to  1.    When  using  Visual 
Backpropagation for training: set Maximum iterations to the number of records in the training data 
set.  This means that when pressing <shift-F9> (Function key F9) – the  Calculate Active Sheet 
command  –  the  backpropagation  algorithm  iterates across  the  entire data  set.  This is called  “1 
epoch” in the machine learning literature.   

Figure A.1.  Excel Calculation Options Dialog. 

A.1    Recursive Formulas  
A circular formula is a formula that refers to itself.  For Automatic Mode this normally indicates 
an error.  In Manual mode circular formulas implement recursion.  See Figure 3.2 for an example.   

Figure A.2.  Circular formulas in cells B2, D2, and D4.  Iteration is set to 1. 

In Figure A.2, the initial contents of these cells after the formulas are entered are shown in the left 
panel. The middle panel shows the formulas.  Note that the default contents are zero.  After formula 
entry, the contents of B2 is the previous value (zero) plus 1.  The contents of D2 is the previous 
contents of D4 (zero) plus 1.  The contents of D4 is the previous contents of D2 (1) plus 1.  Now 
press <shift F9> to calculate this  (the active) sheet.  After 1 iteration, the right panel shows the 
values of these cells.   

39 

 
 
 
 
   
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Calculation in Manual Mode proceeds in a systolic style from left-to-right and then top-down [17].  
Microsoft  calls  this  “Row  Major  Order”  calculation  [18].    Figure  A.3  and  A.4  show  another 
example  which  demonstrates  how  previous  states  can  be  saved  (this  has  been  used  in  Visual 
Backpropagation  to  model  autoregressive  time  series  models  (called  recurrent  networks  in  the 
machine learning literature).   

Figure A.3.  Left panel:  initial state after formulas are entered.   
Right panel:  formulas.  Note that G20 and E28 are the only circular formulas. 

Figure A.4.   Left: State after two presses to <shift-F9> to Calculate active sheet.   
Right: State after another press: three presses total to SHIFT-F9 (Calculate Sheet).   

Note that the unshaded values in row 20 to the left of the circular formula are lagged.  The colored 
values in column E that are above the circular formula are also lagged.   

40 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

A.2  Application: Selecting Data   
We exploit these computational semantics to create formulas that dynamically select input records 
and corresponding target values from a dataset represented as a spreadsheet region.  This helps 
enable “stochastic search” for visual backpropagation.  Figure A.5 shows the spreadsheet regions 
and formulas.  Define dataset TrData as the region E6:H9.  There are 4 sample records.  Cells K5, 
K6 and K7 contain circular formulas that always evaluate to be between 0 and 3: we define itc as 
K6 and itcp1 as K7.   

Figure. A.5.   Dynamic Sample Selection: Top: values view.  Bottom: formula view. 

Rows  11  and  12  contain  array  formulas  with  the  OFFSET  worksheet  function.    This  function 
returns a reference to a region of a specified size, offset by rows and columns.  At the end of the 
first iteration, note that itc=1 and itcp1=2.  The OFFSET formulas use these values to select a single 
row of TrData displaced by itc rows and itcp1 rows.  With these values the second and third rows 
of TrData are selected.   With more iterations, subsequent rows are selected; the MOD function 
(which gives the remainder after division) ultimately resets itcp1 (and then itc) to 0. 

Data can also shuffled before selected or can be randomly selected.  Figure A.6 shows some 

examples of such formulas on a larger data set of 360 records. 

41 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

Figure A.6.  Shuffle and random selection of data. 

A.3  Monitoring Absolute Error with Exponential Moving Averages 
Given  a  sequence  of  values

the  exponential  moving  average  (EMA)  is  iteratively 

specified by 

We use this to approximate the simple moving average [19, 20] over the last N-periods.  It can be 
shown that for  

then the exponential moving average is a reasonable approximation to the N-period simple moving 
average, in that the last N terms represent almost 90% of the total weight in the calculation.  The 
representation is: 

We use the exponential moving average to monitor the error for Visual Backpropagation training 
(e.g., see Figure 3.7).  Figure A.7 shows the circular array formulas for a 4-period EMA.  

Figure. A.7.   A 4-period EMA for absolute errors. 

42 

12,...,,...,nyyy1111:(1)nnnEMAyEMAyEMA2/(1)N121111:NnnnNNEMAyEMA 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
R.S. FREEDMAN     

  VISUAL BACKPROPAGATION 

A.4  Tabulating out-Sample Values 
Suppose we want to tabulate the outputs of a complex set of formulas whose inputs are driven by 
a dynamic sample section as described above.  For example, look at Figure A.8.    

Figure. A.8.   Tabulating Data: Input Selection (current record number selected in D11). 

Here D11 contains the sample number for the input that cycles through the dataset.  The two outputs 
we want tabulated are shown in Figure A.9. 

Figure. A.9.   Tabulating Data: Output Selection (P18:P19). 

Formulas  (for  this  example  dataset  of  4  records)  that  enable  this  are  in  Figure  A.10.    After  1 
iteration, formulas in columns E and F ask if the sample number of the input (an integer between 0 
and 3) equals to the cell label in the tabulated table (D30:D33) where the outputs are tabulated.  
The cells where the label equals the input sample number are changed to the corresponding output; 
otherwise the cell values in the tabulation are left unchanged.  After 4 iterations the output values  
are all tabulated. 

Figure. A.7.   Tabulating values.  Top: Values view.  Bottom: Formulas view. 

43 

 
 
 
 
 
 
 
 
 
