0
2
0
2

r
a

M
2

]

O
R
.
s
c
[

2
v
8
9
1
8
0
.
1
0
0
2
:
v
i
X
r
a

Safety Considerations in Deep Control Policies with Safety Barrier
Certiﬁcates Under Uncertainty

Tom Hirshberg1∗, Sai Vemprala2 and Ashish Kapoor2

Abstract— Recent advances in Deep Machine Learning have
shown promise in solving complex perception and control loops
via methods such as reinforcement and imitation learning.
However, guaranteeing safety for such learned deep policies
has been a challenge due to issues such as partial observability
and difﬁculties in characterizing the behavior of the neural
networks. While a lot of emphasis in safe learning has been
it is non-trivial to guarantee safety
placed during training,
at deployment or test time. This paper extends how under
mild assumptions, Safety Barrier Certiﬁcates can be used to
guarantee safety with deep control policies despite uncertainty
arising due to perception and other latent variables. Speciﬁcally
for scenarios where the dynamics are smooth and uncertainty
has a ﬁnite support, the proposed framework wraps around
an existing deep control policy and generates safe actions by
dynamically evaluating and modifying the policy from the
embedded network. Our framework utilizes control barrier
functions to create spaces of control actions that are safe under
uncertainty, and when the original actions are found to be in
violation of the safety constraint, uses quadratic programming
to minimally modify the original actions to ensure they lie
in the safe set. Representations of the environment are built
through Euclidean signed distance ﬁelds that are then used to
infer the safety of actions and to guarantee forward invariance.
We implement this method in simulation in a drone-racing
environment and show that our method results in safer actions
compared to a baseline that only relies on imitation learning
to generate control actions.

I. INTRODUCTION

Trained deep control policies via reinforcement learning
(RL) and imitation learning (IL) allow generating control
outputs directly from sensor inputs. However, in contrast to
simulations and games, applying such techniques to real-
world safety-critical applications remains an incredibly chal-
lenging task. The strong reliance on deep neural networks
makes them vulnerable to overconﬁdent or unpredictable
results when presented with data distributions unseen during
training. Additionally, in real-life scenarios there might be
environmental factors (e.g. friction, wind, viscosity, etc.) and
uncertainties due to machine perception, that may not have
been explicitly modeled in the formulation. The importance
of predictions and actions being robust to such exogenous
variations is paramount in the safety-critical aspects of real-
world robotics.

Much of the previous work on safety in deep control
policies has focused on modifying the training phase. These
include reward engineering [1], constrained optimization to

*Work done while interning at Microsoft Corporation, Redmond
1CS Department, Technion - Israel Institute of Technology, Haifa, Israel

tomhirshberg@campus.technion.ac.il

2Microsoft Corporation, Redmond, WA {sai.vemprala,

akapoor}@microsoft.com

incorporate safety constraints [2] and worst-case optimiza-
tion [3]. Providing safety guarantees that would hold at
the deployment phase (test time) is challenging, since it is
difﬁcult to characterize or enumerate the complete state space
of the agent. For example, it is impossible to characterize
the
a-priori all
mathematical structure of the deep policies further makes
it difﬁcult to provide an analysis of the deep policies.

images a robot would see. Furthermore,

In this work, we explore a runtime alternative that aims
to keep the system safe by providing minimal deviations of
control signals stemming from an embedded deep control
policy. The framework attempts to continuously preserve
the safety via a barrier function, while the agent continues
to make progress towards the task it was trained for. In
particular, the work extends Safety Barrier Certiﬁcates (SBC)
[4] to handle safety considerations in deep control policies,
and focuses solely on a test-time implementation, thus not
affecting the training phase. We speciﬁcally focus on the
problem of autonomous drone-racing, where a quadrotor
needs to negotiate several gates without collisions while
moving as fast as possible on a racing track. There are
several technical challenges that we encounter and address.
First, deep control policies are often used when there is no
explicit model of systems dynamics available. In the absence
of such a dynamics model, it is non-trivial to use SBCs.
Second, the safety constraints for applications such as drone-
racing can be complex. For example, in our application case,
the quadrotors are not allowed to collide with the gates or
other objects in the environment. Representations such as
Euclidean signed distance ﬁelds (ESDF) are popular and
useful to formally deﬁne the safety conditions. However, it
is unclear how SBCs can be applied here. Finally, the safety
framework also needs to account for any uncertainty and non-
determinism that might arise due to environmental factors.
in this work is that for many real-
world applications system dynamics are well approximated
as an ordinary differential equation that
is uniformly
continuous, bounded and Lipschitz
continuous. This
allows us to make a locally linear approximation while
ensuring that the approximation error is small. Similarly,
we incorporate safety constraints deﬁned over ESDFs via
smooth approximations and ﬁnally discuss safety under
uncertainty and non-determinism. We
implement our
framework in simulation and show that our method results
in guaranteeable safety and improved avoidance compared
to the original deep control policies, even under perception
uncertainty. In summary,
the main contributions of this
paper include:

The core insight

 
 
 
 
 
 
• Enhancing any trained deep control policy using SBC

under uncertainty.

• Simplifying safety constraints for barrier function-based

avoidance in complex environments.

• Improving the safety of deep control policies.

II. RELATED WORK

Recent research has focused on learning control policies
directly from raw data using deep neural networks (DNN)
by either imitation or reinforcement learning [5], [6]. Much
of the work in safe deep control focuses on training time
and aims to induce risk-aversion via reward function or
through constrained optimization [1], [7], [8]. However,
none of these approaches guarantee safety during the test
or deployment phase. Formal veriﬁcation and certiﬁcation
has also been proposed to address the application of deep
neural networks in safety-critical applications. For example,
[9], [10] focus on veriﬁcation procedure of DNNs through
analysis of activation functions and layers. Similarly, [11]
perform veriﬁcation for a feedback control network using a
receding horizon formulation that attempts to enforce prop-
erties such as reachability, safety and stability. [12] discuss
control-theoretic modiﬁcations to reinforcement learning for
safety analysis. The notion of probabilistic safety under
uncertainty has also been explored previously via formal
methods [13]. Much of this work results in computationally
intensive procedures that cannot be easily used in real-time
systems.

Safety Barrier Certiﬁcates (SBC) with permissive con-
trol barrier functions (CBF), have been previously used to
guarantee runtime safety in both deterministic and non-
deterministic settings [14], [15], [4]. CBF were also used for
a safe exploration during learning of RL models [16], [17],
[18]. The key idea is to ﬁrst deﬁne a barrier function by
considering a set of unsafe states and the system dynamics,
and then use it to minimally modify a given controller so
that the resulting solution is safe. The framework can be
extended to handle uncertainty in the environment to proba-
bilistically guarantee safety [19]. Recent work by [20] also
proposed a real-time safety framework on top of learning-
based planners, based on Hamilton-Jacobi reachability. This
paper builds upon this line of works where the key idea
centers on wrapping a deep control policy within the SBC
framework. However, unlike most applications of SBC, in
our case there is no explicit system dynamics model avail-
able. The use of CBF to ensure safety was also intro-
duced in [21], [22], which enforced CBF constraints and
obtained control actions through a mixed integer program
or a quadratic program. These works assumed that
the
obstacles were convex, whereas our framework can handle
non-convex obstacles in a fast and easy way, forming a
QP to be solved for CBF obstacle avoidance. In [23], the
authors present a deep controller that performs autonomous
drone landing while providing stability guarantees under
unsteady dynamics. In comparison, we explicitly also focus
on observation uncertainty along with system uncertainty: as

in most of the cases where deep controllers are applied there
is uncertainty that stems from the sensing and perception
part of the system.

We demonstrate the framework on the task of autonomous
drone-racing, where within a realistic simulator [24], a
quadrotor uses an RGB camera to perceive and negotiate
multiple gates as fast as possible on a racing track. The
approaches to solve drone-racing consider inferring simple
representation of the environment, and then using either
classical control and planning methods [25], [26] or building
deep controllers [27], [28]. In this paper we explore deep
control policies and assume that at least one gate is always
in the ﬁeld of view of the quadrotor.

III. PROPOSED FRAMEWORK

Assume we have been given a policy, that produces a
control signal ˜u. The goal of the proposed framework is
to provide a projection of ˜u, such that the system is safe
with respect to safety constraints. In the context of this
discussion, we make use of an important assumption that
the uncertainties pertaining to system dynamics as well as
the perception observations can be modeled as distributions
with ﬁnite support. The ﬁnite support assumption is obtained
from knowledge of the application domain. As the system
of interest is a quadrotor, due to physical actuation limits,
the uncertainties arising from translational and rotational
dynamics are bounded. At the same time, the perception
based localization system that is responsible for generating
control actions only attempts to localize the drone gate within
the viewing frustum of the camera, hence bounding the
observation uncertainty accordingly.

In order to use the SBC framework, we need to ﬁrst
characterize (1) system dynamics, (2) safety constraints and
(3) handle uncertainty. We describe these in detail below:

A. System Dynamics Model

Due to the absence of an explicit system dynamics model
in most deep control scenarios, we need to make certain
assumptions. First, we consider a simpliﬁed dynamics model
for a robot evolving as a continuous-time system:

˙x = f (x, u) + w, w ∼ U (−∆w, ∆w)
ˆx = x + v, v ∼ U (−∆v, ∆v)

(1)

where x ∈ X ⊂ Rd is the system state, the noisy observation
is ˆx ∈ Rd, u ∈ U ⊂ Rd is the control
input action
and w, v ∈ Rd are the process and measurement noises.
U denotes the uniform distribution, with a ﬁnite support
deﬁned by ∆w and ∆v. Speciﬁcally, similar to [29] we make
the assumption that the underlying unknown dynamics of
the system is uniformly continuous, bounded and Lipschitz
continuous. Additionally, as mentioned earlier, we make an
assumption that approximation error, uncertainty, and non-
determinism in the system could be explained via noise with
ﬁnite support [19]. Thus, we can simplify the robot dynamics
˙x ∈ Rd as stochastic control-afﬁne single integrator dy-
namics of the form ˙x = u + w, where w accounts for
both the model non-linearity and any model uncertainties.

Fig. 1. The distance map deﬁned by (2), by XY and YZ planes (upper row and the lower row). The reader is guided to zoom-in the ﬁgures for details:
The green arrows direct the safest action as computed by the SBC and providing the safety constraints, and the black crosses indicate positions that are
unsafe in every direction. The actions sampled for every position and for every angle are |u| = velocity. The right image shows the original gate.

This also allows us to make locally linear approximations
of the dynamics over the control input. In the context of
a quadrotor, the virtual control inputs provided through the
single integrator dynamics are mapped to the corresponding
non-linear physical model inside the simulation.

B. Safety Constraints

Given our application domain of drone-racing, we propose
safety constraints based on rich representations common
in robotics. Our obstacle model is a static model and is
represented through a distance transform inspired by the
Euclidean signed distance ﬁeld (ESDF). We deﬁne three
regions for every obstacle i with a pose denoted as pi, that
are subsets of the 3D metric space: Ω−
inside the obstacle,
i
Ω+
i outside the obstacle and ∂Ωi as its border. For any point
in 3D space x that is the position of the robot, we deﬁne a
custom distance function to obstacle i as follows:
(cid:107)x − y(cid:107) , x ∈ (Ω+
x ∈ Ω−
i

miny∈∂Ωi∪Ω−
−1,

d(x, pi) =

i ∪ ∂Ωi)

(cid:40)

i

(2)

Under the assumption that a robot cannot physically be
inside an obstacle, i.e. for every state x /∈ Ω−
and for
i
every obstacle i, there are properties of the distance function
deﬁned in (2) that are useful in deﬁning the safety barrier.
In particular, we make the following observation:

Remark 1: d(x, pi) where x /∈ Ω−
i

is Lipschitz continu-
ous, differentiable almost everywhere and bounded under a
ﬁnite support.

We deﬁne a state x /∈ Ω− to be safe with respect to an
obstacle i with the pose pi if the following conditions hold:

i (x) = d2(x, pi) − R2,
hs
i = {x ∈ Rd : hs
Hs

i (x) ≥ 0},

x /∈ Ω−
i
x /∈ Ω−
i

(3)

(4)

The set Hs
i

indicates the set of states that are safe with
respect to the obstacle i, where R is a buffer safety radius.
Naturally, the condition of x /∈ Ω−
ensures valid robot
i
states lie only outside obstacles. For our application in drone-
racing, we consider square gates as obstacles (see Fig. 1).
Additionally, considering the ﬁnite boundary of (2) and the
fact that all our obstacles are the same, we precompute the
signed distance ﬁeld using (2) for a set of sampled poses of
the robot within a region of interest relative to the gate, thus
creating a distance map.

C. Safety Under Uncertainty

In most of the deep control scenarios, the state variable
used to deﬁne and evaluate safety is latent. Consequently,
we assume that a state estimation routine is available that
would provide the system state with a bounded error. In
our work on drone-racing, we use a Variational Autoencoder
(VAE) based module to estimate the pose of the gates. This
estimation not precise but considered to have ﬁnite support.
To address safety under the uncertainty arising due to such
state estimation, we perform worst-case safety computation.
Formally, we deﬁne a new distance function as follows:

d∗(x, pi) =




minp∗

i ∈Pi d(x, p∗

i ),



−1,

i ∪ ∂Ωi),

{x ∈ (Ω+
∀p∗
i ∈ Pi}
x ∈ Ω−
i

(5)

where Pi is the set of points that is occupied when the
obstacle i is replicated at all possible positions within the
error threshold of the predicted pose.

Remark 2: We can represent a new obstacle with pose ˆpi
i ∈ Pi such that

that comprises of all possible positions p∗
d∗(x, pi) = d(x, ˆpi).

−2−1012Y0123XLow Velocity 1[m/s]−2−1012Y0123XMedium Velocity 2[m/s]−2−1012Y0123XFast Velocity 3[m/s]−10123Distance−2−1012Y−2−1012ZLow Velocity 1[m/s]−2−1012Y−2−1012ZMedium Velocity 2[m/s]−2−1012Y−2−1012ZFast Velocity 3[m/s]−1.0−0.50.00.51.01.52.0DistanceFig. 2. The distance map deﬁned by (5), by XY and YZ planes (upper row and lower row). The reader is guided to zoom-in the ﬁgures for details:
The green arrows direct the safest action as computed by the SBC and providing the safety constraints, and the black crosses indicate positions that are
unsafe in every direction. The actions sampled for every position and for every angle are |u| = velocity. The right image shows the new gate’s structure,
inﬂated by including all the possible positions the gate can take while considering uncertainty. Applying safety constraints affects the modiﬁcations to the
original controller and consequently leads to more restrictions to it, as can be seen compared to Fig. 1.

This new obstacle allows us to consider the worst-case
scenario under gate pose estimation uncertainty and can be
tackled using the same safety deﬁnition as in (4). A new
corresponding distance map can also be precomputed. This
method simpliﬁes the way to provide safety under uncertain-
ties as the basic underlying fabric stays unchanged. Fig. 1 and
Fig. 2 show the distance maps with and without considering
uncertainty, and the difference in the measurements of the
obstacle. Now the safe set considering worst-case scenarios
under uncertainty can be deﬁned by:

H∗s
where h∗s

i = {x ∈ Rd : h∗s

i (x) ≥ 0},
i (x) = d∗2(x, pi) − R2, and x /∈ Ω−

i

(6)

It is easy to show using Remark (2) that there exists an
equivalent safety set that considers the original ESDF using
the newly constructed obstacle with pose ˆpi:
i = {x ∈ Rd : hs

i (x) ≥ 0},
i (x) = d2(x, ˆpi) − R2, and x /∈ Ω−
i

Hs
where hs

(7)

IV. SAFETY BARRIER CERTIFICATES UNDER
UNCERTAINTY

Barrier certiﬁcates, or barrier functions, are used to ensure
that robots remain in safe sets for all
time. Controllers
are expected to satisfy the barrier certiﬁcates while taking
control actions that are as close as possible to the nominal
action. For the discussion in this section, we assume the
perspective of the robot that is running the perception-action
loop, thus expressing the obstacle poses relative to the pose
of the robot. This allows for a simpliﬁcation of the notation
for the distance function from d(x, p) to d(p). Under the
assumption that at least one obstacle is in the ﬁeld of view of
the robot at any time, we simplify the notation and represent
the safety set and constraints as a function of the next

obstacle pose p ∈ P ⊂ Rd relative to the robot. The set
P is again deﬁned by all states that correspond to the center
of the robot being outside the obstacle. The safety set in the
new simpler representation is deﬁned similarly to (4):

Hs = {p ∈ P | hs(p) ≥ 0}
where hs(p) = d2(p) − R2, and p ∈ P

(8)

Based on the theory of Zeroing Control Barrier Functions
(ZCBF) and SBC, some conditions need to be applied to
the controller u ∈ U to guarantee forward invariance of the
safety set. A continuously differentiable function hs : P →
R is a ZCBF, and the admissible control space can be deﬁned
as:

S(p) = {u ∈ U | ˙hs(p) + κ(hs(p)) ≥ 0}, p ∈ P

(9)

Any Lipschitz continuous action u ∈ S(p) guarantees that
the set Hs is forward invariant. Considering the extended
class-K function as κ(hs(p)) = γhs(p) for γ >> 0, and
based on the admissible control space, the SBC that deﬁnes
the constraints can be formulated as:

Bs(p) = {u ∈ Rd : ˙hs(p) + γhs(p) ≥ 0}, p ∈ P (10)

Remark 3: For all the positions occupied by the robot
where the distance map is differentiable, and assuming
the initial position is collision-free, it can be shown that
the constrained control space described by (10) induces a
linear constraint over the robot controller. Proof and further
discussion can be found in [19].

We recall here that as we pre-compute a distance map
for hs(p) over a grid of sampled poses, it is possible to
determine the relevant constraints efﬁciently at runtime.

In order to ensure hs(p) deﬁned in (8) is continuously
differentiable, we can use a smooth approximation to (2)

−2−1012Y0123XLow Velocity 1[m/s]−2−1012Y0123XMedium Velocity 2[m/s]−2−1012Y0123XFast Velocity 3[m/s]−10123Distance−2−1012Y−2−1012ZLow Velocity 1[m/s]−2−1012Y−2−1012ZMedium Velocity 2[m/s]−2−1012Y−2−1012ZFast Velocity 3[m/s]−1.0−0.50.00.51.01.52.0Distance(a) Performance on Safety

(b) Performance on Task Success

Fig. 3.
Performance comparison on (a) safety and (b) success metrics for different difﬁculty levels. We observe that the proposed framework provides
better performance on safety, especially when considering the uncertainty in gate position, while still performing better than original baselines on the task.
The effects are more pronounced when the difﬁculty level is higher.

(for example using the softmax trick). In our experiments,
we simply work with ESDFs noting that the regions of
non-differentiability (with respect to a drone gate’s shape)
arise only at the places where the vehicle is guaranteed
to be safe by a wide margin. Second, we transform the
VAE’s estimated gate position coordinates from spherical to
Euclidean coordinates, where the quadrotor’s yaw angle is
equal to the predicted angle with respect to the obstacle.
These actions help prove the continuity of the derivative of
hs(p). Thus we can formally rewrite Bs as:

Bs(p) =

(cid:110)

u ∈ Rd : 2d(p)

(cid:19)T

(cid:18) ∂d(p)
∂x

˙x

+ γ(d2(p) − R2) ≥ 0

, p ∈ P

(cid:111)

(11)

it

Similar to precomputing the distance map,

is also
possible to precompute its gradient ∂d(p)
∂x . Further, under our
assumed locally linear dynamics (1), we can write ˙x in (11)
to be u + w. Given that w has ﬁnite support (|w| ≤ ∆w)
and d(p) is Lipschitz continuous, we can compute a constant
C ≥ 2d(p)( ∂d(p)
∂x )T w, thus resulting in a linear constraint
on u that guarantees the inequality in (11). Finally, we
formulate our safety problem as a Quadratic Program (QP)
to minimally change the action if needed, i.e. modify the
original control action if it is found to violate the safety
constraints. Formally, we solve the following program using
the safety constraints deﬁned in (8) and the SBC (11):

min
u∗∈Rd

(cid:107)˜u − u∗(cid:107)2

s.t. p ∈ Hs

u∗ ∈ Bs(p)
(cid:107)u∗(cid:107) ≤ α

(12)

(13)

(14)

(15)

where α is the boundary of the controller action, ˜u is the
original deep policy control and the safe action is denoted
by u∗. In practice, considering worst-case safety leads to a
difference in the modiﬁcations to the original controller, and
it is more restricted near the obstacle (for example see in
Fig. 1 and Fig. 2).

V. EXPERIMENTS AND RESULTS

We performed experiments to verify the robustness of the
method and understand its limitations via a drone-racing
simulation built on top of AirSim [24]. Each experiment
comprised of a quadrotor navigating through a set of ten
racing tracks for three laps. Each track was around 50m
in length, built by eight gates positioned randomly. Each
experiment was associated with one of four difﬁculty levels
(ranging from 0 to 1.5 with a step size of 0.5), deﬁned by
the maximum offset between the centers of two consecutive
gates, where a larger offset requires more maneuvering to
stay on track.

We use two key metrics for evaluation: safety and the
ability to solve the given tasks successfully. A trial consists
of maneuvering through three consecutive laps of the track,
and it is deﬁned as safe when the quadrotor stays collision-
free over the entire trial. The percent of gates negotiated
safely through a trial is a measure of success on the task.
We wish to explore if the proposed framework allows us
to be safe while still being competitive, as deﬁned by the
success criterion.

For the perception module and baseline control policies,
we use the networks from [28]: a variational auto-encoder
(VAE-constrained) that predicts next gate poses and Behavior
Cloning (BC) policies constrained and unconstrained, which
are the best performing networks for control in the mentioned
work. We compare both deep control policies with our
proposed safety framework with and without uncertainty
(corresponding to gate pose localization). Our uncertainty
estimation is based on the errors in gate pose estimation
computed empirically by [28].

Fig. 3 shows the performances of both baselines when
augmented with our optimization method, by safety and suc-
cess metrics. In our experiments, the success rate seems to be
almost similar for all methods, with a slight advantage for the
safety method considering uncertainty. As the track difﬁculty
increases to 1.5, we observed the safety performance of
the original policies deteriorate drastically, while that of the
safety policies decreases slower. We observe that the best
safety rates were achieved when considering uncertainty.

(a) Trajectories on four track difﬁculty levels

(b) Minimum distance - Averaged over track difﬁculty level

Fig. 4.
(a) Sample trajectories from a trial for the original policy and the safety policies. (b) Boxplots summarizing the closest distance a quadrotor got to
any gate, based on (2), over the 10 experiments and for each difﬁculty level. The horizontal line indicates the median, while the boundaries of the boxes
denote the 25% and 75% quantile levels, respectively. The whiskers correspond to the most extreme data points not considered outliers, and the outliers
are plotted individually as ‘o’. For difﬁcult tracks, the proposed framework leads to a better separation between the gates and the quadrotor.

Fig. 5. An example of our proposed framework in simulation, as the quadrotor was moving towards the gate. The ﬁgures are arranged from left to right
with respect to the quadrotor’s movement. In each ﬁgure, the red arrow indicates the original deep policy control, and the green arrow shows the safe
action as inferred by the framework, while considering the uncertainty in the gate position.

that under uncertainty,

We evaluated the experiments also by a distance metric
deﬁned in (2). For every trial, we recorded the minimum
distance between the quadrotor and the next gate at each
time step, which indicates how close the quadrotor was to a
possible collision. If a trial ended in collision, then the score
is zero. Fig. 4b shows the minimum distance values seen,
averaged for each difﬁculty level. The results show that our
proposed safety method considering uncertainties achieves
the best performance overall. While we sometimes observe
lesser minimum distances when considering uncertainty, this
can be attributed to the fact
the
obstacle is artiﬁcially inﬂated to a larger size. A visualization
of safe control commands and trajectories are shown in Fig.
4a and Fig. 5, applied to the BC unconstrained network. In
Fig. 4a, we show the differences between the original policy
trajectories and the safety controls with and without con-
sidering uncertainties. The trajectories are almost the same
for the ﬁrst three difﬁculty levels, but when the difﬁculty
level increases to 1.5, then the only safe trajectory is when
considering safety, which leads out of track. For the same
track,
the original policy and the safety method lead to
a collision with the second and fourth gates, respectively.
A detailed control visualization is shown in Fig. 5, where
the actions of the original policy are violating the safety
constraints and would lead to a possible collision with a

gate, whereas the safety method with uncertainty computes
a collision-free action.

We have observed a few limitations of the proposed
method in our experiments. For example, when the angle
between the current gate and next gate was too sharp but still
in the ﬁeld of view of the quadrotor, occasionally, all methods
caused a collision with the current gate. Another issue we
encountered was when the quadrotor starts a trial facing a
gate’s pole, and in close proximity. In this situation, the
quadrotor most of the time collided with the gate for all the
methods, which could have been because of signiﬁcant noise
in gate estimated position and estimation errors exceeding
the worst-case values considered. One way to overcome such
an issue is to consider optimization for the next two gates
instead of only one.

VI. CONCLUSIONS AND FUTURE WORK

We have presented a framework for safe deep control
policies for the task of drone-racing. At the heart of our
method are safety barrier certiﬁcates, used to minimally
change the controller to ensure forward invariance of safety.
The main idea to overcome uncertainty in obstacle position
is considering the worst case in error threshold of predicted
obstacle pose and building a pre-computed distance map
through Euclidean signed distance ﬁeld. Our experiments
show that using our proposed method is elevating the safety

[18] X. Li and C. Belta, “Temporal logic guided safe reinforcement learning
using control barrier functions,” arXiv preprint arXiv:1903.09885,
2019.

[19] W. Luo and A. Kapoor, “Airborne collision avoidance systems with
probabilistic safety barrier certiﬁcates,” in NeurIPS 2019, 2019.
[20] A. Bajcsy, S. Bansal, E. Bronstein, V. Tolani, and C. J. Tomlin, “An
efﬁcient reachability-based framework for provably safe autonomous
navigation in unknown environments,” 2019.

[21] Y. Chen, H. Peng, and J. Grizzle, “Obstacle avoidance for low-speed
autonomous vehicles with barrier function,” IEEE Transactions on
Control Systems Technology, vol. 26, no. 1, pp. 194–206, 2017.
[22] G. Yang, B. Vang, Z. Serlin, C. Belta, and R. Tron, “Sampling-
based motion planning via control barrier functions,” in Proceedings
of the 2019 3rd International Conference on Automation, Control and
Robots, 2019, pp. 22–29.

[23] G. Shi, X. Shi, M. OConnell, R. Yu, K. Azizzadenesheli, A. Anand-
kumar, Y. Yue, and S.-J. Chung, “Neural lander: Stable drone landing
control using learned dynamics,” in 2019 International Conference on
Robotics and Automation (ICRA).

IEEE, 2019, pp. 9784–9790.

[24] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-ﬁdelity
visual and physical simulation for autonomous vehicles,” in Field and
service robotics. Springer, 2018, pp. 621–635.

[25] E. Kaufmann, M. Gehrig, P. Foehn, R. Ranftl, A. Dosovitskiy,
V. Koltun, and D. Scaramuzza, “Beauty and the beast: Optimal meth-
ods meet learning for drone racing,” in 2019 International Conference
on Robotics and Automation (ICRA).

IEEE, 2019, pp. 690–696.

[26] G. Li, M. Mueller, V. Michael Casser, N. Smith, D. Michels,
and B. Ghanem, “Oil: Observational imitation learning,” Robotics:
Science and Systems XV, Jun 2019.
[Online]. Available: http:
//dx.doi.org/10.15607/RSS.2019.XV.005

[27] E. Kaufmann, A. Loquercio, R. Ranftl, A. Dosovitskiy, V. Koltun,
and D. Scaramuzza, “Deep drone racing: Learning agile ﬂight in
dynamic environments,” in Proceedings of The 2nd Conference on
Robot Learning, ser. Proceedings of Machine Learning Research,
A. Billard, A. Dragan, J. Peters, and J. Morimoto, Eds., vol. 87.
PMLR, 29–31 Oct 2018, pp. 133–145.
[Online]. Available:
http://proceedings.mlr.press/v87/kaufmann18a.html

[28] R. Bonatti, R. Madaan, V. Vineet, S. Scherer, and A. Kapoor, “Learn-
ing controls using cross-modal representations: Bridging simulation
and reality for drone racing,” arXiv preprint arXiv:1909.06993, 2019.
[29] S. L. Herbert, M. Chen, S. Han, S. Bansal, J. F. Fisac, and C. J.
Tomlin, “Fastrack: a modular framework for fast and guaranteed safe
motion planning,” in 2017 IEEE 56th Annual Conference on Decision
and Control (CDC).

IEEE, 2017, pp. 1517–1522.

rate of deep control policies, while still achieving competi-
tive results. Future work includes investigating a prediction
process of more than one gate position. We would also
be interested in exploring the use of this method during
training time of deep control policies, to balance safety and
performance before execution.

ACKNOWLEDGMENT

We would like to thank Ratnesh Madaan and Rogerio
Bonatti for their inputs regarding the baseline perception and
control policies; as well as Matthew Brown and Nicholas
Gyde for their help with the simulations.

REFERENCES

[1] P. Long, T. Fanl, X. Liao, W. Liu, H. Zhang, and J. Pan, “Towards
optimally decentralized multi-robot collision avoidance via deep re-
learning,” in 2018 IEEE International Conference on
inforcement
Robotics and Automation (ICRA).

IEEE, 2018, pp. 6252–6259.

[2] M. Bouton, A. Nakhaei, K. Fujimura, and M. J. Kochenderfer,
“Safe reinforcement learning with scene decomposition for navigating
complex urban environments,” arXiv preprint arXiv:1904.11483, 2019.
[3] Y. C. Tang, J. Zhang, and R. Salakhutdinov, “Worst cases policy

gradients,” arXiv preprint arXiv:1911.03618, 2019.

[4] L. Wang, A. Ames, and M. Egerstedt, “Safety barrier certiﬁcates for
collisions-free multirobot systems,” IEEE Transactions on Robotics,
vol. PP, pp. 1–14, 02 2017.

[5] Y. Pan, C.-A. Cheng, K. Saigol, K. Lee, X. Yan, E. Theodorou, and
B. Boots, “Agile autonomous driving using end-to-end deep imitation
learning,” in Robotics: science and systems, 2018.

[6] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforce-
ment learning,” 2015.

[7] J. Garcia and F. Fern´andez, “Safe exploration of state and action spaces
in reinforcement learning,” Journal of Artiﬁcial Intelligence Research,
vol. 45, pp. 515–564, 2012.

[8] J. Achiam, D. Held, A. Tamar, and P. Abbeel, “Constrained policy
optimization,” in Proceedings of the 34th International Conference on
Machine Learning-Volume 70.

JMLR. org, 2017, pp. 22–31.

[9] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, “Re-
luplex: An efﬁcient smt solver for verifying deep neural networks,” in
International Conference on Computer Aided Veriﬁcation. Springer,
2017, pp. 97–117.

[10] C. Liu, T. Arnon, C. Lazarus, C. Barrett, and M. J. Kochender-
fer, “Algorithms for verifying deep neural networks,” arXiv preprint
arXiv:1903.06758, 2019.

[11] S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari, “Learning and
veriﬁcation of feedback control systems using feedforward neural
networks,” IFAC-PapersOnLine, vol. 51, no. 16, pp. 151–156, 2018.
[12] J. F. Fisac, N. F. Lugovoy, V. Rubies-Royo, S. Ghosh, and C. J. Tomlin,
“Bridging hamilton-jacobi safety analysis and reinforcement learning,”
in 2019 International Conference on Robotics and Automation (ICRA).
IEEE, 2019, pp. 8550–8556.

[13] D. Sadigh and A. Kapoor, “Safe control under uncertainty with prob-
abilistic signal temporal logic,” in Proceedings of Robotics: Science
and Systems, AnnArbor, Michigan, June 2016.

[14] A. D. Ames, X. Xu, J. W. Grizzle, and P. Tabuada, “Control barrier
function based quadratic programs for safety critical systems,” IEEE
Transactions on Automatic Control, vol. 62, no. 8, pp. 3861–3876,
2016.

[15] A. D. Ames, S. Coogan, M. Egerstedt, G. Notomista, K. Sreenath,
and P. Tabuada, “Control barrier functions: Theory and applications,”
2019 18th European Control Conference (ECC), Jun 2019. [Online].
Available: http://dx.doi.org/10.23919/ECC.2019.8796030

[16] R. Cheng, G. Orosz, R. M. Murray, and J. W. Burdick, “End-to-end
safe reinforcement learning through barrier functions for safety-critical
continuous control tasks,” in Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, vol. 33, 2019, pp. 3387–3395.

[17] M. Ohnishi, L. Wang, G. Notomista, and M. Egerstedt, “Barrier-
certiﬁed adaptive reinforcement learning with applications to brushbot
navigation,” IEEE Transactions on Robotics, vol. 35, no. 5, pp. 1186–
1205, 2019.

