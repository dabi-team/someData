2
2
0
2

l
u
J

8

]

G
L
.
s
c
[

1
v
6
7
9
3
0
.
7
0
2
2
:
v
i
X
r
a

Generalization-Memorization Machines
Zhen Wang, Yuanhai Shao∗

1

Abstract—Classifying the training data correctly without over-ﬁtting is one of the goals in machine learning. In this paper, we propose
a generalization-memorization mechanism, including a generalization-memorization decision and a memory modeling principle. Under
this mechanism, error-based learning machines improve their memorization abilities of training data without over-ﬁtting. Speciﬁcally,
the generalization-memorization machines (GMM) are proposed by applying this mechanism. The optimization problems in GMM
are quadratic programming problems and could be solved efﬁciently. It should be noted that the recently proposed generalization-
memorization kernel and the corresponding support vector machines are the special cases of our GMM. Experimental results show the
effectiveness of the proposed GMM both on memorization and generalization.

Index Terms—Classiﬁcation, support vector machines, generalization-memorization mechanism, memory machines, generalization-
memorization kernel.

✦

1 INTRODUCTION

M EMORY is both important for humans and learning

machines [1], [2]. For a learning machine, the label
memory represents the ability to memorize the training
samples, also known as the ability of classifying the training
samples correctly. Generally, classifying all of the training
samples correctly is called zero empirical risk. Most of
machine learning models pursue a well generalization per-
formance by minimizing the empirical risks with regular-
ization terms [3], [4]. However, the problem of over-ﬁtting
may appear if we pursue to minimize the empirical risk in
machine learning, especially for small data problems. Recent
studies [1], [5], [6] show that the deep neural networks
(DNN) obtain almost zero empirical risk with a well gen-
eralization performance. A natural question is whether all
machine learning models could obtain almost zero empirical
risks with well generalization performances.

Recently, Vapnik and Izmailov [7] studied the memo-
rization problem of support vector machines (SVM) [8], and
a generalization-memorization kernel which is the sum of
two RBF kernels [9] is introduced into SVM to improve
its memorization ability, called SVMm. By adjusting the
generalization-memorization kernel parameters properly,
SVMm could classify training samples correctly while with
a comparable generalization compared with SVM. Though
[7] shows a path to realize both of the generalization and
memorization of SVM, the performances of applying the
generalization-memorization kernel into other SVM-type
models remain unclear. More importantly, it is difﬁcult to
apply the generalization-memorization kernel into other
non-kernel machine learning models [3], [10]–[12] by this
path. Therefore, how to keep machine learning models
have almost zero empirical risks with well generalization
performances is still a problem.

• Z. Wang is with the School of Mathematical Sciences, Inner Mongolia
University, Hohhot, 010021, P.R. China. Email: wangzhen@imu.edu.cn.
the Management
author)
School, Hainan University, Haikou, P.R. China. Email: shaoyuan-
hai@hainanu.edu.cn.

• Y.H. Shao

(*Corresponding

is with

Manuscript received xx, xx; revised xx, xx.

In this paper, we propose a generalization-memorization
mechanism for machine learning models to improve the
memorization ability with competitive generalizations. This
mechanism contains a new generalization-memorization de-
cision and a memory modeling principle. The memorization
ability of the new decision relies on a memory cost function,
and it affects the generalization by a new memory inﬂu-
ence function. In memory modeling, the principle gives a
path to preserve the generalization. Further, we apply the
generalization-memorization mechanism in SVM to propose
the generalization-memorization machines (GMM), which
can obtain zero empirical risks with competitive generaliza-
tions.

The main contributions of this paper are:

(i) For label memory problems, we propose a generalization-
memorization mechanism to guide the generalization and
memorization of the learning models. The proposed mech-
anism could be applied into various error-based machine
learning models directly, whatever kernel or non-kernel
based models.
(ii) Generalization-memorization machines (GMM) are pro-
posed based on the above generalization-memorization
mechanism, including a hard version (HGMM) and a soft
version (SGMM). The HGMM requires to classify all of the
training samples correctly, and the necessary and sufﬁcient
condition that HGMM has a zero empirical risk is given.
In theory, a convenient sufﬁcient condition is also given
for practical applications. When the zero empirical risk is
unnecessary, the HGMM is relaxed to the SGMM to obtain a
competitive generalization ability. Particularly, our HGMM
and SGMM are very simple and their problem sizes are the
same as SVM.
(iii) The generalization-memorization decision endows the
generalization-memorization kernel of SVMm [7] with more
clear geometric interpretations, and it is proved that the
SVMm is a special case of our SGMM.
(iv) Experimental results conﬁrm the well performance of
the proposed GMM on both memorization and generaliza-
tion.

This paper is organized as follows. In the next section,

 
 
 
 
 
 
a brief overview of SVM and SVMm is given. Section 3
establishes the new generalization-memorization mecha-
nism, including a generalization-memorization decision and
a memory modeling principle. Section 4 proposes the GMM
and discusses the relationship between GMM and SVMs.
Numerical experiments and concluding remarks are given
in the last two sections.

2 REVIEW OF SVM AND SVMm
In this paper, we consider a binary classiﬁcation problem in
the n-dimensional real space Rn. The set of training samples
(xi, yi)
is represented by T =
∈
|
{
Rn is the input and yi
+1,
is the corresponding
∈ {
label. The training samples together with their labels are
Rn×m and diagonal matrix Y with
organized in matrix X
diagonal elements Yii = yi (i = 1, . . . , m), respectively.

i = 1, 2, ..., m

, where xi

−

∈

}

1

}

The classical SVM [8], [13] constructs a classiﬁcation
R

+ b with w

Rd and b

decision f (x) =
by solving

w, ϕ(x)
i
h

∈

∈

m

2

1

w

ξi,

s.t.

Xi=1

+ C

+ b)

ξi, ξi

min
w,b,ξ·

1
2 ||
||
w, ϕ(xi)
yi(
≥
i
h
denotes the inner product, ϕ(
·

· · ·
(1)
) is a mapping,
where
C is a positive parameter, and ξi is a slack variable. For a
new sample x, it would be classiﬁed into positive class with
w, ϕ(x)
+ b > 0, and otherwise, it is classiﬁed
y = +1 if
i
h
1. Generally, we solve the
into the negative class with y =
dual problem of (1) as

0, i = 1,

, m,

−

≥

−

·i

h·

,

m

Pi=1

balance the generalization and memorization. Its decision
becomes to

2

f (x) =

yiαi(K1(xi, x) + τ K2(xi, x)) + b.

(5)

Due to the memorization kernel K2(xi, x) with very large
σ∗
, it only affects a small area near xi, and it will not greatly
increase the Vapnik-Chervonenkis (VC) dimension [15] of
the decision.

However, when and why SVMm works for memoriza-
tion are implicit, and its geometric interpretations are un-
clear due to lacking of the primal problem. Thus, it is
indistinct to apply the generalization-memorization kernel
to other SVM-type models, and it is difﬁcult to apply this
kernel to other non-kernel learning models to improve the
memorization ability.

3 GENERALIZATION-MEMORIZATION MECHANISM
The goal of the generalization-memorization mechanism is
to classify the training data correctly with a well generaliza-
tion performance. Taking SVM as an example, we give some
observations.

3.1 Memory cost function

Recalling the SVM’s primal problem (1), for each training
sample (xi, yi)

T , we have
yif (xi) = yi(w⊤ϕ(xi) + b)

∈

1

−

≥

ξi.

(6)

To classify all of the training samples correctly, we modify
the decision as

min
α
s. t.

1
2 α⊤YK(X, X)Yα
e⊤Yα = 0, 0

α

≤

≤

e⊤α

−
C,

(2)

where

g(x) = w⊤ϕ(x) + b +

ξ(x),
I

(7)

Rm is the Lagrangian multiplier vector, K(
) =
where α
·
∈
)
), ϕ(
ϕ(
is a kernel function [9], and e is a vector of ones
h
i
·
·
with an appropriate dimension. Speciﬁcally, x is classiﬁed
as +1 or

1 according to decision

·

,

−

f (x) =

m

Pi=1

yiK(x, xi)αi + b.

(3)

The above SVM has a nice generalization ability for
many problems, but it cannot always classify all of the train-
ing samples correctly. Intuitively, for an intelligent model, it
should have the ability to remember what it has learned.
If we choose a particular kernel such as an RBF kernel
function [14] with a very small variance, SVM could classify
the training samples correctly, but its generalization ability
would degenerate largely, which is the over-ﬁtting problem.
Recently, Vapnik and Izmailov [7] hired a combination of
two RBF kernels into dual problem (2) to propose the
SVMm, which memories the training samples with well gen-
eralization. Without proposing the primal problem, SVMm
directly solves the following dual problem

min
α
s. t.

1
2 α⊤Y(K1(X, X) + τ K2(X, X))Yα
e⊤Yα = 0, 0
α

C,

≤

≤

e⊤α

−

(4)

where K1(
·
σ, K2(
·
(σ∗
≫

,

,

·

) is a generalization RBF kernel with parameter
) is a memorization RBF kernel with parameter σ∗
·
σ > 0), and τ > 0 is a weighting parameter to

∈

(cid:26)

(8)

X,

ξ(x) =
I

if x = xi,
xi
∃
otherwise.

yiξi,
0,
Thus, for each training sample, yig(xi)
1 > 0 always
holds true with i = 1, . . . , m, i.e., all of the training samples
are classiﬁed correctly by (7), which realizes an extremely
high memorization ability of SVM. For an unknown sample
except it is just the one of the training samples, its decision
is the same as the standard SVM. This is an extreme case
that SVM memorizes the training samples easily.

≥

Without computing ξi, decision (7) can be more general

as

where

g(x) = f (x) +

(x),

I

(x) =

I

(cid:26)

yic(xi),
0,

if x = xi,
xi
∃
otherwise.

∈

X,

(9)

(10)

Function c(x) is called memory cost function w.r.t. x, where
X is its domain of deﬁnition and f (x) can be any previous
decision function. The larger c(x) , the more cost to mem-
orize x. If c(xi) is large enough, training sample xi will be
classiﬁed correctly, e.g., c(xi)

≥
Decision (9) with memory cost function is a realistic path
for memorizing training samples. This decision consists of
two parts, i.e., the generalization function f (x) and memo-
(x). From this point of view, for machine
rization function

ξi in SVM.

I

learning models where empirical risks are concerned, we
can hire (9) by setting c(xi)
loss(f (xi), yi). In other
words, we can classify all of the training samples correctly
by using proper decision (9) no matter whether the corre-
sponding learning model has good memorization ability. At
the same time, the generalization of f (x) is retained.

≥

3.2 Memory inﬂuence function

Though decision (9) could memorize all training samples,
there is little impact on generalization. In order to study the
inﬂuence of memorization on generalization, we construct a
new generalization-memorization decision as

g(x) = f (x) +

yic(xi)δ(xi, x),

(11)

where δ(xi, x) is a function w.r.t. x to measure the inﬂuence
of memorizing xi, called memory inﬂuence function. Gen-
erally, the memory inﬂuence function could be different. For
instance, supposing the samples that are very similar to xi
would be classiﬁed into the same class of xi, δ(xi, x) could
be a similarity function between xi and x. Now, we give
some speciﬁc memory inﬂuence functions, e.g.,
2), σ > 0,

δ(xi, x) = exp(

(12)

xi

x

σ

−

−
||
xi
x
1,
||
0, otherwise,

−

if

||

|| ≤

δ(xi, x) =

(cid:26)

εi, εi > 0,

m

Pi=1

δ(xi, x) = max

ρ

{

− k

x

xi

, 0

k

}

−

, ρ > 0,

or

δ(xi, x) =

1, x is k nearest neighbors of xi,
0, otherwise.

(cid:26)

The above functions measure the similarity between xi and
x, where the former three functions are symmetric and the
last one is not. Each training sample would have a memory
inﬂuence on prediction only if it has a nonzero memory cost.
The ﬁnal decision (11) is our generalization-memorization
decision.

3.3 Memory model principle

Based on generalization-memorization decision (11), we
discuss the principle of constructing the memory model.
Recalling the traditional SVM, if we use its training model
with decision (11), all of the training samples would be
classiﬁed correctly but the memory cost and memory inﬂu-
ence functions should be selected carefully. At this time, the
training objective is inconsistent with this decision, which
may bring degradation in modeling. As pointed by previ-
ous studies [1], [16], the memorized training samples may
have positive or negative effects on the model, which may
improve or deteriorate its generalization ability. Notice that
the memorization ability affects the empirical risk largely,
and decision (11) can be a very complexity function with a
much higher VC dimension [15]. We show some results on
the expected and empirical risks.

) and Ggap =
Let h be the VC dimension of g(
·
(ln (2m/h + 1)
(0, 1). Then,
ln η/4)/(m/h) where η
expected risk R(g) is bounded by empirical risk Remp(g)
and a conﬁdence interval (i.e., generalization gap) from the

−

∈

(13)

(14)

(15)

statistical learning theory (Section 5 in [7]) as follows. If
Remp(g)

Ggap,

≫

3

holds true with probability 1

η, and if Remp(g)

R(g)

≤

Remp(g) +

2Ggap

p

−
Remp(g) + √2Ggap

R(g)

≤

(16)

Ggap,

≪

(17)

−

p

holds true with probability 1

η.
Note that the square root of a small number is much
larger than that number. Inequality (16) indicates that conﬁ-
2Ggap cannot be ignored though empirical
dence interval
risk Remp(g) is much larger than Ggap. Instead of reducing
the VC dimension as much as possible, the previous learn-
ing models balance the empirical risk and VC dimension
to control the expected risk. However, for a learning model
with high memorization ability, it often can achieve zero or
almost zero empirical risk. Thus, inequality (17) indicates
that we should reduce the VC dimension as much as possi-
ble (i.e., make the model as simple as possible) for a memory
model to improve the generalization ability. Now, a simple
and intuitive assumption for memory modeling appears.

• When a learning machine is built to memorize train-
ing samples, we should try not to increase the VC
dimension of decision-making and the model com-
plexity.

Based on the above principle, we may design different
memory cost and memory inﬂuence functions to propose
the corresponding memory models.

4 GENERALIZATION-MEMORIZATION MACHINES

By applying the generalization-memorization mechanism
into SVM, we build the generalization-memorization ma-
chines (GMM) for classiﬁcation in this section.

4.1 Hard and soft generalization-memorization ma-
chines

To memorize all of the training samples, we propose a hard
generalization-memorization machine (HGMM) under the
large margin principle. Our HGMM considers a quadratic
programming problem (QPP) as
2 + λ

w

c

2

min
w,b,c

1
2 ||
||
w, ϕ(xi)
s.t. yi(
i
h

2 ||
||
+ b +

m

yjcjδ(xj , xi))

1, i = 1, ..., m,

Pj=1
(18)
where λ is a positive parameter, c = (c1, . . . , cm)⊤
denotes
the memory costs of the training samples, and δ(xi, x) is
a user-deﬁned memory inﬂuence function. The decision of
our HGMM is

≥

m

Pi=1

g(x) =

w, ϕ(x)
i
h

+ b +

yjcjδ(xi, x).

(19)

Obviously, we hire generalization function f (x) =
w, ϕ(x)
+ b, set the memory cost be variables, and pre-
h
i
deﬁne the memory inﬂuence function in decision (11). From
the constraints of (18), it requires to memorize all of the
training samples. The objective of (18) seeks the large mar-
gin with memory costs as lower as possible, and it controls

the complexity of the model meanwhile. For the sake of
argument, we reformulate problem (18) as

w

1
2 ||

2 + λ
min
w,b,c
s.t. Y(ϕ(X)⊤w + be + ∆Yc)

2 ||

||

||

c

2

(20)

e,

≥

∈

Rm×m with element δ(xj , xi) (i, j = 1, . . . , m).

where ∆
The following theorem holds apparently.
Theorem 4.1. The empirical risk of HGMM is zero if and
only if there is at least a feasible solution to problem
(20).

The feasibility of problem (20) strongly depends on the
properties of memory inﬂuence matrix ∆. In fact, we have
the following sufﬁcient condition for practical applications.
Theorem 4.2. The empirical risk of HGMM is zero if ∆ is

nonsingular.

Proof: Consider the feasibility of Y(ϕ(X)⊤w + be)

≥
e. If it is infeasible, we can slack it by adding an extra vector
ξ, i.e., Y(ϕ(X)⊤w + be)
ξ always holds; Otherwise,
e
−
set ξ = 0. Besides, if ∆ is nonsingular, the linear system
of equations Y∆Yc = ξ must have a solution, which
indicates the feasibility of problem (20). From Theorem 4.1,
the conclusion holds.

≥

Now, we consider the dual problem of (18) via kernel
Rm be the Lagrangian multiplier vector,

tricks [9]. Let α
the Karush-Kuhn-Tucker (KKT) conditions [17] of (20) are

∈

w = ϕ(X)Yα, e⊤Yα = 0, c = 1
λ

0.
(21)
By substituting conditions (21) into problem (20), its dual
problem is equivalent to

Y∆⊤Yα, and α

≥

min
α
s.t.

1

2 α⊤Y(K(X, X) + 1
e⊤Yα = 0, α

0.

≥

λ ∆∆⊤)Yα

e⊤α

−

(22)

After solving the above QPP, the ﬁnal decision is

g(x) =

m

Pi=1

yiαiK(xi, x) + b +

m

Pi=1

yiciδ(xi, x).

(23)

In addition, by ﬁnding a nonzero component αk in solution

α, we have b = yk

yk

−

m

Pi=1

yi(αiK(xi, xk) + ciδ(xi, xk)).

When the HGMM is infeasible or memorizing all of the
training samples is unnecessary (e.g., data with label noises),
our HGMM could be relaxed to a soft generalization-
memorization machine (SGMM) by considering a QPP as

min
w,b,c,η

s.t.

2 + C
m

c

||

w

2 λ
||

2 + 1

1
2 ||
||
w, ϕ(xi)
yi(
i
h
0, i = 1, ..., m,

+ b +

ηi

Pj=1

≥

m

ηi

Pi=1
yjcjδ(xi, xj))

1

−

≥

ηi,

(24)
where C is a positive parameter and ηi is a slack variable.
Instead of memorizing all of the training samples in the
HGMM, our SGMM memorizes the training samples by
adjusting C, and it approximates to the HGMM when C
is large enough. Accordingly, the dual problem of (24) is

min
α
s.t.

1

2 α⊤Y(K(X, X) + 1
e⊤Yα = 0, 0

λ ∆∆⊤)Yα
C.

α

≤

≤

e⊤α

−

(25)

3

2.5

2

1.5

1

0.5

0
0.6

Class +1
Class -1

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

2.6

3

2.5

2

1.5

1

0.5

0
0.6

4

Class +1
Class -1

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

2.6

(a) HGMM

(b) SGMM

Fig. 1: A toy example to show the memorization ability of
HGMM and SGMM, where memory inﬂuence function (12)
and K(xi, xj) = x⊤
xj are hired in problems (22) and (25).
i

After solving problem (25) and computing b with 0 < αk <
C similar to HGMM, our SGMM predicts samples by (23).

Fig. 1 illustrates the memorization abilities of our
HGMM and SGMM on a toy example. It can be seen that
the HGMM classiﬁes all of the training samples correctly
with a smooth boundary similar to the linear SVM, while
the SGMM correctly classiﬁes most of the training samples
except a positive sample that is surrounded by many neg-
ative samples. Thus, our HGMM forces to memorize all of
the training samples, and our SGMM has the ability to select
and memorize samples that are easy to memorize.

4.2 Discussion

In this subsection, let us discuss the relationship between
our GMM and SVMs. By analyzing their solutions, we
obtain the relationship between our GMM and the classical
SVM [8], [13] immediately.
Theorem 4.3.

i) If the memory costs of training samples
are all zeros after training our HGMM or SGMM, the
HGMM is equivalent to hard margin SVM (i.e., problem
(1) without slack variables), and the SGMM is equivalent
to soft margin SVM (i.e., problem (1)).
ii) If the selected δ(xi, x) keeps ∆ be the identity matrix,
our HGMM is equivalent to L2 loss SVM with decision
(11).
For Vapnik’s SVMm [7], we have the following conclu-

sion.
Theorem 4.4. There exists a memory inﬂuence function such
1
that by setting K2(X, X) = ∆∆⊤
λ = τ , dual
problem (4) of SVMm is equivalent to dual problem (25)
of our SGMM, and decision (5) of SVMm is equivalent to
decision (23) of our SGMM.

and

,

·

,

Proof: For an RBF kernel K2(
·

), the kernel matrix
K2(X, X) is symmetric apparently. Thus, we have the real
symmetric matrix decomposition K2(X, X) = P P ⊤
. By
selecting the memory inﬂuence function δ(
) properly, it
·
1
is easy to build P = ∆. Then, by setting
λ = τ , problem (4)
is equivalent to problem (25). On the other hand, substitute
the KKT condition c = 1
Y∆⊤Yα into decision (23), and let
λ
1
λ = τ , K(x, X) = K1(x, X) and K2(x, X) = δ(x, X)∆⊤
.
We immediately get the conclusion that decision (23) is
equivalent to decision (5).

The above theorem shows that SVMm is a special case
of our SGMM. Moreover, it reveals that primal problem
(24) of SGMM with a speciﬁc memory inﬂuence function

·

TABLE 1: Details of benchmark datasets

Name
Bupa
Echocard

ID
(a)
(b)
(c) Heartc
(d) Heartstatlog
(e) Hepatitis
Hourse
(f)
Ionosphere
(g)
Sonar
(h)
(i)
Spect
(j) Wpbc

m
345
131
303
270
155
300
351
208
267
198

n
6
10
14
13
19
26
33
60
44
34

Name
ID
Bank
(k)
Creditcard
(l)
Electrical
(m)
German
(n)
(o)
Htru2
(p) Musk
Qsar
(q)
Ring
(r)
Shoppers
(s)
Two
(t)

m
45,211
30,000
10,000
1,000
17,898
6,598
1,055
7,400
12,330
7,400

n
16
23
13
20
8
166
41
20
17
20

I

≈

≈

is actually the primal problem of SVMm. Thus, the clear
geometric interpretations of SGMM explain when and why
SVMm works. In the SVMm, RBF kernel matrix K2(X, X)
with large σ∗
is roughly an identity matrix I, and thus, we
K2(X, X) (i.e.,
have its decomposition matrix ∆
the memorization term is similar to (10)). Subsequently, we
have memory costs c
τ α. Thus, the larger τ , the higher
memorization ability of SVMm, and memorization kernel
) is actually a memory inﬂuence function controls
K2(
·
the inﬂuence region. If we seek the zero empirical risk
by SVMm, a feasible method is to set C and σ∗
large
sufﬁciently. In fact, the SVMm approximates to our feasible
HGMM in such situation. Finally, we conclude that the
memorization ability of SVMm could be maintained by
many other different memorization kernel K2(
) whereas
·
not merely the special RBF kernel.

≈

,

·

,

·

1

2

or SVM

σ
−
i =

xi
−
10,

2),
xj
k
9, . . . , 5

5 EXPERIMENTS
This section analyzes the performance of our HGMM
and SGMM compared with SVMs on several benchmark
datasets1. Table 1 shows the details of datasets. Thereinto,
the classical SVM [8], [13] with the linear or RBF kernel is de-
noted as SVM
respectively, the linear generaliza-
tion kernel and RBF memorization kernel are hired in SVMm
[7], and the linear kernel is hired in our HGMM and SGMM
(available at https://github.com/gamer1882/GMM). All of
these models were implemented by MATLAB 2017a on a
PC with an Intel Core Duo Processor (double 4.2 GHz) with
32GB RAM, and their QPPs were solved by the same algo-
rithm with the same tolerance. For RBF kernel K(xi, xj ) =
its parameter σ was selected from
exp(
2i
, and the other tradeoff parameters
{
i =
.
of these models were selected from
Firstly, we test the memorization ability and its inﬂuence
of our HGMM on several small size datasets. The memory
inﬂuence functions (i.e., formations (12), (13), (14) and (15))
were preloaded in our HGMM and evaluated by the m-
fold cross validation (i.e., level-one-out validation, LOO for
short). We set the baseline by setting the memory inﬂuence
function be an identity matrix which is actually L2 loss SVM
with decision (7) according to Theorem 4.3 (ii). Thereinto, all
the parameters εi in (13) were set to equal to each other and
, param-
selected from
eter ρ in (14) was selected from
,
}
and parameter k in (15) was selected from
. Ta-
ble 2 reports their highest LOO training and testing accura-
cies. From Table 2, it is observed that our HGMM with either

5, 1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001
}
9, . . . , 10

10,
−
1, 2, . . . , 7

7, . . . , 7

−
{

k
−

i =

2i

2i

8,

−

−

−

{

{

}

{

}

}

|

|

|

1. http://archive.ics.uci.edu/ml/index.php

TABLE 2: LOO accuracies (%) of HGMM with memory
inﬂuence functions (12)-(15) on benchmark datasets

5

ID

(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
Avg

All
train
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00

Baseline
test
69.28
90.08
100.0
84.81
87.10
81.00
90.31
78.37
82.02
82.83
84.58

(12)
test
72.46
90.08
100.0
85.19
87.74
82.67
95.16
87.98
82.77
83.33
86.74

(13)
test
70.43
90.08
100.0
84.81
87.10
81.00
95.16
81.73
82.02
82.83
85.52

(14)
test
72.46
90.08
100.0
84.81
87.74
81.67
96.01
87.98
82.02
82.83
86.56

(15)
test
69.28
90.08
100.0
84.81
87.10
81.00
90.60
87.02
82.02
82.83
85.47

memory inﬂuence function has 100% training accuracies on
all of these datasets. Compare with the training accuracy of
baseline, the memory inﬂuence function does not directly
affect the memorization ability, and thus the memory cost
function mainly decides it. Compared with the testing ac-
curacy of baseline, our HGMM with either memory inﬂu-
ence function performs better on the LOO testing, which
indicates that our generalization-memorization mechanism
could improve the learning performance. Among these
functions, formation (12) obtains the highest average LOO
testing accuracy, so we hire formation (12) for our HGMM
and SGMM in the rest experiments.

2

1

1

, SVM

In the following, we compare the memorization and gen-
and SVMm
eralization of our HGMM with the SVM
on the small datasets used in Table 2 by the LOO validation.
The highest average LOO training accuracies and their stan-
dard deviations together with the corresponding highest
LOO testing accuracies were recorded in Table 3, and the
best testing accuracies for each dataset and average result
were bold. It can be seen from Table 3 that these models ex-
can memorize all of the training samples. Thus,
cept SVM
2
, SVMm and our HGMM have a much higher
the SVM
. Compared to the 100%
memorization ability than SVM
training accuracies, the testing accuracies do not decrease
, SVMm and our HGMM, especially
too much for the SVM
for our HGMM that owns the best LOO testing accuracies on
all of these datasets. It indicates that our HGMM is a suitable
path to improve the generalization ability without over-
ﬁtting. To sum up, our HGMM obtains better performance
on both of the memorization and generalization abilities.

2

1

Next, we implemented these models on some larger
datasets to further test their generalization abilities with the
best empirical risks. For each dataset from (k)-(t) in Table 1,
the number of training samples were selected randomly and
increased from 50 to 500. We maintained that half of them
were from positive class and the other half were from nega-
tive class, and the rest consisted of the testing set. The above
procedure was repeated 20 times to compute the average
highest training accuracies and standard deviations together
with the corresponding highest average testing accuracies
and standard deviations, and the results were reported in
Table 4. Obviously, SVM
cannot get zero empirical risks on
most of datasets similar to the results in Table 3. Though
SVM
obtains zero empirical risks on all these datasets,
its average testing accuracies are around 50% on part of
datasets, e.g., on data (l), (q) and (s). The performance of

1

2

6

TABLE 3: LOO accuracies (%) on benchmark datasets

ID

(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
Avg

1
SVM
train(%)
71.50±0.33
93.22±0.40
100.0±0.00
86.68±0.24
90.43±0.61
81.53±0.29
95.08±0.26
99.21±0.35
91.02±0.37
90.86±0.43
89.95±0.33

2
SVM
train(%)
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00

SVMm
train(%)
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00

1
HardMSVM SVM
train(%)
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00
100.0±0.00

test(%)
69.28
86.26
100.0
85.19
83.23
81.33
86.89
74.04
80.90
79.29
82.64

2
SVM
test(%)
66.09
83.97
100.0
78.52
85.81
81.33
95.16
87.98
81.27
77.78
83.79

SVMm HGMM
test(%)
test(%)
72.46
72.17
90.08
90.08
100.0
100.0
85.19
84.07
87.74
84.52
82.67
81.00
95.16
91.74
87.98
78.37
82.77
81.27
83.33
82.83
86.74
84.61

1

with the linear kernel;

2

with the RBF kernel.

2

2

. Compared with SVM

SVM
on these datasets approximates to the random guess,
so it is inferred that the over-ﬁtting problem appears in
2
, SVMm and our HGMM do
SVM
not fall into the over-ﬁtting problem. Among these models,
our HGMM maintains the best average testing accuracies
with zero empirical risks on most of the comparisons and is
comparable with the best ones on the rest several compar-
isons, which supports the previous conclusions from Table
3.

Practically, the zero empirical risk may be unnecessary
in some real applications, e.g., the learning tasks with label
noises. In the following, we consider the performance of
these models compared with our HGMM and SGMM on
the benchmark datasets with label noises. For each dataset
from (k)-(t) in Table 1, 500 training samples were selected
randomly as the training set and the rest consisted of the
testing set. Thereinto, the labels of 5%, 10% or 15% training
samples were set to be opposite, and the actual training set
was the baseline. These models were implemented on the
datasets, and this procedure was repeated 20 times to record
the highest testing accuracies with standard deviations and
the corresponding average training accuracies with stan-
dard deviations in Table 5. From Table 5, we observe that:
i) The training accuracies of these models are not 100%
on many comparisons except our HGMM; ii) The testing
performances of SVM
are always lower than other models
and unstable with increasing the noises; iii) The testing
performances of the other four models decrease regularly
with increasing the noises; iv) Their rates of decline are
different, where the rates of SVM
and HGMM are larger
and the rate of SVMm and SGMM are lower; and v) The
testing performances of our SGMM are the highest on most
of the comparisons and comparable with the highest ones
on the rest comparisons.

1

2

2

1

From the above observations, it is inferred that the per-
is unacceptable with lower memorization
formance of SVM
ability on noises data, whereas the noises strongly affect the
generalization ability of models with higher memorization
and HGMM. The eclectic SVMm
ability such as the SVM
and our SGMM perform better than others to avoid the
inﬂuence of noises as much as possible and preserve the
generalization ability. Notice that our SGMM performs bet-
ter than SVMm due to it is a special case of SGMM, and
we may further improve the performance of SGMM by
choosing different memory inﬂuence functions. Therefore,
for the problem that zero empirical risk is unnecessary, our

SGMM is a competitive choice. Additionally, it should be
pointed out that SVMm, our HGMM and SGMM hired the
linear kernel in the experiments, and their generalization
abilities might be improved by choosing other nonlinear
kernels for speciﬁc problems.

6 CONCLUSIONS

In this paper, a general generalization-memorization mech-
anism has been presented by introducing the memory
cost and memory inﬂuence functions on decision. Further,
applying SVM into this mechanism, two generalization-
memorization machines (GMM) have been proposed, where
the hard GMM (HGMM) can memorize all of the training
samples and the soft GMM (SGMM) could abandon some
of them. The optimization problems of GMM are quadratic
programming problems similar to that of SVM. Addi-
tionally, the recently proposed generalization-memorization
kernel and SVMm are the special cases of our SGMM.
Experimental results show the better generalization ability
of our HGMM with zero empirical risk and the well adap-
tation of our SGMM for label noises. In the future work,
it is interesting to use this mechanism into other machine
learning problems. Deeper studies on memory cost and
memory inﬂuence functions are also necessary.

ACKNOWLEDGEMENTS

This work is supported in part by National Natural Sci-
ence Foundation of China (Nos. 61966024, 61866010 and
11871183), in part by the Natural Science Foundation of
Hainan Province (No.120RC449).

REFERENCES

[1]

S. Chatterjee, “Learning and memorization,” in International Con-
ference on Machine Learning, 2018, pp. 755–763.

[2] B. Yang, A. Ma, and P. Yuen, “Revealing task-relevant model mem-
orization for source-protected unsupervised domain adaptation,”
IEEE Transactions on Information Forensics and Security, vol. 17, pp.
716–731, 2022.

[3] L. Yann, B. Yoshua, and H. Geoffrey, “Deep learning,” Nature, vol.

521, no. 7553, pp. 436–444, 2015.

[4] M. Jordan and T. Mitchell, “Machine learning: trends, perspec-
tives, and prospects,” Science, vol. 349, no. 6245, pp. 255–260, 2015.
[5] D. Arpit, S. Jastrz˛ebski, N. Ballas, D. Krueger, E. Bengio, M. Kan-
wal, T. Maharaj, A. Fischer, A. Courville, and Y. Bengio, “A
closer look at memorization in deep networks,” in International
Conference on Machine Learning, 2017, pp. 233–242.

7

TABLE 4: Classiﬁcation accuracies (%) on grouped benchmark datasets with increasing training samples

ID

(k)

(l)

(m)

(n)

(o)

(p)

(q)

(r)

(s)

(t)

Training
number
50
100
150
200
300
400
500
50
100
150
200
300
400
500
50
100
150
200
300
400
500
50
100
150
200
300
400
500
50
100
150
200
300
400
500
50
100
150
200
300
400
500
50
100
150
200
300
400
500
50
100
150
200
300
400
500
50
100
150
200
300
400
500
50
100
150
200
300
400
500

1
SVM
train
72.00±8.03
67.05±7.57
66.70±6.93
66.45±6.32
66.05±5.30
65.70±4.59
67.12±4.57
65.90±6.70
66.10±4.96
64.57±5.05
62.58±5.28
63.53±3.45
62.26±4.26
61.83±4.29
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
94.98±16.08
79.98±23.51
73.83±22.70
74.50±10.30
67.35±12.55
64.37±5.32
66.22±4.66
65.67±3.33
65.56±3.73
–
82.00±22.16
80.30±9.94
79.87±13.71
80.38±14.37
79.98±14.82
79.91±9.97
76.58±18.15
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
83.75±22.03
70.03±8.21
70.73±5.90
71.95±6.49
70.92±6.80
–
89.10±17.53
70.80±6.93
70.57±3.17
69.38±3.49
69.97±3.01
68.89±2.88
69.03±2.43
75.20±19.66
64.25±6.26
64.10±6.70
62.15±6.65
61.58±7.35
62.22±5.10
62.45±4.83
100.00±0.00
100.00±0.00
99.13±3.88
98.32±5.34
98.12±5.95
89.22±14.16
86.28±3.58

2
SVM
train
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
–
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
–
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00

m

SVM
train
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
–
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
–
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00

HGMM
train
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
–
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
–
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00

1
SVM
test
78.55±18.63
69.00±15.46
77.75±7.94
70.61±16.99
75.01±9.94
76.18±8.44
75.73±7.02
61.75±8.68
63.15±13.30
66.13±9.43
63.28±11.67
65.72±12.17
59.61±13.39
61.96±13.94
91.29±3.10
94.60±1.83
96.41±1.06
97.13±0.83
93.27±15.83
78.12±23.91
72.55±22.54
65.82±8.71
69.45±13.78
62.45±11.29
65.74±9.32
63.41±8.81
66.05±7.81
–
79.83±19.77
81.37±15.74
80.68±23.74
85.09±18.14
81.13±23.50
86.95±11.70
79.75±25.22
69.78±4.93
74.31±3.98
77.88±3.16
80.07±2.27
82.26±1.69
83.33±1.76
84.58±1.52
73.47±4.00
69.82±11.24
68.09±10.98
69.38±9.53
71.54±6.76
69.08±8.31
–
64.61±5.60
66.54±2.90
67.62±2.41
65.91±1.84
68.78±2.08
66.73±2.84
68.13±1.99
61.79±15.59
62.80±17.85
70.06±13.86
64.92±18.02
66.95±19.88
72.20±14.44
74.57±14.98
93.86±2.35
94.98±1.25
94.65±2.46
94.32±3.72
94.02±4.33
87.08±11.31
86.33±3.37

2
SVM
test
70.88±5.48
72.17±3.49
71.31±3.47
71.82±3.07
72.72±2.62
70.51±1.66
71.34±1.32
50.24±8.08
52.26±7.23
52.17±4.82
47.30±22.79
51.05±25.14
50.71±26.56
46.22±1.77
91.07±2.30
93.57±1.64
94.62±0.93
94.47±0.65
95.20±0.37
95.61±0.51
96.15±0.44
64.15±3.54
65.15±3.57
65.88±2.66
66.04±2.41
66.49±2.51
66.62±2.19
–
84.68±4.65
78.79±4.11
79.31±2.87
81.24±2.22
77.84±1.92
79.16±2.12
80.25±1.72
86.98±3.11
91.43±1.27
91.79±1.08
92.42±0.99
93.12±0.89
93.95±0.69
94.42±0.57
79.96±3.58
82.26±1.99
83.29±1.67
83.98±1.58
84.10±1.59
54.66±28.35
–
97.77±0.42
98.09±0.27
98.17±0.22
98.26±0.19
98.34±0.12
98.39±0.09
98.43±0.09
69.11±6.08
71.13±3.32
71.94±3.59
72.26±3.15
50.54±3.58
52.06±2.66
53.47±2.36
96.29±1.08
96.67±0.59
96.86±0.53
96.97±0.43
97.14±0.28
97.21±0.19
97.30±0.15

m

SVM
test
76.27±4.16
78.35±3.55
79.36±2.97
80.24±2.54
81.22±1.99
81.45±1.36
81.86±1.28
62.06±4.62
64.15±2.70
65.26±3.82
65.53±3.31
65.95±2.06
67.12±2.65
67.03±2.35
92.13±2.89
95.44±1.70
96.70±0.89
97.38±0.60
98.22±0.61
98.64±0.41
98.96±0.38
65.75±3.55
67.56±2.64
68.95±2.51
69.29±1.93
70.39±1.99
70.64±2.06
–
95.11±2.21
96.04±1.50
96.43±0.98
96.84±0.77
96.94±0.56
97.04±0.56
97.07±0.51
71.66±4.80
77.70±3.38
81.74±3.25
84.55±2.34
87.07±1.50
88.93±1.40
90.90±0.83
78.23±3.62
81.43±3.25
82.52±2.45
83.08±2.11
83.54±1.88
83.86±1.63
–
90.25±3.31
95.48±1.35
96.92±0.96
97.62±0.66
98.19±0.23
98.34±0.12
98.41±0.10
75.22±6.40
80.88±3.33
81.91±3.42
82.03±4.10
83.08±2.91
83.85±2.39
84.64±1.98
95.86±0.77
96.48±0.57
96.69±0.42
96.89±0.34
97.09±0.26
97.23±0.22
97.35±0.17

HGMM
test
79.27±4.35
81.19±2.99
81.57±2.71
81.79±2.42
82.25±1.88
82.42±1.34
82.69±1.25
63.62±5.94
65.80±4.56
66.50±5.30
66.57±4.45
67.33±3.52
68.16±3.69
67.95±2.91
92.13±2.88
95.47±1.67
96.71±0.83
97.38±0.63
98.30±0.72
98.73±0.42
99.02±0.35
67.07±4.27
69.05±3.16
70.30±2.64
70.84±2.18
71.07±1.60
71.33±1.95
–
96.75±0.91
96.67±0.87
96.44±0.94
96.84±0.75
96.94±0.55
97.05±0.56
97.08±0.51
84.99±4.36
90.87±1.93
92.45±1.62
92.34±1.48
93.40±1.53
94.32±0.89
94.99±0.79
79.86±3.78
81.53±2.21
83.22±2.62
83.08±2.09
83.49±1.89
83.92±1.51
–
97.91±0.55
98.20±0.28
98.30±0.19
98.38±0.17
98.41±0.14
98.46±0.16
98.42±0.11
77.33±5.30
80.85±3.29
81.93±3.26
82.06±3.52
83.07±2.75
83.81±2.47
84.57±2.04
97.18±0.28
97.39±0.25
97.49±0.17
97.54±0.17
97.60±0.10
97.65±0.10
97.65±0.11

‘–’ denotes lack of enough training samples.

TABLE 5: Classiﬁcation accuracies (%) on grouped benchmark datasets with opposite labels

ID

(k)

(l)

(m)

(n)

(o)

(p)

(q)

(r)

(s)

(t)

∗

FL
%
0%
5%
10%
15%
0%
5%
10%
15%
0%
5%
10%
15%
0%
5%
10%
15%
0%
5%
10%
15%
0%
5%
10%
15%
0%
5%
10%
15%
0%
5%
10%
15%
0%
5%
10%
15%
0%
5%
10%
15%

1
SVM
train
64.87±5.10
58.70±6.09
58.75±6.40
62.62±3.83
59.62±6.20
58.95±6.63
57.36±5.66
61.15±4.46
73.83±22.70
66.49±5.51
68.18±6.02
65.95±5.00
56.37±2.91
59.71±5.06
65.29±3.41
60.17±5.34
73.20±13.74
68.39±15.22
72.02±13.67
63.10±13.95
100.00±0.00
96.78±14.40
86.34±26.45
63.31±6.69
66.63±8.84
69.03±6.10
63.69±10.72
65.48±8.51
69.03±2.43
69.18±1.73
69.28±1.98
70.28±2.14
54.78±5.93
53.48±4.03
58.39±6.01
55.14±4.51
86.28±3.58
86.44±4.68
87.04±2.97
88.40±2.15

2
SVM
train
64.73±5.08
52.56±6.09
76.29±2.68
76.22±2.28
63.78±3.75
54.85±4.28
54.54±4.97
54.96±6.33
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
85.21±1.30
82.65±5.42
78.53±4.19
75.05±11.66
99.77±0.16
99.76±0.17
99.95±0.09
89.13±1.26
80.89±1.28
80.99±1.25
81.00±1.14
80.81±1.14
99.87±0.15
99.81±0.18
99.76±0.20
99.74±0.18
62.46±3.16
56.43±4.09
54.87±5.69
54.36±5.93
98.10±0.58
97.17±0.60
96.95±0.58
96.26±0.66

m

SVM
train
64.92±5.16
71.67±1.94
71.94±1.67
74.65±1.27
63.86±4.41
70.17±1.86
70.00±1.88
71.06±1.91
100.00±0.00
99.52±0.26
99.63±0.10
98.97±0.20
62.92±3.30
56.67±1.30
56.97±1.65
57.13±1.48
94.82±0.80
94.62±0.86
94.41±0.97
93.80±1.06
99.98±0.06
99.78±0.20
99.70±0.19
99.37±0.31
96.13±0.70
95.89±0.77
95.68±0.87
95.62±0.90
99.94±0.09
99.59±0.20
99.41±0.21
98.68±0.26
82.84±2.02
83.73±1.71
82.43±1.97
81.22±1.91
98.17±0.46
97.91±0.48
97.81±0.44
96.92±0.47

HGMM
train
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00

SGMM
train
74.83±4.54
75.28±4.44
75.67±4.34
84.64±3.96
68.46±3.77
68.69±3.89
68.50±4.17
70.06±3.31
100.00±0.00
99.74±0.11
99.64±0.10
99.06±0.22
62.48±5.01
58.43±3.46
60.81±3.69
63.70±6.04
92.34±1.65
92.79±1.80
92.21±1.57
90.40±1.81
98.30±1.45
98.45±1.35
98.37±1.35
98.68±1.34
84.50±2.88
82.93±3.26
83.06±3.15
81.65±3.45
98.82±0.43
98.63±0.45
98.49±0.44
97.64±0.44
77.55±5.74
77.56±5.87
73.34±7.60
76.01±5.35
100.00±0.00
100.00±0.00
100.00±0.00
99.62±0.18

1
SVM
test
78.06±10.43
79.40±16.72
81.27±14.33
75.29±10.05
63.70±18.00
65.44±16.46
65.63±17.14
62.67±14.57
72.55±22.54
65.51±6.23
65.60±8.57
67.08±5.79
78.36±12.90
73.61±16.78
71.09±6.17
71.25±16.77
86.82±16.72
88.92±9.98
87.10±17.67
88.76±13.49
84.58±1.52
81.12±12.83
72.10±22.55
64.97±12.25
74.78±9.91
73.25±8.64
75.33±8.06
75.71±11.48
68.13±1.99
67.96±1.67
67.95±1.60
68.54±2.40
81.03±10.24
84.03±3.77
82.05±5.65
83.80±3.65
86.33±3.37
86.48±4.69
86.65±3.19
88.90±2.64

2
SVM
test
89.00±0.97
80.02±23.61
77.59±4.40
77.86±3.57
80.67±0.60
76.71±7.19
76.79±6.44
76.07±5.14
96.15±0.44
95.30±0.49
95.04±0.54
93.85±0.59
81.91±0.09
81.91±0.09
81.90±0.09
81.91±0.09
97.08±0.09
94.44±1.76
95.93±1.79
95.57±2.15
94.43±0.61
94.30±0.61
94.23±0.58
93.92±0.98
86.60±1.26
86.48±1.40
86.32±1.52
86.17±1.46
98.45±0.08
98.44±0.07
98.44±0.08
98.44±0.09
86.70±0.83
86.64±0.44
86.52±0.69
86.43±0.80
97.59±0.12
97.32±0.28
97.32±0.29
97.27±0.32

m

SVM
test
88.95±1.03
86.96±1.62
86.15±2.15
84.18±1.89
80.68±0.55
78.64±2.17
78.35±2.32
77.17±1.50
98.96±0.38
98.75±0.28
98.57±0.29
98.31±0.30
81.90±0.86
81.83±0.27
81.90±0.31
81.84±0.37
97.47±0.32
97.47±0.34
97.45±0.41
97.39±0.45
90.92±0.85
90.61±1.03
90.47±0.95
89.42±1.32
86.21±1.58
85.87±1.63
85.59±1.66
85.14±1.66
98.46±0.10
98.46±0.11
98.46±0.13
98.48±0.15
86.89±1.14
86.83±1.13
86.91±1.12
86.99±1.06
97.69±0.12
97.68±0.12
97.68±0.11
97.66±0.12

HGMM
test
82.69±1.25
82.54±1.24
82.33±1.24
81.59±1.31
67.95±2.91
67.47±2.91
67.08±3.08
65.30±2.94
99.02±0.35
98.36±0.39
98.24±0.43
97.92±0.38
76.74±2.26
76.25±2.04
76.02±2.24
76.28±2.29
97.08±0.51
97.05±0.49
96.99±0.49
96.85±0.48
94.99±0.79
92.81±3.28
87.51±1.31
86.75±1.31
84.30±1.85
83.64±1.94
83.35±1.70
82.35±1.83
98.42±0.11
98.35±0.14
98.06±0.19
97.92±0.18
84.57±2.04
84.57±1.92
84.50±1.88
84.67±1.64
97.65±0.11
97.66±0.12
97.67±0.10
97.56±0.11

∗
‘

’ denotes fake labels.

8

SGMM
test
87.96±2.29
87.69±2.34
87.36±2.49
85.32±2.95
78.60±2.41
78.06±2.79
77.58±3.44
76.71±3.01
99.02±0.35
98.76±0.32
98.67±0.30
98.33±0.32
82.05±0.68
82.05±0.49
82.04±0.78
81.92±0.59
97.86±0.27
97.86±0.35
97.87±0.24
97.85±0.24
96.14±0.54
96.05±0.51
95.99±0.53
95.47±0.80
87.70±1.74
87.39±1.69
87.37±1.58
86.89±1.88
98.61±0.06
98.62±0.06
98.61±0.08
98.61±0.07
87.88±1.67
88.05±1.31
88.29±1.01
88.35±0.96
97.65±0.11
97.66±0.13
97.67±0.10
97.61±0.12

[6] V. Feldman, “Does learning require memorization? a short tale
about a long tail,” in Proceedings of the 52nd Annual ACM SIGACT
Symposium on Theory of Computing, 2020, pp. 954–959.

[7] V. Vapnik and R. Izmailov, “Reinforced svm method and mem-
orization mechanisms,” Pattern Recognition, vol. 119, p. 108018,
2021.

[8] C. Cortes and V. Vapnik, “Support vector networks,” Machine

Learning, vol. 20, pp. 273–297, 1995.

[9] B. Schölkopf and A. Smola, Learning with kernels. Cambridge:

MA:MIT Press, 2002.

[10] P. Resende and A. Drummond, “A survey of random forest based
methods for intrusion detection systems,” ACM Computing Surveys
(CSUR), vol. 51, no. 3, pp. 1–36, 2018.

[11] B. Geng, D. Tao, C. Xu, L. Yang, and X. Hua, “Ensemble manifold
regularization,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 34, no. 6, pp. 1227–1233, 2012.

[12] L. Jing and Y. Tian, “Self-supervised visual feature learning with
deep neural networks: A survey,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4037–4058,
2020.

[13] N. Deng, Y. Tian, and C. Zhang, Support Vector Machines: Theory,
Algorithms, and Extensions. CRC Press, Philadelphia, 2012.
[14] M. ´Smieja, L. Struski, J. Tabor, and M. Marzec, “Generalized rbf
kernel for incomplete data,” Knowledge-Based Systems, vol. 173, pp.
150–162, 2019.

[15] V. Vapnik, Statistical Learning Theory, H. Simon, Ed. Wiley-

Interscience, New York, USA, 1998.

[16] R. Shepard, C. Hovland, and H. Jenkins, “Learning and memo-
rization of classiﬁcations.” Psychological Monographs: General and
applied, vol. 75, no. 13, p. 1, 1961.

[17] R. Fletcher, Practical Methods of Optimization. John Wiley and Sons:

Chichester and New York, 1987.

Zhen Wang received his bachelor’s, master’s,
and Ph.D. degrees in Mathematics from the
Department of Mathematics, Jilin University,
Changchun, China, in 2006, 2010, and 2014,
respectively. He is currently a Full Professor
with the School of Mathematical Sciences, In-
ner Mongolia University, Hohhot, China. He has
published over 20 papers on IEEE TNNLS, IEEE
TFS, IEEE TCYB, etc. His research interests in-
clude classiﬁcation techniques, text categoriza-
tion, and data mining.

Yuan-Hai Shao received his B.S. degree in
information and computing science in College
of Mathematics from Jilin University, the mas-
ter’s degree in applied mathematics, and Ph.D.
degree in operations research and manage-
ment in College of Science from China Agricul-
tural University, China, in 2006, 2008 and 2011,
respectively. Currently, he is a Full Professor
at the Management School, Hainan University,
Haikou, China. His research interests include
support vector machines, optimization methods,
machine learning and data mining. He has published over 80 refereed
papers on IEEE TPAMI, IEEE TNNLS, IEEE TFC, IEEE TCYB, PR, etc.

