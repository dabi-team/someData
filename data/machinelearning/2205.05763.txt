Individual Fairness Guarantees for Neural Networks

Elias Benussi1∗ , Andrea Patane1 , Matthew Wicker1 , Luca Laurenti2 and Marta Kwiatkowska1
1University of Oxford
2TU Delft
1{elias.benussi, andrea.patane, matthew.wicker, marta.kwiatkowska}@cs.ox.ac.uk, 2l.laurenti@tudelft.nl

2
2
0
2

y
a
M
1
1

]

G
L
.
s
c
[

1
v
3
6
7
5
0
.
5
0
2
2
:
v
i
X
r
a

Abstract

We consider the problem of certifying the individ-
ual fairness (IF) of feed-forward neural networks
(NNs). In particular, we work with the (cid:15)-δ-IF for-
mulation, which, given a NN and a similarity met-
ric learnt from data, requires that the output differ-
ence between any pair of (cid:15)-similar individuals is
bounded by a maximum decision tolerance δ ≥ 0.
Working with a range of metrics, including the Ma-
halanobis distance, we propose a method to over-
approximate the resulting optimisation problem us-
ing piecewise-linear functions to lower and upper
bound the NN’s non-linearities globally over the in-
put space. We encode this computation as the solu-
tion of a Mixed-Integer Linear Programming prob-
lem and demonstrate that it can be used to compute
IF guarantees on four datasets widely used for fair-
ness benchmarking. We show how this formulation
can be used to encourage models’ fairness at train-
ing time by modifying the NN loss, and empirically
conﬁrm our approach yields NNs that are orders of
magnitude fairer than state-of-the-art methods.

1 Introduction
Reservations have been raised about the application of neu-
ral networks (NN) in contexts where fairness is of con-
cern [Barocas and Selbst, 2018]. Because of inherent biases
present in real-world data, if unchecked, these models have
been found to discriminate against individuals on the basis of
sensitive features, such as race or sex [Bolukbasi et al., 2016,
Angwin et al., 2016]. Recently, the topic has come under the
spotlight, with technologies being increasingly challenged for
bias [Hardesty, 2018, Kirk et al., 2021, Hern, 2020], leading
to the introduction of a range of deﬁnitions and techniques
for capturing the multifaceted properties of fairness.

Fairness approaches are broadly categorised into: group
fairness [Hardt et al., 2016], which inspects the model over
data demographics; and individual fairness (IF) [Dwork et al.,
2012], which considers the behaviour over each individual.
Despite its wider adoption, group fairness is only concerned
with statistical properties of the model so that a situation may

∗Corresponding Author

arise where predictions of a group-fair model can be per-
ceived as unfair by a particular individual. In contrast, IF is
a worst-case measure with guarantees over every possible in-
dividual in the input space. However, while techniques exist
for group fairness of NNs [Albarghouthi et al., 2017, Bastani
et al., 2018], research on IF has thus far been limited to de-
signing training procedures that favour fairness [Yurochkin
et al., 2020, Yeom and Fredrikson, 2020, McNamara et al.,
2017] and veriﬁcation over speciﬁc individuals [Ruoss et al.,
2020]. To the best of our knowledge, there is currently no
work targeted at global certiﬁcation of IF for NNs.

We develop an anytime algorithm with provable bounds
for the certiﬁcation of IF on NNs. We build on the (cid:15)-δ-IF
formalisation employed by John et al. [2020]. That is, given
(cid:15), δ ≥ 0 and a distance metric dfair that captures the similar-
ity between individuals, we ask that, for every pair of points
x(cid:48) and x(cid:48)(cid:48) in the input space with dfair(x(cid:48), x(cid:48)(cid:48)) ≤ (cid:15), the NN’s
output does not differ by more than δ. Although related to it,
IF certiﬁcation on NNs poses a different problem than adver-
sarial robustness [Tjeng et al., 2019], as both x(cid:48) and x(cid:48)(cid:48) are
here problem variables, spanning the whole space. Hence,
local approximation techniques developed in the adversarial
literature cannot be employed in the context of IF.

Nevertheless, we show how this global, non-linear require-
ment can be encoded in Mixed-Integer Linear Programming
(MILP) form, by deriving a set of global upper and lower
piecewise-linear (PWL) bounds over each activation function
in the NN over the whole input space, and performing lin-
ear encoding of the (generally non-linear) similarity metric
dfair(x(cid:48), x(cid:48)(cid:48)). The formulation of our optimisation as a MILP
allows us to compute an anytime, worst-case bound on IF,
which can thus be computed using standard solvers from the
global optimisation literature [Dantzig, 2016]. Furthermore,
we demonstrate how our approach can be embedded into the
NN training so as to optimise for individual fairness at train-
ing time. We do this by performing gradient descent on a
weighted loss that also accounts for the maximum δ-variation
in dfair-neighborhoods for each training point, similarly to
what is done in adversarial learning [Goodfellow et al., 2014,
Gowal et al., 2018, Wicker et al., 2021].

We apply our method on four benchmarks widely em-
ployed in the fairness literature, namely, the Adult, German,
Credit and Crime datasets [Dua and Graff, 2017], and an ar-
ray of similarity metrics learnt from data that include (cid:96)∞,

 
 
 
 
 
 
Mahalanobis, and NN embeddings. We empirically demon-
strate how our method is able to provide the ﬁrst, non-trivial
IF certiﬁcates for NNs commonly employed for tasks from
the IF literature, and even larger NNs comprising up to thou-
sands of neurons. Furthermore, we ﬁnd that our MILP-based
fair training approach consistently outperforms, in terms of
IF guarantees, NNs trained with a competitive state-of-the-
art technique by orders of magnitude, albeit at an increased
computational cost.

The paper makes the following main contributions:1
• We design a MILP-based, anytime veriﬁcation approach
for the certiﬁcation of IF as a global property on NNs.
• We demonstrate how our technique can be used to mod-
ify the loss function of a NN to take into account certiﬁ-
cation of IF at training time.

• On four datasets, and an array of metrics, we show how
our techniques obtain non-trivial IF certiﬁcates and train
NNs that are signiﬁcantly fairer than state-of-the-art.
Related Work A number of works have considered IF by
employing techniques from adversarial robustness. Yeom
and Fredrikson [2020] rely on randomized smoothing to ﬁnd
the highest stable per-feature difference in a model. Their
method, however, provides only (weak) guarantees on model
statistics. Yurochkin et al. [2020] present a method for IF
training that builds on projected gradient descent and optimal
transport. While the method is found to decrease model bias
to state-of-the-art results, no formal guarantees are obtained.
Ruoss et al. [2020] adapted the MILP formulation for adver-
sarial robustness to handle fair metric embeddings. However,
rather than tackling the IF problem globally as introduced by
Dwork et al. [2012], the method only works iteratively on a
ﬁnite set of data, hence leaving open the possibility of unfair-
ness in the model. In contrast, the MILP encoding we obtain
through PWL bounding of activations and similarity metrics
allows us to provide guarantees over any possible pair of indi-
viduals. Urban et al. [2020] employ static analysis to certify
causal fairness. While this method yields global guarantees,
it cannot be straightforwardly employed for IF, and it is not
anytime, making exhaustive analysis impractical. John et al.
[2020] present a method for the computation of IF, though
limited to linear and kernel models. MILP and linear relax-
ation have been employed to certify NNs in local adversar-
ial settings [Ehlers, 2017, Tjeng et al., 2019, Wicker et al.,
2020]. However, local approximations cannot be employed
for the global IF problem. While Katz et al. [2017], Leino
et al. [2021] consider global robustness, their methods are re-
stricted to (cid:96)p metrics. Furthermore, they require the knowl-
edge of a Lipschitz constant or are limited to ReLU.

2 Individual Fairness
We focus on regression and binary classiﬁcation with NNs
with real-valued inputs and one-hot encoded categorical
features.2 Such frameworks are often used in automated

1Proofs and additional details can be found in Appendix of an
extended version of the paper available at http://www.fun2model.
org/bibitem.php?key=BPW+22.

2Multi-class can be tackled with component-wise analyses.

decision-making, e.g. for loan applications [Hardt et al.,
2016]. Formally, given a compact input set X ⊆ Rn and
an output set Y ⊆ R, we consider an L layer fully-connected
NN f w : X → Y , parameterised by a vector of weights
w ∈ Rnw trained on D = {(xi, yi), i ∈ {1, ..., nd}}. For
an input x ∈ X, i = 1, . . . , L and j = 1, . . . , ni, the NN is
deﬁned as:

φ(i)
j =

ni−1
(cid:88)

k=1

W (i)

jk ζ (i−1)

k

+ b(i)
j ,

j = σ(i) (cid:16)
ζ (i)

φ(i)
j

(cid:17)

(1)

j

where ζ (0)
j = xj. Here, ni is the number of units in the ith
layer, W (i)
jk and b(i)
are its weights and biases, σ(i) is the
activation function, φ(i) is the pre-activation and ζ (i) the ac-
tivation. The NN output is the result of these computations,
f w(x) := ζ (L). In regression, f w(x) is the prediction, while
for classiﬁcation it represents the class probability.
In this
paper we focus on fully-connected NNs as widely employed
in the IF literature [Yurochkin et al., 2020, Urban et al., 2020,
Ruoss et al., 2020]. However, we should stress that our frame-
work, being based on MILP, can be easily extended to convo-
lutional, max-pool and batch-norm layers or res-nets by using
embedding techniques from the adversarial robustness litera-
ture (see e.g. [Boopathy et al., 2019].
Individual Fairness Given a NN f w, IF [Dwork et al.,
2012] enforces the property that similar individuals are sim-
ilarly treated. Similarity is deﬁned according to a task-
dependent pseudometric, dfair : X × X (cid:55)→ R≥0, provided
by a domain expert (e.g., a Mahalanobis distance correlat-
ing each feature to the sensitive one), whereas similarity of
treatment is expressed via the absolute difference on the NN
output f w(x). We adopt the (cid:15)-δ-IF formulation of John et al.
[2020] for the formalisation of input-output IF similarity.

Deﬁnition 1 ((cid:15)-δ-IF [John et al., 2020]). Consider (cid:15) ≥ 0 and
δ ≥ 0. We say that f w is (cid:15)-δ-individually fair w.r.t. dfair iff
∀x(cid:48), x(cid:48)(cid:48) s.t. dfair(x(cid:48), x(cid:48)(cid:48)) ≤ (cid:15) =⇒ |f w(x(cid:48)) − f w(x(cid:48)(cid:48))| ≤ δ.

Here, (cid:15) measures similarity between individuals and δ is
the difference in outcomes (class probability for classiﬁca-
tion). We emphasise that individual fairness is a global no-
tion, as the condition in Deﬁnition 1 must hold for all pairs
of points in X. We remark that the (cid:15)-δ-IF formulation of
John et al. [2020] (which is more general than IF formulation
typically used in the literature [Yurochkin et al., 2020, Ruoss
et al., 2020]) is a slight variation on the Lipschitz property
introduced by Dwork et al. [2012]. While introducing greater
ﬂexibility thanks to its parametric form, it makes an IF para-
metric analysis necessary at test time. In Section 4 we analyse
how (cid:15)-δ-IF of NNs is affected by variations of (cid:15) and δ. A cru-
cial component of IF is the similarity dfair. The intuition is
that sensitive features, or their sensitive combination, should
not inﬂuence the NN output. While a number of metrics has
been discussed in the literature [Ilvento, 2019], we focus on
the following representative set of metrics which can be au-
tomatically learnt from data [John et al., 2020, Ruoss et al.,
2020, Mukherjee et al., 2020, Yurochkin et al., 2020]. Details
on metric learning is given in Appendix B.

i − x(cid:48)(cid:48)

i=1 θi|x(cid:48)

In this case dfair(x(cid:48), x(cid:48)(cid:48)) is deﬁned as a
Weighted (cid:96)p:
i.e. dfair(x(cid:48), x(cid:48)(cid:48)) =
weighted version of an (cid:96)p metric,
p(cid:112)(cid:80)n
i |p. Intuitively, we set the weights θi re-
lated to sensitive features to zero, so that two individuals are
considered similar if they only differ with respect to those.
The weights θi for the remaining features can be tuned ac-
cording to their degree of correlation to the sensitive features.
In this case we have dfair(x(cid:48), x(cid:48)(cid:48)) =
Mahalanobis:
(cid:112)(x(cid:48) − x(cid:48)(cid:48))T S(x(cid:48) − x(cid:48)(cid:48)), for a given positive semi-deﬁnite
(SPD) matrix S. The Mahalanobis distance generalises the (cid:96)2
metric by taking into account the intra-correlation of features
to capture latent dependencies w.r.t. the sensitive features.
Feature Embedding: The metric is computed on an embed-
ding, so that dfair(x(cid:48), x(cid:48)(cid:48)) = ˆd(ϕ(x(cid:48)), ϕ(x(cid:48)(cid:48))), where ˆd is ei-
ther the Mahalanobis or the weighted (cid:96)p metric, and ϕ is a
feature embedding map. These allow for greater modelling
ﬂexibility, at the cost of reduced interpretability.

2.1 Problem Formulation

We aim at certifying (cid:15)-δ-IF for NNs. To this end we formalise
two problems: computing certiﬁcates and training for IF.

Problem 1 (Fairness Certiﬁcation). Given a trained NN f w,
a similarity dfair and a distance threshold (cid:15) ≥ 0, compute

δmax =

max
x(cid:48),x(cid:48)(cid:48)∈X
dfair(x(cid:48),x(cid:48)(cid:48))≤(cid:15)

|f w(x(cid:48)) − f w(x(cid:48)(cid:48))|.

Problem 1 provides a formulation in terms of optimisation,
seeking to compute the maximum output change δmax for any
pair of input points whose dfair distance is no more than (cid:15).
One can then compare δmax with any threshold δ: if δmax ≤ δ
holds then the model f w has been certiﬁed to be (cid:15)-δ-IF.

While Problem 1 is concerned with an already trained NN,
the methods we develop can also be employed to encourage
IF at training time. Similarly to the approaches for adversarial
learning [Goodfellow et al., 2014], we modify the training
loss L(f w(x), y) to balance between the model ﬁt and IF.

Problem 2 (Fairness Training). Consider an NN f w, a train-
ing set D, a similarity metric dfair and a distance threshold
(cid:15) ≥ 0. Let λ ∈ [0, 1] be a constant. Deﬁne the IF-fair loss as

Lfair(f w(xi), yi, f w(x∗

i ), λ) =
λL(f w(xi), yi) + (1 − λ)|f w(xi) − f w(x∗

i )|,

i = arg maxx∈X s.t. dfair(xi,x)≤(cid:15) |f w(xi) − f w(x)|.

where x∗
The (cid:15)-IF training problem is deﬁned as ﬁnding wfair s.t.:

wfair = arg min

w

nd(cid:88)

i=1

Lfair(f w(xi), yi).

In Problem 2 we seek to train a NN that not only is ac-
curate, but whose predictions are also fair according to Def-
inition 1. Parameter λ balances between accuracy and IF. In
particular, for λ = 1 we recover the standard training that
does not account for IF, while for λ = 0 we only consider IF.

3 A MILP Approach For Individual Fairness

Certiﬁcation of individual fairness on a NN thus requires us to
solve the following global, non-convex optimisation problem:

max
x(cid:48),x(cid:48)(cid:48)∈X

|δ|

subject to δ = f w(x(cid:48)) − f w(x(cid:48)(cid:48))

dfair(x(cid:48), x(cid:48)(cid:48)) ≤ (cid:15).

(2)

(3)

We develop a Mixed-Integer Linear Programming (MILP)
over-approximation (i.e., providing a sound bound) to this
problem. We notice that there are two sources of non-linearity
here, one induced by the NN (Equation (2)), which we refer
to as the model constraint, and the other by the fairness met-
ric (Equation (3)), which we call fairness constraint. In the
following, we show how these can be modularly bounded by
piecewise-linear functions. In Section 3.3 we bring the results
together to derive a MILP formulation for (cid:15)-δ-IF.

j

j

j

j,0, . . . , φ(i)

3.1 Model Constraint
We develop a scheme based on piecewise-linear (PWL) upper
and lower bounding for over-approximating all commonly
used non-linear activation functions. An illustration of the
PWL bound is given in Figure 1. Let φ(i)L
and φ(i)U
∈ R be
j
lower and upper bounds on the pre-activation φ(i)
j .3 We pro-
ceed by building a discretisation grid over the φ(i)
j values on
M grid points: φgrid = [φ(i)
j,0 := φ(i)L
j,M ], with φ(i)
j,M := φ(i)U
and φ(i)
, such that, in each partition interval
[φ(i)
j,l , φ(i)
j,l+1], we have that σ(i) is either convex or concave.
We then compute linear lower and upper bound functions for
σ(i) in each [φ(i)
j,l+1] as follows. If σ(i) is convex (resp.
concave) in [φ(i)
j,l+1], then an upper (resp. lower) linear
bound is given by the segment connecting the two extremum
points of the interval, and a lower (resp. upper) linear bound
is given by the tangent through the mid-point of the interval.
We then compute the values of each linear bound in each of
its grid points, and select the minimum of the lower bounds
and the maximum of the upper bound values, which we
store in two vectors ζPWL,(i),U
, . . . , ζ PWL,(i),U
= [ζ PWL,(i),U
]
j
j,0
j,M
, . . . , ζ PWL,(i),L
= [ζ PWL,(i),L
and ζPWL,(i),L
]. The following
j,0
j
j,M
lemma is a consequence of this construction.
Lemma 1. Let φ ∈ [φ(i)L
, φ(i)U
]. Denote with l the index as-
j
sociated to the partition of φgrid in which φ falls and consider
η ∈ [0, 1] such that φ = ηφ(i)L

j,l , φ(i)
j,l , φ(i)

j,l−1 + (1 − η)φ(i)L

. Then:

j,l

j

σ(i)(φ) ≥ ηζ PWL,(i),L
σ(i)(φ) ≤ ηζ PWL,(i),U

j,l−1

j,l−1

+ (1 − η)ζ PWL,(i),L
+ (1 − η)ζ PWL,(i),U

j,l

j,l

,

,

that is, ζPWL,(i),L
j
lower and upper bounds for φ in [φ(i)L

and ζPWL,(i),U
j

, φ(i)U
j

].

j

deﬁne continuous PWL

3Computed by bound propagation over X [Gowal et al., 2018].

Figure 1: Upper and lower PWL functions to sigmoid for an increasing number of partition points M (marked with red ticks).

Lemma 3.1 guarantees that we can bound the non-linear
activation functions using PWL functions. Crucially, PWL
functions can then be encoded into the MILP constraints.
Proposition 1. Let y(i)
ables, and η(i)
[φ(i)L
j

j,l for l = 1, . . . , M , be binary vari-
j,l ∈ [0, 1] be continuous ones. Consider φ(i)
j ∈
] then it follows that ζ (i)
implies:

j = σ(i) (cid:16)

, φ(i)U
j

φ(i)
j

(cid:17)

M
(cid:88)

l=1

y(i)
j,l = 1,

η(i)
j,l + η(i)

j,l+1,

M
(cid:88)

l=1

M
(cid:88)

l=1

j,l = 1, φ(i)
η(i)

j =

M
(cid:88)

l=1

j,l η(i)
φ(i)L

j,l , y(i)

j,l ≤

ζ PWL,(i),L
j,l

j,l ≤ ζ (i)
η(i)

j ≤

M
(cid:88)

l=1

ζ PWL,(i),U
j,l

η(i)
j,l .

A proof can be found in Appendix A. Proposition 1 en-
sures that the global behaviour of each NN neuron can be
over-approximated by 5 linear constraints using 2M auxiliary
variables. Employing Proposition 1 we can encode the model
constraint of Equation (2) into the MILP form in a sound way.
The over-approximation error does not depend on the
MILP formulation (which is exact), but on the PWL bound-
ing, and is hence controllable through the selection of the
number of grid points M , and becomes exact in the limit.
Notice that in the particular case of ReLU activation func-
tions the over-approximation is exact for any M > 0.

Proposition 2. Assume σ(i) to be continuously differentiable
everywhere in [φ(i)L
], except possibly in a ﬁnite set.
Then PWL lower and upper bounding functions of Lemma
3.1 converge uniformly to σ(i) as M goes to inﬁnity.

, φ(i)U
j

j

Furthermore, deﬁne ∆M = (φ(i)U

)/M , then
for ﬁnite values of M the error on the lower (resp. up-
per) bounding in convex (resp. concave) regions of σ(i) for
φ ∈ [φ(i)

− φ(i)L
j

j

j,l , φ(i)

j,l+1] is given by:
(cid:18)

e1(φ) ≤

∆M
2

σ(cid:48)(φ(i)

j,l+1) − σ(cid:48)

(cid:18)

φ(i)
j,l+1 −

∆M
2

(cid:19)(cid:19)

and upper (resp. lower) in concave (resp. convex) regions:

e2(φ) ≤ ∆M

(cid:16)

σ





(cid:17)

φ(i)
j,l + ∆M
∆M

− σ(φ(i)
j,l )



+ σ(cid:48)(φ(i)
j,l )

 .

A proof of Proposition 2 is given in Appendix A, alongside

an experimental analysis of the convergence rate.

We remark that the PWL bound can be used over all com-
monly employed activation functions σ. The only assumption
made is that σ has a ﬁnite number of inﬂection points over
any compact interval of R. For convergence (Prop. 2) we re-
quire continuous differentiability almost everywhere, which
is satisﬁed by commonly used activations.

3.2 Fairness Constraint

The encoding of the fairness constraint within the MILP for-
mulation depends on the speciﬁc form of the metric dfair.
Weighted (cid:96)p Metric: The weighted (cid:96)p metric can be tackled
by employing rectangular approximation regions. While this
is straightforward for the (cid:96)∞ metric, for the remaining cases
interval abstraction can be used [Dantzig, 2016].
Mahalanobis Metric: We ﬁrst compute an orthogonal de-
composition of S as in U T SU = Λ, where U is the eigenvec-
tor matrix of S and Λ is a diagonal matrix with S eigenvalues
as entries. Consider the rotated variables z(cid:48) = U T x(cid:48) and
z(cid:48)(cid:48) = U T x(cid:48)(cid:48), then we have that Equation (3) can be re-written
as (z(cid:48)−z(cid:48)(cid:48))T Λ(z(cid:48)−z(cid:48)(cid:48)) ≤ (cid:15)2. By simple algebra we thus have
i )2 ≤ (cid:15)2
that, for each i, (z(cid:48)
. By transforming back to the
Λii
original variables, we obtain that Equation (3) can be over-
.
approximated by: −

≤ U T x(cid:48) − U T x(cid:48)(cid:48) ≤

i −z(cid:48)(cid:48)

(cid:15)√

(cid:15)√

diag(Λ)

diag(Λ)

Feature Embedding Metric We tackle the case in which
dfair(x(cid:48), x(cid:48)(cid:48)) =
ϕ used in the metric deﬁnition,
ˆd(ϕ(x(cid:48)), ϕ(x(cid:48)(cid:48))), is a NN embedding. This is straightforward
as ϕ can be encoded into MILP as for the model constraint.

i.e.

3.3 Overall Formulation

We now formulate the MILP encoding for
the over-
approximation δ∗ ≥ δmax of (cid:15)-δ-IF. For Equation (2), we
proceed by deriving a set of approximating constraints for
the variables x(cid:48) and x(cid:48)(cid:48) by using the techniques described in
Section 3.1. We denote the corresponding variables as φ(cid:48)(i)
,
ζ (cid:48)(i)
, respectively. The NN ﬁnal output on x(cid:48)
j
and on x(cid:48)(cid:48) will then respectively be ζ (cid:48)(L) and ζ (cid:48)(cid:48)(L), so that
δ = ζ (cid:48)(L) − ζ (cid:48)(cid:48)(L). Finally, we over-approximate Equation
(3) as described in Section 3.2. In the case of Mahalanobis

and φ(cid:48)(cid:48)(i)
j

, ζ (cid:48)(cid:48)(i)
j

j

420240.00.20.40.60.81.0()PWL,LPWL,UFigure 2: Certiﬁed bounds on IF (δ∗) for different architecture parameters (widths and depths) and maximum similarity ((cid:15)) for the Adult and
the Crime datasets. Top Row: Mahalanobis metric used for dfair. Bottom Row: Weighted (cid:96)∞ metric used for dfair.

distance, we thus obtain:
|δ|

max
x(cid:48),x(cid:48)(cid:48)∈X
subjectto = ζ (cid:48)(L) − ζ (cid:48)(cid:48)(L)

for i = 1, . . . , L, j = 1, . . . , ni, † ∈ {(cid:48),(cid:48)(cid:48) } :

(4)

M
(cid:88)

l=1

y†(i)
j,l = 1,

M
(cid:88)

l=1

η†(i)
j,l = 1,

j,l ≤ η(i)
y(i)

j,l + η(i)

j,l+1

φ†(i)
j =

ni−1
(cid:88)

k=1

W (i)

jk x†

k + b(i)

j , φ†(i)

j =

M
(cid:88)

l=1

j,l η†(i)
φ(i)L

j,l

Theorem 1, whose proof can be found in Appendix A,
states that a solution of the MILP problem provides us with a
sound estimation of individual fairness of an NN. Crucially, it
can be shown that branch-and-bound techniques for the solu-
tion of MILP problems converge in ﬁnite time to the optimal
solution [Del Pia and Weismantel, 2012], while furthermore
providing us with upper and lower bounds for the optimal
value at each iteration step. Therefore, we have:
Corollary 1. Let δL
k and δU
k lower and upper bounds com-
puted by a MILP solver at step k > 0. Then we have that:
δL
k ≤ δ∗ ≤ δU
k . Furthermore, given a precision, τ , there exist
a ﬁnite k∗ such that δU
k∗

− δL
k∗

≤ τ .

M
(cid:88)

l=1

ζ PWL,(i),L
j,l

j,l ≤ ζ †(i)
η†(i)

j ≤

M
(cid:88)

l=1

ζ PWL,(i),U
j,l

η†(i)
j,l

−

(cid:15)2
(cid:112)diag(Λ)

≤ U x(cid:48) − U x(cid:48)(cid:48) ≤

(cid:15)2
(cid:112)diag(Λ)

.

Though similar, the above MILP is signiﬁcantly different
from those used for adversarial robustness (see e.g. Tjeng
et al. [2019]). First, rather than looking for perturbations
around a ﬁxed a point, here we have both x(cid:48) and x(cid:48)(cid:48) as
variables. Furthermore, rather than being local, the MILP
problem for (cid:15)-δ-IF is global, over the whole input space X.
As such, local approximations of non-linearities cannot be
used, as the bounding needs to be valid simultaneously over
the whole input space. Finally, while in adversarial robust-
ness one can ignore the last sigmoid layer, for IF, because of
the two optimisation variables, one cannot simply map from
the last pre-activation value to the class probability, so that
even for ReLU NNs one needs to employ bounding of non-
piecewise activations for the ﬁnal sigmoid.

By combining the results from this section, we have:

Theorem 1. Consider (cid:15) ≥ 0, a similarity dfair and a NN
f w. Let x(cid:48)
∗ be the optimal points for the optimisation
problem in Equation (4). Deﬁne δ∗ = |f w(x(cid:48)
∗) − f w(x(cid:48)(cid:48)
∗ )|.
Then f w is (cid:15)-δ-individually fair w.r.t. dfair for any δ ≥ δ∗.

∗ and x(cid:48)(cid:48)

That is, our method is sound and anytime, as at each itera-
tion step in the MILP solving we can retrieve a lower and an
upper bound on δ∗, which can thus be used to provide prov-
able guarantees while converging to δ∗ in ﬁnite time.
Complexity Analysis The encoding of the model con-
straint can be done in O(LM nmax), where nmax is the max-
imum width of f w, L is the number of layers, and M is the
number of grid points used for the PWL bound. The com-
putational complexity of the fairness constraints depends on
the similarity metric employed. While for (cid:96)∞ no process-
ing needs to be done, the computational complexity is O(n3)
for the Mahalanobis distance and again O(LM nmax) for the
feature embedding metric. Each iteration of the MILP solver
entails the solution of a linear programming problem and is
hence O((M nmaxL)3). Finite time convergence of the MILP
solver to δ∗ with precision τ is exponential in the number of
problem variables, in τ and (cid:15).

3.4 Fairness Training for Neural Networks
The (cid:15)-δ-IF MILP formulation introduced in Section 3 can
be adapted for the solution of Problem 2. The key step
is the computation of x∗
in the second component of the
i
modiﬁed loss introduced in Problem 2, which is used to
introduce fairness directly into the loss of the neural net-
work. This computation can be done by observing that, for

every training point xi drawn from D, the computation of
x∗
i = arg maxx∈X s.t. dx(xi,x)≤(cid:15) |f w(xi) − f w(x)| is a par-
ticular case of the formulation described in Section 3, where,
instead of having two variable input points, only one input
point is a problem variable while the other is given and drawn
from the training dataset D. Therefore, x∗
i can be computed
by solving the MILP problem, where we ﬁx a set of the prob-
lem variables to xi, and can be subsequently used to obtain
the value of the modiﬁed loss function. Note that these con-
straints are not cumulative, since they are built for each mini-
batch, and discarded after optimization is solved to update the
weights.

i=0 ∼ D

for b = 1, . . . , (cid:100)|D|/nbatch(cid:101) do
{X, Y } ← {xi, yi}nbatch
Yclean ← f w(X)
[φ(cid:48), ζ(cid:48), φ(cid:48)(cid:48), ζ(cid:48)(cid:48)] ← InitM ILP (f w, dfair, (cid:15))
XMILP ← ∅
for i = 0, . . . nbatch do

Algorithm 1 Fair Training with MILP.
Input: NN architecture: f w, Dataset: D, Learning rate: α, Itera-
tions: nepoch, Batch Size: nbatch, Similarity metric: dfair, Maximum
similarity: (cid:15), Fairness Loss Weighting: λ.
Output: wfair: weight values balancing between accuracy and fair-
ness.
B1: wfair ← InitW eights(f w)
2: for t = 1, . . . , nepoch do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end for
18: return wfair

end for
YMILP ← f w(XMILP)
l ← Lfair(Yclean, Y, YMILP, λ)
wfair ← wfair − α∇wl

#MILP inputs forward pass
#Fair Loss
#Optimizer step (here, SGD)

i, ζ(cid:48)
φ(cid:48)
i, ζ(cid:48)
i ← M ILP (xi, φ(cid:48)
x∗
i)
(cid:83){x∗
XMILP ← XMILP
i }

#Sample Batch
#Standard forward pass
# Section 3

#Fix constraints
# Solve ‘local’ MILP prob.

#Weights optimized for fairness & accuracy

i ← F ixV arConst(xi)

end for

We summarise our fairness training method in Algo-
rithm 1. For each batch in each of the nepoch training epochs,
we perform a forward pass of the NN to obtain the output,
Yclean (line 5). We then formulate the MILP problem as in
Section 3 (line 6), and initialise an empty set variable to col-
lect the solutions to the various sub-problems (line 7). Then,
for each training point xi in the mini-batch, we ﬁx the MILP
constraints to the variables associated with xi (line 9), solve
the resulting MILP for x∗
i in the set that col-
lects the solutions, i.e. XMILP. Finally, we compute the NN
predictions on XMILP (line 13); the result is used to compute
the modiﬁed loss function (line 14) and the weights are up-
dated by taking a step of gradient descent. The resulting set
of weights wfair balances the empirical accuracy and fairness
around the training points.

i , and place x∗

The choice of λ affects the relative importance of standard
training w.r.t. the fairness constraint: λ = 1 is equivalent to
standard training, while λ = 0 only optimises for fairness.
In our experiments we keep λ = 1 for half of the training
epochs, and then change it to λ = 0.5.

Figure 3: Balanced accuracy / individual fairness trade-off for NNs.

4 Experiments
In this section, we empirically validate the effectiveness of
our MILP formulation for computing (cid:15)-δ-IF guarantees as
well as for fairness training of NNs. We perform our ex-
periments on four UCI datasets [Dua and Graff, 2017]: the
Adult dataset (predicting income), the Credit dataset (predict-
ing payment defaults), the German dataset (predicting credit
risk) and the Crime dataset (predicting violent crime).
In
each case, features encoding information regarding gender
or race are considered sensitive. In the certiﬁcation experi-
ments we employ a precision τ for the MILP solvers of 10−5
and a time cutoff of 180 seconds. We compare our train-
ing approach with two different learning methods: Fairness-
Through-Unawareness (FTU), in which the sensitive features
are simply removed, and SenSR [Yurochkin et al., 2020]. Ex-
ploration of the cutoff, group fairness, certiﬁcation of addi-
tional NNs, scalability of the methods and additional details
on the experimental settings are given in Appendix D and C.4
Fairness Certiﬁcation We analyse the suitability of our
method in providing non-trivial certiﬁcates on (cid:15)-δ-IF with re-
spect to the similarity threshold (cid:15) (which we vary from 0.01
to 0.25), the similarity metric dfair , the width of the NN (from
8 to 64), and its number of layers (from 1 to 4). These reﬂect
the characteristics of NNs and metrics used in the IF litera-
ture [Yurochkin et al., 2020, Ruoss et al., 2020, Urban et al.,
2020]; for experiments on larger architectures, demonstrating
the scalability of our approach, see Appendix D.3. For each
dataset we train the NNs by employing the FTU approach.

The results for these analyses are plotted in Figure 2 for
the Adult and the Crime datasets (results for Credit and Ger-
man datasets can be found in Appendix D.1). Each heat map
depicts the variation of δ∗ as a function of (cid:15) and the NN archi-
tecture. The top row in the ﬁgure was computed by consid-
ering the Mahalanobis similarity metric; the bottom row was

4An implementation of the method and of the experiments can be
found at https://github.com/eliasbenussi/nn-cert-individual-fairness.

to those trained by Yurochkin et al. [2020]. As expected,
we observe that FTU performs the worst in terms of certi-
ﬁed fairness, as simple omission of the sensitive features is
unable to obfuscate latent dependencies between the sensi-
tive and non-sensitive features. As previously reported in the
literature, SenSR signiﬁcantly improves on FTU by account-
ing for features latent dependencies. However, on all four
datasets, our MILP-based training methodology consistently
improves IF by orders of magnitude across all the architec-
tures when compared to SenSR. In particular, for the archi-
tectures with more than one hidden layer, on average, MILP
outperforms FTU by a factor of 78598 and SenSR by 27739.
Intuitively, while SenSR and our approach have a similar for-
mulation, the former is based on gradient optimisation so that
no guarantees are provided in the worst case for the training
loss. In contrast, by relying on MILP, our method optimises
the worst-case behaviour of the NN at each step, which fur-
ther encourages training of individually fair models. The cost
of the markedly improved guarantees is, of course, a higher
computational costs.
In fact, the training of the models in
Figure 3 with MILP had an average training time of about
3 hours. While the increased cost is signiﬁcant, we highlight
that this is a cost that is only paid once and may be justiﬁed in
sensitive applications by the necessity of fairness at deploy-
ment time. We furthermore notice that, while our implemen-
tation is sequential, parallel per-batch solution of the MILP
problems during training would markedly reduce the compu-
tational time and leave for future work the parallelisation and
tensorisation of the techniques. Interestingly, we ﬁnd that bal-
anced accuracy also slightly improved with SenSR and MILP
training in the tasks considered here, possibly as a result of
the bias in the class labels w.r.t. sensitive features. Finally, in
Figure 4 we further analyse the certiﬁed δ∗-proﬁle w.r.t. to the
input similarity (cid:15), varying the value of (cid:15) used in for the ceri-
tiﬁcation of (cid:15)-δ-IF. In the experiment, both SenSR and MILP
are trained with (cid:15) = 0.2, which means that our method, based
on formal IF certiﬁcates, is guaranteed to outperform SenSR
up until (cid:15) = 0.2 (as in fact is the case). Beyond 0.2, no such
statement can be made, and it is still theoretically possible
for SenSR to outperform MILP in particular circumstances.
Empirically, however, MILP-based training still largely out-
performs SenSR in terms of certiﬁed fairness obtained.

5 Conclusion

We introduced an anytime MILP-based method for the certiﬁ-
cation and training of (cid:15)-δ-IF in NNs, based on PWL bounding
and MILP encoding of non-linearities and similarity metrics.
In an experimental evaluation comprising four datasets, a se-
lection of widely employed NN architectures and three types
of similarity metrics, we empirically found that our method
is able to provide the ﬁrst non-trivial certiﬁcates for (cid:15)-δ-IF in
NNs and yields NNs which are, consistently, orders of magni-
tude more fair than those obtained by a competitive IF train-
ing technique.

Acknowledgements This project was funded by the ERC
European Union’s Horizon 2020 research and innovation pro-
gramme (FUN2MODEL, grant agreement No. 834115).

Figure 4: Certiﬁed δ∗ as a function of the maximum similarity (cid:15).

computed for a weighted (cid:96)∞ metric (with coefﬁcients chosen
as in John et al. [2020]) and results for the feature embedding
metrics are given in Appendix D.2. As one might expect, we
observe that, across all the datasets and architectures, increas-
ing (cid:15) correlates with an increase in the values for δ∗, as higher
values of (cid:15) allow for greater feature changes. Interestingly, δ∗
tends to decrease (i.e., the NN becomes more fair) as we in-
crease the number of NN layers. This is the opposite to what
is observed for the adversarial robustness, where increased
capacity generally implies more fragile models [Madry et al.,
2017]. In fact, as those NNs are trained via FTU, the main
sensitive features are not accessible to the NN. A possible ex-
planation is that, as the number of layers increases, the NN’s
dependency on the speciﬁc value of each feature diminishes,
and the output becomes dependent on their nonlinear combi-
nation. The result suggests that over-parametrised NNs could
be more adept at solving fair tasks – at least for IF deﬁnitions
– though this would come with a loss of model interpretabil-
ity, and exploration would be needed to assess under which
condition this holds. Finally, we observe that our analysis
conﬁrms how FTU training is generally insufﬁcient in pro-
viding fairness on the model behaviour for (cid:15)-δ-IF. For each
model, individuals that are dissimilar by (cid:15) ≥ 0.25 can al-
ready yield a δ∗ > 0.5, meaning they would get assigned to
different classes if one was using the standard classiﬁcation
threshold of 0.5.

Fairness Training We investigate the behaviour of our fair-
ness training algorithm for improving (cid:15)-δ-IF of NNs. We
compare our method with FTU and SenSR [Yurochkin et al.,
2020]. For ease of comparison, in the rest of this section we
measure fairness with dfair equal to the Mahalanobis similar-
ity metric, with (cid:15) = 0.2, for which SenSR was developed.
The results for this analysis are given in Figure 3, where
each point in the scatter plot represents the values obtained
for a given NN architecture. We train architectures with up
to 2 hidden layers and 64 units, in order to be comparable

References
Aws Albarghouthi, Loris D’Antoni, Samuel Drews, and
Aditya Nori. FairSquare: Probabilistic Veriﬁcation for Pro-
gram Fairness. In OOPSLA ’17. ACM, 2017.

Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and
Mykel J Kochenderfer. Reluplex: An efﬁcient smt solver
In CAV. Springer,
for verifying deep neural networks.
2017.

Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirch-
ner. Machine Bias: There’s software used across the
country to predict future criminals. And it’s biased against
blacks. ProPublica, 2016.

Solon Barocas and Andrew D. Selbst. Big Data’s Disparate

Impact. SSRN Electronic Journal, 2018.

Osbert Bastani, Xin Zhang, and Armando Solar-Lezama.
Probabilistic veriﬁcation of fairness properties via concen-
tration, 2018.

Tolga Bolukbasi, Kai Wei Chang, James Zou, Venkatesh
Saligrama, and Adam Kalai. Man is to computer program-
mer as woman is to homemaker? Debiasing word embed-
dings. In NeurIPS, 2016.

Akhilan Boopathy, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu,
and Luca Daniel. Cnn-cert: An efﬁcient framework for
certifying robustness of convolutional neural networks. In
AAAI, volume 33, 2019.

George Dantzig.

Linear programming and extensions.

Princeton university press, 2016.

Alberto Del Pia and Robert Weismantel. On convergence in
mixed integer programming. Mathematical programming,
135(1):397–412, 2012.

Dheeru Dua and Casey Graff. UCI machine learning reposi-

tory, 2017.

Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Rein-
gold, and Richard Zemel. Fairness through awareness. In
ITCS 2012 - Innovations in Theoretical Computer Science
Conference, 2012.

Ruediger Ehlers. Formal veriﬁcation of piece-wise linear
feed-forward neural networks. In ATVA. Springer, 2017.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572, 2014.

Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth,
Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arand-
jelovi´c, Timothy Mann, and Pushmeet Kohli. On the ef-
fectiveness of interval bound propagation for training veri-
ﬁably robust models, 2018.

Larry Hardesty. Study ﬁnds gender and skin-type bias in com-

mercial artiﬁcial-intelligence systems, Feb 2018.

Moritz Hardt, Eric Price, and Nathan Srebro. Equality of
opportunity in supervised learning. In NeurIPS, 2016.
Alex Hern. Twitter apologises for ’racist’ image-cropping al-

gorithm, Sep 2020.

Christina Ilvento. Metric Learning for Individual Fairness.

arXiv e-prints, page arXiv:1906.00250, 6 2019.

Philips George John, Deepak Vijaykeerthy, and Diptikalyan
Saha. Verifying Individual Fairness in Machine Learning
Models, 2020.

Hannah Kirk, Yennie Jun, Haider Iqbal, Elias Benussi, Fil-
ippo Volpin, Fr´ed´eric A. Dreyer, Aleksandar Shtedritski,
and Yuki Markus Asano. How true is gpt-2? an empirical
analysis of intersectional occupational biases. 2021.

Klas Leino, Zifan Wang, and Matt Fredrikson. Globally-
robust neural networks. arXiv preprint arXiv:2102.08452,
2021.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learn-
ing models resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083, 2017.

Daniel McNamara, Cheng Soon Ong, and Robert C.
CoRR,

Provably fair representations.

Williamson.
abs/1710.04394, 2017.

Michela Milano, Greger Ottosson, Philippe Refalo, and Er-
lendur S Thorsteinsson. The beneﬁts of global constraints
for the integration of constraint programming and integer
programming. In Working Notes AAAI-2000 Workshop In-
tegration of AI and OR Techniques for Combinatorial Op-
timization, 2000.

Debarghya Mukherjee, Mikhail Yurochkin, Moulinath Baner-
jee, and Yuekai Sun. Two Simple Ways to Learn Individual
Fairness Metrics from Data, 2020.

Dino Pedreshi, Salvatore Ruggieri, and Franco Turini.
In 14th SIGKDD,

Discrimination-aware data mining.
2008.

Anian Ruoss, Mislav Balunovi´c, Marc Fischer, and Martin
Vechev. Learning Certiﬁed Individually Fair Representa-
tions, 2020.

Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating ro-
bustness of neural networks with mixed integer program-
ming. In ICLR, 2019.

Caterina Urban, Maria Christakis, Valentin W¨ustholz, and
Fuyuan Zhang. Perfectly parallel fairness certiﬁcation of
neural networks. Proceedings of the ACM on Program-
ming Languages, 4(OOPSLA), 2020. ISSN 24751421.
Matthew Wicker, Luca Laurenti, Andrea Patane, and Marta
Kwiatkowska. Probabilistic safety for bayesian neural net-
works. In Conference on Uncertainty in Artiﬁcial Intelli-
gence, pages 1198–1207. PMLR, 2020.

Matthew Wicker, Luca Laurenti, Andrea Patane, Zhuotong
Chen, Zheng Zhang, and Marta Kwiatkowska. Bayesian
In AIS-
inference with certiﬁable adversarial robustness.
TATS, pages 2431–2439. PMLR, 2021.

Samuel Yeom and Matt Fredrikson. Individual Fairness Re-
visited: Transferring Techniques from Adversarial Robust-
ness, 2020.

Mikhail Yurochkin, Amanda Bower, and Yuekai Sun. Train-
ing individually fair ML models with Sensitive Subspace
Robustness, 2020.

Appendix to:
Individual Fairness Guarantees for Neural
Networks

In Section A we empirically investigate the convergence of
the PWL bounds w.r.t. M in the sigmoid case, and provide de-
tailed proofs for the statements of propositions and theorem
from the main paper. In Section B we discuss how the learn-
ing of the similarity metric dfair was performed. Section C
details the experimental settings used in the paper and brieﬂy
describes fairness-through-unawareness and SenSR. Finally,
additional experimental results on group fairness, veriﬁca-
tion, and feature embedding metrics are given in Section D.

A Additional Details on MILP
A.1 Analysis of Number of Grid Points
Interestingly, by inspecting the error bounds derived in
Proposition 2 we notice how the uniform error of the PWL
bounds goes to zero with the product between the inverse of
M and the increments of the derivative of σ parametrised with
the inverse of M . In practice, this means that choosing the
interval points of the grid adaptively depending on the values
of σ(cid:48) yields improved rate of convergence for the bounds. In
fact, in Appendix A, by choosing the grid points in inverse
proportion to M in practice, for M = 32, we have almost
perfect overlap of the PWL with σ. We visualised this in Fig-
ure 1 in the main paper, where we plot the lower and upper
PWL functions used in our MILP construction (the plots il-
lustrate the explicit case of the sigmoid activation function
in the interval [−5, 5]). The inﬂection point in the case of
the sigmoid is in the axis origin, so it is straightforward to
discretise the x-axis into convex and concave parts of the sig-
moid. In particular, we achieve this by using a non-uniform
discretisation of the x-axis that follows the y-axis of the plot.
Empirically, we found that this provides better bounds than a
uniform x-axis discretisation in the case in which M (number
of grid points used) is small. The ﬁgures visually show how
the bounds converge as M increases. Already for M = 32
the maximum approximation error is of the order of 10−5,
and thus this is the value we utilise in the experiments.

(cid:17)

φ(i)
j

it follows that there exist values for η(i)
j φ(i)

Proof of Proposition 1 Consider the j-th activation func-
tion and the i-th layer we want to show that everytime ζ (i)
j =
σ(i) (cid:16)
j,l and y(i)
for l = 1, . . . , M , such that ζ (i)
satisﬁes the constraints
in the proposition statement. This would imply that the fea-
sible region deﬁned by the latter equation is larger than that
j = σ(i) (cid:16)
deﬁned by ζ (i)
, and that it hence provide a safe
over-approximation of it.

φ(i)
j

(cid:17)

j,l

j

By using Lemma 3.1, we know that

ζ (i)
j = σ(i)(φ(i)
j = σ(i)(φ(i)
ζ (i)

j ) ≥ η(i)
j ) ≤ η(i)

j,l ζ PWL,(i),L
j,l ζ PWL,(i),U

j,l−1

j,l−1

+ η(i)
+ η(i)

j,l+1ζ PWL,(i),L
j,l+1ζ PWL,(i),U

j,l

j,l

,

,

where we notice that η(i)
j,l ). By employing
the Special Ordered Set (SOS) 2 reformulation of piecewise

j,l+1 = (1 − η(i)

functions [Milano et al., 2000], we then obtain:

M
(cid:88)

l=1

y(i)
j,l = 1,

M
(cid:88)

l=1

η(i)
j,l = 1,

φ(i)
j =

M
(cid:88)

l=1

j,l η(i)
φ(i)L

j,l , y(i)

j,l ≤ η(i)

j,l + η(i)

j,l+1,

M
(cid:88)

l=1

M
(cid:88)

l=1

ζ PWL,(i),L
j,l

η(i)
j,l ≤ ζ (i)

j

ζ PWL,(i),U
j,l

η(i)
j,l ≥ ζ (i)

j

which is equivalent to the Proposition statement.
Proof of Proposition 2 For simplicity of notation, we drop
the subscripts and superscripts from the proof, and refer to a
general activation of a general hidden layer of the NN f w.

Without loss of generality, assume the non-linearity σ(φ)
is convex in [φl, φl+1], with φl+1 − φl = φU −φL
M (the con-
cave case follows specularly from the convex by opportunely
considering −σ(φ)).

Following the construction discussed in Section 3.1, the
lower bound in this case is given by the tangent through the
midpoint, i.e., σL(φ) = σ(c) + (φ − c)σ(cid:48)(c), where c =
(φl+1 − φl)/2, where c = (φl+1 − φl+1)/2. We consider the
lower bounding error e1(φ) = |σL(φ) − σ(φ)|. By deﬁnition
of convexity and differentiability of σ we have:
σ(c) ≥ σ(φl) + (c − φl)σ(cid:48)(φl).

Hence, for the error we obtain the following chain of inequal-
ities:

e1(φ) = σ(φ) − σL(φ) =
σ(φ) − σ(c) − (φ − c)σ(cid:48)(c) ≤
− (c − φ)σ(cid:48)(φ) − (φ − c)σ(cid:48)(c) =
(φ − c)(σ(cid:48)(φ) − σ(cid:48)(c)).

which can be reformulated in terms of M :

e1(φ) ≤
φU − φL
2M

(cid:18)

σ(cid:48)(φl+1) − σ(cid:48)

(cid:18)

φl+1 −

(cid:19)(cid:19)

φU − φL
2M

(5)

For the upper-bound function, we have: σU (φ) = σ(φl) +

(φ − φl) σ(φl+1)−σ(φl)

. Again by convexity we obtain:

φl+1−φl
σ(φ) ≥ σ(φl) − σ(cid:48)(φl)(φ − σ(φl)).

so that for the error we have the following chain of inequali-
ties:

e2(φ) = σU (φ) − σ(φ) =

σ(φl) + (φ − φl)

σ(φl+1) − σ(φl)
φl+1 − φl

− σ(φ) ≤

(

σ(φl+1) − σ(φl)
φl+1 − φl

+ σ(cid:48)(φl))(φ − φl).

Model Learning Rate Regularization Epochs

Hidden Layers

Adult

Credit

German

Crime

FTU
SenSR
MILP
FTU
SenSR
MILP
FTU
SenSR
MILP
FTU
SenSR
MILP

0.025
0.001
0.001
0.002
0.0025
0.0025
0.001
0.0025
0.0025
0.001
0.025
0.025

0.0125
0.05
0.05
0.02
0.04
0.04
0.02
0.04
0.04
0.02
0.025
0.025

35
400
400
50
100
100
35
250
250
35
100
100

[8], [16], [24], [64], [8,8], [16,16]

[8], [16], [24], [64], [8,8], [16,16]

[8], [16], [24], [64], [8,8], [16,16]

[8], [12], [16], [24], [8,8], [16,16]

Table 1: Hyperparameter values used for the models employed in the analysis in Section 4 and in the comparison between training methods
in Section 4

Hence, by rewriting it in terms of M , we obtain:

e2(φ) ≤
(cid:32)

σ(φl + φU −φL
M ) − σ(φl)
φU −φL
M

(cid:33)

+ σ(cid:48)(φl)

φU − φL
M

.

(6)

Uniform convergence as M tends to inﬁnity follows
straightforwardly from the fact that Equations (5) and (6) are
independent of any particular value of φ and that they tend to
zero as M goes to inﬁnity.

Proof of Theorem 1 The theorem statement follows if we
show that the feasible region of the MILP of Equation (4)
over-approximates the feasible region of the individual fair-
ness optimisation problem whose constraints are given in
Equations (2) and (3). In fact, if this holds then any solution
δ∗ of the optimisation problem of Equation (4) would provide
an upper bound to the solution of Problem 1, so that for any
δ ≥ δ∗ we would have that f w is (cid:15)-δ-IF.

Fairness Constraint: For the model constraint, this fol-
lows directly from the construction of Section 3.1, so that we
have that −
implies
dfair(x(cid:48), x(cid:48)(cid:48)) ≤ (cid:15).

≤ U T x(cid:48) − U T x(cid:48)(cid:48) ≤

(cid:15)√

(cid:15)√

diag(Λ)

diag(Λ)

Model Constraint: We ﬁrst rewrite the NN explicitly by
using the notation of Equation (1) in x(cid:48) and x(cid:48)(cid:48), so that we
have δ = ζ (cid:48)(L) − ζ (cid:48)(cid:48)(L), and for i = 1, . . . , L:

ζ (cid:48)(i) = σ(i) (cid:16)
ζ (cid:48)(0) = x(cid:48), φ(cid:48)(i) = W (i)ζ (cid:48)(i−1) + b(cid:48)(i),
ζ (cid:48)(cid:48)(0) = x(cid:48)(cid:48), φ(cid:48)(cid:48)(i) = W (i)ζ (cid:48)(cid:48)(i−1) + b(cid:48)(cid:48)(i), ζ (cid:48)(cid:48)(i) = σ(i) (cid:16)

φ(cid:48)(i)(cid:17)
φ(cid:48)(cid:48)(i)(cid:17)

.

The ﬁrst two constraints in each of the two rows above are
already linear constraints, and in this form appear in the
MILP formulation. For the activation constraints, i.e. ζ †(i)
j =
σ(i) (cid:16)
for † ∈ {(cid:48),(cid:48)(cid:48) } and j = 1, . . . , ni, we proceed
by computing PWL lower and upper bound functions using

φ†(i)
j

(cid:17)

Lemma 3.1 and converting it into MILP form using Proposi-
tion 1. This yields the ﬁnal form of the MILP we obtain.

B Metric Learning
Recently, a line of work aimed at practical methods of learn-
ing more expressive fair distance metrics from data has been
developed [Ilvento, 2019, Mukherjee et al., 2020, Yurochkin
et al., 2020]. In this section we expand on the methodology
used for metric learning in our experiments.

B.1 Mahalanobis
For the learning of the similarity metric dfair in the form of
a Mahalanobis distance, we rely on the techniques described
in Yurochkin et al. [2020] that form the basis of the SenSR
approach (to which we compare in our experiments). Brieﬂy,
this works as follows. Consider for simplicity the case of one
sensitive feature (e.g., race) with K possible categorical val-
ues. We train a softmax model to predict each value of the
sensitive feature by relying on the non-sensitive features. Let
xnon-sens denote the feature vector corresponding to only the
non-sensitive features, and similarly xsens denoting the sensi-
tive features. We then have:
exp (aT
k=1 exp (aT

k xnon-sens + bk)

k xnon-sens + bk)

p(xsens = k) =

, k = 1, ..., K

(cid:80)K

(7)
where p(xsens = k) indicates the conﬁdence given by the soft-
max model to the sensitive feature having the k-th value. In-
tuitively, the vector aT
k , for k = 1, ..., K, then represents a
sensitive direction in the non-sensitive features space that cor-
relates xnon-sens to the k-th value of xsens. We then stack the
weights of each model, deﬁning the matrix A = [a1, . . . , aK],
and compute its matrix span ran(A), which combines all the
sensitive directions in deﬁning a sensitive subspace. We ﬁ-
nally ﬁnd its orthogonal projector S = I − Pran(A), which
is then used to deﬁne the Mahalanobis distance metric as:
dfair(x(cid:48), x(cid:48)(cid:48)) = (cid:112)(x(cid:48) − x(cid:48)(cid:48))T S(x(cid:48) − x(cid:48)(cid:48)).

Figure 5: Certiﬁed bounds on individual fairness (δ∗) for different architecture parameters (widths and depths) and maximum similarity ((cid:15))
for the Credit (ﬁrst and second column) and the German (third and fourth column) datasets. Top Row: Mahalanobis similarity metric used
for dfair. Bottom Row: Weighted (cid:96)∞ similarity metric used for dfair.

In the case in which the sensitive feature has a continuous
rather than a categorical value, the softmax model of Equa-
tion (7) can be replaced by a linear ﬁtting model, and the
remainder of the computation follows analogously. Finally,
we remark that in the case in which many features are se-
lected as sensitive, one can proceed similarly to what has been
described just above, by learning a different model for each
sensitive feature, and then stacking all the weights obtained
together when deﬁning the matrix A.

B.2 Weighted (cid:96)p
For ease of comparison, we rely on the approach of John et al.
[2020], which in particular focuses on a weighted (cid:96)∞ metric,
by setting up the weights to zero for the sensitive features and
to a common (cid:15) for all the remaining features (we remark that
our method is not limited just to (cid:96)∞, but can be used for any
general weighted (cid:96)p metric). In the experiments described in
Section 4 of the main paper, we consider multiple values for
(cid:15) varying from 0.01 to 0.25.

B.3 Feature Embedding

In addition to the Mahalanobis and weighted (cid:96)p distance met-
ric, we also allow for the metric dfair to be computed on
an embedding.
Intuitively, this allows for more ﬂexibility
in modelling the intra-relationship between the sensitive and
non-sensitive features in each data point and can be used
to certify individual fairness in data representations such as
those discussed by [Ruoss et al., 2020]. As a proof of con-
cept, we do this by learning a one-layer neural network em-
bedding of 10 neurons, and employ the weighted (cid:96)∞ metric.
Results for this analysis will be given in Section D.2.

C Experimental Setting
In this section we describe the datasets used in this paper and
any preprocessing performed prior to training and certiﬁca-
tion. We then report the hyperparameter values used to train
the different models used in the experiments. All experiments
were run on a NVIDIA 2080Ti GPU with a 20-core Intel Core
Xeon 6230.

Figure 6: Distribution of the true labels of the Crime dataset. Note
how the dataset is very imbalanced towards lower values.

C.1 UCI Datasets
We consider the following UCI datasets [Dua and Graff,
2017], popular in the fairness literature, with the ﬁrst three

being binary classiﬁcation tasks and the last one being a re-
gression task. For all datasets we take an 80/20 train/test split,
drop features with missing values, normalise continuous fea-
tures and one-hot encode categorical features.

Adult: the objective is to classify whether individuals earn
more or less than $50K/year (binary classiﬁcation). Here we
follow similar preprocessing steps as Yurochkin et al. [2020].
After removing native-country and education, and prepro-
cessing, this dataset contains 40 features, it has 45,222 points,
0.24/0.76 class imbalance, and we consider sex and race to be
categorical sensitive attributes.

Credit:

the goal is to predict whether people will de-
fault on their payments (binary classiﬁcation). After prepro-
cessing, the dataset has 144 features, 30,000 data points, a
0.22/0.78 class imbalance, and x2 (corresponding to sex) is
considered a sensitive attribute.

German: the goal is to classify individuals as good/bad
credit risks (binary classiﬁcation). After preprocessing, the
dataset has 58 features, 1000 data points, a 0.3/0.7 class im-
balance and status sex is considered a categorical sensitive
attribute.

Crime: the goal is to predict the normalised total number
of violent crimes per 100K population. After preprocessing,
the dataset has 97 features, 1993 data points, and racepct-
black, racePctWhite, racePctAsian, racePctHisp are consid-
ered continuous sensitive attributes. The true label distribu-
tion of this dataset is very imbalanced, as shown in Figure 6.

C.2 Hyperparameters
The hyperparameters used to train all of the FTU, SenSR and
MILP models used in the experiments are reported in Ta-
ble 1. The hidden layer values were selected to match the
type of models trained in related literature (e.g. Yurochkin
et al. [2020], Urban et al. [2020], Ruoss et al. [2020]). The
values of learning rate, regularisation and number of epochs
were selected as the result of some hyperparameter tuning, to
provide accuracy results matching those found in literature.

C.3 Training Methods
Below we describe the alternative fair training methods that
are employed for comparison with our proposed training
method. We note that for all methods, categorical variables
are one-hot encoded, and, since MILP solvers can deal with
both continuous and integer variables, no further processing
is required.

Fairness through unawarness (FTU) The general princi-
ple of fairness through unawareness training is that by remov-
ing the sensitive features (e.g. features containing information
about gender or race) the classiﬁer will no longer use such
information to make decisions. Despite removal of the sen-
sitive features, it is often found that these have correlations
with non-sensitive features, which can lead to classiﬁers that
are still greatly inﬂuenced by the sensitive features [Pedreshi
et al., 2008].

SenSR SenSR is a methodology proposed by Yurochkin
et al. [2020] that leverages PGD to generate individually un-
fair adversarial examples to augment the training procedure.
It supports similarity metrics in the form of a Mahalanobis

distance, akin to the one we describe in Subsection B.1. We
adapt their code to work on both binary classiﬁcation and re-
gression tasks to compare with our MILP method. Our MILP
method bears many similarity to theirs, hence why we use
it for comparison. However, while both our training meth-
ods rely on adversarial training to mitigate against unfairness,
SenSR does not provide any veriﬁcation methodology. Fur-
thermore, our MILP training, while being meaningfully more
computationally intensive, achieves better local optimisation
thus proving upon veriﬁcation to train models order of mag-
nitude fairer than SenSR.

D Additional Experimental Results
In this section we give further empirical evidence support-
ing the effectiveness of our certiﬁcation framework as well
as our fairness training methodology. We start by giving ex-
tended results and discussion on certiﬁcation for the German
and Credit datasets, as well as demonstrate that our method
can scale to larger networks than those reported in the main
text. We then proceed to do the same for our fairness training
algorithm and extend the discussion where appropriate.

D.1 Fairness Certiﬁcation for Credit and German

Datasets

In Figure 5 we report similar analyses to that of Figure 2 (in
main text), illustrating how the values of δ∗ change with re-
spect to changes in (cid:15) and number of neurons and hidden lay-
ers used for the neural network architecture (using the same
parameters as in Section 4) for Credit and German datasets.
The top row in the ﬁgure was computed considering a Ma-
halanobis individual similarity metric; the bottom row was
computed for a weighted (cid:96)∞ metric. Notice that we obtain
similar trends as those discussed for Figure 2.

D.2 Certiﬁcation with Feature Embedding

Similarity Metric

In Figure 7a we depict the results for individual fairness cer-
tiﬁcation by using the feature embedding approach for the
deﬁnition of dfair. In particular, we use a neural network with
10 neurons as the embedding. The results are given for a NN
with 2 layers, 16 hidden units per layer, using FTU for each
dataset. The results show that we are able to obtain non-trivial
bounds even when an embedding is used.

D.3 Additional Hyperparameters Analysis
Up to this point we have studied architectures matching those
explored in related literature [Yurochkin et al., 2020, Ruoss
et al., 2020, Urban et al., 2020]. We now show that our ver-
iﬁcation method can be applied to far larger architectures to
obtain non-trivial results.
In Figure 7b we train with FTU
six different neural networks on the Crime dataset, each with
2 hidden layers with the same number of neurons and up to
2048 neurons per layer. For reference, the largest network
trained by Yurochkin et al. [2020] has a single hidden layer
with 100 neurons, and comes with no formal guarantees.

We then report the value of δ∗ certiﬁed by the solver for
each model (referred to by its total number of neurons, e.g.,
16 stands for a NN with 2 hidden layers with 8 neurons

(a)

(b)

Figure 7: 7a Individual fairness for the various datasets after ap-
plying a 10 hidden units linear embedding. All networks are fully
connected and have two layers with 16 neurons each. As expected,
we observe that δ increases with (cid:15). Furthermore, the Adult dataset is
particularly sensitive to perturbations. 7b Certiﬁcation for increasing
architecture size. We notice that our method provides non-vacuous
bounds even for hundreds of neurons.

(a)

(b)

(c)

(d)

Figure 8: In each plot we analyse how the value of delta changes
for different cutoff times given to the solver. We analyse this over 5
different architectures.The details of the architectures and their cor-
responding colors are given in the plot (a), the top-most ﬁgure. We
also report the time taken to build the MILP problem for each archi-
tecture, to show how scaling the model size affects each portion of
the MILP solution (building and solving constraints).

each). We notice the value of δ∗ seems to increase. This is
most likely due to the fact that for larger networks the MILP
problem becomes more difﬁcult to solve, and the time cut-
off could has bigger impact on the tightness of the approxi-
mation found. However we note that, even for larger mod-
els with thousands of neurons, our veriﬁcation method yields
non-trivial bounds after only 3 minutes of computations.

In Figure 8 we train 5 different fully connected architec-
tures for each dataset and verify it for (cid:15) = 0.2. The de-
tails of each architecture and their corresponding colors in
the plots are given in the legend of Figure 8(a), the top most
plot of Figure 8. In particular these architectures vary width
and depth. We repeat the veriﬁcation ﬁve times and average
the results to mitigate any oscillation due to the solver’s in-
herent randomness. We then observe the effect of varying the
cutoff time given to the solver on the value of δ∗. We notice
that the value of δ∗ appears to converge for all datasets, with
the sharpest improvements happening within the ﬁrst 200s.
Therefore, despite the problem being exponential in the worst
case scenario, in practice the over-approximation we imple-
ment seems to stabilise relatively quickly, at least empirically
in these four datasets.

Figure 9: Boxplot showing the quartiles of the distribution of the
values of δ∗ for the three models over each dataset training with 5
independent random seeds

FTU SenSR MILP

Adult
Credit
German
Crime

0.024
0.038
0.026
0.046

0.017
0.22
0.023
0.017

0.0056
0.12
0.0022
0.00055

Table 2: Standard deviation values for δ∗ obtained training models
with an 8-neurons one hidden layer architecture for the 5 indepen-
dent random seeds

D.4 Training
In Figure 10 we re-plot the scatter plots from Figure 4 (in the
main text) for the binary classiﬁcation tasks using the stan-
dard test accuracy on the x-axis (instead of the balanced accu-
racy). Notice that, in terms of standard accuracy, FTU clearly

Model

Adult Credit German Crime

[8]

[12]

[16]

[24]

[64]

[8,8]

[16,16]

FTU
SenSR
MILP
FTU
SenSR
MILP
FTU
SenSR
MILP
FTU
SenSR
MILP
FTU
SenSR
MILP
FTU
SenSR
MILP
FTU
SenSR
MILP

21s
17s
121m
-
-
-
22s
19s
169m
22s
21s
196m
23s
22s
302m
23s
20s
203m
22s
22s
353m

28s
7s
361m
-
-
-
29s
7s
354m
28s
7s
428m
30s
8s
628m
28s
7s
447m
30s
9s
710m

6s
4s
12m
-
-
-
6s
4s
13m
6s
4s
15m
6s
5s
19m
6s
4s
14m
6s
5s
19m

6s
3s
11m
6s
3s
14m
6s
3s
11m
6s
3s
16m
-
-
-
6s
3s
9m
6s
4s
20m

Table 3: Training times for different architectures on the various
datasets used for Algorithm 1 using (cid:15) = 0.2

outperforms the other two methods. Intuitively, FTU is only
concerned with the maximisation of accuracy, while the other
two methods (SenSR and our MILP approach) also optimise
for fairness, which, like discussed in Section 4 of the main pa-
per, seems to provide them some regularisation against class
imbalance. We also study the standard deviation in the val-
ues of δ∗ training models with different random seeds in Ta-
ble 2, and make two observations. Firstly, given those values
the difference in values of delta observed for each method
remains signiﬁcant. Secondly, while each of these methods
is affected by randomness, our method obtains the smallest
values of standard deviation, thus providing more consistent
results. However, as mentioned in the main paper, our MILP
based training method has the main drawback of computa-
tional overhead. While in the next section we perform further
experiments on the scalability of our training method, in Ta-
ble 3 we report the training times for all the NNs we trained in
our experiments, to show that the trade off for orders of mag-
nitude better fairness is signiﬁcantly longer training times.

D.5 Training Scalability

We train a model with the same architecture as the largest
network trained by Yurochkin et al. [2020], with a single
hidden layer and 100 neurons, on the Adult dataset (using
same values of learning rate, regularization and number of
epochs as in Table 1). Upon veriﬁcation, our model is guaran-
teed to have δ∗ = 0.0007221, while the same model trained
with the methodology from Yurochkin et al. [2020] obtains
δ∗ = 0.042. This notable improvement in guaranteed fair-
ness bound comes at the signiﬁcant cost in training time, as

our model takes 9h to train. We also notice that the guarantees
that this network obtains are very similar to the ones obtained
with smaller network.

D.6 Group Fairness
In this section, we inspect how our MILP individual fair-
ness training impacts group fairness as measured by Equal-
ized Odds Difference (EOD) (calculated using the Fairlearn
library, and inspired by the deﬁnition given by Hardt et al.
[2016]). Group fairness deﬁnitions largely concern them-
selves with summary statistics of the model performance on
the entire test dataset rather than for an individual, so our
training method does not optimize w.r.t. group fairness neces-
sarily, though one would expect more individually-fair mod-
els to also improve on group fairness. We report our results
on group fairness in Table 4.

Interestingly, we observe that our method performs com-
parably to SenSR and both improve on EOD when compared
with FTU.

Figure 10: Accuracy / individual fairness trade-off for NNs trained with fairness-by-unawareness, SenSR, and MILP for (cid:15) = 0.2. Each dot
in the scatter plot represents a different architecture.

Adult

Credit

German

Model Bal. Accuracy

Accuracy

δ∗

EOD

FTU
0.718 ± 0.035
SenSR 0.743 ± 0.012
0.761 ± 0.001
MILP
FTU
0.648 ± 0.002
SenSR 0.687 ± 0.005
0.699 ± 0.006
MILP
FTU
0.623 ± 0.069
SenSR 0.719 ± 0.013
0.719 ± 0.011
MILP

0.831 ± 0.007
0.694 ± 0.020
0.715 ± 0.002
0.817 ± 0.001
0.743 ± 0.007
0.779 ± 0.004
0.751 ± 0.032
0.705 ± 0.009
0.685 ± 0.010

0.848 ± 0.017
0.063 ± 0.009
0.006 ± 0.005
0.230 ± 0.016
0.024 ± 0.015
0.008 ± 0.003
0.090 ± 0.048
0.107 ± 0.024
0.002 ± 0.001

0.663 ± 0.070
0.550 ± 0.087
0.600 ± 0.000
0.014 ± 0.003
0.021 ± 0.011
0.007 ± 0.004
0.384 ± 0.181
0.295 ± 0.006
0.304 ± 0.015

Table 4: Comparison of different metrics for 3 different model types across 3 different datasets (all binary classiﬁcation tasks). The last
column is the group fairness metric Equalized Odds Difference (EOD).

