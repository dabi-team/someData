2
2
0
2

p
e
S
3
1

]

Y
S
.
s
s
e
e
[

2
v
6
9
1
2
1
.
3
0
2
2
:
v
i
X
r
a

Safe and Efﬁcient Model Predictive Control Using Neural Networks:
An Interior Point Approach

Daniel Tabas and Baosen Zhang

Abstract— Model predictive control (MPC) provides a useful
means for controlling systems with constraints, but suffers from
the computational burden of repeatedly solving an optimization
problem in real time. Ofﬂine (explicit) solutions for MPC
attempt to alleviate real time computational challenges using
either multiparametric programming or machine learning. The
multiparametric approaches are typically applied to linear or
quadratic MPC problems, while learning-based approaches
can be more ﬂexible and are less memory-intensive. Exist-
ing learning-based approaches offer signiﬁcant speedups, but
the challenge becomes ensuring constraint satisfaction while
maintaining good performance. In this paper, we provide a
neural network parameterization of MPC policies that explicitly
encodes the constraints of the problem. By exploring the interior
of the MPC feasible set in an unsupervised learning paradigm,
the neural network ﬁnds better policies faster than projection-
based methods and exhibits substantially shorter solve times.
We use the proposed policy to solve a robust MPC problem,
and demonstrate the performance and computational gains on
a standard test system.

I. INTRODUCTION

Model predictive control (MPC) [1] is a powerful tech-
nique for controlling systems that are subject to state and
input constraints, such as agricultural [2], automotive [3],
and energy systems [4]. However, many applications require
fast decision-making which may preclude the possibility of
repeatedly solving an optimization problem online [5].

A popular approach for accelerating MPC is to move
as much computation ofﬂine as possible [5], [6]. These
techniques, known as explicit MPC, involve precomputing
the solution to the MPC problem over a range of parameters
or initial conditions. Most of the research effort has focused
on problems with linear dynamics and constraints, and linear
or quadratic cost functions. In this case, the explicit MPC
solution is a piecewise afﬁne (PWA) function deﬁned over a
polyhedral partition of the state constraints. However, many
of the applications of interest have cost functions that are not
necessarily linear or quadratic, or even convex. Further, the
memory required to store the partition and afﬁne functions
can be prohibitive even for modestly-sized problems.

In order to reduce the complexity of explicit MPC, the op-
timal ofﬂine solution can be approximated. Approximations

This work is partially supported by the National Science Foundation
Graduate Research Fellowship Program under Grant No. DGE-1762114,
NSF grants ECCS-1930605 and ECCS-2023531. Any opinions, ﬁndings,
conclusions, or recommendations expressed in this material are those of the
authors and do not necessarily reﬂect the views of the National Science
Foundation.

Authors are with the department of Electrical and Computer Engi-
neering, University of Washington, Seattle, WA, United States. {dtabas,
zhangbao}@uw.edu.

generally fall into two categories: partition-based solutions
[7]–[9] that generate piecewise control laws over coarser
state space partitions, and learning-based solutions [10]–[13]
that use function approximation to compactly represent the
optimal MPC policy. In this paper, we focus on the latter
with the key contribution of ensuring constraint satisfaction
while exploring all feasible policies.
Constraint satisfaction is crucial

in many engineering
applications, and the ability of MPC to enforce constraints is
a major factor in its popularity. However, it is not straightfor-
ward to guarantee that a learning-based solution will satisfy
constraints. The main challenge arises from the fact that
while neural networks can limit their outputs to be in simple
regions, there is no obvious way of forcing complex con-
straint satisfaction at the output. In [10], [14], supervised and
unsupervised learning were used to approximate the solution
of MPCs, but did not provide any feasibility guarantees. By
contrast, [11] trains an NN using a policy gradient approach,
and guarantees feasibility by projecting the NN output into
the feasible action set. However, this extra optimization step
slows down the speed of online implementation, making it
difﬁcult to use in applications that require high-frequency
solutions [15]. Supervised learning approaches that provide
safety guarantees [12], [13] rely on a choice of MPC oracle
that is not obvious when persistent disturbances are present.
In this paper, we propose an NN architecture for approx-
imating explicit solutions to ﬁnite-horizon MPC problems
with linear dynamics, linear constraints, and arbitrary differ-
entiable cost functions. The proposed architecture guarantees
constraint satisfaction without relying on projections or MPC
oracles. By exploring the interior of the feasible set, we
demonstrate faster training and evaluation, and comparable
closed-loop performance relative to other NN architectures.
The proposed approach has parallels in interior point
methods for convex optimization [16]. Interior point methods
ﬁrst solve a Phase I problem to ﬁnd a strictly feasible
starting point. This solution is used to initialize the Phase II
algorithm for optimizing the original problem. Our approach
accelerates both phases. The Phase I solution is given by a
simple function (e.g., afﬁne map) and the Phase II problem
is solved using an NN architecture that can encode arbitrary
polytopic constraints (Fig. 1).

The Phase II solution builds on a technique ﬁrst proposed
in [17], which uses a gauge map to establish equivalence
between compact, convex sets. With respect to [17], the
current work has three novel aspects. First, the reinforcement
learning (RL) algorithm in [17] only uses information about
the constraints, and does not use information about the cost

 
 
 
 
 
 
function or dynamics. The resulting policy is safe, but can
exhibit suboptimal performance. The MPC formulation in
the current paper gives rise to a training algorithm that can
exploit knowledge about the system, improving performance.
Second, the MPC formulation permits explicit consideration
for future time steps. The RL formulation cannot optimize
entire trajectories due to the presence of constraints. This
inability to “look ahead” again limits the performance of
the RL algorithm. Finally,
the previous work required a
Phase I that used a linear feedback to ﬁnd a strictly feasible
point. A linear feedback, however, may not exist for some
problems. The current work proposes a more general class of
Phase I solutions (piecewise afﬁne), while providing a way
to manage the complexity of the Phase I solution.

We demonstrate the effectiveness of the proposed tech-
nique on a 3-state test system, and compare to standard
projection- and penalty-based approaches for learning with
constraints. The results show that the proposed technique
achieves Pareto efﬁciency in terms of closed-loop perfor-
mance and online computation effort. All code is available
at github.com/dtabas/gauge networks.

Notation: The p-norm ball for p ≥ 1 is Bp = {z | (cid:107)z(cid:107)p ≤
1}. A polytope P ⊂ Rn := {z ∈ Rn | F z ≤ g} is
the (bounded) intersection of a ﬁnite number of halfspaces.
Scaling of polytopes by a factor λ > 0 is deﬁned as
λP = {λz ∈ Rn | F z ≤ g} = {z ∈ Rn | F z ≤ λg}. Given
a matrix F and a vector g, the ith row of F is denoted
F (i)T and the ith component of g is g(i). The interior of
any set Q is denoted int Q. The value of a variable y at a
time interval t is denoted yt. A state or control trajectory of
length τ is written as the vector x = (cid:2)xT
∈ Rnτ
or u = (cid:2)uT
∈ Rmτ . The column vector of all
ones is 1. The symbol ◦ denotes function composition.

0 , . . . , uT

1 , . . . , xT
τ

τ −1

(cid:3)T

(cid:3)T

II. PROBLEM FORMULATION

In this paper, we consider the problem of regulating

discrete-time dynamical systems of the form

xt+1 = Axt + But + dt
where xt ∈ Rn is the system state at time t, ut ∈ Rm is the
control input, and dt ∈ Rn is an uncertain input that captures
exogenous disturbances and/or linearization error (if the true
system dynamics are nonlinear) [18]. We assume the pair

(1)

(A, B) is stabilizable. The input constraints (actuation limits)
are U = {u ∈ Rm | Fuu ≤ gu} while the state constraints
arising from safety-critical engineering considerations are
X = {x ∈ Rn | Fxx ≤ gx}.

We consider the problem of operating the system (1)
using ﬁnite-horizon model predictive control. The goal is to
choose, given initial condition x0 ∈ X , a sequence of inputs
u of length τ that minimizes the cost of operating the system
while respecting the operational constraints.

However, since the disturbances dt are unknown ahead of
time, the designer must carefully consider how to achieve
both optimality and constraint satisfaction. Robust MPC
literature contains many ways to handle the presence of
disturbances in both the cost and constraints [19]. For exam-
ple, the certainty-equivalent approach [5] considers only the
nominal system trajectory, while the min-max approach [9]
considers the worst-case disturbance. Interpolating between
these two extremes, the tube-based approach [20] considers
the cost of a nominal trajectory while guaranteeing that the
true trajectory satisﬁes constraints. A stochastic point of view
in [21] considers the disturbance as a random variable and
minimizes the expected cost while providing probabilistic
guarantees for constraint satisfaction.

In most robust MPC formulations, the set of possible
disturbances is modeled as either a ﬁnite set, a bounded set,
or a probability distribution [22]. In this paper, we assume
the disturbances lie in a closed and bounded set D := {d ∈
Rn | Fdd ≤ gd}. In order to ensure constraint satisfaction,
we operate the system within a robust control invariant set
(RCI) S ⊆ X , deﬁned as a set of initial conditions for
which there exists a feedback policy in U keeping all system
trajectories in S, under any disturbance sequence in D [23].
In our simulations, we used approximately-maximal RCIs
computed with the semideﬁnite program from [24].

s = g(i)

s −maxd∈D F (i)T

With S := {x ∈ Rn | Fsx ≤ gs}, we deﬁne the target set
T as {x ∈ Rn | x + d ∈ S, ∀ d ∈ D} = {x ∈ Rn | Fsx ≤
˜gs} where for each row i, ˜g(i)
d [23].
Any policy that maps S to T under the nominal dynamics
will map S to itself under the true dynamics, rendering S
robustly invariant. By constraining the nominal state to the
target set, robust constraint satisfaction is guaranteed for the
ﬁrst time step. Since S is RCI, this is sufﬁcient for keeping
closed-loop trajectories inside S. Under this formulation, the
MPC problem is posed as follows, given initial state x0:

s

min
u

τ −1
(cid:88)

k=0

l(xk, uk) + lF (xτ )

subject to ∀ k: xk+1 = Axk + Buk
xk+1 ∈ T
uk ∈ U

(2a)

(2b)

(2c)

(2d)

Fig. 1.
Illustration of the interior point approach to learning-based MPC.
The set F (x0) represents the MPC feasible set, while µ0(x0) and µθ(x0)
are control input sequences representing solutions to the Phase I and Phase
II problems, respectively. The neural network µθ moves the Phase I solution
to a more optimal solution.

where l and lF are stage and terminal costs that are
differentiable but possibly nonlinear or even non-convex.
Although (2) differs from the standard tube-based approach,
the techniques introduced in this paper can be applied to a
variety of MPC formulations.

In this paper, we seek to derive a safe feedback policy
πθ : Rn → Rm that approximates the explicit solution
to (2) by ﬁrst approximating the optimal control sequence
with a function µθ : Rn → Rmτ and then implementing
the ﬁrst action of the sequence in the closed loop. In
practice, any MPC policy implemented in closed loop must
be stabilizing and recursively feasible. Recursive feasibility
is the property that closed-loop trajectories generated by
the MPC controller will not lead to states in which the
MPC problem is infeasible. This property is guaranteed when
S is RCI [23]. If recursive feasibility is not guaranteed,
then a backup controller must be developed or a control
sequence that is feasible for the most immediate time steps
can be used. There is suggestion in the literature that the
latter approach performs quite well in practice [25], but
the theoretical aspects remain open. In terms of stability,
recursive feasibility guarantees that trajectories will remain
within a bounded set. Since this work focuses on constraint
satisfaction, we do not consider stricter notions of stability.

III. PHASE I: FINDING A FEASIBLE POINT
The feasible set of (2) is a polytope F(x0) ⊆ Rmτ , deﬁned

by the following inequalities in u:

we take advantage of the fact that feasibility problems have
low accuracy requirements: any suboptimal solution to (4)
that achieves a cost s < 0 for all x0 ∈ S is acceptable.

Deﬁnition 1: A function π0 : Rn → Rm is said to solve
(4) if, for all x0 ∈ S, the optimal cost of (4) is negative
when the decision variable u is ﬁxed at π0(x0).

Existing techniques for approximate multiparametric lin-
ear programming [26], especially those that generate con-
tinuous solutions [27], can be used to reduce the memory
requirements of ofﬂine solutions to (4).

To show just how far one can go with reducing complexity,
we will construct an afﬁne (rather than PWA) function that
solves (4), for the system studied in Section V. Let π0(x0) =
W x0 + w. If W ∈ Rm×n and w ∈ Rm satisfy

Fx(Ax0 + B(W x0 + w)) < ˜gx
Fu(W x0 + w) < gu

(5a)

(5b)

for all x0 ∈ S, then π0(x0) = W x0 + w solves (4). The
following optimization problem can be solved to ﬁnd W
and w or certify that none exists. Let Y(s) = {x0 ∈ Rn |
Fs(Ax0+B(W x0+w)) ≤ ˜gs+s1, Fu(W x0+w) ≤ gu+s1}.
If the optimal cost of

(3a)

min
W,w,s

s subject to S ⊆ Y(s)

(6)

Hs(M0x0 + Muu) ≤ ˜hs,
Huu ≤ hu

(3b)
where Hs, Hu, M0, Mu, ˜hs, and hu are block matrices and
vectors derived from the system dynamics and constraints. In
this paper, we assume that F(x0) has nonempty interior for
all x0 ∈ S. Since the state constraints S form an RCI, F(x0)
is already guaranteed to be nonempty, and the assumption of
nonempty interior is only marginally more restrictive.

The gauge map technique introduced in [17] provides a
way to constrain the outputs of a neural network µθ : Rn →
Rmτ to F(x0) without a projection or penalty function, but
F(x0) must contain the origin in its interior. If this is not the
case, then we must temporarily “shift” F(x0) by subtracting
any one of its interior points. In this section, we discuss
several ways to reduce the complexity of ﬁnding an interior
point.

We begin by considering the feasibility problem for the
one-step safe action set deﬁned as V(x0) = {u ∈ Rm |
u ∈ U, Ax0 + Bu ∈ T }, which is guaranteed to have an
interior point by the assumption on F(x0). One way to
ﬁnd an interior point of V(x0) is to minimize the maximum
constraint violation:

s

(4a)

min
u,s
subject to: Fs(Ax0 + Bu) ≤ ˜gs + s1
Fuu ≤ gu + s1
which has an optimal cost s∗ ≤ 0 if V(x0) is nonempty, and
s∗ < 0 if V(x0) has nonempty interior [16]. To avoid solving
a linear program online during closed-loop implementation,
the solution to (4) can be stored as a piecewise afﬁne (PWA)
function π0(x0) : Rn → Rm [7]. Although solutions to mul-
tiparametric LPs can be demanding on computer memory,

(4b)

(4c)

is negative, then (5) holds for all x0 ∈ S, thus π0 solves
(4). This happens to be the case for the example in Section
in (6) is a polytope
V,
containment constraint in halfspace representation, thus (6)
can be solved as a linear program [28].

taken from [6]. The constraint

Now consider the feasibility problem for F(x0), which
is obtained by replacing (4b) and (4c) with (3a) and (3b),
and changing the optimization variable from u ∈ Rm to
u ∈ Rmτ . One would naturally expect the complexity of
the PWA solution to this feasibility problem to increase
rapidly with the time horizon τ , as more decision variables
and constraints are added. However, the next proposition
shows that the cardinality of the stored partition can be made
constant in τ .

Proposition 1 (Phase I solution): If π0 solves (4), then
the vector µ0(x0) := (cid:2)π0(x0)T , . . . , π0(xτ −1)T (cid:3)T
, where
xk+1 = Axk + Bπ0(xk), is an interior point of F(x0) for
any x0 ∈ S.
Proof:

If π0 solves (4), then π0(x) ∈ int V(x) for all
x ∈ S. Applying the deﬁnition of V in an inductive argu-
ment, it is straightforward to show that the state trajectory
associated with µ0(x0) is entirely contained in S. Fix any
such trajectory {x1, . . . , xτ } ⊂ S originating from x0 ∈ S
under policy π0. For any k ∈ {1, . . . , τ }, xk ∈ S implies
π0(xk) ∈ int V(xk), which implies π0(xk) ∈ int U and
Axk + Bπ0(xk) ∈ int T . Since this holds for all k, the
constraints deﬁning F(x0) hold strictly at µ0(x0).

In our simulations on the example from [6], (6) was feasi-
ble with negative optimal cost, meaning that a polyhedral
partition of the state space was not needed (see Section
V). This indicates that the minimum number of regions in
a polyhedral state space partition associated with a PWA

is conﬁned to F(x0). Let ψθ : S → B∞ be a neural
network parameterized by θ. A safe policy is constructed
by composing the gauge map G : B∞ → ˜F(x0) with ψθ,
then adding µ0(x0) to map the solution into F(x0):
µθ(x0) = G(· | B∞, ˜F(x0)) ◦ ψθ(x0) + µ0(x0).

(9)

Computing the gauge map online simply requires evaluating
HsM0x0 from (3a) as well as the operations in (7).

The function µθ has several

important properties for
approximating the optimal solution to (2). First, it leverages
the universal function approximation properties of neural
networks [29] along with the bijectivity of the gauge map
(Proposition 2) to explore all interior points of F(x0). This is
an advantage over projection-based methods [11] which may
be biased towards the boundary of F(x0) when the optimal
solution may lie on the interior. Second, µθ is evaluated in
closed form, and its outputs are constrained to F(x0) without
the use of an optimization layer [14] that may have high
computational overhead. Finally, the subdifferentiability of
the gauge map (Proposition 2) enables selection of parameter
θ using standard automatic differentiation techniques.

Optimizing the parameter θ

Similar to the approach taken in [10], we optimize θ by
sampling x ∈ S and applying stochastic gradient descent. At
each iteration, a new batch of initial conditions {xj
j=1 is
sampled from S and the loss is computed as

0}M

J(θ) =

1
M

M
(cid:88)

τ −1
(cid:88)

j=1

k=0

l(xj

k, uj

k) + lF (xj
τ )

(10)

with the control sequences uj given by µθ(xj
0) and state
trajectories xj generated according to the nominal dynamics.
The parameters θ are updated in the direction of ∇θJ, which
is easily computed using automatic differentiation [30].

V. SIMULATIONS

A. Test systems

We simulate the proposed policy using a modiﬁed example
from [6] with n = 3, m = 2, and τ = 5. The system
matrices, constraints, costs, and Phase I solution (found
using (6)) are given below:








A =



.3 −1
.6
.6 −.6

−.5
.2 −.5
1
(cid:107)x(cid:107)∞ ≤ 5, (cid:107)u(cid:107)∞ ≤ 1, (cid:107)d(cid:107)∞ ≤ 0.1,

−.601 −.890
.955 −.715
.246 −.184

 , B =



 ,

(11)

2 + c1(cid:107)u(cid:107)2
l(x, u) = (cid:107)x(cid:107)2
(cid:20) 0.116
0.210 −0.370
−0.320 −0.104 −0.122

2, lF (x) = c2(cid:107)x(cid:107)2
2
(cid:20) −0.157
−0.0533

, w =

(cid:21)

W =

(12)

(13)
(cid:21)

where c1 and c2 are positive constants. Although quadratic
costs are used in the simulations, the proposed method can
work with any differentiable cost.

We evaluate the performance of a given policy in both
open- and closed-loop experiments. In the open-loop experi-
ments, we evaluate the MPC cost (2a) and compare it to the

Fig. 2. The proposed control policy uses a neural network combined with
the Phase I solution and a gauge map to constrain the decision u to the
MPC feasible set F (x0). The ﬁrst action from the sequence u is extracted
and implemented. On the right, the action of the gauge map is illustrated.

solution to (4) is in general very small relative to the number
of regions in an explicit solution to (2).

IV. PHASE II: OPTIMIZING PERFORMANCE

In this section, we construct a class of policies from
x0 ∈ S to F(x0), that can be trained using standard machine
learning packages. Although it is difﬁcult to constrain the
output of a neural network to an arbitrary polytope such as
F(x0), it is easy to constrain the output to the hypercube B∞
by applying a clamping function elementwise in the output
layer. We apply a mapping between polytopes that is closed-
form, differentiable, and bijective. This mapping establishes
an equivalence between B∞ and F(x0), allowing one to
constrain the outputs of the policy to F(x0). The mapping
from B∞ to F(x0) is called the gauge map. The concept is
illustrated in Figure 2.

We begin constructing the gauge map by introducing some
preliminary concepts. A C-set is a convex, compact set that
contains the origin as an interior point. The gauge function
with respect to C-set P ⊂ Rn, denoted γP : Rn → R+, is
the function whose sublevel sets are scaled versions of P.
Speciﬁcally, the gauge of a vector v with respect to P is
given by γP (v) = inf{λ ≥ 0 | v ∈ λP}. If P is a polytopic
C-set given by {v ∈ Rk | F v ≤ g}, then γP is the pointwise
maximum over a ﬁnite set of afﬁne functions [17]:

γP (v) = max

i

F (i)T v
g(i)

.

(7)

Given two C-sets P and Q, the gauge map G : P → Q is

G(v | P, Q) =

γP (v)
γQ(v)

· v.

(8)

This function maps level sets of γP to level sets of γQ.

Proposition 2: Given two polytopic C-sets P and Q, the
gauge map G : P → Q is subdifferentiable and bijective.
Further, given a function π0 from Proposition 1, the set
˜F(x) := [F(x) − π0(x)] is a C-set for all x ∈ S.

Proof: The properties of subdifferentiability and bi-
jectivity come from [17]. For the C-set property, ﬁx x ∈ S.
Since S, U, and D are convex and compact, so is F(x). Since
µ0(x) is an interior point of F(x), the set ˜F(x) contains the
origin as an interior point and is therefore a C-set.

We now use the gauge map in conjunction with the
Phase I solution to construct a neural network whose output

optimal cost. The fraction suboptimality is

δ =

cnn − cmpc
cmpc

(14)

where cnn is the average cost (2a) incurred by the control
sequence µθ on a validation set {xj
j=1 ⊂ S and cmpc is
the optimal cost.

0}Nval

In the closed-loop experiments, we evaluate the perfor-
mance of a policy πθ(xt) : Rn → Rm, t ≥ 0 which
is derived from µθ(xt) by taking the ﬁrst action in the
sequence. We simulate (1) for T (cid:29) τ time steps. The
trajectory cost in the closed-loop experiments is computed
as (cid:80)T −1
t=0 l(xt, ut) + lF (xT ) and the disturbance is modeled
as an autoregressive sequence [31], dt+1 = αdt + (1 − α) ˆd
where α ∈ (0, 1) and ˆd is drawn uniformly over D.

B. Benchmarks

We compare the proposed method to two of the most
common approaches for learning a solution to (2). The ﬁrst
benchmark is a penalty-based approach [32] which enforces
the constraints (2c) and (2d) by augmenting the cost (10)
with a linear penalty term on constraint violations given
by β · max{0, Fxxt − ˜gx} where the max is evaluated
elementwise and β > 0. Since the penalty-based approach
does not encode state constraints in the policy, the policy is
constrained to the Cartesian product U τ = (cid:81)τ −1
k=0 U using
scaled tanh functions elementwise.

The second benchmark is a projection-based approach [11]
which constrains the policy to the set F(x0) by solving a
convex quadratic program in the output layer of a neural
network [33]. The optimization layer v → u returns

arg min
u

(cid:107)v − u(cid:107)2

2 subject to u ∈ F(x0).

Another class of approaches to learning-based MPC seeks
to learn the optimal solution to (2) using regression [12]–
[14]. Speciﬁcally, data-label pairs (x0, u∗
0) are generated
by sampling x0 from S, solving (2) for each sample, and
extracting u∗
0 from the optimal solution u∗. Then, a neural
network or other function approximator is trained to learn the
relationship between x0 and u∗
0. Performance and constraint
satisfaction are handled e.g. by bounding the approximation
error with respect to the MPC oracle. We do not compare
against this type of approach since it requires a large number
of trained samples, making it difﬁcult to compare with our
and the other unsupervised examples.

C. Neural network design

The neural networks were designed with n inputs, mτ
outputs, and two hidden layers with rectiﬁed linear unit
(ReLU) activation functions. The width of the networks
was chosen during hyperparameter tuning. In particular, we
performed 30 iterations of random search over the width
of the network (number of neurons per hidden layer) ∈
{64, . . . , 1024}, the batch size (number of initial conditions,
M ) ∈ {100, . . . , 3000} and the learning rate (LR,
the
step size for gradient descent) ∈ [10−5, 10−3]. For each
set of hyperparameters under consideration, we computed

HYPERPARAMETERS FOR THE THREE NEURAL NETWORKS.

TABLE I

Type
Gauge
Penalty
Projection

Width
859
318
956

LR
4.7 × 10−4
8.7 × 10−4
9.0 × 10−5

M
1655
133
813

TABLE II
OPEN-LOOP TEST RESULTS.

Type
Gauge
Projection

δ (14)
0.007
0.010

Solve time (sec)
.0015
.024

the validation score using (14) with Nval = 100. The
hyperparameters after tuning are reported in Table I.

D. Simulation results

Here we compare our proposed approach (Gauge NN), the
penalty-based approach (Penalty NN), the projection-based
approach (Projection NN) and the “ground truth” obtained
by solving (2) online in cvxpy. The results of the open-
loop experiments are shown in Table II, with performance
computed relative to the optimal MPC solution using (14)
with Nval = 100 trials. The proposed Gauge NN achieves
lower cost compared to the projection-based method, and
has a much lower computational complexity (solve time is
only 6% of projection). Table II only compares the NNs
with safety guarantees because constraint violations are not
accounted for in (14).

Figure 3 shows the training curves for each type of
network. The lower training cost achieved by the Gauge NN
illustrates that it can be more efﬁcient to explore the interior
of the feasible set than the boundary. Since the MPC cost in
the simulations is strictly convex, solutions with lower cost
are closer to the optimal solution.

Figure 4 compares the policies in terms of computation
time and test performance. The box-and-whisker plots in-
dicate the range of performance over 100 test trajectories
of length T = 50, while the vertical position of each box
indicates the average time to compute a control action. Of the
policies with safety guarantees (Gauge NN, Projection NN,
and online MPC), the Gauge NN achieves Pareto efﬁciency
in terms of average solve time and median trajectory cost.

Fig. 3. Training trajectories for the three types of neural netwokrs. Our
proposed Gauge-based approach achieves lower cost at a much faster rate.

[10] B. M.

˚Akesson and H. T. Toivonen, “A neural network model
predictive controller,” J. Process Control, vol. 16, no. 9, pp. 937–946,
2006.

[11] S. Chen, K. Saulnier, N. Atanasov, D. D. Lee, V. Kumar, G. J.
Pappas, and M. Morari, “Approximating Explicit Model Predictive
Control Using Constrained Neural Networks,” Proc. Am. Control
Conf., vol. 2018-June, pp. 1520–1527, 2018.

[12] T. Parisini and R. Zoppoli, “A Receding-Horizon Regulator for Non-
linear Systems and a Neural Approximation,” Automatica, vol. 31,
no. 10, pp. 1443–1451, 1995.

[13] A. Domahidi, M. N. Zeilinger, M. Morari, and C. N. Jones, “Learning
a feasible and stabilizing explicit model predictive control law by
robust optimization,” in Proc. IEEE Conf. Decision Control, pp. 513–
519, IEEE, 2011.

[14] E. T. Maddalena, C. G. da Moraes, G. Waltrich, and C. N. Jones,
“A neural network architecture to learn explicit MPC controllers from
data,” IFAC-PapersOnLine, vol. 53, no. 2, pp. 11362–11367, 2020.

[15] L. Zheng, Y. Shi, L. J. Ratliff, and B. Zhang, “Safe reinforcement
learning of control-afﬁne systems with vertex networks,” in Learning
for Dynamics and Control, pp. 336–347, PMLR, 2021.

[16] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge

University Press, 2009.

[17] D. Tabas and B. Zhang, “Computationally Efﬁcient Safe Reinforce-
ment Learning for Power Systems,” arXiv:2110.10333, 2021.
[18] S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan, Linear Matrix
Inequalities in System and Control Theory, vol. 15. Philadelphia:
Society for Industrial and Applied Mathematics, 1994.

[19] A. Bemporad and M. Morari, “Robust model predictive control: A
survey,” in Robustness in identiﬁcation and control, pp. 207–226,
London: Springer, 1999.

[20] W. Langson, I. Chryssochoos, S. V. Rakovi´c, and D. Q. Mayne,
“Robust model predictive control using tubes,” Automatica, vol. 40,
no. 1, pp. 125–133, 2004.

[21] M. Farina, L. Giulioni, and R. Scattolini, “Stochastic linear Model
Predictive Control with chance constraints - A review,” J. Process
Control, vol. 44, pp. 53–67, 2016.

[22] M. B. Saltık, L. ¨Ozkan, J. H. Ludlage, S. Weiland, and P. M. Van den
Hof, “An outlook on robust model predictive control algorithms:
Reﬂections on performance and computational aspects,” J. Process
Control, vol. 61, pp. 77–102, 2018.

[23] F. Blanchini and S. Miani, Set-theoretic methods

in control.

Birkhauser, 2015.

[24] C. Liu and I. M. Jaimoukha, “The computation of full-complexity
polytopic robust control invariant sets,” in Proc. 54th IEEE Conf.
Decision Control, pp. 6233–6238, 2015.

[25] Y. Wang and S. Boyd, “Fast model predictive control using online
optimization,” IEEE Trans. Control Syst. Technol., vol. 18, no. 2,
pp. 267–278, 2010.

[26] C. Filippi, “An algorithm for approximate multiparametric linear
programming,” Journal of Optimization Theory and Applications,
vol. 120, no. 1, pp. 73–95, 2004.

[27] J. Spjøtvold, P. Tøndel, and T. A. Johansen, “A method for obtaining
continuous solutions to multiparametric linear programs,” IFAC Proc.
Volumes (IFAC-PapersOnline), vol. 38, no. 1, pp. 253–258, 2005.
[28] S. Sadraddini and R. Tedrake, “Linear Encodings for Polytope Con-
tainment Problems,” Proc. IEEE Conf. Decision Control, pp. 4367–
4372, 2019.

[29] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward
networks are universal approximators,” Neural Networks, vol. 2, no. 5,
pp. 359–366, 1989.

[30] A. Baydin, A. A. Radul, B. A. Pearlmutter, and J. M. Siskind, “Au-
tomatic Differentiation in Machine Learning: a Survey,” J. Machine
Learning Research, vol. 18, pp. 1–43, 2018.

[31] M. D. Srinath, P. Rajasekaran, and R. Viswanathan, Introduction to
statistical signal processing with applications. Prentice-Hall, Inc.,
1995.

[32] J. Drgona, K. Kis, A. Tuor, D. Vrabie, and M. Klauco, “Differentiable
Predictive Control: Deep Learning Alternative to Explicit Model Pre-
dictive Control for Unknown Nonlinear Systems,” arXiv:2011.03699,
2020.

[33] A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and
J. Zico Kolter, “Differentiable convex optimization layers,” Advances
in Neural Information Processing Systems, vol. 32, no. NeurIPS, 2019.

Fig. 4. Solve time vs. trajectory cost for the networks under consideration
applied to the 3-state system. The Gauge NN is Pareto-efﬁcient in terms
of cost and computation time compared to the other techniques with safety
guarantees (Online MPC and Projection NN).

Our intuition behind the high performance of the neural
networks is that (2) is a heuristic and the unsupervised
learning approach can lead to better closed-loop policies.

VI. CONCLUSION

In this paper, we provided an efﬁcient way of exploring the
interior of the MPC feasible set for learning-based approx-
imate explicit MPC, and demonstrated the performance and
computational gains that can be achieved by approaching the
problem from the interior. The paradigm relies on a Phase I
solution that exploits the structure of the MPC problem and
a Phase II solution that features a projection-free feasibility
guarantee. The results compare favorably against common
approaches that use unsupervised learning, as well as against
the oracle itself used in supervised approaches. Future work
includes applications to MPC problems with convex but non-
polytopic constraint sets, and to distributed settings.

REFERENCES

[1] J. B. Rawlings, D. Q. Mayne, and M. M. Diehl, Model Predictive
Control: Theory and Design. Santa Barbara, CA: Nob Hill, 2 ed.,
2019.

[2] Y. Ding, L. Wang, Y. Li, and D. Li, “Model predictive control and its
application in agriculture: A review,” Computers and Electronics in
Agriculture, vol. 151, pp. 104–117, 2018.

[3] D. Hrovat, S. Di Cairano, H. E. Tseng, and I. V. Kolmanovsky, “The
development of Model Predictive Control in automotive industry: A
survey,” Proc. IEEE Int. Conf. on Control Applications, pp. 295–302,
2012.

[4] A. Ademola-Idowu and B. Zhang, “Frequency Stability Using MPC-
Based Inverter Power Control in Low-Inertia Power Systems,” IEEE
Trans. Power Syst., vol. 36, no. 2, pp. 1628–1637, 2021.

[5] A. Alessio and A. Bemporad, “A Survey on Explicit Model Predictive
Control,” in Nonlinear Model Predictive Control (L. Magni, D. Rai-
mondo, and F. Allg¨ower, eds.), Berlin, Heidelberg: Springer, 2009.
[6] M. N. Zeilinger, C. N. Jones, and M. Morari, “Real-time suboptimal
model predictive control using a combination of explicit MPC and
online optimization,” IEEE Trans. Autom. Control, vol. 56, no. 7,
pp. 1524–1534, 2011.

[7] C. N. Jones, M. Bari´c, and M. Morari, “Multiparametric linear pro-
gramming with applications to control,” European Journal of Control,
vol. 13, no. 2-3, pp. 152–170, 2007.

[8] T. A. Johansen, “Approximate explicit receding horizon control of
constrained nonlinear systems,” Automatica, vol. 40, no. 2, pp. 293–
300, 2004.

[9] A. Grancharova and T. A. Johansen, “Computation, approximation
and stability of explicit feedback min-max nonlinear model predictive
control,” Automatica, vol. 45, no. 5, pp. 1134–1143, 2009.

