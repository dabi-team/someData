Synthesizing Datalog Programs Using Numerical Relaxation

Xujie Si , Mukund Raghothaman , Kihong Heo and Mayur Naik
University of Pennsylvania
{xsi, rmukund, kheo, mhnaik}@cis.upenn.edu

9
1
0
2

n
u
J

5
2

]
I

A
.
s
c
[

2
v
3
6
1
0
0
.
6
0
9
1
:
v
i
X
r
a

Abstract

The problem of learning logical rules from examples
arises in diverse ﬁelds, including program synthesis,
logic programming, and machine learning. Existing
approaches either involve solving computationally
difﬁcult combinatorial problems, or performing pa-
rameter estimation in complex statistical models.
In this paper, we present DIFFLOG, a technique to
extend the logic programming language Datalog
to the continuous setting. By attaching real-valued
weights to individual rules of a Datalog program, we
naturally associate numerical values with individual
conclusions of the program. Analogous to the strat-
egy of numerical relaxation in optimization prob-
lems, we can now ﬁrst determine the rule weights
which cause the best agreement between the training
labels and the induced values of output tuples, and
subsequently recover the classical discrete-valued
target program from the continuous optimum.
We evaluate DIFFLOG on a suite of 34 benchmark
problems from recent literature in knowledge dis-
covery, formal veriﬁcation, and database query-by-
example, and demonstrate signiﬁcant improvements
in learning complex programs with recursive rules,
invented predicates, and relations of arbitrary arity.

1 Introduction
As a result of its rich expressive power and efﬁcient imple-
mentations, the logic programming language Datalog has wit-
nessed applications in diverse domains such as bioinformat-
ics [Seo, 2018], big-data analytics [Shkapsky et al., 2016],
robotics [Poole, 1995], networking [Loo et al., 2006], and for-
mal veriﬁcation [Bravenboer and Smaragdakis, 2009]. Users
on the other hand are often unfamiliar with logic programming.
The programming-by-example (PBE) paradigm aims to bridge
this gap by providing an intuitive interface for non-expert
users [Gulwani, 2011].

Typically, a PBE system is given a set of input tuples and
sets of desirable and undesirable output tuples. The central
computational problem is that of synthesizing a Datalog pro-
gram, i.e., a set of logical inference rules which produces,
from the input tuples, a set of conclusions which is compatible

with the output tuples. Previous approaches to this problem fo-
cus on optimizing the combinatorial exploration of the search
space. For example, ALPS maintains a small set of syntacti-
cally most-general and most-speciﬁc candidate programs [Si et
al., 2018], Zaatar encodes the derivation of output tuples as a
SAT formula for subsequent solving by a constraint solver [Al-
barghouthi et al., 2017], and inductive logic programming
(ILP) systems employ sophisticated pruning algorithms based
on ideas such as inverse entailment [Muggleton, 1995]. Given
the computational complexity of the search problem, however,
these systems are hindered by large or difﬁcult problem in-
stances. Furthermore, these systems have difﬁculty coping
with minor user errors or noise in the training data.

In this paper, we take a fundamentally different approach
to the problem of synthesizing Datalog programs. Inspired
by the success of numerical methods in machine learning and
other large scale optimization problems, and of the strategy of
relaxation in solving combinatorial problems such as integer
linear programming, we extend the classical discrete semantics
of Datalog to a continuous setting named DIFFLOG, where each
rule is annotated with a real-valued weight, and the program
computes a numerical value for each output tuple. This step
can be viewed as an instantiation of the general K-relation
framework for database provenance [Green et al., 2007] with
the Viterbi semiring being chosen as the underlying space K of
provenance tokens. We then formalize the program synthesis
problem as that of selecting a subset of target rules from a large
set of candidate rules, and thereby uniformly capture various
methods of inducing syntactic bias, including syntax-guided
synthesis (SyGuS) [Alur et al., 2015], and template rules in
meta-interpretive learning [Muggleton et al., 2015].

The synthesis problem thus reduces to that of ﬁnding the
values of the rule weights which result in the best agreement
between the computed values of the output tuples and their
speciﬁed values (1 for desirable and 0 for undesirable tuples).
The fundamental NP-hardness of the underlying decision prob-
lem manifests as a complex search surface, with local minima
and saddle points. To overcome these challenges, we devise
a hybrid optimization algorithm which combines Newton’s
root-ﬁnding method with periodic invocations of a simulated
annealing search. Finally, when the optimum value is reached,
connections between the semantics of DIFFLOG and Datalog
enable the recovery of a classical discrete-valued Datalog pro-
gram from the continuous-valued optimum produced by the

 
 
 
 
 
 
optimization algorithm.

A particularly appealing aspect of relaxation-based synthe-
sis is the randomness caused by the choice of the starting posi-
tion and of subsequent Monte Carlo iterations. This manifests
both as a variety of different solutions to the same problem,
and as a variation in running times. Running many search in-
stances in parallel therefore enables stochastic speedup of the
synthesis process, and allows us to leverage compute clusters
in a way that is fundamentally impossible with deterministic
approaches. We have implemented DIFFLOG and evaluate it on
a suite of 34 benchmark programs from recent literature. We
demonstrate signiﬁcant improvements over the state-of-the-
art, even while synthesizing complex programs with recursion,
invented predicates, and relations of arbitrary arity.
Contributions. Our work makes the following contributions:

1. A formulation of the Datalog synthesis problem as that of
selecting a set of desired rules. This formalism generalizes
syntax-guided query synthesis and meta-rule guided search.
2. A fundamentally new approach to solving rule selection
by numerically minimizing the difference between the
weighted set of candidate rules and the reference output.
3. An extension of Datalog which also associates output tu-
ples with numerical weights, and which is a continuous
reﬁnement of the classical semantics.

4. Experiments showing state-of-the-art performance on a
suite of diverse benchmark programs from recent literature.

2 Related Work
Weighted logical inference. The idea of extending logi-
cal inference with weights has been studied by the com-
munity in statistical relational learning. Markov Logic Net-
works [Richardson and Domingos, 2006; Kok and Domingos,
2005] view a ﬁrst order formula as a template for generat-
ing a Markov random ﬁeld, where the weight attached to
the formula speciﬁes the likelihood of its grounded clauses.
ProbLog [Raedt et al., 2007] extends Prolog with probabilistic
rules and reduces the inference problem to weighted model
counting. DeepProbLog [Manhaeve et al., 2018] further ex-
tends ProbLog with neural predicates (e.g., input data can be
images). These frameworks could potentially serve as the
central inference component of our framework but we use
the Viterbi semiring due to two critical factors: (a) the ex-
act inference problem of these frameworks is #P-complete
whereas inference in the Viterbi semiring is polynomial; and
(b) automatic differentiation cannot easily be achieved without
signiﬁcantly re-engineering these frameworks.
Inductive logic programming (ILP). The Datalog synthe-
sis problem can be seen as an instance of the classic ILP
problem. Metagol [Muggleton et al., 2015] supports higher-
order dyadic Datalog synthesis but the synthesized program
can only consist of relations of arity two. Metagol is built
on top of Prolog which limits its scalability, and also intro-
duces issues with non-termination, especially when predicates
have not already been partially ordered by the user. In con-
trast, ALPS [Si et al., 2018] builds on top of Z3 ﬁxed point
engine and exhibits much better scalability. Recent works
such as NeuralLP [Yang et al., 2017] and ∂ILP [Evans and

Will

Ann

Jim

Ava

Input tuples (EDB)

Output tuples (IDB)

Noah

Emma

Liam

(a)

parent (Will, Noah)
parent (Ann, Noah)
parent (Jim, Emma)
parent (Ava, Emma)
parent (Noah, Liam)
parent (Emma, Liam)

samegen (Noah, Emma)
samegen (Ann, Will)
samegen (Jim, Ann)
· · ·

(b)

(c)

Figure 1: Example of a family tree (a), and its representation as a set
of input tuples (b). An edge from x to y indicates that x is a parent
of y, and is represented symbolically as the tuple parent(x, y). The
user wishes to realize the relation samegen(x, y), indicating the
fact that x and y occur are from the same generation of the family (c).

Grefenstette, 2018] reduce Datalog program synthesis to a
differentiable end-to-end learning process by modeling rela-
tion joins as matrix multiplication, which also limits them to
relations of arity two. NTP [Rockt¨aschel and Riedel, 2017]
constructs a neural network as a learnable proof (or derivation)
for each output tuple up to a predeﬁned depth (e.g. ≤ 2) with
a few (e.g. ≤ 4) templates, where the neural network could
be exponentially large when either the depth or the number of
templates grows. The predeﬁned depth and a small number
of templates could signiﬁcantly limit the richness of learned
programs. Our work seeks to synthesize Datalog programs
consisting of relations of arbitrary arity and support rich fea-
tures like recursion and predicate invention.
MCMC methods for program synthesis. Markov chain
Monte-Carlo (MCMC) methods have also been used for pro-
gram synthesis. For example, in STOKE, [Schkufza et al.,
2016] apply the Metropolis-Hastings algorithm to synthesize
efﬁcient loop free programs. Similarly, [Liang et al., 2010]
show that program transformations can be efﬁciently learned
from demonstrations by MCMC inference.

3 The Datalog Synthesis Problem
In this section, we concretely describe the Datalog synthesis
problem, and establish some basic complexity results. We
use the family tree shown in Figure 1 as a running example.
In Section 3.1, we brieﬂy describe how one may compute
samegen(x, y) from parent(x, y) using a Datalog program.
In Section 3.2, we formalize the query synthesis problem as
that of rule selection.

3.1 Overview of Datalog
The set of tuples inhabiting relation samegen(x, y) can be
computed using the following pair of inference rules, r1 and r2:
r1: samegen(x, y) :− parent(x, z), parent(y, z).
r2: samegen(x, u) :− parent(x, y), parent(u, v), samegen(y, v).

Rule r1 describes the fact that for all persons x, y, and z,
if both x and y are parents of z, then x and y occur at the
same level of the family tree. Informally, this rule forms the
base of the inductive deﬁnition. Rule r2 forms the inductive
step of the deﬁnition, and provides that x and u occur in the
same generation whenever they have children y and v who
themselves occur in the same generation.

By convention, the relations which are explicitly provided as
part of the input are called the EDB, I = {parent}, and those

which need to be computed as the output of the program are
called the IDB, O = {samegen}. To evaluate this program,
one starts with the set of input tuples, and repeatedly applies
rules r1 and r2 to derive new output tuples. Note that because
of the appearance of the literal samegen(y, v) on the right side
of rule r2, discovering a single output tuple may recursively
result in the further discovery of additional output tuples. The
derivation process ends when no additional output tuples can
be derived, i.e., when the set of conclusions reaches a ﬁxpoint.
More generally, we assume a collection of relations,
{P, Q, . . . }. Each relation P has an arity k ∈ N, and is a
set of tuples, each of which is of the form P (c1, c2, . . . , ck),
for some constants c1, c2, . . . , ck. The Datalog program is a
collection of rules, where each rule r is of the form:

Ph(uh) :− P1(u1), P2(u2), . . . , Pk(uk),

where Ph is an output relation, and uh, u1, u2, . . . , uk are
vectors of variables of appropriate length. The variables u1,
u2, . . . , uk, uh appearing in the rule are implicitly universally
quantiﬁed, and instantiating them with appropriate constants
v1, v2, . . . , vk, vh yields a grounded constraint g of the form
P1(v1) ∧ P2(v2) ∧ · · · ∧ Pk(vk) =⇒ Ph(vh): “If all of the
antecedent tuples Ag = {P1(v1), P2(v2), . . . , Pk(vk)} are
derivable, then the conclusion cg = Ph(vh) is also derivable.”

3.2 Synthesis as Rule Selection
The input-output examples, I, O+, and O−.
Instead of ex-
plicitly providing rules r1 and r2, the user provides an example
instance of the EDB I, and labels a few tuples of the output
relation as “desirable” or “undesirable” respectively:

O+ = {samegen(Ann, Jim)}, and

O− = {samegen(Ava, Liam), samegen(Jim, Emma)},
indicating that Ann and Jim are from the same generation, but
Ava and Liam and Jim and Emma are not. Note that the user is
free to label as many potential output tuples as they wish, and
the provided labels O+ ∪ O− need not be exhaustive. The goal
of the program synthesizer is to ﬁnd a set of rules Rs which
produce all of the desired output tuples, i.e., O+ ⊆ Rs(I),
and none of the undesired tuples, i.e., O− ∩ Rs(I) = ∅.
The set of candidate rules, R. The user often possesses
additional information about the problem instance and the
concept being targeted. This information can be provided to
the synthesizer through various forms of bias, which direct the
search towards desired parts of the search space. A particularly
common form in the recent literature on program synthesis
is syntactic: for example, SyGuS requires a description of
the space of potential solution programs as a context-free
grammar [Alur et al., 2015], and recent ILP systems such as
Metagol [Muggleton et al., 2015] require the user to provide
a set of higher-order rule templates (“metarules”) and order
constraints over predicates and variables that appear in clauses.
In this paper, we assume that the user has provided a large set
of candidate rules R and that the target concept Rs is a subset
of these rules: Rs ⊆ R.

These candidate rules can express various patterns that
could conceivably discharge the problem instance. For exam-
ple, R can include the candidate rule rs, “samegen(x, y) :−

samegen(y, x)”, which indicates that the output relation is
symmetric, and the candidate rule rt, “samegen(x, z) :−
samegen(x, y), samegen(y, z)”, which indicates that the re-
lation is transitive. Note that the assumption of the candidate
rule set R uniformly subsumes many previous forms of syn-
tactic bias, including those in SyGuS and Metagol.

Also note that R can often be automatically populated: In
our experiments in Section 6, we automatically generate R
using the approach introduced by ALPS [Si et al., 2018]. We
start with seed rules that follow a simple chain pattern (e.g.,
“P1(x1, x4) :− P2(x1, x2), P3(x2, x3), P4(x3, x4)”), and re-
peatedly augment R with simple edits to the variables, pred-
icates, and literals of current candidate rules. The candidate
rules thus generated exhibit complex patterns, including recur-
sion, and contain literals of arbitrary arity. Furthermore, any
conceivable Datalog rule can be produced with a sufﬁciently
large augmentation distance.
Problem 1 (Rule Selection). Let the following be given: (a) a
set of input relations, I and output relations, O, (b) the set of
input tuples I, (c) a set of positive output tuples O+, (d) a set
of negative output tuples O−, and (e) a set of candidate rules
R which map the input relations I to the output relations O.
Find a set of target rules Rs ⊆ R such that:

O+ ⊆ Rs(I), and O− ∩ Rs(I) = ∅.

Finally, we note that the rule selection problem is NP-hard:
this is because multiple rules in the target program Rs may in-
teract in non-compositional ways. The proof proceeds through
a straightforward encoding of the satisﬁability of a 3-CNF
formula, and is provided in the Appendix.
Theorem 2. Determining whether an instance of the rule
selection problem, (I, O, I, O+, O−, R), admits a solution is
NP-hard.

4 A Smoothed Interpretation for Datalog
In this section, we describe the semantics of DIFFLOG, and
present an algorithm to evaluate and automatically differentiate
this continuous-valued extension.

4.1 Relaxing Rule Selection
The idea motivating DIFFLOG is to generalize the concept of
rule selection: instead of a set of binary decisions, we asso-
ciate each rule r with a numerical weight wr ∈ [0, 1]. One
possible way to visualize these weights is as the extent to
which they are present in the current candidate program. The
central challenge, which we will now address, is in specifying
how the vector of rule weights w determine the numerical
values vR,I
(w) for the output tuples t of the program. We use
notation vt(w) when the set of rules R and the set of input
tuples I are evident from context.

t

Every output tuple of a Datalog program is associated with
a set of derivation trees, such as those shown in Figure 2.
Let rg be the rule associated with each instantiated clause g
that appears in the derivation tree τ . We deﬁne the value of τ ,
vτ (w), as the product of the weights of all clauses appearing in
τ , and the value of an output tuple t as being the supremum of
the values of all derivation trees of which it is the conclusion:
(1)

vτ (w) =

wrg , and

(cid:89)

clause g∈τ

parent(Noah, Liam)

parent(Noah, Liam)

r1(Noah, Noah, Liam)

samegen(Noah, Noah)

Algorithm 1 EVALUATE(R, w, I), where R is a set of rules,
w is an assignment of weight to each rule in R, and I is a set
of input tuples.

parent(Will, Noah)

parent(Ann, Noah)

parent(Will, Noah)

parent(Ann, Noah)

r1(Will, Ann, Noah)

samegen(Will, Ann)

(a)

r2(Will, Noah, Ann, Noah)

samegen(Will, Ann)

(b)

Figure 2: Examples of derivation trees, τ1 (a) and τ2 (b) in-
duced by various combinations of candidate rules, applied to the
EDB of familial relationships from Figure 1. The input tuples are
shaded in grey. We present two derivation trees for the conclusion
samegen(Will, Ann) using rules r1 and r2 in Section 3.1.

· · ·

rs(Ann, Will)

· · ·

samegen(Will, Ann)

samegen(Ann, Will)

rs(Will, Ann)

wr

Figure 3: The rule rs, “someone(x, y) :− samegen(y, x)”, in-
duces cycles in the clauses obtained at ﬁxpoint. When unrolled into
derivation trees such as those in Figure 2, these cycles result in the
production of inﬁnitely many derivation trees for a single output tuple.

1. Initialize the set of tuples in each relation, FP := ∅, their valua-
tions, u(t) := 0, and their provenance l(t) = {r (cid:55)→ ∞ | r ∈ R}.
2. For each input relation P , update FP := IP , and for each t ∈ IP ,

update u(t) := 1 and l(t) = {r (cid:55)→ 0 | r ∈ R}.

3. Until (F, u) reach ﬁxpoint,

(a) Compute the immediate consequence of each rule, r,

“Ph(uh) :− P1(u1), P2(u2), . . . , Pk(uk)”:

F (cid:48)

Ph = πuh (FP1 (u1) (cid:46)(cid:47) FP2 (u2) (cid:46)(cid:47) · · · (cid:46)(cid:47) FPk (uk)).

Furthermore, for each tuple t ∈ F (cid:48)
Ph , determine all sets of an-
tecedent tuples, Ag(t) = {P1(v1), P2(v2), . . . , Pk(vk)},
which result in its production.
Ph .

(b) Update FPh := FPh ∪ F (cid:48)
(c) For each tuple t ∈ F (cid:48)

Ph and each Ag(t): (i) compute u(cid:48)

t =

(cid:81)k

i=1 u(Pi(vi)), and (ii) if u(t) < u(cid:48)

t, update:

u(t) := u(cid:48)

t, and l(t) := {r (cid:55)→ 1} +

l(Pi(vi)),

where addition of provenance values corresponds to the
element-wise sum.

i=1

k
(cid:88)

vt(w) =

sup
τ with conclusion t

vτ (w),

(2)

4. Return (F, u, l).

with the convention that sup(∅) = 0. For example, if wr1 =
0.8 and wr2 = 0.6, then the weight of the trees τ1 and τ2
from Figure 2 are respectively vτ1(w) = wr1 = 0.8 and
vτ2 (w) = wr1wr2 = 0.48.

Since 0 ≤ wr ≤ 1, it follows that vτ (w) ≤ 1. Also note
that a single output tuple may be the conclusion of inﬁnitely
many proof trees (see the derivation structure in Figure 3),
leading to the deliberate choice of the supremum in Equation 2.
One way to consider Equations 1 and 2 is as replacing the
traditional operations (∧, ∨) and values {true, false} of the
Boolean semiring with the corresponding operations (×, max)
and values [0, 1] of the Viterbi semiring. The study of var-
ious semiring interpretations of database query formalisms
has a rich history motivated by the idea of data provenance.
The following result follows from Prop. 5.7 in [Green et al.,
2007], and concretizes the idea that DIFFLOG is a reﬁnement
of Datalog:

Theorem 3. Let R be a set of candidate rules, and w be an
assignment of weights wr ∈ [0, 1] to each of them, r ∈ R.
Deﬁne Rs = {r | wr (cid:13) 0}, and consider a potential output
tuple t. Then, vR,I
(w) (cid:13) 0 iff t ∈ Rs(I).

t

Furthermore, in the Appendix, we show that the output

values vt(w) is well-behaved in its domain of deﬁnition:
Theorem 4. The value of the output tuples, vt(w), varies
monotonically with the rule weights w, and is continuous in
the region 0 < wr < 1.

We could conceivably have chosen a different semiring in
our deﬁnitions in Equations 1 and 2. One alternative would be
to choose a space of events, corresponding to the inclusion of
individual rules, and choosing the union and intersection of

events as the semiring operations. This choice would make the
system coincide with ProbLog [Raedt et al., 2007]. However,
the #P-completeness of inference in probabilistic logics would
make the learning process computationally expensive. Other
possibilities, such as the arithmetic semiring (R, +, ×, 0, 1),
would lead to unbounded values for output tuples in the pres-
ence of inﬁnitely many derivation trees.

4.2 Evaluating and Automatically Differentiating

DIFFLOG Programs

Because the set of derivation trees for an individual tuple t may
be inﬁnite, note that Equation 2 is merely deﬁnitional, and does
not prescribe an algorithm to compute vt(w). Furthermore,
numerical optimization requires the ability to automatically
differentiate these values, i.e., to compute ∇wvt.

The key to automatic differentiation is tracking the prove-
nance of each output tuple [Green et al., 2007]. Pick an output
tuple t, and let τ be its derivation tree with greatest value.
For the purposes of this paper, the provenance of t is a map,
lt = {r (cid:55)→ #r in τ | r ∈ R}, which maps each rule r to
the number of times it appears in τ . Given the provenance lt
of a tuple, the derivative of vt(w) can be readily computed:
dvt(w)/dwr = lt(r)vt(w)/wr.

In Algorithm 1, we present an algorithm to compute the
output values vt(w) and provenance lt, given R, w, and the
input tuples I. The algorithm is essentially an instrumented
version of the “naive” Datalog evaluator [Abiteboul et al.,
1995]. We outline the proof of the following correctness and
complexity claims in the Appendix.
Theorem 5. Fix a set of input relations I, output relations O,
and candidate rules R. Let EVALUATE(R, w, I) = (F, u, l).

Then: (a) F = R(I), and (b) u(t) = vt(w). Furthermore,
EVALUATE(R, w, I) returns in time poly(|I|).

5 Formulating the Optimization Problem
We formulate the DIFFLOG synthesis problem as ﬁnding the
value of the rule weights w which minimizes the difference
between the output values of tuples, vt(w), and their expected
values, 1 if t ∈ O+, and 0 if t ∈ O−. Speciﬁcally, we seek to
minimize the L2 loss,
(cid:88)

(cid:88)

(1 − vt(w))2 +

vt(w)2.

(3)

L(w) =

t∈O+

t∈O−

At the optimum point, Theorem 3 enables the recovery of a
classical Datalog program from the optimum value w∗.
Hybrid optimization procedure.
In program synthesis, the
goal is often to ensure exact compatibility with the pro-
vided positive and negative examples. We therefore seek
zeros of the loss function L(w), and solve for this us-
ing Newton’s root-ﬁnding algorithm: w(i+1) = w(i) −
L(w)∇wL(w)/(cid:107)∇wL(w)(cid:107)2. To escape from local minima
and points of slow convergence, we periodically intersperse
iterations of the MCMC sampling, speciﬁcally simulated an-
nealing. We describe the parameters of the optimization algo-
rithm in the Appendix.
Separation-guided search termination. After computing
each subsequent w(i), we examine the provenance values for
each output tuple to determine whether the current position
can directly lead to a solution to the rule selection problem.
In particular, we compute the sets of desirable—R+ = {r ∈
l(t) | t ∈ O+}—and undesirable rules—R− = {r ∈ l(t) |
t ∈ O−}, and check whether R+ ∩ R− = ∅. If these sets
are separate, then we examine the candidate solution R+, and
return if it satisﬁes the output speciﬁcation.

6 Empirical Evaluation
Our experiments address the following aspects of DIFFLOG:
1. effectiveness at synthesizing Datalog programs and compar-
ison to the state-of-the-art tool ALPS [Si et al., 2018], which
already outperforms existing ILP tools [Albarghouthi et al.,
2017; Muggleton et al., 2015] and supports relations with
arbitrary arity, sophisticated joins, and predicate invention;
2. the beneﬁt of employing MCMC search compared to a

purely gradient-based method; and

3. scaling with number of training labels and rule templates.
We evaluated DIFFLOG on a suite of 34 benchmark prob-
lems [Si et al., 2018]. This collection draws benchmarks from
three different application domains: (a) knowledge discovery,
(b) program analysis, and (c) relational queries. The character-
istics of the benchmarks are shown in Table 3 of the Appendix.
These benchmarks involve up to 10 target rules, which could
be recursive and involve relations with arity up to 6. The imple-
mentation of DIFFLOG comprises 4K lines of Scala code. We
use Newton’s root-ﬁnding method for continuous optimization
and apply MCMC-based random sampling every 30 iterations.
All experiments were conducted on Linux machines with Intel
Xeon 3GHz processors and 64GB memory.

Table 1: Characteristics of benchmarks and performance of DIFFLOG
compared to ALPS. Rel shows the number of relations. Rule rep-
resents the number of expected (Exp) and candidate rules (Cnd).
Tuple shows the number of input and output tuples. Iter and Smpl
report the number of iterations and MCMC samplings. Time shows
the running time of DIFFLOG and ALPS in seconds.

Benchmark

Rel

inflamation 7
abduce
4
animals
13
ancestor
4
buildWall
5
samegen
3
scc
3

polysite
6
downcast
9
rv-check
5
andersen
5
1-call-site 9
2-call-site 9
1-object
11
1-type
12
escape
10
modref
13

Rule

Tuple

DIFFLOG

ALPS

Exp Cnd In Out

Iter Smpl Time Time

2
3
4
4
4
3
3

134 640
12
80
50
336
8
80
30
472
7
188
9
384

49
20
64
27
4
22
68

1
1
1
1
5
1
6

1
0
0 < 1
1
0
0 < 1
7
1
2
0
28
1

2
2
40
14
67
12
60

3
552
4 1,267
335
5
175
4
173
4
122
4
46
4
70
4
140
6
129
10

97
27
89 175
74
7
28
30
40
48
13
18

17
5
2 1,205
1
7
4
16
25
15
3
13
3
22
2
19
1
34

1
1
41
0
1
1
1
1
1
0

27
84
30 1,646
22
195
4
27
4
106
53
676
3
345
4
13
1
5
1 2,836

sql-10
sql-14
sql-15

3
4
4

2
3
2

734
23
186

10
11
50

2
6
7

7
1
902

11
1
0 < 1
875
31

41
54
11

6.1 Effectiveness

We ﬁrst evaluate the effectiveness of DIFFLOG and compare it
with ALPS. The running time and solution of DIFFLOG depends
on the random choice of initial weights. DIFFLOG exploits this
characteristic by running multiple synthesis processes for each
problem in parallel. The solution is returned once one of the
parallel processes successfully synthesizes a correct Datalog
program. We populated 32 processes in parallel and measured
the running time until the ﬁrst solution was found. The timeout
is set to 1 hour for each problem.

Table 1 shows the running of DIFFLOG and ALPS. We ex-
cluded 14 out of 34 benchmarks that both DIFFLOG and ALPS
solve within a second (13 benchmarks) or run out of time
(1 benchmark). DIFFLOG outperforms ALPS on 19 of the re-
maining 20 benchmarks. In particular, DIFFLOG is orders of
magnitude faster than ALPS on most of the program analysis
benchmarks. Meanwhile, the continuous optimization may
not be efﬁcient when the problem has many local minimas and
the space is not convex. For example, sql-15 has a lot of
sub-optimal solutions that generate not only all positive output
tuples but also some negative ones.

Figure 4 depicts the distribution of running time on the pro-
gram analysis benchmarks.1 The results show that DIFFLOG is
always able to ﬁnd solutions for all the benchmarks except for
2 timeouts for downcast and rv-check respectively. Also
note that even the median running time of DIFFLOG is smaller
than the running time of ALPS for 6 out of 10 benchmarks.

(a) 2-call-site

(b) downcast

Figure 5: Running time distributions for 2-call-site and
downcast with different number of templates.

Figure 4: Distribution of DIFFLOG’s running time from 32 parallel
runs. The numbers on top represents the number of timeouts. Green
circles represent the running time of ALPS.

Table 2: Effectiveness of MCMC sampling in terms of the best
running time (Time) and the number of timeouts (T/O).

Benchmark

polysite
downcast
rv-check
andersen
1-call-site
2-call-site
1-object
1-type
escape
modref

Total

Hybrid

Newton

MCMC

Time T/O Time

T/O Time T/O

27
30
22
4
4
53
3
4
1
1

10
16
N/A
3
8
27
3
3
1
1

0
2
2
0
0
0
0
0
0
0

4

0
9
32
10
1
17
17
18
17
4

12
70
N/A
4
N/A
42
N/A
N/A
N/A
N/A

0
7
32
9
32
9
32
32
32
32

125

217

6.2

Impact of MCMC-based sampling

We next evaluate the impact of our MCMC-based sampling by
comparing the performance of three variants of DIFFLOG: a) a
version that uses both Newton’s method and the MCMC-based
technique (Hybrid), which is the same as in Section 6.1, b) a
version that uses only Newton’s method (Newton), and c) a
version that uses only the MCMC-based technique (MCMC).
Table 2 shows the running time of the best run and the number
of timeouts among 32 parallel runs for these three variants.
The table shows that our hybrid approach strikes a good bal-
ance between exploitation and exploration. In many cases,
Newton gets stuck in local minima; for example, it cannot
ﬁnd any solution for rv-check within one hour. MCMC
cannot ﬁnd any solution for 6 out of 10 benchmarks. Overall,
Hybrid outperforms both Newton and MCMC by reporting
31× and 54× less number of timeouts, respectively.

6.3 Scalability

We next evaluate the scalability of DIFFLOG, which is essen-
tially affected by two factors: the number of templates and the
size of training data. Our general observation is that increas-
ing either of these does not signiﬁcantly increase the effective
running time (i.e., the best of 32 parallel runs).

(a)

(b)

Figure 6: Performance of DIFFLOG on andersen with different
sizes of data: (a) the distribution of number of iterations, (b) the
distribution of running time.

Figure 5 shows how running time increases with the num-
ber of templates.2 As shown in Figure 5a, the running time
distribution for 2-call-site tends to have larger variance
when the number of templates increases, but the best running
time (out of 32 i.i.d samples) only increases modestly. The
running time distribution for downcast, shown in Figure 5b,
has a similar trend except that smaller number of templates
does not always lead to smaller variance or faster running time.
For instance, the distribution in the setting with 180 templates
has larger variance and median than distributions in the subse-
quent settings with larger number of templates. This indicates
that the actual combination of templates also matters.

The size of training data is another important factor af-
fecting the performance of DIFFLOG. Figure 6a shows the
distribution of the number of iterations for andersen with
different sizes of training data. According to the results, the
size of training data does not necessarily affect the number
of iterations of DIFFLOG. Meanwhile, Figure 6b shows that
the end-to-end running time increases with more training data.
This is mainly because more training data impose more cost
on the DIFFLOG evaluator. However, the statistics shows that
the running time increases linearly with the size of data.

7 Conclusion

We presented a technique to synthesize Datalog programs
by numerical optimization. The central idea was to formu-
late the problem as an instance of rule selection, and then
relax classical Datalog to a reﬁnement named DIFFLOG. In a
comprehensive set of experiments, we show that by learning
a DIFFLOG program and then recovering a classical Datalog
program, we can achieve signiﬁcant speedups over the state-of-
the-art Datalog synthesis systems. In future, we plan to extend
the approach to other synthesis problems such as SyGuS and
to applications in differentiable programming.

1We provide results for the other domains in the Appendix.

2We ensure that a smaller set is always a subset of a larger one.

102030405060708090100Number of templates05101520Running Time (min)306090120150180210240270300Number of templates02040Running Time (min)1X2X3X4X5X6X7X8X9X10X0255075100Number of iterations36912151821242730Number of templates02040Running time (second)References
[Abiteboul et al., 1995] Serge Abiteboul, Richard Hull, and
Victor Vianu. Foundations of Databases. Addison-Wesley,
1995.

[Albarghouthi et al., 2017] Aws Albarghouthi, Paraschos
Koutris, Mayur Naik, and Calvin Smith. Constraint-based
synthesis of datalog programs. In Proceedings of the 23rd
International Conference on Principles and Practice of
Constraint Programming, CP, pages 689–706, 2017.

[Alur et al., 2015] Rajeev Alur, Rastislav Bod´ık, Eric Dal-
lal, Dana Fisman, Pranav Garg, Garvit Juniwal, Hadas
Kress-Gazit, P. Madhusudan, Milo Martin, Mukund
Raghothaman, Shambwaditya Saha, Sanjit Seshia, Rishabh
Singh, Armando Solar-Lezama, Emina Torlak, and Ab-
hishek Udupa. Syntax-guided synthesis. In Dependable
Software Systems Engineering, pages 1–25. 2015.

[Bravenboer and Smaragdakis, 2009] Martin

Bravenboer
and Yannis Smaragdakis. Strictly declarative speciﬁcation
of sophisticated points-to analyses. In Proceedings of the
24th Annual Conference on Object-Oriented Programming,
Systems, Languages, and Applications, OOPSLA, pages
243–262, 2009.

[Evans and Grefenstette, 2018] Richard Evans and Edward
Grefenstette. Learning explanatory rules from noisy data
(extended abstract). In Proceedings of the Twenty-Seventh
International Joint Conference on Artiﬁcial Intelligence,
IJCAI 2018, July 13-19, 2018, Stockholm, Sweden., pages
5598–5602, 2018.

[Green et al., 2007] Todd Green, Gregory Karvounarakis,
and Val Tannen. Provenance semirings. In Proceedings
of the 26th Symposium on Principles of Database Systems,
PODS, pages 31–40, 2007.

[Gulwani, 2011] Sumit Gulwani. Automating string process-
ing in spreadsheets using input-output examples. In Pro-
ceedings of the 38th Symposium on Principles of Program-
ming Languages, POPL, pages 317–330, 2011.

[Kok and Domingos, 2005] Stanley Kok and Pedro M.
Domingos. Learning the structure of markov logic net-
works. In Machine Learning, Proceedings of the Twenty-
Second International Conference (ICML 2005), Bonn, Ger-
many, August 7-11, 2005, pages 441–448, 2005.

[Liang et al., 2010] Percy Liang, Michael I. Jordan, and Dan
Klein. Learning programs: A hierarchical bayesian ap-
proach. In Proceedings of the 27th International Confer-
ence on Machine Learning (ICML-10), June 21-24, 2010,
Haifa, Israel, pages 639–646, 2010.

[Loo et al., 2006] Boon Thau Loo, Tyson Condie, Minos
Garofalakis, David Gay, Joseph Hellerstein, Petros Ma-
niatis, Raghu Ramakrishnan, Timothy Roscoe, and Ion
Stoica. Declarative networking: Language, execution and
optimization. In Proceedings of the 2006 International Con-
ference on Management of Data, SIGMOD, pages 97–108,
2006.

[Manhaeve et al., 2018] Robin Manhaeve, Sebastijan Duman-
cic, Angelika Kimmig, Thomas Demeester, and Luc De

Raedt. Deepproblog: Neural probabilistic logic program-
ming. In Advances in Neural Information Processing Sys-
tems 31: Annual Conference on Neural Information Pro-
cessing Systems 2018, NeurIPS 2018, 3-8 December 2018,
Montr´eal, Canada., pages 3753–3763, 2018.

[Muggleton et al., 2015] Stephen H. Muggleton, Dianhuan
Lin, and Alireza Tamaddoni-Nezhad. Meta-interpretive
learning of higher-order dyadic Datalog: Predicate inven-
tion revisited. Machine Learning, 100(1):49–73, 2015.
[Muggleton, 1995] Stephen Muggleton. Inverse entailment
and Progol. New Generation Computing, 13(3&4):245–
286, 1995.

[Poole, 1995] David Poole. Logic programming for robot
In Proceedings of the 14th International Joint
control.
Conference on Artiﬁcial Intelligence, IJCAI, pages 150–
157, 1995.

[Raedt et al., 2007] Luc De Raedt, Angelika Kimmig, and
Hannu Toivonen. Problog: A probabilistic prolog and its
application in link discovery. In IJCAI 2007, Proceedings
of the 20th International Joint Conference on Artiﬁcial
Intelligence, Hyderabad, India, January 6-12, 2007, pages
2462–2467, 2007.

[Richardson and Domingos, 2006] Matthew Richardson and
Pedro M. Domingos. Markov logic networks. Machine
Learning, 62(1-2):107–136, 2006.

[Rockt¨aschel and Riedel, 2017] Tim Rockt¨aschel and Sebas-
tian Riedel. End-to-end differentiable proving. In Advances
in Neural Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems 2017,
4-9 December 2017, Long Beach, CA, USA, pages 3791–
3803, 2017.

[Schkufza et al., 2016] Eric Schkufza, Rahul Sharma, and
Alex Aiken. Stochastic program optimization. Commun.
ACM, 59(2):114–122, 2016.

[Seo, 2018] Jiwon Seo. Datalog extensions for bioinformatic
data analysis. In 40th Annual International Conference
of the IEEE Engineering in Medicine and Biology Society,
EMBC, pages 1303–1306, 2018.

[Shkapsky et al., 2016] Alexander Shkapsky, Mohan Yang,
Matteo Interlandi, Hsuan Chiu, Tyson Condie, and Carlo
Zaniolo. Big data analytics with Datalog queries on Spark.
In Proceedings of the 2016 International Conference on
Management of Data, SIGMOD, pages 1135–1149, 2016.
[Si et al., 2018] Xujie Si, Woosuk Lee, Richard Zhang, Aws
Albarghouthi, Paraschos Koutris, and Mayur Naik. Syntax-
In Proceedings
guided synthesis of Datalog programs.
of the Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software
Engineering, FSE, pages 515–527, 2018.

[Yang et al., 2017] Fan Yang, Zhilin Yang, and William W.
Cohen. Differentiable learning of logical rules for knowl-
edge base reasoning. In Advances in Neural Information
Processing Systems 30: Annual Conference on Neural In-
formation Processing Systems 2017, 4-9 December 2017,
Long Beach, CA, USA, pages 2316–2325, 2017.

A Proof of Theorems 2, 4 and 5
Theorem 2. Determining whether an instance of the rule
selection problem, (I, O, I, O+, O−, R), admits a solution is
NP-hard.

Proof. Consider a 3-CNF formula ϕ over a set V of variables:

ϕ = (l11 ∨ l12 ∨ l13) ∧ (l21 ∨ l22 ∨ l23) ∧ · · · ∧ (lk1 ∨ lk2 ∨ lk3),

be the given 3-CNF formula, where each literal lij appearing
in clause ci is either a variable, vij ∈ V , or its negation,
¬vij. Assume that there are no trivial clauses in ϕ, which
simultaneously contain both a variable and its negation. We
will now encode its satisﬁability as an instance of the rule
selection problem.

1. For each variable v ∈ V , deﬁne the input relations:

posv = {(c) | v ∈ c}, and
negv = {(c) | ¬v ∈ c},

(4)
(5)

consisting of all one-place tuples posv(c) and negv(c)
indicating whether the variable v occurs positively or
negatively in the clause c.

2. Also, for each variable v, deﬁne the input relation varv

which is inhabited by a single tuple varv(v):

varv = {(v)}.

(6)

3. The idea is to set up the candidate rules so that subsets
of chosen rules correspond to assignments of true / false
values to the variables of ϕ. Let C2(c, v) be an output
relation: we are setting up the problem so that if the tuple
C2(c, v) is derivable in the synthesized solution, then
there is a satisfying assignment of ϕ where clause c is
satisﬁed due to the assignment to variable v.

4. For each variable v, create a pair of candidate rules rv

and r¬v as follows:

rv = “C2(c, v(cid:48)) :− posv(c), varv(v(cid:48))”, and
r¬v = “C2(c, v(cid:48)) :− negv(c), varv(v(cid:48))”.

Selecting the rule rv corresponds to assigning the value
true to the corresponding variable v, and selecting the
rule r¬v corresponds to assigning it the value false.
5. To prevent the simultaneous choice of rules rv and r¬v,
we set up the three-place input relation conﬂict(c, c(cid:48), v),
which indicates that the reason for the simultaneous sat-
isfaction of clauses c and c(cid:48) cannot be a contradictory
variable v:

conﬂict = {(c, c(cid:48), v) | v ∈ c and ¬v ∈ c(cid:48)} ∪ {(a, a, a)},
(7)

where a is some new constant not seen before. We will
motivate its necessity while deﬁning the canary output
relation error next.

6. We detect the simultaneous selection of a pair of rules rv

and r¬v using the rule re:
re = “error(c, c(cid:48), v) :− C2(c, v), C2(c(cid:48), v), conﬂict(c, c(cid:48), v)”

Here error is a three-place output relation indicating the
selection of an inconsistent assignment. We would like
to force the synthesizer to choose the error-detecting rule
re. The selection of the rule re, the presence of the input
tuple conﬂict(a, a, a), and the selection of the rule ra:

ra = “C2(x, x) :− conﬂict(x, x, x)”

is the only way to produce the output tuple error(a, a, a),
which we will mark as desired.

7. The output tuple C2(c, v) indicates the satisfaction of the
clause c because of the assignment to variable v. We use
the presence of such tuples to mark the clause c itself as
being satisﬁed: let C1(c) be a one-place output relation,
and include the rule:

rc = “C1(c) :− C2(c, v)”.

8. In summary,

let the rule selection problem Pϕ =

(I, O, I, O+, O−, R) be deﬁned as follows:
(a) I = {varv, posv, negv | v ∈ V } ∪ {conﬂict}.
(b) O = {C2, C1, error}.
(c) Deﬁne the set of input tuples, I, using equations 4,

5, 6, and 7.

(d) O+ = {C1(c) | clause c ∈ ϕ} ∪ {error(a, a, a)}.
(e) O− = {error(c, c(cid:48), v) | clauses c, c(cid:48) and variable v

occurring in ϕ}.

(f) R = {rv, r¬v | v ∈ V } ∪ {re, ra, rc}.

Given a 3-CNF formula ϕ, the corresponding instance Pϕ of
the rule selection problem can be constructed in polynomial
time. Furthermore, it can be seen that, by construction, Pϕ
admits a solution iff ϕ is satisﬁable. It follows that the rule
selection problem is NP-hard.

Next, we turn our attention to Theorem 4. The ﬁrst part of
the claim follows immediately from the deﬁnition in Equa-
tion 2. We therefore focus on the second part: Note that the
proof of continuity does not immediately follow from Equa-
tion 2 because the supremum of an inﬁnite set of continuous
functions need not itself be continuous. It instead depends
on the observation that there is a ﬁnite subset of dominating
derivation trees whose values sufﬁce to compute vt(w).
Theorem 4. The value of the output tuples, vt(w), varies
monotonically with the rule weights w, and is continuous in
the region 0 < wr < 1.

Proof. Fix an assignment of rule weights w. Next, focus
on a speciﬁc output tuple t, and consider the set of all its
derivation trees τ . Let στ be a pre-order traversal over
its nodes. For example, for the tree τ1 in Figure 2a, we
obtain στ1 = samegen(Will, Ann), r1(Will, Ann, Noah),
parent(Will, Noah), parent(Ann, Noah). It can be shown that
the set of all pre-order traversals, στ , over all derivation trees
τ forms a context-free grammar Lt.

We are interested in trees τ with high values vτ (w), where
the value of a tree depends only on the number of occurrences
of each rule r. It therefore follows that the weight vτ (w) is
completely speciﬁed by the Parikh image, {r (cid:55)→ #r in τ },
which counts the number of occurrences of each symbol in

each string of the language Lt. From Parikh’s lemma, we
conclude that this forms a semilinear set. Let

p(Lt) =

m
(cid:91)

(ci0 +

i=1

n
(cid:88)

j=1

cij)

be the Parikh image of Lt, and for each i ∈ {1, 2, . . . , m}, let
τi be the derivation tree corresponding to the rule count ci0. It
follows that:

vt(w) =

sup
τ with conclusion t

vτ (w) =

m
max
i=1

vτi (w).

We have reduced the supremum over an inﬁnite set of con-
tinuous functions to the maximum of a ﬁnite set of continu-
ous functions. It follows that vt(w) varies continuously with
w.

Finally, we turn to the proof of Theorem 5.

Theorem 5. Fix a set of input relations I, output relations O,
and candidate rules R. Let EVALUATE(R, w, I) = (F, u, l).
Then: (a) F = R(I), and (b) u(t) = vt(w). Furthermore,
EVALUATE(R, w, I) returns in time poly(|I|).

Proof. The ﬁrst part of the following result follows from sim-
ilar arguments as the correctness of the classical algorithm.
We brieﬂy describe the proof of the second claim. For each
output tuple t, consider all of its derivation trees τhi with
maximal value, and identify the tree τt with shortest height
among these. All ﬁrst-level sub-trees of τt must themselves
possess the shortest-height-maximal-value property, so that
their height is bounded by the number of output tuples. Since
the (F, u, l)-loop in step 3 of Algorithm 1 has to hit a ﬁxpoint
within as many iterations, and since each iteration runs in
polynomial time, the claim about running time follows.

B Learning Details
We initialize w by uniformly sampling weights wr ∈
[0.25, 0.75]. We apply MCMC sampling after every 30 it-
erations of Newton’s root-ﬁnding method, and sample new
weights as follows:

Table 3: Benchmarks characteristics. Rec and #Rel shows the
programs that require recursive rules, and the number of relations.
#Rules represents the number of expected (Exp) and candidate rules
(Cand). #Tuples shows the number of input and output tuples.

Benchmark

Rec

#Rel

#Rules

#Tuples

Exp Cand

In Out

i

y inflamation
r
e
v
o
c
s
D
e
g
e
w
o
n
K

abduce
animals
ancestor
buildWall
samegen
path
scc

l

l

i

s
s
y
a
n
A
m
a
r
g
o
r
P

polysite
downcast
rv-check
andersen
1-call-site
2-call-site
1-object
1-type
1-obj-type
escape
modref

s
e
i
r
e
u
Q

l

a
n
o
i
t
a
e
R

l

sql-01
sql-02
sql-03
sql-04
sql-05
sql-06
sql-07
sql-08
sql-09
sql-10
sql-11
sql-12
sql-13
sql-14
sql-15

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

7
4
13
4
5
3
2
3

6
9
5
5
9
9
11
12
13
10
13

4
2
2
3
3
3
2
4
4
3
7
6
3
4
4

2
3
4
4
4
3
2
3

3
4
5
4
4
4
4
4
5
6
10

1
1
1
2
1
2
1
3
2
2
4
3
1
3
2

134
80
336
80
472
188
6
384

552
1,267
335
175
173
122
46
70
12
140
129

33
16
70
7
17
9
52
206
52
734
170
32
10
23
186

640
12
50
8
30
7
7
9

97
89
74
7
28
30
40
42
48
13
18

21
3
4
9
12
9
5
6
6
10
30
36
17
11
50

49
20
64
27
4
22
31
68

27
175
2
7
16
15
13
15
22
19
34

2
1
2
6
5
9
5
2
1
2
2
7
7
6
7

X ∼ U (0, 1)

(cid:26) wold

√

2X

wnew =

1 − (1 − wold)(cid:112)2(1 − X)
The temperature T used in simulated annealing is as follows:

if X < 0.5
otherwise.

T =

1.0
C ∗ log(5 + #iter)

where C is initially 0.0001 and #iter is the number of itera-
tions. We accept the newly proposed sample with probability

pacc = min(1, πnew/πcurr),
where πcurr = exp(−L2(wcurr)/T ) and πnew =
exp(−L2(wnew)/T ).

C Benchmarks and Experimental Results
The characteristics of benchmarks are shown in Table 3. Fig-
ure 7 shows that the distribution of running time for the re-
maining benchmarks.

Figure 7: Distribution of DIFFLOG’s running time from 32 parallel
runs. The numbers on top represents the number of timeouts. Green
circles represent the running time of ALPS.

inﬂamationabduceanimalsancestorbuildWallsamegensccsql-10sql-14sql-150102030405060RunningTime(min)3020