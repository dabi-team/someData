Moses: Efﬁcient Exploitation of Cross-device Transferable Features for Tensor
Program Optimization

Zhihe Zhao†, Xian Shuai†, Yang Bai†, Neiwen Ling†, Nan Guan‡, Zhenyu Yan†, Guoliang Xing†,§
†The Chinese University of Hong Kong
‡City University of Hong Kong
§Corresponding Author

2
2
0
2

n
a
J

5
1

]

G
L
.
s
c
[

1
v
2
5
7
5
0
.
1
0
2
2
:
v
i
X
r
a

Abstract

Achieving efﬁcient execution of machine learning
models has attracted signiﬁcant attention recently.
To generate tensor programs efﬁciently, a key com-
ponent of DNN compilers is the cost model that
can predict the performance of each conﬁguration
on speciﬁc devices. However, due to the rapid
emergence of hardware platforms, it is increasingly
labor-intensive to train domain-speciﬁc predictors
for every new platform. Besides, current design
of cost models cannot provide transferable features
between different hardware accelerators efﬁciently
and effectively. In this paper, we propose Moses,
a simple and efﬁcient design based on the lottery
ticket hypothesis, which fully takes advantage of
the features transferable to the target device via
domain adaptation. Compared with state-of-the-
art approaches, Moses achieves up to 1.53X efﬁ-
ciency gain in the search stage and 1.41X inference
speedup on challenging DNN benchmarks.

1 Introduction
Efﬁcient inference of deep neural networks (DNN) is of great
importance for real-time AI applications such as autonomous
driving and augmented reality, especially given that they usu-
ally run on embedded devices with limited compute power
[Zhao et al., 2021]. Existing approaches usually rely on hand-
optimized acceleration libraries, e.g., NVIDIA cuDNN and
Intel MKL. However, they are vendor-speciﬁc, which cannot
support a wide range of diverse hardware devices and require
signiﬁcant engineering efforts for tuning.

Typically, the tensor computation inside deep learning op-
erators is implemented by a set of compute-intensive nested
loops. For example, Figure 1 shows an optimized tensor pro-
gram of a 2D convolutional operator, which consists of mul-
tiple for-loops and involves three schedule primitives: block-
ing, unrolling, and vectorization. However, generating such
high-performance tensor programs from a given high-level
expression is extremely difﬁcult, as the optimal organization
and the parameters of the for-loops can vary signiﬁcantly
for different devices. Therefore, to accelerate the end-to-
end model inference on various hardware platforms, exist-
ing DNN compilers ﬁrst generate a large space of conﬁgura-

Figure 1: An example of the program for a 2D convolution operator:
Conv2d(in channels = 3, out channels = 64, kernel size =
3, stride = 1, padding = 0, bias = F alse) produced by TVM.

tions, and then search for the best-performing one based on
on-device measurements [Chen et al., 2018b]. This process
is termed as auto-tuning.

Although DNN compilers can produce optimized pro-
grams for DNN models on speciﬁc platforms, they suffer
excessively long search time. For example, although Au-
toTVM can outperform nearly 2× over default TensorFlow
on ResNet-18, the auto-tuning time can take up to tens of
hours on embedded devices (e.g. NVIDIA Jetson TX2). To
reduce the time-consuming on-device measurements, the ten-
sor program optimizer employs a cost model to directly pre-
dict the performance of the potential candidates in the search
space. However, training a cost model ofﬂine still requires a
large number of measurements. For instance, Tenset [Zheng
et al., 2021] provides a tensor program performance dataset
collected from 6 devices, containing 52 million program per-
formance records. Based on this dataset, it was shown that
the cost model can be transferred between two Intel CPUs by
ﬁne-tuning. As a result, the online search time can be reduced
without sacriﬁcing the quality of optimized tensor programs.
However, when two hardware platforms differ signiﬁcantly
in architecture, such a vanilla ﬁne-tuning approach would fail
to learn the runtime behaviors of a new device, and hence
performs poorly at generating high-performance tensor pro-
grams, as evidenced by our results in Section 4.

Previous efforts trying to address this challenge mainly fo-
cus on either designing a new cost model [Baghdadi et al.,
2021][Kaufman et al., 2020], or exploring effective search
algorithms during auto-tuning [Ahn et al., 2020][Li et al.,
2020]. However, these approaches still need large amounts

 
 
 
 
 
 
2.2 Auto-Tuning with Cost Models
Figure 2 shows the pipeline of a search-based low-level ten-
sor code generation used by TVM [Chen et al., 2018a]. The
intact neural network is ﬁrst partitioned by the graph-level
optimizer. For instance, the ResNet-50 will be be divided
into 29 subgraphs and each subgraph normally includes one
or two convolutional layers. Given an intact mathematical
expression or a computational graph in a high-level mathe-
matical expression, the search algorithm will search for the
best low-level tensor implementation for the target hardware.
Usually, the search space is in the order of millions for CPUs
and billions for GPUs, as there are a variety of schedule prim-
itives such as tiling, unrolling, vectorization, parallelization,
and thread binding. Each schedule primitive can involve mul-
tiple tunable knobs. On one hand, such a large search space
enables the automatic tensor compiler to ﬁnd the program
that is better than the hand-optimized implementation. On the
other hand, the large search space can incur signiﬁcant search
time, especially for embedded devices with limited computa-
tion power.

To accelerate the searching process, TVM introduces the
cost model to directly predict the time cost of the inner-
most non-loop program rather than extensively measuring
program’s runtime. In this paper, we adopt the 164-d features
in Ansor [Zheng et al., 2020a] to depict the program. In each
iteration, based on predictions from the cost model, a batch
of candidate programs are sampled by an evolutionary search
engine. Then, TVM measures the actual time costs of these
sampled programs. Finally, the measurements are added to
the training data of the cost model. Therefore, the cost model
and searched tensor program are improved iteratively.

Notably, TVM includes two search frameworks: Au-
toTVM [Chen et al., 2018b] and Ansor [Zheng et al., 2020a].
AutoTVM requires hand-written scheduling templates for
searching, while Ansor is a fully automated framework. Due
to the large number of hardware platforms and the substantial
efforts to develop templates, we use Ansor in this paper.

2.3 Cross-Device Cost Model Adaptation
As discussed in the previous section, a major practical draw-
back of the current automatic tensor program optimization
approach [Chen et al., 2018b] is the extremely long search
time. Recent work such as [Ahn et al., 2020] provides a
breakdown of the search time and shows that the time for
on-device measurements dominates. Therefore, a solution to
shorten the online searching time is to collect an comprehen-
sive tensor program performance dataset ofﬂine, and pre-train
a cost model that can be directly utilized [Zheng et al., 2021].
However, the cost model is device-speciﬁc, as its input fea-
tures include conﬁguration knobs that are closely related to
the hardware architecture such as the lengths of BlockIdx and
ThreadIdx. When two architectures are signiﬁcantly different
(e.g., server GPUs and mobile GPUs), the traditional transfer
learning approach may fail.

3 Moses
In this section, we present the overview, problem formulation
and design of Moses.

Figure 2: The complete pipeline of automatic tensor program gener-
ation for a given neural network.

of iterations during the search and the performance is usually
highly program-dependent. Motivated by these challenges,
we propose Moses, a novel cost model adaptation framework
based on the lottery ticket hypothesis [Frankle and Carbin,
2019], which can adapt the trained cost model from a source
device to a new target device with high efﬁciency. We sum-
marize the contributions of this work as follows:

1) To the best of our knowledge, Moses is the ﬁrst work
that achieve highly efﬁcient for auto-tuning between dif-
ferent hardware platforms with transfer learning. Moses
enables the DNN compiler to generate optimized tensor
programs with signiﬁcantly shorter search time for a new
device.

2) We propose a novel approach that can automatically
identify the transferable hardware-independent param-
eters of a pre-trained cost model, and achieve cross-
device cost model adaption via ﬁne-tuning.

3) We conduct comprehensive experiments and show that
Moses is a general and effective approach for diverse
hardware platforms and DNNs.

2 Related Work and Background

2.1 DNN Compilers.

General DNN compilers optimize the computation ﬂow of
DNN tasks in two levels: graph-level and tensor-level. Some
notable compilers are TVM [Chen et al., 2018a], TASO [Jia
et al., 2019], XLA [Abadi et al., 2016] and Halide [Li et
al., 2018]. These compilers either utilize compiler techniques
such as graph substitutions to optimize the intermediate rep-
resentation (IR) level graph [Bai et al., 2021], or focus on ten-
sor program optimization using heuristic and learning-based
algorithms to joint-optimize the polyhedral patterns for DNN.
Building on these DNN compliers, recent works treat the op-
timization process as a black box and propose some advanced
searching or cost model training approaches based on run-
time information [Chen et al., 2018b][Ahn et al., 2020][Li et
al., 2020][Haj-Ali et al., 2020].

g() represents the tensor programs generating functions with
knobs and the transformation pass as inputs. The approxima-
tion of a cost model function to real hardware measurement
can be denoted by C() ∼ Perf(); Thus, given an input sub-
graph i, the objective of the auto-tuning process is ﬁnding
the optimal combination of knobs ψ∗ to maximize the perfor-
mance deﬁned as below:

ψ∗ = arg max

Perf(g(ψ, t))

(1)

ψ∈Ψ

Precise estimate of the performance of tensor program can-
didates can effectively reduce the time-consuming on-device
measurements. Thus, the objective of the cost model is to
minimize the difference between the predictions and the real-
world measurements, which is:

min (cid:107)C(g(Ψ, t)|ψ, θ) − Perf(g(Ψ, t))(cid:107)

(2)

Here, θ represents the parameter sets in the trained cost
model. We train a high-performance cost model is to ﬁnd the
best parameter conﬁguration Θ∗, which can efﬁciently guide
the auto-tuning search process.
The on-device measurements especially for embedded/mo-
bile devices are extremely time-consuming. For example, the
time consumption for on-device measurements of a VGG16
model can be up to 10 hours on an NVIDIA TX2. In Section
3.3, we design a cross-device domain adaption method, which
transfers the cost model trained on the source device to the
target device. The objective of the cross-device domain adap-
tation is to ﬁnd a new parameter set Θ† which can minimize
the difference between the real-world measurements and cost
model predictions, and thus to guide the tuning process for
each subgraph on the target device in an adaptive manner.
3.3 Feature Space Representations
In our problem, the feature space is independent of hardware
architecture while the outputs of the cost model (throughput
predictions for different tensor program conﬁgurations) are
dependent, shown as:

(3)

H {X } ≡ H {XDIV } + H {XDV }
where H maps the hidden feature space representations,
XDIV and XDV present
the decoupled feature space:
hardware-independent and hardware-dependent information,
respectively. The SOTA Tenset [Zheng et al., 2021] shows
that the model ﬁne-tuning on the target domain (devices)
is effective only when the architecture difference between
two devices is small. However, Tenset does not provide any
speciﬁc solutions, e.g., the cost model domain adaptation
between a server level GPU and a mobile level GPU. In order
to solve this problem, we propose to leverage the following
lottery ticket hypothesis in this paper: only part of the
parameters in the trained cost model on source devices are
essential for learning the hardware-independent knowledge.
In other words, Only part of the information needs to be
adapted to the target device while other parameters tend to ﬁt
the domain. Motivated by this, we can transform our problem
into the following question: how to learn domain invariant
parameters to minimize the domain discrepancy brought by
the hardware difference?

Figure 3: Given a cost model A trained on the source device, we aim
to obtain a cost model B that can accurately predict the throughput
of the target device under various tensor program conﬁgurations.

3.1 Overview
We propose Moses, a novel cross-device cost model transfer
approach based on the lottery ticket hypothesis [Frankle and
Carbin, 2019]. As shown in Figure 3, we transfer the cost
model trained on source devices (e.g., Server GPUs) to tar-
get devices (e.g., mobile GPUs) by only ﬁne-tuning portion
of the model parameters while keeping the rest of parameters
deactivated. The rationale of our design is two-fold. First, to
accelerate the online searching instead of collecting a dataset
for every new device ofﬂine, we need to take advantage of
the cost model pre-trained on source devices rather than train-
ing a new model from scratch. Second, as vanilla ﬁne-tuning
approaches may fail due to substantial architecture changes,
we have to utilize the prior knowledge from the pre-trained
cost model wisely. Speciﬁcally, we leverage the lottery ticket
hypothesis to identify the transferable parameters in the cost
model space [Han et al., 2021]. By distilling the transferable
parameters and dropping out the untransferable ones, we can
not only shrink the size of the cost model but also reduce the
device-speciﬁc information, which accelerates the training of
the cost model and improves the generalizability.
3.2 Problem Formulation
The Auto-tuning process in a DNN compiler aims to gen-
erate a large search space of tensor program conﬁgurations
and to ﬁnd the optimal one based on the on-device perfor-
mance measurement records [Ryu and Sung, 2021]. We de-
note the transformation pass of tensor programs by t and the
cost model by C . During the search process in auto-tuning,
the compiler picks the top-k (where k can be one) programs
with the performance predictions from the cost model for
each task. The process can utilize mixed on-device mea-
surements and cost model predictions. If the on-device mea-
surement cost is large, the process can completely rely on
the cost model; Function Perf() represents the run-time hard-
ware measurement (throughput, GFLOPs); Function C() rep-
resents the cost model predictions on codes performance; i is
an input task, which can be a computing subgraph in the input
DNN model. For example, SqueezeNet consists of 23 tasks
(a.k.a, the subgraphs) in total. Note that a subgraph is a unit
with the ﬁnest granularity during the compilation process; Ψ
is the set of possible tensor program conﬁgurations of a task
which consists of a combination of parameters called knobs;

s, yi

3.4 Lottery-Ticket-Based Cross-Device Adaptation
The training data for cost model updating is collected on-
line during the search, which makes the search very time-
consuming due to inevitable on-device measurements. We
denote the set of labeled tensor program records (program
s)}m
knobs, throughput) on source device by S = {(xi
,
where m is the training data points on source device and (xi
s,
yi
s) is the ith record and its corresponding label respectively
in domain s. Now given a target unlabeled program perfor-
mance records set T = {(xj
T )}k with different conﬁgurations
of program knobs x sampled from target device DT , in which
a small set of (cid:98)T = {(xj
)}n can be collected by on-device
(cid:98)T
measurements (which are conducted typically under a given
time budgets due to the time-consuming nature). We have
n (cid:28) k (cid:28) m. Formally, The expected domain adaptation er-
ror on target device can be deﬁned as ε(hT arget(Θ†)) where
h ∈ H, a hypothesis that learned from (cid:98)T , thus the following
inequity holds:

, yj
(cid:98)T

ε(hT arget(Θ†)) ≤ ε(hSource(Θ∗))
+dist(DS(X ), DT (X )) + ϕ,
(4)
In the terminology of semi-supervised domain adaptation,
dist represents the distribution discrepancy over the cross-
device domains, and ϕ represents the ideal error or risks
achieving cross-device domain adaptation. Since the feature
representations are inﬂuenced by hardware difference in our
problem, to achieve the cross-device domain adaptation, in-
stead of minimizing the distance between feature represen-
tations and their resulting data distribution discrepancy, we
show the effectiveness of bound minimization strategy on
solving the cross device domain adaptation and propose to
ﬁnd the bound limitation by optimizing the labeling black-
box functions. Such an approach is inspired by the lottery
ticket hypothesis, which was originally proposed in the con-
text of model compression, showing that only part of param-
eters are ﬁt for model generalization [Frankle and Carbin,
2019].

Similar as in [Frankle and Carbin, 2019], we show ex-
perimentally in this paper the same hypothesis holds in our
problem (see Section 4). That is, there exists a super-subnet,
named winning ticket, with a set of essential parameters of
the trained cost model on source devices, which would be the
domain invariant information. In other words, training from
a super-subnet on the target device would achieve the optimal
transfer performance in our cross-device domain adaptation
problem.

We name parameters in these super-subnets as transferable
parameters, and the remaining set of parameters as untrans-
ferable parameters. A key question is how to distill these
transferable parameters during each subgraph auto-tuning
stage. We identify the distilling boundary criterion ξ(ph) as
below:

(5)
ξ(i) = |w(ph) ∗ ∇w(ph)|
where we denote the tuning phase of the subgraphs by ph.
w(ph) ∈ W (ph) represents the parameter weights and its
gradient is ∇w(ph). If ξ(ph) is larger than a certain threshold
ϑ (e.g., 0.5), the corresponding w(ph) can be viewed as trans-
In contrast, w(ph) would be regarded
ferable parameters.

as domain-variant parameters if ξ(ph) is small(e.g., close to
zero). We also provide a ranking mechanism here, speciﬁ-
cally, we rank these parameters based on their cross-domain
importance according to Eq.(5). Thus the users can set the
transferable parameters ratio manually. We iteratively update
the boundary of domain-invariant parameters as well as vari-
ant parameters and update these invariant ones during each
online training epoch to learn invariant representations to
achieve minimization of bound limitation ϕ. Lastly, we adopt
the adversarial loss training objective function Linv(S, T ) de-
ﬁned as below:

Linv(S, T ) = Lx∼p(S)(lg(b(w(ph, inv))))

(6)

+βLx∼p(T, (cid:101)T )(lg(1 − b(w(ph, inv))))
where b() represents the labeling black-box functions, β as
the coefﬁcient that control the entropy effects (usually set as
a small number). In such a way, the parameters with higher
gradient ﬂows, representing more beneﬁcial to the domain-
invariant information learning process, are considered. As
for the parameters that represent domain variant information,
We update theses parameters with a weight decay mechanism
as penalty, which is deﬁned as:

wv(ph + 1) → wv(ph) − α(wd(wv(ph)))

(7)

where α is the learning rate, wv(ph) represents the domain-
variant parameters and function wd() represents the weight
decay process for each updating phase.
3.5 Adaptive Tuning Data Partition
To gather on-device measurements efﬁciently and maintain
the performance of the online domain adaptation cost model,
we use an adaptive controller (AC) module to early terminate
the hardware tuning data collection stage. The basic idea of
AC is to statistically analyze the certainty of online training
cost model. For a give subgraph s which is to be tuned, we
initially divide the total tuning tasks into tuning tasks for on-
line training ttrain with hardware measurement data collec-
tion and tuning tasks for cost model predictions tpred with a
ratio of p. We further divide ttrain into q ∈ 1, 2, .., q batches
and collect both on-device measurement records C(ttrain(s))
and Perf(ttrain(s)). Then, we use the coefﬁcient of variations
(the standard deviation divided by the mean) formulated as
CV = σ(C(ttrain(s))1,C(ttrain(s))2..C(ttrain(s))q)
µ(C(ttrain(s))1,C(ttrain(s))2..C(ttrain(s))q) to dynami-
cally estimate the certainty of the existing cost model, and
terminate the hardware measurement phase in advance if the
value is smaller than a certain value. We empirically set these
hyper-parameters based on multiple trials in our experiments.
3.6 Putting Everything Together
In previous sections, we described the design details of the
lottery-ticket based cross-device cost model transfer and the
adaptive online training data partition mechanism. We now
put these components together and summarize the working
ﬂow of Moses.
Step 1. Pre-training a cost model on the source device: We
pre-train a cost model ofﬂine using the dataset of on-device
measurement records from the source device. This dataset in-
cludes randomly generated tensor programs for widely deep

learning models.
Step 2. Transferring the trained model to the target de-
vice: The learned cost model from source device is directly
transferred to the target device in this step, to guide the search
stage during auto-tuning.
Step 3. Adaptive training data partition with the AC mod-
ule: For each tuning task, we dynamically control the hard-
ware measurement costs by using the AC module, with which
the portion of on-device measurements can be adjusted by the
evaluations of cost model performance in that epoch.
Step 4. Online updating the cost model with iterative
pruning: For each tuning task, we divide the parameters
of the cost model into domain-invariant ones and domain-
variant ones based on the calculation of ξ(i), and update the
domain-invariant parameters with gradient decent while let-
ting the rest gradually decrease to zero due to weight decay.
During the auto-tuning process, Moses keeps updating the
cost model in an adaptive and iteratively manner based on the
collected hardware measurements records, while the search
algorithms keep querying the newest cost model for efﬁcient
explorations of optimal program conﬁgurations.

4 Experiments
In previous sections, we describe how we enable the cross-
device cost model domain adaptation and make the auto-
tuning process on a new device more adaptive In this section,
we evaluate the effectiveness of Moses, with our proposed
lottery-ticket based cost model adaptation method. We imple-
ment Moses as a plug-in cross-device cost model adaptation
tool in TVM auto-tuning [Chen et al., 2018a]. Speciﬁcally,
the cost model ﬁne-tuning is integrated with the tensor pro-
grams random sampling and an evolutionary search algorithm
[Zheng et al., 2020a]. The training of the cost model is imple-
mented in PyTorch. We set the max epoch to 30. We set the
initial learning rate alpha to 0.001, the distilling boundary
criterion threshold ϑ to 0.5.

4.1 Generated Dataset for Embedded Devices
As mentioned before, learning-based tensor compilers can
greatly boost DNN model inference performance. At the
core of the auto-tuning process in these compilers, is a cost
model which estimates the performance of the combinations
of tensor representation knobs on different devices with an
input DNN model. To perform the evaluation and ease the
training efforts on cost models especially on embedded de-
vices. In this paper, we collect a comprehensive tensor pro-
gram dataset for two embedded devices: NVIDIA Jetson TX2
and XAIVER. We collect tasks from over 50 DNN models
including the popular mobile transformers (e.g. mobileViT
[Mehta and Rastegari, 2021]). More than ten million of pro-
gram records are included in this dataset.

4.2 Experimental Settings
Our experiments are conducted on NVIDIA GeForce GTX
2080 and NVIDIA Jetson TX2 with Pascal GPU architec-
ture with 256 NVIDIA CUDA cores. The current deploy-
ments such as [Zheng et al., 2020a; Zheng et al., 2020b;
Li et al., 2020] optimize the input DNN models in a sub-
graph basis, which means to optimize the fused operators one

Figure 4: End-to-end DNN model inference latency reductions
GAIN comparisons among MobileNet, ResNet18, BERT-Base and
SqueezeNet over two domain adaptation baselines.

Figure 5: Auto-tuning search efﬁciency GAIN comparisons among
MobileNet, ResNet18, BERT-Base and SqueezeNet over two do-
main adaptation baselines.

by one. Thus the the processes are independent of the DNN
model, which means we do not need to include multiple com-
plicated DNN models to validate Moses. The current popu-
lar operators of DNN models for both academia and indus-
try can be summarised as: convolutional layers, depthewise
separable convolutional layers; multi-head attention module;
residual block and other types of layers such as fully con-
nected layers and pooling layers. We include four DNN mod-
els in our experiments: ResNet-18, MobileNet, BERT-base
and SqueezeNet. We use the default settings for other hyper-
parameters provided by Ansor [Zheng et al., 2020a]. As for
the backbone of the cost model, we choose the representative
one used in Ansor, which is an MLP with two hidden lay-
ers, with 512 neurons for each. We train the MLP cost model
with ranking loss on NVIDIA Tesla K80, which is noted as
the source device (domain), based on the dataset provided by
Tenset [Zheng et al., 2021]. The two main domain adaptation
tasks we validate are K80 → 2060 and K80 → T X2.
4.3 Evaluation Metrics
We use the end-to-end latency/throughput and the end-to-end
search efﬁciency of auto-tuning as two main metrics. Specif-
ically, we measure the obtained speedups of tuned tensor
programs and the reductions of searching time of an input
DNN model over other baselines including the stat-of-art cost
model transfer method provided in Tenset. We also intro-
duce a concept named Cost Model & Auto-tuning Efﬁciency
Gain Score (CMAT) to evaluate the cost model inﬂuence on

Table 1: A summary of comparisons of CMAT under small and large trials. S, R, M and B refer to four DNNs mentioned in Fig. 5.

CMAT (%)

2060-S

2060-R 2060-M 2060-B TX2-S TX2-R TX2-M

Small Trials (200)
Large Trials (20000/5000)

57.2
48.1

19.6
32.7

105
45.8

66.7
87.4

28.7
44.7

66.4
53.1

64.5
45.9

K80 → 2060 and K80 → T X2 settings. It can also be ob-
served that, for some input DNN models such as SqueezeNet
and MobileNet, Ansor-Random and Tenset-Pretrain could be
more efﬁcient than Moses. This is because these baselines
provide no online learning during the auto-tuning. There-
fore, the corresponding end-to-end model inference latency
of these models can be greatly lower than Moses. The eval-
uation results show that, the search efﬁciency gain of the
K80 → 2060 setting can be up to 47.8% while up to 58.5%
for the K80 → T X2 setting. This is because the on-
device data collection costs on TX2 is much higher than on
RTX2060.
CMAT Score Comparisons.
Table. 1 shows the superior comprehensive performance of
Moses over both small (200) and large (20000 for 2060, 5000
for TX2) number of trials across all input DNN models. As
mentioned above, although Tenset achieves 15% auto-tuning
efﬁciency gain on MobileNet based on the K80 → 2060 set-
ting, which is better than Moses (9.6%), the corresponding
CMAT is -14.75% over Tenset ﬁne-tuning, which is much
worse than Moses (up to 45.8%). We can observe that for
some cases (e.g. 2060-S), the CMAT gain under the small-
trial setting can even be better than the large-trial one, due
to the characteristics of the heuristic searching algorithm em-
bedded in the auto-tuning component in TVM. The base per-
formance of Tenset-Finetune can be extremely low with no
prior knowledge during the transfer process of the cost model.
4.5 Ablation Study: Ratio of Transferable

Parameters.

Here we provide more analysis on the ratio of transferable
parameters. Fig. 6 shows the results of Moses on a wider
setting of transferable parameters ratio: {0.01, 0.3, 0.5, 0.7}.
According to the end-to-end performance results, we can ob-
serve that the optimal performance can be around 0.5. Gener-
ally speaking, the std value for settings of {0.3, 0.5, 0.7} ra-
tio is not large, which suggests that, the optimal performance
produced by different ratios is not sensitive to the ratio setting
when it is ranging from 0.3 to 0.7.

5 Conclusion & Future Work
We present Moses, a new framework to optimize the auto-
tuning process in DNN compiler. Speciﬁcally, our approach
enables cross-device domain adaptation of a trained cost
model by updating the domain invariant parameters during
online learning, which greatly improves the efﬁciency of
auto-tuning process and the end to end throughput of tuned
tensor programs on the target device. Besides, we generate
a large-scale program performance dataset on two embedded
GPUs for learning based DNN compilers. Our future work in-
cludes extending Moses to support knowledge transfer from
the cross-subgraph tensor optimization perspective.

Figure 6: An illustration of Moses performance with a wider ratios
of transferable parameters, the yellow box shows the results of ra-
tio=0.01.

the end-to-end inference performance at the same time, de-
ﬁned as: CMAT = (Gain on Search Efﬁciency * Reduction on
Tuned Model Latency-1)*100%. As CMAT considers both
search efﬁcieny and end-to-end inference latency, it is an
effective metric for evaluating the overall cross-device cost
model adaptation performance.
4.4 Results
The main results we provided aim to answer the following
questions: 1). Can Moses transfer the trained cost model
to different hardware platforms and outperform other online
ﬁne-tuning method provided by Tenset? 2). Can Moses ac-
celerate the auto-tuning process by adaptively scheduling the
portions of on-device measuring records trials? To answer
these questiones, we compare Moses with four baselines:
1) Raw: inference with original CUDA acceleration.
2) Ansor-Random [Zheng et al., 2020a]: randomly initial-
ize the cost model and train it from scratch during the
auto-tuning.

3) Tenset-Pretrain: pre-train a cost model on TenSet dataset
and directly apply it to the target device without ﬁne-
tuning.

4) Tenset-Finetune: utilize the cost model pre-trained on
TenSet dataset and then perform the vanilla online ﬁne-
tuning.

Inference Time & Search Efﬁciency Comparisons.
Fig. 4 shows the comparison of the ﬁnal end-to-end infer-
ence time of the input DNN model optimized on each base-
line. Moses achieves up to 41.1% faster inference speed over
Tenset-Finetune and up to 53% higher speed over Tenset-
Pretrain on the K80 → 2060 baseline. Moses also achieves
up to 26.2% over Tenset-Finetune and up to 52% over Tenset-
Pretrain on the K80 → T X2 baseline, respectively. Over-
all, Moses yields the best inference performance among all
other conﬁgurations and algorithms. Fig. 5 shows the auto-
tuning search efﬁciency gains comparisons over these base-
lines. Moses also outperforms all other baselines for both

References
[Abadi et al., 2016] Mart´ın Abadi, Paul Barham, Jianmin
Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry
Moore, Derek Gordon Murray, Benoit Steiner, Paul A.
Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,
Yuan Yu, and Xiaoqiang Zhang. Tensorﬂow: A system
for large-scale machine learning. CoRR, abs/1605.08695,
2016.

[Ahn et al., 2020] Byung Hoon Ahn, Prannoy Pilligundla,
Amir Yazdanbakhsh, and Hadi Esmaeilzadeh. Chameleon:
Adaptive code optimization for expedited deep neural net-
work compilation. CoRR, abs/2001.08743, 2020.

[Baghdadi et al., 2021] Riyadh Baghdadi, Massinissa Mer-
ouani, Mohamed-Hicham Leghettas, Kamel Abdous, Taha
Arbaoui, Karima Benatchba, and Saman P. Amarasinghe.
A deep learning based cost model for automatic code op-
timization. CoRR, abs/2104.04955, 2021.

[Bai et al., 2021] Yang Bai, Xufeng Yao, Qi Sun, and Bei Yu.
Autogtco: Graph and tensor co-optimize for image recog-
nition with transformers on gpu. 2021.

[Chen et al., 2018a] Tianqi Chen, Thierry Moreau, Ziheng
Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan
Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos
Guestrin, and Arvind Krishnamurthy. TVM: An auto-
mated end-to-end optimizing compiler for deep learning.
In 13th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 18), pages 578–594, Carlsbad,
CA, October 2018. USENIX Association.

[Chen et al., 2018b] Tianqi Chen, Lianmin Zheng, Eddie Q.
Yan, Ziheng Jiang, Thierry Moreau, Luis Ceze, Carlos
Guestrin, and Arvind Krishnamurthy. Learning to opti-
mize tensor programs. CoRR, abs/1805.08166, 2018.
[Frankle and Carbin, 2019] Jonathan Frankle and Michael
Carbin. The lottery ticket hypothesis: Finding sparse,
trainable neural networks. In International Conference on
Learning Representations, 2019.

[Haj-Ali et al., 2020] Ameer Haj-Ali, Hasan Genc, Qijing
Huang, William S. Moses,
John Wawrzynek, Krste
Asanovic, and Ion Stoica. Protuner: Tuning programs with
monte carlo tree search. CoRR, abs/2005.13685, 2020.
[Han et al., 2021] Zhongyi Han, Haoliang Sun, and Yilong
Yin. Learning transferable parameters for unsupervised
arXiv preprint arXiv:2108.06129,
domain adaptation.
2021.

[Jia et al., 2019] Zhihao Jia, Oded Padon, James Thomas,
Todd Warszawski, Matei Zaharia, and Alex Aiken. Taso:
Optimizing deep learning computation with automatic
In Proceedings of the
generation of graph substitutions.
27th ACM Symposium on Operating Systems Principles,
SOSP ’19, page 47–62, New York, NY, USA, 2019. Asso-
ciation for Computing Machinery.

Mendis, Sudip Roy, Amit Sabne, and Mike Burrows. A
learned performance model for tensor processing units.
arXiv preprint arXiv:2008.01040, 2020.

[Li et al., 2018] Tzu-Mao Li, Micha¨el Gharbi, Andrew
Adams, Fr´edo Durand, and Jonathan Ragan-Kelley. Dif-
ferentiable programming for image processing and deep
learning in halide. ACM Trans. Graph., 37(4), July 2018.
[Li et al., 2020] Menghao Li, Minjia Zhang, Chi Wang, and
Mingqin Li. Adatune: Adaptive tensor program com-
pilation made efﬁcient.
In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in
Neural Information Processing Systems, volume 33, pages
14807–14819. Curran Associates, Inc., 2020.

[Mehta and Rastegari, 2021] Sachin Mehta and Moham-
Mobilevit: Light-weight, general-
mad Rastegari.
purpose, and mobile-friendly vision transformer. CoRR,
abs/2110.02178, 2021.

[Ryu and Sung, 2021] Jaehun Ryu

and Hyojin Sung.
Metatune: Meta-learning based cost model for fast and
efﬁcient auto-tuning frameworks. CoRR, abs/2102.04199,
2021.

[Zhao et al., 2021] Zhihe Zhao, Kai Wang, Neiwen Ling,
and Guoliang Xing. Edgeml: An automl framework for
In Proceedings of
real-time deep learning on the edge.
the International Conference on Internet-of-Things Design
and Implementation, IoTDI ’21, page 133–144, New York,
NY, USA, 2021. Association for Computing Machinery.
[Zheng et al., 2020a] Lianmin Zheng, Chengfan Jia, Minmin
Sun, Zhao Wu, Cody Hao Yu, Ameer Haj-Ali, Yida Wang,
Jun Yang, Danyang Zhuo, Koushik Sen, Joseph E. Gonza-
lez, and Ion Stoica. Ansor: Generating high-performance
tensor programs for deep learning. In 14th USENIX Sym-
posium on Operating Systems Design and Implementa-
tion (OSDI 20), pages 863–879. USENIX Association,
November 2020.

[Zheng et al., 2020b] Size Zheng, Yun Liang, Shuo Wang,
Renze Chen, and Kaiwen Sheng. Flextensor: An auto-
matic schedule exploration and optimization framework
for tensor computation on heterogeneous system. In Pro-
ceedings of the Twenty-Fifth International Conference on
Architectural Support for Programming Languages and
Operating Systems, ASPLOS ’20, page 859–873, New
York, NY, USA, 2020. Association for Computing Ma-
chinery.

[Zheng et al., 2021] Lianmin Zheng, Ruochen Liu, Junru
Shao, Tianqi Chen, Joseph E. Gonzalez, Ion Stoica, and
Ameer Haj Ali. Tenset: A large-scale program perfor-
In Thirty-
mance dataset for learned tensor compilers.
ﬁfth Conference on Neural Information Processing Sys-
tems Datasets and Benchmarks Track (Round 1), 2021.

[Kaufman et al., 2020] Samuel

Kaufman,
Phitchaya Mangpo Phothilimthana, Yanqi Zhou, Charith

J

