9
1
0
2

v
o
N
1
1

]
E
C
.
s
c
[

2
v
7
1
1
5
0
.
0
1
9
1
:
v
i
X
r
a

Data-driven discovery of free-form governing
differential equations

Steven Atkinson∗
steven.atkinson1@ge.com

Waad Subber∗
waad.subber@ge.com

Liping Wang∗
wangli@ge.com

Genghis Khan∗
khan@ge.com

Philippe Hawi†
hawi@usc.edu

Roger Ghanem†
ghanem@usc.edu

Abstract

We present a method of discovering governing differential equations from data
without the need to specify a priori the terms to appear in the equation. The input
to our method is a dataset (or ensemble of datasets) corresponding to a particular
solution (or ensemble of particular solutions) of a differential equation. The output
is a human-readable differential equation with parameters calibrated to the individ-
ual particular solutions provided. The key to our method is to learn differentiable
models of the data that subsequently serve as inputs to a genetic programming
algorithm in which graphs specify computation over arbitrary compositions of func-
tions, parameters, and (potentially differential) operators on functions. Differential
operators are composed and evaluated using recursive application of automatic dif-
ferentiation, allowing our algorithm to explore arbitrary compositions of operators
without the need for human intervention. We also demonstrate an active learning
process to identify and remedy deﬁciencies in the proposed governing equations.

1

Introduction

The modern scientist enjoys access to ever-growing amount of laboratory data to help uncover new
knowledge. However, this abundance of data can be unwieldy to analyze using traditional methods of
deduction from which governing laws usually arise. Motivated by the notion of machine learning as a
partner in the scientiﬁc process, we introduce a method to automate the process of understanding
and manipulating data for the purpose of hypothesizing, criticizing, and ultimately discovering novel
physical governing laws. Our work is similar in spirit to a growing literature on machine learning and
differential programming to solve differential equations (1; 2; 3; 4), and calibration of physical laws
where the involved terms are known a priori (5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18).

Here, we focus on discovering free-form equations in that, unlike previous work, we do not require
an a priori speciﬁcation of the terms that may appear in the equation to be discovered; instead we
take as input a library of primitive (algebraic and differential) operators on functions from which
expressions are composed, calibrated, and evaluated on the ﬂy. The result is a method that allows
substantial ﬂexibility in discovering differential equations while still producing a highly structured
result (a compositional expression represented as a graph) conducive to further study and analysis.

∗GE Research, Niskayuna, NY, USA.
†University of Southern California, Los Angeles, CA.

Second Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
2 Methodology

Consider the space of pairs of functions UL = {(u : Ωx → Ωu ⊆ Rdu , f : Ωx → R)} such
that L[u] = f ∀(u, f ) ∈ UL, where Ωx ⊂ Rdst is a dst-dimensional spatiotemporal domain of
interest. Some (possibly stochastic) observation operator O : U → D produces discrete samples of
these functions as a data set Di = {dij}du+1
(e.g. sensor digital readouts). Given N “experiments”
j=1
{Di}N
i=1 from possibly-distinct (u, f ) pairs, we seek to determine the differential equation L obeyed
within our class of physical systems of interest UL as well as to calibrate any parameters in L
particular to the data observed. We present a methodology, summarized schematically in Fig. 1, to
solve this problem, explained presently.

Figure 1: Schematic of our methodology for discovering governing differential equations from data.

The ﬁrst step to our approach is to ﬁt explicit, differentiable models {Mij}N,du
i,j=1, Mij ∈ M to each
scalar output data set {dij}. We considered fully-connected feedforward neural networks (15) and
Gaussian processes3. Note that the model architectures (e.g. choice of nonlinearities and kernels)
must be selected to reﬂect the desired differentiability properties matching our inductive biases about
the nature of UL.

Second, having equipped our data {dij} with differentiable function representations {Mij}, we
utilize a genetic programming algorithm to compose differential equations. Let a differential equation
(hereafter referred to as an individual) be represented by a tree graph G(V, E) whose leaves are
instances of the ﬁtted models and parent verticies are n-ary operators selected from a user-supplied
library of operators. We do not restrict ourselves to algebraic operators (+, −, ×, . . . ), but include
differential operators as well (∂/∂xi, ∇(·), ∇ · (·), . . . ). Each leaf of the graph template is primarily
associated with models of a particular output dimension j, but realizations of the graph instantiate
all of the leaves from some “experiment” i. Differential operators are computed using automatic
differentiation using PyTorch (19); Critically and in contrast to prior work, arguments to the operator
nodes are functions, not arrays of numbers, allowing us to compose G as a function to evaluate at any
point in Ωx to return a residual r(x). Figure 2 provides some example expressions represented as
graphs that such as those that our method handles.

Finally, parameters θ to be calibrated enter G as leaves representing constant functions; we calibrate
them in a Bayesian manner using black-box variational inference (20). Reﬂecting our ignorance
about the role of the {θi}’s in G, we use a ﬂat prior for p(θ) and non-factorized multivariate Gaussian
variational posterior q(θ). The likelihood is a Gaussian r(x) with point estimate variance.

The hypothesis space over graphs is explored via genetic programming (21); we use the evolutionary
algorithm implemented by deap (22) to initialize, mutate, and mate graphs. Individuals compete using
the evidence lower bound (ELBO) evaluated at the inputs associated with the {Di}’s as the ﬁtness
function L. By iterating the EA, differential equations are automatically hypothesized and criticized;
the outcome of the EA is a list of candidate differential equations (G1, G2, . . . ) ranked by their
ﬁtness so L(G1, D) ≥ L(G2, D) ≥ . . . , reﬂecting the degree to which they (through the ﬁt models
M ) explain the observed data. Minimality may be encouraged through multi-objective optimization
with a second ﬁtness function favoring parsimony as in (23; 24) but was not considered in our current
work as we did not require it to discover the correct underlying equations in our examples.

3We use the posterior mean of the GP model in subsequent manipulations.

2

∇x · ()

×

−()

∇x

+

∂t

∂t

∂t

a(x)

u(x)

x(t)

x(t)

−

θ1

×

+

x(t)

∂t

∂t

u(x, t)

∇x · ()

∇x

u(x, t)

(a) The elliptic operator:
∇ · (−a(x)∇u(x)).

(b) Second-order ordinary differ-
ential operator:
¨x(t) + ˙x(t) + x(t).

(c) The wave equation operator:
¨u(x, t) − θ1∇2u(x, t)

Figure 2: Examples of compositional differential operators represented as graphs combining functions
with primitive algebraic and differential operators.

If one has the ability to gather more data, then our approach may be paired with an active learning
loop to sequentially obtain additional samples to augment the Di’s using a(x) = r2(x) under G1
as an acquisition function. This causes one to look more closely where the best explanation of
the observed data is most inadequate. One may also deﬁne a tolerance δ such that when a∗ =
arg maxx∈Ωx a(x) < δ, the iterative algorithm is said to have converged and the discovered equation
explains the observed data to a user-speciﬁed degree.

3 Examples

Code for examples in this section will be made public upon publication. In all of the following
examples, we use the posterior means of Gaussian process models with squared exponential kernels
(25) to ﬁt functions to the data; Fully-connected neural networks tended not to capture the gradients
of the data accurately. We hypothesize this can be attributed to inferior inductive biases conferred
by typical architectures, and is a subject of ongoing investigation. The evolutionary algorithm uses
a population of 512 individuals, probability of mutation 0.2, and probability of mating 0.8. These
hyperparameters were not ﬁne-tuned as we did not ﬁnd it necessary in order to obtain satisfactory
performance.

3.1 Synthetic benchmarks

We benchmark the above methodology by applying it to a number of classic differential equations
enumerated in Table 1. We select ground truths with unity coefﬁcients to focus here on discovering
the structure of the equation; calibration of parameters will be emphasized in the example of Sec.
3.2. Data for the ODEs and one-dimensional elliptic problems were produced using Python’s SciPy
library and FEniCS (26), respectively. For the two-dimensional elliptic equation, data was obtained
from (27). We repeat each experiment 40 times with different random number generator seeds to
understand the robustness of our method.

Table 1: Summary of the benchmark differential equations discovered using our methodology.

Differential equation Differential operator Dependent variables

ODE
Heterogeneous elliptic PDE
Nonlinear elliptic PDE
2D Heterogeneous elliptic PDE

d

¨u + ˙u + u
dx (−a(x) du
dx )
dx (−a(u) du
dx )
∇ · (−a(x)∇u)

d

u(t) : [0, 10] → R
a : [0, 1] → R+, u(x) : [0, 1] → R
u(x) : [0, 1] → R
a : [0, 1]2 → R+, u(x) : [0, 1]2 → R

f ?= 0
Yes
No
No
Yes

Figure 3a shows the success rate for the benchmark systems listed in Table 1 as the number of samples
available from the experiment n is varied. We report results in terms of n1/dst, where dst is the
number of independent (spatiotemporal) variables in the problem. Figure 3b shows the performance

3

of our method in the adaptive setting in which data are added sequentially following Sec. 2, as
quantiﬁed by the evolution of a∗ with increasing n. We see that a termination tolerance of δ = 0.1
seems sufﬁcient to ensure the discovery of the correct equations, and many runs consistently identify
the correct equation far in advance of termination.

(a) Success rates

(b) Adaptive sampling

Figure 3: (a) Frequency of success in discovering the underlying differential equations for the
benchmark problems as a function of n1/dst. (b) Maximum discrepancy as quantiﬁed by a∗ during
adaptive acquisition of data. Different curves show repeats for a given problem. Heavy lines show
the median performance over 40 repeats.

3.2 Real-world ultrasound experiment

Finally, we demonstrate our approach on discovering the physics of a laboratory ultrasound experi-
ment. A laser is used to measure the deﬂection of a cracked Aluminum alloy specimen subjected to
an acoustic impulse. The wave travels through the material and exhibits back-scattering at the crack
location (Fig. 4).

Figure 4: Snapshots of the ultrasound experiment at various time steps.

It is expected based on expert knowledge that the data should be well-described by the wave equation,
Lw[u] = utt − α∆2uxx = 0, throughout the bulk of the homogeneous material. However, this fails
to capture inhomogeneities, dissipative forces, and out-of-plane behavior that might be amended with
corrective terms. It is of interest to build a computational model based on the discovered physics from
which we may ultimately solve the inverse problem of inferring unseen crack morphologies from a
sparse sensor array. We restrict ourselves to searching for a corrector L(cid:48) such that utt − L[u; θ] = 0,
with L = θ1∇2u + L(cid:48)[u; θ2:]. Parameters θ are calibrated as explained in Sec. 2.
We preprocess the data by cropping to a smaller window in spacetime to exclude data where no
signiﬁcant activity is observed; the resulting data are ﬁtted with a GP. Table 2 enumerates the ﬁttest
correctors discovered from the ﬁrst 32 proposed individuals along with the mean and standard
deviation of the parameters’ marginal posteriors and ﬁtness (ELBO) associated with the individuals.
Figure 5 illustrates a slice of the dataset along with its GP model, the differential terms utt and ∇2u,
and the resulting residual associated with the calibrated wave equation using the mode of q(θ).
4 Conclusions

We have described a novel method for discovering governing differential equations with arbitrary
structure from raw data and demonstrated its effectiveness on a number of standard differential

4

05101520n1/dst0.00.20.40.60.81.0Success rateODE0102030n1/dst0.00.20.40.60.81.0Success rateElliptic (Heterogeneous)0102030n1/dst0.00.20.40.60.81.0Success rateElliptic (Nonlinear)05101520n1/dst0.00.20.40.60.81.0Success rateElliptic (2D)020406080100n102101100101102a*(n)ODEElliptic (Heterogeneous)Elliptic(Nonlinear)Table 2: Potential correctors discovered for the ultrasound dataset. The uncorrected equation is
provided last as a baseline.

L[u]
θ1∇2u + θ2 + θ3u
θ1∇2u + θ2u + θ3u2
θ1∇2u + θ2 + θ3u2
θ1∇2u + θ2u2 + θ3u3
θ1∇2u + θ2u + θ3ut
θ1∇2u

q(θ) (mean, std)
ELBO
(21.7, 0.14), (120, 2300), (−69, 2900) −6.17445 × 104
−6.17450 × 104
−6.17452 × 104
−6.17455 × 104
−6.17509 × 104
−6.17626 × 104

(21.7, 0.14), (−69, 3400), (−3.78, 4.70)
(21.7, 0.14), (110, 2300), (83, 2400)
(21.7, 0.14), (84, 2200), (−45, 1400)
(21.7, 0.14), (−69, 3400), (−3.78, 4.70)
(21.7, 0.14)

Figure 5: From left to right: A slice in time of the ultrasound data, its GP function representation
used in exploring differential equations, the term utt(x) evaluated on the function, the term ∇2u
evaluated on the function, and the residual of the calibrated wave equation using the mode of q(θ).

equations. Our method paves the way for an artiﬁcial intelligence “research assistant” as a companion
to scientists seeking to understand novel physics. Furthermore, the output of our method, being
human-readable differential equations, is compatible with existing workﬂows available to engineers
and scientists such as theoretical analysis and numerical simulation, including so-called physics-
informed machine learning methods.

Acknowledgments

The authors thank the United States Air Force Research Laboratory for providing the ultrasound
data analyzed in Sec. 3.2 for public release under Distribution A as deﬁned by the United States
Department of Defense Instruction 5230.24. This material is based upon work supported by the
Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR00111990032.
Approved for public release; distribution is unlimited.

References

[1] Ioannis G Tsoulos and Isaac E Lagaris. Solving differential equations with genetic programming.

Genetic Programming and Evolvable Machines, 7(1):33–54, 2006.

[2] Dario Izzo, Francesco Biscani, and Alessio Mereta. Differentiable genetic programming. In

European Conference on Genetic Programming, pages 35–51. Springer, 2017.

[3] Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. PDE-net: Learning PDEs from data.

arXiv preprint arXiv:1710.09668, 2017.

[4] Waldir Jesus de Araujo Lobão, Marco Aurélio Cavalcanti Pacheco, Douglas Mota Dias, and Ana
Carolina Alves Abreu. Solving stochastic differential equations through genetic programming
and automatic differentiation. Engineering Applications of Artiﬁcial Intelligence, 68:110–120,
2018.

[5] Josh Bongard and Hod Lipson. Automated reverse engineering of nonlinear dynamical systems.

Proceedings of the National Academy of Sciences, 104(24):9943–9948, 2007.

[6] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data.

science, 324(5923):81–85, 2009.

5

DataModel2.01.51.00.50.00.51.0utt50000005000001000000div(grad u)2000002000040000|r(x)|100000200000300000[7] Daniel L Ly and Hod Lipson. Learning symbolic representations of hybrid dynamical systems.

Journal of Machine Learning Research, 13(Dec):3585–3618, 2012.

[8] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations
from data by sparse identiﬁcation of nonlinear dynamical systems. Proceedings of the National
Academy of Sciences, 113(15):3932–3937, 2016.

[9] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Machine learning of linear
differential equations using gaussian processes. Journal of Computational Physics, 348:683–
693, 2017.

[10] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics informed deep learn-
ing (part i): Data-driven solutions of nonlinear partial differential equations. arXiv preprint
arXiv:1711.10561, 2017.

[11] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics informed deep learning
(part ii): Data-driven discovery of nonlinear partial differential equations. arXiv preprint
arXiv:1711.10566, 2017.

[12] Samuel H Rudy, Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Data-driven discovery

of partial differential equations. Science Advances, 3(4):e1602614, 2017.

[13] Hayden Schaeffer. Learning partial differential equations via data discovery and sparse optimiza-
tion. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences,
473(2197):20160446, 2017.

[14] Maziar Raissi and George Em Karniadakis. Hidden physics models: Machine learning of
nonlinear partial differential equations. Journal of Computational Physics, 357:125–141, 2018.

[15] Maziar Raissi. Deep hidden physics models: Deep learning of nonlinear partial differential

equations. The Journal of Machine Learning Research, 19(1):932–955, 2018.

[16] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Multistep neural networks for
data-driven discovery of nonlinear dynamical systems. arXiv preprint arXiv:1801.01236, 2018.

[17] Samuel Rudy, Alessandro Alla, Steven L Brunton, and J Nathan Kutz. Data-driven identiﬁcation
of parametric partial differential equations. SIAM Journal on Applied Dynamical Systems,
18(2):643–660, 2019.

[18] Seungjoon Lee, Mahdi Kooshkbaghi, Konstantinos Spiliotis, Constantinos I. Siettos, and
Ioannis G. Kevrekidis. Coarse-scale PDEs from ﬁne-scale observations via machine learning.
arXiv preprint arXiv:1909.05707, 2019.

[19] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.

[20] Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artiﬁcial

Intelligence and Statistics, pages 814–822, 2014.

[21] Wolfgang Banzhaf, Peter Nordin, Robert E Keller, and Frank D Francone. Genetic programming:

an introduction, volume 1. Morgan Kaufmann San Francisco, 1998.

[22] Félix-Antoine Fortin, François-Michel De Rainville, Marc-André Gardner, Marc Parizeau, and
Christian Gagné. DEAP: Evolutionary algorithms made easy. Journal of Machine Learning
Research, 13:2171–2175, jul 2012.

[23] Jeff Clune, Jean-Baptiste Mouret, and Hod Lipson. The evolutionary origins of modularity.

Proceedings of the Royal Society b: Biological sciences, 280(1755):20122863, 2013.

[24] Adam Gaier and David Ha. Weight agnostic neural networks. arXiv preprint arXiv:1906.04358,

2019.

[25] Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning.

MIT press Cambridge, MA, 2006.

6

[26] Anders Logg, Kent-Andre Mardal, and Garth Wells. Automated solution of differential equations
by the ﬁnite element method: The FEniCS book, volume 84. Springer Science & Business
Media, 2012.

[27] Steven Atkinson and Nicholas Zabaras. Structured bayesian gaussian process latent variable
model: Applications to data-driven dimensionality reduction and high-dimensional inversion.
Journal of Computational Physics, 383:166–195, 2019.

7

