Local Policy Optimization for Trajectory-Centric Reinforcement Learning

Patrik Kolaric1, Devesh K. Jha2, Arvind U. Raghunathan2, Frank L. Lewis1,
Mouhacine Benosman2, Diego Romeres2 Daniel Nikovski2

0
2
0
2

n
a
J

2
2

]

G
L
.
s
c
[

1
v
2
9
0
8
0
.
1
0
0
2
:
v
i
X
r
a

Abstract— The goal of this paper is to present a method for
simultaneous trajectory and local stabilizing policy optimization
to generate local policies for trajectory-centric model-based
reinforcement learning (MBRL). This is motivated by the fact
that global policy optimization for non-linear systems could be a
very challenging problem both algorithmically and numerically.
However, a lot of robotic manipulation tasks are trajectory-
centric, and thus do not require a global model or policy.
Due to inaccuracies in the learned model estimates, an open-
loop trajectory optimization process mostly results in very
poor performance when used on the real system. Motivated by
these problems, we try to formulate the problem of trajectory
optimization and local policy synthesis as a single optimization
problem. It is then solved simultaneously as an instance of
nonlinear programming. We provide some results for analysis
as well as achieved performance of the proposed technique
under some simplifying assumptions.

I. INTRODUCTION

Reinforcement learning (RL) is a learning framework that
addresses sequential decision-making problems, wherein an
‘agent’ or a decision maker learns a policy to optimize a
long-term reward by interacting with the (unknown) environ-
ment. At each step, the RL agent obtains evaluative feedback
(called reward or cost) about the performance of its action,
allowing it to improve the performance of subsequent actions
[1], [2]. Although RL has witnessed huge successes in recent
times [3], [4], there are several unsolved challenges, which
restrict the use of these algorithms for industrial systems. In
most practical applications, control policies must be designed
to satisfy operational constraints, and a satisfactory policy
should be learnt in a data-efﬁcient fashion [5].

Model-based reinforcement learning (MBRL) methods [6]
learn a model from exploration data of the system, and then
exploit the model to synthesize a trajectory-centric controller
for the system [7]. These techniques are, in general, harder
to train, but could achieve good data efﬁciency [8]. Learning
reliable models is very challenging for non-linear systems
and thus, the subsequent trajectory optimization could fail
when using inaccurate models. However, modern machine
learning methods such as Gaussian processes (GP), stochas-
tic neural networks (SNN), etc. can generate uncertainty esti-
mates associated with predictions [9], [10]. These uncertainty
estimates could be used to estimate the conﬁdence set of
system states at any step along a given controlled trajectory

1UTA Research Institute, University of Texas at Arlington, Fort
patrik.kolaric@mavs.uta.edu,

(Email:

Worth,
TX,USA
lewis@uta.edu

2

Mitsubishi

Laborato-
ries
Email:
{jha,raghunathan,benosman,romeres,nikovski}@merl.com

Cambridge,

(MERL),

Research

Electric

USA.

MA,

for the system. The idea presented in this paper considers the
stabilization of the trajectory using a local feedback policy
that acts as an attractor for the system in the known region
of uncertainty along the trajectory [11].

We present a method for simultaneous trajectory opti-
mization and local policy optimization, where the policy
optimization is performed in a neighborhood (local sets)
of the system states along the trajectory. These local sets
could be obtained by a stochastic function approximator
(e.g., GP, SNN, etc.) that used to learn the forward model
of the dynamical system. The local policy is obtained by
considering the worst-case deviation of the system from
the nominal trajectory at every step along the trajectory.
Performing simultaneous trajectory and policy optimization
could allow us to exploit the modeling uncertainty as it
drives the optimization to regions of low uncertainty, where
it might be easier to stabilize the trajectory. This allows us
to constrain the trajectory optimization procedure to generate
robust, high-performance controllers. The proposed method
automatically incorporates state and input constraints on the
dynamical system.

Contributions. The main contributions of the current

paper are:

1) We present a novel formulation of simultaneous tra-
local policy

jectory optimization and time-invariant
synthesis for stabilization.

2) We present analysis of the proposed technique that
allows us to analytically derive the gradient of the
robustness constraint for the optimization problem.

is noted that

It
this paper only presents the controller
synthesis part for MBRL – a more detailed analysis of
the interplay between model uncertainties and controller
synthesis is deferred to another publication.

II. RELATED WORK

MBRL has raised a lot of interest recently in robotics
applications, because model learning algorithms are largely
task independent and data-efﬁcient [12], [8], [6]. However,
MBRL techniques are generally considered to be hard to
train and likely to result in poor performance of the re-
sulting policies/controllers, because the inaccuracies in the
learned model could guide the policy optimization process
to low-conﬁdence regions of the state space. For non-
linear control, the use of trajectory optimization techniques
such as differential dynamic programming [13] or its ﬁrst-
order approximation, the iterative Linear Quadratic Regu-
lator (iLQR) [14] is very popular, as it allows the use of
gradient-based optimization, and thus could be used for

 
 
 
 
 
 
high-dimensional systems. As the iLQR algorithm solves
the local LQR problem at every point along the trajectory,
it also computes a sequence of feedback gain matrices to
use along the trajectory. However,
the LQR problem is
not solved for ensuring robustness, and furthermore the
controller ends up being time-varying, which makes its use
somewhat inconvenient for robotic systems. Thus, we believe
that the controllers we propose might have better stabilization
properties, while also being time-invariant.

Most model-based methods use a function approximator to
ﬁrst learn an approximate model of the system dynamics, and
then use stochastic control techniques to synthesize a policy.
Some of the seminal work in this direction could be found
in [8], [6]. The method proposed in [8] has been shown to be
very effective in learning trajectory-based local policies by
sampling several initial conditions (states) and then ﬁtting
a neural network which can imitate the trajectories by
supervised learning. This can be done by using ADMM [15]
to jointly optimize trajectories and learn the neural network
policies. This approach has achieved impressive performance
on several robotic tasks [8]. The method has been shown
to scale well for systems with higher dimensions. Several
different variants of the proposed method were introduced
later [16], [17], [18]. However, no theoretical analysis could
be provided for the performance of the learned policies.

Another set of seminal work related to the proposed
work is on the use of sum-of-square (SOS) programming
methods for generating stabilizing controller for non-linear
systems [11]. In these techniques, a stabilizing controller,
expressed as a polynomial function of states, for a non-linear
system is generated along a trajectory by solving an opti-
mization problem to maximize its region of attraction [19].
Some other approaches to trajectory-centric policy opti-
mization could be found in [20]. These techniques use path
integral optimal control with parameterized policy represen-
tations such as dynamic movement primitives (DMPs) [21]
to learn efﬁcient local policies [22]. However, these tech-
niques do not explicitly consider the local sets where the
controller robustness guarantees could be provided, either.
Consequently, they cannot exploit the structure in the model
uncertainty. A workshop version of our paper could be found
here [23].

III. PROBLEM FORMULATION

In this section, we describe the problem studied in the
rest of the paper. To perform trajectory-centric control, we
propose a novel formulation for simultaneous design of
open-loop trajectory and a time-invariant, locally stabilizing
controller that
to bounded model uncertainties
and/or system measurement noise. As we will present in
this section, the proposed formulation is different from that
considered in the literature in the sense it allows us to exploit
sets of possible deviation of a system to design stabilizing
controller.

is robust

A. Trajectory Optimization as Non-linear Program

Consider the discrete-time dynamical system

(1)

xk+1 = f (xk, uk)
where xk ∈ Rnx , uk ∈ Rnu are the differential states and
controls, respectively. The function f : Rnx+nu → Rnx
governs the evolution of the differential states. Note that
the discrete-time formulation (1) can be obtained from a
continuous time system ˙x = ˆf (x, u) by using the explicit
Euler integration scheme (xk+1 − xk) = ∆t ˆf (xk, uk) where
∆t is the time-step for integration.

For clarity of exposition we have limited our focus to
discrete-time dynamical systems of the form in (1) although
the techniques we describe can be easily extended to implicit
discretization schemes.

In typical applications the states and controls are restricted
to lie in sets X := {x ∈ Rnx | x ≤ x ≤ x} ⊆ Rnx and
U := {u ∈ Rnu | u ≤ u ≤ u} ⊆ Rnu , i.e. xk ∈ X , uk ∈ U.
We use [K] to denote the index set {0, 1, . . . , K}. Further,
there may exist nonlinear inequality constraints of the form

(2)

g(xk) ≥ 0
with g : Rnx → Rm. The inequalities in (2) are termed
as path constraints. The trajectory optimization problem is
to manipulate the controls uk over a certain number of
time steps [T − 1] so that the resulting trajectory {xk}k∈[T ]
minimizes a cost function c(xk, uk). Formally, we aim to
solve the trajectory optimization problem

min
xk,uk

(cid:88)

k∈[T ]

c(xk, uk)

s.t. Eq. (1) − (2) for k ∈ [T ]

x0 = ˜x0
xk ∈ X for k ∈ [T ]
uk ∈ U for k ∈ [T − 1]

(TrajOpt)

where ˜x0 is the differential state at initial time k = 0. Before
introducing the main problem of interest, we would like to
introduce some notations.

||v||2

In the following text, we use the following short-
M = vT M v. We denote the nomi-
hand notation,
nal trajectory as X ≡ x0, x1, x2, x3, . . . , xT −1, xT , U ≡
u0, u1, u2, u3, ..., uT −1. The actual trajectory followed by the
system is denoted as ˆX ≡ ˆx0, ˆx1, ˆx2, ˆx3, . . . , ˆxT −1, ˆxT . We
denote a local policy as πW , where π is the policy and W
denotes the parameters of the policy. The trajectory cost is
also sometimes denoted as J = (cid:80)
k∈[T ]

c(xk, uk).

B. Trajectory Optimization with Local Stabilization

This subsection introduces the main problem of interest in
this paper. A schematic of the problem studied in the paper
is also shown in Figure 1. In the rest of this section, we will
describe how we can simplify the trajectory optimization and
local stabilization problem and turn it into an optimization
problem that can be solved by standard non-linear optimiza-
tion solvers.

problem as following:
(cid:88)

c(xk, uk)

min
xk,uk,W

k∈[T ]

s.t. Eq. (1) − (2) for k ∈ [T ]

x0 = ˜x0
xk ∈ X for k ∈ [T ]
uk ∈ U for k ∈ [T − 1]

max
δxk∈Rk

||xk+1 − f (xk + δxk, uk + πW (δxk))||2 ≤ (cid:15)k

The additional constraint

(RobustTrajOpt)
introduced in RobustTrajOpt
allows us to ensure stabilization of the trajectory by esti-
mating parameters of the stabilizing policy πW . It is easy
to see that RobustTrajOpt solves the worst-case problem for
the optimization considered in (4). However, RobustTrajOpt
introduces another hyperparameter to the optimization prob-
lem, (cid:15)k. In the rest of the paper, we refer to the following
constraint as the robust constraint:

max
k Skδxk≤1

δxT

||xk+1 − f (xk + δxk, uk + πW (δxk))||2 ≤ (cid:15)k

(5)
Solution of the robust constraint for generic non-linear
system is out of scope of this paper. Instead, we linearize
the trajectory deviation dynamics as shown in the following
Lemma.

Lemma 1. The trajectory deviation dynamics δxk+1 =
xk+1 − ˆxk+1 approximated locally around the optimal tra-
jectory (X, U ) are given by

δxk+1 = A(xk, uk) · δxk + B(xk, uk) · πW (δxk)

A(xk, uk) ≡ ∇xk
B(xk, uk) ≡ ∇uk

ˆf (xk, uk)
ˆf (xk, uk)

(6)

Proof. Use Taylor’s series expansion to obtain the desired
(cid:4)
expression.

To ensure feasibility of the RobustTrajOpt problem and
avoid tuning the hyperparameter (cid:15)k, we make another re-
laxation by removing the robust constraint from the set of
constraints and move it to the objective function. Thus, the
simpliﬁed robust trajectory optimization problem that we
solve in this paper can be expressed as following (we skip
the state constraints to save space).

min
xk,uk,W

(cid:88)

(
k∈[T ]

c(xk, uk) + α

(cid:88)

dmax,k)

k∈[T ]
s.t. Eq. (1) − (2) for k ∈ [T ]

(RelaxedRobustTrajOpt)
where the term dmax,k is deﬁned as following after lineariza-
tion.

dmax,k ≡
max
k Skδxk≤1

δxT

||A(xk, uk) · δxk + B(xk, uk) · πW (δxk)||2
P

(7)
Note that the matrix P allows to weigh states differently.
In the next section, we present the solution approach to

Fig. 1: A schematic representation of the time-invariant local
control introduced in this paper.

Consider the case where the system dynamics, f is only
partially known, and the known component of f is used to
design the controller. Consider the deviation of the system
at any step ’k’ from the state trajectory X and denote it as
δxk ≡ xk − ˆxk. We introduce a local (time-invariant) policy
πW that regulates the local trajectory deviation δxk and thus,
the ﬁnal controller is denoted as ˆuk = uk + πW (δxk). The
closed-loop dynamics for the system under this control is
then given by the following:

ˆxk+1 = f (ˆxk, ˆuk) = f (xk + δxk, uk + πW (δxk))

(3)

The main objective of the paper is to ﬁnd the time-invariant
feedback policy πW that can stabilize the open-loop trajec-
tory X locally within Rk ⊂ Rnx where Rk deﬁnes the set of
uncertainty for the deviation δxk. The uncertainty region Rk
can be approximated by ﬁtting an ellipsoid to the uncertainty
estimate using a diagonal positive deﬁnite matrix Sk such
that Rk = {δxk : δxT
k Skδxk ≤ 1}. The general optimization
problem that achieves that is proposed as:

E
J ∗ = min
δxk∈Rk
U,X,W
xk+1 = ˆf (xk, uk)

[J(X + δX, U + πW (δxk)]

(4)

where ˆf (·, ·) denotes the known part of the model. Note
that in the above equation, we have introduced additional
optimization parameters corresponding to the policy πW
when compared to TrajOpt in the previous section. However,
to solve the above, one needs to resort to sampling in order to
estimate the expected cost. Instead we introduce a constraint
that solves for the worst-case cost for the above problem.

Robustness Certiﬁcate. The robust trajectory optimiza-
tion problem is to minimize the trajectory cost while at the
same time satisfying a robust constraint at every step along
the trajectory. This is also explained in Figure 1, where the
purpose of the local stabilizing controller is to push the max-
deviation state at every step along the trajectory to (cid:15)-tolerance
balls around the trajectory. Mathematically, we express the

compute the gradient for the RelaxedRobustTrajOpt which
is then used to solve the optimization problem. Note that
rthis esults in simultaneous solution to open-loop and the
stabilizing policy πW .

IV. SOLUTION APPROACH

This section introduces the main contribution of the paper,
which is a local feedback design that regulates the deviation
of an executed trajectory from the optimal trajectory gener-
ated by the optimization procedure.

To solve the optimization problem presented in the last
section, we ﬁrst need to obtain the gradient information of the
robustness heuristic that we introduced. However, calculating
the gradient of the robust constraint is not straightforward,
because the max function is non-differentiable. The gradient
of the robustness constraint is computed by the application
of Dankins Theorem [24], which is stated next.
Dankin’s Theorem: Let K ⊆ Rm be a nonempty, closed
set and let Ω ⊆ Rn be a nonempty, open set. Assume that
the function f : Ω × K → R is continuous on Ω × K and
that ∇xf (x, y) exists and is continuous on Ω × K. Deﬁne
the function g : Ω → R ∪ {∞} by

g(x) ≡ sup
y∈K

f (x, y), x ∈ Ω

and

M (x) ≡ {y ∈ K | g(x) = f (x, y)}.

Let x ∈ Ω be a given vector. Suppose that a neighborhood
N (x) ⊆ Ω of x exists such that M (x(cid:48)) is nonempty for
all x(cid:48) ∈ N (x) and the set ∪x(cid:48)∈N (x)M (x(cid:48)) is bounded. The
following two statements (a) and (b) are valid. (a)

1) The function g is directionally differentiable at x and

g(cid:48)(x; d) = sup

∇xf (x, y)T d.

y∈M (x)

2) If M (x) reduces to a singleton, say M (x) = {y(x)},

then g is Gˆaeaux differentiable at x and

∇g(x) = ∇xf (x, y(x)).

Proof See [25], Theorem 10.2.1.
Dankin’s theorem allows us to ﬁnd the gradient of the
robustness constraint by ﬁrst computing the argument of
the maximum function and then evaluating the gradient of
the maximum function at the point. Thus, in order to ﬁnd
the gradient of the robust constraint (5), it is necessary to
interpret it as an optimization problem in δxk, which is
presented next. In Section III-B, we presented a general
formulation for the stabilization controller πW , where W
are the parameters that are obtained during optimization.
However, solution of the general problem is beyond the
scope of the current paper. Rest of this section considers a
linear πW for analysis.

Lemma 2. Assume the linear feedback πW (δxk) = W δxk.
Then, the constraint (7) is quadratic in δxk,

max
δxk
s.t.

||Mkδxk||2

P = max
δxk

δxT

k Skδxk ≤ 1

δxT

k M T

k · P · Mkδxk

(8)

where Mkis shorthand notation for

Mk(xk, uk, W ) ≡ A(xk, uk) + B(xk, uk) · W (9)

Proof. Write dmax from (7) as the optimization problem

dmax =
max
δxk
s.t.

δxT

k Skδxk ≤ 1

||A(xk, uk) · δxk + B(xk, uk) · πW (δxk)||2
P

(10)

Introduce the linear controller and use the shorthand notation
(cid:4)
for Mk to write (8).

The next lemma is one of the main results in the paper. It
connects the robust trajectory tracking formulation Relaxe-
dRobustTrajOpt with the optimization problem that is well
known in the literature.

Lemma 3. The worst-case measure of deviation dmax is

dmax =
λmax(S− 1

2

k M T

k · P · MkS− 1

2

k

) = ||P

1

2 MkS− 1

2

k

||2
2

where λmax(·) denotes the maximum eigenvalue of a matrix
and || · ||2 denotes the spectral norm of a matrix. Moreover,
the worst-case deviation δmax is the corresponding maximum
eigenvector

δmax =
(cid:104)

{δxk :

S− 1
k M T

k · P · MkS− 1

k

2

2

(cid:105)

· δxk = dmax · δxk}

(10)

Proof. Apply coordinate transformation δ ˜xk = Sk
(8) and write

1

2 δxk in

k · P · MkS− 1

k δ ˜xk

2

(11)

2

δ ˜xkS− 1

k M T

max
δ ˜xk
s.t.
δ ˜xkδ ˜xk ≤ 1
k · P · MkS− 1

2

2

k M T

Since S− 1
is positive semi-deﬁnite, the
maximum lies on the boundary of the set deﬁned by the
inequality. Therefore, the problem is equivalent to

k

max
δ ˜xk
s.t.

δ ˜xkS− 1

k M T

k · P · MkS− 1

k δ ˜xk

2

2

δ ˜xkδ ˜xk = 1

(12)

The formulation (12) is a special case with a known analytic
solution. Speciﬁcally, the maximizing deviation δmax that
solves (12) is the maximum eigenvector of S− 1
k · P ·
MkS− 1
, and the value dmax at the optimum is the corre-
(cid:4)
sponding eigenvalue.

k M T

k

2

2

This provides us with the maximum deviation along the
trajectory at any step ’k’, and now we can use Danskin’s
theorem to compute the gradient which is presented next.

2

k (z) · P · Mk(z)S− 1

Theorem 1. Introduce the following notation, M(z) =
S− 1
k M T
. The gradient of the robust
inequality constraint dmax with respect to an arbitrary vector
z is

k

2

∇zdmax = ∇zδT

maxM(z)δmax

Where δmax is maximum trajectory deviation introduced in
Lemma 3.

Proof. Start from the deﬁnition of gradient of robust con-
straint

∇zdmax = ∇z max
δ ˜xk

δ ˜xkM(z)δ ˜xk

Use Danskin’s Theorem and the result from Lemma 3 to
write the gradient of robust constraint with respect to an
arbitrary z,

∇zdmax = ∇zδT

maxM(z)δmax

which completes the proof.

(cid:4)

The gradient computed from Theorem 1 is used in solution
of the RelaxedRobustTrajOpt– however, this is solved only
for a linear controller. The next section shows some results
in simulation and on a real physical system.

V. EXPERIMENTAL RESULTS

In this section, we present some results using the proposed
algorithm for an under-actuated inverted pendulum, as well
as on a experimental setup for a ball-and-beam system.
We use a Python wrapper for the standard interior point
solver IPOPT to solve the optimization problem discussed
in previous sections. We perform experiments to evaluate
the following questions:

1) Can an off-the-shelf optimization solver ﬁnd feasible
solutions to the joint optimization problem described
in the paper?

2) Can the feedback controller obtained by this optimiza-
tion stabilize the open-loop trajectory in the presence
of bounded uncertainties?

3) How good is the performance of the controller on a
physical system with unknown system parameters ?

In the following sections, we try to answer these questions
using simulations as well as experiments on real systems.

A. Simulation Results for Underactuated Pendulum

The objective of this subsection is twofold: ﬁrst, to provide
insight into the solution of the optimization problem; and
second, to demonstrate the effectiveness of that solution in
the stabilization of the optimal trajectory. For clarity of pre-
sentation, we use an underactuated pendulum system, where
trajectories can be visualized in state space. The dynamics
of the pendulum is modeled as I ¨θ + b ˙θ + mgl · sin(θ) = u.
The continuous-time model is discretized as (θk+1, ˙θk+1) =
f ((θk, ˙θk), uk). The goal state is xg = [π, 0], and the initial
state is x0 = [0, 0] and the control limit is u ∈ [−1.7, 1.7].
The cost is quadratic in both the states and input. The initial
solution provided to the controller is trivial (all states and
control are 0). The number of discretization points along the

Fig. 2: State-space representation of the optimal trajectory
(green), stable closed-loop system using the obtained solu-
tion (blue), and unstable open-loop trajectory without the
local feedback (red). Note that the feedback is time-invariant.

trajectory is N = 120, and the discretization time step is
∆t = 1/30. The cost weight on robust slack variables is
selected to be α = 10. The uncertainty region is roughly
estimated as xT
k

xk < 1 along the trajectory. De-

(cid:21)

(cid:20)1.0
0.0

0.0
5.5

tailed analysis on uncertainty estimation based on Gaussian
processes is deferred to future work, due to space limits. The
optimization procedure terminates in 50 iterations with the
static solution W = [−2.501840, −7.38725].

The controller generated by the optimization procedure is
then tested in simulation, with noise added to each state of
the pendulum model at each step of simulation as xk+1 =
f (xk, uk) + ω with ωθ ∼ U(−0.2rad, 0.2rad]) and ω ˙θ ∼
U(−0.05rad/s, 0.05rad/s]).

We tested the controller over several settings and found
that the underactuated setting was the most challenging to
stabilize. In Figure 2, we show the state-space trajectory
for the controlled (underactuated) system with additional
noise as described above. As seen in the plot, the open-
loop controller becomes unstable with the additional noise,
while the proposed controller can still stabilize the whole
trajectory.

Fig. 3: a) open-loop control b) static feedback matrix ob-
tained from optimization c) LQR feedback

In Figure 3, we show the control inputs, the time-invariant
feedback gains obtained by the optimization problem. We
also the time-varying LQR gains obtained along the trajec-

tory to show provide some insight between the two solutions.
As the proposed optimization problem is ﬁnding the feedback
gain for the worst-case deviation from the trajectory, the
solutions are different than the LQR-case. Next, in Figure 4,
we plot the error statistics for the controlled system (in the
underactuated setting) over 2 different uncertainty balls using
each 12 sample for each ball. We observe that the steady-state
error goes to zero and the closed-loop system is stable along
the entire trajectory. As we are using a linear approximation
of the system dynamics, the uncertainty sets are still small,
however the results are indicating that incorporating the full
dynamics during stabilization could allow to generate much
bigger basins of attraction for the stabilizing controller.

by

¨x =

mballx ˙θ2 − b1 ˙x − b2mballg cos(θ) − mballg sin(θ)

Iball
r2
ball

+ mball

,

where mball is the mass of the ball, Iball is the moment of
inertia of the ball, rball is the radius of the ball, b1 is the
coefﬁcient of viscous friction of the ball on the beam, b2 is
the coefﬁcient of static (dry) friction of the ball on the beam,
and g is the acceleration due to gravity. The beam is actuated
by a servo motor (position controlled) and an approximate
model for its motion is estimated by ﬁtting an auto-regressive
model. We use this model for the analysis where the ball’s

Fig. 4: Error statistics for the controlled system using the
proposed method on the underactuated pendulum with noise
amplitude

B. Results on Ball-and-Beam System

Next, we implemented the proposed method on a ball-
and-beam system (shown in Figure 5) [26]. The ball-and-
beam system is a low-dimensional non-linear system with
the non-linearity due to the dry friction and delay in the
servo motors attached to the table (see Figure 5). The ball-
and-beam system can be modeled with 4 state variables
[x, ˙x, θ, ˙θ], where x is the position of the ball, ˙x is the ball’s
velocity, θ is the beam angle in radians, and ˙θ is the angular
velocity of the beam. The acceleration of the ball, ¨x, is given

Fig. 5: The ball-and-beam system used for the experiments.
There is an RGB camera above that measures the location
of the ball. The encoder (seen in the ﬁgure) measures the
angular position of the beam.

Fig. 6: Comparison of the performance of the proposed
controller on a ball-and-beam system with the open-loop
solution. The plot shows the error in the position of the ball
from the regulated position averaged over 12 runs.

rotational inertia is ignored and we approximately estimate
the dry friction. The model is inaccurate, as can be seen
from the performance of the open-loop controller in Figure 6.
However, the proposed controller is still able to regulate
the ball position at the desired goal showing the stabilizing
behavior for the system (see the performance of the closed-
loop controller in Figure 6). The plot shows the mean and the
standard deviation of the error for 12 runs of the controller.
It can be observed that
the mean regulation error goes
to zero for the closed-loop controller. We believe that the
performance of the controller will improve as we improve
the model accuracy. In future research, we would like to
study the learning behavior for the proposed controller by
learning the residual dynamics using GP [10].

VI. CONCLUSION AND FUTURE WORK

This paper presents a method for simultaneously comput-
ing an optimal trajectory along with a local, time-invariant
stabilizing controller for a dynamical system with known
uncertainty bounds. The time-invariant controller was com-
puted by adding a robustness constraint to the trajectory op-
timization problem. We prove that under certain simplifying
assumptions, we can compute the gradient of the robustness
constraint so that a gradient-based optimization solver could
be used to ﬁnd a solution for the optimization problem. We
tested the proposed approach that shows that it is possible to
solve the proposed problem simultaneously. We showed that
even a linear parameterization of the stabilizing controller

with a linear approximation of the error dynamics allows us
to successfully control non-linear systems locally. We tested
the proposed method in simulation as well as a physical
system. Due to space limitations, we have to skip extra
results regarding the behavior of the algorithm.

However, the current approach has two limitations– it
makes linear approximation of dynamics for ﬁnding the
worst-case deviation, and secondly, the linear parameteriza-
tion of the stabilizing controller can be limiting for a lot
of robotic systems. In future research, we will incorporate
these two limitations using Lyapunov theory for non-linear
control, for better generalization and more powerful results
of the proposed approach.

REFERENCES

[1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction

(2nd Edition). MIT press Cambridge, 2018, vol. 1.

[2] D. Vrabie, K. G. Vamvoudakis, and F. L. Lewis, Optimal Adaptive
Control and Differential Games by Reinforcement Learning Principles,
ser. IET control engineering series.
Institution of Engineering and
Technology, 2013.

[3] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
M. Lanctot et al., “Mastering the game of go with deep neural
networks and tree search,” nature, vol. 529, no. 7587, p. 484, 2016.
[4] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering
the game of go without human knowledge,” Nature, vol. 550, no. 7676,
p. 354, 2017.

[5] K. Vamvoudakis, P. Antsaklis, W. Dixon, J. Hespanha, F. Lewis,
H. Modares, and B. Kiumarsi, “Autonomy and machine intelligence in
complex systems: A tutorial,” in American Control Conference (ACC),
2015, July 2015, pp. 5062–5079.

[6] M. Deisenroth and C. E. Rasmussen, “PILCO: A model-based and
data-efﬁcient approach to policy search,” in Proceedings of the 28th
International Conference on machine learning (ICML-11), 2011, pp.
465–472.

[7] S. Levine and V. Koltun, “Guided policy search,” in International

Conference on Machine Learning, 2013, pp. 1–9.

[8] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training
of deep visuomotor policies,” The Journal of Machine Learning
Research, vol. 17, no. 1, pp. 1334–1373, 2016.

[9] C. E. Rasmussen, “Gaussian processes in machine learning,” in

Summer School on Machine Learning. Springer, 2003, pp. 63–71.

[10] D. Romeres, D. K. Jha, A. DallaLibera, B. Yerazunis, and D. Nikovski,
“Semiparametrical gaussian processes learning of forward dynamical
models for navigating in a circular maze,” in 2019 International
Conference on Robotics and Automation (ICRA).
IEEE, 2019, pp.
3195–3202.

[11] R. Tedrake, I. R. Manchester, M. Tobenkin, and J. W. Roberts, “LQR-
trees: Feedback motion planning via sums-of-squares veriﬁcation,” The
International Journal of Robotics Research, vol. 29, no. 8, pp. 1038–
1052, 2010.

[12] T. Wang, X. Bao, I. Clavera, J. Hoang, Y. Wen, E. Langlois, S. Zhang,
G. Zhang, P. Abbeel, and J. Ba, “Benchmarking Model-Based Rein-
forcement Learning,” arXiv e-prints, p. arXiv:1907.02057, Jul 2019.

[13] D. H. Jacobson, “New second-order and ﬁrst-order algorithms for
determining optimal control: A differential dynamic programming
approach,” Journal of Optimization Theory and Applications, vol. 2,
no. 6, pp. 411–440, 1968.

[14] Y. Tassa, T. Erez, and E. Todorov, “Synthesis and stabilization of
complex behaviors through online trajectory optimization,” in 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems.
IEEE, 2012, pp. 4906–4913.

[15] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein et al., “Dis-
tributed optimization and statistical learning via the alternating di-
rection method of multipliers,” Foundations and Trends R(cid:13) in Machine
learning, vol. 3, no. 1, pp. 1–122, 2011.

[16] Y. Chebotar, M. Kalakrishnan, A. Yahya, A. Li, S. Schaal, and
S. Levine, “Path integral guided policy search,” in 2017 IEEE in-
ternational conference on robotics and automation (ICRA).
IEEE,
2017, pp. 3381–3388.

[17] W. H. Montgomery and S. Levine, “Guided policy search via approx-
imate mirror descent,” in Advances in Neural Information Processing
Systems, 2016, pp. 4008–4016.

[18] A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine, “Neural network
dynamics for model-based deep reinforcement learning with model-
free ﬁne-tuning,” in 2018 IEEE International Conference on Robotics
and Automation (ICRA).

IEEE, 2018, pp. 7559–7566.

[19] A. Majumdar, A. A. Ahmadi, and R. Tedrake, “Control design
along trajectories with sums of squares programming,” in 2013 IEEE
International Conference on Robotics and Automation.
IEEE, 2013,
pp. 4054–4061.

[20] E. Theodorou, J. Buchli, and S. Schaal, “A generalized path integral
control approach to reinforcement learning,” journal of machine learn-
ing research, vol. 11, no. Nov, pp. 3137–3181, 2010.

[21] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal,
“Dynamical movement primitives: learning attractor models for motor
behaviors,” Neural computation, vol. 25, no. 2, pp. 328–373, 2013.

[22] G. Williams, N. Wagener, B. Goldfain, P. Drews, J. M. Rehg, B. Boots,
and E. A. Theodorou, “Information theoretic mpc for model-based
reinforcement learning,” in 2017 IEEE International Conference on
Robotics and Automation (ICRA).

IEEE, 2017, pp. 1714–1721.

[23] D. Jha, P. Kolaric, D. Romeres, A. Raghunathan, M. Benosman, and
D. N. Nikovski, “Robust optimization for trajectory-centric model-
based reinforcement learning.”

[24] D. P. Bertsekas, “Nonlinear programming,” Journal of the Operational

Research Society, vol. 48, no. 3, pp. 334–334, 1997.

[25] F. Facchinei and J.-S. Pang, Finite-Dimensional Variational Inequal-
ities and Complementarity Problems, Volume II. New York, NY:
Springer, 2003.

[26] D. K. Jha, D. Nikovski, W. Yerazunis, and A. Farahmand, “Learning
to regulate rolling ball motion,” in 2017 IEEE Symposium Series on

Computational Intelligence (SSCI), Nov 2017, pp. 1–6.

