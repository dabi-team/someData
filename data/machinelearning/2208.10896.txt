2
2
0
2

g
u
A
3
2

]

M
E
.
n
o
c
e
[

1
v
6
9
8
0
1
.
8
0
2
2
:
v
i
X
r
a

The Stata Journal (yyyy)

vv, Number ii, pp. 1–20

pystacked: Stacking generalization and machine
learning in Stata

Achim Ahrens
ETH Z¨urich
achim.ahrens@gess.ethz.ch

Christian B. Hansen
University of Chicago
christian.hansen@chicagobooth.edu

Mark E. Schaﬀer
Heriot-Watt University
Edinburgh, United Kingdom
m.e.schaﬀer@hw.ac.uk

Abstract.
pystacked implements stacked generalization (Wolpert, 1992) for
regression and binary classiﬁcation via Python’s scikit-learn. Stacking combines
multiple supervised machine learners—the “base” or “level-0” learners—into a sin-
gle learner. The currently supported base learners include regularized regression,
random forest, gradient boosted trees, support vector machines, and feed-forward
neural nets (multi-layer perceptron). pystacked can also be used with as a ‘reg-
ular’ machine learning program to ﬁt a single base learner and, thus, provides an
easy-to-use API for scikit-learn’s machine learning algorithms.

Keywords:
Stata, Python, sci-kit learn

st0001, machine learning, stacked generalization, model averaging,

1 Introduction

When faced with a new prediction or classiﬁcation task, it is a priori rarely obvious
which machine learning algorithm is best suited. A common approach is to evaluate
the performance of a set of machine learners on a hold-out partition of the data or
via cross-validation and then select the machine learner that minimizes a chosen loss
metric. However, this approach is incomplete as combining multiple learners into one
ﬁnal prediction might lead to superior performance compared to each individual learner.
This is the motivation for stacked generalization, or simply stacking, due to Wolpert
(1992) and Breiman (1996). Stacking is a form of model averaging. Theoretical results
in van der Laan et al. (2007) supports the use of stacking as it performs asymptotically
at least as well the best-performing individual learner if the number of base learners is
polynomial in sample size.

In this article, we introduce pystacked for stacking regression and binary classiﬁ-
cation in Stata. pystacked allows users to ﬁt multiple machine learning algorithms via
Python’s scikit-learn (Pedregosa et al. 2011; Buitinck et al. 2013) and combine these

© yyyy StataCorp LP

st0001

 
 
 
 
 
 
2

pystacked

into one ﬁnal prediction as a weighted average of individual predictions.1,2,3 pystacked
adds to the growing number of programs for machine learning in Stata, including, among
others, lassopack for regularized regression (Ahrens et al. 2020), rforest for random
forests (Schonlau and Zou 2020) and svm for support vector machines (Guenther 2016).
Similar to pystacked, Cerulli (2021) and Droste (2020) provide an interface to scikit-
learn in Stata. mlrtime allows Stata users to make use of R’s parsnip machine learning
library (Huntington-Klein 2021). pystacked diﬀers from these in that it is, to our
knowledge, the ﬁrst to make stacking available to Stata users. Furthermore, pystacked
can also be used to ﬁt a single machine learner and thus provides an easy-to-use and
versatile API to scikit-learn’s machine learning algorithms.

The use of stacking as a method and pystacked as a program is not only restricted
to pure prediction or classiﬁcation tasks. A growing literature exploits machine learning
to facilitate causal inference (Athey and Imbens 2019). Indeed, a motivation for writ-
ing pystacked is that it can be used in combination with ddml (Ahrens et al. 2022),
which implements Double-debiased Machine Learning for the estimation of structural
parameters (Chernozhukov et al. 2018).

Section 2 introduces stacking. Section 3 presents the main features of the pystacked

program. Section 4 demonstrates use of the program using examples.

2 Methodology

In this section, we brieﬂy summarize the stacking approach for regression and binary
classiﬁcation tasks. For a text book treatment, see Hastie et al. (2009).

We ﬁrst focus on stacking for regression problems where the aim is to predict the
continuous outcome yi using predictors xi. The idea of stacking is to combine a set of
“base” (or “level-0”) learners using a “ﬁnal” (or “level-1”) estimator. It is advisable
to include a relatively large and diverse set of base learners to capture diﬀerent types
of patterns in the data. The same algorithm can also be included more than once
using diﬀerent tuning or hyper-tuning parameters. Typical choices for base learners are
regularized regression or ensemble methods such as random forests or gradient boosting.

In the ﬁrst step of stacking, we obtain cross-validated predicted values ˆy(j)

for each
base learner j. The use of cross-validation is necessary as stacking would otherwise give
more weight to base learners that suﬀer from over-ﬁtting. The second step is to ﬁt
a ﬁnal learner using the observed yi as the outcome and the cross-validated predicted

i

1. We stress that pystacked relies on scikit-learn (version 0.24 or higher) and the on-going work of
the scikit-learn contributors. We thus kindly ask users to cite scikit-learn along with this article
when using pystacked.

2. Throughout we refer to v0.4.3 of pystacked, which is the latest version at the time of writing.
Please check for updates to pystacked on a regular basis and consult the help ﬁle to be informed
about new features.

3. pystacked relies on Stata’s Python integration which was introduced in Stata 16.0. The pystacked
help ﬁle includes information on how to install a recent Python version and set up Stata’s Python
integration.

Ahrens, Hansen & Schaﬀer

3

i

i

, . . . , ˆy(J)

values ˆy(1)
as predictors. A typical choice for the ﬁnal learner is constrained
least squares, which enforces the stacking weights to be non-negative and sum to one.
This restriction facilitates the interpretation of stacking as a weighted average of base
learners and may lead to better performance (Breiman 1996; Hastie et al. 2009). Algo-
rithm 1 summarizes the stacking algorithm for regression problems as it is implemented
in pystacked.

Algorithm 1: Stacking regression.

1. Cross-validation:

a. Split the data randomly into K partitions of approximately equal size. These
partitions are referred to as folds. Denote the set of observations in fold k
K such that I c
as Ik, and its complement as I c
K = {1, ..., n}/Ik. Ik constitutes
the validation set and I c
k the training sample.

b. For each fold k = 1, . . . , K and each base learner j = 1, . . . , J, ﬁt machine
k and obtain out-of-sample predicted values

learner j to the training data I c
ˆy(j)
i

for i ∈ Ik.

2. Final learner: Fit the ﬁnal learner to the full sample. The default choice is non-

negative least squares (NNLS):

min
w1,...,wJ



yi −

n
(cid:88)

i=1

J
(cid:88)

j=1



2

wj ˆy(j)
i



s.t. wj ≥ 0,

The stacking predicted values are deﬁned as ˆy(cid:63)

i = (cid:80)

j ˆwj ˆy(j)

i

wj = 1

J
(cid:88)

j=1

.

It is instructive to compare Step 2 with the classical ‘winner-takes-all’ approach
that selects one base learner as the one which exhibits the lowest the cross-validated
loss. Stacking, in contrast, may assign non-zero weights to multiple base learners,
thus combining their strengths to produce a better overall predictor than any of the
individual base learners. Other choices for the ﬁnal learner are possible. In addition
to the default ﬁnal learner, pystacked supports non-negative least squares without the
constraint (cid:80) wj = 1, ridge regression, the aforementioned ‘winner-takes-all’ approach
that selects the base learner with the smallest cross-validated mean-squared error and
unconstrained least square.

2.1 Stacking classiﬁcation

Stacking can be applied in a similar way to classiﬁcation problems. pystacked supports
stacking for binary classiﬁcation problems where the outcome yi takes the values 0 or
1. The main diﬀerence to stacking regression is that the ﬁnal learner is ﬁt using cross-
validated predicted probabilities ˆp(j)

.

i

4

2.2 Base learners

pystacked

In the following paragraphs, we brieﬂy describe the base learners supported by pystacked
and highlight central tuning parameters. We stress that each of the machine learners
discussed below can be ﬁt using pystacked as a regular stand-alone machine learner
without the stacking layer.

Note that it goes beyond the scope of the article to describe each learner in detail.
Familiarity with linear regression, logistic regression, and classiﬁcation and regression
trees is assumed. We recommend consultation of machine learning textbooks, e.g. Hastie
et al. (2009), for more detailed discussion.

Regularized regression imposes a penalty on the size of coeﬃcients to control over-
ﬁtting. The lasso penalizes the absolute size of coeﬃcients, whereas the ridge penalizes
the sum of squared coeﬃcients. Both methods shrink coeﬃcients toward zero, but only
the lasso yields sparse solutions where some coeﬃcient estimates are set to exactly zero.
The elastic net combines lasso and ridge-type penalty. For classiﬁcation tasks with
a binary outcome, logistic versions of lasso, ridge and elastic net are available. The
severity of the penalty is most commonly chosen by cross-validation. For lasso only,
pystacked also supports selecting the penalty by AIC or BIC. The use of AIC or BIC
has the advantage that is computationally less intensive than cross-validation. Ahrens
et al. (2020) compare the two approaches.

Random forests rely on ﬁtting a large number of regression or decision trees on boot-
strap samples of the data. The random forest prediction is obtained as the average
across individual trees. A crucial aspect of random forests is that, at each split when
growing a tree, only a random subset of predictors is considered. This restrictions aims
at de-correlating the individual trees. Central tuning parameters are the number of
trees (n_estimators()), the maximum depth of individual trees (max_depth()), the
minimum number of observations per leaf (min_samples_leaf()), the number of fea-
tures to be considered at each split (max_features()) and the size of the bootstrap
samples (max_samples()).

Gradient boosted trees also rely on ﬁtting a large number of trees. In contrast to ran-
dom forests, these trees are ﬁt sequentially to the residuals from the current model. The
learning rate determines how much the latest tree contributes to the overall model. Indi-
vidual trees are usually ﬁt to the whole sample, although sub-sampling is possible. In ad-
dition to tuning parameters relating to the trees, the learning rate (learnings_rate())
and number of trees (n_estimators()) are the most important tuning parameters.

Support vector machines (SVM). Support vector classiﬁers span a hyperplane that
separates observations by their outcome class. The hyperplane is chosen to maximize
the distance (margin) to correctly classiﬁed observations, while allowing for some clas-
siﬁcation errors. The tuning parameter C (C()) controls the frequency and degree of

Ahrens, Hansen & Schaﬀer

5

classiﬁcation mistakes. The hyperplane can be either linear or ﬁtted using kernels. The
SVM algorithm can also be adopted for regression tasks. To this end, the hyperplane is
constructed to include as many observations as possible in a tube of size 2(cid:15) around the
hyperplane. Central tuning parameters for regression are (cid:15) (epsilon()) and C (C()),
which determines the cost of observations outside of the tube.

Feed-forward neural networks consist of hidden layers that link the predictors (referred
to as input layers) to the outcome. Each hidden layer is composed of a multiple units
(nodes) which pass signals to the next layer using an activation function. Central tuning
parameters are the choice of the activation function (activation()), and the number
and size of hidden layers (hidden_layer_sizes()). Further tuning choices relate to
stochastic gradient decent algorithms which are typically used to ﬁt neural networks.
The default solver is Adam (Kingma and Ba 2014). The option early_stopping can
be used to set aside a random fraction of the data for validation. The optimization
algorithm stops if there is no improvement in performance over a pre-speciﬁed number
of iterations (see related options n_iter_no_change() and tol()).

3 Program

This section introduces the program pystacked and its main features. pystacked oﬀers
two alternatives syntaxes between which the user can choose (see Section 3.1 and 3.2).
Section 3.3 and 3.4 list post-estimation commands and general options, respectively.
Section 3.5 discusses supported base learners. Section 3.6 is a note on learner-speciﬁc
predictors. Section 3.7 explains the pipeline feature.

3.1 Syntax 1

The ﬁrst syntax uses methods(string) to select base learners, where string is a list of base
learners. Options are passed on to base learners via cmdopt1(string), cmdopt2(string)
to cmdopt10(string). That is, up to 10 base learners can be speciﬁed and options
are passed on in the order in which they appear in method(string) (see Section 3.5).
Likewise, the pipe*(string) option can be used for pre-processing predictors within
Python on the ﬂy, where ‘*’ is a placeholder for ‘1’ to ‘10’ (see Section 3.7). Finally,
xvars*(predictors) allows specifying a learner-speciﬁc variable lists of predictors.

pystacked depvar predictors (cid:2) if (cid:3) (cid:2) in (cid:3) (cid:2) , methods(string) cmdopt1(string)
cmdopt2(string) ... cmdopt10(string) pipe1(string) pipe2(string) ...

pipe10(string) xvars1(predictors) xvars2(predictors) ...
xvars10(predictors) general options (cid:3)

where general options are discussed in Section 3.4 below.

6

3.2 Syntax 2

pystacked

The second syntax is more ﬂexible in that it imposes no limit on the number of base
learners (aside from the increasing computational complexity). Base learners are added
before the comma using method(string) along with further learner-speciﬁc settings and
separated by ‘||’.

pystacked depvar (cid:2) indepvars (cid:3) || method(string) opt(string) pipe(string)
xvars(predictors) (cid:2) || method(string) opt(string) pipe(string)
xvars(predictors) ...

|| (cid:3) (cid:2) if (cid:3) (cid:2) in (cid:3) (cid:2) , general options (cid:3)

3.3 Post-estimation programs

Predicted values. To get predicted values:

predict type newname (cid:2) if (cid:3) (cid:2) in (cid:3) (cid:2) , pr xb (cid:3)

To get ﬁtted values for each base learner:

predict type stub (cid:2) if (cid:3) (cid:2) in (cid:3) (cid:2) , transform (cid:3)

Predicted values (in- and out-of-sample) are calculated when pystacked is run and
stored in Python memory. predict pulls the predicted values from Python memory
and saves them in Stata memory. This storage structure means that no changes on the
data in Stata memory should be made between the pystacked call and the predict
call. If changes to the data set are made, predict will return an error.

Tables. After estimation, pystacked can report a table of in-sample and, optionally,
out-of-sample (or holdout sample) performance for both the stacking regression and the
base learners. For regression problems, the table reports the mean-squared prediction
error (MSPE). For classiﬁcation problems, a confusion matrix is reported. The default
holdout sample used for out-of-sample performance with the holdout option is all obser-
vations not included in the estimation. Alternatively, the user can specify the holdout
sample explicitly using the syntax holdout(varname). The table can be requested after
estimation as a replay command or as part of the pystacked estimation.

pystacked (cid:2) , table holdout[(varname)] (cid:3)

Graphs. pystacked can also create graphs of in-sample and, optionally, out-of-sample
performance for both the stacking regression and the base learners. For regression
problems, the graphs compare predicted and actual values of depvar. For classiﬁcation
problems, the default is to generate receiver operator characteristic (ROC) curves; op-
tionally, histograms of predicted probabilities are reported. As with the table option,

Ahrens, Hansen & Schaﬀer

7

the default holdout sample used for out-of-sample performance is all observations not
included in the estimation, but the user can instead specify the holdout sample explic-
itly. The table can be requested after estimation or as part of the pystacked estimation
command. The graph option on its own reports the graphs using pystacked’s default
settings. Because graphs are produced using Stata’s twoway, roctab and histogram
commands, the user can control either the combined graph (graph(options)) or the
individual learner graphs (lgraph(options)) by passing options to these commands.

pystacked (cid:2) , graph[(options)] lgraph[(options)] histogram

holdout[(varname)] (cid:3)

3.4 General options

A full list of general options is provided in the pystacked help ﬁle. We list only the
most important general options here:

type(string) allows reg(ress) for regression problems or class(ify) for classiﬁcation prob-

lems. The default is regression.

finalest(string) selects the ﬁnal estimator used to combine base learners. The default
is non-negative least squares without an intercept and the additional constraint
that weights sum to 1 (nnls1). Alternatives are nnls0 (non-negative least squares
without intercept without the sum-to-one constraint), singlebest (use base learner
with minimum MSE), ols (ordinary least squares) or ridge for (logistic) ridge,
which is the scikit-learn default.

folds(integer ) speciﬁes number of folds used for cross-validation. The default is 5.
pyseed(integer ) sets the Python seed. Note that since pystacked uses Python, we
also need to set the Python seed to ensure replicability. There are three options:
1. pyseed(-1) draws a number between 0 and 108 in Stata which is then used
as a Python seed. This way, you only need to deal with the Stata seed. For
example, set seed 42 is suﬃcient, as the Python seed is generated auto-
matically.

2. Setting pyseed(x) with any positive integer x allows to control the Python

seed directly.

3. pyseed(0) sets the seed to None in Python.

The default is pyseed(-1).

njobs(integer ) sets the number of jobs for parallel computing. The default is 0 (no

parallelization), -1 uses all available CPUs, -2 uses all CPUs minus 1.

voting selects voting regression or classiﬁcation which uses pre-speciﬁed weights. By
default, the voting regressor uses equal weights; voting classiﬁer uses a majority
rule.

voteweights(numlist) deﬁnes positive weights used for voting regression or classiﬁca-
tion. The length of numlist should be the number of base learners - 1. The last
weight is calculated to ensure that the sum of weights equal 1.

8

pystacked

sparse converts predictor matrix to a sparse matrix. This conversion will only lead to
speed improvements if the predictor matrix is suﬃciently sparse. Not all learners
support sparse matrices and not all learners will beneﬁt from sparse matrices in
the same way. You can also use the sparse pipeline to use sparse matrices for some
learners, but not for others.

3.5 Base learners

The base learners are chosen using the option method(string) in combination with
type(string). The latter can take the value reg(ress) for regression and class for clas-
siﬁcation problems. Table 1 provides an overview of supported base learners and their
underlying scikit-learn routines.

cmdopt*(string) (Syntax 1) and opt(string) (Syntax 2) are used to pass options to
the base learners. Due the large number of options, we do not list all options here. We
instead provide a tool that lists options for each base learner. For example, to get the
default options for lasso with cross-validated penalty, type

. pystacked, type(reg) methods(lassocv) printopt
Default options:
alphas(None) l1_ratio(1) eps(.001) n_alphas(100) fit_intercept(True)
max_iter(1000) tol(.0001) cv(5) n_jobs(None) positive(False) selection(cyclic)
random_state(rng)

The naming of the options follows scikit-learn. Allowed settings for each option can
be inferred from the scikit-learn documentation. We strongly recommend that the user
reads the scikit-learn documentation carefully.4

3.6 Learner-speciﬁc predictors

By default, pystacked uses the same set of predictors for each base learner. Using the
same predictors for each method is often not desirable as the optimal set of predictors
may vary across base learners. For example, when using linear machine learners such as
the lasso, adding polynomials, interactions and other transformations of the base set of
predictors might greatly improve out-of-sample prediction performance. The inclusion
of transformations of base predictors is especially worth considering if the base set of
observed predictors is small (relative to the sample size) and the relationship between
outcome and predictors is likely non-linear. Tree-based methods (e.g. random forests
and boosted trees), on the other hand, can detect certain types of non-linear patterns
automatically. While adding transformations of the base predictors may still lead to
performance gains, the added beneﬁt is less striking relative to linear learners and often
will not outweigh the costs in terms of computational complexity.

There are two approaches to implement learner-speciﬁc sets of predictors: Pipelines,

4. The scikit-learn documentation is available at https://scikit-learn.org/stable/index.html (last ac-

cessed on August 18, 2022).

Ahrens, Hansen & Schaﬀer

9

method()
ols
logit
lassoic
lassocv

ridgecv

elasticcv

svm

gradboost

rf

linsvm
nnet

type() Machine learner description
Linear regression
regress
Logistic regression
class
Lasso with AIC/BIC penalty
regress
Lasso with CV penalty
regress
Logistic lasso with CV penalty
class
Ridge with CV penalty
regress
Logistic ridge with CV penalty
class
Elastic net with CV penalty
regress
Logistic elastic net with CV
class
Support vector regression
regress
Support vector classiﬁcation
class
regress Gradient boosting regression
Gradient boosting classiﬁcation
class
Random forest regression
regress
Random forest classiﬁcation
class
Linear SVC
class
regress Neural net regression
class

Neural net classiﬁcation

scikit-learn program
linear model.LinearRegression
linear model.LogisticRegression
linear model.LassoLarsIC
linear model.ElasticNetCV
linear model.LogisticRegressionCV
linear model.ElasticNetCV
linear model.LogisticRegressionCV
linear model.ElasticNetCV
linear model.LogisticRegressionCV
svm.SVR
svm.SVC
ensemble.GradientBoostingRegressor
ensemble.GradientBoostingClassifier
ensemble.RandomForestRegressor
ensemble.RandomForestClassifier
svm.LinearSVC
sklearn.neural network.MLPRegressor
sklearn.neural network.MLPClassifier

Note: The ﬁrst two columns list all allowed combinations of method(string) and type(string), which
are used to select base learners. Column 3 provides a description of each machine learner. The last
‘CV penalty’ indicates that the penalty level
column lists the underlying scikit-learn learn routine.
is chosen to minimize the cross-validated MSPE. ‘AIC/BIC penalty’ indicates that the penalty level
minimizes either either the Akaike or Bayesian information criterion. SVC refers to support vector
classiﬁcation.

Table 1: Overview of machine learners available in pystacked.

discussed in the next section, can be used to create some transformations on the ﬂy
for speciﬁc base learners. A more ﬂexible approach is the xvars*(predictors) option,
which allows to specify predictors for a speciﬁc learner. xvars*() supports standard
Stata factor variable notation.

3.7 Pipelines

scikit-learn uses pipelines to pre-preprocess input data on the ﬂy.
In pystacked,
pipelines can be used to impute missing values, create polynomials and interactions,
and to standardize predictors. Table 2 lists the pipelines currently supported pipelines
by pystacked.

Remarks. First, regularized regressors (i.e., the methods lassoic lassocv ridgecv elas-
ticcv) use the stdscaler pipeline by default. pipe*(nostdscaler) disables this behavior.
Second, the stdscaler0 pipeline is useful in combination with sparse which transforms
the predictor matrix into a sparse matrix. stdscaler0 does not center predictors, so that
the predictor matrix retains its sparsity property.

10

pipe*()
stdscaler

nostdscaler

stdscaler0
sparse
onehot

Description
Standardize to mean zero and unit
variance (default for regularized
regression)
Overwrites default standardization for
regularized regression
Standardize to unit variance
Transform to sparse matrix
Create dummies from categorical
variables
Scale to 0-1 range

minmaxscaler
medianimputer Median imputation
knnimputer
poly2
poly3
interact

KNN imputation
Add 2nd-order polynomials
Add 3rd-order polynomials
Add interactions

pystacked

scikit-learn programs
StandardScaler()

n/a

StandardScaler(with mean=False)
SparseTransformer()
OneHotEncoder()

MinMaxScaler()
SimpleImputer(strategy=’median’)
KNNImputer()
PolynomialFeatures(degree=2)
PolynomialFeatures(degree=3)
PolynomialFeatures(

include bias=False,
interaction only=True)

Table 2: Pipelines supported by pystacked.

4 Applications

This section demonstrates how to apply pystacked for regression and classiﬁcation
tasks. Before we discuss stacking, we ﬁrst show how to use pystacked as a ‘regular’
machine learning program for ﬁtting a single learner (see next sub-section). We then
illustrate stacking regression and stacking classiﬁcation in Section 4.2 and 4.3, respec-
tively.

4.1 Single base learner

We import the California house price data from Pace and Barry (1997), and split the
sample randomly into training and validation partition using a 75/25 split. The aim of
the prediction task is to predict median house prices (medhousevalue) using a set of
house price characteristics.5 We prepare the data for analysis as follows.

. clear all
. use https://statalasso.github.io/dta/cal_housing.dta, clear
. set seed 42

. gen train=runiform()
. replace train=train<.75
(20,640 real changes made)
. replace medh = medh/10e3

5. The following predictors are included in the data set in this order:

district longitude
(longitude),
latitude (latitude), median house age (houseage), average number of rooms
per household (rooms), average number of bedrooms per household (bedrooms), block group
population (population), average number of household members (households), median in-
come in block group (medinc). The data set was retrieved from the StatLib repository at
https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html (last accessed on February
19, 2022). There are 20,640 observations in the data set.

Ahrens, Hansen & Schaﬀer

11

variable medhousevalue was long now double
(20,640 real changes made)

. label var medh

Gradient boosting. As a ﬁrst example, we use pystacked to ﬁt gradient boosted regres-
sion trees and save the out-of-sample predicted values. Since we consider a regression
task rather than a classiﬁcation task, we specify type(reg) (which is also the default).
The option method(gradboost) selects gradient boosting. We will later see that we can
specify more than one learner in methods(), and that we can also ﬁt gradient boosted
classiﬁcation trees.

. pystacked medh longi-medi if train, type(reg) methods(gradboost)
Single base learner: no stacking done.
Stacking weights:

Method

Weight

gradboost

1.0000000

. predict double yhat_gb1 if !train

The output shows the stacking weights associated with each base learner. Since we
only consider one method, the output is not particularly informative and simply shows
a weight of one for gradient boosting. Yet, pystacked has ﬁtted 100 boosted trees (the
default) in the background using scikit-learn’s ensemble.GradientBoostedRegressor.

Before we tune our gradient boosting learner, we retrieve a list of available options.
The default options for gradient boosting can be listed in the console, and the scikit-
learn documentation provides more detail on the allowed parameters of each option.

. pystacked, type(reg) methods(gradboost) print
Default options:
loss(squared_error) learning_rate(.1) n_estimators(100) subsample(1)
criterion(friedman_mse) min_samples_split(2) min_samples_leaf(1)
min_weight_fraction_leaf(0) max_depth(3) min_impurity_decrease(0) init(None)
random_state(rng) max_features(None) alpha(.9) max_leaf_nodes(None)
warm_start(False) validation_fraction(.1) n_iter_no_change(None)
tol(.0001) ccp_alpha(0)

We consider two additional speciﬁcations. Note that we restrict ourselves to a few
selected speciﬁcations for illustrative purposes. We stress that careful parameter tuning
should consider a grid of values across multiple learner parameters. The ﬁrst speciﬁca-
tion reduces the learning rate from 0.1 (the default) to 0.01. The second speciﬁcation
reduces the learning rate and increase the number of trees from 100 (the default) to
1000. We use cmdopt1() since gradient boosting is the ﬁrst (and only) method listed
in methods().

. pystacked medh longi-medi if train,
>
>

type(regress) methods(gradboost)
cmdopt1(learning_rate(.01))

///
///

12

pystacked

Single base learner: no stacking done.
Stacking weights:

Method

Weight

gradboost

1.0000000

. predict double yhat_gb2 if !train
. pystacked medh longi-medi if train,
>
>
Single base learner: no stacking done.
Stacking weights:

type(regress) methods(gradboost)
cmdopt1(learning_rate(.01) n_estimators(1000))

///
///

Method

Weight

gradboost

1.0000000

. predict double yhat_gb3 if !train

We can then compare the performance across the three models using the out-of-

sample mean-squared prediction error (MSPE):

. gen double res_gb1_sq=(medh-yhat_gb1)^2 if !train
(15,448 missing values generated)
. gen double res_gb2_sq=(medh-yhat_gb2)^2 if !train
(15,448 missing values generated)
. gen double res_gb3_sq=(medh-yhat_gb3)^2 if !train
(15,448 missing values generated)
. sum res_gb* if !train

Variable

Obs

Mean

Std. dev.

Min

Max

res_gb1_sq
res_gb2_sq
res_gb3_sq

5,192
5,192
5,192

29.86727
71.10136
30.37472

83.17099
129.1446
82.90974

3.92e-06
4.56e-07
1.24e-06

1424.438
1230.348
1384.924

The initial gradient booster achieves an out-of-sample MSPE of 29.87. The second
gradient booster uses a reduced learning rate of 0.01 and performs much worse, with a
MSPE of 71.10. The third gradient booster performs only slightly worse than the ﬁrst,
illustrating the trade-oﬀ between learning rate and number of trees.

Pipelines. We can make use of pipelines to pre-process our predictors. This is especially
useful in the context of stacking when we want to, for example, use second-order poly-
nomials of predictors as inputs for one method, but only use elementary predictors for
another methods. Here, we compare lasso with and without the poly2 pipeline:

. pystacked medh longi-medi if train, type(reg) methods(lassocv)
Single base learner: no stacking done.
Stacking weights:

Method

Weight

Ahrens, Hansen & Schaﬀer

13

lassocv

1.0000000

. predict double yhat_lasso1 if !train
.
. pystacked medh longi-medi if train, type(reg) methods(lassocv) ///
>
Single base learner: no stacking done.

pipe1(poly2)

Stacking weights:

Method

lassocv

Weight

1.0000000

. predict double yhat_lasso2 if !train

We could replace pipe1(poly2) with xvars1(c.(medh longi-medi)##c.(medh longi
In fact, the later is more ﬂexible and allows, for example, to create inter-

-medi)).
actions for some predictors and not for others.

We again calculate the out-of-sample MSPE:

. gen double res_lasso1_sq=(medh-yhat_lasso1)^2 if !train
(15,448 missing values generated)
. gen double res_lasso2_sq=(medh-yhat_lasso2)^2 if !train
(15,448 missing values generated)
. sum res_lasso1_sq res_lasso2_sq if !train

Variable

Obs

Mean

Std. dev.

Min

Max

res_lasso1~q
res_lasso2~q

5,192
5,192

47.00385
43.0813

108.0759
116.0953

8.43e-07
3.46e-07

2392.572
2972.798

The poly2 pipeline improves the performance of the lasso, indicating that squared
and interaction terms constitute important predictors. However, the lasso does not
perform as well as gradient boosting in this application.

4.2 Stacking regression

We now consider a stacking regression application with ﬁve base learners: (1) linear
regression, (2) lasso with penalty chosen by cross-validation, (3) lasso with second order
polynomials and interactions, (4) random forest with default settings and (5) gradient
boosting with a learning rate of 0.01 and 1000 trees. That is, we use the lasso twice—
once with and once without the poly2 pipeline. Indeed, nothing keeps us from using
the same algorithm multiple times. This way, we can combine the same algorithm with
diﬀerent settings.

Note the numbering of the pipe*() and cmdopt*() options in the code below. We
apply the poly2 pipe to the ﬁrst and third method (ols and lassoic). We also change the
default learning rate and number of estimators for gradient boosting (the 5th estimator).

. set seed 42

14

pystacked

. pystacked medh longi-medi if train,
>
>
>
>

type(regress)
methods(ols lassocv lassocv rf gradboost)
pipe3(poly2) cmdopt5(learning_rate(0.01)
n_estimators(1000))

Stacking weights:

///
///
///
///

Method

ols
lassocv
lassocv
rf
gradboost

Weight

0.0000000
0.0000000
0.4692388
0.2505079
0.2802533

The above syntax becomes, admittedly, a bit diﬃcult to read, especially with many
methods and many options. We oﬀer an alternative syntax for easier use with many
base learners. Another advantage of this alternative syntax is that it does not impose
a limit on the number of base learners, whereas the previous syntax only supports up
to 10 base learners.

. set seed 42

. pystacked medh longi-medi
>
>
>
>
>
>
Stacking weights:

m(ols)
m(lassocv)
m(lassocv) pipe(poly2)
m(rf)
m(gradboost) opt(learning_rate(0.01) n_estimators(1000))
if train, type(regress)

|| ///
|| ///
|| ///
|| ///
|| ///
///

Method

ols
lassocv
lassocv
rf
gradboost

Weight

0.0000000
0.0000000
0.4692388
0.2505079
0.2802533

The stacking weights shown in the output determine how much each method con-
tributes to the ﬁnal stacking contribution. The lasso only using the base predictors
received a weight of zero, while the lasso with the poly2 gets a weight of 0.47. Gradi-
ent boosting and random forest contribute together a little more than half to the ﬁnal
predictor.

Predicted values. In addition to the stacking predicted values, we can also get the
predicted values of each base learner using the transform option:

. predict double yhat, xb
. predict double ytrans, transf
. list yhat ytrans* if _n <= 5

Ahrens, Hansen & Schaﬀer

15

yhat

ytrans1

ytrans2

ytrans3

ytrans4

ytrans5

1.
2.
3.
4.
5.

41.35619
42.880349
39.221593
33.686226
25.253782

41.315834
41.45306
38.212036
32.332498
25.382839

41.24048
41.434102
38.176811
32.291791
25.374455

40.36963
46.420851
39.068403
33.645113
25.608601

43.16083
38.289107
41.268902
33.4869
23.8597

41.394926
41.056291
37.648071
33.933232
25.905815

Plotting. pystacked also comes with plotting features. The graph option creates a
scatter plot of predicted values on the vertical and observed values on the horizontal
axis for stacking and each base learner, see Figure 1. The red line is a 45-degree line
and shown for reference. There is no need to re-run the stacking estimation. You can
use pystacked with graph as a post-estimation command.

. pystacked, graph holdout
Number of holdout observations: 5192

Figure 1 shows the out-of-sample predicted values. To see the in-sample predicted
values, simply omit the holdout option. Note that the holdout option will not work if
the estimation was run on the whole sample.

MSPE table. The table option allows to compare stacking weights with in-sample and
out-of-sample MSPE. As with the graph option, we can use table as a post-estimation
command:

. pystacked, table holdout
Number of holdout observations: 5192
MSPE: In-Sample and Out-of-Sample

Method

Weight

In-Sample

Out-of-Sample

STACKING
ols
lassocv
lassocv
rf
gradboost

.

0.000
0.000
0.469
0.251
0.280

4.795
6.986
6.986
6.613
1.847
5.312

5.473
6.853
6.855
6.564
4.963
5.511

4.3 Stacking classiﬁcation

pystacked can be applied to binary classiﬁcation problems. For demonstration, we
consider the Spambase Data Set of Cranor and LaMacchia (1998) which we retrieve
from the UCI Machine Learning Repository. We load the data and split the data into
training (75%) and validation sample (%25).

16

pystacked

Figure 1: Out-of-sample predicted values and observed values created using the graph
option after stacking regression.

020406001020304050medhousevalueSTACKING02040608001020304050medhousevalueweight = 0.000Learner: ols02040608001020304050medhousevalueweight = 0.000Learner: lassocv-40-20020406001020304050medhousevalueweight = 0.469Learner: lassocv0102030405001020304050medhousevalueweight = 0.251Learner: rf020406001020304050medhousevalueweight = 0.280Learner: gradboostOut-of-sample PredictionsAhrens, Hansen & Schaﬀer

17

. insheet using ///
> https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
> clear comma
(58 vars, 4,601 obs)
. set seed 42
. gen train=runiform()

. replace train=train<.75
(4,601 real changes made)

The example below is more complicated. We go through it step-by-step:

• We use ﬁve base learners: logistic regression, two gradient boosters and two neural

nets.

• We apply the poly2 pipeline to the logistic regression, which creates squares and

interaction terms of the predictors, but not to other methods.

• We employ gradient boosting with 600 and 1000 classiﬁcation trees.
• We consider two speciﬁcations for the neural nets: one neural net with two hidden
layers of 5 nodes each, and another neural net with a single hidden layer of 5 nodes.
• Finally, we use type(class) to specify that we consider a classiﬁcation task and

njobs(8) switches parallelization on utilizing 8 cores.

. pystacked v58 v1-v57
>
>
>
>
>
>
Stacking weights:

m(logit) pipe(poly2)
m(gradboost) opt(n_estimators(600))
m(gradboost) opt(n_estimators(1000))
m(nnet) opt(hidden_layer_sizes(5 5))
m(nnet) opt(hidden_layer_sizes(5))
if train, type(class) njobs(8)

|| ///
|| ///
|| ///
|| ///
|| ///
|| ///

Method

logit
gradboost
gradboost
nnet
nnet

Weight

0.0024093
0.3704480
0.3460850
0.0579345
0.2231232

As in the previous regression example, gradient boosting receives the largest stacking

weight and thus contributes most to the ﬁnal stacking prediction.

Confusion matrix. Confusion matrices allow comparing actual and predicted outcomes
in a 2 × 2 matrix. pystacked provides a compact table format that combines confusion
matrices for each base learner and for the ﬁnal stacking classiﬁer, both for the training
and validation partition.

. pystacked, table holdout
Number of holdout observations: 1133
Confusion matrix: In-Sample and Out-of-Sample

18

pystacked

Method

Weight

In-Sample
1
0

STACKING
STACKING
logit
logit
gradboost
gradboost
gradboost
gradboost
nnet
nnet
nnet
nnet

0
1
0
1
0
1
0
1
0
1
0
1

.
.

0.002
0.002
0.370
0.370
0.346
0.346
0.058
0.058
0.223
0.223

2079
2
1266
815
2078
3
2079
2
1834
247
2005
76

6
1381
88
1299
9
1378
1
1386
55
1332
146
1241

Out-of-Sample

0

681
26
413
294
679
28
680
27
617
90
671
36

1

29
397
32
394
30
396
29
397
28
398
51
375

For example, the table shows that 25 false positives for stacking and 294 for lo-
gistic regression in the validation partition, while the number of false negatives is
29 and 32, respectively. The accuracy of stacking and logistic regression is given by
(682+397)/1133=95.2% and (413+394)/1133=71.2%.

Plotting. pystacked supports ROC curves which allow to assess the classiﬁcation per-
formance for varying discrimination thresholds. The y-axis in an ROC plot corresponds
to sensitivity (true positive rate) and the x-axis corresponds to 1-speciﬁcity (false pos-
itive rate). The Area Under the Curve (AUC) displayed below each ROC plot is a
common evaluation metric for classiﬁcation problems.

. pystacked, graph(subtitle(Spam data))
lgraph(plotopts(msymbol(i)
>
ylabel(0 1, format(%3.1f))))
>
>
holdout
Number of holdout observations: 1133

///
///
///

Ahrens, Hansen & Schaﬀer

19

Figure 2: Out-of-sample ROC curves created using the graph option after stacking
regression.

5 Acknowledgments

We thank Jan Ditzen and Thomas Wiemann for testing an early version of the program.
All remaining errors are our own.

6 References
Ahrens, A., C. B. Hansen, and M. E. Schaﬀer. 2020.

lassopack: Model selection and
prediction with regularized regression in Stata. The Stata Journal 20(1): 176–235.
https://doi.org/10.1177/1536867X20909697.

. 2022. ddml: Stata package for Double Debiased Machine Learning.

Athey,

S.,

and G. W.

economists should know about.
725.
https://doi.org/10.1146/annurev-economics-080217-053433.

that
685–
https://doi.org/10.1146/annurev-economics-080217-053433.

learning methods
Annual Review of Economics 11(1):

Imbens. 2019.

Tex.eprint:

Machine

Breiman, L. 1996.

Stacked regressions.

Machine Learning 24(1):

49–64.

http://link.springer.com/10.1007/BF00117832.

0.01.0Sensitivity0.000.250.500.751.001 - speciﬁcityArea under ROC curve = 0.9967STACKING0.01.0Sensitivity0.000.250.500.751.001 - speciﬁcityArea under ROC curve = 0.9133Learner: logit0.01.0Sensitivity0.000.250.500.751.001 - speciﬁcityArea under ROC curve = 0.9968Learner: gradboost0.01.0Sensitivity0.000.250.500.751.001 - speciﬁcityArea under ROC curve = 0.9973Learner: gradboost0.01.0Sensitivity0.000.250.500.751.001 - speciﬁcityArea under ROC curve = 0.9562Learner: nnet0.01.0Sensitivity0.000.250.500.751.001 - speciﬁcityArea under ROC curve = 0.9763Learner: nnetSpam dataOut-of-sample ROC20

pystacked

Buitinck, L., G. Louppe, M. Blondel, F. Pedregosa, A. Mueller, O. Grisel, V. Niculae,
P. Prettenhofer, A. Gramfort, J. Grobler, R. Layton, J. VanderPlas, A. Joly, B. Holt,
and G. Varoquaux. 2013. API design for machine learning software: experiences from
the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and
Machine Learning, 108–122.

Cerulli, G. 2021. Machine Learning using Stata/Python.

Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duﬂo, C. Hansen, W. Newey,
treatment
C1–C68.
John Wiley & Sons, Ltd (10.1111).

and J. Robins. 2018.
and structural parameters.
Tex.ids= Chernozhukov2018a publisher:
https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097.

learning for
The Econometrics Journal 21(1):

Double/debiased machine

Cranor, L. F., and B. A. LaMacchia. 1998. Spam! Communications of the ACM 41(8):

74–83.

Droste, M. 2020. pylearn. https://github.com/NickCH-K/MLRtime/. [Online; accessed

02-December-2021].

Guenther, N. 2016. Support vector machines. Stata Journal 16(4): 917–937(21).

www.stata-journal.com/article.html?article=st0461.

Hastie, T., R. Tibshirani, and J. Friedman. 2009. The Elements of Statistical Learning.

2nd ed. New York: Springer-Verlag.

Huntington-Klein, N. C. 2021. mlrtime. https://github.com/mdroste/stata-pylearn/.

[Online; accessed 02-December-2021].

Kingma, D. P., and J. Ba. 2014. Adam: A method for stochastic optimization. arXiv

preprint arXiv:1412.6980 .

Pace,

R. K.,

gressions.
https://www.sciencedirect.com/science/article/pii/S016771529600140X.

Letters

33(3):

&

and
Statistics

R.

Barry.

1997.
Probability

Sparse

spatial

autore-
291–297.

Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in
Python. Journal of Machine Learning Research 12: 2825–2830.

Schonlau, M., and R. Y. Zou. 2020. The random forest algorithm for statistical learning.

The Stata Journal 20(1): 3–29. https://doi.org/10.1177/1536867X20909688.

van der Laan, M. J., E. C. Polley, and A. E. Hubbard. 2007. Super Learner. Statistical
Applications in Genetics and Molecular Biology 6(1). https://doi.org/10.2202/1544-
6115.1309.

Wolpert, D. H. 1992.

Stacked generalization. Neural Networks 5(2): 241–259.

https://www.sciencedirect.com/science/article/pii/S0893608005800231.

Ahrens, Hansen & Schaﬀer

About the authors

21

Achim Ahrens is Post-Doctoral Researcher and Senior Data Scientist at the Public Policy
Group and Immigration Policy Lab, ETH Z¨urich.

Christian B. Hansen is the Wallace W. Booth Professor of Econometrics and Statistics at the
University of Chicago Booth School of Business.

Mark E. Schaﬀer is Professor of Econonomics in the School of Social Sciences at Heriot-Watt
University, Edinburgh, UK, and a Research Fellow at the Institute for the Study of Labour
(IZA), Bonn.

