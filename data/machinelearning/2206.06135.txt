Flexible Differentiable Optimization via Model
Transformations

Akshay Sharma
Columbia University, NY, USA

Mathieu Besançon
Zuse Institute Berlin, Germany

Joaquim Dias Garcia
PSR & PUC-Rio, Rio de Janeiro, Brazil

Benoît Legat
Massachussets Institute of Technology, MA, USA

akshay.s@columbia.edu

besancon@zib.de

joaqim@psr-inc.com

blegat@mit.edu

Abstract
We introduce DiffOpt.jl, a Julia library to differentiate through the solution of convex op-
timization problems with respect to arbitrary parameters present in the objective and/or
constraints. The library builds upon MathOptInterface, thus leveraging the rich ecosys-
tem of solvers and composing well with modelling languages like JuMP. DiffOpt offers
both forward and reverse differentiation modes, enabling multiple use cases from hyper-
parameter optimization to backpropagation and sensitivity analysis, bridging constrained
optimization with end-to-end differentiable programming.

1. Introduction

Differentiable Optimization (𝜕) has become a center of interest in the last years, both as
a stand-alone methodology providing additional information on the sensitivity of general
convex optimization problems and as a principled way to integrate complex convex op-
timization components in Machine Learning, following a broader trend of differentiable
programming which extends the set of computer programs for which derivatives can be
computed [Innes et al., 2019]. We present DiffOpt, a library implementing key methods
for 𝜕 in Julia [Bezanson et al., 2017] based on the MathOptInterface (MOI) abstract
data structure [Legat et al., 2021]. Consequently, the extensions to the usual mathemat-
ical optimization machinery are also available in JuMP [Dunning et al., 2017] with MOI
as its backend.

The rest of this paper is structured as follows. Section 2 covers the background and
related research on 𝜕. Section 3 presents the structure of the DiffOpt package and impor-
tant features. Section 4 highlights some applications illustrating the use of the package.

2
2
0
2

n
u
J

0
1

]

G
L
.
s
c
[

1
v
5
3
1
6
0
.
6
0
2
2
:
v
i
X
r
a

1

 
 
 
 
 
 
2. Differentiating Convex Optimization Problems

𝜕 regroups a set of methods to compute derivative information of a function including
an optimization problem. If we consider a generic optimization problem parameterized
by (𝜃0, 𝜃1, … , 𝜃𝑚):

 ∶ min

𝐹 (𝑥, 𝜃0)

𝑥
s.t. 𝐺𝑖(𝑥, 𝜃𝑖) ∈ 𝑖 ∀𝑖 ∈ {1, … , 𝑚}.

Differentiable optimization in its generic form entails computing the derivative of the
 with respect to all parameters, i.e., computing the Jacobian matrix:

output 𝑥 ∗ ∈ argmin𝑥

𝜕𝑥 ∗
𝜕𝜃

≡

(

𝜕𝑥 ∗
𝑗
𝜕𝜃𝑖 )

.

𝑖𝑗

Instead of the Jacobian matrix of the solution map, 𝜓 (𝜃) = 𝑥 ∗, we often reason on the the
derivative of the solution map as a linear map denoted as 𝐷𝜓 (𝜃), with adjoint 𝐷𝑇 𝜓 (𝑥).
In general, the solution map is not smooth, it is even a set-valued map and not a simple
function from parameters to solution. Some lines of work have investigated non-smooth
automatic (sub)differentiation and the behaviour of automatic differentiation in the non-
smooth case [Kakade and Lee, 2018, Bolte and Pauwels, 2020]. However, in most cases
in differentiable programming, a heuristic quantity is computed when the limit of the
directional derivative is ill-defined1. The recent survey [Kotary et al., 2021] on learning
with constrained optimization offers a review of 𝜕 techniques. We briefly go over the
lines of work that are closest to the methods implemented in DiffOpt.jl.

A part of the recent work on 𝜕 has focused on specific optimization problems. In
[Blondel et al., 2020], the authors consider a differentiable optimization method for sort-
ing and ranking problems. In [Berthet et al., 2020], the differentiation of the solution to
a generic convex optimization problem with respect to a linear objective is considered.
When the feasible set is a polytope, the issue of a Jacobian matrix being zero almost every-
where arises similarly to the ranking problem from [Blondel et al., 2020]. The approach
followed is that of perturbed optimization, considering the input cost vector as a random
variable centered around a nominal value. This allows sampling the output solution and
Jacobian matrix from input cost vectors generated from the distribution, yielding unbi-
ased estimators for both while only requiring access to a linear minimization oracle i.e.
duality information is not required.

For differentiation with respect to arbitrary problem parameters, two main meth-
ods have recently appeared in the literature. The first method, from [Amos and Kolter,
2019], can be applied to convex quadratic optimization problems while the second, from
[Agrawal et al., 2020], is applied to conic optimization problems. Both methods are based
on rewriting necessary optimality conditions for the optimization problem as a system
of non-linear equations and then applying the Implicit Function Theorem [Dontchev and
Rockafellar, 2009].

1. See the ChainRules.jl recommendations https://juliadiff.org/ChainRulesCore.jl/

v1.14/maths/nondiff_points.html, accessed April 2022

2

A major step towards integrating differentiable optimization into ML pipelines was
presented in [Amos and Kolter, 2019, Amos, 2019] for convex quadratic problems (QP).
The considered optimization models are of the form:

𝑥 𝑇 𝑄𝑥 + 𝑐𝑇 𝑥

min
𝑥∈𝑅𝑛

𝐺𝑥 ≤ ℎ ∶ (𝜆)

𝐴𝑥 = 𝑏 ∶ (𝜇)

where 𝜆 ∈ 𝑅𝑝, 𝜇 ∈ 𝑅𝑚 denote the dual variables associated with the inequality, equal-
ity constraints respectively. Unlike prior work, the solution map is differentiated with
respect to all problem data (𝐴 ∈ 𝑅𝑚×𝑛, 𝑏 ∈ 𝑅𝑚, 𝐺 ∈ 𝑅𝑝×𝑛, ℎ ∈ 𝑅𝑝, 𝑄 ∈ 𝑅𝑛×𝑛, 𝑐 ∈ 𝑅𝑛).
In particular, differentiating the solution with respect to constraint coefficients opens
new applications including learning the constraints of the convex problem along with
solutions as illustrated in [Amos, 2019] on generic polytopes and Sudoku problems. The
solution method consists in representing the solution process as solving a system of equa-
tions. The solution can then be differentiated with respect to its parameters using implicit
differentiation. In the case of QPs, the KKT conditions of the system fully describe the
optimality conditions for a primal-dual solution:

𝑄𝑥 + 𝑐 + 𝐴𝑇 𝜇 + 𝐺𝑇 𝜆 = 0
𝐴𝑥 = 𝑏

0 ≤ ℎ − 𝐺𝑥 ⊥ 𝜆 ≥ 0

(∇𝐿)
(𝑃𝑒𝑞)
(𝐶),

where (∇𝐿) represents the gradient of the Lagrangian, (𝑃𝑒𝑞) represents the primal feasi-
bility of equality constraints and (𝐶) includes primal feasibility of inequality constraints,
complementarity and dual feasibility for inequalities. A system of necessary equality
constraints can be derived:

(∇𝐿), (𝑃𝑒𝑞),
𝜆𝑖(ℎ − 𝐺𝑥)𝑖 = 0 ∀𝑖.

This representation can be implicitly differentiated:

𝑑𝑄𝑥 + 𝑄𝑑𝑥 + 𝑑𝑐 + 𝑑𝐴𝑇 𝜇 + 𝐴𝑇 𝑑𝜇 + 𝑑𝐺𝑇 𝜆 + 𝐺𝑇 𝑑𝜆 = 0
𝑑𝐴𝑥 + 𝐴𝑑𝑥 − 𝑑𝑏 = 0
𝑑𝜆𝑖(ℎ − 𝐺𝑥)𝑖 + 𝜆𝑖(𝑑ℎ − 𝑑𝐺 𝑥 − 𝐺 𝑑𝑥)𝑖 = 0 ∀𝑖.

Regrouping the differential forms of parameters and solution variables results in the fol-
lowing system:

𝐺𝑇

𝑄

⎡
⎢
𝐷(𝜆)𝐺 𝐷(ℎ − 𝐺𝑥)
⎢
⎣

𝐴

0

𝐴𝑇
0
0

⎤
⎥
⎥
⎦

𝑑𝑥
⎡
⎢
𝑑𝜆
⎢
𝑑𝜇
⎣

⎤
⎥
⎥
⎦

= −

𝑑𝑄 𝑥 + 𝑑𝑐 + 𝑑𝐺𝑇 𝜆 + 𝑑𝐴𝑇 𝜇
⎡
⎢
𝐷(𝜆)𝑑ℎ − 𝐷(𝜆)𝑑𝐺 𝑥
⎢
𝑑𝐴𝑥 − 𝑑𝑏
⎣

⎤
⎥
⎥
⎦

.

where 𝐷(⋅) denotes the diagonal matrix formed from a given vector. The system above
can be used to compute several quantities of interest. At a given point

(𝜃 = (𝑐, 𝑄, 𝐴, 𝐺, 𝑏, ℎ), 𝑥, 𝜆, 𝜇),

3

solving the system can let us derive the full Jacobian matrix. As pointed out in [Amos,
2019], the highly sparse structure of the matrix on the right-hand side can be exploited
to compute some partial derivatives first by selecting only some rows, which are then
back-substituted. However, in most applications where derivative information is needed,
we only need to perform Jacobian-vector products (JVP) or vector-transpose-Jacobian
products (VJP), thus exposing the two principal modes of differentiation present in dif-
ferentiable libraries or Automatic Differentiation (AD) tools: forward- and reverse-mode
AD.

Although QPs capture numerous problems of interest, many problems require a richer
set of constraints which can be modelled as conic constraints. Convex conic optimization
problems have standard closed-form primal and dual expressions,

Primal Problem

Dual Problem

min
𝑥∈𝑅𝑛

𝑐𝑇 𝑥

s.t. 𝐴𝑥 + 𝑠 = 𝑏
𝑠 ∈ 

𝑏𝑇 𝑦

min
𝑦∈𝑅𝑚
s.t. 𝐴𝑇 𝑦 + 𝑐 = 0
𝑦 ∈ ∗

where 𝑥 is the primal variable, 𝑦 is the dual variable, 𝑠 ∈ 𝑅𝑚 is the primal slack variable,
 ⊆ 𝑅𝑚 is a closed convex cone and ∗ ⊆ 𝑅𝑚 is the corresponding dual cone. Note that
in above form, 𝐴 ∈ 𝑅𝑚×𝑛, 𝑏 ∈ 𝑅𝑚, 𝑐 ∈ 𝑅𝑛 are problem data.

The methodology of 𝜕 has been developed for conic problems in [Amos, 2019] and
[Agrawal et al., 2020] in parallel and extended in [Agrawal et al., 2019]. The conic case
is more involved due to the conic constraints, we follow closely the notation of [Agrawal
Instead of
et al., 2020] for a complete and self-contained description of the method.
deriving the necessary system of equations from the KKT conditions, the proposed ap-
proach will rely on the Homogeneous Self-Dual Embedding (HSDE) [O’Donoghue et al.,
2016].
In this case the solution map from the problem data to the primal dual pair,
(𝑥, 𝑦, 𝑠) = 𝜓 (𝐴, 𝑏, 𝑐), is represented as a composition of three functions: 𝜓 = 𝜙◦◦.
The definition of each of these functions and their derivatives are given by:

1.  maps the problem data, (𝐴, 𝑏, 𝑐) to the a skew-symmetric matrix 𝑄 ∈ ℝ𝑛+𝑚+1:

𝑄 =

𝐴𝑇
0
−𝐴
0
−𝑐𝑇 −𝑏𝑇

⎡
⎢
⎢
⎣

𝑐
𝑏
0

⎤
⎥
⎥
⎦

, and its differential is: 𝑑 =

𝑑𝐴𝑇
0

0
−𝑑𝐴
−𝑑𝑐𝑇 −𝑑𝑏𝑇

⎡
⎢
⎢
⎣

𝑑𝑐
𝑑𝑏
0

⎤
⎥
⎥
⎦

2.  maps the 𝑄 matrix from the HSDE into its solution 𝑧 = (𝑢, 𝑣, 𝑤). In order to compute
the derivative of this function, the author [Agrawal et al., 2020] use the normalized residual
map, defined in [Busseti et al., 2019]:

 (𝑧, 𝑄) = ((𝑄 − 𝐼 )Π + 𝐼 )

𝑧
|𝑧𝑛+𝑚+1| )

,

(

where Π is the projection operator onto the Cartesian product ℝ𝑛 × ∗ × ℝ+. They key point
is that, given a matrix 𝑄, the solution 𝑧 of  (𝑧, 𝑄) = 0 is the solution of the HSDE. Hence,

4

it is possible to implicitly differentiate the equation  ((𝑄), 𝑄) = 0 and obtain:

𝐷(𝑄) = −(𝐷𝑧 ((𝑄), 𝑄))−1𝐷𝑄 ((𝑄), 𝑄),

where

𝐷𝑄 (𝑧, 𝑄) [𝑈 ] = 𝑈 Π(

𝑧
𝑧𝑛+𝑚+1

)

𝐷𝑧 (𝑧, 𝑄) =

((𝑄 − 𝐼 )𝐷Π(𝑧) + 𝐼 )
𝑧𝑛+𝑚+1

− sign(𝑧𝑛+𝑚+1)((𝑄 − 𝐼 )Π + 𝐼 )(

𝑧

𝑧2
𝑛+𝑚+1

)𝑒𝑇

𝑛+𝑚+1

where 𝐷𝑥 𝑓 (𝑥, 𝑦)[𝑣] denotes the directional derivative of 𝑓 at 𝑥 in direction 𝑣 and 𝑒𝑛+𝑚+1 the
basis vector with 1 at index 𝑛 + 𝑚 + 1. Note that the second term of 𝐷𝑧 (𝑧, 𝑄) vanishes if
𝑧 is a solution of the HSDE.

3. 𝜙 maps the solution of the HSDE to the primal-dual pair:

(𝑥, 𝑦, 𝑠) = 𝜙(𝑧) = 𝜙(𝑢, 𝑣, 𝑤) = (𝑢, Π∗(𝑣), Π∗(𝑣) − 𝑣)/𝑧𝑛+𝑚+1.

Its derivative is given by:

𝐷𝜙(𝑧) =

0
𝐷Π∗(𝑣)

𝐼
0
0 𝐷Π∗(𝑣) − 𝐼

⎡
⎢
⎢
⎣

−𝑥
−𝑦
𝑠

⎤
⎥
⎥
⎦

The derivatives of projections onto classic cones like the positive orthant, second-order ,
positive semidefinite, and exponential cones are given in [Busseti et al., 2019] and imple-
mented for DiffOpt in MathOptSetDistances.jl[Besançon et al., 2022]. Exponential
cone projections are performed using the technique from [Friberg, 2021].

The methodology of differentiable conic problems has been extended to log-log con-
vex problems in [Agrawal and Boyd, 2020], using the grammar from Disciplined Geomet-
ric Programming and the rules established for parameterized disciplined convex problems
in [Agrawal et al., 2019]. A differentiable method has been developed for submodular
functions in [Djolonga and Krause, 2018], opening 𝜕 to optimization problems with dis-
crete structures. In [Gould et al., 2019], deep learning models are studied with nonlinear
optimization problems as nodes instead of closed-form functions, defining a differentiable
optimization method for nonlinear problems without requiring convexity. The authors
leverage implicit differentiation of the Lagrangian reformulation at the optimal point to
estimate derivative information of the output solution with respect to input parameters
of the node.

The framework proposed in [Paulus et al., 2021] extends 𝜕𝑂 to optimization prob-
lems including integer constraints. Similar to previous work in the convex setting and
unlike prior models tackling combinatorial problems, their method handles the differ-
entiation of constraints using an estimation of active constraints at the optimum in the
backpropagation phase.

5

In [Blondel et al., 2021], differentiable optimization is viewed as a problem of implicit
differentiation. By expressing the solution map of the optimization problem as the root of
a system of equation (such as the KKT conditions) or the solution to a fixed point equa-
tion, the system can leverage automatic differentiation (AD) of the implicit equations to
differentiate the solution map. The approach generalizes the differentiation of quadratic
and conic optimization from [Amos and Kolter, 2019] and [Agrawal et al., 2019] but re-
quires a user-defined system of equations that are necessary for optimality.

3. Package structure

DiffOpt is a Julia package that offers differentiable optimization algorithms to the JuMP
ecosystem. In order to integrate seamlessly with other packages, DiffOpt is built on top
of MathOptInterface.jl (MOI), a foundational unifying package for constrained
optimization, designed to be a backend for modelling interfaces such as JuMP.jl or
Convex.jl. MOI allows the user to describe structured optimization problems in a
unified format based on the constraint representation of functions of variables belonging
to sets. This abstraction covers a wide variety of problems including linear, quadratic, and
conic constraints as well as more specific sets, such as Special Ordered Sets or comple-
mentarity constraints. Moreover, MOI includes a bridging mechanism turning the user-
provided problem into a problem structure that the chosen solver accepts through suc-
cessive transformations (or bridges) of the function-set pairs.

The design of MOI enables extending its interface though various mechanisms. First,
constraints are defined as pairs func-in-set where func is a function of the decision
variables and set is a set in which the value of the function should belong when eval-
uated at feasible points. Solvers that can exploit special problem structure can therefore
allow the user to communicate it by defining new function or set types.

Second, most of the MOI interface is built on top of attributes. This enables both the
user to communicate custom information to the solver such as starting values or callbacks
but also the solver to communicate custom results such as basis status for simplex solvers
or Irreducible Inconsistent Subsystems (IIS) for infeasible instances.

Third, MOI optimizers support a layered structure to combine several features. These
layers also supports custom constraints and attributes that are typically defined by an
inner layer thanks to the well-posed API of MOI. These layers are commonly referred to
as meta-solvers as they form solvers parameterized by other solvers. Bridges are defined
as MOI layers that transform constraints into constraints of different types for its inner
layer. When defining a new constraint type, defining bridges to transform it into classical
constraint types allows the user to encode this special structure in models while still
being able to use solvers not supporting this structure. Below are a few examples of MOI
extensions that illustrate this.

SumOfSquares.jl defines the Sum-of-Squares cone as a new set type [Weisser
et al., 2019] and Gram matrices, Moment matrices and Sum-of-Squares decompositions as
new attributes. It then defines a bridge for transforming Sum-of-Squares constraints into
semidefinite constraints. The bridges is applied for all semidefinite programming solvers
except for the solver Hypatia.jl [Coey et al., 2020] which supports this custom struc-
ture.

6

Dualization.jl [Bodin et al., 2021] offers a dualization meta-solver. The opti-
mization problem is automatically converted to its dual form and reaches the internal
solver only in dual form, of which the solution is then mapped back to the user.

Other extensions have been developed, like QuadraticToBinary.jl [Garcia,
2021], another meta-solver that converts quadratically-constrained problems into Mixed
Integer Linear Programs. Also, constraint programming solvers have defined new sets
and functions [Kröger, 2020].

DiffOpt can be decomposed into 5 components and aspects that are covered in the
following five sections. First, it extends MOI by creating new attributes allowing the user
and solver to communicate forward or reverse differentiation input and output. Second,
it implements the quadratic and conic problem differentiation rules described in Section 2
as MOI models implementing these attributes. Third, it implements the communication
of these attributes through the MOI caching and bridging layers. Fourth, it implements
a meta-solver that implements the computation of the differentiation attributes and is
parametrized by a solver that should support solving the problem. Fifth, it allows to
integrate optimization layers into AD systems in Julia using the ChainRulesCore.jl
package.

3.1 Interface

After the problem is solved, the user can pass parameter perturbations in case of for-
ward differentiation or sensitivities with respect to solution variable values in the case of
reverse differentiation (or even both). These are passed to the DiffOpt.Optimizer
using the attributes detailed in Table 1. After sensitivities are loaded, the user might call
DiffOpt.forward_differentiate! or DiffOpt.reverse_differentiate!
to compute the derivatives with respect to the input sensitivities. The resulting deriva-
tives are queried again as typical solver attributes, detailed in Table 2.

ForwardObjectiveFunction
Forward-mode tangent for the objective function
ForwardConstraintFunction Forward-mode tangent for a constraint function
ReverseVariablePrimal

Reverse-mode tangent for a variable value

Table 1: Differential Optimization attributes (DiffOpt attributes for short) for passing per-

turbations and sensitivities.

ForwardVariablePrimal
Forward-mode tangent for a variable value
ReverseObjectiveFunction
Reverse-mode tangent for the objective function
ReverseConstraintFunction Reverse-mode tangent for a constraint function

Table 2: Differential Optimization attributes (DiffOpt attributes for short) for querying

resulting derivatives.

Perturbations are passed and queried in form of MOI functions (affine functions and
quadratic functions) associated with constraints or the objective. The coefficients of vari-

7

ables (or quadratic terms) are the perturbations related to those variables (or quadratic
terms) and the respective associated constraint or objective. This API allows the pertur-
bation to flow through model transformations defined by the above mentioned bridges.
To keep the API efficient the returned functions are lazily computed so that the only the
coefficients required by the final user are actually evaluated.

DiffOpt integrates smoothly with JuMP just like any solver implementing the MOI
interface. Because derivative-related attributes are slightly more complex than traditional
attributes, JuMP was added as a dependency so that we could overload a small subset of
the JuMP API to achieve a better use experience. Moreover, DiffOpt can communicate
with any solver that has an MOI interface, more than 30 of them are listed in the JuMP
Manual [JuMP Developers, 2021].

3.2 Differentiation rules as MOI models

DiffOpt implements the differentiation rules for QPs (resp. CPs) described in Section 2 as a
differentiation optimization MOI model (DiffOpt model for short) QuadraticDiffProblem
(resp. ConicDiffProblem). These DiffOpt models represent QPs (resp. CPs) in the
matrix standard form described in Section 2. They do not support solving the QP (resp.
CP) but they support having the primal and dual solution being set by the user. Then,
they support the differentiation API described in Section 3.1.

ℎ,
≥

the QP form supports

inequality constraints 𝐺𝑥

≤
but not 𝐺𝑥

The QP and CP standard forms described in Section 2 seems quite restrictive. For in-
stance,
i.e.,
MOI.ScalarAffineFunction-in-MOI.LessThan,
ℎ,
i.e., MOI.ScalarAffineFunction-in-MOI.GreaterThan,. This requires the user
to transform their model in order to fit the solver-compatible representation. It is then
quite tedious and error-prone to map both the primal and dual solutions through these
transformations as well as the DiffOpt attributes. Fortunately, as described in the next
section, the transformation of these attributes through bridges is implemented. There-
fore, adding a bridging outer-layer on top of DiffOpt.QuadraticDiffProblem or
DiffOpt.ConicDiffProblem allows the user to model the QP or CP in the most
convenient form while all these transformations are carried out transparently. Moreover,
it also allows extending DiffOpt to new problem classes through bridges. For instance,
as SumOfSquares.jl defines the transformation from a Sum-of-Squares constraint
to a semidefinite constraint using a bridge, defining how to transform the DiffOpt at-
tributes through these bridges automatically broadens the class of programs supported
by DiffOpt.ConicDiffProblem to Sum-of-Squares programs.

3.3 Model transformations

Most model transformations rely in an affine relation between two sets 1 ⊆ ℝ𝑛, 2 of
the form

1 = { 𝑥 ∈ ℝ𝑛 ∣ ∃𝑢 ∈ ℝ𝑚 s.t. 𝐴𝑥 + 𝐵𝑢 + 𝑐 ∈ 2 }.

In fact, at the time of writing, the only bridge in MOI not based on such relation is the
bridge from a convex quadratic constraint to a rotated second order cone constraint as it
relies on the Cholesky decomposition which is not a linear map.

8

As detailed in [Legat, 2020, Section 2.1.2], there is an automated way to implement
the transformation for primal and dual results given the transformation data 𝐴, 𝐵 and 𝑐.
Similarly, we develop in this section the transformation for the DiffOpt attributes.

Given a constraint 𝑓1(𝑥) ∈ 1, the bridges transforms it into a constraint 𝑓2(𝑥, 𝑢) ∈ 2
by adding variables 𝑢 ∈ ℝ𝑚 where 𝑓2(𝑥, 𝑢) = 𝐴𝑓1(𝑥) + 𝐵𝑢 + 𝑐. We have d𝑓2 = 𝐴 d𝑓1. There-
fore, we see that, the ForwardConstraintFunction forward-mode tangent Δ𝑓1
should be mapped to 𝐴Δ𝑓1 and the ReverseConstraintFunction reverse-mode
tangent Δ𝑓2 should be mapped to 𝐴∗Δ𝑓2, with 𝐴∗ the adjoint matrix of 𝐴 satisfying

⟨𝐴𝑥, 𝑦⟩2 = ⟨𝑥, 𝐴∗𝑦⟩1

for all 𝑥 in the space of 1 and 𝑦 in the dual space of 2 where ⟨⋅, ⋅⟩𝑖 is the scalar product
between the space of 𝑖 and its dual space for 𝑖 = 1, 2.

3.4 Meta-solver

DiffOpt is designed as a meta-solver with a structure illustrated in Figure 1. The main
structure made available by the package is DiffOpt.Optimizer that is parametrized
by a mathematical programming inner solver. This inner solver may be any object sup-
porting storing and solving the problem provided by the user. As shown in Figure 1,
the entry point is a cache that is added to ensure efficient storage and access of the user
model. Also, bridge layers are added as the user model may use constraint types that are
not natively supported by the inner solver (or the DiffOpt models) and need to be trans-
formed. The solution found by the inner solver is communicated to the DiffOpt model
as described in Section 3.2. This model is then used to compute forward and/or reverse
differentiation tangents.

Note that the bridges used for the inner solver and for the DiffOpt model are com-
pletely independent. They can be entirely different as the primal and dual results are
automatically transformed through the inner solver bridge layer from the solver solution
into the solution corresponding to the user model and then through the DiffOpt model
bridges into the solution corresponding to the DiffOpt model standard form.

This design of DiffOpt.Optimizer also enables adding new DiffOpt models in
addition to DiffOpt.QuadraticProgram.Model and DiffOpt.ConicProgram.
Model in order to further broaden the class of supported model.

DiffOpt.Optimizer

Cache

Bridges

Bridges

Inner solver

QuadraticProgram.Model

ConicProgram.Model

Figure 1: Design of the DiffOpt.Optimizer structure

9

Because MOI does not differentiate between the two considered classes QP and CP,
the user can pass any of the two problem classes to DiffOpt without ever having to know
which of the two methods will be used. DiffOpt automatically selects the appropriate
problem class based on the type of constraints and objective function.

In the following code excerpt, we demonstrate the usage of DiffOpt, starting from a

simple JuMP model, then going through a reverse differentiation procedure.

(cid:7)

using JuMP, DiffOpt, Clp

model = JuMP.Model(() -> diff_optimizer(Clp.Optimizer))
@variable(model, x)
@constraint(model, cons, x >= 3)
@objective(

model,
Min,
2x,

)

optimize!(model) # solve

MOI.set.(

# set pertubations / gradient inputs

model,
DiffOpt.ReverseVariablePrimal(),
x,
1.0,

)
DiffOpt.reverse_differentiate!(model) # differentiate

# fetch expression of the gradient of constraint
grad_exp = MOI.get(

# -3x+1

model,
DiffOpt.ReverseConstraintFunction(),
cons

)
JuMP.constant(grad_exp)
JuMP.coefficient(grad_exp, x)

# 1

(cid:6)

# -3

(cid:4)

(cid:5)

3.5 ChainRules integration

Automatic Differentiation (AD) has become a cornerstone of machine learning, evolv-
ing from the static transformation of program sources to a fully dynamic process [Innes
et al., 2019]. The Julia AD landscape has been evolving rapidly [Schäfer et al., 2021] and
is now converging towards a shared set of derivative primitives implemented for elemen-
tary functions and exploited by AD libraries to compute derivatives of full programs. The
methods to implement to provide derivative information are defined in ChainRulesCore.jl
[JuliaDiff, 2021] and implemented in ChainRules.jl for Julia Base and standard li-
brary functions.

One drawback is that the ChainRules.jl system reasons on the differentiation of
a function’s output with respect to its inputs. The DiffOpt interface however is based on
MOI and thus on the incremental construction of a model represented as a single mutable
object. One approach is to construct an implementation of the solution map which takes
as argument the model parameters, builds and optimizes the model object, and returns the
optimal solution. The solution map is a pure function and its derivatives can be expressed

10

in terms of ChainRules.jl primitives and implemented using derivative information
from DiffOpt.

The implementation allows external users to effortlessly bring any MOI model built
directly or through a modelling interface like JuMP or Convex.jl to a differentiable
pipeline, regardless of the underlying solver used to solve the main model. However, the
user still needs to write down the solution map and implement the ChainRules.jl
interface functions frule and rrule for forward and reverse differentiation respec-
tively. We will demonstrate how DiffOpt and ChainRules.jl can be combined in the
following section.

4. Application examples

We discuss how differentiating an optimization program allows a variety of applications
for different computational tasks. These examples use various convex solvers to compute
the primal and dual solutions, in order to highlight the ease to swap a solver for another
in a single line.

All the following examples can be seen in detail in the Tutorials section of the DiffOpt

manual. The code and data use version 0.4 of the package.

4.1 Sensitivity Analysis

Sensitivity analysis [Saltelli et al., 2004] focuses on studying how the changes in the inputs
of a mathematical model affects its output. Sensitivities of a JuMP model can be computed
automatically using the previously described methods. We illustrate sensitivity analysis
for a classification and a regression task.

4.1.1 Classification using SVM

Support vector machines (SVM) classify labelled data points with the hyperplane mini-
mizing the norm of classification errors on all points (or achieve the largest margin if the
two classes are separable). Assuming 𝑋 ∈ ℝ𝑛×𝑑 the feature matrix of 𝑛 data points with 𝑑
features and 𝑦 their labels, a soft-margin 𝓁1-SVM with 𝓁2 regularization can be modelled
as:

𝑛
∑
𝑖=1

𝜉𝑖 + 𝜆‖𝑤‖2

min
𝜉 ,𝑤,𝑏
s.t. 𝑦𝑖(𝑤𝑇 𝑋𝑖 + 𝑏) ≥ 1 − 𝜉𝑖 ∀𝑖 ∈ 1..𝑛

𝜉𝑖 ≥ 0 ∀𝑖 ∈ 1..𝑛,

where 𝑒𝑖 is the soft margin loss on the 𝑖 − 𝑡ℎ data point, 𝑤𝑇 𝑥 = 𝑏 is the SVM hyperplane,
𝜆 is the regularization parameter.

The plots and code transcripts are from Sensitivity Analysis of SVM tutorial. The

model is implemented below and solved using Ipopt [Wächter and Biegler, 2006].

(cid:7)

(cid:4)

# N, D, X, y are given
𝜆 = 0.05
model = Model(() -> DiffOpt.diff_optimizer(Ipopt.Optimizer))

# Add the variables

11

@variable(model, 𝜉 [1:N] >= 0)
@variable(model, w[1:D])
@variable(model, b)

# Add the constraints.
@constraint(model,
con[i in 1:N],
y[i] * (dot(X[i,:], w) + b) >= 1 - 𝜉 [i]

)

# Define the objective and solve
@objective(model, Min, 𝜆 * dot(w, w) + sum(𝜉 ))
optimize!(model)

(cid:6)

Using the forward differentiation mode of DiffOpt, we compute the partial Jacobian with
respect to each 𝑖-th individual data point and display its norm:

‖
‖
‖
‖

𝜕𝑤
𝜕𝑋𝑖

‖
‖
‖
‖

+

|
|
|
|

𝜕𝑏
𝜕𝑋𝑖

|
|
|
|

.

The computation is performed as follows:

(cid:7)

∇ = zeros(N)
for i in 1:N

for j in 1:N

if i == j

# identical perturbations on all x_i
MOI.set(

model,
DiffOpt.ForwardConstraintFunction(),
con[j],
y[j] * sum(w),

)

else

MOI.set(

model,
DiffOpt.ForwardConstraintFunction(),
con[j],
0.0,

)

end

end
DiffOpt.forward_differentiate!(model)
dw = MOI.get.(
model,
DiffOpt.ForwardVariablePrimal(),
w,

)
db = MOI.get(
model,
DiffOpt.ForwardVariablePrimal(),
b,

)
∇[i] = norm(dw) + norm(db)

end

(cid:6)

(cid:5)

(cid:4)

(cid:5)

A perturbation of the feature matrix 𝑋 affects the solution and can induce a change
in the separating hyperplane decisions (𝑤, 𝑏). Unlike other classification models, not

12

Figure 2: Learned SVM and sensitivities. Data points colors indicate the class and marker

size denoting the sensitivity of the hyperplane to 𝑥𝑖.

all points affect the hyperplane with small enough perturbations, the optimal solution
depends only on a few data points, the support vectors that name the method. The impact
of these perturbations are displayed in Figure 2.

4.1.2 Ridge regression sensitivity

Ridge regression avoids overfitting with an 𝓁2-norm penalty added to a linear regression
model and is particularly advantageous when the number of features is as large as the
number of observations. Assume 𝑋 = {(𝑥, 𝑦)} ⊂ ℝ𝑑+1 to be the set of 𝑛 data points. Then
a ridge regression fitting problem can be modelled as:

min
𝑤,𝑏

𝑁
∑
𝑖=1

(𝑦𝑖 − 𝑤𝑇 𝑥𝑖 − 𝑏)2 + 𝛼(‖𝑤‖2

2 + 𝑏2)

with 𝛼 the regularization constant.

The plots and code transcripts are from Sensitivity Analysis of Ridge Regression tuto-
rial. We implement and solve below a univariate example with DiffOpt using Ipopt as the
underlying QP solver.

(cid:7)

# X, Y, N are given

(cid:4)

model = Model(() -> diff_optimizer(Ipopt.Optimizer))

@variable(model, w)
@variable(model, b)
alpha = 0.8
@expression(model, e[i=1:N], Y[i] - w * X[i] - b)

# regularization constant

@objective(

13

model,
Min,
1 / N * dot(e, e) + alpha * (wˆ2 + bˆ2),

)

optimize!(model)

(cid:6)

Similar to the SVM example, a change in a single independent or dependent variable
value 𝑥𝑖 or 𝑦𝑖 for a given data point can affect the learned model. We use DiffOpt in
forward mode to quantify these sensitivities to individual data points:

𝜕𝑤
𝜕𝑥𝑖

and

𝜕𝑤
𝜕𝑦𝑖

.

A ForwardInObjective attribute can be set for the perturbation of the objective
function. It takes as input the expression proportionally dependent on the perturbed pa-
rameter 𝜃. Given a generic expression 𝑓 (𝑥; 𝜃) = 𝜃𝑔(𝑥) with parameter 𝜃, the expected
input is 𝑔(𝑥). A particular aspect here is that the 𝑥𝑖 and 𝑦𝑖 values appear as linear and
quadratic term in the loss function. If the parameter 𝜃 appears both linearly and quadrat-
ically, the corresponding objective perturbation can be derived with a first-order Taylor
expansion:

𝑓 (𝑥; 𝜃) = 𝜃𝑔(𝑥) + 𝜃 2ℎ(𝑥)

𝑓 (𝑥; 𝜃 + 𝛿) = (𝜃 + 𝛿)𝑔(𝑥) + 𝜃 2ℎ(𝑥) + 2𝛿𝜃ℎ(𝑥) + 𝛿 2ℎ(𝑥)
𝑓 (𝑥; 𝜃 + 𝛿) ≈ 𝑓 (𝑥; 𝜃) + 𝛿(𝑔(𝑥) + 2𝜃ℎ(𝑥)).

When applied to the loss function, the perturbation 𝛿 (𝑥)
in first-order perturbation approximations:

𝑖

, 𝛿 (𝑦)
𝑖

on 𝑥𝑖, 𝑦𝑖 respectively results

𝛿 (𝑥)
𝑖
𝛿 (𝑦)
𝑖

(2𝑤2𝑥𝑖 + 2𝑏𝑤 − 2𝑤𝑦𝑖),

(2𝑦𝑖 − 2𝑏 − 2𝑤𝑥𝑖)

respectively. Using these input perturbations we can extract the output sensitivity of the
slope 𝑤 using the DiffOpt.ForwardOutVariablePrimal variable attribute:

(cid:7)

∇y = zero(X)
∇x = zero(X)
for i in 1:N
MOI.set(

model,
DiffOpt.ForwardObjectiveFunction(),
2wˆ2 * X[i] + 2b * w - 2 * w * Y[i]

)
DiffOpt.forward_differentiate!(model)
∇x[i] = MOI.get(

model,
DiffOpt.ForwardVariablePrimal(),
w,

)

14

(cid:5)

(cid:4)

(a) 𝑥 sensitivity of the regression slope 𝑤.

(b) 𝑦 sensitivity of the regression slope 𝑤.

Figure 3: Sensitivity analysis of the data points in ridge regression. Radius of the mark-
ers is proportional to the sensitivity of the slope to x or y perturbations. Blue
markers indicate a negative sensitivity, red markers a positive one.

MOI.set(

model,
DiffOpt.ForwardObjectiveFunction(),
(2Y[i] - 2b - 2w * X[i]),

)
DiffOpt.forward_differentiate!(model)
∇y[i] = MOI.get(model, DiffOpt.ForwardOutVariablePrimal(), w)

end

(cid:6)

(cid:5)

Figure 3 shows how sensitive in amplitude and direction is the slope 𝑤 to perturbations
of each data point.

4.2 Convex Optimization for Neural Network Layers

Most of the common neural network layers are either simple closed-form operators or
a composition of several operators. 𝜕 opens new possibilities by providing derivatives
for layers defined as solutions to optimization problems. The derivatives computed by
DiffOpt can be used to backpropagate through the convex layer.

4.2.1 Custom ReLU layer

This example will follow the tutorial Custom ReLU layer of the documentation.

The Rectified Linear Unit or ReLU, a commonly used linear layer in machine learning
networks, is defined as 𝑓 (𝑥) = max{𝑥, 0}. It can be interpreted as projecting a point 𝑥 ∈ ℝ𝑛
onto the non-negative orthant.

min
𝑦≥0

‖𝑥 − 𝑦‖2
2

where 𝑦 is the optimization variable and 𝑥 is the output.
Using the above definition, we model ReLU as a layer in a neural network created in
Flux.jl [Innes, 2018]. The model was trained on MNIST image dataset [LeCun et al., 2010]

15

with 60, 000 greyscale training bitmaps of size 28 × 28. We define the function matrix_-
relu with a matrix input because it allows training in batches, the first dimension of the
matrix is the layer size, while the second dimension is the size of the batch.

(cid:7)

function matrix_relu(

y::Matrix;
model = Model(() -> DiffOpt.diff_optimizer(Ipopt.Optimizer))

)

end

(cid:6)

layer_size, batch_size = size(y)
empty!(model)
set_silent(model)
@variable(model, x[1:layer_size, 1:batch_size] >= 0)
@objective(model, Min, x[:]'x[:] -2y[:]'x[:])
optimize!(model)
return value.(x)

Using our function as a neural network layer requires a corresponding derivative
implementation to differentiate the model and propagate the gradients backward. This
can be achieved using DiffOpt in conjunction with ChainRules by defining a method
for the reverse-mode primitive function rrule.

(cid:7)

function ChainRulesCore.rrule(::typeof(matrix_relu), y::Matrix{T}) where T

model = Model(() -> DiffOpt.diff_optimizer(Ipopt.Optimizer))
pv = matrix_relu(y, model = model)
function pullback_matrix_relu(dl_dx)

x = model[:x] # load decision variable `x` into scope
dl_dy = zeros(T, size(dl_dx))
dl_dq = zeros(T, size(dl_dx))
# set sensitivities
MOI.set.(model, DiffOpt.ReverseVariablePrimal(), x[:], dl_dx[:])
# compute grad
DiffOpt.reverse_differentiate!(model)
# return gradient wrt objective parameters
obj_exp = MOI.get(model, DiffOpt.ReverseObjectiveFunction())
# coeff of `x` in q'x = -2y'x
dl_dq[:] .= JuMP.coefficient.(obj_exp, x[:])
dq_dy = -2 # dq/dy = -2
dl_dy[:] .= dl_dq[:] * dq_dy
return (ChainRulesCore.NoTangent(), dl_dy)

end
return pv, pullback_matrix_relu

end

(cid:6)

Note the ChainRulesCore.NoTangent term which corresponds to the derivative
of the output with respect to the function matrix_relu itself. The next example will
demonstrate how to handle stateful functions. Since this is a pure function, no derivative
is defined. We can now define the neural network architecture including our custom layer
and train it on the MNIST data.

(cid:7)

using MLDatasets
using Flux

(cid:4)

(cid:5)

(cid:4)

(cid:5)

(cid:4)

16

# neural network definition
layer_size = 10
m = Flux.Chain(

Flux.Dense(784, layer_size), # 784 being image linear dimension (28 x 28)
matrix_relu,
Flux.Dense(layer_size, 10), # 10 being the number of outcomes (0 to 9)
Flux.softmax,

)

# dataset preprocessing
N = 1000 # batch size
imgs = MLDatasets.MNIST.traintensor(1:N)
labels = MLDatasets.MNIST.trainlabels(1:N)
train_X = float.(reshape(imgs, size(imgs, 1) * size(imgs, 2), N))
train_Y = Flux.onehotbatch(labels, 0:9);
epochs = 50
dataset = repeated((train_X, train_Y), epochs)

# neural network optimization
custom_loss(x, y) = crossentropy(m(x), y)
opt = Flux.ADAM()
Flux.train!(custom_loss, params(m), dataset, opt)

(cid:6)

Implementing a custom layer for a known closed-form function is not directly use-
ful, and solving a quadratic problem is costly in contrast with a simple ReLU operation.
However, it opens the door to more flexible variations of the layer.

4.2.2 Polyhedral projection layer

We generalize the custom layer defined as an optimization problem from the ReLU ex-
ample. This use case is available in the tutorials as Polyhedral QP layer.

Given 𝑚 vector-scalar pairs (𝑤𝑖, 𝑏𝑖)∀𝑖 ∈ 1..𝑚, we define the layer taking 𝑦 as input

and projecting it on the polytope defined by the 𝑚 hyperplanes:

‖𝑥 − 𝑦‖2
2

min
𝑥
s.t. 𝑤𝑇

𝑖 𝑥 ≥ 𝑏𝑖 ∀𝑖 ∈ 1..𝑚.

Instead of a function, we will represent the layer with a functor (or callable object).

(cid:7)

struct Polytope{N}

w::NTuple{N, Vector{Float64}}
b::Vector{Float64}

end

Polytope(w::NTuple{N}) where {N} = Polytope{N}(w, randn(N))

(cid:6)

We define a "call" operation on the polytope, making it a so-called functor. Calling the
polytope with a matrix y operates an Euclidean projection of each of the matrix columns
onto the polytope.

(cid:7)

function (polytope::Polytope{N})(

y::AbstractMatrix;
model = direct_model(DiffOpt.diff_optimizer(Ipopt.Optimizer))

) where {N}

17

(cid:5)

(cid:4)

(cid:5)

(cid:4)

layer_size, batch_size = size(y)
empty!(model)
set_silent(model)
@variable(model, x[1:layer_size, 1:batch_size])
@constraint(model,

greater_than_cons[idx in 1:N, sample in 1:batch_size],
dot(polytope.w[idx], x[:, sample]) ≥ polytope.b[idx]

)
@objective(model, Min, dot(x - y, x - y))
optimize!(model)
return JuMP.value.(x)

end

Flux.@functor Polytope

(cid:6)

The @functor macro from Flux implements auxiliary functions for collecting the
parameters of our custom layer and operating backpropagation. Similarly to the ReLU
example, ChainRulesCore.rrule is used to implement the reverse-mode differen-
tiation of the layer.

(cid:7)

function ChainRulesCore.rrule(
polytope::Polytope{N},
y::AbstractMatrix) where {N}

model = direct_model(DiffOpt.diff_optimizer(Ipopt.Optimizer))
xv = polytope(y; model = model)
function pullback(dl_dx)

`dl_dy` is the derivative of `l` wrt `y`

layer_size, batch_size = size(dl_dx)
dl_dx = ChainRulesCore.unthunk(dl_dx)
#
x = model[:x]
# grad wrt input parameters
dl_dy = zeros(size(dl_dx))
# grad wrt layer parameters
dl_dw = zero.(polytope.w)
dl_db = zero(polytope.b)
# set sensitivities
MOI.set.(model, DiffOpt.ReverseVariablePrimal(), x, dl_dx)
# compute grad
DiffOpt.reverse_differentiate!(model)
# compute gradient wrt objective function parameter y
obj_expr = MOI.get(model, DiffOpt.ReverseObjectiveFunction())
dl_dy .= -2 * JuMP.coefficient.(obj_expr, x)
greater_than_cons = model[:greater_than_cons]
for idx in 1:N, sample in 1:batch_size

cons_expr = MOI.get(model,

DiffOpt.ReverseConstraintFunction(),
greater_than_cons[idx, sample])

dl_db[idx] -= JuMP.constant(cons_expr)/batch_size
dl_dw[idx] .+= JuMP.coefficient.(cons_expr, x[:,sample])/batch_size

end
dself = ChainRulesCore.Tangent{Polytope{N}}(; w = dl_dw, b = dl_db)
return (dself, dl_dy)

end
return xv, pullback

end

(cid:6)

Note that the inner pullback returns a ChainRulesCore.Tangent that repre-
sents the tangent of a composite type. This will allow Flux to operate gradient descent on

18

(cid:5)

(cid:4)

(cid:5)

the parameters of the Polytope struct directly. Similarly to the previous example, we
can now build and train the network (we omit other details like dataset preprocessing):

(cid:4)

(cid:5)

(cid:7)

layer_size = 20
m = Flux.Chain(

Flux.Dense(784, layer_size), # 784 being image linear dimension (28 x 28)
Polytope((randn(layer_size), randn(layer_size), randn(layer_size))),
Flux.Dense(layer_size, 10), # 10 being the number of outcomes (0 to 9)
Flux.softmax,

)
Flux.train!(custom_loss, Flux.params(m), dataset, opt)

(cid:6)

The capacity to embed a convex problem as a neural network layer enables formula-
tions of SVMs or regression layers with custom constraints which have direct applications
in meta-learning [Lee et al., 2019].

4.3 Hyperparameter optimization

Most machine learning algorithms involve hyperparameters that require tuning to ac-
celerate the training process and reach good out-of-sample performance, helping achieve
balance between model variance and bias. In the past few years, many developments were
made in gradient-based methods [Maclaurin et al., 2015] where researchers typically in-
troduced extra hyperparameters for tuning. Recent interest in automated machine learn-
ing (AutoML) has resulted in a resurgence of research in this field [Feurer and Hutter,
2019].

When the learning process can be formulated as solving an optimization problem of
which the hyperparameters are parameters, the problem of out-of-sample optimization
can be formulated as a bilevel optimization problem [Guyon et al., 2019]:

min
𝜃
s.t.

𝑓 (𝑋𝑡𝑒𝑠𝑡 ,

̂𝑤, 𝜃)

̂𝑤 ∈ argmin

𝑤

𝑓 (𝑋𝑡𝑟𝑎𝑖𝑛, 𝑤, 𝜃),

with 𝑤 the learned weights of the prediction model, 𝑋𝑡𝑟𝑎𝑖𝑛/𝑡𝑒𝑠𝑡 the training and testing
data, 𝑓 the loss function and 𝜃 the hyperparameter. Depending on the expression of
∇𝜃 𝑓 (⋅, ⋅, ⋅), computing (even local) optima of the bilevel optimization problem can be chal-
lenging. We showcase how DiffOpt can be used to meta-optimize the weights and the
hyperparameters following the DiffOpt tutorial Auto-tuning Hyperparameters.

4.3.1 Optimization problem

Let 𝑋 = {(𝑥, 𝑦)|𝑥 ∈ ℝ𝑑 , 𝑦 ∈ ℝ} be the set of 𝑁 data points of dimension 𝐷. The regularized
linear model can be modelled as an optimization problem of the form:

min
𝑤

1
2𝑛𝐷

𝑛
∑
𝑖=1

(𝑦𝑖 − 𝑤𝑇 𝑥𝑖)2 +

𝛼
2𝐷

‖𝑤‖2
2

(1)

where 𝑤 ∈ ℝ𝐷 are the learned weights and 𝛼, the regularization parameter, is the only
hyperparameter. Since the problem is strongly convex, it admits a unique minimum 𝑤∗.
Its implementation in JuMP is given below.

19

(cid:7)

import JuMP

function fit_ridge(model, X, y, 𝛼)

JuMP.empty!(model)
set_silent(model)
N, D = size(X)
@variable(model, w[1:D])
@expression(model, err_term, X * w - y)
@objective(

model,
Min,
dot(err_term, err_term) / (2 * N * D) + 𝛼 * dot(w, w) / (2 * D),

)
optimize!(model)
return w

end

(cid:6)

4.3.2 Model differentiation

We want to find the optimal regularization parameter for the loss on a test set that was
not used to find the optimal weights. We will apply gradient descent of the unregularized

test loss with respect to 𝛼, for which we need to compute its gradient
achieved using the chain rule:

𝜕𝑙
𝜕𝛼

. This can be

𝜕𝑙
𝜕𝛼

(𝑤, 𝛼) = ∇𝑤𝑙(𝑤, 𝛼)

𝜕𝑤
𝜕𝛼

(𝑤, 𝛼)

𝜕𝑤
𝜕𝛼

where
and can be found using DiffOpt as follows:

is the derivative of the optimal solution of Problem (1) w.r.t. the parameter

(cid:7)

function compute_dw_d𝛼(model, w)

D = length(w)
dw_d𝛼 = zeros(D)
MOI.set(

model,
DiffOpt.ForwardObjectiveFunction(),
dot(w, w)

/ (2 * D),

)
DiffOpt.forward_differentiate!(model)
for i in 1:D

dw_d𝛼[i] = MOI.get(

model,
DiffOpt.ForwardVariablePrimal(),
w[i],

)

end
return dw_d𝛼

end

function d_testloss_d𝛼(model, X_test, y_test, w, ˆw)

N, D = size(X_test)
dw_d𝛼 = compute_dw_d𝛼(model, w)
err_term = X_test * ˆw - y_test
return sum(eachindex(err_term)) do i

dot(X_test[i,:], dw_d𝛼) * err_term[i]

end / (N * D)

end

(cid:6)

20

(cid:4)

(cid:5)

(cid:4)

(cid:5)

Figure 4: Mean squared error on the training

Figure 5: Gradient descent on the test set for

and test set against 𝛼

𝛼 using DiffOpt

4.3.3 Hyperparameter gradient descent

The value of 𝛼 is updated using a fixed-step gradient descent scheme implemented below:

(cid:7)

function descent(𝛼0, max_iters=100; fixed_step = 0.01, grad_tol=1e-3)

𝛼_s = Float64[]
𝜕 𝛼_s = Float64[]
test_loss = Float64[]
𝛼 = 𝛼0
N, D = size(X_test)
model = Model(() -> DiffOpt.diff_optimizer(Ipopt.Optimizer))
for iter in 1:max_iters

w = fit_ridge(model, X_train, y_train, 𝛼)
ˆw = value.(w)
err_term = X_test * ˆw - y_test
𝜕 𝛼 = d_testloss_d𝛼(model, X_test, y_test, w, ˆw)
push!(𝛼_s, 𝛼)
push!( 𝜕 𝛼_s, 𝜕 𝛼)
push!(test_loss, norm(err_term)ˆ2 / (2 * N * D))
𝛼 -= fixed_step * 𝜕 𝛼
if abs( 𝜕 𝛼) ≤ grad_tol

break

end

end
return 𝛼_s, 𝜕 𝛼_s, test_loss

end

(cid:6)

4.3.4 Numerical results

(cid:4)

(cid:5)

The mean squared error of the regression on the training and test sets is displayed in Fig-
ure 4, normalized for display. For values of 𝛼 below ≈ 0.23, the model is under-regularized
i.e. overfitted to the training data, increasing 𝛼 improves the test error. After that point,
increasing 𝛼 excessively shrinks the regression coefficients, increasing the error.

Figure 5 displays the trajectory of the gradient descent procedure described in Sec-

tion 4.3.3, starting from 𝛼0 = 0.1 and performing about 120 iterations.

21

Cross-validation is the typical procedure applied to tune the hyperparameters of
learning models. Leveraging 𝜕 lets us replace black-box optimization procedures that
are typical for hyperparameters with any first-order method that leverages the gradient
obtained from DiffOpt.

5. Conclusion

DiffOpt allows users to differentiate through optimization problems implementing
MathOptInterface, formulated with high-level modelling systems like JuMP or
Convex.jl. The package brings a flexible differentiable optimization framework be-
yond disciplined convex optimization, separating the underlying implementation from
the MOI idiomatic interface. This separation of concerns opens the possibility for the
integration of novel differentiable optimization techniques for classes of problems not
already covered.

Future work will also consider performance improvements. In particular, using Dif-
fOpt to tune hyperparameters or construct a convex optimization layer requires fast com-
putation of derivatives and reoptimization of a perturbed primal problem. The two steps
could be integrated to cache more information and reduce the computational burden per
iteration.

Acknowledgments

The work of A. Sharma on DiffOpt.jl was funded by the Google Summer of Code
program through NumFocus. M. Besançon was partially supported through the Research
Campus Modal funded by the German Federal Ministry of Education and Research (fund
numbers 05M14ZAM,05M20ZBM). J. Dias Garcia was supported in part by the Coorde-
nação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Finance Code
001. The authors would like to thank all contributors and users of the package for their
feedback and improvements, and in particular Invenia Technical Computing for support,
feedback on the API and documentation, and the integration with ChainRules. We also
thank Guillaume Dalle for feedback and discussion on early versions of this manuscript.

References
[Agrawal et al., 2019] Agrawal, A., Amos, B., Barratt, S., Boyd, S., Diamond, S., and Kolter, Z.
(2019). Differentiable Convex Optimization Layers. arXiv:1910.12430 [cs, math, stat]. arXiv:
1910.12430.

[Agrawal et al., 2020] Agrawal, A., Barratt, S., Boyd, S., Busseti, E., and Moursi, W. M. (2020).

Differentiating Through a Cone Program. arXiv:1904.09043 [math]. arXiv: 1904.09043.

[Agrawal and Boyd, 2020] Agrawal, A. and Boyd, S. (2020). Differentiating through Log-Log Con-

vex Programs. arXiv:2004.12553 [math]. arXiv: 2004.12553.

[Amos, 2019] Amos, B. (2019). Differentiable optimization-based modeling for machine learning.

PhD Thesis, PhD thesis. Carnegie Mellon University.

22

[Amos and Kolter, 2019] Amos, B. and Kolter, J. Z. (2019). OptNet: Differentiable Optimization as

a Layer in Neural Networks. arXiv:1703.00443 [cs, math, stat]. arXiv: 1703.00443.

[Berthet et al., 2020] Berthet, Q., Blondel, M., Teboul, O., Cuturi, M., Vert, J.-P., and Bach, F. (2020).
Learning with Differentiable Perturbed Optimizers. arXiv:2002.08676 [cs, math, stat]. arXiv:
2002.08676.

[Besançon et al., 2022] Besançon, M., Diamandis, T., Sharma, A., and Garcia, J. D. (2022). Math-

OptSetDistances.jl: v0.2.2. https://doi.org/10.5281/zenodo.6505452.

[Bezanson et al., 2017] Bezanson, J., Edelman, A., Karpinski, S., and Shah, V. B. (2017). Julia: A

fresh approach to numerical computing. SIAM review, 59(1):65–98.

[Blondel et al., 2021] Blondel, M., Berthet, Q., Cuturi, M., Frostig, R., Hoyer, S., Llinares-López, F.,
Pedregosa, F., and Vert, J.-P. (2021). Efficient and modular implicit differentiation. arXiv preprint
arXiv:2105.15183.

[Blondel et al., 2020] Blondel, M., Teboul, O., Berthet, Q., and Djolonga, J. (2020). Fast differen-
In International Conference on Machine Learning, pages 950–959.

tiable sorting and ranking.
PMLR.

[Bodin et al., 2021] Bodin, G., Garcia, J. D., Legat, B., Besançon, M., Lubin, M., Dowson, O.,
and Cornejo, M. (2021). Dualization.jl: v0.3.4. https://doi.org/10.5281/zenodo.
4718987.

[Bolte and Pauwels, 2020] Bolte, J. and Pauwels, E. (2020). A mathematical model for auto-
matic differentiation in machine learning. Advances in Neural Information Processing Systems,
33:10809–10819.

[Busseti et al., 2019] Busseti, E., Moursi, W. M., and Boyd, S. (2019). Solution refinement at reg-
ular points of conic problems. Computational Optimization and Applications, 74(3):627–643.
Publisher: Springer.

[Coey et al., 2020] Coey, C., Kapelevich, L., and Vielma, J. P. (2020). Solving natural conic formu-

lations with hypatia. jl. arXiv preprint arXiv:2005.01136.

[Djolonga and Krause, 2018] Djolonga, J. and Krause, A. (2018). Differentiable learning of sub-

modular models. Advances in Neural Information Processing Systems 30, 2:1014–1024.

[Dontchev and Rockafellar, 2009] Dontchev, A. L. and Rockafellar, R. T. (2009). Implicit functions

and solution mappings, volume 543. Springer.

[Dunning et al., 2017] Dunning, I., Huchette, J., and Lubin, M. (2017). Jump: A modeling language

for mathematical optimization. SIAM Review, 59(2):295–320.

[Feurer and Hutter, 2019] Feurer, M. and Hutter, F. (2019). Hyperparameter optimization. Auto-

mated Machine Learning, pages 3–33.

[Friberg, 2021] Friberg, H. A. (2021). Projection onto the exponential cone: a univariate root-

finding problem.

23

[Garcia, 2021] Garcia, J. D. (2021). QuadraticToBinary.jl: v0.2.4. https://doi.org/10.

5281/zenodo.4718981.

[Gould et al., 2019] Gould, S., Hartley, R., and Campbell, D. (2019). Deep declarative networks: A

new hope. arXiv preprint arXiv:1909.04866.

[Guyon et al., 2019] Guyon, I., Sun-Hosoya, L., Boullé, M., Escalante, H. J., Escalera, S., Liu, Z.,
Jajetic, D., Ray, B., Saeed, M., Sebag, M., et al. (2019). Analysis of the automl challenge series.
Automated Machine Learning, page 177.

[Innes, 2018] Innes, M. (2018). Flux: Elegant machine learning with julia. Journal of Open Source

Software.

[Innes et al., 2019] Innes, M., Edelman, A., Fischer, K., Rackauckas, C., Saba, E., Shah, V. B., and
Tebbutt, W. (2019). A differentiable programming system to bridge machine learning and sci-
entific computing. arXiv preprint arXiv:1907.07587.

[JuliaDiff, 2021] JuliaDiff (2021). Chainrules.jl, recipes for ad. https://www.juliadiff.

org/ChainRulesCore.jl/stable/.

[JuMP Developers, 2021] JuMP Developers (2021).

JuMP Manual. https://jump.dev/

JuMP.jl/v0.21.9/installation/#Supported-solvers.

[Kakade and Lee, 2018] Kakade, S. M. and Lee, J. D. (2018). Provably correct automatic sub-
differentiation for qualified programs. Advances in neural information processing systems, 31.

[Kotary et al., 2021] Kotary, J., Fioretto, F., Van Hentenryck, P., and Wilder, B. (2021). End-to-end

constrained optimization learning: A survey. arXiv preprint arXiv:2103.16378.

[Kröger, 2020] Kröger, O. (2020). ConstraintSolver.jl. https://github.com/Wikunia/

ConstraintSolver.jl.

[LeCun et al., 2010] LeCun, Y., Cortes, C., and Burges, C. (2010). Mnist handwritten digit database.

ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2.

[Lee et al., 2019] Lee, K., Maji, S., Ravichandran, A., and Soatto, S. (2019). Meta-learning with
In Proceedings of the IEEE/CVF Conference on Computer

differentiable convex optimization.
Vision and Pattern Recognition, pages 10657–10665.

[Legat, 2020] Legat, B. (2020). Set programming : theory and computation. PhD thesis, UCLouvain.

[Legat et al., 2021] Legat, B., Dowson, O., Garcia, J., and Lubin, M. (2021). Mathoptinterface: a
INFORMS Journal on Computing (in

data structure for mathematical optimization problems.
press).

[Maclaurin et al., 2015] Maclaurin, D., Duvenaud, D., and Adams, R. (2015). Gradient-based hy-
perparameter optimization through reversible learning. In International conference on machine
learning, pages 2113–2122. PMLR.

24

[O’Donoghue et al., 2016] O’Donoghue, B., Chu, E., Parikh, N., and Boyd, S. (2016). Conic opti-
mization via operator splitting and homogeneous self-dual embedding. Journal of Optimization
Theory and Applications, 169(3):1042–1068.

[Paulus et al., 2021] Paulus, A., Rolínek, M., Musil, V., Amos, B., and Martius, G. (2021). Combopt-
net: Fit the right np-hard problem by learning integer programming constraints. arXiv preprint
arXiv:2105.02343.

[Saltelli et al., 2004] Saltelli, A., Tarantola, S., Campolongo, F., and Ratto, M. (2004). Sensitivity
analysis in practice: a guide to assessing scientific models, volume 1. Wiley Online Library.

[Schäfer et al., 2021] Schäfer, F., Tarek, M., White, L., and Rackauckas, C. (2021). Abstract-
arXiv preprint

differentiation.jl: Backend-agnostic differentiable programming in julia.
arXiv:2109.12449.

[Wächter and Biegler, 2006] Wächter, A. and Biegler, L. T. (2006). On the implementation of an
interior-point filter line-search algorithm for large-scale nonlinear programming. Mathematical
programming, 106(1):25–57.

[Weisser et al., 2019] Weisser, T., Legat, B., Coey, C., Kapelevich, L., and Vielma, J. P. (2019). Poly-

nomial and moment optimization in julia and jump. In JuliaCon.

25

