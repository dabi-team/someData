2
2
0
2

r
a

M
1
1

]

G
L
.
s
c
[

1
v
5
0
7
5
0
.
3
0
2
2
:
v
i
X
r
a

DNN Training Acceleration via Exploring
GPGPU Friendly Sparsity

Zhuoran Song, Yihong Xu, Han Li, Naifeng Jing, Xiaoyao Liang, Li Jiang

Shanghai Jiao Tong University

Abstract

The training phases of Deep neural network (DNN) consumes enor-
mous processing time and energy. Compression techniques utilizing the
sparsity of DNNs can eﬀectively accelerate the inference phase of DNNs.
However, it is hardly used in the training phase because the training
phase involves dense matrix-multiplication using General-Purpose Com-
putation on Graphics Processors (GPGPU), which endorse the regular
and structural data layout. In this paper, we ﬁrst propose the Approx-
imate Random Dropout that replaces the conventional random dropout
of neurons and synapses with a regular and online generated row-based
or tile-based dropout patterns to eliminate the unnecessary computation
and data access for the multilayer perceptron (MLP) and long short-term
memory (LSTM). We then develop a SGD-based Search Algorithm that
produces the distribution of row-based or tile-based dropout patterns to
compensate for the potential accuracy loss. Moreover, aiming at the con-
volution neural network (CNN) training acceleration, we ﬁrst explore the
importance and sensitivity of input feature maps; and then propose the
sensitivity-aware dropout method to dynamically drop the input feature
maps based on their sensitivity so as to achieve greater forward and back-
ward training acceleration while reserving better NN accuracy. To facil-
itate DNN programming, we build a DNN training computation frame-
work that uniﬁes the proposed techniques in the software stack. As a
result, the GPGPU only needs to support the basic operator—matrix
multiplication and can achieve signiﬁcant performance improvement re-
gardless of DNN model. Experiments results on MLP and LSTM using
well-known benchmarks show that the speedup rate brought by the pro-
posed Approximate Random Dropout ranges from 1.18-2.16 (1.24-1.85)
when dropout rate is 0.3-0.7 on MLP (LSTM) with negligible accuracy
drop. As for CNN, the proposed sensitivity-aware dropout method can
achieve up to 2.17× speedup with 0.2% accuracy loss. Our codes are
available at https://github.com/songzhuoran/video-block-based-acc.

Introduction

1
Deep Neural Networks (DNNs) have emerged as critical technologies to solve
various complicated problems [6, 12]. The inference of DNNs is computationally

1

 
 
 
 
 
 
expensive and memory intensive and therefore has an urgent need for accelera-
tion before we can fully embrace DNNs in the power-limited devices. Extensive
works are proposed to reduce the computation by compressing the size of synap-
tic weights, such as weight pruning [3, 1], quantization, low rank and Compact
Network Design [15, 2, 5]. The above compression techniques may require re-
training the DNN with limited accuracy loss (< 1%). The success of these
techniques relies on the sparsity and plasticity of DNNs; however, these tech-
niques cannot directly apply to the training phase of DNNs.

The training phase, involving the back-propagation through the network to
update the weights, demands three-times more computation eﬀort. GPU is
suitable for such task attributed to GPU’s superior parallelism for large matrix
multiplication [16, 7]. Extensive works propose to accelerate the training phase
on the distributed GPU-based system [11, 13]. Other works focus on accelerating
the training phase using gradient pruning and weight quantization, respectively.
Random Dropout technique addresses the over-ﬁtting problem and is widely
used in training MLP and LSTM. The most common method randomly drop-
ping some neurons of each layer in every training iteration, while the other
(DropConnect [9]) aims the same goal by randomly dropping some synapses
connections between layers (i.e., some elements in weight matrix). Theoreti-
cally, we can reduce the number of multiplication to 30%-70% if we can skip
the calculation of all the dropped neurons or synapses while the dropout rate
changes from 0.3 to 0.7. However, the potential tremendous saving of multipli-
cation and data access is hard to exploit because the neurons or synapses are
randomly and irregularly dropped following the Bernoulli distribution. Such
irregularity prevents the GPU’s single instruction multiple threads (SIMT) ar-
chitecture to skip the unnecessary multiplication and memory access.

Therefore, for accelerating the training process of MLP and LSTM, we pro-
pose the approximate random dropout technique that replaces the random
dropout with two types of regular dropout patterns to make the choices of
dropped neurons or synapses predictable, which allow GPU to skip calculation
of those dropped neurons or synapses. We further developed an SGD-based
Search Algorithm to produce the distribution K of dropout patterns such that
the dropout rate of each neuron is approximately subjected to a Bernoulli distri-
bution (We provide a brief proof). In each iteration, we sample a dropout pat-
tern subjected to K and then eliminate the redundant computation by omitting
the dropped data during the hardware allocation. Consequently, the training
process of MLP and LSTM can be substantially accelerated by our proposed
regular dropout patterns.

Since the errors can accumulate and magnify through multiple layers, the
previously published approximate random dropout technique will lead to severe
accuracy loss if is directly implemented to the large CNN model. Dedicated
training mechanism targeting for the CNN is therefore required to accelerate the
CNN training process. Facing its increasing computing power and high training
accuracy demand, we ﬁrst explore the impact of the input feature maps and
weights on the training accuracy to determine which one should we drop. And
we ﬁnd that the input feature maps have smaller inﬂuence than weights on the

2

training accuracy. We consequently choose the input feature map as the drop
target during CNN training.

Since the input feature maps have diﬀerent sensitivity as reported in the
previous works, we propose the sensitivity-aware dropout technique that can
capture the feature characteristics in the input feature maps during CNN train-
ing. The sensitivity-aware dropout technique ﬁrst dynamically identiﬁes the
sensitive regions in the input feature map which are likely to contain impor-
tant feature information for the ﬁnal training results. It then applies low/high
dropout possibilities to sensitive/insensitive regions. In this way, the charac-
teristics of the input feature map will be reserved and fortiﬁed for higher CNN
accuracy while saving a signiﬁcant amount of computing resources from the
insensitive regions. We further provide the dynamic dropout ratio tuning tech-
nique to dynamically tune the dropout ratio during the CNN training process
to ensure accuracy. Moreover, considering the GPU’s SIMT characteristic, we
propose the hardware-aware dropout mechanism that drops the input feature
maps in row- and block-level.

Last but not least, to deploy the proposed techniques, we deﬁne the basic
operator in them as the matrix multiplication. We then provide the DNN [14,
10, 15, 1] training computation framework that uniﬁes the proposed techniques
in the software stack. The computation framework ﬁrst carries out the approx-
imate random dropout and the sensitivity-aware dropout techniques to gener-
ate the binary mask for MLP, LSTM, or CNN. Afterward, the framework will
constitute the matrix based on the binary mask that is used to indicate the
keeping/dropping row/blocks. As a result, the GPU only needs to support the
basic operator—matrix multiplication and can achieve satisfactory performance
gain regardless of the DNN model.

Our experiments show that the speedup rate brought by the proposed Ap-
proximate Random Dropout ranges from 1.18-2.16 (1.24-1.85) when dropout
rate is 0.3-0.7 on MLP (LSTM) with less than 0.5% accuracy loss. We ﬁnd that
when the batch size increases, the speedup rate increases with accuracy of neural
network declines. As for CNN training acceleration, the proposed sensitivity-
aware dropout method achieves 2.17× speedup with negligible accuracy loss.

The reminder of the paper is organized as follows: Section 2 introduces the
background. Section 4 illustrates the motivation of our proposed sensitivity-
aware dropout design. Section 3 describes the proposed Approximate Ran-
dom Dropout Technique. Section 5 introduces the proposed sensitivity-aware
dropout method. Experiments are shown in Section 7. Section 8 concludes this
paper.

2 Background

2.1 Accelerating DNN inference and training

There are considerable works pitch into accelerating inference of DNN by lever-
aging the sparsity of DNN. Han et al. prune synaptic weights which are close

3

to zero and then retrain the DNN to maintain the classiﬁcation accuracy. The
zero weights are then indexed, compressed and moved onto the on-chip memory.
Special decoder is deployed in the accelerator to skip the computation of zero
weights. Consequently, above methods can only beneﬁt ASIC/FPGA based
DNN accelerator instead of GPU. Jaderberg et al. and Ioannou et al. use low-
rank representations to create computationally eﬃcient neural networks. These
methods cannot be used in training phase because of the subtle change of the
weights degrades the convergence and accuracy of the training phase.

Extensive works propose to accelerate the training phase on the distributed
GPU-based system [11, 13]. Wen et al. propose to use ternary gradients to
accelerate distributed deep learning in data parallelism. Zhang et al. propose
a variant of the asynchronous SGD algorithm to guarantee the convergence of
this algorithm and accelerate the training in a distributed system. Other works
are relative to the acceleration in the training process using gradient pruning
and weight quantization. Kster et al [3]. share the exponent part of the binary
coding of the weights and thereby convert ﬂoating-point operations to ﬁxed-
point integer operations. Noted that this work is compatible with ours and we
leave this topic to further research. Sun et al [4]. prune those comparatively
small gradients to speed up training phase. However, their work focuses on
software-level optimization and thus yields marginal training acceleration while
this work enable computation reduction on hardware-level.

2.2 Basics of the GPGPU

GPGPU is commonly used for DNN training.
It is composed of dozens of
streaming multiprocessors (SMs) [9, 10]. Each SM consists of single instruc-
tion multiple threads (SIMT) cores and a group of on-chip memories including
register ﬁle, shared memory, L1D cache and etc. Each SM manages and exe-
cutes multi-threads on it. Those threads are clustered into warps, executing the
same instruction at the same time. Thus, the branch divergence occurs when
programmers write conditional branch (if-else).

Shared memory is a performance-critical on-chip memory. The latency of
accessing the global memory (DRAM) is roughly 100x higher than that of ac-
cessing the shared memory. Hence, reducing the frequency of accessing global
memory is critical for performance. The capacity of the shared memory per
block is 48KB in Nvidia GTX 1080Ti, which is much smaller than the capac-
ity of the global memory. Therefore, reducing the superﬂuous data in shared
memory is also important.

The key purpose of this work is to reduce the scale of matrices, by which we
can reduce the access frequency of the shared memory and the global memory
as well as the computation eﬀort to accelerate the training.

2.3 Random Dropout

Random dropout is widely used to prevent over-ﬁtting. It randomly omits part
of the neurons or synapses on each training iteration. The probability of a

4

Figure 1: Overview of the Approximate Random Drouput.

Figure 2: Row-based Dropout Pattern.

neuron or a synapses to be dropped is subjected to a Bernoulli distribution
In a nutshell, the main reason why
parameterized with a dropout rate [8].
random dropout can eﬀectively prevent over-ﬁtting is that it generates adequate
diﬀerent sub-models to learn diverse features during the training process and
ensembles those sub-models to maximize the capability of DNN for inference [9].
A question arises: why not skipping the calculation of those dropped neu-
rons to reduce the redundant time spent on the matrix multiplication and the
data movement. Intuitively, we can write conditional branch (if - else) to skip
the redundant calculation. However, such conditional branches incur branch di-
vergence in GPU, which is a great hurdle for performance. In GPGPU’s SIMT
architecture, the red threads have to wait for the green threads. Thus, some
process elements (PEs) are idle, represented by the red cross. The total execu-
tion time is not reduced (even increased) due to the branch divergence. Thus,
it is non-trivial to exploit the dropout for speedup in GPGPU.

3 Approximate Random Dropout

The key idea of accelerating the MLP and LSTM training is to reduce the
scale of matrices involved in multiplication and avoid the divergence of GPU.
However, the randomness in conventional dropout methods hamper the scale
reduction.

5

SGD-Based Search AlgorithmDropout Pattern GenerationDNN TrainingSampled Dropout Pattern for Iteration i...dp = 4b = 1Two sets of Dropout Patterns346445635684454257578594797544794524763424...dp = 3Drop 2 rows every 3 rowsDRAMFetch data346445578594763424...897435Shared Mem...Row 0Row 1Row 2Row x...ExecutePE 0PE 1PE 2Output MatrixInput Matrix*=Dropped dataWeight Matrix`Row 0Row 1Row 2Row x…...Row x*dpRow 3…Row 0Row 1~2Row 4~5Row 6PE x123For accelerating the MLP and LSTM, we deﬁne dropout pattern as the com-
bination of dropped neurons in each training iteration. As shown in Fig. 1, we
replace the random dropout with regular dropout patterns generated online.
Resulted from the replacement, we can forecast which neurons or synapses to
be dropped and thereafter assist GPU to skip the calculation and data access
of the dropped neurons without incurring the divergence.

However, the loss of randomness induced by the regular dropout patterns
increases the risk of over-ﬁtting the MLP or LSTM. To cope with this issue,
we further develop a Stochastic Gradient Decedent (SGD) based Search Algo-
rithm (see section 3.3), to ﬁnd a distribution of all possible dropout patterns
such that the probability distributions of each neuron or synapse being dropped
between our method and conventional method is equivalent. We provide a brief
proof of that.

In this section, based on the computation characteristic of GPU, we ﬁrstly
propose two sets of Dropout Patterns—Row-based Dropout Pattern (RDP) and
Tile-based Dropout Pattern (TDP)—and then elaborate how to reduce compu-
tation and data access. After that, we introduce our SGD-based Search Algo-
rithm which produce a distribution of possible dropout patterns as well as the
dropout pattern generation procedure in each iteration.

3.1 Row-based Dropout Pattern

In conventional random dropout method, a dropped neuron incurs the multipli-
cation of zero and the correspondent row in the weight matrix of next layer; in
RDP, we drop the whole row in the weight matrix, which is equivalent to drop
all the synapses of a dropped neuron.

Concretely, RDP is parameterized by two scalars dp and bias b as follow: we
uniformly choose a bias b ∈ {1, ..., dp} and drop all rows in the weight matrix
whose indices i satisfy

i : (i − b) mod dp (cid:54)= 0

(1)

Consequently, (i − 1)/(i) of the neurons are dropped. When dp=3, b=1, for
instance (the left of Fig. 2), we drop two rows (i.e., neurons) in every successive
three rows (neurons) in the weight matrix from the top. Note that two scalars
dp and b changes in every training iteration.

Given the size of the output matrix as M × N , the maximum dp is dpmax =
i=1 i = (M + 1)/2 consid-

M , and the maximum number of the sub-models is (cid:80)M
ering the number of possible bias is i when dp = i.

The execution processes in GPU is also shown in Fig. 2. DRAM stores the
whole weight matrix (as shown in step 1); the gray block denotes the rows of
weight matrix correspondent to the dropped neurons. We write the kernel func-
tion to prevent GPU from fetching those dropped data into shared memory (as
shown in step 2) and build two compact matrices (input matrix and weight ma-
trix) for next step. After data fetch, every PE multiplies one row of the weight
matrix by the whole input matrix. Thus, only 1
dp of the original weight matrix
as well as the input matrix is fetched and calculated. The resulting rows ﬁll

6

Figure 3: Tile-based Dropout Pattern.

1 × dp rows in the Output Matrix using the same pattern. The rest dp−1
dp of the
Output Matrix is set to zero by default. Note that the RDP is agnostic to the
matrix-multiplication algorithm as it temporarily compresses the matrices into
a compact layout. Therefore, RDP can comply to any optimization method for
matrix multiplication.

3.2 Tile-based Dropout Pattern (TDP)

Tile is a sub-matrix in weight matrix and contains multiple synapses connec-
tions. We use tiles as the unit to drop rather than synapse (namely the size of
tiles is 1) for the purpose of regularity. TDP is also parameterized by dp and
bias b. dp − 1 tiles are dropped in every dp tiles, resulting in dp−1
dp of synapses
connections being dropped. When dp = 4, b = 1, as shown in the left of Fig. 3,
we drop 3 tiles in every 4 successive tiles starting from ﬁrst tile.

TDP has similar procedure compared to RDP but is diﬀerent in two as-
pects: (1)TDP fetches non-dropped tiles into the shared memory rather than
rows, and builds two compact matrices. (2) each PE conduct the multiplica-
tion of one tile of compact weight matrix and the corresponding tile of compact
input matrix, according to their PE index. In the right of Fig. 3, GPU only
conduct multiplication of two compact matrices whose scale is 1
4 of the original
scale. This Dropout Pattern can naturally work with Tiling Method in matrix
multiplication, which is an essential optimization technique.

Given the size of the output matrix M × N , the size of the tile x × y, the
maximum dp is dpmax = (cid:98)M/x(cid:99) × (cid:98)N/y(cid:99) and the maximum number of sub-
models is (1 + dpmax)/2. TDP can generate more sub-models than RDP, when
N is roughly greater than x × y.

The choice of tile size is critical: small tile leads to more diverse Dropout
Patterns as well as sub-models, but ﬁne-grained control. Under such circum-
stances, the size of tile is set to be 32 × 32 to balance the maximization of the
number of sub-models and avoiding shared memory’s bank conﬂict since the
shared memory has 32 banks in NVIDIA GPU.

A typical training process of MLP and LSTM is composed of three steps:
fully connected layer computation, activation layer computation and dropout
layer computation using the mask matrix. After applying the Dropout Pattern
with dp = 2, we only need to spend half of the time for fully connected layer
computing and skip the dropout layer computing. Consequently, given the
dropout pattern, the time spending on training can be overtly reduced.

7

344545238574432374343434427953585345368667425873858094694213487446955351…dp = 4Drop 3 tile every 4 tiles13DRAM...Fetch data34454323535867424213535189743535...Shared MemTile 0Tile 1Tile 2Tile x...ExecutePE 0PE 1PE 2Output MatrixInput Matrix*=Dropped dataWeight Matrix`Tile 0Tile 1Tile 2Tile x…...PE xRow:x*dp/NColumn:(x*dp)%N TileTile 0Tile 1Tile 2Tile 3Tile 5Tile 6Tile 7Tile 4Tile 823.3 SGD-based Search Algorithm for Dropout Pattern Dis-

tribution

For each iteration in training procedure, only one regular dropout pattern is
applied to the network. In order to approximate the traditional dropout pro-
cess, the dropout pattern we choose in each iteration should satisfy that: (1)the
dropout probability of each neuron should subject to a given Bernoulli distribu-
tion, and (2)diﬀerent sub-models derived from that series of dropout patterns
should be adequate.

With the above two requirements in mind, we propose an eﬃcient SGD-
based Search Algorithm to ﬁnd a desired probability distribution. We then
randomly generate the dropout patterns following the found probability distri-
bution. Statistically, we want the derived dropout patterns can satisfy the above
two demands. SGD consumes tractable time and is convenient in optimizing
the continuous variables. More speciﬁcally, the algorithm obtains a probability
distribution K = {ki}dpmax
i=1 which contains the probability ki of each possible
Dropout Pattern i ∈ {1, 2, ..., dpmax}, which is subjected to (cid:80)dpmax

ki = 1.

Here we deﬁne the global dropout rate as the proportion of neurons or
synapses who are set to zero. Noted that the global dropout rate is diﬀer-
ent from the conventional dropout rate which refer to the probability of a single
neuron or synapse to be dropped. However, we prove that within our approach
the two dropout rate are statistically equivalent.

i=1

i }N

Given the target global dropout rate p, and the maximum dp as N , we
use Algorithm ?? to search for desired distribution K. A vector v with length
N is ﬁrst arbitrary initialized (line 1) and the sof tmax(v) serve as the ﬁnal
probability distribution of each dropout pattern (line 4). Then we setup a
constant vector pu = { i−1
i=1 whose element denotes the global dropout rate
of a given dropout pattern. Consequently, dT · pu is the expected global dropout
rate and the diﬀerence between it and the target global dropout rate is our
optimization objective (line 5). The negative information entropy of d is added
to the loss to enforce d to be dense and to produce more diversiﬁed sub-models
(line 6, 7). The algorithm uses SGD algorithm to update v (line 8, 9) and
return the distribution d ∈ [0, 1]N when the loss is stuck. The loss function
deﬁned in line 7 derives the algorithm to ﬁnd a distribution K that (1)makes
the global dropout rate equal to required value p and (2)maximizes the sub-
models diversity.

3.4 Dropout Pattern Generation

The acquired distribution K is then used to sample dropout pattern in each iter-
ation. In each iteration, we randomly sample a dropout pattern (parameterized
by dp and b) subjected to the distribution K, and then uniformly choose a bias
b ∈ {1, ..., dp}. Dropout pattern is then determined.

In our method, global dropout rate is statistically equivalent to the single
neurons or synapse dropout rate. For each neuron or synapse, the probability

8

of it to be dropped (conventional dropout rate) is:

pn =

dpmax
(cid:88)

i=1

pbki =

dpmax
(cid:88)

i=1

i − 1
i

ki

The global dropout rate of K is:

pg = dT · pu =

dpmax
(cid:88)

i=1

ki

i − 1
i

≈ p

(2)

(3)

Therefore, in terms of the whole training process, the dropout rate pn of a
single neurons or synapse is equal to the global dropout rate pg and thus is
approximately equal to the target dropout rate p by the SGD-based Search
Algorithm.

4 Exploring the Sensitivity of Feature Map

Based on the value distribution in the input feature maps of the CNN model,
we observe that the majority of feature map values are close to zero, while a
small number of feature map values are large. Because dropping values reduces
the NN training accuracy as revealed by prior work, three questions naturally
arise: 1) what type of data is more important during CNN training? weights
or input feature maps? 2) whether there are input values in input feature maps
that are crucial to the CNN training accuracy?; 3) how are these input values
distributed in the spatial domain?

To answer the questions, we ﬁrst ﬁnd that input values are less important
compared to weights so that we should drop input values instead of weights
during CNN training. Moreover, we also observe that a majority of input values
have smaller inﬂuence on the CNN accuracy and they aggregate in space that
are termed as insensitive regions. As a result, insensitive regions should be
tracked and dropped in a hardware friendly manner to accelerate the CNN
training process, while the sensitive regions should be reserved for negligible
CNN accuracy loss. This motivates our feature map sensitivity-aware dropout
algorithm as elaborated in Section 5.

4.1 The Importance of Weights

In this section, we will explore the roles of input feature maps playing in the
CNN training process so that we can decide which types of data should we
choose to drop. We take MNIST dataset on AlexNet as a case study.

First, we drop weights and input feature maps separately. Then we measure
the CNN accuracy to study the signiﬁcance of weights and input feature maps.
As shown in Fig. 4, we ﬁnd that dropping weights during the training process
seriously deteriorates the CNN accuracy to 89.2% when we drop 40% weights.
Alternatively, the CNN accuracy still remains 92.41% when we drop 40% input

9

Figure 4: Comparsion between pruning feature map and pruning weight.

Figure 5: Comparing the impact of diﬀerent sensitive values on speciﬁc network.

feature maps, as illustrated in Fig. 4. We can conclude that the input feature
maps have a smaller impact on the ﬁnal CNN accuracy compared to weights.
This is because dropping weights will make the training process becoming in-
valid and lead to a sharp accuracy degradation. As a result, we choose the
input feature map as the dropout target to speed up the training process while
reserving the CNN accuracy.

4.2 Sensitivity of Input Feature Map Values

In this section, we will investigate the sensitivity of input feature maps that
aﬀects the CNN training accuracy. We take the widely used dataset CIFAR-10
on VGG19 as case study.

First, we classify the input feature map values of each layer into several parts
according to their magnitudes at runtime. For example, we classify the values
into four parts. Part 1 contains the largest 25% values of the feature map, part
2 and 3 contain the middle 50% values, and part 4 contains the smallest 25%
values. We then drop values of diﬀerent parts with diﬀerent dropout ratio during
the training process. Finally, we measure the ﬁnal CNN accuracy and study the
sensitivity of diﬀerent parts. Fig. 5 shows the results, where we have several bars
representing accuracy results of diﬀerent parts. For example, bar 1 represents
dropping part 1, while bar 2 represents dropping part 2. And we can see that
part 4 holding the smallest input values can maintain accuracy even when we
drop 60% values in part 4. In addition, the CNN accuracy decreases drastically
when we drop values in part 1 no matter what the dropout ratio is. Those

10

90.74%90.08%89.21%92.49%92.98%92.41%88%89%90%91%92%93%94%0.10.250.4AccuracyPrune RatioWeight PruningFeature Map Pruning40%50%60%70%80%90%100%0.20.40.6AccuracyPrune RatioPart 1Part 2Part 3Part 4observations indicate that diﬀerent input values in diﬀerent parts have diﬀerent
inﬂuence on the CNN training accuracy that can be seen as the sensitivity of
input feature maps. For example, part 1 containing the largest values is more
sensitive to accuracy than other parts. In other words, we can guarantee the
accuracy by carefully dropping the sensitive parts, while freely dropping the
insensitive parts. But identifying the sensitive and insensitive values is still a
big challenge as the input feature maps are not available until run time. This
process needs to be eﬃcient and hardware friendly, and therefore prompts our
sensitivity-aware dropout technique as elaborated in Section 5.

4.3 Sensitivity of Feature Map Regions

In this section, we will explore how the sensitive input values are distributed in
the spatial domain. We take CIFAR-10 on VGG19 as an visualized example.
We randomly peak one training iteration, and then use diﬀerent colors to mark
the input values into three parts as depicted in Fig ??. We can clearly see that
the values with the same sensitivity gather together that terms as sensitive
regions, while the insensitive values belonging to parts 2 and 3 dominate the
input feature map.

As a result, we can apply the structured dropout pattern on the insensitive
regions considering the SIMT characteristics of GPU. Thanks to the majority of
insensitive regions, we can achieve tremendous speedup with the runtime struc-
tured dropout. Therefore, we propose the sensitivity-aware dropout scheme to
accelerate CNN training performance with negligible accuracy loss, as illustrated
in the following section.

5 Sensitivity-aware Dropout Method

Section 4 has shown that there are sensitive regions in the input feature maps
during the training process. Based on this observation, we propose the Sensitivity-
aware Dropout Method to speed up the CNN training process. There are three
more problems that need to be addressed:

1. How to design a sensitivity identiﬁcation algorithm? As the input fea-
ture maps of CNN are dynamically changed, their sensitivities have to be
identiﬁed eﬃciently and hardware-friendly.

2. How to conduct the sensitivity-aware dropout with eﬃciency and high ac-
curacy? For sensitive and insensitive regions, we should apply diﬀerent
dropout possibilities. As a result, we can reserve as many key features in
sensitive regions as possible to maintain the training accuracy. In addi-
tion, the dropout granularity should be carefully considered. Compared
to the ﬁne-grained dropout, the coarse-grained dropout is more hardware-
friendly considering the SIMT characteristics of GPU. Therefore, we will
drop input feature maps in a coarse-grained manner.

11

3. How to determine the dropout ratio? Given that the CNN accuracy varies
during training, we should dynamically tune the dropout ratio so as to
balance the ﬁnal performance improvement and accuracy.

To this end, we propose the sensitivity-aware dropout method mainly con-
tains three steps: 1) sensitivity prediction; 2) hardware-aware dropout; and 3)
dynamic dropout ratio tuning: First, we design a sensitivity prediction algo-
rithm to eﬃciently locate the sensitive regions in the input matrix. Then, we
generate a sensitivity mask, which records the dropout possibilities for sensitive
and insensitive regions. We will provide more details in Section 5.1. Afterward,
we drop the input matrix in a hardware-friendly manner, which is elaborated
in Section 5.2. During the sensitivity-aware dropout, the dropout ratio will be
dynamically tuned considering both speedup and accuracy. We will give the
details in Section 5.3.

5.1 Sensitivity Prediction

Given input feature map of C ×H ×W dimension and weight of Cout ×C ×K ×K
size, GPU ﬁrst calls Im2col function to transform the input feature map to in-
put matrix of (H × W ) × (C × K × K) size, and the weight to weight matrix
of Cout × (C × K × K) size (as shown in Step 1). Next, we split the input
matrix into multiple x × y regions. For each region, we need to identify its sen-
sitivity. For real-time sensitivity prediction without hurting the CNN training
speed, we propose to randomly select k% values to determine the sensitivity of
the region (Step 2). We then compare k% values to the predeﬁned threshold.
The comparison process can be treated as using the step activation function.
The corresponding region is sensitive if there exists t% values are larger than
the threshold. Otherwise, the region is insensitive (Step 3). Note that k%,
t% are empirically chosen by a subset of training dataset. Finally, we use dif-
ferent dropout possibilities for diﬀerent regions, which will be recorded in the
sensitivity mask with a dimension of h×w
x×y (Step 4). Speciﬁcally, we apply low
dropout possibility for the sensitive region, while high dropout possibility for
the insensitive region. For instance, we apply m% dropout possibility for the
sensitive region, while n% dropout possibility for the insensitive region (Note
that n > m). This can be applied to the following hardware-aware dropout
technique in Section 5.2.

Fig. 6 shows the example of sensitivity prediction. To identify the sensitivity
of one input region on-the-ﬂy, we ﬁrst randomly select 30% (k%) values in the
region and then compare them with the predeﬁned threshold 0.5. Since the
proportion of values that are greater than 0.5 is larger than 50% (t%), we deﬁne
the region as sensitive region. We iteratively execute the sensitivity prediction
and generate the sensitivity mask indicating the dropout possibilities of sensitive
and insensitive regions.

12

Figure 6: Overview of Sensitivity Prediction.

Figure 7: Example of Row-based Sensitivity-aware Dropout.

5.2 Hardware Aware Dropout

To accelerate the CNN training process considering the GPU’s SIMT character-
istic, we drop the input matrix using the row-based or block-based sensitivity-
aware dropout pattern.

5.2.1 Row-based Sensitivity-aware Dropout Pattern

In the Row-based Sensitivity-aware Dropout Pattern (RSDP), a row of input
matrix can be seen as the dropout granularity. And we will drop the rows in the
input matrix based on their corresponding dropout possibilities in each training
iteration. We depict an example of RSDP in Fig. 7(a). In this example, after
sensitivity prediction, the ﬁrst, third, and fourth rows are predicted as sensitive
rows and they have m% possibility to be dropped. And since the second, ﬁfth,
and sixth rows are predicted as insensitive rows, they have n% possibility to
be dropped. Next, we apply the hardware aware dropout to each iteration and
drop a subset of rows in the input matrix. In the current iteration, the second,
ﬁfth, and sixth rows are dropped.

The execution process of the hardware aware dropout is shown in Fig. 7(b).
Initially, DRAM stores the whole input matrix (as shown in Step 1). During
each CNN training iteration, we will ﬁrst sample the dropped rows in the input
matrix based on their dropout possibilities. Then we write the kernel function
to avoid fetching the dropped rows of input matrix into shared memory (Step
2) and build the dense and small input matrix for next step. Afterward, each
PE multiplies one row of the input matrix by the whole weight matrix (Step 3).
Next, the resulting rows ﬁll rows in the output matrix using the same pattern.

13

Im2col…Input Feature MapC*H*WInput MatrixH*W*(C*K*K)Random Selection0.80.40.60.10.70.20.30.90.6Input Region0.80.40.60.10.70.20.30.90.6Input Region>threshold?Randomly selected value0.80.40.60.10.70.20.30.90.6Input RegionLarger than the thresholdSmaller than the thresholdProportion>50%?0.80.40.60.10.70.20.30.90.6Sensitive RegionSensitivity Mask Generationm%n%n%n%m%n%n%n%m%Sensitivity Mask Sensitivity PredictionInput MatrixH*W*(C*K*K)Input MatrixH*W*(C*K*K)Sensitive RegionInsensitive RegionHardware Aware DropoutInput MatrixH*W*(C*K*K)Dropped DataDRAMFetch DataExecutexInput MatrixWeight Matrix…(a)(b)Row 1Row 3Row 413Dropped DataPE0PE1PE2Shared Mem2Row 1Row 3Row 4Figure 8: Example of Block-based Sensitivity-aware Dropout.

In short, as only a subset of the input matrix is fetched and calculated, the
CNN training process can be well accelerated.

5.2.2 Block-based Sensitivity-aware Dropout Pattern

Given two large matrices, GPU applies tiling technique to optimize the matrix
multiplication and memory accesses. Motivated by this, we propose the Block-
based Sensitivity-aware Dropout Pattern (BSDP) that regards the block of input
matrix as the dropout granularity.

Intuitively, we can directly drop the blocks in the input matrix at run-time.
In this way, the second, third, and sixth blocks are sampled to be dropped, as
depicted in Fig. 8(a). However, given that the rows of the input matrix have
uneven sparsity and each PE multiplies one row of the input matrix by the
whole weight matrix, the PEs’ workloads are unbalanced, as shown in Fig. 8(b).
In a nutshell, dropping the blocks by only considering the sensitivity results in
lower utilization of PE and hampers the performance improvement.

To handle the extreme workload imbalance, we propose to redistribute the
sensitive blocks and make them distributed evenly. Speciﬁcally, we ﬁrst deﬁne
the blocks that belong to the same row as a group. We then predict the sensitive
blocks within each group. During the prediction, we will limit the number of
sensitive blocks in all groups to be nearly equal. As a result, given that the rows
of the input matrix have even sparsity, the PEs’ workloads will be balanced. The
detail of the block-based sensitivity-aware dropout considering the workload
balance is shown in Fig. ??.

During the conventional CNN training, there are two steps: convolution
layer computation and activation layer computation. To implement our pro-
posed method, we insert the sensitivity prediction and RSDP or BSDP oper-
ations before the convolution layer computation to determine which rows or
blocks in the input matrix will be pruned. Consequently, we only need to spend
little time for convolution layer computing and skip the pruned rows or blocks
computing. Given the dropout pattern, the time spending on training can be

14

Sensitivity PredictionInput MatrixH*W*(C*K*K)Input MatrixH*W*(C*K*K)Sensitive RegionInsensitive RegionHardware Aware DropoutInput MatrixH*W*(C*K*K)Dropped Data(a)DRAMFetch DataShared MemExecutePE0PE1PE2xInput MatrixWeight Matrix…(b)Row 0Row 1Row 2123Dropped DataRow 3Row 4Row 5PE3PE4PE6Row 0Row 1Row 2Row 3Row 4Row 5xxxxxxxxxgreatly reduced.

5.3 Dynamic Dropout Ratio Tuning

To reduce the accuracy loss, we propose the Dynamic Possibility Pruning tech-
nique to dynamically tune pruning possibility during CNN training.

The input feature map is calculated by convolving the randomly initialized
weight matrix and input data as the ﬁrst step. Obviously, the initial input
feature map is insuﬃcient to extract key information in the input data. There-
fore, pruning too many input feature maps at ﬁrst will delay the incrementation
of CNN training accuracy. As depicted in Fig. ??, compared to the baseline,
the accuracy can increase more when we use a lower prune possibility (20%)
given the training epochs. Therefore, we start with a low prune possibility to
assist the CNN model extracting features. Afterward, we should gradually in-
crease the prune possibility to achieve tremendous training speedup. However,
to guarantee the ﬁnal training accuracy reaches to a satisfactory degree, we
use a relatively low prune possibility in the last period until the CNN model
converges.

In summary, we can see that the trend of the prune possibility obeys the
skewness distribution that is shown in Eqn. 4. And the remaining challenge
is to generate the mean value µ and the standard deviation σ in the skewness
distribution so that we can obtain the prune possibility given a training epoch.
As shown in Eqn. 5, the mean value µ can be calculated given the expectation
E(Y ) as the total prune possibility. Moreover, the standard deviation σ can be
calculated using the similar way to that used for the mean value µ, as shown in
Eqn. 6.

fY (y) =

2
σ

ϕ(

y − µ
σ

)Φ(λ

y − µ
σ

)

E(Y ) = µ + µ0(λ)σ

µ0(λ) =

(cid:114) 2
π

√

λ
1 + λ2

D(Y ) = σ2

0(λ)σ2

0(λ) = 1 − µ2
σ2

0 = 1 −

2
π

λ2
1 + λ2

(4)

(5)

(6)

6 DNN Training Computation Framework

To build the programmer-friendly interface and support the proposed methods,
we propose the DNN training computation framework.

15

Figure 9: The overall DNN training ﬂow using the proposed methods.

The key ideas of the proposed approximate random dropout and dynamic
pruning technique are both generating the sparse and structured weight or input
matrix for eliminating the unnecessary computations during DNN training. The
overall DNN training ﬂow after applying the proposed methods is shown in
Fig. 9. Speciﬁcally, for MLP and LSTM models, we leverage the weight binary
mask to record the positions of zeros in the weight matrix. Moreover, we leverage
the input binary mask to record the positions of zeros in the input matrix
for the CNN model. For example, for a CNN model, given the input matrix
that has six rows and we choose to skip the computations in row-level, we will
generate the binary mask with six values, where “0” indicates the row should
be zero while “1” indicates the row should be reserved. And the binary mask
is “1,0,1,1,0,0” for the example in Fig. 7(a). Consequently, GPU can fetch the
non-zero data from DRAM to the shared memory by utilizing the binary mask.
Moreover, the PEs will execute the dense matrix multiplication and write the
results back to their corresponding positions according to the binary mask. The
detail execution process in GPU is depicted in Fig. 7(b).

In the proposed computation framework, the programming ﬂow is quite sim-
ilar to the conventional DNN training ﬂow, which is illustrated in Fig. 10. The
framework ﬁrst invokes the approximate random dropout or the dynamic prun-
ing technique for generating the binary mask given the DNN model. Then it
launch kernels to constitute the dense matrix. In the infrastructure software,
the framework applies the highly optimized CUDA library such as cuBLAS or
cuDNN to eﬃciently execute the matrix multiplication. And in the hardware,
the shared memory will only fetch the non-zero values from DRAM, which sig-
niﬁcantly reduces the memory stress. Moreover, the PEs only consume the
non-zero values and conduct the dense matrix multiplications.

In summary, our proposed methods can be well adopted in the proposed
DNN training computation framework, which only introduces the binary mask
to guide the execution of matrix multiplications. And the bottom hardware—
GPU only needs to support the basic matrix multiplication regardless of DNN
model.

16

Approximate Random DropoutDynamic PruningMLP/LSTMCNNWeight Binary MaskInput Binary MaskMatrix MultiplicationDense Weight MatrixDense Input MatrixInput MatrixWeight MatrixMatrix MultiplicationFigure 10: The DNN Training Computation Framework.

7 Experiments

7.1 Experimental Methodology

To evaluate the eﬀectiveness of proposed approximate random dropout for MLP
and LSTM, we compare it with the conventional dropout technique in terms of
the accuracy and the training time. In section 7.2.1, we vary diﬀerent dropout
possibility on a MLP to explore the inﬂuence of the dropout rate on the per-
formance of a speciﬁc 4-layer MLP. Note that the dropout rate in our method
refer to the target dropout rate p as described in Section 3.3. In section 7.2.2,
we compare diﬀerent MLPs with a speciﬁc dropout rate. The data set we use
with MLP is MNIST. LSTM is used in section 7.2.3 to verify the scalability of
our method. The dataset we used with LSTM include a dictionary whose size
is 8800, and the Penn Treebank (PTB) data set which has long been a central
data set for language modeling. The experiment codes is implemented in Caﬀe
and use a single GTX1080Ti GPU to run.

In addition, to evaluate the eﬀectiveness of proposed dynamic pruning method
for CNN, we use two widely used datasets—CIFAR-10 and MNIST. The CNN
models we used are LeNet-5, AlexNet and VGG19, which cover a wide range
of CNN with diﬀerent parameter sizes. As for the baseline setting, we take
the accuracy of the traditional CNN training of the selected model under the
corresponding dataset as the baseline. In this section, we vary the prune possi-
bility from 0.3 to 0.6 and record the accuracy and training time for each prune
possibility.

7.2 Results of Approximate Random Dropout

7.2.1 Comparison of diﬀerent dropout rate

The structure of the 4-layer MLP is described as follow: the input layer is
shaped according to the batch size; the output layer has 10 neurons for digit 0
to 9; the two hidden layers have 2048 neurons both. During training, we set the

17

Approximate Random DropoutDynamic PruningMLP/LSTMCNNDNN ModelAlgorithm SoftwarecuBLAS LibrariesCompilerShared MemoryPEResultInfrastructure SoftwareHardwareFigure 11: Comparing diﬀerent dropout rate combinations on speciﬁc network.

following hyper-parameters: the batch size is 128, the learning rate is 0.01, and
momentum is 0.9.

We vary the dropout rate from (0.3, 0.3) to (0.7, 0.7) (two hidden layers may
have varied dropout rate), and record the accuracy and training time for each
dropout rate. The comparison of two metrics of RDP and TDP against the
conventional dropout are shown in Fig. 11. The training time of conventional
dropout is divided by the new training time of proposed approximate random
dropout to obtain the speedup rate.

The results show that the speedup rate brought by RDP ranges from 1.20 to
1.77 compared with the traditional random dropout technique when the dropout
rate varies between 0.3 and 0.7, which comply to our intuition as the amount
of data that require no calculation expands with the increment of the dropout
rate. The speedup rate brought by TDP ranges from 1.18 to 1.6. The little
slowdown is induced by the calculation of the nonzero positions in the output
matrix before matrix multiplication. The accuracy loss of these two classes of
dropout patterns is less than 0.47%, which is the evadible concession to the
speedup. TDP has less accuracy loss than RDP which can be attributed to the
abundance of sub-models in TDP.

7.2.2 Comparison of diﬀerent networks

We investigate the speedup in diﬀerent MLP structures using a ﬁxed dropout
rate (0.7, 0.7). Those MLPs have the same input and output layer as described
in section 7.2.1. Their hidden layer size is shown in Table 1. For instance,
1024×64 in the second column means the ﬁrst and the second hidden layer’s size
are 1024 and 64, respectively. The hyper-parameters of optimization algorithm
follow above experiments.

From Table 1, the accuracy degradation is less than 0.5%. In some cases, the
accuracy even increases. Moreover, the speedup rate increases as the network
size increases. Especially, in the case of 4096 × 4096 network, both of the
proposed dropout patterns reach a 2× speedup.

7.2.3 Scaling to Long Short-Term Memory Model

We evaluate the speedup rate and the model performance on LSTM, which
predicts the following word based on the given words. Each of the two hidden

18

11.21.41.61.829092949698100SpeedupAccuracy/%speedupold accuracynew accuracy11.21.41.61.89092949698100SpeedupAccuracy/%speedupold accuracynew accuracyTable 1: Comparing diﬀerent network with speciﬁc dropout rate
Speedup
Dropout
rate
rate

Network
size

Accuracy(its loss)

0.7

1024*64

1024*1024

2048*2048

4096*4096

Dropout
pattern
ROW
TILE
ROW
TILE
ROW
TILE
ROW
TILE

98.07%(-0.42%)
98.11%(-0.38%)
98.01%(-0.35%)
98.15%(-0.21%)
98.44%(0.37%)
98.5%(-0.31%)
98.00%(-0.47%)
98.16%(-0.31%)

1.27
1.19
1.45
1.41
1.77
1.60
2.16
1.95

dropout rate

Table 2: A dictionary data set which contains 8800 words on LSTM.
(0.3,0.3)
original
47.9%
ROW 46.9%
47.2%
TILE
original
1.0
ROW 1.18
1.18
TILE

(0.5,0.5)
47.3%
46.0%
46.5%
1.0
1.47
1.43

(0.7,0.7)
45.9%
44.5%
44.4%
1.0
1.53
1.49

accuracy

speedup

layers of LSTM contain 1500 neurons. During training, we set the following
hyper-parameters: the base learning rate is 1 (the base learning rate will gradu-
ally decrease), batch size is 20, the maximum epoch is 50, and the length of the
sequence is 35. Note that the execution of LSTM is also performed as matrix
multiplication, thus our proposed approximate dropout can be easily applied to
LSTM.

As shown in Table 2, the accuracy degradation is less than 1%. When
dropout rate is increasing, the speedup rate increases without undermining the
accuracy loss.

To illustrate the eﬀectiveness of the proposed method, we ﬁx the dropout
rate to 0.5 and trace the accuracy of RDP until it’s convergence. As shown
in Fig. 12, the red curve records our approximate random dropout training
process; the blue one records the traditional random dropout. The convergence
of our method is earlier than the traditional random dropout. Moreover, the
smoothness of red curve indicates the approximate random dropout is helpful
for the training process.

The result using the Penn Treebank data set (PTB) on the 3-layer LSTM is
shown in Fig. 13(a). The test perplexity using RDP only increases 0.04 given the
dropout rate is 0.7, which further shows that our proposed approximate dropout
algorithm can generate adequate sub-models for PTB data set. Besides, when
dropout rate increases from 0.3 to 0.7, the speedup rate also increases from 1.2
to 1.6.

19

Figure 12: The training process of RDP and traditional random dropout.

Figure 13: Speedup rate and accuracy of Row approximate dropout on 3-layer
LSTM.

Figure 14: Accuracy after applying the RPP on MNIST dataset.

We vary the batch size from 20 to 40. Noted that SGD based search and data
initialization are an one-time eﬀort. When the batch size is increased, only the
matrix operation and data transmission time increase accordingly. As shown in
Fig. 13(b), the speedup rate increases when batch size increases. However, since
one dropout pattern is applied to the whole batch, the sub-models generated
during training may not be suﬃcient, which raises the perplexity.

20

                          7 L P H                   $ F F X U D F \ E D V H O L Q H U R Z B G U R S R X W B S D W W H U Q00.40.81.21.628082848688(0.3,0.3)(0.5,0.5)(0.7,0.7)Speedupperplexity(PPL)PPL, baselinePPL, proposedspeedup00.40.81.21.627580859095100(0.3,0.3)(0.5,0.5)(0.7,0.7)Speedupperplexity(PPL)PPL, batch size=20PPL, batch size=40speedup, batch size=20speedup, batch size=40(a)(b)98.5%98.7%98.9%99.1%99.3%99.5%99.7%99.9%LeNet-5AlexNetVGG19Accuracy0.30.40.50.6baselineFigure 15: Accuracy after applying the RPP on CIFAR-10 dataset.

Figure 16: Accuracy after applying the BPP on MNIST dataset.

7.3 Results of Dynamic Pruning Method

Fig 14 shows the CNN accuracy for three networks on MNIST dataset. For
example, regarding VGG19, our dynamic pruning method shows negligible ac-
curacy loss for MNIST compared to the baseline, even when the prune possibil-
ity is as high as 60%, accuracy loss can still be maintained at 0.57%. AlexNet
and LeNet-5 perform even better and can guarantee the accuracy loss is less
than 0.2% when the prune possibility is 60%. Moreover, the CNN accuracy
exceeds the baseline sometimes. This is because dynamic pruning method can
reserve the key information in input feature map while reducing the redundant
information during the training process.

Fig. 15 shows the CNN accuracy for three networks on CIFAR-10 dataset
by applying the RPP. Since CIFAR-10 dataset is more complex than MNIST
dataset and its input feature map is more diverse, a larger prune possibility
will lead to more accuracy loss. LeNet-5 can guarantee accuracy loss when the
prune possibility is 0.4, while other two networks can maintain accuracy when
the prune possibility is 0.5 as they are more redundant compared to LeNet-5.
Therefore, for more complex datasets, we need to make a trade-oﬀ between
prune possibility and accuracy loss.

Fig 16 and Fig. 17 show the CNN accuracy for three networks on MNIST
and CIFAR-10 datasets by applying the BPP. Compared to the RPP, the BPP
can achieve higher accuracy when choosing the same pruning possibility. This is
because the BPP can better match the sensitive region for satisfactory accuracy.
Fig. 18 shows the speedup for three networks. The speedup brought by RPP
and BPP averagely achieves 2.15× and 2.17× on MNIST dataset, respectively.

21

70.0%75.0%80.0%85.0%90.0%95.0%LeNet-5AlexNetVGG19Accuracy0.30.40.50.6baseline98.5%98.7%98.9%99.1%99.3%99.5%99.7%99.9%LeNet-5AlexNetVGG19Accuracy0.30.40.50.6baselineFigure 17: Accuracy after applying the BPP on CIFAR-10 dataset.

Figure 18: Speedup after applying the RPP and BPP on MNIST and CIFAR-10
datasets.

Moreover, since CIFAR-10 dataset is more complicated than MNIST dataset,
the speedup brought by RPP and BPP only achieves 1.5× and 1.9× on CIFAR-
10 dataset.

8 Conclusion

Accelerating DNN training is nontrivial because it is diﬃcult to leverage the
sparsity of DNN in the dense matrix-multiplication. In this work, we ﬁrst pro-
pose the approximate random dropout approach to eliminate the unnecessary
multiplication and data access in MLP and LSTM by replacing the traditional
random dropout with an approximate random dropout. The two classes of
dropout patterns can avoid the divergence issue in GPU, reduce the scale of
the matrix, and thus gain signiﬁcant improvement on the energy-eﬃciency with
marginal decline of the model performance. The proposed SGD-based search
algorithm can guarantee the dropout rate of a single neuron or synapse is equiv-
alent to the conventional random dropout, as well as the convergence and ac-
curacy of the models. In addition, we propose the dynamic pruning method for
accelerating CNN training process. The proposed method oﬀers a promising
opportunity to leverage the sensitivity in input feature maps to accelerate CNN
training. In general, the speedup rate ranges from 1.18-2.16 (1.24-1.85) when
dropout rate is 0.3-0.7 on MLP (LSTM) with negligible accuracy drop. And the
speedup rate can reach to 2.17× on CNN with satisfactory training accuracy.
The proposed method has been wrapped as an API and integrated into Caﬀe.

22

70.0%75.0%80.0%85.0%90.0%95.0%LeNet-5AlexNetVGG19Accuracy0.30.40.50.6baseline0.00.51.01.52.02.5LeNet-5AlexNetVGG19SpeedupRow-MNISTRow-CIFARBlock-MNISTBlock-CIFARThe speedup can be much higher if the proposed method can be integrated into
the cuBLAS Library.

References

[1] Caiwen Ding et. al. Circnn: Accelerating and compressing deep neural

networks using block-circulantweight matrices. 2017.

[2] Yunchao Gong et. al. Compressing deep convolutional networks using vec-

tor quantization. Computer Science, 2014.

[3] Song Han et. al. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huﬀman coding. In ICLR, 2016.

[4] et. al Max Jaderberg. Speeding up convolutional neural networks with low

rank expansions. Computer Science, 4(4):XIII, 2014.

[5] Cong Leng et. al. Extremely low bit neural network: Squeeze the last bit

out with admm. 2017.

[6] Wenjie Luo et. al. Understanding the eﬀective receptive ﬁeld in deep con-

volutional neural networks. 2017.

[7] Siddhartha Puri. Training convolutional neural networks on graphics pro-

cessing units, 2010.

[8] Nitish Srivastava et. al. Dropout: a simple way to prevent neural networks
from overﬁtting. Journal of Machine Learning Research, 15(1):1929–1958,
2014.

[9] Li Wan et. al. Regularization of neural networks using dropconnect. In
International Conference on Machine Learning, pages 1058–1066, 2013.

[10] Wei Wen et. al. Learning intrinsic sparse structures within long short-term

memory. 2017.

[11] Wei Wen et. al. Terngrad: Ternary gradients to reduce communication in

distributed deep learning. 2017.

[12] Tom Young et. al. Recent trends in deep learning based natural language

processing. 2017.

[13] Wei Zhang et. al. Staleness-aware async-sgd for distributed deep learning.
In International Joint Conference on Artiﬁcial Intelligence, pages 2350–
2356, 2016.

[14] Xiangyu Zhang et. al. Shuﬄenet: An extremely eﬃcient convolutional

neural network for mobile devices. 2017.

23

[15] Aojun Zhou et. al.

Incremental network quantization: Towards lossless

cnns with low-precision weights. 2017.

[16] Michal ˇCerˇnansk´y. Training recurrent neural network using multistream
extended kalman ﬁlter on multicore processor and cuda enabled graphic
processor unit. In International Conference on Artiﬁcial Neural Networks,
pages 381–390, 2009.

24

