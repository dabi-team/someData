Noname manuscript No.
(will be inserted by the editor)

A Variable Metric and Nesterov Extrapolated Proximal
DCA with Backtracking for A Composite DC Program

Yu You · Yi-Shuai Niu

2
2
0
2

y
a
M
5

]

C
O
.
h
t
a
m

[

1
v
7
6
6
2
0
.
5
0
2
2
:
v
i
X
r
a

Abstract In this paper, we consider a composite diﬀerence-of-convex (DC)
program, whose objective function is the sum of a smooth convex function
with Lipschitz continuous gradient, a proper closed and convex function, and
a continuous concave function. This problem has many applications in ma-
chine learning and data science. The proximal DCA (pDCA), a special case of
the classical DCA, as well as two Nesterov-type extrapolated DCA – ADCA
(Phan et al. IJCAI:1369–1375, 2018) and pDCAe (Wen et al. Comput Optim
Appl 69:297–324, 2018) – can solve this problem. The algorithmic step-sizes
of pDCA, pDCAe, and ADCA are ﬁxed and determined by estimating a prior
the smoothness parameter of the loss function. However, such an estimate
may be hard to obtain or poor in some real-world applications. Motivated by
this diﬃculty, we propose a variable metric and Nesterov extrapolated proxi-
mal DCA with backtracking (SPDCAe), which combines the backtracking line
search procedure (not necessarily monotone) and the Nesterov’s extrapolation
for potential acceleration; moreover, the variable metric method is incorpo-
rated for better local approximation. Numerical simulations on sparse binary
logistic regression and compressed sensing with Poisson noise demonstrate the
eﬀectiveness of our proposed method.

Keywords Diﬀerence-of-convex programming · Nesterov’s extrapolation ·
backtracking line search · variable metric · sparse binary logistic regression ·
compressed sensing with Poisson noise

Mathematics Subject Classiﬁcation (2010) 65K05 · 90C26 · 90C30

Yu You
School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai, China
E-mail: youyu0828@sjtu.edu.cn

Yi-Shuai Niu
Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong
E-mail: yi-shuai.niu@polyu.edu.hk

 
 
 
 
 
 
2

1 Introduction

Yu You, Yi-Shuai Niu

Diﬀerence-of-convex (DC) programming has been an active area of research in
nonconvex and nonsmooth optimization [19,20, 49, 44,27,1,42]. They arise in
extensive applications such as sparsity learning [55,28], clustering [23], molec-
ular conformation [25], trust region subproblem [45,7], portfolio optimization
[48,47,38], control theory [36], natural language processing [40], image denois-
ing [56], mixed-integer optimization [22,35, 31, 39] and eigenvalue complemen-
tarity problem [24,37,34,33], to name a few; see [27] and the references therein
for a comprehensive introduction about DC programming and its applications.

In this paper, we focus on a composite DC program in form of

min{F (x) := f (x) + g(x) − h(x) : x ∈ Rn},

(P )

where the following are assumed throughout the manuscript:
Assumption 1 (a) f and g are proper convex functions from Rn to (−∞, ∞],

h : Rn → R is convex; moreover, g is closed.

(b) There exists a nonempty closed and convex set Y ⊆ Rn such that dom(g) ⊆

Y ⊆ int(dom(f )).

(c) f is L-smooth over Y , i.e., the gradient of f is L-Lipschitz over Y .
(d) F is bounded from below, i.e., there exists scalar F ∗ such that F (x) ≥ F ∗

for all x ∈ Rn.

Note that we do not require that the loss function f is real-valued over Rn
since the scope of the model can be limited in this way. Next, by setting h ≡ 0
in (P ) we derive the convex optimization problem

min{Φ(x) := f (x) + g(x) : x ∈ Rn},

(Pc)

for which we additionally assume the following:

Assumption 2 The optimal solution set of (Pc) is nonempty, and its optimal
value is denoted by Φ∗.

The classical DCA [44,45,26,46] is renowned for solving (P ), which has
been introduced by Pham Dinh Tao in 1985 and extensively developed by Le
Thi Hoai An and Pham Dinh Tao since 1994. One possible (commonly used)
DC decomposition for (P ) takes the following form

min{F (x) := G(x) − H(x)1 : x ∈ Rn},

(1)

where G(x) = L(cid:107)x(cid:107)2/2 + g(x) and H(x) = L(cid:107)x(cid:107)2/2 − f (x) + h(x) when
x ∈ Y and ∞ otherwise. Then DCA applied for this DC decomposition yields
the proximal DC algorithm (pDCA) proposed in [17]. For the special case
where Y = Rn, the accelerated variant of DCA (ADCA) [50], which incor-
porated the Nesterov’s extrapolation [29, 30] for potential acceleration, can be
applied for solving (1); meanwhile, Wen et al. [54] used the same extrapolation

1 The convention ∞ − ∞ is adopted.

A variable metric and extrapolated proximal DCA with backtracking

3

technique for (P ) and proposed their accelerated algorithm pDCAe, which is
a variant of pDCA with additional extrapolation term. The major diﬀerence
between ADCA and pDCAe may be that ADCA needs to determine whether
or not to conduct the extrapolation zk = xk + βk(xk − xk−1) by comparing
the functions values of the current extrapolation point zk and the previous
q + 1 iterates xk, · · · , xk−q, while there is not such a necessity for pDCAe.
Thus, ADCA costs more per iteration than pDCAe. However, ADCA has a
wider range of applications. For example, pDCAe will reduce to DCA in the
image denoising with nonconvex total variation regularizer [43], thus lossing
the eﬀect of extrapolation; while ADCA still occupies the extrapolation term
and performs better than DCA [56]. Note that the model for both ADCA and
pDCAe assumes that f is real-valued over the whole space, which limits the
scope of the algorithms for some loss functions with domain not being Rn, such
as the generalized Kullback-Leibler divergence in the Poisson noise model [18].
Indeed, ADCA and pDCAe can be generalized for addressing this situation by
additionally projecting the extrapolation point onto the set Y .

Now, let us consider the convex model (Pc). In some practical applications,
the Lipschitz constant of the gradient of f might be hard to estimate. The
(monotone) backtracking line search procedure [6] is a common strategy for
addressing this issue. However, in this way the step-size is nonincreasing, which
may seriously aﬀect the convergence speed if the local curvature of the loss
function is relatively large near the initial iterates but small near the tail; see
[53] for an illustration of this by the compressed sensing and the binary logistic
regression problems. In addition, for those problems with the above property,
the (global) Lipschitz constant is generally lager than the local one, thus the
constant step-size strategy is not recommended. Taking this into account, [53]
proposed a non-monotone backtracking strategy for addressing this kind of
problems. Note that model (P ) inherits the above issues in (Pc).

In this paper, in term of the success of Nesterov’s extrapolation techniques
for acceleration, the variable metric method for better local approximation [10,
11], and the non-monotone backtracking for adaptive step-sizes selection, we
incorporate all of these techniques to propose a scaled proximal DC algorithm
with extrapolation and backtracking (SPDCAe) for solving problem (P ). We
prove that for suitable selections of the extrapolation parameters and the scal-
ing matrices, if the generated sequence of SPDCAe is bounded, then any limit
point of this sequence is a critical point of (P ). Moreover, we also demonstrate
that SPDCAe for the convex case (Pc), denote as SFISTA, enjoys the optimal
O(1/k2) convergence rate in function values. This rate of convergence coin-
cides with that of the well-known FISTA algorithm [6] and SFBEM [10] (a
scaling version of FISTA with monotone backtracking). Finally, we point out
that our SPDCAe (resp. SFISTA) actually covers pDCAe (resp. SFBEM).

The rest of the paper is organized as follows. Sect. 2 reviews some no-
tations and preliminaries in convex analysis. In Sect. 3, we describe the DC
optimization problem as well as its convex case we are concerned in this pa-
per and present our algorithm SPDCAe and its convex version SFISTA. Sect.
4 focuses on establishing the subsequential convergence of SPDCAe and the

4

Yu You, Yi-Shuai Niu

optimal convergence rate of SFISTA. Numerical results on problems of sparse
binary logistic regression and compressed sensing with Poisson noise are sum-
marized in Sect. 5.

2 Notations and preliminaries

Let Rn denote the n-dimensional column vector space endowed with canonical
inner product (cid:104)·, ·(cid:105) and its induced norm (cid:107) · (cid:107). For an extended real-valued
function p : Rn → (−∞, ∞], the set

dom(p) := {x ∈ Rn : p(x) < ∞}

denotes its eﬀective domain. If dom(p) (cid:54)= ∅ and p does not attain the value
−∞, then p is called a proper function. The notation int(dom(p)) denotes the
interior set of dom(f ), and

epi(p) := {(x, t) ∈ Rn × R : p(x) ≤ t}

denotes the epigraph of p. We have the deﬁnition that p is closed (or convex)
if epi(p) is closed (or convex).

Let p : Rn → (−∞, ∞] be a proper function and x ∈ int(dom(p)). Then p

is said to be diﬀerentiable at x if there exists g ∈ Rn such that

lim
d→0

p(x + d) − p(x) − (cid:104)g, d(cid:105)
(cid:107)d(cid:107)

= 0.

The unique vector g as above is called the gradient of p at x, and is denoted
as ∇p(x). Finally, we say that p is L-smooth over Y if p is diﬀerentiable over
Y and (cid:107)∇p(x) − ∇p(y)(cid:107) ≤ L(cid:107)x − y(cid:107) for any x and y. In addition, if Y is
convex, then we have the next descent lemma.
Lemma 2.1 (see e.g., [5]) Let p : Rn → (−∞, ∞] be an L-smooth function
(L ≥ 0) over a given convex set Y . Then for any x, y ∈ Y ,

p(y) ≤ p(x) + (cid:104)∇p(x), y − x(cid:105) +

L
2

(cid:107)x − y(cid:107)2.

(2)

Now, let p : Rn → (−∞, ∞] be a proper closed and convex function. A

vector g ∈ Rn is called a subgradient of p at x if

p(y) ≥ p(x) + (cid:104)g, y − x(cid:105), ∀y ∈ Rn,

and the subdiﬀerential of g at x is deﬁned by

∂p(x) = {g ∈ Rn : p(y) ≥ p(x) + (cid:104)g, y − x(cid:105), ∀y ∈ Rn}.

The proximal mapping of p is the operator deﬁned by
(cid:27)

(cid:26)

Proxp(x) = argmin

u∈Rn

p(u) +

1
2

(cid:107)u − x(cid:107)2

, ∀x ∈ Rn.

Next, we summarize some properties about the proximal mapping.

A variable metric and extrapolated proximal DCA with backtracking

5

Lemma 2.2 (see e.g., [5]) Let p : Rn → (−∞, ∞] be a proper closed and
convex function. Then
– Proxp(x) is a singleton for any x ∈ Rn.
– Proxp is nonexpansive, i.e., for any x and y it holds that

(cid:107)Proxp(x) − Proxp(y)(cid:107) ≤ (cid:107)x − y(cid:107).

– If z = Proxp(x), then x − z ∈ ∂p(z).

Note that if p is the indicator function δY of a nonempty and closed convex
set Y , i.e., δY (x) is equal to 0 if x ∈ Y and ∞ otherwise, then ProxδY is the
projection operator over Y, denoted as ΠY . The notations ProxQ
Y (Q
is a positive deﬁnite matrix) are adopted, the former actually means

p and ΠQ

ProxQ

p (x) = argmin
u∈Rn

(cid:26)

p(u) +

1
2

(cid:27)

(cid:107)u − x(cid:107)2
Q

, ∀x ∈ Rn.

In this case, z = ProxQ

p (x) implies that x − z ∈ Q−1∂p(z).
For two vectors x, y ∈ Rn, their Hardmard product, denoted as x (cid:12) y, is
the vector comprising the component-wise products: x (cid:12) y = (xiyi)n
i=1. The
nonnegative orthant is denoted by Rn
+. Next, diag(x) denotes the n×n diagonal
matrix with its diagonal vector being x. The set of all n×n symmetric matrices
is denoted as Sn; moreover, Sn
++) means the set of all n × n positive
semideﬁnite (or positive deﬁnite) matrices. For D1, D2 ∈ Sn, D1 (cid:22) D2 means
+. For µ > 0, Dµ denotes the set {D ∈ Sn : µI (cid:22) D}, where
that D2 − D1 ∈ Sn
I is the identity matrix.

+ (or Sn

Finally, we present a technical lemma, adapted from [8, Proposition A.31],
which will be utilized for establishing the convergence property of our proposed
method SPDCAe for (P ) and its convex version SFISTA for (Pc).

Lemma 2.3 ([8]) Let {ak}, {ζk}, {ξk}, and {γk} be nonnegative sequences of
real numbers such that

ak+1 ≤ (1 + ζk)ak − ξk + γk,

k = 1, 2, · · · ,

and

∞
(cid:88)

k=1

ζk < ∞,

∞
(cid:88)

k=1

γk < ∞.

Then {ak} converges to a ﬁnite value and {ξk} converges to 0.

3 Problem formulation and the proposed algorithms

In this section, we ﬁrst introduce the DC optimization problem we focus on in
this paper and present our proposed algorithm SPDCAe for (P ), which incor-
porates the Nesterov’s extrapolation, the backtracking line search (monotone
and non-monotone), and the variable metric method. Next, we describe in

6

Yu You, Yi-Shuai Niu

detail the algorithmic procedure of SFISTA, which is the convex version of
SPDCAe with non-monotone backtracking for (Pc).

In this paper, we speciﬁcally focus on the composite DC program

min{F (x) := f (x) + g(x) − h(x) : x ∈ Rn}.

(P )

Some assumptions for the involved functions are summarized in Assumption 1.
Note that we do not follow the assumption of the two accelerated algorithms
ADCA and pDCAe developed respectively in [50, 54] that f is real-valued over
the whole space since the scope of the model can be limited in this way. For
example, the loss function in the Poisson noise model [18] is the generalized
Kullback-Leibler divergence, whose eﬀective domain is larger than the non-
negative orthant but not equal to the whole space. Indeed, their algorithms can
be generalized for this situation by additionally projecting the extrapolation
point onto the set Y at each iteration. We describe in Algorithm 1 our proposed

Algorithm 1: SPDCAe for (P )

Input: η > 1, L > 0. Set x0 = x−1 ∈ Y ⊆ Rn.

1 for k = 1, 2, 3, · · · do

compute any hk−1 ∈ ∂h(xk−1) and pick Lk ≥ L;
set i = 0;
repeat

(1) update Lk = ηiLk and set tk = 1/Lk;
(2) pick βk ∈ [0, 1) and Dk ∈ Sn
yk = ΠDk
(3) compute xk = ProxDk
(4) update i = i + 1;

(cid:0)xk−1 + βk(xk−1 − xk−2)(cid:1);
tkg(yk − tkD−1

Y

++, calculate

k [∇f (yk) − hk−1]);

until f (xk) ≤ f (yk) + (cid:104)∇f (yk), xk − yk(cid:105) + 1
2tk

(cid:107)xk − yk(cid:107)2

Dk

;

2

3

4

5

6

7

8

9

10 end

algorithm SPDCAe for solving problem (P ). The parameters βk, Lk, and Dk
remain to be speciﬁed. Later, we will prove that with suitable selections of
parameters, any limit point of the sequence generated by SPDCAe, denoted
as x∗, is a critical point of problem (P ), i.e.,

0 ∈ ∇f (x∗) + ∂g(x∗) − ∂h(x∗).

At the moment, we just assume the following for the scaling matrices {Dk}
and defer the speciﬁed selection to the numerical part in Sect. 5.

Assumption 3 The scaling matrices {Dk} satisfy that

{Dk} ⊆ Dµ, Dk+1 (cid:22) (1 + ηk)Dk,

k = 1, 2, · · · ,

where µ > 0 and {ηk} is a nonnegative sequence such that (cid:80)∞

k=1 ηk < ∞.

A variable metric and extrapolated proximal DCA with backtracking

7

Remark 3.1 Assumption 3 implies that there exists c > 0 such that

µI (cid:22) Dk (cid:22) cD1,

k = 1, 2, · · · ,

where c = (cid:81)∞

k=1(1 + ηk).

To determine βk and Lk, we distinguish the next two backtracking strategies:

• Non-monotone backtracking Here, we use the heuristic method sug-
gested in [53] to determine {Lk}. Speciﬁcally, let T1 be a positive integer,
the initial guess Lk (as in line 2 of Algorithm 1) is set as the previously
returned Lk−1 if k is not divisible by T1 and as ρLk otherwise, where
ρ ∈ (0, 1). Next, two common methods for updating {βk} is as follows:
– Contract method sets β1 = 0 and βk = θk−1−1

δ for k ≥ 2, where
δ ∈ (0, 1) is a contractive factor (generally close to 1) and {θk}k≥1 is
the sequence recursively deﬁned by

θk

θ1 = 1,

θl =

(cid:113)

1 +

1 + 4θ2

l−1Ll/Ll−1

2

,

l = 2, 3, · · · .

(3)

– Restart method updates {βk} via the ﬁxed restart scheme or the
ﬁxed and adaptive restart scheme proposed in [41]. Speciﬁcally, let T2
be a positive integer. For the ﬁxed restart scheme, one starts to update
{θk} by (3) and obtain β1 = 0 and βk = θk−1−1
for k ≥ 2, then reset
θk+1 = 1 if T2 is divisible by k. For the ﬁxed and adaptive restart
scheme, the restart condition shall be modiﬁed as either T2 is divisible
by k or (cid:104)xk − xk−1, yk − xk(cid:105) > 0.

θk

• Monotone backtracking In this situation, the initial trail Lk is set as
the previously returned Lk−1. Then, about the update of {βk}, the only
diﬀerence from the non-monotone one is that (3) should be modiﬁed as

θ1 = 1,

θl = (1 +

(cid:113)

1 + 4θ2

l−1)/2,

l = 2, 3, · · · .

Remark 3.2 1. The restart schemes as described above were also used in [54]
for their proposed algorithm pDCAe. Moreover, our SPDCAe with mono-
tone backtracking is exactly pDCAe if the domain of f is the whole space,
Lk is selected as the smoothness parameter of f , and the scaling matrix
Dk is set as the identity matrix at each iteration.

2. For SPDCAe with monotone backtracking, xk−1 + βk(xk−1 − xk−2) can be

computed out of the inner loop.

3. The inner loop in Algorithm 1 can stop in ﬁnite step [10].
4. For the monotone backtracking, Lk ≥ L can be omitted, besides, the
boundedness of {Dk} as in Remark 3.1 can imply that 0 < lim inf Lk ≤
lim sup Lk < ∞; while for the non-monotone one, Lk ≥ L guarantees that
lim inf Lk > 0, and thus 0 < lim inf Lk ≤ lim sup Lk < ∞.

8

Yu You, Yi-Shuai Niu

Algorithm 2: SFISTA with non-monotone backtracking for (Pc)

Input: θ0 = 1, t0 = 0, η > 1. Set x0 = x−1 ∈ Y ⊆ Rn.

1 for k = 1, 2, 3, · · · do
pick Lk > 0;
2
set i = 0;
repeat

3

4

(1) update Lk by ηiLk and set tk = 1/Lk;
(cid:113)

1+

1+4θ2

k−1tk−1/tk
2

(2) compute θk =
(3) pick Dk ∈ Sn
(4) compute xk = ProxDk
(5) update i by i + 1.

;
++, calculate yk = ΠDk
tkg(yk − tkD−1

(cid:16)

xk−1 +
Y
k ∇f (yk));

θk−1−1
θk

(xk−1 − xk−2)

(cid:17)
;

until f (xk) ≤ f (yk) + (cid:104)∇f (yk), xk − yk(cid:105) + 1
2tk

(cid:107)xk − yk(cid:107)2

Dk

;

5

6

7

8

9

10

11 end

Next, we describe in Algorithm 2 the algorithmic procedure of SFISTA with
non-monotone backtracking line search (the convex version of SPDCAe with
non-monotone backtracking). Note that SFISTA with monotone backtracking
is exactly SFBEM proposed in [10] and thus we omit to present it here. In
[53], Scheinberg et al. showed that for some practical applications, such as the
compressed sensing and the logistic regression problems, the local Lipschitz
constant of the gradient of loss function is generally smaller than the global
one, especially when approaching the tail. Thus, the monotone backtracking
will lead to slow convergence near the solution set, while the non-monotone
backtracking helps to overcome this issue to some extent. Later, we will show
that SFISTA is equipped with the optimal convergence rate Φ(xk) − Φ∗ ≤
O(1/k2), which coincides with that of SFBEM and the classical FISTA [6].

4 Convergence analysis

In this section, we will investigate the convergence property of SPDCAe for (P )
and its convex version SFISTA for (Pc). For the former, we demonstrate the
subsequential convergence to a critical point; while for the latter, the optimal
convergence rate Φ(xk) − Φ∗ ≤ O(1/k2) is established.

The convergence analyses of both SPDCAe and SFISTA are based on the

following key inequality.

Proposition 4.1 Suppose that f , g, and h satisfy properties (a), (b), and (c)
of Assumption 1. For any x ∈ Rn, h ∈ ∂h(x), y ∈ Y , D ∈ Sn
++, and t > 0
satisfying

f (¯y) ≤ f (y) + (cid:104)∇f (y), ¯y − y(cid:105) +

(cid:107)¯y − y(cid:107)2

D,

(4)

1
2t

where ¯y := ProxD

tg(y − tD−1[∇f (y) − h]), it holds that

F (¯y) ≤ F (x) +

1
2t

(cid:107)x − y(cid:107)2

D −

1
2t

(cid:107)x − ¯y(cid:107)2

D.

A variable metric and extrapolated proximal DCA with backtracking

9

Proof It follows from ¯y = ProxD
h] − ¯y ∈ tD−1∂g(¯y) (term 3 of Lemma 2.2), which has the equivalent form

tg(y−tD−1[∇f (y)−h]) that y−tD−1[∇f (y)−

D(y − ¯y) − t[∇f (y) − h] ∈ t∂g(¯y).

Then, it holds that

tg(x) ≥ tg(¯y) + (cid:104)x − ¯y, D(y − ¯y) − t[∇f (y) − h](cid:105).

(5)

Thus, we have

F (¯y) = f (¯y) + g(¯y) − h(¯y)

(4),(5)

≤ f (y) + (cid:104)∇f (y), ¯y − y(cid:105) +

1
2t

(cid:107)¯y − y(cid:107)2
D

+ g(x) −

1
t

(cid:104)x − ¯y, D(y − ¯y) − t[∇f (y) − h](cid:105) − h(¯y)

=f (y) + g(x) + (cid:104)∇f (y), x − y(cid:105) +

− (cid:104)x − ¯y, h(cid:105) − h(¯y)

1
t

(cid:104)x − ¯y, D(¯y − y)(cid:105) +

1
2t

(cid:107)¯y − y(cid:107)2
D

≤F (x) +

≤F (x) +

=F (x) +

1
t
1
t
1
2t

(cid:104)x − ¯y, D(¯y − y)(cid:105) +

(cid:104)x − ¯y, D(¯y − y)(cid:105) +

(cid:107)x − y(cid:107)2

D −

1
2t

(cid:107)x − ¯y(cid:107)2

D,

1
2t
1
2t

(cid:107)¯y − y(cid:107)2

D − (cid:104)x − ¯y, h(cid:105) − h(¯y) + h(x)

(cid:107)¯y − y(cid:107)2
D

where the second inequality follows by f (y) + (cid:104)f (y), x − y(cid:105) ≤ f (x), the last
inequality holds since h ∈ ∂h(x) and thus h(¯y) ≥ h(x) + (cid:104)¯y − x, h(cid:105).
(cid:117)(cid:116)

4.1 Subsequential convergence of SPDCAe

Now, we start to establish the subsequential convergence of SPDCAe with
non-monotone backtracking and the update rule for {βk} as introduced in
Sect. 3. The proof for SPDCAe with monotone backtracking follows similarly
and thus omitted.

Theorem 4.1 Suppose that Assumption 1 and Assumption 3 hold. Let {xk}
be the sequence generated by Algorithm 1. Then, the following statements hold:

(i) The sequence {F (xk)} converges and lim
k→∞

(cid:107)xk − xk−1(cid:107) = 0;

(ii) If {xk} is bounded, then any limit point of {xk} is a critical point of (P ).

Proof (i) Let k ≥ 1. Replace x, h, y, ¯y, D, t in Proposition 4.1 by xk−1, hk−1,
yk, xk, Dk, tk, respectively. Then, we obtain

F (xk) ≤ F (xk−1) +

1
2tk

(cid:107)xk−1 − yk(cid:107)2

Dk

−

1
2tk

(cid:107)xk − xk−1(cid:107)2

Dk

.

(6)

10

Yu You, Yi-Shuai Niu

Note that yk = ΠDk
Y

(cid:0)xk−1 + βk(xk−1 − xk−2)(cid:1) and xk−1 ∈ Y , thus we have

(cid:107)xk−1 − yk(cid:107)2

Dk

≤ β2

k(cid:107)xk−1 − xk−2(cid:107)2
Dk

.

(7)

Plunge (7) into (6) and then consider the update rule for {βk}, it is easy to
check that for k ≥ 2,

F (xk) ≤ F (xk−1) +

αk
2tk−1

(cid:107)xk−1 − xk−2(cid:107)2

Dk

−

1
2tk

(cid:107)xk − xk−1(cid:107)2

Dk

,

where αk = δ2(1 − 1/θk)(1 − 1/θk−1)2 (resp. αk = (1 − 1/θk)(1 − 1/θk−1)2) for
the contract method (resp. restart method) for updating {βk} as introduced in
Sect. 3. Clearly, for either of the selection methods, it holds that {αk} ⊆ [0, 1)
with supk αk < 1. Next, denote (cid:101)F (xk) = F (xk) − F ∗ + 1
, we
2tk
have

(cid:107)xk − xk−1(cid:107)2

Dk

(cid:101)F (xk) = F (xk) − F ∗ +

1
2tk

(cid:107)xk − xk−1(cid:107)2

Dk

≤F (xk−1) − F ∗ +

≤F (xk−1) − F ∗ +

1
2tk−1
1
2tk−1

(cid:107)xk−1 − xk−2(cid:107)2

Dk

−

1 − αk
2tk−1

(cid:107)xk−1 − xk−2(cid:107)2

Dk

(cid:107)xk−1 − xk−2(cid:107)2

(1+ηk−1)Dk−1

−

1 − αk
2tk−1

(cid:107)xk−1 − xk−2(cid:107)2

Dk

≤(1 + ηk−1) (cid:101)F (xk−1) −

1 − αk
2tk−1

(cid:107)xk−1 − xk−2(cid:107)2

Dk

,

where the second inequality follows by Assumption 3 for {Dk}. Based on the
facts that supk αk < 1 and {tk} is bounded from above (term 4 of Remark
3.2), it follows that the sequence { 1−αk
}k≥2 has positive limit inferior. Then
2tk−1
invoking Lemma 2.3, it holds that

lim
k→∞

(cid:107)xk−1 − xk−2(cid:107)2

Dk

= 0,

(8)

and { (cid:101)F (xk)} converges to a ﬁnite value. Clearly, we have {F (xk)} converges
to a ﬁnite value. Besides, {Dk} satisﬁes that µI (cid:22) Dk for k ≥ 1. Thus,
≥ µ(cid:107)xk−1 −xk−2(cid:107)2, then (8) implies lim
(cid:107)xk−1 −xk−2(cid:107) = 0.
(cid:107)xk−1 −xk−2(cid:107)2
Dk
k→∞
(cid:107)xk − xk−1(cid:107) = 0 and µI (cid:22) Dk for all k, we
(ii) Take into account that lim
k→∞
(cid:107)xk − yk(cid:107) = 0. Let ¯x be any limit point of {xk},

obtain from (7) that lim
k→∞

yki = ¯x.
then there exists a convergent subsequence such that lim
i→∞
Moreover, because the sequence {hki−1} is bounded in Rn, we can assume
without loss of generality that {hki−1} is convergent with limit ¯z. Then, it
follows from hki−1 ∈ ∂h(xki−1) for all i, hki−1 → ¯z, xki−1 → ¯x, and the
closedness of the graph ∂h that ¯z ∈ ∂h(¯x) [52, Theorem 24.4] . On the other
hand, xk = ProxDk

k [∇f (yk) − hk−1]) implies that

tkg(yk − tkD−1

xki = lim
i→∞

1
tki

Dki(yki − xki) − [∇f (yki) − hki−1] ∈ ∂g(xki ).

(9)

A variable metric and extrapolated proximal DCA with backtracking

11

Consider also {Dk} bounded from above (Remark 3.1), thus the left-hand side
of (9) converges to −∇f (¯x) + ¯z. Combining this with the facts that {xki} → ¯x
and the closedness of the graph ∂g, we have

−∇f (¯x) + ¯z ∈ ∂g(¯x),

thus 0 ∈ ∇f (¯x) + ∂g(¯x) − ¯z. Then we conclude that ¯x is a critical point of
(P ) since ¯z ∈ ∂h(¯x).
(cid:117)(cid:116)

4.2 Optimal convergence rate O(1/k2) of SFISTA

In this part, we start to establish the optimal convergence rate of SFISTA
with non-monotone backtracking (Algorithm 2) for (Pc).

Theorem 4.2 Suppose that Assumption 2 holds. Let {xk} be the sequence
generated by Algorithm 2. Then, for any optimal solution x∗ of (Pc), the
following statements hold:

(i) For k ≥ 0, denote vk = xk−1 + θk(xk − xk−1). Then, for k ≥ 1 we have

tkθ2

k(Φ(xk)−Φ∗)+

1
2

(cid:107)x∗−vk(cid:107)2

Dk

≤ tk−1θ2

k−1(Φ(xk−1)−Φ∗)+

1
2

(cid:107)x∗−vk−1(cid:107)2

Dk

.

(ii) If {Dk} satisﬁes Assumption 3, then there exists C > 0 such that for k ≥ 2

Φ(xk) − Φ∗ ≤

C
(k + 1)2 .

Proof (i) Let k ≥ 1. Note that in this situation h ≡ 0 and ∂h(x) = {0}.
Replace h, y, ¯y, D, t in Proposition 4.1 by 0, yk, xk, Dk, tk, respectively.
Then, we obtain

Φ(xk) ≤ Φ(x) +

= Φ(x) +

1
2tk
1
tk

(cid:107)x − yk(cid:107)2

Dk

−

1
2tk

(cid:107)x − xk(cid:107)2

Dk

(cid:104)x − xk, Dk(xk − yk)(cid:105) +

1
2tk

(cid:107)xk − yk(cid:107)2

Dk

.

(10)

Set x = xk−1 and x = x∗ respectively in (10), it follows that

Φ(xk) ≤Φ(xk−1) +

1
tk

(cid:104)xk−1 − xk, Dk(xk − yk)(cid:105) +

1
2tk

(cid:107)xk − yk(cid:107)2

Dk

,

Φ(xk) ≤Φ∗ +

1
tk

(cid:104)x∗ − xk, Dk(xk − yk)(cid:105) +

1
2tk

(cid:107)xk − yk(cid:107)2

Dk

.

12

Multiply the above two inequalities by 1 − θ−1
add them together, then we obtain

k and θ−1

k

Yu You, Yi-Shuai Niu

respectively and next

Φ(xk) − Φ∗ ≤(1 − θ−1

(cid:107)xk − yk(cid:107)2

Dk

k )(Φ(xk−1) − Φ∗) +

1
2tk
k )xk−1 − xk + θ−1

(cid:104)(1 − θ−1

+

1
tk

=(1 − θ−1

k )(Φ(xk−1) − Φ∗) +

k x∗, Dk(xk − yk)(cid:105)
1
2tk

(cid:107)(1 − θ−1

k )xk−1 + θ−1

k x∗ − yk(cid:107)2
Dk

−

1
2tk

Note that

(cid:107)(1 − θ−1

k )xk−1 + θ−1

k x∗ − xk(cid:107)2
Dk

.

(11)

yk = ΠDk
Y

= ΠDk
Y

(cid:18)

xk−1 +

(cid:0)(1 − θ−1

θk−1 − 1
θk
k )xk−1 + θ−1

k vk−1(cid:1)

(xk−1 − xk−2)

(cid:19)

and (1 − θ−1

k )xk−1 + θ−1
(cid:107)(1 − θ−1

k x∗ ∈ Y , thus

k )xk−1 + θ−1
moreover, vk = xk−1 + θk(xk − xk−1) implies that
k )xk−1 + θ−1

k x∗ − yk(cid:107)2
Dk

k x∗ − xk(cid:107)2
Dk

(cid:107)(1 − θ−1

≤ (cid:107)θ−1

= (cid:107)θ−1

k (x∗ − vk−1)(cid:107)2
Dk

,

(12)

k (x∗ − vk)(cid:107)2
Dk

.

(13)

Thus, combining (11), (12), and (13), we have

tkθ2

k(Φ(xk)−Φ∗)+

1
2

(cid:107)x∗−vk(cid:107)2

Dk

≤ tk(θ2

k−θk)(Φ(xk−1)−Φ∗)+

1
2

(cid:107)x∗−vk−1(cid:107)2

Dk

.

Moreover, the update of {βk} (line 6 of Algorithm 2) implies that θ2
tk−1/tkθ2
k−1 = 0. Plunge this into the above inequality, we prove (i).
k(Φ(xk) − Φ∗) + 1
2 (cid:107)x∗ − vk(cid:107)2
(ii) For k ≥ 1, deﬁne ak = tkθ2
. Thus,
Dk
1
2
(cid:107)x∗ − vk(cid:107)2

k+1(Φ(xk+1) − Φ∗) +

ak+1 = tk+1θ2

(cid:107)x∗ − vk+1(cid:107)2

k(Φ(xk) − Φ∗) +

≤ tkθ2

Dk+1

Dk+1

k − θk −

≤ tkθ2

k(Φ(xk) − Φ∗) +

(cid:107)x∗ − vk(cid:107)2

Dk

≤ (1 + ηk)(tkθ2

k(Φ(xk) − Φ∗) +

1
2

(cid:107)x∗ − vk(cid:107)2

Dk

)

= (1 + ηk)ak,

where the ﬁrst inequality follows from the targeted inequality in term (i).
Then, invoking Lemma 2.3 we conclude that the sequence {ak} converges and
thus bounded. Let C1 > 0 satisfy ak ≤ C1 for all k, then we have for k ≥ 1

Φ(xk) − Φ∗ ≤

C1
tkθ2
k

.

1
2
1 + ηk
2

A variable metric and extrapolated proximal DCA with backtracking

13

Now, for k ≥ 2 we have

(cid:112)tk−1θk−1 = (cid:112)θk(θk − 1)tk ≤

√

tkθk −

√

tk
2

.

Summing the above inequality from 2 to k and then by rearranging, we have
√

√

√

tk. Note tmin := inf{tk} > 0, thus for k ≥ 2,

tkθk ≥

t1 + 1
2

(cid:80)k

i=2

1
tkθ2
k

≤

√

(

t1 + 1
2

1
(cid:80)k

i=2

≤

√

tk)2

4
(k + 1)2tmin

.

Then, let C = 4C1/tmin, it holds that for all k ≥ 2

Φ(xk) − Φ∗ ≤

C
(k + 1)2 .

Therefore, we obtain the optimal convergence rate.

(cid:117)(cid:116)

5 Numerical results

In this section, we conduct numerical simulations on problems of sparse bi-
nary logistic regression and compressed sensing with Poisson noise. All exper-
iments are implemented in MATLAB 2019a on a 64-bit PC with an Intel(R)
Core(TM) i5-6200U CPU (2.30GHz) and 8GB of RAM.

5.1 Sparse binary logistic regression

In this part, we consider the following DC regularized sparse binary logistic
regression model:

min
x∈Rn

{

1
m

m
(cid:88)

i=1

log(1 + exp(−bi(aT

i x))) + λ(cid:107)x(cid:107)1 − λ(cid:107)x(cid:107)2},

(14)

i=1 log(1 + exp(−bi(aT

where λ > 0 is a regularization parameter and {(a1, b1), · · · , (am, bm)} is a
training set with observations ai ∈ Rn and labels bi ∈ {−1, 1}. This ﬁts (P )
with f (x) = (cid:80)m
i x))), g(x) = λ(cid:107)x(cid:107)1, and h(x) = λ(cid:107)x(cid:107)2.
Here, we use (cid:96)1−2 regularizer [55] for promoting sparsity; see [12] for alternative
nonconvex regularizers. For this model, Y in Algorithm 1 is the whole space
and ΠDk
is the identity operator. Besides, the computation about the proximal
Y
map ProxDk
tkg has closed form, which can be found in [5, Example 6.8].
Two tested datasets, w8a and CINA, obtained from libsvm [13] are utilized
to show the performances of our proposed method SPDCAe (Algorithm 1).
We compare four algorithms for (14): the ﬁrst two are versions derived from
SPDCAe, the other are pDCAe [54] and ADCA [50]. Note that all of these
algorithms are equipped with Nesterov’s extrapolation, and here wo do not
compare that out of this type, such as pDCA [17] and the general iterative

14

Yu You, Yi-Shuai Niu

shrinkage and thresholding algorithm (GIST) [16], since in general these meth-
ods do not work better than that with extrapolation; see [54] for a comparison
of pDCA, pDCAe, and GIST. Next, we discuss the implementation details of
the involved algorithms below:

• SPDCAe1 is SPDCAe with scaling and non-monotone backtracking line
search. The scaling matrix Dk is selected by the following procedure. Let
gk = ∇f (yk) and ¯gk = (cid:80)k
i=1 gk (cid:12) gk, then set
(cid:113)

(cid:19)(cid:19)(cid:19)

(cid:18)

(cid:18)

Dk = diag

max

, min

γk,

¯gk + εen

,

(cid:18) 1
γk

(cid:113)

1 + 1013

where ε = 10−6 and γk =
(k+1)2 . Here, all of the above operators are
conducted componentwise. Note that the update of the scaling matrices as
above is motivated by that in the adaptive gradient (AdaGrad) algorithm
[14]. The selection of {γk} can guarantee that {Dk} satisﬁes Assumption 3
since {Dk} converges to the identity matrix. In addition, L is set as 10−10.
The factor η is set as 2 and the initial guesses L1 for the datasets are ﬁxed
as 1; then for k ≥ 2, the initial guess Lk is selected as Lk−1/2 if k is not
divisible by 5, otherwise as Lk−1. The selection of extrapolation parameter
{βk} uses the ﬁxed (T2 = 200) and adaptive restart scheme as introduced
in Sect. 3.

• PDCAe1 is SPDCAe without scaling and with non-monotone backtrack-
ing line search. Compared with that implementation of SPDCAe1, two
modiﬁcations are in order. First, the initial guesses L1 for the datasets are
set as 0.1. Second, the scaling matrix is the identity matrix.

• pDCAe also uses the ﬁxed (T2 = 200) and restart scheme for updating
{βk}. The smoothness parameter of logistic loss f is estimated by comput-
ing the largest eigenvalue of the Hessian matrix of f .

• ADCA uses the same bound of smoothness parameter of f as that in

pDCAe. Besides, the parameter q in ADCA is set as 3.

In our experiments, λ are all set as 10−3. The initial point for all of
the involved algorithms is randomly generated by the MATLAB command
rand(n,1). We stop the algorithms via the relative error (F (xk) − F ∗)/F ∗ ≤
tol, where x∗ is an approximately solution derived by running PDCAe1 for
10000 iterations and F ∗ is its corresponding function value.

Table 1 demonstrates the performances for w8a and CINA datasets. The
total number of iterations and CPU time (in seconds) averaged on 10 runs
for tol ∈ {10−2i : i ∈ {1, 2, 3, 4}} are reported. The mark “Max” means the
number of iterations exceeds 10000. We visualize in Fig. 1 the trend of the
relative error with respect to the CPU time. It is observed tat SPDCAe1
performs the best among the involved algorithms, hitting all of the tolerances
with the least amount of time and the least number of iterations. On the
contrary, pDCAe does not work well, especially for CINA dataset. For w8a
dataset, PDCAe1 is better than ADCA; while for CINA dataset the result
is opposite. It seems that ADCA converges very fast when approaching the

A variable metric and extrapolated proximal DCA with backtracking

15

tail. Next, by comparing SPDCAe1 with PDCAe1, the beneﬁt from scaling
is indicated; while by comparing PDCAe1 and pDCAe, we observe that the
non-monotone backtracking promotes the performance. Finally, consider that
SPDCAe1 performs the best, the beneﬁts from both scaling and the non-
monotone backtracking are indicated.

Table 1 Total number of iterations and CPU time (in seconds) averaged on 10 runs for
w8a and CINA datasets. Bold values correspond to the best results for each dataset

Dataset Algorithm

w8a

CINA

SPDCAe1
PDCAe1
pDCAe
ADCA
SPDCAe1
PDCAe1
pDCAe
ADCA

tol : 10−2

tol : 10−4

tol : 10−6

tol : 10−8

Time
0.11
0.20
0.84
1.03
0.16
0.47
3.43
0.91

Time
0.18
0.32
2.93
2.12
0.49
1.42

Iter.
Time
Iter.
50
0.23
18
89
0.38
36
1571
5.84
148
739
3.08
153
1524
0.72
186
5964
3.56
743
5781 Max Max Max Max Max Max
3152
1.76
1082

Iter.
41
70
1113
486
1059
3799

Time
0.29
0.48
7.75
4.24
1.01
3.89

Iter.
32
57
587
356
684
1992

1656

2467

2.28

1.29

(a) w8a

(b) CINA

Fig. 1 Trend of the relative error for w8a and CINA datasets

5.2 Compressed sensing with Poisson noise

In this subsection, we consider the problem of recovering the sparse signal
corrupted by Poisson noise. Speciﬁcally, we assume the observed data b ∈
Rm is the realization of a Poisson random vector with expected values being
Axtrue + bg, where xtrue is the (actually unknown) signal of interest, A ∈
Rm×n is the measurement matrix, and bg (a small scalar close to 0) is a
positive background. In case of Poisson noise, the generalized Kullback-Leibler

012345678910Time10-1010-810-610-410-2100102( F(xk)-F*)/F*SPDCAe1PDCAe1pDCAeADCA024681012Time10-1010-810-610-410-2100102( F(xk)-F*)/F*SPDCAe1PDCAe1pDCAeADCA16

divergence

Yu You, Yi-Shuai Niu

KL(x) :=

n
(cid:88)

(cid:26)

i=1

bi log

bi
(Ax + bg)i

+ (Ax + bg)i − bi

(cid:27)

is used to measure the distance of x from the observed data b. To recover the
true solution xtrue, we use the following DC optimization model

min
x∈Rn
+

{KL(x) + λ(cid:107)x(cid:107)1 − λ(cid:107)x(cid:107)2},

(15)

where λ > 0 is a regularization parameter. In [10], the (cid:96)1 norm is utilized for
inducing sparsity, here we use (cid:96)1−2 penalty instead. Then (15) matches (P )
with f (x) = KL(x), g(x) = λ(cid:107)x(cid:107)1 + δR+(x), and h(x) = λ(cid:107)x(cid:107)2. Thus Y in
Algorithm 1 is the nonnegative orthant and ΠDk
is not up to the underling
Y
inner product. Besides, the computation about the proximal map ProxDk
tkg has
closed form, which can be found in [5, Lemma 6.5].

We use the same procedure in [10] to generate the experimental data. The
measurement matrix A actually depends on a probability, which is set as 0.9
in our test. For reader’s convenience, we describe the procedure below:

– The matrix A has been generated as detailed in [51] so that A preserves
both the positivity and the ﬂux of any signal (i.e., if z ≥ 0, then (Az)i ≤
(cid:80)n

– The signal xtrue ∈ R5000 has all zeros except for 20 nonzeros entries drawn

i=1 zi).

uniformly in the interval [0, 105].

– The observed signal b ∈ R1000 has been obtained by corrupting the vector
Axtrue + bg (bg = 10−10) by means of the MATLAB imnoise function.

Note that in this situation, the estimate of the bound of the smoothness pa-
rameter of KL is problematic since it is an extremely large number [18]. Thus,
taking into account the eﬃciency, the aforementioned pDCAe and ADCA are
not applicable; however, SPDCAe is still valid, from which we derive four ver-
sions: SPDCAe1, PDCAe1, SPDCAe0, and PDCAe0. The ﬁrst two have al-
ready been introduced in the above subsection. Here, SPDCAe0 and PDCAe0
are their counterparts with monotone backtracking line search. Speciﬁcally, the
ﬁrst is SPDCAe with scaling and monotone backtracking, while the second is
SPDCAe without scaling and with monotone backtracking. Our intention here
is to show the eﬀect from incorporating both the variable metric method and
the non-monotone backtracking.

In our experiment, λ is set as 10−3. The scaling matrices for SPDCAe1

and SPDCAe0 have been selected by writing the gradient of KL as

−∇KL(x) = UKL(x) − VKL(x)

with UKL(x) ≥ 0 and VKL(x) > 0; see [21] for a detailed description of such a
decomposition. Then Dk is deﬁned as

(cid:18)

Dk = diag

max

(cid:18)

, min

γk,

(cid:18) 1
γk

yk
VKL(yk)

(cid:19)(cid:19)(cid:19)−1

A variable metric and extrapolated proximal DCA with backtracking

17

(cid:113)

1 + 1013

(k+1)2 . Next for SPDCAe1 and PDCAe1, L is set as 10−10;
with γk =
besides, the factor η in SPDCAe1 and PDCAe1 is set as 2, while that in
SPDCAe0 and PDCAe0 is set as 1.2. The initial guesses L1 for SPDCAe1 and
SPDCAe0 are set as 0.1, and that for PDCAe1 and PDCAe0 are set as 10−5.
Moreover, for k ≥ 2, the initial guesses for SPDCAe1 and PDCAe1 are selected
as Lk−1/2 if k is not divisible by 5, otherwise as Lk−1. Next for the involved
algorithms, the initial point is the n-dimensional column vector of all ones
and the extrapolation parameter {βk} is all updated via the ﬁxed (T2 = 200)
and adaptive restart scheme. We stop the algorithms via the relative error
(F (xk) − F ∗)/F ∗ ≤ tol, where x∗ is an approximately solution derived by
running PDCAe1 for 10000 iterations and F ∗ is its corresponding function
value.

In Table 2, we present in detail the total number of iterations and CPU
time (in seconds) averaged on 10 runs for tol ∈ {10−i : i ∈ {1, 2, · · · , 5}}.
The mark “Max” means the number of iterations exceeds 10000. Here, we ob-
serves that among all of the involved algorithms, PDCAe0 performs the worst,
which requires 2910 steps to reach the tolerance tol = 10−1 and always hits
the maximum iterations 10000 for other tolerances; on the contrary, SPDCAe1
performs the best, hitting all of the tolerances with the least amount of time
and the least number of iterations. Next in Fig. 2(a), the trend of the relative
error with respect to the CPU time is plotted. Here, by comparing the trends of
SPDCAe1 and SPDCAe0 (also PDCAe1 and PDCAe0) we observe the beneﬁt
from introducing the non-monotone backtracking; next by comparing SPD-
CAe1 and PDCAe1 (also SPDCAe0 and PDCAe0), the beneﬁt from scaling
is indicated; besides, by comparing SPDCAe0 and PDCAe1 we observe that
the former beneﬁts much from the scaling procedure and later is worse than
PDCAe1. Anyway, SPDCAe1 works the best, indicating the beneﬁt from both
the scaling method and the non-monotone backtracking. Finally, we observe
from Fig. 2(b) that the solution recovered by SPDCAe1 is very close to the
true solution.

As a conclusion, for the Poisson denoising problem, SPDCAe1 could be
a promising algorithm since the beneﬁt from incorporating the inexpensive
diagonally scaling procedure for better local approximation and using the non-
monotone backtracking for adaptive step-sizes selection.

Table 2 Total number of iterations and CPU time (in seconds) averaged on 10 runs. Bold
values correspond to the best results

Algorithm

SPDCAe1
PDCAe1
SPDCAe0
PDCAe0

tol : 10−1

tol : 10−2

tol : 10−3

tol : 10−4

tol : 10−5

Time
0.22
0.57
0.56
19.66

Iter.
28
78
79

Iter.
Iter.
55
42
112
104
2508
456
2910 Max Max Max Max Max Max Max Max

Time
0.40
0.97
18.62

Time
0.31
0.81
3.25

Time
0.29
0.80
1.86

Time
0.39
0.96
6.91

Iter.
54
110
953

Iter.
38
101
298

18

Yu You, Yi-Shuai Niu

(a) Trend of relative error

(b) The true solution and the solution obtained
by SPDCAe1 under tol = 10−5

Fig. 2 Performance for the Poisson denoising problem

6 Conclusion

In this paper, we propose a DC programming algorithm, called SPDCAe, for
solving a composite DC program (P ), which incorporates the variable metric
method, the Nesterov’s extrapolation, and the backtracking line search (not
necessarily monotone). We establish the subsequential convergence to a criti-
cal point of SPDCAe under suitable selections of the extrapolation parameters
and the scaling matrices. Besides, we also demonstrate that the convex ver-
sion of SPDCAe for (Pc), denoted by SFISTA, enjoys the optimal O(1/k2)
convergence rate in function values. This rate of convergence coincides with
that of the well-known FISTA algorithm [6] and SFBEM [10] (a scaling ver-
sion of FISTA with monotone backtracking). Numerical simulations on sparse
binary logistic regression demonstrate the good performances of our meth-
ods (especially that with scaling and non-monotone backtracking) compared
with pDCAe [54] and ADCA [50]. Furthermore, for compressed sensing with
Poisson noise problem, both pDCAe and ADCA are not applicable, while our
algorithm is still valid, where we show the beneﬁts of including scaling and
non-monotone backtracking.

As further researches, ﬁrst, the global sequential convergence of SPDCAe
to a critical point under the Kurdyka-(cid:32)Lojasiewicz property [9,3,4] is worth
noting; second, the SPDCAe could be compared with other accelerated algo-
rithms, e.g., boosted-DCA [2,38], inertial-DCA [43, 56] and accelerated meth-
ods based on second-order ODE [32,15] for some suitable applications; more-
over, it deserves developing some ingenious procedures for both scaling and
adaptive step-sizes selection. Finally, it might be meaningful for designing
some inexact variants for addressing the case where the computation of the
proximal mapping of g can not be exactly conducted.

10-210-1100101102Time10-610-510-410-310-210-1100101( F(xk)-F*)/F*SPDCAe1PDCAe1SPDCAe0PDCAe00123456789101040500100015002000250030003500400045005000 true sol. recovered sol.A variable metric and extrapolated proximal DCA with backtracking

19

Acknowledgements This work is supported by the National Natural Science Foundation
of China (Grant 11601327).

References

1. Aleksandrov, A.D.: On the surfaces representable as diﬀerence of convex functions.

Sibirskie Elektronnye Matematicheskie Izvestiia 9, 360–376 (2012)

2. Artacho, F.J.A., Vuong, P.T.: The boosted dc algorithm for nonsmooth functions. SIAM

J. Optim. 30(1), 980–1006 (2020)

3. Attouch, H., Bolte, J.: On the convergence of the proximal algorithm for nonsmooth
functions involving analytic features. Mathematical Programming 116(1), 5–16 (2009)
4. Attouch, H., Bolte, J., Svaiter, B.F.: Convergence of descent methods for semi-algebraic
and tame problems: proximal algorithms, forward–backward splitting, and regularized
gauss–seidel methods. Mathematical Programming 137(1), 91–129 (2013)
5. Beck, A.: First-order methods in optimization. SIAM, Philadelphia (2017)
6. Beck, A., Marc, T.: A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM journal on imaging sciences 2(1), 183–202 (2009)

7. Beck, A., Vaisbourd, Y.: Globally solving the trust region subproblem using simple

ﬁrst-order methods. SIAM Journal on Optimization 28(3), 1951–1967 (2018)

8. Bertsekas, D.: Convex optimization algorithms. Athena Scientiﬁc (2015)
9. Bolte, J., Daniilidis, A., Lewis, A.: The (cid:32)lojasiewicz inequality for nonsmooth subana-
lytic functions with applications to subgradient dynamical systems. SIAM Journal on
Optimization 17(4), 1205–1223 (2007)

10. Bonettini, S., Porta, F., Ruggiero, V.: A variable metric forward-backward method with

extrapolation. SIAM J. Optim. 38(4), A2588–A2584 (2016)

11. Bonettini, S., Porta, F., Ruggiero, V., L., Z.: Variable metric techniques for forward-
backward methods in imaging. Journal of Computational and Applied Mathematics
385, 113192 (2021)

12. Cand`es, E.J., Wakin, M.B., Boyd, S.P.: Enhancing sparsity by reweighted (cid:96)1 minimiza-

tion. Journal of Fourier Analysis and Applications 14, 877–905 (2008)

13. Chang, C.C., Lin, C.J.: Libsvm: A library for support vector machines 2 (2011)
14. Duchi, J., Hazan, E., Singer, Y.: Adaptive subgradient methods for online learing and

stochastic optimization. J. Mach. Learn. Res 12, 2121–2159 (2011)

15. Fran¸ca, G., Robinson, D.P., Vidal, R.: Gradient ﬂows and proximal splitting methods:
A uniﬁed view on accelerated and stochastic optimization. Physical Review E 103(5),
053304 (2021)

16. Gong, P., Zhang, C., Lu, Z., Huang, J., Ye, J.: A general iterative shrinkage and thresh-
olding algorithm for non-convex regularized optimization problems. pp. 37–45. PMLR
(2013)

17. Gotoh J. Y., T.A., Tono, K.: DC formulations and algorithms for sparse optimization

problems. Mathematical Programming 169(1), 141–176 (2018)

18. Harmany, Z.T., Marcia, R.F., M, R.: This is spiral-tap: Sparse poisson intensity recon-
struction algorithms–theory and practice. In: IEEE Transactions on Image Processing,
pp. 1084–1096 (2011)

19. Hiriart-Urruty, J.B.: Generalized diﬀerentiability/duality and optimization for problems
dealing with diﬀerences of convex functions. In: Convexity and duality in optimization,
pp. 37–70. Springer (1985)

20. Horst, R., Thoai, N.V.: DC programming: Overview. Journal of Optimization Theory

and Applications 103, 1–43 (1999)

21. Lant´eri, H., Roche, M., Cuevas, O., Aime, C.: A general method to devise maximum-
likelihood signal restoration multiplicative algorithms with non-negativity constraints.
Signal Processing 81(5), 945–974 (2001)

22. Le Thi, H., Pham Dinh, T.: A continuous approach for large-scale constrained quadratic

zero-one programming. Optimization 45(3), 1–28 (2001)

23. Le Thi, H.A., Belghiti, M.T., Pham, D.T.: A new eﬃcient algorithm based on dc pro-
gramming and dca for clustering. Journal of Global Optimization 37(4), 593–608 (2007)

20

Yu You, Yi-Shuai Niu

24. Le Thi, H.A., Moeini, M., Pham, D.T., Judice, J.: A dc programming approach for solv-
ing the symmetric eigenvalue complementarity problem. Computational Optimization
and Applications 51(3), 1097–1117 (2012)

25. Le Thi, H.A., Pham, D.T.: Large-scale molecular optimization from distance matrices

by a dc optimization approach. SIAM Journal on Optimization 14(1), 77–114 (2003)

26. Le Thi, H.A., Pham, D.T.: The dc (diﬀerence of convex functions) programming and
dca revisited with dc models of real world nonconvex optimization problems. Annals of
operations research 133(1-4), 23–46 (2005)

27. Le Thi, H.A., Pham, D.T.: DC programming and DCA: thirty years of developments.

Mathematical Programming 169(1), 5–68 (2018)

28. Le Thi, H.A., Pham, D.T., Le, H.M., Vo, X.T.: DC approximation approaches for sparse

optimization. European Journal of Operational Research 244(1), 26–46 (2015)

29. Nesterov, Y.E.: A method for solving the convex programming problem with conver-

gence rate O(1/k2). In: Dokl. akad. nauk Sssr, vol. 269, pp. 543–547 (1983)

30. Nesterov, Y.E.: Gradient methods for minimizing composite functions. Mathematical

Programming 140(1), 125–161 (2013)

31. Niu, Y.S.: Programmation dc et dca en optimisation combinatoire et optimisation poly-

nomiale via les techniques de sdp. Ph.D. thesis, INSA de Rouen, France (2010)

32. Niu, Y.S., Glowinski, R.: Discrete dynamical system approaches for boolean poly-
to appear in Journal of Scientiﬁc Computing, arXiv preprint

nomial optimization.
arXiv:1912.10221 (2019)

33. Niu, Y.S., J´udice, J., Le Thi, H.A., Pham, D.T.: Improved dc programming approaches
for solving the quadratic eigenvalue complementarity problem. Applied Mathematics
and Computation 353, 95–113 (2019)

34. Niu, Y.S., J´udice, J., Thi, H.A.L., Dinh, T.P.: Solving the quadratic eigenvalue comple-
mentarity problem by dc programming. In: Modelling, Computation and Optimization
in Information Systems and Management Sciences, pp. 203–214. Springer (2015)
35. Niu, Y.S., Pham, D.T.: A dc programming approach for mixed-integer linear programs.
In: International Conference on Modelling, Computation and Optimization in Informa-
tion Systems and Management Sciences, pp. 244–253. Springer (2008)

36. Niu, Y.S., Pham, D.T.: Dc programming approaches for bmi and qmi feasibility prob-
In: Advanced Computational Methods for Knowledge Engineering, pp. 37–63.

lems.
Springer (2014)

37. Niu, Y.S., Pham, D.T., Le Thi, H.A., Judice, J.J.: Eﬃcient dc programming approaches
for the asymmetric eigenvalue complementarity problem. Optimization Methods and
Software 28(4), 812–829 (2013)

38. Niu, Y.S., Wang, Y.J., Le Thi, H.A., Pham, D.T.: Higher-order moment portfolio op-
timization via an accelerated diﬀerence-of-convex programming approach and sums-of-
squares. arXiv :1906.01509 (2019)

39. Niu, Y.S., You, Y., Liu, W.Z.: Parallel dc cutting plane algorithms for mixed binary
In: World Congress on Global Optimization, pp. 330–340. Springer

linear program.
(2019)

40. Niu, Y.S., You, Y., Xu, W., Ding, W., Hu, J., Yao, S.: A diﬀerence-of-convex program-
ming approach with parallel branch-and-bound for sentence compression via a hybrid
extractive model. Optimization Letters 15(7), 2407–2432 (2021)

41. O’Donoghue, B., Cand`es, E.: Adaptive restart for accelerated gradient schemes. Foun-

dations of Computational Mathematics 15, 715–732 (2015)

42. de Oliveira, W.: The abc of dc programming. Set-Valued and Variational Analysis

28(4), 679–706 (2020)

43. de Oliveira, W., Tcheou, M.P.: An inertial algorithm for dc programming. Set-Valued

and Variational Analysis 27(4), 895–919 (2019)

44. Pham, D.T., Le Thi, H.A.: Convex analysis approach to d.c. programming: theory,

algorithms and applications. Acta Math. Vietnam. 22(1), 289–355 (1997)

45. Pham, D.T., Le Thi, H.A.: A dc optimization algorithm for solving the trust-region

subproblem. SIAM Journal on Optimization 8(2), 476–505 (1998)

46. Pham, D.T., Le Thi, H.A.: Recent advances in DC programming and DCA. In: Trans-

actions on Computational Intelligence XIII, pp. 1–37 (2014)

A variable metric and extrapolated proximal DCA with backtracking

21

47. Pham, D.T., Le Thi, H.A., Pham, V.N., Niu, Y.S.: Dc programming approaches for
discrete portfolio optimization under concave transaction costs. Optimization letters
10(2), 261–282 (2016)

48. Pham, D.T., Niu, Y.S.: An eﬃcient dc programming approach for portfolio decision
with higher moments. Computational Optimization and Applications 50(3), 525–554
(2011)

49. Pham, D.T., Souad, E.B.: Algorithms for solving a class of nonconvex optimization
problems. methods of subgradients. In: Fermat Days 85: Mathematics for Optimization,
vol. 129, pp. 249–271 (1986)

50. Phan, D.N., Le, H.M., Le Thi, H.A.: Accelerated diﬀerence of convex functions algorithm
and its application to sparse binary logistic regression. In: IJCAI, pp. 1369–1375 (2018)
51. Raginsky, M., Willett, R.M., Harmany, Z.T., Marcia, R.F.: Compressed sensing perfor-
mance bounds under poisson noise. In: IEEE Transactions on Signal Process, vol. 58,
pp. 3990–4002 (2010)

52. Rockafellar, R.T.: Convex analysis, vol. 36. Princeton university press (1970)
53. Scheinberg, K., Goldfarb, D., Bai, X.: Fast ﬁrst-order methods for composite convex
optimization with backtracking. Foundations of Computational Mathematics 14, 389–
417 (2014)

54. Wen, B., Chen, X., Pong, T.K.: A proximal diﬀerence-of-convex algorithm with extrap-

olation. Computational optimization and applications 69(2), 297–324 (2018)

55. Yin, P., Lou, Y., He, Q., Xin, J.: Minimization of (cid:96)1−2 for compressed sensing. SIAM

J. Sci. Comput. 37, A536–A563 (2016)

56. You, Y., Niu, Y.S.: A reﬁned inertial dc algorithm for dc programming. Optimization

and Engineering (2022)

