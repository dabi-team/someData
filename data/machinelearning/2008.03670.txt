0
2
0
2

g
u
A
9

]
i
c
s
-
l
r
t

m

.
t
a
m
-
d
n
o
c
[

1
v
0
7
6
3
0
.
8
0
0
2
:
v
i
X
r
a

Lattice Thermal Conductivity Prediction using
Symbolic Regression and Machine Learning

Christian Loftis
Department of Computer Science and Engineering
University of South Carolina, Columbia, SC, 29201, USA

Kunpeng Yuan
Department of Mechanical Engineering
University of South Carolina, Columbia, SC, 29201, USA
Key Laboratory of Ocean Energy Utilization and Energy Conservation of Ministry of Education
School of Energy and Power Engineering,Dalian University of Technology
Dalian, 116024, China

Yong Zhao
Department of Computer Science and Engineering
University of South Carolina, Columbia, SC, 29201, USA

Ming Hu*
Department of Mechanical Engineering
University of South Carolina, Columbia, SC, 29201, USA

Jianjun Hu *
Department of Computer Science and Engineering
University of South Carolina
jianjunh@cse.sc.edu

Abstract

Prediction models of lattice thermal conductivity (κL) have wide applications in the
discovery of thermoelectrics, thermal barrier coatings, and thermal management of
semiconductors. κL is notoriously difﬁcult to predict. While classic models such as
the Debye-Callaway model and the Slack model have been used to approximate the
κL of inorganic compounds, their accuracy is far from being satisfactory. Herein,
we propose a genetic programming based Symbolic Regression (SR) approach for
explicit κL models and compare it with Multi-Layer Perceptron neural networks
and a Random Forest Regressor using a hybrid cross-validation approach including
both K-Fold CV and holdout validation. Four formulae have been discovered by
our symbolic regression approach that outperform the Slack formula as evaluated
on our dataset. Through the analysis of our models’ performance and the formulae
generated, we found that the trained formulae successfully reproduce the correct
physical law that governs the lattice thermal conductivity of materials. We also
identiﬁed that extrapolation prediction remains to be a key issue in both symbolic
regression and regular machine learning methods and ﬁnd the distribution of the
samples place a key role in training a prediction model with high generalization
capability.

Preprint. Under review.

 
 
 
 
 
 
1

Introduction

Having the capability to predict lattice thermal conductivity (κL) of a crystalline material based on its
composition and structure information has wide applications in new materials discovery, and thus has
received noticeable attention in the thermodynamics ﬁeld [20, 3, 5]. It enables materials scientists
to screen materials with desired κL without having to synthesize the materials ﬁrst for testing. The
advantages of materials with both high and low κL abound. For example, materials with high κL
are desirable for conducting heat, and their uses range from being used for coolant pipes in nuclear
power plants to being used for heat sinks. κL is especially important for semiconductors, whose
electrical resistance rise as their temperature falls. Being able to optimize for thermal conductivity
independently of electron conductivity enables materials researchers to create electrical insulators
that conduct heat well, or inversely, to create electrical conductors that do not transfer heat well.
Possessing this degree of control over a material’s conductivity (both thermal and electric) will
allow researchers to synthesize materials for use in electronics that can transmit electricity easily yet
conduct less heat than other materials with the same electrical conductivity. This leads to electronics
that do not overheat as quickly, despite high transistor density. Slack and Morelli state that “Its
manipulation and control have impacted an enormous variety of technical applications, including
thermal management of mechanical, electrical, chemical, and nuclear systems; thermal barriers and
thermal insulation materials; more efﬁcient thermoelectric materials; and sensors and transducers."
[20]

The thermodynamics research ﬁeld has contributed several analytical models for calculating lattice
thermal conductivity of materials including the well-known Slack model (Formula 1) [20] and
Debye-Callaway Model (Formula 2) [3].

κL = A ·

(θe)3M 3(cid:112)V np
n4/3T γ2
a

κL,tot = A1

M v3
s
T V 2/3n1/3

+ A2

vs
V 2/3

(1 −

1
n2/3

)

(1)

(2)

While these models are insightful, comparison with experimentally measured thermal conductivity
has indicated that there are still plenty room for improvement [18, 1, 21]. They have also been shown
to be less accurate than machine learning models that have been developed to predict κL [5, 3].

Recently, several studies have applied machine learning (ML) methods for thermal conductivity
prediction [5, 25, 13, 35, 30, 29, 31, 34, 33]. Chen et. al. [5] propose a Gaussian Process Regression
combined with feature engineering by recursive feature elimination and Random Forest based feature
selection for LTC prediction. When applied to the small data set of 100 samples, they report a
performance of R2 0.93 when trained on 76 samples and tested on 19 samples. However, this result
is questionable and may be due to the high redundancy/similarity of the samples. To improve the
generalization performance, Juneja and Singh [13] propose a localized regression based patchwork
kriging approach with elemental and structural descriptors for κL prediction. When applied to a
dataset of 2838 materials, higher transferability has been achieved. Wan et al. [29] applied XGBoost
algorithm based on the descriptors of crystal structural and compositional information to κL prediction.
Two geometric descriptors have also been shown to be closely related to thermal conductivities [31].
To address the issue of limited materials with annotated κL, a shortgun transfer learning approaches
has been proposed and applied to a small κL dataset of 95 samples. A major improvement of κL
prediction comes from Zhu et al.’s work [35], in which both Graph convolution network and Random
Forest with elemental and structure features have been used to derive a prediction model over a
much larger dataset with 2700 training samples. However, all these studies have not evaluated the
real extrapolation performance [32]. While these machine learning approaches have demonstrated
themselves to be suited to predicting κL, but they are unfortunately limited by nature in the insight
that they can provide to the thermal science community as most of the ML models are essentially
based on interpolation.

This study seeks to bridge the gap between the analytical models and machine learning models for
κL prediction by exploring three types of models by focusing on the extrapolative prediction or
generalization performance of three types of prediction models. Our ﬁrst model is based on genetic

2

programming Symbolic Regression (SR), which is an evolutionary algorithm that can generate
formulae to map ordinal material properties to κL. The second model is a deep neural network model
using a Multi-Layer Perceptron (MLP) powered by the Adam optimizer to predict κL by analyzing
both the linear and nonlinear relationships in the data. Finally, the third model uses the Random
Forest Regressor (RFR), a traditional machine learning method that has been shown to be effective
in predicting κL [4, 35, 5]. We derive several formulae using the symbolic regression method that
outperform the Slack formula on our test dataset. In addition, analysis of our models’ performance
and formulae highlight interesting variable relationships to κL calculation and prediction, which
showed the advantage of interpretable models of symbolic regression.

The Symbolic Regression models in this study take three forms. The ﬁrst form, referred to as GP1,
uses a limited function set with the intention of discovering models similar to the classic Slack model.
The second, GP2, is provided with a richer function set to ﬁnd formulae that are better than the Slack
formula or are otherwise analytically distinct. Finally, the third model is a proof-of-concept model
that illustrates the effectiveness of the Symbolic Regression methodology by attempting to rediscover
the Slack formula from raw data points.

2 Methods

2.1 Dataset and features

Each model is provided with the same set of descriptors (Table 1), with the exception that the
symbolic regression models are unable to use the space group variable. This is due to the nature of
the SR models, which require numeric ﬁelds that can be used as variables inside of formulae. In
order to mitigate issues with ﬁtting the models to the data, all materials with observed κL above 120
are recognized as outliers and thus trimmed from the dataset used for training and validation. The
value 120 is chosen because the dataset contained a much higher concentration of data points with κL
immediately below 120 than those above 120. The distribution and range of the dataset’s κL values
can be seen in Figure 1. In total, there are 347 samples.

3

Figure 1: A histogram depicting the range and distribution of the κL of the materials in the dataset,
with each bar representing a precision of 2.4 W m−1K −1. The X axis displays the κL of the materials,
while the Y axis shows how many samples in the dataset fall within this range. The top 20% of the
κL range (highlighted here in green) is excluded from the training set in select experiments.

To prepare the dataset, all the ﬁrst-principles calculations are carried out based on density functional
theory (DFT) as implemented in the Vienna ab-initio Simulation Package (VASP) [15]. The projector-
augmented wave (PAW) pseudopotentials [16] are used to describe the interaction among atoms and
the generalized gradient approximation (GGA) in the Perdew-Burke-Ernzerhof (PBE) [23] form is
chosen as the exchange-correlation functional. The kinetic energy cutoff of the plane-wave function
is set as the default maximum energy cutoff for each material. A Monkhorst-Pack [19] k-point grid
of 0.4 2π/Å is used to sample the ﬁrst Brillouin zone. The convergent criterion for the total energy
difference between two successive self-consistency steps is 10−5 eV and all the geometries are fully
relaxed until the maximum force acting on each atom is less than 0.01 eV/Å. The elastic constants
are calculated from the strain-stress relationship. According to the Voigt-Reuss-Hill theory [6], the
corresponding elastic properties, such as the bulk modulus B and shear modulus G, can be evaluated
from the elastic constants. To obtain the Grüneisen parameter, we calculate the change in the elastic
properties with volume by changing the volume from -1.5 % to 1.5% (5 points in total) [12].

2.2 Preprocessing

The space group descriptor is converted into binary encoding in order for the MLP model to be able
to process the categorical value properly. In addition to this, the ﬁelds shown in Table 1 are scaled
using min-max scaling to restrict all variables to a minimum and maximum of 0 and 1. The RF
regression model requires that the space group descriptor be converted to ordinal values that represent
the various space groups. The dataset needs no preprocessing modiﬁcation for the SR model, aside
from the removal of the space group descriptor.

The architectures for the various models are described below. Barring the Symbolic Regression
models, there is only one architecture used to create each model.

4

Table 1: List of descriptors and their respective deﬁnitions.

Variable Symbol
V
T
M
n
np
B
G
E
ν
H
B(cid:48)
G(cid:48)
ρ
vL
vS
va
ΘD
γL
γS
γa
A

a

Deﬁnition
Volume per atom
Temperature (constant: 300 K)
Average atomic mass
Total number of atoms in unit cell
Total number of atoms in primitive cell
Bulk modulus calculated from Cij
Shear modulus calculated from Cij
Young’s modulus
Poisson’s ratio
Estimated hardness
(δ B/δ.V)
(δ G/δ.V)
Mass density
Sound velocities of the longitude
Sound velocities of the shear
Corresponding average velocity
Debye temperature
Longitude acoustic Grüneisen parameters
Shear acoustic Grüneisen parameters
Average acoustic Grüneisen parameters
Empirical parameter

aSecond-order elastic constants

2.3 Symbolic Regression

Symbolic Regression is a form of regression that uses mathematical operators as building blocks
to intelligently create formulae, with two objectives: minimizing the prediction error, and max-
imizing the simplicity of the formulae produced. To accomplish this, it uses the concept of the
Pareto Frontier[17] to optimize both attributes simultaneously. Through producing simple formulae,
Symbolic Regression substantially decreases the likelihood of overﬁtting to latent trends in the dataset
that do not generalize. This is particularly applicable to the ﬁelds of physics and material science,
as most of the physical laws, when expressed as equations, are relatively mathematically simple.
Examples include F = ma and E = mc2.

Our methodology for creating these formulae is through the genetic programming (GP) approach[2].
In GP, formulae are represented as unique function trees (see Figure 2 for example), with operators,
input variables/descriptors, and constants as nodes in each individual tree. They are evaluated for
their performance on the dataset and their simplicity, and using the Pareto frontier a ﬁtness value is
generated for each model. The models are then compared to each other by ﬁtness rankings, with
the ﬁtter models having higher probability to proceed onto the next stage of the evolution process:
crossover and mutation. Subtrees are randomly selected from two partner trees, and offspring in the
form of permutations of the parents are created through this crossover stage. In this way, we use
natural selection to select successful traits from parents, and pass them down to offspring. From
here, the genetic process is repeated for a set number of generations, and the most successful formula
is returned. Genetic programming provides a method for allowing beneﬁcial traits and terms to
remain in the function while simultaneously discarding unhelpful terms from the equation. It does
not guarantee a perfect solution, but rather through exploring several partial solutions, it is able to
intelligently combine them together to create a uniﬁed formula for approximating a function that lies
underneath a dataset.

5

(a) 5 ∗ (3 + 4)

(b) F = ma

(c) E = mc2

Figure 2: Examples of function trees to be represented in the symbolic regression algorithm. The
algorithm creates trees similar to the ones above, using the descriptors in the dataset as variables.

Symbolic regression has a few disadvantages when compared to other ML approaches, but it has one
unique advantage that standard ML is unable to replicate. Symbolic regression generates a human-
readable function in a mathematical notation. This formula can be analyzed to derive physical insight
into the processes that drive the subject to behave the way that it does. As aforementioned, symbolic
regression is not a ﬂawless method. It has several disadvantages, among which are computational
inefﬁciency during training and size of the search space. GP based Symbolic Regression is notoriously
computationally inefﬁcient, drawing much more resources for the training phase than statistics-based
ML models require. However, once the formulae are produced, they can be run instantly based on
their respective complexity, which the algorithm aims to reduce throughout its process.

The size of the search space is of much more concern when applying Symbolic Regression versus most
statistics-based ML algorithms. The search space for a Symbolic Regression algorithm is theoretically
inﬁnite, as there are inﬁnite formulae that can be produced from the genetic programming functions
(GP functions) provided to the algorithm. The odds of the algorithm ﬁnding and settling for the
formula that perfectly maps the provided ﬁelds to the desired output is low. In order to offset this large
search space, we restrict the height that the function trees are permitted to obtain. This places a ﬁnite
capacity on the amount of formulae that can be generated while simultaneously ensuring that the
formulae we generate remain below a maximum complexity threshold. As aforementioned, physical
formulae are mathematically simple, so it is a safe way to prune the search space. To narrow down
the search space even farther, we restrict the function set that the algorithm is allowed to make use
of. The function sets for these two models are described in Table 2 below. The number of functions
provided to the model has a direct correlation to the size of the search space; therefore, by limiting
the GP function set, the dimensionality of the problem is reduced, and the likelihood of convergence
is increased.

This experiment explores two methodologies for calculating κL through Symbolic Regression, as
described in Table 2. In addition, it uses a third methodology to prove the validity of the Symbolic
Regression algorithm for this dataset. The implementation for the SR Model was provided by the
FastSR library[8].

2.4 Verifying Effectiveness of Symbolic Regression

In order to provide a benchmark for the validity of our Symbolic Regression algorithm and demon-
strate its ability to learn from a dataset, we created a separate experiment in which a Symbolic
Regression model is allowed to train from the Slack predictions for the dataset provided. We provided
the Symbolic Regression model with the V, M, θD, γa, n, np, A, and T variables, and let it view
the Slack model calculated κL in order to learn. The goal of this experiment was to demonstrate
the learning capacity of our Symbolic Regression methodology by allowing it to train on the Slack

6

predictions, and see how closely it can approximate the Slack formula through exposure to the
variables that the Slack formula uses.

The model was permitted to use the following GP functions:
×, ÷, f −1(x), ln(|x|), ex, 3(cid:112)(x), x3, 2(cid:112)(x), x2,
In addition, randomly generated constants following a Gaussian distribution with µ=0 and σ=10 are
also provided to evolve the coefﬁcients in the formulae. The algorithm created 1,000 generations of
1,500 formulae. The model is limited to producing formula trees with a maximum height of 7, and 5
of the 1500 functions introduced with each generation were generated completely randomly, in order
to prevent the model from ﬁxating on a local minimum in the cost function gradient. Ultimately, our
SR algorithm evolved a formula (see Formula 3) that is extremely close to that of the Slack formula,
its target. The evolved formula achieves a RMSE of 5.296 and R2 of 0.946. The parity plot of the
predicted κL versus Slack model values are shown in Figure 3. This results reﬂect the ability of the
evolved SR formula to map inputs to their predicted κL Slack values.

θe2 3(cid:113)

κL =

√
nM 2θeγe 9

√
n 3

T 7

θe2

(3)

Figure 3: Parity plot for the evolved formula 3

Formula 3 is the simpliﬁed form of the evolved formula, which boasts a very high R2 score of 0.946,
meaning that it very closely mirrors the Slack equation. Interestingly, Formula 3 does not use the
A, V, and np variables. Despite this, it still obtains an effective approximation. Another interesting
observation is that Formula 3 places the γa variable in the numerator rather than the denominator,
and changes the exponent from 2 to 1
3 . This creates a relationship where the κL and the γa values
have a partial direct correlation. The θD variable in the numerator of Formula 1 has an exponent
of 3, yet the same variable in Formula 3, through some algebraic manipulation, has an exponent of

7

2 + 2
27 , meaning that Formula 3 places less importance on the Debye temperature than the original
Slack equation. These changes in scaling could be a result of the model compensating for the missing
variables, thus demonstrating the plastic and adaptive nature of the Symbolic Regression algorithm.
The fact that it is able to reproduce the Slack equation with an R2 value of 0.946 means that it has
the potential to regress a formula with a comparable coefﬁcient of determination with the actual κL
values set as the supervised learning set.

Table 2: Conﬁgurations of two SR Model architectures.

GP1

×, ÷, f −1(x), random constants on a Gaussian
distribution with mu=0 and σ=10
500 Generations
2000 Population size
7 Max height
30% Mutation probability
70% Crossover probability

GP2
×, ÷, f −1(x), ln(|x|), ex, x2, x3, 2(cid:112)(x), 3(cid:112)(x), sin(x),
cos(x), tan(x), random constants on a Gaussian
distribution with µ=0 and σ=10
500 Generations
2000 Population size
10 Max height
30% Mutation probability
70% Crossover probability

2.5 Multi-Layer Perceptron (MLP) Neural Network

Neural networks are mathematical models that take in a predeﬁned amount of inputs and convert
them through multiple layers or linear or nonlinear transformation to generate a predeﬁned number of
outputs. Through stacking the artiﬁcial neurons in layers that feed their outputs forward through the
network, and using optimization algorithms such as stochastic gradient descent or the Adam optimizer,
the weights on these neurons are adjusted to be able to output values close to those in the training set.
It is well known that deep neural networks are excellent at learning nonlinear relationships [9], but
deep learning approaches such as the MLP require vast amounts of data to effectively learn trends
and relationships. In addition, MLP models form a black box system that, while accurate, is unable to
provide scientists with insight into how the model is able to map the input variables to their expected
output variables; they are a tool that can be used but their processes for reaching their solutions cannot
be understood easily despite recent efforts for explainable deep neural network models[11].

Many optimization algorithms require the data to be passed over multiple times in order for trends
to be accurately learned, with one forward and backward pass constituting what is referred to as an
epoch. The amount of epochs needed to accurately learn trends is inversely related to the size of the
training set and directly related to the complexity of the problem. In order to ensure that our model is
able to adequately learn from the dataset, we allowed the model to train over 30 epochs for each step
in the 5-fold cross-validation process. To offset any overﬁtting which may manifest as a result of this
process, we make use of random dropout to address the issue, in which connections between neurons
are randomly deactivated when the architecture is compiled.

The MLP model, as depicted in Figure 4, makes use of 5 hidden layers with 1024 neurons in each
layer, and a 20% dropout between otherwise densely connected layers. ReLU (Rectiﬁed Linear Unit)
is the activation function for all layers leading up to the ﬁnal layer, which uses linear activation. The
network was trained with MAE as the loss function, and makes use of the the Adam optimizer [14].

8

Figure 4: Architecture of the MLP model

2.6 Random Forest Regressor (RFR)

Random Forest is an ensemble ML algorithm that takes advantage of a predeﬁned number of decision
trees. The dataset is divided up among the different decision trees, and each tree is given samples
of the dataset through the bootstrap aggregation process. They are then trained on their individual
samples, during which time the ﬁtting process trims and prunes branches of the decision trees that are
unnecessary or detrimental to the prediction of the ﬁnal result. When the training process is complete
and the model is provided with a query input data, each of the trees returns a prediction. In the case
of a Random Forest Regressor, the model collects all of the predictions, calculates the average, and
returns it as the ﬁnal prediction.

Our RFR implementation uses a standard random forest model as provided by scikit-learn [22] Python
package. The number of estimators is set as 100 and the loss function is set to MAE. By minimizing
MAE rather than RMSE, the RFR model aims to provide a smooth prediction over all values, rather
than overpunishing high residuals in κL predictions.

3 Experiments and Results

3.1 Training Process

We conduct two types of experiments to compare the symbolic regression and ML models including
the standard cross-validation tests and forward cross-validation extrapolation performance tests.

For cross-validation experiments, during the training process, the data points are split into 10 equal
subsets, and then 10-Fold cross-validation is performed. The data is split into sets of tenths, and one
of these ten sets is hidden from the model and used as a validation set while the others are used for
training. This process is repeated until each of the ten subsections are each used once as a validation
set, and then the training phase is complete. The MLP was permitted to train for 30 epochs over the
training set during each training interval of the cross validation process.

For extrapolation test experiments, all models are trained on 80% of the dataset, and then evaluated
on a block of the remaining 20% of the dataset in a process known as extrapolation testing [32]. We
also implement 5-fold cross validation on the training set, to reduce the chance of overﬁtting. For
example, one of the extrapolation tests (depicted in Table 7 and Figure 7) sorts the materials in order

9

of ascending κL, and allows the model to train on the middle 80% of the data points. The bottom 10%
and the top 10% are withheld from the model during the training phase and retained for the validation
set. After the model has been trained, the model is tested on the validation set. If the model is able to
perform similarly on both the training and validation sets, it is understood that the model has learned
an underlying relationship between the input variables and κL. Because most of the test samples are
not neighboring training samples in such tests, it is guaranteed that this relationship is not purely
based on the sample’s proximity to one another. The relationship learned can be used to predict κL
values of any sample, and thus must reﬂect the physical law that underlies κL approximation.

3.2 Random cross-validation Results

Table 3: 10-fold cross-validation performance of SR models, other ML models, and the slack model.
Bold values correspond to the best ML/SR models

GP1

G
H·np
15.914
0.368

Formula

RMSE
R2

GP2
√
V ·n

6√

n

√
12

cos(cos(E))
16.184
0.346

MLP

RFR

—

— A ·

11.816
0.651

5.98
0.911

Slack
√
(θe)3M 3

V np

n4/3T γ2
a

16.349
-1.206

Best

—

RFR
RFR

As shown in Figure 5, the SR models and the MLP and RFR models have all achieved good cross-
validation prediction performance with R2 scores of 0.368, 0.346, 0.651, and 0.911. In this evaluation
approach, the samples are randomly shufﬂed and split into K=10 folds. Thus the test samples
also have chance to ﬁnd neighbor similar samples, thus good prediction performance. Overall, the
Random Forest regressor has achieved the best performance with a R2 of 0.911. Compared to its
low extrapolation performance as shown in next section, the standard cross-validation is best for
estimating interpolation performance.

Figure 5: Parity plots for the 10-fold cross-validation experiments of GP1, GP2, MLP, RFR

3.3 Extrapolation Testing Results

Tables 6, 7, 4, and 5 show the results from the four extrapolation testing sets. The Best column
indicates the model that had the best performance on that set. All of the new formulae found in
this section are analyzed in greater detail in Section 3.6. The formulae displayed in the table have
been simpliﬁed from the original forms created by the computational models, and thus may contain
operators that exist outside of their associated function sets as deﬁned in Table 2. These new operators
are the result of combining operators used by the models. For example, np · np is simpliﬁed to n2
p.

10

Table 4: Results after training the models on the top 80% of the samples with the highest κL values
and testing on the bottom 20% samples. Bold values correspond to the best ML/SR models

Formula
Training RMSE
Testing RMSE
Training R2
Testing R2

GP1

M n
np
21.458
5.089
-0.059
-38.381

GP2

ln3(| 3(cid:112)Bcos(ln(|p|))| · |ln(|ln3(|

√

G|)|)|)

19.759
5.558
0.102
-45.97

MLP

RFR

—
18.792
4.166
0.188
-45.97

—
20.680
6.327
0.016
-59.871

A ·

Slack
√
(θe)3M 3

V np

n4/3T γ2
a

20.697
4.727
0.053
-32.971

Best

—
MLP
MLP
MLP
GP1

Table 5: Results after training the models on the top and bottom 80% of samples with lowest 40%
and highest 40% κL values and testing on the middle samples. Bold values correspond to the best
ML/SR models

Formula

GP1

0.31B
np

B( 1

2 )n

Training RMSE 20.293
4.788
Testing RMSE
Training R2
0.159
Testing R2
-6.718

1
9

GP2
p cos(cos(sin(cos(V ))))cos2(
18.398
5.825
0.309
-10.424

1
tan3(en) )

MLP

RFR

—
17.332
4.992
0.387
-7.391

—
20.941
5.457
0.105
-9.027

A ·

Slack
√
(θe)3M 3

V np

n4/3T γ2
a

19.392
14.029
0.036
-65.266

Best

—
MLP
GP1
MLP
GP1

Table 6: Results after training the models on the 80% of the samples with the lowest κL values and
testing on the top 20% samples. Bold values correspond to the best ML/SR models

Formula
Training RMSE
Testing RMSE
Training R2
Testing R2

GP1

0.68M n
np
5.534
45.257
0.196
-1.98

(cid:113)
4ln2(n) · 3

n ) · 9

(cid:113) B

sin2(M )

GP2
Bn · cos2( 15.49
4.729
44.038
0.413
-1.821

MLP

RFR

—
3.113
43.979
0.745
-1.814

—
4.070
45.640
0.565
-2.030

Slack
√
(θe)3M 3

V np

n4/3T γ2
a

A ·

19.451
37.999
0.083
-1.101

Best

—
MLP
MLP
MLP
MLP

Table 7: Results after training the models on the 80% samples with middle κL values and testing on
the top and bottom 10% of samples. Bold values correspond to the best ML/SR models

Formula
Training RMSE
Testing RMSE
Training R2
Testing R2

GP1

G
8.36B
8.396
42.99
-0.01
-0.353

GP2
(cid:112)Ecos(ln2(|cos(ln(|cos(np)|))|))
7.206
40.869
0.256
-0.223

MLP

RFR

—
5.423
40.922
0.579
-0.226

—
6.623
42.839
0.372
-0.334

A ·

Slack
√
(θe)3M 3

V np

n4/3T γ2
a

18.808
36.101
0.124
0.046

Best

—
MLP
GP2
MLP
GP2

3.3.1 Performance comparison of SR and ML models to Slack model

First, Table 4 shows the prediction performance of the algorithms when trained with top 80% samples
and tested on bottom 20% samples. Over the training sets, the MLP model achieves the best
performance with RMSE of 18.792 and R2 of 0.188. On the testing set, the MLP model outperforms
the GP1 model in RMSE (4.166 vs 5.089), though not in R2 (-45.97 vs -38.381), indicating that the
neural network’s predictions have more variance than the function that GP1 produces. It should be
noted that the coefﬁcient of determination R2 can become negative when evaluated over test sets
which are not included in the training set. GP2 is able to obtain 0.938 less RMSE than the Slack

11

model on the training set, but 0.831 more error on the testing set, which shows that them model
learned ungeneralizable trends in the subset of the data that it was shown. Strangely, while GP2 is
evolved with more evaluations than the GP1 model and had access to more GP functions than GP1,
the GP1 model achieved better metrics on the testing set than the GP2 model did. We attribute this
incongruity to the larger search space that the GP2 model must navigate due to its larger pool of
genetic programming functions.

We observe similar performance advantages of SR models compared to MLP, RFR, and Slack models
in Table 5, which shows the performance of models when trained with top 40% and bottom 40%
samples and tested on the middle 20% samples. The MLP model is able to outperform all of the
other models when evaluated against the set of data that it was trained upon; however, the GP1 model
is superior on the testing set. This demonstrates that the GP1 model was able to learn trends from
extremely poor and extremely successful thermal conductors, and accurately apply those trends to
gain insight on the materials that lie in between those extremes. Interestingly, the GP1 model and
RFR model are the only models that performed worse than the Slack model on the training set –
both GP2 and MLP were able to outperform the Slack model on the training set. The GP2 model is
able to outperform the Slack Berman model across all metrics on all subsets of the dataset. It has a
spectacularly better performance than the Slack model on the testing set, and performs better on the
training set as well.

For the other two extrapolation experiments shown in Table 6 and Table 7, the results are a little bit
different. Table 6 shows the results of models trained with bottom 80% samples and tested on the
top 20% samples while Table 7 shows the results of models trained with middle 80% samples and
tested on the two-ends 20% samples. In Table 6, the MLP model performs extraordinarily well. It
is able to outperform all other SR/ML models across all of the testing metrics, and obtains 6.25x
lower RMSE than the Slack model on the training set. Unfortunately, it underperforms on the testing
set, demonstrating less aptitude than the Slack model for predicting materials at the upper end of
the κL spectrum. In Table 6, the best SR model GP2 achieves a RMSE of 44.038 and R2 of -1.821
over the test set which is worse than the RMSE of 37.999 and R2 of -1.101 of the Slack model. In
Table 7, the GP2 model outperforms all of the other ML/SR models on the testing set, though it still
underperforms when compared to the Slack model. Further attention to Table 6 and Table 7 reveals
that when trained on the middle 80%, the GP1 model produces a formula with an 139.48% increase
in R2 score as compared to its score in Table 6. Similarly, GP2 sees a 156.36% increase in its own
R2 score. As expected, this demonstrates that training the model on a diverse set of data points yields
increased extrapolative power. Naturally, it should follow that the most diverse training set should
provide the most extrapolative potential. Table 5 shows that this is true.

As noted above, training models on a diverse set of data provides the most extrapolative potential as
opposed to alternative methods. Naturally, it follows that training the model on the bottom 40% of
the dataset and top 40% of the dataset would yield the most accurate formulae, as the SR models
would be exposed to examples of both materials with high and low κL. This is supported by Table
5, in which both GP2 and GP1 yield formulae that outperform the Slack-Berman equation. On the
training set, GP1 and GP2 perform comparably to the Slack model, with GP1 estimating κL with
0.901 more RMSE and GP2 estimating κL with 0.994 less RMSE. However, on the testing sets,
GP1 and GP2 demonstrate that they are signiﬁcantly more accurate. GP1 has 9.241 RMSE less than
the Slack model on the testing set, and GP2 achieves an RMSE of 5.825, 8.204 less than the Slack
model on the same set (14.029). The two formulae produced by GP1 and GP2 perform similar to
the Slack model on the training set, but are able to predict the median 20% of materials with 2.93x
and 2.4x less error than the Slack model, despite the fact that they have never seen materials with
κL in that range before. This successful prediction proves that the SR models are not overﬁtting to
latent trends in their training sets, but have uncovered relationships that govern the calculation of κL.
Formula 4 represents the formula generated by GP1 from this training set, and Formula 5 represents
that generated by GP2. We discuss these formulae further in Section 3.6, but for convenience we
provide them here.

Now there is one remaining question: Why do the SR models work better when trained with the top
80% and tested on the bottom 20% compared to when they are trained with the bottom 80% and
tested on the top 20%? After close inspection of the sample distribution in Figure 1, it seems that
this is caused by the extremely sparse amount of samples in the high-κL area compared to the dense
amount of samples in the bottom κL area. As a result, whenever the test set includes the top κL area,

12

the extrapolation performance will be very low. This result conﬁrms the importance of training ML
and SR models with balanced diverse data samples.

κL =

0.31B
np

κL = B( 1

2 )n

1
9

p cos(cos(sin(cos(V ))))cos2(

1
tan3(en)

)

(4)

(5)

3.3.2 Performance comparison of SR, RFR and MLP

Comparing the SR and machine learning models performance against each other on the datasets
reveals a few interesting results. While the MLP model is very effective at learning from the data it is
shown, it does not have the same extrapolative potential that the SR models have. MLP outperforms
all other models on the training sets when evaluated by both RMSE and R2, as shown by Tables
6, 7, 4 and 5. However, it is not always able to outperform the GP models on the testing sets. The
MLP model’s predictions on the validation sets are close to that of the SR models, with the largest
difference in RMSE being 2.068 in Table 7. The fact that it is unable to consistently match the
Symbolic Regression models’ performance on the testing sets yet outperforms them on the training
sets demonstrates that while the neural network is excellent at creating a model that accurately
predicts materials in the range it has seen before, it does not always transfer this knowledge to
materials outside of this range.

The RFR model does not perform better than the MLP model for any metric on any set of materials
from the dataset. However, it does obtain lower RMSE and higher R2 values than the SR models
on the training sets for Tables 6 and 7. Despite this, it is unable to compare with the SR models on
any of the validation sets except for Table 5 and Table 7, where it outperforms one of the SR models
but is surpassed by the other SR model. For example, in Table 5, the RFR model outperforms the
GP2 formula’s RMSE by 0.368, but is worse than GP1’s formula by 0.669. None of the models were
able to perform better than the Slack model on the extrapolation sets depicted in Table 6 and Table 7.
However, this is not to say that the Slack formula is superior for calculating the κL of materials in
those validation sets; it simply means that the models needed data points from those sets in order to
learn the relationships for them. The models demonstrated their efﬁcacy for κL approximation on
those materials through the RMSE reported in Tables 4 and 5, where they demonstrated comparable
performance to the Slack formula.

3.3.3 Performance comparison of GP1 and GP2

As shown in Table 2, we evaluated two symbolic regression algorithms to evolve SR models: GP1
and GP2, where GP1 corresponds to a simpler function set with max tree height of 7, leading to
simpler models. On the other hand, GP2 model is trained with more complex function set with a tree
height of 10, leading to more complex models.

All the GP1 and GP2 performance results with four extrapolation experiments are shown in Table
6, Table 7, Table 5, and Table 4. We ﬁnd that the GP1 model outperforms the GP2 model when the
top portion of the dataset is included in the testing set (Table 4 and Table 5). This can be noted by
observing Table 4, in which GP1’s RMSE (5.089) is 0.469 less than GP2’s RMSE of 5.558. However,
when the top section of the dataset is excluded from the training set (as in Table 6 and Table 7), the
GP2 model outperforms the GP1 model. This can be observed by comparing GP1’s RMSE score of
42.99 to GP2’s RMSE of 40.869 in Table 7.

3.4 Parity Plot Analysis

To further understand why the SR and ML models have unexpected low extrapolation prediction
performance, we create a set of parity plots (Figures 6 - 9) for all the four extrapolation experiments
and aim to ﬁgure how the sample distribution affects the prediction performance of ML and SR
models. In all of the plots, the orange points represent training samples while the blue points are
test samples. However, for the Slack model, both colors represent testing points. This is because the
Slack model is an empirical model, and thus does not require training.

13

Firstly, we ﬁnd that compared to the random cross-validation performance results(Figure 5, the
extrapolation prediction performances of all SR and ML models are unexpectedly low, consistent
with our previous observations [32] along with other analysis[7] on out-of-distribution generalization
issues.

Second, across all the parity plots, there is a clear propensity for the prediction models to underesti-
mate κL (most of the samples are below the diagonal line). This is not an unexpected development,
as the dataset contains many more materials on the lower spectrum of κL materials than the upper
bound. As Figure 1 demonstrates, its distribution is positively skewed. The models that were trained
on materials with lower κL values often underestimate the values of their testing sets, as shown
in Figures 6. The models trained on the upper side of the material κL spectrum tend to generate
overestimates, as demonstrated by Figure 8. In Figure 7, the models both over-approximate the lower
κL materials and under-estimate the higher κL materials’ values, as the models shown in that ﬁgure
were trained on the middle 80% of the dataset. Finally, Figure 9 shows a more even balance, with
variance both above and below the y = x line. This is due to the fact that the models were provided a
diverse training set of both high and low κL materials, as previously noted in Section 3.3. They still
show a tendency to underpredict the values on the upper bound, with the SR models demonstrating a
slightly more standard yet still skewed variance.

The parity plots (Figures 6 - 9) for all extrapolation sets indicate that the models behaved largely as
expected on their training sets; on some materials, the models overestimated the κL, and on others,
they underestimated the κL. The RFR model’s parity plots have an interesting spread on the training
sets. Most easily seen in Figure 8, the RFR model predicts the lower end of its training sets with
relatively low error, but then abruptly begins predicting nearly the same κL for all of its materials
with some variation. This variation is lowest in Figure 6, and highest in Figure 9. The point at which
the RFR model experiences its estimation accuracy falloff occurs at a logical point in each of its
parity plots. In Figure 6, the jump in error occurs relatively early in the plot, whereas in Figures 8
and 9 it occurs later. This is because Figure 6 contains materials of low κL, so the error spikes when
the κL rises. This same spike occurs later in the other ﬁgures, because they include member nodes
of higher κL values for the model to learn from. The RFR models’ parity plot testing sets continue
trends identiﬁed by the models for the higher values in their training sets, which indicates that the
models have found similarities in the ﬁelds of the two subsets. This sudden spike in performance is
not an unexpected development, as the RFR model is decision tree based [32].

14

Figure 6: Parity plots for the GP1, GP2, MLP, RFR, and Slack-Berman models on the top 20%
extrapolation testing set.

15

Figure 7: Parity plots for the GP1, GP2, MLP, RFR, and Slack-Berman models on the top 10% and
bottom 10% extrapolation testing set.

16

Figure 8: Parity plots for the GP1, GP2, MLP, RFR, and Slack-Berman models on the bottom 20%
extrapolation testing set.

17

Figure 9: Parity plots for the GP1, GP2, MLP, RFR, and Slack-Berman models on the middle 20%
extrapolation testing set.

3.5 Formula Comparison

Comparing the formulae generated by the SR models yields some interesting insight, particularly
when they are compared to the Slack-Berman formula (1). Both formulae produced by the SR model
in the extrapolation experiments (Equations 4 and 5) place the B ﬁeld in the numerator, indicating that
κL scales with bulk modulus. This is consistent with current kinetic theory of phonon transport that
the inclusion of bulk modulus as a variable in the formula is essential to approximating a material’s
κL.

Very interestingly, Equations 6 and 7 do not make use of the B ﬁeld whatsoever. This contradicts the
aforementioned kinetic theory of phonon transport. In addition, the only variable that Equation 6 has
in common with Equations 4 and 5 is np. Equation 5 and Equation 7 have some overlap in that they
both make use of the V and n variables.

Equation 5 places the np variable in the numerator, which corresponds to the Slack formula’s usage
of the ﬁeld. However, Equation 4 places it in the denominator. This disparity indicates a disagreement
between the formulae, where Equation 5 assumes that κL has a positive correlation with the number
of atoms in a primitive cell, and Equation 4 indicates that they have a negative correlation. We

18

conclude that Equation 5’s usage of the ﬁeld is most likely correct, as it boasts a lower RMSE than
Equation 4 (16.643 vs 18.255).

Perhaps the most interesting of all is the set of variables selected by the formulae. The Slack formula
makes use of 6 variables and 2 constants (A&T ), whereas the most accurate formula that our models
produced (Formula 7) uses only three variables and achieves a higher accuracy. The two formulae
have the n and V variables in common.

While the models produced a multitude of potential formulae, we have elected to only include those
with the least error in the primary section of this work. A selection of other noteworthy formulae
have been collected based on their interesting properties, and have been included in the Appendix.

κL =

0.31B
np

κL = B( 1

2 )n

1
9

p cos(cos(sin(cos(V ))))cos2(

1
tan3(en)

)

κL =

G
H · np

κL =

3.6 Discussion

√
6

√

V · n
12(cid:112)cos(cos(E))

n

(4)

(5)

(6)

(7)

All of the machine learning models reviewed in this study have their own unique advantages and
disadvantages to their use. Symbolic Regression is computationally expensive and time consuming
during the training stage, but it leads to formulae that are physically meaningful and have enhanced
extrapolative capacity and also run fast during the prediction stage. Random Forest reduces overﬁtting
and variance through the usage of bagging and ensemble learning. Multilayer Perceptron neural
networks are able to accurately discover nonlinear relationships from training data, and with a large
enough dataset, they are able to use this information to estimate data points that lie outside their
training set.
Using RMSE and R2 as metrics for evaluation, the Symbolic Regression models used in this work
were collectively more effective than any other model on the extrapolation validation sets. The MLP
model performed comparably on a number of validation sets, and outperformed the GP1 and GP2
models on some others, but overall it was less effective on the validation sets. In addition, the SR
models provide formulae that can be analyzed to obtain physical insight into the relationships of the
variables in the formulae; Multilayer Perceptron and Random Forest models cannot provide the same
level of insight.

Our Symbolic Regression models produced formulae that are able to calculate κL with comparable
or greater accuracy than the traditional Slack formula (Equation 1), all while using less variables to
do so. We demonstrate the validity of our Symbolic Regression methodology by showing that it can
approximate the Slack formula with an R2 score of 0.946 (Fig 3). There are a multitude of other
sources that have proven the Symbolic Regression algorithm’s capacity for discovering physical laws
[10][24]. Symbolic Regression provides computers with the ability to discover natural laws from raw
data, and even bring insights to boot. Formulae 4 and 5 successfully reproduces the physical insight,
although already known, that κL scales positively with bulk modulus. Formulae 4 and 6 reproduce
the physical insight that κL scales negatively with the number of atoms in the primitive unit cell of a
material.

In addition to the other models discussed in the paper, we also ran the lastest Symbolic Regression
algorithm, the AI-Feynman algorithm [27] over our dataset using the implementation in the github
repository by Udrescu [26] [28]. Initially, the algorithm did not converge to any useable formula
because our dataset contained too many input variables. However, even after we restricted the
dimensionality of the problem to only the six variables used by the orginal Slack model and allowed
the model to run continuously for nine days, it still did not converge. The AI-Feynman algorithm

19

on paper is a very strong candidate for predicting formulae for LTC, as it is a symbolic regression
algorithm that does not rely on genetic programming. Rather than using an evolutionary algorithm,
the AI-Feynman algorithm uses neural networks to simplify the data it is provided with before using
a brute force algorithm to try all symbolic expressions possible in order of ascending complexity. The
algorithm is very promising and has the capability to exploit the units of variables. Unfortunately,
with our currently limited dataset we were unable to successfully apply it to get better formulae.

There are a few areas in which this study could be improved. Firstly, collating a larger dataset of
materials with measured κL values will enable all three types of models explored in this study to
obtain lower error metrics and resolve issues with variance and bias from the models. Obtaining a
balanced dataset that has a normally distributed range of κL-ranked materials will permit the models
to improve their performance across all categories, especially when predicting materials with high
κL. Outside of changes to the dataset, the symbolic regression methodology has some room for
improvement. Attributing units and types to the variables in the dataset before feeding them to the SR
models will allow for the inclusion of binary operators that require consistent units, such as addition
and subtraction. The inclusion of these operators would substantially increase the hypothesis space
of the SR models, potentially leading to more accurate models.

Even with these limitations, Symbolic Regression has demonstrated that it can learn from raw
experimental data and intelligently produce equations and formulae that can predict unseen values.
In this work, we have proven that genetic programming has the capacity to create formulae that are
more accurate and more consistent than models that have been derived by physicists for the same task
(Formula 1). It can infer relationships that are relevant to materials outside of the range that it was
trained on, and it does so with less error than neural networks and random forest regressors trained
on the exact same data.

4

contributon

Conceptualization, J.H. and M.H.; methodology, J. H. and C.L.; software, C.L., J.H. and Y.Z.;
validation, C.L., J.H. and M.H.; investigation, C.L., J.H. and M.H. ; resources, J.H.; data curation,
K.Y. and M.H.; writing–original draft preparation, C.L and J.H.; writing–review and editing, C.L.,
J.H., Y.Z., K.Y. and M.H.; visualization, C.L.; supervision, J.H.; project administration, J.H.; funding
acquisition, J.H. and M.H.

5 Acknowledgments

This research was supported, in part, by a grant from the Magellan Scholar program, from the
Department of Undergraduate Research at the University of South Carolina, Columbia. Research
reported in this publication was also partially supported by the National Science Foundation under
grant numbers: 1940099, 1905775, OIA-1655740 (via SC EPSCoR/IDeA 20-SA05) and by DOE
under grant number DE-SC0020272.

6 Conﬁct of Interest Declarations

The authors declare no conﬂict of interest.

Appendix

A Noteworthy Formulae from Symbolic Regression Models

The Symbolic Regression models generated a large quantity of formulae during the training phase.
Ultimately, most were uninteresting. While we elected to only include the best performing formulae
in the main body of this work, it would be neglectful to omit several of the more physically interesting

20

formulae that were found by the models. Below are some of the aforementioned interesting candidates,
and their average MAE and R2 scores across both the training and validation sets.

(8)

(9)

(10)

(11)

(12)

(13)

κL = 3.55 ·

(cid:115)

B
(np)2 ·

√

H

κL =

B
H · n2
p

κL =

E · v
np

κL =

B
n2
p

κL =

G 2
3 ln(2.60)
ln(|np|)H 1

9

(cid:115)
6

√
B3 · H

e · (cid:112)ln(0.45 · |n|)

H

κL =

Table 8: Noteworthy formulae MAE & R2

Formula
Slack-Berman
8
9
10
11
12
13

MAE
10.62
9.461
9.373
9.360
8.916
9.303
9.927

R2
0.078
0.135
0.215
0.175
0.234
0.210
0.091

References

[1] Allen, P. B. Improved callaway model for lattice thermal conductivity. Physical Review B, 88(14):144302,

2013.

[2] Banzhaf, W., Nordin, P., Keller, R. E., and Francone, F. D. Genetic programming. Springer, 1998.

[3] Callaway, J. Model for Lattice Thermal Conductivity at Low Temperatures. Physical Review, 113(4):

1046–1051, Feb 1959. doi: 10.1103/PhysRev.113.1046.

[4] Carrete, J., Li, W., Mingo, N., Wang, S., and Curtarolo, S. Finding unprecedentedly low-thermal-
conductivity half-heusler semiconductors via high-throughput materials modeling. Phys. Rev. X, 4:011019,
Feb 2014. doi: 10.1103/PhysRevX.4.011019.

[5] Chen, L., Tran, H., Batra, R., Kim, C., and Ramprasad, R. Machine learning models for the lattice thermal
conductivity prediction of inorganic materials. Computational Materials Science, 170:109155, 2019. ISSN
0927-0256. doi: https://doi.org/10.1016/j.commatsci.2019.109155.

[6] den Toonder, J. M. J., van Dommelen, J. A. W., and Baaijens, F. P. T. The relation between single
crystal elasticity and the effective elastic behaviour of polycrystalline materials: theory, measurement and
computation. Modelling and Simulation in Materials Science and Engineering, 7(6):909–928, nov 1999.
doi: 10.1088/0965-0393/7/6/301.

[7] Fannjiang, C. and Listgarten, J. Autofocused oracles for model-based design.

arXiv preprint

arXiv:2006.08052, 2020.

21

[8] Fusting, C. cfusting/fast-symbolic-regression, Oct 2019.

[9] Goodfellow, I., Bengio, Y., and Courville, A. Deep learning. MIT press, 2016.

[10] Hernandez, A., Balasubramanian, A., Yuan, F., Mason, S. A. M., and Mueller, T. Fast, accurate, and
transferable many-body interatomic potentials by symbolic regression. npj Computational Materials, 5(1):
112, 2019. ISSN 2057-3960. doi: 10.1038/s41524-019-0249-1.

[11] Hu, R., Andreas, J., Darrell, T., and Saenko, K. Explainable neural computation via stack neural module
networks. In Proceedings of the European conference on computer vision (ECCV), pp. 53–69, 2018.

[12] Jia, T., Chen, G., and Zhang, Y. Lattice thermal conductivity evaluated using elastic properties. Phys. Rev.

B, 95:155206, Apr 2017. doi: 10.1103/PhysRevB.95.155206.

[13] Juneja, R. and Singh, A. K. Guided patchwork kriging to develop highly transferable thermal conductivity

prediction models. Journal of Physics: Materials, 3(2):024006, 2020.

[14] Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization, 2014.

[15] Kresse, G. and Furthmüller, J. Efﬁcient iterative schemes for ab initio total-energy calculations using a
plane-wave basis set. Phys. Rev. B, 54:11169–11186, Oct 1996. doi: 10.1103/PhysRevB.54.11169.

[16] Kresse, G. and Joubert, D. From ultrasoft pseudopotentials to the projector augmented-wave method. Phys.

Rev. B, 59:1758–1775, Jan 1999. doi: 10.1103/PhysRevB.59.1758.

[17] Lu, L., Anderson-Cook, C. M., and Robinson, T. J. Optimization of designed experiments based on
multiple criteria utilizing a pareto frontier. Technometrics, 53(4):353–365, 2011. ISSN 00401706.

[18] Ma, J., Li, W., and Luo, X. Examining the callaway model for lattice thermal conductivity. Physical

Review B, 90(3):035203, 2014.

[19] Monkhorst, H. J. and Pack, J. D. Special points for brillouin-zone integrations. Phys. Rev. B, 13:5188–5192,

Jun 1976. doi: 10.1103/PhysRevB.13.5188.

[20] Morelli, D. T. and Slack, G. A. High Lattice Thermal Conductivity Solids, pp. 44. Springer New York,

New York, NY, 2006. ISBN 978-0-387-25100-4. doi: 10.1007/0-387-25100-6_2.

[21] Nath, P., Plata, J. J., Usanmaz, D., Toher, C., Fornari, M., Nardelli, M. B., and Curtarolo, S. High
throughput combinatorial method for fast and robust prediction of lattice thermal conductivity. Scripta
Materialia, 129:88–93, 2017.

[22] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer,
P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and
Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:
2825–2830, 2011.

[23] Perdew, J. P., Burke, K., and Ernzerhof, M. Generalized gradient approximation made simple. Phys. Rev.

Lett., 77:3865–3868, Oct 1996. doi: 10.1103/PhysRevLett.77.3865.

[24] Schmidt, M. and Lipson, H. Distilling free-form natural laws from experimental data. Science, 324(5923):

81–85, 2009. ISSN 0036-8075. doi: 10.1126/science.1165893.

[25] Tawﬁk, S. A., Isayev, O., Spencer, M. J., and Winkler, D. A. Predicting thermal properties of crystals using

machine learning. Advanced Theory and Simulations, 3(2):1900208, 2020.

[26] Udrescu, S. M. Sj001/ai-feynman.

[27] Udrescu, S.-M. and Tegmark, M. Ai feynman: A physics-inspired method for symbolic regression. Science

Advances, 6(16):eaay2631, 2020.

[28] Udrescu, S.-M., Tan, A., Feng, J., Neto, O., Wu, T., and Tegmark, M. Ai feynman 2.0: Pareto-optimal

symbolic regression exploiting graph modularity, 2020.

[29] Wan, X., Feng, W., Wang, Y., Wang, H., Zhang, X., Deng, C., and Yang, N. Materials discovery and
properties prediction in thermal transport via materials informatics: A mini review. Nano letters, 19(6):
3387–3395, 2019.

[30] Wang, X., Zeng, S., Wang, Z., and Ni, J. Identiﬁcation of crystalline materials with ultra-low thermal
conductivity based on machine learning study. The Journal of Physical Chemistry C, 124(16):8488–8495,
2020.

22

[31] Wei, H., Bao, H., and Ruan, X. Genetic algorithm-driven discovery of unexpected thermal conductivity

enhancement by disorder. Nano Energy, 71:104619, 2020.

[32] Xiong, Z., Cui, Y., Liu, Z., Zhao, Y., Hu, M., and Hu, J. Evaluating explorative prediction power of
machine learning algorithms for materials discovery using k-fold forward cross-validation. Computational
Materials Science, 171:109203, 2020.

[33] Yamada, H., Liu, C., Wu, S., Koyama, Y., Ju, S., Shiomi, J., Morikawa, J., and Yoshida, R. Predicting
materials properties with little data using shotgun transfer learning. ACS central science, 5(10):1717–1730,
2019.

[34] Yan, J., Wei, H., Xie, H., Gu, X., and Bao, H. Seeking for low thermal conductivity atomic conﬁgurations

in sige alloys with bayesian optimization. ES Energy & Environment, 2020.

[35] Zhu, T., Gong, S., Xie, T., Gorai, P., and Grossman, J. C. Charting lattice thermal conductivity of inorganic

crystals. arXiv preprint arXiv:2006.11712, 2020.

23

