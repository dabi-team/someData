9
1
0
2

v
o
N
9
1

]

G
L
.
s
c
[

2
v
1
2
7
7
0
.
1
1
9
1
:
v
i
X
r
a

Program synthesis performance constrained by non-linear spatial

relations in Synthetic Visual Reasoning Test

Lu Yihe1, Scott C. Lowe2, Penelope A. Lewis3, Mark C. W. van Rossum1

November 2, 2021

1School of Psychology, University of Nottingham, Nottingham, United Kingdom

2Faculty of Computer Science, Dalhousie University, Halifax and Vector Institute, Toronto, Canada

3School of Psychology, Cardiﬀ University, Cardiﬀ, United Kingdom

Abstract

Despite remarkable advances in automated visual recognition by machines, some visual tasks remain

challenging for machines. Fleuret et al. [2011] introduced the Synthetic Visual Reasoning Test (SVRT)

to highlight this point, which required classiﬁcation of images consisting of randomly generated shapes

based on hidden abstract rules using only a few examples. Ellis et al. [2015] demonstrated that a program

synthesis approach could solve some of the SVRT problems with unsupervised, few-shot learning, whereas

they remained challenging for several convolutional neural networks trained with thousands of examples.

Here we re-considered the human and machine experiments, because they followed diﬀerent protocols

and yielded diﬀerent statistics. We thus proposed a quantitative reintepretation of the data between

the protocols, so that we could make fair comparison between human and machine performance. We

improved the program synthesis classiﬁer by correcting the image parsings, and compared the results to

the performance of other machine agents and human subjects. We grouped the SVRT problems into

diﬀerent types by the two aspects of the core characteristics for classiﬁcation: shape speciﬁcation and

location relation. We found that the program synthesis classiﬁer could not solve problems involving shape

distances, because it relied on symbolic computation which scales poorly with input dimension and adding

distances into such computation would increase the dimension combinatorially with the number of shapes

in an image. Therefore, although the program synthesis classiﬁer is capable of abstract reasoning, its

performance is highly constrained by the accessible information in image parsings.

1

Introduction

Progress in visual recognition by machine has been impressive due to the remarkable development of machine

learning (ML) in the recent decade. However, it has been argued that machines can only be successful in

certain tasks, while they fail to achieve human-like performance in others. To highlight the diﬀerence between

machine and human intelligence, Fleuret et al. [2011] introduced the Synthetic Visual Reasoning Test (SVRT),

consisting of 23 image classiﬁcation problems. All the images are composed of randomly generated shapes

that are simple closed contours without intersections. In each classiﬁcation problem there are two categories

of such images. When presented with a new image in a particular problem, a human subject or a machine

agent has to classify it according to only the previous images and their categories that have been seen. A

1

 
 
 
 
 
 
Table 1: Problem #1 in the original SVRT. Each image parsing contains several Shape(x-coordinate,
y-coordinate, shape identity, scale), which describes all the shapes respectively in an image. If two
shapes are in contact or one shape is inside another, a parsing might further contain borders(shape index,
shape index) or contains(shape index, shape index), where shape index is implicitly assigned to each
shape in the order as they are represented in the parsing. The example images and parsings were obtained
by our SVRT generator, forked from the original one [Fleuret et al., 2011]. The example programs were
synthesized by our PS classiﬁer, forked from Sasquatch [Ellis et al., 2015]. There are no diﬀerences in image
generation per se between the original generator and ours, while there are modiﬁcations in parsing extraction
and program synthesis between the previous approach and ours. Example images and classiﬁcation rules of
more SVRT problems can be found in Table 5.1.

hidden abstract rule determines whether the images belong to the same category (see Table 1 for example

images and classiﬁcation rules).

Since all images are composed of randomly generated contours, they contain little real-world meanings.

Therefore, the hidden classiﬁcation rules are designed to be associated with abstract reasoning in visual

recognition. In addition, the random generation of images prevents the usage of low-level cues or brute-force

memory, and allows us to generate any number of images.

Human subjects outperformed the machine agents: the human subjects could detect or deduce the hidden

rules and thus classify images successfully after seeing and classifying a handful of images (about 6 for most

of the 23 problems), while Support Vector Machine (SVM) and Adaptive Boosting (AdaBoost) only achieved

reasonably high accuracy after being trained with 10,000 examples [Fleuret et al., 2011]. Subsequent studies

[Ellis et al., 2015, Stabinger et al., 2016, Kim et al., 2018] took the SVRT as a challenge for automated

visual recognition, and employed more advanced ML techniques, in particular Convolutional Neural Network

(CNN) and its variants. Speciﬁcially, ConvNet with 2,000 examples [Ellis et al., 2015], LeNet and GoogLeNet

with 20,000 examples [Stabinger et al., 2016], and vanilla CNNs consisting of diﬀerent numbers of layers and

diﬀerent sizes of receptive ﬁelds with 2 million examples [Kim et al., 2018], all failed to achieve human

performance in several problems.

Kim et al. [2018] sorted the original SVRT problems according to their CNN performance, and noticed

their CNNs could solve problems involving detection of spatial relations (SR) between shapes, but not whether

two shapes were same or diﬀerent (SD) in an image. They further pointed out that the hidden rules of the

2

original 23 problems were chosen in an arbitrary manner, which made it diﬃcult to analyse what characeteris-

tics of the problems constrain their CNN performance. They thus developed the Parametric SVRT (PSVRT),

a variant version of the SVRT in which each image can be characterised by some variability parameters (e.g.,

number of shapes), so that the classiﬁcation diﬃculty of a problem (in terms of machine performance) could

be correlated to these image variability parameters. They found that only the SD problems led to relatively

low performance of their CNNs; especially when image sizes were large, the CNNs needed more than 10

million examples to reach a reasonably high classiﬁcation accuracy in the SD problems.

However, the general conclusions was criticised by Borowski et al. [2019] that all CNNs are unable to solve

some diﬃcult SVRT problems and thus a feedforward convolutional architecture is incapable of corresponding

abstract reasoning, because a poor machine performance might not be caused by the architecture, but by
sub-optimal parameter choices or training strategies. In fact, they showed that their ResNet50 could reach

over 90% test accuracy on all the original SVRT problems [Borowski et al., 2019].

To achieve an unsupervised, few-shot learning machine performance, Ellis et al. [2015] explored the

possibility to solve the SVRT via the program synthesis (PS) approach. The PS approach is generative; it

aims to construct a program that generates the images of a given class. As the PS approach scales extremely

poorly with input dimension, images were ﬁrst parsed to yield compact descriptions, i.e.

image parsings.

These image parsings were then send to the synthesizer which contructed an optimal program for each image

category. To classify test images for a problem, a pair of such programs (for the two categories respectively)

were deployed to check which one was more compatible with the parsing of each test image.

Here we re-considered the experiments and results, as the human and machine experiments were conducted

with diﬀerent protocols, and diﬀerent statistics were used for comparison between human and machine per-

formance. We were concerned more on the PS approach than the neural approach due to our interest in

its somewhat human-like performance (capable of unsupervised, few-shot learning). Moreover, we aimed to

investigate the PS approach in details, particularly whether the program contruction by the synthesizer or

simply the problem reduction by the parser played a crucial role in solving the SVRT. Although Ellis et al.

[2015] showed that the classiﬁcation based on the parsings did not lead to human-like performance when

they were used as inputs for AdaBoost, it did not necessarily imply, as Borowski et al. [2019] pointed out,

that the method is unable to solve the SVRT by design, and AdaBoost was employed for baseline machine

performance after all [Fleuret et al., 2011, Ellis et al., 2015]. Similar to the previous studies deepening our

understanding in diﬀerent CNNs by focusing on the diﬃcult problems [Stabinger et al., 2016, Kim et al., 2018,

Borowski et al., 2019], we aimed to learn limitations of the PS approach and thanks to its interpretability we

could analyse relevant problems, ﬁnd out possible reasons and suggest possible solutions.

In Section 2, we will discuss the protocols of the machine and human experiments and how to interpret

and compare their results despite protocol diﬀerences. In Section 3, we will investigate the results of our

machine experiments, highlighting the improvements in the PS performance by correcting image parsings,

comparing them to the human and machine results reported by the other studies with respect to diﬀerent

problem types. In Section 4, we will summarise the advantages and disadvantages of the PS approach and

discuss the reasons and further improvements might be made in automated visual recognition.

2 Materials and methods

2.1 Using program synthesis on SVRT

Ellis et al. [2015] demonstrated that their machine agent, namely Sasquatch, was able to achieve human-

3

like performance on the SVRT in the sense that it could perform unsupervised, few-shot learning on many

SVRT problems. Sasquatch classiﬁes SVRT images in three steps. Firstly, its parser pre-processes the raw

images and encodes them into low-dimensional parsings. A parsing of an image contains only the information

of individual shapes and their spatial relations; speciﬁcally, the two-dimensional coordinates of the centre

of mass, the identity and the scale of every shape and whether a shape is bordering to or contained by

another. If two shapes in an image are identical under any geometric similarity transforms (i.e., translation,

rotation, rescaling and reﬂection), they share the same identity. The normalised sizes of shapes that share

the same identity in an image are considered to be their scales, with the scale of the largest shape set to

1; if two shapes do not share the same identity, it is meaningless to compare their scales. Secondly, the

Sasquatch synthesizer constructs an optimal program for each image category by training on 3 examples.
As each problem contains two categories (which we call positive and negative categories), it constructs two

programs respectively for them with 6 training examples in total. Finally, when classifying a test image, the

evaluator computes how compatible an image is with either program by some likelihood function and classiﬁes

the image accordingly (see Table 1 for example parsings and programs). In the second and third steps, it

employs a theorem prover for general formal veriﬁcation, Z3 (which is made public by Bjørner et al. [2018]
at https://github.com/Z3Prover/), to check the satisﬁability of constructed programs and test images.
In other words, Sasquatch attempted to solve the SVRT as a Satisﬁability Modulo Theory (SMT) problem.

Since an SMT problem is proved to be NP-complete [Cook, 1971], verifying a program requires time that

is superpolynomial in the input dimension. Therefore, a ﬁxed time limit was set for Z3 in each veriﬁcation

iteration. In total, the construction of a single optimal program took from around 0.05 seconds to more than

2 minutes depending on the problems (running on an individual iMac with a 2.7 GHz Intel Core i5 processor).

However, Sasquatch did not fully solve the SVRT. We noticed that some of its poor performance was

caused by incorrect parsings. In particular, Sasquatch did not assign the same shape identity to identical

shapes if rotated, rescaled or reﬂected (unless the rotation or rescaling is trivially small); it could tell which

shape was bigger if two shapes were diﬀerent; and occasionally when several shapes were close to one another

it parsed the space surrounded by them as an extra shape. In order to test the impact of such image parsing

errors on classiﬁcation performance, we obtained our corrected parsings by extracting shape information

directly from the SVRT generator, and then conducted experiments on both sets of parsings. In particular,

our parsings contain the correct identities of every shape and the accurate coordinates of the generation centre

(which is the point for generating random contour around it, not the centre of mass). Moreover, we re-deﬁne

their scales to be the real sizes rather than the normalised ones, and a negative scale was used if a shape is
a reﬂected copy of another. Our code can be found at https://github.com/anish-lu-yihe/pySVRT. In
addition, rotated angles of similar shapes can be encoded in our new parsings as an extra feature. However,

we did not use this feature for two reasons: ﬁrstly, the synthesizer scales poorly with its input dimension;

secondly, rotation is not a necessary feature for classifying any image in the 23 SVRT problems.

Since we do not changed the synthesizer and the evaluator, except for the parameters relevant to the
corrections in parsings, our machine agent remains a PS classiﬁer, whose code can be found at https:
//github.com/anish-lu-yihe/SVRT-by-Sasquatch. We also used AdaBoost for a baseline machine perfor-
mance [Fleuret et al., 2011, Ellis et al., 2015]. The input vectors were taken directly from the image parsing

by ripping oﬀ the non-numeric tags, and the outputs are binary. In particular, we employed the AdaBoost

classiﬁer of Scikit-learn [Pedregosa et al., 2011]. We found that there were little diﬀerences in the results

across diﬀerent numbers of stumps and thus report only the performance of AdaBoost with 10,000 stumps in
Table 4. The code and results can be found at https://github.com/anish-lu-yihe/SVRT-by-AdaBoost.

4

2.2 Comparing machine to human performance

Although Fleuret et al. [2011] pointed out humans outperformed machines in the SVRT, it is worth noting

that the experimental protocols of human and machine experiments are diﬀerent. In the human experiment,

20 subjects were presented with images in a sequence. After making the classiﬁcation decision, the subject

was informed of the true category of the test image and a test image was shown while past test images

remained on the display. The subject was considered successful if 7 correct decisions were made in a row

within 35 images. For a given SVRT task, the number of the succesful subjects and the average number of

images before making 7 correct decisions in a row characterised the average human performance.

In the machine experiment, an agent was ﬁrstly trained with up to 10,000 example images, and then

tested on a separate set of images without further learning. Subsequent analysis of the SVRT followed the

same experimental protocol [Ellis et al., 2015, Stabinger et al., 2016, Kim et al., 2018, Borowski et al., 2019].

We also followed the same protocol when performing our machine experiments.

While for most of the ML techniques thousands or even millions of examples were used for training, few-

shot learning can be realised by PS. In particular, Sasquatch was trained on 3 pairs of positive and negative

examples for each problem [Ellis et al., 2015], and for comparison we used the same size for the training

sets, except when we looked into the learning curves by PS. The test set contained 94 unseen images. For

each problem, we repeated the same training and testing procedure with our PS classiﬁer for 40 times, and

averaged the test classiﬁcation accuracies. Due to the repetition, the classiﬁer could spend more than 1 hour

on some problem (without parallelisation).

The number of successful human subjects and the test accurracies of machine agents are related but

diﬀerent statistics. For more quantitative analysis, we denote the machine classiﬁcation accuracy in a single

test trial by α on one hand. Since a machine can only learn during the training but have no funtion to update

itself further when being tested, we assume each test trial is independent to one another, and approximate α

by the overall test accuracy. On the other hand, we denote the proportion of successful human subjects by

β, which is found by dividing the number of successful subjects through 20 (the total number of subjects),

using the numbers reported in Fleuret et al. [2011].

We consider β as the average human performance, overlooking individual diﬀerences. Although both α and
β indicate average human and machine performance in percentage, their values cannot be directly compared

due to the diﬀerent human and machine experimental protocols. Stabinger et al. [2016] reinterpreted the
human performance by α∗(β) = (1+β)/2, assuming any successful subject can perfectly classify all test images
while any unsuccessful subjects make decisions at the chance level (i.e., 50%). In contrast, we reintepret the
machine performance by β∗(α), the probability for a trained machine to be successful in a human experiment
(see Figure 1).
It is clear to see that the α∗ reinterpretation overestimates the human performance and
underestimates the machine performance comparing to our β∗ reinterpretation.

2.3 Grouping SVRT problems by core characteristics

Fleuret et al. [2011] plotted for the 23 problems the mean number of the trials before a human subject

successfully detected the classiﬁcation rule and the number of unsuccessful subjects, showing that there were

4 problems relatively diﬃcult to human.

In these problems, no less than 7 out of the 20 subjects were

unsuccessful (β < 70%), which also took on average more than 12 images for the successful subjects before

detecting the classiﬁcation rules while the average is less than 7 for all other problems. In contrast, AdaBoost

yielded a test error over 10% (α < 90%) in 13 out of the 23 problems, when trained on 10,000 examples.

5

Figure 1: Reinterpretation of machine performance with the success probability β∗(α) for comparison with
the human data. A machine agent of a test classiﬁcation accuracy α attending a human experiment can be
modelled as a discrete Markov chain (as the diagram on the left), where Qk(N ) is the probability of the k
correct classiﬁcations in a row at the N -th step. As a success in the human experiment of Fleuret et al. [2011]
required 7 correct classiﬁcations in a row by seeing up to 35 examples, β∗(α) = Q7(35) is the probability
of the machine agent to succeed in the human experiment (the blue curve in the graph on the right). The
detailed derivation can be found in Section 5. The chance level of α is 50%, and that of β∗(α) is approximately
11.34%. Stabinger et al. [2016] reinterpreted the human performance by α∗(β) with β = [2α∗ −1]+ (the green
curve in the graph on the right), which eﬀectively overestimated the human performance and underestimated
the machine performance.

6

00.10.20.30.40.60.70.80.91Test classification accuracy in machine experiment00.10.20.30.40.50.60.70.80.91Success probability in human experiment*()*()chance level0.11340.5Ellis et al. [2015] considered a problem as solved by machines if α ≥ 90%, calculated the correlations between

individual machines and human performance (i.e., the Pearson’s r between α and β for each machine agent),

and remarked that their PS classiﬁer Sasquatch was the best machine agent, comparing to AdaBoost and

ConvNet, in terms of the number of solved problems and the correlation to the human performance.

As pointed out by Kim et al. [2018], however, comparing the number of solved problems is not a good

idea for the general comparison between human and machine performance on SVRT. If a machine agent is

able (or unable) to solve some problem, it is highly likely it would succeed (or fail) in similar problems. The

23 problems could contain many easy (or diﬃcult), similar problems for this agent, because the original 23

problems were arbitrarily designed. Moreover, these relatively high (or low) machine accuracy might have

uncorrected impact on the Pearson’s r, which is known to be not robust with respect to outliers, especially
when sample size is small and non-normal; in the statistical analysis on the correlation between human

and machine accuracies by Ellis et al. [2015], there are eﬀectively only 23 data points. In order to freely

control the generation of SVRT-like problems, Kim et al. [2018] proposed the PSVRT, so that they could

systematically investigate the correlations between the parameters that modulate the image variability and

the performance of their CNNs. Speciﬁcally, they ﬁrstly sorted the original 23 SVRT problems according

to their CNN performances, noticing that their CNNs were good at the SR problems but poor at the SD

problems. Then they developed the PSVRT which could generate random images based on to two core

parameters, shape sizes (relative to image size) and number of shapes, modulating SR and SD respectively

[Kim et al., 2018].

Based on the information encoded by image parsings and inspired by the idea of SD and SR, we propose

to group SVRT problems into diﬀerent types with respect to their core characteristics for classiﬁcation,

considering only two aspects of classiﬁcation features, namely shape speciﬁcation (SS) and location relation

(LR). In particular, we assume four levels of complexity in both SS and LR, which are summarised in

Table 2. With respect to SS, a classiﬁcation rule can only rely on the core characteristics whether two

shapes are similar in size (SS = 1), or whether they are identical (SS = 2) or identical under similarity

transformations (SS = 3). Any other geometric transformation would eﬀectively change shape identities

and reduce identical shapes to completely diﬀerent shapes, because all shapes are randomly generated. In

such cases, the classiﬁcation rule is independent of shape speciﬁcation (SS = 0). Thus, the four levels of SS

complexity characterise in principle all possible individual random shapes.

With respect to LR, a classiﬁcation rule not based on a trivial, isolating relation (LR = 0) either takes

shape coordinates into account (LR = 2 or 3) or not (LR = 1, a non-trivial topological relation). Generally

on a two-dimensional plane, there exists only four topological relations between two shapes of ﬁnite area:

isolating (the trivial one), bordering, containing and intersecting [Egenhofer and Franzosa, 1991]. As the

SVRT generator was designed to produce non-intersecting, random contours to avoid ambiguity in detecting

shape identities, the lowest two levels of LR (LR = 0 and 1) exhaust all possible pairwise topological relations,

which implicitly induce all possible topological relations among shapes in an image. In the cases when speciﬁc

shape coordinates contribute to the core characteristics for classiﬁcation, we assume that a non-linear relation

in the shape coordinates (LR = 3) is harder than a linear one (LR = 2). This assumption is made based
on the fact that PS processes non-linear numerical expressions much slower than linear ones, because the

underlying algorithm for non-linear arithmetics is diﬀerent from and more diﬃcult than linear arithmetics

[Bjørner et al., 2018]. In addition, a pattern determined by a linear relation is also seemingly easier for human

to detect, as shapes are likely to form a regular pattern, e.g., aligning in a line or at the corners of a square.

Thus, the four levels of LR cover in principle all possible location relations among shapes.

7

Shape speciﬁcation
All shapes are completely diﬀerent.
Some shapes are not identical but similar in size to one another.
Some shapes are identical to one another.
Some shapes are identical to one another under similarity transformations (i.e., scaling, reﬂection, rotation).

SS
0
1
2
3
LR Location relation
0
1
2
3

All shapes have a default, isolating relation.
Some shapes have a topological relation (i.e., a bordering or containing relation).
Some shapes have a linear relation in their coordinates (e.g., shapes aligned in a line/parallelogram).
Some shapes have a non-linear relation in their coordinates (e.g., equidistances between shapes).

Table 2: Deﬁnitions of the four levels of complexity with respect to SS and LR. All shapes are randomly
generated contours with no intersections (there is at least 1 pixel between two contours). Thus, the default,
null relation between two shapes corresponds to SS = LR = 0, and a problem cannot belong to this null
type because otherwise no classiﬁcation rule exists.

In summary, we can group any SVRT problems into 15 = 4 × 4 − 1 types in total according to SS

and LR (no problem can have SS = LR = 0). In order to solve problems of a particular type, the same

type of features are necessary for any human or machine; we call such information the core characteristics

for classiﬁcation. Since the core characteristics ignores any features shared by both categories, the pair of

(SS, LR) qualitatively characterises the lowest amount of information necessary for classiﬁcation.

The 23 original SVRT problems can be grouped into 8 types as shown in Table 3. It is worth noting that

the core characteristics for classiﬁcation is only necessary but not suﬃcient for classiﬁcation; in other words,

although diﬀerent problems of the same type require common core characteristics, their classiﬁcation rules are

not necessarily identical. For example, the classiﬁcation rule of problem #9 depends on whether the location

of a larger shape lies between two smaller ones, while problem #13 depends on whether the relative locations

of a larger shape with respect to a smaller one in two pairs are identical (i.e., whether there are two identical

meta-shapes, each consisting of a larger and a smaller one). In addition, SS and LR can be correlated to each

other in an image to some extent, whereas they seem conceptually independent to each other. For example,
detecting whether two shapes have a bordering relation (LR = 1) solves problem #2, while calculating how

close their coordinates are can also be useful (SS = 2), because the smaller shape is always inside the larger

one and with a high probability it lies about the centre of the larger shape if not having a bordering relation.

We assume problem #2 belonging to type (SS, LR) = (0, 1) rather than (SS, LR) = (2, 0), and the same

idea applies and determines problem types for similar situations. Moreover, there are some disagreements

between our SS-LR grouping and the SD-SR sorting by Kim et al. [2018], whereas the idea of SD is similar

to SS as they both describe indivdual shapes. They sorted problem #6, #9, #12, #13 and #17 into the

purely LR type, while we consider SS important for solving these 5 problems. For example, solving problem

#9 clearly requires the core characteristics of shape sizes, which we believe is a characteristic of individual

shapes rather than spatial relations between them.

3 Results

3.1 Baseline Adaboost performance with correct parsings

As pointed out in Section 2.1, the image parsings obtained by the Sasquatch parser contained systematic

errors in shape identity. They encoded no information of shape reﬂection, which is necessary for problem #16

8

LR

#10, #14, #18

3
2
1 #2, #3, #4, #11, #23
0

#12
#9, #13

#6, #17

#8

#1, #5, #7, #15, #19, #20, #21, #22 #16

0

1

2

3

SS

Table 3: Diﬀerent types of the original 23 SVRT problems. The problems are grouped by the core charac-
teristics for classiﬁcation deﬁned in Table 2.

Parsing type
Training size
Test size
#2
#3
#11
#16
#20
#21

Sasquatch

Corrected

20
80

1000
1000
59.90%
57.50%
51.25%
51.80%
50.00%
68.75%
100.00% 100.00%
100.00%
95.00%
100.00% 100.00%
50.00% 100.00% 100.00%
47.40%
38.75%
52.50%

Error type in Sasquatch parsings

Extra glitchy shapes

Incorrect shape identity

Table 4: AdaBoost performance with Sasquatch and our corrected parsings. For the problems not listed here,
the Sasquatch parsings are correct, and thus no signiﬁcant performance diﬀerences can be seen comparing
to our parsings. However, even for the listed problems that the Sasquatch parsings contain errors, only in
problem #20 the performance diﬀerence can be seen.

and #20. In problem #21, rotated and rescaled shapes were assigned diﬀerent shape identities unless the

transformations were trivially small. Occassionally in problem #2, #3 and #11 when the shapes in contact

were too close to one another, the parsings contained tiny shapes that should not exist; such glitchy shapes

were in fact the random spaces well bounded by other shapes.

Correcting these errors in the parsings, we expected the machine performance to increase for these prob-

lems. However, the performance of AdaBoost was boosted only in problem #20 (see Table 4). It is worth

noting that, while the Sasquatch parsings failed to serve AdaBoost well in problems #20, they led to decent

performance in problem #16. Although they were blind to shape reﬂection in both problem #16 and #20,

it treated reﬂected shapes as diﬀerent shapes and thus the classifcation rule of problem #16 changed from

‘whether shapes are reﬂected’ to ‘whether shapes are identical’, which coincidentally resulted in an indiﬀerent

performance. Meanwhile, this reﬂection blindness resulted in a complete failure in problem #20, because the

classiﬁcation rule depends on whether shapes are identical or not and this information was unretrievable in

the Sasquatch parsings. Since extra glitchy shapes appeared infrequently in problem #2, #3 and #11, the

performance were not changed signiﬁcantly.

Since the number of stumps is essentially the only parameter that determines the ﬁnal performance of

AdaBoost (whereas there is a tradeoﬀ in the learning eﬃciency for the learning rate or the base boosting

algorithm) [Pedregosa et al., 2011], we conducted the experiments for 10, 100, 1,000 or 10,000 stumps and

found the results extremely similar except for 10 stumps (and thus reported only the results for 10,000 stumps

in Table 4). Both Sasquatch and our parsings are in general poor higher-level representations for AdaBoost;

in fact, it yielded worse performance on such parsings than on features extracted directly from raw images

[Fleuret et al., 2011, Ellis et al., 2015].

9

Figure 2: The program synthesis performance with the Sasquatch and our corrected parsings. The machine
performance α is reinterpreted with β∗(α) by Figure 1B. The three histograms summarise the corresponding
distributions of β and β∗(α), where the averages are 86.30 ± 3.55% for the human data, 81.40 ± 7.01% for
the Sasquatch parsings and 88.99 ± 5.62% for the corrected parsings. By modifying the representation of
image parsings, the machine performance were signiﬁcantly enhanced in problem #20 and #21. Although
the machine performance achieved, even surpassed, the human performance at many tasks, it was noticeably
poor in problems #6, #12 and #17 (whose classiﬁcation rule is dependent on equidistance relations between
shapes). The human performance was calculated from the original data in Fleuret et al. [2011], and the
machine performance of Sasquatch was reproduced by the code made public by Ellis et al. [2015] at https:
//github.com/ellisk42/sasquatch.

3.2

Improved PS performance with corrected parsings

By correcting the parsings, the performance of the PS classiﬁer was boosted from the chance level to nearly

the perfect in problems #20 and #21, while it was similar in other problems (see Figure 2 and Table 6 for

more details). The PS performance is highly dependent on the parsings, which is one reason for grouping the

SVRT into diﬀerent types as any parsing should in principle encode all necessary information for classiﬁcation
(see Section 2.3).

Other than the performance measured by the statistics (α or β∗(α)), we also consider the performance
of the PS classiﬁer was improved in the sense of interpretability, which is one of the main advantages of

the PS approach. Similar to the discussion on the AdaBoost performance, the PS classiﬁer achieved decent

performance with both the Sasquatch and our parsings in problem #16. However, the synthesized programs

in the two cases were diﬀerent. With the incorrect Sasquatch parsings, the program for the positive category

was to ﬁrstly draw a group of three shapes and then another group; within either group the shapes were

identical. With our corrected parsings, the program was to draw six identical shapes with three of them

reﬂected.

10

SasquatchCorrectedHuman010.20.40.60.8#20,#21#16#16#20#21SasquatchCorrectedHuman10.80.60.40.20Other problems#6, #12, #17: Challenging for program synthesis#16, #20, #21: Incorret shape identities in Sasquatch #2, #3, #11: Extra glitchy shapes in Sasquatch Success probability in human experimentSuccess probability in human experiment015015015Number of problemsaverage success probabilityAlthough the performance of the PS classiﬁer was improved with the corrected parsings, achieving even

surpassing human performance in many problems, it was relatively poor in problem #17 and merely above

chance level in problem #6 and #12. The PS performance on these three problems rely on the most complex

core characteristics for classiﬁcation (LR = 3) as equidistance between shapes (a non-linear relation in

the shape coordinates) are involved. Although there was no constraint for the PS classiﬁer to access the

coordinate information in the parsings, its performance was inferior to the human level.

We did not attempt to re-engineer the parsings to encode the distance information directly. Although

the PS classiﬁer could conduct linear arithmetics over the distances rather than non-linear ones over the

coordinates, saving some computational expense, the re-engineering would be at a huge cost in a combinatorial

increase in the input sizes, which the PS classiﬁer could not aﬀord because it scales poorly with the input
dimensionality. Moreover, even if the re-engineered parsings had resulted in a decent performance in the

three problems, it could still be diﬃcult for the classiﬁer to generalise to unseen problems, as they encode

only the pieces of information according to explicit human instructions. For example, we did not extract the

information of shape rotation for the parsings, because it is not the core characteristics for classiﬁcation in all

the 23 problems. Thus, it is straightforward to design a new SVRT problem #101, which would be unsolvable

problem for the PS classiﬁer due to the rotation blindness of parsings, but not impossible for human subjects:

all images in both categories contain two shapes; in one category the two shapes are identical, while in the

other category the two shapes become identical after rotating.

In summary, we corrected the parsings so that they contain in principle all possible core characteristics

for classiﬁcation, which did lead to some improvement in the PS performance on many, but not all, SVRT

problems.

3.3 Challenging problems for humans and machines

Next, we compared the human and machine performance on the orignal 23 SVRT problems across the 8

diﬀerent types to which they belong (see Table 3). For compactness, we took the best CNN performance

reported in Stabinger et al. [2016], Kim et al. [2018] as their performance were similar on most of the problems

(see Table 6 for the detailed data). By averaging the performance on diﬀerent problems within each type,
we obtained human and machine performance (β and β∗(α)) with respect to diﬀerent levels of SS and LR as
shown in Figure 3. In general, problem types on the left or lower in this (SS, LR) plane are more complex

than those on the right or upper in terms of the core characteristics for classiﬁcation. Although the humans,

the best CNN and the PS classiﬁer with our corrected parsings actually yielded great performance in most of

the problem types, they exposed their weaknesses when facing some problems types. The human performance

was relatively poor in (SS, LR) = (2, 3) and (SS, LR) = (3, 0). The best CNN failed in (SS, LR) = (2, 0).

The performance of the PS classiﬁer with the correct parsings in (SS, LR) = (1, 3) and (SS, LR) = (2, 3)

was not signiﬁcantly better than the chance level.

As discussed in Section 3.2, the PS classiﬁer failed to achieve a decent performance because it could not

detected the equidistance relations between shapes, whereas the corrected parsings in principle provided it

with the information to calculate such relations. The problem types of LR = 3 are challenging to the PS

classiﬁer because non-linear arithmetics could be diﬃcult to its underlying theorem prover to verify.

The best CNN was worst in the problem type of (SS, LR) = (2, 0), which means it is diﬃcult for the

CNNs to detect identical shapes within an image. This limitation of CNNs was thoroughtly studied by Kim

et al. [2018]. However, the performance was high in more diﬃcult problem types of (SS, LR) = (3, 0), (2, 1)

and (2, 3). The great performance in the type of (SS, LR) = (3, 0) was only due to LeNet, as its test accuracy

11

Figure 3: Comparison between human and machine (the best CNN and the PS classiﬁer) performance on
diﬀerent groups of the SVRT problems. The group performance is depicted in propotion to the radii of the
arcs.

12

01230123100%050CNNPSHumanLocation RelationsShape Specificationis nearly perfect while that is poor for GoogLeNet and vanilla CNNs (see Table 6) [Stabinger et al., 2016,

Kim et al., 2018]. In contrast, they all yielded decent performance in the problem types of (SS, LR) = (2, 1)

and (2, 3), speciﬁcally, on problem #6, #8 and #17.

The human performance could be an evidence showing that it was sensible for us to group the SVRT

problems into these types. With an increase in the levels of either SS or LR, it required a human to extract

more core characteristics for classiﬁcation, making the problems more diﬃcult. Other than problem #6, #16

and #17, which lie on the most top and right on this (SS, LR) plane and thus yielded the worst 3 human

results out of the 23 problems, we also remark that problem #21 was the next most challenging problem for

many humans. In contrast, the human performance was nearly perfect on problem #19 and #20, whereas

the classiﬁcation rules of these 3 problems are seemingly alike in words and all belong to (SS, LR) = (0, 2).
There are always two shapes in an image, for the positive category the two shapes are identical up to a

similarity transformation, scaling for problem #19, reﬂection for problem #20 and rotation for problem #21,

and for the negative category the two shapes are simply diﬀerent.

In summary, according to our grouping by SS and LR, we found that ﬁnding non-linear relations in

shape coordinates are the most challenging to the PS classiﬁer, detection of identical shapes to the CNNs,

and complex core characteristics for classiﬁcation to the human subjects, whereas some diﬀerences between

problems within the same types might have been overlooked.

3.4 Few-shot learning by PS classiﬁer

One advantage of the PS classiﬁer is its capability of learning from a small set of examples [Ellis et al.,

2015]; in particular, data in the previous ﬁgures were obtained by testing the classiﬁer after training it with

3 positive and 3 negative examples. By varying the number of pairs t, the test accuracy α(t) is eﬀectively

the learning curve of the PS classiﬁer, which is depicted in Figure 4 for all the original 23 SVRT problems.

Although the PS classiﬁer never achieved decent performance on problem #6, #12 and #17, it started to

make correct classiﬁcations consistently if not perfectly in many other problems after seeing no more than 6

pairs of positive and negative examples. Comparing to the other ML techniques (e.g., AdaBoost, SVM and

CNNs), the PS classiﬁer needed a extremely small training set.

However, a noticeable decline in the performance was observed for problem #2 and #13. This behaviour of

PS was essentially caused by its formal veriﬁcation nature . As any program can be synthesized to instantiate
all the training images, an insigniﬁcant but misleading, random variability in the image parsings might be

treated incorrectly as the core characteristics for classiﬁcation, which weakens the program in generalising

to unseen images. Moreover, if such misleading variability is ever taken into account by PS, it becomes one

of the logical premises of the program. In other words, its impact cannot be removed or reduced. With an

increasing number of training examples, the probability of at least one appearance of such random mistakes

grows rapidly, which makes the PS classiﬁer less likely to achieve high performance. In contrast, a statistical

ML model (e.g., CNNs) may suﬀer a deterioration in its performance due to overﬁtting, which might also be

a result of too powerful a model and too small a training set. However, a statistical model is only penalised

by the deviation between its predication and the true label. As the frequency of such misleading variability

was presumbly constant, its impact could be balanced out given more input data. In summary, more data

could be beneﬁcial to the generalisation ability of a statistical ML classiﬁer. However, the same thing might

be destructive to that ability of a PS classiﬁer, unless the data (the training image parsings) are completely

free of random mistakes.

13

Figure 4: Learning curves of the PS classiﬁer for the 23 tasks. The classiﬁer reached optimal performance
on many tasks after learning a few examples (in green). However, noticeable declines were observed for task
#2 and #13 (in blue), which was caused by incorrect image parsings. Moreover, the classiﬁer struggled at
about chance level for task #6, #12, and #17 regardless of the number of the training examples (in orange).
The classiﬁcation rules of these three tasks are all dependent on equidistance relations between shapes, and
the program synthesizer was unable to contruct optimal programs encoding such relations. Standard errors
were calculated for all the data points, but only the maximal one for each size of training sets is depicted to
avoid messy plots. When the accuracies approach 100% (for many green curves), the corresponding standard
errors drop to 0%.

14

012345678910Number of pairs of postive and negative examples0.40.50.60.70.80.91Test classification accuracy#2#13#6,#12,#17chance levelmaximal standard error  among all the tasks4 Discussion

In this paper we have proposed a quantitative reintepretation of the human and machine performance on

each SVRT problem for a fair comparison despite the diﬀerences in the experimental protocols. We have also

grouped the problems into diﬀerent types according to their core characteristics for classiﬁcation, so that we

could analyse and compare the human and machine performance on all the SVRT problems in a systematic

manner despite of the arbitrary design of the original problems.

We conﬁrmed with AdaBoost that the classiﬁcation based on the parsings did not make the SVRT trivial,

while incorrect parsings could make a problem unsolvable. The PS performance was dependent on the quality

of the images parsings. With the corrected parsings, we improved the general performance on the original

SVRT problems. However, the PS classiﬁer still failed to detect equidistance relations in problem #6, #12

and #17, whereas it had access to the shape coordinates and was able to construct a program including a

command encoding distances (computed from the coordinates). We also conducted the machine experiments

on these problems with the time limit of the synthesizer set to be 10 times and even 100 times larger than the

original one. No improvement in the performance was observed. We believe the failure is essentially due to

combinatorially complexity increase in the non-linear veriﬁcation in SMT. Movement is the only command in

the synthesizer that is non-linear in general because a displacement is calculated from a moving distance and

an orientation angle, which happens to be linear when there is only one initial angle. Thus, the PS classiﬁer

could solve, for example, problem #13 but not #12.

It would also fail to generalise to other problems

involving distances according to our grouping of the problems, unless a substaintially larger computational

power could be deployed.

We grouped the problems based on the core characteristics for classiﬁcation. The complexity level of the
core characteristics was qualitatively ranked in the two aspects of SS and LR, while the complexity of the

real classiﬁcation rules could be completely diﬀerent.

In fact, the PS classiﬁer chooses optimal programs

by its evaluator measuring the complexity of constructed programs. We did not group the problems by the

classiﬁcation rules, because the core characteristics is more objective. The complexity of programs is only

deﬁned for the PS classiﬁer. Humans might deduce diﬀerent rules for the same category due to individual

diﬀerences, especially when the images appear complicated. It is also not trivial for human to interpret a

hyperplane found by AdaBoost or SVM or latent variables of a well-trained CNN, whereas they are virtually

the classiﬁcation rules to the machine agents. As the classiﬁcation rules had to be found from the core

characteristics, it was expected that the problems of higher SS and LR should on average yield no better

performance than those of a lower SS and LR. However, we noted that the CNNs were better in the problem

types of (SS, LR) = (2, 1) and (2, 3) than (2, 0). One possible explanation could be the correlation between

SS and LR, whereas conceptually they might seem independent. Imposing more or stronger constraints solely

on the locations of identical shapes might have made the entire image more regular, resulting in a spatial

pattern that could be more easily detected by the CNNs without even noticing which shapes were identical.

This argument would be consistent with the point of view of Kim et al. [2018], as they considered problem

#6 and #17 to be purely about LR.

On the contrary, the recent success of training ResNet50 (a variant of CNN) to a high performance on

all the 23 SVRT problems by Borowski et al. [2019] demonstrated the ability of feedforward convolutional

architectures to detect identical shapes, whereas some problem types might remain diﬃcult. Thus, in terms of

classiﬁcation accuracy, a well-trained CNN is for now the best machine agent for the SVRT. However, decent

machine performance does not necessarily imply a human-like concept being learnt by a machine [Borowski

et al., 2019], while the PS approach is intepretable to human as constructed programs are counterpart to the

15

classiﬁcation rules, which might be associated to semantic memory in human brain. Considering its capability

of unsupervised, few-shot learning, the PS approach is advantageous in some aspects, whereas scalability is

a major, general limitation. Due to this inherent limitation, the PS performance has to be dependent on the

quality of image parsings.

One of the most fundamental diﬀerences between the PS and the statistical ML classiﬁer (e.g., AdaBoost,

CNNs) is how much prior knowledge correlated to the SVRT is manually encoded into the machine. We

consider such prior knowledge particularly worthy of discussion when comparing machine and human per-

formance and when investigating human-like computation in machines. Despite the fact that the human

subjects had never seen the SVRT previously as the images were randomly generated, the human subjects

could hold some prior knowledge correlated to the SVRT before the experiment, perhaps because the images
involve higher-level conﬁgurations that biological visiual systems can perceive eﬀortlessly due to evolution

[Fleuret et al., 2011].

In contrast, the statistical ML classiﬁers were trained from scratch [Fleuret et al.,

2011, Ellis et al., 2015, Stabinger et al., 2016, Kim et al., 2018, Borowski et al., 2019], holding little prior

knowledge except that for the general purpose of visual recognition. The SVRT images are known to consist

of black and white pixels only; AdaBoost and SVM were based on some feature extractors; and all the CNNs

deployed ﬁlters assuming translational symmetry. Although transfer learning is a common recipe for ﬁlling

the gap of such prior knowledge to achieve few-shots learning in many ML contexts [Pan and Yang, 2009,

Yosinski et al., 2014], it is not a promising solution to the SVRT, because without careful choice of model

parameters they cannot perform decent classiﬁcations even after being trained on thousands of examples in

some problem. Much more prior knowledge were manually encoded in the PS classiﬁer. Although it assumes

that in general the synthesizer can construct a program to instantiate any SVRT images in any category, it

could be arguable whether the classiﬁer is specially engineered for the original 23 problems. Our grouping

thus became relevant in the analysis, because we could see which problem types, not individual problems,

are trivial but which are challenging to machines.

Although Borowski et al. [2019] showed that their machine agent could solve all the 23 problems, the

comparison between human and machine performance remains a complicated issue, because the protocols

of the human and the machine experiments are diﬀerent. Each human subject was learning and tested

at the same time througout the experiment, while the machines are trained and tested separately.

It is

straightforward to simulate how a machine agent, capable of online learning, makes classiﬁcations as if it is

in a human experiment. However, the issue of prior knowledge would become more relevant as one would

have to decide to what degree the agent should be trained before implementing transfer learning and online

learning. For the PS classiﬁer, its poor performance on distance relations has to be ﬁxed and a training

strategy preventing its fast performance decline needs to be deployed.

The PS classiﬁer is advantageous for its capability of unsupervised, few-shot learning, because the un-

derlying theorem prover, Z3, performs symbolic computation using high level representation of images (i.e.,

parsings). For the same reason, it has a scalability problem and relies on the parsings, while the CNNs do

not have such limitations. The diﬀerent advantages and disadvantages of the symbolic and neural approaches

suggests their combination might lead to greater machine performance, e.g., Ellis et al. [2018], Huang et al.
[2018], Minervini et al. [2018], Selsam et al. [2018].

16

References

Nikolaj Bjørner, Leonardo de Moura, Lev Nachmanson, and Christoph Wintersteiger. Programming Z3,

2018. URL http://http://theory.stanford.edu/~nikolaj/programmingz3.html.

Judy Borowski, Christina M Funke, Karolina Stosio, Wieland Brendel, Thomas S A Wallis, and Bethge

Matthias. The notorious diﬃculty of comparing human and machine perception. In Proceedings of the

Conference on Cognitive Computational Neuroscience, 2019.

Stephen A Cook. The complexity of theorem-proving procedures. In Proceedings of the third annual ACM

symposium on Theory of computing, pages 151–158. ACM, 1971.

Max J Egenhofer and Robert D Franzosa. Point-set topological spatial relations. International Journal of

Geographical Information System, 5(2):161–174, 1991.

Kevin Ellis, Armando Solar-Lezama, and Josh Tenenbaum. Unsupervised learning by program synthesis. In

Advances in Neural Information Processing Systems, pages 973–981, 2015.

Kevin Ellis, Lucas Morales, Mathias Sabl Meyer, Armando Solar-Lezama, and Joshua B Tenenbaum. Dream-

coder: Bootstrapping domain-speciﬁc languages for neurally-guided bayesian program learning. In Neural

Abstract Machines and Program Induction Workshop at NIPS, 2018.

François Fleuret, Ting Li, Charles Dubout, Emma K Wampler, Steven Yantis, and Donald Geman. Compar-

ing machines and humans on a visual categorization test. Proceedings of the National Academy of Sciences,

2011.

Daniel Huang, Prafulla Dhariwal, Dawn Song, and Ilya Sutskever. Gamepad: A learning environment for

theorem proving. arXiv preprint arXiv:1806.00608, 2018.

Junkyung Kim, Matthew Ricci, and Thomas Serre. Not-So-CLEVR: learning same–diﬀerent relations strains

feedforward neural networks. Interface focus, 8(4):20180011, 2018.

Pasquale Minervini, Matko Bosnjak, Tim Rocktäschel, and Sebastian Riedel. Towards neural theorem proving

at scale. arXiv preprint arXiv:1807.08204, 2018.

Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data

engineering, 22(10):1345–1359, 2009.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,

R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.

Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo de Moura, and David L Dill. Learning

a sat solver from single-bit supervision. arXiv preprint arXiv:1802.03685, 2018.

Sebastian Stabinger, Antonio Rodríguez-Sánchez, and Justus Piater. 25 years of CNNs: Can we compare to

human abstraction capabilities? In International Conference on Artiﬁcial Neural Networks, pages 380–387.

Springer, 2016.

Jason Yosinski, Jeﬀ Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural

networks? In Advances in neural information processing systems, pages 3320–3328, 2014.

17

5 Appendix

5.1 Summary of the SVRT problems: example images, classiﬁcation rules and

problem types.

The original 23 SVRT problems were generated using our code, which was forked from Fleuret et al. [2011].

Problem Example

Basic feature

#1

+

-

Basic feature

#2

+

-

Classiﬁcation rule

There are two shapes.

(SS, LR)

(2, 0)

The two shapes are identical.

The two shapes are diﬀerent.

There are two shapes. The small

shape is inside the large one.

(0, 1)

The small shape is near the

centre of the large one.

The small shape is near the

boundary of the large one.

Basic feature

There are four shapes.

#3

(0, 1)

+

-

Three shapes are in contact.

Within each pair, the two

shapes are in contact.

18

Problem Example

Basic feature

#4

+

-

Classiﬁcation rule

There are two shapes.

The small shape is inside the

large one.

(SS, LR)

(0, 1)

The small shape is outside the

large one.

Basic feature

There are four shapes.

#5

(2, 0)

+

-

Basic feature

#6

+

-

There are two pairs of identical

shapes.

The four shapes are diﬀerent.

There are two pairs of identical

shapes.

(2, 3)

The distances within the pairs

are the same.

The distances within the pairs

are random.

Basic feature

There are six shapes.

#7

(2, 0)

+

-

There are three groups of two

identical shapes.

There are two groups of three

identical shapes.

19

Problem Example

Basic feature

#8

+

-

Classiﬁcation rule

There are two shapes.

(SS, LR)

(2, 1)

The small shape is inside the

large one AND they are similar.

The small shape is outside the

large one OR they are diﬀerent.

Basic feature

There are three shapes in a line.

+

-

The large shape is in between
the two small ones.

The large shape is on one end.

Basic feature

There are four identical shapes.

+

-

The shapes form a square.

The shape locations are random.

Basic feature

There are two shapes.

(1, 2)

(0, 2)

(0, 1)

#9

#10

#11

+

-

The two shapes are in contact.

The two shapes are not in

contact.

20

Problem Example

Classiﬁcation rule

(SS, LR)

Basic feature

There are three shapes.

#12

(1, 3)

+

-

Basic feature

#13

+

-

The two small shapes are close

to each other.

The shape locations are random.

There are two identical large

shapes and two identical small

shapes.

(1, 2)

The two meta-shapes are

identical; a meta-shape is a pair

of a large and a small shape.

The shape locations are random.

Basic feature

There are three identical shapes.

#14

(0, 2)

+

-

The shapes form a line.

The shape locations are random.

Basic feature

#15

There are four shapes of the

same size. They form a square.

(2, 0)

+

-

The four shapes are identical.

The four shapes are diﬀerent.

21

Problem Example

Classiﬁcation rule

(SS, LR)

Basic feature

#16

+

-

Basic feature

#17

+

-

There are six identical shapes.

Their locations are symmetric

with respect to the vertical

bisector of the image.

(3, 0)

Three shapes are reﬂected.

No shapes are reﬂected.

There are four shapes, three of

which are identical.

The distance between each of

the three identical shapes and

the diﬀerent one is the same.

The shape locations are random.

(2, 3)

(0, 2)

Basic feature

There are six identical shapes.

#18

+

-

The shape locations are

symmetric with respect to the

vertical bisector of the image.

The shape locations are random.

22

Problem Example

Classiﬁcation rule

(SS, LR)

Basic feature

#19

+

-

There are two shapes of

diﬀerent sizes.

(2, 0)

The two shapes are identical by

scaling.

The two shapes are diﬀerent.

#20

#21

#22

Basic feature

There are two shapes.

+

-

The two shapes are identical by

reﬂection.

The two shapes are diﬀerent.

Basic feature

There are two shapes.

+

-

The two shapes are identical by

scaling and rotation.

The two shapes are diﬀerent.

Basic feature

There are three shapes in a line.

+

-

The shapes are identical.

The three shapes are diﬀerent.

(2, 0)

(2, 0)

(2, 0)

23

Problem Example

Classiﬁcation rule

(SS, LR)

Basic feature

#23

There are two small and one

large shapes.

(0, 1)

+

-

Basic feature

#101

+

-

Both small shapes are inside

OR outside the large one.

One small shape is inside the

large one AND the other is

outside.

There are two shapes (up to

rotation).

The two shapes are identical

without rotation.

The two shapes are identical by

rotation.

(3, 0)

5.2 Reinterpretation of machine performance with the success probability

Since the statistical ML classiﬁers needs to be trained with thousands of examples, they cannot perform

classiﬁcation and learning from scratch simultaneously with only 35 images. We thus consider that they

have been well trained, and assume that, when participating this human-like machine experiment, they

do not update anymore. Under this assumption, the experiment for a classiﬁer of the test classiﬁcation

accuracy α can be modelled as a Markov chain as shown in Figure 1. It consists of K + 1 states representing

k ∈ {0, 1, 2, . . . , γ − 1, K} correct classiﬁcations in a row. The initial state is k = 0, and k = K is an

absorbing state which represents the success in the test. The transition probability from the state k to
k + 1 is α and that from k to 0 is 1 − α for k (cid:54)= K. By deﬁning Qk(N ) to be the probability of the
state k and Q(N ) = [Q0(N ), Q1(N ), Q2(N ), · · · , QK−1(N ), QK(N )]T to be the probability distribution over
k ∈ {0, 1, 2, · · · , K − 1, K} at the N -th step, we can write down

Q(N ) = MKQ(N − 1),

(1)

24

Human

PS
Sasquatch Corrected

Best

CNN

Task
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

β
95%
100%
100%
100%
80%
40%
80%
100%
85%
95%
100%
90%
85%
95%
90%
55%
55%
85%
95%
95%
65%
100%
100%

100.00%
100.00%
99.17%
100.00%
98.59%
20.57%
94.17%
99.98%
100.00%
100.00%
100.00%
16.56%
88.91%
100.00%
100.00%
99.96%
40.06%
99.99%
92.59%
11.34%
11.34%
100.00%
99.05%

46.24%
98.06%
22.82%
99.95%

β∗(α)
33.88%
57%
100.00%
100.00% 100%
92.51%
97.29%
100.00% N/A
100.00% 100.00% 100%
54%
99.75%
76%
15.51%
53%
93.79%
99.97%
94%
100.00% 100.00% 100%
100.00% 100.00% 99%
100.00% 100.00% N/A
100.00% 97%
12.51%
99.32%
98.09%
N/A
100.00% 90%
99.99%
100.00%
52%
57.76%
100.00% 100.00% 98%
99.98%
37.73%
75%
100.00% 99%
99.94%
51%
30.94%
100.00%
55%
22.82%
100.00%
51%
28.15%
100.00%
59%
37.24%
100.00%
100.00% 87%
99.67%

LeNet GoogLeNet Vanilla
α
50%
100%
N/A
100%
50%
86%
50%
91%
100%
100%
N/A
100%
N/A
100%
50%
50%
95%
99%
50%
50%
51%
50%
100%

61.1%
100%
100%
100.0%
65.3%
87%
56.6%
93.4%
88.6%
100.0%
100.0%
100.0%
89.7%
96.1%
68.9%
76.5%
88.4%
100.0%
60.0%
56.6%
58.9%
62.3%
93.2%

Table 6: Summary of the machine and human performances on the original 23 SVRT problems. The human
data came from Fleuret et al. [2011]. The PS data was obtained by our experiments. With the Sasquatch
parsing, the experiment was a replication of that in Ellis et al. [2015]. The LeNet and GoogLeNet data came
from Stabinger et al. [2016]. The vanilla CNN data came from Kim et al. [2018]. The highest classiﬁcation
accuracy among LeNet, GoogLeNet and vanilla CNN for each problem was chosen as the best CNN accuracy
α, and was reinterpreted as success probability β∗(α).

where

MK =














1 − α 1 − α · · ·

1 − α 1 − α 0

α

0
...
0

0

0

α
...
0

0

· · ·

· · ·
. . .
· · ·

· · ·

0

0
...
α

0

0

0
...
0

α

0

0
...
0

1














is the probability transition matrix. Therefore,

Q(N ) = MN

KQ(0),

(2)

(3)

where Q(0) = [1, 0, 0, · · · , 0, 0]T is the initial condition. In particular, β∗(α) = Q7(35) is the probability for a
classiﬁer to achieve a success using the human criterion of Fleuret et al. [2011] (depicted in Figure 1). Using
this relationship, we can interpret the data of machine accuracy α directly into machine success rate β∗(α)
which is directly comparable to the human success rate β (see Table 6).

25

