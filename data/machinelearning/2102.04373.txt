1
2
0
2

t
c
O
0
2

]

C
O
.
h
t
a
m

[

2
v
3
7
3
4
0
.
2
0
1
2
:
v
i
X
r
a

Partition-Based Formulations for Mixed-Integer
Optimization of Trained ReLU Neural Networks

Calvin Tsay
Department of Computing
Imperial College London
c.tsay@imperial.ac.uk

Jan Kronqvist
Department of Mathematics
KTH Royal Institute of Technology
jankr@kth.se

Alexander Thebelt
Department of Computing
Imperial College London
alexander.thebelt18@imperial.ac.uk

Ruth Misener
Department of Computing
Imperial College London
r.misener@imperial.ac.uk

Abstract

This paper introduces a class of mixed-integer formulations for trained ReLU
neural networks. The approach balances model size and tightness by partitioning
node inputs into a number of groups and forming the convex hull over the partitions
via disjunctive programming. At one extreme, one partition per input recovers the
convex hull of a node, i.e., the tightest possible formulation for each node. For
fewer partitions, we develop smaller relaxations that approximate the convex hull,
and show that they outperform existing formulations. Speciﬁcally, we propose
strategies for partitioning variables based on theoretical motivations and validate
these strategies using extensive computational experiments. Furthermore, the
proposed scheme complements known algorithmic approaches, e.g., optimization-
based bound tightening captures dependencies within a partition.

1

Introduction

Many applications use mixed-integer linear programming (MILP) to optimize over trained feed-
forward ReLU neural networks (NNs) [5, 14, 18, 23, 33, 37]. A MILP encoding of a ReLU-NN
enables network properties to be rigorously analyzed, e.g., maximizing a neural acquisition function
[34] or verifying robustness of an output (often classiﬁcation) within a restricted input domain [6].
MILP encodings of ReLU-NNS have also been used to determine robust perturbation bounds [9],
compress NNs [28], count linear regions [27], and ﬁnd adversarial examples [17]. The so-called big-
M formulation is the main approach for encoding NNs as MILPs in the above references. Optimizing
the resulting MILPs remains challenging for large networks, even with state-of-the-art software.

Effectively solving a MILP hinges on the strength of its continuous relaxation [10]; weak relaxations
can render MILPs computationally intractable. For NNs, Anderson et al. [3] showed that the big-M
formulation is not tight and presented formulations for the convex hull (i.e., the tightest possible
formulation) of individual nodes. However, these formulations require either an exponential (w.r.t.
inputs) number of constraints or many additional/auxiliary variables. So, despite its weaker continuous
relaxation, the big-M formulation can be computationally advantageous owing to its smaller size.

Given these challenges, we present a novel class of MILP formulations for ReLU-NNs. The
formulations are hierarchical: their relaxations start at a big-M equivalent and converge to the
convex hull. Intermediate formulations can closely approximate the convex hull with many fewer
variables/constraints. The formulations are constructed by viewing each ReLU node as a two-part
disjunction. Kronqvist et al. [21] proposed hierarchical relaxations for general disjunctive programs.

 
 
 
 
 
 
This work develops a similar hierarchy to construct strong and efﬁcient MILP formulations speciﬁc to
ReLU-NNs. In particular, we partition the inputs of each node into groups and formulate the convex
hull over the resulting groups. With fewer groups than inputs, this approach results in MILPs that are
smaller than convex-hull formulations, yet have stronger relaxations than big-M.

Three optimization tasks evaluate the new formulations: optimal adversarial examples, robust
veriﬁcation, and (cid:96)1-minimally distorted adversaries. Extensive computation, including with convex-
hull-based cuts, shows that our formulations outperform the standard big-M approach with 25% more
problems solved within a 1h time limit (average 2.2X speedups for solved problems).

Related work. Techniques for obtaining strong relaxations of ReLU-NNs include linear program-
ming [16, 35, 36], semideﬁnite programming [12, 25], Lagrangian decomposition [7], combined
relaxations [31], and relaxations over multiple nodes [30]. Recently, Tjandraatmadja et al. [32]
derived the tightest possible convex relaxation for a single ReLU node by considering its multivariate
input space. These relaxation techniques do not exactly represent ReLU-NNs, but rather derive valid
bounds for the network in general. These techniques might fail to verify some properties, due to their
non-exactness, but they can be much faster than MILP-based techniques. Strong MILP encoding of
ReLU-NNs was also studied by Anderson et al. [3], and a related dual algorithm was later presented
by De Palma et al. [13]. Our approach is fundamentally different, as it constructs computationally
cheaper formulations that approximate the convex hull, and we start by deriving a stronger relaxation
instead of strengthening the relaxation via cutting planes. Furthermore, our formulation enables input
node dependencies to easily be incorporated.

Contributions of this paper. We present a new class of strong, yet compact, MILP formulations for
feed-forward ReLU-NNs in Section 3. Section 3.1 observes how, in conjunction with optimization-
based bound tightening, partitioning input variables can efﬁciently incorporate dependencies into
MILP formulations. Section 3.2 builds on the observations of Kronqvist et al. [21] to prove the
hierarchical nature of the proposed ReLU-speciﬁc formulations, with relaxations spanning between
big-M and convex-hull formulations. Sections 3.3–3.4 show that formulation tightness depends on
the speciﬁc choice of variable partitions, and we present efﬁcient partitioning strategies based on both
theoretical and computational motivations. The advantages of the new formulations are demonstrated
via extensive computational experiments in Section 4.

2 Background

2.1 Feed-forward Neural Networks

A feed-forward neural network (NN) is a directed acyclic graph with nodes structured into k layers.
Layer k receives the outputs of nodes in the preceding layer k − 1 as its inputs (layer 0 represents
inputs to the NN). Each node in a layer computes a weighted sum of its inputs (known as the
preactivation), and applies an activation function. This work considers the ReLU activation function:
y = max(0, wT x + b)
(1)
where x ∈ Rη and y ∈ [0, ∞) are, respectively, the inputs and output of a node (wT x + b is termed
the preactivation). Parameters w ∈ Rη and b ∈ R are the node weights and bias, respectively.

2.2 ReLU Optimization Formulations

In contrast to the training of NNs (where parameters w and b are optimized), optimization over a
NN seeks extreme cases for a trained model. Therefore, model parameters (w, b) are ﬁxed, and the
inputs/outputs of nodes in the network (x, y) are optimization variables instead.

Big-M Formulation. The ReLU function (1) is commonly modeled [9, 23]:

y ≥ (wT x + b)
y ≤ (wT x + b) − (1 − σ)LB 0
y ≤ σUB 0

(2)

(3)

(4)
where y ∈ [0, ∞) is the node output and σ ∈ {0, 1} is a binary variable corresponding to the on-off
state of the neuron. The formulation requires the bounds (big-M coefﬁcients) LB 0, UB 0 ∈ R, which
should be as tight as possible, such that (wT x + b) ∈ [LB 0, UB 0].

2

Disjunctive Programming [4]. We observe that (1) can be modeled as a disjunctive program:
(cid:21)

(cid:20)

(cid:21)

y = 0
wT x + b ≤ 0

(cid:20) y = wT x + b
wT x + b ≥ 0

∨

(5)

The extended formulation for disjunctive programs introduces auxiliary variables for each disjunction.
Instead of directly applying the standard extended formulation, we ﬁrst deﬁne z := wT x and assume
z is bounded. The auxiliary variables za ∈ R and zb ∈ R can then be introduced to model (5):

wT x = za + zb
za + σb ≤ 0
zb + (1 − σ)b ≥ 0
y = zb + (1 − σ)b
σLB a ≤ za ≤ σUB a
(1 − σ)LB b ≤ zb ≤ (1 − σ)UB b

(6)
(7)

(8)

(9)
(10)

(11)

where again y ∈ [0, ∞) and σ ∈ {0, 1}. Bounds LB a, UB a, LB b, UB b ∈ R are required for this
formulation, such that za ∈ σ[LB a, UB a] and zb ∈ (1 − σ)[LB b, UB b]. Note that the inequalities in
(5) may cause the domains of za and zb to differ. The summation in (6) can be used to eliminate either
za or zb; therefore, in practice, only one auxiliary variable is introduced by formulation (6)–(11).

Importance of relaxation strength. MILP is often solved with branch-and-bound, a strategy that
bounds the objective function between the best feasible point and its tightest optimal relaxation. The
integral search space is explored by “branching” until the gap between bounds falls below a given
tolerance. A tighter, or stronger, relaxation can reduce this search tree considerably.

3 Disaggregated Disjunctions: Between Big-M and the Convex Hull

Our proposed formulations split the sum z = wT x into partitions: we will show these formulations
have tighter continuous relaxations than (6)–(11). In particular, we partition the set {1, ..., η} into
subsets S1 ∪ S2 ∪ ... ∪ SN = {1, ..., η}; Sn ∩ Sn(cid:48) = ∅ ∀n (cid:54)= n(cid:48). An auxiliary variable is then
introduced for each partition, i.e., zn = (cid:80)
n=1 zn, the
disjunction (5) becomes:

wixi. Replacing z = wT x with (cid:80)N

i∈Sn

(cid:20)

y = 0
n=1 zn + b ≤ 0

(cid:80)N

(cid:21)

(cid:20)

∨

y = (cid:80)N

n=1 zn + b
y ≥ 0

(cid:21)

(12)

Assuming now that each zn is bounded, the extended formulation can be expressed using auxiliary
variables za
n results in our
proposed formulation (complete derivation in appendix A.1):

n for each zn. Eliminating za

n via the summation zn = za

n and zb

n + zb

(cid:88)

n
(cid:88)

n

y =

(cid:33)

wixi − zb
n

+ σb ≤ 0

(cid:32)

(cid:88)

i∈Sn

zb
n + (1 − σ)b ≥ 0

(cid:88)

zb
n + (1 − σ)b

n
σLB a

n ≤

(cid:88)

wixi − zb

n ≤ σUB a
n,

(13)

(14)

(15)

(16)

∀n = 1, ..., N

i∈Sn
n ≤ zb

(1 − σ)LB b

n ≤ (1 − σ)UB b
n,
where y ∈ [0, ∞) and σ ∈ {0, 1}. We observe that (13)–(17) exactly represents the ReLU node
(given that the partitions zn are bounded in the original disjunction): the sum zn = (cid:80)
wixi is
substituted in the extended convex hull formulation [4] of disjunction (12). Note that, compared to the
general case presented by Kronqvist et al. [21], both sides of disjunction (12) can be modeled using
the same partitions, resulting in fewer auxiliary variables. We further note that domains [LB a
n, UB a
n]
and [LB b

n] may not be equivalent, owing to inequalities in (12).

∀n = 1, ..., N

n, UB b

(17)

i∈Sn

3

3.1 Obtaining and Tightening Bounds

The big-M formulation (2)–(4) requires valid bounds (wT x + b) ∈ [LB 0, UB 0]. Given bounds for
¯ i, ¯xi], interval arithmetic gives valid bounds:
each input variable, xi ∈ [x
LB 0 =
¯ imax(0, wi) + ¯ximin(0, wi)) + b
(x

(18)

(cid:88)

UB 0 =

i
(cid:88)

i

(¯ximax(0, wi) + x

¯ imin(0, wi)) + b

(19)

But (18)–(19) do not provide the tightest valid bounds in general, as dependencies between the input
nodes are ignored. Propagating the resulting over-approximated bounds from layer to layer leads
to increasingly large over-approximations, i.e., propagating weak bounds through layers results in a
signiﬁcantly weakened model. These bounds remain in the proposed formulation (13)–(17) in the
form of bounds on both the output y and the original variables x (i.e., outputs of the previous layer).

Figure 1: Hierarchical relaxations from N = 1 (equiv. big-M) to N = 20 (convex hull of each
node over a box domain) for a two-input (x1, x2) NN trained on scaled Ackley function, with output
f (x). Top row: zb
n bounds obtained using interval arithmetic; Bottom row: zb
n bounds obtained by
optimization-based bound tightening. The partitions are formed using the equal size strategy.

n, UB b

n]. In other words, [LB a

Optimization-Based Bound Tightening (OBBT) or progressive bounds tightening [33], tightens
variable bounds and constraints [15]. For example, solving the optimization problem with the
objective set to minimize/maximize (wT x + b) gives bounds: min(wT x + b) ≤ wT x + b ≤
max(wT x + b). To simplify these problems, OBBT can be performed using the relaxed model (i.e.,
σ ∈ [0, 1] rather than σ ∈ {0, 1}), resulting in a linear program (LP). In contrast to (18)–(19), bounds
from OBBT incorporate variable dependencies. We apply OBBT by solving one LP per bound.
The partitioned formulation (13)–(17) requires bounds such that za
n ∈ (1 −
σ)[LB b
n, UB a
n when the node is inactive
(σ = 1) and vice versa. These bounds can also be obtained via OBBT: min((cid:80)
n ≤
max((cid:80)
wixi); wT x + b ≤ 0. The constraint on the right-hand side of disjunction (12) can
be similarly enforced in OBBT problems for zb
n. In our framework, OBBT additionally captures
dependencies within each partition Sn. Speciﬁcally, we observe that the partitioned OBBT problems
effectively form the convex hull over a given polyhedron of the input variables, in contrast to the
convex hull formulation, which only considers the box domain deﬁned by the min/max of each
input node [3]. Since (cid:80)
n = 0, the bounds [min((cid:80)
wixi),
max((cid:80)
n. These bounds can be from (18)–(19), or by solving
two OBBT problems for each partition (2N LPs total). This simpliﬁcation uses equivalent bounds for
za
n and zb
n separately
(4N LPs total).

n, but tighter bounds can potentially be found by performing OBBT for za

n] is a valid domain for za

wixi)] are valid for both za

n ∈ σ[LB a

wixi) ≤ za

wixi = za

n and za

n and zb

n and zb

n, UB a

n + zb

n], zb

nzb

i∈Sn

i∈Sn

i∈Sn

i∈Sn

i∈Sn

Figure 1 compares the proposed formulation with bounds from interval arithmetic (top row) vs OBBT
(bottom row). The true model outputs (blue), and minimum (orange) and maximum (green) outputs
of the relaxed model are shown over the input domain. The NNs comprise two inputs, three hidden
layers with 20 nodes each, and one output. As expected, OBBT greatly improves relaxation tightness.

4

x101x20100.511.5N=1x101x20100.511.5N=2x101x20100.511.5N=5x101x201f(x)01.5N=20x101x20100.511.5N=1x101x20100.511.5N=2x101x20100.511.5N=5x101x201f(x)01.5N=203.2 Tightness of the Proposed Formulation

Proposition 1. (13)–(17) has the equivalent non-lifted (i.e., without auxiliary variables) formulation:

(cid:88)

i∈I\Ij

(cid:88)

y ≤

wixi + σ(b +

i∈Ij
y ≥ wT x + b
y ≤ σUB 0

UB i) + (σ − 1)(

(cid:88)

LB i),

∀j = 1, ..., 2N

(20)

i∈Ij

(21)

(22)

where UB i and LB i denote the upper and lower bounds of wixi. The set I denotes the input indices
{1, ..., η}, and the subset Ij denotes the indices contained by the union of the j-th combination of
partitions in {S1, ..., SN }.

Proof. Formulation (13)–(17) introduces N auxiliary variables zb
Fourier-Motzkin elimination (appendix A.2), resulting in combinations I1, ..., IJ , J = 2N .

n, which can be projected out using

Remark. When N < η, the family of constraints in (20) represent a subset of the inequalities used to
deﬁne the convex hull by Anderson et al. [3], where UB i and LB i would be obtained using interval
arithmetic. Therefore, though a lifted formulation is proposed in this work, the proposed formulations
have non-lifted relaxations equivalent to pre-selecting a subset of the convex hull inequalities.

Proposition 2. Formulation (13)–(17) is equivalent to the big-M formulation (2)–(4) when N = 1 .

i=1 wixi = wT x, and
n zn = z1 = z. Conversely, big-M can be seen as the convex hull over a single aggregated

Proof. When N = 1, it follows that S1 = {1, .., η}. Therefore, z1 = (cid:80)η
(cid:80)
“variable,” z = wT x.

Proposition 3. Formulation (13)–(17) represents the convex hull of (1), given the inputs x are
bounded, for the case of N = η.

Proof. When N = η, it follows that Sn = {n}, ∀n = 1, .., η, and, consequently, zn = wnxn. Since
zn are linear transformations of each xn (as are their respective bounds), forming the convex hull
over zn recovers the convex hull over x. An extended proof is provided in appendix A.3.

Proposition 4. A formulation with N partitions is strictly tighter than any formulation with (N − 1)
partitions that is derived by combining two partitions in the former.

Proof. When combining two partitions, i.e., S(cid:48)
N −1 ⊆
Ij are also obtained by {SN −1, SN } ⊆ Ij. In contrast, those obtained by SN −1 ∨ SN ⊆ Ij cannot
be modeled by S(cid:48)
N −1. Since each constraint in (20) is facet-deﬁning [3] and distinct from each other,
omissions result in a less tight formulation.

N −1 := SN −1 ∪ SN , constraints in (20) where S(cid:48)

Remark. The convex hull can be formulated with η auxiliary variables (62)–(67), or 2η constraints
(20). While these formulations have tighter relaxations, they can perform worse than big-M due
to having more difﬁcult branch-and-bound subproblems. Our proposed formulation balances this
tradeoff by introducing a hierarchy of relaxations with increasing tightness and size. The convex hull is
created over partitions zn, n = 1, ..., N , rather than the input variables xn, n = 1, ..., η. Therefore,
only N auxiliary variables are introduced, with N ≤ η. The results in this work are obtained
using this lifted formulation (13)–(17); Appendix A.5 compares the computational performance of
equivalent non-lifted formulations, i.e., involving 2N constraints.

Figure 1 shows a hierarchy of increasingly tight formulations from N = 1 (equiv. big-M ) to N = 20
(convex hull of each node over a box input domain). The intermediate formulations approximate the
convex-hull (N = 20) formulation well, but need fewer auxiliary variables/constraints.

5

3.3 Effect of Input Partitioning Choice
Formulation (13)–(17) creates the convex hull over zn = (cid:80)
wixi, n = 1, ..., N . The hyperpa-
rameter N dictates model size, and the choice of subsets, S1, ..., SN , can strongly impact the strength
of its relaxation. By Proposition 4, (13)–(17) can in fact give multiple hierarchies of formulations.
While all hierarchies eventually converge to the convex hull, we are primarily interested in those with
tight relaxations for small N .

i∈Sn

n ≤ (cid:80)

Bounds and Bounds Tightening. Bounds on the partitions play a key role in the proposed for-
mulation. For example, consider when a node is inactive: σ = 1, zb
n = 0, and (16) gives the
bounds σLBa
Intuitively, the proposed formulation represents the
i∈Sn
convex hull over the auxiliary variables, zn = (cid:80)
wixi, and their bounds play a key role in
model tightness. We hypothesize these bounds are most effective when partitions Sn are selected
such that wixi, ∀i ∈ Sn are of similar orders of magnitude. Consider for instance the case of
w = [1, 1, 100, 100] and xi ∈ [0, 1], i = 1, ..., 4. As all weights are positive, interval arithmetic gives
0 ≤ (cid:80) xiwi ≤ (cid:80) ¯xiwi. With two partitions, the choices of S1 = {1, 2} vs S1 = {1, 3} give:

wixi ≤ σU Ba
n.

i∈Sn

(cid:20) x1 + x2 ≤ σ2
x3 + x4 ≤ σ2

(cid:21)

vs.

(cid:20) x1 + 100x3 ≤ σ101
x2 + 100x4 ≤ σ101

(cid:21)

(23)

where σ is a binary variable. The constraints on the right closely approximate the η-partition (i.e.,
convex hull) bounds: x3 ≤ σ and x4 ≤ σ. But x1 and x2 are relatively unaffected by a perturbation
σ = 1 − δ (when σ is relaxed). Whereas the formulation on the left constrains the four variables
equivalently. If the behavior of the node is dominated by a few inputs, the formulation on the right is
strong, as it approximates the convex hull over those inputs (z1 ≈ x3 and z2 ≈ x4 in this case). For
the practical case of N << η, there are likely fewer partitions than dominating variables.

The size of the partitions can also be selected to be uneven:

(cid:20)

x1 ≤ σ1
x2 + 100x3 + 100x4 ≤ σ201

(cid:21)

(cid:20)

vs.

x3 ≤ σ1
x1 + x2 + 100x4 ≤ σ102

(cid:21)

(24)

Similar tradeoffs are seen here: the ﬁrst formulation provides the tightest bound for x1, but x2 is
effectively unconstrained and x3, x4 approach “equal treatment.” The second formulation provides the
tightest bound for x3, and a tight bound for x4, but x1, x2 are effectively unbounded for fractional σ.
The above analyses also apply to the case of OBBT. For the above example, solving a relaxed model for
max(x1+x2) obtains a bound that affects the two variables equivalently, while the same procedure for
max(x1 + 100x3) obtains a bound that is much stronger for x3 than for x1. Similarly, max(x1 + x2)
captures dependency between the two variables, while max(x1 + 100x3) ≈ max(100x3).

Relaxation Tightness. The partitions (and their bounds) also directly affect the tightness of the
relaxation for the output variable y. The equivalent non-lifted realization (20) reveals the tightness of
the above simple example. With two partitions, the choices of S1 = {1, 2} vs S1 = {1, 3} result in
the equivalent non-lifted constraints:

(cid:20) y ≤ x1 + 100x3 + σ(b + 101)
y ≤ x2 + 100x4 + σ(b + 101)

(cid:21)

(cid:20)

vs.

y ≤ x1 + x2 + σ(b + 200)
y ≤ 100x3 + 100x4 + σ(b + 2)

(cid:21)

(25)

Note that combinations Ij = ∅ and Ij = {1, 2, 3, 4} in (20) are not analyzed here, as they correspond
to the big-M/1-partition model and are unaffected by choice of partitions. The 4-partition model is
the tightest formulation and (in addition to all possible 2-partition constraints) includes:

y ≤ xi + σ(b + 201), i = 1, 2
y ≤ 100xi + σ(b + 102), i = 3, 4

(26)
(27)

The unbalanced 2-partition, i.e., the left of (25), closely approximates two of the 4-partition (i.e.,
convex hull) constraints (27). Analogous to the tradeoffs in terms of bound tightness, we see that this
formulation essentially neglects the dependence of y on x1, x2 and instead creates the convex hull
over z1 = x1 + 100x3 ≈ x3 and z2 ≈ x4. For this simple example, the behavior of y is dominated
by x3 and x4, and this turns out to be a relatively strong formulation. However, when N << η, we
expect neglecting the dependence of y on some input variables to weaken the model.

The alternative formulation, i.e., the right of (25), handles the four variables similarly and creates
the convex hull in two new directions: z1 = x1 + x2 and z2 = 100(x3 + x4). While this does not

6

model the individual effect of either x3 or x4 on y as closely, it includes dependency between x3 and
x4. Furthermore, x1 and x2 are modeled equivalently (i.e., less tightly than individual constraints).
Analyzing partitions with unequal size reveals similar tradeoffs. This section shows that the proposed
formulation has a relaxation equivalent to selecting a subset of constraints from (20), with a tradeoff
between modeling the effect of individual variables well vs the effect of many variables weakly.

Deriving Cutting Planes from Convex Hull. The convex-hull constraints (20) not implicitly mod-
eled by a partition-based formulation can be viewed as potential tightening constraints, or cuts.
Anderson et al. [3] provide a linear-time method for selecting the most violated of the exponentially
many constraints (20), which is naturally compatible with our proposed formulation (some constraints
will be always satisﬁed). Note that the computational expense can still be signiﬁcant for practical
instances; Botoeva et al. [5] found adding cuts to <0.025% of branch-and-bound nodes to balance
their expense and tightening, while De Palma et al. [13] proposed adding cuts at the root node only.
Remark. While the above 4-D case may seem to favor “unbalanced” partitions, it is difﬁcult to
illustrate the case where N << η. Our experiments conﬁrm “balanced” partitions perform better.

3.4 Strategies for Selecting Partitions

The above rationale motivates selecting partitions that result in a model that treats input variables
(approximately) equivalently for the practical case of N << η. Speciﬁcally, we seek to evenly
distribute tradeoffs in model tightness among inputs. Section 3.3 suggests a reasonable approach
is to select partitions such that weights in each are approximately equal (weights are ﬁxed during
optimization). We propose two such strategies below, as well as two strategies in red for comparison.
Equal-Size Partitions. One strategy is to create partitions of equal size, i.e., |S1| = |S2| = ... = |SN |
(note that they may differ by up to one if η is not a multiple of N ). Indices are then assigned to
partitions to keep the weights in each as close as possible. This is accomplished by sorting the weights
w and distributing them evenly among partitions (array_split(argsort(w), N ) in Numpy).

Equal-Range Partitions. A second strategy is to partition with equal range, i.e., range
i∈S1

(wi) = ... =

(wi). We deﬁne thresholds v ∈ RN +1 such that v1 and vN +1 are min(w) and max(w). To

range
i∈SN
reduce the effect of outliers, we deﬁne v2 and vN as the 0.05 and 0.95 quantiles of w, respectively.
The remaining elements of v are distributed evenly in (v2, vN ). Indices i ∈ {1, ..., η} are then
assigned to Sn : wi ∈ [vn, vn+1). This strategy requires N ≥ 3, but, for a symmetrically distributed
weight vector, w, two partitions of equal size are also of equal range.

We compare our proposed strategies against the following:
Random Partitions. Input indices {1, ..., η} are assigned randomly to partitions S1, ..., SN .
Uneven Magnitudes. Weights are sorted and “dealt” in reverse to partitions in snake-draft order.

4 Experiments

Computational experiments were performed using Gurobi v 9.1 [19]. The computational set-up,
solver settings, and models for MNIST [22] and CIFAR-10 [20] are described in appendix A.4.

4.1 Optimal Adversary Results

The optimal adversary problem takes a target image ¯x, its correct label j, and an adversarial label
k, and ﬁnds the image within a range of perturbations maximizing the difference in predictions.
(fk(x)−fj(x)); x ∈ X , where fk and fj correspond to the
Mathematically, this is formulated as max
k- and j-th elements of the NN output layer, respectively, and X deﬁnes the domain of perturbations.
We consider perturbations deﬁned by the (cid:96)1-norm (||x − ¯x||1 ≤ (cid:15)1 ∈ R), which promotes sparse
perturbations [8]. For each dataset, we use the ﬁrst 100 test-dataset images and randomly selected
adversarial labels (the same 100 instances are used for models trained on the same dataset).

x

Table 1 gives the optimal adversary results. Perturbations (cid:15)1 were selected such that some big-M
problems were solvable within 3600s (problems become more difﬁcult as (cid:15)1 increases). While our
formulations consistently outperform big-M in terms of problems solved and solution times, the best

7

Table 1: Number of solved (in 3600s) optimal adversary problems and average solve times for
big-M vs N = {2, 4} equal-size partitions. Average times computed for problems solved by all 3
formulations. Grey indicates partition formulations strictly outperforming big-M.

Dataset

Model

(cid:15)1

Big-M

2 Partitions

4 Partitions

MNIST

CIFAR-10

2 × 50
2 × 50
2 × 100
2 × 100
CNN1*
CNN1*
2 × 100
2 × 100

5
10
2.5
5
0.25
0.5
5
10

solved
100
97
92
38
68
2
62
23

avg.time(s)
57.6
431.8
525.2
1587.4
1099.7
2293.2
1982.3
2319.0
*OBBT performed on all NN nodes

solved
100
98
100
59
86
16
69
28

avg.time(s)
42.7
270.4
285.1
587.8
618.8
1076.0
1083.4
1320.2

solved
100
98
94
48
87
11
85
34

avg.time(s)
83.9
398.4
553.9
1084.7
840.0
2161.2
752.8
1318.1

choice of N is problem-dependent. For instance, the 4-partition formulation performs best for the
2 × 100 network trained on CIFAR-10; Figure 2 (right) shows the number of problems solved for this
case. The 2-partition formulation is best for easy problems, but is soon overtaken by larger values of
N . Intuitively, simpler problems are solved with fewer branch-and-bound nodes and beneﬁt more
from smaller subproblems. Performance declines again near N ≥ 7, supporting observations that the
convex-hull formulation (N = η) is not always best [3].

Convex-Hull-Based Cuts. The cut-generation strategy for big-M by Anderson et al. [3] is compatible
with our formulations (Section 3.3), i.e., we begin with a partition-based formulation and apply cuts
during optimization. Figure 2 (left) gives results with these cuts added (via Gurobi callbacks) at
various cut frequencies. Speciﬁcally, a cut frequency of 1/k corresponds to adding the most violated
cut (if any) to each NN node at every k branch-and-bound nodes. Performance is improved at certain
frequencies; however, our formulations consistently outperform even the best cases using big-M with
cuts. Moreover, cut generation is not always straightforward to integrate with off-the-shelf MILP
solvers, and our formulations often perform just as well (if not better) without added cuts. Appendix
A.5 gives results with most-violated cuts directly added to the model, as in De Palma et al. [13].

Figure 2: Number solved vs convex-hull-cut frequency/run time for optimal adversary problems
((cid:15) = 5) for 2 × 100 models, without OBBT. Left: N = 1 (equivalent to big-M) performs worst on
MNIST (dotted) and CIFAR-10 (solid) models for most cut frequencies. Right: each line shows 100
runs of CIFAR-10 model (no convex-hull cuts). N = 1 performs the worst; N = 2 performs well for
easier problems; and intermediate values of N balance model size and tightness well.

Partitioning Strategies. Figure 3 shows the result of the input partitioning strategies from Section 3.4
on the MNIST 2×100 model for varying N . Both proposed strategies (blue) outperform formulations
with random and uneven partitions (red). With OBBT, big-M (N = 1) outperforms partition-based
formulations when partitions are selected poorly. Figure 3 again shows that our formulations perform
best for some intermediate N ; random/uneven partitions worsen with increasing N .

8

0020406080100solved (#)106105104103102101100cut frequencyN=1N=2N=4102103time (s)020406080100solved (#)N=1N=2N=3N=4N=5N=6N=7N=8N=9N=10Figure 3: Number solved (left) and solution times (right) for optimal adversary problems for MNIST
2 × 100 ((cid:15) = 5). Each point shows 100 runs, max time of 3600s. Dashed lines show runs with OBBT.
The equal range strategy requires N ≥ 3. Our proposed (blue) partitioning strategies solve more
problems (top) faster (bottom) than random and uneven partitions (red)

Optimization-Based Bounds Tightening. OBBT was implemented by tightening bounds for all zb
n.
We found that adding the bounds from the 1-partition model (i.e., bounds on (cid:80) zb
n) improved all
models, as they account for dependencies among all inputs. Therefore these bounds were used in all
models, resulting in 2N + 2 LPs per node (N ≥ 2). We limited the OBBT LPs to 5s; interval bounds
were used if an LP was not solved. Figure 3 shows that OBBT greatly improves the optimization
performance of all formulations. OBBT problems for each layer are independent and could, in
practice, be solved in parallel. Therefore, at a minimum, OBBT requires the sum of max solution
times found in each layer (5s × # layers in this case). This represents an avenue to signiﬁcantly
improve MILP optimization of NNs via parallelization. In contrast, parallelizing branch-and-bound
is known to be challenging and may have limited beneﬁts [1, 26].

Table 2: Number of veriﬁcation problems solved in 3600s and average solve times for big-M vs
N = {2, 4} equal-size partitions. Average times computed for problems solved by all 3 formulations.
OBBT was performed for all problems. Grey indicates formulations strictly outperforming big-M.

Dataset

Model

(cid:15)∞

Big-M

2 Partitions

4 Partitions

MNIST

CNN1
CNN1
CNN2
CNN2
CIFAR-10 CNN1
CNN1
CNN2
CNN2

0.050
0.075
0.075
0.100
0.007
0.010
0.007
0.010

sol.(#)
82
30
21
1
99
98
80
40

avg.time(s)
198.5
632.5
667.1
505.3
100.6
119.1
300.5
743.6

sol.(#)
92
52
36
5
100
100
95
72

avg.time(s)
27.3
139.6
160.7
134.7
25.9
25.7
85.1
176.4

sol.(#)
90
42
31
5
99
100
68
35

avg.time(s)
52.4
281.6
306.0
246.3
45.4
45.0
928.6
1041.3

4.2 Veriﬁcation Results

The veriﬁcation problem is similar to the optimal adversary problem, but terminates when the sign of
the objective function is known (i.e., the lower/upper bounds of the MILP have the same sign). This
problem is typically solved for perturbations deﬁned by the (cid:96)∞-norm (||x − ¯x||∞ ≤ (cid:15)∞ ∈ R). Here,
problems are difﬁcult for moderate (cid:15)∞: at large (cid:15)∞ a mis-classiﬁed example (positive objective) is
easily found. Several veriﬁcation tools rely on an underlying big-M formulation, e.g., MIPVerify
[33], NSVerify [2], making big-M an especially relevant point of comparison. Owing to the early
termination, larger NNs can be tested compared to the optimal adversary problems. We turned off cuts
(cuts= 0) for the partition-based formulations, as the models are relatively tight over the box-domain
perturbations and do not seem to beneﬁt from additional cuts. On the other hand, removing cuts
improved some problems using big-M and worsened others. Results are presented in Table 2. Our
formulations again generally outperform big-M (N =1), except for a few of the 4-partition problems.

9

12345678910N20406080100Number SolvedEq. SizeEq. RangeRandomUeven12345678910N103Avg Time SolvedEq. SizeEq. RangeRandomUevenTable 3: Number of (cid:96)1-minimally distorted adversary problems solved in 3600s and average solve
times for big-M vs N = {2, 4} equal-size partitions. Average times and (cid:15)1 are computed for
problems solved by all 3 formulations. OBBT was performed for all problems. Grey indicates
partition formulations strictly outperforming big-M.

Dataset Model

avg((cid:15)1)

Big-M

2 Partitions

4 Partitions

MNIST

2 × 50
2 × 75
2 × 100

6.51
4.41
2.73

solved
52
16
7

avg.time(s)
675.0
547.3
710.8

solved
93
37
13

avg.time(s)
150.9
310.5
572.9

solved
89
31
10

avg.time(s)
166.6
424.0
777.9

4.3 Minimally Distorted Adversary Results

In a similar vein as Croce and Hein [11], we deﬁne the (cid:96)1-minimally distorted adversary problem:
given a target image ¯x and its correct label j, ﬁnd the smallest perturbation over which the NN
(cid:15)1; ||x − ¯x||1 ≤ (cid:15)1; fk(x) ≥ fj(x). The
predicts an adversarial label k. We formulate this as min
(cid:15)1,x
adversarial label k is selected as the second-likeliest class of the target image. Figure 4 provides
examples illustrating that the (cid:96)1-norm promotes sparse perturbations, unlike the (cid:96)∞-norm. Note that
the sizes of perturbations (cid:15)1 are dependent on input dimensionality; if it were distributed evenly,
(cid:15)1 = 5 would correspond to an (cid:96)∞-norm perturbation (cid:15)∞ ≈ 0.006 for MNIST models.

Target

(cid:15)1 = 4

(cid:15)∞ = 0.05

Figure 4: Sample (cid:96)1- vs (cid:96)∞-based minimally distorted adversaries for the MNIST 2 × 50 model. The
true label (j) is ‘9,’ and the adversarial label (k) is ‘4.’

Table 3 presents results for the minimally distorted adversary problems. As input domains are
unbounded, these problems are considerably more difﬁcult than the above optimal adversary problems.
Therefore, only smaller MNIST networks were manageable (with OBBT) for all formulations. Again,
partition-based formulations consistently outperform big-M, solving more problems and in less time.

5 Conclusions

This work presented MILP formulations for ReLU NNs that balance having a tight relaxation
and manageable size. The approach forms the convex hull over partitions of node inputs; we
presented theoretical and computational motivations for obtaining good partitions for ReLU nodes.
Furthermore, our approach expands the beneﬁts of OBBT, which, unlike conventional MILP tools,
can easily be parallelized. Results on three classes of optimization tasks show that the proposed
formulations consistently outperform standard MILP encodings, allowing us to solve 25% more
of the problems (average >2X speedup for solved problems). Implementations of the proposed
formulations and partitioning strategies are available at https://github.com/cog-imperial/
PartitionedFormulations_NN.

Acknowledgments

This work was supported by Engineering & Physical Sciences Research Council (EPSRC) Fellowships
to CT and RM (grants EP/T001577/1 and EP/P016871/1), an Imperial College Research Fellowship
to CT, a Royal Society Newton International Fellowship (NIF\R1\182194) to JK, a grant by the
Swedish Cultural Foundation in Finland to JK, and a PhD studentship funded by BASF to AT.

10

References

[1] Tobias Achterberg and Roland Wunderling. Mixed integer programming: Analyzing 12 years

of progress. In Facets of Combinatorial Optimization, pages 449–481. Springer, 2013.

[2] Michael Akintunde, Alessio Lomuscio, Lalit Maganti, and Edoardo Pirovano. Reachability
analysis for neural agent-environment systems. In International Conference on Principles of
Knowledge Representation and Reasoning., pages 184–193, 2018.

[3] Ross Anderson, Joey Huchette, Will Ma, Christian Tjandraatmadja, and Juan Pablo Vielma.
Strong mixed-integer programming formulations for trained neural networks. Mathematical
Programming, pages 1–37, 2020.

[4] Egon Balas. Disjunctive Programming. Springer International Publishing, 2018. doi: 10.1007/

978-3-030-00148-3.

[5] Elena Botoeva, Panagiotis Kouvaros, Jan Kronqvist, Alessio Lomuscio, and Ruth Misener.
Efﬁcient veriﬁcation of ReLU-based neural networks via dependency analysis. In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 3291–3299, 2020.

[6] Rudy Bunel, Ilker Turkaslan, Philip HS Torr, Pushmeet Kohli, and M Pawan Kumar. A uniﬁed
view of piecewise linear neural network veriﬁcation. arXiv preprint arXiv:1711.00455, 2017.

[7] Rudy Bunel, Alessandro De Palma, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet
Kohli, Philip Torr, and M Pawan Kumar. Lagrangian decomposition for neural network
veriﬁcation. In Conference on Uncertainty in Artiﬁcial Intelligence, pages 370–379. PMLR,
2020.

[8] Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Ead: elastic-net attacks
to deep neural networks via adversarial examples. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 32, 2018.

[9] Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. Maximum resilience of artiﬁcial
neural networks. In International Symposium on Automated Technology for Veriﬁcation and
Analysis, pages 251–268. Springer, 2017.

[10] Michele Conforti, Gérard Cornuéjols, and Giacomo Zambelli. Integer Programming, volume

271 of Graduate Texts in Mathematics, 2014.

[11] Francesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast
adaptive boundary attack. In International Conference on Machine Learning, pages 2196–2205.
PMLR, 2020.

[12] Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan
Uesato, Rudy Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy Liang, et al.
Enabling certiﬁcation of veriﬁcation-agnostic networks via memory-efﬁcient semideﬁnite
programming. arXiv preprint arXiv:2010.11645, 2020.

[13] Alessandro De Palma, Harkirat Singh Behl, Rudy Bunel, Philip HS Torr, and M Pawan Kumar.
Scaling the convex barrier with sparse dual algorithms. In International Conference on Learning
Representations, 2021.

[14] Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari. Output range
analysis for deep feedforward neural networks. In NASA Formal Methods Symposium, pages
121–138. Springer, 2018.

[15] Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A Mann, and Pushmeet
Kohli. A dual approach to scalable veriﬁcation of deep networks. In UAI, volume 1, page 3,
2018.

[16] Ruediger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In
International Symposium on Automated Technology for Veriﬁcation and Analysis, pages 269–
286. Springer, 2017.

11

[17] Matteo Fischetti and Jason Jo. Deep neural networks and mixed integer linear optimization.

Constraints, 23(3):296–309, 2018.

[18] Bjarne Grimstad and Henrik Andersson. ReLU networks as surrogate models in mixed-integer

linear programs. Computers & Chemical Engineering, 131:106580, 2019.

[19] Gurobi Optimization, LLC. Gurobi optimizer reference manual, 2020. URL http://www.

gurobi.com.

[20] Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, The

University of Toronto, 2009.

[21] Jan Kronqvist, Ruth Misener, and Calvin Tsay. Between steps: Intermediate relaxations between
big-M and convex hull formulations. In International Conference on Integration of Constraint
Programming, Artiﬁcial Intelligence, and Operations Research, pages 299–314. Springer, 2021.

[22] Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. ATT Labs

[Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.

[23] Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward

ReLU neural networks. arXiv preprint arXiv:1706.07351, 2017.

[24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems,
pages 8024–8035. 2019.

[25] Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semideﬁnite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems,
volume 31, pages 10877–10887, 2018.

[26] Ted Ralphs, Yuji Shinano, Timo Berthold, and Thorsten Koch. Parallel solvers for mixed integer
linear optimization. In Handbook of Parallel Constraint Reasoning, pages 283–336. Springer,
2018.

[27] Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam. Bounding and counting
linear regions of deep neural networks. In International Conference on Machine Learning,
pages 4558–4566. PMLR, 2018.

[28] Thiago Serra, Abhinav Kumar, and Srikumar Ramalingam. Lossless compression of deep neural
networks. In Integration of Constraint Programming, Artiﬁcial Intelligence, and Operations
Research, pages 417–430. Springer, 2020.

[29] Gagandeep Singh, Jonathan Maurer, Christoph Müller, Matthew Mirman, Timon Gehr, Adrian
Hoffmann, Petar Tsankov, Dana Drachsler Cohen, Markus Püschel, and Martin Vechev. ERAN
veriﬁcation dataset. URL https://github.com/eth-sri/eran.

[30] Gagandeep Singh, Rupanshu Ganvir, Markus Püschel, and Martin Vechev. Beyond the single
neuron convex barrier for neural network certiﬁcation. In Advances in Neural Information
Processing Systems, pages 15098–15109, 2019.

[31] Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin T Vechev. Boosting robustness
certiﬁcation of neural networks. In International Conference on Learning Representations,
2019.

[32] Christian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Kishor Patel,
and Juan Pablo Vielma. The convex relaxation barrier, revisited: Tightened single-neuron
relaxations for neural network veriﬁcation. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,
pages 21675–21686, 2020.

12

[33] Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with

mixed integer programming. arXiv preprint arXiv:1711.07356, 2017.

[34] Michael Volpp, Lukas P Fröhlich, Kirsten Fischer, Andreas Doerr, Stefan Falkner, Frank Hutter,
and Christian Daniel. Meta-learning acquisition functions for transfer learning in bayesian
optimization. In International Conference on Learning Representations, 2019.

[35] Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certiﬁed robustness for ReLU networks. In
International Conference on Machine Learning, pages 5276–5285. PMLR, 2018.

[36] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pages 5286–5295.
PMLR, 2018.

[37] Ga Wu, Buser Say, and Scott Sanner. Scalable planning with deep neural network learned

transition models. Journal of Artiﬁcial Intelligence Research, 68:571–606, 2020.

13

A Appendix

A.1 Derivation of Proposed Formulation

Given the disjunction:

(cid:20)

y = 0
n=1 zn + b ≤ 0

(cid:80)N

(cid:21)

(cid:20)

∨

y = (cid:80)N

n=1 zn + b
y ≤ 0

(cid:21)

(28)

The extended, or “multiple choice,” formulation for disjunctions [4] introduces auxiliary variables za
n
and zb

n for each zn:

(cid:88)

wixi = za

n + zb
n

i∈Sn
(cid:88)

za
n + σb ≤ 0

n
(cid:88)

n

y =

zb
n + (1 − σ)b ≥ 0

(cid:88)

zb
n + (1 − σ)b

n
σLB a
n ≤ za
(1 − σ)LB b

n ≤ σUB a
n,
n ≤ zb

n ≤ (1 − σ)UB b
n,

(29)

(30)

(31)

(32)

(33)

(34)

∀n = 1, ..., N

∀n = 1, ..., N

where again σ ∈ {0, 1}. We observe that (29)–(34) exactly represents the ReLU node described
by (12): substituting zn = (cid:80)
wixi in the extended convex hull formulation [4] of the original
disjunction (5), directly gives (29)–(34). Eliminating za
n via (29) results in our proposed formulation:

i∈Sn

(cid:88)

n
(cid:88)

n

y =

(cid:33)

wixi − zb
n

+ σb ≤ 0

(cid:32)

(cid:88)

i∈Sn

zb
n + (1 − σ)b ≥ 0

(cid:88)

zb
n + (1 − σ)b

n
σLB a

n ≤

(cid:88)

wixi − zb

n ≤ σUB a
n,

(1 − σ)LB b

n ≤ (1 − σ)UB b
n,

i∈Sn
n ≤ zb

(35)

(36)

(37)

(38)

(39)

∀n = 1, ..., N

∀n = 1, ..., N

A.2 Derivation of Equivalent Non-Extended Formulation

We seek an equivalent (non-lifted) formulation to (35)–(39) without zb
isolated in (35)–(37), giving:

n. The term (cid:80)

n zb

n can be

(cid:88)

n
(cid:88)

n
(cid:88)

n

n ≥ wT x + σb
zb

zb
n ≥ −(1 − σ)b

zb
n = y − (1 − σ)b

(40)

(41)

(42)

We observed in Section 3.1 that [min((cid:80)
n and
zb
n. Let UB i and LB i denote, respectively, valid upper and lower bounds of weighted input wixi.

wixi)] are valid for both za

wixi), max((cid:80)

i∈Sn

i∈Sn

14

Isolating zb

n in the bounds (38)–(39) gives:

zb
n ≤

zb
n ≥

(cid:88)

i∈Sn
(cid:88)

i∈Sn

wixi − σ

wixi − σ

(cid:88)

i∈Sn
(cid:88)

i∈Sn

LB i

UB i

zb
n ≤ (1 − σ)

zb
n ≥ (1 − σ)

(cid:88)

i∈Sn
(cid:88)

i∈Sn

UB i

LB i

(43)

(44)

(45)

(46)

Fourier-Motzkin Elimination. We will now eliminate the auxiliary variables using the above
inequalities. Equations that appear in the non-extended formulation are marked in green. First, we
examine the equations containing (cid:80)
n. Combining (40)+(42) and (41)+(42) gives, respectively:

n zb

y ≥ wT x + b
y ≥ 0

(47)
(48)

Secondly, we examine equations containing only zb
(cid:80)
(43)+(46) and (44)+(45) recover the deﬁnition of lower and upper bounds on the partitions:

n. Combining (43)+(44) gives the trivial constraint
LBi. Combining (45)+(46) gives the same constraint. The other combinations

U Bi ≥ (cid:80)

i∈Sn

i∈Sn

(cid:88)

i∈Sn

LB i ≤

(cid:88)

i∈Sn

wixi ≤

(cid:88)

i∈Sn

UB i

(49)

Now examining both equations containing zb
n, the combinations between
(42) and (43)–(46) are most interesting. Here, each zb
n can be eliminated using either inequality
(43) or (45). Note that combining these with (40) or (41) results in trivially redundant constraints.
Deﬁning A as the indices of partitions for which (43) is used, and B as the remaining indices:

n and those containing (cid:80)

n zb

y − (1 − σ)b ≤

(cid:88)

(cid:88)

wixi − σ

(cid:88)

(cid:88)

LB i + (1 − σ)

(cid:88)

(cid:88)

U Bi

(50)

n∈A

n∈A
Similarly, the inequalities of opposite sign can be chosen from either (44) or (46). Deﬁning now A(cid:48)
as the indices of partitions for which (44) is used, and B(cid:48) as the remaining indices:
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

i∈Sn

i∈Sn

i∈Sn

n∈B

wixi − σ

UB i + (1 − σ)

LBi

(51)

y − (1 − σ)b ≥

n∈A(cid:48)

i∈Sn

n∈A(cid:48)

i∈Sn

n∈B

i∈Sn

Deﬁning Ij as the union of Sn∀n ∈ A and I (cid:48)
(cid:88)

(cid:88)

j as the union of Sn∀n ∈ A(cid:48), (50)–(51) become:
(cid:88)

y ≤

wixi − σ

LB i + (1 − σ)(b +

UB i)

y ≥

i∈Ij
(cid:88)

i∈I(cid:48)
j

wixi − σ

i∈Ij
(cid:88)

i∈I(cid:48)
j

UB i + (1 − σ)(b +

i∈I\Ij
(cid:88)

i∈I\I(cid:48)
j

LB i)

(52)

(53)

The lower inequality (53) is redundant. Consider that σ ∈ {0, 1}. For σ = 0 and σ = 1, we recover:

wixi + (b +

(cid:88)

LB i)

i∈I\I(cid:48)
j

(wixi − UB i)

y ≥

y ≥

(cid:88)

i∈I(cid:48)
j
(cid:88)

i∈I(cid:48)
j

(54)

(55)

The former is less tight than y ≥ wT x + b, while the latter is less tight than y ≥ 0. Finally, setting
σ(cid:48) = 1 − σ in (52) gives:

y ≤

(cid:88)

i∈Ij

wixi + (σ(cid:48) − 1)

(cid:88)

i∈Ij

15

LB i + σ(cid:48)(b +

(cid:88)

UB i)

i∈I\Ij

(56)

The combination Ij = ∅ removes all xi, giving an upper bound for y:

y ≤ σ(cid:48)(b +

(cid:88)

i∈I

UB i)

Combining all remaining equations gives a nonextended formulation:

(cid:88)

y ≤

wixi + σ(cid:48)(b +

(cid:88)

UB i) + (σ(cid:48) − 1)(

(cid:88)

LB i)

i∈I\Ij

i∈Ij

i∈Ij
y ≥ wT x + b
y ≤ σ(cid:48)UB 0
y ∈ [0, ∞)

A.3 Equivalence of η-Partition Formulation to Convex Hull

(57)

(58)

(59)

(60)
(61)

When N = η, it follows that Sn = {n}, ∀n = 1, .., η, and, consequently, zn = wnxn. The auxiliary
variables can be expressed in terms of xn, rather of zn (i.e., za
n). Rewriting
(29)–(34) in this way gives:

n = wnxa

n = wnxb

n and zb

n + xb
n

xn = xa
wT xa + σb ≤ 0
wT xb + (1 − σ)b ≥ 0
y = wT xb + (1 − σ)b
UB a
n
wn

LB a
n
wn

n ≤ σ

≤ xa

σ

,

(1 − σ)

LB b
n
wn

≤ xb

n ≤ (1 − σ)

UB b
n
wn

,

(62)

(63)

(64)

(65)

(66)

(67)

∀n = 1, ..., η

∀n = 1, ..., η

This formulation with a copy of each input—sometimes referred to as a “multiple choice”
formulation—represents the convex hull of the node, e.g., see [3]; however, the overall model
tightness still hinges on the bounds used for xn, n = 1, ..., η. We note that (66)–(67) are presented
here with xa
n and xb
n isolated for clarity. In practice, the bounds can be written without wn in their
denominators, as in (13)–(17).

A.4 Experiment Details

Computational Set-Up. All computational experiments were performed on a 3.2 GHz Intel Core
i7-8700 CPU (12 threads) with 16 GB memory. Models were implemented and solved using Gurobi
v 9.1 [19] (academic license). The LP algorithm was set to dual simplex, cuts = 1 (moderate cut
generation), TimeLimit = 3600s, and default termination criteria were used. We set parameter
MIPFocus = 3 to ensure consistent solution approaches across formulations. Average times were
computed as the arithmetic mean of solve times for instances solved by all formulations for a particular
problem class. Thus, no time outs are included in the calculation of average solve times.

Neural Networks. We trained several NNs on MNIST [22] and CIFAR-10 [20], including both fully
connected NNs and convolutional NNs (CNNs). MNIST (CC BY-SA 3.0) comprises a training set of
60,000 images and a test set of 10,000 images. CIFAR-10 (MIT License) comprises a training set of
50,000 images and a test set of 10,000 images. Dense models are denoted by nLayers × nHidden and
comprise nLayers×nHidden hidden plus 10 output nodes. CNN2 is based on ‘ConvSmall’ of the ERAN
dataset [29]: {Conv2D(16, (4,4), (2,2)), Conv2D(32, (4,4), (2,2)), Dense(100), Dense(10)}. CNN1
halves the channels in each convolutional layer: {Conv2D(8, (4,4), (2,2)), Conv2D(16, (4,4), (2,2)),
Dense(100), Dense(10)}. The implementations of CNN1/CNN2 have 1,852/3,604 and 2,476/4,852
nodes for MNIST and CIFAR-10, respectively. NNs are implemented in PyTorch [24] and obtained
using standard training (i.e., without regularization or methods to improve robustness). MNIST
models were trained for ten epochs, and CIFAR-10 models were trained for 20 epochs, all using the
Adadelta optimizer with default parameters.

16

A.5 Additional Computational Results

Comparing formulations. Proposition 1 suggests an equivalent, non-lifted formulation for each
partition-based formulation. In contrast to our proposed formulation (13)–(17), which adds a linear
(w.r.t. number of partitions N ) number of variables, this non-lifted formulation involves adding an
exponential (w.r.t. N ) number of constraints. Table 4 computationally compares our proposed formu-
lations against non-lifted formulations with equivalent relaxations on 100 optimal adversary instances
of medium difﬁculty. These results clearly show the advantages of the proposed formulations; a
non-lifted formulation equivalent to 20-partitions (220 constraints per node) cannot even be fully
generated on our test machines due to its computational burden. Additionally, our formulations
naturally enable OBBT on partitions. As described in Section 3.1, this captures dependencies among
each partition without additional constraints.

Table 4: Number of solved (in 3600s) optimal adversary problems for N = {2, 5, 10, 20} equal-size
partitions for MNIST 2×100 model, (cid:15)1=5. Performance is compared between proposed formulations
and non-lifted formulations with equivalent relaxations.

Partitions
N
2
5
10
20

Proposed Formulation Non-Lifted Formulation
avg.time(s)
solved
1111.4
59
2329.8
45
-
31
-
22
*The non-lifted formulation cannot be generated on our test machines.

avg.time(s)
530.1
792.5
-
-

solved
41
7
0
0*

As suggested in Proposition 1, the convex hull for a ReLU node involves an exponential (in terms of
number of inputs) number of non-lifted constraints:

y ≤

(cid:88)

i∈Ij

wixi + σ(b +

(cid:88)

i∈I\Ij

UB i) + (σ − 1)(

(cid:88)

LB i),

∀j = 1, ..., 2η

(68)

i∈Ij

where UB i and LB i denote the upper and lower bounds of wixi. The set I denotes the input indices
{1, ..., η}, and the subset Ij contains the union of the j-th combination of partitions {S1, ..., Sη}.
Anderson et al. [3] provide a linear-time method for identifying the most-violated constraint in (68).
First, the subset ˆI is deﬁned:
(cid:110)

ˆI =

i ∈ I | wixi < wi

(cid:0)LB xi(1 − σ) + UB xiσ(cid:1)(cid:111)

(69)

where LB xi and UB xi are, respectively, the lower and upper bounds of input xi. Then, the constraint
in (68) corresponding to the subset ˆI is checked:
(cid:88)

(cid:88)

(cid:88)

y ≤

wixi + σ(b +

UB i) + (σ − 1)(

LB i)

(70)

i∈ ˆI

i∈I\ ˆI

i∈ ˆI

If this constraint (70) is violated, then it is the most violated in the family (68). If it is not, then
no constraints in (68) are violated. This method can be used to solve the separation problem and
generate cutting planes during optimization.

Figure 2 (left) shows that dynamically generated, convex-hull-based cuts only improve optimization
performance when added at relatively low frequencies, if at all. Alternatively, De Palma et al. [13]
only add the most violated cut (if one exists) at the initial LP solution to each node in the NN. Adding
the cut before solving the MILP may produce different performance than dynamic cut generation: the
De Palma et al. [13] approach to adding cuts includes the cut in the solver (Gurobi) pre-processing.
Tables 5–8 give the results of the optimal adversary problem (cf. Table 1) with these single “most-
violated” cuts added to each node. These cuts sometimes improve the big-M performance, but
partition-based formulations still consistently perform best.

17

Table 5: Number of solved (in 3600s) optimal adversary problems for big-M vs N = {2, 4} equal-size
partitions. Columns marked “w/cuts” denote most violated convex-hull cut at root MILP node added
to each NN node. Most solved for each set of problems is in bold. Green text indicates more solved
compared to no convex-hull cuts.

Dataset

Model

(cid:15)1

MNIST

CIFAR-10

2 × 50
2 × 50
2 × 100
2 × 100
CNN1*
CNN1*
2 × 100
2 × 100

5
10
2.5
5
0.25
0.5
5
10

Big-M

4 Partitions

2 Partitions
w/ cuts w/o cuts w/ cuts w/o cuts w/ cuts w/o cuts
100
98
94
48
87
11
85
34

100
98
97
45
83
9
61
26

100
98
100
55
85
18
65
30

100
98
100
59
86
16
69
28

100
97
92
38
68
2
62
23

100
98
95
51
91
7
45
11

*OBBT performed on all NN nodes

Table 6: Average solve times of optimal adversary problems for big-M vs N = {2, 4} equal-size
partitions. Columns marked “w/cuts” denote most violated convex-hull cut at root MILP node added
to each NN node. Average times computed for problems solved by all 6 formulations. Lowest average
time for each set of problems is in bold. Green text indicates lower time compared to no convex-hull
cuts.

Dataset

Model

(cid:15)∞

MNIST

CIFAR-10

2 × 50
2 × 50
2 × 100
2 × 100
CNN1*
CNN1*
2 × 100
2 × 100

5
10
2.5
5
0.25
0.5
5
10

Big-M
w/ cuts w/o cuts
57.6
431.8
525.2
1586.0
1067.9
2293.2
1914.6
1908.9
*OBBT performed on all NN nodes

2 Partitions
w/ cuts w/o cuts
42.7
270.4
285.1
536.9
609.4
1076.0
1192.4
1621.4

45.3
285.2
505.2
930.5
420.7
1318.2
1739.2
1883.3

60.3
305.0
309.3
691.7
567.2
1586.7
595.1
1573.4

4 Partitions
w/ cuts w/o cuts
83.9
398.4
553.9
1040.3
817.1
2161.2
538.4
2094.5

86.0
404.5
597.4
1131.4
784.6
1100.9
834.8
1767.44

Table 7: Number of solved (in 3600s) veriﬁcation problems for big-M vs N = {2, 4} equal-size
partitions. OBBT was performed for all partitions, and columns marked “w/cuts” denote most
violated convex-hull cut at root MILP node added to each NN node.. Most solved for each set of
problems is in bold. Green text indicates more solved compared to no convex-hull cuts.

Dataset

Model

(cid:15)∞

MNIST

CNN1
CNN1
CNN2
CNN2
CIFAR-10 CNN1
CNN1
CNN2
CNN2

0.050
0.075
0.075
0.100
0.007
0.010
0.007
0.010

Big-M

4 Partitions

2 Partitions
w/ cuts w/o cuts w/ cuts w/o cuts w/ cuts w/o cuts
90
42
31
5
99
100
68
35

92
52
36
5
100
100
95
72

91
53
34
5
100
94
94
74

82
30
21
1
99
98
80
40

78
25
16
1
98
80
78
29

89
45
11
0
99
89
73
34

18

Table 8: Average solve times of veriﬁcation problems for big-M vs N = {2, 4} equal-size partitions.
OBBT was performed for all partitions, and columns marked “w/cuts” denote most violated convex-
hull cut at root MILP node added to each NN node. Average times computed for problems solved
by all 6 formulations. Lowest average time for each set of problems is in bold. Green text indicates
lower time compared to no convex-hull cuts.

Dataset

Model

(cid:15)1

MNIST

CNN1
CNN1
CNN2
CNN2
CIFAR-10 CNN1
CNN1
CNN2
CNN2

0.050
0.075
0.075
0.100
0.007
0.010
0.007
0.010

Big-M

4 Partitions

2 Partitions
w/ cuts w/o cuts w/ cuts w/o cuts w/ cuts w/o cuts
33.3
133.7
161.2
504.7
175.1
637.8
-
-
38.1
102.5
33.9
599.4
812.2
308.3
828.2
525.9

19.3
84.4
95.4
-
21.4
19.1
74.5
147.4

37.2
178.2
901.3
-
30.6
137.1
380.9
657.0

92.2
239.0
414.8
-
74.7
66.5
262.7
394.2

20.4
155.8
101.9
-
18.7
68.5
69.5
100.6

*OBBT performed on all NN nodes

19

