Noname manuscript No.
(will be inserted by the editor)

An Inexact Projected Gradient Method with
Rounding and Lifting by Nonlinear Programming
for Solving Rank-One Semideﬁnite Relaxation of
Polynomial Optimization

Heng Yang · Ling Liang
Luca Carlone · Kim-Chuan Toh

October 27, 2021

Abstract We consider solving high-order semideﬁnite programming (SDP) relax-
ations of nonconvex polynomial optimization problems (POPs) that often admit
degenerate rank-one optimal solutions. Instead of solving the SDP alone, we pro-
pose a new algorithmic framework that blends local search using the nonconvex
POP into global descent using the convex SDP. In particular, we ﬁrst design a
globally convergent inexact projected gradient method (iPGM) for solving the
SDP that serves as the backbone of our framework. We then accelerate iPGM by
taking long, but safeguarded, rank-one steps generated by fast nonlinear program-
ming algorithms. We prove that the new framework is still globally convergent for
solving the SDP. To solve the iPGM subproblem of projecting a given point onto
the feasible set of the SDP, we design a two-phase algorithm with phase one using a
symmetric Gauss-Seidel based accelerated proximal gradient method (sGS-APG)
to generate a good initial point, and phase two using a modiﬁed limited-memory
BFGS (L-BFGS) method to obtain an accurate solution. We analyze the con-
vergence for both phases and establish a novel global convergence result for the

Heng Yang
Laboratory for Information and Decision Systems
Massachusetts Institute of Technology
E-mail: hankyang@mit.edu

Ling Liang
Department of Mathematics
National University of Singapore
E-mail: liang.ling@u.nus.edu

Luca Carlone
Laboratory for Information and Decision Systems
Massachusetts Institute of Technology
E-mail: lcarlone@mit.edu

Kim-Chuan Toh
Department of Mathematics and Institute of Operations Research and Analytics
National University of Singapore
E-mail: mattohkc@nus.edu.sg

The ﬁrst two authors contributed equally.
Code available: https://github.com/MIT-SPARK/STRIDE

1
2
0
2

t
c
O
6
2

]

C
O
.
h
t
a
m

[

2
v
3
3
0
4
1
.
5
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Heng Yang et al.

modiﬁed L-BFGS that does not require the objective function to be twice contin-
uously diﬀerentiable. We conduct numerical experiments for solving second-order
SDP relaxations arising from a diverse set of POPs. Our framework demonstrates
state-of-the-art eﬃciency, scalability, and robustness in solving degenerate rank-
one SDPs to high accuracy, even in the presence of millions of equality constraints.

Keywords Semideﬁnite programming · Polynomial optimization · Inexact
projected gradient method · Rank-one solutions · Nonlinear programming ·
Degeneracy

Mathematics Subject Classiﬁcation (2010) 90C06 · 90C22 · 90C23 · 90C55

1 Introduction

Let p, h1, . . . , hl ∈ R[x] be real-valued multivariate polynomials in x ∈ Rd, we
consider the following equality constrained polynomial optimization problem1

min
x∈Rd

{p(x) | hi(x) = 0, i = 1, . . . , l} .

(POP)

Assuming problem (POP) is feasible and bounded below, we denote −∞ < p(cid:63) < ∞
as the global minimum and x(cid:63) as a global minimizer. Finding (p(cid:63), x(cid:63)) has applica-
tions in various ﬁelds of engineering and science [38]. It is, however, well recognized
that problem (POP) is NP-hard in general (e.g., it can model the binary constraint
x ∈ {+1, −1} via x2 = 1). As a result, solving convex relaxations of (POP), es-
pecially semideﬁnite programming (SDP) relaxations, has been the predominant
method for ﬁnding (p(cid:63), x(cid:63)). In this paper, we are interested in designing eﬃcient,
robust, and scalable algorithms for computing (p(cid:63), x(cid:63)) through SDP relaxations.
SDP Relaxations for Polynomial Optimization Problems. Towards this
goal, let us ﬁrst give an overview of the celebrated Lasserre’s moment/sums-of-
squares (SOS) semideﬁnite relaxation hierarchy [37, 55]. Let κ ≥ 1 be an integer
such that 2κ is equal or greater than the maximum degree of p, hi, i = 1, . . . , l,
and denote v := [x]κ as the standard vector of monomials in x of degree up to κ.
We build the moment matrix X := [x]κ[x]T
κ that contains the standard monomials
in x of degree up to 2κ. As a result, p and hi’s can be written as linear functions
in X. For example, choosing x = (x1, x2) ∈ R2 and κ = 2 leads to the following
moment matrix that contains all monomials of x up to degree 4:

X =











1 x1x2 x2
x2
x1
x2
1
2
1x2 x1x2
1 x2
1 x1x2 x3
x2
x1
2
x2 x1x2 x2
2 x2
1x2 x1x2
2 x3
2
1x2 x2
1 x3
1x2 x4
x3
x2
1x2
1 x2
2
1
2 x1x3
1x2
1x2 x2
2 x3
1x2 x1x2
x1x2 x2
2
x2
2 x1x2
2 x3
2 x2
1x2
2 x1x3
2 x4
2











.

(1)

We then consider two types of necessary linear constraints that can be imposed
on X: (i) the entries of X are not linearly independent, in fact the same monomial
could appear in multiple locations of X (e.g., the monomial x1x2 in (1)); (ii)

1 Although our framework can be generalized to problems with inequality constraints, here

we focus on equality constraints to simplify the presentation.

STRIDE for Solving Rank-One Semideﬁnite Relaxations

3

each constraint hi in the original (POP) generates a set of equalities on X of the
form hi(x)[x]2κ−deg(hi) = 0, where deg(hi) is the degree of hi (i.e., if hi = 0,
then hi · x1 = 0, hi · (x1x2) = 0 and so on). Since X is positive semideﬁnite
by construction, one can therefore write the resulting semideﬁnite relaxation in
standard primal form as

min
X∈Sn

{(cid:104)C, X(cid:105) | A(X) = b, X (cid:23) 0} ,

(P)

κ

where n := ¯dκ (cid:44) (cid:0)d+κ
a linear map A(X) (cid:44) ((cid:104)Ai, X(cid:105))m
independent linear constraints generated by the relaxation of X = [x]κ[x]T
X (cid:23) 0. Let A∗ : Rm → Sn be the adjoint of A deﬁned as A∗y (cid:44) (cid:80)m
the Lagrangian dual of problem (P) reads

(cid:1) is the dimension of [x]κ, b ∈ Rm, A : Sn → Rm is
i=1 with Ai ∈ Sn, that is onto and collects all
κ to
i=1 yiAi, then

max
y∈Rm,S∈Sn

(cid:8)(cid:104)b, y(cid:105) | A∗y + S = C, S (cid:23) 0(cid:9) ,

(D)

and admits an interpretation using sums-of-squares polynomials [55]. SDP relax-
ations (P)-(D) correspond to the so-called dense hierarchy, while many sparse
variants have been proposed to exploit the sparsity structure of p and hi’s to gen-
erate SDP relaxations with multiple blocks and smaller sizes, see [67, 69, 70] and
references therein. We remark that the algorithms developed in this paper can be
extended to multiple blocks in a straightforward way to solve sparse relaxations.
Throughout this paper, we assume that strong duality holds for (P) and (D),
and both (P) and (D) admit at least one solution.2 We denote X (cid:63) and (y(cid:63), S(cid:63))
as an optimal solution for (P) and (D), respectively. Then, the KKT condition for
(P) and (D) given as

A(X) − b = 0, A∗y + S − C = 0,

(cid:104)X, S(cid:105) = 0, X, S (cid:23) 0,

(2)

P and f (cid:63)

P = f (cid:63)
has (X (cid:63), y(cid:63), S(cid:63)) as one of its solutions. Moreover, it follows that p(cid:63) ≥ f (cid:63)
D,
where f (cid:63)
D are the optimal values of (P) and (D), respectively. The SDP
P. In such cases, for any global minimizer x(cid:63) of
relaxation is said to be tight if p(cid:63) = f (cid:63)
(POP), the rank one lifting X = [x(cid:63)]κ[x(cid:63)]T
κ is a minimizer of (P), and any rank one
optimal solution X (cid:63) of (P) corresponds to a global minimizer of (POP). A special
case of the relaxation hierarchy is Shor’s relaxation (κ = 1) for quadratically
constrained quadratical programming problems [63, 42], of which applications and
analyses have been extensively studied [18, 1, 15, 61, 58, 21, 68, 17]. Although κ = 1
is certainly an interesting case, it cannot handle p and hi’s with degrees above
2, and it is known to be not tight for many problems such as MAXCUT [29].
Moreover, it typically leads to SDPs with small m that can often be handled well
by existing SDP solvers [66]. Therefore, in this paper, we mainly focus on solving
high-order (κ ≥ 2) relaxations that are more powerful in attaining tightness, as
we describe below.

In the seminal work [37], Lasserre proved that f (cid:63)

P asymptotically approaches p(cid:63)
as κ increases to inﬁnity [37, Theorem 4.2]. Later on, several authors showed that
tightness indeed happens for a ﬁnite κ [39, 52, 53]. Notably, Nie proved that, under
the archimedean condition, if constraint qualiﬁcation, strict complementarity and

2 Particularly, if a redundant ball constraint is included in the relaxation, then dual Slater

and strong duality hold for Lasserre’s moment relaxations of all orders [34, Theorem 1].

4

Heng Yang et al.

second-order suﬃcient condition hold at every global minimizer of (POP), then the
hierarchy converges at a ﬁnite κ [53, Theorem 1.1]. What is even more encouraging
is that, empirically, for many important (POP) instances, tightness can be attained
at a very low relaxation order such as κ = 2, 3, 4. For instance, Lasserre [36]
showed that κ = 2 attains tightness for a set of 50 randomly generated MAXCUT
problems; In the experiments of Henrion and Lasserre [31], tightness holds at a
small κ for most of the (POP) problems in the literature; Doherty, Parrilo and
Spedalieri [25] demonstrated that the second-order SOS relaxation is suﬃcient to
decide quantum separability in all bound entangled states found in the literature
of dimensions up to 6 by 6; Cifuentes [20] designed and proved that a sparse
variant of the second-order moment relaxation is guaranteed to be tight for the
structured total least squares problem under a low noise assumption; Yang and
Carlone applied a sparse second-order moment relaxation to globally solve a broad
family of nonconvex computer vision problems [74, 76, 75]

Computational Challenges in Solving High-order Relaxations. The
surging applications make it imperative to design computational tools for solving
high-order (tight) SDP relaxations. However, high-order relaxations pose notori-
ous challenges to existing SDP solvers. For this reason, in all applications above,
the experiments were performed on solving (POP) problems of very small size,
e.g., the original (POP) has d up to 20 for dense relaxations. The computational
challenges mainly come from two aspects. In the ﬁrst place, in stark contrast to
Shor’s semideﬁnite relaxation, high order relaxations lead to SDPs with a large
number of linear constraints, even if the dimension of the original (POP) is small.
Taking the binary quadratic programming as an example (i.e., xi ∈ {+1, −1}
and p is quadratic in (POP)), when d = 60, the number of equality constraints
for κ = 2 is m = 1, 266, 971. In the second place, the resulting SDP is not only
large, but also highly degenerate. Speciﬁcally, when the relaxation (P) is tight and
admits rank-one optimal solutions, it necessarily fails the primal nondegeneracy
condition [4], which is crucial for ensuring numerical stability and fast conver-
gence of existing algorithms [5, 82]. Due to these two challenges, our experiments
show that no existing solver can consistently solve semideﬁnite relaxations with
rank-one solutions to a desired accuracy when m is larger than 500, 000. In par-
ticular, interior point methods (IPM), such as SDPT3 [66] and MOSEK [7], can only
handle up to m = 50, 000 before they run out of memory on an ordinary work-
station.3 First-order methods based on ADMM and conditional gradient method,
such as CDCS [83] and SketchyCGAL [79], converge very slowly and cannot attain even
modest accuracy for challenging instances. The best performing existing solver
is SDPNAL+ [78], which employs an augmented Lagrangian framework where the
inner problem is solved inexactly by a semi-smooth Newton method with a conju-
gate gradient method (SSNCG), and hence has very good scalability. The problem
with SDPNAL+ is that, in the presence of large m and degeneracy, solving the inex-
act SSN step involves solving a large and singular linear system of size m × m, and
CG becomes incapable of computing the search direction with suﬃcient accuracy.
A tempting alternative approach, since (P) has rank-one optimal solutions, is to
apply the low-rank factorization method of Burer and Monteiro [16], i.e., solving

3 When chordal sparsity exists in the SDP such that a large positive semideﬁnite constraint
can be decomposed into multiple smaller ones, IPMs can be more scalable [27, 81]. However,
for problems considered in this paper, there is no chordal sparsity.

STRIDE for Solving Rank-One Semideﬁnite Relaxations

5

the nonlinear optimization minV ∈Rn×r {(cid:10)C, V V T(cid:11) | A(V V T) = b} for some small
r. We note that since ∇V ((cid:10)Ai, V V T(cid:11)) = 2AiV , if rank (V (cid:63)) = 1 at the optimal
solution V (cid:63), then rank (2[A1V (cid:63), . . . , AmV (cid:63)]) ≤ n (cid:28) m no matter how large r is.
Therefore, V (cid:63) fails the linear independence constraint qualiﬁcation (LICQ) and it
may not be a KKT point, making it pessimistic for nonlinear programming solvers
to ﬁnd. In fact, successful applications of B-M factorization require m ≈ n so that
the dual multipliers can be obtained in closed-form from nonlinear programming
solutions [58, 57, 14]. Nevertheless, exploiting low rankness and nonlinear program-
ming tools is a great source of inspiration, and as we will show, our solver also
exploits similar insights but in a fashion that is not subject to the large m and
degeneracy of problem (P).

Our Contribution. In this paper, we contribute the ﬁrst solver that can solve
high-order SDP relaxations of (POP) to high accuracy in spite of large m and
degeneracy. Our solver employs a globally convergent inexact projected gradient
method (iPGM) as the backbone for solving the convex SDP (P). Despite having
favorable global convergence, the iPGM steps are typically short and lead to slow
convergence, particularly in the presence of degeneracy. Therefore, we blend short
iPGM steps with long rank-one steps generated by fast nonlinear programming
(NLP) algorithms. The intuition is that, once we have achieved a suﬃcient amount
of descent via iPGM, we can switch to NLP to perform local reﬁnement to “jump”
to a nearby rank-one point for rapid convergence. Due to this reason, we name
our framework SpecTrahedRon Inexact projected gradient Descent along vErtices
(STRIDE), which essentially strides along the rank-one vertices of the spectrahedron
until the global minimum of the SDP is reached.

Before presenting the mathematical details of STRIDE in a general setup, let us
ﬁrst introduce an informal version of STRIDE by running an illustrative example.

1.1 A Univariate Example

Consider minimizing a quartic polynomial subject to a single quartic equality
constraint

(cid:26)

p(x) := x4 +

2
3

min
x∈R

x3 − 8x2 − 8x

(cid:12)
(cid:12)
(cid:12)
(cid:12)

h(x) := (x2 − 4)(x2 − 1) = 0

(cid:27)

,

(3)

where it is obvious that x can only take four real values {+2, −2, +1, −1} due to
the constraint h(x), among which x(cid:63) = 2 attains the global minimum p(cid:63) = − 80
3 ≈
−26.67. A plot of p(x) is shown in Fig. 1(a). However, let us discard our “global”
view of p(x) and design an algorithm to numerically obtain (x(cid:63), p(cid:63)) through its
SDP relaxation. Towards this, according to our brief introduction of Lasserre’s
hierarchy, let us denote v(x) := [1, x, x2]T to be the vector of monomials in x of
degree up to 2, and build the moment matrix X := vvT. X has monomials of x
with degree up to 4, which we denote as zi := xi, i = 1, . . . , 4. With h(x) = 0, we
further have x4 = 5x2 − 4, i.e., z4 = 5z2 − 4. Now we can write the cost function
p(x) as a linear function of z: f (z) = z4 + 2
3 z3 − 8z2 − 8z1 = 2
3 z3 − 3z2 − 8z1 − 4.
With this construction, the second-order relaxation of (3) reads






f (z) :=

2
3

min
z∈R3

z3 − 3z2 − 8z1 − 4

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

X(z) :=





1 z1
z1 z2
z2 z3 5z2 − 4

z2
z3








 (cid:23) 0

.

(4)

6

Heng Yang et al.

Problem (4) allows us to explicitly visualize its feasible set, which is in general
called a spectrahedron. Using the fact that X(z) (cid:23) 0 if and only if the coeﬃcients
of its characteristic polynomial weakly alternate in sign [12, Proposition A.1], we
plot the spectrahedron in Fig. 1(b) using Mathematica [32]. The four rank-one
vertices are annotated with associated coordinates (z1, z2, z3), where z1 = x ∈
{+2, −2, +1, −1} corresponds to the four values that x can take. The negative
3 ]T is annotated by blue solid arrows. The
gradient direction −∇f (z) = [8, 3, − 2
hyperplane deﬁned by f (z) taking a constant value is shown as a green solid line
in Fig. 1(b). Clearly, the rank-one vertex z(cid:63) = (2, 4, 8) (“ (cid:70)” in (b)) attains the
minimum of the SDP (4), which corresponds to x(cid:63) = 2 (“ (cid:70)” in (a)) in POP (3).

Fig. 1 Overview of our algorithm using a univariate example. (a) Plot of the univariate poly-
nomial in (3). (b) Visualization of the feasible set of the SDP (4) arising from a second-order
moment relaxation of (3). “ ”: suboptimal local minimum of the POP and its corresponding
(lifted) vertex in the SDP. “(cid:70)”: globally optimal solutions of POP and SDP. “•”: A descent
point in SDP and corresponding rounded hypotheses in POP. Blue arrows in (b): direction
along the negative gradient of the SDP objective function. Green line in (b): a hyperplane of
constant objective going through the optimal solution.

We now state our numerical algorithm that ﬁnds both x(cid:63) and z(cid:63).
Initialization. We use a standard nonlinear programming (NLP) solver (e.g.,
an interior point method interfaced via fmincon in Matlab) to solve problem (3).
We initialize at x(0) = −10, and NLP converges to x(1) = −2, with a cost
p(x(1)) ≈ −5.33 (“ ” in Fig. 1(a)). Note that x(1) = −2 is a strict local mini-
mum because choosing a Lagrangian multiplier of “0” for the constraint h leads to
∇p(−2) = 0, ∇2p(−2) = 24, thereby satisfying ﬁrst-order optimality and second-
order suﬃciency [54].

Lift and Descend. The initialization step converges to a suboptimal solution.
Unfortunately, it is very challenging, if not impossible, for NLP to be aware of this
suboptimality, not to mention about escaping it. The SDP relaxation now comes
into play. By lifting x(1) to X = v(x(1))v(x(1))T according to the SDP relaxation,
we obtain the suboptimal rank-one vertex “ ” in Fig. 1(b) with z(1) = [−2, 4, −8]T.
Although it is challenging to descend from x(1) in POP (3), descending from z(1)
in the convex SDP (4) is relatively easy. We ﬁrst take a step along the negative
direction of the gradient to arrive at z(1)
+ (σ) := z(1) − σ∇f (z(1)) = [−2 + 8σ, 4 +

STRIDE for Solving Rank-One Semideﬁnite Relaxations

7

3 σ]T for a given step size σ > 0, and then project z(1)
3σ, −8 − 2
+ back to the
spectrahedron in Fig. 1(b), denoted as ¯z(1). Note that the projection step solves the
problem minz∈R3 {(cid:107)X(z) − X(z(1)
+ )(cid:107)2 | X(z) (cid:23) 0}, which is itself a quadratic SDP.
For this small-scale example, we can compute the projection exactly (using e.g.,
IPMs). Taking σ = 5 yields z(1)
3 ]T and ¯z(1) = [−0.35, 3.99, −1.67]T,
and the projection ¯z(1) is plotted in Fig. 1(b) as “•”.

+ = [38, 19, − 34

Rounding and Local Search. The step we just performed is called projected
gradient descent, and it is well known that iteratively performing this step guaran-
tees convergence to the SDP optimal solution [11]. However, the drawback is that
it typically requires many iterations for the convergence to happen. Now we will
show that, by leveraging local search back in POP (3), we can quickly arrive at
the rank-one optimal point. The intuition is, from Fig. 1(b), we can observe that
¯z(1) has moved “closer” to z(cid:63), and hence maybe ¯z(1) already contains useful local
information about z(cid:63). Particularly, let us perform a spectral decomposition of the
top-left 2 × 2 block of X(¯z(1)):

(cid:21)

= λ1v1vT

1 + λ2v2vT

2 , with

(cid:20)

1 −0.35
−0.35 3.99
(cid:20)

(cid:21)

1
−8.59

v1 = 0.12

, v2 = 0.99

(cid:21)

(cid:20) 1
0.12

, λ1,2 = (4.04, 0.96),

where we have written v1,2 by normalizing their leading entries as 1, and ordered
them such that λ1 ≥ λ2. We then generate two hypotheses ¯x(1)
1,2 = (−8.59, 0.12)
(“•” in Fig. 1(a)) and use NLP to perform local search starting from the hypotheses
still converges to x(1) = −2, ¯x(1)
1,2, respectively. Surprisingly, although ¯x(1)
¯x(1)
converges to x(cid:63) = 2. We call this step rounding and local search.

2

1

Lift and Certify. Now that NLP has visited both x(1) and x(cid:63), with x(cid:63) at-
taining a lower cost, we are certain that x(cid:63) is at least a better local minimum.
To certify the global optimality of x(cid:63), we need the SDP relaxation again. We lift
x(cid:63) to z(cid:63) = [2, 4, 8]T, and try to descend from z(cid:63) by performing another step of
projected gradient descent. However, because z(cid:63) is optimal, this step ends up back
to z(cid:63) again, i.e., no more descent is possible [11]. Therefore, we conclude that z(cid:63)
is optimal for SDP (4). Because X(z(cid:63)) is rank one, and p(x(cid:63)) = f (z(cid:63)), we can say
that x(cid:63) is indeed globally optimal for the nonconvex POP (3) (i.e., a numerical
certiﬁcate of global optimality is obtained for x(cid:63)).

Despite the simplicity of this example, it clearly demonstrates the stark con-
trast between our algorithm STRIDE and existing approaches. While all existing
methods solve the SDP relaxation independently from the original POP (i.e., they
relax the POP into an SDP and solve the SDP without computationally revis-
iting the POP), our method alternates between local search in POP and global
descent in SDP, and the connection between the POP and the SDP is established
by lifting and rounding. The advantages for doing so are threefold. (i) Scalability:
Although the SDP may have millions of variables (n and m can grow rapidly), the
original POP has only hundreds or thousands of variables (recall that d is moder-
ate) and an NLP solver can perform local search quickly (typically in a negligible
amount of time compared to solving the SDP); (ii) Degeneracy: When the optimal
SDP solution is rank one and degenerate, convergence of existing scalable SDP
solvers (such as ﬁrst-order methods) is typically very slow or even unachievable

8

Heng Yang et al.

(cf. results in Section 5). However, the original POP is much less sensitive to the
degeneracy. Once the SDP iterate gets close to being optimal, NLP can quickly
arrive at the global optimal solution. (iii) Warm-starting: Unlike IPMs, for which
designing good initialization is relatively challenging, our algorithm easily beneﬁts
from warm-starting. As we will show in Section 5, in many practical engineering
applications, there exist powerful heuristics that ﬁnd the globally optimal POP
solution with high probability of success. In this case, our algorithm only needs to
perform the last step of lifting and certiﬁcation.

However, several questions need to be answered in order for our algorithm to

be general and practical for large-scale problems.

Question 1 Since the projection onto a spectrahedron is itself an SDP and typi-
cally cannot be done exactly when n, m are large, how can one design a globally
convergent solver for the original SDP (P) that can tolerate inexactness of the
projection subproblem?

Question 2 With rounding and local search in the loop, is it still possible for the
algorithm to be globally convergent?

Question 3 How can one design a scalable and eﬃcient numerical algorithm to
handle millions of constraints (i.e., m) when computing the (inexact) projection
onto the spectrahedron?

Question 4 While our univariate example shows that this framework works on the
simple example above, will it work on more complicated problems arising in real
world applications?

Section 2 answers Question 1, where we design an inexact projected gradient
method (iPGM) for solving a generic SDP pair (P)-(D), prove its global conver-
gence, and provide complexity analysis. Although our results are inspired by [33],
several nontrivial extensions are made. In answering Question 2 (Section 3), we
incorporate rounding and local search into iPGM and formally present STRIDE.
In a nutshell, STRIDE follows the globally convergent trajectory driven by iPGM,
but simultaneously probes long, but safeguarded, rank-one vertices of the spectra-
hedron generated from solutions of NLP, to seek rapid descent and convergence.
Notably, we prove that, even with rounding and local search in the loop, STRIDE is
guaranteed to converge to the optimal solution of (P)-(D). In Section 4, we focus
on solving the critical subproblem of projecting a given symmetric matrix onto the
feasible set of (P) and provide an eﬃcient answer to Question 3. We propose a two-
phase algorithm where phase one uses a symmetric Gauss-Seidel based accelerated
proximal gradient method (sGS-APG) to generate a good initial point, and phase
two applies a modiﬁed limited-memory BFGS (L-BFGS) method to compute an
accurate solution. This two-phase algorithm is simple, robust, easy to implement,
and can scale to very large SDP problems with millions of constraints. Addition-
ally, for the modiﬁed L-BFGS algorithm, we establish a novel convergence result
where the objective function does not need to be at least twice continuously diﬀer-
entiable. Finally, we answer Question 4 by providing extensive numerical results
in Section 5. We apply STRIDE to solve (dense and sparse) second-order moment
relaxations arising from a diverse set of POP problems and demonstrate its supe-
rior performance. We observe that STRIDE is the only solver that can consistently

STRIDE for Solving Rank-One Semideﬁnite Relaxations

9

solve rank-one semideﬁnite relaxations to high accuracy (e.g., KKT residuals below
1e−9) and it is up to 1-2 orders of magnitude faster than existing SDP solvers.

√

+ (resp. Sn

Notation. We use X and Y to denote ﬁnite dimensional Euclidean spaces. Let
Sn be the space of real symmetric n × n matrices, and Sn
++) be the set of
positive semideﬁnite (resp. deﬁnite) matrices. We also write X (cid:23) 0 (resp. X (cid:31) 0)
to indicate that X is positive semideﬁnite (resp. deﬁnite) when the dimension of
X is clear. We use S d−1 := {x ∈ Rd | (cid:107)x(cid:107) = 1} to denote the d-dimensional
unit sphere. For x ∈ Rn, (cid:107)x(cid:107) =
xTx is the standard (cid:96)2 norm. For X ∈ Rm×n,
(cid:107)X(cid:107) = (cid:112)tr (X TX) denotes the Frobenius norm. (cid:107)x(cid:107)H := (cid:112)(cid:104)x, Hx(cid:105) denotes the
H-weighted norm for a positive semideﬁnite mapping H. We use I to denote the
identity map from Sn → Sn. We use δC(·) to denote the indicator function of set C.
For a positive integer d, we use t(n) := d(d+1)
to denote the d-th triangle number,
and we use ¯dκ := (n+κ
κ ) for some positive integer κ, which is particularly helpful
when describing the number of monomials in x ∈ Rd of degree up to κ. For x ∈ Rd,
we use [x]κ ∈ R ¯dκ to denote the full set of standard monomials of degree up to κ
(as already used in the introductory paragraphs).

2

2 An inexact Projected Gradient Method for SDPs

In this section, we describe an inexact Projected Gradient Method (iPGM) for
solving the SDP (P) and analyze its global convergence properties. Accelerating
iPGM via rounding and lifting by nonlinear programming will be presented in
Section 3.

Algorithm 1 presents the pseudocode for iPGM. Denoting the feasible set of

the primal SDP (P) as

FP := {X ∈ Sn | A(X) = b, X ∈ Sn

+} ,

the k-th iteration of iPGM ﬁrst moves along the direction of the negative gradient
(i.e., −C) with step size σk > 0, and then performs an inexact projection of the
trial point X k−1 − σkC onto the primal feasible set FP (cf. (5)), with inexactness
conditions given in (6). In Section 4, we will present eﬃcient algorithms that can
fulﬁll the inexactness conditions in (6).

The following theorem states the convergence properties of Algorithm 1 (iPGM).

Theorem 1 Let {(X k, yk, Sk)} be any sequence generated by Algorithm 1, and
suppose that there exists a constant M > 0 such that (cid:13)
(cid:13) ≤ M for all k ≥ 0.
Then, for all k ≥ 1, it holds that

(cid:13)yk(cid:13)






−

≤

(cid:107)y∗(cid:107) kεk
k
(cid:13)
(cid:13)
(cid:13)A(X k) − b
(cid:13)
(cid:13)
(cid:13) ≤ εk ≤ O
(cid:13)
(cid:13)
(cid:13)A∗yk + Sk − C
(cid:13)
(cid:13)
(cid:13) ≤ O

C, X k − X (cid:63)(cid:69)
(cid:68)
(cid:18) 1
(cid:19)
k
(cid:18) 1
√

(cid:19)

,

kσk

(cid:32)

1
k

1
2σ0

≤

,

(cid:13)
(cid:13)

(cid:13)X 0 − X (cid:63)(cid:13)

2
(cid:13)
(cid:13)

+ 2M

(cid:33)

iεi

,

k
(cid:88)

i=1

where X (cid:63) is an optimal solution of the primal problem (P) and (y(cid:63), S(cid:63)) is an opti-
k) > 0
mal solution of the dual problem (D). In particular, if one chooses σk ≥ O(

√

10

Heng Yang et al.

Algorithm 1 An inexact projected gradient method (iPGM) for SDP (P).
Input: Initial points (X 0, y0, S0) ∈ Sn

+, a nondecreasing positive sequence {σk}

+ × Rm × Sn

and a nonnegative sequence {εk} such that {k(cid:15)k} is summable.

Repeat For k ≥ 1:

Compute

X k ≈ ΠFP

(cid:16)
X k−1 − σkC

(cid:17)

, X k ∈ Sn
+

with associated dual pair (yk, Sk) ∈ Rm × Sn

+ such that the following conditions hold:

A∗yk + Sk − C −

1
σk

(cid:13)
(cid:13)
(cid:13)A(X k) − b
(cid:13)
(cid:13)
(cid:13) ≤ εk,
X k − X k−1(cid:17)
(cid:16)

= 0,

(cid:68)

X k, Sk(cid:69)

= 0.

Until: Termination conditions are met (cf. Section 5).
Output: (X k, yk, Sk).

(5)

(6)

for all k ≥ 1, then we have

max

(cid:110)(cid:12)
(cid:12)
(cid:12)

C, X k − X (cid:63)(cid:69)(cid:12)
(cid:68)
(cid:12)
(cid:12) ,

(cid:13)
(cid:13)
(cid:13)A(X k) − b
(cid:13)
(cid:13)
(cid:13) ,

(cid:13)
(cid:13)A∗yk + Sk − C
(cid:13)

(cid:111)

(cid:13)
(cid:13)
(cid:13)

≤ O

(cid:19)

.

(cid:18) 1
k

The proof of Theorem 1 is given in Appendix A. Although an accelerated
version of Algorithm 1 and its convergence analysis have been studied in [33],
Theorem 1 and its proof are new, to the best of our knowledge. By the presented
results, if one chooses εk to be suﬃciently small and σk to be suﬃciently large,
then the convergence of primal and dual infeasibilities can be as fast as O(1/k).
However, a small εk or a large σk will make the projection problem in (5) more
diﬃcult to solve. Hence, for better overall eﬃciency, one may choose εk and σk
dynamically to balance the convergence speed of Algorithm 1 and the eﬃciency of
the (inexact) projection onto FP.

3 STRIDE: Accelerating iPGM by Nonlinear Programming

Despite being globally convergent, it may take many iterations for Algorithm 1
to converge to a solution of high accuracy, especially when the SDP (P) has large
scale and the optimal solution X (cid:63) is low-rank and degenerate. Therefore, in this
section, we propose to accelerate Algorithm 1 by rounding and local search, just
as what we have shown in the simple numerical example in Fig. 1 of Section 1.
The intuition here is simple: when the SDP relaxation (P) is exact, there exist
rank-one optimal solutions and they may be computed much more eﬃciently by
performing local search in the low-dimensional (POP) with proper initialization.
With this intuition, we now develop the details of the acceleration scheme. At
+, we ﬁrst follow (5) in Algorithm 1

each iteration with current iterate X k−1 ∈ Sn
and compute the projection

k

X

≈ ΠFP

(cid:16)
X k−1 − σkC

(cid:17)

STRIDE for Solving Rank-One Semideﬁnite Relaxations

11

under the inexactness conditions described in (6), where σk > 0 is a given step size.
k
Then by the analysis in Theorem 1, X
is guaranteed to make certain progress
k
towards optimality. However, the point X
may not be a promising candidate for
the next iteration since it may not attain rapid convergence. Motivated by the
success of rounding and local search in the simple example in Fig. 1, we propose
to compute a potentially better candidate based on X

via the following steps:

k

1. (Rounding). Let X

k

= (cid:80)n

i=1 λivivT

i be the spectral decomposition of X

k

with λ1 ≥ . . . ≥ λn in nonincreasing order. Compute r ≥ 1 hypotheses for the
(POP) from the leading r eigenvectors v1, . . . , vr

¯xk
i = rounding(vi),

i = 1, . . . , r,

(7)

where the function rounding can be problem-dependent and we provide exam-
ples in the numerical experiments in Section 5. Generally, if the SDP (P) comes
from a dense relaxation, and the feasible set of the original (POP), denoted as
FPOP, is simple to project, then we can design rounding to be

vi ←

vi
vi[1]

,

¯xk
i ← ΠFPOP (vi[x]),

(8)

where one ﬁrst normalizes vi such that its leading entry vi[1] is equal to 1, and
then projects its entries corresponding to order-one monomials, i.e., vi[x], to
the feasible set of (POP). Note that the rationale for designing this rounding
method is that practical POP applications often involve simple constraints such
as binary [29], unit sphere [74], and orthogonality [76, 15], whose projection
maps are simple (e.g., if x is binary, then ΠFPOP (vi[x]) = sgn (vi[x]) just takes
the sign of each entry of vi[x]). If FPOP is not easy to project, then we omit
the projection, in which case the local search will start from an infeasible
initialization.

2. (Local Search). Apply a local search method for the POP (as an NLP) with
for each hypothesis i = 1, . . . , r. Denote the
i , with associated objective value p(ˆxk
i ), choose

the initial point chosen as ¯xk
i
solution of each local search as ˆxk
the best local solution with minimum objective value. Formally, we have

i = nlp(¯xk
ˆxk

i ),

i = 1, . . . , r,

ˆxk = arg min
ˆxk
i ,i=1...,r

p(ˆxk

i ).

(9)

3. (Lifting). Perform a rank-one lifting of the best local solution according to

the SDP relaxation scheme

(cid:98)X k = v

ˆxk(cid:17)
(cid:16)

v

ˆxk(cid:17)T
(cid:16)

,

(10)

where v : Rd → Rn is a dense or sparse monomial lifting (e.g., v(x) = [x]κ is the
full set of monomials up to degree κ in the case of dense Lasserre’s hierarchy
at order κ).

Now, we are given two candidates for the next iteration, namely X

(generated
by computing the projection of X k−1 − σkC onto the feasible set FP inexactly)
and (cid:98)X k (obtained by rounding, local search and lifting described just now), the
follow-up question is: which one should we choose to be the next iterate X k such
that the entire sequence {X k} is globally convergent for the SDP (P)?

k

12

Heng Yang et al.

k

The answer to this question is quite natural –we accept (cid:98)X k if and only if it
attains a strictly lower cost than X
– and guarantees that the algorithm visits a
sequence of rank-one vertices (local minima via NLP) with descending costs. With
this insight, we now introduce our algorithm STRIDE in Algorithm 2. STRIDE is a
combination of Algorithm 1 and the acceleration techniques in items 1-3, but with
a judicious safeguarding policy (11) that ensures (i) the rank-one iterate (cid:98)X k is
feasible (i.e., (cid:98)X k ∈ FP), and (ii) the rank-one iterate (cid:98)X k attains a strictly lower
and previously accepted rank-one iterates. Notice that requiring (cid:98)X k
cost than X
to attain a lower cost than previous rank-one iterates can prevent the algorithm
from revisiting the same vertex.

k

Algorithm 2 An inexact projected gradient method accelerated via rounding and
lifting by NLP for solving tight SDP relaxations of POPs (STRIDE).
Input: Initial points (X 0, y0, S0) ∈ Sn

+ × Rm × Sn
+, a nondecreasing positive sequences {σk}, a
nonnegative sequence {εk} such that {kεk} is summable, a stopping criterion η : Sn
+ × Rm ×
+ → R+ with a tolerance Tol > 0. Initialize V = {X 0} if X 0 ∈ FP and V = ∅ otherwise.
Sn
Choose a positive integer r ∈ [1, n], and a positive constant (cid:15) > 0.

Iterate the following steps for k = 1, . . . :
Step 1 (Projection). Compute (X
Step 2 (Certiﬁcation). If η(X
Step 3 (Acceleration). Compute (cid:98)X k as in items 1-3:

k

k

, yk, Sk) < Tol, output (X

, yk, Sk) satisfying (5) and (6) (see Section 4).
k

, yk, Sk) and stop.

k

= (cid:80)n
X
i = nlp(¯xk
ˆxk

i=1 λivivT
i ,
i ),

¯xk
i = rounding(vi),

i = 1, . . . , r,

ˆxk = arg minˆxk

i = 1, . . . , r,
i ,i=1...,r p(ˆxk
i ),

Step 4 (Policy). Update X k according to

(cid:98)X k = v (cid:0)ˆxk(cid:1) v (cid:0)ˆxk(cid:1)T .

X k =






(cid:98)X k
k
X

if

(cid:68)

C, (cid:98)X k(cid:69)
otherwise

< min

(cid:16)(cid:68)

C, X

k(cid:69)

, minX∈V {(cid:104)C, X(cid:105) | X ∈ V}

(cid:17)

− (cid:15), (cid:98)X k ∈ FP

.

(11)

If X k = (cid:98)X k, set V ← V ∪ { (cid:98)X k}.

We now state the convergence result for STRIDE.

k

Theorem 2 Let {(X
(cid:13)yk(cid:13)
suppose that there exists a constant M > 0 such that (cid:13)
k(cid:69)(cid:111)
one chooses σk ≥ O(
value of the SDP problem (P).

, yk, Sk)} be any sequence generated by Algorithm 2, and
(cid:13) ≤ M for all k ≥ 0. If
converges to the optimal

k) for all k ≥ 1, then,

C, X

(cid:110)(cid:68)

√

Proof Let us only consider the case with X 0 ∈ FP such that V is initialized as
{X 0} (the case with V initialized as ∅ can be argued in a similar manner). By
the construction of the set V, it contains a sequence of feasible points of (P),
and the objective value along this sequence is strictly decreasing according to the
acceptance policy (11). Hence, we have the following relation:

inf {(cid:104)C, X(cid:105) | X ∈ V} ≤

(cid:68)

C, X 0(cid:69)

− (|V|−1)(cid:15),

STRIDE for Solving Rank-One Semideﬁnite Relaxations

13

If |V|= ∞, then there exists an inﬁnite sequence of feasible points to problem
(P) whose objective function value converges to −∞. In this case, problem (P)
is unbounded, a contradiction (recall that we assumed strong duality throughout
this paper in Section 1). Hence, |V|< ∞. If |V|= 1, then none of the (cid:98)X k generated
by Step 3 has been accepted, in which case Algorithm 2 reduces to Algorithm 1
iPGM. Therefore, by Theorem 1, Algorithm 2 converges as O(1/k) if one chooses
k) for all k ≥ 1. If |V|> 1, then some of the (cid:98)X k generated by Step 3 have
σk ≥ O(
been accepted. Denote the last element of V as (cid:98)XV with associated dual variables
(yV , SV ) (from Step 1), then Algorithm 2 essentially reduces to iPGM with a
new initial point at ( (cid:98)XV , yV , SV ). Again, by Theorem 1, Algorithm 2 converges as
(cid:117)(cid:116)
O(1/k). This completes the proof.

√

We now make a few remarks about Algorithm 2 (STRIDE).
The ﬁrst remark is on the local search algorithm. In general, one can use any
nonlinear programming method to perform local search on the original (POP)
starting from an initial guess. A good choice is to use a solver that is based on
a primal-dual interior point method, for which the global and local convergence
properties are well studied [54, Chapter 19]. Notice that even if the NLP solver can
fail to converge to a feasible point of (POP), the safeguarding policy (11) ensures
that all the rank-one points in V are feasible for the SDP (P) (i.e., a failed NLP
solution will not be accepted). In many POPs arising from practical engineering ap-
plications, the constraint set typically deﬁnes a smooth manifold (see examples in
Section 5). In such cases, we prefer to use unconstrained optimization algorithms on
the manifold as the local search method, for example the Riemannian trust region
method that admits favorable global and local convergence properties [2, Chap-
ter 7]. A general interior point method for NLP is available through fmincon in
Matlab (see [54, Section 19.9] for other available software), while the Riemannian
trust region method is available through Manopt [13]. We emphasize here that the
idea of using local search algorithms for accelerating iPGM is heuristic and only
supported by numerical experiments in Section 5. In other words, while we guar-
antee global convergence of STRIDE for solving the SDP (P) (under mild technical
assumptions), the amount of acceleration gained by nonlinear programming may
be problem dependent and is mostly observed empirically. In the present paper,
we are not able to establish conditions under which the local search algorithms can
provide provably better rank-one candidates than the iterates generated by iPGM.
We think this aspect deserves deeper future research. Nevertheless, the nice prop-
erty of STRIDE is that, even if we reject all the candidates provided by local search,
global convergence is still guaranteed by taking the safeguarded iPGM steps.

Next, we comment on the number of eigenvectors to round. In STRIDE, the
hyperparameter r decides how many eigenvectors to round and how many hy-
potheses to generate at each iteration. Since the NLP solver performs local search
very quickly, it is aﬀordable to choose a large r. However, we empirically observed
that choosing r between 2 and 5 is suﬃcient for ﬁnding the global optimal solution.
Lastly, we provide a possible extension to STRIDE. The careful reader may have
noticed that Algorithm 2 uses the general Algorithm iPGM without acceleration.
We emphasize here that the accelerated version studied in [33] can also be used
as the backbone of STRIDE with the same global convergence property. The reason
we only implement the unaccelerated version is because empirically, with a proper
warmstart (described in Section 3.1), we observe that Algorithm 2 converges in 1

14

Heng Yang et al.

or 2 iterations, just like what we have shown in the simple univariate example in
Section 1. Therefore, the acceleration scheme would not have major improvement
on the eﬃciency of Algorithm 2.

3.1 Initialization

+ × Rm × Sn

STRIDE requires an initial guess (X 0, y0, S0) ∈ Sn
+ as described in Algo-
rithm 2. Although one can choose an arbitrary initial point and the algorithm is
guaranteed to converge, a good initialization can signiﬁcantly promote fast conver-
gence. We ﬁrst mention that, for many POPs arising from engineering applications,
practitioners often have designed powerful heuristics for solving the POPs based
on their speciﬁc domain knowledge. By heuristics we mean algorithms that can
ﬁnd the globally optimal POP solutions with very high probability of success, but
cannot provide a certiﬁcate of global optimality (or suboptimality). Therefore,
when such heuristics exist, we could use them to generate x0 for the (POP) and
perform a rank-one lifting of x0 to form X 0, i.e., X 0 = v(x0)v(x0)T, where v is
the lifting monomials. In Section 5.3, we demonstrate the eﬀectiveness of one such
heuristic in speeding up STRIDE on a computer vision application.

Here we describe a general initialization scheme based on a convergent semi-
proximal alternating direction method of multipliers (sPADMM) [64]. sPADMM can
be used to generate the dual initialization (y0, S0) when POP heuristic exists, or
to generate both X 0 and (y0, S0) when no POP heuristics exist. We begin by
reformulating problem (D) as follows:

min
y∈Rm,S∈Sn

(cid:110)

δ∗
Sn
+

(−S) − (cid:104)b, y(cid:105) | A∗y + S = C

(cid:111)

.

(12)

Given σ > 0, the augmented Lagrangian associated with (12) is given as

Lσ(y, S; X) = δ∗
Sn
+

(−S) − (cid:104)b, y(cid:105) + (cid:10)X, A∗y + S − C(cid:11) +

(cid:13)A∗y + S − C(cid:13)
(cid:13)
2 ,
(cid:13)

σ
2

for (X, y, S) ∈ Sn × Rm × Sn. The sPADMM method for solving problem (12)
is described in Algorithm 3, and its convergence is well studied in [64]. An im-
plementation of Algorithm 3 is included in the software package SDPNAL+ [78],
namely the admmplus subroutine. Let the output of sPADMM be (X, y, S), we use
(ΠSn
(X), y, S) to be the initial point for Algorithm 2 (recall that STRIDE requires
+
X 0 ∈ Sn

+, so we project X to be positive semideﬁnite).

√

We note that the steplength γ in Algorithm 3 can take values in the interval
(0, 2) instead of the usual interval of (0, (1 +
5)/2). In Step 1 and 3 of the al-
gorithm, ˆyk+1 and yk+1 are computed by using the sparse Cholesky factorization
of AA∗, which is computed once at the beginning of the algorithm. One can also
compute ˆyk+1 and yk+1 inexactly by using a precondtioned conjugate gradient
method without aﬀecting the convergence of the overall algorithm as long as the
residual norms of the inexact solutions are bounded by a summable error sequence;
we refer the reader to [19] for the details. We further add that for the computa-
tion of Sk+1 in Step 2, one can make use of the expected low-rank property of
(cid:0)X k + σ(A∗ ˆyk+1 − C)(cid:1). In our implementation we use the subroutine dsyevx
ΠSn
+
in LAPACK to compute only the positive eigen-pairs of X k + σ(A∗ ˆyk+1 − C).

STRIDE for Solving Rank-One Semideﬁnite Relaxations

15

Algorithm 3 A semi-proximal ADMM for solving (12).
Input: Initial points X 0 = S0 = 0 ∈ Sn and γ ∈ (0, 2).
Iterate the following steps for k = 1, . . . :

Step 1. Compute

ˆyk+1 = arg min

y

Lσ(y, Sk; X k) = (AA∗)−1

(cid:18) 1
σ

b − A

(cid:18) 1
σ

X k + Sk − C

(cid:19)(cid:19)

.

Step 2. Compute

Sk+1 = arg min

S
(cid:16)

ΠSn
+

=

1
σ

Step 3. Compute

Lσ(ˆyk+1, S; X k)

(cid:16)

X k + σ(A∗ ˆyk+1 − C)

(cid:17)

− (X k + σ(A∗ ˆyk+1 − C))

(cid:17)

.

yk+1 = arg min

y

Lσ(y, Sk+1; X k) = (AA∗)−1

(cid:18) 1
σ

b − A

(cid:18) 1
σ

X k + Sk+1 − C

(cid:19)(cid:19)

.

Step 4. Compute

X k+1 = X k + γσ(Sk+1 + A∗yk+1 − C).

Until termination conditions are met.
Output: (X k+1, yk+1, Sk+1).

4 Solving the Projection Subproblem

Recall that the feasible set of (P) is FP = {X ∈ Sn | A(X) = b, X (cid:23) 0}. In this
section, we aim at computing the projection onto FP eﬃciently since it is required
at each iteration in STRIDE (Algorithm 2). Note that since there are m linear
equality constraints deﬁning FP with m possibly as large as a few millions, the
projection algorithm has to be scalable. To this end, we propose a two-phase
algorithm. In phase one (Section 4.1), we use an sGS-based accelerated proximal
gradient method to generate a reasonably good initial point for the purpose of
warm starting. Then in phase two (Section 4.2), we apply a modiﬁed version of the
classical limited-memory BFGS (L-BFGS) method to compute a highly accurate
solution.

Formally, given a point Z ∈ Sn, the projection problem is described as ﬁnding

the closest point in FP with respect to Z

(cid:26) 1
2

min
X∈Sn

(cid:107)X − Z(cid:107)2 | X ∈ FP

(cid:27)

.

(13)

Notice that the feasible set FP is a spectrahedron deﬁned by the intersection of
two convex sets, namely the hyperplane {X ∈ Sn : A(X) = b} and the PSD cone
Sn
+. Therefore, a natural idea is to apply Dykstra’s projection (see e.g., [22]) for
generating an approximate solution to (13) by alternating the projection onto
the hyperplane and the projection onto the PSD cone, both of which are easy to
compute. However, Dykstra’s projection is known to have slow convergence rate
and it may take too many iterations until a satisfactory approximate projection is
found. In this paper, instead of solving (13) directly, we consider its dual problem
for which we can handle more eﬃciently.

16

Heng Yang et al.

constant term − 1

It is not diﬃcult to write down the Lagrangian dual problem (ignoring the
2 (cid:107)Z(cid:107)2 and converting “max” to “min”) of (13) as:
(cid:27)
(cid:26) 1
2

(cid:13)W + A∗ξ + Z(cid:13)
(cid:13)
2 − (cid:104)b, ξ(cid:105) | ξ ∈ Rm, W ∈ Sn
(cid:13)
+

min
W,ξ

(14)

.

For the rest of this section, we assume that the KKT system for (13)-(14) given
as

A(X) = b, A∗ξ + W = X − Z, X, W (cid:23) 0,

(cid:104)X, W (cid:105) = 0,

(15)

admits at least one solution, which is satisﬁed if problem (13) (or (P)) satisﬁes the
Slater’s condition, i.e., there exists an X † ∈ Sn such that

A(X †) − b = 0, X † (cid:31) 0.

Before presenting our two-phase algorithm, let us brieﬂy discuss about stopping
conditions for solving the pair of projection SDPs (13) and (14). To this end, let
(W, ξ) ∈ Sn
+×Rm be the output of any algorithm that solves the dual problem (14).
From the KKT conditions (15), we deem

X(W, ξ) := A∗ξ + W + Z ∈ Sn

(16)

as an approximate solution for the primal problem (13). Given a tolerance pa-
rameter tol, we accept X(W, ξ) as an approximate projection point if the relative
KKT residue is below tol:

ηproj(W, ξ) := max






(cid:107)A(X(W, ξ)) − b(cid:107)
1 + (cid:107)b(cid:107)

,

(cid:13)
(cid:13)
(cid:13)X(W, ξ) − ΠSn

+

(A∗ξ + Z)

1 + (cid:107)X(W, ξ)(cid:107) + (cid:107)W (cid:107)

(cid:13)
(cid:13)
(cid:13)






≤ tol.

(17)

The ﬁrst-order optimality conditions for problem (14), i.e.,

0 ∈

(cid:32)W + A∗ξ + Z + ∂δSn

+

(cid:33)

(W )

A(W + A∗ξ + Z) − b

=

(cid:32)X(W, ξ) + ∂δSn

+

(cid:33)

(W )

A(X(W, ξ)) − b

,

ensures that ηproj(W, ξ) is small if (W, ξ) is suﬃciently accurate, since 0 ∈ X(W, ξ)+
(A∗ξ+Z) = 0.
∂δSn
(W ) implies that X(W, ξ)−ΠSn
+
+
Now let us present our two-phase algorithm for solving the dual (14) for the

(X(W, ξ)−W ) = X(W, ξ)−ΠSn
+

remaining part of this section.

4.1 Phase One: An sGS-based Accelerated Proximal Gradient Method

Problem (14) is in the form of a convex composite minimization problem

min
W ∈Sn,ξ∈Rm

{F (W, ξ) := f (W, ξ) + g(W, ξ)} ,

(18)

where g(W, ξ) := δSn
+
but non-smooth, and f (W, ξ) is the following convex quadratic function

(W ) denotes the indicator function for Sn

+ which is convex

f (W, ξ) :=

(cid:42)(cid:32)W

(cid:33)

ξ

, Q

(cid:32)W

(cid:33)(cid:43)

ξ

+

1
2

(cid:42)(cid:32)

(cid:33)

Z

A(Z) − b

(cid:32)W

(cid:33)(cid:43)

ξ

,

STRIDE for Solving Rank-One Semideﬁnite Relaxations

17

with Q written as

Q :=

(cid:32)I A∗
A AA∗

(cid:33)

.

It is desirable to apply a proximal-type method for problem (18) whose objective
is the sum of a smooth and a non-smooth functions. The key idea in a proximal-
type method is to approximate the smooth part f (·) by a wisely chosen convex
quadratic function, namely qk(·), such that (i) qk + g is a good approximation to
f + g near the current iterate, and (ii) minimizing qk + g can be solved eﬃciently.
In particular, one needs to choose an appropriate positive deﬁnite mapping Hk
(k ≥ 1) for approximating the function f (·) by qk(·) that is deﬁned as follows:

qk(W, ξ) := f ((cid:102)W k, ˜ξk) +

∇f ((cid:102)W k, ˜ξk),

(cid:42)

(cid:33)(cid:43)

(cid:32)W − (cid:102)W k
ξ − ˜ξk

+

(cid:13)
(cid:32)W − (cid:102)W k
(cid:13)
(cid:13)
(cid:13)
ξ − ˜ξk
(cid:13)

1
2

(cid:33)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(19)

Hk

for a given ((cid:102)W k, ˜ξk) ∈ Sn
+ × Rm. There are many possible choices for Hk. For ex-
ample, Hk can be chosen as Q since the function f (·) is itself a quadratic function.
However, since the variables W and ξ are coupled, minimizing the approximate
function qk(W, ξ) + g(W, ξ), for Hk = Q, is as diﬃcult as solving the original pro-
jection problem (18). Another possibility is to choose Hk = Lkdiag(I, Im) − Q
where Lk is a given positive scalar. However, the value of Lk could be diﬃcult to
choose. Indeed, choosing Lk too small or too large can both harm the eﬃciency
and convergence. Even though one could apply certain back-tracking techniques
(see e.g., [10, Section 4]) for estimating Lk at each iteration, the computational
cost may be expensive. To decouple the variables W and ξ and to achieve fast
convergence, we will choose the operator Hk by applying the symmetric block
Gauss-Seidel (sGS) decomposition technique in [40].

To present the sGS decomposition method, we ﬁrst need to introduce some

useful notation. Let Q have the following decomposition

Q = U + D + U ∗,

(20)

where

U =

(cid:32)0 A∗

(cid:33)

0 0

, D =

(cid:32)I

0
0 AA∗

(cid:33)

,

and D (cid:31) 0 because A is onto. Deﬁne the linear operator TQ := UD−1U ∗. Clearly,
TQ is positive semideﬁnite. In sGS decomposition, we choose

Hk := Q + TQ, ∀k.

(21)

Let us ﬁrst verify that Hk is positive deﬁnite. To show this, we write

Q + TQ = (D + U + U ∗) + UD−1U ∗ = (D + U)D−1(D + U ∗),

(22)

and easily see that Hk (cid:31) 0 due to D (cid:31) 0 and D + U being nonsingular. Inserting
this choice of Hk = Q + TQ into the expression qk in (19), we have

qk(W, ξ) = f (W, ξ) +

(cid:33)

(cid:42)(cid:32)W − (cid:102)W k
ξ − ˜ξk

1
2

, TQ

(cid:32)W − (cid:102)W k
ξ − ˜ξk

(cid:33)(cid:43)

,

(23)

18

Heng Yang et al.

and the subproblem at the k-th iteration to be solved is

min
W ∈Sn,ξ∈Rm

{qk(W, ξ) + g(W, ξ)} .

(24)

The nice property of choosing Hk as in (21) and arriving at the subproblem (24)
is that an optimal solution for (24) can be computed eﬃciently by solving three
simpler problems, as stated in the following theorem.

Theorem 3 Assume AA∗ is nonsingular, then the optimal solution (W +, ξ+) ∈
+ × Rm for (24) can be computed exactly via the following steps:
Sn






ξ† := arg min

ξ∈Rm

(cid:110)
f ((cid:102)W k, ξ)

(cid:111)

,

W + := arg min
W ∈Sn

y+ := arg min

ξ∈Rm

(cid:110)

(cid:110)

(W ) + f (W, ξ†)

δSn
+

(cid:111)

,

(25)

f (W +, ξ)

(cid:111)

.

The proof of Theorem 3 is given in [40, Theorem 1] and is omitted here. The
reason that procedure (25) is simple is because (i) with W ﬁxed as (cid:102)W k or W +
(ﬁrst and third line in (25)), optimizing ξ boils down to solving a linear system
and can be solved exactly due to the sparsity of A; (ii) with ξ ﬁxed as ξ† (second
line in (25)), optimizing W resorts to performing a projection onto Sn
+ and can be
done in closed form via computing eigenvalue decompositions.

With these preparations, we now formally present the sGS-based accelerated
proximal gradient method (sGS-APG) for solving (14) in Algorithm 4, where we
also give the closed-form solution of the procedure (25) in (26). Note that in Algo-
rithm 4, the computation of ξk in Step 1 is only needed for checking terminations.

Algorithm 4 An sGS-based accelerated proximal gradient method for (14).
Input: Initial points (cid:102)W 1 = W 0 ∈ Sn
Iterate the following steps for k = 1, . . . :

+, termination tolerance tol > 0, and t1 = 1.

Step 1 (sGS update). Compute

˜ξk = (AA∗)−1 (cid:16)

(cid:17)
b − A(Z) − A((cid:102)W k)

W k = ΠSn
+

(cid:16)

−A∗ ˜ξk − Z

(cid:17)

= ΠSn
+

(cid:16)

A∗ ˜ξk + Z

(cid:16)

(cid:17)

−

A∗ ˜ξk + Z

(cid:17)

ξk = (AA∗)−1 (cid:16)

b − A(Z) − A(W k)

(cid:17)

,

,

.

(26)

Step 2. Compute tk+1 =
Until: ηproj(W k, ξk) ≤ tol.
Output: (W k, ξk).

(cid:113)

1+

1+4t2
k
2

, and (cid:102)W k+1 = W k + tk−1
tk+1

(W k − W k−1).

The convergence of sGS-APG is stated as follows.

STRIDE for Solving Rank-One Semideﬁnite Relaxations

19

Theorem 4 Suppose that (W (cid:63), ξ(cid:63)) ∈ Sn
+ × Rm is an optimal solution of problem
(14) and that AA∗ is nonsingular. Let {(W k, ξk)} be the sequence generated by
Algorithm 4. Then there exists a constant c ≥ 0 such that

0 ≤ f (W k, ξk) − f (W (cid:63), ξ(cid:63)) ≤

c
(k + 1)2 .

The proof of Theorem 4 can be taken directly from [40, Proposition 2]. Note
that the proposed method in this subsection is in fact a direct application of the one
proposed in [40, Section 4] which is designed for a more general convex composite
quadratic programming model with multiple blocks. Moreover, [40, Section 4] also
investigates the inexact computation for the corresponding subproblem. Thus,
when AA∗ cannot be factorized eﬃciently, one may apply an iterative solver for
solving the linear systems in (26) of Algorithm 4. We omit the details here since
for the applications studied in this paper, factorizing AA∗ can always be done
eﬃciently due to the sparsity of A. We further remark that for the computation of
W k in (26), we can make use of the expected low-rank property of the projection
(A∗ ˜ξk + Z) corresponding to the primal variable. Again, we use the subroutine
ΠSn
+
dsyevx in LAPACK to compute only the positive eigen-pairs of A∗ ˜ξk + Z.

4.2 Phase Two: A Modiﬁed Limited-memory BFGS Method

Even though the sGS-APG method, as presented in Algorithm 4, converges as
O(1/k2) in terms of objective value, the constant c depends on the distance be-
tween the initial point and the optimal solution, which can be quite large if the
initial point is not well chosen. Therefore, it may require many iterations to reach
a satisfactory solution when the initial point is far way from the optimal solution
set. Moreover, the sGS-APG method may not have a fast local linear convergent
property since the problem (14) is not strongly convex. To speed up the compu-
tation of the dual projection problem (14), we now propose a modiﬁed version
of the classical limited-memory BFGS (L-BFGS) method that is warm-started by
the solution from Algorithm 4.

To proceed, we observe that, ﬁxing the unconstrained ξ, the dual projection
+ to the matrix −(A∗ξ + Z) and

problem (14) becomes ﬁnding the closest W ∈ Sn
admits a closed-form expression

W = ΠSn
+

(cid:0)−A∗ξ − Z(cid:1) .

As a result, problem (14) can be further simpliﬁed, after inserting (27), as

min
ξ∈Rm

φ(ξ) :=

(cid:13)
(cid:13)
(cid:13)ΠSn

+

1
2

(A∗ξ + Z)

(cid:13)
2
(cid:13)
(cid:13)

− (cid:104)b, ξ(cid:105) ,

with the gradient of φ(ξ) given as

∇φ(ξ) = AΠSn
+

(A∗ξ + Z) − b.

We have used the following equality in getting (28)

ΠSn
+

(cid:0)−A∗ξ − Z(cid:1) + A∗ξ + Z = ΠSn

+

(A∗ξ + Z)

(27)

(28)

(29)

20

Heng Yang et al.

due to the Moreau identity [45, Theorem 2.2]. Thus, if ξ(cid:63) is an optimal solution for
problem (28), then we can recover W (cid:63) from (27). Formulating the dual projection
problem as (28) has appeared multiple times; see, for instances [82, 44]. We note
that the function φ(·) is smooth but not twice continuously diﬀerentiable, and it
is convex but not strongly convex.

Now that (28) is an unconstrained convex minimization problem in ξ ∈ Rm
with gradient given in (29), many eﬃcient algorithms are available for solving the
problem, such as (accelerated) gradient descent methods [51], nonlinear conjugate
gradient methods [23], quasi-Newton methods [54] and the semi-smooth Newton
method [82]. For this paper, we decide to propose a modiﬁed limited-memory
BFGS (L-BFGS) method because L-BFGS is easy to implement, can handle very
large unconstrained optimization problems, and is typically the “the algorithm
of choice” for large-scale problems. Empirically, we observed that our proposed
L-BFGS warm-started by sGS-APG is eﬃcient and robust for various class of
applications (shown in Section 5). This observation also supports the discussions
made in [54, Chapter 7]. To the best of our knowledge, this is the ﬁrst work that
demonstrates the eﬀectiveness of L-BFGS, or in general quasi-Newton methods,
on solving large and challenging SDPs.4

The template for the modiﬁed L-BFGS method is presented in Algorithm 5.

Algorithm 5 A modiﬁed Limited-memory BFGS method for (28).
Input: Starting point ξ0, integer mem > 0, a positive constant K, termination tolerance tol >

0, µ ∈ (0, 1/2), µ ≤ µ(cid:48) < 1, ρ ∈ (0, 1) and τ1, τ2 ∈ (0, ∞).

Iterate the following steps for k = 1, . . . :

τ2 and compute dk =
Step 1 (Search direction). Choose Q0
−βk∇φ(ξk) − gk where gk := Qk∇φ(ξk) with Qk (cid:23) 0, is obtained via the two-loop recursion
as in [54, Algorithm 7.4]. If (cid:13)
Step 2 (Line search). Set αk = ρmk , where mk is the smallest nonnegative integer m
such that

(cid:13) ≥ K, then choose dk = −βk∇φ(ξk) (i.e., set Qk = 0).

k (cid:31) 0, βk := τ1

(cid:13)∇φ(ξk)(cid:13)
(cid:13)
(cid:13)

(cid:13)dk(cid:13)

φ(ξk + ρkdk) ≤ φ(ξk) + µρm (cid:68)

∇φ(ξk), dk(cid:69)

.

Compute ξk+1 = ξk + αkdk.
Step 3 (Update memory). Compute and save uk = ξk+1 − ξk and wk = ∇φ(ξk+1) −
∇φ(ξk). If k > mem, discard the vector {uk−mem, wk−mem} from storage.

Until: ηproj(W k+1, ξk+1) ≤ tol with W k+1 = ΠSn
Output: (W k+1, ξk+1).

+

(−A∗ξk+1 − Z).

In Algorithm 5, we always choose Q0

k = Im for all k ≥ 0 which implies that
Qk (cid:23) 0 (see e.g., [54, eq. (7.19)]). Therefore, the matrix Qk + βkIm (cid:31) 0 when
∇φ(ξk) is not zero, and dk computed in Step 1 in Algorithm 5 can be shown to be
a descent direction (i.e., the algorithm is well-deﬁned). We state the convergence
property of Algorithm 5 in the following theorem, whose proof is presented in
Appendix B.

Theorem 5 Suppose that AA∗ is nonsingular and the Slater condition holds for
(P). Then Algorithm 5 is well deﬁned. Let the sequence {ξk} be generated by Al-

4 From our extensive numerical trials, we found that accelerated gradient descent, nonlinear
conjugate gradient, and semismooth Newton with CG iterative solver fail to give satisfactory
performance, especially when m is very large.

STRIDE for Solving Rank-One Semideﬁnite Relaxations

21

gorithm 5 such that ∇φ(ξk) (cid:54)= 0, ∀ k ≥ 0. Then, the sequence {ξk} is bounded and
any accumulation point ˆξ of this sequence is an optimal solution of (28).

4.3 Connection between the Projection SDP and the Original SDP

Recall from Algorithm 2 that, Z = X k−1 − σkC at iteration k for k ≥ 1. Also
(−A∗ξ − Z) ∈ Sn
recall from (16) that X(W, ξ) = A∗ξ + Z + W with W = ΠSn
+.
+ and hence,
Combining these equations, we have X(W, ξ) = ΠSn
+
(cid:104)X(W, ξ), W (cid:105) = 0. Furthermore, X(W, ξ) = A∗ξ + W + Z implies

(A∗ξ + Z) ∈ Sn

+

(cid:16)

X(W, ξ) − X k−1(cid:17)

= A∗

1
σk

(cid:19)

ξ

+

(cid:18) 1
σk

1
σk

W − C,

(30)

which means that when X(W, ξ) is close to X k−1 (which is the case when X k−1
W, 1
is an approximate solution for the
is approximately optimal), then
σk
original dual problem (D). Therefore, from the solution ξ of L-BFGS, we output

(cid:16) 1
σk

(cid:17)

ξ

W =

ΠSn
+

(−A∗ξ − Z)

X(W, ξ) = A∗ξ + Z + W = ΠSn
(cid:16)

+

(A∗ξ + Z)
(cid:17)

X(W, ξ), 1
σk

ξ, 1
σk

W

k

(X

, yk, Sk) =

(31)

(32)

(33)

for Step 1 of the k-th iteration of Algorithm 2. In particular, if

(cid:107)∇φ(ξ)(cid:107) =

(cid:13)
(cid:13)
(cid:13)A(X

k

) − b

(cid:13)
(cid:13)
(cid:13) ≤ εk

for given εk ≥ 0, then (X

k

, yk, Sk) satisﬁes the inexact conditions (6).

5 Applications and Numerical Experiments

In this section, we conduct numerical experiments by using STRIDE to solve SDP
relaxations for several important classes of POPs. In Section 5.1 and 5.2, we solve
dense second-order moment relaxation of random binary quadratic programming
problems, and random quartic optimization problems over the unit square, both
with increasing number of variables d. We demonstrate, for the ﬁrst time, that the
relaxation is always tight for the instances we have generated up to d = 60. In
Section 5.3 and 5.4, we solve sparse second-order moment relaxations coming from
two engineering applications, namely an outlier-robust Wahba problem that under-
pins many computer vision applications, and a nearest structured rank deﬁcient
matrix problem that ﬁnds extensive applications in control, statistics, computer
algebra, among others. We demonstrate that, with a sparse relaxation scheme,
we can globally solve POP problems with d up to 1000, far beyond the reach of
SDP solvers used in the corresponding engineering literature. When solving the
outlier-robust Wahba problem in Section 5.3, we additionally show that (i) lever-
aging domain-speciﬁc POP heuristics for primal initialization can further speed
up STRIDE by 2-3 times, and (ii) STRIDE can certiﬁably optimally solve two real ap-
plications of the outlier-robust Wahba problem, namely image stitching and scan

22

Heng Yang et al.

matching. Our experiments show that STRIDE is the only solver that can consis-
tently solve rank-one tight semideﬁnite relaxations to high accuracy (e.g., KKT
residuals below 1e−9), in the presence of millions of equality constraints.

Before presenting each application, let us describe the details about implemen-

tation and experimental setup.

Implementation. We implement the STRIDE Algorithm 2 in MATLAB R2020a,
with core subroutines, such as projection onto the PSD cone, implemented in C
for eﬃciency. Our implementation is available at

https://github.com/MIT-SPARK/STRIDE

and also supports SDP problems with multiple PSD blocks (hence, it can also
handle relaxations of (POP) problems with inequality constraints).

Note that diﬀerent from generic SDP solvers, we also implement the rounding,
local search and lifting procedures required in Step 2 of Algorithm 2, for each
POP. Since these procedures are problem dependent, we will describe them in the
corresponding subsections.

Stopping Conditions. To measure the feasibility and optimality at a given
+, we deﬁne the following standard

+ × Rm × Sn

approximate solution (X, y, S) ∈ Sn
relative KKT residues:

ηp =

(cid:107)A(X) − b(cid:107)
1 + (cid:107)b(cid:107)

, ηd =

(cid:107)A∗y + S − C(cid:107)
1 + (cid:107)C(cid:107)

, ηg =

|(cid:104)C, X(cid:105) − (cid:104)b, y(cid:105) |
1 + |(cid:104)C, X(cid:105) |+|(cid:104)b, y(cid:105) |

.

(34)

For a given tolerance tol > 0, we terminate STRIDE when max{ηp, ηd, ηg} ≤ tol,
and we choose tol = 1e−8 for all our experiments. Because our goal is to obtain
a solution of the original (POP) with an optimality or suboptimality certiﬁcate,
we also compute a relative suboptimality gap from the SDP solution (X, y, S) as

ηs =

|p(ˆx) − ((cid:104)b, y(cid:105) + Mbλmin(C − A∗y))|
1 + |p(ˆx)| + |(cid:104)b, y(cid:105) + Mbλmin(C − A∗y)|

,

(35)

where ˆx ∈ FPOP is a feasible approximate solution to the (POP) that is rounded
from the leading eigenvector of X,5 λmin denotes the minimum eigenvalue, and
Mb ≥ tr (X) is a bound on the trace of X when X is generated by a rank-one
lifting. We easily have p(ˆx) ≥ p(cid:63) ≥ (cid:104)b, y(cid:105) + Mbλmin(C − A∗y), and hence ηs = 0
(i.e., the upper bound and the lower bound coincide) certiﬁes that p(ˆx) = p(cid:63) and
ˆx is a global minimizer of the nonconvex (POP).

Baseline Solvers. We compare STRIDE with a diverse set of existing SDP
solvers. We choose SDPT3 [66] and MOSEK [7] as representative interior point
methods; CDCS [83] and SketchyCGAL [79] as representative ﬁrst-order methods;
and SDPNAL+ [78] as a representative method that combines ﬁrst-order and second-
order Newton-type methods. For SDPT3 and MOSEK, we use default parameters.
For CDCS, we use the sos solver with maximum 20, 000 iterations instead of the
default homogeneous self-dual embedding solver because we found sos to typically
perform better. For SketchyCGAL, we use the default parameters with sketching size
10. We set the maximum runtime to be 10, 000 seconds and maximum number of
iterations to be 20, 000. For SDPNAL+, we use 1e−8 as the tolerance, and we run
it for maximum 20, 000 iterations and 10, 000 seconds. For very large problems

5 For a generic POP, even ﬁnding a feasible x ∈ FPOP is NP-hard. However, in all our

numerical examples, rounding a feasible point from the approximate SDP solution is easy.

STRIDE for Solving Rank-One Semideﬁnite Relaxations

23

(e.g., m above one million), we increase the maximum runtime of SketchyCGAL and
SDPNAL+, which will be described in relevant subsections.

Hardware. All experiments are performed on a Linux PC with 12-core Intel

i9-7920X CPU@2.90GHz and 128GB RAM.

5.1 Binary quadratic programming

Consider minimizing a quadratic polynomial over the d-dimensional binary cube

min
xi∈{+1,−1},i=1,...,d

(cid:104)c, [x]2(cid:105)

(BQP)

where [x]2 : Rd → R ¯d2 is the vector of monomials in x of degree up to 2, and
c ∈ R ¯d2 contains the coeﬃcients of all monomials. Problem (BQP) is a classical NP-
hard combinatorial problem with examples such as the maximum cut (MAXCUT)
problem [29], the 0-1 knapsack problem [30], the number partitioning problem [50,
28], and the linear quadratic regulator control problem with binary inputs [71].
It is well known that the standard Shor’s semideﬁnite relaxation for (BQP) is
typically not tight, e.g., in MAXCUT problems, and hence a globally optimal
solution cannot be obtained with an optimality certiﬁcate (albeit a lower bound
can be obtained).

We consider the second-order dense moment relaxation for (BQP), which cre-
ates a positive semideﬁnite moment matrix X := [x]2[x]T
2 , using which the cost
function in (BQP) can be written as (cid:104)C, X(cid:105). The moment matrix X ∈ S ¯d2 con-
tains all the monomials in x of degree up to 4, hence is a linear subspace of S ¯d2
with dimension ¯d4. As a result, X must satisfy mmom = t( ¯d2) − ¯d4 linearly inde-
pendent equality constraints, referred to as the moment constraints. In addition,
X must satisfy redundant equality constraints, obtained by the fact that since
x2
i − 1 = 0, it also holds [x]2(x2
i − 1) = 0 for each i = 1, . . . , d. This leads to a total
of mloc = d× ¯d2 linear equality constraints. Last but not least, the top-left entry of
X is equal to 1 due to the leading element of [x]2 is 1 (the zero-order monomial).6
Therefore, the second-order relaxation for (BQP) leads to an SDP with size

n = ¯d2, m = mmom + mloc + 1 = t( ¯d2) − ¯d4 + d × ¯d2 + 1,

(36)

which grows rapidly with d. Lasserre [36] showed that the second-order relaxation is
empirically tight on a set of 50 randomly generated MAXCUT problems. However,
due to the limitation of interior point methods back then, the experiments were
performed on problems with small size d = 10.

In this paper, we aim to solve (BQP) instances with much larger d. We gen-
erate random instances of (BQP) by sampling the coeﬃcients vector c from the
standard zero-mean Gaussian distribution, i.e., ci ∼ N (0, 1), i = 1, . . . , ¯d2. At
d ∈ {10, 20, 30, 40, 50, 60}, we randomly generate three instances each and solve
the second-order moment relaxation using SDPT3, MOSEK, SDPNAL+, CDCS, Sketchy-
CGAL, and STRIDE. For STRIDE, we use the standard fmincon interface in Matlab
with an interior point solver (supplied with analytical objective and constraint

6 The reader can refer to [43] or our code for details about generating SDP data A, b, C.

24

Heng Yang et al.

gradients) as the nlp method. For rounding hypotheses from the moment matrix,
we follow

X =

n
(cid:88)

i=1

λivivT
i ,

vi ← vi/vi[1],

¯xi = sgn(vi[x]),

(37)

which ﬁrst performs a spectral decomposition of X with λ1 ≥ . . . ≥ λn in non-
increasing order, then normalizes the i-th eigenvector vi so that its leading element
is 1, and ﬁnally generates ¯xi by taking the sign of the elements of vi that corre-
spond to the order-one monomials. We round r = 5 hypotheses using the ﬁrst 5
eigenvectors (i = 1, . . . , 5). In order to compute ηs, we set Mb = n because the
diagonal entries of X are all equal to 1.

Table 1 gives the numerical results of diﬀerent solvers. We make the following
observations. (i) We ﬁrst look at the performance of interior point methods (IPMs,
SDPT3 and MOSEK). For small-scale problems (d = 10), IPMs can solve the SDPs
eﬃciently to high accuracy (e.g., around 1 second). For medium-scale problems
(d = 20), although IPMs can still obtain solutions with high accuracy, the com-
putational time starts to grow signiﬁcantly (e.g., 300-400 seconds). Moreover, for
large-scale problems (d ≥ 30), IPMs cannot be executed on ordinary workstations
due to intensive memory consumption. The fundamental challenge of IPMs lies in
solving large and dense linear systems (i.e., the m × m Schur complement system)
at each iteration. Although it is possible to use iterative solvers to solve the lin-
ear system [65], they are known to suﬀer from slow convergence as interior-point
iterates approach optimality. (ii) First-order solvers (CDCS and SketchyCGAL) can
solve problems to medium or low accuracy for d ≤ 20, but their runtime can be
worse than IPMs (although CDCS is faster than MOSEK for d = 20, its accuracy is
orders of magnitude worse than MOSEK). However, ﬁrst-order methods are indeed
advantageous in terms of memory consumption and they can still be executed for
problems with up to d = 60. Nevertheless, they are not able to compute POP
solutions of certiﬁed global optimality within reasonable time, as shown by the
nonzero ηs in Table 1 (i.e., they are not able to show that the relaxation is in-
deed tight). This phenomenon further stresses the challenge for solving degenerate
rank-one SDP relaxations. We also observe that, between CDCS and SketchyCGAL,
CDCS seems to perform much better for such problems. This suggests that sketch-
ing may not be the best choice for degenerate SDPs with large m. (iii) SDPNAL+
has the best performance among existing solvers for (BQP). It can solve (BQP)
instances to certiﬁed global optimality for up to d = 40, and it is over 10 times
faster than MOSEK and CDCS when d = 20. However, when d = 50 and d = 60,
SDPNAL+ cannot solve the SDPs to suﬃcient accuracy (within 10000 seconds), and
hence the POP solution cannot be certiﬁed as globally optimal (cf. the nonzero
ηs). (iv) Finally, we look at the performance of our solver STRIDE. We observe that
STRIDE solved all the SDPs to high accuracy, certiﬁed the global optimality of the
POP solutions, and demonstrated the tightness of the SDP relaxations (cf. the
numerically zero ηs). For small and medium problems (d = 10 and 20), STRIDE
attains accuracy that is comparable to MOSEK, while being about 30 times faster
when d = 20. For large problems (d ≥ 30), STRIDE attains accuracy that is superior
to SDPNAL+, while being 5-10 times faster when d = 50. At d = 60 with m over a
million, STRIDE becomes the only solver that can obtain solutions of high accuracy.

STRIDE for Solving Rank-One Semideﬁnite Relaxations

25

Table 1 Results on solving second-order relaxation of random (BQP) instances. “∗∗” indicates
solver out of memory.

Dimension

Run Metric

SDPT3 [66] MOSEK [7]

CDCS [83]

SketchyCGAL [79]

SDPNAL+ [78]

d: 10

n: 66

m: 1871

d: 20

n: 231

m: 20,791

d: 30

n: 496

m: 91,761

d: 40

n: 861

m: 269,781

#1

#2

#3

#1

#2

#3

#1

#2

#3

#1

#2

#3

#1

d: 50

n: 1,326

#2

m: 629,851

#3

#1

d: 60

n: 1,891

#2

m: 1,266,971

#3

ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time

1.4e−10
4.9e−11
1.5e−8
1.2e−12
2.2
8.1e−11
1.2e−11
6.0e−10
5.9e−14
1.8
2.4e−10
2.8e−11
3.5e−8
2.3e−10
1.9

3.0e−9
3.9e−9
1.8e−5
9.9e−8
351.8
2.4e−10
7.8e−9
2.1e−6
1.5e−12
341.5
1.6e−11
6.7e−10
1.3e−7
4.6e−13
342.8

2.3e−9
1.0e−9
2.8e−11
5.6e−14
1.3
7.4e−11
3.0e−11
5.7e−13
2.5e−16
1.3
5.5e−12
5.8e−12
5.1e−15
7.8e−17
1.4

1.1e−8
3.0e−9
1.9e−11
5.7e−13
246.9
1.8e−12
4.3e−13
6.9e−15
5.0e−16
250.0
4.0e−11
1.0e−11
5.5e−14
3.4e−16
223.5

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

9.7e−9
8.4e−9
8.0e−11
1.6e−12
8.8
8.6e−9
9.9e−9
1.1e−11
2.4e−13
3.0
9.2e−9
9.3e−9
3.0e−11
4.4e−12
10.3

1.1e−5
2.0e−4
9.6e−9
2.3e−4
119.6
2.9e−5
0.0013
3.9e−7
0.0013
119.5
3.9e−5
2.3e−5
3.5e−8
1.8e−5
117.4

5.0e−5
0.0025
4.8e−7
0.0448
403.6
1.0e−4
0.0023
2.8e−7
0.0082
398.0
1.1e−4
0.0032
1.0e−6
0.0115
405.5

1.2e−4
0.0030
7.8e−7
0.0602
1836
9.5e−5
0.0027
6.8e−7
0.0395
1752
2.1e−4
0.0043
7.8e−7
0.0444
1780

5.9e−5
0.0020
3.6e−7
0.0446
5232
4.2e−5
0.0016
2.3e−7
0.0532
5203
6.8e−5
0.0024
3.6e−7
0.0749
5312

5.8e−5
0.0026
2.8e−7
0.0891
9731
6.8e−5
0.0028
3.2e−7
0.0411
9732
6.0e−5
0.0026
2.9e−7
0.0973
10567

0.45
4.1e−6
0.0112
3.0e−9
50.4
0.022
1.8e−6
0.0037
1.7e−9
49.6
1.37
1.3e−14
0.384
0.0033
49.8

7.34
0.046
0.124
0.0076
405.4
4.64
0.0438
0.1529
0.0047
403.2
2.0963
0.0409
0.0391
2.9e−4
404.3

7.86
0.1199
0.1850
0.0208
2028
3.93
0.1311
0.0624
0.0049
2051
5.62
0.0995
0.2739
0.0084
2026

8.79
0.1386
0.4433
0.0828
8962
8.60
0.1340
0.3349
0.1746
8781
7.23
0.1331
0.2462
0.0295
8845

18.07
0.5285
0.9033
0.9990
7654
18.06
0.5526
0.7747
0.9394
7656
17.98
0.5607
0.7953
0.9023
7562

25.91
2.17
0.9721
0.9756
8387
25.87
2.16
0.8612
0.9998
8361
25.87
2.23
0.8746
0.9993
8320

2.4e−9
7.8e−9
8.8e−8
1.6e−12
0.85
6.7e−9
6.0e−10
6.2e−9
2.5e−14
0.63
2.89e−9
5.3e−10
5.6e−9
2.1e−13
0.89

6.7e−9
9.6e−11
3.3e−9
1.8e−16
12.55
3.7e−11
1.4e−8
2.6e−7
5.0e−16
19.6
2.2e−10
2.8e−11
4.2e−10
1.1e−16
13.0

2.4e−10
5.5e−9
8.1e−7
5.4e−15
61.9
2.8e−9
7.0e−10
1.4e−8
0
94.2
5.0e−11
1.6e−8
1.4e−6
6.3e−15
95.5

3.4e−9
1.5e−9
5.2e−8
3.1e−16
1384
3.66e−10
2.1e−9
8.1e−8
4.0e−16
1295
9.1e−9
2.8e−9
1.0e−7
2.9e−16
414.6

6.9e−12
0.0012
0.0133
0.0490
10000
9.2e−10
3.5e−7
0.0013
6.2e−8
5805
1.3e−7
5.3e−8
2.5e−6
7.4e−14
9884

8.3e−13
4.7e−4
0.0209
0.0015
10000
1.4e−11
3.5e−5
0.0019
9.1e−5
10000
0.5488
2.3e−7
0.9969
0.7549
10000

STRIDE

8.2e−16
7.9e−16
5.0e−13
3.1e−16
1.1
8.2e−16
8.8e−16
6.1e−13
0.0
1.03
8.2e−16
6.3e−14
5.1e−13
0.0
0.96

1.6e−15
5.1e−14
8.1e−13
1.2e−16
10.67
1.61e−15
8.2e−14
1.3e−12
9.9e−17
10.4
1.6e−15
3.3e−14
5.4e−13
1.1e−16
10.0

2.4e−15
1.7e−15
2.6e−12
4.5e−16
40.1
2.4e−15
6.8e−14
1.6e−12
2.3e−16
56.7
2.4e−15
1.0e−13
2.9e−12
6.4e−17
56.3

3.2e−15
1.5e−14
5.7e−13
2.3e−16
461.6
3.2e−15
5.8e−14
2.2e−12
4.0e−16
460.6
3.2e−15
1.7e−13
5.7e−12
7.2e−17
385.8

4.0e−15
7.7e−14
4.0e−12
4.1e−16
1180
4.0e−15
3.0e−14
1.6e−12
5.7e−17
1440
4.0e−15
1.6e−15
8.2e−13
4.4e−16
1368

4.7e−15
4.5e−13
3.0e−11
1.2e−15
4083
4.7e−15
1.6e−13
1.1e−11
1.0e−15
3868
4.7e−15
2.6e−13
1.7e−11
1.7e−16
3804

26

Heng Yang et al.

5.2 Quartic programming on the sphere

Consider minimizing a quartic polynomial over the d-dimension unit sphere

min
x∈Sd−1

(cid:104)c, [x]4(cid:105)

(Q4S)

where [x]4 : Rd → R ¯d4 is the vector of monomials in x of degree up to 4, and c ∈ R ¯d4
is the vector of known coeﬃcients. Problem (Q4S) is known to be NP-hard with
important examples such as computing the largest stable set of a graph [24, The-
orem 3.4], computing the 2 → 4 norm of a matrix [8], and the best separable state
problem in quantum information theory [25]. See [26, 41] and references therein for
a thorough discussion about problem (Q4S).

Here we consider the dense second-order (also the lowest order) moment relax-
ation of (Q4S) and numerically show that they are indeed tight and admit rank-one
solutions. By following the same relaxation scheme as in Section 5.1 (i.e., build
the moment matrix X = [x]2[x]T
2 and add equality constraints), we can count the
size of the SDP relaxation to be

n = ¯d2, m = t( ¯d2) − ¯d4 + ¯d2 + 1.

(38)

At each d ∈ {10, 20, 30, 40, 50, 60}, we generate three random instances of
(Q4S) by drawing c from the standard normal distribution. We solve the resulting
SDP using SDPT3, MOSEK, CDCS, SketchyCGAL, SDPNAL+, and STRIDE. For STRIDE,
we exploit the manifold structure of the sphere constraint and adopt Manopt [13]
with a trust region solver as the nlp method. One can also treat (Q4S) as a stan-
dard nonlinear programming and solve it with fmincon, but we found that Manopt is
faster and more robust for this problem. To generate hypotheses for nlp from the
moment matrix, we follow

X =

n
(cid:88)

i=1

λivivT
i ,

vi ← vi/vi[1],

¯xi =

vi[x]
(cid:107)vi[x](cid:107)

,

(39)

which ﬁrst performs a spectral decomposition of X with λ1 ≥ . . . ≥ λn in non-
increasing order, then normalizes the i-th eigenvector vi so that its leading element
is 1, and ﬁnally generates ¯xi by projecting the elements of vi that correspond to
the order-one monomials onto the unit sphere. We generate r = 5 hypotheses using
the ﬁrst 5 eigenvectors. A valid upper bound Mb on tr (X) can be obtained as

tr (X) = tr

(cid:16)
[x]2[x]T
2

(cid:17)

= [x]T

2 [x]2

= 1 +

d
(cid:88)

i=1

x2
i +

(cid:88)

(xixj)2 ≤ 2 +

1≤i≤j≤d

(cid:33)2

(cid:32) d
(cid:88)

i=1

x2
i

= 3 := Mb.

Table 2 gives the numerical results of diﬀerent solvers. We make the following
observations. (i) Similar to Table 1 for the (BQP) problem, IPMs can solve small
and medium problems (d = 10 and 20) to high accuracy, although the runtime
grows signiﬁcantly from d = 10 (about 1 second) to d = 20 (about 100 seconds).
(ii) Both CDCS and SDPNAL+ are able to solve all SDPs to high accuracy and
certify the tightness of the second-order relaxation (despite that CDCS only attained
medium accuracy for #2 at d = 40). However, SDPNAL+ is signiﬁcantly faster than

STRIDE for Solving Rank-One Semideﬁnite Relaxations

27

Table 2 Results on solving second-order relaxation of random (Q4S) instances. “∗∗” indicates
solver out of memory.

Dimension

Run Metric

SDPT3 [66] MOSEK [7]

CDCS [83]

SketchyCGAL [79]

SDPNAL+ [78]

d: 10

n: 66

m: 1277

d: 20

n: 231

m: 16,402

d: 30

n: 496

m: 77,377

d: 40

n: 861

m: 236,202

#1

#2

#3

#1

#2

#3

#1

#2

#3

#1

#2

#3

#1

d: 50

n: 1,326

#2

m: 564,877

#3

#1

d: 60

n: 1,891

#2

m: 1,155,402

#3

ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time

4.5e−9
5.1e−11
5.7e−9
3.6e−12
1.2
1.7e−9
1.4e−10
6.5e−9
1.3e−12
1.0
2.8e−9
1.5e−11
1.8e−10
2.2e−12
1.0

3.0e−11
1.3e−10
3.4e−6
4.1e−9
173.8
1.5e−11
7.2e−11
2.9e−6
1.0e−8
192.0
2.2e−10
1.2e−10
2.2e−6
2.6e−9
174.8

3.3e−8
5.1e−9
3.4e−9
3.2e−11
0.6
1.3e−8
2.0e−9
2.1e−10
6.5e−10
0.8
1.6e−12
9.1e−11
1.1e−13
3.9e−15
0.6

2.2e−9
1.3e−10
4.3e−11
4.7e−13
99.6
2.1e−8
1.2e−9
1.1e−9
1.5e−10
98.7
7.7e−9
4.5e−10
9.3e−11
5.1e−11
90.3

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

**

1.2e−11
9.7e−11
9.4e−13
1.4e−12
2.5
8.8e−11
2.4e−11
8.2e−11
4.0e−12
0.7
9.8e−11
3.2e−11
8.5e−11
3.1e−12
0.6

3.6e−12
9.9e−11
2.3e−11
2.5e−12
11.2
2.2e−12
1.0e−10
2.2e−12
4.6e−13
30.7
9.4e−11
4.5e−11
9.1e−11
1.7e−11
9.0

8.8e−13
1.0e−10
1.2e−11
2.1e−12
202.3
8.5e−13
1.0e−10
4.7e−13
1.0e−13
98.0
8.8e−13
1.0e−10
5.2e−12
9.7e−13
224.0

1.3e−12
1.0e−10
5.0e−12
1.0e−12
413.0
5.1e−7
8.3e−6
1.0e−5
0.0063
1690
9.5e−13
1.0e−10
6.1e−12
1.1e−12
622.8

8.0e−11
1.4e−8
8.4e−10
4.0e−8
4201
3.9e−12
1.0e−10
3.0e−11
6.4e−12
1015
9.4e−13
1.0e−10
2.4e−11
4.2e−12
1284

2.1e−13
1.0e−10
4.1e−12
7.9e−13
5059
3.1e−13
1.0e−10
3.1e−12
7.1e−10
7684
6.2e−13
1.0e−10
3.5e−12
6.9e−13
3669

0.1388
8.7e−4
0.1823
0.0116
58.0
0.0291
6.0e−4
0.0244
1.8e−4
57.7
0.0427
6.4e−4
0.0384
2.3e−4
58.2

0.0800
4.9e−4
0.0875
4.3e−4
566.3
0.1975
0.0010
0.2571
0.0207
553.3
0.0373
6.3e−4
0.0323
3.8e−4
551.7

0.2407
7.3e−4
0.3913
0.0037
1092
0.2198
7.3e−4
0.3468
0.0070
1052
0.2637
1.3e−4
0.4770
0.0196
1046

0.3876
1.8e−14
0.7867
0.0804
3005
0.3401
2.0e−14
0.7936
0.1052
3034
0.3397
2.1e−14
0.8206
0.1558
3045

0.3188
3.2e−14
0.8868
0.3756
5364
0.3341
3.3e−14
0.8910
0.4208
5356
0.3199
3.1e−14
0.9205
0.4937
5356

0.3276
3.8e−14
0.9610
0.6813
6895
0.3301
5.2e−14
0.9281
0.5856
6905
0.3133
4.0e−14
0.9572
0.5980
6909

4.6e−12
1.9e−11
6.9e−12
2.1e−11
0.6
1.8e−11
1.7e−11
1.7e−11
2.4e−13
0.6
2.5e−11
3.6e−11
3.4e−11
1.4e−12
0.6

6.1e−11
2.9e−11
6.4e−11
1.6e−12
2.9
1.7e−11
7.1e−11
1.5e−11
7.4e−13
3.0
8.8e−11
1.9e−11
1.3e−10
1.5e−12
2.7

4.4e−13
1.0e−12
9.6e−10
9.5e−13
15.1
5.8e−11
7.4e−11
7.5e−11
2.6e−13
10.6
1.5e−12
8.3e−11
1.3e−11
2.4e−12
12.3

2.4e−12
4.4e−11
7.4e−12
1.1e−12
32.7
9.4e−11
7.2e−11
5.9e−10
7.5e−12
50.5
1.5e−11
5.4e−11
3.1e−11
3.1e−12
32.1

2.6e−13
3.4e−10
1.6e−9
7.2e−11
156.5
1.1e−12
6.1e−11
6.9e−12
1.2e−12
75.9
5.3e−14
5.7e−11
6.5e−9
1.4e−10
113.3

3.4e−13
1.8e−10
5.2e−9
8.2e−10
321.9
4.0e−14
9.9e−11
1.6e−10
3.5e−11
286.3
7.9e−15
6.1e−11
4.3e−9
2.6e−10
268.9

STRIDE

1.5e−16
3.5e−15
6.8e−10
2.0e−11
1.2
1.6e−16
1.0e−11
5.0e−11
2.9e−12
1.0
1.4e−16
5.5e−12
2.1e−11
1.9e−12
1.1

1.0e−16
4.6e−15
2.7e−11
2.5e−12
4.4
1.2e−16
7.9e−14
5.5e−13
8.6e−14
14.7
2.7e−16
4.5e−13
4.5e−13
2.7e−13
4.5

2.1e−16
5.0e−15
7.9e−12
1.3e−12
17.3
1.8e−16
4.4e−14
5.7e−13
1.0e−13
18.5
1.5e−16
1.1e−12
1.5e−11
2.7e−12
16.1

2.9e−16
5.2e−15
3.9e−12
6.8e−13
46.8
2.1e−16
5.5e−13
4.2e−12
7.2e−12
76.7
2.1e−16
5.2e−15
6.0e−11
9.8e−12
73.4

2.0e−16
5.8e−15
2.7e−11
4.8e−12
240.2
2.6e−16
5.8e−15
1.7e−12
3.1e−13
138.5
1.8e−16
5.9e−15
1.9e−12
3.2e−13
145.9

2.1e−16
6.2e−15
3.0e−12
5.7e−13
283.4
4.2e−16
6.3e−15
5.5e−14
8.8e−15
303.5
4.5e−16
6.1e−15
3.6e−11
6.6e−12
311.5

28

Heng Yang et al.

CDCS, in most cases 10-30 times faster. Compared to Table 1, these results suggest
that the (Q4S) relaxation is easier to solve than the (BQP) relaxation, perhaps
because (Q4S) only has a single unit-norm constraint. (iii) SketchyCGAL, however,
failed to solve most of the SDPs to high accuracy, despite taking more time than
CDCS and SDPNAL+. (iv) Our solver STRIDE achieved similar performance compared
to SDPNAL+. Although STRIDE can be slightly slower than SDPNAL+, it generally
attained higher accuracy than SDPNAL+.

5.3 Outlier-robust Wahba problem

Consider the problem of ﬁnding the best 3D rotation to align two sets of 3D points
while explicitly tolerating outliers

min
q∈S3

N
(cid:88)

i=1

min

(cid:40) (cid:13)

(cid:13)˜zi − q ◦ ˜wi ◦ q−1(cid:13)
2
(cid:13)
β2
i

(cid:41)

, 1

(40)

where q ∈ S 3 is the unit quaternion parametrization of a 3D rotation, (zi ∈
R3, wi ∈ R3)N
i=1 are given N pairs of 3D points (often normalized to have unit
norm), ˜z (cid:44) [zT, 0]T ∈ R4 denotes the zero-homogenization of a 3D vector z, q−1 (cid:44)
[−q1, −q2, −q3, q4]T is the inverse quaternion, “◦” denotes the quaternion product
deﬁned as

q ◦ p (cid:44)







q4 −q3 q2 q1
q4 −q1 q2
q3
−q2 q1
q4 q3
−q1 −q2 −q3 q4







p,

∀q, p ∈ R4,

(41)

βi > 0 is a given threshold that determines the maximum inlier residual, and
min {·, ·} realizes the so-called truncated least squares (TLS) cost function in robust
estimation [6]. Intuitively, the term q ◦ ˜wi ◦ q−1 is the rotated copy of wi, and the
(cid:96)2 norm in (40) measures the Euclidean distance between zi and wi after rotation
(a metric for the goodness of ﬁt). Problem (40) therefore seeks to ﬁnd the best
3D rotation that minimizes the sum of (normalized) squared Euclidean distances
between zi and wi while preventing outliers from damaging the estimation via the
usage of the TLS cost function, which assigns a constant value to those pairs of
points that cannot be aligned well (i.e., outliers). A pictorial description of the
outlier-robust Wahba problem is presented in Fig. 2. Problem (40) is nonsmooth,
but can be equivalently reformulated as

min
q∈S3,
θi∈{+1,−1},i=1,...,N

N
(cid:88)

i=1

1 + θi
2

(cid:13)˜zi − q ◦ ˜wi ◦ q−1(cid:13)
(cid:13)
2
(cid:13)
β2
i

+

1 − θi
2

(Wahba)

by introducing N binary variables {θi} that expose the combinatorial nature. Each
θi acts as the selection variable for determining whether the i-th pair of 3D points
(zi, wi) is an inlier or an outlier. Problem (Wahba) is a fundamental problem in
aerospace, robotics and computer vision, and is the rotation subproblem in point
cloud registration [77, 73].

To solve (Wahba) to global optimality, Yang and Carlone [74] proposed the
following semideﬁnite relaxation that was empirically shown to be always tight. Let

STRIDE for Solving Rank-One Semideﬁnite Relaxations

29

Fig. 2 An example of the outlier-robust (Wahba) problem. (a) Four pairs of 3D points (zi, wi)
lying around a unit sphere, with one of the pairs being an outlier that cannot be aligned well by
a 3D rotation. (b) A truncated least squares (TLS) cost function compared with a least squares
cost function. The TLS cost function prevents the outlier from contaminating the estimation
problem by assigning a constant cost to the outlier. Adapted from [74].

x = [qT, θ1, . . . , θN ]T ∈ Rd, d = 4+N , be the variable of the nonlinear programming
problem (Wahba), construct

[x]s = [qT, θ1qT, . . . , θN qT]T ∈ Rn, n = 4N + 4

(42)

as the sparse set of monomials in x of degree up to 2 (a technique that was
dubbed binary cloning), and then build X = [x]s[x]T
s as the sparse moment matrix.
Because of the binary constraint θ2
i = 1, it can be easily seen that: (i) the diagonal
4 × 4 blocks of X are all identical (θ2
i qqT = qqT), and (ii) the oﬀ-diagonal 4 × 4
blocks are symmetric (θiθjqqT ∈ S4). Because of the unit quaternion constraint,
X satisﬁes tr (X) = N + 1. Therefore, this leads to a semideﬁnite relaxation of size

n = 4N + 4, m = 10N + 1 + 3N (N + 1).

(43)

Compared to the dense second-order moment relaxation in Section 5.1 and 5.2, this
sparse second-order relaxation is much more manageable, and the largest N whose
relaxation was successfully solved by interior point method was N = 100 [74]. This
sparse second-order relaxation scheme has been shown as a general framework for
certiﬁable outlier-robust machine perception [76].

Here we show the scalability of our solver by solving instances of (Wahba)
up to N = 1000 and obtaining the globally optimal solution. At each N =
50, 100, 200, 500, 1000, we generate three random instances of the (Wahba) prob-
lem as follows. (i) We draw a random 3D rotation R ∈ SO(3) (a rotation matrix
can be converted from and to a unit quaternion easily); (ii) we simulate N 3D unit
vectors wi, i = 1, . . . , N uniformly on the unit sphere; (iii) we generate

zi = Rwi + (cid:15)i,

(cid:15)i ∼ N (0, 0.012), i = 1, . . . , N

(44)

by rotating wi and adding Gaussian noise; (iv) we replace 50% of the zi’s by ran-
dom unit vectors on the sphere so that they do not follow the generative model
(44) and are considered as outliers. We then use SDPT3, MOSEK, CDCS, Sketchy-
CGAL, SDPNAL+, and STRIDE to solve the SDP relaxations. For STRIDE, we use
Manopt with a trust region solver as the nlp method for solving the nonlinear pro-
gramming (Wahba). Speciﬁcally, q ∈ S 3 is modeled as a sphere manifold, and

ziTruncated Least SquaresLeast SquaresResidualCostoutlierinlierwi−βi(a)(b)βi30

Heng Yang et al.

θ ∈ {+1, −1}N is modeled as an oblique manifold of size 1 × N (an oblique man-
ifold of size n × m is the set of matrices of size n × m with unit-norm columns),
and the problem is treated as an unconstrained problem on the product of two
manifolds. To round hypotheses from a moment matrix X, we follow

X =

n
(cid:88)

i=1

λivivT
i ,

qi =

vi[q]
(cid:107)vi[q](cid:107)

,

j = sgn(qT
θi

i vi[θjq]), j = 1, . . . , N,

(45)

where we ﬁrst perform spectral decomposition of X with eigenvalues in nonincreas-
ing order, then round qi by normalizing the corresponding entries of vi to have
unit norm, and ﬁnally generate θi
j by taking the sign of the dot product between
the rounded qi and the entries of vi corresponding to each θjq block (the rationale
for using this rounding method is easily seen from (42) where θi
j is identiﬁed with
qT
i (θjqi) for the rounded qi with unit norm). We generate r = 5 hypotheses by
rounding 5 eigenvectors from the moment matrix. We set Mb = N + 1 = tr (X) to
compute ηs as in (35).

Table 3 gives the numerical results for diﬀerent solvers. Notice that at N =
1000, we increased the maximum runtime of SketchyCGAL and SDPNAL+ to be 50000
seconds for a fair comparison with STRIDE. We make the following observations. (i)
IPMs can solve small and medium problems (N = 50 and 100) to high accuracy
and certify global optimality of the POP solutions. However, their runtime grows
quickly and they cannot scale to problems with N ≥ 200. (ii) CDCS, SketchyCGAL
and SDPNAL+ perform poorly on this problem. Notably, CDCS and SketchyCGAL
failed on all instances and they cannot certify global optimality and tightness
of the relaxation. SDPNAL+ succeeded on problems with N = 50 but failed to
attain high accuracy for all other problems. Comparing Table 3 with Tables 1-
2, the degraded performance of CDCS and SDPNAL+ seems to suggest that sparse
relaxations are more challenging to solve than dense relaxations. (iii) STRIDE was
able to solve all SDP instances to high accuracy. Particularly, for N = 50 and
N = 100, STRIDE achieved similar accuracy compared to MOSEK, while being 3
times faster at N = 50 and 20 times faster at N = 100. For N ≥ 200, STRIDE is
the only solver than can attain high accuracy and certify global optimality and
tightness (despite taking much less time than the other solvers).

STRIDE with Domain-Speciﬁc Primal Initialization. In Section 3.1, we
mentioned that STRIDE can beneﬁt from domain-speciﬁc primal initialization. We
now use the (Wahba) problem to support our claim. Although the (Wahba) prob-
lem is a combinatorial problem with binary variables, Yang et al. [72] have designed
a heuristic method called graduated non-convexity (GNC) that can solve (Wahba)
to global optimality with high probability of success (note that GNC only outputs
a solution without optimality certiﬁcate). Therefore, we ﬁrst use GNC to solve the
combinatorial (Wahba) problem and then use its solution as a primal initializa-
tion for STRIDE. Particularly, let (ˆq, ˆθ) be the output of GNC, we input a rank-one
s to STRIDE, where [ˆx]s is computed from (42) with ˆx = (ˆq, ˆθ).
point X 0 = [ˆx]s[ˆx]T
The last column of Table 3 shows the numerical results for STRIDE with primal
initialization supplied by GNC. We can see that the GNC primal initialization gives
STRIDE an additional 2-3 times speedup.

Outlier-Robust Wahba Problem on Real Data: To show the practical
usefulness of STRIDE, we test it on two applications of the Wahba problem on real
data. The ﬁrst application is image stitching on PASSTA [49] shown in Fig. 3(a).

STRIDE for Solving Rank-One Semideﬁnite Relaxations

31

Table 3 Results on solving sparse second-order relaxation of random (Wahba) instances.
“∗∗” indicates solver out of memory. The last column shows results for STRIDE with primal
initialization using graduated non-convexity (GNC) [72].

Dimension

Run Metric

SDPT3 [66] MOSEK [7]

CDCS [83]

SketchyCGAL [79]

SDPNAL+ [78]

N : 50

n: 204

m: 8,151

N : 100

n: 404

m: 31,301

N : 200

n: 804

m: 122,601

N : 500

n: 2004

m: 756,501

N : 1000

n: 4004

m: 3,013,001

#1

#2

#3

#1

#2

#3

#1

#2

#3

#1

#2

#3

#1

#2

#3

ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time

4.9e−11
1.0e−12
1.2e−8
9.5e−13
65.8
5.0e−9
1.3e−12
1.8e−7
3.6e−12
67.4
1.3e−9
2.2e−12
3.4e−8
4.0e−15
64.6

**

**

**

**

**

**

**

**

**

**

**

**

1.6e−13
3.7e−15
1.3e−10
8.9e−15
32.7
2.2e−12
8.0e−11
8.6e−10
1.9e−13
33.2
3.0e−12
3.5e−15
1.3e−9
5.8e−13
34.7

1.3e−11
4.5e−15
4.1e−11
7.0e−14
974.9
3.8e−12
5.6e−15
5.7e−11
7.8e−14
849.6
6.7e−12
4.9e−15
3.9e−9
1.3e−12
880.7

**

**

**

**

**

**

**

**

**

2.4e−6
3.3e−5
0.0020
0.1974
90.2
2.4e−6
3.4e−5
0.0021
0.0887
87.2
2.2e−6
3.2e−5
0.0019
0.0717
86.7

2.4e−7
2.6e−5
1.5e−4
0.1338
326.8
3.6e−7
2.5e−5
2.3e−4
0.3539
321.4
2.2e−7
2.5e−5
1.4e−4
0.2188
323.9

1.0e−6
1.6e−5
4.8e−4
0.3140
1208
1.0e−6
1.8e−5
4.8e−4
0.2921
1206
9.7e−7
1.8e−5
4.6e−4
0.3027
1204

4.1e−7
1.3e−5
1.7e−4
0.3385
7659
1.8e−7
1.5e−5
7.4e−5
0.3813
7718
5.7e−7
1.3e−5
2.3e−4
0.3326
7849

1.5e−6
1.7e−5
4.0e−3
0.5787
62900
1.5e−6
1.8e−5
4.0e−3
0.5907
60554
1.5e−6
1.8e−5
4.0e−3
0.5846
64392

0.8356
0.0354
0.9996
0.9990
302.3
0.8372
0.0346
0.9996
0.9996
303.8
0.8352
0.0348
0.9996
0.9996
301.2

0.8761
0.0513
0.9999
0.9996
830.9
0.8622
0.0518
0.9999
0.9999
831.7
0.8570
0.0514
0.9999
0.9997
830.5

0.8931
0.0904
1.0
0.9999
1723
0.8913
0.0919
1.0
0.9999
1646
0.8892
0.0821
1.0
0.9999
1712

0.8985
0.1318
1.0
1.0
6035
0.8984
0.1295
1.0
1.0
6023
0.8984
0.1142
1.0
1.0
5994

0.8993
0.1747
1.0
1.0
42546
0.8994
0.1316
1.0
1.0
42504
0.8994
0.1212
1.0
1.0
42498

4.6e−14
5.0e−10
5.2e−6
2.1e−12
48.8
3.4e−11
3.5e−11
3.8e−7
9.6e−14
39.8
4.5e−13
1.7e−10
1.7e−6
3.9e−14
44.7

7.1e−15
9.6e−6
0.0750
0.0219
526.0
1.1e−14
1.0e−5
0.0797
0.0212
488.2
2.2e−14
1.1e−5
0.0882
0.0186
454.1

8.9e−15
1.2e−5
0.1086
0.0029
1941
4.5e−8
1.3e−5
0.0671
0.0136
2349
8.8e−6
1.3e−5
0.0693
0.0398
2392

3.1e−14
1.1e−5
0.0662
0.2480
10001
2.6e−4
1.1e−5
0.0780
0.1154
10001
2.5e−4
1.1e−5
0.0745
0.0644
10219

4.0e−14
8.4e−6
6.5e−3
0.3102
50004
3.7e−8
8.4e−6
1.2e−3
0.3118
44929
1.3e−13
8.7e−6
0.0174
0.3265
50003

STRIDE

1.4e−16
2.4e−15
1.0e−9
5.4e−14
13.7
2.6e−15
3.6e−15
8.7e−12
9.6e−14
13.3
1.4e−16
4.0e−15
3.9e−9
3.8e−12
13.5

5.3e−15
1.6e−13
2.3e−9
1.9e−11
50.7
0.0
5.5e−15
8.4e−9
5.5e−14
46.4
1.5e−15
2.5e−13
3.7e−9
1.4e−13
44.0

0.0
2.6e−13
5.3e−9
3.9e−11
291.9
5.9e−15
2.1e−13
1.2e−9
5.5e−9
311.5
2.6e−13
8.2e−15
1.7e−9
4.5e−13
272.6

4.3e−14
1.5e−14
3.8e−11
2.3e−13
4389
7.7e−13
2.7e−14
7.8e−10
2.6e−14
2848
2.6e−14
1.4e−14
1.1e−10
4.4e−15
3743

1.0e−13
3.1e−14
1.3e−10
1.2e−13
30269
8.0e−13
3.1e−14
1.4e−10
2.2e−13
50423
2.4e−13
2.6e−14
2.0e−11
7.3e−14
48846

w/ GNC

1.2e−15
4.6e−15
4.0e−10
1.1e−13
4.0
2.6e−15
5.6e−13
5.9e−9
1.3e−13
4.2
0.0
3.3e−13
3.3e−9
3.4e−14
3.9

5.3e−15
5.4e−15
4.7e−9
1.9e−15
32.7
4.5e−13
4.9e−15
8.3e−10
1.3e−13
17.7
1.7e−15
5.9e−15
2.4e−9
1.4e−13
16.1

9.1e−15
2.9e−13
1.8e−9
4.3e−9
174.6
4.5e−15
2.6e−13
5.2e−9
4.7e−13
185.2
2.9e−13
1.2e−14
1.5e−9
1.8e−11
171.7

3.2e−13
1.4e−13
4.3e−9
1.7e−12
1819
1.1e−12
5.0e−14
1.5e−9
2.1e−13
1335
5.5e−14
1.7e−14
1.2e−11
1.0e−13
2316

1.4e−14
3.9e−14
3.3e−13
1.2e−9
26894
1.1e−16
3.3e−14
1.2e−9
9.8e−13
41647
3.4e−15
3.6e−14
6.2e−10
2.5e−13
45100

32

Heng Yang et al.

Given two images taken by the same camera with an unknown relative rotation,
we ﬁrst use SURF [9] to establish N = 70 putative keypoint matches, and then
use STRIDE to solve the SDP relaxation of the Wahba problem (n = 284, m =
15, 611) to estimate the relative rotation and stitch the two images. STRIDE obtains
the globally optimal solution (max{ηp, ηd, ηg} = 6.2e−13, ηs = 7.8e−14) in 79
seconds. The second application is point cloud registration on 3DMatch [80] shown
in Fig. 3(b). Given two point clouds with an unknown relative rotation, we ﬁrst
use FPFH [60] and ROBIN [62] to establish N = 108 keypoint matches, and then
use STRIDE to solve the SDP relaxation (n = 436, m = 36, 397) to estimate the
rotation and register the point clouds. STRIDE obtains the globally optimal solution
(max{ηp, ηd, ηg} = 1.6e−10, ηs = 1.8e−13) in 53 seconds.

(a) Image stitching on PASSTA [49].

(b) Point cloud registration on 3DMatch [80].

Fig. 3 STRIDE solves outlier-robust Wahba problems on real datasets to global optimality.

5.4 Nearest structured rank deﬁcient matrices

Let N, N1, N2 be positive integers with N1 ≤ N2, and let L : RN → RN1×N2 be an
aﬃne map. Consider ﬁnding the nearest structured rank deﬁcient matrix problem

(cid:110)

min
u∈RN

(cid:107)u − θ(cid:107)2 | L(u) is rank deﬁcient

,

(46)

(cid:111)

where θ ∈ RN is a given point. Problem (46) is commonly known as the struc-
tured total least squares (STLS) problem [59, 48], and has numerous applications in
control, systems theory, statistics [46], approximate greatest common divisor [35],
camera triangulation [3], among others [47]. Problem (46) can be reformulated as
the following polynomial optimization problem

min
z∈SN1−1,u∈RN

(cid:40)

(cid:107)u − θ(cid:107)2

(cid:32)

zT

L0 +

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N
(cid:88)

i=1

(cid:33)

(cid:41)

uiLi

= 0

,

(STLS)

where Li ∈ RN1×N2 , i = 0, . . . , N , are the set of independent bases of the aﬃne
map L, z ∈ S N1−1 is a unit vector in the left kernel of L(u) and acts as a witness
of rank deﬁciency. Problem (STLS) is easily seen to be nonconvex and the best
algorithm in practice is based on local nonlinear programming [47].

Recently, Cifuentes [20] devised a semideﬁnite relaxation for (STLS) and proved
that the relaxation is guaranteed to be tight under a low-noise assumption [21].

SURF Matches, Green: Inliers, Red: OutliersSTRIDE StitchingFPFH Matches, Green: Inliers, Red: OutliersSTRIDE RegistrationSTRIDE for Solving Rank-One Semideﬁnite Relaxations

33

This semideﬁnite relaxation is very similar to the relaxation for (Wahba) pre-
sented in Section 5.3 and is also a sparse second-order moment relaxation. Let
x = [zT, uT]T ∈ Rd, d = N1 + N be the vector of unknowns in (STLS), construct
the sparse monomial vector of degree up to 2 in x as

[x]s = [zT, u1zT, . . . , uN zT]T ∈ Rn, n = (N + 1)N1

(47)

and build the moment matrix X = [x]s[x]T
s . It can be easily checked that all the
oﬀ-diagonal N1 × N1 blocks, uiujzzT, are symmetric by construction. Using [x]s,
the N2 equality constraints in (STLS) can be conveniently written as [x]T
s ai =
0, i = 1, . . . , N2, for constant vectors ai ∈ Rn. In addition, each of the equality
constraint also gives rise to n redundant constraints of the form ([x]T
s ai)[x]s = 0.
Finally, the unit sphere constraint z ∈ S N1−1 implies the trace of the leading
N1 × N1 block of X is equal to 1. The construction above leads to a semideﬁnite
relaxation with size

n = (N + 1)N1, m = 1 + nN2 + t(N1 − 1) × t(N ).

(48)

Due to the limitation of interior point methods, Cifuentes [20] was only able to
numerically verify the tightness of the relaxation for very small problems (e.g.,
N1 ≤ N2 ≤ 10, N < 20).

We aim to compute globally optimal solutions of (STLS) with much larger di-
mensions. We perform experiments on random instances of (STLS) where the aﬃne
map L is structured to be a square Hankel matrix such that N = N1 +N2 −1, N1 =
N2. We set N1 ∈ {10, 20, 30, 40}, and at each level we randomly generate three
problem instances by drawing θ ∼ N (0, IN ) from the standard Gaussian distri-
bution. We then solve the sparse semideﬁnite relaxation using SDPT3, MOSEK,
CDCS, SketchyCGAL, SDPNAL+, and STRIDE. For STRIDE, since the nonlinear pro-
graming (STLS) does not admit any manifold structure, we use fmincon with an
interior point method as the nlp method. To generate hypotheses for nlp from the
moment matrix X, we follow

X =

n
(cid:88)

i=1

λivivT
i ,

zi =

vi[z]
(cid:107)vi[z](cid:107)

,

j = zT
ui

i vi[ujz], j = 1, . . . , N,

(49)

where we ﬁrst perform a spectral decomposition, then round zi by projecting
the entries of vi corresponding to block z onto the unit sphere, and round ui
j
by computing the inner product between zi and the entries of vi corresponding
to block ujz (again, the rationale for this rounding comes from the lifting (47)).
We generate r = 5 hypotheses by rounding 5 eigenvectors. In order to set Mb
for computing ηs, we make the assumption that the search variable u of (STLS)
contains random vectors that follow N (0, IN ), and its squared norm follows a chi-
square distribution of degree N . As a result, we choose the quantile corresponding
to a 99.9% probability, denoted as M , as the bound on (cid:107)u(cid:107)2, such that Mb = M +1
can upper bound the trace of X.

Table 4 gives the numerical results of diﬀerent solvers. Notice that at N1 = 40,
we have increased the maximum runtime of CDCS and SDPNAL+ to 20000 seconds to
make a fair comparison with STRIDE. Generally, Table 4 has similar results as Table
3. (i) IPMs can solve small problems to high accuracy, but cannot handle large
problems due to memory issues. (ii) First-order solvers (CDCS and SketchyCGAL) are

34

Heng Yang et al.

Table 4 Results on solving sparse second-order relaxation of random (STLS) instances. “∗∗”
indicates solver out of memory.

Dimension

Run Metric

SDPT3 [66] MOSEK [7]

CDCS [83]

SketchyCGAL [79]

SDPNAL+ [78]

4.8e−12
1.2e−10
5.5e−6
5.7e−8
94.0
1.2e−10
1.7e−10
5.5e−5
6.4e−6
92.2
1.6e−10
2.3e−11
1.9e−6
2.4e−8
99.2

1.6e−11
2.2e−15
1.1e−12
8.7e−14
48.2
1.8e−10
4.1e−11
5.2e−9
2.4e−9
43.9
9.0e−12
9.7e−9
1.7e−10
1.5e−11
44.4

N1: 10
n: 200

m: 10,551

N1: 20
n: 800

m: 164,201

N1: 30
n: 1800

m: 823,951

N1: 40
n: 3200

m: 2,592,801

#1

#2

#3

#1

#2

#3

#1

#2

#3

#1

#2

#3

ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time
ηp
ηd
ηg
ηs
time

**

**

**

**

**

**

**

**

**

3.7e−9
1.5e−5
9.4e−9
0.0078
138.6
4.5e−6
3.8e−4
1.9e−4
0.1179
140.9
8.3e−7
1.4e−4
5.3e−5
0.1406
139.8

2.7e−6
3.4e−4
2.2e−4
0.3945
4206
2.5e−6
2.7e−4
1.6e−4
0.4805
4187
4.3e−6
3.4e−4
3.1e−4
0.4493
4224

1.4e−5
1.0e−3
1.7e−4
0.87
12234
1.6e−5
0.0011
3.3e−4
0.8248
12324
1.4e−5
0.0010
3.7e−4
0.8191
12360

**

**

**

**

**

**

**

**

**

**

**

**

0.3602
4.3e−14
9.1e−4
0.8808
339.7
0.3787
2.6e−5
0.2894
0.8544
344.2
0.3943
1.6e−5
0.0164
0.8993
338.1

0.9608
0.0325
0.1207
0.9863
2171
0.9326
0.0109
0.4961
0.9714
2163
0.9450
0.0200
0.1536
0.9798
2178

1.3862
1.3752
0.5175
0.9997
6047
1.3826
1.3544
0.4171
0.9996
5994
1.3819
1.3376
0.3730
0.9996
5976

1.6887
2.8283
0.4648
1.0
17222
1.6865
2.8528
0.4391
1.0
17270
1.6875
2.9918
0.5329
1.0
17227

1.0e−10
1.2e−11
5.3e−9
1.2e−8
13.3
1.7e−11
1.0e−10
6.3e−10
1.5e−8
48.6
7.5e−11
8.9e−11
7.3e−11
3.9e−13
38.9

3.2e−13
7.8e−10
2.8e−7
1.2e−8
1189
5.6e−13
1.7e−10
2.0e−10
9.1e−8
870.6
5.3e−11
7.9e−11
1.3e−8
4.1e−9
1169

1.7e−12
2.4e−5
0.1060
0.2083
10000
2.0e−14
1.2e−5
0.1077
0.0454
10001
2.2e−12
5.8e−6
0.0569
0.0640
10000

9.7e−14
2.3e−5
0.0439
0.2345
20003
2.3e−5
2.6e−5
0.0750
0.3296
20008
3.5e−4
1.4e−4
0.1743
0.6361
20345

STRIDE

6.2e−16
6.3e−15
4.8e−12
1.1e−11
14.1
1.5e−15
2.5e−14
2.1e−11
2.5e−11
18.4
8.2e−16
1.0e−13
8.0e−11
1.9e−10
17.4

1.9e−15
3.8e−14
1.8e−10
2.8e−10
292.5
1.5e−15
2.4e−15
2.8e−10
7.2e−10
257.7
1.6e−15
2.2e−15
1.2e−10
2.5e−10
324.4

5.4e−15
4.9e−14
1.6e−9
6.8e−10
1679
1.6e−8
2.3e−11
2.8e−7
2.7e−7
1813
4.2e−15
2.3e−15
6.9e−10
4.6e−10
2050

5.6e−15
2.2e−12
4.3e−10
1.3e−7
14541
4.9e−15
1.0e−13
2.1e−9
3.5e−9
9994
7.9e−15
5.3e−12
2.0e−8
2.8e−7
10120

memory eﬃcient but exhibit slow convergence and cannot attain high accuracy
to certify global optimality and tightness of the relaxation. SDPNAL+ can solve
problems with N1 = 10 and 20 to high accuracy, but failed for N1 = 30 and 40.
The degraded performance of CDCS, SketchyCGAL, and SDPNAL+ compared to Tables
1-2 again suggests the diﬃculty in solving sparse relaxations compared to dense
relaxations. (iii) STRIDE computed solutions of high accuracy for all test instances
and it is about 5 times faster than SDPNAL+.

STRIDE for Solving Rank-One Semideﬁnite Relaxations

35

6 Concluding Remarks

In this paper, we have designed an eﬃcient algorithmic framework, STRIDE, to solve
large-scale tight semideﬁnite programming (SDP) relaxations of polynomial opti-
mization problems (POPs). STRIDE employs an inexact projected gradient method
(iPGM) as its backbone, but leverages fast nonlinear programming (NLP) meth-
ods to seek rapid primal acceleration. By conducting a novel convergence analysis
for the iPGM and taking safeguarded steps, we have proved the global conver-
gence of STRIDE. For solving the projection subproblem in every iPGM step, we
have designed a modiﬁed limited-memory BFGS method and proved its global
convergence. In addition to the contributions in algorithmic design and conver-
gence analysis, we have studied several important classes of POPs and their dense
or sparse (tight) relaxations. In our extensive numerical experiments, STRIDE glob-
ally solved all the POPs and the corresponding SDP relaxations to very high
accuracy, and it oﬀered state-of-the-art eﬃciency and robustness. We hope the
practical performance of STRIDE can encourage more theoretical study towards in-
vestigating when and why the NLP iterates can produce eﬀective acceleration to
the iPGM backbone.

Acknowledgements The authors would like to thank Diego Cifuentes for useful discussions
about the nearest structured rank deﬁcient matrix problem, and Jie Wang for clariﬁcations
about sparse semideﬁnite relaxations. H. Yang and L. Carlone were partially funded by ARL
DCIST CRA W911NF-17-2-0181 and NSF CAREER award “Certiﬁable Perception for Au-
tonomous Cyber-Physical Systems”.

A Proof of Theorem 1

Let us ﬁrst prove the following useful lemma for later convenience.

Lemma 1 Under the inexactness conditions in (6), for any X ∈ Sn

+, it holds that

(cid:68)

C, X − X k(cid:69)

≥

1
σk

(cid:13)
(cid:13)

(cid:13)X k − X k−1(cid:13)

(cid:13)
(cid:13)

2

+

1
σk

(cid:68)

X k−1 − X, X k − X k−1(cid:69)

(cid:68)

A∗yk, X − X k(cid:69)

.

+

Proof From the last two conditions in (6), A∗yk +Sk −C = 1
σk
0, we have that

(cid:0)X k − X k−1(cid:1) and (cid:10)X k, Sk(cid:11) =

(cid:68)

C, X − X k(cid:69)

=

(cid:28)

A∗yk + Sk −

(cid:16)

X k − X k−1(cid:17)

, X − X k

(cid:29)

1
σk

=

≥

1
σk

1
σk

(cid:68)

X k − X k−1, X k − X

(cid:69)

(cid:68)

A∗yk, X − X k(cid:69)

+

(cid:68)

+

(cid:13)
(cid:13)

(cid:13)X k − X k−1(cid:13)

(cid:13)
(cid:13)

(cid:124)

2

+

1
σk

(cid:68)

X k−1 − X, X k − X k−1(cid:69)

where the last inequality is due to the following fact:

Sk, X
(cid:123)(cid:122)
≥0
(cid:68)

+

−

(cid:69)

(cid:125)

(cid:68)

(cid:124)

Sk, X k(cid:69)
(cid:123)(cid:122)
(cid:125)
=0

A∗yk, X − X k(cid:69)

(cid:68)

1
σk

X k − X k−1, X k − X

(cid:69)

=

=

1
σk

Thus, the proof is completed.

(cid:68)

1
σk
(cid:13)
(cid:13)X k − X k−1(cid:13)
(cid:13)

(cid:13)
(cid:13)

X k − X k−1, X k − X k−1 + X k−1 − X

(cid:69)

2

+

1
σk

(cid:68)

X k−1 − X, X k − X k−1(cid:69)

.

,

(cid:117)(cid:116)

36

Heng Yang et al.

Then, we shall conduct our proof of Theorem 1 as follows.

Proof For notational simplicity, denote rk = A(X k) − b and ek = X k − X (cid:63) for all k ≥ 0.
Then, A(ek) = rk since A(X (cid:63)) = b. Moreover, we have (cid:13)
(cid:13) ≤ εk from the ﬁrst condition in
(6). Therefore, the convergence of (cid:8)(cid:13)
(cid:13)rk(cid:13)
(cid:13) ≤ εk and {kεk} is summable.
Recall that X (cid:63) is an optimal solution to problem (P). Let X = X k−1 in Lemma 1, we get
(cid:13)ek − ek−1(cid:13)
On the other hand, let X = X (cid:63) in Lemma 1, we obtain

(cid:13)rk(cid:13)
(cid:9) is obvious, since (cid:13)

A∗yk, ek−1 − ek(cid:69)

C, X k−1 − X k(cid:69)

(cid:13)rk(cid:13)
(cid:13)

1
σk

(50)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

+

≥

(cid:68)

(cid:68)

2

(cid:68)

C, X (cid:63) − X k(cid:69)

≥

1
σk

(cid:13)
(cid:13)

(cid:13)ek − ek−1(cid:13)

2
(cid:13)
(cid:13)

+

1
σk

(cid:68)

ek−1, ek − ek−1(cid:69)

(cid:68)

A∗yk, ek(cid:69)

.

−

(51)

Multiplying k − 1 to (50) and add the resulting inequality to (51) yields

X k−1 − X (cid:63)(cid:17)
(cid:68)
1
σk

2
(cid:13)
(cid:13)

+

X k − X (cid:63)(cid:17)(cid:69)
(cid:16)

− k

ek−1, ek − ek−1(cid:69)

+ (k − 1)

− (2k − 1)

2

2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)ek−1(cid:13)
(cid:13)ek−1(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:68)

C, (k − 1)

(cid:16)

≥

=

≥

=

≥

k
σk
1
σk
1
σk
1
2σk
1
2σk

2

k

(cid:18)

(cid:18)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)ek − ek−1(cid:13)
(cid:13)ek(cid:13)
(cid:13)ek(cid:13)
(cid:13)ek(cid:13)
(cid:13)ek(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

−

k

2

2

2

1
2σk
1
2σk−1

+ (k − 1)

(cid:13)
(cid:13)

(cid:13)ek−1(cid:13)

(cid:13)
(cid:13)

2

(cid:68)

yk, (k − 1)rk−1 − krk(cid:69)

+

(cid:13)
(cid:13)

(cid:13)ek−1(cid:13)

(cid:13)
(cid:13)

2

(cid:68)

yk, (k − 1)rk−1 − krk(cid:69)

,

+

(cid:68)

A∗yk, (k − 1)ek−1 − kek(cid:69)
+
ek, ek−1(cid:69)(cid:19)

+

(cid:68)

(cid:68)

yk, (k − 1)rk−1 − krk(cid:69)

−

2k − 1
2

(cid:18)(cid:13)
(cid:13)ek(cid:13)
(cid:13)

2
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)

(cid:13)ek−1(cid:13)

(cid:13)
(cid:13)

2(cid:19)(cid:19)

(cid:68)

yk, (k − 1)rk−1 − krk(cid:69)

+

(52)
where in the last inequality, we use the fact that σk ≥ σk−1 > 0 for all k ≥ 1. By summing
the inequality in (52) for k = 1, . . . , k, we get (recall that (cid:13)

(cid:13) ≤ M for all k ≥ 0)

(cid:13)yk(cid:13)

(cid:68)

C, X k − X (cid:63)(cid:69)

≥

−k

≥

1
2σk

1
2σk

(cid:13)
(cid:13)

(cid:13)ek(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)ek(cid:13)

(cid:13)
(cid:13)

2

2

−

−

1
2σ0

1
2σ0

(cid:107)e0(cid:107)2 +

k
(cid:88)

i=1

(cid:107)e0(cid:107)2 − 2M

(cid:10)yi, (i − 1)ri−1 − iri(cid:11)

k
(cid:88)

i=1

iεi.

The above inequality leads to the following upper bound on (cid:10)C, X k − X (cid:63)(cid:11):

(cid:68)

C, X k − X (cid:63)(cid:69)

≤

(cid:32)

1
k

1
2σ0

2

(cid:13)e0(cid:13)
(cid:13)
(cid:13)

+ 2M

(cid:33)

iεi

.

k
(cid:88)

i=1

(53)

Next, we shall provide a lower bound for (cid:10)C, X k − X (cid:63)(cid:11) for k ≥ 1. To this end, let us deﬁne
the set Fk, which is an enlargement of the primal feasible set FP with respect to εk ≥ 0, as
follows:

Fk := (cid:8)X ∈ Sn (cid:12)

(cid:12) (cid:107)A(X) − b(cid:107) ≤ εk, X ∈ Sn
+

(cid:9) ,

∀ k ≥ 1.

Moreover, denote

Since (y(cid:63), S(cid:63)) ∈ Rm × Sn

k := arg min {(cid:104)C, X(cid:105) | X ∈ Fk} ,

X (cid:63)
+ is a dual optimal solution, by [33, Lemma 3.4], we have that

∀ k ≥ 1.

0 ≤ (cid:104)C, X (cid:63) − X (cid:63)

k (cid:105) ≤ (cid:107)y(cid:63)(cid:107) εk,

∀ k ≥ 1.

As a consequence, because X k ∈ Fk and (cid:10)C, X k(cid:11) ≥ (cid:10)C, X (cid:63)

k

(cid:11), it holds that

(cid:68)

C, X k − X (cid:63)(cid:69)

≥ (cid:104)C, X (cid:63)

k − X (cid:63)(cid:105) ≥ − (cid:107)y(cid:63)(cid:107) εk = −

(cid:107)y(cid:63)(cid:107) kεk
k

.

(54)

STRIDE for Solving Rank-One Semideﬁnite Relaxations

37

Combining (53) and (54), we see that

−

(cid:107)y(cid:63)(cid:107) kεk
k

(cid:68)

C, X k − X (cid:63)(cid:69)

≤

≤

(cid:32)

1
k

1
2σ0

2

(cid:13)e0(cid:13)
(cid:13)
(cid:13)

+ 2M

(cid:33)

iεi

.

k
(cid:88)

i=1

Finally, let us estimate the dual infeasibility. To this end, we get from (50) that

1
σk

(cid:13)
(cid:13)

(cid:13)X k − X k−1(cid:13)

2
(cid:13)
(cid:13)

(cid:68)

C, X k−1 − X (cid:63)(cid:69)

(cid:68)

C, X k − X (cid:63)(cid:69)

−

+

≤

(cid:13)
(cid:13)

(cid:13)yk(cid:13)

(cid:13)
(cid:13)

(cid:16)(cid:13)
(cid:13)

(cid:13)rk−1(cid:13)
(cid:13)
(cid:13) +

which implies that

(cid:13)
(cid:13)A∗yk + Sk − C
(cid:13)

(cid:13)
(cid:13)
(cid:13) =

1
σk

(cid:13)
(cid:13)

(cid:13)X k − X k−1(cid:13)

(cid:13)
(cid:13)

(cid:17)

(cid:13)
(cid:13)

(cid:13)rk(cid:13)

(cid:13)
(cid:13)

1
k − 1

(cid:32)

1
2σ0

(cid:107)e0(cid:107)2 + 2M

k−1
(cid:88)

i=1

(cid:33)

iεi

+ (cid:107)y(cid:63)(cid:107) εk + M (εk−1 + εk)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
√
σk
(cid:18) 1
√

≤

≤ O

(cid:19)

,

kσk

and establishes the convergence of the dual infeasibility. Therefore, the proof is completed. (cid:117)(cid:116)

B Proof for Theorem 5

From Step 1 of the modiﬁed L-BFGS algorithm, we have dk = (−βkI + Qk)∇φ(ξk), with
Qk = 0 if (cid:13)
(cid:13) ≥ K. This leads to (cid:10)∇φ(ξk), dk(cid:11) = (cid:10)∇φ(ξk), −(βkI + Qk)∇φ(ξk)(cid:11), which
implies that

(cid:13)dk(cid:13)

λmin(βkI + Qk) ≤

,

(55)

(cid:10)−∇φ(ξk), dk(cid:11)
(cid:13)∇φ(ξk)(cid:13)
(cid:13)
2
(cid:13)

where λmin(·) denotes the minimum eigenvalue. This implies that dk is a descent direction and
the line search scheme is well-deﬁned.

Let the sequence {ξk} be generated by the algorithm. We next show the second part of the
theorem, i.e., {ξk} is a bounded sequence. Since AA∗ is nonsigular and the Slater condition
holds, by [56, Theorem 17’ & 18’], the level set Lφ := (cid:8)ξ ∈ Rm : φ(ξ) ≤ φ(ξ0)(cid:9) is compact.
Thus, the boundedness of the sequence is proven.

Let us now prove the third part of the theorem, which states that every accumulation point
of {ξk} is an optimal solution solution of problem (28). To this end, without loss of generality,
we assume that {ξk} is an inﬁnite sequence (the case for a ﬁnite sequence is trivial and we
omit it here) and ξk → ˆξ as k → ∞, and we shall prove that ∇φ( ˆξ) = 0. We plan to prove the
latter result by contraction. Let us assume that ∇φ( ˆξ) (cid:54)= 0. Under this assumption, we will
deduce a contradiction in three steps.
Step A. We ﬁrst claim that when ∇φ( ˆξ) (cid:54)= 0, (cid:13)

(cid:13) is bounded from below and above, i.e.,
(cid:13)dk(cid:13)

there exist two positive constants M1 and M2 such that 0 < M1 ≤ (cid:13)
On the one hand, by Step 1 of the L-BFGS Algorithm 5, we know that either dk =
−(βkI + Qk)∇φ(ξk), in which case (cid:13)
(cid:13) ≤ K (otherwise the algorithm rejects it), or
dk = −βk∇φ(ξk), in which case (cid:13)
(cid:13)∇φ(ξk)(cid:13)
(cid:13)
τ2 ). Therefore,
(cid:13)
we have (cid:13)
(cid:13) is upper bounded.
(cid:13) ≤ max
On the other hand, if for some index set J, (cid:13)
(cid:13) → 0 for j ∈ J, then we can deduce that

:= M2 < ∞, and (cid:13)
(cid:13)dkj (cid:13)

(due to βk = τ1
(cid:13)dk(cid:13)

(cid:13)dk(cid:13)
(cid:13) ≤ τ1M 1+τ2
(cid:111)

(cid:13) ≤ M2 for all k.

K, τ1M 1+τ2

(cid:13)dk(cid:13)

(cid:13)dk(cid:13)

(cid:13)dk(cid:13)

(cid:110)

0

0

(cid:13)
(cid:13)
(cid:13)∇φ(ξkj )
(cid:13)
(cid:13)
(cid:13) =

(cid:13)
(cid:13)(βkj I + Qkj )−1dkj
(cid:13)

(cid:13)
(cid:13)
(cid:13) ≤

1
βkj

(cid:13)
(cid:13)dkj
(cid:13)

(cid:13)
(cid:13)
(cid:13) =

1
(cid:13)∇φ(ξkj (cid:13)
(cid:13)
(cid:13)

τ1

(cid:13)
(cid:13)dkj
(cid:13)

(cid:13)
(cid:13)
(cid:13) ,

j ∈ J.

The above inequality implies that
Thus, (cid:13)

(cid:13)
(cid:13)
(cid:13)∇φ(ξkj )(cid:13)
(cid:13)
(cid:13)∇φ( ˆξ)
(cid:13)
(cid:13)
(cid:13) = limj∈J
(cid:13) must also be lower bounded, by some constant M1 > 0.

(cid:13)dk(cid:13)

(cid:13) = 0, a contradiction.

38

Heng Yang et al.

Step B. We next claim that {ρmk } is bounded away from zero.

mkj → 0, j ∈ J (i.e., mkj → ∞ due to ρ ∈ (0, 1)).
Suppose that, for some index set J, ρ
Recall that mkj is the smallest nonnegative integer to satisfy the inequality in the line
search scheme (so that mkj − 1 does not satisfy the inequality). Thus, it holds that

φ(ξkj + ρ

mkj

−1

dkj ) > φ(ξkj ) + µρ

mkj

−1 (cid:68)

∇φ(ξkj ), dkj

(cid:69)

.

(56)

Meanwhile, we can expand φ(ξkj + ρ

mkj

−1

dkj ) as

φ(ξkj + ρ

mkj

−1

dkj ) = φ(ξkj ) +

(cid:90) 1

(cid:68)

0

∇φ(ξkj + tρ

mkj

−1

dkj ), ρ

mkj

−1

dkj

(cid:69)

dt.

(57)

By substituting (57) into (56), we obtain the following inequality:

(cid:90) 1

(cid:68)

0

∇φ(ξkj + tρ

mkj

−1

dkj ), dkj

(cid:69)

dt > µ

(cid:68)

∇φ(ξkj ), dkj

(cid:69)

,

j ∈ J.

(58)

Since the sequence {dkj } is bounded from both below and above, by taking a subset of J
if necessary, we assume that there exists ˆd such that dkj → ˆd, j ∈ J. Taking limit with
(cid:69)
∇φ( ˆξ), ˆd
, which further implies
respect to j ∈ J in (58) yields that

≥ µ

(cid:68)

(cid:68)

(cid:68)

that
(cid:68)

(cid:69)
∇φ( ˆξ), ˆd
(cid:69)
∇φ( ˆξ), ˆd

≥ 0 by the fact that µ ∈ (0, 1/2). Also, since

≤ 0, we derive
= 0. However, denoting ˆβ as the limit of βkj as kj → ∞, j ∈ J, and noting

(cid:69)
∇φ( ˆξ), ˆd
(cid:68)

(cid:69)
∇φ( ˆξ), ˆd

that Qk (cid:23) 0 for all k, the inequality in (55) shows that

0 = −

(cid:68)

(cid:69)
∇φ( ˆξ), ˆd

≥ ˆβ

(cid:13)
(cid:13)
(cid:13)∇φ( ˆξ)
(cid:13)
(cid:13)
(cid:13)

2

≥ τ1

(cid:13)
(cid:13)
(cid:13)∇φ( ˆξ)
(cid:13)
(cid:13)
(cid:13)

2+τ2

,

which implies that ∇φ( ˆξ) = 0. This is also a contradiction. Therefore, {ρmk } is bounded
away from zero.

Step C. Now, using the fact that φ is bounded from below on the bounded sequence {ξk}, we
have that {φ(ξk+1) − φ(ξk)} → 0 (since the sequence {φ(ξk)} is nonincreasing and φ(·)
is continuous). This implies, by the line search rule again, that ρmk (cid:10)∇φ(ξk), dk(cid:11) → 0,
= 0, since {ρmk } is bounded away from
as k → ∞. Therefore, it holds that
zero, which again implies that ∇φ( ˆξ) = 0, and this contradicts our initial hypothesis that
∇φ( ˆξ) (cid:54)= 0.
At this point, we have shown that ∇φ( ˆξ) = 0, as desired. At last, by the convexity of φ(·),
vanishing of the gradient at ˆξ implies that ˆξ is an optimal solution of problem (28). Therefore,
(cid:117)(cid:116)
the proof is completed.

(cid:69)
∇φ( ˆξ), ˆd

(cid:68)

References

1. Abbe, E., Bandeira, A.S., Hall, G.: Exact recovery in the stochastic block model. IEEE

Transactions on Information Theory 62(1), 471–487 (2015) 3

2. Absil, P.A., Mahony, R., Sepulchre, R.: Optimization algorithms on matrix manifolds.

Princeton University Press (2009) 13

3. Aholt, C., Agarwal, S., Thomas, R.: A QCQP approach to triangulation. In: European

Conference on Computer Vision, pp. 654–667. Springer (2012) 32

4. Alizadeh, F., Haeberly, J.P.A., Overton, M.L.: Complementarity and nondegeneracy in

semideﬁnite programming. Mathematical programming 77(1), 111–128 (1997) 4

5. Alizadeh, F., Haeberly, J.P.A., Overton, M.L.: Primal-dual interior-point methods for
semideﬁnite programming: convergence rates, stability and numerical results. SIAM Jour-
nal on Optimization 8(3), 746–768 (1998) 4

6. Antonante, P., Tzoumas, V., Yang, H., Carlone, L.: Outlier-robust estimation: Hardness,

minimally-tuned algorithms, and applications. IEEE Trans. Robotics (2021) 28

STRIDE for Solving Rank-One Semideﬁnite Relaxations

39

7. ApS, M.: The MOSEK optimization toolbox for MATLAB manual. Version 9.0. (2019).

URL http://docs.mosek.com/9.0/toolbox/index.html 4, 22, 25, 27, 31, 34

8. Barak, B., Brandao, F.G., Harrow, A.W., Kelner, J., Steurer, D., Zhou, Y.: Hypercontrac-
tivity, sum-of-squares proofs, and their applications. In: Proceedings of the forty-fourth
annual ACM symposium on Theory of computing, pp. 307–326 (2012) 26

9. Bay, H., Tuytelaars, T., Van Gool, L.: Surf: Speeded up robust features. In: European

conference on computer vision, pp. 404–417. Springer (2006) 32

10. Beck, A., Teboulle, M.: A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM journal on imaging sciences 2(1), 183–202 (2009) 17

11. Bertsekas, D.: Nonlinear Programming. Athena Scientiﬁc (1999) 7
12. Blekherman, G., Parrilo, P.A., Thomas, R.R.: Semideﬁnite optimization and convex alge-

braic geometry. SIAM (2012) 6

13. Boumal, N., Mishra, B., Absil, P.A., Sepulchre, R.: Manopt, a Matlab toolbox for opti-
mization on manifolds. Journal of Machine Learning Research 15(42), 1455–1459 (2014).
URL https://www.manopt.org 13, 26

14. Boumal, N., Voroninski, V., Bandeira, A.S.: The non-convex Burer-Monteiro approach
works on smooth semideﬁnite programs. In: Conference on Neural Information Processing
Systems (NeurIPS) (2016) 5

15. Briales, J., Gonzalez-Jimenez, J.: Convex global 3d registration with lagrangian duality.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 4960–4969 (2017) 3, 11

16. Burer, S., Monteiro, R.D.: A nonlinear programming algorithm for solving semideﬁnite
programs via low-rank factorization. Mathematical Programming 95(2), 329–357 (2003)
4

17. Burer, S., Ye, Y.: Exact semideﬁnite formulations for a class of (random and non-random)

nonconvex quadratic programs. Mathematical Programming pp. 1–17 (2019) 3

18. Candes, E.J., Eldar, Y.C., Strohmer, T., Voroninski, V.: Phase retrieval via matrix com-

pletion. SIAM review 57(2), 225–251 (2015) 3

19. Chen, L., Sun, D., Toh, K.: An eﬃcient inexact symmetric gauss-seidel based majorized
ADMM for high-dimensional convex composite conic programming. Mathematical Pro-
gramming 161, 237–270 (2017) 14

20. Cifuentes, D.: A convex relaxation to compute the nearest structured rank deﬁcient matrix.
SIAM Journal on Matrix Analysis and Applications 42(2), 708–729 (2021) 4, 32, 33
21. Cifuentes, D., Agarwal, S., Parrilo, P.A., Thomas, R.R.: On the local stability of semidef-

inite relaxations. Mathematical Programming (2021) 3, 32

22. Combettes, P.L., Pesquet, J.C.: Proximal splitting methods in signal processing. In: Fixed-
point algorithms for inverse problems in science and engineering, pp. 185–212. Springer
(2011) 15

23. Dai, Y.H., Yuan, Y.: A nonlinear conjugate gradient method with a strong global conver-

gence property. SIAM Journal on optimization 10(1), 177–182 (1999) 20

24. De Klerk, E.: The complexity of optimizing over a simplex, hypercube or sphere: a short
survey. Central European Journal of Operations Research 16(2), 111–125 (2008) 26
25. Doherty, A.C., Parrilo, P.A., Spedalieri, F.M.: Complete family of separability criteria.

Physical Review A 69(2), 022308 (2004) 4, 26

26. Fang, K., Fawzi, H.: The sum-of-squares hierarchy on the sphere and applications in quan-

tum information theory. Mathematical Programming pp. 1–30 (2020) 26

27. Fukuda, M., Kojima, M., Murota, K., Nakata, K.: Exploiting sparsity in semideﬁnite
programming via matrix completion i: General framework. SIAM Journal on optimization
11(3), 647–674 (2001) 4

28. Gamarnik, D., Kızıldağ, E.C.: Algorithmic obstructions in the random number partitioning

problem. arXiv preprint arXiv:2103.01369 (2021) 23

29. Goemans, M.X., Williamson, D.P.: Improved approximation algorithms for maximum cut
and satisﬁability problems using semideﬁnite programming. Journal of the ACM (JACM)
42(6), 1115–1145 (1995) 3, 11, 23

30. Helmberg, C., Rendl, F., Weismantel, R.: A semideﬁnite programming approach to the
quadratic knapsack problem. Journal of combinatorial optimization 4(2), 197–215 (2000)
23

31. Henrion, D., Lasserre, J.B.: GloptiPoly: Global optimization over polynomials with Matlab
and SeDuMi. ACM Transactions on Mathematical Software (TOMS) 29(2), 165–194
(2003) 4

40

Heng Yang et al.

32. Inc., W.R.: Mathematica, Version 12.2. URL https://www.wolfram.com/mathematica.

Champaign, IL, 2020 6

33. Jiang, K., Sun, D., Toh, K.C.: An inexact accelerated proximal gradient method for large
scale linearly constrained convex sdp. SIAM Journal on Optimization 22(3), 1042–1064
(2012) 8, 10, 13, 36

34. Josz, C., Henrion, D.: Strong duality in Lasserre’s hierarchy for polynomial optimization.

Optimization Letters 10(1), 3–10 (2016) 3

35. Kaltofen, E., Yang, Z., Zhi, L.: Approximate greatest common divisors of several polyno-
mials with linearly constrained coeﬃcients and singular polynomials. In: Proceedings of
the 2006 international symposium on Symbolic and algebraic computation, pp. 169–176
(2006) 32

36. Lasserre, J.B.: An explicit exact SDP relaxation for nonlinear 0-1 programs. In: Interna-
tional Conference on Integer Programming and Combinatorial Optimization, pp. 293–303.
Springer (2001) 4, 23

37. Lasserre, J.B.: Global optimization with polynomials and the problem of moments. SIAM

Journal on Optimization 11(3), 796–817 (2001) 2, 3

38. Lasserre, J.B.: Moments, positive polynomials and their applications, vol. 1. World Scien-

tiﬁc (2009) 2

39. Laurent, M.: Semideﬁnite representations for ﬁnite varieties. Mathematical programming

109(1), 1–26 (2007) 3

40. Li, X., Sun, D., Toh, K.C.: A block symmetric Gauss–Seidel decomposition theorem for
convex composite quadratic programming and its applications. Mathematical Program-
ming 175(1), 395–418 (2019) 17, 18, 19

41. Ling, C., Nie, J., Qi, L., Ye, Y.: Biquadratic optimization over unit spheres and semideﬁnite
programming relaxations. SIAM Journal on Optimization 20(3), 1286–1310 (2010) 26
42. Luo, Z.Q., Ma, W.K., So, A.M.C., Ye, Y., Zhang, S.: Semideﬁnite relaxation of quadratic

optimization problems. IEEE Signal Processing Magazine 27(3), 20–34 (2010) 3

43. Mai, N.H.A., Magron, V., Lasserre, J.B.: A hierarchy of spectral relaxations for polynomial

optimization. arXiv preprint arXiv:2007.09027 (2020) 23

44. Malick, J., Povh, J., Rendl, F., Wiegele, A.: Regularization methods for semideﬁnite pro-

gramming. SIAM Journal on Optimization 20(1), 336–356 (2009) 20

45. Malick, J., Sendov, H.S.: Clarke generalized Jacobian of the projection onto the cone of

positive semideﬁnite matrices. Set-Valued Analysis 14(3), 273–293 (2006) 20

46. Markovsky, I.: Structured low-rank approximation and its applications. Automatica 44(4),

891–909 (2008) 32

47. Markovsky, I., Usevich, K.: Software for weighted structured low-rank approximation.

Journal of Computational and Applied Mathematics 256, 278–292 (2014) 32

48. Markovsky, I., Van Huﬀel, S.: Overview of total least-squares methods. Signal processing

87(10), 2283–2302 (2007) 32

49. Meneghetti, G., Danelljan, M., Felsberg, M., Nordberg, K.: Image alignment for panorama
In: Scandinavian Conference on Image

stitching in sparsely structured environments.
Analysis, pp. 428–439. Springer (2015) 30, 32

50. Mertens, S.: Number partitioning. Computational Complexity and Statistical Physics p.

125 (2006) 23

51. Nesterov, Y.: Lectures on convex optimization, vol. 137. Springer (2018) 20
52. Nie, J.: Polynomial optimization with real varieties. SIAM Journal On Optimization 23(3),

1634–1646 (2013) 3

53. Nie, J.: Optimality conditions and ﬁnite convergence of Lasserre’s hierarchy. Mathematical

programming 146(1), 97–121 (2014) 3, 4

54. Nocedal, J., Wright, S.: Numerical Optimization. Springer Science & Business Media

(2006) 6, 13, 20

55. Parrilo, P.A.: Semideﬁnite programming relaxations for semialgebraic problems. Mathe-

matical programming 96(2), 293–320 (2003) 2, 3

56. Rockafellar, R.T.: Conjugate duality and optimization. SIAM (1974) 37
57. Rosen, D.M.: Scalable low-rank semideﬁnite programming for certiﬁably correct machine
In: Intl. Workshop on the Algorithmic Foundations of Robotics (WAFR),

perception.
vol. 3 (2020) 5

58. Rosen, D.M., Carlone, L., Bandeira, A.S., Leonard, J.J.: SE-Sync: A certiﬁably correct
algorithm for synchronization over the special Euclidean group. The International Journal
of Robotics Research 38(2-3), 95–125 (2019) 3, 5

STRIDE for Solving Rank-One Semideﬁnite Relaxations

41

59. Rosen, J.B., Park, H., Glick, J.: Total least norm formulation and solution for structured

problems. SIAM Journal on matrix analysis and applications 17(1), 110–126 (1996) 32

60. Rusu, R., Blodow, N., Beetz, M.: Fast point feature histograms (fpfh) for 3d registration.
In: IEEE Intl. Conf. on Robotics and Automation (ICRA), pp. 3212–3217. Citeseer (2009)
32

61. Shi, J., Yang, H., Carlone, L.: Optimal pose and shape estimation for category-level 3d

object perception. In: Robotics: Science and Systems (RSS) (2021) 3

62. Shi, J., Yang, H., Carlone, L.: ROBIN: a graph-theoretic approach to reject outliers in
In: IEEE Intl. Conf. on Robotics and Automation

robust estimation using invariants.
(ICRA) (2021) 32

63. Shor, N.Z.: Dual quadratic estimates in polynomial and boolean programming. Annals of

Operations Research 25(1), 163–168 (1990) 3

64. Sun, D., Toh, K.C., Yang, L.: A convergent 3-block semiproximal alternating direction
method of multipliers for conic programming with 4-type constraints. SIAM journal on
Optimization 25(2), 882–915 (2015) 14

65. Toh, K.C.: Solving large scale semideﬁnite programs via an iterative solver on the aug-

mented systems. SIAM Journal on Optimization 14(3), 670–698 (2004) 24

66. Toh, K.C., Todd, M.J., Tütüncü, R.H.: SDPT3—a MATLAB software package for semidef-
inite programming, version 1.3. Optimization methods and software 11(1-4), 545–581
(1999) 3, 4, 22, 25, 27, 31, 34

67. Waki, H., Kim, S., Kojima, M., Muramatsu, M.: Sums of squares and semideﬁnite program-
ming relaxations for polynomial optimization problems with structured sparsity. SIAM J.
Optimization 17, 218–242 (2006) 3

68. Wang, A.L., Kılınç-Karzan, F.: On the tightness of SDP relaxations of QCQPs. Mathe-

matical Programming pp. 1–41 (2021) 3

69. Wang, J., Magron, V., Lasserre, J.B.: Chordal-TSSOS: a moment-SOS hierarchy that
exploits term sparsity with chordal extension. SIAM Journal on Optimization 31(1),
114–141 (2021) 3

70. Wang, J., Magron, V., Lasserre, J.B.: TSSOS: A Moment-SOS hierarchy that exploits

term sparsity. SIAM Journal on Optimization 31(1), 30–58 (2021) 3

71. Wu, G., Sun, J., Chen, J.: Optimal linear quadratic regulator of switched systems. IEEE

transactions on automatic control 64(7), 2898–2904 (2018) 23

72. Yang, H., Antonante, P., Tzoumas, V., Carlone, L.: Graduated non-convexity for robust
spatial perception: From non-minimal solvers to global outlier rejection. IEEE Robotics
and Automation Letters (RA-L) 5(2), 1127–1134 (2020) 30, 31

73. Yang, H., Carlone, L.: A polynomial-time solution for robust registration with extreme

outlier rates. In: Robotics: Science and Systems (RSS) (2019) 28

74. Yang, H., Carlone, L.: A quaternion-based certiﬁably optimal solution to the Wahba prob-
lem with outliers. In: Proceedings of the IEEE/CVF International Conference on Com-
puter Vision, pp. 1665–1674 (2019) 4, 11, 28, 29

75. Yang, H., Carlone, L.: In perfect shape: Certiﬁably optimal 3d shape reconstruction from
2d landmarks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 621–630 (2020) 4

76. Yang, H., Carlone, L.: One ring to rule them all: Certiﬁably robust geometric perception
with outliers. In: Conference on Neural Information Processing Systems (NeurIPS) (2020)
4, 11, 29

77. Yang, H., Shi, J., Carlone, L.: Teaser: Fast and certiﬁable point cloud registration. IEEE

Transactions on Robotics (2020) 28

78. Yang, L., Sun, D., Toh, K.C.: SDPNAL+: a majorized semismooth Newton-CG augmented
Lagrangian method for semideﬁnite programming with nonnegative constraints. Mathe-
matical Programming Computation 7(3), 331–366 (2015) 4, 14, 22, 25, 27, 31, 34

79. Yurtsever, A., Tropp, J.A., Fercoq, O., Udell, M., Cevher, V.: Scalable semideﬁnite pro-
gramming. SIAM Journal on Mathematics of Data Science 3(1), 171–200 (2021) 4, 22,
25, 27, 31, 34

80. Zeng, A., Song, S., Nießner, M., Fisher, M., Xiao, J., Funkhouser, T.: 3dmatch: Learning
the matching of local 3d geometry in range scans. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, vol. 1, p. 4 (2017) 32

81. Zhang, R.Y., Lavaei, J.: Sparse semideﬁnite programs with guaranteed near-linear time
complexity via dualized clique tree conversion. Mathematical programming 188(1), 351–
393 (2021) 4

42

Heng Yang et al.

82. Zhao, X.Y., Sun, D., Toh, K.C.: A Newton-CG augmented Lagrangian method for semidef-

inite programming. SIAM Journal on Optimization 20(4), 1737–1765 (2010) 4, 20

83. Zheng, Y., Fantuzzi, G., Papachristodoulou, A., Goulart, P., Wynn, A.: Chordal decom-
position in operator-splitting methods for sparse semideﬁnite programs. Mathematical
Programming 180(1), 489–532 (2020) 4, 22, 25, 27, 31, 34

