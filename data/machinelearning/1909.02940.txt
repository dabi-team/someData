1
2
0
2

r
a

M
9
1

]

G
L
.
s
c
[

3
v
0
4
9
2
0
.
9
0
9
1
:
v
i
X
r
a

Reinforcement Learning for Joint Optimization of Multiple
Rewards

Mridul Agarwal and Vaneet Aggarwal
Purdue University, West Lafayette IN 47907

Abstract

Reinforcement Learning (RL) algorithms such as DQN owe their success to Markov Decision
Processes, and the fact that maximizing the sum of rewards allows using backward induction
and reduce to the Bellman optimality equation. However, many real-world problems require
optimization of an objective that is non-linear in cumulative rewards for which dynamic
programming cannot be applied directly. For example, in a resource allocation problem,
one of the objectives is to maximize long-term fairness among the users. We notice that
when the function of the sum of rewards is considered, the problem loses its Markov nature.
This paper addresses and formalizes the problem of optimizing a non-linear function of the
long term average of rewards. We propose model-based and model-free algorithms to learn
the policy, where the model-based policy is shown to achieve a regret of ˜O
for K users. Further, using the fairness in cellular base-station scheduling, and queueing
(cid:17)
system scheduling as examples, the proposed algorithm is shown to signiﬁcantly outperform
the conventional RL approaches.

KDSA

q

A
T

(cid:16)

1. Introduction

Many practical applications of sequential decision making often have multiple objectives.
For example, a hydro-power project may have conﬂicting gains with respect to power genera-
tion and ﬂood management (Castelletti et al., 2013). Similarly, a building climate controller
can have conﬂicting objectives of saving energy and maximizing comfort of the residents
of the building (Kwak et al., 2012). Video streaming applications also account for multiple
objectives like stall duration and average video quality (Elgabli et al., 2018). Access of ﬁles
from cloud storage aims to optimize the latency of ﬁle download and the cost to store the
ﬁles (Xiang et al., 2015). We call each objective as an agent, thus making the analogy to
a multi-agent environment. We note that the diﬀerent agents are not necessarily diﬀerent
users, but diﬀerent objectives of the problem, and thus our framework can be used in both
single-user as well as multi-user environments. The rewards generated by each objective
(agent) may be interlinked. This paper aims to provide a novel formulation for joint decision
making among multiple agents using reinforcement learning approaches and to analyze the
performance of the proposed algorithms.

We consider a setup where we want to optimize a possibly nonlinear joint objective
function of long-term rewards of all the agents (or, diﬀerent objectives). As an example,
many problems in resource allocation for networking and computation resources use fairness
among the long-term average rewards of the users as the metric of choice (Lan et al., 2010;
Kwan et al., 2009; Bu et al., 2006; Li et al., 2018; Aggarwal et al., 2011; Margolies et al.,

1

 
 
 
 
 
 
2016; Wang et al., 2014; Ibrahim et al., 2010), which is a non-linear metric. For fairness
optimization, a controller wants to optimize a fairness objective among the diﬀerent agents,
e.g., proportional fairness, α-fairness, or improve the worst-case average reward of the users
(Altman et al., 2008).
In such situations, the overall joint objective function cannot be
written as sum utility at each time instant. This prohibits the application of standard
single-agent reinforcement learning based policies as the backward induction step update
cannot be directly applied here. For example, if a process has 2 agents and T > 1 steps,
1 steps. Then, at T th step the
and all the resource was allocated to the ﬁrst agent till T
resource should be allocated to the second agent to ensure fairness. This requires the need
to track past allocation of all the resources and not just the current state of the system. We
also note that the optimal policy cannot take a deterministic action in a state in general,
and thus the optimal policy is not a deterministic policy in general. Consider a case where a
scheduler needs to fairly allocate a resource between two users, and the system has only one
state. A deterministic policy policy will allocate the resource to only one of the user, and
hence is not optimal. We deﬁne a novel multi-agent formulation, making several practical
assumptions, which optimizes the joint function of the average per-step rewards of the
diﬀerent agents to alleviate the need for maintaining history.

−

SARSA and Q-Learning algorithms (Sutton and Barto, 2018), and their deep neural
network based DQN algorithm (Mnih et al., 2015) provide policies that depend only on the
current state, hence are sub-optimal. Further, these algorithms learn a Q-value function
which can be computed based on a dynamic programming approach, which is not valid in
our work. Using evaluations on fair resource allocation and network routing problems, we
verify that algorithms based on ﬁnding ﬁxed point of Bellman equations do not perform
well. This further motivates the study to identify novel RL based algorithms to optimize
non-linear functions.

We further note that even though multi-agent reinforcement learning algorithms have
been widely studied, (Tan, 1993; Shoham et al., 2003; Bu¸soniu et al., 2010; Ono and Fukumoto,
1996), there are no convergence proofs to the optimal joint objective function without the
knowledge of the transition probability, to the best of our knowledge. This paper assumes
no knowledge of the state transition probability of the agents and aims to provide algo-
rithms for the decision making of the diﬀerent agents. We provide two algorithms; The ﬁrst
is a model-based algorithm that learns the transition probability of the next state given the
current state and action. The second algorithm is model-free, which uses policy gradients
to ﬁnd the optimal policy.

The proposed model-based algorithm uses posterior sampling with Dirichlet distribution.
We show that the proposed algorithm converges to an optimal point when the joint objective
function is Lipschitz continuous. Since the optimal policy is a stochastic policy, policy search
space is not-ﬁnite. We show that the problem is convex under a certain class of functions
and can be eﬃciently solved. In setups where the joint objective function is max-min, the
setup reduces to a linear programming optimization problem. In addition, we show that
the proposed algorithm achieves a regret bound sub-linear in the number of time-steps and
number of agents. This regret bound characterizes the gap between the optimal objective
and the objective achieved by the algorithm in T time-steps. We show a regret bound of
˜O

, where K, T denotes the number of agents, and time steps, respectively.

KDSA

A
T

(cid:18)

q

(cid:19)

2

The proposed model-free algorithm can be easily implemented using deep neural net-
works for any diﬀerentiable objective function. Further, we note that the reward functions of
the diﬀerent agents can be very diﬀerent, and can optimize diﬀerent metrics for the agents.
As long as there is a joint objective function, the diﬀerent agents can make decisions to
optimize this function and achieve the optimal decision at convergence.

The proposed framework works for any number of agents, while is novel even for a single
agent (K = 1). In this case, the agent wishes to optimize a non-linear concave function
of the average reward. Since this function is not assumed to be monotone, optimizing the
function is not equivalent to optimizing the average reward. For any general non-linear
concave function, regret bound is analyzed for model-based case.

We also present evaluation results for both the algorithms for optimizing proportional
fairness of multiple agents connecting to a cellular base station. We compare the obtained
policies with existing asymptotically optimal algorithm of optimizing proportional fairness
for wireless networks (Margolies et al., 2016) and SARSA based RL solution proposed by
(Perez et al., 2009). We developed a simulation environment for wireless network for mul-
tiple number of agents and states for each agent. The proposed algorithm signiﬁcantly
outperform the SARSA based solution, and achieves close-enough performance as BGE.
We also considered α-fairness for an inﬁnite state space to show the scalability of the pro-
posed model-free algorithm. In this case, the domain-speciﬁc algorithm was not available,
while we show that we outperform Deep Q-Network (DQN) based algorithm (Mnih et al.,
2015). Finally, a queueing system is considered which models multiple roads merging into a
single lane. The queue selection problem is modeled using the proposed framework and the
proposed approach is shown to improve the fair latency reward metric among the queues
signiﬁcantly as compared to the DQN and the longest-queue-ﬁrst policies.

Key contributions of our paper are:

• A structure for joint function optimization with multiple agents based on average per-step

rewards.

• Pareto Optimality guarantees when the joint objective is an element-wise monotone func-

tion.

• A model-based algorithm using posterior sampling with Dirichlet distribution, and its

regret bounds.

• A model-free policy gradient algorithm which can be eﬃciently implemented using neural

networks.

• Evaluation results and comparison with existing heuristics for optimizing fairness in cel-

lular networks, and queueing systems.

The rest of the paper is organized as follows. Section 2 describes related works in the
ﬁeld of RL and MARL. Section 3 describes the problem formulation. Pareto optimality
of the proposed framework is shown in Section 4. The proposed model based algorithm
and model free algorithm are described in Sections 5 and 6, respectively. In Section 7, the
proposed algorithms are evaluated for cellular scheduling problem. Section 8 concludes the
paper with some future work directions.

3

2. Related Work

Reinforcement learning for single agent has been extensively studied in past (Sutton and Barto,
2018). Dynamic Programming was used in many problems by ﬁnding cost to go at each
stage (Puterman, 1994; Bertsekas, 1995). These models optimize linear additive utility and
utilize the power of Backward Induction.

Following the success of Deep Q Networks (Mnih et al., 2015), many new algorithms
have been developed for reinforcement learning (Schulman et al., 2015; Lillicrap et al., 2015;
Wang et al., 2015; Schulman et al., 2017). These papers focus on single agent control, and
provide a framework for implementing scalable algorithms. Sample eﬃcient algorithms
based on rate of convergence analysis have also been studied for model based RL algorithms
(Agrawal and Jia, 2017; Osband et al., 2013), and for model free Q learning (Jin et al.,
2018). However, sample eﬃcient algorithms use tabular implementation instead of a deep
learning based implementation.

Owing to high variance in the policies obtained by standard Markov Decision Processes
and Reinforcement Learning formulations, various authors worked in reducing the risk in RL
approaches (Garcıa and Fern´andez, 2015). Even though the risk function (e.g., Conditional
Value at Risk (CVaR)) is non-linear in the rewards, this function is not only a function
of long-term average rewards of the single agent but also a function of the higher order
moments of the rewards of the single agent. Thus, the proposed framework does not apply
to the risk measures. However, for both the risk measure and general non-linear concave
function of average rewards, optimal policy is non-Markovian.

Non-Markovian Decision Processes is a class of decision processes where either rewards,
the next state transitions, or both do not only depends on the current state and actions but
also on the history of states and actions leading towards the current state. One can augment
the state space to include the history along with the current state and make the new process
Markovian (Thi´ebaux et al., 2006). However, this increases the memory footprint of any
Q-learning algorithm. (McCallum, 1995) considers only H states of history to construct
an approximate MDP and then use Q-learning. (Li et al., 2006) provide guarantees on Q-
learning for non-MDPs where an agent observes and work according to an abstract MDP
instead of the ground MDP. The states of the abstract MDP are an abstraction of the
states of the ground MDP. (Hutter, 2014) extend this setup to work with abstractions of
histories. (Majeed and Hutter, 2018) consider a setup for History-based Decision Process
(HDP). They provide convergence guarantees for Q-learning algorithm for a sub-class of
HDP where for histories h and h′, Q(h, a) = Q(h′, a) if the last observed state is identical
for both h and h′. They call this sub-class Q-value uniform Decision Process (QDP) and
this subsumes the abstract MDPs. We note that our work is diﬀerent from these as the
Q-values constructed using joint objective is not independent of history.

In most applications such as ﬁnancial markets, swarm robotics, wireless channel access,
etc., there are multiple agents that make a decision (Bloembergen et al., 2015), and the
decision of any agent can possibly aﬀect the other agents. In early work on multi-agent
reinforcement learning (MARL) for stochastic games (Littman, 1994), it was recognized
that no agent works in a vacuum. In his seminal paper, Littman (Littman, 1994) focused
on only two agents that had opposite and opposing goals. This means that they could use
a single reward function which one tried to maximize and the other tried to minimize. The

4

agent had to work with a competing agent and had to behave to maximize their reward in the
worst possible case. In MARL, the agents select actions simultaneously at the current state
and receive rewards at the next state. Diﬀerent from the frameworks that solve for a Nash
equilibrium in a stochastic game, the goal of a reinforcement learning algorithm is to learn
equilibrium strategies through interaction with the environment (Tan, 1993; Shoham et al.,
2003; Bu¸soniu et al., 2010; Ono and Fukumoto, 1996; Shalev-Shwartz et al., 2016).

(Roijers et al., 2013; Liu et al., 2014; Nguyen et al., 2020) considers the multi-objective
Markov Decision Processes. Similar to our work, they consider function of expected cu-
mulative rewards. However, they work with linear combination of the cumulative rewards
whereas we consider a possibly non-linear function f . Further, based on the joint objective
as a function of expected average rewards, we provide regret guarantees for our algorithm.
For joint decision making, (Zhang and Shah, 2014, 2015) studied the problem of fairness
with multiple agents and related the fairness to multi-objective MDP. They considered
maximin fairness and used Linear Programming to obtain optimal policies. For general
functions, linear programming based approach provided by (Zhang and Shah, 2014) will
not directly work. This paper also optimizes joint action of agents using a centralized
controller and propose a model based algorithm to obtain optimal policies. Based on our
assumptions, maximin fairness becomes a special case of our formulation and optimal poli-
cies can be obtained using the proposed model based algorithm. We also propose a model
free reinforcement learning algorithm that can be used to obtain optimal policies for any
general diﬀerentiable functions of average per-step rewards of individual agents. Recently,
(Jiang and Lu, 2019) considered the problem of maximizing fairness among multiple agents.
However, they do not provide a convergence analysis for their algorithm. We attempt to
close these gap in the understanding of the problem of maximizing a non-linear monotone
increasing functions, such as fairness, of multiple objectives with our work.

3. Problem Formulation

,

A

· · ·

S
(cid:0)

, rK, ρ0

, P, r1, r2,
S
denotes a ﬁnite set of A actions. P :

We consider an inﬁnite horizon discounted Markov decision process (MDP)
.
the tuple

deﬁned by
denotes a ﬁnite set of state space of size S, and
[0, 1]S denotes the probability transition
A
is the set of K agents.
distribution. K denotes the number agents and [K] =
Many problems in practice require explicit reward shaping. Hence, the controller or the
learner is aware of the bounds on the rewards. We consider the bounds to be [0, 1] for our
case which is easy to satisfy by reward shaping. Hence we let rk :
[0, 1] denote the
reward generated by agent k

[0, 1] is the distribution of initial state.

S × A →

S × A →

1, 2,
{

, K

M

· · ·

[K]. ρ0 :

}

(cid:1)

∈

S →

We use a joint stochastic policy π :

selecting action a
and expected per step reward of the agent k are given by J k

for any given state s

∈ A

∈ S

S × A →

[0, 1] which returns the probability of
. The expected discounted long term reward
π, respectively, when the

π and λk

5

joint policy π is followed. Formally, for discount factor γ

∈

τ

(0, 1), J k

π and λk

π are deﬁned as

π = Es0,a0,s1,a1,···
J k

ρ0(s0), at ∼
s0 ∼
π = Es0,a0,s1,a1,···
λk

"
γ)J k
π

= lim
γ→1

(1

−

lim
τ →∞

"
π(at|
lim
τ →∞

γtrk(st, at)
#
P (st+1|

rk (st, at)

#

t=0
X
st), st+1 ∼
1
τ

τ

Xt=0

st, at)

(1)

(2)

(3)

(4)

Equation (4) follows from the Laurent series expansion of J k
π (Puterman, 1994). For brevity,
in the rest of the paper Est,at,st+1;t≥0[
] will be denoted as Eρ,π,P [
ρ0(s0), at ∼
], where s0 ∼
·
·
P (st+1|
π(st|
with transition probabil-
a∈A π(a
s, a). After deﬁning a policy, we can now deﬁne the diameter
ities Ps,s′ =
|
as:
of the MDP

at), st+1 ∼
Note that each policy induces a Markov Chain on the states

s)P (s′
|

st, at).

S

M
P

. Let T (s′

Deﬁnition 1 (Diameter) Consider the Markov Chain induced by the policy π on the
MDP
, π, s) be a random variable that denotes the ﬁrst time step when
this Markov Chain enters state s′ starting from state s. Then, the diameter of the MDP
is deﬁned as:

|M

M

M

D(

) = max
s′6=s

M

E

T (s′

min
π

, π, s)

|M

(5)

has a ﬁnite diameter D which means that there is a policy,

We assume that the MDP
such that following that policy all s

M

R,
The agents aim to collaboratively optimize the joint objective function f : RK :
which is deﬁned over the long-term rewards of the individual agents. We make certain
practical assumptions on this joint objective function f , which are listed as follows:

→

∈ S

communicate with each other.

(cid:2)

(cid:3)

Assumption 1 The objective function f is jointly concave. Hence for any arbitrary distri-
bution

, the following holds.

D

f (Ex∼D [x])

≥

Ex∼D [f (x)] ; x

RK

∈

(6)

The objective function f represents the utility obtained from the expected per step reward
of each agent. These utility functions are often concave to reduce variance when the agents
are risk averse (Pratt, 1964). To model this concave utility function, we assume the above
form of Jensen’s inequality. For optimizing risk, negative variance can be maximized. The
negative variance will satisfy our concavity assumption and hence is a special case of our
formulation.

Assumption 2 The function f is assumed to be a L

Lipschitz function, or

f (x)

|

f (y)

L

x

k

−

y

| ≤

−

6

−
k2 ; x, y

RK

∈

(7)

Assumption 2 ensures that for a small change in long run rewards of any agent does not
cause drastic changes in the objective.

Based on Assumption 1, we maximize the function of expected sum of rewards of agents.
Further to keep the formulation independent of time horizon or γ, we maximize the function
over expected per-step rewards of each agent. Hence, our goal is to ﬁnd the optimal policy
as the solution for the following optimization problem.

π∗ = arg max

π

f (λ1
π,

· · ·

, λK
π )

(8)

If f (
) is also monotone, we note that the optimal policy in (8) can be shown to be Pareto
·
optimal. The detailed proof is omitted due to lack of space.
Any online algorithm A starting with no prior knowledge will require to obtain estimates
of transition probabilities P and obtain rewards rk,
[K] for each state action pair.
Initially, when algorithm A does not have good estimates of the model, it accumulates
a regret for not working as per optimal policy. We deﬁne a time dependent regret RT
to achieve an optimal solution deﬁned as the diﬀerence between the optimal value of the
function and the value of the function at time T , or

∈

∀

k

RT = Est,at

"
(cid:12)
(cid:12)
(cid:12)

f

λ1
π∗,

, λK
π∗

· · ·

f

−

(cid:0)

(cid:1)

1
T

T

t=0
X

r1(st, at),

,

1
T

· · ·

T

t=0
X

rK(st, at)

!

(9)

#
(cid:12)
(cid:12)
(cid:12)

The regret deﬁned in Equation (9) is the expected deviation between the value of the
function obtained from the expected rewards of the optimal policy and the value of the
function obtained from the observed rewards from a trajectory. Following the work of
(Roijers et al., 2013), we note that the outer expectation comes for running the decision
process for a diﬀerent set of users or running a separate and independent instance for
the same set of users. Since the realization can be diﬀerent from the expected rewards, the
function values can still be diﬀerent even when following the optimal policy. However, when
following the optimal policy, the learning agent does not need to explore the environment
and the regret is not observed in problem speciﬁc terms, S and A.

) to be monotone. Thus, even for a single agent,
We note that we do not require f (
·
t f (rt)]) is not equivalent to optimizing E[
optimizing f (E[
t rt]. Hence, the proposed
framework can be used to optimize functions of cumulative or long term average rewards of
single agents as well.

P

P

In the following sections, we ﬁrst show that the joint-objective function of average re-
wards allows us to obtain Pareto-optimal policies with an additional assumption of mono-
tonicity. Then, we will present a model-based algorithm to obtain this policy π∗, and regret
accumulated by the algorithm. We will present a model-free algorithm in Section 6 which
can be eﬃciently implemented using Deep Neural Networks.

4. Obtaining Pareto-Optimal Policies

Many multi-objective or multi-agent formulations require the policies to be Pareto-Optimal
(Roijers et al., 2013; Sener and Koltun, 2018; Van Moﬀaert and Now´e, 2014). The conﬂict-
ing rewards of various agents may not allow us to attain simultaneous optimal average

7

 
rewards for any agent with any joint policy. And hence, an envy-free Pareto optimal policy
is desired. We now provide an additional assumption on the joint objective function, and
show that the optimal policy satisfying Equation (8) is Pareto optimal.

Assumption 3 If f is an element-wise monotonically strictly increasing function. Or,

k

∈

∀

[K], the function satisﬁes,

xk > yk =

⇒

f

, xk,

· · ·

> f

(cid:17)

, yk,

· · ·

(cid:17)

· · ·

(cid:16)

· · ·

(cid:16)

(10)

Element wise increasing property motivates the agents to be strategic as by increasing
its per-step average reward, agent can increase the joint objective. Based on Equation (10),
we notice that the solution for Equation (8) is Pareto optimal.

Deﬁnition 2 A policy π∗ is said to be Pareto optimal if and only if there is exists no
other policy π such that the average per-step reward is at least as high for all agents, and
strictly higher for at least one agent. In other words,

≥
Theorem 1 Solution of Equation (8), or the optimal policy π∗ is Pareto Optimal.

∈

∀

∃

k

[K], λk
π∗

λk
π and

k, λk

π∗ > λk
π

(11)

Proof We will prove the result using contradiction. Let π∗ be the solution of Equation
(8) and not be Pareto optimal. Then there exists some policy π for which the following
equation holds,

π ≥
From element-wise monotone increasing property in Equation (10), we obtain

∈

∀

∃

k

[K], λk

λk
π∗ and

k, λk

π > λk
π∗

f (

· · ·

, λk
π,

· · ·

) > f (

, λk

· · ·
= arg max

π

π∗,
· · ·
f (λ1
π,

)

· · ·

, λK
π )

(12)

(13)

(14)

This is a contradiction. Hence, π∗ is a Pareto optimal solution.

This result shows that algorithms presented in this paper can be used to optimally

allocate resources among multiple agents using average per step allocations.

5. Model-based Algorithm

RL problems typically optimize the cumulative rewards, which is a linear function of rewards
at each time step because of the addition operation. This allows the Bellman Optimality
Equation to require only the knowledge of the current state to select the best action to
optimize future rewards (Puterman, 1994). However, since our controller is optimizing a
joint non-linear function of the long-term rewards from multiple sources, Bellman Optimality
Equations cannot be written as a function of the current state exclusively. Our goal is to
ﬁnd the optimal policy as solution of Equation (8). Using average per-step reward and

8

inﬁnite horizon allows us to use Markov policies. An intuition into why this works is there
is always inﬁnite time available to optimize the joint objective f .

The individual long-term average-reward for each agent is still linearly additive ( 1
τ

τ
t=0 rk(st, at)).

For inﬁnite horizon optimization problems (or τ
of the state to obtain expected cumulative rewards. For all k

→ ∞

), we can use steady state distribution

P

[K], we use

∈

λk
π =

rk(s, a)dπ(s, a)

Xs∈S Xa∈A

(15)

where dπ(s, a) is the steady state joint distribution of the state and actions under policy π.
Equation (15) suggests that we can transform the optimization problem in terms of optimal
policy to optimal steady-state distribution. Thus, we have the joint optimization problem
in the following form which uses steady state distributions

d∗ = arg max

f

d



r1(s, a)d(s, a),

Xs∈S,a∈A
with the following set of constraints,



,

· · ·

Xs∈S,a∈A

rK(s, a)d(s, a)





d(s′, a) =

Xa∈A

Xs∈S,a∈A

d(s, a) = 1

P (s′

s, a)d(s, a)
|

∀

s′

∈ S

Xs∈S,a∈A

d(s, a)

0

s

∀

≥

∈ S

, a

∈ A

(16)

(17)

(18)

(19)

Constraint (17) denotes the transition structure for the underlying Markov Process. Con-
straint (18), and constraint (19) ensures that the solution is a valid probability distribution.
Since f (
) is jointly concave, arguments in Equation (16) are linear, and the constraints
in Equation (17,18,19) are linear, this is a convex optimization problem. Since convex op-
timization problems can be solved in polynomial time (Bubeck et al., 2015), we can use
standard approaches to solve Equation (16). After solving the optimization problem, we
ﬁnd the optimal policy from the obtained steady state distribution d∗(s, a) as,

· · ·

π∗(a

s) =
|

P r(a, s)
P r(s)

=

d∗(a, s)
a∈A d∗(s, a)

(20)

The proposed model-based algorithm estimates the transition probabilities by interact-
ing with the environment. We need the steady state distribution dπ to exist for any policy
π. We note that when the priors of the transition probabilities P (
s, a) are a Dirichlet dis-
tribution for each state and action pair, such a steady state distribution exists. Proposition
1 formalizes the result of the existence of a steady state distribution when the transition
probability is sampled from a Dirichlet distribution.

P

·|

Proposition 1 For MDP
probabilities
steady state distribution ˆdπ given as

M

c

P come from a Dirichlet distribution. Then, any policy π for

with state space

and action space

S

A

, let the transition
will have a

M

b

ˆdπ(s′) =

ˆdπ(s)

Xs∈S





Xa∈ bA

π(a

9

P (s, a, s′)

s)
|

c

(21)

s′

∀

.

∈ S



b



P (s, a,

Proof Transition probabilities
) follow Dirichlet distribution for all state-action
·
s) is a proba-
pairs (s, a), and hence they are strictly positive. Further, as the policy π(a
|
s) = 1. So, there
s)
bility distribution on actions conditioned on state, π(a
|
|
is a non zero transition probability to reach from state s
. Since the
single step transition probability matrix is strictly positive for any policy π, a steady state
distribution exists for any policy π.

a π(a
to state s′
P

≥
∈ S

∈ S

0,

b

To complete the setup for our algorithm, we make an additional assumption stated

below.

Assumption 4 The transition probabilities P (
·|
a Dirichlet prior for all state action pairs (s, a).

s, a) of the Markov Decision Process have

Since we assume that transition probabilities of the MDP

tions, all policies on

M

have a steady-state distribution.

follow Dirichlet distribu-

M

5.1 Algorithm Description

S

A

, action space

Algorithm 1 describes the overall procedure that estimates the transition probabilities and
the reward functions. The algorithm takes as input the state space
, set
of agents [K], discount factor γ, the reward structure r, and the objective function f . It
initializes the next state visit count for each state-action pair N (s, a, s′) by one for Dirichlet
sampling. For initial exploration, the policy uses a uniform distribution over all the actions.
The algorithm proceeds in epochs e and νe(s, a) stores the number of times a state-action
pair is visited in epoch e. We assume that the controller is optimizing for inﬁnite horizon
and thus there is no stopping condition for epoch loop in Line 6. For each time index
t in epoch, the controller observes the state, samples and plays the action according to
π(
s), and observes the rewards for each agent and next state. It then updates the state
visit count for the observed state and played action pair. We break the current epoch e
if the total number of visitations of any state action pair in current epoch exceeds total
visitations in previous epoch. After breaking the epoch, we sample transition probabilities
from the updated posterior and ﬁnd a new optimal policy for the sampled MDP by solving
the optimization framework described in Equation (16).

·|

10

∀
|A| ∀

, [K], γ, f, r)

∀
(s, a)
(a, s)

∈ S × A
∈ A × S

A
∈ S × A × S

Initialize N (s, a, s′) = 1
Initialize ν0(s, a) = 0
s) = 1
Initialize π0(a
|
Initialize e = 1
for time steps t = 1, 2,

Algorithm 1 Model-Based Joint Decision Making
1: procedure Model Based Online(
,
S
(s, a, s′)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

Observe current state st
Sample action to play at
∼
Play at, obtain reward rt and observe next state st+1
N (st, at, st+1) + 1, νe(st, at)
Update N (st, at, st+1)
if νe(st, at)
e′<e νe′ (st, at)
1,
}

∀
Dir(N (s, a,
·

←
(s, a)
))

∈ S × A

then

(s, a)

e + 1

πe(

st)

do

· · ·

←

≥

∼

∀

·|

)

·

max
{
Update epoch counter e
P
Initialize νe(s, a) = 0
ˆP (s, a,
Solve steady state distribution d(s, a) as the solution of the optimization problem in

νe(st, at) + 1

←

Equations (16-19)

16:

Obtain optimal policy π as

s) =

π(a

|

d(s, a)
a∈A d(s, a)

P

end if

17:
18:
19: end procedure

end for

5.2 Regret

We now prove the regret bounds of Algorithm 1 in the form of the following theorem.

Theorem 2 For T
expected diameter D is bounded.

≥

SA, the regret RT of Algorithm 1 for MDP with Dirichlet priors and

˜O

RT ≤

LKADS

A
T !

r

(22)

We ﬁrst describe a high level approach used in obtaining the bounds on regret and then we
formally obtain the required result. We perform the following sequence of steps to bound
the regret:

• Convert the regret in terms of fairness metric to the regret in the rewards obtained

by any agent.

• Bound the diﬀerence between the rewards obtained by an agent from the average
reward obtained using the optimal policy, by breaking the total regret into regret of
epochs. We also bound the total number of epochs of Algorithm 1 (Lemma 2).

• We now divide the regret into two cases, which are:

11

 
1. Probability estimates are close to the true transition probabilities. We
use average reward-criteria from (Puterman, 1994) to split the regret in each
epoch into:

(a) Deviation from expected rewards of policy: Deviation of the observed
rewards and the obtained rewards. We will use Azuma-Hoeﬀdings inequality
(Lemma 3) to bound this term.

(b) Deviation from true transition probabilities: Regret because of not

knowing the true transition probabilities.

(c) Deviation from expected next state: Deviation of the gain of the ob-
served state from the expected gain of the next state. We will again use
Azuma-Hoeﬀdings inequality (Lemma 3).

2. Probability estimates are far from the true transition probabilities We
bound the probability of the event when the estimated transition probabilities
are far in ℓ1 metric.

• Finally, we will sum up the regret incurred in each epoch, over all epochs, to bound

the regret.

We ﬁrst state some auxiliary lemmas required for completion of the proof. The proofs
for the lemmas are provided in the Appendix. The ﬁrst lemma bounds the number of time
the algorithm samples a transition matrix and generates a new policy.

Lemma 1 (Posterior Sampling Lemma 1 (Osband et al., 2013)) For any σ(Ht)-measurable
function g, if P follows distribution φ, then for transition probabilities ˜P sampled from φ
we have,

σ(Ht)] = E
E [g(P )
|

g( ˜P )
σ(Ht)
|
i

h

(23)

Lemma 2 The total number of epochs E for the model-free IFRL Algorithm 1 up to step
T

SA is upper bounded as

≥

E

≤

1 + 2AS + AS log2

T
SA

(cid:19)

(cid:18)

(24)

The next lemma is the Azuma-Hoeﬀding’s inequality, which we use to bound Martingale

diﬀerence sequences.

Lemma 3 (Azuma-Hoeﬀding’s Inequality) Let X1,
, then,
sequence such that
}

Xi| ≤
|

c for all i

1, 2,

∈ {

· · ·

, n

· · ·

, Xn be a Martingale diﬀerence

n

P

 |

Xi=1

Xi| ≥

ǫ

! ≤

2 exp

ǫ2
2nc2

(cid:19)

−

(cid:18)

(25)

Lemma 4 (Bounded Span of MDP) For an MDP
, for any stationary policy π with
average reward ρ, the diﬀerence of bias of any two states s, and s′, is upper bounded by the
diameter of the MDP D as:

M

V (s)

V (s)

D

≤

∀

−

s, s′

.

∈ S

(26)

12

We also want to bound the deviation of the estimates of the estimated transition prob-
. For that we use ℓ1 deviation bounds from

abilities of the Markov Decision Processes
(Weissman et al., 2003). Consider, the following event,

M

Et =

ˆP (
·|

(k

s, a)

P (

s, a)

−

·|

k1 ≤ s

14S log(2AT )
1, n(s, a)
max
}
{

∀

(s, a)

∈ S × A)

(27)

where n =

t
t′ 1{st′ =s,at′ =a}. Then we have, the following lemma:

Lemma 5 The probability that the event

P

Et fails to occur us upper bounded by

1
20t6 .

After stating all the auxiliary lemmas, we are now ready to prove the regret bound in

Theorem 2 as follows.
Proof The deﬁnition of regret RT in Equation (9) gives us:

f

−

 · · ·

(cid:17)
T

,

1
T

T

Xt=0

rk(st, at),

"

f

(cid:12)
(cid:12)
(cid:12)
L

"

"

L
T

K

RT = E

E

≤

= E

=

L
T

Xk=1

LK
T

≤

max
k∈[K]

, λk

π∗ ,

· · ·

λk
π∗

1
T

−

· · ·

(cid:16)
K

Xk=1 (cid:12)
(cid:12)
K
(cid:12)

T λk
π∗

Xk=1 (cid:12)
(cid:12)
(cid:12)
E
T λk
π∗
(cid:12)
(cid:12)
(cid:12)
E

"

t=0
X
T

−

−

Xt=0
T

t=0
X

T λk
π∗

−

"
(cid:12)
(cid:12)
(cid:12)

rk(st, at)
#
(cid:12)
(cid:12)
(cid:12)

rk(st, at)
#
(cid:12)
(cid:12)
(cid:12)
rk(st, at)
#
(cid:12)
(cid:12)
(cid:12)
rk(st, at)
(cid:12)
(cid:12)
(cid:12)

T

,

#

· · · !

#
(cid:12)
(cid:12)
(cid:12)

(28)

(29)

(30)

(31)

(32)

t=0
X
where Equation (29) follows from the assumption of Lipschitz continuity. Equation (31)
follows from the linearity of expectation. We now want to bound the maximum regret
incurred by any agent in Equation (32).

Note that the algorithm proceeds in epochs. Hence, we can bound the regret of each
epoch and them sum over all the epochs to bound the total cumulative regret for which the
algorithm runs. We bound the expectation term inside Equation (32) for all k

[K] as:

E[∆] = E

T

−

t=0
X

T λk
π∗
"
(cid:12)
(cid:12)
(cid:12)

rk(st, at)
#
(cid:12)
(cid:12)
(cid:12)

= E

= E

E

≤

"
(cid:12)
(cid:12)
(cid:12)

"
(cid:12)
(cid:12)
(cid:12)

"

∈

T

λk
π∗

t=0 (cid:16)
X
E

te+1−1

−

e=1
X
E

t=te (cid:16)
X
te+1−1

t=te (cid:16)
X

e=1 (cid:12)
X
(cid:12)
(cid:12)
E [
]
∆e|
|

rk(st, at)

#

(cid:17) (cid:12)
(cid:12)
(cid:12)
rk(st, at)

rk(st, at)

λk
π∗

−

λk
π∗

−

#
(cid:17) (cid:12)
(cid:12)
(cid:12)

#
(cid:17) (cid:12)
(cid:12)
(cid:12)

(33)

(34)

(35)

(36)

E

e=1
X

≤

13

where ∆e is the regret in each epoch e, and E is the total number of epochs. Recall that,
we have that the total number of epochs E bounded by O(SA log(T /(SA)) from Lemma 2.

As stated in the proof intuition, we divide the regret into two cases. We ﬁrst consider the
case where the estimated transition probabilities are close to the true transition probabilities
or part 1. We now bound the expected regret E [
] for all epoch e using the gain-bias
∆e|
|
relationship as:

E [
∆e|
|

] = E

= E

= E

E

≤

= E

te+1−1

"

"

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

"

"

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

t=te (cid:16)
X
te+1−1

t=te (cid:16)
X
te+1−1

t=te (cid:16)
X
te+1−1

t=te (cid:16)
X
te+1−1

t=te (cid:16)
X
te+1−1

"

(cid:12)
(cid:12)
(cid:12)
+ E

"
t=te (cid:16)
(cid:12)
X
(cid:12)
te+1−1
(cid:12)

= E

"

(cid:12)
(cid:12)
(cid:12)
+ E

t=te (cid:16)
X
te+1−1

λk
π∗

−

˜λk
˜πe −

rk(st, at)

rk(st, at)

#
(cid:17) (cid:12)
(cid:12)
(cid:12)

#
(cid:17) (cid:12)
(cid:12)
(cid:12)
˜V k
˜πe(st) + rk

( ˜P˜πe

˜V k
˜πe)(st)

−

˜πe(st)

−

rk(st, at)

#

(cid:17) (cid:12)
(cid:12)
(cid:12)
˜V k
˜πe)(st)

te+1−1

( ˜P˜πe

t=te (cid:16)
X

+ E

"

(cid:12)
(cid:12)
(cid:12)

−

˜V k
˜πe(st)

#
(cid:17) (cid:12)
(cid:12)
(cid:12)

rk
˜πe(st)

rk
˜πe(st)

−

−

rk(st, at)

rk(st, at)

#
(cid:17) (cid:12)
(cid:12)
(cid:12)

#
(cid:17) (cid:12)
(cid:12)
(cid:12)
(P˜πe

( ˜P˜πe

˜V k
˜πe)(st)

−

˜V k
˜πe)(st) + (P˜πe

˜V k
˜πe)(st)

−

˜V k
˜πe(st)

rk
˜πe(st)

rk(st, at)

−

( ˜P˜πe

˜V k
˜πe)(st)

−

te+1−1

+ E

t=te (cid:16)
X

"

(cid:12)
(cid:12)
(cid:12)

#
(cid:17) (cid:12)
(cid:12)
(cid:12)
˜V k
˜πe(st)

#
(cid:17) (cid:12)
(cid:12)
(cid:12)
˜V k
(P˜πe
˜πe)(st)

(P˜πe

˜V k
˜πe)(st)

−

"

t=te (cid:16)
X
= R1(e) + R2(e) + R3(e),

(cid:12)
(cid:12)
(cid:12)

#
(cid:17) (cid:12)
(cid:12)
(cid:12)

(37)

(38)

(39)

(40)

#

(cid:17) (cid:12)
(cid:12)
(cid:12)

(41)

where Equation (38) follows from Lemma 1, and Equation (39) follows from the Theorem
8.2.6 of (Puterman, 1994). We now bound the three terms and their summations over the
epochs.

Deviation from expected rewards of policy: Note that the R1(e) term is the
e (st).
e (st) is a zero mean Martingale adapted
. Hence, we can bound the R2 term using Azuma-Hoeﬀding’s

deviation of observed reward rk(st, at) from the expected reward of the policy ˜πe, r˜πk
Further, r˜πk
to ﬁltration

e (st) = E[rk(st, at)
st], and rk(st, at)
|

r˜πk

−

te+1
σ(Hk)
k=te
}
{

14

Lemma as follows:

te+1−1

R1(e) = E

≤

t=te (cid:16)
X

"
(cid:12)
(cid:12)
(cid:12)
(te+1 −

rk
˜πe(st)

−

rk(st, at)

#
(cid:17) (cid:12)
(cid:12)
te+1−1
(cid:12)

te)P

te) log (te+1 −

te+1−1

te) P

t=te (cid:16)
X

(cid:12)
(cid:12)
(cid:12)
te) log (te+1 −

te) +

p
+ (te+1 −

(te+1 −

(te+1 −

te) log (te+1 −

te) +

(te+1 −

te) log T +

1
(te+1 −

te)

p

p

≤

≤

≤

p

≥

p

(cid:17) (cid:12)
(cid:12)
(cid:12)

1
(te+1 −
1
(te+1 −

te)2 (te+1 −

te)

te)

rk
˜πe(st)

rk(st, at)

<

−

(te+1 −

te) log 2 (te+1 −

te)

!

(cid:12)
(cid:12)
(cid:12)
rk
˜πe(st)

t=te (cid:16)
X

rk(st, at)

−

p

(cid:17) (cid:12)
(cid:12)
(cid:12)
(te+1 −
te) log 2 (te+1 −

(42)

te)

!
(43)

(44)

(45)

(46)

Here, Equation (44) is obtained from Equation (43) by bounding the ﬁrst probability term
using the upper bound of 1 and by bounding the second probability term using the Azuma-
Hoeﬀding’s inequality (Equation (25)). We can now sum over Equation (46) over all epochs
and bound the total deviation using Cauchy-Schwarz inequality as:

E

R1(e) =

E

e=1
X

e=1 (cid:18)
X

p
E

E

(te+1 −

te) log T +

1
(te+1 −

te)

(cid:19)

te) log T +

E

Xe=1

1
(te+1 −

te)

(te+1 −

te) + E max

e∈{1,2,··· ,E}

1
(te+1 −

te)

(te+1 −

Xe=1

Xe=1!

E

(E)(log T )

≤ v
u
u
t

≤ v
u
u
t

e=1
X
T
SA

SA log2

(T log T ) + E

≤ s(cid:18)

≤

(cid:18)
(log2 T ) √SAT + SA log2 T

(cid:19)(cid:19)

(47)

(48)

(49)

(50)

(51)

where Equation (50) follows from the fact that te+1 −
from Lemma 2. This completes part 1a.

te ≥

1, and the the value of E comes

Deviation from the true transition estimates: We ﬁrst consider the case where
the estimated probability distribution lies in some neighborhood of the true distribution.
Particularly, for all s, a, we construct the set of probability distributions P ′(
·|

s, a),

Pt =

P ′ :

(

ˆP (
·|
k

s, a)

−

P ′(
·|

s, a)

k1 ≤ s

14S log(2AT )
1, n(s, a)
max
{

} )

(52)

15

 
 
 
where n =
events

t
t′ 1{st′ =s,at′ =a}. Using the construction of the set
Et as:

Et, and ˜
P

Pt, we can now deﬁne the

, and ˜

˜P

{

P

Et =

∈ Pt}

Et =
Pt is σ(Ht) measurable and hence from Lemma 1 we have P( ˜

Et).
We now consider the case when the event

Further, note that
∈
Pt) = P(P
Et occurs whenever we sample the transition
probabilities to update the policy at time step te. The expected value of R(e) when event
Ete and ˜

∈ Pt) = P(

Ete holds is:

Et) = P( ˜P

∈ Pt

(53)

n

o

R(e)

|Ete , ˜
Ete

E

h

i

= E

= E

"
(cid:12)
(cid:12)
(cid:12)

te+1−1

( ˜P˜πe

˜V k
˜πe)(st)

t=te (cid:16)
X
te+1−1

(P˜πe

−

˜V k
˜πe)(st)

Ete, ˜
Ete
(cid:12)
(cid:12)
(cid:17) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
st, at) ˜V k
( ˜P (st+1|

#

˜πe)(st)

(54)

(55)

˜πe(at|

st)

"
t=te Xat∈A
(cid:12)
X
(cid:12)
(cid:12)
(P (st+1|

st, at) ˜V k

˜πe)(st)

˜πe(at|

st)

˜πe(at|

st)

te+1−1





(cid:12)
(cid:12)
(cid:12)





(cid:12)
(cid:12)
(cid:12)

t=te Xat∈A
X

te+1−1

t=te Xat∈A
X

te+1−1

Xst+1∈S (cid:16)
Ete, ˜
Ete

#

(cid:12)
(cid:12)
(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xst+1∈S (cid:16)

˜P (st+1|

st, at)

˜P (st+1|

st, at)

Xst+1∈S (cid:16)

−

−

P (st+1|

st, at)

(cid:17)

P (st+1|

st, at)

(cid:17)

Ete, ˜

Ete



˜V k
˜πe(st)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(56)
(cid:12)
(cid:12)
(cid:12)
Ete, ˜
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(57)
(cid:12)
(cid:12)
(cid:12)

˜D

Ete 



˜D

˜P (st+1|
k

st, at)

−

P (st+1|

st, at)

k1

t=te Xat∈A
X
te+1−1

"
(cid:12)
(cid:12)
(cid:12)
2 ˜D
"

t=te Xat∈A s
X

te+1−1

14S log(2AT )
1, Ne(st, at)
{

max

Ete, ˜
Ete
} (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#

14S log(2AT )
1, Ne(st, at)
}
{

max

t=te Xat∈A s
X

Xa∈A  

Xs∈S Xa∈A

νe(s, a)

s

14S log(2AT )
1, Ne(s, a)
{

max

} !

Ete, ˜
Ete
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#

(58)

(59)

(60)

(61)

From Lemma 4 value of maxs ˜V k
˜πe(s) is bounded by ˜D for any stationary policy
with maximum reward 1. Further, translating ˜V k
˜πe(s) does not change the gain λ˜πe of the
stationary policy ˜πe. These two facts gives Equation (57). Equation (58) comes from the
1. Further, we have E[ ˜D] = D from Lemma 1. Now, summing over the
fact that ˜π(at|

mins ˜V k

˜πe(s)

st)

−

≤

16

−

= E

E

≤

E

E

≤

≤

2D

≤

= 2D

epochs, we get the total regret from choosing the optimal policy for the sampled transition
probabilities instead of the true transition probabilities as:

(62)

(63)

(64)

(65)

(66)

(67)

(68)

(69)

(70)

(71)

(72)

E

E

e=1
X

h

R2(e)

|Ete , ˜
Ete

i

=

=

=

=

E

e=1
X

Xa∈A

D

D

D

νe(s, a)

14S log(2AT )
1, Ne(s, a)
{

max

} !

s

Xa∈A  

Xs∈S Xa∈A
14S log(2AT )

p

14S log(2AT )

Xs∈S Xa∈A

νe(s, a)

max

1, Ne(s, a)
}
{

p

N (s, a)

Xa∈A

p

D

14S log(2AT )

Xs∈S Xa∈A
SA

p

N (s, a)

Xa∈A
= AD

p
14S log(2AT )√SAT

s

Xs∈S Xa∈A

= ASD
p

14AT log(2AT )

This bounds the expected regret for part 1b.

p

Deviation from expected next state: R3(e) term denotes the deviation from landing
in a state st+1 from the expected value of the gains from all the possible st+1 from st, at.
We bound the R3(e) as:

te+1−1

˜V k
˜πe)(st)

−

V k
˜πe(ste) +

˜V k
˜πe(st)

te+1−1

#
(cid:17) (cid:12)
(cid:12)
(cid:12)
(P˜πe

E[R3(e)] = E

= E

E

E

≤

≤

D + E

(P˜πe

t=te (cid:16)
X

"
(cid:12)
(cid:12)
(cid:12)
V k
˜πe(ste+1)
"
(cid:12)
(cid:12)
(cid:12)
V k
˜πe(ste+1)

−

−

h(cid:12)
(cid:12)
(cid:12)
˜D

h

i

"
(cid:12)
(cid:12)
(cid:12)

+ E

"
(cid:12)
(cid:12)
te+1−1
(cid:12)

t=te (cid:16)
X

V k
˜πe(ste)
(cid:12)
(cid:12)
(cid:12)
(P˜πe

te+1−1

t=te (cid:16)
X

(P˜πe

˜V k
˜πe)(st)

−

#
(cid:17) (cid:12)
(cid:12)
(cid:12)

˜V k
˜πe(st+1)

#
(cid:17) (cid:12)
(cid:12)
(cid:12)

˜V k
˜πe)(st)

−

˜V k
˜πe(st+1)

(P˜πe

˜V k
˜πe)(st)

−

#
(cid:17) (cid:12)
(cid:12)
(cid:12)
˜V k
˜πe(st+1)

#
(cid:17) (cid:12)
(cid:12)
(cid:12)

t=te (cid:16)
X
+ E

te+1−1

"

i

(cid:12)
(cid:12)
(cid:12)
˜V k
˜πe)(st)

t=te (cid:16)
X
˜V k
˜πe(st+1)

−

˜V k
˜πe)(st)

The ﬁrst term in Equation (71) follows from Lemma 4. The second term in Equation (71)
te+1
˜V k
σ(Hk)
˜πe(st+1) is a zero mean Martingale process adapted to ﬁltration
(P˜πe
.
k=te
}
{
−
˜πe(st+1) is bounded by ˜D for all t. Hence, using
˜V k
Further, the diﬀerence (P˜πe
Azuma-Hoeﬀding’s inequality, and E[ ˜D] = D gives us:

˜V k
˜πe)(st)

−

E[R3(e)]

D + D

≤

(te+1 −

te) log(2(te+1 −

te)) +

p

17

1
te+1 −

te

(73)

Now, summing Equation (73) for all epochs we get.

E

e=1
X
E

Xe=1
E

Xe=1

E [R3(e)] =

E

e=1 (cid:18)
X

E [R3(e)] = ED + D

v
u
u
t
E [R3(e)] = DSA log2

p
E

E

Xe=1
T
SA

(cid:19)

(cid:18)

D + D

(te+1 −

te) log(2(te+1 −

te)) +

1
te+1 −

te (cid:19)

(te+1 −

te) log(2(te+1 −

te)) + E max

e

+ D

s

SA log

T
SA

(cid:19)

(cid:18)

T log T + SA log2

te

1
te+1 −
T
SA

(cid:18)

(cid:19)

This bounds the regret from part 1c.

We now consider the other case, where the event in Equation (52) does not occur. We
already bounded the probability of this event in Lemma 5 using result from (Weissman et al.,
2003). In particular, we have:

t

P(
E
(cid:16)

t ) + P( ˜
c
c
te)
E
(cid:17)

T

Xt=1

T

tP(
E

c
t )

E

E

Xe=1

h

R(e)

|E

c
te ∪

˜
c
te
E

≤

i

≤

≤

≤

≤

≤

≤

≤

E

s,a
Xe=1 X
E
tkP(
E

Xe=1
T

νk(s, a)P(
E

c
te )

c
te ∪

˜
c
te)
E

≤

2

2

2

t=1
X
T 1/4

Xt=1
T 1/4

tP(
E

c
t )

tP(
E

c
t ) + 2

t.1 + 2

Xt=T 1/4+1
∞
1
t5

t=T 1/4
Z
1
4T

Xt=1
2√T + 2

2√T + 2

4√T

Xt=T 1/4+1
T
1
t6

t

where Equation (78) follows from the fact that
s,a νk(s, a)
≥
Equation (81) follows from Lemma 5. This completes part 2.

P

P

s,a Nk(s, a) = tk. Further,

18

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

Summing over all the possible sources of regret, we now have,

E

e=1
X
E

E [∆] =

=

E

E [∆e] =

∆e|Ete, ˜
Ete
h
R1(e) + E[R2(e)] + E[R3(e)] +

e=1
X

+

i

e=1
X
E

E

E

c
te ∪

˜
c
te
E

∆e|E
h

i

E

∆e|E
h

c
te ∪

˜
c
te
E

i

e=1
X

E

(cid:17)

e=1
X
= ˜
O

(cid:16)

SAD√AT

Substituting Equation (86) in Equation (32), we get:

E [RT ]

≤

≤

SAD√AT

LK
T

˜O

(cid:16)

˜O

LKADS

(cid:17)

A
T !

r

(85)

(86)

(87)

(88)

This completes the proof of the Theorem.

Note that when the optimal policy is known for the true MDP, only the ﬁrst and the third
term remain in Equation (85), which is upper bounded by ˜O(D√T ).

6. Model Free Algorithm

In the previous section, we developed a model based tabular algorithm for joint function
optimization. However, as the state space, action space, or number of agents increase the
tabular algorithm becomes infeasible to implement. In this section, we consider a policy
gradient based algorithm which can be eﬃciently implemented using (deep) neural networks
thus alleviating the requirement of a tabular solution for large MDPs.

∞

in our MDP

For the model free policy gradient algorithm, we will use ﬁnite time horizon MDP, or
T <
. This is a practical scenario where communication networks optimize
fairness among users for ﬁnite duration (Margolies et al., 2016). We now describe a model
free construction to obtain the optimal policy. We use a neural network parameterized by
θ. The objective thus becomes to ﬁnd optimal parameters θ∗, which maximizes,

M

arg max

θ

f

(1

−

γ)J 1

πθ ,

, (1

−

· · ·

γ)J K
πθ

.

(89)

For model-free algorithm, we assume that the function is diﬀerentiable. In case, the function
is not diﬀerentiable, sub-gradients of f can be used to optimize the objective (Nesterov).
Gradient estimation for Equation (89) can be obtained using chain rule:

(cid:0)

(cid:1)

∇θf =

(1

Xk∈[K]

γ)

−

∂(1

∂f

−

π ∇θJ k

π

γ)J k

(90)

For all k, J k

π can be replaced with averaged cumulative rewards over N trajectories
for the policy at ith step, where a trajectory τ is deﬁned as the tuple of observations,
or τ = (s0, a0, r1
π can be estimated using
0,

0 , s1, a1, r1
1,

). Further,

, rK
1 ,

, rK

· · ·

· · ·

∇θJ k

· · ·

19

 
REINFORCE algorithm proposed in (Williams, 1992; Sutton et al., 2000) for all k, and is
given as

1
N

π =

∇θJ k
b

N

T

Xj=1

t=0
X

∇θ log πθ(at,j|

st,j)

T

Xt′=t

rk(st′,j, at′,j)

k

∀

∈

[K].

(91)

Further, J k

π is estimated as

ˆJ k
π =

1
N

T

t=0
X

rk(st,j, at,j)

k

∀

∈

[K].

For a learning rate η, parameter update step to optimize the parameters becomes

θi+i = θi + η(1

γ)

−

∂f

∂(1

−

γ)J k
π

Xk∈[K]

For example, consider alpha-fairness utility deﬁned as

π

∇θJ k
b

π = ˆJ k
J k
π

(cid:12)
(cid:12)
(cid:12)

f ((1

−

γ)J 1
π ,

, (1

−

· · ·

γ)J K

π ) =

K

Xk=1

1

−

1

α

π )1−α

(J k
(cid:16)

−

1

(cid:17)

Then the corresponding gradient estimate can be obtained as

(92)

(93)

(94)

ˆ
∇θf =

Xk∈[K]

N

T

st,j)

j=1
P

t=0 ∇θ log π(at,j|
P
(1

γ)α−1

N

T

−

T

τ =t
P

γτ rk(sτ,j, aτ,j)

α

.

(95)

γtrk(st,j, at,j)

!

t=0
P

j=1
P
The proposed Model Free Policy Gradient algorithm for joint function optimization is
, [K], T, γ, f
described in Algorithm 2. The algorithm takes as input the parameters
of MDP
, number of sample trajectories N , and learning rate η as input. The policy
neural network is initialized with weights θ randomly. It then collects N sample trajectories
using the policy with current weights in Line 4. In Line 5, the gradient is calculated using
Equations (90), (91), and (92) on the N trajectories. In optimization step of Line 6, the
weights are updated using gradient ascent.

M

A

S

,

7. Evaluations

In this section, we consider two systems. The ﬁrst is the cellular scheduling, where multiple
users connect to the base station. The second is a multiple-queue system which models
multiple roads merging into a single lane. In both these systems, the proposed algorithms
are compared with some baselines including the linear metric adaptation of reward at each
time.

20

 
, [K], T, γ, f, N, η)

Initialize πθ0 (a, s)
for i = 0, 1,

Algorithm 2 Model Free Joint Policy Gradient
1: procedure Joint Policy Gradient (
,
S
⊲ Initialize the neural network with random weights θ
2:
3:
4:
5:
6:
7:
8:
9: end procedure

Collect N trajectories using policy πθi
Estimate gradient using Equation (90), (91), (92)
Perform Gradient Ascent using Equation (93)

, until convergence do

end for
Return πθ

· · ·

A

7.1 Cellular fairness maximization

The fairness maximization has been at the heart of many other resource allocation problems
such as cloud resource management, manufacturing optimization, etc. (Perez et al., 2009;
Zhang and Shah, 2015). The problem of maximizing wireless network fairness has been
extensively studied in the past (Margolies et al., 2016; Kwan et al., 2009; Bu et al., 2006;
Li et al., 2018). With increasing number of devices that need to access wireless network and
ever upgrading network architectures, this problem still remains of practical interest. We
consider two problem setup for fairness maximization, one with ﬁnite state space, and other
with inﬁnite state space. For ﬁnite state space, we evaluate both the proposed model-based
algorithm (Algorithm 1) and the proposed model-free algorithm (Algorithm 2) while for
inﬁnite state space, we evaluate only model-free algorithm as tabular model-based algorithm
cannot be implemented for this case.

7.1.1 Problem Setup

We consider fairness metric of the form of generalized α-fairness proposed in (Altman et al.,
2008). For the rest of the paper, we will call this metric as α-fairness rather than generalized
α-fairness. The problem of maximizing ﬁnite horizon α-fairness for multiple agents attached
to a base station is deﬁned as

K

Xk=1

1

−

1

1
T

α 


1, 2,

∈ {

ak,t = 1

t

∀

, T

}

· · ·

(1−α)

T

t=1
X

ak,trk,t

!

1


−



Cα(T ) = max

{ak,t}K×T

K

s.t.

Xk=1
ai,j ∈

0, 1

(96)

(97)

(98)

where, ak,t = 1 if the agent k obtains the network resource at time t, and 0 otherwise. This
implies, at each time t, the scheduler gives all the resources to only one of the attached user.
Further, rk,t denotes the rate at which agent k can transmit at time t if allocated network
resource. Cellular networks typically use Proportional Fair (PF) utility which is a special

21

 
case of the above metric for α

→

1 (Holma and Toskala, 2005), and is deﬁned as:

C1(T ) = max

{ak,t}K×T

K

log

T

ak,trk,t

1
T

!

(99)

Xk=1
We note that rk,t is only known causally limiting the use of oﬄine optimization techniques
and making the use of learning-based strategies for the problem important. We evaluate
our algorithm for α = 2 fairness, and proportional fairness for T = 1000.

t=1
X

7.1.2 Proportional Fairness

. The
2, 4, 6
We let the number of users attached to the network, K, belong to the set
}
{
state space of each agent comes from its channel conditions. We assume that the channel
for a agent can only be in two conditions
, where the good and bad conditions
good, bad
}
{
for each agent could be diﬀerent. The action at each time is a one-hot vector with the
= 2K
entry corresponding to the agent receiving the resources set to one. This gives
(corresponding to the joint channel state of all agents), and
= K (K actions correspond
to the agent that is selected in a time slot). Based on the channel state of agent, the
scheduling decision determines the agent that must be picked in the time-slot. Rate rk,t,
for agent k at time t, is dependent on the state of the agent sk,t and is mentioned in Table
1. Each agent remains in the same state with probability of 0.8, and moves to a diﬀerent
state s

) with probability 0.2. The state transition model becomes,

|A|

|S|

U (

∼

S

k, sk,t+1 =

∀

sk,t,
s

∼

(

U (
{

good, bad
}

w.p. 0.8
) w.p. 0.2

(100)

Agent state
good
bad

r1,t
1.50
0.768

r2,t
2.25
1.00

r3,t
1.25
0.384

r4,t
1.50
1.12

r5,t
1.75
0.384

r6,t
1.25
1.12

Table 1: Agent rate rk,t (in Mbps) based on agent state sk,t. Rate values are practically

observable data rates over a wireless network such as 4G-LTE.

We compare our model-based and model-free algorithms with practically implemented
algorithm of Blind Gradient Estimation (Margolies et al., 2016; Bu et al., 2006) in network
schedulers and SARSA based algorithm devised by (Perez et al., 2009). We ﬁrst describe the
algorithms used in evaluations for maximizing proportional fairness in ﬁnite state systems.
• Blind Gradient Estimation Algorithm (BGE): This heuristic allocates the resources
based on the previously allocated resources to the agents. Starting from t = 1, this policy
allocates resource to agent k∗

t at time t, where

k∗
t = arg max
k∈[K]

rk,t
t−1
t′=0 αk,t′rk,t′

; αk,t′ =

1, k = k∗
t′
= k∗
0, k
t′

(

(101)

BGE is used as de facto standard for scheduling in cellular systems (Holma and Toskala,
2005), and has been shown to be asymptotically optimal for the proportional fairness
metric (Viswanath et al., 2002).

P

22

 
6
• DQN Algorithm: This algorithm based on SARSA (Sutton et al., 1998) and DQN
(Mnih et al., 2015). The reward at each time τ is the fairness of the system at time τ , or
fτ = Cα(t) The DQN neural network consists of two fully connected hidden layers with
100 units each with ReLU activation and one output layer with linear activation. We
use γ = 0.99, ǫ = 0.05, and Adam optimizer with learning rate η = 0.01 to optimize the
network. The batch size is 64 and the network is trained for 1500 episodes.

• Vanilla Policy-Gradient Algorithm: This algorithm is based on the REINFORCE
policy gradient algorithm (Williams, 1992). Similar to the SARSA algorithm, the reward
at each time τ is the fairness of the system at time τ , or fτ = Cα(t) We use γ = 0.99,
and learning rate η = 1

10−4.

×

• Proposed Model Based Algorithm: We describe the algorithm for inﬁnite horizon, so
we maximize the policy for inﬁnite horizon proportional fairness problem by discounting
the rewards as

lim
T →∞

log

K

Xk=1

1
T

T

t=1
X

γtαk,trk,t

!

(102)

The learned policy is evaluated on ﬁnite horizon environment of T = 1000. We keep
γ = 0.99 for implementation of Algorithm 1. We use a ﬁxed episode length of 100 and
In Algorithm 1, the convex optimization is
update policy after every τ = 100 steps.
solved using CVXPY (Diamond and Boyd, 2016).

• Proposed Model Free Algorithm: Since log(
) is diﬀerentiable, the gradient in Equa-
·
tion (90) is evaluated using Equation (95) at α = 1. The neural network consists of a
single hidden layer with 200 neurons, each having ReLU activation function. The out-
put layer uses softmax activation. The value of other hyperparameters are γ = 0.99,
10−3, and batch size N = 100. The algorithm source codes for the proposed
η = 1
algorithms have been provided at (Agarwal and Aggarwal, 2019).

×

Proportional Fairness Simulation Results: We trained the SARSA algorithm and
the model based Algorithm 1 for 5000 time steps for each value of K. To train model
free Algorithm 2 and the Vanilla Policy Gradient algorithm, we used 1000 batches where
each batch contains 36 trajectories of length 1000 time steps. Note that the Blind Gradient
Estimation algorithm doesn’t need training as it selects the agent based on observed rewards.
For all the algorithms we performed a grid search to ﬁnd the hyperparameters.

We show the performance of policies implemented by each of the algorithm. Each policy
is run 50 times and median and inter-quartile range is shown in Figure 1 for each policy.
The policy performance for K = 2, K = 4, and K = 6 is shown in Figure 1(a), Figure 1(c),
and Figure 1(e), respectively.

We note that the performance of model-based algorithm (Algorithm 1) and that of the
model-free algorithm (Algorithm 2) are close. For K = 2, the model-based algorithm out-
, the gap between the model based algorithm
4, 6
performs the BGE algorithm. For K
}
and BGE algorithm is because of the ﬁnite time horizon. The proposed framework assumes
an inﬁnite horizon framework, but the algorithm is trained for ﬁnite time horizon. The

∈ {

23

 
−0.40

−0.45

t
f

−0.50

−0.55

−0.60

−3.8

−4.0

−4.2

t
f

−4.4

−4.6

−4.8

−5.0

t
f

−8.2

−8.4

−8.6

−8.8

−9.0

−9.2

−9.4

−9.6

Model Free JPG
BGE
Model Based JDM
Vanilla PG

−0.5

−1.0

−1.5

−2.0

t
f

−2.5

−3.0

−3.5

−4.0

Model Free JPG
BGE
Model Based JDM
Vanilla PG
DQN

200

400

600

t

(a) K = 2

800

1000

200

400

600

t

800

1000

(b) K = 2 (with DQN)

Model Free JPG
BGE
Model Based JDM
Vanilla PG

t
f

0

−20

−40

−60

−80

−100

Model Free JPG
BGE
Model Based JDM
Vanilla PG
DQN

200

400

600

t

(c) K = 4

800

1000

200

400

600

t

800

1000

(d) K = 4 (with DQN)

Model Free JPG
BGE
Model Based JDM
Vanilla PG

−20

−40

t
f

−60

−80

−100

Model Free JPG
BGE
Model Based JDM
Vanilla PG
DQN

200

400

600

t

(e) K = 6

800

1000

200

400

600

t

800

1000

(f) K = 6 (with DQN)

Figure 1: Proportional Fairness for Cellular Scheduling, ft v/s t (Best viewed in color)

24

regret of ˜O

LDKSA

A
T

for large T .

(cid:18)

q

(cid:19)

also guarantees that the proposed algorithm will be optimal

From Figure 1(b), Figure 1(d), and Figure 1(f), we note that the DQN algorithm per-
forms much worse than expected. The reason for this is that the joint objective function
of fairness is non-linear and is not properly modelled with standard RL formulation. Also,
from Figure 1(a), we note that for K = 2, policy gradient algorithm which uses fairness
till time t can still learn a a good policy, but the performance is still not at par with the
proposed framework. This is because, using the value of the joint objective as reward works
as a linear approximation of the joint reward function. Hence, if the approximation is worse,
the policy gradient algorithm with joint objective as reward will not be able to optimize the
true reward function. Note that as K increases, the approximation error
λk
increase. In the next experiment, we will demonstrate that if the approximation error is
too large, the policy gradient algorithm can perform even worse than the DQN algorithm.

k log(λk)

P

−

7.1.3 α = 2 Fairness

We now evaluate our algorithm with the metric of α-fairness, where no optimal baseline is
known. We also consider a large state-space to show the scalability of the proposed model-
free approach. We consider a Gauss-Markov channel model (Ariyakhajorn et al., 2006) for
modeling the channel state to the diﬀerent users, and let the number of users be K = 8.
Under Gauss-Markov Model, channel state of each user k varies as,

Xk,t =

1

−

β2Xk,t−1 + βǫt,

ǫt ∼ N

(0, 1).

(103)

We assume Xk,0 ∼ N
channel state Xi,t is given as,

(0, 1) for each k

p

[K]. The rate for each user i at time t and in

∈

rk,t = Pk|

2,
Xk,t|

(104)

where Pk is multiplicative constant for the kth user, which indicates the average signal-to-
noise ratio to the user. We let Pk = k−0.2.

Since the state space is inﬁnite, we only evaluate the model free algorithm. The gradient
update equation is deﬁned in Equation (95) with α = 2. The neural network consists of
a single hidden layer with 200 neurons, each having ReLU activation function. We use
10−3 to train the network. The value
stochastic gradient ascent with learning rate η = 1
of other hyperparameters are γ = 0.99, and batch size N = 36. The network is trained for
1000 epochs.

×

Also, since no optimal baseline is known, we compare the model free algorithm with the
Deep Q-Network (DQN) algorithm (Mnih et al., 2015) and the Policy Gradient algorithm
(Williams, 1992). Reward for both DQN algorithm and the Policy Gradient algorithm at
time τ is taken as the value C2(τ ). For DQN, the neural network consists of two fully
connected hidden layers with 100 units each with ReLU activation and one output layer
with linear activation. We use Adam optimizer with learning rate 0.01 to optimize the
DQN network. The batch size is 64 and the network is trained for 1500 episodes. For the
Policy Gradient algorithm, we choose a single hidden layer of 200 neurons, with learning
10−5. Similar to the implementation of the Algorithm 2, we select batch size N = 36
rate 1
and train the network for 1000 epochs.

×

25

α-fairness Simulations Results The results for α-fairness are provided in Figure 2.
Each policy is run 50 times and median and we show the inter-quartile range for each
policy. As a baseline, we also consider a strategy that chooses a user in each time uniformly
at random, this strategy is denoted as “uniform random”. In Figure 2(a), we note that the
DQN algorithm and the policy gradient algorithm are not able to outperform the uniform
random policy while the proposed model free policy outperforms the uniform random policy.
Further Figure 2(b) shows the detrimental eﬀect of using incorrect gradients. The linear
approximation has larger error for α = 2 joint objective than compared to the proportional
fairness joint objective. The standard policy gradient with fairness as rewards now performs
as worse as the DQN algorithm.

−500

−550

t
f

−600

−650

−700

−670

−680

−690

t
f

−700

−710

Model Free JPG
uniform random
Vanilla PG
DQN

Vanilla PG
DQN

200

400

600

t

(a) K = 2

800

1000

200

400

600

t

800

1000

(b) K = 2 (standard RL algorithms)

Figure 2: Alpha Fairness for Cellular Scheduling, ft v/s t (Best viewed in color)

7.2 Multiple Queues Latency Optimization

We consider a problem, where multiple roads merge into a single lane, which is controlled
by a digital sign in front of each road to indicate which road’s vehicle proceeds next. This
problem can be modeled as having K queues with independent arrival patterns, where the
arrival at queue k
is Bernoulli with an arrival rate of λi. At each time, the
user at the head of the selected one of out these K queues is served. The problem is to
determine which queue user is served at each time. Such problems also occur in processor
scheduling systems, where multiple users send their requests to the server and the server
decides which user’s computation to do next (Haldar and Subramanian, 1991).

∈ {

, K

· · ·

1,

}

In such a system, latency is of key concern to the users. The authors of (Zhang et al.,
2019) demonstrated that the eﬀect of latency on the Quality of Experience (QoE) to the
end user is a “sigmoid-like” function. Thus, we deﬁne the latency of a user w as

QoE(w) =

1

e−3

−
1 + e(w−3)

.

(105)

1), and
Note that in Equation (105), QoE(w) remains close to 1 for small wait times (w
close to 0 for high wait times (w
10). We let the service distribution of the queue be
deterministic, where each user takes one unit of time for service. We model the problem

≥

≤

26

as a non-linear multi-agent system. The diﬀerent queues are the agents. The state is the
queue lengths of the diﬀerent queues. The action at each time is to determine which of
the non-empty queue is selected. The reward of each agent at time t, rk,t is zero if queue
k is not selected at time t, and if the QoE of the latency of the user served, if queue k
is selected at time t, where the latency of a user w is the time spent by a user w in the
system (from entering the queue to being served). We again consider α-fairness for α = 2,
and proportional fairness for comparisons.

7.2.1 α = 2 Fairness

We consider K = 8 queues in our system. We let the arrival rate λi in each queue be as
given in Table 2. We also assume that each queue has bounded capacity of 100, and the
user is dropped if the queue is full. The overall reward function among diﬀerent agents is
chosen as α-fairness, where α = 2. The objective can be written as

C2(T ) = max

{ak,t}K×T

−1

K

Xk=1

1

−

1
T

T

t=1
X

ak,trk,t

!

λ1
0.2

λ2
0.1

λ3
0.05

λ4
0.25

λ5
0.15

λ6
0.21

λ7
0.01

λ8
0.3

(106)

Table 2: Arrival rates λk (in number of packets per step) for α = 2 fairness

Since the number of states is large, we only evaluate the model-free algorithm with
T = 1000. The gradient update equation for policy gradient algorithm as given in Equation
10−3 is used
(95) is used for α = 2. Stochastic Gradient Ascent with learning rate η = 5
to train the network. The value of discount factor γ is set to 0.99 and the batch size N is
kept as 30.

×

We compare the proposed algorithm with the DQN Algorithm for Q-learning implemen-
tation. We use fairness at time t or C2(t) as the reward for DQN network. The network
consists of two fully connected hidden layers with 100 units each, ReLU activation function,
and one output layer with linear activation. Adam optimizer with learning rate 0.01 is used
to optimize the network. The batch size is 64 and the network is trained for 500 episodes.
We also compare the proposed algorithm with the Longest Queue First (LQF) Algorithm,
which serves the longest queue of the system. This algorithm doesn’t require any learning,
and has no hyperparameters.

We again run each policy for 50 times and median and we show the inter-quartile range
for each policy. The results for fairness maximization for this queuing system are provided
in Figure (3). We note that the overall objective decreases for all the policies. This is
because the queue length is increasing and each packet has to wait for longer time on an
average to be served till the queue becomes steady. At the end of the episode, the proposed
policy gradient algorithm outperforms both the DQN and the LQF policy. We note that
the during the start of the episode, LQF is more fair because the queues are almost empty,
and serving the longest queue would decrease the latency of the longest queue. However,
serving the longest queue is not optimal in the steady state.

27

 
Our M del Free
LQF
DQN

−300

−400

t
f

−500

−600

−700

200

400

600

800

1000

t

Figure 3: Alpha Fairness for the Queueing System, ft v/s t (Best viewed in color)

7.2.2 Proportional Fairness

We also compare the performance of the policy learned by our algorithm with the policy
learned by DQN algorithm for another synthetic example where we reduce the arrival rates
in order to make the system less loaded. We let the arrival rate λi in each queue be as given
in Table 3. We also assume that each queue has bounded capacity of 10, and the user is
dropped if the queue is full. The overall reward function among diﬀerent agents is chosen
as weighted-proportional fairness. The objective can be written as

C(T ) = max

{ak,t}K×T

wk log

K

Xk=1

1
T

T

t=1
X

ak,trk,t

!

(107)

The weights wk are given in Table 4. The weights were generated from a Normal
distribution with mean 1, and variance 0.1. After sampling, the weights were normalized
to make

k wk = 1.

P

λ1
0.014

λ2
0.028

λ3
0.042

λ4
0.056

λ5
0.069

λ6
0.083

λ7
0.097

λ8
0.11

Table 3: Arrival rates λk (in number of packets per step) for proportional fairness

w1
0.146

w2
0.112

w3
0.145

w4
0.119

w5
0.119

w6
0.123

w7
0.114

w8
0.122

Table 4: Weights wk for weighted proportional fairness

Again, we only evaluate the model-free algorithm with T = 1000. The gradient update
equation for policy gradient algorithm as given in Equation (95) is used for α = 1. Stochastic
10−1 is used to train the network. The value of
Gradient Ascent with learning rate η = 5
discount factor γ is set to 0.999 and the batch size N is kept as 80.

×

28

 
We compare the proposed algorithm with the DQN Algorithm for Q-learning implemen-
tation. We use fairness at time t or C2(t) as the reward for DQN network. The network
consists of two fully connected hidden layers with 100 units each, ReLU activation function,
and one output layer with linear activation. Adam optimizer with learning rate 0.01 is used
to optimize the network. The batch size is 64 and the network is trained for 5000 episodes.

The results for fairness maximization for this queuing system are provided in Figure
Similar to previous cases, we run each policy for 50 times and median and we show
4.
the inter-quartile range for each policy. We note that compared to previous case, the
objective increases as the episode progress. This is because the arrival rates are low, and
queue lengths are short. For the entire episode, the proposed policy gradient algorithm
outperforms the DQN policy. This is because of incorrect Q-learning (and DQN) cannot
capture the non-linear functions of rewards, which is weighted proportional fairness in this
case.

α-Fairness (ft v/s t) for K = 8 and α=1

−5

−10

t
f

−15

−20

−25

200

300

400

500

600
t

700

800

900

1000

Our Model-Free
DQN

Figure 4: Weighted Proportional Fairness for the Queueing System, ft v/s t (Best viewed in color)

8. Conclusion

This paper presents a novel average per step reward based formulation for optimizing joint
objective function of long-term rewards of each agent for inﬁnite horizon multi-agent systems.
In case of ﬁnite horizon, Markov policies may not be able to optimize the joint objective
function, hence an average reward per step formulation is considered. A tabular model
based algorithm which uses Dirichlet sampling to obtain regret bound of ˜O

KDSA

A
T

(cid:19)
for K agents over a time horizon T is provided where S is the number of states and D is the
diameter of the underlying Markov Chain and A is the number of actions available to the
centralized controller. Further, a model free algorithm which can be eﬃciently implemented
using neural networks is also proposed. The proposed algorithms outperform standard
heuristics by a signiﬁcant margin for maximizing fairness in cellular scheduling problem, as
well as for a multiple-queue queueing system.

q

(cid:18)

29

Possible future works include modifying the framework to obtain actions from policies
instead of probability values for inﬁnite action space, and obtaining decentralized policies
by introducing a message passing architecture.

30

References

Mridul Agarwal and Vaneet Aggarwal. Source Code for Non-Linear Reinforcement Learning.

https://github.rcac.purdue.edu/Clan-labs/non-markov-RL, 2019.

Vaneet Aggarwal, Rittwik Jana, Jeﬀrey Pang, KK Ramakrishnan, and NK Shankara-
narayanan. Characterizing fairness for 3g wireless networks. In 2011 18th IEEE Workshop
on Local & Metropolitan Area Networks (LANMAN), pages 1–6. IEEE, 2011.

Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning:
worst-case regret bounds. In Advances in Neural Information Processing Systems, pages
1184–1194, 2017.

Eitan Altman, Konstantin Avrachenkov, and Andrey Garnaev. Generalized α-fair resource
allocation in wireless networks. In 2008 47th IEEE Conference on Decision and Control,
pages 2414–2419. IEEE, 2008.

Jinthana Ariyakhajorn, Pattana Wannawilai, and Chanboon Sathitwiriyawong. A compar-
ative study of random waypoint and gauss-markov mobility models in the performance
evaluation of manet. In 2006 International Symposium on Communications and Infor-
mation Technologies, pages 894–899. IEEE, 2006.

Dimitri P Bertsekas. Dynamic programming and optimal control, volume 1. Athena scientiﬁc

Belmont, MA, 1995.

Daan Bloembergen, Karl Tuyls, Daniel Hennes, and Michael Kaisers. Evolutionary dy-
namics of multi-agent learning: A survey. Journal of Artiﬁcial Intelligence Research, 53:
659–697, 2015.

T Bu, L Li, and R Ramjee. Generalized proportional fair scheduling in third generation
wireless data networks. In Proceedings IEEE INFOCOM 2006. 25TH IEEE International
Conference on Computer Communications, pages 1–12. IEEE, 2006.

S´ebastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations

and Trends® in Machine Learning, 8(3-4):231–357, 2015.

Lucian Bu¸soniu, Robert Babuˇska, and Bart De Schutter. Multi-agent reinforcement learning:
An overview. In Innovations in multi-agent systems and applications-1, pages 183–221.
Springer, 2010.

A Castelletti, Francesca Pianosi, and Marcello Restelli. A multiobjective reinforcement
learning approach to water resources systems operation: Pareto frontier approximation
in a single run. Water Resources Research, 49(6):3476–3486, 2013.

Steven Diamond and Stephen Boyd. Cvxpy: A python-embedded modeling language for

convex optimization. Journal of Machine Learning Research, 17(83):1–5, 2016.

Anis Elgabli, Vaneet Aggarwal, Shuai Hao, Feng Qian, and Subhabrata Sen. Lbp: Ro-
bust rate adaptation algorithm for svc video streaming. IEEE/ACM Transactions on
Networking, 26(4):1633–1645, 2018.

31

Javier Garcıa and Fernando Fern´andez. A comprehensive survey on safe reinforcement

learning. Journal of Machine Learning Research, 16(1):1437–1480, 2015.

Sibsankar Haldar and DK Subramanian. Fairness in processor scheduling in time sharing

systems. ACM SIGOPS Operating Systems Review, 25(1):4–18, 1991.

Harri Holma and Antti Toskala. WCDMA for UMTS.: Radio Access for Third Generation

Mobile Communications. john wiley & sons, 2005.

Marcus Hutter. Extreme state aggregation beyond mdps. In International Conference on

Algorithmic Learning Theory, pages 185–199. Springer, 2014.

Shadi Ibrahim, Hai Jin, Lu Lu, Song Wu, Bingsheng He, and Li Qi. Leen: Locality/fairness-
aware key partitioning for mapreduce in the cloud. In 2010 IEEE Second International
Conference on Cloud Computing Technology and Science, pages 17–24. IEEE, 2010.

Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforce-

ment learning. Journal of Machine Learning Research, 11(Apr):1563–1600, 2010.

Jiechuan Jiang and Zongqing Lu. Learning fairness in multi-agent systems. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances
in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.

Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably
eﬃcient? In Advances in Neural Information Processing Systems, pages 4863–4873, 2018.

Jun-young Kwak, Pradeep Varakantham, Rajiv Maheswaran, Milind Tambe, Farrokh Jaz-
izadeh, Geoﬀrey Kavulya, Laura Klein, Burcin Becerik-Gerber, Timothy Hayes, and
Wendy Wood. Saves: A sustainable multiagent application to conserve building en-
ergy considering occupants. In Proceedings of the 11th International Conference on Au-
tonomous Agents and Multiagent Systems-Volume 1, pages 21–28, 2012.

Raymond Kwan, Cyril Leung, and Jie Zhang. Proportional fair multiuser scheduling in lte.

IEEE Signal Processing Letters, 16(6):461–464, 2009.

Tian Lan, David Kao, Mung Chiang, and Ashutosh Sabharwal. An axiomatic theory of

fairness in network resource allocation. IEEE, 2010.

Lihong Li, Thomas J Walsh, and Michael L Littman. Towards a uniﬁed theory of state

abstraction for mdps. ISAIM, 4:5, 2006.

Xiaoshuai Li, Rajan Shankaran, Mehmet A Orgun, Gengfa Fang, and Yubin Xu. Resource
allocation for underlay d2d communication with proportional fairness. IEEE Transactions
on Vehicular Technology, 67(7):6244–6258, 2018.

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval
Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement
learning. arXiv preprint arXiv:1509.02971, 2015.

Michael L Littman. Markov games as a framework for multi-agent reinforcement learning.

In Machine learning proceedings 1994, pages 157–163. Elsevier, 1994.

32

Chunming Liu, Xin Xu, and Dewen Hu. Multiobjective reinforcement learning: A compre-
hensive overview. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 45
(3):385–398, 2014.

Sultan Javed Majeed and Marcus Hutter. On q-learning convergence for non-markov deci-

sion processes. In IJCAI, pages 2546–2552, 2018.

Robert Margolies, Ashwin Sridharan, Vaneet Aggarwal, Rittwik Jana, NK Shankara-
narayanan, Vinay A Vaishampayan, and Gil Zussman. Exploiting mobility in propor-
tional fair cellular scheduling: Measurements and algorithms. IEEE/ACM Transactions
on Networking (TON), 24(1):355–367, 2016.

R Andrew McCallum.

Instance-based utile distinctions for reinforcement learning with

hidden state. In Machine Learning Proceedings 1995, pages 387–395. Elsevier, 1995.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.

Yurii Nesterov. Lectures on convex optimization, volume 137. Springer.

Thanh Thi Nguyen, Ngoc Duy Nguyen, Peter Vamplew, Saeid Nahavandi, Richard Dazeley,
and Chee Peng Lim. A multi-objective deep reinforcement learning framework. Engineer-
ing Applications of Artiﬁcial Intelligence, 96:103915, 2020.

Norihiko Ono and Kenji Fukumoto. Multi-agent reinforcement learning: A modular ap-
proach. In Second International Conference on Multiagent Systems, pages 252–258, 1996.

Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) eﬃcient reinforcement learning
In Advances in Neural Information Processing Systems, pages

via posterior sampling.
3003–3011, 2013.

Julien Perez, C´ecile Germain-Renaud, Bal´azs K´egl, and Charles Loomis. Responsive elastic
computing. In Proceedings of the 6th international conference industry session on Grids
meets autonomic computing, pages 55–64. ACM, 2009.

John W. Pratt. Risk aversion in the small and in the large. Econometrica, 32(1/2):122–136,

1964. ISSN 00129682, 14680262. URL http://www.jstor.org/stable/1913738.

Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Program-
ming. John Wiley & Sons, Inc., New York, NY, USA, 1st edition, 1994. ISBN 0471619779.

Diederik M. Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. A survey
of multi-objective sequential decision-making. J. Artif. Int. Res., 48(1):67–113, October
2013. ISSN 1076-9757.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust
region policy optimization. In International conference on machine learning, pages 1889–
1897, 2015.

33

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal

policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. arXiv

preprint arXiv:1810.04650, 2018.

Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforce-

ment learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.

Yoav Shoham, Rob Powers, and Trond Grenager. Multi-agent reinforcement learning: a

critical survey. Web manuscript, 2003.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT

press, 2018.

Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 2.

MIT press Cambridge, 1998.

Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy
gradient methods for reinforcement learning with function approximation. In Advances
in neural information processing systems, pages 1057–1063, 2000.

Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents.

In
Proceedings of the tenth international conference on machine learning, pages 330–337,
1993.

Sylvie Thi´ebaux, Charles Gretton, John Slaney, David Price, and Froduald Kabanza.
Decision-theoretic planning with non-markovian rewards. Journal of Artiﬁcial Intelli-
gence Research, 25:17–74, 2006.

Kristof Van Moﬀaert and Ann Now´e. Multi-objective reinforcement learning using sets of
pareto dominating policies. The Journal of Machine Learning Research, 15(1):3483–3512,
2014.

P. Viswanath, D. N. C. Tse, and R. Laroia. Opportunistic beamforming using dumb an-
tennas. IEEE Transactions on Information Theory, 48(6):1277–1294, June 2002. ISSN
0018-9448. doi: 10.1109/TIT.2002.1003822.

Wei Wang, Baochun Li, and Ben Liang. Dominant resource fairness in cloud computing
In IEEE INFOCOM 2014-IEEE Conference on

systems with heterogeneous servers.
Computer Communications, pages 583–591. IEEE, 2014.

Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando
De Freitas. Dueling network architectures for deep reinforcement learning. arXiv preprint
arXiv:1511.06581, 2015.

Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Wein-

berger. Inequalities for the l1 deviation of the empirical distribution. 2003.

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist rein-

forcement learning. Machine learning, 8(3-4):229–256, 1992.

34

Yu Xiang, Tian Lan, Vaneet Aggarwal, and Yih-Farn R Chen. Joint latency and cost opti-
mization for erasure-coded data center storage. IEEE/ACM Transactions on Networking,
24(4):2443–2457, 2015.

Chongjie Zhang and Julie A Shah. Fairness in multi-agent sequential decision-making. In

Advances in Neural Information Processing Systems, pages 2636–2644, 2014.

Chongjie Zhang and Julie A Shah. On fairness in decision-making under uncertainty: Deﬁ-
nitions, computation, and comparison. In Twenty-Ninth AAAI Conference on Artiﬁcial
Intelligence, 2015.

Xu Zhang, Siddhartha Sen, Daniar Kurniawan, Haryadi Gunawi, and Junchen Jiang. E2e:
Embracing user heterogeneity to improve quality of experience on the web. In Proceedings
of the ACM Special Interest Group on Data Communication, SIGCOMM ’19, pages 289–
302, New York, NY, USA, 2019. ACM. ISBN 978-1-4503-5956-6. doi: 10.1145/3341302.
3342089. URL http://doi.acm.org/10.1145/3341302.3342089.

35

Appendix A. Proof of Lemmas

A.1 Proof of Lemma 2

Proof We note that the epoch update condition follows identical to the epoch update
condition of the UCRL algorithm by Jaksch et al. (2010). Hence, we can use the proof of
(Jaksch et al., 2010, Proposition 18) construct the proof if Lemma 2. Let N (s, a) be the
number of times the controller visits state action pair (s, a) till time T . Further, let νe(s, a)
be the number of times the controllor visits state action pair (s, a) in epoch e. Let Ne(s, a)
be the total number of visitations to (s, a) before epoch e starts. Then, we have,

e

Ne(s, a) =

νe′(s, a)

(108)

Xe′=1
The controller triggers change of epoch if νe(s, a) = Ne(s, a). Let E(s, a) be the number

of new epochs triggered because of state action pair (s, a), then we have

E

Ne(s, a) =

νe(s, a)

νi,k(s, a)

Nk(s, a)

Xe:νe(s,a)=Nk(s,a)

Xe:νe(s,a)=Nk(s,a)
E(s,a)

2e−1

e=1
X
1 +

1 +

≥

≥

1 +

≥

e=1
X

= 2E(s,a)

T is the total duration for which the algorithm runs, and is equal to the total visitations to
any state action pair. Hence, we have

T =

N (s, a)

2Ei(s,a)

−

1

(cid:17)

AS

−

≥

s,a (cid:16)
s,a
X
X
AS2Pi Ps,a Ei(s,a)/AS
T
SA

AS log2

+ 1

(cid:18)

(cid:19)

≥

≤

E(s, a)

=

⇒

s,a
X

Now, a new epoch is triggered when νi,k = Nk(s, a) or when Ni,k(s, a) = 0. This gives,

E

≤

1 + 2AS +

Ei(s, a)

1 + AS

≤

s,a
X
2 + log2

(cid:18)

T
SA

(cid:18)

(cid:19)(cid:19)

where Equation (118) follows from T > SA. This completes the proof.

36

(109)

(110)

(111)

(112)

(113)

(114)

(115)

(116)

(117)

(118)

A.2 Proof of Lemma 4

Proof Consider two states s, s′
variable deﬁned as:

∈ S

such that V (s)

≥

V (s′). Also, let τ be a random

Then, we have

t
τ = min
{

≥

1 : st = s′, s1 = s

}

V (s) = lim
T →∞

1
T

T

V (t)(s)

Eπ

t=1
X
1
T

lim
T →∞

T

"
t=τ
X
Eπ
V (s′) + ρτ
V (s′) + ρEπ [τ ]
(cid:2)
(cid:3)
ρEπ [τ ]
ρD

≤

≤

≤

≤

≤

=

⇒

V (s)

−

V (s′)

V (t−τ )(s′) + ρτ

#

(119)

(120)

(121)

(122)

(123)

(124)

(125)

For ρ

≤

1, we get the required result.

A.3 Proof of Lemma 5

Proof From the result of (Weissman et al., 2003), the ℓ1 distance of a probability distri-
bution over S events with n samples is bounded as:

P

s, a)

P (
k
(cid:16)

·|

Thus, for ǫ =

−

q

ˆP (
·|

s, a)

k1 ≥

(2S

−

2) exp

ǫ

(cid:17)

≤

nǫ2
2

−

(cid:18)

≤

(cid:19)

(2S) exp

nǫ2
2

(cid:19)

−

(cid:18)

(126)

2

n(s,a) log(2S20SAt7)

14S
n(s,a) log(2At)

≤

q

≤

q

14S
n(s,a) log(2AT ), we have

P

P (

s, a)

 k

·|

ˆP (
·|

−

s, a)

k1 ≥ s

14S
n(s, a)

log(2At)

! ≤

(2S ) exp

−

(cid:18)

n(s, a)
2

2
n(s, a)

= 2S

1
2S20SAt7
1
20ASt7

=

log(2S20SAt7)

(cid:19)

(127)

(128)

(129)

We sum over the all the possible values of n(s, a) till t time-step to bound the probability

that the event

Et does not occur as:

t

1

20SAt7 ≤

Xn(s,a)=1

1
20SAt6

37

(130)

Finally, summing over all the s, a, we get

P

P (

s, a)

 k

·|

ˆP (
·|

−

s, a)

k1 ≥ s

14S
n(s, a)

log(2At)

s, a

∀

! ≤

1
20t6

(131)

38

