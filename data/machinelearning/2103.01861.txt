Follow Your Nose -
Which Code Smells are Worth Chasing?

Idan Amit1,2,∗

Nili Ben Ezra1

Dror G. Feitelson1

1Dept. Computer Science, The Hebrew University, Jerusalem, Israel
2Acumen Labs, Tel-Aviv, Israel

1
2
0
2

r
a

M
2

]
E
S
.
s
c
[

1
v
1
6
8
1
0
.
3
0
1
2
:
v
i
X
r
a

Abstract—The common use case of code smells assumes causal-
ity: Identify a smell, remove it, and by doing so improve the
code. We empirically investigate their ﬁtness to this use. We
present a list of properties that code smells should have if they
indeed cause lower quality. We evaluated the smells in 31,687
Java ﬁles from 677 GitHub repositories, all the repositories with
200+ commits in 2019. We measured the inﬂuence of smells
on four metrics for quality, productivity, and bug detection
efﬁciency. Out of 151 code smells computed by the CheckStyle
smell detector, less than 20% were found to be potentially causal,
and only a handful are rather robust. The strongest smells deal
with simplicity, defensive programming, and abstraction. Files
without the potentially causal smells are 50% more likely to be
of high quality. Unfortunately, most smells are not removed, and
developers tend to remove the easy ones and not the effective
ones.

previous commit of the same author, for commits on the same
date. Time to ﬁnd a bug is the difference from the previous
modiﬁcation of the ﬁle, a process metric lower bound to the
time from the bug introduction.

Note that all our target metrics are process metrics, unlike
smells which are based on code, making dependency less
likely.

One could measure correlation of the smells and the metrics,
but correlation is not causation. In a world of noisy observa-
tions, it is not easy to show causality. On the other hand, it
is possible to specify properties that are expected to hold if
causality actually exists. We suggest the following properties:

• Predictive power: the ability to use smells to detect low-

I. INTRODUCTION

High quality is desirable in software development. A com-
mon method to improve quality is to identify low-quality code
and improve it. Code smells are often advocated as indicators
of low-quality code. They encapsulate software engineering
knowledge and facilitate large-scale scanning of source code
to identify potential problems [6], [53], [73]. Once found,
removing the smell
is believed to improve the quality of
the code. However, it seems that this approach suffers from
many drawbacks. Code smells have been criticized for not
representing problems of interest [11] and for having low
predictive power [67], [69], [46]. As a result they are not
widely used by developers [9], [40], [24], [67], [38].

The common method to assess causality is controlled exper-
iments. Choose a smell, ﬁnd instances where it occurs, remove
them, and observe the results. However, CheckStyle alone
provides 151 smells and assessing all of them will require
tremendous effort. We use the data on the smells behaviour
‘in nature’ in order to identify the most promising smells. After
that, it is much easier to follow up and validate the causality.
We use a set of metrics in order to measure the inﬂuence
of smells: The corrective commit probability, commit size,
commit duration, and time to ﬁnd bugs. The corrective commit
probability (CCP) estimates the fraction of development effort
that is invested in ﬁxing bugs [3]. Commit size measures
coupling using the average of the number of ﬁles appearing
in commit [80], [2], [3]. Commit duration, the gross time to
complete a commit, computed as the average time from the

* idan.amit@mail.huji.ac.il

quality ﬁles.

• Monotonicity: the smell should be more common in low-

quality ﬁles than in high-quality ﬁles.

• Co-change: an improvement in quality once a smell is

removed.

• Predictive power when the developer is controlled, com-
paring the quality of ﬁles of the same developer differing
by a smell.

• Predictive power when the length is controlled, to avoid
smells that are merely indications of large code volume.

A. Novelty and Contribution

We suggest a novel method to evaluate if a smell might
be causal. We checked these properties for all CheckStyle’s
smells, on a large data set. The results were that less than 20%
of the smells were found to have all the desired properties with
respect to a target metric.

We did this evaluation on four target metrics, representing
quality, productivity, and bug detection efﬁciency. We also
used a random metric as a negative control. Our results agree
with prior work on the importance of simplicity, abstraction,
and defensive programming, as well as infamous smells like
‘IllegalToken’ (go to) and ‘InnerAssignment’ (e.g., assignment
in if statement).

We analyzed the smells that developers currently remove
and show that they focus on easy smells and not in causal
smells.

We identify a small set of smells that might be causal
and therefore their removal might improve the desired metric.
We show that ﬁles lacking them indeed have higher quality,

 
 
 
 
 
 
promising results for developers that will remove them, if these
smells are indeed causal.

II. RELATED WORK

A. Code quality

Metrics are essential for quantitative research and therefore
there is a long history of code-quality metrics research. The
need has motivated the development of several software quality
models (e.g. [16], [25]) and standards.

For some, this term refers to the quality of the software
product as perceived by its users [68]. Capers Jones deﬁned
software quality as the combination of low defect rate and
high user satisfaction [41], [42]. Such a deﬁnition is not
suitable for our needs since we don’t have data about the users’
satisfaction.

A different approach is using industry standards. A typical
example is the ISO/IEC 25010:2011 standard [39]. It includes
metrics like ﬁtness for purpose, satisfaction, freedom from
risk, etc. These metrics might be subjective, hard to measure,
which makes them less suitable to our research goals. Indeed,
studies of the ISO/IEC 9126 standard [36] (the precursor of
ISO/IEC 25010) found it to be ineffective in identifying design
problems [1].

Code smells themselves were suggested as code quality
metrics [26], [73]. Yet, it is debatable whether they indeed
reﬂect real quality problems [1], [11]. One could treat each
code smell as a labeling function and evaluate them using
their relations [4], [65]. Yet, this direction is not trivial and
simpler smell-based methods might be circular.

B. Code Smells

A common method to improve code quality is by refactoring
[26]. Code smells [26], [73] identify code artifacts that are of
low quality, potential for improvement by refactoring.

Smells, as their name indicates, are not necessarily problems
but a warning. Smells are de facto deﬁned by the developers
implementing them. While they capture knowledge, they are
not always validated, a gap that we would like to ﬁll.

Static analysis [56] is analysis that is based on the source
code, as opposed to dynamic analysis in which the code is
being run. Static analysis is a common way to implement code
smells identiﬁcation. CheckStyle, the code smell identiﬁcation
tool that we used, is also based on static analysis.

It seems that developers lost faith in code smells and don’t
tend to use static analysis tools [40], choose refactoring targets
based on smells, consider them in pull request approval [47],
or remove the smells [11], [18].

Defect prediction is the attempt to predict if a ﬁle will
contain bugs [22], [30], [31], [56], [61], [82], [57], [51]. The
common features used for defect prediction are code metrics
and code smells.

There are two main differences between our work and the
common defect prediction. Our work is an easier task since
we don’t try to predict a bug but focus on quality groups. On
the other hand, it is harder since we measure differences in
quality levels, requiring the ability to identify differences.

Our work resembles that of Basili at al. [10] who validated
the K&C metrics [19] as predictors of fault-prone classes. The
difference is that we validate the smells as potential causes and
not as predictors. Couto et al. [21] used Granger causality tests,
showing additional predictive power in time series for future
events [29], in order to predict defects using code metrics.
Our work is also close in spirit to the work of Trautsch et al.
that investigated the evolution of PMD static analyzer alerts
and code quality, operationalized as defect density (defects per
LOC) [72].

III. OUR TARGET METRICS

We aim to measure the inﬂuence of the smells with re-
spect to quality (smells common use), productivity, and bug
detection efﬁciency. The target metrics that we use are the
corrective commit probability (CCP) and coupling for quality
[3], commit duration for productivity, and time to detect bugs
for detection efﬁciency. Our main metric of interest is CCP
since many smells are used to alert on possible bugs. We use
the other metrics in order to understand if smells serve other
goals too and as comparison.

The corrective commit probability (CCP) [3] uses the
intuition that bugs are bad and measures the ratio of the
commits dedicated to bug ﬁxes. Self-admitted technical-debt
[64], developers referring to low-quality, and even swearing
agree with CCP. Other than the code quality, CCP is also
inﬂuenced by detection efﬁciency. For example, as predicted
by Linus’s law [66], popular projects with more eyeballs detect
bugs better and have higher CCP. Bigger projects with more
developers also tend to have higher CCP. In the co-change
property we compare a ﬁle to itself, so these variables are
being controlled. It is also important to note that CCP, like
the other metrics that we use, are process metrics, and not
source code metrics. If we used the same data source that
might lead to dependency between the smells and the metrics
(as we show for length) which might invalidate the results.
Assuming conditional-independence given the concept [15],
[48] between the source code and process metrics, an increase
in the CCP is due to an increase of problems in the source
code.

We chose to divide the metrics into groups of the 25%
‘low-quality’, 25% ‘high-quality’ and the ‘other’ 50% in the
middle. This way the groups’ quality is different and we avoid
problems due to imbalanced data sets [74], [45], [60]. In the
case of CCP, low quality ﬁle are in the spirit of hot spots,
ﬁles with many bugs, [71], [77], [52], [43] and high quality
resemble ﬁles of reduced-risk [59]. Note that the high-quality
group for CCP translates to no bugs, in at least 10 commits
in the given year.

We use the commit size as a coupling metric, the target of
few smells. A commit is a unit of work ideally reﬂecting the
completion of a task. It should contain only the ﬁles relevant
to that task. Many ﬁles needed for a task means coupling.
Therefore, the average number of ﬁles in a commit can be
used as a metric for coupling [80], [2], [3]. This metric agrees
with the way developers think about coupling, being higher

when “coupled” or “coupling” is mentioned by the developer
[3].

We measure productivity using the average gross duration
of ﬁle’s commits. Smelly code is assumed to be harder to
understand and modify, hurting productivity. We compute the
gross duration as the time between a commit to the previous
commit of the developer, on the same day. The requirement
for the same day overcomes large gaps of inactivity of open
source developers that could be misinterpreted as long tasks.
We manually labeled 50 such cases to validate that they ﬁt the
duration needed for the code change. The same-day duration
is highly stable with 0.87 adjacent-years Pearson correlation,
and co-changes with commits per developer and pull requests
per developer. In turn, commits per developer is correlated
with self-rated productivity[55] and team lead perception of
productivity [62]. Commit duration can be applied to a ﬁle by
analyzing its commits and therefore we choose it.

One could measure bug detection efﬁciency by the duration
between the bug inducing commit and the bug ﬁxing commit.
Unreadable code, edge cases, and unexpected behaviour can
lead to hard to detect bugs. In order to identify the bug
inducing commit, one should use a method like the SZZ
algorithm [70] which is not a process metric and requires a
considerable computational effort on our data set. Instead we
use the last time the ﬁle was touched before the bug-ﬁxing
commit. This is a lower bound on the time to ﬁx the bug
since the bug inducing commit is this one or an earlier one.
The adjacent-year repository Pearson correlation is 0.52. In
order to validate the “Time from Previous Update” metric,
we observe the inﬂuence of behaviours that should change
the time needed to identify bugs. Projects that hardly have
tests (less than 1% test ﬁles in commits, allowing few false
positives) ﬁx a bug after 72 day on average, 33% more than
the rest. Similarly to Linus’s law inﬂuence on CCP analysis
[3], the extremely popular projects identify bugs in 51 days
on average, compared to 78 days for the others from the same
organizations (53% higher).

The target metrics implementation has many small details,

listed in the supplementary materials.

As a negative control, a metric with which causality is not
expected, we used a variable uniformly distributed between 1
and 100. Table I shows that random has 4 potentially causal
smells, all with hit rate 0.00 (rounded) and no robust smells.
Using this negative control, we can estimate the expected
noise.

Most smells are designed to alert on potential bugs, hence
most are expected to be aligned with CCP. We observed
good alignment with productivity too, as many smells alert on
aspects like readability (e.g. ‘BooleanExpressionComplexity’,
‘OneStatementPerLine’), which inﬂuence productivity. The
alignment with coupling and detection efﬁciency is rather
low, as with random. Hence their related smells might be
accidental, as further discussed later.

IV. DATA SET CREATION

Our data set starts with all BigQuery GitHub schema
non-redundant1 Java repositories with 200+ commits during
2019, taken from the CCP paper [3]. There are 909 such
repositories. From these we extract all non-test ﬁles with at
least 10 commits during 2019 that we could analyze with the
CheckStyle tool. This led to 31,687 ﬁles from 677 repositories.
For the predictive power analysis, we used commits of
2019 and the code version of January 1st 2019. For temporal
analysis (stability, co-change, smell removal), we used the
commits of 2017, 2018, and 2019 the code versions of January
1st of the relevant year.

The data set mere computation is long. Source code tends
to deviate from language speciﬁcations, and static analyzers
might fail due to such deviations [14]. Therefore running
CheckStyle requires a lot of tuning and monitoring. This was
the main technical constraint in extending the data set into the
past.

We used CheckStyle to analyze the code and identify smells.
This tool currently recognizes 174 smells, 151 appeared 200+
times and are relevant to us. We chose CheckStyle since it
is popular [12], open-source, with many diverse smells. In
addition, its smells tend to be general and not speciﬁc low-
level erroneous patterns. Due to the data set creation effort
we couldn’t use more than one static analyzer. We list in the
threats to validity other static analyzers that could be used for
similar research.

We excluded test ﬁles since their behaviour is different from
that of functional code. We identiﬁed test ﬁles by looking for
the string ‘test’ in the ﬁle path. Berger at el. reported 100%
precision of this classiﬁer, based on a sample of 100 ﬁles [13].

V. DESIRED PROPERTIES OF CODE SMELLS

Though smells might be valuable due to their predictive
power alone,
the implicit assumption is of causality. The
common use is “once you ﬁnd a smell, remove it, in order
to avoid the problems it causes”.

A common approach in causality studies is to build models
of possible relations among all variables [63]. However, smells
usage assumes that a smell occurrence is important on its own,
regardless of other smells. We do the same and treat each smell
separately, aiming for ﬁnding a smell-concept relation and not
relations between smells. This allows us to use properties that
examine each smell on its own and show the expected temporal
behaviour. However, the separate analysis poses a threat of
identifying a non causal smell as causal since it is very related
to a causal smell.

The direction of causality, if it exists, is known. Given
concept C and smell S, one could build models for S → C and
for C → S, and see which is better supported. However, we
know that a smell in the source code might lead to a bug, but
the bugs won’t modify the source code, invalidating C → S.
Due to these assumptions we are left with a simple model for
smells and concepts causality.

1Excluding forks and other related repositories.

We suggest a set of properties that should hold given
causality. The properties are predictive power, monotonicity,
inﬂuence when the
co-change of smell and target metric,
developer is controlled, and inﬂuence when the length is
controlled.

By removing smells that don’t have these properties, we
remain with a group of less than 20% of the 151 original
smells that can be further investigated for causality. The rest
of the smells should be treated with more suspicion.

Note that these properties are not meant to be descriptive of
the developers actual activity, discussed in Section VII. These
properties are derived from the required causality and justiﬁed
due to mathematical reasons.

A. Predictive Power

We start with requiring predictive power. Precision, deﬁned
as P (positive|hit), measures a classiﬁer’s tendency to avoid
false positives. In a software development context, this mea-
sures the probability of actually ﬁnding a low-quality ﬁle when
checking a smell alert.

Precision might be high simply since the positive rate
is high. Precision lift, deﬁned as P recision/P (positive) −
1 = P (positive|hit)−P (positive)
, copes with this difﬁculty and
measures the additional probability of having a true positive
relative to the base positive rate. For example, a useless
random classiﬁer will have precision equal to the positive rate,
but a precision lift of 0.

P (positive)

There are additional supervised learning metrics that we
discuss below. However, the precision lift best captures the
usage of smells and therefore it is a metric of high interest.
When a developer looks for low-quality code to improve,
the precision lift measures the value of using the smell as a
guide rather than going over the source code randomly. High
precision and precision lift make the use of smells effective.
We deﬁne the predictive power property to be the conjunc-

tion of three terms:

• We require a positive precision lift in order to claim that
observing the hits of a smell is better than observing
random ﬁles. This is deliberately the minimal threshold
of improvement.

• We also require that the target metric average given a
smell will be higher than the metric average. Note that
this measures how bad the code is, and not only whether
it is bad.

• We required that the smell appears in 200+ ﬁles since
a small number of cases might make the evaluation
inaccurate. Other than that, a rare smell provides less
opportunities for quality improvement.

B. Monotonicity

In case that a smell causes more problems, we expect it to
appear more in problematic ﬁles. Hence the smell hit rate in
‘high-quality’ ﬁles will be lower than in ‘other’ ﬁles, which
in its turn will be lower than in ‘low-quality’ ﬁles.

Here is the justiﬁcation. Let L be ‘low-quality’ and let S
be smell. If S causes L, then P (L|S) > P (L) > P (L|¬S).

Using Bayes rule P (L|S) = P (S|L)·P (L)
the inequality
,
P (L|S) > P (L) can be rewritten P (S|L)·P (L)
> P (L), which
implies P (S|L) > P (S) as desired. The same derivation for
no smell leads to P (¬S|L) < P (¬S).

P (S)

P (S)

Extending to three groups, if we compare the smell appear-
ance probability in ‘high-quality’, ‘other’, and ‘low-quality’
ﬁles we expect it to be strictly monotonically increasing.

C. Co-Change with Target Metric

In co-change analysis we compare the change of two metrics
on the same entity in two consecutive periods. As with
correlation, co-change of two metrics does not necessarily
imply causality. However, causal relations do imply co-change,
since a change in the causing metric should lead to a change in
the affected metric. Co-change analysis was used in software
engineering to show that coupling improvement co-changes
with CCP improvement [3].

In short, denote the event that metric i improved from one
year to the next by mi↑. The probability P (mj↑ | mi↑) (the
equivalent to precision in supervised learning) indicates how
likely we are to observe an improvement in metric j knowing
of an improvement in metric i. Smell improvement is deﬁned
as a reduction in the smell occurrences relative to the same ﬁle
the year before. Target metric improvement is a reduction in its
value between the years. It might be that we will observe high
precision but it will be simply since P (mj↑) is high. In order
to exclude this possibility, we also observe the precision lift,
P (mj↑ | mi↑)
− 1. Note that these are the same precision and
P (mj↑)
precision lift used in the predictive power, this time applied
on temporal data.

D. Inﬂuence when Controlling the Developer

The developer writing the code inﬂuences it in many ways.
Consider a smell
that has no inﬂuence from a software
engineering point of view, yet due to sociological reasons it
divides the developer community into two groups. If one of
the groups tends to produce code of higher quality, it will look
as if this smell inﬂuences the quality.

The statistical method to avoid this is to control the relevant
variables: Investigate P (Q|S, C) instead of P (Q|S), where
Q is quality, S is smell, and C is community. Note that in
case that we condition on a community that has no inﬂuence
then P (Q|S, C) = P (Q|S), and this property will not be
inﬂuential.

However, we do not have information about the community
or any other confounding variables. A popular solution in psy-
chology is “twin experiments”. Identical twins have the same
biological background, so a difference in their behaviour is
attributed to another difference (e.g., being raised differently).
This idea was used to proﬁle malware of the same developer
[5], and to investigate the project inﬂuence on coupling [3].
For example, if project A is more coupled than project B, a
developer’s coupling in A also is usually higher than in B [3].
Twin experiments thus help to factor out personal style, bi-
ases, and skill, and can neutralize differences in the developer
community and other inﬂuencing factors.

In this paper we consider ﬁles written by the same developer

VI. RESULTS

in the same repository as twins.

E. Inﬂuence when Controlling File Length

The inﬂuence of length on error-proneness is one of the
most established results of software engineering [49], [27]. As
Figure 1 shows, ﬁle of at most 104 lines, the ﬁrst 3 deciles,
have average CCP less than 0.1, which is enough to enter the
top 20% of projects [3].

Doing a co-change analysis of line count and CCP, an im-
provement in line count has a 5% precision lift predicting CCP
improvement, and 6% precision lift predicting a signiﬁcant
CCP improvement of 10 percentage points.

Figure 1. Corrective Commit Probability by Line Count Deciles.

Hence, the appearance of a smell might seem indicative
of low quality simply since the smell is indicative of length,
which in turn is indicative of lower quality [27]. In order to
avoid that, we divide ﬁles into length groups, and require
the property of predictive power when the length group is
controlled. Speciﬁcally, we require precision lift in the short
(ﬁrst 25%, up to 83 lines), medium (25% to 75%), and long
(highest 25%, at least 407 lines) length groups.

Also, we ﬁlter out smells whose Pearson correlation with
line count is higher than 0.5, which is considered to indicate
high correlation. This should have been a relatively easy
requirement since the ‘JavaNCSS’ smell (number of Java
statements above a threshold) has Pearson correlation of 0.45
and the ‘FileLength’ smell (length higher than a threshold) has
a Pearson correlation of 0.44 with the line count. These smells
are basically step functions on length and have lower than 0.5
correlation. Hence, 0.5 correlation is a strong correlation also
in our speciﬁc case. Indeed, 127 smells have lower than 0.5
Pearson with length.

Table I summaries how many smells have each property
with respect to each target metric. ’Robust’ and ’Almost’ are
used for sensitivity analysis, either slightly higher or lower
requirements. In the ’Robust’ we require a precision lift of
10% in predictive power,
length and cochange. In
twins,
’Almost’ we allow a small decrease of -10% precision lift.

We present statistics on smells that appeared in 200+ non-
test ﬁles. The potentially causal smells with respect to CCP
are presented in Table II, for commit duration in Table III, for
detection efﬁciency in Table IV, and for coupling in Table V.
We analyze 31,687 ﬁles that had at least 10 commits during
2019, from 677 repositories.

Smells are extremely stable year to year, even when count-
ing smell occurrences in a ﬁle and not just existence. The
average Pearson correlation of the smells year to year is 0.93.

A. Potentially Causal Smells Investigation

In this section we investigate the resulting potentially causal
smells, and some smells surprisingly not considered as such.
The descriptions are based on the CheckStyle tutorial.

CheckStyle groups the smells by type, which allows to
analyze the ﬁndings with respect to these groups. For CCP,
Table II (Section VI-B) shows that many of the smells are
‘coding’ checks, typically alerting on a local speciﬁc case. The
‘metrics’ smells are more general, representing simplicity and
abstraction. The ‘size’ group also represents simplicity while
its member ‘AnonInnerLength’ also checks abstraction. The
‘class-design’ smell ‘VisibilityModiﬁer’ checks abstraction
too. The ‘import’ and the ‘illegal’ groups (subsets of the
coding group) check that a too wide action is not used.

The ‘whitespace’, ‘JavaDoc comments’, and ‘misc’ repre-
sent smells that the compiler is indifferent to. Despite that,
refactoring of the same spirit were shown to be useful [2].
It might be due to their inﬂuence on the developer,
like
in improving the readability. Note that since we use twin
experiments, it is possible that part of their predictive power
is due to indication about the developer’s behaviour, yet more
inﬂuence exists.

It is interesting that entire groups are not represented at all,
questioning their utility: ‘naming conventions’, ‘annotations’,
‘unnecessary’, ‘block’, ‘header’, ‘modiﬁers’, and ‘regexp’
groups.

Duration potentially causal smells (Table III) come mainly
from ‘coding’, ‘metrics’, ‘naming conventions’ and ‘size vi-
olation’. Yet it is more diverse than the CCP groups and the
indirect groups are more represented.

The results for coupling and detection (Tables IV and V) are
suspicious. Note that they both have a very small number of
potentially causal smells. Each smell comes from a different
group, undermining the hypothesis that a group represents a
more general reason to the inﬂuence. The coupling related
smells do not seem to have anything to do with coupling.
Note that the hit rate of most of them is very small, making
them very sensitive to noise.

Detection efﬁciency is represented by ‘MutableException’
from ‘class design’ group and ‘TypecastParenPad’
from
‘whitespace’ group. While ‘MutableException’ seems like a
source of bugs that are hard to detect, ‘TypecastParenPad’
alerts on the use of white space in casting and seems acci-
dental. In both cases the hit rate is small and rounded to 0.00.

Table I
SMELLS WITH EACH OF THE PROPERTIES

Concept
Random
Duration
Detection
Coupling
CCP

Description
Negative control
Avg. commit duration
Avg. time from modiﬁcation
Avg. # ﬁles in commit
Fix commit probability

Potential
4
20
2
4
27

Robust
0
1
0
0
5

Almost
41
62
7
10
62

Predictive
48
119
19
15
105

Cochange
62
97
63
112
111

Twins Monotonicity
30
73
86
114
15
28
14
122
89
113

Length
32
52
46
28
53

Table II
POTENTIAL CODE SMELLS FOR CCP

Smell

Group

Precision Mean Hit Rate

Recall

Jaccard

Co-change

Twins

NestedTryDepth
FallThrough
EmptyForIteratorPad
InnerAssignment
IllegalThrows
ParameterAssignment
NPathComplexity
MethodParamPad
AnonInnerLength
UnnecessaryParentheses
NestedIfDepth
IllegalCatch
JavaNCSS
AvoidStaticImport
RequireThis
ClassDataAbstractionCoupling
ExecutableStatementCount
JavadocParagraph
TrailingComment
VisibilityModiﬁer
VariableDeclarationUsageDistance
IllegalToken
ExplicitInitialization
HiddenField
EqualsHashCode
AvoidStarImport
WhitespaceAround

Coding
Coding
Whitespace
Coding
Coding
Coding
Metrics
Whitespace
Size Violation
Coding
Coding
Coding
Metrics
Import
Coding
Metrics
Size Violation
JavaDoc Comments
Misc
Class Design
Coding
Coding
Coding
Coding
Coding
Import
Whitespace

0.39
0.39
0.39
0.35
0.34
0.34
0.34
0.34
0.33
0.33
0.33
0.32
0.32
0.32
0.32
0.32
0.32
0.32
0.32
0.31
0.31
0.31
0.30
0.30
0.29
0.29
0.28

0.32
0.29
0.31
0.28
0.24
0.28
0.27
0.26
0.27
0.26
0.27
0.25
0.27
0.26
0.32
0.26
0.26
0.25
0.26
0.25
0.26
0.27
0.24
0.23
0.22
0.24
0.24

0.01
0.01
0.01
0.03
0.00
0.11
0.17
0.02
0.07
0.13
0.20
0.20
0.15
0.21
0.00
0.19
0.20
0.19
0.22
0.23
0.10
0.01
0.13
0.46
0.00
0.15
0.24

0.02
0.01
0.02
0.04
0.00
0.14
0.22
0.03
0.09
0.16
0.25
0.25
0.19
0.26
0.00
0.23
0.25
0.23
0.26
0.27
0.12
0.01
0.15
0.52
0.00
0.17
0.26

0.02
0.01
0.01
0.04
0.00
0.11
0.16
0.03
0.07
0.12
0.17
0.16
0.13
0.17
0.00
0.16
0.16
0.16
0.17
0.17
0.09
0.01
0.11
0.23
0.00
0.12
0.16

0.01
0.16
0.05
0.01
0.16
0.13
0.18
0.30
0.31
0.21
0.24
0.15
0.23
0.16
0.45
0.12
0.20
0.09
0.30
0.14
0.21
0.98
0.18
0.18
1.02
0.06
0.09

0.24
0.54
0.43
0.34
0.93
0.32
0.27
0.28
0.48
0.34
0.31
0.37
0.26
0.31
0.28
0.36
0.25
0.37
0.33
0.12
0.19
0.43
0.21
0.32
0.33
0.27
0.20

Removal
Probability
0.07
0.11
0.09
0.05
0.09
0.05
0.05
0.14
0.09
0.08
0.04
0.04
0.05
0.04
0.13
0.04
0.04
0.10
0.05
0.05
0.09
0.03
0.05
0.03
0.37
0.08
0.13

Table III
POTENTIAL CODE SMELLS FOR DURATION

Smell

Group

Precision Mean

Hit Rate

Recall

Jaccard

Co-change

Twins

InterfaceTypeParameterName
AvoidEscapedUnicodeCharacters
ArrayTypeStyle
FileLength
MethodTypeParameterName
BooleanExpressionComplexity
EmptyForInitializerPad
EmptyCatchBlock
FallThrough
UnnecessarySemicolonInEnumeration
IllegalImport
NestedForDepth
NPathComplexity
OneStatementPerLine
ParameterNumber
JavadocParagraph
TrailingComment
ClassTypeParameterName
NoFinalizer
WriteTag

Naming Conventions
Misc
Misc
Size Violation
Naming Conventions
Metrics
Whitespace
Block
Coding
Coding
Import
Coding
Metrics
Coding
Size Violation
JavaDoc Comments
Misc
Naming Conventions
Coding
JavaDoc Comments

0.48
0.46
0.40
0.40
0.37
0.36
0.36
0.35
0.35
0.33
0.33
0.33
0.32
0.32
0.32
0.31
0.31
0.31
0.29
0.27

146.09
137.42
132.44
134.45
152.63
126.56
136.80
118.11
120.35
133.25
121.88
119.86
118.79
122.85
116.64
113.84
117.03
120.42
109.10
104.99

0.00
0.01
0.02
0.02
0.00
0.05
0.00
0.03
0.01
0.01
0.00
0.02
0.17
0.01
0.08
0.19
0.22
0.00
0.00
0.55

0.00
0.01
0.03
0.04
0.00
0.07
0.00
0.05
0.01
0.01
0.00
0.03
0.23
0.01
0.10
0.24
0.27
0.00
0.00
0.60

0.00
0.01
0.03
0.04
0.00
0.06
0.00
0.04
0.01
0.01
0.00
0.03
0.15
0.01
0.08
0.16
0.17
0.00
0.00
0.23

1.00
0.24
0.08
0.11
0.11
0.12
0.33
0.19
0.09
0.27
0.72
0.12
0.01
0.16
0.11
0.05
0.06
0.50
0.43
0.01

0.60
0.26
0.28
0.14
0.13
0.12
0.10
0.21
0.26
0.09
0.25
0.13
0.10
0.06
0.22
0.15
0.06
0.64
0.49
0.18

Removal
Probability
0.04
0.05
0.10
0.02
0.04
0.06
0.43
0.11
0.11
0.08
0.33
0.08
0.05
0.11
0.05
0.08
0.05
0.02
0.13
0.02

Table IV
POTENTIAL CODE SMELLS FOR DETECTION

Smell

Group

Precision Mean

Hit Rate

Recall

Jaccard

Co-change

Twins

MutableException
TypecastParenPad Whitespace

Class Design

0.43
0.29

81,505.26
37,117.36

0.00
0.00

0.00
0.00

0.00
0.00

0.24
1.48

0.01
0.11

Removal
Probability
0.07
0.21

Table V
POTENTIAL CODE SMELLS FOR COUPLING

Smell

Group

Precision Mean Hit Rate

Recall

Jaccard

Co-change

Twins

LambdaParameterName
IllegalImport
AnnotationUseStyle
LeftCurly

Naming Conventions
Import
Annotation
Block

0.34
0.29
0.26
0.24

20.24
19.99
19.95
19.77

0.00
0.00
0.03
0.15

0.01
0.00
0.03
0.15

0.01
0.00
0.03
0.10

0.14
0.63
0.05
0.24

1.05
0.79
1.06
0.52

Removal
Probability
0.06
0.33
0.08
0.08

Few smells are potential for more than one target metric,

indicating extra validation of their usefulness.

sider abstraction: ‘ClassDataAbstractionCoupling’ , ‘Visibili-
tyModiﬁer’, and ‘AnonInnerLength’.

NPathComplexity [58] was designed based on cyclomatic
complexity [50], checking the number of execution paths, is
potential for CCP and duration. So is ’FallThrough’, dropping
in switch statement options without a break.

’JavadocParagraph’, a JavaDoc formatting smell, and ’Trail-
ingComment’, a comment in a code line, are also potential for
CCP and commit duration. They seem not harmful on their
own but indicative of deviation from a standard.

’IllegalImport’ is potential for duration and coupling. It
alerts on wide imports like ‘import sun.*’, which can lead
to collisions, unintended use of the wrong package and read-
ability problems.

More smells of special interest are the robust ones, which
have higher lift. The robust smells for CCP are ‘AvoidStaticIm-
port’, ‘IllegalCatch’, ‘ParameterAssignment’, ‘Unnecessary-
Parentheses’, and the already discussed ‘NPathComplexity’
[58].

is robust

Defensive programming [79] is programming in a way
to future programming mistakes, and not
that
implementa-
in the current
avoiding only incorrectness
tion.
‘AvoidStaticImport’ allows hiding the relation be-
tween a static method and its source (e.g., ‘import static
java.lang.Math.pow;’ and calling pow instead of Math.pow),
which is a source of confusion. ‘ParameterAssignment’ disal-
lows assignment of parameters. ‘IllegalCatch’ alerts on the use
of a too general class in catch (e.g., Error, Exception). Besides
the robust defensive smells, pay attention to ‘IllegalToken’, a
structured variant of ‘go to’, the most notorious construct in
software engineering [23]. Also, ‘InnerAssignment’ “Checks
for assignments in subexpressions, such as in String s =
Integer.toString(i = 2);”, is causing bugs since the early days
of C. ‘UnnecessaryParentheses’ is harmless on its own but
might indicate misunderstanding of the developer.

The only robust

smell with respect

to duration is
‘AvoidEscapedUnicodeCharacters’, which advocates a read-
able representation of Unicode. That might hurt understanding
the text.

Another interesting aspect is that some of the smells con-

Coupling and detection had no robust smells, making our

suspicion in the result stronger.

There were smells that we were surprised by their failure
to be potentially causal for CCP. In the easier to under-
stand cases, some smells didn’t have a positive precision lift
when conditioned on a certain length group value. These are
‘TodoComment’ (whose removal was shown beneﬁcial [2]),
the classic ‘CyclomaticComplexity’ [50] (ancestor of NPath-
Complexity) and ‘MagicNumber’. This might be accidental
since we conducted many experiments and some smells have a
low hit rate. It is also possible that a smell is indeed beneﬁcial
but only on a certain length group.

Harder cases are smells that don’t have the predictive or co-
change property. This is more disturbing since these properties
are the basic assumptions in smells usage. We were surprised
that
the smells ‘BooleanExpressionComplexity’ and ‘Con-
stantName’ didn’t have a positive co-change precision lift,
and ‘SimplifyBooleanExpression’ had a negative predictive
precision lift.

It was also surprising to notice that some smells designed
for coupling are not potentially causal for it. These are ‘Class-
DataAbstractionCoupling’ [8] with -35% predictive precision
lift, and ‘ClassFanOutComplexity’ [33] with -27% predictive
precision lift.

In order to evaluate the sensitivity of the properties, we
present in Table I smells that are ‘Robust’ and ‘Almost’ smells.
Even with the generous ‘Almost’ setting, about two thirds of
the smells are not included. Requiring robust properties leads
to only a handful of smells.

Another way to perform sensitivity analysis is to count the
number of properties they have. The CCP potentially causal
smells, 27 of them, have all 5 properties. 49 smells have
4 properties, 31 smells have 3 properties, 31 smells have 2
properties, 13 smells have 1 property and 2 have none.

Out of the smells having 4 properties, 73% lack the length
property, 12% lack the co-change property, 10% lack the twins,
4% lack the monotonicity, and none lack predictive power.

B. Smells Inﬂuence

VII. ACTED UPON SMELLS

Supervised learning metrics shed light on the common
behaviour of a classiﬁer and a concept. However, a metric
should be used with respect to a need. We explain to which
software engineering needs each metric that we use here
corresponds.

The cases in which the concept is true are called ‘positives’
and the positives rate is denoted P (positive). Cases in which
the classiﬁer is true are called ‘hits’ and the hit rate is P (hit).
A high hit rate of a smell means that the developer will have
to examine many ﬁles.

Ideally, hits correspond to positives but usually they differ.
Precision, discussed above, is one example of a metric for such
differences.

Recall, deﬁned as P (hit|positive), measures how many of
the positives are also hits; in our case, this is how many of the
low quality ﬁles a smell identiﬁes. Recall is of high importance
to developers who aim to remove all low quality.

Accuracy, P (positive = hit) is probably the most common
supervised learning metric. However, it is misleading when
there is a large group of null records, samples that are
neither hits nor positives. For example, the vast majority of
programs are benign, so always predicting benign will have
high accuracy but detect no malware [60]. True Positives
(TP, correct hits), False Positives (FP, wrong hits), and False
Negatives (FN, unidentiﬁed positives) are the cases in which
either the concept or the classiﬁer are true—without the null
records. The Jaccard index copes with the problem of null
records by omitting them, computing

T P
T P +F P +F N .

Tables II, III, V, and IV presents the above statistics for the
robust smells, ordered by precision. The concept is being low-
quality, and the precision, recall, and Jaccard are computed
with respect to this concept. The mean (of the target metric)
is not a supervised learning metric. It doesn’t measure if the
quality is bad but how bad it is (the higher, the worse in our
target metrics). Co-change and Twins present the precision lift
in these properties. Removal probability is the probability of
a smell existing in a ﬁle in one year to be removed in the year
after it, further discussed in section VII.

The strongest result observed is the poor performance of
even the potentially causal smells. The highest precision is
less than 50%, implying at least 1 mistake per successful
identiﬁcation, and the common results are around 30%, not
much more than the 25% expected from a random guess. Such
performance quickly leads to mistrust.

The hit rate of most smells is low, possibly since developers
already consider them to be a bad practice and avoid them in
the ﬁrst place.

The recall and the Jaccard index, aggregating the precision
and recall, is also very low for all potentially causal smells.
Thus, these smells are of limited value either for developers
looking for easy wins (precision oriented) or those aiming to
win the war (recall oriented).

Smells are usually not translated into action, due to reasons
like low predictive power, lack of trust, or just not being
aware of them. Identifying actionable alerts adds value to the
developer. On the other hand, a developer taking action upon
an alert is implicitly approving it [78], [32], [67].

We used the developers “wisdom of the crowd” to check
which smells are acted upon. We did this by checking the
probability of a smell type existing in a ﬁle in one year to
be removed in the next year. According to Imtiaz et al. the
median time required for acting on an alert is 96 days [37]. We
require that all the smells of this type be removed as indication
of intent. On the other hand, if an entire method was removed
(e.g., due to deprecation), we will consider its smells as being
removed too.

In prior properties we evaluated smells with respect
to
code quality. In this experiment, we evaluate the importance
attributed to the smells by the developers and the code quality
is not used.

Table VI, presenting most removed smells with 200+ re-
moval opportunities, agree with prior work that smells are
generally not acted upon [37], [44], [72]. The leading smell is
‘UnusedImports’, which is a common best-practice, with 42%
removal probability.

Removal probability continues to decrease rapidly, and the
vast majority of smells are unlikely to be removed. The
potentially causal smells with the highest removal probability
on 200+ cases are ‘EqualsHashCode’ with 37% and ‘Method-
ParamPad’ with 14%.

The lack of effort to remove smells is an indication of
disbelief in the value of investing in this activity. Most
acted upon smells are usually supported by IDEs and even
automatic source code formatting utilities. Hence, not only
that developers don’t tend to act up on smells, smells that are
acted upon are the easy ones, not the ones likely to provide
beneﬁt.

VIII. MODELING

We checked if the combined power of the smells is higher
than each of them alone. For each metric, we checked the
probability of being either high-quality or low-quality, given
that none of the potentially causal smells appear in the ﬁle (see
Table VII). Note that in this section we no longer investigate
causality but modeling, as we observe the smells existence.
If the smells are causal, their removal is expected to lead to
what we observe.

For CCP the gap is very high, and the probability of being
high-quality is 44%, with a lift of 50%. Note that the high-
quality CCP means no bugs (at least in 10 commits, real
probability might be somewhat higher), making the result
outstanding. When we control for length the precision lift is
only 1% for the already good short ﬁles, 32% for medium,
and 59% for long.

In order to know the relative power of the CCP result, we
compared it to a machine learning model for high quality
using all smells. First, we reach over-ﬁtting deliberately when

Table VI
ACTED UPON SMELLS INFLUENCE (200+ REMOVAL OPPORTUNITIES, AT LEAST 0.15)

Smell
UnusedImports
EqualsHashCode
AtclauseOrder
GenericWhitespace
NoWhitespaceBefore
EmptyStatement
MissingDeprecated
ParenPad
ArrayTrailingComma
CommentsIndentation
InvalidJavadocPosition
SingleSpaceSeparator

Group
Import
Coding
JavaDoc Comments
Whitespace
Whitespace
Coding
Annotation
Whitespace
Coding
Misc
JavaDoc Comments
Whitespace

Precision Mean Hit Rate
0.22
0.26
0.22
0.29
0.17
0.22
0.22
0.29
0.23
0.29
0.22
0.26
0.23
0.26
0.24
0.30
0.23
0.29
0.25
0.31
0.24
0.33
0.24
0.29

0.06
0.00
0.05
0.01
0.03
0.01
0.03
0.08
0.02
0.08
0.03
0.12

Recall
0.06
0.00
0.04
0.01
0.04
0.01
0.03
0.09
0.02
0.09
0.03
0.13

Jaccard
0.05
0.00
0.04
0.01
0.03
0.01
0.02
0.08
0.02
0.08
0.03
0.10

Co-change
0.23
1.02
-0.17
0.17
0.32
0.05
0.40
-0.04
0.67
0.15
0.13
0.08

Twins
0.33
0.33
-0.48
0.64
0.07
-0.01
0.21
0.19
0.17
0.18
-0.06
0.22

Removal Probability
0.42
0.37
0.29
0.25
0.23
0.21
0.17
0.16
0.15
0.15
0.15
0.15

in order to
training and evaluating on the same data set
evaluate the model representation power of the data [7].
It reached accuracy of 98%, a good representation. When
training and evaluating on different sets, it reached accuracy of
74% and precision (equivalent of high-quality probability) of
71%. Hence the descriptive model has precision 75% higher
than the group, basing on indicative smells that might not be
causal. When we built a model with only the potentially causal
smells, we reached over-ﬁtting accuracy of just 71%, and test
set precision of 54%, 22% higher than using the entire set.

The gap for duration is quite moderate. For detection there
is no gap and for coupling it
is in the reverse direction,
supporting the hypothesis that the smells are not inﬂuential
with respect to them. Note that coupling and duration had
very few potentially causal smells, which also contributes to
the inﬂuence of their groups. 86% of the ﬁles have no coupling
smells and almost none have detection smells.

Table VII
SMELLS GROUPS INFLUENCE

Metric
CCP
Coupling
Detection
Duration

Hit Rate High Quality
0.15
0.86
1.00
0.27

0.44
0.22
0.25
0.25

Low Quality
0.24
0.24
0.25
0.22

For the predictive analysis of the smells we used data from
2019. In order to avoid evaluation on the same data we used
2018 in this section. Yet, we alert on some data leakage. Some
of it is due to co-change that uses prior years data. More
leakage comes from the stability of the smells and the metrics.
The ﬁle CCP has adjacent-year Pearson correlation of 0.45,
and most smells are even more stable, with average Pearson
correlation of 0.93. This stability means that the train and test
samples are similar, very close to our over-ﬁtting setting. Note
that this leakage is common in defect prediction and might
explain why cross-project defect-prediction is harder than
within-project defect-prediction, resulting in lower predictive
power [81].

IX. THREATS TO VALIDITY
Our data set contains only Java projects, simplifying its con-
struction and taking language differences out of the equation.

But this raises a question whether the results hold for other
languages too.

We used a single code smell detector, CheckStyle. Other
tools (e.g., FindBugs [14], SonarCube [17], Coverity [14],
[37], PMD [72], Klocwork, and DECOR [54]) offer different
smells, some of which might be causal.

A basic threat is that a smell might be useful, yet we might
fail to identify it due to insensitivity of the target metric or
computational limitations. Indeed, the concept of computa-
tional indistinguishably teach us of different constructs that
cannot be differed in polynomial time [28]. However, for a
practitioner such a smell is (indistinguishable from) a useless
smell and there is no reason to invest time in its removal. On
the other hand, we know that smells that alert on bugs do
change metrics as those that we use so this threat is mainly
theoretical.

Smell removals are not always small atomic actions. Com-
mits might be tangled [35], [34] and serve more than one goal.
Even when serving one goal, a smell might be removed as part
of a large clean up, along with more inﬂuential smells. It is
also possible that while one smell was removed, a different
smell was introduced. In principle, this can be taken care off
by controlling all other smells. However, controlling all other
smells will be too restrictive in a small data set, leading to
basing the statistics on very few cases. In the same spirit,
while we controlled length and the developer, there might be
other confounding variables inﬂuencing the results.

In the acted upon smells, we know that a smell was not
removed but we don’t know why. The developer might not
use CheckStyle and was not necessarily aware of the smell.
It is also possible that developers consider smell removal as
less important than their other tasks. Another threat might be
when the smell is considered important yet the ﬁle itself is
not considered important (e.g., code that is no longer run, or
unimportant functionality). Since we require 10 commits in a
year, the ﬁle might be unimportant, yet it is still maintained.
The data set contains only ﬁles on which we could run
CheckStyle. Using uncommon syntax can make static analyz-
ers fail [14], so one shouldn’t assume that the omitted ﬁles
behave like those in our data set. We also restricted our ﬁles
to be ﬁles with at least 10 commits. We did so since estimating

the quality based on less commits will be very noisy. While
812 projects had such ﬁles, these ﬁles are only 26% of the ﬁles.
It is possible that in ﬁles in which the number of commits is
small, the behaviour of smells is different.

One could use other target metrics or even aim for different
software engineering goals, obtaining different results, and
identifying different smells of interest. We used four metrics
in order not to be governed by a single one.

We analyzed the source code on a speciﬁc date and used the
commits done in an entire year in order to measure quality. It is
possible that we identiﬁed a smell in January, and a developer
removed it in February, making it irrelevant for the rest of the
year. But we showed that smells are very stable, indicating
that this scenario is rare.

Our classiﬁcation of ﬁles into ‘low-quality’, ‘high-quality’
and ‘other’ are necessarily noisy. As an illustrative example
of the implication of a small number of commits, consider a
ﬁle with CCP of 0.1, a probability of 10% that a commit will
be a bug ﬁx. The ﬁle has probability of 35% to have no hit
in 10 commits and it will be mis-classiﬁed as ‘high-quality’.
On top of that, the VC dimension theory [76], [75] warns us
that given a ﬁxed size data set, the more statistics we compute,
the more likely that some of them will deviate from the true
underlying probabilities. We started with 151 smells and for
each one of them we computed ﬁve properties for four target
metrics, relying on even more statistics. This might lead to
missing robust smells or misidentifying a smell as robust.
Note that a random smell has a low probability to achieve
all properties. On the other hand, doing a lot of statistical
tests, some will fail. Some causal smells that have the desired
properties might accidentally fail on our data. Examining these
properties on more ﬁles will help to further validate the results.
Another threat comes from the use of the linguistic model
in order to identify corrective actions. The linguistic model
accuracy is 93%, close to that of human annotators [3]. A
misclassiﬁcation of a commit as a bug ﬁx might change a ﬁle
quality group.

Projects differ in their quality properties and their reasons
[3]. Popular projects have higher bug detection efﬁciency, as
predicted by Linus’s law. However, we have relative properties
like co-change and twins where the project properties are ﬁxed.
The suitability of the properties to our needs pose a threat
on the suitability of the results. Aiming for causality existence,
we required positive inﬂuence and ignored the effect size. One
can argue that a smell with high effect size is interesting even
if it lacks other properties. We also examined overall precision,
neglecting the possibility of conditional beneﬁt. For example,
defensive programming smells might be valuable if a mistake
will be done in the future. However, if this mistake won’t be
done, the value of the defensive act won’t be captured in our
properties.

X. FUTURE WORK

The smells that we examined represent a lot of knowledge
regarding software engineering. It is possible that the concept
that a smell attempts to capture is indeed indicative and

inﬂuential, yet its smells are not suitable. For example, many
smells are parameterized. Cyclomatic Complexity [50] might
be a causal smell, once the threshold of complexity is set to
the right value. Our data set can help in ﬁnding the optimal
value.

Low-quality ﬁles lacking important smells are hints for
developing new smells, capturing hitherto unrecognized prob-
lems. Defect prediction is usually based on smells, and smells
are usually hand-crafted by experts. Given the data set of high
and low quality ﬁles, one can try an “end-to-end” approach and
build models whose input is the source code itself. The “end-
to-end” approach showed value in many ﬁelds of machine
learning and also in the domain of software engineering [20].
We did a large step from mere correlation, yet we didn’t
prove a causality relation between smells and the target
metrics. Our data set provides intervention opportunities. One
can choose a smell of interest and retrieve its occurrences.
Then deliberately modify the code to remove the smell, either
by a suggestion to the author or directly. Such experiments
are the classical causality testbed and given our data they can
be focused and easier to conduct. In such experiments we
will know that the removal targets were not chosen due to a
confounding variable that also inﬂuences the quality. Avoiding
complex modiﬁcations will remove the threat that the smell
removal was a by-product of a complex change, changing the
quality due to different reasons.

XI. CONCLUSIONS

Identiﬁcation of causes for low quality is an important step
in quality improvement. We showed that many code smells
do not seem to cause low quality. That can explain why
developers tend not to act upon smells.

We did identify smells that have properties expected from
low quality causes. For researchers, we presented a method-
ology for identifying possibly causal variables. Out of 151
candidate smells, we found that
less than 20% have our
properties and may have a causal relationship with quality.

For developers, we ﬁrst

justify their mistrust of most
smells. We also suggest a small set of interesting smells and
basic software engineering guidelines: simplicity, defensive
programming, and abstraction.

Files that have no potential CCP smells have 44% proba-
bility to have no bugs and show improvement even when the
length is controlled. While we still didn’t prove causality, the
results indicate a stronger relation between these smells and
improved quality, making the decision to act upon them more
justiﬁable.

SUPPLEMENTARY MATERIALS

The language models are available at https://github.
com/evidencebp/commit-classiﬁcation and the analysis util-
ities (e.g., co-change) are at https://github.com/evidencebp/
analysis utils, both from [3]. Database contruction code is
at https://github.com/evidencebp/general. All other supplemen-
tary materials can be found at https://github.com/evidencebp/
follow-your-nose.

ACKNOWLEDGEMENTS

This research was supported by the ISRAEL SCIENCE
FOUNDATION (grant No. 832/18). We thank Udi Lavi,
Daniel Shir, Yinnon Meshi, and Millo Avissar for their help,
discussions and insights.

REFERENCES

[1] H. Al-Kilidar, K. Cox, and B. Kitchenham. The use and usefulness of
the ISO/IEC 9126 quality standard. In Intl. Synp. Empirical Softw. Eng.,
pages 126–132, Nov 2005.

[2] I. Amit and D. G. Feitelson. Which refactoring reduces bug rate? In
Proceedings of the Fifteenth International Conference on Predictive
Models and Data Analytics in Software Engineering, PROMISE’19,
page 12–15, New York, NY, USA, 2019. Association for Computing
Machinery.

[3] I. Amit and D. G. Feitelson. The corrective commit probability code

quality metric. ArXiv, abs/2007.10912, 2020.

[4] I. Amit, E. Firstenberg, and Y. Meshi. Framework for semi-supervised
learning when no labeled data is given. U.S. patent application
#US20190164086A1, 2017.

[5] I. Amit, J. Matherly, W. Hewlett, Z. Xu, Y. Meshi, and Y. Weinberger.
Machine learning in cyber-security - problems, challenges and data sets,
2019.

[6] F. Arcelli Fontana, V. Ferme, A. Marino, B. Walter, and P. Martenka.
Investigating the impact of code smells on system’s quality: An empirical
study on systems of different application domains. pages 260–269, 09
2013.

[7] D. Arpit, S. Jastrze¸bski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal,
T. Maharaj, A. Fischer, A. Courville, Y. Bengio, et al. A closer look
at memorization in deep networks. arXiv preprint arXiv:1706.05394,
2017.

[8] A. Asad and I. Alsmadi. Design and code coupling assessment based
on defects prediction. part 1. Computer Science Journal of Moldova,
62(2):204–224, 2013.

[9] N. Ayewah, W. Pugh, D. Hovemeyer, J. D. Morgenthaler, and J. Penix.
Using static analysis to ﬁnd bugs. IEEE Software, 25(5):22–29, 2008.
[10] V. R. Basili, L. C. Briand, and W. L. Melo. A validation of object-
IEEE Transactions on

oriented design metrics as quality indicators.
Software Engineering, 22(10):751–761, 1996.

[11] G. Bavota, A. De Lucia, M. Di Penta, R. Oliveto, and F. Palomba. An
experimental investigation on the innate relationship between quality and
refactoring. J. Syst. & Softw., 107:1–14, Sep 2015.

[12] M. Beller, R. Bholanath, S. McIntosh, and A. Zaidman. Analyzing
the state of static analysis: A large-scale evaluation in open source
In 2016 IEEE 23rd International Conference on Software
software.
Analysis, Evolution, and Reengineering (SANER), volume 1, pages 470–
481, 2016.

[13] E. D. Berger, C. Hollenbeck, P. Maj, O. Vitek, and J. Vitek. On the im-
pact of programming languages on code quality. CoRR, abs/1901.10220,
2019.

[14] A. Bessey, K. Block, B. Chelf, A. Chou, B. Fulton, S. Hallem, C. Henri-
Gros, A. Kamsky, S. McPeak, and D. Engler. A few billion lines of code
later: Using static analysis to ﬁnd bugs in the real world. Commun. ACM,
53(2):66–75, Feb 2010.

[15] A. Blum and T. Mitchell. Combining labeled and unlabeled data with
In Proceedings of the Eleventh Annual Conference on
co-training.
Computational Learning Theory, COLT’ 98, pages 92–100, New York,
NY, USA, 1998. ACM.

[16] B. W. Boehm, J. R. Brown, and M. Lipow. Quantitative evaluation of
software quality. In Intl. Conf. Softw. Eng., number 2, pages 592–605,
Oct 1976.

[17] G. Campbell and P. P. Papapetrou. SonarQube in action. Manning

Publications Co., 2013.

[18] D. Cedrim, A. Garcia, M. Mongiovi, R. Gheyi, L. Sousa, R. de Mello,
B. Fonseca, M. Ribeiro, and A. Ch´avez. Understanding the impact of
refactoring on smells: A longitudinal study of 23 software projects. In
Proceedings of the 2017 11th Joint Meeting on Foundations of Software
Engineering, pages 465–475, 2017.

[19] S. R. Chidamber and C. F. Kemerer. A metrics suite for object oriented

design. IEEE Trans. Softw. Eng., 20(6):476–493, Jun 1994.

[20] M. Choetkiertikul, H. K. Dam, T. Tran, T. Pham, A. Ghose, and

T. Menzies. A deep learning model for estimating story points, 2016.

[21] C. Couto, P. Pires, M. T. Valente, R. S. Bigonha, and N. Anquetil.
Predicting software defects with causality tests. Journal of Systems and
Software, 93:24–41, 2014.

[22] M. D’Ambros, M. Lanza, and R. Robbes. An extensive comparison of
bug prediction approaches. In 2010 7th IEEE Working Conference on
Mining Software Repositories (MSR 2010), pages 31–41, May 2010.

[23] E. W. Dijkstra. Letters to the editor: go to statement considered harmful.

Communications of the ACM, 11(3):147–148, 1968.

[24] L. N. Q. Do, J. Wright, and K. Ali. Why do software developers
use static analysis tools? a user-centered study of developer needs and
motivations. IEEE Trans. Softw. Eng., 2020.

[25] G. Dromey. A model for software product quality. IEEE Trans. Softw.

Eng., 21(2):146–162, Feb 1995.

[26] M. Fowler.

Refactoring: Improving the Design of Existing Code.
Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA,
1999.

[27] Y. Gil and G. Lalouche. On the correlation between size and metric

validity. Empirical Softw. Eng., 22(5):2585–2611, Oct 2017.

[28] O. Goldreich. A note on computational indistinguishability. Information

Processing Letters, 34(6):277–281, 1990.

[29] C. W. Granger.

Investigating causal relations by econometric models
and cross-spectral methods. Econometrica: journal of the Econometric
Society, pages 424–438, 1969.

[30] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Counsell. A systematic
literature review on fault prediction performance in software engineer-
ing. IEEE Trans. Softw. Eng., 38(6):1276–1304, Nov 2012.

[31] A. E. Hassan. Predicting faults using the complexity of code changes.
In 2009 IEEE 31st International Conference on Software Engineering,
pages 78–88, 2009.

[32] S. Heckman and L. Williams. A systematic literature review of action-
able alert identiﬁcation techniques for automated static code analysis.
Information and Software Technology, 53(4):363–387, 2011.

[33] S. Henry and D. Kafura. Software structure metrics based on information

ﬂow. IEEE Trans. Softw. Eng., SE-7(5):510–518, Sep 1981.

[34] S. Herbold, A. Trautsch, B. Ledel, A. Aghamohammadi, T. A. Ghaleb,
K. K. Chahal, T. Bossenmaier, B. Nagaria, P. Makedonski, M. N.
Ahmadabadi, K. Szabados, H. Spieker, M. Madeja, N. Hoy, V. Lenar-
duzzi, S. Wang, G. Rodr´ıguez-P´erez, R. Colomo-Palacios, R. Verdec-
chia, P. Singh, Y. Qin, D. Chakroborti, W. Davis, V. Walunj, H. Wu,
D. Marcilio, O. Alam, A. Aldaeej, I. Amit, B. Turhan, S. Eismann, A.-
K. Wickert, I. Malavolta, M. Sulir, F. Fard, A. Z. Henley, S. Kourtzanidis,
E. Tuzun, C. Treude, S. M. Shamasbi, I. Pashchenko, M. Wyrich,
J. Davis, A. Serebrenik, E. Albrecht, E. U. Aktas, D. Str¨uber, and
J. Erbel. Large-scale manual validation of bug ﬁxing commits: A ﬁne-
grained analysis of tangling. arXiv:2011.06244 [cs.SE], 2020.

[35] K. Herzig and A. Zeller. The impact of tangled code changes. In 2013
10th Working Conference on Mining Software Repositories (MSR), pages
121–130, 2013.

[36] I. IEC. 9126-1 (2001). software engineering product quality-part 1:
Quality model. International Organization for Standardization, page 16,
2001.

[37] N. Imtiaz, B. Murphy, and L. Williams. How do developers act on static
analysis alerts? an empirical study of coverity usage. In 2019 IEEE 30th
International Symposium on Software Reliability Engineering (ISSRE),
pages 323–333. IEEE, 2019.

[38] N. Imtiaz, A. Rahman, E. Farhana, and L. Williams. Challenges with
In 2019 IEEE/ACM 16th
responding to static analysis tool alerts.
International Conference on Mining Software Repositories (MSR), pages
245–249, 2019.

[39] International Organization for Standardization. Systems and software
engineering – systems and software quality requirements and evaluation
(square) – system and software quality models, 2011.

[40] B. Johnson, Y. Song, E. Murphy-Hill, and R. Bowdidge. Why don’t
software developers use static analysis tools to ﬁnd bugs? In 2013 35th
International Conference on Software Engineering (ICSE), pages 672–
681, 2013.

[41] C. Jones. Applied Software Measurement: Assuring Productivity and

Quality. McGraw-Hill, Inc., New York, NY, USA, 1991.

[42] C. Jones. Software quality in 2012: A survey of the state of the art,

2012. [Online; accessed 24-September-2018].

[43] Y. Kamei, S. Matsumoto, A. Monden, K. i. Matsumoto, B. Adams, and
A. E. Hassan. Revisiting common bug prediction ﬁndings using effort-
In 2010 IEEE International Conference on Software
aware models.
Maintenance, pages 1–10, Sept 2010.

[68] N. F. Schneidewind. Body of knowledge for software quality measure-

ment. Computer, 35(2):77–83, Feb 2002.

[69] H. Shen, J. Fang, and J. Zhao. Eﬁndbugs: Effective error ranking for
ﬁndbugs. In 2011 Fourth IEEE International Conference on Software
Testing, Veriﬁcation and Validation, pages 299–308, 2011.

[70] J. ´Sliwerski, T. Zimmermann, and A. Zeller. When do changes induce

ﬁxes? SIGSOFT Softw. Eng. Notes, 30(4):1–5, May 2005.

[71] A. Tosun, A. Bener, and R. Kale. Ai-based software defect predictors:
Applications and beneﬁts in a case study. AI Magazine, 32:57–68, 06
2011.

[72] A. Trautsch, S. Herbold, and J. Grabowski. A longitudinal study of static
analysis warning evolution and the effects of pmd on software quality
in apache open source projects. arXiv preprint arXiv:1912.02179, 2019.
Java quality assurance by detecting
In Ninth Working Conference on Reverse Engineering,

[73] E. van Emden and L. Moonen.

code smells.
2002. Proceedings., pages 97–106, Nov 2002.

[74] J. Van Hulse, T. M. Khoshgoftaar, and A. Napolitano. Experimental
perspectives on learning from imbalanced data. In Proceedings of the
24th international conference on Machine learning, pages 935–942,
2007.

[75] V. Vapnik. The nature of statistical learning theory. Springer science

& business media, 2013.

[76] V. Vapnik and A. Chervonenkis. On the uniform convergence of relative
frequencies of events to their probabilities. Theory of Probability & Its
Applications, 16(2):264–280, 1971.

[77] N. Walkinshaw and L. Minku. Are 20% of ﬁles responsible for 80% of
defects? In Proceedings of the 12th ACM/IEEE International Symposium
on Empirical Software Engineering and Measurement, ESEM ’18, pages
2:1–2:10, New York, NY, USA, 2018. ACM.

[78] X. Yang, J. Chen, R. Yedida, Z. Yu, and T. Menzies. How to recognize

actionable static code warnings (using linear svms), 2020.

[79] M. Zaidman. Teaching defensive programming in java. J. Comput. Sci.

Coll., 19(3):33–43, Jan 2004.

[80] T. Zimmermann, S. Diehl, and A. Zeller. How history justiﬁes system
architecture (or not). In Sixth International Workshop on Principles of
Software Evolution, 2003. Proceedings., pages 73–83, Sept 2003.
[81] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy. Cross-
project defect prediction: A large scale experiment on data vs. domain
vs. process. In Proceedings of the 7th Joint Meeting of the European
Software Engineering Conference and the ACM SIGSOFT Symposium on
The Foundations of Software Engineering, ESEC/FSE ’09, page 91–100,
New York, NY, USA, 2009. Association for Computing Machinery.
[82] T. Zimmermann, R. Premraj, and A. Zeller. Predicting defects for
eclipse. In Proceedings of the Third International Workshop on Predictor
Models in Software Engineering, PROMISE ’07, page 9, USA, 2007.
IEEE Computer Society.

[44] S. Kim and M. D. Ernst. Which warnings should i ﬁx ﬁrst?

In
Proceedings of the the 6th Joint Meeting of the European Software
Engineering Conference and the ACM SIGSOFT Symposium on The
Foundations of Software Engineering, ESEC-FSE ’07, page 45–54, New
York, NY, USA, 2007. Association for Computing Machinery.

[45] B. Krawczyk. Learning from imbalanced data: open challenges and
future directions. Progress in Artiﬁcial Intelligence, 5(4):221–232, 2016.
[46] T. Kremenek and D. Engler. Z-ranking: Using statistical analysis to
counter the impact of static analysis approximations. In International
Static Analysis Symposium, pages 295–315. Springer, 2003.

[47] V. Lenarduzzi, V. Nikkola, N. Saarim¨aki, and D. Taibi. Does code quality

affect pull request acceptance? an empirical study, 2019.

[48] D. D. Lewis. Naive (bayes) at forty: The independence assumption in
information retrieval. In C. N´edellec and C. Rouveirol, editors, Machine
Learning: ECML-98, pages 4–15, Berlin, Heidelberg, 1998. Springer
Berlin Heidelberg.

[49] M. Lipow. Number of faults per line of code. IEEE Transactions on

software Engineering, (4):437–439, 1982.

[50] T. McCabe. A complexity measure.

IEEE Trans. Softw. Eng., SE-

2(4):308–320, Dec 1976.

[51] T. Menzies, J. Greenwald, and A. Frank. Data mining static code
IEEE transactions on software

attributes to learn defect predictors.
engineering, 33(1):2–13, 2006.

[52] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, and A. Bener.
Defect prediction from static code features: current results, limitations,
Automated Software Engineering, 17(4):375–407,
new approaches.
2010.

[53] R. Mo, Y. Cai, R. Kazman, and L. Xiao. Hotspot patterns: The formal
deﬁnition and automatic detection of architecture smells. In 2015 12th
Working IEEE/IFIP Conference on Software Architecture, pages 51–60,
May 2015.

[54] N. Moha, Y.-G. Gueheneuc, L. Duchien, and A.-F. Le Meur. Decor: A
method for the speciﬁcation and detection of code and design smells.
IEEE Trans. Softw. Eng., 36(1):20–36, Jan 2010.

[55] E. Murphy-Hill, C. Jaspan, C. Sadowski, D. C. Shepherd, M. Phillips,
C. Winter, A. K. Dolan, E. K. Smith, and M. A. Jorde. What
predicts software developers’ productivity? Transactions on Software
Engineering, 2019.

[56] N. Nagappan and T. Ball. Static analysis tools as early indicators of pre-
In Proceedings. 27th International Conference

release defect density.
on Software Engineering, 2005. ICSE 2005., pages 580–586, 2005.
[57] N. Nagappan, T. Ball, and A. Zeller. Mining metrics to predict
component failures. In Proceedings of the 28th International Conference
on Software Engineering, ICSE ’06, page 452–461, New York, NY,
USA, 2006. Association for Computing Machinery.

[58] B. A. Nejmeh. Npath: A measure of execution path complexity and its

applications. Commun. ACM, 31(2):188–200, Feb 1988.

[59] R. Niedermayr, T. R¨ohm, and S. Wagner. Too trivial to test? an inverse
view on defect prediction to identify methods with low fault risk. 10
2018.

[60] R. Oak, M. Du, D. Yan, H. Takawale, and I. Amit. Malware detection
on highly imbalanced data through sequence modeling. In Proceedings
the 12th ACM Workshop on Artiﬁcial Intelligence and Security,
of
AISec’19, page 37–48, New York, NY, USA, 2019. Association for
Computing Machinery.

[61] N. Ohlsson and H. Alberg. Predicting fault-prone software modules
IEEE Transactions on Software Engineering,

in telephone switches.
22(12):886–894, 1996.

[62] E. Oliveira, E. Fernandes, I. Steinmacher, M. Cristo, T. Conte, and
A. Garcia. Code and commit metrics of developer productivity: a study
on team leaders perceptions. Empirical Software Engineering, 04 2020.

[63] J. Pearl. Causality. Cambridge university press, 2009.
[64] A. Potdar and E. Shihab. An exploratory study on self-admitted technical
debt. In 2014 IEEE International Conference on Software Maintenance
and Evolution, pages 91–100. IEEE, 2014.

[65] A. Ratner, C. D. Sa, S. Wu, D. Selsam, and C. R´e. Data programming:

Creating large training sets, quickly, 2016.

[66] E. Raymond. The cathedral and the bazaar. First Monday, 3(3), 1998.
[67] J. R. Ruthruff, J. Penix, J. D. Morgenthaler, S. Elbaum, and G. Rother-
Predicting accurate and actionable static analysis warnings:
mel.
In Proceedings of the 30th International
An experimental approach.
Conference on Software Engineering, ICSE ’08, page 341–350, New
York, NY, USA, 2008. Association for Computing Machinery.

