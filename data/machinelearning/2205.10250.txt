Machine learning manuscript No.
(will be inserted by the editor)

Explanatory machine learning for sequential human
teaching

Lun Ai · Johannes Langer · Stephen H.
Muggleton · Ute Schmid

Abstract The topic of comprehensibility of machine-learned theories has recently
drawn increasing attention. Inductive Logic Programming (ILP) uses logic program-
ming to derive logic theories from small data based on abduction and induction
techniques. Learned theories are represented in the form of rules as declarative de-
scriptions of obtained knowledge. In earlier work, the authors provided the ﬁrst evi-
dence of a measurable increase in human comprehension based on machine-learned
logic rules for simple classiﬁcation tasks. In a later study, it was found that the pre-
sentation of machine-learned explanations to humans can produce both beneﬁcial
and harmful eﬀects in the context of game learning. We continue our investigation
of comprehensibility by examining the eﬀects of the ordering of concept presenta-
tions on human comprehension. In this work, we examine the explanatory eﬀects
of curriculum order and the presence of machine-learned explanations for sequential
problem-solving. We show that 1) there exist tasks A and B such that learning A
before B has a better human comprehension with respect to learning B before A
and 2) there exist tasks A and B such that the presence of explanations when learn-
ing A contributes to improved human comprehension when subsequently learning
B. We propose a framework for the eﬀects of sequential teaching on comprehen-
sion based on an existing deﬁnition of comprehensibility and provide evidence for
support from data collected in human trials. Empirical results show that sequential
teaching of concepts with increasing complexity a) has a beneﬁcial eﬀect on human
comprehension and b) leads to human re-discovery of divide-and-conquer problem-
solving strategies, and c) studying machine-learned explanations allows adaptations
of human problem-solving strategy with better performance.

Keywords Explainable artiﬁcial intelligence; Machine learning comprehensibility;
Meta-interpretive learning; Inductive logic programming.

Lun Ai
Department of Computing, Imperial College London, London, UK
E-mail: lun.ai15@imperial.ac.uk

Johannes Langer
University of Bamberg, Bamberg, Germany
E-mail: johannes-miran.langer@stud.uni-bamberg.de

Stephen H. Muggleton
Department of Computing, Imperial College London, London, UK
E-mail: s.muggleton@imperial.ac.uk

Ute Schmid
Cognitive Systems Group, University of Bamberg, Bamberg, Germany
E-mail: ute.schmid@uni-bamberg.de

2
2
0
2

y
a
M
0
2

]
I

A
.
s
c
[

1
v
0
5
2
0
1
.
5
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

1 Introduction

Lun Ai et al.

Human learning can be described as a concept formation progression in which ob-
servations of objects and events are summarised and formulated by some inter-
dependent hierarchical structure [36]. As an example, in the context of teaching
algorithms to human students, we may imagine a human teacher using the material
in Figure 1 for teaching the merge sort algorithm. In a bottom-up teaching ap-
proach with increasing concept complexity, students learn how to merge ﬁrst (left)
and then move on to study sorting (right). In contrast, via a top-down teaching
approach which presents concepts with decreasing complexity, students learn to sort
ﬁrst (right) without prior knowledge of merging and study merging afterwards (left).
It might be of interest to the human teacher to question which approach yields higher
student sorting performance and what eﬀects the two teaching approaches have on
students’ sorting strategies.

Fig. 1: An illustration of teaching materials of merging and sorting that can be
learned in incremental order (left to right). All fruits have diﬀerent weights which
can be pairwise compared using the balance scale at the corner. On the left-hand
side, two sets of apples should be merged into a larger collection in increasing weights
from left to right. On the right-hand side, a set of bananas needs to be arranged in
increasing weights from left to right.

It was stressed by the Machine Learning pioneer Donald Michie [59] that an in-
telligent machine not only needs to excel humans in performance but should also
possess the capacity to eﬀectively interact with and transfer knowledge to humans.
An intelligent machine can in this way fulﬁl an integrated role that contributes to
both the “stock of software” and the “stock of knowledge”. Owing to increasing
awareness of the importance of explainable AI, reported by comprehensive reviews
[9,1,62], a great number of studies have emerged with an emphasis on systems that
provide support for human understanding. In addition, there is an increasing em-
phasis on AI that interacts to keep humans “in the loop” [61,79]. At the intersection
of these two paradigms is the research area that focuses on the comprehensibility
of machine learning. An operational deﬁnition of comprehensibility was provided
in [82] for examining machine-human teaching interaction. This deﬁnition relates
human comprehension to the out-of-sample prediction accuracy of human learners
after studying training examples and a machine-learned logic program. It allowed the
ﬁrst demonstration [64] of Michie’s Ultra-Strong Machine Learning (USML) which

Explanatory machine learning for sequential human teaching

3

stressed the role of teaching machine-learned knowledge to humans such that their
proﬁciency on a task can be raised to a higher level with respect to learning from
randomly selected examples. Later results in [2] showed that explanations generated
from machine-learned logic programs can have both beneﬁcial and harmful eﬀects
on human comprehension and suggested, via a cognitive window framework, that
explanatory eﬀects are dependent on both descriptive and executional complexity.

The present paper aims to study the eﬀects on human learning of the order of
concepts in a teaching curriculum - an essential topic of machine learning and human
problem solving that is lacking from existing studies of comprehensibility in machine
learning. Based on established results and teaching procedures in [64, 2], we extend
the frameworks of comprehensibility and explanatory eﬀects to account for the im-
pacts of sequential teaching on comprehension. We refer to the cognitive window
framework which attributes the explanatory eﬀects to 1) the descriptive complexity
of machine-learned rules and 2) the cost of knowledge application with respect to an
execution stack. A human trial of sequential teaching was conducted to examine the
eﬀects of concept ordering in curricula and explanations in the context of learning an
eﬃcient sorting strategy. We hypothesise the improvement of human comprehension
from sequential teaching as the result of the reduction in the size of the hypothesis
space associated with the target concept deﬁnition. Our proposed framework is con-
sistent with empirical evidence collected from the human trial involving participants
with no background in programming.

We summarise the paper’s contributions as follows:

– We deﬁne a measure to evaluate beneﬁcial/harmful explanatory eﬀects of a se-

quential teaching curriculum on human comprehension.

– We hypothesise the improvement of human comprehension from sequential teach-

ing curricula based on the Blumer bound [16].

– We demonstrate based on an analysis of empirical results that a sequential teach-
ing curriculum with increasing concept complexity has a beneﬁcial eﬀect on hu-
man comprehension.

– We show results that indicate the re-discovery of divide-and-conquer algorithms

from novices after learning from concepts with increasing complexity.

– We provide evidence of the optimisation of problem-solving strategy as a result

of studying explanations generated from machine-learned logic rules.

This paper is organised as follows. In Section 2, we review the literature rele-
vant to the paper. In Section 3, we present our theoretical framework of sequential
teaching curricula. In Section 4, we describe our empirical approach including the
experimental hypotheses. In Section 5, we show the results of our main Amazon Me-
chanical Turk experiment that involved teaching human participants to sort based on
the training of merging two ordered sequences. In Section 5.5, we provide a detailed
discussion of our ﬁndings on the eﬀects of curriculum order and explanations on
human comprehension. In Section 6, we provide a review of our work and comment
on our contributions. We additionally discuss future extensions to the demonstrated
results, behavioural cloning and human knowledge re-discovery.

4

2 Related work

Lun Ai et al.

2.1 Human learning in curricula with explanations

The ability of humans to recognise and extract knowledge useful to a problem-solving
context is dependent on prior experience and understanding of related domains [10].
While a classiﬁcation decision is dependent only on information concerning the cur-
rent state, goal-directed decision making requires consideration of a decision’s possi-
ble eﬀect on future states [14]. For sequential decision-making tasks such as the Tower
of Hanoi, state transitions of the problem solution can be explicitly expressed by rules
[83]. “Chunking” reusable sub-goals was demonstrated as an eﬃcient problem-solving
approach to encode problem-solution structures and reduce the load on short-term
memory [10].

The reliance of human problem solving on the problem domain and experience of
a person [32] can be attributed to implicit (System 1) and explicit (System 2) knowl-
edge [45]. In contrast to implicit knowledge which is only attainable through practice,
declarative knowledge can be transferred explicitly in the form of explanations [24].
For knowledge accumulation, explicit explanations and hints help make sense of the
new information and allow integration into existing knowledge [34]. In the context of
solving simple logical problems [26,47] and generating LOGO programs [50], guided
learning has been demonstrated more eﬃcient than exploration-oriented approaches.
However, multiple studies suggest that explanations should not be presented alone.
As demonstrated by [6,15,3], human learners do not always beneﬁt from explanations
when the speciﬁc problem-solving context is absent. The combination of explicit guid-
ance and worked examples provides a good hands-on problem-solving experience for
learners to study and practise for themselves due to the limited capacity of working
memory [90]. The presentation of worked examples along with explanations makes
this speciﬁc task context available and was shown to support learning [75,4].

2.2 Teaching between the machine and humans

Machine teaching is an area of machine learning that focuses on the optimisation
of training sets and the design of teaching protocols in an example-based setting
[96]. Studies in machine teaching have provided formal methods for teaching situa-
tions in which the goal is to train human students via an optimal set of examples
to learn a target hypothesis. The machine teacher in these setups [73, 54,93] is pro-
vided information about the quantiﬁed cognitive model of the human learner and has
access to the learning progress based on the student’s responses to examples. The
teacher can then monitor the reactions of students which allows the teacher to make
adjustments to teaching materials. However, often excessive assumptions about the
teacher’s knowledge are made for deriving theoretical results. The teacher is usually
assumed to have perfect knowledge about the computational model of students such
as their learning rate and their background knowledge [31]. Thus, owing to the speci-
ﬁcity of these theoretical models, they are diﬃcult to apply in real-world teaching
settings.

Communication of machine-learned solutions can be alternatively realised through
explanations in the form of rules. For Inductive Logic Programming (ILP) [65], sys-
tems use logic programs to represent hypotheses as generalisations of examples and

Explanatory machine learning for sequential human teaching

5

background knowledge. ILP systems make use of both induction and abduction,
which are critical mechanisms for human knowledge attainment [51, 42]. A hypoth-
esis learned by an ILP algorithm is represented as a set of rules and can be used to
communicate the discovered knowledge to humans. For instance, an ILP system [13]
learns optimal strategies of chess endgames for depths 0, 1 and 2 and incorporates a
complexity constraint on the number of clauses in a predicate deﬁnition based on the
hypothesised limitation of working memory capacity of 7 ± 2 “chunks” [60]. A fully
automated discovery system [46] re-discovers the role of genes of known function by
combining ILP-based scientiﬁc discovery software with a laboratory robot. In [88],
the ILP system produces hypotheses as logic rules that depict the chemical con-
nectivity of molecules and predict mutagenicity, which accommodates collaboration
with a human chemist. However, in most systems, it remains an ongoing challenge to
ensure human understanding due to the high complexity of information encoded via
these interfaces. In a recent work [2], it was shown that visual and textual explana-
tions for playing a simple two-player game, generated based on rules learned by an
ILP system, may cause improvement or degradation in human comprehension after
a brief study. The authors demonstrated that the outcome of studying explanations
provided is aﬀected by a cognitive window which takes account of the descriptive
complexity and execution requirements of the rules.

2.3 Operational measures of human understanding in AI

Explainable Artiﬁcial Intelligence (XAI) is an area of AI that studies AI systems
that allow human understanding by providing human-readable explanations of de-
cisions based on structures and functions. Researchers have attempted to devise AI
systems to present knowledge attained in various formats, such as texts [41, 55], vi-
sualisations [92,77,91,72] and visualisation-text hybrid demonstrations [80]. Simpler
explanations of local predictions can be devised to provide a localised understanding
of a learned classiﬁcation model [76]. For reinforcement learning applications, learned
strategies are implicitly encoded by policy functions with continuous domains which
are diﬃcult for humans to understand. This concern about the opacity of learned
strategies was addressed by incorporating the use of relational biases [33,94, 21] to
enhance human understandability. In addition, case-based summaries of a policy [5]
from sets of selected states in a larger state space of an agent allow a limited human
understanding of the agent’s decisions.

In XAI, there exists a diversity of motivations and technical descriptions of
systems which support human understanding. Terms such as transparency, inter-
pretability, explainability and comprehensibility are frequently used terms at the
core of XAI [11]. Transparency typically refers to the ability of a model or a sys-
tem to be human-understandable on its own [11]. Interpretability relates to the
degree of clarity of information revealed to the user by a system [62]. Explainability
stresses the role of a system as an “explainer” interface to the user [11]. Compre-
hensibility denotes a system’s ability to represent machine-learned knowledge in a
human-understandable format [11,38]. However, owing to the absence of operational
deﬁnitions and procedures of evaluation, a great body of recent results was estab-
lished based on subjective views [61,35] which are discordant [53] with respect to the
sub-problem of XAI that the studies attempted to address. Studies were often found
to report limited empirical evidence as support of the proclaimed eﬀect and did not

6

Lun Ai et al.

take into consideration that human understanding is just as vital as the computa-
tional procedures that generate explanatory information [61]. It was stressed in [1]
that a great body of studies did not evaluate the eﬀect of explanations on their target
users. Those works that did include explanatory eﬀect evaluations often failed to pro-
vide a good account of the context, results and limitations [9]. A number of reports
also brought to attention the potential misinterpretation and misuse of explanatory
information. For example, partially transparent explanatory information may cause
more confusion if key decision making of systems is omitted [78]. The authors in [89]
expressed trustworthiness concern and showed that human decision making over-
relied on the intelligent system even when explanatory information provided by the
system was inaccurate.

The importance of comprehensibility has long been recognised in machine learn-
ing of human-oriented knowledge. Michalski [56] suggested that comprehensible ma-
chine learning should produce outputs that share similar structure and semantics as
those produced by human experts and should present learned knowledge in human-
understandable “chunks”. Owing to the challenges of quantiﬁcation, comprehensi-
bility has usually been associated with the complexity of machine-learned models
[40]. Research on the operational deﬁnition of machine learning comprehensibility
can be traced back to Michie’s deﬁnition of Ultra-Strong Machine Learning in the
1980s [58]. This criterion stresses machine learners’ ability to teach hypotheses to
humans such that their performance over the task is improved by a signiﬁcant margin
compared with human learning alone. An operational deﬁnition of comprehensibility
[64, 82] was proposed which assesses human comprehension based on the human out-
of-sample classiﬁcation accuracy. In a recent work [2], both beneﬁcial and harmful
eﬀects of explanatory machine learning were identiﬁed in the context of symbolic
machine learning. However, the eﬀects of the teaching order of concepts on human
comprehension have not been fully explored in the literature on machine learning
comprehensibility.

3 Theoretical framework

3.1 Meta-interpretive learning

An Inductive Logic Programming (ILP) [65] algorithm uses background knowledge B
to induce a hypothesis in the form of a logic program which entails all of the positive
examples E+ and none of the negative examples E−. Meta-Interpretive Learning
(MIL) [66, 67] is a variant of ILP which supports predicate invention [66], depen-
dent learning [52] and learning of recursive and higher-order programs. An MIL
algorithm solves the following problem: given a tuple (B, M, E+, E−, I, H) where
the background knowledge B is a ﬁrst-order logic program, M is a set of second-
order clauses, positive examples E+ and negative examples E− are ground atoms,
learns a logic program H such that M ∪ H ∪ B |= E+ and M ∪ H ∪ B 6|= E−.
I is a set of predicate symbols reserved for predicate invention. Output logic pro-
gram H is a hypothesis in the hypothesis space H described by a ﬁnite number of
predicate symbols and constants. The background knowledge B contains a set of
predicate deﬁnitions as primitives. The set of second-order clauses M is referred to
as meta-rules. Each meta-rule contains existentially quantiﬁed second-order variables
and universally quantiﬁed ﬁrst-order variables. A MIL algorithm employs meta-rules

Explanatory machine learning for sequential human teaching

7

and substitutes second-order variables with predicate symbols to derive ﬁrst-order
theories as logical generalisations.

3.2 MIL for learning sort algorithm

The MIL system M etagol [30] is implemented using a meta-interpreter in Prolog.
M etagol supports predicate invention and dependent learning which reduces the size
of the hypothesis space and improves learning performance [27]. M etagolO [29] makes
use of composite objects and actions deﬁned from primitive objects and actions.
M etagolO addresses the issue that most ILP systems only considered the textual
complexity of hypothesis programs. The inability to distinguish two algorithms, for
instance, insertion sort and quick sort, would result in the less computationally
eﬃcient but textually compact algorithm being learned. Both textual and resource
costs of hypothesis programs are minimised by M etagolO using a technique called
iterative descent [29] which computes the resource cost of hypotheses of increasing
sizes according to a pre-deﬁned cost function. For learning robot postman and robot
sorting strategies, the background knowledge is deﬁned in composite actions and
objects and provided to M etagolO. The authors showed that M etagolO converges to
hypotheses with the optimal resource complexity and learns resource-eﬃcient robot
strategies [29].

We modify the background knowledge of M etagolO for learning a bottom-up
variant of the merge sort algorithm [39] for sorting positive integers. Merge sort is
a recursive sorting algorithm based on a divide-and-conquer approach. The conven-
tional implementation recursively splits an input sequence into units and performs
merging on these units to build up short sorted sequences. The aforementioned vari-
ant takes an input sequence as a list of sub-sequences of length one and iteratively
merges two sub-sequences in a bottom-up fashion. For human learners with little or
no prior knowledge of recursive sorting algorithms, we consider that iterative algo-
rithms are conceptually easier teaching materials in the case of unfamiliarity with
recursive programs. In addition, we assume that human sorting strategies do not
usually involve merging. Humans may learn to utilise merging for creative sorting
strategies.

A demonstration of the execution of the bottom-up merge sort algorithm is in-
cluded in Example 1. To learn the bottom-up variant of merge sort1, we deﬁne the
world state that the robot sorter would see. M etagolO is provided with primitives to
manipulate expressions which compactly represent ordered sequences. Let Lt_expr
represent a non-empty monotonically increasing sequence of integers and be deﬁned
as Lt_expr := Int | Lt_expr < Lt_expr.

Example 1 Let the sequence [4, 6, 5, 2, 3, 1] be an input to the bottom-up variant
of merge sort. Merging is iteratively performed from the end of the list with lower
indices. Merging take two expressions as inputs, e.g. 4 and 6, and outputs one ex-
pression, e.g. 4 < 6. In iteration 1, three merging would be executed on expressions
4 and 6, 5 and 2, 3 and 1 which would return [4 < 6, 2 < 5, 1 < 3]. In iteration 2,
one merging would be executed on two leftmost expressions 4 < 6 and 2 < 5 which
would give [2 < 4 < 5 < 6, 1 < 3 ]. In iteration 3, expressions 2 < 4 < 5 < 6 and 1
< 3 are merged which would return [1 < 2 < 3 < 4 < 5 < 6]. Only one expression

1 Source code and demos are available on https://github.com/LAi1997/sequential-teaching

8

Lun Ai et al.

Table 1: When the robot performs parse_exprs/2, it parses two expressions by
removing the “<” symbols, and it puts one sequence of numbers into the left bag and
the other sequence of numbers into the right bag. compare_nums/2 ﬁrst selects a
number from the left bag and a number from the right bag. Then it compares the two
numbers. Afterwards, compare_nums/2 uses the smaller number to extend the last
expression in the memory and puts the larger number back in its original bag. When
one of the bags is empty, the robot performs drop_bag_remaining/2 to append
the rest of the numbers to the last expression in the memory. recycle_memory/2
takes all expressions in the memory and ﬁlls the expression list exprs. single_expr/2
checks if there exists only one expression in the expression list exprs.

Deﬁnition

merger/2

Rules

merger(A,B):-parse_exprs(A,C),merger_1(C,B).
merger_1(A,B):- compare_nums(A,C),merger_1(C,B)
merger_1(A,B):-compare_nums(A,C),drop_bag_remaining(C,B).

sorter/2
(after learning
merger/2)

sorter(A,B):-merger(A,C),sorter(C,B).
sorter(A,B):-recycle_memory(A,C), sorter(C,B).
sorter(A,B):-single_expr(A,C), single_expr(C,B).

sorter/2
(without
learning
merger/2)

sorter(A,B):-parse_exprs(A,C),sorter(C,B).
sorter(A,B):-compare_nums(A,C), sorter(C,B).
sorter(A,B):-drop_bag_remaining(A,C), sorter(C,B).
sorter(A,B):-recycle_memory(A,C), sorter(C,B).
sorter(A,B):-single_expr(A,C), single_expr(C,B).

Table 2: M etagolO uses two meta-rules. P , Q, and R are existentially quantiﬁed
higher-order variables and x, y, and z are universally quantiﬁed ﬁrst-order variables.
(cid:31) is a total ordering constraint over the Herbrand base to guarantee termination of
hypotheses.

Name
Chain
Tailrec

Meta-rule
P (x, y) ← Q(x, z), R(z, y)
P (x, y) ← Q(x, z), P (z, y)

Orders
P (cid:31) Q, P (cid:31) R
P (cid:31) Q,
x (cid:31) z (cid:31) y

remains which represents the sorted sequence and the algorithm terminates with [1
< 2 < 3 < 4 < 5 < 6] as output.

Objects are separated into primitive objects Oprim and composite objects Ocomp
where O = Oprim ∪ Ocomp denotes the set of all objects in the robot world. Each
composite object is deﬁned by primitive objects or other composite objects. S is a
set of states and each state is a tuple of objects. In particular, a state is deﬁned
as a tuple (exprs, energy, lef t_bag, right_bag, memory) with an arity of 5. The
exprs is a list of Lt_expr. The energy records the resource cost. The lef t_bag and
right_bag are lists of the parsed numbers. The memory is a list of newly created
Lt_expr. An action a ∈ A is a function such that a : S → S. Each action is either a
primitive action included in Aprim or a composite action included in Acomp. Let A

Explanatory machine learning for sequential human teaching

9

Table 3: A summary of the connections between deﬁnitions of comprehensibility,
explanatory eﬀects and cognitive window in [64,2] and the proposed framework to
account for eﬀects of sequential teaching curricula.

Framework

Deﬁnitions and
results in previous works

Extended deﬁnitions and
results for sequential teaching

Comprehensibility

Deﬁnitions 1 and 2

Deﬁnitions 5 to 7

Explanatory eﬀects

Deﬁnitions 3 and 4

Deﬁnitions 8 and 9

Cognitive window

Deﬁnitions 10 to 14 and
Conjecture 1 to 2

Remark 1 and Conjecture 3

be an enumerable set of actions where A = Aprim ∪ Acomp. Every composite action
is constructed based on primitive actions or other composite actions. A resource
function r : A × S → N deﬁnes the resources consumed by carrying out an action
a ∈ A in state s ∈ S.

M etagolO’s background knowledge is supplied with composite actions Acomp =

{parse_exprs/2, compare_nums/2, single_expr/2, drop_bag_remaining/2,
recycle_memory/2} for learning merger/2 and sorter/2 in Table 1. In merger/2,
two Lt_expr from exprs are ﬁrst parsed into lef t_bag and right_bag. Then numbers
are compared pair-wise from two bags and the smaller number is appended to the
last Lt_expr in memory. sorter/2 applies merger/2 over all Lt_expr in exprs.
Constructed Lt_expr expressions in memory are then recycled back to exprs by
recycle_memory/2. This process is iterated until there is only one Lt_expr left
in exprs and memory is empty. We deﬁne a cost function for compare_nums/2
such that the energy is incremented by one whenever two numbers are successfully
compared. We additionally deﬁne a constraint over the world states. This constraint
based on Spearman’s rank correlation coeﬃcient [87] ensures the expressions in the
current state are not less sorted compared with expressions in the immediate next
state.

3.3 Explanatory eﬀect of curriculum order

The previous works [82,64] provided an operational deﬁnition of comprehensibil-
ity. Based on this deﬁnition, the authors in [2] explored the explanatory eﬀects of
machine-learned logic rules on human comprehension and proposed a framework of
a cognitive window that focuses on the symbolic subset of machine learning. The
deﬁnitions of comprehensibility and explanatory eﬀects are extended in this present
work to account for sequential teaching curricula in the context of symbolic machine
learning. We use the logic programs learned from M etagolO to generate explanations
for our human trial. For the purpose of experimentation, we only consider curricula
with noise-free training examples. This does not mean, however, that the proposed
deﬁnitions could not be extended outside of the considered scope.

The operational deﬁnition of comprehensibility [82] is an objective means of eval-
uating human comprehension of a concept based on human out-of-sample predictive

10

Lun Ai et al.

accuracy. It was extended in [2] to deﬁne the explanatory eﬀectiveness of machine-
learned logic programs which measures the extent to which examples with explana-
tions generated from machine-learned logic programs can be simulated by humans.

Deﬁnition 1 (Unaided human comprehension of examples, Ch(D, H, E))
Given that D is a logic program representing the deﬁnition of a target predicate, H
is a human group and E is a set of examples of the target predicate. The unaided
human comprehension of examples E is the mean accuracy with which a human
h ∈ H after a brief study of E and without further sight can classify new material
sampled randomly from the domain of D.

Compared with the unaided human comprehension of examples, the machine-
explained human comprehension of examples corresponds to the out-of-sample clas-
siﬁcation accuracy after studying the machine-learned explanation M (E) where M
is a machine learning algorithm and E is a set of training examples.

Deﬁnition 2 (Machine-explained human comprehension of examples, Cex(D,
H, M (E))): Given that D is a logic program representing the deﬁnition of a target
predicate, H is a human group, M (E) is a theory learned using machine learning
algorithm M and E is a set of examples of the target predicate. The machine-
explained human comprehension of examples E is the mean accuracy with which a
human h ∈ H after a brief study of an explanation based on M (E) and without
further sight can classify new material sampled randomly from the domain of D.

The explanatory eﬀect of a machine-learned theory on human comprehension
of a task is deﬁned as the diﬀerence between machine-explain and unaided human
comprehension of examples of the task.

Deﬁnition 3 (Explanatory eﬀect of a machine-learned theory, Eex(D, H,
M (E))): Given a logic program D representing the deﬁnition of a target predicate,
a human group H and a machine learning algorithm M , the explanatory eﬀect of
the theory M (E) learned from examples E is

Eex(D, H, M (E)) = Cex(D, H, M (E)) − Ch(D, H, E)

In the case that the explanations provided by the machine lead to a signiﬁcantly
positive improvement, the explanatory eﬀect of the learned theory is beneﬁcial to
human comprehension. When this diﬀerence is signiﬁcantly negative, the explanatory
eﬀect of the machine-learned theory is harmful to human comprehension.

Deﬁnition 4 (Beneﬁcial/harmful eﬀect of a machine-learned theory): Given
a logic program D representing the deﬁnition of a target predicate, a group of humans
H, a symbolic machine learning algorithm M :

– M (E) learned from examples E is beneﬁcial to H if Eex(D, H, M (E)) > 0
– M (E) learned from examples E is harmful to H if Eex(D, H, M (E)) < 0
– Otherwise, M (E) learned from examples E does not have an observable eﬀect

on H

Explanatory machine learning for sequential human teaching

11

The deﬁnitions of human comprehension and explanatory eﬀects are extended
to account for the ordering of concepts. We deﬁne a sequential teaching curriculum
to contain a set of concept deﬁnitions, sets of examples and explanations. The ar-
rangement of concepts in a curriculum can be decided based on an ordering function
which outputs a score to rank concepts.

Deﬁnition 5 (Curriculum rank function, λ(D, E, M )) Given a set of predicate
deﬁnitions Ds, sets of examples Es and a set of machine learning algorithms M s,
λ : Ds×Es×M s → N is a rank function that returns the rank value of any predicate
deﬁnition D ∈ Ds in a curriculum.

A sequential teaching curriculum contains “chunks” of teaching material and a
curriculum has at least one such “chunk”. The order of concepts received by humans
is determined by a deﬁned rank function which assigns a score to every block of
teaching material which contains the concept deﬁnitions, examples and explanations.

Deﬁnition 6 (Sequential teaching curriculum, ST (Ds, Es, λ, M s)): Given
that Ds = {D1,D2, ... Dn} is a set of predicate deﬁnitions of size n, Es = {E1,E2,
..., En} contains sets of examples where Ei are examples of Di for 1 ≤ i ≤ n, M s
denotes a set of machine learning algorithms and λ : Ds × Es × M s → N is a rank
function, a sequential teaching curriculum ST (Ds, Es λ, M s) is a enumerable set
of tuples {(R1, D1, E1, M1), (R2, D2, E2, M2), ..., (Rn, Dn, En, Mn)} in which Rj =
λ(Dj, Ej, Mj) is the rank of a concept in the curriculum for 1 ≤ j ≤ n. A sequential
teaching curriculum enumerates concepts with respect to increasing rank value.

Example 2 Let Ds = {merger, sorter} denote predicate deﬁnitions of merging and
merge sort, Emerge and Esort be the respective example sets and Es = {Emerge, Esort}
and M s = {M etagolO} which generates explanations. We deﬁne λmerge/sort (merger,
Emerge, M etagolO) = 0 and λmerge/sort (sorter, Esort, M etagolO) = 1. Let λsort/merge
(merger, Emerge, M etagolO) = 1 and λsort/mergel (sorter, Esort, M etagolO) = 0.
The merge-then-sort curriculum is represented by ST (Ds, Es, λmerge/sort,M s) which
teaches merging before teaching sorting and the sort-then-merge curriculum is de-
noted by ST (Ds, Es, λsort/merge,M s) which teaches sorting before teaching merg-
ing. λmerge/sort speciﬁes the ordering of concepts in the merge-then-sort curriculum
ST (Ds, Es, λmerge/sort, M s) and λsort/merge deﬁnes the ordering of concepts in the
sort-then-merge curriculum ST (Ds, Es, λsort/merge, M s).

Deﬁnition 7 (Human comprehension of examples in a sequential teaching
curriculum, Cseq(T, H)): Given that T = ST (Ds, Es, λ, M s) is a sequential teach-
ing curriculum where Ds is a set of n predicate deﬁnitions, Es are sets of examples,
λ : Ds × Es × M s → N is a rank function, M s is a set of machine learning algorithms
and H is a human group, the human comprehension of examples in the sequential
teaching curriculum is

Cseq(T, H) =






(Ri, τ )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

τ = Ch(Di, H, Ei),
τ = Cex(Di, H, Mi(Ei)),

Mi(Ei) = ∅
Mi(Ei) 6= ∅

for (Ri, Di, Ei, Mi) ∈ S, 1 ≤ i ≤ n






12

Lun Ai et al.

Human comprehension is considered aided by explanations if the machine-learned
logic program in a teaching material block is not empty. Otherwise, human compre-
hension is established based on training examples alone. We proceed to deﬁne the
eﬀect of a sequential teaching curriculum on the human comprehension of a concept.

Deﬁnition 8 (Eﬀect of a sequential teaching curriculum, Eseq(C1, C2, D)):
C1 = Cseq(ST (Ds, Es, λ1, M s), H) and C2 = Cseq(ST (Ds, Es, λ2, M s), H) are
sequential teaching curricula where Ds is a set of n predicate deﬁnitions and D ∈ Ds
is a predicate deﬁnition, Es = {E, ...} are sets of examples, λ1, λ2 : Ds × Es × M s →
N are two rank functions, M s is a set of machine learning algorithms and H is
a human group. R1 = λ1(D, E, M1) and R2 = λ2(D, E, M2) are the ranks of D
in C1 and C2 where M1, M2 ∈ M s. The explanatory eﬀect of curriculum C1 over
curriculum C2 on predicate deﬁnition D is

where (R1, τ1) ∈ C1, (R2, τ2) ∈ C2.

Eseq(C1, C2, D) = τ1 − τ2

In Deﬁnition 7, comprehension measurements are obtained with respect to the
ranking of concepts in a sequential teaching curriculum. The eﬀect of a sequential
teaching curriculum reﬂects the degree to which human comprehension of a concept
is aﬀected by a curriculum. Given two sequential teaching curricula and a concept
of interest, the eﬀect of one curriculum over another curriculum corresponds to the
diﬀerence in comprehension measurements of the concept. A signiﬁcant diﬀerence
in human comprehension as a result of learning from one curriculum over another
curriculum can be classiﬁed as a beneﬁcial or harmful eﬀect of learning from the
former curriculum.

Deﬁnition 9 (Beneﬁcial/harmful eﬀect of a sequential teaching curricu-
lum): Given that C1 = Cseq(ST (Ds, Es, λ1, M s), H) and C2 = Cseq(ST (Ds,
Es, λ2, M s), H) are sequential teaching curricula where Ds is a set of n pred-
icate deﬁnitions and D ∈ Ds is a predicate deﬁnition, Es are sets of examples,
λ1, λ2 : Ds × Es × M s → N are two rank functions, M s is a set of machine learning
algorithms and H is a human group, the curriculum C1 in comparison with the
curriculum C2 is

– beneﬁcial to H on D if Eseq(C1, C2, D) > 0
– harmful to H on D if Eseq(C1, C2, D) < 0
– Otherwise there is no observable eﬀect to H on D from curriculum C1 compared

with curriculum C2

In Deﬁnition 9, the eﬀect on the predicate D is beneﬁcial when the curriculum
ordered by λ1 results in a better comprehension of D than the comprehension of
D in the curriculum ordered by λ2. Conversely, the eﬀect on D is harmful in the
case that signiﬁcant degradation of comprehension is observed when comparing the
comprehension of D between curricula described by λ1 and λ2.

3.4 Cognitive cost of a logic program

A consensus between the literature on cognitive psychology and artiﬁcial intelligence
[60, 44,68] is that the information processing of humans in working memory can be

Explanatory machine learning for sequential human teaching

13

modelled by manipulation of symbols in machines which can be formally captured
by Turing machines. The limitation on working memory capacity corresponds to a
bounded tape length and instruction complexity in Turing Machines. For rule-based
concept acquisition [20], human concept attainment is analogical to a search in a
collection of hypotheses guided by some preference ordering which is comparable to
version space learning in machine learning [63]. In addition, the organisation of com-
plex action sequences into hierarchical structures in human information processing
is an eﬃcient mechanism for general problem-solving. In the contexts of analogi-
cal problem solving [22] and production systems [68], top-level goals are decomposed
into sub-goals which can be described by a set of rules that guide actions in problem-
solving sequences. Although rules generally can be considered as procedural knowl-
edge, in complex domains human verbalisation of rules utilises declarative memory
[7,81]. The explicit representations of a human problem-solving solution correspond
to rule-like descriptions. For the following deﬁnitions, we assume that:

– Human learners are version space learners who can make only limited hypothesis

space searches

– Human learning is guided by meta-rules which construct the sub-goal structure

and uses predicates as background knowledge

– An increase in the execution complexity of the problem solution can have a

negative eﬀect on performance

– Rules can be declaratively represented in a verbalisable form whose complexity

can be measured by the Kolmogorov complexity

The cognitive cost is a variant of Kolmogorov complexity [48] proposed in [2].
The cognitive cost estimates the textual complexity of logic terms and atoms de-
clared in the execution stack of working memory. These logic terms and atoms are
instances of predicates in a datalog program during the execution of a goal. Data-
log program is a declarative subset of logic programs that represent data structures
using predicate arguments but do not use function symbols. For instance, given
logic terms and atoms T1, T2, ..., Tn−1, Tn, a tuple of ﬁnite arity can represented as
tuple(T1, tuple(T2, tuple(..., tuple(Tn−1, tuple(Tn, (cid:15)))...))) and list(T1, list(T2,
list(..., list(Tn−1, list(Tn, (cid:15)))...))) represents a list of ﬁnite size where (cid:15) denotes an
empty character. The cost of Logic term and atom instances is computed based on
the length of the representation, in particular, the number of symbols involved.

Deﬁnition 10 (Cognitive cost of a logic term and atom, C(T )): Given T a
logic term or atom, the cost of C(T ) can be computed as follows:

– C(>) = C(⊥) = 1
– A variable V has cost C(V ) = 1
– A constant c of a ﬁnite length has cost C(c) which is the number of digits and

characters in c

– An atom Q(T1, T2, ...) of a ﬁnite arity has cost C(Q(T1, T2, ...)) = 1 + C(T1) +

C(T2)+ . . .

Example 3 An atom list(1, list(0, (cid:15))) which represents a list [1, 0] has a cognitive
cost C(list(1, list(0, (cid:15)))) = 5.

Example 4 An atom merger(State1, State2) with variables State1 and State2 has
a cognitive cost C(merger(State1, State2)) = 3.

14

Lun Ai et al.

In the event of a query q, an execution stack resembles the mental computation

and represents the memory of grounded logic terms and atoms instantiated.

Deﬁnition 11 (Execution stack of a datalog program, S(P, l, q)): Given a
query q and a non-negative integer l, the execution stack S(P, l, q) of a datalog
program P is a ﬁnite set of size l of atoms or terms evaluated during the execution
of P to compute an answer for q. An evaluation in which an answer to the query
is found ends with value >, and an evaluation in which no answer to the query is
found ends with ⊥.

The cognitive cost represents the tax of memorising goals in the human short
term memory. The eﬀort of maintaining goals in working memory correlates to the
performance of human problem-solving [23]. The complexity of executing a set of
rules is estimated by the sum of the cognitive cost of logic terms and atoms in the
execution stack. The step size bound of an execution stack denotes the limitation of
working memory and ensures the computation of the sum of cognitive costs halts.

Deﬁnition 12 (Cognitive cost of a datalog program, Cog(P, q)): Given a
query q, a non-negative integer l, the cognitive cost of a datalog program P is

Cog(P, q) =

X

C(t)

t∈S(P,l,q)

Example 5 The program merger/2 in Table 1 takes two input Lt_expr expres-
sions in the expression list of a world state and produces an expression Lt_expr
in the memory of the output world state. The output expression contains integers
of increasing magnitude from left to right connected by the “<” symbol. Given
ex1 = list(1, list(0, (cid:15))), lb1 = (cid:15), rb1 = (cid:15) and m1 = (cid:15), s1 = tuple(ex1, tuple(0,
tuple(lb1,tuple(rb1,m1)))) denotes the initial world state and C(s1) = 13. Let ex2
= list((cid:15), (cid:15)), lb2 = list(1, (cid:15)), rb2 = list(0, (cid:15)) and m2 = (cid:15). s2 = tuple(ex2, tuple(0,
tuple(lb2,tuple(rb2,m2)))) denotes the world state after executing parse_expr/2 and
C(s2) = 15. parse_expr/2 takes two expressions from the expression list and puts
one expression into the left bag and puts the other into the right bag. Let V1 and V2 be
variables. The length of the execution stack denoted by S(merger, 4, merger(s1, V1))
is 4. The cognitive cost of executing merger/2 given the query merger(s1, V1) is 60.

S(merger, 4, merger(s1, V1))
merger(s1, V1)
parse_expr(s1, V2)
parse_expr(s1, s2)
⊥
Cog(merger, merger(s1, V1))

C(T )
15
15
29
1
60

In human problem solving, the role of background knowledge is to facilitate the
transfer of existing solutions [8, 69] to the current context. In the case that auxiliary
knowledge is absent, the construction of the solution relies on lower level sub-goals
and performance is limited by human operational error. A primitive problem solution
thus denotes a program that involves the minimum amount of background knowledge
relating to the task.

Explanatory machine learning for sequential human teaching

15

Deﬁnition 13 (Minimum primitive solution program, ¯M (φ, E)): Given a set
of primitives φ and examples E, a datalog program learned from examples E using a
symbolic machine learning algorithm ¯M and a set of primitives φ0 ⊆ φ is a minimum
primitive solution program ¯M (φ, E) if and only if for all sets of primitives φ00 ⊆ φ
where |φ00| < |φ0| and for all symbolic machine learning algorithm M 0 using φ00, there
exists no machine-learned program M 0(E) that is consistent with examples E.

Deﬁnition 14 (Cognitive cost of a problem solution, CogP (E, ¯M , φ, q)): Given
examples E, primitive set φ, a query q and a symbolic machine learning algorithm ¯M
that learns a minimum primitive solution, the cognitive cost of a problem solution is

CogP (E, ¯M , φ, q) = Cog( ¯M (φ, E), q)

where ¯M (φ, E) is a minimum primitive solution program.

The presence of informative knowledge enables eﬃcient problem solving as pro-
jections from background knowledge and allows shortcuts in the construction of
the current solution process. In comparison with a primitive problem solution, ex-
ecutional shortcuts in a non-primitive solution could lead to less cognitive load on
working memory and fewer performance errors during the execution of the solution.

3.5 Constraints on human comprehension

Donald Michie [57] discussed the eﬀect of the representational and executional cost of
a program on human understanding. He described the notion of a human window in
terms of a class of programs with the right balance of storage and computational com-
plexity that are ﬁt for human understanding. The cognitive window framework [2]
shares Michie’s view of the human window and describes two constraints of machine-
learned theory in regard to its eﬀects on human comprehension. The ﬁrst constraint
relates to the size of the hypothesis space associated with the representation of
a machine-learned logic theory. This constraint speciﬁes the limitation of human
search in the hypothesis space.

Conjecture 1 (Cognitive bound on the hypothesis space size, B(P, H)): Con-
sider a symbolic machine-learned datalog program P using p predicate symbols and
m meta-rules each having at most j body literals. Given a group of humans H,
B(P, H) is a population-dependent bound on the size of hypothesis space such that
at most n clauses in P can be comprehended by all humans in H and B(P, H) =
mnp(1+j)n.

The deﬁnition of the human searchable hypothesis space size B(P, H) relates
to the MIL complexity analysis in [52,27]. The authors in [2] demonstrated that
humans learned can only learn some of the rules for a two-players simple game
when the descriptive size of the rules is high. This implies that the hypothesis space
associated with the program class is too large for a complete search and imposes a
high cognitive load on working memory. Since only a fraction of the original rules
is learned, problem solutions which produce action sequences are not complete and
errors become more likely to occur.

16

Lun Ai et al.

Conjecture 2 (Cognitive window of a machine-learned theory): Given a logic
program D representing the deﬁnition of a target predicate, a machine learning
algorithm M , a minimum primitive solution learning algorithm ¯M and examples
E, M (E) is a machine-learned theory using the primitive set φ and belongs to the
program class with hypothesis space S. For a human group H, Eex satisﬁes both

1. Eex(D, H, M (E)) < 0 if |S| > B(M (E), H)
2. Eex(D, H, M (E)) ≤ 0 if Cog(M (E), x) ≥ CogP (E, ¯M , φ, x) for all unseen

query x that h ∈ H have to perform after study

The cognitive window deﬁnition describes that a machine-learned theory has a
harmful explanatory eﬀect on comprehension when learning requires a search in a
hypothesis space that is too large for working memory to manage. In addition, a
machine-learned theory has no beneﬁcial explanatory eﬀect if its cognitive complex-
ity in executing explained knowledge is not lower than the cognitive complexity of a
solution that uses less auxiliary knowledge. Explanations are only beneﬁcial if they
are of appropriate complexity and are not overwhelming nor more cognitively ex-
pensive than a primitive solution of the problem. This cognitive window suggests
that complex machine-learned models and models which cannot provide abstract
descriptions are diﬃcult to be comprehended by humans eﬀectively.

Remark 1 M etagolO is provided a set of composite predicate deﬁnitions Acomp =
{parse_exprs/2, compare_nums/2, single_expr/2, drop_bag_remaining/2,
recycle_memory/2} to learn merger/2 in Table 1. Acomp is constructed from a
set of primitive action deﬁnitions Aprim. The composite actions of breaking num-
ber sequences into individual numbers (parse_exprs/2), comparing two numbers
(compare_nums/2) and appending two sorted sequences (drop_bag_remaining/2)
are suﬃcient for merging and the respective predicates appear in the learned logic
program merger/2. Given action deﬁnitions A = Aprim ∪ Acomp, since composite
actions Acomp are constructed from multiple primitive actions from Aprim, Acomp
is the necessary and suﬃcient subset of A to learn a deﬁnition of merging. This
implies that merger/2 learned by M etagolO(E) is the minimum primitive solution.
Therefore, given test examples X of merging and suﬃcient training examples E,
Cog(merger, x) = CogP (E, M etagolO, A, x) for all x ∈ X.

The complexity analysis based on the Blumer bound [16] in [28] attributes a
sample complexity decrease to the introduction of new predicate symbols to the
machine learner. Previously learned predicate deﬁnitions extend the background.
Reusing learned concepts leads to a decrease in the size of the target hypothesis and
a reduction in the size of the hypothesis space which improves learning performance
[28]. Based on our assumptions in Section 3.4, we consider an analogy between the
human reusing of previously learned concepts and the introduction of predicate def-
initions into a new learning context. By referring to Deﬁnition 9, we attribute a
beneﬁcial eﬀect of a curriculum on human comprehension over another curriculum
to an improvement in the sample complexity based on the analysis in [28].

Conjecture 3 (Sequential teaching curriculum improvement): C1 = Cseq(ST (Ds,
Es, λ1, M s), H) and C2 = Cseq(ST (Ds, Es, λ2, M s), H) are sequential teaching
curricula where Ds is a set of n predicate deﬁnitions and D ∈ Ds is a predicate
deﬁnition, Es are sets of examples, λ1, λ2 : Ds × Es × M s → N are rank functions,

Explanatory machine learning for sequential human teaching

17

M s is a set of machine learning algorithms and H is a human group. R1, R2 denote
ranks of D such that (R1, D) ∈ C1 and (R2, D) ∈ C2. For 1 ≤ i ≤ n such that
(Ri, Di) ∈ C1 and Ri < R1, let p denote the sum of the number of predicates in Di
and u be the minimum number of clauses to express D using Di. For 1 ≤ j ≤ n such
that (Rj, Dj) ∈ C2 and Rj < R2, let p+c denote the number of predicates in Dj and
u + k be the minimum number of clauses to express D using Dj. Eseq(C1, C2, D) > 0
when:

u ln(p) < (u + k) ln(p + c)

Given a target predicate D and a ﬁxed human search bound B(D, H) for a human
group H, human learners are more likely to ﬁnd D if the hypothesis space associated
becomes smaller as a result of learning new knowledge prior to learning D. For a
ﬁxed set of training examples in two curricula, a reduction in sample complexity
correlates to a decrease in predictive error and an improvement in performance.

4 Experimental framework

4.1 Evaluating human comprehension of sorting

The measurements of problem-solving performance for classiﬁcation tasks and se-
quential decision-making tasks are usually diﬀerent due to the nature of the oper-
ations involved. In previous works [64,2], human comprehension was measured by
the accuracy with which participants answered classiﬁcation questions. For the tasks
of merging and sorting, human comprehension corresponds to the competence of
participants to arrange items with respect to the total ordering of the items. Deci-
sions related to the arrangement of items are reﬂected by the classiﬁcation of items
into respective positions in the sequence. While the out-of-sample classiﬁcation ac-
curacy is an objective measure of human comprehension for classiﬁcation tasks, it
does not estimate the error of sequential predictions. Although alternative metrics
exist, we use Spearman’s rank correlation coeﬃcient [87] as a means to assess the
error between the expected sequence and the actual sequence provided by a partic-
ipant. Spearman’s rank correlation coeﬃcient is a non-parametric rank correlation
test and indicates the degree of monotonic ordering between two variables. Given
two sequences of values representing samples of two variables, when Spearman’s rank
correlation coeﬃcient is +1 or −1, it means the variables are perfectly monotonic
functions of each other. On the other hand, when Spearman’s rank correlation co-
eﬃcient tends to 0, it indicates there is no monotonic relationship between the two
variables. We use Spearman’s rank correlation coeﬃcient to examine the extent of
monotonic alignment between a sequence of integers that has been provided by a
human and the correctly arranged sequence produced by a machine algorithm.

In addition, we take into consideration the possibility that participants provide
incorrect answers that are inversely sorted with respect to the order speciﬁed in the
experiment instructions. In this case, a coeﬃcient score tending to −1 would mean
that the participant successfully managed to sort the sequence but misremembered
the speciﬁed ordering. In the experiment trials, we found that such answers rarely
occurred. However, in order to avoid diluting the signiﬁcance of results due to the
inclusion of negative coeﬃcients, we deﬁne the following function to discount negative

18

Lun Ai et al.

coeﬃcients and normalise all of them into the interval [0, 1]. The Spearman’s rank
correlation coeﬃcient lies in the interval [-1, 1] and the absolute value of the negative
coeﬃcient is multiplied by a discounting constant.

Deﬁnition 15 (Normalised performance score, perf (Rh, Rm)) Given two sets
of integers Rh and Rm, a deﬁned discounting constant 0 < c < 1 and the Spearman’s
rank correlation coeﬃcient ρ, the normalised performance score is deﬁned as

(

perf (Rh, Rm) =

ρ(Rh, Rm),
|ρ(Rh, Rm)| · c,

ρ(Rh, Rm) ≥ 0
ρ(Rh, Rm) < 0

Example 6 Let Rh1 denote the sequence [4, 6, 5, 2, 3, 1], Rh2 denote the sequence
[1, 2, 6, 3, 4, 5] and Rm be the perfectly monotonic sequence [1, 2, 3, 4, 5, 6] sorted
by a machine program. Compared with Rm, Rh2 has only one number that is out of
place. However, Rh1 is almost a sequence with decreasing magnitude with numbers 1
and 4 misplaced. Given Rm is a monotonically increasing sequence, Rh2 aligns better
with the machine output Rm and perf (Rh1, Rm) = .386 < perf (Rh2, Rm) = .657.

Based on results from multiple trials, the discounting constant c is set to .5 to

moderately penalise responses that involve reversed sorted sequences.

4.2 Analysis of human sort strategy

In addition to the performance score, we analyse the correspondence between human
sorting strategies and machine sorting algorithms. For sorting, sequential decision
making is revealed through the actions that human sorters make to arrange items
according to a speciﬁed order. In particular, we consider the comparisons of objects
as traces of decision making which indicate the sorting strategy used by human
sorters and the sorting algorithm implemented by a machine program.

Deﬁnition 16 (Human sorting trace, ht(N s, Mh)) Given an input set of integers
N s and a human sorting algorithm Mh, the human sorting trace is a list of pairs
< ni, nj > denoting the comparisons of integers made by the human sorter Mh(N s)
where ni, nj ∈ N s.

Deﬁnition 17 (Machine sorting trace, mt(N s, M )) Given an input set of inte-
gers N s and a machine sorting algorithm M , the machine sorting trace is a list of
pairs < ni, nj > denoting the comparisons of integers made by the machine algorithm
M (N s) where ni, nj ∈ N s.

When there is a consistently high correlation between a human sorter and a ma-
chine algorithm in the way that comparisons are performed, we assume that this
human executes a strategy that closely implements the machine algorithm. Consid-
ering the target human population has limited programming experience, we addi-
tionally assume that a human sorting strategy would implement at best quick sort,
merge sort, insertion sort, bubble sort or dictionary sort. Based on the results and
preferences of the trial participants, we include hybrid variants of insertion sort and
dictionary sort. These hybrid variants initially perform insertion sort until the inter-
mediate result reaches length k and then execute binary searches of the conventional

Explanatory machine learning for sequential human teaching

19

dictionary sort to insert the rest of the numbers into the partially sorted sequence.
In total, we implemented twenty-four algorithms including the conventional imple-
mentations of quick sort, merge sort, insertion sort, bubble sort and dictionary sort
as well as their variants (diﬀerent pivoting, merge and insertion implementations).
We employ two statistical tests to estimate the correspondence between a human
sorting trace and a machine sorting trace. Let n be an integer and N s denote the set
of integers in the interval [1, n]. Let T r be a cross product deﬁned by T r : N s × N s,
a comparison in a trace is a pair < ni, nj >∈ T r where ni, nj ∈ N s. Given a human
trace with sorting algorithm Mh and a machine trace with algorithm M , for every
pair < ni, nj >∈ T r, we check if < ni, nj > is a member of ht(N s, Mh) and if
< ni, nj > is a member of mt(N s, M ). We then perform a Chi-squared test of in-
dependence with a signiﬁcance level α = .025 using the 2 × 2 contingency table and
check for rejection. The signiﬁcance criterion of the Chi-squared test is set at .025 to
reduce the probability of mismatching a human trace with a wrong machine trace.
A rejection of the Chi-squared test’s null hypothesis implies an association between
comparisons made by the human sorter and comparisons made by the program.
Once a test is rejected, we exclude comparison pairs that are not in the intersec-
tion of ht(N s, Mh) and mt(N s, M ) from both traces and compute Spearman’s rank
correlation coeﬃcient [87]. The intention of computing Spearman’s rank correlation
coeﬃcient is to examine the ordering alignment of comparison pairs.

Example 7 Given N s = [4, 6, 5, 2, 3, 1] is a sequence of numbers for a human
participant h and a set of machine algorithms to sort. Let Mh denote the human
sorting algorithm and ht(N s, Mh) = [(6, 4), (5, 2), (3, 1), (4, 2), (5, 4), (6, 5), (2, 1),
(3, 2), (4, 3)]. We are able to ﬁnd a close match of the human trace. The machine
trace is produced by the bottom-up variant of merge sort algorithm denoted as M 0
and mt(N s, M 0) = [(4, 6), (5, 2), (2, 4), (4, 5), (5, 6), (3, 1), (1, 2), (2, 3), (3,
4)]. For each of the N × N combinations of number pairs, we check whether it is a
member of the traces. Two symmetric pairs of numbers, e.g. (4, 6) and (6, 4), are
considered identical. We create the following 2 × 2 contingency table. We add one to

Not in machine trace
In machine trace

Not in human trace
13
1

In human trace
1
10

all frequency counts to avoid including zeros in the table for the Chi-squared test.
The Chi-squared test in this example has a X 2 = 14.3 with p < .001 which conﬁrms
the correlation of number pairs in ht(N s, M h) and mt(N s, M 0). For common pairs
of numbers in ht(N s, M h) and mt(N s, M 0), we obtain their ranks and compute the
Spearman’s rank correlation coeﬃcient. The Spearman’s rank correlation coeﬃcient
is .9 and p < .001 which conﬁrms the monotonic relationship between the two traces.

Since performing comparisons in the reverse order of the machine trace does not
correspond to a reverse execution of the algorithm, we conﬁrm a match (α = .05)
between a human strategy and a machine algorithm only when Spearman’s rank
correlation coeﬃcient is positive.

20

Lun Ai et al.

Table 4: We consider two independent experimental variables. The order of con-
cepts in the curriculum (CO) has two treatments, learning merging before learning
sorting (MS) and learning sorting before learning merging (SM). The presence of
explanations generated from machine-learned rules (EX) also has two treatments,
learning with explanations (WEX) and learning without explanations (WOEX).
The performance score of human participants (PS) is the dependent experimental
variable.

Variable name

Variable type

Treatment abbreviations

Curriculum order (CO)

Independent variable

MS, SM

Presence of explanations (EX)

Independent variable

WEX, WOEX

Performance score (PS)

Dependent variable

–

4.3 Experimental hypotheses

We deﬁne our experimental hypotheses in this section. Table 4 shows the experimen-
tal variables and treatments. We examine the eﬀect of the curriculum order (CO)
via two treatments: a) learning merging before learning sorting (MS) and b) learn-
ing sorting before learning merging (SM). In addition, we assess the eﬀect of the
presence of explanations (EX) by two treatments: a) learning with explanations of
merging (WEX) b) learning without explanations of merging (WOEX). Explana-
tions are not provided for learning sorting to avoid introducing merge prematurely
in the curricula that learn sorting before learning merging. We use the normalised
performance score perf in Deﬁnition 15 to measure both human performances of
merging and sorting (PS).

Let Ds = {merger, sorter} denote target theories of merging and sorting. Let
Es = {Emerge, Esort} where Emerge denotes a suﬃcient set of examples for learning
the target theory of merging and Esort is a suﬃcient set of examples for learning the
target theory of sorting. Both human learners and M etagolO are provided with the
same sets of examples Es. Let M s = {M etagolO, ∅} denote the machine learning
algorithms used to learn rules for generating explanations and ∅ is an empty program.
H stands for a human group. The rank functions for curricula that learn merging
before learning sorting are deﬁned as λM S/W EX and λM S/W OEX . The rank functions
of curricula that learn merging before learning sorting are deﬁned as λSM/W EX and
λSM/W OEX . The deﬁnitions of the rank functions are summarised in Table 5.

Let CM S/W EX = Cseq(ST (Ds, Es, λM S/W EX , M s), H) denote the human
comprehension of the sequential teaching curriculum that learns merging before
learning sorting with explanations of merging. Let CM S/W OEX = Cseq(ST (Ds, Es,
λM S/W OEX , M s), H) denote the human comprehension of the sequential teach-
ing curriculum that learns merging before learning sorting without explanations of
merging. Let CSM/W EX = Cseq(ST (Ds, Es, λSM/W EX , M s), H) denote the hu-
man comprehension of the sequential teaching curriculum that learns sorting before
learning merging with explanations of merging. Let CSM/W OEX = Cseq(ST (Ds, Es,

Explanatory machine learning for sequential human teaching

21

Table 5: Deﬁnitions of the rank functions. In curricula MS/WEX and MS/WOEX,
humans learn merging before learning sorting.
In curricula SM/WEX and
SM/WOEX, humans learn sorting before learning merging. The rank functions
of MS assign a lower rank value to merging. The rank functions of SM assign a
lower rank value to sorting. In curricula MS/WEX and SM/WEX, explanations
about merging are generated from M etagolO.

Curricula abbreviation

Rank function deﬁnitions

MS/WEX

λM S/W EX (merger, Emerge, M etagolO) = 0
λM S/W EX (sorter, Esort, ∅) = 1

MS/WOEX

λM S/W OEX (merger, Emerge, ∅) = 0
λM S/W OEX (sorter, Esort, ∅) = 1

SM/WEX

λSM/W EX (merger, Emerge, M etagolO) = 1
λSM/W EX (sorter, Esort, ∅) = 0

SM/WOEX

λSM/W OEX (merger, Emerge, ∅) = 1
λSM/W OEX (sorter, Esort, ∅) = 0

Table 6: A summary of experimental independent variables, deﬁnitions and results
related to each experimental hypothesis.

Hypotheses

Independent experimental variables

Related deﬁnitions and results

H1
H2
H3

H4

H5

CO
EX
CO and EX

EX

EX

Deﬁnition 5 to 9 and Conjecture 3

Deﬁnition 1 to 4 and Conjecture 2

Deﬁnition 16 and 17

λSM/W OEX , M s), H) denote the human comprehension of the sequential teach-
ing curriculum that learns sorting before learning merging without explanations of
merging. We then introduce hypotheses that focus on how curriculum order and
explanations generated from machine-learned rules aﬀect human learning of sorting.

H1: Learning merging before learning sorting leads to a beneﬁcial eﬀect on human

comprehension of sorting (Eseq(CM S/W EX , CSM/W EX , sorter) + Eseq(CM S/W OEX ,
CSM/W OEX , sorter) > 0) with respect to learning sorting before learning merg-
ing.

22

Lun Ai et al.

In H1, we examine whether human learners achieve better sorting performance
from the curriculum with increasing concept complexity (MS) than from the cur-
riculum with decreasing concept complexity (SM).

H2: Learning merging with explanations results in beneﬁcial explanatory eﬀect on
human comprehension of sorting (Eseq(CM S/W EX , CM S/W OEX ,sorter) +
Eseq(CSM/W EX , CSM/W OEX ,sorter) > 0) with respect to learning merging
without explanations.

In H2, we assess if learning merging with aids in the form of explanations con-
structed from machine-learned rules (WEX) improves sorting performance com-
pared with learning merging without explanations (WOEX).

H3: Learning merging with explanations further increases the beneﬁcial eﬀect of cur-

riculum order on human comprehension of sorting (Eseq(CM S/W EX , CM S/W OEX ,
sorter) - Eseq(CSM/W EX , CSM/W OEX , sorter) > 0) with respect to learning
merging without explanations.

In H3, we inspect if there is an interaction eﬀect between curriculum order (CO)

and explanations (EX) on sorting performance.

We suspect that for human learners who have no previous programming experi-
ence, learning merging with the presence of explanations can result in adapting new
sorting strategies in comparison with learning merging in the absence of explanations.
Since merging can be taught in isolation and it does not depend on the knowledge of
sorting in the curricula, we refer to Deﬁnition 4 and the cognitive window to account
for the eﬀect of explanations on merging. However, owing to M etagolO learning a
minimum primitive solution merger/2, we anticipate no beneﬁcial eﬀect on human
comprehension of merging as a result of learning from explanations. Let A denote the
background knowledge of M etagolO containing deﬁnitions of primitive and compos-
ite actions and X denote questions that the human group H performs in the merge
performance test.

H4: Learning merging with explanations generated from rules without a low cognitive
cost (Cog(merger, x) ≥ CogP (Emerge, M etagolO, A, x) for all x ∈ X) does
not result in a beneﬁcial explanatory eﬀect on human comprehension of merging
(Eex(merger, H, merger) ≤ 0).

In H4, we examine if learning with explanations of merging (WEX) results in
no observable performance diﬀerence or worse performance with respect to learning
without explanations of merging (WOEX).

H5: Learning merging with explanations (WEX) before learning sorting leads to

switching of eﬃcient human sorting strategies with better performance.

Through human trace analysis, we estimate the correspondence between human
sorting strategies and machine sorting algorithms in training and performance tests.
In H5, we inspect the performance of speciﬁc sorting strategies that increase in usage
from sort training to sort performance test.

Explanatory machine learning for sequential human teaching

23

5 Experiments

5.1 MaRs-IB pre-test

We have reason to assume that the participant’s performance in the experimental
task varies depending on their cognitive abilities. Recording their cognitive ability
provides us with two possibilities:

1. It allows statistically control of the participant’s cognitive ability.
2. It can be determined whether the mean cognitive ability in all experimental con-
ditions is the same. If that is the case, mean diﬀerences in performance between
the conditions can be interpreted without having to control for each participant’s
cognitive ability ﬁrst.

Fig. 2: Each MaRs-IB item consists of an incomplete 3 × 3 matrix in which 8 cells
with shaped icons depict a modiﬁcation rule to (an) abstract shape(s). The test asks
the participant to ﬁll the missing piece at the right bottom corner of the 3 × 3 matrix
by choosing one of four provided options (A, B, C and D). The correct answer to
this example is A.

There have been two major suggestions for recording this control: either confront
participants with a version of the experimental task itself or use a short intelligence
test in the form of a Raven Matrices short test [12] or similar material. We argue
that using the experimental task will undermine the sequential nature of our exper-
iment, as either, groups would start with the low complexity task or with the high
complexity task. A third option is for each group to start with their respective ﬁrst
task, which would introduce task familiarity eﬀects (the high-complexity-ﬁrst con-
dition would have completed more high-complexity tasks than the low-complexity
ﬁrst condition). This approach does not pose a good control since each group would
complete a diﬀerent task. Using an intelligence test avoids interference with the ex-
periment since the test should not inﬂuence strategy development in the experimental
task in a way that could be considered to work against the sequential order of the
experiment. However, the original Raven Matrices test and many comparable tests

24

Lun Ai et al.

are expensive and cannot be digitized due to copyright. The matrix reasoning item
bank (MaRs-IB) has neither of these two issues [25], thus making it an appropriate
candidate for establishing a measure of intelligence in an online experiment. In ad-
dition, the authors of the MaRs-IB test set reported acceptable internal consistency
and convergent validity, and reasonable test-retest reliability [25]. Though there is no
normalized score which would be comparable to the IQ, the MaRs-IB test can serve
as a measure of the relative cognitive ability of participants. Another concern regards
the open access to the material [25] and familiarity with the test from other studies.
This risk can be mitigated by randomised arrangement and selection of items from
the multiple test sets provided by the authors.

Application

The authors [25] provided three sets of 80 items (Figure 2). Each of these sets uses
diﬀerent symbols and is available in three further variations to compensate for colour
vision deﬁcient participants. The testing procedure described in the validation paper
[25] will be consistently completed in under 10 minutes (8 minutes test and a series of
practice items beforehand) and is thus applicable for our use case. Participants will
train on 10 practice items until they completed three of them correctly. An item in the
full test starts with a 500 milliseconds ﬁxation cross, followed by a 100 milliseconds
white screen mask [25]. The matrix will then be displayed for 30 seconds, or until the
participant responded (whichever happens earlier). After 25 seconds a clock indicates
that participants have 5 seconds left for this speciﬁc item [25]. Participants complete
these items for a total of 8 minutes. The accuracy with which the participant chooses
the correct missing pattern in the given time frame is considered the measurement
of pre-test performance.

5.2 Materials

The arrangement of objects arguably makes up a great proportion of human daily
tasks. This implies a high familiarity of human participants with the task of sort-
ing. The preferred sorting approaches of humans with no background knowledge
in programming could resemble some sorting algorithms. However, we assume that
learning the recursive algorithms such as merge sort without a well-deﬁned curricu-
lum demands a great eﬀort and applying such algorithms eﬃciently is challenging.
To collect information about participants’ previous experience relating to sorting
algorithms, we additionally asked participants to ﬁll out the following questions af-
ter the completion of all performance tests: 1) whether participants have a degree
in computer science or a certiﬁcate in programming, 2) if they have studied or are
learning sorting algorithms in school and 3) whether they know or have applied any
sorting algorithms while being presented with a list of sorting algorithms (two of
which are distracting choices and do not exist).

To prevent participants from relying on the total ordering of natural numbers, we
designed an isomorphic problem that represented integer sequences by the weights
of fruits in piles. Numbers were masked by visual icons of fruits and the numeric
values were hidden from participants. A balance scale mechanism (Figure 4a) was
introduced to allow participants to compare the weights between two fruits and acted

Explanatory machine learning for sequential human teaching

25

as an instrument for constructing the total ordering of the numbers. The immedi-
ate access to the total ordering of natural numbers was prevented and participants
had to establish it themselves. This setup was used for teaching both merging and
sorting. A solution of this setup corresponded to a solution of merging and sorting a
sequence of numbers without changing the underlying solution structure. According
to the deﬁnition of isomorphic problems [86], the new fruit weighting problems are
isomorphic to the respective problems for merging and sorting numbers. Such iso-
morphic translations only aﬀect the initial stage of problem-solving while attempting
to recognise a useful analogy via analogical access but do not hinder problem-solving
via analogical inference if an analogy for the problem has been consciously recognized
[37, 43,74]. Owing to the diﬃculty of immediately recalling past experiences of merg-
ing and sorting, we anticipated that participants would initially perform less optimal
merging and sorting strategies and there would be more potential for performance
improvement later.

In creating the training and performance test for merging and sorting, we ran-
domly sampled sequences of various lengths to provide a spectrum of problem diﬃ-
culty. For each of the question sequences, we made sure that it was not trivial and
required an appropriate level of computational eﬀort for a machine program to sort.
A question in the merge training and performance test involved two sub-sequences
of similar sizes as inputs and the length of each sub-sequence ranged from 1 to 4. In
the sort training and performance test, a question consisted of a fruit sequence and
the length of sequences varied from 6 to 10. In the pilot trials, we identiﬁed a higher
usage of the insertion sort algorithm. For generating sort training and testing ques-
tions, we made an adjustment to include sequences that would lead to an advantage
in the number of comparisons made when applying the merge sort algorithm over
the insertion sort algorithm. This change would allow room for people to optimise
their sorting strategy instead of adhering to what they already knew.

5.3 Method and design

The experiment2 was implemented based on a four-group factorial 2×2 design to ac-
count for all combinations (Table 7) of the two independent experimental variables,
the curriculum order (CO) and the presentation of explanations (EX). To make the
experiment more engaging, we introduced the task background where each partici-
pant was asked to help two robots, Alice and Bob, perform tasks in two warehouses.
Each robot was responsible for a distinct task and provided information about the
associated task to the participant. The robot Alice was designed to teach merging
while the other one Bob taught participants how to sort. To evoke an understanding
of the connection between merging and sorting, at the beginning of the experiment,
participants were explicitly reminded with a note of the link between task materi-
als taught by two robots and were encouraged to pay attention to the connection.
Task-related information was given by the robot and presented via the interface
throughout the experiment.

The problem of merging was set up in a warehouse where participants were asked
to learn from the robot Alice and help it operate a special machine to arrange fruits

2 The experiment interface is available on https://github.com/LAi1997/sequential-teaching.
The interface was created using PsychoPy [70], an open-source package for implementing free
interfaces with stimulus presentation and control in Python and JavaScript.

26

Lun Ai et al.

Table 7: Group 1 learned merging before learning sorting and was provided with
visual and textual explanations of merging based on machine-learned rules. Group 2
received the same curriculum as Group 1 but without explanations. The participants
in Group 3 learned sorting prior to learning merging with explanations in the same
format as received by Group 1. Group 4 did not get explanations.

EX

CO

WEX

WOEX

MS

Group 1 (MS / WEX) Group 2 (MS / WOEX)

SM

Group 3 (SM / WEX) Group 4 (SM / WOEX)

Fig. 3: All groups received the same training examples and questions in all sections.
The numbers of questions in experiment sections merge training, merge testing, sort
training and sort testing were 6, 5, 4 and 8. All experiment sessions would start
with a participation consent followed by the MaRs-IB pre-test and an introduction
to the task background. In setup (a), participants learned merging before learning
sorting (MS). In setup (b), participants learned sorting before learning merging. All
groups received a performance test of sorting last. This experiment design ensured
all groups experienced the same amount of training and performance tests respecting
the curriculum order.

on a conveyor belt. In Figure 4a, fruits labelled with A, B, C and D were inputs to a
machine called the “blue star” that represented the merging operation. Each question
in the sections related to merging asked the participant to merge two sequences
of fruits. The balance scale to the left of the interface page (Figure 4a) allowed
participants to compare the weights of fruits by entering two fruits’ alphabetic labels
on both sides of the balance. Participants were presented with instructions to use
the balance scale. The balance would tilt to the side of the fruit that is heavier. A
piece of text would be shown to further clarify the result of comparing two fruits.
Participants were provided with the information that the two input sequences of
fruits in the questions had increasing weights from left to right. Two output answer
choices diﬀered in the fruits that were highlighted with the yellow colour. Participants
were asked to select from one of the two output answer choices that had the fruits in
the correct order. Feedback was presented (Figure 4b) on whether the participant’s
selection was correct. Group 1 (MS/WEX) and Group 3 (SM/WEX) additionally
received textual and graphical explanations that instantiated the logic rules learned
by M etagolO in Table 1.

Explanatory machine learning for sequential human teaching

27

(a) Participants were asked to distinguish between two alternative choices by using the provided
balance scale and submit the correct sequence by pressing a button to the right.

(b) On the top row, the visualisations show the comparison result (fruits B and C). The
textual explanations explicitly describe the objects involved (fruits B and C) and the actions
performed (comparison and appending) which resulted in C being appended before B due to
C’s lesser weight. On the bottom row, the interface highlighted in red that fruits were out of
order (B and C) and described the correct ordering.

Fig. 4: In merge training, participants ﬁrst answered the displayed question illus-
trated by Figure 4a and were taken to the next page where feedback and explanations
were presented. To the right of the page in Figure 4b, explanations are presented in
two rows which demonstrate the correct and the wrong action sequences.

The format in which participants were asked to input their responses is the same
in the merge performance test, sort training and sort performance test. When pro-
viding a sequence response, participants were required to input labels representing
the fruit sequence that they believed to have increasing weights from left to right.
Another robot instructor Bob helped participants learn to operate the machine “pur-
ple diamond” to sort fruits. In the sort training and performance test, participants

28

Lun Ai et al.

Fig. 5: Sort training interface. In sort training, participants were advised to sort
sequences of fruits (in this case fruits A, D, B, C, E and F) in the centre white box
called the “whiteboard” and provided instructions to eﬃciently use it. The “white-
board” was a device of the interface which allowed a free arrangement of the input
fruit sequence by using a mouse to drag and drop fruits. The labels and fruits were in-
tentionally randomised before the experiment so that participants would not receive
the wrong impression that the sequence had been already alphabetically ordered.

should put fruits into shipping boxes by entering fruit labels into a sequence of text
boxes. In the sort training, participants were informed of the number of compar-
isons their robot instructor Bob made to sort the input sequences. Participants were
asked to sort the same input sequences and encouraged to use fewer comparisons.
The intention of this competition was to provide an incentive for participants to
consciously think about how to revise their strategy in order to improve sorting eﬃ-
ciency. This information was accessible by participants in all groups during the sort
training. In the sorting sections, participants were also provided with the balance
scale instrument.

Participants were encouraged to take two sessions of one-minute rest, one after
the MaRs-IB pre-test and one prior to the ﬁnal performance test of sorting. The break
sessions allowed participants to recharge and regain attention to perform tasks in
later stages. The progress bar at the top of the interface showed the percentage of
materials that the participants had completed with respect to all materials in the
experiment. Participants could use the progress bar to estimate the time that they
would need to ﬁnish the experiment within the allocated session. For all training and
performance test sections, we recorded response time, answers and the comparisons
made for analysis. The ﬁnal section included four reﬂective questions. We asked
participants whether they understood the connection between the two tasks or used
what they learned in one task for the other task. We also checked their background
in programming and sorting algorithms in the survey at the end of the experiment.

Explanatory machine learning for sequential human teaching

29

(a) Mean merge performance test scores and
standard errors.

(b) Mean sort performance test scores and
standard errors.

Fig. 6: Performance test scores (PS) of four groups, Group 1 (MS/WEX) Group 2
(MS/WOEX), Group 3 (SM/WEX) and Group 4 (SM/WOEX) with standard
errors. Across the x-axis in (a), data points indicate the change in the mean merge
performance score as a result of learning with explanations. In (b), the red line shows
the mean sort performance score when participants learned with explanations and
the blue line represents the mean sort performance score when participants learned
without explanations. Across the x-axis in (b), data points show the change in the
mean sort performance score as a result of altering the curriculum arrangement.

5.4 Results

Prior to the experiment, we conducted two experimental trials with one sample of
students from the University of Bamberg and another student sample from Imperial
College of London. The trial studies provided additional insights into the interface
and helped us revise the experimental design. However, due to the limited scale
of the trials, we were not able to observe any statistically signiﬁcant outcome. We
proceeded to recruit a larger sample of 124 mixed background participants from
Amazon Mechanical Turk (AMT).

These 124 participants from AMT were randomised and assigned to the four
groups. A partition of 79 participants was created based on the MaRs-IB test and
those included had a test accuracy within one standard deviation (σ = .169) of the
mean (µ = .654). Demographically, this partition had a lower male-to-female ratio
(36 to 42, one opted out), most participants were between 25 to 54 in age (61 of 79)
and almost all had a college degree or higher (67 of 79). We focused on this AMT
sample partition and employed a quantitative approach to test hypotheses H1 to
H5. The AMT partition had an even distribution of participants in the four groups,
Group 1 (MS and WEX, n = 20), Group 2 (MS and WOEX, n = 20), Group
3 (SM and WEX, n = 20) and Group 4 (SM and WOEX, n = 19). Figure 6a
and Figure 6b present the mean performance score (PS) of the groups in the merge
performance test and sort performance test. For each performance test, we carried

30

Lun Ai et al.

Table 8: Mean strategy frequency in sort training and sort performance test. Each
human sorting trace in the sort training and sort performance test was matched to
one of the following categories of sorting algorithms, bubble sort (BS), dictionary
sort (DS), insertion sort (IS), merge sort (M S), quick sort (QS), a combination
of insertion and dictionary sort (Hybrid) and unclassiﬁed (Other). The number of
human sorting traces in each category was averaged by the total number of traces. We
highlight positive diﬀerences greater than .05 in bold to indicate signiﬁcant increases
in algorithm application frequency from sort training to sort performance test.

Categories

PS

Group 1 (MS/WEX)
Training
Performance test
Diﬀerences
Group 2 (MS/WOEX)
Training
Performance test
Diﬀerences
Group 3 (SM/WEX)
Training
Performance test
Diﬀerences
Group 4 (SM/WOEX)
Training
Performance test
Diﬀerences

BS

–
.012
.056
.044
–
.000
.012
.012
–
.012
.019
.007
–
.000
.013
.013

DS

IS

M S

QS

Hybrid Other

–
.075
.094
.019
–
.062
.038
-.024
–
.050
.138
.088
–
.079
.099
.020

–
.150
.162
.012
–
.162
.181
.019
–
.088
.100
.012
–
.184
.243
.059

–
.000
.025
.025
–
.025
.100
.075
–
.038
.025
-.013
–
.026
.053
.027

–
.175
.238
.063
–
.162
.194
.032
–
.225
.244
.019
–
.158
.158
.000

–
.162
.175
.013
–
.225
.181
-.044
–
.175
.119
-.056
–
.237
.237
.000

–
.425
.250
-.175
–
.362
.294
-.068
–
.412
.356
-.056
–
.316
.197
-.119

out an ANOVA test to assess the eﬀect of the independent experimental variables
followed by a Tukey’s HSD test. We used a signiﬁcance criterion α = .05 for both
tests.

Figure 6a illustrates almost no change to the mean merge performance score
when explanations were introduced. The one-way ANOVA test did not demonstrate
a signiﬁcant eﬀect of the presence of explanations (EX) on the merge performance
test score (PS). Figure 6b illustrates a pattern that switching from the curriculum
order MS to SM results in a reduction in performance. The two-way ANOVA test
indicated a signiﬁcant eﬀect (F = 10.4, p = .001 < .05) of curriculum order (CO)
on sort performance test score (PS). The post-hoc Tukey’s HSD test conﬁrmed a
signiﬁcant positive diﬀerence (p = .001 < .05) between MS groups (Group 1 and
Group 2) and SM (Group 3 and Group 4). The two-way ANOVA test did not show
a signiﬁcant eﬀect of the presence of explanations (EX) on sort performance test
score (PS) and there is no evidence of an interaction eﬀect between curriculum order
(CO) and presence of explanations (EX).

We then inspected the correlation between human sorting strategy and machine
sorting algorithms based on the trace analysis described in Section 4.2. Given the
same input number sequences, we analysed comparisons made by participants in
sort training and performance test and comparisons made by machine sorting al-
gorithms. The correspondence between human comparison traces and comparison
traces produced by machine algorithms was ﬁrst examined by a Chi-squared test
followed by the computation of the Spearman rank correlation coeﬃcient. According
to Table 8, from training to performance test the average use of quick sort in Group

Explanatory machine learning for sequential human teaching

31

1 (MS/WEX) increased by .063 and the average application of dictionary sort in
Group 3 (SM/WEX) improved by .088. Further t-tests (α = .05) on performance
test scores showed quick sort like approaches (µ = .834, σ = .274) achieved a higher
mean score (p = .043 < .05) than the rest of the strategies (µ = .729, σ = .337)
and responses corresponding to dictionary sort like approaches (µ = .885, σ = .288)
had a better performance (p = .0074 < .05) than the other strategy categories
(µ = .679, σ = .373).

For Group 2 (MS/WOEX), the frequency of application of merge sort increased
by .075 and for Group 4 (SM/WOEX) the average use of insertion sort increased
by .059. These two groups did not received explanations (WOEX). Further t-tests
(α = .05) showed no performance diﬀerence in Group 2 between merge sort like
approaches (µ = .872, σ = .209) and the other strategies (µ = .827, σ = .288)
whereas insertion like approaches (µ = .877, σ = .285) in Group 4 achieved a higher
mean score (p = .001 < .05) than the other strategies (µ = .644, σ = .399).

5.5 Discussion

In Table 9, we present experimental hypotheses H1 to H5 and summarise their test
outcomes. Based on the deﬁnition of sorter/2 learned by M etagolO after learning
merger/2 in Table 1, the size of the program is u = 3 and the number of predicates
including those used in merger/2 is p = 8. For the deﬁnition of sorter/2 learned
by M etagolO without learning merger/2 in the same table, the size of sorter/2 is
u + k = 5 and the number of predicates used is p + c = 6. According to the Conjec-
ture 3, 3 · ln(8) < 5 · ln(6) and learning merging before learning sorting results in a
reduction in the size of the hypothesis space associated with the target hypothesis
of sorting. The ANOVA test and Tukey’s HSD test on sort performance test scores
(Figure 6b) demonstrated a beneﬁcial eﬀect from curriculum order as changing the
arrangement from learning sorting ﬁrst to learning merging ﬁrst corresponds to an
improvement in performance score and hence a better human comprehension of sort-
ing. This evidence conﬁrms hypothesis H1 and Eseq(CM S/W EX , CSM/W EX , sorter)
+ Eseq(CM S/W OEX , CSM/W OEX , sorter) > 0 which supports Conjecture 3.

Results on merge and sort performance tests did not show explanatory eﬀects of
the merger/2 rules learned by M etagolO. The ANOVA test did not show a signif-
icant eﬀect of explanations on human comprehension and there was no signiﬁcant
interaction eﬀect on human comprehension. Therefore, we reject hypotheses H2 and
H3 due to the lack of evidence. Learning merging with explanations did not improve
nor degrade human comprehension of sorting in regard to learning merging without
explanations. In addition, the ANOVA test on the merge performance test scores
did not demonstrate an eﬀect of explanations on human comprehension. This result
conﬁrms H4 and supports Conjecture 2. Since merging was taught in isolation inde-
pendent from other secondary or tertiary concepts, we refer to the cognitive window
(Remark 1) for a plausible account of this lack of explanatory eﬀect on human com-
prehension of merging. The cognitive cost of executing explained rules of merging is
not suﬃciently lower than the cognitive cost of operating a merging solution without
the auxiliary information. Owing to limited cognitive capacities, a reduction in com-
putational cost usually corresponds to an improvement in performance. However,
solutions of merging after receiving explanations were not suﬃciently optimised to
yield an observably higher performance score compared with human primitive so-

32

Lun Ai et al.

Table 9: H1 to H3 are hypotheses concerning eﬀects of curriculum order and expla-
nations on human sorting comprehension. Hypotheses H4 and H5 relate to explana-
tory eﬀects on human comprehension of merging and sorting strategy. At the top of
table, H stands for hypothesis, T denotes test outcome. In the rightmost column, C
is a abbreviation for conﬁrmed, and N stands for not conﬁrmed.

H

H1 Learning merging before learning sorting leads to a beneﬁcial eﬀect on hu-
man comprehension of sorting with respect to learning sorting before learning
merging

H2 Learning merging with explanations results in a beneﬁcial explanatory eﬀect
on human comprehension of sorting with respect to learning merging without
explanations

H3 Learning merging with explanations further increases the beneﬁcial eﬀect of
curriculum order on human comprehension of sorting with respect to learning
merging without explanations

H4 Learning merging with explanations generated from rules without a low cog-
nitive cost does not result in a beneﬁcial explanatory eﬀect on human com-
prehension of merging

T

C

N

N

C

H5 Learning merging with explanations before learning sorting leads to switching

C

of eﬃcient human sorting strategies with better performance

lutions in the absence of explanations. Given the tasks and the sequential teaching
setup, an incremental curriculum order had a signiﬁcant eﬀect which improved hu-
man comprehension. The results provided a demonstration of the beneﬁcial eﬀect of
sequential teaching on human comprehension.

Furthermore, we examined human comparisons and estimated sorting algorithms
that best resembled human sorting traces. This analysis (Table 8) led to the recogni-
tion of four distinguished sorting strategy adaptions. Strategy adaptions in Group 1
(MS/WEX), Group 3 (SM/WEX) and Group 4 (SM/WOEX) resulted in higher
performance scores of the associated responses compared with the other strategies.
After sort training, Group 1 (MS/WEX) used a higher volume of quick sort like
strategies. The quick sort algorithm is computationally eﬃcient with an average lin-
earithmic run-time and a worst-case quadratic run-time. We observed that the adap-
tation to quick sort led to better comprehension compared with responses that used
the other sorting strategies. This evidence allows us to conﬁrm hypothesis H5. In
addition, there was an increased number of dictionary sort like strategies in Group
3 (SM/WEX) after training. Utilising the eﬃciency beneﬁt of concentrating on
parts of the original problem one at a time, quick sort creates a pivot to construct
two sorted sub-sequences. Though dictionary sort is not conventionally considered
a divide-and-conquer algorithm, it iteratively makes binary searches for the correct
position of an object in a sequence.

The adaptations to eﬃcient sorting approaches with improved performance only
happened in groups that received explanations. We attribute this phenomenon to
a better understanding of eﬃciently merging: two input sequences are sorted and
redundant comparisons can be avoided by interleaving comparisons of fruits from

Explanatory machine learning for sequential human teaching

33

input sequences. Without explanations, participants might direct more of their at-
tention towards the correctness of their choices. Provided explanations emphasised
the optimal problem-solution structure and illustrated the action sequence by walk-
ing through examples with participants. The contexts provided by explanations and
examples are eﬀective for the human learning of abstract concepts [3, 6, 15, 75]. We
suggest that in the presented teaching setup, explanations of merging involving ex-
amples grounded the concept of binary selection and contextualised the construction
of problem solutions that utilised this information. As a result, problem solutions
that incorporated this idea had less potential for errors and achieved higher per-
formance. This proposition can be partially supported by the increase in usage of
insertion sort like strategies in Group 4 (SM/WOEX) which did not receive ex-
planations. Although insertion sort like strategies correlated to better scores in sort
performance test responses of Group 4, the insertion sort algorithm does not employ
a divide-and-conquer nor binary selection approach to sorting and does not share
these features with quick sort or dictionary sort.

Another observation from Table 8 is that a signiﬁcant number of responses in
Group 2 (MS/WOEX) adapted to merge sort. While this strategy adaption of
Group 2 to merge sort did not correlate with a higher performance score compared
with the other strategies, divide-and-conquer algorithms such as quick sort and merge
sort that require knowledge in programming could be diﬃcult for novices to fully
learn in a short time. It is noteworthy that the sorting traces of Group 1 and Group
2 indicated that a signiﬁcant number of participants appeared to have developed
sorting approaches similar to these well-known divide-and-conquer algorithms when
participants were presented with concepts with increasing complexity. An incremen-
tal curriculum scheme might accommodate the analogical transfer of problem-solving
structure which makes learning recursion less challenging [71]. A tentative account
of this phenomenon can be referred to Conjecture 3. Learning knowledge in an in-
cremental fashion reduces the size of hypothesis space and thus makes it easier for
humans to ﬁnd a consistent hypothesis that incorporates new information. Although
it cannot be assessed by the present framework whether the participants mentally
formulated quick sort or merge sort in an explicit way, further investigations of
sequential teaching could devise machine learning to derive human strategy from
behavioural traces.

6 Conclusions and further work

Previous publications [64,2] on the topic of comprehensibility have investigated the
classiﬁcation of explanatory machine learning into beneﬁcial and harmful categories.
The current work proposes an extension of frameworks of comprehensibility [64]
and explanatory eﬀects [2] to account for the eﬀects of sequence teaching. Owing to
the reduction in the size of the hypothesis space [28], we hypothesise that learning
concepts with increasing complexity enables humans to learn more compact target
hypotheses. This conjecture is supported by our empirical results. We have identi-
ﬁed an instance of sequential teaching curricula in which learning merging before
learning sorting results in a better human comprehension of sorting in contrast to
learning sorting before learning merging. This result demonstrates an improvement
in human comprehension as a result of learning concepts with increasing complexity.
We refer to the cognitive window [2] to account for the lack of explanatory eﬀects on

34

Lun Ai et al.

human comprehension of merging. It is diﬃcult to improve human comprehension
from explanations generated from machine-learned rules when the cognitive cost of
the task is already low. In this case, explanations do not cause a suﬃciently high
improvement in the cognitive cost of performing the task. However, we have taken a
rather conservative approach which has led to interesting preliminary results of se-
quential teaching curricula in a speciﬁc domain. We also acknowledge the potential
of extending comprehensibility deﬁnitions beyond symbolic machine learning and
our noise-free framework for future work.

Instruction-based teaching approaches have the advantage of clarity and direct-
ness for human learning [90] and allow previous knowledge to be integrated with new
problem-solving contexts [34]. However, such teaching approaches may impose an
over-restriction on the links between actions and outcomes and therefore fail to trig-
ger an individualised understanding generalised from the material [19]. In the human
trial, we employed a mixed approach to teaching that combined instruction-based
learning with discovery learning. We explored a machine-human teaching interaction
where curricula allow a higher degree of freedom for learners to re-discover compu-
tationally eﬃcient algorithms. We observed adaptations of human sorting strategies
to utilise techniques of divide-and-conquer and recursion. Such strategy adaptations
were observed in humans who learned with explanations and in humans who learned
concepts with incremental complexity. We attribute these results to the contextuali-
sation of abstract concepts by explanations and the accessibility of learned knowledge
from the reduction in complexity in searching the hypothesis space. In these cases,
human learners were able to adapt sorting strategies reminiscent of eﬃcient machine
sorting algorithms. Although the analysed participant sample had no background in
programming, they adapted sorting strategies by employing divide-and-conquer and
recursion like techniques commonly found in eﬃcient machine sorting algorithms.
While not all inspected human learners in the experiment managed to consistently
perform these sorting algorithms after studying from short learning sessions, being
able to develop “near-miss” versions of the merge sort algorithm is itself an achieve-
ment for those novices of programming.

An exciting prospect of sequential teaching curricula is that machines and hu-
mans could take up more active roles to enable two-way cooperative learning [85].
Sequential teaching curricula can be extended to represent the coordination between
humans and machines in a back-and-forth fashion. Human implicit decision making
(System 1) could be made explicit (System 2) [45] to beneﬁt human problem solving
[81] by machine-learned theories via a process known as behavioural cloning [18].
Traces of human problem solutions can be provided as inputs to an ILP system,
which might produce explicit algorithms from the traces to present to the human as
explanations. A “clean up” eﬀect on a human’s behaviours can be achieved based
on the error estimation of the machine-learned human algorithm in training. The
debugging of behaviours guided by the prediction of error of a human strategy can
be beneﬁcial in intelligent tutoring [95] and for cooperative programming in algo-
rithmic debugging [84]. Future work may also explore the phenomenon of human
re-discovery of well-established domain knowledge. Our observation shows the cre-
ativeness of human learning in exploiting the analogical transfer of partial solution
structure. For machine learners, insights obtained in other domains can be utilised to
learn ﬂexible and eﬃcient problem solutions [17,49]. For human learners, the division
of information into smaller “chunks” and the presentation of rule-based explanations
facilitate the transfer of prior knowledge into a new problem-solving context [10]. The

Explanatory machine learning for sequential human teaching

35

inclusion of both teaching techniques has the potential to trigger sudden realisations
of previously incomprehensible concepts that might lead to innovative learning out-
comes. However, further investigations are required to gain a better understanding of
the degree to which sequential teaching curricula with machine-learned explanations
facilitate novel human comprehension.

Acknowledgements

This research was partially supported by TAILOR, a project funded by EU Horizon
2020 research and innovation programme under GA No. 952215. The third author
acknowledges support from the UK’s EPSRC Human-Like Computing Network.

References

1. A. Adadi and M. Berrada. Peeking inside the black-box: A survey on explainable artiﬁcial

intelligence (xai). IEEE Access, 6:52138–52160, 2018.

2. L. Ai, S. Muggleton, C. Hocquette, M. Gromowski, and U. Schmid. Beneﬁcial and harmful

explanatory machine learning. Machine Learning, 110:695–721, 2021.

3. V. Aleven and K. R. Koedinger. An eﬀective metacognitive strategy: Learning by doing
and explaining with a computer-based cognitive tutor. Cognitive science, 26(2):147–179,
2002.

4. L. Alﬁeri, P. Brooks, N. Aldrich, and H. Tenenbaum. Does discovery-based instruction

enhance learning? Journal of Educational Psychology, 103:1–18, 2011.

5. O. Amir, F. Doshi-Velez, and D. Sarne. Summarizing agent strategies. Autonomous Agent

Multi-Agent System, 33:628–644, 2019.

6. J. R. Anderson, J. M. Fincham, and S. Douglass. The role of examples and rules in the
acquisition of a cognitive skill. Journal of experimental psychology: learning, memory,
and cognition, 23(4):932, 1997.

7. J. R. Anderson, N. Kushmerick, and C. Lebiere. Rules of the Mind, chapter The Tower

of Hanoi and goal structures, pages 121–142. Hillsdale, NJ: L. Erlbaum, 1993.

8. J. R. Anderson and R. Thompson. Use of Analogy in a Production System Architecture,

page 267–297. Cambridge University Press, USA, 1989.

9. S. Anjomshoae, A. Najjar, D. Calvaresi, and K. Främling. Explainable agents and robots:
In Proceedings of the 18th International

Results from a systematic literature review.
Conference on Autonomous Agents and MultiAgent Systems, page 1078–1088, 2019.
10. Y. Anzai and H. Simon. The theory of learning by doing. Psychological Review,

86(2):124–140, 1979.

11. A. Arrieta, N. Díaz-Rodríguez, J. Ser, A. Bennetot, S. Tabik, A. Barbado, S. Garcia,
S. Gil-Lopez, D. Molina, R. Benjamins, R. Chatila, and F. Herrera. Explainable artiﬁcial
intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible
ai. Information Fusion, 58:82–115, 2020.

12. W. Arthur Jr and D. V. Day. Development of a short form for the raven advanced
progressive matrices test. Educational and Psychological measurement, 54(2):394–403,
1994.

13. M. Bain and S. H. Muggleton. Machine Intelligence 13: Machine Intelligence and Induc-
tive Learning, chapter Learning Optimal Chess Strategies, page 291–309. Oxford Univer-
sity Press, Inc., USA, 1994.

14. A. G. Barto, R. S. Sutton, and C. Watkins. Learning and sequential decision making.

University of Massachusetts Amherst, MA, 1989.

15. D. C. Berry and D. E. Broadbent. Complex problem solving: The European perspective,
chapter Implicit learning in the control of complex systems, pages 131–150. Lawrence
Erlbaum Associates, Inc., 1995.

16. A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the

vapnik–chervonenkis dimension. Journal of the ACM, 36(4):929–965, 1989.

17. I. Bratko. Discovery Science. DS 2010. Lecture Notes in Computer Science, volume 6332,
chapter Discovery of Abstract Concepts by a Robot. Springer, Berlin, Heidelberg, 2010.

36

Lun Ai et al.

18. I. Bratko, T. Urbančič, and C. Sammut. Behavioural cloning: Phenomena, results and

problems. IFAC Proceedings Volumes, 28(21):143–149, 1995.

19. J. S. Bruner. The art of discovery. Harvard Educational Review, 31:21–32, 1961.
20. J. S. Bruner, J. J. Goodnow, and G. A. Austin. A study of thinking. New York, NY:

Wiley, 1956.

21. M. Burke, S. Penkov, and S. Ramamoorthy. From explanation to synthesis: Compositional
program induction for learning from demonstration. In A. Bicchi, H. Kress-Gazit, and
S. Hutchinson, editors, Robotics: Science and System XV, 2019.

22. J. Carbonell. Derivational analogy: A theory of reconstructive problem solving and exper-

tise acquisition. Machine Learning, 11:26, 1985.

23. P. Carpenter, M. Just, and P. Shell. What one intelligence test measures: A theoretical
account of the processing in the raven progressive matrices test. Psychological review,
97:404–431, 1990.

24. M. Chi and S. Ohlsson. Complex Declarative Learning. Cambridge University Press, 2005.
25. G. Chierchia, D. Fuhrmann, L. J. Knoll, B. P. Pi-Sunyer, A. L. Sakhardande, and S. Blake-
more. The matrix reasoning item bank (mars-ib): novel, open-access abstract reasoning
items for adolescents and adults. Royal Society open science, 6(190232), 2019.

26. R. C. Craig. Directed versus independent discovery of established relations. Journal of

Educational Psychology, 47:223–234, 1956.

27. A. Cropper. Eﬃciently learning eﬃcient programs. PhD thesis, Imperial College London,

2017.

28. A. Cropper. Playgol: Learning programs through play.

In Proceedings of the Twenty-
Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI-19, pages 6074–
6080, 2019.

29. A. Cropper and S. H. Muggleton. Learning eﬃcient logical robot strategies involving
In Proceedings of the 24th International Conference on Artiﬁcial

composable objects.
Intelligence, page 3423–3429, 2015.

30. A. Cropper and S. H. Muggleton. Metagol system. https://github.com/metagol/metagol,

2016.

31. R. Devidze, F. Mansouri, L. Haug, Y. Chen, and A. Singla. Understanding the power and
In Proceedings of the Twenty-Ninth

limitations of teaching with imperfect knowledge.
International Joint Conference on Artiﬁcial Intelligence, pages 2647–2654, 2020.

32. Z. Dienes and J. Perner. A theory of implicit and explicit knowledge. Behavioral and

brain sciences, 22(5):735–808, 1999.

33. S. Džeroski, L. De Raedt, and K. Driessens. Relational reinforcement learning. Machine

Learning, 43:7–52, 2001.

34. M. R. E. Should there be a three-strikes rule against pure discovery learning? the case for

guided methods of instruction. The American psychologist, 59(1):14–19, 2004.

35. A. A. Freitas. Comprehensible classiﬁcation models: A position paper. SIGKDD Explor.

Newsl., 15:1–10, 2014.

36. J. H. Gennari, P. Langley, and D. Fisher. Models of incremental concept formation.

Artiﬁcial Intelligence, 40(1):11–61, 1989.

37. D. Gentner and R. Landers. Analogical reminding: A good match is hard to ﬁnd. Pro-

ceedings of the International Conference on Systems, Man and Cybernetics, 1985.

38. M. Gleicher. A framework for considering comprehensibility in modeling. Big data,

4(2):75–88, 2016.

39. H. Goldstine and J. Neumann. Planning and coding of problems for an electronic com-
puting instrument, part ii, volume 2. In John von Neumann Collected Works, Volume
V: Design of Computers, Theory of Automata and Numerical Analysis,, pages 152–214.
Pergamon Press, Oxford, England, 1963.

40. R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi. A survey
of methods for explaining black box models. ACM Computing Surveys, 51(5):1–42, 2018.
41. M. Hind, D. Wei, M. Campbell, N. Codella, A. Dhurandhar, and A. e. a. Mojsilovic. Ted:
Teaching ai to explain its decisions. Proceedings of the 2019 AAAI/ACM Conference on
AI, Ethics, and Society, 2019.

42. J. R. Hobbs. In The Handbook of Pragmatics, chapter Abduction in Natural Language

Understanding. Stevenage: Peter Peregrinus, 2008.

43. K. J. Holyoak and K. Koh. Surface and structural similarity in analogical transfer. Memory

& Cognition 15(4), pages 332–340, 1987.

44. P. N. Johnson-Laird. Mental Models: Towards a Cognitive Science of Language, Inference,

and Consciousness. Harvard University Press, USA, 1986.

Explanatory machine learning for sequential human teaching

37

45. D. Kahneman. Thinking, fast and slow. Macmillan, 2011.
46. R. King, K. Whelan, F. Jones, P. Reiser, C. Bryant, S. Muggleton, D. Kell, and S. Oliver.
Functional genomic hypothesis generation and experimentation by a robot scientist. Na-
ture, 427:247–252, 2004.

47. J. E. Kittel. An experimental study of the eﬀect of external direction during learning on
transfer and retention of principles. Journal of Educational Psychology, 48:391–405, 1957.
48. A. N. Kolmogorov. On tables of random numbers. Sankhya: The Indian Journal of

Statistics, Series A,, 207(25):369–375, 1963.

49. G. Leban, J. Žabkar, and I. Bratko. An experiment in robot discovery with ilp.

In
Proceedings of the 18th International Conference on Inductive Logic Programming, ILP
’08, page 77–90, Berlin, Heidelberg, 2008. Springer-Verlag.

50. M. Lee and A. Thompson. Guided instruction in logo programming and the development of
cognitive monitoring strategies among college students. Journal of Educational Computing
Research, 16:125–144, 1997.

51. E. Lemke, H. Klausmeier, and C. Harris. Relationship of selected cognitive abilities to
concept attainment and information processing. Journal of educational psychology, 58:27–
35, 1967.

52. D. Lin, E. Dechter, K. Ellis, J. Tenenbaum, and S. H. Muggleton. Bias reformulation for
one-shot function induction. In Proceedings of the 23rd European Conference on Artiﬁcial
Intelligence, pages 525–530, 2014.

53. Z. Lipton. The mythos of model interpretability. Communications of the ACM, 61:36–43,

2018.

54. F. Mansouri, Y. Chen, A. Vartanian, X. Zhu, and A. Singla. Preference-based batch and
sequential teaching: Towards a uniﬁed view of models. In Advances in Neural Information
Processing Systems 32: Annual Conference on Neural Information Processing Systems
2019., pages 9195–9205, 2019.

55. J. McAuley and J. Leskovec. Hidden factors and hidden topics: Understanding rating
dimensions with review text. In Proceedings of the 7th ACM Conference on Recommender
Systems, page 165–172, New York, NY, USA, 2013. Association for Computing Machinery.
56. R. Michalski. A theory and methodology of inductive learning. Machine Learning. Sym-

bolic Computation, 1983.

57. D. Michie. Experiments on the mechanization of game-learning. 2—rule-based learning

and the human window. The Computer Journal, 25:105–113, 1982.

58. D. Michie. Machine learning in the next ﬁve years. In Proceedings of the Third European

Working Session on Learning, pages 107–122. Pitman, 1988.

59. D. Michie. Machine intelligence and the human window. Applied Artiﬁcial Intelligence.,

5(1):1–10, 1991.

60. G. A. Miller. The magical number seven, plus or minus two: Some limits on our capacity

for processing information. The Psychological Review, 63:81–97, 1956.

61. T. Miller. Explanation in artiﬁcial intelligence: Insights from the social sciences. Artiﬁcial

Intelligence, 267:1–38, 2019.

62. D. Minh, H. Wang, Y. Li, and T. N. Nguyen. Explainable artiﬁcial intelligence: a com-

prehensive review. Artiﬁcial Intelligence Review, 2021.

63. T. M. Mitchell. Generalization as search. Artiﬁcial Intelligence, 18:203–226, 1982.
64. S. Muggleton, U. Schmid, C. Zeller, A. Tamaddoni-Nezhad, and T. Besold. Ultra-strong
machine learning: comprehensibility of programs learned with ilp. Machine Learning,
107:1119–1140, 2018.

65. S. H. Muggleton. Inductive logic programming. New Gen. Comput., 8:295–318, 1991.
66. S. H. Muggleton and D. Lin. Meta-interpretive learning of higher-order dyadic datalog:
In Proceedings of the Twenty-Third International Joint

Predicate invention revisited.
Conference on Artiﬁcial Intelligence, page 1551–1557, 2013.

67. S. H. Muggleton, D. Lin, N. Pahlavi, and A. Tamaddoni-Nezhad. Meta-interpretive learn-

ing: application to grammatical inference. Machine Learning, 94:25–49, 2014.
68. A. Newell. Uniﬁed Theories of Cognition. Harvard University Press, USA, 1990.
69. L. Novick and K. Holyoak. Mathematical problem solving by analogy. Journal of experi-

mental psychology. Learning, memory, and cognition, 17:398–415, 1991.

70. J. W. Peirce, J. R. Gray, S. Simpson, M. R. MacAskill, R. Höchenberger, H. Sogo, E. Kast-
man, and J. Lindeløv. Psychopy2: experiments in behavior made easy. Behavior Research
Methods, 2019.

71. P. L. Pirolli and J. R. Anderson. The role of learning from examples in the acquisition
of recursive programming skills. Canadian Journal of Psychology/Revue canadienne de
psychologie, 39(2):240, 1985.

38

Lun Ai et al.

72. B. Poulin, R. Eisner, D. Szafron, P. Lu, R. Greiner, D. Wishart, A. Fyshe, B. Pearcy,
C. Macdonell, and J. Anvik. Visual explanation of evidence with additive classiﬁers. In
Proceedings of the National Conference on Artiﬁcial Intelligence, volume 2, 2006.

73. A. N. Raﬀerty, E. Brunskill, T. L. Griﬃths, and P. Shafto. Faster teaching via pomdp

planning. Cognitive science, 40:1290–1332, 2016.

74. S. K. Reed, C. C. Ackinclose, and A. A. Voss. Selecting analogous problems: Similarity

versus inclusiveness. Memory & Cognition 18(1), pages 83–98, 1990.

75. S. K. Reed and C. A. Bolstad. Use of examples and procedures in problem solving. Journal

of Experimental Psychology: Learning, Memory, and Cognition, 17(4):753, 1991.

76. M. T. Ribeiro and C. Singh, S.and Guestrin. "why should i trust you?": Explaining the
predictions of any classiﬁer.
In Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, page 1135–1144, New York, NY,
USA, 2016. Association for Computing Machinery.

77. M. T. Ribeiro, S. Singh, and C. Guestrin. Anchors: High-precision model-agnostic expla-

nations. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2018.

78. C. Rudin. Stop explaining black box machine learning models for high stakes decisions
and use interpretable models instead. Nature Machine Intelligence, 1:206–215, 2019.
79. U. Schmid. Human-Like Machine Intelligence., chapter Interactive Learning with Mutual

Explanations in Relational Domains. Oxford University Press, 2021.

80. U. Schmid and B. Finzel. Mutual explanations for cooperative decision making in medicine.

KI-Künstliche Intelligenz, 34(2):227–233, 2020.

81. U. Schmid and E. Kitzelmann. Inductive rule learning on the knowledge level. Cognitive

Systems Research, 12:237–248, 2011.

82. U. Schmid, C. Zeller, T. Besold, A. Tamaddoni-Nezhad, and S. Muggleton. How does pred-
icate invention aﬀect human comprehensibility? In Proceedings of the 26th International
Conference on Inductive Logic Programming, pages 52–67, 2017.

83. C. A. Seger. Implicit learning. Psychological bulletin, 115(2):163, 1994.
84. E. Y. Shapiro. Algorithmic program debugging. acm distinguished dissertation, 1982.
85. T. Sheridan. Human–robot interaction: Status and challenges. Human Factors, 58(4):525–

532, 2016.

86. H. A. Simon and J. R. Hayes. The understanding process: Problem isomorphs. Cognitive

Psychology 8, pages 165–190, 1976.

87. C. Spearman. The proof and measurement of association between two things. American

Journal of Psychology, 15(1), 1904.

88. M. J. E. Sternberg and S. H. Muggleton. Structure activity relationships (sar) and phar-
macophore discovery using inductive logic programming (ilp). QSAR and Combinatorial
Science, 22(5):527–532, 2003.

89. S. Stumpf, A. Bussone, and D. O’sullivan. Explanations considered harmful? user inter-
actions with machine learning systems. In Proceedings of the ACM SIGCHI Conference
on Human Factors in Computing Systems (CHI), 2016.

90. J. Sweller, P. A. Kirschner, and R. E. Clark. Why minimally guided teaching techniques
do not work: A reply to commentaries. Educational Psychologist, 42(2):115–121, 2007.
91. P. Tamagnini, J. Krause, A. Dasgupta, and E. Bertini. Interpreting black-box classiﬁers
using instance-level visual explanations. In Proceedings of the 2nd Workshop on Human-
In-the-Loop Data Analytics, New York, NY, USA, 2017. Association for Computing Ma-
chinery.

92. K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and Y. Bengio.
Show, attend and tell: neural image caption generation with visual attention.
In Pro-
ceedings of the 32nd International Conference on Machine Learning, pages 2048–2057,
2015.

93. T. Yeo, P. Kamalaruban, A. Singla, A. Merchant, T. Asselborn, L. Faucon, P. Dillen-
bourg, and V. Cevher. Iterative classroom teaching. In Proceedings of the Thirty-Third
AAAI Conference on Artiﬁcial Intelligence and Thirty-First Innovative Applications of
Artiﬁcial Intelligence Conference and Ninth AAAI Symposium on Educational Advances
in Artiﬁcial Intelligence, 2019.

94. V. F. Zambaldi, D. C. Raposo, A. Santoro, V. Bapst, Y. Li, and I. e. a. Babuschkin. Deep

reinforcement learning with relational inductive biases. In ICLR, 2019.

95. C. Zeller and U. Schmid. Automatic generation of analogous problems to help resolv-
ing misconceptions in an intelligent tutor system for written subtraction. In Workshops
Proceedings for the Twenty-fourth International Conference on Case-Based Reasoning,
volume 1815, pages 108–117, 2016.

Explanatory machine learning for sequential human teaching

39

96. X. Zhu. Machine teaching: An inverse problem to machine learning and an approach toward
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,

optimal education.
volume 29, 2015.

