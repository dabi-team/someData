1
2
0
2

b
e
F
8

]
I

A
.
s
c
[

1
v
1
3
2
4
0
.
2
0
1
2
:
v
i
X
r
a

Neurogenetic Programming Framework
for Explainable Reinforcement Learning
Preprint, compiled February 9, 2021

Vadim Liventsev1∗, Aki Härmä2, and Milan Petkovi´c3

1,3Eindhoven University of Technology
1,2,3Philips Research Eindhoven

Abstract

Automatic programming, the task of generating computer programs compliant with a speciﬁcation without a hu-
man developer, is usually tackled either via genetic programming methods based on mutation and recombination
of programs, or via neural language models. We propose a novel method that combines both approaches using a
concept of a virtual neuro-genetic programmer 2, or scrum team. We demonstrate its ability to provide performant
and explainable solutions for various OpenAI Gym tasks, as well as inject expert knowledge into the otherwise
data-driven search for solutions. Source code is available at https://github.com/vadim0x60/cibi

Keywords Reinforcement Learning · Program Synthesis · Genetic Programming

1 Introduction

Automatic programming is a discipline that studies application
of mathematical models to infer programs from data and use
these generated programs to solve various tasks. For example,
given a dataset of clinical decision-making generate a program
that describes how doctors make treatment decisions based on
clinical history available to them.

One of the primary motivations behind automatic programming
is enabling an exchange of knowledge between human experts
and machine learning models: black box models have achieved
impressive results in diverse decision support settings [2], some-
times competing with human experts in the ﬁeld. The advantage
of automatic programming systems is that they can also cooper-
ate with the experts:

• Code generation models can be trained to produce pro-
grams similar to what experts wrote, incorporating ex-
pert knowledge into the model

• Code generation models can generate new programs
by applying modiﬁcations to expert-written programs,
using them as the basis

• Experts can examine the generated programs, under-
stand the algorithm suggested by the system and learn
from it

The beneﬁts of regulating the program induction task, especially
in limited data conditions have been demonstrated earlier [3].
The programming language used in program induction is a reg-
ulating constraint that may be expected to guide the learning
algorithm to favor solutions with operations that are most natural

for the given language [4]. For example, a strictly sequential lan-
guage such as C seems most natural in cases where the expected
solution is a sequential protocol. A parallel language, such as
various cellular automata models or deep neural networks, favor
concurrent solutions.

We may also approach this from the angle of algorithmic in-
formation theory. The Kolmogorov complexity of the solution
corresponds to the length of the program and the number of free
parameters [5]. In the case of a black box deep model, the Kol-
mogorov complexity is an architectural constant of the parallel
network model while in program induction it is optimized by the
learning algorithm based on the available training data. When
data is limited, the PI training may use Occam’s razor 3 to ﬁnd a
minimal-complexity solution matching the training data, while
a standard NN solution always has the same complexity.

In fact, the use of program synthesis for solving a machine
learning problem can be seen as generalized hyperparameter
optimization; the program optimization may, in principle, con-
verge to a program that implements a particular deep neural
network optimized for the problem. In genetic programming
[8, 9] new programs are generated by mutating and mixing a
population of programs. A more recent approach, largely draw-
ing on the earlier success of deep neural language models (see
CodeBERT [10] inspired by BERT [11]), have been to train
black box neural models that generate executable programs as
text [12, 13, 14]. Neural program synthesis and genetic pro-
gramming both have unique advantages [15]. In this paper we
propose a novel hybrid of the two families of methods. We
call the method Instant Scrum in reference to a popular Agile
software team work model [16]. We show that Instant Scrum,

∗v.liventsev@tue.nl
2not to be confused with neuroevolution [1]: using evolutionary methods as an alternative to gradient descent for neural network training
3"We consider it a good principle to explain the phenomena by the simplest hypothesis possible" [6, book 3, chapter 2], misattributed [7] to

William of Ockham

*correspondence: v.liventsev@tue.nl

 
 
 
 
 
 
Preprint – Neurogenetic Programming Framework

2

IS, can solve several reinforcement-guided program synthesis
tasks in standard OpenAI gym benchmarks tasks (section 4.3).

After the introduction to the relevant background in the next
section, we will give a detailed description of the proposed IS
methodology. Next, we describe the experimental setup and the
OpenAI gym test tasks, and the results of the experiments in
several variations of the core IS method. Finally, we discuss
the results and propose future research directions and potential
applications for hybrid neurogenetic programming.

An agent action a ∈ A at the state s causes the environment
state to change probabilistically, and the destination state fol-
lows the distribution ps(·|s, a). At state s, the probability of
making observation o is po(o|s) and the probability of obtaining
reward r is pr(r|s, a). The process continues until a ps yields
a terminal state s ∈ St. This distinction is what sets episodic
POMDP popularized by OpenAI gym [31] apart from the more
traditional approach [32, 33] where the process is inﬁnite.

A program is a sequence of tokens

2 Background

Speciﬁcation is central to the ﬁeld of automatic programming:
if no requirements to generated programs are speciﬁed, the task
becomes random program generation - not useful in most real-
world settings beyond testing compilers [17, 18]. The ﬁeld
of automatic programming can be subdivided by what type of
speciﬁcation is used.

One way to specify expected behavior of a program is a dataset
of input-output pairs [13, 12]. A lot of work in this area has
been inspired [19, 20, 21] by the task of learning a formula that
ﬁts a series of cells in spreadsheet applications [22].

Alternatively, the program’s pre- and postcondition predicates
can be described in a formal language: proof-theoretic synthesis
[23] is the task of generating a program such that if it’s input
conforms to the precondition, its output has to conform to the
postcondition.

In the task of semantic parsing [24, 25, 26], speciﬁcation is a
textual description of an algorithm that has to be translated from
natural language into machine language.

Finally, speciﬁcation can manifest in the form of a reinforcement
learning environment [27] - a program deﬁnes behavior of an
agent that interacts with its environments and receives positive
and negative rewards from it. The goal is to ﬁnd a program that
maximises rewards. In this work, we choose RL speciﬁcation,
because it generalizes other methods: input-output pairs can
be seen as an environment that negatively reinforces a metric
of diﬀerence between expected and observed program output,
while in proof-theoretic synthesis the reward is determined by
the postcondition. Many real-world problems, such as robot con-
trol [28] and clinical decision making [29, 30] are formulated as
reinforcement learning as well.

2.1 Programmatically Interpretable Reinforcement Learning

More concretely, we model the task as Episodic Partially Ob-
servable Markov Decision Process:

M = (Snt, St, A, O, po(o|s, a), ps(snext|sprev), pr(r|s, a), pinit(s))

c = (c(1), c(2), . . . )

(2)

that deﬁnes behavior of an agent. Depending on the program-
ming language and implementation choices the tokens can be
characters or higher-level tokens, i.e. keywords. We denote the
language’s alphabet, i.e. the set of all possible tokens as L.

An interpreter is a tuple (cid:104)α, µ(cid:105) where µ(ck, mk, ok) is the memo-
rization function that deﬁnes how the agent’s memory updates
upon making an observation and α(c, mk) is the action function
deﬁnes which action the agent at a certain memory state takes
in the POMDP. Memory is intialized at state minit

The agent’s goal is maximizing total reward collected in the
environment, calculated as follows:

Algorithm 1 Evaluating total reward for a program

1: function Eval(c)
Rtot ← 0
2:
m ← minit
3:
s ∼ pinit(s)
4:
while s ∈ Snt do
5:
(cid:46) Observe
6:
o ∼ po(o|s)
7:
m ← µ(c, m, o)
8:
(cid:46) Act
9:
a ← α(c, m)
10:
r ∼ pr(s, a)
11:
(cid:46) Get rewarded
12:
Rtot ← Rtot + r
13:
(cid:46) Next state
14:
snext ∼ ps(snext|s, a)
15:
s ← snext
16:
end while
17:
return Rtot
18:
19: end function

Since the algorithm for computing this function involves re-
peatedly sampling values from distributions, function Eval(c)
is a mapping from the set of programs to the set of real-valued
random variables.

(1)

Here, Snt is the set of non-terminal (environment) states and
St is the set of terminal states. A is the set of actions that the
learning agent can perform, and O is the set of observations
about the current state that the agent can make.

A reinforcement learning episode starts in a state s ∈ Snt sam-
pled from pinit, the initial distribution over environment states.

Programmatically Interpretable Reinforcement Learning [27]
is the task of maximizing expected total reward with respect
to c, i.e. ﬁnding a program c that is best for a given POMDP
environment:

E(Eval(c)) −→ max

c

(3)

Preprint – Neurogenetic Programming Framework

3

3 Methodology

How does one manage a composition of code generators in
such a way that the composition yields better programs than
individual contributors are capable of? This question is studied
extensively in software project management literature [34]. And
while, admittedly, project management literature is concerned
with human developers and, admittedly, there exist considerable
diﬀerences between human developers and mathematical mod-
els of code generation [35], we mitigate these diﬀerences with
several simplifying assumptions.

3.1 Modeling the codebase

Following from traditional genetic programming, we deﬁne a
population of programs. The codebase is a tuple of 2-tuples,
representing a program C(i)
c = c and the total rewaard it collected
C(i)
R

= Rtot ∼ Eval(c) (see section 3.3):

C = (cid:104)(cid:104)C(1)

c , C(1)

R (cid:105), (cid:104)C(2)

c , C(2)

R (cid:105) . . . (cid:104)C(|C|)

c

, C(|C|)
R

(cid:105)(cid:105)

(4)

However, unlike in traditional genetic programming, the initial
population can (optionally) be empty.

3.2 Modeling a software developer

A software developer can:

1. Check out programs from the codebase C

2. Output new a program c

3. Receive feedback on their program’s quality q

4. Learn from the feedback by moﬁdying its strategy

Thus, a developer is a 2-tuple of a program distribution
pdev(c|θ, C) and a parameter update procedure Update(θ, c, q)
Distribution pdev(c|θ, C) is deﬁned over programs and is
parametrized with learnable parameters θ as well as codebase C.
Having codebase as parameter enables the developer to generate
new programs as a modiﬁcation and/or combination of existing
programs, i.e. to apply genetic programming.

Learnable parameters θ encode the developer’s current method-
ology of programming that can be modiﬁed upon receipt of
positive or negative feedback using the developer’s update pro-
cedure.

The team of developers is a tuple of 2-tuples:

T = (cid:104)(cid:104)T (1)

p , T (1)

upd(cid:105), (cid:104)T (2)

p , T (2)

upd(cid:105) . . . (cid:104)T (|T |)

p

, T (|T |)

upd (cid:105)(cid:105)

(5)

3.3 Modeling program quality

We deﬁne two empirical metrics of overall program ﬁtness. The
ﬁrst is empirical total reward:

I[C(i)

c = c]C(i)
R

R(c|C) =

I[C(i)

c = c]

(6)

|C|(cid:80)
i=1
|C|(cid:80)
i=1

If a program has been tested in the environment (Eval() function)
several times, there will be several copies of it in the codebase
with diﬀerent quality samples. Averaging over them yields an un-
biased estimate of the expectation from equation 3, E(Eval(c)),
that we set out to maximize.

The second is empirical program quality, deﬁned as

Q(c|C) =

|C|(cid:80)
i=1

I[C(i)

c = c]eC(i)

R

|C|(cid:80)
i=1

I[C(i)

c = c]

(7)

Empirical program quality is an unbiased estimate of E(eEval(c))
The idea behind exponentiating the total reward is to encour-
age exploration [36]. Programs that on average perform poorly,
but sometimes, stochastically, collect high rewards, will have
a higher Q(c|C) than R(c|C). We consider these programs to
be high-quality additions to the codebase because they contain
the knowledge necessary for solving environment M, even if
on average they don’t solve it. We hypothesize that applying
genetic operators (section 3.5) to programs with high Q(c|C)
can yield programs with high R(c|C). For this reason we train
developers to maximize Q, but when the training is complete,
we pick programs from the codebase with the highest R as "best
programs".

Q(c|C) has an additional technical advantage over R(c|C): invari-
ant Q(c|C) ≥ 0 holds for all c. This lets one sample programs
from the codebase with probabilities proportional to their quality,
see eq. 19.

3.4 Populating the codebase

Just like instant run-oﬀ voting achieves similar results to exhaus-
tive ballot runoﬀ voting, but does it much faster by replacing a
series of ballots cast in a series of elections with a ballot cast
once that goes on to participate in a series of virtual elections
[37], our Instant Scrum algorithm does the same to Scrum [16]:
it simulates the iterative software development process recom-
mended by Scrum methodology without humans in the loop
making it possible to run many sprints per second:

N ← 0
while N < Nmax do

(cid:46) For each developer in the team
for i = 1, 2, . . . , |T | do

Algorithm 2 Instant Scrum with a team of developers
1: procedure InstantScrum(T, C, Nmax)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: end procedure

(cid:46) Sample a program from the developer
cnew ∼ pi(c|θi, C)
Rtot ← Eval(cnew)
(cid:46) Save the code and test result to the codebase
C ← C ∪ {(cid:104)cnew, Rtot(cid:105)}
Updatei(θi, c, q)
N ← N + 1

(cid:46) Train the developer
(cid:46) Increment sprint counter

end for
end while

(cid:46) Test the program

Preprint – Neurogenetic Programming Framework

4

Parent 1
Parent 2
Shuﬄe mutation
Uniform mutation
1-point crossover
2-point crossover
Uniform crossover
Messy crossover
Pruning

ae>>>>>34+
a[e>-a-]b[e>>-b-]

>>4+>3>e>a
ae@>!>>35+
ae>>>>-]b[e>>-b-]
ae>>-a-34+
aee>->>3b+
ae>>>>e>-a-]b[e>>-b-]
e>>>>>4+

(b) All operators applied a pair of BF++ [39] programs

(a) 1-point and 2-point crossover [38]

Figure 1: Genetic operators by example

To combine genetic programming and neural program synthe-
sis we introduce 3 types of developers: genetic and neural and
dummy, create a team that contains developers of all types and
run Instant Scrum.

3. Using the operator to modify the parents and yield a

new (child) program

A genetic operator is a probability distribution pop over child
programs given 2 parent programs.

3.5 Genetic developers

A genetic developer writes programs by

pop(cchild|c1, c2)

(8)

1. Selecting one of the 7 available stochastic genetic op-

erators (described below)

2. Selecting two programs from the codebase C (parents

c1 and c2)

Operators whose pop is invariant to c2 and depends on c1 only
are called mutation operators: they generate a new program
by mutating one program c1. The rest are called combination
operators as they combine 2 existing programs to generate a new
one.

3.5.1 Mutation operators
The simplest method for randomly modifying a program is shuﬄe mutation: randomly re-order the tokens of c1. Let A be the set
of all possible permutations of size |c1|. |A| = |c1|!. Then

pshuﬄe(cchild|c1, c2) =

(cid:80)
α∈A

I[α(cparent) = cchild]

|c1|!

(9)

Another approach is uniform mutation where a loaded coin is tossed for every token in c1. With probability pind it is replaced with
a random token from the alphabet L of the programming language, with probability 1 − pind it stays the same. The evolution of a
single token under shuﬄe mutation is deﬁned by distribution

Hence over full programs the operator is deﬁned as

p(cnew|cold) = pind
|L|

+ (1 − pind)I[cnew = cold]

punimut(cchild|c1, c2) = I[|cchild| = |c1|]

(cid:32)

|c1|(cid:89)

i=0

pind
|L|

+ (1 − pind)I[c(i)

child

(cid:33)
= c(i)
1 ]

(10)

(11)

3.5.2 Combination operators

The combination operators we propose are all variants of crossover - a classic genetic programming technique rooted in the way a
pair of DNA molecules exchanges genes during mitosis and meiosis, displayed on ﬁgure 1a.

In DNA [38], as well as in most genetic programming literature [8, 9] the crossover operator combines 2 parent sequences to
produce 2 children. In this section, in order to reduce complexity, we deﬁne the distributions as if only the ﬁrst child program is

Preprint – Neurogenetic Programming Framework

5

saved and the second one is forgotten. Since program pair (cid:104)c2, c1(cid:105) is equally likely to be selected for combination as (cid:104)c1, c2(cid:105) (see
eq. 19) this modiﬁcation does not aﬀect the resulting genetic developer distribution.

In one-point crossover a random cut position k is selected and the trailing sections of 2 parent programs beginning with the cut
point are swapped with each other. If the parent programs have diﬀerent lengths, the cut point has to ﬁt within both programs:

|c1, c2| = min{|c1|, |c2|}

2 ≤ k ≤ |c1, c2|

Hence the probability of cchild being born out of one-point crossover is

p1ptcx(cchild|c1, c2) =

I[|cchild| = |c2|]
|c1, c2| − 1

|c1,c2|(cid:88)

k−1(cid:89)

k=2

i=1

I[c(i)

child

= c(i)
1 ]

|c2|(cid:89)

i=k

I[c(i)

child

= c(i)
2 ]

(12)

(13)

(14)

Two-point crossover is similar, but instead of swapping the trailing ends of programs, a section in the middle of the programs is
chosen, determined by randomly selected cut-oﬀ indices k1 and k2 and swapped:

p2ptcx(cchild|c1, c2) =

2I[|cchild| = |c1|]
(|c1, c2| − 2)(|c1, c2| − 1)

|c1,c2|−1(cid:88)

|c1,c2|(cid:88)

k1−1(cid:89)

k1=2

k2=k1+1

i=1

I[c(i)

child

= c(i)
1 ]

k2−1(cid:89)

i=k1

I[c(i)

child

= c(i)
2 ]

|c1|(cid:89)

i=k2

I[c(i)

child

= c(i)
1 ]

(15)

Uniform crossover mirrors uniform mutation in that a loaded coin is tossed for each token in c1. With probability pind the token is
replaced, but the replacement is not drawn randomly from the alphabet. Instead, the replacement comes from c2:

punicx(cchild|c1, c2) =

|c1|(cid:89)

i=0

I[|cchild| = |c1|]

(cid:16)

pindI[c(i)

child

= c(i)

2 ] + (1 − pind)I[c(i)

child

(cid:17)
= c(i)
1 ]

(16)

Finally, messy crossover is a version of one-point crossover without the assumption that both parent programs have to be cut at the
same index k. In messy crossover, one parent is cut at index k1, another is cut at index k2 and the head of one is attached to the tail
of the other:

pmessy(cchild|c1, c2) =

1
(|c1, c2| − 1)2

|c1,c2|(cid:88)

|c1,c2|(cid:88)

k1=2

k2=2

I[|cchild| = k1 + |c2| − k2]

k1−1(cid:89)

i=1

I[c(i)

child

= c(i)
1 ]

|c2|−k2(cid:89)

i=1

I[c(k1+i)
child

= c(k2+i)
2

]

(17)

3.5.3 Pruning operator

3.5.4 Operator and parent selection

After initial experiments we found that generated programs often
contain sections of unreachable code or code that makes changes
to the execution state and fully reverses them. To address this,
we introduced an additional operator for removing dead code
(pruning): when Instant Scrum encounters a successful program,
pruning helps separate sections of this program that led to its
success from sections that appeared in a highly-rated program
by accident.

Implementation of the pruning operator depends on the program-
ming language at hand, here we deﬁne it as a pruning function
cpruned = Prune(c1) that outputs a program functionally equiva-
lent to c1 (memory functions (α, µ) of cpruned are equal to that of
c1) and |cpruned| ≤ c1 and a degenerate probability distribution:

Let Pgenetic be a tuple of all available genetic operators, in order
of introduction, i.e. P(1)

= pshuﬄe and P(4)

= p2ptcx

genetic

genetic

Genetic developer’s program distribution is a mixture distribu-
tion, combining diﬀerent operators that can be applied, weighted
by learnable parameters, and diﬀerent programs that can be sam-
pled from the codebase, weighted by empirical quality (eq. 7).

pgenetic(c|θ, C) =

C(cid:88)

C(cid:88)

Q(c1|C)Q(c2|C)

|Pgenetic|
(cid:88)

c1

c2

(

C(cid:80)
c

Q(c|C))2

i=0

θiP(i)

genetic(c|c1, c2)

This is a true probability distribution if and only if

(19)

|Pgenetic|
(cid:80)
i=0

θi = 1

pprune(cchild|c1, c2) =

(cid:40)

cchild = Prune(c1)

1
0 otherwise

(18)

One challenge that remains to be solved to fully deﬁne the ge-
netic developer (folowing section 3.2) is to deﬁne a learning

3.5.5 Training the genetic developer

Preprint – Neurogenetic Programming Framework

6

from feedback strategy Updategenetic. To do this, we notice that
equation 19 contains a multi-armed bandit [40] hiding in plain
sight. Indeed, once the genetic developer samples c1 and c2
from the codebase, it has to pick one of 7 available options
(pull one of 7 levers) to then receive a reward Eval(cchild). This
subproblem can be represented with a POMDP of its own and
solved using one of the standard bandit algorithms [41].

Following Occam’s razor, we picked the simplest method,
epsilon-greedy optimization: we calculate the value of each
operator as mean total reward of programs generated with this
operator:

V (i) =

1
|C(P(i)
genetic)|

|C(P(i)
genetic)|
(cid:88)

k=1

C(P(i)

genetic)(k)

R

(20)

where C(P(i)
operator P(i)

genetic) is the subset of the codebase produced via
genetic.

The Updategenetic procedure recalculates values V and sets oper-
ator probabilities to

θi =

(cid:15)
|P(i)
genetic|

+ I[i = arg max

V (i)](1 − (cid:15))

(21)

i

where (cid:15) is a hyperparameter responsible for regulating the
exploration-exploitation tradeoﬀ [42]

In future work, however, other bandit optimization algorithms
can be used in its place 4.

3.5.6 Hyperparameters

The genetic developer, as described above, has 2 hyperparame-
ters:

1. pind deﬁnes severity of mutation in punimut and punicx

2. (cid:15) deﬁnes learnability of genetic operator distribution

Note that the team mechanism aﬀorded by Instant Scrum can be
used not only to combine genetic and neural program synthesis,
but also to combine several genetic developers with diﬀerent
hyperparameters.

3.6 Neural developers

The neural developer, also known as the senior developer be-
cause of their unique ability to write original programs, is an
LSTM [43] network followed by a linear layer that generates a
sequence of vectors h1, h2, h3, . . . where hi ∈ R|L|+1∀i and j-th
element of vector hi, h( j)
, represents the probability of i-th token
i
of the program being j-th token in the alphabet, p(c(i) = L( j)).
The last element of the vector represents a special end of pro-
gram symbol. This vector depends deterministically on the full
set of neural network parameters (LSTM and linear layer) θ and
can be represented as a function hi(θ). Then

pneural(c|θ, C) = hL+1
(|c|+1)

|c|(cid:89)

|L|(cid:88)

i=1

j=1

I[c(i) = L( j)]hi(θ)

(22)

For the Updateneural procedure we use the algorithm proposed in
[12]. The subproblem of generating a program c is considered
as a reinforcement learning episode of it’s own, where tokens
are actions and token number |c| + 1 (end of program token) is
assigned reward q = eR; R ∼ Eval(c). In this subenvironment
hi(θ) is the policy network [44, chapter 13] trained using REIN-
FORCE algorithm with Priority Queue Training. This algorithm
involves a priority queue of best known programs: we imple-
ment it as programs from C with highest Q(c|C) which means
that the neural developer can train on programs written by other
developers.
hi(θ) can also represent several LSTM layers stacked or a diﬀer-
ent type of recurrent neural network, i.e. GRU [45]. Hyperpa-
rameters of this neural network, such as hidden state size and/or
number of stacked layers are hyperparameters of the neural
developer.

3.7 Dummy developer

The last developer we introduce is the simplest one:

pdummy(cchild|c1, c2) = Q(cchild|C)
Q(c|C)

C(cid:80)
c

(23)

Dummy developer does not generate novel programs. Instead, it
uses the same quality-weighted program sampling as in equation
19 to decide which existing program to copy. Their utility may
not be obvious at ﬁrst, but note (section 3.3) that when the same
program is added to the codebase several times, it’s total reward
and quality estimates are averaged and grow more accurate.

Dummy developer is a smart compromise between speed at
which Instant Scrum (algorithm 2) is searching the program
space and the quality of it’s working map of the program space,
focusing on its most "interesting" (high Q(c|C)) parts. Without
dummy developer, all empirical total rewards E[Eval(c)] would
be low quality estimates of true ﬁtness of the program and one
spurious success of an otherwise bad program could steer the
search in the wrong direction. On the other hand, we could test
each program many times before adding it to the codebase, but
that would slow down the search prohibitively.

4 Experimental setup

4.1 Teams

In the table below, we introduce 5 teams. Neural developers
are denoted as lstm(hidden state dimensionality), several num-
bers mean a stacked LSTM. Genetic developers are denoted as
gen(pind, (cid:15)), see secion 3.5.6. Tsmall and Tlarge are recommended
conﬁgurations while Tgenetic, Tneural are ablation studies to prove
that combination of neural and genetic methods is useful.

4Our open-source software implementation allows for drop-in replacement of bandit algorithms

Preprint – Neurogenetic Programming Framework

7

Tsmall

(cid:88)

(cid:88)

(cid:88)

Tlarge
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

Developer
lstm(10)
lstm(50)
lstm(256)
lstm(10, 10)
lstm(50, 50)
lstm(256, 256)
gen(0.2, 0.2)
3 , 0.2)
6 , 0.2)
12 , 0.2)
dummy

gen( 1
gen( 1
gen( 1

4.2 Language

Tgenetic

Tneural

(cid:88)

(cid:88)

(cid:88)

and it ﬁnds good programs by exhaustion as opposed to learning
- if that was the case, early stopping would ﬁre immediately. Taxi
environment is treated diﬀerently because programs that cannot
pick up and drop oﬀ at least one passenger are always rewarded
with -200 and at ﬁrst it takes many iterations to synthesize at
least one program that can. In addition to these stopping rules, a
hard timelimit was set.

After the process is stopped, we pick 100 programs with the
highest R(c|C) and make sure each of them has been tested at
least 100 times, otherwise we run Eval(c) and add result to the
codebase until 100 samples is reached.

4.6

Implementation

Instant Scrum can be used to generate programs in any program-
ming language provided:

We implemented the framework with Python and Tensorﬂow
as well as DEAP [51] for genetic operators. It is available at
https://github.com/vadim0x60/cibi

1. An interpreter (cid:104)α, µ(cid:105), see section 2.1
2. A known ﬁnite alphabet L
3. A pruning function Prune(c)

The complexity of the chosen language is important since in
complex languages random perturbations of program source
code often produce grammatically invalid programs. This issue
has been addressed with structural models [46, 14] [9, chapter
4], however, we sidestep the issue entirely by using BF++ [39] -
a simple language developed for programmatically interpretable
reinforcement learning where most random combinations of
characters are valid programs. Each BF++ command is repre-
sented with a single character, thus the only way to tokenize it
is to let tokens c(1), c(2), c(3), . . . be single characters.

4.3 Tasks

Following from [39] we synthesize programs for CartPole-
v1 [47], MountainCarContinuous-v0 [48], Taxi-v3 [49]
BipedalWalker-v2 OpenAI Gym [31] environments, see ﬁg-
ure 2.

4.4

Initial populations

Where possible, we run all experiments twice - a control ex-
periment with empty intial codebase, and an experiment where
codebase is pre-populated with human-written programs from
[39]. Exceptions to this rule are

• Teams Tgenetic and Tpure that only have code modiﬁca-
tion (not generation) capability and thus require initial-
ization

• BipedalWalker-v2 environment, because no programs

for this environment were provided in [39]

4.5 Stopping and scoring

For Taxi we set an Nmax to 100000|T | sprints, meaning every de-
veloper in the team trains for 100000 iterations. For other tasks
we used Exponential Variance Elimination [50] early stopping
algorithm to stop the process when the positive trend in Eval(c)
is not present for 10000 sprints. This approach rules out the hy-
pothesis that Instant Scrum is equivalent to enumerative search

5 Results

See table 1 for a summary of best programs generated. The
metric used, average R over 100 evaluations is the same metric
that’s used in the OpenAI gym leaderboard, so we include the
threshold required to join the leaderboard for context. Initial
programs refers to the best program in the codebase before In-
stant Scrum starts when it is prepopulated with programs from
[39].

The main hypothesis of this paper is conﬁrmed: neurogenetic
approach is superior to neural program induction or genetic
programming separately. Besides, one unintuitive result of our
experiments is that initialization of the codebase with previously
available programs can be harmful, see Tlarge. Overall, best
results were acheived without inspiration from human experts,
however, it is very valuable for lightweight teams with few small
(in terms of |θ|) developers.

Additionally, we can explore Tlarge to see which of its many
developers actually produced the best programs:

Task

Init

R(c|C)

Developer

157.35
CartPole-v1
CartPole-v1 (cid:88)
57.47
91.65
MountainCar
MountainCar (cid:88)
91.42
Taxi-v3
-32.12
Taxi-v3 (cid:88) -150.44

BipedalWalker-v2

8.13

lstm(256)
lstm(256,256)
lstm(10,10) and pruning
lstm(50)
lstm(50)
human
lstm(256,256)

The same is true for Tsmall:
Init

Task

R(c|C)

Developer

60.93
CartPole-v1
CartPole-v1 (cid:88)
143.9
92.53
MountainCar
MountainCar (cid:88)
88.2
Taxi-v3
-148.23
Taxi-v3 (cid:88) -150.44
-0.15

BipedalWalker-v2

lstm(50,50)
lstm(50,50)
lstm(50,50)
lstm(50,50)
gen(0.2, 0.2), punicx
human
lstm(50,50)

However, comparing results for Tsmall versus Tneural proves that
genetic developers have been intstrumental to the quality of

Preprint – Neurogenetic Programming Framework

8

(a) CartPole-v1

(b) MountainCarContinuous-v0

(c) Taxi-v3

(d) BipedalWalker-v2

Figure 2: Selected tasks, visualized

Environment

CartPole-v1

MountainCarContinuous-v0

Taxi-v3

BipedalWalker-v2

Initial programs

Tsmall
Tlarge
Tgenetic
Tneural
Leaderboard threshold

20.48

143.91
57.47
59.12
96.64

92.53
91.65
-
88.41

60.93
157.35
-
71.38

195

195

90

-6.55

88.20
91.42
0
91.38

90

-150.44

-150.44
-150.44
-47.54
-150.44

-148.23
-32.12
-
-198.9

0

0

Table 1: Averaged 100-episode reward acheived by the best program in each category

-0.16
8.13
-
6.17

300

these neural networks - this is to be expected with Priority Queue
Training (see sec. 3.6).

6 Discussion

oper currently absent from our experiments is a neural mutation
- a neural network that modiﬁes existing programs and can be
trained to modify them in a way that improves their performance.
Another important direction is applying the framework to more
specialized tasks like robotics or healthcare decision support.

We have introduced a neurogenetic programming framework,
demonstrated its eﬃcacy and advantages over simpler program
induction methods.

We believe that this framework can become a basis for many
future methods - new methods of program synthesis can be built
into the Instant Scrum framework as developers and combined
with existing ones as necessary. In particular, one type of devel-

Acknowledgements

This work was funded by the European Union’s Horizon 2020 research
and innovation programme under grant agreement 812882. This work is
part of "Personal Health Interfaces Leveraging HUman-MAchine Natu-
ral interactionS" (PhilHumans) project: https://www.philhumans.
eu

References

[1] Dario Floreano, Peter Dürr, and Claudio Mattiussi. Neu-
roevolution: from architectures to learning. Evolutionary
intelligence, 1(1):47–62, 2008.

[2] Yuxi Li. Deep reinforcement learning: An overview. arXiv

preprint arXiv:1701.07274, 2017.

[3] Jacob Devlin, Rudy Bunel, Rishabh Singh, Matthew J.
Hausknecht, and Pushmeet Kohli. Neural program meta-
induction. CoRR, abs/1710.04157, 2017. URL http:
//arxiv.org/abs/1710.04157.

[4] Max Garzon. Models of Massive Parallelism: Analysis of
Cellular Automata and Neural Networks. Springer Pub-
lishing Company, Incorporated, 1st edition, 2012. ISBN
3642779077.

[6] Claudius Ptolemaeus, Nicolaus Copernicus, Johannes Ke-
pler, and Charles Glenn tr Wallis. The almagest. Ency-
clopaedia Britannica, 1952.

[7] William M Thorburn. The myth of occam’s razor. Mind,

27(107):345–353, 1918.

[8] Wolfgang Banzhaf, Peter Nordin, Robert E Keller, and
Frank D Francone. Genetic programming: an introduction,
volume 1. Morgan Kaufmann Publishers San Francisco,
1998.

[9] John R Koza. Genetic programming: on the programming
of computers by means of natural selection, volume 1. MIT
press, 1992.

[10] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting
Liu, Daxin Jiang, and Ming Zhou. Codebert: A pre-trained
model for programming and natural languages, 2020.

[5] Andrei N Kolmogorov. Three approaches to the quanti-
tative deﬁnition oﬁnformation’. Problems of information
transmission, 1(1):1–7, 1965.

[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. BERT: pre-training of deep bidirec-
tional transformers for language understanding. CoRR,

Preprint – Neurogenetic Programming Framework

abs/1810.04805, 2018. URL http://arxiv.org/abs/
1810.04805.

[12] Daniel A. Abolaﬁa, Mohammad Norouzi, Jonathan Shen,
Rui Zhao, and Quoc V. Le. Neural Program Synthesis with
Priority Queue Training. arXiv preprint arXiv:1801.03526,
2018. URL http://arxiv.org/abs/1801.03526.

[13] Matej Balog, Alexander L. Gaunt, Marc Brockschmidt,
Sebastian Nowozin, and Daniel Tarlow. Deepcoder: Learn-
ing to write programs. CoRR, abs/1611.01989, 2016. URL
http://arxiv.org/abs/1611.01989.

[14] Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. Struc-
tural language models of code. In International Conference
on Machine Learning, pages 245–256. PMLR, 2020.

[15] Algorithm synthesis:

Deep learning and genetic
http://iao.hfuu.edu.cn/blogs/

programming.
33-algorithm-synthesis-deep-learning-and-genetic-programming.
(Accessed on 02/03/2021).

[16] Ken Schwaber and Mike Beedle. Agile software develop-
ment with Scrum, volume 1. Prentice Hall Upper Saddle
River, 2002.

[17] Vsevolod Livinskii, Dmitry Babokin, and John Regehr.
Random testing for c and c++ compilers with yarpgen.
Proc. ACM Program. Lang., 4(OOPSLA), November 2020.
doi: 10.1145/3428264. URL https://doi.org/10.
1145/3428264.

[18] Gergö Barany. Liveness-driven random program gen-
eration. CoRR, abs/1709.04421, 2017. URL http:
//arxiv.org/abs/1709.04421.

9

[25] Pengcheng Yin and Graham Neubig. A syntactic neural
model for general-purpose code generation. In Proceedings
of the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
440–450, 2017.

[26] Maxim Rabinovich, Mitchell Stern, and Dan Klein. Ab-
stract syntax networks for code generation and semantic
parsing. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Volume 1:
Long Papers), pages 1139–1149, 2017.

[27] Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh,
Pushmeet Kohli, and Swarat Chaudhuri. Programmatically
interpretable reinforcement learning. In Jennifer Dy and
Andreas Krause, editors, Proceedings of the 35th Inter-
national Conference on Machine Learning, volume 80 of
Proceedings of Machine Learning Research, pages 5045–
5054, Stockholmsmässan, Stockholm Sweden, 10–15 Jul
2018. PMLR. URL http://proceedings.mlr.press/
v80/verma18a.html.

[28] Jens Kober, J. Andrew Bagnell, and Jan Peters. Re-
inforcement learning in robotics: A survey. The In-
ternational Journal of Robotics Research, 32(11):1238–
1274, 2013. doi: 10.1177/0278364913495721. URL
https://doi.org/10.1177/0278364913495721.

[29] Vadim Liventsev.

Heartpole:

for

reinforcement

task
https://github.com/vadim0x60/heartpole/
blob/master/HeartPole_abstract.pdf.
on 01/31/2021).

learning

(Accessed

A transparent
healthcare.
in

[19] Oleksandr Polozov and Sumit Gulwani. Flashmeta: A
framework for inductive program synthesis. In Proceed-
ings of the 2015 ACM SIGPLAN International Conference
on Object-Oriented Programming, Systems, Languages,
and Applications, pages 107–126, 2015.

[30] Amirhossein Kiani, Tianli Ding, and Peter Henderson.
Gymic: An openai gym environment for simulating
sepsis treatment for icu patients. https://github.com/
akiani/rlsepsis234/blob/master/writeup.pdf.
(Accessed on 01/31/2021).

[20] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju,
Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet
Kohli. Robustﬁll: Neural program learning under noisy i/o.
In International conference on machine learning, pages
990–998. PMLR, 2017.

[21] Vu Le and Sumit Gulwani. Flashextract: A framework for
data extraction by examples. In Proceedings of the 35th
ACM SIGPLAN Conference on Programming Language
Design and Implementation, pages 542–553, 2014.

[22] Sumit Gulwani. Automating string processing in spread-
sheets using input-output examples. ACM Sigplan Notices,
46(1):317–330, 2011.

[23] Saurabh Srivastava, Sumit Gulwani, and Jeﬀrey S. Foster.
From program veriﬁcation to program synthesis. SIG-
PLAN Not., 45(1):313–326, January 2010. ISSN 0362-
1340. doi: 10.1145/1707801.1706337. URL https:
//doi.org/10.1145/1707801.1706337.

[24] Wang Ling, Phil Blunsom, Edward Grefenstette,
Karl Moritz Hermann, Tomáš Koˇcisk`y, Fumin Wang, and
Andrew Senior. Latent predictor networks for code gen-
eration. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Volume 1:
Long Papers), pages 599–609, 2016.

[31] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas
Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. Openai gym, 2016.

[32] K J Åström. Optimal control of Markov processes with
incomplete state information. Journal of Mathematical
Analysis and Applications, 10(1):174–205, 1965. ISSN
0022-247X. doi: https://doi.org/10.1016/0022-247X(65)
90154-X. URL http://www.sciencedirect.com/
science/article/pii/0022247X6590154X.

[33] Jr Kramer, J David R. Partially Observable Markov Pro-

cesses., 1964.

[34] Frederick P Brooks Jr. The mythical man-month: essays
on software engineering. Pearson Education, 1995.

[35] Claire Le Goues, Michael Dewey-Vogt, Stephanie Forrest,
and Westley Weimer. A systematic study of automated pro-
gram repair: Fixing 55 out of 105 bugs for $8 each. In 2012
34th International Conference on Software Engineering
(ICSE), pages 3–13. IEEE, 2012.

[36] Thomas Rückstiess, Frank Sehnke, Tom Schaul, Daan
Wierstra, Yi Sun, and Jürgen Schmidhuber. Exploring pa-
rameter space in reinforcement learning. Paladyn, 1(1):
14–24, 2010.

Preprint – Neurogenetic Programming Framework

10

[37] Christopher R Devlin. Voting Systems: From Method to
Algorithm. PhD thesis, California State University Channel
Islands.

Yoshua Bengio. Learning phrase representations using rnn
encoder-decoder for statistical machine translation. arXiv
preprint arXiv:1406.1078, 2014.

[38] Thomas Hunt Morgan. A Critique of the Theory of Evolu-

tion. Princeton University Press, 1916.

[39] Vadim Liventsev, Aki Härmä, and Milan Petkovi´c. Bf++:a
language for general-purpose neural program synthesis,
2021.

[40] Herbert Robbins. Some aspects of the sequential design
of experiments. Bulletin of the American Mathematical
Society, 58(5):527–535, 1952.

[41] Volodymyr Kuleshov and Doina Precup. Algorithms for
multi-armed bandit problems. CoRR, abs/1402.6028, 2014.
URL http://arxiv.org/abs/1402.6028.

[42] William G Macready and David H Wolpert. Bandit prob-
lems and the exploration/exploitation tradeoﬀ. IEEE Trans-
actions on evolutionary computation, 2(1):2–22, 1998.
[43] Felix A Gers, Jürgen Schmidhuber, and Fred Cummins.
Learning to forget: Continual prediction with lstm. 1999.
[44] Richard S Sutton and Andrew G Barto. Reinforcement

learning: An introduction. MIT press, 2018.

[45] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and

[46] Peter A Whigham et al. Grammatically-based genetic

programming.

[47] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike
adaptive elements that can solve diﬃcult learning control
problems. IEEE Transactions on Systems, Man, and Cyber-
netics, SMC-13(5):834–846, Sep. 1983. ISSN 2168-2909.
doi: 10.1109/TSMC.1983.6313077.

[48] Andrew William Moore. Eﬃcient memory-based learning

for robot control. Technical report, 1990.

[49] Thomas G. Dietterich. Hierarchical reinforcement learning
with the maxq value function decomposition. Journal of
Artiﬁcial Intelligence Research, 13:227–303, 2000.
[50] vadim0x60/evestop: Early stopping with exponential vari-
ance elmination. https://github.com/vadim0x60/
evestop. (Accessed on 01/20/2021).

[51] Félix-Antoine Fortin, François-Michel De Rainville, Marc-
André Gardner, Marc Parizeau, and Christian Gagné.
DEAP: Evolutionary algorithms made easy. Journal of
Machine Learning Research, 13:2171–2175, jul 2012.

