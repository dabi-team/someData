2
2
0
2

l
u
J

9
2

]

C
O
.
h
t
a
m

[

1
v
6
5
7
4
1
.
7
0
2
2
:
v
i
X
r
a

Asymptotic Consistency for Nonconvex Risk-Averse Stochastic
Optimization with Inﬁnite Dimensional Decision Spaces

Johannes Milz∗

Thomas M. Surowiec†

July 22, 2022

Abstract. Optimal values and solutions of empirical approximations of stochastic optimization prob-
lems can be viewed as statistical estimators of their true values. From this perspective, it is important to
understand the asymptotic behavior of these estimators as the sample size goes to inﬁnity, which is both
of theoretical as well as practical interest. This area of study has a long tradition in stochastic program-
ming. However, the literature is lacking consistency analysis for problems in which the decision variables
are taken from an inﬁnite-dimensional space, which arise in optimal control, scientiﬁc machine learning, and
statistical estimation. By exploiting the typical problem structures found in these applications that give
rise to hidden norm compactness properties for solution sets, we prove consistency results for nonconvex
risk-averse stochastic optimization problems formulated in inﬁnite-dimensional space. The proof is based on
several crucial results from the theory of variational convergence. The theoretical results are demonstrated
for several important problem classes arising in the literature.

Key words. asymptotic consistency, empirical approximation, sample average approximation, Monte
Carlo sampling, risk-averse optimization, PDE-constrained optimization, uncertainty quantiﬁcation, stochas-
tic programming

AMS subject classiﬁcations. 90C15, 90C06, 62F12, 35Q93, 49M41, 49J52

1

Introduction

The asymptotic behavior of empirical approximations is a central point of study in optimization
under uncertainty. There is a long tradition going back to the fundamental contributions [29, 20,
57, 52, 58, 31, 59, 47, 60, 50, 48, 38, 64, 51]. These works have since given rise to standard derivation
techniques for problems with ﬁnite-dimensional decision spaces. There are in essence three main
techniques used to obtain asymptotic statements. The ﬁrst possibility uses epi-convergence of
sample-based approximations of objective functions over compact sets and therefore draws from
powerful statements in the theory of variational convergence. The second type of method employs
a uniform law of large numbers for sample-based approximations of objective functions. Finally,
asymptotic statements can also be derived from stability estimates for optimal values and solutions
with respect to probability semimetrics. This requires, amongst other things, that the class of
integrands in the objective constitutes a P -uniformity class for the semimetric in question.

∗Department of Mathematics, Technical University of Munich, Boltzmannstr. 3, 85748 Garching, Germany,

milz@ma.tum.de.

†FB12 Mathematik

und

Informatik,

Philipps-Universit¨at Marburg,

Marburg,

Germany,

surowiec@mathematik.uni-marburg.de.

1

 
 
 
 
 
 
Given a general stochastic optimization problem

EP[F (z)],

min
z∈Zad

an empirical approximation would take the form

min
z∈Zad

EPN [F (z)],

(1)

(2)

where the original probability measure P is replaced by a (sequence of) typically discrete approxima-
tion(s) PN for N ∈ N. For example, the probability measure PN could be an empirical probability
measure associated with a random sample of size N from P. This is a common approach often
referred to as “sample average approximation” (SAA), see e.g., [32, 63]. A data-driven viewpoint
can be drawn from machine learning in which (1) represents the “population risk minimization”
problem and (2) the corresponding “empirical risk minimization” problem. Here, the underlying
probability measure of the data P is typically unknown. It is therefore of interest to understand
the behavior of solutions in the big data limit (as N → ∞).

The main questions can be easily stated: Do the optimal values and solution sets of (2) converge
to their “true” counterparts for (1) as N passes to inﬁnity and what is the strongest form of
stochastic convergence that can be guaranteed? If we treat the N -dependent objects as statistical
estimators of the true values and seek to prove at least convergence in probability, then these are
questions of consistency, cf. [61].

Motivated by recent advances in partial diﬀerential equation (PDE)-constrained optimization
under uncertainty [22, 35], scientiﬁc machine learning [9, 46], nonconvex stochastic programming
[40, 49, 16], and statistical estimation [54, 55, 41], we provide such consistency results for stochastic
optimization problems in which the decision variables z may be taken in an inﬁnite-dimensional
space Z. We will consider more general “risk-averse” problems in which the expectation EP is
allowed to be replaced by certain classes of convex risk functionals R. And as it is often lacking
in the application areas mentioned above, we do not assume convexity of the integrand F . For
consistency results on ﬁnite-dimensional risk-averse stochastic optimization problems, we refer the
reader to [61, 63].

From an abstract perspective, we consider stochastic optimization problems of the type

min
z∈Zad

R[F (z)] + ℘(z).

(3)

Here, Zad is typically a closed convex subset of an inﬁnite-dimensional space, e.g., L2(D); ℘ is a
deterministic convex cost function; F is a random integrand that typically depends on the solution
of a diﬀerential equation subject to random inputs; and R is a convex functional that acts as a
numerical surrogate for our risk preference, e.g., a convex combination of EP[X] and a semideviation
EP[max{0, X − E[X]}].

Despite the past successes in consistency analysis listed above, there is a major diﬃculty in
extending the ﬁnite-dimensional arguments to the inﬁnite-dimensional setting. In order to use both
the epigraphical as well as the uniform law of large numbers approaches, we need an appropriately
deﬁned norm compact set that contains both the approximate N -dependent solutions as well as
true solutions. It is not enough for the feasible set to be closed and bounded. For example, the
simple set of pointwise bilateral constraints

Zad :=

z ∈ L2(0, 1) : 0 ≤ z(x) ≤ 1 for a.e. x ∈ (0, 1)

is weakly sequentially compact in L2(0, 1), but not norm compact. The literature is not void of
results for inﬁnite-dimensional problems. However, the stability statements developed in [28, 53]

(cid:8)

(cid:9)

2

and the large deviation-type bounds derived in [44, 43] have only been demonstrated for strongly
convex risk-neutral problems. While it may be possible to extend some of these results to a
risk-averse setting, it appears rather challenging to obtain statements about the consistency of
minimizers without strong convexity. In the recent preprint [42], consistency results for optimal
values and solutions are established for risk-neutral PDE-constrained optimization using a uniform
law of large numbers.

The paper is structured as follows. In Section 2, we introduce the basic notation, assumptions,
and several preliminary results necessary for the remaining parts of the text. Afterwards, in Section
3, we present our main result. Finally, the utility of the main consistency result is demonstrated
for several problem classes in Section 4.

2 Notation, Assumptions, and Preliminary Results

We introduce several concepts, notation and assumptions that are required in the text.

2.1 Probability and Function Spaces

Throughout the text, all spaces are deﬁned over the real numbers R and metric spaces are equipped
with their Borel σ-ﬁeld. Let Ξ be a complete separable metric space, A the associated Borel σ-
algebra, and P : A → [0, 1] a probability measure. The triple (Ξ, A, P) is always assumed to be a
complete probability space. Throughout the manuscript, (Ω, F, P ) is a probability space.

If Υ is a Banach space, then its topological dual space is denoted by Υ∗.

If Υ is reﬂexive,
we identify its bi-dual (Υ∗)∗ with Υ. Throughout the text, we will use p ∈ [1, ∞) for a general
integrability exponent. In the application section, we will consider problems involving random par-
tial diﬀerential equations. These require several function spaces. The underlying physical domain
D ⊂ Rd with d ∈ {1, 2, 3} will always be an open bounded Lipschitz domain.

For a Banach space (V, k·kV ) we will denote the Lebesgue–Bochner space Lp(Ξ, A, P; V ) of all

strongly F-measurable V -valued functions by

Lp(Ξ, A, P; V ) = {u : Ξ → V : u strongly F-measurable and kukLp(Ξ,A,P;V ) < ∞}

ξ∈Ξ

endowed with the natural norms kukLp(Ξ,A,P;V ) = (EP[kukp
V ])1/p for p ∈ [1, ∞) and for bounded
ﬁelds: kukL∞(Ξ,A,P;V ) = P-ess sup
ku(ξ)kV . In the event that V = R, we simply write Lp(Ω, F, P ).
For the PDE applications, we use Lp(D) to denote the usual Lebesgue space of p-integrable (or
essentially bounded) functions over D. For more details on Lebesgue–Bochner spaces, we refer the
reader to [27, Chapter III]. We denote convergence in the norm by → and weak convergence by ⇀.
Given two random variables X1, X2 ∈ Lp(Ω, F, P ) for p ∈ [1, ∞), we say that X1 and X2 are
distributionally equivalent with respect to P if P (X1 ≤ t) = P (X2 ≤ t) for all t ∈ R. A functional
ρ : Lp(Ω, F, P ) → R is said to be law invariant with respect to P if for all distributionally equivalent
random variables X1, X2 ∈ Lp(Ω, F, P ) we have ρ(X1) = ρ(X2). In this setting, it therefore makes
sense to use the (abuse of) notation ρ(FX ), where FX (t) = P (X ≤ t) with t ∈ R as opposed
to ρ(X). We caution that this does not mean we redeﬁne the function ρ over a space of c`adl`ag
functions.

2.2 Convex Analysis and Several Key Functionals

Given a Banach space V , the (eﬀective) domain of an extended real-valued function f : V →
(−∞, ∞], will be denoted by dom(f ) := {x ∈ V : f (x) < ∞}. We typically exclude convex

3

functions that take the value −∞. For f : V → (−∞, ∞] and ε > 0, xε ∈ V is a ε-minimizer of f
provided inf v∈V f (v) is ﬁnite and f (xε) ≤ inf v∈V f (v) + ε. The ε-solution set (ε ≥ 0) is then the
set S ε := {x ∈ V : f (x) ≤ infv∈V f (v) + ε}. We use the convention S = S 0.

Let Υ be a normed space. For x ∈ Γ ⊂ Υ and Ψ ⊂ Υ, we deﬁne
kx − ykΥ and D(Γ, Ψ) = sup
x∈Γ

dist(Γ, Ψ) = inf
y∈Ψ

dist(x, Ψ).

(4)

We recall that a Banach space V has the Radon–Riesz (Kadec–Klee) property if vk → v whenever
(vk) ⊂ V is a sequence with vk ⇀ v ∈ V and kvkkV → kvkV as k → ∞. More generally, we will
say that a function ϕ : V → [0, ∞) is an R-function if it is convex and continuous, and if vk → v
as k → ∞ whenever (vk) ⊂ V is a sequence with vk ⇀ v ∈ V and ϕ(vk) → ϕ(v) as k → ∞. If V
is a reﬂexive Banach space, then there exists an R-function on V [13, p. 154]. Notions related to
that of an R-function are available in the literature, such as functions having the Kadec property
and strongly rotund functions [13, 14] (see also [30]).

As the following fact demonstrates, the class of R-functions is rather large and includes, e.g.,

typical cost functions and regularizers used in PDE-constrained optimization.

Lemma 1. Let V be a Banach space.
If ℘ : [0, ∞) → [0, ∞) is convex and strictly increasing
and ϕ : V → [0, ∞) is an R-function, then ℘ ◦ ϕ is an R-function. In particular, if V has the
Radon–Riesz property, then ℘ ◦ k · kV is an R-function.

Proof. The function ℘ ◦ ϕ is convex and continuous. Let vk ⇀ v and ℘(ϕ(vk)) → ℘(ϕ(v)). Since ℘
is strictly increasing on [0, ∞), it has a continuous inverse. Hence ϕ(vk) → ϕ(v).

For a Banach space V and (Ξ, A, P) as above, f : V × Ξ → (−∞, ∞] is said to be random lower
semicontinuous provided f is jointly measurable (with respect to the tensor-product σ-algebra of
Borel σ-algebras on V and A) and f (·, ξ) is lower semicontinuous for every ξ ∈ Ξ. If Υ1 and Υ2 are
metric spaces, then G : Υ1 × Ξ → Υ2 is a Carath´eodory mapping provided G(υ, ·) is measurable for
all v ∈ Υ1 and G(·, ξ) is continuous for all ξ ∈ Ξ.

Finally, there are many concepts of risk measures in the literature. We will work with the
following with further reﬁnements as needed in the text below. Let ρ : Lp(Ω, F, P ) → (−∞, ∞].
We consider the following conditions on the functional ρ.
(R1) Convexity. For all X, Y ∈ Lp(Ω, F, P ) and λ ∈ (0, 1), we have ρ(λX + (1 − λ)Y ) ≤ λρ(X) +

(1 − λ)ρ(Y ).

(R2) Monotonicity. For all X, Y ∈ Lp(Ω, F, P ) such that X ≤ Y w.p. 1, we have ρ(X) ≤ ρ(Y ).
(R3) Translation equivariance. If X ∈ Lp(Ω, F, P ) and C is a degenerate random variable with

C ≡ c w.p. 1 for some c ∈ R, then ρ(X + C) = ρ(X) + c.

(R4) Positive homogeneity. If X ∈ Lp(Ω, F, P ) and γ > 0, then ρ(γX) = γρ(X).
The risk measure ρ is called convex if it satisﬁes (R1)–(R3) and it is referred to as coherent if it
satisﬁes (R1)–(R4), see [2], [21], and in particular [63, p. 231].

2.3 Epiconvergence and Weak Inf-Compactness

Variational convergence, in particular (Mosco-)epiconvergence play a central role in consistency
analysis. We provide here the necessary deﬁnitions and results from the literature. In addition, we
prove several new results that are tailored to applications involving PDEs with random inputs.

We recall the notions of epiconvergence and Mosco-epiconvergence [4].

Deﬁnition 2 (Epiconvergence). Let V be a complete metric space. Let φk : V → (−∞, ∞] be a
sequence and let φ : V → (−∞, ∞] be a function. The sequence (φk) epiconverges to φ if for each
v ∈ V

4

1. and each (vk) ⊂ V with vk → v as k → ∞, lim inf k→∞ φk(vk) ≥ φ(v), and
2. there exists (vk) ⊂ V with vk → v as k → ∞ such that lim supk→∞ φk(vk) ≤ φ(v).

In many instances in inﬁnite-dimensional optimization, especially the calculus of variations,
optimal control, and PDE-constrained optimization we are forced to work with weaker topologies
in the context of variational convergence. If the underlying space is a reﬂexive Banach space, then
we may appeal to epiconvergence in the sense of Mosco, which was introduced in [45].

Deﬁnition 3 (Mosco-epiconvergence). Let V be a reﬂexive Banach space and let V0 ⊂ V be a
closed, convex set. Let φk : V0 → (−∞, ∞] be a sequence and let φ : V0 → (−∞, ∞] be a function.
The sequence (φk) Mosco-epiconverges to φ if for each v ∈ V0

1. and each (vk) ⊂ V0 with vk ⇀ v as k → ∞, lim inf k→∞ φk(vk) ≥ φ(v), and
2. there exists (vk) ⊂ V0 with vk → v such that lim supk→∞ φk(vk) ≤ φ(v).

In the deﬁnition of Mosco-epiconvergence, we allow for the sequence (φk) and the epi-limit φ to
be deﬁned on a convex, closed subset of a reﬂexive Banach space. This allows us to model constraints
without the need for indicator functions. We will see below in Theorem 4 that this variation on the
original deﬁnition leaves the crucial implications of Mosco-epiconvergence intact. In other words,
Theorem 4 provides conditions suﬃcient for consistency of optimal values of Mosco-epiconvergent
objective functions; compare with [3, Thm. 1.10], [17, Thm. 5.3], and [14, Thm. 6.2.8], for example.

Theorem 4. Let V be a reﬂexive Banach space and let V0 ⊂ V be a closed, convex set. Suppose
that hk : V0 → (−∞, ∞] Mosco-epiconverges to h : V0 → (−∞, ∞]. Let (vk) ⊂ V0 and (εk) ⊂ [0, ∞)
be sequences such that εk → 0+ and for each k ∈ N, let vk satisfy

hk(vk) ≤ inf
v∈V0

hk(v) + εk.

If (vk)K is a subsequence of (vk) such that vk ⇀ ¯v as K ∋ k → ∞, then

1. ¯v ∈ V0,
2. h(¯v) = inf v∈V0 h(v),
3. inf v∈V0 hk(v) → inf v∈V0 h(v) as K ∋ k → ∞,
4. hk(vk) → h(¯v) as K ∋ k → ∞.

Proof. Since (hk) Mosco-epiconverges to h on V0, it epiconverges to h, where V0 may be understood
as a complete metric space using the norm topology. Hence

lim sup
k→∞

inf
v∈V0

hk(v) ≤ inf
v∈V0

h(v);

(5)

see, e.g., [3, Props. 1.14 and 2.9]. Since (vk) ⊂ V0 and V0 is weakly sequentially closed, we have
¯v ∈ V0. Then Mosco-epiconvergence ensures

lim inf
K∋k→∞

inf
v∈V0

hk(v) = lim inf
K∋k→∞

[εk + inf
v∈V0

hk(v)] ≥ lim inf
K∋k→∞

hk(vk) ≥ h(¯v).

Combined with (5), we ﬁnd that h(¯v) = infv∈V0 h(v) and infv∈V0 hk(v) → infv∈V0 h(v) as K ∋ k →
∞. The assertion hk(vk) → h(¯v) as K ∋ k → ∞ is implied by the above derivations and

lim sup
K∋k→∞

hk(vk) ≤ lim sup
K∋k→∞

[εk + inf
v∈V0

hk(v)] = lim sup
K∋k→∞

inf
v∈V0

hk(v) ≤ lim sup
k→∞

inf
v∈V0

hk(v).

5

Proposition 5 demonstrates a weak compactness property of approximate minimizers to “regu-
larized” optimization problems with Mosco-epiconvergent objective functions. Let Z be a reﬂexive
Banach space, let Zad ⊂ Z be a closed, convex set, and let fk, f : Zad → (−∞, ∞]. Furthermore,
let ϕ : Z → [0, ∞) be a convex, continuous function. We deﬁne the optimal values

m∗

k := inf
z∈Zad

{ fk(z) + ϕ(z) }

and m∗ := inf
z∈Zad

{ f (z) + ϕ(z) }

(6)

and the solution sets

k := { z ∈ Zad : fk(z) + ϕ(z) ≤ m∗
S εk

k + εk }

and S := { z ∈ Zad : f (z) + ϕ(z) = m∗ }.

The optimal values in (6) are somewhat related to evaluations of certain epi-sums [4].

Proposition 5. Let Z be a reﬂexive Banach space, let Zad ⊂ Z be a closed, convex set, let
ϕ : Z → [0, ∞) be a convex, continuous function, and let Z0 ⊂ Z be bounded. Suppose that
fk : Zad → (−∞, ∞] Mosco-epiconverges to f : Zad → (−∞, ∞]. Let (εk) ⊂ [0, ∞) be a sequence
with εk → 0+. Suppose that S 6= ∅ and that for all k ∈ N,

S εk
k ⊂ Z0

and S εk

k 6= ∅.

If (zk) is a sequence with zk ∈ S εk
for all k ∈ N and (zk)K is a subsequence of (zk), then (zk)K has
k
a further subsequence (zk)K1 converging weakly to some ¯z ∈ S and ϕ(zk) → ϕ(¯z) as K1 ∋ k → ∞.

Proof. Since (zk)K ⊂ Zad, (zk)K ⊂ Z0, Z0 is bounded, and Zad is closed and convex, (zk)K has
a further subsequence (zk)K1 such that zk ⇀ ¯z ∈ Zad as K1 ∋ k → ∞ [12, Thms. 2.23 and 2.28].
Since ¯z ∈ Zad, the Mosco-epiconvergence of (fk) to f ensures the existence of a sequence (˜zk) ⊂ Zad
such that ˜zk → ¯z ∈ Zad as k → ∞ and lim supk→∞ fk(˜zk) ≤ f (¯z). Since ˜zk → ¯z implies ˜zk ⇀ ¯z,
we have limk→∞ fk(˜zk) = f (¯z). Since ˜zk ∈ S εk

k , we have for all k ∈ N,

fk(zk) + ϕ(zk) ≤ fk(˜zk) + ϕ(˜zk) + εk.

(7)

Since (fk) Mosco-epiconverges to f , we have f (¯z) ≤ lim inf K1∋k→∞ fk(zk). Combined with the fact
that ϕ is continuous and

lim inf
K1∋k→∞

fk(zk) + lim sup
K1∋k→∞

ϕ(zk) ≤ lim sup
K1∋k→∞

fk(zk) + ϕ(zk),

the estimate (7) ensures

f (¯z) + lim sup
K1∋k→∞

ϕ(zk) ≤ lim sup
K1∋k→∞

fk(˜zk) + ϕ(˜zk) + εk ≤ lim sup
k→∞

fk(˜zk) + ϕ(˜zk) + εk

= lim
k→∞

fk(˜zk) + ϕ(˜zk) + εk = f (¯z) + ϕ(¯z).

Since zk ⇀ ¯z as K1 ∋ k → ∞, S 6= ∅, and (fk) Mosco-epiconverges to f , Theorem 4 ensures
¯z ∈ S. Since ¯z ∈ S, we have f (¯z) ∈ R. Thus lim supK1∋k→∞ ϕ(zk) ≤ ϕ(¯z). Since ϕ is convex and
continuous, it is weakly lower semicontinuous. Combined with zk ⇀ ¯z as K1 ∋ k → ∞, we have
ϕ(zk) → ϕ(¯z) as K1 ∋ k → ∞.

While the sum of an Mosco-epiconvergent sequence and a convex, continuous function Mosco-
epiconverge, Proposition 5 allows us to draw further conclusions about the minimizers to composite
optimization problems deﬁned by sums of Mosco-epiconvergent and convex, continuous functions
than a direct application of the “sum rule.” For example, if ϕ is an R-function, then the sequence
(zk)K1 considered in Proposition 5 converges strongly to an element of S.

6

Corollary 6. If the hypotheses of Proposition 5 hold true and ϕ is an R-function, then each
subsequence of (zk) has a further subsequence converging strongly to an element of S.

Remark 7. If Zad is bounded, then we can choose Z0 = Zad in Proposition 5. The condition
S εk
k ⊂ Z0 for all k ∈ N in Proposition 5 is related to a “weak inf-compactness” condition, provided
that Z0 is also convex and bounded. In this case, Z0 is weakly (sequentially) compact. Instead of
requiring S εk
k ⊂ Z0 for all k ∈ N, we could require for some γ ∈ R and for all k ∈ N,

∅ 6= { z ∈ Zad : fk(z) + ϕ(z) ≤ γ } ⊂ Z0.

(8)

The level set condition (8) ensures that S εk
is nonempty, provided that fk is weakly lower semi-
k
continuous. In case that Z0 is norm compact, the condition (8) has been used, for example, in
Theorem 2.1 in [38] to establish consistency properties for inﬁnite-dimensional stochastic programs.
If supk∈N m∗

k < ∞, γ > supk∈N m∗

k and for all k ∈ N,

{ z ∈ Z : fk(z) + ϕ(z) ≤ γ } ⊂ Z0,

k ⊂ Z0 for all suﬃciently large k ∈ N since we eventually have supk∈N m∗

then S εk
Corollary 8. If the hypotheses of Proposition 5 hold, then m∗
is an R-function, then D(S εk

k , S) → 0 as k → ∞.

k → m∗ as k → ∞. If furthermore ϕ

k + εk ≤ γ.

k)K be a subsequence of (m∗

It must still be shown that D(S εk

k) has a further subsequence converging to m∗. Hence m∗

Proof. Let zk ∈ S εk
k for each k ∈ N. The hypotheses ensure that (fk) Mosco-epiconverges to f . Let
(m∗
k). Proposition 5 ensures that (zk)K has a further subsequence (zk)K1
that weakly converges to some element in S. Combined with Theorem 4, we ﬁnd that m∗
k → m∗ as
K1 ∋ k → ∞. Since S is nonempty, m∗ ∈ R. Putting together the pieces, we have shown that each
subsequence of (m∗

k → m∗ as k → ∞.
k ⊂ Z0 and S ⊂ Z are
nonempty, and Z0 is bounded, we have D(S εk
k , S) ≤ D(Z0, S) < ∞ for all k ∈ N. Let us deﬁne
the sequence ̺k := D(S εk
k , S). Let (̺k)K be a subsequence of (̺k). We have just shown that
̺k ≤ D(Z0, S) < ∞. Moreover ̺k ≥ 0. Using the deﬁnition of the deviation, we ﬁnd that there
exists for each k ∈ N, ˜zk ∈ S εk
k such that ̺k ≤ dist(˜zk, S) + 1/k. Corollary 6 ensures that (˜zk)K has
a further subsequence (˜zk)K1 that strongly converges to some ¯z ∈ S. Since S is nonempty, dist(·, S)
is (Lipschitz) continuous [1, Thm. 3.16]. It follows that dist(˜zk, S) → 0 as K1 ∋ k → ∞. Hence
̺k → 0 as K1 ∋ k → ∞. Since each subsequence of (̺k) has a further subsequence converging to
zero, ̺k → 0 as k → ∞.

k , S) → 0 as k → ∞. Since S εk

Proposition 9 demonstrates that epiconvergence can imply Mosco-epiconvergence. This result is
particularly relevant for PDE-constrained problems in which operators of the type B often appear.
If V is a Banach space and Y is a complete metric space, we refer to a mapping G : V → Y as
completely continuous if (vk) ⊂ V and vk ⇀ v ∈ V implies G(vk) → G(v).

Proposition 9. Let Z0 ⊂ Z be a closed, convex subset of a reﬂexive Banach space Z and let
Y0 ⊂ Y be a closed subset of a Banach space Y . Suppose that B : Z → Y is linear and completely
continuous with B(Z0) ⊂ Y0.
If hk : Y0 → (−∞, ∞] epiconverges to h : Y0 → (−∞, ∞] and
hk ◦ B : Z0 → (−∞, ∞] epiconverges to h ◦ B : Z0 → (−∞, ∞], then hk ◦ B : Z0 → (−∞, ∞]
Mosco-epiconverges to h ◦ B : Z0 → (−∞, ∞].

Proof. Fix ¯z ∈ Z0. Let (zk) ⊂ Z0 be a sequence with zk ⇀ ¯z. We have ¯z ∈ Z0 and Bzk,
B¯z ∈ Y0. The complete continuity of B yields Bzk → B¯z as k → ∞. Since (hk) epiconverges to h,
lim inf k→∞ hk(Bzk) ≥ h(B¯z). The hypotheses ensure that hk ◦ B epiconverges to h ◦ B. Putting
together the pieces, we conclude that (hk ◦ B) Mosco-epiconverges to h ◦ B.

7

3 Consistency of Empirical Approximations

We consider the potentially inﬁnite-dimensional risk-averse stochastic program

min
z∈Zad

R[F (Bz)] + ℘(z),

(9)

where

F (y)(ω) := f (y, ξ(ω)).

Let ξ : Ω → Ξ be a random element and ξ1, ξ2, . . . deﬁned on (Ω, F, P ) be independent identi-
cally distributed Ξ-valued random elements with law P = P ◦ ξ−1. Let R : Lp(Ω, F, P ) → R be a
law invariant risk measure. For y ∈ Y and N ∈ N, let ˆHy,N = ˆHy,N,ω be the empirical distribution
function of the sample f (y, ξ1), . . . , f (y, ξN ). The empirical approximation of (9) is given by

R[ ˆHBz,N ] + ℘(z).

min
z∈Zad

(10)

Recall from our discussion on law invariant risk measures that R[ ˆHBz,N ] means the risk mea-
sure R does not distinguish between z, N -dependent random variables with distribution functions
equivalent to ˆHBz,N .

Assumption 10.

1. The space Z is a separable, reﬂexive Banach space, Zad ⊂ Z is nonempty,
closed, convex and bounded. The space Y is a Banach space, Y0 ⊂ Y is closed, and p ∈ [1, ∞).

2. The mapping B : Z → Y is linear and completely continuous, and B(Zad) ⊂ Y0.
3. The function ℘ : Z → [0, ∞) is convex and continuous.
4. The function f : Y0 × Ξ → R is a Carath´eodory function.
5. For all y ∈ Y0, f (y, ξ(·)) ∈ Lp(Ω, F, P ) and F : Y0 → Lp(Ω, F, P ) is continuous.
6. For each ¯y ∈ Y0, there exists a neighborhood Y¯y of ¯y and a random variable h ∈ Lp(Ω, F, P )

such that f (y, ξ(·)) ≥ h(·) for all y ∈ V¯y.

Let m∗ be the optimal value of problem (9) and let S be its solution set. Let r ≥ 0. Further-

more, let ˆm∗

N be the optimal value of (10) and let ˆS r

N be its set of r-minimizers.

Theorem 11. Let Assumption 10 hold. Suppose further that (Ω, F, P ) is nonatomic and complete.
Let R : Lp(Ω, F, P ) → R be a convex, law invariant risk measure. If (rN ) ⊂ [0, ∞) is a deterministic
N → m∗ w.p. 1 as N → ∞. If furthermore ℘ is an
sequence such that rN → 0 as N → ∞, then ˆm∗
R-function, then D( ˆS rN

N , S ) → 0 w.p. 1 as N → ∞.

To establish Theorem 11, we verify the hypotheses of Corollaries 8 and 22 (in the Appendix).

Lemma 12. If Assumption 10 holds and R : Lp(Ω, F, P ) → R is a convex risk measure, then
Z ∋ z 7→ R[F (Bz)] is completely continuous.

Proof. Since R is a ﬁnite-valued convex risk measure, it is continuous [56, Cor. 3.1]. Assumption 10
ensures the continuity of y 7→ F (y). Hence y 7→ R[F (y)] is continuous. Now the complete continuity
of B implies that of z 7→ R[F (Bz)].

Lemma 13. Let Assumption 10 hold. Suppose further that (Ω, F, P ) is nonatomic and complete.
Let R : Lp(Ω, F, P ) → R be a convex, law invariant risk measure. Then Y ×Ω ∋ (y, ω) 7→ R[ ˆHy,N,ω]
is a Carath´eodory function.

8

Proof. Let ([0, 1], Σ, ν) be a probability space with Σ the Lebesgue sigma-ﬁeld of [0, 1] and ν the
N
uniform distribution on [0, 1]. We recall ˆHy,N,ω(t) = (1/N )
i=1 1(−∞,t](f (y, ξi(ω))) for t ∈ R.
Let f (y, ξ(1)) ≤ · · · ≤ f (y, ξ(N )) be the order statistics of the sample f (y, ξ1), . . . , f (y, ξN ). For
q ∈ (0, 1] and y ∈ Y , we have ˆH −1
y,N,ω(q) = f (y, ξ(j)(ω)) if q ∈ ((j − 1)/N, j/N ] irrespective of
whether the sample is distinct.

P

We show that y 7→ R[ ˆHy,N,ω] is continuous for each ω ∈ Ω. Let yk → y and ﬁx ω ∈ Ω. Using

the fact that ν is the uniform distribution, we have

| ˆH −1

yk,N,ω(q) − ˆH −1

y,N,ω(q)|pdν(q) =

Z[0,1]

1
N

N

Xi=1

|f (yk, ξi(ω)) − f (y, ξi(ω))|p.

Since f is a Carath´eodory function and p ∈ [1, ∞), it follows that ˆH −1
y,N,ω in Lp([0, 1], Σ, ν).
Since (Ω, F, P ) is nonatomic and complete, there exists a random variable Q : Ω → [0, 1] with law ν,
that is, P ◦Q−1 = ν [19, Prop. A.5]. The functions Xω,k : Ω → R deﬁned by Xω,k := ˆH −1
yk,N,ω(Q) are
random variables with distribution function ˆHyk,N,ω [18, Prop. 9.1.2]. Similarly, Xω := ˆH −1
y,N,ω(Q)
is a random variable with distribution function ˆHy,N,ω [18, Prop. 9.1.2]. Combined with the law
invariance of R, we have R[Xω,k] = R[ ˆHyk,N,ω] and R[Xω] = R[ ˆHy,N,ω]. Since ˆH −1
y,N,ω
in Lp([0, 1], Σ, ν), a change of variables (see, e.g., Theorem 3.6.1 in [11]) yields Xω,k → Xω in
Lp(Ω, F, P ). Indeed, using P ◦ Q−1 = ν, we have

yk,N,ω → ˆH −1

yk,N,ω → ˆH −1

|Xω,k(¯ω) − Xω(¯ω)|pdP (¯ω) =

ZΩ

ZΩ

| ˆH −1

yk,N,ω(Q(¯ω)) − ˆH −1

y,N,ω(Q(¯ω))|pdP (¯ω)

=

Z[0,1]

| ˆH −1

yk,N,ω(q) − ˆH −1

y,N,ω(q)|pdν(q).

Combined with the continuity of R [56, Cor. 3.1], we have R[Xω,k] → R[Xω]. Consequently,
y 7→ R[ ˆHy,N,ω] is continuous for each ω ∈ Ω.

For each ﬁxed y ∈ Y , the function ω 7→ ˆH −1

y,N,ω(Q) ∈ Lp(Ω, F, P ) is measurable because it is

the composition of a piecewise constant and measurable functions.

Combining these arguments, we ﬁnd that (y, ω) 7→ R[ ˆHy,N,ω] is a Carath´eodory mapping.

Corollary 14. Under the hypotheses of Lemma 13, (a) S is nonempty and closed, (b)
nonempty, closed images for each r ∈ [0, ∞), and (c) ˆm∗
[0, ∞).

N has
N are measurable for each r ∈

N and

ˆS r

ˆS r

Proof.
bined with the direct method of the calculus of variations ensures the assertions.

(a) Since the set Zad is nonempty, closed, convex, and bounded, Lemma 12 when com-

(b) Using the properties of Zad listed in part (a), Lemma 13 when combined with the direct

method of the calculus of variations and the complete continuity of B ensures the assertions.

(c) Since B is completely continuous and Z is a Banach space, B is continuous. Lemma 13,

the continuity of B, and Theorem 8.2.11 in [5] imply the measurability assertions.

Proof of Theorem 11. To establish the consistency statements, we verify the hypotheses of Corol-
laries 8 and 22. Corollary 14 ensures that S is nonempty. Hence dist(·, S ) is (Lipschitz) continuous
[1, Thm. 3.16]. Corollary 14 implies that ˆm∗
N is measurable with closed,
nonempty images. Combined with Theorem 8.2.11 in [5], it follows that D( ˆS rN
N , S ) is measurable.
Corollary 22 ensures that Zad ∋ z 7→ R[ ˆHBz,N ] Mosco-epiconverges to Zad ∋ z 7→ R[F (Bz)]
N and S ⊂ Zad are nonempty, and ℘ is

N is measurable and that ˆS rN

w.p. 1 as N → ∞. We have

N ⊂ Zad. Moreover,

ˆS rN

ˆS rN

9

continuous and convex. Now, Corollary 8 ensures that w.p. 1, ˆm∗
℘ is an R-function, then Corollary 8 ensures w.p. 1, D( ˆS rN
D( ˆS rN

N , S ) are measurable, we obtain the almost sure convergence statements.

N → m∗ as N → ∞. If furthermore
N and

N , S ) → 0 as N → ∞. Since ˆm∗

4 Applications

We conclude with the application of our main result, Theorem 11, to several problem classes.

4.1 Consistency of Epi-Regularized and Smoothed Empirical Approximations

Using Theorem 11, we demonstrate the consistency of solutions to epi-regularized and smoothed
risk-averse programs using the average value-at-risk. These types of risk measures are popular
in numerical approaches, see [34, 36, 37, 6, 67, 15]. For β ∈ [0, 1), the average value-at-risk
AVaRβ : L2(Ω, F, P ) → R is deﬁned by

AVaRβ[X] = inf
t∈R

{ t + 1
1−β

E[(X − t)+] },

where (x)+ = max{0, x} for x ∈ R. Throughout the section, m∗ and S denotes the optimal value
and solution set of (9), respectively, with the risk measure R = AVaRβ. Moreover, we denote by
m∗
N and ˆS rN
N their empirical counterparts. The average value-at-risk AVaRβ is a law invariant risk
measure [62].

Epi-regularization of risk measures has been proposed and analyzed in [36]. We apply the

epi-regularization to the average value-at-risk. We deﬁne Φ : L2(Ω, F, P ) → (−∞, ∞] by

Φ[X] := (1/2)E[X 2] + E[X].

For ε > 0, the epi-regularization AVaRε

β : L2(Ω, F, P ) → R of AVaRβ is given by

AVaRε

β[X] :=

inf
Y ∈L2(Ω,F ,P )

{ AVaRβ[X − Y ] + εΦ[ε−1Y ] }.

(11)

The risk functional AVaRε

β can be shown to be law invariant. See Appendix B.

For ε > 0, we consider the epi-regularized empirical average value-at-risk optimization problem

min
z∈Zad

{ AVaRε

β[ ˆHBz,N ] + ℘(z) }.

epi,N be its optimal value and ˆS ε

We let ˆmε
epi,N be its solution set. Note that for ﬁxed ε > 0, our
main result, Theorem 11, already provides an asymptotic consistency result. However, in numerical
procedures, the ε-parameter is typically driven to zero. Therefore, we prove a stronger statement
here.

Proposition 15. Let Assumption 10 hold. Suppose further that (Ω, F, P ) is nonatomic and com-
epi,N → m∗ w.p. 1 as N → ∞. If
plete. Let (εN ) ⊂ (0, ∞) with εN → 0 as N → ∞. Then ˆmεN
furthermore ℘ is an R-function, then D( ˆS εN

epi,N , S ) → 0 w.p. 1 as N → ∞.

The proof of Proposition 15 is based on the following result.

Lemma 16. Fix ε > 0. The functional AVaRε
measure. For all X ∈ L2(Ω, F, P ), it holds that

β : L2(Ω, F, P ) → R is a law invariant, convex risk

AVaRβ[X] − εβ

2(1−β) ≤ AVaRε

β[X] ≤ AVaRβ[X].

10

Proof. The functional AVaRε
β is a convex risk measure [36, pp. 3 and 5–6]. By the arguments in
Appendix B, it is law invariant. Since Φ[0] = 0, the second estimate is implied by Proposition 1
in [36]. Fix X ∈ L2(Ω, F, P ). Since AVaRβ is subdiﬀerentiable [63, p. 243], Proposition 2 in [36]
yields for all ϑ ∈ ∂AVaRβ[X],

AVaRε

β[X] ≥ AVaRβ[X] − εΦ∗[ϑ].

Let ϑ ∈ ∂AVaRβ[X] be arbitrary. We have 0 ≤ ϑ ≤ 1/(1 − β) w.p. 1, E[ϑ] = 1 [63, p. 243] and
Φ∗[ϑ] = (1/2)E[(ϑ − 1)2]; see Remark 5 in [36]. Here Φ∗ is the Fenchel conjugate to Φ. Hence

Φ∗[ϑ] = (1/2)E[ϑ2] − E[ϑ] + (1/2) = (1/2)E[ϑ2] − (1/2) ≤

1
2

1 − (1 − β)
1 − β

=

1
2

β
1 − β

.

Proof of Proposition 15. Following the proof of Corollary 14 and using the fact that AVaRεN
β
law invariant, convex risk measure (see Lemma 16), we ﬁnd that ˆmεN
Lemma 16 ensures that ˆm∗
yields ˆm∗
N → ∞.
If zεN
N . Using (4), we ﬁnd that D( ˆS εN

is a
epi,N are measurable.
N . Applying Theorem 11 with R = AVaRβ
epi,N → m∗ w.p. 1 as

N ∈ ˆS rN
epi,N ⊂
N , S ). Applying Theorem 11 with R = AVaRβ

epi,N , then Lemma 16 ensures that zεN
epi,N , S ) ≤ D( ˆS rN

N → m∗ w.p. 1 as N → ∞. Combined with εN → 0, we ﬁnd that ˆmεN

N , where rN := εN β

2(1−β) . Hence ˆS εN

2(1−β) ≤ ˆmεN

epi,N and S εN

epi,N ≤ ˆm∗

N ∈ ˆS εN

N − εN β

ˆS rN
yields the second assertion.

Next, we establish the consistency of solutions to smoothed average value-at-risk problems using
a smoothing function for (·)+. For brevity, we focus on a particular smoothing function for the plus
function (·)+. For ε > 0, we deﬁne the smoothed plus function (·)+

ε : R → R by

(x)+

ε := ε ln(1 + exp(x/ε)).

Using (·)+

ε , we deﬁne the smoothed average value-at-risk σε

β : L2(Ω, F, P ) → R by

σε
β[X] := inf
t∈R

{ t + 1
1−β

E[(X − t)+

ε ] }.

(12)

This version of the smoothed average value-at-risk has been used in [67] for stochastic stellarator coil
design and in [6] for adaptive sampling techniques for risk-averse optimization. See the Appendix
B for a short proof of its law invariance.

For ε > 0, we consider the smoothed empirical average value-at-risk optimization problem

min
z∈Zad

{ σε

β[ ˆHBz,N ] + ℘(z) },

We let ˆmε

s,N be its optimal value and ˆS ε

s,N be its solution set.

Proposition 17. Let Assumption 10 hold. Suppose further that (Ω, F, P ) is nonatomic and com-
s,N → m∗ w.p. 1 as N → ∞. If
plete. Let (εN ) ⊂ (0, ∞) with εN → 0 as N → ∞. Then ˆmεN
furthermore ℘ is an R-function, then D( ˆS εN

s,N , S ) → 0 w.p. 1 as N → ∞.

Proposition 17 is established using Lemma 18.

11

Lemma 18. Fix ε > 0. The functional σε
measure. For all X ∈ L2(Ω, F, P ), it holds that

β : L2(Ω, F, P ) → R is a law invariant, convex risk

AVaRβ[X] ≤ σε

β[X] ≤ AVaRβ[X] + ln(2)ε/(1 − β).

Proof. The smoothed average value-at-risk σε
arguments in Appendix B, it is law invariant. For x ∈ R, we have (x)+ ≤ (x)+
yielding the error bounds.

β is a convex risk measure [34, Props. 4.4–4.6]. By the
ε ≤ (x)+ + ε ln(2),

s,N and ˆS εN

Proof of Proposition 17. The proof is similar to that of Proposition 15. Following the proof of
Corollary 14 and using the fact that σεN
is a law invariant, convex risk measure (see Lemma 18),
β
we ﬁnd that ˆmεN
N +
ln(2)εN /(1 − β). Applying Theorem 11 with R = AVaRβ yields ˆm∗
N → m∗ w.p. 1 as N → ∞.
s,N → m∗ w.p. 1 as N → ∞.
Combined with εN → 0, we ﬁnd that ˆmεN
N ∈ ˆS rN
s,N , S ) ≤ D( ˆS rN

N , where rN := ln(2)εN /(1 − β). Hence
N , S ). Applying Theorem 11 with

s,N are measurable. Lemma 16 ensures that ˆm∗

s,N , then Lemma 16 ensures that zεN

N . Using (4), we ﬁnd that D( ˆS εN

If zεN
s,N ⊂ ˆS rN

s,N ≤ ˆm∗

N ∈ ˆS εN

N ≤ ˆmεN

ˆS εN
R = AVaRβ yields the second assertion.

4.2 Risk-Averse Semilinear PDE-Constrained Optimization

Our consistency result, Theorem 11, is applicable to risk-averse semilinear PDE-constrained opti-
mization as we demonstrate in this section. Following [37] (see also [23, 24]), we consider

min
z∈Zad

(1/2)R[k(1 − ιS(z))+k2

L2(D)] + (α/2)kzk2

L2 (D),

(13)

where α > 0, ι : H 1(D) → L2(D) is the embedding operator of the compact embedding H 1(D) ֒→
L2(D), Zad := { z ∈ L2(D) : l ≤ z ≤ u } with l, u ∈ L2(D) and l ≤ u, and for each (z, ξ) ∈ L2(D)×Ξ,
S(z)(ξ) ∈ H 1(D) is the solution to:

ﬁnd u ∈ H 1(D) : A(u, ξ) = B1(ξ)ι∗z + b(ξ),

(14)

where A : H 1(D) × Ξ → H 1(D)∗, B1 : Ξ → L (H 1(D)∗, H 1(D)∗), and b : Ξ → H 1(D)∗ are deﬁned
by

hA(u, ξ), viH 1(D)

∗

,H 1(D) :=

hB1(ξ)y, viH 1(D)

∗

,H 1(D) :=

ZD

ZD

a(ξ)(x)[∇u(x)T ∇v(x) + u(x)v(x)]dx +

u(x)3v(x)dx,

ZD

[B(ξ)y](x)v(x)dx,

hb(ξ), viH 1(D)

∗

,H 1(D) :=

b(ξ)(x)v(x)dx.

ZD

Here, b : Ξ → L2(D) is essentially bounded, a : Ξ → C 0( ¯D) is measurable and there exists
constants κmin, κmax > 0 such that κmin ≤ a(ξ)(x) ≤ κmax for all (ξ, x) × Ξ × ¯D. It remains to
deﬁne B(ξ) : H 1(D)∗ → H 1(D). Fix (y, ξ) ∈ H 1(D)∗ ×Ξ. We deﬁne B(ξ)y ∈ H 1(D) as the solution
to: ﬁnd w ∈ H 1(D) such that

[r(ξ)∇w(x)T v(ξ) + w(x)v(x)]dx = hy, viH 1(D)

∗

,H 1(D)

for all

v ∈ H 1(D),

ZD

where r : Ξ → (0, ∞) is random variable such that there exists rmin, rmax > 0 with rmin ≤ r(ξ) ≤
rmax for all ξ ∈ Ξ. Since ιu = u for all u ∈ H 1(D), we have hι∗z, viH 1(D)
,H 1(D) = (z, v)L2(D) for
all z ∈ L2(D) and v ∈ H 1(D) [12, p. 21].

∗

12

We express (13) in the form given in (9) and verify Assumption 10. For each (y, ξ) ∈ H 1(D)∗×Ξ,

we consider the auxiliary random operator equation

Find u ∈ H 1(D) : A(u, ξ) = B1(ξ)y + b(ξ).

(15)

Lemma 19. Under the above hypotheses, for each (y, ξ) ∈ H 1(D)∗ × Ξ, the operator equation (15)
S(y) ∈ Lq(Ξ, A, P; H 1(D)) for each q ∈ [1, ∞] and y ∈ H 1(D)∗, and
has a unique solution
S : H 1(D)∗ → Lq(Ξ, A, P; H 1(D)) is Lipschitz continuous for each q ∈ [1, ∞].

S(y)(ξ),

e

e

Let U be a reﬂexive Banach space. We recall that an operator A : U → U ∗ is κ-strongly

e
monotone if there exists κ > 0 such that

hA(u2) − A(u1), u2 − u1iU ∗,U ≥ κku2 − u1k2
U

for all u1, u2 ∈ U.

Proof of Lemma 19. For each ξ ∈ Ξ, A(·, ξ) is κmin-strongly monotone and it holds that

cf. [37, p. 13]. The existence, uniqueness and the stability estimate

kB1(ξ)kL (H 1(D)∗,H 1(D)) ≤ 1/ min{rmin, 1};

S(y)(ξ)kH 1(D) ≤ (1/κmin)kB1(ξ)ykH 1(D)∗ + (1/κmin)kb(ξ)kH 1(D)∗
k
e

are a consequence of the Minty–Browder theorem [68, Thm. A.26], for example. Using Filippov’s
S(y) is measurable. Combined with the stability
theorem [5, Thm. 8.2.10], we can show that
S(y) ∈ Lq(Ξ, A, P; H 1(D)) for each q ∈ [1, ∞]
estimate and H¨older’s inequality, we conclude that
and y ∈ H 1(D)∗. Since for all y1, y2 ∈ H 1(D)∗ and ξ ∈ Ξ, we have (cf. [37, eq. (3.7)])

e

e

the mapping

S : H 1(D)∗ → Lq(Ξ, A, P; H 1(D)) is Lipschitz continuous for all q ∈ [1, ∞].

S(y2)(ξ) −
k

e

S(y1)(ξ)kU ≤ (1/κmin)kB1(ξ)[y2 − y1]kH 1(D)∗,
e

e

The function ℘ deﬁned by ℘(z) := (α/2)kzk2

L2 (D) is an R-function according to Lemma 1, as
α > 0 and L2(D) is a Hilbert space and hence has the Radon–Riesz property [12, Prop. 2.35]. The
operator B := ι∗ with ι∗ being the adjoint operator to ι is linear and completely continuous because
ι is a compact operator by the Sobolev embedding theorem. We deﬁne f : H 1(D)∗ × Ξ → [0, ∞)
L2(D). The mapping J : L2(Ξ, A, P; H 1(D)) → [0, ∞) given by
S(y)(ξ))+k2
by f (y, ξ) := (1/2)k(1 − ι
J (y) = (1/2)k(1 − ιy)+k2
S, Lemma 19 ensures that
L2(D) is continuous [37, p. 14]. Since F = J ◦
e
F : H 1(D)∗ → L2(Ξ, A, P; H 1(D)) is well-deﬁned and continuous. Having veriﬁed Assumption 10,
we can apply Theorem 11 to study the consistency of empirical approximations of (13).

e

4.3 Risk-Averse Optimization with Variational Inequalities

We consider a risk-averse optimization problem governed by an elliptic variational inequality with
random inputs. Our presentation is inspired by that in [26]. We consider

min
z∈Zad

(1/2)R[kιS(z) − udk2

L2(D)] + (α/2)kzk2

L2 (D),

(16)

where α > 0, ud ∈ L2(D), ι : H 1
H 1(D) ֒→ L2(D), and Zad is as in Section 4.2. For each (z, ξ) ∈ L2(D) × Ξ, S(z)(ξ) ∈ H 1
the solution to the parameterized elliptic variational inequality:

0 (D) → L2(D) is the embedding operator of the compact embedding
0 (D) is

ﬁnd u ∈ Kψ :

hA(ξ)u − ι∗z, v − uiH −1(D),H 1

0 (D) ≥ 0 for all

v ∈ Kψ,

(17)

13

0 (D), H −1(D)) is a
where ι∗ is the adjoint operator to ι, H −1(D) := H 1
parameterized elliptic operator, and Kψ := { u ∈ H 1
0 (D) : u ≥ ψ } with ψ ∈ H 1(D) and ψ∂D ≤ 0 is
the obstacle. The set Kψ is nonempty [66, p. 129]. For (y, ξ) ∈ H −1(D) × Ξ, we also consider the
auxiliary parameterized elliptic variational inequality:

0 (D)∗, A : Ξ → L (H 1

ﬁnd u ∈ Kψ :

hA(ξ)u − y, v − uiH −1(D),H 1

0 (D) ≥ 0 for all

v ∈ Kψ.

(18)

If

S(y)(ξ) with y = ι∗z is a solution to (18), then it is a solution to (17).
We assume that A : Ξ → L (H 1
e

0 (D), H −1(D)) is uniformly measurable, that is, there ex-
0 (D), H −1(D)) of simple mappings such that Ak(ξ) → A(ξ) in
0 (D), H −1(D)) as k → ∞ for each ξ ∈ Ξ. Moreover, we assume that there exists constants
0 (D),H −1(D)) ≤

ists a sequence Ak : Ξ → L (H 1
L (H 1
κmin, κmax > 0 such that for each ξ ∈ Ξ, A(ξ) is κmin-strongly monotone and kA(ξ)kL (H 1
κmax. Under these conditions, the auxiliary variational inequality (17) has a unique solution
S(z)(ξ) for each (y, ξ) ∈ H −1(D) × Ξ and
S(·)(ξ) is Lipschitz continuous with Lipschitz con-
stant 1/κmin; cf. [26, Thm. 7.3]. Using results established in [25, p. 180], we can show that
e
0 (D)) for all q ∈ [1, ∞] and y ∈ H −1(D). Combined with the Lipschitz
S(y) ∈ Lq(Ξ, A, P; H 1
continuity, we ﬁnd that
e

0 (D)) is continuous for each q ∈ [1, ∞].

S : H −1(D) → Lq(Ξ, A, P; H 1

e

e

We express (16) in the form given in (9) and verify Assumption 10. The function ℘ deﬁned
L2 (D) is an R-function; see Section 4.2. The operator B := ι∗ is linear and
by ℘(z) := (α/2)kzk2
completely continuous because ι is a compact operator. We deﬁne f : H −1(D) × Ξ → [0, ∞) by
f (y, ξ) := (1/2)kι
0 (D)) → [0, ∞) given by
J (y) := (1/2)kιy − udk2
S,
F : H −1(D) → L2(Ξ, A, P; H 1
0 (D)) is well-deﬁned and continuous. Having veriﬁed Assumption 10,
e
we can apply Theorem 11, which in turn yields the consistency of empirical approximations of (16).

L2(D) is continuous [35, Example 3.2 and Theorem 3.5]. Since F = J ◦

L2(D). The mapping J : L2(Ξ, A, P; H 1

S(y)(ξ) − udk2
e

5 Conclusion

We have seen that consistency results, in particular, norm consistency of empirical minimizers
for nonconvex, risk-averse stochastic optimization problems involving inﬁnite-dimensional decision
spaces are in fact available. The central property on which the entire discussion depends is the
ability to draw compactness from the structure of the objective function. As the examples illustrate,
this is much more the rule rather than the exception. In fact, even in examples such as topology
optimization, [8], where the decision variable enters the PDE in a nonlinear fashion, the required
use of either ﬁlters or other regularization strategies, see e.g. [39, 65], also provides compactness.

There remain many open challenges. These include applications to multistage or dynamic
problems, large deviation results for optimal values and solutions, and central limit theorems. In
many instances, the known techniques are limited by nonsmoothness of the risk measure R and
the inﬁnite-dimensional decision spaces. However, the main result in this text, Theorem 11, is a
ﬁrst major step and an essential tool towards verifying the convergence of numerical optimization
methods that make use of empirical approximations.

A Law of Large Numbers for Risk Functionals

We generalize the epigraphical law of large numbers for law invariant risk function established
in [61] to allow for random lower semicontinuous functions deﬁned on complete, separable metric
spaces. The proof provided in [61] generalizes to this more general setting with only a few notational

14

changes needed. Nevertheless, we verify the liminf-condition of epiconvergence using ideas from the
proof of Proposition 7.1 in [55]. The limsup-condition is established as in [61].

Assumption 20. Let (Ω, F, P ) be nonatomic, complete probability space, and let (Θ, Σ) be a
measurable space. Let ζ : Ω → Θ be a random element and ζ 1, ζ 2, . . . deﬁned on (Ω, F, P ) be
independent identically distributed Θ-valued random elements each having the same distribution
as that of ζ. Let (V, dV ) be a complete, separable metric space and let 1 ≤ p < ∞.
(R1) The function Ψ : V × Θ → R is random lower semicontinuous.
(R2) For each v ∈ V , Ψv(·) := Ψ(v, ζ(·)) ∈ Lp(Ω, F, P ).
(R3) For each ¯v ∈ V , there exists a neighborhood V¯v of ¯v and a random variable h ∈ Lp(Ω, F, P )

such that Ψ(v, ζ(·)) ≥ h(·) for all v ∈ V¯v.

Theorem 21 is as Theorem 3.1 in [61] but allows for complete, separable metric spaces V instead
of Rn. Let ρ : Lp(Ω, F, P ) → R be a law invariant risk measure. Let v ∈ V and let ˆHv,N = ˆHv,N,ω
be the empirical distribution function of Ψ(v, ζ 1), . . . , Ψ(v, ζ N ). We deﬁne ˆφN : V × Ω → R and
φ : V → R by ˆφN (v) := ˆφN (v, ω) := ρ( ˆHv,N ) and φ(v) := ρ(Ψv).

Theorem 21. If Assumption 20 holds and ρ : Lp(Ω, F, P ) → R is a law invariant, convex risk
measure, then φ is lower semicontinuous and ﬁnite-valued, and ˆφN epiconverges to φ w.p. 1 as
N → ∞.

Before establishing Theorem 21, we formulate a law of large numbers with respect to Mosco-

epiconvergence.

Corollary 22. Let Y0 ⊂ Y be a closed subset of a Banach space Y and let W0 be a closed, convex
subset of a reﬂexive, separable Banach space W . Let the hypotheses of Theorem 21 hold with
V = Y0. Suppose that B : W → Y is linear and completely continuous with B(W0) ⊂ Y0. Then
ˆφN ◦ B : W0 → R Mosco-epiconverges to φ ◦ B : W0 → R w.p. 1 as N → ∞.

Proof. Theorem 21 ensures that ˆφN epiconverges to φ w.p. 1 as N → ∞. Since W0 deﬁnes a
complete separable metric space and B is continuous, Theorem 21 further ensures that ˆφN ◦ B
epiconverges to φ ◦ B w.p. 1 as N → ∞. Combined with Proposition 9 and the complete continuity
of B, we conclude that ˆφN ◦ B Mosco-epiconverges to φ ◦ B w.p. 1 as N → ∞.

As already mentioned, the proof of Theorem 21 presented in [61, Thm. 3.1] for V = Rn can be
generalized to the above setting without much eﬀort. A key result for establishing Theorem 21 is
Theorem 23. To formulate Theorem 23, let X : Ω → R be a random variable and X1, X2, . . . deﬁned
on (Ω, F, P ) be independent identically distributed real-valued random variables each having the
same distribution as that of X. Moreover, let ˆHN be the empirical distribution function of the
sample X1, . . . , XN .

Theorem 23 (see [61, Thm. 2.1] and [63, Thm. 9.65]). If (Ω, F, P ) is complete and nonatomic,
1 ≤ p < ∞, and ρ : Lp(Ω, F, P ) → R is a law invariant, convex risk measure, then ρ( ˆHN ) converges
to ρ(X) w.p. 1 as N → ∞.

Proof of Theorem 21. The fact that φ is ﬁnite-valued and lower semicontinuous can be established
as in the proof of Theorem 3.1 in [61]. To establish the epiconvergence, we make use of the
constructions made in the proof of Proposition 7.1 in [55]. Proposition 7.1 in [55] establishes
epiconvergence in case that ρ(·) = E[·], but without assuming (Ω, F, P ) be nonatomic. Let E ⊂ V

15

be a countable dense subset of V and Q+ be the nonnegative rational numbers. For v ∈ E and
r ∈ Q+, we deﬁne πv,r : Ω → R by

πv,r(ω) := inf

v′∈B(v,r)

Ψ(v, ζ(ω))

if

r > 0 and πv,0(ω) := Ψ(v, ζ(ω))

if

r = 0,

where B(v, r) := {v′ ∈ V : dV (v′, v) < r}. Theorem 3.4 in [33], (R1), and (R2) ensure that πv,r is
a real-valued random variable for each v ∈ V and r ∈ [0, ∞). Combined with (R3), we ﬁnd that
for every v ∈ E, there exists a neighborhood Vv of v and rv ∈ (0, ∞) such that

B(v, rv) ⊂ Vv

and πv,r ∈ Lp(Ω, F, P )

for all

r ∈ [0, rv].

Let ˆρN (πv,r, ω) be the empirical estimate of ρ(πv,r) based on the same sample as that used to
estimate ˆρ(·), that is, ˆρN (πv,r, ω) := ρ( ˜Hv,N,ω), where ˜Hv,N,ω is the empirical distribution function
of the sample πv,r(ζ 1), . . . , πv,r(ζ N ). For every v ∈ E and r ∈ [0, rv] ∩ Q+, Theorem 23 ensures that
ˆρN (πv,r) → ρ(πv,r) w.p. 1 as N → ∞. Since {(v, r) : r ∈ [0, rv] ∩ Q+ : v ∈ E} is countable, there
exists Ω0 ⊂ Ω with Ω0 ∈ F and P (Ω0) = 1 such that

ˆρN (πv,r, ω) → ρ(πv,r) as N → ∞ for all ω ∈ Ω0

and r ∈ [0, rv] ∩ Q+, v ∈ E.

Now, we verify the liminf-condition of epiconvergence. Fix v ∈ V and ﬁx vN → v as N → ∞.
There exists ¯N (ℓ) ∈ N, zℓ ∈ E and rℓ ∈ [0, rv] ∩ Q+, ℓ ∈ N such that zℓ → v, rℓ → 0,

B(zℓ+1, rℓ+1) ⊂ B(zℓ, rℓ),

and vN ∈ B(zℓ, rℓ)

for all N ≥ ¯N (ℓ), ℓ ∈ N.

Fix ℓ ∈ N. For all N ≥ ¯N (ℓ) and ω ∈ Ω0, Theorem 6.50 in [63] when combined with the fact

that ρ is law invariant and monotone, and vN ∈ B(zℓ, rℓ) ensures

ˆφN (vN , ω) ≥ ˆρN (πzℓ,rℓ, ω ˆφN (vN , ω) ≥ ˆρN (πzℓ,rℓ, ω).

(19)

Moreover, for all ω ∈ Ω0,

ˆρN (πzℓ,rℓ, ω) → ρ(πzℓ,rℓ) as N → ∞.
Since B(zℓ+1, rℓ+1) ⊂ B(zℓ, rℓ), we have πzℓ,rℓ ≤ πzℓ+1,rℓ+1. For all ℓ ∈ N and for all ω ∈ Ω,
the lower semicontinuity of Ψ(·, ζ(ω)) (see (R1)) ensures πzℓ,rℓ(ω) ր πv,0(ω) = Ψ(v, ζ(ω)) as
ℓ → ∞ [33, p. 432]. Thus πv,0 − πz1,r1 ≥ πv,0 − πzℓ+1,rℓ+1 ≥ 0 for all ℓ ∈ N. Consequently,
|πv,0 − πz1,r1|p ≥ |πv,0 − πzℓ+1,rℓ+1|p. Since πz1,r1, πv,0 ∈ Lp(Ω, F, P ), the dominated convergence
theorem implies πzℓ,rℓ → πv,0 as ℓ → ∞ in Lp(Ω, F, P ). Using the fact that the risk measure ρ is
real-valued and convex, it follows that ρ is continuous [56, Cor. 3.1] and monotone. Consequently,
ρ(πzℓ,rℓ) ր ρ(πv,0) = φ(v) as ℓ → ∞. Combined with (19) and (20), we ﬁnd that

(20)

lim inf
N→∞

ˆφN (vN , ω) ≥ φ(v).

Now, we verify the limsup-condition of epiconvergence using the arguments in [61]. Since
φ is deﬁned on a separable metric space, ﬁnite-valued and lower semicontinuous, there exists a
countable set D ⊂ V such that for each v ∈ V , there exists a sequence (vk) ⊂ D such that vk → v
and φ(vk) → φ(v) as k → ∞ [69, Lem. 3]. Since D is countable, Theorem 23 ensures the existence
of Ω1 ⊂ Ω with Ω1 ∈ F and P (Ω1) = 1 such that for each v ∈ D and all ω ∈ Ω1, we have
ˆφN (v, ω) → φ(v). Fix v ∈ V and let (vk) ⊂ D be a sequence such that vk → v and φ(vk) → φ(v) as
k → ∞. We now proceed with a diagonalization argument (see, e.g., Corollary 1.16 or 1.18 in [3]).
For each k ∈ N and every ω ∈ Ω1, we have ˆφN (vk, ω) → φ(vk) as N → ∞. Moreover φ(vk) → φ(v)
as k → ∞. Consequently, for each ω ∈ Ω1, there exists a mapping N ∋ N 7→ kω(N ) ∈ N increasing
to ∞ such that ˆφN (vkω(N ), ω) → φ(v) as N → ∞. Since vk → v as k → ∞, we further have
vkω(N ) → v as N → ∞ for each ω ∈ Ω1.

16

B Law Invariance of AVaRε

β and σε

β

β deﬁned in (11) and σε

Both AVaRε
β given in (12) are optimized certainty equivalents in the sense
of [7], i.e. they are fully characterized by convex, continuous scalar regret functions vepi, vs : R → R
such that for each X ∈ L2(Ω, F, P),

AVaRε

β[X] = inf
t∈R
σε
β[X] = inf
t∈R

{ t + 1
1−β
{ t + 1
1−β

E[vepi(X − t)] },

E[vs(X − t)] }.

The explicit form for vepi can be found in Example 2 in [36] and vs(·) = (·)+

ε for ﬁxed ε > 0.

It is not essential for the underlying probability space to be nonatomic for the law invariance
of these functionals. Indeed, start by letting v : R → R be continuous and hence, measurable. For
each X ∈ Lp(Ω, F, P ) and t ∈ R, let v(X − t) be integrable indendently of X and t, which is the
case for both vepi and vs. Let X1, X2 ∈ Lp(Ω, F, P ) be distributionally equivalent with respect to
P . Since the distribution functions of X1 and X2 are equal and each distribution function uniquely
determines a probability law on R [10, Thm. 12.4], it holds that P ◦ X −1
2 . For all t ∈ R,
we have

1 = P ◦ X −1

E[v(X1 − t)] =

=

ZΩ

ZR

v(X1(ω) − t)dP (ω) =

v(x − t)dP ◦ X −1

1 (x)

ZR

v(x − t)dP ◦ X −1

2 (x) =

ZΩ

v(X2(ω) − t)dP (ω) = E[v(X2 − t)].

Hence, AVaRε
β and σε
certainty equivalents are law invariant.

β are law invariant. As a result, a large class of risk measures/optimized

References

[1] C. D. Aliprantis and K. C. Border, Inﬁnite Dimensional Analysis: A Hitchhiker’s Guide,

Springer, Berlin, 3rd ed., 2006, https://doi.org/10.1007/3-540-29587-9.

[2] P. Artzner, F. Delbaen, J.-M. Eber, and D. Heath, Coherent measures of risk, Math.

Finance, 9 (1999), pp. 203–228, https://doi.org/10.1111/1467-9965.00068.

[3] H. Attouch, Variational Convergence for Functions and Operators, Applicable Mathematics

Series, Pitman, Boston, MA, 1984.

[4] H. Attouch and R. J.-B. Wets, Epigraphical analysis, Ann. Inst. H. Poincar´e Anal. Non

Lin´eaire, 6 (1989), pp. 73–100, https://doi.org/10.1016/S0294-1449(17)30036-7.

[5] J.-P. Aubin and H. Frankowska, Set-Valued Analysis, Mod. Birkh¨auser Class., Springer,

Boston, MA, 2009, https://doi.org/10.1007/978-0-8176-4848-0.

[6] F. Beiser, B. Keith, S. Urbainczyk, and B. Wohlmuth, Adaptive sampling strategies for
risk-averse stochastic optimization with constraints, arXiv preprint arXiv:2012.03844, (2020),
http://arxiv.org/abs/2012.03844.

[7] A. Ben-Tal and M. Teboulle, An old-new concept

risk mea-
sures: The optimized certainty equivalent, Math. Finance, 17 (2007), pp. 449–476,
https://doi.org/10.1111/j.1467-9965.2007.00311.x.

convex

of

17

[8] M. P. Bendsøe and O. Sigmund, Topology Optimization: Theory, Methods and Applicatons,

Springer, Berlin, 2003, https://doi.org/10.1007/978-3-662-05086-6.

[9] J. Berner, P. Grohs, and A. Jentzen, Analysis of the generalization error: empirical risk
minimization over deep artiﬁcial neural networks overcomes the curse of dimensionality in the
numerical approximation of Black-Scholes partial diﬀerential equations, SIAM J. Math. Data
Sci., 2 (2020), pp. 631–657, https://doi.org/10.1137/19M125649X.

[10] P. Billingsley, Probability and Measure, Wiley Ser. Probab. Stat., John Wiley & Sons,

Hoboken, NJ, 2012.

[11] V.

I.

Bogachev,

Measure

Theory,

Springer,

Berlin,

2007,

https://doi.org/10.1007/978-3-540-34514-5.

[12] J. F. Bonnans and A. Shapiro, Perturbation Analysis of Optimization Problems, Springer
Ser. Oper. Res., Springer, New York, 2000, https://doi.org/10.1007/978-1-4612-1394-9.

[13] J. M. Borwein and A. S. Lewis, Strong rotundity and optimization, SIAM J. Optim., 4

(1994), pp. 146–158, https://doi.org/10.1137/0804008.

[14] J. M. Borwein and J. D. Vanderwerff, Convex functions: constructions, characteri-
zations and counterexamples, Encyclopedia Math. Appl. 109, Cambridge University Press,
Cambridge, 2010, https://doi.org/10.1017/CBO9781139087322.

[15] S. Curi, K. Y. Levy,

pling
for
tion Processing
can,
https://proceedings.neurips.cc/paper/2020/file/0b6ace9e8971cf36f1782aa982a708db-Paper.pdf.

sam-
Informa-
Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Bal-
2020, pp. 1036–1047,

and A. Krause, Adaptive
in Advances

eds., vol. 33, Curran Associates,

S.
risk-averse

Jegelka,
learning,

and H. Lin,

in Neural

stochastic

Inc.,

[16] D. Davis and D. Drusvyatskiy, Graphical convergence of
convex optimization and learning, Math. Oper. Res.,
https://doi.org/10.1287/moor.2021.1126.

47

subgradients
pp.

(2022),

in non-
209–231,

[17] M. X. Dong and R. J.-B. Wets, Estimating density functions:
12 (2000),

J. Nonparametr. Statist.,

maximum likelihood approach,
https://doi.org/10.1080/10485250008832822.

a constrained
pp. 549–595,

[18] R. M. Dudley, Real Analysis and Probability,

vol.

74

in Advanced Mathematics, Cambridge University Press, Cambridge,

ies
https://doi.org/10.1017/CBO9780511755347.

of Cambridge Stud-
2002,

[19] R. M. Dudley and R. Norvaiˇsa, Concrete Functional Calculus, Springer Monographs in

Mathematics, Springer, New York, 2011, https://doi.org/10.1007/978-1-4419-6950-7.

[20] J. Dupaˇcov´a and R. J.-B. Wets, Asymptotic behavior of statistical estimators and of
optimal solutions of stochastic optimization problems, Ann. Statist., 16 (1988), pp. 1517–1549,
https://doi.org/10.1214/aos/1176351052.

[21] H. F¨ollmer and A. Schied, Convex measures of risk and trading constraints, Finance

Stoch., 6 (2002), pp. 429–447, https://doi.org/10.1007/s007800200072.

18

[22] S. Garreis, T. M. Surowiec, and M. Ulbrich, An interior-point approach for solving risk-
averse PDE-constrained optimization problems with coherent risk measures, SIAM J. Optim.,
31 (2021), pp. 1–19, https://doi.org/10.1137/19M125039X.

[23] S. Garreis and M. Ulbrich, A fully adaptive method for the optimal control of semilin-
ear elliptic PDEs under uncertainty using low-rank tensors, Preprint, Technische Universit¨at
M¨unchen, M¨unchen, 2019, http://go.tum.de/204409.

[24] C. Geiersbach and T. Scarinci, Stochastic proximal gradient methods for non-
convex problems in Hilbert spaces, Comput. Optim. Appl., 78 (2021), pp. 705–740,
https://doi.org/10.1007/s10589-020-00259-y.

[25] J. Gwinner, B. Jadamba, A. A. Khan, and F. Raciti, Uncertainty Quantiﬁcation in
Variational Inequalities: Theory, Numerics, and Applications, CRC, Boca Raton, FL, 2022,
https://doi.org/10.1201/9781315228969.

[26] L. Hertlein, A.-T. Rauls, M. Ulbrich, and S. Ulbrich, An inexact bundle method and
subgradient computations for optimal control of deterministic and stochastic obstacle prob-
lems, in Non-Smooth and Complementarity-Based Distributed Parameter Systems: Simula-
tion and Hierarchical Optimization, M. Hinterm¨uller, R. Herzog, C. Kanzow, M. Ulbrich,
and S. Ulbrich, eds., Internat. Ser. Numer. Math. 172, Birkh¨auser, Cham, 2022, pp. 467–497,
https://doi.org/10.1007/978-3-030-79393-7_19.

[27] E. Hille and R. S. Phillips, Functional Analysis and Semi-Groups, Colloq. Publ. 31, AMS,

Providence, RI, 1974, https://doi.org/10.1090/coll/031.

[28] M. Hoffhues, W. R¨omisch, and T. M. Surowiec, On quantitative stability in
inﬁnite-dimensional optimization under uncertainty, Optim. Lett., 15 (2021), pp. 2733–2756,
https://doi.org/10.1007/s11590-021-01707-2.

[29] P. J. Huber, The behavior of maximum likelihood estimates under nonstandard conditions,
in Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability,
Volume 1: Statistics, L. M. Le Cam and J. Neyman, eds., Berkeley, CA, 1967, University of
California Press, pp. 221–233.

[30] A. Juditsky, J. Kwon, and E. Moulines, Unifying mirror descent and dual averaging,

Math. Program., (2022), https://doi.org/10.1007/s10107-022-01850-3.

[31] A. J. King and R. T. Rockafellar, Asymptotic theory for solutions in statisti-
cal estimation and stochastic programming, Math. Oper. Res., 18 (1993), pp. 148–162,
https://doi.org/10.1287/moor.18.1.148.

[32] A. J. Kleywegt, A. Shapiro, and T. Homem-de Mello, The sample average approxi-
mation method for stochastic discrete optimization, SIAM J. Optim., 12 (2002), pp. 479–502,
https://doi.org/10.1137/S1052623499363220.

[33] L. A. Korf and R. J.-B. Wets, Random LSC Functions: An Ergodic Theorem, Math.
Oper. Res., 26 (2001), pp. 421–445, https://doi.org/10.1287/moor.26.2.421.10548.

[34] D. P. Kouri and T. M. Surowiec, Risk-averse PDE-constrained

optimiza-
tion using the conditional value-at-risk, SIAM J. Optim., 26 (2016), pp. 365–396,
https://doi.org/10.1137/140954556.

19

[35] D. P. Kouri and T. M. Surowiec, Existence and optimality conditions for risk-averse
PDE-constrained optimization, SIAM/ASA J. Uncertain. Quantif., 6 (2018), pp. 787–815,
https://doi.org/10.1137/16M1086613.

[36] D. P. Kouri and T. M. Surowiec, Epi-regularization of risk measures, Math. Oper. Res.,

45 (2020), pp. 774–795, https://doi.org/10.1287/moor.2019.1013.

[37] D. P. Kouri and T. M. Surowiec, Risk-averse optimal control of semilinear elliptic PDEs,

ESAIM Control. Optim. Calc. Var., 26 (2020), https://doi.org/10.1051/cocv/2019061.

[38] P. Lachout, E. Liebscher, and S. Vogel, Strong convergence of estimators as ǫn-
minimisers of optimisation problems, Ann. Inst. Statist. Math., 57 (2005), pp. 291–313,
https://doi.org/10.1007/BF02507027.

[39] B. S. Lazarov and O. Sigmund, Filters in topology optimization based on Helmholtz-
type diﬀerential equations, Internat. J. Numer. Methods Engrg., 86 (2011), pp. 765–781,
https://doi.org/10.1002/nme.3072.

[40] J. Liu, Y. Cui, and J.-S. Pang, Solving nonsmooth and nonconvex compound stochas-
tic programs with applications to risk measure minimization, Math. Oper. Res., (2022),
https://doi.org/10.1287/moor.2021.1247.

[41] S. Mei, Y. Bai, and A. Montanari, The landscape of empirical risk for nonconvex losses,

Ann. Statist., 46 (2018), pp. 2747–2774, https://doi.org/10.1214/17-AOS1637.

[42] J. Milz, Consistency of Monte Carlo estimators for risk-neutral PDE-constrained optimiza-

tion, arXiv preprint arXiv:2204.04809, (2022), http://arxiv.org/abs/2204.04809.

[43] J. Milz, Reliable error estimates for optimal control of linear elliptic PDEs with random
inputs, arXiv preprint arXiv:2206.09160, (2022), https://arxiv.org/abs/2206.09160.

[44] J. Milz, Sample average approximations of strongly convex stochastic programs in Hilbert

spaces, Optim. Lett., (2022), https://doi.org/10.1007/s11590-022-01888-4.

[45] U. Mosco, Convergence of convex sets and of solutions of variational inequalities, Advances
in Mathematics, 3 (1969), pp. 510–585, https://doi.org/10.1016/0001-8708(69)90009-7.

[46] N. H. Nelsen and A. M. Stuart, The random feature model

for input-output
maps between Banach spaces, SIAM J. Sci. Comput., 43 (2021), pp. A3212–A3243,
https://doi.org/10.1137/20M133957X.

[47] G. Ch. Pflug, Asymptotic stochastic programs, Math. Oper. Res., 20 (1995), pp. 769–789,

https://doi.org/10.1287/moor.20.4.769.

[48] G. Ch. Pflug, Stochastic optimization and statistical

in Stochastic Pro-
gramming, Handbooks Oper. Res. Manag. Sci. 10, Elsevier, 2003, pp. 427 – 482,
https://doi.org/10.1016/S0927-0507(03)10007-2.

inference,

[49] Z. Qi, Y. Cui, Y. Liu, and J.-S. Pang, Asymptotic properties of stationary solutions
of coupled nonconvex nonsmooth empirical risk minimization, Math. Oper. Res., (2021),
https://doi.org/10.1287/moor.2021.1198.

20

[50] S. T. Rachev and W. R¨omisch, Quantitative stability in stochastic program-
the method of probability metrics, Math. Oper. Res., 27 (2002), pp. 792–818,

ming:
https://doi.org/10.1287/moor.27.4.792.304.

[51] S. M. Robinson, Analysis of sample-path optimization, Math. Oper. Res., 21 (1996), pp. 513–

528, https://doi.org/10.1287/moor.21.3.513.

[52] W. R¨omisch and R. Schultz, Distribution sensitivity in stochastic programming, Math.

Program., 50 (1991), pp. 197–226, https://doi.org/10.1007/BF01594935.

[53] W. R¨omisch and T. M. Surowiec, Asymptotic properties of Monte Carlo methods in elliptic
PDE-constrained optimization under uncertainty, arXiv preprint arXiv:2106.06347, (2021),
https://arxiv.org/abs/2106.06347.

[54] J. O. Royset, Approximations of semicontinuous functions with applications to stochas-
tic optimization and statistical estimation, Math. Program., 184 (2020), pp. 289–318,
https://doi.org/10.1007/s10107-019-01413-z.

[55] J. O. Royset and R. J.-B. Wets, Variational analysis of constrained M-estimators, Ann.

Statist., 48 (2020), pp. 2759–2790, https://doi.org/10.1214/19-AOS1905.

[56] A. Ruszczy´nski and A. Shapiro, Optimization of Convex Risk Functions, Math. Oper.

Res., 31 (2006), pp. 433–452, https://doi.org/10.1287/moor.1050.0186.

[57] A. Shapiro, Asymptotic properties of statistical estimators in stochastic programming, Ann.

Statist., 17 (1989), pp. 841–858, https://doi.org/10.1214/aos/1176347146.

[58] A. Shapiro, Asymptotic analysis of stochastic programs, Ann. Oper. Res., 30 (1991), pp. 169–

186, https://doi.org/10.1007/BF02204815.

[59] A. Shapiro, Asymptotic behavior of optimal solutions in stochastic programming, Math. Oper.

Res., 18 (1993), pp. 829–845, https://doi.org/10.1287/moor.18.4.829.

[60] A. Shapiro, Statistical Inference of Stochastic Optimization Problems,

in Probabilis-
tic Constrained Optimization: Methodology and Applications, S. P. Uryasev,
ed.,
Probabilistic Constrained Optimization, Springer, Boston, MA, 2000, pp. 282–307,
https://doi.org/10.1007/978-1-4757-3150-7_16.

[61] A. Shapiro, Consistency of sample estimates of risk averse stochastic programs, J. Appl.

Probab., 50 (2013), pp. 533–541, https://doi.org/10.1239/jap/1371648959.

[62] A. Shapiro, On Kusuoka representation of law invariant risk measures, Math. Oper. Res., 38

(2013), pp. 142–152, https://doi.org/10.1287/moor.1120.0563.

[63] A. Shapiro, D. Dentcheva, and A. Ruszczy´nski, Lectures on Stochastic Programming:
Modeling and Theory, MOS-SIAM Ser. Optim., SIAM, Philadelphia, PA, 3rd ed., 2021,
https://doi.org/10.1137/1.9781611976595.

[64] A. Shapiro and A. Nemirovski, On complexity of stochastic programming problems,
in Continuous Optimization: Current Trends and Modern Applications, V. Jeyakumar
and A. Rubinov, eds., Appl. Optim. 99, Springer, Boston, MA, 2005, pp. 111–146,
https://doi.org/10.1007/0-387-26771-9_4.

21

[65] O.

Sigmund

and K. Maute,

Topology

tural
https://doi.org/10.1007/s00158-013-0978-6.

and Multidisciplinary

Optimization,

optimization
48

(2013),

approaches,
pp.

Struc-
1031–1055,

[66] T. M. Surowiec, Numerical optimization methods for the optimal control of elliptic vari-
ational inequalities, in Frontiers in PDE-Constrained Optimization, H. Antil, D. P. Kouri,
M.-D. Lacasse, and D. Ridzal, eds., IMA Vol. Math. Appl. 163, Springer, New York, NY,
2018, pp. 123–170, https://doi.org/10.1007/978-1-4939-8636-1_4.

[67] F. Wechsung, A. Giuliani, M. Landreman, A. J. Cerfon, and G. Stadler, Single-
stage gradient-based stellarator coil design: stochastic optimization, Nuclear Fusion, 62 (2022),
p. 076034, https://doi.org/10.1088/1741-4326/ac45f3.

[68] E. Zeidler, Nonlinear Functional Analysis and its Applications II/B: Nonlinear Monotone
Operators, Springer, New York, 1990, https://doi.org/10.1007/978-1-4612-0981-2.

[69] M. Zervos, On the epiconvergence of stochastic optimization problems, Math. Oper. Res., 24

(1999), pp. 495–508, https://doi.org/10.1287/moor.24.2.495.

22

