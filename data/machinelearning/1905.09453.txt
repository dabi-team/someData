9
1
0
2

y
a
M
3
2

]

G
L
.
s
c
[

1
v
3
5
4
9
0
.
5
0
9
1
:
v
i
X
r
a

Ensemble Model Patching: A Parameter-Efﬁcient
Variational Bayesian Neural Network

Oscar Chang†, Yuling Yao‡, David Williams-King†, Hod Lipson†

†Department of Computer Science,

‡ Department of Statistics

Columbia University
{oscar.chang, yy2619, hod.lipson}@columbia.edu, dwk@cs.columbia.edu

Abstract

Two main obstacles preventing the widespread adoption of variational Bayesian
neural networks are the high parameter overhead that makes them infeasible on
large networks, and the difﬁculty of implementation, which can be thought of as
“programming overhead." MC dropout [1] is popular because it sidesteps these
obstacles. Nevertheless, dropout is often harmful to model performance when
used in networks with batch normalization layers [2], which are an indispensable
part of modern neural networks. We construct a general variational family for
ensemble-based Bayesian neural networks that encompasses dropout as a special
case. We further present two speciﬁc members of this family that work well
with batch normalization layers, while retaining the beneﬁts of low parameter
and programming overhead, comparable to non-Bayesian training. Our proposed
methods improve predictive accuracy and achieve almost perfect calibration on a
ResNet-18 trained with ImageNet.

1

Introduction

As deep learning becomes ubiquitous in safety-critical applications like autonomous driving and
medical imaging, it is important for neural network practitioners to quantify the degree of belief they
have in their predictions [3–5]. Unlike conventional deep neural networks which are poorly calibrated
[6–9], Bayesian neural networks learn a probability distribution over parameters. This design enables
uncertainty estimation, allows for better-calibrated probability prediction, and reduces overﬁtting.

However, exact posterior inference for deep Bayesian neural networks is intractable in general, so
approximate methods like variational inference are often used [1, 10–15]. Unfortunately, most of the
proposed variational methods still require signiﬁcant (i.e. ≥100%) parameter overhead and do not
scale well to modern neural networks with millions of parameters. For example, a mean-ﬁeld Gaussian
[10, 12] doubles the parameter use (by learning both means and variances). These techniques also
incur signiﬁcant programming overhead since the programmer must perform extensive changes to
their neural network architecture to make it Bayesian. For example, [11, 12] require a modiﬁcation to
the backpropagation algorithm, while [13, 14, 16] involve complicated weight-sampling techniques.

High parameter overhead is an important concern, as deep learning models already utilize available
hardware resources to the fullest, often maxing out GPU memory usage [17]. In 2015, the highest
end consumer-grade GPU had 12GB of memory; by 2018, this number had doubled to 24GB. This
means that in the period 2015-2018, a programmer could only have Bayesianized state-of-the-art
neural networks from prior to 2015. Any newer designs, once their parameter use had been doubled,
would not ﬁt into her GPU’s memory. With GPU memory capacity doubling approximately every 3.2
years (Figure 1)—an eternity in the rapidly progressing ﬁeld of deep learning—the programmer’s
Bayesianized networks will be up to three years behind the state of the art.

Preprint. Under review.

 
 
 
 
 
 
Table 1: Variational methods in the literature incur signiﬁcant
parameter overhead. We computed these parameter counts
based on recommended hyper-parameter settings.

Variational
Method

Mean-Field
Gaussian [10, 12]
PBP [11]

MNFG [14]

Dropout [1, 15]

Ensemble Model
Patching (ours)

ResNet-18
Parameters
[Overhead]

PyramidNet
Parameters
[Overhead]

23.4M
[100%]
23.4M
[100%]
3, 510M
[29900%]
11.7M
[0.00%]
13.8M
[17.9%]

57.0M
[100%]
57.0M
[100%]
8, 550M
[29900%]
28.5M
[0.00%]
28.8M
[1.10%]

Figure 1: GPU RAM capacity grows exponen-
tially, doubling every 3.2 years for the last 15
years. We scraped the data for 1599 GPUs and
ﬁtted a log-linear model; details in Appendix E.

We survey in Table 1 the parameter use of several existing Variational Bayesian Neural Network
(VBNN) methods for ResNet-18 [18] and PyramidNet [19], among which dropout is the only method
that does not add a signiﬁcant (≥100%) parameter overhead. Nonetheless, in the presence of batch
normalization, a mainstay of modern deep networks, dropout layers have been found to be either
redundant [20], or downright harmful to model performance due to shifts in variance [2].

In this paper, our contributions are two-fold. First, we construct a general variational family for
ensemble-based Bayesian neural networks. This uniﬁed family extends and bridges the Bayesian
interpretation for both implicit and explicit ensembles, where methods like dropout, DropConnect
[21] and explicit ensembling [9, 22] can be viewed as special cases. We show the low parameter
overhead and large ensemble sizes for several implicit ensemble methods, thus suggesting their
suitability for use in large neural networks.

Second, we present two novel variational distributions (Ensemble Model Patching) for implicit ensem-
bles. They are better alternatives to dropout especially in batch-normalized networks. Our proposed
methods outperform MC dropout and non-Bayesian networks on test accuracy, probability calibration,
and robustness against common image corruptions. The parameter overhead and computational cost
are nearly the same as in non-Bayesian training, making Bayesian networks affordable on large
scale datasets and modern deep residual networks with hundreds of layers. To our knowledge, we
are the ﬁrst to scale a Bayesian neural network to the ImageNet dataset, achieving almost perfect
calibration. Our methods are both scalable and easy to implement, serving as one-to-one replacements
for particular layers in a neural network without further changes to its architecture.

The remainder of the paper is organized as follows. We propose a general variational distribution
for ensemble-based VBNNs in Section 2, before introducing Ensemble Model Patching and its
two variants in Section 3. We discuss related work in Section 4, validate our proposed methods
experimentally in Section 5, and ﬁnally conclude our ﬁndings in Section 6. For an introduction to
technical background, we refer readers to Appendix A. We provide derivation details and theoretical
proofs in Appendix B. More implementation details can be found in Appendices C and D.

2 Ensemble-Based Variational Bayesian Neural Networks

Implicit vs Explicit Ensembles A classical ensemble, which we call an explicit ensemble in
this paper, involves ﬁtting several different models and combining the output using a method like
averaging or majority vote. An explicit ensemble with K components thus has to maintain K
models, which is inefﬁcient. Implicit ensembles, by contrast, are more efﬁcient because the different
components arise through varying parts of each model rather than the whole model. For example,
the Bernoulli-Gaussian model [23], which can be considered a conceptual predecessor to dropout,
maintains D products of a Bernoulli and a Gaussian random variable. Thus, each realization of the
Bernoulli results in a different model, leading implicitly to 2D mixture components.

Can we unify these two approaches with a continuous expansion [24]? Yes, assuming that all
the ensemble components have the same network architecture, we can indeed model both explicit

2

20042008201220162020Year1664256102440961638465536GPU RAM Size (MB)GPU RAM Capacity Doubles Every 3.2 YearsTITAN X MaxwellTITAN RTXTesla P40Tesla V100 32GBand implicit neural network ensembles as variational Bayes. This general variational family, which
includes MC dropout as a speciﬁc case, lends a Bayesian interpretation to both implicit ensembles like
DropConnect [21] and also explicit ensembles like Lakshminarayanan et al. [9]’s Deep Ensembles.

A General Variational Family In Equation (1), we construct a general variational family with
a factorial distribution over mixtures of Gaussians for both the network’s weights W = {Wi}L
i=1
and biases b = {bj}L
j=1, where i, j are layer indices, L is the number of layers, Ki is the number
of mixture components in the i-th layer, and Hi is the output dimension of the i-th layer. Zi ∈
{1, . . . , Ki}Hi−1×Hi and zj ∈ {1, . . . , Kj}Hj are categorical variables that indicate the assignment
among mixture components and are generated from a Bernoulli or multinoulli distribution with
probability pi or p = {pik}. (cid:12) denotes the Hadamard product and Θ = (W, b) the set of parameters.

Wi =

bj =

Ki(cid:88)

k=1
Kj
(cid:88)

k=1

1[Zi = k] (cid:12) (Mik + σ(cid:15)ik),

(cid:15)ik ∼ N (0, IHi−1×Hi),

1 ≤ i ≤ L,

(1)

1[zj = k] (cid:12) (mjk + σ(cid:15)jk),

(cid:15)jk ∼ N (0, IHj ),

1 ≤ j ≤ L.

Throughout this paper, the categorical mixing probabilities p are ﬁxed to be uniform over all K
ensemble components for simplicity. If Z is marginalized out, the variational distribution is fully
parametrized by (M = {Mik}Ki,L
k=1,j=1). They are centroids of the mixing
components of W and b, and are parameters in each component of the neural network ensemble.

k=1,i=1, m = {mjk}Kj ,L

We show examples of speciﬁc members of this family in Table 2.

Table 2: Special Cases of Variational Distributions for Different Ensemble-Based VBNNs

Number of
Components

Mixing
Assignment

Constants

Variational
Parameters

Dropout

DropConnect

Explicit Ensemble

Ki = 2, Kj = 1

Ki = 2, Kj = 1

Ki = Kj = K ≥ 2

zih1 ∼ Bernoulli(pi)
Zi,h1h2 = zih1

Zi,h1h2 ∼ Bernoulli(pi)

z ∼ Categorical(p)
Zi,h1h2 = z, zj,h3 = z

Mi2 = 0Hi−1×Hi
zj = 1Hj
{pi}L
i=1
{Mi1}L

i=1, {mj1}L

j=1

Mi2 = 0Hi−1×Hi
zj = 1Hj
{pi}L
i=1
{Mi1}L

i=1, {mj1}L

j=1

N/A

p
{Mik}K,L

k=1,i=1, {mjk}K,L

k=1,j=1

Evaluating the Evidence Lower Bound (ELBO)
In all cases, we set the prior on weights
pprior(Θ) to be a zero-centered isotropic Gaussian with precision τ and ρ, and evaluate the Kull-
back–Leibler (KL) divergence in the ELBO as follows (for further details, see Appendix B):

KL (cid:0)q(Θ|M, m)|| pprior(Θ)(cid:1) ≈

L
(cid:88)

Ki(cid:88)

i=1

k=1

τipik
2

where pik =

(cid:26)pi
pk

for Dropout and DropConnect
for Explicit Ensemble

L
(cid:88)

Kj
(cid:88)

||Mik||2

2 +

k=1

j=1
(cid:26)1
pk

p∗
jk =

ρjp∗
jk
2

||mjk||2

2 + Constant. (2)

for Dropout and DropConnect
for Explicit Ensemble

The approximation (2) is reasonable as long as the individual ensemble components do not signiﬁ-
cantly overlap, which will be satisﬁed when the dimension of Θ is high. A practical implementation
is to simply enforce L2 regularization on all the learnable variational parameters.

3 Ensemble Model Patching

Through Bernoulli mixing, dropout ﬁxes a component centered at zero. The zeros are harmful to
a batch-normalized network as shown in [2] and conﬁrmed by our experiments. We thus seek a
method where the weights in each layer mix over several ensemble components and all components
are simultaneously optimized. This is prohibitively expensive if we apply it to the entire network, but
since deep neural networks are over-parametrized, we can target the small fraction of weights (model
patches) that have a disproportionate effect—a technique ﬁrst described by Mudrakarta et al. [25].

3

3.1 Partitioning the Variational Distribution

A model patch [25] refers to a small subset of a neural network, which can be substituted for task-
adapted weights in multi-task and transfer learning. We use Λp to denote the set of indices for
patched layers and Λs for shared layers. This divides the set of all network parameters Θ into patched
parameters Θpatch = ({Wi}i∈Λp , {bj}j∈Λp ) and shared ones Θshared = ({Wi}i∈Λs , {bj}j∈Λs ).
To reduce the parameter overhead, we construct the variational distribution as a product of shared and
patched parameters separately, q(Θ) = q(Θshared)q(Θpatch), where q(Θpatch) is an ensemble-based
distribution from Equation (1), and q(Θshared) is a mean-ﬁeld Gaussian distribution

q(Θshared|M, m) =

(cid:89)

i∈Λs

q(Wi)q(bi) =

(cid:89)

i∈Λs

N (Wi|Mi, σ2I)N (bi|mi, σ2I).

(3)

We further simplify the variational distribution by ﬁxing σ to be small, e.g. machine epsilon.

3.2 Proposed Algorithms: EMP and ECMP

We identify two variational distributions for q(Θpatch) that avoid the hard zeros in dropout while
retaining its low parameter and programming overhead. We ﬁx the number of ensembles in each
layer to be K, and we write the Ensemble Model Patching (EMP) distribution as follows.

Ki = K ≥ 2, zi ∼ Categorical(pi), for i ∈ Λp,
Zi,h1h2 = zi for h1 ∈ [1, Hi−1], h2 ∈ [1, Hi], i ∈ Λp,

zj,h3 = zj for h3 ∈ [1, Hj], j ∈ Λp.

(4)

In (4), given i, the matrix Zi,h1h2 remains the same for all elements (h1, h2). Instead, we can sample
each element in Zi,h1h2, zj,h3 independently. We call this variant Ensemble Cross Model Patching
(ECMP); see Figure 2 for a visual illustration of the distinction between the two methods. In ECMP,

Ki = K ≥ 2, for i ∈ Λp,

Zi,h1h2 ∼ Categorical(pi) for h1 ∈ [1, Hi−1], h2 ∈ [1, Hi], i ∈ Λp,

(5)

zj,h3 ∼ Categorical(pj) for h3 ∈ [1, Hj], j ∈ Λp.

ECMP masks the hidden weight matrix, analogous to DropConnect, but avoids its hard zeros.
Combining (4) or (5) with (1), we obtain the complete variational distribution for q(Θpatch). In both
cases, the variational parameters to optimize over are:

(cid:16)

{Mik}Ki

k=1,i∈Λp

, {mjk}Kj

k=1,j∈Λp

, {Mi}i∈Λs, {mj}j∈Λs

(cid:17)

.

We set the prior on all parameters to be a zero-centered isotropic Gaussian:

pprior(Θ) =

L
(cid:89)

i=1

N (Wi|0, τ −1

i

I)

L
(cid:89)

j=1

N (bj|0, ρ−1

j I).

Applying Equation (2) to the patch layers, we approximate the KL (cid:0)q(Θ | M, m)|| pprior(Θ)(cid:1) (up to
an additive constant) for both EMP and ECMP as

(cid:88)

Ki(cid:88)

i∈Λp

k=1

τipik
2

||Mik||2

2 +

(cid:88)

Kj
(cid:88)

j∈Λp

k=1

ρjpjk
2

||mjk||2

2 +

τi
2

(cid:88)

i∈Λs

||Mi||2

2 +

ρj
2

(cid:88)

j∈Λs

||mj||2
2.

(6)

The ﬁnal loss function (negative ELBO) is then (6) minus the Monte Carlo (MC) estimation of the
likelihood 1/S (cid:80)S
s=1 log p(Y|X, Θs), where Θs is the s-th MC draw of Θ. In our experiments, we
set S = 1 for gradient evaluation, so it becomes the conventional squared error or cross entropy
loss for outcome y. Details for the derivation can be found in Appendix B. Note that the term (6)
resembles L2 regularization in non-Bayesian training, but we are penalizing variational parameters
(M, m) and learning the distribution over Θ = (W, b). Posterior predictive distributions [26] can be
constructed though MC draws of Θ (for details, see Appendix B.3).

We summarize the forward pass for EMP/ECMP in Algorithm 1, and showcase an example imple-
mented in PyramidNet using PyTorch in the Supplementary. We theoretically justify the unbiasedness
of the MC integration and therefore the convergence of Algorithm 1 in Appendix B.4.

4

Algorithm 1: A forward pass with Ensemble Model
Patching. We initialize M and m randomly before the
start of training. Input is x, S is the number of MC
draws; S = 1 for training in our experiments.
ˆy := 0;
for [MC sample index] s ← 1 to S do

ys := x;
for [Layer index] i ← 1 to L do

if i ∈ Λpatch then

Sample mixing assignment Zi;
Sample Θi and thus Layeri through
EMP (4) or ECMP (5) and (1);

else if i ∈ Λshared then

Sample Θi and thus Layeri through (3);

ys := Layeri(ys);

end
ˆy := ˆy + (ys − ˆy)/(s + 1);

end
return ˆy

Figure 2: Assignment of weights using different
schemes. Dropout draws each column of the weight
matrix either from the weight or zero component. EMP
samples each patch layer jointly, thus the weight matrix
is from a single component. ECMP draws each weight
in a layer from an ensemble, thus each element in a
weight matrix can belong to different components.

Table 3: Comparison of ensemble size and resource requirements between Ensemble-Based VBNNs

Method

Effective En-
semble Size

Memory Overhead
(K << L, H)

Parameters
in MLP

Parameters
in ResNet-18

Parameters in
PyramidNet

EMP
ECMP
Dropout
DropConnect
Explicit Ensemble O(K)

O(K L)
O(K(H 2 + LH))
O(K H2+LH ) O(K(H 2 + LH))
O(2LH )
O(2LH2

)

O(LH)
O(LH 2)
O(KLH 2)

1, 129, 100
1, 129, 100
1, 009, 900
1, 009, 900
5, 049, 500

13, 779, 912
13, 779, 912
11, 689, 512
11, 689, 512
58, 447, 560

28, 825, 299
28, 825, 299
28, 511, 307
28, 511, 307
142, 556, 535

3.3 Choice of Model Patch

Following Mudrakarta et al. [25], we recommend that model patches Θpatch be chosen from
parameter-efﬁcient layers that are disproportionately expressive. In most networks, that would
be the normalization or afﬁne layers [27–32] (for example, the batch normalization layers are only
0.1% of the parameters in InceptionV3 [33]). It typically also helps to include the encoder/decoder
layers, which are the input/output layers in most networks, since they interface with the data.

We can compute the layer-wise overhead as follows, assuming N total parameters in the network and
K mixture components in each layer. For Batch Normalization (BN) layers, given a fully connected
layer with size N1 × N2 followed by a BN Layer, the parameter overhead is 2(K − 1)N2 (scales
O(K(cid:112)N/L)). Given a convolution layer with N1 input channels, N2 output channels, kernel width
k, the parameter overhead is 2(K − 1)N2 (scales O(K(cid:112)N/L/k2)). For Input/Output layers, given
a feedforward network with L fully connected layers, where layer i has input dimension Ii and output
dimension Oi, the parameter overhead is (K − 1)((I1 + 1)O1 + (IL + 1)OL). Given a feedforward
network with L convolutional layers having Ii input channels, Oi output channels, kernel width ki,
and no bias for layer i, the parameter overhead is (K − 1)(k2
L ).

LILOL). They scale O( KN

1I1O1 + k2

3.4 Trade-off Between Ensemble Expressiveness and Computational Resources

In theory, any posterior distribution can be approxi-
mated by an inﬁnite mixture of Gaussians, therefore
a larger ensemble size should result in a more expres-
sive variational approximation. However, this may
incur higher, maybe infeasible, memory overhead.

In Table 3, we compare the effective ensemble size
(product of each layer) and resource requirements
between ensemble-based VBNN methods, using a
feedforward architecture that has L fully connected
layers with H hidden units each (total number of

5

Figure 3: We simulate functions from a network
with one single hidden layer. The weights are gen-
erated from a normal prior via dropout or EMP.
EMP expresses ﬁner details. See Appendix D.1.

InputOutputDropoutInputEMPparameters N = O(LH 2)). We assume no in-place modiﬁcation for dropout and DropConnect, as
well as patched BN and output layers for EMP/ECMP, with K mixture components for each layer.
We assume K = 5 to compute parameter counts for an MLP (L = 100, H = 100), ResNet-18, and
PyramidNet (depth= 110, α = 270, no-bottleneck). Memory overhead is dominated by parameter
overhead but also includes temporary variables stored during program execution.

A caveat to the ensemble size analysis is that the kinds of networks expressible in different ensembles
are different, with explicit ensembles more ﬂexible than EMP/ECMP, which are in turn more ﬂexible
than dropout/DropConnect (Figure 3). That said, implicit ensembles are the product of ensembles in
each layer, making the effective ensemble size exponentially larger than explicit ensembles.

We observe that ECMP expresses an asymptotically larger ensemble size than dropout, while requiring
a small extra memory and parameter overhead, thus being a good trade-off between expressiveness
and computational cost. The number of patch layers |Λp| as well as the number of components in
each layer’s ensemble Ki can be tuned as hyperparameters in EMP and ECMP, thus allowing the
programmer the ﬂexibility of adapting to her available compute budget and desired approximation
accuracy. In our experiments, we choose K = 5 and ﬁnd it achieves reasonable expressiveness.

4 Related Work

There is a long history of approximate Bayesian inference for neural networks [34]. MacKay [35]
proposed the use of the Laplace approximation, where L2 regularization can be viewed as a special
case. Neal [36, 37] demonstrated the use of Markov chain MC (MCMC) methods, which are memory
intensive because of the need to store samples. There has been recent work that attempts to sidestep
this by learning a Generative Adversarial Network [38] to recreate these samples [39].

Variational Bayesian neural networks require signiﬁcantly fewer computational resources. The
downside is that variational inference is not guaranteed to reasonably approximate the true posterior
[40], especially given the multi-modal nature of neural networks [41]. [10] proposed a factorial
Gaussian approximation, and presented a biased estimator for the variational parameters, which [12]
subsequently improved with an unbiased estimator and a scale mixture prior. Standard Bernoulli
and Gaussian dropout can both be interpreted as variational inference [1, 15]. We improve upon
[1] by presenting a general variational family for ensemble-based Bayesian neural networks and
showing that Bernoulli dropout is a special case. Other variational Bayesian neural networks
include [14, 16], which use a sequence of invertible transformations known as a normalizing ﬂow
to increase the expressiveness of the approximate posterior. Normalizing ﬂow methods incur a
signiﬁcant computational and memory overhead. [13] proposed a parameter-efﬁcient matrix Gaussian
approximate posterior by assuming independent rows and columns, which is orthogonal to our work.

Using mixture distributions to enrich the expressiveness of variational Bayes is not a new idea. Earlier
work has either used a mixture mean-ﬁeld approximation to model the posterior [42–46] or variational
parameters [47]. The variational family (1) we consider is essentially a mixture mean-ﬁeld method.
However, a direct application of mixture variational methods is prohibitively expensive in large
models, where even a non-mixture mean-ﬁeld approximation incurs a 100% parameter overhead. Our
methods, by virtue of a light parameter overhead, are tailored for large Bayesian neural networks. In
the proposed methods, we marginalize out the discrete variables by one MC draw in the training step,
which resembles particle variational methods [48].

Explicit ensembles of neural networks can be used to model uncertainty [9], and even done in
parameter-efﬁcient ways [49, 50]. However, these methods typically lack a Bayesian interpretation,
which is a principled paradigm of modeling uncertainty. Our work addresses this shortcoming.
[22] proposed an ensemble-based Bayesian neural network. Our variational family is more general,
covering implicit ensembles as well as parameter-efﬁcient members like EMP and ECMP.

5 Experiments

We investigate our methods using deep residual networks on ImageNet, ImageNet-C, CIFAR-100,
and a shallow network on a collection of ten regression datasets. Our aim is to show how our proposed
methods can be used to Bayesianize existing deep neural network architectures, rather than show
state-of-the-art results. As such, we do not tune hyper-parameters and use Adam [51] on the default

6

Method

Parameter
Overhead

Top-5
Accuracy

Top-1
Accuracy

Expected
Calibration Error

Maximum
Calibration Error

Table 4: ResNet-18 on ImageNet

EMP (BN+out)
EMP (BN)
ECMP (BN+out)
ECMP (BN)
Dropout
Vanilla

17.9%
0.328%
17.9%
0.328%
0.00%
0.00%

87.2%
86.8%
87.2%
86.8%
86.7%
86.7%

67.0%
66.6%
67.1%
66.4%
65.9%
66.1%

3.91%
5.74%
1.65%
4.62%
7.61%
8.09%

6.83%
11.0%
3.15%
8.07%
14.0%
14.2%

Figure 4: The calibration curve for ResNet-18 on
ImageNet. EMP and ECMP are better calibrated.

Figure 5: The calibration curve for PyramidNet on
CIFAR-100. EMP and ECMP are better calibrated.

settings. We use ensemble size K = 5 for each layer. While it is not uncommon to report the best
test accuracy found during the course of training, we only evaluate the models found at the end of
training. More implementation details can be found in Appendix D.

ImageNet is a 1000-class image classiﬁcation dataset with 1.28M training images and
ImageNet
50K validation images used for testing [52]. It is a commonly used benchmark in deep learning, but
to our knowledge, no Bayesian neural network has been reported on it, likely due to the parameter
inefﬁciency of standard methods. We evaluated our methods on ResNet-18 against these metrics:
parameter overhead, top-5/top-1 test accuracy, expected calibration error (ECE), maximum calibration
error (MCE), and robustness against common image corruptions (using the ImageNet-C dataset).

Table 4 shows that dropout is slightly better calibrated than the vanilla (non-Bayesian) model, but
has lower test accuracy. Here, Bayesianizing a network by dropout forces a trade-off between test
accuracy and calibration. At the cost of a slight parameter overhead, EMP and ECMP avoid this
trade-off by having both higher test accuracy and lower calibration error than the vanilla and dropout
models. In particular, ECMP (patched on BN and output layers) achieves almost perfect calibration.

The output layer in ResNet-18 is not parameter-efﬁcient, incurring a 17.6% overhead over just
model-patching the BN layers. But it signiﬁcantly improves the model performance and calibration
for EMP and ECMP. For reference, a 6% increase in top-5 accuracy on ImageNet corresponds to
approximately 5 years of progress made by the community [53], so the 0.5% improvement in top-5
accuracy for EMP/ECMP (BN+output) over the vanilla model is signiﬁcant—half a year’s progress.

The vanilla model took approximately a week to train on our multi-GPU system, with our Bayesian
models requiring only a few additional hours. In comparison, related work that incurs a 100%
parameter overhead would have required twice as many FLOPs, and may no longer ﬁt in GPU
memory. If so, we would have needed to halve the batch size, which doubles the training time
[54]. Thus, even if we discount the longer convergence time required by a bigger model, the cost
of Bayesianizing a neural network via a method with a 100% parameter overhead is a 2-4x longer
training time. This discourages the use of parameter-inefﬁcient VBNN methods on deep learning
scale datasets (ImageNet) and architectures (ResNet-18).

ImageNet-C ImageNet-C is a dataset that measures the robustness of ImageNet-trained models
to ﬁfteen common kinds of image corruptions reﬂecting realistic artifacts found across four distinct
categories and ﬁve different levels of severity. We test the ResNet-18 models trained in the previous

7

0.00.51.0Confidence0.00.51.0AccuracyPerfect CalibrationVanillaDropoutEMP (BN+out)ECMP (BN+out)0.00.51.0Confidence0.00.51.0AccuracyPerfect CalibrationVanillaDropoutEMP (BN+out)ECMP (BN+out)Method

EMP (BN+O)
ECMP (BN+O)
Dropout
Vanilla

mCE

97.8
99.5
100
100

Table 5: Mean Corruption Error On ImageNet-C

Noise
Gauss Shot

Impulse Defoc Glass Motion Zoom Snow Frost

Fog

Bright Cont

Blur

Weather

Digital
Elastic Pixel

JPEG

98.5
98.2
98.4
101
102
101
99.8 99.4 98.7
100
100
100

98.8 96.9
97.9
98.3
99.5 99.3 97.7 97.8
101
101
100
100

100
100

98.5
100

99.6
98.0 99.1
101
101
100
100

101.1 99.8
103
104
100

97.9
99.1
105
100

100
102
102
100

91.3 94.2
95.4
97.2 97.3 98.6
99.5 97.6 98.5
100
100
100

Table 6: PyramidNet on CIFAR-100

Method

EMP
ECMP
Dropout
Vanilla

Over-
head

Top5
Acc.

Top1
Acc.

ECE MCE

1.10% 92.0% 74.0% 13.6% 28.9%
1.10% 91.9% 73.5% 8.73% 18.1%
0.00% 91.4% 72.0% 21.1% 52.3%
0.00% 90.9% 72.6% 21.6% 54.6%

Figure 6: How MC sample size affects calibration.

section with S = 100 MC samples, and compute the mean corruption error and the relative mean
corruption error in Table 5 and 8 respectively. Again, EMP and ECMP outperform dropout and the
vanilla model, and provide more robust predictions.

CIFAR-100 CIFAR-100 is a 100-class image classiﬁcation problem with 500 training images and
100 testing images for each class. We test our methods on PyramidNet against top-5/top-1 accuracy
and expected/maximum calibration error, as summarized in Table 6. We observe that EMP and ECMP
are both better calibrated and more accurate than dropout and the vanilla model. The PyramidNet
architecture and the smaller size of CIFAR-100 images contribute to a compact output layer. For a
1.10% parameter overhead, we more than halved the calibration error with ECMP.

We evaluate our methods for ImageNet and CIFAR-100 using S = 200 MC samples in testing. Each
sample potentially captures one mode of the posterior, therefore a higher number of samples results
in better calibration, as shown in Figure 6. This explains why the larger effective ensemble size in
our methods is highly desirable for approximating the posterior and calibrating for uncertainty.

Regression Experiments Following [11, 1, 13, 9], we test the predictive performance of our
methods on a collection of ten regression datasets. We observe that EMP and ECMP consistently
outperform dropout and the vanilla model measured by either test root mean squared error or expected
log predictive density. Detailed comparisons can be found in Table 12 and 13 at Appendix D.7.

6 Discussion and Future Work

Our work bridges implicit and explicit ensembles with a general variational distribution. We focused
on scaling VBNNs for deep learning practitioners, and hence proposed two members of the family,
EMP and ECMP, that economize both parameter and programming overhead. While common
methods like mean-ﬁeld variational inference double parameter use, making them infeasible for state-
of-the-art architectures, our methods scale easily to deep learning scale datasets like ImageNet and
architectures like ResNet and PyramidNet. We showed experimentally that VBNNs constructed with
Ensemble Model Patching work well with batch-normalized networks, achieving better prediction
accuracy and probability calibration than dropout and the non-Bayesian alternative. We hope this
work will draw more attention to computationally efﬁcient methods in large scale Bayesian inference.

There are several research directions for future work. First, instead of ﬁxing them to be uniform,
the mixing probabilities can be made learnable with an ancestral sampling technique [55] or post-
inference reweighting [56]. Second, we can pursue more complicated variational distributions for
the patch layers using methods like normalizing ﬂows. Third, we can investigate other potentially
superior variants within the general ensemble-based variational family, for example mixing EMP on
the normalization layers and dropout/ECMP on the fully connected layers. These methods increase
the complexity of implementation (and thus were avoided in this work), but might prove to be more
effective at calibration and accuracy (at the expense of some additional parameter overhead).

8

1(cid:10)100(cid:10)200(cid:10)0(cid:10)5(cid:10)10(cid:10)ResNet/ImageNet(cid:10)1(cid:10)100(cid:10)200(cid:10)0(cid:10)10(cid:10)20(cid:10)30(cid:10)PyramidNet/CIFAR-100(cid:10)Monte Carlo Samples for Testing(cid:10)Expected Calibration Error (%)(cid:10)ECMP(cid:10)Vanilla(cid:10)Dropout(cid:10)EMP(cid:10)ECMP(cid:10)EMP(cid:10)Dropout(cid:10)Vanilla(cid:10)Acknowledgments

This research was supported in part by the US Defense Advanced Research Project Agency (DARPA)
Lifelong Learning Machines Program, grant HR0011-18-2-0020. The authors would like to thank
Aki Vehtari, Andrew Gelman, and Lampros Flokas for helpful discussion and comments.

References

[1] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing
model uncertainty in deep learning. In International Conference on Machine Learning, pages
1050–1059, 2016.

[2] Xiang Li, Shuo Chen, Xiaolin Hu, and Jian Yang. Understanding the disharmony between
dropout and batch normalization by variance shift. arXiv preprint arXiv:1801.05134, 2018.

[3] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple
and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 2574–2582, 2016.

[4] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.

Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016.

[5] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.

[6] Miguel Lázaro-Gredilla, Joaquin Quiñonero Candela, Carl Edward Rasmussen, and Aníbal R.
Figueiras-Vidal. Sparse spectrum gaussian process regression. Journal of Machine Learning
Research, 11(Jun):1865–1881, 2010.

[7] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversar-

ial examples. arXiv preprint arXiv:1412.6572, 2014.

[8] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In Proceedings of the 34th International Conference on Machine Learning, pages
1321–1330, 2017.

[9] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. In Advances in Neural Information
Processing Systems, pages 6402–6413, 2017.

[10] Alex Graves. Practical variational inference for neural networks.

In Advances in neural

information processing systems, pages 2348–2356, 2011.

[11] José Miguel Hernández-Lobato and Ryan Adams. Probabilistic backpropagation for scalable
learning of Bayesian neural networks. In International Conference on Machine Learning, pages
1861–1869, 2015.

[12] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty

in neural networks. arXiv preprint arXiv:1505.05424, 2015.

[13] Christos Louizos and Max Welling. Structured and efﬁcient variational deep learning with matrix
Gaussian posteriors. In International Conference on Machine Learning, pages 1708–1716,
2016.

[14] Christos Louizos and Max Welling. Multiplicative normalizing ﬂows for variational Bayesian

neural networks. arXiv preprint arXiv:1703.01961, 2017.

[15] Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparam-
eterization trick. In Advances in Neural Information Processing Systems, pages 2575–2583,
2015.

[16] David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, and Aaron

Courville. Bayesian hypernetworks. arXiv preprint arXiv:1710.04759, 2017.

9

[17] Jamie Hanlon.

How to solve the memory challenges of deep neural net-
works. https://www.topbots.com/how-solve-memory-challenges-deep-learning-
neural-networks-graphcore/, 2017.

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016.

[19] Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal residual networks. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5927–5935,
2017.

[20] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training

by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

[21] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of
neural networks using Dropconnect. In International conference on machine learning, pages
1058–1066, 2013.

[22] Tim Pearce, Nicolas Anastassacos, Mohamed Zaki, and Andy Neely. Bayesian inference with
anchored ensembles of neural networks, and application to reinforcement learning. arXiv
preprint arXiv:1805.11324, 2018.

[23] Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.

[24] Andrew Gelman and Cosma Rohilla Shalizi. Philosophy and the practice of bayesian statistics.

British Journal of Mathematical and Statistical Psychology, 66(1):8–38, 2013.

[25] Pramod Kaushik Mudrakarta, Mark Sandler, Andrey Zhmoginov, and Andrew Howard. K for the
price of 1: Parameter efﬁcient multi-task and transfer learning. arXiv preprint arXiv:1810.10703,
2018.

[26] Andrew Gelman, Jessica Hwang, and Aki Vehtari. Understanding predictive information criteria

for Bayesian models. Statistics and computing, 24(6):997–1016, 2014.

[27] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for

artistic style. arXiv preprint arXiv:1610.07629, 2017.

[28] Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent Dumoulin, and Jonathon Shlens.
Exploring the structure of a real-time, arbitrary neural artistic stylization network. arXiv
preprint arXiv:1705.06830, 2017.

[29] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative

adversarial networks. arXiv preprint arXiv:1812.04948, 2018.

[30] Taesup Kim, Inchul Song, and Yoshua Bengio. Dynamic layer normalization for adaptive neural

acoustic modeling in speech recognition. arXiv preprint arXiv:1707.06065, 2017.

[31] Vincent Dumoulin, Ethan Perez, Nathan Schucher, Florian Strub, Harm de Vries, Aaron

Courville, and Yoshua Bengio. Feature-wise transformations. Distill, 2018.

[32] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film:

Visual reasoning with a general conditioning layer. arXiv preprint arXiv:1709.07871, 2017.

[33] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9,
2015.

[34] Jouko Lampinen and Aki Vehtari. Bayesian approach for neural networks—review and case

studies. Neural networks, 14(3):257–274, 2001.

[35] David JC MacKay. A practical Bayesian framework for backpropagation networks. Neural

computation, 4(3):448–472, 1992.

10

[36] Radford M Neal. Bayesian training of backpropagation networks by the hybrid Monte Carlo
method. Technical report, Technical Report CRG-TR-92-1, Dept. of Computer Science, Univer-
sity of Toronto, 1992.

[37] Radford M Neal. Bayesian learning for neural networks. Springer, 1996.

[38] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems, pages 2672–2680, 2014.

[39] Kuan-Chieh Wang, Paul Vicol, James Lucas, Li Gu, Roger Grosse, and Richard Zemel. Ad-
versarial distillation of Bayesian neural network posteriors. arXiv preprint arXiv:1806.10317,
2018.

[40] Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Yes, but did it work?: Evaluat-
ing variational inference. In International Conference on Machine Learning, pages 5577–5586,
2018.

[41] Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learn-
ing: A survey and taxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence,
41(2):423–443, 2019.

[42] Christopher M Bishop, Neil D Lawrence, Tommi Jaakkola, and Michael I Jordan. Approximat-
ing posterior distributions in belief networks using mixtures. In Advances in neural information
processing systems, pages 416–422, 1998.

[43] Tommi S Jaakkola and Michael I Jordan. Improving the mean ﬁeld approximation via the use
of mixture distributions. In Learning in graphical models, pages 163–173. Springer, 1998.

[44] O. Zobay. Variational Bayesian inference with Gaussian-mixture approximations. Electronic

Journal of Statistics, 8(1):355–389, 2014.

[45] Samuel Gershman, Matt Hoffman, and David Blei. Nonparametric variational inference. In

International Conference on Machine Learning, 2012.

[46] Andrew C Miller, Nicholas J Foti, and Ryan P Adams. Variational boosting: Iteratively reﬁning
posterior approximations. In International Conference on Machine Learning, pages 2420–2429,
2017.

[47] Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In Interna-

tional Conference on Machine Learning, pages 324–333, 2016.

[48] Ardavan Saeedi, Tejas D Kulkarni, Vikash K Mansinghka, and Samuel J Gershman. Variational
particle approximations. The Journal of Machine Learning Research, 18(1):2328–2356, 2017.

[49] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger.

Snapshot ensembles: Train 1, get m for free. arXiv preprint arXiv:1704.00109, 2017.

[50] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon
Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint
arXiv:1803.05407, 2018.

[51] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

[52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
Imagenet large scale visual recognition challenge. International Journal of Computer Vision,
115(3):211–252, 2015.

[53] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do Imagenet

classiﬁers generalize to Imagenet? arXiv preprint arXiv:1902.10811, 2019.

[54] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model

of large-batch training. arXiv preprint arXiv:1812.06162, 2018.

11

[55] Alex Graves. Stochastic backpropagation through mixture density distributions. arXiv preprint

arXiv:1607.05690, 2016.

[56] Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Using stacking to average
Bayesian predictive distributions (with discussion). Bayesian Analysis, 13(3):917–1003, 2018.

[57] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for
computer vision? In Advances in neural information processing systems, pages 5574–5584,
2017.

[58] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Appendix. arXiv

preprint arXiv:1506.02157, 2015.

[59] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. The Journal of Machine
Learning Research, 15(1):1929–1958, 2014.

[60] Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M Blei. Automatic
differentiation variational inference. The Journal of Machine Learning Research, 18(1):430–474,
2017.

[61] Radford M Neal. Priors for inﬁnite networks. In Bayesian Learning for Neural Networks, pages

29–53. Springer, 1996.

[62] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common

corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.

[63] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report,

Citeseer, 2009.

[64] TechPowerUp. GPU specs database. https://www.techpowerup.com/gpu-specs/, 2019.

12

Appendix

A Background

We provide a brief introduction to the basic technical background of Bayesian neural networks. The
reader familiar with the area may want to skip Section A.

A.1 Learning a Point Estimate of a Bayesian Neural Network

In a regression or classiﬁcation task, a neural network is given input X and has to model output
Y = f (X, Θ) using weights and biases Θ in the network. The size of the dataset is denoted N . For
example, in regression p(Y|X, Θ) is usually assumed to be Gaussian N (f (X, Θ), τ −1
outputI), with
f (X, Θ) modeling the epistemic uncertainty and τ −1

outputI modeling the aleatoric uncertainty [57].

The weights in the network can be learned via maximum a posteriori (MAP) estimation.

ΘMAP = arg max

Θ

log p(Θ|X, Y) = arg max

log p(Y|X, Θ) + log pprior(Θ).

Θ

The prior on weights p(Θ) is commonly chosen to be Gaussian, which results in L2 regularization.
With a Laplace prior, we end up with L1 regularization instead.

A.2 Variational Bayesian Neural Networks

When Θ is a point estimate, there can be no epistemic uncertainty in f (x, Θ). One of the aims
of a Bayesian neural network is to learn the posterior distribution over Θ to model the epistemic
uncertainty in the network.

Variational inference approximates the posterior distribution by q(Θ) such that the Kullback-Leibler
(KL) divergence between the two, KL(q(Θ) || p(Θ|X, Y)), is minimized. Minimizing the KL
divergence is equivalent to maximizing the log evidence lower bound (ELBO):

(cid:90)

ELBO =

q(Θ) log p(Y|X, Θ) dΘ − KL(q(Θ) || pprior(Θ)).

A.3 MC dropout

MC dropout [58, 1] interprets dropout [59] as variational Bayesian inference in a deep probabilistic
model, speciﬁcally a deep Gaussian process. It employs a variational distribution factorized over
the weights, where each weight factor Wi is a mixture of Gaussians and each bias factor bi is a
multivariate Gaussian.

For a network with L dropout layers, each containing Hi hidden units for i ∈ [1, L] (H0 is the number
of input units), we can describe the variational approximation as follows.

Wi = Zi (cid:12) (Mi + σ(cid:15)i) + (1Hi−1×Hi − Zi) (cid:12) σ(cid:15)i

Zi,h1h2 = zih1

zih1 ∼ Bernoulli(pi)

(cid:15)i ∼ N (0, IHi−1×Hi)
bj = mj + σ(cid:15)j
(cid:15)j ∼ N (0, IHj )

for σ > 0, h1 ∈ [1, Hi−1], h2 ∈ [1, Hi], i ∈ [1, L], j ∈ [1, L]

1Hi−1×Hi denotes a matrix where all the entries are ones. (cid:12) denotes the Hadamard product. Zi,h1h2
denotes the entry at the h1th row and h2th column of the matrix Zi.
The variational parameters are the Bernoulli probabilities {pi}L
{Mi}L

j=1. The Bernoulli probabilities are typically ﬁxed and not learned.

i=1 and the neural network weights

i=1, {mj}L

13

B Computation of ELBO for the General Ensemble-Based Variational

Family

The ELBO can always be decomposed into the expected log likelihood and the KL divergence
between the approximate posterior q(Θ) and prior pprior(Θ)
(cid:90)

ELBO =

q(Θ) log p(Y|X, Θ) dΘ − KL(q(Θ) || pprior(Θ)).

The likelihood term, (cid:82) q(Θ) log P (Y|X, Θ) dΘ can be approximated by MC draws Θ1 . . . ΘS .

(cid:90)

q(Θ) log p(Y|X, Θ) dΘ ≈

S
(cid:88)

s=1

log p(Y|X, Θs).

In particular, for regression problems when assuming a ﬁxed output precision τoutput,

Y|Θ, X ∼ N (Y| ˆY, τ −1

outputI),

where ˆY is the (point estimation) prediction of Y in the neural network with input X and parameter
Θ, we have

(cid:90)

q(Θ) log p(Y|X, Θ) dΘ ≈ −

τoutput
2S

S
(cid:88)

s=1

||Y − ˆYs||2
2.

where ˆYs is the (point estimation) prediction of Y in the neural network with input X and parameter
Θs for MC draw s.

For classiﬁcation,

(cid:90)

q(Θ) log p(Y|X, Θ) dΘ ≈

1
S

N
(cid:88)

S
(cid:88)

n=1

s=1

log ˆpyn,s.

where ˆpyn,s is the point estimation of yn in the neural network with the input Xn and the s-th MC
draw of the parameters Θs.

In the following two sections B.1 and B.2, we will prove

Theorem 1 In Equation (6), the KL divergence can be approximated by

KL(q(Θ | M, m)|| pprior(Θ)) ≈

(cid:88)

Ki(cid:88)

(cid:88)

Kj
(cid:88)

||Mik||2

2 +

τipik
2

ρjpjk
2

||mjk||2
2

i∈Λp
(cid:88)

i∈Λs

+

k=1
τi
2

||Mi||2

2 +

ρj
2

(cid:88)

j∈Λp

k=1

j∈Λp
||mj||2

2 + Constant,

where the constant is referred to as constant with respect to M, m.

B.1 Approximating the Entropy of a Gaussian Mixture by the Sum of Individual Entropies

Consider the most general case where the variational distribution q(Θ) is a mixture of D-dimensional
Gaussians. q is fully parameterized by µ and Σ,

q(Θ | µ, Σ) =

K
(cid:88)

k=1

pkN (Θ; µk, Σk) , Θ ∈ RD.

where K is the number of components, µi ∈ RD, Σk ∈ RD×D are variational parameters, and pk is
ﬁxed.

For a mixture of Gaussians, there is no closed form expression of the entropy term H(q(Θ)) =
− (cid:82) q(Θ) log q(Θ)dΘ. Nevertheless, it can be upper-bounded by the sum of entropies belonging to
each individual component. More precisely,

H(q(Θ)) ≤

=

K
(cid:88)

k=1

K
(cid:88)

k=1

pkH(N (Θ; µk, Σk))

pk
2

(log |Σk|) +

KD
2

(1 + log 2π).

14

(7)

The ﬁrst line is due to the fact that overlap among mixture components reduces the entropy (for proof,
see for example, Zobay [44]). When dimension D is high, and the number of mixture components K
is not large, the overlap among components is negligible. Therefore, we can approximate the entropy
using just the second line of (7). It is a similar approximation to Gal and Ghahramani [58].
Further, when pprior(Θ) is a multivariate normal centered at 0,
pprior = N (cid:0)Θi|0, τ −1ID
(cid:1) ,

the cross-entropy can be computed as

(cid:90)

−

q(Θ) log p(Θ)dΘ = −

(cid:90)

pk

K
(cid:88)

k=1

N (Θ; µk, Σk) log N (cid:0)0, τ −1ID

(cid:1) dΘ.

=

D
2

(log(2π) − log τ ) +

K
(cid:88)

i=1

τ pi
2

(µT

i µi + Tr (Σi)).

Putting them together, we get:

KL(q(Θ) || pprior(Θ)) ≈

K
(cid:88)

i=1

pk
2

(τ (µT

k µk + tr (Σk)) − log |Σk|) + Constant.

In many cases for computational simplicity, we set Σk = σ2ID where σ is a ﬁxed constant. Hence,
we get:

KL(q(Θ) || pprior(Θ)) ≈

K
(cid:88)

k=1

τ pk
2

||µk||2

2 + Constant.

(8)

B.2 Deriving the ELBO for the General Variational Ensemble

Now if we write the variational distribution for all batch parameters to be

q(W, b) =

L
(cid:89)

i=1

q(Wi)

L
(cid:89)

j=1

q(bj).

Then for each dimension,
(cid:88)

q(Wi) =

k

pikN (cid:0)Wi|Mik, σ2I(cid:1) ,

q(bj) =

pjkN (cid:0)bj; mjk, σ2I(cid:1) .

(cid:88)

k

In Equation (8) with µk = Mik and τ = τi, we obtain

Similarly,

KL(q(Wi))||pprior(Wi) =

Ki(cid:88)

k=1

τipik
2

||Mik||2

2 + Constant.

KL(q(bj))||pprior(bj) =

Kj
(cid:88)

k=1

ρjpjk
2

||mjk||2

2 + Constant.

Finally, each layer is modeled independently in the variational approximation. Thus we get Equation
(2):

KL(q(Θ)|| pprior(Θ)) ≈

L
(cid:88)

Ki(cid:88)

i=1

k=1

τipik
2

||Mik||2

2 +

L
(cid:88)

Kj
(cid:88)

j=1

k=1

ρjpjk
2

||mjk||2

2 + Constant.

In the presence of model patching,

q(Θshared) =

(cid:89)

i∈Λs

q(Wi)q(bi) =

(cid:89)

i∈Λs

15

N (Wi|Mi, σsI)N (bi|mi, σsI).

Then for i ∈ Λs, the KL term is just the KL divergence between two mean-ﬁeld Gaussians:

KL(q(Wi)||N (Wi|0, τ −1

i

I)) =

KL(q(bi)||N (bi|0, ρ−1

j I)) =

τi
2
ρj
2

||Mik||2

2 + C, ∀i ∈ Λs,

||mi||2

2 + C, ∀i ∈ Λs.

This leads to the following

KL(q(Θ)|| pprior(Θ)) ≈

(cid:88)

Ki(cid:88)

(cid:88)

Kj
(cid:88)

||Mik||2

2 +

τipik
2

ρjpjk
2

||mjk||2
2

i∈Λp
(cid:88)

i∈Λs

+

k=1
τi
2

||Mi||2

2 +

ρj
2

(cid:88)

j∈Λp

k=1

j∈Λp
||mj||2

2 + Constant.

which is precisely Equation (6). To arrive at the ELBO, we just have to combine the KL term and the
likelihood term. For regression problems, this becomes

−ELBO =

τoutput
2S

S
(cid:88)

N
(cid:88)

s=1

n=1

||Yn − ˆYn,s||2
2

+

+

(cid:88)

Ki(cid:88)

i∈Λp
(cid:88)

i∈Λs

k=1
τi
2

(cid:88)

Kj
(cid:88)

||Mik||2

2 +

τipik
2

k=1

j∈Λp
||mj||2
2.

ρj
2

(cid:88)

j∈Λp

||Mi||2

2 +

For classiﬁcation, this becomes

−ELBO = −

1
S

S
(cid:88)

N
(cid:88)

s=1

n=1

log ˆpyn,s

+

+

(cid:88)

Ki(cid:88)

i∈Λp
(cid:88)

i∈Λs

k=1
τi
2

(cid:88)

Kj
(cid:88)

||Mik||2

2 +

τipik
2

k=1

j∈Λp
||mj||2
2.

ρj
2

(cid:88)

j∈Λp

||Mi||2

2 +

ρjpjk
2

||mjk||2
2

(9)

ρjpjk
2

||mjk||2
2

B.3 Posterior Uncertainty

In training, we run stochastic gradient descent (SGD) with one MC draw (S=1) for gradient evaluation.
Hence, maximizing the ELBO above is going to resemble the non-Bayesian training that minimizes
the usual squared error or cross entropy plus the L2 regularization. This interpretation makes it easy
for deep learning programmers to incorporate Bayesian training into their neural networks without
much additional programming overhead. However, we emphasize the fundamental differences
between conventional point estimation with L2 regularization and our Bayesian approach:

• The L2 regularization is over all variational parameters M and m, not over neural net weights

W and b.

• Even if we run SGD with one MC draw with one realization of the categorical variable Z at
each iteration, the MC gradient is still unbiased. Thus, the optimization converges to the
desired variational distribution.

• In the testing phase, we will draw S > 1 to obtain the approximate posterior distribution

Θ1, . . . , ΘS.

16

In particular, we are able to obtain the posterior predictive distribution for the whole model using the
variational approximation. The posterior predictive density at a new input x∗ can be approximated by

p(y∗|x∗, X, Y) =

(cid:90)

p(y∗|x∗, Θ) p(Θ|X, Y) dΘ ≈ 1/S

S
(cid:88)

(cid:90)

s=1

p(y∗|x∗, Θs).

Any posterior predictive check and posterior uncertainty can then be performed through samping
{Θs} and then {y∗} from p(·|x∗, Θs).
Denote f (x∗, Θ) to be the prediction of outcome y∗ at input x∗. Then the predictive mean and
variance can be calculated through MC estimation

Epost[y∗|x∗]approx

(cid:90)

q(Θ)f (x∗, Θ) dΘ ≈

1
S

S
(cid:88)

s=1

f (x∗, Θs),

Varpost[y∗|x∗] ≈

1
S

S
(cid:88)

s=1

f (x∗, Θs)T f (x∗, Θs) − (cid:0)Epost[y∗|x∗](cid:1)T Epost[y∗|x∗] + τ −1

outputI.

B.4 Stochastic Gradients, MC Integration, and Marginalization of Discrete Variables

Theorem 1 establishes a closed form approximation of KL divergence in the ELBO. What remains
left is the expected log likelihood, which is typically estimated through MC integration. We justify
the use of Algorithm 1 with the following theorem.

Theorem 2 The gradient evaluation in Algorithm 1 is unbiased, and thus SGD will converge to its
(local) optimum given other regularization conditions.

Algorithm 1 is implemented through SGD. The entropy term has a closed form. Essentially, we are
using MC estimation three times for the log likelihood term:

• We use a minibatch.
• We draw MC sample θ from q(Θ|M, m) (by convention, we use θ to emphasize that it is
one MC realization of Θ. The log likelihhod and its gradient can be evaluated through the
equation

Eq log p(y|Θ, x) =

q(Θ) log p(y|Θ, x) dΘ ≈ log p(y|θ, x).

(cid:90)

• Indeed, we do not have to derive the explicit form for q(Θ|M, m), as it depends on discrete
variables Z (integers that indicate the assignments of mixture components). However,
we draw one realization of the discrete assignment z for each layer (again, we use z to
emphasize it is one realization of Z). That approximates

q(Θ|M, m) =

(cid:90)

q(Θ|M, m, Z)q(Z) dZ ≈ q(Θ|M, m, z).

where q(Θ|M, m, Z) is from the construction in Equation (1), and q(Z) is a multinoulli
distribution speciﬁed from before.

In all these three steps, the MC approximations are unbiased even with one MC draw, hence so will
the gradient of the ELBO. More precisely, we estimate the likelihood term in the ELBO with the
following MC approximation

(cid:90)

(cid:90)

=

q(Θ|M, m) log p(Y|X, Θ) dΘ

q(Θ|M, m, Z)q(Z) log p(Y|X, Θ) dΘ

(10)

≈ log p(Y|X, Θ = θ),

where we ﬁrst draw a realization z from q(Z), and draw a realization θ from q(Θ|M, m, Z = z),
which is exactly what Algorithm 1 does. The results are similar where the number of draws S > 1.

17

Approximation (10) is always unbiased, based on which the reparametrized gradients

(cid:90)

∂
∂M

q(Θ|M, m) log p(Y|X, Θ) dΘ,

(cid:90)

∂
∂m

q(Θ|M, m) log p(Y|X, Θ) dΘ.

are also unbiased. Therefore the convergence theorem of SGD holds.

It remains unclear how many MC draws (S) will be the most efﬁcient for training. In the limit where
S = [Effective Ensemble Size], we can marginalize all the discrete variables Z and get q(θ|M, m)
exactly. On the other hand, one MC draw is commonly used in practice, and has been commonly
reported to be the most efﬁcient setting in variational inference [60, 40].

Implicit Variational Distribution We also emphasize that Z in Algorithm 1 is a three way tensor,
Z = {Zi,h1,h2}, where i indexes the layer, and h1, h2 are the indices of the parameter of the weight
matrix in layer i. Marginally, each element Zi,h1,h2 is from a multinoulli with its corresponding
mixing variable. However, different Zi,h1,h2 are not necessarily independent. For example, in EMP,
all the Z = Zi,h1,h2 are equal for the same layer i.
Writing down the joint distribution of q(Z) can be messy. Nevertheless, in our MC integration, we
are only required to be able to sample Z. In this sense, we are constructing an implicit variational
distribution through different constructions of the assignment Z.

C Implementation and Hyper-parameters

C.1 Tuning Hyper-parameters

From the Bayesian perspective, the hyper-parameters, which include the prior precisions τ, ρ, the
mixing probability p, and the output precision τout, should also be taken into account when evaluating
model uncertainty.

Equation (6) can be either extended or simpliﬁed. We can extend it by including all the parameters as
variational parameters but this signiﬁcantly increases both the parameter and programming overhead.
To simplify the implementation, we can assume uniformity and rewrite the loss function as L2
regularization. Then, regression (9) becomes

Loss = Mean Squared Error

+ λ1

+ λ3

(cid:88)

Ki(cid:88)

||Mik||2

2 + λ2

(cid:88)

Kj
(cid:88)

||mjk||2
2

k=1
||Mi||2

2 + λ4

i∈Λp
(cid:88)

i∈Λs

(cid:88)

j∈Λp

k=1

j∈Λp
||mj||2
2.

Then, only four terms λ1, λ2, λ3, λ4 have to be tuned. It is an interesting research question to
determine how one can tune these parameters to obtain better calibration and posterior uncertainty.

In our experiments, we simply use an arbitrarily chosen value

λ1 = λ2 = λ3 = λ4 =

0.001
[batch size]

C.2 Parallelization

Since all the training in Ensemble Model Patching is done via SGD and backpropagation, it can be
easily trained with a regular distributed SGD algorithm with no modiﬁcations, like the one outlined in
Mudrakarta et al. [25]. It is also possible to speed the training process by using Coordinate Descent.
The learning of Θshared and Θpatch can be split into two alternating phases, by holding one ﬁxed
while the other is being trained. The learning of non-overlapping Θpatch is trivially lock-free and can
be done by separate worker processes. The learning of Θshared can be done with distributed SGD as
before. This training strategy will be most effective when the ensemble size K is large.

18

D Details and Extensions of the Experiments

D.1 The Implicit Prior on the Function

It is a well-known result that a normal prior on the weights and biases leads to a Gaussian process on
the ﬁnal model y = f (x) when the number of hidden units goes to inﬁnity [61, 36, 37]. The varia-
tional inference approximation can also be viewed as an implicit prior that restricts the distribution of
weights and biases. What prior does it imply on the function y = f (x)?

In Figure 3, we generate a toy example with one hidden layer:

y = B +

H
(cid:88)

h=1

Whuh,

uh = g(bh + whx),

h = 1, . . . , H

We use (B, W ) and (b, w) to denote the bias and weights in the output and hidden unit layer. g is the
activation function. We stick to g(x) = sign(x) because of its theoretical convenience.

We then generate both B and b from N(0, 20), as well as W and w from N(0, 5), where the number
of hidden units is H = 100. This approximately results in a Gaussian process.

Now, with probability 0.5 some weights W and w are dropped to 0. Notice that x will never be
expressed in a hidden unit h if wh is dropped to 0. Therefore, this implies a rough and piece-wise
constant function. The left panel of Figure 3 simulates three such functions.

By contrast, in EMP, the weight and bias are uniformly chosen from K = 5 independent Gaussian
components with the same parameter mentioned above. This leads to a smoother function (right
panel) that are indeed closer to a Gaussian process and is able to express ﬁner details.

We use this example to demonstrate the restriction of ﬁxing one component to be constant at 0, which
is intrinsic to dropout. Our preliminary experiments involving restricting one of the components in
EMP and ECMP to zero also indicate worse performance.

D.2 Calibration Error

The calibration error [8] was computed by ﬁrst splitting the prediction probability interval [0, 1] into
20 equally sized bins, and then measuring the accuracy, conﬁdence, expected calibration error, and
maximum calibration error of the model over these 20 bins. Let Br be the set of indices denoting the
samples whose prediction probability falls in the interval ( r−1

20 ] for r ∈ [1, 20]. Then we have

20 , r

accuracy(Br) =

conﬁdence(Br) =

1
|Br|

1
|Br|

(cid:88)

i∈Br
(cid:88)

i∈Br

1[ˆyi = yi]

ˆpi

Expected Calibration Error =

20
(cid:88)

r=1

|Br|
n

|accuracy(Br) − conﬁdence(Br)|

Maximum Calibration Error = max

r

|accuracy(Br) − conﬁdence(Br)|

where n is the total number of samples

If a model predicts a 60% probability that a given sample belongs to a certain class, then it ought
to be correct 60% of the time. Intuitively, this means that a perfectly calibrated model should have
conﬁdence(Br) = accuracy(Br).

We remove bins with at most 5 samples in them to get rid of outliers.

D.3 ImageNet

We use the ILSVRC 2012 version of the dataset [52], as is commonly used to benchmark new
architectures in deep learning. As is standard practice, the images are randomly cropped and resized
to 224 by 224 pixels.

19

Figure 7: ResNet-18 calibration curves on ImageNet. Patching the output layers in addition to the BN layers
improve calibration for both EMP and ECMP. This graph is an expanded version of the graph shown in Figure 4.

The ResNet-18 was trained for 100 epochs with batch size 256 and tested using S = 200 MC samples.
We include different conﬁgurations of EMP and ECMP where both the BN and output layers were
model patched, and where only the BN layers were patched. This is because the output layer in
ResNet-18 is parameter dense due to the size of the images in ImageNet, and we wanted to see the
relative effect of including versus excluding the output layer.

We use p = 0.005 for MC dropout, with the dropout layer occurring before every BN layer. It is
difﬁcult to tune the optimal dropout rate without using multiple runs, so this dropout rate is probably
not optimal.

D.4

ImageNet-C

ImageNet-C is a dataset that measures the robustness of ImageNet-trained models to ﬁfteen common
kinds of image corruptions reﬂecting realistic artifacts found across four distinct categories: noise,
blur, weather, and digital [62]. Each corruption comes in ﬁve different levels of severity.

The Mean Corruption Error (mCE) and Relative Mean Corruption Error (rmCE) can be measured as
follows:

Em

c = Top-1 error for model m summed across 5 different severity levels for corruption c,

where c = clean represents the no-corruption setting

CEm

c =

rCEm

c =

mCEm =

rmCEm =

1
15

clean
− Em

clean

c

Em
c
Evanilla
c − Em
Em
Evanilla
c
1
(cid:88)
15

CEm
c

c
(cid:88)

c

rCEm
c

Intuitively, the mCE reﬂects the additional robustness a VBNN method adds to an existing model.
But because a model can have lower mCE by virtue of having lower test accuracy in the no-corruption
setting. The rmCE taking that into account by measuring the relative change in test performance
caused by the corruption.

A priori, we should not expect that being Bayesian will necessarily confer a model with robustness
against noise and corruption. For example, we observe that dropout confers no advantage to the
vanilla model against corruption.

EMP offers more robustness against common corruptions than ECMP. We hypothesize that this is
likely because corruptions introduce more noise at the level of individual weights than at the level of
the layer.

20

0.00.51.0Confidence0.00.51.0AccuracyPerfect CalibrationVanillaDropoutEMP (BN)EMP (BN+out)ECMP (BN)ECMP (BN+out)Table 7: Mean Corruption Error in ImageNet-C

Noise
Gauss Shot

Impulse Defoc Glass Motion Zoom Snow Frost

Fog

Bright Cont

Blur

Weather

98.4
98.5
98.2
99.7 99.6 99.4
101
102
101
99.3 99.9
100
99.8 99.4 98.7
100
100
100

98.8 96.9
100

98.3
97.9
98.9 98.4
97.9
99.5 99.3 97.7 97.8
98.7 99.5 98.3 98.8
101
101
100
100

100
100

98.5
100

99.6
101
98.0 99.1
101
101
100

101.1 99.8
100.9 100
103
99.6 103
104
101
100
100

97.9
101
99.1
103
105
100

100
100
102
102
102
100

Digital
Elastic Pixel

JPEG

91.3 94.2
95.4
99.0 90.8
94.7
97.2 97.3 98.6
99.1 96.0 98.4
99.5 97.6 98.5
100
100
100

Method

EMP (BN+O)
EMP (BN)
ECMP (BN+O)
ECMP (BN)
Dropout
Vanilla

mCE

97.8
98.8
99.5
99.8
100
100

Table 8: Relative Mean Corruption Error in ImageNet-C

Noise

Blur

Method

EMP (BN+O)
EMP (BN)
ECMP (BN+O)
ECMP (BN)
Dropout
Vanilla

rmCE Gauss Shot

Impulse Defoc Glass Motion Zoom Snow Frost

97.4
99.0
102
101
100
100

98.8 98.4 98.8
99.9
100
100
103
104
104
99.2 100
101
97.0
98.1
98.7
100
100
100

98.6
97.9 99.4 96.1
99.2 98.3
101
97.3
101
98.0 98.3
101
98.3 99.5 97.4 98.2
98.9 101
100
100
100
100

96.4
100

101
102
98.5
101
101
100

104
102
103
102
101
110
108
99.7
99.9 106
100
100

Weather
Fog

Bright Cont

97.7
109
105
113
114
100

102
102
106
104
102
100

Digital
Elastic Pixel

JPEG

84.8 89.5
92.2
99.6 83.1
89.9
97.1 97.4 101
98.7 92.7 97.2
97.1 93.8 94.8
100
100
100

D.5 CIFAR-100

CIFAR-100 is another commonly used dataset for benchmarking new architectures and algorithms in
deep learning [63]. It contains images of size 32 by 32 pixels. The PyramidNet we used has the follow-
ing conﬁguration: depth= 110, α = 270, no-bottleneck. It was trained for 400 epochs with batch size
256 and tested with 200 MC samples. We apply our methods based on the PyramidNet/CIFAR-100
implementation provided by the authors Han et al. [19] at https://github.com/dyhan0920/PyramidNet-
PyTorch, which uses a standard data augmentation process involving horizontal ﬂipping and random
cropping with padding.

Both the BN and output layers were patched in EMP/ECMP for our experiments, given that the
parameter overhead for both layers combined is very slight (1.10%). The dropout rate is 0.005 and a
dropout layer was placed before every BN layer.

D.6 Choosing the MC Sample Size S for Testing

The number of MC samples S used for testing is also the number of forward passes that the network
has to use to evaluate a given data point. We assume that the BN and output layers are patched
for EMP/ECMP and evaluate our trained models with 1, 20, 100, and 200 samples. Generally, we
observe that a increase in S results in better calibration and higher test accuracy. We see in Table
9 and Table 10 that there is no signiﬁcant difference in ECE between S = 100 and S = 200, with
S = 200 performing slightly better in some cases, due to noise in the MC sampling process. This
suggests that the additional beneﬁt of using more samples past S = 100 is slight at best. Another
interesting observation is that on PyramidNet, dropout seems to confer little to no advantage in
calibration.

Table 9: Effect of MC Sample Size S on Test Accuracy and Calibration for EMP

S

1
20
100
200
Vanilla

Top-5

86.4%
87.1%
87.2%
87.2%
86.7%

ResNet/ImageNet
Top-1

ECE

65.6%
66.9%
67.0%
67.0%
66.1%

7.03%
4.08%
3.88%
3.91%
8.09%

PyramidNet/CIFAR-100

MCE

13.8%
7.00%
7.71%
6.83%
14.2%

Top-5

90.9%
91.6%
91.7%
92.0%
90.9%

Top-1

72.2%
73.2%
74.2%
74.0%
72.6%

ECE

21.2%
14.2%
13.6%
13.6%
21.6%

MCE

50.1%
30.0%
30.3%
28.9%
54.6%

D.7 Predictive Performance on Ten Regression Datasets

This experiment tests the predictive performance of Bayesian neural networks on a collection of ten
regression datasets. (The collection of datasets for this purpose was ﬁrst proposed by Hernández-
Lobato and Adams [11], and later followed by several other authors in the Bayesian deep learning

21

Table 10: Effect of MC Sample Size S on Test Accuracy and Calibration for ECMP

S

1
20
100
200
Vanilla

Top-5

85.6%
87.1%
87.2%
87.2%
86.7%

ResNet/ImageNet
Top-1

ECE

64.1%
66.9%
67.1%
67.1%
66.1%

7.52%
1.70%
1.50%
1.65%
8.09%

MCE

14.5%
4.18%
3.05%
3.15%
14.2%

PyramidNet/CIFAR-100

Top-5

89.3%
91.6%
91.8%
91.9%
90.9%

Top-1

69.3%
72.9%
72.7%
73.5%
72.6%

ECE

23.9%
9.35%
8.81%
8.73%
21.6%

MCE

53.0%
21.4%
18.2%
18.1%
54.6%

Table 11: Effect of MC Sample Size S on Test Accuracy and Calibration for Dropout

S

1
20
100
200
Vanilla

Top-5

86.3%
86.6%
86.7%
86.7%
86.7%

ResNet/ImageNet
Top-1

ECE

65.3%
65.9%
65.9%
65.9%
66.1%

8.99%
7.64%
7.64%
7.61%
8.09%

PyramidNet/CIFAR-100

MCE

16.6%
14.1%
14.2%
14.0%
14.2%

Top-5

91.0%
91.4%
91.1%
91.4%
90.9%

Top-1

72.0%
72.3%
72.2%
72.0%
72.6%

ECE

21.8%
21.0%
21.0%
21.1%
21.6%

MCE

54.2%
50.7%
51.9%
52.3%
54.6%

literature [1, 13, 9].) Each dataset is split 90:10 randomly into training and test sets. Twenty random
splits are done (except Y ear and P rotein which uses one and ﬁve splits respectively). The average
test performance for ECMP, EMP, dropout, and an explicit ensemble is reported in Table 12 and Table
13.

Unlike an experiment on a normal dataset with a train-val-test split, Hernández-Lobato and Adams
[11]’s experimental setup uses repeated subsampling cross-validation. For each split, the hyperparam-
eters have to be chosen without looking at the test set. While Hernández-Lobato and Adams [11] and
Gal and Ghahramani [1] use Bayesian optimization to select hyperparameters, it is important to note
that the search range of hyperparameters used for different datasets are different, and was determined
by looking at the data. (For example, see these two different hyperparameter search conﬁgurations in
Gal and Ghahramani [1]’s code repository.) Louizos and Welling [13] and Lakshminarayanan et al.
[9] conduct the experiment without using a validation set at all.

We choose to forgo this exercise in hyperparameter tuning, and use ﬁxed hyperparameters throughout.
As such, our results are not directly comparable with the results in the literature. It is possible that
MC dropout and the explicit ensemble might have signiﬁcantly different performance under different
hyperparameter settings. We do not recommend that others use this experiment as a benchmark,
because the experimental setup is fundamentally ﬂawed, as was explained above.

The data in the training set is normalized to have zero mean and unit variance. The neural network
used has the ReLU activation function, and one hidden layer with 50 hidden units, except Y ear and
P rotein where we use 100 hidden units. The BN/dropout layers are placed after the input and after
the hidden layer, and only the BN layers (γ initialized at 0.2) are model patched. The networks in the
explicit ensemble do not contain any BN or dropout layers. We train the network for 4000 epochs
across all the methods with a batch size of 100, τoutput = 0.1, and weight decay of 0.01. Where
applicable, the dropout rate is 0.005, ensemble size K = 5, number of MC samples used S = 10, 000
(same setting as in MC dropout [1]).

We observe that ECMP and EMP have the lowest test root mean squared error in eight of the
ten datasets, and the highest test log likelihood in nine of them.

ECMP and EMP did worse than dropout and the explicit ensemble in the Y ear dataset. We think that
this is probably caused by the poor performance of BN on layers that are excessively large compared
to the batch size. The Kin8nm and Naval datasets likely have τoutput = 0.1 in the wrong scale, which
explains why all the methods show similar results for these two datasets.

22

Table 12: Predictive Performance on Ten Regression Datasets (Root Mean Squared Error)

Dataset

Size

Features,
Targets

Boston

506

13, 1

Concrete

1,030

Energy

768

Kin8nm

8,192

8, 1

8, 2

8, 1

Naval

11,934

16, 2

Power

9,568

4, 1

Protein

45,730

9, 1

WineRed

1,599

11, 1

Yacht

308

6, 1

Year

515,345 90, 1

Avg. Test RMSE and Std. Error

ECMP

EMP

Dropout Ensemble

3.48
±0.18
5.61
±0.14
1.35
±0.06
0.08
±0.00
0.00
±0.00
4.24
±0.05
1.95
±0.06
0.60
±0.02
1.59
±0.23
10.27
±N/A

3.56
±0.22
5.64
±0.15
1.24
±0.04
0.08
±0.00
0.00
±0.00
4.29
±0.05
2.00
±0.07
0.62
±0.02
1.60
±0.28
12.50
±N/A

3.97
±0.26
7.06
±0.19
2.63
±0.05
0.08
±0.00
0.01
±0.00
4.07
±0.04
2.01
±0.07
0.63
±0.01
12.90
±1.26
8.47
±N/A

4.29
±0.27
8.81
±0.15
3.40
±0.31
0.08
±0.00
0.01
±0.00
4.04
±0.04
2.24
±0.06
0.88
±0.06
29.48
±5.14
8.69
±N/A

Table 13: Predictive Performance on Ten Regression Datasets (Log Predictive Density)

Dataset

Size

Features,
Targets

Boston

506

13, 1

Concrete

1,030

Energy

768

Kin8nm

8,192

8, 1

8, 2

8, 1

Naval

11,934

16, 2

Power

9,568

4, 1

Protein

45,730

9, 1

WineRed

1,599

11, 1

Yacht

308

6, 1

Year

515,345 90, 1

Avg. Test LPD and Std. Error

ECMP

EMP

Dropout Ensemble

-2.70
±0.07
-3.59
±0.08
-2.15
±0.01
-2.07
±0.00
-2.07
±0.00
-2.99
±0.02
-2.27
±0.01
-2.09
±0.00
-2.22
±0.05
-6.66
±N/A

-2.92
±0.11
-4.60
±0.14
-2.42
±0.01
-2.07
±0.00
-2.07
±0.00
-2.90
±0.01
-2.27
±0.01
-2.09
±0.00
-10.79
±1.66
-5.66
±N/A

-2.81
±0.07
-5.09
±0.13
-2.58
±0.03
-2.07
±0.00
-2.07
±0.00
-2.87
±0.01
-2.33
±0.01
-2.14
±0.01
-5.24
±0.36
-4.47
±N/A

-2.65
±0.05
-3.46
±0.07
-2.19
±0.01
-2.07
±0.00
-2.07
±0.00
-2.95
±0.02
-2.26
±0.01
-2.09
±0.00
-2.25
±0.02
-5.70
±N/A

23

E GPU Memory Analysis Details

In this section, we describe how we created Figure 1. After examining several websites, we decided
to use TechPowerUp [64] due to its breadth of information and relatively accurate GPU release
dates (speciﬁed in days rather than months). We used a series of HTTP requests for different GPU
generations to collect all relevant data. After discarding data older than 15 years, we obtained a total
of 1599 unique GPUs. This number is so high because it includes mobile GPUs, desktop GPUs, and
workstation GPUs.

We converted the textual representation of each GPU’s release date into an integral timestamp, and
then plotted this against each GPU’s total RAM. A more ﬁne-grained analysis might separate different
types of GPUs, or compute the price-per-GB to distinguish inexpensive from high-end GPUs, but we
wanted to get an overall idea of the memory trend. It clearly grows exponentially. Fitting the model
[RAM ] = 2α[Y ear]+β results in α ≈ 1
2.8 , meaning the doubling period is every 2.8 years, but this
model visually does not approximate the earlier GPUs very well. We decided to add an additional
intercept to ﬁt the model [RAM ] = 2α[Y ear]+β + γ, and here α ≈ 1

3.2 , as shown in Figure 1.

Our analysis is similar to the well-known Moore’s Law, which observes that the number of transistors
that can be placed in an integrated circuit doubles about every two years. (The transistors also
become faster, hence real computing power doubles every 18 months.) Denser silicon can beneﬁt
GPU RAM as well, because this memory (DRAM) stores each bit in a single capacitor. Increasing
the number of capacitors has a near-linear affect on the amount of bits the RAM can store — a
logarithmic proportion of the silicon must be dedicated to addressing the bits, which are arranged
in banks and must be refreshed periodically to prevent capacitors from losing their charge. It is
interesting that we observe RAM capacity doubling every three years, somewhat slower than CPU
speed increases, but less research effort is likely dedicated to shrinking capacitors compared with
transistors. Furthermore, memory requires several layers of cache to be useful (even in GPUs), which
requires some processor/motherboard co-design and may also contribute to the longer doubling time.

Although Moore’s Law has started to break down recently because physical limits are being reached,
the observation that technological capabilities grow exponentially is still sound; research is pushing
to use additional silicon for other purposes, such as massively parallel CPU cores and special-purpose
hardware (of which GPUs are an early example). As deep learning grows in signiﬁcance, we are even
starting to see special-purpose hardware for it, such as Google’s Tensor Processing Units (TPUs).
We believe that the pressures of increasingly large models will drive new hardware to include more
and more memory. Even if access latency is increased, neural-network hardware may move towards
an even deeper memory hierarchy, much as traditional operating systems embrace swap memory to
increase their capabilities. In the past few decades, the clock speed of individual cores was the most
signiﬁcant metric of computing progress, but as deep learning and other frontiers of computer science
utilize increasing parallelization, we hypothesize that memory capacity will be the more relevant
metric in the future of computing.

24

