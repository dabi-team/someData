0
2
0
2

t
c
O
2
2

]

G
L
.
s
c
[

3
v
4
8
0
8
0
.
6
0
0
2
:
v
i
X
r
a

Neural Execution Engines: Learning to Execute
Subroutines

Yujun Yan∗
The University of Michigan
yujunyan@umich.edu

Kevin Swersky
Google Research
kswersky@google.com

Danai Koutra
The University of Michigan
dkoutra@umich.edu

Parthasarathy Ranganathan, Milad Hashemi
Google Research
{parthas, miladh}@google.com

Abstract

A signiﬁcant effort has been made to train neural networks that replicate algorith-
mic reasoning, but they often fail to learn the abstract concepts underlying these
algorithms. This is evidenced by their inability to generalize to data distributions
that are outside of their restricted training sets, namely larger inputs and unseen
data. We study these generalization issues at the level of numerical subroutines that
comprise common algorithms like sorting, shortest paths, and minimum spanning
trees. First, we observe that transformer-based sequence-to-sequence models can
learn subroutines like sorting a list of numbers, but their performance rapidly
degrades as the length of lists grows beyond those found in the training set. We
demonstrate that this is due to attention weights that lose ﬁdelity with longer se-
quences, particularly when the input numbers are numerically similar. To address
the issue, we propose a learned conditional masking mechanism, which enables
the model to strongly generalize far outside of its training range with near-perfect
accuracy on a variety of algorithms. Second, to generalize to unseen data, we show
that encoding numbers with a binary representation leads to embeddings with rich
structure once trained on downstream tasks like addition or multiplication. This
allows the embedding to handle missing data by faithfully interpolating numbers
not seen during training.

Introduction

1
Neural networks have become the preferred model for pattern recognition and prediction in perceptual
tasks and natural language processing [13, 4] thanks to their ﬂexibility and their ability to learn
complex solutions. Recently, researchers have turned their attention towards imbuing neural networks
with the capability to perform algorithmic reasoning, thereby allowing them to go beyond pattern
recognition and logically solve more complex problems [9, 12, 10, 14]. These are often inspired by
concepts in conventional computer systems (e.g., pointers [29], external memory [22, 9]).

Unlike perceptual tasks, where the model is only expected to perform well on a speciﬁc distribution
from which the training set is drawn, in algorithmic reasoning the goal is to learn a robust solution
that performs the task regardless of the input distribution. This ability to generalize to arbitrary
input distributions—as opposed to unseen instances from a ﬁxed data distribution—distinguishes the
concept of strong generalization from ordinary generalization. To date, neural networks still have
difﬁculty learning algorithmic tasks with strong generalization [28, 8, 12].

∗Work completed during an internship at Google.

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
In this work, we study this problem by learning to imitate the composable subroutines that form
the basis of common algorithms, namely selection sort, merge sort, Dijkstra’s algorithm for shortest
paths, and Prim’s algorithm to ﬁnd a minimum spanning tree. We choose to focus on subroutine
imitation as: (1) it is a natural mechanism that is reminiscent of how human developers decompose
problems (e.g., developers implement very different subroutines for merge sort vs. selection sort), (2)
it supports introspection to understand how the network may fail to strongly generalize, and (3) it
allows for providing additional supervision to the neural network if necessary (inputs, outputs, and
intermediate state).

By testing a powerful sequence-to-sequence transformer model [26] within this context, we show that
while it is able to learn subroutines for a given data distribution, it fails to strongly generalize as the
test distribution deviates from the training distribution. Subroutines often operate on strict subsets of
data, and further analysis of this failure case reveals that transformers have difﬁculty separating “what”
to compute from “where” to compute, manifesting in attention weights whose entropy increases
over longer sequence lengths than those seen in training. This, in turn, results in misprediction and
compounding errors.

Our solution to this problem is to leverage the transformer mask. First, we have the transformer
predict both a value and a pointer. These are used as the current output of the subroutine, and the
pointer is also used as an input to a learned conditional masking mechanism that updates the encoder
mask for subsequent computation. We call the resulting architecture a Neural Execution Engine
(NEE), and show that NEEs achieve near-perfect generalization over a signiﬁcantly larger range of
test values than existing models. We also ﬁnd that a NEE that is trained on one subroutine (e.g.,
comparison) can be used in a variety of algorithms (e.g., Dijkstra, Prim) as-is without retraining.

Another essential component of algorithmic reasoning is representing and manipulating numbers
[30]. To achieve strong generalization, the employed number system must work over large ranges
and generalize outside of its training domain (as it is intractable to train the network on all integers).
In this work, we leverage binary numbers, as binary is a hierarchical representation that expands
exponentially with the length of the bit string (e.g., 8-bit binary strings represent exponentially more
data than 7-bit binary strings), thus making it possible to train and test on signiﬁcantly larger number
ranges compared to prior work [8, 12]. We demonstrate that the binary embeddings trained on down-
stream tasks (e.g., addition, multiplication) lead to well-structured and interpretable representations
with natural interpolation capabilities.

2 Background

2.1 Transformers and Graph Attention Networks

Transformers are a family of models that represent the current state-of-the-art in sequence learn-
ing [26, 4, 19]. Given input token sequences x1, x2, . . . , xL1 ∈ X and output token sequences
y1, y2, . . . , yL2 ∈ Y, where xi, yj ∈ Z+, a transformer learns a mapping X → Y. First, the tokens
are individually embedded to form ˆxi, ˆyj ∈ Rd. The main module of the transformer architecture
is the self-attention layer, which allows each element of the sequence to concentrate on a subset of
the other elements.2 Self-attention layers are followed by a point-wise feed-forward neural network
layer, forming a self-attention block. These blocks are composed to form the encoder and decoder of
the transformer, with the outputs of the encoder being used as queries and keys for the decoder. More
details can be found in [26].

An important component for our purposes is the self-attention mask. This is used to prevent certain
positions from propagating information to other positions. A mask, b, is a binary vector where the
value bi = 0 indicates that the ith input should be considered, and bi = 1 indicates that the ith input
should be ignored. The vector b is broadcast to zero out attention weights of ignored input numbers.
Typically this is used for decoding, to ensure that the model can only condition on past outputs during
sequential generation. Graph Attention Networks [27] are essentially transformers where the encoder
mask reﬂects the structure of a given graph. In our case, we will consider masking in the encoder as
an explicit way for the model to condition on the part of the sequence that it needs at a given point in
its computation, creating a dynamic graph. We ﬁnd that this focuses the attention of the transformer
and is a critical component for achieving strong generalization.

2We do not use positional encodings (which we found to hurt performance) and use single-headed attention.

2

Figure 1: (Top) Pseudocode for four common algorithms: selection sort, merge sort, Dijkstra’s algorithm for
shortest paths, and Prim’s algorithm for minimum spanning tree. Blue/red/yellow boxes highlight comparison,
arithmetic, and difﬁcult pointer manipulation subroutines respectively. (Bottom) Flow charts of the algorithms
with NEE components implementing the subroutines.

2.2 Numerical Subroutines for Common Algorithms

We draw examples from different algorithmic categories to frame our exploration into the capability
of neural networks to perform generalizable algorithmic reasoning. Figure 1 shows the pseudocode
and subroutines for several commonly studied algorithms; speciﬁcally, selection sort, merge sort,
Dijkstra’s algorithm for shortest paths, and Prim’s algorithm to ﬁnd a minimum spanning tree. These
algorithms contain a broad set of subroutines that we can classify into three categories:

• Comparison subroutines are those involving a comparison of two or more numbers.

• Arithmetic subroutines involve transforming numbers through arithmetic operations (we

focus on addition in Figure 1, but explore multiplication later).

• Pointer manipulation requires using numerical values (pointers) to manipulate other data
values in memory. One example is shown for merge sort, which requires merging two sorted
lists. This could be trivially done by executing another sort on the concatenated list, however
the aim is to take advantage of the fact that the two lists are sorted. This involves maintaining
pointers into each list and advancing them only when the number they point to is selected.

2.3 Number Representations

Beyond subroutines, numerics are also critically important in teaching neural networks to learn algo-
rithms. Neural networks generally use either categorical, one-hot, or integer number representations.
Prior work has found that scalar numbers have difﬁculty representing large ranges [25] and that
binary is a useful representation that generalizes well [12, 21]. We explore embeddings of binary
numbers as a form of representation learning, analogous to word embeddings in language models
[15], and show that they learn useful structure for algorithmic tasks.

3

Neural Execution Enginemin_elementappend()sorted_listdatalearned_maskNeural Execution Enginepartially_sorted_datareshape()datalearned_maskend-startNeural Execution Enginedistsgraph.adjshortest_pathNeural Execution EngineNeural Execution Engineappend()anchor_node(learned_mask)min_distpossible_pathsshortest_pathnodesanchor_nodemerge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path)    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  merge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path)    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  merge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path(anchor_node))    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  minimum_spanning_tree(graph, source_node, node_val):    mst_nodes = [ ]    mst_weights = [ ]    anchor_node = source_node    res_nodes = graph.get_nodes()-mst_nodes        while node_list:        adj_list = graph.adj(anchor_node)        node_val(res_nodes) = min(node_val(res_nodes),                                                     adj_list(res_nodes))        anchor_node, min_weight = min(node_val(res_nodes))                mst_nodes.append(anchor_node)        mst_weights.append(min_weight)        res_nodes.delete(anchor_node)        return mst_nodes, mst_weightsNeural Execution EngineNeural Execution Enginegraph.adjanchor_nodenode_valres_nodes(learned_mask)anchor_noderes_nodesmin_weightappend()mst_nodesmst_weightsNeural Execution Enginemin_elementappend()sorted_listdatalearned_maskNeural Execution Enginepartially_sorted_datareshape()datalearned_maskend-startNeural Execution Enginedistsgraph.adjshortest_pathNeural Execution EngineNeural Execution Engineappend()anchor_node(learned_mask)min_distpossible_pathsshortest_pathnodesanchor_nodemerge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path)    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  merge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path)    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  merge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path(anchor_node))    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  Neural Execution Enginemin_elementappend()sorted_listdatalearned_maskNeural Execution Enginepartially_sorted_datareshape()datalearned_maskend-startNeural Execution Enginedistsgraph.adjshortest_pathNeural Execution EngineNeural Execution Engineappend()anchor_node(learned_mask)min_distpossible_pathsshortest_pathnodesanchor_nodemerge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path)    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  merge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path)    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  merge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path(anchor_node))    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  merge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    return merge(data, start, mid, end)Neural Execution Enginemin_elementappend()sorted_listdatalearned_maskNeural Execution Enginepartially_sorted_datareshape()datalearned_maskend-startNeural Execution Enginedistsgraph.adjshortest_pathNeural Execution EngineNeural Execution Engineappend()anchor_node(learned_mask)min_distpossible_pathsshortest_pathnodesanchor_nodemerge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path)    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  merge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path)    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  merge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path(anchor_node))    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  Neural Execution Enginemin_elementappend()sorted_listdatalearned_maskNeural Execution Enginepartially_sorted_datareshape()datalearned_maskend-startNeural Execution Enginedistsgraph.adjshortest_pathNeural Execution EngineNeural Execution Engineappend()anchor_node(learned_mask)min_distpossible_pathsshortest_pathnodesanchor_nodemerge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path)    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  merge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path)    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  merge_sort(data, start, end):  if (start < end):    mid = (start + end ) / 2    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)selection_sort(data):  sorted_list = []  while (len(data) > 0):    min_index, min_element = ﬁnd_min(data)        data.delete(min_index)    sorted_list.append(min_element)  return sorted_listﬁnd_min(data):    min_element = -1    min_index = -1    for index, element in enumerate(data):      if (element < min_element):        min_element = element        min_index = index    return [min_index, min_element]shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj(anchor_node),                                          shortest_path(anchor_node))    shortest_path = min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path)    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  shortest_path(graph, source_node, shortest_path):  dists = []  nodes = []  anchor_node = source_node  node_list = graph.get_nodes()  while node_list:    possible_paths = sum(graph.adj[anchor_node],                                          shortest_path)    shortest_path = ele_min(possible_paths, shortest_path)    anchor_node, min_dist = min(shortest_path(node_list))    node_list.delete(anchor_node)    nodes.append(anchor_node)    dists.append(min_dist)  return dists, nodes  minimum_spanning_tree(graph, source_node, node_val):    mst_nodes = [ ]    mst_weights = [ ]    anchor_node = source_node    res_nodes = graph.get_nodes()-mst_nodes        while node_list:        adj_list = graph.adj(anchor_node)        node_val(res_nodes) = min(node_val(res_nodes),                                                     adj_list(res_nodes))        anchor_node, min_weight = min(node_val(res_nodes))                mst_nodes.append(anchor_node)        mst_weights.append(min_weight)        res_nodes.delete(anchor_node)        return mst_nodes, mst_weightsNeural Execution EngineNeural Execution Enginegraph.adjanchor_nodenode_valres_nodes(learned_mask)anchor_noderes_nodesmin_weightappend()mst_nodesmst_weights3 Neural Execution Engines

A neural execution engine (NEE) is a transformer-based network that takes as input binary numbers
and an encoding mask, and outputs either data values, a pointer, or both 3. Here, we consider input
and output data values to be n-bit binary vectors, or a sequence of such vectors, and the output pointer
to be a one-hot vector of the length of the input sequence. The pointer is used to modify the mask the
next time the NEE is invoked. A NEE is essentially a graph attention network [27] that can modify
its own graph, resulting in a new decoding mechanism.

The NEE architecture is shown in Figure 2. It is a modiﬁcation of the transformer architecture. Rather
than directly mapping one sequence to another, a NEE takes an input sequence and mask indicating
which elements are relevant for computation. It encodes these using a masked transformer encoder.
The decoder takes in a single input, the zero vector, and runs the transformer decoder to output a
binary vector corresponding to the output value. The last layer of attention in the last decoder block
is used as a pointer to the next region of interest for computation. This and the original mask vector
are fed into a ﬁnal temporal convolution block that outputs a new mask. This is then applied in a
recurrent fashion until there are no input elements left to process (according to the mask). In the
remainder of this section, we go into more detail on the speciﬁc elements of the NEE.

Conditional Masking The encoder of
a NEE takes as input both a set of values
and a mask, which is used to force the
encoder to ignore certain inputs. We use
the output pointer of the decoder to mod-
ify the mask for a subsequent call of the
encoder. In this way, the inputs represent
a memory state, and the mask represents
a set of pointers into the memory. A NEE
effectively learns where to focus its atten-
tion for performing computation.

Learning an attention mask update is a
challenging problem in general, as the
mask updates themselves also need to
strongly generalize. In many algorithms,
including the ones considered here, the
mask tends to change within a local
neighbourhood around the point of inter-
est (the element pointed to by the NEE).
For example, in iterative algorithms, the
network needs to attend to the next ele-
ment to be processed which is often close
to the last element that was processed.

Figure 2: NEE architecture. The ﬁrst step of decoding is equiva-
lent to passing in a trainable decoder vector.

We therefore use a small temporal (1D) convolutional neural network (CNN), T . The CNN accepts
as input the current mask vector bI and the one-hot encoded output pointer bP from the decoder.
It outputs the next mask vector ˆb. Mathematically, ˆb = σ(T (bI (cid:107) bP )) = σ(F (C(N (bI (cid:107) bP ))),
where (cid:107) denotes concatenation, σ denotes sigmoid function, F , C and N represent the point-wise
feed-forward layer, 1D convolutional layer and feature-wise normalization layer, respectively. At
inference, we simply choose the argmax of the pointer output head to produce bP .

The intuition behind this design choice is that through convolution, we enforce a position ordering to
the input by exchanging information among the neighbourhoods. The convnet is shift invariant and
therefore amenable to generalizing over long sequences. We also experimented with a transformer
encoder-decoder, using an explicit positional encoding, however we found that this often fails due to
the difﬁculty in dealing with unseen positions.

Bitwise Embeddings As input to a NEE, we embed binary vectors using a linear projection. This
is equivalent to deﬁning a learnable vector for each bit position, and then summing these vectors

3Code for this paper can be found at https://github.com/Yujun-Yan/Neural-Execution-Engines

4

BitwiseEmbeddingMasked attentionFeed ForwardAdd & NormAdd & NormInputsequenceNxInputmask0 vectorAttentionAdd & NormAttentionAdd & NormFeed ForwardAdd & NormLinearValuePointerNxLastlayerSigmoidConcatFeed ForwardNormalize1D convolutionOutput maskSigmoidFigure 3: Sorting performance of transformers trained
on sequences of up to length 8.

Figure 4: Visualizing decoder attention weights.
Attention is over each row. Transformer attention
saturates as the output sequence length increases,
while NEE maintains sharp attention.

elementwise, modulated by the value of their corresponding bit. That is, given an embedding vector
vi for each bit i, for an n-bit input vector x, we would compute ˆx = (cid:80)n
i=1 xivi. For example,
emb(1001) = v0+v3.

Two important tokens for our purposes are start s and end e. These are commonly used in natural
language data to denote the start and end of a sequence. We use s as input to the decoder, and e to
denote the end of an input sequence. This allows us to train a NEE to learn to emit e when it has
completed an algorithm.

Additionally, we also use these symbols to deﬁne both 0 and ∞. These concepts are important
for many algorithms, particularly for initialization. For addition, we require that 0 + x = x and
∞ + x = ∞. As a more concrete example, in shortest path, the distance from the source node to
other nodes in the graph can be denoted by ∞ since they’re unexplored. We set s = 0 and train the
model to learn an embedding vector for e such that e = ∞. That is, the model will learn e > x for all
x (cid:54)= e and that e + x = e.

4 Current Limitations of Sequence to Sequence Generalization

Learning Selection Sort We ﬁrst study how well a state-of-the-art transformer-based sequence
to sequence model (Section 2.1) learns selection sort. Selection sort involves iteratively ﬁnding
the minimum number in a list, removing that number from the list, and adding it to the end of the
sorted list. We model selection sort using sequence to sequence learning [24] with input examples
of unsorted sequences (L ≤ 8 integers, each within the range [0, 256)) and output examples of the
correctly sorted sequences. To imitate a sorting subroutine, we provide supervision on intermediate
states: at each stage of the algorithm the transformer receives the unsorted input list, the partially
sorted output list, and the target number. The numbers used as inputs and outputs to a vanilla
transformer are one-hot encoded4. The decoder uses a greedy decoding strategy.

The performance of this vanilla transformer, evaluated as achieving an exact content and positional
match to the correct output example, is shown in Figure 3. The vanilla transformer is able to learn to
sort the test-length distribution (at 8 numbers) reasonably well, but performance rapidly degrades as
the input data distribution shifts to longer sequences and by 100 integers, performance is under 10%.

One of the main issues we found is that the vanilla transformer has difﬁculty distinguishing close
numbers (e.g., 1 vs. 2, 53 vs. 54)5. We make a number of small architectural modiﬁcations in order
to boost its accuracy in this regime, including but not limited to using a binary representation for the
inputs and outputs. We describe these modiﬁcations, and provide ablations in Appendix A.2.

As Figure 3 also shows, given our modiﬁcations to a vanilla transformer, sequence-to-sequence
transformers are capable of learning this algorithm on sequences of length ≤ 8 with a high degree of
accuracy. However, the model still fails to generalize to longer sequences than those seen at training
time, and performance sharply drops as the sequence length increases.

4We also experimented with one-hot 256-dimensional outputs for other approaches used in the paper with

similar results. See the supplementary material.

5Throughout this work, the preponderance of errors are regenerated numbers that are off by small differences.

Therefore, our test distribution consists of 60% random numbers and 40% numbers with small differences

5

NEE100.00%Modified98.22%0   20   40   60   80   1000   20   40   60   80  1000   20   40   60   80  100(a) Fuzzy Attention(seq2seq)(b) Clear Attention(Selection Sort NEE) Attention Fidelity To understand why performance degrades as the test sequences get longer, we
plot the attention matrix of the last layer in the decoder (Figure 4a). During decoding, the transformer
accurately attends to the ﬁrst few numbers in the sequence (distinct dots in the chart) but the attention
distribution becomes “fuzzy” as the number of decoding steps increases beyond 8 numbers, often
resulting in the same number being repeatedly predicted.

Since the transformer had difﬁculty clearly attending to values beyond the training sequence length,
we separate the supervision of where the computation needs to occur from what the computation
is. Where the computation needs to occur is governed by the transformer mask. To avoid overly
soft attention scores, we aim to restrict the locations in the unsorted sequence where the transformer
could possibly attend in every iteration. This is accomplished by producing a conditional mask which
learns to ignore the data elements that have already been appended to the sorted_list and feed
that mask back into the transformer (shown on the bottom-left side of Figure 1). Put another way,
we have encoded the current algorithmic state (the sorted vs. unsorted list elements) in the attention
mask rather than the current decoder output.

This modiﬁcation separates the control (which elements should be considered) from the computation
itself (ﬁnd the minimum value of the list). This allows the transformer to learn output logits of much
larger magnitude, resulting in sharper attention, as shown in Figure 4b. Our experimental results
consequently demonstrate strong generalization, sorting sequences of up to length 100 without error,
as shown in Figure 3. Next, we evaluate this mechanism on a variety of other algorithms.

5 Experiments

In this section, we evaluate using a NEE to execute various algorithms, including selection sort and
merge sort, as well as more complex graph algorithms like shortest path and minimum spanning tree.

5.1 Executing Subroutines

Selection Sort Selection sort (de-
scribed in Sec. 4) is translated to the
NEE architecture in Figure 1. The
NEE learns to ﬁnd the minimum of
the list, and learns to iteratively up-
date the mask by setting the mask
value of the location of the minimum
to 1. We show the results for selec-
tion sort in Figure 3 and Table 1, the
NEE is able to strongly generalize to
inputs of length 100 with near-perfect
accuracy.

Table 1: Performance of different tasks on variable sizes of test exam-
ples (trained with examples of size 8). (cid:63)Two exceptions: accuracy for
graphs of 92 nodes and 97 nodes are 99.99 and 99.98, respectively.
We run the evaluation once, and minimally tune hyper-parameters.

Accuracy

Sizes

25

50

75

100

Selection sort

100.00

100.00

100.00

100.00

Merge sort

100.00

100.00

100.00

Shortest path

100.00

100.00

100.00

100.00
100.00(cid:63)

Minimum spanning tree

100.00

100.00

100.00

100.00

Merge Sort The code for one implementation of merge-sort is shown in Figure 1. It is broadly
broken up into two subroutines, data decomposition (merge_sort) and an action (merge). Every call
to merge_sort divides the list in half until there is one element left, which by deﬁnition is already
sorted. Then, merge unrolls the recursive tree, combining every 2 elements (then every 4, 8, etc.)
until the list is fully sorted. Recursive algorithms like merge sort generally consist of these two steps
(the “recursive case" and the “base case").

We focus on the merge function, as it involves challenging pointer manipulation. For two sorted
sequences that we would like to merge, we concatenate them and delimit them using the e token:
[seq1, e, seq2, e]. Each sequence has a pointer denoting the current number being considered, rep-
resented by setting that element to 0 in the mask and all other elements in that sequence to 1, e.g.,
binit = [0 1 1 0 1 1] for two length-2 sequences delimited by e tokens. The smaller of the two
currently considered numbers is chosen as the next number in the merged sequence. The pointer for
the chosen sequence is advanced by masking out the current element in the sequence and unmasking
the next, and the subroutine repeats.

More concretely, the NEE in Figure 1 implements this computation. Every timestep, the model
outputs the smallest number from the unmasked numbers and the two positions to be considered next.
When the pointers both point to e, then the subroutine returns. Table 1 demonstrates that the NEE is

6

able to strongly generalize on merge sort over long sequences (up to length 100) while trained on
sequences of length smaller or equal to 8.

Composable Subroutines: Shortest Path While both merge sort and selection sort demonstrated
that a NEE can compose the same subroutine repeatedly to sort a list with perfect accuracy, programs
often compose multiple different subroutines to perform more complex operations. In this section,
we study whether multiple NEEs can be composed to execute a more complicated algorithm.

To that end, we study a graph algorithm, Dijkstra’s algorithm to ﬁnd shortest paths, shown in Figure 1.
The algorithm consists of four major steps:

(1) Initialization: set the distance from the source node to the other nodes to inﬁnity, then append
them into a queue structure for processing; (2) Compute newly found paths from the source node
to all neighbours of the selected node; (3) Update path lengths if they are smaller than the stored
lengths; (4) Select the node with the smallest distance to the source node and remove it from the
queue. The algorithm repeats steps (2)–(4) as long as there are elements in the queue.

Computing Dijkstra’s algorithm requires the NEEs to learn the three corresponding subroutines
(Figure 1). Finding the minimum between the possible_paths and shortest_path as well as the
minimum current shortest_path can be accomplished through the NEE trained to accomplish the
same goal for sorting. The new challenge is to learn a numerical subroutine, addition. This process is
described in detail in Section 5.2.

We compose pre-trained NEEs to perform Dijkstra’s algorithm (Figure 1). The NEEs themselves
strongly generalize on their respective subroutines, therefore they also strongly generalize when
composed to execute Dijkstra’s algorithm. This persists across a wide range of graph sizes. A
step-by-step view is shown in the Appendix. The examples are Erd˝os-Rényi random graphs. We train
on graphs with up to 8 nodes and test on graphs of up to 100 nodes, with 100 graphs evaluated at
each size. Weights are randomly assigned within the allowed 8-bit number range. We evaluate the
prediction accuracy on the ﬁnal output (the shortest path of all nodes to the source nodes) and achieve
100% test accuracy with graph sizes up to 100 nodes (Table 1).

Composable Subroutines: Minimum Spanning Tree As recent work has evaluated generaliza-
tion on Prim’s algorithm [28], we include it in our evaluation. This algorithm is shown in Figure 1:
We compose pre-trained NEEs to compute the solution, training on graphs of 8 nodes and testing on
graphs of up to 100 nodes. The graphs are Erd˝os-Rényi random graphs. We evaluate the prediction
accuracy on the whole set, which means the prediction is correct if and only if the whole set predicted
is a minimum spanning tree. Table 1 shows that we achieve strong generalization on graphs of up to
100 nodes, whereas [28] sees accuracy drop substantially at this scale. We also test on other graph
types (including those from [28]) and perform well. Details are provided in Appendix A.3.

5.2 Number representations

Learning Arithmetic A core component of many algorithms, is simple addition. While neural
networks internally perform addition, our goal here is to see if NEEs can learn an internal number
system using binary representations. This would allow it to gracefully handle missing data and can
serve as a starting point towards more complex numerical reasoning. To gauge the relative difﬁculty
of this versus other arithmetic tasks, we also train a model for multiplication.

The results are shown in Table 2. Training on the entire 8-bit number range (256 numbers) and testing
on unseen pairs of numbers, the NEE achieves 100% accuracy. In addition to testing on unseen pairs,
we test performance on completely unseen numbers by holding out random numbers during training.
These results are also shown in Table 2, and the NEE demonstrates high performance even while
training on 25% of the number range (64 numbers). This is a promising result as it suggests that
we may be able to extend the framework to signiﬁcantly larger bit vectors, where observing every

Table 2: NEE 8-bit addition performance.

Training Numbers

256

224

192

128

89

76

64

Accuracy%

100.00

100.00

100.00

100.00

100.00

99.00

96.53

7

Figure 5: 3D PCA visualization of learned bitwise embeddings for different numeric tasks. The embeddings
exhibit regular, task-dependent structure, even when most numbers have not been seen in training (c).

number in training is intractable. In the case of multiplication, we train on 12-bit numbers and also
observe 100% accuracy.

To understand the number system that the NEE has learned, we visualize the structure of the learned
embeddings using a 3-dimensional PCA projection, and compare the embeddings learned from
sorting, multiplication, and addition, shown in Figure 5 (a), (b), and (c) respectively. For the addition
visualization, we show the embeddings with 65% of the numbers held out during training. In Figure 5
(a) and (b), each node is colored based on the number it represents; in Figure 5 (c), held-out numbers
are marked red. We ﬁnd that a highly structured number system has been learned for each task. The
multiplication and addition embeddings consist of multiple lines that exhibit human-interpretable
patterns (shown with arrows in Figure 5 (b) and (c)). The sorting embeddings exhibit many small
clusters, and the numbers placed in a "Z" curve increase by 1 (shown with arrows in Figure 5 (a)).
On held out numbers for the addition task, NEE places the embeddings of the unseen numbers in
their logical position, allowing for accurate interpolation. More detailed visualizations are provided
in Appendix A.4.

6 Related Work

Learning subroutines
Inspired by computing architectures, there have been a number of proposals
for neural networks that attempt to learn complex algorithms purely from weak supervision, i.e.,
input/output pairs [9, 10, 12, 14, 29]. Theoretically, these are able to represent any computable
function, though practically they have trouble with sequence lengths longer than those seen in training
and do not strongly generalize. Unlike these networks that are typically trained on scalar data values
in limited ranges, focus purely on pointer arithmetic, or contain non-learnable subroutines, we train
on signiﬁcantly larger (8-bit) number ranges, and demonstrate strong generalization in a wide variety
of algorithmic tasks.

Recent work on neural execution [28] explicitly models intermediate execution states (strong supervi-
sion) in order to learn graph algorithms. They also ﬁnd that the entropy of attention weights plays a
signiﬁcant role in generalization, and address the problem by using max aggregation and entropy
penalties [28]. Despite this solution, a drop in performance is observed over larger graphs, including
with Prim’s algorithm. On the other hand, in this work, we demonstrate strong generalization on
Prim’s algorithm on much larger graphs than those used in training (Section 5). NEE has the added
beneﬁt that it does not require additional heuristics to learn a low-entropy mask—it naturally arises
from conditional masking.

Work in neural program synthesis [16, 18, 5, 1, 6]—which uses neural networks with the goal
of generating and ﬁnding a “correct” program such that it will generalize beyond the training
distribution—has also employed strong supervision in the form of execution traces [20, 21, 2].
For instance, [2] uses execution traces with tail recursion (where the subroutines call themselves).

8

160161162163164165166167168169178179+1+1+1250200150100500(a) Sorting                                    (b) Multiplication                      (c) Addition with 65% holdout60226230228225229227231224646266+2+2Not seen in the trainingSeen in the training255253254252243241242240239237238236227225226+1InfInfThough [2] shows that recursion leads to improved generalization, their model relies on predeﬁned
non-learnable operations like SHIFT & SWAP.

The computer architecture community has also explored using neural networks to execute approximate
portions of algorithms, as there could be execution speed and efﬁciency advantages [7]. Unlike
traditional microprocessors, neural networks and the NEE are a non-von Neumann computing
architecture [30]. Increasing the size of our learned subroutines could allow neural networks and
learned algorithms to replace or augment general purpose CPUs on speciﬁc tasks.

Learning arithmetic Several works have used neural networks to learn number systems for per-
forming arithmetic, though generally on small number ranges [3]. For example, [17] directly embeds
integers in the range [−10, 10] as vectors and trains these, along with matrices representing relation-
ships between objects. [23] expands on this idea, modeling objects as matrices so that relationships
can equivalently be treated as objects, allowing the system to learn higher-order relationships. [25]
explores the (poor) generalization capability of neural networks on scalar-values inputs outside
of their training range, and develops new architectures that are better suited to scalar arithmetic,
improving extrapolation.

Several papers have used neural networks to learn binary arithmetic with some success [11, 12]. [8]
develops a custom architecture that is tested on performing arithmetic, but trains on symbols in the
range of [1, 12] and does not demonstrate strong generalization. Also, recent work has shown that
graph neural networks are capable of learning from 64-bit binary memory states provided execution
traces of assembly code, and observes that this representation numerically generalizes better than
one-hot or categorical representations [21]. Going beyond this, we directly explore computation with
binary numbers, and the resultant structure of the learned representations.

7 Conclusion

We propose neural execution engines (NEEs), which leverage a learned mask to imitate the function-
ality of larger algorithms. We demonstrate that while state-of-the-art sequence models (transformers)
fail to strongly generalize on tasks like sorting, imitating the smaller subroutines that compose to
form a larger algorithm allows NEEs to strongly generalize across a variety of tasks and number
ranges. There are many natural extensions within and outside of algorithmic reasoning. For example,
one could use reinforcement learning to replace imitation learning, and learn to increase the efﬁciency
of known algorithms, or link the generation of NEE-like models to source code. Growing the sizes of
the subroutines that a NEE learns could allow neural networks to supplant general purpose machines
for execution efﬁciency, since general-purpose machines require individual sequentially encoded
instructions [30]. Additionally, the concept of strong generalization allows us to reduce the size of
training datasets, as a network trained on shorter sequences or small graphs is able to extrapolate to
much longer sequences or larger graphs, thereby increasing training efﬁciency. We also ﬁnd the link
between learned attention masks and strong generalization as an interesting direction for other areas,
like natural language processing.

8 Broader Impact of this Work

This work is a very incremental step in a much broader initiative towards neural networks that can
perform algorithmic reasoning. Neural networks are currently very powerful tools for perceptual
reasoning, and being able to combine this with algorithmic reasoning in a single uniﬁed system
could form the foundation for the next generation of AI systems. True strong generalization has a
number of advantages: strongly generalizing systems are inherently more reliable. They would not be
subject to issues of data imbalance, adversarial examples, or domain shift. This could be especially
useful in many important domains like medicine. Strong generalization can also reduce the size of
datasets required to learn tasks, thereby also providing environmental savings by reducing the carbon
footprint of running large-scale workloads. However, strong generalization could be more susceptible
to inheriting the biases of the algorithms on which they are based. If the underlying algorithm is
based on incorrect assumptions, or limited information, then strong generalization will simply reﬂect
this, rather than correct it.

9

Acknowledgments and Disclosure of Funding

We thank Danny Tarlow and the anonymous NeurIPS reviewers for their insightful feedback. This
work was supported by a Google Research internship and a Google Faculty Research Award.

References

[1] Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging
grammar and reinforcement learning for neural program synthesis. International Conference on Learning
Representations, 2018.

[2] Jonathon Cai, Richard Shin, and Dawn Song. Making neural programming architectures generalize via

recursion. International Conference on Learning Representations, 2017.

[3] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal

transformers. International Conference on Learning Representations, 2019.

[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Association for Computational Linguistics, 2019.

[5] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet
Kohli. Robustﬁll: Neural program learning under noisy i/o. International Conference on Machine Learning,
2017.

[6] Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic machines.

International Conference on Learning Representations, 2019.

[7] Hadi Esmaeilzadeh, Adrian Sampson, Luis Ceze, and Doug Burger. Neural acceleration for general-purpose
approximate programs. In 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture,
2012.

[8] Karlis Freivalds, Em¯ıls Ozolin, š, and Agris Šostaks. Neural shufﬂe-exchange networks-sequence processing

in o (n log n) time. In Advances in Neural Information Processing Systems, 2019.

[9] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401,

2014.

[10] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwi´nska,
Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing
using a neural network with dynamic external memory. Nature, 538(7626):471, 2016.

[11] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In

Advances in neural information processing systems, pages 190–198, 2015.

[12] Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. International Conference on Learning

Representations, 2016.

[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012.

[14] Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. International

Conference on Learning Representations, 2016.

[15] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of words and phrases and their compositionality. In Advances in neural information processing systems,
pages 3111–3119, 2013.

[16] Arvind Neelakantan, Quoc V Le, and Ilya Sutskever. Neural programmer: Inducing latent programs with

gradient descent. International Conference on Learning Representations, 2016.

[17] Alberto Paccanaro and Geoffrey E. Hinton. Learning distributed representations of concepts using linear

relational embedding. IEEE Transactions on Knowledge and Data Engineering, 13(2):232–244, 2001.

[18] Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet
Kohli. Neuro-symbolic program synthesis. International Conference on Learning Representations, 2017.

[19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models

are unsupervised multitask learners. 2019.

[20] Scott Reed and Nando De Freitas. Neural programmer-interpreters. International Conference on Learning

Representations, 2016.

[21] Zhan Shi, Kevin Swersky, Daniel Tarlow, Parthasarathy Ranganathan, and Milad Hashemi. Learning
execution through neural code fusion. International Conference on Learning Representations, 2020.

[22] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in

neural information processing systems, pages 2440–2448, 2015.

10

[23] Ilya Sutskever and Geoffrey E Hinton. Using matrices to model symbolic relationship. In Advances in

neural information processing systems, pages 1593–1600, 2009.

[24] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In

Advances in neural information processing systems, pages 3104–3112, 2014.

[25] Andrew Trask, Felix Hill, Scott E Reed, Jack Rae, Chris Dyer, and Phil Blunsom. Neural arithmetic logic

units. In Advances in Neural Information Processing Systems, 2018.

[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural information processing
systems, 2017.

[27] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.

Graph attention networks. International Conference on Learning Representations, 2018.

[28] Petar Velickovic, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural execution of

graph algorithms. International Conference on Learning Representations, 2020.

[29] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural Information

Processing Systems, pages 2692–2700, 2015.

[30] John Von Neumann. First draft of a report on the EDVAC. IEEE Annals of the History of Computing,

15(4):27–75, 1993.

11

A Appendix

A.1 Hyperparameters

8-bit binary numbers are used in all tasks except the multiplication task, where 12-bit binary numbers
are used. For sorting, we found it sufﬁcient to use bitwise embeddings of dimension d = 16. For
more difﬁcult tasks like addition and multiplication, we found it necessary to increase the dimension
to d = 24 and d = 28, respectively. We used no positional encoding for the sorting tasks, and
single-headed attention for all tasks. The remaining NEE hyperparameters, aside from the changes
described next, were set to their defaults.

Table 3: Architectural and training hyperparameters
Hyperparameters
Number of encoder (decoder) layers
Number of layers in the feed forward network
Number of hidden units in the feed forward network
Mask ﬁlter size
Mask number of ﬁlters
Ratio of residual connection
Dropout rate
Optimizer
Warm-up steps
Learning rate

d · min(

√

Value
6
2
128
3
16
1.5
0.1
Adam
4000
t, t · 4000−1.5)

√

A.2 Sorting ablations

In this section, we extensively study the impact of different factors on the generalization ability of
the transformer model. Speciﬁcally, we focus on attention mask supervision, encoding schemes and
various architectural changes.

Unless otherwise speciﬁed, the task performed in this section is selection sort (Section 4). The
models are trained on 20000 sequences of lengths no longer than 8 and tested on 100 sequences of
various lengths. The numbers are integers in the range [0, 256) and we include an end token into the
number system. The test data consists of 60% examples drawn uniformly, and 40% drawn from a
more difﬁcult distribution, where the numbers are closer in value. The accuracy is measured by the
percentage of correctly predicted numbers (correct position and values).

A.2.1

Impact of supervision on attention masks

Figure 6: Sorting performances of the transformers w/o mask supervision

Figure 6 shows the sorting performance of the transformers w/o mask supervision. It can be seen that
providing supervision on the mask hurts generalization ability during test, as the model can overﬁt on
the attention mask during training.

12

Figure 7: Comparisons of one_hot and binary encoding schemes

A.2.2

Impact of different encoding schemes

Figure 7 shows sorting performances with different encoding schemes. In general, vanilla transformers
with binary encoded output perform better.

A.2.3

Impact of different architectural changes

In Figure 3, we show the performance of a modiﬁed transformer model in a sequence-to-sequence
setup (Section 4). In this section, we will elaborate on the modiﬁcations we made to the transformer,
and how those modiﬁcations affect the generalization performance. We illustrate this by evaluating
the results of selection sort.

We study 3 different data distributions: the ﬁrst is where we train on uniformly random sequences
with tokens in [0, 255] ∪ {e}. The second is a mixed setting, where 60% of the examples are drawn
uniformly, and 40% are drawn from a more difﬁcult distribution, where the numbers are closer in
value. The third is the most difﬁcult setting, where all sequences have numbers that are close to each
other in value.

(a) Vanilla encoder

(b) Modiﬁed encoder

Figure 8: Baseline transformer (a) and modiﬁed transformer (b).

We ablate speciﬁc architectural changes in these settings. The original and modiﬁed encoder are
represented visually in in Figure 8. Speciﬁcally, the architectural choices we test are as follows and
the ones applied in the modiﬁed transformer are checked (in that they provide a net beneﬁt):

• C1: Scaling up the strength of the residual connections by a factor of 1.5 instead of 1. ((cid:88))
• C2: Using an MLP-based attention module [1] instead of using the standard scaled dot

product attention. ((cid:88))

• C3: Symmetrizing the MLP-based attention by ﬂipping the order of the inputs and averaging

the resulting logit values. ((cid:88))

• C4: Sharing the projection layer between the query, key, and value in the attention mecha-

nism. ((cid:88))

13

InputEmbeddingSelf-AttentionLayerFeed-ForwardAdd & NormAdd & NormPositionalEncoding(optional)BitwiseEmbeddingFeed ForwardSelf-attentionFeed ForwardAdd & normAdd & normScaledResidualConnectionPositionalEncoding(optional)InputEmbeddingSelf-AttentionLayerFeed-ForwardAdd & NormAdd & NormPositionalEncoding(optional)InputEmbeddingSelf-AttentionLayerFeed-ForwardAdd & NormAdd & NormPositionalEncoding(optional)InputEmbeddingSelf-AttentionLayerFeed-ForwardAdd & NormAdd & NormPositionalEncoding(optional)BitwiseEmbeddingFeed ForwardSelf-attentionFeed ForwardAdd & normAdd & normScaledResidualConnectionPositionalEncoding(optional)-Shared Projection• C5: Using a binary encoding of input values instead of using a one-hot encoding of the

input values. ((cid:88))

• C6: Using a binary encoding as the input, but without any linear embedding.

We use the following conventions to refer to different transformer variants: all_mod stands for
applying all the checked modiﬁcations, vanilla stands for the original transformer model with one-hot
encoded input and "+" and "-" stand for choosing the corresponding modiﬁcations or keeping the
original structures, respectively.

The test accuracy on sequences of length 8 in the mixed setting, is shown in Table 4. We can see that
the architectural changes help improve performance on these sequences up to near-perfect accuracy.

Table 4: Seq2Seq performance for transformer variants at training length of 8 on mixed test sets.

Models
all_mod
all_mod-C1
all_mod-C2
all_mod-C3
all_mod-C4
all_mod-C5
all_mod+C6
vanilla
vanilla+C5
vanilla+C6

Accuracy @ seq_len = 8
99.00%
95.89%
97.56%
98.33%
98.44%
89.56%
84.78%
93.11%
96.67%
77.11%

In Figure 9, we show the strong generalization performance of the different architectures. While
some changes are able to improve performance in this regime, the performance ultimately drops
steeply as the length of the test sequence increases. This is consistent across all test scenarios and
suggests that standard modiﬁcations on the transformer architecture are unlikely to prevent attention
weights from losing sharpness with longer sequences (Fig. 3).

Here we list out some random and hard examples as well as the corresponding output (containing
some errors) from the vanilla transformer (each number has an independent embedding), which is
commonly used in natural language models. The symbol e represents the end token. It can be seen
that the model makes more mistakes (in bold and italics) with hard examples.

Random examples:

100
181

62
52

114
71

66
254

241
246

1
145

63
118

237
28

Output from vanilla:
63
62
71
52

1
28

Hard examples:

66
118

100
145

114
181

237
246

53
254

132
238

126
239

131
241

129
240

127
243

130
237

128
242

125
244

Output from vanilla:
127
126
240
238

125
237

128
244

129
243

130
e

132
237

e
242

e
e

e
e

e
e

e
e

14

(a) Mixed test sets

(b) Random test sets

(c) Hard test sets

Figure 9: Seq2Seq strong generalization performance on (a) mixed test sets, where test sets consist of 60%
uniformly random examples and 40% hard examples where the numbers are close to each other. (b) uniformly
random test sets. (c) hard test sets, where test sets consist of 100% hard examples where the numbers are close
to each other. All models trained on sequences ≤ 8 and tested up to length 100. Vanilla corresponds to the
original transformer, with bitwise embeddings and all_modiﬁcations_excp_[change] means all modiﬁcations
except a certain change.

15

Next, we will show the performance of transformer variants with a binary encoded output. From
Figure 10, we can see that among all the models with binary encoded output, all_mod performs
the best, which is consistent with the result obtained from one-hot encoded output models. Though
all_mod with one-hot output outperforms all models with binary output, models with binary output
use fewer parameters and scale better to the input data range.

Figure 10: Performances of transformer models (binary output) in mixed test sets

A.3 Graph algorithms tested on different graph types

Prior work [6] has shown that performance on graph algorithms may depend on different types of
graphs. For comparison, we further explore NEE performance on graph algorithms (Dijkstra and
Prim) and we consider two scenarios: (1) Training NEEs with traces from selection sort (and addition)
(2) Training NEEs with traces from corresponding graph algorithms and using Erd˝os-Rényi random
graphs as training graphs. For both scenarios, we use 20000 training sequences/graphs of size 8 and
2000 validation sequences/graphs and test on 100 graphs of the following types with various sizes:

• Erd˝os-Rényi random graphs [3]: each pair of nodes has probability p to form an edge, we

use p uniformly sampled from [0, 1].

• Newman–Watts–Strogatz random graphs [4]: First create a ring of n nodes, where each node
is connected to its k nearest neighbors (or k − 1 neighbors if k is odd). Then for each edge
(u, v) in the original ring, add a new edge (u, w) with probability p. We choose 2 ≤ k ≤ 5
and p ∈ [0, 1].

• D-regular random graphs: every node is connected to other d nodes (nd needs to be even

and 2 ≤ d ≤ n).

• Barabási–Albert random graphs [2]: A graph of n nodes is grown by attaching new nodes
each with m edges that are preferentially attached to existing nodes with high degree. We
choose 2 ≤ m ≤ 5.

We assign random weights to the graphs such that they do not overﬂow the current number system
(integers 0-255). Based on the ﬁndings in Section A.2 that close numbers are hard to identify, thus in
the training data, 50% (20%) are hard examples (weights are very close) when training shortest paths
(minimum spanning tree). All the training graphs are Erd˝os-Rényi random graphs while in the test
graphs, every graph type contributes to 25 graph samples.

16

Table 5: Performance of graph algorithms with mixed graph types. The accuracy of Dijkstra’s shortest paths
is evaluated on the portion of correctly predicted shortest paths from all the other nodes to the source node.
The accuracy of Prim’s minimum spanning tree is evaluated on whether the predicted node sequence forms a
minimum spanning tree and the corresponding edge weights are correct. Training with scenario 1(2) is labeled
with S1(S2) in parentheses.

Accuracy

Sizes

25

50

75

100

Shortest path (S1)
Minimum spanning tree (S1)
Shortest path (S2)
Minimum spanning tree (S2)

100.00

100.00

100.00

100.00

100.00

100.00

100.00

100.00

100.00

100.00

100.00

100.00

99.00

93.00

99.91

92.00

From Table 5, we can see that NEEs are robust to different graph types and can achieve high
performance when test graph sizes are much larger than the training graphs or the distributions of
training and test graphs are different. Using Erd˝os-Rényi random graphs as training data (S2), we
observe a slight drop in performance due to distribution shift from the training data to the test data
(11). This drop in performance does not occur using our subroutines trained to strong generalization
(S1).

Figure 11: Histograms of data used in MST (S2)

A.4 Detailed visualization of learned number embeddings

In Figure 12 we show more detailed visualizations of the learned bitwise embeddings. These are
3-dimensional PCA projections of the full embedding matrix, capturing approximately 75% of the
total variance. The main takeaway is that the network is able to learn a coherent number system with
a great deal of structure, and that this structure varies depending on the speciﬁc task of the network.
This is reminiscent of [5], where linear embeddings learned the correct structure to solve a simple
modular arithmetic task. Also, the network learns to embed inﬁnity, outside of this structure.

Future work will also investigate the resulting embedding from a NEE that performs multiple or more
complex tasks.

17

050100150200250Values0250500750100012501500CountsTrainingTest(a) Sorting Embedding

18

2.0    1.5 1.0 0.5 0 -0.5 -1.0 -1.5 -2.0 0.50-0.5-1.0-1.5-2.02.01.51.00.50-0.5(b) Addition Embedding

(c) Generalization on Addition (numbers randomly held out of training colored red)

19

1.51.00.50-0.5-1.0-0.500.5-0.50-0.5-1.0-1.51.02.0    1.5 1.0 0.5 0 -0.5 -1.0 -1.5 1.51.00.50-0.5-1.0-1.52.0    1.0 0 (d) Multiplication Embedding (250 numbers are shown)

Figure 12: 3-dimensional PCA projections of learned bitwise embeddings for (a) sorting, (b) addition, and (c)
addition with 65% of the numbers withheld from training, (d) multiplication

20

2.01.51.00.50-0.5-1.0-1.5-2.02.5-0.500.51.01.52.0-3.0-2.0-1.001.02.0References

[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. International Conference on Learning Representations, 2015.
[2] Albert-László Barabási and Réka Albert. Emergence of scaling in random networks. science,

286(5439):509–512, 1999.

[3] Paul Erd˝os and Alfréd Rényi. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad.

Sci, 5(1):17–60, 1960.

[4] Mark EJ Newman and Duncan J Watts. Renormalization group analysis of the small-world

network model. Physics Letters A, 263(4-6):341–346, 1999.

[5] Alberto Paccanaro and Geoffrey E. Hinton. Learning distributed representations of concepts
using linear relational embedding. IEEE Transactions on Knowledge and Data Engineering,
13(2):232–244, 2001.

[6] Petar Velickovic, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural
execution of graph algorithms. International Conference on Learning Representations, 2020.

21

