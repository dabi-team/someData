2
2
0
2

p
e
S
9
2

]
E
S
.
s
c
[

3
v
6
9
7
7
0
.
6
0
2
2
:
v
i
X
r
a

Preprint

FIXEVAL: EXECUTION-BASED EVALUATION OF
PROGRAM FIXES FOR PROGRAMMING PROBLEMS

Md Mahim Anjum Haque§, Wasi Uddin Ahmad†, Ismini Lourentzou§, Chris Brown§
§Virginia Tech, †University of California, Los Angeles
§{mahim, ilourentzou, dcbrown}@vt.edu, †wasiahmad@ucla.edu

ABSTRACT

The increasing complexity of software has led to a drastic rise in time and costs
for identifying and ﬁxing bugs. Various approaches are explored in the literature
to generate ﬁxes for buggy code automatically. However, few tools and datasets
are available to evaluate model-generated ﬁxes effectively due to the large com-
binatorial space of possible ﬁxes for a particular bug. In this work, we introduce
FIXEVAL, a benchmark comprising buggy code submissions to competitive pro-
gramming problems and their respective ﬁxes. FIXEVAL is composed of a rich
test suite to evaluate and assess the correctness of model-generated program ﬁxes
and further information regarding time and memory constraints and acceptance
based on a verdict. We consider two Transformer language models pretrained on
programming languages as our baselines and compare them using match-based
and execution-based evaluation metrics. Our experiments show that match-based
metrics do not reﬂect model-generated program ﬁxes accurately. At the same
time, execution-based methods evaluate programs through all cases and scenarios
designed explicitly for that solution. Therefore, we believe FIXEVAL provides a
step towards real-world automatic bug ﬁxing and model-generated code evalua-
tion. The dataset and models are open-sourced.1

1

INTRODUCTION

Repairing software programs is one of the hardest and most expensive processes in software engi-
neering. Finding and ﬁxing errors, or debugging, takes up nearly 50% of the total software devel-
opment costs (Britton et al., 2013) and 70 − 80% of software engineers’ time (National Institute
of Standards and Technology, 2002). Current research aims to provide better solutions to auto-
mate this process (Goues et al., 2019; Monperrus, 2018), but this problem is still far from being
solved. Automatic program repair is an active area of research2 that can greatly relieve program-
mers from the burden of manually ﬁxing bugs in large codebases (Mesbah et al., 2019; Ding et al.,
2020; Dinella et al., 2020). Researchers have recently increasingly applied statistical and neural
methods to automate program repair tasks. Models such as BART (Lewis et al., 2020) and GPT
(Chen et al., 2021) have demonstrated great success in solving problems relevant to code. Tech-
niques to automatically repair programs, and the availability of accurate benchmarks for evaluating
them, are useful for enhancing programming productivity (Seo et al., 2014) and reducing software
development costs (Le Goues et al., 2012).

While many approaches are being studied to automate program repair, more support is needed to
evaluate generated ﬁxes. Prior work, such as Tﬁx (Berabi et al., 2021), BIFI (Yasunaga & Liang,
2021), and CodeBLEU (Ren et al., 2020) rely on token-level matching metrics, such as Exact Match.
A limitation of match-based metrics is that they penalize generated ﬁxes if they differ from the ref-
erence by a single token, even if the ﬁx is valid. CodeBLEU attempts to mitigate this by aggregating
weighted n-gram, data ﬂow, and Abstract Syntax Tree (AST) matches. However, this does not ac-
count for the fact that bugs can be ﬁxed using various code instructions, and programs do not have
to use the same algorithm, syntax, data ﬂow, or ASTs to solve the same problem. Thus, match-
based methods cannot effectively evaluate generated code since they cannot account for the large

1https://github.com/mahimanzum/FixEval
2See https://program-repair.org

1

 
 
 
 
 
 
Preprint

and complex space of program functionality in solutions. A well-deﬁned test suite is, therefore,
necessary (Arcuri, 2008; Kim et al., 2013; DeMarco et al., 2014; Ackling et al., 2011) to evaluate a
ﬁx’s correctness using program behavior instead of syntax. A generated ﬁx is considered function-
ally correct if it passes a set of unit tests. Overall, there is an increasing need for better evaluation
methods and benchmarks to assess code ﬁxes generated by program repair models.

In this work, we introduce FIXEVAL, a benchmark dataset consisting of competitive programming
submissions from users of AtCoder (Atcoder, 2020) and Aizu Online Judge (Aizu Online Judge,
2004). Competitive programming consists of programmers attempting some of the toughest prob-
lems to be solved within a speciﬁc time and memory limit. The process of competitive programming
comes down to submitting code, receiving a verdict, making educated changes, and repeating until
an acceptable solution is reached. Thus using competitive programming submissions to construct
FixEval provides data useful for evaluating automated bug ﬁxes, in particiular parallel pairs of buggy
and corrected programs with access to corresponding unit tests. FIXEVAL contains solutions to 700
Python and Java problems along with more than 40, 000 test cases for evaluation. We demonstrate
the effectiveness of our benchmark through an experimental analysis evaluating bug ﬁx generation
for state-of-the-art models in program repair.

Contributions: The contributions of our work are summarized as follows: (1) We introduce FIXE-
VAL, a context-aware dataset that incorporates additional considerations for programs, namely time
and space complexity, for evaluating code generated by deep learning models to automatically ﬁx
programming bugs. (2) We provide a comprehensive comparison of state-of-the-art models on FIX-
EVAL and evaluate their accuracy for repairing buggy code. Further, we open source this dataset and
the baseline models used for our evaluation. (3) Finally, we experimentally verify the advantages of
the proposed execution-based program repair evaluation derived from our introduced test suite.

2 RELATED WORK

Program Repair Automated program repair aims to improve debugging tasks for developers by
generating ﬁxed programs from buggy code automatically (Goues et al., 2019). Prior work tackles
this problem in various ways. One of the most common methods is to model code generation as
a machine translation task from a buggy code to a ﬁxed one, e.g., by training a language model
on code with various pretraining objectives (Ahmad et al., 2021a). Several researchers have shown
language modeling is effective for automating coding tasks, such as program generation (Ahmad
et al., 2021a; Wang et al., 2021), program translation between languages (Ahmad et al., 2021b),
and program auto-completion (Chen et al., 2021). Nevertheless, there needs to be more research on
applying language modeling in automated bug ﬁxing and code repair.

Evaluating Pretrained Language Models Due to the recent success of large-scale language mod-
els in many domains (Raffel et al., 2020; Brown et al., 2020; Shoeybi et al., 2019), new techniques
have been introduced with different pretraining objectives relevant to code. Models such as BART
(Lewis et al., 2020), GPT (Chen et al., 2021), and T5 (Raffel et al., 2020) have been applied to
software engineering tasks, demonstrating improvements in automating development tasks such as
code generation, translation, bug detection, etc. For example, PLBART (Ahmad et al., 2021a) is a
BART model trained on programming corpora with token masking, token deletion, and token in-
ﬁlling training strategies. In contrast, Tﬁx (Berabi et al., 2021) is a proposed method evaluating
T5 (Raffel et al., 2020) by leveraging commits from GitHub repositories to solve bugs detected by
ESLint,3 the most popular static analyzer for JavaScript code. We train a subset of these models on
our dataset with various input conﬁgurations to evaluate their performance.

Program Repair Benchmarks There are several existing examples of benchmarks to help re-
searchers evaluate deep learning techniques for automatically ﬁxing bugs. A comparison of FIXE-
VAL to these recent benchmarks are available in Table 1. DeepFix (Gupta et al., 2017) consists of
approximately 7k C programs written by students in an introductory programming course across 93
programming tasks. However, DeepFix only covers compiler errors, does not provide test cases for
evaluation, and fails to reﬂect real-world software applications. Review4Repair (Huq et al., 2022)
contains 55, 060 training data along with 2, 961 test data for Java patches. This work aims to repair
code patches with the help of code reviews, making match-based methods the only way to evaluate

3https://eslint.org/

2

Preprint

DeepFix Review4Repair Bug2Fix

Github-Python

FIXEVAL

Language
Dataset Test Size
Avg. #Tokens
Input Type
Error Type
Test Cases

Java
2961
320 + 37

C
6971
203
Program Program + CR
CE Only All
No
No

Java
5835, 6545
≤ 50, ≤100
Function
All
No

Python
15k
10 - 128
Program
CE Only
No

Java, Python
43k, 243k
331, 236
Program
All
Yes

Table 1: A comparison between FIXEVAL and other existing code repair datasets for machine
learning. CR and CE indicate code review comments and compilation errors, respectively.

performance. Incorporating review comments as conditional input results in high linguistic vari-
ation, making the learning process more difﬁcult and requiring more training examples. Bug2Fix
(Tufano et al., 2019) is a popular corpus used in CodeXGLUE (Lu et al., 2021) that contains buggy
and ﬁxed Java code. However, the dataset is only stored at the function level, so cross-function
dependencies cannot be modeled. Further, Bug2Fix also lacks unit tests to check for functional cor-
rectness. The GitHub-Python dataset (Yasunaga & Liang, 2021) is a collection of 38K buggy and
3M correct unparalleled code snippets from GitHub open-source Python projects. The 128 token
limit signiﬁcantly reduces the overall problem complexity. However, the output code is deﬁned as
successful if it has no AST errors, which limits the focus only to compiler errors.

Existing program repair benchmarks incorporating test suites have also been introduced to sup-
port automated program repair research. For instance, datasets such as IntroClass (Le Goues et al.,
2015) and Refactory (Hu et al., 2019) consist of student assignments from introductory programming
courses and provide unit tests. However, these benchmarks lack relevance to real-world software.
QuixBugs (Lin et al., 2017) and Defects4J (Just et al., 2014) both provide more relevant buggy pro-
grams with test suites. FIXEVAL is substantially larger than both datasets, consisting of more lines
of code (QuixBugs: 1,034; Defects4J: 321,000; FIXEVAL: 54 Million for Java and 61 Million for
Python). This allows large-scale training and testing of machine learning techniques for automated
program repair. QuixBugs only contains 40 “small” programs and Tufano et al. (2018) note that the
limited size of Defects4J restricts its usage as training data with machine learning models. Further,
our benchmark is more diverse and representative of software in practice. QuixBugs only consists of
programs with one-line defects, whereas Defects4J consists of Java code from only ﬁve open source
programs. On the other hand, FIXEVAL contains bugs that span multiple lines of code derived from
712K Java and 3.28 Million Python program submissions that vary in size and difﬁculty.

This paper aims to ﬁll the gaps and limitations of existing datasets by providing better benchmarks
for deep learning models in automated program repair research. Existing benchmarks contain intro-
ductory programming assignments that do not capture the representation of large real-world bugs. In
addition, these automated program repair datasets focus on domain-speciﬁc open-source code with-
out considering additional constraints and contexts that vary from project to project. In contrast,
FIXEVAL provides a thorough test suite to evaluate repairs on efﬁciency and correctness. To the
best of our knowledge FIXEVAL is the ﬁrst context-aware program repair evaluation dataset with a
comprehensive test suite that also considers runtime and memory consumption.

3 DATASET: FIXEVAL

The FIXEVAL dataset consists of Java and Python program submissions from CodeNet (Puri et al.,
2021), a collection of programs submitted by competitive programmers to different online judges
for evaluation. We enrich the dataset with test suites for the programs in the validation and test
set, where each problem has multiple test cases to evaluate the correctness of program submissions.
Each test was created for speciﬁc problems, enabling rigorous evaluation of program functionality.

While competitive programming is not an exact reﬂection of real-world professional software de-
velopment environments, FIXEVAL consists of programs with varying difﬁculty and takes time and
memory requirements into consideration, a common practice for evaluating software engineers to
hire (McDowell, 2019) and crucial for writing efﬁcient code in industrial settings (Mens, 2012).

3

Preprint

Problem Statement4: A biscuit making machine produces B biscuits at the following moments:
A seconds, 2A seconds, 3A seconds and each subsequent multiple of A seconds after activation.
Find the total number of biscuits produced within T + 0.5 seconds after activation.
Constraints: 1 ≤ A, B, T ≤ 20, All input values are integers
Time Limit: 2 secs; Memory Limit: 1024MB; Problem Difﬁculty: A

Buggy Program in Java

Fixed Program in Java

1 i m p o r t
j a v a . u t i l . * ;
2 p u b l i c c l a s s Main {
3
4
5
6
7
8
9
10 }

p u b l i c s t a t i c v o i d main ( S t r i n g [ ] args ) {
Scanner sc = new Scanner ( System . i n ) ;
i n t A = sc . n e x t I n t ( ) ;
i n t B = sc . n e x t I n t ( ) ;
i n t T = sc . n e x t I n t ( ) ;
i n t S = T / A System . o u t . p r i n t l n ( s * b ) ;

}

1 i m p o r t
j a v a . u t i l . * ;
2 p u b l i c c l a s s Main {
3
4
5
6
7
8
9
10
11 }

p u b l i c s t a t i c v o i d main ( S t r i n g [ ] args ) {
Scanner sc = new Scanner ( System . i n ) ;
i n t A = sc . n e x t I n t ( ) ;
i n t B = sc . n e x t I n t ( ) ;
i n t T = sc . n e x t I n t ( ) ;
i n t S = T / A ;
System . o u t . p r i n t l n ( s * b ) ;

}

Figure 1: Example submissions from the FixEval dataset. Buggy and ﬁxed statements are marked
in red and green, respectively.

Dataset Construction For each user, we considered the chronologically ordered submission path
for each of the problems solved. If the code passes all of the hidden test cases, then the result,
also termed as the verdict for the code, is considered Accepted (AC). Otherwise, programs may
receive a verdict from among 12 different options, the most frequent being: (i) Wrong Answer (WA),
i.e., failed one or more test cases; (ii) Time Limit Exceeded (TLE), i.e., the program did not run
within the intended time limit; (iii) Compilation Error (CE), i.e., the program did not compile; and
(iv) Runtime Error (RE), i.e., program execution was not successful (Puri et al., 2021). A full list
of possible verdicts for submitted programs is available in the Appendix B.2. Each submission is
associated with a verdict. For a user trying to solve a particular problem, a sample submission path
could be [WA, WA, TLE, AC] representing 3 failed attempts consisting of two incorrect programs
and one inefﬁcient algorithm, before arriving at the correct solution.

We paired each wrong submission with the accepted submission and consider that as a single data
point (example) consisting of a pair of a buggy and a ﬁxed program. A sample example can be
viewed in Figure 1. FIXEVAL consists of submission paths for the corresponding problems in Java
and Python for all 154k users and 6.5 million submission paths. On average, users submitted 90
programs from individual accounts. We de-duplicated the submissions using Jaccard similarity to
remove multiple submissions for a speciﬁc problem. We used the javalang5 tokenizer for Java and
the tokenizer6 standard library for Python. As shown in Table 2, we created stratiﬁed dataset splits
based on problems to ensure that there is a clear partition (80-10-10) in train, test, and validation
splits, with no overlapping problems or submissions across splits. Finally, we ensured that all the
test and validation data examples include test cases.

Test Suite Collection We download all test cases that are used for evaluating the submitted pro-
grams from the open source test pool (Atcoder, 2020) shared by the ofﬁcial AtCoder7 site. To
construct our dataset test suite, we matched the problem names from the CodeNet problem meta-
data with the AtCoder published website. Then, we matched each problem with the corresponding
input and output ﬁles provided with the test cases. We also cleaned the test case data manually.
For example, there exist programs with different precision cutoffs for numerical output. In other
words, the solution of the program is accepted if the difference between the output and the target
is below a certain precision threshold. For simplicity, we kept the most frequent precision cutoff,
10−8, across all programs. Also, there are constraint satisfaction problems where the main goal is to
satisfy conditions based on rules or design constraints. Consequently, many combinatorial outputs
are equivalently valid. We removed such problems and test cases from our evaluation pipeline. More
details are available in Appendix B.1.

5https://github.com/c2nes/javalang
6https://docs.python.org/3/library/tokenize.html
7https://atcoder.jp/posts/21

4

Preprint

Language

Problem Count

Example Count

Train Valid Test

Total

Train Valid

Test

Total

Java
Python

2,160
1,951

279
244

279
244

2,718
2,439

156k
567k

44k
301k

45k
243k

245k
1,111k

Table 2: Statistics of the FixEval dataset.

Finally, our validation set contains an average of 24 test cases per problem, and our test set contains
an average of 25 test cases per problem. Each test case is manually generated by domain experts
(e.g., problem setters for competitive programming challenges) to ensure the functional correctness
of the submitted programs.

Problem Difﬁculty While the length of a problem solution does
not indicate a problem’s difﬁculty, we still emphasize that FIX-
EVAL is composed of Java and Python programs that consist of
331 and 236 tokens, respectively, on average. As shown in Table
1, FIXEVAL programs’ are larger than the existing program repair
datasets. Therefore, FIXEVAL is a challenging benchmark for auto-
matic program repair models as they need to tackle longer programs
and generate ﬁxes accordingly. In AtCoder, contest problems come
with a Task Label (e.g., A, B, C, D, E) and we conjecture that the
label indicates the difﬁculty of the problem. In FIXEVAL, we retain
the task labels and verify that there is an even difﬁculty of problems
partitioned in all splits by stratiﬁed sampling. The distribution of
problem difﬁculty from the overall dataset is presented in Figure 2.

4 EXPERIMENT SETUP

Figure 2: Test set difﬁculty.

We consider the following two Transformer language models as the baseline methods.

PLBART (Ahmad et al., 2021a) is a BART (Lewis et al., 2020) model trained on programming
language corpora using three learning strategies: token masking, token deletion, and token inﬁlling.

CodeT5 (Wang et al., 2021) is a T5 model (Raffel et al., 2020) pretrained on programming languages
via multiple objectives, such as span prediction and identiﬁer tagging prediction. CodeT5 uses
unimodal (code only) and bimodal (code text pairs) data for pretraining.

Apart from the two baseline models, we consider Naive Copy as a baseline. The input buggy code
is copied to the output in this approach. Since there is a signiﬁcant overlap between the buggy code
and its ﬁx, this baseline shows the minimum a model could achieve in match-based metrics.

Setup We ﬁnetune PLBART and CodeT5 on FIXEVAL Java and Python programs using the base
variant of both the models, shared by the respective authors and test on our FIXEVAL dataset using
beam search with a beam size of 5 and batch size of 32. We train with AdamW optimizer (Loshchilov
& Hutter, 2019), 5 × e−5 learning rate, early stopping with patience set to 3 and 100 warm-up steps.

4.1 EVALUATION METRICS

To understand how accurately models perform on FIXEVAL, we evaluate in both conventional
match-based metrics and our proposed execution-based metrics, explained next.

4.1.1 MATCH-BASED METRICS

Exact Match (EM) evaluates whether a generated program ﬁx exactly matches the reference.

BLEU computes the match-based overlap between a model-generated ﬁx and the reference. We use
corpus-level BLEU score (Papineni et al., 2002).

7https://atcoder.jp/contests/abc125/tasks/abc125_a

5

ABCDEDifficulty0102030Parcentage (%)Preprint

CodeBLEU (CB) (Ren et al., 2020) is designed to measure the quality of a code with respect to a
reference. Compared to BLEU, CodeBLEU also considers logical correctness based on an Abstract
Syntax Tree (AST), in conjunction with data ﬂow structure and grammatical similarity.

Compilation Accuracy (CA) indicates the percentage of generated programs that are compilable.
We use off-the-shelf compilers, i.e., javac for Java and py compile8 for Python.

Syntax Match (SM) represents the percentage of the sub-trees extracted from the candidate pro-
gram’s Abstract Syntax Tree (AST) that match the subtrees in the reference programs’ AST.

Dataﬂow Match (DM) (Ren et al., 2020) is the ratio of the number of matched candidate dataﬂows
and the total number of reference dataﬂows.

4.1.2 EXECUTION-BASED METRICS

In program repair tasks, the input and output typically have high lexical overlapping. However,
match-based metrics may not estimate the actual functional correctness of model-generated program
ﬁxes. Further, a program can be ﬁxed in multiple ways that differ from the reference program.
Therefore, match-based metrics may not be ideal for program repair evaluation. Thus, we also
evaluate FIXEVAL with execution-based metrics to alleviate these limitations.

Evaluating all generated programs on execution for all available test cases is memory-intensive and
time-consuming. To reduce time complexity, we randomly selected two data points per problem
from the test split, with a similar distribution of error verdicts, to ensure the evaluation data follows
the actual distribution of the total test data for different verdicts (AC, WA, TLE, etc.). Since our
goal is not to exhaustively evaluate all models but to showcase the efﬁcacy of the proposed dataset,
we only evaluate CodeT5, the current state-of-the-art, on relevant tasks. We generate top-10 outputs
using beam search decoding. Then, we evaluate the output programs by running our test suite that
simulates how online judges evaluate submitted programs. Our execution-based evaluation metrics,
pass@k and TCA@k, were introduced by Kulal et al. (2019) and hen. For self-containment, we
provide the descriptions in the following paragraphs.

Pass@k: Kulal et al. (2019) evaluate functional correctness using the pass@k metric, where k code
samples are generated per problem. A problem is considered solved if any sample passes all the unit
tests, and the total fraction of problems solved is reported. However, this computation of pass@k
can have high variance. Hence, we follow Chen et al. (2021) to evaluate pass@k, i.e., we generate
k ≤ n samples per task (in this paper, n = 10 and k ≤ 10), count the number of correct samples
c ≤ n that pass all unit tests, and calculate the unbiased estimator of pass@k as follows:

pass@k := E
Dtest

(cid:34)

1 −

(cid:35)

(cid:1)

,

(cid:0)n−c
k
(cid:1)
(cid:0)n
k

where Dtest denotes the FIXEVAL test set. Note that this is a strict measure, as a code repair is
considered unsuccessful if a single failed test case exists.

Test Case Average (TCA@k): We follow hen to compute the average number of test cases passed.
Concretely, let P be the set of problems in the test set and |P | be the number of problems in P .
(cid:11), where i denotes the
Let the code ﬁxes generated to solve problem p ∈ P be denoted as (cid:10)codei
index of generated ﬁx and k is the total number of generated ﬁxes. Furthermore, let the set of test
cases for problem p be {(xp,c, yp,c)}|Cp|
c=1 , where xp,c and yp,c are the input, output pair and Cp is
the number of available test case pairs for that problem. Then, the test case average for k generated
ﬁxes (TCA@k) is

p

1
|P |

(cid:88)

p∈P

1
k

k
(cid:88)

i=1

1
|Cp|

|Cp|
(cid:88)

c=1

1 (cid:8)eval (cid:0)(cid:10)codei

p

(cid:11) , xp,c

(cid:1) = yp,c

(cid:9) ,

(1)

where eval is the function evaluating a code ﬁx in a test case by matching the output with the
intended result. Often, solutions can successfully pass a subset of the test cases but may only cover
some corner cases. This allows for a less stringent model evaluation, as strict accuracy may obscure
model improvements. Thus, we consider the test case average as soft accuracy and report results for
a varying number of generations k.

8https://docs.python.org/3/library/py_compile.html

6

Preprint

Method

Language Verdict BLEU EM SM

DM

CB

CA

Naive Copy

PLBART

CodeT5

Java
Python

Java
Java
Python
Python

Java
Java
Python
Python

(cid:55)
(cid:55)

(cid:55)
(cid:88)
(cid:55)
(cid:88)

(cid:55)
(cid:88)
(cid:55)
(cid:88)

80.28
68.55

58.49
59.84
61.89
62.25

62.31
62.54
64.92
64.67

0.0
0.0

0.45
1.46
2.32
2.46

2.96
2.45
2.74
2.97

84.22
70.12

66.92
68.01
64.32
63.31

74.01
73.93
68.79
68.45

53.64
60.51

43.08
44.99
48.81
49.73

52.30
53.29
56.21
56.04

75.43
68.47

57.23
58.62
61.13
62.21

63.37
63.71
63.53
63.28

89.93
96.56

31.36
33.04
91.16
92.21

63.03
64.23
92.80
92.70

Table 3: Match-based results of program repair language models, trained and tested on Java or
Python code pairs from our proposed FIXEVAL corpus. EM (Exact Match), SM (Syntax Match),
DM (Dataﬂow Match), CB (CodeBLEU), and CA (Compilation Accuracy).

5 RESULTS

We aim to address the following questions through our preliminary experiments and analysis: (1)
How well do pretrained Transformer models perform on FIXEVAL? and (2) How match-based
metrics track performance relative to execution-based evaluation? Our results validate the need for
better program repair evaluation practices, demonstrating that FIXEVAL can ﬁll a critical need in
the research community.

5.1 PRETRAINED MODEL PERFORMANCE

To answer our ﬁrst research question, we calculated the match-based metrics for our baseline meth-
ods, Naive Copy, PLBART, and CodeT5. Table 3 presents the results of all compared models on the
match-based metrics described in subsection 4.1.1. The Verdict column indicates the use of verdict
information as conditional input when generating a candidate program ﬁx. We observe that Naive
Copy performs the best in all match-based measures except for Exact Match (EM). This is because
the submitted code pairs will most likely change the program after receiving anything other than an
“Accepted” verdict. Between the compared language models, CodeT5 and PLBART, CodeT5 per-
forms better than PLBART across all metrics and programming languages. We believe this is due
to their novel identiﬁer-aware pre-training objective that helps CodeT5 learn useful patterns from
programming languages.

Further, we observe a marginal performance increase in our baseline models with verdict information
as conditional input for Java programs. However, there was no such correlation for Python. We
hypothesize that the verbosity of Java has a positive effect when the model is trained with the verdict
information, but since Python is not as verbose as Java, the effect may not be the same. We further
analyze the impact of verdicts, reporting our results in Section 5.3.

5.2 MATCH-BASED VS EXECUTION-BASED EVALUATION

We compare match-based and execution-based evaluation metrics to check whether both corre-
late with model performance. To answer our second research question, we evaluate CodeT5 with
execution-based metrics and report the results in Table 4a for the sampled test set. While Naive Copy
performed the best for match-based metrics (see Table 3), we observe that it performs the worst in
both of the execution-based evaluation metrics (see Table 4b). This suggests that TCA and pass@k
are better indicators for functional program correctness and evaluate models better than match-based
metrics. Further, Figure 3a demonstrates that, with increasing difﬁculty, TCA decreases, whereas
the match-based metrics have no such clear correlation. This means that problems with increasing
difﬁculty become harder to ﬁx, leading to low TCA scores. We speculate that TCA and match-based
metrics (BLEU, DM, SM, and CB) do not behave similarly because high match-based similarity
does not necessarily indicate program correctness.

7

Preprint

pass@k

top-k TCA

Language Verdict

k = 1

k = 3

k = 5

k = 10

k = 1

k = 3

k = 5

k = 10

Java
Java

Python
Python

(cid:55)
(cid:88)

(cid:55)
(cid:88)

8.65
10.94

6.86
7.32

15.62
18.77

13.07
13.94

19.63
22.66

16.27
17.47

24.44
27.96

20.51
22.63

41.00
44.99

50.20
48.75

34.00
38.80

41.20
41.16

32.70
35.87

38.50
38.37

29.60
32.90

35.20
34.88

(a) Execution-based Results for CodeT5.

Language

pass@1

top-1 TCA

Java
Python

0.0
0.0

37.95
41.55

(b) Execution-based Results for Naive Copy.

Table 4: Execution-based evaluation metrics on the sampled evaluation FIXEVAL.

(a) Comparison of BLEU and Test Case Average
(TCA) with varying problem difﬁculty.

(b) TCA increases as edit similarity between buggy
and reference code increases.

5.3 ABLATION ANALYSIS

We further perform ablation analyses on FIXEVAL to understand the effects of several components,
e.g., verdict information, decoding algorithms, etc. The following analyses are based on the CodeT5
with verdicts on sampled Java examples. Qualitative examples are provided in Appendix B.

Correlation to Edit Similarity We analyze model performance based on edit similarity, assuming
lower edit similarity between the buggy and ﬁxed code indicates a more difﬁcult problem to ﬁx that
error. We sort all data points on our evaluation set based on the buggy and reference (ﬁxed) code’s
edit similarity and plot the test case average of our best model for both Java and Python. In Figure 3b,
we observe a mostly upward trending curve for Java, which indicates a positive correlation between
edit similarity and model performance. However, no such correlation is evident for Python.

Correlation to Problem Difﬁculty We analyze the effect of problem difﬁculty as described in
Section 3. From Figure 4a, we observe that as difﬁculty increases, i.e., problems become harder
to solve (difﬁculty is increasing from A to E), the model performance degrades. At the same time,
accuracy increases as we generate more programs for the same input code (pass@1 to pass@10).

Correlation to Evaluation Verdict We analyze the effect of verdict type on performance. Figure
4b shows that compilation errors (CE) are the easiest to solve as these mostly deal with syntactical
changes to correct a program, whereas runtime errors (RE) or time limit exceeded errors (TLE) are
much harder to ﬁx since these indicate semantically incorrect code that requires multiple changes,
sometimes even over the entire algorithm.

Effect of Decoding Algorithms We generate candidate ﬁxed programs with various decoding
strategies: (i) greedy, (ii) beam search with beam size 10, and (iii) top-k sampling with top-k proba-

8

ABCDEDifficulty30354045505560657075ScoresTCASMDMCBBLEU020406080100Edit Similarity0.20.30.40.5Test Case Average (%)JavaPythonPreprint

(a) Accuracy at different difﬁculty levels. Task labels
A to E indicates increasing difﬁculty.

(b) Accuracy at different verdict labels, where CE =
compilation error, WA = wrong answer, TLE = time
limit exceeded, and RE = runtime error.

Figure 4: Pass@k accuracy breakdown based on criteria (a) and (b).

bility and temperature empirically set to 0.95 and 0.7, respectively. Figure 5 shows that beam search
decoding usually performs better than greedy and sampling. We also experiment with varying sam-
pling temperatures from 0.2 to 1.2 and observe minor performance changes. We believe this is due to
the nature of the problem, as the ﬁxed program remains mostly similar to the buggy version, which
results in the model becoming more conﬁdent in its predictions. Hence, the temperature doesn’t
result in substantial model output changes.

Effect of Modeling Verdict We analyze the test exam-
ple cases successfully repaired only when the model had
access to the verdict information. We observe that verdict
information is crucial in ﬁxing some instances of buggy
code. When we input the same code to program repair
models both with and without the verdict, the model with-
out verdict information attempts to add unnecessary but
syntactically correct code snippets that cannot ﬁx the ac-
tual error. In contrast, the model with verdict information
as input can pinpoint the exact location of the error and
make the code more consistent. We provide relevant ex-
amples in Appendix B, Figure 6.

Summary of Findings We study performance trends
concerning edit similarity, problem difﬁculty, and evalu-
ation verdicts to show that some bug-ﬁxing tasks are triv-
ial while many are challenging. Therefore, we encourage
future work to consider all aforementioned aspects while
performing evaluations. Choice of decoding strategy produces marginal differences; therefore, it is
not a crucial factor in improving bug-ﬁxing models. We encourage future works to study feedback-
based (e.g., feedback from an oracle) approaches to improve bug ﬁxing models.

Figure 5: Accuracy sensitivity for strict
(pass@1) and soft (top-1 TCA) accu-
racy for greedy, beam search and top-k
sampling decoding algorithms.

6 CONCLUSION

We introduce FIXEVAL, a context-aware dataset to improve bug ﬁx model creation and evaluation.
In contrast to previous benchmarks that evaluate models with open-source GitHub repositories or
programming assignments, we provide a new evaluation corpus that can capture model-generated
code’s acceptability, accuracy, and efﬁciency. We assess the performance of state-of-the-art mod-
els on this dataset and showcase that traditional evaluation metrics are sub-optimal compared to
execution-based metrics derived from test suites that capture contextual program repair requirements
often found in practice. Our FIXEVAL dataset facilitates several other potential future directions and
applications. It can also be used to evaluate the automation of software engineering tasks such as
code completion, code editing, code search, verdict-conditioned code repair, verdict prediction, and
chain edit suggestion tasks. In the future, since the provided test cases are language-independent,
our work can be easily extended to other programming languages, such as C++ and JavaScript.
We hope that FIXEVAL will spur the development of more advanced program repair models that
consider realistic program requirements.

9

A (147)B (86)C (161)D (68)E (19)Difficulty0.00.10.20.3pass@k Accuracypass@1pass@3pass@5pass@10CE (76)WA (288)TLE (67)RE (59)0.00.10.20.30.4pass@k Accuracypass@1pass@3pass@5pass@10pass@1TCAMetrics010203040Accuracy(%)GreedyBeam SearchTop-1Preprint

ETHICS STATEMENT

This work uses language models, for which the risks and potential harms are discussed in Bender &
Koller (2020), Brown et al. (2020), Bender et al. (2021) and others. The dataset consists of computer
programs that are submitted to online judging sites for competitive programming problems, along
with their accompanying test cases and metadata. Any information related to the programmer who
wrote these programs has been anonymized. As with all labeled datasets, undesirable biases may be
encoded in data. Due to the many technical and practical complexities involved, training general-
izable models with no biases cannot be guaranteed in most machine learning applications (though
we certainly hope the newly introduced dataset would help assess some of these issues). The rich
metadata and diversity of FIXEVAL enables model evaluation for several other software engineering
tasks. Speciﬁcally, FIXEVAL can be used to evaluate code completion, code editing, code search,
verdict-conditioned code repair, verdict prediction, and chain edit suggestion and can also be ex-
tended to other programming languages beyond Java and Python, such as C++ and JavaScript.

REFERENCES

Thomas Ackling, Bradley Alexander, and Ian Grunert. Evolving patches for software repair.

In
Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation, GECCO
’11, pp. 1427–1434, New York, NY, USA, 2011. Association for Computing Machinery. ISBN
9781450305570. doi: 10.1145/2001576.2001768. URL https://doi.org/10.1145/
2001576.2001768.

Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Uniﬁed pre-training for pro-
gram understanding and generation. In Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.
2655–2668, Online, June 2021a. Association for Computational Linguistics. doi: 10.18653/v1/
2021.naacl-main.211. URL https://aclanthology.org/2021.naacl-main.211.

Wasi Uddin Ahmad, Md Golam Rahman Tushar, Saikat Chakraborty, and Kai-Wei Chang. Avatar:
A parallel corpus for java-python program translation. arXiv preprint arXiv:2108.11590, 2021b.

Aizu Online Judge, 2004. https://judge.u-aizu.ac.jp/onlinejudge.

Andrea Arcuri. On the automation of ﬁxing software bugs.

In Companion of the 30th In-
ternational Conference on Software Engineering, ICSE Companion ’08, pp. 1003–1006, New
York, NY, USA, 2008. Association for Computing Machinery.
ISBN 9781605580791. doi:
10.1145/1370175.1370223. URL https://doi.org/10.1145/1370175.1370223.

Atcoder.

Atcoder opensourced test

cases.

https://www.dropbox.com/sh/

nx3tnilzqz7df8a/AAAYlTq2tiEHl5hsESw6-yfLa?dl=0, 2020.

Emily M. Bender and Alexander Koller. Climbing towards NLU: On meaning, form, and under-
standing in the age of data. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pp. 5185–5198, Online, July 2020. Association for Computational
Linguistics. doi: 10.18653/v1/2020.acl-main.463. URL https://aclanthology.org/
2020.acl-main.463.

Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the
dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM
Conference on Fairness, Accountability, and Transparency, FAccT ’21, pp. 610–623, New York,
NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/
3442188.3445922. URL https://doi.org/10.1145/3442188.3445922.

Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin Vechev. Tﬁx: Learning to ﬁx coding
errors with a text-to-text transformer. In Marina Meila and Tong Zhang (eds.), Proceedings of
the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine
Learning Research, pp. 780–791. PMLR, 18–24 Jul 2021. URL https://proceedings.
mlr.press/v139/berabi21a.html.

10

Preprint

Tom Britton, Lisa Jeng, Graham Carver, and Paul Cheak. Reversible debugging software “quantify
the time and cost saved using reversible debuggers”. 2013. URL http://citeseerx.ist.
psu.edu/viewdoc/download?doi=10.1.1.444.9094&rep=rep1&type=pdf.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-
Language models are few-shot
dlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad-
learners.
vances in Neural Information Processing Systems, volume 33, pp. 1877–1901. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Favio DeMarco, Jifeng Xuan, Daniel Le Berre, and Martin Monperrus. Automatic repair of buggy if
conditions and missing preconditions with smt. In Proceedings of the 6th International Workshop
on Constraints in Software Testing, Veriﬁcation, and Analysis, CSTVA 2014, pp. 30–39, New
York, NY, USA, 2014. Association for Computing Machinery. ISBN 9781450328470. doi: 10.
1145/2593735.2593740. URL https://doi.org/10.1145/2593735.2593740.

Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang. Hoppity: Learning
graph transformations to detect and ﬁx bugs in programs. In International Conference on Learn-
ing Representations, 2020. URL https://openreview.net/forum?id=SJeqs6EFvB.

Yangruibo Ding, Baishakhi Ray, Premkumar Devanbu, and Vincent J. Hellendoorn. Patching as
In Proceedings of the 35th IEEE/ACM International
translation: The data and the metaphor.
Conference on Automated Software Engineering, ASE ’20, pp. 275–286, New York, NY, USA,
2020. Association for Computing Machinery.
ISBN 9781450367684. doi: 10.1145/3324884.
3416587. URL https://doi.org/10.1145/3324884.3416587.

Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. Automated program repair. Commun.
ACM, 62(12):56–65, nov 2019. ISSN 0001-0782. doi: 10.1145/3318162. URL https://doi.
org/10.1145/3318162.

Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. Deepﬁx: Fixing common c language
errors by deep learning. In Thirty-First AAAI Conference on Artiﬁcial Intelligence, volume 31,
Feb. 2017. doi: 10.1609/aaai.v31i1.10742. URL https://ojs.aaai.org/index.php/
AAAI/article/view/10742.

Yang Hu, Umair Z. Ahmed, Sergey Mechtaev, Ben Leong, and Abhik Roychoudhury. Re-
In Proceedings of the
factoring based program repair applied to programming assignments.
34th IEEE/ACM International Conference on Automated Software Engineering, ASE ’19, pp.
ISBN 9781728125084. doi: 10.1109/ASE.2019.00044. URL
388–398. IEEE Press, 2019.
https://doi.org/10.1109/ASE.2019.00044.

Faria Huq, Masum Hasan, Md Mahim Anjum Haque, Sazan Mahbub, Anindya Iqbal, and Touﬁque
Ahmed. Review4repair: Code review aided automatic program repairing. Information and Soft-
ware Technology, 143:106765, 2022.

Ren´e Just, Darioush Jalali, and Michael D. Ernst. Defects4j: A database of existing faults to enable
controlled testing studies for java programs. In Proceedings of the 2014 International Symposium
on Software Testing and Analysis, ISSTA 2014, pp. 437–440, New York, NY, USA, 2014. Asso-
ciation for Computing Machinery. ISBN 9781450326452. doi: 10.1145/2610384.2628055. URL
https://doi.org/10.1145/2610384.2628055.

Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim. Automatic patch generation learned
from human-written patches. In Proceedings of the 2013 International Conference on Software

11

Preprint

Engineering, ICSE ’13, pp. 802–811. IEEE Press, 2013. ISBN 9781467330763. doi: 10.1109/
ICSE.2013.6606626. URL https://doi.org/10.1109/ICSE.2013.6606626.

Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy S
Liang. Spoc: Search-based pseudocode to code. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.
cc/paper/2019/file/7298332f04ac004a0ca44cc69ecf6f6b-Paper.pdf.

Claire Le Goues, Michael Dewey-Vogt, Stephanie Forrest, and Westley Weimer. A systematic study
of automated program repair: Fixing 55 out of 105 bugs for $8 each. In 2012 34th International
Conference on Software Engineering (ICSE), pp. 3–13, 2012. doi: 10.1109/ICSE.2012.6227211.
URL https://doi.org/10.1109/ICSE.2012.6227211.

Claire Le Goues, Neal Holtschulte, Edward K. Smith, Yuriy Brun, Premkumar Devanbu, Stephanie
Forrest, and Westley Weimer. The ManyBugs and IntroClass benchmarks for automated repair of
C programs. IEEE Transactions on Software Engineering (TSE), 41(12):1236–1256, December
ISSN 0098-5589. doi: 10.1109/TSE.2015.2454513. URL https://doi.org/10.
2015.
1109/TSE.2015.2454513.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-
In Proceedings of
training for natural language generation, translation, and comprehension.
the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880, On-
line, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703.
URL https://aclanthology.org/2020.acl-main.703.

Derrick Lin, James Koppel, Angela Chen, and Armando Solar-Lezama. Quixbugs: A multi-
In Proceedings Com-
lingual program repair benchmark set based on the quixey challenge.
panion of the 2017 ACM SIGPLAN International Conference on Systems, Programming, Lan-
guages, and Applications: Software for Humanity, SPLASH Companion 2017, pp. 55–56, New
York, NY, USA, 2017. Association for Computing Machinery.
ISBN 9781450355148. doi:
10.1145/3135932.3135941. URL https://doi.org/10.1145/3135932.3135941.

I Loshchilov and F Hutter. Decoupled weight decay regularization, 7th international conference on

learning representations, iclr. New Orleans, LA, USA, May, (6-9):2019, 2019.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,
Michele Tufano, MING GONG, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,
Shengyu Fu, and Shujie LIU. CodeXGLUE: A machine learning benchmark dataset for code
understanding and generation. In Thirty-ﬁfth Conference on Neural Information Processing Sys-
tems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/
forum?id=6lE4dQXaUcb.

Gayle Laakmann McDowell. Cracking the Coding Interview: 189 Programming Questions and

Solutions. CareerCup, 2019.

Tom Mens. On the complexity of software systems. Computer, 45(08):79–81, 2012. doi: 10.1109/

MC.2012.273. URL https://doi.org/10.1109/MC.2012.273.

Ali Mesbah, Andrew Rice, Emily Johnston, Nick Glorioso, and Edward Aftandilian. Deepdelta:
In Proceedings of the 2019 27th ACM Joint Meeting
Learning to repair compilation errors.
on European Software Engineering Conference and Symposium on the Foundations of Soft-
ware Engineering, ESEC/FSE 2019, pp. 925–936, New York, NY, USA, 2019. Association
for Computing Machinery.
ISBN 9781450355728. doi: 10.1145/3338906.3340455. URL
https://doi.org/10.1145/3338906.3340455.

Martin Monperrus. Automatic software repair: A bibliography. ACM Comput. Surv., 51(1), jan
ISSN 0360-0300. doi: 10.1145/3105906. URL https://doi.org/10.1145/

2018.
3105906.

12

Preprint

National Institute of Standards and Technology. The economic impacts of inadequate infrastructure

for software testing. U.S. Department of Commerce Technology Administration, 2002.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pp. 311–318, Philadelphia, Pennsylvania, USA, July 2002.
Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https:
//aclanthology.org/P02-1040.

Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov,
Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh
Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss. Codenet: A large-
scale AI for code dataset for learning a diversity of coding tasks. In Thirty-ﬁfth Conference on
Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL
https://openreview.net/forum?id=6vZVBkCDrHT.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http:
//jmlr.org/papers/v21/20-074.html.

Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou,
Ambrosio Blanco, and Shuai Ma. Codebleu: a method for automatic evaluation of code synthesis.
arXiv preprint arXiv:2009.10297, 2020.

Hyunmin Seo, Caitlin Sadowski, Sebastian Elbaum, Edward Aftandilian, and Robert Bowdidge.
Programmers’ build errors: A case study (at google). In Proceedings of the 36th International
Conference on Software Engineering, ICSE 2014, pp. 724–734, New York, NY, USA, 2014.
Association for Computing Machinery. ISBN 9781450327565. doi: 10.1145/2568225.2568255.
URL https://doi.org/10.1145/2568225.2568255.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan
Catanzaro. Megatron-lm: Training multi-billion parameter language models using model par-
allelism. arXiv preprint arXiv:1909.08053, 2019.

Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys
Poshyvanyk. An empirical investigation into learning bug-ﬁxing patches in the wild via neu-
In Proceedings of the 33rd ACM/IEEE International Conference on
ral machine translation.
Automated Software Engineering, ASE 2018, pp. 832–837, New York, NY, USA, 2018. Associ-
ation for Computing Machinery. ISBN 9781450359375. doi: 10.1145/3238147.3240732. URL
https://doi.org/10.1145/3238147.3240732.

Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, and Denys Poshyvanyk.
On learning meaningful code changes via neural machine translation. In Proceedings of the 41st
International Conference on Software Engineering, ICSE ’19, pp. 25–36. IEEE Press, 2019. doi:
10.1109/ICSE.2019.00021. URL https://doi.org/10.1109/ICSE.2019.00021.

Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven C.H. Hoi. CodeT5: Identiﬁer-aware uniﬁed
pre-trained encoder-decoder models for code understanding and generation. In Proceedings of
the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 8696–8708,
Online and Punta Cana, Dominican Republic, November 2021. Association for Computational
Linguistics. doi: 10.18653/v1/2021.emnlp-main.685. URL https://aclanthology.org/
2021.emnlp-main.685.

Michihiro Yasunaga and Percy Liang. Break-it-ﬁx-it: Unsupervised learning for program repair.
In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, volume
139, pp. 11941–11952. PMLR, 2021. URL http://proceedings.mlr.press/v139/
yasunaga21a.html.

13

Preprint

A LIMITATIONS

There are a few limitations of the dataset presented in this work. In our experiments, we manually
analyze the mistakes made by models with and without access to verdict information. We ﬁnd that
common model mistakes include additions of code segments that are unnecessary to ﬁx a given bug.
We believe this is due to insufﬁcient information about the bug and the problem. Another limita-
tion of this work is that, while the code bug ﬁx pair dataset can be extended for other programming
languages, code and test suite expansion for more problems is solely dependent upon AtCoder avail-
ability. Also, the execution-based evaluation metric calculation requires substantial amount of time
to evaluate a single program. To reduce computation complexity, we select two datapoints at random
per problem from the test dataset. Nevertheless, parallelism and high performance computing can
be utilized towards making execution-based evaluation faster and more efﬁcient for large-scale data.
In terms of research insights, our dataset incorporates Java and Python bugs, and may not generalize
to other programming languages. Further research is necessary to evaluate the impact of FixEval
for analyzing deep learning models for additional languages. Additionally, competitive programs
submitted online may not accurately reﬂect real-world software bugs from professional develop-
ers. More work is necessary to develop benchmarks that simulate authentic software programs to
evaluate deep learning models for automated program repair.

B FURTHER DETAILS OF OUR ANALYSIS

B.1 DESCRIPTION OF TEST SUITE COLLECTION

On the AtCoder website we found the names of each contest.9 Contests are named in the format of
ABC123, AGC123, or ARC123 to indicate the AtCoder Beginner Contest, AtCoder Grand Contest,
or AtCoder Regular Contest, followed by the speciﬁc contest number (i.e. 123 in this example) to
create a unique identiﬁer for a given problem. The name and contest number are also stored on
CodeNet, where we also get the exact same name and the contest number with problems sorted by
difﬁculty. We then match these to retreive the test cases in addition to input and expected output
values from the DropBox link to create the test suite for FIXEVAL.

B.2 LIST OF VERDICTS

The following is a list of all the possible verdict outcomes for submitted competitive programs with
a brief description:

• Accepted (AC): Passed all test cases.
• Wrong Answer (WA): Failed one or more test cases.
• Compile Error (CE): Program did not compile.
• Runtime Error (RE): Program execution was not successful.
• Presentation Error (PE): Output is correct, but it is not formatted in the proper way.
• Time Limit Exceeded (TLE): The program did not run within the intended time limit.
• Memory Limit Exceeded (MLE): The program did not run within the intended memory limit.
• Output Limit Exceeded (OLE): Program tried to write too much information.
• Waiting for Judging (WJ): Judge is busy.
• Waiting for Re-judging (WR): Waiting for Judge to run the tests again.
• Judge Not Available (JNA): Error encountered by Judge.
• Internal Error (IE): Judge encountered an error or the problem setter’s conﬁguration is incorrect.

9https://atcoder.jp/contests/

14

Preprint

Buggy Program (verdict: Wrong Answer)

Fixed Program

p u b l i c s t a t i c v o i d main ( S t r i n g [ ] args ) {
Scanner sc = new Scanner ( System . i n ) ;
i n t a = sc . n e x t I n t ( ) ;
i n t b = sc . n e x t I n t ( ) ;
l o n g ans = a * b / gcd ( a , b ) ;
System . o u t . p r i n t l n ( ans ) ;
sc . c l o s e ( ) ;

}
p u b l i c s t a t i c l o n g gcd ( l o n g m, l o n g n ) {

j a v a . u t i l . * ;
1 i m p o r t
2 i m p o r t
j a v a . l a n g . * ;
3 p u b l i c c l a s s Main {
4
5
6
7
8
9
10
11
12
13
14
15
16
17 }

(m < n )
( n==0)

}

r e t u r n gcd ( n , m) ;

i f
i f
r e t u r n gcd ( n , m % n ) ;

r e t u r n m;

p u b l i c s t a t i c v o i d main ( S t r i n g [ ] args ) {
Scanner sc = new Scanner ( System . i n ) ;
l o n g a = sc . n e x t I n t ( ) ;
l o n g b = sc . n e x t I n t ( ) ;
l o n g ans = a * b / gcd ( a , b ) ;
System . o u t . p r i n t l n ( ans ) ;
sc . c l o s e ( ) ;

}
p u b l i c s t a t i c l o n g gcd ( l o n g m, l o n g n ) {

j a v a . u t i l . * ;
1 i m p o r t
2 i m p o r t
j a v a . l a n g . * ;
3 p u b l i c c l a s s Main{
4
5
6
7
8
9
10
11
12
13
14
15
16
17 }

(m < n )
( n==0)

}

r e t u r n gcd ( n , m) ;

i f
i f
r e t u r n gcd ( n , m % n ) ;

r e t u r n m;

Buggy Program (verdict: Compilation Error)

Fixed Program

1 i m p o r t
j a v a . u t i l . * ;
2 p u b l i c c l a s s Main {
3
4
5
6
7
8
9
10
11
12
13
14 }

}
e l s e {

}

}

p u b l i c s t a t i c v o i d main ( S t r i n g [ ] args ) {
Scanner sc = new Scanner ( System . i n ) ;
i n t a = sc . n e x t I n t ( ) ;
i n t b = sc . n e x t I n t ( ) ;
i f ( ( A − B) % 2 == 0 ) {

System . o u t . p r i n t l n ( ( A + B) / 2 ) ;

p u b l i c s t a t i c v o i d main ( S t r i n g [ ] args ) {
Scanner sc = new Scanner ( System . i n ) ;
i n t a = sc . n e x t I n t ( ) ;
i n t b = sc . n e x t I n t ( ) ;
i f ( ( a−b ) % 2 == 0 ) {

System . o u t . p r i n t l n ( ( a+b ) / 2 ) ;

System . o u t . p r i n t l n ( ” IMPOSSIBLE ” ) ;

System . o u t . p r i n t l n ( ” IMPOSSIBLE ” ) ;

1 i m p o r t
j a v a . u t i l . * ;
2 p u b l i c c l a s s Main {
3
4
5
6
7
8
9
10
11
12
13
14 }

}
e l s e {

}

}

Figure 6: Examples of successful ﬁxes of buggy programs in Java when the verdict information
is provided as additional model input. Buggy and ﬁxed statements are marked in red and green,
respectively.

15

Preprint

Model learned to cast the output

Buggy Program in Python

Fixed Program in Python

1 n = i n t ( i n p u t ( ) )
2 n = p r i n t ( i n t ( n−1+1) * ( n −1) / 2 )

1 n = i n t ( i n p u t ( ) )
2 n = p r i n t ( i n t ( ( n−1+1) * ( n −1) / 2 ) )

Model learned add sort function correctly

Buggy Program in Python

Fixed Program in Python

1 n = i n t ( i n p u t ( ) )
2 a = l i s t (map( i n t , i n p u t ( ) . s p l i t ( ) ) ) . s o r t ( )
3 p r i n t ( a [ −1] − a [ 0 ] )

1 n = i n t ( i n p u t ( ) )
2 a = l i s t (map( i n t , i n p u t . s p l i t ( ) ) )
3 a . s o r t ( )
4 p r i n t ( a [ −1] − a [ 0 ] )

Model Learned to change the comparison sign

Buggy Program in Python

Fixed Program in Python

i n p u t ( ) . s p l i t ( ) )

1 N, K = map( i n t ,
2 h = i n p u t ( ) . s p l i t ( )
3 c = 0
i
f o r
4
5
6
7 p r i n t ( c )

i n range (N) :
i n t ( h [ i ] ) > k :

c += 1

i f

i n p u t ( ) . s p l i t ( ) )

1 N, K = map( i n t ,
2 h = i n p u t ( ) . s p l i t ( )
3 c = 0
i
f o r
4
5
6
7 p r i n t ( c )

i n range (N) :
i n t ( h [ i ] ) > k :

c += 1

i f

Model Learned to change loop range for corrcetion

Buggy Program in Python

Fixed Program in Python

1 from c o l l e c t i o n s i m p o r t d e f a u l t d i c t
2 N = i n t ( i n p u t ( ) )
3 d = d e f a u l t d i c t ( i n t )
f o r n i n range (N) :
4
5
6
7
8
9 s = 0
i
f o r
10
f o r
11
12

n = s t r ( n )
a = n [ 0 ]
b = n [ − 1 ]
d [ ( a , b ) ] += 1

i n range ( 1 , 10) :
i n range ( 1 0 ) :
j

s += d [ ( s t r ( j ) , s t r ( i ) ) ]

s t r ( j ) ) ]

13 p r i n t ( s )

1 from c o l l e c t i o n s i m p o r t d e f a u l t d i c t
2 N = i n t ( i n p u t ( ) )
3 d = d e f a u l t d i c t ( i n t )
f o r n i n range (N+1) :
4
5
6
7
8
9 s = 0
i
f o r
10
f o r
11
12

n = s t r ( n )
a = n [ 0 ]
b = n [ − 1 ]
d [ ( a , b ) ]+=1

i n range ( 1 , 1 0 ) :
i n range ( 1 0 ) :
j

s += d [ ( s t r ( j ) , s t r ( i ) ) ]

s t r ( j ) ) ]

13 p r i n t ( s )

* d [ ( s t r ( i ) ,

* d [ ( s t r ( i ) ,

Figure 7: Examples of successful ﬁxes of buggy programs in Python. Buggy and ﬁxed statements
are marked in red and green, respectively.

16

Preprint

Model learned to copy entire line out of scope
Buggy Program (Wrong Answer)

Fixed Program

p u b l i c s t a t i c v o i d main ( S t r i n g [ ] args ) {
Scanner sc = new Scanner ( System . i n ) ;
i n t D = sc . n e x t I n t ( ) ;
i n t N = sc . n e x t I n t ( ) ;
i n t a ;
a = ( i n t ) Math . pow( 1 0 0 , D ) *N;
i f

a = ( i n t ) Math . pow( 1 0 0 , D) * (N+1) ;
System . o u t . p r i n t l n ( a ) ;

1 i m p o r t
j a v a . u t i l . * ;
2 p u b l i c c l a s s Main {
3
4
5
6
7
8
9
10
11
12
13
14 }

(N == 100){

}

}

p u b l i c s t a t i c v o i d main ( S t r i n g [ ] args ) {
Scanner sc = new Scanner ( System . i n ) ;
i n t D = sc . n e x t I n t ( ) ;
i n t N = sc . n e x t I n t ( ) ;
i n t a ;
a = ( i n t ) Math . pow( 1 0 0 , D ) *N;
i f

(N == 100){

a = ( i n t ) Math . pow( 1 0 0 , D) * (N+1) ;

1 i m p o r t
j a v a . u t i l . * ;
2 p u b l i c c l a s s Main {
3
4
5
6
7
8
9
10
11
12
13
14 }

}

}
System . o u t . p r i n t l n ( a ) ;

Model learned change logic from ”or” to ”and”
Buggy Program (Compilation Error)

Fixed Program

1 i m p o r t
j a v a . u t i l . * ;
2 p u b l i c c l a s s Main {
3
4
5
6
7
8
9
10
11
12
13
14 }

}
e l s e {

}

}

p u b l i c s t a t i c v o i d main ( S t r i n g [ ] args ) {
Scanner sc = new Scanner ( System . i n ) ;
i n t a = sc . n e x t I n t ( ) ;
i n t b = sc . n e x t I n t ( ) ;
i f

( a <= 8 | | b <= 8 ) {

System . o u t . p r i n t l n ( ” Yay ! ” ) ;

p u b l i c s t a t i c v o i d main ( S t r i n g [ ] args ) {
Scanner sc = new Scanner ( System . i n ) ;
i n t a = sc . n e x t I n t ( ) ;
i n t b = sc . n e x t I n t ( ) ;
i f

( a <= 8 && b <= 8 ) {

System . o u t . p r i n t l n ( ” Yay ! ” ) ;

System . o u t . p r i n t l n ( ” : ( ” ) ;

System . o u t . p r i n t l n ( ” : ( ” ) ;

1 i m p o r t
j a v a . u t i l . * ;
2 p u b l i c c l a s s Main {
3
4
5
6
7
8
9
10
11
12
13
14 }

}
e l s e {

}

}

Model Learned to change return statement to a print statement

Buggy Program (Compilation Error)

Fixed Program

1 i m p o r t
j a v a . u t i l . * ;
2 p u b l i c c l a s s Main {
3
4
5
6
7
8
9
10 }

p u b l i c s t a t i c v o i d main ( S t r i n g [ ] args ) {
Scanner sc = new Scanner ( System . i n ) ;
i n t a = sc . n e x t I n t ( ) ;
i n t a2 = a * a ;
i n t a3 = a2 * a ;
r e t u r n a + a2 + a3 ;

}

1 i m p o r t
j a v a . u t i l . * ;
2 p u b l i c c l a s s Main {
3
4
5
6
7
8
9
10 }

p u b l i c s t a t i c v o i d main ( S t r i n g [ ] args ) {
Scanner sc = new Scanner ( System . i n ) ;
i n t a = sc . n e x t I n t ( ) ;
i n t a2 = a * a ;
i n t a3 = a2 * a ;
System . o u t . p r i n t l n ( a + a2 + a3 ) ;

}

Model Learned to change syntax and compilation error

Buggy Program (Compilation Error)

Fixed Program

j a v a . u t i l . *

1 i m p o r t
2 p u b l i c c l a s s Main {
3
4
5
6
7
8
9
10 }

p u b l i c s t a t i c v o i d main ( S t r i n g [ ] args ) {
Scanner sc = new Scanner ( System . i n ) ;
i n t A = sc . n e x t I n t ( ) ;
i n t B = sc . n e x t I n t ( ) ;
i n t T = sc . n e x t I n t ( ) ;
i n t S = T / A System . o u t . p r i n t l n ( s * b ) ;

}

j a v a . u t i l . *

1 i m p o r t
2 p u b l i c c l a s s Main {
3
4
5
6
7
8
9
10
11 }

p u b l i c s t a t i c v o i d main ( S t r i n g [ ] args ) {
Scanner sc = new Scanner ( System . i n ) ;
i n t A = sc . n e x t I n t ( ) ;
i n t B = sc . n e x t I n t ( ) ;
i n t T = sc . n e x t I n t ( ) ;
i n t S = T / A ;
System . o u t . p r i n t l n ( s *B) ;

}

Figure 8: Examples of successful ﬁxes of buggy programs in Java. Buggy and ﬁxed statements are
marked in red and green, respectively.

17

