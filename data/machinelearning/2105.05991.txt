Improving Code Autocompletion with Transfer
Learning

Wen Zhou
Facebook Inc.
Menlo Park, U.S.A.
zhouwen@fb.com

Seohyun Kim
Facebook Inc.
Menlo Park, U.S.A.
skim131@fb.com

Vijayaraghavan Murali
Facebook Inc.
Menlo Park, U.S.A.
vijaymurali@fb.com

Gareth Ari Aye
Facebook Inc.
Menlo Park, U.S.A.
gaa@fb.com

1
2
0
2

t
c
O
3
1

]
E
S
.
s
c
[

2
v
1
9
9
5
0
.
5
0
1
2
:
v
i
X
r
a

Abstract—Software language models have achieved promising
results predicting code completion usages, and several industry
studies have described successful IDE integrations. Recently,
accuracy in autocompletion prediction improved 12.8% [1] from
training on a real-world dataset collected from programmers’
IDE activity. But what if limited examples of IDE autocompletion
in the target programming language are available for model
training? In this paper, we investigate the efﬁcacy of pretraining
autocompletion models on non-IDE, non-autocompletion, and
different-language example code sequences. We ﬁnd that these
unsupervised pretrainings improve model accuracy by over 50%
on very small ﬁne-tuning datasets and over 10% on 50k labeled
examples. We conﬁrm the real-world impact of these pretrainings
in an online setting through A/B testing with thousands of IDE
autocompletion users, ﬁnding that pretraining is responsible for
increases of up to 6.63% autocompletion usage.

Index Terms—Machine learning, neural networks, software
language models, naturalness, code completion, integrated de-
velopment environments, software tools

I. INTRODUCTION

Autocompletion is the most frequently used IDE feature [2].
Signiﬁcant attention has been given to improving suggestion
prediction through machine learning [3]–[6] by feeding code to
models as a sequence of tokens or even AST nodes [7]. Figure
1 shows an example of autocomplete powered by deep learning
in an IDE. Several recent studies [1], [8] have demonstrated the
strengths of real-world and weaknesses of synthetic datasets in
training and evaluating autocompletion models. Concept drift
between real-world and synthetic examples can be ameliorated
by only showing models real-world autocompletion selections.
But what if autocompletion examples from IDE usage in the
target language are hard to come by?

Even with a large user population, the tool’s vendor may
not be able to log autocompletion events to build a training
dataset. But perhaps the vendor is able to collect random, non-
autocompletion code sequences during code authoring. These
would sufﬁce to train a task-agnostic language model (LM)
for autocompletion prediction, but under the assumption that
tokens used in autocompletion follow the same distribution as
tokens in arbitrary code sequences. One ﬂaw in this assump-
tion is that modern IDE autocompletion tools restrict the token
types which can be suggested, commonly disallowing punctu-
ation, literals, and variable declarations. Furthermore, several
recent studies [1], [8] have shown a myriad of differences
between random tokens sampled from source code and those

Fig. 1: Example of autocomplete in an IDE. The ﬁrst 3 suggestions with
thunderbolt icons are provided by our deep learning model.

used in autocompletion. Can knowledge from these unlabeled
code sequences transfer to the autocompletion task?

Consider the various stages of code that could be lever-
aged for autocompletion model training. Figure 2 shows the
different code datasets. First, we have code as it appears in
the IDE–snapshots of code taken from real-time code author-
ing. Branching from there, two events that create additional
datasets are autocompletion and commit upload. The former
is when a developer accepts an autocompletion suggestion in
the IDE. The selection, list of suggestions, and surrounding
context are logged. The latter is when a commit is uploaded
to version control. The commit contains a snapshot of each of
the impacted ﬁles. There is an intuitive relationship between
code commits and developers’ IDE activity since a commit
is the ﬁrst code artifact that a developer will produce after
code authoring. Can knowledge from these commits transfer
to modeling code authoring behaviors?

Another potential cause for insufﬁcient labeled data is a

 
 
 
 
 
 
non-uniform programming language usage distribution. While
there are undeniably shared concepts and constructs across
programming languages, there are often major differences in
syntax, programming style, and language constructs. Addition-
ally, language-speciﬁc differences in IDE tools may impact
programmers’ autocompletion behaviors. At Facebook, we
have a large developer activity dataset in the Hack program-
ming language containing millions of real-world autocomple-
tion examples. But there is also a long tail of less frequently
used languages such as Rust. These languages lack enough
labeled data for robust model training, but they are still used
by a sufﬁcient developer population to warrant autocompletion
support. Can knowledge from the more popular languages
transfer to others where labeled data is insufﬁcient?

In this paper, we explore these questions regarding trans-
fer learning for autocompletion empirically. Advances in
Transformer-based neural models [9]–[13] have popularized
transfer learning in the deep learning community. Often trans-
fer learning consists of “pre-training” such a model on large
corpora of unlabeled data in an unsupervised manner and then
“ﬁne-tuning” on a smaller corpus of labeled data. The latter
ﬁne-tuning corpus is typically drawn from the same distribu-
tion as the test dataset. In our study, the evaluation task is to
predict IDE autocompletion selections made by programmers
in a given language, and our ﬁne-tuning datasets consist of
real-world autocompletion events collected from IDE usage at
Facebook. As visualized in Figure 3, we explore the effect
of transfer learning by pretraining models on non-IDE, non-
autocompletion, and different programming language code
sequences. Speciﬁcally, we answer the following research
questions:

RQ1: How do autocompletion models beneﬁt from combining
unsupervised pretraining with task-speciﬁc ﬁne-tuning? How
does their performance improve across ofﬂine and online
evaluation?

Our experimental results show that pretraining on code
sequences collected during code authoring and ﬁne-tuning
on tokens selected through autocompletion produces models
which outperform training on only one of these two datasets.
We show further that such a model drives greater online tool
usage.

RQ2: What is the impact of pretraining on a large source
code dataset obtained from outside of code authoring? Can
these pretrained software language models be ﬁne-tuned on
IDE autocompletion to achieve better accuracy with fewer
real-world examples?

We show that starting from a model pretrained on ﬁles
appearing in version control commits drastically decreases the
number of real-world autocompletion examples required to
achieve high accuracy. Additionally, our experiments suggest
that there is diminishing marginal beneﬁt to pretraining as the
number of real-world examples grows.

Fig. 2: Different code authoring stages that could be used for autocompletion
training data. IDE dataset consists of snapshots of source code ﬁles collected
during code authoring. Once a developer submits the code to code review, it
becomes part of Facebook’s version control data. From the IDE, a developer
can also choose an autocompletion suggestion, which would be logged (along
with the surrounding context code).

Fig. 3: 3D illustration of the transfer learning space. We explored transfer
learning across tasks (RQ1), domains (RQ2), and languages (RQ3) in this
paper.

RQ3: Consider the case where a large training corpus is
available in one language but not another. Can pretraining
a multilingual model on the language with more training
data beneﬁt the language with less data?

To answer

this question, we pretrain a multilingual
model on examples from one language before ﬁne-tuning
results show that many
on examples from another. Our
fewer target-language examples are required to achieve high
accuracy after pretraining on different-language examples.
We again observe diminishing marginal beneﬁt to pretraining
on different-language examples as the number of available
target-language examples grows.

Contributions

1) We pretrain two transformer software language models
GPT-2 [9] and BART [11] on source code ﬁles obtained
from version control commits and show how their per-
formance on autocompletion prediction improves through
ﬁne-tuning on real-world IDE code sequences.

2) The GPT-2 model is trained on two real-world datasets:
code sequences logged during IDE authoring and auto-
completion selections. A third variant is pretrained on the
former and ﬁne-tuned on the latter corpus to demonstrate
how the combination of pretraining and task-speciﬁc ﬁne-
tuning lead to a superior model, outperforming the base
model by 3.29%.

3) We show that pretraining on a different programming
language boosts accuracy by 13.1% when comparing a
model pretrained on Hack examples and ﬁne-tuned on
10k Python examples versus only training on Python
examples.

4) We prove that improvements across these three trans-
fer learning dimensions—task, domain, and language
—translate into increased autocompletion tool usage by
3.86%, 6.63%, and 3.62%, respectively, comparing these
models through online A/B tests.

Outline

The rest of this paper is organized to ﬁrst introduce our
experimental setup in Section II. In this section we describe
the corpora, language models, and evaluation methodologies
employed in our study. Section III reports experimental results,
supporting the answers to our research questions. Section IV,
Section V, and Section VI discuss threats to validity, related
work, and future work, respectively. Finally, Section VII
concludes with a summation and key insights for autocomplete
tool designers.

II. EXPERIMENTAL SETUP

A. Datasets

shows a diagram of the datasets as part of the various stages
of the code cycle.

1) Autocompletion: Autocompletion events logged when-
ever a programmer accepts a completion suggestion.
The accepted suggestion, other unused suggestions (from
static analysis), and the surrounding program context are
logged.

2) IDE: Snapshots of source code ﬁles collected during code
authoring. In addition to capturing the ﬁle contents, we
collect the cursor position so that speciﬁc code fragments
undergoing modiﬁcation can be identiﬁed. This dataset
shares the code authoring domain with Autocompletion,
but while Autocompletion is task-speciﬁc, IDE is impor-
tantly task-agnostic.

3) Commit: The set of ﬁles created or modiﬁed in a version
control commit and uploaded to Facebook’s code review
service. This version control dataset is included in our
investigation to explore transfer learning across domains
in light of the concept drift reported in [1] and [8]. In a
typical programming workﬂow, version control commits
represent the software artifacts with the closest relation-
ship to IDE code authoring.

4) All: A union of Autocompletion and IDE constructed to
explore the effect of mixing labeled data into pretraining.
It’s important to note that the datasets we explore contain
minimal overlap since they were collected from different time
ranges. Major differences between these three datasets are
catalogued in [1]. Furthermore, training and testing datasets
were split by a random cutoff period, ensuring no data leaks
between the datasets.

Additionally, a valid concern is raised in [14] regarding the
potential of duplicate examples in source code models split
between training and evaluation datasets due to the prevalence
of copy-paste in software development. In a nutshell, model
accuracy may be overestimated if many held-out test examples
appear in the model’s training data owing to code clones.
Luckily, this issue does not apply in our evaluation since
the test dataset consists of real-world autocompletion events.
Unlike fragments of source code, programmers will never copy
autocompletion events. Furthermore, all of the improvements
we attribute to transfer learning are conﬁrmed in online A/B
tests, so increased model performance cannot be caused by
ﬂaws in our held-out test dataset.

We train a variety of monolingual models on examples from
only one of Hack and Python as well as several multilingual
models on a union of examples from both languages. When
training on a union of Hack and Python examples, we construct
the model vocabulary V = Vhack ∪ Vpython. Code sequences
fed to our multilingual model are modiﬁed by prepending a
control code [15] to indicate the source language.

This study’s datasets (summarized in Table I) are sourced
from real-world developer activity at Facebook. We focus on
two languages, Hack and Python, with signiﬁcant developer
populations at Facebook. Going forward, we will refer to each
dataset by [language][dataset] (e.g. HackCommit). Figure 2

B. Tokenization

One difﬁculty in modeling source code as compared to
natural language is that code introduces new vocabulary at a
far higher rate [16]. Recognizing and predicting rare and novel
tokens from an open vocabulary poses a challenge when our

TABLE I: Various datasets used in this paper.

Python

Hack

Autocompletion
IDE
Commit

# tokens
543,401,684
540,200,385
-

# samples
3,201,299
3,201,299
-

# tokens
642,886,605
639,094,120
629,192,335

# samples
3,792,485
3,792,485
3,452,434

models are trained to recognize a ﬁxed set of terms. A strategy
explored in [16] is to model code as a sequence of partial
tokens. This scheme allows an open vocabulary of code tokens
to be represented using a ﬁxed-size vocabulary. Another idea
from [7] is copy mechanism where out-of-vocabulary (OOV)
tokens are encoded so that model can recognize matches and
predict these terms by reference. In this study we tokenize
code fragments in two different ways:

a) Byte-pair encoding (BPE): This scheme used in our
BART model tokenizes source code tokens as a sequence
of partial tokens. Common character sequences are encoded
with a single subword whereas rare character sequences are
encoded with longer sequences of shorter subwords. BPE has
been applied successfully across natural language [17] and
programming language [16] modeling.

b) Bigram encoding + copy mechanism: This scheme
used in our GPT-2 model tokenizes snake case and camel-
Case identiﬁer tokens as exactly two subtokens (bigrams).
We selected this encoding strategy for online experiments in
autocompletion ranking since it yields a short, ﬁxed-height
partial token tree to search during inference. In our online de-
ployment, we worked with a 100ms latency budget for model
predictions. Searching a height-2 tree is fast enough to meet
low latency user experience requirements in autocompletion.
Concretely, consider a vocabulary V of partial tokens and an
example token t = ‘‘ fooBarBazQuux”. First t is broken into
a list of partial tokens [‘‘ foo ”,
‘‘ Bar”, ‘‘ Baz”, ‘‘ Quux”].
Then we choose a split point that cuts the list into two equal
length sublists and join the elements of each list to form a
bigram (b1, b2) = (‘‘ fooBar”, ‘‘ BazQuux”). Finally the ith
unique, OOV bigram bi /∈ V is replaced with a synthetic token
<var−i> as in [18]. The special case of t ∈ V receives a
length-2 encoding of [ t , </t>] where </t> is a synthetic
end-of-token identiﬁer.

C. Models

In this paper, we evaluate the effects of transfer learning us-
ing two models, both incorporating Transformer architectures.
Since the main focus of this paper is to examine the impact
of transfer learning, we limit our focus to these two models
and do not compare their performance to other state-of-the-art
models. Our experiments leverage:

a) GPT-2: a decoder transformer model

[9], which
has achieved state-of-the-art performance in code prediction
[6] due to the transformer’s ability to observe all sequence
elements simultaneously in its self-attention blocks.

b) BART: a bidirectional model that utilizes an encoder
to encode the context surrounding the code completion point,

as well as a GPT-2-like decoder for auto-regressive generation
[11]. It is trained on a denoising objective. BART demonstrates
state-of-the-art performance across a variety of source code
modeling tasks in [13].

D. Training

Each of the software language models were trained in
two phases. The ﬁrst phase is pretraining, in which models
are shown a large number of source code examples drawn
from a dataset with concept drift from the evaluation dataset.
The second phase is ﬁne-tuning where models are shown
source code examples drawn from the same distribution as
the evaluation dataset. Some models are ﬁne-tuned twice (e.g.
row 3 of Table II) to achieve knowledge transfer across two
different axes). All models were trained for up to twenty
epochs (with early termination at convergence) using Nvidia
Tesla V100 GPUs. The learning rates for pretraining and ﬁne-
tuning were set to 5−4 and 5−6 respectively1.

E. Evaluation

We measure the performance of our study’s models through

both ofﬂine and online evaluation.

a) Ofﬂine evaluation: For ofﬂine evaluation, we use 10%
of HackAutocompletion and PythonAutocompletion as held-out
test datasets. The evaluation task is for our autocompletion
models to predict users’ real, historical IDE autocompletion se-
lections given the surrounding program context and suggestion
list (from static analysis). HackAutocompletion examples have
an average of 99.5 candidate suggestions to choose from and
PythonAutocompletion examples have an average of 26.3. The
candidate suggestions list allows us to frame our evaluation as
a ranking problem. For each ofﬂine measurement, we report
top-1 and top-3 accuracy as well as mean reciprocal rank at k
= 3 (MRR@3). MRR is deﬁned as:

MRR =

1
n

n
(cid:88)

i=1

1
ranki

(1)

where n is the size of test dataset and ranki is the rank of
the correct token predicted by the model as the ith candidate.
In our evaluation, we only consider the top k = 3 results
(otherwise the score will be zero).

b) Online evaluation: The ultimate goal of our research
is to improve the developer IDE experience. While ofﬂine
evaluation is faster and lower-cost,
it’s imperative to test
improvements with real users. In this study, we ran several
live A/B experiments with thousands of Hack developers
at Facebook. In each experiment, developers are randomly
assigned to an experiment or control group, and we measure
daily completions per user (DCPU). DCPU refers to the raw
number of autocompletion suggestions a developer accepts on
a given day. Using this metric, A/B test observations are taken
as the number of times a given developer in one of these two
groups uses autocompletion on a given day. We conduct each

1 We reduced the ﬁne-tuning learning rate after observing models aban-
doning solutions discovered during pretraining and overﬁtting the smaller ﬁne-
tuning datasets.

trained on HackAutocompletion without pretraining (same
setting as in ofﬂine evaluation Table III rows 3 and 1).
Experiment 2 in table II (visualized in ﬁgure 4) shows an even
larger improvement over the model without pretraining—the
experiment group DCPU was 6.63% greater at p = 0.017.

RQ2: What is the effect of pretraining on a large source
code dataset obtained from outside of code authoring? Can
pretrained software language models be ﬁne-tuned on IDE
autocompletion to achieve better accuracy with fewer real-
world examples?

Ofﬂine evaluation. Given limited real-world examples of
IDE autocompletion, pretraining on version control commits
can make a big difference. However, as the number of
the beneﬁt of pretraining
autocompletion examples grows,
diminishes. These observations held constant across the GPT-2
and BART models we pretrained on a commits dataset.

Table VI row 5 vs. 3 shows that HackCommit pretraining
with HackAutocompletion ﬁne-tuning outperforms HackAuto-
completion by 3.29% top-1 accuracy (39.61% vs. 36.32%).
The top-1 accuracy improvement is even greater for BART:
6.52% (51.06% vs. 44.54%). However, pretraining on Hack-
Commit yields worse performance compared to pretraining
on IDE code sequences (HackIde) as shown in row 5-6 in
Table VI. The GPT-2 variant has 1.99% lower top-1 accuracy
(39.61% vs. 41.60%) whereas the BART variant is 2.17%
weaker (51.06% vs. 53.23%). This result aligned with our
expectations as the HackCommit dataset, being sourced from a
different stage of software development, exhibits greater con-
cept drift from autocompletion than code sequences randomly
sampled during code authoring.

We also experimented with stacking the two pretrainings
(ﬁrst HackCommit and then HackAll) before ﬁne-tuning on
HackAutocompletion and found that these models, whether
GPT-2 or BART, did not show meaningful improvement over
a single HackAll pretraining. To understand why multiple
pretrainings did not result in better performance, we conducted
a ﬁnal experiment in which the number of real-world ﬁne-
tuning examples was varied from 0% to 100%. The results
are shown in Figure 5.

What we found is that pretraining on HackCommit has a
diminishing marginal beneﬁt as the number of ﬁne-tuning real-
world completion samples grows. Given a small number of
real-world examples, pretraining on HackCommit has a major
impact. For example, there is an improvement of 17.25%
(37.48% vs. 20.23%) when we limited the number of auto-
completion examples to 25k! However, at a certain point, given
enough training data points drawn from the same distribution
as our evaluation dataset, pretraining on a dataset with domain
concept drift is no longer helpful.

Online evaluation. We conducted an A/B experiment to
explore the real-world impact of pretraining on code commits.
The GPT-2 model trained on 100k IDE + Autocompletion
samples was compared against a variant pretrained on all of
the data from HackCommit and ﬁne-tuned on 100k IDE +

Fig. 4: Production A/B test results of three experiments. In the “pretraining”
test the experiment group model is pretrained on HackIde and the control
group model is not pretrained. In the “ﬁne-tuning” test the experiment group
is ﬁne-tuned on HackAutocompletion and the control group is not ﬁne-tuned.
In the ﬁnal “diff pretraining” test the experiment group model is pretrained on
version control code commits and the control group model is not pretrained.
For all
the experiment group had greater DCPU at a statistically
signiﬁcant p-value.

tests,

experiment until it reaches statistical signiﬁcance of at least
95%2.

III. RESULTS

RQ1: How do autocompletion models beneﬁt from combining
unsupervised pretraining with task-speciﬁc ﬁne-tuning? How
does their performance improve across ofﬂine and online
evaluation?

Ofﬂine evaluation. Autocompletion models ﬁne-tuned on
labeled data (HackAutocompletion) outperform models with-
out task-speciﬁc ﬁne-tuning across ofﬂine and online evalua-
tion. In ofﬂine evaluation, Table III rows 2-3 show that ﬁne-
tuning led to a top-1 accuracy increase from 39.73% to 41.91%
(5.5% improvement) for the GPT-2 model.

When labeled examples were mixed into pretraining (Hack-
All), top-1 accuracy jumped from 40.25% to 41.6% (3.4%
improvement) as shown in Table III rows 4-5. For BART, top-
1 accuracy jumped from 44.91% to 53.23% (18.5% improve-
ment) as shown in Table VI row 2 vs. 6.

The same trends were observed when training Python au-

tocompletion models (Table IV rows 2-3 and 4-5).

Online evaluation. For online evaluation, we trained a
GPT-2 autocompletion model on HackIde. The experiment
variant was then ﬁne-tuned on HackAutocompletion whereas
the control variant did not undergo ﬁne-tuning (same setting as
in ofﬂine evaluation Table III rows 2-3). Experiment 1 in table
II (visualized in ﬁgure 4) shows that daily completions per user
(DCPU) was 3.86% greater in the experiment group at p =
0.0238, consistent with the improvement in ofﬂine evaluation.
We conducted a second A/B experiment comparing the
better model from the previous experiment (pretraining on
HackIde and ﬁne-tuning on HackAutocompletion) to a model

2 Each experiment takes approximately two weeks to reach statistical

signiﬁcance.

TABLE II: Production A/B test results. The evaluation metrics is DCPU - daily completions accepted per user. Our threshold for p-value is 0.05.

Pretraining

Fine-tuning

# unique developers

improvement

p-value

HackIde
HackIde

-
HackIde

-
HackAutocompletion

HackAutocompletion
HackAutocompletion

-
HackCommit

100k (HackAll → HackAutocompletion)
100k (HackAll → HackAutocompletion)

1

2

3

3912
3933

3002
3022

3704
3697

-
0.0386

-
0.0663

-
0.0362

-
0.0238

-
0.0172

-
0.0494

Autocompletion samples. Experiment 3 in table II (visualized
in ﬁgure 4) shows that the pretrained model drove an improve-
ment of 3.62% DCPU at p = 0.049.

RQ3: Consider the case where a large training corpus is
available in one language but not another. Can pretraining
a multilingual model on the language with more training
data beneﬁt the language with less data?

Ofﬂine evaluation. We ﬁrst combined the Hack and Python
corpora to investigate whether a larger, diverse pretraining
would improve performance. In addition to incorporating
multiple languages in pretraining, we tested ﬁne-tuning on
examples from one language against ﬁne-tuning on examples
from multiple. Fine-tuning on a union of Hack and Python
examples MultiAutocompletion led to the best-performing mul-
tilingual model across Hack and Python evaluation. Table V
shows improvements of 0.5% and 1.34% above ﬁne-tuning
on HackAutocompletion and PythonAutocompletion respec-
tively. However, none of the multilingual models showed
signiﬁcant improvement over the best monolingual models.
The best multilingual model had 0.53% better top-1 accuracy
than the best Python model but showed 0.02% worse top-
1 accuracy than the best Hack model. We hypothesize that
combining programming language examples across languages
has a diminishing marginal beneﬁt as the number of examples
available in each language grows.

To verify this hypothesis, we pretrain GPT-2 models on
HackAll, ﬁne-tune on varying amounts of PythonAll, and
evaluate on held-out PythonAutocompletion examples. The
baselines we use for comparison are models with the same
conﬁguration trained on an equal number of PythonAll exam-
ples without any pretraining. This experiment was designed
to show whether models pretrained on Hack data exhibited
superior prediction performance on Python autocompletion
examples, indicating knowledge transfer across programming
languages.

Figure 6 shows that the models pretrained on HackAll had
better performance independent of the number of PythonAll
examples used in ﬁne-tuning. The marginal impact was great-
est when we limited models to only 10k (13.1% better top-1
accuracy, 37.11% vs. 24.01%) and 25k (12.6% better top-
1 accuracy, 41.26% vs. 28.66%) PythonAll examples. This
shows clear evidence of knowledge transfer across program-
ming languages in autocompletion. The performance of the

model pretrained on HackAll and ﬁne-tuned with 25k and
50k PythonAll examples is similar to the performance of
training from scratch on 50k and 100k PythonAll examples,
respectively. This shows that half as many examples were
required for comparable performance after pretraining on an
other-language dataset.

This is a meaningful insight for IDE autocompletion de-
velopers. Consider the case of providing predictive autocom-
pletion ranking for less common programming languages (or
ones for which real-world training examples are scarce). Our
results show that pretraining on real-world examples from
other languages makes it possible to achieve high accuracy
with a relatively small ﬁne-tuning dataset.

Online evaluation. Our multilingual model’s vocabulary
was twice as large as either of the monolingual model
vocabularies because it combines the vocabularies of two
languages. Because language model latency is highly sensitive
to vocabulary size, we could not perform a fair online A/B test
comparing a multilingual source code model to a monolin-
gual one under our experiment conﬁguration. The prediction
latency for the multilingual model was too high.

Fig. 5: Ofﬂine top-1 accuracy evaluation starting at HackCommit checkpoint
vs random initialization as size of (HackAll → HackAutocompletion) ﬁne-
tuning samples increases

IV. RELATED WORK
IDE Autocompletion. Many studies at the intersection of
software development tools and machine learning have in-

TABLE III: Ofﬂine evaluations of mono-lingual GPT-2 models on Hack

Pretraining

Fine-tuning

Convergence Epochs

Top1 Acc

Top3 Acc

MRR

1 -
2 HackIde
3 HackIde
4 HackAll
5 HackAll

HackAutocompletion
-
HackAutocompletion
-
HackAutocompletion

6 HackCommit
7 HackCommit
8 HackCommit
9 HackCommit

-
HackAutocompletion
HackAll
HackAll → HackAutocompletion

6
18
18+11
12
12+7

20
20+4
20+20
20+20+20

0.3632
0.3973
0.4191
0.4025
0.4160

0.3479
0.3961
0.4026
0.4145

0.5614
0.5816
0.5987
0.5858
0.5970

0.5357
0.5857
0.5867
0.5962

0.4508
0.4787
0.4988
0.4835
0.4963

0.4306
0.4801
0.4841
0.4953

TABLE IV: Ofﬂine evaluations of mono-lingual GPT-2 models on Python

Pretraining

Fine-tuning

Convergence Epochs

Top1 Acc

Top3 Acc

MRR

1
2
3
4
5

-
PythonIde
PythonIde
PythonAll
PythonAll

PythonAutocompletion
-
PythonAutocompletion
-
PythonAutocompletion

6
15
15+13
19
19+8

0.5282
0.5610
0.5751
0.5605
0.5723

0.6951
0.7228
0.7286
0.7188
0.7253

0.6029
0.6331
0.6439
0.6313
0.6408

TABLE V: Ofﬂine evaluations of multi-lingual GPT-2 models on Hack and Python

Pretraining

Fine-tuning

Epochs

Top1 Acc

Hack
Top3 Acc

-
MultiIde
MultiIde
MultiIde
MultiIde
MultiAll
MultiAll
MultiAll
MultiAll

MultiAutocompletion
-
HackAutocompletion
PythonAutocompletion
MultiAutocompletion
-
HackAutocompletion
PythonAutocompletion
MultiAutocompletion

7
20
20+14
20+20
20+12
14
14+10
14+12
14+9

0.3690
0.3981
0.4178
0.3616
0.4187
0.4046
0.4178
0.3703
0.4190

0.5677
0.5825
0.5973
0.5541
0.5987
0.5882
0.5977
0.5584
0.5988

MRR

0.4569
0.4796
0.4975
0.4465
0.4986
0.4859
0.4977
0.4532
0.4988

Top1 Acc

Python
Top3 Acc

0.5416
0.5614
0.4951
0.5746
0.5759
0.5688
0.5321
0.5791
0.5804

0.7037
0.7260
0.6829
0.7274
0.7278
0.7302
0.7042
0.7297
0.7308

MRR

0.6142
0.6348
0.5780
0.6429
0.6439
0.6408
0.6086
0.6466
0.6478

vestigated next code token prediction and its application to
autocompletion. Earlier attempts at modeling software lan-
guages and completion ranking were based on n-gram lan-
guage models [4], [19], [20] or probabilistic models using
the information from ASTs [21]. With the advancement of
deep learning models, RNNs (and their variants) [7], [16] have
shown promising improvements. More recently, Transform-
ers have achieved state-of-the-art performance for software
language modeling [6], [22]. Galois (Radford et al., 2019)
and TabNine are two additional code completion tools that
employ the GPT-2 Transformer for next
token prediction.
Furthermore, some studies have focused on the industry use
case of autocompletion for IDE users within a company. While
[5] showed that a pointer mixture network performs well on
an open-source GitHub corpus as well as Google’s internal
Dart language corpus,
[8] warns that accuracy achieved on
synthetic benchmarks may not translate to real-world comple-
tion performance. Aye et al. [1] demonstrated how training on
real-world developer activity can combat this challenge.

Fig. 6: Ofﬂine top-1 accuracy evaluation starting at HackAll checkpoint vs
random initialization as size of PythonAll ﬁne-tuning samples increases

TABLE VI: Ofﬂine evaluations of GPT-2 and BART in each of 7 conﬁgs on Hack

Conﬁg

1
2
3
4
5
6
7

HackCommit
HackAll
HackAutocompletion
HackCommit → HackAll
HackCommit → HackAutocompletion
HackAll → HackAutocompletion
HackCommit→HackAll→HackAutocompletion

Top1 Acc

GPT-2
Top3 Acc

0.3479
0.4025
0.3632
0.4026
0.3961
0.4160
0.4145

0.5357
0.5858
0.5614
0.5867
0.5857
0.5970
0.5962

MRR

0.4306
0.4835
0.4508
0.4841
0.4801
0.4963
0.4953

Top1 Acc

BART
Top3 Acc

0.4280
0.4491
0.4454
0.4471
0.5106
0.5323
0.5323

0.6608
0.6863
0.6806
0.6820
0.7315
0.7497
0.7490

MRR

0.5312
0.5545
0.5498
0.5514
0.6096
0.6296
0.6293

Transfer Learning for Code. Transfer learning has revolu-
tionized natural language processing (NLP) since the sem-
inal papers on models such as GPT-2 [9], BERT [10] and
RoBERTa [23]. These works proposed the idea of pretraining
Transformer models on large, diverse text corpora in an unsu-
pervised manner and ﬁne-tuning them for speciﬁc downstream
tasks. Since then several works have been proposed [12], [13],
[24]–[30] to apply the idea of transfer learning in the domain
of code.

CodeBERT [12] was one of the ﬁrst models pretrained
on pairs of code and natural language sequences in order
learn a bimodal representation of both entities. It was applied
on natural
language code search and code documentation
generation to demonstrate the beneﬁt of transfer learning
compared to training directly on the downstream tasks. A
follow-up study [24] showed that CodeBERT can also transfer
knowledge to the problem of automated program repair. More
recently, BART [13] showed that the BART architecture [11]
is better-suited for source code generation tasks compared to
BERT-based models.

In comparison to these works, the goal of our paper is
not to produce a state-of-the-art model for any speciﬁc task.
While there is at least one effort [30] to create a multitask
benchmark of source code modeling tasks in the style of the
natural language decathlon [31], it is out of scope for us to
compare the efﬁcacy of our off-the-shelf models. Rather it is
to show how transfer learning across various dimensions can
beneﬁt IDE autocompletion prediction regardless of the model.
In our experiments we used two state-of-the-art models
GPT-2 and BART for this purpose, and showed that both
models can beneﬁt from transfer learning.

In a closely related work, Mastropaolo et al. [27] study
empirically how the T5 model [32] behaves when pretrained
and ﬁne-tuned on four code-related tasks. They also make
an observation that an unsupervised pretraining phase helps
the model achieve better performance on their set of tasks,
such as code summarization and mutation. In our paper, in
addition to observing this effect in the downstream task of IDE
autocompletion prediction, we also explored transfer learning
in other dimensions, such as across programming languages,
and validated our results through online A/B tests.

Outside of code authoring, transfer learning has been en-
abled by Transformer-based models in other software artifacts.
Lin et al. [25] apply pretrained BERT models for learning

relationships between issues and commits in a software repos-
itory. Sharma et al. [26] detect code smells in programming
languages where sufﬁcient training data is not available by
transferring knowledge from other data-rich languages. Pei
et al. [28] propose a tool called TREX that
is better at
detecting binary-level function similarity than state-of-the-art
tools owing to transfer learning from binary-level execution
traces.

V. THREATS TO VALIDITY

a) Intersection of vocabulary between languages:
In
our investigation of multilingual software language modeling,
one method we used to reduce the out-of-vocabulary (OOV)
problem was to tokenize code sequences using a bigram
encoding. Although this reduced the OOV rate for individual
languages, there was only a small overlap between the different
languages’ vocabularies. Applying BPE or another encoding
scheme may have resulted in more tokens receiving the same
encoding across languages which could increase the efﬁcacy
of transfer learning across programming languages.

b) Testing on more languages: We examined the effects
of transfer learning on two languages: Hack and Python.
While we saw that transfer learning (over various dimensions)
improves performance, we did not evaluate on other languages.
Hack and Python are both dynamic, object-oriented program-
ming languages. It’s conceivable that knowledge transfer to a
static language like C++ or a different paradigm language like
OCaml could be less effective.

c) Facebook vs open-source: When exploring pretraining
on code commits from version control, we leveraged Face-
book’s internal version control repository. It’s possible that
some of the transfer learning effects we observed when ﬁne-
tuning on IDE autocompletion would be lessened if we had
instead pretrained on GitHub code commits. In addition to the
greater source code diversity in the GitHub corpus, there is
undoubtedly concept drift between code in open-source and at
Facebook. There may even be different version control usage
patterns that would affect pretraining on code commits.

VI. FUTURE WORK

a) Stronger ablation study on PLBART: For both lan-
guages, PLBART outperformed the GPT-2 model by more
than 10%. However,
to make an apples-to-
is difﬁcult
apples comparison. A bidirectional model, PLBART leverages

it

context after the predicted token while GPT-2 only uses the
before-cursor context. PLBART also uses BPE instead of a
bigram + copy mechanism encoding. In the future we wish to
do a more thorough ablation study to determine the biggest
contributing factors to PLBART’s performance.

b) Transfer learning across multiple languages: In this
paper, we focused on transfer learning between two languages:
Hack and Python. However, there are many other languages
that are used in software engineering. Our experiments showed
that pretraining on one language transfers knowledge to the
other. Does the impact of transfer learning grow or diminish
as we add more languages to pretraining? Could pretraining
on multiple languages decrease even further the number of
ﬁne-tuning examples needed in the target language?

c) Transfer learning across source code tasks: The eval-
uation task for this paper was IDE autocompletion prediction.
Could we leverage our pretrained models for transfer learning
to other source code tasks such as as bug ﬁxing or similar code
detection? Furthermore, could the pretrained model for another
task be used effectively as the base model for autocompletion
as well?

VII. CONCLUSION

In this paper, we explored ways in which transfer learning
can improve autocompletion. Previous work showed that a
training corpus consisting of real-world examples collected
from programmers’ IDE usage leads to the highest accuracy
autocompletion models. But for some tool developers, there
may be a limited number of real-world autocompletion exam-
ples in the desired programming language. This study showed
how the power of transfer learning enables pretraining on non-
IDE, non-autoompletion, and different-language example code
sequences before ﬁne-tuning on the autocompletion prediction
task. Our results show that we can reach comparable accuracy
while drastically reduce the number of ﬁne-tuning examples
by starting from a pretrained model. These ﬁndings in ofﬂine
evaluation were conﬁrmed in online A/B experiments con-
ducted on thousands of software developers at Facebook.

REFERENCES

[1] G. A. Aye, S. Kim, and H. Li, “Learning autocompletion from real-world

datasets,” 2020.

[2] G. C. Murphy, M. Kersten, and L. Findlater, “How are java software
developers using the eclipse ide?” IEEE Softw., vol. 23, no. 4, pp. 76–83,
Jul. 2006. [Online]. Available: http://dx.doi.org/10.1109/MS.2006.105

[3] M. Bruch, M. Monperrus, and M. Mezini, “Learning from examples to
improve code completion systems,” in Proceedings of the the 7th Joint
Meeting of the European Software Engineering Conference and the ACM
SIGSOFT Symposium on The Foundations of Software Engineering,
ser. ESEC/FSE ’09. New York, NY, USA: ACM, 2009, pp. 213–222.
[Online]. Available: http://doi.acm.org/10.1145/1595696.1595728
[4] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu, “On the
the 34th International
Piscataway,
[Online]. Available:

naturalness of software,” in Proceedings of
Conference on Software Engineering, ser. ICSE ’12.
NJ, USA:
http://dl.acm.org/citation.cfm?id=2337223.2337322

IEEE Press, 2012, pp. 837–847.

[5] G. A. Aye and G. E. Kaiser, “Sequence model design for code comple-

tion in the modern ide,” 2020.

[6] S. Kim, J. Zhao, Y. Tian, and S. Chandra, “Code prediction by feeding

trees to transformers,” 2020.

[7] J. Li, Y. Wang, M. R. Lyu, and I. King, “Code completion with neural
attention and pointer networks,” Proceedings of
the Twenty-Seventh
International Joint Conference on Artiﬁcial Intelligence, Jul 2018.
[Online]. Available: http://dx.doi.org/10.24963/ijcai.2018/578

[8] V. J. Hellendoorn, S. Proksch, H. C. Gall, and A. Bacchelli, “When
code completion fails: A case study on real-world completions,”
ser. ICSE ’19.
IEEE Press, 2019, p. 960–970. [Online]. Available:
https://doi.org/10.1109/ICSE.2019.00101

[9] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,

“Language models are unsupervised multitask learners,” 2019.

[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805, 2018.

[11] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,
V. Stoyanov, and L. Zettlemoyer, “BART: Denoising sequence-to-
sequence pre-training for natural
translation,
and comprehension,” in Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics. Online: Association
for Computational Linguistics, Jul. 2020, pp. 7871–7880. [Online].
Available: https://www.aclweb.org/anthology/2020.acl-main.703
[12] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang, and M. Zhou, “Codebert: A pre-trained model for
programming and natural languages,” 2020.

language generation,

[13] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “Uniﬁed pre-
training for program understanding and generation,” in Proceedings of
the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics, 2021.

[14] M. Allamanis, “The adverse effects of code duplication in machine

learning models of code,” 2019.

[15] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher,
“Ctrl: A conditional transformer language model for controllable gen-
eration,” 2019.

[16] R. Karampatsis and C. Sutton, “Maybe deep neural networks are the
best choice for modeling source code,” CoRR, vol. abs/1903.05734,
2019. [Online]. Available: http://arxiv.org/abs/1903.05734

[17] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation
of rare words with subword units,” CoRR, vol. abs/1508.07909, 2015.
[Online]. Available: http://arxiv.org/abs/1508.07909

[18] N. Chirkova and S. Troshin, “A simple approach for handling
out-of-vocabulary identiﬁers in deep learning for source code,” CoRR,
vol. abs/2010.12663, 2020. [Online]. Available: https://arxiv.org/abs/
2010.12663

[19] T. T. Nguyen, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen, “A
statistical semantic language model for source code,” in Proceedings
of
the 2013 9th Joint Meeting on Foundations of Software
Engineering, ser. ESEC/FSE 2013. New York, NY, USA: Association
for Computing Machinery, 2013, p. 532–542. [Online]. Available:
https://doi.org/10.1145/2491411.2491458

[20] V. J. Hellendoorn and P. Devanbu, “Are deep neural networks the best
choice for modeling source code?” in Proceedings of the 2017 11th Joint
Meeting on Foundations of Software Engineering, ser. ESEC/FSE 2017.
New York, NY, USA: Association for Computing Machinery, 2017, p.
763–773. [Online]. Available: https://doi.org/10.1145/3106237.3106290
[21] P. Bielik, V. Raychev, and M. Vechev, “Phog: Probabilistic model for
code,” in Proceedings of the 33rd International Conference on Inter-
national Conference on Machine Learning - Volume 48, ser. ICML’16.
JMLR.org, 2016, p. 2933–2942.

[22] M. Brockschmidt, M. Allamanis, A. L. Gaunt, and O. Polozov, “Gen-
erative code modeling with graphs,” arXiv preprint arXiv:1805.08490,
2018.

[23] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert
pretraining approach,” 2019.

[24] E. Mashhadi and H. Hemmati, “Applying codebert for automated

program repair of java simple bugs,” 2021.

[25] J. Lin, Y. Liu, Q. Zeng, M. Jiang, and J. Cleland-Huang, “Traceabil-
ity transformed: Generating more accurate links with pre-trained bert
models,” 2021.

[26] T. Sharma, V. Efstathiou, P. Louridas, and D. Spinellis, “Code smell
detection by deep direct-learning and transfer-learning,” Journal of
Systems and Software, vol. 176, p. 110936, 2021. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0164121221000339

[27] A. Mastropaolo, S. Scalabrino, N. Cooper, D. N. Palacio, D. Poshy-
vanyk, R. Oliveto, and G. Bavota, “Studying the usage of text-to-text
transfer transformer to support code-related tasks,” 2021.

[28] K. Pei, Z. Xuan, J. Yang, S. Jana, and B. Ray, “Trex: Learning execution

semantics from micro-traces for binary similarity,” 2021.

[29] A. Elnaggar, W. Ding, L. Jones, T. Gibbs, T. Feher, C. Angerer,
S. Severini, F. Matthes, and B. Rost, “Codetrans: Towards cracking the
language of silicone’s code through self-supervised deep learning and
high performance computing,” 2021.

[30] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco,
C. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou,
L. Zhou, M. Tufano, M. Gong, M. Zhou, N. Duan, N. Sundaresan, S. K.
Deng, S. Fu, and S. Liu, “Codexglue: A machine learning benchmark
dataset for code understanding and generation,” 2021.

[31] B. McCann, N. S. Keskar, C. Xiong, and R. Socher, “The natural

language decathlon: Multitask learning as question answering,” 2018.

[32] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning
with a uniﬁed text-to-text transformer,” Journal of Machine Learning
Research, vol. 21, no. 140, pp. 1–67, 2020.
[Online]. Available:
http://jmlr.org/papers/v21/20-074.html

