2
2
0
2

g
u
A
9
2

]

G
L
.
s
c
[

2
v
2
4
4
5
0
.
6
0
2
2
:
v
i
X
r
a

A Dataset and Benchmark for Automatically
Answering and Generating Machine Learning
Final Exams

Sarah Zhang
EECS
MIT
sazhang@mit.edu

Reece Shuttleworth
EECS
MIT
rshuttle@mit.edu

Derek Austin
CS
Columbia University
da2986@columbia.edu

Yann Hicke
CS
Cornell University
ylh8@cornell.edu

Leonard Tang
Mathematics
Harvard University
leonardtang@college.harvard.edu

Sathwik Karnik
EECS
MIT
skarnik@mit.edu

Darnell Granberry
EECS
MIT
darnellg@mit.edu

Iddo Drori
EECS and CS
MIT and Columbia University
idrori@mit.edu,idrori@cs.columbia.edu

Abstract

Can a machine learn machine learning? We propose to answer this question
using the same criteria we use to answer a similar question: can a human learn
machine learning? We automatically answer MIT ﬁnal exams in Introduction to
Machine Learning at a human level. The course is a large undergraduate class
with around ﬁve hundred students each semester. Recently, program synthesis and
few-shot learning solved university-level problem set questions in mathematics
and STEM courses at a human level. In this work, we solve questions from ﬁnal
exams that differ from problem sets in several ways: the questions are longer,
have multiple parts, are more complicated, and span a broader set of topics. We
provide a new dataset and benchmark of questions from eight MIT Introduction
to Machine Learning ﬁnal exams between Fall 2017 and Spring 2022 and provide
code for automatically answering these questions and generating new questions.
We perform ablation studies comparing zero-shot learning with few-shot learning,
chain-of-thought prompting, GPT-3 pre-trained on text and Codex ﬁne-tuned on
code on a range of machine learning topics and ﬁnd that few-shot learning methods
perform best. We make our data and code publicly available for the machine
learning community.

1

Introduction

Can a machine learn machine learning? This work presents a new dataset of machine learning
ﬁnal exams with around 500 questions and a benchmark of baselines using transformers and their
respective grade performance, demonstrating that the best baseline performs at a human level.

In university-level STEM courses, students complete assignments (including problem sets and
labs) and exams throughout the course. Recent work has opened the door for a machine to solve
course problem sets [4] using language models and few-shot learning. However, ﬁnal exams remain

Preprint. Under review.

 
 
 
 
 
 
Figure 1: 500 MIT students taking the ﬁnal exam in Introduction to Machine Learning in Fall 2021
(no student faces or identifying information appears in the image to maintain anonymity). The exam
is three hours long and is taken in an indoor track-and-ﬁeld stadium accommodating many students.

challenging, and this work is the ﬁrst to present a structured dataset of machine learning ﬁnals and a
benchmark of baseline methods for answering them. Final exams differ from problem sets because
they serve as a benchmark of cumulative understanding of material learned over a semester and
evaluate the students’ depth and breadth of expertise. Further, questions on ﬁnal exams are longer,
have multiple parts, span a broader set of topics, and are more complicated and nuanced.

All the above holds for MIT’s Introduction to Machine Learning (Intro to ML), a rigorous undergrad-
uate course with around 500 students each semester, as shown in Figure 1, making it one of MIT’s
largest undergraduate courses offered.
Intro to ML is a core class in MIT’s computer science program. The course description1 is:

“Introduction to Machine Learning introduces principles, algorithms, and appli-
cations of machine learning from the point of view of modeling and prediction;
formulation of learning problems; representation, over-ﬁtting, generalization; clus-
tering, classiﬁcation, probabilistic modeling; and methods such as support vector
machines, hidden Markov models, and neural networks.”

The prerequisites for the course are Python Programming and Multivariate Calculus, with Introduction
to Algorithms and Linear Algebra recommended. The class typically consists of weekly exercises,
labs, quizzes, homework, a midterm, and a ﬁnal exam. There were no ﬁnal exams in Fall 2020 and
Spring 2020 due to COVID-19.

Intro to ML ﬁnal exams differ from problem sets in several ways, and the experience of solving each
varies. First, Intro to ML ﬁnals are long, containing around nine questions with seven parts. Final
exam questions are also multifaceted and multi-stepped: different parts of a single question require
applying different concepts and problem-solving skills, and parts may build upon each other. While
weekly problem sets focus on a single topic, ﬁnals span topics from the entire semester. Further,
Intro to ML ﬁnal questions are often story-based problems that may require mathematical modeling.
Due to the time constraint of these exams, ﬁnals are also designed to test core understanding and
application of course material over rote calculations. Thus, asking a machine to answer questions
from ﬁnals allows for testing whether the model is able to learn a breadth and depth of topics beyond
problem sets.

In this work, we present a new dataset curated from the eight most recent ﬁnal exams of MIT’s
Intro to ML course, totaling nearly 500 questions spanning twelve topics in machine learning:
regression, classiﬁers, logistic regression, features, loss functions, neural networks, convolutional
neural networks (CNNs), Markov decision processes (MDPs), recurrent neural networks (RNNs),
reinforcement learning, clustering, and decision trees. Our dataset covers the ﬁnals given for the

1MIT Registrars Ofﬁce Catalog for Course 6: Electrical Engineering and Computer Science, 6.3900 (6.036)

Introduction to Machine Learning http://student.mit.edu/catalog/m6c.html

2

semesters Fall 2017, Spring 2018, Fall 2018, Spring 2019, Fall 2019, Spring 2021, Fall 2021, and
Spring 2022. Due to the COVID-19 pandemic, no ﬁnals were in the course during 2020.

Table 1: Spring 2021, Question 10.a: question part, solution, Codex Zero-Shot answer, program output,
GPT-3 Zero-Shot answer, and GPT-3 Zero-Shot with CoT prompt answer.

Topic

Question Part

Classiﬁers
Given a set of data Dtrain = {(x(i), y(i))}, a weighted nearest neighbor
regressor has the form

h(x; θ) =

A typical choice for f is

(cid:80)

(x(i),y(i))∈Dtrain
(cid:80)

(x(i),y(i))∈Dtrain

f (x, x(i); θ)y(i)
f (x, x(i); θ)

.

f (x, x(cid:48); θ) = e−θ(cid:107)x−x(cid:48)(cid:107)2

where θ is a scalar and (cid:107)x − x(cid:48)(cid:107)2 = (cid:80)d
j)2. Assume our
training data Dtrain = ((1, 1), (2, 2), (3, 6)). What is h(10; 0)? That is,
letting θ = 0, what is our prediction for x = 10?

j=1(xj − x(cid:48)

Solution

3

Codex Zero-Shot Answer

import numpy as np

def f(x, x_i, theta):

return np.exp(-theta * np.linalg.norm(x - x_i)**2)

def h(x, theta):

D = np.array([(1, 1), (2, 2), (3, 6)])
numerator = 0
denominator = 0
for x_i, y_i in D:

numerator += f(x, x_i, theta) * y_i
denominator += f(x, x_i, theta)

return numerator / denominator

print(h(10, 0))

Program Output

GPT-3 Zero-Shot Answer

GPT-3 ZS with CoT Answer

f (10,1,0)+f (10,2,0)+f (10,3,0)

3.0
h(10; 0) = f (10,1,0)y(1)+f (10,2,0)y(2)+f (10,3,0)y(3)
9
3 = 3
f (10, (1, 1), 0) = e−0(cid:107)10−(1,1)(cid:107)2
f (10, (2, 2), 0) = e−0(cid:107)10−(2,2)(cid:107)2
f (10, (3, 6), 0) = e−0(cid:107)10−(3,6)(cid:107)2

= e−0(cid:107)9(cid:107)2
= e−0(cid:107)8(cid:107)2
= e−0(cid:107)7(cid:107)2

= 1·1+1·2+1·6

1+1+1

=

= e−0·81 = 1
= e−0·64 = 1
= e−0·49 = 1
1+1+1 = 9

h(10, 0) =

(cid:80)

(x(i),y(i))∈Dtrain
(cid:80)

(x(i) ,y(i))∈Dtrain

f (x,x(i),θ)y(i)
f (x,x(i),θ) = 1·1+1·2+1·6

3 = 3

We verify that our results are not due to overﬁtting because we include ﬁnals from 2021 and 2022
that are unavailable online. Also, the Spring 2022 ﬁnal exam was given after GPT-3 and Codex were
last updated, which means that the models were not trained on this data. The ﬁnal exam questions
have many parts, each posing a new problem, and each question in the dataset corresponds to one
part. The questions in the ﬁnals are varied in topics and solution types. Solutions are primarily
open-ended questions with some true/false and multiple-choice questions on theory, math, and code
implementations.

3

Grading:
Answers:
Questions:

Spring 2021
Fall 2021
Spring 2022

Mean

Human
Human
All

75.84
74.38
69.07

73.10

Human
Human

Human
Machine

Machine
Machine

Non-Image Non-Image Non-Image Non-Open

80.77
60.88
70.82

70.82

62.09
58.94
68.86

63.29

64
51.33
73.53

62.95

Table 2: Human and machine grading of human and machine solved ﬁnal exams. Mean human and
machine grades on Introduction to Machine Learning ﬁnal exams by semester. Spring 2021, Fall
2021, and Spring 2022 ﬁnal exams were unavailable online when GPT-3 and Codex were trained,
ensuring that our results are not due to overﬁtting. Non-image grades consider question parts that do
not contain images that are required for solving the question.

We make the dataset publicly available and welcome others to use it to aid in developing and assessing
new language models and methods. Due to the diversity of Intro to ML ﬁnal questions, our dataset
uniquely assesses advanced problem-solving and reasoning skills in machine learning, math, and
natural language processing. This dataset opens the door to achieving breakthroughs in machine
learning performance in machine learning ﬁnal exams.

In addition to the dataset, we present a benchmark using several baseline methods. We apply zero-shot
and few-shot learning to GPT-3 and Codex, adding chain-of-thought prompting for GPT-3. We ﬁnd
that few-shot learning methods perform best. As shown in Table 2 the best performing methods
pass the ﬁnal exams, and their grade is comparable with human grades of MIT students on the same
machine learning ﬁnals evaluated by the same human graders.

Following our previous work [4] that showed that questions generated by these models are indistin-
guishable from human-written questions, we also generate new ﬁnal exam questions.

1.1 Related Work

In the context of learning, humans are often considered to be generalists, whereas machines are
specialists. However, large language models based on transformers such as GPT-3 [1], Gopher [9],
and PaLM [3], also called foundation models, are generalist learners. Speciﬁcally, in our setting,
while humans care about the number of topics in an exam and therefore ﬁnd ﬁnals more difﬁcult
than problem sets, foundation models effortlessly scale to many topics without re-training. Language
models may be pre-trained on text and ﬁne-tuned on speciﬁc datasets such as code, for example
OpenAI’s Codex [2], which allows generating programs from text.

There are several ways to improve the mathematical reasoning ability of language models: (1) using
chain-of-thought (CoT) prompting [6, 14], (2) using the top-k ranking solutions [7] and merging
them by voting [13] or least-to-most prompting [15], and (3) using program synthesis and few-shot
learning to generate code that answers questions [4].

Much of the prior work focuses on high school or middle school level material [8]. The ﬁrst work to
tackle university-level machine learning course problem set questions [12] used a transformer and
GNN architecture and heavily relied on data augmentation. This resulted in overﬁtting and did not
scale up to other types of questions or courses. Probability and statistics course problem-set questions
have been answered [11] by probabilistic program synthesis with human performance. Problem-set
questions from the core university math courses [4] have been automatically solved using few-shot
learning and program synthesis at a human level. Other work considers university-level course
questions across a variety of domains [5] and identifying theorems [10]. Prior work on question
generation includes question–answer pair generation based on a text passage [8] and question text
generation based on other questions [4].

2 Dataset

We present a new dataset of 496 questions from the eight most recent ﬁnal exams of MIT’s In-
troduction to Machine Learning course. The dataset spans questions on the 12 machine learning

4

Semester

Questions

Parts

Topic

Questions

Parts

Fall 2017
Spring 2018
Fall 2018
Spring 2019
Fall 2019
Spring 2021
Fall 2021
Spring 2022

Mean
Total

10
9
9
9
8
12
8
9

9.25
74

61
42
59
58
61
70
86
59

62
496

Regression
Classiﬁers
Logistic Regression
Features
Neural Networks
Loss Functions
CNNs
MDPs
RNNs
Reinforcement Learning
Clustering
Decision Trees

Mean
Total

6
10
1
3.5
12.5
2
8
9
7
8
1
6

6.73
74

58
60
8
21
76
14
63
75
33
50
8
32

45.09
496

Table 3-4: The number of questions and parts in the ﬁnal for each semester and topic of Introduction
to Machine Learning. Spring 2020 and Fall 2020 did not have ﬁnal exams due to COVID-19. Topics
can have half-questions attributed to them if a question has some parts under one topic and the other
parts under another topic.

topics covered in the course: (1) regression, (2) classiﬁers, (3) logistic regression, (4) features, (5)
loss functions, (6) neural networks, (7) convolutional neural networks (CNNs), (8) Markov decision
processes (MDPs), (9) recurrent neural networks (RNNs), (10) reinforcement learning, (11) clustering,
and (12) decision trees. We make our data and code publicly available.2

The breakdown of questions, parts, points, and non-image points by each semester and topic are
shown in Table 2. Each question in a ﬁnal exam consists of multiple parts. Questions are written
by providing set-up and context information ﬁrst, followed by the question parts (which may come
with additional information). Set-up and context information may contain (1) story elements (ex.,
character names, and motivations), (2) relevant deﬁnitions and equations, and (3) data points. We
format questions in the dataset by concatenating the question context, any context or solutions from
prior parts of the question required for answering the part, and the part’s context and question. We
split the questions into their corresponding parts. Questions consist of English text, mathematical
notation, and images. Mathematical notation is represented in the dataset by LaTeX and images by
screenshots from pdfs ﬁles. The types of question answers are diverse. A few are multiple-choice or
true/false questions. Most are open-ended, for which the evaluation requires modeling the problem,
mathematical manipulation, or code writing. Many questions require providing an explanation.

We used eight ﬁnal exams from different semesters for data curation. We had access to the Latex
version for the three most recent semesters of Spring 2021, Fall 2021, and Spring 2022, and therefore
did not require transcription. For the ﬁve remaining exams, Fall 2017, Spring 2018, Fall 2018, Spring
2019, Fall 2019, we had access to the pdf versions. In these cases, we used mathpix mathpix.com
for an initial transcription, and curators then evaluated and manually corrected the input questions
and veriﬁed the correctness of each input question.

We extract questions and solutions for all parts of all types of questions, including those that rely
on images. We curated the ﬁve exams between 2017 and 2019 from publicly available pdf ﬁles.
Spring 2020 and Fall 2020 do not have ﬁnal exams due to COVID-19. The three exams between
2021 and 2022 were unavailable online; therefore, the model does not overﬁt their solutions. The
aggregate average grades were available to the students and did not contain any personally identiﬁable
information. Three duplicate questions were originally on the ﬁnal exam of Fall 2017 (questions 1, 3,
6) and appeared again in the ﬁnal exam of Spring 2022.

2Data and code: https://github.com/idrori/mlﬁnalsQ

5

3 Benchmark

3.1 Baselines

We provide a benchmark by comparing six baselines for answering the ﬁnal exam questions: (1)
GPT-3 with zero-shot learning, (2) GPT-3 with few-shot learning, (3) GPT-3 with zero-shot learning
and chain-of-thought (CoT) prompting, (4) GPT-3 with few-shot learning and chain-of-thought (CoT)
prompting, (5) Codex with zero-shot learning, and (6) Codex with few-shot learning.

Table 4 shows the prompt used for each approach. GPT-3 zero-shot uses the question as-is, whereas
GPT-3 zero-shot with CoT uses the sufﬁx “Let’s think step by step.” after the question to encourage
multi-step output. Codex zero-shot uses the preﬁx “Write a program that answers” before the question
within Python comments denoted by triple quotes """ to encourage Codex to write code. GPT-3
few-shot ﬁnds the closest questions in the embedding space, measured by cosine similarity, and uses
them and their corresponding answers before the new question as examples in the prompt. Codex
few-shot ﬁnds the closest questions in the embedding space also as measured by cosine similarity
and uses these questions and their corresponding code as examples.

For students, a good study technique is to use previous ﬁnal exams to review and practice for their
upcoming ﬁnal. We model this method by few-shot learning using the question–answer pairs (for
GPT-3) or question–code (for Codex) with the closest question embeddings from previous ﬁnals. We
implement this by considering all the exam questions, marking each question by its semester and year,
and using only previous semesters’ questions for few-shot learning. The Fall 2017 and Spring 2022
exams contain three duplicate questions, and we handle these same questions the same way humans
do by allowing few-shot learning in Spring 2022 based on successful Fall 2017 zero-shot answers. It
is reasonable that if a student studies all previous exams, there may be 8.5% of repeated question
points. Since Fall 2017 is the ﬁrst ﬁnal exam, we do not perform few-shot learning on this exam.

3.1.1 Comparison with Open Language Models

We also evaluated our dataset on an open-source language model, Meta’s OPT-175B. OPT-175B
is a model consisting of 175 billion parameters. Our dataset consists of ﬁnal exam questions from
machine learning courses and ﬁt to be used by OPT-175B. Tables 5 and 6 compare the results of
OpenAI GPT-3, OpenAI Codex, and Meta OPT. We evaluated OPT on only 163 question parts, since
OPT was limited to handling questions under 256 characters in length. We implement the inference
for the OPT-175B model using Alpa. Alpa is a particular framework designed for training and
inference of large models. For the hardware, we use an 8x A100 PCIE cluster. The model requires
about 560 GB of VRAM in our run case and each example takes nine minutes for inference.

3.2 Grading

3.2.1 Human Grading

The questions are of different types: multiple-choice, numerical, expressions, and open-text. We
grade answers and aim to keep all factors equal in grading human and machine answers. Human and
machine answers are graded based on the number of points allocated to each question part, giving
full, partial, or no credit for each answer. We approximate partial credit by assigning half-credit.
The course staff graded student ﬁnal exams, which included graduate TAs and instructors. Two of
the same graduate TAs and the instructor that graded the student answers also graded the machine
answers. Grading instructions are the same for grading student answers as grading machine answers.

3.2.2 Automatic Grading

We label each question’s answer type into one or two categories out of four options - multiple choice
(MC), numerical, expression, or open. We consider answers multiple choice if the test-taker is
presented with an enumerated list of choices, numerical if the answer is a number, expression if
the answer includes variables or other notation, and open if the answer calls for free-response text.
We categorize questions that have additional questions nested within them by the multiple relevant
categories. Most often, this is the case when a question with one of MC, numerical, or expression, is
followed by a follow-up question asking the student to explain their previous answer. The breakdown
of the 479 questions is: 76 are multiple-choice, 100 numerical, 89 expressions, and 214 are open. The

6

Method

GPT-3 Zero-Shot

GPT-3 Few-Shot

Prompt

<question>

Q: <similar question>
A: <similar question’s answer>
Q: <question>
A:

GPT-3 Zero-Shot with CoT Q: <question>

GPT-3 Few-Shot with CoT

Codex Zero-Shot

Codex Few-Shot

A: Let’s think step by step.

Q: <similar question>
A: <similar question’s answer>
Q: <question>
A: Let’s think step by step.

"""
Write a program that answers the following question: <question>
"""

"""
Write a program that answers the following question: <similar question>
"""
<similar question’s correct code>
"""
Write a program that answers the following question: <question>
"""

Table 4: Input prompt for each of six baseline methods (1) GPT-3 Zero-Shot, (2) GPT-3 Few-Shot,
(3) GPT-3 Zero-Shot with CoT, (4) GPT-3 Few-Shot with CoT, (5) Codex Zero-Shot, and (6) Codex
Few-Shot. Similar questions (as measured by cosine similarity) are drawn from previous ﬁnals only.

’Non-Open Points’ column of Tables 7 and 8 show the answer type breakdown by number of points.
Table 7 shows the number of question parts that do not rely on images, the number of points that do
not rely on images, and the number of non-open question points in Introduction to Machine Learning
ﬁnals for each semester. Spring 2020 and Fall 2020 did not have ﬁnal exams due to COVID-19. Table
8 shows the breakdown by topic. Our automatic grading uses string matching and regular expressions.
In the case of multiple-choice results, we check that the output of the code is equal to the solution. In
the case of numerical answers, we look for a matching integer or real number.

3.3 Performance

Table 5 shows the machine grades by semester and Table 6 shows the machine grades by topic,
excluding question parts that rely on images. We compare the average grade of GPT-3 with zero-shot
(ZS), GPT-3 with ZS and chain-of-thought (CoT) prompting, GPT-3 with few-shot (FS) learning,
GPT-3 with FS and CoT prompting, Codex with ZS, Codex with FS, and OPT with ZS. Fall 2017
is the ﬁrst semester, so few-shot learning results based on previous semesters are unavailable (NA).
Spring 2020 and Fall 2020 did not have ﬁnal exams due to COVID-19. Spring 2021, Fall 2021, and
Spring 2022 ﬁnal exams were unavailable online when GPT-3 and Codex were trained, ensuring that
the model is not overﬁtting content it has seen previously. The results consistently demonstrate that
few-shot learning methods perform best across semesters and topics, as marked in bold.

3.4 Limitations

Our dataset consists of all question parts and their solutions, including images. However, our baseline
methods do not handle questions that rely on an image containing the information required to solve
the question since GPT-3 and Codex do not handle images. Tables 7 and 8 show the breakdown of the
number of question parts and points of questions that do not rely on image information for answering
the question. On average, 28.42% of the question parts, which make up 35.63% of the points in ﬁnal
exams, are questions that rely on image information. The points attributed to the non-image parts are
tallied, recorded, and used to calculate non-image percentage grades.

7

Semester

GPT-3 ZS GPT-3 FS GPT-3 ZS CoT GPT-3 FS CoT Codex ZS Codex FS OPT ZS

Fall 2017
Spring 2018
Fall 2018
Spring 2019
Fall 2019
Spring 2021
Fall 2021
Spring 2022

38.21
44.35
51.99
43.45
53.17
44.33
58.94
42.78

NA
60.48
52.18
54.23
62.70
55.81
58.94
68.86

22.86
38.71
61.63
41.07
28.97
53.45
50.35
32.03

NA
70.97
64.17
58.81
56.35
60.21
54.90
53.48

21.43
32.26
49.78
15.54
25.40
33.62
18.11
51.01

NA
67.74
54.00
41.55
59.13
62.09
42.00
65.46

NA
33.33
47.54
15.32
44.44
11.11
15.32
60.71

Table 5: We benchmark different baselines for each semester, excluding question parts that rely on
images. We compare the average grade of GPT-3 with zero-shot (ZS), GPT-3 with few-shot (FS)
learning, GPT-3 with ZS, and chain-of-thought (CoT) prompting, GPT-3 with FS and CoT prompting,
Codex with ZS, Codex with FS and OPT with ZS. Fall 2017 is the ﬁrst semester, so few-shot learning
results based on previous semesters are unavailable (NA). Spring 2020 and Fall 2020 did not have
ﬁnal exams due to COVID-19. Spring 2021, Fall 2021, and Spring 2022 ﬁnal exams were unavailable
online when GPT-3 and Codex were trained, ensuring that the model is not overﬁtting. The result of
the best-performing method for each semester is marked in bold.

Topic

GPT-3 ZS GPT-3 FS GPT-3 ZS CoT GPT-3 FS CoT Codex ZS Codex FS OPT ZS

Regression
Classiﬁers
Logistic Reg.
Features
Loss Functions
Neural Networks
CNNs
MDPs
RNNs
RL
Clustering
Decision Trees

31.71
38.18
50.00
58.65
NA
48.34
37.50
49.19
61.46
36.09
100.00
54.70

50.00
46.21
60.00
75.96
NA
60.23
53.58
52.01
71.88
42.99
100.00
71.80

25.61
26.28
77.50
53.85
NA
44.54
28.36
46.03
57.29
36.67
100.00
32.48

40.85
42.35
77.50
77.31
NA
68.42
47.81
54.23
66.14
50.11
100.00
51.28

40.24
18.88
55.00
68.85
NA
37.82
13.38
24.38
12.50
28.79
50.00
46.15

50.00
53.74
70.00
81.54
NA
63.45
36.77
38.03
40.63
45.11
50.00
54.70

50.00
50.00
16.67
10.00
NA
27.27
23.83
4.27
39.28
24.28
7.14
55.00

Table 6: We benchmark different baselines for each course topic, excluding question parts that rely
on images. We compare the grade of GPT-3 with zero-shot (ZS), GPT-3 with few-shot (FS) learning,
GPT-3 with zero-shot and chain-of-thought (CoT) prompting, GPT-3 with FS and CoT, Codex with
zero-shot, Codex with few-shot learning, and OPT ZS. The question parts on loss functions rely on
image information and are therefore unavailable (marked NA). The result of the best-performing
method for each semester is marked in bold.

Semester

Non-Image Parts / All Non-Image Points / 100 Non-Open Points

Fall 2017
Spring 2018
Fall 2018
Spring 2019
Fall 2019
Spring 2021
Fall 2021
Spring 2022

OPT Total

Total

49 / 61
27 / 42
29 / 59
41 / 58
47 / 61
50 / 70
56 / 86
46 / 59

163 / 496

345 / 496

70
62
62
70
63
61
48
69

66
52
45
44
66
66
36
48

246 / 800

505 / 800

133 / 505

423 / 505

Table 7: The number of question parts that do not rely on images, the number of points that do not
rely on images, and the number of non-open question points, in Introduction to Machine Learning
ﬁnals for each semester. Spring 2020 and Fall 2020 did not have ﬁnal exams due to COVID-19.

8

Topic

Non-Image Parts / All Non-Image Points / All Non-Open Points

Regression
Classiﬁers
Logistic Regression
Features
Loss Functions
Neural Networks
CNNs
MDPs
RNNs
Reinforcement Learning
Clustering
Decision Trees

33 / 58
52 / 60
8 / 8
18 / 21
0 / 14
56 / 76
52 / 63
38 / 72
27 / 33
42 / 50
1 / 8
18 / 32

Total

345 / 496

41 / 67
101 / 123
10 / 10
26 / 38
0 / 18
87 / 128
57 / 76
39 / 112
48 / 67
55 / 79
2 / 12
39 / 72

505 / 800

36
71
3
15
10
66
44
72
35
37
5
29

423 / 505

Table 8: The number of questions parts that do not rely on images, number of points that do not rely
on images, and number of non-open question points in Introduction to Machine Learning ﬁnals for
each topic of the course.

3.5 Generating New Questions

The creation of new, high-quality questions by course instructors and TA is often a time-consuming,
high-effort process. These new questions must be varied from past questions while still testing the
same core concepts. We explore the potential of using GPT-3 to write exam content efﬁciently by
using the dataset of exam questions to generate new questions automatically. We use questions from
our dataset as prompts to create new high-quality questions not present in our dataset. We create a list
of various questions in our curated dataset and use the resulting list to prompt GPT-3 to create a new
question. The supplementary material demonstrates the results of this process for each topic in the
course. The Appendix consists of new generated questions and the closest question from our dataset
as measured by the cosine similarity of the embedding of each question. These new questions are
diverse and qualitatively similar to questions on previous MIT ﬁnal exams. This provides an efﬁcient
way for course TAs and instructors to generate new ﬁnal questions.

3.5.1 Student Survey

To evaluate the machine-generated questions, we conducted an anonymous online student survey
comparing them with the human-written questions in terms of quality, appropriateness relative to
the course, and question difﬁculty. We surveyed 15 students who have taken the Introduction to
Machine Learning course or its equivalent. The survey was optional and included informed consent,
with the following description: “We are conducting a survey to assess the quality and difﬁculty of
automatically generated questions for an introductory machine learning course ﬁnal. You will be
presented with a series of questions, either human-written (taken from an actual course ﬁnal exam) or
machine generated, but you will not be told the source of a given question. For each question, you
will be asked (a) whether you think the question is human-written or machine-generated, (b) whether
the question is appropriate for the given course ﬁnal, and ﬁnally (c) how you would rate the difﬁculty
of the question. Please carefully read each question and answer to the best of your ability”.

We randomly sampled one generated question and its closest (measured by cosine similarity) original,
human-written question for each of the twelve machine learning topics. Students were asked to read
these 24 questions in the survey, mixed and presented randomly, and then answer three questions for
each: (1) “Is the question human-written or machine-generated?”, (2) “Is the question appropriate or
not appropriate for the speciﬁc course ﬁnal?”, and (3) “What is the question’s difﬁculty level on a
scale between 1 (easiest) and 5 (hardest)?”. We ask the students to provide ratings and not to solve
the questions.

The results of our survey are as follows: Out of the human-written questions, students identiﬁed
56.11% of them correctly as human-written and 43.89% incorrectly as machine-generated. Of the
machine-generated questions, students identiﬁed 45% of them correctly as machine-generated and
55% of them incorrectly as human-written. The difﬁculty ratings were between 1 (the easiest) and
5 (the hardest). Students rated machine-generated questions with a difﬁculty level of 2.55 with a

9

1.11 standard deviation and rated human-written questions with a difﬁculty level of 2.85 with a 1.12
standard deviation. Students rated machine-generated questions as appropriate 82.6% of the time and
human-written questions as appropriate 85.0% of the time.

The conclusions we draw from the survey are that (1) survey participants considered human-written
questions to be as likely to be human-written or machine-generated, and similarly, machine-generated
questions were considered equally likely to be machine-generated as human-written, (2) survey
participants considered the machine-generated questions slightly easier than human-written questions,
and (3) survey participants considered machine-generated questions as appropriate as human-written
questions. Based on these results, we conclude that across multiple aspects, the machine-generated
questions are highly similar to human-generated questions and can be adapted to generate questions
for machine learning courses.

3.6

Implementation Details

We use the latest OpenAI GPT-3 and Codex models and do not re-train these very large language
models. We ﬁx all the hyperparameters of the models so that the answers are deterministic and
reproducible. Speciﬁcally, we set both top P, which controls diversity, and sampling temperature,
which controls randomness, to 0. The frequency and presence penalties are also set to 0, and we do
not halt on any stop sequences. We allow diversity for generating new questions by setting the top
P and temperature to 0.1. We run Codex with an upper bound of generating programs with 1024
tokens. We use the OpenAI text-davinci-002 and code-davinci-002 engines for generating text and
programs. For few-shot-learning and question generation, we use the text-similarity-babbage-001
engine to embed the questions and ﬁnd the closest questions in the dataset by cosine similarity. The
running time for answering or generating each question part is a few seconds.

4 Conclusions

We present a dataset and benchmark for answering and generating university-level ﬁnal exams in
machine learning. The dataset is the ﬁrst of its kind, and the benchmark compares state-of-the-art
language model approaches. Machine performance and human performance are evaluated by the
same graders and grading instructions. A comparison of baselines shows that few-shot learning
methods perform best across semesters and topics. A limitation of our work is that our benchmark
does not consider questions that rely on images for their solution. Potential negative societal impacts
of the work may be applications that are used for helping answer ﬁnals during the exams. Potential
positive societal impacts include improving students learning for ﬁnal exams, helping course staff
generate questions for ﬁnals, and comparing levels of difﬁculty of exams across semesters and
schools. We hope this dataset and benchmark serve the machine learning community and advance the
state-of-the-art in the ﬁeld. Future work will extend the benchmark to handle questions that rely on
images and extend the dataset to ﬁnals exams in many STEM courses and universities.

10

References

[1] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models
are few-shot learners. In Proceedings of Advances in Neural Information Processing Systems
(NeurIPS), volume 33, pages 1877–1901, 2020.

[2] Mark Chen et al. Evaluating large language models trained on code.

arXiv preprint

arXiv:2107.03374, 2021.

[3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[4] Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin
Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L. Patti,
Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, and Gilbert Strang. A neural network
solves, explains, and generates university math problems by program synthesis and few-shot
learning at human level. Proceedings of the National Academy of Sciences, 119(32), 2022.
[5] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding. In Proceedings of the
International Conference on Learning Representations (ICLR), 2021.

[6] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large

language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.

[7] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond,
Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code
generation with alphacode. arXiv preprint arXiv:2203.07814, 2022.

[8] Fanyi Qu, Xin Jia, and Yunfang Wu. Asking questions like educational experts: Automatically
generating question-answer pairs on real-world examination data. In Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana,
Dominican Republic, November 2021. Association for Computational Linguistics.

[9] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,
John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language
models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446,
2021.

[10] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,
Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.
Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
arXiv preprint arXiv:2206.04615, 2022.

[11] Leonard Tang, Elizabeth Ke, Nikhil Singh, Bo Feng, Derek Austin, Nakul Verma, and Iddo
Drori. Solving probability and statistics problems by probabilistic program synthesis at human
level and predicting solvability. In Proceedings of the International Conference on Artiﬁcial
Intelligence in Education (AIED), 2022.

[12] Sunny Tran, Pranav Krishna, Ishan Pakuwal, Prabhakar Kaﬂe, Nikhil Singh, Jayson Lynch, and
Iddo Drori. Solving machine learning problems. In Proceedings of the Asian Conference on
Machine Learning (ACML), pages 470–485, 2021.

[13] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-
arXiv preprint

consistency improves chain of thought reasoning in language models.
arXiv:2203.11171, 2022.

[14] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903, 2022.

[15] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale
Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables
complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.

11

Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on
how to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or
[N/A] . You are strongly encouraged to include a justiﬁcation to your answer, either by referencing
the appropriate section of your paper or providing a brief inline description. For example:

• Did you include the license to the code and datasets? [Yes]

• Did you include the license to the code and datasets? [No] The code and the data are

proprietary.

• Did you include the license to the code and datasets? [N/A]

Please do not modify the questions and only use the provided macros for your answers. Note that the
Checklist section does not count towards the page limit. In your paper, please delete this instructions
block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s

contributions and scope? [Yes]

(b) Did you describe the limitations of your work? [Yes] See Section 3.4.
(c) Did you discuss any potential negative societal impacts of your work? [Yes] See

Section 4.

(d) Have you read the ethics review guidelines and ensured that your paper conforms to

them? [Yes]

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]

3. If you ran experiments (e.g. for benchmarks)...

(a) Did you include the code, data, and instructions needed to reproduce the main ex-
perimental results (either in the supplemental material or as a URL)? [Yes] See our
repository

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they

were chosen)? [Yes] See Section 3.6.

(c) Did you report error bars (e.g., with respect to the random seed after running experi-

ments multiple times)? [N/A]

(d) Did you include the total amount of compute and the type of resources used (e.g., type

of GPUs, internal cluster, or cloud provider)? [Yes] See Section 3.6.

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...

(a) If your work uses existing assets, did you cite the creators? [Yes]
(b) Did you mention the license of the assets? [Yes]
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]

See our repository

(d) Did you discuss whether and how consent was obtained from people whose data you’re

using/curating? [Yes] See Section 2.

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable

information or offensive content? [Yes] See Section 2.

5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [N/A]

(b) Did you describe any potential participant risks, with links to Institutional Review

Board (IRB) approvals, if applicable? [N/A]

(c) Did you include the estimated hourly wage paid to participants and the total amount

spent on participant compensation? [N/A]

12

A Appendix

1. Submission introducing new datasets must include the following in the supplementary

materials:
(a) Documentation and intended uses. Documentation can be found in the README
ﬁles of the GitHub repository https://github.com/idrori/mlﬁnalsQ. The authors intend
for the dataset to be used by the machine learning and broader research community to
improve

(b) URL to dataset.

The full dataset can be accessed and downloaded at

https://github.com/idrori/mlﬁnalsQ.

(c) Author statement. The authors bear all responsibility in case of violation of rights,

etc., and conﬁrm the data license.

(d) Hosting, licensing, and maintenance plan. The data and code is hosted and main-

tained on GitHub under an MIT license.

2. To ensure accessibility, the supplementary materials for datasets must include the following:
(a) Links to access the dataset and its metadata. The dataset and its metadata is acces-

sible at https://github.com/idrori/mlﬁnalsQ.

(b) Reading the dataset. Questions in the dataset are presented in json ﬁle format, with

the following ﬁelds:

Field

Description

Semester
Question Number
Part Number
Points
Topic

Type
Question
Solution

The semester the question’s ﬁnal was given in (ex. Fall 2017)
The number of the question from the ﬁnal (ex. 1, 2...)
The question part label the question has (ex. a, b.i)
The number of points the question is worth
The primary machine learning topic that the question targets
Topics are regression, classiﬁers, logistic regression,
features, loss functions, neural networks, CNNs, MDPs, RNNs,
reinforcement learning, clustering, and decision trees
Text if the question only relies on text, Image if the question relies on an image
The original question text as presented from the source
The solution to the question

Table 9: Dataset json ﬁelds and their descriptions.

The dataset is also available to download as a CSV ﬁle with the ﬁelds described in 9 as
column headers.

(c) Long-term preservation. We ensure the longevity of this dataset by keeping it publicly

available in a GitHub repository.

(d) Explicit license. The code and data is licensed under the MIT license in the repository.
(e) Structured Metadata. https://github.com/idrori/mlfinalsQ/blob/main/

data/schema_dataset.json

(f) Dataset Identiﬁcation. Our data and code are maintained in a GitHub repository,

allowing for easy access, Our dataset thus does not have a DOI.

3. Reproducibility. Data, code, and evaluation procedures for reproducing the benchmark
results in this paper are available at https://github.com/idrori/mlﬁnalsQ. The code provided
allows Note that grading machine outputs were done manually, so none of the provided code
will produce those.

13

B Appendix

Table 10: Generating new questions: example of a new question for each topic automatically generated and the closest
question in the dataset based on cosine similarity of the questions embeddings.

Topic

Regression

Similarity

0.945

Question
Generated Question: “We’re given a data set D = (cid:8)(cid:0)x(i), y(i)(cid:1)(cid:9)n
i=1,
where x(i) ∈ Rd and y(i) ∈ R. Let X be a d × n matrix in which the
x(i) are the columns and let Y be a 1 × n vector containing the values
of y(i). Using the ordinary least-squares formula, we can compute
Wols = (cid:0)XX T (cid:1)−1
XY T Using ridge regression, we can compute
Wridge = (cid:0)XX T + λI(cid:1)−1
XY T We decide to try to use these methods
to initialize a single-unit neural network with a linear activation function.
Assume that XX T is neither singular nor equal to the identity matrix,
and that neither Wols nor Wridge is equal to (0, 0, . . . , 0). Consider a
neuron initialized with Wridge . Provide an objective function J(W ) that
depends on the data, such that batch gradient descent to minimize J will
have no effect on the weights, or argue that one does not exist."

Closest Question: “We’re given a data set D = (cid:8)(cid:0)x(i), y(i)(cid:1)(cid:9)n
i=1, where
x(i) ∈ Rd and y(i) ∈ R. Let X be a d × n matrix in which the x(i) are
the columns and let Y be a 1 × n vector containing the values of y(i).
Using the analytical regression (ordinary least-squares) formula, we can
compute

Wols = (cid:0)XX T (cid:1)−1
can
we

XY T

ridge

compute Wridge

=
regression,
Using
(cid:0)XX T + λI(cid:1)−1
XY T We decide to try to use these methods to
initialize a single-unit "neural network" with a linear activation function
and no offset: h(x; W ) = W T x. Assume that XX T is invertible and
not equal to the identity matrix, and that neither Wols nor Wridge is equal
to (0, 0, . . . , 0). Note also that we are not using an explicit offset/bias
term. Rory has solved many problems from this particular domain
before and the solution has typically been close to W ∗ = (1, . . . , 1)T .
Deﬁne an objective function J(W ) that we could minimize in order to
obtain good estimates for Rory’s next problem, even with very little
data."

Classiﬁers

Generated Question: “Consider a binary classiﬁcation problem with two
classes, +1 and -1. Assume that the training data is linearly separable.
Consider the following two models:
Model 1: g(x) = sgn(wT x)
Model 2: g(x) = sgn(wT x + b)
Assume that the training data is linearly separable. Which model is more
likely to overﬁt?

0.782

Closest Question: “In some cases, we will have a validation set
in addition to training and test sets. Assume the validation set is
approximately the same size as the test set. This validation set is often
used to tune hyperparameters such as λ. Imagine we have trained a
classiﬁer using regularization, with λ chosen based on performance on
the training set. Which will have the highest accuracy the training set,
the validation set or the test set?"

14

Table 10: Generating new questions: example of a new question for each topic automatically generated and the closest
question in the dataset based on cosine similarity of the questions embeddings.

Topic
Logistic Regression

Question
Generated Question: “In logistic regression, we use the sigmoid function
to "squish" the output of our linear function to be between 0 and 1. What
is the output of the sigmoid function when the input is 0?"

Similarity
0.810

Features

Loss Functions

Closest Question: “Consider the simplest of all neural networks,
consisting of a single unit with a sigmoid activation function:
h(x; w = σ(w0 + w1x) where σ(z) = (1 + exp(−z))−1 Let’s start
with a classiﬁer deﬁned by w0 = −1 and w1 = 1. Which range of input
values x are classiﬁed as positive? Which as negative?"

Generated Question: “You are working on a project to predict how
successful a given Kickstarter campaign will be, based on information
about the campaign (such as the amount of money pledged, the number
of backers, the category of the project, the country where the project is
based, and so forth). One of the features you want to use is the project’s
title. What is a good way to encode the project’s title for use in a
machine-learning predictor?"

0.761

Closest Question:
“Mac O’Larnin is considering selling an app
on Frugal Play. You have a friend with inside info at Frugal, and they’re
able to share data on how previous apps have performed on the store.
Mac decides that he will learn a neural network with no hidden layer
(i.e., consisting only of the output layer). He needs help in ﬁguring out
the precise formulation for machine learning. What is the best way
to encode the app characteristic ’Suitable for people ages (2–4, 5–10,
11–15, 16 and over)’ as a feature for an input to the neural network?
Choose from among the following: multiple unary features (one-hot
encoding), multiple binary features (thermometer encoding), an integer
or real-valued feature. Also give the exact function that maps each input
to its corresponding feature(s)."

0.790

Generated Question: “We have a data set of images, each of which is
either a picture of a dog or a picture of a cat. We want to train a binary
classiﬁer to distinguish between the two classes. Which of the following
is a better choice of loss function for this task:
(a) The 0-1 loss, which gives a loss of 1 if the classiﬁer predicts the
wrong class and a loss of 0 otherwise.
(b) The cross-entropy loss, which is given by -log(p) if the correct class
is predicted and -log(1-p) otherwise, where p is the predicted probability
of the correct class.
(c) The hinge loss, which is given by max(0, 1-s) if the correct class is
predicted and max(0, 1+s) otherwise, where s is the predicted score for
the correct class."

Closest Question: “Consider the following 2D dataset in (x,y) format:
((1,-1), +1), ((1,1), +1), ((1,2.5),+1), ((2,-2),-1), ((2,1),+1),((2,3),+1),((5,-
1),-1),((5,-2),-1).
Break ties in distance by choosing the point
with smaller x1 coordinate, and if still tied, by smaller x2 coordi-
nate. Compute the leave-one-out cross validation accuracy of the
3-nearest-neighbor learning algorithm on this dataset."

15

Table 10: Generating new questions: example of a new question for each topic automatically generated and the closest
question in the dataset based on cosine similarity of the questions embeddings.

Similarity

0.880

Topic

Question

Neural Networks

Generated Question: “Consider a neural network with two hidden layers,
each with two units. The input layer has two units, and the output layer
has one unit. The hidden layers use the sigmoid activation function, and
the output layer uses the linear activation function. The weights from the
input layer to the ﬁrst hidden layer are w11 = 1, w12 = 1, w21 = 1, and
w22 = 1. The weights from the ﬁrst hidden layer to the second hidden
layer are w11 = 1, w12 = 1, w21 = 1, and w22 = 1. The weights from
the second hidden layer to the output layer are w11 = 1, w21 = 1. The
bias terms are all zero.
What is the output of the neural network for the input x1 = 1, x2 = 1?"

Closest Question: “A neural network is given as Z 1 = X ∗ W 1,
A1 = f 1(Z 1), Z 2 = W 2 ∗ A1, ˆy = f 2(Z 2). Speciﬁcally, the input X
is a 4 × 1 column vector, ˆy is a 1 × 1 scalar. W 2 is a 3 × 1 vector. We
also know that, Z 1 = (W 1)T X and Z 2 = (W 2)T A1. What are the
dimensions of Z 2?"

CNNs

Generated Question: “Suppose we have a 3x3 image and we use a 2x2
ﬁlter with stride 1. What are the dimensions of the output image?"

0.895

MDPs

Closest Question: “A neural network is given as Z 1 = X ∗ W 1,
A1 = f 1(Z 1), Z 2 = W 2 ∗ A1, ˆy = f 2(Z 2). There is only one data
point which is: X = [1, 1, 1, 1]T and y = [1]. If W 1 and W 2 are both
matrices/vectors of all ones, what is the resulting Loss where the Loss =
(y − ˆy)2?"

Generated Question: “Consider an MDP with four states, called A, B, C,
and D, and with two actions called Move and Stay. The discount factor
γ = 0.9. Here is a reminder of the Q-learning update formula, based on
experience tuple (s, a, r, s(cid:48)) :

0.988

Q(s, a) := (1 − α)Q(s, a) + α

(cid:16)

r + γ max

a(cid:48)

Q (s(cid:48), a(cid:48))

(cid:17)

Let α = 1. Assume we see the following state-action-reward sequence:
A, Move, 0 B, Move, 0 C, Move, 1 A, Move, 0 B, Move, 0 C, Move, 1
With Q-values all starting at 0, we run the Q-learning algorithm on that
state-action sequence. Provide the q-learning value for Q(C, Move)."

Closest Question:
“Consider an MDP with four states, called
A, B, C, and D, and with two actions called Move and Stay. The
discount factor γ = 0.9. Here is a reminder of the Q-learning update
formula, based on experience tuple (s, a, r, s(cid:48)) :

Q(s, a) := (1 − α)Q(s, a) + α

(cid:16)

r + γ max

a(cid:48)

Q (s(cid:48), a(cid:48))

(cid:17)

Let α = 1. Assume we see the following state-action-reward sequence:
A, Move, 0 B, Move, 0 C, Move, 1 A, Move, 0 With Q-values all starting
at 0, we run the Q-learning algorithm on that state-action sequence.
Provide the q-learning value for Q(A, move).”

16

Table 10: Generating new questions: example of a new question for each topic automatically generated and the closest
question in the dataset based on cosine similarity of the questions embeddings.

Topic

RNNs

Similarity

0.907

Question

the following RNN: st =
Generated Question:
tanh(w1xt + w2st−1 + b) , yt = w3st + b2
. Assume s0 = 0 and
b2 = 0. What values of w1, w2, w3 and b would generate output
sequence [0, 0, 0, 1, 1, 1, 1] given input sequence [0, 0, 0, 1, 0, 1, 0]”

“Consider

Closest Question: “Ronnie makes a simple RNN with state dimension 1
and a step function for f1, so that st = step(w1xt + w2st−1 + b) where
step(z) = 1 if z > 0.0 and equals 0 otherwise, and where the output

yt = st

.

Assuming s0 = 1, we want to generate output sequence [0, 0, 0, 1, 1, 1, 1]
given input sequence [0, 0, 0, 1, 0, 1, 0]. Rennie thinks this is not possible
using Ronnie’s architecture. Rennie makes an argument based on the
relationships in the table above. Is Rennie right?"

Reinforcement Learning Generated Question: “What is the tabular Q-learning update equation,

based on experience tuple (s, a, r, s(cid:48))?"

0.796

Closest Question:
are dimensions of b for multi k-class classiﬁcation?"

“b is the bias for classiﬁer network. What

17

Table 10: Generating new questions: example of a new question for each topic automatically generated and the closest
question in the dataset based on cosine similarity of the questions embeddings.

Topic
Clustering

Question
Generated Question: “Suppose that we have a dataset with n data points,
k clusters, and d features. After running the k-means algorithm, the
within-cluster sum of squared errors (WCSS) is given by:

Similarity
0.767

1
n

n
(cid:88)

i=1

(cid:107)xi − µyi(cid:107)2

where yi is the cluster label of the ith data point, and µyi is the cluster
center associated with the ith data point. The within-cluster sum of
squared errors (WCSS) is a measure of how well the clusters ﬁt the data.
Suppose that we have two datasets, X1 and X2, where X1 has n1 data
points and X2 has n2 data points. We run the k-means algorithm on
both datasets. We ﬁnd that the WCSS for X1 is smaller than the WCSS
for X2. Does this imply that the clusters for X1 are better than the
clusters for X2? Why or why not?"

j,s

(cid:1) + ( fraction of points in B) · H (cid:0)RB

Closest Question: “Consider the following 2D dataset in (x,y) format:
((1,-1), +1), ((1,1), +1), ((1,2.5),+1), ((2,-2),-1), ((2,1),+1),((2,3),+1),((5,-
1),-1),((5,-2),-1). We will construct a tree using a greedy algorithm
that recursively minimizes weighted average entropy. Recall that the
weighted average entropy of a split into subsets A and B is: (fraction
of points in A) · H (cid:0)RA
(cid:1)
j,s
where the entropy H (Rm) of data in a region Rm is given by
H (Rm) = − (cid:80)
ˆPmk. The ˆPmk is the empirical probability,
which is in this case the fraction of items in region m that are of class
k. Some facts that might be useful to you: H(0) = 0, H(3/5) = 0.97,
H(3/8) = 0.95, H(3/4) = 0.81, H(5/6) = 0.65, H(1) = 0. Draw the
decision tree that would be constructed by our tree algorithm for this
dataset. Clearly label the test in each node, which case (yes or no) each
branch corresponds to, and the prediction that will be made at each leaf.
Assume there is no pruning and that the algorithm runs until each leaf
has only members of a single class."

ˆPmk log2

k

Decision Trees

Generated Question: “The Gini score is a measure of how often
a randomly chosen element would be incorrectly labeled if it was
randomly labeled according to the distribution of labels in the subset. It
ranges from 0 to 1, with 0 meaning that there are no mislabeled elements
and 1 meaning that the subset is perfectly mixed. Explain whether or not
it would be a good idea to use the Gini score as a scoring function for
pruning decision trees."

0.867

Closest Question: “There are different strategies for pruning de-
cision trees. We assume that we grow a decision tree until there is one or
a small number of elements in each leaf. Then, we prune by deleting
individual leaves of the tree until the score of the tree starts to get worse.
The question is how to score each possible pruning of the tree. Here is a
deﬁnition of the score: The score is the percentage correct of the tree on
a separate validation set. Explain whether or not it would be a good idea
and give a reason why or why not."

18

