Multi-step Greedy Reinforcement Learning Algorithms

Manan Tomar * 1 Yonathan Efroni * 2 Mohammad Ghavamzadeh 3

0
2
0
2

l
u
J

3
1

]

G
L
.
s
c
[

3
v
9
1
9
2
0
.
0
1
9
1
:
v
i
X
r
a

Abstract
Multi-step greedy policies have been extensively
used in model-based reinforcement learning (RL),
both when a model of the environment is available
(e.g., in the game of Go) and when it is learned. In
this paper, we explore their beneﬁts in model-free
RL, when employed using multi-step dynamic
programming algorithms: κ-Policy Iteration (κ-
PI) and κ-Value Iteration (κ-VI). These methods
iteratively compute the next policy (κ-PI) and
value function (κ-VI) by solving a surrogate deci-
sion problem with a shaped reward and a smaller
discount factor. We derive model-free RL algo-
rithms based on κ-PI and κ-VI in which the sur-
rogate problem can be solved by any discrete or
continuous action RL method, such as DQN and
TRPO. We identify the importance of a hyper-
parameter that controls the extent to which the
surrogate problem is solved and suggest a way to
set this parameter. When evaluated on a range of
Atari and MuJoCo benchmark tasks, our results in-
dicate that for the right range of κ, our algorithms
outperform DQN and TRPO. This shows that our
multi-step greedy algorithms are general enough
to be applied over any existing RL algorithm and
can signiﬁcantly improve its performance.

1. Introduction

Reinforcement learning (RL) algorithms solve sequential
decision-making problems through repeated interaction with
the environment. By incorporating deep neural networks
into RL algorithms, the ﬁeld has recently witnessed remark-
able empirical success (e.g., Mnih et al., 2015; Lillicrap
et al., 2015; Levine et al., 2016; Silver et al., 2017). Much
of this success has been achieved by model-free RL algo-
rithms, such as Q-learning and policy gradients. These
algorithms are known to suffer from high variance in their

*Equal contribution 1Facebook AI Research, Menlo Park, USA
2Technion, Haifa, Israel 3Google Research, Mountain View, USA.
Correspondence to: Manan Tomar <manan.tomar@gmail.com>.

Proceedings of the 37 th International Conference on Machine
Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by
the author(s).

estimations (Greensmith et al., 2004) and to have difﬁculties
in handling function approximation (e.g., Thrun & Schwartz,
1993; Baird, 1995; Van Hasselt et al., 2016; Lu et al., 2018).
These issues are intensiﬁed in decision problems with long
horizon, i.e., when the discount factor, γ, is large. Although
using smaller values of γ addresses the discount factor-
dependent issues and leads to more stable algorithms (Petrik
& Scherrer, 2009; Jiang et al., 2015), it does not come for
free, as the algorithm may return a biased solution, i.e., it
may not converge to an optimal (or good) solution for the
original decision problem (the one with larger value of γ).

Efroni et al. (2018a) recently proposed another approach
to mitigate the γ-dependant instabilities in RL in which
they study multi-step greedy versions of the well-known
Dynamic Programming (DP) algorithms: Policy Iteration
(PI) and Value Iteration (VI) (Bertsekas & Tsitsiklis, 1996).
They also proposed an alternative formulation of the multi-
step greedy policy, called κ-greedy policy, and studied the
convergence of the resulted PI and VI algorithms: κ-PI and
κ-VI. These algorithms iteratively solve a γκ-discounted
decision problem, whose reward has been shaped by the
solution of the decision problem at the previous iteration.
Unlike the biased solution obtained by solving the decision
problem with a smaller value of γ (discussed above), by iter-
atively solving decision problems with a shorter γκ horizon,
the κ-PI and κ-VI algorithms could converge to an optimal
policy of the original decision problem.

In this paper, we derive model-free RL algorithms based on
the κ-greedy formulation of multi-step greedy policies. As
mentioned earlier, the main component of this formulation is
(approximately) solving a surrogate decision problem with a
shaped reward and a smaller discount factor. Our algorithms
build on κ-PI and κ-VI, and solve the surrogate decision
problem with the popular deep RL algorithms: Deep Q-
Network (DQN) (Mnih et al., 2015) and Trust Region Policy
Optimization (TRPO) (Schulman et al., 2015). We call the
resulting algorithms κ-PI-DQN, κ-VI-DQN, κ-PI-TRPO,
and κ-VI-TRPO, and empirically evaluate and compare
them with DQN, TRPO, and Generalized Advantage Esti-
mation (GAE) (Schulman et al., 2016) on Atari (Bellemare
et al., 2013) and MuJoCo (Todorov et al., 2012) bench-
marks. Our results indicate that for the right range of κ, our
algorithms outperform DQN and TRPO. This suggests that
the performance of these two deep RL algorithms can be

 
 
 
 
 
 
Multi-step Greedy Reinforcement Learning Algorithms

improved by using them as a solver within the multi-step
greedy PI and VI schemes.

Moreover, our results indicate that the success of our algo-
rithms depends on a number of non-trivial design choices. In
particular, we identify the importance of a hyper-parameter
that controls the extent to which the surrogate decision prob-
lem is solved, and use the theory of multi-step greedy DP to
derive a recipe for setting this parameter. We show the ad-
vantage of using hard over soft updates, verifying the theory
in Efroni et al. (2018b, Thm. 1). By hard and soft update,
we refer to fully solving the surrogate MDP in a model-free
manner and then evaluating the resulting policy (policy im-
provement and evaluation steps are separated) vs. changing
the policy at each iteration (policy improvement and evalua-
tion steps are concurrent – each improvement is followed
by an evaluation).

We also establish a connection between our multi-step
greedy algorithms and GAE. In particular, we show that
our κ-PI-TRPO algorithm coincides with GAE and we can
obtain GAE by minor modiﬁcations to κ-PI-TRPO. Finally,
we show the advantage of using our multi-step greedy al-
gorithms over lowering the discount factor in DQN (value-
based) and TRPO (policy-based) algorithms. Our results
indicate that while lowering the discount factor is detrimen-
tal to performance, our multi-step greedy algorithms indeed
improve over DQN and TRPO.

2. Preliminaries

In this paper, we assume that the agent’s interaction with the
environment is modeled as a discrete time γ-discounted
Markov Decision Process (MDP), deﬁned by Mγ =
(S, A, P, R, γ, µ), where S and A are the state and action
spaces; P ≡ P (s(cid:48)|s, a) is the transition kernel; R ≡ r(s, a)
is the reward function with the maximum value of Rmax;
γ ∈ (0, 1) is the discount factor; and µ is the initial state
distribution. Let π : S → P(A) be a stationary Markovian
policy, where P(A) is a probability distribution on the set A.
The value function of a policy π at any state s ∈ S is deﬁned
as V π(s) ≡ E[(cid:80)
t≥0 γtr(st, at)|s0 = s, π], where the ex-
pectation is over all the randomness in the policy, dynamics,
and rewards. Similarly, the action-value function of π is
deﬁned as Qπ(s, a) = E[(cid:80)
t≥0 γtr(st, at)|s0 = s, a0 =
a, π]. Since the rewards are bounded by Rmax, both V and Q
functions have the maximum value of Vmax = Rmax/(1 − γ).
An optimal policy π∗ is the policy with maximum value at
every state. We call the value of π∗ the optimal value, and
deﬁne it as V ∗(s) = maxπ V π(s), ∀s ∈ S. We denote by
Q∗(s, a), the state-action value of π∗, and remind that the
following relation holds V ∗(s) = maxa Q∗(s, a), for all
s. The algorithms by which we solve an MDP (obtain an
optimal policy) are mainly based on two popular DP algo-
rithms: Policy Iteration (PI) and Value Iteration (VI). While

VI relies on iteratively computing the optimal Bellman op-
erator T applied to the current value function V (Eq. 1),
PI relies on (iteratively) calculating a 1-step greedy policy
π1-step w.r.t. to the value function of the current policy V
(Eq. 2): for all s ∈ S, we have

(T V )(s) = max
a∈A

E[r(s0, a) + γV (s1) | s0 = s],

(1)

π1-step(s) ∈ arg max
a∈A

E[r(s0, a) + γV (s1) | s0 = s]. (2)

It is known that T is a γ-contraction w.r.t. the max-norm and
its unique ﬁxed-point is V ∗, and the 1-step greedy policy
w.r.t. V ∗ is an optimal policy π∗. In practice, the state space
is often large, and thus, we can only approximately compute
Eqs. 1 and 2, which results in approximate PI (API) and VI
(AVI) algorithms. These approximation errors then propa-
gate through the iterations of the API and AVI algorithms.
However, it has been shown that this (propagated) error can
be controlled (Munos, 2003; 2005; Farahmand et al., 2010),
and after N steps, the algorithms approximately converge
to a solution πN , whose difference with the optimal value is
bounded (see e.g., Scherrer, 2014 for API):

η(π∗) − η(πN ) ≤ Cδ/(1 − γ)2 + γN Vmax.

(3)

In Eq. 3, the scalar η(π) = Es∼µ[V π(s)] is the expected
value function at the initial state,1 δ represents the per-
iteration error, and C upper-bounds the mismatch between
the sampling distribution and the distribution according to
which the ﬁnal value function is evaluated (µ in Eq. 3), de-
pending heavily on the dynamics. Finally, the second term
on the RHS of Eq. 3 is the error due to initial values of
policy/value and decays with the number of iterations N .

3. κ-Greedy Policy: κ-PI and κ-VI Algorithms

The optimal Bellman operator T (Eq. 1) and 1-step greedy
policy π1-step (Eq. 2) can be generalized to their multi-step
versions. The most straightforward form of this generaliza-
tion is realized by replacing T and π1-step with h-optimal
Bellman operator and h-step greedy policy (i.e., a lookahead
of horizon h), respectively. This is done by substituting the
1-step return in Eqs. 1 and 2, r(s0, a) + γV (s1), with the
h-step return, (cid:80)h−1
t=0 r(st, at) + γhV (sh), and computing
the maximum over actions a0, . . . , ah−1, instead of just
a0 (Bertsekas & Tsitsiklis, 1996). Efroni et al. (2018a) pro-
posed an alternative form for the multi-step optimal Bellman
operator and greedy policy, called κ-optimal Bellman oper-

1Note that the LHS of Eq. 3 is the (cid:96)1-norm of (V π∗

− V πN )

w.r.t. the initial state distribution µ.

Multi-step Greedy Reinforcement Learning Algorithms

ator, Tκ, and κ-greedy policy, πκ, for κ ∈ [0, 1], i.e.,
(cid:88)

(γκ)trt(κ, V ) | s0 = s, π],

(TκV )(s) = max

E[

π

πκ(s) ∈ arg max

π

E[

t≥0
(cid:88)

(γκ)trt(κ, V ) | s0 = s, π],

t≥0

(4)

(5)

for all s ∈ S. In Eqs. 4 and 5, the shaped reward rt(κ, V )
w.r.t. the value function V is deﬁned as

rt(κ, V ) ≡ rt + γ(1 − κ)V (st+1).

(6)

It can be shown that the κ-greedy policy w.r.t. the value
function V is the optimal policy w.r.t. a κ-weighted geomet-
ric average of all future h-step returns (from h = 0 to ∞).
This can be interpreted as TD(λ) (Sutton & Barto, 2018)
for policy improvement (see Efroni et al., 2018a, Sec. 6).
The important difference is that TD(λ) is used for policy
evaluation and not for policy improvement.

It is easy to see that solving Eqs. 4 and 5 is equivalent to
solving a surrogate γκ-discounted MDP with the shaped
reward rt(κ, V ), which we denote by Mγκ(V ) throughout
the paper. The optimal value and policy of the surrogate
MDP Mγκ(V ) are TκV and the κ-greedy policy πκ, respec-
tively. Using the notions of κ-optimal Bellman operator, Tκ,
and κ-greedy policy, πκ, Efroni et al. (2018a) derived κ-PI
and κ-VI algorithms, whose pseudocodes are shown in Al-
gorithms 1 and 2. κ-PI iteratively (i) evaluates the value V πi
of the current policy πi, and (ii) sets the new policy, πi+1,
to the κ-greedy policy w.r.t. the value of the current policy
V πi, by solving Eq. 5. On the other hand, κ-VI repeat-
edly applies the Tκ operator to the current value function
Vi (solves Eq. 4) to obtain the next value function, Vi+1,
and returns the κ-greedy policy w.r.t. the ﬁnal value VNκ.
Note that for κ = 0, the κ-optimal Bellman operator and
κ-greedy policy are equivalent to their 1-step counterparts,
deﬁned by Eqs. 1 and 2, which indicates that κ-PI and κ-VI
are generalizations of the seminal PI and VI algorithms.

Algorithm 1 κ-Policy Iteration

1: Initialize: κ ∈ [0, 1], π0, Nκ
2: for i = 0, 1, . . . , Nκ − 1 do
t≥0 γtrt | πi]
3:
4:

V πi = E[(cid:80)
πi+1 ← arg max

E[(cid:80)

π

5: end for
6: Return πNκ

t≥0(κγ)trt(κ, V πi ) | π]

Algorithm 2 κ-Value Iteration

1: Initialize: κ ∈ [0, 1], V0 , Nκ
2: for i = 0, 1, . . . , Nκ − 1 do
Vi+1 = maxπ E[(cid:80)
3:
4: end for
5: πNκ ← arg max
6: Return πNκ

E[(cid:80)

π

t≥0(γκ)trt(κ, Vi) | π]

t≥0(κγ)trt(κ, VNκ ) | π]

It has been shown that both PI and VI converge to the
optimal value with an exponential rate that depends on
the discount factor γ, i.e., (cid:107)V ∗ − V πN (cid:107)∞ ≤ O(γN ) (see
e.g., Bertsekas & Tsitsiklis, 1996). Analogously, Efroni et al.
(2018a) showed that κ-PI and κ-VI converge with a faster
exponential rate ξκ = γ(1−κ)
1−γκ ≤ γ, i.e., (cid:107)V ∗ − V πNκ (cid:107)∞ ≤
O(ξNκ
κ ), with the cost that each iteration of these algorithms
is computationally more expensive than that of PI and VI. Fi-
nally, we state the following property of κ-PI and κ-greedy
policies that we use in our κ-PI and κ-VI based RL algo-
rithms described in Section 4:

Asymptotic performance depends on κ. Efroni et al. (2018b,
Thm. 5) proved the following bound on the performance of
κ-PI that is similar to the one in Eq. 3 for API:
+ ξNκ
κ Vmax
(cid:125)
(cid:123)(cid:122)
(cid:124)
Decaying Term

η(π∗) − η(πNκ ) ≤ Cκδκ/(1 − γ)2
(cid:123)(cid:122)
(cid:125)
Asymptotic Term

(7)

(cid:124)

,

where δκ and Cκ are quantities similar to δ and C in Eq. 3.
Note that while the second term on the RHS of Eq. 7 decays
with Nκ, the ﬁrst one is independent of Nκ.

4. κ-PI and κ-VI based RL Algorithms

As described in Section 3, implementing κ-PI and κ-VI
requires iteratively solving a γκ-discounted surrogate MDP
with a shaped reward. If a model of the problem is given,
the surrogate MDP can be solved using a DP algorithm (see
Efroni et al., 2018a, Sec. 7). When a model is not available,
we should approximately solve the surrogate MDP using
a model-free RL algorithm. In this paper, we focus on the
latter case and propose RL algorithms inspired by κ-PI and
κ-VI. In our algorithms, we use model-free RL algorithms
DQN (Mnih et al., 2015) and TRPO (Schulman et al., 2015)
(for discrete and continuous action problems, respectively)
as subroutines for estimating a κ-greedy policy (Line 4 in
Alg. 1, κ-PI, and Line 5 in Alg. 2, κ-VI) and an optimal
value of the surrogate MDP (Line 3 in Alg. 2, κ-VI). We
refer to the resulting algorithms as κ-PI-DQN, κ-VI-DQN,
κ-PI-TRPO, and κ-VI-TRPO.

In order to have an efﬁcient implementation of our κ-PI and
κ-VI based algorithms, the main question to answer is how
a ﬁxed number of samples T should be allocated to different
parts of the κ-PI and κ-VI algorithms? More precisely, how
shall we set Nκ ∈ N, i.e., the total number of iterations
of our algorithms, and determine the number of samples
to solve the surrogate MDP at each iteration? To answer
these questions, we devise a heuristic approach based on
the theory of κ-PI and κ-VI algorithms, and in particular
Eq. 7. Since Nκ only appears explicitly in the second term
on the RHS of Eq. 7, an appropriate choice of Nκ is such
that Cκδκ/(1 − γ)2 (cid:39) ξNκ
κ Vmax. Note that setting Nκ to
a higher value would not signiﬁcantly improve the perfor-
mance, because the asymptotic term in Eq. 7 is independent

Multi-step Greedy Reinforcement Learning Algorithms

of Nκ. In practice, since δκ and Cκ are unknown, we set
Nκ to satisfy the following equality:

ξNκ
κ = CFA,

(8)

where CFA is a hyper-parameter that depends on the ﬁnal-
accuracy we aim for. For example, if our goal is to obtain
90% accuracy, we would set CFA = 0.1, which results
in Nκ=0.99 (cid:39) 4 and Nκ=0.5 (cid:39) 115, for γ = 0.99. Our
experimental results in Section 5 suggest that this approach
leads to a reasonable choice for the total number of iterations
Nκ. It is important to note the following facts: 1) as we
increase κ, we expect less iterations are needed for κ-PI
and κ-VI to converge to a good policy, and 2) the effective
horizon2 of the surrogate MDP that κ-PI and κ-VI solve at
each iteration increases with κ.

Lastly, we need to determine the number of samples for each
iteration of our κ-PI and κ-VI based algorithms. We allocate
equal number of samples per iteration, denoted by Tκ. Since
the total number of samples, T , is known beforehand, we
set the number of samples per iteration to

Tκ = T /Nκ.

(9)

In the rest of the paper, we ﬁrst derive our DQN-based and
TRPO-based algorithms in Sections 4.1 and 4.2. It is impor-
tant to note that for κ = 1, our algorithms are reduced to
DQN and TRPO. We then conduct a set of experiments with
our algorithms in Sections 5.1 and 5.2 in which we carefully
study the effect of κ and Nκ (or equivalently the hyper-
parameter CFA, deﬁned by Eq. 8) on their performance.

4.1. κ-PI-DQN and κ-VI-DQN Algorithms

Algorithm 3 presents the pseudo-code of κ-PI-DQN. Due
to space constraints, we report the detailed pseudo-code
in Appendix A.1 (Alg. 5). We use four neural networks
in this algorithm, two to represent the Q-function of the
original MDP (with discount factor γ and reward r), Qφ
(Q-network) and Q(cid:48)
φ (target network), and two for the
Q-function of the surrogate MDP, Qθ (Q-network) and
Q(cid:48)
θ (target network). In the policy improvement step, we
use DQN to solve the γκ-discounted surrogate MDP with
the shaped reward rj(κ, Vφ) = rj + γ(1 − κ)Vφ(sj+1),
i.e., Mγκ(Vφ), where Vφ (cid:39) V πi−1 and is computed as
Vφ(s) = Qφ(s, arg maxa Q(cid:48)
θ(s, a)). The output of DQN is
(approximately) the optimal Q-function of Mγκ(Vφ), and
thus, the new policy πi, which is the (approximate) κ-greedy
policy w.r.t. Vφ is equal to πi(·) = arg maxa Q(cid:48)
θ(·, a). In
the policy evaluation step, we use off-policy TD(0) to evalu-
ate the Q-function of the current policy πi, i.e., Qφ (cid:39) Qπi.
Although what is needed is an estimate of the value function

2The effective horizon of a γκ-discounted MDP is 1/(1−γκ).

Algorithm 3 κ-PI-DQN
1: Initialize replay buffer D; Q-networks Qθ, Qφ; target net-

works Q(cid:48)

θ, Q(cid:48)
φ;
2: for i = 0, . . . , Nκ − 1 do
3:
4:
5:

# Policy Improvement
for t = 1, . . . , Tκ do

6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

Act by an (cid:15)-greedy policy w.r.t. Qθ(st, a), observe
rt, st+1, and store (st, at, rt, st+1) in D;
Sample a batch {(sj, aj, rj, sj+1)}N
Update θ using DQN with
{(sj, aj, rj(κ, Vφ), sj+1)}N
Vφ(sj+1) = Qφ(sj+1, πi−1(sj+1)) and
πi−1(·) ∈ arg maxa Q(cid:48)
Copy θ to θ(cid:48) occasionally

j=1 from D;

j=1, where

(θ(cid:48) ← θ);

θ(·, a);

end for
# Policy Evaluation of πi(s) ∈ arg maxa Q(cid:48)
for t = 1, . . . , Tκ do

θ(s, a)

Sample a batch {(sj, aj, rj, sj+1)}N
Update φ using this data and off-policy TD(0) to estimate
the Q-function of the current policy πi;
(φ(cid:48) ← φ);
Copy φ to φ(cid:48) occasionally

j=1 from D;

17:
18:
19: end for

end for

of the current policy, Vφ (cid:39) V πi, we chose to evaluate its Q-
function, because the available data (the transitions stored in
the replay buffer) is off-policy, and unlike the value function,
the Q-function of a ﬁxed policy can be easily evaluated with
this type of data using off-policy TD(0).

Remark 1. In the policy improvement phase, πi−1 is com-
puted as the arg max of Q(cid:48)
θ(·, a) (Line 10), a quantity that
is not constant and is (slowly) changing during this phase.
Addressing this issue requires using an additional target net-
work that is set to Qθ only at the end of each improvement
step and its arg max is used to compute πi−1 throughout
the improvement step of the next iteration. We tested using
this additional network in our experiments, but it did not
improve the performance, and thus, we decided to report
the algorithm without it.

We report the pseudo-code of κ-VI-DQN in Appendix A.1
(Alg. 6). Note that κ-VI simply repeats V ← TκV and com-
putes TκV , which is the optimal value of the surrogate MDP
Mγκ(V ). In κ-VI-DQN, we repeatedly solve Mγκ(V ) by
DQN and use its (approximately) optimal Q-function to
shape the reward of the next iteration. The algorithm uses
three neural networks, two to solve the surrogate MDP by
DQN, Qθ (Q-network) and Q(cid:48)
θ (target network), and one
to store its optimal Q-function to use it for the shaped re-
ward in the next iteration, Qφ. Let Q∗
γκ,V be
the optimal Q and V functions of Mγκ(V ). Then, we
have maxa Q∗
γκ,V (s) = (TκV )(s), where
the ﬁrst equality is by deﬁnition (Sec. 2) and the second one
holds since TκV is the optimal value of Mγκ(V ) (Sec. 3).
Therefore, in κ-VI-DQN, we shape the reward at each itera-
tion by maxa Qφ(s, a), where Qφ is the output of the DQN

γκ,V (s, a) = V ∗

γκ,V and V ∗

Multi-step Greedy Reinforcement Learning Algorithms

Breakout

SpaceInvaders

Figure 1: Training performance of the ‘naive’ baseline Nκ = T and κ-PI-DQN, κ-VI-DQN for CF A = 0.05 on Breakout
(top) and SpaceInvaders (bottom). See Appendix A.2 for performance w.r.t. different CF A values.

Algorithm 4 κ-PI-TRPO
1: Initialize V -networks Vθ and Vφ; policy network πψ;
2: for i = 0, . . . , Nκ − 1 do
3:
4:

for t = 1, . . . , Tκ do

t=j(γκ)t−jrt(κ, Vφ) and

Simulate the current policy πψ for M steps and calculate
the following two returns for all steps j:
Rj(κ, Vφ) = (cid:80)M
ρj = (cid:80)M
t=j γt−jrt;
Update θ by minimizing the batch loss function:
LVθ = 1
N
# Policy Improvement
Update ψ using TRPO and the batch
{(Rj(κ, Vφ), Vθ(sj))}N

j=1(Vθ(sj) − Rj(κ, Vφ))2;

(cid:80)N

j=1;

end for
# Policy Evaluation
Update φ by minimizing the batch loss function:
j=1(Vφ(sj) − ρj)2;
LVφ = 1
N

(cid:80)N

11: end for

5:

6:
7:

8:
9:
10:

from the previous iteration, i.e., maxa Qφ(s, a) (cid:39) TκVi−1 .

4.2. κ-PI-TRPO and κ-VI-TRPO Algorithms

Algorithm 4 presents the pseudo-code of κ-PI-TRPO (a
detailed pseudo-code is reported in Appendix B.1, Alg. 7).
Recall that TRPO iteratively updates the current policy using
its return and an estimate of its value function. At each
iteration i of κ-PI-TRPO: 1) we use the estimate of the

value of the current policy Vφ (cid:39) V πi−1 to calculate the
return R(κ, Vφ) and the estimate of the value function Vθ of
the surrogate MDP Mγκ(Vφ), 2) we use R(κ, Vφ) and Vθ to
compute the new policy πi (TRPO style), and 3) we estimate
the value of the new policy Vφ (cid:39) V πi in the original MDP
(with discount factor γ and reward r). The algorithm uses
three neural networks, one for the value function of the
original MDP, Vφ, one for the value function of the surrogate
MDP, Vθ, and one for the policy, πψ.

We report the pseudo-code of κ-VI-TRPO in Appendix B.1,
Alg. 8. As previously noted, κ-VI iteratively solves the
surrogate MDP and uses its optimal value TκVi−1 to shape
the reward of the surrogate MDP in the next iteration. In κ-
VI-TRPO, we solve the surrogate MDP Mγκ(Vi−1 = Vφ)
with TRPO until its policy πψ converges to the optimal
policy of Mγκ(Vi−1 = Vφ) and its value Vθ converges to
TκVi−1 = TκVφ. We then replace Vi with Vθ (Vi = Vφ ←
Vθ) and repeat this process.

5. Experimental Results

In our experiments, we speciﬁcally focus on answering the
following questions:

1. Does the performance of DQN and TRPO improve
when using them as κ-greedy solvers in κ-PI and κ-
VI? Is there a performance tradeoff w.r.t. to κ?

Multi-step Greedy Reinforcement Learning Algorithms

Walker-v2

Ant-v2

Figure 2: Training performance of the ‘naive’ baseline Nκ = T and κ-PI-TRPO, κ-VI-TRPO for CF A = 0.2 on Walker
(top) and Ant (bottom). See Appendix B.2 for performance w.r.t. different CF A values. Each iteration corresponds to
roughly 1000 environment samples, and thus, the total number of training samples is 2 millions.

2. Following κ-PI and κ-VI, our DQN and TRPO imple-
mentations of these algorithms devote a signiﬁcant Tκ
number of samples to each iteration. Is this needed or
a ‘naive’ choice of Tκ = 1, or equivalently Nκ = T ,
works just well for all values of κ?

We choose to test our κ-DQN and κ-TRPO algorithms on
the Atari and MuJoCo benchmarks, respectively. Both of
these algorithms use standard setups, including the use of the
Adam optimizer for performing gradient descent, a discount
factor of 0.99 across all tasks, target Q value networks
in the case of κ-DQN and an entropy regularizer with a
coefﬁcient of 0.01 in the case of κ-TRPO. We choose to
run our experiments for multiple values of κ between zero
and one, which roughly follow a logarithmic scale. Note
that just by using the deﬁnition of κ-greedy algorithms, we
reduce to the base cases of DQN and TRPO when setting
κ = 1.0 for all experiments. This forms as one of the two
baselines we consider in this work. The second baseline
essentially refers to using our κ-greedy algorithms for a
ﬁxed Nκ = T value. Thus, independent of the value of κ,
the surrogate MDP is solved for a single time-step per each
iteration. Below, we describe the experiments and results in
further detail. The implementation details for the κ-DQN
and κ-TRPO cases are provided in Appendix A, Table 3 and
Appendix B, Table 4, respectively.

5.1. κ-PI-DQN and κ-VI-DQN Experiments

In this section, we empirically analyze the performance of
the κ-PI-DQN and κ-VI-DQN algorithms on the Atari do-
mains: Breakout, SpaceInvaders, Seaquest, Enduro, Beam-
Rider, and Qbert (Bellemare et al., 2013). We start by per-
forming an ablation test on three values of hyper-parameter
CF A = {0.001, 0.05, 0.2} on the Breakout domain. The
value of CF A sets the number of samples per iteration Tκ
(Eq. 8) and the total number of iterations Nκ (Eq. 9). The
total number of samples is set to T (cid:39) 106. This value rep-
resents the number of samples after which our DQN-based
algorithms approximately converge. For each value of CF A,
we test κ-PI-DQN and κ-VI-DQN for several κ values. In
both algorithms, the best performance was obtained with
CF A = 0.05. Therefore, CF A is set to 0.05 for all our
experiments with other Atari domains.

Figure 1 shows the training performance of κ-PI-DQN and
κ-VI-DQN on Breakout and SpaceInvaders for the best
value of CF A = 0.05, as well as for the ‘naive’ baseline
Tκ = 1, or equivalently Nκ = T . The results on Breakout
for the other values of CF A and the training plots for all
other Atari domains (with CF A = 0.05) are reported in
Appendices A.2 and A.3, respectively. Table 1 shows the
ﬁnal training performance of κ-PI-DQN and κ-VI-DQN

Multi-step Greedy Reinforcement Learning Algorithms

Domain

Breakout

SpaceInv.

Seaquest

Enduro

BeamRider

Qbert

Alg.
κ-PI
κ-VI
κ-PI
κ-VI
κ-PI
κ-VI
κ-PI
κ-VI
κ-PI
κ-VI
κ-PI
κ-VI

κbest
223(±7), κ=0.84
181(±7), κ=0.68
755(±23), κ=0.84
712(±25), κ=0.92
5159(±292), κ=0.92
3253(±402), κ=0.84
533(±12), κ=0.84
486(±23), κ=0.84
3849(±110), κ=1.0
4277 (±269), κ=0.84
8157(±265), κ=0.84
8060 (±158), κ=0.84

κ = 0
154(±3)
174(±5)
613(±20)
687(±32)
2612(±238)
2680(±382)
478(±10)
443 (±90)
3103(±279)
3714 (±437)
6719(±520)
7563 (±398)

DQN, κ = 1 Nκ = T , κbest (κ-PI)

134(±4)

170(±2), κ=0.68

656(±17)

700(±21), κ=0.98

3099(±191)

3897(±218), κ=0.68

224(±110)

535(±13), κ=0.68

3849(±110)

3849(±110), κ=1.0

7258(±385)

7968(±218), κ=0.98

Table 1: The ﬁnal training performance of κ-PI-DQN and κ-VI-DQN on the Atari domains, for the hyper-parameter
CF A = 0.05. The values are reported for a 95% conﬁdence interval across 10 random runs (empirical mean ± 1.96 ×
empirical standard deviation /
n = 10). The best scores are in bold and multiple bold values for a domain denote an
insigniﬁcant statistical difference between them.

√

Domain

Walker

Ant

HalfCheetah

HumanoidStand

Swimmer

Hopper

Alg.
κ-PI
κ-VI
κ-PI
κ-VI
κ-PI
κ-VI
κ-PI
κ-VI
κ-PI
κ-VI
κ-PI
κ-VI

κbest
1438(±188), κ=0.68
954(±88), κ=0.68
1377(±183), κ=0.68
2998(±264), κ=0.68
1334(±151), κ=0.36
1447(±346), κ=0.36
72604(±1219), κ=0.99
72821(±908), κ=0.99
107(±12), κ=1.0
114(±15), κ=1.0
1486(±324), κ=0.68
1069(±76), κ=0.92

κ = 0
1371 (±192)
830(±262)
1006(±106)
1879(±128)
907(±176)
1072(±30)
52936(±1529)
51148(±1377)
42(±3)
46(±1)
1012(±263)
531(±125)

TRPO, κ = 1 Nκ = T , κbest (κ-PI)

GAE, λbest

594 (±99)

1082(±110), κ=0.0

1601(±190), λ=0.0

-19(±1)

1090(±99), κ=0.68

1094(±139), λ=0.0

-18(±87)

1195(±218), κ=0.36

1322(±213), λ=0.36

68143(±1031)

71331(±1149), κ=0.98

71932(±2122), λ=0.98

107(±12)

107(±12), κ=1.0

103(±13), λ = 1.0

1142(±141)

1434(±129), κ=0.98

1600(±134), λ = 0.84

Table 2: The ﬁnal training performance of κ-PI-TRPO and κ-VI-TRPO on the MuJoCo domains, for the hyper-parameter
CF A = 0.2. The values are reported for a 95% conﬁdence interval across 10 random runs (empirical mean ± 1.96 ×
empirical standard deviation /
n = 10). The best scores are in bold and multiple bold values for a domain denote an
insigniﬁcant statistical difference between them.

√

on the Atari domains with CF A = 0.05. Note that the
scores reported in Table 1 are the actual returns on the Atari
domains, while the vertical axis in the plots of Figure 1
corresponds to a scaled return. We plot the scaled return,
since this way it can be easier to reproduce our results using
the OpenAI Baselines codebase (Hill et al., 2018).

Our results exhibit that both κ-PI-DQN and κ-VI-DQN
improve the performance of DQN (κ = 1). Moreover, they
show that setting Nκ = T leads to a clear degradation of
the ﬁnal training performance on all of the domains except
for Enduro, which gets to approximately the same score.
Although the performance degrades, the results for Nκ = T
are still better than for DQN.

5.2. κ-PI-TRPO and κ-VI-TRPO Experiments

In this section, we empirically analyze the performance
of the κ-PI-TRPO and κ-VI-TRPO algorithms on the Mu-
JoCo (Todorov et al., 2012) based OpenAI Gym domains:

Walker2d-v2, Ant-v2, HalfCheetah-v2, HumanoidStandup-
v2, Swimmer-v2, and Hopper-v2 (Brockman et al., 2016).
As in Section 5.1, we start by performing an ablation test
on the parameter CF A = {0.001, 0.05, 0.2} on the Walker
domain. We set the total number of iterations to 2000, with
each iteration consisting 1000 samples. Thus, the total num-
ber of samples is T (cid:39) 2 × 106. This is the number of sam-
ples after which our TRPO-based algorithms approximately
converge. For each value of CF A, we test κ-PI-TRPO and
κ-VI-TRPO for several κ values. In both algorithms, the
best performance was obtained with CF A = 0.2, and thus,
we set CF A = 0.2 in our experiments with other MuJoCo
domains.

Figure 2 shows the training performance of κ-PI-TRPO
and κ-VI-TRPO on Walker and Ant domains for the best
value of CF A = 0.2, as well as for the ‘naive’ baseline
Tκ = 1, or equivalently Nκ = T . The results on Walker
for the other CF A values and the other MuJoCo domains
(with CF A = 0.2) are reported in Appendices B.2 and B.3,

Multi-step Greedy Reinforcement Learning Algorithms

Figure 3: Lowering the discount factor γ for Atari domains Breakout and SpaceInvaders and MuJoCo domains Walker-v2
and Ant-v2

respectively. Table 2 shows the ﬁnal training performance
of κ-PI-TRPO and κ-VI-TRPO on the MuJoCo domains
with CF A = 0.2.

The results exhibit that both κ-PI-TRPO and κ-VI-TRPO
yield better performance than TRPO (κ = 1). Furthermore,
they show that the algorithms with CF A = 0.2 perform bet-
ter than with Nκ = T for three out of six domains (Walker,
Ant and HalfCheetah) and equally well for the remaining
three (HumanoidStandup, Swimmer and Hopper).

6. Discussion and Related Work

Comparison with GAE: There is a close connection be-
tween the Generalized Advantage Estimation (GAE) algo-
rithm (Schulman et al., 2016) and κ-PI. In GAE, the policy
is updated by the following gradient:
(cid:2)V πθ (s)(cid:3) = Es∼dµ

(cid:2)∇θ log πθ(s)

∇θEs∼µ

(γλ)tδV

(cid:88)

(cid:3),

π

δV = rt + γVt+1 − Vt,

(10)

t

where dµ
π is the occupancy measure of policy π. Eq. 10
can be interpreted as a gradient in a γλ-discounted MDP
with shaped rewards δV , which we refer to as Mγλ(δV ).
As noted in Efroni et al. (2018a, Sec. 6), an optimal policy
π∗
γλ of the MDP Mγλ(δV ) is also optimal in Mγκ(V ) with
κ = λ. This means that π∗
γλ is the κ-greedy policy w.r.t. V .
Comparing the two algorithms, we notice that GAE is con-
ceptually similar to κ-PI-TRPO with Tκ = 1, or equiva-
lently Nκ = T . In fact, in the case of Tκ = 1, we can

obtain a pseudo-code of GAE by removing Line 5 (no need
to estimate the value of the surrogate MDP) and replacing
rt(κ, Vφ) with the TD-error of the original MDP on Line 4
in Algorithm 4.

In Section 5.2, we compared the empirical performance of
GAE with that of κ-PI-TRPO and κ-VI-TRPO (see Figure 2
and Table 2). The results show that κ-PI-TRPO and κ-VI-
TRPO perform better than or on par with GAE. Moreover,
we observe that in most domains, the performance of GAE
is equivalent to that of κ-PI-TRPO with Nκ = T . This is
in accordance with the description above connecting GAE
to our naive baseline implementation. We report all GAE
results in Appendix B.3.

Remark 2 (GAE Implementation). In the OpenAI imple-
mentation of GAE, the value network is updated w.r.t. to the
target (cid:80)
t(γλ)trt, whereas in the GAE paper (Schulman
et al., 2016), (cid:80)
t γtrt is used as the target. We chose the
latter form in our implementation of GAE to be in accord
with the paper.

Lowering Discount Factor in DQN and TRPO: To show
the advantage of κ-PI and κ-VI based algorithms over sim-
ply lowering the discount factor γ, we test the performance
of the “vanilla” DQN and TRPO algorithms with values of
γ lower than the one previously used (i.e., γ = 0.99). As ev-
ident from Figure 3, only in the Ant domain, this approach
results in an improved performance (for γ = 0.68). On the
other hand, in the Ant domain, the performance of κ-PI-
TRPO, and especially κ-VI-TRPO, surpasses that of TRPO

Multi-step Greedy Reinforcement Learning Algorithms

with the lower value of γ = 0.68. In Breakout, SpaceIn-
vaders, and Walker, the performance of DQN and TRPO
worsens or remains unchanged when we lower the discount
factor (DQN and TRPO do not beneﬁt from lowering γ),
while our κ-PI and κ-VI based algorithms perform better
with lowering the value of κ (note that our algorithms are
reduced to DQN and TRPO for κ = 1).

Remark 3. While we observe better performance for
smaller γ values in some MuJoCo domains, e.g., γ = 0.68
in Ant, lowering γ always results in inferior performance in
the Atari domains. This is due to the fact that a number of
MuJoCo domains, such as Ant, are inherently short-horizon
decision problems, and thus, their performance does not
degrade (even sometimes improves) with lowering the dis-
count factor. On the other hand, the Atari problems are
generally not short-horizon, and thus, their performance
degrades with lowering γ.

7. Conclusion and Future Work

In this paper, we studied the use of multi-step greedy poli-
cies in model-free RL and showed that in most problems,
the algorithms derived from this formulation achieve a bet-
ter performance than their single-step counterparts. We
adopted the κ-greedy formulation of multi-step greedy poli-
cies (Efroni et al., 2018a) and derived four model-free RL
algorithms. The main component of the policy and value
iteration algorithms derived from this formulation, κ-PI and
κ-VI, is solving a surrogate decision problem with a shaped
reward and a smaller discount factor. Our algorithms use
popular deep RL algorithms, DQN and TRPO, to solve the
surrogate decision problem, and thus, we refer to them as κ-
PI-DQN, κ-VI-DQN, κ-PI-TRPO, and κ-VI-TRPO. We em-
pirically evaluated our proposed algorithms and compared
them with DQN, TRPO, and GAE on Atari and MuJoCo
benchmarks. Our experiments show that for a large range
of κ, our algorithms perform better than DQN and TRPO.
Furthermore, we proposed a recipe to allocate the total sam-
ple budget to the evaluation and improvement phases of our
algorithms, and empirically demonstrated the importance
of this allocation. We also showed how GAE can be de-
rived by minor modiﬁcations to κ-PI-TRPO, and thus, is a
κ-greedy RL algorithm. Finally, we showed the advantage
of multi-step greedy formulation over lowering the discount
factor in DQN and TRPO. Our results indicate that while the
performance of DQN and TRPO degrades with lowering the
discount factor, our multi-step greedy algorithms improve
over DQN and TRPO.

An interesting future direction would be to use other multi-
step greedy formulations (Bertsekas & Tsitsiklis, 1996; Bert-
sekas, 2018; Efroni et al., 2018a; Sun et al., 2018; Shani
et al., 2019) to derive model-free RL algorithms. Another
direction is to use multi-step greedy in model-based RL

(e.g., Kumar et al., 2016; Talvitie, 2017; Luo et al., 2018;
Janner et al., 2019) and solve the surrogate decision prob-
lem with an approximate model. We conjecture that in this
case one may set κ – or more generally, the planning hori-
zon – as a function of the quality of the approximate model:
gradually increasing κ as the approximate model gets closer
to the real one. We leave theoretical and empirical study
of this problem for future work. Finally, we believe using
adaptive κ would greatly improve the performance of our
proposed algorithms. We leave verifying this and how κ
should change as a function of errors in gradient and value
estimation for future work.

References

Baird, L. Residual algorithms: Reinforcement learning with
function approximation. In Proceedings of the Twelfth
International Conference on Machine Learning, pp. 30–
37, 1995.

Bellemare, M., Naddaf, Y., Veness, J., and Bowling, M. The
arcade learning environment: An evaluation platform for
general agents. Journal of Artiﬁcial Intelligence Research,
47:253–279, 2013.

Bertsekas, D. Feature-based aggregation and deep reinforce-
ment learning: A survey and some new implementations.
Journal of Automatica Sinica, 6(1):1–31, 2018.

Bertsekas, D. and Tsitsiklis, J. Neuro-dynamic program-

ming, volume 5. 1996.

Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. OpenAI Gym.
Preprint arXiv:1606.01540, 2016.

Efroni, Y., Dalal, G., Scherrer, B., and Mannor, S. Beyond
the one step greedy approach in reinforcement learning.
In Proceedings of the 35th International Conference on
Machine Learning, 2018a.

Efroni, Y., Dalal, G., Scherrer, B., and Mannor, S. Multiple-
step greedy policies in approximate and online reinforce-
ment learning. In Advances in Neural Information Pro-
cessing Systems, pp. 5238–5247, 2018b.

Farahmand, A., Szepesvári, C., and Munos, R. Error prop-
agation for approximate policy and value iteration. In
Advances in Neural Information Processing Systems, pp.
568–576, 2010.

Greensmith, E., Bartlett, P., and Baxter, J. Variance reduc-
tion techniques for gradient estimates in reinforcement
learning. Journal of Machine Learning Research, 5:1471–
1530, 2004.

Multi-step Greedy Reinforcement Learning Algorithms

Scherrer, B. Approximate policy iteration schemes: a com-
parison. In International Conference on Machine Learn-
ing, pp. 1314–1322, 2014.

Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz,
In International
P. Trust-region policy optimization.
conference on machine learning, pp. 1889–1897, 2015.

Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel,
P. High-dimensional continuous control using generalized
advantage estimation. In Proceedings of the International
Conference on Learning Representations, 2016.

Shani, L., Efroni, Y., and Mannor, S. Exploration conscious
reinforcement learning revisited. In International Confer-
ence on Machine Learning, pp. 5680–5689, 2019.

Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou,
I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,
Bolton, A., et al. Mastering the game of Go without
human knowledge. Nature, 550(7676):354, 2017.

Sun, W., Gordon, G., Boots, B., and Bagnell, J. Dual policy
iteration. In Advances in Neural Information Processing
Systems, pp. 7059–7069, 2018.

Sutton, R. and Barto, A. Reinforcement learning: An intro-

duction. 2018.

Talvitie, E. Self-correcting models for model-based rein-
forcement learning. In Thirty-First AAAI Conference on
Artiﬁcial Intelligence, 2017.

Thrun, S. and Schwartz, A. Issues in using function approx-
imation for reinforcement learning. In Proceedings of
the Connectionist Models Summer School, pp. 255–263,
1993.

Todorov, E., Erez, T., and Tassa, Y. MuJoCo: A physics
engine for model-based control. In International Confer-
ence on Intelligent Robots and Systems, pp. 5026–5033,
2012.

Van Hasselt, H., Guez, A., and Silver, D. Deep reinforce-
ment learning with double Q-learning. In AAAI confer-
ence on artiﬁcial intelligence, 2016.

Hill, A., Rafﬁn, A., Ernestus, M., Gleave, A., Kan-
ervisto, A., Traore, R., Dhariwal, P., Hesse, C.,
Klimov, O., Nichol, A., Plappert, M., Radford, A.,
Schulman, J., Sidor, S., and Wu, Y.
Stable base-
lines. GitHub repository https://github.com/
hill-a/stable-baselines, 2018.

Janner, M., Fu, J., Zhang, M., and Levine, S. When to trust
your model: Model-based policy optimization. arXiv
preprint arXiv:1906.08253, 2019.

Jiang, N., Kulesza, A., Singh, S., and Lewis, R. The depen-
dence of effective planning horizon on model accuracy.
In Proceedings of the International Conference on Au-
tonomous Agents and Multiagent Systems, pp. 1181–1189,
2015.

Kumar, V., Todorov, E., and Levine, S. Optimal control
with learned local models: Application to dexterous ma-
nipulation. In International Conference on Robotics and
Automation, pp. 378–383, 2016.

Levine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-end
training of deep visuomotor policies. Journal of Machine
Learning Research, 17(1):1334–1373, 2016.

Lillicrap, T., Hunt, J., Pritzel, A., Heess, N., Erez, T., Tassa,
Y., Silver, D., and Wierstra, D. Continuous control with
deep reinforcement learning. Preprint arXiv:1509.02971,
2015.

Lu, T., Schuurmans, D., and Boutilier, C. Non-delusional
In Proceedings of the
Q-learning and value iteration.
International Conference on Neural Information Process-
ing Systems, pp. 9971–9981, 2018.

Luo, Y., Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma,
T. Algorithmic framework for model-based deep rein-
forcement learning with theoretical guarantees. Preprint
arXiv:1807.03858, 2018.

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
J., Bellemare, M., Graves, A., Riedmiller, M., Fidjeland,
A., Ostrovski, G., et al. Human-level control through deep
reinforcement learning. Nature, 518(7540):529, 2015.

Munos, R. Error bounds for approximate policy iteration. In
Proceedings of the International Conference on Machine
Learning, pp. 560–567, 2003.

Munos, R. Error bounds for approximate value iteration.
In Proceedings of the National Conference on Artiﬁcial
Intelligence, pp. 1006–1011, 2005.

Petrik, M. and Scherrer, B. Biasing approximate dynamic
programming with a lower discount factor. In Advances
in neural information processing systems, pp. 1265–1272,
2009.

Multi-step Greedy Reinforcement Learning Algorithms

Appendix

A. κ-PI-DQN and κ-VI-DQN Algorithms

A.1. Detailed Pseudo-codes

In this section, we report the detailed pseudo-codes of κ-PI-DQN and κ-VI-DQN algorithms, described in Section 4.3,
side-by-side.

θ and Q(cid:48)

φ with weights θ(cid:48) ← θ and φ(cid:48) ← φ;

Algorithm 5 κ-PI-DQN
1: Initialize replay buffer D; Q-networks Qθ and Qφ with random weights θ and φ;
2: Initialize target networks Q(cid:48)
3: for i = 0, . . . , Nκ − 1 do
4:
5:
6:
7:
8:
9:

Select at as an (cid:15)-greedy action w.r.t. Qθ(st, a);
Execute at, observe rt and st+1, and store the tuple (st, at, rt, st+1) in D;
Sample a random mini-batch {(sj, aj, rj, sj+1)}N
Update θ by minimizing the following loss function:

# Policy Improvement
for t = 1, . . . , Tκ do

j=1 from D;

(cid:80)N

LQθ = 1
N
Vφ(sj+1) = Qφ(sj+1, πi−1(sj+1))

j=1

(cid:2)Qθ(sj, aj) − (cid:0)rj(κ, Vφ) + γκ maxaQ(cid:48)

θ(sj+1, a)(cid:1)(cid:3)2
,
πi−1(sj+1) ∈ arg maxa Q(cid:48)

and

where
θ(sj+1, a);

10:
11:
12:
13:
14:
15:
16:
17:
18:

19:
20:
21:
22: end for

end for

Copy θ to θ(cid:48) occasionally

(θ(cid:48) ← θ);

end for
# Policy Evaluation
Set πi(s) ∈ arg maxa Q(cid:48)
for t(cid:48) = 1, . . . , T (κ) do

θ(s, a);

Sample a random mini-batch {(sj, aj, rj, sj+1)}N
Update φ by minimizing the following loss function:

j=1 from D;

LQφ = 1
N
Copy φ to φ(cid:48) occasionally

(cid:2)Qφ(sj, aj) − (rj + γQ(cid:48)
(φ(cid:48) ← φ);

(cid:80)N

j=1

φ(sj+1, πi(sj+1)))(cid:3)2

;

Algorithm 6 κ-VI-DQN
1: Initialize replay buffer D; Q-networks Qθ and Qφ with random weights θ and φ;
2: Initialize target network Q(cid:48)
θ with weights θ(cid:48) ← θ;
3: for i = 0, . . . , Nκ − 1 do
4:
5:
6:
7:
8:
9:

Select at as an (cid:15)-greedy action w.r.t. Qθ(st, a);
Execute at, observe rt and st+1, and store the tuple (st, at, rt, st+1) in D;
Sample a random mini-batch {(sj, aj, rj, sj+1)}N
Update θ by minimizing the following loss function:

# Evaluate TκVφ and the κ-greedy policy w.r.t. Vφ
for t = 1, . . . , Tκ do

j=1 from D;

(cid:80)N

LQθ = 1
j=1
N
Vφ(sj+1) = Qφ(sj+1, π(sj+1))
(θ(cid:48) ← θ);

Copy θ to θ(cid:48) occasionally

(cid:2)Qθ(sj, aj) − (rj(κ, Vφ) + κγ maxaQ(cid:48)
and

θ(sj+1, a))(cid:3)2

,

where

π(sj+1) ∈ arg maxa Qφ(sj+1, a);

10:
11:
12:
13:
14:
15: end for

end for
Copy θ to φ (φ ← θ)

Multi-step Greedy Reinforcement Learning Algorithms

Hyperparameter
Horizon (T)
Adam stepsize
Target network update frequency
Replay memory size
Discount factor
Total training time steps
Minibatch size
Initial exploration
Final exploration
Final exploration frame
#Runs used for plot averages
Conﬁdence interval for plot runs

Value
1000
1 × 10−4
1000
100000
0.99
10000000
32
1
0.1
1000000
10
∼ 95%

Table 3: Hyperparameters for κ-PI-DQN and κ-VI-DQN.

A.2. Ablation Test for CFA

Figure 4: Performance of κ-PI-DQN and κ-VI-DQN on Breakout for different values of CFA.

A.3. κ-PI-DQN and κ-VI-DQN Plots

In this section, we report additional results of the application of κ-PI-DQN and κ-VI-DQN on the Atari domains. A summary
of these results has been reported in Table 1 in the main paper.

Figure 5: Training performance of the ‘naive’ baseline Nκ = T and κ-PI-DQN, κ-VI-DQN for CFA = 0.05 on SpaceInvaders

Multi-step Greedy Reinforcement Learning Algorithms

Figure 6: Training performance of the ‘naive’ baseline Nκ = T and κ-PI-DQN, κ-VI-DQN for CFA = 0.05 on Seaquest

Figure 7: Training performance of the ‘naive’ baseline Nκ = T and κ-PI-DQN, κ-VI-DQN for CFA = 0.05 on Enduro

Figure 8: Training performance of the ‘naive’ baseline Nκ = T and κ-PI-DQN, κ-VI-DQN for CFA = 0.05 on BeamRider

Figure 9: Training performance of the ‘naive’ baseline Nκ = T and κ-PI-DQN, κ-VI-DQN for CFA = 0.05 on Qbert

Multi-step Greedy Reinforcement Learning Algorithms

B. κ-PI-TRPO and κ-VI-TRPO Algorithms

B.1. Detailed Pseudo-codes

In this section, we report the detailed pseudo-codes of the κ-PI-TRPO and κ-VI-TRPO algorithms, described in Section 4.4,
side-by-side.

Algorithm 7 κ-PI-TRPO
1: Initialize V -networks Vθ and Vφ with random weights θ and φ; policy network πψ with random weights ψ;
2: for i = 0, . . . , Nκ − 1 do
for t = 1, . . . , Tκ do
3:
4:
5:
6:
7:
8:

Simulate the current policy πψ for M time-steps;
for j = 1, . . . , M do

end for
Sample a random mini-batch {(sj, aj, rj, sj+1)}N
Update θ by minimizing the loss function: LVθ = 1
N
# Policy Improvement
Sample a random mini-batch {(sj, aj, rj, sj+1)}N
j=1 from the simulated M time-steps;
Update ψ using TRPO with advantage function computed by {(Rj(κ, Vφ), Vθ(sj))}N
j=1;

j=1 from the simulated M time-steps;

Calculate Rj(κ, Vφ) = (cid:80)M

j=1(Vθ(sj) − Rj(κ, Vφ))2;

t=j(γκ)t−jrt(κ, Vφ)

t=j γt−jrt;

ρj = (cid:80)M

(cid:80)N

and

end for
# Policy Evaluation
Sample a random mini-batch {(sj, aj, rj, sj+1)}N
Update φ by minimizing the loss function: LVφ = 1
N

j=1 from the simulated M time-steps;
j=1(Vφ(sj) − ρj)2;

(cid:80)N

9:
10:
11:
12:
13:
14:
15:

16:
17: end for

# Evaluate TκVφ and the κ-greedy policy w.r.t. Vφ
for t = 1, . . . , Tκ do

Algorithm 8 κ-VI-TRPO
1: Initialize V -networks Vθ and Vφ with random weights θ and φ; policy network πψ with random weights ψ;
2: for i = 0, . . . , Nκ − 1 do
3:
4:
5:
6:
7:
8:
9:

Simulate the current policy πψ for M time-steps;
for j = 1, . . . , M do

end for
Sample a random mini-batch {(sj, aj, rj, sj+1)}N
j=1 from the simulated M time-steps;
Update θ by minimizing the loss function: LVθ = 1
j=1(Vθ(sj) − Rj(κ, Vφ))2;
N
Sample a random mini-batch {(sj, aj, rj, sj+1)}N
j=1 from the simulated M time-steps;
Update ψ using TRPO with advantage function computed by {(Rj(κ, Vφ), Vθ(sj))}N
j=1;

Calculate Rj(κ, Vφ) = (cid:80)M

t=j(γκ)t−jrt(κ, Vφ);

(cid:80)N

10:

11:
12:
13:
14:
15: end for

end for
Copy θ to φ (φ ← θ);

Multi-step Greedy Reinforcement Learning Algorithms

Hyperparameter
Horizon (T)
Adam stepsize
Number of samples per Iteration
Entropy coefﬁcient
Discount factor
Number of Iterations
Minibatch size
#Runs used for plot averages
Conﬁdence interval for plot runs

Value
1000
1 × 10−3
1024
0.01
0.99
2000
128
10
∼ 95%

Table 4: Hyper-parameters of κ-PI-TRPO and κ-VI-TRPO on the MuJoCo domains.

B.2. Ablation Test for CF A

Figure 10: Performance of κ-PI-TRPO and κ-VI-TRPO on Walker2d-v2 for different values of CF A.

B.3. κ-PI-TRPO and κ-VI-TRPO Plots

In this section, we report additional results of the application of κ-PI-TRPO and κ-VI-TRPO on the MuJoCo domains. A
summary of these results has been reported in Table 2 in the main paper.

Figure 11: Performance of GAE, ‘Naive’ baseline and κ-PI-TRPO, κ-VI-TRPO on Ant-v2.

Multi-step Greedy Reinforcement Learning Algorithms

Figure 12: Performance of GAE, ‘Naive’ baseline and κ-PI-TRPO, κ-VI-TRPO on HalfCheetah-v2.

Figure 13: Performance of GAE, ‘Naive’ baseline and κ-PI-TRPO, κ-VI-TRPO on HumanoidStandup-v2.

Figure 14: Performance of GAE, ‘Naive’ baseline and κ-PI-TRPO, κ-VI-TRPO on Swimmer-v2.

Figure 15: Performance of GAE, ‘Naive’ baseline and κ-PI-TRPO, κ-VI-TRPO on Hopper-v2.

