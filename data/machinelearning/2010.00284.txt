0
2
0
2

t
c
O
1

]

G
L
.
s
c
[

1
v
4
8
2
0
0
.
0
1
0
2
:
v
i
X
r
a

Bayesian Policy Search for Stochastic Domains

DAVID TOLPIN, Ben-Gurion University of the Negev and PUB+, Israel
YUAN ZHOU, University of Oxford, United Kingdom
HONGSEOK YANG, School of Computing, KAIST, South Korea

1 INTRODUCTION

AI planning can be cast as inference in probabilistic models, and probabilistic programming was
shown to be capable of policy search in partially observable domains. Wingate et al. [12] introduce
policy search through Markov Chain Monte Carlo. However, the work only considers POMDPs with
deterministic actions and fixed initial and final states, as well as deterministic policies. Handling of
non-deterministic POMDPs, uncertain initial and final states, or stochastic policies are not addressed
(and would involve some form of nesting in the models [6] requiring inference schemes beyond
those employed in the paper). Van de Meent et al. [11] adapt black-box variational inference [8] to
policy search with probabilistic programs. The use of black-box variational inference facilitates
handling of stochastic POMDPs, in which observations and state transitions are noisy with known
noise distributions. Still, the handling of stochasticity is tied to a particular inference algorithm
rather than expressed in the model in an algorithm-agnostic manner. In addition, the application of
black-box variational inference is not strictly Bayesian: the policy search is performed by optimizing
parameters of the ‘prior’. As such, the prior just defines the shape of the variational distribution
and the starting point for optimization rather than encodes prior knowledge about policies.

In this work, we cast policy search in stochastic domains as a Bayesian inference problem and
provide a scheme for encoding such problems as nested probabilistic programs. We argue that
probabilistic programs for policy search in stochastic domains should involve nested conditioning [6],
and provide an adaption of Lightweight Metropolis-Hastings [13] (LMH) for robust inference in
such programs. We apply the proposed scheme to stochastic domains introduced in [11] and show
that policies of similar quality are learned, despite a simpler and more general inference algorithm.
We believe that the proposed variant of LMH is both novel and, with additional effort, applicable to
a wider class of probabilistic programs with nested conditioning.

2 PROBABILISTIC PROGRAMS FOR POLICY SEARCH

We consider the problem of inference of a deterministic parametric policy for an agent in a stochastic
partially observable environment. An environment is described by sets of actions A and observations
O. An agent repeatedly performs an action a ∈ A and receives an observation o ∈ O and a reward.
A parametric policy π (θ ) : O 1:t → A is a mapping from the history of observations to an action,
fully determined by its parameter θ . The purpose of an agent is to maximize the sum of rewards
R = (cid:205)∞
t =1 rt . The objective of policy inference is to find θ that maximizes the expected reward. The
interaction between the agent and the environment is often formalized as a partially observable
Markov decision process (POMDP) [2], where the history of observations summarized by the agent
as a belief state, although formalization as POMDP is not necessary for policy inference with
probabilistic programs.

A probabilistic program for policy inference is built around a stochastic simulator S(θ ) → r
which receives policy parameters θ and returns reward r . The simulator is usually designed such
that it always terminates after a bounded number of steps, and the reward is also bounded from
both above and below (a reward can be negative). Since the simulator is stochastic, r |θ is a random
variable. For inference, a prior Dθ is imposed on policy parameters, and the posterior distribution

1

 
 
 
 
 
 
David Tolpin, Yuan Zhou, and Hongseok Yang

of θ is conditioned on an auxiliary Bernoulli variable, with 1 corresponding to the optimal policy,
given the expected reward E[r ]. The conditioning is chosen such that the mode of the posterior
distribution of θ maximizes the expected reward, and thus defines the optimal policy for given S.

A common choice for the conditional distribution is [3]

pc1 (1|r ) = exp(r − Ur ),

where Ur is an upper bound on the reward. This results in the following generative model:

θ ∼ Dθ
1 ∼ Bernoulli(exp(E[S(θ )] − Ur ))

(1)

(2)

Model (2) is a model with nested conditioning [6]. The expectation over outcomes of the stochastic
simulator S cannot be computed exactly, and must be approximated by Monte Carlo samples. For
(2) an unbiased estimate of p(θ ) cannot be obtained [6], however a Monte Carlo estimate of log p(θ )
is unbiased. Therefore, inference methods which involve an unbiased estimate of log p(θ ), such as
stochastic gradient Markov Chain Monte Carlo [4] or stochastic variational inference [1, 8], can be
efficiently applied to (2), but application of simpler and more general methods, which do not require
computing gradients, such as importance sampling or Metropolis-Hastings, faces difficulties of
nested Monte Carlo estimation [7].

An alternative model may use instead

pc2(1|r ) = r − Lr
Ur − Lr

,

where Ur and Lr are an upper and a lower bound on the reward:

θ ∼ Dθ

1 ∼ Bernoulli

(cid:18) E[S(θ )] − Lr
Ur − Lr

(cid:19)

(3)

(4)

The form of nested conditioning in (4) allows flattening of nested inference [6]. Importance sampling
with independent samples of θ and r (the latter through a single run of S) gives an unbiased
Monte Carlo estimate of the posterior distribution of θ . This class of models does not require
nested Monte Carlo estimation or computing derivatives of probability functions for inference,
and thus potentially applies to a broader range of problems. Still, importance sampling is of
limited use in high-dimensional models, and a non-trivial parametric policy is likely to depend
on a high-dimensional parameter vector θ . Metropolis-Hastings, a general Markov Chain Monte
Carlo algorithm that scales better with the dimensionality of the random space, cannot be applied
unmodified. However, an insight about the structure of the posterior distribution leads to a simple
adaptation of Metropolis-Hastings to (4), introduced in the next section.

3 STOCHASTIC LIGHTWEIGHT METROPOLIS-HASTINGS

The conditioning probability of the expected reward in (2) and (4) can be re-written in terms of
conditioning probabilities of individual rewards. Indeed, denoting by pS(r |θ ) the probability that S

2

Bayesian Policy Search for Stochastic Domains

returns r given θ , we get 1
pc1(1|E(r |θ )) = exp(E(r |θ ) − Ur ) = (cid:214)

(exp(r − Ur ))pS (r |θ ) = (cid:214)

pc1(1|r )pS (r |θ ),

for model (2),

pc2(1|E(r |θ )) =

E(r |θ ) − Lr
Ur − Lr

r

pS(r |θ )

(cid:19)

(cid:18) r − Lr
Ur − Lr

= (cid:213)
r

= (cid:213)
r

r

pS(r |θ )pc2(1|r ),

for model (4).

(5)
In words, (2) corresponds to conditioning on conjunction of values of r (all values), and (4) — on
disjunction of values of r (any value), according to their probabilities.

The pc2(1|E(r |θ )), in particular, takes the form of the probability of a mixture with a component
for each r , component weights pS(r |θ ) and conditional probability of a sample given the component
pc2(1|r ). Componentwise Markov chain Monte Carlo sampling from a mixture is straightforwardly
accomplished by updating component assignments (r ) and component parameters (θ ). A difficulty
arises though because, while r can be updated given θ by re-running S, updating θ for given r
would require running S backward. This difficulty can be overcome by noting that stochastic
simulator S(θ ) is a probabilistic routine itself, and as such can be substituted by a deterministic
procedure P(θ, τ ) which takes an additional parameter τ ∼ Dτ — the trace — containing all random
choices taken during the run of the simulator. With this substitution, (4) takes the form

θ ∼ Dθ

1 ∼ Bernoulli

(cid:18) Eτ [P(θ, τ )] − Lr
Ur − Lr

(cid:19)

(6)

Thus, from the calculation similar to the one for the pc2 term in (5), it follows that posterior samples
θ of (6) can be obtained by first sampling pairs (τ , θ ) from the joint distribution

p(τ )p(θ )pc2 (1|P(θ, τ ))
Z

= p(τ ) ×

p(θ )pc2(1|P(θ, τ ))
Zτ

= p(τ ) ×

p(θ )(P(θ, τ ) − Lr )
Zτ (Ur − Lr )

(7)

for some normalising constants Z and Zτ , and then selecting the θ part of each sampled pair. This
observation leads to the following sampling algorithm:

Algorithm 1 Sampling from (6) with stochastic Metropolis-Hastings

1: loop
2:
3:
4:

5:

τ ∼ Dτ {always accept}
θ ′ ∼ Dθ
u ∼ Uniform(0, 1)
if u < min
θ ← θ ′

1, P(θ ′,τ )−Lr
P(θ,τ )−Lr

(cid:16)

6:
7:
8:
9: end loop

end if
Output θ

(cid:17)

then

To prove correctness of Algorithm 1, we consider the joint distribution of τ , θ . To sample from
this distribution with componentwise Metropolis-Hastings, change is proposed to either τ or θ on

1The sum and product are to be replaced by the integral and the type II geometric integral for the continuous case.

3

each step. The proposal is accepted or rejected based on the MH acceptance ratio:

David Tolpin, Yuan Zhou, and Hongseok Yang

paccept (θ ′|θ, τ ) = min

paccept (τ ′|θ, τ ) = min

(cid:18)

(cid:18)

1,

1,

p(θ )p(θ ′)(P(θ ′, τ ) − Lr )
p(θ ′)p(θ )(P(θ, τ ) − Lr )
p(τ )p(τ ′)
p(τ ′)p(τ )

= 1.

(cid:19)

(cid:19)

= min

(cid:18)

1,

P(θ ′, τ ) − Lr
P(θ, τ ) − Lr

(cid:19)

,

(8)

Note that paccept (τ ′|θ, τ ) = 1, which is reflected by “always accept” in the algorithm.

Expected reward need not be estimated for sampling. This allows to re-write (6) as a probabilistic
program without nesting, but with a syntax extension tagging τ as a stochastic choice — a random
variable for which the posterior, rather than the prior distribution is known:
θ ∼ Dθ
∗∼ Dτ

τ

(9)

1 ∼ Bernoulli

(cid:18) P(θ, τ ) − Lr
Ur − Lr

(cid:19)

Correspondingly, in the stochastic variant of lightweight Metropolis-Hastings [13], the only modifi-
∗∼). The
cation required is always accepting an update to a component marked as stochastic choice (
mode of the marginal distribution of θ can be found by simulated annealing, again, applied to θ
but not to τ . We believe that stochastic LMH, while introduced here for policy search in stochastic
domains, is applicable to probabilistic programs with nested conditioning in general, providing a
more scalable alternative to importance sampling. It can also be extended to gradient-based variants
of Markov Chain Monte Carlo when the gradient is available.

4 CASE STUDIES

We re-examine here two of the case studies in [11], Canadian traveller problem and RockSample.
We re-state the probabilistic programs for policy inference according to (9) and perform posterior
inference using the stochastic variant of lightweight Metropolis-Hastings with simulated annealing
to find the optimal policy as the mode of the posterior. Based on the original case studies, we
implemented our versions of the probabilistic programs and the inference algorithm in Anglican [10].
The code and data for the case studies are in repository https://bitbucket.org/dtolpin/stochastic-
conditioning.

Apart from cosmetic changes, two modifications of the probabilistic programs were made:
(1) stochastic choices where tagged to distinguish between them and policy parameters.
(2) the conditioning was changed from (1) to (3).

In both case studies, we report the expected rewards of the inferred policies, for a range of tem-
peratures from 100 to 0.001. We include temperatures greater than 1 to obtain policy distributions
which are close to policy priors, and show that as we decrease the temperature, the expected policy
reward increases and then levels out when the policies are sampled from a close proximity of the
mode. For each temperature, we run the inference for 100 000 iterations, and use 10 000 episodes to
estimate the expected reward of the inferred policies. The numerical results are similar to those
reported in [11].

4.1 Canadian Traveller Problem
In the Canadian traveller problem (CTP) [5], an agent must traverse a graph, in which edges may
be missing at random. The agent knows the length and probability of being open of each edge.

The agent traverses the graph by depth-first search, and the policy determines the order in which
the edges are selected at each node. The reward in CTP is the negative travel distance. The reward

4

Bayesian Policy Search for Stochastic Domains

Fig. 1. Mean travel distances for inferred policies in the Canadian traveller problem. Dashed and dotted lines
are mean travel distances by clairvoyant and random agents.

Fig. 2. Expected rewards for the RockSample POMDP.

is bounded between the negative sum of lengths of all edges in the graph (depth-first search visits
any edge at most once) and the shortest path through the graph. Figure 1 shows the expected travel
distances of inferred policies for a problem instance with 20 nodes and 46 edges, for a range of
edge openness probabilities, compared to the expected travel distances of a random agent and of
the clairvoyant agent (an agent that knows the state of all edges upfront and travels through the
shortest open path).

5

1021011001011021031/T246810mean distancep=0.6p=0.7p=0.8p=0.9p=1.01021011001011021031/T1001020Expected reward4x45x57x810x10David Tolpin, Yuan Zhou, and Hongseok Yang

4.2 RockSample
In the RockSample POMDP [9], a square field N × N with M rocks is given. A rover is initially
located in the middle of the left edge of the square. Each of the rocks can be either good or bad;
and the rover must traverse the field and collect samples of good rocks. The rover can sense the
quality of a rock remotely with an accuracy decreasing with the distance to the rock. The objective
is to cross the field fast, while still sampling as many good rocks as possible. The policy determines
whether to visit or skip a rock based on sensing. The agent gets a reward of 10 for each good rock,
as well as for reaching the right edge. Every step incurs a cost of -1. The reward is not bounded
from below, however even a simple heuristic policy yields a positive reward on the instances used
for evaluation. Due to that, we use softplus-transformed reward log(exp(r ) + 1) for conditioning.
Figure 2 shows the expected rewards of inferred policies for four instances of different sizes and
with different numbers of stones for a range of temperatures.

5 DISCUSSION

We cast policy search in stochastic domains as a Bayesian inference problem and provided a
scheme for encoding such problems as nested probabilistic programs. Bayesian treatment plays
an important role in policy inference; consider, for example, the Canadian traveller problem. We
followed the model of the original case study and imposed a uniform prior on the traversal order.
Instead, we could impose a prior that favors directions which are more likely to lead to the goal,
such as directions of the shortest paths from each node to the goal in the fully unblocked path. [12]
allows to exploit priors for policy search in deterministic domains, and our works extends the fully
Bayesian approach to stochastic domains.

We formalized policy search in stochastic domains as a case of nested conditioning [6]. We
proposed a model in which nested conditioning can be flattened, and non-nested importance
sampling can be used for inference. However, the performance of importance sampling degrades
exponentially with the dimensionality of the probability space, and policies often have many
parameters. For example, in the model for the Canadian traveller problem used here, the number
of policy parameters is proportional to the number of edges in the graph. To facilitate faster
inference, we introduced a stochastic variant of lightweight Metropolis-Hastings [13], suitable for
Bayesian policy search in stochastic domains, but also in models with the basic form of stochastic
conditioning in general. The adaptation of LMH to stochastic domains can be applied to other
MCMC variants, e.g. those exploiting the gradient information where available.

REFERENCES
[1] Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. 2013. Stochastic variational inference. Journal of

Machine Learning Research 14 (2013), 1303–1347.

[2] Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. 1998. Planning and acting in partially observable
stochastic domains. Artificial Intelligence 101, 1 (1998), 99 – 134. https://doi.org/10.1016/S0004-3702(98)00023-X
[3] Hilbert J. Kappen. 2007. An introduction to stochastic control theory, path integrals and reinforcement learning. In

American Institute of Physics Conference Series, Vol. 887. 149–181. https://doi.org/10.1063/1.2709596

[4] Yi-An Ma, Tianqi Chen, and Emily Fox. 2015. A Complete Recipe for Stochastic Gradient MCMC. In Advances in Neural
Information Processing Systems 28, C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (Eds.). Curran
Associates, Inc., 2917–2925. http://papers.nips.cc/paper/5891-a-complete-recipe-for-stochastic-gradient-mcmc.pdf
[5] Christos H. Papadimitriou and Mihalis Yannakakis. 1989. Shortest paths without a map. In Automata, Languages and
Programming, Giorgio Ausiello, Mariangiola Dezani-Ciancaglini, and Simonetta Ronchi Della Rocca (Eds.). Springer
Berlin Heidelberg, Berlin, Heidelberg, 610–620.

[6] Tom Rainforth. 2018. Nesting Probabilistic Programs. arXiv:arXiv:1803.06328
[7] Tom Rainforth, Rob Cornish, Hongseok Yang, Andrew Warrington, and Frank Wood. 2018. On Nesting Monte
Carlo Estimators (Proceedings of Machine Learning Research), Jennifer Dy and Andreas Krause (Eds.), Vol. 80. PMLR,
StockholmsmÃďssan, Stockholm Sweden, 4267–4276.

6

Bayesian Policy Search for Stochastic Domains

[8] Rajesh Ranganath, Sean Gerrish, and David M Blei. 2014. Black Box Variational Inference. In Artificial Intelligence and

Statistics.

[9] Trey Smith and Reid Simmons. 2004. Heuristic Search Value Iteration for POMDPs. In Uncertainty in Artificial

Intelligence. AUAI Press, Arlington, Virginia, United States, 520–527.

[10] David Tolpin, Jan-Willem van de Meent, Hongseok Yang, and Frank Wood. 2016. Design and implementation of
probabilistic programming language Anglican. In Proceedings of the 28th Symposium on the Implementation and
Application of Functional Programming Languages (IFL 2016). ACM, New York, NY, USA, Article 6, 12 pages.

[11] Jan-Willem van de Meent, Brooks Paige, David Tolpin, and Frank Wood. 2016. Black-box policy search with probabilistic
programs. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz,
Spain, May 9-11, 2016. 1195–1204.

[12] David Wingate, Noah D Goodman, Daniel M Roy, Leslie P Kaelbling, and Joshua B Tenenbaum. 2011. Bayesian policy
search with policy priors. In International Joint Conference on Artificial Intelligence. 1565–1570. https://doi.org/10.
5591/978-1-57735-516-8/IJCAI11-263

[13] David Wingate, Andreas Stuhlmüller, and Noah D. Goodman. 2011. Lightweight Implementations of Probabilistic

Programming Languages Via Transformational Compilation. In Proc. of the 14th Artificial Intelligence and Statistics.

7

