2
2
0
2

n
u
J

0
3

]
E
N
.
s
c
[

1
v
6
8
8
4
0
.
7
0
2
2
:
v
i
X
r
a

Bio-inspired Machine Learning:
programmed death and replication

Andrey Grabovskya,b and Vitaly Vanchurinc,d

aBudker Institute of Nuclear Physics, 11, Lavrenteva avenue, 630090, Novosibirsk, Russia
bNovosibirsk State University, 630090, 2, Pirogova street, Novosibirsk, Russia
cNational Center for Biotechnology Information, NIH, Bethesda, Maryland 20894, USA
dDuluth Institute for Advanced Study, Duluth, Minnesota, 55804, USA

E-mail: agrabovsky@gmail.com, vitaly.vanchurin@gmail.com

Abstract. We analyze algorithmic and computational aspects of biological phenomena, such
as replication and programmed death, in the context of machine learning. We use two diﬀer-
ent measures of neuron eﬃciency to develop machine learning algorithms for adding neurons
to the system (i.e. replication algorithm) and removing neurons from the system (i.e. pro-
grammed death algorithm). We argue that the programmed death algorithm can be used
for compression of neural networks and the replication algorithm can be used for improving
performance of the already trained neural networks. We also show that a combined algo-
rithm of programmed death and replication can improve the learning eﬃciency of arbitrary
machine learning systems. The computational advantages of the bio-inspired algorithms are
demonstrated by training feedforward neural networks on the MNIST dataset of handwritten
images.

 
 
 
 
 
 
Contents

1 Introduction

2 Neural networks

3 Statistical analysis

3.1 Covariance matrix
3.2 Eﬃciency of neurons
3.3 Conditional distribution

4 Machine learning algorithms

4.1 Programmed death
4.2 Replication

5 Numerical results

5.1 Programmed death
5.2 Replication

6 Discussion

1 Introduction

1

2

4
4
5
6

7
7
8

9
10
12

14

Artiﬁcial neural networks [1–3] have been successfully used for solving computational prob-
lems in natural language processing, pattern recognition, data analysis etc. In addition to
the empirical results a number of statistical approaches to learning where developed [4–6]
(see also [7] for a recent book on the subject) and some steps were taken towards developing a
fully thermodynamic theory of learning [8]. The thermodynamic theory was recently applied
to model biological systems with evolutionary phenomena viewed as (either fundamental
or emergent) learning algorithms [9, 10]. In particular, the so-called programmed death and
replication (of information processing units, such as cells or individual organisms) were shown
to be of fundamental importance for biological evolution modeled through learning dynam-
ics. More generally, many statistical [8], quantum [11], critical [12] and even gravitational
[13] systems can be modeled using learning dynamics, and perhaps the entire universe may
be viewed as a neural network that is undergoing learning evolution [14]. However, for the
quantum behavior to emerge it is essential that the total number of neurons is not ﬁxed and
could change over time [11]. This implies that in the emergent quantum systems, individual
neurons should be constantly removed and added, similar to biological organisms, where in-
dividual cells are constantly die and replicate. In this paper, we analyze the machine learning
algorithms that are inspired, ﬁrst and foremost, by biology [9] (i.e. the programmed death
and replication), but at the same time have direct connection to physics in general [14] and
to emergent quantum mechanics in particular [11] (i.e. removal and addition of neurons).

Consider an artiﬁcial neural network that is being trained for some training dataset
In this very general setup all of the
using some version of stochastic gradient descent.
neurons process information, but it is not clear which ones are more eﬃcient and which ones
are less eﬃcient. For example, if we are to remove a single neuron what should it be so that

– 1 –

the overall increase of the average loss function would be minimal? In other words, how do
we ﬁnd the least eﬃcient neuron that has the least impact on the overall performance of the
neural network? Can such a neuron be identiﬁed locally (e.g. by analyzing its state, bias and
weights) or do we need to study global properties of the network (e.g. by analyzing non-local
statistical or spectral properties )? If all neurons with low eﬃciency (or least loaded) can be
identiﬁed, then one may be able to develop an algorithm (i.e. programmed death algorithm)
that would be useful, for example, for compression of neural networks. Likewise, if one can
identify the neurons with high eﬃciency (or most loaded), then, perhaps, one can use this
information to develop replication algorithms where additional neurons would be added to
the system to reduce the load on the most eﬃcient neurons. Moreover, the two algorithms
may also be used in conjunction (with programmed death followed by replication) in order
to improve the learning rate of the existing machine learning algorithms. In this paper we
will give answers to all of the above questions by carrying out analytical calculations (a more
general, but only approximate method) and by conducting numerical experiments (a more
special, but exact method).

The paper is organized as follows.

In the following section we discuss the basics of
artiﬁcial neural network and of the learning dynamics.
In Sec. 3 we perform statistical
analysis of the learning dynamics and introduce two diﬀerent deﬁnitions of neuron eﬃciency.
In Sec. 4 we develop, the “programmed death”, the “replication” and the combined, i.e.
programmed death followed by replication, algorithms.
In Sec. 5 we present numerical
results for feedforward neural networks with two hidden layers and diﬀerent architectures.
In Sec. 6 the main results are summarized and discussed.

2 Neural networks

A classical neural network with N neurons can be deﬁned as a septuple (x, ˆP , p∂, ˆw, b, f , H),
where

1. x is a (column) state vector of neurons,

2. ˆP is a boundary projection operator to subspace spanned by input/output neurons,

3. p∂( ˆP x) is a probability distribution which describes the training dataset,

4. ˆw is a weight matrix,

5. b is a (column) bias vector,

6. f (y) is an activation map and

7. H(x, b, ˆw) is a loss function.

The training data are associate only with boundary neurons ˆP x(t) that are updated period-
ically from the probability distribution p∂( ˆP x), but the period depends on the architecture.
For example, for a feedforward architecture, the period could equal to the number of layers
so that, in-between updates of the training data, the signal has time to propagate through-
out the entire network. In contrast to the boundary neurons, evolution of the bulk neurons
depends on the state of all neurons,

(cid:16) ˆI − ˆP

(cid:17)

x(t + 1) =

(cid:16) ˆI − ˆP

(cid:17)

f ( ˆwx(t) + b) ,

(2.1)

– 2 –

where the activation map acts separately on each component, i.e. fi(y) = fi(yi). For the
sake of concreteness, we set activation functions on all boundary neurons to be linear

ˆP f (y) = ˆP y

and on all bulk neurons to be hyperbolic tangent

(cid:16) ˆI − ˆP

(cid:17)

f (y) =

(cid:16) ˆI − ˆP

(cid:17)

tanh (y) .

(2.2)

(2.3)

The main objective of machine learning is to ﬁnd bias vectors b and weight matrices
ˆw which minimize a time (or ensemble) average of some suitably deﬁned loss function. For
example, the boundary loss function is given by

H∂(x, b, ˆw) =

1
2

(x − f ( ˆwx + b))T ˆP (x − f ( ˆwx + b))

(2.4)

where because of the inserted projection operator ˆP the sum is taken over squared error
at only boundary neurons. For example, in a feedforward architecture there is no error in
the input boundary layer and all of the error is on the output boundary layer due to a
mismatch between propagated data and training data. Another example is the bulk loss
function deﬁned as

H(x, b, ˆw) =

1
2

(x − f ( ˆwx + b))T (x − f ( ˆwx + b)) + V (x)

(2.5)

where in addition to the ﬁrst term, which represents a sum of local errors over all neurons,
there may be a second term which represents local objectives.

During learning the trainable variables (i.e. bias vector b and weight matrix ˆw) are

continuously adjusted (or transformed)

bi(t + T ) = bi(t) − γ

∂(cid:104)H(x, b, ˆw)(cid:105)
∂bi

wij(t + T ) = wij(t) − γ

∂(cid:104)H(x, b, ˆw)(cid:105)
∂wij

where the time-averaged quantities are deﬁned as

(cid:104)...(cid:105) ≡ lim
T →∞

1
T

T
(cid:88)

t=1

...

(2.6)

(2.7)

(2.8)

and the time interval T can depend on the mini-batch size and on the number of layers.
Note that in general the ensemble average (for a given training dataset p∂( ˆP x)) and the time
average (for a given time interval T ) need not be the same, but, if the trainable variables
(i.e. weights and biases) change very slowly, the two averages are approximately the same.

In the long run, the learning system settles down in a local minimum of the average loss
function and the learning eﬀectively stops. For certain algorithms the system can still transi-
tion to an even lower minimum of the loss function, but such transitions are usually exponen-
tially suppressed. The main problem is that in a local minimum continuous transformations
(e.g. stochastic gradient decent) cannot be eﬀective and discontinuous transformations must
be performed instead. In Secs. 4 and 5 we shall describe one such transformation by com-
bining two algorithms: programmed death and replication. More generally, the programmed

– 3 –

death and replication transformations (or algorithms) need not be combined and can be used
separately. The programmed death algorithm may be used to compress, either gradually (i.e.
one neuron at a time) or suddenly (i.e. a bunch of neurons at once), the neural network. Such
compression could be relevant, for example, if relatively large computational resources are
available during the training phase, but the resources are limited during predicting phase. In
addition, the replication algorithm may be used for improving the performance of an already
pre-trained neural network. This can be done, for example, by adding new neurons in order
to assist the most eﬃcient neuron.

3 Statistical analysis

In the previous section, we mentioned the discrete machine learning algorithms (programmed
death and replication) that rely on determining the eﬃciency of individual neurons. In this
section we will describe the two deﬁnitions of neuron eﬃciency that will be used in Sec. 4
for developing these algorithms and in Sec. 5 for presenting the results of the numerical
experiments.

3.1 Covariance matrix

To study the statistical eﬃciency of neurons it is convenient to introduce a covariance matrix,

Cij ≡ (cid:104)(xi − (cid:104)xi(cid:105))(xj − (cid:104)xj(cid:105))(cid:105) ≡ (cid:104)∆xi∆xj(cid:105) = (cid:104)xixj(cid:105) − (cid:104)xi(cid:105) (cid:104)xj(cid:105) .

(3.1)

which is a positive deﬁnite symmetric matrix whose eigenvalues λk are real nonnegative
numbers and the corresponding orthonormal eigenvectors v(k) are given by

If the states of neurons are approximately linearly dependent,

ˆCv(k) = λkv(k).

(cid:88)

k

akxk ≈ a0,

(3.2)

(3.3)

then at least one of the eigenvalues must be zero. For example, if λ1 = 0, then by setting

we get

or

v(1)
k = ak

v(1)
k (cid:104)xk(cid:105) =

(cid:88)

k

(cid:42)

(cid:88)

k

(cid:43)

akxk

≈ (cid:104)a0(cid:105) = a0,

(xk − (cid:104)xk(cid:105)) v(1)

k ≈ 0.

(cid:88)

k

in agreement with zero-eigenvalue equation

ˆCv(1) =

(cid:88)

k

ˆCikv(1)

k = (cid:104)∆xi

(cid:88)

k

ak∆xk(cid:105) ≈ 0.

(3.4)

(3.5)

(3.6)

(3.7)

In this limit any one of the neurons can be removed after appropriate adjustment of weights.

– 4 –

3.2 Eﬃciency of neurons

In general, λi (cid:54)= 0 for all i and then one can deﬁne eﬃciency of individual neurons as the
degree of non-linearity or how poorly the output of a given neuron can be approximated by a
linear function of the outputs of all other neurons (for a given training dataset p∂( ˆP x)). The
accuracy of approximation of the state of neuron k is high (and thus the eﬃciency should be

low) if λi is small and

(cid:17)2

(cid:16)

v(i)
k

is large. Therefore we can deﬁne the eﬃciency of neuron k as

E(cid:48)

k = min

i

λi
v(i)
k

(cid:16)

(cid:17)2 ,

(3.8)

where the smallest ratio is obtained by looking at all eigenvalues and the corresponding
eigenvectors. Then we use the i-th equation (3.6) to remove the k’th neuron.

xk =

(cid:80)

j v(i)

j xj − (cid:80)
v(i)
k

j(cid:54)=k v(i)

j xj

≈

(cid:104)(cid:80)

j v(i)

j xj(cid:105) − (cid:80)
v(i)
k

j(cid:54)=k v(i)

j xj

By doing so we make the following mistake in xk

(cid:80)

j v(i)
j ∆xj
v(i)
k

= O(

(cid:113)

E(cid:48)

k),

.

(3.9)

(3.10)

since

ˆCijv(m)

i v(n)

j =

(cid:42)
(

(cid:88)

i

v(m)
i ∆xi)(

(cid:88)

j

(cid:43)

v(n)
j ∆xj)

= λnδmn.

(3.11)

Unfortunately, such algorithm may not be very useful in practice since it involves calculation
of the covariance matrix — a computational task which scales as O(N 2). What we really
want is to be able to identify an approximate linear dependence of neurons, but with an
algorithm whose complexity would scale as O(N ).

Consider a linear expansion of activation function

where

xi = fi (yi) + f (cid:48)

i (yi)

(cid:88)

k

wik (xk − (cid:104)xk(cid:105)) + ...

yi ≡

(cid:88)

k

wik(cid:104)xk(cid:105) + bi.

At the zeroth order (cid:104)xi(cid:105) ≈ fi (yi) and at the ﬁrst order

xi − (cid:104)xi(cid:105) ≈ f (cid:48)

i (yi)

(cid:88)

k

wik (xk − (cid:104)xk(cid:105)) .

If the direct impact of neuron k on neuron i is small, then

Cii (cid:29) f (cid:48)

i (yi)2 w2

ikCkk

or

1 (cid:29) f (cid:48)

i (yi)2 w2

ikCkkC−1
ii

.

– 5 –

(3.12)

(3.13)

(3.14)

(3.15)

(3.16)

In this limit all possible variations of the signal

xk ≈ (cid:104)xk(cid:105)

(3.17)

do not signiﬁcantly modify the expected signal xi, and (if our task is to implement a pro-
grammed death algorithm) then we must identify the least eﬃcient neuron by summing over
all impacts that a given neuron k has on all other neurons i, i.e.

Ek ≡ Ckk

(cid:88)

i

i (yi)2 w2
f (cid:48)

ikC−1
ii

.

(3.18)

If Ek (cid:28) 1, then we can drop all of the connection from neuron k to neuron i, or the neuron k
can be removed without sacriﬁcing much of the neural network performance. More precisely,
we can set wik = 0 for all i and then use (3.17) to readjust biases bi’s of all other neurons
so that the input to i-th neuron remains approximately the same. See Eq. (4.3) with k = 1.
Note that all of Cjj’s and all of f (cid:48)
j (yj)’s can be calculated locally by analyzing statistics of
the signals for each of the neurons separately — a computational task which scales as O(N ).

3.3 Conditional distribution

In the limit opposite to (3.15) or (3.16), the impact of neuron k on neuron i is large,

Cii (cid:28) f (cid:48)

i (yi)2 w2

ikCkk

or

1 (cid:28) f (cid:48)

i (yi)2 w2

ikCkkC−1
ii

,

(3.19)

(3.20)

and variations of signal xk can signiﬁcantly modify the signal xi. However, since variations
of f (cid:48)
i (yi) wik (xk − (cid:104)xk(cid:105)) must remain much larger than variations of (xi − (cid:104)xi(cid:105)), variations of
all other incoming signals f (cid:48)
i (yi) wij (xj − (cid:104)xj(cid:105)) (where j (cid:54)= k) must be anti-correlated with
f (cid:48)
i (yi) wik (xk − (cid:104)xk(cid:105)). More precisely, all of the incoming signals must be approximately
linearly dependent and then the output xk can be approximated as a linear function of the
outputs of all other neurons

xk ≈

(cid:88)

j

wij
wik

(cid:104)xj(cid:105) −

(cid:88)

j(cid:54)=k

wij
wik

xj +

xi − (cid:104)xi(cid:105)
f (cid:48)
i (yi) wik

.

(3.21)

Then the conditional distribution for variable xk (with all other xj’s for j (cid:54)= k ﬁxed) can be
modeled as a Gaussian

p(xk) ∝ exp

−

(cid:32)

(cid:33)

1
2

(µi − xk)2
σ2
i

with mean

and variance

(cid:88)

µi ≡

j

(cid:16)

σ2
i =

wij
wik

(cid:104)xj(cid:105) −

(cid:88)

j(cid:54)=k

wij
wik

xj

i (yi)2 w2
f (cid:48)

ikC−1
ii

(cid:17)−1

.

(3.22)

(3.23)

(3.24)

To improve the estimate further, we can average over all such approximations for i (cid:54)= k

and then the overall distribution is proportional to a product of Gaussians



p(xk) ∝ exp

−

1
2

(cid:88)

i(cid:54)=k

(µi − xk)2
σ2
i



(cid:32)

 ∝ exp

−

(cid:33)

1
2

(Mk − xk)2
S2
k

(3.25)

– 6 –

with mean

Mk =

(cid:80)

i(cid:54)=k µiσ−2
i
(cid:80)
i(cid:54)=k σ−2
i

= S2
k

(cid:88)

i(cid:54)=k

i (yi)2 wikC−1
f (cid:48)
ii





(cid:88)

j

wij(cid:104)xj(cid:105) −



wijxj



(cid:88)

j(cid:54)=k

and variance





−1



S2

k =

(cid:88)



σ−2
i



i(cid:54)=k

=

(cid:88)



i(cid:54)=k



−1

i (yi)2 w2
f (cid:48)

ikC−1
ii



.

Note that the neurons eﬃciency (3.18) is inversely proportional to the variance

Ek =

Ckk
S2
k

(3.26)

(3.27)

(3.28)

and so the variance is large for neurons with smaller eﬃciencies and vise versa.

4 Machine learning algorithms

In this section we develop the three machine algorithms: programmed death, replication
and combined (i.e. programmed death followed by replication), using the two measures of
eﬃciencies of individual neurons, or just eﬃciencies, that were introduce in the previous
section. Without loss of generality, we assume that the least eﬃcient neuron is the k = 1
neuron where the eﬃciency is deﬁned either by Eq. (3.8) (and then the linear dependence of
Eq. (3.6) should be used) or by Eq.(3.18) (and then the linear dependence of Eqs. (3.17) or
(3.26) should be used). For the numerical experiments of Sec. 5 we shall refer to

A1. “Connection cut” algorithm — method of Eqs. (3.17) and (3.18)

A2. “Probability” algorithm — Eqs. (3.26) and (3.18).

A3. “Covariance” algorithm — method of Eqs. (3.6) and (3.8)

However, once the least eﬃcient k = 1 neuron is identiﬁed (using either (3.8) or (3.18)) and
once an approximate linear relation is established (using either (3.6), (3.17) or (3.26))

(cid:88)

j

ajxj ≈ a0

(4.1)

all three algorithms (i.e. covariance, connection cut and probability) are treated similarly.
Also note, that in a feedforward architecture the approximate linear dependence (4.1) can
only be established between neurons j on the same layer with the least eﬃcient neuron k = 1.

4.1 Programmed death

In the programmed death algorithm the least eﬃcient neuron is removed from the neural
network and the rest of the network is re-wired to accommodate the changes. It is important
to emphasize that the removed neuron must be in the bulk (i.e. in the hidden layers) so that
no training data are associated with it directly.

– 7 –

The activation dynamics of neuron xi = fi(yi) is determined by the state of all other

neurons xj only through a linear function

yi =

(cid:88)

j

wijxj + bi

which can be approximated using (4.1) as

yi =

≈

=

(cid:88)

j(cid:54)=1

(cid:88)

j(cid:54)=1

(cid:88)

j(cid:54)=1

wijxj + wi1x1 + bi

wijxj + wi1





a0
a1

(cid:18)

wij − wi1

(cid:19)

aj
a1



xl

 + bi

al
a1

(cid:88)

−

l(cid:54)=1
(cid:18)

xj +

bi + wi1

(cid:19)

.

a0
a1

(4.2)

(4.3)

If we are to disconnect a neuron from the network with a minimal modiﬁcation to the
states of other neurons, then we have to readjust the biases and weights so that yi’s remain
approximately the same. This can be done using the following discrete transformation of the
weight matrix

and of the bias vector

w(cid:48)

ij =

(cid:40)
0
wij − wi1

aj
a1

i = 1 or j = 1

if
otherwise

b(cid:48)
i =

(cid:40)
0
bi + wi1

a0
a1

if
i = 1
otherwise.

(4.4)

(4.5)

In other words, the transformation sets all of the signals to and from the “dead” neuron to
zero and readjusts all other weights and biases so that equations (4.3) are approximately
satisﬁed for i (cid:54)= 1. In this way, it is ensured that the performance of the neural network
is not signiﬁcantly altered or that the value of the average loss function, for a given set of
training data, is not signiﬁcantly changed.

The programmed death algorithm is equivalent to a biological phenomenon known as
the programmed death, e.g. programmed cell death. From a more practical point of view,
the algorithm can be used for compression of neural networks for further use, for example,
on devices with constrained computational resources.

4.2 Replication

In the replication algorithm the learning system adds a new neuron to the neural network
in order to reduce the load on the most eﬃcient neuron, not necessarily immediately, but in
the long run. Once again, the eﬃciency of individual neurons can be deﬁne by either (3.8)
or (3.18), but now the main challenge is to introduce coupling between the two neurons: the
new (or “child”) neuron “c” and the most eﬃcient (or “parent”) neuron “p”. By following
the biological analogy of the phenomenon of cell replication, we shall only couple the neurons
at the time of the replication. This must correspond to some clever reinitialization of biases

– 8 –

and weights to and from the child and parent neurons that in the long run would lead to the
largest decrease of the average loss function.

For example, consider reinitialization described by a discrete transformation of the

weight matrix

w(cid:48)

ij =

and of the bias vector






wpj
1
2 wip
wij

i = c
j = c or j = p

if
if
otherwise

b(cid:48)
i =

(cid:40)

bp
bi

i = c
if
otherwise.

(4.6)

(4.7)

This transformation ﬁrst splits in half all of the outgoing weights from the parent neuron “p”
and then copies all of the outgoing weights from the parent “p” to child neuron “c”. As a
result, the overall performance of the neural network is not altered and we end up with two
identical (or replicated) neurons that are, however, linearly dependent

xc = xp.

(4.8)

This means that if the programmed death procedure were to be executed right after repli-
cation, then it would immediately identify and delete one of these neurons. Moreover, if
the two neurons carry exactly the same information, the positive eﬀect of the replication on
learning would be only marginal.

To avoid the problem of immediate removal of a newly replicated neuron and to improve
the learning eﬃciency we can set the outgoing weights from the child and parent neurons to
be arbitrary given that

w(cid:48)

ip + w(cid:48)

ic = wip.

(4.9)

For example, we can split the outgoing signals between the child and parent neurons by
deﬁning a discrete transformation of the weight matrix

w(cid:48)
w(cid:48)

ip = χiwip
ic = (1 − χi)wip

(4.10)

(4.11)

where χi ∈ {0, 1} is a random bit. Note, that the randomization procedure can be improved
further by employing continuous probability distributions P (χi) which may or may not be
symmetric.

The replication algorithm can be used for increasing the eﬀective dimensionality of the
space of trainable variables in the regions where the learning resources are most needed.
This may be important for initialization of the neural networks as well as for improving
the performance of already trained neural networks. More generally, as we shall see below,
the replication algorithm can be used in conjunction with programmed death algorithm for
improving the convergence of arbitrary learning systems.

5 Numerical results

In the previous section we developed two bio-inspired machine learning algorithms (i.e. pro-
grammed death, replication) and suggested that the programmed death followed by replica-
tion can be used for increasing the learning eﬃciency of arbitrary algorithms. In this section

– 9 –

we will analyze the performance of these algorithms by training a feedforward neural network
with four layers (i.e. two hidden layers) and diﬀerent neural architectures:

N1. 784 neurons → 5 neurons → 20 neurons → 10 neurons

N2. 784 neurons → 10 neurons → 10 neurons → 10 neurons

N3. 784 neurons → 20 neurons → 5 neurons → 10 neurons

N4. 784 neurons → 100 neurons → 5 neurons → 10 neurons

The neural networks were trained on the MNIST [15] dataset of 60000 handwritten images,
p∂( ˆP x); with linear activation function, f (y) = y, for boundary neurons (i.e. ﬁrst and fourth
layers); with non-linear activation function, f (y) = tanh(y), for bulk neurons (i.e. second and
third layers); and with cross-entropy loss function, H(x, b, ˆw). The training was done via the
stochastic gradient descent method with the batch size 600, momentum 0, L2 regularization
parameter 0.001, and the constant learning rate 0.001. For plotting the numerical results we
use the neuron eﬃciency calculated according to Eq. (3.8) for the “covariance” algorithm, A3,
and according to Eq. (3.18) for the “connection cut” algorithm, A1, and the “probability”
algorithm, A2 (see Sec. 4 for details).

5.1 Programmed death

For numerical testing of the programmed death algorithms, described in the previous section,
we trained the feedforward neural networks N1, N2 and N3 for 50000 epochs.

On Fig. 1 we plot the eﬃciency of neuron, Ek or E(cid:48)

k, vs. change in the average loss
function, ∆(cid:104)H(cid:105), (i.e. loss after neuron is removed minus loss before neuron is removed) for
three diﬀerent neural networks (N1, N2 and N3) and three diﬀerent algorithms (A1, A2 and
A3) at epochs 500 and 50000. For the individual runs, eﬃciency of every neuron on the
second layer is calculated, then each neuron is removed and the change in the loss function
is calculated. Statistics is acquired by running ﬁfty simulations with diﬀerent initialisation
for every algorithm and neural architecture. In the third row (or for the neural network N3)
and for ”connection cut” and ”probability” plots (or for A1 and A3 algorithms) the log-log
plot is used to show that there are many neurons with eﬃciency smaller than ≤ 10−3 at
epoch 500, but only one such neuron at epoch 50000. In programmed death algorithms, such
low-eﬃciency neurons can be removed without signiﬁcantly changing the performance of the
neural network.

On Fig. 2 we plot change in the average loss function, ∆(cid:104)H(cid:105), as a function of time (or
the number of epochs) for three diﬀerent algorithms (A1, A2 and A3) and for three diﬀerent
neural architectures (N1, N2 and N3). For each run of the simulation every neuron on the
second layer is removed and then ∆(cid:104)H(cid:105) is calculated. For N1 neural architecture we executed
50 separate runs of the simulation, for N2 neural architecture — 25 runs, and for N3 neural
architecture — 13 runs all with diﬀerent initial conditions. The solid lines show the mean
∆(cid:104)H(cid:105) averaged over all neurons on the second layer and all runs, and the dashed lines show
the mean ∆(cid:104)H(cid:105) for only the least eﬃcient neuron on the second layer in each run averaged
over all runs. The least eﬃcient neuron is the one with the smallest eﬃciency, i.e. smallest
E(cid:48)
k for the A1 and smallest Ek for A2 or A3 algorithms. Clearly, only when the dotted line
is much lower than the sold line (of the same color) the removal of the least eﬃcient neuron
would lead to the smallest distortion to the overall performance of the network. With this

– 10 –

Figure 1. Eﬃciency of neuron, Ek or E(cid:48)
change in the average loss function, ∆(cid:104)H(cid:105), for
algorithms A1 (ﬁrst row), A2 (second row) and A3 (third row), and for neural architectures N1 (blue
dots), N2 (pink dots) and N3 (brown dots) after 500 (left plots) and 50000 (right plots) epochs.

k, vs.

respect only for the neural networks N2 and N3, and only for the algorithm A2 (or pink lines)
the corresponding probability method is not very useful.

Next, we make linear ﬁts of Ek vs. ∆(cid:104)H(cid:105) (for algorithms A1 and A2),

or E(cid:48)

k vs. ∆(cid:104)H(cid:105) (for algorithm A3),

Ek = a + b ∆(cid:104)H(cid:105),

E(cid:48)

k = a + b ∆(cid:104)H(cid:105),

– 11 –

(5.1)

(5.2)

0.20.40.60.81.01.2024681012ChangeinlossΔ<H>EfficiencyEk(Ek')Epoch500,NeuralnetworkN10.51.01.5024681012ChangeinlossΔ<H>EfficiencyEk(Ek')Epoch50000,NeuralnetworkN10.00.10.20.30.401234ChangeinlossΔ<H>EfficiencyEk(Ek')Epoch500,NeuralnetworkN20.20.40.60.801234567ChangeinlossΔ<H>EfficiencyEk(Ek')Epoch50000,NeuralnetworkN210-710-50.0010.10010-610-510-40.0010.0100.1001ChangeinlossΔ<H>EfficiencyEk(Ek')Epoch500,NeuralnetworkN3Connectioncut,A1Probability,A2Covariance,A310-610-510-40.0010.0100.100110-610-40.011ChangeinlossΔ<H>EfficiencyEk(Ek')Epoch50000,NeuralnetworkN3Figure 2. ∆(cid:104)H(cid:105) as a function of learning time for algorithms A1 (blue lines), A2 (pink lines) and
A3 (brown lines) and for neural networks N1 (left plot), N2 (middle plot) and N3 (right plot).

Figure 3. Slopes b(t) of the linear ﬁts (of Ek (or E(cid:48)

k) vs. ∆(cid:104)H(cid:105)) as a function of time t.

for all times. On Fig. 3 we plot the slopes b(t) as a function of learning time, where the data
are obtained from the same runs as for Fig. 2. Note that for algorithm A2 (second plot)
and neural network N3 (green line) the slopes are almost zero (or slightly negative) and so
the probability method is not very useful for identifying and removing a neuron which would
give the smallest change in loss. In fact, according to Fig. 2, one should remove the least
eﬃcient neuron using algorithm A1 or A2 for neural network N1 and using algorithm A1 or
A3 for neural networks N2 and N3. Moreover, according to Fig. 3, the algorithms A1 and
A3 should work for all neural networks since the blue and yellow lines remain positive, which
allows us to predict the eﬀect of removing a given neuron, but the algorithm A2 would not
work for neural network N3 since the green line on the rightmost plot remains near zero.

5.2 Replication

For numerical testing of the replication algorithms, described in the previous section, we
trained the feedforward neural networks N1, N2 and N3 for 25000 epochs. Then we use
one of the algorithms to add one more neuron and run the simulation for 10, 100, and 1000
additional epochs.

On Fig. 4 we plot PDF (or probability distribution function) of ∆(cid:104)H(cid:105), acquired from
ﬁfty runs with diﬀerent initialisation, for diﬀerent neural networks and for diﬀerent numbers
of additional epochs. We use the β distribution to model probability distribution P (χi) for
splitting weights between the parent and child neurons in Eqs. (4.10) and (4.11). The ﬁve
lines on each plot describe ﬁve diﬀerent algorithms:

B1. “Beta(0.01, 0.01)” — neuron is replicated with Beta distribution and both shape pa-

rameters equal to 0.01 (blue lines on Fig. 4),

– 12 –

010000200003000040000500000.00.20.40.60.8EpochChangeinlossΔ<H>NeuralnetworkN1Connectioncut,A1meanConnectioncut,A1best010000200003000040000500000.000.050.100.150.200.25EpochChangeinlossΔ<H>NeuralnetworkN2Probability,A2meanProbability,A2best010000200003000040000500000.00.10.20.30.4EpochChangeinlossΔ<H>NeuralnetworkN3Covariance,A3meanCovariance,A3bestNeuralnetworkN1NeuralnetworkN2NeuralnetworkN30100002000030000400005000005101520EpochEfficiencyslopebConnectioncut,A10100002000030000400005000002468EpochEfficiencyslopebProbability,A2010000200003000040000500000123456EpochEfficiencyslopebCovariance,A3Figure 4. PDF of ∆(cid:104)H(cid:105) for neural networks N1 (ﬁrst row), N2 (second row) and N3 (third row)
after 10 (ﬁrst column), 100 (second column), and 1000 (third column) additional epochs.

B2. “Beta(1, 1)” — neuron is replicated with Beta distribution and both shape parameters

equal to 1 (yellow lines on Fig. 4),

B3. “Beta(100, 100)”— neuron is replicated with Beta distribution and both shape param-

eters equal to 100 (green lines on Fig. 4),

B4. “Random”– neuron with random weights (drawn from the same distribution as other

weights) is added (purple line),

B5. “Original”— no new neurons are added (red lines on Fig. 4).

k or E(cid:48)

For algorithms B1, B2, and B3 the most eﬃcient neuron (smallest E(cid:48)
k) on the second
layer was replicated. One can see on Fig. 4 that after 1000 additional epochs the most eﬃcient
algorithm is B1 (i. e. outgoing connections from a parent neuron are randomly split between
parent and child neurons) and the least eﬃcient algorithm is B3 (i. e. new outgoing weights
from parent and child neuron equal to half of old outgoing weight from parent neuron). The
main reason is that algorithm B1 creates a child neuron which is maximally independent
from parent neuron while for algorithm B3 the parent and child neurons are equivalent and
it would take time for them to diverge due to stochastic learning dynamics. B4 algorithm
is ineﬃcient in the short run (i.e. after 10 or 100 epochs), but the algorithm becomes as
eﬃcient as B1 in a long run (i.e. after 1000 epochs).

To demonstrate the computational advantage of a combined algorithm, i.e. programmed
death followed by replication, we used algorithm A1 (for neuron removal) and B1 (for neuron
addition) with neural architecture N4. The main reason for using N4 (as opposed to N1, N2
or N3) is that one needs a large number of neurons on the second layer for the eﬀect to be

– 13 –

Beta(0.01,0.01)Beta(1,1)Beta(100,100)OriginalRandom0.000.010.020.030.0410501005001000Δ<H>PDFNeuralnetworkN1,after10epochs-0.010-0.0050.0000.0050.0100.01510501005001000Δ<H>PDFNeuralnetworkN1,after100epochs-0.025-0.020-0.015-0.010-0.0050.00010501005001000Δ<H>PDFNeuralnetworkN1,after1000epochs0.0000.0050.0100.0150.0200.0250.0300.03510501005001000Δ<H>PDFNeuralnetworkN2,after10epochs-0.0020.0000.0020.0040.0060.0080.0100.01210501005001000Δ<H>PDFNeuralnetworkN2,after100epochs-0.006-0.005-0.004-0.003-0.002-0.0010.00010501005001000Δ<H>PDFNeuralnetworkN2,after1000epochs0.0000.0050.0100.0150.0200.02510501005001000Δ<H>PDFNeuralnetworkN3,after10epochs0.0000.0020.0040.0060.0080.01010501005001000Δ<H>PDFNeuralnetworkN3,after100epochs-0.0030-0.0025-0.0020-0.0015-0.0010-0.00050.00000.000510501005001000Δ<H>PDFNeuralnetworkN3,after1000epochsmost noticeable. We ﬁrst run the simulation for ∆t epochs, calculate eﬃciencies (3.18) of
all neurons on the second layer and use algorithm A1 to remove all neurons (but not more
than N/2) whose eﬃciencies are less than a cutoﬀ (cid:15). If n neurons were removed, then we use
algorithm B1 to replicates n most eﬃcient neurons and continue the simulation for another
∆t epochs and then execute the combined A1-B1 algorithm again, etc.

On Fig. 5 we plot the average loss function for the combined A1-B1 algorithm for

Figure 5. Log-linear plot of average loss function (minus asymptotic loss function) vs. time for a
combined A1-B1 algorithm with cutoﬀ (cid:15) = 0.005 and ∆t = 500 epochs (yellow line with error bands)
and for original algorithm (cid:15) = 0 (blue line with error bands) averaged over 28 runs with diﬀerent
initialization.

5000 training epochs with (cid:15) = 0.005 and ∆t = 500 epochs, and the original algorithm (or
more precisely with (cid:15) = 0 and ∆t = 500). The parameters (cid:15) = 0.005 and ∆t = 500 were
chosen so that about 30% of neurons would be aﬀected by the algorithm after ∆t = 500
epochs, but this fraction goes to 15% after 2∆t = 1000 epochs, etc. The computational
Indeed, the inactive
advantage of the combined A1-B1 algorithm is easy to understand.
neurons are constantly removed by the programmed death algorithm A1 and become active
by the replication algorithm B1 which improves the learning eﬃciency for a carefully chosen
cutoﬀ threshold.

6 Discussion

In this article, we took a small step towards developing machine learning algorithms inspired
by the well-known biological phenomena. In particular, we analysed the computational ad-
vantage of the programmed death and replication, since they represent the most essential
phenomena not only in biology [9], but also in physics [11]. Indeed, in both biological and

– 14 –

ϵ=0ϵ=0.005100020003000400050000.020.050.100.200.50EpochLossfunction<H>-0.08physical systems, the fundamental information processing units can either be added to the
system or removed from the system, which gives rise to the biological phenomena of repli-
cation and programmed death [9] and to the physical phenomena of emergent quantumness
[11].

The developed programmed death and replication algorithms may have a wide range
of applications in machine learning. For example, the programmed death algorithm may be
useful for compression of neural networks for the use on devices with limited computational
resources. In contrast, the replication algorithm may be useful for improving the performance
of already trained neural networks on the devices where additional computational resources
are available. We have also shown that a combination of programmed death and replication
algorithms (i.e. reconnecting the least eﬃcient neuron to reduce the load on the most eﬃcient
neuron) may be useful for improving the learning eﬃciency of an arbitrary machine learning
system.

More generally, when the machine learning system is stuck in a local minimum of the
average loss function, the continuous transformations (e.g. stochastic gradient decent) cannot
be eﬃcient and a discrete transformation must be performed instead. With this respect the
programmed death followed by replication is an example of such transformation. Indeed, the
rewiring of connections from the least eﬃcient (i.e. programmed death) to assist the most
eﬃcient neurons (i.e. replication) is a discrete transformation that may turn out to be useful
for certain machine learning tasks.

Although our analytical results are robust, the numerical results are only preliminary
and a lot more numerical testing is needed in order to conﬁrm the computational advantages
of the proposed algorithms. Moreover, so far we have analyzed the discrete transformations
that correspond to only two biological phenomena, programmed death and replication, and
there are many other important biological [9] and physical [14] phenomena that can be
analyzed in a similar manner which we leave for future work.

Acknowledgments. V.V. was supported in part by the Foundational Questions Institute

(FQXi) and the Oak Ridge Institute for Science and Education (ORISE).

References

[1] A.I. Galushkin, “Neural Networks Theory,” Springer, 396 p., (2007)

[2] J. Schmidhuber, “Deep Learning in Neural Networks: An Overview,” Neural Networks. 61:

85-117. (2015)

[3] Haykin, Simon S. “Neural Networks: A Comprehensive Foundation,” Prentice Hall. (1999)

[4] Vapnik, Vladimir N, “The Nature of Statistical Learning Theory,” Information Science and

Statistics (2000)

[5] J. J. Hopﬁeld, ”Neural networks and physical systems with emergent collective computational

abilities”, PNAS 79(8) pp. 2554-2558, 1982

[6] R. Shwartz-Ziv, N. Tishby, “Opening the black box of deep neural networks via information,”

arXiv:1703.00810 [cs.LG], (2017)

[7] Roberts, D., Yaida, S., Hanin, B. “The Principles of Deep Learning Theory: An Eﬀective
Theory Approach to Understanding Neural Networks,” Cambridge: Cambridge University
Press. (2022)

[8] V. Vanchurin, “Toward a theory of machine learning,” Mach. Learn.: Sci. Technol. 2 035012

(2021)

– 15 –

[9] V. Vanchurin, Y. I. Wolf, M. O. Katsnelson, E. V. Koonin, “Towards a theory of evolution as

multilevel learning,” Proc. Natl. Acad. Sci. U.S.A. 119 (2022)

[10] V. Vanchurin, Y. I. Wolf, E. V. Koonin, M. O. Katsnelson, “Thermodynamics of evolution and

the origin of life,” Proc. Natl. Acad. Sci. U.S.A. 119 (2022)

[11] Mikhail I. Katsnelson, Vitaly Vanchurin, ”Emergent Quantumness in Neural Networks”,

Foundations of Physics 51 (5):1-20 (2021),

[12] M.I. Katsnelson, V. Vanchurin, T. Westerhout, “Self-organized criticality in Neural Networks,”

arXiv:2107.03402

[13] V. Vanchurin, “Towards a Theory of Quantum Gravity from Neural Networks,” Entropy, 24, 7

(2022)

[14] V. Vanchurin, “The world as a neural network,” Entropy, 22, 1210 (2020)

[15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner, “Gradient-based learning applied to

document recognition,” Proceedings of the IEEE, 86(11):2278-2324, 1998.

– 16 –

