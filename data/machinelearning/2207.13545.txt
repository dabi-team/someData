2
2
0
2

l
u
J

7
2

]

G
L
.
s
c
[

1
v
5
4
5
3
1
.
7
0
2
2
:
v
i
X
r
a

Learned Label Aggregation for Weak Supervision

Renzhi Wu
Georgia Tech
renzhiwu@gatech.edu

Shen-En Chen
Georgia Tech
achen353@gatech.edu

Xu Chu
Georgia Tech
xu.chu@cc.gatech.edu

Abstract

The lack of labeled training data is the bottleneck of machine learning in many
applications. To resolve the bottleneck, one promising direction is the data pro-
gramming approach that aggregates different sources of weak supervision signals
to generate labeled data easily. Data programming encodes each weak supervision
source with a labeling function (LF), a user-provided program that predicts noisy
labels. The quality of the generated labels depends on a label aggregation model
that aggregates all noisy labels from all LFs to infer the ground-truth labels.
Existing label aggregation methods typically rely on various assumptions and are
not robust across datasets, as we will show empirically. We for the ﬁrst time provide
an analytical label aggregation method that makes minimum assumption and is
optimal in minimizing a certain form of the averaged prediction error. Since the
complexity of the analytical form is exponential, we train a model that learns to
be the analytical method. Once trained, the model can be used for any unseen
datasets and the model predicts the ground-truth labels for each dataset in a single
forward pass in linear time. We show the model can be trained using synthetically
generated data and design an effective architecture for the model. On 14 real-world
datasets, our model signiﬁcantly outperforms the best existing methods in both
accuracy (by 3.5 points on average) and efﬁciency (by six times on average).

1

Introduction

The lack of labeled training data is a major challenge impeding the practical application of machine
learning (especially deep learning) techniques. Traditionally, labeled data is obtained with human
annotators manually annotating each data point. This process is expensive and does not scale to
labeling large amounts of unlabeled data points. Therefore, practitioners have been increasingly
turned to weak supervision in which large amounts of cheaply generated noisy labels are used.
There are many forms of weak supervision sources, e.g. external knowledge bases [42], existing
pre-trained models [15], and heuristics/rules [53]. To unify different sources, the data programming
paradigm [48, 46] was proposed. In data programming, the user expresses each available weak and
noisy supervision signal from different sources with a labeling function (LF), a small program that
takes in a data point and outputs a noisy label. After that, each LF is applied to unlabeled data
of arbitrary size to obtain a noisy label vector; then, a label aggregation model (also referred as
label model in literature) is used to aggregate all noisy label vectors to infer the unknown ground-
truth labels. The inferred labels can then be used to train any downstream end models, just like
the manually provided labels. The data programming paradigm has been successful in various
tasks [60, 21, 38, 37, 22, 51] and industry scenarios [40, 8, 20].

The core challenge in data programming is how to aggregate all noisy label vectors to infer the
ground-truth labels. The existing approaches typically rely on various assumptions that may not hold
in practice. For example, some works assume that the noise of each LF is only dependent on the
hidden ground truth (e.g. DS [17]), while the noise can be distributed arbitrarily in practice. Many
works assume that the dependency structure of the LFs is known as a prior or can be inferred reliably

Preprint. Under review.

 
 
 
 
 
 
without ground truth (e.g. DP [48], FS [22] and MeTaL [47]), while in practice the dependency
structure is never provided as a priory nor can be inferred reliably. Some probabilistic graphical
model (PGM) based methods (e.g. DP [48], FS [22] and NPLM [61]) make the Markov assumption
(or its simpliﬁed form, LFs are conditionally independent), which also may not hold in practice. Due
to these assumptions, existing approaches do not have robust performance across datasets, as we will
show in experiments. The existing approaches also involve an unsupervised learning process for each
dataset which can be very expensive (e.g. Gibbs sampling on PGM [48]) for big datasets.

In this paper, we for the ﬁrst time present an analytical label aggregation method which does not
make any of the aforementioned assumptions. The analytical method is optimal in the sense that it
minimizes a certain form of averaged prediction error. The analytical method is conceptually simple
but is of exponential complexity to compute. Fortunately, we show that we can train an ML model
that learns to be the analytical method. Once trained, the model predicts the ground-truth labels
in a single forward pass with a complexity linear to the input size. To train a model, we need to
consider what training data to use and what model architecture to use. We ﬁrst design a training
data generation method and formally prove that a model trained on our synthetically generated data
learns to be the analytical method. We then design an effective model architecture that makes the
trained model able to work on arbitrary number of noisy label vectors of arbitrary size and ensures
the trained model to be invariant (and equivariant) to random permutations of LFs (and data points).

We highlight that, once our model is trained on synthetically generated data, it can be used for any
unseen real datasets without needing any forms of update or ﬁne-tuning. This is due to the fact that
our model learns to be the analytical solution which is applicable to any datasets. We also highlight
that, all existing methods (except majority vote) involve an unsupervised learning process, while our
model predicts the ground-truth labels in a single forward pass.

Contributions. We make the following contributions:

• We for the ﬁrst time present an analytical method for label aggregation which is optimal in the
sense that it minimizes a certain form of the averaged prediction error, though directly using the
analytical method is of exponential complexity.

• We train a model to learn the analytical method. The trained model can be used to infer the

ground-truth labels for any unseen dataset in a single forward pass.

• We design a training data generation method and prove that a model trained on the synthetically

generated data learns to be the analytical method.

• We design an effective model architecture so that the trained model is applicable to arbitrary number
of LF label vectors of arbitrary size and is invariant/equivariant to the permutation of LF label
vectors and data points.

• We empirically show that our method signiﬁcantly outperforms the best existing methods over 14
real-world weak supervision datasets in both accuracy (by 3.5 points on average) and efﬁciency (by
a speedup of six times on average).

2 Related Work

Label Aggregation for Weak Supervision. Most existing methods use probabilistic graphical
models (PGM) [48, 22, 61]. These methods can have some guarantees under some assumptions [48,
22]. The popular system Snorkel [55] currently adopts a matrix completion model (MeTaL [47])
which is shown to be more efﬁcient and accurate than PGM-based methods. The existing methods
typically assume the dependency structure of the LFs is given by the users [47, 48, 22] and there is
another line of work on inferring the dependency of LFs [7, 58]. Our method provides an analytical
form with minimum assumption and is fundamentally different from all existing methods.

Joint Training of Label Aggregation Model and End Model. There are methods that jointly train
a label aggregation model and an end model (e.g. classiﬁer on images) [51, 64]. These methods has
the beneﬁt of incorporating additional information from the raw data (e.g. images). However, training
an end model is very slow, while LF development is a trial-and-error process [12] in which immediate
feedback from a label aggregation model can be crucial for debugging the LFs [59, 51]. Therefore,
coupling with end model training inevitably hampers the LF development process [51]. Also, the
aggregated labels (or even the developed LFs) are tied to the particular end model and may not work

2

well when the model architecture or loss function changes. For these reasons, in most applications,
label aggregation and end model training are two separate processes [40, 8, 20, 60, 21, 38, 22].

Crowdsourcing. Crowdsourcing has a similar setting where each crowd worker can be seen as a
LF. The Dawid and Skene’s method (DS) [17] is the seminal method that has been widely used even
until recently [68, 54]. It assumes each crowd worker to be independent and assumes the noise of
each crowd worker is only dependent on the hidden ground-truth labels. Many methods have been
developed by extending DS (e.g. to have theoretical guarantees under some assumptions or to improve
efﬁciency) [67, 29, 14, 54]. A recent method (EBCC) that models the joint distribution of the crowd
workers with a mixture of low dimensional tensors achieves the state-of-the-art performance [36].
One major difference of crowdsourcing from weak supervision is that crowd workers provide labels
without coordination so typically one can assume them to be independent, while in weak supervision
one tends to write multiple LFs with a similar labeling heuristic [41].

3 Problem Setup

Let n and m denote the number of data points and the number of LFs respectively. Let X ∈
{+1, −1, 0}n×m denote a label matrix where X[i, j] ∈ {+1, −1, 0} denotes the weak label of the
ith (i ∈ [0, n − 1]) data point provided by the jth LF (j ∈ [0, m − 1]). The values +1 and −1 denote
the positive and negative classes respectively and 0 denotes abstention (meaning that an LF does not
have enough information to label a data point as either positive or negative [48]). The goal of a label
aggregation model is to infer the unknown ground-truth label vector y ∈ {+1, −1}n using X.
The Better-than-random Assumption. In principle, X could be any matrix in {+1, −1, 0}n×m
and y can be any vector in {+1, −1}n. For an arbitrary X and an arbitrary y, there is no way to infer
y from X with a better performance than random guess. In weak supervision literature, it is assumed
that X encodes some knowledge about y so that one can make a meaningful estimation of y using
X. Speciﬁcally, most existing work on weak supervision [48, 22, 47, 51] explicitly or implicitly
makes the assumption that each LF is a better-than-random estimation of y. Formally, the jth LF
being better-than-random can be described as p(X[i, j] = y|y[i] = y) > p(X[i, j] = −y|y[i] = y)
for y ∈ {+1, −1}. Replacing the probability as empirical frequencies observed in (X, y), the
assumption can be written as:

where g(X, y, j, y) =

g(X, y, j, +1) = 1 and g(X, y, j, −1) = 1
(cid:26)1, if (cid:80)n−1
i=0 1X[i,j]=y & y[i]=y > (cid:80)n−1
0, otherwise;

i=0 1X[i,j]=−y & y[i]=y

(1)

Intuitively, g(X, y, j, y) denotes whether the jth LF is better than random for class y. When
g(X, y, j, +1) = g(X, y, j, −1) = 1, the jth LF is better than random for both the positive class and
the negative class, and hence the LF is said to be better than random.

A Weaker Form of the Better-than-random Assumption. The assumption that every LF being
better-than-random intuitively makes sense because LFs are developed by humans. However, humans
tend to make mistakes, so it is unrealistic to assume every LF is better than random. Further, it is
possible that one LF makes highly accurate predictions for one class while being worse than random
for another class, so we should also consider different classes separately. Therefore, we propose to
loosen the assumption to be that, for each class, the majority of LFs are better than random; formally:

m−1
(cid:88)

j=0

g(X, y, j, +1) >

m
2

and

m−1
(cid:88)

j=0

g(X, y, j, −1) >

m
2

(2)

We deﬁne σ(X, y) = 1 when Equation 2 is satisﬁed and σ(X, y) = 0 otherwise. We say a pair
(X, y) is valid when σ(X, y) = 1. Intuitively, σ constrains the space of the predicted label vector ˆy
and we would only predict one of those label vectors with σ(X, ˆy) = 1 for a label matrix X. Note
the method we are going to propose is not tied to this form of the assumption, and it also works with
the original assumption (or even any other assumptions to deﬁne σ). We will show the gain of the
weaker form of the assumption in ablation study in Section 5.3.

3

4 Learned Label Aggregation

An Optimal Estimator of the Ground-truth Label Vector. For each label matrix X, let U (X) =
{y|σ(X, y) = 1} denote the set of valid candidate ground-truth label vectors for X. The expected
error of an estimator h of the ground-truth label vector on each X is:

(cid:15)(X, h) =

(cid:88)

y∈U (X)

p(y|X)||y − h(X)||

(3)

where p(y|X) is a distribution of y deﬁned on set U (X) and || · || denotes L2 loss (i.e. squared error).
p(y|X) is unknown and can be different in different real-world applications. Without additional
information apart from X, there is no way to determine the preference of some valid choices of y
over other valid choices of y, so the uniform distribution (i.e. p(cid:48)(y|X) = 1
|U (X)| ) is intuitively the
"best" approximate for the unknown p(y|X). In fact, using the uniform distribution has optimalities
in both the worst case and the average case. To maintain the ﬂow of the paper, we defer the formal
deﬁnition and proof of the optimalities of using the uniform distribution to Appendix B. Replacing
p(y|X) by the uniform distribution, Equation 3 becomes:

(cid:15)(cid:48)(X, h) =

1
|U (X)|

(cid:88)

y∈U (X)

||y − h(X)||

(4)

(cid:15)(cid:48)(X, h) can be interpreted as the average error of all possible outcomes. An estimator h can be said
to be the optimal if it minimizes the error (cid:15)(cid:48)(X, h), ∀X.
Theorem 1. h∗(X) = 1

y∈U (X) y, ∀X is the optimal estimator for the ground-truth vector.

(cid:80)

|U (X)|

We omit the proof as it is straightforward (The mean minimizes mean squared error.). Theorem 1
makes sense intuitively: since X is the only information we have, y can be any element in U (X) and
there is no information to support preferences of some elements over other elements in U (X), so the
best prediction one can make is the average of all elements in U (X).
Although we have the analytical form of the optimal estimator h∗, computing it is of exponential
complexity as U (X) is exponentially large for any X. Therefore, we propose to train a model h to
learn the optimal estimator. Once trained, the inference complexity of the model will be linear to the
size of the input, i.e. O(nm). To materialize this idea, we need to answer two questions: (1) What
training data to use? (2) What model architecture to use? We discuss both in the following sections.

Discussion. We note that our overall method is not tied to the choice of using an uniform distribution
for p(y|X). If one can come up with a better distribution, our overall method still applies and one
only needs to make two changes: The form of h∗ will be a weighted average of the y vectors i.e.
h∗(X) = (cid:80)
y∈U (X) p(y|X)y and our data generation method (to be introduced in the next section)
should also be adapted according to the new distribution. Then, the overall pipeline will be the same.

4.1 Training Data Generation.

We aim to synthetically generate many pairs of (X, y) as training data. Let D = {(X1, y1), . . . }
denote the training set. We design a way of generating the training set D so that the model h trained
on D by minimizing the cross entropy loss learns the optimal estimator h∗.

Let U = {(X, y)|σ(X, y) = 1} denote the set of all valid pairs. Any method M to generate a
training set D is to sample |D| elements from set U by a certain distribution pM (X, y) that is deﬁned
on U . It can be shown that, when |D| → +∞, for each X, h(X) = (cid:80)
y∈U (X) pM (y|X)y minimizes
the cross entropy loss (see proof in Appendix C). In other words, for each X, the model h learns to
predict the expected label vector. If the data generation method M makes pM (y|X) uniform, i.e.
pM (y|X) = 1
y∈U (X) y = h∗(X).
We present a training data generation method M that ensures pM (y|X) to be uniform. Note there is
no constraint on the distribution pM (X). We ﬁrst randomly generate the shape of X, by randomly
draw m (and n) from a uniform distribution [Lm, Hm] (and [Ln, Hn]). We provide details of how to
choose Lm, Hm, Ln and Hn in Appendix F and show the trained model generalizes very well outside
of the regions [Lm, Hm] and [Ln, Hn] in experiments. Given each sampled m and n, the shape of X
and y is determined, we then generate the values in X and y uniformly at random. In other words,

|U (X)| , the model learns the optimal estimator h(X) = 1

|U (X)|

(cid:80)

4

each element X[i, j] in the matrix X is sampled from the uniform distribution on set {+1, −1, 0}
and each element y[i] in vector y is sampled from the uniform distribution on set {+1, −1}. If
σ(X, y) = 1, we keep it as a training data point; otherwise, we randomly generate another pair; This
process is repeated many times. Apparently, since y is generated uniformly, for any two different
vectors y1 and y2 with σ(X, y1) = σ(X, y2) = 1, the probability of generating y1 equals to the
probability of generating y2. Therefore, the condition that pM (y|X) is uniform is satisﬁed. The data
generation method is also efﬁcient because the probability of generating a valid pair in one trial is
about 0.2 (see Appendix D).

4.2 Model Architecture

When training on data generated by the above method, the model h learns to be the optimal estimator
h∗. However, how well it learns depends on the architecture design of h. The input of the model
h is a matrix X of size n × m and the output of the model h is a vector ˆy of size n. The model
h should satisfy the following three properties: (1) Ability to Accept Arbitrary Input Size. The
number of data points n and LFs m can be different for different datasets. The model h should be
able to accept an input matrix X of arbitrary size. (2) Invariance to Permutation of LFs. Intuitively,
randomly shufﬂing the LFs should not change the prediction of any data point. Formally, let Pm
denote one arbitrary permutation of the m integers in [0, m − 1]. For example when m = 4, Pm
could be {3, 0, 1, 2}. Invariance to permutation of LFs means that h(X[:, Pm]) = h(X), ∀Pm. (3)
Equivariance to Permutation of Data Points. Intuitively, randomly shufﬂing the data points should
not change the prediction of each data point. Formally, equivariance to permutation of data points
means that h(X[Pn, :]) = h(X)[Pn], ∀Pn where Pn is deﬁned similarly as Pm.

Our intuition is that a graph neural network (GNN) can accept input graph of arbitrary size and is
permutation equivariant to the nodes [52]. Therefore, it is possible that by somehow representing the
input matrix X as a graph and then use a GNN, we can have an architecture that satisﬁes all the above
three properties. Speciﬁcally, we ﬁrst represent the input matrix X as a graph G. For example, the
left-most matrix and graph in Figure 1 illustrate how we represent an input matrix of size 3 × 2 as a
graph. The weak label of the ith data point provided by the jth LF is represented as a node Vi,j with
value X[i, j]. There are two types of edges: solid yellow edge and dashed blue edge. Nodes from
the same LF (i.e. same column in matrix X) are connected with solid yellow edges and nodes from
the same data point (i.e. same row in matrix X) are connected with dashed blue edges. It is easy to
see the graph representation G loses no information as one can easily recover X (or its permutation
X[Pn, Pm]) from G. In graph G, if we only look at dashed blue edges, there would be n strongly
connected components and each corresponds to one data point. Speciﬁcally, the strongly connected
component SCCi={Vi,0, Vi,1, . . . } corresponds to the ith data point.

Figure 1: Overall network architecture.

The idea of designing a model architecture with the mentioned three properties is that: ﬁrst encode the
graph with a GNN of K layers and each node Vi,j is encoded with embedding V k
i,j at the kth layer;
then after the ﬁnal layer, we obtain an embedding for each SCCi (i.e. each data point) by pooling all
of its nodes ¯V K
i,j ; The embedding of each SCCi is passed to a Multilayer perceptron
(MLP) to obtain the ﬁnal prediction. The overall model architecture is shown in Figure 1. It’s
straightforward to see that the architecture satisﬁes all three mentioned properties (see Appendix E.1).

i,: = 1
m

j V K

(cid:80)

We adopt the standard design of GNN. Since we have two types of edges, we perform message
passing for neighboring nodes connected with different edges separately. Speciﬁcally, at the kth layer

5

in the GNN, the embedding V k

i,j for the node Vi,j is obtained as:

i,j = fk(Ak(W k
V k
1

1
n

(cid:88)

q

V k−1
q,j

, W k
2

1
m

(cid:88)

q

V k−1
i,q

, W k
3

1
nm

(cid:88)

q,l

V k−1
q,l

, W k

4 V k−1
i,j

))

(5)

(cid:80)

i V k−1
i,j
(cid:80)

1 , ... ,W k

4 are weight matrices; 1
n

where W k
denotes average pooling over neighboring nodes
j V k−1
of Vi,j connected with solid yellow edges and 1
denotes average pooling over neighboring
m
nodes of Vi,j connected with dashed blue edges; Note we use average pooling because the graph
can be of variable size as recommended by [52] and we also include the node’s previous embedding
V k−1
in the average in case the node has no neighbors (this is equivalent to adding a self-edge to each
i,j
node.). We also add the global context of the graph 1
to enable message passing beyond
nm
neighboring nodes, following the standard practice [23, 10]; Ak(·, ·, ·, ·) denotes an aggregation
operation and we use simple concatenation; fk denotes a linear layer with Relu activation.

j,j V k−1

(cid:80)

i,j

i,j

Inference Complexity. The complexity of a forward pass is dominated by the GNN. Although
there are O(n2) edges in the graph, there is no need to actually materialize the O(n2) edges and the
complexity of each GNN layer is only O(nm). In each GNN layer, for the three averaged pooling
operations in Equation 5, the ﬁrst one with complexity O(n) needs to be computed once for each LF
totaling m times so the complexity is O(nm); Similarly, the second one and the third one also have a
complexity of O(mn). Apparently, the space complexity is also O(mn).

Handling Abstention. Abstention (denoted by 0 in the label matrix X) means an LF does not have
enough information to label a data point as either +1 or -1 [48]; Handling abstention is straightforward
in our approach. We can simply remove the corresponding nodes in our graph. For example, when
the jth LF abstains on the ith data point, we simply remove the node Vi,j from the graph.
Permutation Equivariance/Invariance to Input Matrix. Many applications with matrix as input
require a model to be equivariant or invariant to the permutation of rows/columns of the matrix [11].
The Matrix Layer with such equivariance property is proposed in [25]. In fact, each layer of our GNN
reduces to the Matrix Layer. We highlight that our way of representing the matrix as a graph then
encoding it with GNN provides an intuitive and straightforward perspective for the Matrix Layer that
was originally derived in a complicated way [25]; There are also methods that represent the matrix as
a bipartite graph where each row is represented as one type of node and each column is represented
as another type of node to achieve permutation equivariance [43, 11].

4.3 Supporting Semi-supervised/Multi-class Label Aggregation

Supporting Semi-supervised Label Aggregation with Fine-tuning. We pretrain a model h0 on
our sythetically generated dataset. When no ground-truth label is provided, our method can infer the
ground-truth label vector y from X in a simple forward pass on h0. When a small set of ground-truth
labels is provided, our method can easily incorporate the labels by ﬁne-tuning the model on the
provided labels. Let I denote the set of indices of the elements in y that are provided. For example,
when I = [2, 3], it means y[2] and y[3] are provided. Fine tuning is done by minimizing the loss
(cid:80)
i∈I CrossEntropy(h(X)[i], y[i]) and h is initialized as the pretrained model h0. After ﬁne-tuning
we obtain a model h(cid:48), and then all labels are obtained by h(cid:48)(X).

Supporting Multi-class Datasets. We have only considered the binary labels and one can extend our
training data generation and network architecture to natively support multi-class tasks. However, it
turns out that our trained model for binary labels can be easily used to support multi-class classiﬁcation
datasets by decomposing a multi-class task with C classes to be C one-vs-rest binary classiﬁcation
tasks. For multi-class tasks, we have X[i, j] ∈ {0, 1, 2, . . . C} where 0 still denotes abstention and
other numbers denote all the classes. We construct the label matrix for the cth class as Xc[i, j] = 1 if
X[i, j] = c, Xc[i, j] = 0 if X[i, j] = 0, and otherwise Xc[i, j] = −1. In this way, we obtain C label
matrices {X1, . . . Xc}. We apply our pre-trained model h0 on each label matrix of each class and
obtain C predicted probability vectors (p1, . . . , pc). Then, for the ith data point, its soft label over
the C classes is ( p1[i]

c pc[i] ). We show in experiments this simple method works well.

c pc[i] , . . . , pc[i])

(cid:80)

(cid:80)

6

5 Experiments

We evaluate from two major aspects (1) the performance of label aggregation and (2) the performance
of end model trained on the aggregated labels. Additional experimental results on running time and
the presence of adversarial LFs are in Appendix H.1 and Appendix H.2. The code and detailed
instructions to reproduce the experiments are available in supplementary materials.

Datasets. We use all 14 classiﬁcation datasets in a recent weak supervision benchmark [65] that
are from diverse domains (e.g. income/sentiment/spam/relation/question/topic classiﬁcation tasks).
We highlight these datasets are only used for evaluation after our model is trained on synthetically
generated data, and we never used these datasets during training. Table 1 shows the statistics of all
datasets. Note that, the original benchmark splits each dataset to training/validation/test sets for the
purpose of evaluating supervised end models. Since our paper focuses on label aggregation, there is
no need to split the datasets [2, 41] and we only split the datasets when we use the generated labels to
train an end model in Section 5.4. Different datasets may require different performance metrics based
on their application background, so we use the metric adopted by the benchmark [65] for each dataset.
All LFs are from the original authors of each dataset and all LFs are hosted in the benchmark [3].

Table 1: 14 classiﬁcation datasets from the WRENCH weak supervision benchmark [65]

Dataset

Census
[6, 32]

IMDB
[50, 39]

Yelp
[50, 66]

Youtube
[4]

SMS
[6, 5]

Spouse
[13, 46]

CDR
[16, 46]

Commercial
[22]

Tennis
[22]

Basketball
[22]

AGNews
[50, 66]

TREC
[35, 6]

SemEval
[27, 69]

ChemProt
[33, 62]

#class 2

metric F1

#LF

83

2

acc

5

2

acc

8

2

acc

10

2

F1

73

2

F1

9

2

F1

33

2

F1

4

2

F1

6

2

F1

4

4

acc

9

6

acc

68

9

acc

164

10

acc

26

#Data

31925 25000

38000

1956

5571 27766

14023

81105

8803

20256

120000

5965

2641

16075

Baselines. We compare our method LEarned Label Aggregation (LELA) to all label aggregation
methods in the weak supervision benchmark [65], and a method published very recently [61]. We
also include two representative methods from crowdsourcing as it has a similar setting.
• Majority Vote (MV). The predicted label of each data point is the most common label given by LFs.
• Data Programming (DP) [48]. DP uses a probabilistic graph model (PGM) where each LF is a

node and the hidden ground truth is a latent variable.

• Flyingsquid (FS) [22]. FS also uses a PGM but gives a closed-form solution with some assumptions.
• MeTaL [47]. MeTaL infers the ground truth using a matrix completion model. The latest version of

the popular Snorkel system [55] adopts MeTaL as its default label aggregation method.

• NPLM [61]. This method is also based on a PGM and assumes LFs are conditionally independent.
It supports partial LFs that predict a subset of class labels and is designed to be very efﬁcient.
• Dawid and Skene’s method (DS) [17]. DS models the confusion matrix of each LF with respect
to the ground truth labels. This method is widely used in crowdsourcing and is the recommended
method for classiﬁcation tasks in a benchmark on crowdsourcing [68].

• Enhanced Bayesian Classiﬁer Combination (EBCC) [36]. This method models the joint distribution
of crowd workers as a mixture of multiple low dimensional tensors. This method achieves the
state-of-the-art results on 17 crowdsourcing datasets [36].

There are methods that learn a label aggregation model and an end model at the same time (e.g.
WeaSEL [51] and AMCL [41]). We note they are in a different setting and are not directly comparable
to methods that focus on label aggregation [65, 64]. We discussed the beneﬁts, downsides, and
applications of both types of methods in related work (Section 2).

Implementation. We provide the implementation details of our method (e.g. setups and all parame-
ters in data generation/model architecture/model training/validation) in Appendix F and implementa-
tion details of the experiments (e.g. hardware/datasets/baselines/setups) in Appendix G.

5.1 Label Aggregation Performance

The performance of all methods on all 14 datasets averaged over ﬁve runs are shown in Table 2. To
maintain the table to be readable, we only show the error bars for the averaged scores. Again, for our
method LELA, we note only synthetically generated data is used for training and the 14 datasets are
only used to evaluate the trained model. For LELA, inference on the real datasets is deterministic, but
the training process on the synthetic data has randomness, so the error bar is obtained by repeating
the training process multiple times and then performing inference with different trained models.

7

First, our results align with the benchmark [65] where MeTaL is the best performing baseline and
is slightly better than MV. The difference in numbers from the benchmark [65] is due to that we do
not split the datasets to training/val/test sets because we focus on label aggregation [2, 41]. Second,
LELA outperforms the best baseline MeTaL by 3.5 points on average which is signiﬁcant considering
MeTaL only outperforms MV by 0.5 points. Third, LELA is the best on 9 out of 14 datasets; On the
remaining 5 datasets, LELA is the second best or is close to the second best method. The superiority
of LELA is due to the fact that it learns the "optimal" solution during the ofﬂine training process.
Finally, the two crowdsourcing methods have the worst performance which also aligns with the results
in the benchmark [65]. We provide a discussion on why crowdsourcing methods fail in Appendix E.2.

Table 2: Performance (F1 or acc score depending on the dataset) on all datasets

Dataset Census IMDB Yelp Youtube SMS Spouse CDR Commercial Tennis Basketball AGNews TREC SemEval ChemProt AVG.

MV

DP

FS

22.2

11.1

17.1

MeTaL 51.1

NPLM 0.0

DS

0.0

EBCC 0.0

LELA 56.1

75.0

74.4

74.5

75.0

55.2

74.4

74.4

75.0

74.4 80.3

84.0 51.6

63.3 85.9

71.9 84.5

83.8 50.3

33.9 77.5

74.0 83.7

74.4 49.9

69.6 82.5

74.4 86.0

57.7 49.9

67.9 83.7

68.3 45.2

0.0

34.3

68.3 45.2

65.0 34.3

69.6 45.2

0.0

34.3

0.0

0.1

8.7

76.5

77.8

77.5

74.4 91.4

84.1 51.6

71.0 83.6

85.0

85.1

84.0

80.9

85.0

85.0

85.0

84.3

18.9

17.1

17.1

19.0

0.0

17.1

17.1

17.1

81.4

81.7

81.3

82.2

81.3

26.6

27.8

81.4

49.9

47.2

50.1

52.1

36.5

20.9

20.8

59.8

84.2

73.5

23.8

84.2

30.2

73.5

30.2

84.2

53.7

56.2

52.4

52.9

48.4

35.1

35.0

52.3

65.0±0.0

60.6±0.1

59.6±0.0

65.5±0.2

40.1±0.0

44.5±0.0

37.6±0.1

69.0±0.2

We report the running time in Table 6 (in Appendix H.1). LELA requires less than 1 seconds on every
dataset. LELA is on average 6 times (and can be up to 18 times) faster than the fastest baseline (except
Majority Vote). This is because all prior methods (except Majority Vote) require an unsupervised
learning process while LELA performs prediction in a single forward pass just like Majority Vote.
We note that these 14 benchmark datasets are relatively small (as creating a large benchmark dataset
with ground-truth labels is expensive). In industry scenarios, LFs can be applied on millions of data
points to create labels [8]. The runtime gain of LELA will be more signiﬁcant and LELA will enable
the LF development process to be more interactive.

5.2 Semi-supervised Label Aggregation

When the ground-truth labels for some data points are provided i.e. some elements in y are known,
LELA can also easily incorporate these labels through the ﬁne-tuning method in Section 4.3.

For each dataset, we randomly sample Ngt data points as the data points with known ground-truth
labels and we evaluate on the remaining data points. When Ngt > 0.7n, we only select 0.7n data
points to keep 30% of the data for evaluation in order to have a reliable evaluation score. We vary
Ngt from 10 to 10000. Since the baseline label aggregation methods don’t support semi-supervised
label aggregation, we train a random forest classiﬁer as our baseline using the provided labels (with
X as the feature matrix). The hyper-parameters of random forest are selected by cross validation.
When ﬁnetuning LELA, we use a smaller learning rate lr = 0.0001 to prevent overﬁtting (originally
lr = 0.001). Intuitively, when Ngt is small, we trust the pre-trained LELA more than the provided
labels; when Ngt is large, we trust the provided labels more than the pre-trained LELA. Therefore,
we relate the number of ﬁnetuning epochs to Ngt by setting the number of epochs as (cid:112)Ngt.
The results are shown in Figure 2. When Ngt is small, semi-supervised LELA can be be slightly
worse than unsupervised LELA (i.e. the pretrained LELA on synthetic data without ﬁne-tuning.).
This is because ﬁnetuning on a small number of label data causes overﬁtting. When Ngt > 30,
semi-supervised LELA outperforms unsupervised LELA. Random forest requires about 800 labels
to match the performance of unsupervised LELA and 4000 labels to catch up with semi-supervised
LELA. When Ngt > 4000, the performance of semi-supervised LELA and random forest is the same.
We highlight that the performance of a supervised method random forest converges at about 70.1
which can be seen as an empirical upperbound for any unsupervised label aggregation method.
Unsupervised LELA is able to achieve 69.0, only 1.57% lower than the empirical upperbound (and
the best existing method is 6.56% lower). This suggests LELA is empirically very close to the
optimal unsupervised label aggregation method. Finally, we note semi-supervised LELA is also very
efﬁcient, e.g. when Ngt = 10000, the running time averaged over all datasets is 3.1 seconds for
semi-supervised LELA and is 4.8 seconds for random forest.

8

LELA

Avg score

69.0

64.2
*data generation
50.2
*model architecture
*better-than-random 67.6

Table 3: Ablation study. "*" denotes
replacing the component it precedes with
a naive one.

Figure 2: Semi-supervsied LELA vs Random forest.

5.3 Ablation Study

We perform ablation study in three aspects: (1) We replace our data generation method with the one
proposed in [65] that was originally used to generate LFs to evaluate label aggregation models. (2)
We replace our model architecture with a naive architecture (see Appendix G). (3) We replace our
proposed weaker form of the better-than-random assumption in Equation 2 with the original form in
Equation 1. The results are shown in Table 3. Replacing each component reduces performance. In
particular, the original form of the better-than-random assumption decreases performance because the
assumption that each LF is better-than-random on each class is not satisﬁed in the real-world datasets.
In fact, every dataset has least one LF that is worse-than-random and on average 27.7% of the LFs
are worse-than-random. This veriﬁes our choice of the weaker assumption.

5.4 End Model Performance

We use the generated labels of each method to train an end model for each dataset. We consider the
two best performing baselines MeTaL and MV. We use the test split provided by the benchmark [65, 3]
for each dataset because some datasets only have ground-truth labels for data points in the provided
test split. We then randomly split the remaining data points to be a training set and a validation
set with a 3:1 ratio. The labels in the training set and validation set are generated labels by each
label aggregation method, while the labels in the test set are ground-truth labels for evaluation.
Following prior work [48, 65], the probabilistic labels instead of the hard labels are used to train the
end model when possible. We adopt the end models used in (and their implementations provided
by) the benchmark [65, 3], i.e. a pretrained BERT model [19] for textual datasets and a multi-layer
perception (MLP) for datasets with numeric features. We report the results on test set in Table 4.
Again, to maintain the table to be readable, we only show the error bars for the averaged scores.

Table 4: Performance of end model trained with labels generated by each method.

Dataset

Census IMDB Yelp Youtube SMS Spouse CDR Commercial Tennis Basketball AGNews TREC SemEval ChemProt AVG.

End model MLP BERT BERT BERT BERT BERT BERT MLP

MLP MLP

BERT

BERT BERT

BERT

MV

MeTaL

LELA

31.7

11.6

56.3

74.7

74.4

74.7

74.2

70.5

75.7

90.9

87.1

93.0

83.5

87.3

82.4

51.6

51.0

52.3

62.9

63.5

64.1

90.1

88.2

87.1

83.5

83.5

83.5

13.4

14.5

17.0

81.9

82.0

80.8

63.9

63.7

68.4

76.8

83.1

82.8

54.6

52.5

53.1

66.7±0.8

65.2±0.2

69.4±0.3

Our results align with those in the benchmark [65] where the end model trained on labels generated
by MeTaL is slightly worse than that by MV. Overall, LELA outperforms the other two methods. On
Yelp, Spouse, and SemEval, LELA tied with MV in label quality (see Table 2) but has better end
model performance as LELA’s probabilistic labels can be more informative. Note the scores of the
end model can be higher than that of the generated labels (as also observed in the benchmark [65]
and prior work [46]) because the end model incorporates additional information from the raw data.

6 Conclusion

We present an analytical label aggregation method for weak supervision, which is optimal in the sense
that it minimizes a certain form of the averaged prediction error. However, the analytical form is of
exponential complexity to compute. We propose to train a model to learn the analytical method and

9

once learned the inference complexity of the model is only linear to input size. We design a synthetic
training data generation method that ensures the model trained on the generated data learns to be
the analytical method. We design an effective model architecture that enables the trained model to
handle arbitrary number of LFs and arbitrary number of data points and also makes the trained model
invariant/equivariant to the permutation of LFs/data points. We experimentally verify the superiority
of our method in both accuracy and efﬁciency with both unsupervised and semi-supervised label
aggregation settings over 14 datasets. We further show that the end model trained using the labels
generated by our method has better performance than existing methods on 14 datasets.

Acknowledgments and Disclosure of Funding

Use unnumbered ﬁrst level headings for the acknowledgments. All acknowledgments go at the
end of the paper before the list of references. Moreover, you are required to declare funding
(ﬁnancial activities supporting the submitted work) and competing interests (related ﬁnancial activities
outside the submitted work). More information about this disclosure can be found at: https:
//neurips.cc/Conferences/2022/PaperInformation/FundingDisclosure.

Do not include this section in the anonymized submission, only in the ﬁnal paper. You can use
the ack environment provided in the style ﬁle to autmoatically hide this section in the anonymized
submission.

References

[1] 2022. sklearn.ensemble.RandomForestClassiﬁer. https://scikit-learn.org/stable/
modules/generated/sklearn.ensemble.RandomForestClassifier.html [Online; ac-
cessed 2. May 2022].

[2] 2022. Wrench Github Issue. "Question on train/val/test split when evaluating label model.".
https://github.com/JieyuZ2/wrench/issues/27 [Online; accessed 12. May. 2022].

[3] 2022. Wrench Project Homepage.

https://github.com/JieyuZ2/wrench [Online;

accessed 12. May. 2022].

[4] Túlio C Alberto, Johannes V Lochter, and Tiago A Almeida. 2015. Tubespam: Comment spam
ﬁltering on youtube. In 2015 IEEE 14th international conference on machine learning and
applications (ICMLA). IEEE, 138–143.

[5] Tiago A Almeida, José María G Hidalgo, and Akebo Yamakami. 2011. Contributions to the
study of SMS spam ﬁltering: new collection and results. In Proceedings of the 11th ACM
symposium on Document engineering. 259–262.

[6] Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal, and Sunita Sarawagi. 2020. Learning from

rules generalizing labeled exemplars. arXiv preprint arXiv:2004.06025 (2020).

[7] Stephen H Bach, Bryan He, Alexander Ratner, and Christopher Ré. 2017. Learning the structure
of generative models without labeled data. In International Conference on Machine Learning.
PMLR, 273–282.

[8] Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia,
Souvik Sen, Alex Ratner, Braden Hancock, Houman Alborzi, et al. 2019. Snorkel drybell:
A case study in deploying weak supervision at industrial scale. In Proceedings of the 2019
International Conference on Management of Data. 362–375.

[9] BatsResearch. 2022.

yu-aistats22-code.

https://github.com/BatsResearch/

yu-aistats22-code [Online; accessed 1. May 2022].

[10] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zam-
baldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner,
et al. 2018. Relational inductive biases, deep learning, and graph networks. arXiv preprint
arXiv:1806.01261 (2018).

10

[11] Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph convolutional matrix

completion. arXiv preprint arXiv:1706.02263 (2017).

[12] Mobilize Center. 2019. Snorkel Workshop 2018: Best Practices for Improving Your Labeling
https://www.youtube.com/watch?v=mrIkus844B4 [Online; accessed 27.

Functions.
Apr. 2022].

[13] David PA Corney, Dyaa Albakour, Miguel Martinez-Alvarez, and Samir Moussa. 2016. What

do a million news articles look like?. In NewsIR@ ECIR. 42–47.

[14] Nilesh Dalvi, Anirban Dasgupta, Ravi Kumar, and Vibhor Rastogi. 2013. Aggregating crowd-
sourced binary ratings. In Proceedings of the 22nd international conference on World Wide Web.
285–294.

[15] Nilaksh Das, Sanya Chaba, Renzhi Wu, Sakshi Gandhi, Duen Horng Chau, and Xu Chu. 2020.
Goggles: Automatic image labeling with afﬁnity coding. In Proceedings of the 2020 ACM
SIGMOD International Conference on Management of Data. 1717–1732.

[16] Allan Peter Davis, Cynthia J Grondin, Robin J Johnson, Daniela Sciaky, Benjamin L King,
Roy McMorran, Jolene Wiegers, Thomas C Wiegers, and Carolyn J Mattingly. 2017. The
comparative toxicogenomics database: update 2017. Nucleic acids research 45, D1 (2017),
D972–D978.

[17] Alexander Philip Dawid and Allan M Skene. 1979. Maximum likelihood estimation of observer
error-rates using the EM algorithm. Journal of the Royal Statistical Society: Series C (Applied
Statistics) 28, 1 (1979), 20–28.

[18] Frederik Michel Dekking, Cornelis Kraaikamp, Hendrik Paul Lopuhaä, and Ludolf Erwin
Meester. 2005. A Modern Introduction to Probability and Statistics: Understanding why and
how. Springer. 181–190 pages.

[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training
of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805
(2018).

[20] Jared A Dunnmon, Alexander J Ratner, Khaled Saab, Nishith Khandwala, Matthew Markert,
Hersh Sagreiya, Roger Goldman, Christopher Lee-Messer, Matthew P Lungren, Daniel L Rubin,
et al. 2020. Cross-modal data programming enables rapid medical machine learning. Patterns
1, 2 (2020), 100019.

[21] Jason A Fries, Paroma Varma, Vincent S Chen, Ke Xiao, Heliodoro Tejeda, Priyanka Saha, Jared
Dunnmon, Henry Chubb, Shiraz Maskatia, Madalina Fiterau, et al. 2019. Weakly supervised
classiﬁcation of aortic valve malformations using unlabeled cardiac MRI sequences. Nature
communications 10, 1 (2019), 1–10.

[22] Daniel Fu, Mayee Chen, Frederic Sala, Sarah Hooper, Kayvon Fatahalian, and Christopher Ré.
2020. Fast and three-rious: Speeding up weak supervision with triplet methods. In International
Conference on Machine Learning. PMLR, 3280–3291.

[23] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl.
2017. Neural message passing for quantum chemistry. In International conference on machine
learning. PMLR, 1263–1272.

[24] Godfrey Harold Hardy, John Edensor Littlewood, George Pólya, György Pólya, et al. 1988.
"Hölder’s Inequality and Its Extensions.". In Inequalities, 2nd ed. Cambridge university press,
Chapter 2.7-2.8, 21–26.

[25] Jason Hartford, Devon Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. 2018. Deep
models of interactions across sets. In International Conference on Machine Learning. PMLR,
1909–1918.

[26] HazyResearch. 2022. ﬂyingsquid. https://github.com/HazyResearch/flyingsquid

[Online; accessed 26. Apr. 2022].

11

[27] Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O Séaghdha,
Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2019. Semeval-
2010 task 8: Multi-way classiﬁcation of semantic relations between pairs of nominals. arXiv
preprint arXiv:1911.10422 (2019).

[28] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon
Wilson. 2018. Averaging weights leads to wider optima and better generalization. arXiv
preprint arXiv:1803.05407 (2018).

[29] David R Karger, Sewoong Oh, and Devavrat Shah. 2014. Budget-optimal task allocation for

reliable crowdsourcing systems. Operations Research 62, 1 (2014), 1–24.

[30] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping
Tak Peter Tang. 2016. On large-batch training for deep learning: Generalization gap and sharp
minima. arXiv preprint arXiv:1609.04836 (2016).

[31] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv

preprint arXiv:1412.6980 (2014).

[32] Ron Kohavi et al. 1996. Scaling up the accuracy of naive-bayes classiﬁers: A decision-tree

hybrid.. In Kdd, Vol. 96. 202–207.

[33] Martin Krallinger, Obdulia Rabal, Saber A Akhondi, Martın Pérez Pérez, Jesús Santamaría,
Gael Pérez Rodríguez, Georgios Tsatsaronis, Ander Intxaurrondo, José Antonio López, Umesh
Nandal, et al. 2017. Overview of the BioCreative VI chemical-protein interaction Track. In
Proceedings of the sixth BioCreative challenge evaluation workshop, Vol. 1. 141–146.

[34] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2018. Visualizing the

loss landscape of neural nets. Advances in neural information processing systems 31 (2018).

[35] Xin Li and Dan Roth. 2002. Learning question classiﬁers. In COLING 2002: The 19th

International Conference on Computational Linguistics.

[36] Yuan Li, Benjamin Rubinstein, and Trevor Cohn. 2019. Exploiting worker correlation for
label aggregation in crowdsourcing. In International Conference on Machine Learning. PMLR,
3886–3895.

[37] Yinghao Li, Pranav Shetty, Lucas Liu, Chao Zhang, and Le Song. 2021. BERTifying the Hidden
Markov Model for Multi-Source Weakly Supervised Named Entity Recognition. In Proceedings
of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing (Volume 1: Long Papers).
6178–6190.

[38] Pierre Lison, Jeremy Barnes, Aliaksandr Hubin, and Samia Touileb. 2020. Named Entity
Recognition without Labelled Data: A Weak Supervision Approach. In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics. 1518–1533.

[39] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher
Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual
meeting of the association for computational linguistics: Human language technologies. 142–
150.

[40] Jose Mathew, Meghana Negi, Rutvik Vijjali, and Jairaj Sathyanarayana. 2021. DeFraudNet: An
End-to-End Weak Supervision Framework to Detect Fraud in Online Food Delivery. In Joint
European Conference on Machine Learning and Knowledge Discovery in Databases. Springer,
85–99.

[41] Alessio Mazzetto, Cyrus Cousins, Dylan Sam, Stephen H Bach, and Eli Upfal. 2021. Adversarial
Multi Class Learning under Weak Supervision with Performance Guarantees. In International
Conference on Machine Learning. PMLR, 7534–7543.

[42] Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation
extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing
of the AFNLP. 1003–1011.

12

[43] Federico Monti, Michael Bronstein, and Xavier Bresson. 2017. Geometric matrix completion
with recurrent multi-graph neural networks. Advances in neural information processing systems
30 (2017).

[44] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.
2021. Deep double descent: Where bigger models and more data hurt. Journal of Statistical
Mechanics: Theory and Experiment 2021, 12 (2021), 124003.

[45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An
imperative style, high-performance deep learning library. Advances in neural information
processing systems 32 (2019).

[46] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher
Ré. 2017. Snorkel: Rapid training data creation with weak supervision. In Proceedings of the
VLDB Endowment. International Conference on Very Large Data Bases, Vol. 11. NIH Public
Access, 269.

[47] Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic Sala, Shreyash Pandey, and
Christopher Ré. 2019. Training complex models with multi-task weak supervision. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence, Vol. 33. 4763–4771.

[48] Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher Ré. 2016.
Data programming: Creating large training sets, quickly. Advances in neural information
processing systems 29 (2016).

[49] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. 2019. On the convergence of adam and

beyond. arXiv preprint arXiv:1904.09237 (2019).

[50] Wendi Ren, Yinghao Li, Hanting Su, David Kartchner, Cassie Mitchell, and Chao Zhang.
2020. Denoising multi-source weak supervision for neural text classiﬁcation. arXiv preprint
arXiv:2010.04582 (2020).

[51] Salva Rühling Cachay, Benedikt Boecking, and Artur Dubrawski. 2021. End-to-End Weak

Supervision. Advances in Neural Information Processing Systems 34 (2021).

[52] Benjamin Sanchez-Lengeling, Emily Reif, Adam Pearce, and Alexander B Wiltschko. 2021. A

gentle introduction to graph neural networks. Distill 6, 9 (2021), e33.

[53] Jaeho Shin, Sen Wu, Feiran Wang, Christopher De Sa, Ce Zhang, and Christopher Ré. 2015.
Incremental knowledge base construction using deepdive. In Proceedings of the VLDB En-
dowment International Conference on Very Large Data Bases, Vol. 8. NIH Public Access,
1310.

[54] Vaibhav B Sinha, Sukrut Rao, and Vineeth N Balasubramanian. 2018. Fast dawid-skene: A fast
vote aggregation scheme for sentiment classiﬁcation. arXiv preprint arXiv:1803.02781 (2018).

[55] snorkel team. 2022. snorkel.
accessed 26. Apr. 2022].

https://github.com/snorkel-team/snorkel [Online;

[56] snorkel

team. 2022.

snorkel-extraction.

snorkel-extraction/blob/master/snorkel/learning/gen_learning.py
accessed 26. Apr. 2022].

https://github.com/snorkel-team/
[Online;

[57] sukrutrao. 2022.

Fast-Dawid-Skene.

https://github.com/sukrutrao/

Fast-Dawid-Skene [Online; accessed 26. Apr. 2022].

[58] Paroma Varma, Frederic Sala, Ann He, Alexander Ratner, and Christopher Ré. 2019. Learning
dependency structures for weak supervision models. In International Conference on Machine
Learning. PMLR, 6418–6427.

[59] Renzhi Wu, Prem Sakala, Peng Li, Xu Chu, and Yeye He. 2021. Demonstration of panda: a
weakly supervised entity matching system. Proceedings of the VLDB Endowment 14, 12 (2021),
2735–2738.

13

[60] Sen Wu, Luke Hsiao, Xiao Cheng, Braden Hancock, Theodoros Rekatsinas, Philip Levis, and
Christopher Ré. 2018. Fonduer: Knowledge base construction from richly formatted data. In
Proceedings of the 2018 international conference on management of data. 1301–1316.

[61] Peilin Yu, Tiffany Ding, and Stephen H. Bach. 2022. Learning from Multiple Noisy Partial

Labelers. In Artiﬁcial Intelligence and Statistics (AISTATS).

[62] Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, and Chao Zhang. 2020. Fine-tuning
pre-trained language model with weak supervision: A contrastive-regularized self-training
approach. arXiv preprint arXiv:2010.07835 (2020).

[63] yuan li. 2022.

https://github.com/yuan-li/truth-inference-at-scale [Online;

accessed 1. May 2022].

[64] Jieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang, and Alexander Ratner. 2022. A Survey on

Programmatic Weak Supervision. arXiv preprint arXiv:2202.05433 (2022).

[65] Jieyu Zhang, Yue Yu, , Yujing Wang, Yaming Yang, Mao Yang, and Alexander Ratner. 2021.
WRENCH: A Comprehensive Benchmark for Weak Supervision. In Proceedings of the Neural
Information Processing Systems Track on Datasets and Benchmarks, J. Vanschoren and S. Yeung
(Eds.), Vol. 1.

[66] Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for

text classiﬁcation. Advances in neural information processing systems 28 (2015).

[67] Yuchen Zhang, Xi Chen, Dengyong Zhou, and Michael I Jordan. 2014. Spectral methods
meet EM: A provably optimal algorithm for crowdsourcing. Advances in neural information
processing systems 27 (2014).

[68] Yudian Zheng, Guoliang Li, Yuanbing Li, Caihua Shan, and Reynold Cheng. 2017. Truth
inference in crowdsourcing: Is the problem solved? Proceedings of the VLDB Endowment 10,
5 (2017), 541–552.

[69] Wenxuan Zhou, Hongtao Lin, Bill Yuchen Lin, Ziqi Wang, Junyi Du, Leonardo Neves, and
Xiang Ren. 2020. Nero: A neural rule grounding framework for label-efﬁcient relation extraction.
In Proceedings of The Web Conference 2020. 2166–2176.

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s

contributions and scope? [Yes]

(b) Did you describe the limitations of your work? [Yes] See Appendix A.1.
(c) Did you discuss any potential negative societal impacts of your work? [Yes] See

Appendix A.2.

(d) Have you read the ethics review guidelines and ensured that your paper conforms to

them? [Yes]

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [Yes] See Appendix B

and Appendix C.

(b) Did you include complete proofs of all theoretical results? [Yes] See Appendix B and

Appendix C.

3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] In supplemental
material.

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they

were chosen)? [Yes] See Appendix F and Appendix G.

14

(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [Yes] We report the scores averaged over datasets with error
bars for label aggregation in the last column in Table 2. We plot the error bars for
semi-supervised label aggregation in Figure 2. We report the scores averaged over
datasets with error bars for the end model in the last column in Table 4.

(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] See the Hardware paragraph in
Appendix G.

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...

(a) If your work uses existing assets, did you cite the creators? [Yes] See Table 1.
(b) Did you mention the license of the assets? [Yes] See readme in supplemental material.
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
(d) Did you discuss whether and how consent was obtained from people whose data you’re

using/curating? [Yes] See the Datasets paragraph in Appendix G.

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable
information or offensive content? [Yes] See the Datasets paragraph in Appendix G.

5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [N/A]

(b) Did you describe any potential participant risks, with links to Institutional Review

Board (IRB) approvals, if applicable? [N/A]

(c) Did you include the estimated hourly wage paid to participants and the total amount

spent on participant compensation? [N/A]

A Limitations and Broader Impact

A.1 Limitations

In certain cases, one might be able to know the quality of the LFs or the dependency structure of the
LFs as a prior. Our method would not be able to incorporate the prior information. It would be an
interesting future work to extend our method to be able to accept these types of prior information as
additional input. Another limitation is memory consumption during inference. For some existing
methods, they can ﬁrst perform unsupervised learning on some data points to learn the model
parameters, then inference can be done one data point by one data point. Therefore, these methods
have low memory consumption during inference. In contrast, our method performs inference in a
single forward pass for all data points all in once. When the dataset is extremely huge, inference
might not ﬁt in GPU memory. One can alleviate the issue by performing inference on CPU, or one
can divide the dataset into smaller batches and perform inference batch by batch.

A.2 Broader Impact

The lack of labeled training data is a major impediment for the adoption of machine learning
techniques to a boarder range of applications. Weak supervision is a promising direction to lift
the impediment. Our work signiﬁcantly improves the accuracy and efﬁciency of label aggregation
in weak supervision. We hope our work will enable more applications to use machine learning
techniques.

Potential negative social impact: since weak supervision is able to create labeled data from unlabeled
data easily, the tech companies will have the incentive to collect more user data which might have a
negative impact on privacy.

B Optimalities of the Uniform Distribution

We aim to approximate an unknown distribution p(y|X) (which can be different in different applica-
tions) with an ﬁxed distribution q(y|X). Since both distributions are deﬁned on a ﬁnite set U (X),
we can use the probabilities of the elements in U (X) to represent each of the two distributions.

15

Speciﬁcally, we represent p(y|X) as p = {p1, . . . , p|U (X)|} and q(y|X) as q = {q1, . . . , q|U (X)|}.
Similarly, we denote the uniform distribution (i.e. p(cid:48)(y|X) = 1
|U (X)| ) as u = {u1, . . . u|U (X)|}.
Apparently, ∀i, we have 0 ≤ pi ≤ 1, 0 ≤ qi ≤ 1, ui =
i qi = 1, and
(cid:80)
i ui = 1. Using the uniform distribution u to approximate p is the optimal in both the worst case
and the average case. The two optimalities are formally deﬁned as the following:

i pi = 1, (cid:80)

|U (X)| , (cid:80)

1

(1) Worst-case Optimal: The uniform distribution u has the minimum maximum distance to the
unknown distribution p:

u = argmin

q

max
p

dist(p, q)

(6)

where "dist" can be the KL divergence or Lα distance ∀α > 1. Note the conventional name is "Lp"
distance, but to avoid reusing the same notation p for different meanings, we use the name "Lα"
distance instead.

(2) Average-case Optimal: The uniform distribution u has the minimum expected KL divergence
to the unknown distribution p under a mild assumption. Let P(p) denote the probability of the
unknown distribution being a speciﬁc distribution p (e.g. P(u) would be denoting the probability of
the unknown distribution being the uniform distribution i.e. p(p = u)). Formally:

u = argmin

q

Ep[KL(p, q)] = argmin

q

(cid:90)

p

KL(p, q)P(p)dp

under the assumption that P(p) is centrally symmetric, formally:

(cid:90)

p

P(p)pdp = u

We provide a formal proof for the two optimalities in the following:

Proof for Worst-case Optimal.

Proof. We ﬁrst prove Equation 6 for KL divergence.

max
p

KL(p, q)

= max
p

= max
p

(cid:88)

i
(cid:88)

i

pi log

pi
qi

pi log pi + pi log

1
qi

(7)

(8)

(9)

The maximum of the ﬁrst term is zero, as pi log pi ≤ 0 due to pi ≥ 0 and log pi ≤ 0. The maximum
is obtained when there is a j such that pj = 1 and pi = 0, ∀i (cid:54)= j;
j also comes into play in the maximum of the second term. We have (cid:80)
(cid:80)
which is obtained when j = arg maxi log( 1
qi

≤
. Therefore, the maximum of of the second term is maxi log 1
qi

), pj = 1 and pi = 0, ∀i (cid:54)= j.

i pi maxk log 1
qk

= maxk log 1
qk

i pi log 1
qi

We can see that the maximum of both terms is achieved at the same time with j = arg maxi log( 1
qi
pj = 1 and pi = 0, ∀i (cid:54)= j. The maximum value is maxi log 1
qi

.

),

Therefore,

max
p

KL(p, q) = max

i

log

1
qi

= log

1
mini qi
≥ log |U (X)|

(10)

|U (X)| (otherwise, (cid:80)
The inequality is because mini qi ≤ 1
|U (X)| > 1). The equality
of the inequality is obtained when q is the uniform distribution, i.e.q = u. Therefore, u =
argminq maxp KL(p, q).

i qi > (cid:80)

1

i

16

Next, we prove Equation 6 for Lα distance ∀α > 1.

The Lα distance is deﬁned as:

Lα(p, q) = (

(cid:88)

|pi − qi|α)1/α

i

(11)

Take the derivative of Lα(p, q) with respect to a pi:

(cid:26) 1

=

∂Lα(p, q)
pi

i |pi − qi|α)1/α−1α(pi − qi)α−1 if pi ≥ qi

α ((cid:80)
α ((cid:80)
− 1
This means if pi − qi ≥ pj − qj, ∂Lα(p,q)
≥ ∂Lα(p,q)
. Therefore, replacing pi, pj with pi + δ, pj − δ
pj
where δ > 0 increases Lα(p, q) and eventually replacing pi, pj with pi + pj, 0 increases Lα(p, q).
Let k = arg maxi pi − qi. For each pair (pk, pi)i (cid:54)= k, we replace pk to be pk + pi and pi to be 0
and eventually we have pk = 1 and pi = 0, i (cid:54)= k:

i |pi − qi|α)1/α−1α(qi − pi)α−1 otherwise

(12)

pi

(cid:88)
(

i

|pi − qi|α)1/α ≤ (

(cid:88)

|qi|α + |1 − qk|α)1/α

(13)

i(cid:54)=k

Apparently, when k = arg mini qk, the right hand side is further maximized. Without loss of
generality, we can assume q1 ≤ q2 ≤ · · · ≤ q|U (X)|. Therefore:

max
p

Lα(p, q) = ((1 − q1)α +

i )1/α
qα

(cid:88)

i>1

By the Hölder’s inequality [24]:

(cid:88)

i>1

i ≥ (|U (X)| − 1)1−α(
qα

(cid:88)

qi)α = (|U (X)| − 1)1−α(1 − q1)α

i>1

where equality in the inequality is obtained when q2 = · · · = q|U (X)|. Therefore:

Lα(p, q) ≥((1 − q1)α + (|U (X)| − 1)1−α(1 − q1)α)1/α

max
p

(14)

(15)

≥((1 −

1
|U (X)|
|U (X)| − 1
|U (X)|

)α + (|U (X)| − 1)1−α(1 −

1
|U (X)|

)α)1/α

(16)

)α +

|U (X)| − 1
|U (X)|α )1/α

=((

where the second inequality is because ((1 − q1)α + (|U (X)| − 1)1−α(1 − q1)α)1/α monotonically
decreases as q1 increases and we have q1 ≤ 1
|U (X)| because q1 is the minimum in q, i.e. q1 ≤ q2 ≤
. . . q|U (X)|.

In summary, the minimum of maxp Lα(p, q) is obtained when q2 = q3 = · · · = q|U (X)| and
q1 =
|U (X)| . In other words, q = u.
Therefore, u = argminq maxp Lα(p, q)

|U (X)| , which means q1 = q2 = q3 = · · · = q|U (X)| =

1

1

Proof for Average-case Optimal.

Proof.

Ep[KL(p, q)] =

=

=

(cid:90)

p
(cid:90)

p
(cid:90)

p

KL(p, q)P(p)dp

P(p)

P(p)

(cid:88)

i
(cid:88)

i

pi log

pi
qi

dp

pi log pidp −

17

(17)

(cid:90)

p

P(p)

(cid:88)

i

pi log qidp

Since the ﬁrst term is irrelevant to q, we have:

Ep[KL(p, q)] =constant −

(cid:90)

(cid:88)

P(p)

pi log qidp

=constant −

=constant −

i
(cid:90)

p

log qi

ui log qi

p

(cid:88)

i
(cid:88)

i

P(p)pidp

(18)

where the last equation is by the assumption that P(p) is centrally symmetric, i.e. (cid:82)
Therefore:

p P(p)pdp = u.

Ep[KL(p, q)] =constant −

(cid:88)

i

ui log qi

=constant −

=constant −

≥constant −

1
|U (X)|

1
|U (X)|

1
|U (X)|

(cid:88)

i

log qi

log

(cid:89)

qi

i

(cid:80)

i qi
|U (X)|

log((

(19)

)|U (X)|)

=constant + log(|U (X)|)

where the inequality is the inequality of arithmetic and geometric means. The equality of the
inequality is obtained when q1 = q2 = · · · = q|U (X)| =
|U (X)| , i.e. q = u. Therefore, u =
argminq Ep[KL(p, q)].

1

C Expected Label Vector Minimizes Cross Entropy Loss

When training a model h(X) on data set D = {(X1, y1), . . . }. When |D| → +∞, for each X:
(cid:88)

h(X) =

pM (y|X)y

(20)

minimizes the cross entropy loss function.

y∈U (X)

Proof. For each X, let D(X) denote the subset {(X, y(cid:48)
loss on D(X) is:

1), (X, y(cid:48)

2), . . . } of D. The cross entropy

|D(X)|
(cid:88)

n
(cid:88)

−

i=1

j=1

i[j] log(h(X)[j]) + (1 − y(cid:48)
y(cid:48)

i[j]) log(1 − h(X)[j])

(21)

where n denotes the number of rows in X. By taking derivative and setting it to zero, the above
equation is minimized when:

h(X)[j] =

, ∀j

(22)

(cid:80)|D(X)|
y(cid:48)
i[j]
i=1
|D(X)|

By the law of large numbers [18], when |D| → +∞ (so that |D(X)| → +∞), h(X)[j] =
(cid:80)|D(X)|
y(cid:48)
i=1
|D(X)|
y∈U (X) pM (y|X)y.

y∈U (X) pM (y|X)y[j] for ∀j.

= E(y[j]|X) = (cid:80)

This means h(X) =

(cid:80)

i[j]

D Probability of Generating a Valid Pair

To simplify our analysis, in the following, we only consider y that contains both −1 and +1, which
has a probability p0 = 1 − 2
2n . p0 ≈ 1 when n ≥ 100 (When generating data, we sample n from
[Ln, Hn] = [100, 2000] which we explain in Appendix F).

18

Let S denote the set of all possible pairs of (X, y). S is made up by three subsets: U =
{(X, y)|σ(X, y) = 1}, Se = {(X, y)| (cid:80)m−1
j=0 g(X, y, j, 1) =
m
2 } and Sc = S − U − Se. Apparently Sc is also made up by three subsets, i.e. Sc =
Sc1 ∪ Sc2 ∪ Sc3 where Sc1 = {(X, y)| (cid:80)m−1
j=0 g(X, y, j, 1) <
2 }, Sc2 = {(X, y)| (cid:80)m−1
m
2 } and Sc3 =
{(X, y)| (cid:80)m−1

j=0 g(X, y, j, −1) < m
2 and (cid:80)m−1
j=0 g(X, y, j, 1) < m

2 and (cid:80)m−1
j=0 g(X, y, j, 1) > m

j=0 g(X, y, j, −1) < m

j=0 g(X, y, j, −1) = m

j=0 g(X, y, j, −1) > m

2 or (cid:80)m−1

2 and (cid:80)m−1

2 }.

Lemma 2. |Sc1 | = |Sc2 | = |Sc3 | = |U |.

Proof. For each element (X, y) in Sc1, (cid:80)m−1
j=0 g(X, y, j, 1) < m
2 .
We can ﬂip X[y = 1, :] to be −X[y = 1, :] and ﬂip X[y = −1, :] to be −X[y = −1, :]. After
ﬂipping, we obtain pair (X (cid:48), y), and apparently (X (cid:48), y) ∈ U . This means for each element in Sc1
there is a corresponding element in U , so we have |Sc1 | ≤ |U |. Similarly, for each element in U ,
we can do ﬂipping to get an element in Sc1, so we also have |U | ≤ |Sc1|. Therefore, |U | = |Sc1 |.
Similarly, one can show that |U | = |Sc2| and |U | = |Sc3 |.

j=0 g(X, y, j, −1) < m

2 and (cid:80)m−1

By Lemma 2, |Sc| = |Sc1 | + |Sc2| + |Sc3| = 3|U |. When m is odd, apparently, |Se| = 0. Therefore,
the probability of a randomly generated pair being valid is:

p((X, y) ∈ U |m) =

|U |
|S|

=

|U |
|U | + |Se| + |Sc|

=

1
4

(23)

Next, we consider when m is even. To simplify our analysis, approximately, p(g(X, y, j, −1)) = 1
2
and p(g(X, y, j, 1)) = 1
2 . This is because the probability that the number of correct elements exactly
equal to the number of incorrect elements for each class is extremely small due to n being relatively
large. Therefore, we have:

p((X, y) ∈ Se|m) =

(cid:1)
(cid:0) m
m/2
2m

Therefore:

p((X, y) ∈ U |m) = (1 −

(cid:0) m
(cid:1)
m/2
2m )

1
4

(24)

(25)

Since m is uniformly sampled from [Lm, Hm] = [2, 60] (which we explain in Appendix F), we have:
(cid:0) m
(cid:1)
m/2
2m )

p((X, y) ∈ U ) =

1
Hm − Lm

1
Hm − Lm

(1 −

(cid:88)

(cid:88)

1
4

+

1
4

m,m%2=1,Lm≤m≤Hm

m,m%2=0,Lm≤m≤Hm

(cid:88)

m,m%2=0,Lm≤m≤Hm

1
Hm − Lm

(1 −

(cid:0) m
(cid:1)
m/2
2m )

1
4

=

1
2

×

1
4

+

≈0.232

This means the probability of generating a valid pair in one trial is about 0.232.

(26)

E Discussions

E.1 The Proposed Architecture Satisﬁes the Three Properties

To see how the proposed architecture in Figure 1 satisﬁes the three properties mentioned in the
begining of Section 4.2. First, GNN accepts arbitrary input size, so X can be of any size; Second,
GNN is permutation equivariant to the nodes, so the output embeddings of GNN are equivariant
to the permutation of data points and LFs. After average pooling for each data point over all LFs
(each SCC with dashed blue edges), the network is invariant to the permutation of LFs and is still
equivariant to the permutation of data points.

19

E.2 Crowdsourcing Methods for Weak Supervision

The two crowdsourcing methods have the worst performance in Table 2. The reason that crowdsourc-
ing methods don’t work well on weak supervision datasets has not been investigated or discussed
in prior work, and we provide our conjecture. First, the label matrix in crowdsourcing tends to be
extremely sparse as there can be many crowd workers while each crowd worker might annotate a
few data points then quit [68]; In contrast, in weak supervision, each LF is applied to each data point.
Second, since crowd workers are humans, the labels provided by the crowd workers tend to have
higher accuracy; In contrast, a LF when applied on data unseen by the LF developer can predict very
noisy labels. In other words, the existing crowdsourcing methods are designed to work in the sparse
scenario with weak labels of higher accuracy, so that they don’t work well in the weak supervision
setting with a denser and noisier label matrix.

F Implementation Details of LELA

Data Generation. When generating each pair (X, y), we ﬁrst randomly generate n and m, the
number of rows/columns of matrix X. Note n is the number of data points and m is the number of
LFs. As we mentioned, we ﬁrst sample n and m uniformly from [Ln, Hn] and [Lm, Hm] respectively.
We set [Ln, Hn] = [100, 2000] where Ln = 100 is because typically there are at least hundreds of
data points otherwise it is not necessary to write LFs as one can just manually label all data points and
we set Hn = 2000 due to memory limit during model training. We set [Lm, Hm] = [2, 60] where
Lm = 2 is because when there is only one LF there is no need to aggregate and we set Hm = 60 due
to memory limit during model training; We highlight our trained model generalizes well to number of
LFs and number of data points (see Table 1) that are not in the region [Lm, Hm] and [Ln, Hn] as we
have shown in experiments. Once we have n and m, we invoke the method mentioned in Section 4.1
to generate (X, y).

Since our data is synthetically generated, there is no need to generate a ﬁxed training set. Our training
data is generated on the ﬂy, i.e. during training when the data loader fetches the next pair of (X, y), a
new pair is immediately generated and returned.

Model Architecture. We implement our model architecture in Pytorch [45]. We use K = 4 layers
of GNN. The embedding dimension of GNN is 32, i.e. each node in the graph is encoded with a 32
dimensional embedding. The ﬁnal MLP consists of three linear layers; the ﬁrst two linear layers use
Relu activation and the last linear layer uses Sigmoid activation.

Model Training. We use the Adam optimizer [31]. We set amsgrad to be true for better conver-
gence [49] and keep all other parameters as the default values provided by Pytorch (e.g. learning rate
lr = 0.001). We use a batch size of 50, i.e. each batch consists of 50 pairs of generated (X, y). We
tested different batch sizes of 16 and 250 and observed no meaningful difference. We train our model
until training loss converges (loss doesn’t decrease in 104 iterations), which takes about one day with
5 × 104 iterations on a K80 GPU. Note one iteration means one gradient update/one batch, and we
don’t have the notion of epoch as training data is generated on-the-ﬂy for each batch.

Validation. We also need to prevent our model from overﬁtting the training set. We highlight that,
different from typical ML settings where one gets access to a validation set that is similar to the test
set, in our setting we have no validation set that is similar to the test set. Again, when training our
model, the real test datasets are unseen and we only have access to synthetic data. Our intuition is
that when the model overﬁts the sythetically generated training set D, its performance will be poor
on data that is different from the training set, for example, on another sythetic dataset D(cid:48) that is
generated in a different way. We synthetically generate the validation set D(cid:48) with size |D(cid:48)| = 100
according to the generation method proposed in [65]; In this method, LFs are independent from each
other conditioned on the ground-truth label.

We note that the way we use the validation set is also different from a typical setting. We train the
model until training loss converges (this typically requires about 5 × 104 iterations), and repeat 10
runs (i.e. train our model 10 times from scratch). We then select the run with the highest averaged
validation accuracy over all iterations (as validation accuracy might ﬂuctuate over iterations); We
use the learned model at the ﬁnal iteration of the selected run in our experiments. We provide our
reasoning of doing this: (1) We do not use the validation set to do early stopping (i.e. to select the best
iteration in a run). In a typical ML setting, the validation set is used to select the best epoch/iteration.

20

Figure 3: Accuracy on the synthetic validation set D(cid:48) vs number of training iterations. The purple
line and yellow line are two different runs. The yellow run is selected as it is more stable with a
higher averaged validation accuracy.

This is possible because in a typical ML setting the validation set is similar to the test set and the
validation set provides very strong signal towards which iteration is a good iteration for the test set.
In our case, the validation set D(cid:48) can be very different from the test set, thus the selected iteration
based on D(cid:48) might not be a good iteration for the test set. (2) We use the validation set to select the
best run. We observed that at different runs, the curve of validation accuracy vs number of iteration
can be different (e.g. the two runs in Figure 3), so the test accuracy of the model in different runs
can be different. We would like to select the best run using the validation set D(cid:48). Intuitively, one
run with better validation accuracy on average over all iterations is stably better (e.g. the yellow run
in Figure 3), so we select the run with an best averaged validation accuracy over iterations. As an
example, for the two runs in Figure 3, although the highest validation accuracy of purple run can be
higher than that of the yellow run, the yellow run has a higher averaged validation accuracy over
iterations and is much more stable, so we select the yellow run. We also observed this run to have
a less degree of ﬂuctuation in validation accuracy, as shown in Figure 3. This suggests the model
converges at a ﬂat minima, which is known to generalize better [34, 30, 28].

One natural question is that why it is possible to select the best run but it is not possible to select the
best iteration. The reason is that selecting the best run out from 10 runs require much less information
than selecting the best iteration out from 5 × 104 iterations. Since the validation set D(cid:48) can be very
different from the test set, the information provided by D(cid:48) is very limited.

An interesting phenomenon in the validation accuracy curve in Figure 3 for the yellow run is
that validation accuracy ﬁrst increases then decreases and ﬁnally increases. This is known as the
Epoch-wise double descent phenomenon [44].

G Implementation Details of Experiments

Hardware. All of our experiments were performed on a machine with a 2.20GHz Intel Xeon(R)
Gold 5120 CPU, a K80 GPU and with 96GB 2666MHz RAM.

Datasets. We use the datasets prepared by the wrench benchmark on Github [3, 65]. All the datasets
and LFs are publicly released by previous work [65]. All datasets do not contain any personally
identiﬁable information [65].

Originally, each dataset include three ﬁles "train.json", "valid.json" and "test.json". Following the
suggestion in a reported issue of the wrench benchmark [2], we combine all three ﬁles to get a
single matrix X and single ground-truth label vector y for the experiments on label aggregation. We
then split the datasets using the original split for the experiment on end model (Section 5.4). The
information of the LFs as well as the raw data for each dataset can be found in the wrench benchmark
project on Github [3].

Baselines. For each baseline, we use existing open-source implementations. The implementations of
DS, DP, FS, MeTaL, EBCC, and NPLM are from [57], [56], [26], [55], [63], and [9] respectively.
For baselines that require class weights as priors, we report the best results from using uniform
weights and using the weights estimated by majority vote.

Setup in Semi-supervised Label Aggregation. When sampling Ngt data points as the data points
with known labels, we make sure that each class has at least two data points. For random forest, we
use the scikit-learn implementation [1]. When training the random forest classiﬁer, we use ﬁve fold

21

cross validation to perform grid search for the "max_depth" parameter in range [2, 4, 8, 16, 32, None]
and the "min_samples_split" parameter in range [2, 5]. We repeat ﬁve runs and report results with
error bars in Figure 2.

Setup in Ablation Study. For the naive model architecture, the input is a ﬂattened vector of a ﬁxed
size matrix 2000 × 50 and the network has several linear layers.

Setup in End Model Experiment. When training the end model, the training set and validation set
both use generated labels by each method and the test set uses ground-truth labels. For the two end
model MLP and BERT, we use the implementation provided by the benchmark [3, 65]. We use grid
search to tune hyper-parameters for each end model based on validation set performance. We use the
same search space as the benchmark [65], as summarized in Table 5. We repeat ﬁve runs and report
the scores averaged over runs in Table 4.

Table 5: Hyper-parameters and search space for the end models.

End Model Hyper-parameter Description

Range

MLP

BERT

batch_size
lr
weight_decay
ffn_num_layer
ffn_hidden_size

batch size
learning rate
weight decay
number of MLP layers
hidden size of MLP layers

32,128,512
1e-5,1e-4,1e-3,1e-2,1e-1
1e-5,1e-4,1e-3,1e-2,1e-1
2
100

batch_size
lr

batch size
learning rate

16,32
2e-5,3e-5,5e-5

H Additional Experiment Results

H.1 Running time

Table 6 shows the running time of all label aggregation methods on all datasets. When measuring the
running time, we use GPU for methods that support GPU (MeTaL, NPLM, and LELA).

Table 6: Running time (seconds) of label aggregation on all datasets

Dataset Census IMDB Yelp Youtube SMS Spouse CDR Commercial Tennis Basketball AGNews TREC SemEval ChemProt AVG.

MV

<0.1

<0.1

<0.1 <0.1

<0.1 <0.1

<0.1 <0.1

147.8

18.8

40.5 2.5

14.4 8.4

29.5 8.5

DP

FS

21.1

MeTaL 0.5

NPLM 15.7

EBCC 3.9

LELA 0.1

1.7

0.3

4.0

5.1

0.1

3.7 0.2

0.4 0.4

5.7 0.4

52.5 2.2

0.1 0.1

3.2

0.4

2.2

2.8

0.1

0.8

0.3

1.8

2.3

0.2

3.7

0.4

6.3

5.8

0.2

0.6

0.4

11.2

3.0

0.3

<0.1

10.0

0.6

0.4

1.5

2.5

0.2

<0.1

14.9

14.9

0.4

3.4

6.0

0.3

<0.1

<0.1

<0.1

<0.1

225.0

100.8 190.2

213.0

22.1

0.5

27.9

18.0

0.4

16.3

69.0

3.6

5.4

9.0

0.2

4.6

3.4

9.8

0.3

26.4

3.6

12.1

84.8

0.2

<0.1

73.2

12.2

1.2

7.2

14.8

0.2

H.2 Adversarial LFs

We test the behavior of all methods with the presence of noisy/adversarial LFs that are worse than
random. Following the setup in prior work [51], we create an adversarial LF for each dataset, then
duplicate it βm times (m is the original number of LFs) and append the labels of the βm LFs to
the label matrix. We vary β from 0 to 1 and report the label aggregation results in Figure 4. As
β increases, the performance of all methods decreases. In particular, the performance of the best
performing baseline MeTaL decreases more signiﬁcantly than other baselines. LELA always achieves
the best results.

22

Figure 4: Averaged score vs amount of adversarial LF duplicates.

23

