ACCEPTED FOR PUBLICATION IN IEEE ROBOTICS AND AUTOMATION LETTERS. DIGITAL OBJECT IDENTIFIER 10.1109/LRA.2021.3062798

1

Exploiting Natural Language for EfÔ¨Åcient
Risk-Aware Multi-robot SaR Planning
Vikram Shree‚àó, Beatriz Asfora‚àó, Rachel Zheng, Samantha Hong, Jacopo BanÔ¨Å, and Mark Campbell

1
2
0
2

r
p
A
8

]

O
R
.
s
c
[

1
v
9
0
8
3
0
.
4
0
1
2
:
v
i
X
r
a

Abstract‚Äî The ability to develop a high-level understanding
of a scene, such as perceiving danger levels, can prove valuable
in planning multi-robot search and rescue (SaR) missions. In
this work, we propose to uniquely leverage natural language
descriptions from the mission commander in chief and image data
captured by robots to estimate scene danger. Given a description
and an image, a state-of-the-art deep neural network is used
to assess a corresponding similarity score, which is then con-
verted into a probabilistic distribution of danger levels. Because
commonly used visio-linguistic datasets do not represent SaR
missions well, we collect a large-scale image-description dataset
from synthetic images taken from realistic disaster scenes and use
it to train our machine learning model. A risk-aware variant of
the Multi-robot EfÔ¨Åcient Search Path Planning (MESPP) problem
is then formulated to use the danger estimates in order to
account for high-risk locations in the environment when planning
the searchers‚Äô paths. The problem is solved via a distributed
approach based on Mixed-Integer Linear Programming. Our
experiments demonstrate that our framework allows to plan safer
yet highly successful search missions, abiding to the two most
important aspects of SaR missions: to ensure both searchers‚Äô
and victim safety.

Index Terms‚ÄîMulti-robot systems, multi-modal perception for

HRI, search and rescue robots.

I. INTRODUCTION

A CCURATE scene awareness is the keystone for success

in search and rescue (SaR) missions. The deployment
of robots in the World Trade Center disaster pinpointed
limitations in different aspects of robotic systems, including
human robot collaboration [1]. For the past
two decades,
sensors in particular have evolved in number and variety, and
are now able to generate gigabytes of data within seconds,
and even extract important features autonomously. However,
rigorous analysis and summarizing of the large amount of
data about a scene in order to infer high-level understanding
of the surrounding world is still a work in progress. Errors

*Equal contribution, order decided randomly. All authors are with the Sibley
School of Mechanical and Aerospace Engineering, Cornell University, Ithaca,
NY USA. Email: {vs476, ba386, rz246, sh974, jb2639,
mc288}@cornell.edu.

Research supported by the NRI program of the National Science Founda-

tion, award #1830497.

Cite as: V. Shree, B. Asfora, R. Zheng, S. Hong, J. BanÔ¨Å and M. Campbell,
‚ÄúExploiting Natural Language for EfÔ¨Åcient Risk-Aware Multi-Robot SaR
Planning,‚Äù in IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 3152-
3159, April 2021.

The ofÔ¨Åcial IEEE published version of this manuscript can be accessed at:

https://ieeexplore.ieee.org/abstract/document/9366368

¬©2021 IEEE. Personal use of this material is permitted. Permission from
IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works.

Figure 1: Scene danger perception and planning pipeline. A set of
descriptors and corresponding danger levels, provided by the mission
commander, is used for estimating danger probability distribution, on
a 5-point scale. This information is used by the planning module to
plan safe paths for the agents.

propagate across and down the multirobot system pipeline,
with detrimental impact to the SaR mission performance.

Prior work in robotic perception has focused on inferring
low-level information about the environment, for example,
building occupancy grid maps [2] or mapping unique land-
marks [3]. These low-level attributes are indeed relevant to
build a representation of the world that is suitable for naviga-
tion. However, planning for a team of agents typically requires
humans to make decisions based on high-level attributes of the
environment. These include the notion of ‚Äúdanger‚Äù, for which
the ability to map low-level aspects of the scene into a high-
level and succinct representation is still an open question in
the robotics community. In this work, we address this problem
by proposing a systematic approach for inferring scene danger
from visio-linguistic inputs to enable high-level planning for
a team of agents.

they feel are important

Our approach, sketched in Fig. 1, asks users for descriptions
to characterize scene danger.
that
Descriptions are matched against images seen from the ro-
bot‚Äôs camera by leveraging a machine learning model, and
the obtained similarity scores are used to keep up-to-date a
probability distribution describing danger in different areas of
the environment. This information, in turn, is used to plan
more informed paths for the searchers. In summary, this paper
makes the following novel contributions:

1) The adaptation of a state-of-the-art deep neural network
[4] to assess similarity between the descriptions and
images in the scene. To facilitate the use of the network
in SaR missions, we introduce a novel dataset, consisting
of language descriptions for synthetic disaster scenes
taken from the DISC dataset [5].

Danger-LevelLanguage Descriptors5Building is burning with huge flames.4Thick black smoke in a room.3Collapsed ceiling with a pile of debris. 2Objects are scattered on the floor.1A window from which sky is visible.Machine Learning ModelProbabilistic ModelSimilarity Scoresùúâ1ùúâ2ùúâ3ùúâ4ùúâ5SceneDanger ProbabilityùúÇ1ùúÇ2ùúÇ3ùúÇ4ùúÇ5Commanderin chiefPlanning 
 
 
 
 
 
2

ACCEPTED FOR PUBLICATION IN IEEE ROBOTICS AND AUTOMATION LETTERS. DIGITAL OBJECT IDENTIFIER 10.1109/LRA.2021.3062798

2) An intuitive probabilistic model for estimating scene
danger by fusing similarity scores obtained from various
user descriptions and multiple images at a scene. Com-
pared against a system (e.g. a classiÔ¨Åer) that is directly
trained on a priori notions of ‚Äúdanger‚Äù, our approach
can adapt to the needs of different missions (e.g. Ô¨Åre,
earthquake, or radioactive incident) without having to
change any of its parameters.

3) The introduction of a risk-aware version of the Multi-
robot EfÔ¨Åcient Search Path Planning (MESPP) prob-
lem [6] which can leverage scene understanding to
ensure agents‚Äô safety by accounting for each hetero-
geneous agent‚Äôs danger tolerance. This is an online
problem, which we solve via a distributed planning
approach based on variants of state-of-the-art Mixed-
Integer Linear Programming (MILP) models [7].

Extensive numerical and realistic ROS/Gazebo simulations
show that our holistic approach to multi-robot SaR planning
enforces the safety of heterogeneous agents under different
notions of risks speciÔ¨Åed by the user, while maintaining
performance in terms of time required to locate the victim.

II. RELATED WORK

A. Collaborative search and rescue missions

The challenges encountered in SaR missions depend upon
the operational environment, which can be divided into three
main categories: maritime, urban, and wilderness [8]. In this
paper, we focus on SaR in urban environments. Robots can
go into more dangerous areas to avoid risking human lives,
however they still need to leverage the human expertise for
scene understanding. Yazdani et al. [9] studied how high-level
instructions from a human can be used to plan actions for
the robot during SaR missions. In order to bridge the gap
between scene perception and decision-making, researchers
have proposed ontology models [10] that consist of rules to
represent the environment and action space. However, these
approaches have been only shown to work in simple settings
and are prone to failure in more complex, uncertain scenarios,
which are often encountered in SaR missions.

B. Language-based scene assessment

In a typical rescue mission, human rescuers talk with each
other on a low-bandwidth radio channel since it has virtually
unlimited range. The human brain is great at interrelating data
from visual and language domains, but when it comes to neural
networks, this task becomes more challenging due to the lack
of a one-to-one relationship between the two inputs.

Modern deep neural architectures [4], [11] tend to address
this challenge by extracting high-level features from each one
of the inputs before jointly reasoning about them. The beneÔ¨Åts
of these models, however, have only been tested on datasets
describing serene environments (e.g. [12]) because of the
constraints associated with replicating a realistic SaR scenario.
In this work, we leverage photo-realistic synthetic images from
disaster scenes [5] to conduct a large-scale survey. This allows

us to obtain language data, which is vital for training state-of-
the-art visual-language models before applying them in SaR
missions.

Compared to other approaches for processing visio-lingual
data that can be found in the computer vision community,
like visual question answering [13] and visual commonsense
reasoning [14], caption-based image retrieval [15] is the most
relevant from the standpoint of this work. This refers to the
task of identifying an image from a large pool given a caption
describing its content, thus, requiring to estimate similarity
between images and text data. There is a rich line of work
towards mapping images and sentences to a common semantic
space for assessing image-text similarity. In this paper, we
compare the usage of SCAN [11] and ViLBERT [4] on a new
dataset built on top of the disaster scenes contained in the
DISC dataset [5], due to their superior performance reported
in the literature [4].

C. Multirobot search

We assess the advantages brought by our danger estimation
pipeline by formulating and evaluating a risk-aware version
of a famous robotic search problem, the Multi-robot EfÔ¨Å-
cient Search Path Planning (MESPP) problem introduced by
Hollinger et al. [6]. In the original version of the MESPP
problem, a team of robots is deployed in a graph-represented
environment with the aim of locating a moving non-adversarial
target within a given deadline. In this new online version,
graph vertices are associated with probabilistic danger estim-
ates, and robots are heterogeneous in terms of their tolerance
to hazardous conditions. This is different from the settings
that can be found in the literature, which either focus on
static threats [16], or consider dynamic threats but in presence
of a single agent that has to reach a given goal location
while maximizing its chances of survival (without optimizing
a second performance metric) [17].

III. PROPOSED ARCHITECTURE

Let us deÔ¨Åne Ô¨Åve intuitive danger levels: low, moderate,
high, very high, and extreme, and assign a value l ‚àà L to each,
respectively L = {1, ..., 5}. We assume that an experienced
user (the commander in chief) is able to provide at least
one sentence characterizing a description of each danger level
(e.g., ‚Äúa room is Ô¨Ålled with Ô¨Åre‚Äù, associated with l = 5).
Our pipeline consists of three layers, as shown in Fig. 2.
Layer I is tasked with estimating similarity between the user‚Äôs
descriptions and images collected in a scene, providing a
matching score for each image-description pair. The scores
of all images are combined into a probabilistic estimate of the
danger level (Layer II), which is in turn used for planning an
efÔ¨Åcient risk-aware search for the team (Layer III). This plan
is then sent to the team of robots operating in the environment,
where they search for victim and acquire new images. Each
layer is described in detail in the following sections.

IV. TEXT-IMAGE SIMILARITY ASSESSMENT

As a Ô¨Årst step to establish the user-perceived danger level
of the scene, we start with determining similarity between the

SHREE et al.: EXPLOITING NATURAL LANGUAGE FOR EFFICIENT RISK-AWARE MULTI-ROBOT SAR PLANNING

3

Figure 2: System layers: text-image similarity assessment (I), scene danger estimation (II) and multi-robot planning (III).

provided language descriptors and the scene. In this work, a
scene consists of a set of images acquired online by the robot.
The robot can compute a similarity score Œæ for each (image,
descriptor) pair. This can be done with one of the following
deep learning architectures.

1) SCAN: Stacked Cross Attention for Image-Text Match-
ing (SCAN) [11] uses a two-stage attention architecture for
calculating text-image similarity. First, the word and image are
converted into a set of word and image features, respectively.
This is followed by calculating a cosine similarity matrix for
each possible word-image feature pair. The Ô¨Årst attention stage
attends to image regions w.r.t each word in the sentence. The
next stage compares the words based on the attended image
vector and decides the importance of each word. The Ô¨Ånal
pooling layer outputs a similarity score.

2) ViLBERT: Visual-Language BERT (ViLBERT) [4] intro-
duces two separate streams for processing vision and language
data that can communicate with each other via co-attention
transformer layers. After having converted word and image
into features, ViLBERT processes the features with trans-
former blocks independently before they could interact with
each other. The co-attention layers produce attention features
for each modality conditioned on the other. The Ô¨Ånal pooling
layer outputs a similarity score.

A. A Novel Emergency Scene Dataset: DISC-L

The DISC-Language dataset1 is built on top of DISC
datset [5], consisting of 300K photo-realistic synthetic images
taken in several environments (like ofÔ¨Åce and subway) with
two types of damage conditions: collapse and Ô¨Åre. The images
are captured from a stereo-camera, moving along a pre-deÔ¨Åned
path in a 3D model world. To reduce redundancy in images,
we uniformly sample 1 out of every 150 images from the left
camera, yielding a set of 1000 images.

We conduct an online survey on Amazon Mechanical Turk
(AMT) to collect sentence descriptions for each image. AMT
is a global service enabling us to reach a huge participant
pool which favorably introduces diversity in the collected data.
Each task requires the user to read the instructions, look at
annotated examples, and describe important aspects of a given
image with two sentences in English. To incorporate diversity

1Freely available for academic purposes at

https://github.com/vikshree/DISC-L.git

Figure 3: A few sample descriptions for images in the DISC-L
dataset, collected through AMT survey.

in language, each image was labelled by four unique workers,
yielding a total of 4000 descriptions. Furthermore, to ensure
high quality language input from the users, the responses
are Ô¨Årst Ô¨Åltered automatically and then manually approved to
remove unsatisfactory descriptions. A few examples are shown
in Fig. 3. More examples of accepted and rejected responses
can be found on the DISC-L Github page.

In summary, DISC-L dataset consists of rich and diverse
descriptions of emergency situations from 720 unique Amazon
users. There are a total of 3,386 unique words in our proposed
dataset. The word-count ranges from 10 words to 70 words
per description, with a median of 20. A key observation is
that the most frequently used words (Ô¨Åre, smoke, Ô¨Çame, dark
etc.) are related to situations commonly encountered during
SaR mission.

B. Caption-based Image Retrieval Performance

1) Dataset: The DISC-L dataset is used to evaluate the
performance of SCAN and ViLBERT. The dataset is divided
into train, validation and test sets such that all images corres-
ponding to a environment remain in the same set. The train set
consists of 828 images from six environments; validation has
75 images from three environments; and test has 97 images
from two environments.

2) Training: SCAN is pre-trained for caption-based image
retrieval on the Flickr30K dataset [12]. A bottom-up attention
network [18] is used to compute a 36 √ó 2048 dimensional
feature vector for each image, before feeding it to the SCAN
network. Finally,
the data batch is shufÔ¨Çed online to get
negative sentence-image pairs, and a triplet loss function is
used to Ô¨Åne-tune the network on the DISC-L dataset. Triplet

For each imagePlanned PathImage Retrieval ModelImageSimilarity scoreDanger thresholds MILP modelTarget beliefAgent positionsSensor modelImage Danger ModelDanger Level Distribution…™v. EnvironmentGoal PointsVictimMarkov ModelSearch Team AgentsLocalizationMotion PlannerControlDynamicsA prioriOnlineCommander in chiefDescription + danger level pairsDescriptionsDanger level for each description…™. Image RetrievalFor each description …™…™. Estimation of Scene DangerFor each node…™…™…™. PlanningFor each time step‚Ä¢The view of a corridor where on the right you see windows with closed blinds.‚Ä¢In the middle of the corridor all the furniture is lying on the floor and overturned.‚Ä¢A number of houses could be seen along the roadside. The place looks green.‚Ä¢The houses are lit on fire and looks like a big accident has occurred.4

ACCEPTED FOR PUBLICATION IN IEEE ROBOTICS AND AUTOMATION LETTERS. DIGITAL OBJECT IDENTIFIER 10.1109/LRA.2021.3062798

Table I: SCAN vs ViLBERT models evaluated on DISC-L.

Model

SCAN

ViLBERT

Training type
Pre-trained
Fine-tuned on DISC-L
Pre-trained
Fine-tuned on DISC-L

R-1
7.5
7.7
9.7
19.4

R-5
24.0
30.4
39.3
52.0

R-10
39.2
48.2
59.7
70.4

loss is a ranking-based loss function, commonly used in
image-text matching task where distance between an anchor
is minimized from the truth value (see [11] for details).

ViLBERT is pre-trained for multiple visual-language tasks
on 12 different datasets [19]. A combination of pre-trained
Faster R-CNN and ResNet [20] is used to compute a 101 √ó
2048 dimensional feature vector for each image, which is fed
to the ViLBERT network. Following the authors‚Äô approach [4],
we adopt a 4-way multiple choice training process where for
each sentence-image pair in the dataset, three distractor pairs
are sampled with no correspondences. Finally, we Ô¨Åne-tune
the model on DISC-L dataset with a cross-entropy loss.

3) Results: We use standard rank-based performance met-
rics (R-1, R-5, R-10) to evaluate the models, where R-k
denotes the proportion of times the correct image is present
in the top k likely hypotheses for a description.

Table I shows the results. Fine-tuning the models on DISC-
L signiÔ¨Åcantly improves the retrieval performance for both
SCAN and ViLBERT, since the pre-trained models have never
encountered danger-related images or sentences. Also, we
observe that ViLBERT outperforms SCAN by a margin of
150%, 100% and 79% for R-1, R5 and R-10 respectively.
The superior performance of ViLBERT can be attributed to
its more complex model architecture and the use of higher
dimensional feature representation for the images. Therefore,
we use the ViLBERT model in the remainder of the paper.

V. SCENE DANGER ESTIMATION

We formulate the task of estimating a probability mass
function over the danger space L = {1, ..., 5}, given a set of
language descriptors from a user and a scene as a probabilistic
inference task. A scene consists of a set of r local images taken
from a given region of the environment: I = {i1, i2, . . . , ir}.
For notational simplicity, let us assume that a scene con-
sists of a single image. A single set of descriptions is not
sufÔ¨Åcient to assess the danger level in different hazard con-
ditions. Hence, our model allows the commander in chief
to specify multiple descriptions for each level, which can
then be grouped together based on m danger ‚Äútypes‚Äù. For
example, those related to descriptions of a Ô¨Åre. In symbols,
j , . . . , s5
S = {S1, S2, . . . , Sm}, where each set Sj = {s1
j }
contains a language descriptor for each of the 5 danger levels
related to the j-th danger type. Given a single image i and
the full set of language descriptors S, the ViLBERT model
is used to generate a matrix of similarity scores, denoted
by Œû = (cid:8){Œæ1
m}(cid:9). To reduce the
impact of noise, the scores are converted into one-hot vectors
y = {y1, . . . , ym} based on a threshold Œ∏, such that:

1}, . . . , {Œæ1

m, . . . , Œæ5

1, . . . , Œæ5

u = 1 if Œæl
yl

u ‚â• Œ∏ else 0 ‚àÄ l ‚àà L, u ‚àà {1, . . . , m}.

(1)

Figure 4: Graph abstraction of the school environment. Each
vertex v = 1, . . . , 46 is color-coded based on its danger level
in NFF scenario. Left: Human reasoned ground truth; Right:
Estimate using 5% of images and Ô¨Åre descriptors.

The posterior distribution of danger
the scene
P (D|Œû, S, I), denoted by the vector Œ∑ = [Œ∑1 Œ∑2 ¬∑ ¬∑ ¬∑ Œ∑5], is
calculated based on the frequency of samples in the data as:

level

for

Œ∑l =

1
œÅ

m
(cid:88)

u=1

yl
u ‚àÄl ‚àà L,

(2)

u=0

(cid:80)

l‚ààL yl

where œÅ = (cid:80)m
u is the normalizing constant. For
the more general case where a scene consists of r images, we
simply include one-hot vectors obtained from each one of the
images while calculating Œ∑l in Eq. (2).

To evaluate our danger estimation approach, we use the
School environment from the DISC dataset [5] because of its
relatively larger size and create the graph structure shown in
Fig. 4, containing n = 46 vertices. Each vertex in the graph is
associated with a set of local images collected from its imme-
diate surroundings. The DISC dataset allows each vertex to be
deÔ¨Åned with images containing three different types of danger:
none (N), collapse (C), and Ô¨Åre (F). Three ‚Äòenvironments‚Äô
are designed for simulation based on a predeÔ¨Åned proportion
of hazards: ‚ÄòNCF‚Äô denotes environments where vertices are
associated with hazardous images in equal proportion; ‚ÄòNFF‚Äô
denotes environments with no C-type danger and 2/3 of ver-
tices with F-type danger, etc. We introduce a set of descriptions
for assessing collapse and Ô¨Åre danger (see the DISC-L dataset
Github page for details). For example, the description ‚ÄúThe
room is engulfed in huge Ô¨Çames‚Äù describes a Ô¨Åre danger level
l = 5. Ground truth danger values lv for each vertex v are
obtained by having a human user reason about danger based
on the descriptions and the images associated with vertex v.
We use the average Bhattacharya coefÔ¨Åcient (BC) [21], a
standard metric to quantify the disparity between two discrete
probability distributions,
to measure the closeness of the
estimates with the ground truth, computed as:

BC =

1
n

n
(cid:88)

‚àö

v=1

Œ∑lv,v,

(3)

where Œ∑lv,v is the probability corresponding to the ground-
truth danger level lv at vertex v. The results are shown in
Table II. First, we observe that the danger estimates for all
environments are most accurate when the descriptions corres-
ponding to that speciÔ¨Åc environment are used. For example,
if it is a Ô¨Åre hazard (NFF), using only Ô¨Åre-related descriptions
yields the best danger estimates (BC = 0.63). If there are

12345678910111213141516171819202122232425262728293031323334353637383940414243444546LowModerateHighVeryHighExtremeGYMCAFEHALL2CLASSDCLASSCCLASSBCLASSASTGEWC1WC2HALL112345678910111213141516171819202122232425262728293031323334353637383940414243444546GYMCAFEHALL2CLASSDCLASSCCLASSBCLASSASTGEWC1WC2HALL1SHREE et al.: EXPLOITING NATURAL LANGUAGE FOR EFFICIENT RISK-AWARE MULTI-ROBOT SAR PLANNING

5

both Ô¨Åre and collapse vertices (NCF),
then using a com-
bination of both descriptors yields the best danger estimates
(BC = 0.53). Furthermore, the average BC obtained from the
‚Äòuniform-prior‚Äô baseline is 0.45, which is lower than the best
estimates for each one of the scene-types: NFF (0.63), NCC
(0.49), and NCF (0.53). Thus, we conclude that customizing
descriptions, depending on the hazard, is indeed beneÔ¨Åcial for
danger estimation. It should be noted that one can leverage
homogeneous domain adaptation techniques [22] to minimize
any performance deterioration when transferring the system to
the real world.
Table II: Comparison of average BC for estimated danger distribu-
tion with different descriptions, for three scene types.

Scene Type
NFF
NCC
NCF

F-Descriptors
0.63
0.30
0.47

C-Descriptors
0.42
0.49
0.49

FC-Descriptors
0.59
0.47
0.53

VI. MULTI-ROBOT PLANNING

A. Risk-Aware MESPP Problem

We now introduce a risk-aware version of the Multi-Robot
Search Path Planning (MESPP) problem [6]. In the original
MESPP problem, a team of robotic agents A = {1, ..., m}
is deployed in a graph-represented environment G = (V, E),
with the aim of locating a possibly moving non-adversarial
‚Äútarget‚Äù (e.g. a victim) within a given deadline œÑ . Time
T = {1, .., œÑ } evolves in discrete steps, and both the agents
and the target can either stay at the same vertex or reach a
neighbor vertex between two subsequent steps, for vertices
V = {1, ..., n}. Agents can communicate with each other at
all time-steps.

The target‚Äôs probable motion in the graph is encoded by a
Markovian matrix M ‚àà [0, 1]n√ón. At each step t, the target‚Äôs
state, resulting from its interactions with agents executing a
set of joint paths œÄ ‚àà P, is represented by the belief vector
bœÄ(t) = [bc(t), b1(t), ..., bn(t)],

(4)

where bc(t) + (cid:80)n
i=1 bv(t) = 1. The Ô¨Årst element, bc(t),
represents the probability that the agents have located the
target by time t when following paths œÄ. The remaining
elements b1(t), ..., bn(t) represent
the
the probability that
target is located in the corresponding vertices at time t.

Detection events are described by matrices Ca,u ‚àà
[0, 1](n+1)√ó(n+1), ‚àÄa ‚àà A, u ‚àà V . Their effect is to connect
the probability of the target being at a particular location with
its detection state. In other words, the matrix Ca,u encodes
which vertices of the graph fall within the sensing range of
agent a, when such is located in vertex u. A belief update
equation links the current belief, the target‚Äôs motion, and the
agents‚Äô paths œÄ with associated detection events as follows:

bœÄ(t + 1) = bœÄ(t)

(cid:20)

1

01√ón
0n√ó1 M

(cid:21) (cid:89)

a‚ààA

Ca,œÄa,t+1

.

(5)

In the original MESPP problem, the goal is to Ô¨Ånd the op-
t=0 Œ≥tbc(t), where Œ≥ ‚àà (0, 1]

timal path œÄ‚àó that maximizes (cid:80)œÑ
is a discount factor.

Our risk-aware version of the MESPP problem is an online
problem deÔ¨Åned as follows. Let us associate a ground truth

danger level lv ‚àà L for each v ‚àà V , and, accordingly, a
probability of agent loss for each danger level, p(loss|lv).
DeÔ¨Åne for each agent a ‚àà A, a Ô¨Åxed nominal danger threshold
Œ∫a ‚àà L, which is the expected danger level that agent a is apt
to endure throughout the mission. We assume the agents are
equipped with a danger estimation module they can leverage
for estimating danger level distributions Œ∑t
v, for each vertex v
and step t. Such a module provides estimates at t = 0 simply
based on a Ô¨Åxed prior, uniform by default. Once an agent
visits a vertex v for the Ô¨Årst time, it is allowed to update
the distribution of v (for example, by leveraging the method
of Section V) and, possibly,
that of neighboring vertices.
We introduce two risk-aware variants of MESPP, based on
different additional path constraints, contingent upon available
danger information.

Point estimate constraints: at each step t, the current plan
of each agent a can not prescribe a visit to a vertex v where
the most probable danger level of the vector Œ∑t
v, denoted by
v ‚àà arg maxl‚ààL Œ∑t
zt
v belongs to a danger level strictly larger
than Œ∫a.

Cumulative probability constraints: deÔ¨Åne an agent‚Äôs
required danger conÔ¨Ådence, Œ±a ‚àà (0, 1] as the cumulative
probability of estimated danger up to that agent‚Äôs threshold
Œ∫a. At each step t, the current plan of each agent a can
only include vertices where the cumulative danger distribution,
denoted by H a,t
l,v is equal or higher than Œ±a. This
allows to express more nuanced constraints since we need
to consider all danger probabilities up to an agent‚Äôs danger
threshold Œ∫a.

v = (cid:80)Œ∫a

l=1 Œ∑t

In both cases, the problem objective remains locating the
victim as soon as possible, considering that agents might be
lost along the mission according to p(loss|lv). Note that while
we deÔ¨Åne our thresholds as static parameters, they can also be
time-dependent, allowing for a dynamic behavior throughout
the mission.

B. MILP Models

Our solution is based on a receding-horizon distributed
planning approach, where planning is performed by iteratively
solving an extension of the MILP models presented in [7]. We
refer the reader to [7] for the complete set of MILP variables
and constraints, as well as implementation details, and focus
here solely on modeling danger-related constraints.

Analogously to the original MILP models, the binary vari-
able xa,t
indicates agent a is at vertex v at time t in the
computed path. Plans compliant with the point estimated
constraints are obtained by enforcing2

v

xa,t
v zt

v ‚â§ Œ∫a, ‚àÄv ‚àà V, t ‚àà T, a ‚àà A.

(6)

When dealing with the cumulative probability constraints,

we instead impose:

H a,t

v ‚â• xa,t

v Œ±a, ‚àÄv ‚àà V, t ‚àà T, a ‚àà A.

(7)

2We remark that a similar effect of usage could be obtained by removing
unsuitable states in the deÔ¨Ånition of legal searchers‚Äô paths, prior to planning
(see again [7] for details).

6

ACCEPTED FOR PUBLICATION IN IEEE ROBOTICS AND AUTOMATION LETTERS. DIGITAL OBJECT IDENTIFIER 10.1109/LRA.2021.3062798

VII. SIMULATIONS

A. Environment

We use the School scenario of the DISC dataset [5] with
NFF image distribution (see Sec. IV) to validate and analyze
the performance of our proposed system in its entirety. Since
the available dataset measurements represent what the robot
would have collected while moving through the simulated
environment, we use the left camera poses to infer our layout,
which is then abstracted into the graph shown in Fig. 4. We
also use it to re-build the environment in Gazebo [23], an
open-source 3D robotics simulator, for more realistic exper-
iments accounting for robot dynamics, asynchronous agents
and navigation challenges.

B. Danger Estimation

The general setup is the same for both numerical and
qualitative simulations. The human-reasoned ground truth for
each vertex (Fig. 4, left), deÔ¨Ånes the probability of losing the
agent p(loss|lv) = 7.47e ‚àí 8(lv ‚àí 1)elv ‚àÄlv ‚àà L, which yields
values between 0.009% and 49.5%.

A partial collection of images (5%) is used for danger
distribution estimation. These correspond to the Ô¨Årst set of
images an agent would see when entering the vertex area,
according to the DISC dataset camera pose. Note from Fig 4
that the estimated danger distribution, and thus the maximum
likelihood level (right), do not always match the human-
reasoned ground truth (left). Processing all images in a scene
requires minutes even with a powerful GPU [24], thus using
the Ô¨Årst few acquired images for estimation aligns well with
practical time limitations.

We divide the school space into neighborhoods, based upon
common structures such as walls and doorways (see Fig. 4).
We start with an a priori danger distribution for each vertex
and the estimated danger distribution for that vertex is made
available when an agent visits it for the Ô¨Årst time, mimicking
online information gathering. This information is then spread
to vertices in the neighborhood, if they have not been visited
yet. For instance, once an agent reaches v = 3, its danger
information (Œ∑t
3 ) is passed on to the other vertices in
the gym neighborhood, v = 4, 5, 6, but not v = 1, 2 since they
have already been visited.

3, H a,t

3, zt

In our simulations, a discrete time-step comprises the fol-
lowing actions: call the planner; get a new plan for the search
team; move the agents towards their next goals; retrieve danger
data; update danger estimation on visited and neighboring
vertices; compute and apply probability of loss given ground
truth; update team status (if agents are lost); scan for the
victim; and Ô¨Ånally, output target detection status.

C. Planner Parameters

In order to deÔ¨Åne a challenging initial belief vector, we
pick nine random vertices across neighborhoods and assign
a uniform probability of victim location among the chosen
vertices, assuming such is static when updating the belief.
Our team of three agents starts from v = 1 (gym entrance)
with probability of capture equal to zero, i.e. the victim is

not reachable at t = 0. For simplicity, we consider a perfect
sensing model with detection when both agent and target are in
the same location. However, assuming different victim motions
and sensing models is straightforward [7].

We use GUROBI [25] on 8 threads of a machine equipped
with Intel-Core i9-9900 K and 32 GB RAM to solve our path
planning problem. We implement a distributed approach, with
a mission deadline and planning horizon of 100 and 14 time-
steps, respectively. The solving time is consistently under 0.1
secs for the settings studied in this paper.

D. ConÔ¨Ågurations

We perform ten sets of experiments, with 1000 instances

each. We vary:

1) planner: without danger constraints (NC), with point

estimate (PT) and cumulative probability constraints (PB);

2) a priori danger knowledge: available to our agents at
t = 0, either perfect (ground truth) knowledge (PK) or no
knowledge at all, where we assume an uniform distribution
for all vertices (PU);

3) team makeup: different threshold combinations; (345)
for Œ∫ = [3, 4, 5], Œ± = [0.6, 0.4, 0.4]; (335) Œ∫ = [3, 3, 5], Œ± =
[0.6, 0.6, 0.4]; and (333) Œ∫ = [3, 3, 3], Œ± = [0.6, 0.6, 0.6];

4) best case baseline: danger-free environment (ND), i.e.,

without probability of loss.

The conÔ¨Ågurations are denoted in this order: {planner ‚Äì a
priori knowledge ‚Äì team makeup}. Thus, PB-PK-345 denotes
cumulative probability planner, perfect a priori knowledge and
team threshold makeup of Œ∫ = [3, 4, 5], Œ± = [0.6, 0.4, 0.4].

E. Metrics

1) Mission outcomes: success, target is found within the
deadline; abort, all agents are lost; and cutoff, deadline is
reached before target is found.

2) Average mission time: discrete time step when mission
ends, either due to target detection, mission abortion or cutoff.
3) Losses: percentage of missions where agents are lost
due to the dangerous environment. To evaluate the safety
potential of our framework, we denote as ‚ÄòMost Valuable
Agent‚Äô (MVA) the agent we want to protect the most, setting
its danger threshold as the lowest among the team. Thus,
for the team conÔ¨Åguration (345), the MVA is agent a = 1
with Œ∫MVA = 3; for (335), there are two MVAs, a = 1, 2;
conÔ¨Åguration (333) represents a homogeneous team. For a fair
comparison, we consider a MVA loss when at least one MVA
is lost (analogously for N-MVA).

F. Numerical Results

The numerical simulation results with varying planner and
a priori knowledge are shown in Fig. 5, for an heterogen-
eous team with danger thresholds Œ∫ = [3, 4, 5] and Œ± =
[0.6, 0.4, 0.4].

The best-case scenario (ND) establishes a baseline of 100%
success in the environment studied, which starts to decrease
when probabilities of loss and danger constraints are added.
The former has the effect of incapacitating the team so they

SHREE et al.: EXPLOITING NATURAL LANGUAGE FOR EFFICIENT RISK-AWARE MULTI-ROBOT SAR PLANNING

7

Figure 5: Numerical simulations with varying planner and a priori danger knowledge.. Planner: point estimate (PT) and cumulative
probability (PB) constraints, with Œ∫ = [3, 4, 5] and Œ± = [0.6, 0.4, 0.4]. A priori knowledge: uniform (PU) and perfect (PK), i.e., equal to the
human-reasoned ground truth. Additionally, we simulate best (danger-free environment, ND) and worst case (no constraints, NC) scenarios.
Left: Mission outcomes. Middle: Mission times. Right: Losses.

are unable to proceed with the mission, while the later may
slow down the search if some plans are considered unsuitable
given the agents thresholds.

left-red):

With perfect knowledge (PT-PK, PB-PK), the team is able
to plan accordingly (Fig. 5,
there are only two
missions aborted, and our MVA is rarely lost (Fig. 5, right-
red). However, as danger estimation becomes less accurate
(PT-PU, PB-PU), the planner cannot be so protective, thus
the MVA losses increase (Fig. 5, right-red) as well as aborted
missions (Fig. 5, left-red), leading to the worst-case scenario,
where danger is present but there are no constraints (NC). Note
the cause of failure with perfect knowledge (PK) is due to the
deadline being reached, while without danger constraints (NC)
the mission fails due to abort, which is corroborated by its
much shorter average mission time3 (Fig. 5, middle-blue). The
target detection time is roughly the same across conÔ¨Ågurations
(Fig. 5, middle-black), indicating efÔ¨Åciency is still the main
goal. Whenever this goal is not achieved, however, the danger
constraints still allow the agents to be safer. Meanwhile, if
danger constraints are not used and the mission fails, both
agents and target are lost.

Results for different team threshold makeups are shown in
Fig. 6. The cumulative probabilistic constraints (PB) have a
more stable performance than the point estimate (PT) with
different teams, as seen in Fig. 6 (left). For a homogeneous
team (PB-PU-333), the mission success rate is only slightly
lower than for a heterogeneous one (PB-PU-345). This is likely
due to the more nuanced thresholding, e.g. 60% conÔ¨Ådence
that a vertex level is between 1-3, rather than considering a
single estimate. However, point estimate is simpler to tune
and therefore may perform better in a multimodal danger
distribution. For both planners, the main cause of failure is
cutoff, particularly for PT-PU-333, which presents a signiÔ¨Åcant
longer mission time (Fig. 6, right), and lower success rate. On
the other hand, the mission abort rate decreases as the team
becomes more homogeneous, illustrating the trade-off between
protecting the agents and exploring the environment.

G. Qualitative Simulations

We use Gazebo 7 [23] with ROS-Kinetic [26] to incorporate
one Hector quadrotor (agent 1) and two Jackal ground robots4
(agents 2, 3), as shown in Fig. 7 (left). The simulator‚Äôs main
capabilities of mapping, localization, and autonomous naviga-
tion are built off the ROS Navigation Stack. Gazebo simulates
the actual robot dynamics, which adds to the possibility of
mission failure. The robots can now become inactive either
due to danger, or incidents while navigating between vertices,
for example getting lost, crashing, or tipping over. If a robot
cannot reach its goal vertex before a given time has elapsed,
that robot is considered to be inactive, and can no longer
participate in the search.

We perform 5 instances per conÔ¨Åguration, with each discrete
time step corresponding in average to 11.72 secs. The team
performance follows the trend of the numerical simulations,
with similar capture time across conÔ¨Ågurations. The best-
case scenario (ND) presents 100% success, followed by 80%
for heterogeneous teams (PT/PB-PU-345), 60% for perfect a
priori knowledge (PT/PB-PK-345), and 40% for the others
(PT-PU-335, PB-PU-335/333); particularly for PT-PU-333,
none of the missions are successful due to cutoff. There are
less agent losses and no abort missions with PK, but also
less exploring, which results in a lower success rate than
PU in these particular instances. All mission failures when
employing constraints are due to cutoff, except for one abort
instance in PB-PU-345/335.

Figure 8 shows the percentage of missions where each agent
becomes inactive, partitioned into losses caused by Danger and
Navigation. There is an expected increase in agent loss when
accounting for navigation, particularly for agents 2 and 3 ‚Äì
which can be attributed to the fact that these are ground robots,
and thus more likely to run into each other or get stuck. Our
results suggest that a risk-aware planner can reduce losses on
one set of agents, without a signiÔ¨Åcant loss in performance,
even when dealing with practical navigation challenges and
imperfect scene knowledge.

3Bars denote 95% conÔ¨Ådence interval; if not shown, values are ‚â§ 1 times-

4ROS packages: https://github.com/tu-darmstadt-ros-pkg/hector quadrotor;

step and overlap with marker.

https://clearpathrobotics.com/jackal-small-unmanned-ground-vehicle

NDPT-PK345PB-PK345PT-PU345PB-PU345NC020406080100MissionOutcomes[%]SuccessCutoÔ¨ÄAbortNDPT-PK345PB-PK345PT-PU345PB-PU345NC102030405060AverageTime[steps]EndCaptureAbortNDPT-PK345PB-PK345PT-PU345PB-PU345NC0102030405060MissionswithLosses[%]AnyN-MVAMVA8

ACCEPTED FOR PUBLICATION IN IEEE ROBOTICS AND AUTOMATION LETTERS. DIGITAL OBJECT IDENTIFIER 10.1109/LRA.2021.3062798

Figure 6: Numerical simulations with varying team threshold
makeups. Left: Mission outcomes. Right: Mission times.

Figure 7: Environment built in ROS/Gazebo based on the poses
extracted from DISC dataset. Left: Structure. Middle: Mapping
process. Right: Resultant map.

VIII. CONCLUSION

In this paper, we presented a danger estimation framework
which is adaptable to different environmental settings. Our
results shows its potential in enhancing team performance in
SaR missions. In the future, we would like to investigate ways
to reject redundant collected images, which could potentially
reduce the computational overhead during online operations.
In order to study a more realistic SaR scenario, we would also
like to apply a dynamic Bayesian model for estimating danger.
We also believe that replacing DISC images with realistic
images from movies can be key in bridging the performance
gap between our simulations and real-world application. Fur-
thermore, we would like to tailor our approach to SaR missions
with human-robot teams, communicating succinct and reliable
information, as Ô¨ÅreÔ¨Åghters do in the real world.

ACKNOWLEDGMENTS

We thank Asst. Chief Tom Basher (City of Ithaca Fire Dept.)
and Chief George Tamborelle (Cayuga Heights Fire Dept.) for
their valuable insights on real-world SaR missions.

REFERENCES

[1] J. Casper and R. R. Murphy, ‚ÄúHuman-robot interactions during the
robot-assisted urban search and rescue response at
the world trade
center,‚Äù IEEE Transactions on Systems, Man, and Cybernetics, Part B
(Cybernetics), vol. 33, no. 3, pp. 367‚Äì385, 2003.

[2] S. Thrun, ‚ÄúLearning occupancy grid maps with forward sensor models,‚Äù

Autonomous robots, vol. 15, no. 2, pp. 111‚Äì127, 2003.

[3] S. Se, D. Lowe, and J. Little, ‚ÄúMobile robot localization and mapping
with uncertainty using scale-invariant visual landmarks,‚Äù Inter J Robot
Res, vol. 21, no. 8, pp. 735‚Äì758, 2002.

[4] J. Lu, D. Batra, D. Parikh, and S. Lee, ‚ÄúVilbert: Pretraining task-agnostic
visiolinguistic representations for vision-and-language tasks,‚Äù in Proc.
NeurIPS, 2019, pp. 13‚Äì23.

[5] H.-G. Jeon, S. Im, B.-U. Lee, D.-G. Choi, M. Hebert, and I. S. Kweon,
‚ÄúDisc: A large-scale virtual dataset for simulating disaster scenarios.‚Äù in
Proc. IROS, 2019, pp. 187‚Äì194.

Figure 8: ROS/Gazebo simulations: percentage of missions where
each agent becomes inactive, due to dangerous environment or
navigation challenges.

[6] G. Hollinger, S. Singh, J. Djugash, and A. Kehagias, ‚ÄúEfÔ¨Åcient multi-
robot search for a moving target,‚Äù Inter J Robot Res, vol. 28, no. 2, pp.
201‚Äì219, 2009.

[7] B. Arruda Asfora, J. BanÔ¨Å, and M. Campbell, ‚ÄúMixed-integer linear
programming models for multi-robot non-adversarial search,‚Äù IEEE
Robotics and Automation Letters, vol. 5, no. 4, pp. 6805‚Äì6812, 2020.

[8] J. P. Queralta, J. Taipalmaa, B. C. Pullinen, V. K. Sarker, T. N.
Gia, H. Tenhunen, M. Gabbouj, J. Raitoharju, and T. Westerlund,
‚ÄúCollaborative multi-robot search and rescue: Planning, coordination,
perception, and active vision,‚Äù IEEE Access, vol. 8, pp. 191 617‚Äì191 643,
2020.

[9] F. Yazdani, M. Scheutz, and M. Beetz, ‚ÄúCognition-enabled task inter-
pretation for human-robot teams in a simulation-based search and rescue
mission,‚Äù in in Proc. AAMAS, 2017, pp. 1772‚Äì1774.

[10] X. Sun, Y. Zhang, and J. Chen, ‚ÄúHigh-level smart decision making of
a robot based on ontology in a search and rescue scenario,‚Äù Future
Internet, vol. 11, no. 11, p. 230, 2019.

[11] K.-H. Lee, X. Chen, G. Hua, H. Hu, and X. He, ‚ÄúStacked cross attention

for image-text matching,‚Äù in Proc. ECCV, 2018, pp. 201‚Äì216.

[12] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hocken-
maier, and S. Lazebnik, ‚ÄúFlickr30k entities: Collecting region-to-phrase
correspondences for richer image-to-sentence models,‚Äù in Proc. ICCV,
2015, pp. 2641‚Äì2649.

[13] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick,
and D. Parikh, ‚ÄúVqa: Visual question answering,‚Äù in Proc. ICCV, 2015,
pp. 2425‚Äì2433.

[14] R. Zellers, Y. Bisk, A. Farhadi, and Y. Choi, ‚ÄúFrom recognition to
cognition: Visual commonsense reasoning,‚Äù in Proc. ICCV, 2019, pp.
6720‚Äì6731.

[15] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, ‚ÄúFrom image
descriptions to visual denotations: New similarity metrics for semantic
inference over event descriptions,‚Äù Transactions of the Association for
Computational Linguistics, vol. 2, pp. 67‚Äì78, 2014.

[16] R. Yehoshua, N. Agmon, and G. A. Kaminka, ‚ÄúRobotic adversarial
coverage of known environments,‚Äù Inter J Robot Res, vol. 35, no. 12,
pp. 1419‚Äì1444, 2016.

[17] J. BanÔ¨Å, V. Shree, and M. Campbell, ‚ÄúPlanning high-level paths in
hostile, dynamic, and uncertain environments,‚Äù Journal of ArtiÔ¨Åcial
Intelligence Research, vol. 69, pp. 297‚Äì342, 2020.

[18] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and
L. Zhang, ‚ÄúBottom-up and top-down attention for image captioning and
visual question answering,‚Äù in Proc. ICCV, 2018, pp. 6077‚Äì6086.
[19] J. Lu, V. Goswami, M. Rohrbach, D. Parikh, and S. Lee, ‚Äú12-in-1: Multi-
task vision and language representation learning,‚Äù in Proc. CVPR, 2020,
pp. 10 437‚Äì10 446.

[20] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image

recognition,‚Äù in Proc. ICCV, 2016, pp. 770‚Äì778.

[21] D. Comaniciu, V. Ramesh, and P. Meer, ‚ÄúReal-time tracking of non-rigid

objects using mean shift,‚Äù in Proc. CVPR, vol. 2, 2000, pp. 142‚Äì149.

[22] M. Wang and W. Deng, ‚ÄúDeep visual domain adaptation: A survey,‚Äù

Neurocomputing, vol. 312, pp. 135‚Äì153, 2018.

[23] Open Source Robotics Foundation, ‚ÄúGazebo. robot simulation made

easy,‚Äù http://gazebosim.org/, 2014.

[24] K. He, G. Gkioxari, P. Doll¬¥ar, and R. Girshick, ‚ÄúMask r-cnn,‚Äù in Proc.

ICCV, 2017, pp. 2961‚Äì2969.

[25] Gurobi Optimization, ‚ÄúGurobi optimizer reference manual,‚Äù 2019.
[26] M. Quigley et al., ‚ÄúRos: an open-source robot operating system,‚Äù in

ICRA workshop on open source software, vol. 3, no. 3.2, 2009, p. 5.

PT-PU345PT-PU335PT-PU333PB-PU345PB-PU335PB-PU333020406080MissionOutcomes[%]SuccessCutoÔ¨ÄAbortPT-PU345PT-PU335PT-PU333PB-PU345PB-PU335PB-PU333020406080AverageTime[steps]EndCaptureAbort