ROBUST 3D CELL SEGMENTATION: EXTENDING THE VIEW OF CELLPOSE

Dennis Eschweiler1∗

Richard S. Smith2

Johannes Stegmaier1∗

1 Institute of Imaging and Computer Vision, RWTH Aachen University, Aachen, Germany
2 John Innes Centre, Norwich Research Park, Norwich, UK

2
2
0
2

b
e
F
1

]

V

I
.
s
s
e
e
[

3
v
4
9
7
0
0
.
5
0
1
2
:
v
i
X
r
a

ABSTRACT

Increasing data set sizes of 3D microscopy imaging experi-
ments demand for an automation of segmentation processes
to be able to extract meaningful biomedical information. Due
to the shortage of annotated 3D image data that can be used
for machine learning-based approaches, 3D segmentation ap-
proaches are required to be robust and to generalize well to
unseen data. The Cellpose approach proposed by Stringer
et al.
[1] proved to be such a generalist approach for cell
In this paper, we extend the
instance segmentation tasks.
Cellpose approach to improve segmentation accuracy on 3D
image data and we further show how the formulation of the
gradient maps can be simpliﬁed while still being robust and
reaching similar segmentation accuracy. The code is publicly
available and was integrated into two established open-source
applications that allow using the 3D extension of Cellpose
without any programming knowledge.

Index Terms— Cellpose, 3D Segmentation, Generalist,

Robust, Fluorescence Microscopy

1. INTRODUCTION

Current developments in ﬂuorescence microscopy have al-
lowed the generation of image data with progressively in-
creasing resolution and often accordingly increasing data size
[2, 3]. 3D microscopy data sets can easily reach terabyte-
scale, which requires a high degree of automation of analyt-
ical processes including image segmentation [4]. Machine
learning-based segmentation approaches require annotated
training data sets for reliable and robust outcomes, which
often must be created manually and are rarely available. Al-
though some tools are available [5, 6, 7, 8], this issue is even
more severe for 3D microscopy image data, since manual an-
notation of 3D cellular structures is very time-consuming and
often infeasible, which causes the acquisition of annotated
3D data sets to be highly expensive.

2D approaches have been applied in a slice-wise manner
to overcome this issue [1], diminishing the need for expen-
sive 3D annotations, but resulting in potential sources of er-
rors and inaccuracies. Segmentation errors are more likely to

*Correspondence should be addressed to {dennis.eschweiler,

johannes.stegmaier}@lfb.rwth-aachen.de

occur at slice transitions, leading to noisy segmentations, and
omitting information from the third spatial dimension, i.e., a
decreased ﬁeld of view, leads to a potential loss of segmenta-
tion accuracy. Conversely, if there are fully-annotated 3D data
sets available, robust and generalist approaches are desired to
make full use of those data sets [1, 9, 10].

Stringer et al. proposed the Cellpose algorithm [1] and
demonstrated that this approach serves as a reliable and gen-
eralist approach for cellular segmentation on a large variety of
data sets. However, this method was designed for 2D appli-
cation. Although a concept to obtain a 3D segmentation from
a successive application of the 2D method in different spatial
dimensions was proposed, the approach is still prone to the
aforementioned sources of errors, missing on the full poten-
tial for 3D image data. In this paper we propose (1) an exten-
sion of the Cellpose approach to fully exploit the available 3D
information for improved segmentation smoothness and in-
creased robustness. Furthermore, we demonstrate (2) how the
training objective can be simpliﬁed without loosing accuracy
for segmentation of ﬂuorescently labeled cell membranes and
(3) we propose a concept for instance reconstruction allowing
for stable runtimes. (4) All of our code is publicly available
and has been integrationed into the open source applications
XPIWIT [11] and MorphoGraphX [7].

2. METHOD

The proposed method extends the Cellpose algorithm pro-
posed by Stringer et al. [1], which formulates the instance
segmentation problem as a prediction of directional gradients
pointing towards the center of each cell. These gradients are
derived from mathematically modelling a heat diffusion pro-
cess, originating at the centroid of a cell and extending to the
cell boundary (Fig. 1, upper row). Gradients are divided into
separate maps for each spatial direction, which can be pre-
dicted by a neural network alongside a foreground probability
map to prevent background segmentations. After prediction
of the foreground and gradient maps using a U-Net architec-
ture [12], each cell instance is reconstructed by tracing the
multidimensional gradient maps to the respective simulated
heat origin. All voxels that end up in the same sink are ulti-
mately assigned to the same cell instance. For processing im-
ages directly in 3D and beneﬁting from the full 3D informa-

 
 
 
 
 
 
ends up in the vicinity of the corresponding cell center, where
they can be grouped and assigned a unique cell label. Instead
of utilizing clustering techniques to assign labels, we rely on
mathematical morphology to identify connected voxel groups
at each cell center. As opposed to clustering, this allows for
a constant and fast run-time, independent of the potentially
large quantity of cells in 3D image data. A morphological
closing operation using a spherical structuring element with
a radius rclosing smaller than the estimated average cell radius
is applied to determine those unique connected components.
Reconstructed cell instances are ﬁltered by their size,
requiring each instance to be within a predeﬁned range of
(rmin, rmax) and by their overlap with the predicted fore-
ground region with a required overlap ratio of at least poverlap.
Similar to the post-processing used in [1], gradient maps are
computed for each reconstructed instance and compared to
the corresponding gradient maps predicted by the network.
In an ideal case, both gradient maps are equivalent, but if
their mean absolute error exceeds errgradient this instance is
assumed to be falsely reconstructed and discarded from the
ﬁnal result.

Fig. 2. Representative visualization of our 3D processing
pipeline, including the input patch [14], the neural network
and the predicted outputs. Instances are reconstructed from
the predicted gradient maps and ﬁltered (f ) by utilizing fore-
ground predictions and ﬂow errors.

3. EXPERIMENTS AND RESULTS

To evaluate our method, we conduct different experiments
comparing the 3D extension to the 2D baseline [1] and as-
sessing the generation of gradient maps. For a comparison to
various state-of-the-art segmentation approaches, we refer to
the results reported in [1]. The data set we use for evaluation
is publicly available [15] and includes 125 3D stacks of con-
focal microscopy image data showing ﬂuorescently labeled
cell membranes in the Meristem of the plant model organism
A. thaliana. We divided the data set into training and test sets,
where plants 1,2,4 and 13 were used for training and plants
15 and 18 were used for testing. The test data set comprises
about 37,000 individual cells. Furthermore, we show how
well each of the approaches performs on unseen data to assess
the generalizability to different microscopy settings. There-

Fig. 1. Gradient map representation of cell instances in x
(red), y (green) and z (blue) directions. The upper row shows
heat diffusion simulations [1], while the lower row shows the
simpliﬁed hyperbolic tangent distributions. Brightness indi-
cates the gradient direction.

tion, we propose multiple additions to the Cellpose approach
and necessary changes to overcome memory limitations and
prevent long run-times. An overview of the entire processing
pipeline is visualized in Fig. 2.

We propose to use a different objective for the genera-
tion of the gradient maps, since calculation of a heat diffu-
sion in 3D (Fig. 1, top) is computationally complex. Instead,
we rely on a hyperbolic tangent spanning values in the range
of (−1, 1) between cell boundaries in each spatial direction
(Fig. 1, bottom), which can be interpreted as the relative direc-
tional distance to reach the cell center axis. Note that the dif-
ferent formulation constitutes a trade-off between lower com-
plexity and the ability to reliably segment overly non-convex
cell shapes. Predictions are obtained by a 3D U-Net [13],
including all three spatial gradient maps for the x, y and z di-
rections, respectively, and an additional 3D foreground map
highlighting cellular regions. Since the training objective is
formulated by a hyperbolic tangent, the output activation of
the U-Net is set to a hyperbolic tangent accordingly. Pro-
cessing images directly in the 3D space has the advantage of
performing predictions only once for an entire 3D region, as
opposed to a slice-wise 2D application, which has to be ap-
plied repetitively along each axis to obtain precise gradient
maps for each of the three spatial directions.

Reconstruction of cell instances is done in an iterative
manner, by moving each voxel within the foreground re-
gion along the predicted 3D gradient ﬁeld g by a step size
δrecon(x, y, z) = g(x, y, z) ∗ srecon, given by the magnitude of
the predicted gradient at each respective position g(x, y, z)
and a ﬁxed integer scaling factor srecon. The number of iter-
ations is deﬁned by a ﬁxed integer Nrecon, which is adjusted
to represent the number of steps necessary to certainly move
a boundary voxel to the cell center. Ultimately, each voxel

e
r
o
c
S

e
c
i
D

1 .0

0 .8

0 .6

0 .4

0 .2

0 .0

Baseline
pretrained

Baseline
specialized

Extension
heat

Extension
tanh

e
r
o
c
S

e
c
i
D

1 .0

0 .8

0 .6

0 .4

0 .2

0 .0

Baseline
pretrained

Baseline
specialized

Extension
heat

Extension
tanh

Fig. 3. Dice scores computed from segmentation results obtained for the public Meristem data set [15] (left) and for the
manually annotated 3D image stack (right). The experiments include pre-trained Cellpose provided by [1] (Baselinepretrained), a
retrained version of the original Cellpose architecture (Baselinespecialized), our 3D extension with the original (Extensionheat) and
the hyperbolic tangent gradient formulation (Extensiontanh). Whiskers represent the 5th and 95th percentile, the box spans from
the 1st to the 3rd quartile, the orange line shows the median value and the green triangle indicates the mean value.

fore, we use a manually annotated 3D confocal stack of A.
thaliana, comprising a total of 972 fully annotated cells. An-
notations were manually obtained using the SEGMENT3D
online platform [6]. Experiments are structured as follows:

Extensiontanh For the fourth experiment, we change the
representation of the gradient maps, formulated by hyper-
bolic tangent functions as described in Sec. 2. Otherwise, the
setup is identical to the setup of Extensionheat.

Baselinepretained As a baseline we use the publicly available
Cellpose cyto-model and apply it to the test data set using the
3D pipeline published in [1]. Since the model was trained on
a large and highly varying data set, we do not perform any
further training.

Baselinespecialized The second experiment is designed to be a
second baseline, as we use the original Cellpose approach [1]
and train it from scratch using the above-mentioned training
data set from Willis et al. [15] for 1000 epochs, which we ﬁnd
sufﬁcient for convergence. To reduce memory consumption
and prevent high redundancies, we extract every fourth slice
of each 3D stack of the training data set to construct the new
2D training data. This experiment constitutes a specialized
case of the baseline approach.

Extensionheat Our proposed 3D extension is ﬁrst trained
with the original representation of the gradient maps, formu-
lated as a heat diffusion process [1]. The 3D U-Net is trained
on patches of size 128 × 128 × 64 voxel for 1000 epochs
using the Meristem training data set from Willis et al. [15].
In every epoch, one randomly located patch is extracted from
each image stack of the training data set. To obtain the ﬁnal
full-size image, a weighted tile merging strategy as proposed
in [16] is used on the predicted foreground and gradient
maps, before reconstructing individual instances.
Instance
reconstruction parameters are empirically set to srecon = 4,
Nrecon = 100 and rclosing = 3. Filtering parameters are set to
(rmin, rmax) = (5, 100), poverlap = 0.2 and errgradient = 0.8.

Final mean Dice values for the public test data set [15]
are 0.656 for Baselinepretrained, 0.723 for Baselinespecialized and
0.905 and 0.897 for Extensionheat and Extensiontanh, respec-
tively (Fig. 3, left). Although Baselinespecialized beneﬁts from
specialized knowledge and shows improved segmentation
scores, both baseline approaches result in poor instance seg-
mentations in regions with low ﬂuorescence intensity, leading
to a large spread of obtained scores. The full 3D extensions,
however, are able to exploit the structural information from
the third dimension to successfully outline poorly visible
cell instances. Furthermore, similar scores are obtained for
both gradient formulations. Results for the manually an-
notated data are shown as boxplots in Fig. 3 (right) with
mean Dice scores of 0.383 for Baselinepretrained, 0.877 for
Baselinespecialized and scores of 0.883 for both, Extensionheat
and Extensiontanh. This conﬁrms that the lack of structural 3D
information leads to a loss in accuracy and that both gradient
formulations perform similarly well.

To further demonstrate the generalizability of the pro-
posed approach, we generated results for the publicly avail-
able 3D image data from a variety of different plant organs in
A. thaliana [14]. We used the model trained in Extensiontanh
and each image was scaled to roughly match the cell sizes
of the training data set in each spatial direction. Since the
ground truth did not include segmentations of all cells being
visible in the image data, we limited the computation of Dice
scores to annotated cells. Average Dice scores (DSC) ob-

 
 
Fig. 4. Slices of raw 3D image data (top), corresponding ground truth and predictions of the Extensiontanh model (center) and
3D views of the predictions (bottom) including the obtained Dice scores (DSC) for different plant organs in A. thaliana [14].

tained for instance segmentations of each image stack range
from 0.54 to 0.91 and slices of raw image data, ground truth
and instance predictions are shown in Fig. 4. This data set
contains cellular shapes never seen during training, which
causes overly elongated cells to be split up into different sec-
tions for some of the images. Nevertheless, overall results
demonstrate robustness and applicability to different cellular
structures.

4. CONCLUSION AND AVAILABILITY

In this work we demonstrated how the concept of Cellpose [1]
can be extended to increase segmentation accuracy for 3D
image data. The utilization of the full 3D information and
the prediction of 3D gradient maps, help to improve seg-
mentation of cells in regions of poor image quality and low
intensity signals. Our alternate formulation of the proposed
gradient maps leads to a comparable accuracy of segmenta-
tion results, while offering a lower complexity with respect

to training data preparation and instance reconstruction. The
morphology-based approach proposed for the instance re-
construction enables the application to 3D microscopy image
data independent from the quantity of captured cells. Results
obtained on completely different data sets never seen during
training support the claim that this approach is generalist and
robust. Code, training and application pipelines are publicly
available at https://github.com/stegmaierj/Cellpose3D. Fur-
thermore, we integrated the approach into the existing open
source applications XPIWIT [11] and MorphoGraphX [7] to
make it accessible to a broad range of community members.

5. ACKNOWLEDGEMENTS

This work was funded by the German Research Foundation
DFG with the grants STE2802/2-1 (DE) and an Institute
Strategic Programme Grant from the BBSRC to the John
Innes Centre (BB/P013511/1).

[10] Uwe Schmidt, Martin Weigert, Coleman Broaddus, and
Gene Myers, “Cell Detection with Star-convex Poly-
gons,” in International Conference on Medical Image
Computing and Computer-Assisted Intervention, 2018,
pp. 265–273.

[11] Andreas Bartschat, Eduard H¨ubner, Markus Reischl,
Ralf Mikut, and Johannes Stegmaier,
“XPIWIT—an
XML pipeline wrapper for the Insight Toolkit,” Bioin-
formatics, vol. 32, no. 2, pp. 315–317, 2016.

[12] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,
“U-Net: Convolutional Networks for Biomedical Image
Segmentation,” in International Conference on Medical
Image Computing and Computer-Assisted Intervention,
2015, pp. 234–241.

[13]

¨Ozg¨un C¸ ic¸ek, Ahmed Abdulkadir, Soeren S Lienkamp,
“3D U-Net:
Thomas Brox, and Olaf Ronneberger,
Learning Dense Volumetric Segmentation from Sparse
Annotation,” in International Conference on Medical
Image Computing and Computer-Assisted Intervention,
2016, pp. 424–432.

[14] Adrian Wolny, Lorenzo Cerrone, Athul Vijayan,
Rachele Tofanelli, Amaya Vilches Barro, Marion Lou-
veaux, and Anna Kreshuk, “Accurate and Versatile 3D
Segmentation of Plant Tissues at Cellular Resolution,”
Elife, vol. 9, pp. e57613, 2020.

[15] Lisa Willis, Yassin Refahi, Raymond Wightman, Benoit
Landrein, Jos´e Teles, Kerwyn Casey Huang, Elliot M
Meyerowitz, and Henrik J¨onsson,
“Cell Size and
Growth Regulation in the Arabidopsis Thaliana Apical
Stem Cell Niche,” Proceedings of the National Academy
of Sciences, vol. 113, no. 51, pp. E8238–E8246, 2016.

[16] Thomas de Bel, Meyke Hermsen, Jesper Kers, Jeroen
van der Laak, and Geert Litjens, “Stain-Transforming
Cycle-Consistent Generative Adversarial Networks for
Improved Segmentation of Renal Histopathology,” in
Proceedings of The 2nd International Conference on
Medical Imaging with Deep Learning, 2019, vol. 102,
pp. 151–163.

6. REFERENCES

[1] Carsen Stringer, Tim Wang, Michalis Michaelos, and
Marius Pachitariu, “Cellpose: a Generalist Algorithm
for Cellular Segmentation,” Nature Methods, vol. 18,
no. 1, pp. 100–106, 2021.

[2] Michael N Economo, Nathan G Clack, Luke D Lavis,
Charles R Gerfen, Karel Svoboda, Eugene W Myers,
and Jayaram Chandrashekar, “A Platform for Brain-
wide Imaging and Reconstruction of Individual Neu-
rons,” Elife, vol. 5, pp. e10566, 2016.

[3] Petr Strnad, Stefan Gunther, Judith Reichmann, Uros
Krzic, Balint Balazs, Gustavo De Medeiros, Nils Nor-
lin, Takashi Hiiragi, Lars Hufnagel, and Jan Ellenberg,
“Inverted Light-Sheet Microscope for Imaging Mouse
Pre-Implantation Development,” Nature Methods, vol.
13, no. 2, pp. 139–142, 2016.

[4] Erik Meijering, “A Bird’s-Eye View of Deep Learning
in Bioimage Analysis,” Computational and Structural
Biotechnology Journal, vol. 18, pp. 2312, 2020.

[5] Claire McQuin, Allen Goodman, Vasiliy Chernyshev,
Lee Kamentsky, Beth A. Cimini, Kyle W. Karhohs,
Minh Doan, Liya Ding, Susanne M. Rafelski, Derek
Thirstrup, Winfried Wiegraebe, Shantanu Singh, Tim
Becker, Juan C. Caicedo, and Anne E. Carpenter, “Cell-
Proﬁler 3.0: Next-Generation Image Processing for Bi-
ology,” PLOS Biology, vol. 17, pp. 543–563, 2018.

[6] Thiago V. Spina, Johannes Stegmaier, Alexandre X.
Falc˜ao, Elliot Meyerowitz, and Alexandre Cunha,
“SEGMENT3D: A Web-based Application for Collab-
orative Segmentation of 3D Images used in the Shoot
Apical Meristem,” in IEEE International Symposium on
Biomedical Imaging, 2018, pp. 391–395.

[7] Pierre Barbier

de Reuille, Anne-Lise Routier-
Kierzkowska, Daniel Kierzkowski, George W Bassel,
Thierry Sch¨upbach, Gerardo Tauriello, Namrata Ba-
jpai, S¨oren Strauss, Alain Weber, Annamaria Kiss,
“MorphoGraphX: A Platform for Quantifying
et al.,
Morphogenesis in 4D,” Elife, vol. 4, pp. e05864, 2015.

[8] Cristoph Sommer, Cristoph Straehle, Ullrich K¨othe, and
Fred A. Hamprecht, “Ilastik: Interactive Learning and
Aegmentation Toolkit,” in IEEE International Sympo-
sium on Biomedical Imaging (ISBI), 2011, pp. 230–233.

[9] Fabian Isensee, Paul F. Jaeger, Simon A. A. Kohl, Jens
Petersen, and Klaus H. Maier-Hein, “nnU-Net: A Self-
Conﬁguring Method for Deep Learning-based Biomed-
ical Image Segmentation,” Nature methods, vol. 18, no.
2, pp. 203–211, 2021.

