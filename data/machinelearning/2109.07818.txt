Learning Logic Programs Through Divide, Constrain, and Conquer

Andrew Cropper

University of Oxford
andrew.cropper@cs.ox.ac.uk

1
2
0
2
c
e
D
7

]
I

A
.
s
c
[

2
v
8
1
8
7
0
.
9
0
1
2
:
v
i
X
r
a

Abstract

We introduce an inductive logic programming approach that
combines classical divide-and-conquer search with modern
constraint-driven search. Our anytime approach can learn opti-
mal, recursive, and large programs and supports predicate inven-
tion. Our experiments on three domains (classiﬁcation, inductive
general game playing, and program synthesis) show that our ap-
proach can increase predictive accuracies and reduce learning
times.

1

Introduction

Inductive logic programming (ILP) (Muggleton 1991) is a form of
machine learning. Given positive and negative examples and
background knowledge (BK), the ILP problem is to ﬁnd a set of
rules (a hypothesis) which with the BK entails all the positive
and none of the negative examples.

The fundamental challenge in ILP is to eﬃciently search
a large hypothesis space (the set of all hypotheses) for a
solution (a hypothesis that correctly generalises the exam-
ples). Divide-and-conquer (D&C) approaches, such as TILDE
(Blockeel and Raedt 1998), divide the examples into disjoint
sets and then search for a hypothesis for each set, similar to
decision tree learners (Quinlan 1986). Separate-and-conquer
(S&C) approaches, such as Progol (Muggleton 1995) and Aleph
(Srinivasan 2001), search for a hypothesis that generalises a
subset of the examples, separate these examples, and then
search for more rules to add to the hypothesis to generalise
the remaining examples.

Although powerful, D&C and S&C approaches struggle to
perform predicate invention (Stahl 1995) and learn recursive
and optimal programs (Cropper et al. 2021), partly because they
only consider subsets of the examples. For instance, Progol
and Aleph need examples of both the base and inductive
cases (in that order) to learn a recursive program. To overcome
these limitations, many modern systems learn from all exam-
ples simultaneously. Modern systems also use powerful con-
straint solvers, such as answer set programming (ASP) solvers
(Gebser et al. 2014), to search for a hypothesis. For instance,
ASPAL (Corapi, Russo, and Lupu 2011) pre-computes the set of
all possible rules that may appear in a hypothesis and uses
an ASP solver to ﬁnd a subset that generalises the exam-
ples. Many modern systems can learn optimal and recursive
programs but struggle to learn large programs. For instance,

Metagol (Muggleton, Lin, and Tamaddoni-Nezhad 2015) strug-
gles to learn programs with more than six rules and ILASP3 (Law
2018) struggles to learn rules with more than a few body literals.
To address these limitations, we introduce an approach that
combines classical D&C search with modern constraint-driven
search. As with D&C approaches, we divide the examples into
disjoint subsets and induce separate hypotheses for them.
Speciﬁcally, we ﬁrst learn a hypothesis for each positive exam-
ple. Each hypothesis will likely be too speciﬁc. We therefore
search again for a hypothesis that generalises pairs of exam-
ples. These new hypotheses will again likely be too speciﬁc but
should generalise better than previous ones. We repeat this
process each time increasing the partition size until it matches
or exceeds the number of positive examples.

The aforementioned approach on its own is pointless as it
simply involves repeated search. The key idea of our approach
is to reuse knowledge discovered when solving smaller tasks to
help solve larger tasks. For instance, when searching for a hy-
pothesis for a single positive example, if we discover a hypoth-
esis that incorrectly entails a negative example then any more
general hypothesis will also entail it. We can therefore ignore
all generalisations of the hypothesis in subsequent iterations.
To realise our idea, we build on the constraint-driven learn-
ing from failures (LFF) (Cropper and Morel 2021a,b) ILP ap-
proach. The goal of LFF is to accumulate constraints to restrict
the hypothesis space. In our approach, we accumulate con-
straints during both the divide and conquer steps. In other
words, we reuse constraints learned during one iteration in sub-
sequent iterations. We call our approach divide, constrain, and
conquer (DCC).

To illustrate our approach, suppose we want to learn the fol-

lowing program h1 to ﬁnd odd elements in a list:

f(A,B) ← head(A,B), odd(B)
f(A,B) ← head(A,B), even(B), tail(A,C), f(C,B)

Suppose we have two positive examples e1 = f([4,3,4,6],3) and e2
= f([2,2,9,4,8,10],9) which correspond to odd elements in the sec-
ond and third positions of a list respectively. Also assume we
have suitable negative examples and that we restrict hypothe-
ses to deﬁnite programs (Lloyd 2012). Given these examples, our
approach ﬁrst learns a hypothesis (in this case a single rule) for
each example:

r1 = f(A,B) ← tail(A,C), head(C,B), odd(B)
r2 = f(A,B) ← tail(A,C), tail(C,D), head(D,B), odd(B)

 
 
 
 
 
 
These rules are too speciﬁc, i.e. they do not generalise. Rather
than stopping at this point, our approach searches again for
a hypothesis h2 that entails the larger chunk {e 1, e 2}. Rather
than blindly searching again, our approach reuses knowledge
from the ﬁrst iteration to restrict the hypothesis space. For in-
stance, let |h| denote the number of literals in the hypothesis
h and assume that r1 and r2 are the smallest (optimal) solu-
tions for e 1 and e 2 respectively. Since r1 and r2 are optimal,
the minimum size of h2 is max (|{r1}|, |{r2}|). Likewise, since
{r1, r2} |= {e 1, e 2}, we can bound the maximum size of h2 as
|{r1, r2}|. In this scenario of ﬁnding odd elements in a list, we
can bound the size of h2 as 5 ≤ |h2| < 9, greatly reducing the
hypothesis space. We can also restrict the hypothesis space us-
ing other knowledge. For instance, before searching for h2, we
can try r1 and r2 on {e 1, e 2}. As neither r1 nor r2 alone gener-
alises both examples, h2 cannot be a specialisation of r1 nor
r2, so we can prune all specialisations of both rules. As we ex-
perimentally show, this reuse of knowledge, i.e. the constrain
step, is important for good learning performance. Given this
constrained space, our approach searches again and ﬁnds the
optimal solution h1.

Our motivation for using a D&C approach is to reduce search
complexity by decomposing a learning task into smaller tasks
that can be solved separately. For instance, suppose we have 10
positive examples and that 6 require a hypothesis with 7 liter-
als; 2 require 8 literals; and 2 require 9 literals, i.e. the solution
for all the examples has 24 literals. For simplicity, consider a
generate-and-test approach that enumerates all hypotheses of
increasing size. With such an approach, the search complexity
sc (h) of ﬁnding the hypothesis h is sc (h) = c |h |
where c de-
notes the number of possible literals allowed in a hypothesis.
In this scenario, the complexity of ﬁnding the whole solution is
c 24
. By contrast, with a D&C approach, we can ﬁnd the individ-
ual hypotheses with the much lower cost of c 7 + c 8 + c 9
. Thus,
our main claim is that DCC can reduce search complexity and
thus improve learning performance.
Overall, our contributions are:

• We introduce a divide, constrain, and conquer (DCC) ILP ap-
proach. This anytime approach can learn optimal, recursive,
and large programs and perform predicate invention.

• We experimentally show on three domains (classiﬁcation,
inductive general game playing, and program synthesis)
that (i) our approach can substantially improve predictive
accuracies and reduce learning times, (ii) reusing learned
knowledge is vital for good learning performance, and (iii)
DCC can outperform other ILP systems.

2 Related Work

TILDE is a D&C approach. TILDE behaves similarly to the deci-
sion tree learning algorithm C4.5 (Quinlan 1993). To ﬁnd a hy-
pothesis TILDE employs a D&C strategy by recursively dividing
the examples into disjoint subsets. TILDE diﬀers from C4.5 by
how it generates candidate splits to partition the examples.
C4.5 generates candidates as attribute-value pairs. By contrast,
TILDE uses conjunctions of literals (i.e. clauses/rules). TILDE can
learn large programs and can scale to large datasets. However,
it has several limitations, notably an inability to learn recursive

programs, no predicate invention, and diﬃculty learning from
small numbers of examples.

Many

Progol is a S&C approach that has inspired many other ap-
proaches (Ray 2009; Ahlgren and Yuen 2013), notably Aleph.
Starting with an empty hypothesis, Progol picks an uncov-
ered positive example to generalise. To generalise an exam-
ple, Progol uses mode declarations to build the bottom clause
(Muggleton 1995), the logically most-speciﬁc clause that ex-
plains the example. The bottom clause bounds the search from
below (the bottom clause) and above (the empty set). Progol
then uses an A* algorithm to generalise the bottom clause in
a top-down (general-to-speciﬁc) manner and uses the other
examples to guide the search. Progol struggles to learn recur-
sive and optimal programs and does not support predicate in-
vention. Note that Progol variants, such as Aleph and ATOM
(Ahlgren and Yuen 2013), have the same limitations.
are meta-level
ILP systems

systems
(Cropper et al. 2021). These approaches encode the ILP
problem as a meta-level logic program, i.e. a program that
reasons about programs. Meta-level approaches often del-
egate the search for a solution to an oﬀ-the-shelf solver
Cropper and Muggleton
(Corapi, Russo, and Lupu
2011;
2016;
2018;
Law
Evans and Grefenstette 2018; Evans et al. 2021) after which
the meta-level solution is translated back to a standard
solution for the ILP task. For instance, ASPAL translates an
ILP task into a meta-level ASP program that describes every
example and every possible rule in the hypothesis space.
ASPAL then delegates the search to an ASP system to ﬁnd a
subset of the rules that covers all the positive but none of the
negative examples. Meta-level approaches can more easily
learn recursive programs and optimal programs.

Kaminski, Eiter, and Inoue

recent

2018;

A major issue with meta-level approaches is scalability. For
instance, ASPAL, HEXMIL (Kaminski, Eiter, and Inoue 2018), and
ILASP3 (Law 2018) all ﬁrst pre-compute every possible rule
in the hypothesis space which they pass to an ASP solver.
This approach scales well when solutions require many rules
with few body literals. However, this approach does not scale
well when solutions require rules with many body literals
(Cropper and Morel 2021a), since there are exponentially more
rules given more body literals.

To improve the scalability of meta-level systems, Popper
(Cropper and Morel 2021a,b) does not precompute every possi-
ble rule in the hypothesis space. Instead, Popper lazily gener-
ates rules. The key idea of Popper is to discover constraints
from smaller rules (and hypotheses) to rule out larger rules.
Popper can learn optimal and recursive programs and perform
predicate invention. However, Popper searches for a single so-
lution for all the examples and struggles to learn solutions
with many literals. As we experimentally demonstrate, our DCC
approach can substantially outperform Popper and other sys-
tems.

We have said that some ILP approaches struggle to learn
large programs. However, what constitutes a large program
is unclear. Most authors measure the size of a logic pro-
gram as either the number of literals (Law 2018) or rules
(Muggleton, Lin, and Tamaddoni-Nezhad 2015) in it. However,
these two metrics are too simple. For instance, many ap-
proaches can easily learn programs with lots of clauses by sim-

ply memorising the examples. Likewise, approaches based on
inverse entailment (Muggleton 1995) can easily learn programs
with lots of literals by simply returning the bottom clause. In
this paper, we do not formally deﬁne what constitutes a large
program. By large, we informally mean programs with reason-
ably large numbers of rules, variables, and literals.

3 Problem Setting

Our problem setting is the learning from failures (LFF)
(Cropper and Morel 2021a) setting. LFF uses hypothesis con-
straints to restrict the hypothesis space. Let L be a lan-
guage that deﬁnes hypotheses, i.e. a meta-language. For in-
stance, consider a meta-language formed of two literals h_lit/4
and b_lit/4 which represent head and body literals respec-
tively. With this language, we can denote the clause last(A,B)
← tail(A,C), head(C,B) as the set of literals {h_lit(0,last,2,(0,1)),
b_lit(0,tail,2,(0,2)), b_lit(0,head,2,(2,1))}. The ﬁrst argument of
each literal is the clause index, the second is the predicate
symbol, the third is the arity, and the fourth is the literal vari-
ables, where 0 represents A, 1 represents B, etc. A hypothesis
constraint is a constraint (a headless rule) expressed in L. Let
C be a set of hypothesis constraints written in a language L. A
set of deﬁnite clauses H is consistent with C if, when written
in L, H does not violate any constraint in C . For instance, the
constraint ← h_lit(0,last,2,(0,1)), b_lit(0,last,2,(1,0)) would be vi-
olated by the deﬁnite clause last(A,B) ← last(B,A). We denote
as HC the subset of the hypothesis space H which does not
violate any constraint in C .

We deﬁne the LFF problem:

(LFF input).

input
is a tuple
Deﬁnition 1
(E +, E −, B, H, C ) where E +
are sets of ground
atoms denoting positive and negative examples respectively;
B is a deﬁnite program denoting background knowledge; H is
a hypothesis space, and C is a set of hypothesis constraints.

The LFF
and E −

We deﬁne a LFF solution:

(LFF

solution).

Deﬁnition 2
tuple
(E +, E −, B, H, C ), a hypothesis H ∈ HC is a solution
when H is complete ([e ∈ E +, B ∪ H |= e ) and consistent
([e ∈ E −, B ∪ H 6|= e ).

Given

input

an

If a hypothesis is not a solution then it is a failure. A hypothesis
is incomplete when \e ∈ E +, H ∪ B 6|= e . A hypothesis is
inconsistent when \e ∈ E −, H ∪ B |= e . A hypothesis is
totally incomplete when [e ∈ E +, H ∪ B 6|= e .

Let cost : H ↦→ R be an arbitrary cost function that mea-
sures the cost of a hypothesis. We deﬁne an optimal solution:

Deﬁnition 3 (Optimal solution). Given an input
(E +, E −, B, H, C ), a hypothesis H
when (i) H is a solution, and (ii) [H ′ ∈ HC , where H ′
solution, cost (H ) ≤ cost (H ′).

tuple
∈ HC is optimal
is a

In this paper, our cost function is the number of literals in the
hypothesis H .

Constraints. The goal of an LFF learner is to learn hypoth-
esis constraints from failed hypotheses. Cropper and Morel
(2021a,b) introduce hypothesis constraints based on subsump-
tion (Plotkin 1971). A clause C1 subsumes a clause C2 (C1 (cid:22)
C2) if and only if there exists a substitution θ such that C1θ ⊆

C2. A clausal theory T1 subsumes a clausal theory T2 (T1 (cid:22) T2)
if and only if [C2 ∈ T2, \C1 ∈ T1 such that C1 subsumes C2.
A clausal theory T1 is a specialisation of a clausal theory T2 if
and only if T2 (cid:22) T1. A clausal theory T1 is a generalisation of
a clausal theory T2 if and only if T1 (cid:22) T2. If a hypothesis H is
incomplete, a specialisation constraint prunes specialisations
of H , as they are guaranteed to also be incomplete. If a hypoth-
esis H is inconsistent, a generalisation constraint prunes gen-
eralisations of H , as they are guaranteed to be inconsistent as
well. If a hypothesis H is totally incomplete, a redundancy con-
straint prunes hypotheses that contain a specialisation of H as
a subset.

4 DCC Algorithm

We now describe our DCC algorithm. We ﬁrst brieﬂy describe
Popper, which we use as our underlying search algorithm.

4.1 Popper

Algorithm 1 shows the Popper algorithm, which solves the
LFF problem (Deﬁnition 1). Popper takes as input background
knowledge (bk), positive (pos) and negative (neg) examples,
a set of hypothesis constraints (in_cons), and lower (min_m)
and upper (max_m) bounds on hypothesis sizes. Popper uses a
generate, test, and constrain loop to ﬁnd a solution.

Popper starts with a ASP program P (hidden in the gener-
ate function) whose models correspond to hypotheses (deﬁnite
programs). Popper augments P with ASP constraints to elimi-
nate models and thus prune hypotheses. In the generate stage
(line 5), Popper uses Clingo (Gebser et al. 2014), an ASP system,
to search for a model (a hypothesis) of P. A constraint ensures
that the hypothesis has exactly m literals. If a model is found,
Popper converts it to a hypothesis (h). Otherwise; Popper in-
crements the hypothesis size (line 7) and loops again.

If there is a hypothesis then in the test stage (line 9), Pop-
per tests it on the given training examples. If a hypothesis fails,
i.e. is incomplete or inconsistent, then in the constrain stage
(line 12), Popper deduces hypothesis constraints (represented
as ASP constraints) from the failure which it adds to the set
of constraints, which are in turn added to P to prune models
and thus restrict the hypothesis space. For instance, if a hypoth-
esis is incomplete, i.e. does not entail all the positive exam-
ples, then Popper builds a specialisation constraint to prune
hypotheses that are logically more speciﬁc.

To ﬁnd an optimal solution (i.e. one with the minimum num-
ber of literals), Popper progressively increases the number of
literals allowed in a hypothesis when the hypothesis space is
empty at a certain size (e.g. when P has no more models). This
loop repeats until either (i) Popper ﬁnds an optimal solution,
or (ii) there are no more hypotheses to test.

4.2 DCC

Algorithm 2 shows the DCC algorithm. Before describing it in
detail, we describe it at a high-level. The idea is to divide the
positive examples into chunks of size k . In the ﬁrst iteration
k = 1, each example is in its own chunk. DCC enumerates the
chunks and calls Popper to ﬁnd a hypothesis for the chunk ex-
amples and all the negative examples. After enumerating all
the chunks, DCC forms an iteration hypothesis as the union of

h = generate(cons, m)
if h == UNSAT:

cons = in_cons
m = min_m
while m ≤ max_m:

Algorithm 1: Popper
1 def popper(bk, pos, neg, in_cons, min_m, max_m):
2
3
4
5
6
7
8
9
10
11
12
13

outcome = test(pos, neg, bk, h)
if outcome == (COMPLETE, CONSISTENT)

cons += constrain(h, outcome)

return {}, cons

return h, cons

m += 1

else:

cons = {}
k = 1
best_h, score = None, 0
all_chunks = {{x} | x in pos}
exs_h = {}
while k ≤ |all_chunks|:

Algorithm 2: DCC
1 def dcc(bk, pos, neg):
2
3
4
5
6
7
8
9
10
11
12
13
14

iteration_hs = {}
for chunk in divide(all_chunks, k):

h = lazy_check(bk, chunk, iteration_hs)
if h == None:

chunk_cons = filter_c(cons, chunk)
min_m, max_m = calc_bounds(exs_h, chunk)
h, new_cons = popper(bk, chunk, neg,

chunk_cons, min_m, max_m)

all the chunk hypotheses. DCC then doubles the chunk size and
repeats the process until the chunk size exceeds the number of
examples. Without optimisations, DCC performs ln n iterations
where n is the number of positive examples.

We now describe DCC in detail. DCC takes as input back-
ground knowledge (bk), and positive (pos) and negative (neg)
examples. It maintains a set of constraints (cons) that is ini-
tially empty (line 2). DCC divides the positive examples into
chunks. In the ﬁrst iteration, each positive example is in its own
chunk (line 5). The while loop in Algorithm 2 builds a hypoth-
esis for a given chunk size. Line 8 creates an empty hypothe-
sis (iteration_hs) for the current chunk size. DCC divides the
chunks into smaller chunks (chunk) of size k and enumerates
them.

In line 10 DCC calls the function lazy_check. We delay dis-
cussion of this function until we have described the main DCC
loop but at a high-level this function tries to ﬁnd an already dis-
covered hypothesis that covers the current chunk. The purpose
is to reduce the number of calls to Popper.

Ignoring the lazy check, DCC tries to ﬁnd a hypothesis for
this chunk. The ﬁrst step (line 12) selects only the relevant con-
straints for the examples in the chunk (chunk_cons). All gen-
eralisation constraints are selected. Specialisation constraints
are only selected if they hold for at least one example in the
chunk. Redundancy constraints are selected only if they hold
for all examples in the chunk. The second step (line 13) deduces
hypothesis size bounds for the subsequent Popper search. The
minimum size is the largest best solution for each chunk exam-
ple. The maximum size is the size of the union.

DCC calls Popper with the ﬁltered constraints and hypothe-
sis size bounds. Popper returns a hypothesis (h) that entails
the chunk examples (if one exists) and a set of new constraints
(new_cons). DCC updates its constraints (line 15) with the new
ones. If a hypothesis is found, DCC adds it to the iteration hy-
pothesis (line 17) and updates a hash table that maintains the
last hypothesis for each example (line 18).

After passing through all the chunks, DCC forms a single it-
eration hypothesis as the union of all the hypotheses (line 19).
DCC calculates the score of this iteration hypothesis on all the
examples. We calculate the score as the number of correctly
generalised examples (true positives + true negatives). In fu-
ture work, we will explore alternative scoring functions, such
as those that minimise description length (Rissanen 1978). If

15
16
17
18
19
20
21
22
23
24
25

cons += new_cons

if h != None:

iteration_hs += h
exs_hs = update(exs_hs, h, chunk)

iteration_h = union(iteration_hs)
h_score = test(pos, neg, bk, iteration_h)
if h_score > best_score:

best_h, best_score = iteration_h, h_score
all_chunks = compress(iteration_hs, all_chunks)
k += k

return best_h

the score improves on the best score, DCC updates the best
hypothesis (line 22). Line 22 compresses the chunks. We delay
discussion of this function until we have described the main
DCC loop but at a high-level this function tries to merge exam-
ples to reduce the number of iterations. After enumerating all
the chunks, DCC doubles the chunk size (line 24) and repeats
the process. Once the loop has ﬁnished, DCC returns the best
hypothesis.

Laziness. The goal of laziness is to reduce the number of
calls to Popper. Suppose we have n chunks of examples
e 1, e 2, . . . , e n and that during the for loop in Algorithm 2 we
ﬁnd the solution h1 for e 1. We now want to ﬁnd a solution h2
for e 2. Suppose that h1 is a solution for e 2. Then do we need
to search for h2? On the one hand, h1 may be sub-optimal in
that there may be a smaller hypothesis that entails e 2. On the
other hand, since we need h1 (or a generalisation of it) to en-
tail e 1, we may as well reuse h1 as it requires adding no more
literals to our iteration hypothesis. Laziness generalises to all
previously found hypotheses. Without laziness, for a chunk with
n examples, DCC requires in the best- and worst-cases n calls
to Popper. Laziness reduces the best-case to 1. Moreover, we
can deduce additional constraints from these lazy checks. Sup-
pose that h1 does not entail e 2. We can therefore rule out all
specialisations of h1 when searching for h2 to further restrict
the hypothesis space. In Section 5, we experimentally evaluate
the impact of laziness on learning performance.
Compression. Given n positive examples e 1, e 2, . . . , e n , Al-
gorithm 2 searches for solutions h1, h2, . . . , hn respectively.
It then increases the chunk size and searches for solutions
h ′
1, h ′
for {e 1, e 2}, {e 3, e 4}, . . . , {e n−1, e n } and so
on. This approach requires in the best- and -worst-cases log n

2, . . . , hn/2

′

iterations and, without laziness, n log n calls to Popper. Sup-
pose that after the ﬁrst iteration we know that h1 entails {e 1},
{e 6}, and {e 9}; h2 entails {e 3} and {e 4}; and h3 entails {e 2},
{e 5}, and {e 13}. Then in the second iteration Algorithm 2 will
search for solutions h ′
1, h ′
2, . . . , hn/2′ etc. The two searches for
1 to cover {e 1, e 2} and h ′
h ′
3 to cover {e 5, e 6} are basically the
same. To reduce the number of iterations, we can compress the
chunks by the hypotheses that entail them. In other words, we
can merge two chunks if they are covered by the same hypoth-
esis. In the above case, we create a h1 bucket with {e 1, e 6, e 9},
a h2 bucket with {e 3, e 4}, and a h3 bucket with {e 2, e 5, e 13}.
With this compression approach in the second iteration, we now
search for h ′
1 for the chunk {e 1, e 6, e 9, e 3, e 4} (buckets 1 and
2) and h ′
2 for the chunk {e 2, e 5, e 13}. Compression reduces the
best-case number of iterations from log n to 1 and the best-
case number of calls (without laziness) to Popper from n log n
to n. In Section 5, we experimentally evaluate the impact of
compression on learning performance.

Anytime. DCC is an anytime algorithm. If at any point a user
stops the search or the search duration exceeds a timeout, DCC
returns the best hypothesis thus far.

5 Experiments

We claim that DCC can reduce search complexity and thus im-
prove learning performance. To evaluate this claim, our experi-
ments aim to answer the question:

Q1 Can DCC improve predictive accuracies and reduce learning

times?

To answer Q1, we compare DCC against Popper. This compari-
son allows us to answer Q1 as DCC uses Popper as its under-
lying search algorithm and both systems use identical biases.
Comparing against other systems will not allow us to answer
Q1.

DCC has various optimisations that we claim improve learn-
ing performance, notably reusing learned constraints; laziness,
and compression. To evaluate these features, our experiments
aim to answer three questions:

Q2 Can reusing constraints reduce learning times?

Q3 Can laziness reduce learning times?

Q4 Can compression reduce learning times?

To answer Q2-Q4, we compare the performance of DCC with and
without these optimisations.

Comparing DCC against other systems besides Popper can-
not help us answer questions Q1-Q4. However, many re-
searchers desire comparisons against ‘state-of-the-art’. To ap-
pease such a researcher, our experiments try to answer the
question:

Task

Num. rules

Num. literals Max rule size

trains1
trains2
trains3
trains4

1
2
3
4

6
11
17
26

6
7
7
7

Table 1: Trains tasks. The values are based on the optimal solu-
tion size. The BK contains 27k facts and 20 relations.

experimental results to conclude that system x is better than
system y .

5.1 Experimental Domains

We consider three domains.

Michalski Trains. Michalski trains (Larson and Michalski 1977)
is a classical problem. The task is to ﬁnd a hypothesis that dis-
tinguishes eastbound trains from westbound trains. Figure 1
shows an example hypothesis that says a train is eastbound if it
has a long carriage with two wheels and another long carriage
with three wheels. We use this domain because we can easily
generate progressively more diﬃcult tasks to test the scalabil-
ity of the approaches as the solution size grows. Table 1 shows
information about the four tasks we consider. There are 1000
examples but the distribution of positive and negative exam-
ples is diﬀerent for each task. We randomly sample the exam-
ples and split them into 80/20 train/test partitions

eastbound(A) ← has_car(A,B), long(B), two_wheels(B),

has_car(A,C), three_wheels(C)

Figure 1: Target solution for the trains1 task.

In

game

game

playing

general

general

inductive

from the

IGGP.
(IGGP)
(Cropper, Evans, and Law 2020) agents are given game
traces
competition
(Genesereth and Björnsson 2013). The task is to induce a
set of rules that could have produced these traces. We use four
IGGP games: minimal decay (md), rock, paper, scissors (rps),
buttons, and coins. We learn the next relation in each game,
which is the most diﬃcult to learn (Cropper, Evans, and Law
2020).

playing

Program synthesis.
Inducing complex recursive programs has
long been considered a diﬃcult problem (Muggleton et al.
2012) and most ILP systems cannot learn recursive pro-
grams. We use the program synthesis dataset introduced by
Cropper and Morel (2021a).

Q5 How does DCC compare against other approaches?

5.2 Systems

To answer Q5 we compare DCC against Popper, Metagol, Aleph,
and ILASP3. We describe these systems in Section 5.2. Note that
we do not claim that DCC is better than other systems. All sys-
tems have strengths and weaknesses. Moreover, as most sys-
tems use diﬀerent biases, a fair comparison is diﬃcult. Indeed,
there will always exist a set of settings whereby system x out-
performs system y . Therefore, the reader should not use our

To answer Q5, we compare DCC against Popper, Metagol, Aleph,
and ILASP3.

Metagol Metagol is one of the few systems that can learn
recursive Prolog programs. Metagol uses user-provided
metarules (program templates) to guide the search for a so-
lution. We use the approximate universal set of metarules
described by Cropper and Tourret (2020).

Aleph Aleph excels at learning many large non-recursive rules
and should excel at the trains and IGGP tasks. Although
Aleph can learn recursive programs, it struggles to do so.
DCC and Aleph use similar biases so the comparison can be
considered reasonably fair.

ILASP3 We tried to use ILASP3. However,

ILASP3 ﬁrst pre-
computes every possible rule in a hypothesis space. This
approach is infeasible for our datasets. For instance, on the
trains tasks, ILASP3 took 2 seconds to pre-compute rules
with three body literals; 20 seconds for rules with four body
literals; and 12 minutes for rules with ﬁve body literals. Since
the simplest train task requires rules with six body literals,
ILASP3 is unusable. In addition, ILASP3 cannot learn Prolog
programs so is unusable in the synthesis tasks.

5.3 Experimental Results

We measure predictive accuracy and learning time. We enforce
a timeout of ﬁve minutes per task. We repeat all the experi-
ments1 20 times and measure the mean and standard deviation.
We use a 3.8 GHz 8-Core Intel Core i7 with 32GB of ram. All the
systems use a single CPU.

Task

trains1
trains2
trains3
trains4

md
buttons
rps
coins

dropk
droplast
evens
ﬁnddup
last
len
sorted
sumlist

DCC

Popper

Aleph

Metagol

100 ± 0
98 ± 0
98 ± 0
100 ± 0

99 ± 0
98 ± 0
97 ± 0
86 ± 0

99 ± 0
100 ± 0
100 ± 0
98 ± 0
100 ± 0
100 ± 0
94 ± 2
100 ± 0

100 ± 0
98 ± 0
81 ± 1
42 ± 5

100 ± 0
19 ± 0
18 ± 0
17 ± 0

100 ± 0
100 ± 0
100 ± 0
98 ± 0
100 ± 0
100 ± 0
96 ± 1
100 ± 0

100 ± 0
100 ± 0
100 ± 0
39 ± 4

94 ± 0
87 ± 0
100 ± 0
17 ± 0

52 ± 2
50 ± 0
51 ± 0
50 ± 0
49 ± 0
50 ± 0
70 ± 1
50 ± 0

27 ± 0
19 ± 0
79 ± 0
32 ± 0

11 ± 0
19 ± 0
18 ± 0
17 ± 0

50 ± 0
50 ± 0
50 ± 0
50 ± 0
55 ± 3
50 ± 0
50 ± 0
62 ± 4

Task

trains1
trains2
trains3
trains4

md
buttons
rps
coins

dropk
droplast
evens
ﬁnddup
last
len
sorted
sumlist

DCC

Popper

Aleph

Metagol

8 ± 2
41 ± 12
106 ± 17
268 ± 9

172 ± 27
300 ± 0
282 ± 12
291 ± 4

3 ± 0.2
2 ± 0.2
5 ± 0.4
47 ± 6
2 ± 0.4
16 ± 2
29 ± 3
18 ± 0.3

2 ± 0
7 ± 0.9
295 ± 3
295 ± 2

52 ± 1
299 ± 0
285 ± 14
299 ± 0

2 ± 0.2
3 ± 0.1
4 ± 0.1
13 ± 0.3
2 ± 0.1
5 ± 0.1
19 ± 1
19 ± 0.6

4 ± 0.2
1 ± 0.1
35 ± 0.9
297 ± 1

3 ± 0
86 ± 1
4 ± 0.1
300 ± 0

3 ± 0.3
300 ± 0
1 ± 0
1 ± 0.1
1 ± 0
1 ± 0
1 ± 0
0.6 ± 0

300 ± 0
300 ± 0
300 ± 0
300 ± 0

300 ± 0
300 ± 0
0.3 ± 0
0.4 ± 0

0.3 ± 0
300 ± 0
217 ± 26
300 ± 0
270 ± 20
300 ± 0
288 ± 11
225 ± 29

Table 3: Learning times. We round times over one second to the
nearest second. The error is standard deviation.

task

trains1
trains2
trains3
trains4

md
buttons
rps
coins

dropk
droplast
evens
ﬁnddup
last
len
sorted
sumlist

DCC

100 ± 0
98 ± 0
98 ± 0
100 ± 0

99 ± 0
98 ± 0
97 ± 0
86 ± 0

99 ± 0
100 ± 0
100 ± 0
98 ± 0
100 ± 0
100 ± 0
94 ± 2
100 ± 0

Without
constraints

95 ± 3
90 ± 1
70 ± 4
77 ± 2

92 ± 0
84 ± 0
86 ± 0
77 ± 4

90 ± 4
100 ± 0
83 ± 5
56 ± 3
82 ± 5
85 ± 5
60 ± 2
95 ± 3

Without
laziness

100 ± 0
96 ± 1
97 ± 0
97 ± 0

95 ± 1
96 ± 0
95 ± 0
71 ± 6

99 ± 0
100 ± 0
100 ± 0
99 ± 0
100 ± 0
98 ± 1
96 ± 1
100 ± 0

Without
compression

100 ± 0
98 ± 0
98 ± 0
100 ± 0

98 ± 0
98 ± 0
97 ± 0
81 ± 3

99 ± 0
100 ± 0
100 ± 0
97 ± 2
100 ± 0
100 ± 0
96 ± 1
100 ± 0

Table 2: Predictive accuracies. We round accuracies to integer
values. The error is standard deviation.

Table 4: Predictive accuracies. We round accuracies to integer
values. The error is standard deviation.

Q1. Can DCC improve predictive accuracies and reduce learn-
ing times? Table 2 shows the predictive accuracies of DCC and
Popper. The results show that DCC outperforms Popper, espe-
cially on the trains and IGGP tasks. A McNemar’s test conﬁrms
the signiﬁcance of the diﬀerence between DCC and Popper at
the p < 0.01 level. Where Popper has low predictive accuracy,
it is because it struggled to ﬁnd a solution in the given time
limit and thus returns an empty hypothesis2.

1The

experimental

code

and

data

are

available

at

https://github.com/logic-and-learning-lab/aaai22-dcc.

2DCC and Popper (and Metagol) are all guaranteed to ﬁnd an opti-
mal solution, if one exists. They key diﬀerence is how long they take to
ﬁnd it. In this experiment, we use a 5 minute timeout. However, increas-
ing the timeout does not change the results. For instance, we repeated

Table 3 shows that learning times of DCC and Popper. A
paired t-test conﬁrms that the learning times of DCC and Pop-
per are not signiﬁcantly diﬀerent. This result may surprise a
reader. How can DCC achieve higher predictive accuracy than
Popper yet have the same learning time given that it calls Pop-
per? The reason is that DCC is an anytime algorithm. On some
tasks, DCC ﬁnds the optimal solution early in its search but is
unable to prove that it is optimal so continues to search until
it reaches the maximum learning time.

one trial of the buttons experiment with 4 hour timeout. Even with this
large timeout, Popper and Metagol could still not learn any solution.
By contrast, DCC learns an almost perfect solution in under 5 minutes.

task

trains1
trains2
trains3
trains4

md
buttons
rps
coins

dropk
droplast
evens
ﬁnddup
last
len
sorted
sumlist

DCC

8 ± 2
41 ± 12
106 ± 17
268 ± 9

172 ± 27
300 ± 0
282 ± 12
291 ± 4

3 ± 0.2
2 ± 0.2
5 ± 0.4
47 ± 6
2 ± 0.4
16 ± 2
29 ± 3
18 ± 0.3

Without
constraints

38 ± 20
285 ± 14
300 ± 0
300 ± 0

300 ± 0
300 ± 0
300 ± 0
300 ± 0

62 ± 27
4 ± 0.2
109 ± 32
272 ± 19
107 ± 32
95 ± 30
286 ± 14
50 ± 19

Without
laziness

300 ± 0
219 ± 20
300 ± 0
300 ± 0

251 ± 20
300 ± 0
300 ± 0
300 ± 0

20 ± 2
101 ± 2
75 ± 3
178 ± 15
46 ± 3
143 ± 14
74 ± 7
300 ± 0

Without
compression

12 ± 3
151 ± 25
300 ± 0
300 ± 0

256 ± 22
300 ± 0
300 ± 0
300 ± 0

4 ± 0.5
4 ± 0.2
11 ± 3
102 ± 19
4 ± 1
36 ± 14
109 ± 21
22 ± 0.5

Table 5: Learning times. We round times over one second to the
nearest second. The error is standard deviation.

Q2. Can reusing constraints reduce learning times? Tables 4
and 5 show that reusing constraints is important for high pre-
dictive accuracy and low learning times. A McNemar’s test and
a paired t-test conﬁrmed the signiﬁcance of the accuracy and
time results respectively at the p < 0.01 level. For instance,
without constraint reuse, DCC takes 107s to learn a last solu-
tion with 82% predictive accuracy. By contrast, with constraint
reuse DCC takes 2s to learn a solution with 100% accuracy.

Q3. Can laziness reduce learning times? The results show that
laziness can drastically reduce learning times. A paired t-test
conﬁrms the signiﬁcance at the p < 0.01 level. For instance, for
the trains1 task, laziness reduces the learning time from 300s
(timeout) to 8s.

Q4. Can compression reduce learning times? The results show
that compression can drastically reduce learning times. A
paired t-test conﬁrms the signiﬁcance at the p < 0.01 level. For
instance, for the sorted task, compression reduces the learning
time from 109s to 29s.

The re-
Q5. How does DCC compare against other ILP systems?
sults show that DCC generally outperforms the other systems in
terms of predictive accuracy. Aleph performs well on the trains
and IGGP tasks but struggles on the program synthesis tasks.
Popper performs well on the trains and program synthesis tasks
but struggles on the IGGP tasks. Metagol struggles on most
tasks because it uses metarules with at most two body literals.
To learn rules with more than two body literals, Metagol must
invent new predicates, thus increasing the hypothesis size and
hypothesis space. The most notable diﬀerence in accuracies is
in the coins task. DCC achieves 86% accuracy. By contrast, the
other systems could only achieve the default accuracy of 17%

6 Conclusions and Limitations

The fundamental challenge in ILP is to eﬃciently search a large
hypothesis space. To address this challenge, we have intro-

duced an approach called divide, constrain, and conquer (DCC).
Our approach combines classical divide-and-conquer search
with modern constraint-driven search. Our anytime approach
can learn optimal, recursive, and large programs and perform
predicate invention. Our experiments on three domains (classi-
ﬁcation, inductive general game playing, and program synthe-
sis) show that (i) our approach can drastically improve predic-
tive accuracies and reduce learning times, (ii) reusing learned
knowledge is vital for good learning performance, and (iii) our
approach can outperform other ILP systems.

Limitations and Future Work

Noise.
In contrast to many other systems, such as TILDE and
Aleph, DCC cannot (explicitly) handle misclassiﬁed examples.
However, due to the D&C approach, DCC naturally provides a
method to handle misclassiﬁed positive examples. In future
work we want to extend the approach to handle misclassiﬁed
negative examples.

Expressivity. As DCC uses Popper as its underlying search al-
gorithm, it inherits some of the limitations of Popper, such
as no explicit support for non-observational predicate learn-
ing (Muggleton and Bryant 2000), where a hypothesis must de-
ﬁne rules for predicates not seen in the training examples3, and
a restriction to deﬁnite programs. Future work should address
these limitations.

Parallelisation. Although multi-core machines are ubiquitous,
most ILP approaches are single-core learners, including all the
systems mentioned in this paper. However, our DCC algorithm
is trivially parallelisable as we can independently search for a
solution for each chunk. In future work, we want to explore par-
allel DCC approaches.

Acknowledgments. This work was supported by the EPSRC fel-
lowship The Automatic Computer Scientist (EP/V040340/1).

References

Ahlgren, J.; and Yuen, S. Y. 2013. Eﬃcient program synthesis us-
J.
ing constraint satisfaction in inductive logic programming.
Machine Learning Res., 14(1): 3649–3682.

Blockeel, H.; and Raedt, L. D. 1998. Top-Down Induction of First-
Order Logical Decision Trees. Artif. Intell.

Corapi, D.; Russo, A.; and Lupu, E. 2011. Inductive Logic Program-
ming in Answer Set Programming. In Muggleton, S.; Tamaddoni-
Nezhad, A.; and Lisi, F. A., eds., Inductive Logic Programming -
21st International Conference, ILP 2011, Windsor Great Park, UK,
July 31 - August 3, 2011, Revised Selected Papers, volume 7207 of
Lecture Notes in Computer Science, 91–97. Springer.

Cropper, A.; Dumancic, S.; Evans, R.; and Muggleton, S. H. 2021.
Inductive logic programming at 30. arXiv.

Cropper, A.; Evans, R.; and Law, M. 2020. Inductive general game
playing. Machine Learning, 109(7): 1393–1434.

Cropper, A.; and Morel, R. 2021a. Learning programs by learning
from failures. Mach. Learn., 110(4): 801–856.

3Because Popper supports predicate invention, the distinction be-
tween OPL and non-OPL is unclear as by deﬁnition an invented predi-
cate symbol is not given in the examples.

Cropper, A.; and Morel, R. 2021b. Predicate Invention by Learning
From Failures. CoRR.

Cropper, A.; and Muggleton, S. H. 2016. Metagol System.
https://github.com/metagol/metagol.

Cropper, A.; and Tourret, S. 2020. Logical reduction of metarules.
Machine Learning, 109(7): 1323–1369.

Evans, R.; and Grefenstette, E. 2018. Learning Explanatory Rules
from Noisy Data. J. Artif. Intell. Res., 61: 1–64.

Evans, R.; Hernández-Orallo, J.; Welbl, J.; Kohli, P.; and Sergot,
M. J. 2021. Making sense of sensory input. Artif. Intell., 293:
103438.

Gebser, M.; Kaminski, R.; Kaufmann, B.; and Schaub, T. 2014.
Clingo = ASP + Control: Preliminary Report. CoRR, abs/1405.3694.

Genesereth, M. R.; and Björnsson, Y. 2013. The International Gen-
eral Game Playing Competition. AI Magazine, 34(2): 107–111.

Kaminski, T.; Eiter, T.; and Inoue, K. 2018. Exploiting Answer
Set Programming with External Sources for Meta-Interpretive
Learning. Theory Pract. Log. Program., 18(3-4): 571–588.

Larson, J.; and Michalski, R. S. 1977.
decision rules. SIGART Newsletter, 63: 38–44.

Inductive inference of VL

Law, M. 2018. Inductive learning of answer set programs. Ph.D.
thesis, Imperial College London, UK.

Lloyd, J. W. 2012. Foundations of logic programming. Springer
Science & Business Media.

Muggleton, S. 1991. Inductive Logic Programming. New Genera-
tion Computing, 8(4): 295–318.

Muggleton, S. 1995. Inverse Entailment and Progol. New Gener-
ation Comput., 13(3&4): 245–286.

Muggleton, S.; and Bryant, C. H. 2000. Theory Completion Using
Inverse Entailment.
In Cussens, J.; and Frisch, A. M., eds., In-
ductive Logic Programming, 10th International Conference, ILP
2000, London, UK, July 24-27, 2000, Proceedings, volume 1866 of
Lecture Notes in Computer Science, 130–146. Springer.

Muggleton, S.; De Raedt, L.; Poole, D.; Bratko, I.; Flach, P. A.; Inoue,
K.; and Srinivasan, A. 2012. ILP turns 20 - Biography and future
challenges. Machine Learning, 86(1): 3–23.

Muggleton, S. H.; Lin, D.; and Tamaddoni-Nezhad, A. 2015. Meta-
interpretive learning of higher-order dyadic Datalog: predicate
invention revisited. Machine Learning, 100(1): 49–73.

Plotkin, G. 1971. Automatic Methods of Inductive Inference. Ph.D.
thesis, Edinburgh University.

Quinlan, J. R. 1986. Induction of Decision Trees. Machine Learn-
ing, 1(1): 81–106.

Quinlan, J. R. 1993. C4.5: Programs for Machine Learning.

Ray, O. 2009. Nonmonotonic abductive inductive learning.
Applied Logic, 7(3): 329–340.

J.

Rissanen, J. 1978. Modeling by shortest data description. Autom.,
14(5): 465–471.

Srinivasan, A. 2001. The ALEPH manual. Machine Learning at
the Computing Laboratory, Oxford University.

Stahl, I. 1995. The Appropriateness of Predicate Invention as
Bias Shift Operation in ILP. Machine Learning, 20(1-2): 95–117.

