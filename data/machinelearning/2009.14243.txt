0
2
0
2

p
e
S
9
2

]
T
E
.
s
c
[

1
v
3
4
2
4
1
.
9
0
0
2
:
v
i
X
r
a

Temporal State Machines: Using temporal memory to stitch time-based graph
computations

ADVAIT MADHAVAN∗, University of Maryland and National Institute of Standards and Technology
MATTHEW W. DANIELS∗ and MARK D. STILES, National Institute of Standards and Technology

Race logic, an arrival-time-coded logic family, has demonstrated energy and performance improvements for applications ranging from

dynamic programming to machine learning. However, the ad hoc mappings of algorithms into hardware result in custom architectures

making them difficult to generalize. We systematize the development of race logic by associating it with the mathematical field called

tropical algebra. This association between the mathematical primitives of tropical algebra and generalized race logic computations

guides the design of temporally coded tropical circuits. It also serves as a framework for expressing high level timing-based algorithms.

This abstraction, when combined with temporal memory, allows for the systematic generalization of race logic by making it possible

to partition feed-forward computations into stages and organizing them into a state machine. We leverage analog memristor-based

temporal memories to design a such a state machine that operates purely on time-coded wavefronts. We implement a version of

Dijkstra’s algorithm to evaluate this temporal state machine. This demonstration shows the promise of expanding the expressibility of

temporal computing to enable it to deliver significant energy and throughput advantages.

Additional Key Words and Phrases: Temporal computing, temporal state machines, graph algorithms

ACM Reference Format:

Advait Madhavan, Matthew W. Daniels, and Mark D. Stiles. 2020. Temporal State Machines: Using temporal memory to stitch time-based

graph computations. 1, 1 (October 2020), 25 pages. https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION

Energy efficiency is a key constraint when designing modern computers. The performance and efficiency of modern

computers, which largely rely on Boolean encoding, can be attributed to developments across the computational stack

from transistors through circuits, architectures, and other mid- to high-level abstractions. The recent stagnation of

progress at the transistor level [29] is leading designers to make improvements at the lowest levels of the stack. These

include re-imagining how data is encoded in physical states and introducing novel devices. The rationale is simple:

making the fundamental mathematical operations required for computation more efficient can have a cascading effect

on the whole architecture. However, novel encoding schemes and devices come with new trade-offs that differ from

those of conventional Boolean computing schemes and which are not yet well understood.

In this paper, we focus on an arrival-time encoding known as race logic [42]. Since digital transitions (edges) account

for much of the energy consumption in traditional computation, race logic encodes multi-bit information in a single

∗Both authors contributed equally to this research.

Authors’ addresses: Advait Madhavan, advait.madhavan@nist.gov, University of Maryland and National Institute of Standards and Technology; Matthew
W. Daniels, matthew.daniels@nist.gov; Mark D. Stiles, mark.stiles@nist.gov, National Institute of Standards and Technology.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

© 2020 Association for Computing Machinery.
Manuscript submitted to ACM

Manuscript submitted to ACM

1

 
 
 
 
 
 
2

Advait Madhavan, Matthew W. Daniels, and Mark D. Stiles

edge per wire. The arrival time t of this single edge is the value encoded by the signal. Encoding multiple bits on a

single wire makes some operations very simple to implement. Standard and and or gates naturally implement the

max and min functions; a unit delay element acts as an increment gate. A fourth logic gate, inhibit, allows its first

(inhibiting) input to block the second input signal, if the inhibiting signal arrives earlier.

The development of race-logic-based architectures has been largely ad hoc. Race logic was first developed to accelerate

dynamic programming algorithms [42], and its application space has expanded to include machine learning [64] and

sorting networks [49], demonstrating energy and performance advantages. Parallel development of logical frame-

works [44, 58, 66], novel device technologies [43, 67], and fabricated chips [41] have contributed to a cross-stack effort

to make this encoding scheme technologically viable. Here, we offer two important developments.

The first development is a systematized method of building computing architectures. An imporant step is to identify

a suitable mathematical foundation that can express problems uniquely suited to a race logic approach. Formal logic,

computation, and verification frameworks have been developed [58, 65, 66]. Continued progress requires identifying a

mathematical algebra in which race logic algorithms and state machines are naturally expressed in the highly parallel

dataflow contexts typical of temporal computing accelerators. We propose tropical algebra to be used in this context.

The second development is a compositional framework for programatically linking low-level subroutines into higher-

order functions. This development expands race logic beyond one-shot application-specific circuits uses accelerator

based architectures coupled with the absence of temporal memory technologies. Recent work has started to explore

several device concepts for efficiently reading and writing time-coded signals [43, 67]. The advantage of such memories

is that they can directly interface with the temporal domain; read and write operations in such a memory can be

performed without conversion to digital encoding.

The introduction of temporal memory technologies allows race logic to serve as an efficient computational fabric

for two distinct but compatible advances. First, because memory breaks symmetries related to translations of the

time coordinate, a temporal computer equipped with a memory is no longer subject to the invariance constraint on

time-coded functions outlined in Refs. [44, 58]. Lifting this restriction, allows tropical algebra to serve as a coherent

algebraic context for designing and interfacing race logic circuits. Second, memories allow us to reach beyond specialized

one-shot temporal computations. Primitive race logic operations can be composed and iterated upon by saving outputs

of one circuit and rerunning that circuit on the saved state, the temporal equivalent of a classical state machine. In this

paper, we develop the temporal state machine as a tool for accelerating tropical algebra in a generalizable computational

fabric of high-efficiency race logic circuits.

Our contributions are:

• A description of a temporal state machine that solves temporal problems in systematized parts, providing a clear

computational abstraction for stitching larger computations out of primitive race logic elements.

• An exposition of tropical algebra as a mathematical framework for working with temporal vectors. We explain
the mapping into tropical algebra from race logic and how it provides a convenient mathematical setting for

working with temporal computations.

• Augmentations to conventional 1T1R (1 transistor, 1 resistor) arrays that make the crossbar architecture natively
perform fundamental tropical operations. We use this, and other temporal operations, to create a more general

feed-forward temporal computation unit.

• Demonstration and evaluation of a temporal state machine which uses Dijkstra’s shortest path algorithm to find

the minimal spanning tree on directed acyclic graphs.

Manuscript submitted to ACM

Temporal State Machines: Using temporal memory to stitch time-based graph computations

3

The paper is organized as follows. Section 2 briefly describes race logic and tropical algebra, showing the mapping

between them. Based on that mapping, we describe circuit implementations of important tropical operations as the basic

generators of higher order temporal functions. Section 3 introduces temporal state machines, explaining time-coded

states and transition functions. We represent simple problems tropically and demonstrate how such a state machine

can solve them in discrete steps. Section 4 presents a case study implementating Dijkstra’s algorithm on a temporal

state machine, and proposes a purely temporal version of the algorithm. Performance and energy numbers follow in

Section 5, followed by a comparison with previous work and discussion in Section 6.

2 TROPICAL ALGEBRA AND RACE LOGIC: MAPPING BETWEEN CIRCUITS AND SEMIRINGS

2.1 Race logic and temporal computing

Computing with time traces back to two communities, one, bio-inspired and the other purely efficiency oriented. The

biological interest in precise timing relationships between spikes grew after the seminal works by Thorpe on the

processing speed of the human visual system [26, 63] and on spike timing dependent plasticity by Bi and Poo [7].

From then, temporal wavefront computation [20, 30, 54, 68] in the biological community expanded to the machine

learning and neuromorphic computing communities [37, 51, 52]. References [8, 48, 53, 62, 71] show state of the art

performance and learning strategies in temporal neural networks, while the neuromorphic computing community in

Refs. [3, 16, 18, 23, 35, 51, 55] developed hardware to emulate precise timing relationships in spiking neural activity.

More recently, precise timing based codes in spiking neural networks perform a variety of applications such as graph

processing [28, 33], median filtering [69], image processing [69] and dynamic programming [2]. For several decades,

the circuit community has independently been using time domain mixed signal analog techniques in Analog/Time to

Digital Converters [50, 74], clock recovery circuits, phase and delay locked loops, phase detectors and arbiters. With

shrinking voltage levels and diminishing headroom, the temporal domain becomes attractive for analog processing.

With the interest in emerging computing paradigms, this community has contributed temporal coded complementary

metal-oxide-semiconductor (CMOS) only computational approaches [14, 19, 45, 46, 56].

Race logic sits between the aforementioned approaches in that it uses biologically-inspired wavefronts as the

fundamental data structure, while using conventional digital CMOS circuits to compute. Race logic encodes information

in the timing of rising digital edges and computes by manipulating delays between racing events. In the conventional
Boolean domain, the electrical behaviour of wires changing voltage from ground to Vdd is interpreted as changing from
logic level 0 to logic level 1 at time t. In race logic, these wires are understood to encode each t as their value, since the
rising edge arrives at t with respect to a temporal origin at t = 0. In some cases, a voltage edge can fail to appear on
a wire within the allotted operational time of a race logic computation. In these cases, we assign the value temporal
infinity, represented by the ∞ symbol.

We define race logic without memory elements as pure race logic, which accounts for most of the extant literature.

We call race logic that uses dynamic memory elements stateful or impure race logic. Our goal here is to describe stateful

race logic, but first we review issues that arise in pure race logic. The class of functions that can be implemented in

pure race logic is constrained by physics [44, 58] through causality and invariance. The causal constraint, also called

non-prescience, requires that the output of a race logic function be greater than or equal to at least one of the function’s

inputs. Any output must be caused by an input that arrives either earlier than or simultaneously with that output.

The invariance constraint arises because the circuit is indifferent to the choice of temporal origin. It is satisfied by
race logic functions f for which f (t1 + δ, t2 + δ, · · · , tN + δ ) = f (t1, t2, · · · , tN ) + δ ; all operations in pure race logic

Manuscript submitted to ACM

4

Advait Madhavan, Matthew W. Daniels, and Mark D. Stiles

Table 1. List of symbols related to race logic and tropical algebra and their meanings

Symbol Meaning

Description

∞
⊗
⊕
⊕′
⊣
=
:=
:(cid:27)

infinity
add
min
max
inhibit
equivalence
storage
normalized storage

additive identity in tropical algebra; edge that never arrives in race logic
multiplicative operation in tropical algebra; temporal delay in race logic
additive operation in tropical algebra; first arrival in race logic
alternate additive operation in tropical algebra; last arrival in race logic
ramp function in tropical algebra; signal blocking in race logic
expressing equality between two statements
storing a signal in memory
storing a signal in memory by first performing a normalizing operation

must obey this equality. Invariance need not apply to impure circuits, which contain a memory or state element: such

circuits perform differently at different times, depending on whether a memory element has been modified. From a

programming perspective, a pure function is akin to a function in mathematics which always gives the same output

when presented with the same input; an impure function is closer to a subroutine that can access and modify global

variables.

2.2 Tropical algebra

Named in honor of Brazilian mathematician Imre Simon, tropical algebra treats the tropical semiring T. In T, the
operations of addition and multiplication obey the familiar rules of commutativity, distributivity, and so on, but are

replaced by different functions. The tropical multiplicative operation is conventional addition, and the tropical additive

operation is either min or max; the choice of additive operation distinguishes two isomorphic semirings. Depending on
the choice of min or max as the additive operation, the semiring is given by T = (R ∪{∞}, ⊕, ⊗) or T = (R ∪{−∞}, ⊕′, ⊗);
±∞ are included to serve as additive identities.1 These symbols, and others used in this paper, are collected for reference
in Table 1. That some of the generating operations of tropical algebra correspond directly to the primitive operations of
race logic suggests that it is an ideal setting for the development of time-coded algorithms.2

Tropical algebra has found numerous applications in the computing literature particularly in a variety of graph

algorithms, such as shortest path finding, graph matching and alignment, and minimal spanning trees. It is used as the

basis of GraphBLAS (Graph Basic Linear Algebra Subprograms) [34]. In mathematics, it is being used to explore problems

in combinatorial optimization [5], control theory, machine learning [78], symplectic geometry [6], and computational

biology [75].

There are some fundamental similarities between tropical algebra and race logic. Both of them have an ∞ element.
In race logic, it physically corresponds to a signal that never arrives, while in tropical algebra it corresponds to the

additive identity, since

α ⊕ ∞ = min(α, ∞) = α .

(1)

1By contrast, the ring of real arithmetic is (R, +, ×).
2While tropical algebra is defined over the real numbers with infinity, a race logic circuit can practically represent only a finite discrete set of signal timings.
Race logic and tropical algebra are therefore not isomorphic, per se. Note that the same is true of a traditional computer with respect to conventional real
arithmetic. However, just as traditional computers can operate over a large enough subring of the reals to produce useful calculations, there exists an
embedding of min-based race logic as a subsemiring of tropical algebra. Regardless of whether we work in a subset of natural numbers (clocked race
logic) or the reals (analog race logic), the fact that this mapping is well-behaved ensures that tropical algebra is a useful mathematical landscape for
understanding race logic operations.

Manuscript submitted to ACM

Temporal State Machines: Using temporal memory to stitch time-based graph computations

5

b

c

2

2

1

(a)

a

(b)

4

1

d

a

b

c

d

(c)

(d)

1

1

2

2

4

a

b

c

d

a

b

c

d

(e)

(f)

1

1

2

2

4

a

b

c

d

∞ ∞ 1 ∞


2 ∞ ∞ ∞


∞ 2 ∞ ∞


1 ∞
∞ 4












=

∞


∞


2


4












∞ ∞ 1 ∞


2 ∞ ∞ ∞


∞ 2 ∞ ∞


1 ∞
∞ 4












∞
0
∞
∞



















=

1


∞


2


1












∞ ∞ 1 ∞


2 ∞ ∞ ∞


∞ 2 ∞ ∞


1 ∞
∞ 4












∞
0
0
∞



















Fig. 1. Tropical matrices for graph exploration: (a) shows an example directed graph; (b) shows the equivalent weighted adjacency
matrix. Panel (c) shows the propagation of a signal originating at node b through a delay network corresponding to the edges of the
example graph. Panel (d) shows the tropical vector-matrix multiplication corresponding to panel (c). Panels (e) and (f) repeat these
representations for the case where signals are injected at both b and c.

Such an addition doesn’t have an inverse, since there is no value of β in min(α, β) that would give ∞. The non-invertibility
of addition means that this algebra is a semi-ring and fundamentally winner-take-all in nature. Every time the additive

operation is performed, the smallest number (the first arriving signal in race logic) “wins” and propagates further
through the computation.3 The multiplicative identity in tropical algebra is zero, rather than one, since α ⊗ 0 = α + 0 = α.

2.3 Graph problems in tropical algebra

Tropical algebra can be especially useful for graph analytics, where it provides a simple mathematical language for

graph traversal. A fundamental concept in graph traversal is the graph’s weighted adjacency matrix A. Figs. 1(a) and
(b) show a directed graph and its weighted adjacency matrix, respectively. The ith column of the weighted adjacency
matrix represents the distances of the outward connections from node i to all other nodes in the graph, so that Aji is
the weight for the edge i → j. Where there is no edge to node i from j, we assign the value Aji = ∞.

The usefulness of tropical algebra for graph traversal is seen when using A in a tropical vector-matrix multiplication.
Tropical vector-matrix multiplication (VMM) proceeds like conventional VMM, but with (⊕, ⊗) instead of (+, ×). As
shown in Fig. 1, each vector element is scaled (tropical multiplication) before they are all accumulated (tropical addition).

Extracting any single column from a matrix can be done by multiplying a one-hot vector as shown in Fig. 1. The tropical
one-hot vector has a single zero element with all other entries set to ∞; from Sec. 2.2. During scaling, the columns of
the adjacency matrix that correspond to the infinities of the one-hot vector get scaled to infinity (tropically multiplied
by ∞) while the remaining column, scaled by the multiplicative identity 0, is the output. The values stored in the output
vector represent the distances from the one hot node in the input vector. This operation represents a search from the

3If we had chosen ⊕′ instead of ⊕, the additive identity would be −∞, though we generally prefer the min-plus version of tropical algebra. In pure
race logic, ∞ corresponds to an edge that never arrived, whereas −∞ would correspond to an edge that had always been present—not to be confused
with an edge that arrived at t = 0. No nontrivial function in pure race logic can output −∞ due to the causality constraint, so the min-plus algebra has
considerably more practical utility in race logic.

Manuscript submitted to ACM

6

Advait Madhavan, Matthew W. Daniels, and Mark D. Stiles

node in question (decided by the one-hot vector) to all the connected nodes in the graph, and reports the distances

along all edges of this parallel search.

Using a “two-hot” vector for input, as shown in Fig. 1(d) outputs a tropical linear combination of two vectors,

corresponding to the “hot” columns of the adjacency matrix. The accumulation phase of the tropical VMM is nontrivial;
the ⊕ operation selects the smallest computed distance to each node for the output. The tropical VMM reports the
shortest distance to each node in the graph after a single edge traversal from either of the initial nodes specified by the

two-hot vector. Both steps—the exploration of a node’s neighbors and the elementwise minimum of possible parent

nodes associated with an output—are performed in parallel by a single matrix operation.

Representing a collective hop through the graph as a single matrix operation allows a series of matrix operations to

represent extended graph traversal. The shortest traversed distance to each node in a graph from an initial node x is

y = x ⊕ (x ⊗ A) ⊕ (x ⊗ A ⊗ A) ⊕ (x ⊗ A ⊗ A ⊗ A) ⊕ · · · .

(2)

The first term represents all single-hop shortest paths starting out from x, while the second term accounts for all the

two-hop shortest paths, and so on. Hence the tropical summation across all the terms in y allows it to encode the shortest

distances between the input node as specified by x, independent of the number of hops. Performing N such hops and

calculating the minimum distance across all of them is the key operation in various dynamic-programming-based

shortest path algorithms. This process makes tropical algebra the natural semiring for Dijkstra’s algorithm [47]. We use

these ideas to implement Dijkstra’s single-source shortest path algorithm in a stateful race logic system in Sec. 4.

2.4 Circuits for tropical linear algebra
Since tropically linear functions (cid:201)
j (aj ⊗ tj ) with the aj values constant satisfy the invariance condition, tropical
linear transformations may be carried out in pure race logic. In Section 2.1, we describe how single rising edges can be

used to encode information in their arrival time. Interpreting the edges as tropical scalars, we can see how or gates

and delay elements are modeled by tropical addition and multiplication. This understanding can also be extended to

tropical vectors. Section 2.3 describes how tropical vectors can be interpreted as distance vectors in graph operations.

These distance vectors can be interpreted temporally as a wavefront or a volley of edges measured with respect to a

temporal origin. Other researchers have proposed using such vectors as the primary data structure underlying temporal

computations [58, 62].

Just as conventional vectors can be normalized, so can tropical vectors. In tropical algebra, the vector norm is the
minimum element of the vector.4 Tropical division of a vector by its norm then corresponds to subtracting out this
minimum value from all the components. This ensures that at least one element of the tropical vector is zero. It is

common to regard a tropical vector as equivalent to its normalized version, implying that it only encodes information

about the shape of its temporal wavefront, and not about an arbitrarily chosen temporal origin. To accept this equivalence

is to work in the projective tropical vector space, and we refer to tropical vectors as being projectively equivalent if their

normalized versions are elementwise equal. In this paper we frequently make use of normalized tropical vectors and

describe a method to store vectors projectively. Not only are they commonly the naturally correct data structure, but

frequent renormalization of the temporal origin helps mitigate the limited dynamic range of our temporal encoding.

Keeping track of the relative normalization constants allows us to encode information that would nominally extend

beyond our dynamic range in a principled way.

4In the max-plus tropical semiring, the vector norm would be the maximum element of the vector. This vector magnitude operation is sometimes called
the L∞ norm.

Manuscript submitted to ACM

Temporal State Machines: Using temporal memory to stitch time-based graph computations

7

Fig. 2. Construction of race logic circuits for tropical algebra: Panel (a) shows a composite circuit for the tropical dot product operation.
A simple array of delay elements takes an incoming wavefront and delays its elements by the values stored in each of the delay
elements. This represents the tropical element-wise multiplication by constant operation. The output is then connected to a p-type,
metal-oxide semiconductor (PMOS) pre-charge pullup coupled with a nor-style pulldown network which behaves like a first arrival
detector and performs the tropical addition operation. Panel (b) combines multiple elements of panel (a) and scales this up to a 2D
array such that it performs tropical vector matrix multiplication, the critical operation for graph traversal as described in section 2.3.
Panel (c) shows a detailed circuit implementation of the tropical VMM cell. Each cell consists of two transistors, one for programming
and the other for operation, and a level shifter. In the programming mode, the array is used like a conventional 1T1R array and the
memristors are written to the appropriate resistance values. In the operation mode, the programming transistor is turned off, while
the gate capacitor (shown in figure) is charged through the memristor. The level shifter is used to make sure that the discharge time
constant is determined by the memristor charging process and not the pulldown of the transistor, by applying full swing inputs to the
pulldown transistor.

Once a wavefront of rising voltage edges is interpreted as a tropical vector, the techniques shown in Fig. 2 can

be used to implement tropical vector operations. Panel (a) shows the vectorized version of the tropical dot product

operation. First the column of delay elements delays each line in the incoming wavefront by a different amount. This

implements tropical multiplication by constants, and can be seen as superimposing the delay wavefront onto the

incoming wavefront. The outputs of such a circuit are then connected to the inputs of a pre-charge-based pullup with

an or-type pulldown network followed by an inverter. The circuit operation is divided in to two phases, the pre-charge

phase followed by the evaluation phase. In the pre-charge phase, the PMOS transistor has its input connected to ground,
causing the critical node to be pulled-up (connected to Vdd ). When the pre-charge phase ends, the PMOS transistor is
turned off, which maintains the potential at the critical node at Vdd . During the evaluation phase, the the first arriving
rising edge at the input of the one of the NMOS transistors, causes the critical node to discharge to ground, hence being

pulled-down to a potential of zero volts. This behaves as a first-arrival detection circuit that outputs a rising edge at the

minimum of the input arrival times, performing the min operation. It implements tropical vector addition. Combining

the delay (multiplication) with the min (summation), we get the tropical dot product operation. By replicating this

behavior across multiple stored vectors, as in panel (b), we get the tropical VMM operation, where the input vector

tropically multiplies a matrix.

To be specific, we consider versions of the tunable delay elements described in the previous paragraph that are

based on memristor or resistive random access memory (ReRAM) technology. In the tropical VMM such a device is

used as programmable resistor with a known capacitance to generate an RC delay [43]. The details of these tropical

vector algebra cells are shown in Fig. 2(c). The main element of this circuits is a 2T1R array comprised of a pulldown

transistor and a programming transistor. During the programming phase, the programming transistor coupled with the

Manuscript submitted to ACM

pre Tunable delay element pre (a) (b) Critical node Level  shifter pre programming  transistor pull-down transistor state capacitor (c) programming control Input line Output line 8

Advait Madhavan, Matthew W. Daniels, and Mark D. Stiles

programming lines can be used to apply the necessary read and write voltages across the memristor, thus changing

the resistance and therefore RC delay time stored in the device. During the operation of the circuit, the programming

transistor is turned off to decouple the programming lines from the active circuitry. In the pre-charge phase, the output
lines are pulled up to Vdd through the pullup transistor. In the evaluation phase, the input lines receive temporally
coded rising edges which charge up the gate capacitors as shown in Fig. 2. This causes the pulldown transistor to be

turned on at the time proportional to input arrival times plus the RC time constant of the coupled memristor-capacitor

in each cell, faithfully performing the tropical VMM operation.

The largest read voltage that can applied across the device without disturbing the state of the device is approximately

600 mV. In a 180 nm process, this value is only a few 100 mV above the transistor threshold voltage and would cause a

slow and delayed leak. This leak allows multiple inputs to affect the pulldown simultaneously, influencing the functional

correctness of the circuit. We propose two solutions to this problem. Figure 2(c) shows a level shifter added between the

memristor and the pulldown transistor, the full swing of which causes the pulldown transistor to work much faster. In
an alternate approach (not shown here), a medium Vth device is used for the pulldown. Such devices ensure a small
footprint as well as correct operation, provided the fabrication process allows them.

In addition to tropical VMM based linear transformations, other primitive vector operations are crucial in many real

applications. Elementwise min and max can be performed with arrays of or and and gates, respectively. Vectors can

also be reduced to scalars by computing min or max amongst all elements using multi-input or and and gates.

2.5 Circuits for nonlinear tropical functions

Apart from circuits that allow race logic to implement tropical linear algebra, additional built-in functions, such

as elementwise inhibit, argmin, and binarization, are required to perform general purpose tropical computations.

Elementwise inhibit, shown in Fig. 3(a), is particularly powerful, as it allows us to implement piecewise functions. Its

technical operation follows directly from the scalar inhibit operation discussed in Sec. 2.1.

The argmin function, shown in Fig. 3(b), converts its vector input to a tropical one-hot vector that labels a minimal

input component. An or gate is used to select a first arriving signal which then inhibits every vector component. Only

one first arriving edge survives its self-inhibition; no other signals in the wavefront are allowed to pass, effectively

sending these other values to infinity. The resulting vector is projectively equivalent to a tropical one-hot, and achieves
the canonical form with a single zero among infinities after normalization.5

The binarization operation, shown in Fig. 3(c), is similar; it converts all finite components to 0 while preserving
infinite components at ∞. This operation utilizes a pre-stored vector which has the maximum finite (non-∞) value
tmax of the computational dynamic range on each component. We define binarize((cid:174)x) = tmax ⊕′ (cid:174)x. Computing the
elementwise max of such a vector with any incoming vector, values that are ∞ remain so while the other values are
converted to the maximal finite input value. Normalizing the result via projective storage saves a many-hot vector

labeling the finite components of the original input.

3 TEMPORAL STATE MACHINES

The finite state machine or finite state automaton is a central concept in computing and lies at the heart of most modern
computing systems. Such a machine is in one of some finite set of states S at any particular instant in time; inputs x ∈ Σ

5A variety of conventions could be taken for the case where more than one signal arrives at the same, earliest time. Note that such a multi-hot vector can
be generated by the circuit shown in Fig. 3(b). When such a situation occurs, a sorted delay vector can be used to select one of the hot input elements and
convert the vector to a one-hot vector.

Manuscript submitted to ACM

Temporal State Machines: Using temporal memory to stitch time-based graph computations

9

Fig. 3. Tropically nonlinear race logic functions: Panel (a) shows the conceptual and circuit diagram for an element-wise inhibit
operator. The inhibiting input is buffered before being fed into the gate terminal of a PMOS. As the inhibiting input turns high, the
PMOS turns off inhibiting the secondary input. Panel(b) shows the argmin operation that takes an input vector and returns a one-hot
vector at the location of the element with the minimum value. This is done by taking the first arrival signal and inhibting everything
else but that signal. Panel (c) shows a binarizer. An input wavefront is maxed with the all n wavefront. This takes all values to this
max value, except ∞, which remains as is, performing binarization.

to the machine both produce outputs y ∈ Γ and induce transitions between these internal states. A state transition
function δ : S × Σ → S determines the next state based on the current state and current input, and an output function
ω : S × Σ → Γ gives the output based on the state and inputs of the machine.6

The presence of state means that there is not a one-to-one correspondence between input and output of the machine;

in the language we have developed above, a state machine is an impure function. This impurity is due entirely to the

state variable; δ and ω are pure mathematical functions. The finite state machine thus provides a template for how

we might compose pure race logic functions together across stateful interfaces to create general purpose temporal

automata. In fact, the temporal state machines we introduce below fits into the mathematical framework given above.

The temporal state machine we introduce differs from conventional automata in that the signals use temporal rather

than Boolean encoding. State is made possible by recent proposals for temporal memories. These memories use temporal

wavefronts as their primary data structure. By coupling nanodevice parameters to pulse duration, they are able to

freeze temporal data in device properties such as resistance. Together with the pure race logic primitives described in

previous sections, we can now build finite state automata in an end-to-end temporal encoding.

Designing such a machine requires addressing several problems intrinsic to the temporal nature of these logic and

memory primitives. We start this section with a brief background on temporal memories, based on hybrid CMOS and

emerging technologies, and explain their benefits and drawbacks. Then we describe the impure tropical multiplication

of two signals in race logic as a first example of composing pure race logic across stateful interfaces in order to break

through the invariance restriction. Finally, we return to the general state machine formulation and argue for the

extensibility of our simple example to more complex systems.

6This specification of a state machine is called a Mealy machine; if ω depends only on the state and not the current input, it is called a Moore machine.
The two models are equally powerful in principle.

Manuscript submitted to ACM

(a) (b) pre n n n n n (c) n Dynamic range delay element 10

Advait Madhavan, Matthew W. Daniels, and Mark D. Stiles

Fig. 4. Memristive wavefront memory: Panel (a) shows a 4 × 4 memristive temporal memory, complete with read and write peripheral
circuits as described in Ref. [43]. Note that bit-line-4 has been replaced by a dummy line where the resistance values are fixed. The
time constant of this line is governed by the parasitics of the circuit and determines the temporal origin of the outgoing wavefront. .
Panel (b) shows the functioning of a 16 × 16, 4-bit, temporal memory as simulated in our 180 nm Silterra process. Strip (i) shows the
capture and playback of a linearly varying digital wavefront, with each color representing one of the sixteen lines involved. These
edges have been collapsed into a single strip for clarity. Note that small timing mismatches cause small changes in the shape of the
wavefront that is played back. Strip (ii) shows in the digital read input applied to a captured column. Strips (iii, iv) show the source
lines and bit lines, but internal to the memory and hence operate at different voltages which are shifted to Vdd with level shifters as
shown in panel(a). Strip (v) shows the state change behaviour of the memristors as given by the memristor model in [13]. Note that
the state change is almost linear. Careful inspection reveals a slight convexity by virtue of higher order terms in the exponential
dependence.

3.1 Temporal Memory

Temporal memories natively operate in the time domain. They operate on wavefronts of rising edges rather than on

Boolean values. Such memories can be implemented with emerging technologies such as memristors (as shown in Fig.4)

and magnetic race tracks [67] because the physics includes a direct coupling between the time variable and some analog

device property. In any case, the memory structures are similar: memory cells are arranged in a crossbar. For the read

operation, a single rising edge represented by the tropical one-hot vector is applied to the input address line of the

memory, creating a wavefront at the output data line. For a write operation, the column of the crossbar where the

memory has to be stored is activated and an incoming wavefront is captured.

The way temporal information is encoded in the devices depends on the technology. For memristors, the dependence

of the RC charging time constants on the resistance R of the memristor and the row capacitance C is used to encode the

temporal information, leading to a linear relationship between timing and resistance. Utilizing a 1T1R-like structure,

the shared row capacitances are the output capacitances that have to be charged. In the write operation the temporal

dependence of state change of the device creates a relative change in resistive state based on arrival time of the edges.

This enables a set of devices to correctly encode the shape of an incoming wavefront.

An alternative proposal in the literature uses magnetic racetracks to store temporal information in the position of a

magnetic domain within each track [67]. Magnetic tracks have the property that a current passing through a metal

layer below the magnet causes motion of the magnetic domain in the direction of the current flow. The speed v of
magnetic domain motion is constant for constant current amplitude, and so the simple equation x = vt determines the
Manuscript submitted to ACM

SL 1 SL 2 SL 3 BL 1 BL 2 BL 3 Ena 1 Ena 2 Ena 3 BL	read/write	circuit	BL	read/write	circuit	BL	read/write	circuit	BL	read/write	circuit	SL	read/write	circuit	SL	read/write	circuit	SL	read/write	circuit	Vdd Vread Vdd Vwrite Vread Vdd Vdd Vwrite DBL 1 DBL 2 DBL 3 DBL 4 DSL 1 DSL 2 DSL BL DBL SL DSL 3 (a) Clock  Line -5.00E-01	0.00E+00	5.00E-01	1.00E+00	1.50E+00	2.00E+00	0	5E-08	0.0000001	1.5E-07	0.0000002	2.5E-07	-2.00E-01	0.00E+00	2.00E-01	4.00E-01	6.00E-01	8.00E-01	1.00E+00	1.20E+00	1.40E+00	1.60E+00	0	5E-08	0.0000001	1.5E-07	0.0000002	2.5E-07	0	5000	10000	15000	20000	25000	30000	35000	40000	45000	0	5E-08	0.0000001	1.5E-07	0.0000002	2.5E-07	-5.00E-01	0.00E+00	5.00E-01	1.00E+00	1.50E+00	2.00E+00	0	5E-08	0.0000001	1.5E-07	0.0000002	2.5E-07	-4.00E-01	-2.00E-01	0.00E+00	2.00E-01	4.00E-01	6.00E-01	8.00E-01	1.00E+00	1.20E+00	1.40E+00	1.60E+00	0 1 2 0 1 2 0 1.4 0.8 1 1.4 0.8 2 3 4 Device Resistance (10KΩ) Source Line (SL, V) Bit  Line (BL, V) Digital Source  Line (DSL, V) Digital Bit Line (DBL, V) 100 200 0 50 150 Read voltage Dead time Wavefront playback Wavefront capture Approx. linear state change Write voltage Write voltage Time (ns) (b) (i) (ii) (iii) (iv) (v) Dummy  row Temporal State Machines: Using temporal memory to stitch time-based graph computations

11

final location of the domain. This linear proportionality provides a straightforward mapping between the timing of a

write signal and the position of the stored magnetic domain.

Regardless of the technology, the behaviour of these memories is qualitatively different from that of registers

or flip-flops. In the case of registers, a single clock tick performs two functions. Not only does it capture the next

state information from the calculation performed in the previous cycle, it also initiates the next cycle’s computation.

Combinational logic is thus “stitched” together by register interfaces. This feature of conventional memory does not

exist in the temporal memories proposed to date because wavefront playback and capture use the same address and

data lines, and cannot be used at the same time. Addressing this deficiency requires memories that can be used both

upstream and downstream for the same operation, as shown in Fig. 4.

Some limitations of the temporal memories discussed above arise because they are analog7: they possess limited
dynamic range, and a dead time is incurred in their use as shown in 4(b). The dead time is as a result of the charging of

the parasitics of the array, which—with growing array size—can become comparable to the delays stored. As measured

from the temporal origin of the calculation, the dead times introduce artificial delays in each component that result in

incorrectly encoded values at a memory write input. To deal with this issue, we introduce an extra dummy line, which
we call the clock line, that always has the minimum Ron value for the resistor. This line serves as a temporal reference
to the origin and hence behaves like a clock. This ensures that the parasitics of the lines are accounted for and only the

relative changes in the resistance values are translated to the output wavefront.

The dynamic range is determined by the relative changes in the stored resistances, which manifest themselves as

changes in the shape of the wavefront with respect to the clock line. Even optimistically, the range is limited to 6 bits to

7 bits with present technologies. Given our constrained dynamic range, we often restrict ourselves to the storage of

normalized tropical vectors. In Sec. 2.2, we describe how this can be achieved by subtracting from each component the

minimal value among all components. This guarantees that at least one element is zero in the normalized result. This

alters the reference time of the calculation by the normalization constant which was subtracted away. Some algorithms

are insensitive to this shift; otherwise, the normalization constant can be stored in an additional memory element for

later recovery. In order to store the normalized version of a tropical vector, the min value of the vector (without the

clock line) is used to replace the clock line for the storage operation, re-assigning it as the temporal origin. This can be
performed by pulling the clock line input in Fig. 4(a) to Vdd .

3.2 Invariance and temporal addition

In Section 2.1, we describe the invariance restriction on pure race logic. It constrains stateless race logic circuits to the

computation of tropically linear functions. An immediate consequence is that pure race logic cannot tropically multiply

two temporal signals. Static delay elements can be used to increment the value of a temporal signal by some fixed
amount, but the raw addition of two time codes t1 + t2 is physically forbidden in the presence of time-translational
symmetry.8 We can break this symmetry in stateful race logic through the introduction of memory.

With temporal memory, tropical multiplication of two wavefronts proceeds by breaking the operation into two

phases as shown in Fig. 5(a,b). The first panel shows the first phase which stores the incoming wavefront in a local

7The computing scheme discussed here can be either analog or digital. Though our evaluation (Sec. 5) is done assuming analog behavior, noise and other
non-idealities will in practice determine the information capacity afforded to such a computing scheme. We discuss this issue further in Sec. 6.
8Under the invariance constraint (Sec. 2.1), if two signals tA and tB are both shifted by a constant time δ , the output of a function of those signals must
also be shifted by the same amount, so that f (tA + δ, tB + δ ) = δ + f (tA, tB ). This doesn’t work for addition: if f (tA, tB ) = tA + tB , shifting the
inputs would result in tA + tB + 2δ at the output. Therefore addition is not temporally invariant and two temporal signals cannot be added together
using pure race logic.

Manuscript submitted to ACM

12

Advait Madhavan, Matthew W. Daniels, and Mark D. Stiles

Fig. 5. State machine operations: The state machine is partitioned into two main units: the temporal wavefront memory and the
arithmetic unit. These are shown in panel (a). The multiplexers and the read/write modes of the memory allow the operations to
be performed sequentially. Depending on the operations, individual memory units can behave as either upstream or downstream
memories. Panels (a) and (b) show tropical multiplication in a temporal state machine split into two state transitions. In panel (a)
storage of the incoming wavefront manifests as a one-argument operation; the vector is stored in the additive memory bank. The
next phase in panel (b) is another one-argument operation, where the incoming wavefront is delayed by the wavefront stored in the
previous phase. Panel (c) shows the tropical VMM operation. Panel (d) shows other element-wise operations that can be performed in
a temporal state machine. Note that all operations, aside from the first phase of the tropical multiplication, store an output back in
the temporal memory. The element-wise operations are the only two-argument operations and involve all three memories: the read
memories are the upstream memories, while the write memory is the downstream memory.

temporal memory using wavefront capture circuits. This stored vector can be temporally added to a new incoming

wavefront, as shown in the next panel. Commutativity ensures that the order of storage and playback does not matter.

Though the state transition and output functions within each phase are pure race logic functions, the state breaks

invariance across the phase boundaries. Using memory for tropical multiplication thus allows us to construct tropical

multinomial functions of arbitrary order.

3.3 A sample temporal state machine

The invariant race logic circuits and temporal wavefront memory described above are sufficient to build a simple

temporal state machine, as shown in Fig. 5(a). It consists of three banks of temporal memory, which can receive address

inputs from external sources as well as data inputs from the output of the machine. The data outputs of the wavefront

memory are multiplexed into the computation unit. This unit consists of a variety of the invariant race logic functions

from Sec. 2.1 as well as a temporal memory unit for tropical VMM described in Sec. 2.4. The structure allows for a

maximum of two-operand operations to be executed at once.

Manuscript submitted to ACM

Addr 1  Addr 2 Addr 3 Read mode Stored in Memory (a) Addr 1  Addr 2 Addr 3 Read mode Write mode Tropical VMM (c) Addr 1  Addr 2 Addr 3 Read mode Added as constant (b) Write mode Addr 1  Addr 2 Addr 3 Read mode Element-wise ops (d) Write mode Read mode Wavefront memory Invariant race logic  functions and addition memory Temporal State Machines: Using temporal memory to stitch time-based graph computations

13

Algorithm 1: Pseudocode for procedural computation of Eq. (3)
Input: temporal vectors (cid:174)b, (cid:174)c, (cid:174)d, and (cid:174)e

(cid:174)c ′ := (cid:174)d ⊗ (cid:174)e;
(cid:174)b ′ := (cid:174)c ⊣ (cid:174)c ′;
(cid:174)a := (cid:174)b ⊕ (cid:174)b ′;

return (cid:174)a;

// temporal vector addition (requires two transitions), Figs. 5(a,b)
// elementwise inhibit, Fig. 5(d)
// elementwise min, Fig. 5(d)

This state machine allows arbitrary expressions such as

(cid:174)a = (cid:174)b ⊕′ ((cid:174)c ⊣ ( (cid:174)d ⊗ (cid:174)e))

(3)

to be calculated. The computation is performed by partitioning it into phases, with each phase implemented serially on

the state machine of Fig 5. By breaking the computation into discrete read-compute-store transitions of a state machine,

we can represent the computation using a procedural algorithm, Algorithm 1.

We follow the regular order of arithmetic and perform the tropical multiplication first. Assume vector (cid:174)d and (cid:174)e
reside in memories one and two. The ⊗ operation is shown in Fig. 5(a),(b). The first phase selects the memory in the
computation unit and applies a one-hot vector at the input of wavefront memory 1, initiating the computation. The
memory places the vector (cid:174)d on the output data bus, which then passes it to the accumulator of the computation unit.
The next step is shown in Fig. 5(b), where memory 3 is setup to receive the output of the operation while being activated

in write mode. A one-hot vector is applied to the input of memory 2, playing the wavefront through the stored vector,
and storing the resulting output in memory 3. This storage operation is indicated by the assignment operator := in the
pseudocode. Tropical vector-matrix multiplication is a similar one-input operation and can be performed in a similar

way, as shown in Fig. 5(c).

Two-operand operations such as elementwise inhibit, and tropical vector addition are all performed in the same way.

Synchronized one-hot vectors are presented to the address input that causes output wavefronts to be triggered. These

wavefronts enter the computational unit where circuits for the requested operations are multiplexed in, and the output

is written to wavefront memory 3. This is all illustrated in Fig. 5(d). In this way, one- and two-operand operations

can be performed in a single state machine. Note that the computation is set up by control circuits not shown in the

figure. These control circuits are the only circuits that are not temporal in nature, and are used to direct the flow of

the computation in the system. These control circuits can be understood as the machine-level subroutines called by a

“tropical interpreter” stepping through the lines of pseudocode in Algorithm 1.

3.4 A nontrivial example: DNA alignment

DNA alignment using a temporal instantiation of the Needleman-Wunsch algorithm was one of the first applications of

race logic [41, 42]. In that work, the alignment matrix of the Needleman-Wunsch algorithm is physically laid out as a

planar graph, and pure race logic operations define the scoring information at each node. Though the implementation

in [41] is extremely fast and energy efficient, it suffers the disadvantage of requiring a dedicated ASIC. In this section

we briefly sketch how Needleman-Wunsch might instead be implemented in a general-purpose tropical state machine

like what we describe in the previous section.

Manuscript submitted to ACM

14

Advait Madhavan, Matthew W. Daniels, and Mark D. Stiles

Algorithm 2: Pseudocode for Needleman-Wunsch (forward pass only; computes optimal alignment cost)
Input: gene sequences (cid:174)x, (cid:174)y ∈ {0, 1, 2, 3}n , indel cost σ , mismatch cost m

(cid:174)µ(0) := [0];
(cid:174)µ(1) := [σ , σ ];

// Upper-left triangular part [dim((cid:174)µ(k )) increasing]:
for k ← 2 to n do

(cid:1);

(cid:174)c ′ := δ (cid:0) (cid:174)x1, ··· ,k −1, (cid:174)yk−1, ··· ,1
(cid:174)c :(cid:27) binarize((cid:174)c ′);
(cid:174)a := σ ⊗ (cid:174)µ(k −1);
(cid:174)b := (m ⊕ (cid:174)c) ⊗ (cid:174)µ(k −2);
(cid:174)r := (cid:174)a0, ··· ,k−2 ⊕ (cid:174)b ⊕ (cid:174)a1, ··· ,k −1;
(cid:174)µ(k) := (cid:2)a0, (cid:174)r, ak −1

(cid:3);

end

// mismatches → ∞, matches → {0, 1, 2, 3}
// mismatches ⇝ ∞, matches ⇝ 0
// apply insertion/deletion (indel) cost σ
// apply mutation cost m for mismatches
// find least-cost local path (Eq. (4))
// append boundary conditions

// Lower-right triangular part [dim((cid:174)µ(k )) decreasing]:
for k ← n + 1 to 2n do

(cid:1);

(cid:174)c ′ := δ (cid:0) (cid:174)xk −n, ··· ,n, (cid:174)yn, ··· ,k −n
(cid:174)c :(cid:27) binarize((cid:174)c ′);
(cid:174)a := σ ⊗ (cid:174)µ(k −1);
(cid:174)b := (m ⊕ (cid:174)c) ⊗ (cid:174)µ(k −2);
(cid:174)µ(k ) := (cid:174)a0, ··· ,2n−k ⊕ (cid:174)b ⊕ (cid:174)a1, ··· ,2n−k +1;

// mismatches → ∞, matches → {0, 1, 2, 3}
// mismatches ⇝ ∞, matches ⇝ 0
// apply insertion/deletions (indel) cost σ
// apply mutation cost m for mismatches
// find least-cost local path (Eq. (4))

end

return (cid:174)µ(2n);

// this is actually just a scalar: lowest possible alignment cost

The Needleman-Wunsch algorithm finds the shortest path through a dynamically constructed score matrix. Each
element of the score matrix Mi j is constructed recursively as Mi j = min{Mi, j−1 + σ , Mi−1, j + σ , Mi−1, j−1 + mδxi,yj },
where σ is the cost of a genetic insertion or deletion (an “indel”) and m is the cost of a single gene mutation.9 This
naturally has the structure of a tropical inner product, but the Kronecker delta function breaks the causality condition

and so cannot be implemented in pure race logic.

To compute the Kronecker delta, we encode the set of four possible genes {G, A, T, C} as temporal values {0, 1, 2, 3}.
We then use the coincidence function [44, 58] to determine equality of the temporally encoded gene values. Tropically
the coincidence function is described as δ (t1, t2) = (t1 ⊕ t2) ⊣ (t1 ⊕′ t2), which is equal to the inputs when they are the
same,10 and ∞ otherwise [44]. The coincidence function could either be a primitive operation of the state machine
or could be accomplished over multiple state transitions using ⊕, ⊕′, and ⊣; we assume the former circumstance.
Binarization followed by projective storage of δ (xi , yj ) would then save zero (tropical one) to memory when xi = yj
and ∞ (tropical zero) otherwise, resulting in a many-hot vector that indexes genewise equality.

To frame the Needleman-Wunch algorithm as a tropical vector problem, we exploit the independence of the skew-
diagonals [40]. We define (cid:174)µ(k ) as the kth skew-diagonal vector of M, so that (cid:174)µ(0) = [M00], (cid:174)µ(1) = [M10, M01], and so on.

9The Kronecker delta δi j is defined as one when i = j and zero otherwise.
10The simple version presented in the text applies to only an idealized coincidence: the exact point where t1 = t2. In practice [44, 58] a nonzero coincidence
window can introduced via a tolerance ϵ , by computing [ϵ ⊗ (t1 ⊕ t2)] ⊣ (t1 ⊕′ t2).
Manuscript submitted to ACM

Temporal State Machines: Using temporal memory to stitch time-based graph computations

15

The first and last elements of (cid:174)µ(k ) are kσ by construction for k ≤ n, that is, until we hit the main skew diagonal. The
defining equation for Mi j is then given through (cid:174)µ(k ) by

µ

(k )
j

= (cid:16)

σ ⊗ µ

(cid:17)

(k −1)
j

⊕

(cid:16) (cid:104)

m ⊕ δx j,yk −j

(cid:105)

⊗ µ

(k −2)
j

(cid:17)

⊕

(cid:16)
σ ⊗ µ

(k −1)
j+1

(cid:17)

.

(4)

The vectorized computation of this recursion relation is presented programmatically in Algorithm 2. The right-hand side

of each assignment is a pure race logic computation; the left-hand side represents a register address. As in Algorithm 1,
the assignment operator (cid:174)x := (cid:174)y indicates storage of (cid:174)y to a temporal memory register represented by (cid:174)x. The projective
storage operator (cid:174)x :(cid:27) (cid:174)y assigns the tropical normalization (cid:174)y − min (cid:174)y to the vector register (cid:174)x.

The interpreter required here is more complex than in Algorithm 1. Though we could in principle implement the
for-loops tropically by assigning k := 1 ⊗ k and monitoring n ⊣ k and 2n ⊣ k, we are not aware of a way to elegantly
perform subarray extraction using temporal signals as indices. We therefore imagine that k, as well as the array slicing

operations, are managed digitally by the interpreter.

4 CASE STUDY: DIJKSTRA’S ALGORITHM AS IMPLEMENTED IN A TEMPORAL STATE MACHINE

In Sec. 3 we demonstrate a simple model state machine, but it is too simple to utilize the graph traversal logic of tropical

linear algebra that we describe in Sec. 2.3. Though the Needleman-Wunsch machine in Sec. 3.4 does perform graph

traversal, it is restricted to a known, uniform progression through a highly regular planar graph. From the discussion of

Sec. 2.3, however, we know that general graph traversal should be accessible to a tropical state machine. In the present

section, we discuss an implementation of Dijkstra’s algorithm in a temporal state machine using the concepts developed

in this paper. We will see that the core neighbor-search operation of Dijkstra’s algorithm is naturally parallelized

by the tropical VMM, leading to very high throughput in terms of graph edges traversed per unit time, and that the

inhibit operation together with projective storage allow the embedding of important Boolean logic structures within

the temporal framework.

4.1 Dijkstra’s algorithm in race logic

We assume that the reader is familiar with the classical implementation of Dijkstra’s algorithm. In Algorithm 3, we map

the operations of Dijkstra’s algorithm into race logic, with each step as a single transition of a temporal state machine.

Two trivial modifications simplify the race logic implementation. First, instead of tracking the known distances to each
node, we mask out the distances of visited nodes with the value ∞. This vector of distances to unvisited nodes is (cid:174)d
in the algorithm listing, and a tropically binarized record of which nodes have been visited is recorded in a vector (cid:174)v.
Second, instead of storing a parent vector directly, we define a parent matrix ˆP as a collection of tropical column vectors
where a finite entry Pi j holds the distance from node i to node j along the current optimal path to j from the source
node s. We assume that the memristors in the VMM are already programmed to their correct values, meaning that the

graph is already stored in the arithmetic unit.

There are several apparent differences in how operations of the algorithm are performed in this (tropical) linear

algebra engine compared to a traditional programming language. There are, loosely speaking, two “modes” in which we
use tropical vectors. First, there are true temporal wavefronts, such as (cid:174)e and (cid:174)d, that represent variable distances measured
throughout the graph. These flow through the data path of the algorithm. Second, there are indicator wavefronts, such
as (cid:174)v and (cid:174)d∗, with elements restricted to 0 or ∞. These are used along the control paths of the algorithm to perform
element lookup from data-carrying temporal wavefronts, modification of tropically binary records such as (cid:174)v, and for

Manuscript submitted to ACM

16

Advait Madhavan, Matthew W. Daniels, and Mark D. Stiles

Algorithm 3: Pseudocode for Temporal Dijkstra’s Algorithm

Input: graph G, source node s

// Variable initializations
(cid:174)d := 0s ;
(cid:174)v := ∞;
ˆP := ∞;
ˆA := adjacency-matrix(G);

// distances to unvisited nodes (tropical one-hot labels source)
// visited nodes (tropical zero vector)
// parent matrix (tropical zero matrix)
// adjacency matrix of the graph

(cid:16)(cid:201)

while

j dj < ∞
(cid:174)n := argmin( (cid:174)d);

(cid:17)

do

// Examine neighbors
(cid:174)e := ˆA ⊗ (cid:174)n;
(cid:174)f := (cid:174)d ⊣ (cid:174)e;

// choose node to visit

// VMM examine neighbors of current node
// keep only newly-found shortest paths

// Update records for the next iteration
(cid:174)v := (cid:174)v ⊕ (cid:174)n;
(cid:174)d ′ := (cid:174)d ⊕ (cid:174)f ;
(cid:174)d :(cid:27) (cid:174)v ⊣ (cid:174)d ′;

// record the current node as visited
// construct new record of shortest paths
// update global unvisited distance vector

// Parent vector update process
(cid:174)f ∗ :(cid:27) binarize( (cid:174)f );
ˆP := (cid:174)f ∗ ⊣ ˆP;
(cid:174)P (cid:174)n := (cid:174)f ;

// vector indices of found nodes
// delete row data of previously recorded parents for found nodes
// record in column (cid:174)n distances (cid:174)f from (cid:174)n to the found nodes

end

return ˆP;

// adjacency matrix of the minimal spanning (from s) subgraph of G

index selection of the parent matrix. Projective storage plays a key role in these processes via binarization of one-hot
vectors. Sometimes, quantities like (cid:174)n can play either of the above roles depending on context.

There are two primary constraints on this algorithm’s application. First, because directed edge weights are encoded

as temporal delays, negative edge weights are physically forbidden. Second, temporal vectors are limited to a finite

dynamic range and resolution constrained by the technology in which they are implemented, and consecutive tropical

multiplication could lead to dynamic range issues. To mitigate this dynamic range issue, we arrange the computation

such that no more than one successive tropical multiplication occurs along a single datapath per state machine transition.
Normalization of (cid:174)u at the end of each cycle shrinks the dynamic range as much as possible between VMMs.

The algorithm initializes by setting the vector (cid:174)d of known distances to unvisited nodes to a tropical one-hot 0s
labeling the source node s. The vector (cid:174)v labeling visited nodes, as well as the parent matrix ˆP keeping track of the
minimal spanning tree through the graph, have all elements set to ∞. We assume the weighted adjacency matrix ˆA of
the desired graph has been programmed to a VMM unit before the algorithm begins. This is a one-time cost that can be

amortized over frequent reuse of the array. The algorithm then begins by cycling the state machine through the main

loop.

Manuscript submitted to ACM

Temporal State Machines: Using temporal memory to stitch time-based graph computations

17

In each iteration, we check to see if any unvisited nodes have are available for exploration by evaluating the minimum
element of (cid:174)d. The algorithm terminates if this operation returns ∞, which indicates that all nodes have either been
visited or are unreachable. Taking the argmin (Sec. 2.5) of (cid:174)d nominally gives us a vector dj ⊗ 0j where j is the index of a
node along a shortest path (of those so far explored) from the source and 0j is the tropical one-hot labeling index j. This
can be thought of as a one-hot vector with a magnitude dj . However, we will see that dj is always zero by construction,
so argmin( (cid:174)d) is just a one-hot. We store this one-hot to the vector register (cid:174)n.

The next step is to examine the directed edges to the neighbors of node (cid:174)n. We use (cid:174)n as the input to a temporal VMM
operation with ˆA, which performs a parallel traversal to all neighbors. The result of this exploration is stored in (cid:174)e. This
vector may contain shorter paths to the neighbors nodes, via node (cid:174)n, than what had been previously found. Such shorter
paths would manifest as elements of (cid:174)e with smaller values than their corresponding elements in (cid:174)d. Those specific nodes
can be extracted by taking an elementwise inhibit of (cid:174)e by (cid:174)d; the resulting updated distance vector is stored as (cid:174)d ′. We
also note that (cid:174)n has been visited, and should not be visited again, by imposing the zero of (cid:174)n onto (cid:174)v and saving it in
memory.

If the dynamic range of our memory were boundless, we could perform this operation repeatedly and determine the
final distance vector of the algorithm. But because we are dynamic-range-limited,11 we have to ensure that accumulation
in the distance vector is minimized. We do this via projective storage of (cid:174)d ′ into (cid:174)d. We also inhibit (cid:174)d ′ by (cid:174)v before storage
to ensure that no nodes we have already visited are candidates for exploration in the next iteration; this also ensures
argmin( (cid:174)d) will be a magnitude-free one-hot on the next cycle. This shifts the temporal origin for the entirety of the
next iteration into the perspective of argmin( (cid:174)v ⊣ (cid:174)d ′); all temporal values in the new (cid:174)d are now expressed relative to the
stopwatch of an observer at the argmin node.

After completing neighbor-exploration, we update the parent matrix. The newly found nodes in (cid:174)f are the ones whose
parents need to be updated. A binarized version (cid:174)f ∗ of (cid:174)f is used to inhibit rows of the parent matrix corresponding to
the new paths in f , erasing these nodes’ now-outdated parent data. This operation is performed row-by-row, requiring
N state machine transitions to complete. The new parent is then added to the parent matrix; (cid:174)n is used to enable the
appropriate column of ˆP for writing. Vector (cid:174)f is then written to this column.

Throughout this algorithm, we require dynamical indexing of memory addresses based on past results of the temporal

computation. Recall that Needlemen-Wunsch algorithm required significantly nontrivial subarray selection operations

in Algorithm 2. We claimed in Sec. 3.4 that these would likely needed to be handled digitally. Those index selections

can be statically determined at compile time, so they could merely be part of the elaborated bytecode controlling the

state machine: there is no need for data to translate back and forth between temporal and digital domains in order to

execute Algorithm 2. In Algorithm 3, index selections of the parent matrix are dynamically determined at runtime, and

cannot be statically embedded in the digital controller around the state machine. But the one-hot nature of the indexing

operations offer natural interfaces to the crossbar architecture, so, again no digital intermediary is required to perform

address lookup.

5 RESULTS

We start the evaluation of this temporal state machine by describing the assumptions in its design and the simulation

framework we use. To understand the scaling of this architecture, we create models for temporal memories and the

11Note that even if our memory were not range-limited, we must still choose a dynamic-range cutoff at which we assign finite time values to ∞; otherwise,
a circuit that outputs ∞ as a valid return value could never halt. In practice, though, memory is the limiting factor. However the finite delay representing
∞ is chosen, it informs the effective clock frequency of the state machine.

Manuscript submitted to ACM

18

Advait Madhavan, Matthew W. Daniels, and Mark D. Stiles

tropical operations required by its design. This process allows us to understand the trade-offs of our design space and

make predictions about optimization targets.

In order to achieve realistic first order performance estimates for this temporal state machine, we designed and

simulated each of the components of our system using commercial very-large-scale integrated circuit (VLSI) design

tools with experimentally validated nanodevice models [13, 32]. These devices exhibit voltage and current ranges

typical of other memristors fabricated by a variety of groups [1, 10, 72, 76]. Though the voltage needed to read these
devices can be low (≈ 200 mV), the voltages needed to write them can be as high as 2 V to 3 V, which puts a lower limit
on the technology node we can use. To secure enough voltage headroom for changing device states, we use the 180 nm
= 1.8 V. Though this may not offer the most energy-efficient results, it does provide an
Silterra12 process with a Vdd
understanding of the general set of trade-offs involved in building realistic temporal state machines. We provide a

description of scaling to lower technology nodes by referring to the scaling laws presented in [60]. A discussion on

resistive switching technologies at deep-sub-micron nodes has been reported in section 6.2.

In this work, the temporal memory is memristive, as is discussed in [43]. The core cell is composed of a 1T1R structure

with supporting circuits that allow temporal read and write operations. The temporal read operation is performed by
down-shifting the input voltage level from 1.8 V to 600 mV before applying it to the 1T1R array, so that the device state
is unaffected. This causes the output voltage to have a maximum value of 600 mV which needs to be up-shifted to 1.8 V
for compatibility with other functional blocks, all of which work at Vdd . The write path of the memory includes circuits
for two different write modes, the conventional and normalized forms described previously. Both these operations

require similar circuits with an input first-arrival detector charging the source line and level-shifting circuits to the

appropriate write voltages, causing the quasilinear state write described in [43].

The read and write energy costs for various N × N array sizes ranging from N = 4 to N = 32 are shown in Fig. 6.
The energy scales superlinearly with array size due to growth in support circuitry size that scales with N , the input

driver needs to be scaled up for larger array sizes; for the write case, larger array sizes require first-arrival circuitry

with more inputs. From the figures, we see that the read cost is approx 2 pJ per line while the write cost is around 10 pJ
per line. This 5× factor between read and write energies drives many of the tradeoff considerations in designs.

The most computationally intensive pure race logic function is the tropical VMM, which implements a single-step
all-to-all graph traversal. Such an operation naturally scales as N 2, which can be seen in Fig. 6(b). On average, this
system ends up costing ≈ 700 fJ per cell, so a 32 × 32 grid consumes ≈ 700 pJ of energy. The large energy cost of
this operation arises from the conservative design strategy we employed. In order to make sure that the or pulldown

network functions properly, we have to ensure that the time constants of the pulldown dynamics are not determined by

the CMOS—that is, we have to ensure that it switches quicker than the resolution of our temporal code. The low read

voltage causes the pulldown transistor to discharge too slowly, causing multiple nodes pulling down the same source

line and leading to functional incorrectness of tropical addition. In order to overcome this issue, we add level-shifters to
each cell to boost the input voltage from Vread to Vdd . These provide the necessary overdrive for correct operation.

Other pure race logic functions such ⊕ = min, ⊕′ = max, and ⊣ = inhibit—as well as compound functions such as
argmin and binarize—are implemented with conventional CMOS gates and hence have a minimal energy cost for this
process node. For example, for a 32 channel elementwise-⊕, the energy cost is approximately 1 pJ. This is negligible
compared to temporal read, write, and VMM operations. The argmin operation has the largest energy cost among

the combinatorial gates, since the first arriving input has to turn of all of the other channels and must therefore drive

12Certain commercial processes are identified in this paper to foster understanding. Such identification does not imply recommendation or endorsement
by the National Institute of Standards and Technology, nor does it imply that the processes identified are necessarily the best available for the purpose.

Manuscript submitted to ACM

Temporal State Machines: Using temporal memory to stitch time-based graph computations

19

Fig. 6. Energy results for various operations: Panel (a) shows the energy costs for vector operations of various array sizes. Panel (b)
shows the energy costs for read and write operations using the memristor temporal memory, while panel(c) shows the energy costs
of the VMM operations. Panel (d) compiles these energies and presents the energy cost of single and multi-operand operations in
a temporal state machine of size 32 × 32. Panel (e) compares the energy cost of such a 32 × 32 kernel with that of state-of-the-art
application specific integrated circuit (ASIC) and graphical processing unit (GPU) designs. It shows simulation results from our
180 nm process as well scaling to more advanced nodes following the procedure described in [60].

circuits with a larger output capacitance. The energy cost of each of these operations with respect to the problem size is

shown in Fig. 6(a).

The energy numbers for the key phases of Algorithm 3 for a problem size of 32 are shown in Fig. 6(d). Every operation
incurs a single write cost, by virtue of the output that has to be written in to the memory cells, except for ⊗, which
incurs two writes [Fig. 5(a,b)]. Read costs, on the other hand, depend on the number of operands. Single operand
operations such as argmin require a single read, while binary operations such as ⊕, ⊕′, and ⊣ incur the twice the read
costs. Energy costs for all operations except tropical VMM are dominated by reads and writes. This is as a result of the

simplicity of the primitive operations which are essentially made of simple Boolean primitives.

6 COMPARISONS AND TECHNICAL CONSIDERATIONS

6.1 Comparison with previous work

Graph processing is a well studied problem in computing and a variety of solutions have been proposed for it at

various scales [25]. Processing of real world graphs—which can contain hundreds of thousands of nodes and millions of

edges—combines both software and hardware frameworks, employing everything from central processing units (CPUs),

field programmable gate arrays (FPGAs) [77, 80], and graphics processing units (GPUs) [21, 70] to application specific

Manuscript submitted to ACM

0 200 400 600 800 1000 1200 argmin32 vmm32 inh32 min32 bin32 add32 writecost readcost opcost 0 200 400 600 800 1000 1200 argmin32 vmm32 inh32 min32 bin32 add32 writecost readcost opcost 0 1 2 3 4 5 6 min32 max32 inh32 argmin32 min16 max16 inh16 argmin16 min8 max8 inh8 argmin8 min4 max4 inh4 argmin4 Energy (pJ) 0 100 200 300 400 500 600 700 800 vmm32 vmm16 vmm8 vmm4 Energy (pJ) 0 50 100 150 200 250 300 350 400 write32 read32 write16 read16 write8 read8 write4 read4 Energy (pJ) 0 100 200 300 400 500 600 700 800 vmm32 vmm16 vmm8 vmm4 Energy (pJ) (a) (d) (c) (b) (e) 0.0001	0.001	0.01	0.1	1	10	100	1000	0.01	0.1	1	10	100	1000	Edge	traversal	rate	(ns-1)	Edge	traversal	efficiency	(nJ-1)	GPU	ASIC	This	work	180nm	45nm	14nm	a	b	c	d	20

Advait Madhavan, Matthew W. Daniels, and Mark D. Stiles

integrated circuits (ASICs) [27, 73] and processing-in-memory (PIM) solutions [11, 59, 79]. Graph operations are known

to have a high communication-to-computation ratio, as the cost of memory movement sometimes accounts for upwards

of 90 % of total energy expenditures. The simple temporal architecture presented in Sec. 4 is not developed adequately

to sensibly compare it to such highly developed systems optimized for much larger graphs. Another impediment to

fair comparison, echoed by the authors of [25], is that much of the extant literature reports relative improvements

against other implementations without providing absolute numbers for comparison. This makes comparison with PIM

implementations especially difficult. The purpose of this work is to demonstrate the viability of temporal computing as

an general approach using a well-studied example, not to compete with the best graph processing engines.

Therefore, we take the following approach. We do not compare against performant CPU and FPGA approaches that

leverage 3D-stacked high-bandwidth memory (HBM) or hybrid memory cube (HMC), since these approaches rely on the

memory management system for their performance advantages. GPU and ASIC approaches with domain-specific kernel

implementations amortize the costs of these memory accesses much more effectively and are more popular. For example,

Map Graph [21] and Gunrock [70] are examples of GPU-based graph analytics packages commonly used as a baseline

when reporting performance. More recently, domain-specific accelerators have emerged that have custom datapaths,

scheduling strategies, scratchpad memory, and other techniques specifically designed to alleviate the irregularities

associated with graph analytics. The literature on these approaches effectively reports the memory versus processing

costs [27, 73], allowing us to compare just the performance of our kernel with the performance of other state-of-the-art

graph kernels. Under this analysis, one could imagine swapping in temporal state machines for existing subgraph

kernels and measuring changes in overall performance metrics.

The metric widely used to make speed comparisons is the edge traversal rate, commonly reported as giga-edge

traversals per second (GETS) in the literature. For energy efficiency, we speak of edges traversed per unit energy—

giga-edge traversals per joule (GETJ) in the literature. Figure 6(e) shows the performance comparison of this work
against GPU and ASIC approaches. A single 32 × 32 kernel in a 180 nm technology node has an edge traversal rate
of 10 ns−1 (10 GETS) and the energy efficiency is about 1 nJ−1 (1 GETJ), which compares favorably with the state of
the art. Using scaling projections from [60], we estimate that a single kernel can theoretically surpass state-of-the-art
kernel performance. When scaled up to larger N × N array sizes, such as N = 128 or N = 256 (not an uncommon core
size for memristor crossbars), we can expect massive performance improvements. Note that the state of the art for

graph processing engines when energy is of no concern is on the order of 100s of GETS, which our analysis indicates to

be feasible for temporal designs.

Independent but parallel work on graph problems is being undertaken by the neuromorphic computing community.

Dikjstra’s algorithm has been studied by researchers in neuromorphic computing as a benchmark application for the

field [2, 17, 28]. State-of-the-art industrial research spiking neural network platforms [18] use Dijkstra’s algorithm to

establish performance metrics for their systems. The Tennessee neuromorphic project uses single-source shortest path

computation to demonstrate their spiking neuromorphic chips [57]. The energy per spike costs have been detailed in

Ref. [57]; when an operation equivalent to the tropical VMM is implemented, it costs approximately 2.5 nJ in a 65 nm

process. By comparison, combining both the memory and VMM primitives, race logic performs the same operation for

1 nJ in a 180 nm process.

6.2 Technical considerations

6.2.1

Scaling from 180 nm to newer technology nodes. Previous work with race logic has demonstrated that most of

the energy expended in race logic architectures is spent in the distribution of timing information, such as in clock

Manuscript submitted to ACM

Temporal State Machines: Using temporal memory to stitch time-based graph computations

21

trees or analog voltages [41]. In order to get an energy advantage over those approaches, the present work relies on

novel technologies such as memristors to locally generate a programmable delay, which has the advantage that the

energy cost is limited by the capacitor. Hence we are limited by today’s memristor technology, which requires large

write voltages (1.2 V to 6 V). This requires that we use a relatively old technology node. The development of memristor

technology is being driven toward the goal of CMOS compatibility at advanced technology nodes, which require lower

read and write voltages [15]. Companies are exploring low write voltage resistive random access memory (ReRAM) and

embedding it into 22 nm fin field-effect transistor (FinFET) stacks [24, 31].

Maturing technology has great promise for the designs proposed in this work. As CMOS transistors become smaller,

the area, energy, and speed all improve. For example, when moving from a 180 nm CMOS to 14 nm FinFET, using
a fan-out-of-4 inverter as a benchmark, the area, energy, and latency numbers improve by 100×, 190× and 19×,
respectively [60]. As memristor technologies become compatible with lower voltages, the energy of the read and write

operations are expected to decrease. The write energy, determined by the voltage and current needed to alter the

memristive state, changes less than the read energy, which follows the inverter characteristics. The scaling performance

of race logic systems is easy to estimate since the spatial nature of the information flow ensures that the architectures

in various technology nodes all have similar design and activity factors. We expect the dynamic energy cost to follow

the energy trend of the inverter as described in Ref. [60]. Though latency and area are determined by other factors such

as memory dynamic range and functional correctness, the overall advantage in energy-delay-product from scaling to a

lower node could be as high as three orders of magnitude.

6.2.2 Memristor device non-idealities. There are a variety of device non-idealities that affect the design. First, the

dynamic range of the technology is limited. Prior work with memristors has demonstrated precision as high as 6 bits to

7 bits [4, 72], but this was accomplished with a carefully programmed write-verify process. With the memristor model

used in this paper, we can extract up to 5 bits of precision. Practical implementations have even lower precision. One

way to increase precision is to use extra wires to encode higher precision bits as done for Boolean logic. A similar idea

has been proposed in Refs. [14, 36].

Another major impediment to smooth operation in our circuits is the linearity requirement of the memristor write

process. A truly linear write would increase the dynamic range of our operations and ensure a clear mapping between

the read and write processes. This linearity requirement has been a major topic of research for the neuromorphic VMM

community with large implications on the hardware training of large scale neural networks [9, 61, 72]. Considerable

effort has been dedicated to this effort. Various groups show highly linear behaviour by operating in the high conductance

regime with proper compliance control [38], exploring new materials [12], and using three terminal lithium devices [22].

An alternate approach utilizes highly linear trench-capacitor based storage [39]. Recently, a temporal magnetic memory

has been proposed which exhibits linear dynamics [67]. This proposal re-purposes magnetic configurations in racetracks

such as domain walls or skyrmions to encode temporal information spatially within the race track.

Finally, issues such as noise, variability, drift, and mismatch will be ultimately responsible for determining the actual

dynamic range that can be extracted from such nanodevices [72]. The advantage of using a charge-based approach to

computation is that the memristors can be used in their high-conductance, low-variability regime, while still maintaining

low read energy costs. Building a noise model that describes how variability accumulates in such systems is beyond the

scope of this text, but will be an important future work. The energy benefits of such the analog approach come at the

cost of error; effectively bounding this tradeoff is a crucial theoretical problem.

Manuscript submitted to ACM

22

Advait Madhavan, Matthew W. Daniels, and Mark D. Stiles

6.2.3

Future design considerations. Temporal computation leads to unconventional architectures that come with their

own design constraints. The cost of primitive operations (aside from the VMM) in temporal computing is cheap

compared to memory access operations. This points to utilizing strategies that amortize the cost of memory accesses

over multiple feed-forward operations. Future systems would greatly benefit by performing many such operations in a
single phase. In Algorithm 3, for instance, neither (cid:174)e nor (cid:174)d ′ need to be stored in memory. A sophisticated compiler could
detect optimally long compositions of pure race logic functions and only use memory where invariance or causality

need to be broken. Though such a state machine would need additional control logic with separate clock and dummy

lines, the energy savings accrued by this sort of optimization would be significant.

As higher level algorithms become more clearly expressible, an important question would be, what kind of complexity

of operations would we want in our designs? Similar to the discussion of reduced versus complex instruction set

computers (RISC vs. CISC), a design with simpler fundamental primitives could be more flexible, but might sacrifice

performance. An example of that can be seen in the parent matrix update of Algorithm 3. A 2D update array similar to

the VMM could amortize the cost of N extra operations, and hence save on N memory reads and writes, in just a single

operation. Hence a more complex operation would have smaller energy and delay, which would be very favorable—at

the cost of specialized circuitry. The sensibility of such tradeoffs is an open question that needs to be addressed.

Finally, we must consider the question of dynamic range. Approaches to extend the dynamic range of such memories

by using a binary-like encoding has been proposed by previous authors [36]. These techniques may be required to

expand dynamic ranges to be compatible with future designs.

7 CONCLUSION

The utility of temporal computation in solving problems expressible by dynamic programming has been widely noted.

Though the first race logic work was proposed as a hardware acceleration for dynamic programming algorithms, it was

constrained in its design: a limited topology and a feed-forward memoryless structure. Only the length of the shortest

path was reported, with extra circuitry nominally required to report the path itself. Since then, other designs with

state-of-the-art performance have been proposed, but they similarly suffer from an ad hoc design approach.

In this work we attempt to make the first steps at generalizability of temporal computing. We provide a generalizeable

data-path and a mathematical algebra, expanding the logical framework of race logic . This leads to novel circuit

designs that are informed by higher level algorithmic requirements. The properties of abstraction and composability

offered by the mathematical framework coupled with native storage from the temporal memory lend themselves to

generalization. We design a state machine that can carry out both specialized and general graph algorithms, such as

Needleman-Wunsch and Dijkstra’s algorithm, respectively. The potential for general purpose graph accelerators built

on temporal computing motivates further exploration of temporal state machines.

8 ACKNOWLEDGMENTS

Authors thank Brian Hoskins, Mark Anders, Jabez McClelland, Melika Payvand, George Tzimpragos, and Tim Sherwood

for helpful discussions. Advait Madhavan acknowledges support under the Cooperative Research Agreement Award

No. 70NANB14H209, through the University of Maryland.

REFERENCES

[1] Gina C Adam, Brian D Hoskins, Mirko Prezioso, Farnood Merrikh-Bayat, Bhaswar Chakrabarti, and Dmitri B Strukov. 2016. 3-D memristor crossbars

for analog and neuromorphic computing applications. IEEE Transactions on Electron Devices 64, 1 (2016), 312–318.

Manuscript submitted to ACM

Temporal State Machines: Using temporal memory to stitch time-based graph computations

23

[2] James B Aimone, Ojas Parekh, Cynthia A Phillips, Ali Pinar, William Severa, and Helen Xu. 2019. Dynamic Programming with Spiking Neural

Computing. In Proceedings of the International Conference on Neuromorphic Systems. 1–9.

[3] Filipp Akopyan, Jun Sawada, Andrew Cassidy, Rodrigo Alvarez-Icaza, John Arthur, Paul Merolla, Nabil Imam, Yutaka Nakamura, Pallab Datta,
Gi-Joon Nam, et al. 2015. Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip. IEEE transactions on
computer-aided design of integrated circuits and systems 34, 10 (2015), 1537–1557.

[4] Fabien Alibart, Ligang Gao, Brian D Hoskins, and Dmitri B Strukov. 2012. High precision tuning of state for memristive devices by adaptable

variation-tolerant algorithm. Nanotechnology 23, 7 (2012), 075201.

[5] Xavier Allamigeon, Pascal Benchimol, Stéphane Gaubert, and Michael Joswig. 2014. Combinatorial simplex algorithms can solve mean payoff games.

SIAM Journal on Optimization 24, 4 (2014), 2096–2117.

[6] Xavier Allamigeon, Pascal Benchimol, Stéphane Gaubert, and Michael Joswig. 2015. Tropicalizing the simplex algorithm. SIAM Journal on Discrete

Mathematics 29, 2 (2015), 751–795.

[7] Guo-qiang Bi and Mu-ming Poo. 1998. Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and

postsynaptic cell type. Journal of neuroscience 18, 24 (1998), 10464–10472.

[8] Sander M Bohte, Joost N Kok, and Han La Poutre. 2000. Poutré. Spike-prop: backpropagation for networks of spiking neurons. In Proc. ESANNâĂŹ2000.

Citeseer.

[9] Geoffrey W Burr, Robert M Shelby, Severin Sidler, Carmelo Di Nolfo, Junwoo Jang, Irem Boybat, Rohit S Shenoy, Pritish Narayanan, Kumar
Virwani, Emanuele U Giacometti, et al. 2015. Experimental demonstration and tolerancing of a large-scale neural network (165 000 synapses) using
phase-change memory as the synaptic weight element. IEEE Transactions on Electron Devices 62, 11 (2015), 3498–3507.

[10] Bhaswar Chakrabarti, Miguel Angel Lastras-Montaño, Gina Adam, Mirko Prezioso, Brian Hoskins, M Payvand, A Madhavan, A Ghofrani, L
Theogarajan, K-T Cheng, et al. 2017. A multiply-add engine with monolithically integrated 3D memristor crossbar/CMOS hybrid circuit. Scientific
reports 7 (2017), 42429.

[11] Nagadastagiri Challapalle, Sahithi Rampalli, Linghao Song, Nandhini Chandramoorthy, Karthik Swaminathan, John Sampson, Yiran Chen, and
Vijaykrishnan Narayanan. 2020. GaaS-X: Graph Analytics Accelerator Supporting Sparse Data Representation using Crossbar Architectures. In 2020
ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA). IEEE, 433–445.

[12] Sridhar Chandrasekaran, Firman Mangasa Simanjuntak, R Saminathan, Debashis Panda, and Tseung-Yuen Tseng. 2019. Improving linearity by

introducing Al in HfO2 as a memristor synapse device. Nanotechnology 30, 44 (2019), 445205.

[13] Pai-Yu Chen and Shimeng Yu. 2015. Compact modeling of RRAM devices and its applications in 1T1R and 1S1R array design. IEEE Transactions on

Electron Devices 62, 12 (2015), 4022–4028.

[14] Zhengyu Chen and Jie Gu. 2019. 19.7 A Scalable Pipelined Time-Domain DTW Engine for Time-Series Classification Using Multibit Time Flip-Flops

With 140Giga-Cell-Updates/s Throughput. In 2019 IEEE International Solid-State Circuits Conference-(ISSCC). IEEE, 324–326.

[15] Sumit Choudhary, Mahesh Soni, and Satinder K Sharma. 2019. Low voltage & controlled switching of MoS2-GO resistive layers based ReRAM for

non-volatile memory applications. Semiconductor Science and Technology 34, 8 (2019), 085009.

[16] Simon Davidson, Stephen B Furber, and Oliver Rhodes. 2020. Spiking Associative Memory for Spatio-Temporal Patterns. arXiv preprint

arXiv:2006.16684 (2020).

[17] Mike Davies. 2019. Benchmarks for progress in neuromorphic computing. Nature Machine Intelligence 1, 9 (2019), 386–388.
[18] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil Imam,

Shweta Jain, et al. 2018. Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro 38, 1 (2018), 82–99.

[19] Piotr Dudek. 2006. An asynchronous cellular logic network for trigger-wave image processing on fine-grain massively parallel arrays. IEEE

Transactions on Circuits and Systems II: Express Briefs 53, 5 (2006), 354–358.

[20] Tommas J Ellender, Wiebke Nissen, Laura L Colgin, Edward O Mann, and Ole Paulsen. 2010. Priming of hippocampal population bursts by individual

perisomatic-targeting interneurons. Journal of Neuroscience 30, 17 (2010), 5979–5991.

[21] Zhisong Fu, Michael Personick, and Bryan Thompson. 2014. Mapgraph: A high level api for fast development of high performance graph analytics

on gpus. In Proceedings of Workshop on GRAph Data management Experiences and Systems. 1–6.

[22] Elliot J Fuller, Farid El Gabaly, François Léonard, Sapan Agarwal, Steven J Plimpton, Robin B Jacobs-Gedrim, Conrad D James, Matthew J Marinella,

and A Alec Talin. 2017. Li-ion synaptic transistor for low power analog computing. Advanced Materials 29, 4 (2017), 1604310.
[23] Steve B Furber, Francesco Galluppi, Steve Temple, and Luis A Plana. 2014. The spinnaker project. Proc. IEEE 102, 5 (2014), 652–665.
[24] Oleg Golonzka, U Arslan, P Bai, M Bohr, O Baykan, Y Chang, A Chaudhari, A Chen, N Das, C English, et al. 2019. Non-volatile RRAM embedded

into 22FFL FinFET technology. In 2019 Symposium on VLSI Technology. IEEE, T230–T231.

[25] Chuang-Yi Gui, Long Zheng, Bingsheng He, Cheng Liu, Xin-Yu Chen, Xiao-Fei Liao, and Hai Jin. 2019. A survey on graph processing accelerators:

Challenges and opportunities. Journal of Computer Science and Technology 34, 2 (2019), 339–371.

[26] Rudy Guyonneau, Rufin VanRullen, and Simon J Thorpe. 2005. Neurons tune to the earliest spikes through STDP. Neural Computation 17, 4 (2005),

859–879.

[27] Tae Jun Ham, Lisa Wu, Narayanan Sundaram, Nadathur Satish, and Margaret Martonosi. 2016. Graphicionado: A high-performance and energy-
efficient accelerator for graph analytics. In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE, 1–13.
[28] Kathleen E Hamilton, Tiffany M Mintz, and Catherine D Schuman. 2019. Spike-based primitives for graph algorithms. arXiv preprint arXiv:1903.10574

(2019).

Manuscript submitted to ACM

24

Advait Madhavan, Matthew W. Daniels, and Mark D. Stiles

[29] John L Hennessy and David A Patterson. 2019. A new golden age for computer architecture. Commun. ACM 62, 2 (2019), 48–60.
[30] Eugene M Izhikevich. 2006. Polychronization: computation with spikes. Neural computation 18, 2 (2006), 245–282.
[31] Pulkit Jain, Umut Arslan, Meenakshi Sekhar, Blake C Lin, Liqiong Wei, Tanaya Sahu, Juan Alzate-vinasco, Ajay Vangapaty, Mesut Meterelliyoz, Nathan
Strutt, et al. 2019. 13.2 A 3.6 Mb 10.1 Mb/mm 2 Embedded Non-Volatile ReRAM Macro in 22nm FinFET Technology with Adaptive Forming/Set/Reset
Schemes Yielding Down to 0.5 V with Sensing Time of 5ns at 0.7 V. In 2019 IEEE International Solid-State Circuits Conference-(ISSCC). IEEE, 212–214.
[32] Zizhen Jiang, Shimeng Yu, Yi Wu, Jesse H Engel, Ximeng Guan, and H-S Philip Wong. 2014. Verilog-A compact model for oxide-based resistive
random access memory (RRAM). In 2014 International Conference on Simulation of Semiconductor Processes and Devices (SISPAD). IEEE, 41–44.
[33] Bill Kay, Prasanna Date, and Catherine Schuman. 2020. Neuromorphic Graph Algorithms: Extracting Longest Shortest Paths and Minimum Spanning

Trees. In Proceedings of the Neuro-inspired Computational Elements Workshop. 1–6.

[34] Jeremy Kepner, Peter Aaltonen, David Bader, Aydin Buluç, Franz Franchetti, John Gilbert, Dylan Hutchison, Manoj Kumar, Andrew Lumsdaine,
Henning Meyerhenke, et al. 2016. Mathematical foundations of the GraphBLAS. In 2016 IEEE High Performance Extreme Computing Conference
(HPEC). IEEE, Waltham, MA, 1–9.

[35] Dion Khodagholy, Jennifer N Gelinas, Thomas Thesen, Werner Doyle, Orrin Devinsky, George G Malliaras, and György Buzsáki. 2015. NeuroGrid:

recording action potentials from the surface of the brain. Nature neuroscience 18, 2 (2015), 310–315.

[36] KwangSeok Kim, Wonsik Yu, and SeongHwan Cho. 2014. A 9 bit, 1.12 ps resolution 2.5 b/stage pipelined time-to-digital converter in 65 nm CMOS

using time-register. IEEE Journal of Solid-State Circuits 49, 4 (2014), 1007–1016.

[37] Xavier Lagorce and Ryad Benosman. 2015. Stick: spike time interval computational kernel, a framework for general purpose computation using

neurons, precise timing, delays, and synchrony. Neural computation 27, 11 (2015), 2261–2317.

[38] Can Li, Miao Hu, Yunning Li, Hao Jiang, Ning Ge, Eric Montgomery, Jiaming Zhang, Wenhao Song, Noraica Dávila, Catherine E Graves, et al. 2018.

Analogue signal and image processing with large memristor crossbars. Nature Electronics 1, 1 (2018), 52.

[39] Y Li, S Kim, X Sun, P Solomon, T Gokmen, H Tsai, S Koswatta, Z Ren, R Mo, CC Yeh, et al. 2018. Capacitor-based cross-point array for analog neural

network with record symmetry and linearity. In 2018 IEEE Symposium on VLSI Technology. IEEE, 25–26.

[40] RJ LIPTON. 1985. A systolic array for rapid string comparison. In Proc. of the Chapel Hill Conf. on VLSI, 1985. 363–376.
[41] Advait Madhavan, Timothy Sherwood, and D.Strukov. 2017. A 4-mm 2 180-nm-CMOS 15-Giga-cell-updates-per-second DNA sequence alignment

engine based on asynchronous race conditions. In 2017 IEEE Custom Integrated Circuits Conference (CICC). IEEE, 1–4.

[42] Advait Madhavan, Timothy Sherwood, and Dmitri Strukov. 2014. Race logic: A hardware acceleration for dynamic programming algorithms. In 2014

ACM/IEEE 41st International Symposium on Computer Architecture (ISCA). IEEE, 517–528.

[43] Advait Madhavan and Mark D Stiles. 2020. Storing and retrieving wavefronts with resistive temporal memory. Accepted at IEEE international

Symposium for Circuits and Systems(ISCAS) 2020 (2020).

[44] Advait Madhavan, Georgios Tzimpragos, Mark Stiles, and Timothy Sherwood. 2019. A Truth-Matrix view into Unary Computing. First unary

computing workshop, ISCA 2019 (2019).

[45] Daisuke Miyashita, Shouhei Kousai, Tomoya Suzuki, and Jun Deguchi. 2017. A neuromorphic chip optimized for deep learning and CMOS technology

with time-domain analog and digital mixed-signal processing. IEEE Journal of Solid-State Circuits 52, 10 (2017), 2679–2689.

[46] Daisuke Miyashita, Ryo Yamaki, Kazunori Hashiyoshi, Hiroyuki Kobayashi, Shouhei Kousai, Yukihito Oowaki, and Yasuo Unekawa. 2013. An LDPC

decoder with time-domain analog and digital mixed-signal processing. IEEE Journal of Solid-State Circuits 49, 1 (2013), 73–83.

[47] Mehryar Mohri. 2002. Semiring frameworks and algorithms for shortest-distance problems. Journal of Automata, Languages and Combinatorics 7, 3

(2002), 321–350.

[48] Harideep Nair, John Paul Shen, and James E. Smith. 2020. Direct CMOS Implementation of Neuromorphic Temporal Neural Networks for Sensory

Processing. arXiv:cs.AR/2009.00457

[49] M Hassan Najafi, David J Lilja, Marc D Riedel, and Kia Bazargan. 2018. Low-cost sorting network circuits using unary processing. IEEE Transactions

on Very Large Scale Integration (VLSI) Systems 26, 8 (2018), 1471–1480.

[50] Taehwan Oh, Hariprasath Venkatram, and Un-Ku Moon. 2013. A time-based pipelined ADC using both voltage and time domain information. IEEE

Journal of Solid-State Circuits 49, 4 (2013), 961–971.

[51] Garrick Orchard, Cedric Meyer, Ralph Etienne-Cummings, Christoph Posch, Nitish Thakor, and Ryad Benosman. 2015. HFirst: a temporal approach

to object recognition. IEEE transactions on pattern analysis and machine intelligence 37, 10 (2015), 2028–2040.

[52] Marc Osswald, Sio-Hoi Ieng, Ryad Benosman, and Giacomo Indiveri. 2017. A spiking neural network model of 3D perception for event-based

neuromorphic stereo vision systems. Scientific reports 7 (2017), 40703.

[53] Filip Ponulak and Andrzej Kasiński. 2010. Supervised learning in spiking neural networks with ReSuMe: sequence learning, classification, and spike

shifting. Neural computation 22, 2 (2010), 467–510.

[54] Filip Jan Ponulak and John J Hopfield. 2013. Rapid, parallel path planning by propagating wavefronts of spiking neural activity. Frontiers in

computational neuroscience 7 (2013), 98.

[55] Shubham Sahay, Mohammad Bavandpour, Mohammad Reza Mahmoodi, and Dmitri Strukov. 2020. A 2T-1R Cell Array with High Dynamic Range

for Mismatch-Robust and Efficient Neurocomputing. In 2020 IEEE International Memory Workshop (IMW). IEEE, 1–4.

[56] Aseem Sayal, Shirin Fathima, SS Teja Nibhanupudi, and Jaydeep P Kulkarni. 2019. 14.4 All-Digital Time-Domain CNN Engine Using Bidirectional
Memory Delay Lines for Energy-Efficient Edge Computing. In 2019 IEEE International Solid-State Circuits Conference-(ISSCC). IEEE, 228–230.

Manuscript submitted to ACM

Temporal State Machines: Using temporal memory to stitch time-based graph computations

25

[57] Catherine D Schuman, Kathleen Hamilton, Tiffany Mintz, Md Musabbir Adnan, Bon Woong Ku, Sung-Kyu Lim, and Garrett S Rose. 2019. Shortest
path and neighborhood subgraph extraction on a spiking memristive neuromorphic implementation. In Proceedings of the 7th Annual Neuro-inspired
Computational Elements Workshop. 1–6.

[58] James E Smith. 2018. Space-time algebra: a model for neocortical computation. In Proceedings of the 45th Annual International Symposium on

Computer Architecture. IEEE Press, 289–300.

[59] Linghao Song, Youwei Zhuo, Xuehai Qian, Hai Li, and Yiran Chen. 2018. GraphR: Accelerating graph processing using ReRAM. In 2018 IEEE

International Symposium on High Performance Computer Architecture (HPCA). IEEE, 531–543.

[60] Aaron Stillmaker and Bevan Baas. 2017. Scaling equations for the accurate prediction of CMOS device performance from 180 nm to 7 nm. Integration

58 (2017), 74–81.

[61] Xiaoyu Sun and Shimeng Yu. 2019. Impact of non-ideal characteristics of resistive synaptic devices on implementing convolutional neural networks.

IEEE Journal on Emerging and Selected Topics in Circuits and Systems 9, 3 (2019), 570–579.

[62] Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kheradpisheh, Timothee Masquelier, and Anthony Maida. 2018. Deep learning in spiking

neural networks. Neural Networks (2018).

[63] Simon J Thorpe. 1990. Spike arrival times: A highly efficient coding scheme for neural networks. Parallel Processing in Neural Systems (1990), 91–94.
[64] Georgios Tzimpragos, Advait Madhavan, Dilip Vasudevan, Dmitri Strukov, and Timothy Sherwood. 2019. Boosted Race Trees for Low Energy
Classification. In Proceedings of the Twenty-Forth International Conference on Architectural Support for Programming Languages and Operating Systems
(ASPLOS ’19).

[65] Georgios Tzimpragos, Nestan Tsiskaridze, Kylie Huch, Advait Madhavan, and Timothy Sherwood. 2019. From Arbitrary Functions to Space-Time

Implementations. First unary computing workshop, ISCA 2019 (2019).

[66] Georgios Tzimpragos, Dilip Vasudevan, Nestan Tsiskaridze, George Michelogiannakis, Advait Madhavan, Jennifer Volk, John Shalf, and Timothy
Sherwood. 2020. A Computational Temporal Logic for Superconducting Accelerators. In Proceedings of the Twenty-Fifth International Conference on
Architectural Support for Programming Languages and Operating Systems. 435–448.

[67] Hamed Vakili, Mohammad Nazmus Sakib, Samiran Ganguly, Mircea Stan, Matthew W Daniels, Advait Madhavan, Mark D Stiles, and Avik W Ghosh.

2020. Temporal Memory with Magnetic Racetracks. arXiv preprint arXiv:2005.10704 (2020).

[68] Rufin VanRullen, Rudy Guyonneau, and Simon J Thorpe. 2005. Spike times make sense. Trends in neurosciences 28, 1 (2005), 1–4.
[69] Stephen J Verzi, Fredrick Rothganger, Ojas D Parekh, Tu-Thach Quach, Nadine E Miner, Craig M Vineyard, Conrad D James, and James B Aimone.

2018. Computing with spikes: The advantage of fine-grained timing. Neural computation 30, 10 (2018), 2660–2690.

[70] Yangzihao Wang, Andrew Davidson, Yuechao Pan, Yuduo Wu, Andy Riffel, and John D Owens. 2016. Gunrock: A high-performance graph processing

library on the GPU. In Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. 1–12.

[71] Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. 2018. Spatio-temporal backpropagation for training high-performance spiking neural

networks. Frontiers in neuroscience 12 (2018), 331.

[72] Qiangfei Xia and J Joshua Yang. 2019. Memristive crossbar arrays for brain-inspired computing. Nature materials 18, 4 (2019), 309–323.
[73] Mingyu Yan, Xing Hu, Shuangchen Li, Abanti Basak, Han Li, Xin Ma, Itir Akgun, Yujing Feng, Peng Gu, Lei Deng, et al. 2019. Alleviating irregularity
in graph analytics acceleration: A hardware/software co-design approach. In Proceedings of the 52nd Annual IEEE/ACM International Symposium on
Microarchitecture. 615–628.

[74] Heemin Y Yang and Rahul Sarpeshkar. 2005. A time-based energy-efficient analog-to-digital converter. IEEE Journal of solid-state circuits 40, 8

(2005), 1590–1601.

[75] Ruriko Yoshida, Leon Zhang, and Xu Zhang. 2019. Tropical principal component analysis and its application to phylogenetics. Bulletin of

mathematical biology 81, 2 (2019), 568–597.

[76] Shimeng Yu. 2018. Neuro-inspired computing with emerging nonvolatile memorys. Proc. IEEE 106, 2 (2018), 260–285.
[77] Jialiang Zhang and Jing Li. 2018. Degree-aware hybrid graph traversal on FPGA-HMC platform. In Proceedings of the 2018 ACM/SIGDA International

Symposium on Field-Programmable Gate Arrays. 229–238.

[78] Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. 2018. Tropical Geometry of Deep Neural Networks. In Proceedings of the 35th International Con-
ference on Machine Learning (Proceedings of Machine Learning Research), Jennifer Dy and Andreas Krause (Eds.), Vol. 80. PMLR, StockholmsmÃďssan,
Stockholm Sweden, 5824–5832. http://proceedings.mlr.press/v80/zhang18i.html

[79] Minxuan Zhou, Mohsen Imani, Saransh Gupta, Yeseong Kim, and Tajana Rosing. 2019. GRAM: graph processing in a ReRAM-based computational

memory. In Proceedings of the 24th Asia and South Pacific Design Automation Conference. 591–596.

[80] Shijie Zhou and Viktor K Prasanna. 2017. Accelerating graph analytics on CPU-FPGA heterogeneous platform. In 2017 29th International Symposium

on Computer Architecture and High Performance Computing (SBAC-PAD). IEEE, 137–144.

Manuscript submitted to ACM

