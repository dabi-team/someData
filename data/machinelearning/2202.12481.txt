Multi-View Graph Representation for Programming
Language Processing: An Investigation into Algorithm Detection

Ting Long*1, Yutong Xie*2, Xianyu Chen1, Weinan Zhang1†, Qinxiang Cao1, Yong Yu1†
1Department of Computer Science and Engineering, Shanghai Jiao Tong University, China
2School of Information, University of Michigan, Ann Arbor, MI, USA
{longting,xianyujun,wnzhang,caoqinxiang}@sjtu.edu.cn, yutxie@umich.edu, yyu@apex.sjtu.edu.cn

2
2
0
2

b
e
F
5
2

]

G
L
.
s
c
[

1
v
1
8
4
2
1
.
2
0
2
2
:
v
i
X
r
a

Abstract

Program representation, which aims at converting program
source code into vectors with automatically extracted fea-
tures, is a fundamental problem in programming language
processing (PLP). Recent work tries to represent programs
with neural networks based on source code structures. How-
ever, such methods often focus on the syntax and consider
only one single perspective of programs, limiting the rep-
resentation power of models. This paper proposes a multi-
view graph (MVG) program representation method. MVG
pays more attention to code semantics and simultaneously
includes both data ﬂow and control ﬂow as multiple views.
These views are then combined and processed by a graph neu-
ral network (GNN) to obtain a comprehensive program repre-
sentation that covers various aspects. We thoroughly evaluate
our proposed MVG approach in the context of algorithm de-
tection, an important and challenging subﬁeld of PLP. Specif-
ically, we use a public dataset POJ-104 and also construct a
new challenging dataset ALG-109 to test our method. In ex-
periments, MVG outperforms previous methods signiﬁcantly,
demonstrating our model’s strong capability of representing
source code.

Introduction
With the advent of big code (Allamanis et al. 2018), pro-
gramming language processing (PLP) gains plenty of atten-
tion in recent years. PLP aims at assisting computers auto-
matically understanding and analyzing source code, which
beneﬁts downstream tasks in software engineering like code
retrieval (Lv et al. 2015; Nie et al. 2016), code annotation
(Yao, Peddamail, and Sun 2019), bug predicting and ﬁx-
ing (Xia et al. 2018; Wang, Su, and Singh 2018), program
translation (Chen, Liu, and Song 2018; Gu et al. 2017). To
take advantage of deep learning, the program representation
problem, i.e., how to convert source code into representa-
tional vectors, becomes a critical issue in PLP.

A great deal of literature devotes its efforts to the prob-
lem of program representation. Among these works, a ma-
jority of them represent source code only based on syntac-
tic information like abstract syntax trees (ASTs) (Mou et al.

*These authors contributed equally.
†Corresponding author.

Copyright © 2022, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

2016; Alon et al. 2019) while ignoring the semantics of pro-
grams. Thus, some researchers propose to include semantic
information by adding semantic edges onto ASTs (Allama-
nis, Brockschmidt, and Khademi 2018; Zhou et al. 2019).
However, the program representation still highly depends on
the syntax, and the semantics are relatively underweighted.
Moreover, in previous methods, information from different
aspects like syntax, data ﬂow, and control ﬂow are often
mixed up into one single view, making the information hard
to be disentangled.

Therefore in this paper, to address the problem mentioned
above, we propose to use a multi-view graph (MVG) rep-
resentation for source code. To obtain a more comprehen-
sive understanding of programs, we consider multiple graph
views from various aspects and levels. In particular, we em-
phasize more on semantics and include the following views
in MVG: the data-ﬂow graph (DFG), control-ﬂow graph
(CFG), read-write graph (RWG), and a combined graph
(CG). Among these views, DFG and CFG are widely used
in compiling and traditional program analysis. We construct
RWG based on DFG and CFG to capture the relationship
between operations and operands. We further include CG, a
combination of the former-mentioned graphs, to have an in-
tegral representation of the program. We then apply a gated
graph neural network (GGNN) (Li et al. 2016) to automati-
cally extract information from the four graph views.

We validate our proposed MVG method in the context
of algorithm detection, which is a fundamental subﬁeld of
PLP and aims at identifying the algorithms and data struc-
tures that appear in the source code. The ﬁrst reason for
which we choose this subﬁeld is because of its wide ap-
plication range: the detection results can be used as inter-
mediate information for further program analysis; we might
also apply algorithm detection in areas like programming
education, e.g., determining which algorithms are mastered
by the students. In addition to the wide range of appli-
cations, algorithm detection is also very challenging and
can serve as a benchmark for PLP program representation.
This is because: (1) A piece of code can contain multi-
ple different algorithms and data structures; (2) One algo-
rithm or data structure can have multiple possible imple-
mentations (e.g., Dynamic Programming, Segment
Tree); (3) Different algorithms can have very similar
implementations (e.g., Dijkstra’s Algorithm and

 
 
 
 
 
 
Prim’s Algorithm). Under this algorithm detection
task, we use two datasets to test our MVG model. The ﬁrst
one is a public dataset POJ-104 (Mou et al. 2016). We also
create a new dataset ALG-109, which is more challenging
than the former one. On both two datasets, our MVG model
outperforms previous methods signiﬁcantly, demonstrating
the outstanding representation power of MVG.
In summary, our contributions are as follows:

• We propose the MVG method, which can understand
source code from various aspects and levels. Specif-
ically, MVG includes four views in total:
the data-
ﬂow graph (DFG), control-ﬂow graph (CFG), read-write
graph (RWG), and a combined graph (CG);

• We create an algorithm classiﬁcation dataset ALG-109

to serve as a program representation benchmark;

• We validate MVG on the challenging algorithm detection
task with a public dataset POJ-104 and our constructed
dataset ALG-109. In experiments, MVG achieves state-
of-the-art performance, illustrating the effectiveness of
our approach.

Related Work
Previous methods on program representation can be di-
vided into four categories: data-based, sequence-based,
tree-based, and graph-based.

Data-based methods assume programs are functions that
map inputs to outputs. Therefore, such methods use the input
and output data to represent the program. Piech et al. (2015)
embed inputs and outputs of programs into a vector space
and use the embedded vectors to obtain program represen-
tations; Wang (2019) collects all the data during program
execution and feeds the data to a long short-term memory
(LSTM) unit (Hochreiter and Schmidhuber 1997) to obtain
program representations. Though seemingly intuitive, data-
based methods are often limited by the availability of the
input or output data, and it might take forever to enumerate
all possible inputs.

Sequence-based methods assume that programming lan-
guage is similar to natural language, and adjacent units in
code (e.g., tokens, instructions, or command lines) will have
a strong correlation. Hence, these methods apply models
in natural language processing (NLP) to source code. For
examples, Harer et al. (2018), Ben-Nun, Jakobovits, and
Hoeﬂer (2018), and Zuo et al. (2019) apply the word2vec
model (Le and Mikolov 2014) to learn the embeddings of
program tokens. Feng et al. (2020),Wang et al. (2020) and
Ciniselli et al. (2021) use a pre-trained BERT model to en-
code programs. Such sequence-based methods are easy to
use and can beneﬁt largely from the NLP community. How-
ever, since source code is highly structured, simple sequen-
tial modeling can result in a great deal of information loss.

Tree-based methods are mostly based on the abstract syn-
tax tree (AST), which is often used in compiling. In con-
trast to the sequential modeling of programming language,
AST contains more structural information of source code.
In the previous work, Mou et al. (2016) parse programs into
ASTs and then obtain program representations by applying a
tree-based convolutional neural network on the ASTs; Alon

et al. (2019) obtain program representations by aggregating
paths on the AST. Tree-based representations usually con-
tain more structural information than sequences, but the pro-
gram semantics might be relatively ignored compared with
the syntactic information.

Graph-based methods parse programs into graphs. Most
approaches from this category construct program graphs
by adding edges onto ASTs. For instance, Allamanis,
Brockschmidt, and Khademi (2018) introduce edges like
LastRead and LastWrite into AST. Then the program
representations are obtained with a gated graph neural net-
work (GGNN) (Li et al. 2016). Zhou et al. (2019) extend
the edges types in the work of Allamanis, Brockschmidt,
and Khademi (2018) and further improve the performance.
Although graph-based methods can have better perfor-
mance than previously mentioned categories (Allamanis,
Brockschmidt, and Khademi 2018), we notice that informa-
tion from different perspectives usually crowds in one single
view, i.e., most methods use one single graph to include var-
ious information in the source code, which can limit the rep-
resentation power of the model. Moreover, such approaches
tend to build their graphs based on the AST and give much
attention to the syntactic information, suppressing the se-
mantics of programs. By contrast, in this paper, we propose
the MVG method, which considers multiple program views
simultaneously. We extract features from data ﬂows, control
ﬂows, and read-write ﬂows, focusing more on the semantic
elements.

Methodology
This section describes how the MVG method converts pro-
grams into representational vectors. In particular, as dis-
played in Figure 1, we ﬁrst represent a piece of source code
as graphs of multiple views. We process these graphs with a
gated graph neural network (GGNN) to extract the informa-
tion in graphs. The extracted information is then combined
to obtain a comprehensive representation.

Program Graphs of Multiple Views

To understand a program from different aspects and lev-
els, we represent the program as graphs of multiple views.
We consider four views in total: (1) data-ﬂow graph (DFG);
(2) control-ﬂow graph (CFG); (3) read-write graph (RWG);
and (4) combined graph (CG).

Data-ﬂow graph (DFG) Data ﬂow is widely used to de-
picts programs in traditional program analysis (Aho, Sethi,
and Ullman 1986; Farrow, Kennedy, and Zucconi 1976;
Fraser and Hanson 1995; Muchnick et al. 1997). We use
DFG to capture the relationship between operands. In DFG,
nodes are operands and edges indicate data ﬂows. As it
is presented in Figure 1(b), the DFG of the code in Fig-
ure 1(a) is the green. DFG includes two types of nodes,
namely non-temporary operands and temporary operands.
Non-temporary operands denote variables and constants that
explicitly exist in the source code, and temporary operands
stand for temporary variables that only exist in program ex-
ecution. Two groups of edges are considered:

(a) The MVG pipeline.

(b) An example of the combined graph.

Figure 1: (a) The pipeline of MVG. Four graphs (i.e., DFG, CFG, RWG, and CG) are constructed based on the given source
code. These constructed graphs are then fed into a GGNN to obtain a ﬁnal program presentation for downstream tasks. (b) An
example of the combined graph (CG) corresponding to the program source code in (a).

• Operation edges exist in non-function-switch code. They
connect the nodes to be operated and the nodes that re-
ceive the operation results. Standard operations are in-
cluded in this category, e.g., =, +, -, *, /, >, <, ==. We
distinguish different types of operations by using various
types of edges.

• Function edges indicate data ﬂows for function calls and
returns, including two types of edges: Argument and
ReturnTo. We use Argument edges in function calls
to connect actual arguments and the corresponding for-
mal arguments. We use ReturnTo edges to associate
return values and the variables that receive the returns.

Control-ﬂow graph (CFG) We utilize CFG to model the
execution order of operations. As Figure 1(b) shows, the
CFG of the code in Figure 1(a) is the red. Based on com-
pilers principles (Aho, Sethi, and Ullman 1986; Allen 1970),
we slightly adjust the design of CFG to better capture the key
information of the program. Nodes in CFG are operations
in the source code, including standard operations, function
calls and returns. Edges indicate the execution order of op-
erations. The following edge types are considered:

• Condition edges indicate conditional jumps in loops or
branches (e.g., while, for, if). We deﬁne PosNext
and NegNext two subtypes to represent situations
where the conditions are True or False respectively.
These edges start from condition operations and end at
the ﬁrst operation in the True or False blocks.

• Iteration edges are denoted as IterJump. We use them
in loops (e.g., while and for) to indicate jumps at the
end of each iteration, connecting the last and the ﬁrst op-
erations in the loop.

• Function edges are used in function calls and returns, in-
cluding two subtypes CallNext and ReturnNext.
CallNext edges start from function call operations

and point to the ﬁrst operations in the called functions.
ReturnNext edges begin with the last operations in
called functions and end at the operations right after the
corresponding function calls.

• Next edges stand for the most common execution order
except for the above cases. Denoted as Next, they con-
nect operations and their successor in execution order.

Read-write graph (RWG) We design the RWG to capture
the interaction between operands and operations. As Figure
1(b) shows, part of RWG for the code in Figure 1(a) is the
yellow edges and the nodes connect to yellow edges. RWG
is a bipartite graph with operands and operations as nodes.
Two types of edges are introduced to connect operands and
operations:

• Read edges start from operands and point to operations,

meaning operations take operands to compute.

• Write edges start from operations and point to operands,

meaning variables receive the operation results.

Combined graph (CG)
In addition to DFG, CFG, and
RWG, we further introduce a combined graph to capture the
comprehensive overall information of a program. CG is an
integral representation of the above three graphs and is ob-
tained by ﬁrst including all nodes and edges in DFG and
CFG, and then adding Read and Write edges to connect
variable and operation nodes as Figure 1(b) shows.

To summarize, formally, we can denote the graph of each
view as Gi = {Vi, Ei} where i ∈ {DFG, CFG, RWG, CG},
Vi is the node set and Ei is the edge set. We have VRWG ⊆
VCG = VDFG ∪ VCFG, and ECG = EDFG ∪ ECFG ∪ ERWG.

Extracting Information with a GGNN
As mentioned above, a program can be represented as four
views in the form of graphs. Here, we adopt a gated graph

Multiple ViewsRepresentationDFGCFGRWGGGNNMax PoolingMax PoolingMax PoolingMax PoolingProgram Source CodeCGCGDownstream Tasksint f(int x) {     int t =2 x;     if (x % 2)          t =3 x + 1;     return t; } int main() {     int a =0 2;     int b =1 f(a);     int c =4 a - b;     int i0 =5 0;     while (i1 < 5)          ++i2; }03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 OperationsOperandsData FlowsControl FlowsRead/Write2x%2ax1f(a)tx+1b0i05i1<5i2i1a-bc=+2+1%1ArugmentReturnTo===-1-2==<2<1++%2%=2=0+Call=1=3Rtn-=5=4++<Return NextNextNextNextNextNextPosNextNextCallNextNextNextNegNextNegNextIterJumpPosNextWriteReadReadReadWriteReadWriteh0
u = xu,

(1)

5:

neural network (GGNN) (Li et al. 2016), a widely used
graph neural network (GNN) model, to extract features from
each graph view.

For a graph Gi of an arbitrary view, this GGNN ﬁrst ini-
tializes nodes’ hidden representations with one-hot encod-
ings of node types (i.e., operation or operand types). That is,
for any node u ∈ Vi, we initialize its hidden state as below:

where h0
encoding of u’s node type.

u is the initial hidden state of u, and xu is the one-hot

The nodes then update their states by propagating mes-

sages in the graph as the following equations:

mt

v

u,v = fe(ht−1
),
u = Mean({mt
¯mt
ht
u = GRU(ht−1

e = (u, v) ∈ Ei,

u,v}v∈N (u)),
u, ),

u , ¯mt

u ∈ Vi,

u ∈ Vi,

(2)

(3)

(4)

where u, v are node indicators and e is the edge which
connects u and v, mt
u,v stands for the message u receives
from v at the t-th iteration, fe(·) is a message passing func-
tion that depends on the edge type of e, ht−1
represents the
v
hidden state of v from the last iteration, ¯mt
u is the aggregated
message received by u, Mean(·) denotes the average pooling
function, N (u) is the set of u’s neighbors, ht
u is the updated
hidden state, and GRU(·) is a gated recurrent unit (Cho et al.
2014).

After T iterations, the hidden states will contain enough
information of the given graph. Therefore, we take the hid-
den states of nodes at the ﬁnal iteration and integrate them
using a max pooling to obtain a ﬁnal vector representation
of the graph view Gi:

zi = MaxPooling({hT

u }u∈Vi).

(5)

Program Representation

To form an overall representation of the program, we con-
catenate representations from all views:

z = zDFG ⊕ zCFG ⊕ zRWG ⊕ zCG,

(6)

where zDFG, zCFG, zRWG, zCG are representations for DFG,
CFG, RWG, and CG respectively computed as Equation 5,
⊕ denotes concatenation.

In summary, our proposed MVG method is outlined in

Algorithm 1.

Experiments

Algorithm 1: MVG Program Representation Method
Input: Source code of a program;
Output: The vector representation of the input program;
1: Construct DFG, CFG, and RWG;
2: Construct CG based on DFG, CFG, and RWG;
3: for Gi ∈ {GDF G, GCF G, GRW G, GCG} do
4:

For ∀u ∈ Vi, initialize its hidden representation with
the one-hot encoding of the node type: h0
Iteratively update node hidden representations with a
GGNN for T steps (Eq. 2-4);
Compute the graph representation zi as Eq. 5;

6:
7: end for
8: Compute the program representation z as Eq. 6;
9: Feed z to downstream tasks, e.g., algorithm detection;

u = xu;

Baselines
We compare our MVG method with four representative pro-
gram representation methods in the recent literature.
• NCC (Ben-Nun, Jakobovits, and Hoeﬂer 2018) is a
sequence-based method that compiles programs into in-
termediate representations (IRs) and obtains program
representations with the skip-gram algorithm.

• TBCNN (Mou et al. 2016) is a tree-based method that

extracts features from program ASTs.

• LRPG1 (Allamanis, Brockschmidt, and Khademi 2018)
is a graph-based method. It introduces semantic edges
such as control ﬂows and data dependencies into the AST
and extracts program features from the resulted graph.
• Devign (Zhou et al. 2019) is an extension of LRPG and
improves the performance by including more types of
control-ﬂow and data dependency edges.

POJ-104: Algorithmic Problem Classiﬁcation
Dataset description POJ-104 is a public dataset that
contains source code solutions for algorithmic programming
problems on the Peking University online judge2 (Mou et al.
2016). This dataset contains 52,000 programs, and each pro-
gram is labeled with an algorithmic problem ID. In total,
104 problems are included, corresponding to a multi-class
single-label classiﬁcation problem with 104 classes.

Typically, a particular algorithmic problem will require
the solution code to contain certain algorithms or data struc-
tures to obtain the correct answer. Therefore, there is an im-
plicit mapping between the problem ID labels and algorithm
types. The statistics for this dataset are listed in Table 1.

Implementation details We implement a rule-based
parser to pre-process the source code of the input pro-
grams to obtain DFG, CFG, RWG, and we merge DFG,
CFG, and RWG to generate the CG. To predict the la-
bel of the input programs, we feed its program representa-
tion to a two-layer multilayer perceptron (MLP) wrapped

In this section, we evaluate our proposed MVG method on
two algorithm detection datasets POJ-104 and ALG-109.
The implementation for our proposed MVG model and the
datasets are available at https://github.com/githubg0/mvg.

1LRPG: In the published paper, this model is called GGNN,
which may be confused with gated graph neural networks. Here
we refer to it as LRPG by taking the abbreviation of the paper title.

2Peking University online judge (POJ): http://poj.org/.

Table 1: Dataset statistics.

Table 3: Most frequent ten algorithms in ALG-109, denoted
as ALG-10.

Classiﬁcation
Label
#Classes
#Samples
Average #lines
Average #labels
Language

POJ-104

ALG-109

ALG-10

Single-label Multi-label Multi-label
Algorithms
Algorithms
Problem ID
10
109
104
7,974
11,913
52,000
94.37
94.27
36.26
1.70
1.94
1.00
C/C++
C/C++
C

Table 2: Experiment results on POJ-104.

Method
Accuracy(%)

NCC
94.83

TBCNN
94.00

LRPG
90.31

Devign MVG
94.96
92.82

by the Softmax function. The dimension is selected from
{100, 120, 140, 160, 180, 200}, the iterations T for message
propagation is selected from {1, 2, 4, 8}. We use the Adam
optimizer (Kingma and Ba 2014) to train the model, the
learning learning rate is selected from {1 × 10−3, 6 ×
10−4, 3 × 10−4, 1 × 10−4}. For all the baselines, the hy-
perparameters are carefully tuned to the best performance.

Results and discussion Following previous work (Mou
et al. 2016; Bui, Yu, and Jiang 2021), we evaluate the ac-
curacy of model predictions on POJ-104. The higher accu-
racy denotes better performance. The experiment results are
shown in Tables 2.

From the results, we can see that MVG achieves the high-
est accuracy 94.96%. However, other baselines can also
achieve very high accuracy, e.g., 94.83%, and 94.00%. We
assume this is because algorithmic problem classiﬁcation
is too easy for the models. For example, algorithmic prob-
lems will often require certain input and output formats, and
this could leak information to the models, providing them a
shortcut to classify the problem ID. Therefore, we do need a
more challenging dataset to further distinguish the program
representation power of models.

ALG-109: Algorithm Classiﬁcation
Dataset description As mentioned above, the algorith-
mic problem classiﬁcation dataset POJ-104 is too easy to
distinguish the representation power of compared models.
Besides, there is no other public annotated algorithm de-
tection dataset in the literature. Therefore, we construct a
more realistic and more challenging algorithm classiﬁcation
dataset ALG-109 by ourselves to serve as a new benchmark.
ALG-109 contains 11,913 pieces of source code collected
from the the CSDN website3. Each program is labeled with
the algorithms and data structures that appear in the source
code. So different from POJ-104, the ALG-109 dataset
corresponds to a much harder multi-class multi-label clas-
siﬁcation problem. The algorithm labels are annotated by

3The CSDN website: https://www.csdn.net/.

Algorithm
Recursion
DepthFirstSearch
BreadthFirstSearch
Queue
SegmentTree

#Samples
4365
3117
1407
1083
775

1
2
3
4
5

Algorithm
Enumeration
GreedyAlgorithm
Recurrence
DisjointSetUnion
QuickSort

#Samples
681
557
551
548
501

6
7
8
9
10

Table 4: Experiment results on ALG-109 and ALG-10.

9
0
1
-
G
L
A

0
1
-
G
L
A

Method Micro-F1(%)
48.96 ± 0.91
35.03 ± 3.54
60.56 ± 0.87
56.90 ± 1.57
65.26 ± 0.85
72.18 ± 0.89
67.53 ± 0.79
78.48 ± 1.51
78.40 ± 0.98
80.15 ± 0.86

NCC
TBCNN
LRPG
Devign
MVG
NCC
TBCNN
LRPG
Devign
MVG

Exact Match(%)
21.01 ± 1.24
9.13 ± 1.34
30.14 ± 1.33
27.67 ± 1.04
36.27 ± 0.67
46.46 ± 1.34
34.34 ± 0.96
55.21 ± 2.85
55.85 ± 1.88
58.36 ± 1.99

Ham-Loss(%)
1.61 ± 1.24
1.44 ± 0.01
1.09 ± 0.02
1.16 ± 0.02
1.03 ± 0.02
9.29 ± 0.28
9.88 ± 0.46
7.31 ± 0.59
7.16 ± 0.23
6.67 ± 0.29

previous programming contest participants who have ade-
quate domain knowledge. Overall, 109 algorithms and data
structures are considered. The most frequently appearing ten
algorithms are listed in Table 3, and we denote this subset as
ALG-10. The statistics of the constructed dataset are listed
in Table 1. We randomly split 80% data for training and val-
idation, and 20% for testing.

Implementation details We implement a rule-based
parser to pre-process the code to obtain DFG, CFG, and
RWG, and we merge DFG, CFG, and RWG to obtain the
CG. To predict the algorithms in the programs, we feed pro-
gram representation to a two-layer MLP wrapped by a Sig-
moid function to obtain the occurrence probability of each
algorithm. If the occurrence probability of an algorithm is
larger than 0.5, we consider it as one of the algorithms
which implement the corresponding program. The dimen-
sion is selected from {120, 144, 168, 192, 216}, the itera-
tions T for message propagation is selected from {1, 2, 4, 8}.
We use the Adam optimizer (Kingma and Ba 2014) to
train the model, the learning learning rate is selected from
{1×10−3, 6×10−4, 3×10−4, 1×10−4}. For the baselines,
the hyperparameters are carefully tuned to the best perfor-
mance.

Results and discussion We evaluate the performance of
models on the testing data with three different metrics: the
micro-F1 score, the exact match accuracy, and the Hamming
loss. A higher micro-F1 score and exact match accuracy in-
dicate a superior performance, while a lower Hamming loss
stands for the better. The experiment results on ALG-109
and ALG-10 are shown in Table 4.

From Table 4, we observe that: (1) Our proposed MVG
method surpasses all the baselines signiﬁcantly on both
ALG-109 and ALG-10, illustrating MVG’s superior per-

Table 5: Ablation study on ALG-109.

Variant Micro-F1(%)
65.26 ± 0.85
MVG
62.34 ± 1.11
– DFG
64.18 ± 0.86
– CFG
64.01 ± 1.06
– RWG
64.38 ± 0.78
– CG
62.02 ± 0.74
OnlyCG
65.19 ± 0.94
+AST

Exact Match(%)
36.27 ± 0.67
32.67 ± 0.85
34.72 ± 1.08
35.00 ± 0.91
34.86 ± 0.93
32.06 ± 0.99
36.10 ± 1.25

Ham-Loss(%)
1.03 ± 0.02
1.09 ± 0.02
1.06 ± 0.02
1.06 ± 0.03
1.06 ± 0.02
1.09 ± 0.02
1.04 ± 0.02

Figure 2: Comparing MVG and baselines on algorithm la-
bels. In each subplot, each point represents one particular
algorithm label. The y-axes are accuracy obtained by MVG,
while the x-axes are accuracy obtained by the baselines. The
number of samples and the algorithm type of each label are
distinguished by the color and point style respectively. A
point lying above y = x means MVG is performing better
than the baseline for this algorithm label.

formance on algorithm classiﬁcation. (2) Graph-based meth-
ods (i.e., LRPG, Devign, and MVG) all perform remarkably
better than the sequence-based method (i.e., NCC) and the
tree-based method (i.e., TBCNN), showing the great poten-
tial of representing programs as graphs. (3) All models’ per-
formances drop when moving from ALG-10 to ALG-109,
because labels in ALG-10 will be bound more training data.
However, we can see the gap of MVG is smaller than others,
which means MVG is relatively less sensitive to the insuf-
ﬁciency of data. (4) Comparing with the experiment results
from POJ-104, we ﬁnd the methods are more distinguish-
able on ALG-109, and there is still large room for mod-
els to further improve their performances on this dataset.
Therefore, our constructed ALG-109 dataset might serve
better as an algorithm detection or PLP program representa-
tion benchmark.

To further investigate how these models perform dissim-
ilarly on each speciﬁc algorithm label, we compare MVG
with the baselines and visualize the results in Figure 2. From
the visualization, we can see that, for almost all algorithm la-
bels, MVG will perform superior to the baselines, especially
for the labels with insufﬁcient data.

Figure 3: Algorithm classiﬁcation accuracy changes of
model variants. For each variant, the top ﬁves labels with the
largest performance drops as well as increases are shown.

Ablation study To obtain a deep understanding of MVG’s
outstanding performance, we conduct some further ablation
studies to learn each view’s impact on the MVG model.
Here, we consider six variants:

• -DFG removes the DFG view from MVG. The data-ﬂow

information in CG is also removed accordingly.

• -CFG removes the CFG view from MVG. The control-
ﬂow information in CG is also removed accordingly.
• -RWG removes the RWG view from MVG. The read-
write information in CG is also removed accordingly.
• -CG removes the combined CG view from MVG, so the
other three views (i.e., DFG, CFG, RWG) will no longer
interact with each other.

• OnlyCG contains only the combined CG view, so the
data-ﬂow, control-ﬂow, and read-write information will
be mixed up together into one single view;

                 7 % & 1 1                0 9 * 5 H F X U V L R Q ' H S W K  ) L U V W  6 H D U F K  D   & R P S D U H G  Z L W K  7 % & 1 1                  1 & &                0 9 * / R Q J H V W  & R P P R Q  6 X E V H T X H Q F H 3 U L R U L W \  4 X H X H 3 O D Q D U  * U D S K ' L J L W  ' \ Q D P L F  3 U R J U D P P L Q J 1 X P E H U  7 K H R U \  E   & R P S D U H G  Z L W K  1 & &                  ' H Y L J Q                0 9 * 6 L P X O D W H G  $ Q Q H D O L Q J + \ S H U S O D Q  , Q W H U V H F W L R Q  F   & R P S D U H G  Z L W K  ' H Y L J Q                  / 5 3 *                0 9 * 3 U L R U L W \  4 X H X H % U H D G W K  ) L U V W  6 H D U F K ' D Q F L Q J  / L Q N V  G   & R P S D U H G  Z L W K  / 5 3 *  0 D W K  D Q G  & R P S X W D W L R Q D O  * H R P H W U \ % D V L F  $ O J R U L W K P ' \ Q D P L F  3 U R J U D P P L Q J * U D S K  $ O J R U L W K P ' D W D  6 W U X F W X U H 6 R U W L Q J              6 D P S O H  Q X P E H U V                         & K D Q J H V  L Q  D F F X U D F \ / R Q J H V W  ' H F U H D V L Q J  6 X E V H T X H Q F H      6 L P X O D W H G  $ Q Q H D O L Q J   6 $        0 D Q D F K H U 
 V  $ O J R U L W K P       / R Q J H V W  & R P P R Q  6 X E V H T X H Q F H       % D O D Q F H G  7 U H H       / R Z H V W  & R P P R Q  $ Q F H V W R U        0 R Q R W R Q H  4 X H X H        6 S U D J X H  * U X Q G \  7 K H R U H P       0 D W U L [  0 X O W L S O L F D W L R Q       3 y O \ D  ( Q X P H U D W L R Q  7 K H R U H P         D   7 K H +AST  Y D U L D Q W                            & K D Q J H V  L Q  D F F X U D F \ % D O D Q F H G  7 U H H       ' L J L W  ' \ Q D P L F  3 U R J U D P P L Q J       ) D V W  ) R X U L H U  7 U D Q V I R U P   ) ) 7        6 L P X O D W H G  $ Q Q H D O L Q J   6 $        6 S O D \  7 U H H       . Q D S V D F N  3 U R E O H P        0 D Q D F K H U 
 V  $ O J R U L W K P        3 L J H R Q K R O H  3 U L Q F L S O H       6 S U D J X H  * U X Q G \  7 K H R U H P       3 y O \ D  ( Q X P H U D W L R Q  7 K H R U H P         E   7 K H DFG  Y D U L D Q W                   & K D Q J H V  L Q  D F F X U D F \ 6 L P X O D W H G  $ Q Q H D O L Q J   6 $        & K L Q H V H  5 H P D L Q G H U  7 K H R U H P      7 R S R O R J L F D O  6 R U W       ' L J L W  ' \ Q D P L F  3 U R J U D P P L Q J       / R Q J H V W  ' H F U H D V L Q J  6 X E V H T X H Q F H      3 U L P 
 V  $ O J R U L W K P        ' D Q F L Q J  / L Q N V       3 L J H R Q K R O H  3 U L Q F L S O H       0 2 
 V  $ O J R U L W K P        3 y O \ D  ( Q X P H U D W L R Q  7 K H R U H P         F   7 K H CFG  Y D U L D Q W                            & K D Q J H V  L Q  D F F X U D F \ ' L J L W  ' \ Q D P L F  3 U R J U D P P L Q J        % D O D Q F H G  7 U H H       6 L P X O D W H G  $ Q Q H D O L Q J   6 $        & K L Q H V H  5 H P D L Q G H U  7 K H R U H P      7 R S R O R J L F D O  6 R U W       0 | E L X V  , Q Y H U V L R Q  ) R U P X O D       0 R Q R W R Q H  4 X H X H       0 2 
 V  $ O J R U L W K P        3 L J H R Q K R O H  3 U L Q F L S O H       3 y O \ D  ( Q X P H U D W L R Q  7 K H R U H P         G   7 K H RWG  Y D U L D Q W                            & K D Q J H V  L Q  D F F X U D F \ ' L J L W  ' \ Q D P L F  3 U R J U D P P L Q J        6 S O D \  7 U H H       + X Q J D U L D Q   . 0   $ O J R U L W K P       4 X H X H      % D O D Q F H G  7 U H H      ) H U P D W 
 V  / L W W O H  7 K H R U H P        0 D Q D F K H U 
 V  $ O J R U L W K P        6 S U D J X H  * U X Q G \  7 K H R U H P       0 2 
 V  $ O J R U L W K P       3 y O \ D  ( Q X P H U D W L R Q  7 K H R U H P         H   7 K H CG  Y D U L D Q W                         & K D Q J H V  L Q  D F F X U D F \ / R Q J H V W  ' H F U H D V L Q J  6 X E V H T X H Q F H      6 L P X O D W H G  $ Q Q H D O L Q J   6 $        6 X I I L [  $ X W R P D W D   6 $ 0       % H O O P D Q  ) R U G  $ O J R U L W K P      7 U H H  % D V H G  ' \ Q D P L F  3 U R J U D P P L Q J      3 L J H R Q K R O H  3 U L Q F L S O H       6 S U D J X H  * U X Q G \  7 K H R U H P       / R Z H V W  & R P P R Q  $ Q F H V W R U        6 O R S H  2 S W L P L ] D W L R Q        3 y O \ D  ( Q X P H U D W L R Q  7 K H R U H P         I   7 K H OnlyCG  Y D U L D Q W ' \ Q D P L F  3 U R J U D P P L Q J % D V L F  $ O J R U L W K P ' D W D  6 W U X F W X U H * U D S K  $ O J R U L W K P 0 D W K  D Q G  & R P S X W D W L R Q D O  * H R P H W U \ 6 R U W L Q J              6 D P S O H  Q X P E H U V• +AST adds an abstract syntax tree (AST) view to MVG,

which will include more syntactic information.

The ablation study results are listed in Table 5. From the
results, we ﬁnd that: (1) Removing any view from MVG
(i.e., -DFG, -CFG, -RWG, and -CG) will cause a drop in
performance, showing the indispensable role of every view
in program representation. (2) Adding the AST view (i.e.,
+AST) harms the performance slightly, which means the
AST view is unnecessary in algorithm detection task and we
should not emphasize too much on the syntax in program
representation. (3) Removing CG undermines the accuracy,
meaning interactively combining the other three views helps
MVG to better understand the programs. On the other hand,
the performance of the OnlyCG variant is also inferior to
MVG. Therefore, we can conclude that both the independent
views (i.e., DFG, CFG, RWG) and the integral view (i.e.,
CG) are necessary for our program representation. (4) Com-
paring all the variants, we ﬁnd by deleting the DFG view,
the performance drops the most, showing that DFG is most
critical in our program representation model.

We also examine how the performance of different algo-
rithms changes when using different model variants. The
results are displayed in Figure 3. Here, for each model
variant, we show the top ﬁve labels with the largest per-
formance drops and increases. From the results, we ob-
serve that: (1) Overall, by changing the design of MVG
into other variants, the performance drops more while in-
creasing less; (2) Compared with the variants, our MVG
seems to have better representation for programs that con-
tain Math and Computational Geometry algorithms, e.g..,
P´olya Enumeration Theorem, since when replacing
MVG with other variants, the detection performance on
these algorithms drop signiﬁcantly.

Case study To intuitively ﬁgure out whether our MVG
model can render better program representations, we use
UMAP (McInnes, Healy, and Melville 2018) to visualize the
representation vectors encoded by the three best-performing
models (i.e., MVG, Devign, and LRPG) in Figure 4.

Three groups of algorithms are compared: (1) We ﬁrst
compare three sorting algorithms (i.e., Merge Sort,
Quick Sort, and Topological Sort). From the vi-
sualization, we can see that both Devign and LRPG can
not distinguish Quick Sort and Topological Sort
well, while MVG represents these two algorithms more dif-
ferently. (2) For the shortest-path algorithms, the represen-
tation power of Devign, LRPG, and MVG seem almost the
same. (3) We also compare Dijkstra’s Algorithm
and Prim’s Algorithm, since they are designed for dif-
ferent intentions while having very similar implementations
(i.e., both the two algorithms utilize the Breadth-First
Search). From the visualization, we can see MVG gives a
much more clear decision boundary of the two algorithms,
meaning our method has higher representation power than
the other two baselines. (4) Associating the results presented
in Table 4 and Figure 2, we ﬁnd MVG is more capable of
representing source code especially under the context of al-
gorithm detection.

(a) Sorting algorithms.

(b) Shortest path algorithms

(c) Dijkstra’s Algorithm and Prim’s Algorithm.

Figure 4: Visualization of program representations.

Conclusion
This paper presents a multi-view graph (MVG) program rep-
resentation method for PLP. To understand source code more
comprehensively and semantically, we propose to include
four graph views of different levels and various aspects: the
data-ﬂow graph (DFG), the control-ﬂow graph (CFG), the
read-write graph (RWG), and an integral combined graph
(CG). We evaluate our proposed MVG method in the con-
text of algorithm detection, which is an important and chal-
lenging subﬁeld of PLP. To ﬁll the vacancy of a high-quality
algorithm detection dataset, we construct ALG-109, an al-
gorithm classiﬁcation dataset that contains 109 algorithms
and data structures in total. In experiments, MVG achieves
state-of-the-art performance, demonstrating its outstanding
capability of representing programs.

For future work, it would be interesting to investigate how
our MVG approach can be combined with other orthogonal
techniques like pre-training. Moreover, we might also apply
the MVG model and the annotated dataset ALG-109 for the
purpose of programming education.

Acknowledgement
We would like to thank Enze Sun and Hanye Zhao from
Shanghai Jiao Tong University for their efforts in reviewing
the data and baselines. We also thank anonymous reviewers
for their constructive comments and suggestions. This work
is partially supported by the Shanghai Municipal Science
and Technology Major Project (2021SHZDZX0102) and the
National Natural Science Foundation of China (62177033).

  D   ' H Y L J Q  E   / 5 3 *  F   0 9 * 0 H U J H  6 R U W 4 X L F N  6 R U W 7 R S R O R J L F D O  6 R U W  D   ' H Y L J Q  E   / 5 3 *  F   0 9 * 6 3 ) $ ) O R \ G  : D U V K D O O  $ O J R U L W K P ' L M N V W U D 
 V  $ O J R U L W K P  D   ' H Y L J Q  E   / 5 3 *  F   0 9 * ' L M N V W U D 
 V  $ O J R U L W K P 3 U L P 
 V  $ O J R U L W K PReferences
Aho, A. V.; Sethi, R.; and Ullman, J. D. 1986. Compilers,
principles, techniques. Addison wesley, 7(8): 9.
Allamanis, M.; Barr, E. T.; Devanbu, P.; and Sutton, C. 2018.
A survey of machine learning for big code and naturalness.
ACM Computing Surveys (CSUR), 51(4): 1–37.
Allamanis, M.; Brockschmidt, M.; and Khademi, M. 2018.
Learning to Represent Programs with Graphs.
Allen, F. E. 1970. Control ﬂow analysis. In ACM Sigplan
Notices, volume 5, 1–19. ACM.
Alon, U.; Zilberstein, M.; Levy, O.; and Yahav, E. 2019.
code2vec: Learning distributed representations of code.
Proceedings of
the ACM on Programming Languages,
3(POPL): 40.
Ben-Nun, T.; Jakobovits, A. S.; and Hoeﬂer, T. 2018. Neu-
ral code comprehension: a learnable representation of code
semantics. In Advances in Neural Information Processing
Systems, 3585–3597.
Bui, N. D.; Yu, Y.; and Jiang, L. 2021. TreeCaps: Tree-based
capsule networks for source code processing. In Proceed-
ings of the 35th AAAI Conference on Artiﬁcial Intelligence.
Chen, X.; Liu, C.; and Song, D. 2018. Tree-to-tree neural
networks for program translation. In NIPS’18 Proceedings
of the 32nd International Conference on Neural Information
Processing Systems, volume 31, 2552–2562.
Cho, K.; van Merri¨enboer, B.; Bahdanau, D.; and Bengio,
Y. 2014. On the Properties of Neural Machine Translation:
In Proceedings of SSST-8,
Encoder–Decoder Approaches.
Eighth Workshop on Syntax, Semantics and Structure in Sta-
tistical Translation, 103–111.
Ciniselli, M.; Cooper, N.; Pascarella, L.; Poshyvanyk, D.;
Di Penta, M.; and Bavota, G. 2021. An empirical study on
In 2021
the usage of BERT models for code completion.
IEEE/ACM 18th International Conference on Mining Soft-
ware Repositories (MSR), 108–119. IEEE.
Farrow, R.; Kennedy, K.; and Zucconi, L. 1976. Graph
In 17th
grammars and global program data ﬂow analysis.
Annual Symposium on Foundations of Computer Science
(sfcs 1976), 42–56. IEEE.
Feng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong, M.;
Shou, L.; Qin, B.; Liu, T.; Jiang, D.; et al. 2020. Code-
BERT: A Pre-Trained Model for Programming and Natural
Languages. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing: Findings,
1536–1547.
Fraser, C. W.; and Hanson, D. R. 1995. A retargetable
C compiler: design and implementation. Addison-Wesley
Longman Publishing Co., Inc.
Gu, X.; Zhang, H.; Zhang, D.; and Kim, S. 2017. DeepAM:
migrate APIs with multi-modal sequence to sequence learn-
ing. In IJCAI’17 Proceedings of the 26th International Joint
Conference on Artiﬁcial Intelligence, 3675–3681.
Harer, J. A.; Kim, L. Y.; Russell, R. L.; Ozdemir, O.; Kosta,
L. R.; Rangamani, A.; Hamilton, L. H.; Centeno, G. I.; Key,
J. R.; Ellingwood, P. M.; et al. 2018. Automated soft-
ware vulnerability detection with machine learning. arXiv
preprint arXiv:1803.04497.

Hochreiter, S.; and Schmidhuber, J. 1997. Long short-term
memory. Neural computation, 9(8): 1735–1780.
Kingma, D. P.; and Ba, J. 2014. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980.
Le, Q.; and Mikolov, T. 2014. Distributed representations
of sentences and documents. In International conference on
machine learning, 1188–1196.
Li, Y.; Tarlow, D.; Brockschmidt, M.; and Zemel, R. S. 2016.
Gated Graph Sequence Neural Networks. In ICLR.
Lv, F.; Zhang, H.; Lou, J.-g.; Wang, S.; Zhang, D.; and Zhao,
J. 2015. Codehow: Effective code search based on api un-
derstanding and extended boolean model (e). In 2015 30th
IEEE/ACM International Conference on Automated Soft-
ware Engineering (ASE), 260–270. IEEE.
McInnes, L.; Healy, J.; and Melville, J. 2018. Umap: Uni-
form manifold approximation and projection for dimension
reduction. arXiv preprint arXiv:1802.03426.
Mou, L.; Li, G.; Zhang, L.; Wang, T.; and Jin, Z. 2016. Con-
volutional neural networks over tree structures for program-
ming language processing. In Thirtieth AAAI Conference on
Artiﬁcial Intelligence.
Muchnick, S.; et al. 1997. Advanced compiler design imple-
mentation. Morgan kaufmann.
Nie, L.; Jiang, H.; Ren, Z.; Sun, Z.; and Li, X. 2016. Query
expansion based on crowd knowledge for code search. IEEE
Transactions on Services Computing, 9(5): 771–783.
Piech, C.; Huang, J.; Nguyen, A.; Phulsuksombati, M.; Sa-
hami, M.; and Guibas, L. 2015. Learning program embed-
In Interna-
dings to propagate feedback on student code.
tional conference on machine Learning, 1093–1102. PMLR.
Learning Scalable and Precise Rep-
Wang, K. 2019.
arXiv preprint
resentation of Program Semantics.
arXiv:1905.05251.
Wang, K.; Su, Z.; and Singh, R. 2018. Dynamic Neural
Program Embeddings for Program Repair. In International
Conference on Learning Representations.
Wang, R.; Zhang, H.; Lu, G.; Lyu, L.; and Lyu, C. 2020.
Fret: Functional reinforced transformer with BERT for code
summarization. IEEE Access, 8: 135591–135604.
Xia, X.; Bao, L.; Lo, D.; Xing, Z.; Hassan, A. E.; and Li,
S. 2018. Measuring program comprehension: a large-scale
ﬁeld study with professionals. In Proceedings of the 40th In-
ternational Conference on Software Engineering, 584–584.
Yao, Z.; Peddamail, J. R.; and Sun, H. 2019. CoaCor: Code
Annotation for Code Retrieval with Reinforcement Learn-
ing. In The World Wide Web Conference, 2203–2214. ACM.
Zhou, Y.; Liu, S.; Siow, J.; Du, X.; and Liu, Y. 2019.
Devign: Effective Vulnerability Identiﬁcation by Learning
Comprehensive Program Semantics via Graph Neural Net-
works. In Advances in Neural Information Processing Sys-
tems, 10197–10207.
Zuo, F.; Li, X.; Young, P.; Luo, L.; Zeng, Q.; and Zhang,
Z. 2019. Neural Machine Translation Inspired Binary Code
Similarity Comparison beyond Function Pairs. representa-
tions, 48: 50.

