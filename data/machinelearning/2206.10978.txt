Noname manuscript No.
(will be inserted by the editor)

Multi-task twin support vector machine with
Universum data

Hossein Moosaei∗ · Fatemeh Bazikar ·
Milan Hlad´ık

2
2
0
2

n
u
J

2
2

]

G
L
.
s
c
[

1
v
8
7
9
0
1
.
6
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract Multi-task learning (MTL) has emerged as a promising topic of ma-
chine learning in recent years, aiming to enhance the performance of numerous
related learning tasks by exploiting beneﬁcial information. During the training
phase, most of the existing multi-task learning models concentrate entirely on the
target task data and ignore the non-target task data contained in the target tasks.
To address this issue, Universum data, that do not correspond to any class of a
classiﬁcation problem, may be used as prior knowledge in the training model. This
study looks at the challenge of multi-task learning using Universum data to employ
non-target task data, which leads to better performance. It proposes a multi-task
twin support vector machine with Universum data (UMTSVM) and provides two
approaches to its solution. The ﬁrst approach takes into account the dual for-
mulation of UMTSVM and tries to solve a quadratic programming problem. The
second approach formulates a least-squares version of UMTSVM and refers to it
as LS-UMTSVM to further increase the generalization performance. The solution
of the two primal problems in LS-UMTSVM is simpliﬁed to solving just two sys-
tems of linear equations, resulting in an incredibly simple and quick approach.
Numerical experiments on several popular multi-task data sets and medical data
sets demonstrate the eﬃciency of the proposed methods.

∗ Corresponding Author

Hossein Moosaei
Department of Mathematics, University of Bojnord, Bojnord, Iran
Department of Applied Mathematics, School of Computer Science, Charles University, Prague,
Czech Republic
E-mail: hmoosaei@gmail.com, hmoosaei@kam.mﬀ.cuni.cz

Fatemeh Bazikar
Department of Applied Mathematics, Faculty of Mathematical Sciences, University of Guilan,
Rasht, Iran
E-mail: f.bazikar@gmail.com, fatemeh−bazikar@phd.guilan.ac.ir

Milan Hlad´ık
Department of Applied Mathematics, Faculty of Mathematics and Physics, Charles University,
Prague, Czech Republic
E-mail: hladik@kam.mﬀ.cuni.cz

 
 
 
 
 
 
2

Hossein Moosaei∗ et al.

Keywords Multi-task learning; Universum; Twin support vector machine; Dual
problem; Least-squares.

1 Introduction

Pattern recognition is becoming more important in various ﬁelds due to the devel-
opment of machine learning. Traditional pattern recognition focuses on single-task
learning (STL), with multi-task learning (MTL) generally being disregarded. The
MTL aims to use helpful information in several related tasks to improve the gen-
eralization performance of all tasks. Multi-task learning aims to enhance predic-
tions by exchanging group knowledge amongst related training data sets known as
“tasks”. Therefore, multi-task learning is a signiﬁcant area of research in machine
learning. The study of multi-task learning has been of interest in diverse ﬁelds, in-
cluding multi-level analysis [2], medical diagnosis [3], semi-supervised learning [1],
web search ranking [7], speech recognition [4], cell biology [19], person identiﬁca-
tion [24], drug interaction extraction [38], object tracking [8], etc.

Several MTL approaches have been proposed throughout the years, which can
be classiﬁed into several groups. SVM-based methods are among such approaches.
Because of the eﬀectiveness of support vector machine (SVM) [5] in multi-task
learning, several researchers have concentrated on multi-task SVM [11, 23, 35, 36].
Evgeniou et al. [9] developed a multi-task learning strategy based on the mini-
mization of regularization functions similar to those used in SVM.

As we know, Jeyadeva et al. [10] proposed twin support vector machine (TSVM)
for binary classiﬁcation in 2007, based on the main notion of GEPSVM [14]. TSVM
divides the positive and negative samples by producing two non-parallel hyper-
planes via solving two smaller quadratic programming problems (QPP) rather
than one large QPP considered in SVM. In contrast to the substantial research
conducted on multi-task support vector machines, there have been few eﬀorts
to incorporate multi-task learning into twin support vector machines (TSVM).
For instance, inspired by multi-task learning and TSVM, Xie and Sun proposed
a multi-task twin support vector machine [31]. They used twin support vector
machines for multi-task learning and referred to the resulting model as directed
multi-task twin support vector machine (DMTSVM). Following that, multi-task
centroid twin support vector machines (MCTSVM) [32] were suggested to cope
with outlier samples in each task. In addition, motivated by least-squares twin sup-
port vector machine (LS-TWSVM) [12], Mei and Xu [15] proposed a multi-task
least-squares twin support vector machine (MTLS-TWSVM). Instead of the two
QPP problems addressed by DMTSVM, MTLS-TWSVM solves only two smaller
linear equations, resulting in quick computation.

Universum data is deﬁned as a set of unlabeled samples that do not belong to
any class [6, 26, 29]. These data demonstrate the ability to encode past knowledge
by providing meaningful information in the same domain as the problem. The Uni-
versum data have eﬀectively improved learning performance in classiﬁcation and
clustering. By incorporating the Universum data into SVM, Vapnik [25] proposed
a novel model and referred to it as support vector machine with Universum (U-
SVM). Weston et al. [29] investigated this new framework (Universum data) and
proved that the use of these data outperformed approaches that just used labeled
samples.

Multi-task Twin support vector machine

3

Inspired by U-SVM, Sinz et al. [6] introduced least-squares support vector
machine with Universum data (ULS-SVM). Also, Zhang et al. [37] proposed semi-
supervised algorithms based on a graph for the learning of labeled samples, un-
labeled samples, and Universum data. Various studies conﬁrm the helpfulness of
Universum data for supervised and semi-supervised learning. The training proce-
dure incorporates Universum data, which increases the total number of samples
and adds substantial computing complexity. As a result, the classical U-SVM has
disadvantages such as high computational complexity due to facing a larger QPP.
Fortunately, in 2012, Qi et al. [18] proposed a twin support vector machine us-
ing Universum data (U-TSVM) that addressed this computational shortcoming.
Instead of solving one large QPP, which is done in the standard U-SVM algo-
rithm, this approach solves two smaller QPPs. The authors demonstrated that
the approach not only reduced the time of computation, but also outperformed
the traditional U-SVM in terms of classiﬁcation accuracy. Following that, Xu et
al. [33] developed least-squares twin support vector machine using Universum data
(ULS-TSVM) for classiﬁcation. They described the way two nonparallel hyper-
planes could be found by solving a pair of systems of linear equations. As a result,
ULS-TSVM works faster than U-TSVM. Inspired by ν-TSVM and U-TSVM, Xu
et al. [34] presented a ν-TSVM with Universum data (Uν -TSVM). It allows the
incorporation of the prior knowledge embedded in the unlabeled samples into
supervised learning to improve generalization performance. Xiao et al. [30] estab-
lished Universum learning in 2021 to make non-target task data behave as previous
knowledge, and suggested a novel multi-task support vector machine using Uni-
versum data (U-MTLSVM). In general, Universum models have been of interest
to many researchers because of their simple structures and good generalization
performance [16, 17, 20, 21].

Despite the work done in MTL, there is still a need to create more eﬃcient ap-
proaches in terms of accuracy and other ﬁeld measures. Inspired by DMTSVM and
Universum data, we present a signiﬁcant multi-task twin support vector machine
using Universum data (UMTSVM). This paper presents two approaches to the
solution of the proposed model. We obtain the dual formulation of UMTSVM, and
try to solve the quadratic programming problems at the ﬁrst approach. In addition,
we propose a least-squares version of the multi-task twin support vector machine
with Universum data (referred to as LS-UMTSVM) to further increase the gen-
eralization performance and reduce the time of computation. The LS-UMTSVM
only deals with two smaller linear equations instead of the two dual quadratic
programming problems which are used in UMTSVM.

The contributions of our research can be summarized as follows.

– Using Universum data, we present a new multi-task twin support vector ma-
chine model. This model naturally extends DMTSVM by adding Universum
data.

– The proposed model has the same advantages as DMTSVM, and furthermore,

improves its performance.

– We propose two approaches to ﬁnding the solution of the proposed model,
namely, solving the dual problem instead of the primal problem, and introduc-
ing the least-squares version of UMTSVM, which is called LS-UMTSVM, for
solving the primal problem.

4

Hossein Moosaei∗ et al.

The remainder of this paper is organized as follows. Following a quick review
of DMTSVM and MTLS-TWSVM in Section 2, we describe the details of our
proposed UMTSVM and introduce its dual problem in Section 3. In Section 4, we
present a new algorithm, namely, LS-UMTSVM. Next, we provide some numerical
experiments in Section 5. Finally, we summarize our ﬁndings in Section 6 within
a brief conclusion.

Notation. We use Rn for the n-dimensional real vector space and I for the iden-
tity matrix. The transpose and Euclidean norm of a matrix A are denoted by the
symbols AT and (cid:107) · (cid:107), respectively. The gradient of the function f with respect to
the variable x is denoted by ∇xf (x) or simply ∇f (x). Next, we use (cid:104)x, y(cid:105) = xT y
to denote the inner product of two n-dimensional vectors x and y. The symbol
blkdiag(P1, . . . , PT ) denotes the block-diagonal matrix created by P1, . . . , PT ma-
trices.

2 Related work

In this section, we ﬁrst give an overview of the multi-task problem. Then, we
introduce the direct multi-task twin support vector machine (DMTSVM) and
the multi-task least-squares twin support vector machine (MTLS-TWSVM). It
is preferable to deﬁne the fundamental foundations of these procedures since they
serve as a solid basis for our suggested method. For the multi-task problem, we
have T training task, and we assume the set St, for t = 1, . . . , T , stores the labeled
samples for the t-th task. It is given by

St = {(x1t, y1t), . . . , (xntt, yntt)},

where, nt is the number of samples in task t, xit ∈ Rn, yit ∈ {±1}, i = 1, . . . , nt.

2.1 Multi-task twin support vector machine

Xie et al. [31] introduced a new classiﬁcation method that directly incorporates
the regularized multi-task learning (RMTL) [9] concept into TSVM and called it
direct multi-task twin support vector machine (DMTSVM).

Suppose positive samples and positive samples in t-th task are presented by
Xp and Xpt, respectively, while Xn represents the negative samples and negative
samples in t-th task are presented by Xnt, that is, X T
p = [Xp1 Xp2 . . . XpT ]. Now,
for every task t ∈ {1, . . . , T } we deﬁne:

A = [Xp e1], At = [Xpt e1t], B = [Xn e2], Bt = [Xnt e2t],

where e1, e2, e1t and e2t are one vectors of appropriate dimensions. Assume that
all tasks have two mean hyperplanes in common, i.e., u0 = [w1, b1]T and v0 =
[w2, b2]T . The two hyperplanes in the t-th task for positive and negative classes
are (u0 + ut) = [w1t, b1t]T and (v0 + vt) = [w2t, b2t]T , respectively. The bias
between task t and the common mean vectors u0 and v0 is represented by ut and
vt, respectively.

Multi-task Twin support vector machine

The DMTSVM optimization problems are expressed below:

and

min
u0,ut,ξt

1
2

(cid:107)Au0(cid:107)2 +

µ1
2T

T
(cid:88)

t=1

(cid:107)Atut(cid:107)2 + c1

T
(cid:88)

t=1

eT
2tξt,

s.t. − Bt(u0 + ut) + ξt ≥ e2t,
ξt ≥ 0,

min
v0,vt,ηt

1
2

(cid:107)Bv0(cid:107)2 +

µ2
2T

T
(cid:88)

t=1

(cid:107)Btvt(cid:107)2 + c2

T
(cid:88)

t=1

eT
1tηt,

s.t. At(v0 + vt) + ηt ≥ e1t,
ηt ≥ 0.

5

(1)

(2)

In problems (1) and (2), t ∈ {1, . . . , T }, c1, c2, and e1t and e2t are one vectors of
appropriate dimensions. Next, µ1 and µ2 are positive parameters used for corre-
lation of all tasks. If µ1 and µ2 give small penalty on vectors ut and vt, then ut
and vt tend to be larger. As a consequence, the models give less similarity. When
µ1 → ∞ and µ2 → ∞, ut and vt tend to be smaller and make the T models
similar [9].

By deﬁning

−1

Q = B(AT A)
α = [αT

1 , . . . , αT

BT , Pt = Bt(AT
T ]T , P = blkdiag(P1, . . . , PT ),

t At)

BT
t ,

−1

the dual problem of the problem (1) may be expressed as follows:

1
2

−

αT (Q + T
µ1

max
α
s.t. 0 ≤ α ≤ c1e2.

P )α + eT

2 α

By resolving the aforementioned dual problem, we may discover:

u0 = −(AT A)−1BT α,

ut = −

T
µ1

(AT

t At)−1BT

t αt.

Similarly, we may derive the dual problem of the problem (2) as follows:

1
2

α∗T

−

(R + T
max
µ2
α∗
s.t. 0 ≤ α∗ ≤ c2e1,

S)α∗ + eT

1 α∗

(3)

(4)

1 , . . . , α∗T

T ]T , R = A(BT B)

where α∗ = [α∗T
t and
S = blkdiag(S1, . . . , ST ). By solving problem (3) and (4), we can set the hyper-
planes of every task (u0 + ut) and (v0 + vt). Meanwhile, a new data point x in
the t-th task is determined to class i ∈ {+1, −1} by using the following decision
function:

AT , and St = At(BT

t Bt)

AT

−1

−1

f (x) = arg min
k=1,2

|xT wkt + bkt|.

(5)

6

Hossein Moosaei∗ et al.

2.2 Multi-task least squares twin support vector machine

Inspired by DMTSVM and the least squares twin support vector machine (LST-
WSVM), Mei et al. [15] proposed a novel multi-task least squares twin support
vector machine and called MTLS-TWSVM.

The notation of Xp, Xpt, Xn, Xnt, A, At, B and Bt is the same as that used in

subsection (2.1). The MTLS-TWSVM problems are formulated as follows:

and

min
u0,ut,ξt

1
2

(cid:107)Au0(cid:107)2 +

µ1
2T

T
(cid:88)

t=1

(cid:107)Atut(cid:107)2 +

c1
2

T
(cid:88)

t=1

(cid:107)ξt(cid:107)2,

s.t. − Bt(u0 + ut) + ξt = e2t,

min
v0,vt,ηt

1
2

(cid:107)Bv0(cid:107)2 +

µ2
2T

T
(cid:88)

t=1

(cid:107)Btvt(cid:107)2 +

c2
2

T
(cid:88)

t=1

(cid:107)ηt(cid:107)2,

s.t. At(v0 + vt) + ηt = e1t,

(6)

(7)

where µ1, c1, µ2 and c2 are positive parameters. The Lagrangian function associ-
ated with the problem (6) is deﬁned by

L1 =

1
2

(cid:107)Au0(cid:107)2 +

−

µ1
2T

T
(cid:88)

t=1

T
(cid:88)

t=1

(cid:107) Atut (cid:107)2 +

c1
2

T
(cid:88)

t=1

(cid:107)ξt (cid:107)2

αT
t

(cid:0) − Bt(u0 + ut) + ξt − e2t

(cid:1),

(8)

T ]T are the Lagrangian multipliers. After writing the partial
where α = [αT
derivatives of Lagrangian function (8) with respect to u0, ut, ξt, and αt, we derive

1 , . . . , αT

(cid:18)

α =

Q +

T
µ1

P +

1
c1

I

(cid:19)−1

e2,

t At)−1BT
where Q = B(AT A)−1BT , Pt = Bt(AT
Then we can compute the solution of problem (6):

t , and P = blkdiag(P1, . . . , PT ).

u0 = −(AT A)−1BT α,

ut = −

T
µ1

(AT

t At)−1BT

t αt.

Similarly, the following relations may be used to ﬁnd the solution of (7):

α∗ =

(cid:18)

R +

T
µ2

S +

1
c2

I

(cid:19)−1

e1,

where R = A(BT B)−1AT , St = At(BT
1 , . . . , α∗T
α∗ = [α∗T
t-th task are determined.

t , S = blkdiag(S1, . . . , ST ) and
T ]T . As a result, the classiﬁer parameters u0, ut, v0 and vt of the

t Bt)−1AT

Multi-task Twin support vector machine

7

3 Multi-task twin support vector machine with Universum data

Motivated by U-TSVM and DMTSVM, we would like to introduce a new multi-
task model and name it as multi-task twin support vector machine with Universum
data (UMTSVM).

For a multi-task problem, we have (cid:101)T training sets, and suppose that the train-
ing set (cid:101)Tt for t = 1, . . . , T consists of two subsets and each task t contains nt
samples as follows

(cid:101)Tt = St ∪ XUt,

where

St = {(x1t, y1t), . . . , (xntt, yntt)},
XUt = {x∗

1t, . . . , x∗

utt},

with xit ∈ Rn, yi ∈ {±1} and i = 1, . . . , nt. Hence, the set St denotes the labeled
samples for the t-th task, and XUt contains the Universum data for the t-th task.
For every task, we expect to build the classiﬁer based on positive and negative
labeled samples as well as Universum data of this task.

All tasks have two mean hyperplanes u0 = [w1, b1]T and v0 = [w2, b2]T . The
two hyperplanes in the t-th task for positive and negative classes are (u0 + ut) =
[w1t, b1t]T and (v0 + vt) = [w2t, b2t]T , respectively. We employ the same notation
of Xp, Xpt, Xn, Xnt, A, At, B and Bt as we used in subsections 2.1 and 2.2. In
addition, XU denotes the Universum samples, and Universum samples in t-th task
is presented by matrix XUt. Then, for every task t ∈ {1, . . . , T }, we can deﬁne:

U = [XU eu] and Ut = [XUt eut],

where eu and eut are vector ones of appropriate dimensions.

3.1 Linear case

−δ[θ] + H t

In this part, we introduce the linear case of our new model (UMTSVM). Before
deﬁne the δ−insensitive loss function for Universum data, we deﬁne the hinge
loss function as Hδ[θ] = max{0, δ − θ}. We will deﬁne the δ−insensitive loss
U t[θ] = H t
−δ[−θ], t = 1, ..., T for Universum data in each task. This
loss measures the real-valued output of our classiﬁer fw1t,b1t (x) = wT
1tx + b1t
and fw2t,b2t (x) = wT
2tx + b2t on XU and penalizes outputs that are far from zero
[18]. We then wish to minimize the total losses (cid:80)T
jt)] and
t=1
(cid:80)T
jt)], and the classiﬁers have greater possibility when these
values are less, and vice versa [37]. Therefore by adding the following terms in the
objective functions of DMTSVM, we introduce our new model (UMTSVM):

j=1 U t[fw1t,b1t (x∗

j=1 U t[fw2t,b2t (x∗

(cid:80)ut

(cid:80)ut

t=1

cu

T
(cid:88)

ut(cid:88)

t=1

j=1

U t[fw1t,b1t (x∗

jt)], and c∗
u

T
(cid:88)

ut(cid:88)

t=1

j=1

U t[fw2t,b2t (x∗

jt)],

where cu and c∗
formulas of our proposed UMTSVM may be stated as follows:

u controls the loss of Universum data. Hence, the optimization

8

and

Hossein Moosaei∗ et al.

min
u0,ut,ξt,ψt

1
2

(cid:107)Au0(cid:107)2 +

µ1
2T

T
(cid:88)

t=1

(cid:107)Atut(cid:107)2 + c1

T
(cid:88)

t=1

eT
2tξt + cu

T
(cid:88)

t=1

eT
utψt

s.t. − Bt(u0 + ut) + ξt ≥ e2t,

Ut(u0 + ut) + ψt ≥ (−1 + ε)eut,
ξt ≥ 0, ψt ≥ 0,

min
v0,vt,ηt,ψ∗
t

1
2

(cid:107)Bv0(cid:107)2 +

µ2
2T

T
(cid:88)

t=1

(cid:107)Btvt(cid:107)2 + c2

T
(cid:88)

t=1

1tηt + c∗
eT
u

T
(cid:88)

t=1

utψ∗
eT
t

s.t. At(v0 + vt) + ηt ≥ e1t,
− Ut(v0 + vt) + ψ∗
ηt ≥ 0, ψ∗

t ≥ 0,

t ≥ (−1 + ε)eut,

(9)

(10)

1 , ..., ψ∗

u are penalty parameters. ξt, ηt, ψt = (ψ1, ..., ψut) and ψ∗

where c1, c2, cu and c∗
t =
(ψ∗
ut) are the corresponding slack vectors. T denotes the number of task
parameters, and µ1 and µ2 are the positive parameters, which controls preference
of the tasks.

The Lagrangian function associated with problem (9) is denoted by

L1 =

1
2

(cid:107)Au0(cid:107)2 +

µ1
2T

T
(cid:88)

t=1

(cid:107)Atut(cid:107)2 + c1

T
(cid:88)

t=1

eT
2tξt + cu

T
(cid:88)

t=1

eT
utψt −

T
(cid:88)

t=1

αT

1t(−Bt(u0 + ut) + ξt

− e2t) −

T
(cid:88)

t=1

βT
1tξt −

T
(cid:88)

t=1

αT

2t(Ut(u0 + ut) + ψt − (−1 + ε)eut) −

T
(cid:88)

t=1

βT
2tψt,

(11)

where α1t, α2t, β1t and β2t are the Lagrange multipliers. Thus, denoting α1 =
[αT
1T ]T and α2 = [αT
2T ]T , the KKT necessary and suﬃcient opti-
21, . . . , αT
mality conditions for (9) are given by

11, . . . , αT

∂L1
∂u0
∂L1
∂ut
∂L1
∂ξt
∂L1
∂ψt

= AT Au0 + BT α1 − UT α2 = 0,

=

µ1
T

AT

t Atut + BT

t α1t − UT

t α2t = 0,

= c1e2 − α1 − β1 = 0,

= cueu − α2 − β2 = 0.

(12)

(13)

(14)

(15)

Since β1 ≥ 0 and β2 ≥ 0, from (14) and (15), we have

0 ≤ α1 ≤ c1e2,

0 ≤ α2 ≤ cueu.

Multi-task Twin support vector machine

9

Also, from the equations (12) and (13), we have

u0 = −(AT A)−1(BT α1 − UT α2),

ut = −

T
µ1

(AT

t At)−1(BT

t α1t − UT

t α2t).

Then, substituting u0 and ut into (11) :

L1 =

1
2

(αT

1 B − αT

2 U)(AT A)−1(BT α1 − UT α2)

+

T
2µ1

T
(cid:88)

t=1

(αT

1tBt − αT

2tUt)(AT

t At)−1(BT

t α1t − UT

t α2t)

T
µ1

T
(cid:88)

t=1

1tBt(AT
αT

t At)−1(BT

t α1t − UT

t α2t)

T
(cid:88)

t=1

2tUt(AT
αT

t At)−1(BT α1 − UT α2)

T
(cid:88)

1tUt(AT
αT

t At)−1(BT

t α1t − UT

t α2t)

−

−

−

T
µ1

t=1
1
2

(αT

= −

1 B − αT

2 U)(AT A)−1(BT α1 − UT α2)

−

T
2µ1

T
(cid:88)

t=1

Deﬁning

(αT

1tBt − αT

2tUt)(AT

t At)−1(BT

t α1t − UT

t α2t).

Q =

Pt =

(cid:21)

(cid:20)B
U
(cid:20)Bt
Ut

(AT A)−1 (cid:2)BT UT (cid:3) ,

(cid:21)

(AT A)−1 (cid:2)BT

t UT
t

(cid:3) ,

P = blkdiag (P1, . . . , PT ),

the dual problem of (9) may be expressed as

max
α1,α2

−

1
2

(cid:104)
1 , αT
αT
2

(cid:105) (cid:18)

Q +

T
µ1

P

(cid:21)

(cid:19) (cid:20)α1
α2

(cid:104)
1 , αT
αT
2

+

(cid:105) (cid:20)

e2
(−1 + ε)eu

(cid:21)

s.t. 0 ≤ α1 ≤ c1e2,
0 ≤ α2 ≤ cueu.

Similarly, by introducing

R =

(cid:21)
(cid:20)A
U

(BT B)−1 (cid:2)AT UT (cid:3) , St =

S = blkdiag (S1, . . . , ST ), α∗

1 = (cid:2)(α∗

1T )T (cid:3)T , α∗

2 = (cid:2)(α∗

21)T , . . . , (α∗

2T )T (cid:3)T ,

t At)−1 (cid:2)AT

t UT
t

(cid:21)

(AT

(cid:20)At
Ut
11)T , . . . , (α∗

(16)

(cid:3) ,

10

Hossein Moosaei∗ et al.

Algorithm 1 A linear multi-task twin support vector machine with Universum
(UMTSVM)
Input:

– The training set ˜T and Universum data XU;
– Decide on the total number of tasks included in the data set and assign this
value to T;
– Select classiﬁcation task St (t = 1, . . . , T ) in training data set ˜T ;
– Divide Universum data XU by t-task and get XUt (t = 1, . . . , T );
– Choose appropriate parameters c1, c2, cu, c∗
The outputs:

u, µ1, µ2, and parameter ε ∈ (0, 1).

– u0, ut, v0, and vt.

The process:

1: Solve the optimization problems (16) and (17), and get α1, α2, α∗
2: Calculate u0, ut, v0, and vt.
3: By utilizing the decision function (5), assign a new point x in the t-th task to

1, and α∗
2.

class +1 or −1.

the dual problem of (10) can be obtained as

(cid:104)
1 , α∗T
α∗T
2

(cid:105) (cid:18)

R +

T
µ2

(cid:21)

(cid:19) (cid:20)α∗
1
α∗
2

S

(cid:104)
1 , α∗T
α∗T
2

+

(cid:105) (cid:20)

e1
(−1 + ε)eu

(cid:21)

1
2

−

max
α∗
1 ,α∗
2
s.t. 0 ≤ α∗
0 ≤ α∗

1 ≤ c2e1,
2 ≤ c∗
ueu.

(17)

By solving problems (16) and (17), we ﬁnd the α1, α2, α∗
2, and then the
classiﬁers parameters u0, ut, v0 and vt of the t-th task can be obtained. The label
of a new sample x ∈ Rn is determined by (5). A linear UMTSVM can be obtained
by the steps Algorithm 1.

1 and α∗

3.2 Nonlinear case

It is understandable that a linear classiﬁer would not be appropriate for training
data that are linearly inseparable. To deal with such issues, employ the kernel
technique. To that end, we introduce the kernel function K(·, ·) and deﬁne D =
(cid:3),
(cid:2)AT
B = [K(B, DT ), e2], Bt = (cid:2)K(Bt, DT ), e2t
(cid:3) and Ut =
(cid:2)K(XUt, DT ), eut
(cid:3). Hence, the nonlinear formulations for UMTSVM are deﬁned
as follow:

(cid:3), At = (cid:2)K(At, DT ), e1t

(cid:3), U = (cid:2)K(XU, DT ), eu

, A = (cid:2)K(A, DT ), e1

2 , . . . , AT

T , BT
T

2 , BT

1 , BT

1 , AT

(cid:3)T

min
u0,ut,ξt,ψt

1
2

(cid:107) ¯Au0(cid:107)2 +

µ1
2T

T
(cid:88)

t=1

(cid:107) ¯Atut(cid:107)2 + c1

T
(cid:88)

t=1

eT
2tξt + cu

T
(cid:88)

t=1

eT
utψt

s.t. − Bt(u0 + ut) + ξt ≥ e2t,

¯Ut(u0 + ut) + ψt ≥ (−1 + ε)eut,
ξt ≥ 0, ψt ≥ 0,

(18)

Multi-task Twin support vector machine

11

and

min
v0,vt,ηt,ψ∗
t

1
2

(cid:107) ¯Bv0(cid:107)2 +

µ2
2T

T
(cid:88)

t=1

(cid:107) ¯Btvt(cid:107)2 + c2

T
(cid:88)

t=1

1tηt + c∗
eT
u

T
(cid:88)

t=1

eT
utψt

s.t. ¯At(v0 + vt) + ηt ≥ e1t,
− Ut(v0 + vt) + ψ∗
ηt ≥ 0, ψ∗

t ≥ 0,

t ≥ (−1 + ε)eut,

(19)

u are penalty parameters, and ξt, ηt, ψt and ψ∗

here c1, c2, cu and c∗
t are the
corresponding slack vectors. By T we denote the number of task parameters, and
µ1 and µ2 are the positive parameters, which control preference of the tasks. After
using the Lagrange multipliers and KKT conditions, the duals of problems (18)
and (19) read as follows:

max
α1,α2

−

1
2

(cid:104)
1 , αT
αT
2

(cid:105) (cid:18)

Q +

T
µ1

P

(cid:21)

(cid:19) (cid:20)α1
α2

(cid:104)
1 , αT
αT
2

+

(cid:105) (cid:20)

e2
(−1 + ε)eu

(cid:21)

s.t. 0 ≤ α1 ≤ c1e2,
0 ≤ α2 ≤ cueu,

(cid:104)
1 , α∗T
α∗T
2

(cid:105) (cid:18)

R +

T
µ2

(cid:21)

(cid:19) (cid:20)α∗
1
α∗
2

S

(cid:104)
1 , α∗T
α∗T
2

+

(cid:105) (cid:20)

e1
(−1 + ε)eu

(cid:21)

1
2

−

max
α∗
1 ,α∗
2
s.t. 0 ≤ α∗
0 ≤ α∗

1 ≤ c2e1,
2 ≤ c∗
ueu,

(20)

(21)

and

where

Q =

(cid:21)

(cid:20) ¯B
¯U

( ¯AT ¯A)−1 (cid:2) ¯BT ¯UT (cid:3) , Pt =

(cid:21)

(cid:20) ¯Bt
¯Ut

( ¯AT
t

¯At)−1 (cid:2) ¯BT

t

¯UT
t

(cid:3) ,

P = blkdiag (P1, . . . , PT ), R =

(cid:21)
(cid:20) ¯A
¯U

( ¯BT ¯B)−1 (cid:2) ¯AT ¯UT (cid:3) ,

St =

(cid:21)

(cid:20) ¯At
¯Ut

( ¯AT
t

¯At)−1 (cid:2) ¯AT

t

¯UT
t

(cid:3) , S = blkdiag (S1, . . . , ST ).

A new data point x in the t-th task is determined to class i ∈ {+1, −1} by

using the following decision function:

f (x) = arg min
k=1,2

(cid:12)
(cid:12)K(cid:0)x, DT (cid:1)wkt + bkt

(cid:12)
(cid:12).

(22)

The nonlinear UMTSVM is described in the steps of Algorithm 2.

12

Hossein Moosaei∗ et al.

Algorithm 2 A nonlinear multi-task twin support vector machine with Universum
(UMTSVM)
Input:

– The training set ˜T and Universum data XU;
– Decide on the total number of tasks included in the data set and assign this
value to T;
– Select classiﬁcation task St (t = 1, . . . , T ) in training data set ˜T ;
– Divide Universum data XU by t-task and get XUt (t = 1, . . . , T );
– Choose appropriate parameters c1, c2, cu, c∗
– Select proper kernel function and kernel parameter.
The outputs:

u, µ1, µ2, and parameter ε ∈ (0, 1).

– u0, ut, v0, and vt.

The process:

1: Solve the optimization problems (20) and (21), and get α1, α2, α∗
2: Calculate u0, ut, v0, and vt.
3: Assign a new point x in the t-th task to class +1 or −1 by using decision

1, and α∗
2.

function (22).

4 Least squares multi-task twin support vector machine with
Universum data

In this section, we introduce the least-squares version of UMTSVM for the linear
and nonlinear cases, to which we refer as least-squares multi-task twin support
vector machine with Universum data (LS-UMTSVM). Our proposed method com-
bines the advantages of DMTSVM, ULS-TSVM, and MTLS-TWSVM. In terms of
generalization performance, the proposed method is superior to MTLS-TWSVM,
because it improves prediction accuracy by absorbing previously embedded knowl-
edge embedded in the Universum data. In terms of the time of computation, LS-
UMTSVM works faster than DMTSVM by solving two systems of linear equations
instead of two quadratic programming problems.

4.1 Linear case

We modify problems (9) and (10) in the least squares sense and replace the in-
equality constraint with equality requirements as follows

min
u0,ut,ξt,ψt

1
2

(cid:107)Au0(cid:107)2 +

µ1
2T

T
(cid:88)

t=1

(cid:107)Atut(cid:107)2 +

c1
2

T
(cid:88)

t=1

(cid:107)ξt(cid:107)2 +

cu
2

T
(cid:88)

t=1

(cid:107)ψt(cid:107)2

s.t. − Bt(u0 + ut) + ξt = e2t,

Ut(u0 + ut) + ψt = (−1 + ε)eut,

(23)

Multi-task Twin support vector machine

and

min
v0,vt,ηt,ψ∗
t

1
2

(cid:107)Bv0(cid:107)2 +

µ2
2T

T
(cid:88)

t=1

(cid:107)Btvt(cid:107)2 +

c2
2

T
(cid:88)

t=1

(cid:107)ηt(cid:107)2 +

c∗
u
2

T
(cid:88)

t=1

(cid:107)ψ∗

t (cid:107)2

s.t. At(v0 + vt) + ηt = e1t,
− Ut(v0 + vt) + ψ∗

t = (−1 + ε)eut.

13

(24)

Here, c1, c2, cu, and c∗
u are penalty parameters, ξt, ηt, ψt and ψ∗
t are slack variables
for t-th task and e1t, e2t, and eut are vectors of appropriate dimensions whose all
components are equal to 1.

It is worth noting that the loss functions in (23) and (24) are the square of the
2-norm of the slack variables ψ and ψ∗ rather than the 1-norm in problems (9)
and (10), which renders the constraints ψt ≥ 0 and ψ∗

t ≥ 0 superﬂuous.

The Lagrangian function for the problem (23) can be written as follows:

L1 =

1
2

(cid:107)Au0(cid:107)2 +

µ1
2T

T
(cid:88)

t=1

(cid:107)Atut(cid:107)2 +

c1
2

T
(cid:88)

t=1

(cid:107)ξt(cid:107)2

αT

t (−Bt(u0 + ut) + ξt − e2t) +

cu
2

T
(cid:88)

t=1

(cid:107)ψt(cid:107)2

βT
t (Ut(u0 + ut) + ψt − (−1 + ε)eut),

(25)

−

−

T
(cid:88)

t=1

T
(cid:88)

t=1

where αt and βt are the Lagrange multipliers. The Lagrangian function (25) is
diﬀerentiable and the KKT optimally conditions can be obtained as follows:

∂L
∂u0
∂L
∂ut
∂L
∂ξt
∂L
∂ψt
∂L1
∂αt
∂L1
∂βt

= AT Au0 + BT α − UT β = 0,

=

µ1
T

AT

t Atut + BT

t αt − UT

t βt = 0,

= c1ξt − αt = 0,

= cuψt − βt = 0,

= Bt(u0 + ut) − ξt + e2t = 0,

= −Ut(u0 + ut) − ψt + (−1 + ε)eut = 0.

From equations (26)–(29), we derive

u0 = −(AT A)−1(BT α − Uβ),

ut = −

(AT

t At)−1(BT

t αt − UT

t βt),

T
µ1

ξt =

ψt =

,

αt
c1
βt
cu

.

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

(34)

(35)

14

Hossein Moosaei∗ et al.

By substituting u0, ut, ξt and ψt into the equations (30) and (31), we have

(cid:20)

Bt

−(AT A)

−1

(BT α − UT β) −

(cid:20)
−(AT A)

−1

−Ut

(BT α − UT β) −

T
µ1

T
µ1

(AT

t At)

−1

(AT

t At)

−1

(BT

t αt − UT

t βt)

(cid:21)

−

(BT

t αt − UT

t βt)

(cid:21)

−

αt
c1

βt
cu

= −e2t,

(36)

= −(−1 + ε)eut,

(37)

where t ∈ {1, . . . , T }, α = [αT

1 , . . . , αT

T ]T and β = [βT

1 , . . . , βT

T ]T . Here, we deﬁne

−1

−1

BT , Q2 = B(AT A)
−1
t , P2t = Bt(AT

Q1 = B(AT A)
P1t = Bt(AT
t At)
P2 = blkdiag(P21, . . . , P2T ), R1t = Ut(AT
R1 = blkdiag(R11, . . . , R1T ), R2 = blkdiag(R21, . . . , R2T ).

UT , S1 = U(AT A)

t At)

t At)

BT

BT

UT

−1

t , P1 = blkdiag(P11, . . . , P1T ),
−1
UT
t ,

t , R2t = Ut(AT

t At)

−1

−1

BT , S2 = U(AT A)

−1

UT ,

Then (36) and (37) can be converted to the following equations:

(Q1α − Q2β) +

(P1α − P2β) +

1
c1

I1α = e2,

(−S1α + S2β) −

(R1α − R2β) +

1
cu

I2β = (−1 + ε) eu.

T
µ1
T
µ2

Combining equations (38) and(39), we obtain

(cid:20) Q1 −Q2
−S1 S2

(cid:21) (cid:20)α
β

(cid:21)

+

T
µ1

(cid:20) P1 −P2
−R1 R2

(cid:21) (cid:20)α
β

(cid:21)

+






I1

1
c1
0






0
1
cu

I2



α







 =



e2

β

(−1 + ε)eu

(38)

(39)



 ,

(40)

then, we can write

(cid:21)

(cid:20)α
β

=






(cid:21)

(cid:20) Q1 −Q2
−S1 S2

+

T
µ1

(cid:21)

(cid:20) P1 −P2
−R1 R2

+






I1

1
c1
0





−1







0
1
cu

I2

(cid:20)

e2
(−1 + ε)eu

(cid:21)

.

(41)

Similarly, the Lagrangian function for the problem (24) can be written as follows:

L2 =

1
2

(cid:107)Bv0(cid:107)2 +

µ2
2T

T
(cid:88)

t=1

(cid:107)Btvt(cid:107)2 +

c2
2

T
(cid:88)

t=1

(cid:107)ηt(cid:107)2

α∗T
t (At(v0 + vt) + ηt − e1t) +

c∗
u
2

T
(cid:88)

t=1

(cid:107)ψ∗

t (cid:107)2

β∗T
t (Ut(v0 + vt) + ψ∗

t − (−1 + ε)eut).

(42)

−

−

T
(cid:88)

t=1

T
(cid:88)

t=1

Multi-task Twin support vector machine

By performing a similar process, the following equations are obtained:

(Q∗

1α∗

1 − Q∗

2β∗) +

(cid:0)S∗

1 α∗ − S∗

2 β∗(cid:1) +

T
µ2
T
µ2

(cid:0)P ∗

1 α∗ − P ∗

2 β∗(cid:1) +

(R∗

1α∗ − R∗

2β∗) −

1
c2
1
c∗
u

I1α∗ = e1,

I2β∗ = −(−1 + ε)eu,

15

(43)

(44)

where

−1

−1

1 = A(BT B)
Q∗
S∗
2 = U(BT B)
1 = blkdiag(P ∗
P ∗
2t = Ut(BT
R∗

t Bt)

AT , Q∗

UT , P ∗
11, . . . , P ∗
−1
t , R∗
UT

2 = A(BT B)
1t = At(BT
1T ), P ∗

t Bt)

2 = blkdiag(P ∗

21, . . . , P ∗

1 = blkdiag(R∗

11, . . . , R∗

1T ), R∗

−1

UT , S∗

−1

AT

t , P ∗

1 = U(BT B)
2t = At(BT

−1

AT ,
−1

t Bt)
2T ), R∗

UT
t ,
1t = Ut(BT
2 = blkdiag(R∗

−1

AT
t Bt)
t ,
21, . . . , R∗

2T ).

Combining equations (43) and (44), we have

(cid:20)Q∗
1 −Q∗
2
1 −S∗
S∗
2

(cid:21) (cid:20)α∗
β∗

(cid:21)

+

T
µ2

(cid:20)P ∗
1 −P ∗
2
1 −R∗
R∗
2

(cid:21) (cid:20)α∗
β∗

(cid:21)

+






1
c2

I1

0 −






I2

0
1
c∗
u



α∗





 =





β∗

e1



 ,

−(−1 + ε)eu

(45)

then, we can write

(cid:21)

(cid:20)α∗
β∗

=






(cid:21)

(cid:20)Q∗
1 −Q∗
2
1 −S∗
S∗
2

+

T
µ2

(cid:21)

(cid:20)P ∗
1 −P ∗
2
1 −R∗
R∗
2

+






I1

1
c2
0





−1







0
1
c∗
u

I2

(cid:20)

e1
−(−1 + ε)eu

(cid:21)

.

(46)

Finally, by ﬁnding solutions (41) and (46), the classiﬁer parameters u0, ut, v0 and
vt are obtained. The decision function (5) can be used to assign a new data point
x ∈ Rn to its appropriate class. According to the discussion above, we illustrate
the LS-UMTSVM via Algorithm 3.

4.2 Nonlinear case

In the following, we introduce a nonlinear version of our proposed LS-UMTSVM
because there are situations that are not linearly separable, in which case the
kernel trick can be used. Therefore, we use the kernel function K(·, ·) and deﬁne

D =

A =

B =

2 , BT
(cid:105)

(cid:104)
AT

1 , BT

1 , AT

(cid:104)
K(A, DT ), e1
(cid:104)
K(B, DT ), e2

(cid:105)

U =

(cid:104)
K(XU, DT ), eu

(cid:105)T

,

, At =

T , BT
2 , . . . , AT
T
(cid:104)
K(At, DT ), e1t
(cid:104)
K(Bt, DT ), e2t
(cid:104)
K(XUt, DT ), eut

, Bt =
(cid:105)

, Ut =

(cid:105)

(cid:105)

,

,

(cid:105)

.

16

Hossein Moosaei∗ et al.

Algorithm 3 A linear least squares multi-task twin support vector machine with
Universum (LS-UMTSVM)
Input:

– The training set ˜T and Universum data XU;
– Decide on the total number of tasks included in the data set and assign this
value to T;
– Select classiﬁcation task St (t = 1, . . . , T ) in training data set ˜T ;
– Divide Universum data XU by t-task and get XUt (t = 1, . . . , T );
– Choose appropriate parameters c1, c2, cu, c∗
The outputs:

u, µ1, µ2, and parameter ε ∈ (0, 1).

– u0, ut, v0, and vt.

The process:

1: Solve the two small systems of linear equations (41) and (46), and get α, β, α∗,

and β∗.

2: Calculate u0, ut, v0, and vt.
3: By utilizing the decision function (5), assign a new point x in the t-th task to

class +1 or −1.

So, the nonlinear formulations of the optimization problems (23) and (24) can be
written as

min
u0,ut,ξt,ψt

1
2

(cid:107)Au0(cid:107)2 +

µ1
2T

T
(cid:88)

t=1

(cid:107)Atut(cid:107)2 +

c1
2

T
(cid:88)

t=1

(cid:107)ξt(cid:107)2 +

cu
2

T
(cid:88)

t=1

(cid:107)ψt(cid:107)2

s.t. − Bt(u0 + ut) + ξt = e2t,

Ut(u0 + ut) + ψt = (−1 + ε)eut,

and

min
v0,vt,ηt,ψ∗
t

1
2

(cid:107)Bv0(cid:107)2 +

µ2
2T

T
(cid:88)

t=1

(cid:107)Btvt(cid:107)2 +

c2
2

T
(cid:88)

t=1

(cid:107)ηt(cid:107)2 +

c∗
u
2

T
(cid:88)

t=1

(cid:107)ψ∗

t (cid:107)2

s.t. At(v0 + vt) + ηt = e1t,
− Ut(v0 + vt) + ψ∗

t = (−1 + ε)eut,

(47)

(48)

here, parameters c1, c2, cu, cu∗ , µ1, and µ2 are as deﬁned in section 4.1. In a
similar way to the linear case, we can written the Lagrangian function problems
(47) and (48) and KKT optimality conditions. After that, the optimal solutions
of problems (47) and (48) take the form

(cid:21)

(cid:20)α
β

=






(cid:21)

(cid:20) Q1 −Q2
−S1 S2

+

T
µ1

(cid:21)

(cid:20) P1 −P2
−R1 R2

+






I1

1
c1
0





−1







0
1
cu

I2

(cid:20)

e2
(−1 + ε)eu

(cid:21)

,

(49)

and

(cid:21)

(cid:20)α∗
β∗

=






(cid:21)

(cid:20)Q∗
1 −Q∗
2
1 −S∗
S∗
2

+

T
µ2

(cid:21)

(cid:20)P ∗
1 −P ∗
2
1 −R∗
R∗
2

+






I1

1
c2
0





−1







0
1
c∗
u

I2

(cid:20)

e1
−(−1 + ε)eu

(cid:21)

,

(50)

Multi-task Twin support vector machine

17

Algorithm 4 A nonlinear least squares multi-task twin support vector machine
with Universum (LS-UMTSVM)
Input:

– The training set ˜T and Universum data XU;
– Decide on the total number of tasks included in the data set and assign this
value to T;
– Select classiﬁcation task St (t = 1, . . . , T ) in training data set ˜T ;
– Divide Universum data XU by t-task and get XUt (t = 1, . . . , T );
– Choose appropriate parameters c1, c2, cu, c∗
– Select proper kernel function and kernel parameter.
The outputs:

u, µ1, µ2, and parameter ε ∈ (0, 1).

– u0, ut, v0, and vt.

The process:

1: Solve the two small systems of linear equations (41) and (46), and get α, β, α∗,

and β∗.

2: Calculate u0, ut, v0, and vt.
3: By utilizing the decision function (5), assign a new point x in the t-th task to

class +1 or −1.

where

T
Q1 = B(A

A)

B
−1

−1

T

T
, Q2 = B(A

−1

A)

T

U

T
, S1 = U(A

A)

−1

T

T
, S2 = U(A

B

A)

−1

T

U

,

B

T
t At)
P1t = Bt(A

T
T
t At)
t , P2t = Bt(A
T
P2 = blkdiag(P21, . . . , P2T ), R1t = Ut(A
t At)
R1 = blkdiag(R11, . . . , R1T ), R2 = blkdiag(R21, . . . , R2T ),

B

U

T
t , P1 = blkdiag(P11, . . . , P1T ),
−1

−1

T
T
t , R2t = Ut(A
t At)

T
t ,

U

−1

and

T

−1

B)

T

A

, Q∗

2 = A(B

T

B)

−1

T

, S∗

1 = U(B

T

B)

−1

A

T

U

, P ∗

T
t Bt)

A

T
t , P ∗

2t = At(B

U
−1

Q∗

1 = A(B
T

−1

S∗

B)

2 = U(B
1 = blkdiag(P ∗
P ∗
R∗

2t = Ut(B

T
t Bt)

1t = At(B
1T ), P ∗

11, . . . , P ∗
−1
T
t , R∗
U

2 = blkdiag(P ∗

21, . . . , P ∗

1 = blkdiag(R∗

11, . . . , R∗

1T ), R∗

T

,
−1

T
t ,

U

T
t Bt)
2T ), R∗

1t = Ut(B
2 = blkdiag(R∗

−1

A

T
t Bt)
21, . . . , R∗

T
t ,

2T ).

Then the corresponding decision function of the t-th task can be computed by (22).
Algorithm 4 describes the process of nonlinear case.

5 Numerical experiments

This section presents the results of experiments on various single-task learning al-
gorithms and multi-task learning algorithms. The single-task learning algorithms
considered consist of TBSVM [22], Iν-TBSVM [27] and ULS-TSVM [33], while the
multi-task learning methods are DMTSVM [31], MTLS-TWSVM [15], and our
proposed methods, i.e., UMTSVM and LS-UMTSVM. All numerical experiments

18

Hossein Moosaei∗ et al.

for both linear and nonlinear models were performed in Matlab R2018b on a PC
with 4 GB of RAM and Core(TM) i7 CPU @2.20 GHz under the Microsoft Win-
dows 64-bit operating system. Moreover, to determine the classiﬁcation accuracies
and performance of the algorithms, we used a ﬁve-fold strategy for cross-validation.
The following steps describe the cross-validation procedure.

– Partition the data sets randomly into ﬁve separate subsets of equal size.
– Apply the model to four of the subsets selected as the training data.
– Consider the one remaining subsets as the test data and evaluate the model

on it.

– Repeat the process until each of the ﬁve sets has been utilized as test data.

The accuracy is deﬁned as the number of accurate predictions divided by the total
number of forecasts. The value is then multiplied by 100 to give the percentage
accuracy. We randomly selected an equal amount of data from each class for the
benchmark and medical data sets, then used half of them to build the Universum
data by averaging pairs of samples from diﬀerent classes.

5.1 Parameters selection

It is obvious that the performance of classiﬁcation algorithms depends on the selec-
tion of proper parameters [13, 30]. Therefore, we will discuss selecting the optimal
parameters of the single-task and multi-task learning methods. This subsection
used ﬁve-fold cross-validation using the grid search approach to select parameters.
The 3D surface plots in Figure 1 demonstrate the inﬂuence of changing in the
various values of parameters c1, c2, cu, cu∗ , µ1 and µ2 on accuracy for proposed
LS-UMTSVM method on Caesarian data set in linear state. From Figure 1(a–b),
it can be inferred that the accuracy obtained is quite sensitive to the selected
parameters. Of course, the accuracy may be stable at some values. Therefore, the
choice of parameters depends on the distribution of points in a particular data set.

Fig. 1 The eﬀect of diﬀerent values of parameter on the Caesarian data sets

The performance of the particular algorithms is determined by the parameters
c1, c2, c3 and c4 in TBSVM; c1, c2, c3, c4 and ν1, ν2 in Iν-TBSVM; c1, c2, cu,
c∗
u and ε in ULS-TSVM; c1, c2, µ1 and µ2 in DMTSVM; c1, c2, µ1 and µ2 in

Multi-task Twin support vector machine

19

MTLS-TWSVM; c1, c2, cu, c∗
u, µ1, µ2 and ε in UMTSVM; and c1, c2, cu, c∗
u,
µ1, µ2 and ε in LS-UMTSVM. Therefore, the following ranges are considered for
selecting the optimal values of the parameters. In our experiments, parameters c1,
c2, c3, c4, cu, c∗
u, µ1 and µ2 are all selected from the set {2i | i = −10, . . . , 10}; ν1,
ν2 and ε are selected from the set {0.1, . . . , 0.9}.

In our experiments, due to the better performance for inseparably data sets, we
use the Gaussian kernel function (that is, K(x, y) = exp(−γ(cid:107)x − y(cid:107)2), γ > 0). We
choose a value for the kernel parameter γ from the range {2i | i = −10, . . . , 10}.

5.2 Benchmark data sets

In this subsection, we have compared multi-task learning algorithms on ﬁve bench-
mark data sets involving: Monk, Landmine, Isolet, Flags and Emotions. Table 1
shows details of data sets.

Table 1 The information of benchmark data sets.

Data set

# Samples # Features # Tasks

Monk
Landmine
Isolet
Flags
Emotions

432
9674
7797
194
593

6
9
617
19
72

3
4
5
7
6

The information of these data sets is as follows.

– Monk: In July 1991, the monks of Corsendonk Priory were confronted with
the 2nd European Summer School on Machine Learning, which was hosted at
their convent. After listening to various learning algorithms for more than a
week, they were confused: which algorithm would be the best? Which ones
should you stay away from? As a consequence of this quandary, they devised
the three MONK’s challenges as a basic goal to which all learning algorithms
should be compared.

– Landmine: This data collection was gathered from 29 landmine regions, each
of which corresponded to a separate landmine ﬁeld. The data set comprises
nine characteristics, and each sample is labeled with a 1 for a landmine or a 0
for a cluster, reﬂecting positive and negative classiﬁcations. The ﬁrst 15 areas
correlate too strongly with foliated regions, whereas the latter 14 belong to
bare ground or uninhabited places. We adopted the following procedures for
the Landmine data set to get better and more equitable experimental outcomes.
This data set has more negative labels than positive ones; as a result, we started
by removing some negative samples to balance things out. In our experiment,
we partition the data into four tasks. As a result, we picked four densely foliated
areas as a selection of positive data. We constructed an experimental data set
using four places from bare ground or desert regions as a negative data subset.
To produce the data set represented by Landmine in Table 2, we selected the
four sites 1, 2, 3, and 4 from foliated regions and identiﬁed the four areas 16,
17, 18, and 19 from bare earth regions.

20

Hossein Moosaei∗ et al.

– Isolet: Isolet is a widely used data set in speech recognition that is collected
as follows. One hundred ﬁfty peoples speak 26 letters of the English alphabet
twice. Therefore, 52 training samples are generated for each speaker. Each
speaker is classiﬁed into one of ﬁve categories. Consequently, we have ﬁve sets
of data sets that can be considered ﬁve tasks. On the one hand, these ﬁve
tasks are closely related because they are gathered from the same utterances.
On the other hand, the ﬁve tasks diﬀer because speakers in diﬀerent groups
pronounce the English letters diﬀerently. In this paper, we selected four pairs
of similar-sounding letters, including (O, U), (X, Y), (H, L) and (P, Q) for our
experiments.

– Flags: The ﬂags data set oﬀers information about the ﬂags of various countries.
It contains 194 samples and 19 features. This data set is divided into seven tasks
based on diﬀerent colors. Each task is represented by a 19-dimensional feature
vector derived from ﬂag images of several nations. Each sample may have a
maximum of seven labels, because the task of recognizing each label may be
seen as connected. Hence, we consider it as a multi-task learning problem. Thus,
this data set contains seven tasks. In Table 2, we compare the performance of
the aforementioned multi-task learning methods on this data set.

– Emotions: Emotion recognition from text is one of the most diﬃcult challenges
in Natural Language Processing. The reason for this is the lack of labeled data
sets and the problem’s multi-class character. Humans experience a wide range
of emotions, and it is diﬃcult to gather enough information for each feeling,
resulting in the issue of class imbalance. We have labeled data for emotion
recognition here, and the goal is to construct an eﬃcient model to identify the
emotion. The Emotions data collection comprises Twitter posts in English that
depict six primary emotions: anger, contempt, fear, joy, sorrow, and surprise.
All samples are labeled in six diﬀerent ways. Each sample may contain more
than one label (or emotion). Diﬀerent emotion recognition tasks have similar
characteristics and can be considered related tasks. So it may be viewed as a
multi-task classiﬁcation issue, with each task requiring the identiﬁcation of a
single kind of emotion. We use 50 samples from this data set to test various
multi-task learning algorithms in this experiment. The results of comparing the
performance of multi-task learning algorithms on this data set are reported in
Table 2.

As mentioned, we compare the performance of the proposed methods with DMTSVM
and MTLS-TWSVM in this subsection. Table 2 shows the average accuracies
(“Acc”), standard deviations (“Std”) and the running time (“Time”) on the ﬁve
popular multi-task data sets. As is seen in Table 2, the best performance is achieved
by the proposed LS-UMTSVM followed by UMTSVM. For example, on the Emo-
tions data set, the accuracies for DMTSVM and MTLS-TWSVM are 65.33%,
and 66%, respectively. In comparison, the UMTSVM and LS-UMTSVM method
achieved the accuracies 69.30%, and 75.73%, which performs better than the other
two multi-task learning algorithms. As a result, due to the nature of multi-task
learning, it is advantageous to combine data with Universum data throughout
the training phase. Furthermore, the results seem to match our intuition that
Universum data play an essential role in the performance of UMTSVM and LS-
UMTSVM. When UMTSVM and LS-UMTSVM compare with DMTSVM and
MTLS-TWSVM, we ﬁnd that our proposed algorithms indeed exploit Univer-

Multi-task Twin support vector machine

21

sum data to improve the prediction accuracy and stability. Therefore, UMTSVM
and LS-UMTSVM perform better than other multi-task learning algorithms, i.e.,
DMTSVM and MTLS-TWSVM.

In terms of learning speed, although our proposed methods are not the fastest
ones due incorporating Universum data, they oﬀer better accuracies at an accept-
able time.

Table 2 Performance comparison of nonlinear multi-task learning methods on benchmark
data sets.

Data set

Acc (%)±Std Acc (%) ±Std

DMTSVM

UMTSVM

Time (s)

Time (s)

MTLS-TWSVM LS-UMTSVM
Acc (%)±Std
Time (s)

Acc (%)±Std
Time (s)

Monk

Landmine

Isolet (O, U)

Isolet (X, Y)

Isolet (H, L)

Isolet (P, Q)

Flags

Emotions

92.76±0.02
27.82
92.33±0.01
40.69
99.60±0.00
11.49
99.50±0.07
11.60
98.16±0.02
11.55
96.83±0.04
12.24
55.29±0.25
3.49
65.33±0.19
2.25

99.61±0.00
36.99
93±0.11
42.21
99.67±0.01
15.86
98.83±0.01
15.28
100±0.04
15.37
96.33±0.05
15.26
57.48±0.07
3.88
69.30±0.21
2.60

94.80±0.02
25.46
94.05±0.05
36.69
99±0.02
10.21
99.60±0.00
10.39
99.83±0.01
10.27
97.83±0.03
10.32
57.13±0.26
2.52
66±0.23
1.15

99.80±0.02
30.21
94.50±0.00
39.88
99.77±0.01
11.54
100±0.00
11.99
100±0.00
11.89
100±0.00
11.88
59.57±0.28
3.12
75.73±0.18
2.30

5.3 Medical data sets

In this subsection of our experiments, we focus on comparing our proposed meth-
ods and several classiﬁer methods, including single-task and multi-task learning
methods in linear and nonlinear states. Therefore, we select four popular medi-
cal data sets to test these algorithms, including Immunotherapy, Ljubljana Breast
Cancer, Breast Cancer Coimbra, and Caesarian data sets. A summary of the data
sets information is provided in Table 3. The details of the data sets are as follows.

Table 3 The information of medical data sets.

Data set

# Samples # Features # Tasks

Immunotherapy
Ljubljana Breast Cancer
Breast Cancer Coimbra
Caesarian

90
286
116
80

8
9
9
5

3
5
3
2

22

Hossein Moosaei∗ et al.

– Immunotherapy: This data collection provides information regarding wart

treatment outcomes of 90 individuals utilizing Immunotherapy. The Immunother-
apy data set includes 90 instances, and each instance has eight features. The
features of this data include sex, age, type, number of warts, induration di-
ameter, area and the result of treatment. For this data set, we partition the
data into three tasks using the variable type: task 1 (type “1” = Common,
47 instances), task 2 (type “2” = Plantar, 22 instances), and task 3 (type “3”
= Both, 21 instances). Since the kind of wart within each job are varied, this
variable is also incorporated in our model.

– Ljubljana Breast Cancer: Nowadays, breast cancer is one of the most com-
mon malignancies in women, which has captured the attention of people all
around the globe. The illness is the leading cause of mortality in women aged
40 to 50, accounting for around one-ﬁfth of all fatalities in this age range. Ev-
ery year, more than 14,000 individuals die, and the number is increasing [28].
Thus, there remains a need to remove the cancer early to reduce recurrence.
Since recurrence within ﬁve years of diagnosis is correlated with the chance of
death, understanding and predicting recurrence susceptibility is critical. The
Ljubljana breast cancer data set provides 286 data points on the return of
breast cancer ﬁve years following surgical removal of a tumor. After deleting
nine instances where values are missing, we are left with 277. Each data point
has one class label (for recurrence or no-recurrence events) and nine attributes,
including age, menopausal status, tumor size, invasive nodes, node caps, degree
of malignancy, breast (left, right), breast quadrant (left-up, left-low, right-up,
right-low, central), and irradiation (yes, no). Here, we divide the data into ﬁve
tasks using the variable tumor size: task 1 (0 ≤tumor size≤19), task 2 (20 ≤
tumor size ≤24), task 3 (25 ≤ tumor size ≤29), task 4 (30 ≤ tumor size ≤34),
and task 5 (35 ≤tumor size ≤54).

– Breast Cancer Coimbra: The Breast Cancer Coimbra data set is the second
breast cancer data set we utilize for comparison. The Gynecology Department
of the Coimbra Hospital and University Center (CHUC) in Portugal collected
this data set between 2009 and 2013. The Breast Cancer Coimbra data set
contains 116 instances, each with nine features. This data set consists of 9
quantitative attributes and a class label attribute indicating if the clinical re-
sult is positive for existing cancer or negative (patient or healthy). Clinical
characteristics were observed or assessed in 64 patients with breast cancer
and 52 healthy controls. Age, BMI, insulin, glucose, HOMA, leptin, resistin,
adiponectin, and MCP-1 are all quantitative characteristics. The features are
anthropometric data and measurements acquired during standard blood anal-
ysis. The qualities have the potential to be employed as a biomarker for breast
cancer. In this experiment, we partition the data set into three tasks using the
feature BMI. Based on tissue mass (muscle, fat, and bone) and height, the BMI
is a simple rule of thumb to classify a person as underweight, normal weight,
overweight, or obese. Underweight (less than 18.5 kg/m2), normal weight (18.5
kg/m2 to 24.9 kg/m2), overweight (25 kg/m2 to 29.9 kg/m2), and obese (30
kg/m2 or more) are the four major adult BMI categories. Hence, we consider
that the ﬁrst task is underweight people, the second task is normal-weight
people, and the third task is overweight and obese.

– Caesarian: This data set, which aims to deliver via cesarean section or natural
birth, provides information on 80 pregnant women who have had the most

Multi-task Twin support vector machine

23

extreme delivery complications in the medical ﬁeld. The Caesarian data set
includes 80 instances, and each instance has ﬁve features. The features of this
data include age, delivery number, blood pressure, delivery time, and heart
problem. The heart problem feature in Caesarian data sets has two forms. We
separate the data into two tasks using the variable a heart problem; task 1:
The patient has a heart problem, and task 2: The patient does not have a heart
problem.

To analyze the performance of our proposed methods, we used medical data sets
and compared our proposed algorithms to ﬁve (multi-task and single-task learn-
ing) algorithms, i.e., TBSVM, Iν-TBSVM, ULS-TSVM, DMTSVM, and MTLS-
TWSVM. We can see from the results of Tables 4 and 5 that our algorithms
outperform all algorithms in linear and nonlinear states. This occurs because pro-
posed methods add Universum data to the model learning process to modify the
classiﬁcation decision boundaries. The proposed methods train all tasks simultane-
ously, and they can take advantage of the underlying information among all tasks
and improve their performance.

Table 4 Performance comparison of linear single-task and multi-task learning methods on
medical data sets.

Methods

TBSVM

Iν-TBSVM

ULS -TSVM

DMTSVM

UMTSVM

MTLS-TWSVM

LS-UMTSVM

Immunotherapy
Acc (%)±Std
Time (s)

77.81±0.09
1.41
79.97±0.15
1.63
78.92±0.10
0.04
81.29±0.06
1.49
84.63±0.07
1.53
86.11±0.08
0.20
88.11±0.11
0.25

Ljubljana Breast Cancer Breast Cancer Coimbra

Acc (%) ±Std
Time (s)

75.11 ±0.06
1.49
73.30±0.03
3.22
74.72±0.08
0.04
71.14±0.09
1.60
73.26±0.08
1.64
75.09±0.19
0.25
75.41±0.00
0.33

Acc (%)±Std
Time (s)

72.36±0.02
1.40
73.26±0.15
1.76
79.31±0.09
0.45
75.43±0.20
1.50
80.24±0.13
1.55
83.39±0.11
0.19
85.37±0.12
0.25

Caesarian
Acc (%)±Std
Time (s)

69.09±0.03
1.42
65.01±0.06
1.58
71.59±0.11
0.04
63.86±0.12
1.48
71.67±0.13
1.48
76.95±0.11
0.16
78.52±0.12
0.20

6 Conclusion

In this paper, we introduced the twin support vector machine in the framework
of multi-task learning with Universum data sets and proposed a new model called
UMTSVM. In addition, we suggested two approaches to solving our novel model.
As the ﬁrst approach, we solved the UMTSVM by the dual problem, a quadratic
programming problem. Also, we suggested the least-squares version of UMTSVM
and called it LS-UMTSVM. The LS-UMTSVM only dealt with two systems of
linear equations. Hence, comprehensive experiments on several popular multi-task
data sets and medical data sets demonstrated the eﬀectiveness of our proposed
methods in terms of classiﬁcation performance. The experiments conﬁrmed that
our algorithms achieved better experimental results compared to three single-task
learning algorithms and two multi-task learning algorithms.

24

Hossein Moosaei∗ et al.

Table 5 Performance comparison of nonlinear single-task and multi-task learning methods
on medical data sets.

Methods

TBSVM

Iν-TBSVM

ULS -TSVM

DMTSVM

UMTSVM

MTLS-TWSVM

LS-UMTSVM

Immunotherapy
Acc (%)±Std
Time (s)

78.88±0.02
1.50
78.92±0.01
1.43
78.92±0.02
0.06
80.25±0.05
1.58
80.25±0.05
1.53
80.22±0.25
0.25
80.56±0.05
0.38

Acknowledgments

Ljubljana Breast Cancer Breast Cancer Coimbra

Acc (%) ±Std
Time (s)

72.54±0.05
1.49
72.90±0.03
1.59
74.74±0.37
0.11
73.71±0.09
2.14
75.62±0.11
2.15
75.32±0.71
0.71
77.15±0.09
1.55

Acc (%)±Std
Time (s)

60.39±0.07
1.46
60.87±0.17
1.45
63.84±0.06
0.07
63.74±0.09
1.67
65.71±0.19
1.69
65.33±0.11
0.30
67.17±0.14
0.46

Caesarian
Acc (%)±Std
Time (s)

71.35±0.04
1.47
71.14±0.14
1.46
72.28±0.02
0.07
72.56±0.13
1.56
74.14±0.1
1.57
73.15±0.09
0.21
76.74±0.11
0.27

H. Moosaei and M Hlad´ık were supported by the Czech Science Foundation Grant
P403-22-11117S. In addition, the work of H. Moosaei was supported by the Center
for Foundations of Modern Computer Science (Charles Univ. project UNCE/S-
CI/004).

Conﬂict of interest

The authors state that they do not have any conﬂicts of interest.

References

1. Ando, R.K., Zhang, T., Bartlett, P.: A framework for learning predictive structures from
multiple tasks and unlabeled data. Journal of Machine Learning Research 6(11) (2005)
2. Bakker, B., Heskes, T.: Task clustering and gating for bayesian multitask learning. Journal

of Machine Learning Research 4 (2003)

3. Bi, J., Xiong, T., Yu, S., Dundar, M., Rao, R.B.: An improved multi-task learning ap-
proach with applications in medical diagnosis. In: Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pp. 117–132. Springer (2008)

4. Birlutiu, A., Groot, P., Heskes, T.: Multi-task preference learning with an application to

hearing aid personalization. Neurocomputing 73(7-9), 1177–1185 (2010)

5. Burges, C.J.C.: A tutorial on support vector machines for pattern recognition. Data Mining

and Knowledge Discovery 2(2), 121–167 (1998)

6. Chapelle, O., Agarwal, A., Sinz, F., Sch¨olkopf, B.: An analysis of inference with the uni-

versum. Advances in Neural Information Processing Systems 20, 1369–1376 (2007)

7. Chapelle, O., Shivaswamy, P., Vadrevu, S., Weinberger, K., Zhang, Y., Tseng, B.: Multi-
task learning for boosting with application to web search ranking. In: Proceedings of the
16th ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 1189–1198 (2010)

8. Cheng, X., Li, N., Zhou, T., Wu, Z., Zhou, L.: Multi-task object tracking with feature selec-
tion. IEICE Transactions on Fundamentals of Electronics, Communications and Computer
Sciences 98(6), 1351–1354 (2015)

9. Evgeniou, T., Pontil, M.: Regularized multi–task learning. In: Proceedings of the tenth
ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
109–117 (2004)

Multi-task Twin support vector machine

25

10. Jayadeva, Khemchandani, R., Chandra, S.: Twin support vector machines for pattern
IEEE Transactions on Pattern Analysis and Machine Intelligence 29(5),

classiﬁcation.
905–910 (2007)

11. Ji, Y., Sun, S.: Multitask multiclass support vector machines: model and experiments.

Pattern Recognition 46(3), 914–924 (2013)

12. Kumar, M.A., Gopal, M.: Least squares twin support vector machines for pattern classi-

ﬁcation. Expert Systems with Applications 36(4), 7535–7543 (2009)

13. Lu, L., Lin, Q., Pei, H., Zhong, P.: The aLS-SVM based multi-task learning classiﬁers.

Applied Intelligence 48(8), 2393–2407 (2018)

14. Mangasarian, O.L., Wild, E.W.: Multisurface proximal support vector machine classiﬁ-
cation via generalized eigenvalues. IEEE Transactions on Pattern Analysis and Machine
Intelligence 28(1), 69–74 (2006)

15. Mei, B., Xu, Y.: Multi-task least squares twin support vector machine for classiﬁcation.

Neurocomputing 338, 26–33 (2019)

16. Moosaei, H., Bazikar, F., Ketabchi, S., Hlad´ık, M.: Universum parametric-margin ν-
support vector machine for classiﬁcation using the diﬀerence of convex functions algorithm.
Appl. Intell. 52(3), 2634–2654 (2022)

17. Moosaei, H., Mousavi, A., Hlad´ık, M., Gao, Z.: Sparse universum quadratic surface support

vector machine models for binary classiﬁcation. arXiv preprint arXiv:2104.01331 (2021)

18. Qi, Z., Tian, Y., Shi, Y.: Twin support vector machine with universum data. Neural

Networks 36, 112–119 (2012)

19. Ren, Y., Xu, B., Zhu, P., Lu, M., Jiang, D.: A multicell visual tracking algorithm using
multi-task particle swarm optimization for low-contrast image sequences. Applied Intelli-
gence 45(4), 1129–1147 (2016)

20. Richhariya, B., Sharma, A., Tanveer, M.: Improved universum twin support vector ma-
chine. In: S. Sundaram (ed.) 2018 IEEE Symposium Series on Computational Intelligence
(IEEE SSCI 2018), pp. 2045–2052. IEEE (2018)

21. Richhariya, B., Tanveer, M.: EEG signal classiﬁcation using universum support vector

machine. Expert Systems with Applications 106, 169–182 (2018)

22. Shao, Y.H., Zhang, C.H., Wang, X.B., Deng, N.Y.: Improvements on twin support vector

machines. IEEE Transactions on Neural Networks 22(6), 962–968 (2011)

23. Shiao, H.T., Cherkassky, V.: Implementation and comparison of SVM-based multi-task
In: The 2012 International Joint Conference on Neural Networks

learning methods.
(IJCNN), pp. 1–7. IEEE (2012)

24. Su, C., Yang, F., Zhang, S., Tian, Q., Davis, L.S., Gao, W.: Multi-task learning with low
rank attribute embedding for multi-camera person re-identiﬁcation. IEEE Transactions
on Pattern Analysis and Machine Intelligence 40(5), 1167–1181 (2017)

25. Vapnik, V.: Estimation of Dependences Based on Empirical Data. Springer, New York

(2006)

26. Vapnik, V.: Transductive inference and semi-supervised learning.

In: O. Chapelle,

B. Sch¨olkopf, A. Zien (eds.) Semi-Supervised Learning, pp. 453–472. MIT Press (2006)
27. Wang, H., Zhou, Z., Xu, Y.: An improved ν-twin bounded support vector machine. Applied

Intelligence 48(4), 1041–1053 (2018)

28. Wang, L.: Data science for characterizing breast cancer. In: 2021 3rd International Con-

ference on Intelligent Medicine and Image Processing, pp. 122–126 (2021)

29. Weston, J., Collobert, R., Sinz, F., Bottou, L., Vapnik, V.: Inference with the universum.
In: Proceedings of the 23rd International Conference on Machine Learning, pp. 1009–1016
(2006)

30. Xiao, Y., Wen, J., Liu, B.: A new multi-task learning method with universum data. Applied

Intelligence 51(6), 3421–3434 (2021)

31. Xie, X., Sun, S.: Multitask twin support vector machines. In: International Conference on

Neural Information Processing, pp. 341–348. Springer (2012)

32. Xie, X., Sun, S.: Multitask centroid twin support vector machines. Neurocomputing 149,

1085–1091 (2015)

33. Xu, Y., Chen, M., Li, G.: Least squares twin support vector machine with universum data
for classiﬁcation. International Journal of Systems Science 47(15), 3637–3645 (2016)
34. Xu, Y., Chen, M., Yang, Z., Li, G.: ν-twin support vector machine with universum data

for classiﬁcation. Applied Intelligence 44(4), 956–968 (2016)

35. Xue, Y., Beauseroy, P.: Multi-task learning for one-class SVM with additional new features.
In: 2016 23rd International Conference on Pattern Recognition (ICPR), pp. 1571–1576.
IEEE (2016)

26

Hossein Moosaei∗ et al.

36. Yang, H., King, I., Lyu, M.R.: Multi-task learning for one-class classiﬁcation. In: The 2010
International Joint Conference on Neural Networks (IJCNN), pp. 1–8. IEEE (2010)
37. Zhang, D., Wang, J., Wang, F., Zhang, C.: Semi-supervised classiﬁcation with universum.
In: Proceedings of the 2008 SIAM International Conference on Data Mining, pp. 323–333.
SIAM (2008)

38. Zhou, D., Miao, L., He, Y.: Position-aware deep multi-task learning for drug–drug inter-

action extraction. Artiﬁcial Intelligence in Medicine 87, 1–8 (2018)

