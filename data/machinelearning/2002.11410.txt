0
2
0
2

b
e
F
6
2

]

C
O
.
h
t
a
m

[

1
v
0
1
4
1
1
.
2
0
0
2
:
v
i
X
r
a

Eﬃcient algorithms for multivariate shape-constrained convex
regression problems ∗

Meixia Lin†, Defeng Sun‡, Kim-Chuan Toh§

January 07, 2020

Abstract

Shape-constrained convex regression problem deals with ﬁtting a convex function to the observed data,
where additional constraints are imposed, such as component-wise monotonicity and uniform Lipschitz
continuity. This paper provides a comprehensive mechanism for computing the least squares estimator
of a multivariate shape-constrained convex regression function in Rd. We prove that the least squares
estimator is computable via solving a constrained convex quadratic programming (QP) problem with
(n + 1)d variables and at least n(n − 1) linear inequality constraints, where n is the number of data
points. For solving the generally very large-scale convex QP, we design two eﬃcient algorithms, one is
the symmetric Gauss-Seidel based alternating direction method of multipliers (sGS-ADMM), and the other
is the proximal augmented Lagrangian method (pALM) with the subproblems solved by the semismooth
Newton method (SSN). Comprehensive numerical experiments, including those in the pricing of basket
options and estimation of production functions in economics, demonstrate that both of our proposed
algorithms outperform the state-of-the-art algorithm. The pALM is more eﬃcient than the sGS-ADMM but
the latter has the advantage of being simpler to implement.

Keywords: convex regression, shape constraints, preconditioned proximal point algorithm, symmetric
Gauss-Seidel based ADMM

AMS subject classiﬁcation: 90C06, 90C25, 90C90

1

Introduction

Convex (or concave) regression is meant to estimate a convex (or concave) function based on a ﬁnite number
of observations. It is a topic of interest in many ﬁelds such as economics, operations research and ﬁnancial
engineering. In economics, production functions [2, 14, 34], demand functions [33] and utility functions [23]
are often required to be concave. In operations research, the performance measure expectations can be proved
to be convex in the underlying model parameters, e.g. in the context of queueing network [7]. In ﬁnancial
engineering, the option pricing function has the convexity restriction under the no-arbitrage condition, as
can be seen from [1].

In the literature, there exists various methods for solving the convex regression problem. With the
speciﬁcation of a functional form, one can apply a parametric approach to estimate the convex function. For
example, the Cobb-Douglas production function is a particular functional form of the production function

∗Funding: Defeng Sun is supported in part by Hong Kong Research Grant Council grantPolyU153014/18p and Kim-Chuan

Toh by the Academic Research Fund (grant R-146-000-257-112) of the Ministry of Education, Singapore.

†Department

of Mathematics, National University of Singapore,

10 Lower Kent Ridge Road,

Singapore

(lin meixia@u.nus.edu).

‡Department

of Applied Mathematics, The Hong Kong Polytechnic University, Hung Hom, Hong Kong

(defeng.sun@polyu.edu.hk).

§Department of Mathematics and Institute of Operations Research and Analytics, National University of Singapore, 10

Lower Kent Ridge Road, Singapore (mattohkc@nus.edu.sg).

1

 
 
 
 
 
 
that is widely used in applied production economics. To avoid strong prior assumptions on the functional
form, one can also use a non-parametric approach to perform the function estimation. Generally, the non-
parametric estimation is based on a given collection of primitive functions, such as local polynomial [21],
trigonometric series, spline estimator [9, 27] and kernel-type estimator [3]. However, such an approach may
face some diﬃculties such as imposing the convexity constraint and choosing appropriate smoothing param-
eters (e.g. the degree of the polynomial, or the kernel density bandwidth). To overcome these diﬃculties,
we use the least squares estimator for the convex regression. The least squares estimator is ﬁrst proposed
in [14], and its theoretical properties are carefully studied in [13, 20, 32].

Suppose that we observe n data points {(Xi, Yi)}n

i=1, which satisfy the regression model Y = ψ(X) + ε
with an unknown convex function ψ : Rd → R. The least squares estimation method is to estimate the
function ψ by minimizing the sum of squares error (cid:80)n
i=1(ψ(Xi) − Yi)2 over the set of convex functions from
Rd to R. This inﬁnite dimensional model appears to be intractable. Fortunately, the authors in [15, 32]
have provided a computationally tractable optimal solution to it. They showed that in the convex regression
problem, the family of convex functions can be characterized by a subset of continuous, piecewise linear
functions θi + (cid:104)ξi, X − Xi(cid:105), i = 1, . . . , n, whose intercepts θi’s and gradient vectors ξi’s are restricted to
satisfy the convexity conditions. The resulting problem is a convex quadratic programming (QP) problem
with (n + 1)d variables and n(n − 1) linear inequality constraints, which can be solved by interior point
solvers such as those implemented in MOSEK when n is not too large, as stated in [32]. However, interior
point solvers may easily run out of memory when n is large due to the presence of a large number of at least
n(n − 1) linear inequality constraints. Aybat et al. [3] proposed a parallel proximal gradient method (PAPG)
to solve the dual of an approximation of the QP by adding a ridge regularization on the ξi’s. The PAPG
method however is not fast enough for solving large problems. For example, it needs 17 minutes to solve
a problem with d = 80, n = 1600 on a 16-core machine sharing 32 GB. Mazumder et al. [22] proposed a
three-block alternating direction method of multipliers (ADMM) for solving the QP, but it has no convergence
guarantee. The computational challenge of handling large-scale cases still remains in need of more progress,
especially for the case when d and n are relatively large, for which existing methods are too expensive even
for computing a solution with low accuracy.

In many real applications, one may need to impose more shape constraints on the convex function
ψ, such as component-wise monotonicity and uniform Lipschitz continuity. For example, the option pricing
function under the no-arbitrage condition needs to be non-decreasing as well as convex as described in [1]. In
addition, when dealing with the Lipschitz convex regression as in [4, 19, 22], the uniform Lipschitz property
of the convex function is added when performing the estimation. To deal with these cases, we minimize
the sum of squares error (cid:80)n
i=1(ψ(Xi) − Yi)2 over the set of convex functions satisfying additional shape
constraints. The addition of the shape constraints obviously would make the QP even more complicated and
diﬃcult to solve.

In this paper, we provide a comprehensive mechanism for computing the least squares estimator for
the shape-constrained convex regression problem. We ﬁrst prove that the minimal sum of squares error can
be achieved via a set of piecewise linear functions whose intercept and gradient vectors are constrained to
satisfy the convexity conditions and required shape constraints (see Theorem 1). This conclusion leads us
to a constrained QP with (n + 1)d variables, n(n − 1) linear inequality constraints and n probably non-
polyhedral inequality constraints. Note that the estimator obtained in this way is nonsmooth, one can apply
the Moreau proximal smoothing technique to obtain a smoothing approximation. In addition, we can use
a generalized form of the proposed constrained QP model as well as a data-driven Lipschitz estimation
method to handle the boundary eﬀect of the least squares estimator for convex functions. The main task
in this mechanism is to solve the constrained QP in a robust and eﬃcient manner. Most existing methods
for the QP in the standard convex regression are either not extendable or diﬃcult to be modiﬁed to solve
the constrained QP due to the additional shape constraints. For the multivariate shape-constrained convex
regression problem, even with only a moderate number of observations, say n = 1000, the memory cost
and computational cost are already massive since the underlying QP has about a million constraints. To
tackle the potentially very large-scale QPs, we design two algorithms that can fully exploit the underlying
structures of the convex QPs of interest. The ﬁrst algorithm is the symmetric Gauss-Seidel based alternating

2

direction method of multipliers (sGS-ADMM), which has been demonstrated to perform better than the possibly
nonconvergent directly extended multi-block ADMM [8]. The sGS-ADMM algorithm is easy to implement, but
it is still just a ﬁrst-order method, which may not be eﬃcient enough to solve a problem to high accuracy.
We also design a proximal augmented Lagrangian method (pALM) for solving the constrained QP, which is
proved to be superlinearly convergent. For the pALM subproblems, we solve them by the semismooth Newton
method (SSN), which is proved to have quadratic convergence. Moreover, we fully uncover and exploit
the second order sparsity structure of the problem to highly reduce the computational cost of solving the
Newton systems. Comprehensive numerical experiments, including those in the pricing of basket options
and estimation of production functions, demonstrate that both of our proposed algorithms outperform the
state-of-the-art algorithm.

2 Model with shape constraints

i=1, where the predictors Xi ∈ Rd and the responses Yi ∈ R, we
Given independent observations {(Xi, Yi)}n
aim to ﬁt a convex regression model of the form Y = ψ(X) + ε. In the model, ψ : Ω → R is an unknown
convex function, Ω ⊂ Rd is a δ-neighborhood of conv(X1, · · · , Xn) (the convex hull of {Xi}n
i=1), ε is a random
variable with expectation E[ε|X] = 0. The least squares estimator ˆψ of ψ is deﬁned as

ˆψ ∈ arg min

ψ∈C

n
(cid:88)

i=1

(ψ(Xi) − Yi)2,

C = {ψ : Ω → R | ψ is a convex function}.

(1)

The authors in [15,32] provided a computationally tractable optimal solution to the above inﬁnite dimensional
model. Speciﬁcally, once an optimal solution {(ˆθi, ˆξi)}n
(θi − Yi)2 (cid:12)
(cid:12)
(cid:12) θi ≥ θj + (cid:104)ξj, Xi − Xj(cid:105), 1 ≤ i, j ≤ n

i=1 to the following ﬁnite dimensional problem

n
(cid:88)

min
θ1,...,θn∈R;ξ1,...,ξn∈Rd

(cid:110) 1
2

(cid:111)

i=1

has been computed, one can construct an optimal solution ˆψ to (1) by taking

ˆψ(x) = max
1≤j≤n

(cid:110)ˆθj + (cid:104) ˆξj, x − Xj(cid:105)

(cid:111)
,

x ∈ Ω.

(2)

For the shape-constrained convex regression problem, the least squares estimator ˆψ is deﬁned as

ˆψ ∈ arg min

n
(cid:88)

ψ∈CS

i=1

(ψ(Xi) − Yi)2,

CS = {ψ : Ω → R | ψ is a convex function with Property S},

(3)

where Property S speciﬁes the shape constraint of ψ. We restrict ourselves to the case that Property S takes
one of the following forms:

(S1) (monotone constraint) ψ is non-decreasing in some of the coordinates (denoted as K1) and non-
increasing in some others (denoted as K2), where K1 and K2 are disjoint subsets of {1, · · · , d};
(S2) (box constraint) the elements in ∂ψ(x) for any x ∈ Ω are bounded by two given vectors L, U ∈ Rd;

(S3) (Lipschitz constraint) ψ is Lipschitz, i.e., |ψ(x) − ψ(y)| ≤ L(cid:107)x − y(cid:107)p for any x, y ∈ Ω, where p = 1, 2, ∞,

and L is a given positive constant.

In the remaining part of this paper, we provide the mechanism for estimating
Structure of the paper.
the multivariate shape-constrained convex function in Section 3. For solving the involved constrained QP,
the sGS-ADMM algorithm is presented in Section 4 and the pALM algorithm is described in Section 5. The
implementation details of the proposed algorithms can be found in Section 6. Section 7 provides the numerical
comparison among MOSEK, sGS-ADMM and pALM, which demonstrates the robustness and eﬃciency of pALM.
Then we apply our mechanism to perform the function estimation in several interesting real applications in
Section 8. Finally, we conclude the paper and discuss some future work.

3

Notation. Denote X = (X1, · · · , Xn) ∈ Rd×n and en = (1, · · · , 1)T ∈ Rn. For any matrix Z ∈ Rm×n,
Zi denotes the i-th column of Z. We use “Diag(z)” to denote the diagonal matrix whose diagonal is
given by the vector z. For any positive semideﬁnite matrix H ∈ Rn×n, we deﬁne (cid:104)x, x(cid:48)(cid:105)H := (cid:104)x, Hx(cid:48)(cid:105),
and (cid:107)x(cid:107)H := (cid:112)(cid:104)x, x(cid:105)H for all x, x(cid:48) ∈ Rn. For a given closed subset C of Rn and x ∈ Rn, we deﬁne
distH (x, C) = min{(cid:107)x − y(cid:107)H | y ∈ C}. The largest (smallest) eigenvalue of H is denoted as λmax(H)
(λmin(H)). Given x ∈ Rn and an index set K ⊂ {1, · · · , n}, xK denotes the sub-vector of x with those
elements not in K being removed. For a closed proper convex function q : Rn → (−∞, ∞], the conjugate of
q is q∗(z) := supx∈Rn {(cid:104)x, z(cid:105) − q(x)}. The Moreau envelope of q at x is deﬁned by

Eq(x) := min
y∈Rn

(cid:110)

q(y) +

1
2

(cid:107)y − x(cid:107)2(cid:111)
,

and the associated proximal mapping Proxq(x) is deﬁned as the unique solution of the above minimization
problem. As proved in [24], Eq(·) is ﬁnite-valued, convex and diﬀerentiable with ∇Eq(x) = x − Proxq(x). In
addition, we can see from [26, 31] that Proxq(x) is Lipschitz continuous with modulus 1.

3 A mechanism for estimating the multivariate shape-constrained

convex function

In this section, we provide a comprehensive mechanism for computing the least squares estimator for the mul-
tivariate shape-constrained convex function deﬁned in (3). Before describing the process, we ﬁrst characterize
Property S in the following proposition. For brevity, we omit the proof.

Proposition 1. A convex function ψ has Property S if and only if for any x ∈ Rd, the subdiﬀerential of ψ
satisﬁes ∂ψ(x) ⊂ D, where D is deﬁned corresponding to Property S as follows:
(S1) (monotone constraint) D = {x ∈ Rd | xK1 ≥ 0, xK2 ≤ 0},
(S2) (box constraint) D = {x ∈ Rd | L ≤ x ≤ U },

(S3) (Lipschitz constraint) D = {x ∈ Rd | (cid:107)x(cid:107)q ≤ L}, where q satisﬁes 1/p + 1/q = 1.

The least squares estimation problem (3) attempts to ﬁnd a best-ﬁtting function ˆψ from the function
family CS , which is inﬁnite dimensional. Therefore, this problem is intractable in practice. In order to design
a tractable approach, we establish the following representation theorem to (3), which is motivated by [15].

Theorem 1. Deﬁne the set of piecewise linear functions as
(cid:12)
(cid:12)
(cid:12) φ(x) = max

φ : Ω → R

KS :=

(cid:110)

1≤j≤n

{θj + (cid:104)ξj, x − Xj(cid:105)}, (θ1, · · · , θn, ξ1, · · · , ξn) ∈ FS

(cid:111)
,

(4)

where

FS := {(θ1, · · · , θn, ξ1, · · · , ξn) | θi ∈ R, ξi ∈ D, i = 1, · · · , n, θi ≥ θj + (cid:104)ξj, Xi − Xj(cid:105), 1 ≤ i, j ≤ n},

(5)

and D is deﬁned as in Proposition 1. Consider the problem

min
φ∈KS

n
(cid:88)

(φ(Xi) − Yi)2.

i=1

Then the following equality holds:

min
ψ∈CS

n
(cid:88)

i=1

(ψ(Xi) − Yi)2 = min
φ∈KS

n
(cid:88)

(φ(Xi) − Yi)2.

i=1

Moreover, any solution ˆφ to (6) is a solution to the problem (3).

4

(6)

(7)

Proof. We ﬁrst prove that KS ⊂ CS , that is, the functions in KS are convex functions with Property S.
Convexity comes from the fact that any pointwise maximum function is convex. Given φ ∈ KS determined by
(θ1, · · · , θn, ξ1, · · · , ξn) ∈ FS , the subdiﬀerential of the piecewise linear function φ is a polyhedron according
to [30, Theorem 25.6], and it is given by

∂φ(x) = conv{ξi | i ∈ I(x)},

I(x) := {i | θi + (cid:104)ξi, x − Xi(cid:105) = φ(x)}.

By the deﬁnition of D and FS , we can see that ∂φ(x) ⊂ D for any x ∈ Ω. According to Proposition 1, the
convex function φ has Property S, which means φ ∈ CS . Therefore, we have that

min
ψ∈CS

n
(cid:88)

i=1

(ψ(Xi) − Yi)2 ≤ min
φ∈KS

n
(cid:88)

(φ(Xi) − Yi)2.

i=1

Next we prove the reverse inequality. For any ε > 0, there exists ˆψε ∈ CS such that

n
(cid:88)

i=1

( ˆψε(Xi) − Yi)2 ≤ min
ψ∈CS

n
(cid:88)

i=1

(ψ(Xi) − Yi)2 + ε.

If we take ˆξε,i ∈ ∂ ˆψε(Xi), i = 1, · · · , n, then

( ˆψε(X1), · · · , ˆψε(Xn), ˆξε,1, · · · , ˆξε,n) ∈ FS ,

ˆφε(x) := max
1≤j≤n

{ ˆψε(Xj) + (cid:104) ˆξε,j, x − Xj(cid:105)} ∈ KS .

The inequalities ˆψε(Xi) ≥ ˆψε(Xj) + (cid:104) ˆξε,j, Xi − Xj(cid:105) for all i, j implies that

ˆφε(Xi) = max
1≤j≤n

{ ˆψε(Xj) + (cid:104) ˆξε,j, Xi − Xj(cid:105)} = ˆψε(Xi),

i = 1, · · · , n.

Then, it holds that

min
φ∈KS

n
(cid:88)

i=1

(φ(Xi) − Yi)2 ≤

n
(cid:88)

i=1

( ˆφε(Xi) − Yi)2 =

n
(cid:88)

i=1

( ˆψε(Xi) − Yi)2 ≤ min
ψ∈CS

n
(cid:88)

i=1

(ψ(Xi) − Yi)2 + ε.

Since the above inequality holds for any ε > 0, the equality (7) follows. Now suppose ˆφ is an optimal solution
to (6), since ˆφ ∈ CS , we have that ˆφ is a solution to the problem (3).

The theorem above provides a tractable approach to compute (3) through solving (6). By deﬁnition,

any function φ in KS , which is determined by (θ1, · · · , θn, ξ1, · · · , ξn) ∈ FS , satisﬁes

φ(Xi) = max
1≤j≤n

{θj + (cid:104)ξj, Xi − Xj(cid:105)} = θi,

i = 1, · · · , n.

Therefore, we can conclude the mechanism for computing an optimal solution to (3) as follows.

The mechanism for shape-constrained convex regression. Suppose {(ˆθi, ˆξi)}n
tion to

i=1 is an optimal solu-

min
θ1,...,θn∈R;ξ1,...,ξn∈Rd

(cid:110) 1
2

(cid:107)θ − Y (cid:107)2 (cid:12)
(cid:12)
(cid:12) (θ1, · · · , θn, ξ1, · · · , ξn) ∈ FS

(cid:111)
,

where the feasible set FS is deﬁned as in (5). We can construct an optimal solution to (3) by taking

ˆψ(x) = max
1≤j≤n

(cid:110)ˆθj + (cid:104) ˆξj, x − Xj(cid:105)

(cid:111)
,

x ∈ Ω.

5

(8)

(9)

As one can see, the main task in our mechanism for estimating the shape-constrained convex function is to
solve the constrained convex quadratic programming problem (8).

Deﬁne the mapping A : Rn → Rn×n as Az = zeT

n − enzT . Then it holds that A∗Z = (Z − Z T )en
i − X T ∈ Rn×d,
for Z ∈ Rn×n, and A∗A = 2nIn − 2eneT
then deﬁne B : Rdn → Rn×n as Bξ = (B1ξ1, · · · , Bnξn) for ξ ∈ Rdn. Therefore, we have that B∗Z =
n Bn)T , and B∗B is a block diagonal matrix in Rdn×dn whose i-th block is BT
(Z T
i Bi. Based on
these notations, the problem (8) can equivalently be written as

n . Denote ξ = (ξ1; · · · ; ξn) ∈ Rdn, and Bi = enX T

1 B1, · · · , Z T

min
θ∈Rn,ξ∈Rdn

(cid:110) 1
2

(cid:107)θ − Y (cid:107)2 + p(ξ) + δ+(Aθ + Bξ)

(cid:111)
,

(P)

where p(ξ) = (cid:80)n

i=1 δD(ξi) and δ±(·) is the indicator function of Rn×n
± .

Smoothing approximation. The function ˆψ obtained by (9) is nonsmooth. When a smooth function is
required, we then need to compute a smooth approximation to ˆψ. The idea of Nesterov’s smoothing [25]
could be applied, and the details is described in [22, Section 3]. Alternatively, one can use the Moreau
envelope as a smooth approximation of ˆψ, namely

ˆψM

τ (x) = τ E ˆψ/τ (x) = min
y∈Rd

(cid:110) ˆψ(y) +

τ
2

(cid:107)y − x(cid:107)2(cid:111)
,

(10)

where τ > 0 is a regularization parameter. Note that

ˆψM

τ (x) = min

y∈Rd,t∈R

(cid:110)

t +

τ
2

(cid:107)y − x(cid:107)2 (cid:12)
(cid:111)
(cid:12) t ≥ (cid:104) ˆξj, y(cid:105) − (cid:104) ˆξj, Xj(cid:105) + ˆθj, j = 1, · · · , n
(cid:12)
,

and the unique solution Prox ˆψ/τ (x) of (10) can be obtained by solving a quadratic programming of dimension
d + 1, which could be eﬃciently computed by Gurobi or MOSEK. One can see that for any τ > 0, ˆψM
τ is convex,
τ (x) = τ (x − Prox ˆψ/τ (x)). In addition, according to [5], the approximation ˆψM
and diﬀerentiable with ∇ ˆψM
of ˆψ satisﬁes the approximation bound

τ

0 ≤ ˆψ(x) − ˆψM

τ (x) ≤

dist2(0, ∂ ˆψ(x)) ≤

1
τ

L2
2τ

,

∀x ∈ Ω,

where L = max{(cid:107)ξj(cid:107)2 | j = 1, · · · , n}.

Dual problem and optimality conditions. To derive its dual, it is convenient for us to write (P) as

min
θ∈Rn,ξ,y∈Rdn,η∈Rn×n

(cid:110) 1
2

(cid:107)θ − Y (cid:107)2 + p(y) + δ−(η)

(cid:12)
(cid:111)
(cid:12)
.
(cid:12) η + Aθ + Bξ = 0, ξ − y = 0

(11)

The associated Lagrangian function is

l(θ, ξ, y, η; u, v) =

1
2

(cid:107)θ − Y (cid:107)2 + p(y) + δ−(η) − (cid:104)u, η + Aθ + Bξ(cid:105) − (cid:104)v, ξ − y(cid:105).

By minimizing l(θ, ξ, y, η; u, v) with respect to θ, ξ, y, η, the dual problem of (P) is given by

max
u∈Rn×n,v∈Rdn

(cid:110)

−

1
2

(cid:107)A∗u(cid:107)2 − (cid:104)Y, A∗u(cid:105) − p∗(−v) − δ+(u)

(cid:12)
(cid:111)
(cid:12) B∗u + v = 0
(cid:12)
.

The Karush-Kuhn-Tucker (KKT) conditions associated with (P) and (D) are given as follows:

θ − Y − A∗u = 0, B∗u + v = 0, −v ∈ ∂p(ξ), −u ∈ ∂δ+(Aθ + Bξ).

(D)

(12)

6

4 Symmetric Gauss-Seidel based alternating direction method of

multipliers (sGS-ADMM) for (P)

The popular ﬁrst-order alternating direction method of multipliers (ADMM) can be applied to solve (P).
In [22, Section A.2], the problem (P) is reformulated as

min
θ∈Rn,ξ∈Rdn,η∈Rn×n

(cid:110) 1
2

(cid:107)θ − Y (cid:107)2 + p(ξ) + δ−(η) | η + Aθ + Bξ = 0

(cid:111)
.

The corresponding augmented Lagrangian function for a ﬁxed σ > 0 is deﬁned by

(cid:101)Lσ(θ, ξ, η; u) =

1
2

(cid:107)θ − Y (cid:107)2 + p(ξ) + δ−(η) +

σ
2

(cid:107)η + Aθ + Bξ −

u
σ

(cid:107)2 −

1
2σ

(cid:107)u(cid:107)2.

Then the two-block ADMM is given as





ξk+1 = arg min (cid:101)Lσ(θk, ξ, ηk; uk) = arg min

(θk+1, ηk+1) = arg min (cid:101)Lσ(θ, ξk+1, η; uk),
uk+1 = uk − τ σ(ηk+1 + Aθk+1 + Bξk+1),

(cid:110)

p(ξ) +

σ
2

(cid:107)ηk + Aθk + Bξ −

(cid:107)2(cid:111)
,

uk
σ

√

where τ ∈ (0, (1 +
5/2)) is a given step length. As described in [22], the subproblem of updating ξ is
separable in the variables ξi’s for i = 1, · · · , n, and the update of each ξi can be solved by using an interior
point method. The update of θ and η is performed by using a block coordinate descent method, which may
converge slowly. One can also apply the directly extended three-block ADMM algorithm as in [22, Section 2.1]
to solve (P), and the steps are given by






ξk+1 = arg min (cid:101)Lσ(θk, ξ, ηk; uk),
θk+1 = arg min (cid:101)Lσ(θ, ξk+1, ηk; uk),
ηk+1 = arg min (cid:101)Lσ(θk+1, ξk+1, η; uk),
uk+1 = uk − τ σ(ηk+1 + Aθk+1 + Bξk+1).

In the directly extended three-block ADMM, the subproblem of updating θ can be computed by solving a linear
system, and that of updating η can be solved by the projection onto Rn×n
− . However, it is shown in [6] that
the directly extended three-block ADMM may not be convergent.

In this section, we aim to present a convergent multi-block ADMM for solving (P). The authors in
[8] have proposed an inexact symmetric Gauss-Seidel based multi-block ADMM for solving high-dimensional
convex composite conic optimization problems, and it was demonstrated to perform better than the possibly
nonconvergent directly extended multi-block ADMM. Given a parameter σ > 0, the augmented Lagrangian
function associated with (11) is deﬁned by

Lσ(θ, ξ, y, η; u, v) = l(θ, ξ, y, η; u, v) +

σ
2

(cid:107)η + Aθ + Bξ(cid:107)2 +

(cid:107)ξ − y(cid:107)2

=

1
2

(cid:107)θ − Y (cid:107)2 + p(y) + δ−(η) +

σ
2

(cid:107)η + Aθ + Bξ −

u
σ

(cid:107)2 +

(cid:107)ξ − y −

v
σ

(cid:107)2 −

1
2σ

(cid:107)u(cid:107)2 −

1
2σ

(cid:107)v(cid:107)2.

(13)

σ
2
σ
2

Then the symmetric Gauss-Seidel based ADMM (sGS-ADMM) algorithm for solving (P) is given as follows.

7

Algorithm sGS-ADMM : Symmetric Gauss-Seidel based ADMM for (P)

Initialization: Choose an initial point (θ0, ξ0, y0, η0, u0, v0) ∈ Rn ×Rdn ×Rdn ×Rn×n ×Rn×n ×Rdn, and parameter
σ > 0. For k = 0, 1, 2, . . .
repeat

Step 1. Compute

Step 2. Compute

Step 3. Compute

(yk+1, ηk+1) = arg min Lσ(θk, ξk, y, η; uk, vk).

Step 2a. (cid:98)θk+1 = arg min Lσ(θ, ξk, yk+1, ηk+1; uk, vk),
Step 2b. ξk+1 = arg min Lσ((cid:98)θk+1, ξ, yk+1, ηk+1; uk, vk),
Step 2c. θk+1 = arg min Lσ(θ, ξk+1, yk+1, ηk+1; uk, vk).

where τ ∈ (0, (1 +

uk+1 = uk − τ σ(ηk+1 + Aθk+1 + Bξk+1), vk+1 = vk − τ σ(ξk+1 − yk+1),
√

5)/2) is the step length that is typically chosen to be 1.618.

until Stopping criterion is satisﬁed.

In Algorithm sGS-ADMM, all the subproblems can be solved explicitly. In Step 1, ηk+1 and yk+1 are

separable and can be solved independently as

yk+1 = Proxp/σ(ξk − vk/σ),

ηk+1 = Π−(−Aθk − Bξk + uk/σ),

where Π±(·) denotes the projection onto Rn×n
following linear system

± . In Step 2a and Step 2c, θ can be computed by solving the

(In + σA∗A)θ = Y − σA∗(η + Bξ − u/σ).

By noting that A∗A = 2nIn − 2eneT

n , one can apply the Sherman-Morrison-Woodbury formula to compute

(In + σA∗A)−1 =

1
1 + 2σn

(In + 2σeneT

n ).

Thus θ can be computed in O(n) operations. For Step 2b, ξk+1 can be computed by solving the linear
equation

(Idn + B∗B)ξ = yk+1 + vk/σ − B∗(ηk+1 + A(cid:98)θk+1 − uk/σ).

As the coeﬃcient matrix Idn + B∗B is a block diagonal matrix consisting n blocks of d × d submatrices, each
ξi can be computed separately, and the inverse of each block only needs to be computed once.

The convergence result of Algorithm sGS-ADMM is presented in the following theorem, which is taken

directly from [8, Theorem 5.1].

Theorem 2. Suppose that the solution set to the KKT system (12) is nonempty. Let {(θk, ξk, yk, ηk, uk, vk)}
be the sequence generated by Algorithm sGS-ADMM. Then the sequence {(θk, ξk, yk, ηk)} converges to an
optimal solution of problem (11), and the sequence {(uk, vk)} converges to an optimal solution of its dual
(D).

We can see that the sGS-ADMM algorithm is easy to implement, but it is just a ﬁrst-order algorithm which
may not be eﬃcient enough for solving (P) to high accuracy. In the next section, we design a superlinearly
convergent proximal augmented Lagrangian method for solving (P). By making full use of the special
structure of the problem, we can exploit the second-order sparsity structure in the problem to greatly reduce
the computational cost required in solving each of its subproblem.

8

5 Proximal augmented Lagrangian method (pALM) for (P)

The augmented Lagrangian method is a desirable method for solving convex composite programming prob-
lems due to its superlinear convergence. To take advantage of the fast local convergence, we design a proximal
augmented Lagrangian method (pALM) for solving (P). Note that the augmented Lagrangian function asso-
ciated with (P) for any ﬁxed σ > 0 can be derived as

(cid:98)Lσ(θ, ξ; u, v) = inf
y,η

Lσ(θ, ξ, y, η; u, v),

where Lσ is deﬁned in (13). Therefore, by making use of the Moreau envelope,

(cid:98)Lσ(θ, ξ; u, v) =

1
2

(cid:107)θ − Y (cid:107)2 + σEp(ξ −

v
σ

) + σEδ− (−Aθ − Bξ +

u
σ

) −

1
2σ

(cid:107)u(cid:107)2 −

1
2σ

(cid:107)v(cid:107)2.

We propose the pALM algorithm for solving (P) as follows.

Algorithm pALM : Proximal augmented Lagrangian method for (P)

Initialization: Let H1 ∈ Rn×n, H2 ∈ Rdn×dn be given positive deﬁnite matrices, and {εk} be a given summable
sequence of nonnegative numbers. Choose a initial point (θ0, ξ0, u0, v0) ∈ Rn × Rdn × Rn×n × Rdn, σ0 > 0. For
k = 0, 1, 2, . . .
repeat

Step 1. Compute

(θk+1, ξk+1) ≈ arg min

(cid:110)

Φk(θ, ξ) := (cid:98)Lσ(θ, ξ; uk, vk) +

1
2σk

(cid:107)θ − θk(cid:107)2

H1 +

1
2σk

(cid:107)ξ − ξk(cid:107)2

H2

(cid:111)

,

such that the approximation solution (θk+1, ξk+1) satisﬁes the following stopping condition:

(cid:107)∇Φk(θk+1, ξk+1)(cid:107) ≤

λmin
σk

εk,

where λmin = min{λmin(H1), λmin(H2), 1}.
Step 2. Update u, v by

(14)

(A)

uk+1 = σk

(cid:104)

vk+1 = −σk

uk/σk − Aθk+1 − Bξk+1 − Π−(uk/σk − Aθk+1 − Bξk+1)
(cid:104)
ξk+1 − vk/σk − Proxp(ξk+1 − vk/σk)

(cid:105)
.

(cid:105)

,

Step 3. Update σk+1 ↑ σ∞ ≤ ∞.
until Stopping criterion is satisﬁed.

5.1 Convergence results for pALM

The Lagrangian function associated with (P) can be derived as

(cid:98)l(θ, ξ, u, v) = inf
y,η

l(θ, ξ, y, η; u, v) =

1
2

(cid:107)θ − Y (cid:107)2 − p∗(−v) − δ+(u) − (cid:104)u, Aθ + Bξ(cid:105) − (cid:104)v, ξ(cid:105).

Deﬁne the maximal monotone operator T

(cid:98)l as

(cid:98)l(θ, ξ, u, v) =
T

(cid:110)

(θ(cid:48), ξ(cid:48), −u(cid:48), −v(cid:48)) | (θ(cid:48), ξ(cid:48), −u(cid:48), −v(cid:48)) ∈ ∂(cid:98)l(θ, ξ, u, v)

(cid:111)
.

For any (¯θ, ¯ξ, ¯u, ¯v) ∈ Rn × Rdn × Rn×n × Rdn, denote

Pk(¯θ, ¯ξ, ¯u, ¯v) = arg min
θ,ξ

max
u,v

(cid:110)

(cid:98)l(θ, ξ; u, v) +

1
2σk

(cid:107)θ − ¯θ(cid:107)2
H1

+

1
2σk

(cid:107)ξ − ¯ξ(cid:107)2
H2

−

1
2σk

(cid:107)u − ¯u(cid:107)2

F −

(cid:107)v − ¯v(cid:107)2(cid:111)
.

1
2σk

9

Deﬁne the block diagonal operator Σ = Diag(H1, H2, In×n, Idn), where In×n is the identity operator of
Rn×n. By mimicking the idea in [17], one can prove the following proposition.

Proposition 2. (1) For any k ≥ 0, it holds that for any (¯θ, ¯ξ, ¯u, ¯v) ∈ Rn × Rdn × Rn×n × Rdn,

Pk(¯θ, ¯ξ, ¯u, ¯v) = (Σ + σkT

(cid:98)l)−1Σ(¯θ, ¯ξ, ¯u, ¯v).

If (θ∗, ξ∗, u∗, v∗) ∈ T −1
(2) For all k ≥ 0,

(cid:98)l

(0), then Pk(θ∗, ξ∗, u∗, v∗) = (θ∗, ξ∗, u∗, v∗).

(cid:107)(θk+1, ξk+1, uk+1, vk+1) − Pk(θk, ξk, uk, vk)(cid:107)Σ ≤

σk
λmin

(cid:107)∇Φk(θk+1, ξk+1)(cid:107),

where λmin = min{λmin(H1), λmin(H2), 1}.

Based on the above proposition, we have the following convergence results, adapted from [17], for

Algorithm pALM.

Theorem 3. (1) Let {(θk, ξk, uk, vk)} be the sequence generated by Algorithm pALM with the stopping crite-
rion (A). Then (θk, ξk, uk, vk) is bounded, {(θk, ξk)} converges to an optimal solution of (P), and {(uk, vk)}
converges to an optimal solution of (D).
(2) Let Λ = T −1
that for this r > 0, there exists a constant κ > 0 such that T

i=0 εk + distΣ((θ0, ξ0, u0, v0), Λ). Assume
(cid:98)l satisﬁes the following error bound assumption

(0) be the primal-dual solution set. Let r := (cid:80)∞

(cid:98)l

dist((θ, ξ, u, v), Λ) ≤ κdist(0, T

(cid:98)l(θ, ξ, u, v)),

∀(θ, ξ, u, v) satisfying dist((θ, ξ, u, v), Λ) ≤ r.

(15)

Suppose that {(θk, ξk, uk, vk)} is the sequence generated by Algorithm pALM with the stopping criteria (A)
and (B), which is deﬁned as

(cid:107)∇Φk(θk+1, ξk+1)(cid:107) ≤

δkλmin
σk

(cid:107)(θk+1, ξk+1, uk+1, vk+1) − (θk, ξk, uk, vk)(cid:107)Σ,

and {δk | 0 ≤ δk < 1} is a given summable sequence. Then it holds for all k ≥ 0 that

distΣ((θk+1, ξk+1, uk+1, vk+1), Λ) ≤ µkdistΣ((θk, ξk, uk, vk), Λ),

(B)

(16)

where

µk =

1
1 − δk

δk + (1 + δk)κλmax
k + κ2λ2

(cid:112)σ2

max

→ µ∞ =

κλmax
∞ + κ2λ2

(cid:112)σ2

max

< 1,

k → ∞,

and λmax = max{λmax(H1), λmax(H2), 1}.

For specifying the convergence rate of Algorithm pALM for diﬀerent choices of the closed convex set D,

we give the following remark.

Remark 1. As one can see from Theorem 3, the linear convergence rate of Algorithm pALM depends on the
error bound assumption (15) for the maximal monotone operator T
(cid:98)l. It is well known that any polyhedral
multifunction is upper Lipschitz continuous at every point of its domain according to [29], which means it
satisﬁes the error bound assumption (15) for any r > 0. For the cases when D is a polyhedral set, e.g.
D = Rd
(cid:98)l is a polyhedral multifunction, and hence
it satisﬁes the error bound assumption (15).

−) or D = {x ∈ Rd | (cid:107)x(cid:107)q ≤ L} with q = 1 or q = ∞, T

+(Rd

10

5.2 A semismooth Newton method for solving the pALM subproblems

One can see that the most computationally intensive step in each of the pALM is in solving the subproblem
(14). Here we described how it can be solved eﬃciently by the semismooth Newton method (SSN). For given
σ > 0, (˜θ, ˜ξ, ˜u, ˜v) ∈ Rn × Rdn × Rn×n × Rdn, we aim to solve the pALM subproblem, which has the form:

min

(cid:110)

Φ(θ, ξ) := (cid:98)Lσ(θ, ξ; ˜u, ˜v) +

1
2σ

(cid:107)θ − ˜θ(cid:107)2
H1

+

(cid:107)ξ − ˜ξ(cid:107)2
H2

(cid:111)
.

1
2σ

(17)

Since Φ(·, ·) is strongly convex, the above minimization problem admits a unique solution (¯θ, ¯ξ), which can
be computed by solving the nonsmooth equation

where

∇Φ(θ, ξ) =






∇Φ(θ, ξ) = 0,

(−Aθ − Bξ +

˜u
σ

) − Π−(−Aθ − Bξ +

(cid:105)

)

+

˜u
σ

˜u
σ

) − Π−(−Aθ − Bξ +

(cid:104)
(ξ −

+ σ

(cid:105)
)

˜u
σ

θ − Y − σA∗(cid:104)
− σB∗(cid:104)

(−Aθ − Bξ +

1
σ

H1(θ − ˜θ)
˜v
σ

) − Proxp(ξ −

(18)




 .

H2(ξ − ˜ξ)

(cid:105)

)

+

˜v
σ

1
σ

To apply the SSN method to solve the above nonsmooth equation, we need a suitable generalized Jacobian
of Φ(·, ·), and we choose the following set as the candidate:

ˆ∂2Φ(θ, ξ) = σ

(cid:32)

(cid:33)

A∗
B∗

(cid:104)
I − ∂Π−(−Aθ − Bξ +

(cid:105) (cid:0) A B(cid:1) +
)

˜u
σ





In+

1
σ
0

H1

(cid:104)

σ

0

I − ∂Proxp(ξ −

(cid:105)

)

+

˜v
σ

1
σ

H2



 .

With the suitably chosen generalized Jacobian, we can design the following SSN method, which is a general-
ization of the standard Newton method, for solving (17).

Algorithm SSN : Semismooth Newton method for (17)

Initialization: Given (θ0, ξ0) ∈ Rn × Rdn, ¯γ ∈ (0, 1), τ ∈ (0, 1], δ ∈ (0, 1), and µ ∈ (0, 1/2). For j = 0, 1, 2, . . .
repeat

Step 1. Select an element Hj ∈ ˆ∂2Φ(θj, ξj). Apply a direct method or the preconditioned conjugate gradient
(PCG) method to ﬁnd an approximate solution (∆θj; ∆ξj) ∈ Rn × Rdn to

Hj(∆θ; ∆ξ) ≈ −∇Φ(θj, ξj)

(19)

such that (cid:107)Hj(∆θj; ∆ξj) + ∇Φ(θj, ξj)(cid:107) ≤ min(¯γ, (cid:107)∇Φ(θj, ξj)(cid:107)1+τ ).
Step 2. Set αj = δmj , where mj is the smallest nonnegative integer m for which

Φ(θj + δm∆θj, ξj + δm∆ξj) ≤ Φ(θj, ξj) + µδm(cid:104)∇Φ(θj, ξj), (∆θj; ∆ξj)(cid:105).

Step 3. Set θj+1 = θj + αj∆θj, ξj+1 = ξj + αj∆ξj.

until Stopping criterion based on θj+1 and ξj+1 is satisﬁed.

The convergence analysis for Algorithm SSN can be established as in [16].

Theorem 4. Suppose that Proxp(·) is strongly semismooth with respect to ∂Proxp(·). Let {(θj, ξj)} be the
inﬁnite sequence generated by Algorithm SSN. Then, {(θj, ξj)} converges to the unique optimal solution (¯θ, ¯ξ)
of problem (17) and

(cid:107)(θj+1, ξj+1) − (¯θ, ¯ξ)(cid:107) = O((cid:107)(θj, ξj) − (¯θ, ¯ξ)(cid:107)1+τ ).

11

Proof. Due to the strong convexity of Φ(·, ·), we can see that (θj, ξj) converges to the unique optimal solution
(¯θ, ¯ξ) [37, Proposition 3.3 and Theorem 3.4]. By the formulation of ˆ∂2Φ(·, ·), we have that all the elements
in ˆ∂2Φ(θ, ξ) are positive deﬁnite for any (θ, ξ) ∈ Rn × Rdn due to the positive deﬁniteness of H1 and H2.
Then the convergence rate of (θj, ξj) can be directly obtained from [37, Theorem 3.5].

Remark 2. As a side note, for the closed convex set D deﬁned in Proposition 1, the assumption that Proxp(·)
is strongly semismooth with respect to ∂Proxp(·) always holds.

6

Implementation of the algorithms

In this section, we discuss some numerical details concerning the eﬃcient implementation of two proposed
algorithms. For implementing the proposed algorithms, we need the proximal mapping Proxp(ξ) for any
ξ ∈ Rdn and its generalized Jacobian. In addition, when evaluating the function value of the dual problem
(D), we need the formula for p∗(·).

6.1 Computation associated with D
For any ξ = (ξ1; · · · ; ξn) ∈ Rdn, since p(ξ) = (cid:80)n

i=1 δD(ξi), we have that

p∗(ξ) =

n
(cid:88)

i=1

δ∗
D(ξi), Proxp(ξ) =






ΠD(ξ1)
...
ΠD(ξn)




 ,

∂Proxp(ξ) =

∂ΠD(ξ1)






. . .




 ,

(20)

∂ΠD(ξn)

which means that we just need to focus on δ∗
D(·), ΠD(·) and ∂ΠD(·) for each of the D’s deﬁned in Proposition
1. We summarize the results in Table 1 - Table 3, and the content on the generalized Jacobian follows the
idea in [10, 18]. The detailed derivation associated with the case when D = {x ∈ Rd | (cid:107)x(cid:107)1 ≤ L} can be
found in the Appendix.

Table 1: Conjugate function δ∗

D(·)

D

δ∗
D(x)

D = {x ∈ Rd | xK1 ≥ 0, xK2 ≤ 0}

δ∗
D(x) = δ−(xK1 ) + δ+(xK2 ) + δ{0}(xK3 ), where K3 := {1, · · · , d}\(K1 ∪ K2)

D = {x ∈ Rd | L ≤ x ≤ U }

D = {x ∈ Rd | (cid:107)x(cid:107)q ≤ L}

δ∗
D(x) = (cid:104)U, max{x, 0}(cid:105) + (cid:104)L, min{x, 0}(cid:105)

δ∗
D(x) = L(cid:107)x(cid:107)p,

1/p + 1/q = 1

Table 2: Proximal mapping ΠD(·)

D

ΠD(·)

D = {x ∈ Rd | xK1 ≥ 0, xK2 ≤ 0}

(ΠD(x))i =

(cid:40)

0

xi

if i ∈ K1, xi < 0, or i ∈ K2, xi > 0
otherwise

D = {x ∈ Rd | L ≤ x ≤ U }

(ΠD(x))i =

(cid:40)

xi
0

if Li ≤ xi ≤ Ui
otherwise

D = {x ∈ Rd | (cid:107)x(cid:107)∞ ≤ L}

(ΠD(x))i =

(cid:40)

xi
if |xi| ≤ L
sign(xi)L if |xi| > L

Continued on next page

12

Table 2 – continued from previous page

D

D = {x ∈ Rd | (cid:107)x(cid:107)2 ≤ L}

ΠD(x) =

ΠD(·)






x

L

x
(cid:107)x(cid:107)2

if (cid:107)x(cid:107)2 ≤ L

otherwise

D = {x ∈ Rd | (cid:107)x(cid:107)1 ≤ L}

(cid:40)

ΠD(x) =

x
LPxΠ∆d (Pxx/L)

if (cid:107)x(cid:107)1 ≤ L
otherwise

Note: Px = Diag(sign(x)) ∈ Rd×d, Π∆d (·) denotes the projection onto the simplex ∆d = {x ∈ Rd |

eT
d x = 1, x ≥ 0}, which can be computed in O(d log(d)) operations.

Table 3: Generalized Jacobian of ΠD(·)

D

∂ΠD(·)

D = {x ∈ Rd | xK1 ≥ 0, xK2 ≤ 0}

∂ΠD(x) = Diag(u), ui ∈






{0}

[0, 1]

{1}

if i ∈ K1, xi < 0, or i ∈ K2, xi > 0
if i ∈ K1 ∪ K2, xi = 0
otherwise

D = {x ∈ Rd | L ≤ x ≤ U }

∂ΠD(x) = Diag(u), ui ∈






{1}

[0, 1]

if Li < xi < Ui
if xi = Li or xi = Ui

{0}

otherwise

D = {x ∈ Rd | (cid:107)x(cid:107)∞ ≤ L}

∂ΠD(x) = Diag(u), ui ∈






{1}

[0, 1]

{0}

if |xi| < L
if |xi| = L
otherwise

D = {x ∈ Rd | (cid:107)x(cid:107)2 ≤ L}

∂ΠD(x) =






D = {x ∈ Rd | (cid:107)x(cid:107)1 ≤ L}

H ∈ ∂ΠD(x), where H =

{Id}
(cid:110)

Id − t

(cid:110) L

(cid:107)x(cid:107)2

xxT
L2 | 0 ≤ t ≤ 1
xxT
(cid:107)x(cid:107)2
2
(cid:40) Id

(cid:111)

)

(Id −

if (cid:107)x(cid:107)2 < L

if (cid:107)x(cid:107)2 = L

(cid:111)

otherwise

if (cid:107)x(cid:107)1 ≤ L

Px (cid:101)HPx

otherwise

Note:

(cid:101)H = Diag(r) − 1

nnz(r) rrT ∈ ∂Π∆d (x), where r ∈ Rd is deﬁned as ri = 1 if (cid:0)Π∆d (Pxx/L)(cid:1)

i (cid:54)= 0, and ri = 0

otherwise.

6.2 Finding a computable element in ˆ∂2Φ(θ, ξ)

The most diﬃcult part of the pALM algorithm is solving the Newton system (19). For eﬃcient practical
implementation, we need to ﬁnd an eﬃciently computable element in ˆ∂2Φ(θ, ξ) for any given (θ, ξ) ∈ Rn×Rdn.
From the deﬁnition of ˆ∂2Φ(θ, ξ), we can rewrite it as

where

ˆ∂2Φ(θ, ξ) = M1(θ, ξ) + M2(ξ),

M1(θ, ξ) = σ

(cid:32)

(cid:33)

A∗
B∗

(cid:104)
I − ∂Π−(−Aθ − Bξ +

(cid:105) (cid:0) A B(cid:1) ,
)

˜u
σ

13

M2(ξ) =





In+

1
σ
0

H1

(cid:104)

σ

0

I − ∂Proxp(ξ −

(cid:105)
)

+

˜v
σ

1
σ

H2



 .

Based on our discussion on ∂Proxp(·) in (20) and ∂ΠD(·) in Table 3, we can see that the elements in ∂Proxp(·)
are block diagonal matrices. In order to maintain the block diagonal structure, we choose H1 and H2 to be
diagonal matrices, and hence the elements in M2(ξ) for any ξ ∈ Rdn will also be block diagonal matrices.
One can easily pick an element in M2(ξ) by choosing an element in ∂Proxp(ξ − ˜v/σ). For M1(θ, ξ), to make
full use of the second-order sparsity structure, we choose an element W in ∂Π−(−Aθ − Bξ + ˜u

σ ), where

Wij =




1



0

if (−Aθ − Bξ +

otherwise.

˜u
σ

)ij ≤ 0,

By denoting the 0-1 matrix In − W as ¯W , then

P := σ

(cid:32)

(cid:33)

A∗
B∗

¯W (cid:0) A B(cid:1) = σ

(cid:32)

A∗ ¯W A A∗ ¯W B
B∗ ¯W A B∗ ¯W B

(cid:33)

is an element in M1(θ, ξ). After some algebraic manipulations by making use of the structure of A and B,
we can prove the following results:

A∗ ¯W A = Diag( ¯W en) + Diag( ¯W T en) − ¯W − ¯W T ,


A∗ ¯W B =

(cid:16)

Diag( ¯W1)B1, · · · , Diag( ¯Wn)Bn

(cid:17)

−




¯W T

1 B1

. . .

B∗ ¯W B =






BT

1 Diag( ¯W1)B1

. . .




 .

BT

n Diag( ¯Wn)Bn




 ,

¯W T

n Bn

It can be seen that the 0-1 structure of ¯W will reduce many operations in matrix-matrix multiplications,
and hence highly reduce the computational cost for P . For all i, Diag( ¯Wi)Bi is a matrix in Rn×d, with its
j-th row being the j-th row of Bi if ( ¯Wi)j = 1, or the zero vector if ( ¯Wi)j = 0. The computation of ¯W T
i Bi
can be obtained by summing the non-zero rows of Diag( ¯Wi)Bi, and the computation of BT
i Diag( ¯Wi)Bi =
(Diag( ¯Wi)Bi)T (Diag( ¯Wi)Bi) can be highly reduced in the same way.

The special structure of the elements in ˆ∂2Φ(θ, ξ) makes it possible for us to apply the second-order
type pALM algorithm for the huge constrained quadratic programming problem (P), which contains n(d + 1)
variables, n(n − 1) linear inequality constraints and n possibly non-polyhedral constraints.

7 Numerical experiments

In this section, we conduct some numerical experiments to demonstrate the performance of the sGS-ADMM
algorithm and the pALM algorithm for solving (P), under each case of D mentioned in Proposition 1. In
addition, we design a data-driven Lipschitz estimation method to deal with the boundary eﬀect of the
convex regression problem. All our computational results are obtained by running MATLAB on a windows
workstation (12-core, Intel Xeon E5-2680 @ 2.50GHz, 128 G RAM).

7.1 Computational performance of the algorithms for solving (P)

In this subsection, we compare the performance of the sGS-ADMM algorithm, the pALM algorithm and MOSEK for
increasing d and n. As for the three-block ADMM proposed in [22], we know that the sGS-ADMM algorithm has

14

been demonstrated to perform better than the possibly nonconvergent directly extended multi-block ADMM.
As we can see in [3], as long as there is enough memory, MOSEK performs quite a lot better than the parallel
proximal gradient method (PAPG). Since there is enough memory on our workstation, we just compare our
algorithms with the state-of-the-art algorithm MOSEK.

Stopping criteria. We measure the infeasibilities for the primal and dual problems (P) and (D) by
RP , RD, and the complementary conditions by RC, where

RP := max

RD := max

,

(cid:110) (cid:107)ξ − Proxp(ξ)(cid:107)
1 + (cid:107)ξ(cid:107)
(cid:107)θ − Y − A∗u(cid:107)
1 + (cid:107)Y (cid:107) + (cid:107)θ(cid:107) + (cid:107)u(cid:107)

(cid:107)Π−(Aθ + Bξ)(cid:107)
1 + (cid:107)Aθ(cid:107) + (cid:107)Bξ(cid:107)
(cid:107)B∗u + v(cid:107)
1 + (cid:107)u(cid:107) + (cid:107)v(cid:107)

(cid:111)
,

(cid:110)

,

(cid:111)
,

RC := max

(cid:110) (cid:107)ξ − Proxp(ξ − v)(cid:107)
1 + (cid:107)ξ(cid:107) + (cid:107)v(cid:107)

,

(cid:107)Aθ + Bξ − Π+(Aθ + Bξ − u)(cid:107)
1 + (cid:107)Aθ(cid:107) + (cid:107)Bξ(cid:107) + (cid:107)u(cid:107)

(cid:111)
.

We stop the algorithm when

RKKT := max{RP, RD, RC} ≤ (cid:15),

where (cid:15) = 10−6 is a given tolerance. In addition, the algorithm will be stopped when it reaches the maximum
computation time of 2 hours or the pre-set maximum number of iterations (200 for pALM, and 10000 for
sGS-ADMM).

Construction of synthetic datasets. For a given convex function ψ : Rd → R, the synthetic dataset
is generated via the procedure in [22]. We ﬁrst generate n samples Xi ∈ Rd, i = 1, · · · , n uniformly from
[−1, 1]d, then the corresponding responses are given as

Yi = ψ(Xi) + εi.

The error ε follows the normal distribution N (0, σ2In), where σ2 = Var({ψ(Xi)}n
ments, we take SNR = 3.

i=1)/SNR. In the experi-

Data preprocessing. Before we run the algorithms for the data X = (X1, · · · , Xn) ∈ Rd×n and Y ∈ Rn,
we process the data so as to build a more predictive model. For the response Y and each row of the predictor
X, we mean center the vector and then standardize it to have unit (cid:96)2-norm.

Numerical results. The numerical results on the comparison among pALM, sGS-ADMM and MOSEK can
be found in Table 4 - Table 9. We conduct experiments on the unconstrained convex regression problem
and each case of shape-constrained convex regression we mentioned before. In Algorithm pALM, we choose
H1 = 10−3In and H2 = 10−3Idn. All the test functions are convex on Rd and satisfy some speciﬁed shape
constraints. As one can see from the tables, both sGS-ADMM and pALM outperform the state-of-the-art solver
MOSEK. To be speciﬁed, when estimating the function ψ(x) = exp(pT x) for the moderate (d, n) = (100, 1000),
sGS-ADMM is about 6 times faster than MOSEK, and pALM is about 19 times faster than MOSEK. For the case
when d = 200, n = 3000, which is a large problem with 603, 000 variables and about 9, 000, 000 inequality
constraints, MOSEK runs out of memory, while pALM could solve it within 3 minutes and sGS-ADMM takes 10
minutes. From the tables, we can see that sGS-ADMM performs much better than MOSEK in each instance, and
pALM performs even better than sGS-ADMM. In most of the cases, pALM is at least 10 times faster than MOSEK.

15

Table 4: Convex regression for test function ψ(x) = exp(pT x), where p
is a given random vector with each coordinate drawn from the standard
normal distribution.

(d, n)

Algorithm

pALM

sGS-ADMM

MOSEK

Iteration
Time
RKKT
Iteration
Time
RKKT
Iteration
Time
RKKT

(50, 500)
14(13)
00:00:02
2.61e-9
389
00:00:08
9.95e-7
10
00:00:19
6.59e-9

(50, 1000)
16(20)
00:00:06
1.43e-9
562
00:00:38
9.88e-7
11
00:01:47
3.92e-9

(100, 1000)
15(21)
00:00:10
1.16e-8
411
00:00:31
9.90e-7
11
00:03:10
2.76e-10

(100, 2000)
18(34)
00:01:05
1.91e-7
719
00:04:47
8.44e-7
10
00:18:50
1.01e-7

(200, 2000)
16(27)
00:01:01
3.02e-7
447
00:03:26
9.91e-7
10
00:37:37
3.27e-9

(200, 3000)
18(35)
00:02:49
4.89e-7
563
00:09:39
9.92e-7
O.M.
O.M.
O.M.

Note:

“16(20)” means “pALM iterations (total inner SSN iterations)”. O.M. means the algorithm runs out of

memory. Time is in the format of hours:minutes:seconds.

Table 5: Convex regression with monotone constraint (non-decreasing)
for the test function ψ(x) = (eT

d x)+.

(d, n)

Algorithm

pALM

sGS-ADMM

MOSEK

Iteration
Time
RKKT
Iteration
Time
RKKT
Iteration
Time
RKKT

(50, 500)
17(19)
00:00:02
9.66e-9
529
00:00:12
8.04e-7
14
00:00:24
1.54e-9

(50, 1000)
17(23)
00:00:08
1.50e-9
917
00:01:07
9.96e-7
13
00:01:58
1.45e-9

(100, 1000)
17(24)
00:00:13
8.24e-8
481
00:00:39
9.92e-7
13
00:03:34
4.65e-7

(100, 2000)
20(63)
00:02:48
2.90e-7
960
00:06:46
7.87e-7
17
00:25:28
8.99e-11

(200, 2000)
18(41)
00:01:59
2.35e-7
541
00:04:28
9.19e-7
12
00:43:34
4.35e-7

(200, 3000)
20(56)
00:05:49
7.69e-7
716
00:13:04
9.88e-7
O.M.
O.M.
O.M.

Table 6: Convex regression with box constraint (L = 0d, U = ed) for
the test function ψ(x) = ln(1 + exp(eT

d x)).

(d, n)

Algorithm

pALM

sGS-ADMM

MOSEK

Iteration
Time
RKKT
Iteration
Time
RKKT
Iteration
Time
RKKT

(50, 500)
25(42)
00:00:04
1.73e-7
663
00:00:16
9.60e-7
19
00:00:30
9.95e-7

(50, 1000)
24(65)
00:00:23
1.63e-7
1016
00:01:17
7.47e-7
24
00:02:54
6.03e-8

(100, 1000)
18(25)
00:00:14
3.67e-8
473
00:00:41
7.46e-7
15
00:04:15
2.06e-8

(100, 2000)
19(55)
00:02:32
4.77e-7
935
00:06:43
8.83e-7
16
00:26:11
3.20e-10

(200, 2000)
17(37)
00:01:50
1.36e-7
546
00:04:33
9.69e-7
16
00:57:01
3.94e-10

(200, 3000)
20(57)
00:06:03
7.39e-7
715
00:13:04
9.95e-7
O.M.
O.M.
O.M.

16

Table 7: Convex regression with Lipschitz constraint (p = 1, q = ∞,
L = 1) for the test function ψ(x) =

1 + xT x.

√

(d, n)

Algorithm

pALM

sGS-ADMM

MOSEK

Iteration
Time
RKKT
Iteration
Time
RKKT
Iteration
Time
RKKT

(50, 500)
15(16)
00:00:02
1.01e-11
531
00:00:13
9.63e-7
10
00:00:22
7.51e-9

(50, 1000)
17(30)
00:00:10
5.67e-8
928
00:01:11
9.97e-7
11
00:01:55
3.46e-10

(100, 1000)
15(20)
00:00:11
1.52e-8
500
00:00:44
9.27e-7
10
00:03:30
3.81e-13

(100, 2000)
18(38)
00:01:24
6.70e-7
921
00:06:46
9.97e-7
11
00:21:23
5.15e-10

(200, 2000)
18(36)
00:01:36
4.05e-7
538
00:04:26
9.04e-7
11
00:46:15
2.86e-15

(200, 3000)
21(43)
00:04:00
8.93e-7
748
00:15:37
8.63e-7
O.M.
O.M.
O.M.

Table 8: Convex regression with Lipschitz constraint (p = 2, q = 2,
L = λmax(Q)) for the test function ψ(x) = (cid:112)xT Qx.

(d, n)

Algorithm

pALM

sGS-ADMM

MOSEK

Iteration
Time
RKKT
Iteration
Time
RKKT
Iteration
Time
RKKT

(50, 500)
13(12)
00:00:02
2.05e-8
541
00:00:14
9.35e-7
10
00:00:22
2.50e-7

(50, 1000)
17(30)
00:00:09
3.52e-8
953
00:01:19
9.64e-7
13
00:02:04
1.06e-9

(100, 1000)
15(20)
00:00:10
7.52e-9
549
00:00:53
7.53e-7
11
00:03:41
4.21e-9

(100, 2000)
19(42)
00:01:39
2.49e-7
1005
00:07:29
8.75e-7
12
00:22:20
3.20e-9

(200, 2000)
17(35)
00:01:31
4.84e-7
499
00:04:18
8.86e-7
11
00:46:19
1.21e-8

(200, 3000)
32(59)
00:04:21
9.95e-7
705
00:13:38
9.63e-7
O.M.
O.M.
O.M.

Note: Q ∈ Rd×d is a randomly generated positive deﬁnite matrix with known largest eigenvalue.

Table 9: Convex regression with Lipschitz constraint (p = ∞, q = 1,
L = 1) for the test function ψ(x) = ln(1 + ex1 + · · · + exd ).

(d, n)

Algorithm

pALM

sGS-ADMM

MOSEK

Iteration
Time
RKKT
Iteration
Time
RKKT
Iteration
Time
RKKT

(50, 500)
14(14)
00:00:02
7.99e-12
413
00:00:11
9.09e-7
12
00:00:40
1.23e-8

(50, 1000)
16(22)
00:00:07
1.29e-8
767
00:01:01
9.96e-7
12
00:03:47
1.25e-7

(100, 1000)
15(20)
00:00:10
2.13e-8
471
00:00:42
7.16e-7
13
00:06:58
2.97e-10

(100, 2000)
18(40)
00:01:23
2.25e-7
813
00:05:55
9.94e-7
8
00:37:26
3.04e-9

(200, 2000)
17(36)
00:01:32
3.85e-7
462
00:03:59
9.89e-7
8
01:02:19
4.52e-9

(200, 3000)
21(41)
00:03:37
9.78e-7
667
00:12:59
9.94e-7
O.M.
O.M.
O.M.

7.2 Data-driven Lipschitz estimation method

An important issue in convex regression is over-ﬁtting near the boundary of conv(X1, · · · , Xn). That is to
say, the norms of the ﬁtted subgradients ξi’s near the boundary can become arbitrarily large. The authors
in [4,19,22] used the idea of Lipschitz convex regression to deal with this problem. They propose to compute
the least squares estimator over the class of convex functions that are uniformly Lipschitz with a given

17

bound, which means that they compute the estimator deﬁned in (3) with Property S taking the form of
(S3). In practice, the challenge is in choosing the unknown Lipschitz constant in the model based on the
given data. Mazumder et al. [22] choose to estimate the Lipschitz constant by using the cross-validation. In
this paper, we provide a data-driven Lipschitz estimation method for the Lipschitz convex regression.

For each Xi, we ﬁrst ﬁnd the k-nearest neighbors N (Xi) of Xi, and then deﬁne

Li = median

(cid:110) |Yi − Yj|

(cid:107)Xi − Xj(cid:107)p

, j ∈ N (Xi)

(cid:111)
,

where p = 1, 2, ∞ is given. Then we solve the generalization form of (8) as

min
θ1,...,θn∈R;ξ1,...,ξn∈Rd

1
2

n
(cid:88)

(θi − Yi)2

i=1

s.t.

θi ≥ θj + (cid:104)ξj, Xi − Xj(cid:105),
i = 1, · · · , n,
ξi ∈ Di,

∀ 1 ≤ i, j ≤ n,

(21)

where Di = {x ∈ Rd | (cid:107)x(cid:107)q ≤ Li} with 1/p+1/q = 1. The proposed sGS-ADMM algorithm and pALM algorithm
can be easily extended to solve (21) by letting p(ξ) = (cid:80)n

i=1 δDi(ξi).

We use an example here to demonstrate the performance of the Lipschitz convex regression with the
data-driven Lipschitz estimation method. Consider the convex function ψ(x) = 2(cid:107)x(cid:107)∞ + (cid:107)x(cid:107)2, we sample
n = 80 data points uniformly from [−1, 1]d and add the Gaussian noise as stated in Section 7.1. The results
for d = 1, 2 can be seen in Figure 1. When estimating the Lipschitz constant for each data point, we take
k = 5 and p = q = 2. As shown in the ﬁgure, Lipschitz convex regression does beneﬁt the performance of
the regression near the boundary of the convex hull of Xi’s.

(a) d = 1

(b) d = 2

Figure 1: Result of Lipschitz convex regression with the data-driven Lipschitz estimation method.

8 Real applications

In this section, we apply our mechanism for estimating the multivariate shape-constrained convex functions
in some real applications, namely, pricing of European call option, pricing of basket option, prediction of
average weekly wages and estimation of production functions.

18

-0.8-0.6-0.4-0.200.20.40.60.8X-2-101234YResult of Lipschitz convex regression (d=1)data pointtrue functionconvex regressionLipschitz convex regression8.1 Option pricing of European call option

Consider a European call option whose payoﬀ at maturity T is (ST − K)+, where ST is a random variable
that stands for the stock price at T , and K is the predetermined strike price. We are interested in the option
price at time t, which is deﬁned as

V (S) := E[e−r(T −t)(ST − K)+ | St = S], S > 0,

where r is the risk-free interest rate. Under Black-Scholes model, the random variable ST satisﬁes

log ST ∼ N

(cid:16)

log St + (r −

σ2)(T − t), σ2(T − t)

(cid:17)

,

1
2

where σ is the volatility. It is well-known that V (·) is a convex function with 0 ≤ V (cid:48)(S) ≤ 1 for S > 0.
Therefore, we can use the shape-constrained convex regression model with Property (S2) to approximate the
function V (·).

There are two reasons why we consider this application to demonstrate the numerical performance of

our mechanism. One is that V (·) admits a closed-form solution as

V (S) = SΦ(d1) − Ke−r(T −t)Φ(d2),

d1,2 =

log S

K + (r ± 1
√
σ

T − t

2 σ2)(T − t)

,

where Φ(·) is the cumulative distribution function of the standard normal distribution. The second reason is
that the approximation of function V (·) is used in pricing American-type options by approximate dynamic
programming, e.g. [21].

In our experiment, we take t = 0.1, T = 0.4, K = 10, r = 0, σ = 0.2. We sample 200 data points, denoted
as {(Si, Vi)}200
i=1. For each Si, log Si is sampled following the distribution N (log K + (r − σ2/2)t, σ2t), and the
corresponding Vi is sampled such that log Vi follows the distribution N (log Si + (r − σ2/2)(T − t), σ2(T − t)).
For comparison, we apply several regression models to approximate the conditional expectation function V :
linear regression, least squares linear regression on a set of basis functions (e.g. weighted Laguerre basis
in [21]), unconstrained convex regression and convex regression with box constraint (L = 0, U = 1).

The comparison among regression models is shown in Figure 2. We can see that the performance
of shape-constrained convex regression is the best. The poor performance of linear regression, Laguerre
regression and unconstrained convex regression appears near the boundary in three aspects. The ﬁrst is
that the results from linear regression and Laguerre regression take negative values when S is small, which
contradicts the fact that V is always non-negative. The second is that the Laguerre regression can not
obtain the required convex property. The last is that when S is large, the gradients of the results obtained
by Laguerre regression and unconstrained convex regression are too large. To deal with this over-ﬁtting
problem, we add the box constraint to the convex regression, which comes from prior knowledge. We can see
that the result of shape-constrained convex regression performs better near the boundary, which demonstrate
the advantage of the additional shape constraint.

19

Figure 2: Results of the estimation of the option pricing of European call option.

8.2 Option pricing of basket option

To test multivariate convex regression problem, we consider pricing the basket option on weighted average
of M underlying assets.

Basket option of two European call options (M = 2). We ﬁrst consider a basket option of two
European call options, where

V (x, y) = E[e−r(T −t)(w1S1

T + w2S2

T − K)+ | S1

t = x, S2

t = y],

x, y > 0,

where w = (w1, w2)T is a given weight vector such that w ≥ 0, w1 + w2 = 1. The random variables S1
S2

T and

T satisfy

(cid:33)

(cid:32) log S1
T
log S2
T

∼ N

(cid:32)(cid:32) log S1
log S2

t + (r − σ2
t + (r − σ2

1/2)(T − t)
2/2)(T − t)

(cid:33)

, (T − t)

(cid:32) σ2

1
ρσ1σ2

(cid:33)(cid:33)

,

ρσ1σ2
σ2
2

where σ1, σ2 are volatilities. One can show that V (·, ·) is a convex function with 0 ≤ ∇V (x, y) ≤ w, and
the proof can be found in the Appendix. We can apply the multivariate shape-constrained convex regression
model with Property (S2) (L = 0, U = w) to estimate the function V .

The convex function V (·, ·) does not admit a closed-form solution. However V is also the solution of
Black-Scholes PDE, which can be solved by the ﬁnite diﬀerence method. The details of the corresponding
convection-diﬀusion equation and the ﬁnite diﬀerence method for solving it could be found in the Appendix.
We use the solution obtained by the ﬁnite diﬀerence method as the benchmark.

In the experiment, we take r = 0, ρ = 0.1, σ1 = 0.2, σ2 = 0.3, K = 10, t = 0, T = 0.5, w1 = w2 = 0.5.
i=1, where Si follows the uniform distribution on the open

We sample 200 data points, denoted as {(Si, Vi)}200
interval (0, 5K) × (0, 5K) and Vi follows the the distribution

(cid:32)

N

log Si + (T − t)

(cid:33)

(cid:32) r − σ2
r − σ2

1/2
2/2

, (T − t)

(cid:32) σ2

1
ρσ1σ2

(cid:33)(cid:33)

.

ρσ1σ2
σ2
2

The numerical result is shown in Figure 3. For better illustration, we also plot the absolute error and relative
error of the results of the unconstrained convex regression and shape-constrained convex regression. As we
can see, the shape-constrained convex regression performs much better than unconstrained convex regression,
especially near the boundary.

20

88.599.51010.51111.512S-10123456VComparison among regression modelsdata pointtrue functionlinear regressionLaguerre regressionuncontrained convex regressionshape-constrained convex regression8.38.48.58.68.78.88.99-0.6-0.4-0.200.2(a)

(b)

(c)

Figure 3: Result of the estimation of the option pricing of basket option (M = 2).

Basket option of more underlying assets (M > 2). The basket option in practice always contains
many underlying assets, possibly greater than two. The ﬁnite diﬀerence method is very time-consuming
when solving the 3-dimensional convection-diﬀusion equation, and even impossible to be applied to the
higher dimensional cases due to the curse of dimensionality. For M > 2, researchers tend to apply the Monte
Carlo simulation to estimate the convex function associated with the basket option. Therefore, we treat the
solution obtained by the Monte Carlo simulation as the benchmark.

To demonstrate the performance of the shape-constrained convex regression, we design the experiments
for estimating the basket option for M = 5 and M = 10. That is, we consider a basket option of M European
call options, which is deﬁned as

V (x1, · · · , xM ) = E[e−r(T −t)(w1S1

T + · · · + wM SM

T − K)+ | S1

t = x1, · · · , SM

t = xM ],

x1, · · · , xM > 0,

where w = (w1, · · · , wM )T is a given weight vector such that w ≥ 0, w1 + · · · + wM = 1. The random

21

variables S1

T , · · · , SM

T satisfy







log S1
T
...
log SM
T







∼ N













log S1

t + (r − σ2

1/2)(T − t)

...













, (T − t)

log SM

t + (r − σ2

M /2)(T − t)

σ2
1
...

· · · ρσ1σM
. . .
ρσ1σM · · ·

...
σ2
M













,

where σ1, · · · , σM are volatilities. Then V is a convex function with 0 ≤ ∇V ≤ w. We apply the multivariate
shape-constrained convex regression model with Property (S2) (L = 0, U = w) to estimate the function V .
In the experiment, we set r = 0, ρ = 0.1, K = 10, t = 0, T = 0.5, wi = 1/M , σi = 0.2 + 0.025(i − 1),
i = 1, · · · , M . We sample N data points as the case for M = 2. To illustrate the performance of our
procedure, we uniformly generate 1000 test points in the range (0, 5K)M . At each test point, we use the
Monte Carlo simulation with 105 samples to compute the “true” function value. We summarize the results of
M = 5 and M = 10 in Table 10 and Table 11, respectively. In the tables, “UC” represents the unconstrained
convex regression, “SC” represents the shape-constrained convex regression, and “MSE” represents the mean
squared error. As one can see, the shape-constrained convex regression takes longer time to be solved than
the unconstrained convex regression, but get a much better estimated result.

Table 10: Estimation of basket option with M = 5.

Table 11: Estimation of basket option with M = 10.

Model Num. of data

UC

SC

200
400
600
200
400
600

MSE
1.35e+2
2.06e+1
7.56e+1
4.34e-1
3.97e-1
5.83e-1

Time
00:00:05
00:00:38
00:01:18
00:00:24
00:01:36
00:02:34

Model Num. of data

UC

SC

200
400
600
200
400
600

MSE
4.60e+2
9.36e+1
4.29e+1
2.91e+0
1.45e+0
1.49e+0

Time
00:00:08
00:00:10
00:00:28
00:00:29
00:01:04
00:03:52

8.3 Prediction of average weekly wages

We consider the problem of estimating the average weekly wages based on years of education and experience
as given in [28, Chapter 10]. This dataset is from 1988 March U.S. Current Population Survey, which can
be downloaded as ex1029 in the R package Sleuth2. The set contains weekly wages in 1987 for a sample of
25632 males between the age of 18 and 70 who worked full-time, with their years of education and years of
experience. After averaging over a grid with cell size of 1 year by 1 year and ignoring the outliers, we ﬁnally
come to a dataset with 857 samples.

A reasonable assumption for this application is that the wages are concave in years of experience and a
transformation of years of education, i.e., 1.2years of education, according to [11]. The estimated result is
shown in Figure 4. The shape-constrained convex regression problem is solved within 1 minute.

22

(a) Estimated function values at each Xi.

(b) Visualization of the function.

Figure 4: Results of the estimation of average weekly wages.

8.4 Estimation of production functions

In economics, a production function gives the technological relation between quantities of inputs and quan-
tities of output of goods. Production functions are known to be concave and non-decreasing [12, 34, 36].
We apply our framework to estimate the production function for the plastic industry (CIIU3 industry code:
2520) in the year 2011. The dataset can be downloaded from the website of Chile’s National Institute of
Statistics (https://www.ine.cl/estadisticas/economicas/manufactura). As in the setting in [36], we
use labor and capital as the input variables, and value added as the output variable. In the dataset, labor
is measured as the total man-hours per year, capital and value added are measured in millions of Chilean
peso. After removing some outliers, the dataset contains 250 samples. The numerical results can be found
in Figure 5. The shape-constrained convex regression problem is solved within 3 seconds.

(a) Correlogram of the function.

(b) Visualization of the function.

Figure 5: Result of estimation of production function of plastic in Chile.

Another example is to explain the labour demand of 569 Belgian ﬁrms for the year 1996. The dataset
can be obtained from [35] (https://www.wiley.com/legacy/wileychi/verbeek2ed/datasets.html). The

23

01234capital10402468labor107050001000015000estimated convex regression value-500005000100001500020000data pointdataset includes the total number of employees (labour), their average wage (wage), the amount of capital
(capital) and a measure of output (value added). The labour is measured as the number of workers, the
wage is measured in units of 1000 euro, and the capital and value added is measured in units of a million
euro. After removing the outliers, the dataset contains 562 samples. The result can be found in Figure 6.
The problem is solved in 22 seconds.

(a) Correlogram of the function.

(b) Visualization of the function.

Figure 6: Result of estimation of production function of Belgian ﬁrms.

9 Conclusion and future work

In this paper, we provide a comprehensive mechanism for computing a least squares estimator for the
multivariate shape-constrained convex regression function. In addition, we propose two eﬃcient algorithms,
the symmetric Gauss-Seidel based alternating direction method of multipliers, and the proximal augmented
Lagrangian method, to solve the large-scale constrained QP in the mechanism. We conduct the extensive
numerical experiments to demonstrate the eﬃciency and robustness of our proposed algorithms. In future,
we may extend the idea to estimate a continuously diﬀerentiable function with a Lipschitz derivative, which
is known to be the diﬀerence of a convex function and a convex quadratic function on a compact convex
set [38].

Acknowledgements

The authors would like to thank Professor Necdet S. Aybat for helpful clariﬁcations on his work in [3].

References

[1] Y. Aıt-Sahalia and J. Duarte, Nonparametric option pricing under shape restrictions, Journal of

Econometrics, 116 (2003), pp. 9–47.

[2] G. Allon, M. Beenstock, S. Hackman, U. Passy, and A. Shapiro, Nonparametric estimation
of concave production technologies by entropic methods, Journal of Applied Econometrics, 22 (2007),
pp. 795–816.

[3] N. S. Aybat and Z. Wang, A parallel method for large scale convex regression problems, in 53rd IEEE

Conference on Decision and Control, IEEE, 2014, pp. 5710–5717.

24

050100wage010002000labour050100150capital-50050100150200estimated convex regression value050100150200250data point[4] G. Bal´azs, A. Gy¨orgy, and C. Szepesv´ari, Near-optimal max-aﬃne estimators for convex regres-

sion, in AISTATS, 2015.

[5] A. Beck and M. Teboulle, Smoothing and ﬁrst order methods: A uniﬁed framework, SIAM Journal

on Optimization, 22 (2012), pp. 557–580.

[6] C. Chen, B. He, Y. Ye, and X. Yuan, The direct extension of ADMM for multi-block convex
minimization problems is not necessarily convergent, Mathematical Programming, 155 (2016), pp. 57–
79.

[7] H. Chen and D. D. Yao, Fundamentals of queueing networks: Performance, asymptotics, and opti-

mization, vol. 46, Springer Science & Business Media, 2013.

[8] L. Chen, D. F. Sun, and K.-C. Toh, An eﬃcient inexact symmetric Gauss–Seidel based majorized
ADMM for high-dimensional convex composite conic programming, Mathematical Programming, 161
(2017), pp. 237–270.

[9] A. L. Dontchev, H. Qi, and L. Qi, Quadratic convergence of Newton’s method for convex interpo-

lation and smoothing, Constructive Approximation, 19 (2003).

[10] J. Han and D. F. Sun, Newton and quasi-Newton methods for normal maps with polyhedral sets,

Journal of Optimization Theory and Applications, 94 (1997), pp. 659–676.

[11] L. A. Hannah and D. B. Dunson, Multivariate convex regression with adaptive partitioning, The

Journal of Machine Learning Research, 14 (2013), pp. 3261–3294.

[12] G. Hanoch and M. Rothschild, Testing the assumptions of production theory: a nonparametric

approach, Journal of Political Economy, 80 (1972), pp. 256–275.

[13] D. Hanson and G. Pledger, Consistency in concave regression, The Annals of Statistics, (1976),

pp. 1038–1050.

[14] C. Hildreth, Point estimates of ordinates of concave functions, Journal of the American Statistical

Association, 49 (1954), pp. 598–619.

[15] T. Kuosmanen, Representation theorem for convex nonparametric least squares, The Econometrics

Journal, 11 (2008), pp. 308–325.

[16] X. Li, D. F. Sun, and K.-C. Toh, A highly eﬃcient semismooth Newton augmented Lagrangian

method for solving Lasso problems, SIAM Journal on Optimization, 28 (2018), pp. 433–458.

[17]

, An asymptotically superlinearly convergent semismooth Newton augmented Lagrangian method

for Linear Programming, arXiv preprint arXiv:1903.09546, (2019).

[18]

, On the eﬃcient computation of a generalized Jacobian of the projector over the Birkhoﬀ polytope,

Mathematical Programming, 180 (2020), pp. 1–28.

[19] E. Lim, On convergence rates of convex regression in multiple dimensions, INFORMS Journal on Com-

puting, 26 (2014), pp. 616–628.

[20] E. Lim and P. W. Glynn, Consistency of multidimensional convex regression, Operations Research,

60 (2012), pp. 196–208.

[21] F. A. Longstaff and E. S. Schwartz, Valuing American options by simulation: a simple least-

squares approach, The Review of Financial Studies, 14 (2001), pp. 113–147.

25

[22] R. Mazumder, A. Choudhury, G. Iyengar, and B. Sen, A computational framework for multi-
variate convex regression and its variants, Journal of the American Statistical Association, 114 (2019),
pp. 318–331.

[23] R. F. Meyer and J. W. Pratt, The consistent assessment and fairing of preference functions, IEEE

Transactions on Systems Science and Cybernetics, 4 (1968), pp. 270–278.

[24] J.-J. Moreau, Proximit´e et dualit´e dans un espace hilbertien, Bulletin de la Soci´et´e math´ematique de

France, 93 (1965), pp. 273–299.

[25] Y. Nesterov, Smooth minimization of non-smooth functions, Mathematical Programming, 103 (2005),

pp. 127–152.

[26] J. Nocedal and S. Wright, Numerical Optimization, Springer Science & Business Media, 2006.

[27] H. Qi and X. Yang, Regularity and well-posedness of a dual program for convex best C1-spline inter-

polation, Computational Optimization and Applications, 37 (2007), pp. 409–425.

[28] F. Ramsey and D. Schafer, The Statistical Sleuth: A Course in Methods of Data Analysis, Boston:

Cengage Learning, 2012.

[29] S. M. Robinson, Some continuity properties of polyhedral multifunctions, in Mathematical Program-

ming at Oberwolfach, Springer, 1981, pp. 206–214.

[30] R. T. Rockafellar, Convex Analysis, vol. 28, Princeton University Press, 1970.

[31]

, Monotone operators and the proximal point algorithm, SIAM Journal on Control and Optimiza-

tion, 14 (1976), pp. 877–898.

[32] E. Seijo and B. Sen, Nonparametric least squares estimation of a multivariate convex regression

function, The Annals of Statistics, 39 (2011), pp. 1633–1657.

[33] H. R. Varian, The nonparametric approach to demand analysis, Econometrica: Journal of the Econo-

metric Society, (1982), pp. 945–973.

[34]

, The nonparametric approach to production analysis, Econometrica: Journal of the Econometric

Society, (1984), pp. 579–597.

[35] M. Verbeek, A Guide to Modern Econometrics, John Wiley & Sons, 2008.

[36] D. Yagi, Y. Chen, A. L. Johnson, and T. Kuosmanen, Shape-constrained kernel-weighted least
squares: Estimating production functions for Chilean manufacturing industries, Journal of Business &
Economic Statistics, (2018), pp. 1–12.

[37] X.-Y. Zhao, D. F. Sun, and K.-C. Toh, A Newton-CG augmented Lagrangian method for semidef-

inite programming, SIAM Journal on Optimization, 20 (2010), pp. 1737–1765.

[38] S. Zlobec, The fundamental theorem of calculus for Lipschitz functions, Mathematical Communica-

tions, 13 (2008), pp. 215–232.

26

Appendices

A Derivation of the proximal mapping and generalized Jacobian

associated with D = {x ∈ Rd | (cid:107)x(cid:107)1 ≤ L}

For x ∈ Rd, let Px = diag(sign(x)) ∈ Rd×d, then

ΠD(x) = arg min
y∈Rd
(cid:16)

= LPx

(cid:107)y − x(cid:107)2 | (cid:107)y(cid:107)1 ≤ L

(cid:111)

(cid:110) 1
2

arg min
y∈Rd

(cid:107)y − Pxx/L(cid:107)2 | eT

d y ≤ 1, y ≥ 0

(cid:111)(cid:17)

(cid:110) 1
2

(cid:40)

=

x
LPxΠ∆d (Pxx/L)

if (cid:107)x(cid:107)1 ≤ L,
otherwise,

where the simplex ∆d = {x ∈ Rd | eT
d x = 1, x ≥ 0}. To derive the generalized Jacobian of ΠD(·), we need
the generalized Jacobian of Π∆d (·). Following the idea in [10, 18], we can explicitly compute an element of
the generalized Jacobian of Π∆d (·) at Pxx/L. Let K be the set of index i such that (Π∆(Pxx/L))i = 0.
Then

(cid:101)H = Id − (cid:2)I T

K ed

(cid:3) (cid:16)

(cid:35)

(cid:34)IK
eT
d

(cid:2)I T

K ed

(cid:35)

(cid:3) (cid:17)† (cid:34)IK
eT
d

is an element in ∂Π∆d(Pxx/L), where IK means the matrix consisting of the rows of the identity matrix Id,
indexed by K. After some algebraic computation, we can see

(cid:101)H = Id − (cid:2)I T

K ed

(cid:3)

(cid:34)I|K| + 1
− 1

n−|K| e|K|eT
n−|K| eT

|K|

|K| − 1

n−|K| e|K|
1
n−|K|

(cid:35)

(cid:35) (cid:34)IK
eT
d

= Diag(r) −

1
nnz(r)

rrT ,

where r ∈ Rd is deﬁned as ri = 1 if (Π∆(Pxx/L))i (cid:54)= 0 and ri = 0 otherwise. Therefore,

H ∈ ∂ΠD(x), where H =

(cid:40) Id

if (cid:107)x(cid:107)1 ≤ L,

Px (cid:101)HPx

otherwise.

B Property of basket option of two European call options

The function V (x, y) is diﬀerentiable since it is the solution of the Black-Scholes PDE. By the deﬁnition
of V , we can see that V is non-decreasing in x and y, which means that ∇V (x, y) ≥ 0. According to the
distribution of S1

T and S2

T , we have that

where

V (x, y) = e−r(T −t)Ezf (x, y, z),

1 /2)(T −t)+

√

f (x, y, z) = (xw1e(r−σ2
(cid:32) σ2

(cid:33)

∼ N (0,

(cid:32)z1
z2

1
ρσ1σ2

ρσ1σ2
σ2
2

T −tz1 + yw2e(r−σ2
(cid:33)

√

2 /2)(T −t)+

T −tz2 − K)+,

).

27

For any x1, x2, y ∈ R, we can see that

|V (x1, y) − V (x2, y)| = e−r(T −t)(cid:12)
(cid:12)
(cid:12)

(cid:12)
Ez[f (x1, y, z) − f (x1, y, z)]
(cid:12)
(cid:12)

≤ e−r(T −t)Ez|f (x1, y, z) − f (x1, y, z)|
≤ e−r(T −t)Ez[w1e(r−σ2
= w1|x1 − x2|e−σ2
= w1|x1 − x2|.

1 /2)(T −t)+
√

1 /2(T −t)Ez[e

T −tz1 ]

√

T −tz1|x1 − x2|]

Similarly, we can prove that for any x, y1, y2 ∈ R,

Therefore, we have that fact that 0 ≤ ∇V (x, y) ≤ w for any x, y.

|V (x, y1) − V (x, y2)| ≤ w2|y1 − y2|.

C Finite diﬀerence method for estimating the basket option of

two European call options

It is well-known that the function V (x, y) = U (0, x, y), where U satisﬁes the Black-Scholes PDE






+ rx

∂U
∂x

∂U
∂t
U (T, x, y) = (w1x + w2y − K)+.

∂U
∂y

+ ry

1
2

1x2 ∂2U
σ2

+

∂2x2 + ρσ1σ2xy

Let τ = T − t, u(τ, x, y) = U (t, x, y), then u satisﬁes






− rx

∂u
∂x

∂u
∂τ
u(0, x, y) = (w1x + w2y − K)+.

∂u
∂y

1x2 ∂2u
σ2

− ry

1
2

−

∂2x2 − ρσ1σ2xy

∂2U
∂xy

+

1
2

2y2 ∂2U
σ2

∂2y2 − rU = 0,

∂2u
∂xy

−

1
2

2y2 ∂2u
σ2

∂2y2 + ru = 0,

The above convection-diﬀusion equation can be solved numerically on a bounded region (0, xmax) × (0, ymax)
by the standard ﬁnite diﬀerence method with the artiﬁcial boundary conditions






u(τ, x, 0) = c(w1x, K, r, τ, σ1),
u(τ, 0, y) = c(w2y, K, r, τ, σ2),
∂
∂x
∂
∂y

u(τ, xmax, y) = w1,

u(τ, x, ymax) = w2,

where

c(x, K, r, τ, σ) = xΦ(d1) − Ke−rτ Φ(d2),

d1,2 =

log x

K + (r ± 1
√
σ

τ

2 σ2)τ

,

and Φ(·) is the cumulative distribution function of the standard normal distribution.

28

