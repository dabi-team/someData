Neuro-Symbolic Constraint Programming for Structured Prediction

Paolo Dragone1 , Stefano Teso2 , Andrea Passerini2
1Criteo
2University of Trento
p.dragone@criteo.com, {stefano.teso, andrea.passerini}@unitn.it

1
2
0
2

r
a

M
1
3

]

G
L
.
s
c
[

1
v
2
3
2
7
1
.
3
0
1
2
:
v
i
X
r
a

Abstract

We propose NESTER, a method for injecting neural networks
into constrained structured predictors. The job of the neu-
ral network(s) is to compute an initial, raw prediction that is
compatible with the input data but does not necessarily satisfy
the constraints. The structured predictor then builds a struc-
ture using a constraint solver that assembles and corrects the
raw predictions in accordance with hard and soft constraints.
In doing so, NESTER takes advantage of the features of its
two components: the neural network learns complex repre-
sentations from low-level data while the constraint program-
ming component reasons about the high-level properties of
the prediction task. The entire architecture can be trained in
an end-to-end fashion. An empirical evaluation on handwrit-
ten equation recognition shows that NESTER achieves better
performance than both the neural network and the constrained
structured predictor on their own, especially when training
examples are scarce, while scaling to more complex problems
than other neuro-programming approaches. NESTER proves
especially useful to reduce errors at the semantic level of the
problem, which is particularly challenging for neural network
architectures.

1

Introduction

Neural networks have revolutionized several sub-ﬁelds of
AI, including computer vision and natural language process-
ing. Their success, however, is mainly limited to “percep-
tion” tasks, where raw unstructured data is abundant and
prior knowledge is scarce or not useful (Darwiche 2018).
Standard deep networks indeed rely completely on induc-
tive reasoning to acquire complex patterns from massive
datasets. This approach cannot be straightforwardly ap-
plied to machine learning problems characterized by lim-
ited amounts of highly structured data.
Solving these
tasks requires to generalize from few examples by lever-
aging background knowledge, which in turn involves ap-
plying inductive and deductive reasoning. This observa-
tion has prompted researchers to develop neuro-symbolic
models that integrate deep learning with higher level rea-
soning (De Raedt et al. 2020; Lake et al. 2017; Zambaldi
et al. 2018; Garnelo, Arulkumaran, and Shanahan 2016;
Santoro et al. 2017).

We tackle neuro-symbolic integration in the context of
In regular

structured output prediction under constraints.

structured output prediction, the goal is to predict a struc-
tured object encoded by multiple interdependent and mutu-
ally constrained output variables (Bakir et al. 2007). Exam-
ples include parse trees, ﬂoor plans, game levels, molecules,
and any other functional objects that must obey validity con-
straints (Erculiani et al. 2018; Di Liello et al. 2020).

Our setting is more complex than regular structured pre-
diction in that it involves predicting structures i) subject to
complex logical and numerical constraints, ii) from sub-
symbolic inputs like images or sensor data. This problem is
beyond the reach of existing methods. Classical frameworks
for structured prediction (Lafferty, McCallum, and Pereira
2001; Tsochantaridis et al. 2004) lack any functionality for
representation learning, which is a key component for han-
dling sub-symbolic data. Most deep structured prediction
approaches, on the other hand, do not support explicit con-
straints on the output, neither hard nor soft (Belanger and
McCallum 2016; Amos, Xu, and Kolter 2017). Existing
neuro-symbolic approaches combining logical constraints
and deep learning either rely on fuzzy logic (Diligenti, Gori,
and Scoca 2016; Hu et al. 2016; Donadello, Seraﬁni, and
d’Avila Garcez 2017) or enforce satisfaction of constraints
only in expectation (Xu et al. 2017), in both cases failing to
deal with hard constraints. Furthermore, these approaches
are usually restricted to logical constraints. The most ex-
pressive approach in this class is DeepProbLog (Manhaeve
et al. 2018), which enriches the probabilistic programming
language ProbLog with neural predicates and is designed
to process sub-symbolic data together with logic and (dis-
crete) numeric constraints. However, probabilistic inference
in DeepProbLog can become prohibitively expensive when
dealing with structured prediction problems with a high de-
gree of non-determinism, as highlighted by our experiments.
We propose NESTER (NEuro-Symbolic consTrainEd
structuRed predictor), a hybrid approach that integrates neu-
ral networks and constraint programming to effectively ad-
dress constrained structured prediction with sub-symbolic
inputs and complex constraints. NESTER combines a neu-
ral model with a constrained structured predictor into a two-
stage approach. On the one hand, the neural network pro-
cesses low-level inputs (e.g., images) and produces a raw
candidate structure based on the data alone. This intermedi-
ate output acts as a initial guess and may violate one or more
of the hard constraints. On the other, the structured predictor

 
 
 
 
 
 
reﬁnes the network’s outputs and leverages a constraint sat-
isfaction solver to impose hard and soft constraints on the ﬁ-
nal output. Inspired by work on declarative structured output
prediction (Teso, Sebastiani, and Passerini 2017; Dragone,
Teso, and Passerini 2018a), NESTER leverages the mid-level
constraint programming language MiniZinc (Nethercote et
al. 2007) to deﬁne the constraint program, thus inheriting
the latter’s ability to deal with hard and soft constraints over
categorical and numerical variables alike. The entire archi-
tecture can be trained end-to-end by backpropagation.

We evaluated NESTER on handwritten equation recogni-
tion (Dragone, Teso, and Passerini 2018a), a sequence pre-
diction task in which the prediction must obey both syntactic
and semantic constraints, cf. Figure 1. Our results show that
the neural network and the constrained structured predictor
work in synergy, achieving better results than either model
Importantly, while the neural network
taken in isolation.
alone can rather easily learn the correct syntax for the out-
put, it struggles to achieve low error at the semantic level of
the problem. Our combined architecture, on the other hand,
outputs predictions that are both syntactically sound and se-
mantically valid.

2 Structured Prediction under Constraints
In structured prediction the goal is to predict an output struc-
ture from an input, also typically structured. Standard ex-
amples include part-of-speech tagging or protein secondary
structure prediction, where input and output are sequences
of symbols, or parse tree prediction, where a tree structure
should be predicted starting from an input sequence. Di-
rectly learning a mapping function f : X → Y is tricky
given the structured nature of Y. Approaches to deal with
the problem fall in two main categories. Energy-based
prediction models (LeCun et al. 2006) rely on a function
F : X × Y → R that computes the compatibility between
input and output. Learning aims at maximizing the score (or
minimizing the energy) of the correct output for a given in-
put, so that inference can be computed by maximizing the
function over the output space:

y = argmax

F (x, y(cid:48))

y(cid:48)∈Y

(1)

Search-based models (Daumé, Langford, and Marcu 2009;
Ross, Gordon, and Bagnell 2011) frame structured predic-
tion as a search in the space of candidate outputs, where
a scoring function evaluates the quality of a certain move
(e.g., labeling the next output in the sequence) given the cur-
rent state (the input and the labels predicted for the previous
elements). Search-based models are typically more efﬁcient
than energy-based ones, as they do not need to run inference
over the entire structure. On the other hand, by jointly infer-
ring the entire structure, energy-based models can naturally
incorporate hard constraints over the output structure. In-
deed, while soft constraints between output variables (e.g.,
the propensity for a verb to follow a noun) can be implicitly
learned by both framework in terms of weights associated
to states or input-output pairs, long-range hard constraints
(e.g., non-overlap in furniture layouts (Erculiani et al. 2018),
playability in game level generation (Di Liello et al. 2020))

are challenging for search-based models as they would re-
quire complex backtracking operations to retain satisfaction
of the constraint. The severity of the problem depends on the
scope of the constraint, with global (even soft) constraints
being much harder to manage than local ones. We thus fo-
cus on energy-based models in this paper.

Structured SVM (Tsochantaridis et al. 2004; Joachims et
al. 2009) is a popular energy-based model1, that has the ad-
vantage of carrying generalization guarantees of SVM to the
structured prediction setting. In structured SVM, F (x, y) is
modelled as a linear function of the type (cid:104)w, φ(x, y)(cid:105), where
φ : X × Y → Rd is a feature map that transforms input-
output pairs into a d-dimensional joint feature space, and
w ∈ Rd is a parameter vector to be learned from data. Given
a training set of n input-output pairs {(xi, yi)}n
i=1, where yi
is a high-quality output for xi, structured SVM learn w by
solving the following quadratic problem:

min
w,ξ

λ
2

(cid:107)w(cid:107)2 +

1
n

n
(cid:88)

i=1

ξi

s.t. ∀ i ∈ [n], y ∈ Y \ {yi}

(2)

(cid:104)w, φ(xi, yi)(cid:105) − (cid:104)w, φ(xi, y)(cid:105) ≥ ∆(yi, y) − ξi

The inequality constraints encourage the scoring function
In particu-
to rank the correct label above all the others.
lar, for each training pair (xi, yi), they ensure that the score
of the correct label (cid:104)w, φ(xi, yi)(cid:105) is larger than the score of
any other label by at least ∆(yi, y), the latter being a task-
dependent loss that measures how much y differs from yi.
These constraints are soft, and the slack variables ξ ∈ Rn
are penalties for not satisfying them. The objective to be
minimized combines the slacks with and an (cid:96)2 regulariza-
tion term λ
2 (cid:107)w(cid:107)2 that encourages simplicity and general-
ization. Many algorithms have been developed for solv-
ing Eq. 1 including cutting plane (Joachims, Finley, and Yu
2009) and block-coordinate Frank-Wolfe (Lacoste-Julien et
al. 2013). In this work we rely on stochastic sub-gradient
descent (Ratliff, Bagnell, and Zinkevich 2007), as it can be
extended to deep models, as discussed in the next section.
Stochastic sub-gradient descent for structured SVM works
by minizing the structured hinge loss:

L(w; xi, yi, y∗

i ) =

λ
2

(cid:107)w(cid:107)2 + (cid:104)w, φ(xi, y∗

i ) − φ(xi, yi)(cid:105)

where

y∗
i = argmax

y∈Y

∆(yi, y) − (cid:104)w, φ(xi, yi) − φ(xi, y)(cid:105)

Given y∗
i , it is possible to compute the sub-gradient ∇wL
and then follow its direction via a standard gradient descent
step or variants thereof.

Depending on the model structure and output space Y,
the hardness of computing a prediction (Equation 1) varies
greatly. If the output objects y ∈ Y are composed of real

1While energy-based models assume to minimize energy, struc-
tured SVM are typically deﬁned in terms of scoring function (or
negated energy) to be maximized. We stick to the standard formu-
lation in this paper.

Figure 1: Architecture of the proposed model in the handwritten equation recognition setting. The ﬁgure shows the procedure for predicting
an instance, i.e. a sequence of images forming an equation. Each image is ﬁrst classiﬁed by a CNN (the same network for each image),
and the predicted labels are grouped into a sequence ˆy. The structured predictor then takes the sequence x and the intermediate prediction ˆy
and predicts a label y by solving a structured prediction problem with the imposed constraints. Note how the network output ˆy violates both
syntactic and semantic constraints, while the ﬁnal output y is guaranteed to satisfy both. Best viewed in colors.

variables only and the space Y is convex, then a prediction
can be computed using gradient-based optimization meth-
ods. Discrete variables render the task much more challeng-
ing. Most research on structured prediction of discrete ob-
jects has focused on problems for which an efﬁcient predic-
tion algorithm is known, such as the Viterbi algorithm for se-
quences and the inside-outside algorithm for trees, or has re-
sorted to approximate inference (Finley and Joachims 2008)
that does not guarantee satisfaction of hard constraints. Fol-
lowing recent improvements of constraint solving technolo-
gies, it has become increasingly more feasible to delegate
the job of solving the inference problem in Equation 1 to
a generic constraint solver (Teso, Sebastiani, and Passerini
2017). Nowadays, mixed integer linear programming frame-
works can solve optimization problems with several hundred
variables in a matter of milliseconds. Constraint program-
ming languages such as MiniZinc (Nethercote et al. 2007)
have also been leveraged in structured prediction problems
involving complex output structures (Dragone, Teso, and
Passerini 2018a), such as layout synthesis (Erculiani et al.
2018) and product bundling recommendation (Dragone et al.
2018). Constraint programming is particularly well suited
for encoding structured prediction tasks, as it allows to eas-
ily embed background knowledge directly into the predictor
in the form of arbitrary logic and algebraic constraints. For
instance, in the equation recognition task described in this
paper, we are able to easily encode the fact that equations
must be valid by specifying constraints that transform the
sequence of labels into integers and another constraint deﬁn-
ing the relationship between these integers (more details in
Section 5).

3 Neural Constrained Structured Prediction

A substantial limitation of approaches based on structured
SVM is that they are shallow, leaving to the user the bur-
den of designing appropriate joint feature mappings φ. On
the other hand,
the ability to learn effective representa-
tions from low-level data is a key reason for the success
of deep learning technology. Indeed, deep structured pre-
diction has become predominant in domains like NLP (Ot-
ter, Medina, and Kalita 2021) which are characterized by
an abundance of labelled data. The standard solution to ap-
ply neural approaches to structured prediction problems is
that of search-based learning, where auto-regressive mod-
els like LSTM or attention-based models are employed to
predict the next output given the subset of already predicted
ones. This solution however suffers from a number of prob-
lems (Deshwal, Doppa, and Roth 2019), like the so-called
exposure bias (during training, the ground-truth is assumed
for the subset of already predicted variables) and the discrep-
ancy between the training loss (cross entropy on the label
of the next output) and the task loss (quality of the over-
all output prediction). While several ad-hoc solutions have
been developed to try addressing these problems, by adapt-
ing techniques developed for shallow search-based struc-
tured prediction (Ross, Gordon, and Bagnell 2011), these
approaches share with their shallow counterparts the lack of
support for hard constraints. Deep energy-based approaches
have also been explored (Belanger and McCallum 2016;
Gygli, Norouzi, and Angelova 2017). However they rely
on gradient-based inference in a relaxed output space, and
thus again they cannot satisfy constraints. Indeed, satisfac-
tion of constraints has been indicated as one of the major

challenges for deep structured prediction (Deshwal, Doppa,
and Roth 2019).

To solve this issue, we propose NESTER, a neural con-
strained structured prediction model. NESTER features a
general-purpose constrained structured predictor (Teso, Se-
bastiani, and Passerini 2017) (see Section 2) that acts as a
“reﬁnement” layer on top of the predictions made by a stan-
dard neural network. The constrained structured predictor is
fed both the input x ∈ X and the predictions of the neural
network ˆy ∈ Y, and outputs a ﬁnal prediction y ∈ Y. Fig-
ure 1 shows, as an example, the architecture that has been
used in our experiments on handwritten equation recogni-
tion (see Section 5), but different architectures may be used
depending on the domain. Note that the neural network does
not have to be aware of the structure of the output, and may
even simply predict the output variables independently. The
constrained structured predictor is in charge of grouping the
predictions into the desired structure while “correcting” the
mistakes of the network and enforcing the constraints of the
problem. To do so, both hard and soft constraints are en-
coded in the constraint program. The former represent hard
requirements that the output needs to satisfy in order to be
considered valid (e.g., algebraic constraints for handwritten
equation recognition). The latter represent features deﬁned
over a combination of inputs, network outputs and reﬁned
outputs, which potentially correlate with the correctness of
the reﬁnement, and whose weights are learned during train-
ing.

More speciﬁcally, prediction consists in solving the fol-

lowing constrained optimization program:
argmax
y∈Y

(cid:104)w, φ(x, y)(cid:105) + (cid:104)wρ, φρ(x, ˆy, y)(cid:105) + wδ δ(y, ˆy)

s.t.

ˆy = g(x; W )

where Y encodes the hard constraints, g(x; W ) is an ar-
bitrary neural network (ignoring the constraints), φ(x, y),
φρ(x, ˆy, y) and δ(y, ˆy) are features, and (w, wρ, wδ) and W
are learnable weights of the constrained structured predictor
and the neural network respectively. We refer to φ(x, y) as
prediction features, whereas φρ(x, ˆy, y) are named reﬁne-
ment features. The former are features that help predicting
the right output from the input, analogously to the ones used
in standard structured prediction models (Lafferty, McCal-
lum, and Pereira 2001). The latter are features that should
help spotting the common mistakes made by the network,
by relating input, output and network predictions. Finally
δ(y, ˆy) represents a distance measure between the prediction
of the structured predictor and the one of the neural network
and, depending on wδ, acts as a regularizer, preventing the
structured predictor from learning to make predictions too
dissimilar from the neural network.

Learning the combined model can be done in three steps:
1. If possible, pre-train the neural network; as mentioned,
the neural network does not have to be aware of the output
structure, so if the structured labels can be deconstructed
into the basic output components of the neural network
then the network can be pre-trained using those.

2. Train the constrained structured prediction model. At this
stage, the structured predictor is trained independently

from the neural network and it gets the predictions of the
neural network as if they were any other inputs.

3. Fine tune the constrained structured predictor and the neu-
ral network end-to-end, using stochastic sub-gradient de-
scent and back-propagating the gradient of the structured
hinge loss through the structured model and through the
network.
The last point can be achieved by simply chaining the
gradients of the structured hinge loss with respect to ˆy and
the gradient of the neural network output with respect to its
weights. For the last layer of the network with weights WK:
(cid:20) ∂L
∂ ˆy
Gradients for the other layers can be obtained by standard
backpropagation through the network, chaining gradients
over the layers starting from the last one.

∂ ˆy
∂WK

∇L =

(cid:21)

4 Neural constrained sequence prediction
In the rest of the paper, we will focus on a common kind of
structured prediction problem, namely sequence prediction.
In (discrete) sequence prediction, the objects to predict are
sequences of varying length m, i.e. y = (y1, . . . , ym), and
each element yj of the sequence takes values from an alpha-
bet of q possible symbols {sk}q
k=1. For each element of the
sequence, a vector of l input features is typically available.
We indicate with xe,i the i-th feature of the e-th element of
the input sequence.

As prediction features we employ standard features for
correlating input and output variables commonly used in e.g.
conditional random ﬁelds (Lafferty, McCallum, and Pereira
2001) for sequence prediction. These come in the form of
two vectors, and are referred as emission and transition fea-
tures. The former are encoded as:

φi,k(x, y) =

m
(cid:88)

e=1

xe,i ·

(cid:74)

ye = sk

(cid:75)

(3)

(cid:74)

·
(cid:75)

where i ranges over input features and k over output sym-
equals to 1 if the argument is
bols, and the expression
true, 0 otherwise. The resulting vector can then be used to
correlate the appearance of a speciﬁc pixel inside input im-
ages with each of the emitted symbols. The transition fea-
tures are deﬁne as follows:
m−1
(cid:88)
e=1 (cid:74)

ye = sk1 ∧ ye+1 = sk2

φk1,k2 (x, y) =

(4)

(cid:75)

where k1, k2 ranges over all the possible combinations of
couples of symbols, counting the number of times in which
two symbols appear one after the other. This formulation
enables the predictor to correlate the appearance of consec-
utive symbols inside the output sequence.

The reﬁnement features, on the other hand, need to corre-
late mistakes of the network with the input and output vari-
ables. For this sequence prediction task, we deﬁne the re-
ﬁnement features vector φρ as:

φi,k(x, ˆy, y) =

m
(cid:88)

e=1

xe,i ·

ye = sk ∧ ˆye (cid:54)= sk
(cid:74)

(cid:75)

(5)

Figure 2: Percentage of erroneously predicted equations by the CNN (left) and by the CNN + LSTM (right) on the test set with increasing
training set size. The plots show the percentage of total errors over the test set size and its decomposition into syntactic and semantic errors.
Best viewed in colors.

where i ranges over input features and k over output sym-
bols. These features basically correlate the values of each in-
put feature with the overriding of neural network predictions
by the reﬁnement layer. By learning appropriate weights for
them, the structured predictor can learn to ﬁx the most com-
mon mistakes made by the neural network. Finally, we de-
ﬁne δ(y, ˆy) as the Hamming distance between y and ˆy:

δ(y, ˆy) =

m
(cid:88)
e=1(cid:74)

ye (cid:54)= ˆye

(cid:75)

Table 1 contains an overview of all the components of the
constrained problem we used in our experiments.

5 Experiments
We tested our technique on the handwritten equation recog-
nition task described in (Dragone, Teso, and Passerini
2018a). All our experiments are implemented using Tensor-
ﬂow and Pyconstruct (Dragone, Teso, and Passerini 2018a).
MiniZinc (Nethercote et al. 2007) was used as constraint
programming engine and Gurobi2 as the underlying con-
straint solver. The code of NESTER will be made available
upon acceptance.

5.1 Setting
Each input object is a sequence of images of variable length,
and the corresponding output is a sequence of symbols of the
same length. The possible symbols include the digits from 0
to 9, + and =. The equations are all of the form a + b = c,
where a, b, c are arbitrary positive integers (of a predeﬁned
maximum number of digits), and the equations are all valid,
meaning that it is always true that a plus b equals c. This
is the background knowledge of the problem. Images are

2http://gurobi.com.

matrices of 9 × 9 black and white pixels. The dataset was
constructed by assembling 10,000 valid equations from the
ICFHR’14 CROHME competition data. All our experiments
were run on the same train-test split, consisting of 8,000
equations for training and 2,000 for testing. To highlight
the behavior of the models with different amounts of train-
ing examples, we divided the training set into 20 chunks of
increasing size and report results over the test set for each
training set chunk.

5.2 Neural model
As highlighted in Figure 1, we use a CNN to predict the sym-
bol of each image in the sequence independently. The CNN
is composed of two convolutional layers with 3×3 ﬁlters and
ReLU activation, each followed by a 2 × 2 max-pool layers,
then a 128 dense layer with dropout regularization (0.5 prob-
ability), and ﬁnally a softmax layer with 12 outputs, one for
each symbol. The CNN is trained using Adam (Kingma and
Ba 2015) with a cross entropy loss.

The left plot in Figure 2 reports the relative misclassiﬁ-
cation error of the CNN over the test set, i.e.
the percent-
age of sequences for which the network made at least one
mistake. The violet line indicates the total percentage of er-
rors across the various training set chunks. As the network
does not enforce any constraint on the output, it is subject
to errors resulting from predicting inconsistent output ob-
jects. We subdivide the constraint satisfaction errors into
syntactic errors, for which the prediction is not properly for-
matted according to the template a + b = c, and semantic
errors, i.e. well-formatted predictions for which the result-
ing equation is not valid, that is a plus b does not equal c.
Occasionally the network makes mistakes that are not due
to unsatisﬁed constraints, but these are a very slim minority.
The shaded areas in Figure 2 show how the different types
of errors contribute to the total. While the total number of

errors decreases with increasing training set size, the pro-
portion between syntactic and semantic errors settles around
33% to 67%, indicating that semantic errors are generally
harder to correct. To make sure that these errors are not sim-
ply due to the fact that the CNN predicts each output symbol
independently, we also experimented with a recurrent archi-
tecture. The recurrent network is assembled by stacking over
each convolutional layer of our CNN an LSTM layer. Dif-
ferently from the CNN, the network is trained with whole
input-output sequence pairs.

The right plot in Figure 2 shows the errors of this archi-
tecture and their decomposition into syntactic and seman-
tic types. Overall, the network made more errors, possi-
bly due to the increased number of trainable parameters
and thus the necessity for more examples to be properly
trained. Relatively speaking, though, the CNN + LSTM
network learned very quickly how to ﬁx syntactic errors,
even faster than the CNN. Yet, semantic errors were much
more difﬁcult to correct, and they ended up being the large
majority of the errors of the network. These results show
that even highly-expressive neural network architectures can
have problems in going beyond the shallow syntactic level
and reach a deeper semantic level of understanding, espe-
cially when limited data is available.

5.3 Constrained structured model
As constrained structured layer (“CST” from now on)
we used an enhanced version of the constrained model
from (Dragone, Teso, and Passerini 2018a). Not being con-
ceived to reﬁne predictions from other modules, the original
model only contains prediction features φ(x, y). Our model
extends the original one by taking the prediction of the neu-
ral network ˆy as input, combining the prediction features
with the reﬁnement features described in Equation 5, and
including the Hamming distance between y and ˆy (see Fig-
ure 1). A summary of the features and constraints used in
the structured predictor is given in Table 1.

The deﬁnition of the constrained structured model is pre-
sented in the MiniZinc code listed in Figure 3. The sequence
of images composing the equation is given as input. The out-
put consists of a sequence of symbols equal to the length of
the input sequence, each symbol being an integer between
zero and eleven (0-9 for the digits, 10 and 11 for the + and
= operators respectively). The input also contains the se-
quence of symbols predicted by the CNN.

Syntactic constraints over the output are encoded in lines
23 and 24. They enforce the presence of a single = and a
single + in the output sequence, plus the constraint that the
+ symbol should come before the = one. The next code
section (from line 26 to 42) is devoted to converting the se-
quence of output symbols into an actual algebraic formula.
This procedure starts in line 27 with the deﬁnition of the
extremes of the three numbers, that are given by the two
operators. The extremes are used to count the number of
digits that compose each of the three numbers (lines 28-29).
These two pieces of information, coupled with the sequence
predictions, are then used in lines 32-37 to populate a 3 × 3
matrix, where each row represents one of the three numbers,
and the columns represent the digits, sorted from the most

Table 1: Summary of the components of the constrained structured
predictor in the handwritten equation recognition setting. The ex-
pression

equals to 1 if the argument is true, 0 otherwise.

·
(cid:74)

(cid:75)

Input x, ˆy

◦ Array of m 9 × 9 images, of {0, 1} pixels

x ∈ {0, 1}m×9×9

where

xe,i,j ∈ {0, 1}

(i, j)-th pixel of the e-th image

◦ Array of m predictions of the neural network (NN)

ˆy ∈ {0, . . . , q}m

where

q = 11 =⇒ digits: 0-9, plus (+): 10, equals (=): 11
ˆye ∈ {0, ..., q} NN prediction for the e-th image

Output y

◦ Array of m output symbols

y ∈ {0, ..., q}m

where

q = 11 =⇒ (same as above)
ye ∈ {0, ..., q}

output symbol for the e-th image

Prediction features φ(x, y)

◦ Emission features: appearance of a pixel in images of a class,

φi,j,k(x, y) = (cid:80)m

e=1 xe,i,j ·

where

ye = k
(cid:74)

(cid:75)

k ∈ {0, . . . , 11} ranges over symbols
(i, j) ∈ {0, . . . , 9}2 range over pixels

◦ Transition features: classes in contiguous images,
ye = k1 ∧ ye+1 = k2
(cid:74)

φk1,k2 (x, y) = (cid:80)m−1

e=1

(cid:75)

where

(k1, k2) ∈ {0, . . . , 11}2 range over pairs of symbols

Reﬁnement features φρ(x, ˆy, y)
◦ Disagreement between the network prediction and the output,
e=1 xe,i,j ·

φi,j,k(x, ˆy, y) = (cid:80)m

ye = k ∧ ˆye (cid:54)= k
(cid:74)

(cid:75)

where

k ∈ {0, . . . , 11} ranges over symbols
(i, j) ∈ {0, . . . , 9}2 range over pixels

Distance measure δ(ˆy, y)

◦ Hamming distance between output and network prediction

δ(y, ˆy) = (cid:80)m

e=1

ye (cid:54)= ˆye
(cid:74)

(cid:75)

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

%% ~~~~~~~~~~~~~~~~~~~~~~~~~~ INPUT/OUTPUT ~~~~~~~~~~~~~~~~~~~~~~~~ %%
%% Max digits per number, dimension of the images (9 x 9)
int: DIGITS = 3;
set of int: HEIGHT = 1 .. 9; set of int: WIDTH = 1 .. 9;

%% Assume "+" is encoded by 10 and "=" is encoded by 11
set of int: SYMBOLS = 0 .. 11; int: PLUS = 10; int: EQ = 11;

%% INPUT: sequence length, tensor of images, CNN prediction
int: len; set of int: SEQ = 1 .. len;
array[SEQ, HEIGHT, WIDTH] of {0, 1}: img;
array[SEQ] of SYMBOLS: cnn_seq;

%% OUTPUT: sequence of symbols
array[SEQ] of var SYMBOLS: seq;

%% ~~~~~~~~~~~~~~~~~~~~~~~~ DOMAIN KNOWLEDGE~~~~~~~~~~~~~~~~~~~~~~~ %%
%% Indices of the two operators
array[1 .. 2] of var 2 .. (length - 1): opr;

%% Syntactic constraints: exactly one "+" and one "=", "+" before "=",
%% positions corresponding to indices "opr" in the sequence
constraint count(seq, PLUS, 1) /\ count(seq, EQ, 1);
constraint increasing(opr) /\ seq[opr[1]] == PLUS /\ seq[opr[2]] == EQ;

%% Extremes and number of digits of the three numbers
array[1 .. 4] of var 0 .. len + 1: ext = [0, opr[1], opr[2], len + 1];
array[1 .. 3] of var 1 .. DIGITS: n_digits = [

ext[i + 1] - ext[i] - 1 | i in 1 .. 3 ];

signiﬁcant to the least signiﬁcant. Each number can be com-
posed at most by 3 digits, if the digits are less than 3, the
number is left padded with the appropriate number of ze-
ros. The matrix is then transformed into 3 integer values
a, b and c (lines 40-42), summing up all digits of each row,
multiplied by the appropriate power of 10, according to their
relative position. In formula, each row i is converted into an
integer ni = (cid:80)
10j · di,j, where mi is the number of
digits in number i and di,j is the j-th digit of the i-th number,
from the least to the most signiﬁcant digit. This conversion
allows us to impose in line 45 the semantic constraint en-
forcing the output sequence to encode a valid formula, i.e.
a + b = c.

0≤j<mi

The last section of the code is devoted to declaring
the features, with the formulation introduced in Section 3
(from line 47 to 62). This code is a template that Pycon-
struct (Dragone, Teso, and Passerini 2018a) uses to solve
different inference problems during training and prediction.
At runtime, the library takes care of determining which
objective function to optimize depending on the inference
problem at hand.

%% Matrix of digits, zero-padded on the left if less than three digits
array[1 .. 3, 1 .. DIGITS] of var SYMBOLS: digits =

5.4 The combined model

array2d(1 .. 3, DIGITS, [

if j <= (DIGITS - n_digits[i]) then 0 else
seq[ext[i] + j + n_digits[i] - DIGITS]

endif

| i in 1 .. 3, j in 1 .. DIGITS ]);

%% The three numbers, sum digits for corresponding power of 10
array[1 .. 3] of var int: num = [

sum(j in 1 .. DIGITS)(pow(10, DIGITS - j) * digits[i, j])

| i in 1 .. 3 ];

%% Semantic constraint: the equation must be valid
constraint num[1] + num[2] == num[3];

%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FEATURES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ %%
%% EMISSION + TRANSITION + REFINEMENT + DISTANCE
int: N_FEATURES = 9*9*12 + 12*12 + 9*9*12 + 1;

array[1 .. N_FEATURES] of var int: features =

%% EMISSION FEATURES
[ sum(e in SEQ)(img[e, i, j] * (seq[e] == s))

| i in HEIGHT, j in WIDTH, s in SYMBOLS ]

%% TRANSITION FEATURES

++ [ sum(e in 1..len - 1)(seq[e] == s1 /\ seq[e + 1] == s2)

| s1, s2 in SYMBOLS ]
%% REFINEMENT FEATURES

++ [ sum(i in SEQ)(img[i, j, k] * (seq[i] == s) * (cnn_seq[i] != s))

| j in HEIGHT, k in WIDTH, s in SYMBOLS ]

%% HAMMING DISTANCE

++ [ -sum(i in SEQ)(cnn_seq[i] != seq[i]) ];

Figure 3: A sketch of the MiniZinc program for the constrained
structured model for the equation recognition problem. The MiniZ-
inc program encodes the input images tensor, and neural network
predictions (lines 10-12) and the output sequence of symbols (line
15), the domain knowledge in the form of hard constraints ensur-
ing the equation is properly syntactically formatted (lines 18-29)
and enforcing the semantic validity of the equation (lines 31-45),
and ﬁnally the features of the structured model as deﬁned in Equa-
tion 3, prediction features (lines 52-57), reﬁnement features (lines
58-60) and distance metric (lines 61-62), whose weights are learn
from data. For brevity, the ﬁgure omits some parts regarding the
deﬁnition of the weights and the objective function. Best viewed in
color.

After training the convolutional network, we combine it with
the CST as described in Section 3.

We train the CST model using the structured SVMs
method with stochastic subgradient descent (Ratliff, Bag-
nell, and Zinkevich 2007) for one epoch over the training
set. The structured loss used in this task is the Hamming
distance between the predicted sequence of symbols and the
true sequence.

Figure 4 shows the learning curves resulting from the
experiments using the Hamming loss.
In the left plot we
compare the combined model (CNN + CST) with the sin-
gle components, i.e., the CNN and the CST in the origi-
nal variant (Dragone, Teso, and Passerini 2018a), and the
CNN + LSTM cascade. The general trend is rather clear,
the CNN has always an edge over the CST, yet their com-
bination always performs better than both. As expected, the
gap between the CNN and the combined model is maximal
for the smallest dataset, but it remains clearly evident when
the curves start to level off (82.9% relative error reduction at
the last iteration). The loss of the CNN + LSTM model is
overall slightly higher than the one of the CNN model, con-
sistent with what happens for the misclassiﬁcation error (see
Figure 2).

We then computed the breakdown of the CNN + CST re-
sults by features of the CST model (see Table 1). When
considering distance only, we set its weight wδ to a negative
value, so as to minimize the distance between the prediction
of the network and the reﬁned prediction while satisfying the
constraints, without any learning on the CST side. We then
decomposed the features of the full combined model into: [i]
reﬁnement; [ii] reﬁnement + distance; [iii] reﬁnement + pre-
diction. The right plot in Figure 4 shows the learning curves
for each of these variants The distance feature is clearly the
least effective, even if it already improves over the CNN
alone (29.6% relative error reduction at the last iteration).

Figure 4: Learning curves for the different setups of our architecture. The plots show how the structural loss (Hamming distance) on the test
set changes with increasing size of the training set. On the left, our proposed technique (red curve) is compared against its basic components
on their own, namely the CNN (blue curve) and the CST (green curve), and against a CNN+LSTM cascade (light blue curve). On the right, a
summary of the contributions of different combinations of features is shown. Best viewed in color.

The other features perform quite similarly, but combining
all features together achieves the overall best performance.

5.5 Comparison with DeepProbLog

As stated in the introduction,
the DeepProbLog frame-
work (Manhaeve et al. 2018) is capable of combining neu-
ral processing with the management of logic and numeric
constraints, and can in principle address a recognition task
like the one presented here. Indeed, the framework proved
capable of learning to add numbers represented as MNIST
images (Manhaeve et al. 2018). The equation recognition
task we address here, however, is more complex, as the
numbers in the equation have variable length and the po-
sition of the operators is not known a-priori. Modelling
this problem with DeepProbLog requires the use of non-
deterministic predicates, which are extremely expensive. In-
deed, we attempted to encode the equation recognition prob-
lem in DeepProbLog3, but inference turned out to be too
memory expensive to be even executed.

6 Related work
Statistical relational learning. Statistical relational learn-
ing (SRL) (Getoor and Taskar 2007) approaches inject prior
knowledge, usually expressed in some logical formalism,
into statistical learning models, with a particular empha-
sis on probabilistic graphical models. In SRL, the logical
framework (either propositional or ﬁrst order logic) is used
to deﬁne the structure of the data, i.e. the known or likely re-
lationships between the variables, which are then weighted
and reﬁned by inductive learning from data. Examples
approaches include Relational Bayesian Networks (Jaeger

3The main developer of DeepProbLog helped us in trying to

optimize the encoding.

1997), Markov Logic Networks (Richardson and Domin-
gos 2006), and ProbLog (De Raedt, Kimmig, and Toivonen
2007). These approaches are not designed for sub-symbolic
inputs.

like conditional

Structured prediction. Traditional approaches to struc-
tured prediction,
random ﬁelds (Laf-
ferty, McCallum, and Pereira 2001) and structured output
SVMs (Tsochantaridis et al. 2004) discriminatively learn an
energy or scoring function over candidate input-output pairs
based on a joint input-output feature map. Prior knowl-
edge can be injected into these models through proposi-
tional (hard and soft) constraints (Kristjansson et al. 2004;
Fersini et al. 2014; Teso, Sebastiani, and Passerini 2017).
MAP inference is solved using either ad-hoc constrained in-
ference techniques or general solvers for combinatorial or
discrete-continuous problems (Roth and Yih 2005; Teso, Se-
bastiani, and Passerini 2017; Dragone, Teso, and Passerini
2018b). These techniques, however, require one to pre-
specify the relevant features (or kernel), which was shown
to be suboptimal in many tasks compared to representation
learning strategies.

Neural structured prediction. More recently energy-
based structured prediction has been tackled using deep
learning models. Notable examples include deep value net-
works (Gygli, Norouzi, and Angelova 2017), structured pre-
diction energy networks (SPENs) (Belanger and McCallum
2016) and input convex neural networks (Amos, Xu, and
Kolter 2017). These techniques implement the energy func-
tion using a neural network and use gradient methods to in-
fer a high-quality candidate output. In contrast to their shal-
low alternatives, deep structured models do not straightfor-
wardly support the addition of prior knowledge through con-

straints. The work by Lee et al. (Lee et al. 2017) addresses
this issue without resorting to combinatorial search, by cast-
ing the constrained inference of a SPEN into an instance-
speciﬁc learning problem with a constraint-based loss func-
tion. Unlike NESTER, this method is not guaranteed to sat-
isfy all the constraints, especially for a complex mix of al-
gebraic and logical ones.

Neuro-symbolic integration. Effectively combining rea-
soning with deep networks is still an ongoing research effort.
Many attempts have been made to address this issue, most
of which focus on forcing the network to learn weights that
ultimately produce predictions satisfying the constraints. A
popular line of research integrates constraints into the objec-
tive function using fuzzy logic, especially the Łukasiewicz
T-norm (Diligenti, Gori, and Scoca 2016; Hu et al. 2016;
Donadello, Seraﬁni, and d’Avila Garcez 2017). These ap-
proaches however cannot guarantee that the predicted out-
puts satisfy the hard constraints. The same is true for the
semantic loss, which encourages the output of a neural net-
work to satisfy given constraints with high probability (Xu
et al. 2017; Di Liello et al. 2020). Furthermore, these ap-
proaches are usually restricted to Boolean variables and log-
ical constraints. DeepProbLog (Manhaeve et al. 2018) ex-
tends the ProbLog language (De Raedt, Kimmig, and Toivo-
nen 2007) with learnable neural predicates. This powerful
framework allows to reason about semantic properties of
the output variables while using neural networks for low-
level inputs. By building on top of a probabilistic program-
ming framework, it inherits its ability to answer probabilistic
queries other than MAP inference, something NESTER can-
not do. On the other hand, DeepProbLog cannot be used to
model a complex structured-output prediction problem efﬁ-
ciently, as discussed in our experimental evaluation.

Declarative structured prediction. Structured learning
modulo theories (Teso, Sebastiani, and Passerini 2017) is a
structured learning framework dealing with (soft and hard)
constraints expressed in a declarative fashion using the
SMT formalism and the MiniZinc constraint programming
language. The framework has been applied to both pas-
sive (Teso, Sebastiani, and Passerini 2017; Dragone, Teso,
and Passerini 2018a) and interactive learning settings (Drag-
one, Teso, and Passerini 2018b). Learning modulo theories
shares with NESTER the ability to handle hybrid discrete-
continuous combinatorial problems. On the other hand its
underlying model is a shallow structured SVM and it thus
lacks a representation learning component to deal with low-
level inputs. NESTER can be seen as a neuro-symbolic ver-
sion of this framework.

Other works. Some recent approaches have tackled prob-
lems at the intersection of constrained optimization and deep
learning. For instance, in predict-then-optimize (Elmach-
toub and Grigas 2017) the goal is to predict the parame-
ters of a constrained optimization model (e.g., a schedul-
ing problem) from data, in some cases using a deep neu-
ral network (Mandi et al. 2020; Mandi and Guns 2020). In

predict-then-optimize, however, the ground-truth value of
the parameters to be predicted (for instance, the cost vec-
tor of a linear program) is available as supervision, whereas
in structured output prediction no supervision is given on
the parameters of the constraint program. This makes the
two problems conceptually very different. Two other re-
lated threads of research are constraint learning (De Raedt,
Passerini, and Teso 2018) and inverse optimization (Tan,
Delong, and Terekhov 2019; Tan, Terekhov, and Delong
2020), which are also concerned with learning constrained
optimization problems from examples. These families of
approaches, however, are not tailored for acquiring neuro-
symbolic programs and it would be non-trivial to extend
them to this purpose.

7 Conclusion
We developed a structured output prediction system com-
bining neural networks and constraint programming, with
the aim of jointly leveraging the effectiveness of neural net-
works in processing raw data with the ability of constraint
programming to deal with a wide range of constraints over
Boolean and numerical data. The system is conceived for
structured prediction tasks where examples are scarce and
prior knowledge is abundant. We tested our approach on
one such task, namely handwritten equation recognition. A
preliminary experiment showed how CNN and even convo-
lutional LSTM, while quickly learning to correct syntactic
errors, have serious problems in coping with semantic errors.
Our combined approach, in addition to producing consistent
predictions by design, is able to improve recognition per-
formance over both the neural network and the constrained
structured model on their own, especially with smaller train-
ing sets.

We posit that our approach could be applied to several im-
portant problems in which prior knowledge is key to their
solution, such as dialogue management (Lison 2015), re-
inforcement learning in complex environments (Garnelo,
Arulkumaran, and Shanahan 2016), and interactive recom-
mendation systems (Dragone, Teso, and Passerini 2018b),
all of which we intend to pursue as future work.

Acknowledgments
We would like to thank Carlo Nicolò for running preliminary
equation recognition experiments, Edoardo Battocchio for
running experiments on a preliminary end-to-end pipeline,
and Robin Manhaeve for helping us in trying to optimize the
DeepProbLog encoding for the equation recognition prob-
lem. The research of ST and AP was partially supported
by TAILOR, a project funded by EU Horizon 2020 research
and innovation programme under GA No 952215.

References
Amos, B.; Xu, L.; and Kolter, J. 2017. Input convex neural
networks. In ICML.
Bakir, G.; Hofmann, T.; Schölkopf, B.; Smola, A.; Taskar,
B.; and Vishwanathan, S. 2007. Predicting Structured Data.
Belanger, D., and McCallum, A. 2016. Structured prediction
energy networks. In ICML.

Darwiche, A. 2018. Human-level intelligence or animal-like
abilities? Communications of the ACM 61(10):56–67.
Daumé, H.; Langford, J.; and Marcu, D. 2009. Search-based
structured prediction. Mach. Learn. 75(3):297–325.
De Raedt, L.; Dumanˇci´c, S.; Manhaeve, R.; and Marra, G.
2020. From Statistical Relational to Neuro-Symbolic Artiﬁ-
cial Intelligence. arXiv preprint arXiv:2003.08316.
De Raedt, L.; Kimmig, A.; and Toivonen, H. 2007. Problog:
A probabilistic prolog and its application in link discovery.
In IJCAI, volume 7, 2462–2467. Hyderabad.
De Raedt, L.; Passerini, A.; and Teso, S. 2018. Learning
constraints from examples. In AAAI.
Deshwal, A.; Doppa, J. R.; and Roth, D. 2019. Learning and
inference for structured prediction: A unifying perspective.
In IJCAI.
Di Liello, L.; Ardino, P.; Gobbi, J.; Morettin, P.; Teso, S.;
and Passerini, A. 2020. Efﬁcient generation of structured
objects with constrained adversarial networks. Advances in
neural information processing systems.
Diligenti, M.; Gori, M.; and Scoca, V. 2016. Learning efﬁ-
ciently in semantic based regularization. In ECML-PKDD.
Donadello, I.; Seraﬁni, L.; and d’Avila Garcez, A. 2017.
Logic tensor networks for semantic image interpretation. In
IJCAI.
Dragone, P.; Pellegrini, G.; Vescovi, M.; Tentori, K.; and
Passerini, A. 2018. No more ready-made deals: Construc-
tive recommendation for telco service bundling. In Proceed-
ings of the 12th ACM Conference on Recommender Systems,
RecSys ’18, 163–171. New York, NY, USA: Association for
Computing Machinery.
Dragone, P.; Teso, S.; and Passerini, A. 2018a. Pyconstruct:
Constraint programming meets structured prediction. In IJ-
CAI.
Dragone, P.; Teso, S.; and Passerini, A. 2018b. Constructive
preference elicitation. Frontiers in Robotics and AI 4.
Elmachtoub, A. N., and Grigas, P. 2017. Smart “Predict,
then Optimize”. arXiv preprint arXiv:1710.08005.
Erculiani, L.; Dragone, P.; Teso, S.; and Passerini, A.
2018. Automating layout synthesis with constructive prefer-
ence elicitation. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases.
Fersini, E.; Messina, E.; Felici, G.; and Roth, D. 2014.
Soft-constrained inference for named entity recognition. In-
formation Processing & Management.
2008. Training structural
Finley, T., and Joachims, T.
In Proceedings
svms when exact inference is intractable.
of the 25th International Conference on Machine Learning,
304–311.
Garnelo, M.; Arulkumaran, K.; and Shanahan, M. 2016. To-
wards deep symbolic reinforcement learning. arXiv preprint
arXiv:1609.05518.
Getoor, L., and Taskar, B. 2007. Introduction to statistical
relational learning.
Gygli, M.; Norouzi, M.; and Angelova, A. 2017. Deep value

networks learn to evaluate and iteratively reﬁne structured
outputs. In ICML.
Hu, Z.; Ma, X.; Liu, Z.; Hovy, E.; and Xing, E. 2016. Har-
nessing deep neural networks with logic rules. In ACL.
Jaeger, M. 1997. Relational bayesian networks. In UAI.
Joachims, T.; Hofmann, T.; Yue, Y.; and Yu, C.-N. 2009.
Predicting structured objects with support vector machines.
Communications of the ACM 52(11).
Joachims, T.; Finley, T.; and Yu, C. 2009. Cutting-plane
training of structural svms. Machine Learning.
Kingma, D. P., and Ba, J. 2015. Adam: A method for
stochastic optimization. In 3rd International Conference on
Learning Representations, ICLR.
Kristjansson, T.; Culotta, A.; Viola, P.; and McCallum, A.
Interactive information extraction with constrained
2004.
conditional random ﬁelds. In AAAI.
Lacoste-Julien, S.; Jaggi, M.; Schmidt, M.; and Pletscher, P.
2013. Block-coordinate frank-wolfe optimization for struc-
tural svms. In ICML.
Lafferty, J.; McCallum, A.; and Pereira, F. 2001. Condi-
tional random ﬁelds: Probabilistic models for segmenting
and labeling sequence data. In ICML.
Lake, B.; Ullman, T.; Tenenbaum, J.; and Gershman, S.
2017. Building machines that learn and think like people.
Behavioral and Brain Sciences.
LeCun, Y.; Chopra, S.; Hadsell, R.; Huang, F. J.; and et al.
In Predicting
2006. A tutorial on energy-based learning.
Structured Data. MIT Press.
Lee, J.; Wick, M.; Tristan, J.; and Carbonell, J. 2017. En-
forcing output constraints via sgd: A step towards neural
lagrangian relaxation. In NIPS.
Lison, P. 2015. A hybrid approach to dialogue management
based on probabilistic rules. Computer Speech & Language.
Mandi, J., and Guns, T. 2020.
Interior Point Solving for
LP-based prediction+ optimisation. In NeurIPS.
Mandi, J.; Stuckey, P. J.; Guns, T.; et al. 2020. Smart predict-
and-optimize for hard combinatorial optimization problems.
In AAAI, volume 34.
Manhaeve, R.; Dumanˇci´c, S.; Kimmig, A.; Demeester, T.;
and De Raedt, L. 2018. DeepProblog: Neural probabilistic
logic programming. In NeurIPS.
Nethercote, N.; Stuckey, P.; Becket, R.; Brand, S.; Duck,
G.; and Tack, G. 2007. Minizinc: Towards a standard cp
modelling language. In CP.
Otter, D. W.; Medina, J. R.; and Kalita, J. K. 2021. A survey
of the usages of deep learning for natural language process-
ing. IEEE Trans. Neural Networks Learn. Syst. 32(2):604–
624.
Ratliff, N.; Bagnell, J.; and Zinkevich, M. 2007. (approxi-
mate) subgradient methods for structured prediction. In AIS-
TATS.
Richardson, M., and Domingos, P. 2006. Markov logic
networks. Machine learning.

Integer linear programming

Ross, S.; Gordon, G.; and Bagnell, D. 2011. A reduction
of imitation learning and structured prediction to no-regret
online learning. In Proceedings of the Fourteenth Interna-
tional Conference on Artiﬁcial Intelligence and Statistics,
627–635.
Roth, D., and Yih, W. 2005.
inference for conditional random ﬁelds. In ICML.
Santoro, A.; Raposo, D.; Barrett, D.; Malinowski, M.; Pas-
canu, R.; Battaglia, P.; and Lillicrap, T. 2017. A simple
neural network module for relational reasoning. In NIPS.
Tan, Y.; Delong, A.; and Terekhov, D. 2019. Deep inverse
optimization. In International Conference on Integration of
Constraint Programming, Artiﬁcial Intelligence, and Oper-
ations Research.
Tan, Y.; Terekhov, D.; and Delong, A. 2020. Learning linear
programs from optimal decisions. In NeurIPS.
Teso, S.; Sebastiani, R.; and Passerini, A. 2017. Structured
learning modulo theories. Artiﬁcial Intelligence 244.
Tsochantaridis, I.; Hofmann, T.; Joachims, T.; and Altun, Y.
2004. Support vector machine learning for interdependent
and structured output spaces. In ICML.
Xu, J.; Zhang, Z.; Friedman, T.; Liang, Y.; and Van den
Broeck, G. 2017. A semantic loss function for deep learning
with symbolic knowledge. In ICML.
Zambaldi, V.; Raposo, D.; Santoro, A.; Bapst, V.; Li, Y.;
Babuschkin, I.; Tuyls, K.; Reichert, D.; Lillicrap, T.; and
Lockhart, E. 2018. Relational deep reinforcement learning.
arXiv preprint arXiv:1806.01830.

