IntegerProgramming-basedError-CorrectingOutputCodeDesignforRobustClassiﬁcationSamarthGuptaSaurabhAminsamarthg@mit.eduMassachusettsInstituteofTechnologyamins@mit.eduMassachusettsInstituteofTechnologyAbstractError-CorrectingOutputCodes(ECOCs)of-feraprincipledapproachforcombiningsim-plebinaryclassiﬁersintomulticlassclassi-ﬁers.Inthispaper,weinvestigatetheprob-lemofdesigningoptimalECOCstoachievebothnominalandadversarialaccuracyus-ingSupportVectorMachines(SVMs)andbinarydeeplearningmodels.Incontrasttopreviousliterature,wepresentanIntegerPro-gramming(IP)formulationtodesignmini-malcodebookswithdesirableerrorcorrectingproperties.OurworkleveragestheadvancesinIPsolverstogeneratecodebookswithopti-malityguarantees.Toachievetractability,weexploittheunderlyinggraph-theoreticstruc-tureoftheconstraintsetinourIPformulation.Thisenablesustouseedgecliquecoverstosubstantiallyreducetheconstraintset.Ourcodebooksachieveahighnominalaccuracyrelativetostandardcodebooks(e.g.,one-vs-all,one-vs-one,anddense/sparsecodes).WealsoestimatetheadversarialaccuracyofourECOC-basedclassiﬁersinawhite-boxset-ting.OurIP-generatedcodebooksprovidenon-trivialrobustnesstoadversarialpertur-bationsevenwithoutanyadversarialtraining.1IntroductionErrorCorrectingOutputCodes(ECOCs)oﬀeraneﬀectiveandﬂexibletooltocombineindividuallytrainedbinaryclassiﬁersformulticlassclassiﬁca-tion.Priorresearch[DietterichandBakiri,1995,Allweinetal.,2000]hasshownthatECOCscanpro-videhighmulticlassclassiﬁcationaccuracyusingsimplebutpowerfulbinaryclassiﬁers(e.g.,SupportVectorMachinesandAdaboost).Ontheotherhand,extensivebodyofworkhasemergedinrecentyearsshowingthat,whenlargeamountoftrainingdataisavailable,deeplearningmodels[LeCunetal.,2015]outperformmostmulticlassclassiﬁers.Still,furtherprogressisneededforclassiﬁcationtaskswhentrainingdataislimitedorconstrained,andmodelinterpretabilityispreferred.Inthispaper,weconsidertheproblemofECOC-basedmulticlassclassiﬁcation,whenindividualbinaryclas-siﬁersareSVMsordeeplearningmodels.Wefocusonthequestionofdesignofcodebooks–alongwithoptimalityguarantees.Importantly,ourapproachtocodebookdesignisdistinctfromthepriorliterature,whichap-proachestheproblemusingacontinuousrelaxationoftheinherentlydiscreteoptimizationproblem,andsolvingtherelaxedproblemusingnonlinearoptimizationtools[CrammerandSinger,2002,ZhaoandXing,2013,XiaoZhangetal.,2009,Martinetal.,2018].Inprinciple,thisapproachcanbescaledtoalargenumberofclasses,butitdoesnotprovideanyoptimalityguarantees.Anotherap-proachintheliterature[DietterichandBakiri,1995]caststhedesignproblemasapropositionalsatisﬁabilityproblemthatcanbesolvedforusingoﬀ-the-shelfSATsolvers.However,onlyafeasiblesolutionmaybereadilycomputableusingthisapproach.Incontrast,weformulatetheoptimalcodebookdesignproblemasalarge-scaleIntegerProgram(IP),andexploitthestructureoftheproblemtoobtainacompactformulationthatcanbesolvedwithmodernIPsolvers.Ourresultingcodebookhasoptimalityguarantees.Thisalsoenablesasystematiccomparisonwithrespecttoseveralwell-knownﬁxed-sizecodebooks.OurIPformulationisﬂexibleinthatitmodelsvariouscodebook(orcodingmatrix)generationcriteria:(i)SuﬃcientlylargeHammingdistancebetweenanypairofcodewords(rowseparation);(ii)Uncorrelatedcolumns(columnseparation);(iii)Relativelyevendistributionofdatapointsacrosstwoclasses(balancedcolumns);and(iv)LargerHammingdistancebetweenpairofcodewordswhosecorrespondingclassesarehardtoseparatefromoneanother.ThesecriteriaareimportantarXiv:2011.00144v1  [cs.LG]  30 Oct 2020IntegerProgramming-basedError-CorrectingOutputCodeDesignforRobustClassiﬁcationnotonlyforthenominalerrorcorrectionperformance,butalsopromoteadversarialrobustness.However,thisinitialformulationcanquicklybecomeintractableforaclassiﬁcationproblemofmorethan10classes.Toaddresstheabovementionedcomputationalbottle-neck,weexploittheinherentgraph-theoreticfeatureoftheconstraintsthatpertaintoselectinganappropriatesubsetofcolumns.Inparticular,weprovethattheconstraintsmodelingthepairofcolumnsthatdonotsatisfycolumnseparationcriterioncanbereplacedbyamuchsmallersetformedbyanedgecliquecoveroftheunderlyinggraph.Thisresultallowsustorefor-mulateouroriginalproblemintoanotherIPwithasubstantiallysmallersetofconstraints.Adistinctadvantageofourdesignapproachisthatitgeneratesrelativelysmallcodebooks,whichachieveahighnominalaccuracyaswellasrobust-nesstoadversarialperturbations[Szegedyetal.,2013,Goodfellowetal.,2015,Suetal.,2017].Inparticu-lar,wedemonstratethatourIP-generatedcodebooksoutperformthewell-knowncodebookssuchasone-vs-allandone-vs-one,andotherdenseorsparsede-signs.Toevaluatetherobustnessofoptimalcode-bookstoadversarialperturbations,weconductexperi-mentsbasedonwhite-boxattacks[Madryetal.,2018,Trameretal.,2020].Importantly,ourcodebooksachievenon-trivialrobustnessevenwithoutanyad-versarialtrainingoftheindividualbinaryclassiﬁers.Thus,ourresultssuggestastrongpotentialofECOCsfortrainingrobustclassiﬁers.Thepaperisorganizedasfollows:Sec.2introducestheECOCframework;Sec.3presentsthedesigncriteriaforcodebooks;Sec.4detailsourIPformulation;Sec.5providescomputationalexperimentsonnumerousdatasets;Sec.6outlinesfuturework.2ECOCsforClassiﬁcationIntheECOC-basedframeworkfork-classclassiﬁca-tion[DietterichandBakiri,1995],eachclassisencodedwithauniquecodewordoflengthl,resultinginacode-book(codingmatrix)M“pmijqofsizekˆl.Forbinary(resp.ternary)codes,theentriesmijofthecodingmatrixMbelongtothesett`1,´1u(resp.t`1,0,´1u).Therows(resp.columns)ofMcorre-spondtodistinctclasses(resp.binaryclassiﬁersorhypotheses).Figure1showsexamplesoftwostandardcodebooks.InthelearningproblemcorrespondingtoeverycolumninM,thesetoftrainingexamplesbelongingtodiﬀerentclassesC1,...,Ckispartitionedintotwogroups:allexamplesfromclasseswithentry`1representthepositiveclass,andallexamplesfromclasseswithentry(a)one-vs-all(binary)(b)one-vs-one(ternary)Figure1:Examplesofcodebooksfora4-classproblem.´1representtheotherclass.Inthecaseofternarycodes,trainingexampleswithentry0arenotincludedinthetrainingsetandareconsideredirrelevant.Letf1p¨q,...,flp¨qrepresentthelearnedbinaryhy-pothesesforthecorrespondingcolumnsofM.ForalearnedhypothesissPt1,...luandatestexamplex,letfs`1pxq(resp.fs´1pxq)denotetheoutput/scoreoftheclass`1(resp.class´1).Then,fspxq:“#`1iffs`1pxqąfs´1pxq´1otherwise@sPt1,...,lu.Afterevaluatingxonallthelhypotheses,weobtainanencoding~fpxq“rf1pxq,...,flpxqs.Toassociate~fpxqwithaclass(i.e.,arowofcodingmatrixM),wecanuseadecodingschemebasedonasimilaritymeasuresuchasHammingdistance.Particularly,onecancomputetheHammingdistancedHp¨,¨qbetween~fpxqandeachcodewordMpr,¨qandselecttheclass,denotedˆy,thatcorrespondstotheminimumdistance:dHpMpr,¨q,~fpxqq“lÿs“1ˆ1´Mpr,sqˆfspxq2˙(1)ˆy“argminrdHpMpr,¨q,~fpxqq.(2)3CodebookGenerationCriteriaTheﬁnalpredictionaccuracyofECOCschemeintro-ducedinSec.2cruciallydependsontheerrorcorrectionabilityofthecodingmatrixM.Toensurelowtester-ror,thecodingmatrixmustbechosencarefully.Belowweintroducethekeypropertiesthatserveasguidelinesforourdesignofbinarycodes.1RowSeparation:Itiswell-knownthatmoresepa-rationbetweenpairsofcodewords(i.e.,rowsinthecodingmatrixM)improvestheerrorcorrectioncapa-bility.Particularly,ifeverypairofdistinctcodewordshasahammingdistanceofat-leastd,thensuchacodecancorrectat-leastXd´12\errors.Thus,weseekcodingmatrixwithahighrowseparationbetweenanypairofcodewords.1Similarcodebookdesignapproachcanbedevelopedforternarycodes(notpresentedhereduetospaceconstraints).SamarthGupta,SaurabhAminColumnSeparation:Additionally,everypairofdis-tinctcolumnsinMshouldbeuncorrelated.Thebeneﬁtoflargecolumnseparationcanbeunderstoodbydraw-inganalogywitherrorcorrectionincommunicationoveranoisychannel.Encodingasignalandtrans-mittingthecodewordoveranoisychannelishighlyeﬀectivewhentheerrorsintroducedduringtransmis-sionarerandom.Bymaintainingasuﬃcientlylargeencoding,onecanrecovertheoriginalsignalatthereceivingendwithhighaccuracy.Analogously,inoursetup,ifanytwocolumns(classiﬁers)makeerrorsintheirpredictionsonthesameinputs(i.e.,theirout-putsarehighlycorrelated),thentheeﬀectivenessofencodingincorrectingerrorswillbereduced.BalancedColumns:Ontheotherhand,topre-ventover-ﬁttingofindividualhypotheses,itisim-portanttoprioritizeselectionofcolumnsforwhichthekclassdatapointsareevenlydistributedacrossthetwoclasses.Thiscriterionisparticularlyrele-vantwhenthetestexamplesareadversariallyper-turbed[Tsiprasetal.,2018].DataDistribution:Finally,inmulti-classprob-lems,someclasspairsaremorediﬃculttoseparatethanothers.Thismakesthepredictionoftheseclassesmorevulnerabletoadversarialattacks.Therefore,itisdesirabletohavelargerHammingdistancesamongpairsofcodewordscorrespondingtohard-to-separateclasspairs.Thishardnessofseparationcanbeestimatedfromthetrainingdatafordiﬀerentclasspairs,eitherusingthesemanticsofclasses[ZhaoandXing,2013],orbycalculat-ingsimilaritymeasuresbetweenclassesforsmalldatasets[XiaoZhangetal.,2009,Martinetal.,2018,Pujoletal.,2006,GaoandKoller,2011,GriﬃnandPerona,2008].4IntegerProgrammingFormulationInthissection,weembedtheabovementionedguide-linesintoadiscreteoptimizationformulationforgen-eratinganoptimalcodebook.Tobeginwith,notethatforak´classproblemacod-ingmatrixcanhaveatmostp2k´2q{2“2k´1´1columns.However,suchanexhaustivecodingmightbefeasibleonlyforasmallk(2to5).Askincreases,thenumberofbinaryclassiﬁersthatneedtobetrainedforexhaustivecodingincreaseexponentially.Practically,itisdesirabletoselectasmallsubset(sayofLcolumns)from2pk´1q´1possiblecolumns.ThissubsetshouldbeselectedinaccordancewiththecodebookgenerationcriteriadescribedinSec.3.Onewaytoformulatethecolumnsubsetselectionprob-lemistocastitasapropositionalsatisﬁabilityprob-lem,andsolveitusinganoﬀ-the-shelfSATsolver.Forexample,theauthorsin[DietterichandBakiri,1995]consideredthefollowingproblemfor8ďkď11:ForapredeﬁnednumberofcolumnsLandsomevalueρ,isthereasolutionsuchthattheHammingdistancebetweenanytwocolumnsisbetweenρandL´ρ?However,thisapproachonlyleadstoafeasible(notnecessarilyoptimal)solution.Incontrast,wepresentanIntegerProgramming(IP)problemthatcapturesthedesigncriteriainamoreﬂexiblemannerandcanbeusedtoﬁndanoptimalcodebook.Forsakeofsimplicity,weﬁrstconsidertherowandcol-umnseparationcriteria;theremainingcriteriaonbal-ancedcolumnsanddatadistributioncanbeaddressedinourIPformulation,asdiscussedsubsequentlyatendofthissection.Initsbasicformourproblemisthefollowing:WewanttoﬁndasolutionwhichmaximizestheminimumHammingdistancebetweenanytworows(ortheerror-correctingproperty).LetxidenotethebinaryvariableassociatedwitheachcolumnioftheexhaustivecodeforiPt1,...2k´1´1u,i.e.thedecisionvariablewhetherornotcolumniisselectedintheﬁnalsolution.Also,letxijbethebinaryvariablewhichrepresentstheoutcomeofANDoperationbetweenvariablesxiandxjforalldistincti,jpairs,i.e.pi,jqPt1,...,2k´1´1u2|iăj.Essentially,whenxij“1meansthatcolumnsiandjsatisfythecolumnseparationcriterion.WecannowwritetheIPformulationtogenerateanoptimalcodebookasfollows:IP1:maxxi,xijmintd1,2Hpxiq,...,dk´1,kHpxiqu(3)s.t.2k´1´1ÿi“1xiďL(4)ρxijďdH`Mp¨,iq,Mp¨,jq˘xijďpL´ρqxij@pi,jqPt1,...,2k´1´1u2|iăj(5)xijďxi(6)xijďxj(7)xi`xj´1ďxij(8)ds,tHpxiq“2k´1´1ÿi“1´1´Mps,¨qˆMpt,¨q2¯xi@ps,tqPt1,...,ku2|săt(9)xiPt0,1u@iPt1,...,2k´1´1u(10)xijPt0,1u@pi,jqPt1,...,2k´1´1u2|iăj(11)IntegerProgramming-basedError-CorrectingOutputCodeDesignforRobustClassiﬁcationInIP1,max-minobjectivecanbesimpliﬁedbyintroducinganauxiliaryvariablet,wheret“mintd1,2Hpxiq,d1,3Hpxiq,...,dk´1,kHpxiqu,andaddingthecorrespondingconstraintstďd1,2Hpxiq,tďd1,3Hpxiq,...,tďdk´1,kHpxiq.Eq.(5)ensureslargecolumnseparationforxij“1.Constraints(6)and(7)ensurethatifxij“1thenbothcolumnsiandjareincludedinthesolution,i.e.xi“1andxj“1.Conversely,Equation(8)ensuresthatifcolumnsiandjareselectedthenxij“1.WenotethatinIP1thereare2k´1´1«Op2k´1qbinaryvariablesforeachcolumn,andforeachpairofcolumnsthereare`2k´1´12˘«Op22k´3qbinaryvari-ables.Thus,thetotalnumberofbinaryvariablesareOp22k´3q.Similarly,thetotalnumberofconstraintsareOp22k´1q.Fork“10,thiswouldentailsolvinganIPofapproximately130,000variablesand650,000constraints.ModernIPsolverslikeGurobiandCPLEXcanhandlesuchprobleminstances.However,forką10,theaboveoptimizationproblemquicklybecomesintractable.Themainreasonisthatwehaveabinaryvariablexijforeachpairofcolumnstocapturethelargecolumnseparationcriterion;see(5).Wenowproposeasecondformulationwhichdoesnotinvolveanewvariableforeverypairofcolumns.LetSpdenotethesetofalldistinctpairsofcolumnsintheexhaustivecodeM,i.e.Sp“tpi,jqPt1,...,2k´1´1u2|iăjuand|Sp|“`2k´1´12˘.WenowconsidertwomutuallydisjointsubsetsSfeaspandSinfp,suchthatSp“SfeaspYSinfp:thesetSfeasp(resp.Sinfp)containsonlythosei,jpairsthatsatisfy(resp.donotsatisfy)thecolumnseparationcriterion(5).Mathematically,wecanwrite:Sp“!pi,jqPt1,...,2k´1´1u2|iăj),(12)Sfeasp“!pi,jqPt1,...,2k´1´1u2|iăjandρďdH`Mp¨,iq,Mp¨,jq˘ďpL´ρq),(13)Sinfp“SpzSfeasp.(14)Inthisnewrepresentation,theconstraint(5)iscap-turedbytheconstructionofSfeasp,whicheliminatestheneedofvariablesxijforcolumnpairs.Similarly,wenolongerneedtheconstraints(6),(7)and(8).Now,foranypi,jqpairofcolumnsinthesetSinfp,at-mostoneofthecolumnscanbeincludedintheﬁnalsolu-tion.Thiscanbeachievedbysettingxij“0in(8).Equivalently,foreverypi,jqpairinSinfp,itissuﬃcienttoimposetheconstraintxi`xj´1ď0.WecannowwriteIP1asthefollowingequivalentform:IP2:maxximintd1,2Hpxiq,...,dk´1,kHpxiqus.t.2k´1´1ÿi“1xiďLxi`xjď1@pi,jqPSinfp(15)ds,tHpxiq“2k´1´1ÿi“1´1´Mps,¨qˆMpt,¨q2¯xi@ps,tqPt1,...,ku2|sătxiPt0,1u@iPt1,...,2k´1´1uSinceIP2doesnotcontainanyxijvariables,thisformulationhassigniﬁcantlylessnumberofvariablesandconstraintsincomparisonto(IP1).Thecomputa-tionalcomplexityofIP2ismainlygovernedbythesizeofSinfp,whichdeterminesthenumberofconstraintsin(15).However,eveninthisnewrepresentation,thesizeofthesetSinfpbecomesprohibitivelylargeaskincreases.Table1,column4showshowquickly|Sinfp|increaseswithkforanappropriatelychosenρ.Fortunately,theconstraints(15)forthesetSinfpcanberepresentedonagraphGinfp,inwhicheachnodecorrespondstoacolumnxiandeachconstraintxi`xjď1correspondstoanedgebetweenthenodeiandnodej;seeFigure2foranillustration.Figure2:AnexampleofSinfpandcorrespondingGinfp.Infact,theabovegraphicalinterpretationleadstoareductioninthenumberofconstraintsinvolvingpi,jqcolumnpairsinthesetSinfp.Beforepresentingthisresult,werecallthatacliqueisasetofnodesinagraphsuchthatthereisanedgebetweenanytwodistinctnodesofthisset.Proofsofallresultsareprovidedinthesupplementarymaterial.Theorem1.Thefeasiblespaceenclosedbythecon-straintsconstitutingtheedgesofanycliqueCinGinfpissameasthatenclosedbythesingleconstraint:ÿiPCxiď1.(16)Fromtheorem1,weobtainthatforacliqueofsizen,npn´1q{2constraintsofformxi`xjď1betweenallpi,jqnodepairsinthecliquecanbesubstitutedSamarthGupta,SaurabhAminwithasingleconstraint(16).ThisconstraintcapturestherequirementthatoutofallthecolumnsinMformingaclique,atmostonecanbepresentinafeasiblesolution.Beforeintroducingournextresult,werecallthefollowingusefuldeﬁnition.Deﬁnition1(EdgeCliqueCover[Conteetal.,2016,Grammetal.,2009,Kouetal.,1978]).AnedgecliquecoverforagraphG,denotedasECCpGq,isasetofcliquesECCpGq“tC1,C2,...,Ckusuchthat:1.NocliqueCiiscontainedinanothercliqueCj,i.eCiĘCjforalli‰j,and2.EveryedgeinthegraphGisincludedinat-leastoneclique.Corollary1.1.Thefeasiblespaceenclosedbythecon-straintsetSinfp(oritsgraphicalequivalentGinfp)inIP2issameasthatenclosedbyamuchsmallercon-straintsetformedbyECCpGinfpq.Figure3:GraphicaldepictionofanexampleSinfpin(a),withtwofeasibleedgecliquecovers((b)and(c)).Foredge-coverin(b),weshowthereducedsetofconstraintscorrespondingtoitscliquesinblueandred.Agivengraphcanhavemanypossibleedgecliquecovers;seeforexampleFig.3.ToreducethesizeoftheconstraintsetSinfpasmuchaspossible,wewouldneedtoﬁndanedgecliquecoverofthesmallestsize.However,theminimumedgecoverproblemisknowntobeNP-hard[GareyandJohnson,1990].Fortunately,severalheuristicshavebeenpro-posedto[Kellerman,1973,Grammetal.,2009,Kouetal.,1978,Conteetal.,2016]ﬁndedgecliquecoverofagraph,andtheyhavebeenveryeﬀec-tiveinmanypracticalapplications.Theheuristic[Conteetal.,2016]isparticularlywell-suitedforlargegraphs–inpractice,itshowsalinearruntimeinthenumberofedges.Wethereforeusethisheuristicforouranalysis.Figure4:EdgeCliqueCovergeneratedbycombin-ingtheedgecliquecoversoftheindividualsubgraphs(Lemma1).WecanfurtherextendCollorary1.1togenerateedge-clique-coversofverylargegraphsinadistributedman-nerusingthefollowingresult:Lemma1.SupposeG1,...Gmareedge-disjointsub-graphsofGinfp,suchthat:1.GiXGj“φ@i,jPt1,...,mu2|iăj2.Ťmi“1Gi“GinfpTheunionoftheedgecliquecoversofindividualsub-graphsG1,...GmisavalidedgecliquecoverofGinfp:Ťmi“1ECCpGiq“ECCpGinfpq.Finally,usingcorollary1.1(oritsextensionlemma1)wecanreduceIP2tothefollowingintegerprogram:IP3:maxximintd1,2Hpxiq,...,dk´1,kHpxiqu(17)s.t.2k´1´1ÿi“1xiďLÿi:@iPCtxiď1@CtPECCpGinfpq(18)ds,tHpxiq“2k´1´1ÿi“1´1´Mps,¨qˆMpt,¨q2¯xi@ps,tqPt1,...,ku2|sătxiPt0,1u@iPt1,...,2k´1´1uFinally,thelasttwocriteriamentionedinSec.3canbeeasilyincorporatedinIP3.Speciﬁcally,therequire-mentforbalancedcolumnscanbeincorporatedbysettingthexi’sviolatingthiscriterionto0inIP3.Equivalently,sinceeachxiPt0,1ucorrespondstowhetheracolumnisselectedfromtheexhaustivecodeIntegerProgramming-basedError-CorrectingOutputCodeDesignforRobustClassiﬁcationTable1:Reducingthesizeoftheconstraintset|Sinfp|inIP2byﬁndingtheEdgeCliqueCoverofGinfp.No.ofclasseskNo.ofColumns2k´1´´´1ρNo.ofconstraints|||Sinfp|||No.ofconstraints(Reduced)ReductionFactorTimeTaken(insec.)10511311,475695160.146111,023328,1051,404200.208122,0474236,3138,165280.991134,0954610,00618,472332.573148,19141,543,81541,088377.3901516,383512,040,77044,91626858.9571632,767531,783,02091,304348249.531765,535582,441,772185,661444935.7618131,0716616,094,5351,073,24857410075.818131,0716616,094,5355,952,906`622,604“6,575,5109316251.667`4977.376“21229.04M,wecansimplyreduceMbyremovingtheunbal-ancedcolumnsandthenformIP3.Incontrastto[XiaoZhangetal.,2009],inourformulation,there-quirementforbalancedcolumnsfurtherreducestheﬁnalproblemsizeandcomplexity.Theremainingcriterionofdatadistributioncanbealsoincorporatedbymodifyingtheobjectivefunc-tion.Previousworkssuchas[Martinetal.,2018,ZhaoandXing,2013,XiaoZhangetal.,2009],pre-computeasimilaritymeasurebetweeneverypairofclasses(fromtrainingdata)andusethiscomputationtoestimatethedesirableclass-pairwisehammingdis-tancesˆdp,q.Finally,theyoptimizetoobtaincodebookswhichattainthesedistancevalues.Thiscanbeeas-ilyincorporatedinourformulationbychangingtheobjectivefunction(17)inIP3tothefollowing:minxiÿpp,qqPt1,...,ku2|păq|dp,qHpxiq´ˆdp,q|.(19)5ExperimentsWerunallourexperimentsonasystemwithasingle1080TiNvidiaGPU,IntelCorei7-6800KCPUand128GBRAM.WeuseGurobiasourIPsolver.Table2:IP3:OptimalityGap(max.time2000s).kLfbestBestBoundGapOptimalityGap|fbest´f˚|102010100%0%112212120%0%122412120%0%132613147.69%7.7%142814157.14%0%153015166.67%0%163216176.25%0%1734161812.2%6.25%1836171911.8%5.5%OurcomputationalexperimentsfocusonsolvingIP3whichusestheedge-clique-coverapproachtoreducetheconstraintsetSinfp.Table1showsthereductioninsizeofsetSinfpasthenumberofclasseskincreases.Notably,forkě15weachieveareductionofmorethantwoordersofmagnitude,whichdemonstratestheadvantageofusingourapproach.Thelastrowshowstheperformanceofgeneratingtheedge-coverontwodiﬀerentsubgraphsobtainedafterpartitioningtheoriginalgraph,thusvalidatingthelemma1.Thankstothereducedconstraintset,wecansolveIP3andobtaintheoptimalitygapfordiﬀerentinstancesasshowninTable2.fbestdenotestheobjectivefunctionvalueofthebestsolutionand“BestBound”denotesthebestupperboundfoundbyGurobi.Weobtainanoptimalsolutionorarelativelysmalloptimalitygap.Thus,ourformulationistightandenablesGurobitoterminatequicklywithoutexploringalargebranch-and-boundtree.Theseresultsdemonstrateourapproachtocodebookdesignindeedprovideslowoptimalitygaps.WenowevaluatetheclassiﬁcationperformanceofourIP-generatedcodebooksinbothnaturalandadversarialsettings.Wecompareperformanceagainstvariousstan-dardcodebooks:1-vs-all[RifkinandKlautau,2004]and1-vs-1aswellasSparseandDensecodesgeneratedusingtheprocedureoutlinedin[Allweinetal.,2000].25.1NaturalClassiﬁcationPerformanceToyDataset(2d):Wegenerateasyntheticdatasetof10classeswherepointsineachclassaresampledfroma2dGaussiandistribution.HereweuseSVMswithRbfkernelsasourbinaryclassiﬁerforindividualhypothesesinallourcodebooks.Figure5showsthede-cisionboundariesofallhypothesesforthreecodebooksalongwiththetrainingset.ThepredictionaccuracyonthetestsetisreportedinTable3.OurIP3gen-eratedcodebookeasilyoutperformsothercodebooks,andalmostmatchestheaccuracyof1-vs-1.NotethatthiscodebookonlyusedL“20columnswhile1-vs-12Pleaseseethesupplementarysectionformoredetails.SamarthGupta,SaurabhAmin(a)IP3generated(89.8%)(b)Sparse(66.8%)(c)1-vs-All(80.6%)Figure5:Decisionboundariesofdiﬀerenthypothesesinthreediﬀerentcodebookson2ddataset.usedL“45columns.ThishighlightsthebeneﬁtofECOCtheory:highaccuracycanbeachievedwithacarefullychosencompactcode-book.Table3:Performanceon2dToydataset(k“10).IP3DenseL“10SparseL“101-vs-AllL“101-vs-1L“45L“10L“2089.8%90.8%88.1%66.8%80.6%91.2%Real-worldDatasets(Small/Medium):Weeval-uatetheperformanceofdiﬀerentcodebooksonsmalltomediumsized,real-worlddatasets.WeconsiderGlass,EcoliandYeastdatasetstakenfromUCIrepository[DuaandGraﬀ,2017].Detailssuchasthenumberofsamples,featuresandclassesforeachdatasetarepro-videdinthesupplementarymaterial.ForDense,Sparse,andIPgeneratedcodebookwesetL“2k.WeagainuseSVMswithRbfkernelasthebinaryclassiﬁerfortrainingdiﬀerenthypothesesinourIP-generatedandothercodebooks.Wesetaside30%ofthesamplesasourtestsetandusedthemtoevaluatetheperformanceofdiﬀerentcodebooks.TheﬁnaltestsetaccuraciesarereportedinTable4.Ourcodebookprovidesbestaccu-racyonEcoliandsecond-bestaccuracyonGlassandYeast,thusprovidingbestperformanceonanaverage.Table4:Performanceofvariouscodebooksondiﬀerentreal-world(small)datasets.IP3DenseSparse1-vs-all1-vs-1Glass67.69%75.38%67.69%59.99%66.15%Ecoli90.09%87.12%83.16%71.28%77.22%Yeast51.79%50.67%43.04%48.20%52.91%Wenowevaluatetheperformanceofdiﬀerentcodebooksonreal-worldimagedatasets:MNISTandCIFAR10.MNIST:Weruntwosetofexperiments:Intheﬁrstset,weuseSVMs(withbothLinearandRbfkernel)onPCA-transformedMNISTdataset(using25principalcomponents).Inthesecondset,weusebinaryConvo-lutionalNeuralNetworksCNNstotraindiﬀerenthy-pothesesinourcodebooks.Tables5and6providethethetestsetaccuracyofdiﬀerentcodebooksfrombothsetsofexperiments.WeobservethatinthecaseofTable5:PerformanceofdiﬀerentcodebooksusingSVMonPCAtransformedMNISTdataset.IP3DenseSparse1-vs-all1-vs-1Linear80.37%75.74%68.87%76.82%92.01%Rbf97.59%97.5%79.18%96.95%98.01%Table6:PerformanceofDiﬀerentCodebookswithbinaryCNNonMNISTdataset.IP3DenseSparse1-vs-all1-vs-1NormalizedRaw98.84%98.8%95.05%84.17%98.6594.51%LinearkernelourIP3codebookoutperformsallothercodebooksexceptfor1-vs-1,whichachievesrelativelyhigheraccuracyofaround92%.Thisisduetofactthattheindividualhypothesesofdiﬀerentcodebooks(except1-vs-1)aresolvingmuchharderproblemswithhighlynon-lineardecisionboundaries.Onthecontrary,1-vs-1solvesonlynaturalclassiﬁcationproblems,wherealinearseparatorcanbeexpecteddowell.Inusingnon-linearRbf-kernel,bothIP3codebookand1-vs-1codebookachievesimilaraccuracy.Ontheotherhand,whenusingCNNsourIP3codebookprovidesbestperformance,indicatingthebeneﬁtofusingpowerfulbinaryclassiﬁersinourECOCapproach.CIFAR10:SincerunningSVMsonthisdatasetisexpensivecomputationally,weresorttoCNNshere.Inparticular,weuseResNet18[Heetal.,2015]asourbinaryclassiﬁertotraintheindividualhypothesesindiﬀerentcodebooks.AsshowninTable7,IP3achievesthebestperformance.NotethatourexperimentsonCIFAR10shouldbeviewedonlyintermsofevaluatingtherelativeperformanceofdiﬀerentcodebooks.WeareIntegerProgramming-basedError-CorrectingOutputCodeDesignforRobustClassiﬁcationawarethatmodernmulti-outputCNNshaveachievedanaccuracyofaround95%(orhigher)onCIFAR10dataset.However,recallthatinthisworkourgoalistohighlightthebeneﬁtofusingECOCswhenworkingwithbinaryclassiﬁers.Table7:PerformanceofDiﬀerentCodebookswithbinaryCNN(ResNet18)onCIFAR10dataset.IP3DenseSparse1-vs-all1-vs-1NormalizedRaw76.25%75.47%68.15%61.53%71.25%68.76%5.2AdversarialRobustnessWenowevaluatetherobustnessofdiﬀerentcodebooksagainstwhite-boxattacks.3Forfurthercomparison,wealsoevaluatetherobustnessofanaturallytrainedmulticlassCNNwithourIP-generatedcodebookintheﬁnallayer–thisissomewhatsimilartotherecentapproachin[VermaandSwami,2019].4However,notethatallourbinaryhypothesesarenaturallytrained,i.e.withoutanyadversarialtraining.Weﬁrstdiscusshowtoobtaintheclassprobabilityestimatesthatarenecessarytoevaluatetheadversarialrobustness.RecallfromSec.2theprocedureofassigningaclasstoaninputxusingHammingdecoding.However,thisdecodingschemeinitselfdoesnotprovideuswithclassprobabilityestimates,whichareessentialforevaluatingtherobustnessofanECOC-basedclassiﬁerwithrespecttowhite-boxattacks[Madryetal.,2018,Goodfellowetal.,2015].Particularly,weneedproba-bilityestimatestocomputetheadversariallossfunc-tion.Furthermore,weneedtobeabletocomputethegradientsoftheloss-functionwithrespecttoinputx.Weadopttheprocedureofcalculatingtheclassprob-abilityestimatesforgeneralcodebooks,asproposedin[Zadrozny,2002,HastieandTibshirani,1998].Af-terevaluatinganinputxoneachbinaryclassiﬁer,weobtainaprobabilityestimate(orscore5),denotedrlpxq,foreachcolumnl(i.e.,binaryclassiﬁer)inM.LetIdenotethesetofclassesforwhichMp¨,lq“1andJdenotethesetofclassesforwhichMp¨,lq“´1.ThentheclassprobabilityestimateforiPt1,...kuonaninputxisgivenasfollows:ˆpipxq“ÿl:Mpi,lq“1rlpxq`ÿl:Mpi,lq“´1p1´rlpxqq,(20)3Diﬀerentattacksincludingwhite-boxattacksarede-ﬁnedinsupplementalsection.4PleaserefertoSupplementarysectionformoredetails.5Classscorescanbeeasilyconvertedintoprobabilitiesusingsigmoidnon-linearity.wherediﬀerentiabilitywithrespecttoxismaintained.Usingtheseestimates,wecancomputealossfunc-tion(e.g,cross-entropyLoss)andthengeneratewhite-boxPGD-attacks[Madryetal.,2018]toevaluatetherobustnessoftheoverallclassiﬁer.Notethatweusethesamediﬀerentiableclassscores(ordecodingscheme)forbothpredictionandtogenerateawhite-boxattackinordertopreventgradient-obfuscation[Athalyeetal.,2018,Trameretal.,2020].Forallourexperiments,weworkwithperturbationsbasedonl8-norm.Inparticular,foragiveninputx1,theallowedsetofperturbationsaregivenbyset:Qpx1q“txPRdˇˇ||x´x1||8ď(cid:15);lďxďuu.MNIST:Werunanl8-normbased100-stepPGDattackwithmultiplevaluesof(cid:15)ondiﬀerentcodebooks;Table8summarizestheseresults.Intermsoftheoverallperformance,ourIP3-generatedcodebooksig-niﬁcantlyoutperformsallothercodebooksexcepttheDensecodebook.Inthiscodebook,diﬀerentpairsofcodewordshavediﬀerenthammingdistances,rangingfrom8-14.Ontheotherhand,inIP3,allcodewordpairshaveidenticalhammingdistanceof10asresultofthemax-minobjectivefunction(17).Thisdisparityinperformancecanthereforebemitigatedbyincor-poratingtheunderlyingdatadistribution(viaclasspairsimilaritymeasures)usingtheobjectivefunction(19).However,notethateﬃcientlycomputingsimi-laritymeasuresforlargeimagedatasetsisinitselfaresearchproblem.Finally,aswediscussinthenextsetofexperiments,theperformanceofDensecodebookdeterioratesasthedata-distributionchanges.Table8:AdversarialAccuracyofNominallyTrainedCodebooksonMNIST.(cid:15)“0.05(cid:15)“0.1(cid:15)“0.15(cid:15)“0.2(cid:15)“0.25(cid:15)“0.3IP395.46%83.6%57.67%29.96%12.99%4.81%1-vs-184.48%59.17%25.57%7.91%2.36%0.66%1-vs-All93.64%70.74%30.89%6.74%1.87%0.86%Sparse86.12%58.67%22.65%5.4%0.63%0.01%Dense95.17%84.08%62.95%43.54%28.6%16.34%Multiclass94.35%70.29%21.72%2.19%0.04%0.0%CIFAR10:Finally,weevaluatetherobustnessofdif-ferentcodebooksonCIFAR10byrunning30-stepPGDattack;seetable9.Inthiscase,ourIP3codebookoutperformsallothercodebooksincludingDensecode-book.Notethatsincethedata-distributionchangedfromMNISTtoCIFAR10,DensecodebooknowshowslowerperformancethanIP3,particularlyforlargerperturbationsof(cid:15)“4{255and(cid:15)“8{255.Importantly,theadversarialaccuracyachievedbyourIP3isbynomeanstrivialasundertheexactlysamesettingothercodebookslike1-vs-1,1-vs-All,Sparsedonotshowanyrobustness.Insimilarsetting,amulti-SamarthGupta,SaurabhAminTable9:AdversarialAccuracyofNominallyTrainedCodebooksonCIFAR10.(cid:15)“2{255(cid:15)“4{255(cid:15)“8{255IP324.04%19.24%16.48%1-vs-14.65%0.11%0.0%1-vs-All2.83%0.14%0.0%Sparse5.05%0.08%0.0%Dense24.2%12.79%11.63%Multiclass15.46%2.55%0.27%classCNNofsimilarnetworkcapacityalsodoesnotprovideanyrobustnesstoadversarialperturbations.ThishighlightstheimpressivecapabilityofECOCstohandleadversarialperturbationseventhoughtheindividualbinaryhypothesesareallnominallytrained.Ourapproachprovidesrobustness-by-design,anddoesnotmakeanyspeciﬁcassumptionsabouttheadversarymodelinthedesignofcodebook.6ConclusionandFutureWorkOurcomputationalresultsvalidatethemeritofouroptimalcodebookdesignapproach.Importantly,ourIP-basedformulationachievessmall(orzero)optimal-itygapswhilemaintainingtractabilityforreasonableproblemsizes.Thisispossiblemainlyduethegraph-theoreticviewpointweadoptedinapplyingtheedge-clique-cover,whichsubstantiallyreducedtheconstraintsetoforiginalIPformulation.Inthenominalsetting,ourcompactIPgeneratedcodebooksoutperformcom-monlyusedstandardcodebooksonmostdatasets.Intheadversarialsetting,ourIP-generatedcodebooksachievenon-trivialrobustness.Thisissurprisingduetothreemainreasons:(1)Wedonotemployanyad-versarialtraining;(2)Mostothercodebooks(exceptDense)donotexhibitanyrobustnessevenwhentheyusemorethantwicethenumberofcolumns;(3)Therobustnessthatweobtainisnotsimplybecauseofthelargenetworkcapacity.Tothebestofourknowledge,wearetheﬁrstonestoreportthatadversarialrobust-nesscanbeachievedbyacarefulcodebookdesignapproach,whileonlyusingnominallytrainedbinaryclassiﬁers.OurresultsprovideguidanceforfurtherresearchintheuseofECOCsforrobustclassiﬁcation.Weplantostudytheeﬀectofrobustifyingtheindividualhypothe-ses.Anothervariantwouldbetouseacombinationofnominallyandadversariallytrainedhypotheses.Weplantopursuetheseaspectsinourfuturework.References[Allweinetal.,2000]Allwein,E.L.,Schapire,R.E.,andSinger,Y.(2000).Reducingmulticlasstobinary:Aunifyingapproachformarginclassiﬁers.JournalofMachineLearningResearch,1:113–141.[Athalyeetal.,2018]Athalye,A.,Carlini,N.,andWagner,D.(2018).Obfuscatedgradientsgiveafalsesenseofsecurity:Circumventingdefensestoadversarialexamples.InProceedingsofthe35thIn-ternationalConferenceonMachineLearning,ICML2018.[Conteetal.,2016]Conte,A.,Grossi,R.,andMarino,A.(2016).CliqueCoveringofLargeReal-WorldNet-works.In31stAnnualACMSymposiumonAppliedComputing(SAC2016),pages1134–1139,Pisa,Italy.ACM.[CrammerandSinger,2002]Crammer,K.andSinger,Y.(2002).Onthelearnabilityanddesignofoutputcodesformulticlassproblems.MachineLearning,47(2):201–233.[DietterichandBakiri,1995]Dietterich,T.G.andBakiri,G.(1995).Solvingmulticlasslearningprob-lemsviaerror-correctingoutputcodes.JournalofArtiﬁcialIntelligenceResearch,2(1):263–286.[DuaandGraﬀ,2017]Dua,D.andGraﬀ,C.(2017).UCImachinelearningrepository.[GaoandKoller,2011]Gao,T.andKoller,D.(2011).Discriminativelearningofrelaxedhierarchyforlarge-scalevisualrecognition.InInternationalConferenceonComputerVision,pages2072–2079.[GareyandJohnson,1990]Garey,M.R.andJohnson,D.S.(1990).ComputersandIntractability;AGuidetotheTheoryofNP-Completeness.[Goodfellowetal.,2015]Goodfellow,I.,Shlens,J.,andSzegedy,C.(2015).Explainingandharnessingadversarialexamples.InInternationalConferenceonLearningRepresentations.[Grammetal.,2009]Gramm,J.,Guo,J.,H¨uﬀner,F.,andNiedermeier,R.(2009).Datareductionandexactalgorithmsforcliquecover.ACMJ.Exp.Algorithmics,13.[GriﬃnandPerona,2008]Griﬃn,G.andPerona,P.(2008).Learningandusingtaxonomiesforfastvisualcategorization.InIEEEConferenceonComputerVisionandPatternRecognition,pages1–8.[HastieandTibshirani,1998]Hastie,T.andTibshi-rani,R.(1998).Classiﬁcationbypairwisecoupling.IntegerProgramming-basedError-CorrectingOutputCodeDesignforRobustClassiﬁcationInAdvancesinNeuralInformationProcessingSys-tems10,NIPS’97,pages507–513,Cambridge,MA,USA.MITPress.[Heetal.,2015]He,K.,Zhang,X.,Ren,S.,andSun,J.(2015).Deepresiduallearningforimagerecognition.arXivpreprintarXiv:1512.03385.[Kellerman,1973]Kellerman,E.(1973).Determina-tionofkeywordconﬂict.IBMTechnicalDisclosureBulletin,16(2):544–546.[Kouetal.,1978]Kou,L.T.,Stockmeyer,L.,andWong,C.(1978).Coveringedgesbycliqueswithregardtokeywordconﬂictsandintersectiongraphs.Commun.ACM,21:135–139.[LeCunetal.,2015]LeCun,Y.,Bengio,Y.,andHin-ton,G.(2015).Deeplearning.Nature,521(7553):436–444.[Madryetal.,2018]Madry,A.,Makelov,A.,Schmidt,L.,Tsipras,D.,andVladu,A.(2018).Towardsdeeplearningmodelsresistanttoadversarialattacks.ArXiv,abs/1706.06083.[Martinetal.,2018]Martin,M.A.B.,Pujol,O.,DelaTorre,F.,andEscalera,S.(2018).Error-correctingfactorization.IEEETransactionsonPatternAnaly-sisandMachineIntelligence,40(10):2388–2401.[Pujoletal.,2006]Pujol,O.,Radeva,P.,andVitria,J.(2006).Discriminantecoc:Aheuristicmethodforapplicationdependentdesignoferrorcorrectingout-putcodes.IEEETransactionsonPatternAnalysisandMachineIntelligence.,28(6):1007–1012.[RifkinandKlautau,2004]Rifkin,R.andKlautau,A.(2004).Indefenseofone-vs-allclassiﬁcation.JournalofMachineLearningResearch,5:101–141.[Suetal.,2017]Su,J.,Vargas,D.V.,andSakurai,K.(2017).Onepixelattackforfoolingdeepneu-ralnetworks.IEEETransactionsonEvolutionaryComputation,23:828–841.[Szegedyetal.,2013]Szegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan,D.,Goodfellow,I.J.,andFergus,R.(2013).Intriguingpropertiesofneuralnetworks.CoRR,abs/1312.6199.[Trameretal.,2020]Tramer,F.,Carlini,N.,Brendel,W.,andMadry,A.(2020).Onadaptiveattackstoadversarialexampledefenses.[Tsiprasetal.,2018]Tsipras,D.,Santurkar,S.,En-gstrom,L.,Turner,A.,andMadry,A.(2018).Ro-bustnessmaybeatoddswithaccuracy.InICLR.[VermaandSwami,2019]Verma,G.andSwami,A.(2019).Errorcorrectingoutputcodesimproveprob-abilityestimationandadversarialrobustnessofdeepneuralnetworks.InAdvancesinNeuralInformationProcessingSystems32,pages8646–8656.[XiaoZhangetal.,2009]XiaoZhang,LinLiang,andHeung-YeungShum(2009).Spectralerrorcorrectingoutputcodesforeﬃcientmulticlassrecognition.In2009IEEE12thInternationalConferenceonCom-puterVision,pages1111–1118.[Zadrozny,2002]Zadrozny,B.(2002).Reducingmulti-classtobinarybycouplingprobabilityestimates.InAdvancesinNeuralInformationProcessingSystems14,pages1041–1048.MITPress.[ZhaoandXing,2013]Zhao,B.andXing,E.P.(2013).Sparseoutputcodingforlarge-scalevisualrecogni-tion.In2013IEEEConferenceonComputerVisionandPatternRecognition,pages3350–3357.SupplementaryMaterial1ProofsTheorem1ThefeasiblespaceenclosedbytheconstraintsconstitutingtheedgesofanycliqueCinGinfpissameasthatenclosedbythesingleconstraint:Xi∈Cxi≤1.(16)Proof:WeusemathematicalinductiontoshowthattheresultholdsforanycliqueCn,ofsizen≥3.Assumethatthetheoremholdsforacliqueofsizen−1.WeknowthatacliqueCn(sizen)containsndistinctcliquesC1n−1,...,Cnn−1ofsizen−1suchthatC1n−1∪C2n−1...Cn−1n−2∪Cnn−1=Cn.Underinductionhypothesis,wecanwritethefollowingsetofnequations:x1+x2+···+xn−1≤1x1+x2+···++xn≤1............x1+···+xn−1+xn≤1x2+···+xn−1+xn≤1Addingtheseequations,weobtain:x1+x2+···+xn−1+xn≤nn−1(21)Forn≥3,weknowthefollowingtrivialbound:nn−1<2(22)Using(21)and(22):x1+x2+···+xn−1+xn<2(23)Sincexi∈{0,1}∀i∈{1,...,n}:x1+x2+···+xn−1+xn∈Z+∪{0}(24)Using(23)and(24):x1+x2+···+xn−1+xn≤1(25)Thus,(16)holdsforacliqueofsizen.Tocompletetheinductionargument,weneedtoshowthattheresultholdsforn=3.Forn=3,wehave:x1+x2≤1x1+x3≤1x2+x3≤1Summingtheaboveequations,weget:x1+x2+x3≤1.5arXiv:2011.00144v1  [cs.LG]  30 Oct 2020SupplementaryMaterialAgain,sincexi∈{0,1}∀i∈{1,2,3}:x1+x2+x3∈Z+∪{0},andweconcludethat:x1+x2+x3≤1.Therefore,theresultalsoholdsforn=3.(cid:4)Corollary1.1ThefeasiblespaceenclosedbytheconstraintsetSinfp(oritsgraphicalequivalentGinfp)inIP2issameasthatenclosedbyamuchsmallerconstraintsetformedbyECC(Ginfp).Proof:SincethegraphGinfpdoesnotcontainanyisolatednodesandloops,andeveryedgeinGinfpiscoveredinatleastoneclique,thereforewecanwrite:k[i=1Ci=Ginfp.Also,Ginfp≡Sinfp,therefore{C1,...,Ck}≡Sinfp.(cid:4)Lemma1SupposeG1,...Gmareedge-disjointsubgraphsofGinfp,suchthat:1.Gi∩Gj=φ∀i,j∈{1,...,m}2|i<j2.Smi=1Gi=GinfpTheunionoftheedgecliquecoversofindividualsubgraphsG1,...GmisavalidedgecliquecoverofGinfp:m[i=1ECC(Gi)=ECC(Ginfp).Proof:RecallformthedeﬁnitionofEdgeCliqueCover,asetofcliquesisavalidedge-clique-coverofagivengraph,ifthefollowingtworequirementsaresatisﬁedbythecliqueset:IEveryedgeofthegraphiscoveredinatleastoneclique.IINocliqueiscompletelycontainedinanotherclique.Considerthefollowingarguments:1.Foranyi∈{1,...m},ECC(Gi)isavalidedge-clique-coverforsubgraphGi.2.EveryedgeinGinfpiscoveredinatleastonesubgraphasSmi=1Gi=Ginfp,thereforeeveryedgeinGinfpiscontainedinatleastoneofthecliquesintheset:Smi=1ECC(Gi).ThusrequirementIissatisﬁed.3.Foracliquetobecompletelycontainedinanotherclique,thereshouldbeatleastonecommonedgebetweenanytwodistinctsubgraphsGiandGj.Since,thesubgraphsareedge-disjoint,i.e.Gi∩Gj=φ,thereforenocliquecanbecompletelycontainedinanotherclique.ThusrequirementIIisalsosatisﬁed.(cid:4)2AsampleexhaustivecodeTable1:Exhaustivecode(allpossiblevalidcolumns)fork=5ClassesCodewordsf1f2f3f4f5f6f7f8f9f10f11f12f13f14f1511111111111111112-1-1-1-1-1-1-1-111111113-1-1-1-11111-1-1-1-11114-1-111-1-111-1-111-1-115-11-11-11-11-11-11-11-13Dense/SparseCodesRandomcodesisanotherwayofgeneratingcodebooksasproposedin[Allweinetal.,2000].Here,authorsproposegenerating10000matrices,whoseentriesarerandomlyselected.Iftheelementsarechosenuniformlyatrandomfrom{+1,−1},thentheresultingcodebooksarecalleddensecodesandiftheelementsaretakenfrom{−1,0,+1}thentheresultingcodebooksarecalledsparsecodes.Insparsecodes,0ischosenwithprobability1/2and±1areeachchosenwithprobability1/4.Outofthe10,000randommatricesgenerated,afterdiscardingmatriceswhichdo-notconstituteavalidcodebook,theonewiththelargestminimumHammingdistanceamongrowsisselected.Notethatsinceoutofthe10,000matricestheonewiththelargestminimumhammingdistanceisselected,thereforedespitethematricesbeinggeneratedrandomly,theﬁnalcodebookcanhavehighrow-separation.4DetailsaboutReal-worlddatasets(Small/Medium)Intable2weprovidedetailsaboutGlass,EcoliandYeastdatasetstakenfromtheUCIrepository[DuaandGraﬀ,2017].Table2:Real-worldDatasetCharacteristics#ofsamples#offeatures#ofclasses(k)Glass21496Ecoli33678Yeast14848105ClassProbabilityEstimatesInsection5.2underAdversarialRobustness,wediscussedhowclassprobabilityestimatesenableustoestimatetheadversarialrobustnessofECOCbasedclassiﬁersusingwhite-boxattacks.Forbinarycodebooksweobtainclassprobabilityestimatesusingtheprocedurefrom[Zadrozny,2002,HastieandTibshirani,1998].Aftereval-uatinganinputxoneachbinaryclassiﬁer,weobtainaprobabilityestimate,denotedrl(x),foreachcolumnl(i.e.,binaryclassiﬁer)inM.LetIdenotethesetofclassesforwhichM(·,l)=1andJdenotethesetofclassesforwhichM(·,l)=−1.Thentheclassprobabilityestimatefori∈{1,...k}onaninputxisgivenasfollows:ˆpi(x)=Xl:M(i,l)=1rl(x)+Xl:M(i,l)=−1(1−rl(x)),(20)wherediﬀerentiabilitywithrespecttoxismaintained.Theaboveestimatesworkwellforbinarycodes,howeverweneedtobecarefulforternary(orsparse)codes.Forternarycodes,hypotheseswhichhavezero(foraparticularclass)do-notcontributetotheabovesumin(20).Thereforeduetozeroentries,estimatesfordiﬀerentclassescansigniﬁcantlyvaryinrelativemagnitude.Thiscanbeeasilyﬁxedbysimplenormalization.Rawestimatesin(20)canbenormalizedasfollows:ˆp∗i=1PLl=11(cid:8)M(i,l)=1∨M(i,l)=−1(cid:9)(cid:16)Xl:M(i,l)=1rl(x)+Xl:M(i,l)=−1(1−rl(x))(cid:17),(26)where1{π}istheindicatorfunctionwhichevaluatesto1whenthepredicateπistrueand0otherwise.Inﬁgure1,weshowhowtheseestimatescanbecomputedfor1-vs-1codebook,whenworkingwithbinarydeepneuralnetworks.Since,eachrowin1-vs-1hasthesamenumberofzerosthereforenormalizationisnotnecessary.SupplementaryMaterialFigure1:Combiningoutputofindividualhypothesesof1-vs-1togenerateclassscoreswhilemaintainingdiﬀer-entiability.6AdversarialAccuracyandDiﬀerenttypesofAttacksForECOCbasedclassiﬁers,evaluationofnaturalorcleanaccuracyoveranexample(generallyfromtest-set)isstraightforward,andcanbeeasilydoneeitherbyusingadecodingschemesuchasHammingdecodingorbycalculatingclassprobabilityestimatesandchoosingtheclasswiththehighestprobability.Wenowmathematicallydeﬁnetheproblemofevaluatingtheadversarialaccuracyusingclassprobabilityesti-mates.Supposecbethetrueclassassociatedwithagiveninputx0andleti∈{1,...,k}/{c}bethetargetclassforwhichtheattackeristryingtogenerateanadversarialperturbation.Attackeraimstosolvethefollowingnon-convexproblem:f∗(x0)=maxδ:x0+δ∈Q(x0)ˆpi(x0+δ)−ˆpc(x0+δ)(27)In(27),setQ(x0)forl∞-normbasedperturbationsisgivenasfollows:Q(x0)={x∈Rd(cid:12)(cid:12)||x−x0||∞≤(cid:15);l≤x≤u}.Foravalid1adversarialperturbationδ,theobjectivefunctionvalueof(27)wouldbestrictlypositiveforsometargetclassi.Diﬀerentattackssuchasblack-boxandwhite-boxattacksattempttosolvetheaboveoutlinedproblem(27)underdiﬀerentsettings(orthreatmodel).Inblack-boxsetting,onlytheoutputoftheclassiﬁeri.e.theclassprobabilitiesorscoreofeachclassisknowntotheattacker.Nomodelinformationisavailabletotheattacker,i.e.thenetworkarchitectureandtheweightsofthenetwork.Inthissetting,sinceonlyclassprobabilityestimatesareavailable,thereforeanalyticalcomputationofgradientsisnotpossible.Theproblemisgenerallysolvedusingoﬀ-the-shelfblack-boxoptimizerscomprisingofheuristicsbasedalgorithmssuchasParticleSwarmOptimization(PSO),GeneticAlgorithms(GAs)etc.However,giventheeﬃcacyofgradientbasedattacks,onecanalsotrytocomputeanestimateofthegradientandthenusethisestimatetorungradient-basedattacks,fordetailssee[Ilyasetal.,2018].SPSAproposedin[Spall,1992]isanotherblack-boxoptimizationmethodwhichisbasedongradientestimation.1Anadversarialperturbationδdoesnotnecessarilyneedtobetheargmaxof(27)Inwhite-boxsetting,theclassprobabilityestimatesalongwiththemodelarchitectureandweightsareknowntotheattacker.White-boxsettingcanalsobereferredtoascompleteinformationsetting.Inwhite-boxsetting,theprojectedgradientdescentorthePGD-attackproposedin[Madryetal.,2018]hasemergedasoneofthestrongestknownattack.AnotherpopulargradientbasedattackknownasFast-Gradient-Signmethod(FGSM)wasproposedin[Goodfellowetal.,2015].FGSMcanbeviewedassimplyasinglestepPGDattackandthereforeisamuchweakerattackincomparisontoPGD-attack.Giventhenon-concavenatureoftheproblem(27),theaboveattacksdonotprovideanyguaranteeintermsofﬁndingtheoptimalsolution,andmainlyaimsatﬁndingafeasiblesolutionto(27)withpositiveobjectivefunctionvalue.Iftheseattacksfailingeneratinganadversarialperturbation(especiallyiftheattackisweak),weconcludethatthemodelisrobustagainstthatparticularattack.Therefore,toestimatetheadversarialrobustnessaccuratelyitisimportanttoevaluateagainststrongestpossibleattack.7ComparisonwithMulticlassCNNInsection5.2wecomparedtheadversarialrobustnessofourIPgeneratedcodebookIP3withotherstandardcodebookssuchas1-vs-1,1-vs-All,SparseandDensecodes.WereportedourresultsonMNISTandCIFAR10datasetsintable8and9respectively.WenotethatourIPgeneratedcodebookachievesnon-trivialrobustnesswithoutanyadversarialtraining.OnCIFAR10,ourcodebookoutperformsallotherstandardcodebooks,achiev-inganadversarialaccuracyof∼16%with(cid:15)=8/255.However,giventhatwearecombiningtheoutputof20binaryclassiﬁers,eachofwhichisaResNet-18,anaturalquestionarises:Isnetworkcapacity(oftheoverallclassiﬁer)themainreasonforthisrobustness?Recallthattoevaluatetherobustnesswecombinetheoutputsofeachofthehypotheses(individuallytrainedbefore)usingourIPgeneratedcodebookandformamulti-classclassiﬁer.Figure1showsthisfor1-vs-1codebookfor3classes.WethendoaPGDbasedevaluationoftheresultingmulticlassclassiﬁer.Toinvestigatetheroleofnetworkcapacity,wenowinthesamemanner,combine20untrainedhypotheses(ResNet-18)toformamulti-classclassiﬁer(sayF(x)).Wenownominallytrainthis10-classclassiﬁerF(x)end-to-endusingtheentiretrainingset.F(x)hasexactlythesamenetworkarchitectureandcapacityasourmulticlassECOCbasedclassiﬁerresultingfromourIPgeneratedcodebook.WenowevaluatetheadversarialaccuracyofF(x)usingthesamePGDattackwhichweusedfordiﬀerentcodebooksincludingourIPgeneratedcodebook.Wereportourresultsinthelastrowoftable8and9withtypeasMulticlass.ThelackofrobustnessofF(x)orMulticlassshowsthatnetworkcapacityaloneinitselfisnotthereasonforrobustnessofIPgeneratedECOCbasedclassiﬁer.Finally,wenotethatsincetheindividualuntrainedhypothesesarecombinedusingacodebookintheﬁnallayer,thereforeF(x)issimilartotheapproachtakenin[VermaandSwami,2019].8EstimatingError-CorrelationbetweenindividualhypothesesofacodebookInourdiscussioninsection3,wehighlightedthatincommunicatingoveranoisychannel,Error-CorrectingCodesarepowerfulonlywhentheerrorsmadeduetonoisearerandom.Forclassiﬁcationsetuplikeours,thisimpliesthatanytwohypotheses(orclassiﬁers)shouldnotmakeerrorsonthesameinputs.Toavoidthis,weensuredlargecolumnseparationinourIPformulation.However,wemaystillendupwithhypotheseswhoseﬁnalpredictions(orerrors)arecorrelated.Therefore,measurementofsuchpairwisecorrelationsbetweenhypothesescanprovideuswithinsightstobetterunderstandtheﬁnalperformanceofaparticularcodebook.Moreover,italsowillprovideuswithcorroborativeevidencetothefactthatcorrelationbetweenhypothesesshouldbeavoided.Assumingthatwehavealreadytrainedeachofourindividualhypothesesforagivencodebook.Also,letNtestdenotethenumberofimagesinourtest-set.Foreverybinaryclassiﬁer(correspondingtoacolumn)inthecodebook,wecancomputethe0-1lossforallimagesinthetestsetsothatwehaveavectorhl∈{0,1}Ntest∀l∈{1,...,L}.Wecannowcomputetheerror-correlationsbetweenthesebinaryvectorshi&hj∀(i,j)∈{1,...,L}2.ThiscanberepresentedinaL×Lmatrix,whichwewillrefertoasthecorrelationmatrix(denotedasP)inoursubsequentdiscussion.Weproposethefollowingmeasure:SupplementaryMaterialPi,j=PNtestn=11{hi[n]=1∧hj[n]=1}PNtestn=11{hi[n]=1∧hj[n]=1}+PNtestn=11{hi[n]=0∧hj[n]=0},(28)where1{π}istheindicatorfunctionwhichevaluatesto1whenthepredicateπistrueand0otherwise.Theabovemeasure(28)accountsforboththecorrectandincorrectpredictionsmadebyindividualhypotheses.Themagnitudeofthiserror-correlationmeasure(orthevaluesintheerror-correlationmatrixP)willhelpusinunderstandingtheaccuracyoftheoverallclassiﬁerorcodebook.Weestimatetheerror-correlationmatrixusingthenaturalimagesfromCIFAR10datasetforthenominallytrainedhypothesesofourIPgeneratedcodebook.Forthesamehypotheses,wealsoestimatetheerror-correlationmatrixusingtheadversarialimagesobtainedfromthePGD-attackwith(cid:15)=8/255ontheoverallclassiﬁer.Weplotboththematricesinﬁgure2.(a)Natural(Accuracy:76.25%)(b)Adversarial(Accuracy:16.48%)Figure2:Error-CorrelationmatricesestimatedusingthehypothesesoftheIP-generatedcodebookonnaturalandadversarialimagesofCIFAR10dataset.Fromﬁgure2,wenotethattheerror-correlationvaluesonthenaturalandadversarialdatasetdiﬀerbyalmostanorderofmagnitude.Onnaturalimages,muchhigheraccuracyisachievedastheerror-correlationislow,whileonadversarialimageshighererror-correlationvaluesresultinloweraccuracy.Thereforeforhigheraccuracy,error-correlationbetweenhypothesesshouldbeavoided.9VariousIPGeneratedCodebooksReferences[Allweinetal.,2000]Allwein,E.L.,Schapire,R.E.,andSinger,Y.(2000).Reducingmulticlasstobinary:Aunifyingapproachformarginclassiﬁers.JournalofMachineLearningResearch,1:113–141.[DuaandGraﬀ,2017]Dua,D.andGraﬀ,C.(2017).UCImachinelearningrepository.[Goodfellowetal.,2015]Goodfellow,I.,Shlens,J.,andSzegedy,C.(2015).Explainingandharnessingadver-sarialexamples.InInternationalConferenceonLearningRepresentations.[HastieandTibshirani,1998]Hastie,T.andTibshirani,R.(1998).Classiﬁcationbypairwisecoupling.InAdvancesinNeuralInformationProcessingSystems10,NIPS’97,pages507–513,Cambridge,MA,USA.MITPress.[Ilyasetal.,2018]Ilyas,A.,Engstrom,L.,Athalye,A.,andLin,J.(2018).Black-boxadversarialattackswithlimitedqueriesandinformation.InProceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pages2137–2146.[Madryetal.,2018]Madry,A.,Makelov,A.,Schmidt,L.,Tsipras,D.,andVladu,A.(2018).Towardsdeeplearningmodelsresistanttoadversarialattacks.ArXiv,abs/1706.06083.SupplementaryMaterial[Spall,1992]Spall,J.C.(1992).Multivariatestochasticapproximationusingasimultaneousperturbationgra-dientapproximation.IEEETransactionsonAutomaticControl,37(3):332–341.[VermaandSwami,2019]Verma,G.andSwami,A.(2019).Errorcorrectingoutputcodesimproveprobabilityestimationandadversarialrobustnessofdeepneuralnetworks.InAdvancesinNeuralInformationProcessingSystems32,pages8646–8656.[Zadrozny,2002]Zadrozny,B.(2002).Reducingmulticlasstobinarybycouplingprobabilityestimates.InAdvancesinNeuralInformationProcessingSystems14,pages1041–1048.MITPress.