1

Understanding Global Loss Landscape of
One-hidden-layer ReLU Networks
Part 2: Experiments and Analysis

Bo Liu

0
2
0
2

n
u
J

5
1

]

G
L
.
s
c
[

1
v
2
9
1
9
0
.
6
0
0
2
:
v
i
X
r
a

Abstract—The existence of local minima for one-hidden-layer
ReLU networks has been investigated theoretically in [8]. Based
on the theory,
in this paper, we ﬁrst analyze how big the
probability of existing local minima is for 1D Gaussian data
and how it varies in the whole weight space. We show that
this probability is very low in most regions. We then design
and implement a linear programming based approach to judge
the existence of genuine local minima, and use it to predict
whether bad local minima exist for the MNIST and CIFAR-
10 datasets, and ﬁnd that there are no bad differentiable local
minima almost everywhere in weight space once some hidden
neurons are activated by samples. These theoretical predictions
are veriﬁed experimentally by showing that gradient descent is
not trapped in the cells from which it starts. We also perform
experiments to explore the count and size of differentiable cells
in the weight space.

Index Terms—deep learning theory, deep neural networks,

ReLU, loss landscape, local minima.

I. INTRODUCTION

I N part 1 of this work [8], we have studied the global

loss landscape of one-hidden-layer ReLU networks from a
theoretical persperctive. For one-hidden-layer ReLU networks,
the space of weight vector is partitioned into a number of
convex cells by input data samples. [8] proved that there are
no bad local minima in the local landscapes inside cells, and
gave the conditions for the existence of genuine local minima
and their locations if they do exist. See section II for a detailed
description.

In this paper, based on the theory in [8], to get an idea of
how big the probability of existing bad local minima is at
anyplace in the whole weight space, we ﬁrst give an analytical
investigation of this probability for 1D Gaussian data. We then
describe how to implement an efﬁcient approach to judge the
existence of genuine local minima when they are in the form
of hyperplanes. We conduct experiments on both synthetic and
real datasets to predict the existence of bad local minima with
our theory, as well as verify the correctness of these theoretical
predications. Finally, we will also carry out experiments to
explore the count and size of differentiable cells in weight
space to give a more complete picture of loss landscape.

More speciﬁcally, we have made the following contributions

in this paper.

Bo Liu is with College of Computer Science, Faculty of Information
Technology, Beijing University of Technology, Beijing, China. e-mail: li-
ubo@bjut.edu.cn.

• Design and implement intersection of half-spaces with
linear programming to judge the existence of genuine
local minima when they are in the form of hyperplanes.
• We show analytically that for 1D Gaussian data the
probability of existing bad local minima is very low in
most regions.

• We use our theory to predict whether bad local minima
exist for the MNIST and CIFAR-10 datasets, and ﬁnd that
there are no bad local minima in typical locations from
far to near in weight space once some hidden neurons are
active.

• The theoretical predictions of whether bad local minima
exist are veriﬁed experimentally by showing that gradient
descent is not trapped in the cells it starts from.

• We conduct experiments to explore the size of differen-

tiable cells in weight space.

These theoretical and experimental results explain, from the
viewpiont of loss landscape, why local search based methods
such as gradient descent can optimize successfully one-hidden-
layer ReLU networks of any size and any input if initialized
at appropriate locations.

This paper is organized as follows. Section II gives a brief
introduction to the theory of existence of genuine differentiable
local minima for one-hidden-layer ReLU networks. In section
III, we compute the probability of existing local minima for
1D Gaussian input, and demonstrate how this probability varies
in weight space with experiments. In section IV, we present
implementation of intersection of half-spaces to judge the
existence of genuine local minima when they are hyperplanes,
and experiments on higher-dimensional Gaussian input. Section
V presents experiments on MNIST and CIFAR-10 datasets,
showing the consistency between theoretical predictions and
experimental results. The count and size of convex cells are
explored in section VI. Section VII is related work. Finally,
we conclude this paper and point out future directions.

II. PRELIMINARIES

A. Locations of Differentiable Local Minima

Suppose there are K hidden neurons with ReLU non-
linearality, d input neurons and a single output neuron in
a one-hidden-layer ReLU network. The input samples are
(xi, yi) (i ∈ [N ]), where xi ∈ Rd is data vector and yi ∈ ±1

 
 
 
 
 
 
is the label of xi, [N ] stands for {1, 2, · · · , N }. The loss of a
one-hidden-layer ReLU network is

B. Criteria for Existence of Genuine Differentiable Local
Minima

L(z, w) =

1
N

N
(cid:88)

K
(cid:88)

l(

i=1

j=1

zj · (cid:2)wj · xi

(cid:3)
+ , yi),

(1)

where z = {zk, k ∈ [K]} are the weights between output
neuron and hidden ones, w = {wk, k ∈ [K]} represent
the weight vectors (augmented with bias) connecting hidden
neurons and input, [y]+ = max(0, y) is the ReLU function
and l is the loss function.

For one-hidden-layer ReLU network model, xi is a hyper-
plane in the weight space, and the samples {xi, i ∈ [N ]}
partition the weight space into a number of convex cells.
Introducing variables Iij which equal 1 if wj · xi > 0 and 0
otherwise, and deﬁning Rj = zj · wj, the loss can be rewritten
as

L(R) =

1
N

N
(cid:88)

K
(cid:88)

l(

j=1

j=1

IijRj · xi, yi)

(2)

where R = (cid:0)R1

T

. . . RK

T (cid:1)T

.

Given any {wj, j ∈ [K]}, each weight vector wj will be
located in a certain cell, and we call these cells in which
{wj, j ∈ [K]} reside as their deﬁning cells. Inside deﬁning
cells, {Iij, i ∈ [N ], j ∈ [K]} are constant, hence L(R) is a
differentiable function of R. It has been proved in [8] that
there are no bad local minima in the local landscape of deﬁning
cells of any {wj, j ∈ [K]} if l is convex.

When loss function l
local minimum for

is squared loss,

the location
the cells speciﬁed by constant

of
{Iij, i ∈ [N ], j ∈ [K]} is given by the following solution
R∗ = A+y + (cid:0)I − A+A(cid:1) c,
(3)
where c ∈ RKd is a arbitrary vector, I is identity matrix. A+
is the Moore-Penrose inverse of A, and

A =






I11xT
1
...
IN 1xT

· · ·
. . .
N · · ·




 , y =

I1KxT
1
...
IN KxT
N








.

(4)








y1
y2
...
yN

The solution R∗ can be characterized by the following cases:
1). R∗ is unique: R∗ = A+y, corresponding to A+A =
I and thus (I − A+A)c = 0. This happens if and only if
rank(A) = Kd. Therefore, N ≥ Kd is necessary in order to
have a unique solution.

2). R∗ has inﬁnite number of continuous solutions. In this
case, I − A+A (cid:54)= 0, hence the arbitrary vector c plays a role.
This happens only if rank(A) (cid:54)= Kd, corresponding to two
possible situations. a). N < Kd. This is usually refered to as
over-parameterization. b). N ≥ Kd but rank(A) < Kd.

In this case, (3) shows R∗ is a afﬁne transformation of
c ∈ RKd. Therefore, R∗
j can be the whole Rd space or a
linear subspace of it (e.g. hyperplanes), depending on whether
the rows in (I − A+A) corresponding to R∗
j is of full rank or
not.

The loss at local minima in (3) is

L(R∗) =

1
N

(cid:13)AA+y − y(cid:13)
(cid:13)
2
(cid:13)
2

(5)

2

Given any {wj, j ∈ [K]} and corresponding deﬁning cells,
the locations of associated local minima are given in (3).
However, such locations may be outside the deﬁning cells,
and if so, there actually exist no local minima in these cells.
We call those local minima that are still located in their deﬁning
cell as genuine local minima.

The conditions under which (cid:8)R∗
their deﬁning cells are given as follows.

j , j ∈ [K](cid:9) will be inside

1). For the case R∗ is unique, in order for w∗ to be inside
the deﬁning cells, w∗ and w should be on the same side of
each sample . Giving {Iij, i ∈ [N ], j ∈ [K]} that specify the
deﬁning cells, this can be expressed as

w∗

j · xi

(cid:40)

> 0 if Iij = 1;
≤ 0 if Iij = 0;

(i ∈ [N ]; j ∈ [K]).

(6)

Corresponding to different signs of z∗

j , the criteria for
existing unique differentiable local minima can be expressed
as: for each R∗

j (j ∈ [K]),
(cid:40)

R∗

j · xi

> 0 if Iij = 1;
≤ 0 if Iij = 0;

(i ∈ [N ])

(7)

or R∗

j · xi

(cid:40)

< 0 if Iij = 1;
≥ 0 if Iij = 0;

(i ∈ [N ])

(8)

2). For the case R∗ is continuous, we need to test whether
the continuous differentiable local minima in (3) are in their
deﬁning cells. For example, substituting (3) into (7), then for
each R∗

j (j ∈ [K]) the criteria become

i ((A+y)j + (I − A+A)jc)
xT

(cid:40)

> 0 if Iij = 1;
≤ 0 if Iij = 0;

(i ∈ [N ])

(9)

where (A+y)j is the rows of A+y corresponding to R∗
j , and
so on. Each inequality of c in (9) deﬁnes a half-space in
RKd. Therefore, the criterion for existing genuine continuous
differentiable local minima is transformed into identifying
whether there exists non-null intersection of all these half-
spaces.

III. PROBABILITY OF EXISTING GENUINE LOCAL MINIMA
FOR 1D GAUSSIAN DATA

A. Locations of Local Minima for 1D Gaussian Data

In this section, in order to see how big the probability of
existing genuine local minima is and how it varies in the weight
space, we will compute the probability of existing genuine
local minima analytically for 1D Gaussian input data. The
core idea is that if no samples lie between the original weight
vector w and the local minima w∗, then (6) will hold and w∗
will be inside the same cell with w and thus be a genuine
local minimum. Therefore, the probability of existing genuine
local minima is actually the probability of having no samples
between w and w∗. However, it is hard to get an analytical
probability starting from the general solution of w∗ in (3). To

get analytical solutions, we consider the simple case of 1D
input, which is also equivalent to the case where all higher-
dimensional weights are parallel. Higher-dimensional Gaussian
input will be discussed in section IV.

For the case of 1D input, denote the unit vector of x
coodinate as i, a weight vector wk is then represented by
its normal nk = i or nk = −i and its x coordinate hk. During
optimization, we ﬁx the normal of each weight vector and only
tune its location. Furthermore, we ﬁx the value of zk and set
zk = 1 if nk = i and zk = −1 if nk = −i. As explained
in [8], the magnitude of zk does not matter for identifying
existence of genuine local minima. Therefore, by the fact that
wk · xi is equal to the signed distance from xi to wk, we have
Rk · xi = zkwk · xi = xi − hk, where xi is the x coodinate
of the ith sample.

The weights partition the 1D input space into a se-
ries of regions Ωj. Each region lies between two adja-
cent weight vectors, and in each region Ωj, Iik (i ∈
i.e., sample xi is located in Ωj) have constant values,
Ωj,
which we denote as IΩj k. The total loss can be written as

L =

(cid:88)

(cid:88)

(cid:88)
(

IΩj kxi −

(cid:88)

2
IΩj khk − yi)

.

(10)

Ωj

i∈Ωj

k

k

By the global optimality conditions ∂L
∂hk

can derive the following linear system,

= 0 (k ∈ [K]), we

F h = f ,
(11)
where h = (h1, h2, · · · , hK)T , F and f are matrix and vector
respectively with the following elements,

F (l, k) =

(cid:88)

Ωj

IΩj lIΩj kNj, (l, k ∈ [K]),

(cid:88)

(cid:88)

f (l) =

IΩj lIΩj k

k

Ωj

(cid:88)

i∈Ωj

xi −

(cid:88)

IΩj l

(cid:88)

yi, (l ∈ [K]).

Ωj

i∈Ωj

(12)

Nj is the number of samples in region Ωj.

(cid:19)

=

Let us discuss the simple case of two weight vectors at ﬁrst,
with normals shown in Fig.1. The analytical solution to the
linear system (11) can be easily obtained in this setting. (11)
becomes
(cid:18)N1

(cid:18)N1+ ¯x1+ + N1− ¯x1− − N1+ + N1−
N3+ ¯x3+ + N3− ¯x3− − N3+ + N3−

0
0 N3

(cid:19) (cid:18)h1
h2

(13)
where N1+ is the number of positive examples in region Ω1,
¯x1+ is the average of x coordinates for all positive samples in
Ω1, and so on. Assuming positive and negative classes have
equal priors (thus the numbers of positive and negative samples
are equal), and denoting the probability of positive (negative)
examples lying in region Ωj as Pj+ (Pj−), the solution to (13)
is as follows

(cid:19)

,

xw∗

1

= h∗

1 =

xw∗

2

= h∗

2 =

P1+ ¯x1+ + P1− ¯x1− − P1+ + P1−
P1+ + P1−
P3+ ¯x3+ + P3− ¯x3− − P3+ + P3−
P3+ + P3−

,

,

(14)

where xw∗

1 is the x coordinate of w∗
1.

If there are more than two weight vectors, we need to solve
the linear system (11), with the following F and f expressed

Fig. 1. Two weight vectors with three regions in input space

in {Pj+, Pj−, ¯xj+, ¯xj−} (assuming equal priors for positive
and negative classes),

F (l, k) =

(cid:88)

IΩj lIΩj k(Pj+ + Pj−),

l, k ∈ [K],

(15)

Ωj
(cid:88)

f (l) =

−

k
(cid:88)

Ωj

(cid:88)

Ωj

IΩj lIΩj k(Pj+ ¯xj+ + Pj− ¯xj−)

IΩj l(Pj+ − Pj−),

l ∈ [K]

(16)

B. Probability of Existing Differentiable Local Minima for 1D
Gaussian Data

Assume data samples are drawn from 1D Gaussian distribu-
tion. As shown in Fig.1, there exist gaps between w∗
j and wj.
If no samples lie in these gaps, there will be a genuine local
minimum in the cells {w1, w2} lie in. Therefore, suppose N
samples are i.i.d. drawn, the probability of existing genuine
local minima is

Pt = (1 − Pg)N

(17)

where Pg is the probability of a sample lying in one of the gaps.
Denote the gaps as g1, g2, ..., gK, we use maxi P (x ∈ gi) to
approximate Pg due to

Pg = P (x ∈ g1 ∪ g2 · · · ∪ gK) ≥ max

i

P (x ∈ gi)

(18)

Since Pt is exponentially vanishing, the probability of existing
local minima is very small as long as one of the gaps is
large enough such that probability of having samples in it
is unnegligible. This conclusion still holds for data samples
drawn from other distributions, due to the fact w∗
i and wi
usually form an intermediate region in which the probability
of having samples is nonzero.

The probabilities of existing saddle points and non-
differentiable local minim are exponentially vanishing as well
due to the gaps.

We now describe the detailed analytical calculations of
Pg and {Pj+, Pj−, ¯xj+, ¯xj−} for 1D Gaussian distribution.
Assume both positive and negative samples are drawn from 1D
Gaussian distributions, with means x+ = 1 and x− = −1
respectively and a standard deviation of 1. Therefore, the
probability densities are as follows

P± (x) =

1
√
2π

e−

(x−x±)2
2

(19)

3

Assuming equal priors for positive and negative samples, the
probability of a sample lying in gap gi is

1
2
1
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2

P (x ∈ gi) =

=

=

=

[P (x ∈ gi | y = 1) + P (x ∈ gi | y = −1)]
(cid:20)(cid:90)

(cid:90)

(cid:21)
P− (x) dx

P+ (x) dx +

gi
(cid:90) xw∗
i

gi

−(x−x+)2
2

e

dx

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
√
2π

−(x−x−)2
2

e

dx

1
√
2π

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

− x+

(cid:1) − Φ (xwi − x+)(cid:12)
(cid:12)

1
2

xwi
(cid:12)
(cid:90) xw∗
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)
xwi
(cid:12)
(cid:12)Φ (cid:0)xw∗
1
(cid:12)
(cid:12)Φ (cid:0)xw∗
2

i

i

+

1
2

+

− x−

(cid:1) − Φ (xwi − x−)(cid:12)
(cid:12) ,

(20)

are the x coordinates of wi and w∗

where xwi, xw∗
i respec-
tively, Φ(x) is the cumulative distribution function of standard
Gaussian distribution N (0, 1).

i

When there are multiple weight vectors and consequently
multiple gaps, the quantities {Pj+, Pj−, ¯xj+, ¯xj−} appeared
in (15) and (16) for each region Ωj are computed as follows.
We will take P1+ and ¯x1+ as examples. Using the Gaussian
distributions in (19), we get

P1+ =

(cid:90) ∞

e−

(x−x+)2
2

1
√
2π
= 1 − Φ (xw1 − x+) .

xw1

(cid:90) ∞

dx =

xw1 −x+

1
√
2π

−x2
2 dx

e

(21)

Using truncated Gaussian distribution, we have

(cid:90) ∞

xw1

(cid:82) ∞
xw1

¯x1+ =

=

x ·

1
√
2π

e

−(x−x+)2
2

dx/

(cid:90) ∞

1
√
2π

e

−(x−x+)2
2

dx

(x − x+)

1√

2π

e

−(x−x+)2
2

xw1

dx

x+

+

=

1
√
2π

e

e

1√

1 − Φ (xw1 − x+)
−(x−x+)2
(cid:82) ∞
2
xw1
2π
1 − Φ (xw1 − x+)
−(xw1 −x+)2
2

dx

/ [1 − Φ (xw1 − x+)] + x+

(22)

P1−, ¯x1− can be obtained similarly, and the quantities for
regions other than Ω1 can be calculated in similar ways, using
corresponding intervals for the integrals.

C. Experimental Results for 1D Gaussian Data

In this subsection, we conduct experiments to show how
big the probability of existing differentiable local minima is
in the whole weight space for 1D Gaussian data. Samples of
both positive and negtive classes are drawn from Gaussian
distribution with a standard deviation of 1 and means locating
at x+ = 1 and x− = −1 respectively. N is set to 100.

We ﬁrst consider the case of two weight vectors: w1 and
w2. At ﬁrst, we ﬁx xw2 = 0 and move w1 in the interval [0,
6]. Fig.2(a)-(c) show three examples of empirical losses w.r.t.

4

1 and xw∗

xw1 obtained by three independant data samplings. In each
empirical loss there is clearly a global minimum. We then use
(14) to compute xw∗
2 , and compute the probability
of existing local minima with (17). Fig.2(d) shows how this
probability varies with xw1 . It clearly shows that there is really
a high probability of existing local minima at the locations
of global minima of empirical losses, and the probability of
existing local minima gets sufﬁciently large when xw1 is big
enough. These are consistent with the landscapes of empirical
losses shown in Fig.2(a)-(c), demonstrating the correctness of
our theory on the probability of existing local minima. When
xw1 is far away from data means, this probability is close to
1 and the loss is almost constant, corresponding to the case
of ﬂat plateau local landscape. This can be attributed to the
fact that although there is still a gap between xw1 and xw∗
1 ,
the probability of samples lying in this gap is very low due to
the exponentially vanishing nature of Gaussian density when
xw1 is far away from data means. In other words, almost no
samples are activated by w1 in this case, thus moving w1 does
not affect the loss. The probability of existing local minima
is very low in other places in the weight space due to high
probability of having samples in the gap.

We then consider the case in which both weight vectors
can move. Fig.2(e) shows the probability of existing local
minima when moving both weights. It is actually the tensor
product of the two probabilities obtained by moving xw1 and
xw2 independently. The small peak close to origin corresponds
to the global minimum of loss landscape. The probability of
existing bad local minima is very low if both weights are not
far away from data means.

Finally, we perform experiments to show the probability of
existing differentiable local minima when there are multiple
weight vectors. We ﬁrst test K = 4 weights whose initial
locations are [-0.05, 0, 0.05, 0.1] respectively. The normals of
the ﬁrst K/2 weight vectors are set to −i, and the normals of
the last K/2 weight vectors are set to i. We then shift these
weights and solve (11) to get their optimal locations. Due to
the hugeness of high-dimensional weight space, we can only
visualize some slices of it. Fig.3(a) shows the probability of
existing differentiable local minima when moving only the
rightmost weight, and Fig.3(b) exhibits the case of moving all
weights simultaneously. We then use K = 60 and K = 200
weight vectors respectively that are evenly located in [-3, 3]
and shift these weights simultaneously, and solve (11) to get
their optimal locations and compute Pt (the probability of
existing genuine local minima) with (17). These results are
given in Fig.3(c) and Fig.3(d) respectively. All these results
demonstrate that probability of existing bad local minima is
usually very low.

IV. EXPERIMENTS ON HIGHER-DIMENSIONAL GAUSSIAN
DATA

A. Dataset and Implementation

In this section, we will present experiments on high-
dimensional Gaussian data to probe the existence of bad local
minima. Again the purpose is to show how the probability of
existing bad local minima varies in the weight space. However,

(a)

(b)

(c)

(d)

(e)

Fig. 2. The probability of existing local minima w.r.t. the locations of two weight vectors. (a)-(c) Three empirical loss landscapes when moving one of the
weights. (d) The probability of existing local minima when moving one of the weights. (e) The probability of existing local minima when moving both weights.

(a) 4 weights: shifting the right-
most one.

(b) 4 weights:
weights.

shifting all

(c) 60 weights:
weights.

shifting all

(d) 200 weights: shifting all
weights.

Fig. 3. The probability of existing local minima w.r.t. the locations of weight vectors

unlike the 1-dimensional case, it is hard to derive an analytical
expression of this probability for higher-dimensional Gaussian
data, since ﬁnding the regions Ωj and computing the integrals
of Gaussian distribution over these polyhedron-shaped regions
are difﬁcult in high-dimensions. Instead of analytical analysis,
we take a sampling approach that works on discrete samples
drawn from Gaussian distribution. Both positive and negative
samples are drawn from symmetrical multivariate Gaussian
distributions. The x coordinate of positive mean is set to 1 and
that of negative mean is set to -1, and all other coordinates of
means are set to 0. Covariance matrices of both distributions
are set as identity matrix.

With Gaussian samples, the optimal weights can be obtained
by (3). We then need to judge whether the optimal weights
are genuine using (7) and (8) if they are isolate points. When
the optimal weights are in the form of hyperplanes, one has
to determine whether the intersection of half-spaces in (9)
is null. A naive implementation by computing the cells at
ﬁrst using arrangement of hyperplanes of samples [1] and
then ﬁnding intersections of cells with hyperplanes of optimal
weights would be very costly since the arrangment algorithm
has a complexity of O(N d). In our implementation, we use
a much efﬁcient scheme based on linear programming. Since
each half-space in (9) is deﬁned by a linear inequality, the
problem of judging whether the intersection of half-spaces is
null is equivalent to determining whether feasible solutions
exist for a linear programming problem with linear inequality
constraints in (9) and a dummy linear objective. We use the
’linprog’ function in Matlab to perform linear programming
with a built-in active-set algorithm.

The ideal goal is to give the probability of existing genuine
local minima at any point in the weight space. However, since
each weight vector is located in a cell, the amount of cells
is enormous (see section VI) and the number of possible
combinations of K cells weight vectors lie in is exponentially

explosive, it is impractical to explore all possible conﬁgurations
of weight vectors. Therefore, in our experiments we test some
typical locations in the weight space. At each location, a
random weight matrix composed of {wj, j ∈ [K]} is drawn
from Gaussian distribution, which is a common practice when
optimizing deep learning models, then the cells weight vectors
{wj, j ∈ [K]} lie in are found by computing {Iij}, and the
genuineness of optimal weight vectors is determined by (7)
and (8), or (9). The location of a weight vector is determined
by its bias, so we shift the weight vectors from far distances
to near the origin by setting appropriate biases. The initial
values of {zj} do not matter since {Iij} and consequently the
deﬁning cells are only determined by {wj}.

More speciﬁcally, we try some typical locations in the weight
space, ranging from locations where all hidden neurons are
activated by all samples (bias = 20), locations with small
distances to the origin (bias = 3 and bias = −3) and around
the origin (bias = 0), to locations where all hidden neuons are
not activated for all samples (bias = −20). At each weight
location, unless otherwise stated, we draw random weight
matrix 100 times and report the average results or percentages.

We also give the probabilities of existing genuine local
minima at different locations. After obtaining the optimal
solution for a speciﬁc weight matrix, we can compute the
gaps and consequently the probability of existing genuine local
minima at this location. To circumvent the diffculty of deriving
analytical expressions, the probability of a sample lying in a
gap is approximated by the ratio of samples in this gap, and
the largest probability among all gaps is used to approximate
the probability of existing genuine local minima, as did in
(18).

Several different combinations of K (the number of hidden
neurons), d (input dimension) and N (the number of sam-
ples) are tried in our experiments. The results for different
combinations are presented in Table I, Table II and Table III.

5

The number of samples is set to N = 2000 for K = 10 and
K = 20, and N = 400 for K = 50. In these tables, besides
percentage of existing genuine local minima, percentage of
them being continuous, and average probability of having
samples in gaps (Pg), we also give the average percentage of
activated states for hidden neurons at different biases.

Our implementation is in Matlab, and all experiments are
conducted on a commodity laptop computer without GPU
acceleration.

B. Results and Analysis

The results in Table I, Table II and Table III show that,
when some hidden neurons are activated (bias = 20, 0, 3, −3),
there exist no genuine local minima. At these locations, the
probability Pg of having a sample in the gaps between weight
vectors and their optimal solutions is big enough such that the
probability of existing genuine local minima vanishes. When
all hidden neuons are dead for all samples (bias = −20), the
movement of weight vectors does not affect the loss and thus
the landscape at this location is a ﬂat plateau with a loss of
1 (since label is either 1 or -1), hence genuine local minima
exist.

In all cases of K, d and N at bias = 20, since all hidden
neurons are activated, Iij = 1 (i ∈ [N ]; j ∈ [K]) and thus
A in (4) is rank-deﬁcient, the local minima will be continous
according to subsection II-A. At bias = −20, the landscape is
a ﬂat plateau and thus is continous as well. These statements
are veriﬁed in Table I, Table II and Table III.

Comparing Table I and Table II, the percentages of activated
neurons remain roughly the same due to same data and same
biases. Moreover, for the cases of bias = 3, 0, −3, more local
minima are continuous when K is bigger. The reason is as
follows. In these two tables there is N > Kd, hence according
to subsection II-A, the local minima will be continuous if
rank(A) < Kd and isolated if rank(A) = Kd. With the
increase of K, more columns in matrix (Iij) ∈ RN ×K will
equal 1s by the almost constant percentage of activated states.
As a result, it is more likely to have rank(A) < Kd because
of identical columns in A, leading to more continuous local
minima.

In Table III, all local minima are continous due to N <
Kd. This case corresponds to over-parameterization and the
locations of some weight vectors can move freely without
changing the loss.

V. EXPERIMENTS ON REAL DATASETS

A. Datasets and Implementation

In order to show the theoretical predictions on the existence
of genuine local minima and verify their correctness for real
datasets, we perform experiments on two datasets commonly
used in machine learning community: MNIST and CIFAR-10
images. The MNIST dataset consists of 28 × 28 images of
handwritten digits, and the CIFAR-10 dataset comprises 32×32
color images of objects belonging to 10 classes including
airplane, automobile and bird etc. We extract the ﬁrst 100
samples of digit 0 and digit 1 respectively from MNIST training
data, and thus construct a binary classiﬁcation problem with

200 samples. For CIFAR-10 dataset, a binary classiﬁcation
problem with 200 samples is formed similarly with images
from the airplane and automobile classes. We convert the
CIFAR-10 color images into gray images to reduce the input
dimension and consequently computational overhead. Image
pixel values are scaled from [0, 255] to [0, 1]. The number of
hidden neurons is set to K = 10 for both datasets.

In order to determine whether a local minimum is genuine
when it is a hyperplane, we again use the linear programming
approach to judge whether the intersection of half-spaces is null.
For MNIST data with N = 200 samples and K = 10 hidden
neurons, each trial of the Matlab ’linprog’ function takes about
1.7 miniutes to converge on our commodity laptop computer
without GPU acceleration, and at each bias we run 100 trials
each with different random weight vectors. For CIFAR-10 data,
it takes about 1.5 hours to converge with N = 200 and K = 10,
and we run 10 trials at each bias to save time. At each bias,
the percentage of existing genuine local minima, percentage of
them being continuous, and the average percentage of activated
states for hidden neurons are reported.

Besides theoretical prediction on the existence of genuine
local minima, we also design and implement a method to verify
whether genuine local minima really exist and compare the
results with our theoretical predictions. We use gradient descent
to search for local minima. Starting from random {zj} and
{wj} with each weight vector wj lying in a certain cell, we
run gradient descent to update {wj} and {zj}. The gradients
are given as follows for squared loss function: ∀j ∈ [K],

∂L
∂zj

=

∂L
∂Rj

· wj,

∂L
∂wj

=

· zj,

(23)

∂L
∂Rj
(cid:33)

IikRk · xi − yi

· Iijxi

(24)

∂L
∂Rj

=

2
N

N
(cid:88)

(cid:32) K
(cid:88)

i=1

k=1

j · xi) ≤ 0, where w(cid:48)

Once gradient descent moves across cell boundaries, that
is, ∃i ∈ [N ], ∃j ∈ [K], (wj · xi) (w(cid:48)
j is
the updated weight vector, we can conclude that there are no
genuine local minima in the cells speciﬁed by {wj}. Otherwise,
if the updated weight vectors are still inside their deﬁning cells
and the gradient magnitudes are sufﬁciently small, we then
conclude that there exist genuine local minima. Therefore, the
results of gradient descent can be directly compared with
theoretical predictions and verify their correctness. In our
implementation, the stepsize of gradient descent is set to
10−6, and the threshold of gradient magnitude for identifying
existence of local minima is set to 10−3.

B. Results and Analysis

The results for MNIST and CIFAR-10 datasets are given
in Table IV and Table V respectively. They show that, except
the case where all hidden neuons are dead for all samples
(bias = −50 and bias = −80 respectively), there exist no
genuine local minima. This is again due to the fact that there
usually have samples in the gaps between weight vectors and
their optimal solutions. When all hidden neuons are dead for
all samples, the landscape is a ﬂat plateau with a loss of 1 and
genuine local minima exist. There must be someplaces where

6

TABLE I
EXISTENCE OF GENUINE LOCAL MINIMA FOR GAUSSIAN DATA (d = 3, K = 10, N = 2000).

Bias Genuineness (%)

Continuousness (%) Average Pg Activated states (%)

20
3
0
-3
-20

0
0
0
0
100

100
60
3
86
100

0.50
0.98
0.92
0.96
0

100
91.27
50.66
8.04
0

TABLE II
EXISTENCE OF GENUINE LOCAL MINIMA FOR GAUSSIAN DATA (d = 3, K = 20, N = 2000).

Bias Genuineness (%)

Continuousness (%) Average Pg Activated states (%)

20
3
0
-3
-20

0
0
0
0
100

100
91
9
98
100

0.51
0.99
0.97
0.99
0

100
91.67
50.23
7.93
0

TABLE III
EXISTENCE OF GENUINE LOCAL MINIMA FOR GAUSSIAN DATA (d = 10, K = 50, N = 400).

Bias Genuineness (%)

Continuousness (%) Average Pg Activated states (%)

20
3
0
-3
-20

0
0
0
0
100

100
100
100
100
100

0.48
0.77
0.73
0.77
0

100
81.99
49.33
18.38
0

TABLE IV
EXISTENCE OF GENUINE LOCAL MINIMA FOR MNIST DATA (d = 784, K = 10, N = 200).

Bias Genuineness (%)

Continuousness (%)

Trapped in cells (%) Activated states (%)

20
3
0
-3
-50

0
0
0
0
100

100
100
100
100
100

0
0
0
0
100

98.70
62.96
52.19
37.00
0

genuine local minima corresponding to global minima exist.
However, the possibility of hitting these locations by randomly
sampling weight matrix is very low, thus they are missed by
the typical locations in Table IV and Table V.

In Table IV and Table V, we have N < Kd, hence
the local minima (whether being genuine or not) are all
continous according to subsection II-A, corresponding to the
over-parameterization case.

The columns ’trapped in cells’ in Table IV and Table
V give the percentages of gradient descent trapped in their
starting cells at different biases. They are all zeros for those
cases no genuine local minima exist by theoretical predictions,
indicating the consistency between our theoretical predictions
and experimental results on the existence of genuine local
minima. For the case all hidden neurons are dead for all

samples (bias = −50 for MNIST, bias = −80 for CIFAR-
10), the gradient magnitudes are zero and gradient descent is
trapped, again coinciding with the theoretical predictions of
ﬂat plateau landscape .

We did not try much larger number of hidden neurons and
samples for real datasets due to limitation of computation
resources. However, the conclusion of no bad local minima
almost everywhere does not change since usually there are
samples in the gaps between weights and their optimal solutions.
Since the goal of this work is to understand the existence of
local minima, we also did not put much effort into performance
improvement.

VI. COUNT AND SIZE OF DIFFERENTIABLE CELLS

For one-hidden-layer ReLU networks, samples (xi, i ∈ [N ])
partition the weight space into a number of convex cells. After

7

TABLE V
EXISTENCE OF GENUINE LOCAL MINIMA FOR CIFAR-10 DATA (d = 1024, K = 10, N = 200).

Bias Genuineness (%)

Continuousness (%)

Trapped in cells (%) Activated states (%)

20
3
0
-3
-80

0
0
0
0
100

100
100
100
100
100

0
0
0
0
100

86.38
57.09
49.94
42.47
0

understanding the existence of local minima inside cells, we
now turn to explore the following questions to give a more
complete picture of local loss landscapes: how many cells are
there and how big are they? As shown in our previous work
[8], the shape of each cell is a polyhedron since it is formed
by intersection of hyperplanes of samples. [11] shows that in
d-dimensional input space, an arrangement of N hyperplanes
of samples in general can generate (cid:80)l
even for a small dataset of 1,000 training examples in 10-
dimensional input space, the number of cells in the whole
weight space will be as high as 1.9 × 1013, which makes it
impractical to search all the cells exhaustively.

(cid:18) N
i

cells. Thus,

i=0

(cid:19)

The size of cells tells us how far a local search can go
before changing the activation pattern of ReLU neurons. We
carry out experiments to explore the average diameter of cells.
By diameter, we mean the largest possible Euclidean distance
between two points in a cell. We take a sampling approach to
calculate the diameters. Starting from a weight vector w located
in a speciﬁc cell, we draw a number of random directions and
go along these directions in two opposite ways until hitting
the hyperplanes of samples conﬁning the cell. Speciﬁcally, for
a direction dj and sample xi, we can compute the distance
from w to xi along dj. When the ray of dj intersects with
the hyperplane of xi, there is (w + sijdj) · xi = 0, where
sij is the distance from w to the hyperplane of xi along
dj. Therefore sij = − w·xi
. The distance from w to the
dj ·xi
nearest sample along direction dj is s1j = mini {sij|sij (cid:62) 0},
and that along direction −dj is s2j = |maxi {sij|sij < 0}|.
Finally, by ﬁnding the largest total distance among all directions
maxj (s1j + s2j), we get the diameter of the cell in which w
resides.

The random directions are drawn from uniform distribuiton
and the number of directions is set to be ten times of input
dimension. We draw uniformly 1000 random weight vectors
and calculate the diameters of the cells in which they lie. The
open cells are excluded since their diameters tend to be inﬁnite,
and the average diameter of remaining cells is reported. We run
experiments on several datasets including MNIST, CIFAR-10,
and Gaussian data with different input dimensions. For each
dataset, we try with two data sizes: N = 2000 and N = 200.
The results are reported in Table VI.

Table VI shows that the size of cells is unsually small
compared with that of data bounding box (each dimension is
in [0, 1] for MNIST and CIFAR-10 datasets), thus when moving
through the weight space, activation pattern of ReLU neurons
changes frequently. Table VI also shows that the size of cells

8

is getting bigger when the number of samples getting smaller.
This is because the space between samples becomes bigger for
sparser dataset. Moreover, one can see that for Gaussian data
the size of cells becomes bigger for higher-dimesional inputs.
This is attributed to the fact that Euclidean distance becomes
bigger with dimension.

VII. RELATED WORK

There have been some experimental studies on the loss
landscape of DNNs and its visualization [5], [7], [2], [4], [6],
[3]. [5] showed that the linear path from initialization to the
optimized solution has a monotonically decreasing loss along
it. In this work, for one-hidden-layer ReLU networks, we
explained theoretically and veriﬁed experimentally that there
are no bad differentiable local minima almost everywhere,
verifying the conclusion of [5].

[2], [4] revealed that global minima usually constitute con-
tinuous regions for over-parameterized networks. In this work,
we give a theoretical explaination of continuous minima for
one-hidden-layer ReLU networks, and point out that continuous
global minima are actually in the form of hyperplanes.

The experimental study of [10] showed that spurious local
minima are common for two-layer networks where the label
is generated by a teacher network with unknown parameters.
Their objective function is different than ours and thus the
results can not be compared directly. It would be interesting
to explore whether our theory of local minima can be applied
to the student-teacher two-layer networks.

Theoretically, [9] computed the probability of getting trapped
in the cells that contain global minima during initialization,
whereas our theory considered how the probability of existing
genuine local minima varies in the whole weight space.

See part 1 of this work [8] for more works related to loss

landscape.

VIII. CONCLUSIONS AND FUTURE WORK

In this paper, based on our theory of local minima for
one-hidden-layer ReLU networks in [8], we ﬁrst analyzed
how the probability of existing genuine differentiable local
minima varies in the whole weight space for 1D Gaussian data.
We showed that this probability is very low in most regions.
We then implemented our theory of existing genuine local
minima with linear programming and used it to predict whether
bad local minima exist for higher-dimensional Gaussian data,
MNIST and CIFAR-10 datasets, and ﬁnd that there are no
bad local minima almost everywhere in weight space once

TABLE VI
DIAMETERS OF DIFFERENTIABLE REGIONS FOR VARIOUS DATASETS.

Dataset

Diameter (N =2000) Diameter (N =200)

MNIST
CIFAR-10
Gaussian (d=3)
Gaussian (d=10)
Gaussian (d=100)
Gaussian (d=300)

0.366
3.374
0.009
0.009
0.034
0.068

2.687
6.443
0.094
0.093
0.351
0.678

some hidden ReLU neurons are activated. These theoretical
predictions were veriﬁed by showing experimentally that
gradient descent was not trapped in the starting cells at different
locations. These theoretical and experimental results explain,
from the perspective of loss landscape, why local search based
methods can optimize one-hidden-layer ReLU networks of
any size and any input dimension if initialized at appropriate
locations.

In future work, we are intertested in conducting experiments
on real datasets to explore the existence of non-differentiable
local minima and saddle points. We also plan to study the
global loss landscape of deep ReLU networks.

REFERENCES

[1] Mark de Berg, Otfried Cheong, Marc van Kreveld, and Mark Overmars.
Computational Geometry: Algorithms and Applications. Springer, 3rd
edition, 2008.

[2] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred A.
Hamprecht. Essentially no barriers in neural network energy landscape.
In International Conference on Machine Learning, 2018.

[3] Stanislav Fort and Stanislaw Jastrzebski. Large scale structure of neural
networks loss landscapes. In Advances in Neural Information Processing
Systems, 2019.

[4] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov,
and Andrew G Wilson. Loss surfaces, mode connectivity, and fast
In Advances in Neural Information Processing
ensembling of dnns.
Systems, 2018.

[5] Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively
characterizing neural network optimization problems. In International
Conference on Learning Representations, 2015.

[6] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
Visualizing the loss landscape of neural nets. In Advances in Neural
Information Processing Systems, 2018.

[7] Qianli Liao and Tomaso Poggio. Theory of deep learning ii: Landscape
of the empirical risk in deep learning. arXiv preprint arXiv:1703.09833,
2017.

[8] Bo Liu. Understanding global loss landscape of one-hidden-layer relu
networks, part 1: theory. arXiv preprint arXiv:2002.04763, 2020.
[9] I. Safran and O. Shamir. On the quality of the initial basin in overspeciﬁed
neural networks. In International Conference on Machine Learning, pages
774–782, 2016.

[10] I. Safran and O. Shamir. Spurious local minima are common in two-
layer relu neural networks. In Proceedings of the 35 th International
Conference on Machine Learning, 2018.

[11] T Zaslavsky. Facing up to arrangements: face-count formulas for
partitions of space by hyperplanes. American Mathematical Society,
1975.

9

