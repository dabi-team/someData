1
2
0
2

r
a

M
4

]

G
L
.
s
c
[

1
v
5
8
3
3
0
.
3
0
1
2
:
v
i
X
r
a

GAUSSIAN PROCESSES MEET NEURALODES: A BAYESIAN
FRAMEWORK FOR LEARNING THE DYNAMICS OF PARTIALLY
OBSERVED SYSTEMS FROM SCARCE AND NOISY DATA

A PREPRINT

Mohamed Aziz Bhouri
Department of Mechanical Engineering
and Applied Mechanics
University of Pennsylvania
Philadelphia, PA 19104
bhouri@seas.upenn.edu

Paris Perdikaris
Department of Mechanical Engineering
and Applied Mechanics
University of Pennsylvania
Philadelphia, PA 19104
pgp@seas.upenn.edu

March 8, 2021

ABSTRACT

This paper presents a machine learning framework (GP-NODE) for Bayesian systems identiﬁ-
cation from partial, noisy and irregular observations of nonlinear dynamical systems. The pro-
posed method takes advantage of recent developments in differentiable programming to prop-
agate gradient information through ordinary differential equation solvers and perform Bayesian
inference with respect to unknown model parameters using Hamiltonian Monte Carlo sampling
and Gaussian Process priors over the observed system states. This allows us to exploit tempo-
ral correlations in the observed data, and efﬁciently infer posterior distributions over plausible
models with quantiﬁed uncertainty. Moreover, the use of sparsity-promoting priors such as the
Finnish Horseshoe for free model parameters enables the discovery of interpretable and parsimo-
nious representations for the underlying latent dynamics. A series of numerical studies is pre-
sented to demonstrate the effectiveness of the proposed GP-NODE method including predator-
prey systems, systems biology, and a 50-dimensional human motion dynamical system. Taken
together, our ﬁndings put forth a novel, ﬂexible and robust workﬂow for data-driven model dis-
covery under uncertainty. All code and data accompanying this manuscript are available online at
https://github.com/PredictiveIntelligenceLab/GP-NODEs.

Keywords Scientiﬁc machine learning; Dynamical systems; Uncertainty quantiﬁcation; Model discovery

1

Introduction

The task of learning dynamical systems from data is receiving increased attention across diverse scientiﬁc disciplines
including bio-mechanics [1], bio-medical imaging [2], biology [3], physical chemistry [4], climate modeling [5], and
ﬂuid dynamics [6]. In order to build a better physical understanding of a dynamical system’s behavior, it is crucial to
identify interpretable underlying features that govern its evolution [7]. The latter enables the possibility of providing
reliable future forecasts, and, as a consequence, the ability to improve intervention strategies [8] via appropriate design
or control decisions [9, 10].

The evolution of physical systems can be often characterized by differential equations, and several data-driven tech-
niques have been recently developed to synthesize observational data and prior domain knowledge in order to learn
dynamics from time-series data [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], infer the solution of differential equa-
tions [22, 23, 24, 25, 26], infer parameters, latent variables and unknown constitutive laws [27, 6, 28, 29], as well
as tackle forward and inverse problems for complex dynamical systems including cardiovascular ﬂow dynamics [30],
meta-materials [31], and epidemiology models [32].

 
 
 
 
 
 
A PREPRINT - MARCH 8, 2021

Most existing data-driven systems identiﬁcation techniques heavily depend on the quality of the available observations
and do not provide uncertainty quantiﬁcation [11, 14, 18, 13, 12]. For instance, the sparse regression methods put forth
in [14, 15] cannot be directly applied to irregularly sampled time-series or to trajectories with missing values, and can
become unstable in the presence of highly noisy data. More recent approaches leverage differentiable programming
to construct continuous-time models such as NeuralODEs [11, 13, 12], and are able to accommodate observations
with irregular sampling frequency, but they are not designed to provide a predictive uncertainty of the discovered
dynamics. The latter has been recently enabled by the Bayesian formulations of Yang et al. [33] and Dandekar et
al. [34]. However, as discussed in [33], these methods typically require tuning to obtain a good initialization for a
Markov Chain Monte Carlo (MCMC) sampler, while the use of Laplace priors cannot consistently promote sparsity
in the underlying dynamics, unless ad-hoc thresholding procedures are employed. Moreover, most aforementioned
approaches typically assume that the initial conditions are known and all state variables can be observed. These
limitations prevent applying such techniques to non-ideal settings where data is noisy, sparse and irregularly sampled,
the initial conditions may be unknown, and/or some of the system variables are latent (i.e. cannot be observed).

In order to build a systems identiﬁcation framework that alleviates difﬁculties related to low frequency and regularity
in the observed data, one can employ Gaussian process (GP) priors to model temporal correlations in observed state
variables [35, 36, 37, 38, 39]. One appealing feature of GPs is their ability to automatically enforce the “Occam’s
razor” principle [40], leading to increased robustness against over-ﬁtting in the small data regime, assuming that
the underlying dynamics is sufﬁciently smooth. However most aforementioned techniques either require a perfectly
known form of the dynamical system, which prevents the possibility of discovering unknown model forms, or model
the whole dynamical system as a Gaussian process, which does not typically yield an interpretable representation of
the latent dynamics.

In this work we employ Gaussian process priors and automatic differentiation through ODE solvers to formulate a
Bayesian framework (GP-NODE) for dynamical systems identiﬁcation under scarce, noisy and partially observed
data. Moreover, we employ a consistent Bayesian treatment for promoting sparsity in latent dynamics by employing
the Horseshoe and Finnish Horseshoe priors [41, 42, 43]. This enables us to seamlessly tackle cases involving a
known model parametrization, or cases for which the underlying model form is unknown but can be represented by a
large dictionary. The novel capabilities of the resulting computational framework can be summarized in the following
points:

• Leveraging GP priors, the proposed GP-NODE workﬂow can naturally model temporal correlations in the data and
remain robust in the presence of scarce and noisy observations, latent variables, unknown initial conditions, irregular
time sampling for each observable variable, and observations at different time instances for the observable variables.

• A key element of our framework is the design of novel GP mean functions with excellent extrapolation performance

using classical ODE integrators (e.g. Runge-Kutta methods).

• Leveraging automatic differentiation, we develop an end-to-end differentiable pipeline that enables the use of state-

of-the-art Hamiltonian Monte Carlo samplers for accelerated Bayesian inference (e.g. NUTS [44]).

• Thanks to sparsity-promoting Finnish Horseshoe priors, the proposed approach can recover interpretable and parsi-

monious representations of the latent dynamics.

Taken all together, our ﬁndings put forth a novel, ﬂexible and robust workﬂow for data-driven model discovery under
uncertainty that can potentially lead to improved algorithms for forecasting, control and model-based reinforcement
learning of complex systems.

The rest of this paper is organized as follows. Section 2 presents the GP-NODE method and the corresponding tech-
nical ingredients needed to perform Bayesian inference using Gaussian processes with NeuralODE mean functions.
Two different problem settings are considered: parameter inference with an unknown model form and parameter infer-
ence with domain knowledge incorporation. The data normalization step is also presented within section 2 in order to
obtain a universal model initialization and uniﬁed prior distributions across the various numerical examples presented
in section 3. These numerical studies include dynamics discovery for predator-prey system with an unknown model
form, including extensive comparisons against the SINDy algorithm of Brunton et. al. [14]. We also present exam-
ples on the calibration of a realistic Yeast Glycolysis model, as well as learning a 50-dimensional dynamical system
from motion capture data, for which we also present comparisons against the Nonparametric ODE Model (npODE)
of Heinonen et al. [37]. Finally, in section 4 we summarize our key ﬁndings, discuss the limitations of the proposed
GP-NODE approach, and carve out directions for future investigation.

2

A PREPRINT - MARCH 8, 2021

2 Methods

This section provides a comprehensive overview of the key ingredients that deﬁne the proposed framework (GP-
NODE), namely differential programming with NeuralODEs [13] and Bayesian inference with Hamiltonian Carlo
sampling and Gaussian process priors [45, 46]. Our presentation is focused on describing how these techniques can
be interfaced to obtain an efﬁcient and robust workﬂow for model discovery from imperfect time-series data.

2.1 GP-NODE: Gaussian processes with NeuralODE mean functions

We consider a general dynamical system of the form:

where x ∈ RD denotes the D-dimensional state space, and θf is a vector unknown parameters that deﬁnes the latent
dynamics f : RD → RD, and the (potentially) unknown initial conditions.

dx
dt

= f (x, t; θf ),

(1)

Let V be the set of indices of observable variables for which we assume we have data, such that for d ∈ V , we assume
that we have observations ˆxd(t(d)
) for i = 1, . . . , n(d); while for d /∈ V , only the initial condition ˆxd(t = 0) are
observed. Let Dv ≤ D denote the size of V and td ∈ Rn(d)
, for i = 1, . . . , n(d) and d ∈ V .
Let Nv = (cid:80)
Let z(t; θf ) denote the solution of equation (1) at time t for the trainable parameters θf , obtained using an differen-
tiable ODE solver [13],

d∈V n(d) and t denote the concatenation of td, for d ∈ V .

the vector containing t(d)

i

i

z(t; θf ) = ODEsolve(z0, t, θf ) ,

(2)

where z0 refers to the initial conditions which are either known, or part of the trainable parameters θf .

Efﬁciency of using Gaussian processes (GPs) as a prior for dynamics learning from sparse data has been shown in
previous studies as long as the underlying dynamics is sufﬁciently smooth, which can be guaranteed if the observed
data are correlated in time [35, 36, 37]. Dynamical systems governed by ordinary differential equations fall within
such category motivating the choice of using GPs as a prior for the system variables. For d ∈ V , the d-th dimension
of the state space x is modeled as a sum of Qd uncorrelated GPs, with a mean equal to the d-th dimension of z(θf ),
and covariance functions kd,q(·, ·, θd,q), q = 1, . . . , Qd such that,

xV (t) ∼ N (zV (t; θf ), K(t, t, θg) + K(cid:15)(θg)) ,

(3)

where xV (t) ∈ RNv is the concatenation of the predictions for the variables xd at the time instances t(d) for d ∈ V ,
zV (t) ∈ RNv the concatenation of zd at the time instances t(d) for d ∈ V , K(t, t; θg) ∈ RNv×Nv the following
block diagonal kernel matrix:



(cid:80)Qd1

q=1 kq,d1(t(d1), t(d1), θd1,q)

K(t, t; θg) =







. . .

(cid:80)QdDv

q=1 kq,dDv




(t(dDv ), t(dDv ), θdDv ,q)

,

(4)

where {d1, . . . , dDv } = V , and K(cid:15)(θg) the covariance matrix characterizing the noise process that may be corrupting
the observations:



(cid:15)d1

I
n(d1 )



K(cid:15)(θg) =




. . .

(cid:15)dDv

I
n(dDv

)


 , {d1, . . . , dDv } = V ,

(5)

where we assume that we have a Gaussian noise of variance (cid:15)d for the observable variable d ∈ V . θg refers to the GP
parameters and is given by:

The global parameter vector to be inferred is then given by:

θg = {θd,q, (cid:15)d, d ∈ V, q = 1, . . . , Qd} .

θ = (θf , θg) .

3

(6)

(7)

A PREPRINT - MARCH 8, 2021

2.2 Bayesian inference with Hamiltonian Monte Carlo

The Bayesian formalism provides a natural way to account for uncertainty, while also enabling the use of prior in-
formation for the unknown model parameters θ (e.g. sparsity for θf when using a dictionary as a representation of
f (·, ·; θf )). More importantly, it enables the complete statistical characterization for all inferred parameters in our
model. The latter, is encapsulated in the posterior distribution which can be factorized as:

p(θ|D) = p(θf , θg|D) ∝ p(D|θf , θg) p(θg) p(θf ),

(8)

where D denotes the available data, and p(D|θf , θg) is a likelihood function that measures the discrepancy between the
observed data and the model’s predictions for a given set of parameters (θf , θg). p(θg), p(θf ) are prior distributions
that can help us encode any domain knowledge about the unknown model parameters (θf , θg). Note that θg accounts
for the noise process that may be corrupting the observations. The posterior of equation (8) is generally intractable, and
approximate samples can be efﬁciently drawn using a state-of-the-art Hamiltonian Monte Carlo (HMC) [45] algorithm
such as NUTS [44] that automatically calibrates the step size of the Hamiltonian chain, and, consequently removes the
need for hand-tuning it as opposed to relying on vanilla HMC [33].

Here, we employ a Bayesian approach corresponding to the following likelihood:

p(D|θf , θg) = N (zV (t; θf ), K(t, t; θg) + K(cid:15)(θg)) ,

(9)

where zV (t) ∈ RNv is the previously introduced concatenation of zd at the time instances t(d) for d ∈ V , and z(t; θf )
is the solution deﬁned in (2) and obtained with a differentiable ODE solver enabling gradient back-propagation [13].
In addition to modeling temporal correlations in the observed state variables, the choice of a GP prior introduces an
implicit regularization mechanism that favors ﬁnding the simplest possible model that best ﬁts the observed data [46].
This so-called Occam’s razor principle [40] essentially enables automatic model selection without the need of resorting
to empirical selection criteria for balancing the trade-off between model complexity and data-ﬁt [35].

Given the observations forming the available data D, we would like to learn the parameters θ that best explain the
observed dynamics as quantiﬁed by the discrepancy L between the observed data and the model’s predictions. A
sufﬁcient way to update the probability p(θ|D) is through HMC [45], however appropriate methods should be consid-
ered in order to effectively back-propagate gradients through the likelihood computation given the dependency on the
dynamical system (1), which requires a differentiable ODE solver. Such task can be carried out by deﬁning the adjoint
of the dynamical system as a(t) = ∂L
∂x(t) . Then, the dynamical system describing the evolution of the adjoint can be
derived as [47, 13]:

da(t)
dt

= −a(t)T ∂f (x(t), t, θ)

∂x

.

(10)

Note that the adjoint a(t) can be computed by an additional call to the chosen ODE solver, and the target derivative
∂L
∂θ can be then computed as:

∂L
∂θ

= −

(cid:90) t0

t1

a(t)T ∂f (x(t), t, θ)

∂θ

dt,

(11)

where ∂f (x(t),t,θ)

∂θ

can be evaluated via automatic differentiation [13].

Taken all together, the main advantages of the GP-NODE approach can be summarized in the following points:

• The observed data does not need to be collected on a regular time grid.

• The data can be collected on different time grids for the different observed variables.

In such scenario,
the dynamical system (1) needs to be solved for the collection of all time instances over which the data is
available, and each variable is sub-sampled from the solution at the corresponding time instances for which
the observations are available. Hence, there is no constrain to impose on td, d ∈ V .

• The time-steps between different observations can be relatively large. For such large time steps, a classical
numerical scheme can be used to integrate equation (11) on a ﬁner temporal discretization, with the latter
being typically chosen according to the stability properties of the underlying ODE solver.

4

A PREPRINT - MARCH 8, 2021

2.3 Learning dynamics

In this section we present two different problem settings that cover a broad range of practical applications and can be
tackled by the GP-NODE method. The ﬁrst class consists of problems in which the model form of the underlying latent
dynamics is completely unknown. In this setting, one can aim to distill a parsimonious and interpretable representation
by constructing a comprehensive dictionary over all possible interactions and try to infer a predictive, yet minimal
model form [14, 15, 18, 33]. The second class of problems contains cases where a model form for the underlying
dynamics is prescribed by domain knowledge, but a number of unknown parameters needs to be calibrated in order
to accurately explain the observed data [29, 48]. The aforementioned problem classes are not mutually exclusive
and can be combined in a hybrid manner to construct “gray-box” models that are only partially informed by domain
knowledge [11, 33, 20]. As we will see in the following, the proposed workﬂow can seamlessly accommodate all
aforementioned cases in a uniﬁed fashion, while remaining robust with respect to incomplete model parametrizations,
as well as imperfections in the observed data.

2.3.1 Inferring an unknown model form via sparse dictionary learning

For the ﬁrst problem setting mentioned above, only the data is available and there is no other prior knowledge. A data-
driven approach is then used to learn the form of the dynamical system. Sparse identiﬁcation has been investigated to
identify the form of nonlinear dynamical systems [14], where the right hand side of the dynamical system is approx-
imated by a large dictionary, and the corresponding parameters for the different terms belonging to the dictionary are
inferred. The proposed method considers a similar setup as in [14] where a dictionary is used to approximate the right
hand side of the dynamical system. To this end, we parametrize the latent dynamics as:

dx
dt

= Aϕ(x),

(12)

where A ∈ RD×K is a matrix of unknown dictionary coefﬁcients, and K the length of the dictionary ϕ(x) ∈ RK.
A contains the parameters that will be estimated and ϕ(x) represents the prescribed dictionary of features (e.g.,
polynomials, Fourier modes, etc., and combinations thereof) [14, 15, 49] that are used to represent the latent the
dynamics.
Let θA ∈ RD×K be the vector containing the coefﬁcients of A, such that θA is a sub-vector of θf . In order to enable
the discovery of interpretable and parsimonious representations for the underlying dynamics, we use the sparsity-
promoting Finnish Horseshoe distribution as a prior [42] over the dictionary weights. The latter was originally devel-
oped for linear regression and can be detailed as follows:

,

τ0 =

m0
1
√
D K − m0
Nv
τ ∼ Half−C(0, τ0) ,
s2) ,
c2 ∼ Inv−G(

,

ν
2

ν
2

λm ∼ Half−C(0, 1) , i.i.d , m = 1, . . . , D K ,

˜λm =

c λm
c2 + τ 2 λ2
m

,

(θA)m ∼ N (0, τ ˜λm) ,

(13)

where ν and s are hyper-parameters which will be set equal to 1 across all numerical examples thanks to the data
normalization procedure we present in section 2.5, and m0 is the anticipated number of non-zero parameters.
In
standard applications of the Finnish Horseshoe in linear regression problems [42], m0 is simply ﬁxed as a prior guess.
In the GP-NODE framework, thanks to the incorporation of the physics knowledge through the ODE, m0 will always
be taken equal to its maximum possible value D × K − 1, independently of the true number of non-zero parameters.
Calibrating the physics-based dynamical system with the observed data allows the GP-NODE framework to discover
the actual number of non-zero parameters, which keeps the proposed method as general as possible, and without any
hyperparameter tuning or manual incorporation of any anticipated results. The heavy-tailed Cauchy prior distribution
for the local scales λm allows the data to push those corresponding to non-zero parameters to large values as needed,
which then pushes the corresponding parameters (θA)m above the global scale τ . Thanks to the parameterization of
the latter, the data is able to reﬁne the scale beyond the prior judgement of τ0. The scale c is introduced as an another
level to the prior hierarchy such that if we integrate it out of the distribution, it implies a marginal Student-t(ν, 0, s)
prior for each of the parameters (θA)m which should be sufﬁciently far above the global scale τ . Hence, the scale
c prevents the parameters (θA)m from surpassing the global scale τ and thus being un-regularized, which would

5

A PREPRINT - MARCH 8, 2021

result in posteriors that diffuse to signiﬁcantly large values. Such behavior would yield to a poor identiﬁcation of the
likelihood to propagate to the posterior and to inaccurate inference results.

The advantage of the Horseshoe and Finnish Horseshoe priors over the Laplace prior employed in [33] stems from the
difference in the “shrinkage effects” [41, 42, 43], which can be interpreted as the amount of weight that the posterior
mean for the inferred parameters θA places on the value of 0 once the data has been observed. In this regard, the
most important characteristic of the Horseshoe and Finnish Horseshoe priors is the distinguished separation between
the global and local shrinkage effects thanks to the use of the global scale τ and local scales λm: the global scale
tries to estimate the overall sparsity level, while the local scales permit ﬂagging the non-zero elements of the inferred
parameters θA. The heavy-tailed Cauchy prior distributions used for the local scales λm play a crucial role in this
process by allowing the estimated non-zero elements of θA to escape the strong pull towards the value of 0 applied
by the global scale τ . Hence, the Horseshoe and Finnish Horseshoe priors have the ability to shrink globally thanks
to the global scale τ , and yet act locally thanks to the local scales λm, which is not possible under the Laplace prior
whose shrinkage effect imposes a compromise between shrinking zero parameters and ﬂagging non-zero ones. Such
characteristic often results to overestimation of sparsity and under-estimation of the larger non-zero parameters [41].

2.3.2 Incorporating domain knowledge

The second problem setting involves parameter discovery, which corresponds to the setting where prior knowledge of
the underlying model form is available. Such physics-informed knowledge can sometimes be critical to learn certain
important parameters related to physical phenomenon such as limit cycles in chemical system [50], shock waves in
ﬂuid dynamics [28], evolution of biological systems [48], development of Darcy ﬂow [29], etc. In this context, the
exact form of the right hand side of the dynamical system as given in equation (1) is available. In other words, the
form of f (·, ·; θf ) is assumed to be known.

2.4 Generating forecasts with quantiﬁed uncertainty

The Hamiltonian Monte Carlo procedure described in section 2.2 produces a set of samples that concentrate in regions
of high-probability in the posterior distribution p(θ|D). Approximating this distribution is central to our workﬂow
as it enables the generation of future forecasts x∗(t∗) at time instances t∗ of size N ∗ with quantiﬁed uncertainty via
computing the predictive posterior distribution:

p(x∗(t∗)|D, t∗) =

(cid:90)

p(x∗(t∗)|θ, t∗)p(θ|D)dθ.

(14)

This predictive distribution provides a complete statistical characterization for the forecasted states by encapsulating
epistemic uncertainty in the inferred dynamics, as well as accounting for the fact that the model was trained on a ﬁnite
set of noisy observations. This allows us to generate GP-NODE-based plausible realizations of x∗(t∗, θ) by sampling
from the predictive posterior distribution as:

dh
dt

= f (h, t; θf ) ,

d(cid:48)(t∗, θ) = hd(cid:48)(t∗, θf ) , d(cid:48) /∈ V ,
x∗

d(t∗, θ) ∼ N (cid:0)µ∗
x∗

d(t∗, θ), K∗(t∗, θg)(cid:1) , d ∈ V ,

(15)

d(t∗, θ) = hd(t∗, θf ) + K(t∗, t; θg) (cid:0)K(t, t; θg) + K(cid:15)(θg)(cid:1)−1 (cid:0)xd(t(d)) − hd(t(d), θf )(cid:1) ,
µ∗

6

A PREPRINT - MARCH 8, 2021

where θf ∼ p(θf |D) and θg ∼ p(θg|D) are approximate samples from p(θf , θg|D) computed during model training
via HMC sampling; K(t, t; θg) and K(cid:15)(θg) are deﬁned in equations (4) and (5), respectively, and

K∗(t∗, θg) = K(t∗, t∗; θg) − K(t∗, t; θg) (K(t, t; θg) + K(cid:15)(θg))−1 K(t∗, t; θg)T ,



(cid:80)Qd1

q=1 kq,d1(t∗, t∗, θd1,q)

K(t∗, t∗; θg) =







(cid:80)Qd1

q=1 kq,d1(t∗, t(d1), θd1,q)

K(t∗, t; θg) =





. . .

(cid:80)QdDv

q=1 kq,dDv

(t∗, t∗, θdDv ,q)







,



(16)

. . .

(cid:80)QdDv

q=1 kq,dDv




(t∗, t(dDv ), θdDv ,q)

,

where {d1, . . . , dDv } = V . h(t∗, θf ) denotes any numerical integrator that predicts the system’s state at time in-
stances t∗.

Finally, it is straightforward to utilize the posterior samples of θ ∼ p(θ|D) to approximate the ﬁrst- and second-order
statistics of the predicted states x∗(t∗) as

(cid:90)

(cid:90)

ˆµx∗ (t∗) =

x∗ (t∗) =
ˆσ2

x∗
θ(t∗)p(θ|D)dθ ≈

1
Ns

Ns(cid:88)

i=1

x∗
θi

(t∗),

[x∗

θ(t∗) − ˆµx∗ (t∗)]2p(θ|D)dθ ≈

1
Ns

Ns(cid:88)

i=1

[x∗
θi

(t∗) − ˆµx∗ (t∗)]2,

(17)

(18)

where Ns denotes the number of samples drawn via Hamiltonian Monte Carlo to simulate the posterior, i.e., θi ∼
p(θ|D), i = 1, . . . , Ns. Note that higher-order moments are also readily computable in a similar manner.

2.5 Model initialization, priors and data pre-processing

To promote robustness and stability of the proposed GP-NODE framework across different numerical examples, the
training data is appropriately normalized in order to prevent gradient pathologies during back-propagation [51]. Hence,
the observable variables are normalized as follows:

˜xd =

xd
|xd(t(d)

i

max
i=1,...,n(d)

, d ∈ V .

)|

The time instances, which are used as input to the Gaussian processes, are also normalized as follows:

˜t(d)
i =

t(d)
i
max
i=1,...,n(d) , d∈V

t(d)
i

, d ∈ V .

(19)

(20)

For all the problems considered in section 3, modeling each of the observable variables using independent Gaussian
process priors is sufﬁcient to obtain satisfactory results. Hence, we have chosen Qd = 1 for d ∈ V (see equation
(4)). Temporal correlations in the observed data are captured via employing an exponentiated quadratic kernel which
implicitly introduces a smoothness assumption for the latent dynamics, such that:

kq,d(t, t(cid:48), θd,q) = wd,qe

− (t−t(cid:48) )2
l2
d,q

, d ∈ V , q = 1 ,

(21)

where θd,q ≡ (wd,q, ld,q). Note that other kernel choices can be readily incorporated to reﬂect more appropriate prior
assumptions for a given problem setting.

Data normalization enables the use of the following uniﬁed priors across the all numerical examples

wd,q ∼ Lognormal(0, 1) , q = 1, . . . , Qd , d ∈ V ,
ld,q ∼ Gamma(α = 1, β = 1/2) , q = 1, . . . , Qd , d ∈ V ,
(cid:15)d ∼ Lognormal(0, 1) , q = 1, . . . , Qd , d ∈ V .

(22)

7

A PREPRINT - MARCH 8, 2021

The prior distributions considered for the parameters θf depend on the class of problems detailed in section 2.3.
For problems for which the undelrying model form is known, the prior distributions of θf are deﬁned based on the
available domain knowledge. For problem settings with an unknown model form, the Finnish horseshoe distribution,
detailed in section 2.3.1, is considered as a prior for the dictionary parameters, while the priors of any remaining
parameters are deﬁned based on the available domain knowledge. Finally, for all the numerical examples presented in
section 3, the NUTS sampler [44] is used with 4, 000 warmup (burn-in) steps, followed by 8, 000 HMC iterations with
a target acceptance probability equal to 0.85.

3 Results

In this section, we present a comprehensive collection of numerical studies to illustrate the key features of the GP-
NODE methodology, and place into context against existing state-of-the-art methods for systems identiﬁcation such as
the SINDy framework of Brunton et al. [14], and of the Nonparametric ODE Model method (npODE) of Heihonen et
al. [37] which has been shown to outperform the Gaussian Process Dynamical Model approach (GPDM) [52] and the
Variational Gaussian Process Dynamical System method (VGPDS) [53]. Speciﬁcally, we expand on three benchmark
problems that cover the problem settings discussed in section 2.3. The algorithmic settings used across all cases follow
the discussion provided in 2.5. Our implementation leverages GPU hardware and automatic differentiation via JAX
[54] and the NumPyro library for probabilistic programming [55, 56]. All code and data presented in this section will
be made publicly available at https://github.com/PredictiveIntelligenceLab/GP-NODEs.

3.1 Dictionary learning for a predator-prey system with with time gaps, different time grids and unknown

initial conditions

This case study is designed to illustrate the capability of the GP-NODE framework to accommodate noisy data, ob-
servations with a time gap and at different time instances, and unknown initial conditions; a common practical setting
that cannot be effectively handled by popular systems identiﬁcation methods [14, 15, 57, 17, 18]. To this end, let us
consider a classical prey-predator system described by the Lotka–Volterra equations:

dx1
dt
dx2
dt

= αx1 + βx1x2 ,

= δx1x2 + γx2 ,

(23)

which is known to exhibit a stable limit cycle behavior for α = 1.0, β = −0.1, γ = −1.5, and δ = 0.75. Our goal
here is to recover this dynamical system directly from data using a polynomial dictionary parametrization taking the
form:

(cid:21)

(cid:20) dx1
dt
dx2
dt

= Aϕ(x) =

(cid:20)a11 a12 a13 a14 a15 a16 a17
a21 a22 a23 a24 a25 a26 a27

(cid:21)












,

(24)












x1
x2
x1x2
x2
1
x2
2
x3
1
x3
2

where the aij’s are unknown scalar coefﬁcients to be estimated. The active non-zero parameters are highlighted with
red color for clarity. To generate the training data, we simulate the exact dynamics of equation (23) in the time interval
t ∈ [0, 16.5] with α = 1.0, β = −0.1, γ = −1.5, δ = 0.75 and the initial condition: (x1, x2) = (5, 5). The training
data-set D is considered such that the simulated data is perturbed by 10% white noise. Both variables for this example
are taken as observable (V = {1, 2}) but their corresponding observations are sampled at different time instances as
follows:

t(1) = (1.5, 1.8, 2.1, . . . , 15.9, 16.2, 16.5) ,
t(2) = (0, 1.65, 1.95, 2.25, . . . , 16.05, 16.35, 16.5) .
As such, we have 51 observations for variable x1 with a time gap of 1.5, a time step equal to 0.3, and an unknown
initial condition. We also have 52 observations for variable x2 with a time gap of 1.65 and a time step equal to 0.3.
For this example, the dynamics parameters vector is given by θf = (θA, x1,0), where θA is the vector containing
the coefﬁcients of A and x1,0 is the inferred initial condition. Given this noisy, sparse and irregular training data, our
goal is to demonstrate the performance of the GP-NODE framework in identifying the unknown model parameters
θ = (θf , θg) (see equation (7)) by inferring their posterior distributions. The Finnish Horseshoe distribution is used

(25)

8

A PREPRINT - MARCH 8, 2021

Figure 1: Parameter inference in a predator-prey system: (a) Learned dynamics versus the true dynamics and the
training data of x1(t). (b) Learned dynamics versus the true dynamics and the training data of x2(t).

Figure 2: Predator-prey system dynamics: Uncertainty estimation of the inferred model parameters obtained using
the GP-NODE method. Estimates for the minimum, maximum, median, ﬁrst quantile and third quantile are provided,
while the true parameter values are highlighted in blue.

as prior for the parameters θA as detailed in section 2.3.1, while the prior distribution of θg is discussed in section 2.5.
Finally, the prior distribution of x1,0 is taken as the uniform distribution over [4, 6].

Our numerical results are presented in ﬁgure 1 showing the training data and the estimated trajectories based on
the inferred parameters. Uncertainty estimates for the inferred parameters can also be deducted from the computed
posterior distribution p(θf |D), as presented in the box plots of ﬁgure 2 where the minimum, maximum, median, ﬁrst
quantile and third quantile obtained from the HMC simulations for each parameter are presented.

It is evident that the GP-NODE approach is: (i) able to provide an accurate estimation for the unknown model param-
eters, (ii) yield an estimator with a predicted trajectory that closely matches the exact system’s dynamics, (iii) return a
posterior distribution over plausible models that captures both the epistemic uncertainty of the assumed parametriza-
tion and the uncertainty induced by training on a ﬁnite amount of noisy training data, and (iv) propagate this uncertainty
through the system’s dynamics to characterize variability in the predicted future states.

To verify the statistical convergence of the HMC chains, we have employed the Geweke diagnostic and Gelman
Rubin tests [58, 59]. The Gelman Rubin tests have provided the following values for the non-zero parameters (a11 :
1.000, a13 : 1.604, a22 : 1.191, a23 : 1.012) and the inferred initial condition (x1,0 : 1.159), which are all close to the
optimal value of 1, as expected. In order to perform the Geweke diagnostic, we have “thinned” the 8, 000 HMC draws
by randomly sampling one data point every eight consecutive steps of the chain. In order to implement the Geweke

9

A PREPRINT - MARCH 8, 2021

Figure 3: HMC convergence diagnostics for the predator-prey system parameters a11, a13, a22, a23, x1,0: (a) The
Geweke diagnostic test based on the chain of 1000 samples: we compare the mean of the ﬁrst 10% of the chain of
1000 samples to the mean of 20 sub-samples from the last 50% of the chain. (b) Auto-correlation as function of the
lag.

a12
0
0
0
0

case 1
case 2
case 3
case 4

a11
0
0
-1.499
0

a27
0
0
0
0
Table 1: Dictionary learning for a predator-prey system: Point estimates for the dictionary coefﬁcients obtained by
the SINDy algorithm [14] for cases 1, . . . , 4. The model’s active non-zero parameters are highlighted with red color
for clarity.

a21
-1.619
-11.803
0
-0.952

a26
0
-0.882
-0
0

a22
-1.039
-0.977
0
-1.330

a13
0
0
0.750
0

a24
0.779
7.057
0
0.194

a23
0.501
0.511
0
0.677

a14
0
0
0
0

a15
0
0
0
0

a17
0
0
0
0

a16
0
0
0
0

a25
0
0
0
0

diagnostic, we considered the ﬁrst 10% of the chain of the 1000 samples and compared their mean with the mean of
20 sub-samples from the last 50% of the chain. As shown in plot (a) of ﬁgure 3, the Geweke z-scores for the four
non-zero parameters are well between −2 and 2. The results of auto-correlation are given in plot (b) of ﬁgure 3. The
Geweke diagnostic, the Gelman Rubin tests results and the auto-correlation values demonstrate a “healthy” mixing
behavior of the NUTS sampler [44].

Finally, we show the merits of the GP-NODE method by comparing it against the SINDy algorithm of Brunton et. al.
[14] for the benchmark presented above with the following simplifying modiﬁcations. First, the deterministic SINDy
approach cannot accommodate inference over unknown initial conditions. Hence, here we will assume the latter as
known. Moreover, SINDy requires that the observations provided for the different variables are sampled at the same
time instances. Hence x1 and x2 observations are provided at the following time instances:

t(1) = t(2) = (0, 1.5, 1.8, 2.1, . . . , 15.9, 16.2, 16.5) .

(26)

To this end, we have 52 observations for each of the variables with a time gap of 1.5 and a time step equal to 0.3.
Under this problem setup (case 1), the SINDy algorithm fails and cannot infer the parameters appropriately as shown
in the ﬁrst row of table 1 and in plots (a) and (b) of ﬁgure 4.

We kept decreasing the time step but never obtained the right inference with SINDy. The smallest time step considered
was dt = 0.00234375 such that the data contains the initial condition plus 6, 402 observations between t = 1.5 and
t = 16.5 for each of the variables. The results obtained with SINDy under such setup (case 2) are given in the second
row of table 1 and in plots (c) and (d) of ﬁgure 4. We further simpliﬁed the problem by considering the previous setup
(case 2), but with noise free data (case 3) and we still obtained erroneous predictions with SINDy, as shown in the
third row of table 1, and in plots (e) and (f) of ﬁgure 4. As a consequence, since considering noise free data and an
extremely small time step did not help us obtain good results with the SINDy algorithm, it is clear that the latter is not
able to handle the presence of a temporal gap in the observations for the predator-prey system considered.

Next, we extend our comparison by removing the time gap in the observations. At ﬁrst, we keep the same time step
(dt = 0.3) as above, and generate training data for variables x1 and x2 at the following time instances:

t(1) = t(2) = (0, 0.3, 0.6, 0.9, . . . , 15.9, 16.2, 16.5) .

(27)

Under this problem setup (case 4), the SINDy algorithm still fails because of the sparsity of the observations and
cannot infer the parameters appropriately as shown in the fourth row of table 1, and in plots (g) and (h) of ﬁgure 4.

10

A PREPRINT - MARCH 8, 2021

Figure 4: Parameter inference in a predator-prey system: (a), (b) SINDy’s predictions for x1(t) and x2(t) respectively
versus the true dynamics and the training data for case 1. (c), (d) SINDy’s predictions for x1(t) and x2(t) respectively
versus the true dynamics and the training data for case 2. (e), (f) SINDy’s predictions for x1(t) and x2(t) respectively
versus the true dynamics and the training data for case 3. (g), (h) SINDy’s predictions for x1(t) and x2(t) respectively
versus the true dynamics and the training data for case 4.

11

a11
1.066

a12
0

a13
-0.102

a14
0

a15
0

a16
0

a17
0

a21
0

a22
-1.465

a23
0.730

a24
0

a25
0

a26
0

a27
0

Table 2: Dictionary learning for a predator-prey system: Point estimates for the dictionary coefﬁcients obtained by the
SINDy algorithm [14] for case 5. The model’s active non-zero parameters are highlighted with red color for clarity.

A PREPRINT - MARCH 8, 2021

Figure 5: Parameter inference in a predator-prey system: (a) SINDy’s prediction versus the true dynamics and the
training data of x1(t) for case 5. (b) SINDy’s prediction versus the true dynamics and the training data of x2(t) for
case 5.

Next, we proceed by decreasing the time step until SINDy is able to provide a reasonable identiﬁcation of the predator-
pray dynamics. The required time step was dt = 0.0375 such that the data does not contain any time gap in the
observations, corresponding to 441 equally spaced samples between t = 0 and t = 16.5 for each of the state variables.
The results obtained with SINDy under this setup (case 5) are given in the table 2, and in panels (a) and (b) of ﬁgure 5.
Although the inferred values for the parameters are close to the exact ones, the small difference between them results
in predicted trajectories that deviate from the true ones, with the deviation being more signiﬁcant as we move further
in time. Compared to the predictions obtained with the GP-NODE methodology in ﬁgure 1, the differences between
the MAP trajectories obtained with the GP-NODE and the true trajectories are clearly smaller than the differences
obtained with the SINDy algorithm, although we considerably simpliﬁed the problem setup in order to favor the
performance of SINDy by removing the time lag in the observations, not inferring any initial condition, assuming that
both variables have observations at the same time instances, and reducing the time step of the trajectories used for
training. Moreover, the GP-NODE method is a Bayesian approach and provides uncertainty estimates for the inferred
parameters and also for future forecasts of the variable trajectories, which is obviously missing in the deterministic
SINDy framework.

3.2 Bayesian calibration of a Yeast Glycolysis model

This example aims to illustrate the performance of the proposed GP-NODE framework applied to a more realistic
setting involving biological systems. To this end, we consider a yeast glycolysis process which can be described by a

12

7-dimensional dynamical system [3, 48] of the form:

A PREPRINT - MARCH 8, 2021

dS1
dt
dS2
dt
dS3
dt
dS4
dt
dN2
dt
dA3
dt
dSex
4
dt

= J0 − v1,

= 2v1 − v2 − v6,

= v2 − v3,

= v3 − v4 − J,

= v2 − v4 − v6,

= −2v1 + 2v3 − v5,

= φJ − v7,

where the terms v1, v2, v3, v4, v5, v6, v7, N1 and A2 on the right hand side are deﬁned as:

v1 = k1S1A3[1 + (

A3
KI

)q]−1,

v2 = k2S2N1,
v3 = k3S3A2,
v4 = k4S4N2,
v5 = k5A3,
v6 = k6S2N2,
v7 = kSex
4 ,

N1 + N2 = N,
A2 + A3 = A.

(28)

(29)

Here we assume that this model form is known from existing domain knowledge. A training data-set is gener-
ated from a single simulated trajectory of the system with an initial condition: (S1, S2, S3, S4, N2, A3, Sex
4 ) =
(0.5, 1.9, 0.18, 0.15, 0.16, 0.1, 0.064) in the time interval t ∈ [0, 3], assuming a ground truth set of parameters ob-
tained from the experimental data provided in [3]: J0 = 2.5 mM/min, k1 = 100.0 mM/min, k2 = 6.0 mM/min,
k3 = 16.0 mM/min, k4 = 100.0 mM/min, k5 = 1.28 mM/min, k6 = 12.0 mM/min, k = 1.8 mM/min, κ = 13.0
mM/min, q = 4.0, KI = 0.52 mM, N = 1.0 nM, A = 4.0 mM and φ = 0.1. The training dataset D is considered
such that the simulated data is perturbed by 10% white noise. Only N2, A3 and Sex
4 are considered as observable vari-
ables for this example (V = {5, 6, 7}), but their corresponding observations are sampled at different time instances as
follows:

t(5) = (0.525, 0.575, 0.625, . . . , 2.925, 2.975, 3) ,
t(6) = (0, 0.5, 0.55, 0.6, . . . , 2.9, 2.95, 3) ,
t(7) = (0, 0.525, 0.575, 0.625, . . . , 2.925, 2.975, 3) .
Hence we have 51 observations for variable N2 with a time gap of 0.525, a time step equal to 0.05, and an unknown
initial condition; 52 observations for variable A3 with a time gap of 0.5 and a time step equal to 0.05; and 52 ob-
servations for variable Sex
4 with a time gap of 0.525 and a time step equal to 0.05. For this example, the dynamics
parameters vector is of size 15 and given by θf = (J0, k1, k2, k3, k4, k5, k6, k, κ, q, KI , φ, N, A, N2,0), where N2,0
is the inferred initial condition for the variable N2. Given this noisy, sparse and irregular training data, our goal is to
demonstrate the performance of the GP-NODE framework in identifying the unknown model parameters θ = (θf , θg)
(see equation (7)) by inferring their posterior distribution. The prior distribution of θg is discussed in section 2.5, and
the prior distributions of θf are taken as the uniform distributions deﬁned over the intervals detailed in table 3.

(30)

Uncertainty estimates for the inferred parameters can be deducted from the computed posterior distribution p(θf |D),
as presented in the box plots of ﬁgure 6 where the minimum, maximum, median, ﬁrst quantile and third quantile
obtained from the HMC simulations for each parameter are presented. All inferred parameters closely agree with the
ground truth values used to generate the training data as reported in [3]. Notice that that all true values fall between
the predicted quantiles, while for a few parameters (only K1, A and N2,0 out of the total 15 parameters considered),
the true values are not perfectly captured by the conﬁdence intervals. This behavior can be explained by the lower

13

A PREPRINT - MARCH 8, 2021

Interval

Interval

Interval

J0
[1,10]
k5
[0.1,2]
KI
[0.1,2]

k1
[80,120]
k6
[2,20]
φ
[0.05,1]

k2
[1,10]
k
[0.1,2]
N
[0.1,2]

k3
[2,20]
κ
[2,20]
A
[1,10]

k4
[80,120]
q
[1,10]
N2,0
[0,1]

Table 3: Yeast Glycolysis dynamics: Interval of uniform distributions considered as priors for parameters θf .

Figure 6: Yeast Glycolysis dynamics: Uncertainty estimation of the inferred model parameters obtained using the
proposed Bayesian differential programming method. Estimates for the minimum, maximum, median, ﬁrst quantile
and third quantile are provided, while the true parameter values are highlighted in blue.

sensitivity of the dynamical trajectories with respect to those parameters, and also by the relatively small size of the
sparse training data considered since only 3 variables of the 7-dimensional dynamical system are observed with one
unknown initial condition.

The inferred model also allows us to produce forecasts of future states with quantiﬁed uncertainty via the predictive
posterior distribution of equation (14). These extrapolated states are shown in ﬁgure 7. It is evident that the uncertainty
estimation obtained via sampling from the joint posterior distribution over all model parameters p(θf , θg|D) is able
to well capture the reference trajectory of the high dimensional dynamical system. As expected, the uncertainty is
larger for latent variables, such as S3, and for unseen time instances. Nonetheless, even for these cases, the uncertainty
estimates always capture the exact trajectory and the MAP trajectory follows the true one with sufﬁcient accuracy.

To verify the statistical convergence of the HMC chains, we have employed the Geweke diagnostic and Gelman Rubin
tests [58, 59]. The Gelman Rubin tests have provided the following values for the parameter θf (J0 : 1.034, k1 :
1.018, k2 : 1.035, k3 : 1.165, k4 : 1.029, k5 : 1.390, k6 : 1.607, k : 1.066, κ : 1.247, q : 1.187, KI : 1.006, φ :
1.274, N : 1.005, A : 1.016, N2,0 : 1.407), which are all close to the optimal value of 1, as expected. In order to
perform the Geweke diagnostic, we have “thinned” the 8, 000 HMC draws by randomly sampling one data point every
eight consecutive steps of the chain. In order to implement the Geweke diagnostic, we considered the ﬁrst 10% of the

14

A PREPRINT - MARCH 8, 2021

Figure 7: Learning Yeast Glycolysis dynamics: (a) Learned dynamics versus the true dynamics and the training data
of S1. (b) Learned dynamics versus the true dynamics and the training data of S2. (c) Learned dynamics versus the
true dynamics and the training data of S3. (d) Learned dynamics versus the true dynamics and the training data of
S4. (e) Learned dynamics versus the true dynamics and the training data of N2. (f) Learned dynamics versus the true
dynamics and the training data of A3. (g) Learned dynamics versus the true dynamics and the training data of Sex
4 .

15

A PREPRINT - MARCH 8, 2021

Figure 8: HMC convergence diagnostics for the Yeast-Glycolysis system parameter θf : (a) The Geweke diagnostic
test based on the chain of 1000 samples: we compare the mean of the ﬁrst 10% of the chain of 1000 samples to the
mean of 20 sub-samples from the last 50% of the chain. (b), (c) and (d) Auto-correlation as function of the lag.

chain of the 1, 000 samples and compared their mean with the mean of 20 sub-samples from the last 50% of the chain.
As shown in plot (a) of ﬁgure 8, the Geweke z-scores for the four non-zero parameters are well between −2 and 2. The
results of auto-correlation are given in plots (b), (c) and (d) of ﬁgure 8. The Geweke diagnostic, the Gelman Rubin
tests results and the auto-correlation values demonstrate a “healthy” mixing behavior of the NUTS sampler.

Finally, to illustrate the generalization capability of the inferred model with respect to different initial conditions than
those used during training, we have assessed the quality of the predicted states starting from a random initial condition
that is uniformly sampled within (S1 ∈ [0.15, 1.60], S2 ∈ [0.19, 2.10], S3 ∈ [0.04, 0.20], S4 ∈ [0.10, 0.35], N2 ∈
[0.08, 0.30], A3 ∈ [0.14, 2.67], Sex
4 ∈ [0.05, 0.10]). Figure 9 shows the prediction obtained with the randomly picked
initial condition [0.428, 1.378, 0.110, 0.296, 0.252, 0.830, 0.064]. The close agreement with the reference solution
indicates that the inferred model is well capable of generalizing both in terms of handling different initial conditions,
as well as extrapolating to reliable future forecasts with quantiﬁed uncertainty.

3.3 Dictionary learning of human motion capture data

In this ﬁnal example, our goal is to investigate the performance of the GP-NODE algorithm in a realistic problem using
experimental data. To this end, we consider a benchmark data-set of human motion capture data from the Carnegie
Mellon University motion capture (CMU mocap) database. The data-set contains 50-dimensional pose measurements
of human walking motion that here we denote as y(ti). Each pose dimension records a measurement in different parts
of the body during movement [1]. Following previous studies on human motion capture [52, 1, 53, 37], we perform
Principal Component Analysis (PCA) in order to project the original data-set to a latent space spanning the 3 dominant
eigen-directions. Hence, the state space considered x(t) is three dimensional, and the original 50-dimensional state
y(t) can be recovered through the corresponding PCA projection.

The performance of the proposed method is evaluated by removing around 20% of the frames from the middle of
the trajectories. The latter will be used to assess the predictive accuracy of the method. The goal is to learn a model
with the remaining data, and to forecast the missing values. We measure the root mean square error (RMSE) over
the original feature space of the 50-dimensional state y(t) for which the experimental measurements are provided.
The original dimensions are reconstructed from the latent space trajectories of x(t). We compare our method to the

16

A PREPRINT - MARCH 8, 2021

Figure 9: Learning Yeast Glycolysis dynamics: Future forecasts with quantiﬁed uncertainty from unseen initial condi-
tions that were not used during model training.

17

A PREPRINT - MARCH 8, 2021

recently proposed Nonparametric ODE Model method (npODE) [37], which was shown to outperform the Gaussian
Process Dynamical Model approach (GPDM) [52] and the Variational Gaussian Process Dynamical System method
(VGPDS) [53]. The npODE results are obtained using the original implementation provided by the authors at https:
//github.com/cagatayyildiz/npode.

First, we apply the proposed Bayesian approach to recover the dynamical system of human motion data using a
polynomial dictionary parameterization taking the form:

(cid:21)

(cid:20) dx1
dt
dx2
dt

= Aϕ(x) =

(cid:34)a11 a12 a13 a14 a15 a16 a17
a21 a22 a23 a24 a25 a26 a27
a31 a32 a33 a34 a35 a36 a37

(cid:35)











: caseA ,

(31)











1
x1
x2
x3
x1x2
x1x3
x2x3

where the aij’s are unknown scalar coefﬁcients that will be estimated.

We consider the trajectory “07 02.amc” from the CMU mocap database, such that we have a total of 82 pose measure-
ments y(ti). The frames from the 35-th to the 49-th measurements are removed, such that the model is trained with
67 observations. For this example, the latent dynamics parameter vector is simply given by θf = θA, where θA is the
vector containing the coefﬁcients of A. Given the truncated training data, the GP-NODE framework is used to identify
the unknown model parameters θ = (θf , θg) (see equation (7)) by inferring their posterior distributions. The Finnish
Horseshoe distribution is used as prior for the parameters θA as discussed in section 2.3.1, while the prior distribution
of θg is detailed in section 2.5.

Our numerical results are summarized in ﬁgure 10 showing the training data and the estimated trajectories based on
the inferred parameters. It is evident that the proposed methodology is able to provide an estimator with a predicted
trajectory that closely matches the exact system’s dynamics even for the unseen data. Indeed, for the time interval
corresponding to the missing data, the MAP trajectories for the different variables xi follow the true ones. Moreover
the true trajectories always reside within the two standard deviations band, showing that the method is able to determine
a posterior distribution over plausible models that captures the epistemic uncertainty of the assumed parametrization
and the uncertainty induced by training on a truncated and noisy training data. Such uncertainty is propagated through
the system dynamics to characterize variability in the unseen system states, since the highest uncertainty is obtained
for the time interval corresponding to missing data.

Uncertainty estimates for the inferred parameters can also be deducted from the computed posterior distribution
p(θf |D), as presented in the box plots of ﬁgure 11 where the minimum, maximum, median, ﬁrst quantile and third
quantile obtained from the MCMC simulations for each parameter are presented. We also assess the performance
of the method by verifying the accuracy of the predicted 50-dimensional state y(t) that are recovered via PCA re-
construction. These numerical results are given in ﬁgure 12 showing the obtained prediction for the PCA-recovered
trajectories of 6 of the most dominant variables of the 50-dimensional state, compared to the corresponding true tra-
jectories. As observed for the state x, the MAP trajectories for the PCA-recovered variables follow the true ones, and
the uncertainty is well quantiﬁed for the time interval corresponding to missing data.

To verify the statistical convergence of the HMC chains, we have again employed the Geweke diagnostic and Gelman
Rubin tests [58, 59]. The Gelman Rubin tests have provided the following values for the most signiﬁcant parameters
of θf (a13 : 1.001, a14 : 1.004, a16 : 1.000, a17 : 1.000, a22 : 1.003, a25 : 1.001, a26 : 1.006, a34 : 1.001, a35 :
1.001, a36 : 1.002), which are all close to the optimal value of 1, as expected.
In order to perform the Geweke
diagnostic, we have “thinned” the 8, 000 HMC draws by randomly sampling one data point every eight consecutive
steps of the chain. In order to implement the Geweke diagnostic, we considered the ﬁrst 10% of the chain of the 1000
samples and compared their mean with the mean of 20 sub-samples from the last 50% of the chain. As shown in plot
(a) of ﬁgure 13, the Geweke z-scores for the four non-zero parameters are well between −2 and 2. The results of
auto-correlation are given in plots (b) and (c) of ﬁgure 13. The Geweke diagnostic, the Gelman Rubin tests results and
the auto-correlation values demonstrate a “healthy” mixing behavior of the NUTS sampler.

In order to demonstrate the robustness of the proposed method with respect to the size of the dictionary, we augment
the previous dictionary (see equation 31) to include a complete second order polynomial approximation for the latent

18

A PREPRINT - MARCH 8, 2021

Figure 10: Parameter inference of a human motion dynamics system, case A: (a) Learned dynamics versus the true
dynamics and the training data of x1(t). (b) Learned dynamics versus the true dynamics and the training data of x2(t).
(c) Learned dynamics versus the true dynamics and the training data of x3(t). The shaded region indicates the time
range of unseen data during model training.

Figure 11: Human motion dynamics system, case A: Uncertainty estimation of the inferred model parameters obtained
using the GP-NODE method. Estimates for the minimum, maximum, median, ﬁrst quantile and third quantile are
provided.

19

A PREPRINT - MARCH 8, 2021

Figure 12: Parameter inference of a human motion dynamics system, case A: (a) Learned dynamics versus the true
dynamics of PCA-recovered y27(t). (b) Learned dynamics versus the true dynamics of PCA-recovered y34(t). (c)
Learned dynamics versus the true dynamics of PCA-recovered y37(t). (d) Learned dynamics versus the true dynamics
of PCA-recovered y39(t).
(f) Learned
dynamics versus the true dynamics of PCA-recovered y48(t). The shaded region indicates the time range of unseen
data during model training.

(e) Learned dynamics versus the true dynamics of PCA-recovered y42(t).

20

A PREPRINT - MARCH 8, 2021

Figure 13: HMC convergence diagnostics for most signiﬁcant parameters of human motion system: (a) The Geweke
diagnostic test based on the chain of 1000 samples: we compare the mean of the ﬁrst 10% of the chain of 1000 samples
to the mean of 20 sub-samples from the last 50% of the chain. (b) and (c) Auto-correlation as function of the lag.

dynamics as follows:

(cid:21)

(cid:20) dx1
dt
dx2
dt

= Aϕ(x) =

(cid:34)a11 a12 a13 a14 a15 a16 a17 a18 a19 a110
a21 a22 a23 a24 a25 a26 a27 a28 a29 a210
a31 a32 a33 a34 a35 a36 a37 a38 a39 a310

(cid:35)

















: caseB ,

(32)

















1
x1
x2
x3
x1x2
x1x3
x2x3
x2
1
x2
2
x2
3

where the aij’s are unknown scalar coefﬁcients that will be estimated.

We consider the same training data-set and prior distributions as detailed above for case A. Uncertainty estimates
for the inferred parameters are presented in the box plots of ﬁgure 14 where the minimum, maximum, median, ﬁrst
quantile and third quantile obtained from the HMC simulations for each parameter are presented. Interestingly, based
on the estimates detailed in ﬁgure 11, it is clear that the method is robust with respect to the chosen dictionary and
does provide close estimates for the parameters aij that correspond to the same terms in the two dictionaries, while it
assigns zero values to the ones corresponding to the additional terms added to the dictionary of case B.

As observed for case A, the MAP trajectories obtained with the case B dictionary for the state x and for the PCA-
recovered variables follow the true ones, and the uncertainty is also well quantiﬁed for the time interval corresponding
to missing data as shown in ﬁgures 15 and 16. Table 4 provides the RMSE between the original 50-dimensional state
y(t) for which the experimental measurements are provided, and the PCA-recovered 50-dimensional state obtained
with the GP-NODE approach for cases A and B. The errors obtained using npODE [37] are also detailed in table
4, showing that the GP-NODE outperforms npODE not only in ﬁtting the observed data, but also on forecasting the
missing region.

21

A PREPRINT - MARCH 8, 2021

Figure 14: Human motion dynamics system, case B: Uncertainty estimation of the inferred model parameters obtained
using the GP-NODE method. Estimates for the minimum, maximum, median, ﬁrst quantile and third quantile are
provided.

Figure 15: Parameter inference of a human motion dynamics system, case B: (a) Learned dynamics versus the true
dynamics and the training data of x1(t). (b) Learned dynamics versus the true dynamics and the training data of x2(t).
(c) Learned dynamics versus the true dynamics and the training data of x3(t). The shaded region indicates the time
range of unseen data during model training.

22

A PREPRINT - MARCH 8, 2021

Figure 16: Parameter inference of a human motion dynamics system, case B: (a) Learned dynamics versus the true
dynamics of PCA-recovered y27(t). (b) Learned dynamics versus the true dynamics of PCA-recovered y34(t). (c)
Learned dynamics versus the true dynamics of PCA-recovered y37(t). (d) Learned dynamics versus the true dynamics
of PCA-recovered y39(t).
(f) Learned
dynamics versus the true dynamics of PCA-recovered y48(t). The shaded region indicates the time range of unseen
data during model training.

(e) Learned dynamics versus the true dynamics of PCA-recovered y42(t).

Model
npODE
GP-NODE, case A
GP-NODE, case B

Fitting observed data
6.89
3.14 ± 0.006
3.14 ± 0.005

Forecasting missing data
9.79
4.67 ± 0.65
4.49 ± 0.60

Table 4: Human motion dynamics system: RMSE in ﬁtting observed data and forecasting missing data.

23

A PREPRINT - MARCH 8, 2021

4 Conclusions

We put forth a novel machine learning framework (GP-NODE) for robust systems identiﬁcation under uncertainty
and imperfect time-series data. The GP-NODE framework leverages state-of-the-art differential programming tech-
niques in combination with sparsity promoting priors, Gaussian processes and gradient-enhanced sampling schemes
for scalable Bayesian inference of high-dimensional posterior distributions, which allows an inference of interpretable
and parsimonious representations of complex dynamical systems with end-to-end uncertainty quantiﬁcation. The
GP-NODE workﬂow can naturally accommodate sparse and noisy time-series data; latent variables; unknown initial
conditions; irregular time sampling for each observable variable; and observations at different time instances for the
observable variables. The effectiveness of the GP-NODE technique has been systematically investigated and compared
to state-of-the-art approaches across different problems including synthetic numerical examples and model discovery
from experimental data for human motion. These applications show how the GP-NODE probabilistic formulation is
robust against erroneous data, incomplete model parametrization, and produces reliable future forecasts with uncer-
tainty quantiﬁcation.

Although the GP-NODE framework offers great ﬂexibility to infer a distribution over plausible parsimonious repre-
sentations of a dynamical system, a number of technical improvements can be further pursued. The ﬁrst relates to
devising more effective initialization procedures for Hamiltonian Monte Carlo sampling since the initial estimations
of the system parameters may cause the system to become stiff, especially for relatively sparse observations, which can
lead to numerical instabilities during model training. Such issue can be tackled by adopting stifﬂy stable ODE solvers
[11, 12]. A second direction of generalization could be in applying the GP-NODE method to parameter identiﬁcation
of partial differential equations (PDEs) with the appropriate improvements, since discretization of PDEs generally
translates into high dimensional dynamical systems. Optimization of the discretization scheme can be investigated
within such a context [11]. Finally, one could try to generalize the GP-NODE approach to stochastic problems where
the dynamics itself are driven by a stochastic process. The approaches developed in [60] could be useful for instance.

Acknowledgements

This work received support from DOE grant DE-SC0019116, AFOSR grant FA9550-20-1-0060, and DOE-ARPA
grant 1256545. The human motion data used in this project was obtained from mocap.cs.cmu.edu. The human
motion database was created with funding from NSF EIA-0196217.

References

[1] J. M. Wang, D. J. Fleet, and A. Hertzmann. Gaussian process dynamical models for human motion.

IEEE

Transactions on Pattern Analysis and Machine Intelligence, 30(2):283–298, 2008.

[2] Avinash C Kak, Malcolm Slaney, and Ge Wang. Principles of computerized tomographic imaging. Medical

Physics, 29(1):107–107, 2002.

[3] Peter Ruoff, Melinda K Christensen, Jana Wolf, and Reinhart Heinrich. Temperature dependency and tempera-
ture compensation in a model of yeast glycolytic oscillations. Biophysical chemistry, 106(2):179–192, 2003.

[4] Martin Feinberg and Friedrich JM Horn. Dynamics of open chemical systems and the algebraic structure of the

underlying reaction network. Chemical Engineering Science, 29(3):775–787, 1974.

[5] Timothy N Palmer. A nonlinear dynamical perspective on climate prediction. Journal of Climate, 12(2):575–591,

1999.

[6] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden ﬂuid mechanics: Learning velocity and

pressure ﬁelds from ﬂow visualizations. Science, 367(6481):1026–1030, 2020.

[7] George Haller. Lagrangian coherent structures from approximate velocity data. Physics of ﬂuids, 14(6):1851–

1861, 2002.

[8] Alexis Tantet, Valerio Lucarini, Frank Lunkeit, and Henk A Dijkstra. Crisis of the chaotic attractor of a climate

model: a transfer operator approach. Nonlinearity, 31(5):2221, 2018.

[9] Alberto Bemporad and Manfred Morari. Control of systems integrating logic, dynamics, and constraints. Auto-

matica, 35(3):407–427, 1999.

[10] Fei Lu, Ming Zhong, Sui Tang, and Mauro Maggioni. Nonparametric inference of interaction laws in systems of
agents from trajectory data. Proceedings of the National Academy of Sciences, 116(29):14424–14433, 2019.

24

A PREPRINT - MARCH 8, 2021

[11] Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar, Dominic
Skinner, and Ali Ramadhan. Universal differential equations for scientiﬁc machine learning. arXiv preprint
arXiv:2001.04385, 2020.

[12] Amir Gholami, Kurt Keutzer, and George Biros. Anode: Unconditionally accurate memory-efﬁcient gradients

for neural odes. arXiv preprint arXiv:1902.10298, 2019.

[13] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations.

In Advances in neural information processing systems, pages 6571–6583, 2018.

[14] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by sparse
identiﬁcation of nonlinear dynamical systems. Proceedings of the National Academy of Sciences, 113(15):3932–
3937, 2016.

[15] Samuel H Rudy, Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Data-driven discovery of partial

differential equations. Science Advances, 3(4):e1602614, 2017.

[16] Catherine Brennan and Daniele Venturi. Data-driven closures for stochastic dynamical systems. Journal of

Computational Physics, 372:281–298, 2018.

[17] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Multistep neural networks for data-driven discovery

of nonlinear dynamical systems. arXiv preprint arXiv:1801.01236, 2018.

[18] Tong Qin, Kailiang Wu, and Dongbin Xiu. Data driven governing equations approximation using deep neural

networks. Journal of Computational Physics, 395:620–635, 2019.

[19] Tom Bertalan, Felix Dietrich, Igor Mezi´c, and Ioannis G Kevrekidis. On learning hamiltonian systems from data.

Chaos: An Interdisciplinary Journal of Nonlinear Science, 29(12):121107, 2019.

[20] Ramiro Rico-Martinez and Ioannis G Kevrekidis. Continuous time modeling of nonlinear systems: A neural
network-based approach. In IEEE International Conference on Neural Networks, pages 1522–1525. IEEE, 1993.

[21] Raul Gonz´alez-Garc´ıa, Ramiro Rico-Mart`ınez, and Ioannis G Kevrekidis. Identiﬁcation of distributed parameter

systems: A neural net based approach. Computers & chemical engineering, 22:S965–S968, 1998.

[22] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Inferring solutions of differential equations using

noisy multi-ﬁdelity data. Journal of Computational Physics, 335:736–746, 2017.

[23] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning
framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal
of Computational Physics, 378:686–707, 2019.

[24] Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physics-constrained deep
learning for high-dimensional surrogate modeling and uncertainty quantiﬁcation without labeled data. Journal
of Computational Physics, 394:56–81, 2019.

[25] Yibo Yang and Paris Perdikaris. Adversarial uncertainty quantiﬁcation in physics-informed neural networks.

Journal of Computational Physics, 394:136–152, 2019.

[26] Liu Yang, Dongkun Zhang, and George Em Karniadakis. Physics-informed generative adversarial networks for

stochastic differential equations. SIAM Journal on Scientiﬁc Computing, 42(1):A292–A317, 2020.

[27] Qian Wang, Jan S Hesthaven, and Deep Ray. Non-intrusive reduced order modeling of unsteady ﬂows using
artiﬁcial neural networks with application to a combustion problem. Journal of computational physics, 384:289–
307, 2019.

[28] Maziar Raissi and George Em Karniadakis. Hidden physics models: Machine learning of nonlinear partial

differential equations. Journal of Computational Physics, 357:125–141, 2018.

[29] Alexandre M Tartakovsky, Carlos Ortiz Marrero, Paris Perdikaris, Guzel D Tartakovsky, and David Barajas-
Solano. Learning parameters and constitutive relationships with physics informed deep neural networks. arXiv
preprint arXiv:1808.03398, 2018.

[30] Georgios Kissas, Yibo Yang, Eileen Hwuang, Walter R Witschey, John A Detre, and Paris Perdikaris. Ma-
chine learning in cardiovascular ﬂows modeling: Predicting arterial blood pressure from non-invasive 4d ﬂow
mri data using physics-informed neural networks. Computer Methods in Applied Mechanics and Engineering,
358:112623, 2020.

[31] Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse

problems in nano-optics and metamaterials. arXiv preprint arXiv:1912.01085, 2019.

25

A PREPRINT - MARCH 8, 2021

[32] Mohamed Aziz Bhouri, Francisco Sahli Costabal, Hanwen Wang, Kevin Linka, Mathias Peirlinck, Ellen Kuhl,
and Paris Perdikaris. Covid-19 dynamics across the us: A deep learning study of human mobility and social
behavior. medRxiv, 2020.

[33] Yibo Yang, Mohamed Aziz Bhouri, and Paris Perdikaris. Bayesian differential programming for robust systems

identiﬁcation under uncertainty. Proc. R. Soc. A., 476, 2020.

[34] Raj Dandekar, Vaibhav Dixit, Mohamed Tarek, Aslan Garcia-Valadez, and Chris Rackauckas. Bayesian neural

ordinary differential equations. arXiv preprint arXiv:2012.07244, 2020.

[35] Gianluigi Pillonetto, Francesco Dinuzzo, Tianshi Chen, Giuseppe De Nicolao, and Lennart Ljung. Kernel meth-
ods in system identiﬁcation, machine learning and function estimation: A survey. Automatica, 50(3):657–682,
2014.

[36] Ben Calderhead, Mark Girolami, and Neil Lawrence. Accelerating bayesian inference over nonlinear differential
equations with gaussian processes. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in
Neural Information Processing Systems, volume 21, pages 217–224. Curran Associates, Inc., 2009.

[37] Markus Heinonen, Cagatay Yildiz, Henrik Mannerstr¨om, Jukka Intosalmi, and Harri L¨ahdesm¨aki. Learning
unknown ODE models with Gaussian processes. In Jennifer Dy and Andreas Krause, editors, Proceedings of the
35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,
pages 1959–1968, Stockholmsm ˜A Cssan, Stockholm Sweden, 10–15 Jul 2018. PMLR.

[38] Philippe Wenk, Alkis Gotovos, Stefan Bauer, Nico S. Gorbach, Andreas Krause, and Joachim M. Buhmann.
Fast gaussian process based gradient matching for parameter identiﬁcation in systems of nonlinear odes.
In
Kamalika Chaudhuri and Masashi Sugiyama, editors, Proceedings of Machine Learning Research, volume 89 of
Proceedings of Machine Learning Research, pages 1351–1360. PMLR, 16–18 Apr 2019.

[39] Boumediene Hamzi and Houman Owhadi. Learning dynamical systems from data: A simple cross-validation

perspective, part i: Parametric kernel ﬂows. Physica D: Nonlinear Phenomena, page 132817, 2021.

[40] David J. C. MacKay. Bayesian interpolation. Neural Computation, 4(3):415–447, 1992.
[41] Carlos M. Carvalho, Nicholas G. Polson, and James G. Scott. Handling sparsity via the horseshoe. In David
van Dyk and Max Welling, editors, Proceedings of the Twelth International Conference on Artiﬁcial Intelligence
and Statistics, volume 5 of Proceedings of Machine Learning Research, pages 73–80, Hilton Clearwater Beach
Resort, Clearwater Beach, Florida USA, 16–18 Apr 2009. PMLR.

[42] Juho Piironen and Aki Vehtari. Sparsity information and regularization in the horseshoe and other shrinkage

priors. Electron. J. Statist., 11(2):5018–5051, 2017.

[43] Juho Piironen and Aki Vehtari. On the Hyperprior Choice for the Global Shrinkage Parameter in the Horseshoe
Prior. In Aarti Singh and Jerry Zhu, editors, Proceedings of the 20th International Conference on Artiﬁcial Intelli-
gence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 905–913, Fort Lauderdale,
FL, USA, 20–22 Apr 2017. PMLR.

[44] Matthew D Hoffman and Andrew Gelman. The no-u-turn sampler: adaptively setting path lengths in hamiltonian

monte carlo. Journal of Machine Learning Research, 15(1):1593–1623, 2014.

[45] Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo, 2(11):2,

2011.

[46] CE. Rasmussen and CKI. Williams. Gaussian Processes for Machine Learning. Adaptive Computation and

Machine Learning. MIT Press, Cambridge, MA, USA, January 2006.

[47] Lev Semenovich Pontryagin. Mathematical theory of optimal processes. Routledge, 2018.
[48] Alireza Yazdani, Lu Lu, Maziar Raissi, and George Em Karniadakis. Systems biology informed deep learning

for inferring parameters and hidden dynamics. PLOS Computational Biology, 16(11):1–19, 11 2020.

[49] Kathleen Champion, Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Data-driven discovery of coordinates
and governing equations. Proceedings of the National Academy of Sciences, 116(45):22445–22451, 2019.
[50] J Schnakenberg. Simple chemical reaction systems with limit cycle behaviour. Journal of theoretical biology,

81(3):389–400, 1979.

[51] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural networks.
In Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, pages 249–256,
2010.

[52] Jack Wang, Aaron Hertzmann, and David J Fleet. Gaussian process dynamical models. In Y. Weiss, B. Sch¨olkopf,
and J. Platt, editors, Advances in Neural Information Processing Systems, volume 18, pages 1441–1448. MIT
Press, 2006.

26

A PREPRINT - MARCH 8, 2021

[53] Andreas Damianou, Michalis Titsias, and Neil Lawrence. Variational gaussian process dynamical systems. In
J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information
Processing Systems, volume 24, pages 2510–2518. Curran Associates, Inc., 2011.

[54] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George
Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transfor-
mations of Python+NumPy programs, 2018.

[55] Du Phan, Neeraj Pradhan, and Martin Jankowiak. Composable effects for ﬂexible and accelerated probabilistic

programming in numpyro. arXiv preprint arXiv:1912.11554, 2019.

[56] Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Ro-
hit Singh, Paul Szerlip, Paul Horsfall, and Noah D. Goodman. Pyro: Deep Universal Probabilistic Programming.
arXiv preprint arXiv:1810.09538, 2018.

[57] Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Deep learning for universal linear embeddings of nonlinear

dynamics. Nature communications, 9(1):4950, 2018.

[58] John Geweke. Bayesian treatment of the independent student-t linear model. Journal of applied econometrics,

8(S1):S19–S40, 1993.

[59] Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. Bayesian data

analysis. CRC press, 2013.

[60] Xuechen Li, Ting-Kam Leonard Wong, Ricky TQ Chen, and David Duvenaud. Scalable gradients for stochastic

differential equations. arXiv preprint arXiv:2001.01328, 2020.

27

