Sampling-based sublinear low-rank matrix arithmetic framework for
dequantizing quantum machine learning

Nai-Hui Chia∗
Han-Hsuan Lin∗

Andr´as Gily´en†
Ewin Tang§

Tongyang Li‡
Chunhao Wang∗

Abstract

We present an algorithmic framework for quantum-inspired classical algorithms on close-to-
low-rank matrices, generalizing the series of results started by Tang’s breakthrough quantum-
inspired algorithm for recommendation systems [STOC’19]. Motivated by quantum linear algebra
algorithms and the quantum singular value transformation (SVT) framework of Gily´en, Su, Low,
and Wiebe [STOC’19], we develop classical algorithms for SVT that run in time independent
of input dimension, under suitable quantum-inspired sampling assumptions. Our results give
compelling evidence that in the corresponding QRAM data structure input model, quantum SVT
does not yield exponential quantum speedups. Since the quantum SVT framework generalizes
essentially all known techniques for quantum linear algebra, our results, combined with sampling
lemmas from previous work, suﬃce to generalize all prior results about dequantizing quantum
machine learning algorithms. In particular, our classical SVT framework recovers and often
improves the dequantization results on recommendation systems, principal component analysis,
supervised clustering, support vector machines, low-rank regression, and semideﬁnite program
solving. We also give additional dequantization results on low-rank Hamiltonian simulation
and discriminant analysis. Our improvements come from identifying the key feature of the
quantum-inspired input model that is at the core of all prior quantum-inspired results: (cid:96)2-norm
sampling can approximate matrix products in time independent of their dimension. We reduce
all our main results to this fact, making our exposition concise, self-contained, and intuitive.

2
2
0
2

n
u
J

0
1

]
S
D
.
s
c
[

3
v
1
5
1
6
0
.
0
1
9
1
:
v
i
X
r
a

∗Department of Computer Science, University of Texas at Austin. Research supported by Scott Aaronson’s
Vannevar Bush Faculty Fellowship from the US Department of Defense. Email: {nai,linhh,chunhao}@cs.utexas.edu
†Alfr´ed R´enyi Institute of Mathematics. Formerly at the Institute for Quantum Information and Matter, California
Institute of Technology. Funding provided by Samsung Electronics Co., Ltd., for the project “The Computational
Power of Sampling on Quantum Computers”, and by the Institute for Quantum Information and Matter, an NSF
Physics Frontiers Center (NSF Grant PHY-1733907), as well as by the EU’s Horizon 2020 Marie Sk(cid:32)lodowska-Curie
program 891889-QuantOrder. Email: gilyen@renyi.hu

‡Department of Computer Science, Institute for Advanced Computer Studies, and Joint Center for Quantum
Information and Computer Science, University of Maryland. Research supported by IBM PhD Fellowship, QISE-NET
Triplet Award (NSF DMR-1747426), and the U.S. Department of Energy, Oﬃce of Science, Oﬃce of Advanced
Scientiﬁc Computing Research, Quantum Algorithms Teams program. Email: tongyang@cs.umd.edu

§University of Washington. This material is based upon work supported by the National Science Foundation

Graduate Research Fellowship Program under Grant No. DGE-1762114. Email: ewint@cs.washington.edu

1

 
 
 
 
 
 
Contents

1 Introduction

3
3
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.3 Technical overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.4 Applications: dequantizing QML & more
. . . . . . . . . . . . . . . . . . . . . . . .
1.5 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.6 Open questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.7 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

2 Preliminaries

14
2.1 Linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

3 Sampling and query access oracles

15

4 Matrix sketches

21
4.1 Approximation results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

5 Singular value transformation

27

6 Applying the framework to dequantizing QML algorithms

30
6.1 Dequantizing QSVT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
6.2 Recommendation systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
6.3 Supervised clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
6.4 Principal component analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
6.5 Matrix inversion and principal component regression . . . . . . . . . . . . . . . . . . 41
6.6 Support vector machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
6.7 Hamiltonian simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
6.8 Semideﬁnite program solving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
6.9 Discriminant analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

7 More singular value transformation

A Proof sketch for Remark 6.4

B Deferred proofs

56

68

69

2

1

Introduction

1.1 Motivation

Quantum machine learning (QML) is a ﬁeld of study with a rapidly growing number of proposals for
how quantum computers could signiﬁcantly speed up machine learning tasks [DW20, CHI+18]. If any
of these proposals yield substantial practical speedups, it could be the killer application motivating
the development of scalable quantum computers [Pre18]. At ﬁrst glance, many applications of QML
seem to admit exponential speedups. However, these exponential speedups are less likely to manifest
in practice compared to, say, Shor’s algorithm for factoring [Sho97], because unlike their classical
counterparts, QML algorithms must make strong input assumptions and learn relatively little from
their output [Aar15]. These caveats arise because both loading input data into a quantum computer
and extracting amplitude data from an output quantum state are hard in their most generic forms.
A recent line of research analyzes the speedups of QML algorithms by developing classical
counterparts that carefully exploit these restrictive input and output assumptions. This began
with a breakthrough 2018 paper by Tang [Tan19] showing that the quantum recommendation
systems algorithm [KP17], previously believed to be one of the strongest candidates for a practical
exponential speedup in QML, does not give an exponential speedup. Speciﬁcally, Tang describes a
“dequantized” algorithm that solves the same problem as the quantum algorithm and only suﬀers
a polynomial slowdown. Tang’s algorithm crucially exploits the input data structure assumed by
the quantum algorithm, which is used for eﬃciently preparing states. Subsequent work relies on
similar techniques to dequantize a wide range of QML algorithms, including those for principal
component analysis and supervised clustering [Tan21], low-rank linear system solving [CGL+20],
low-rank semideﬁnite program solving [CLLW20], support vector machines [DBH21], nonnegative
matrix factorization [CLS+19], and minimal conical hull [DHLT20]. These results show that the
advertised exponential speedups of many QML algorithms disappear when compared to classical
algorithms with input assumptions analogous to the state preparation assumptions of the quantum
algorithms, drastically changing our understanding of the landscape of potential QML algorithm
speedups.

A recent line of work in quantum algorithms has worked to unify many quantum algorithms
ranging from quantum walks to QML, under a quantum linear algebra framework called quantum
singular value transformation (QSVT) [LC17, CGJ19, GSLW19]. Since this framework captures
essentially all known linear algebraic QML techniques [MRTC21], including all prior dequantized
QML algorithms (up to minor technical details), a natural question is whether this framework can
be dequantized. One cannot hope to dequantize all of QSVT, because with sparse block-encodings
of input data, QSVT can simulate Harrow, Hassidim, and Lloyd’s pioneering poly-logarithmic time
algorithm (HHL) for the BQP-complete problem of sampling from the solution of a sparse system
of linear equations [HHL09]. However, one could hope to dequantize QSVT provided that the input
data comes in the state preparation data structure used commonly for quantum linear algebra. This
data structure allows for eﬃcient QML when the input is close-to-low-rank, but as dequantized
algorithms show, it also gives signiﬁcant power to classical algorithms. Prior work [Tan21, CGL+20]
has made similar speculations that these techniques could feasibly dequantize wide swathes of
quantum linear algebra. In this work, we give evidence for these hopes by presenting a classical
analogue of the QSVT framework and applying it to dequantize QML algorithms.

1.2 Results

We describe a simple framework for quantum-inspired classical algorithms with wide applicability,
grasping the capabilities and limitations of these techniques. We use this framework to dequantize

3

many quantum linear algebra algorithms. We also prove QSVT-like extensibility properties of our
framework, giving evidence that it can dequantize any QSVT algorithms in the QRAM input model.

Input model: Oversampling and query access. Our framework assumes a speciﬁc input
model called oversampling and query access, which can be thought of as a classical analogue to
quantum state preparation assumptions, i.e., the ability to prepare a quantum state |v(cid:105) proportional
to some input vector v. Our conceptual contribution is to deﬁne this generalization of sampling and
query access, because it has better closure properties.

We have sampling and query access to a vector v ∈ Cn, denoted SQ(v), if we can eﬃciently make
the following kinds of queries (Deﬁnition 3.2): (1) given an index i ∈ [n], output the corresponding
entry v(i); (2) sample an index j ∈ [n] with probability |v(j)|2/(cid:107)v(cid:107)2; and (3) output the vector’s
(cid:96)2-norm (cid:107)v(cid:107). We have sampling and query access to a matrix A ∈ Cm×n, denoted SQ(A), if
we have SQ(A(i, ·)) for all rows of A, A(i, ·), and also SQ(a) for a the vector of row norms (i.e.,
a(i) := (cid:107)A(i, ·)(cid:107)). We have φ-oversampling and query access to a vector v, denoted SQφ(v), if (1)
we can query for entries of v and (2) we have sampling and query access to an “entry-wise upper
bound” vector ˜v satisfying (cid:107)˜v(cid:107)2 = φ(cid:107)v(cid:107)2 and |˜v(i)| ≥ |v(i)| for all indices i; the deﬁnition for a
matrix is analogous (Deﬁnition 3.4).

The parameter φ should be seen as a form of overhead that comes out in the runtime of
algorithms: through rejection sampling, SQφ(v) can do approximate versions of all the queries of
SQ(v) with a factor φ of overhead (Lemma 3.5). In this paper, we most often think of φ as being
independent of input size.

To motivate this deﬁnition, we make the following observations about this input model. First,
as far as we know, if input data is given classically,1 classical algorithms in the sampling and query
model can be run whenever the corresponding algorithms in the quantum model can (Remark 3.11).
For example, if input is loaded in the QRAM data structure, as commonly assumed in QML in
order to satisfy state preparation assumptions [Pra14, CHI+18], then we have log-time sampling and
query access to it. So, a fast classical algorithm for a problem in this classical model implies lack of
quantum speedup for the problem, at least in the usual settings explored in the QML literature. In
particular, a polynomial-time classical algorithm in this model implies lack of exponential quantum
speedup. Second, oversampling and query access has many similarities to the notion of quantum
block-encodings in quantum singular value transformation [GSLW19]. The commonly used data-
structures that enable oversampling and query access to a matrix A also enable implementing an
eﬃcient quantum circuit whose unitary is a block-encoding of A. Further, in both input models one
can perform eﬃcient matrix arithmetic.

Matrix arithmetic. The thrust of our main results is to demonstrate that oversampling and
query access is approximately closed under arithmetic operations. We argue that the essential power
of quantum-inspired algorithms lies in their ability to leverage sampling and query access of the
input matrices to provide oversampling and query access to complex arithmetic expressions as output
(possibly with some approximation error), without paying the (at least) linear time necessary to
compute such expressions in conventional ways. While the proven closure properties are important
from a complexity theoretic point of view, some of them don’t explicitly come into play in the
demonstrated applications of our framework for dequantizing QML, since they often come with
undesirable polynomial overhead. This is in contrast with quantum block-encodings, which generally
compose with minimal overhead.

1This assumption is important. When input data is quantum (say, it is coming directly from a quantum system), a

classical computer has little hope of performing linear algebra on it eﬃciently, see for example [ACQ22, HKP21].

4

We now list the closure properties that we show, along with the corresponding closure properties
proven for block-encodings in [GSLW19]. For all of these, the query time for access to the output
is just polynomial in the query times for access to the input, so in particular, these procedures
run in time independent of input dimension. More speciﬁcally, we will compare what we call
(sub)normalization “overhead” between the two, which is the value φ in the classical setting and
what [GSLW19] denotes as α in the quantum setting. The two quantities are analogous, and roughly
correspond to overheads in rejection sampling and post-selection, inducing a multiplicative factor in
sampling times for both.

(cid:80)τ

• Given access to a constant number of vectors v1, . . . , vτ , we have access to linear combinations
t=1 λtvt, and analogously with linear combinations of matrices (Lemmas 3.6 and 3.9). This
is a classical analogue to the “linear combinations of unitaries” technique for block-encodings
[GSLW19, Lemma 52]. In the quantum setting, there is less overhead,2 allowing for eﬃcient
block encodings for linear combinations of arbitrarily many matrices in certain settings.

• Given access to two matrices A, B with Frobenius norm at most one, we have access to a
matrix Z ε-close to the product A†B in Frobenius norm (Lemma 4.6 and Remark 4.7). In the
quantum setting, closure of block-encodings under products is almost immediate [GSLW19,
Lemma 53] and is not approximate. In both cases the individual input overheads of A and
B are multiplied. With the same overheads one can also form Kronecker products A ⊗ B
exactly—this is immediate both in the classical and quantum case [CVB20]. In particular,
given access to two vectors u and v, we have access to their outer product uv† (Lemma 3.8).

• Given access to a matrix A with Frobenius norm at most one and a Lipschitz function f ,
we have access to a matrix Z ε-close to f (A†A) in Frobenius norm (Theorem 5.1)3. In the
quantum setting, block-encodings are closed under even and odd polynomial singular value
transformations [GSLW19, Lemmas 8, 10] without approximation, provided the polynomial is
low-degree and bounded. This block-encoding closure property can be viewed as a corollary of
the above two properties, but can also be achieved directly with some more eﬃcient technical
machinery.
An even polynomial singular value transformation of A is precisely f (A†A) for f a low-degree
polynomial (which means f is Lipschitz), and odd polynomials can be decomposed into a
product of an even polynomial with A, so our closure property is as strong as the quantum
one. The full details are derived in Section 6.1.

To summarize, every arithmetic operation of matrices with block-encodings in [GSLW19] (with the
possible exception of long linear combinations) can be mimicked by matrices with oversampling
and query access, up to Frobenius norm error, provided that an input matrix in a block-encoding
corresponds to having4 SQ(A) and SQ(A†). The “linear combinations of vectors” and “outer
products” classical closure properties have been used in prior work [Tan19, CGL+20]. However,
without our new deﬁnition of oversampling and query access, it was not clear that these algorithms
could be chained indeﬁnitely as we show with these closure properties.

(cid:13)
2In fact, already
(cid:13) which can be seen using that the root mean-square is at least the average.
3For a Hermitian matrix H and a function f : R (cid:55)→ C, f (H) denotes applying f to the eigenvalues of H. That is,

φ ≥ α/(cid:13)

t=1 λtvt

(cid:13)(cid:80)τ

√

f (H) := (cid:80)n

i=1 f (λi)viv†

i , for λi and vi the eigenvalues and eigenvectors of H.

4We take some care here to distinguish whether we have oversampling and query access to A or A†. We don’t need
to: we show that having either one of them implies having the other, up to approximation (Remark 6.4). However, the
accesses assumed in our closure properties are in some sense the most natural choices and require the least overhead.

5

Implications for quantum singular value transformation. Our results give compelling
evidence that there is indeed no exponential speedup for QRAM-based QSVT, and show that
oversampling and query access can be thought of as a classical analogue to block-encodings in
the bounded Frobenius norm regime. Nevertheless, we do not rule out the possibility for large
polynomial speedups, as the classical runtimes tend to have impractically large polynomial exponents.
To elaborate more on this connection, we now recall the QSVT framework in more detail.

The QSVT framework of Gily´en, Su, Low, and Wiebe [GSLW19] assumes that the input matrix
A is given by a block-encoding, which is a quantum circuit implementing a unitary transformation
whose top-left block contains (up to scaling) A itself [LC17]. Given a block-encoding of A, one
can apply it to a quantum state or form block-encodings of other expressions using the closure
properties mentioned above. One can get a block-encoding of an input matrix A through various
methods. If A is s-sparse with eﬃciently computable elements and (cid:107)A(cid:107) ≤ 1, then one can directly
get a block-encoding of A/s [GSLW19, Lemma 48]. If A is in the QRAM data structure (used for
eﬃcient state preparation for QML algorithms [Pra14]), one can directly get a block-encoding of
A/(cid:107)A(cid:107)F [GSLW19, Lemma 50]. We will use the term QRAM-based QSVT to refer to the family of
quantum algorithms possible in the QSVT framework when all input matrices & vectors are given
in the QRAM data structure.

The normalization in QRAM-based QSVT means that it has an implicit dependence on the
Frobenius norm (cid:107)A(cid:107)F. Since (cid:107)A(cid:107)F is also the key parameter in the complexity of our corresponding
classical algorithms, this suggests that QRAM-based QSVT does not give inherent exponential quan-
tum speedups (though, if input preparation/output analysis protocols have no classical analogues,
they can act as a subroutine in an algorithm that does give an exponential quantum speedup). Our
closure results conﬁrm this: if input matrices and vectors are given in QRAM data structure, then
on one (quantum) hand we can construct block-encodings of these matrices normalized to have
Frobenius norm one and on the other (classical) hand we have sampling and query access to the
input. The conclusion is that, up to some controllable approximation error, an algorithm using the
block-encoding framework has a classical analogue in the oversampling and query access model. The
classical algorithm’s runtime is only polynomially slower than the corresponding quantum algorithm,
except in the ε parameter.5 One can argue similarly that there should be no exponential speedup for
QSVT for block-encodings derived from (puriﬁcations of) density operators [GSLW19, Lemma 45]
that come from some well-structured classical data (see Section 6.1). This stands in contrast to,
for example, block-encodings that come from sparsity assumptions [GSLW19, Lemma 48], where
the matrix in the block-encoding can have Frobenius norm as large as
sn (where we take A to be
n × n), and so the classical techniques cannot be applied without incurring dependence on n in the
runtime.

√

1.3 Technical overview

We now illustrate the ﬂavor of the algorithmic ideas underlying our main results, by showing why
the “oversampling” input model is closed under approximate matrix products. Suppose we are given
sampling and query access to two matrices A ∈ Cm×n and B ∈ Cm×p, and desire (over)sampling
and query access to A†B. A†B is a sum of outer products of rows of A with rows of B (that is,
A†B = (cid:80)m
i=1 A(i, ·)†B(i, ·)), so a natural idea is to use the outer product closure property to get
access to each outer product individually, and then use the linear combination closure property to
get access to their sum, which is A†B as desired. However, there are m terms in the sum, which is

5The QML algorithms we discuss generally only incur polylog( 1

ε ) terms, but need to eventually pay poly(1/ε) to
extract information from output quantum states. So, we believe this exponential speedup is artiﬁcial. See the open
questions section for more discussion of this error parameter.

6

too large: we can’t even compute entries of A†B in time independent of m. So, we use sampling to
approximate this sum of m terms by a linear combination over far fewer terms, allowing us to get
access to Z for Z ≈ A†B. This type of matrix product approximation is well-known in the classical
literature [DKM06]. Given SQ(A), we can pull samples i1, . . . , is according to the row norms of A, a
p(ik) A(ik, ·)†B(ik, ·).
distribution we will denote p (so p(i) = (cid:107)A(i, ·)(cid:107)2/(cid:107)A(cid:107)2
Z is an unbiased estimator of A†B: E[Z] = 1
(cid:96)=1 A((cid:96), ·)†B((cid:96), ·) =
s
A†B. Further, the variance of this estimator is small. In the following computation, we consider
s = 1, because the variance for general s decreases as 1/s.

F). Consider Z := 1
s
(cid:96)=1 p((cid:96)) A((cid:96),·)†B((cid:96),·)
(cid:80)m

k=1
= (cid:80)m

(cid:80)s

(cid:80)s

k=1

p((cid:96))

1

E[(cid:107)A†B − Z(cid:107)2

F] ≤

(cid:88)

i,j

E[|Z(i, j)|2] =

(cid:88)

(cid:88)

p((cid:96))

1
p((cid:96))2 |A((cid:96), i)|2|B((cid:96), j)|2

(cid:96)

i,j
1
p((cid:96))

(cid:88)

=

(cid:96)

(cid:107)A((cid:96), ·)(cid:107)2(cid:107)B((cid:96), ·)(cid:107)2 =

(cid:88)

(cid:96)

(cid:107)A(cid:107)2

F(cid:107)B((cid:96), ·)(cid:107)2 = (cid:107)A(cid:107)2

F(cid:107)B(cid:107)2
F.

(cid:1) to get that (cid:107)Z − A†B(cid:107)F < ε(cid:107)A(cid:107)F(cid:107)B(cid:107)F with
By Chebyshev’s inequality, we can choose s = O(cid:0) 1
ε2
probability 0.99. Since Z is a linear combination of s outer products, this gives us oversampling
and query access to Z as desired. In our applications we would keep Z as an outer product A(cid:48)†B(cid:48)
for convenience. We have just sketched the proof of our key lemma: an approximate matrix product
protocol.

Key lemma [DKM06] (informal version of Lemma 4.6). Suppose we are given SQ(X) ∈ Cm×n and
SQ(Y ) ∈ Cm×p. Then we can ﬁnd normalized submatrices of X and Y , X (cid:48) ∈ Cs×n and Y (cid:48) ∈ Cs×p,
in O(s) time for s = Θ( 1

ε2 log 1
δ ), such that
(cid:104)

Pr

(cid:107)X (cid:48)†Y (cid:48) − X †Y (cid:107)F ≤ ε(cid:107)X(cid:107)F(cid:107)Y (cid:107)F

(cid:105)

> 1 − δ.

We subsequently have O(s)-time SQ(X (cid:48)), SQ(X (cid:48)†), SQ(Y (cid:48)), SQ(Y (cid:48)†).

Prior quantum-inspired algorithms [Tan19, Tan21, CLW18, CLLW20] indirectly used this lemma
by using [FKV04], which ﬁnds a low-rank approximation to the input matrix in the form of an
approximate low-rank SVD and relies heavily on this lemma in the analysis.

One of our main results, mentioned earlier as our singular value transformation closure property,
is that, given SQ(A) ∈ Cm×n, in time independent of m and n, we can access an approximation of
f (A†A) for a Lipschitz-function f that, without loss of generality, satisﬁes f (0) = 0 (Theorem 5.1).
One could use [FKV04] to give a classical algorithm for SVT, but a more eﬃcient approach is to
directly apply the key lemma twice to get an approximate decomposition of f (A†A):

f (A†A) ≈ f (R†R)

= R† ¯f (RR†)R
≈ R† ¯f (CC†)R

by key lemma, with R ∈ Cr×n normalized rows of A
by computation, where ¯f (x) := f (x)/x)
by key lemma, with C ∈ Cr×c normalized columns of R

We call R† ¯f (CC†)R an RUR decomposition because R ∈ Cr×n is a subset of rows of the input
matrix and U is a matrix with size independent of input dimension (R corresponds to the ‘R’ of
the RUR decomposition, and ¯f (CC†) ∈ Cr×r corresponds to the ‘U’). In other words, an RUR
decomposition expresses a desired matrix as a linear combination of r2 outer products of rows of
the input matrix ((cid:80)
i,j[ ¯f (CC†)](i, j)R(i, ·)†R(j, ·), for example).6 We want our output in the form
6This is the relevant variant of the notion of a CUR decomposition from the randomized numerical linear algebra

and theoretical computer science communities [DMM08].

7

of an RUR decomposition, since we can describe such a decomposition implicitly just as a list of
row indices and some additional coeﬃcients, which avoids picking up a dependence on m or n in
our runtimes. Further, having SQ(A) gives us SQφ(R†U R) via closure properties, enabling eﬃcient
access to matrix-vector expressions like R†U Rb.

More general results follow as corollaries of our main result on even SVT (Theorems 7.1 and 7.2).
However, using only our main theorem about even SVT, we can directly recover most existing
quantum-inspired machine learning algorithms without using these general results, yielding faster
dequantization for QML algorithms. We now outline our results recovering such applications.

1.4 Applications: dequantizing QML & more

We use the results above to recover existing quantum-inspired algorithms for recommendation
systems [Tan19], principal component analysis [Tan21], supervised clustering [Tan21], support
vector machines [DBH21], low-rank matrix inversion [CGL+20], and semideﬁnite program solving
[CLLW20]. We also propose new quantum-inspired algorithms for low-rank Hamiltonian simulation
and discriminant analysis (dequantizing the quantum algorithm of Cong & Duan [CD16]).

Fig. 1 has a summary of our results, along with a comparison of runtimes to the corresponding
quantum algorithms and prior quantum-inspired work, where it exists. All our results match or
improve on prior dequantized algorithms apart from that for matrix inversion, where prior work
gives an incomparable runtime that only holds for strictly low-rank matrices of rank k. Our results
for matrix inversion and semideﬁnite program solving solve the problem in greater generality than
prior work, without the restriction that the input matrices are strictly rank-k.7

We do not claim any meaningful breakthroughs for these problems in the classical literature: the
problems that these QML algorithms solve diﬀer substantially from their usual classical counterparts.
For example, the quantum recommendation systems algorithm of Kerenidis and Prakash [KP17]
performs sampling from a low-rank approximation of the input instead of low-rank matrix completion,
which is the typical formalization of the recommendation systems problem [Tan19]. Evaluating
these quantum algorithms’ justiﬁcations for their versions of problems is outside the scope of
this work:
instead, we argue that these algorithms would likely not give exponential speedups
when implemented, regardless of whether such implementations would be useful. The goal of our
framework is to demonstrate what can be done classically and establish a classical frontier for
quantum algorithms to push past.

The proofs for these dequantization results follow the same general structure: consider the
quantum algorithm and formulate the problem that this algorithm solves, and in particular, the linear
algebra expression that the quantum algorithm computes. From there, repeatedly use the SVT result
and key lemma to approximate this expression by something like an RUR decomposition. Finally,
use closure properties to gain oversampling and query access to that output decomposition. This
procedure is relatively straightfoward and ﬂexible. Also, unlike previous work [CGL+20, CLLW20],
our results need not assume that the input is strictly low-rank. Instead, following [Tan19, GSLW19],
our algorithms work on close-to-low-rank matrices by doing SVTs that smoothly threshold to
eﬀectively only operate on large-enough singular values.

1.5 Related work

Quantum-inspired algorithms. Our approach and analysis is much simpler than that of Frieze,
Kannan, and Vempala [FKV04], while it also gives improved results in our applications, and has
several other advantages. For example, the reduction to [FKV04] ﬁrst given by Tang to get an

7For semideﬁnite program solving, (cid:107)A(·)(cid:107)F ≤

√

k, which makes the runtimes comparable.

8

d22(cid:107)A(cid:107)6
F(cid:107)b(cid:107)6
ε6(cid:107)p(QV)(A)b(cid:107)6

(cid:107)A(cid:107)6

F(cid:107)A(cid:107)10
σ16ε6

(cid:107)M (cid:107)4
F(cid:107)w(cid:107)4
ε2

(cid:107)X(cid:107)6
F
(cid:107)X(cid:107)2λ2
kη6ε6
F(cid:107)A(cid:107)22
σ28ε6

(cid:107)A(cid:107)6

quantum algorithm

prior work

this work

simple QSVT
[GSLW19], §6.1

d(cid:107)A(cid:107)F(cid:107)b(cid:107)
(cid:107)p(QV)(A)b(cid:107)

recommendation systems
[CGJ19], [Tan19], §6.2

(cid:107)A(cid:107)F
σ

(cid:107)A(cid:107)24
F
σ24ε12

supervised clustering
[LMR13], [Tan21], §6.3

(cid:107)M (cid:107)2
F(cid:107)w(cid:107)2
ε

(♣)

(cid:107)M (cid:107)4
F(cid:107)w(cid:107)4
ε2

principal component analysis
[CGJ19], [Tan21], §6.4

(cid:107)X(cid:107)F(cid:107)X(cid:107)
λkε

matrix inversion
[GSLW19], [GLT18], §6.5

support vector machines
[RML14], [DBH21], §6.6

Hamiltonian simulation
[GSLW19], §6.7

(cid:107)A(cid:107)F
σ

(♦)

1
λ3ε3

(cid:107)H(cid:107)F

(cid:107)X(cid:107)36
F
(cid:107)X(cid:107)12λ12
k η6ε12
F(cid:107)A(cid:107)16k6
σ22ε6

(cid:107)A(cid:107)6

(♦)

poly

(cid:17)(♣)

(cid:16) 1
λ

,

1
ε

1
λ28ε6

F(cid:107)H(cid:107)16
(cid:107)H(cid:107)6
max(1, σ16)ε6

semideﬁnite program solving
[vAG19], [CLLW20], §6.8

(cid:107)A(·)(cid:107)7
F
ε7.5 +

√

m(cid:107)A(·)(cid:107)2
F
ε4

(♦)

mk57
ε92

(cid:107)A(·)(cid:107)22
F

ε46 +

m(cid:107)A(·)(cid:107)14
F
ε28

discriminant analysis
[CD16], §6.9

(cid:107)B(cid:107)7
F
ε3σ7 +

(cid:107)W (cid:107)7
F
ε3σ7

(♦)

(cid:107)B(cid:107)6

F(cid:107)B(cid:107)4
ε6σ10 +

(cid:107)W (cid:107)6

F(cid:107)W (cid:107)10
ε6σ16

Figure 1: The time complexity for our algorithms, the quantum algorithms they are based on,
and prior quantum-inspired algorithms (where they exist). We assume our sampling and query
accesses to the input takes O(1) time. There are data structures that can support such queries
(Remark 3.11), and if the input is in QRAM, the runtime only increases by at most a factor of log
of input size.

We list the runtime of the algorithm, not including the time it takes to access the output (denoted
with (cid:102)sq). The runtimes as listed ignore polylog terms, particularly those in error parameters (ε and
δ) and dimension parameters (m and n). The matrices and vectors referenced in these runtimes
are always the input, σ refers to a singular value threshold of the input matrices, λ refers to an
eigenvalue threshold (which can be thought of here as σ2), and η > ε is a (dimensionless) gap
parameter.

(♣) indicates that the error analyses of the corresponding results are incomplete; we list the runtime
they achieve for completeness.

(♦) indicates that the corresponding results only hold in the restricted setting where the input
matrices are strictly rank k. For the quantum algorithms with this tag, they allow for general
matrices, but only have an informal error analysis arguing that singular values outside the range
considered don’t aﬀect the ﬁnal result.

9

SVT-based low-rank approximation bound from the standard notion of low-rank approximation
[Tan19, Theorem 4.7] induces a quadratic loss in precision, which appears to be only an artifact of
the analysis. Also, [FKV04] gives Frobenius norm error bounds, though for applications we often
only need spectral norm bounds; our main theorem can get improved runtimes by taking advantage
of the weaker spectral norm bounds. Finally, we take a reduced number of rows compared to
columns, whereas [FKV04] approximates the input by taking the same number of rows and columns.

Randomized numerical linear algebra. All of the results presented here are more or less
randomized linear algebra algorithms [Mah11, Woo14]. The kind of sampling we get from sampling
and query access is called importance sampling or length-square sampling in that body of work: see the
survey by Kannan and Vempala [KV17] for more on importance sampling. Importance sampling, and
speciﬁcally, its approximate matrix product property, is the core primitive of this work. In addition
to the low-rank approximation algorithms [FKV04] used in the quantum-inspired literature, others
have used importance sampling for, e.g., orthogonal tensor decomposition [DM07, MMD08, SWZ16]
(generalizing low-rank approximation [FKV04]) and support vector machines [HKS11].

The fundamental diﬀerence between quantum-inspired algorithms and traditional sketching
algorithms is that we assume “we can perform quantum measurements” of states corresponding
to input in time independent of input dimension (that is, we have eﬃcient sampling and query
access to input), and in exchange want algorithms that run in time independent of dimension and
provide only (over)sampling and query access to the output. This quantum-inspired model is weaker
than the standard sketching algorithm model (Remark 3.11): an algorithm taking T time in the
quantum-inspired model for an input matrix A can be converted to a standard algorithm that runs
in time O(nnz(A) + T ), where nnz(A) is the number of nonzero entries of A. So, we can also think
about an O(T )-time quantum-inspired algorithm as an O(nnz(A) + T )-time sketching algorithm,
where the nnz(A) portion of the runtime can only be used to facilitate importance sampling.8 This
restriction makes for algorithms that may perform worse in generic sketching settings, but work
in more settings, and so demonstrate lack of exponential quantum speedup for a wider range of
problems.

A natural question is whether more modern sketching techniques can be used in our model. After
all, importance sampling is only one of many sketching techniques studied in the large literature
on sketching algorithms. Notably, though, other types of sketches seem to fail in the input regimes
where quantum machine learning succeeds: assuming sampling and query access to input, importance
sampling takes time independent of dimension, whereas other randomized linear algebra methods
such as Count-Sketch and Johnson-Lindenstrauss still take time linear in input-sparsity.

Subsequent work by Chepurko, Clarkson, Horesh, Lin, and Woodruﬀ [CCH+20] notes that
importance sampling oversamples leverage score sampling, so usual analyses for leverage score
sampling also hold for importance sampling, up to some small overhead. It is reasonable to suspect
that this connection could lead to signiﬁcant improvements over the results presented here. However,
exploiting this connection for improved runtimes seems nontrivial, since most approaches using
leverage score sampling requires performing O(nnz(A))-time pre-processing operations, even if one
has SQ(A). As a simple example, for low-rank approximation of an input matrix A, if we wish
to adapt the algorithm of Clarkson and Woodruﬀ [CW17, Woo14], importance sampling of A can
replace CountSketch for one of the sketches [Woo14, Lemma 4.2], but importance sampling of SA
cannot replace the other [Woo14, Theorem 4.3]. Versions of importance sampling may work for the
second sketch (for example, sampling from a low-rank approximation of SA), but we are aware

8The same holds for quantum algorithms using the QRAM data structure input model: the data structure itself

can be built during an O(nnz(A))-time (classical) preprocessing phase.

10

of none that can be obtained easily from SQ(A). This may be a manifestation of the diﬃculty of
achieving relative-error estimates to quantities like leverage scores and residuals in this model.

An alternative approach is to use a projection-cost preserving sketch (PCP) like ridge leverage
score sampling to sketch A on both sides [CMM17]. The importance sampling from SQ(A) λ/2-
oversamples ridge leverage score sampling, where λ := (cid:107)A(cid:107)2
k+1, so using
importance sampling in place of ridge leverage score sampling can give algorithms. This is how
[CCH+20] gets their algorithms for low-rank sampling (recommendation systems) and quantum-
inspired linear regression. This does appear to be a promising approach, with possibility to extend
to dequantizing all of QSVT, but to the authors’ knowledge, it is still an open question how to
improve the algorithms presented in this work, with the exception of [CCH+20] improving over our
recommendation systems algorithm. Their use of PCPs signiﬁcantly improves the runtime for this
low-rank sampling task down below (cid:107)A(cid:107)6
σ6ε6 , but getting a similarly good runtime for all functions
seems nontrivial. For example, their linear regression algorithm requires that the input matrix is
strictly rank-k (or is regularized).

F/(cid:107)A − Ak(cid:107)2

F ≥ (cid:107)A(cid:107)2

F/σ2

F

The quantum-like closure properties of importance sampling shown here may be useful in the
context of classical sketching algorithms. This insight unlocks surprising power in importance
sampling. For example, it reveals that Frieze, Kannan, and Vempala’s low-rank approximation
algorithm [FKV04], which, as stated, requires O(kmn) time to output the desired matrix, actually
can produce useful results (samples and entries) in time independent of input dimension. To use
the language in [FKV04], if Assumptions 1 and 2 hold for the input matrix, they also hold for the
output matrix!

Classical algorithms for quantum problems. We are aware of two important prior results
from before Tang’s ﬁrst paper [Tan19] that connect quantum algorithms to randomized numerical
linear algebra. The ﬁrst is Van den Nest’s work on using probabilistic methods for quantum
simulation [VdN11], which deﬁnes a notion of “computationally tractable” (CT) state equivalent
to our notion of sampling and query access and then uses it to simulate restricted classes of
quantum circuits. We share some essential ideas with this work, such as the simple sampling
lemmas Lemmas 3.6 and 4.12, but also diﬀer greatly since we focus on low-rank matrices relevant
for QML, whereas [VdN11] focuses on simulating potentially large quantum circuits that correspond
to high-rank matrices. The second is a paper by Rudi, Wossnig, Ciliberto, Rocchetto, Pontil, and
Severini [RWC+20] that uses the Nystr¨om method to simulate a sparse Hamiltonian H on a sparse
input state in time poly-logarithmic in dimension and polynomial in (cid:107)H(cid:107)F, assuming sampling and
query access to H. Our Hamiltonian simulation results do not require a sparsity assumption and
still achieve a dimension-independent runtime, but get slightly larger exponents in exchange.

Practical implementation. A work by Arrazola, Delgado, Bardhan, and Lloyd [ADBL20]
implements and benchmarks quantum-inspired algorithms for regression and recommendation
systems. The aforementioned paper of Chepurko, Clarkson, Horesh, Lin, and Woodruﬀ [CCH+20]
does the same for the quantum-inspired algorithms they introduce. The former work makes various
conclusions, including that the ε2 scaling in the number of rows/columns taken in our recommendation
systems algorithm is inherent and that the quantum-inspired algorithms performed slower and worse
than direct computation for practical datasets. The latter work ﬁnds that their algorithms perform
faster than direct algorithms, with an accompanying increase in error comparable to that of other
sketching algorithms [DKW18]. This improvement appears to come from both a better-performing
implementation as well as an algorithm with better asymptotic runtime. Nevertheless, it is diﬃcult
to draw deﬁnitive conclusions about the practicality of quantum-inspired algorithms as a whole

11

from these experimental results. Since quantum-inspired algorithms are a restricted, weaker form of
computation than classical randomized numerical linear algebra algorithms (see the comparison
made above), it seems possible that they perform worse than standard sketching algorithms, despite
seemingly having exponentially improved runtime in theory.

Modern sketching algorithms use similar techniques to quantum-inspired algorithms, but are
more natural to run on a classical computer and are likely to be faster. For example, Dahiya,
Konomis, and Woodruﬀ [DKW18] conducted an empirical study of sketching algorithms for low-
rank approximation on both synthetic datasets and the movielens dataset, reporting that their
implementation “ﬁnds a solution with cost at most 10 times the optimal one . . . but does so 10 times
faster.” Sketching algorithms like those in [DKW18] may become a relevant point of reference for
benchmarking quantum linear algebra, when the implementation of these quantum algorithms on
actual quantum hardware becomes possible. In a sense, our work shows using asymptotic runtime
bounds that in many scenarios sketching and sampling techniques give similar computational power
to quantum linear algebra, which is a counterintuitive point since the former typically leads to linear
runtimes and the latter leads to poly-logarithmic ones.

Quantum machine learning. Our work has major implications for the landscape of quantum
machine learning. Since we have presented many dequantized versions of QML algorithms, the
question remains of what QML algorithms don’t have such versions. In other words, what algorithms
still have the potential to give exponential speedups?

There are two general paradigms for employing quantum linear algebra techniques in quantum
machine learning: the low-rank approach and the high-rank approach.
In both, we need to
turn classical input data vectors to quantum states (via what’s called an amplitude encoding),
perform linear algebra operations on those vectors, and extract information about the output via
sampling [Aar15]. Since preparing generic quantum states require a number of quantum gates
that is proportional to the dimension of the vectors, we need some state preparation assumptions
(like having QRAM with an appropriate data structure, etc., cf. Remark 3.11) in order to achieve
sublinear runtimes. The main diﬀerence between the two paradigms is how the input matrices
are given: in the low-rank approach, they are also given in QRAM, and so must be rescaled to
have Frobenius norm one. QSVT with block-encodings coming from the QRAM data structure9
or density operators is an example framework in this vein. The restrictions of this block-encoding
method means that the rank needs to be small, but assuming the hardware needed for QRAM
can be realized, this is still a ﬂexible setting, one that QML researchers ﬁnd interesting. In the
high-rank approach, which is used in the HHL algorithm [HHL09] and its derivatives, the matrix
needs to be represented by a concise quantum circuit and have a small (poly-logarithmic in input
dimension) condition number in order to gain an exponential speedup over classical algorithms. This
doesn’t happen in typical datasets. The collection of these demanding requirements hamstrings most
attempts to ﬁnd applications of HHL [HHL09] with the potential for practical super-polynomial
speedups.

Our results give evidence for the lack of exponential speedup for the former, low-rank approach.
It is important to note however, that our results do not rule out the possibility of large polynomial
quantum speedups. In order to assess the potential usefulness of QML algorithms in this regime it is
important to improve on classical upper and lower bounds for these problems, which we leave as an
open question. On the other hand, high-rank block-encodings, such as those coming from sparsity

9The QRAM data structure used here has alternatives which in some sense vary the “norm” in which one stores
the input matrix [KP20, Theorem IV.4], [CGJ19, Lemma 25]. We do not expect that these alternatives would give
sampling and query access to the matrix they store or could be otherwise dequantized, since these datastructures
generalize and strengthen the sparse-access input model, which is known to be BQP-complete [HHL09].

12

assumptions in the original HHL algorithm [HHL09], remain impervious to our techniques. This
suggests that the most promising way to get exponential quantum speedups for QML algorithms is
by assuming sparse matrices as input, or utilizing other eﬃciently implementable high-rank quantum
operation such as the Quantum Fourier Transform.

Works from Zhao, Fitzsimons, and Fitzsimons on Gaussian process regression [ZFF19]; from
Lloyd, Garnerone, and Zanardi on topological data analysis [LGZ16, GCD20]; and from Yamasaki,
Subramanian, Sonoda, and Koashi [YSSK20] on learning random features attempt to address these
issues to get a super-polynomial quantum speedup. Though these works avoid the dequantization
barrier to large quantum speedups, it remains to be seen how broad will be their impact on QML
and whether these speedups manifest for data seen in practice.

Independently from our work, Jethwani, Le Gall, and Singh
Related independent work.
simultaneously derived similar results [JLGS20]. They implicitly derive a version of our even SVT
result, and use it to achieve generic SVT (approximate SQ(b†f (SV)(A)) for a vector b) by writing
f (SV)(A) = Ag(A†A) for g(x) = f (
x and then using sampling subroutines to get the solution
from the resulting expression b†AR†U R. It is diﬃcult to directly compare the main SVT results,
because the parameters that appear in their runtime bounds are somewhat non-standard, but one
can see that for typical choices of f , their results require a strictly low-rank A. In comparison our
results apply to general A, and we also demonstrate how to apply them to (re)derive dequantized
algorithms.

x)/

√

√

1.6 Open questions

Our framework recovers recent dequantization results, and we hope that it will be used for de-
quantizing more quantum algorithms.
In the meantime, our work leaves several natural open
questions:

(a) Is there an approach to QML that does not go through HHL (whose demanding assumptions
make exponential speedups diﬃcult to demonstrate even in theory) or a low-rank assumption
(which, as we demonstrate, makes the tasks “easy” for classical computers) and yields a
provable superpolynomial speedup for a practically relevant ML problem?

(b) Our algorithms still have signiﬁcant slowdown as compared to their quantum counterparts.

Can we shave condition number factors to get runtimes of the form (cid:101)O
(for the rec-
ommendation systems application, for instance), without introducing additional assumptions?
Can we get even better runtimes by somehow avoiding SVD computation?

(cid:16) (cid:107)A(cid:107)6
F

σ6ε6 log3 1

δ

(cid:17)

(c) Do the matrix arithmetic closure properties we showed for (cid:96)2-norm importance sampling
hold for other kinds of sampling and sketching distributions, like leverage score or (cid:96)p-norm
sampling?

(d) In the quantum setting, linear algebra algorithms [GSLW19] can achieve logarithmic depen-
dence on the precision ε. Can classical algorithms also achieve such exponentially improved
dependence, when the goal is restricted to sampling from the output (i.e., without the re-
quirement to query elements of the output)? If not, is there a mildly stronger classical model
that can achieve this? Can one prove that this exponential advantage for sampling problems
cannot be conferred to estimation/decision problems?

13

1.7 Organization

The paper proceeds as follows. Section 3 introduces the notion of (over)sampling and query access
and some of its closure properties. Section 4 gives the fundamental idea of using sampling and
query access to sketch matrices used for the approximation results in Section 4.1 and singular value
transformation results in Section 5. These results form the framework that is used to dequantize
QSVT in Section 6.1 and recover all the quantum-inspired results in Section 6. These applications
of our framework contain various tricks and patterns that we consider to be “best practice” for
coercing problems into our framework, since they have given us the best complexities and generality.
More general results of SVT are shown in Section 7.

2 Preliminaries

To begin with, we deﬁne notation to be used throughout this paper. For n ∈ N, [n] := {1, . . . , n}.
For z ∈ C, its absolute value is |z| =
z∗z, where z∗ is the complex conjugate of z. f (cid:46) g denotes
the ordering f = O(g) (and respectively for (cid:38) and (cid:104)). (cid:101)O(g) is shorthand for O(g poly(log g)).
Finally, we assume that arithmetic operations (e.g., addition and multiplication of real numbers)
and function evaluation oracles (computing f (x) from x) take unit time, and that queries to oracles
(like the queries to input discussed in Section 3) are at least unit time cost.

√

2.1 Linear algebra

In this paper, we consider complex matrices A ∈ Cm×n for m, n ∈ N. For i ∈ [m], j ∈ [n], we let
A(i, ·) denote the i-th row of A, A(·, j) denote the j-th column of A, and A(i, j) denote the (i, j)-th
element of A. (A | B) denotes the concatenation of matrices A and B and vec(A) ∈ Cmn denotes
the vector formed by concatenating the rows of A. For vectors v ∈ Cn, (cid:107)v(cid:107) denotes standard
Euclidean norm (so (cid:107)v(cid:107) := ((cid:80)n
i=1|vi|2)1/2). For a matrix A ∈ Cm×n, the Frobenius norm of A
is (cid:107)A(cid:107)F := (cid:107)vec(A)(cid:107) = ((cid:80)m
(cid:80)n
j=1|A(i, j)|2)1/2 and the spectral norm of A is (cid:107)A(cid:107) := (cid:107)A(cid:107)Op :=
i=1
supx∈Cn,(cid:107)x(cid:107)=1 (cid:107)Ax(cid:107). We say that U is an isometry if (cid:107)U x(cid:107) = (cid:107)x(cid:107) for all x, or equivalently, if U is a
subset of columns of a unitary.

A singular value decomposition (SVD) of A is a representation A = U DV †, where for N :=
min(m, n), U ∈ Cm×N and V ∈ Cn×N are isometries and D ∈ RN ×N is diagonal with σi := D(i, i)
and σ1 ≥ σ2 ≥ · · · ≥ σN ≥ 0. We can also write this decomposition as A = (cid:80)N
i , where
ui := U (·, i) and vi := V (·, i). For Hermitian A, an (unitary) eigendecomposition of A is a singular
value decomposition where U = V , except the entries of D are allowed to be negative.

i=1 σiuiv†

Using SVD, we can deﬁne the rank-k approximation of A to be Ak := (cid:80)k
viu†

i and the pseu-
i . We now formally deﬁne singular value transformation:

doinverse of A to be A+ := (cid:80)rank(A)

i=1 σiuiv†

i=1

1
σi

Deﬁnition 2.1. For a function f : [0, ∞) → C such that f (0) = 0 and a matrix A ∈ Cm×n, we
deﬁne the singular value transform of A via a singular value decomposition A = (cid:80)min(m,n)
σiuiv†
i :

i=1

f (SV)(A) :=

min(m,n)
(cid:88)

i=1

f (σi)uiv†
i .

(1)

The requirement that f (0) = 0 ensures that the deﬁnition is independent of the (not necessarily
unique) choice of SVD.

14

Deﬁnition 2.2. For a function f : R → C and a Hermitian matrix A ∈ Cn×n, we deﬁne the
eigenvalue transform of A via a unitary eigendecomposition A = (cid:80)n

i=1 λiviv†
i :

f (EV)(A) :=

n
(cid:88)

i=1

f (λi)viv†
i .

(2)

Since we only consider eigenvalue transformations of Hermitian matrices, where singular vec-
tors/values and eigenvectors/values (roughly) coincide, the key diﬀerence between singular value
transformation and eigenvalue transformation is that the latter can distinguish eigenvalue sign. As
eigenvalue transformation is the standard notion of a matrix function, we will usually drop the
superscript in notation: f (A) := f (EV)(A).

We will use the following standard deﬁnition of a Lipschitz function.

Deﬁnition 2.3. We say f : R → C is L-Lipschitz on F ⊆ R if for all x, y ∈ F, |f (x)−f (y)| ≤ L|x−y|.

We deﬁne approximate isometry as follows:10

Deﬁnition 2.4. Let m, n ∈ N and m ≥ n. A matrix V ∈ Cm×n is an α-approximate isometry if
(cid:13)V †V − I(cid:13)
(cid:13)
(cid:13) ≤ α. It is an α-approximate projective isometry if (cid:107)V †V − Π(cid:107) ≤ α for Π an orthogonal
projector.

If V is an α-approximate isometry, among other things, it implies that |(cid:107)V (cid:107)2 − 1| ≤ α and that
there exists an isometry U ∈ Cm×n with im(U ) = im(V ) such that (cid:107)U − V (cid:107) ≤ α. We show this and
other basic facts in the following lemma, whose proof is deferred to Appendix B.

Lemma 2.5. If ˆX ∈ Cm×n is an α-approximate isometry, then there is an exact isometry X ∈ Cm×n
with the same columnspace as ˆX such that (cid:107) ˆX − X(cid:107) ≤ α. Furthermore, for any matrix Y ∈ Cn×n,

(cid:107) ˆXY ˆX † − XY X †(cid:107) ≤ (2α + α2)(cid:107)Y (cid:107).

If α < 1, then (cid:107) ˆX +(cid:107) ≤ (1 − α)−1 and

(cid:107) ˆXY ˆX † − XY X †(cid:107) ≤ α

2 − α
(1 − α)2 (cid:107) ˆXY ˆX †(cid:107).

3 Sampling and query access oracles

Since we want our algorithms to run in time sublinear in input size, we must carefully deﬁne our
access model. The sampling and query oracle we present below is unconventional, being designed as
a reasonable classical analogue for the input model of some quantum algorithms. It will also be
used heavily to move between intermediate steps of these quantum-inspired algorithms. First, as a
warmup, we deﬁne a simple query oracle:

Deﬁnition 3.1 (Query access). For a vector v ∈ Cn, we have Q(v), query access to v, if for
all i ∈ [n], we can query for v(i). Likewise, for a matrix A ∈ Cm×n, we have Q(A) if for all
(i, j) ∈ [m] × [n], we can query for A(i, j). Let q(v) (respectively q(A)) denote the (time) cost of
such a query.

10This is the notion of approximate orthonormality as given by the ﬁrst arXiv version of [Tan19].

15

For example, in the typical RAM access model, we are given our input v ∈ Cn as Q(v) with
q(v) = 1. For brevity, we will sometimes abuse this notation (and other access notations) and,
for example, abbreviate “Q(A) for A ∈ Cm×n” as “Q(A) ∈ Cm×n”. We will also sometimes abuse
complexity notation like q to refer to known bounds on the complexity, instead of the complexity
itself.

Deﬁnition 3.2 (Sampling and query access to a vector). For a vector v ∈ Cn, we have SQ(v),
sampling and query access to v, if we can:

1. query for entries of v as in Q(v);

2. obtain independent samples i ∈ [n] following the distribution Dv ∈ Rn, where Dv(i) :=

|v(i)|2/(cid:107)v(cid:107)2;

3. query for (cid:107)v(cid:107).

Let q(v), s(v), and n(v) denote the cost of querying entries, sampling indices, and querying the
norm respectively. Further deﬁne sq(v) := max(q(v), s(v), n(v)).

We will refer to these samples as importance samples from v, though one can view them as

measurements of the quantum state |v(cid:105) := 1
(cid:107)v(cid:107)

(cid:80) vi|i(cid:105) in the computational basis.

Quantum-inspired algorithms typically don’t give exact sampling and query access to the output
vector. Instead, we get a more general version of sampling and query access, which assumes we can
only access a sampling distribution that oversamples the correct distribution.11

Deﬁnition 3.3. For p, q ∈ Rn
p φ-oversamples q if, for all i ∈ [n], p(i) ≥ q(i)/φ.

≥0 that are distributions, meaning (cid:80)

i p(i) = (cid:80)

i q(i) = 1, we say that

The motivation for this deﬁnition is the following: if p φ-oversamples q, then we can convert
a sample from p to a sample from q with probability 1/φ using rejection sampling: sample an i
distributed as p, then accept the sample with probability q(i)/(φp(i)) (which is ≤ 1 by deﬁnition).
Deﬁnition 3.4 (Oversampling and query access). For v ∈ Cn and φ ≥ 1, we have SQφ(v), φ-
oversampling and query access to v, if we have Q(v) and SQ(˜v) for ˜v ∈ Cn a vector satisfying
(cid:107)˜v(cid:107)2 = φ(cid:107)v(cid:107)2 and |˜v(i)|2 ≥ |v(i)|2 for all i ∈ [n]. Denote sφ(v) := s(˜v), qφ(v) := q(˜v), nφ(v) := n(˜v),
and sqφ(v) := max(sφ(v), qφ(v), q(v), nφ(v)).

The distribution D˜v φ-oversamples Dv, since for all i ∈ [n],

D˜v(i) =

|˜vi|2
(cid:107)˜v(cid:107)2 =

|vi|2
φ(cid:107)v(cid:107)2 =

|˜vi|2
φ(cid:107)v(cid:107)2 ≥
For this reason, we call D˜v a φ-oversampled importance sampling distribution of v. SQ(v) is the
same as SQ1(v), by taking ˜v = v. Note that we do not assume knowledge of φ (though it can be
estimated, (though it can be estimated as shown in Lemma 3.5). However, we do need to know
(cid:107)˜v(cid:107) (even if (cid:107)v(cid:107) is known), as it cannot be deduced from a small number of queries, samples,
or probability computations. So, we will be choosing ˜v (and, correspondingly, φ) such that (cid:107)˜v(cid:107)2
remains computable, even if potentially some c˜v satisﬁes all our other requirements for some c < 1
(giving a smaller value of φ).

Dv(i).

1
φ

Intuitively speaking, estimators that use Dv can also use D˜v via rejection sampling at the expense
of a factor φ increase in the number of utilized samples. From this observation we can prove that
oversampling access implies an approximate version of the usual sampling access:

11Oversampling turns out to be the “natural” form of approximation in this setting; other forms of error do not

propagate through quantum-inspired algorithms well.

16

Lemma 3.5. Suppose we are given SQφ(v) and some δ ∈ (0, 1]. Denote (cid:102)sq(v) := φ sqφ(v) log 1
δ .
We can sample from Dv with probability ≥ 1 − δ in O((cid:102)sq(v)) time. We can also estimate (cid:107)v(cid:107) to ν
multiplicative error for ν ∈ (0, 1] with probability ≥ 1 − δ in O(cid:0) 1

ν2 (cid:102)sq(v)(cid:1) time.

Proof. Consider the following rejection sampling algorithm to generate samples: sample an index i
from ˜v, and output it as the desired sample with probability r(i) := |v(i)|2
|˜v(i)|2 . Otherwise, restart. We
can perform this: we can compute r(i) in O(cid:0)sqφ(v)(cid:1) time and r(i) ≤ 1 since ˜v bounds v.

The probability of accepting a sample in a round is (cid:80)

i D˜v(i)r(i) = (cid:107)v(cid:107)2/(cid:107)˜v(cid:107)2 = φ−1 and,
conditioned on a sample being accepted, the probability of it being i is |v(i)|2/(cid:107)v(cid:107)2, so the output
distribution is Dv as desired. So, to get a sample with ≥ 1 − δ probability, run rejection sampling
for at most 2φ log 1

To estimate (cid:107)v(cid:107)2, notice that we know (cid:107)˜v(cid:107)2, so it suﬃces to estimate (cid:107)v(cid:107)2/(cid:107)˜v(cid:107)2 which is φ−1.
The probability of accepting the rejection sampling routine is φ−1, so we run 3ν−2φ log 2
δ rounds
of it for estimating φ−1. Let Z denote the fraction of them which end in acceptance. Then, by a
Chernoﬀ bound we have

δ rounds.

Pr[|Z − φ−1| ≥ νφ−1] ≤ 2 exp

(cid:16)

−

(cid:17)

ν2zφ−1
2 + ν

≤ δ,

so Z(cid:107)˜v(cid:107)2 is a good multiplicative approximation to (cid:107)v(cid:107)2 with probability ≥ 1 − δ.

Generally, compared to a quantum algorithm that can output (and measure) a desired vector |v(cid:105),
our algorithms will output SQφ(u) such that (cid:107)u − v(cid:107) is small. So, (cid:102)sq(u) is the relevant complexity
measure that we will analyze and bound: if we wish to mimic samples from the output of the quantum
algorithm we dequantize, we will pay a one-time cost to run our quantum-inspired algorithm for
“obtaining” SQφ(u), and then pay (cid:102)sq(u) cost per additional measurement. As for error, bounds on
(cid:107)u − v(cid:107) imply that measurements from u and v follow distributions that are close in total variation
distance [Tan19, Lemma 4.1]. Now, we show that oversampling and query access of vectors is closed
under taking small linear combinations.
Lemma 3.6 (Linear combinations, Proposition 4.3 of [Tan19]). Given SQϕt(vt) ∈ Cn and λt ∈ C for
(cid:80) ϕt(cid:107)λtvt(cid:107)2
(cid:107) (cid:80) λtvt(cid:107)2 and sqφ((cid:80) λtvt) = max
all t ∈ [τ ], we have SQφ((cid:80)τ
t=1 q(vt)
(after paying O((cid:80)τ
t=1 nϕt(vt)) one-time pre-processing cost to query for norms).
Proof. Denote u := (cid:80) λtvt. To compute u(s) for some s ∈ [n], we just need to query vt(s) for all
t ∈ [τ ], paying O((cid:80) q(vt)) cost. So, it suﬃces to get SQ(˜u) for an appropriate bound ˜u. We choose

t=1 λtvt) for φ = τ

sϕt(vt)+(cid:80)τ

t∈[τ ]

t=1 |λt˜vt(s)|2,
so that |˜u(s)| ≥ |u(s)| by Cauchy–Schwarz, and (cid:107)˜u(cid:107)2 = τ (cid:80)τ
the desired value of φ.

˜u(s) = (cid:112)τ (cid:80)τ

t=1 (cid:107)λt˜vt(cid:107)2 = τ (cid:80)τ

t=1 ϕt(cid:107)λtvt(cid:107)2, giving

We have SQ(˜u): we can compute (cid:107)˜u(cid:107)2 by querying for all norms (cid:107)˜vt(cid:107), compute ˜u(s) by querying
(cid:107)λt ˜vt(cid:107)2
(cid:96) (cid:107)λ(cid:96) ˜v(cid:96)(cid:107)2 , and

˜vt(s) for all t ∈ [τ ]. We can sample from ˜u by ﬁrst sampling t ∈ [τ ] with probability
then taking our sample to be j ∈ [n] from ˜vt. The probability of sampling j ∈ [n] is correct:

(cid:80)

τ
(cid:88)

t=1

(cid:107)λt˜vt(cid:107)2
(cid:96) (cid:107)λ(cid:96)˜v(cid:96)(cid:107)2

(cid:80)

|˜vt(j)|2
(cid:107)˜vt(cid:107)2 =

(cid:80)τ
(cid:80)τ

t=1|λt˜vt(j)|2
(cid:96)=1 (cid:107)λ(cid:96)˜v(cid:96)(cid:107)2 =

|˜u(j)|2
(cid:107)˜u(cid:107)2 .

If we pre-process by querying all the norms (cid:107)˜v(cid:96)(cid:107) in advance, we can sample from the distribution
over i’s in O(1) time, using an alias sampling data structure for the distribution (Remark 3.11),
and we can sample from ˜vt using our assumed access to it, SQϕt(vt).

17

So, our general goal will be to express our output vector as a linear combination of a small number
of input vectors that we have sampling and query access to. Then, we can get an approximate
SQ access to our output using Lemma 3.5, where we pay an additional “cancellation constant”
(cid:80) ϕt(cid:107)λtvt(cid:107)2
(cid:107) (cid:80) λtvt(cid:107)2 . This factor is only large when the linear combination has signiﬁcantly
factor of φ = τ
smaller norm than the components vt in the sum suggest. Usually, in our applications, we can
intuitively think about this overhead being small when the desired output vector mostly lies in a
subspace spanned by singular vectors with large singular values in our low-rank input. Quantum
algorithms also have the same kind of overhead. Namely, the QSVT framework encodes this in
the subnormalization constant α of block-encodings, and the overhead from the subnormalization
appears during post-selection [GSLW19]. When this cancellation is not too large, the resulting
overhead typically does not aﬀect too badly the runtime of our applications.

We also deﬁne oversampling and query access for a matrix. The same model (under an alternative
deﬁnition) is also discussed in prior work [FKV04, DKR02] and is the right notion for the sampling
procedures we will use.

Deﬁnition 3.7 (Oversampling and query access to a matrix). For a matrix A ∈ Cm×n, we have
SQ(A) if we have SQ(A(i, ·)) for all i ∈ [m] and SQ(a) for a ∈ Rm the vector of row norms
(a(i) := (cid:107)A(i, ·)(cid:107)).

We have SQφ(A) if we have Q(A) and SQ( ˜A) for ˜A ∈ Cm×n satisfying (cid:107) ˜A(cid:107)2

F = φ(cid:107)A(cid:107)2

F and

| ˜A(i, j)|2 ≥ |A(i, j)|2 for all (i, j) ∈ [m] × [n].

The complexity of (over)sampling and querying from the matrix A is denoted by sφ(A) :=
max(s( ˜A(i, ·)), s(˜a)), qφ(A) := max(q( ˜A(i, ·)), q(˜a)), q(A) := max(q(A(i, ·))), and nφ(A) := n(˜a)
respectively. We also denote sqφ(A) := max(sφ(A), qφ(A), q(A), nφ(A)). We omit subscripts if
φ = 1.

Observe that access to a matrix, SQφ(A), implies access to its vectorized version, SQφ(vec(A)):
we can take (cid:94)vec(A) = vec( ˜A), and the distribution for vec( ˜A) is sampled by sampling i from D˜a,
and then sampling j from D ˜A(i,·). This gives the output (i, j) with probability | ˜A(i, j)|2/(cid:107) ˜A(cid:107)2
F.
Therefore, one can think of SQφ(A) as SQφ(vec(A)), with the addition of having access to samples
(i, j) from vec(A), conditioned on ﬁxing a particular row i and also knowing the probabilities of
these conditional samples.

Now we prove that oversampling and query access is closed under taking outer products. The

same idea also extends to taking Kronecker products of matrices.

Lemma 3.8. Given vectors SQϕu(u) ∈ Cm and SQϕv (v) ∈ Cn, we have SQφ(A) for their outer
product A := uv† with φ = ϕuϕv and sφ(A) = sϕu(u) + sϕv (v), qφ(A) = qϕu(u) + qϕv (v), q(A) =
q(u) + q(v), and nφ(A) = nϕu(u) + nϕv (v),

Proof. We can query an entry A(i, j) = u(i)v(j)† by querying once from u and v. Our choice of
upper bound is ˜A = ˜u˜v†. Clearly, this is an upper bound on uv† and (cid:107) ˜A(cid:107)2
F = (cid:107)˜u(cid:107)2(cid:107)˜v(cid:107)2 = ϕuϕv(cid:107)A(cid:107)2
F.
We have SQ( ˜A) in the following manner: ˜A(i, ·) = ˜u(i)˜v†, so we have SQ( ˜A(i, ·)) from SQ(˜v) after
querying for ˜u(i), and ˜a = (cid:107)˜v(cid:107)2 ˜u, so we have SQ(˜a) from SQ(˜u) after querying for (cid:107)˜v(cid:107).

Using the same ideas as in Lemma 3.6, we can extend sampling and query access of input

matrices to linear combinations of those matrices.

Lemma 3.9. Given SQϕ(t)(A(t)) ∈ Cm×n and λt ∈ C for all t ∈ [τ ], we have SQφ(A) ∈ Cm×n
for A := (cid:80)τ
t=1 qϕ(t)(A(t)),

t=1 λtA(t) with φ = τ

sϕ(t)(A(t)) + (cid:80)τ

(cid:80)τ

t=1 ϕ(t)(cid:107)λtA(t)(cid:107)2
F
(cid:107)A(cid:107)2
F

and sφ(A) = max
t∈[τ ]

18

qφ(A) = (cid:80)τ
one-time pre-processing cost).

t=1 qϕ(t)(A(t)), q(A) = (cid:80)τ

t=1 q(A(t)), and nφ(A) = 1 (after paying O

(cid:16)(cid:80)τ

t=1 nϕ(t)(A(t))

(cid:17)

Proof. To compute A(i, j) = (cid:80)τ
for all t ∈ [τ ], paying O(cid:0)(cid:80)
We choose

t=1 λtA(t)(i, j) for (i, j) ∈ [m] × [n], we just need to query A(t)(i, j)
t q(A(t))(cid:1) cost. So, it suﬃces to get SQ( ˜A) for an appropriate bound ˜A.

˜A(i, j) =

(cid:113)

τ (cid:80)τ

t=1 |λt ˜A(t)(i, j)|2.

That | ˜A(i, j)| ≥ |A(i, j)| follows from Cauchy–Schwarz, and we get the desired value of φ:

(cid:107) ˜A(cid:107)2

F = τ

τ
(cid:88)

t=1

(cid:107)λi ˜A(t)(cid:107)2

F = τ

τ
(cid:88)

t=1

ϕ(t)(cid:107)λiA(t)(cid:107)2
F.

(cid:113)

τ (cid:80)τ

We have SQ( ˜A): we can compute (cid:107) ˜A(cid:107)F by querying for all norms (cid:107) ˜A(t)(cid:107)F, compute ˜a(i) =
t=1 (cid:107)λt ˜A(t)(i, ·)(cid:107)2 by querying ˜a(t)(i) for all t ∈ [τ ], and compute ˜A(i, j) by querying
(cid:107) ˜A(i, ·)(cid:107) =
˜A(t)(i, j) for all t ∈ [τ ]. Analogously to Lemma 3.6, we can sample from ˜a by ﬁrst sampling s ∈ [τ ]
with probability (cid:107)λs ˜A(s)(cid:107)2
, then taking our sample to be i ∈ [m] from D˜a(s). If we pre-process by
t (cid:107)λt ˜A(t)(cid:107)2
(cid:17)
querying all the Frobenius norms (cid:107) ˜A(t)(cid:107)F in advance, we can sample from ˜a in O
maxt∈[τ ] sϕ(t)(A(t))
time. We can sample from ˜A(i, ·) by ﬁrst sampling s ∈ [τ ] with probability (cid:107)λs ˜A(s)(i,·)(cid:107)2
t (cid:107)λt ˜A(t)(i,·)(cid:107)2 , then
(cid:80)
(cid:17)
t=1 qϕ(t)(A(t)) + maxt∈[τ ] sϕ(t)(A(t))

taking our sample to be j ∈ [n] from D ˜A(s)(i,·). This takes O
time.

(cid:16)(cid:80)τ

(cid:80)

(cid:16)

F

F

Remark 3.10. With the lemmas we’ve introduced, we can already get oversampling and query
access to some modest expressions. For example, consider RUR decompositions, which show up
frequently in our results: suppose we have SQ(A) for A ∈ Cm×n, R ∈ Cr×n a (possibly normalized)
subset of rows of A, and a matrix U ∈ Cr×r. Then

R†U R =

r
(cid:88)

r
(cid:88)

i=1

j=1

U (i, j)R(i, ·)†R(j, ·),

which is a linear combination of r2 outer products involving rows of A. So, by Lemma 3.8 and
Lemma 3.9, we have SQφ(R†U R).

For us, the most interesting scenario is when our sampling and query oracles take poly-logarithmic
time, since this corresponds to the scenarios where quantum state preparation procedures can run
in time polylog(n). In these scenarios, quantum machine learning have the potential to achieve
exponential speedups. We can provide such classical access in various ways.

Remark 3.11. Below, we list settings where we have sampling and query access to input matrices
and vectors, and whenever relevant, we compare the resulting runtimes to the time to prepare
analogous quantum states. Note that because we do not analyze classical algorithms in the bit
model, i.e., we do not count each operation bitwise, their runtimes may be missing log factors that
should be counted for a fair comparison between classical and quantum.

(a) (Data structure) Given v ∈ Cn in the standard RAM model, the alias method [Vos91] takes
Θ(n) pre-processing time to output a data structure that uses Θ(n) space and can sample

19

(cid:107)a(cid:107)2 = (cid:107)A(cid:107)2
F

|a1|2 = (cid:107)A(1, ·)(cid:107)2

|a2|2 = (cid:107)A(2, ·)(cid:107)2

|A(1, 1)|2 + |A(1, 2)|2

|A(1, 3)|2 + |A(1, 4)|2

|A(2, 1)|2 + |A(2, 2)|2

|A(2, 3)|2 + |A(2, 4)|2

|A(1, 1)|2

|A(1, 2)|2

|A(1, 3)|2

|A(1, 4)|2 |A(2, 1)|2

|A(2, 2)|2

|A(2, 3)|2

|A(2, 4)|2

A(1,1)
|A(1,1)|

A(1,2)
|A(1,2)|

A(1,3)
|A(1,3)|

A(1,4)
|A(1,4)|

A(2,1)
|A(2,1)|

A(2,2)
|A(2,2)|

A(2,3)
|A(2,3)|

A(2,4)
|A(2,4)|

Figure 2: Dynamic data structure for a matrix A ∈ C2×4 discussed in Remark 3.11 part (b). We
compose the data structure for a with the data structure for A’s rows.

from v in Θ(1) time. In other words, we can get SQ(v) with sq(v) = Θ(1) in O(n) time, and
by extension, for a matrix A ∈ Cm×n, SQ(A) with sq(A) = Θ(1) in O(mn) time.

If the input vector (resp. matrix) is given as a list of nnz(v) (resp. nnz(A)) of its non-zero
entries, then the pre-processing time is linear in that number of entries. Therefore, the
quantum-inspired setting can be directly translated to a basic randomized numerical linear
algebra algorithm. More precisely, with this data structure, a fast quantum-inspired algorithm
(say, one running in time O(T sq(A)) for T independent of input size) implies an algorithm in
the standard computational model (running in O(nnz(A) + T ) time).

(b) (Dynamic data structure) QML algorithms often assume that their input is in a data structure
with a certain kind of quantum access [Pra14, KP20, GLM08, WZP18, RSW+19, CGJ19].
They argue that, since this data structure allows for circuits preparing input states with
linear gate count but polylog depth, hardware called QRAM might be able to parallelize these
circuits enough so that they run in eﬀectively polylog time. In the interest of considering the
best of all possible worlds for QML, we will treat circuit depth as runtime for QRAM and
ignore technicalities.

This data structure (see Fig. 2) admits sampling and query access to the data it stores with
just-as-good runtimes: speciﬁcally, for a matrix A ∈ Cm×n, we get SQ(A) with q(A) = O(1),
s(A) = O(log mn), and n(A) = O(1). So, quantum-inspired algorithms can be used whenever
QML algorithms assume this form of input.

Further, unlike the alias method stated above, this data structure supports updating entries
in O(log mn) time, which is used in applications of QML where data accumulates over time
[KP17].

(cid:80)

(c) (Integrability assumption) For v ∈ Cn, suppose we can compute entries v(i) and sums
i∈I(b)|v(i)|2 in time T , where I(b) ⊂ [n] is the set of indices whose binary representation
begins with the bitstring b. Then we have SQ(v) where q(v) = O(T ), s(v) = O(T log n), and
n(v) = O(T ). Analogously, the quantum state that encodes v in its amplitudes, |v(cid:105) = (cid:80)
vi
(cid:107)v(cid:107) |i(cid:105),
i
can be prepared in time O(T log n) via Grover-Rudolph state preparation [GR02]. (One can
think about the QRAM data structure as pre-computing all the necessary sums for this
protocol.)

20

(d) (Uniformity assumption) Given O(1)-time Q(v) ∈ Cn and a β such that max|v(i)|2 ≤ β/n,
we have SQφ(v) with φ = β/(cid:107)v(cid:107)2 and sqφ(v) = O(1), by using the vector whose entries are
all (cid:112)β/n as the upper bound ˜v. Assuming the ability to query entries of v in superposition, a
quantum state corresponding to v can be prepared in time O(cid:0)√

φ log n(cid:1).

(e) (Sparsity assumption) If A ∈ Cm×n has at most s non-zero entries per row (with eﬃciently
computable locations) and the matrix elements are |A(i, j)| ≤ c (and eﬃciently computable),
then we have SQφ(A) for φ = c2 sm
, simply by using the uniform distribution over non-zero
(cid:107)A(cid:107)2
F
entries for the oversampling and query oracles. For example, for SQ(˜a) we can set ˜a(i) := c
s,
and for ˜A(i, ·) we use the vector with entries c at the non-zeros of A(i, ·) (potentially adding
some “dummy” zero locations to have exactly s non-zeroes).

√

Note that similar sparse-access assumptions are often seen in the QML and Hamiltonian
simulation literature [HHL09]. Also, if A is not much smaller than we expect, then φ can
be independent of dimension. For example, if A has exactly s non-zero entries per row and
|A(i, j)| ≥ c(cid:48) for non-zero entries, then φ ≤ (c/c(cid:48))2.

(f) (CT states) In 2009, Van den Nest deﬁned the notion of a “computationally tractable”
(CT) state [VdN11]. Using our notation, |ψ(cid:105) ∈ Cn is a CT state if we have SQ(ψ) with
sq(ψ) = polylog(n). Van den Nest’s paper identiﬁes several classes of CT states, including
product states, quantum Fourier transforms of product states, matrix product states of
polynomial bond dimension, stabilizer states, and states from matchgate circuits. For more
details on how can one get eﬃcient sampling and query access to such vectors we direct the
reader to [VdN11].

4 Matrix sketches

We now introduce the workhorse of our algorithms: the matrix sketch. Using sampling and query
access, we can generate these sketches eﬃciently, and these allow one to reduce the dimensionality
of a problem, up to some approximation. Most of the results presented in this section are known in
the classical sketching literature: we present them here for completeness, and to restate them in the
context of sampling and query access.

Deﬁnition 4.1. For a distribution p ∈ Rm, we say that a matrix S ∈ Rs×m is sampled according to
p if each row of S is independently chosen to be ei/(cid:112)s · p(i) with probability p(i), where ei is the
vector that is one in the ith position and zero elsewhere. If p is an (cid:96)2-norm sampling distribution
Dv as deﬁned in Deﬁnition 3.2, then we also say S is sampled according to v.

We call S an importance sampling sketch for A ∈ Cm×n if it is sampled according to A’s row
norms a, and we call S a φ-oversampled importance sampling sketch if it is sampled according to
the bounding row norms from SQφ(A), ˜a (or, more generally, from a φ-oversampled importance
sampling distribution of a).

One should think of S as a description of how to sketch A down to SA. The following lemma shows
that (cid:107)SA(cid:107)F approximates (cid:107)A(cid:107)F, giving a simple example of the phenomenon that SA approximates
A in certain senses: it shows that (cid:107)SA(cid:107)F = Θ((cid:107)A(cid:107)F) with probability ≥ 0.9 when S has Ω( 1
φ2 ) rows.
We show later (Lemma 4.8) that a similar statement holds for spectral norm: (cid:107)SA(cid:107) = Θ((cid:107)A(cid:107)) with
probability ≥ 0.9 when S has ˜Ω(φ2(cid:107)A(cid:107)2

F /(cid:107)A(cid:107)2) rows.

21

Lemma 4.2 (Frobenius norm bounds for matrix sketches). Let S ∈ Cr×m be a φ-oversampled
importance sampling sketch of A ∈ Cm×n. Then (cid:107)[SA](i, ·)(cid:107) ≤ (cid:112)φ/r(cid:107)A(cid:107)F for all i ∈ [r], so
(cid:107)SA(cid:107)2

F (unconditionally). Equality holds when φ = 1. Further,

F ≤ φ(cid:107)A(cid:107)2

(cid:104)

Pr

|(cid:107)SA(cid:107)2

F − (cid:107)A(cid:107)2

F| ≥

(cid:114)

φ2 ln(2/δ)
2r

(cid:105)

(cid:107)A(cid:107)2
F

≤ δ.

Proof. Let p be the distribution used to create S, and let si be the sample from p used for row i of
S. Then (cid:107)SA(cid:107)2

F is the sum of the row norms (cid:107)[SA](i, ·)(cid:107)2 over all i ∈ [r], and

(cid:107)[SA](i, ·)(cid:107)2 =

E[(cid:107)[SA](i, ·)(cid:107)2] =

m
(cid:88)

s=1

p(s)

(cid:107)A(si, ·)(cid:107)2
r · p(si)

≤

φ
r
(cid:107)A(s, ·)(cid:107)2
r · p(s)

(cid:107)A(cid:107)2
F

=

1
r

(cid:107)A(cid:107)2
F

The ﬁrst equation shows the unconditional bounds on (cid:107)SA(cid:107)F. When φ = 1, p(i) = (cid:107)A(i, ·)(cid:107)2/(cid:107)A(cid:107)2
F
so the inequality becomes an equality. By the second equation, (cid:107)SA(cid:107)2
F has expected value
zero and is the sum of independent random variables bounded in [−(cid:107)A(cid:107)2
F], so the
probabilistic bound follows immediately from Hoeﬀding’s inequality.

F, (φ − 1)(cid:107)A(cid:107)2

F − (cid:107)A(cid:107)2

In the standard algorithm setting, computing an importance sampling sketch requires reading
all of A, since we need to sample from Da. If we have SQφ(A), though, we can eﬃciently create a
φ-oversampling sketch S in O(cid:0)s(sφ(A) + qφ(A)) + nφ(A)(cid:1) time: for each row of S, we pull a sample
from p, and then compute (cid:112)p(i). After ﬁnding this sketch S, we have an implicit description of SA:
it is a normalized multiset of rows of A, so we can describe it with the row indices and corresponding
normalization, (i1, c1), . . . , (is, cs).

SA can be used to approximate matrix expressions involving A. Further, we can chain sketches
using the lemma below, which shows that from SQφ(A), we have SQ≤2φ((SA)†), under a mild
assumption on the size of the sketch S. This can be used to ﬁnd a sketch T † of (SA)†. The
resulting expression SAT is small enough that we can compute functions of it in time independent
of dimension, and so will be used extensively. When we discuss sketching A down to SAT , we are
referring to the below lemma for the method of sampling T .

Lemma 4.3. Consider SQϕ(A) ∈ Cm×n and S ∈ Rr×m sampled according to ˜a, described as pairs
(i1, c1), . . . , (ir, cr). If r ≥ 2ϕ2 ln 2
δ , then with probability ≥ 1 − δ, we have SQφ(SA) and SQφ((SA)†)
for some φ satisfying φ ≤ 2ϕ. If ϕ = 1, then for all r, we have SQ(SA) and SQ((SA)†).

The runtimes for SQφ(SA) are q(SA) = q(A), sφ(SA) = sϕ(A), qφ(SA) = qϕ(A), and
nφ(SA) = O(1), after O(nϕ(A)) pre-processing cost. The runtimes for SQφ((SA)†) are q((SA)†) =
q(A), sφ((SA)†) = sϕ(A) + r qϕ(A), qφ((SA)†) = r qϕ(A), and nφ((SA)†) = nϕ(A).
Proof. By Lemma 4.2, (cid:107)SA(cid:107)2
F/2 with probability ≥ 1 − δ. Suppose this bound holds. To
get SQφ(SA), we take (cid:102)SA = S ˜A, which bounds SA by inspection. Further, (cid:107)S ˜A(cid:107)2
F by
F ≤ 2ϕ. Analogously, (S ˜A)† works as a bound
Lemma 4.2, so φ = (cid:107)S ˜A(cid:107)2
F/(cid:107)SA(cid:107)2
for SQφ((SA)†). We can query an entry of SA by querying the corresponding entry of A, so all
that suﬃces is to show that we have SQ(S ˜A) and SQ((S ˜A)†) from SQ( ˜A). (When ϕ = 1, we can
ignore the above argument: the rest of the proof will show that we have SQ(SA) and SQ((SA)†)
from SQ(A).)

F = ϕ(cid:107)A(cid:107)2

F = (cid:107) ˜A(cid:107)2

F ≥ (cid:107)A(cid:107)2

F/(cid:107)SA(cid:107)2

We have SQ(S ˜A). Because the rows of S ˜A are rescaled rows of ˜A, we have SQ access to them
F/r, after precomputing

F and (cid:107)[S ˜A](i, ·)(cid:107)2 = (cid:107) ˜A(cid:107)2

from SQ access to ˜A. Because (cid:107)S ˜A(cid:107)2

F = (cid:107) ˜A(cid:107)2

22

F, we have SQ access to the vector of row norms of S ˜A (pulling samples simply by pulling

(cid:107) ˜A(cid:107)2
samples from the uniform distribution).

We have SQ((S ˜A)†). (This proof is similar to one from [FKV04].) Since the rows of (S ˜A)† are
length r, we can respond to SQ queries to them by reading all entries of the row and performing
some linear-time computation. (cid:107)(S ˜A)†(cid:107)2
F, so we can respond to a norm query by querying
the norm of ˜A. Finally, we can sample according to the row norms of (S ˜A)† by ﬁrst querying an
index i ∈ [r] uniformly at random, then outputting the index j ∈ [n] sampled from [S ˜A](i, ·) (which
we can sample from because it is a row of ˜A). The distribution of the samples output by this
procedure is correct: the probability of outputting j is

F = (cid:107) ˜A(cid:107)2

1
r

r
(cid:88)

i=1

|[S ˜A](i, j)|2
(cid:107)[S ˜A](i, ·)(cid:107)2

=

r
(cid:88)

i=1

|[S ˜A](i, j)|2
(cid:107)S ˜A(cid:107)2
F

=

(cid:107)[S ˜A](·, j)(cid:107)2
(cid:107)S ˜A(cid:107)2
F

.

4.1 Approximation results

Here, we present approximation results on sketched matrices that we will use heavily throughout
our results. We begin with a fundamental observation: given sampling and query access to a matrix
A, we can approximate the matrix product A†B by a sum of rank-one outer products. We formalize
this with two variance bounds, which we can use together with Chebyshev’s inequality.

Lemma 4.4 (Asymmetric matrix multiplication to Frobenius norm error, [DKM06, Lemma 4]).
Consider X ∈ Cm×n, Y ∈ Cm×p, and take S ∈ Rr×m to be sampled according to p ∈ Rm a
φ-oversampled importance sampling distribution from X or Y . Then,

E[(cid:107)X †S†SY − X †Y (cid:107)2

F] ≤

φ
r

(cid:107)X(cid:107)2

F(cid:107)Y (cid:107)2
F

(cid:104) r
(cid:88)

and E

(cid:107)[SX](i, ·)(cid:107)2(cid:107)[SY ](i, ·)(cid:107)2(cid:105)

≤

i=1

φ
r

(cid:107)X(cid:107)2

F(cid:107)Y (cid:107)2
F.

Proof. To show the ﬁrst equation, we use that E[(cid:107)X †X †SY − X †Y (cid:107)2
F] is a sum of variances, one
for each entry (i, j), since E[X †S†SY − XY ] is zero in every entry. Furthermore, for every entry
(i, j), the matrix expression is the sum of r independent, mean-zero terms, one for each row of S:

[X †S†SY − XY ](i, j) =

r
(cid:88)

(cid:16)

s=1

[SX](s, i)†[SY ](s, j) −

[X †Y ](i, j)

(cid:17)

.

1
r

So, we can use standard properties of variances12 to conclude that

E[(cid:107)X †S†SY − X †Y (cid:107)2

F] = r · E[(cid:107)[SX](1, ·)†[SY ](1, ·) − 1
r X †Y (cid:107)2
m
(cid:88)

m
(cid:88)

(cid:107)X(i, ·)†Y (i, ·)(cid:107)2
F
r2p(i)2

=

1
r

= r

p(i)

i=1

i=1

F] ≤ r · E[(cid:107)[SX](1, ·)†[SY ](1, ·)(cid:107)2
F]

(cid:107)X(i, ·)(cid:107)2(cid:107)Y (i, ·)(cid:107)2
p(i)

≤

φ
r

(cid:107)X(cid:107)2

F(cid:107)Y (cid:107)2
F.

The second other inequality follows by the same computation:

(cid:104) r
(cid:88)

E

i=1

(cid:107)[SX](i, ·)(cid:107)2(cid:107)[SY ](i, ·)(cid:107)2(cid:105)

= r · E[(cid:107)[SX](1, ·)(cid:107)2(cid:107)[SY ](1, ·)(cid:107)2] ≤

φ
s

(cid:107)X(cid:107)2

F(cid:107)Y (cid:107)2
F.

The above result shows that, given SQ(X), X †Y can be approximated by a sketch with constant
failure probability. If we have SQ(X) and SQ(Y ), we can make the failure probability exponential
small. To show this tighter error bound, we use an argument of Drineas, Kannan, and Mahoney
for approximating matrix multiplication. We state their result in a slightly stronger form, which is
actually proved in their paper. For completeness, a proof of this statement is in the appendix.

12See the proof of Lemma 4.5 in Appendix B for this kind of computation done with more detail.

23

Lemma 4.5 (Matrix multiplication by subsampling [DKM06, Theorem 1]). Suppose we are given
X ∈ Cn×m, Y ∈ Cn×p, r ∈ N and a distribution p ∈ Rn satisfying the oversampling condition that,
for some φ ≥ 1,

p(k) ≥

(cid:107)X(k, ·)(cid:107)(cid:107)Y (k, ·)(cid:107)

φ (cid:80)

(cid:96) (cid:107)X((cid:96), ·)(cid:107)(cid:107)Y ((cid:96), ·)(cid:107)

.

Let S ∈ Rr×n be sampled according to p. Then X †S†SY is an unbiased estimator for X †Y and

(cid:104)
(cid:107)X †S†SY − X †Y (cid:107)F <

Pr

(cid:114)

8φ2 ln(2/δ)
r

(cid:88)

(cid:96)

(cid:124)

(cid:107)X((cid:96), ·)(cid:107)(cid:107)Y ((cid:96), ·)(cid:107)

(cid:105)

> 1 − δ.

(cid:123)(cid:122)
≤(cid:107)X(cid:107)F(cid:107)Y (cid:107)F

(cid:125)

From a simple application of Lemma 4.5, we get a key lemma used frequently in Section 6.

Lemma 4.6 (Approximating matrix multiplication to Frobenius norm error; corollary of [DKM06,
Theorem 1]). Consider X ∈ Cm×n, Y ∈ Cm×p, and take S ∈ Rr×m to be sampled according to
q := q1+q2
, where q1, q2 ∈ Rm are φ1, φ2-oversampled importance sampling distributions from x, y,
the vector of row norms for X, Y , respectively. Then S is a 2φ1, 2φ2-oversampled importance
sampling sketch of X, Y , respectively. Further,

2

(cid:104)

Pr

(cid:107)X †S†SY − X †Y (cid:107)F <

(cid:114)

8φ1φ2 log 2/δ
r

(cid:107)X(cid:107)F(cid:107)Y (cid:107)F

(cid:105)

> 1 − δ.

Proof. First, notice that 2q(i) ≥ q1(i) and 2q(i) ≥ q2(i), so q oversamples the importance sampling
distributions for X and Y with constants 2φ1 and 2φ2, respectively. We get the bound by using
φ1φ2(cid:107)X(cid:107)F(cid:107)Y (cid:107)F
Lemma 4.5; q satisﬁes the oversampling condition with φ =
(cid:96) (cid:107)X((cid:96),·)(cid:107)(cid:107)Y ((cid:96),·)(cid:107) , using the inequality
of arithmetic and geometric means:

(cid:80)

√

1
q(i)

(cid:107)X(i, ·)(cid:107)(cid:107)Y (i, ·)(cid:107)
(cid:96) (cid:107)X((cid:96), ·)(cid:107)(cid:107)Y ((cid:96), ·)(cid:107)

(cid:80)

=

≤

≤

=

2
q1(i) + q2(i)
1
(cid:112)q1(i)q2(i)
√

(cid:80)

(cid:107)X(i, ·)(cid:107)(cid:107)Y (i, ·)(cid:107)
(cid:96) (cid:107)X((cid:96), ·)(cid:107)(cid:107)Y ((cid:96), ·)(cid:107)
(cid:107)X(i, ·)(cid:107)(cid:107)Y (i, ·)(cid:107)
(cid:96) (cid:107)X((cid:96), ·)(cid:107)(cid:107)Y ((cid:96), ·)(cid:107)

(cid:80)

φ1φ2(cid:107)X(cid:107)F(cid:107)Y (cid:107)F
(cid:107)X(i, ·)(cid:107)(cid:107)Y (i, ·)(cid:107)

(cid:107)X(i, ·)(cid:107)(cid:107)Y (i, ·)(cid:107)
(cid:96) (cid:107)X((cid:96), ·)(cid:107)(cid:107)Y ((cid:96), ·)(cid:107)

(cid:80)

√

φ1φ2(cid:107)X(cid:107)F(cid:107)Y (cid:107)F
(cid:96) (cid:107)X((cid:96), ·)(cid:107)(cid:107)Y ((cid:96), ·)(cid:107)

.

(cid:80)

Remark 4.7. Lemma 4.6 implies that, given SQφ1(X) and SQφ2(Y ), we can get SQφ(M ) for M a
suﬃciently good approximation to X †Y , with φ ≤ φ1φ2
. This is an approximate closure
property for oversampling and query access under matrix products.

F(cid:107)Y (cid:107)2
F
(cid:107)M (cid:107)2
F

(cid:107)X(cid:107)2

Given the above types of accesses, we can compute the sketch S necessary for Lemma 4.6 by
taking p = D˜x and q = D˜y), thereby ﬁnding a desired M := X †S†SY . We can compute entries of M
with only r queries each to X and Y , so all we need is to get SQ( ˜M ) for ˜M the appropriate bound.
We choose | ˜M (i, j)|2 := r (cid:80)r
(cid:96)=1 |[S ˜X]((cid:96), i)†[S ˜Y ]((cid:96), j)|2; showing that we have SQ(M ) follows from
the proofs of Lemmas 3.8 and 3.9, since M is simply a linear combination of outer products of rows

24

of ˜X with rows of ˜Y . Finally, this bound has the appropriate norm. Notating the rows sampled by
the sketch as s1, . . . , sr, we have

(cid:107) ˜M (cid:107)2

F = r

r
(cid:88)

(cid:96)=1

(cid:107)[S ˜X]((cid:96), ·)(cid:107)2(cid:107)[S ˜Y ]((cid:96), ·)(cid:107)2 = r

r
(cid:88)

(cid:96)=1

(cid:107) ˜X(s(cid:96), ·)(cid:107)2(cid:107) ˜Y (s(cid:96), ·)(cid:107)2
+ (cid:107) ˜Y (s(cid:96),·)(cid:107)2
2(cid:107) ˜Y (cid:107)2
F

r2( (cid:107) ˜X(s(cid:96),·)(cid:107)2
2(cid:107) ˜X(cid:107)2
F

)2

≤

r
(cid:88)

(cid:96)=1

(cid:107) ˜X(s(cid:96), ·)(cid:107)2(cid:107) ˜Y (s(cid:96), ·)(cid:107)2
r( (cid:107) ˜X(s(cid:96),·)(cid:107)(cid:107) ˜Y (s(cid:96),·)(cid:107)
)2
(cid:107) ˜X(cid:107)F(cid:107) ˜Y (cid:107)F

= (cid:107) ˜X(cid:107)2

F(cid:107) ˜Y (cid:107)2

F = φ1φ2(cid:107)X(cid:107)2

F(cid:107)Y (cid:107)2
F.

If X = Y , we can get an improved spectral norm bound: instead of depending on (cid:107)X(cid:107)2

F , error

depends on (cid:107)X(cid:107)(cid:107)X(cid:107)F .

Lemma 4.8 (Approximating matrix multiplication to spectral norm error [RV07, Theorem 3.1]).
Suppose we are given A ∈ Rm×n, ε > 0, δ ∈ [0, 1], and S ∈ Rr×n a φ-oversampled importance
sampling sketch of A. Then

(cid:104)

Pr

(cid:107)A†S†SA − A†A(cid:107) (cid:46)

(cid:114)

φ2 log r log 1/δ
r

(cid:107)A(cid:107)(cid:107)A(cid:107)F

(cid:105)

> 1 − δ.

The above results can be used to approximate singular values, simply by directly translating the

bounds on matrix product error to bounds on singular value error.

Lemma 4.9 (Approximating singular values). Given SQφ(A) ∈ Cm×n and ε ∈ (0, 1], we can form
importance sampling sketches S ∈ Rr×m and T † ∈ Rc×n in O(cid:0)(r + c) sqφ(A)(cid:1) time satisfying the
following property. Take r, c ≥ s for some suﬃciently large s = (cid:101)O
. Then, if σi and ˆσi
are the singular values of A and SAT , respectively (where ˆσi = 0 for i > min(r, c)), we have with
probability ≥ 1 − δ that

ε2 log 1

(cid:16) φ2

(cid:17)

δ

(cid:118)
(cid:117)
(cid:117)
(cid:116)

min(m,n)
(cid:88)

(ˆσ2

i − σ2

i )2 ≤ ε(cid:107)A(cid:107)2
F.

i=1

If we additionally assume that ε (cid:46) (cid:107)A(cid:107)/(cid:107)A(cid:107)F, we can conclude |σ2

i − ˆσ2

i | ≤ ε(cid:107)A(cid:107)(cid:107)A(cid:107)F for all i.

This result follows from results bounding the error between singular values by errors of matrix
products. For notation, let σi(M ) be the ith largest singular value of M . We will use the following
inequalities relating norm error of matrices to error in their singular values:

Lemma 4.10 (Hoﬀman-Wielandt inequality [KV17, Lemma 2.7]). For symmetric X, Y ∈ Rn×n,

(cid:88)

|σi(X) − σi(Y )|2 ≤ (cid:107)X − Y (cid:107)2
F.

Lemma 4.11 (Weyl’s inequality [Bha97, Corollary III.2.2]). For A, B ∈ Cm×n, |σk(A) − σk(B)| ≤
(cid:107)A − B(cid:107). When A, B are Hermitian, the same bound holds for their eigenvalues.13

Proof of Lemma 4.9. We use known theorems, plugging in the values of r and c. Using Lemma 4.6
for the sketch S, we know that

(cid:104)

Pr

(cid:107)A†S†SA − A†A(cid:107)F ≤

(cid:105)

(cid:107)A(cid:107)2
F

ε
2

≥ 1 − δ;

13[Bha97, Corollary III.2.2] actually proves the Hermitian version. The result about singular values is an easy

consequence, see for example the blog of Terence Tao [Tao10, Exercise 22(iv)].

25

by Lemma 4.3, T † is an ≤ 2φ-oversampled importance sampling sketch of (SA)†, so by Lemma 4.6
for T †,

Pr

and from Lemma 4.2,

(cid:104)
(cid:107)SAT T †A†S† − SAA†S†(cid:107)F ≤

(cid:105)

(cid:107)SA(cid:107)2
F

ε
4

≥ 1 − δ,

(cid:104)
(cid:107)SA(cid:107)2

F ≤ 2(cid:107)A(cid:107)2
F

(cid:105)

Pr

≥ 1 − δ.

By rescaling δ and union bounding, we can have all events happen with probability ≥ 1 − δ. Then,
from triangle inequality followed by Lemma 4.10,

(cid:113)(cid:88)

|σi(SAT )2 − σi(A)2|2 ≤

(cid:113)(cid:88)

(cid:113)(cid:88)

|σi(SAT )2 − σi(SA)2|2 +

|σi(SA)2 − σi(A)2|2
≤ (cid:107)(SAT )(SAT )† − (SA)(SA)†(cid:107)F + (cid:107)(SA)†(SA) − A†A(cid:107)F
≤ ε(cid:107)A(cid:107)2
F.

The analogous result holds for spectral norm via Lemma 4.8 and Lemma 4.11; the only additional
complication is that we need to assert that (cid:107)SA(cid:107) (cid:46) (cid:107)A(cid:107). We use the following argument, using the
upper bound on ε:

(cid:107)SA(cid:107)2 = (cid:107)A†S†SA(cid:107) ≤ (cid:107)A†S†SA − A†A(cid:107) + (cid:107)A†A(cid:107) ≤ (cid:107)A(cid:107)2 + ε(cid:107)A(cid:107)(cid:107)A(cid:107)F (cid:46) (cid:107)A(cid:107)2.

Finally, if we wish to approximate a vector inner product u†v, a special case of matrix product,
we can do so with only sampling and query access to one of the vectors while still getting log 1
δ
dependence on failure probability. The proof of this statement is in Appendix B.

Lemma 4.12 (Inner product estimation, [Tan19, Proposition 4.2]). Given SQφ(u), Q(v) ∈ Cn,
we can output an estimate c ∈ C such that |c − (cid:104)u, v(cid:105)| ≤ ε with probability ≥ 1 − δ in time
O(cid:0)φ(cid:107)u(cid:107)2(cid:107)v(cid:107)2 1

δ (sqφ(u) + q(v))(cid:1).

ε2 log 1

Remark 4.13. Lemma 4.12 also applies to higher-order tensor inner products:

(a) (Trace inner products, [GLT18, Lemma 11]) Given SQφ(A) ∈ Cn×n and Q(B) ∈ Cn×n, we

can estimate Tr[AB†] to additive error ε with probability at least 1 − δ by using

(cid:18)

O

φ

(cid:107)A(cid:107)2

F(cid:107)B(cid:107)2
F
ε2

(cid:0) sqφ(A) + q(B)(cid:1) log

(cid:19)

1
δ

time. To do this, note that SQφ(A) and Q(B) imply SQφ(vec(A)) and Q(vec(B)). Tr[AB] =
(cid:104)vec(B), vec(A)(cid:105), so we can just apply Lemma 4.12 to conclude.

(b) (Expectation values) Given SQφ(A) ∈ Cn×n and Q(x), Q(y) ∈ Cn, we can estimate x†Ay to

additive error ε with probability at least 1 − δ in

(cid:18)

O

φ

(cid:107)A(cid:107)2

F(cid:107)x(cid:107)2(cid:107)y(cid:107)2

ε2

(cid:0) sqφ(A) + q(x) + q(y)(cid:1) log

(cid:19)

1
δ

time. To do this, observe that x†Ay = Tr(x†Ay) = Tr(Ayx†) and that Q(yx†) can be simulated
with Q(x), Q(y). So, we just apply the trace inner product procedure.

Finally, we observe a simple technique to convert importance sampling sketches into approximate
isometries, by inserting the appropriate pseudoinverse. This will be used in some of the more
involved applications.

26

Lemma 4.14. Given A ∈ Cm×n, S ∈ Cr×m sampled from a φ-oversampled importance sampling
distribution of A, and T † ∈ Cn×c sampled from an ≤ φ-oversampled importance sampling distribution
of (SA)†, let R := SA and C := SAT . Let σk be the kth singular value of A. If, for α ∈ (0, 1],
r = ˜Ω( φ2(cid:107)A(cid:107)2(cid:107)A(cid:107)2
log 1
δ ), then with probability ≥ 1 − δ, ((Ck)+R)† is an
α-approximate projective isometry onto the image of (Ck)+. Further, (DV †R)† is an α-approximate
k = U DV † is a singular value decomposition truncated so that D ∈ Rk(cid:48)×k(cid:48) is full
isometry, where C+
rank (so k(cid:48) ≤ min(k, rank(A))).

δ ) and c = ˜Ω( φ2(cid:107)A(cid:107)2(cid:107)A(cid:107)2

log 1

σ4
kα2

σ4
k

F

F

Proof. The following occurs with probability ≥ 1 − δ. By Lemma 4.9, (cid:107)C+
. By Lemma 4.8,
(cid:107)R†R − A†A(cid:107) (cid:46) (cid:107)A(cid:107)2, which implies that (cid:107)R(cid:107) (cid:46) (cid:107)A(cid:107), and by Lemma 4.2, (cid:107)R(cid:107)F (cid:46) (cid:107)A(cid:107)F. Further,
(cid:107)RR† − CC†(cid:107) ≤ ασ2
k Ck is an orthogonal projector. So, with
k
probability ≥ 1 − δ,

k. Finally, C+

k C = C+

k (cid:107) (cid:46) 1
σ2
k

(cid:107)R(cid:107)(cid:107)R(cid:107)F
(cid:107)A(cid:107)(cid:107)A(cid:107)F

(cid:46) ασ2

(cid:107)(C+

k R)(C+

k R)† − (C+

k C)(C+

k C)†(cid:107) = (cid:107)C+

k (RR† − CC†)(C+

k )†(cid:107) ≤ (cid:107)C+

k (cid:107)2(cid:107)RR† − CC†(cid:107) = O(α).

We get the computation for the α-approximate isometry by restricting attention to the span of U :

(cid:107)(DV †R)(DV †R)† − I(cid:107) = (cid:107)DV †(RR† − CC†)V D†(cid:107) ≤ (cid:107)U DV †(cid:107)2(cid:107)RR† − CC†(cid:107) = O(α).

One can also observe that, for a suﬃciently good sketch C, R ≈ Ck(Ck)+R in spectral norm,
giving a generic way to approximate a sketch R by a product of a small matrix with an approximate
projective isometry. We do not need it in our proofs, so this computation is not included.

5 Singular value transformation

Our main result is that, given SQφ(A) and a smooth function f , we can approximate f (A†A) by a
decomposition R†U R + f (0)I. This primitive is based on the even singular value transformation
used by Gily´en, Su, Low, and Wiebe [GSLW19].

Theorem 5.1 (Even singular value transformation). Let A ∈ Cm×n and f : R+ → C be such that
f (x) and ¯f (x) := (f (x) − f (0))/x are L-Lipschitz and ¯L-Lipschitz, respectively, on ∪min(m,n)
i −
d, σ2
∗(cid:107)A(cid:107)2) and
δ ∈ (0, 1]. Choose a norm ∗ ∈ {F, Op}.

i + d] for some d > 0. Take parameters ε and δ such that 0 < ε (cid:46) min(L(cid:107)A(cid:107)2

i=1
∗, ¯L(cid:107)A(cid:107)2

[σ2

Suppose we have SQφ(A). Consider the importance sampling sketch S ∈ Rr×m corresponding to
SQφ(A) and the importance sampling sketch T † ∈ Rc×n corresponding to SQ≤2φ((SA)†) (which we
have by Lemma 4.3). Then, for R := SA and C := SAT , we can achieve the bound

(cid:104)

(cid:105)
(cid:107)R† ¯f (CC†)R + f (0)I − f (A†A)(cid:107)∗ > ε

Pr

< δ,

if r, c > (cid:107)A(cid:107)2(cid:107)A(cid:107)2

Fφ2 1

d2 log 1

δ (or, equivalently, d > ¯ε := (cid:107)A(cid:107)∗(cid:107)A(cid:107)F( φ2 log(1/δ)

min(r,c) )1/2) and

(cid:16)

r = ˜Ω

1
ε2 log
First, we make some technical remarks. The assumption that ε (cid:46) L(cid:107)A(cid:107)2

φ2 ¯L2(cid:107)A(cid:107)4(cid:107)A(cid:107)2

φ2L2(cid:107)A(cid:107)2

∗(cid:107)A(cid:107)2
F

c = ˜Ω

1
δ

∗(cid:107)A(cid:107)2
F

(cid:17)

(cid:16)

∗ is for non-degeneracy: if
ε ≥ L(cid:107)A(cid:107)2, then the naive approximation f (0)I of f (A†A) would suﬃce, since (cid:107)f (0)I − f (A†A)(cid:107) ≤
L(cid:107)A(cid:107)2 ≤ ε as desired.14 The parameter d (or, rather, the parameter ¯ε) speciﬁes the domain where

1
ε2 log

1
δ

(cid:17)

.

14The choice f (0)I assumes that f is Lipschitz on {0, (cid:107)A(cid:107)2}. More generally, we can choose f (x)I for any

x ∈ ∪min(m,n)
i=1

[σ2

i − d, σ2

i + d] in order to get a suﬃciently good naive approximation.

27

(3)

(4)

f (x) and ¯f (x) should be smooth: the condition in the theorem is that they should be Lipschitz on
the spectrum of A†A, with ¯ε room for approximation. This will not come into play often, though,
since we can often design our singular value transforms such that we can take d = ∞. For example,
if our desired transform f becomes non-smooth outside the relevant interval [0, (cid:107)A(cid:107)2], we can apply
Theorem 5.1 with d = ∞ and the function g such that g(x) = g((cid:107)A(cid:107)2) for x ≥ (cid:107)A(cid:107)2 and g(x) = f (x)
otherwise. Then g(A†A) = f (A†A) and g is smooth everywhere, so we do not need to worry about
the d parameter. Finally, we note that no additional log terms are necessary (i.e., ˜Ω becomes Ω)
when the Frobenius norm is used.

By our discussion in Section 4, ﬁnding the sketches S and T for Theorem 5.1 takes time
O(cid:0)(r + c) sφ(A) + rc qφ(A) + nφ(A)(cid:1), querying for all of the entries of C takes additional time
O(rc q(A)), and computing ¯f (CC†) takes additional time O(cid:0)min(r2c, rc2)(cid:1) (if done naively). For
our applications, this ﬁnal matrix function computation will dominate the runtime, and the rest of
the cost we will treat as O(cid:0)rc sqφ(A)(cid:1).

For some intuition on error bounds and time complexity, we consider how the parameters in our
main theorem behave in a restricted setting: suppose we have SQ(A) with minimum singular value
σ and such that (cid:107)A(cid:107)F/σ is dimension-independent.15 This condition simultaneously bounds the
rank and condition number of A. Further suppose16 that f is L-Lipschitz on the interval [0, (cid:107)A(cid:107)2]
and satisﬁes

L(cid:107)A(cid:107)2 < ΓD where D := max

x∈[0,(cid:107)A(cid:107)2]

f (x) − min

y∈[0,(cid:107)A(cid:107)2]

f (y),

for some dimension-independent Γ. Γ must be at least one, so we can think about such an f as being
at most Γ times “steeper” compared to the least possible “steepness”. Under these assumptions, we
can get a decomposition satisfying

(cid:107)R† ¯f (CC†)R + f (0)I − f (A†A)(cid:107) > εD

with probability ≥ 1 − δ by taking

(cid:16)
r = ˜Θ

Γ2 (cid:107)A(cid:107)2
F
(cid:107)A(cid:107)2

1
ε2 log

1
δ

(cid:17)

(cid:16)
and c = ˜Θ

Γ2 (cid:107)A(cid:107)2(cid:107)A(cid:107)2
σ4

F

1
ε2 log

1
δ

(cid:17)

.

The time to compute the decomposition is

(cid:18) (cid:107)A(cid:107)6
F
(cid:107)A(cid:107)2σ4

(cid:101)O

Γ6

ε6 log3 1

δ

(cid:19)
.

These quantities are all dimensionless. Dependence on σ arises because we bound ¯L ≤ L/σ2: our
algorithm’s dependence on ¯L implicitly enforces a low-rank constraint in this case. All of our analyses
give qualitatively similar results to this, albeit in more general settings allowing approximately
low-rank input.

To perform error analyses, we will need bounds on the norms of the matrices in our decomposition.

The following lemma gives the bounds we need for Section 6.

Lemma 5.2 (Norm bounds for even singular value transformation). Suppose the assumptions
from Theorem 5.1 hold. Then with probability at least 1 − δ, the event in Eq. (3) occurs (that is,

15By a dimension-independent or dimensionless quantity, we mean a quantity that is both independent of the size of

the input matrix and is scale-invariant, i.e., does not change under scaling A ← αA.

16This criterion is fairly reasonable. For example, the polynomials used in QSVT satisfy it.

28

R† ¯f (CC†)R ≈ f (A†A) − f (0)I) and moreover, the following bounds also hold:

(cid:107)R(cid:107) = O((cid:107)A(cid:107)) and

(cid:107) ¯f (CC†)(cid:107) ≤ max

(cid:110)

| ¯f (x)|

(cid:12)
(cid:12)
(cid:12) x ∈

(cid:107)R(cid:107)F = O((cid:107)A(cid:107)F),
min(r,c)
(cid:91)

[σ2

i − ¯ε, σ2

(cid:111)
,

i + ¯ε]

(5)

(6)

when ∗ = Op,

(cid:13)
¯f (CC†)
(cid:13)
(cid:13) ≤
Eq. (7) is typically a better bound than combining Eqs. (5) and (6). For intuition, notice this is
true if ε, ¯ε = 0: the left-hand and right-hand sides of the following inequality are the two ways to
bound (cid:107)R†(cid:112) ¯f (CC†)(cid:107)2, up to constant factors (σ below runs over the singular values of A):

(cid:107)f (A†A) − f (0)I(cid:107) + ε.

(cid:13)
(cid:13)R†
(cid:13)

(7)

(cid:113)

i=1

(cid:113)

(cid:107)f (A†A) − f (0)I(cid:107) ≤ max

σ

|f (σ2) − f (0)| ≤ max

σ

σ2 max
σ

|f (σ2) − f (0)|
σ2

= (cid:107)A(cid:107)2 max

σ

| ¯f (σ2)|.

The rest of this section will be devoted to proving Theorem 5.1 and Lemma 5.2. A mathematical tool
we will need is a matrix version of the deﬁning inequality of L-Lipschitz functions, |f (x) − f (y)| ≤
L|x − y| when f is L-Lipschitz. The Frobenius norm version of this bound (Lemma 5.3) follows by
computing matrix derivatives; the spectral norm version (Lemma 5.4) has a far less obvious proof.
Lemma 5.3 ([Gil10, Corollary 2.3]). Let A and B be Hermitian matrices and let f : R → C be
L-Lipschitz continuous on the eigenvalues of A and B. Then (cid:107)f (EV)(A) − f (EV)(B)(cid:107)F ≤ L(cid:107)A − B(cid:107)F.
Lemma 5.4 ([AP11, Theorem 11.2]). Let A and B be Hermitian matrices and let f : R → C be
L-Lipschitz continuous on the eigenvalues of A and B. Then

(cid:13)
(cid:13)
(cid:13)f (EV)(A) − f (EV)(B)
(cid:13)
(cid:13)
(cid:13)

(cid:46) L(cid:107)A − B(cid:107) log min(rank A, rank B).

Proof of Theorem 5.1 and Lemma 5.2. Since g(A†A) = f (A†A) + g(0)I for f (x) := g(x) − g(0), we
can assume without loss of generality that f (0) = 0. As a reminder, in the statement of Theorem 5.1
we take

(cid:16)
r = ˜Ω

φ2L2(cid:107)A(cid:107)2

∗(cid:107)A(cid:107)2
F

(cid:17)

1
ε2 log

1
δ

(cid:16)
c = ˜Ω

φ2 ¯L2(cid:107)A(cid:107)4(cid:107)A(cid:107)2

∗(cid:107)A(cid:107)2
F

1
ε2 log

1
δ

(cid:17)

.

These values are chosen such that the following holds with probability ≥ 1 − δ simultaneously.

1. The ith singular value of CC† does not diﬀer from the ith singular value of A†A by more than
∗ max(L−1, ¯L−1(cid:107)A(cid:107)−2).

¯ε. This follows from Lemma 4.9 with error parameter ε(cid:107)A(cid:107)−1
This immediately implies Eq. (6).

F (cid:107)A(cid:107)−1

2. (cid:107)R(cid:107)2 = O(cid:0)(cid:107)A(cid:107)2(cid:1). This is the spectral norm bound in Eq. (5) (the Frobenius norm bound

follows from Lemma 4.2). We use Lemma 4.8:

(cid:107)R(cid:107)2 ≤ (cid:107)A(cid:107)2 + (cid:107)R†R − A†A(cid:107) ≤ (cid:107)A(cid:107)2 +

ε(cid:107)A(cid:107)2
L(cid:107)A(cid:107)2
∗

= O((cid:107)A(cid:107)2).

3. (cid:107)f (R†R) − f (A†A)(cid:107)∗ = O(ε). We need the polylog factors in our number of samples to deal

with the log r that arises from Lemma 5.4 in the spectral norm case.

(cid:107)f (R†R) − f (A†A)(cid:107)∗
(cid:46) L(cid:107)R†R − A†A(cid:107)∗ log rank(R†R)

(Lemma 5.4 or Lemma 5.3)

(cid:114)

(cid:46) L

(cid:46) ε.

φ2 log r log(1/δ)
r

(cid:107)A(cid:107)∗(cid:107)A(cid:107)F log r

(Lemma 4.8 or Lemma 4.6)

(plugging in value for r)

29

4. (cid:107) ¯f (CC†) − ¯f (RR†)(cid:107)∗ = O(cid:0)ε/(cid:107)A(cid:107)2(cid:1). This follows similarly to the above point.

When all of the above bounds hold, we can conclude:

(cid:107)R† ¯f (CC†)R − f (A†A)(cid:107)∗
≤ (cid:107)R† ¯f (RR†)R − f (A†A)(cid:107)∗ + (cid:107)R†( ¯f (RR†) − ¯f (CC†))R(cid:107)∗
= (cid:107)f (R†R) − f (A†A)(cid:107)∗ + (cid:107)R†( ¯f (RR†) − ¯f (CC†))R(cid:107)∗
≤ (cid:107)f (R†R) − f (A†A)(cid:107)∗ + (cid:107)R(cid:107)2(cid:107) ¯f (RR†) − ¯f (CC†)(cid:107)∗
(cid:46) ε + (cid:107)R(cid:107)2ε/(cid:107)A(cid:107)2
(cid:46) ε.

(Deﬁnition of ¯f )

This gives Eq. (3) after rescaling ε by an appropriate constant factor. When ∗ = Op, we also have
Eq. (7), since

(cid:13)
(cid:13)R†
(cid:13)

(cid:113)

(cid:13)
¯f (CC†)
(cid:13)
(cid:13) =

(cid:113)

(cid:107)R† ¯f (CC†)R(cid:107) ≤

(cid:113)

(cid:107)f (A†A) − f (0)I(cid:107) + ε.

We remark here that the log term in Lemma 5.4 unfortunately cannot be removed (because some
Lipschitz functions are not operator Lipschitz). However, several bounds hold under various mild
assumptions, and for particular functions, the log term can be improved to log log or completely
removed. For example, the QSVT literature [GSLW19] cites the following result:

Lemma 5.5 ([AP10, Corollary 7.4]). Let A and B be Hermitian matrices such that aI (cid:22) A, B (cid:22) bI,
and let f : R → C be L-Lipschitz continuous on the interval [a, b]. Then

(cid:13)
(cid:13)f (EV)(A) − f (EV)(B)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:46) L(cid:107)A − B(cid:107) log

(cid:18)
e

b − a
(cid:107)A − B(cid:107)

(cid:19)
.

Though we will not use it, we can extend these results on eigenvalue transformation of Hermitian
matrices to singular value transformation of general matrices via the reduction from [GSLW19,
Corollary 21]. For example, Lemma 5.4 implies the following:

Lemma 5.6. Let A, B ∈ Cm×n be matrices and let f : [0, ∞) → C be L-Lipschitz continuous on the
singular values of A and B such that f (0) = 0. Then

(cid:107)f (SV)(A) − f (SV)(B)(cid:107) (cid:46) L(cid:107)A − B(cid:107) log min(rank A, rank B).

In Section 7, we prove results on generic singular value transformation and eigenvalue trans-
formation by bootstrapping Theorem 5.1. Since these are slower, though, we will use primarily
the even singular value transformation results that we just proved to recover “dequantized QML”
results. This will be the focus of next section.

6 Applying the framework to dequantizing QML algorithms

Now, with our framework, we can recover previous dequantization results: recommendation systems
(Section 6.2), supervised clustering (Section 6.3), principal component analysis (Section 6.4), low-
rank matrix inversion (Section 6.5), support-vector machines (Section 6.6), and low-rank semideﬁnite
programs (Section 6.8). We also propose new quantum-inspired algorithm for other applications,
including QSVT (Section 6.1), Hamiltonian simulation (Section 6.7), and discriminant analysis

30

(Section 6.9). We give applications in roughly chronological order; this also happens to be a rough
diﬃculty curve, with applications that follow more easily from our main results being ﬁrst.

Everywhere it occurs, K := (cid:107)A(cid:107)2

F/σ2, where A is the input matrix. κ := (cid:107)A(cid:107)2

2/σ2. For simplicity,
we will often describe our runtimes as if we know spectral norms of input matrices (so, for example,
If we do not know the spectral norm, we can run Lemma 4.9 repeatedly with
we know κ).
multiplicatively decreasing ε until we ﬁnd a constant factor upper bound on the spectral norm,
which suﬃces for our purposes. Alternatively, we can bound the spectral norm by the Frobenius
norm, which we know from sampling and query access to input.

6.1 Dequantizing QSVT

We begin by dequantizing the quantum singular value transformation described by Gily´en, Su, Low,
and Wiebe [GSLW19] for close-to-low-rank matrices.

Deﬁnition 6.1. For a matrix A ∈ Cm×n and p(x) ∈ C[x] degree-d polynomial of parity-d (i.e., even
if d is even and odd if d is odd), we deﬁne the notation p(QV)(A) in the following way:

1. If p is even, meaning that we can express p(x) = q(x2) for some polynomial q(x), then

p(QV)(A) := q(A†A) = p(

√

A†A).

2. If p is odd, meaning that we can express p(x) = x · q(x2) for some polynomial q(x), then

p(QV)(A) := A · q(A†A).

Theorem 6.2. Suppose we are given a matrix A ∈ Cm×n satisfying (cid:107)A(cid:107)F = 1 via the oracles for
SQ(A) with sq(A) = O(log(mn)), a vector SQ(b) ∈ Cn with (cid:107)b(cid:107) = 1 and sq(b) = O(log n), and a
degree-d polynomial p(x) of parity-d such that |p(x)| ≤ 1 for all x ∈ [−1, 1].

Then with probability ≥ 1 − δ, for ε a suﬃciently small constant, we can get SQφ(v) ∈ Cn such

that (cid:107)v − p(QV)(A)b(cid:107) ≤ ε(cid:107)p(QV)(A)b(cid:107) in poly
Speciﬁcally, for p even, the runtime is

(cid:16)

d,

1
(cid:107)p(QV)(A)b(cid:107)

ε , 1
, 1

δ , log mn

(cid:17)

time.

(cid:18)

(cid:101)O

d16(cid:107)A(cid:107)10
(ε(cid:107)p(QV)(A)b(cid:107))6

log3 1
δ

+

d12(cid:107)A(cid:107)8 + d6(cid:107)A(cid:107)2
(ε(cid:107)p(QV)(A)b(cid:107))4

log2 1
δ

(cid:19)

log(mn)

with

(cid:102)sq(v) = (cid:101)O

(cid:18)

d12(cid:107)A(cid:107)4
ε4(cid:107)p(QV)(A)b(cid:107)6

log(mn) log3 1
δ

(cid:19)

,

and for p odd, the runtime is

(cid:18)

(cid:101)O

d22(cid:107)A(cid:107)16
(ε(cid:107)p(QV)(A)b(cid:107))6

+

d16(cid:107)A(cid:107)12 + d10(cid:107)A(cid:107)4δ−1
(ε(cid:107)p(QV)(A)b(cid:107))4

(cid:19)

log(mn)

with

(cid:102)sq(v) = (cid:101)O

(cid:18)

d8
ε2δ(cid:107)p(QV)(A)b(cid:107)4

log(mn)

(cid:19)
.

31

From this result it follows that QSVT, as described in [GSLW19, Theorem 17], has no exponential
speedup when the block-encoding of A comes from a quantum-accessible “QRAM” data structure as
in [GSLW19, Lemma 50]. In the setting of QSVT, given A and b in QRAM, one can prepare |b(cid:105) and
construct a block-encoding for A/(cid:107)A(cid:107)F = A in polylog(mn) time. Then one can apply (quantum)
SVT by a degree-d polynomial on A and apply the resulting map to |b(cid:105) with d·polylog(mn) gates and
(cid:1)
ﬁnally project down to get the state |p(QV)(A)b(cid:105) with probability ≥ 1 − δ after Θ(cid:0)
log 1
1
δ
(cid:107)p(QV)(A)b(cid:107)
iterations of the circuit. So, getting a sample from |p(QV)(A)b(cid:105) takes Θ(cid:0)d
polylog(mn/δ)(cid:1)
time. This circuit gives an exact outcome, possibly with some log(1/ε) factors representing the
discretization error in truncating real numbers to ﬁnite precision (which we ignore, since we do not
account for them in our classical algorithm runtimes).

1
(cid:107)p(QV)(A)b(cid:107)

Analogously, by Remark 3.11, having A and b in (Q)RAM implies having SQ(A) and SQ(b) with
sq(A) = O(log mn) and sq(b) = O(log n). Since QSVT also needs to assume maxx∈[−1,1]|p(x)| ≤ 1,
the classical procedure matches the assumptions for QSVT. Our algorithm runs only polynomially
1
slower than the quantum algorithm, since the quantum runtime clearly depends on d,
,
(cid:107)p(QV)(A)b(cid:107)
and log(mn). We are exponentially slower in ε and δ (these errors are conﬂated for the quantum
algorithm). However, this exponential advantage vanishes if the desired output is not a quantum
state but some ﬁxed value (or an estimate of one). In that case, the quantum algorithm must
also pay 1
ε during the sampling or tomography procedures and the classical algorithm can boost a
constant success probability to ≥ 1 − δ, only paying a log 1
δ factor. Note that, unlike in the quantum
output, we can query entries of the output, which a quantum algorithm cannot do without paying
at least a 1

ε factor.

Theorem 6.2 also dequantizes QSVT for block-encodings of density operators when the density
operator comes from some well-structured classical data. Indeed, [GSLW19, Lemma 45] assumes
we can eﬃciently prepare a puriﬁcation of the density operator ρ. The rough classical analogue is
the assumption that we have sampling and query access to some A ∈ Cm×n with ρ = A†A. Since
Tr(ρ) = 1, we have (cid:107)A(cid:107)F = 1. Then, p(QV)(ρ) = r(QV)(A) for r(x) = p(x2) and (cid:107)ρ(cid:107) = (cid:107)A(cid:107)2, so we
can repeat the above argument to show the lack of exponential speedup for this input model too.
We can mimic the quantum algorithm with our techniques because low-degree polynomials are

smooth, in the sense that we formalize with the following lemma (proven in Appendix B).

Lemma 6.3. Consider p(x) a degree-d polynomial of parity-d such that |p(x)| ≤ 1 for x ∈ [−1, 1].
Recall that, for a function f : C → C, we deﬁne ¯f (x) := (f (x) − f (0))/x (and ¯f (0) = f (cid:48)(0) when f
is diﬀerentiable at zero).

• If p is even, then max
x∈[0,1]

|q(x)| ≤ 1, max

|q(cid:48)(x)| (cid:46) d2, max
x∈[−1,1]

|¯q(x)| (cid:46) d2, and max
x∈[−1,1]

|¯q(cid:48)(x)| (cid:46) d4.

x∈[−1,1]

• If p is odd, then max
x∈[−1,1]

|q(x)| (cid:46) d, max
x∈[−1,1]

|q(cid:48)(x)| (cid:46) d3, max
x∈[−1,1]

|¯q(x)| (cid:46) d3, and max
x∈[−1,1]

|¯q(cid:48)(x)| (cid:46) d5.

These bounds are tight for Chebyshev polynomials. In general, these bounds can be loose, so for

any particular QML application we recommend using our main results for faster algorithms.

Proof of Theorem 6.2. Consider the even case: take p(x) = q(x2) for q a degree-d/2 polynomial, so
p(QV)(A) = q(A†A), and we have the correct form to apply Theorem 5.1. q is uncontrolled outside
of [−1, 1], so we instead apply the singular value transformation which is constant outside of [−1, 1]:

f (x) :=






q(−1) x ≤ −1
q(x)
q(1)

−1 ≤ x ≤ 1
1 ≤ x

.

32

We can do this because the singular values of A lie in [0, 1], so q(A†A) = f (A†A). Then, by
Lemma 6.3, f and ¯f are Lipschitz with L = O(cid:0)d2(cid:1), ¯L = O(cid:0)d4(cid:1). So, by Theorem 5.1, we can get
R ∈ Cr×n and C ∈ Cr×c such that (cid:107)R† ¯f (CC†)R + f (0)I − f (A†A)(cid:107) ≤ ε, where

(cid:18)

r = (cid:101)O

d4(cid:107)A(cid:107)2(cid:107)A(cid:107)2
F

(cid:19)

1
ε2 log

1
δ

and

c = (cid:101)O

(cid:18)

d8(cid:107)A(cid:107)6(cid:107)A(cid:107)2
F

1
ε2 log

1
δ

(cid:19)
.

(We will later rescale ε; note that ε(cid:107)p(QV)(A)b(cid:107) (cid:46) L(cid:107)A(cid:107)2(cid:107)b(cid:107), so ε is small enough for the theorem
assumption.) This reduces the problem to approximating R† ¯f (CC†)Rb + f (0)b. We further approx-
(cid:17)
imate Rb ≈ u ∈ Cr such that (cid:107)Rb − u(cid:107) ≤ ε/d. Using Lemma 4.6, this needs O

ε2 log 1
F(cid:107)b(cid:107)2 d2
, using that (cid:107)R(cid:107)F (cid:46) (cid:107)A(cid:107)F
samples, which can be done in time O
(Eq. (5)) and sq(R†) = O(r sq(A)) (Lemma 4.3). This suﬃces to maintain the error bound because
(using Eqs. (6) and (7) and Lemma 6.3),

ε2 r log(mn) log 1

F(cid:107)b(cid:107)2 d2

(cid:107)A(cid:107)2

(cid:107)A(cid:107)2

(cid:16)

(cid:16)

(cid:17)

δ

δ

(cid:107)R† ¯f (CC†)(Rb − u)(cid:107) ≤

≤

(cid:13)
(cid:13)R†
(cid:13)
(cid:113)

(cid:113)

(cid:13)
¯f (CC†)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:113)

¯f (CC†)

(cid:107)f (A†A) − f (0)I(cid:107) + ε

(cid:13)
(cid:13)
(cid:13)(cid:107)Rb − u(cid:107)
ε
¯f (x)
d

max
x

(cid:113)

√

≤

2 + εd

ε
d

(cid:46) ε.

As a consequence, v := R† ¯f (CC†)u + f (0)b satisﬁes (cid:107)v − p(QV)(A)b(cid:107) ≤ ε. Via Lemma 3.6, we can
get SQφ(v) with

1
(cid:102)sq(v) = φ SQφ(v) log
δ
F(cid:107) ¯f (CC†)u(cid:107)2 + p(0)2(cid:107)b(cid:107)2
(cid:107)R(cid:107)2
(cid:16)

(cid:17)(cid:16)

(r + 1)

=
(cid:46) r2 (cid:107) ¯f (CC†)(cid:107)2((cid:107)Rb(cid:107) + (cid:107)Rb − u(cid:107))2 + p(0)2

(cid:107)v(cid:107)2

((cid:107)p(QV)(A)b(cid:107) − (cid:107)v − p(QV)(A)(cid:107))2

(r + 1) log(mn)

(cid:17)

log

1
δ

log(mn) log

1
δ

(cid:46) r2 d4(1 + ε/d)2 + 1
((cid:107)p(QV)(A)b(cid:107) − ε)2

log

(cid:46)

r2d4
(cid:107)p(QV)(A)b(cid:107)2

log(mn) log

.

1
δ
1
δ

In the last step, we use that ε (cid:46) (cid:107)p(QV)(A)b(cid:107); if we do not have that assumption, (cid:102)sq(v) (cid:46)
r2d4
(cid:107)v(cid:107)2 log(mn) log 1
δ . We rescale ε ← ε(cid:107)p(QV)(A)b(cid:107) to get the desired bound. The runtime is dominated
by ﬁnding C in O(rc log(mn)) time, computing ¯f (CC†) in O(cid:0)r2c(cid:1) time, and estimating R†b in
time. We also need to compute the matrix-vector product ¯f (CC†)u, but this
O
can be done in O(rc) time by instead multiplying through with the expression U ¯f (D2)U † = ¯f (CC†),
where U ∈ Cr×c comes from the SVD of C.

r d2
ε2 log(mn) log 1

(cid:16)

(cid:17)

δ

Now for the odd case: we could use Theorem 7.1 here, but we will continue to use Theorem 5.1
here. Similarly to the even case, we take g(x) to be q(x) in [−1, 1] and held constant ouside it, so
p(QV)(A) = A · g(A†A). Then, by plugging in the smoothness parameters from Lemma 6.3, we get
R, C such that (cid:107)R†¯g(CC†)R + g(0)I − g(A†A)(cid:107) < ε

(cid:107)A(cid:107) with probability ≥ 1 − δ where

(cid:18)

d6(cid:107)A(cid:107)4(cid:107)A(cid:107)2
F

r = (cid:101)O

(cid:19)

1
ε2 log

1
δ

(cid:18)

d10(cid:107)A(cid:107)8(cid:107)A(cid:107)2
F

c = (cid:101)O

1
ε2 log

1
δ

(cid:19)

.

We now use the approximating matrix product lemmas Lemmas 4.4 and 4.6 three times.

33

1. We use Lemma 4.4 to approximate AR† ≈ A(cid:48)R(cid:48)† such that (cid:107)AR† − A(cid:48)R(cid:48)†(cid:107) ≤ εd−2. We can
do this since we have SQ(A†) (by assumption) and SQ(R†), in O(cid:0)(cid:107)A(cid:107)2
Fd4ε−2δ−1(cid:1) =
O(cid:0)d4ε−2δ−1(cid:1) samples, with each sample costing O(r log(mn)) time (by Lemma 4.3). Then
using Lemma 5.2 and the bounds on |q(x)| and |¯q(x)| in Lemma 6.3,

F(cid:107)R(cid:107)2

(cid:107)(AR† − A(cid:48)R(cid:48)†)¯g(CC†)R(cid:107) ≤ (cid:107)AR† − A(cid:48)R(cid:48)†(cid:107)(cid:107)

(cid:113)

¯g(CC†)(cid:107)(cid:107)

(cid:113)

¯g(CC†)R(cid:107)

√

≤ (εd−2)

(cid:114)

d3

(cid:107)g(A†A) − g(0)I(cid:107) +

ε
(cid:107)A(cid:107)

(cid:46) ε.

2. We use Lemma 4.6 to approximate Rb ≈ u such that (cid:107)Rb − u(cid:107) ≤ ε
= O(cid:0)d4(cid:107)A(cid:107)2ε−2 log 1
δ

F(cid:107)b(cid:107)2 d4(cid:107)A(cid:107)2

(cid:1) samples.

log 1
δ

(cid:107)R(cid:107)2

O

(cid:16)

(cid:17)

ε2

d2(cid:107)A(cid:107) , where we use

(cid:107)A(cid:48)R(cid:48)†¯g(CC†)(Rb − u)(cid:107) ≤ (cid:107)AR†¯g(CC†)(Rb − u)(cid:107) + (cid:107)(A(cid:48)R(cid:48)† − AR†)¯g(CC†)Rb(cid:107)

(cid:46) (cid:107)A(cid:107)(cid:107)R†¯g(CC†)(cid:107)(cid:107)Rb − u(cid:107) + ε (cid:46) (cid:107)A(cid:107)d2(εd−2(cid:107)A(cid:107)−1) + ε (cid:46) ε.

3. Using SQ(b) and Lemma 4.4, we approximate Ab ≈ A(cid:48)(cid:48)b(cid:48)(cid:48) such that (cid:107)Ab − A(cid:48)(cid:48)b(cid:48)(cid:48)(cid:107) ≤ ε/d (and

consequently, q(0)(cid:107)Ab − A(cid:48)(cid:48)b(cid:48)(cid:48)(cid:107) ≤ ε) with O(cid:0)(cid:107)A(cid:107)2

F(cid:107)b(cid:107)2d2ε−2δ−1(cid:1) = O(cid:0)d2ε−2δ−1(cid:1) samples.

So, we have shown that v := A(cid:48)R(cid:48)†¯g(CC†)u + q(0)A(cid:48)(cid:48)b(cid:48)(cid:48) satisﬁes (cid:107)v − p(QV)(A)(cid:107) (cid:46) ε. v is a linear

combination of columns of A; via Lemma 3.6, we can get SQφ(v) with

(cid:102)sq(v) = φ SQφ(v) log

1
δ

(cid:32) (cid:80)

i (cid:107)A(cid:48)(·, i)(cid:107)2(cid:107)R(cid:48)(·, i)†¯g(CC†)u(cid:107)2 + (cid:80)

j q(0)2(cid:107)A(cid:48)(cid:48)(·, j)(cid:107)2(cid:107)b(cid:48)(cid:48)(j)(cid:107)2

= (cid:101)O



= (cid:101)O



(cid:107)A(cid:107)2

(cid:107)v(cid:107)2
d4ε−2δ−1 ((cid:107)¯g(CC†)R(cid:107)(cid:107)b(cid:107) + (cid:107)¯g(CC†)(cid:107)(cid:107)Rb − u(cid:107))2 + q(0)2 (cid:107)A(cid:107)2
((cid:107)p(QV)(A)b(cid:107) − (cid:107)p(QV)(A)b − v(cid:107))2

F(cid:107)R†(cid:107)2
F

F(cid:107)b(cid:107)2
d2ε−2δ−1

d8
ε4δ2 log(mn)





(cid:16) d4 + d2
ε2δ

(cid:17)2

(cid:33)

log(mn)

(cid:18) d−4(d2 + d3εd−2(cid:107)A(cid:107)−1)2 + 1
d8
ε2δ
((cid:107)p(QV)(A)b(cid:107) − ε)2
(cid:17)
d8(cid:107)p(QV)(A)b(cid:107)−2ε−2δ−1 log(mn)

(cid:16)

.

(cid:19)

log(mn)

= (cid:101)O

= (cid:101)O

Above, we used that ε (cid:46) (cid:107)p(QV)(A)b(cid:107) ≤ (cid:107)A(cid:107)(cid:107)q(A†A)(cid:107)(cid:107)b(cid:107) ≤ d(cid:107)A(cid:107). Now, we rescale ε ←
ε(cid:107)p(QV)(A)b(cid:107) to get the desired statement. The runtime is dominated by the sampling for C
in O(rc log(mn)) time, the computation of ¯g(CC†) in O(cid:0)r2c(cid:1) time, and the approximation of
AR† ≈ A(cid:48)R(cid:48)† in O(cid:0)rd4ε−2δ−1 log(mn)(cid:1) time.

Remark 6.4. Here, we make a brief remark about a technical detail we previously elided. Technically,
QSVT can use A† in QRAM instead of A (cf. [GSLW19, Lemma 50]), leaving open the possibility
that there is a quantum algorithm that does not give an exponential speedup when A is in QRAM,
but does when A† is in QRAM. We sketch an argument why this is impossible by showing that,
given SQ(A), we can simulate SQφ(B) (and SQφ(B†)) for B such that (cid:107)B − A†(cid:107) ≤ ε(cid:107)A(cid:107) with
probability ≥ 1 − δ. Unfortunately, this argument is fairly involved, so we defer it to Appendix A.

34

6.2 Recommendation systems

Our framework gives a simpler and faster variant of Tang’s dequantization [Tan19] of Kerenidis and
Prakash’s quantum recommendation systems [KP17]. Tang’s result is notable for being the ﬁrst
result in this line of work and for dequantizing what was previously believed to be the strongest
candidate for practical exponential quantum speedups for a machine learning problem [Pre18].

We want to ﬁnd a product j ∈ [n] that is a good recommendation for a particular user i ∈ [m],
given incomplete data on user-product preferences. If we store this data in a matrix A ∈ Rm×n with
sampling and query access, in the strong model described by Kerenidis and Prakash [KP17], ﬁnding
good recommendations reduces to the following:

Problem 6.5. For a matrix A ∈ Rm×n, given SQ(A) and a row index i ∈ [m], sample from ˆA(i, ·)
up to δ error in total variation distance, where (cid:107) ˆA − Aσ,η(cid:107)F ≤ ε(cid:107)A(cid:107)F.

Here, Aσ,η is a certain type of low-rank approximation to A. The standard notion of low-rank
approximation is that of Ar := (cid:80)r
i=1 σiU (·, i)V (·, i)†, which is the rank-r matrix closest to A in
spectral and Frobenius norms. Using singular value transformation, we deﬁne an analogous notion
thresholding singular values instead of rank.

Deﬁnition 6.6 (Aσ,η). We deﬁne Aσ,η as a singular value transform of A satisfying:

Aσ,η := P (SV)

σ,η

(A)

Pσ,η(λ)






= λ
= 0
∈ [0, λ]

λ ≥ σ(1 + η)
λ < σ(1 − η)
otherwise

.

Note that Pσ,η is not fully speciﬁed in the range [σ(1 − η), σ(1 + η)), so Aσ,η is any of a family of
matrices with error η.

For intuition, P (SV)

σ,η (A) is A for (right) singular vectors with value ≥ σ(1 + η), zero for those
with value < σ(1 − η), and something in between for the rest. Our analysis simpliﬁes the original
algorithm, which passes through the low-rank approximation guarantee of Frieze, Kannan, and
Vempala [FKV04].

Our algorithm uses that we can rewrite our target low-rank approximation as A · t(A†A), where
t is a smoothened projector. So, we can use our main theorem, Theorem 5.1, to approximate
t(A†A) by some R†U R. Then, the ith row of our low-rank approximation is A(i, ·)R†U R, which is a
product of a vector with an RUR decomposition. Thus, using the sampling techniques described in
Section 4.1, we have SQφ(A(i, ·)R†U R), so we can get the sample from this row as desired.

Corollary 6.7. Suppose 0 < ε (cid:46) (cid:107)A(cid:107)/(cid:107)A(cid:107)F and η ≤ 0.99. A classical algorithm can solve
Problem 6.5 in time

(cid:32)

(cid:101)O

K3κ5

η6ε6 log3 1

δ

+

K2κ(cid:107)A(i, ·)(cid:107)2
η2ε2(cid:107) ˆA(i, ·)(cid:107)2

log2 1
δ

(cid:33)

.

The assumption on ε is a weak non-degeneracy condition in the low-rank regime. For reference,
η = 1/6 in the application of this algorithm to recommendation systems. So, supposing the ﬁrst
term of the runtime dominates, the runtime is (cid:101)O
(cid:17)

, which improves on the previous

F(cid:107)A(cid:107)10
σ16ε6

log3 1
δ

(cid:16) (cid:107)A(cid:107)6

(cid:17)

(cid:17)

of [Tan19]. The quantum runtime for this problem is (cid:101)O

, up to

(cid:16) (cid:107)A(cid:107)24
σ24ε12 log3 1
F

δ

runtime (cid:101)O
polylog(m, n) terms [CGJ19, Theorem 27].

(cid:16) (cid:107)A(cid:107)F
σ

35

Proof. Note that Aσ,η = A · t(A†A), where t is the thresholding function shown below.

t(x) =


0

4ησ2 (x − (1 − η)2σ2)

1

1

x < (1 − η)2σ2
(1 − η)2σ2 ≤ x < (1 + η)2σ2
x ≥ (1 + η)2σ2

.

We will apply Theorem 5.1 with error parameter ε to get matrices R, C such that AR†¯t(CC†)R
satisﬁes

(cid:107)Aσ,1/6 − AR†¯t(CC†)R(cid:107)F ≤ (cid:107)A(cid:107)F(cid:107)t(A†A) − R†¯t(CC†)R(cid:107) ≤ ε(cid:107)A(cid:107)F.

(8)

Since t(x) is (4ησ2)−1-Lipschitz and t(x)/x is (4η(1 − η)2σ4)−1-Lipschitz, the sizes of r and c are

(cid:18)

L2(cid:107)A(cid:107)2(cid:107)A(cid:107)2
F

r = (cid:101)O

(cid:18)

c = (cid:101)O

¯L2(cid:107)A(cid:107)6(cid:107)A(cid:107)2
F

(cid:19)

(cid:19)

1
ε2 log
1
ε2 log

1
δ
1
δ

= (cid:101)O

= (cid:101)O

(cid:18) (cid:107)A(cid:107)2(cid:107)A(cid:107)2
F
σ4η2ε2
(cid:18) (cid:107)A(cid:107)6(cid:107)A(cid:107)2
F
σ8η2ε2

log

log

(cid:19)

(cid:19)

1
δ
1
δ

= (cid:101)O

= (cid:101)O

(cid:18) Kκ

η2ε2 log

(cid:18) Kκ3

η2ε2 log

(cid:19)
;

(cid:19)

.

1
δ
1
δ

So, it suﬃces to compute the SVD of an r × c matrix, which has a runtime of

(cid:18) K3κ5
η6ε6 log3 1

δ

(cid:19)
.

(cid:101)O

Next, we want to approximate AR† ≈ A(cid:48)R(cid:48)†. If we had SQ(A†) (in particular, if we could compute
column norms (cid:107)A(·, j)(cid:107)), we could do this via Lemma 4.6, and if we were okay with paying factors
of 1
δ , we could do this via Lemma 4.4. Here, we will instead implicitly deﬁne an approximation by
approximating each row [AR†](i, ·) = A(i, ·)R† via Lemma 4.6, since we then have SQ(A(i, ·)†) and
SQ(R†). With this proposition, we can estimate [AR†](i, ·) ≈ (A(i, ·)S†S)R† to ε√
(cid:107)A(i, ·)(cid:107)(cid:107)R†(cid:107)F =
K
ε(cid:107)A(i, ·)(cid:107)σ error using r(cid:48) := O(ε−2K log 1
δ ) samples17. Here, A(cid:48)(i, ·) := A(i, ·)S†S is our r(cid:48)-sparse
approximation, giving that
(cid:113)(cid:80)m

(cid:113)(cid:80)m

i=1 (cid:107)[AR†](i, ·) − [A(cid:48)R†](i, ·)(cid:107)2 ≤

i=1 ε2(cid:107)A(i, ·)(cid:107)2σ2 = εσ(cid:107)A(cid:107)F.

(9)

(cid:107)AR† − A(cid:48)R†(cid:107)F =

Using this and the observation that maxx ¯t(x) = (1 + η)−2σ−2 ≤ σ−2, we can bound the quality of
our ﬁnal approximation as

(cid:107) ˆA − Aσ,η(cid:107)F ≤ (cid:107)(A(cid:48)R† − AR†)¯t(CC†)R(cid:107)F + (cid:107)AR†¯t(CC†)R − Aσ,η(cid:107)F
(cid:113)

(cid:113)

≤ (cid:107)A(cid:48)R† − AR†(cid:107)F
≤ εσ(cid:107)A(cid:107)Fσ−1

√

(cid:13)
(cid:13)
(cid:13)

(cid:13)
¯t(CC†)
(cid:13)
(cid:13)
1 + ε + ε(cid:107)A(cid:107)F (cid:46) ε(cid:107)A(cid:107)F.

(cid:13)
(cid:13)
(cid:13)

¯t(CC†)R

(cid:13)
(cid:13)
(cid:13) + ε(cid:107)A(cid:107)F

by triangle inequality

by Eq. (8)

by Lemma 5.2 and Eq. (9)

We can sample from ˆA(i, ·) = A(cid:48)(i, ·)R† ¯f (CC†)R by naively computing x := A(cid:48)(i, ·)R† ¯f (CC†),
taking O(r(cid:48)r + rc) time. Then, we use Lemmas 3.5 and 3.6 to get a sample from xR with probability
≥ 1 − δ in O(cid:0)

(cid:1), where sqφ(xR) = O(r) and

(cid:102)sqφ(xR)(cid:1) time, which is O(cid:0)φ sqφ(xR) log 1
j=1|x(j)|2(cid:107)R(j, ·)(cid:107)2
(cid:107)xR(cid:107)2

φ = r

(cid:46) r

(cid:80)r

(cid:80)r

δ

j=1|x(j)|2(cid:107)A(cid:107)2
F
(cid:107) ˆA(i, ·)(cid:107)2r

=

(cid:107)x(cid:107)2(cid:107)A(cid:107)2
F
(cid:107) ˆA(i, ·)(cid:107)2

.

17Formally, to get a true approximation AR ≈ A(cid:48)R, we need to union bound the failure probability for each row,
paying a log m factor in runtime. However, we will ignore this consideration: our goal is to sample from one row, so
we only need to succeed in our particular row.

36

Then, using previously established bounds and bounds from Lemma 5.2, we have

(cid:107)x(cid:107)2(cid:107)A(cid:107)2
F
(cid:107) ˆA(i, ·)(cid:107)2

=

≤

(cid:107)A(cid:48)(i, ·)R†¯t(CC†)(cid:107)2(cid:107)A(cid:107)2
F
(cid:107) ˆA(i, ·)(cid:107)2
(cid:13)R†(cid:113)

¯t(CC†)

(cid:107)A(i, ·)(cid:107)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:16)

(cid:46) ((cid:107)A(i, ·)(cid:107)σ−1 + εσ(cid:107)A(i, ·)(cid:107)σ−2)2

(cid:46) (cid:107)A(i, ·)(cid:107)2(cid:107)A(cid:107)2
(cid:107) ˆA(i, ·)(cid:107)2σ2

F

.

(cid:113)

¯t(CC†)

(cid:13)
(cid:13) + (cid:107)A(i, ·)(cid:48)R† − A(i, ·)R†(cid:107)(cid:107)¯t(CC†)(cid:107)
(cid:13)
(cid:107)A(cid:107)2
F
(cid:107) ˆA(i, ·)(cid:107)2

(cid:17)2

(cid:107)A(cid:107)2
F
(cid:107) ˆA(i, ·)(cid:107)2

This sampling procedure and the SVD dominate the runtime. Since the sampling is exact, the only
error in total variation distance is the probability of failure.

Remark 6.8. This algorithm implicitly assumes that the important singular values are ≥ σ.
Without such an assumption, we can take σ = ε(cid:107)A(cid:107)F and η = 1/2, and have meaningful bounds on
the output matrix ˆA. Observe that, for p(x) = x(t(

x) − 1),

√

(cid:107)A · t(A†A) − A(cid:107) = (cid:107)p(SV)(A)(cid:107) ≤

3
2

ε(cid:107)A(cid:107)F.

(cid:16) (cid:107)A(cid:107)6
(cid:107)A(cid:107)6ε22 log3 1
F

So, our low-rank approximation output ˆA satisﬁes (cid:107) ˆA − A(cid:107) (cid:46) ε(cid:107)A(cid:107)F, with no assumptions on A,
time. This can be subsequently used to get SQφ( ˆA(i, ·)) = SQφ(ei ˆA) where
in (cid:101)O
(cid:107) ˆA(i, ·) − A(i, ·)(cid:107) (cid:46) ε(cid:107)A(cid:107)F (in a myopic sense, solving the same problem as Problem 6.5), or more
generally, any product of ˆA with a vector, in time independent of dimension.

(cid:17)

δ

6.3 Supervised clustering

The 2013 paper of Lloyd, Mohseni, and Rebentrost [LMR13] gives two algorithms for the machine
learning problem of clustering. The ﬁrst algorithm is a simple swap test procedure that was
dequantized by Tang [Tan21] (the second is an application of the quantum adiabatic algorithm
with no proven runtime guarantees). We will reproduce the algorithm from [Tan21] here: since the
dequantization just uses the inner product protocol, so it rather trivially ﬁts into our framework.
We have a dataset of points in Rd grouped into clusters, and we wish to classify a new data
point by assigning it to the cluster with the nearest average, aka centroid. We do this by estimating
the distance between the new point p ∈ Rd to the centroid of a cluster of points q1, . . . , qn−1 ∈ Rd,
namely, (cid:107)p − 1

n−1 (q1 + · · · + qn−1)(cid:107)2. This is equal to (cid:107)wM (cid:107)2, where


M :=




p/(cid:107)p(cid:107)
√

−q1/((cid:107)q1(cid:107)

n−1)

...

−qn−1/((cid:107)qn−1(cid:107)

√

n−1)


 ∈ Rn×d,


(cid:20)

w :=

(cid:107)p(cid:107),

√

(cid:107)q1(cid:107)
n − 1

, . . . ,

(cid:21)

(cid:107)qn−1(cid:107)
√
n − 1

∈ Rn.

Because the quantum algorithm assumes input in quantum states, we can assume sampling and
query access to the data points, giving the problem

Problem 6.9. Given SQ(M ) ∈ Rn×d, Q(w) ∈ Rn, approximate (wM )(wM )T to additive ε error
with probability at least 1 − δ.

Corollary 6.10 ([Tan21, Theorem 4]). There is a classical algorithm to solve Problem 6.9 in
O(cid:0)(cid:107)M (cid:107)4

(cid:1) time.

F(cid:107)w(cid:107)4 1

ε2 log 1

δ

37

Note that (cid:107)M (cid:107)2

F = 2 and (cid:107)w(cid:107)2 = (cid:107)p(cid:107)2 + 1
n−1

quadratically faster runtime of O(cid:0)(cid:107)M (cid:107)2

(cid:80)n−1

i=1 (cid:107)qi(cid:107)2. The quantum algorithm has a

(cid:1), ignoring polylog(n, d) factors [LMR13, Tan21].

F(cid:107)w(cid:107)2 1
ε

Proof. Recall our notation for the vector of row norms m := [(cid:107)M (1, ·)(cid:107), . . . , (cid:107)M (n, ·)(cid:107)] coming from
Deﬁnition 3.7. We can rewrite (wM )(wM )T as an inner product (cid:104)u, v(cid:105) where

u :=

v :=

n
(cid:88)

d
(cid:88)

n
(cid:88)

i=1

j=1

k=1

n
(cid:88)

d
(cid:88)

n
(cid:88)

i=1

j=1

k=1

M (i, j)(cid:107)M (k, ·)(cid:107)ei ⊗ ej ⊗ ek = M ⊗ m

wiwkM (j, k)
(cid:107)M (k, ·)(cid:107)

ei ⊗ ej ⊗ ek,

where u and v are three-dimension tensors. By ﬂattening u and v, we can represent them as two
vectors in R(n·d·n)×1. We clearly have Q(v) from queries to M and w. As for getting SQ(u) from
SQ(M ): to sample, we ﬁrst sample i according to m, sample j according to M (i, ·), and sample
k according to m; to query, compute ui,j,k = M (i, j)m(k). Finally, we can apply Lemma 4.12
F and (cid:107)v(cid:107) = (cid:107)w(cid:107)2, so estimating (cid:104)u, v(cid:105) to ε additive error with
to estimate (cid:104)u, v(cid:105). (cid:107)u(cid:107) = (cid:107)M (cid:107)2
probability at least 1 − δ requires O((cid:107)M (cid:107)4

F(cid:107)w(cid:107)4ε−2 log 1

δ ) samples.

6.4 Principal component analysis

Principal component analysis (PCA) is an important data analysis tool, ﬁrst proposed to be feasible
via quantum computation by Lloyd, Mohseni, and Rebentrost [LMR14]. Given copies of states with
density matrix ρ = X †X, the quantum PCA algorithm can prepare the state (cid:80) λi|vi(cid:105)(cid:104)vi| ⊗ |ˆλi(cid:105)(cid:104)ˆλi|,
where λi and vi are the eigenvalues and eigenvectors of X †X, and ˆλi are eigenvalue estimates (up to
additive error). See Prakash’s PhD thesis [Pra14, Section 3.2] for a full analysis and Chakraborty,
Gily´en, and Jeﬀery for a faster version of this algorithm in the block-encoding model [CGJ19].
Directly measuring the eigenvalue register is called spectral sampling, but such sampling is not
directly useful for machine learning applications.

Though we do not know how to dequantize this protocol exactly, we can dequantize it in the
low-rank setting, which is the only useful poly-logarithmic time application that Lloyd, Mohseni,
and Rebentrost [LMR14] suggests for quantum PCA.
Problem 6.11 (PCA for low-rank matrices). Given a matrix SQ(X) ∈ Cm×n such that X †X has
top k eigenvalues {λi}k
i=1, with probability ≥ 1 − δ, compute eigenvalue
estimates {ˆλi}k
i=1 such that
(cid:107)ˆvi − vi(cid:107) ≤ ε for all i.

i=1|ˆλi − λi| ≤ ε Tr(X †X) and eigenvectors {SQφ(ˆvi)}k

i=1 and eigenvectors {vi}k

i=1 such that (cid:80)k

Note that we should think of λi as σ2

i , where σi is the ith largest singular value of X. To robustly
avoid degeneracy conditions, our runtime must depend on parameters for condition number and
spectral gap:

K := Tr(X †X)/λk ≥ k and η := min
i∈[k]

|λi − λi+1|/(cid:107)X(cid:107)2.

(10)

We also denote κ := (cid:107)X(cid:107)2/λk. Dependence on K and η are necessary to reduce Problem 6.11
to spectral sampling. If K = poly(n), then λk = Tr(X †X)/ poly(n), so distinguishing λk from
λk+1 necessarily takes poly(n) samples, and even sampling λk once takes poly(n) samples. As a
result, learning vk is also impossible. A straightforward coupon collector argument (given e.g. by
Tang [Tan21]) shows that Problem 6.11 can be solved by a quantum algorithm performing spectral
sampling18, with runtime depending polynomially on K and 1
η . We omit this argument for brevity.

18The quantum analogue to SQ(X) is eﬃcient state preparation of X, a puriﬁcation of ρ.

38

Classically, we can solve this PCA problem with quantum-inspired techniques, as ﬁrst noted in
[Tan21].

Corollary 6.12. For 0 < ε (cid:46) η(cid:107)X(cid:107)2/(cid:107)X(cid:107)2
(cid:16) (cid:107)X(cid:107)4
λi(cid:107)X(cid:107)2 η−2ε−2 log2 1
F

time to get SQφ(ˆvi) where (cid:102)sq(ˆvi) = (cid:101)O

F, we can solve Problem 6.11 in (cid:101)O
(cid:17)

.

δ

(cid:16) (cid:107)X(cid:107)6
F
λ2

k(cid:107)X(cid:107)2 η−6ε−6 log3 k

δ

(cid:17)

(cid:16) (cid:107)X(cid:107)36
F

This improves signiﬁcantly over prior work [Tan21, Theorem 8], which achieves the runtime of
(cid:17)

(cid:101)O
time, up to factors of polylog(m, n) [CGJ19, Theorem 27].20

η−6ε−12 log3 k
δ

(cid:107)X(cid:107)12λ12
k

.19 The best quantum algorithm for this problem runs in (cid:101)O

(cid:16) (cid:107)X(cid:107)F(cid:107)X(cid:107)
λkε

(cid:17)

We approach the problem as follows. First, we use that an importance-sampled submatrix
of X has approximately the same singular values as X itself (Lemma 4.9) to get our estimates
{ˆλi}k
i=1. With these estimates, we can deﬁne smoothened step functions fi for i ∈ [k] such that
fi(X †X) = v†
i vi. We can then use our main theorem to ﬁnd an RUR decomposition for fi(X †X).
We use additional properties of the RUR description to argue that it is indeed a rank-1 outer
product ˆv†
i ˆvi, which is our desired approximation for the eigenvector. We have sampling and query
access to ˆvi because it is R†x for some vector x. Our runtime is quite good because these piecewise
linear step functions have relatively tame derivatives, as opposed to the thresholded inverse function,
whose Lipschitz constants must incur quadratic and cubic overheads in terms of condition number.

Proof. We will assume that we know λk and η. If both are unknown, then we can estimate them
with the singular value estimation procedure described below (Lemma 4.9).

Notice that η(cid:107)X(cid:107)2 ≤ λk follows from our deﬁnition of η. The algorithm will proceed as follows:

ﬁrst, consider C := SXT ∈ Cc×r as described in Theorem 5.1, with parameters

r := (cid:101)O

(cid:18) (cid:107)X(cid:107)2
F

η2(cid:107)X(cid:107)2ε2 log

k
δ

(cid:19)

c := (cid:101)O

(cid:18) (cid:107)X(cid:107)2
η2λ2

F(cid:107)X(cid:107)2
kε2

log

(cid:19)

.

k
δ

Consider computing the eigenvalues of CC†; denote the ith eigenvalue ˆλi. Since r, c = Ω( (cid:107)X(cid:107)2
by Lemma 4.9 with error parameter ε

, with probability ≥ 1 − δ,

√

F

λkε2 log 1
δ ),

λk
8(cid:107)X(cid:107)F

(cid:114)

(cid:88)min(m,n)
i=1

(ˆλi − λi)2 ≤

√

λk
ε
8(cid:107)X(cid:107)F

(cid:107)X(cid:107)2
F.

These ˆλi’s for i ∈ [k] have the desired property for eigenvalue estimates:

|ˆλi − λi| ≤

√

k

(cid:114)

(cid:88)k

i=1

k
(cid:88)

i=1

(ˆλi − λi)2 ≤ ε

(cid:112)

kλk(cid:107)X(cid:107)F ≤ ε(cid:107)X(cid:107)2
F.

19This runtime comes from taking εσ = εv = ε and changing the normalization of the gap parameter η =

η(cid:107)X(cid:107)2/(cid:107)X(cid:107)2

F to correspond to the problem as formulated here.

20Given X in QRAM, this follows from applying Theorem 27 to a quantum state with density matrix of X †X
with α = (cid:107)X(cid:107)F and ∆ = ε(cid:107)X(cid:107)2
(cid:46) η(cid:107)X(cid:107)
λi to ∆ error, which when squared is
. The output is some estimate of
(cid:107)X(cid:107)F
an estimate of λi to ∆(cid:107)X(cid:107) = ε(cid:107)X(cid:107)2
F error as desired. Then, the density matrix is a probability distribution over
eigenvectors with their corresponding eigenvalue estimate (which is enough to identify the eigenvector). The coupon
collector argument mentioned above gives us access to all the top k eigenvalues and eigenvectors by running this
algorithm (cid:107)X(cid:107)2

F/λk times [Tan21].

(cid:107)X(cid:107)

√

F

39

This bound also implies that, for all i, |ˆλi − λi| ≤ ε
tions fi for i ∈ [k], deﬁned

8 (cid:107)X(cid:107)2

F. Next, consider the eigenvalue transforma-

fi(x) :=


0
2 + 8

1
2 − 8




0

x − ˆλi < − 1

4 η(cid:107)X(cid:107)2
4 η(cid:107)X(cid:107)2 ≤ x − ˆλi < − 1
η(cid:107)X(cid:107)2 (x − ˆλi) − 1
8 η(cid:107)X(cid:107)2 ≤ x − ˆλi < 1
− 1
8 η(cid:107)X(cid:107)2 ≤ x − ˆλi < 1
1
4 η(cid:107)X(cid:107)2 ≤ x − ˆλi

η(cid:107)X(cid:107)2 (x − ˆλi)

1

8 η(cid:107)X(cid:107)2

8 η(cid:107)X(cid:107)2

.

4 η(cid:107)X(cid:107)2

8 η(cid:107)X(cid:107)2, zero when |x− ˆλi| ≥ 1

This is a function that is one when |x− ˆλi| ≤ 1
between them otherwise. From the eigenvalue gap and the aforementioned bound |ˆλi −λi| ≤ 1
we can conclude that fi(X †X) = viv†
R† ¯fi(CC†)R approximates viv†
The conditions of Theorem 5.1 are satisﬁed because ε (cid:46) 8 ≤ 8
of fi. The values of r, c are chosen so that (cid:107)R† ¯fi(CC†)R − fi(X †X)(cid:107) ≤ ε/2 (note fi(0) = 0):

4 η(cid:107)X(cid:107)2, and interpolates
8 η(cid:107)X(cid:107)2,
i exactly. Further, by Theorem 5.1, we can conclude that
i , with C, R the exact approximations used to estimate singular values.
η = L(cid:107)X(cid:107)2 for L the Lipschitz constant

(cid:18)

L2(cid:107)X(cid:107)2(cid:107)X(cid:107)2
F

(cid:18)

¯L2(cid:107)X(cid:107)6(cid:107)X(cid:107)2
F

r = (cid:101)O

c = (cid:101)O

(cid:19)

(cid:19)

1
ε2 log
1
ε2 log

1
δ

1
δ

= (cid:101)O

(cid:18) (cid:107)X(cid:107)2
F

(cid:107)X(cid:107)2η2ε2 log

1
δ

(cid:19)

(cid:32)

= (cid:101)O

(cid:107)X(cid:107)6(cid:107)X(cid:107)2
F

η2(cid:107)X(cid:107)4(ˆλi − 1

4 η(cid:107)X(cid:107)2)2ε2

(cid:33)

1
δ

log

= (cid:101)O

(cid:18) (cid:107)X(cid:107)2(cid:107)X(cid:107)2
F
η2λ2

kε2

log

(cid:19)

.

1
δ

Further, fi is chosen with respect to ˆλi such that R† ¯fi(CC†)R is rank one, since CC† has one
eigenvalue between ˆλi − 1
4 η(cid:107)X(cid:107)2. Thus, this approximation is an outer product,
R† ¯fi(CC†)R = ˆviˆv†
i , and we take the corresponding vector to be our eigenvector estimate: (cid:107)ˆvi(cid:107) ≤
(cid:112)1 + ε/2 ≤ 1 + ε/4, so

4 η(cid:107)X(cid:107)2 and ˆλi + 1

ε/2 ≥ (cid:107)(ˆviˆv†

i − viv†

i )vi(cid:107)
= (cid:107)(cid:104)ˆvi, vi(cid:105)ˆvi − vi(cid:107)
≥ (cid:107)ˆvi − vi(cid:107) − ((cid:104)ˆvi, vi(cid:105) − 1)(cid:107)ˆvi(cid:107)
≥ (cid:107)ˆvi − vi(cid:107) − ((cid:107)ˆvi(cid:107)(cid:107)vi(cid:107) − 1)(cid:107)u(cid:107)
≥ (cid:107)ˆvi − vi(cid:107) − (1 + ε/4 − 1)(1 + ε/4)
≥ (cid:107)ˆvi − vi(cid:107) − ε/2,

by deﬁnition
by (cid:107)vi(cid:107)2 = 1
by triangle inequality

by Cauchy–Schwarz
by (cid:107)ˆvi(cid:107) ≤ 1 + ε/4

which is the desired bound. By choosing failure probability δ/k, the bound can hold true for all k
with probability ≥ 1 − δ.

(cid:107)¯v†

Finally, we can get access to ˆvi = R†¯vi, where ¯vi ∈ Cr satisﬁes ¯v†
i (cid:107) ≤ (cid:112)maxx ¯fi(x) (cid:46) λ
(cid:80)r

, using Lemmas 3.5 and 3.6, we have SQφ(ˆvi) with

− 1
2
i

(cid:80)r

φ = r

s=1|ˆvi(s)|2(cid:107)R(s, ·)(cid:107)2
(cid:107)R†¯vi(cid:107)2

= r

s=1|ˆvi(s)|2(cid:107)X(cid:107)2
F
(cid:107)R†¯vi(cid:107)2r

=

(cid:107)ˆvi(cid:107)2(cid:107)X(cid:107)2
F
(cid:107)R†¯vi(cid:107)2

(cid:46) (cid:107)X(cid:107)2

F

λi(1 − ε)2

(cid:46) (cid:107)X(cid:107)2
λi

F

,

i ¯vi = ¯fi(CC†). Since

so (cid:102)sqφ(ˆvi) = φ sqφ(v) log 1

δ

(cid:46) (cid:107)X(cid:107)2
λi

F

r log 1
δ .

40

6.5 Matrix inversion and principal component regression

The low-rank matrix inversion algorithms given by Gily´en, Lloyd, and Tang [GLT18] and Chia, Lin,
and Wang [CLW18] dequantize Harrow, Hassidim, and Lloyd’s quantum matrix inversion algorithm
(HHL) [HHL09] in the regime where the input matrix is low-rank instead of sparse. The corresponding
quantum algorithm in this regime is given by Chakraborty, Gily´en, and Jeﬀery [CGJ19], among
others. Since sparse matrix inversion is BQP-complete, it is unlikely that one can eﬃciently
dequantize it. However, the variant of low-rank (non-sparse) matrix inversion appears often in
quantum machine learning [Pra14, WZP18, RML14, CD16, RL18], making it an inﬂuential primitive
in its own right.

Using our framework, we can elegantly derive the low-rank matrix inversion algorithm in a
manner similar to prior quantum-inspired work [GLT18, CLW18]. Moreover, we can also handle
the approximately low-rank regime and only invert the matrix on a well-conditioned subspace,
solving principal component regression—for more discussion see [GSLW19]. Namely, we can ﬁnd a
thresholded pseudoinverse of an input matrix:
Deﬁnition 6.13 (A+

σ,η to be any singular value transform of A satisfying:

σ,η). We deﬁne A+

A+

σ,η := tinv(SV)

σ,η (A)

tinvσ,η(λ)






= 1/λ
= 0
∈ [0, σ−1]

λ ≥ σ
λ < σ(1 − η)
otherwise

.

(11)

This deﬁnition is analogous to Aσ,η in Section 6.2:

it is A+ for singular vectors with value
≥ σ, zero for singular vectors with value ≤ σ(1 − η), and a linear interpolation between the two in
between.
Problem 6.14. Given SQϕ(A) ∈ Cm×n, Q(b) ∈ Cm, with probability ≥ 1 − δ, get SQφ(ˆx) such
that (cid:107)ˆx − x∗(cid:107) ≤ ε(cid:107)x∗(cid:107), where x∗ := A+
Corollary 6.15. For 0 < ε (cid:46) (cid:107)A(cid:107)2

σ,ηb.

(cid:17)

σ2 and η ≤ 0.99, we can solve Problem 6.14 in (cid:101)O
(cid:17)
.

(cid:16) ϕ4K2κ5
η2ε2

time to give SQφ(ˆx) for (cid:102)sqφ(ˆx) = (cid:101)O

(cid:107)x∗(cid:107)2
(cid:107)ˆx(cid:107)2 log2 1
This should be compared to [GLT18], which applies only to strictly rank-k A with ϕ = 1 and
(cid:17)
log3 1
gets the incomparable runtime of (cid:101)O
. The corresponding quantum algorithm using
δ
block-encodings takes O((cid:107)A(cid:107)F/σ) time, up to polylog(m, n) factors, to get this result for constant
η [GSLW19, Theorem 41].

(cid:16) K3κ8k6
η6ε6

δ

(cid:16) ϕ6K3κ11
η6ε6

log3 1
δ

If we further assume that ε < 0.99, then (cid:102)sqφ(ˆx) can be simpliﬁed, since (cid:107)x∗(cid:107)

(cid:107)x∗(cid:107)−ε(cid:107)x∗(cid:107) ≤ 100.
However, this algorithm also works for larger ε; namely, if we only require that (cid:107)ˆx − x∗(cid:107) ≤ εσ−1(cid:107)b(cid:107)
(a “worst-case” error bound), then this algorithm works with runtime smaller by a factor of κ3 (and
(cid:102)sqφ(ˆx) smaller by a factor of κ).

The algorithm comes from rewriting A+

σ,ηb = ι(A†A)A†b for ι a function encoding a thresholded
inverse. Namely, ι(x) = 1/x for x ≥ σ2, ι(x) = 0 for x ≤ (1 − η)σ2, and is a linear interpolation
between the endpoints for x ∈ [(1 − η)σ2, σ2]. By our main theorem, we can ﬁnd an RUR
decomposition for ι(A†A), from which we can then get SQ(R†U RA†b) via sampling techniques.

(cid:107)ˆx(cid:107) ≤

(cid:107)x∗(cid:107)

Proof. We will solve our problem for x∗ = A+

σ,ηb = ι(A†A)A†b where


0

ησ4 (x − σ2(1 − η)) σ2(1 − η) ≤ x < σ2

x < σ2(1 − η)

1

.

ι(x) :=



1
x

σ2 ≤ x

41

So, if we can estimate ι(A†A) such that (cid:107)ι(A†A) − R†¯ι(CC†)R(cid:107) ≤ ε

(cid:107)A(cid:107)2 , then as desired,

(cid:107)A+

σ,ηb − R†¯ι(CC†)RA†b(cid:107) ≤

ε
(cid:107)A(cid:107)

(cid:107)b(cid:107) ≤ ε(cid:107)A+

σ,ηb(cid:107).

By Theorem 5.1 with L = 1

1

η2(1−η)2σ6 , we can ﬁnd such R and C with

ησ4 and ¯L =
(cid:32)

ϕ2 (cid:107)A(cid:107)2(cid:107)A(cid:107)2
η2σ8 ε2
(cid:107)A(cid:107)4

F

r = (cid:101)O

(cid:33)

1
δ

= (cid:101)O

(cid:18) ϕ2Kκ3
η2ε2

log

(cid:19)

1
δ

log

(cid:32)

c = (cid:101)O

ϕ2

(cid:107)A(cid:107)6(cid:107)A(cid:107)2
F
η2(1 − η)2σ12 ε2
(cid:107)A(cid:107)4

(cid:33)

1
δ

= (cid:101)O

(cid:18) ϕ2Kκ5
η2ε2

log

(cid:19)
.

1
δ

log

Computing the SVD of a matrix of this size dominates the runtime, giving the complexity in the
theorem statement. Next, we would like to further approximate R†¯ι(CC†)RA†b. We will do this
by estimating RA†b by some vector u to εσ3(cid:107)A(cid:107)−1(cid:107)b(cid:107) = ε(cid:107)A(cid:107)2
2 error, since then, using
the bounds from Lemma 5.2,

F(cid:107)b(cid:107)K−1κ− 1

(cid:107)R†¯ι(CC†)RA†b − R†¯ι(CC†)u(cid:107) ≤

¯ι(CC†)

(cid:13)R†(cid:113)
(cid:13)
(cid:13)
(cid:46) (cid:113)
σ−2 + ε

(cid:113)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

¯ι(CC†)

(cid:13)
(cid:13)(cid:107)RA†b − u(cid:107)
(cid:13)

(cid:107)A(cid:107)2 σ−2(εσ3(cid:107)A(cid:107)−1(cid:107)b(cid:107)) (cid:46) ε(cid:107)A(cid:107)−1(cid:107)b(cid:107).

We use Remark 4.13 to estimate u(i) = R(i, ·)A†b, for all i ∈ [r], to ε(cid:107)R(i, ·)(cid:107)(cid:107)A(cid:107)F(cid:107)b(cid:107)K−1κ− 1
2 error,
log r
with probability ≥ 1 − δ/r. This takes O
samples for each of the r entries. This implies
δ
that ˆx := R†¯ι(CC†)u has the desired error and failure probability. Finally, we can use Lemmas 3.5
and 3.6 with matrix R† and vector ¯ι(CC†)u to get SQφ(ˆx) for

ϕ K2κ
ε2

(cid:16)

(cid:17)

(cid:80)r

φ = ϕr

s=1|[¯ι(CC†)u](s)|2(cid:107)R(s, ·)(cid:107)2
(cid:107)ˆx(cid:107)2
= ϕ2 (cid:107)¯ι(CC†)u(cid:107)2(cid:107)A(cid:107)2

F

(cid:107)ˆx(cid:107)2

≤ ϕ2 ((cid:107)¯ι(CC†)R(cid:107)(cid:107)A†(cid:107)(cid:107)b(cid:107) + (cid:107)¯ι(CC†)(cid:107)(cid:107)RA†b − u(cid:107))2(cid:107)A(cid:107)2

F

(cid:107)ˆx(cid:107)2

(cid:46) ϕ2 (σ−3(cid:107)A(cid:107)(cid:107)b(cid:107) + σ−4εσ3(cid:107)b(cid:107)/(cid:107)A(cid:107))2(cid:107)A(cid:107)2

F

(cid:107)ˆx(cid:107)2

(cid:46) ϕ2 σ−6(cid:107)A(cid:107)2(cid:107)b(cid:107)2(cid:107)A(cid:107)2

F

(cid:107)ˆx(cid:107)2
≤ ϕ2Kκ2 (cid:107)x∗(cid:107)2
(cid:107)ˆx(cid:107)2 ,

by (cid:107)R(s, ·)(cid:107) ≤ (cid:107)A(cid:107)F

(cid:112)ϕ/r

by linear algebra

by prior bounds

by ε (cid:46) (cid:107)A(cid:107)2/σ2

by (cid:107)A(cid:107)−1(cid:107)b(cid:107) ≤ (cid:107)x∗(cid:107)

so (cid:102)sqφ(ˆx) = φ sqφ(ˆx) log 1

δ = O

(cid:16)

rϕ2Kκ2 (cid:107)x∗(cid:107)2

(cid:107)ˆx(cid:107)2 log 1

δ

(cid:17)

.

6.6 Support vector machines

In this section, we use our framework to dequantize Rebentrost, Mohseni, and Lloyd’s quantum
support vector machine [RML14], which was previously noted to be possible by Ding, Bao, and

42

Huang [DBH21]. Mathematically, the support vector machine is a simple machine learning model
attempting to label points in Rm as +1 or −1. Given input data points x1, . . . , xm ∈ Rn and
their corresponding labels y ∈ {±1}m. Let w ∈ Rn and b ∈ R be the speciﬁcation of hyperplanes
separating these points. It is possible that no such hyperplane satisﬁes all the constraints. To
resolve this, we add a slack vector e ∈ Rm such that e(j) ≥ 0 for j ∈ [m]. We want to minimize the
squared norm of the residuals:

min
w,b

s.t.

+

(cid:107)w(cid:107)2
2

1
2
y(i)(wT xi + b) = 1 − e(i),

(cid:107)e(cid:107)2

γ
2

∀i ∈ [m].

The dual of this problem is to maximize over the Karush-Kuhn-Tucker multipliers of a Lagrange
function, taking partial derivatives of which yields the linear system

(cid:104) 0
(cid:126)1T
(cid:126)1 XX T +γ−1I

(cid:105)
α ] = (cid:2) 0
[ b

y

(cid:3),

(12)

where (cid:126)1 is the all-ones vector and X = {x1, . . . , xm} ∈ Cm×n. Call the above m + 1 × m + 1 matrix
F , and ˆF := F/ Tr(F ).

The quantum algorithm, given X and y in QRAM, outputs a quantum state | ˆF +
λ3ε3 polylog(mn)(cid:1) time. The quantum-inspired analogue is as follows.

(Deﬁnition 6.13) in (cid:101)O(cid:0) 1
Problem 6.16. Given SQ(X) ∈ Rm×n and SQ(y) ∈ Rm, for (cid:107) ˆF (cid:107) ≤ 1, output SQφ(v) ∈ Rm+1 such
that (cid:107)ˆx − ˆF +

y ](cid:107) with probability ≥ 1 − δ.

y ](cid:107) ≤ ε(cid:107) ˆF +

λ,0.01[ 0

y ](cid:105)

λ,η[ 0

λ,η[ 0

Note that we must assume (cid:107) ˆF (cid:107) ≤ 1; the quantum algorithm makes the same assumption21.
Another dequantization was reported in [DBH21], which, assuming X is strictly low-rank (with
minimum singular value σ), outputs a description of (XX T )+y that can be used to classify points.
This can be done neatly in our framework: express (XX T )+ (or, more generally, (XX T )+
σ,η) as
Xf (X T X)X T for the appropriate choice of f . Then, use Theorem 5.1 to approximate f (X T X) ≈
RT ZR and use Lemma 4.4 to approximate XRT ≈ CW T . This gives an approximate “CUC”
decomposition of the desired matrix, since Xf (X T X)X T ≈ XRT ZRX T ≈ CW T ZW CT , which we
can use for whatever purpose we like.

For our solution to Problem 6.16, though, we simply reduce to matrix inversion as described
in Section 6.5: we ﬁrst get SQφ( ˆF ), and then we apply Corollary 6.15 to complete. Section VI.C
of [DBH21] claims to dequantize this version, but gives no correctness bounds22 or runtime bounds
(beyond arguing it is polynomial in the desired parameters).
Corollary 6.17. For 0 < ε (cid:46) 1 and η ≤ 0.99, we can solve Problem 6.16 in (cid:101)O(cid:0)λ−28η−6ε−6 log3 1
time, where we get SQφ(v) for (cid:102)sqφ(v) = (cid:101)O(cid:0)λ−14η−2ε−4 log2( 1

δ ) log( m

δ )(cid:1).

(cid:1)

δ

The runtimes in the statement are not particularly tight, but we chose the form to mirror the

runtime of the QSVM algorithm, which similarly depends polynomially on 1

λ and 1
η .

Proof. Consider constructing SQϕ(K) ∈ Cm×m as follows. To query an entry K(i, j), we estimate
X(i, ·)X(j, ·)T to ε(cid:107)X(i, ·)(cid:107)(cid:107)X(j, ·)(cid:107) error. We deﬁne K(i, j) to be this estimate. Using Lemma 4.12,
(cid:1) time. q here refers to the number of times the query oracle is used, so
we can do this in O(cid:0) 1

ε2 log q

δ

21The algorithm as written in [RML14] assumes that (cid:107)F (cid:107) ≤ 1; we conﬁrmed with an author that this is a typo.
22The correctness of this dequantization is unclear, since the approximations performed in this section incur

signiﬁcant errors.

43

in total the subsequent algorithm will only have an errant query with probability ≥ 1 − δ. (q will
not appear in the runtime because it’s folded into a polylog term.) Then, we can take ˜K := xxT ,
where x ∈ Rm is the vector of row norms of X, since by Cauchy–Schwarz,

K(i, j) ≤ X(i, ·)X(j, ·)T + ε(cid:107)X(i, ·)(cid:107)(cid:107)X(j, ·)(cid:107) ≤ (1 + ε)(cid:107)X(i, ·)(cid:107)(cid:107)X(j, ·)(cid:107) = ˜K(i, j).

Since we have SQ(x) from SQ(X), we have SQ( ˜K) with sq( ˜K) = O(1) by Lemma 3.8. (cid:107) ˜K(cid:107)2
. We can trivially get SQ(L) for L := (cid:2) 0 (cid:126)1T
(1 + ε)2(cid:107)X(cid:107)4
(cid:126)1 γ−1I
with sq(L) = O(1). Our approximation to ˆF is

F, so we have SQϕ(K) for ϕ = (1 + ε)2 (cid:107)X(cid:107)4

F
(cid:107)K(cid:107)2
F

F =
(cid:3)

M :=

(cid:16)

1
Tr(F )

L +

(cid:104) 0 (cid:126)0T
(cid:126)0 K

(cid:105)(cid:17)
;

(cid:107)M − ˆF (cid:107) ≤

1
Tr(F )

(cid:107)K − XX T (cid:107)F ≤

1
Tr(F )

ε(cid:107)X(cid:107)2

F ≤ ε.

Using Lemma 3.9, we have SQϕ(cid:48)(M ) with

2((1 + ε)2 (cid:107)X(cid:107)4
F
(cid:107)K(cid:107)2
F

(cid:107)K(cid:107)2

F + (cid:107)L(cid:107)2
F)

ϕ(cid:48) =

Tr(F )2(cid:107)M (cid:107)2
F

(cid:46) (cid:107)X(cid:107)4
((cid:107)X(cid:107)2

F + γ−2m + 2m
F + mγ−1)2(cid:107)M (cid:107)2
F

(cid:46) 1

(cid:107)M (cid:107)2
F

,

√

where the last inequality uses that Tr(F ) ≥

m, which follows from (cid:107) ˆF (cid:107) ≤ 1:

1 = (cid:107) ˆF (cid:107)(cid:13)
(cid:2)
(cid:13)

0
√

(cid:126)1/

m

(cid:13) ≥ (cid:13)
(cid:3)(cid:13)

(cid:13) ˆF (cid:2)

0
√

(cid:126)1/

m

(cid:3)(cid:13)
(cid:13) ≥

√

m
Tr(F )

.

Note that we can compute Tr(F ) given SQ(X). So, applying Corollary 6.15, we can get the desired
SQφ(v) in runtime

(cid:18) ϕ6(cid:107)M (cid:107)6

F(cid:107)M (cid:107)22

λ28η6ε6

(cid:19)

log3 1
δ

(cid:18)

(cid:46) (cid:101)O

(cid:101)O

(cid:107)M (cid:107)22

Fλ28η6ε6 log3 1

δ

(cid:107)M (cid:107)6

(cid:19)

(cid:18)

(cid:46) (cid:101)O

1

λ28η6ε6 log3 1

δ

(cid:19)
.

Here, we used that (cid:107)M (cid:107) ≤ (cid:107)M (cid:107)F (cid:46) 1, which we know since ϕ(cid:48) ≥ 1 (by our deﬁnition of oversampling
(cid:1) does not aﬀect the runtime, since the dominating cost
and query access). That Q(M ) = O(cid:0) 1
is still the SVD. On the other hand, this does come into play for the runtime for sampling:

ε2 log q

δ

(cid:102)sqφ(v) = (cid:101)O

(cid:18) ϕ4(cid:107)M (cid:107)4
η2ε2

F(cid:107)M (cid:107)10

log2 (cid:0) 1
δ

ε2 log (cid:0) m
(cid:1) 1

δ

(cid:19)

(cid:1)

.

We take q = m to guarantee that all future queries will be correct with probability ≥ 1 − δ.

The normalization used by the quantum and quantum-inspired SVM algorithms means that
these algorithms fail when X has too small Frobenius norm, since then the singular values from
XX T are all ﬁltered out. In Appendix B, we describe an alternative method that relies less on
normalization assumptions, instead simply computing F +. This is possible if we depend on (cid:107)X(cid:107)2
Fγ
in the runtime. Recall from Eq. (12) that we regularize by adding γ−1I, so γ−1 acts as a singular
value lower bound and (cid:107)X(cid:107)2

Fγ implicitly constrains.

Corollary 6.18. Given SQ(X T ) and SQ(y), with probability ≥ 1 − δ, we can output a real
number ˆb such that |b − ˆb| ≤ ε(1 + b) and SQφ(ˆα) such that (cid:107)ˆα − α(cid:107) ≤ εγ(cid:107)y(cid:107), where α and b
come from Eq. (12). Our algorithm runs in (cid:101)O(cid:0)(cid:107)X(cid:107)6
(cid:1) time, with (cid:102)sqφ(ˆα) =
F(cid:107)X(cid:107)16γ11ε−6 log3 1
δ
. Note that when γ−1/2 is chosen to be suﬃciently large (e.g.
(cid:101)O
O((cid:107)X(cid:107)F)) and (cid:107)α(cid:107) = Ω(γ(cid:107)y(cid:107)), this runtime is dimension-independent.

F(cid:107)X(cid:107)6γ5 γ2m

(cid:107) ˆα(cid:107)2 ε−4 log2 1

(cid:107)X(cid:107)4

(cid:16)

(cid:17)

δ

Notice that εγ(cid:107)y(cid:107) is the right notion, since γ is an upper bound on the spectral norm of the
inverse of the matrix in Eq. (12). We assume SQ(X T ) instead of SQ(X) for convenience, though
both are possible via the observation that f (XX T ) = X ¯f (X T X)X T .

44

6.7 Hamiltonian simulation

The problem of simulating the dynamics of quantum systems was the original motivation for quantum
computers proposed by Feynman [Fey82]. Speciﬁcally, given a Hamiltonian H, a quantum state |ψ(cid:105),
a time t > 0, and a desired error ε > 0, we ask to prepare a quantum state |ψt(cid:105) such that

(cid:107)|ψt(cid:105) − eiHt|ψ(cid:105)(cid:107) ≤ ε.

This problem, known as Hamiltonian simulation, sees wide application, including in quantum physics
and quantum chemistry. A rich literature has developed on quantum algorithms for Hamiltonian
simulation [Llo96, ATS03, BCK15], with an optimal quantum algorithm for simulating sparse
Hamiltonians given in [LC17]. In this subsection, we apply our framework to develop classical
algorithms for Hamiltonian simulation. Speciﬁcally, we ask:

Problem 6.19. Consider a Hermitian matrix H ∈ Cn×n, a unit vector b ∈ Cn, and error parameters
ε, δ > 0. Given SQ(H) and SQ(b), output SQφ(ˆb) with probability ≥ 1 − δ for some ˆb ∈ Cn satisfying
(cid:107)ˆb − eiH b(cid:107) ≤ ε.

We give two algorithms that are fundamentally the same, but operate in diﬀerent regimes: the

ﬁrst works for low-rank H, and the second for arbitrary H.

Corollary 6.20. Suppose H has minimum singular value σ and ε < min(0.5, σ). We can solve
Problem 6.19 in (cid:101)O

time, giving SQφ(ˆb) with (cid:102)sqφ(ˆb) = (cid:101)O

F(cid:107)H(cid:107)16
σ16ε6

F(cid:107)H(cid:107)8
σ8ε4

log3 1
δ

log3 1
δ

(cid:16) (cid:107)H(cid:107)6

(cid:16) (cid:107)H(cid:107)4

(cid:17)

(cid:17)

.

(cid:17)

(cid:16) (cid:107)H(cid:107)6

F(cid:107)H(cid:107)10
σ16ε6

This runtime is dimensionless in a certain sense. The natural error bound to require is that
(cid:107)ˆb − eiH b(cid:107) ≤ ε(cid:107)H(cid:107), since | d
dx (e−i(cid:107)H(cid:107)x)| = (cid:107)H(cid:107). So, if we rescale ε to ε(cid:107)H(cid:107), the runtime is
log3 1
(cid:101)O
, which is dimensionless. The runtime of the algorithm in the following corollary
δ
does not have this property, so its scaling with (cid:107)H(cid:107) is worse, despite being faster for, say, (cid:107)H(cid:107) = 1.
Corollary 6.21. For ε < min(0.5, (cid:107)H(cid:107)3), we can solve Problem 6.19 in (cid:101)O(cid:0)(cid:107)H(cid:107)16(cid:107)H(cid:107)6
time, giving SQφ(ˆb) with (cid:102)sqφ(ˆb) = (cid:101)O(cid:0)(cid:107)H(cid:107)8(cid:107)H(cid:107)4

Fε−4 log3 1
δ

Fε−6 log3 1
δ

(cid:1).

(cid:1)

Our strategy proceeds as follows: consider a generic function f (x) and Hermitian H. We can
write f (x) as a sum of an even function a(x) := 1
2 (f (x) + f (−x)) and an odd function b(x) :=
1
2 (f (x) − f (−x)). For the even function, we can use Theorem 5.1 to approximate it via the function
fa(x) := a(
x); the odd function can be written as H times an even function, which we approximate
x. In other words, f (H) = fa(H †H) + fb(H †H)H. Since
using Theorem 5.1 for fb(x) := b(
|a(cid:48)(x)|, |b(cid:48)(x)| ≤ |f (cid:48)(x)|, the Lipschitz constants don’t blow up by splitting f into even and odd
parts.

x)/

√

√

√

Now, we specialize to Hamiltonian simulation. We ﬁrst rewrite the problem, using the function

sinc(x) := sin(x)/x.

eiH b = cos(H)b + i · sinc(H)Hb = fcos(H †H)b + fsinc(H †H)Hb,
√

√

where fcos(λ) := cos(

λ) and fsinc(λ) := i · sinc(

λ). When applying Theorem 5.1 on fcos and fsinc,

45

we will use the following bounds on the smoothness of fcos and fsinc.

|f (cid:48)

cos(x)| =

| ¯f (cid:48)

cos(x)| =

|f (cid:48)

sinc(x)| =

| ¯f (cid:48)

sinc(x)| =

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

√
sin(
√
2

x)
x
2 − 2 cos(

√

x cos(

√

√

2

x +

√

(cid:12)
(cid:12)
(cid:12) ≤ min
√
x) −
2x2
x) − sin(
2x3/2
x cos(

√

2x5/2

(cid:17)

,

(cid:16) 1
1
√
2
x
2
√
√
x sin(

x)

(cid:12)
(cid:12)
(cid:12) ≤ min
(cid:16) 1
1
x
4

,

(cid:17)

√

x)

(cid:12)
(cid:12)
(cid:12) ≤ min
√
x)

x) − 3 sin(

(cid:16) 1
24

(cid:17)

,

5
2x3/2

(cid:12)
(cid:12)
(cid:12) ≤ min

(cid:17)

(cid:16) 1
60

,

3
x2

We separate these bounds into the case where x ≥ 1, which we use when we assume H has a
minimum singular value, and the case where x < 1, which we use for arbitrary H.

Proof of Corollary 6.21. Using the Lipschitz bounds above with Theorem 5.1, we can ﬁnd Rcos ∈
Crcos×n, Ccos ∈ Crcos×ccos, Rsinc ∈ Crsinc×n, Csinc ∈ Crsinc×csinc such that

cos

¯fcos(CcosC†

(cid:107)R†
¯fsinc(CsincC†

cos)Rcos + I − fcos(H †H)(cid:107) ≤ ε
sinc)Rsinc + i · I − fsinc(H †H)(cid:107) ≤

(cid:107)R†

sinc

ε
(cid:107)H(cid:107)

(13)

(14)

where, using that our Lipschitz constants are all bounded by constants,

rcos = (cid:101)O

rsinc = (cid:101)O

(cid:18)

(cid:107)H(cid:107)2

F(cid:107)H(cid:107)2ε−2 log

(cid:18)

(cid:107)H(cid:107)2

F(cid:107)H(cid:107)4ε−2 log

(cid:19)

(cid:19)

1
δ
1
δ

(cid:18)

(cid:18)

ccos = (cid:101)O

csinc = (cid:101)O

(cid:107)H(cid:107)2

F(cid:107)H(cid:107)6ε−2 log

(cid:107)H(cid:107)2

F(cid:107)H(cid:107)8ε−2 log

(cid:19)

(cid:19)
.

1
δ
1
δ

As a consequence,
(cid:13)
(cid:16)
(cid:13)eiH b −
(cid:13)

R†

cos

¯fcos(CcosC†

cos)Rcosb + b + R†

sinc

¯fsinc(CsincC†

sinc)RsincHb + iHb

(cid:17)(cid:13)
(cid:13)
(cid:13)

(cid:46) ε.

Note that, by Lemma 5.2, (cid:107)Rcos(cid:107) (cid:46) (cid:107)H(cid:107), (cid:107) ¯fcos(CcosC†
same bounds hold for the sinc analogues. We now approximate using Lemma 4.6 four times.

cos)(cid:107) (cid:46) 1, and (cid:107)R†

¯fcos(CcosC†

cos)(cid:107) (cid:46) 1; the

cos

(cid:113)

1. We approximate Rcosb ≈ u to ε(cid:107)b(cid:107) error, requiring O(cid:0)(cid:107)H(cid:107)2
2. We approximate RsincH ≈ W C to ε error, requiring O(cid:0)(cid:107)H(cid:107)4
3. We approximate Cb ≈ v to ε(cid:107)H(cid:107)−1
4. We approximate Hb ≈ R†w to ε(cid:107)b(cid:107) accuracy, requiring r := O(cid:0)(cid:107)H(cid:107)2

Fε−2 log 1
δ
Fε−2 log 1
δ
Fε−2 log 1
δ
Fε−2 log 1
δ

F (cid:107)b(cid:107) error, requiring O(cid:0)(cid:107)H(cid:107)4

(cid:1) samples.
(cid:1) samples.

(cid:1) samples.

(cid:1) samples.

Our output will be

ˆb := R†

cos

¯fcos(CcosC†

cos)u + b + R†

sinc

¯fsinc(CsincC†

sinc)W v + iR†w,

which is close to eiH b because
(cid:16)

(cid:13)
ˆb −
(cid:13)
(cid:13)
≤ (cid:107)R†

R†

¯fcos(CcosC†

cos
¯fcos(CcosC†

cos)Rcosb + b + R†
cos)(u − Rcosb)(cid:107) + (cid:107)R†

sinc

¯fsinc(CsincC†

sinc)RsincHb + iHb

(cid:17)(cid:13)
(cid:13)
(cid:13)

cos
+ (cid:107)R†

sinc
sinc)W (Cb − v)(cid:107) + (cid:107)iR†w − iHb(cid:107)
(cid:46) (cid:107)u − Rcosb(cid:107) + (cid:107)RsincH − W C(cid:107)(cid:107)b(cid:107) + (cid:107)H(cid:107)F(cid:107)Cb − v(cid:107) + (cid:107)R†w − Hb(cid:107) ≤ 4ε(cid:107)b(cid:107).

¯fsinc(CsincC†

sinc

¯fsinc(CsincC†

sinc)(RsincH − W C)b(cid:107)

46

Now, we have expressed ˆb as a linear combination of a small number of vectors, all of which
we have sampling and query access to. We can complete using Lemmas 3.5 and 3.6, where
the matrix is the concatenation (R†
sinc | i · R†), and the vector is the concatenation
( ¯fcos(CcosC†
sinc)W v | w). The length of this vector is rcos + 1 + rsinc + r (cid:46) rsinc.
We get SQφ(ˆb) where

cos)u | 1 | ¯fsinc(CsincC†

cos | b | R†

φ (cid:46) rsinc

(cid:107) ¯fcos(CcosC†

cos)u(cid:107)2 + (cid:107)b(cid:107)2 +

(cid:16) (cid:107)H(cid:107)2
F
rcos
(cid:107)H(cid:107)2

(cid:46)

(cid:16) rsinc
rcos
= (cid:101)O(cid:0)(cid:107)H(cid:107)2

F(1 + ε)2(cid:107)b(cid:107)2 + rsinc(cid:107)b(cid:107)2 + (cid:107)H(cid:107)2

F(cid:107)H(cid:107)2 + rsinc + (cid:107)H(cid:107)2

F + (cid:107)H(cid:107)2

F(cid:107)H(cid:107)4(cid:1) = (cid:101)O(rsinc).

(cid:107) ¯fsinc(CsincC†

(cid:107)H(cid:107)2
F
rsinc
F(1 + ε)2(cid:107)b(cid:107)2 +

sinc)W v(cid:107)2 +
rsinc
r

(cid:107)H(cid:107)2

(cid:107)H(cid:107)2
F
r
F(cid:107)b(cid:107)2(cid:17)
(cid:107)b(cid:107)−2

(cid:107)w(cid:107)2(cid:17)

(cid:107)ˆb(cid:107)−2

In the second inequality, we use the same bounds for proving (cid:107)ˆb − eiH b(cid:107) ≤ ε, repurposed to argue
that all approximations are suﬃciently close to the values they are estimating, up to relative error.
So, (cid:102)sqφ(ˆb) = (cid:101)O(cid:0)r2
Proof of Corollary 6.20. Our approach is the same, though with diﬀerent parameters. For The-
orem 5.1, we use that in the interval [σ2/2, ∞), fcos has Lipschitz constants of L = O(1/σ) and
¯L = O(1/σ3) and fsinc has L = O(1/σ2) and ¯L = O(1/σ4). So, if we take

sinc log 1
δ

(cid:1).

rcos = (cid:101)O

rsinc = (cid:101)O

F

(cid:18)
(cid:107)H(cid:107)2 (cid:107)H(cid:107)2
σ2 ε−2 log
(cid:18)
F(cid:107)H(cid:107)2
(cid:107)H(cid:107)2 (cid:107)H(cid:107)2
σ4

(cid:19)

1
δ

ε−2 log

ccos = (cid:101)O

csinc = (cid:101)O

(cid:18)

(cid:18)

F(cid:107)H(cid:107)4
(cid:107)H(cid:107)2 (cid:107)H(cid:107)2
σ6
F(cid:107)H(cid:107)6
(cid:107)H(cid:107)2 (cid:107)H(cid:107)2
σ8

ε−2 log

ε−2 log

(cid:19)

(cid:19)
,

1
δ
1
δ

(cid:19)

1
δ

all the conditions of Theorem 5.1 are satisﬁed: in particular, σ2/2 > ¯ε in both cases, up to rescaling
ε by a constant factor:

¯εcos (cid:46) (cid:107)H(cid:107)(cid:107)H(cid:107)F

¯εsinc (cid:46) (cid:107)H(cid:107)(cid:107)H(cid:107)F

εσ
(cid:107)H(cid:107)(cid:107)H(cid:107)F
εσ2
(cid:107)H(cid:107)2(cid:107)H(cid:107)F

= εσ ≤ σ2

= εσ2(cid:107)H(cid:107)−1 ≤ σ2

Here, we used our initial assumption that ε ≤ σ. So, the bounds Eqs. (13) and (14) hold. Note that,
by Lemma 5.2, (cid:107)Rcos(cid:107) (cid:46) (cid:107)H(cid:107), (cid:107) ¯fcos(CcosC†
bounds hold for the sinc analogues. We now approximate using Lemma 4.6 four times.

cos)(cid:107) (cid:46) σ−2, and (cid:107)R†

cos)(cid:107) ≤ 1; the same

¯fcos(CcosC†

(cid:113)

cos

1. We approximate Rcosb ≈ u to εσ(cid:107)b(cid:107) error, requiring O(cid:0)(cid:107)H(cid:107)2
2. We approximate RsincH ≈ W C to εσ error, requiring O(cid:0)(cid:107)H(cid:107)4

Fσ−2ε−2 log 1
δ
Fσ−2ε−2 log 1
δ

(cid:1) samples.
(cid:1) samples.

3. We approximate Cb ≈ v to εσ(cid:107)H(cid:107)−1
4. We approximate Hb ≈ R†w to ε(cid:107)b(cid:107) accuracy, requiring r := O(cid:0)(cid:107)H(cid:107)2

F (cid:107)b(cid:107) error, requiring O(cid:0)(cid:107)H(cid:107)4

Fσ−2ε−2 log 1
δ
Fε−2 log 1
δ

(cid:1) samples.
(cid:1) samples.

Our output will be

ˆb := R†

cos

¯fcos(CcosC†

cos)u + b + R†

sinc

¯fsinc(CsincC†

sinc)W v + iR†w,

47

which is close to eiH b by the argument

(cid:16)

(cid:13)
ˆb −
(cid:13)
(cid:13)
≤ (cid:107)R†

R†

¯fcos(CcosC†

cos
¯fcos(CcosC†

cos)Rcosb + b + R†
cos)(u − Rcosb)(cid:107) + (cid:107)R†

sinc

¯fsinc(CsincC†

sinc)RsincHb + iHb

(cid:17)(cid:13)
(cid:13)
(cid:13)

¯fsinc(CsincC†

sinc)(RsincH − W C)b(cid:107)

cos
+ (cid:107)R†

¯fsinc(CsincC†

sinc

sinc
sinc)W (Cb − v)(cid:107) + (cid:107)iR†w − iHb(cid:107)

(cid:46) σ−1(cid:107)u − Rcosb(cid:107) + σ−1(cid:107)RsincH − W C(cid:107)(cid:107)b(cid:107) + σ−1(cid:107)H(cid:107)F(cid:107)Cb − v(cid:107) + (cid:107)R†w − Hb(cid:107) ≤ 4ε(cid:107)b(cid:107)

Now, we have expressed ˆb as a linear combination of a small number of vectors, all of which
we have sampling and query access to. We can complete using Lemmas 3.5 and 3.6, where
the matrix is the concatenation (R†
sinc | i · R†), and the vector is the concatenation
( ¯fcos(CcosC†
sinc)W v | w). The length of this vector is rcos + 1 + rsinc + r (cid:46) rsinc.
We get SQφ(ˆb) where

cos)u | 1 | ¯fsinc(CsincC†

cos | b | R†

φ (cid:46) rsinc

(cid:107) ¯fcos(CcosC†

cos)u(cid:107)2 + (cid:107)b(cid:107)2 +

Fσ−2(cid:107)b(cid:107)2 + rsinc(cid:107)b(cid:107)2 + (cid:107)H(cid:107)2

(cid:16) (cid:107)H(cid:107)2
F
rcos
(cid:107)H(cid:107)2

(cid:46)

(cid:16) rsinc
rcos
= (cid:101)O(cid:0)(cid:107)H(cid:107)2

(cid:107)H(cid:107)2
F
rsinc
Fσ−2(cid:107)b(cid:107)2 +

(cid:107) ¯fsinc(CsincC†
rsinc
r

(cid:107)H(cid:107)2

sinc)W v(cid:107)2 +
F(cid:107)b(cid:107)2(cid:17)
(cid:18)

(cid:107)b(cid:107)−2

(cid:107)H(cid:107)2
F
r

(cid:107)w(cid:107)2(cid:17)

(cid:107)ˆb(cid:107)−2

t2(cid:107)H(cid:107)2
F
σ4

(cid:19)

.

F(cid:107)H(cid:107)2σ−4 + rsinc + (cid:107)H(cid:107)2

Fσ−2 + (cid:107)H(cid:107)4σ−4(cid:107)H(cid:107)2
F

(cid:1) = (cid:101)O

rsinc +

F(cid:107)H(cid:107)2σ−4) log 1
δ

(cid:1). Since ε < σ, the r2

So, (cid:102)sqφ(ˆb) = (cid:101)O(cid:0)rsinc(rsinc + (cid:107)H(cid:107)2
Remark 6.22. In the case where H is not low-rank, we could still run a modiﬁed version of
Corollary 6.20 to compute a modiﬁed “expσ,η(iH)” where singular values below σ are smoothly
thresholded away. Following the same logic as Deﬁnition 6.13, we could redeﬁne fcos such that
λ) for x ≥ σ2, and is a linear interpolation between
fcos(x) = 1 for x < σ2(1 − η), fcos(x) = cos(
the endpoints for the x in between (and fsinc similarly). These functions have the same Lipschitz
constants as their originals, up to factors of 1
η , and give the desired behavior of “smoothing away”
small singular values (though we do keep the 0th and 1st order terms of the exponential).

sinc term dominates.

√

Remark 6.23. Our result generalizes those of Ref. [RWC+20], which achieves essentially the same
result only in the much easier regime where H and b are sparse. They achieve a signiﬁcant speedup
due to these assumptions: note that when H is sparse, and a subsample of rows R is taken, RR† can
be computed in time independent of dimension; so, we only need to take a subsample of rows, and
not of columns. More corners can be cut from our algorithm in this fashion. In summary, though
our algorithm is signiﬁcantly slower, their sparsity assumptions are essential for their fast runtime,
and our framework can identify where these tradeoﬀs occur.

6.8 Semideﬁnite program solving

A recent line of inquiry in quantum computing [BS17, vAGGdW20, BKL+19, vAG19] focuses
on ﬁnding quantum speedups for semideﬁnite programs (SDPs), a central topic in the theory of
convex optimization with applications in algorithms design, operations research, and approximation
algorithms. Chia, Li, Lin, and Wang [CLLW20] ﬁrst noticed that quantum-inspired algorithms
could dequantize these quantum algorithms in certain regimes. We improve on their result, giving
an algorithm which is as general as the quantum algorithms, if the input is given classically (e.g., in
a data-structure in RAM). Our goal is to solve the ε-feasibility problem; solving an SDP reduces by
binary search to solving log(1/ε) instances of this feasibility problem.

48

Problem 6.24 (SDP ε-feasibility). Given an ε > 0, m real numbers b1, . . . , bm ∈ R, and Hermitian
n × n matrices SQ(A(1)), . . . , SQ(A(m)) such that −I (cid:22) A(i) (cid:22) I for all i ∈ [m], we deﬁne Sε as the
set of all X satisfying23

Tr[A(i)X] ≤ bi + ε ∀ i ∈ [m];

X (cid:23) 0;

Tr[X] = 1.

If Sε = ∅, output “infeasible”. If S0 (cid:54)= ∅, output an X ∈ Sε. (If neither condition holds, either
output is acceptable.)

Corollary 6.25. Let F ≥ maxj∈[m]((cid:107)A(j)(cid:107)F), and suppose24 F = Ω(1). Then we can solve
Problem 6.24 with success probability ≥ 1 − δ in cost

(cid:18)(cid:16) F 18

ε40 log20(n) sq(A) +

F 22
ε46 log23(n) + m

F 8
ε18 log8(n) q(A) + m

F 14
ε28 log13(n)

(cid:17)

log3 1
δ

(cid:19)
,

(cid:101)O

providing sampling and query access to a solution.
(cid:16) F 22

(cid:17)
ε28 log13(n)

ε46 log23(n) + m F 14

Assuming sq(A) = (cid:101)O(1), this runtime is (cid:101)O

. For the same feasi-
bility problem, the previous quantum-inspired SDP solver [CLLW20] proved a complexity bound
(cid:101)O(cid:0)mr57ε−92 log37(n)(cid:1), assuming that the constraint matrices have rank at most r. Since the
rank constraint implies that (cid:13)
r, under this assumption our algorithm has complexity
(cid:101)O(cid:0)r11ε−46 log23(n) + mr7ε−28 log13(n)(cid:1). So, our new algorithm both solves a more general problem
and also greatly improves the runtime. The paper with the current best runtime for SDP solving
does not discuss this precise model, but if we use the runtime they achieve in quantum state input
model, making reasonable substitutions of γ → 1
ε and B → F 2, the corresponding quantum runtime
is (cid:101)O

, up to polylog(n) factors.

(cid:13)A(·)(cid:13)

(cid:13)F ≤

(cid:16) F 7

√

(cid:17)

√

ε7.5 +

mF 2
ε4

Like prior work on quantum algorithms for SDP-solving, we use the matrix multiplicative weights
(MMW) framework [AK16, Kal07] to solve Problem 6.24. Corollary 6.25 immediately follows from
running the algorithm this framework admits (Algorithm 1), where we solve an instance of the
problem described in Lemma 6.26 with precision θ = ε/4 in each of the O(cid:0)log(n)/ε2(cid:1) iterations.

MMW works as a zero-sum game with two players, where the ﬁrst player wants to provide an
X ∈ Sε, and the second player wants to ﬁnd a violation for any proposed X, i.e., a j ∈ [m] such
that Tr[A(j)X] > bj + ε. At the tth round of the game, if the second player points out a violation jt
for the current solution Xt, the ﬁrst player proposes a new solution

Xt+1 ∝ exp[−ε(A(j1) + · · · + A(jt))].

Solutions of this form are also known as Gibbs states. It is known that MMW solves the SDP
ε-feasibility problem in O
iterations; a proof can be found, e.g., in the work of Brand˜ao,
Kalev, Li, Lin, Svore, and Wu [BKL+19, Theorem 3] or in Lee, Raghavendra and Steurer [LRS15,
Lemma 4.6].

(cid:16) log n
ε2

(cid:17)

Our task is to execute Lines 3 and 4 of Algorithm 1, for an implicitly deﬁned matrix with the

form given in Line 6.

23For simplicity, we assume here that X is normalized to have trace 1. This can be relaxed; for an example,

see [vAGGdW20].

24Because of the normalization assumption that (cid:107)A(·)(cid:107) ≤ 1, F is eﬀectively a dimensionless “stable rank”-type

constant, normalized by maxi (cid:107)A(i)(cid:107).

49

Algorithm 1: MMW based feasibility testing algorithm for SDPs
n , and the number of iterations T := 16 log n
1 Set X1 := In
2 for t = 1, . . . , T do

ε2

;

3

4

5

6

7

ﬁnd a jt ∈ [m] such that Tr[A(jt)Xt] > bjt + ε
2

or conclude correctly that Tr[A(jt)Xt] ≤ bjt + ε for all j ∈ [m]

if a jt ∈ [m] is found then

Xt+1 := exp[− ε
4

(cid:80)t

i=1 A(ji)]/ Tr[exp[− ε
4

(cid:80)t

i=1 A(ji)]]

else conclude that Xt ∈ Sε

return Xt

8
9 end
10 If no solution found, conclude that the SDP is infeasible and terminate the algorithm

Lemma 6.26 (“Eﬃcient” trace estimation). Consider the setting described in Corollary 6.25. Given
θ ∈ (0, 1], t ≤ log(n)
i=1 A(ji)], we can estimate
Tr(A(i)H)/ Tr(H) with success probability ≥ 1 − δ for all i ∈ [m] to precision θ in cost

and ji ∈ [m] for i ∈ [t], deﬁning H := exp[−θ (cid:80)t

θ2

(cid:18)(cid:20) F 18

θ38 log19(n) sq(A) +

F 22
θ44 log22(n) + m

F 8
θ16 log7(n) q(A) + m

F 14
θ26 log12(n)

(cid:21)

log3 1
δ

+

log(n)

(cid:19)
,
θ2 n(A)

(cid:101)O

where sq(A) = maxj∈[m] sq(A(j)), and s(A), q(A), n(A) are deﬁned analogously.

To estimate Tr[A(i)H], we ﬁrst notice that we have SQφ(θ (cid:80)t

i=1 A(ji)), since it is a linear
combination of matrices that we have sampling and query access to (Lemma 3.9). Then, we can ﬁnd
approximations of the Gibbs state by applying eigenvalue transformation (Theorem 7.2) according to
the exponential function to get exp[−θ (cid:80)t
i=1 A(ji)] as an RUR decomposition. Then the estimation
of Tr[A(i)H] can be performed by usual techniques (namely, Remark 4.13).

In order to understand how precisely we need to approximate the matrix in Line 6 we prove the
following lemmas. Our ﬁrst lemma will show that, to estimate Tr(A(i)H)/ Tr(H) to θ precision, it
suﬃces to estimate both Tr(A(i)H) and Tr(H) to 1

3 θ Tr(H) precision.

Lemma 6.27. Suppose that θ ∈ [0, 1] and a, ˜a, Z, ˜Z are such that |a| ≤ Z, |a − ˜a| ≤ θ
|Z − ˜Z| ≤ θ

3 Z, and

3 Z, then

(cid:12)
(cid:12)
(cid:12)
(cid:12)

˜a
˜Z

−

a
Z

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ θ.

Proof.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

˜a
˜Z

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

a
Z

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

˜aZ
Z ˜Z

−

a ˜Z
Z ˜Z

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

˜aZ − aZ
Z ˜Z

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

aZ − a ˜Z
Z ˜Z

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

3
2Z

|˜a − a| +

3a
2Z2 |Z − ˜Z| ≤

1
2

θ +

1
2

θ ≤ θ.

Next, we will prove that the approximations we will use to Tr(A(i)H) and Tr(H) suﬃce. We
introduce some useful properties of matrix norms. For a matrix A ∈ Cm×n and p ∈ [1, ∞], we
denote by (cid:107)A(cid:107)p the Schatten p-norm, which is the (cid:96)p-norm of the singular values ((cid:80)
i (A))1/p. In
particular, (cid:107)A(cid:107)F = (cid:107)A(cid:107)2 and (cid:107)A(cid:107)Op = (cid:107)A(cid:107)∞. We recall some useful inequalities [Bha97, Section
IV.2]. H¨older’s inequality states that for all B ∈ Cn×k and r, p, q ∈ (0, ∞] such that 1
r , we
have (cid:107)AB(cid:107)r ≤ (cid:107)A(cid:107)p(cid:107)B(cid:107)q. The trace-norm inequality states that if n = m, then |Tr(A)| ≤ (cid:107)A(cid:107)1.

q = 1

p + 1

i σp

50

Lemma 6.28 (Perturbations of the partition function). For all Hermitian matrices H, ˜H ∈ Cn×n,
˜H − eH (cid:13)
(cid:13)
(cid:13)1
The bound in the above lemma is tight, as shown by the example ˜H := H + εI. The proof is
in the appendix. Before proving the following lemma, we observe that for any Hermitian matrix
H ∈ Cn×n with (cid:107)H(cid:107)2

˜H ) − Tr(eH )

e(cid:107) ˜H−H(cid:107) − 1

(cid:12)
(cid:12)
(cid:12)Tr(e

Tr(eH ).

(cid:12)
(cid:12)
(cid:12) ≤

(cid:13)
(cid:13)
(cid:13)e

≤

(cid:16)

(cid:17)

Tr(eH ) = n + Tr(eH − I) = n +

F ≤ n

4 , we have by H¨older’s inequality that
(eλi − 1) ≥ n +

(cid:88)

(cid:88)

λi = n + Tr(H) ≥ n −

√

n(cid:107)H(cid:107)F ≥ n/2. (15)

i

i

Lemma 6.29. Consider a Hermitian matrix H ∈ Cn×n such that (cid:107)H(cid:107)2
4 . Let H have an
approximate eigendecomposition in the following sense: for r ≤ n, suppose we have a diagonal
matrix D ∈ Rr×r and (cid:101)U ∈ Cr×n that satisfy (cid:107) (cid:101)U (cid:101)U † − I(cid:107) ≤ δ and (cid:107)H − (cid:101)U †D (cid:101)U (cid:107) ≤ ε for ε ≤ 1
2 and
δ ≤ min(

F ≤ n

ε

4((cid:107)H(cid:107)+ε) , ε

2 ). Then we have

|(Tr(eD) + n − r) − Tr(eH )| ≤ 2(e − 1)ε Tr(eH ),

(16)

and, moreover, for all A ∈ Cn×n we have

|Tr(A (cid:101)U †(eD − I) (cid:101)U ) + Tr(A) − Tr(AeH )| (cid:46) ε(cid:107)A(cid:107) Tr(eH ).
Proof. First, recall that, by Lemma 2.5, there is unitary U such that (cid:107) (cid:101)U − U (cid:107) ≤ δ. Consequently,
also using facts from Lemma 2.5, along with bounds on δ,

(cid:107)H − U †DU (cid:107) ≤ (cid:107)H − (cid:101)U †D (cid:101)U (cid:107) + δ

2 − δ
(1 − δ)2 (cid:107) (cid:101)U †D (cid:101)U (cid:107) ≤ ε + 4δ((cid:107)H(cid:107) + ε) ≤ 2ε.

(17)

By Lemma 6.28 we have

(cid:13)eU †DU − eH (cid:13)
(cid:13)

(cid:13)1 ≤ (e2ε − 1) Tr(eH ) ≤ 2(e − 1)ε Tr(eH ),

and since eU †DU = U †(eD − I)U + I, by the linearity of trace, the trace-norm inequality, and H¨older’s
inequality,

|Tr(AU †(eD − I)U ) + Tr(A) − Tr(AeH )|

= |Tr(A(eU †DU − eH ))| ≤ (cid:107)A(cid:107)(cid:107)eU †DU − eH (cid:107)1 ≤ 2(e − 1)(cid:107)A(cid:107)ε Tr(eH ).

(18)

In particular, setting A = I, we get the ﬁrst desired bound

|(Tr(eD) + n − r) − Tr(eH )| = |Tr(U †(eD − I)U + I) − Tr(eH )| ≤ 2(e − 1)ε Tr(eH ).

Note that the two identity matrices in the equation above refer to identities of two diﬀerent sizes.
Now, if we show that Tr(AU †(eD − I)U ) − Tr(A (cid:101)U †(eD − I) (cid:101)U ) is suﬃciently small, then the second
desired bound follows by Eq. (18) and triangle inequality.
| Tr(AU †(eD − I)U ) − Tr(A (cid:101)U †(eD − I) (cid:101)U )|
= | Tr((U AU † − (cid:101)U A (cid:101)U †)(eD − I))|
≤ (cid:13)
(cid:13)eD − I(cid:13)
(cid:13)
(cid:13)U AU † − (cid:101)U A (cid:101)U †(cid:13)
(cid:13)1
(cid:13)
≤ (2δ + δ2)(cid:107)A(cid:107)(cid:13)
(cid:13)eD − I(cid:13)
(cid:13)1
≤ 2ε(cid:107)A(cid:107)(cid:13)
(cid:13)eD − I(cid:13)
(cid:13)1
≤ 2ε(cid:107)A(cid:107)(cid:0)Tr(eD) + r(cid:1)
(cid:46) ε(cid:107)A(cid:107) Tr(eH ).

by trace-norm and H¨older’s inequality

by assumption that δ ≤ ε/2

by Eqs. (15) and (16)

by triangle inequality

by Lemma 2.5

51

Now we are ready to devise our upper bound on the trace estimation subroutine.

√

If F

Proof of Lemma 6.26. By Lemma 6.27, it suﬃces to ﬁnd estimates of Tr(eH ) and Tr(AeH ) for all
3 Tr(eH ) additive precision. Recall from the statement that H := −θ (cid:80)t
A = A(i), to θ
i=1 A(ji). By
triangle inequality, (cid:107)H(cid:107)F ≤ F
θ log(n). Because H is a linear combination of matrices, by Lemma 3.9,
after paying log(n)
with q(H) = qφ(H) ≤
log(n)
θ2 q(A) and sφ(H) = s(A).

θ2 n(A) cost, we can obtain SQφ(H) for φ ≤ F 2 log2(n)
θ2(cid:107)H(cid:107)2
F

(cid:16) F 6

If F

θ log(n) >

(cid:17)
θ4 log4(n)

θ6 q(A) log6(n) + m F 4

n/18, then we simply compute the sum H by querying all matrix elements
of every A(ji) in the sum, costing O(cid:0)tn2 q(A)(cid:1). Then we compute eH and its trace Tr(eH ) all in
time O(cid:0)n3(cid:1) [PC99]. Finally, we compute all the traces Tr(eH A(m)) in time O(cid:0)mn2(cid:1). The overall
complexity is O(cid:0)n2(t q(A) + n + m)(cid:1) = (cid:101)O
√
n/18 we do the following. Note that if (cid:107)H(cid:107) ≤ 1, then Tr(eH ) ≥ n/e
θ log(n) ≤
and Tr(A(i)eH ) ≤ (cid:13)
(cid:13)A(i)(cid:13)
n ≤ θ, and out-
(cid:13)F
putting 0 as estimates is acceptable. We use Theorem 7.2 (with f (x) = x, so that L = 1,
and choosing ε := Θ(θ)) to ﬁnd a diagonal matrix D ∈ Rs×s with s = (cid:101)O
=
(cid:101)O(cid:0)F 6θ−6 log6(n)ε−6 log(1/δ)(cid:1) = (cid:101)O(cid:0)F 6θ−12 log6(n) log(1/δ)(cid:1) together with an approximate isome-
(cid:13)
try (cid:101)U = N (SH) ∈ Cs×n such that
(cid:13)
(cid:13) ≤ O(ε). If every diagonal element is less than
3/4, then we conclude that (cid:107)H(cid:107) ≤ 1, and return 0. Otherwise we have (cid:107)H(cid:107) ≥ 1/2 and thus by
(cid:13)
(cid:13) (cid:101)U (cid:101)U † − I
(cid:13)
Theorem 7.2 we have
2 . As per
Theorem 7.2, the cost of this is log3(1/δ) times at most

(cid:107)H(cid:107)+ε + ε with probability at least 1 − δ

n, so Tr(A(i)eH )/ Tr(eH ) ≤ e2F/

(cid:13)
(cid:13)H − (cid:101)U †D (cid:101)U
(cid:13)

(cid:17)
F/ε6 log(1/δ)

(cid:46) ε3(cid:107)H(cid:107)−3 (cid:46) ε

(cid:16)
φ2(cid:107)H(cid:107)6

(cid:13)F ≤ F e

(cid:13)eH (cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

√

√

.

(cid:18) (cid:107)H(cid:107)18
F

ε18 φ7 sqφ(H) +

(cid:19)

(cid:107)H(cid:107)22
ε22 φ6
F

(cid:101)O

= (cid:101)O

= (cid:101)O

= (cid:101)O

= (cid:101)O

(cid:18) (cid:107)H(cid:107)4
F
ε18
(cid:18) (cid:107)H(cid:107)4
F
ε18

F 14
θ14 log14(n) sqφ(H) +
F 14
θ16 log15(n) sq(A) +
1
ε22

(cid:18) 1
ε18
(cid:18) F 18

F 18
θ20 log19(n) sq(A) +
F 22
θ44 log22(n)

θ38 log19(n) sq(A) +

(cid:19)
F 12
θ12 log12(n)
(cid:19)
F 12
θ12 log12(n)
(cid:19)

(cid:107)H(cid:107)10
F
ε22
(cid:107)H(cid:107)10
F
ε22
F 22
θ22 log22(n)
(cid:19)
.

By Lemma 6.29 we have25 that Tr(eD) + (n − s) is a multiplicative θ
as desired, and for all A = A(i), Tr((eD − I) (cid:101)U A (cid:101)U †) + Tr(A) is an additive ( θ
of Tr(AeH ). We can ignore the Tr(A) in our approximation: by Eq. (15) we have

3 -approximation of Tr(eH )
9 Tr(eH ))-approximation

Tr(A) ≤ (cid:107)A(cid:107)F(cid:107)I(cid:107)F ≤ F

√

n ≤ θn/18 ≤ θ Tr(eH )/9,

so | Tr((eD − I) (cid:101)U A (cid:101)U †) − Tr(AeH )| ≤ 2θ
approximation of Tr((eD − I) (cid:101)U A (cid:101)U †) = Tr(A (cid:101)U †(eD − I) (cid:101)U ) to obtain the ( θ
of Tr(AeH ) we seek.

9 Tr(eH )). So, it suﬃces to compute an additive ( θ

9 Tr(eH ))-
3 Tr(eH ))-approximation

We use Remark 4.13 to estimate Tr(A (cid:101)U †(eD − I) (cid:101)U ) to additive precision ( θ

9 Tr(eH )). Note that

by Lemma 6.29 and Eq. (15) we have

(cid:13)
(cid:13) (cid:101)U †(eD − I) (cid:101)U
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

≤ (cid:107) ˜U (cid:107)2(cid:107)eD − I(cid:107)F ≤ 2(cid:107)eD − I(cid:107)F ≤ 2(cid:107)eD − I(cid:107)1 (cid:46) Tr(eH ),

25In case applying Theorem 7.2 would result in s > n, we instead directly diagonalize H ensuring s ≤ n.

52

and since s = (cid:101)O(cid:0)F 6θ−12 log6(n) log(1/δ)(cid:1) and q(H) ≤ log(n)

θ2 q(A), we also have

q( (cid:101)U †(eD − I) (cid:101)U ) = q((SH)†N †(eD − I)N (SH))

= O(cid:0)s · q(H) + s2(cid:1)
= (cid:101)O(cid:0)F 6θ−14 log7(n) log(1/δ) q(A) + F 12θ−24 log12(n) log2(1/δ)(cid:1).

Therefore, Remark 4.13 tells us that given SQ(A), a ( θ
can be computed with success probability at least 1 − δ

2m in time

9 Tr(eH ))-approximation of Tr(A (cid:101)U †(eD − I) (cid:101)U )

(cid:18) (cid:107)A(cid:107)2
F
θ2

O

(cid:0) sq(A) + s · q(H) + s2(cid:1) log

(cid:19)
.

m
δ

Since we do this for all i ∈ [m], the overall complexity of obtaining the desired estimates Tr(A(i)eH )
with success probability at least 1 − δ

2 is m times

(cid:18) F 8
θ16 log7(n) log(1/δ) log(m/δ) q(A) +

(cid:19)
F 14
θ26 log12(n) log2(m/δ) log(m/δ)
.

(cid:101)O

6.9 Discriminant analysis

Discriminant analysis is used for dimensionality reduction and classiﬁcation over large data sets.
Cong and Duan introduced a quantum algorithm to perform both with Fisher’s linear discriminant
analysis [CD16], a generalization of principal component analysis to data separated into classes.

The problem is as follows: given classiﬁed data, we wish to project our data onto a subspace
that best explains between-class variance, while minimizing within-class variance. Suppose there
are M input data points {xi ∈ RN : 1 ≤ i ≤ M } each belonging to one of k classes. Let µc denote
the centroid (mean) of class c ∈ [k], and ¯x denote the centroid of all data points. Following the
notation of [CD16], let

k
(cid:88)

(µc − ¯x)(µc − ¯x)T and SW =

SB =

c=1

k
(cid:88)

(cid:88)

(µc − x)(µc − x)T .

c=1

x∈c

denote the between-class and within-class scatter matrices of the dataset respectively. The original
goal is to solve the generalized eigenvalue problem SBvi = λiSW vi and output the top eigenvalues
and eigenvectors; for dimensionality reduction using linear discriminant analysis, we would project
onto these top eigenvectors. If SW would be full-rank, this problem would be equivalent to ﬁnding the
eigenvalues of S−1
W SB. However, this does not happen in general, and therefore various relaxations are
considered in the literature [BHK97, Wel09]. For example, Welling [Wel09] considers the eigenvalue
problem of

1
2

1
2

BS−1
S
W S
Cong and Duan further relax the problem, as they ignore small eigenvalues of SW and SB, and
only compute approximate eigenvalues of Eq. (19) (after truncating eigenvalues), leading to inexact
eigenvectors. We construct a classical analogue of their quantum algorithm.26 Cong and Duan also
describe a quantum algorithm for discriminant analysis classiﬁcation; this algorithm does a matrix
inversion procedure very similar to those described in Section 6.5 and Section 6.6, so for brevity we
will skip dequantizing this algorithm.

(19)

B.

26Analyzing whether or not the particular relaxation used in this and other quantum machine learning papers

provides a meaningful output is unfortunately beyond the scope of our paper.

53

To formally analyze this algorithm, we could, as in Section 6.4, assume the existence of an
eigenvalue gap, so the eigenvectors are well-conditioned. However, let us instead use a diﬀerent
BU ≈ U D,

convention: if we can ﬁnd diagonal D and an approximate isometry U such that S
then we say we have found approximate eigenvalues and eigenvectors of S+
W SB.

BS−1
W S

1
2

1
2

Problem 6.30 (Linear discriminant analysis). Consider the functions

sqrt(x) =


0

2x/σ − σ σ2/2 ≤ x < σ2
√

x < σ2/2



x

x ≥ σ2

inv(x) =

x < σ2/2


0

2x/σ4 − 1/σ2 σ2/2 ≤ x < σ2
1/x

x ≥ σ2



.

Given SQ(B, W ) ∈ Cm×n, with SW := W †W and SB := B†B, ﬁnd an α-approximate isometry
U ∈ Cn×p and diagonal D ∈ Cp×p such that we have SQφ(U (·, i)) for all i, |Dii − λi| ≤ ε(cid:107)B(cid:107)2/σ2
for λi the eigenvalues of sqrt(SB) inv(SW ) sqrt(SB), and

(cid:107) sqrt(SB) inv(SW ) sqrt(SB)U − U D(cid:107) ≤ ε(cid:107) sqrt(SB)(cid:107)2(cid:107) inv(SW )(cid:107) ≤ ε(cid:107)B(cid:107)2/σ2.

The choice of error bound is natural, since (cid:107)B(cid:107)2/σ2 is essentially (cid:107) sqrt(SB)(cid:107)2(cid:107) inv(SW )(cid:107): we
, up to

aim for additive error. The quantum algorithm achieves a runtime of (cid:101)O
polylog(m, n) factors [CD16, Theorem 2].27

(cid:16) (cid:107)B(cid:107)7
ε3σ7 + (cid:107)W (cid:107)7
F

F
ε3σ7

(cid:17)

Corollary 6.31. For ε < σ/(cid:107)B(cid:107), we can solve Problem 6.30 in (cid:101)O
ε2σ4 log2 1

time, with (cid:102)sqφ(U (·, i)) = (cid:101)O

(cid:16) (cid:107)B(cid:107)4
F

(cid:17)

.

δ

(cid:16)

( (cid:107)B(cid:107)6

F(cid:107)B(cid:107)4
ε6σ10 + (cid:107)W (cid:107)6

F(cid:107)W (cid:107)10
ε6σ16

(cid:17)

) log3 1
δ

We prove this by using Theorem 5.1 to approximate sqrt(W †W ) ≈ R†

R†
BUBRB by RUR decompositions. Then, we use Lemma 4.6 to approximate RW R†
W R(cid:48)†
submatrices R(cid:48)
and vectors we want to ﬁnd, R†

W UW RW and inv(B†B) ≈
B by small
B. This yields an approximate RUR decomposition of the matrix whose eigenvalues

W U RW for U = UW R(cid:48)

BUBR(cid:48)

W UW .

W R(cid:48)†

BR(cid:48)†

Finding eigenvectors from an RUR decomposition follows from an observation (Lemma 4.14):
for a matrix CW formed by sampling columns from RW (using SQ(W )), and [CW ]k the rank-k
approximation to CW (which can be computed because CW has size independent of dimension),
(([CW ]k)+RW )† has singular values either close to zero or close to one. This roughly formalizes the
intuition of CW preserving the left singular vectors and singular values of RW . We can rewrite
W U RW = R†
R†
k RW , which holds by choosing k suﬃciently large and choosing C
to be the same sketch used for U . Then, we can compute the eigendecomposition of the center
C†
kU Ck = V DV †, which gives us an approximate eigendecomposition for R†
k RW )†V is
an approximate isometry, so we choose its columns to be our eigenvectors, and our eigenvalues are
the diagonal entries of D. We show that this has the approximation properties analogous to the
quantum algorithm.

W U RW : (C+

kU CkC+

k )†C†

W (C+

Proof. By Theorem 5.1, we can ﬁnd RB, CB, RW , CW such that

(cid:107) sqrt(B†B) − R†
(cid:107) inv(W †W ) − R†

Bsqrt(CBC†
W inv(CW C†
27This is the runtime of Step 2 of Algorithm 1. The normalization factor of max((cid:107)B(cid:107)F, (cid:107)S(cid:107)F) is implicit there,
κef f corresponds to max((cid:107)B(cid:107)F,(cid:107)S(cid:107)F)
, and the error bound the algorithm achieves is the one we describe here, since the
authors must implicitly rescale the inverse and square root function by a cumulative factor of (cid:107)B(cid:107)2/σ2 to apply their
Theorem 1.

B)RB(cid:107) ≤ ε(cid:107)B(cid:107)
W )RW (cid:107) ≤ ε/σ2

σ2

54

with

rB = (cid:101)O

rW = (cid:101)O

(cid:19)

(cid:18) (cid:107)B(cid:107)2
F

1
ε2σ2 log
δ
(cid:18) (cid:107)W (cid:107)2(cid:107)W (cid:107)2
F
ε2σ4

log

(cid:19)

1
δ

cB = (cid:101)O

cW = (cid:101)O

(cid:18) (cid:107)B(cid:107)4(cid:107)B(cid:107)2
F
ε2σ6
(cid:18) (cid:107)W (cid:107)6(cid:107)W (cid:107)2
F
ε2σ8

log

(cid:19)

1
δ

log

(cid:19)

.

1
δ

Let ZB := sqrt(CBC†

B) and ZW := inv(CW C†

W ). These approximations suﬃce for us:

(cid:107) sqrt(SB) inv(SW ) sqrt(SB) − R†

BZBRBR†
≤ (cid:107) sqrt(SB) − R†

W ZW RW R†
BZBRB(cid:107)(cid:107) inv(SW ) sqrt(SB)(cid:107)

BZBRB(cid:107)

+ (cid:107)R†

BZBRB(cid:107)(cid:107) inv(SW ) − R†
+ (cid:107)R†

W ZW RW (cid:107)(cid:107) sqrt(SB)(cid:107)
BZBRBR†

W ZW RW (cid:107)(cid:107) sqrt(SB) − R†

BZBRB(cid:107),

each of which is bounded by ε(cid:107)B(cid:107)2/σ2. Next, we approximate (cid:107)RBR†
since then

W − R(cid:48)

BR(cid:48)†

W (cid:107)F ≤ εσ3/2(cid:112)(cid:107)B(cid:107),

1
2
B

(cid:107) ¯Σ

1
2

W

1
BRBR†
¯U †
¯UW ¯Σ
2
B
1
¯U †
B(cid:107)(cid:107)RBR†
≤ (cid:107) ¯Σ
2
B
≤ σ− 1
2 (cid:107)RBR†
(cid:112)
(cid:107)B(cid:107)/σ2,

W − R(cid:48)

≤ ε

W − ¯Σ
W − R(cid:48)
BR(cid:48)†

BR(cid:48)†
W (cid:107)σ−2

¯UW ¯Σ

1
2

W (cid:107)

BR(cid:48)†
¯U †
BR(cid:48)
W (cid:107)(cid:107) ¯UW ¯Σ

W
1
2

W (cid:107)

and so

(cid:107)R†

BZBRBR†

W ZW RW R†

BZBRB − R†

BZBR(cid:48)

BR(cid:48)†

W ZW R(cid:48)

W R(cid:48)†

BZBRB(cid:107) (cid:46) ε(cid:107)B(cid:107)2/σ2.

Now, we can compute Z := ZBR(cid:48)

BR(cid:48)†

W ZW R(cid:48)

W R(cid:48)†

BZB and, using that ZB = ZB[CB] σ√

[CB]+
σ√
2

, rewrite

2

BZRB = R†
R†

B([CB]+
σ√
2

)†[CB]†
σ√
2

Z[CB] σ√

2

[CB]+
σ√
2

RB.

RB)† is an εσ/(cid:107)B(cid:107)-approximate projective isometry28 onto the image of

(where we use that ε < σ/(cid:107)B(cid:107)). To turn this approximate projective isometry into an

By Lemma 4.14, ([CB]+
σ√
2
[CB]+
σ√
2
isometry, we compute the eigendecomposition [CB]†
σ√
2
V is full rank. Consequently, U := R†
image of [CB]+
σ√
2
eigenvalues are D := Σ. This satisﬁes the desired bounds because

B([CB]+
σ√
2

Z[CB] σ√

2

= V ΣV †, where we truncate so that

)†V is full rank—the image of V is contained in the

—and thus is an εσ/(cid:107)B(cid:107)-approximate isometry. So, our eigenvectors are U and our

(cid:107) sqrt(SB) inv(SW ) sqrt(SB)U − U D(cid:107)
≤ (cid:107) sqrt(SB) inv(SW ) sqrt(SB)U − U DU †U (cid:107) + (cid:107)U DU †U − U D(cid:107)

≤ ε

(cid:107)B(cid:107)2
σ2 (cid:107)U (cid:107) + (cid:107)U D(cid:107)(cid:107)U †U − I(cid:107) (cid:46) ε

(cid:107)B(cid:107)2
σ2 .

28We get more than we need here: an ε-approximate projective isometry would suﬃce for the subsequent arguments.

55

The eigenvalues are correct because, by the approximate isometry condition, (cid:107)U − ˜U (cid:107) (cid:46) ε σ
an isometry, and so we can use Lemma 2.5 to conclude

(cid:107)B(cid:107) for ˜U

(cid:107) sqrt(SB) inv(SW ) sqrt(SB) − ˜U D ˜U †(cid:107)
≤ (cid:107) sqrt(SB) inv(SW ) sqrt(SB) − U DU †(cid:107) + (cid:107)U DU † − ˜U D ˜U †(cid:107)

(cid:46) ε

(cid:107)B(cid:107)2
σ2 + ε

σ
(cid:107)B(cid:107)

(cid:107)D(cid:107) (cid:46) ε

(cid:107)B(cid:107)2
σ2 .

˜U D ˜U † is an eigendecomposition. Furthermore, this is an approximation of a Hermitian PSD
matrices, where singular value error bounds align with eigenvalue error bounds. So, Weyl’s
inequality (Lemma 4.11) implies the desired bound |Dii − λi| (cid:46) ε (cid:107)B(cid:107)2
for λi the true eigenvalues.
σ2
We have SQφ(U (·, i)) by Lemmas 3.5 and 3.6, since U (·, i) = R†
B([CB]+
)†V (·, i). The runtime
σ√
2

is (cid:102)sqφ(U (·, i)) = rBφ log 1

δ , where

φ = rB

(cid:80)rB

j=1 (cid:107)RB(j, ·)(cid:107)2|[([CB]+
σ√
2
(cid:107)U (·, i)(cid:107)

)†V (·, i)](j)|2

(cid:46) (cid:107)B(cid:107)2

F(cid:107)([CB]+
σ√
2

)†V (·, i)(cid:107)2 (cid:46) (cid:107)B(cid:107)2
σ2

F

.

This gives the stated runtime.

7 More singular value transformation

In this section, we present more general versions of our algorithm for even SVT to get results
for generic SVT (Theorem 7.1) and eigenvalue transformation (Theorem 7.2). In applications we
mainly use even SVT to allow for more ﬁne-tuned control over runtime, but we do use eigenvalue
transformation in Section 6.8.

For generic SVT: consider a matrix A ∈ Cm×n and a function f : [0, ∞) → C satisfying f (0) = 0
(so the singular value transformation f (SV)(A) is well-deﬁned as in Deﬁnition 2.1). Given SQ(A)
and SQ(A†), we give an algorithm to output a CUR decomposition approximating f (SV)(A).

Theorem 7.1 (Generic singular value transformation). Let A ∈ Cm×n be given with both SQφ(A)
and SQφ(A†) and let f : [0, ∞) → C be a function such that f (0) = 0, g(x) := f (
x is
L-Lipschitz, and ¯g(x) := (g(x) − g(0))/x is ¯L-Lipschitz. Then, for 0 < ε ≤ min(L(cid:107)A(cid:107)3, ¯L(cid:107)A(cid:107)5), we
can output sketches R := SA ∈ Cr×n and C := AT ∈ Cm×c, along with M ∈ Cr×c such that

x)/

√

√

(cid:104)

(cid:105)
(cid:107)CM R + g(0)A − f (SV)(A)(cid:107) > ε

Pr

< δ,

with r = (cid:101)O(cid:0)φ2L2(cid:107)A(cid:107)2(cid:107)A(cid:107)4
takes time

F

1

ε2 log 1

δ

(cid:1) and c = (cid:101)O(cid:0)φ2L2(cid:107)A(cid:107)4(cid:107)A(cid:107)2

F

1

ε2 log 1

δ

(cid:1). Finding S, M , and T

(cid:16)

˜O

( ¯L2(cid:107)A(cid:107)8(cid:107)A(cid:107)2

F + L2(cid:107)A(cid:107)2(cid:107)A(cid:107)4
F)

φ2
1
ε2 log
δ
+ (L2 ¯L2(cid:107)A(cid:107)12(cid:107)A(cid:107)4
F + L4(cid:107)A(cid:107)6(cid:107)A(cid:107)6
F)

(sφ(A) + sφ(A†) + qφ(A) + qφ(A†))
φ4

q(A)

ε4 log2 1
F + L6(cid:107)A(cid:107)10(cid:107)A(cid:107)8
F)

δ

+ (L4 ¯L2(cid:107)A(cid:107)16(cid:107)A(cid:107)6

φ6

ε6 log3 1

δ

+ nφ(A)

(cid:17)

.

If we only wish to assume SQφ(A), we can do so by using Lemma 4.4 instead of Lemma 4.6 in

our proof, paying an additional factor of 1
δ .

56

Note that if sqφ(A), sqφ(A†) = O(1), then this runtime is dominated by the last term. Moreover,
if A is strictly low-rank, with minimum singular value σ, or essentially equivalently, if f (x) = 0 for
x ≤ σ and so g(x) = 0 for x ≤ σ2, then L ≤ (cid:96)/σ2 and ¯L = 2(cid:96)/σ4 for (cid:96) the Lipschitz constant of f .
In this case the complexity is

(cid:18)(cid:16) (cid:107)A(cid:107)10(cid:107)A(cid:107)6
F

σ16

(cid:101)O

+

(cid:107)A(cid:107)4(cid:107)A(cid:107)8
F
σ12

(cid:17)(cid:16) (cid:96)(cid:107)A(cid:107)

(cid:17)6

ε

φ6 log3 1
δ

(cid:19)
.

(20)

Importantly, when ε = O((cid:96)(cid:107)A(cid:107)) (that is, if we want relative error), this runtime is independent of
dimension. If one desires greater generality, where we only need to depend on the Lipschitz constant
of f , we can use a simple trick: as we aim for a spectral norm bound, we can essentially treat A as
if it had strictly low rank. Consider the variant of f , f≥σ, which is zero below σ/2, f above σ, and
is a linear interpolation in between.

f≥σ(x) :=


0
0 ≤ x < σ/2

(2x/σ − 1)f (σ) σ/2 ≤ x < σ
f (x)

σ ≤ x



Then (cid:107)f (SV)(A) − f (SV)
≥ε/(cid:96) (A)(cid:107) ≤ ε, because f (ε/(cid:96)) ≤ ε. Further, the Lipschitz constant of f≥ε/(cid:96) is at
most 2(cid:96): the slope of the linear interpolation is 2f (σ)/σ ≤ 2(cid:96)σ/σ. So, we can run our algorithm for
arbitrary (cid:96)-Lipschitz f in the time given by Eq. (20), with σ = ε/(cid:96).

Our proof strategy is to apply our main result Theorem 5.1 to g(A†A), for g(x) := f (

x,
and subsequently approximate matrix products with Lemma 4.6 to get an approximation of the
form A(cid:48)R(cid:48)†U R + g(0)A:

x)/

√

√

f (SV)(A) = Ag(A†A) ≈ AR†U R + A(g(0)I) ≈ A(cid:48)R(cid:48)†U R + g(0)A.

Here, A(cid:48)R(cid:48)†U R is a CUR decomposition as desired, since A(cid:48) is a normalized subset of columns of A.
One could further approximate g(0)A by a CUR decomposition if necessary (e.g. by adapting the
eigenvalue transformation result below).

We do not use this theorem in our applications. Sometimes we implicitly use a similar strategy
(e.g. in Section 6.5), but because we apply our matrix to a vector (f (A†)b) we can use Lemma 4.12
instead of Lemma 4.6 when approximating. This allows for the algorithm to work with only SQφ(A)
and still achieve a poly-logarithmic dependence on 1
δ .

√

√

Proof. If we want to compute ˆf (SV)(A), we can work with f (x) := ˆf (x) − g(0)x, so that g(0) = 0
without loss of generality. Notice that ˆf (SV)(A) = f (SV)(A)+g(0)A, so if we get a CUR decomposition
for f (SV)(A) we can add g(0)A after to get the decomposition in the theorem statement.

Consider the SVT g(x) := f (

x)/

x, so that f (SV)(A) = Ag(A†A). First, use Theorem 5.1 to

get SA ∈ Cr×n, SAT ∈ Cr×c such that, with probability ≥ 1 − δ/4,

(cid:107)(SA)†¯g((SAT )(SAT )†)SA − g(A†A)(cid:107) ≤

ε
2(cid:107)A(cid:107)

.

Second, use Lemma 4.6 to get a sketch T (cid:48)† ∈ Cc(cid:48)×n such that, with probability ≥ 1 − δ/4,

(cid:107)A(SA)† − AT (cid:48)(SAT (cid:48))†(cid:107) ≤ ε(3L(cid:107)A(cid:107))−1.

(21)

(22)

57

The choices of parameters necessary are as follows (using that (cid:107)SA(cid:107)F = O((cid:107)A(cid:107)F) by Eq. (5) and
we have a 2φ-oversampled distribution for (SA)† by Lemma 4.3):

(cid:16)
r = ˜Θ

(cid:16)
c = ˜Θ

(cid:16)
c(cid:48) = ˜Θ

φ2L2(cid:107)A(cid:107)4(cid:107)A(cid:107)2
F

φ2 ¯L2(cid:107)A(cid:107)8(cid:107)A(cid:107)2
F

φ2L2(cid:107)A(cid:107)2(cid:107)A(cid:107)4
F

1
ε2 log
1
ε2 log
1
ε2 log

(cid:17)

(cid:17)

(cid:17)

1
δ
1
δ
1
δ

This implies the desired bound through the following sequence of approximations:

f (SV)(A) = Ag(A†A)

≈ A(SA)†¯g((SAT )(SAT )†)SA
≈ AT (cid:48)
(cid:124)(cid:123)(cid:122)(cid:125)
C

(SAT (cid:48))†¯g((SAT )(SAT )†)
(cid:123)(cid:122)
(cid:125)
(cid:124)
M

.

SA
(cid:124)(cid:123)(cid:122)(cid:125)
R

This gives us a CUR decomposition of f (SV)(A). These two approximations only incur O(ε) error
in spectral norm; for the ﬁrst, notice that

(cid:107)Ag(A†A) − A(SA)†¯g((SAT )(SAT )†)SA(cid:107)

≤ (cid:107)A(cid:107)(cid:107)(SA)†¯g((SAT )(SAT )†)SA − g(A†A)(cid:107) ≤

ε
2

.

(by (21))

For the second approximation observe that |g(x)| ≤ L|x| (and, by corollary, ¯g(x) ≤ L) due to g
being L-Lipschitz and g(0) = 0, therefore

(cid:107)(A(SA)† − AT (cid:48)(SAT (cid:48))†)¯g((SAT )(SAT )†)SA(cid:107)

≤ (cid:107)AT (cid:48)(SAT (cid:48))† − A(SA)†(cid:107)(cid:107)

(cid:113)

¯g((SAT )(SAT )†)(cid:107)(cid:107)

¯g((SAT )(SAT )†)SA(cid:107)

(cid:113)

(cid:113)

(cid:113)

≤ ε(3L(cid:107)A(cid:107))−1(cid:107)
√

≤ ε(3L(cid:107)A(cid:107))−1

¯g((SAT )(SAT )†)(cid:107)(cid:107)
(cid:114)

L

(cid:107)g(A†A)(cid:107) +

ε
2(cid:107)A(cid:107)

≤ ε(3L(cid:107)A(cid:107))−1

(cid:114) 3
2

L(cid:107)A(cid:107) <

ε
2

.

¯g((SAT )(SAT )†)SA(cid:107)

(by (22))

(since ¯g(x) ≤ L and by (21))

(since |g(x)| ≤ L|x| and ε ≤ L(cid:107)A(cid:107)3)

The time complexity of this procedure is
nφ(A) + (r + c + c(cid:48))(sφ(A) + qφ(A)) + c(cid:48)(sφ(A†) + qφ(A†)) + (rc + rc(cid:48)) q(A) + r2c + r2c(cid:48)(cid:17)

(cid:16)

O

,

which comes from producing sketches, querying all the relevant entries of SAT and SAT (cid:48), the
singular value transformation of SAT , and the matrix multiplication in M . We get r factors in
the latter two terms because we can separate ¯g((SAT )(SAT )†) =
¯g(SAT ))† where
√

¯g(SAT )(

√

√

¯g(x) := (cid:112)¯g(x).

As for eigenvalue transformation, consider a function f : R → C and a Hermitian matrix
A ∈ Cn×n, given SQ(A). If f is even (so f (x) = f (−x)), then f (A) = f (
A†A), so we can use
Theorem 5.1 to compute the eigenvalue transform f (A). For non-even f , we cannot use this result,
and present the following algorithm to compute it.

√

58

Theorem 7.2 (Eigenvalue transformation). Suppose we are given a Hermitian SQφ(A) ∈ Cn×n
with eigenvalues λ1 ≥ · · · ≥ λn, a function f : R → C that is L-Lipschitz on ∪n
i=1[λi − d, λi + d] for
L , and some ε ∈ (0, L(cid:107)A(cid:107)/2]. Then we can output matrices S ∈ Cr×n, N ∈ Cs×r, and
some d > ε
D ∈ Cs×s, with r = (cid:101)O

and s = (cid:101)O

, such that

φ2(cid:107)A(cid:107)4(cid:107)A(cid:107)2
F

(cid:107)A(cid:107)2
F

L6
ε6 log 1

L2
ε2

(cid:16)

(cid:17)

(cid:16)

(cid:17)

δ

(cid:105)
(cid:104)
(cid:107)(SA)†N †DN (SA) + f (0)I − f (EV)(A)(cid:107) > ε

Pr

< δ,

in time

(cid:16)

˜O

(L10ε−10(cid:107)A(cid:107)8(cid:107)A(cid:107)2

Fφ2 log

1
δ

+ (L16ε−16(cid:107)A(cid:107)12(cid:107)A(cid:107)4

+ L6ε−6(cid:107)A(cid:107)6
Fφ4 log2 1
δ

Fφ log

1
δ

)(sφ(A) + qφ(A))

+ L18ε−18(cid:107)A(cid:107)8(cid:107)A(cid:107)10

F φ5 log3 1
) q(A)
δ
Fφ6 log3 1
+ L22ε−22(cid:107)A(cid:107)16(cid:107)A(cid:107)6
δ

+ nφ(A)

(cid:17)

.

Moreover, this decomposition satisﬁes the following two properties. First, N SA is an approximate
isometry: (cid:107)(N SA)(N SA)† − I(cid:107) ≤ (
L(cid:107)A(cid:107) )3. Second, D is a diagonal matrix and its diagonal entries
satisfy |D(i, i) + f (0) − f (λi)| ≤ ε for all i ∈ [n] (where D(i, i) := 0 for i > s).

ε

Under the reasonable assumptions29 that sq(A) and φ are small (O(1), say) and ε ≤ L(cid:107)A(cid:107) (cid:107)A(cid:107)
(cid:107)A(cid:107)F

,

the complexity of this theorem is (cid:101)O(cid:0)L22ε−22(cid:107)A(cid:107)16(cid:107)A(cid:107)6

F log3 1
δ

(cid:1).

We now outline our proof. Our strategy is similar to the one used for quantum-inspired
semideﬁnite programming [CLLW20]: ﬁrst we ﬁnd the eigenvectors and eigenvalues of A and then
apply f to the eigenvalues. Let π(x) be a (smoothened) step function designed so it can zeroes out
small eigenvalues of A (in particular, eigenvalues smaller than ε/

2L). Then

√

A ≈ π(A†A)Aπ(A†A)

by deﬁnition of π

≈ R†¯π(CC†)RAR†¯π(CC†)R
≈ R†¯π(CC†)M ¯π(CC†)R
= R†(CσC+

σ )†¯π(CC†)M ¯π(CC†)CσC+

σ R.

by Theorem 5.1
by sketching M ≈ RAR†

where σ = ε/

2L

√

ˆU := C+

Here, Cσ is the low-rank approximation of C formed by transforming C according to the “ﬁlter”
σ R ∈ Cc×n is an approximate
function on x that is 0 for x < σ and x otherwise.
isometry by Lemma 4.14. We are nearly done now: since the rest of the matrix expression,
C†
σ ¯π(CC†)M ¯π(CC†)Cσ ∈ Cc×c, consists of submatrices of A of size independent of n, we can
directly compute its unitary eigendecomposition U DU †. This gives the approximate decomposition
A ≈ ( ˆU U )D( ˆU U )†, with ˆU U and D acting as approximate eigenvectors and eigenvalues of A,
respectively. An application of Lemma 5.4 shows that f (A) ≈ ( ˆU U )f (D)( ˆU U )† in the desired sense.
Therefore, our output approximation of f (A) comes in the form of an RUR decomposition that can
be rewritten in the form of an approximate eigendecomposition. The only major diﬀerence between
this proof sketch and the proof below is that we perform our manipulations on the SVD of Cσ, to
save on computation time: note that the SVD can be made small in dimension, using that the rank
of Cσ is bounded by (cid:107)C(cid:107)2

F/σ2.

29The correct way to think about ε is as some constant fraction of L(cid:107)A(cid:107). If ε > L(cid:107)A(cid:107) then f (0)I is a satisfactory
approximation. The bound we give says that we want an at least (cid:107)A(cid:107)F/(cid:107)A(cid:107) improvement over trivial, which is modest
in the close-to-low-rank regime that we care about. Similar assumptions appear in Section 6.

59

Proof. Throughout this proof ε is not dimensionless; if choices of parameters are confusing, try
replacing ε with ε(cid:107)A(cid:107). We will take f (0) = 0 without loss of generality. First, consider the “smooth
projection” singular value transformation

π(x) =


0

2L2
ε2 x − 1

1

x < ε2
2L2
2L2 ≤ x < ε2
ε2
ε2
L2 ≤ x

L2

Since π is a projector onto the large eigenvectors of A, we can add these projectors to our expression
without incurring too much spectral norm error.

(cid:107)π(A†A)Aπ(A†A) − A(cid:107) = max
i∈[n]

|π(λ2

i )λiπ(λ2

i ) − λi| ≤ ε/L

Second, use Theorem 5.1 to get SA ∈ Cr×n, SAT ∈ Cr×c such that, with probability ≥ 1 − δ,

(cid:107)(SA)†¯π((SAT )(SAT )†)SA − π(A†A)(cid:107) ≤

ε
L(cid:107)A(cid:107)

.

The necessary sizes for these bounds to hold are as follows (Lipschitz constants for π are 2L2/ε2
and 4L4/ε4, (cid:107)SA(cid:107)F = O((cid:107)A(cid:107)F) by Eq. (5), and we have a 2φ-oversampled distribution for (SA)†
by Lemma 4.3):30

(cid:16)
r = ˜Θ

(cid:16)
c = ˜Θ

φ2 L4
φ2 L8

ε4 (cid:107)A(cid:107)2(cid:107)A(cid:107)2
ε8 (cid:107)A(cid:107)6(cid:107)A(cid:107)2

F

F

L2(cid:107)A(cid:107)2
ε2
L2(cid:107)A(cid:107)2
ε2

log

log

(cid:17)

(cid:16)
= ˜Θ

(cid:17)

(cid:16)
= ˜Θ

1
δ
1
δ

φ2(cid:107)A(cid:107)4(cid:107)A(cid:107)2
F

φ2(cid:107)A(cid:107)8(cid:107)A(cid:107)2
F

This approximation does not incur too much error:

(cid:17)

,

L6
ε6 log
L10
ε10 log

1
δ
1
δ

(cid:17)

.

(cid:107)R†¯π(CC†)RAR†¯π(CC†)R − π(A†A)Aπ(A†A)(cid:107)
≤ (cid:107)π(A†A)A(π(A†A) − R†¯π(CC†)R)(cid:107) + (cid:107)(π(A†A) − R†¯π(CC†)R)AR†¯π(CC†)R(cid:107)
(cid:17)(cid:17)

(cid:17)

(cid:16)

(cid:16)

(cid:16)

(cid:107)π(A†A)(cid:107)(cid:107)A(cid:107) + (cid:107)A(cid:107)(cid:107)R†¯π(CC†)R(cid:107)

≤

(cid:107)A(cid:107) + (cid:107)A(cid:107)

1 +

≤

ε
L(cid:107)A(cid:107)

ε
L(cid:107)A(cid:107)

ε
L(cid:107)A(cid:107)

≤ 3

ε
L

.

δ

(cid:16)

(cid:17)

ε6 log r2
L6

Third, apply Remark 4.13(b) r2 times to approximate each entry of RAR†: pull t samples from
such that, given some Q(x), Q(y), with probability ≥ 1 − δ
φ(cid:107)A(cid:107)6
SQφ(A) for t := O
r2 ,
F
one can output an estimate of x†Ay up to ε3(cid:107)x(cid:107)(cid:107)y(cid:107)
additive error with no additional queries to
L3(cid:107)A(cid:107)2
F
SQφ(A). Then, by union bound, with probability ≥ 1 − δ, using the same t samples from A each
time, one can output an estimate of R(i, ·)AR(j, ·)† up to ε3(cid:107)R(i,·)(cid:107)(cid:107)R(j,·)(cid:107)
error for all i, j ∈ [r] such
that i ≤ j. Let M be the matrix of these estimates. Then, using that (cid:107)R(cid:107)F = O((cid:107)A(cid:107)F) from
Eq. (5),

L3(cid:107)A(cid:107)2
F

(cid:107)M − RAR†(cid:107)2

F ≤

r
(cid:88)

r
(cid:88)

i=1

j=1

(cid:16) ε3(cid:107)R(i, ·)(cid:107)(cid:107)R(j, ·)(cid:107)
L3(cid:107)A(cid:107)2
F

(cid:17)2

=

ε6(cid:107)R(cid:107)4
F
L6(cid:107)A(cid:107)4
F

(cid:46) ε6
L6 .

30The constraint on the size of ε from Theorem 5.1 here is ε(L(cid:107)A(cid:107))−1 (cid:46) min(L2(cid:107)A(cid:107)2/ε2, L4(cid:107)A(cid:107)4/ε4), which is true

since ε ≤ L(cid:107)A(cid:107)/2.

60

From Eqs. (6) and (7),

(cid:107)R†¯π(CC†)(RAR† − M )¯π(CC†)R(cid:107) (cid:46) ε3

L3 (cid:107)R†¯π(CC†)(cid:107)2 ≤

(cid:16)

ε3
L3

1 +

ε
L(cid:107)A(cid:107)

(cid:17) L2
ε2

(cid:46) ε
L

.

So far, we have shown that we can ﬁnd an RUR approximation to A, with

(cid:107)R†¯π(CC†)M ¯π(CC†)R − A(cid:107) (cid:46) ε
L

However, if we wish to apply an eigenvalue transformation to A, we need to access the eigenvalues of A
as well. To do this, we will express this decomposition as an approximate unitary eigendecomposition.
Using that ¯π zeroes out singular values that are smaller than ε2
2L2 , we can write our expression as
ˆU ˆD ˆU †, for ˆU ∈ Cn×s and ˆD ∈ Cs×s:

R†¯π(CC†)M ¯π(CC†)R
= (cid:0)C+
ε√
= (cid:0)R†U (C)
ε√

R(cid:1)†(cid:0)C†
ε√

2L

2L

2L

(D(C)
ε√
(cid:123)(cid:122)
ˆU

2L

)−1(cid:1)
(cid:0)D(C)
ε√
(cid:124)
(cid:125)

2L

(U (C)
ε√

2L

(cid:124)

¯π(CC†)M ¯π(CC†)C ε√

(cid:1)(cid:0)C+
ε√

2L

R(cid:1)

2L

)†¯π(CC†)M ¯π(CC†)U (C)
ε√

(cid:123)(cid:122)
ˆD

2L

D(C)
ε√

2L

(cid:1)

(cid:0)(D(C)
ε√
(cid:124)
(cid:125)

2L

)−1(U (C)
ε√
(cid:123)(cid:122)
ˆU †

2L

.

)†R(cid:1)
(cid:125)

(23)

ε√

2L

2L

2L

2L

2L

√

Here, we are using an SVD of C truncated to ignore singular values smaller than
U (C)
ε√

∈ Cr×s, D(C)
∈ Cs×s, V (C)
ε√
ε√
2L)2 (cid:46) (cid:107)A(cid:107)2
ε√
. Note that, as a result, s ≤ (cid:107)C(cid:107)2
are at least
By Lemma 4.14 with our values of r and c, we get that ˆU := R†U (C)
ε√
approximate isometry; we rescale ε until this is at most 1
2 .

, where
∈ Cc×s, where s is the number of singular values of C that
FL2ε−2 and s ≤ min(r, c, n).
(cid:17)
(D(C)
)−1 is an O
-
ε√

The rest of this error analysis will show that, since ˆU is an approximate isometry, f (A) ≈
ˆU f ( ˆD) ˆU † in the senses required for the theorem statement. Though ˆD is not diagonal, since it is s×s,
we can compute its unitary eigendecomposition U ( ˆD)D( ˆD)(U ( ˆD))†; so, we can take D := f (D( ˆD))
and N := (U ( ˆD))†(D(C)
)† to get the decomposition in the theorem statement. (Including
ε√
the isometry (U ( ˆD))† in our expression for ˆU does not change the value of α).

)+(U (C)
ε√

F/(ε/

(cid:16) ε3

L3(cid:107)A(cid:107)3

2L

2L

2L

2L

First, consider the eigenvalues of ˆD. Note that (cid:107) ˆU † ˆU − I(cid:107) ≤ α since ˆU is an α-approximate
isometry, and by Lemma 2.5, there exists an isometry U such that (cid:107)U − ˆU (cid:107) ≤ α. We ﬁrst observe
that, using Lemma 2.5 and our bound on α,

(cid:107)A − U ˆDU †(cid:107) ≤ (cid:107)A − ˆU ˆD ˆU †(cid:107) + (cid:107) ˆU ˆD ˆU † − U ˆDU †(cid:107)

≤

≤

ε
L
ε
L

+ α

+ α

2 − α
(1 − α)2 (cid:107) ˆU ˆD ˆU †(cid:107)
(cid:16)
ε
2 − α
(cid:107)A(cid:107) +
(1 − α)2
L

(cid:17)

(cid:46) ε
L

.

Consequently, by Weyl’s inequality (Lemma 4.11), the eigenvalues of U ˆDU †, ˆλ1 ≥ · · · ≥ ˆλn satisfy
|λi − ˆλi| (cid:46) ε
L for all i ∈ [n], and by assumption, f is L-Lipschitz on the spectrums of A and
U ˆDU . From this, we can conclude that we can compute estimates for the eigenvalues of f (A),
since the eigenvalues of U ˆDU † are the eigenvalues of ˆD (padded with zero eigenvalues) which we
know from our eigendecomposition of ˆD Further, our estimates f (ˆλi) satisfy the desired bound

61

|f (ˆλi) − f (λi)| ≤ ε. Finally, since f is Lipschitz on our spectrums of concern, the desired error bound
for our approximation holds by the following computation (which uses Lemma 2.5 extensively):

(cid:107)f (A) − ˆU f ( ˆD) ˆU †(cid:107) ≤ (cid:107)f (A) − U f ( ˆD)U †(cid:107) + (cid:107)U f ( ˆD)U † − ˆU f ( ˆD) ˆU †(cid:107)

≤ (cid:107)f (A) − U f ( ˆD)U †(cid:107) + (2α + α2)(cid:107)f ( ˆD)(cid:107)
(cid:46) L((cid:107)A − U ˆDU †(cid:107) + (2α + α2)(cid:107) ˆD(cid:107)) log s
≤ L((cid:107)A − ˆU ˆD ˆU †(cid:107) + 2(2α + α2)(cid:107) ˆD(cid:107)) log s

by Lemma 5.4

≤ L

≤ L

(cid:16) ε
L
(cid:16) ε
L

+

+

2(2α + α2)
(cid:17)
(1 − α)2 (cid:107) ˆU ˆD ˆU †(cid:107)
2(2α + α2)
(cid:16)
ε
(1 − α)2
L

(cid:107)A(cid:107) +

log s

(cid:17)(cid:17)

log s (cid:46) ε log s.

by α ≤ ε

L(cid:107)A(cid:107)

Finally, we rescale ε down by log2 s so that this ﬁnal bound is O(ε). This term is folded into the
polylog terms of r, c, and s. (We need to scale by more than log s because s has a dependence on
1
ε2 .) This completes the error analysis.

The complexity analysis takes some care: we want to compute our matrix expressions in the
correct order. First, we will sample to get S and T , and then compute the truncated singular
value decomposition of C := SAT , which we have denoted C ε√
∈
2L
Cr×s, D(C)
ε√
times to get our estimate M ∈ Cr×r, and compute the eigendecomposition of

∈ Cc×s. Then, we will perform the inner product estimation protocol r2

∈ Cs×s, V (C)
ε√

)† for U (C)
ε√

= U (C)
ε√

(V (C)
ε√

D(C)
ε√

2L

2L

2L

2L

2L

2L

ˆD = D(C)
ε√

2L

(U (C)
ε√

2L

)†¯π(CC†)M ¯π(CC†)U (C)
ε√

2L

D(C)
ε√

2L

= D(C)
ε√

2L

¯π((D(C)
ε√

2L

)2)(U (C)
ε√

2L

)†M U (C)
ε√

2L

¯π((D(C)
ε√

2L

)2)D(C)
ε√

2L

via the ﬁnal expression above, with the truncations propagated through the matrices, to get
ˆD = U ( ˆD)D( ˆD)(U ( ˆD))†. Then, we compute and output D = ˆD and N = (U ( ˆD))†(D(C)
)†.
ε√
By evaluating the expression for ˆD from left-to-right, we only need to perform matrix multiplications
that (naively) take s3 or sr2 time. The only cost of c we incur is in computing the SVD of C. The
runtime is

)+(U (C)
ε√

2L

2L

O(cid:0)(r + c + t)(sφ(A) + qφ(A)) + (rc + r2t) q(A) + s3 + r2s + r2c + nφ(A)(cid:1).

Acknowledgments

ET thanks Craig Gidney for the reference to alias sampling. AG is grateful to Saeed Mehraban
for insightful suggestions about proving perturbation bounds on partition functions. Part of this
work was done while visiting the Simons Institute for the Theory of Computing. We gratefully
acknowledge the Institute’s hospitality.

References

[Aar15]

Scott Aaronson. Read the ﬁne print. Nature Physics, 11(4):291, 2015.

[ACQ22]

Dorit Aharonov, Jordan Cotler, and Xiao-Liang Qi. Quantum algorithmic measure-
ment. Nature Communications, 13, 2022. arXiv: 2101.04634

62

[ADBL20]

Juan Miguel Arrazola, Alain Delgado, Bhaskar Roy Bardhan, and Seth Lloyd.
Quantum-inspired algorithms in practice. Quantum, 4:307, 2020. arXiv: 1905.10415

[AK16]

[AP10]

[AP11]

[vAG19]

Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semideﬁnite
programs. Journal of the ACM, 63(2):12:1–12:35, 2016. Earlier version in STOC’07.

Alexei B. Aleksandrov and Vladimir V. Peller. Operator H¨older–Zygmund functions.
Advances in Mathematics, 224(3):910–966, 2010. arXiv: 0907.3049

Alexei B. Aleksandrov and Vladimir V. Peller. Estimates of operator moduli of con-
tinuity. Journal of Functional Analysis, 261(10):2741–2796, 2011. arXiv: 1104.3553

Joran van Apeldoorn and Andr´as Gily´en. Improvements in quantum SDP-solving
with applications. In Proceedings of the 46th International Colloquium on Automata,
Languages, and Programming (ICALP), pages 99:1–99:15, 2019. arXiv: 1804.05058

[vAGGdW20] Joran van Apeldoorn, Andr´as Gily´en, Sander Gribling, and Ronald de Wolf. Quantum
SDP-solvers: Better upper and lower bounds. Quantum, 4:230, 2020. Earlier version
in FOCS’17. arXiv: 1705.01843

[ATS03]

[BCK15]

[BE95]

[Bel97]

[Bha97]

[BHK97]

[BKL+19]

Dorit Aharonov and Amnon Ta-Shma. Adiabatic quantum state generation and
statistical zero knowledge.
In Proceedings of the 35th ACM Symposium on the
Theory of Computing, pages 20–29, New York, NY, USA, 2003. ACM, Association
for Computing Machinery. arXiv: quant-ph/0301023

Dominic W. Berry, Andrew M. Childs, and Robin Kothari. Hamiltonian simulation
with nearly optimal dependence on all parameters. In Proceedings of the 56th IEEE
Symposium on Foundations of Computer Science (FOCS), pages 792–809, 2015.
arXiv: 1501.01715

Peter Borwein and Tam´as Erd´elyi. Polynomials and polynomial inequalities, volume
161 of Graduate Texts in Mathematics. Springer, New York, NY, USA, 1995.

R. Bellman. Introduction to matrix analysis. Society for Industrial and Applied
Mathematics, Philadelphia, PA, USA, second edition, 1997.

Rajendra Bhatia. Matrix Analysis, volume 169 of Graduate Texts in Mathematics.
Springer, 1997.

Peter N. Belhumeur, Jo˜ao P. Hespanha, and David J. Kriegman. Eigenfaces vs.
Fisherfaces: recognition using class speciﬁc linear projection. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 19(7):711–720, 1997.

Fernando G. S. L. Brand˜ao, Amir Kalev, Tongyang Li, Cedric Yen-Yu Lin, Krysta M.
Svore, and Xiaodi Wu. Quantum SDP solvers: Large speed-ups, optimality, and
applications to quantum learning. In Proceedings of the 46th International Colloquium
on Automata, Languages, and Programming (ICALP), pages 27:1–27:14, 2019. arXiv:
1710.02581

[BS17]

Fernando G. S. L. Brand˜ao and Krysta M. Svore. Quantum speed-ups for solving
semideﬁnite programs. In Proceedings of the 58th IEEE Symposium on Foundations
of Computer Science (FOCS), pages 415–426, 2017. arXiv: 1609.05537

63

[CCH+20]

[CD16]

[CGJ19]

[CGL+20]

[CHI+18]

[CLLW20]

[CLS+19]

[CLW18]

[CMM17]

[CVB20]

[CW17]

[DBH21]

Nadiia Chepurko, Kenneth L. Clarkson, Lior Horesh, Honghao Lin, and David P.
Woodruﬀ. Quantum-inspired algorithms from randomized numerical linear algebra,
2020. arXiv: 2011.04125

Iris Cong and Luming Duan. Quantum discriminant analysis for dimensionality
reduction and classiﬁcation. New Journal of Physics, 18(7):073011, jul 2016. arXiv:
1510.00113

Shantanav Chakraborty, Andr´as Gily´en, and Stacey Jeﬀery. The power of block-
encoded matrix powers: Improved regression techniques via faster Hamiltonian
simulation.
In Proceedings of the 46th International Colloquium on Automata,
Languages, and Programming (ICALP), pages 33:1–33:14, 2019. arXiv: 1804.01973

Nai-Hui Chia, Andr´as Gily´en, Han-Hsuan Lin, Seth Lloyd, Ewin Tang, and Chunhao
Wang. Quantum-inspired algorithms for solving low-rank linear equation systems with
logarithmic dependence on the dimension. In Proceedings of the 31st International
Symposium on Algorithms and Computation (ISAAC), pages 47:1–47:17, 2020.

Carlo Ciliberto, Mark Herbster, Alessandro Davide Ialongo, Massimiliano Pontil,
Andrea Rocchetto, Simone Severini, and Leonard Wossnig. Quantum machine
learning: a classical perspective. Proceedings of the Royal Society A: Mathematical,
Physical and Engineering Sciences, 474(2209):20170551, January 2018.

Nai-Hui Chia, Tongyang Li, Han-Hsuan Lin, and Chunhao Wang. Quantum-inspired
sublinear algorithm for solving low-rank semideﬁnite programming. In Proceedings of
the 45th International Symposium on Mathematical Foundations of Computer Science
(MFCS), pages 23:1–23:15, 2020. arXiv: 1901.03254

Zhihuai Chen, Yinan Li, Xiaoming Sun, Pei Yuan, and Jialin Zhang. A quantum-
inspired classical algorithm for separable non-negative matrix factorization.
In
Proceedings of the 28th International Joint Conference on Artiﬁcial Intelligence,
pages 4511–4517, Palo Alto, CA, USA, 2019. AAAI Press, AAAI. arXiv: 1907.05568

Nai-Hui Chia, Han-Hsuan Lin, and Chunhao Wang. Quantum-inspired sublinear
classical algorithms for solving low-rank linear systems. arXiv: 1811.04852, 2018.

Michael B. Cohen, Cameron Musco, and Christopher Musco. Input sparsity time
low-rank approximation via ridge leverage score sampling. In Proceedings of the
Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1758–
1777, Philadelphia, PA, USA, January 2017. Society for Industrial and Applied
Mathematics.

Daan Camps and Roel Van Beeumen. Approximate quantum circuit synthesis using
block encodings. Physical Review A, 102(5):052411, 2020. arXiv: 2007.01417

Kenneth L. Clarkson and David P. Woodruﬀ. Low-rank approximation and regression
in input sparsity time. J. ACM, 63(6), January 2017.

Chen Ding, Tian-Yi Bao, and He-Liang Huang. Quantum-inspired support vector
machine. IEEE Transactions on Neural Networks and Learning Systems, pages 1–13,
2021. arXiv: 1906.08902

64

[DHLT20]

[DKM06]

[DKR02]

[DKW18]

[DM07]

[DMM08]

[DW20]

[Fey51]

[Fey82]

[FKV04]

[GCD20]

[Gil10]

[GLM08]

[GLT18]

[GR02]

Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, and Dacheng Tao. Quantum-inspired
algorithm for general minimum conical hull problems. Physical Review Research,
2(3):033199, 2020. arXiv: 1907.06814

Petros Drineas, Ravi Kannan, and Michael W. Mahoney. Fast Monte Carlo algorithms
for matrices I: Approximating matrix multiplication. SIAM Journal on Computing,
36(1):132–157, 2006.

Petros Drineas, Iordanis Kerenidis, and Prabhakar Raghavan. Competitive recom-
mendation systems. In Proceedings of the 34th ACM Symposium on the Theory
of Computing (STOC), pages 82–90, New York, NY, USA, 2002. Association for
Computing Machinery.

Yogesh Dahiya, Dimitris Konomis, and David P Woodruﬀ. An empirical evaluation
of sketching for numerical linear algebra. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, pages 1292–1300,
New York, NY, USA, 2018. ACM, Association for Computing Machinery.

Petros Drineas and Michael W. Mahoney. A randomized algorithm for a tensor-
based generalization of the singular value decomposition. Linear Algebra and its
Applications, 420(2-3):553–571, 2007.

Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Relative-error CUR
matrix decompositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844–
881, January 2008.

Vedran Dunjko and Peter Wittek. A non-review of Quantum Machine Learning:
trends and explorations. Quantum Views, 4:32, March 2020.

Richard P. Feynman. An operator calculus having applications in quantum electro-
dynamics. Physical Review, 84:108–128, 1951.

Richard P. Feynman. Simulating physics with computers. International Journal of
Theoretical Physics, 21(6-7):467–488, 1982.

Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast Monte-Carlo algorithms for
ﬁnding low-rank approximations. Journal of the ACM, 51(6):1025–1041, 2004.

Casper Gyurik, Chris Cade, and Vedran Dunjko. Towards quantum advantage via
topological data analysis, 2020. arXiv: 2005.02607

Michael I. Gil. Perturbations of functions of diagonalizable matrices. Electronic
Journal of Linear Algebra, 20:303–313, 2010.

Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. Quantum random access
memory. Physical Review Letters, 100(16):160501, 2008. arXiv: 0708.1879

Andr´as Gily´en, Seth Lloyd, and Ewin Tang. Quantum-inspired low-rank stochastic
regression with logarithmic dependence on the dimension. arXiv: 1811.04909, 2018.

Lov Grover and Terry Rudolph. Creating superpositions that correspond to eﬃciently
integrable probability distributions. arXiv: quant-ph/0208112, 2002.

65

[GSLW19]

Andr´as Gily´en, Yuan Su, Guang Hao Low, and Nathan Wiebe. Quantum singular
value transformation and beyond: Exponential improvements for quantum matrix
arithmetics. In Proceedings of the 51st ACM Symposium on the Theory of Computing
(STOC), pages 193–204, 2019. arXiv: 1806.01838

[HHL09]

[HKP21]

[HKS11]

[JLGS20]

[Kal07]

[KP17]

[KP20]

[KS48]

[KV17]

[LC17]

[LGZ16]

Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for
linear systems of equations. Physical Review Letters, 103(15):150502, 2009. arXiv:
0811.3171

Hsin-Yuan Huang, Richard Kueng, and John Preskill. Information-theoretic bounds
on quantum advantage in machine learning. Physical Review Letters, 126(19):190505,
2021. arXiv: 2101.02464

Elad Hazan, Tomer Koren, and Nati Srebro. Beating SGD: Learning SVMs in
sublinear time. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q.
Weinberger, editors, Advances in Neural Information Processing Systems 24, pages
1233–1241, Red Hook, NY, USA, 2011. Curran Associates, Inc.

Dhawal Jethwani, Fran¸cois Le Gall, and Sanjay K. Singh. Quantum-inspired classical
algorithms for singular value transformation. In Proceedings of the 45th International
Symposium on Mathematical Foundations of Computer Science (MFCS), pages 53:1–
53:14, 2020. arXiv: 1910.05699

Satyen Kale. Eﬃcient algorithms using the multiplicative weights update method.
PhD thesis, Princeton University, 2007.

Iordanis Kerenidis and Anupam Prakash. Quantum recommendation systems. In
Proceedings of the 8th Innovations in Theoretical Computer Science Conference
(ITCS), pages 49:1–49:21, 2017. arXiv: 1603.08675

Iordanis Kerenidis and Anupam Prakash. Quantum gradient descent for linear systems
and least squares. Physical Review A, 101(2):022316, 2020. arXiv: 1704.04992

Robert Karplus and Julian Schwinger. A note on saturation in microwave spectroscopy.
Physical Review, 73:1020–1026, 1948.

Ravindran Kannan and Santosh Vempala. Randomized algorithms in numerical
linear algebra. Acta Numerica, 26:95–135, 2017.

Guang Hao Low and Isaac L. Chuang. Optimal Hamiltonian simulation by quantum
signal processing. Physical Review Letters, 118(1):010501, 2017. arXiv: 1606.02685

Seth Lloyd, Silvano Garnerone, and Paolo Zanardi. Quantum algorithms for topolog-
ical and geometric analysis of data. Nature Communications, 7:10138, 2016. arXiv:
1408.3106

[Llo96]

Seth Lloyd. Universal quantum simulators. Science, 273(5278):1073–1078, 1996.

[LMR13]

[LMR14]

Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum algorithms for
supervised and unsupervised machine learning, 2013. arXiv: 1307.0411

Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum principal component
analysis. Nature Physics, 10:631–633, 2014. arXiv: 1307.0401

66

[LRS15]

[Mah11]

[McD89]

[MMD08]

[MRTC21]

[PC99]

[Pra14]

[Pre18]

[RL18]

[RML14]

[RSW+19]

[RV07]

[RWC+20]

[Sho97]

James R. Lee, Prasad Raghavendra, and David Steurer. Lower bounds on the size of
semideﬁnite programming relaxations. In Proceedings of the 47th ACM Symposium
on the Theory of Computing (STOC), pages 567–576, 2015. arXiv: 1411.6317

Michael W. Mahoney. Randomized algorithms for matrices and data. Foundations
and Trends® in Machine Learning, 3(2):123–224, 2011.

Colin McDiarmid. On the method of bounded diﬀerences, page 148–188. London
Mathematical Society Lecture Note Series. Cambridge University Press, Cambridge,
England, 1989.

Michael W. Mahoney, Mauro Maggioni, and Petros Drineas. Tensor-CUR decompo-
sitions for tensor-based data. SIAM Journal on Matrix Analysis and Applications,
30(3):957–987, 2008.

John M. Martyn, Zane M. Rossi, Andrew K. Tan, and Isaac L. Chuang. Grand
uniﬁcation of quantum algorithms. Physical Review X, 2(4):040203, 2021. arXiv:
2105.02859

Victor Y. Pan and Zhao Q. Chen. The complexity of the matrix eigenproblem. In
Proceedings of the 31st ACM Symposium on the Theory of Computing (STOC), page
507–516, 1999.

Anupam Prakash. Quantum Algorithms for Linear Algebra and Machine Learning.
PhD thesis, University of California at Berkeley, 2014.

John Preskill. Quantum Computing in the NISQ era and beyond. Quantum, 2:79,
2018. arXiv: 1801.00862

Patrick Rebentrost and Seth Lloyd. Quantum computational ﬁnance: quantum
algorithm for portfolio optimization. arXiv: 1811.03975, 2018.

Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. Quantum support vector
machine for big data classiﬁcation. Physical Review Letters, 113(13):130503, 2014.
arXiv: 1307.0471

Patrick Rebentrost, Maria Schuld, Leonard Wossnig, Francesco Petruccione, and Seth
Lloyd. Quantum gradient descent and Newton’s method for constrained polynomial
optimization. New Journal of Physics, 21(7):073023, 2019. arXiv: 1612.01789

Mark Rudelson and Roman Vershynin. Sampling from large matrices: An approach
through geometric functional analysis. Journal of the ACM (JACM), 54(4):21–es,
July 2007.

Alessandro Rudi, Leonard Wossnig, Carlo Ciliberto, Andrea Rocchetto, Massimiliano
Pontil, and Simone Severini. Approximating Hamiltonian dynamics with the Nystr¨om
method. Quantum, 4:234, 2020. arXiv: 1804.02484

Peter W. Shor. Polynomial-time algorithms for prime factorization and discrete
logarithms on a quantum computer. SIAM Journal on Computing, 26(5):1484–1509,
1997. Earlier version in FOCS’94. arXiv: quant-ph/9508027

67

[SWZ16]

[Tan19]

[Tan21]

[Tao10]

[VdN11]

[Vos91]

[Wel09]

[Woo14]

[WZP18]

[YSSK20]

Zhao Song, David Woodruﬀ, and Huan Zhang. Sublinear time orthogonal tensor
decomposition. In Advances in Neural Information Processing Systems 29, pages
793–801. Curran Associates, Inc., Red Hook, NY, USA, 2016.

Ewin Tang. A quantum-inspired classical algorithm for recommendation systems.
In Proceedings of the 51st ACM Symposium on the Theory of Computing (STOC),
pages 217–228, 2019. arXiv: 1807.04271

Ewin Tang. Quantum principal component analysis only achieves an exponential
speedup because of its state preparation assumptions. Physical Review Letters,
127(6):060503, 2021. arXiv: 1811.00414

Terence Tao.
mitian matrices,
254a-notes-3a-eigenvalues-and-sums-of-hermitian-matrices/.

254a, Notes

of Her-
and
3a:
https://terrytao.wordpress.com/2010/01/12/

Eigenvalues

2010.

sums

Maarten Van den Nest. Simulating quantum computers with probabilistic methods.
Quantum Information and Computation, 11(9&10):784–812, 2011. arXiv: 0911.1624

Michael D. Vose. A linear algorithm for generating random numbers with a given
distribution. IEEE Transactions on Software Engineering, 17(9):972–975, 1991.

Max Welling. Fisher linear discriminant analysis. https://www.ics.uci.edu/∼welling/
teaching/273ASpring09/Fisher-LDA.pdf, 2009.

David P. Woodruﬀ. Sketching as a tool for numerical linear algebra. Foundations
and Trends® in Theoretical Computer Science, 10(1–2):1–157, 2014.

Leonard Wossnig, Zhikuan Zhao, and Anupam Prakash. Quantum linear system
algorithm for dense matrices. Physical Review Letters, 120(5):050502, 2018. arXiv:
1704.06174

Hayata Yamasaki, Sathyawageeswar Subramanian, Sho Sonoda, and Masato Koashi.
Learning with optimized random features: Exponential speedup by quantum machine
learning without sparsity and low-rank assumptions. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information
Processing Systems, volume 33, pages 13674–13687. Curran Associates, Inc., Red
Hook, NY, USA, 2020. arXiv: 2004.10756

[ZFF19]

Zhikuan Zhao, Jack K. Fitzsimons, and Joseph F. Fitzsimons. Quantum-assisted Gaus-
sian process regression. Physical Review A, 99(5):052331, 2019. arXiv: 1512.03929

A Proof sketch for Remark 6.4

Recall that we wish to show that, given SQ(A), we can simulate SQφ(B) for B such that (cid:107)B − A†(cid:107) ≤
ε(cid:107)A(cid:107) with probability ≥ 1 − δ.

Following the argument from Remark 6.8, we can ﬁnd a B := AR†¯t(CC†)R satisfying the above
(rescaling ε appropriately). Here, R and ¯t(CC†) come from an
property in (cid:101)O
application of Theorem 5.1 with t : R → C a smooth step function that goes from zero to one around
(ε(cid:107)A(cid:107))2. If we had sampling and query access to the columns of AR†, we would be done, since then

(cid:16) (cid:107)A(cid:107)28
(cid:107)A(cid:107)28ε22 log3 1
F

(cid:17)

δ

68

i=1

(cid:80)r

j=1[¯t(CC†)](i, j)[A(cid:48)R(cid:48)†](·, i)R(j, ·), and we can express B as a sum of r2 outer products

B = (cid:80)r
of vectors that we have sampling and query access to. This gives us both SQφ(B) and SQφ(B†).
ε(cid:107)A(cid:107)/2, for U DV † the
ε(cid:107)A(cid:107)/2 the SVD truncated to singular values at least ε(cid:107)A(cid:107)/2, we can

We won’t get exactly this, but using that ¯t(CC†) = (C+

ε(cid:107)A(cid:107)/2)†t(C†C)C+

SVD of C and Uε(cid:107)A(cid:107)/2Dε(cid:107)A(cid:107)/2V †
rewrite

B = A(R†Uε(cid:107)A(cid:107)/2D+

ε(cid:107)A(cid:107)/2)(t(D2)D+

ε(cid:107)A(cid:107)/2U †

ε(cid:107)A(cid:107)/2)R.

Now it suﬃces to get sampling and query access to the columns of A(R†Uε(cid:107)A(cid:107)/2D+
Lemma 4.14, R†Uε(cid:107)A(cid:107)/2D+
norms of these columns, using that R†R ≈ A†A and CC† ≈ RR†.

ε(cid:107)A(cid:107)/2), and by
ε(cid:107)A(cid:107)/2 is an ε3-approximate isometry. Further, we can lower bound the

(cid:107)A(R†Uε(cid:107)A(cid:107)/2D+

ε(cid:107)A(cid:107)/2)†RA†AR†(Uε(cid:107)A(cid:107)/2D+
ε(cid:107)A(cid:107)/2)†RR†RR†(Uε(cid:107)A(cid:107)/2D+

ε(cid:107)A(cid:107)/2)(cid:107)
ε(cid:107)A(cid:107)/2)(cid:107)

ε(cid:107)A(cid:107)/2)(cid:107)2 = (cid:107)(Uε(cid:107)A(cid:107)/2D+
≈ (cid:107)(Uε(cid:107)A(cid:107)/2D+
= (cid:107)RR†(Uε(cid:107)A(cid:107)/2D+
≈ (cid:107)CC†Uε(cid:107)A(cid:107)/2D+
= (cid:107)U D2U †Uε(cid:107)A(cid:107)/2D+
≥ ε2(cid:107)A(cid:107)2

ε(cid:107)A(cid:107)/2)(cid:107)2
ε(cid:107)A(cid:107)/2(cid:107)2

ε(cid:107)A(cid:107)/2(cid:107)2

Consider one particular column v := [R†Uε(cid:107)A(cid:107)/2D+
ε(cid:107)A(cid:107)/2](·, (cid:96)); summarizing our prior arguments,
2 from approximate orthonormality and (cid:107)Av(cid:107) (cid:38) ε(cid:107)A(cid:107), which we just showed.
we know (cid:107)v(cid:107) ≥ 1
We can also query for entries of v since it is a linear combination of rows of R. We make one
more approximation Av ≈ u, using Lemma 4.12 as we do in Corollary 6.7. That is, if we want to
know [Av](i) = A(i, ·)v, we use our inner product protocol to approximate it to γ(cid:107)A(i, ·)(cid:107)(cid:107)v(cid:107) error,
and declare it to be u(i). This implicitly deﬁnes u via an algorithm to compute its entries from
SQ(A) and Q(v). Let B(cid:48) be the version of B, with the columns of AR†Uε(cid:107)A(cid:107)/2D+
ε(cid:107)A(cid:107)/2 replaced
with their u versions. One can set γ such that the correctness bound (cid:107)B(cid:48) − A†(cid:107) (cid:46) ε and our lower
bound u (cid:38) ε(cid:107)A(cid:107) both still hold. All we need now to get SQφ(u) (thereby completing our proof
sketch) is a bound ˜u such that we have SQ(˜u). We will take ˜u(i) := 2(cid:107)A(i, ·)(cid:107). We have SQ(˜u)
immediately from SQ(A), φ = (cid:107)˜u(cid:107)2/(cid:107)u(cid:107)2 (cid:46) ε2(cid:107)A(cid:107)2
F/(cid:107)A(cid:107)2 (from our lower bound on (cid:107)u(cid:107)), and
|˜u(i)| ≥ (cid:107)A(i, ·)(cid:107) + γ(cid:107)A(i, ·)(cid:107)v(cid:107) ≥ |u(i)| (from our correctness bound from Lemma 4.12).

B Deferred proofs

Lemma 2.5. If ˆX ∈ Cm×n is an α-approximate isometry, then there is an exact isometry X ∈ Cm×n
with the same columnspace as ˆX such that (cid:107) ˆX − X(cid:107) ≤ α. Furthermore, for any matrix Y ∈ Cn×n,

(cid:107) ˆXY ˆX † − XY X †(cid:107) ≤ (2α + α2)(cid:107)Y (cid:107).

If α < 1, then (cid:107) ˆX +(cid:107) ≤ (1 − α)−1 and

(cid:107) ˆXY ˆX † − XY X †(cid:107) ≤ α

2 − α
(1 − α)2 (cid:107) ˆXY ˆX †(cid:107).

Proof. Let ˆX = U DV † be a singular value decomposition of ˆX, with singular values σ1, . . . , σn and
U ∈ Cm×n, D ∈ Rn×n, V ∈ Cn×n. We set X := U V † which is an isometry since (U V †)†U V † = I,

69

and has the same columnspace as ˆX, and

(cid:107)U V † − ˆX(cid:107) = (cid:107)U V † − U DV †(cid:107) = (cid:107)I − D(cid:107) = max
i∈[n]

|1 − σi| ≤ max
i∈[n]

|1 − σi||1 + σi|

= max
i∈[n]

|1 − σ2

i | = (cid:107)I − D2(cid:107) = (cid:107)I − V DU †U DV †(cid:107) = (cid:107)I − ˆX † ˆX(cid:107) ≤ α.

Consequently,

(cid:107) ˆXY ˆX † − XY X †(cid:107) ≤ (cid:107) ˆXY ( ˆX − X)†(cid:107) + (cid:107)( ˆX − X)Y X(cid:107)

≤ α((cid:107) ˆXY (cid:107) + (cid:107)Y X(cid:107))
≤ α((cid:107)XY (cid:107) + α(cid:107)Y (cid:107) + (cid:107)Y X(cid:107))
= (2α + α2)(cid:107)Y (cid:107)

Suppose α < 1, ruling out the possibility that ˆX is the zero matrix. Then by Lemma 4.11 we have

1
1 − α
(cid:107) ˆXY ˆX † − XY X †(cid:107) ≤ α((cid:107) ˆXY (cid:107) + (cid:107)Y (cid:107))

(cid:107) ˆX +(cid:107) = max
i∈[n]

1
σi

≤

, and consequently

≤ α((cid:107) ˆXY ˆX †(cid:107)(cid:107) ˆX +(cid:107) + (cid:107) ˆXY ˆX †(cid:107)(cid:107) ˆX +(cid:107)2)

≤ α

1 − α + 1
(1 − α)2 (cid:107) ˆXY ˆX †(cid:107).

Lemma 4.5 (Matrix multiplication by subsampling [DKM06, Theorem 1]). Suppose we are given
X ∈ Cn×m, Y ∈ Cn×p, r ∈ N and a distribution p ∈ Rn satisfying the oversampling condition that,
for some φ ≥ 1,

p(k) ≥

(cid:107)X(k, ·)(cid:107)(cid:107)Y (k, ·)(cid:107)

φ (cid:80)

(cid:96) (cid:107)X((cid:96), ·)(cid:107)(cid:107)Y ((cid:96), ·)(cid:107)

.

Let S ∈ Rr×n be sampled according to p. Then X †S†SY is an unbiased estimator for X †Y and

(cid:104)
(cid:107)X †S†SY − X †Y (cid:107)F <

Pr

(cid:114)

8φ2 ln(2/δ)
r

(cid:88)

(cid:96)

(cid:124)

(cid:107)X((cid:96), ·)(cid:107)(cid:107)Y ((cid:96), ·)(cid:107)

(cid:105)

> 1 − δ.

(cid:123)(cid:122)
≤(cid:107)X(cid:107)F(cid:107)Y (cid:107)F

(cid:125)

70

Proof. Using that the rows of S are selected independently, we can conclude the following:

E[(SX)†(SY )] = r · E[[SX](1, ·)†[SY ](1, ·)] = r

n
(cid:88)

i=1

p(i)

X(i, ·)†Y (i, ·)
rp(i)

= X †Y

E[(cid:107)X †S†SY − X †Y (cid:107)2

F] =

m
(cid:88)

p
(cid:88)

E(cid:2)(cid:12)

(cid:12)[X †S†SY − X †Y ](i, j)(cid:12)
2(cid:3)
(cid:12)

i=1
m
(cid:88)

j=1
p
(cid:88)

= r

E(cid:2)(cid:12)

(cid:12)[SX](1, i)†[SY ](1, j) − [X †Y ](i, j)(cid:12)
(cid:12)

2(cid:3)

i=1
m
(cid:88)

j=1
p
(cid:88)

i=1

j=1

≤ r

E(cid:2)(cid:12)

(cid:12)[SX](1, i)†[SY ](1, j)(cid:12)
2(cid:3)
(cid:12)

= rE(cid:2)(cid:107)[SX](1, ·)(cid:107)2(cid:107)[SY ](1, ·)(cid:107)2(cid:3)
(cid:107)Y (k, ·)(cid:107)2
(cid:107)X(k, ·)(cid:107)2
r · p(k)
r · p(k)

n
(cid:88)

p(k)

= r

(cid:107)X(k, ·)(cid:107)2(cid:107)Y (k, ·)(cid:107)2

k=1
n
(cid:88)

k=1
n
(cid:88)

1
p(k)
φ (cid:80)

=

≤

=

1
r

1
r

φ
r

(cid:96) (cid:107)X((cid:96), ·)(cid:107)(cid:107)Y ((cid:96), ·)(cid:107)

(cid:107)X(k, ·)(cid:107)(cid:107)Y (k, ·)(cid:107)

(cid:107)X(k, ·)(cid:107)(cid:107)Y (k, ·)(cid:107)

(cid:17)2

.

k=1
(cid:16) (cid:88)

k

(cid:107)X(k, ·)(cid:107)2(cid:107)Y (k, ·)(cid:107)2

To prove concentration, we use McDiarmid’s “independent bounded diﬀerence inequality” [McD89].

Lemma B.1 ([McD89, Lemma (1.2)]). Let X1, . . . , Xc be independent random variables with Xs
taking values in a set As for all s ∈ [c]. Suppose that f is a real valued measurable function on the
product set ΠsAs such that |f (x) − f (x(cid:48))| ≤ bs whenever the vectors x and x(cid:48) diﬀer only in the s-th
coordinate. Let Y be the random variable f [X1, . . . , Xc]. Then for any γ > 0:

Pr[|Y − E[Y ]| ≥ γ] ≤ 2 exp

(cid:16)

−

2γ2
(cid:80)
s b2
s

(cid:17)

.

To use Lemma B.1, we think about this expression as a function of the indices that are randomly

chosen from p. That is, let f be the function [n]r → R deﬁned to be

f (i1, i2, . . . , ir) :=

(cid:13)
(cid:13)X †Y −
(cid:13)

r
(cid:88)

s=1

1
r · p(is)

X(is, ·)†Y (is, ·)

(cid:13)
(cid:13)
(cid:13)F

,

Then, by Jensen’s inequality, we have

E[f ] = E[(cid:107)X †S†SY − X †Y (cid:107)F] ≤

(cid:113)

E[(cid:107)X †S†SY − XY (cid:107)2

F] ≤

(cid:114)

φ
r

(cid:88)

k

(cid:107)X(k, ·)(cid:107)(cid:107)Y (k, ·)(cid:107).

71

Now suppose that the index sequences (cid:126)i and (cid:126)i(cid:48) only diﬀer at the s-th position. Then by the triangle
inequality,

|f ((cid:126)i) − f ((cid:126)i(cid:48))| ≤

≤

1
r

2
r

(cid:13)
(cid:13)
(cid:13)

1
p(is)
(cid:13)
(cid:13)
(cid:13)

max
k∈[n]

1
p(k)

X(is, ·)†Y (is, ·) −

X(i(cid:48)

s, ·)†Y (i(cid:48)

(cid:13)
(cid:13)
s, ·)
(cid:13)F

X(k, ·)†Y (k, ·)

2φ
r

n
(cid:88)

k=1

(cid:107)X(k, ·)(cid:107)(cid:107)Y (k, ·)(cid:107).

1
p(i(cid:48)
s)
(cid:13)
(cid:13)
(cid:13)F

≤

Now, by Lemma B.1, we conclude that

(cid:34)

Pr

|f − E[f ]| ≥

(cid:114)

2φ2 ln(2/δ)
r

(cid:88)

k

(cid:35)

(cid:107)X(·, k)(cid:107)(cid:107)Y (k, ·)(cid:107)

≤ δ.

So, with probability ≥ 1 − δ,

(cid:107)X †S†SY − X †Y (cid:107)F ≤ E[(cid:107)X †S†SY − X †Y (cid:107)F] +

(cid:114)

2φ2 ln(2/δ)
r

(cid:88)

k

(cid:107)X(·, k)(cid:107)(cid:107)Y (k, ·)(cid:107)

≤

≤

(cid:114)

(cid:16)(cid:114)

φ
r

+

2φ2 ln(2/δ)
r

(cid:17) (cid:88)

k

(cid:107)X(·, k)(cid:107)(cid:107)Y (k, ·)(cid:107)

(cid:114)

8φ2 ln(2/δ)
r

(cid:88)

k

(cid:107)X(·, k)(cid:107)(cid:107)Y (k, ·)(cid:107).

Lemma 4.12 (Inner product estimation, [Tan19, Proposition 4.2]). Given SQφ(u), Q(v) ∈ Cn,
we can output an estimate c ∈ C such that |c − (cid:104)u, v(cid:105)| ≤ ε with probability ≥ 1 − δ in time
O(cid:0)φ(cid:107)u(cid:107)2(cid:107)v(cid:107)2 1

δ (sqφ(u) + q(v))(cid:1).

ε2 log 1

Proof. Deﬁne a random variable Z by sampling an index from the distribution p given by SQφ(u),
and setting Z := u(i)v(i)/p(i). Then

E[Z] = (cid:104)u, v(cid:105)

and E[|Z|2] =

n
(cid:88)

i=1

p(i)

|u(i)v(i)|2
p(i)2

≤

n
(cid:88)

i=1

|u(i)v(i)|2 φ(cid:107)u(cid:107)2

|u(i)|2 = φ(cid:107)u(cid:107)2(cid:107)v(cid:107)2.

So, we just need to boost the quality of this random variable. Consider taking ¯Z to be the mean
of x := 8φ(cid:107)u(cid:107)2(cid:107)v(cid:107)2 1
ε2 independent copies of Z. Then, by Chebyshev’s inequality (stated here for
complex-valued random variables),

√

2 Var[Z]

1
4

Pr[| ¯Z − E[ ¯Z]| ≥ ε/

2] ≤

xε2 ≤
δ independent copies of ¯Z, which we call
Next, we take the (component-wise) median of y := 8 log 1
˜Z, to decrease failure probability. Consider the median of the real parts of ¯Z. The key observation
is that if (cid:60)( ˜Z − E[Z]) ≥ ε/
2. Let
√
Ei = χ((cid:60)( ¯Zi − E[Z]) ≥ ε/
2) be the characteristic function for this event for a particular mean.
The above argument implies that Pr[Ei] ≤ 1

2, then at least half of the ¯Z’s satisfy (cid:60)( ¯Z − E[Z]) ≥ ε/

4 . So, by Hoeﬀding’s inequality,

√

√

.

(cid:34)

Pr

1
q

q
(cid:88)

i=1

Ei ≥

(cid:34)

≤ Pr

(cid:35)

1
2

1
q

q
(cid:88)

i=1

Ei ≥

1
4

(cid:35)

+ Pr[Ei]

≤ exp(−q/8) ≤

δ
2

.

72

With this combined with our key observation, we can conclude that Pr[(cid:60)( ˜Z − (cid:104)u, v(cid:105)) ≥ ε/
2] ≤ δ/2.
From a union bound together with the analogous argument for the imaginary component, we have
Pr[| ˜Z − (cid:104)u, v(cid:105)| ≥ ε] ≤ δ as desired. The time complexity is the number of samples multiplied by the
time to create one instance of the random variable Z, which is O(sq(u) + q(v)).

√

Lemma 6.3. Consider p(x) a degree-d polynomial of parity-d such that |p(x)| ≤ 1 for x ∈ [−1, 1].
Recall that, for a function f : C → C, we deﬁne ¯f (x) := (f (x) − f (0))/x (and ¯f (0) = f (cid:48)(0) when f
is diﬀerentiable at zero).

• If p is even, then max
x∈[0,1]

|q(x)| ≤ 1, max

|q(cid:48)(x)| (cid:46) d2, max
x∈[−1,1]

|¯q(x)| (cid:46) d2, and max
x∈[−1,1]

|¯q(cid:48)(x)| (cid:46) d4.

x∈[−1,1]

• If p is odd, then max
x∈[−1,1]

|q(x)| (cid:46) d, max
x∈[−1,1]

|q(cid:48)(x)| (cid:46) d3, max
x∈[−1,1]

|¯q(x)| (cid:46) d3, and max
x∈[−1,1]

|¯q(cid:48)(x)| (cid:46) d5.

Proof. We use the following Markov-Bernstein inequality [BE95, 5.1.E.17.f]. For every p ∈ C[x] of
degree at most d

max
x∈[−1,1]

|p(k)(x)| (cid:46)

(cid:16)

(cid:16)

min

d2,

√

(cid:17)(cid:17)k

d
1 − x2

max
x∈[−1,1]

|p(x)|,

(24)

where (cid:46) hides a constant depending on k. Note that by replacing x in the above equation with 2y −1,
we get that maxy∈[0,1]|p(k)(y)| (cid:46) d2k maxy∈[0,1]|p(y)| (paying an additional 2k constant factor).

We make a couple observations about ¯r(x) using Taylor expansions, where r(x) is any degree-d

polynomial. First,

¯r(x) =

r(x) − r(0)
x

=

r(x) − (r(x) − r(cid:48)(y)x)
x

= r(cid:48)(y),

where y ∈ [0, x] comes from the remainder term of the Taylor expansion of r(x) at x. Similarly,

¯r(cid:48)(x) =

(cid:16) r(x) − r(0)
x

(cid:17)(cid:48)

=

1
x2

(cid:16)

(cid:17)
xr(cid:48)(x) − r(x) + r(0)

for some y ∈ [0, x]. Then, for p even, maxx∈[0,1] |q(x)| ≤ 1 by deﬁnition. We also have

=

(cid:16)

1
x2

xr(cid:48)(x) − r(x) + r(x) − r(cid:48)(x)x + r(cid:48)(cid:48)(y)

(cid:17)

x2
2

=

1
2

r(cid:48)(cid:48)(y)

max
x∈[0,1]

|q(cid:48)(x)| (cid:46) d2 max
x∈[0,1]

|q(x)| ≤ d2

max
x∈[0,1]

|¯q(x)| ≤ max
y∈[0,1]

|q(cid:48)(y)| (cid:46) d2

max
x∈[0,1]

|¯q(cid:48)(x)| ≤ max
y∈[0,1]

1
2

|q(cid:48)(cid:48)(y)| (cid:46) d4

For p odd, the same argument applies provided we can show that maxx∈[0,1] |q(x)| (cid:46) d, which we do
by splitting into two cases: x ≤ 1

2 and x > 1
2 .

max
x∈[0, 1
2 ]

|q(x)| = max
x∈[0, 1
2 ]

(cid:12)
(cid:12)
(cid:12)

p(x)
x

(cid:12)
(cid:12)
(cid:12) = max
y∈[0, 1
2 ]

|p(cid:48)(y)| (cid:46) max
x∈[0, 1
2 ]

√

d
1 − x2

max
x∈[−1,1]

|p(x)| (cid:46) d;

max
x∈( 1
2 ,1]

|q(x)| = max
2 ,1]

x∈( 1

(cid:12)
(cid:12)
(cid:12)

p(x)
x

(cid:12)
(cid:12)
(cid:12) ≤ max
x∈( 1
2 ,1]

|2p(x)| ≤ 2.

73

Corollary 6.18. Given SQ(X T ) and SQ(y), with probability ≥ 1 − δ, we can output a real
number ˆb such that |b − ˆb| ≤ ε(1 + b) and SQφ(ˆα) such that (cid:107)ˆα − α(cid:107) ≤ εγ(cid:107)y(cid:107), where α and b
(cid:1) time, with (cid:102)sqφ(ˆα) =
come from Eq. (12). Our algorithm runs in (cid:101)O(cid:0)(cid:107)X(cid:107)6
F(cid:107)X(cid:107)16γ11ε−6 log3 1
δ
. Note that when γ−1/2 is chosen to be suﬃciently large (e.g.
(cid:101)O
O((cid:107)X(cid:107)F)) and (cid:107)α(cid:107) = Ω(γ(cid:107)y(cid:107)), this runtime is dimension-independent.

F(cid:107)X(cid:107)6γ5 γ2m

(cid:107) ˆα(cid:107)2 ε−4 log2 1

(cid:107)X(cid:107)4

(cid:16)

(cid:17)

δ

Proof. Denote σ2 := γ−1, and redeﬁne X ← X T (so we have SQ(X) instead of SQ(X T )). By the
block matrix inversion formula31 we know that

(cid:34)

(cid:104) 0 (cid:126)1T
(cid:126)1 M

(cid:105)−1

=

−

1
(cid:126)1T M −1(cid:126)1
M −1(cid:126)1
(cid:126)1T M −1(cid:126)1

(cid:126)1T M −1
(cid:126)1T M −1(cid:126)1
M −1− M −1(cid:126)1(cid:126)1T M −1

(cid:126)1T M −1(cid:126)1

(cid:35)

⇒

(cid:104) 0 (cid:126)1T
(cid:126)1 M

(cid:105)−1(cid:2) 0
y

(cid:3) =

(cid:34)

M −1

(cid:126)1T M −1y
(cid:126)1T M −1(cid:126)1
(cid:16)

y−

(cid:126)1T M −1y
(cid:126)1T M −1(cid:126)1

(cid:35)
.

(cid:17)
(cid:126)1

So, we have reduced the problem of inverting the modiﬁed matrix to just inverting M −1 where
M = X T X + σ−2I. M is invertible because M (cid:23) σ2I. Note that M −1 = f (X T X), where

f (λ) :=

1
λ + σ2 .

So, by Theorem 5.1, we can ﬁnd R† ¯f (CC†)R such that (cid:107)R† ¯f (CC†)R + 1
where (because L = σ−4, ¯L = σ−6)

σ2 I − f (X T X)(cid:107) ≤ εσ−2,

r = (cid:101)O

c = (cid:101)O

(cid:18) L2(cid:107)A(cid:107)2(cid:107)A(cid:107)2

Fσ4

ε2

(cid:18) ¯L2(cid:107)A(cid:107)6(cid:107)A(cid:107)2

Fσ4

ε2

log

log

(cid:19)

(cid:19)

1
δ
1
δ

= (cid:101)O

(cid:18) Kκ

ε2 log

= (cid:101)O

(cid:18) Kκ3
ε2

log

(cid:19)
,

1
δ
1
δ

(cid:19)

.

log3 1
So, the runtime for estimating this is (cid:101)O
δ
we ﬁnd r1 ≈ R†(cid:126)1, ry ≈ R†(cid:126)y, and γ ≈ (cid:126)1†y in O(r K
time (for the last one) such that the following bounds hold:
√

ε2 log 1

√

(cid:107)R†(cid:126)1 − r1(cid:107) ≤ ε

mσ

(cid:107)R†y − ry(cid:107) ≤ ε

mσ

(cid:16) K3κ5
ε6

(cid:17)

. We further approximate using Lemma 4.6:
ε2 log 1
δ )

δ ) time (for the ﬁrst two) and O( 1

|(cid:126)1†y − γ| ≤ εm

(25)

Via Lemma 5.2, we observe the following additional bounds:

(cid:107)M −1(cid:107) ≤ σ−2

(cid:107)R†( ¯f (CC†))1/2(cid:107) ≤ (1 + ε)σ−1

(cid:107)( ¯f (CC†))1/2(cid:107) ≤ σ−2

(26)

Now, we compute what the subsequent errors are for replacing M −1 with N := R†ZR + 1

σ2 I,

31In a more general setting, we would use the Sherman-Morrison inversion formula, or the analogous formula for

functions of matrices subject to rank-one perturbations.

74

where Z := ¯f (CC†).

(cid:126)1†M −1y
(cid:126)1†M −1(cid:126)1

=

=

=

=

=

=

=

=

(cid:126)1†(R†ZR + σ−2I)y ± (cid:107)(cid:126)1(cid:107)(cid:107)y(cid:107)(cid:107)R†ZR + σ−2I − M −1(cid:107)
(cid:126)1†(R†ZR + σ−2I)(cid:126)1 ± (cid:107)(cid:126)1(cid:107)2(cid:107)R†ZR + σ−2I − M −1(cid:107)
(cid:126)1†R†ZRy + σ−2(cid:126)1†y ± εσ−2m
(cid:126)1†R†ZR(cid:126)1 + σ−2(cid:126)1†(cid:126)1 ± εσ−2m
(cid:126)1†R†Zry ± (cid:107)(cid:126)1†R†Z(cid:107)(cid:107)Ry − ry(cid:107) + σ−2γ ± σ−2|γ − (cid:126)1†y| ± εσ−2m
(cid:126)1†R†Zr1 ± (cid:107)(cid:126)1†R†Z(cid:107)(cid:107)R(cid:126)1 − r1(cid:107) + (1 ± ε)σ−2m
√
m) + σ−2γ ± 2εσ−2m
m) + σ−2m ± εσ−2m

√

√

√

√

(cid:126)1†R†Zry ± (
m(1 + ε)σ−3)(εσ
(cid:126)1†R†Zr1 ± (
m(1 + ε)σ−3)(εσ
1Zry ± (cid:107)R(cid:126)1 − r1(cid:107)(cid:107)Zry(cid:107) + σ−2γ ± O(cid:0)εσ−2m(cid:1)
r†
r†
1Zr1 ± (cid:107)R(cid:126)1 − r1(cid:107)(cid:107)Zr1(cid:107) + σ−2m ± O(εσ−2m)
r†
1Zry ± εσ
r†
1Zr1 ± εσ
1Zry + σ−2γ ± O(cid:0)εσ−2m(cid:1)
r†
r†
1Zr1 + σ−2m ± O(εσ−2m)
r†
1Zry + σ−2γ
r†
1Zr1 + σ−2m

(1 ± O(ε)) ± O(ε).

√

by SVT bound

by Eqs. (25) and (26)

by Eqs. (25) and (26)

by r†

1Zr1 ≥ 0

m((cid:107)ZRy(cid:107) + (cid:107)Z(cid:107)(cid:107)Ry − ry(cid:107)) + σ−2γ ± O(cid:0)εσ−2m(cid:1)
m((cid:107)ZR(cid:126)1(cid:107) + (cid:107)Z(cid:107)(cid:107)R(cid:126)1 − r1(cid:107)) + σ−2m ± O(εσ−2m)

by Eq. (25)

We will approximate the output vector as

M −1y −

(cid:126)1†M −1y
(cid:126)1†M −1(cid:126)1

M −1(cid:126)1 ≈ R†Zry + σ−2y −

r†
1Zry + σ−2γ
r†
1Zr1 + σ−2m

(R†Zr1 + σ−2(cid:126)1).

To analyze this, we ﬁrst note that

(cid:107)M −1y − R†Zry + σ−2y(cid:107) ≤ (cid:107)M −1 − R†ZR − σ−2I(cid:107)(cid:107)y(cid:107) + (cid:107)R†Z(cid:107)(cid:107)Ry − ry(cid:107)

≤ εσ−2√

m + (1 + ε)σ−3εσ

√

m (cid:46) εσ−2√

m

and analogously, (cid:107)M −1(cid:126)1 − R†Zr1 + σ−2(cid:126)1(cid:107) (cid:46) εσ−2√

m. We also use that

(cid:126)1†M −1y
(cid:126)1†M −1(cid:126)1

≤

(cid:107)(cid:126)1M −1/2(cid:107)(cid:107)M −1/2y(cid:107)
(cid:107)M −1/2(cid:126)1(cid:107)2

=

(cid:107)M −1/2y(cid:107)
(cid:107)M −1/2(cid:126)1(cid:107)

≤

(cid:107)X(cid:107)
σ

.

(27)

75

With these bounds, we can conclude that (continuing to use Eqs. (25) and (26))

(cid:13)
(cid:13)
M −1(cid:16)
(cid:13)
(cid:13)
(cid:13)

y −

(cid:126)1†M −1y
(cid:126)1†M −1(cid:126)1

(cid:17)
(cid:126)1

−

(cid:16)

R†Zry + σ−2y −

≤ (cid:107)M −1y − R†Zry + σ−2y(cid:107) +

M −1(cid:126)1 −

≤ εσ−2√

m +

(cid:126)1†M −1y
(cid:126)1†M −1(cid:126)1

(cid:107)M −1(cid:126)1 − R†Zr1 − σ−2(cid:126)1(cid:107) +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:126)1†M −1y
(cid:126)1†M −1(cid:126)1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:17)
(R†Zr1 + σ−2(cid:126)1)

r†
1Zry + σ−2γ
r†
1Zr1 + σ−2m
r†
1Zry + σ−2γ
r†
1Zr1 + σ−2m
r†
(cid:126)1†M −1y
1Zry + σ−2γ
(cid:12)
(cid:12)
(cid:12)
(cid:126)1†M −1(cid:126)1
r†
1Zr1 + σ−2m

−

(cid:17)
(R†Zr1 + σ−2(cid:126)1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:12)
(cid:12)(cid:107)R†Zr1 + σ−2(cid:126)1(cid:107)
(cid:12)

(cid:17)

εσ−2√

m + ε

(cid:16)

1 +

(cid:17)

(cid:126)1†M −1y
(cid:126)1†M −1(cid:126)1

(cid:107)R†Zr1 + σ−2(cid:126)1(cid:107)

(cid:17)(cid:16)

σ−2√

m + (cid:107)R†Zr1 + σ−2(cid:126)1(cid:107)

(cid:17)

(cid:17)
m + (cid:107)M −1(cid:126)1(cid:107) + (cid:107)(R†ZR + σ−2I − M −1)(cid:126)1(cid:107) + (cid:107)R†Zr1 − R†ZR(cid:126)1(cid:107)
m + σ−2√

m + εσ−2√

m + εσ−2√

m

(cid:17)

by Eq. (27)

(cid:16)

(cid:46)

1 +

(cid:16)

= ε

1 +

(cid:46) ε

(cid:46) ε

(cid:46) ε

(cid:107)X(cid:107)
σ
(cid:107)X(cid:107)
σ
(cid:107)X(cid:107)
σ

(cid:126)1†M −1y
(cid:126)1†M −1(cid:126)1
(cid:126)1†M −1y
(cid:126)1†M −1(cid:126)1
σ−2√
(cid:16)
σ−2√
σ−2√

m.

(cid:16)

So, by rescaling ε down by (cid:107)X(cid:107)

σ , it suﬃces to sample from

ˆα := R†Z

(cid:16)

ry −

r†
1Zry + σ−2γ
r†
1Zr1 + σ−2m

(cid:17)

− σ−2(cid:16)

r1

y −

r†
1Zry + σ−2γ
r†
1Zr1 + σ−2m

(cid:17)

.

(cid:126)1

To gain sampling and query access to the output, we consider this as a matrix-vector product, where
the matrix is (R† | y | (cid:126)1) and the vector is the corresponding coeﬃcients in the linear combination.
Then, by Lemmas 3.5 and 3.6, we can get SQφ(ˆα) for

(cid:32)

φ = (r + 2)

(cid:107)X(cid:107)2
F
r

(cid:13)
(cid:13)
(cid:13)Z

(cid:16)

ry −

r†
1Zry + σ−2γ
r†
1Zr1 + σ−2m

(cid:17)(cid:13)
2
(cid:13)
(cid:13)

r1

+ σ−4(cid:16)

(cid:107)y(cid:107)2 +

(cid:18)

(cid:107)X(cid:107)2
F

(cid:46)

(cid:107)X(cid:107)2

(cid:19)
σ2 σ−6m + rσ−4 (cid:107)X(cid:107)2
σ2 m
r( (cid:107)X(cid:107)2

(cid:16)

F

δ = O

(cid:107)ˆα(cid:107)−2 (cid:46)

(cid:16) (cid:107)X(cid:107)2
F

σ2 + r
(cid:17)

so (cid:102)sqφ(ˆα) = φ sqφ(ˆα) log 1

σ2 + r) (cid:107)X(cid:107)2

σ2

σ−4m
(cid:107) ˆα(cid:107)2 log 1

δ

.

(cid:16) r†
1Zry + σ−2γ
r†
1Zr1 + σ−2m
(cid:17) (cid:107)X(cid:107)2
σ2

σ−4m
(cid:107)ˆα(cid:107)2

(cid:33)

(cid:17)2

(cid:107)(cid:126)1(cid:107)2(cid:17)

(cid:107)ˆα(cid:107)−2

Lemma 6.28 (Perturbations of the partition function). For all Hermitian matrices H, ˜H ∈ Cn×n,
˜H − eH (cid:13)
(cid:13)
(cid:13)1

˜H ) − Tr(eH )

e(cid:107) ˜H−H(cid:107) − 1

(cid:12)
(cid:12)
(cid:12)Tr(e

Tr(eH ).

(cid:12)
(cid:12)
(cid:12) ≤

(cid:13)
(cid:13)
(cid:13)e

≤

(cid:16)

(cid:17)

Proof. We will use the following formula introduced by [KS48, Fey51] (see also [Bel97, Page 181]):

d
dt

eM (t) =

(cid:90) 1

0

eyM (t) dM (t)

dt

e(1−y)M (t)dy.

(28)

76

=

≤

≤

≤

(cid:90) 1

0
(cid:90) 1

0
(cid:90) 1

0
(cid:90) 1

0

Let A ∈ Cn×n with (cid:107)A(cid:107) ≤ 1, we deﬁne the function gA(t) := Tr (cid:0)AeH+t( ˜H−H)(cid:1), and observe that

g(cid:48)
A(t) =

d
dt

(cid:16)

Tr
(cid:18)

= Tr

A

(cid:18)

= Tr

A

AeH+t( ˜H−H)(cid:17)
(cid:19)
d
dt
(cid:90) 1

eH+t( ˜H−H)

ey[H+t( ˜H−H)]( ˜H − H)e(1−y)[H+t( ˜H−H)]dy

by deﬁniton

by linearity of trace

by Eq. (28)

(cid:19)

0

(cid:16)

Tr

Aey[H+t( ˜H−H)]( ˜H − H)e(1−y)[H+t( ˜H−H)](cid:17)

dy

by linearity of trace32

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)Aey[H+t( ˜H−H)]( ˜H − H)e(1−y)[H+t( ˜H−H)](cid:13)
(cid:13)
(cid:13)1
(cid:13)Aey[H+t( ˜H−H)](cid:13)
(cid:13)ey[H+t( ˜H−H)](cid:13)

(cid:13)( ˜H − H)e(1−y)[H+t( ˜H−H)](cid:13)

(cid:13)
˜H − H
(cid:13)
(cid:13)

(cid:13)e(1−y)[H+t( ˜H−H)](cid:13)

(cid:13)
(cid:13) 1
1−y

(cid:13)
(cid:13) 1
y

(cid:107)A(cid:107)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13) 1
y

(cid:13)
(cid:13) 1
1−y

dy

by trace-norm inequality

dy

by H¨older’s inequality

dy

by H¨older’s inequality

(cid:90) 1

0

(cid:13)
˜H − H
(cid:13)
(cid:13)
(cid:13)
˜H − H
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

=

(cid:13)
(cid:13)

(cid:13)eH+t( ˜H−H)(cid:13)
(cid:13)
(cid:13)1

(cid:13)
(cid:13)

(cid:13)ey[H+t( ˜H−H)](cid:13)

(cid:13)
(cid:13)

(cid:13)e(1−y)[H+t( ˜H−H)](cid:13)

(cid:13)
(cid:13) 1
1−y

dy

since (cid:107)A(cid:107) ≤ 1

(cid:13)
(cid:13) 1
y

.

(29)

Now we consider z(t) := gI (t) = Tr (cid:0)eH+t( ˜H−H)(cid:1). From Eq. (29) we have z(cid:48)(t) ≤ (cid:107) ˜H − H(cid:107)z(t).
Using Gr¨onwall’s diﬀerential inequality, we can conclude that z(t) ≤ z(0)et(cid:107) ˜H−H(cid:107) for every t ∈ [0, ∞).
Finally, we use the fact that there exists a matrix A of operator norm at most 1 such that
(cid:13)1 = Tr(A(e ˜H − eH )) (take, e.g., sgn(e ˜H − eH )). We ﬁnish the proof by observing that for

(cid:13)e ˜H − eH (cid:13)

(cid:13)1 = Tr(Ae ˜H ) − Tr(AeH ) = gA(1) − gA(0) = (cid:82) 1
(cid:90) 1

(cid:90) 1

0 g(cid:48)

A(t)dt and

(cid:107) ˜H − H(cid:107)z(t)dt ≤ z(0)

(cid:107) ˜H − H(cid:107)et(cid:107) ˜H−H(cid:107)dt = Tr(eH )

(cid:16)

e(cid:107) ˜H−H(cid:107) − 1

(cid:17)

.

(cid:13)e ˜H − eH (cid:13)
(cid:13)
such an A, (cid:13)

(cid:90) 1

0

g(cid:48)
A(t)dt

(29)
≤

0

0

32Note that in case A = I, by the cyclicity of trace, this equation implies that d

dt Tr(eH(t)) = Tr(eH(t) d

dt H(t)).

77

