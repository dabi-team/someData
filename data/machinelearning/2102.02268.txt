A Heuristic for Dynamic Output Predictive Control
Design for Uncertain Nonlinear Systems

Mazen Alamir

1

1
2
0
2

b
e
F
3

]

Y
S
.
s
s
e
e
[

1
v
8
6
2
2
0
.
2
0
1
2
:
v
i
X
r
a

Abstract—In this paper, a simple heuristic is proposed for the
design of uncertainty aware predictive controllers for nonlinear
models involving uncertain parameters. The method relies on
Machine Learning-based approximation of ideal deterministic
MPC solutions with perfectly known parameters. An efﬁcient
construction of the learning data set from these off-line solutions
is proposed in which each solution provides many samples in the
learning data. This enables a drastic reduction of the required
number of Non Linear Programming problems to be solved off-
line while explicitly exploiting the statistics of the parameters
dispersion. The learning data is then used to design a fast on-line
output dynamic feedback that explicitly incorporate information
of the statistics of the parameters dispersion. An example is
provided to illustrate the efﬁciency and the relevance of the
proposed framework. It is in particular shown that the proposed
solution recovers up to 78% of the expected advantage of having
a perfect knowledge of the parameters compared to nominal
design.

Index Terms—Machine Learning, Nonlinear Systems, Output

Feedback, Stochastic NMPC.

I. INTRODUCTION

Over the last

two decades, Nonlinear Model Predictive
Control (NMPC) [13] spread in all domains and became a
ﬁrst choice when it comes to design control feedbacks for
systems that admit faithful models. This is due to the maturity
of its stability assessment as well as to the availability of
user-friendly solvers [4]. As a direct consequence of this
investigations started regarding the best way to
success,
extend the use of NMPC to systems that are represented
through uncertain models.

After some early attempts involving over stringent robust
NMPC frameworks [11], robust adaptive MPC frameworks
have been proposed [2],
to provide on-line model
[7]
adaptation for uncertain systems under the assumption of
identiﬁability, persistent excitation
afﬁne parameters, strict
and full state measurement. More recently, the concept of
Stochastic NMPC (SNMPC) [12], [14] emerged. In a nutshell,
SNMPC frameworks implement
the basic NMPC scheme
with the standard deterministic cost and constraints being
replaced by their expected values, considered as functions of
the model’s uncertainties. While the conceptual simplicity of
this shift in paradigm is appealing, its consequence on the
computational burden appears as a deal breaker despite some
recent attempts [3], [9], [16]. Indeed, SNMPC requires the
on-line computation of the expectation of nonlinear functions

This work was supported by the MIAI @ GrenobleAlpes under Grant ANR-

19-P31A-003.

The author is with the University of Grenoble Alpes, CNRS, Grenoble INP,
GIPSA-lab, 38000 Grenoble, France (e-mail: mazen.alamir@grenoble-inp.fr).

of several variables. This computation is to be performed
for any value of the decision variable that is encountered
during the on-line iterations of the optimization algorithm.
This complexity justiﬁed an increasing interest
in off-line
computation based solutions.

/ Reinforcement Learning (LR) alternatives

One of these is related to Stochastic Dynamic Programming
(SDP)
[5],
[14] which offer elegant solutions for small-size problems.
Another research track is related to the approximation of
the appropriate optimal feedback/strategy via Deep/Machine
learning-based ﬁtting schemes (see [8], [10] and the references
therein). More precisely, a cloud of initial states is created and
for each element in the cloud, a multi-stage SNMPC is solved
(this needs ﬁnite and generally small number of possible
uncertainty values to be allowed). Then a neural network is
trained via deep-learning in order to ﬁt a function that maps
the initial state to the stochastically optimal feedback control
and or strategy. This approach leads to extremely heavy
off-line computational burden without providing theoretical
guarantees, not only because of the inherent and unavoidable
data incompleteness which is a shared issue by all data-driven
solutions (including the one proposed in this paper), but
also because all these schemes mainly use the multi-stage
SNMPC framework which needs the uncertainty realizations
to be limited to a few number of possible values which
obviously scarcely ﬁts the real-life situations. Consequently,
these approaches should be viewed as smart and reasonably
tractable heuristics to address a problem that admits no
rigorously computable solution.

is not

the intention of this paper to propose a better
It
framework than the ones cited above and the references
therein. This is because the comparison between heuristics is
a complex multi-criteria based process that would go beyond
the scope of this short contribution. In particular, while the
scheme proposed hereafter is appealing from a computational
point of view, its scope of application is limited to nonlinear
models with ﬁxed (or slowly varying) uncertain parameters
as main source of uncertainty. In this particular but highly
relevant case, the following intuition motivates this paper:

the deterministic op-
under speciﬁc circumstances,
solution that would have been computed,
timal
should the parameters be known, can be learned from
the previous measurement over some observation win-
dow.

 
 
 
 
 
 
this statement

In the ideal case,
results from a perfect
identiﬁability of the pair of state and parameter vectors
leading to the optimal MPC control being an exact function
of the previous measurement y(−)

k

:
J(cid:0)u | ˆx(y(−)

k

), ˆw(y(−)

k

)(cid:1)

(1)

ˆu(cid:63)(cid:0)y(−)

k

(cid:1) = arg min
u∈UN

k

k

and ˆu(cid:63)

) and ˆw(y(−)

making y(−)
k ideal candidates to be viewed respectively
as features vector and a label in a Machine Learning (ML)
identiﬁcation step. When extended observability does not hold,
the above relationship can be used with the understanding
that ˆx(y(−)
) are non uniquely determined
quantities since for a given instance of the measurement
proﬁle y(−)
, there is a cloud of possible values of (ˆx, ˆw)
and hence a cloud of possibly indistinguishable optimal
control inputs ˆu(cid:63). The data generation using the statistics
of the dispersion of w and the following ML identiﬁcation
step helps making a statistically rational choice among the
elements of this cloud.

k

k

Following the above lines, this paper proposes a framework
for the design of uncertainty-aware output feedback law
which is ﬁtted from learning data that is constructed in a
computationally efﬁcient manner. The latter involves two
main differences with standard approaches: 1) the off-line
computation involves only deterministic optimal control
problems. and can therefore directly beneﬁt from the state-
of-the-art available NMPC solvers. 2) Each single solution
is used to extract multiple instances (that can be as high as
hundreds) to be included in the learning data set. This is done
by exploring the open-loop optimal optimal trajectories and
concatenate the sub-optimal instances of (y(−)
))
rather than collecting a single pair.

k+i(y(−)

k+i, u(cid:63)

k

This paper
is organized as follows: Section II presents
some deﬁnitions and notation and states precisely the
problem under study. The proposed framework is detailed in
Section III together with the working assumptions that might
induce its success. An illustrative example is proposed in
Section IV in order to show the steps of the framework and
evaluate the impact of parameters choice on the quality of
the results. Finally, Section V concludes the paper and gives
some hints for further investigations.

II. DEFINITION, NOTATION AND PROBLEM STATEMENT

Let us consider nonlinear systems governed by the following

discrete-time dynamics:

xk+1 = f (xk, uk, w)
yk = h(xk, uk, w)

(2)

(3)

where (2) and (3) describe the dynamics and the measurement
equation respectively. The notation xk ∈ Rnx , uk ∈ U ⊂ Rnu
and yk ∈ Rny stand for the state, input and measured output
vectors at instant k. The vector of parameters w ∈ Rnw is
supposed to be constant although uncertain. Moreover, it is
assumed that w belongs to some bounded subset W over
which a probability density function (pdf) W is known so

2

that a statistically relevant samples of w can be drawn if
required. It is also assumed that a set of states X of interest
is considered with some random sampling rule which reﬂects
some form of relative importance or somehow relevance1. In
what follows, a boldfaced notation refer to signals proﬁles over
some past or future time windows, in particular the notation
N −1)T ∈ UN denotes a sequence of
u := (uT
future control actions over some prediction horizon of length
N while y(−)
refers to the sequence of M + 1 previous
measurement vectors, namely

2 , . . . , uT

0 , uT

k








∈ (cid:2)Rny (cid:3)M +1

(4)








yk
yk−1
...
yk−M

y(−)
k

:=

The notation





x(u)
0
x(u)
1

x(u)(x0, w) =

(x0, w)
(x0, w)
...
x(u)
N (x0, w)
denotes the state trajectory under (2) when the control proﬁle
u and when the parameter vector w is used, namely:













(cid:105)N +1

Rnx

∈

(cid:104)

x(u)
(x0, w) = x0
0
k+1(x0, w) = f (x(u)
x(u)

k (x0, w), uk, w)

It is assumed that the control objective would be deﬁned
through a cost function of the form

J(u | xk, w) :=

N
(cid:88)

(cid:0)x(u)

i

(cid:96)i

(xk), ui−1

(cid:1)

(5)

i=1
should the current state xk and the parameter vector w be
known. The design problem that is addressed in the present
paper can be shortly stated as follows:

UNCERTAINTY-AWARE DYNAMIC OUTPUT FEEDBACK

Design a dynamic output feedback of the form:
uk := K(y(−)

)

k

that associates to each past measurement proﬁle
y(−)
an associated input in such a way that in-
k
corporates the presumably known statistics W and
that is oriented towards the optimization of the cost
function J.

REMARK 1: it is assumed that all the constraints except the
control saturation related ones (which are expressed through
U) are included in the very deﬁnition of the stage cost map (cid:96)
for the sake of simplicity of exposition. Moreover, it should be
underlined that the speciﬁc structure of the cost function, being
the sum of decoupled stage-cost terms is not explicitly needed
although it is kept because it ﬁts all the available optimal
control problems’ solvers that we rely on in the design step.

1In the absence of any knowledge, uniform distribution over some hyper-

box can be simply used.

3

III. THE PROPOSED FRAMEWORK

A. The design viewed as a map identiﬁcation problem

The starting point lies in the fact that the problem reduces
to a standard deterministic problem if the pair (xk, w) of
state and parameter vector were reconstructible via extended
observation [6], namely, if one can reconstruct estimations
ˆxk and ˆw of these two quantities as functions of the previous
measurement:

ˆx(y(−)
k

)

;

ˆw(y(−)
k

)

(6)

Indeed, in this case, the answer to the problem stated in the
previous section would obviously be given by:

K(y(−)
k

) := ˆu(cid:63)
0

(cid:0)y(−)

k

(cid:1)

(7)

0(y(−)

where ˆu(cid:63)
that solves the following optimization problem:

) is the ﬁrst control in the optimal sequence

k

computed in the previous item for the speciﬁc value of
q = (x0, w). Indeed, consider the pair (see Figure 1):
(cid:16)

(cid:16)

(cid:17)

[y(cid:63)(q)](−)

M , u(cid:63)

M (q)

(9)

y(−), ˆu(cid:63)

(cid:17)
0(y(−))

:=

The key idea is that the pair deﬁned in (9) approximates a
pair of the form (7) provided that the prediction horizon N
is sufﬁciently long. Indeed, y(−) := [y(cid:63)(q)](−)
M , considered
at instant M is the vector of past measurements. It therefore
contains the information regarding the pair (x(cid:63)
M (x0), w) of
current state and parameter vector.

0(y(−)) := u(cid:63)
ˆu(cid:63)

M (q)

y(−) =

[y(cid:63)(q)](−)
M

x0

Optimal trajectories u(cid:63)(q)

ˆu(cid:63)(cid:0)y(−)

k

(cid:1) = arg min
u∈UN

J(cid:0)u | ˆx(y(−)

k

), ˆw(y(−)

k

)(cid:1)

(8)

0

M

N

time

k

), ˆw(y(−)

the pairs
When extended observability does not hold,
ˆx(y(−)
) are non uniquely determined quantities
k
since for a given instance of the measurement proﬁle y(−)
,
there is a cloud of possible values of (ˆx, ˆw) and hence a
cloud of possibly indistinguishable optimal control
inputs
ˆu(cid:63). The data generation using the statistics of the dispersion
of w and the following ML identiﬁcation step helps making
a statistically rational choice among the elements of this cloud.

k

To summarize:

THE MAP TO BE IDENTIFIED

Given the statistical dispersion of the parameters,
ﬁt the best map F (y(−)
0(·) that links the
k
sequence of previous measurement y(−)
to the
optimal control uk = F (y(−)

) ≈ ˆu(cid:63)

).

k

k

Now the question is:

How to build a relevant and rich learning data that
can be used to ﬁt the map F while reducing as far as
possible the amount of off-line computation ?

B. Building the learning data

Fig. 1. Generation of a sample pair (y(−), ˆu(cid:63)
control problem for a given sampled pair q = (x0, w).

0(y(−)) by solving an optimal

Now it is true that in order for (9) to be of the form (7), the
following approximation:

M (q) ≈ u(cid:63)
u(cid:63)

0(x(cid:63)

M (q))

(10)

should be satisﬁed. But this approximation would have been
a strict equality should N be inﬁnite thanks to the Bellman
principle! Indeed, u(cid:63)
M (q) is the beginning of the remaining
part of an optimal solution while u(cid:63)
M (q)) is the optimal
solution of the updated problem and these two solutions
would be identical should N be inﬁnite.

0(x(cid:63)

Now the same argument holds if we iterate the process
(still for the same q) using moving windows that end at
instants M + j, for j = 1, . . . , m − 1. This means that
by performing a single deterministic NLP solution, we can
generate m samples in the learning data, namely:

D(q) :=

(cid:40)

(cid:16)

[y(cid:63)(q)](−)

k

(cid:17)

, u(cid:63)

k(q)

(cid:41)M +m−1

k=M

(11)

This procedure is shown in Figure 2.

Repeating this procedure for a cloud of nq
values of the pair q, one gets a learning data:

sampled

Consider the following procedure that is the elementary bloc

in the data generation step:

D :=

(cid:110)

D(q((cid:96)))

(cid:111)nq

(cid:96)=1

(12)

1) Choose a long prediction horizon N .
2) Choose an observation horizon M < N .
3) Randomly choose a pair q = (x0, w)∈ Rnx × Rnw
4) Use

instance
(for
efﬁcient NLP
IPOPT-Casadi [4])
a minimizer
to
u(cid:63)(q) to J(· | q). One gets the situation depicted in
Figure 1.

solver
compute

an

5) This item explains how m samples in the learning
data is created using the optimal open-loop trajectories

containing (nq · m) samples at the price of solving nq deter-
ministic optimal control problems.

C. Performance evaluation

The learning data D can now be used to ﬁt a model of the

form2:

uk = ML(y(−)

k

)

(13)

2ML stands for (Machine Learning)-based model

x0

x0

x0

0(y(cid:63)(−)
ˆu(cid:63)

M ) := u(cid:63)

M (q)

y(cid:63)(−)
M

Optimal trajectories u(cid:63)(q)

0

M

N

time

4

0(y(cid:63)(−)
ˆu(cid:63)

M +1) := u(cid:63)

M +1(q)

y(cid:63)(−)
M +1

Optimal trajectories u(cid:63)(q)

Fig. 3. Dispersion of the parameters in the learning data in terms of ratio to
the nominal components. (Ratios to nominal values are shown)

0

M + 1

N

time

0(y(cid:63)(−)
ˆu(cid:63)

M +2) := u(cid:63)

M +2(q)

y(cid:63)(−)
M +2

Optimal trajectories u(cid:63)(q)

0

M + 2

N

time

Fig. 2. Construction of the learning data D(q): by going forward in a moving
window along the optimal trajectory computed for a single pair q = (x0, w)
it is possible to generate a high number of different samples of pair (y(−), u)
that can be used in the construction of the learning data.

This feedback can be applied in closed-loop to a new cloud
of values of (cid:8)q(j) = (x(j)
0 , w(j))(cid:9)nv
j=1. Namely for each of
these values, the feedback is implemented without knowledge
of the associated w(j) (only the vector of past measurement
is used through (13)). If the closed-loop is simulated dur-
ing N samples (equal to the prediction horizon), then the
corresponding closed-loop cost, say J cl
j can be compared to
the exact optimal cost J (cid:63)
that can be viewed as the ideal
j
solution (since deterministic problems are solved assuming
that the parameter values w(j) is known). Obviously, if the
optimization is perfectly done, one should systematically have
J cl
j ≤ J (cid:63)
j since any closed-loop sequence over the prediction
horizon N is naturally a candidate sequence of the open-loop
optimization problem.

IV. NUMERICAL INVESTIGATION

Consider the parallel reactor system [1] commonly used in

the study of deterministic Economic NMPC:

1e−1/x3 − w2x1e−w3/x3 − x1

˙x1 = 1 − w1x2
˙x2 = w1x2
˙x3 = u − x3

1e−1/x3 − x2

(14)

(15)

(16)

where x1 and x2 stand for the concentrations of reactant and
product respectively while x3 represents the temperature of
the mixture in the reactor. The control variable is given by the

heat ﬂow u ∈ U := [0.049, 0.449]. The objective of the control
is to maximize the amount of product x2 which is precisely
the only measured variable. This leads to the following cost
function:

J(u | x0, w) = −

N
(cid:88)

i=1

Cx(u)
i

(x0, w) with C = (0, 1, 0)

In the forthcoming computations, a sampling period of 0.02
time units is used. Based on this value and observing typical
optimal trajectories, the prediction horizon length N = 250
has been considered as it leads to a prediction horizon that is
slightly greater than the settling time. The past measurement
horizon length used to deﬁne y(−)
is taken equal to M = 10.

k

A. Generating the learning and validation data

The initial states x0 are uniformly sampled inside the set

X0 := [10−4, 0.5] × [10−4, 0.2] × [10−4, 0.25]

(17)

which is known to include almost all relevant evolutions of the
system. As for the values of the unknown parameter vector w,
a normal distribution around the nominal value given by:

w0 = (104, 400, 0.55)

is used so that the random sampling is based on the rule:

wi = (1 + νi)w0
i

with

νi ∈ N (0, σi)

(18)

meaning that the true values are normally distributed around
the nominal value with a known standard deviations. The
values σi = 0.33 are used for i ∈ {1, 2, 3}. Figure 3 shows
the dispersion of the parameter values that results from the
random sampling law (18). Note that the variations of the
parameters span the interval going from 20% to 180% of the
nominal values.

The learning data set is generated according to (11)-(12) in
which nq = 500 samples of pairs (x0, w) are drawn inside the
above described sets. This leads to a learning set of cardinality
(500 · m) where m is the number of times the moving window
is translated in order to generate samples from a single solution
a deterministic optimal control problem. In what follows,

5

Fig. 5. Histogram of the values of the control u present in the 500 scenarios
(of length N = 250 each) contained in the learning data.

B. Identifying the output feedback model
The features vector is build using the previous proﬁle of x2
as output. The Identiﬁcation of the map ML(y(−)) has been
obtained using a RandomForestClassifier from the
scikit-learn freely available python library [15]. In order
to avoid overﬁtting and enhance the quality of the extrapola-
tion on unseen data, the max_leaf_nodes parameter has
been set to 500.

Fig. 6. Fitting results on the training and test data (split ratio (33%) for the
different values of receding horizon moves m used in the construction of the
learning data. Recall that the label the classiﬁer tries to guess lies in {1, 2, 3}
as explained in (19).

The confusion matrices corresponding to the training and
the test data (using a test-size ratio of 33% of the learning data)
are shown in Figure 6 for m ∈ {10, 100, 200}. These matrices
clearly show that when m = 10 is used (cardinality of the
learning set = 5000) the learning data is still not representative
enough (as the precision on training data is signiﬁcantly better
than on test data). This clearly highlights by itself the relevance
of the proposed heuristic since the computation time reported
in Figure 4 suggests that the time that would be needed to
generate 5000 rigorously optimal although still insufﬁcient
data would be around 21 hours while the time needed here
to generate 100000 samples is around 2h.

C. Performance evaluation

A new set of 100 pairs of (x0, w) is generated and the
100 initial states are used to generate learning data using the

Fig. 4. Cumulative histogram of computation time (sec) needed by IPOPT
(multiple-shooting) to solve a single optimal control problem when creating
the learning data for the illustrative example.

several identiﬁcation settings are tested for different values
of m ∈ {10, 50, 100, 150, 200} leading to learning data sets
of different cardinality {5000, 25000, 50000, 75000, 100000}
samples having all in common almost the same computation
time (the time needed to solve nq = 500 deterministic NLP
problems).

The cumulative histogram of computation time (in sec)
needed by a multiple-shooting implementation of the optimal
control solver IPOPT implemented using Casadi is shown
in Figure 4. The computation time shown in Figure 4 might
seem too long for those who are commonly using IPOPT to
solve standard regulation problems. Indeed, the median time
is close de 15 sec! This is probably due to the oscillating
character of optimal trajectory that is very speciﬁc to the
underlying economic NMPC formulation and to the relatively
long prediction horizon N = 250 being used.

The histogram of control values over the prediction horizon
of the nq = 500 sampled scenarios (including hence 125000
values) is shown in Figure 5. This histogram suggests that
classiﬁcation tools are best suited for the learning of the
control using ML tools compared to regression tools3. Indeed,
the following three-valued set of label values can be used:

label =






1
2
3

if u ≤ 0.075
if u ∈]0.07, 0.14]
otherwise

(19)

with this deﬁnition of

is possible to
identify a classiﬁer that associates to any time series
of measurement y(−) an element of the set {1, 2, 3} and
hence its corresponding control value in {0.049, 0.11, 0.449}.

the label,

it

if

the distribution of

the control values was
Note that
more continuously distributed between umin = 0.049 and
umax = 0.449, a regressor would have been more appropriate
to address the identiﬁcation problem.

3for this speciﬁc example.

nominal value w0 of the parameter vector. This is done in
order to evaluate the beneﬁt from explicitly learn the feedback
from sampled parameter vector compared to a nominal design
that only use the nominal expected value.

Then for each value of m,
the corresponding identiﬁed
feedback law is used to generate 100 closed-loop in which
the newly drawn samples of the parameter vector (which are
unknown to the previously identiﬁed feedbacks) is used in
the simulated model.

The results are shown in Figure 7 where one can ﬁnd
1) the ideal optimal closed-loop cost given by the NLP
solver IPOPT in the unrealistic case where the disturbance
is perfectly known 2) the closed-loop performance obtained
using the nominal feedback described above and 3) the
closed-loop performance of the identiﬁed feedback using the
different values of m.

Fig. 7. Comparison of the closed-loop performances on the validation data
set.

From Figure 7, the following observations can be made:

• The optimal achievable values of the cost when the
parameter vector is perfectly known is 31% better
(in average) than what the nominal identiﬁed controller
enables to achieve.

• The proposed methodology enables to recover 78% of
the advantages4 of knowing perfectly the value of the
parameter vector when m = 200 is used in the construc-
tion of the learning data. This corresponds to 24% better
average than the nominally optimal identiﬁed controller.
Almost similar results are obtained when m ∈ {150, 100}
is used.

• The average performance drops to 20% (66% of re-
covered advantage) when m = 50 is used while a
drastic decrease corresponding to 3.9% (and only 12% of
recovered advantage) when short m = 10 is used in the
building of the learning data. It is important to underline
that if strict optimality is used in the data set building
step (only one sample per NLP solution), the time that
would be needed to solve the 100000 NLP problems 5
would have been equal to approximately 17 days.

4This is the ratio 0.245/0.312
5Cardinality of the learning set for m = 200

6

V. CONCLUSION AND FUTURE WORK

In this paper a heuristic is proposed for the design of
uncertainty-aware dynamic output
feedback for uncertain
nonlinear models. This heuristic is based on off-line
solution of deterministic NLP problems over a cloud of
sampled contexts followed by a receding horizon samples
collection that enriches the learning data for the same off-line
computation load. The results show promising closed-loop
performance when compared to ideal performances obtained
with the rigorous knowledge of
the parameters. These
performance highly outperform nominal design based on the
most expected value of the parameters.

Undergoing works concern the application of the framework
to more challenging examples and investigate heuristics to
quickly guess the optimal
triplet (M, m, N ) for a given
uncertain nonlinear model.

REFERENCES

[1] M¨uller M. A., David Angeli, Frank Allg¨ower, Rishi Amrit, and James B.
Rawlings. Convergence in economic model predictive control with
average constraints. Automatica, 50(12):3100 – 3111, 2014.

[2] Veronica Adetola and Martin Guay. Robust adaptive mpc for constrained
uncertain nonlinear systems. International Journal of Adaptive Control
and Signal Processing, 25(2):155–167, 2011.

[3] M. Alamir. On the use of supervised clustering in stochastic NMPC
design. IEEE Transactions on Automatic Control (Early access via DOI
:10.1109/TAC.2020.2970424), 2020.

[4] J. A. E. Andersson, J. Gillis, G. Horn, J. B Rawlings, and M. Diehl.
CasADi – A software framework for nonlinear optimization and optimal
control. Mathematical Programming Computation, 2018.

[5] D. Bertsekas. Reinforcement Learning and Optimal Control. Athena

Scientiﬁc, 2019.

[6] G. Besanc¸on. Nonlinear Observers and Applications. Lecture Notes in
Control and Information Sciences. Springer Berlin Heidelberg, 2007.
[7] Martin Guay, Veronica Adetola, and Darryl DeHaan. Robust and
Institution

Adaptive Model Predictive Control of Nonlinear Systems.
of Engineering and Technology, 2015.

[8] Benjamin Karg, Teodoro Alamo, and Sergio Lucia.

Probabilistic
performance validation of deep learning-based robust nmpc controllers,
2019.

[9] Sergio Lucia, Tiago Finkler, and Sebastian Engell. Multi-stage nonlinear
model predictive control applied to a semi-batch polymerization reactor
under uncertainty. Journal of Process Control, 23(9):1306 – 1319, 2013.
[10] Sergio Lucia and Benjamin Karg. A deep learning-based approach
IFAC-PapersOnLine,
to robust nonlinear model predictive control.
51(20):511 – 516, 2018. 6th IFAC Conference on Nonlinear Model
Predictive Control NMPC 2018.

[11] L. Magni and R. Scattolini. Robustness and Robust Design of MPC
for Nonlinear Discrete-Time Systems, pages 239–254. Springer Berlin
Heidelberg, Berlin, Heidelberg, 2007.

[12] D. Mayne. Robust and stochastic model predictive control: Are we going
in the right direction? Annual Reviews in Control, 41:184 – 192, 2016.
[13] D. Q. Mayne, J.B. Rawlings, C. V. Rao, and P. O. M. Scokaert. Con-
strained model predictive control: Stability and optimality. Automatica,
36:789–814, 2000.

[14] A. Mesbah. Stochastic model predictive control with active uncertainty
learning: A survey on dual control. Annual Reviews in Control, 45:107
– 117, 2018.

[15] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vander-
plas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.

[16] G. Schildbach, L. Fagiano, Ch. Frei, and M. Morari. The scenario
approach for stochastic model predictive control with bounds on closed-
loop constraint violations. Automatica, 50(12):3009 – 3018, 2014.

