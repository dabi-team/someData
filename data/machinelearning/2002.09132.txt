1
2
0
2

b
e
F
2
2

]
L
M

.
t
a
t
s
[

2
v
2
3
1
9
0
.
2
0
0
2
:
v
i
X
r
a

Computing Valid p-value for Optimal Changepoint by
Selective Inference using Dynamic Programming

Vo Nguyen Le Duy∗
Nagoya Institute of Technology and RIKEN
duy.mllab.nit@gmail.com

Hiroki Toda∗
Nagoya Institute of Technology
toda.h.mllab.nit@gmail.com

Ryota Sugiyama
Nagoya Institute of Technology
sugiyama.r.mllab.nit@gmail.com

Ichiro Takeuchi†
Nagoya Institute of Technology and RIKEN
takeuchi.ichiro@nitech.ac.jp

Abstract

Although there is a vast body of literature related to methods for detecting change-
points (CPs), less attention has been paid to assessing the statistical reliability of
the detected CPs. In this paper, we introduce a novel method to perform statistical
inference on the signiﬁcance of the CPs, estimated by a Dynamic Programming
(DP)-based optimal CP detection algorithm. Our main idea is to employ a Selective
Inference (SI) approach — a new statistical inference framework that has recently
received a lot of attention — to compute exact (non-asymptotic) valid p-values
for the detected optimal CPs. Although it is well-known that SI has low statistical
power because of over-conditioning, we address this drawback by introducing a
novel method called parametric DP, which enables SI to be conducted with the
minimum amount of conditioning, leading to high statistical power. We conduct
experiments on both synthetic and real-world datasets, through which we offer
evidence that our proposed method is more powerful than existing methods, has
decent performance in terms of computational efﬁciency, and provides good results
in many practical applications.

1

Introduction

Changepoint (CP) detection is a fundamental problem and has been studied in many areas. The
goal of CP detection is to ﬁnd changes in the underlying mechanism of the observed sequential
data. Analyzing the detected CPs beneﬁts to several applications [14, 35, 15, 22, 19]. There is a vast
body of literature related to methods for detecting CPs [2, 47, 32, 42, 29, 11, 46] — nice surveys
can be found in [1, 44]. CP detection is usually formulated as the problem of minimizing the cost
over segmentations, where Dynamic Programming (DP) is commonly used because it can solve the
minimization problem efﬁciently, and exactly ﬁnd the optimal CPs under the given criteria.

Unfortunately, less attention has been paid to the statistical reliability of the detected CPs. Without
statistical reliability, the results may contain many false detections, i.e., the detected CPs may not be
true CPs. These falsely detected CPs are harmful when they are used for high-stake decision making
such as medical diagnosis or automatic driving. Therefore, it is highly necessary to develop a valid
statistical inference for the detected CPs that can properly control the risk of false detection.

Valid statistical inference on CPs is intrinsically difﬁcult because the observed data is used twice
— one for detection and another for inference, which is often referred to as double dipping [23]. In

∗Equal contribution
†Corresponding author

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
Figure 1: An illustrative example of the problem and the methods considered in this paper. The blue line and
the grey circles indicate the underlying mean and the observed sequence, respectively. The red dotted lines are
the results of optimal segmentation (OptSeg) and binary segmentation (BinSeg). Here, the results of OptSeg
and BinSeg were the same. With Bonferroni correction, to control false detection rate at 0.05, the signiﬁcance
level is decided by 0.05
6 ≈ 0.008. The naive p-value is small even for falsely detected CP (E). BinSeg p-values
can identify falsely detected CPs, but it fails to detect some true CPs (C, D, F) due to the lack of power. The
proposed p-values (OptSeg-SI) can successfully identify both true positive and false positive detections.

statistics, it has been recognized that naively computing p-values in double dipping is highly biased,
and correcting this bias is challenging. Our idea is to introduce Selective Inference (SI) framework
for resolving this challenge.

Existing works and their drawbacks. In the case of testing for single CP, most of the existing
inference methods rely on asymptotic distribution of the maximum discrepancy measure, such as
CUSUM score [33], Fisher discriminant score [30, 16], and MMD [25], which is derived under some
restrictive assumptions such as weak dependence among the data points. Asymptotic inference for
multiple CPs was proposed by [14] under the name of SMUCE. These asymptotic approaches often
fail to control type I error when the sequence is short or contains highly correlated data points. Besides,
it has been observed that these approaches are often conservative, i.e., low statistical power [17].

In the past few years, SI has been actively studied and applied to various problems [3, 13, 8, 41, 7,
4, 28, 27, 34, 43, 24, 48, 38, 40, 9, 10]. The basic idea of SI is to make inference conditional on
the selection event, which allows us to derive the exact (non-asymptotic) sampling distribution of
test statistic. However, characterizing the necessary and sufﬁcient selection event is computationally
challenging. For example, in [24], the authors considered inference conditional not only on the
selected features but also on their signs for computational tractability. However, such an over-
conditioning leads to loss of power [24, 26, 9].

SI was ﬁrst discussed in the context of CP detection problem by Hyun et al. [17], in which the authors
studied Fused Lasso. Later, Umezu et al. [45] and Hyun et al. [18] studied SI for CUSUM-based CP
detection and binary segmentation, respectively. Unfortunately, these methods inherit the drawback
of other SI studies, i.e., the loss of power by over-conditioning. In other words, the inference is made
not only conditional on the detected CPs, but also on other unnecessary extra events.

Contributions. We provide an exact (non-asymptotic) inference method for optimal CPs, which
we call OptSeg-SI, based on the concept of SI. To our knowledge, this is the ﬁrst method that can
provide valid p-values to the CPs detected by DP. Unlike existing SI approaches for CPs [17, 18, 45],
the inference in the OptSeg-SI method is made under the minimum amount of conditioning, leading
to high statistical power. To this end, we develop a new method called parametric DP, which
enables us to efﬁciently characterize the selection event. We conduct experiments on both synthetic
and real-world datasets, by which, we offer the evidence that the OptSeg-SI 1) is more powerful
than the existing methods [18, 14], 2) successfully controls false detection probability, 3) has good
performance in terms of computational efﬁciency, and 4) provides good results in many practical
applications.

Figure 1 shows an illustrative example of the problem and the methods we consider in this paper. For
reproducibility, our implementation is available at

https://github.com/vonguyenleduy/parametric_selective_inference_changepoint

2

ABC0.0111×10−66×10−60.3410.00.9990.3610.03×10−11NaiveBinSeg-SIOptSeg-SIMethodLocationp-value tableABCDEValueLocationFDEF0.00.0010.00.0130.0321.01×10−50.0251×10−15NaiveBinSeg-SIOptSeg-SIMethodLocation2 Problem Statement

We consider CP detection problem for mean-shift, which is the most studied model in the literature,
and has been applied to many real-world applications, especially in bioinformatics [31, 6]. Mean-shift
CP detection is the base of many other CP detection methods. If one knows what kind of changes
to focus on (e.g., changes in variance), we can convert the problem into mean-shift CP detection.
Otherwise, nonparametric CP detection methods such as kernel CP detection [25] can be used. It is
well known that many nonparametric methods can be cast into a mean-shift CP detection. Therefore,
mean-shift CP detection is worth investigating as a canonical form of the more complex problems.
Let us consider a random sequence X = (X1, . . . , XN )(cid:62) ∼ N(µ, Σ), where N is the length,
µ ∈ RN is unknown mean vector, and Σ ∈ RN ×N is covariance matrix which is known or estimable
N )(cid:62) ∈ RN , the goal
from external data 3. Given an observed sequence xobs = (xobs
of CP detection is to estimate the true CPs. The vector of detected CP locations is denoted as
, . . . , τ det
τ det = (τ det
K are the CP locations
1
K+1 = N ). We deﬁne xs:e (cid:118) x as a subsequence of x ∈ RN from positions
(we set τ det
0 = 0 and τ det
(cid:80)e
s to e, where 1 ≤ s ≤ e ≤ N . The average of xs:e is written as ¯xs:e = 1
i=s xi, and the cost
function which measures the “homogeneity" of xs:e is deﬁned as C(xs:e) = (cid:80)e
i=s(xi − ¯xs:e)2.

K ), where K is the number of CPs, and τ det

1 < · · · < τ det

, . . . , xobs

e−s+1

1

2.1 Optimal CP detection

Although we do not assume any true structures in the mean vector µ = (µ1, . . . , µN )(cid:62), we consider
the case where data analyst believes that the data can be reasonably approximated by a piecewise
constant function. When the number of change points K is known, it is reasonable to formulate the
CP detection problem as the following optimization problem

τ det = arg min

τ

K+1
(cid:88)

k=1

C(xobs

τk−1+1:τk

).

When the number of CPs K is unknown, the CP detection problem is deﬁned as

τ det = arg min

τ

dim(τ )+1
(cid:88)

k=1

C(xobs

τk−1+1:τk

) + βdim(τ ),

(1)

(2)

where dim(τ ) is the dimension of a CP vector τ , and β ∈ R+ is a hyper-parameter, which can be
deﬁned based on several methods such as BIC [36]. The optimal solutions of (1) and (2) can be
obtained by DP.
Deﬁnition 1. We denote the event that the optimal CP vector τ det is detected by applying DP
algorithm A to the observed sequence xobs as

τ det = A(xobs).

(3)

2.2

Inference on the detected CPs

For the inference on the kth detected CP τ det

H0,k :

1
k − τ det
τ det
k−1

H1,k :

1
τ det
k − τ det
k−1

(cid:16)

(cid:16)

k , k ∈ [K], we consider the following statistical test
(cid:16)

(cid:17)

(cid:17)

µτ det

k−1+1 + · · · + µτ det

k

=

µτ det

k +1 + · · · + µτ det

k+1

1
k+1 − τ det
τ det

k

µτ det

k−1+1 + · · · + µτ det

k

vs.

(cid:17)

(cid:54)=

1
τ det
k+1 − τ det

k

(cid:16)

µτ det

k +1 + · · · + µτ det

k+1

(4)

(cid:17)

,

where [K] = {1, ..., K} indicates the set of natural numbers up to K. A natural choice of the test
statistic is the difference between the average of the two segments before and after the kth CP

k X = ¯Xτ det
η(cid:62)

k−1+1:τ det

k

− ¯Xτ det

k +1:τ det
k+1

,

(5)

3The covariance matrix Σ is typically estimated by “null” sequences which are known to have no CP (see

Takeuchi et al. [39] for an example in bioinformatics).

3

k

−

1N

k−1+1:τ det
τ det

1
k+1−τ det
τ det

1
k −τ det
τ det
k−1

s:e ∈ RN is a vector whose
where ηk =
elements from position s to e are set to 1, and 0 otherwise. Remember that we do not assume that
the true mean values are piecewise constant, i.e., we do not assume that µτ det
nor
µτ det
. Even without assuming true piecewise constant functions, the population
quantities in (4) are well-deﬁned as the best constant approximations of the subsequences between
two detected CPs.

k−1+1 = . . . = µτ det

k +1 = . . . = µτ det

k +1:τ det
τ det
k+1

, and 1N

1N

k+1

k

k

(cid:0)|η(cid:62)

k X| ≥ |η(cid:62)

k xobs|(cid:1) = 2 min{F0,η(cid:62)

Suppose, for now, that the hypotheses in (4) are ﬁxed, i.e., non-random. Then, the naive (two-sided)
p-value is given as
= PH0,k
pnaive
k Σηk
k
where Fm,s2 is the c.d.f. of Normal distribution N(m, s2).
However, since the hypotheses in (4) are actually not ﬁxed in advance, the naive p-value is not valid
in the sense that, if we reject H0,k with a signiﬁcance level α (e.g., α = 0.05), the false detection
rate (type-I error) cannot be controlled at level α. This is due to the fact that the hypotheses in (4)
are selected by data, and selection bias exists. One way to avoid the selection bias is to consider
the sampling distribution of a test statistic conditional on the selection event. Thus, we employ the
following conditional p-value

k xobs), 1 − F0,η(cid:62)

k xobs)}, (6)

(η(cid:62)

(η(cid:62)

k Σηk

pselective
k

= PH0,k

(cid:16)

|η(cid:62)

k X| ≥ |η(cid:62)

k xobs| | A(X) = A(xobs), q(X) = q(xobs)

(cid:17)

,

(7)

where A(X) = A(xobs) indicates the event that the detected CP vector for a random sequence
X is the same as the detected CP vector for the observed sequence xobs. The second condition
q(X) = q(xobs) indicates that the component which is independent of the test statistic η(cid:62)
k X for a
random sequence X is the same as the one for xobs 4. The q(X) corresponds to the component z in
the seminal paper (see [24], Sec 5, Eq 5.2 and Theorem 5.2), and it is given by
k Σηk)−1.

k )X where c = Σηk(η(cid:62)

q(X) = (IN − cη(cid:62)

The p-value in (7) is called selective type I error or selective p-values in SI literature [12]. Figures 8
and 9 in Appendix A.4 show the distribution of naive p-values and selective p-values when the null
hypothesis H0,k is true. The naive p-values are not uniformly distributed, while selective p-values
are. The uniformly distributed property is necessary for valid p-values since it indicates

PH0,k

(cid:0)pselective

k

< α(cid:1) = α, ∀α ∈ [0, 1].

Our contribution is to provide an efﬁcient method for computing selective p-value in (7) by character-
izing the selection event A(X) = A(xobs), which is computationally challenging because we have
to ﬁnd the whole set of sequences in RN having the same optimal CP vectors on xobs.

3 Proposed Method

We propose a method for computing selective p-values in (7). We focus here on the case where the
number of CPs K is ﬁxed. The case for unknown K will be discussed in §4. Figure 2 shows the
schematic illustration of the OptSeg-SI method.

3.1 Conditional Data Space Characterization

Let us deﬁne the set of x ∈ RN which satisﬁes the conditions in (7) by

X = {x ∈ RN | A(x) = A(xobs), q(x) = q(xobs)}.

Based on the second condition q(x) = q(xobs), the data in X is restricted to a line (see Sec 6 in [26],
and [12]). Therefore, the set X can be re-written, using a scalar parameter z ∈ R, as
X = {a + bz | z ∈ Z}, where Z = {z ∈ R | A(a + bz) = A(xobs)}

4In the unconditional case (6), the condition q(X) = q(xobs) does not change the sampling distribution since
k X and q(X) are (marginally) independent. On the other hand, under the condition with A(X) = A(xobs),
η(cid:62)
η(cid:62)
k X and q(X) are not conditionally independent. See Fithian et al. [12], Lee et al. [24] for the details.

4

Figure 2: Schematic illustration of the proposed OptSeg-SI method.
By applying a CP detection algorithm on the observed sequence xobs,
the optimal CP vector τ det is obtained. In the OptSeg-SI method, the
statistical inference is conducted conditional on the subspace X whose
data has the same optimal CP vector as xobs. We introduce a parametric
programming method for efﬁciently characterizing the conditional data
space X .

Figure 3: A set of QFs each of which
corresponds to a CP vector τ ∈ Tk,n.
The dotted grey QFs correspond to CP
vectors that are not optimal for any z ∈
R. A set {t1, t2, t3, t4} contains CP
vectors that are optimal for some z ∈
R.

k Σηk)−1. Now, let us denote a random variable Z ∈ R and its
with a = q(xobs) and b = Σηk(η(cid:62)
observation zobs ∈ R, which satisfy X = a + bZ and xobs = a + bzobs. Then, the selective p-value
in (7) is re-written as
pselective
k

(cid:0)|η(cid:62)
k X| > |η(cid:62)
Since variable Z ∼ N(0, η(cid:62)
k Σηk) under the null hypothesis, the law of Z | Z ∈ Z follows a
truncated Normal distribution. Once the truncation region Z is identiﬁed, the selective p-value in (8)
can be computed as

k xobs| | X ∈ X (cid:1) = PH0,k

(cid:0)|Z| > |zobs| | Z ∈ Z(cid:1) .

= PH0,k

(8)

pselective
k

= F Z

0,η(cid:62)

k Σηk

(−|zobs|) + 1 − F Z

0,η(cid:62)

k Σηk

(|zobs|),

where F E
truncation region E. Therefore, the main task is to identify Z.

m,s2 is the c.d.f. of the truncated Normal distribution with mean m, variance s2 and the

Important notations.
set of sequences parametrized by a scalar parameter z ∈ R, we denote these sequences by

In the rest of this paper, we use the following notations. Since we focus on a

x(z) = a + bz

(9)

or just simply by z. For a sequence with length n ∈ [N ], the set of all possible CP vectors with dimen-
sion k ∈ [K] is written as Tk,n. Given x(z), the loss of segmenting its ﬁrst n sub-sequence x(z)1:n
(cid:1) . For
with a k-dimensional CP vector τ ∈ Tk,n is written as Lk,n(z, τ ) = (cid:80)k+1
a subsequence x(z)1:n, the optimal loss and the optimal k-dimensional CP vector are respectively
written as

κ=1 C (cid:0)x(z)τκ−1+1:τκ

Lopt

k,n(z) = min
τ ∈Tk,n

Lk,n(z, τ ), T opt

k,n (z) = arg min
τ ∈Tk,n

Lk,n(z, τ ).

(10)

Note that the notation z ∈ R in the deﬁnition (10) indicates that it corresponds to the sequence x(z).

Main idea for identifying truncation region Z. Since we denoted x(z) = a + bz as in (9),
truncation region Z is re-written as follows

Z = {z ∈ R | A(x(z)) = A(xobs)} = {z ∈ R | T opt

K,N (z) = A(xobs)}.

(11)

The main idea is to efﬁciently compute the optimal path of CP vectors T opt
values of z ∈ R, which is computationally challenging. After T opt
truncation region Z can be easily characterized, and the selective p-value in (8) can be computed.

K,N (z) ∈ TK,N for all
K,N (z) is identiﬁed for all z ∈ R,

3.2 Parametric CP detection

We introduce an efﬁcient way to compute T opt
K,N (z) for all z ∈ R. Although it seems intractable to
solve this problem for inﬁnitely many values of z, we can complete the task with a ﬁnite number

5

All possible changepoint (CP) vectors,,,,CP detection algorithmData spaceConditional data spaceParametrized lineProposed Method1020304050600102030-30-20-10𝑧Loss𝒕1𝒕2𝒕3𝒕4Algorithm 1 paraCP(n, k, ˆTk,n)
Input: n, k, ˆTk,n
1: u ← 1, z1 ← −∞, t1 ← T opt

k,n (zu) = arg min
τ ∈ ˆTk,n

Lk,n(zu, τ )

2: while zu < +∞ do
3:

Find the next breakpoint zu+1 > zu and the next optimal CP vector tu+1 such that

Lk,n(zu+1, tu) = Lk,n(zu+1, tu+1).

u ← u + 1

4:
5: end while
6: U ← u
Output: {(zu, tu)}U

u=1

of operations. Algorithm 1 shows the overview of our parametric CP detection method. Here, the
algorithm is described in terms of general n ∈ [N ] and k ∈ [K] along with a set of CP vectors ˆTk,n.
In the current subsection, we set n = N , k = K and ˆTk,n = Tk,n. The case with general n, k and
ˆTk,n will be discussed in §3.3.
In our parametric CP detection method, we exploit the fact that, for each CP vector τ ∈ Tk,n, the
loss function is written as a quadratic function (QF) of z whose coefﬁcients depend on τ ∈ Tk,n.
Since the number of possible CP vectors in Tk,n is ﬁnite, the parametric CP detection problem can
be characterized by a ﬁnite number of these QFs. Figure 3 illustrates the set of QFs each of which
corresponds to a CP vector τ ∈ Tk,n. Since the minimum loss for each z ∈ R is the point-wise
minimum of these QFs, the optimal loss function Lopt
k,n(z) is the lower envelope of the set of QFs,
which is represented as a piecewise QF of z ∈ R. Parametric CP detection is interpreted as the
problem of identifying this piecewise QF.

In Algorithm 1, multiple breakpoints z1 < z2 < . . . < zU are computed one by one. Each breakpoint
zu, u ∈ [U ], indicates a point at which the optimal CP vector is replaced from one to the other in the
piecewise QF. By ﬁnding all these breakpoints {zu}U
u=1, the
piecewise QF as in Figure 3 can be identiﬁed.

u=1 and the optimal CP vectors {tu}U

The algorithm is initialized at the optimal CP vector for z = −∞, which can be easily identiﬁed
based on the coefﬁcients of the QFs. At step u, u ∈ [U ], the task is to ﬁnd the next breakpoint zu+1
and the next optimal CP vector tu+1. This task can be done by ﬁnding the smallest zu+1 greater
than zu among the intersections of the current QF Lk,n(z, tu) and each of the other QFs Lk,n(z, τ )
for τ ∈ Tk,n \ {tu}. This step is repeated until we ﬁnd the optimal CP vector when z = +∞. The
algorithm returns the sequences of breakpoints and optimal CP vectors {(zu, tu)}U
u=1. The entire
path of optimal CP vectors for z ∈ R is given by T opt

k,n (z) = tu, u ∈ [U ], if z ∈ [zu, zu+1].

3.3 Parametric DP

Unfortunately, parametric CP detection algorithm with the inputs N , K and TK,N in the previous
subsection is computationally impractical because the number of all possible CP vectors |TK,N | is
exponentially increasing with N and K. To resolve this computational issue, we utilize the concept of
standard DP, and apply to parametric case, which we call parametric DP. The basic idea of parametric
DP is to exclude the CP vectors τ ∈ Tk,n that cannot be optimal at any z ∈ R.

In standard DP for a CP detection problem (for a speciﬁc z)
Standard DP (speciﬁc value of z).
with N and K, we use K × N table whose (k, n)th element contains T opt
k,n (z), the vector of optimal
k CPs for the subsequence x(z)1:n. The optimal CP vector for each of the subproblem with n and k
can be used for efﬁciently computing the optimal CP vector for the original problem with N and K.

Let concat(v, s) be the operator for concatenating a vector v and a scalar s. Then, it is known that
the following equation, which is often called Bellman equation, holds:

T opt

k,n (z) = arg min
τ (m)

{Lk,n (z, τ (m))}n−1

m=k ,

(12)

6

for n = 1 to N do

Algorithm 2 paraDP(x(z), K)
Input: x(z) and K
1: for k = 1 to K do
2:
3:
4:
5:
6:
7: end for
Output: T opt
K,N

ˆTk,n ← Lemma 1
{(zu, tu)}U
T opt
k,n ← {tu}U
end for

u=1

u=1 ← paraCP(n, k, ˆTk,n)

Algorithm 3 SI for Optimal CPs (OptSeg-SI)
Input: xobs and K
1: τ det ← A(xobs)
2: for τ det
3:
4:
5:

k ∈ τ det do
x(z) ← Eq.(9)
T opt
K,N ← paraDP(x(z), K)
Z ← ∪T opt
K,N (z)∈T opt

{z : T opt

K,N

K,N (z) = A(xobs)}

pselective
k

6:
7: end for
Output: {(τ det

← Eq.(8)

k , pselective
k

)}K

k=1

where τ (m) = concat(T opt
k−1,m(z), m), m ∈ {k, . . . , n − 1}. The Bellman equation (12) enables
us to efﬁciently compute the optimal CP vector for the problem with n and k by using the optimal CP
vectors of its sub-problems.

(cid:110)

k,n =

τ ∈ Tk,n | ∃z ∈ R s.t. Lopt

Parametric DP (for all values of z ∈ R). Our basic idea is to similarly construct a K × N table
whose (k, n)th element contains T opt
, which is
k,n , we construct a set ˆTk,n ⊇ T opt
a set of CP vectors that are optimal for some z ∈ R. To identify T opt
k,n ,
which is a set of CP vectors having potential to be optimal. In the same way as (12), we can consider
Bellman equation for constructing ˆTk,n as described in the following Lemma.
Lemma 1. For n ∈ [N ] and k ∈ [K], the set of CP vectors having potential to be optimal is
constructed as ˆTk,n = ∪n−1
k−1,m, m)}, where we extend the concat operator for the
case where the ﬁrst argument is a set of vectors, which simply returns the set of concatenated vectors.

m=k{concat(T opt

k,n(z) = Lk,n(z, τ )

(cid:111)

In other words, the set ˆTk,n can be generated from the optimal CP vectors of its sub-problems T opt
k−1,m
for m ∈ {k, . . . , n − 1}. The proof for this result is deferred to Appendix A.1. From Lemma 1, we
can efﬁciently construct ˆTk,n which is subsequently used to identify T opt
k,n . By repeating the recursive
procedure and storing T opt
k,n in the (k, n)th element of the table from smaller n and k to larger n and k,
we can end up with ˆTK,N ⊇ T opt
K,N . By using parametric DP, the size of ˆTK,N can be smaller than the
size of all possible CP vectors TK,N , which makes the computational cost of paraCP(N, K, ˆTK,N )
substantially decreased compared to paraCP(N, K, TK,N ).

The parametric DP method is presented in Algorithm 2 and the entire OptSeg-SI method for computing
selective p-values of the optimal CPs is summarized in Algorithm 3. Although they are not explicitly
described in the algorithm, we also used several computational tricks for further reducing the size of
ˆTk,n. See Appendix A.3 for the details.

4 Extension to Unknown K Case

We present an approach for testing the signiﬁcance of CPs detected by (2). The basic idea is the
same as the proposed method for ﬁxed K. With a slight abuse of notations, we use the following
similar notations as the ﬁxed K case. For a sequence with length n ∈ [N ], the set of all possible CP
vectors is written as Tn. Given x(z) as in (9), the loss of segmenting its sub-sequence x(z)1:n with a
(cid:1) + βdim(τ ). The optimal
C (cid:0)x(z)τκ−1+1:τκ
CP vector τ ∈ Tn is written as Ln(z, τ ) = (cid:80)dim(τ )+1
loss and the optimal CP vector on x(z)1:n are respectively written as Lopt
n (z) = minτ ∈Tn Ln(z, τ ),
T opt
n (z) = arg minτ ∈Tn Ln(z, τ )5.

κ=1

Identiﬁcation of truncation region Z. To calculate pselective
terize the truncation region Z = {z ∈ R | T opt

N (z) = A(xobs)}, by computing T opt

for the kth detected CP, we charac-
N (z) for all

k

5We recently noticed that (cid:96)1-penalty based SI for CP detection was extended to (cid:96)0-penalty [20], which results

in a similar approach with the “unknown K case” in our algorithm.

7

z ∈ R. We can slightly modify Algorithm 1 to the unknown K case to compute T opt
z ∈ R. Let T opt
as T opt

N (z) for all
denote a set of CP vectors that are optimal at some z ∈ R for subsequence x(x)1:n

n = (cid:8)τ ∈ Tn | ∃z ∈ R s.t. Lopt

n (z) = Ln(z, τ )(cid:9) .

n

Since the set of all possible CP vectors TN is huge, we use parametric DP with two additional
computational tricks (Lemmas 2 and 3 below) for ﬁnding a substantially reduced set of CP vectors
ˆTN ⊆ TN which contains all the optimal CP vectors for any z ∈ R, i.e., ˆTN ⊇ T opt
N . The following
two lemmas show how to construct ˆTn by removing the CP vectors that never belong to T opt
n .
Lemma 2. For m < n, if a vector τ (cid:54)∈ T opt
Lemma 3. For m < n, if τ (cid:54)∈ T opt

m , then concat(τ , m) (cid:54)∈ T opt
n .

m (z) for any z ∈ R, then τ (cid:54)∈ T opt
n .

m and Lm(z, τ ) − β > Lopt

Proofs for these two lemmas are deferred to Appendix A.2. Based on Lemmas 2 and 3, ˆTn can
be constructed by ˆTn = ∪n−1
m that does
not satisfy Lemma 3. Then, we can use ˆTn to ﬁnd T opt
and continue this process
recursively for larger n until we get T opt
N , we can fully characterize truncation
region Z and ﬁnally calculate selective p-values.

m , m) ∪ S}, where S is a set of τ (cid:54)∈ T opt

N . After identifying T opt

m=k{concat(T opt

n . We store T opt

n

5 Numerical Experiments

We only highlight the main results. More details can be found in Appendix A.5.

Methods for comparison. We compared our OptSeg-SI method with SMUCE [14], which is an
asymptotic test for multiple detected CPs, and SI for Binary Segmentation [18] (BinSeg-SI). It was
reported that SI for Fused Lasso (proposed by the same authors), is worse than BinSeg-SI. Therefore,
we only compared to BinSeg-SI. We additionally compared our method with SI method for optimal
CPs with over-conditioning (OptSeg-SI-oc) to demonstrate the advantage of minimum conditioning.
The details of OptSeg-SI-oc are shown in Appendix A.7 6.

Simulation setup. Regarding false positive rate (FPR) experiments, we generated 1,000 null se-
quences x = (x1, ..., xN ) in which xi∈[N ] ∼ N(0, 1) for each N ∈ {10, 20, 30, 40}. In regard of
testing the power, we generated sequences x = (x1, ..., xN ) with sample size N = 60, in which



xi∈[N ] ∼ N(µi, 1), µi =

1
1 + ∆µ
1 + 2∆µ



if 1 ≤ i ≤ 20,
if 21 ≤ i ≤ 40
otherwise ,

for each ∆µ ∈ {1, 2, 3, 4}. For each case, we ran 250 trials. Since the tests are performed only when
a CP is selected, the power is deﬁned as follows [18]:

Power (or Conditional Power) =

# correctly detected & rejected
# correctly detected

.

A detection is considered to be correct if it is within ±2 of the true CP locations. Since it is often
difﬁcult to accurately identify exact CPs in the presence of noise, many existing CP detection studies
consider a detection to be correct if it is within L positions of the true CP locations [44]. We
considered L = 2 to be consistent with our competitive method [18]. We used BIC [36] for the
choice of β when K is unknown. We chose the signiﬁcance level α = 0.05. We used Bonferroni
correction to account for the multiplicity in all the experiments.

Experimental results. Figures 4 and 5 respectively show the comparison results of the false positive
rate (FPR) and true positive rate (TPR) when K is ﬁxed and K is unknown. In both cases, since
SMUCE guarantee is only asymptotic, it could not control the FPR when N is small. While BinSeg-
SI and OptSeg-SI-oc properly control the FPR, their powers are low because of over-conditioning.
OptSeg-SI always has high power while properly controlling the FPR. Figure 6 shows the power
demonstration of the OptSeg-SI method. While the existing methods missed many of true CPs, our
method could identify almost all of them. Figure 7 shows the efﬁciency of OptSeg-SI method.We
generated data for each case (N, K) ∈ {(200, 9), ..., (1200, 59)}. We ran 10 trials for each case.

6We ﬁrst developed OptSeg-SI-oc as our ﬁrst SI method for optimal CPs detected by DP (unpublished).

Later, its drawback (the over-conditioning) was removed by the OptSeg-SI method in this paper.

8

(a) False Positive Rate

(b) Power

(a) False Positive Rate

(b) Power

Figure 4: False positive rate (FPR) and power compari-
son when K is ﬁxed.

Figure 5: False positive rate (FPR) and power compari-
son when K is unknown.

Figure 6: Power demonstration of the OptSeg-SI method. The under-
lying mechanism (blue), data points (grey), and the results of each
method (red) are shown in each panel. The result of OptSeg-SI is
mostly close to the ground truth compared to the other methods.

Figure 7: Computing time of the
OptSeg-SI method. The computing
time of our proposed method is almost
linear.

Table 1: Power comparison on real-world bioinformatics related datasets.

SMUCE OptSeg-SI-oc

BinSeg-SI OptSeg-SI

D1
D2

0.53
0.62

0.24
0.27

0.33
0.32

0.75
0.71

Besides, we also conducted the following experiments to demonstrate the robustness of the OptSeg-SI
method in terms of the FPR control:

• Non-normal data: we consider the data following Laplace distribution, skew normal distribution
(skewness coefﬁcient 10) and t20 distribution. In each experiment, we generated 12,000 null sequences
for N ∈ {10, 20, 30, 40}. We test the FPR for both α = 0.05 and α = 0.1. We conﬁrmed that
our method still maintains good performance on FPR control. The results are shown in Appendix
A.5. Besides, for dealing with the case of non-normal data, we can also apply a popular Box-Cox
transformation [5] to the data before performing our method.

• Unknown σ2: we consider the case when the variance is also estimated from the data. We
generated 12,000 null sequences for N ∈ {50, 60, 70, 80}. Our OptSeg-SI method still can properly
control the FPR. The results are shown in Appendix A.5.

We also performed TPR comparison on real-world dataset in which we showed that our method
always has higher power compared to other existing method. We used jointseg package [35] to
generate realistic DNA copy number proﬁles of cancer samples with “known" truth. Two datasets
with 1,000 proﬁles of length N = 60 and true K = 2 for each were created as follows:

• D1: Resample from GSE11976 with tumor fraction = 1
• D2: Resample from GSE29172 with tumor fraction = 1

The results are shown in Table 1. Our proposed OptSeg-SI has higher power than the other methods
in all cases. We also applied OptSeg-SI to the Array CGH data provided by Snijders et al. [37] and
the Nile data which contains annual ﬂow volume of the Nile river. All of the results are consistent
with Snijders et al. [37], Jung et al. [21]. More details of the results can be found in Appendix A.6.

9

00.020.040.060.080.10.1210203040FPRNSMUCEOptSeg-SI-ocBinSeg-SIOptSeg-SI00.10.20.30.40.50.60.70.80.911234PowerΔuSMUCEOptSeg-SI-ocBinSeg-SIOptSeg-SI00.020.040.060.080.10.1210203040FPRNSMUCEOptSeg-SI-ocBinSeg-SIOptSeg-SI00.10.20.30.40.50.60.70.80.911234PowerΔuSMUCEOptSeg-SI-ocBinSeg-SIOptSeg-SIOptSeg-SI-ocBinSeg-SISMUCEOptSeg-SI6 Conclusion

In this paper, we have introduced a powerful SI approach for the CP detection problem. We have
conducted experiments on both synthetic and real-world datasets to show the good performance of
the proposed OptSeg-SI method. In the future, we could extend our method to the case of multi-
dimensional sequences [45]. For this case, computational efﬁciency is also a big challenge. Therefore,
providing an efﬁcient approach would also represent a valuable contribution.

Broader Impact

Reliable machine learning (ML), which is the problem of assessing the reliability of data-driven
knowledge obtained by ML algorithms, is one of the most important issues in the ML community.
Changepoint (CP) detection is an important unsupervised learning task, and has been studied in many
areas. Unfortunately, less attention has been paid to the statistical reliability of the detected CPs.
Without statistical reliability, the results may contain many false detections. These falsely detected
CPs are harmful when they are used for high-stake decision making.

The main idea of this paper is to employ a selective inference — a new promising approach for
assessing the statistical reliability of data-driven hypotheses selected by complex data analysis
algorithms — to quantify the reliability of the detected CPs. By mainly focusing on the reliability,
this paper can have potential impact on reducing the risky as well as improving the quality of
several CP detection-based data analysis tasks such as bioinformatics [14, 35], ﬁnancial analysis
[15], climatology [22], signal processing [19]. Especially for applications in healthcare domain,
since the p-value that we introduced in the paper is valid and it is guaranteed that the probability
of making false decisions is properly controlled, valid p-values can be used as one of many other
possible criteria for making medical decisions.

Acknowledgments and Disclosure of Funding

This work was partially supported by MEXT KAKENHI (20H00601, 16H06538), JST CREST
(JPMJCR1502), RIKEN Center for Advanced Intelligence Project, and RIKEN Junior Research
Associate Program.

References

[1] S. Aminikhanghahi and D. J. Cook. A survey of methods for time series change point detection.

Knowledge and information systems, 51(2):339–367, 2017.

[2] I. E. Auger and C. E. Lawrence. Algorithms for the optimal identiﬁcation of segment neighbor-

hoods. Bulletin of mathematical biology, 51(1):39–54, 1989.

[3] F. Bachoc, H. Leeb, and B. M. Pötscher. Valid conﬁdence intervals for post-model-selection

predictors. arXiv preprint arXiv:1412.4605, 2014.

[4] F. Bachoc, G. Blanchard, P. Neuvial, et al. On the post selection inference constant under

restricted isometry properties. Electronic Journal of Statistics, 12(2):3736–3757, 2018.

[5] G. E. Box and D. R. Cox. An analysis of transformations. Journal of the Royal Statistical

Society: Series B (Methodological), 26(2):211–243, 1964.

[6] J. Chen and Y.-P. Wang. A statistical change point model approach for the detection of dna copy
number variations in array cgh data. IEEE/ACM Transactions on Computational Biology and
Bioinformatics, 6(4):529–541, 2008.

[7] S. Chen and J. Bien. Valid inference corrected for outlier removal. Journal of Computational

and Graphical Statistics, pages 1–12, 2019.

[8] Y. Choi, J. Taylor, R. Tibshirani, et al. Selecting the number of principal components: Estimation

of the true rank of a noisy matrix. The Annals of Statistics, 45(6):2590–2617, 2017.

[9] V. N. L. Duy and I. Takeuchi. Parametric programming approach for powerful lasso selective

inference without conditioning on signs. arXiv preprint arXiv:2004.09749, 2020.

10

[10] V. N. L. Duy, S. Iwazaki, and I. Takeuchi. Quantifying statistical signiﬁcance of neural network
representation-driven hypotheses by selective inference. arXiv preprint arXiv:2010.01823,
2020.

[11] P. Fearnhead, R. Maidstone, and A. Letchford. Detecting changes in slope with an l 0 penalty.

Journal of Computational and Graphical Statistics, 28(2):265–275, 2019.

[12] W. Fithian, D. Sun, and J. Taylor. Optimal inference after model selection. arXiv preprint

arXiv:1410.2597, 2014.

[13] W. Fithian, J. Taylor, R. Tibshirani, and R. Tibshirani. Selective sequential model selection.

arXiv preprint arXiv:1512.02565, 2015.

[14] K. Frick, A. Munk, and H. Sieling. Multiscale change point inference. Journal of the Royal

Statistical Society: Series B (Statistical Methodology), 76(3):495–580, 2014.

[15] P. Fryzlewicz et al. Wild binary segmentation for multiple change-point detection. The Annals

of Statistics, 42(6):2243–2281, 2014.

[16] Z. Harchaoui, E. Moulines, and F. R. Bach. Kernel change-point analysis. In Advances in

neural information processing systems, pages 609–616, 2009.

[17] S. Hyun, M. G’Sell, R. J. Tibshirani, et al. Exact post-selection inference for the generalized

lasso path. Electronic Journal of Statistics, 12(1):1053–1097, 2018.

[18] S. Hyun, K. Lin, M. G’Sell, and R. J. Tibshirani. Post-selection inference for changepoint detec-
tion algorithms with application to copy number variation data. arXiv preprint arXiv:1812.03644,
2018.

[19] V. Jandhyala, S. Fotopoulos, I. MacNeill, and P. Liu. Inference for single and multiple change-

points in time series. Journal of Time Series Analysis, 34(4):423–446, 2013.

[20] S. Jewell, P. Fearnhead, and D. Witten. Testing for a change in mean after changepoint detection.

arXiv preprint arXiv:1910.04291, 2019.

[21] M. Jung, S. Song, and Y. Chung. Bayesian change-point problem using bayes factor with
hierarchical prior distribution. Communications in Statistics-Theory and Methods, 46(3):1352–
1366, 2017.

[22] R. Killick, P. Fearnhead, and I. A. Eckley. Optimal detection of changepoints with a linear
computational cost. Journal of the American Statistical Association, 107(500):1590–1598,
2012.

[23] N. Kriegeskorte, W. K. Simmons, P. S. Bellgowan, and C. I. Baker. Circular analysis in systems

neuroscience: the dangers of double dipping. Nature neuroscience, 12(5):535, 2009.

[24] J. D. Lee, D. L. Sun, Y. Sun, J. E. Taylor, et al. Exact post-selection inference, with application

to the lasso. The Annals of Statistics, 44(3):907–927, 2016.

[25] S. Li, Y. Xie, H. Dai, and L. Song. M-statistic for kernel change-point detection. In Advances

in Neural Information Processing Systems, pages 3366–3374, 2015.

[26] K. Liu, J. Markovic, and R. Tibshirani. More powerful post-selection inference, with application

to the lasso. arXiv preprint arXiv:1801.09037, 2018.

[27] J. R. Loftus. Selective inference after cross-validation. arXiv preprint arXiv:1511.08866, 2015.
[28] J. R. Loftus and J. E. Taylor. A signiﬁcance test for forward stepwise model selection. arXiv

preprint arXiv:1405.3920, 2014.

[29] R. Maidstone, T. Hocking, G. Rigaill, and P. Fearnhead. On optimal multiple changepoint

algorithms for large data. Statistics and Computing, 27(2):519–533, 2017.

[30] S. Mika, G. Ratsch, J. Weston, B. Scholkopf, and K.-R. Mullers. Fisher discriminant analysis
with kernels. In Neural networks for signal processing IX: Proceedings of the 1999 IEEE signal
processing society workshop (cat. no. 98th8468), pages 41–48. Ieee, 1999.

[31] V. M. Muggeo and G. Adelﬁo. Efﬁcient change point detection for genomic sequences of

continuous measurements. Bioinformatics, 27(2):161–166, 2011.

[32] A. B. Olshen, E. Venkatraman, R. Lucito, and M. Wigler. Circular binary segmentation for the

analysis of array-based dna copy number data. Biostatistics, 5(4):557–572, 2004.
[33] E. S. Page. Continuous inspection schemes. Biometrika, 41(1/2):100–115, 1954.

11

[34] S. Panigrahi, J. Taylor, and A. Weinstein. Bayesian post-selection inference in the linear model.

arXiv preprint arXiv:1605.08824, 28, 2016.

[35] M. Pierre-Jean, G. Rigaill, and P. Neuvial. Performance evaluation of dna copy number

segmentation methods. Brieﬁngs in bioinformatics, 16(4):600–615, 2014.

[36] G. Schwarz et al. Estimating the dimension of a model. The annals of statistics, 6(2):461–464,

1978.

[37] A. M. Snijders, N. Nowak, R. Segraves, S. Blackwood, N. Brown, J. Conroy, G. Hamilton, A. K.
Hindle, B. Huey, K. Kimura, et al. Assembly of microarrays for genome-wide measurement of
dna copy number. Nature genetics, 29(3):263, 2001.

[38] S. Suzumura, K. Nakagawa, Y. Umezu, K. Tsuda, and I. Takeuchi. Selective inference for
sparse high-order interaction models. In International Conference on Machine Learning, pages
3338–3347, 2017.

[39] I. Takeuchi, H. Tagawa, A. Tsujikawa, M. Nakagawa, M. Katayama-Suguro, Y. Guo, and
M. Seto. The potential of copy number gains and losses, detected by array-based comparative
genomic hybridization, for computational differential diagnosis of b-cell lymphomas and genetic
regions involved in lymphomagenesis. haematologica, 94(1):61–69, 2009.

[40] K. Tanizaki, N. Hashimoto, Y. Inatsu, H. Hontani, and I. Takeuchi. Computing valid p-values
for image segmentation by selective inference. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 9553–9562, 2020.

[41] X. Tian, J. Taylor, et al. Selective inference with a randomized response. The Annals of Statistics,

46(2):679–710, 2018.

[42] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. Sparsity and smoothness via the
fused lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(1):
91–108, 2005.

[43] R. J. Tibshirani, J. Taylor, R. Lockhart, and R. Tibshirani. Exact post-selection inference for
sequential regression procedures. Journal of the American Statistical Association, 111(514):
600–620, 2016.

[44] C. Truong, L. Oudre, and N. Vayatis. Selective review of ofﬂine change point detection methods.

Signal Processing, page 107299, 2019.

[45] Y. Umezu and I. Takeuchi. Selective inference for change point detection in multi-dimensional

sequences. arXiv preprint arXiv:1706.00514, 2017.

[46] G. J. van den Burg and C. K. Williams. An evaluation of change point detection algorithms.

arXiv preprint arXiv:2003.06222, 2020.

[47] L. Y. Vostrikova. Detecting “disorder” in multidimensional random processes. In Doklady

Akademii Nauk, volume 259, pages 270–274. Russian Academy of Sciences, 1981.

[48] F. Yang, R. F. Barber, P. Jain, and J. Lafferty. Selective inference for group-sparse linear models.

In Advances in Neural Information Processing Systems, pages 2469–2477, 2016.

12

A Appendix

A.1 Proof for Lemma 1

Lemma 1. For n ∈ [N ] and k ∈ [K], the set of CP vectors having potential to be optimal is
constructed as

k−1,m, m)},

ˆTk,n = ∪n−1

m=k{concat(T opt
where we extend the concat operator for the case where the ﬁrst argument is a set of vectors, which
simply returns the set of concatenated vectors.
Proof. We prove the lemma by showing that any CP vector τ (cid:54)∈ T opt
k−1,m, for m ∈ {k, . . . , n − 1},
cannot be subvector of the optimal CP vectors for problems with larger n and k for any z ∈ R, i.e.,
concat(τ , m) (cid:54)∈ T opt
k−1,m be a CP vector which
is NOT optimal for all z ∈ R, i.e.,

k,n for n > m. For m ∈ {k, . . . , n − 1}, let τ (cid:54)∈ T opt

(13)

It suggests that, for any m ∈ {k, . . . , n − 1} and z ∈ R,

Lk−1,m(z, τ ) > Lopt

k−1,m(z) ∀z ∈ R.

Lopt

k,n(z) =

min
m(cid:48)∈{k,...,n−1}

(cid:16)

Lopt

k−1,m(cid:48)(z) + C(x(z)m(cid:48)+1:n)

(cid:17)

≤ Lopt
k−1,m(z) + C(x(z)m+1:n)
< Lk−1,m(z, τ ) + C(x(z)m+1:n)

for all z ∈ R. Thus, for any choice of m ∈ {k, . . . , n − 1} and z ∈ R, τ (cid:54)∈ T opt
k−1,m cannot be a
subvector of the optimal CP vector for problems with larger n and k. In other words, only the CP
vectors in ∪n−1
k−1,m can be used as the subvector of optimal CP vectors for problems with larger
n and k.

m=kT opt

A.2 Proofs for Lemma 2 and 3 for the case when K is unknown in §4

Lemma 2. For m < n, if a vector τ (cid:54)∈ T opt
Proof. For m < n, if a vector τ (cid:54)∈ T opt
m ,

m , then concat(τ , m) (cid:54)∈ T opt
n .

It suggests that, for any m ∈ {0, ..., n − 1} and z ∈ R,

Lm(z, τ ) > Lopt

m (z) ∀z ∈ R.

Lopt

n (z) =

min
m(cid:48)∈{0,...,n−1}

{Lopt

m(cid:48) (z) + C(x(z)m(cid:48)+1:n) + β}

≤ Lopt
m (z) + C(x(z)m+1:n) + β
< Lm(z, τ ) + C(x(z)m+1:n) + β.

Therefore, for any m ∈ {0, ..., n − 1}, if τ (cid:54)∈ T opt
Lemma 3. For m < n, if τ (cid:54)∈ T opt

m , then concat(τ , m) (cid:54)∈ T opt
n .

m and
Lm(z, τ ) − β > Lopt

m (z) ∀z ∈ R

holds, then τ (cid:54)∈ T opt
n .
Proof. For any m ∈ {0, ..., n − 1} and z ∈ R, we have

Lopt

n (z) =

min
m(cid:48)∈{0,...,n−1}

{Lopt

m(cid:48) (z) + C(x(z)m(cid:48)+1:n) + β}

≤ Lopt

m (z) + C(x(z)m+1:n) + β.

For any m ∈ {0, ..., n − 1}, if a CP vector τ (cid:54)∈ T opt

m satisﬁes Lemma 3, then it suggests

Lopt
⇔ Lopt
⇔ Lopt

m (z) + C(x(z)m+1:n) + β

n (z) ≤ Lopt
n (z) < Lm(z, τ ) − β + C(x(z)m+1:n) + β
n (z) < Lm(z, τ ) + C(x(z)m+1:n)

13

for all z ∈ R. On the other hand, we have

Lm(z, τ ) + C(x(z)m+1:n) ≤ Ln(z, τ )
holds for any z ∈ R because the cost is always reduced when adding a changepoint at position m
without the penalty term. Hence, we have

Lopt

n (z) < Ln(z, τ )

for all z ∈ R. Therefore, τ (cid:54)∈ T opt

n

and Lemma 3 holds.

A.3 Additional tricks for methods proposed in §3.

Finding optimal CP vector when z = −∞ in paraCP(n, k, ˆTk,n) in Algorithm 1. For each
τ ∈ ˆTk,n, the corresponding loss function at τ is written as a positive deﬁnite quadratic function.
Therefore, at z = −∞, the optimal CP vector is the one whose corresponding loss function Ln(z, τ )
has the smallest coefﬁcient of the quadratic term. If there are more than one quadratic function having
the same smallest quadratic coefﬁcient, we then choose the one that has the largest coefﬁcient in
the linear term. If those quadratic functions still have the same largest linear coefﬁcient, we ﬁnally
choose the one that has the smallest constant term.

Additional pruning condition for parametric DP when K is ﬁxed.
In §3.3, we showed that
k,n can be constructed from the set ˆTk,n ⊆ Tk,n. By using the following lemma, we can construct a
T opt
smaller superset of T opt
k,n , which leads to further efﬁciency of parametric DP.

Lemma 4. For n ∈ [N ], and k ∈ [K], let

¯Tk,n = {τ ∈ ˆTk,n−1 \ Pprune} ∪ {concat(T opt

k−1,n−1, n − 1)},

where

Pprune = {τ ∈ ˆTk,n−1 | Lk,n−1(z, τ ) > Lopt

k−1,n−1(z), ∀z ∈ R}.

Then T opt

k,n ⊆ ¯Tk,n ⊆ ˆTk,n.

Proof. First, to show ˆTk,n ⊇ ¯Tk,n, from (13),
ˆTk,n = ∪n−1
m=k{concat(T opt
m=k{concat(T opt
= ∪n−2
= ˆTk,n−1 ∪{concat(T opt
⊇ { ˆTk,n−1 \ Pprune} ∪{concat(T opt

k−1,n−1, n − 1)}

k−1,m, m)}
k−1,m, m)} ∪{concat(T opt

k−1,n−1, n − 1)} = ¯Tk,n.

k−1,n−1, n − 1)}

Next, to show T opt
k, n, i.e., τ (cid:54)∈ T opt

k,n ⊆ ¯Tk,n, we only need to prove that τ ∈ Pprune never be the optimal CP vector at
k,n . For any τ ∈ Pprune

Lk,n(z, τ ) ≥ Lk,n−1(z, τ )
> Lopt
k−1,n−1(z)
= Lopt
k−1,n−1(z) + C(x(z)n:n)

≥

min
m(cid:48)∈{k,...,n−1}

(Lopt

k−1,m(cid:48)(z) + C(x(z)(m(cid:48)+1):n))

= Lopt

k,n(z),

for any z ∈ R. Therefore, τ ∈ Pprune never belongs to T opt
k,n .

A.4 Distribution of naive p-value and selective p-value when the null hypothesis is true

We demonstrate the validity of our proposed OptSeg-SI method by conﬁrming the uniformity of
p-value when the null hypothesis is true. We generated 12,000 null sequences x = (x1, ..., xN ) in

14

which xi∈[N ] ∼ N(0, 1) for each case N ∈ {10, 20, 30, 40} and performed the experiments to check
the distribution of naive p-values and selective p-values. From Figure 8, it is obvious that naive
p-value does not follow uniform distribution. Therefore, it fails to control the false positive rate. The
empirical distributions of selective p-value are shown in Figure 9. The results indicate our proposed
method successfully control the false detection probability.

(a) N = 10

(b) N = 20

(c) N = 30

(d) N = 40

Figure 8: Distribution of naive p-value when the null hypothesis is true.

(a) N = 10

(b) N = 20

(c) N = 30

(d) N = 40

Figure 9: Distribution of selective p-value when the null hypothesis is true.

A.5 Details for numerical experiments.

Methods for Comparison. We compared the performance of the OptSeg-SI with the following
approaches:

• SMUCE [14]. This is asymptotic test for multiple detected CPs. The implementation of SMUCE
is available at https://cran.r-project.org/web/packages/stepR/index.html.

• [BinSeg-SI] SI for Binary Segmentation [18] In Hyun et al. [18], it was reported that SI for
Fused Lasso (proposed by the same authors), is worse than BinSeg-SI. Therefore, we only compare
to BinSeg-SI. BinSeg-SI had been considered as a computationally efﬁcient approximation of the
problem in (7), where the authors additionally condition on extra information for computational
tractability, e.g., the order that CPs are detected. This is one of the reasons why BinSeg-SI has
low power. The implementation of BinSeg-SI is available at https://github.com/robohyun66/
binseginf.

• [OptSeg-SI-oc] SI method for optimal CPs with over-conditioning. In SI, there are mainly
two approaches to characterize the selection event. In the ﬁrst approach, the selection event is
only constructed based on the optimality condition of the problem, which is usually difﬁcult or
computationally impractical. Therefore, the second approach is used to overcome the computational
challenge by additionally conditioning on extra event. Although the type I error can be properly
controlled in the second approach, the power is generally low because of over-conditioning.

To see the advantage of minimum conditioning of the proposed method, we compare with two variants
of SI for optimal CPs (each for ﬁxed K and unknown K cases), which we call OptSeg-SI-oc. In each
of these variants, instead of the truncation region Z characterized in the main paper, its subsets are
used as the conditioning set. These subsets are constructed by considering all the operations when DP
algorithm is used for detecting the optimal CPs. The OptSeg-SI-oc method and BinSeg-SI in Hyun
et al. [18] are categorized as the second approach. We actually ﬁrst developed OptSeg-SI-oc as our
ﬁrst SI method for optimal CPs (unpublished). The derivation of OptSeg-SI-oc is shown in Appendix
A.7. Then, its drawback (over-conditioning) was resolved by the proposed OptSeg-SI method in this
paper.

15

Figure 10: Additional results for power demonstration. In the left ﬁgure, the blue line and the grey circles
indicate the underlying mean and the observed sequence, respectively. The red dotted lines are the results of
optimal segmentation (OptSeg) and binary segmentation (BinSeg) algorithms. Here, the CP detection results of
OptSeg and BinSeg were the same. Then, the signiﬁcance of each CP is tested. With Bonferroni correction, to
control false detection rate at 0.05, the signiﬁcance level is decided by 0.05
9 ≈ 0.006. Three different p-values
are shown for each detected CP: BinSeg-SI p-value, OptSeg-SI-oc p-value and OptSeg-SI p-value. BigSeg-SI
missed many true CPs (D, G, I). This problem is the same for OptSeg-SI-oc (D, E, F, I). The OptSeg-SI method
can identify all true CPs. The segments recovered based on the results of the signiﬁcant testing from each method
are shown in the right ﬁgure.

Experimental Results. We show the detail of experimental results as follows:

• Additional experiment for power demonstration of the proposed method. In Figure 10, we
show additional results to demonstrate that our OptSeg-SI method can identify many true CPs.

• The robustness of the proposed OptSeg-SI method in terms of the FPR control.

– Non-normal data: we considered the data following Laplace distribution, skew normal
distribution (skewness coefﬁcient 10) and t20 distribution. In each experiment, we generated
12,000 null sequences for N ∈ {10, 20, 30, 40}. We tested the FPR for both α = 0.05
and α = 0.1. The FPR results are shown in Figure 11a, 11b and 11c. In case of Laplace
distribution and skew normal distribution, our proposed method can properly control the
FPR. For the case of t20 distribution, the FPR is just a bit higher than the signiﬁcance level.

– Unknown σ2: We generated 12,000 null sequences x = (x1, ..., xN ), in which xi∈[N ] ∼
N(0, 1), for N ∈ {50, 60, 70, 80} and conducted experiments. In this case, the value of
σ2 is also estimated from the data. We ﬁrst perform CP detection algorithm to detect the
segments. Since the estimated variance tends to be smaller than the true value, we calculated
the empirical variance of each segment and set the maximum value for σ2. The results are
shown in Figure 11d. Our proposed method still can properly control the FPR.

• Comparison of FPR control when the sequence contains correlated data points. In this ex-
periment, we demonstrate that the asymptotic method (SMUCE) cannot control the FPR when
the sequence contains correlated data points while our OptSeg-SI method can successfully con-
trol the FPR under the signiﬁcance level α = 0.05. We generated 1,200 null sequences
x = (x1, ..., xN ) ∼ N(µ, Ξ), where N = 20, µ = (µ1, ..., µN ) in which µi∈[N ] = 0, and
Ξ = σ2(ξ|i−j|)i,j∈[N ] in which ξ is degree of correlation and σ2 = 1. We conducted experiments
for ξ ∈ {0.0, 0.2, 0.4, 0.6, 0.8}. The results are shown in Figure 12. When ξ = 0.0, i.e., there is no
correlation between the data points, SMUCE can control the FPR at α = 0.05. However, when ξ
increases, the FPR also increases. It indicates that SMUCE cannot control the FPR when the data
points are correlated. On the other hand, our proposed OptSeg-SI method can successfully control
the FPR under α in all cases.

16

LocationABCDE05×10−140100.0044×10−60.0020.080.0076×10−62×10−124×10−185×10−417×10−10BinSeg-SIOptSeg-SI-ocOptSeg-SIMethodLocationp-value tableABCDEFGHIFGHI01010.74×10−60.0020.1476×10−62×10−124×10−185×10−41BinSeg-SIOptSeg-SI-ocOptSeg-SIMethodLocationValueBinSeg-SIOptSeg-SI-ocOptSeg-SI(a) Laplace distribution

(b) Skew normal distribution

(c) t20 distribution

(d) σ2 is unknown

Figure 11: False positive rate of the proposed OptSeg-SI method when data is non-normal or σ2 is unknown.

Figure 12: Comparison of FPR control when the sequence contains correlated data points. With SMUCE, the
FPR increases when the degree of correlation increases. On the other hand, our proposed OptSeg-SI method can
successfully control the FPR under α = 0.05 in all cases.

A.6 Details for real-data experiments.

Array CGH data. Array CGH analyses detect changes in expression levels across the genome.
The dataset with ground truth was provided in Snijders et al. [37]. The results from our method were
shown in Figure 13 and 14. The solid red line denotes the signiﬁcant changepoint which has the
p-value smaller than the signiﬁcance level after Bonferroni correction. All of the results are consistent
with Snijders et al. [37].

Nile data. The interest lies in unexpected event such as natural disasters. This data is the annual
ﬂow volume of the Nile river at Aswan from 1871 to 1970 (100 years). In Figure 15, the proposed
algorithm results the changepoint at the 28th position, corresponding to year 1899. This result is
consistent with Jung et al. [21].

A.7 Derivation of OptSeg-SI-oc mentioned in §5

As our ﬁrst idea of SI for optimal CPs, we developed OptSeg-SI-oc. However, this method inherits
the drawback of current SI studies (over-conditioning). Therefore, we have not ofﬁcially published it
yet. Later, we developed novel parametric programming techniques and proposed OptSeg-SI, which
is presented in this paper, to address the over-conditioning problem. Here, we show the derivation of
OptSeg-SI-oc.

17

00.10.20.30.410203040FPRNα = 0.05α = 0.100.050.10.1510203040FPRNα = 0.05α = 0.100.10.20.30.40.510203040FPRNα = 0.05α = 0.100.050.10.150.20.250.350607080FPRNα = 0.05α = 0.100.10.20.30.40.50.60.70.800.20.40.60.8FPRCorrelationSMUCEOptSeg-SI(a) Chromosomes 1, 2, 3.

(b) Chromosomes 20, 21, 22.

Figure 13: Experimental results for cell line GM03576.

(a) Chromosome 14.

(b) Chromosomes 17, 18, 19

Figure 14: Experimental results for cell lines GM00143 and GM01750.

Figure 15: Experimental result for Nile data. The changepoint is detected at 28th position which indicates there
is a change in volume level in year 1899.

The main idea behinds OptSeg-SI-oc is to characterize the conditional data space based on all steps
of DP algorithm, i.e., performing inference conditional on all steps of DP. We focus on the case when
K is ﬁxed, and it is easy to extend to the case when K is unknown.

Notation. We denote X (cid:48) as a conditional data space in OptSeg-SI-oc. The difference between
X in §3.1 and X (cid:48) here is that the latter is characterized with additional constraints on DP process.
For an observed sequence xobs ∈ RN , its optimal CP vector is deﬁned as τ det. For a sequence with
length n ∈ [N ], a set of all possible CP vectors with dimension k ∈ [K] is deﬁned as Tk,n. Given
x ∈ RN , the loss of segmenting its sub-sequence x1:n with τ ∈ Tk,n is written as

C(xτκ−1+1:τκ ).

Lk,n(x, τ ) =

k+1
(cid:88)

κ=1

18

For a sub-sequence x1:n, the optimal loss and the optimal k-dimensional CP vector are respectively
written as

Lopt

k,n(x) = min
τ ∈Tk,n

Lk,n(x, τ )

T opt

k,n (x) = arg min
τ ∈Tk,n

Lk,n(x, τ ).

Conditional data space characterization. Since the inference is conducted conditional on all
steps of DP, the conditional data space X (cid:48) is written as

(cid:40)

X (cid:48) =

x ∈ RN |

K
(cid:92)

N
(cid:92)

k=1

n=k

k,n (x) = T opt
T opt

k,n (xobs), q(x) = q(xobs)

.

(14)

(cid:41)

For simplicity, we denote τ det

k,n = T opt

k,n (xobs), the conditional data space X (cid:48) can be re-written as

(cid:40)

X (cid:48) =

x ∈ RN |

K
(cid:92)

N
(cid:92)

k=1

n=k

T opt
k,n (x) = τ det

k,n , q(x) = q(xobs)

.

(15)

(cid:41)

From the second condition, the data is restricted to the line [26, 12]. Therefore, the remaining task is
to characterize the region in which x ∈ RN satisﬁes the ﬁrst condition.
For each value of k ∈ [K] and n ∈ [N ], T opt

k,n (x) = τ det

k,n if and only if

min
τ ∈Tk,n

Lk,n(x, τ ) = Lk,n(xobs, τ det
k,n )

⇔

Lopt
k,n(x) = Lk,n(xobs, τ det

k,n ).

Based on the recursive structure of DP, we have
(cid:110)

Lopt

k,n(x) =

min
m∈{k,...,n−1}

Lopt

k−1,m(x) + C(xm+1:n)

Combining (17) and (18), we have

Lopt
k−1,m(x) + C(xm+1:n) ≥ Lk,n(xobs, τ det

k,n ),

(cid:111)

.

(16)

(17)

(18)

(19)

for m ∈ {k, ..., n − 1}. Since the cost function is in the quadratic form, (19) can be easily written in
the form of x(cid:62)Ak,n,mx ≤ 0, where the matrix Ak,n,m ∈ RN ×N depends on k, n and m. It suggests
that the conditional data space in (14) can be ﬁnally characterized as

(cid:40)

X (cid:48) =

x ∈ RN |

K
(cid:92)

N
(cid:92)

n−1
(cid:92)

k=1

n=k

m=k

x(cid:62)Ak,n,mx ≤ 0, q(x) = q(xobs)

.

(cid:41)

Now that the conditional data space X (cid:48) is identiﬁed, we can easily compute the truncation region and
calculate p-value for each detected CP.

19

