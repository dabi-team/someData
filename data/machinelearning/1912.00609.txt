9
1
0
2
c
e
D
2

]
L
C
.
s
c
[

1
v
9
0
6
0
0
.
2
1
9
1
:
v
i
X
r
a

GANCoder: An Automatic Natural
Language-to-Programming Language Translation
Approach based on GAN

Yabing Zhu, Yanfeng Zhang, Huili Yang, and Fangjing Wang

Northeastern University, China

Abstract. We propose GANCoder, an automatic programming approach
based on Generative Adversarial Networks (GAN), which can generate
the same functional and logical programming language codes conditioned
on the given natural language utterances. The adversarial training be-
tween generator and discriminator helps generator learn distribution of
dataset and improve code generation quality. Our experimental results
show that GANCoder can achieve comparable accuracy with the state-
of-the-art methods and is more stable when programming languages.

Keywords: GAN · Semantic parsing · Automatic programming · NLP.

1

Introduction

With the development of deep learning and natural language processing (NLP),
translation tasks and techniques have been signiﬁcantly enhanced. The problem
of cross-language communication has been well solved. In this digital age, we are
no longer a passive receiver of information but also a producer and analyst of
data. We need to have data management, query, and analysis skills. Especially,
programming is an essential skill in the era of AI. However, it requires strong
professional knowledge and practical experience to learn programming languages
and write codes to process data eﬃciently. Although programming languages,
such as SQL and Python, are relatively simple, due to education and professional
limitations, it is still diﬃcult for many people to learn. How to lower the access
threshold of learning programming languages and make coding easier is worth
studying.

In this paper, we explore how to automatically generate programming codes
from natural language utterances. The inexperienced users only need to describe
what they want to implement in natural language, then the programming codes
with the same functionality can be generated via a generator [1], so that can
simply complete complex tasks, such as database management, programming,
and data processing.

Automatic programming is a diﬃcult task in the ﬁeld of artiﬁcial intelligence.
It is also a signiﬁcant symbol of strong artiﬁcial intelligence. Many researchers
have been studying how to convert natural language utterances into program
code for a long time. Before deep learning is applied, pattern matching was

 
 
 
 
 
 
2

Y.Zhu et al.

the most popular method. But due to the need for a large number of artiﬁcial
design templates and the diversity and fuzziness of natural language expressions,
matching-based methods are not ﬂexible and hard to meet the needs. With
the development of machine translation, some researchers try to use statistical
machine learning to solve the problem of automatic programming, but due to
the diﬀerence between the two language models, the results are not satisfactory.
In recent years, GANs have been proposed to deal with the problem of data
generation. The game training between GAN’s discriminator and generator make
the generator learn data distribution better. In this paper, we propose an au-
tomatic program generator GANCoder, a GAN-based encoder-decoder frame-
work which realizes the translation between natural language and programming
language. In the training phase, we adopt GAN to improve the accuracy of au-
tomatic programming generator [2]. The main contributions of this model are
summarized as follows. (1) Introducing GAN into automatic programming tasks,
the antagonistic game between GAN’s Generator and Discriminator can make
Generator learn better distribution characteristics of data; (2) Using Encoder-
Decoder framework to achieve the end-to-end conversion between two languages;
(3) Using grammatical information of programming language when generating
program codes, which provides prior knowledge and template for decoding, and
also solves the problem of inconsistency between natural language model and
programming language model. Our results show that GANCoder can achieve
comparable accuracy with the state-of-the-art methods and is more stable when
working on diﬀerent programming languages.

2 Related Work

2.1 Semantic Parsing and Code Generation

Semantic parsing is the task of converting a natural language utterance to a
logical form: a machine-understandable representation of its meaning, such as
ﬁrst-order logical representation, lambda calculus, semantic graph, and etc. Se-
mantic parsing can thus be understood as extracting the precise meaning of
an utterance [3]. Applications of semantic parsing include machine translation,
question answering and code generation. We focus on code generation in this
paper.

The early semantic analysis systems for code generation are rule-based and
limited to speciﬁc areas, such as the SAVVY system [4]. It relies on pattern
matching to extract words and sentences from natural language utterances ac-
cording to pre-deﬁned semantic rules. The LUNAR system [22] works based on
grammatical features, which converts the natural language into a grammar tree
with a self-deﬁned parser, and then transforms the grammar tree into an SQL
expression. Feature extraction by handcraft not only relies on a large amount of
manual work but also impacts the performance of such semantic analysis systems
which is relatively fragile. Because the pre-speciﬁed rules and semantic templates
cannot match the characteristics of natural language expression with ambiguity
and expression diversity, the early system functionalities are relatively simple.

GANCoder

3

Only simple semantic analysis tasks are supported. Later, researchers have pro-
posed WASP [17] and KRISP [18], combined with the grammar information of
the logical forms, using statistical machine learning and SVM (Support Vector
Machine) to convert natural utterances’ grammar tree to the grammar tree of
the logical forms. Chris Quirk et al. propose a translation approach from natural
language utterances to the If-this-then-that program using the KRISP algorithm
[5].

Encoder-Decoder frameworks based on RNNs (Recurrent Neural Networks)
have been introduced into the code generation tasks. These frameworks have
shown state-of-the-art performance in some ﬁelds, such as machine translation,
syntax parsing, and image caption generation. The use of neural networks can
reduce the need for custom lexical, templates, and manual features, and also do
not need to produce intermediate representations. Li Dong et al. use the Encoder-
Decoder model to study the code generation task and propose a general neural
network semantic parser [3]. Xi Victoria Lin et al. propose an encoder-decoder-
based model that converts natural language utterances into Linux shell scripts
[6]. As shown in ﬁg.1, as an end-to-end learning framework, encoder encodes
natural language utterances into intermediate semantic vectors, and decoder
decodes intermediate vectors into logical forms. Generally speaking, encoder and
decoder can be any neural networks, but LSTM and RNN are mostly used.

Although programming languages are sequential strings in form, they have
a hierarchical structure. A number of works utilize the hierarchical structure
property of programs to generate codes .For example, the selective clause and
the where clause in SQL belong to diﬀerent logical levels. Based on this observa-
tion, researchers propose tree-based LSTM and tree-based CNN [7,8]. Besides,
EgoCoder, a hierarchical neural network based on Python program’s AST (Ab-
stract Syntax Tree), achieves code auto-completion and code synthesis [9]. Yin
and Neubig propose an Encoder-Decoder model that uses syntax information as
the prior knowledge to help decoder reduce search space [10].

(cid:49)(cid:47)(cid:3)(cid:88)(cid:87)(cid:87)(cid:72)(cid:85)(cid:68)(cid:81)(cid:70)(cid:72)

(cid:36)(cid:79)(cid:79)(cid:3)(cid:83)(cid:85)(cid:76)(cid:80)(cid:72)(cid:86)(cid:3)(cid:74)(cid:85)(cid:72)(cid:68)(cid:87)(cid:72)(cid:85)(cid:3)
(cid:87)(cid:75)(cid:68)(cid:81)(cid:3)(cid:20)(cid:19)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:40)(cid:89)(cid:72)(cid:81)

(cid:72)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)

(cid:71)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)

(cid:86)(cid:82)(cid:73)(cid:87)(cid:16)(cid:68)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:51)(cid:92)(cid:87)(cid:75)(cid:82)(cid:81)(cid:3)(cid:70)(cid:82)(cid:71)(cid:72)

(cid:51)(cid:85)(cid:76)(cid:80)(cid:72)(cid:86)(cid:17)(cid:3)(cid:73)(cid:76)(cid:79)(cid:87)(cid:72)(cid:85)(cid:11)(cid:79)(cid:68)(cid:80)(cid:69)(cid:71)(cid:68)(cid:3)
(cid:91)(cid:29)(cid:91)(cid:33)(cid:20)(cid:19)(cid:3)(cid:9)(cid:9)(cid:3)(cid:91)(cid:8)(cid:21)(cid:3)(cid:32)(cid:32)(cid:20)(cid:12)

Fig. 1. Encoder-Decoder model for code generation

2.2 Generative Adversarial Network (GAN)

GAN [2], proposed by Ian Goodfellow in 2014, is a method of unsupervised
learning. GAN consists of a generator network and a discriminator network.
The generator produces what we want, and the discriminator judges whether the
output of the generator is fake or subject to the real distribution. Generator and
discriminator improve themselves and adjust parameters via their adversarial
training. Since GAN was proposed, it has attracted a lot of attention, and more

4

Y.Zhu et al.

GANs have been used in image generation, speech synthesis, etc. and achieved
much success. CDGAN [19], WGAN [20], VAEGAN [21] are typical models of
GANs. Since the object processed in NLP is discrete characters, the gradient of
discriminator cannot be passed to the generator, so the application of GAN in
NLP is not very successful. Lantao Yu et al. proposed SeqGAN [12] to optimize
the GAN network by using the strategy gradient in reinforcement learning to
improve the quality of text generation. This is also a successful attempt of GAN
in NLP tasks .

(cid:38)(cid:82)(cid:81)(cid:87)(cid:72)(cid:91)(cid:87)(cid:3)(cid:41)(cid:85)(cid:72)(cid:72)(cid:3)(cid:42)(cid:85)(cid:68)(cid:80)(cid:80)(cid:72)(cid:85)(cid:86)

(cid:3)(cid:3)(cid:3)(cid:36)(cid:69)(cid:86)(cid:87)(cid:85)(cid:68)(cid:70)(cid:87)(cid:3)(cid:3)(cid:54)(cid:92)(cid:81)(cid:87)(cid:68)(cid:91)(cid:3)(cid:55)(cid:85)(cid:72)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:51)(cid:92)(cid:87)(cid:75)(cid:82)(cid:81)

(cid:54)(cid:87)(cid:80)(cid:87)(cid:3)(cid:198)(cid:40)(cid:91)(cid:83)(cid:85)(cid:11)(cid:72)(cid:91)(cid:83)(cid:85)(cid:3)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:12)
(cid:40)(cid:91)(cid:83)(cid:85)(cid:3)(cid:198)
(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)
(cid:3)(cid:3)(cid:3)(cid:3)(cid:38)(cid:68)(cid:79)(cid:79)(cid:11)(cid:72)(cid:91)(cid:83)(cid:85)(cid:3)(cid:73)(cid:88)(cid:81)(cid:70)(cid:15)(cid:3)(cid:72)(cid:91)(cid:83)(cid:85)(cid:13)(cid:3)(cid:68)(cid:85)(cid:74)(cid:86)(cid:15)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)
(cid:78)(cid:72)(cid:92)(cid:90)(cid:82)(cid:85)(cid:71)(cid:13)(cid:3)(cid:78)(cid:72)(cid:92)(cid:90)(cid:82)(cid:85)(cid:71)(cid:86)(cid:12)(cid:3)

(cid:72)(cid:91)(cid:83)(cid:85)(cid:62)(cid:73)(cid:88)(cid:81)(cid:70)(cid:64)

(cid:87)(cid:23)

(cid:3)(cid:3)(cid:3)(cid:3)(cid:95)(cid:3)(cid:36)(cid:87)(cid:87)(cid:85)(cid:76)(cid:69)(cid:88)(cid:87)(cid:72)(cid:11)(cid:72)(cid:91)(cid:83)(cid:85)(cid:3)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:15)(cid:3)(cid:76)(cid:71)(cid:72)(cid:81)(cid:87)(cid:76)(cid:73)(cid:76)(cid:72)(cid:85)(cid:3)(cid:68)(cid:87)(cid:87)(cid:85)(cid:12)
(cid:3)(cid:3)(cid:3)(cid:3)(cid:95)(cid:3)(cid:49)(cid:68)(cid:80)(cid:72)(cid:11)(cid:76)(cid:71)(cid:72)(cid:81)(cid:87)(cid:76)(cid:73)(cid:76)(cid:72)(cid:85)(cid:3)(cid:76)(cid:71)(cid:12)
(cid:3)(cid:3)(cid:3)(cid:3)(cid:95)(cid:3)(cid:54)(cid:87)(cid:85)(cid:11)(cid:86)(cid:87)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:86)(cid:12)

(cid:87)(cid:24)

(cid:49)(cid:68)(cid:80)(cid:72)

(cid:87)(cid:25)

(cid:86)(cid:87)(cid:85)(cid:11)(cid:86)(cid:82)(cid:85)(cid:87)(cid:72)(cid:71)(cid:12)

(cid:87)(cid:19)

(cid:87)(cid:20)

(cid:87)(cid:21)

(cid:87)(cid:22)

(cid:85)(cid:82)(cid:82)(cid:87)

(cid:40)(cid:91)(cid:83)(cid:85)

(cid:72)(cid:91)(cid:83)(cid:85)(cid:62)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:64)

(cid:38)(cid:68)(cid:79)(cid:79)

(cid:87)(cid:26)

(cid:87)(cid:27)

(cid:87)(cid:28)

(cid:72)(cid:91)(cid:83)(cid:85)(cid:3)(cid:13)(cid:62)(cid:68)(cid:85)(cid:74)(cid:86)(cid:64)

(cid:78)(cid:72)(cid:92)(cid:90)(cid:82)(cid:85)(cid:71)(cid:3)(cid:13)(cid:62)(cid:78)(cid:72)(cid:92)(cid:90)(cid:82)(cid:85)(cid:71)(cid:86)(cid:64)

(cid:87)(cid:20)(cid:20)

(cid:72)(cid:91)(cid:83)(cid:85)

(cid:49)(cid:68)(cid:80)(cid:72)

(cid:87)(cid:20)(cid:21)

(cid:78)(cid:72)(cid:92)(cid:90)(cid:82)(cid:85)(cid:71)(cid:3)

(cid:87)(cid:20)(cid:22)

(cid:87)(cid:20)(cid:23)

(cid:86)(cid:87)(cid:85)(cid:11)(cid:85)(cid:72)(cid:89)(cid:72)(cid:85)(cid:86)(cid:72)(cid:12)

(cid:72)(cid:91)(cid:83)(cid:85)(cid:62)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:64)

(cid:87)(cid:20)(cid:19)

(cid:86)(cid:87)(cid:85)(cid:11)(cid:80)(cid:92)(cid:66)(cid:79)(cid:76)(cid:86)(cid:87)(cid:12)

(cid:49)(cid:68)(cid:80)(cid:72)

(cid:86)(cid:87)(cid:85)(cid:11)(cid:55)(cid:85)(cid:88)(cid:72)(cid:12)

(cid:87)(cid:20)(cid:24)

(cid:87)(cid:20)(cid:25)

Fig. 2. Python context free grammar (left) and abstract syntax tree structure (right)

3 Model

3.1 GAN-based semantic parsing

We aim to design an automatic programming model which can generate a pro-
gram code sequence Y = {y1, y2, · · · , ym} based on a natural language utterance
X = {x1, x2, · · · , xn}. We introduce GAN into the Encoder-Decoder framework,
and propose a new model GANCoder, as shown in ﬁg.3 . In GANCoder, the
generator Gθ uses an encoder-decoder framework, which converts the natural
language utterances into program codes, where θ represents parameters. The en-
coder encodes the natural language utterances as intermediate semantic vectors,
and the decoder decodes the semantic vectors into ASTs with the guidance of the
programming language grammar information. At last, we parse the ASTs into
program codes. The discriminator is responsible for judging whether the ASTs
generated by the generator are consistent with the natural language utterance
semantics. We use GAN to improve the generative model Gθ, the optimization
equation of the GAN network is as follows [13]:

minGθmaxD∅ L(θ, ∅) = EX∼px log D∅(X) + EY ∼Gθ log(1 − D∅(Y ))

(1)

GANCoder

5

Gθ = p(Y |X) =

|Y |

Y
t=1

p(yt|Y<t, X, GrammarCF G)

(2)

where Y<t = y1, y2, · · · , yt−1 represents the sequence of the ﬁrst t − 1 characters
of the program fragment, and GrammarCF G represents the CFG (Context-Free
Grammars) of the programming language, which provides guidance in the code
generation process. X ∼ px indicates that the data sample X is subject to the
distribution of real data, and Y ∼ Gθ means that the generator generates the
data sample Y . Generator and discriminator play two-palyer minimax game. Dis-
criminator can diﬀerentiates between the tow distributions. In practice, equation

(cid:53)(cid:72)(cid:68)(cid:79)(cid:3)(cid:36)(cid:54)(cid:55)

(cid:39)(cid:76)(cid:86)(cid:70)(cid:85)(cid:76)(cid:80)(cid:76)(cid:81)(cid:68)(cid:87)(cid:82)(cid:85)

(cid:49)(cid:47)(cid:3)(cid:88)(cid:87)(cid:87)(cid:72)(cid:85)(cid:68)(cid:81)(cid:70)(cid:72)

(cid:42)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:82)(cid:85)

(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)

(cid:41)(cid:68)(cid:78)(cid:72)(cid:3)(cid:36)(cid:54)(cid:55)

(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:16)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)

(cid:53)(cid:72)(cid:90)(cid:68)(cid:85)(cid:71)

(cid:55)(cid:85)(cid:72)(cid:72)(cid:3)(cid:47)(cid:54)(cid:55)(cid:48)

(cid:127)(cid:1009)

(cid:455)(cid:1009)

(cid:127)(cid:1007)
(cid:455)(cid:1007)

(cid:127)(cid:1008)
(cid:455)(cid:1008)

(cid:127)(cid:1005)

(cid:455)(cid:1005)

(cid:127)(cid:1006)

(cid:455)(cid:1006)

(cid:49)(cid:47)(cid:3)(cid:86)(cid:72)(cid:80)(cid:68)(cid:81)(cid:87)(cid:76)(cid:70)

(cid:54)(cid:82)(cid:73)(cid:87)(cid:80)(cid:68)(cid:91)

Fig. 3. GAN-based automatic program generator

1 may not provide suﬃcent gradient for generator when generating discrete data
in NLP. Inspired by the optimization strategy of strategy gradient proposed by
SeqGAN, combined with the characteristics of automatic programming tasks,
we optimize GANCoder as follows:

J (θ) = EY ∼Gθ log(Gθ(y1|s0)

T

Y
t=2

Gθ(yt|Y1:t−1))R(Y1:t)

R(Y1:t) = D∅(Y1:T )

(3)

(4)

where R(Y1:t) represents the generator reward function, which quantiﬁes the
quality of the generated program fragments. In the other words, it is the prob-
ability of semantic between the natural language utterances and the generated
program fragments.

3.2 CFG-based GAN generator

The main task of the GAN generator is to encode the semantics of natural
language utterances and then to decode the semantics into AST based on CFG
of the programming language. The conversion from one language to another

6

Y.Zhu et al.

uses the Encoder-Decoder model to reduce the interaction between diﬀerent
languages. The two ends are independently responsible for the processing of
their data, simplifying the complexity of the problem. This end-to-end learning
framework is more general, and both ends can select their own deep learning
models according to the characteristics of the data. Fig.4 shows the framework
diagram of the Generator.

(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)

(cid:38)(cid:82)(cid:81)(cid:87)(cid:72)(cid:91)(cid:87)(cid:3)(cid:41)(cid:85)(cid:72)(cid:72)(cid:3)(cid:42)(cid:85)(cid:68)(cid:80)(cid:80)(cid:72)(cid:85)

(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)

(cid:38)(cid:20)

(cid:38)(cid:21)

(cid:38)(cid:22)

(cid:51)(cid:92)(cid:87)(cid:75)(cid:82)(cid:81)(cid:3)(cid:36)(cid:54)(cid:55)

(cid:91)(cid:20)

(cid:91)(cid:21)

(cid:91)(cid:22)

(cid:49)(cid:47)(cid:3)(cid:88)(cid:87)(cid:87)(cid:72)(cid:85)(cid:68)(cid:81)(cid:70)(cid:72)

Fig. 4. Encoder-Decoder-based generator in GANCoder

The encoder is responsible for encoding the semantics of the natural language
utterances, as shown in Fig.4. We use a bidirectional LSTM to encode the text
−→
ht respectively represent the
sequence of natural language description.
hidden state of the t-th unit of the natural language description sequence from
−→
left to right and from right to left,let ht = [
ht]. be the intermediate hidden
vector of the character. The last character’s intermediate vector is the semantic
presentation of the whole natural utterance.

←−
ht,and

←−
ht :

The decoder decodes the intermediate semantic vector generated by Encoder.
Inspired by the model proposed in [10], we ﬁrst decode the intermediate seman-
tic vector into an abstract syntax tree based on the CFG of the programming
language. According to the characteristics of CFG and AST, we deﬁne two kinds
of actions, corresponding to the generation of non-leaf nodes and leaf nodes in
the abstract syntax tree in Fig.2 (right). Logically, we use LSTM to recursively
build AST top-down and left-right, as shown in Fig.2 (right). We convert the
task into predicting the grammatic action sequence. Based on the CFG, not only
can the template be generated for the decoding process, but also the prediction
range can be constrained, so that the search space can be reduced to improve
the calculation eﬃciency.

Unlike encoder, decoder uses a normal LSTM to maintain state variables to

form an AST,

st = fLST M ([at−1 : ct : pt], st−1)

paction = Sof tmax(st)

(5)

(6)

where ”[:]” represents the concatenation operation between multiple vectors, st
represents the state vector at time t in the decoder, The probability of diﬀerent
grammatical actions can be calculated using the Sof tmax(st) function. at−1
represents the vector of the previous action, and ct represents the state based

GANCoder

7

on the input hX = {hx1, hx2, · · · , hx|X|}. We use the soft-attention mechanism
to calculate the attention, as shown in Fig.4. Then the decoder predicts the
probability of each action by the state at time t. When a character is generated
during a predicted action, we use PointNet to copy the character from the natural
language description to AST [11]. In the process of constructing an AST, the
Beam Search algorithm is used to aviod over-ﬁtting. As shown in Fig.2 (right),
ti represents the step of decoding. The order in which nodes are generated is
also clearly marked in Fig.2 (right).

3.3 Tree-based Semantic GAN discriminator

The Generator can generate an AST of the program fragments, and the discrimi-
nator quantiﬁes the similarity of the semantic relationship between the generated
ASTs and the natural language utterances. How to quantify the semantic simi-
larity between two diﬀerent languages is very diﬃcult. In the discriminator, the
encoding of natural language utterances still uses the same encoder method in
the generator, which uses a bidirectional LSTM to encode the entire sequence
into intermediate semantic vectors. When encoding the semantics of a program,
there are two diﬀerent ways. The ﬁrst is to treat the program code sequence as
a string, and still use the same method as the generator processint it in a bidi-
rectional LSTM. The processing is simple, but the logic and syntax information
of the program cannot be captured. The second method is to use the structure
of the AST generated by the generator to encode the semantics of the program.
However, the semantics of the encoding program is somewhat diﬀerent from the
generation of the AST. In the generator, the structure of the AST is generated
recursively top-down and left-right, but in the discriminator, the entire AST is
encoded bottom-up from leaf node to root nodes of AST, and the ﬁnal vector is
used as the semantic vector of the program fragment. In this way, the syntax and
logic of the program fragment can be learned in a bottom-up manner [13,14].

Let hr be the ﬁnal encoding vector for the entire abstract syntax tree. Then
the hr and the semantic vector hN L of the natural language description are
classiﬁed into two categories:

out = hrW dishN L + bdis

Psim = sof tmax(out) =

esim
Pe∈out ei

(7)

(8)

where Psim ∈ [0, 1] represents the probability that the AST is consistent with
the semantics of the natural language description. Since the AST is generated
based on the CFG of the programming language, the AST is grammatically
standardized. The semantics of the generated program fragments need to be
consistent with the natural language semantics.

4 Experiments

The experiment and evaluation are carried out in a single machine environment.
The speciﬁc hardware conﬁguration is: processor Intel-i7-8700, memory 32GB,

8

Y.Zhu et al.

NVIDIA GTX1080 graphics card, memory 8GB. The software environment is:
Ubuntu16.04, Python2.7, pytorch3, cuda9. Natural language, Python program
characters, and context-free grammar characters embedding are initialized by the
xavier uniform method [23]. The optimization function of the model is Adam.

4.1 Datasets

1. Django is a web framework for Python, where each line of code is manually
labeled with the corresponding natural language description text. In this
paper, there are 16,000 training data sets and 1805 veriﬁcation data sets.
2. Atis is the ﬂight booking system dataset, where the natural language utter-
ances are the user booking inquiries, and the codes are expressed in the form
of λ calculus. There are 4434 training data sets and 140 test data sets.
3. Jobs is a job query dataset, where the user queries are natural language
utterance, and the program codes are expressed in the form of Prolog. There
are 500 training data sets and 140 test data sets.

4.2 Experimental results and analysis

If the sequence of the generated program is the same as the program sequence of
the training data, it means that the generated data is correct, and the correctness
of the test set indicates the generation eﬀect of the model. As can be seen from
Table 1, regarding the Django and Jobs datasets, the pre-trained GANCoder
model improves 2.4% and 0.72% over the normal generator, respectively, and
improves 2.6% and 2.93% over that without pre-training. This demonstrates
that when training GANCoder, the pre-training has dramatically improved the
model. On the ATIS training set, the normal generator is the best. In this model,
GAN crashes, which is related to the training data and the grammar rule details
of diﬀerent logical forms. The natural language description sequence of the Jobs
data set is relatively simple, so the accuracy of the model is high. The Python
language of the Django dataset has relatively good syntax information, but the
logic of the program is also more diﬃcult. The game training of GAN also has a
good eﬀect. The ATIS dataset is logically diﬃcult, but the grammar information
is simple, which cannot provide more details when generating ASTs.

Table 2 compares the state-of-the-art code generation models with our GAN-
Coder model presented in this paper. Compared to the traditional Encoder-
Decoder models, such as SEQ2SEQ and SEQ2TREE, GANCoder increases the
accuracy by 24.6% and 30.3% on the Django dataset. These two models work
better on the Jobs dataset, but the results on the other two datasets are not sat-
isfactory. The ASN model achieves the best performance on the ATIS dataset
but cannot obtain results on the other two datasets. The LPN+COPY model
and the SNM+COPY model show good results on the Django dataset, but they
also do not have results on the other two datasets. Although the GANCoder
proposed in this paper is not always the best compared with the other mod-
els, GANCoder can achieve relatively stable and satisfactory results on various
datasets and is a promising code generation method worth improving.

Table 1. Model accuracy based of three training methods

GANCoder

9

Dataset Generator normal GAN without pretraining GAN with pretraining
Django
Jobs
ATIS

69.7
86.43
79.23

67.3
85.71
82.6

67.1
83.5
81.5

Table 2. Accuracy comparison of diﬀerent models

Models
SEQ2SEQ[3]
SEQ2TREE[3]
ASN[15]
LPN+COPY[16]
SNM+COPY[10]
GANCoder(Our model)

ATIS
84.2
84.6
85.3
-
-
81.5

Django
45.1
39.4
-
62.3
72.1
69.7

Jobs
87.1
90.0
-
-
-
86.43

5 Conclusion

This paper proposes a semantic programming-based automatic programming
method GANCoder. Through the game confrontation training of GAN generator
and discriminator, it can eﬀectively learn the distribution characteristics of data
and improve the quality of code generation. The experimental results show that
the proposed GANCoder can achieve comparable accuracy with the state-of-the
art code generation model, and the stability is better. The method proposed in
this paper can only realize the conversion between single-line natural language
description and single-line code. Future work will study how to convert long
natural language description text and multi-line code.

Acknowledgements

This work was partially supported by National Key R&D Program of China
(2018YFB1003404), National Natural Science Foundation of China (61672141),
and Fundamental Research Funds for the Central Universities (N181605017).

References

1. Aishwarya Kamath, Rajarshi Das.:A Survey on Semantic Parsing, 2018, CoRR

abs/1812.00978

2. I. Goodfellow, J. Pouget-Abadie, M. Mirza, et al.: Generative adversarial nets. Proc.
28th Advances in Neural Information Processing Systems (NIPS’14), 2014, pp. 2672-
2680

3. Li Dong, Mirella Lapata. Language to logical form with neural attention. In Proceed-
ings of the 54th Annual Meeting of the Association for Computational Linguistics,
2016, pages 33?43, Berlin, Germany

10

Y.Zhu et al.

4. William A Woods.:Progress in natural language understanding: an application to
lunar geology. In proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics of the June 4-8,1973, national computer conference and
exposition, ACM, 1973, pages 441-450.

5. Chris Quirk, Raymond J. Mooney, Michel Galley.: Language to Code: Learning

Semantic Parsers for If-This-Then-That Recipes. ACL (1) 2015: 878-888

6. Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, et al.: NL2Bash: A Corpus
and Semantic Parser for Natural Language Interface to the Linux Operating System.
LREC 2018

7. Kai Sheng Tai, Richard Socher, Christopher D. Manning.: Improved Semantic Rep-
resentations From Tree-Structured Long Short-Term Memory Networks. ACL (1)
2015: 1556-1566

8. Lili Mou, Ge Li, Lu Zhang, et al.: Convolutional Neural Networks over Tree Struc-

tures for Programming Language Processing. AAAI 2016: 1287-1293

9. Jiawei Zhang, Limeng Cui, Fisher B.: Gouza.EgoCoder: Intelligent Program Synthe-
sis with Hierarchical Sequential Neural Network Model,2018, CoRRabs/1805.08747
10. Pengcheng Yin, Graham Neubig.: A Syntactic Neural Model for General-Purpose

Code Generation. ACL (1) 2017: 440-450

11. Oriol Vinyals, Meire Fortunato, Navdeep Jaitly.: Pointer Networks. NIPS 2015:

2692-2700

12. Lantao Yu, Weinan Zhang, Jun Wang, et al.: SeqGAN: Sequence Generative Ad-

versarial Nets with Policy Gradient. AAAI 2017: 2852-2858

13. Xinyue Liu, Xiangnan Kong, Lei Liu, et al.: TreeGAN: Syntax-Aware Sequence

Generation with Generative Adversarial Networks. ICDM 2018: 1140-1145

14. Liu Chen, Guangping Zeng, Qingchuan Zhang, et al.: Tree-LSTM Guided Attention

Pooling of DCNN for Semantic Sentence Modeling. 5GWN 2017: 52-59

15. Maxim Rabinovich, Mitchell Stern, Dan Klein.: Abstract syntax networks for code
generation and semantic parsing. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics, 2017, pages 1139-1149, Vancouver,
Canada

16. Wang Ling, Phil Blunsom, Edward Grefenstette, et al.: Latent predictor networks
for code generation. In Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics, 2016, pages 599-609, Berlin, Germany

17. Yuk Wah Wong, Raymond J. Mooney.: Learning for Semantic Parsing with Sta-

tistical Machine Translation. HLT-NAACL 2006

18. Rohit J. Kate, Raymond J. Mooney.: Using String-Kernels for Learning Semantic

Parsers. ACL 2006

19. Alec Radford, Luke Metz, Soumith Chintala.: Unsupervised Representation Learn-
ing with Deep Convolutional Generative Adversarial Networks. ICLR (Poster) 2016
20. Martn Arjovsky, Soumith Chintala, Lon Bottou.: Wasserstein Generative Adver-

sarial Networks. ICML 2017: 214-223

21. Anders Boesen Lindbo Larsen, Sren Kaae Snderby, Hugo Larochelle, Ole Winther.:
Autoencoding beyond pixels using a learned similarity metric. ICML 2016: 1558-
1566

22. William A Woods.: Progress in natural language understanding: an application to
lunar geology. In Proceedings of the June 4-8, 1973, national computer conference
and exposition, pages 441450. ACM, 1973.

23. Xavier Pennec, Nicholas Ayache: Uniform Distribution, Distance and Expectation
Problems for Geometric Features Processing. Journal of Mathematical Imaging and
Vision 9(1): 49-67 (1998)

