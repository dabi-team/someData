2
2
0
2

b
e
F
5

]

G
L
.
s
c
[

3
v
7
1
3
7
0
.
0
1
1
2
:
v
i
X
r
a

ReGVD: Revisiting Graph Neural Networks for Vulnerability
Detection

Van-Anh Nguyen∗
VNU - University of Science, Vietnam
vananhnt57@gmail.com

Dai Quoc Nguyen∗†
Oracle Labs, Australia
dai.nguyen@oracle.com

Van Nguyen
Monash University, Australia
van.nk@monash.edu

Trung Le
Monash University, Australia
trunglm@monash.edu

Quan Hung Tran
Adobe Research, San Jose, CA, USA
qtran@adobe.com

Dinh Phung
Monash University, Australia
dinh.phung@monash.edu

ABSTRACT
Identifying vulnerabilities in the source code is essential to protect
the software systems from cyber security attacks. It, however, is
also a challenging step that requires specialized expertise in se-
curity and code representation. To this end, we aim to develop a
general, practical, and programming language-independent model
capable of running on various source codes and libraries without
difficulty. Therefore, we consider vulnerability detection as an in-
ductive text classification problem and propose ReGVD, a simple
yet effective graph neural network-based model for the problem. In
particular, ReGVD views each raw source code as a flat sequence
of tokens to build a graph, wherein node features are initialized
by only the token embedding layer of a pre-trained programming
language (PL) model. ReGVD then leverages residual connection
among GNN layers and examines a mixture of graph-level sum
and max poolings to return a graph embedding for the source code.
ReGVD outperforms the existing state-of-the-art models and ob-
tains the highest accuracy on the real-world benchmark dataset
from CodeXGLUE for vulnerability detection. Our code is available
at: https://github.com/daiquocnguyen/GNN-ReGVD.

CCS CONCEPTS
• Security and privacy → Systems security; • Computing method-
ologies → Natural language processing; Neural networks.

KEYWORDS
Graph Neural Networks, Vulnerability Detection, Security, Text
Classification

ACM Reference Format:
Van-Anh Nguyen, Dai Quoc Nguyen, Van Nguyen, Trung Le, Quan Hung
Tran, and Dinh Phung. 2022. ReGVD: Revisiting Graph Neural Networks

∗The first two authors contributed equally to this work.
†Corresponding author. This work was done before Dai Quoc Nguyen joined Oracle
Labs, Australia.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICSE ’22 Companion, May 21–29, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9223-5/22/05. . . $15.00
https://doi.org/10.1145/3510454.3516865

for Vulnerability Detection. In 44th International Conference on Software
Engineering Companion (ICSE ’22 Companion), May 21–29, 2022, Pittsburgh,
PA, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3510454.
3516865

1 INTRODUCTION
The software vulnerability problems have rapidly grown recently, ei-
ther reported through publicly disclosed information-security flaws
and exposures (CVE) or exposed inside privately-owned source
codes and open-source libraries. These vulnerabilities are the main
reasons for cyber security attacks on the software systems that
cause substantial damages economically and socially [18, 30]. There-
fore, vulnerability detection is an essential yet challenging step to
identify vulnerabilities in the source codes to provide security solu-
tions for the software systems.

Early approaches [18, 22, 25] have been proposed to carefully
design hand-engineered features for machine learning algorithms
to detect vulnerabilities. These early approaches, however, suffer
from two major drawbacks. First, creating good features requires
prior knowledge, hence needs domain experts, and is usually time-
consuming. Second, hand-engineered features are impractical and
not straightforward to adapt to all vulnerabilities in numerous
open-source codes and libraries evolving over time.

To reduce human efforts on feature engineering, some approaches
[15, 23] consider each raw source code as a flat natural language
sequence and explore deep learning architectures applied for natu-
ral language processing (NLP) (such as LSTMs [9] and CNNs [11])
in detecting vulnerabilities. Recently, pre-trained language models
such as BERT [4] have emerged as a trending learning paradigm,
achieving significant success in NLP applications. Inspired by this
BERT-style trending paradigm, pre-trained programming language
(PL) models such as CodeBERT [5] have improved the performance
of PL downstream tasks such as vulnerability detection. However,
as mentioned in [20], all interactions among all positions in the
input sequence inside the self-attention layer of the BERT-style
model build up a complete graph, i.e., every position has an edge to
all other positions; thus, this limits learning local structures within
the source code to differentiate vulnerabilities.

Graph neural networks (GNNs) have recently become a primary
method to embed nodes and graphs into low-dimensional contin-
uous vector spaces [7, 19, 26]. GNNs provide faster and practical
training, higher accuracy, and state-of-the-art results for down-
stream tasks such as text classification [10, 21, 28, 29]. Devign [30]

 
 
 
 
 
 
ICSE ’22 Companion, May 21–29, 2022, Pittsburgh, PA, USA

Van-Anh Nguyen, Dai Quoc Nguyen, Van Nguyen, Trung Le, Quan Hung Tran, and Dinh Phung

Figure 1: An illustration for two graph construction methods with a fixed-size sliding window of length 3.

is proposed to utilize Gated GNNs [14] for vulnerability detection,
wherein Devign uses a PL parser to extract multi-edged graph in-
formation. However, Devign is difficult of being practiced in reality.
The main reason is that there is not a perfect parser in reality for
each PL, which can successfully parse a variety of source codes and
libraries without any internal compile errors and exceptions.

In this paper, our goal is to develop a general, practical, and
programming language-independent model capable of running on
various source codes and libraries without difficulty. Hence, we
consider vulnerability detection as an inductive text classification
problem and introduce ReGVD – a simple yet effective GNN-based
model for vulnerability detection as follows: (i) ReGVD views each
raw source code as a flat sequence of tokens to construct a graph (in
Section 2.2), wherein node features are initialized by only the token
embedding layer of a pre-trained PL model. (ii) ReGVD leverages
GNNs (such as GCNs [13] or Gated GNNs [14]) using residual con-
nection among GNN layers (in Section 2.3). (iii) ReGVD examines
a mixture between the sum and max poolings to produce a graph
embedding for the source code (in Section 2.4). This graph embed-
ding is fed to a single fully-connected layer followed by a softmax
layer to predict the code vulnerabilities. Extensive experiments
show that ReGVD significantly outperforms the existing state-of-
the-art models on the benchmark vulnerability detection dataset
from CodeXGLUE [17]. ReGVD produces the highest accuracy of
63.69%, gaining absolute improvements of 1.61% and 1.39% over
CodeBERT and GraphCodeBERT, respectively; thus, ReGVD can
act as a new strong baseline for future work.

2 THE PROPOSED REGVD
2.1 Problem definition
We consider vulnerability detection for source code at the func-
tion level, i.e., we aim to identify whether a given function in raw
source code is vulnerable or not [30]. We define a data sample
as {(c𝑖, y𝑖 )|c𝑖 ∈ C, y𝑖 ∈ Y}𝑛
𝑖=1, where C represents the set of raw
source codes, Y = {0, 1} denotes the label set with 1 for vulnerable
and 0 otherwise, and 𝑛 is the number of instances. In this work, we

consider vulnerability detection as an inductive text classification
problem and leverage GNNs for the problem. Therefore, we con-
struct a graph g𝑖 (V, 𝑿, 𝑨) ∈ G for each source code c𝑖 , wherein V
is a set of 𝑚 nodes in the graph; 𝑿 ∈ R𝑚×𝑑 is the node feature ma-
trix, wherein each node v𝑗 ∈ V is represented by a 𝑑-dimensional
real-valued vector 𝒙 𝑗 ∈ R𝑑 ; 𝑨 ∈ {0, 1}𝑚×𝑚 is the adjacency ma-
trix, where 𝑨v,u equal to 1 means having an edge between node v
and node u, and 0 otherwise. We aim to learn a mapping function
𝑓 : G → Y to determine whether a given source code is vulnerable
or not. The mapping function 𝑓 can be learned by minimizing the
loss function with the regularization on model parameters 𝜽 as:

min

𝑛
∑︁

𝑖=1

L (𝑓 (g𝑖 (V, 𝑿, 𝑨), y𝑖 |c𝑖 )) + 𝜆∥𝜽 ∥2
2

where L (.) is the cross-entropy loss function and and 𝜆 is an ad-
justable weight.

2.2 Graph construction
We consider a raw source code as a flat sequence of tokens and
illustrate two graph construction methods [10, 29] in Figure 1,
wherein we omit self-loops in these two methods since the self-
loops do not help to improve performance in our pilot experiments.1

Unique token-focused construction. We represent unique tokens
as nodes and co-occurrences between tokens (within a fixed-size
sliding window) as edges, and the obtained graph has an adjacency
matrix 𝑨 as:




1 If v and u co-occur within a sliding window

0 Otherwise.

and v ≠ u.

𝑨v,u =

Index-focused construction. Given a flat sequence of 𝑙 tokens
{𝑡i}𝑙
i=1, we represent all tokens as the nodes, i.e., treating each index
i as a node to represent token 𝑡i. The number of nodes equals the

1In our implementation, we firstly tokenize the source code using the corresponding
tokenizer of the pre-trained PL model, and then we construct the graph from the
tokenized sequence.

ReGVD: Revisiting Graph Neural Networks for Vulnerability Detection

ICSE ’22 Companion, May 21–29, 2022, Pittsburgh, PA, USA

sequence length. We also consider co-occurrences between indexes
(within a fixed-size sliding window) as edges, and the obtained
graph has an adjacency matrix 𝑨 as:

1 If i and j co-occur within a sliding window

and i ≠ j.
0 Otherwise.

𝑨i,j =





Node feature initialization. It is worth noting that pre-trained
programming language (PL) models such as CodeBERT [5] have
recently improved the performance of PL downstream tasks such as
vulnerability detection. To attain the advantage of the pre-trained
PL model and also to make a fair comparison, we use only the token
embedding layer of the pre-trained PL model to initialize node
feature vectors for reporting our final results.

2.3 Graph neural networks with residual

connection

GNNs aim to update vector representations of nodes by recursively
aggregating vector representations from their neighbours [13, 24].
Mathematically, given a graph g(V, 𝑿, 𝑨), we simply formulate
GNNs as follows:

H(𝑘+1) = GNN

(cid:16)

𝑨, H(𝑘) (cid:17)

where H(𝑘) is the matrix representation of nodes at the 𝑘-th itera-
tion/layer; and H(0) = 𝑿 . There have been many GNNs proposed
in recent literature [26], wherein Graph Convolutional Networks
(GCNs) [13] is the most widely-used one, and Gated graph neural
networks (“Gated GNNs” or “GGNNs” for short) [14] is also suitable
for our data structure. Our ReGVD leverages GCNs and GGNNs as
the base models.

Formally, GCNs is given as follows:

(𝑘+1)
v

h

= 𝜙 (cid:169)
(cid:173)
(cid:171)

𝑎v,u𝑾 (𝑘) h

∑︁

u∈Nv

(𝑘)
u (cid:170)
(cid:174)
(cid:172)

, ∀v ∈ V

where 𝑎v,u is an edge constant between nodes v and u in the Lapla-
cian re-normalized adjacency matrix D− 1
2 (as we omit self-
loops), wherein D is the diagonal node degree matrix of 𝑨; 𝑾 (𝑘) is
a weight matrix; and 𝜙 is a nonlinear activation function such as
ReLU.

2 𝑨D− 1

GGNNs adopts GRUs [2], unrolls the recurrence for a fixed num-
ber of timesteps, and removes the need to constrain parameters to
ensure convergence as:

(𝑘+1)
v

a

=

∑︁

𝑎v,uh

(𝑘)
u

(𝑘+1)
v

z

(𝑘+1)
r
v

(𝑘+1)
(cid:158)h
v

(𝑘+1)
h
v

u∈Nv
(cid:16)

= 𝜎

𝑧
W

(𝑘+1)
v

a

𝑧
+ U

(𝑘)
h
v

= 𝜎

(cid:16)

𝑟
W

(𝑘+1)
v

a

𝑟
+ U

(𝑘)
h
v

(cid:17)

(cid:17)

= 𝜙

=

(cid:16)

𝑜
W
(cid:16)1 − z

(𝑘+1)
v

a

𝑜 (cid:16)

+ U

(𝑘+1)
v

r

⊙ h

(𝑘)
v

(cid:17)(cid:17)

(𝑘+1)
v

(cid:17)

⊙ h

(𝑘)
v + z

(𝑘+1)
v

(𝑘+1)
⊙ (cid:158)h
v

The residual connection [8] is used to incorporate information
learned in the lower layers to the higher layers, and more impor-
tantly, to allow gradients to directly pass through the layers to avoid
vanishing gradient or exploding gradient problems. Motivated by
that, we follow [1] to adapt residual connection among the GNN
layers, with fixing the same hidden size for the different layers. In
particular, ReGVD redefines GNNs as:

H(𝑘+1) = H(𝑘) + GNN

(cid:16)

𝑨, H(𝑘) (cid:17)

Figure 2: An illustration for our proposed ReGVD.

2.4 Graph-level readout pooling layer
The graph-level readout layer is used to produce a graph embed-
ding for each input graph. ReGVD leverages the sum pooling as
it produces better results for graph classification [27].2 Besides,
ReGVD utilizes the max pooling to exploit more information on the
node representations. ReGVD then considers a mixture between
the sum and max poolings to produce the graph embedding eg as:

ev = 𝜎

(cid:16)

(𝐾)
v

(cid:17)

+ b

⊙ 𝜙

(cid:16)

Wh

(𝐾)
v

+ b

(cid:17)

(cid:33)

wTh
(cid:32)

∑︁

eg = MIX

ev, MaxPool {ev}v∈V

v∈V

(cid:16)

wTh

where ev is the final vector representation of node v, wherein
(cid:17) acts as soft attention mechanisms over nodes [14],
𝜎

(𝐾)
+ b
v
(𝐾)
and h
is the vector representation of node v at the last 𝐾-th
v
layer; and MIX(.) denotes an arbitrary function. ReGVD examines
three MIX functions consisting of SUM, MUL, and CONCAT as:

SUM :

eg =

MUL

CONCAT

:

:

eg =

eg =

∑︁

v∈V
∑︁

ev + MaxPool {ev}v∈V

ev ⊙ MaxPool {ev}v∈V

v∈V
(cid:34)

∑︁

v∈V

ev

∥ MaxPool {ev}v∈V

(cid:35)

After that, ReGVD feeds eg to a single fully-connected layer fol-
lowed by a softmax layer to predict whether the source code is
vulnerable or not as: ^yg = softmax (cid:0)W1eg + b1(cid:1) Finally, ReGVD is
trained by minimizing the cross-entropy loss function as mentioned
in Section 2.1. We illustrate the proposed ReGVD in Figure 2.

where z and r are the update and reset gates; 𝜎 is the sigmoid
function; and ⊙ is the element-wise multiplication.

2In our pilot studies, using the sum pooling (cid:205)
than using the mean pooling 1
|V |

(cid:205)

v∈V ev employed in [29].

v∈V ev also provides higher accuracies

ICSE ’22 Companion, May 21–29, 2022, Pittsburgh, PA, USA

Van-Anh Nguyen, Dai Quoc Nguyen, Van Nguyen, Trung Le, Quan Hung Tran, and Dinh Phung

3 EXPERIMENTAL SETUP AND RESULTS
3.1 Experimental setup

Dataset. We use the real-world benchmark from CodeXGLUE
[17] for vulnerability detection at the function level.3 The dataset
was firstly created by Zhou et al. [30], including 27,318 manually-
labeled vulnerable or non-vulnerable functions extracted from
security-related commits in two large and popular C programming
language open-source projects (i.e., QEMU and FFmpeg) and diver-
sified in functionality. Then Lu et al. [17] combined these projects
and then split into the training/validation/test sets.

Training protocol. We construct a 2-layer model, set the batch
size to 128, and employ the Adam optimizer [12] to train our model
up to 100 epochs. As mentioned in Section 2.3, we set the same
hidden size (“hs”) for the hidden GNN layers, wherein we vary
the size value in {128, 256, 384}. We vary the sliding window size
(“ws”) in {2, 3, 4, 5} and the Adam initial learning rate (“lr”) in
(cid:8)1𝑒−4, 5𝑒−4, 1𝑒−3(cid:9). The final accuracy on the test set is reported for
the best model checkpoint, which obtains the highest accuracy on
the validation set.

Baselines. We compare our ReGVD with strong and up-to-date

baselines as follows:

• BiLSTM [9] and TextCNN [11] are two well-known stan-

dard models applied for text classification.

• RoBERTa [16] is built based on BERT [4] by removing the
next-sentence objective and training on a massive dataset
with larger mini-batches and learning rates.

• Devign [30] builds a multi-edged graph from a raw source
code, then uses Gated GNNs [14] to update node representa-
tions, and finally utilizes a 1-D CNN-based pooling (“Conv”)
to make a prediction. We note that Zhou et al. [30] did not
release the official implementation of Devign. Thus, we re-
implement Devign using the same training and evaluation
protocols.

• CodeBERT [5] is a pre-trained model also based on BERT
for 6 programming languages (Python, Java, JavaScript, PHP,
Ruby, Go), using masked language model [4] and replaced
token detection [3] objectives.

• GraphCodeBERT [6] is a new pre-trained PL model, ex-
tending CodeBERT to consider the inherent structure of code
data flow into the training objective.

3.2 Main results
Table 1 presents the accuracy results of the proposed ReGVD and
the strong and up-to-date baselines on the real-world benchmark
dataset from CodeXGLUE for vulnerability detection. We note that
both the recent models CodeBERT and GraphCodeBERT obtain
competitive performances and perform better than Devign, indicat-
ing the effectiveness of the pre-trained PL models. More importantly,
ReGVD gains absolute improvements of 1.61% and 1.39% over Code-
BERT and GraphCodeBERT, respectively. This shows the benefit of
ReGVD in learning the local structures inside the source code to
differentiate vulnerabilities (w.r.t using only the token embedding
layer of the pre-trained PL model). Hence, our ReGVD significantly

3https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Defect-detection

Table 1: Vulnerability detection accuracies (%) on the test set.
The best scores are in bold, while the second best scores are
in underline. The results of BiLSTM, TextCNN, RoBERTa,
and CodeBERT are taken from [17]. ★ denotes that we report
our own results for other baselines. “Idx” and “UniT” denote
the index-focused graph construction and the unique token-
focused one, respectively. “CB” and “G-CB” denote using
only the token embedding layer of CodeBERT and Graph-
CodeBERT to initialize the node features, respectively.

Model
BiLSTM
TextCNN
RoBERTa
CodeBERT
GraphCodeBERT★
Devign (Idx + CB)★
Devign (Idx + G-CB)★
Devign (UniT + CB)★
Devign (UniT + G-CB)★
ReGVD (GGNN + Idx + CB)
ReGVD (GGNN + Idx + G-CB)
ReGVD (GGNN + UniT + CB)
ReGVD (GGNN + UniT + G-CB)
ReGVD (GCN + Idx + CB)
ReGVD (GCN + Idx + G-CB)
ReGVD (GCN + UniT + CB)
ReGVD (GCN + UniT + G-CB)

Accuracy
59.37
60.69
61.05
62.08
62.30
60.43
61.31
60.40
59.77
63.54
63.29
63.62
62.41
62.63
62.70
63.14

63.69

outperforms the up-to-date baseline models. In particular, ReGVD
produces the highest accuracy of 63.69% – a new state-of-the-art
result on the CodeXGLUE vulnerability detection dataset.

We look at Figure 3a to investigate whether the graph-level
readout layer proposed in ReGVD performs better than the Conv
pooling layer utilized in Devign. Since Devign also uses Gated GNNs
to update the node representations and gains the best accuracy of
61.31% for the setting (Idx+G-CB); thus, we consider the ReGVD
setting (GGNN+Idx +G-CB) without using the residual connection
for a fair comparison, wherein ReGVD achieves an accuracy of
63.51%, which is 2.20% higher accuracy than that of Devign. More
generally, we get a similar conclusion from the results of three
remaining ReGVD settings (without using the residual connection)
that the graph-level readout layer utilized in ReGVD outperforms
that used in Devign.

We analyze the influence of the residual connection and the
mixture function. We first look back Figure 3a for the ReGVD accu-
racies w.r.t with and without using the residual connection among
the GNN layers. It demonstrates that the residual connection helps
to boost the GNNs performance on seven settings, where the maxi-
mum accuracy gain is 2.05% for the ReGVD setting (GCN+Idx+G-
CB). Next, we look at Figure 3b for the ReGVD results w.r.t the
MIX functions. We find that ReGVD generally gains the highest
accuracies on six settings using the MUL operator and on two re-
maining settings using the SUM operator. But it is worth noting

ReGVD: Revisiting Graph Neural Networks for Vulnerability Detection

ICSE ’22 Companion, May 21–29, 2022, Pittsburgh, PA, USA

Representations using RNN Encoder–Decoder for Statistical Machine Translation.
In EMNLP. 1724—-1734.

[3] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020.
Electra: Pre-training text encoders as discriminators rather than generators. arXiv
preprint arXiv:2003.10555 (2020).

[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).

[5] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In Findings of the
Association for Computational Linguistics: EMNLP 2020. 1536–1547.

[6] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun
Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
and Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with
Data Flow. In ICLR.

[7] William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Representation learning

on graphs: Methods and applications. preprint arXiv:1709.05584 (2017).

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual

learning for image recognition. In CVPR. 770–778.

[9] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory.

Neural Computation 9 (1997), 1735–1780.

[10] Lianzhe Huang, Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang. 2019.
Text Level Graph Neural Network for Text Classification. In EMNLP-IJCNLP.
[11] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In

EMNLP. 1746–1751.

[12] Diederik Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimiza-

tion. arXiv preprint arXiv:1412.6980 (2014).

[13] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with

Graph Convolutional Networks. In ICLR.

[14] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2016. Gated

Graph Sequence Neural Networks. In ICLR.

[15] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun
Deng, and Yuyi Zhong. 2018. Vuldeepecker: A deep learning-based system for
vulnerability detection. arXiv preprint arXiv:1801.01681 (2018).

[16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).

[17] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio
Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong
Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan
Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021.
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding
and Generation. arXiv preprint arXiv:2102.04664 (2021).

[18] Stephan Neuhaus, Thomas Zimmermann, Christian Holler, and Andreas Zeller.
2007. Predicting vulnerable software components. In ACM CCS. 529–540.
[19] Dai Quoc Nguyen. 2021. Representation Learning for Graph-Structured Data. Ph.D.

Dissertation. Monash University. https://doi.org/10.26180/14450496.v1

[20] Dai Quoc Nguyen, Tu Dinh Nguyen, and Dinh Phung. 2019. Universal Graph

Transformer Self-Attention Networks. arXiv preprint arXiv:1909.11855 (2019).

[21] Dai Quoc Nguyen, Tu Dinh Nguyen, and Dinh Phung. 2021. Quaternion Graph

Neural Networks. In Asian Conference on Machine Learning.

[22] Viet Hung Nguyen and Le Minh Sang Tran. 2010. Predicting vulnerable software

components with dependency graphs. In MetriSec. 1–8.

[23] Rebecca Russell, Louis Kim, Lei Hamilton, Tomo Lazovich, Jacob Harer, Onur
Ozdemir, Paul Ellingwood, and Marc McConley. 2018. Automated vulnerability
detection in source code using deep representation learning. In ICMLA.
[24] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele
Monfardini. 2009. The graph neural network model. IEEE Transactions on Neural
Networks 20 (2009), 61–80.

[25] Yonghee Shin, Andrew Meneely, Laurie Williams, and Jason A Osborne. 2010.
Evaluating complexity, code churn, and developer activity metrics as indicators
of software vulnerabilities. IEEE transactions on software engineering 37 (2010).
[26] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang,
and Philip S Yu. 2019. A comprehensive survey on graph neural networks.
arXiv:1901.00596.

[27] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful

Are Graph Neural Networks?. In ICLR.

[28] Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Graph convolutional networks

for text classification. In AAAI. 7370–7377.

[29] Yufeng Zhang, Xueli Yu, Zeyu Cui, Shu Wu, Zhongzhen Wen, and Liang Wang.
2020. Every Document Owns Its Structure: Inductive Text Classification via
Graph Neural Networks. In ACL. 334–339.

[30] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. De-
vign: Effective Vulnerability Identification by Learning Comprehensive Program
Semantics via Graph Neural Networks. In NeurIPS.

(a) Accuracy with and without
residual connection.

(b) Accuracy w.r.t the MIX func-
tions.

Figure 3: Accuracy with different settings.

that the ReGVD setting (GGNN+Idx+CB) using the CONCAT oper-
ator obtains an accuracy of 62.59%, which is still higher than that
of Devign, CodeBERT, and GraphCodeBERT.

Furthermore, our model achieves satisfactory performance with
limited training data, compared to the baselines using the full train-
ing data. For example, ReGVD obtains an accuracy of 61.68% with
60% training set, which is higher than the accuracies of BiLSTM,
TextCNN, RoBERTa, and Devign. It also achieves an accuracy of
62.55% with 80% training set, which is better than those of Code-
BERT and GraphCodeBERT.

4 CONCLUSION
We consider vulnerability identification as an inductive text classi-
fication problem and introduce a simple yet effective graph neural
network-based model, named ReGVD, to detect vulnerabilities in
source code. ReGVD transforms each raw source code into a graph,
wherein ReGVD utilizes only the token embedding layer of the
pre-trained programming language model to initialize node fea-
ture vectors. ReGVD then leverages residual connection among
GNN layers and a mixture of the sum and max poolings to learn
graph representation. To demonstrate the effectiveness of ReGVD,
we conduct extensive experiments to compare ReGVD with the
strong and up-to-date baselines on the benchmark vulnerability de-
tection dataset from CodeXGLUE. Experimental results show that
the proposed ReGVD is significantly better than the baseline mod-
els and obtains the highest accuracy of 63.69% on the benchmark
dataset. ReGVD can be seen as a general, practical, and program-
ming language-independent model that can run on various source
codes and libraries without difficulty.

ACKNOWLEDGEMENTS
This research was partially supported under the Defence Science
and Technology Group’s Next Generation Technologies Program.
We would like to thank Anh Bui (tuananh.bui@monash.edu) for
his help and support.

REFERENCES
[1] Xavier Bresson and Thomas Laurent. 2017. Residual gated graph convnets. arXiv

preprint arXiv:1711.07553 (2017).

[2] Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase

