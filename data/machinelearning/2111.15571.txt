Published at https://doi.org/10.1016/j.cor.2022.105958

An Exact Algorithm for Semi-supervised Minimum Sum-of-Squares
Clustering

Veronica Piccialli∗

Department of Computer, Control and Management Engineering,

Sapienza University of Rome, Via Ariosto 25, 00185, Italy

Anna Russo Russo

Department of Civil Engineering and Computer Science Engineering,

University of Rome Tor Vergata, Via del Politecnico 1, 00133, Italy

Antonio M. Sudoso

Department of Civil Engineering and Computer Science Engineering,
University of Rome Tor Vergata, Via del Politecnico 1, 00133, Italy

Abstract

The minimum sum-of-squares clustering (MSSC), or k-means type clustering, is traditionally con-
sidered an unsupervised learning task. In recent years, the use of background knowledge to improve

the cluster quality and promote interpretability of the clustering process has become a hot research

topic at the intersection of mathematical optimization and machine learning research. The prob-

lem of taking advantage of background information in data clustering is called semi-supervised

or constrained clustering. In this paper, we present branch-and-cut algorithm for semi-supervised

MSSC, where background knowledge is incorporated as pairwise must-link and cannot-link con-

straints. For the lower bound procedure, we solve the semideﬁnite programming relaxation of the

MSSC discrete optimization model, and we use a cutting-plane procedure for strengthening the

bound. For the upper bound, instead, by using integer programming tools, we use an adaptation
of the k-means algorithm to the constrained case. For the ﬁrst time, the proposed global optimiza-
tion algorithm eﬃciently manages to solve real-world instances up to 800 data points with diﬀerent

combinations of must-link and cannot-link constraints and with a generic number of features. This

problem size is about four times larger than the one of the instances solved by state-of-the-art

exact algorithms.

Keywords: Constrained clustering, Branch and cut, Semideﬁnite programming, Global
optimization

∗Corresponding author
Email addresses: veronica.piccialli@uniroma1.it (Veronica Piccialli), anna.russo.russo@uniroma2.it

(Anna Russo Russo), antonio.maria.sudoso@uniroma2.it (Antonio M. Sudoso)

Preprint submitted to Computers & Operations Research

July 26, 2022

2
2
0
2

l
u
J

4
2

]

C
O
.
h
t
a
m

[

2
v
1
7
5
5
1
.
1
1
1
2
:
v
i
X
r
a

 
 
 
 
 
 
Published at https://doi.org/10.1016/j.cor.2022.105958

1. Introduction

Cluster analysis or clustering is the task of grouping a set of patterns or observations, repre-

sented as points in a multidimensional space, in such a way that objects in the same group are

more similar to each other than to those in other groups (Rao, 1971; Hartigan & Wong, 1979; Jain

et al., 1999). The groups are called clusters and the set of groups is a clustering. Clustering is

a fundamental tool in modern applications, such as pattern recognition, data mining, computer

vision, machine learning, and knowledge discovery. The clustering process is by deﬁnition unsuper-

vised, which means that it only uses unlabeled data. However, without any supervision, clustering

algorithms often produce solutions that are not relevant to expert knowledge. In order to improve

the cluster quality and obtain meaningful solutions, researchers have focused on integrating knowl-

edge to allow guidance on the clustering process. Incorporating such knowledge into the clustering

process transforms the MSSC from an unsupervised learning problem to a semi-supervised one.

Semi-supervised learning is a branch of machine learning where some information on the ground

truth, as for example the labels of a subset of data, is available (Zhu & Goldberg, 2009).

Among many criteria used in cluster analysis, the most natural,

intuitive, and frequently
adopted criterion is the minimum sum-of-squares clustering (MSSC) or k-means type clustering.
Given a set of n data points in the d-dimensional Euclidean space, p1, . . . , pn, pi ∈ Rd, the MSSC
aims to partition them into k clusters, C1, . . . , Ck, by minimizing the sum of squared Euclidean
distances between the data points and the centers m1, . . . , mk of the clusters they belong to. It
can be formulated as:

k
X

X

min

j=1

pi∈Cj

kpi − mjk2
2.

(1)

In recent years, the MSSC has attracted a lot of attention in the area of data science and

operations research (Gambella et al., 2021), and it has been extended to integrate background

knowledge on objects and/or on clusters through user constraints (Basu et al., 2008; Brieden

et al., 2017; Gançarski et al., 2020). Nevertheless, with the presence of user constraints, clustering

problems become harder and require the development of dedicated algorithms. The literature about

solving the MSSC in the semi-supervised setting goes under the name of “constrained clustering”.

Constraints encountered in clustering problems can be categorized as instance-level and cluster-

level constraints. Instance-level constraints are expressed as pairwise constraints, typically must-

link and cannot-link, indicating that two points must, or cannot, be placed in the same cluster.

These constraints can be inferred from class labels: if two objects have the same label then they

are linked by a must-link constraint, otherwise by a cannot-link constraint. Pairwise constraints

naturally arise in many domains such as gene clustering (Pensa & Boulicaut, 2008; Maraziotis, 2012;

Tran et al., 2021), land consolidation (Borgwardt et al., 2014), and document clustering (Huang &

Mitchell, 2006; Hu et al., 2008). On the other hand, cluster-level constraints impose some knowledge

on the structure of the clusters (Lai et al., 2021; Gnägi & Baumann, 2021). Constrained clustering

2

Published at https://doi.org/10.1016/j.cor.2022.105958

methods can also be classiﬁed into two main categories: search-based (also known as constraint-

based) methods and distance-based (also known as similarity-based) methods (Dinler & Tural,

2016). In search-based methods (Basu et al., 2004; Davidson & Ravi, 2005; Zhang et al., 2019),

a clustering algorithm is modiﬁed to incorporate the prior knowledge into the clustering search

space. This can be realized either by enforcing user constraints (hard clustering) or by including

penalty terms for unsatisﬁed constraints in the objective function (soft clustering). In distance-

based methods (Xiang et al., 2008; Li et al., 2020), a clustering algorithm is used in conjunction

with a novel distance measure modiﬁed in accordance with the prior knowledge expressed by the

user constraints. For example, the distance measure should be adjusted in such a way that two

observations involved in a must-link constraint will be closer to each other while two observations

involved in a cannot-link constraint will be farther away from each other. Furthermore, some

hybrid methods integrate search-based and distance-based methods (Bilenko et al., 2004). In the

following, we will focus on hard search-based methods, since our method falls into this class.

It is worth noticing that enforcing constraints in the context of an existing clustering algorithm

is not a trivial step since, for a generic set of constraints, it is not straightforward to even deter-

mine whether a feasible clustering exists. Feasibility analysis for instance-level and cluster-level

constraints has received a lot of attention in the literature: Davidson & Ravi (2005, 2007) discuss

complexity results for the feasibility problem in presence of must-link and cannot-link constraints

showing that determining the feasibility of general subsets of constraints is NP-complete.

Unconstrained MSSC is NP-hard (Pacheco, 2005; Aloise et al., 2009); as a consequence, exact

algorithms for semi-supervised clustering are not very common in the literature. Moreover, these

algorithms are incredibly sensitive to the number of observations and constraints involved, meaning

that the majority of the existing methods are only capable to solve problems of very limited size.

In this paper, we propose a global optimization algorithm for semi-supervised MSSC where

prior knowledge on the data points is incorporated in the form of instance-level constraints. This

method is based on the branch-and-cut technique, which is a very general framework for ﬁnding

optimal solutions in discrete optimization problems. Lower bounds are obtained by relaxing the

discrete optimization model for semi-supervised MSSC with pairwise constraints and solving the

resulting Semideﬁnite Programming (SDP) relaxation. The main contributions of the paper are:

1. We solve semi-supervised MSSC to global optimality with an SDP-based branch-and-cut

algorithm.

2. We propose an SDP-based heuristic for solving semi-supervised MSSC providing a bound on

the optimality gap of the produced clustering.

3. Numerical experiments show that our semi-supervised algorithm solves instances up to n =
800 points with up to n/2 cannot-link and/or must-link constraints, that is a problem size
about 4 times larger than the one solved by state-of-the-art exact algorithms.

3

Published at https://doi.org/10.1016/j.cor.2022.105958

2. Related Work

This section presents a review of search-based clustering methods with pairwise constraints,

both heuristic and exact. As for the heuristic approaches, most of them have the following proper-

ties in common: (1) they extend an unconstrained clustering algorithm to integrate user constraints;

(2) they integrate instance-level constraints (i.e., must-link and cannot-link constraints) (3) they

are usually fast and ﬁnd an approximate solution, and therefore do not guarantee the satisfaction

of all the constraints nor the global solution even when the constraints are satisﬁed.

The most notable example of a search-based method enforcing pairwise hard constraints on the
MSSC problem is COP-k-means (Wagstaﬀ et al., 2001). COP-k-means adapts the classic k-means
algorithm to handle must-link and cannot-link constraints: in each iteration it tries to assign each

observation to the nearest cluster center so that no pairwise constraints are violated, but performs

this task in a greedy fashion without contemplating any backtracking. As a consequence, not only

this algorithm lacks optimality guarantees but it can also fail to return a solution when a feasible
assignment exists.
Indeed, clustering solutions produced by COP-k-means depend both on the
initial assignment, as in the classic k-means algorithm, and on the assignment order in each itera-
tion. To address the issue of constraint violation in COP-k-means, Tan et al. (2010) and Rutayisire
et al. (2011) propose a modiﬁed version comprising a pre-computation of the assignment order:
in the former algorithm, named ICOP-k-means, the assignment order is based on the certainty of
each data point, computed through the technique of clustering ensemble; in the latter, the order

is determined by carrying out a breadth-ﬁrst search of the cannot-link set. Huang et al. (2008)

propose MLC-KMeans and use the concept of assistant centroids, which are calculated using the

points implicated by must-link constraints for each cluster. The assistant centers are also used

to compute the similarity of data points and clusters. Two recent heuristics make use of integer

programming. In Vrain et al. (2020), integer programming is used to post-process the solution

produced by an unconstrained algorithm and force the pairwise constraints. Baumann (2020) de-
ﬁnes a k-means like heuristic, where the assignment step solves a binary programming problem to
enforce all the pairwise constraints.

Some clustering algorithms relax the user constraints, and consequently do not guarantee to

satisfy all of them. Methods falling in this class use penalties as a trade-oﬀ between ﬁnding the

best cluster assignment and satisfying as many constraints as possible. Basu et al. (2004) pro-

pose PCKmeans, a clustering method with pairwise constraints that allows some constraints to

be violated. PCKmeans minimizes a modiﬁed version of the MSSC objective function by adding

penalties for the violation of must-link and cannot-link constraints yielding a soft-clustering as-

signment. Davidson & Ravi (2005) propose another soft constrained clustering algorithm. This

method penalizes constraint violation using the Euclidean distance:

if a must-link constraint is

violated, then the cost is given by the distance between the centroids of the clusters containing

the data points that should be together; if a cannot-link constraint is violated, then the penalty is

4

Published at https://doi.org/10.1016/j.cor.2022.105958

computed as the distance between the center of the cluster the two data points are assigned to and

the nearest cluster center. The Lagrangian constrained clustering approach in Ganji et al. (2016)

considers a penalty for violating only cannot-link constraints. Must-link constraints are used to

aggregate the data points into super-points so that they are all satisﬁed. This method uses an

iterative approach where in each iteration a Lagrangian relaxation is solved by increasing penalties

for constraints which remain unsatisﬁed in subsequent iterations. Lastly, González-Almagro et al.

(2020) propose an iterative local search procedure for clustering instances with a big number of

pairwise constraints.

Although there has been considerable methodological research activity in the area of semi-

supervised clustering, there exist few exact methods for semi-supervised MSSC and most of them

are extensions of unconstrained MSSC exact algorithms. All these approaches use general opti-

mization tools, such as integer programming or constraint programming, and they search for a

global optimum that satisﬁes all the constraints but can only solve instances with limited number

of data points. Global optimization algorithms proposed in the literature for unconstrained MSSC

are based on cutting plane (Sherali & Desai, 2005; Peng & Xia, 2005), branch-and-bound (Koontz

et al., 1975; Diehr, 1985; Brusco, 2006; Krislock et al., 2016), branch-and-cut (Aloise & Hansen,

2009; Piccialli et al., 2022) and column generation algorithms (Du Merle et al., 1999; Aloise et al.,

2012a).

To the best of our knowledge, the ﬁrst exact method for semi-supervised MSSC is presented in

Xia (2009). Xia extends the global optimization method in Peng & Xia (2005) for unsupervised

MSSC to deal with instance-level constraints. Approximate results are obtained by halting the

algorithm before global convergence, but according to Aloise et al. (2012a), this kind of method can

produce exact solutions only on instances with about 25 data points. Aloise et al. (2012b) extend

their previous column-generation algorithm (Aloise et al., 2012a) for unsupervised MSSC and show

that they are able to solve instances with less than 200 data points and few tens of must-link and

cannot-link constraints. The column generation algorithm in Aloise et al. (2012b) is also used as a

starting point in Babaki et al. (2014), where the authors propose a column-generation framework

to solve the semi-supervised MSSC with must-link and cannot-link constraints, as well as other

monotonic constraints. However, its application scope is limited to instances with less than 200

data points. Most of the recently proposed approaches are based on the constraint programming

paradigm which is a general framework for solving combinatorial optimization problems; among

these, Duong et al. (2013, 2015, 2017) describe a declarative framework for several optimization

criteria, including that of MSSC. Duong et al. (2015) show that their method outperforms the

column-generation algorithm in Babaki et al. (2014), but results are reported only on instances

consisting of less than 200 data points. Guns et al. (2016) extend the repetitive branch-and-bound

algorithm (RBBA) proposed by Brusco for unconstrained MSSC (Brusco, 2006). They replace the

internal branch-and-bound of RBBA by a constraint programming solver, and use it to compute

5

Published at https://doi.org/10.1016/j.cor.2022.105958

tight lower and upper bounds. This algorithm, named as CPRBBA, can incorporate must-link and

cannot-link constraints and can be used in a multi-objective constrained clustering setting, which

minimizes the MSSC objective and maximizes the separation between clusters. However, results

are presented only for few instances of less than 200 data points with up to 250 constraints.

The hardest task when developing exact algorithms for constrained MSSC is to compute good

bounds in a reasonable amount of time. Although their focus is not on exact methods, Liberti &

Manca (2021) propose several MINLP reformulations of MSSC with side constraints, both exact

and approximate. Among these, some reformulations yield convex continuous relaxations that may

be embedded in global optimization algorithms.

In the next section we describe the ingredients of the proposed branch-and-cut algorithm.

3. Branch-and-Cut Algorithm

3.1. Notation

Let N = {1, . . . , n} be the set of indices of the data points pi ∈ Rd, K = {1, . . . , k} the set of
indices of the clusters, ML ⊆ N × N the set of must-link (ML) constraints and CL ⊆ N × N the
set of cannot-link (CL) constraints. Let S n denote the set of all n × n real symmetric matrices.
We denote by M (cid:23) 0 that matrix M is positive semideﬁnite and let S n
be the set of all positive
+
semideﬁnite matrices of size n × n. We denote by h·, ·i the trace inner product. That is, for any
A, B ∈ Rm×n, we deﬁne hA, Bi := trace(B>A). Given a matrix A, we denote by Ai· the i-th row
of A. We denote by en the vector of all ones of length n and by In the identity matrix of size n × n.
We omit the subscript in case the dimension is clear from the context. For a symmetric matrix A
we denote the set of its eigenvalues by λ(A).

3.2. Problem deﬁnition

The semi-supervised MSSC with pairwise constraints can be formulated as follows:

min

s.t.

n
X

k
X

i=1

j=1

xijkpi − mjk2
2

k
X

j=1
n
X

xij = 1

∀i ∈ N ,

xij ≥ 1

∀j ∈ K,

i=1
xih = xjh
∀h ∈ K, ∀(i, j) ∈ ML,
xih + xjh ≤ 1 ∀h ∈ K, ∀(i, j) ∈ CL,
xij ∈ {0, 1}
mj ∈ Rd

∀i ∈ N , ∀j ∈ K,

∀j ∈ K.

6

(2a)

(2b)

(2c)

(2d)

(2e)

(2f)

(2g)

Published at https://doi.org/10.1016/j.cor.2022.105958

In (2), the cluster centers are at the unknown points mj and the binary decision variable xij
expresses whether data point i is assigned to cluster j or not. Constraints (2b) ensure that each
data point is assigned to exactly one cluster, Constraints (2c) avoid empty clusters and Constraints

(2d) and (2e) enforce ML and CL constraints, respectively. From now on, we denote the discrete
feasible region of (2) by F = (cid:8)xij ∈ {0, 1} (cid:12)

(cid:12) (2b), (2c), (2d), (2e) hold(cid:9).

Problem (2) is a mixed-integer programming problem with nonlinear objective function, which

is known to be NP-hard like the unconstrained MSSC (Davidson & Ravi, 2007; Aloise et al., 2009).

The diﬃculty of this problem derives from two diﬀerent aspects. First, the decision variables are

discrete and the pairwise constraints are embedded as hard constraints. Secondly, the objective

is nonlinear and nonconvex. These diﬃculties in the objective as well as in the constraints make

semi-supervised MSSC extremely hard and challenging to solve exactly.

Setting the gradient of the objective function with respect to mj to zero yields

mj =

Pn
i=1 xijpi
Pn
i=1 xij

,

∀j ∈ K,

that is the average of all the points assigned to cluster j.
In other words, the optimal cluster
centers are always at the centroids of the clusters. Replacing the formula for mj in (2), we obtain

n
X

k
X

i=1

j=1

xij

(cid:13)
(cid:13)
(cid:13)
pi −
(cid:13)
(cid:13)

Pn
l=1 xljpl
Pn
l=1 xlj

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

xij ∈ F ∀i ∈ N , ∀j ∈ K.

min

s.t.

(3)

The unconstrained MSSC, i.e., Problem (3) with ML = CL = ∅, is known to be equivalent to
a nonlinear SDP problem, the so-called 0-1 SDP (Peng & Wei, 2007). Here we derive ﬁrst the

equivalent 0-1 SDP in the unconstrained setting, following the derivation process in Peng & Wei

(2007), and then we extend the formulation to the constrained case.

For convenience, we collect all the data points pi as rows in a matrix Wp. Let W = WpW >
p

be
the matrix of the inner products of the data points, i.e., Wij = p>
i pj for all i, j ∈ N . Furthermore,
collect the binary decision variables xij from (3) in the n × k matrix X and deﬁne the n × n matrix
Z := X(X >X)−1X >. It is easy to verify that Z is a symmetric matrix with nonnegative entries
satisfying Z2 = Z. Furthermore, it follows immediately that the sum of each row (or column) is
equal to 1 and the trace is equal to k. By rearranging the terms in the objective function we obtain
the equivalent reformulation of unconstrained MSSC:

min tr(W (I − Z))

s.t. Ze = e, tr(Z) = k,

Z ≥ 0, Z2 = Z, Z = Z>.

7

(4)

Published at https://doi.org/10.1016/j.cor.2022.105958

Peng & Wei (2007) proved that any feasible solution Z for Problem (4) is necessarily associated
with a feasible unconstrained MSSC assignment matrix X. Therefore, the equivalence among the
MSSC formulations (2) (without constraints (2d) and (2e)) and (4) is established. Note that for a
given data matrix Wp, the trace of W is a ﬁxed quantity. Therefore, we can solve Problem (4) by
maximizing tr(W Z). From now on we refer to any feasible solution Z as “clustering matrix”. For
any feasible clustering matrix it is easy to verify that:

• If data points i and j are in the same cluster C, then Zi· = Zj· and the non-zero entries in

rows i and j are equal to 1
|C|

, where |C| is the cardinality of cluster C.

• If data points i and j are not in the same cluster, then Zij = 0.

This allows to express ML and CL constraints by equality constraints on the matrix Z, getting

the following 0-1 SDP reformulation of semi-supervised MSSC with pairwise constraints:

min tr(W (I − Z))

s.t. Ze = e, tr(Z) = k,

Zih = Zjh
Zij = 0
Z ≥ 0, Z2 = Z, Z = Z>.

∀h ∈ N , ∀(i, j) ∈ ML,
∀(i, j) ∈ CL,

(5)

It is worthwhile comparing (3) with (5). Diﬀerently from (3), the objective function in (5) is
linear and the constraints are all linear except Z2 = Z, that is even more complex than the binary
constraints in (3) since it constrains the eigenvalues of Z being binary. However, by relaxing this
constraint, we can get very good lower bounds on the original problem, as shown in the next

subsection. The SDP relaxation we get is the main ingredient of our branch-and-cut approach.

3.3. Lower bound

By relaxing the non-convex constraint Z2 = Z in (5) into the semideﬁnite constraint Z (cid:23) 0,

we obtain the following SDP relaxation:

min tr(W (I − Z))

s.t. Ze = e, tr(Z) = k,

Zih = Zjh
Zij = 0
Z ≥ 0, Z ∈ S n
+.

∀h ∈ N , ∀(i, j) ∈ ML,
∀(i, j) ∈ CL,

(6)

Interestingly enough, ML constraints allow to reduce the size of the SDP relaxation to be

solved at each node. In particular, we construct a graph from the set of ML constraints where each

8

Published at https://doi.org/10.1016/j.cor.2022.105958

node is a data point, and there is an edge between two nodes if the corresponding data points are

involved in a must-link constraint. Then we compute the transitive closure of the graph leading to
a partition of nodes into s components B1, . . . , Bs. These connected components collect data points
that must be in the same cluster. We collapse each connected component Bi into a single “super
point” pi
that is obtained as the sum of the data points in Bi. Therefore, instead of clustering
.
the set of initial points p1, . . . , pn, we search for a clustering on the set of super points p1, . . . , ps
A similar approach for handling ML constraints, and thus reducing the size of the problem, is

used in Guns et al. (2016) where they replace all the points in each connected component with a

single point which is the sum of all the points in that set, weighted by the size of the connected

component. In our case, we exploit this idea to obtain an equivalent reformulation over a lower

dimensional positive semideﬁnite cone.

Formally, given a set of n initial data points, assume that there are s ≤ n connected components

B1, . . . , Bs with super points p1, . . . , ps

. Deﬁne the s × n binary matrix

T s
ij

=

( 1 if j ∈ Bi
0 if j /∈ Bi

encoding the data points that have been merged (i.e., the i-th row of T s is the indicator vector
of Bi for i = 1, . . . , s) and vector es = T se containing the size of each connected component.
Since we are interested in clustering the super points arising from ML constraints, we observe that
T sW (T s)> shrinks the size of Wp (i.e., the matrix having data points as rows) by substituting
p and omitting the rows of the merged data points. Instance-level constraints
row i by pi
that remain to be satisﬁed are only CL constraints. To this end, we denote by CL the set of
CL constraints between two connected components. We add to CL a CL between super-points pi
and pj
if there exists a CL constraint on two data points p and q such that p ∈ Bi and q ∈ Bj.
Therefore, the SDP solved for computing the lower bound becomes

= P

p∈Bi

min hI, W i − hT sW (T s)>, Zi

s.t. Zes = e,

hDiag(es), Zi = k,
Zij = 0
Z ≥ 0, Z ∈ S s
+.

∀(i, j) ∈ CL,

(7)

The proof of equivalence between (6) and (7) can be easily derived by using Theorem 2 in

Piccialli et al. (2022). In the latter, ML and CL constraints are added one at a time when visiting

the branch-and-bound tree, since the children are generated either by merging two points thanks

to an ML or adding the corresponding CL constraint. Therefore, the size of the matrix is reduced

by one at each branching level, and the eﬀect of the size reduction is essentially negligible thanks

9

Published at https://doi.org/10.1016/j.cor.2022.105958

to the small number of visited nodes. In the semi-supervised setting, instead, pairwise constraints

are included at the beginning of the formulation, leading to a diﬀerent transformation (the one
described by T s) and to a signiﬁcant reduction in size. In particular, for the case k = 2 we can
further reduce the size of SDP since a CL constraint determines in which cluster a data point
should be. Hence, we can derive additional ML constraints as follows: for any i, j, h ∈ N such that
(i, h) ∈ CL and (j, h) ∈ CL, then we add (i, j) to ML.

Our algorithm produces a binary enumeration tree. Every time a node is split into two children,
a pair of points (i, j) is chosen, and an ML constraint and a CL constraint are respectively imposed
on the left and the right child. These two types of constraints partition the set of solutions

associated to the parent child into two disjoint subsets. As for the branching strategy, we observe
that in a matrix Z corresponding to a clustering, for each pair (i, j) either Zij = 0 or Zi· = Zj·.
Suppose that for the optimal solution of the SDP relaxation there are indices i and j such that
2 6= 0, then we generate a cannot-link branch with Zij = 0 and a must-link branch
ZijkZi· − Zj·k2
with Zi· = Zj·. Regarding the variable selection, we choose indices i and j such that in both
branches we expect a signiﬁcant improvement of the lower bound. The branching pair is chosen as

the

3.4. Valid inequalities

arg max
i,j

n min (cid:8)Zij, kZi· − Zj·k2

2

(cid:9)o
.

In this subsection, we present three classes of inequalities we use for strengthening the bound.

Pair. In any feasible clustering, it holds that

Zij ≤ Zii, Zij ≤ Zjj ∀i, j ∈ N , i 6= j.

(8)

Triangle. If data points i and j are in the same cluster and data points j and h are in the same
cluster, then i and h necessarily must be in the same cluster:

Zij + Zih ≤ Zii + Zjh ∀i, j, h ∈ N , i, j, h distinct.

(9)

Clique. If the number of clusters is k, for any subset Q of k + 1 points at least two points must be
in the same cluster:

X

Zij ≥

(i,j)∈Q,i<j

1

n − k + 1 ∀Q ⊂ N , |Q| = k + 1.

(10)

Piccialli et al. (2022) report detailed computational results on benchmark instances showing

that lower bounds provided by the Peng-Wei SDP relaxation with these inequalities are very close

to the optimal values. Here we add pair, triangle and clique inequalities to the SDP relaxation via

a cutting-plane procedure only if they are violated. After each cutting-plane iteration we remove

10

Published at https://doi.org/10.1016/j.cor.2022.105958

constraints that are not active at the optimal solution of the SDP relaxation.

In this way, we

keep each bounding problem to a computationally tractable size. Moreover, inequalities that are

included in the parent model during the last cutting-plane iteration are also inherited by its children

and added in their model from the beginning. This procedure allows to quickly retrieve several

eﬀective inequalities, and save a signiﬁcant number of cutting-plane iterations in the children.

3.5. Valid lower bounds and feasibility issues

Oﬀ-the-shelf solvers for SDP problems are generally based on Interior Point Methods (IPMs)

(Alizadeh, 1995). While these algorithms solve SDPs to arbitrary accuracy in polynomial time,

in practice, they suﬀer from scalability and performance issues. When the considered clustering

problems have large number of variables and constraints, solution time and memory requirements

tend to explode, making SDPs prohibitive to work with. Compared to IPMs, solvers based on

semi-proximal ADMM/ALM (Alternating Direction Method of Multipliers/Augmented Lagrangian

Method) can scale to signiﬁcantly larger problem sizes, while trading oﬀ the accuracy of the result-

ing output (Sun et al., 2015; Yang et al., 2015). However, they show two limitations that need to be

addressed to guarantee the theoretical validity of a branch-and-bound algorithm. First, when using

ADMM/ALM methods, it is hard to reach a solution to high precision in a reasonable amount of

time. This implies that the bound provided by an SDP relaxation solved to a moderate precision

may not be a valid lower bound. Second, existing SDP solvers based on ADMM/ALM frameworks

can not detect primal and dual infeasibilities since they are designed on the assumption that the

primal and the dual problems are feasible.

Valid lower bounds. To overcome the ﬁrst limitation, we use the post-processing technique devel-
oped in Jansson et al. (2008) where the authors propose a method to obtain rigorous lower bounds

on the optimal value of SDP problems. Recall that the dual objective function value of any dual

feasible solution yields a bound on the optimal objective function of the primal. Therefore, every

dual feasible solution and, in particular, the optimal solution of the dual SDP, gives a valid bound

on the solution of the discrete optimization problem. However, the dual objective function value

represents a valid dual bound only if the SDP relaxation is solved to high precision. In the follow-

ing, we generate a “safe” underestimate for the primal SDP, and hence a valid lower bound for the

MSSC problem with pairwise constraints. We start with the following lemma.

Lemma 3.1. Let S, X ∈ S n be matrices that satisfy 0 ≤ λmin(X) and λmax(X) ≤ ¯x for some
¯x ∈ R. Then the inequality

hS, Xi ≥ ¯x X

λi(S)

i : λi(S)<0

holds.

Proof. See the proof in (Jansson et al., 2008, Lemma 3.1 on p. 184).

11

Published at https://doi.org/10.1016/j.cor.2022.105958

be the symmetric matrix such that hEs

At this point, we can adapt the post-processing procedure in Jansson et al. (2008) to our SDP.
Let Es
i , Zi = (Zes)i for i ∈ {1, . . . , s} and Eij be symmetric
i
matrix such that hEij, Zi = Zij for (i, j) ∈ CL. Furthermore, we deﬁne a mapping τ such that for
(i, j) ∈ CL we have τ (i, j) = h ∈ {1, . . . , |CL|}.

Theorem 3.2. Let Z? be the optimal solution of Problem (7) with objective function value p?.
Consider the dual variables ˜y ∈ Rs+1, ˜µ ∈ R|CL|, ˜V ∈ S s, ˜V ≥ 0 and set ˜S = −T sW (T s)> −
Ps

˜µτ (i,j)Eij − ˜V , then a safe lower bound for p? is given by

i − ˜ys+1Diag(es) − P

˜yiEs

i=1

(i,j)∈CL

lb =

s
X

i=1

˜yi + k ˜ys+1 + ¯z X

λi( ˜S),

i : λi( ˜S)<0

where ¯z ≥ λmax(Z?).

Proof. See the proof in Appendix A.

If matrix ˜S is positive semideﬁnite, then (˜y, ˜µ, ˜V , ˜S) is a dual feasible solution and Ps
˜yi +
k ˜ys+1 is already a valid lower bound. Otherwise, we decrease the dual objective by adding the neg-
i : λi( ˜S)<0 λi( ˜S) to it where ¯z is an upper bound on the maximum eigenvalue
ative perturbation ¯z P
of any feasible solution of the primal. We can bound the maximum eigenvalue of any feasible Z
by 1 since Ze ≤ Zes = e. Therefore, we set ¯z = 1 and after the SDP relaxation has been solved to
a moderate precision, we apply Theorem 3.2 to obtain valid lower bounds.

i=1

Feasibility. It is well known that for infeasible convex optimization problems some of the iterates
of ADMM/ALM methods diverge. However, terminating the algorithm when the iterates become

large is unreliable in practice: such termination criterion is just an indication that a problem

might be infeasible, and not a certiﬁcate of infeasibility. Given a large number of initial pairwise

constraints and those derived from branching decisions, it is very likely to visit infeasible nodes

while going down the tree. Hence, we want to detect infeasibility as soon as possible to save

ourselves the expensive part of calculating the bound. To this end, by using an oﬀ-the-shelf integer

programming solver such as Gurobi (Gurobi Optimization, 2021), we solve the feasibility Problem

(11) to check whether a feasible clustering exists. If such clustering does not exist then we simply

prune the node, otherwise we solve the SDP relaxation to obtain a lower bound.

min 0

s.t.

xij ∈ F ∀i ∈ N , ∀j ∈ K.

(11)

3.6. Heuristic

The most popular heuristic for solving unconstrained MSSC is unarguably the k-means algo-
rithm (MacQueen et al., 1967; Lloyd, 1982). Given the initial cluster centers, k-means proceeds

12

Published at https://doi.org/10.1016/j.cor.2022.105958

by alternating between two steps until convergence: in the ﬁrst step, each data point is assigned

to the closest cluster center, whereas in the second step, the cluster centers are updated by taking

the average of all the data points that belong to each cluster. These two steps are repeated until

the assignment of data points to clusters no longer changes.

Because of its simplicity, eﬃciency and empirical success, it has been commonly used as a
template for developing constrained clustering algorithms. In this direction, COP-k-means adapts
k-means to handle ML and CL constraints:
in each iteration it tries to assign each data point
to the nearest cluster center so that no pairwise constraints are violated (Wagstaﬀ et al., 2001).

Vrain et al. (2020) propose a novel approach for constrained clustering developing a post-processing

procedure to enforce constraints a posteriori. Given a matrix that contains the degree of member-

ship of each data point to each cluster (obtained by means of any constrained or unconstrained

clustering algorithm) this method ﬁnds a hard assignment satisfying all the constraints by solving

a combinatorial optimization problem. The post-processed clustering matrix does not minimize

the within-cluster sum of squares criterion (i.e., the MSSC objective function) but it is constructed

in such a way that it resembles the initial cluster membership.

In Vrain et al. (2020), cluster-

ing solutions provided by this procedure score a better accuracy than those obtained by running
COP-k-means several times, though no optimality guarantee is given. Another recent heuristic
algorithm for constrained clustering has been proposed by Baumann (2020). The main idea of
this approach consists in modifying the assignment step of k-means to ﬁnd an optimal clustering
satisfying all the pairwise constraints by using an Integer Linear Programming (ILP) formulation.
Diﬀerently from k-means, this assignment step does not admit an analytic solution, but it requires
the solution of a linear program with binary variables. The objective function and the feasible set

are the same as in (2), but the centers are ﬁxed at the current value. After the assignment step,
this procedure behaves exactly like k-means, since the cluster centers are updated by averaging the
data points assigned to each of them. The initial cluster centers are randomly picked from the set
of data points. Unlike COP-k-means, if there exists a feasible clustering the assignment step can
never fail to assign each data point to a cluster.

In order to make our branch-and-bound algorithm eﬃcient, we need a heuristic procedure to

obtain a feasible high quality clustering at each node. Greedy heuristic algorithms tend to fail
when the number of CL and ML is high, while k-means, also in its constrained variant proposed by
Baumann (2020), is very sensitive to the choice of the initial cluster centers, similarly to other local

methods for non-convex optimization problems. To overcome this drawback, several initialization
strategies have been proposed to prevent k-means to get stuck in a low quality local minimum
(Pena et al., 1999; Celebi et al., 2013). In Piccialli et al. (2022), an initialization technique has

been deﬁned based on the primal solution provided by the SDP relaxation. The idea is that if
the SDP relaxation (7) were tight, then the solution Z would be feasible for the 0-1 SDP (5), and
hence would allow to easily recover the centroids. If the relaxation is not tight, then we ﬁnd the
closest rank-k approximation of Z in terms of the Frobenius norm. More precisely, we solve an
13

Published at https://doi.org/10.1016/j.cor.2022.105958

optimization problem where the norm of the diﬀerence between Z and a rank-k approximating
matrix is minimized. This problem admits an analytic solution that can be obtained by computing
the truncated singular value decomposition (SVD) of Z: instead of taking all the eigenvalues and
their corresponding eigenvectors, we only take the k largest eigenvalues and their corresponding
eigenvectors (see the low-rank approximation theorem in Eckart & Young (1936)).

In our branch-and-cut algorithm we use an eﬀective heuristic combining this SDP-based initial-

ization technique with the assignment step developed by Baumann (2020). We name this procedure
IPC-k-means, short for integer pairwise constrained k-means, and we describe it in Algorithm 1.

Algorithm 1: IPC-k-means

Input: Data points p1, . . . , pn, number of clusters k, sets of constraints ML and CL, data
matrix Wp, optimal solution ˜Z of the SDP relaxation with ML and CL constraints
1. Solve ˆZ = arg min{|| ˜Z − Z||F s.t. rank(Z) = k} by computing the truncated SVD of ˜Z.
2. Compute the centroid matrix approximation ˆM = ˆZWp.
3. Cluster the rows of ˆM with k-means to get the initial cluster centers m1, . . . , mk.
repeat

4.1. Compute the optimal cluster assignments x?
ij

by solving:

min

s.t.

n
X

k
X

xijkpi − mjk2
2

j=1

i=1
xij ∈ F ∀i ∈ N , ∀j ∈ K.

(12)

4.2. Set Cj ← {pi : x?
ij
4.3. Update the cluster centers m1, . . . , mk by taking the mean of the data points assigned to

= 1} for each j = 1, . . . , k.

each cluster C1, . . . , Ck.

until convergence;
Output: Clusters C1, . . . , Ck.

Our heuristic requires the solution of a small number of SDP problems. Hence it is more

expensive from the computational point of view in comparison to random initialization techniques.

However, solving the SDP relaxation provides a lower bound on the optimal value, so that it

allows to certify an optimality gap for the provided feasible clustering. Furthermore, numerical

experiments in Section 4.4 show that the initial choice of cluster centers retrieved from the SDP

relaxation that incorporates ML and CL constraints is robust and yields high quality clustering

solutions. The overall branch-and-cut algorithm is illustrated in Algorithm 2.

4. Computational Results

In this section, we describe the details of the implementation and we show the numerical results

on real-world datasets.

14

Published at https://doi.org/10.1016/j.cor.2022.105958

Algorithm 2: Branch-and-Cut Algorithm

Input: Sets of constraints ML and CL, number of clusters k, linear kernel matrix W
1. Build T s, es and CL from ML and CL. Let P0 be the initial 0-1 SDP in (5) and set Q = {P0}.
2. Set X ? = null with objective value v? = ∞.
3. While Q is not empty:

3.1. Select and remove problem P from Q.
3.2. Solve the feasibility problem (11). If it is infeasibile, go to Step 3.
3.3. Solve the SDP relaxation (7) to get a lower bound LB and the optimal solution Z.
3.4. If LB ≥ v?, go to Step 3.
3.5. Search for pair (8), triangle (9), and clique (10) inequalities violated by Z. If any

are found, add them to the current SDP relaxation and go to Step 3.3.

3.6. Run the heuristic in Algorithm 1 to get an assignment X and an upper bound U B.

If U B < v? then set v? ← U B, X ? ← X.

3.7. Select the branching pair (i, j) and partition problem P into ML and CL

sub-problems. For each problem update T s, es and CL accordingly, add them to Q
and go to Step 3.

Output: Optimal assignment matrix X ? with objective value v?

4.1. Implementation details

PC-SOS-SDP, which stands for Pairwise Constrained SOS-SDP, is implemented in C++ with

some routines written in MATLAB. The SDP relaxation at each node is solved by means of

SDPNAL+, a MATLAB software that implements an augmented Lagrangian method to solve

large scale SDPs with bound constraints (Sun et al., 2020). We set the accuracy tolerance of the

solver to 10−5 in the relative KKT residual. We also use Gurobi (Gurobi Optimization, 2021) in

order to deal with the ILPs required for the upper bound computation and the feasibility check.

We run the experiments on a machine with Intel(R) Xeon(R) 8124M CPU @ 3.00GHz with 16

cores, 64 GB of RAM, and Ubuntu Server 20.04. For pair and triangle inequalities, we randomly

separate at most 100000 valid cuts, we sort them in decreasing order with respect to the violation,

and we add the ﬁrst 5% of violated ones in the current cutting-plane iteration. For the separation

of clique inequalities, we use the heuristic procedure described in Piccialli et al. (2022) that returns
at most n valid cuts. The tolerance for checking the violation is set to 10−4. The maximum number
of cutting plane iterations at the root node is set to 50, whereas for the ML and CL children this

number is set to 30. We stop the cutting-plane procedure not only when there are no violated

inequalities, but also when the lower bound does not improve signiﬁcantly in the current cutting-

plane iteration. Finally, we visit the tree with the best-ﬁrst search strategy. In order to improve the

eﬃciency of the branch-and-bound search, PC-SOS-SDP processes many nodes in parallel using a

thread pool of ﬁxed size: whenever an ML or CL sub-problem is created, it is assigned to one of

the available threads and run in parallel with the other threads of the pool. Furthermore, each

thread calls SDPNAL+ in a separate MATLAB session. For the parallel setting, we use diﬀerent
conﬁgurations depending on the instance size. For small instances (n < 300) we use a pool of 16
15

Published at https://doi.org/10.1016/j.cor.2022.105958

threads, each of them running on a MATLAB session with a single component thread. For larger
instances (n ≥ 300) we use a pool of 8 threads, each of them running on a MATLAB session with
2 component threads. In all cases, the session for the computation at the root node uses all the

available cores. The source code of PC-SOS-SDP and the instances used in our tests are available
at https://github.com/antoniosudoso/pc-sos-sdp.

4.2. Instances generation

We build our semi-supervised clustering instances from real-world datasets for classiﬁcation

problems. For each dataset, we generate several instances diﬀering in the type and amount of user
constraints. Speciﬁcally, given a dataset with n data points, we chose to build sets of constraints
consisting of approximately n/2 and n/4 constraints; each set either contains only ML constraints,
only CL constraints, or an equal number of ML and CL constraints. All these constraints are

enforced according to the dataset true class partitioning, which is, in general, in contradiction with

the unconstrained MSSC global optimum. For each of these conﬁgurations, we generate 5 random

sets of constraints using a classic procedure described in Wagstaﬀ et al. (2001) and then also used

in more recent works on exact semi-supervised clustering methods (Babaki et al., 2014; Duong

et al., 2015; Guns et al., 2016): at each step, a pair of data points is randomly selected and either

an ML or a CL constraint is deﬁned depending on the true labels of the data points; the procedure

stops when the desired amount of ML and CL constraints is achieved.

4.3. Results on real-world instances

We consider 12 real-world datasets for classiﬁcation problems, with a number of data points
n ranging between 150 and 801, and with a number of features d ranging between 4 and 20531.
For each dataset, we consider 10 instances with only CL constraints, 10 instances with only ML

constraints, and 10 instances with an equal amount of ML and CL constraints. Overall, we build

and solve 360 constrained clustering instances. In the experiments, an instance is solved successfully

when the optimality gap is less or equal than 10−4. This gap measures the diﬀerence between the
best upper and lower bounds and it is calculated as (U B − LB)/U B. The MSSC requires the user
to specify the number of clusters k to generate. In the literature, clustering validity indices are
commonly used to determine a suitable number of clusters. In the semi-supervised setting instead,

the number of clusters is known and assumed to be equal to the number of classes. The datasets

characteristics, i.e., number of data points, features, and clusters, are reported in Table 1.

The results of our experiments are outlined in Tables 2, 3, 4, comprising respectively tests on

instances with only ML constraints, only CL constraints and both types of constraints. Every entry

of these tables involves a single dataset, whose name, size and number of clusters are reported, and

shows aggregated statistics of 5 random instances with a certain number of ML and CL constraints:

these statistics include the average number of separate data points at the root node (i.e., the size

of the problem at the root), the average root gap, the average size of the branching tree and

16

Published at https://doi.org/10.1016/j.cor.2022.105958

Dataset

Iris
Wine
Connectionist
Seeds
Glass
Heart

n

150
178
208
210
214
299

d

4
13
60
7
9
12

k

3
3
2
3
6
2

Dataset

Vertebral
Accent
Ecoli
ECG5000
Computers
Gene

n

310
329
336
500
500
801

d

6
12
7
140
720
20531

k

2
6
8
5
2
5

Table 1: Characteristics of the real-world datasets. They all can be downloaded at the UCI (Dua & Graﬀ, 2017) and
UCR (Dau et al., 2018) websites.

the average completion time. Finally, in the last column, a percentage expresses the share of

successfully solved instances, i.e., instances that were solved before reaching the maximum number

of nodes, which is set to 200; in those cases where one or more instances can not be solved within

this limit, in the last column, between brackets, we report the average gap reached before halting

the branch-and-cut algorithm on the unsolved instances.

Must-Link constraints only. Table 2 shows that, when dealing with ML constraints only, our al-
gorithm can solve to optimality every instance in less than half an hour on average and within a

handful of nodes. It can be noted that the mean number of nodes and mean completion time are

generally lower when a higher number of constraints are included: indeed, adding an ML constraint

has the eﬀect of merging two separate data points into one, thus overall decreasing the initial size

of the clustering problem. The boxplots in Figure 1 oﬀer a more detailed view of the computational

time required on each of our 120 instances with ML constraints only, grouped by dataset. Here we

can see that on datasets with up to 300 points our branch-and-cut algorithm always converged to

optimality in less than 500 seconds, while on bigger instances the highest time required is slightly

over 2000 seconds, with 95% of these instances being solved in less than 1500 seconds.

Cannot-Link constraints only. Table 3 displays results for instances consisting of CL constraints
only. These kinds of constraints usually make the clustering problem much harder than the uncon-

strained version. Indeed, the computational time required to solve these instances to optimality

tends to grow larger as a greater number of constraints is included. Instances with just two clus-

ters represent a consistent exception to this tendency since in this case CL constraints allow to

infer non-redundant ML constraint, therefore decreasing the size of the initial clustering problem.

Overall, 4 out of 120 instances can not be solved within the threshold that is set on the branching

tree size, but for these instances we are still able to provide a very good clustering solution with a
certiﬁed relative gap smaller than 0.03%. As can be seen in Figure 2, the instances with less than
300 points are all solved in 800 seconds or less; for the bigger instances the maximum computa-

tional time spent is about 16000 seconds but 91% of them are solved to optimality in less than 2

hours.

17

Published at https://doi.org/10.1016/j.cor.2022.105958

Figure 1: Boxplot of computational times on instances with ML constraints only.

Combination of Must-Link and Cannot-Link constraints. Results for instances with mixed types
of constraints are reported in Table 4. When dealing with both types of constraints, our algorithm

fails to solve 2 out of 120 instances, as the maximum number of nodes is encountered before reaching

optimality. Nonetheless on these instances we ﬁnd a feasible solution with relative gap not greater
than 0.04%. All the remaining instances are solved within one hour on average: the highest time
required was 800 seconds for datasets made of less than 300 data points, and barely more than

5000 seconds for larger datasets, as shown in the boxplots of Figure 3.

Note that the time needed to solve the ILPs for ﬁnding feasible clusterings is neglectable: on

the largest instance, it is about 10 seconds.

These results show that our method is able to solve successfully instances up to a size of n = 801
data points and n/2 pairwise constraints, with CL constraints being the most challenging kind.
Moreover, we can not miss to point out that our average root gap is smaller than 1% on each dataset
and for each type of constraints, and smaller than 0.01% on 47% of our instances. Furthermore, it
is worth noticing that we are able to solve instances with a very large number of features d (over 20
thousand), as our algorithm is minimally sensitive to the dimension of the feature space. Indeed,

the number of features has no inﬂuence on the lower bound computation since it is hidden in the
matrix W that is computed only once.

Non-aggregated statistics on each of our instances can be found in Appendix B, where each

PC-SOS-SDP execution is described in more detail.

18

Published at https://doi.org/10.1016/j.cor.2022.105958

Figure 2: Boxplot of computational times on instances with CL constraints only.

Figure 3: Boxplot of computational times on instances with ML and CL constraints.

19

Published at https://doi.org/10.1016/j.cor.2022.105958

dataset

constraints

Iris
Iris
Wine
Wine
Seeds
Seeds
Connectionist
Connectionist
Glass
Glass
Heart
Heart
Vertebral
Vertebral
Accent
Accent
Ecoli
Ecoli
ECG5000
ECG5000
Computers
Computers
Gene
Gene

50
100
50
100
50
100
50
100
50
100
100
150
100
150
100
150
100
150
150
250
150
250
200
400

n

150
150
178
178
210
210
208
208
214
214
299
299
310
310
329
329
336
336
500
500
500
500
801
801

k

3
3
3
3
3
3
2
2
6
6
2
2
2
2
6
6
8
8
5
5
2
2
5
5

size

root gap nodes

time (s)

success rate

100.4
56.0
128.2
81.4
160.0
111.6
158.2
109.0
164.4
117.8
199.0
152.8
210.6
161.4
229.4
186.0
236.4
189.4
350.4
253.2
350.4
252.0
601.6
417.2

0.004%
0.001%
0.016%
0.009%
0.032%
0.009%
0.005%
0.007%
0.038%
0.059%
0.001%
0.001%
0.018%
0.050%
0.049%
0.374%
0.059%
0.015%
0.014%
0.324%
0.003%
0.001%
0.001%
0.001%

1.8
1.0
1.8
1.4
4.6
1.8
1.0
1.8
7.0
4.6
1.0
1.0
5.0
4.6
3.0
5.0
33.4
3.4
9.0
6.2
1.4
1.0
1.0
1.0

16.6
5.8
101.8
31.6
84.4
19.2
160.8
75.6
234.2
312.4
309.2
147.0
594.8
260.8
371.0
679.6
852.0
558.2
1086.4
679.2
1652.2
387.6
503.8
550.6

100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%

Table 2: PC-SOS-SDP results on the instances where only ML constraints are included.

20

Published at https://doi.org/10.1016/j.cor.2022.105958

dataset

constraints

Iris
Iris
Wine
Wine
Seeds
Seeds
Connectionist
Connectionist
Glass
Glass
Heart
Heart
Vertebral
Vertebral
Accent
Accent
Ecoli
Ecoli
ECG5000
ECG5000
Computers
Computers
Gene
Gene

50
100
50
100
50
100
50
100
50
100
100
150
100
150
100
150
100
150
150
250
150
250
200
400

n

150
150
178
178
210
210
208
208
214
214
299
299
310
310
329
329
336
336
500
500
500
500
801
801

k

3
3
3
3
3
3
2
2
6
6
2
2
2
2
6
6
8
8
5
5
2
2
5
5

size

root gap nodes

time (s)

success rate (gap)

150.0
150.0
178.0
178.0
210.0
210.0
186.6
139.4
214.0
214.0
239.6
182.4
254.8
199.2
329.0
329.0
336.0
336.0
500.0
500.0
427.2
321.2
801.0
801.0

2.2
0.010%
2.6
0.102%
1.4
0.039%
6.2
0.063%
9.0
0.054%
11.0
0.097%
1.0
0.002%
1.4
0.006%
32.6
0.104%
23.4
0.108%
1.0
0.001%
1.0
0.001%
1.4
0.008%
12.2
0.030%
57.2
0.067%
0.107%
75.0
0.104% 150.6
0.270% 116.6
64.8
0.032%
22.6
0.032%
1.0
0.001%
1.0
0.001%
2.6
0.011%
3.0
0.014%

78.4
58.6
85.2
216.6
227.2
225.8
165.6
102.6
539.4
406.4
481.8
232.6
425.0
387.0
1264.4
1393.0
2432.0
2231.8
8042.8
5254.2
2351.2
642.4
1540.0
1768.0

100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
80% (0.013%)
100%
60% (0.024%)
100%
80% (0.012%)
100%
100%
100%
100%
100%

Table 3: PC-SOS-SDP results on the instances where only CL constraints are included.

21

Published at https://doi.org/10.1016/j.cor.2022.105958

dataset

constraints

Iris
Iris
Wine
Wine
Seeds
Seeds
Connectionist
Connectionist
Glass
Glass
Heart
Heart
Vertebral
Vertebral
Accent
Accent
Ecoli
Ecoli
ECG5000
ECG5000
Computers
Computers
Gene
Gene

25
50
25
50
25
50
25
50
25
50
50
75
50
75
50
75
50
75
75
125
75
125
100
200

n

150
150
178
178
210
210
208
208
214
214
299
299
310
310
329
329
336
336
500
500
500
500
801
801

k

3
3
3
3
3
3
2
2
6
6
2
2
2
2
6
6
8
8
5
5
2
2
5
5

size

root gap nodes

time (s)

success rate (gap)

125.0
100.0
153.0
128.4
185.0
160.0
177.6
139.6
189.0
164.6
231.2
189.4
244.0
199.2
279.2
254.4
286.0
261.4
425.2
375.0
404.0
323.2
701.0
602.2

1.8
0.014%
1.4
0.004%
1.4
0.016%
2.2
0.022%
5.8
0.034%
3.8
0.042%
1.0
0.005%
1.0
0.003%
52.2
0.168%
53.4
0.431%
1.8
0.021%
1.0
0.001%
3.4
0.010%
3.8
0.017%
0.050%
16.2
0.442% 103.4
27.0
0.050%
47.4
0.295%
33.8
0.044%
29.0
0.069%
1.4
0.012%
1.0
0.001%
2.6
0.008%
2.6
0.006%

28.2
11.2
65.8
93.0
164.6
72.2
225.2
88.0
542.0
454.6
427.8
218.0
385.6
200.8
783.4
878.4
760.8
912.2
3338.6
2087.8
2045.2
1166.0
1185.6
957.4

100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
60% (0.03%)
100%
100%
100%
100%
100%
100%
100%
100%

Table 4: PC-SOS-SDP results on the instances where both ML and CL constraints are included.

22

Published at https://doi.org/10.1016/j.cor.2022.105958

We directly compare the results of our algorithm to those of CPRBBA, developed by Guns et al.

(2016). CPRBBA is chosen as a benchmark because, to the best of our knowledge, it is the most

recent and most eﬃcient global algorithm for solving MSSC with pairwise constraints. CPRBBA

is run on the same instances and on the same machine used for testing our own algorithm. A time

limit of one hour is set on each instance. Tables 5, 6 and 7 show the results obtained by CPRBBA

on the 4 smallest datasets that we considered in our experiments, i.e., Iris, Wine, Connectionist

and Seeds. We also made an attempt to solve instances consisting of just ML constraints for the

datasets Glass, Heart, Vertebral and Ecoli but none of those instances were solved within our time

limit. Guns et al. (2016) only report results for datasets with about 200 data points or less, therefore

we feel safe in assuming that the CPRBBA approach would fail to ﬁnd the optimal solution on

all the instances that are missing in Tables 5, 6 and 7. In the same tables we compare the results

obtained by CPRBBA with the performance of PC-SOS-SDP. More speciﬁcally, for each dataset

and instance type we report the average resolution time and the success rate of both algorithms.

For CPRBBA we consider success as solving to optimality an instance within one hour of time; for

our algorithm PC-SOS-SDP we deﬁne success as solving the instance to optimality within one hour

of time and within 200 nodes. As can be seen in these tables, despite being impressively eﬃcient at

solving instances of the Iris dataset when ML constraints are involved, CPRBBA does not behave

well on slightly bigger instances: not only it fails to solve any of the Connectionist instances, but its

success rate drastically falls whenever CL constraints are enforced. On the other hand, in the same

time frame our algorithm is able to solve all the considered instances to optimality. It also appears

consistently faster than CPRBBA on almost all the instances, with the only exceptions being

instances of the Iris dataset. Moreover, even in those cases marked as a failure, our branch-and-cut

approach is usually able to provide a very good clustering solution (maybe even the optimal one),

whose quality is guaranteed by the extremely low gap obtained before halting the branch-and-cut

procedure. On the contrary, CPRBBA can not produce a feasible clustering solution before the

algorithm termination, since it works by incrementally adding data points to the constrained MSSC

until the entire dataset is considered. We also stress that the average gap found by PC-SOS-SDP

at the root node is already extremely low, and this excellent performance is due to the eﬀectiveness

of our heuristic procedure, as we can see in the next section, combined with the goodness of the

lower bound.

4.4. Heuristic algorithm results

We test the eﬃciency of our heuristic algorithm IPC-k-means by comparing the quality of
its clustering solutions with the solutions produced by three other heuristic methods for semi-
supervised clustering: COP-k-means, the post-processing approach (PP) proposed by Vrain et al.
(2020) applied to the unconstrained clustering obtained by k-means, and the heuristic BLP KMCC
proposed by Baumann (2020). These heuristic algorithms address the MSSC problem with ML and

CL constraints while ensuring that no user constraints are violated in the ﬁnal clustering partition.

23

Published at https://doi.org/10.1016/j.cor.2022.105958

dataset

constraints

Iris
Iris
Wine
Wine
Connectionist
Connectionist
Seeds
Seeds

50
100
50
100
50
100
50
100

n

150
150
178
178
208
208
210
210

k

3
3
3
3
2
2
3
3

CPRBBA

PC-SOS-SDP

time (s)

success rate

time (s)

success rate

0.20
0.02
275.58
39.02
3600.00
3600.00
174.08
35.76

100%
100%
100%
100%
0%
0%
100%
100%

16.6
5.8
101.8
31.6
160.8
75.6
84.4
19.2

100%
100%
100%
100%
100%
100%
100%
100%

Table 5: Comparison between CPRBBA and PC-SOS-SDP on instances with ML constraints only.

dataset

constraints

Iris
Iris
Wine
Wine
Connectionist
Connectionist
Seeds
Seeds

50
100
50
100
50
100
50
100

n

150
150
178
178
208
208
210
210

k

3
3
3
3
2
2
3
3

CPRBBA

PC-SOS-SDP

time (s)

success rate

time (s)

success rate

750.74
724.54
3600.00
3600.00
3600.00
3600.00
3008.29
3600.00

80%
80%
0%
0%
0%
0%
20%
0%

78.4
58.6
85.2
216.6
165.6
102.6
227.2
225.8

100%
100%
100%
100%
100%
100%
100%
100%

Table 6: Comparison between CPRBBA and PC-SOS-SDP on instances with CL constraints only.

dataset

constraints

Iris
Iris
Wine
Wine
Connectionist
Connectionist
Seeds
Seeds

25
50
25
50
25
50
25
50

n

150
150
178
178
208
208
210
210

k

3
3
3
3
2
2
3
3

CPRBBA

PC-SOS-SDP

time (s)

success rate

time (s)

success rate

0.30
3.32
1642.63
3136.97
3600.00
3600.00
782.40
458.63

100%
100%
60%
20%
0%
0%
80%
100%

28.2
11.2
65.8
93.0
225.2
88.0
164.6
72.2

100%
100%
100%
100%
100%
100%
100%
100%

Table 7: Comparison between CPRBBA and PC-SOS-SDP on instances with both ML and CL constraints.

24

Published at https://doi.org/10.1016/j.cor.2022.105958

dataset

Iris
Wine
Seeds
Connectionist
Glass
Heart
Vertebral
Accent
Ecoli
ECG5000
Computers
Gene

IPC-k-means
opt
best

BLPKM++

PP

best

opt

best

opt

COP-k-means
opt

fail

best

96.7% 96.7% 60%
60% 16.7% 16.7% 3.3% 3.3% 6.7%
100% 100% 56.7% 56.7% 16.7% 16.7% 3.3% 3.3% 3.3%
30% 16.7% 16.7% 20%
96.7% 96.7% 70% 66.7% 30%
96.7% 93.3% 26.7% 26.7% 6.7% 6.7%
63.3%
0%
0%
100% 43.3%
0%
0%
10%
100% 96.7% 10%
0%
96.7% 96.7% 73.3% 73.3%
0%
96.7% 33.3%
0%
100% 40%
0%
100% 63.3%
100% 96.7% 16.7% 13.3%
0%
96.7% 96.7% 50% 46.7% 43.3% 40% 46.7% 46.7%

0%
0%
0%
3.3%
0%
0%
0%

0%
0%
0%
0%
0%
0%
0%
0%

0%
0%
0%
0%
0%
0%
0%
0%

63.3%
66.7%

0%
0%
0%

0%
0%
0%

26.7%

Table 8: Rates of being the best among the considered heuristic algorithms (on the left) and of ﬁnding the optimal
clustering (on the right); for COP-k-means the frequency of not being able to produce any solution at all is reported
in the rightmost column

All the competitor heuristic methods are executed with 100 diﬀerent initializations, all built
with the k-means++ algorithm (Vassilvitskii & Arthur, 2006), whereas IPC-k-means is run only
once since it comprises a sophisticated SDP-based initialization for determining an initial set of

centroids.

Table 8 reports the frequency of each algorithm succeeding in ﬁnding the best solution among

the competitors and the frequency of each algorithm succeeding in ﬁnding the solution obtained by

PC-SOS-SDP on the 30 semi-supervised clustering instances of each dataset. Note that, when the
constraints set is feasible, both IPC-k-means and the post-processing procedure are guaranteed to
return a valid clustering solution, while COP-k-means can potentially fail in ﬁnding any solution
at all. The rate at which this event occurred in our experiments is reported in the rightmost COP-
k-means column. The results presented in Table 8 show that IPC-k-means is almost unbeaten
from any of the other heuristic algorithms, and it also succeeds in ﬁnding the optimal clustering

solution on 79% of our instances.

The boxplot in Figure 4 shows a comparison among the relative gaps obtained by each of the

four algorithms and the optimal solution on all the 30 instances of each dataset.

The exact computational time required for running IPC-k-means on each instance, including
the time spent for the SDP-based initialization, can be found in Appendix B since it coincides

with the running time required for processing the root node in PC-SOS-SDP. Naturally, relying on

the resolution of an SDP problem, our heuristic does not scale well with the dataset cardinality.

However, we have shown that it is able to produce better solutions in comparison to the other

heuristic algorithms that we considered. Considering the scalability of SDPNAL+ and Gurobi, this

25

Published at https://doi.org/10.1016/j.cor.2022.105958

Figure 4: Relative gaps between the heuristic solutions and the optimal clustering.

heuristic can still be eﬃciently used on instances consisting of up to a few thousands data points.

Indeed, solving the SDP (7) to moderate precision with data points ranging from 1000 to 2000 and

a few thousands of inequalities takes between 10 and 30 minutes (Yang et al., 2015), whereas the

time needed by Gurobi for the assignment problem (12) is much smaller (Gurobi Optimization,

2021). Clearly, the exact algorithm can not be applied on larger instances since a very large number

of SDPs may be needed to be solved, making the approach impractical.

4.5. Clustering evaluation

Evaluating the quality of the solution produced by clustering algorithms is a diﬃcult problem

in general, since there is no “gold standard” solution in cluster analysis. In addition to the MSSC

objective function, we consider two widely used metrics for cluster evaluation. Following the semi-

supervised literature on this subject, we measure the agreement between the produced clustering

solution and the true solution which is obtained on the basis of given class labels. To this end, the

Adjusted Rand Index (ARI) (Hubert & Arabie, 1985) and the Adjusted Mutual Information (AMI)

(Vinh et al., 2010) are recommended as the indices of choice for measuring agreement between two
partitions in semi-supervised clustering analysis. Denote by U = {C1, . . . , Ck} the ground truth
class assignment and by V = { ˆC1, . . . , ˆCk} a clustering of n data points. The overlap between
partitions U and V can be summarized in a contingency table where each entry cij represents the
26

Published at https://doi.org/10.1016/j.cor.2022.105958

number of data points that are common to clusters Ci and ˆCj, i.e., cij = |Ci ∩ ˆCj|. Furthermore,
let ai and bj be the partial sums of the contingency table, i.e., ai = Pk
i=1 cij.
The ARI is computed as

j=1 cij and bj = Pk

ARI(U, V ) =

Pk

i=1

Pk

(cid:2) Pk

i=1

1
2

(cid:0)ai
2

j=1
(cid:1) Pk

(cid:0)cij
2

j=1

(cid:0)ai
2

(cid:1) − (cid:2) Pk
(cid:0)bj
2

i=1
(cid:1)(cid:3) − (cid:2) Pk

i=1

(cid:1) Pk

j=1
(cid:1) Pk

(cid:0)bj
2

j=1

(cid:1)
(cid:1)(cid:3)/(cid:0)n
2
(cid:0)bj
(cid:1)(cid:3)/(cid:0)n
2
2

(cid:1) .

(cid:0)ai
2

It takes a value of 1 when the two partitions are identical, the value 0 when the index equals its

expected value and it can yield negative values if the index is less than the expected index. The

AMI is an information theoretic measure that quantiﬁes the information shared by the partitions.

It is computed as

AM I(U, V ) =

M I(U, V ) − E[M I(U, V )]
max(H(U ), H(V )) − E[M I(U, V )] ,

where M I(U, V ) and E[M I(U, V )] are the mutual information and the expected mutual information
between U and V , and H(U ), H(V ) are the entropy associated with U and V , respectively. These
quantities are deﬁned as

M I(U, V ) =

k
X

k
X

i=1

j=1

(cid:19)

cij
n

log

(cid:18) ncij
aibj

, H(U ) = −

k
X

i=1

ai
n

(cid:19)

log

(cid:18) ai
n

, H(V ) = −

k
X

j=1

bj
n

log

(cid:19)

,

(cid:18) bj
n

E[M I(U, V )] =

k
X

k
X

min(ai,bj )
X

i=1

j=1

nij =max(1,ai+bj −n)

cij
n

log

(cid:18) ncij
aibj

(cid:19)

ai!bj!(n − ai)!(n − bj)!
n!cij!(ai − cij)!(bj − cij)!(n − ai − bj + cij)! .

The AMI takes a value of 1 when the two partitions perfectly match and 0 when the mutual

information between the two partitions equals its expected value. Note that the AMI can take

negative values since random partitions (independent labellings) have an expected AMI around 0

on average.

In this section, we address the following research questions:

1. Does the cluster quality increase when ML and/or CL constraints are added?

2. Is there an advantage in applying an exact algorithm, i.e., does a more accurate solution of

the semi-supervised MSSC correspond to a higher cluster quality?

In Tables 9, 10 and 11 for each dataset we consider the instances where only ML, only CL and

both ML and CL constraints are included, respectively. In all these tables, we report:

• violated: the average percentage of pairwise constraints that are violated in the global mini-

mum of the unconstrained MSSC.

• MSSC ARI (AMI): the ARI (AMI) computed on the global solution of the unconstrained

MSSC (by means of the exact algorithm proposed in Piccialli et al. (2022)).

27

Published at https://doi.org/10.1016/j.cor.2022.105958

• PP ARI (AMI): the ARI (AMI) computed on the feasible clustering produced by the heuristic

algorithm in Vrain et al. (2020).

• PC-SOS-SDP ARI (AMI): the ARI (AMI) computed on the solution produced by the exact

algorithm proposed in this paper.

Looking at Tables 9-11, it emerges that there is always an increase in both indices (ARI and AMI)

when any pairwise constraint is added. This observation holds even if the solution is not optimal

(the average ARI and AMI of the PP solution are already higher than the ones of the unconstrained

MSSC). However, when the exact solver ﬁnds a better solution, there is a further improvement in

the indices. In some cases (see, for example, the Heart dataset), the starting value of the ARI/AMI

indices is close to zero, meaning that the unconstrained solution does not overlap with the ground

truth. For these examples, the indices increase when ML and CL are included, but they remain

low. When this is the case, a kernel-based clustering (see Filippone et al. (2008)) may be a vi-

able approach. In some cases, there is already a very good agreement between the unconstrained

solution and the semi-supervised one (see, for example, the Gene dataset). Also, in this case,

the indices increase and get very close to 1 when constraints are added. Note that there is some

correlation between the percentage of violated constraints and the quality of the unconstrained

solution. Indeed, if the agreement between the ground truth and the unconstrained solution is low,

most of the randomly generated constraints will be violated. When there is a good agreement, it

becomes harder to randomly generate constraints violated by the unconstrained solution. There-

fore, the constraints become less informative. There are some intermediate cases, where adding

the constraints leads to a signiﬁcant improvement in the solution quality: see, for example, the

datasets Wine, Ecoli, and ECG5000. Not surprisingly, the most informative constraints are the

must link, allowing a higher improvement in both ARI and AMI. Overall, these results conﬁrm the

importance of using background knowledge, if available, since they show the increased quality of

the obtained clustering. Furthermore, ﬁnding the globally optimal constrained clustering solution

always translates into better clustering results.

5. Conclusions

In this paper, we have proposed PC-SOS-SDP: a branch-and-cut algorithm for semi-supervised

MSSC with pairwise constraints. For the lower bound procedure, we use an SDP relaxation that

exploits ML constraints to reduce the size of the problem. In addition, we add three types of valid

inequalities in a cutting plane fashion to generate tight bounds. We have also developed a heuristic
named IPC-k-means that is reminiscent of the popular k-means algorithm. When the problem is
feasible this heuristic returns a good quality clustering and an upper bound on the optimality gap

of the provided solution; otherwise it returns a certiﬁcate of infeasibility. Numerical results of the

overall branch-and-cut algorithm impressively exhibit the eﬃciency of PC-SOS-SDP: we can solve

28

Published at https://doi.org/10.1016/j.cor.2022.105958

dataset

violated MSSC

PP

PC-SOS-SDP MSSC

PP

PC-SOS-SDP

ARI

AMI

Iris
Wine
Connectionist
Seeds
Glass
Heart
Vertebral
Accent
Ecoli
ECG5000
Computers
Gene

0.71
16.1%
41.3%
0.361
0.005
51.6%
0.707
18.4%
57.7%
0.161
46.0% -0.004
42.1%
0.105
0.032
76.7%
0.426
57.9%
0.465
50.0%
46.6% -0.001
0.971
1.3%

0.874
0.519
0.051
0.822
0.211
0.015
0.067
0.211
0.569
0.582
0.025
0.993

0.891
0.528
0.065
0.847
0.269
0.464
0.381
0.291
0.731
0.611
0.038
0.998

0.741
0.413
0.003
0.682
0.294
-0.003
0.253
0.162
0.555
0.51
-0.001
0.962

0.871
0.538
0.037
0.790
0.329
0.003
0.233
0.237
0.671
0.576
0.019
0.987

0.886
0.563
0.056
0.810
0.394
0.350
0.389
0.350
0.736
0.603
0.035
0.993

Table 9: Clustering evaluation metrics computed on solutions of instances with only ML constraints.

dataset

violated MSSC

PP

PC-SOS-SDP MSSC

PP

PC-SOS-SDP

ARI

AMI

Iris
Wine
Connectionist
Seeds
Glass
Heart
Vertebral
Accent
Ecoli
ECG5000
Computers
Gene

0.71
9.7%
20.8%
0.361
0.005
51.6%
0.707
10.7%
24.0%
0.161
53.4% -0.004
47.0%
0.105
0.032
18.7%
3.4%
0.426
5.2%
0.465
52.3% -0.001
0.971
0.5%

0.816
0.409
0.056
0.792
0.169
0.022
0.273
0.028
0.449
0.459
0.013
0.989

0.836
0.429
0.077
0.812
0.205
0.463
0.375
0.057
0.452
0.512
0.041
0.995

0.741
0.413
0.003
0.682
0.294
-0.003
0.253
0.162
0.555
0.51
-0.001
0.962

0.815
0.436
0.041
0.764
0.290
0.015
0.279
0.180
0.592
0.522
0.010
0.981

0.829
0.454
0.061
0.779
0.322
0.376
0.345
0.203
0.613
0.553
0.037
0.986

Table 10: Clustering evaluation metrics computed on solutions of instances with only CL constraints.

real-world instances up to 800 data points with diﬀerent combinations of ML and CL constraints

and with a huge number of features. To the best of our knowledge, no other exact methods can

handle generic instances of that size. PC-SOS-SDP can deal with other constrained versions of

MSSC like those with diameter and split constraints (Davidson & Ravi, 2005). These constraints

can be represented as a disjunction and conjunction of ML and CL constraints, thus making their

implementation easy in our specialized solver. As future work, we plan to extend PC-SOS-SDP

for semi-supervised MSSC with cluster-level constraints concerning the cardinality of the clusters.

29

Published at https://doi.org/10.1016/j.cor.2022.105958

dataset

violated MSSC

PP

PC-SOS-SDP MSSC

PP

PC-SOS-SDP

ARI

AMI

Iris
Wine
Connectionist
Seeds
Glass
Heart
Vertebral
Accent
Ecoli
ECG5000
Computers
Gene

0.71
14.9%
32.8%
0.361
0.005
47.2%
0.707
16.0%
40.9%
0.161
50.4% -0.004
44.2%
0.105
0.032
46.3%
0.426
31.9%
0.465
27.7%
49.8% -0.001
0.971
0.9%

0.848
0.457
0.042
0.820
0.195
0.005
0.229
0.064
0.463
0.484
0.015
0.991

0.873
0.491
0.088
0.832
0.223
0.447
0.355
0.071
0.554
0.489
0.035
0.997

0.741
0.413
0.003
0.682
0.294
-0.003
0.253
0.162
0.555
0.51
-0.001
0.962

0.844
0.484
0.031
0.789
0.320
0.005
0.262
0.192
0.619
0.534
0.011
0.985

0.863
0.512
0.070
0.798
0.345
0.352
0.332
0.234
0.667
0.555
0.032
0.991

Table 11: Clustering evaluation metrics computed on solutions of instances with both ML and CL constraints.

Appendix A. Proof of Theorem 3.2

Proof. We introduce dual variables y ∈ Rs+1, µ ∈ R|CL|, ˜V ∈ S s and we write the dual of Problem
(7) (omitting the constant part of the objective function) as

max

s
X

i=1

yi + kys+1

s.t.

− T sW (T s)> −

s
X

i=1

S ∈ S s

+, V ≥ 0.

yiEs

i − ys+1Diag(es) − X

µτ (i,j)Eij − S = V,

(i,j)∈CL

(A.1a)

(A.1b)

(A.1c)

Given Z?, ﬁrst observe that the dual objective function can be written as

s
X

i=1

yi + kys+1 + X

µτ (i,j)hEij, Z?i

(i,j)∈CL

30

Published at https://doi.org/10.1016/j.cor.2022.105958

since for any feasibile Z of the primal problem, Zij = 0 for (i, j) ∈ CL. In order to show that
p? ≥ lb, consider the diﬀerence between the primal and the dual objective as

h−T sW (T s)>, Z?i −

(cid:16) s
X

i=1

˜yi + k ˜ys+1 + X

˜µτ (i,j)hEij, Z?i

(cid:17)

(i,j)∈CL

= h−T sW (T s)>, Z?i −

s
X

i=1

˜yihEs

i , Z?i − hDiag(es), Z?i˜ys+1 − X

˜µτ (i,j)hEij, Z?i

(i,j)∈CL

= h−T sW (T s)> −

= h ˜V + ˜S, Z?i.

s
X

i=1

˜yiEs

i − Diag(es)˜ys+1 − X

˜µτ (i,j)Eij, Z?i

(i,j)∈CL

Using Lemma 3.1, we obtain

h ˜V , Z?i + h ˜S, Z?i ≥ h ˜V , Z?i + ¯z X

λi( ˜S) ≥ ¯z X

λi( ˜S),

i : λi( ˜S)<0

i : λi( ˜S)<0

where the last inequality holds because ˜V is nonnegative.

31

Published at https://doi.org/10.1016/j.cor.2022.105958

Appendix B. PC-SOS-SDP Detailed Numerical Results

Here we report a more detailed description of the resolution of each of the 360 instances gen-

erated for our computational experiments. The instances are divided by datasets and presented in

Tables B.12 - B.23. For each instance we report the following data:

• ml: number of must-link constraints;

• cl: number of cannot-link constraints;

• seed: seed used in the random generation process;

• size: number of separate data points on the root node (it can diﬀer from the dataset size in

the presence of must-link constraints);

• f : the optimal solution value;

• cp0: the number of cutting plane iterations performed on the root node for the lower bound

computation;

• ineq0: the number of inequalities of the last SDP solved at the root in the cutting-plane

procedure;

• gap0: the relative gap between the root lower and upper bound calculated as (U B −LB)/U B;

• time0: running time on the root node, expressed in seconds;

• nodes: number of branch-and-bound nodes explored;

• time: total running time, expressed in seconds;

32

Published at https://doi.org/10.1016/j.cor.2022.105958

ml

50
50
50
50
50
25
25
25
25
25
0
0
0
0
0
100
100
100
100
100
50
50
50
50
50
0
0
0
0
0

cl

seed

size

f

cp0

ineq0

gap0

time0(s)

nodes

time(s)

0
0
0
0
0
25
25
25
25
25
50
50
50
50
50
0
0
0
0
0
50
50
50
50
50
100
100
100
100
100

0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4

101
100
100
100
101
125
125
125
125
125
150
150
150
150
150
57
55
54
59
55
100
100
100
100
100
150
150
150
150
150

83.6299
85.1940
87.6941
84.7522
83.5486
79.9578
86.7623
82.0447
81.4878
81.5452
84.1465
85.6707
80.2087
84.8764
85.0928
85.6052
87.9862
87.9577
84.8172
87.0724
84.5632
83.3805
86.8824
84.9081
88.1519
81.5149
83.2654
86.1146
87.2248
86.6330

2
1
1
1
2
2
2
2
3
2
3
5
4
2
2
1
1
2
1
1
2
1
2
1
1
3
2
3
2
2

6198
5356
5419
5365
5301
5742
6149
5854
7231
5717
9887
3779
6353
6362
5686
3732
5093
2709
5136
3395
5311
5352
5526
5299
5237
6156
6017
8540
6391
7546

0.0004%
0.0010%
0.0007%
0.0060%
0.0100%
0.0013%
0.0003%
0.0284%
0.0013%
0.0363%
0.0047%
0.0172%
0.0173%
0.0012%
0.0103%
0.0011%
0.0007%
0.0026%
0.0004%
0.0001%
0.0132%
0.0012%
0.0052%
0.0003%
0.0008%
0.0031%
0.0716%
0.0248%
0.3634%
0.0495%

Table B.12: Dataset Iris, k=3.

9
5
8
5
25
21
18
12
35
8
77
93
53
29
26
3
4
3
3
3
11
3
13
3
4
43
26
50
40
25

1
1
1
1
5
1
1
3
1
3
1
5
3
1
1
1
1
1
1
1
3
1
1
1
1
1
3
3
3
3

12
7
11
8
45
23
21
35
38
24
80
178
74
31
29
6
6
5
6
6
23
6
15
5
7
45
46
76
75
51

33

Published at https://doi.org/10.1016/j.cor.2022.105958

ml

50
50
50
50
50
25
25
25
25
25
0
0
0
0
0
100
100
100
100
100
50
50
50
50
50
0
0
0
0
0

cl

seed

size

f

cp0

ineq0

gap0

time0(s)

nodes

time(s)

0
0
0
0
0
25
25
25
25
25
50
50
50
50
50
0
0
0
0
0
50
50
50
50
50
100
100
100
100
100

0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4

128
128
128
128
129
153
153
153
153
153
178
178
178
178
178
80
80
83
82
82
129
129
128
128
128
178
178
178
178
178

3.62636e+06
3.64084e+06
3.33862e+06
3.30173e+06
3.51784e+06
3.23052e+06
3.13854e+06
3.28121e+06
3.14043e+06
2.95302e+06
2.60234e+06
2.86950e+06
2.63343e+06
2.97316e+06
2.73127e+06
4.19548e+06
4.43060e+06
4.30206e+06
4.04948e+06
4.02972e+06
3.56553e+06
3.75127e+06
3.74904e+06
4.03569e+06
4.01734e+06
3.09635e+06
3.09061e+06
3.12229e+06
3.33813e+06
2.90243e+06

4
6
3
6
5
3
7
3
3
4
3
4
3
4
5
1
2
4
3
1
5
5
2
3
3
4
6
5
5
4

4286
8179
9597
8268
10713
6765
6174
3223
6403
4415
7192
6385
7186
6263
6259
5671
10046
8660
7119
5563
5844
8668
6734
8517
6883
8677
6676
6284
6330
6420

0.0117%
0.0074%
0.0041%
0.0331%
0.0233%
0.0299%
0.0001%
0.0041%
0.0235%
0.0209%
0.0142%
0.0014%
0.1571%
0.0028%
0.0174%
0.0099%
0.0018%
0.0334%
0.0007%
0.0004%
0.0775%
0.0264%
0.0021%
0.0025%
0.0001%
0.0125%
0.0190%
0.0393%
0.2200%
0.0221%

Table B.13: Dataset Wine, k=3.

49
138
48
95
117
40
98
56
48
57
42
81
54
87
129
4
8
86
22
7
128
84
21
59
53
86
156
142
101
92

1
1
1
3
3
3
1
1
1
1
1
1
3
1
1
1
1
3
1
1
3
5
1
1
1
1
3
9
11
7

51
141
50
118
149
61
100
58
50
60
44
83
78
89
132
6
10
108
24
10
151
174
23
61
56
89
196
270
275
253

34

Published at https://doi.org/10.1016/j.cor.2022.105958

ml

50
50
50
50
50
25
25
25
25
25
0
0
0
0
0
100
100
100
100
100
50
50
50
50
50
0
0
0
0
0

cl

seed

size

f

cp0

ineq0

gap0

time0(s)

nodes

time(s)

0
0
0
0
0
25
25
25
25
25
50
50
50
50
50
0
0
0
0
0
50
50
50
50
50
100
100
100
100
100

0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4

158
159
158
158
158
176
181
177
177
177
189
189
182
190
183
108
108
109
110
110
135
139
144
141
139
148
139
133
137
140

296.063
301.857
299.851
304.610
304.781
298.022
302.256
305.470
305.012
300.234
302.604
299.810
301.590
300.461
308.610
331.031
327.204
322.181
326.708
331.773
329.379
330.708
324.190
329.774
322.186
329.864
325.496
316.950
331.657
332.885

4
6
5
5
6
6
5
10
5
7
5
5
8
5
6
5
4
6
6
5
5
6
4
3
6
6
4
4
3
4

5703
5995
6085
5918
6348
5142
6001
5965
6078
6190
6147
6438
5965
6414
5243
5805
5618
6458
5614
5716
1932
3704
5820
5637
3629
4583
7230
5555
5661
5672

0.0050%
0.0010%
0.0062%
0.0040%
0.0066%
0.0053%
0.0060%
0.0023%
0.0039%
0.0055%
0.0003%
0.0006%
0.0062%
0.0002%
0.0045%
0.0131%
0.0059%
0.0015%
0.0161%
0.0009%
0.0013%
0.0044%
0.0002%
0.0041%
0.0055%
0.0049%
0.0018%
0.0069%
0.0001%
0.0156%

294
113
180
100
104
113
133
352
138
377
241
110
192
118
154
49
49
92
71
52
99
117
84
42
86
137
108
61
61
74

Table B.14: Dataset Connectionist, k=2.

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
1
1
3
1
1
1
1
1
1
1
1
1
1
3

297
116
182
102
107
116
135
355
140
380
243
113
195
120
157
68
52
95
108
55
101
119
86
45
89
139
110
64
64
136

35

Published at https://doi.org/10.1016/j.cor.2022.105958

ml

50
50
50
50
50
25
25
25
25
25
0
0
0
0
0
100
100
100
100
100
50
50
50
50
50
0
0
0
0
0

cl

seed

size

f

cp0

ineq0

gap0

time0(s)

nodes

time(s)

0
0
0
0
0
25
25
25
25
25
50
50
50
50
50
0
0
0
0
0
50
50
50
50
50
100
100
100
100
100

0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4

160
160
160
160
160
185
185
185
185
185
210
210
210
210
210
112
110
110
112
114
160
160
160
160
160
210
210
210
210
210

620.233
617.303
619.150
624.169
616.667
616.182
596.609
617.730
600.371
608.407
601.960
626.553
605.245
604.747
603.729
636.851
632.934
644.974
651.855
652.323
634.578
641.830
634.669
641.844
635.987
616.050
611.600
624.302
656.197
632.656

3
2
4
4
2
9
3
4
4
5
4
8
6
5
3
2
2
1
1
2
5
2
2
2
2
4
4
4
7
6

5623
5834
5861
5925
5903
5453
5923
9109
5969
5661
6473
3577
6274
5972
7102
5796
5537
5299
5422
4820
7792
6036
5690
10555
5840
6241
5732
6436
6763
5856

0.0005%
0.0041%
0.0193%
0.0352%
0.0989%
0.0408%
0.0540%
0.0492%
0.0067%
0.0194%
0.1434%
0.0119%
0.0498%
0.0620%
0.0032%
0.0012%
0.0385%
0.0003%
0.0045%
0.0004%
0.0538%
0.1153%
0.0317%
0.0004%
0.0069%
0.1831%
0.0731%
0.1108%
0.0917%
0.0263%

Table B.15: Dataset Seeds, k=3.

47
29
61
64
20
160
46
71
63
83
76
152
104
103
100
17
15
8
12
10
68
33
28
19
25
73
73
89
138
130

1
1
5
13
3
11
5
5
1
7
15
3
11
15
1
1
5
1
1
1
7
7
3
1
1
33
7
3
9
3

49
32
102
194
45
311
119
161
66
166
255
186
380
213
102
19
39
11
15
12
146
108
57
22
28
206
215
131
391
186

36

Published at https://doi.org/10.1016/j.cor.2022.105958

ml

50
50
50
50
50
25
25
25
25
25
0
0
0
0
0
100
100
100
100
100
50
50
50
50
50
0
0
0
0
0

cl

seed

size

f

cp0

ineq0

gap0

time0(s)

nodes

time(s)

0
0
0
0
0
25
25
25
25
25
50
50
50
50
50
0
0
0
0
0
50
50
50
50
50
100
100
100
100
100

0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4

165
164
164
165
164
189
189
189
189
189
214
214
214
214
214
120
115
119
116
119
165
165
164
164
165
214
214
214
214
214

86.1435
94.1670
98.5834
103.6630
100.8140
85.4317
85.0901
82.9471
79.0361
87.6976
76.2862
75.4997
76.0591
75.8723
76.8616
105.7710
109.2680
113.0420
116.8250
107.3600
100.7680
95.7760
98.9496
100.3210
101.1380
76.7631
77.3701
78.1937
76.7243
78.8200

4
7
8
7
8
11
8
6
9
13
12
11
11
13
11
5
4
5
5
4
9
8
10
6
10
8
10
8
11
7

5787
7296
3789
6280
6767
2671
6455
6609
2644
2510
3211
2942
3595
6813
5588
4089
6000
6618
6330
6296
3672
4572
4906
6059
2946
2620
3643
6346
3811
6495

0.0097%
0.0780%
0.0212%
0.0171%
0.0633%
0.3518%
0.0827%
0.0004%
0.1926%
0.2106%
0.1659%
0.0820%
0.0787%
0.1417%
0.0500%
0.0518%
0.0074%
0.0256%
0.0805%
0.1300%
0.2211%
0.7826%
0.6066%
0.2626%
0.2828%
0.1253%
0.2124%
0.0062%
0.1442%
0.0522%

Table B.16: Dataset Glass, k=6.

104
121
125
176
196
162
192
157
162
178
222
237
218
301
204
100
255
307
237
70
112
130
149
113
91
178
230
159
198
168

1
23
3
3
5
47
21
1
141
51
115
13
11
15
9
5
1
3
3
11
73
85
11
51
47
11
81
1
21
3

107
377
148
208
331
497
647
159
761
646
720
416
455
731
375
219
258
403
339
343
529
555
301
499
389
328
773
162
527
242

37

Published at https://doi.org/10.1016/j.cor.2022.105958

ml

100
100
100
100
100
50
50
50
50
50
0
0
0
0
0
150
150
150
150
150
75
75
75
75
75
0
0
0
0
0

cl

seed

size

f

cp0

ineq0

gap0

time0(s)

nodes

time(s)

0
0
0
0
0
50
50
50
50
50
100
100
100
100
100
0
0
0
0
0
75
75
75
75
75
150
150
150
150
150

0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4

199
199
199
199
199
236
231
225
232
232
241
240
238
238
241
153
152
154
155
150
188
191
187
194
187
173
190
183
182
184

3336.54
3352.49
3337.51
3331.70
3319.86
3333.49
3349.22
3332.96
3328.87
3342.07
3341.28
3338.45
3338.48
3354.66
3322.84
3364.21
3347.59
3335.71
3356.64
3362.72
3344.25
3353.23
3366.68
3356.26
3361.72
3369.50
3357.44
3365.68
3356.13
3357.68

8
8
9
8
9
7
10
7
9
8
12
7
10
8
7
5
5
6
4
5
4
6
5
5
4
5
7
5
5
5

6448
5856
3019
6019
6842
6901
8046
6520
6737
8136
6609
7820
4485
7571
7114
6084
3358
5383
5802
6103
5664
6434
5425
6524
5998
2457
3630
3162
5729
6063

0.0002%
0.0000%
0.0002%
0.0003%
0.0002%
0.0001%
0.0002%
0.1026%
0.0002%
0.0002%
0.0000%
0.0004%
0.0002%
0.0002%
0.0002%
0.0000%
0.0000%
0.0003%
0.0000%
0.0001%
0.0029%
0.0000%
0.0002%
0.0001%
0.0002%
0.0002%
0.0002%
0.0001%
0.0010%
0.0000%

Table B.17: Dataset Heart, k=2.

181
377
349
390
234
197
336
167
477
333
717
348
756
364
209
162
180
195
70
116
127
186
313
295
155
323
255
269
196
106

1
1
1
1
1
1
1
5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

184
380
352
393
237
199
339
785
480
336
720
351
759
367
212
164
182
198
72
119
130
188
316
298
158
326
258
272
199
108

38

Published at https://doi.org/10.1016/j.cor.2022.105958

ml

100
100
100
100
100
50
50
50
50
50
0
0
0
0
0
150
150
150
150
150
75
75
75
75
75
0
0
0
0
0

cl

seed

size

f

cp0

ineq0

gap0

time0(s)

nodes

time(s)

0
0
0
0
0
50
50
50
50
50
100
100
100
100
100
0
0
0
0
0
75
75
75
75
75
150
150
150
150
150

0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4

212
210
211
210
210
242
240
249
245
244
252
257
252
256
257
161
162
161
163
160
205
198
199
196
198
197
200
197
201
201

552058
563454
538309
567156
533412
540393
524886
531530
539132
547714
548778
544749
538657
532571
528318
605694
584261
575488
596006
592318
580718
583640
577566
594787
584670
568118
554651
562492
576285
565172

15
9
8
5
8
12
5
10
7
7
9
9
8
9
9
8
2
4
4
6
4
7
5
2
4
5
5
7
5
5

9326
10098
11644
8848
14579
6696
7678
8682
8610
6281
7476
7391
8410
7834
4842
8834
7638
7159
8288
9186
10844
8379
7823
7880
9096
9549
7186
7015
9863
7615

0.0174%
0.0229%
0.0189%
0.0096%
0.0215%
0.0006%
0.0011%
0.0048%
0.0327%
0.0106%
0.0087%
0.0049%
0.0100%
0.0020%
0.0131%
0.1775%
0.0120%
0.0149%
0.0080%
0.0395%
0.0142%
0.0204%
0.0200%
0.0066%
0.0214%
0.0233%
0.0149%
0.0606%
0.0222%
0.0300%

744
345
308
411
295
404
152
377
263
287
432
324
322
488
460
325
36
63
61
382
140
234
143
56
171
168
117
194
225
136

Table B.18: Dataset Vertebral, k=2.

3
9
5
1
7
1
1
1
13
1
1
1
1
1
3
13
3
3
1
3
3
3
7
1
5
3
3
29
13
13

974
636
461
413
490
407
155
379
697
290
434
327
325
491
548
609
68
87
63
477
174
274
229
58
269
234
147
609
593
352

39

Published at https://doi.org/10.1016/j.cor.2022.105958

ml

100
100
100
100
100
50
50
50
50
50
0
0
0
0
0
150
150
150
150
150
75
75
75
75
75
0
0
0
0
0

cl

seed

size

f

cp0

ineq0

gap0

time0(s)

nodes

time(s)

0
0
0
0
0
50
50
50
50
50
100
100
100
100
100
0
0
0
0
0
75
75
75
75
75
150
150
150
150
150

0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4

230
229
230
229
229
280
279
279
279
279
329
329
329
329
329
184
192
182
183
189
254
254
254
255
255
329
329
329
329
329

35191.0
36093.7
37363.4
37940.2
35165.5
28695.2
29047.5
29424.3
30652.2
26835.9
18796.9
19050.2
18803.8
18965.5
18808.6
42917.2
41496.5
42197.4
41485.8
40234.0
29027.3
30703.0
33039.1
35238.8
33575.1
19194.0
19127.6
19117.6
18995.9
19181.9

8
11
7
9
8
12
10
10
13
8
10
13
11
11
12
5
6
8
7
8
11
11
10
9
7
15
12
14
12
13

2985
11928
7224
5590
2928
5365
8920
4748
7428
8879
5106
7343
6707
7663
4871
6023
6923
7770
7047
7108
7667
7529
9060
10840
12486
8291
7877
7485
5656
7281

0.0164%
0.2191%
0.0028%
0.0015%
0.0039%
0.0197%
0.0256%
0.0096%
0.1675%
0.0269%
0.0117%
0.1368%
0.0203%
0.1467%
0.0204%
0.8044%
0.0020%
1.0198%
0.0182%
0.0234%
0.2384%
0.1313%
0.0278%
1.8065%
0.0070%
0.2900%
0.0739%
0.0941%
0.0239%
0.0540%

141
297
424
246
161
298
324
298
194
280
327
316
339
274
343
617
269
147
736
402
164
188
230
115
167
281
267
299
321
321

3
9
1
1
1
15
3
1
45
17
5
51
7
200 (0.013%)
7
3
1
9
5
7
200 (0.027%)
51
33
200 (0.032%)
1
125
19
207
15
9

249
766
427
249
164
715
518
301
1622
761
575
1259
800
3008
680
824
272
414
1124
764
1308
740
788
1386
170
1817
1017
2686
698
747

Table B.19: Dataset Accent, k=6.

40

Published at https://doi.org/10.1016/j.cor.2022.105958

ml

100
100
100
100
100
50
50
50
50
50
0
0
0
0
0
150
150
150
150
150
75
75
75
75
75
0
0
0
0
0

cl

seed

size

f

cp0

ineq0

gap0

time0(s)

nodes

time(s)

0
0
0
0
0
50
50
50
50
50
100
100
100
100
100
0
0
0
0
0
75
75
75
75
75
150
150
150
150
150

0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4

236
236
237
236
237
286
286
286
286
286
336
336
336
336
336
189
186
189
194
189
262
261
262
261
261
336
336
336
336
336

16.6586
15.8052
17.2182
17.3390
16.7166
14.8403
14.9577
15.9729
15.1497
15.8739
13.8480
13.9326
13.9852
14.0955
13.9024
16.6943
17.0550
17.6560
16.9603
17.7616
16.1113
17.0244
16.3000
16.0741
16.0650
14.1895
14.1301
14.2708
14.0998
13.9936

6
5
5
9
7
6
8
6
9
8
11
8
9
11
8
7
4
6
5
5
6
7
8
5
8
8
14
10
11
8

8249
6358
5993
5781
6787
6485
7541
7198
8834
7708
8135
7998
8319
9490
6923
13495
6228
8364
7442
6345
7573
7595
6991
8682
7768
8102
5932
9263
10039
7811

0.0137%
0.0259%
0.0799%
0.1500%
0.0277%
0.0978%
0.0412%
0.0111%
0.0602%
0.0381%
0.0640%
0.0887%
0.0435%
0.2282%
0.0971%
0.0112%
0.0074%
0.0083%
0.0343%
0.0141%
0.0166%
0.7033%
0.6547%
0.0416%
0.0594%
0.4973%
0.1828%
0.1760%
0.1347%
0.3591%

146
155
151
254
187
241
237
188
265
186
331
256
294
291
266
693
162
543
315
158
197
149
189
701
213
201
313
348
301
226

Table B.20: Dataset Ecoli, k=8.

5
43
59
35
25
11
17
19
3
85
129
200 (0.024%)
109
200 (0.023%)
83
5
1
1
5
5
17
91
77
3
49
83
29
199
113
159

249
1117
1010
1192
692
633
755
528
426
1462
2591
3054
2029
2962
1524
1184
165
546
552
344
649
1051
1052
779
1030
1844
1008
3432
2263
2612

41

Published at https://doi.org/10.1016/j.cor.2022.105958

ml

150
150
150
150
150
75
75
75
75
75
0
0
0
0
0
250
250
250
250
250
125
125
125
125
125
0
0
0
0
0

cl

seed

size

f

cp0

ineq0

gap0

time0(s)

nodes

time(s)

0
0
0
0
0
75
75
75
75
75
150
150
150
150
150
0
0
0
0
0
125
125
125
125
125
250
250
250
250
250

0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4

350
350
351
350
351
426
425
425
425
425
500
500
500
500
500
252
253
254
255
252
375
375
375
375
375
500
500
500
500
500

13157.1
13063.7
13141.3
13504.3
13302.7
12644.9
12684.0
12880.3
12397.3
12750.0
11615.7
11681.6
11737.5
11690.5
11613.6
14234.3
14096.2
14113.0
13594.2
14297.6
13127.4
13129.9
12656.5
13034.5
13049.9
11791.9
11760.4
11685.1
11758.2
11846.7

10
16
11
18
11
19
22
21
18
21
23
23
26
28
23
8
10
6
6
8
15
20
12
19
16
23
30
19
24
30

9700
8438
7749
8176
7926
9083
9297
9778
5537
8853
5724
10838
10256
8392
6071
9178
7906
6366
8954
7014
3903
5653
7445
5570
4840
10565
6126
9463
8575
10099

0.0105%
0.0068%
0.0230%
0.0125%
0.0154%
0.0377%
0.0457%
0.0732%
0.0384%
0.0238%
0.0089%
0.0624%
0.0271%
0.0275%
0.0332%
0.0501%
1.4892%
0.0317%
0.0104%
0.0387%
0.0152%
0.0728%
0.0031%
0.2095%
0.0437%
0.0957%
0.0025%
0.0318%
0.0036%
0.0266%

655
895
448
771
548
1052
1133
1327
1008
1417
2181
1796
2421
2452
2066
246
483
267
386
424
812
888
1097
882
824
1791
2811
1468
2322
2775

3
1
23
11
7
15
123
3
11
17
1
75
5
200 (0.012%)
35
5
7
11
3
5
5
3
1
39
97
45
1
7
1
59

842
899
1103
1519
1069
2561
5470
2553
2259
3850
2186
7566
4555
15416
10491
427
922
772
542
733
1110
1454
1101
2590
4184
8007
2817
3136
2327
9984

Table B.21: Dataset ECG5000, k=5.

42

Published at https://doi.org/10.1016/j.cor.2022.105958

ml

150
150
150
150
150
75
75
75
75
75
0
0
0
0
0
250
250
250
250
250
125
125
125
125
125
0
0
0
0
0

cl

seed

size

f

cp0

ineq0

gap0

time0(s)

nodes

time(s)

0
0
0
0
0
75
75
75
75
75
150
150
150
150
150
0
0
0
0
0
125
125
125
125
125
250
250
250
250
250

0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4

350
350
351
351
350
411
402
405
404
398
425
425
432
427
427
251
253
250
253
253
324
327
318
325
322
318
329
320
322
317

315572
316777
317242
316870
319417
317837
317099
317551
316084
318000
317954
317061
317114
317781
318294
323167
324203
322362
323319
322140
325922
323445
326321
323682
322687
324062
323982
324604
325706
325240

13
16
18
16
25
19
16
16
12
25
18
19
20
20
19
6
10
8
10
7
14
10
14
8
10
10
8
8
11
15

7511
4452
7530
7341
5954
6894
6948
7938
7576
8418
7900
6704
8044
8547
8152
6592
2839
2069
6509
2650
4988
6743
8061
6815
2413
6619
7062
7116
7502
7276

0.0002%
0.0003%
0.0004%
0.0135%
0.0004%
0.0005%
0.0002%
0.0004%
0.0001%
0.0594%
0.0002%
0.0003%
0.0001%
0.0021%
0.0002%
0.0003%
0.0004%
0.0007%
0.0006%
0.0004%
0.0004%
0.0005%
0.0005%
0.0001%
0.0002%
0.0002%
0.0001%
0.0003%
0.0008%
0.0007%

1376
1479
1707
998
2283
1571
2546
1790
1338
2350
1869
2084
2797
2529
2457
187
774
247
384
332
1585
1075
2111
360
684
733
615
514
493
842

Table B.22: Dataset Computers, k=2.

1
1
1
3
1
1
1
1
1
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

1380
1483
1710
1402
2286
1575
2550
1793
1341
2967
1873
2088
2801
2533
2461
190
777
250
387
334
1588
1078
2114
363
687
736
618
517
496
845

43

Published at https://doi.org/10.1016/j.cor.2022.105958

ml

200
200
200
200
200
100
100
100
100
100
0
0
0
0
0
400
400
400
400
400
200
200
200
200
200
0
0
0
0
0

cl

seed

size

f

cp0

ineq0

gap0

time0(s)

nodes

time(s)

0
0
0
0
0
100
100
100
100
100
200
200
200
200
200
0
0
0
0
0
200
200
200
200
200
400
400
400
400
400

0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4

604
601
601
601
601
701
701
701
701
701
801
801
801
801
801
413
419
419
414
421
602
603
602
602
602
801
801
801
801
801

1.78019e+07
1.78183e+07
1.78077e+07
1.78019e+07
1.78082e+07
1.78171e+07
1.78019e+07
1.78132e+07
1.78257e+07
1.78064e+07
1.78022e+07
1.78019e+07
1.78031e+07
1.78027e+07
1.78019e+07
1.78171e+07
1.78067e+07
1.78130e+07
1.78073e+07
1.78125e+07
1.78137e+07
1.78023e+07
1.78026e+07
1.78125e+07
1.78168e+07
1.78064e+07
1.78064e+07
1.78131e+07
1.78068e+07
1.78026e+07

2
1
2
2
2
2
2
2
2
2
2
3
2
2
5
2
2
1
1
3
2
2
2
3
4
4
3
3
2
2

6361
5639
6629
6987
8324
7395
8785
7318
8034
7866
10000
9779
10000
10000
11572
10402
5317
5837
6059
3448
6452
6083
5327
9862
8226
10125
8839
9704
9638
10000

0.0013%
0.0002%
0.0019%
0.0019%
0.0004%
0.0059%
0.0057%
0.0026%
0.0153%
0.0117%
0.0094%
0.0119%
0.0133%
0.0159%
0.0068%
0.0002%
0.0001%
0.0000%
0.0003%
0.0001%
0.0208%
0.0002%
0.0002%
0.0009%
0.0074%
0.0031%
0.0171%
0.0179%
0.0174%
0.0120%

Table B.23: Dataset Gene, k=5.

505
439
516
524
501
660
690
623
718
669
859
918
828
758
1075
345
649
337
311
1091
537
446
519
527
629
1111
907
949
784
760

1
1
1
1
1
1
1
1
7
3
1
3
5
3
1
1
1
1
1
1
9
1
1
1
1
1
3
3
5
3

512
446
522
531
508
670
700
633
2608
1317
873
1764
2407
1567
1089
349
653
341
315
1095
2638
453
526
534
636
1124
1809
1880
2434
1593

44

Published at https://doi.org/10.1016/j.cor.2022.105958

References

Alizadeh, F. (1995). Interior point methods in semideﬁnite programming with applications to combinatorial opti-

mization. SIAM journal on Optimization, 5 , 13–51.

Aloise, D., Deshpande, A., Hansen, P., & Popat, P. (2009). NP-hardness of euclidean sum-of-squares clustering.

Machine Learning, 75 , 245–248. doi:10.1007/s10994-009-5103-0.

Aloise, D., & Hansen, P. (2009). A branch-and-cut SDP-based algorithm for minimum sum-of-squares clustering.

Pesquisa Operacional, 29 , 503–516.

Aloise, D., Hansen, P., & Liberti, L. (2012a). An improved column generation algorithm for minimum sum-of-squares

clustering. Mathematical Programming, 131 , 195–220.

Aloise, D., Hansen, P., & Rocha, C. (2012b). A column generation algorithm for semi-supervised minimum sum-of-

squares clustering. In GLOBAL OPTIMIZATION WORKSHOP 2012 (pp. 19–22).

Babaki, B., Guns, T., & Nijssen, S. (2014). Constrained clustering using column generation.

In International
Conference on AI and OR Techniques in Constriant Programming for Combinatorial Optimization Problems (pp.
438–454). Springer.

Basu, S., Banerjee, A., & Mooney, R. (2004). Active semi-supervision for pairwise constrained clustering. Proceedings

of the SIAM International Conference on Data Mining, . doi:10.1137/1.9781611972740.31.

Basu, S., Davidson, I., & Wagstaﬀ, K. (2008). Constrained clustering: Advances in algorithms, theory, and applica-

tions. CRC Press.

Baumann, P. (2020). A binary linear programming-based k-means algorithm for clustering with must-link and cannot-
link constraints. In 2020 IEEE International Conference on Industrial Engineering and Engineering Management
(IEEM) (pp. 324–328). IEEE.

Bilenko, M., Basu, S., & Mooney, R. J. (2004).

Integrating constraints and metric learning in semi-supervised

clustering. In Proceedings of the twenty-ﬁrst international conference on Machine learning (p. 11).

Borgwardt, S., Brieden, A., & Gritzmann, P. (2014). Geometric clustering for the consolidation of farmland and

woodland. The Mathematical Intelligencer, 36 , 37–44.

Brieden, A., Gritzmann, P., & Klemm, F. (2017). Constrained clustering via diagrams: A uniﬁed theory and its

application to electoral district design. European Journal of Operational Research, 263 , 18–34.

Brusco, M. J. (2006). A repetitive branch-and-bound procedure for minimum within-cluster sums of squares parti-

tioning. Psychometrika, 71 , 347–363.

Celebi, M. E., Kingravi, H. A., & Vela, P. A. (2013). A comparative study of eﬃcient initialization methods for the

k-means clustering algorithm. Expert systems with applications, 40 , 200–210.

Dau, H. A., Keogh, E., Kamgar, K., Yeh, C.-C. M., Zhu, Y., Gharghabi, S., Ratanamahatana, C. A., Yanping,
Hu, B., Begum, N., Bagnall, A., Mueen, A., & Batista, G. (2018). The UCR Time Series Classiﬁcation Archive.
https://www.cs.ucr.edu/~eamonn/time_series_data_2018/.

Davidson, I., & Ravi, S. (2005). Clustering with constraints: Feasibility issues and the k-means algorithm.

In

Proceedings of the 2005 SIAM international conference on data mining (pp. 138–149). SIAM.

Davidson, I., & Ravi, S. (2007). Intractability and clustering with constraints. In Proceedings of the 24th international

conference on Machine learning (pp. 201–208).

Diehr, G. (1985). Evaluation of a branch and bound algorithm for clustering. SIAM Journal on Scientiﬁc and

Statistical Computing, 6 , 268–284.

Dinler, D., & Tural, M. K. (2016). A survey of constrained clustering. In Unsupervised learning algorithms (pp.

207–235). Springer.

Du Merle, O., Hansen, P., Jaumard, B., & Mladenovic, N. (1999). An interior point algorithm for minimum sum-of-

squares clustering. SIAM Journal on Scientiﬁc Computing, 21 , 1485–1505.

Dua, D., & Graﬀ, C. (2017). UCI machine learning repository. URL: http://archive.ics.uci.edu/ml.

45

Published at https://doi.org/10.1016/j.cor.2022.105958

Duong, K.-C., Vrain, C. et al. (2013). A declarative framework for constrained clustering.

In Joint European

Conference on Machine Learning and Knowledge Discovery in Databases (pp. 419–434). Springer.

Duong, K.-C., Vrain, C. et al. (2015). Constrained minimum sum of squares clustering by constraint programming.
In International Conference on Principles and Practice of Constraint Programming (pp. 557–573). Springer.
Duong, K.-C., Vrain, C. et al. (2017). Constrained clustering by constraint programming. Artiﬁcial Intelligence,

244 , 70–94.

Eckart, C., & Young, G. (1936). The approximation of one matrix by another of lower rank. Psychometrika, 1 ,

211–218.

Filippone, M., Camastra, F., Masulli, F., & Rovetta, S. (2008). A survey of kernel and spectral methods for clustering.

Pattern recognition, 41 , 176–190.

Gambella, C., Ghaddar, B., & Naoum-Sawaya, J. (2021). Optimization problems for machine learning: A survey.

European Journal of Operational Research, 290 , 807–828.

Gançarski, P., Dao, T.-B.-H., Crémilleux, B., Forestier, G., & Lampert, T. (2020). Constrained clustering: Current

and new trends. In A Guided Tour of Artiﬁcial Intelligence Research (pp. 447–484). Springer.

Ganji, M., Bailey, J., & Stuckey, P. J. (2016). Lagrangian constrained clustering. In Proceedings of the 2016 SIAM

International Conference on Data Mining (pp. 288–296). SIAM.

Gnägi, M., & Baumann, P. (2021). A matheuristic for large-scale capacitated clustering. Computers & Operations

Research, 132 , 105304.

González-Almagro, G., Luengo, J., Cano, J.-R., & García, S. (2020). DILS: constrained clustering through dual

iterative local search. Computers & Operations Research, 121 , 104979.

Guns, T., Dao, T.-B.-H., Vrain, C., & Duong, K.-C. (2016). Repetitive branch-and-bound using constraint pro-
gramming for constrained minimum sum-of-squares clustering.
In Proceedings of the Twenty-Second European
Conference on Artiﬁcial Intelligence ECAI’16 (p. 462–470). NLD: IOS Press. URL: https://doi.org/10.3233/
978-1-61499-672-9-462. doi:10.3233/978-1-61499-672-9-462.

Gurobi Optimization, L. (2021). Gurobi optimizer reference manual. URL: http://www.gurobi.com.
Hartigan, J. A., & Wong, M. A. (1979). Algorithm as 136: A k-means clustering algorithm. Journal of the royal

statistical society. series c (applied statistics), 28 , 100–108.

Hu, G., Zhou, S., Guan, J., & Hu, X. (2008). Towards eﬀective document clustering: A constrained k-means based

approach. Information Processing & Management, 44 , 1397–1409.

Huang, H., Cheng, Y., & Zhao, R. (2008). A semi-supervised clustering algorithm based on must-link set.

In

International Conference on Advanced Data Mining and Applications (pp. 492–499). Springer.

Huang, Y., & Mitchell, T. M. (2006). Text clustering with extended user feedback. In Proceedings of the 29th annual

international ACM SIGIR conference on Research and development in information retrieval (pp. 413–420).

Hubert, L., & Arabie, P. (1985). Comparing partitions. Journal of classiﬁcation, 2 , 193–218.
Jain, A. K., Murty, M. N., & Flynn, P. J. (1999). Data clustering: a review. ACM computing surveys (CSUR), 31 ,

264–323.

Jansson, C., Chaykin, D., & Keil, C. (2008). Rigorous error bounds for the optimal value in semideﬁnite programming.

SIAM Journal on Numerical Analysis, 46 , 180–200. doi:10.1137/050622870.

Koontz, W. L. G., Narendra, P. M., & Fukunaga, K. (1975). A branch and bound clustering algorithm. IEEE

Transactions on Computers, 100 , 908–915.

Krislock, N., Malick, J., & Roupin, F. (2016). Computational results of a semideﬁnite branch-and-bound algorithm

for k-cluster. Computers & Operations Research, 66 , 153–159.

Lai, X., Hao, J.-K., Fu, Z.-H., & Yue, D. (2021). Neighborhood decomposition-driven variable neighborhood search

for capacitated clustering. Computers & Operations Research, (p. 105362).

Li, X., Yin, H., Zhou, K., & Zhou, X. (2020). Semi-supervised clustering with deep metric learning and graph

embedding. World Wide Web, 23 . doi:10.1007/s11280-019-00723-8.

46

Published at https://doi.org/10.1016/j.cor.2022.105958

Liberti, L., & Manca, B. (2021). Side-constrained minimum sum-of-squares clustering: mathematical programming

and random projections. Journal of Global Optimization, (pp. 1–36).

Lloyd, S. (1982). Least squares quantization in PCM. IEEE transactions on information theory, 28 , 129–137.
MacQueen, J. et al. (1967). Some methods for classiﬁcation and analysis of multivariate observations. In Proceedings
of the ﬁfth Berkeley symposium on mathematical statistics and probability 14 (pp. 281–297). Oakland, CA, USA.
Maraziotis, I. A. (2012). A semi-supervised fuzzy clustering algorithm applied to gene expression data. Pattern

Recognition, 45 , 637–648.

Pacheco, J. A. (2005). A scatter search approach for the minimum sum-of-squares clustering problem. Computers &

operations research, 32 , 1325–1335.

Pena, J. M., Lozano, J. A., & Larranaga, P. (1999). An empirical comparison of four initialization methods for the

k-means algorithm. Pattern recognition letters, 20 , 1027–1040.

Peng, J., & Wei, Y. (2007). Approximating k-means-type clustering via semideﬁnite programming. SIAM journal

on optimization, 18 , 186–205.

Peng, J., & Xia, Y. (2005). A cutting algorithm for the minimum sum-of-squared error clustering. In Proceedings of

the 2005 SIAM International Conference on Data Mining (pp. 150–160). SIAM.

Pensa, R. G., & Boulicaut, J.-F. (2008). Constrained co-clustering of gene expression data. In Proceedings of the

2008 SIAM International Conference on Data Mining (pp. 25–36). SIAM.

Piccialli, V., Sudoso, A. M., & Wiegele, A. (2022). SOS-SDP: an exact solver for minimum sum-of-squares clustering.

INFORMS Journal on Computing, . doi:10.1287/ijoc.2022.1166.

Rao, M. (1971). Cluster analysis and mathematical programming. Journal of the American statistical association,

66 , 622–626.

Rutayisire, T., Yang, Y., Lin, C., & Zhang, J. (2011). A modiﬁed cop-kmeans algorithm based on sequenced cannot-
link set. In J. Yao, S. Ramanna, G. Wang, & Z. Suraj (Eds.), Rough Sets and Knowledge Technology (pp. 217–225).
Berlin, Heidelberg: Springer Berlin Heidelberg.

Sherali, H. D., & Desai, J. (2005). A global optimization RLT-based approach for solving the hard clustering problem.

Journal of Global Optimization, 32 , 281–306.

Sun, D., Toh, K.-C., & Yang, L. (2015). A convergent 3-block semiproximal alternating direction method of multipliers

for conic programming with 4-type constraints. SIAM journal on Optimization, 25 , 882–915.

Sun, D., Toh, K.-C., Yuan, Y., & Zhao, X.-Y. (2020). SDPNAL+: A matlab software for semideﬁnite programming

with bound constraints (version 1.0). Optimization Methods and Software, 35 , 87–115.

Tan, W., Yang, Y., & Li, T. (2010). An improved COP-kmeans algorithm for solving constraint violation.

In

Computational Intelligence: Foundations and Applications (pp. 690–696). World Scientiﬁc.

Tran, D. H., Babaki, B., Van Daele, D., Leyman, P., & De Causmaecker, P. (2021). Local search for constrained

graph clustering in biological networks. Computers & Operations Research, 132 , 105299.

Vassilvitskii, S., & Arthur, D. (2006). k-means++: The advantages of careful seeding. In Proceedings of the eighteenth

annual ACM-SIAM symposium on Discrete algorithms (pp. 1027–1035).

Vinh, N. X., Epps, J., & Bailey, J. (2010). Information theoretic measures for clusterings comparison: Variants,
properties, normalization and correction for chance. The Journal of Machine Learning Research, 11 , 2837–2854.
Vrain, C., Davidson, I. et al. (2020). Constrained clustering via post-processing. In International Conference on

Discovery Science (pp. 53–67). Springer.

Wagstaﬀ, K., Cardie, C., Rogers, S., Schroedl, S. et al. (2001). Constrained k-means clustering with background

knowledge. In Icml (pp. 577–584). volume 1.

Xia, Y. (2009). A global optimization method for semi-supervised clustering. Data mining and knowledge discovery,

18 , 214–256.

Xiang, S., Nie, F., & Zhang, C. (2008). Learning a mahalanobis distance metric for data clustering and classiﬁcation.

Pattern recognition, 41 , 3600–3612.

47

Published at https://doi.org/10.1016/j.cor.2022.105958

Yang, L., Sun, D., & Toh, K.-C. (2015). SDPNAL+: a majorized semismooth Newton-CG augmented lagrangian
method for semideﬁnite programming with nonnegative constraints. Mathematical Programming Computation, 7 ,
331–366.

Zhang, H., Basu, S., & Davidson, I. (2019). A framework for deep constrained clustering-algorithms and advances.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 57–72). Springer.
Zhu, X., & Goldberg, A. B. (2009). Introduction to semi-supervised learning. Synthesis lectures on artiﬁcial intelli-

gence and machine learning, 3 , 1–130.

48

