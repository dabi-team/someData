Fair Training of Decision Tree Classiﬁers

Francesco Ranzato1, Caterina Urban2, Marco Zanella1
1 Dipartimento di Matematica, University of Padova, Italy
{ranzato, mzanella}@math.unipd.it
2 INRIA and ´Ecole Normale Sup´erieure | Universit´e PSL, France
caterina.urban@inria.fr

1
2
0
2

n
a
J

4

]

G
L
.
s
c
[

1
v
9
0
9
0
0
.
1
0
1
2
:
v
i
X
r
a

Abstract

We study the problem of formally verifying individual fair-
ness of decision tree ensembles, as well as training tree mod-
els which maximize both accuracy and individual fairness. In
our approach, fairness veriﬁcation and fairness-aware train-
ing both rely on a notion of stability of a classiﬁcation model,
which is a variant of standard robustness under input pertur-
bations used in adversarial machine learning. Our veriﬁca-
tion and training methods leverage abstract interpretation, a
well established technique for static program analysis which
is able to automatically infer assertions about stability prop-
erties of decision trees. By relying on a tool for adversarial
training of decision trees, our fairness-aware learning method
has been implemented and experimentally evaluated on the
reference datasets used to assess fairness properties. The ex-
perimental results show that our approach is able to train tree
models exhibiting a high degree of individual fairness w.r.t.
the natural state-of-the-art CART trees and random forests.
Moreover, as a by-product, these fair decision trees turn out
to be signiﬁcantly compact, thus enhancing the interpretabil-
ity of their fairness properties.

1

Introduction

The widespread adoption of data-driven automated decision-
making software with far-reaching societal impact, e.g.,
for credit scoring (Khandani, Kim, and Lo 2010), recidi-
vism prediction (Chouldechova 2017), or hiring tasks (Schu-
mann et al. 2020), raises concerns on the fairness proper-
ties of these tools (Barocas and Selbst 2016). Several fair-
ness veriﬁcation and bias mitigation approaches for ma-
chine learning (ML) systems have been proposed in recent
years, e.g. (Aghaei, Azizi, and Vayanos 2019; Grari et al.
2020; Roh et al. 2020; Ruoss et al. 2020; Urban et al. 2020;
Yurochkin, Bower, and Sun 2020; Zafar et al. 2017) among
the others. However, most works focus on neural networks
(Roh et al. 2020; Ruoss et al. 2020; Urban et al. 2020;
Yurochkin, Bower, and Sun 2020) or on group-based no-
tions of fairness (Grari et al. 2020; Zafar et al. 2017), e.g.,
demographic parity (Dwork et al. 2012) or equalized odds
(Hardt, Price, and Srebro 2016). These notions of group-
based fairness require some form of statistical parity (e.g.

Copyright © 2021, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

between positive outcomes) for members of different pro-
tected groups (e.g. gender or race). On the other hand,
they do not provide guarantees for individuals or other sub-
groups. By contrast, in this paper we focus on individual
fairness (Dwork et al. 2012), intuitively meaning that sim-
ilar individuals in the population receive similar outcomes,
and on decision tree ensembles (Breiman 2001; Friedman
2001), which are commonly used for tabular datasets since
they are easily interpretable ML models with high accuracy
rates.

Contributions. We propose an approach for verifying in-
dividual fairness of decision tree ensembles, as well as train-
ing tree models which maximize both accuracy and fairness.
The approach is based on abstract interpretation (Cousot
and Cousot 1977; Rival and Yi 2020), a well known static
program analysis technique, and builds upon a framework
for training robust decision tree ensembles called Meta-
Silvae (Ranzato and Zanella 2020b), which in turn lever-
ages a veriﬁcation tool for robustness properties of deci-
sion trees (Ranzato and Zanella 2020a). Our approach is
fully parametric on a given underlying abstract domain rep-
resenting input space regions containing similar individuals.
We instantiate it with a product of two abstract domains:
(a) the well-known abstract domain of hyper-rectangles (or
boxes) (Cousot and Cousot 1977), that represents exactly the
standard notion of similarity between individuals based on
the (cid:96)∞ distance metric, and does not lose precision for the
univariate split decision rules of type xi ≤ k; and (b) a spe-
ciﬁc relational abstract domain which accounts for one-hot
encoded categorical features.

Our Fairness-Aware Tree Training method, called FATT,
is designed as an extension of Meta-Silvae (Ranzato and
Zanella 2020b), a learning methodology for ensembles of
decision trees based on a genetic algorithm which is able to
train a decision tree for maximizing both its accuracy and
its robustness to adversarial perturbations. We demonstrate
the effectiveness of FATT in training accurate and fair mod-
els on the standard datasets used in the literature on fair-
ness. Overall, the experimental results show that our fair-
trained models are on average between 35% and 45% more
fair than naturally trained decision tree ensembles at an av-

 
 
 
 
 
 
erage cost of −3.6% of accuracy. Moreover, it turns out that
our tree models are orders of magnitude more compact and
thus more interpretable. Finally, we show how our models
can be used as “hints” for setting the size and shape hyper-
parameters (i.e., maximum depth and minimum number of
samples per leaf) when training standard decision tree mod-
els. As a result, this hint-based strategy is capable to output
models that are about 20% more fair and just about 1% less
accurate than standard models.

Related Work. The most related work to ours is by
Aghaei et al. (Aghaei, Azizi, and Vayanos 2019), Raff et
al. (Raff, Sylvester, and Mills 2018) and Ruoss et al. (Ruoss
et al. 2020).

By relying on the mixed-integer optimization learning ap-
proach by Bertsimas and Dunn (Bertsimas and Dunn 2017),
Aghaei et al. (Aghaei, Azizi, and Vayanos 2019) put forward
a framework for training fair decision trees for classiﬁcation
and regression. The experimental evaluation shows that this
approach mitigates unfairness as modeled by their notions
of disparate impact and disparate treatment at the cost of a
signiﬁcantly higher training computational cost. Their no-
tion of disparate treatment is distance-based and thus akin to
individual fairness with respect to the nearest individuals in
a given dataset (e.g., the k-nearest individuals). In contrast,
we consider individual fairness with respect to the nearest
individuals in the input space (thus, also individuals that are
not necessarily part of a given dataset).

Raff et al. (Raff, Sylvester, and Mills 2018) propose a
regularization-based approach for training fair decision trees
as well as fair random forests. They consider both group
fairness as well as individual fairness with respect to the k-
nearest individuals in a given dataset, similarly to Aghaei
et al. (Aghaei, Azizi, and Vayanos 2019). In their experi-
ments they use a subset of the datasets that we consider in
our evaluation (i.e., the Adult, German, and Health datasets).
Our fair models have higher accuracy than theirs (i.e., be-
tween 2% and 5.5%) for all but one of these datasets (i.e.,
the Health dataset). Interestingly, their models (in particular
those with worse accuracy than ours) often have accuracy on
par with a constant classiﬁer due to the highly unbalanced
label distribution of the datasets (cf. Table 1).

Finally, Ruoss et al. (Ruoss et al. 2020) have proposed
an approach for learning individually fair data representa-
tions and training neural networks (rather than decision tree
ensembles as we do) that satisfy individual fairness with re-
spect to a given similarity notion. We use the same notions
of similarity in our experiments (cf. Section 6.1).

2 Background
Given an input space X ⊆ Rd of numerical vectors and a ﬁ-
nite set of labels L = {y1, . . . , ym}, a classiﬁer is a function
C : X → ℘+(L), where ℘+(L) is the set of nonempty sub-
sets of L, which associates at least one label to every input in
X. A training algorithm takes as input a dataset D ⊆ X × L
and outputs a classiﬁer C : X → ℘+(L) which optimizes
some objective function, such as the Gini index or the infor-
mation gain for decision trees.

Categorical features can be converted into numerical ones
through one-hot encoding, where a single feature with k pos-
sible distinct categories {c1, ..., ck} is replaced by k new bi-
nary features with values in {0, 1}. Then, each value cj of
the original categorical feature is represented by a bit-value
assignment to the new k binary features in which the j-th
feature is set to 1 (and the remaining k − 1 binary features
are set to 0).

Classiﬁers can be evaluated and compared through sev-
eral metrics. Accuracy on a test set is a basic metric: given
a ground truth test set T ⊆ X × L, the accuracy of C on T
is accT (C) (cid:44) |{(x, y) ∈ T | C(x) = {y}}|/|T |. However,
according to a growing belief (Goodfellow, McDaniel, and
Papernot 2018), accuracy is not enough in machine learn-
ing, since robustness to adversarial inputs of a ML classiﬁer
may signiﬁcantly affect its safety and generalization proper-
ties (Carlini and Wagner 2017; Goodfellow, McDaniel, and
Papernot 2018). Given an input perturbation modeled by a
function P : X → ℘(X), a classiﬁer C : X → ℘+(L)
is stable (Ranzato and Zanella 2020a) on the perturbation
P (x) of x ∈ X when C consistently assigns the same la-
bel(s) to every attack ranging in P (x), i.e.,

stable(C, x, P ) (cid:52)⇔ ∀x(cid:48) ∈ P (x) : C(x(cid:48)) = C(x).

When the sample x ∈ X has a ground truth label yx ∈ L,
robustness of C on x boils down to stability stable(C, x, P )
together with correct classiﬁcation C(x) = {yx}.

We consider standard classiﬁcation decision trees com-
monly referred to as CARTs (Classiﬁcation And Re-
gression Trees) (Breiman et al. 1984). A decision tree
t : X → ℘+(L) is deﬁned inductively. A base tree t
is a single leaf λ storing a frequency distribution of la-
bels for the samples of the training dataset, hence λ ∈
[0, 1]|L|, or, equivalently, λ : L → [0, 1]. Some algorith-
mic rule converts this frequency distribution into a set of
labels, typically as arg maxy∈L λ(y). A composite tree t
is γ(split, tl, tr), where split : X → {tt, ﬀ } is a Boolean
split criterion for the internal parent node of its left and
right subtrees tl and tr; thus, for all x ∈ X, t(x) (cid:44)
if split(x) then tl(x) else tr(x). Although split rules can
be of any type, most decision trees employ univariate hard
splits of type split(x) (cid:44) xi ≤ k for some feature i ∈ [1, d]
and threshold k ∈ R.

Tree ensembles, also known as forests, are sets of decision
trees which together contribute to formulate a unique classi-
ﬁcation output. Training algorithms as well as methods for
computing the ﬁnal output label(s) vary among different tree
ensemble models. Random forests (RFs) (Breiman 2001) are
a major instance of tree ensemble where each tree of the
ensemble is trained independently from the other trees on
a random subset of the features. Gradient boosted decision
trees (GBDT) (Friedman 2001) represent a different train-
ing algorithm where an ensemble of trees is incrementally
build by training each new tree on the basis of the data sam-
ples which are mis-classiﬁed by the previous trees. For RFs,
the ﬁnal classiﬁcation output is typically obtained through
a voting mechanism (e.g., majority voting), while GBDTs
are usually trained for binary classiﬁcation problems and use

some binary reduction scheme, such as one-vs-all or one-vs-
one, for multi-class classiﬁcation.

3

Individual Fairness

Dwork et al. (Dwork et al. 2012) deﬁne individual fair-
ness as “the principle that two individuals who are similar
with respect to a particular task should be classiﬁed simi-
larly”. They formalize this notion as a Lipschitz condition
of the classiﬁer, which requires that any two individuals
x, y ∈ X whose distance is δ(x, y) ∈ [0, 1] map to dis-
tributions Dx and Dy, respectively, such that the statistical
distance between Dx and Dy is at most δ(x, y). The intu-
ition is that the output distributions for x and y are indistin-
guishable up to their distance δ(x, y). The distance metric
δ : X × X → R≥0 is problem speciﬁc and satisﬁes the basic
axioms δ(x, y) = δ(y, x) and δ(x, x) = 0.

By following Dwork et al’s standard deﬁnition (Dwork
et al. 2012), we consider a classiﬁer C : X → ℘+(L) to
be fair when C outputs the same set of labels for every pair
of individuals x, y ∈ X which satisfy a similarity relation
S ⊆ X × X. Thus, S can be derived from a distance δ
as (x, y) ∈ S (cid:52)⇔ δ(x, y) ≤ (cid:15), where (cid:15) ∈ R is a thresh-
old of similarity. In order to estimate a fairness metric for a
classiﬁer C, we count how often C is fair on sets of similar
individuals ranging into a test set T ⊆ X × L:

fair T (C) (cid:44) |{(x, y) ∈ T | fair(C, x, S)}|

|T |

(1)

where fair(C, x, S) is deﬁned as follows:

Deﬁnition 3.1 (Individual Fairness). A classiﬁer C : X →
℘+(L) is fair on an input sample x ∈ X with respect to a
similarity relation S ⊆ X × X, denoted by fair(C, x, S),
when ∀x(cid:48) ∈ X : (x, x(cid:48)) ∈ S ⇒ C(x(cid:48)) = C(x).

Hence, fairness for a similarity relation S boils down to
stability on the perturbation PS(x) (cid:44) {x(cid:48) ∈ X | (x, x(cid:48)) ∈
S}, namely, for all x ∈ X,

fair(C, x, S) ⇔ stable(C, x, PS)

(2)

Let us remark that fairness is orthogonal to accuracy since
it does not depend on the correctness of the label assigned by
the classiﬁer, so that that training algorithms that maximize
accuracy-based metrics do not necessarily achieve fair mod-
els. Thus, this is also the case of a natural learning algorithm
for CART trees and RFs, that locally optimizes split crite-
ria by measuring entropy or Gini impurity, which are both
indicators of the correct classiﬁcation of training data.

It is also worth observing that fairness is monotonic with

respect to the similarity relation, meaning that

fair(C, x, S) ∧ S(cid:48) ⊆ S ⇒ fair(C, x, S(cid:48))

(3)

We will exploit this monotonicity property, since this im-
plies that, on one hand, fair classiﬁcation is preserved for
smaller similarity relations and, on the other hand, fairness
veriﬁcation and fair training is more challenging for larger
similarity relations.

4 Verifying Fairness
As individual fairness is equivalent to stability, individual
fairness of decision trees can be veriﬁed by Silva (Ranzato
and Zanella 2020a), an abstract interpretation-based algo-
rithm for checking stability properties of decision tree en-
sembles.

4.1 Veriﬁcation by Silva
Silva performs a static analysis of an ensemble of decision
trees in a so-called abstract domain A that approximates
properties of real vectors, meaning that each abstract value
a ∈ A represents a set of real vectors γ(a) ∈ ℘(Rd).
Silva approximates an input region P (x) ∈ ℘(Rd) for an
input vector x ∈ Rd by an abstract value a ∈ A such
that P (x) ⊆ γ(a) and for each decision tree t, it com-
putes an over-approximation of the set of leaves of t that
can be reached from some vector in γ(a). This is computed
by collecting the constraints of split nodes for each root-
leaf path, so that each leaf λ of t stores the minimum set
of constraints Cλ which makes λ reachable from the root
of t. It is then checked if this set of constraints Cλ can be
satisﬁed by the input abstract value a ∈ A: this check is
denoted by a |=? Cλ and its soundness requirement means
that if some input sample z ∈ γ(a) may reach the leaf λ
then a |= Cλ must necessarily hold. When a |= Cλ holds
the leaf λ is marked as reachable from a. For example, if
Cλ = {x1 ≤ 2, ¬(x1 ≤ −1), x2 ≤ −1} then an ab-
stract value such as (cid:104)x1 ≤ 0, x2 ≤ 0(cid:105) satisﬁes Cλ while a
relational abstract value such as x1 + x2 = 4 does not.
This over-approximation of the set of leaves of t reachable
from a allows us to compute a set of labels, denoted by
tA(a) ∈ ℘+(L) which is an over-approximation of the set of
labels assigned by t to all the input vectors ranging in γ(a),
i.e., ∪z∈γ(a)t(z) ⊆ tA(a) holds. Thus, if P (x) ⊆ γ(a) and
tA(a) = t(x) then t is stable on P (x).

For standard classiﬁcation trees with hard univariate splits
of type xi ≤ k, we will use the well-known hyper-rectangle
abstract domain HR whose abstract values for vectors x ∈
Rd are of type

h = (cid:104)xi ∈ [l1, u1], . . . , xd ∈ [ld, ud](cid:105) ∈ HRd
where lower and upper bounds l, u ∈ R ∪ {−∞, +∞} with
l ≤ u (more on this abstract domain can be found in (Ri-
val and Yi 2020)). Thus, γ(h) = {x ∈ Rd | ∀i. li ≤
xi ≤ ui}. The hyper-rectangle abstract domain guarantees
that for each leaf constraint Cλ and h ∈ HR, the check
h |=? Cλ is (sound and) complete, meaning that h |= Cλ
holds iff there exists some input sample in γ(h) reaching λ.
This completeness property therefore entails that the set of
labels tHR(h) computed by this analysis coincides exactly
with the set of classiﬁcation labels computed by t for all
the samples in γ(h), so that for the (cid:96)∞-based perturbation
such that P∞(x) = γ(h) then it turns out that t is stable on
P∞(x) iff tHR(h) = t(x) holds.

In order to analyse a forest F of trees, Silva reduces the
whole forest to a single tree tF , by stacking every tree t ∈ F
on top of each other, i.e., each leaf becomes the root of the
next tree in F , where the ordering of this unfolding operation

does not matter. Then, each leaf λ of this huge single tree tF
collects all the constraints of the leaves in the path from the
root of tF to λ. Since this stacked tree tF suffers from a com-
binatorial explosion of the number of leaves, Silva deploys a
number of optimisation strategies for its analysis. Basically,
Silva exploits a best-ﬁrst search algorithm to look for a pair
of input samples in γ(a) which are differently labeled, hence
showing instability. If one such instability counterexample
can be found then instability is proved and the analysis ter-
minates, otherwise stability is proved. Also, Silva allows to
set a safe timeout which, when met, stops the analysis and
outputs the current sound over-approximation of labels.

4.2 Veriﬁcation with One-Hot Enconding

As described above, the soundness of Silva guarantees that
no true reachable leaf is missed by this static analysis. More-
over, when the input region P (x) is deﬁned by the (cid:96)∞ norm
and the static analysis is performed using the abstract do-
main of hyper-rectangles HR, Silva is also complete, mean-
ing that no false positive (i.e., a false reachable leaf) can
occur. However, that this is not true anymore when dealing
with classiﬁcation problems involving some categorical fea-
tures.

colorwhite ≤ 0.5

colorblack ≤ 0.5

t1

t2

{(cid:96)1}

{(cid:96)2}

{(cid:96)2}

{(cid:96)1}

The diagram above depicts a toy forest F consisting of
two trees t1 and t2, where left/right branches are followed
when the split condition is false/true. Here, a categori-
cal feature color ∈ {white, black} is one-hot encoded by
colorwhite, colorblack ∈ {0, 1}. Since colors are mutually
exclusive, every white individual in the input space, i.e.
colorwhite = 1, colorblack = 0, will be labeled as (cid:96)1 by
both trees. However, by running the stability analysis on
the hyper-rectangle (cid:104)colorwhite ∈ [0, 1], colorblack ∈ [0, 1](cid:105),
Silva would mark the classiﬁer as unstable because there ex-
ists a sample in [0, 1]2 whose output is {(cid:96)1, (cid:96)2} (cid:54)= {(cid:96)1} =
F (colorwhite = 1, colorblack = 0). This is due to the point
(0, 0) ∈ [0, 1]2 which is a feasible input sample for the anal-
ysis, although it does not represent any actual individual in
the input space. In fact, t1(0, 0) = {(cid:96)2} and t2(0, 0) = {(cid:96)1},
so that by a majority voting F (0, 0) = {(cid:96)1, (cid:96)2}, thus making
F unstable (i.e., unfair) on (1, 0) (i.e., on white individuals).
To overcome this issue, we instantiate Silva to an abstract
domain which is designed as a reduced product (more de-
tails on reduced products can be found in (Rival and Yi
2020)) with a relational abstract domain keeping track of
the relationships among the multiple binary features intro-
duced by one-hot encoding a categorical feature. More for-
mally, this relational domain maintains the following two
additional constraints on the k features xc
k introduced
by one-hot encoding a categorical variable xc with k distinct
values:
1. the possible values for each xc

i are restricted to {0, 1};

1, ..., xc

2. the sum of all xc

i must satisfy (cid:80)k

i=1 xc

i = 1.

Hence, these conditions guarantee that any abstract value for
xc
k represents precisely one possible category for xc.
1, ..., xc
This abstract domain for a categorical variable x with k dis-
tinct values is denoted by OHk(x). In the example above,
any hyper-rectangle (cid:104)colorwhite ∈ [0, 1], colorblack ∈ [0, 1](cid:105)
is reduced by OH2(color), so that just two different val-
ues (cid:104)colorwhite = 0, colorblack = 1(cid:105) and (cid:104)colorwhite =
1, colorblack = 0(cid:105) are allowed.

Summing up, the generic abstract value of the reduced
hyper-rectangle domain computed by the analyzer Silva for
data vectors consisting of d numerical variables xj ∈ R and
m categorical variables cj with kj ∈ N distinct values is:

(cid:104)xj ∈ [lj, uj](cid:105)d

i=1 cj
where lj, uj ∈ R ∪ {−∞, +∞} and lj ≤ uj.

i ∈ {0, 1} | (cid:80)kj

j=1 × (cid:104)cj

i = 1(cid:105)m

j=1

5 FATT: Fairness-Aware Training of Trees

Several algorithms for training robust decision trees and en-
sembles have been put forward (Andriushchenko and Hein
2019; Calzavara, Lucchese, and Tolomei 2019; Calzavara
et al. 2020; Chen et al. 2019; Kantchelian, Tygar, and Joseph
2016; Ranzato and Zanella 2020b). These algorithms encode
the robustness of a tree classiﬁer as a loss function which is
minimized either by either exact methods such as MILP or
by suboptimal heuristics such as genetic algorithms.

The robust learning algorithm of (Ranzato and Zanella
2020b), called Meta-Silvae, aims at maximizing a tunable
weighted linear combination of accuracy and stability met-
rics. Meta-Silvae relies on a genetic algorithm for evolving
a population of trees which are ranked by their accuracy and
stability, where tree stability is computed by resorting to the
veriﬁer Silva (Ranzato and Zanella 2020a). At the end of
this genetic evolution, Meta-Silvae returns the best tree(s). It
turns out that Meta-Silvae typically outputs compact models
which are easily interpretable and often achieve accurate and
stable models already with a single decision tree rather than
a forest. By exploiting the equivalence (2) between individ-
ual fairness and stability and the instantiation of the veri-
ﬁer Silva to the product abstract domain tailored for one-hot
encoding, we use Meta-Silvae as a learning algorithm for
decision trees, called FATT, that enhances their individual
fairness.

While standard learning algorithms for tree ensembles
require tuning some hyper-parameters, such as maximum
depth of trees, minimum amount of information on leaves
and maximum number of trees, Meta-Silvae is able to infer
them automatically, so that the traditional tuning process is
not needed. Instead, some standard parameters are required
by the underlying genetic algorithm, notably, the size of the
evolving population, the maximum number of evolving iter-
ations, the crossover and mutation functions (Holland 1984;
Srinivas and Patnaik 1994). Moreover, we need to specify
the objective function of FATT that, for learning fair deci-
sion trees, is given by a weighted sum of the accuracy and
individual fairness scores over the training set. It is worth
remarking that, given an objective function, the genetic al-
gorithm of FATT converges to an optimal (or suboptimal)

solution regardless of the chosen parameters, which just af-
fect the rate of convergence and therefore should be chosen
for tuning its speed.

Crossover and mutation functions are two main distinc-
tive features of the genetic algorithm of Meta-Silvae. The
crossover function of Meta-Silvae combines two parent trees
t1 and t2 by randomly substituting a subtree of t1 with a
subtree of t2. Also, Meta-Silvae supports two types of muta-
tion strategies: grow-only, which only allows trees to grow,
and grow-and-prune, which also allows pruning the mutated
trees. Finally, let us point out that Meta-Silvae allows to set
the basic parameters used by generic algorithms: population
size, selection function, number of iterations. In our instan-
tiation of Meta-Silvae to fair learning: the population size is
kept ﬁxed to 32, as the experimental evaluation showed that
this provides an effective balance between achieved fairness
and training time; the standard roulette wheel algorithm is
employed as selection function; the number of iterations is
typically dataset-speciﬁc.

6 Experimental Evaluation
We consider the main standard datasets used in the fairness
literature and we preprocess them by following the steps of
Ruoss et al. (Ruoss et al. 2020, Section 5) for their exper-
iments on individual fairness for deep neural networks: (1)
standardize numerical attributes to zero mean and unit vari-
ance; (2) one-hot encoding of all categorical features; (3)
drop rows/columns containing missing values; and (4) split
into train and test set. These datasets concern binary clas-
siﬁcation tasks, although our fair learning naturally extends
to multiclass classiﬁcation with no speciﬁc effort. We will
make all the code, datasets and preprocessing pipelines of
FATT publicly available upon publication of this work.
Adult. The Adult income dataset (Dua and Graff 2017) is
extracted from the 1994 US Census database. Every sam-
ple assigns a yearly income (below or above $50K) to
an individual based on personal attributes such as gender,
race, and occupation.

Compas. The COMPAS dataset contains data collected on
the use of the COMPAS risk assessment tool in Broward
County, Florida (Angwin et al. 2016). Each sample pre-
dicts the risk of recidivism for individuals based on per-
sonal attributes and criminal history.

Crime. The Communities and Crime dataset (Dua and
Graff 2017) contains socio-economic, law enforcement,
and crime data for communities within the US. Each sam-
ple indicates whether a community is above or below the
median number of violent crimes per population.

German. The German Credit dataset (Dua and Graff 2017)
contains samples assigning a good or bad credit score to
individuals.

Health. The heritage Health dataset (https://www.kaggle.
com/c/hhp) contains physician records and insurance
claims. Each sample predicts the ten-year mortality
(above or below the median Charlson index) for a patient.
Table 1 displays size and distribution of positive sam-
ples for these datasets. As noticed by (Ruoss et al. 2020),

dataset
adult
compas
crime
german
health

#features
103
371
147
56
110

Test Set

positive

Training Set
size
30162
4222
1595
800
174732

size
24.9% 15060
1056
53.3%
399
50.0%
69.8%
200
68.1% 43683

positive
24.6%
55.6%
49.6%
71.0%
68.0%

Table 1: Overview of Datasets.

some datasets exhibit a highly unbalanced label distribu-
tion. For example, for the adult dataset, the constant clas-
siﬁer C(x) = 1 would achieve 75.4% test set accuracy and
100% individual fairness with respect to any similarity re-
lation. Hence, we follow (Ruoss et al. 2020) and we will
evaluate and report the balanced accuracy of our FATT clas-
siﬁers, i.e.,

0.5 (

truePositive
truePositive+falseNegative +

trueNegative
trueNegative+falsePositive ).

6.1 Similarity Relations
We consider four different types of similarity relations, as
described by Ruoss et al. (Ruoss et al. 2020, Section 5.1). In
the following, let I ⊆ N denote the set of indexes of features
of an individual after one-hot encoding.
NOISE: Two individuals x, y ∈ X are similar when a sub-
set of their (standardized) numerical features indexed by
a given subset I (cid:48) ⊆ I differs less than a given thresh-
old τ ≥ 0, while all the other features are unchanged:
(x, y) ∈ Snoise iff |xi − yi| ≤ τ for all i ∈ I (cid:48), and
xi = yi for all i ∈ I (cid:114) I (cid:48). For our experiments, we con-
sider (cid:15) = 0.3 in the standardized input space, e.g., for
adult two individuals are similar if their age difference is
at most 3.95 years.

CAT: Two individuals are similar if they are identical except
for one or more categorical sensitive attributes indexed
by I (cid:48) ⊆ I: (x, y) ∈ Scat iff xi = yi for all i ∈ I (cid:114)
I (cid:48). For adult and german, we select the gender attribute.
For compas, we identify race as sensitive attribute. For
crime, we consider two individuals similar regardless of
their state. Lastly, for health, neither gender nor age group
should affect the ﬁnal prediction.

NOISE-CAT: Given noise and categorical similarity rela-
tions Snoise and Scat, their union Snoise-cat (cid:44) Snoise ∪ Scat
models a relation where two individuals are similar when
some of their numerical attributes differ up to a given
threshold while the other attributes are equal except some
categorical features.

CONDITIONAL-ATTRIBUTE: Here, similarity is a disjunc-
tion of two mutually exclusive cases. Consider a numeri-
cal attribute xi, a threshold τ ≥ 0 and two noise similar-
ities Sn1, Sn2. Two individuals are deﬁned to be similar
if their i-th attributes are similar for Sn1 and are bounded
by τ or these attributes are above τ and similar for Sn2:
Scond (cid:44) {(x, y) ∈ Sn1 | xi ≤ τ, yi ≤ τ } ∪ {(x, y) ∈
Sn2 | xi > τ, yi > τ }. For adult, we consider the median
age as threshold τ = 37, and two noise similarities based

on age with thresholds 0.2 and 0.4, which correspond to
age differences of 2.63 and 5.26 years respectively. For
german, we also consider the median age τ = 33 and the
same noise similarities on age, that correspond to age dif-
ferences of 0.24 and 0.47 years.

Note that our approach is not limited to supporting these
similarity relations. Further domain-speciﬁc similarities can
be deﬁned and handled by our approach by instantiating
the underlying veriﬁer Silva with an appropriately over-
approximating abstract domain to retain soundness. More-
over, if the similarity relation can be precisely represented
in the chosen abstract domain, we also retain completeness.

6.2 Setup
Our experimental evaluation compares CART trees and Ran-
dom Forests with our FATT tree models. CARTs and RFs
are trained by scikit-learn. We ﬁrst run a preliminary phase
for tuning the hyper-parameters for CARTs and RFs. In par-
ticular, we considered both entropy and Gini index as split
criteria, and we checked maximum tree depths ranging from
5 to 100 with step 10. For RFs, we scanned the maximum
number of trees (5 to 100, step 10). Cross validation inferred
the optimal hyper-parameters, where the datasets have been
split in 80% training and 20% validation sets. The hyper-
parameters of FATT (i.e, weights of accuracy and fairness
in the objective function, type of mutation, selection func-
tion, number of iterations) by assessing convergence speed,
maximum ﬁtness value and variance among ﬁtness in the
population during the training phase. FATT trained single
decision trees rather than forests, thus providing more com-
pact and interpretable models. It turned out that accuracy and
fairness of single FATT trees are already competitive, where
individual fairness may exceed 85% for the most challeng-
ing similarities. We therefore concluded that ensembles of
FATT trees do not introduce statistically signiﬁcant beneﬁts
over single decision trees. Since FATT trees are stochastic by
relying on random seeds, each experimental test has been re-
peated 1000 times and the results refer to their median value.

6.3 Results

Acc. %

Bal.Acc. %

CAT

Individual Fairness fair T %
NOISE

Dataset
adult
compas
crime
german
health
Average

RF
82.76
66.57
80.95
76.50
85.29
78.41

FATT
80.84
64.11
79.45
72.00
77.87
74.85

RF
70.29
66.24
80.98
63.62
83.27
72.88

FATT
61.86
63.83
79.43
52.54
73.59
66.25

RF
91.71
48.01
86.22
91.50
7.84
65.06

FATT
100.00
100.00
100.00
100.00
99.99
100.00

RF
85.44
35.51
31.83
92.00
47.66
58.49

FATT
95.21
85.98
75.19
99.50
97.04
90.58

Table 2: RF and FATT comparison.

NOISE-CAT
RF
77.50
30.87
32.08
90.00
2.91
46.67

FATT
95.21
85.98
75.19
99.50
97.03
90.58

Table 2 shows a comparison between RFs and FATTs. We
show accuracy, balanced accuracy and individual fairness
with respect to the NOISE, CAT, and NOISE-CAT similarity
relations as computed on the test sets T . As expected, FATT
trees are slightly less accurate than RFs — 3.6% on aver-
age, which also reﬂects to balanced accuracy — but outper-
form them in every fairness test. On average, the fairness
increment ranges between +35% to +45% among different
similarity relations. Table 3 shows the comparison for the

conditional-attribute similarity, which applies to adult and
german datasets only. Here, the average fairness increase of
FATT models is +8.5%.

Individual Fairness fair T %
RF
84.75
91.50

FATT
94.12
99.50

Dataset
adult
german

Table 3: Comparison for conditional-attribute.

Figure 1: Accuracy (top) and Fairness (bottom).

Fig. 1 shows the distribution of accuracy and individual
fairness for FATT trees over 1000 runs of the FATT learning
algorithm. This plot is for fairness with respect to noise-cat
similarity, as this is the most challenging relation to train for
(this is a consequence of (3)). We can observe a stable be-
haviour for accuracy, with ≈ 50% of the observations laying
within one percentile from the median. The results for fair-
ness are analogous, although for compas we report a higher
variance of the distribution, where the lowest observed fair-
ness percentage is ≈ 10% higher than the corresponding one
for RFs. We claim that this may depend by the high number
of features in the dataset, which makes fair training a chal-
lenging task.

Table 4 compares the size of RF and FATT models, de-
ﬁned as total number of leaves. It turns out that FATT tree
models are orders of magnitude smaller and, thus, more in-
terpretable than RFs (while having comparable accuracy and

 60 65 70 75 80 85adultcompascrimegermanhealthAccuracy %Distribution of Accuracy over diﬀerent Trainings 40 50 60 70 80 90 100adultcompascrimegermanhealthFairness %Distribution of Fairness over diﬀerent TrainingsModel size

Avg. veriﬁcation time per sample (ms)

CAT

NOISE

Dataset
adult
compas
crime
german
health

RF
1427
147219
14148
5743
2558676

FATT
43
75
11
2
84

RF
0.03
0.36
0.12
0.06
1.40

FATT
0.02
0.07
0.07
0.03
0.06

RF
0.03
0.47
2025.13
0.06
0.91

FATT
0.02
0.07
0.07
0.02
0.05

NOISE-CAT
RF
0.03
0.61
2028.47
0.07
3.10

FATT
0.02
0.07
0.07
0.03
0.06

Table 4: Model sizes and veriﬁcation times.

signiﬁcantly enhanced fairness). Let us also remark that the
average veriﬁcation time per sample for our FATT models is
always less than 0.01 milliseconds.

FATT

Natural CART

Hinted CART

Dataset Acc. % Fair. % Size Acc. % Fair. % Size Acc. % Fair. % Size
47
adult
56
compas
8
crime
4
german
100
health
43
Average

95.21
85.98
75.19
99.50
97.03
90.58

85.32
65.91
77.69
75.50
83.85
77.65

77.56
22.25
24.31
57.50
79.98
52.32

80.84
64.11
79.45
72.00
77.87
74.85

84.77
65.91
77.44
73.50
82.25
76.77

87.46
22.25
60.65
86.00
93.64
70.00

270
56
48
115
2371
572

43
75
11
2
84
43

Table 5: Decision trees comparison.

Finally, in Table 5 compares FATT models with natural
CART trees in terms of accuracy, size, and fairness with re-
spect to the noise-cat similarity. While CARTs are approx-
imately 3% more accurate than FATT on average, they are
roughly half less fair and more than ten times larger.

It is well known that decision trees often overﬁt (Bramer
2007) due to their high number of leaves, thus yielding un-
stable/unfair models. Post-training techniques such as tree
pruning are often used to mitigate overﬁtting (Kearns and
Mansour 1998), although they are deployed when a tree has
been already fully trained and thus often pruning is poorly
beneﬁcial. As a byproduct of our approach, we trained a set
of natural CART trees, denoted by Hint in Table 5, which
exploits hyper-parameters as “hinted” by FATT training. In
particular, in this “hinted” learning of CART trees, the max-
imum tree depth and the minimum number of samples per
leaf are obtained as tree depth and minimum number of sam-
ples of our best FATT models. Interestingly, the results in
Table 5 show that these “hinted” decision trees have roughly
the same size of our FATT trees, are approximately 20%
more fair than natural CART trees and just 1% less accu-
rate. Overall, it turns out that the general performance of
these “hinted” CARTs is halfway between natural CARTs
and FATTs, both in term of accuracy and fairness, while hav-
ing the same compactness of FATT models.

7 Conclusion
We believe that this work contributes to push forward the
use of formal veriﬁcation methods in decision tree learn-
ing, in particular a very well known program analysis tech-
nique such as abstract interpretation is proved to be success-
ful for training and verifying decision tree classiﬁers which
are both accurate and fair, improve on state-of-the-art CART
and random forest models, while being much more compact
and interpretable. We also showed how information from our
FATT trees can be exploited to tune the natural training pro-
cess of decision trees. As future work we plan to extend fur-

ther our fairness analysis by considering alternative fairness
deﬁnitions, such as group or statistical fairness.

References
Aghaei, S.; Azizi, M. J.; and Vayanos, P. 2019. Learn-
ing Optimal and Fair Decision Trees for Non-Discriminative
Decision-Making. In Proceedings of the Thirty-Third AAAI
Conference on Artiﬁcial Intelligence, AAAI 2019, 1418–
1426. AAAI Press.
doi:10.1609/aaai.v33i01.33011418.
URL https://doi.org/10.1609/aaai.v33i01.33011418.

Andriushchenko, M.; and Hein, M. 2019. Provably Robust
Boosted Decision Stumps and Trees against Adversarial At-
tacks. In Proc. 33rd Annual Conference on Neural Informa-
tion Processing Systems (NeurIPS 2019).

Angwin, J.; Larson, J.; Mattu, S.; and Kirchner, L. 2016.
Machine Bias. ProPublica, May 23: 2016.

Barocas, S.; and Selbst, A. D. 2016. Big Data’s Disparate
Impact. California Law Review 104: 671.

Bertsimas, D.; and Dunn, J. 2017. Optimal classiﬁcation
trees. Mach. Learn. 106(7): 1039–1082. URL http://dblp.
uni-trier.de/db/journals/ml/ml106.html#BertsimasD17.

Bramer, M. 2007. Avoiding overﬁtting of decision trees.
Principles of data mining 119–134.

Breiman, L. 2001. Random Forests. Machine Learning
45(1): 5–32. doi:10.1023/A:1010933404324. URL https:
//doi.org/10.1023/A:1010933404324.

Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J.
1984. Classiﬁcation and Regression Trees. Wadsworth.
ISBN 0-534-98053-8.

Calzavara, S.; Lucchese, C.; and Tolomei, G. 2019. Adver-
sarial Training of Gradient-Boosted Decision Trees. In Proc.
28th ACM International Conference on Information and
Knowledge Management (CIKM 2019), 2429–2432. ISBN
978-1-4503-6976-3. doi:10.1145/3357384.3358149. URL
http://doi.acm.org/10.1145/3357384.3358149.

Calzavara, S.; Lucchese, C.; Tolomei, G.; Abebe, S. A.;
and Orlando, S. 2020. TREANT: training evasion-aware
decision trees. Data Mining and Knowledge Discovery
doi:10.1007/s10618-020-00694-9. URL https://doi.org/10.
1007/s10618-020-00694-9.

Carlini, N.; and Wagner, D. A. 2017. Towards Evaluating the
Robustness of Neural Networks. In Proc. of 38th IEEE Sym-
posium on Security and Privacy (S & P 2017), 39–57. doi:
10.1109/SP.2017.49. URL https://doi.org/10.1109/SP.2017.
49.

Chen, H.; Zhang, H.; Boning, D. S.; and Hsieh, C. 2019. Ro-
bust Decision Trees Against Adversarial Examples. In Proc.
36th Int. Conf. on Machine Learning, (ICML 2019), 1122–
1131. URL http://proceedings.mlr.press/v97/chen19m.html.

Chouldechova, A. 2017. Fair Prediction with Disparate Im-
pact: A Study of Bias in Recidivism Prediction Instruments.
Big Data 5(2): 153–163. doi:10.1089/big.2016.0047. URL
https://doi.org/10.1089/big.2016.0047.

Ranzato, F.; and Zanella, M. 2020b. Genetic Adversar-
ial Training of Decision Trees. arXiv:2012.11352, Github:
https://github.com/abstract-machine-learning/meta-silvae .
Rival, X.; and Yi, K. 2020. Introduction to Static Analysis:
An Abstract Interpretation Perspective. The MIT Press.
Roh, Y.; Lee, K.; Whang, S.; and Suh, C. 2020.
FR-
Train: A Mutual Information-Based Approach to Fair and
Robust Training. In Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18 July
2020, Virtual Event, volume 119 of Proceedings of Ma-
chine Learning Research, 8147–8157. PMLR. URL http:
//proceedings.mlr.press/v119/roh20a.html.
Ruoss, A.; Balunovic, M.; Fischer, M.; and Vechev, M. 2020.
Learning Certiﬁed Individually Fair Representations.
In
Proc. 34th Annual Conference on Advances in Neural In-
formation Processing Systems (NeurIPS 2020).
Schumann, C.; Foster, J. S.; Mattei, N.; and Dickerson,
J. P. 2020. We Need Fairness and Explainability in Al-
In Proc. 19th International Confer-
gorithmic Hiring.
ence on Autonomous Agents and Multiagent Systems (AA-
MAS 2020), 1716–1720. URL https://dl.acm.org/doi/abs/10.
5555/3398761.3398960.

Srinivas, M.; and Patnaik, L. M. 1994. Genetic algorithms:
a survey. Computer 27(6): 17–26.
Urban, C.; Christakis, M.; W¨ustholz, V.; and Zhang, F.
2020. Perfectly Parallel Fairness Certiﬁcation of Neural
Networks. Proceedings of the ACM on Programming Lan-
guages 4(OOPSLA): 185:1–185:30.
Yurochkin, M.; Bower, A.; and Sun, Y. 2020. Training in-
dividually fair ML models with sensitive subspace robust-
ness. In 8th International Conference on Learning Repre-
sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net. URL https://openreview.net/forum?
id=B1gdkxHFDH.

Zafar, M. B.; Valera, I.; Gomez-Rodriguez, M.; and Gum-
madi, K. P. 2017. Fairness Beyond Disparate Treatment
& Disparate Impact: Learning Classiﬁcation without Dis-
In Proc. 26th International Confer-
parate Mistreatment.
ence on World Wide Web (WWW 2017), 1171–1180. doi:
10.1145/3038912.3052660. URL https://doi.org/10.1145/
3038912.3052660.

Cousot, P.; and Cousot, R. 1977. Abstract interpretation:
a uniﬁed lattice model for static analysis of programs by
In Proc. 4th
construction or approximation of ﬁxpoints.
ACM Symposium on Principles of Programming Languages
(POPL 1977), 238–252. doi:10.1145/512950.512973. URL
http://doi.acm.org/10.1145/512950.512973.
Dua, D.; and Graff, C. 2017. UCI machine learning reposi-
tory.
Dwork, C.; Hardt, M.; Pitassi, T.; Reingold, O.; and Zemel,
R. 2012. Fairness Through Awareness. In Proc. 3rd Inno-
vations in Theoretical Computer Science Conference, 214–
226.
Friedman, J. H. 2001. Greedy Function Approximation: A
Gradient Boosting Machine. Annals of statistics 1189–1232.
Goodfellow, I.; McDaniel, P.; and Papernot, N. 2018. Mak-
ing Machine Learning Robust Against Adversarial Inputs.
Commun. ACM 61(7): 56–66.
ISSN 0001-0782. doi:10.
1145/3134599. URL http://doi.acm.org/10.1145/3134599.
Grari, V.; Ruf, B.; Lamprier, S.; and Detyniecki, M. 2020.
Achieving Fairness with Decision Trees: An Adversarial
Approach. Data Sci. Eng. 5(2): 99–110.
doi:10.1007/
s41019-020-00124-2. URL https://doi.org/10.1007/s41019-
020-00124-2.
Hardt, M.; Price, E.; and Srebro, N. 2016.
ity of Opportunity in Supervised Learning.
30th Annual Conference on Neural
cessing Systems (NeurIPS 2016), 3315–3323.
http://papers.nips.cc/paper/6374-equality-of-opportunity-
in-supervised-learning.
Holland, J. H. 1984. Genetic algorithms and adaptation. In
Adaptive Control of Ill-Deﬁned Systems, 317–333. Springer.
Kantchelian, A.; Tygar, J. D.; and Joseph, A. D. 2016. Eva-
sion and Hardening of Tree Ensemble Classiﬁers. In Proc.
33rd International Conference on Machine Learning (ICML
2016), 2387–2396. URL http://dl.acm.org/citation.cfm?id=
3045390.3045642.
Kearns, M. J.; and Mansour, Y. 1998. A Fast, Bottom-Up
Decision Tree Pruning Algorithm with Near-Optimal Gen-
In Proceedings of the Fifteenth International
eralization.
Conference on Machine Learning (ICML 1998), 269–277.
Khandani, A. E.; Kim, A. J.; and Lo, A. W. 2010. Con-
sumer Credit-Risk Models via Machine-Learning Algo-
rithms. Journal of Banking & Finance 34(11): 2767–2787.
doi:https://doi.org/10.1016/j.jbankﬁn.2010.06.001.
Raff, E.; Sylvester, J.; and Mills, S. 2018. Fair Forests:
Regularized Tree Induction to Minimize Model Bias.
In
Proc. 1st AAAI/ACM Conference on AI, Ethics, and Soci-
ety (AIES 2018), 243–250. doi:10.1145/3278721.3278742.
URL https://doi.org/10.1145/3278721.3278742.
Ranzato, F.; and Zanella, M. 2020a. Abstract Interpreta-
tion of Decision Tree Ensemble Classiﬁers. In Proc. 34th
AAAI Conference on Artiﬁcial Intelligence (AAAI 2020),
Github: https://github.com/abstract-machine-learning/silva,
5478–5486.
URL https://aaai.org/ojs/index.php/AAAI/
article/view/5998.

Equal-
In Proc.
Information Pro-
URL

