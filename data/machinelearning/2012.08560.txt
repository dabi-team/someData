0
2
0
2

c
e
D
5
1

]

G
L
.
s
c
[

1
v
0
6
5
8
0
.
2
1
0
2
:
v
i
X
r
a

Robust Optimal Classiﬁcation Trees under Noisy Labels

V´ıctor Blanco†, Alberto Jap´on‡ and Justo Puerto‡

†Institute of Mathematics, Universidad de Granada
‡Institute of Mathematics, Universidad de Sevilla

Abstract. In this paper we propose a novel methodology to construct Op-
timal Classiﬁcation Trees that takes into account that noisy labels may occur
in the training sample. Our approach rests on two main elements: (1) the
splitting rules for the classiﬁcation trees are designed to maximize the sepa-
ration margin between classes applying the paradigm of SVM; and (2) some
of the labels of the training sample are allowed to be changed during the
construction of the tree trying to detect the label noise. Both features are
considered and integrated together to design the resulting Optimal Classiﬁca-
tion Tree. We present a Mixed Integer Non Linear Programming formulation
for the problem, suitable to be solved using any of the available oﬀ-the-shelf
solvers. The model is analyzed and tested on a battery of standard datasets
taken from UCI Machine Learning repository, showing the eﬀectiveness of
our approach.

1. Introduction

Discrete Optimization has experienced a tremendous growth in the last decades,
both in its theoretical and practical sides, partially provoked by the emergence of
new computational resources as well as real-world applications that have boosted
this growth. This impulse has also motivated the use of Discrete Optimization
models to deal with problems involving a large number of variables and con-
straints, that years before would have not been possible to be dealt with. One
of the ﬁelds in which Discrete Optimization has caused a larger impact is in Ma-
chine Learning. The incorporation of binary decisions to the classical approaches
as Support Vector Machine [15], Classiﬁcation Trees [13], Linear Regression and
Clustering, amongst other, has considerably enhanced their performance in terms
of accuracy and interpretability (see e.g. [2, 4, 7, 9, 10, 11, 12, 17, 21]). In partic-
ular, one of the most interesting applications of Machine Learning is that related
with Supervised Classiﬁcation.

Supervised classiﬁcation aims at ﬁnding hidden patterns from a training sam-
ple of labeled data in order to predict the labels of out-of-sample data. Several
methods have been proposed in order to construct highly predictive classiﬁcation

Date: December 17, 2020.

1

 
 
 
 
 
 
2

V. BLANCO, A. JAP ´ON and J. PUERTO

tools. Some of the most widely used methodologies are based on Deep Learning
mechanisms [1], k-Nearest Neighborhoods [16, 26], Na¨ıve Bayes [23], Classiﬁca-
tion Trees [13, 20] and Support Vector Machines [15]. The massive use of these
tools has induced, in many situations, that malicious adversaries adaptively ma-
nipulate their data to mislead the outcome of an automatic analysis, and new
classiﬁcation rules must be designed to handle the possibility of this noise in
the training labels. A natural example are the spam ﬁlters for emails, where
malicious emails are becoming more diﬃcult to automatically be detected since
they have started to incorporate patterns that typically appear in legitimate
emails. As a consequence, the development of robust methods against these
kind of problems has attracted the attention of researchers (see e.g., [5, 8]).

In the context of binary classiﬁcation problems, Support Vector Machines
(SVM), introduced by Cortes and Vapnik [15], builds the decision rule by means
of a separating hyperplane with large margin between classes. This hyperplane
is obtained by solving a Non Linear Problem (NLP), in which the goal is to sep-
arate data by their two diﬀerentiated classes, maximizing the margin between
them and minimizing the misclassiﬁcation errors. In Classiﬁcation and Regres-
sion Trees (CART), ﬁrstly introduced by Breiman et. al [13], one constructs
the decision rule based on a hierarchical relation among a set of nodes which is
used to deﬁne paths that lead observations from the root node (highest node in
the hierarchical relation), to some of the leaves in which a class is assigned to
the data. These paths are obtained according to diﬀerent optimization criteria
over the predictor variables of the training sample. The decision rule comes
up naturally, the classes predicted for new observations are the ones assigned
to the terminal nodes in which observations fall in. Historically, CART is ob-
tained heuristically through a greedy approach, in which each level of the tree is
sequentially constructed: Starting at the root node and using the whole train-
ing sample, the method minimizes an impurity measure function obtaining as
a result a split that divides the sample into two disjoint sets which determine
the two descendant nodes. This process is repeated until a given termination
criterion is reached (minimum number of observations belonging to a leaf, max-
imum depth of the tree, or minimum percentage of observations of the same
class on a leaf, among others).
In this approach, the tree grows following a
top-down greedy approach, an idea that is also shared in other popular decision
tree methods like C4.5 [25] or ID3 [24]. The advantage of these methods is that
the decision rule can be obtained rather quickly even for large training samples,
since the whole process relies on solving manageable problems at each node.
Furthermore, these rules are interpretable since the splits only take into account
information about lower or upper bounds on a single feature. Nevertheless, there
are some remarkable disadvantages in these heuristic methodologies. The ﬁrst

3

one is that they may not obtain the optimal classiﬁcation tree, since they look
for the best split locally at each node, not taking into account the splits that
will come afterwards. Thus, these local branches may not capture the proper
structure of the data, leading to misclassiﬁcation errors in out-of-sample obser-
vations. The second one is that, specially under some termination criteria, the
solutions provided by these methods can result into very deep (complex) trees,
resulting in overﬁtting and, at times, loosing interpretability of the classiﬁcation
rule. This diﬃculty is usually overcome by pruning the tree as it is being con-
structed by comparing the gain on the impurity measure reduction with respect
to the complexity cost of the tree.

Mixing together the powerful features of standard classiﬁcation methods and
Discrete Optimization has motivated the study of supervised classiﬁcation meth-
ods under a new paradigm (see [6]). In particular, recently, Bertsimas and Dunn
[4] introduced the notion of Optimal Classiﬁcation Trees (OCT) by approaching
CART under optimization lens, providing a Mixed Integer Linear Programming
formulation to optimally construct Classiﬁcation Trees. In this formulation, bi-
nary variables are introduced to model the diﬀerent decisions to be taken in
the construction of the trees: deciding whether a split is applied and if an ob-
servation belongs to a terminal node. Moreover, the authors proved that this
model can be solved for reasonable size datasets, and equally important, that for
many diﬀerent real datasets, signiﬁcant improvements in accuracy with respect
to CART can be obtained. In contrast to the standard CART approach, OCT
builds the tree by solving a single optimization problem taking into account
(in the objective function) the complexity of the tree, avoiding post pruning
processes. Moreover, every split is directly applied in order to minimize the mis-
classiﬁcation errors on the terminal nodes, and hence, OCTs are more likely to
capture the essence of the data. Furthermore, OCTs can be easily adapted in
the so-called OCT-H model to decide on splits based on hyperplanes (oblique)
instead of in single variables. Another remarkable advantage of using optimiza-
tion tools in supervised classiﬁcation methods is that features such as sparsity or
robustness, can be incorporated to the models by means of binary variables and
constraints [22]. The interested reader is refereed to the recent survey [14]. We
would like to ﬁnish this discussion pointing out one of the main diﬀerences be-
tween SVM and Classiﬁcation Trees: SVM accounts for misclassiﬁcation errors
based on distances (to the separating hyperplane), i.e., the closer to the correct
side of the separating hyperplane, the better, whereas in Classiﬁcation Trees all
misclassiﬁed observations are equally penalized.

Recently, Blanco et. al [8] proposed diﬀerent SVM-based methods that provide
robust classiﬁers under the hypothesis of label noises. The main idea support-
ing those methods is that labels are not reliable, and in the process of building

4

V. BLANCO, A. JAP ´ON and J. PUERTO

classiﬁcation rules it may be beneﬁcial to ﬂip some of the labels of the train-
ing sample to obtain more accurate classiﬁers. With this paradigm, one of the
proposed methods, RE-SVM, is based on constructing a SVM separating hy-
perplane, but simultaneously allowing observations to be relabeled during the
training process. The results obtained by this method, in datasets in which noise
was added to the training labels, showed that this strategy outperforms, in terms
of accuracy, classical SVM and other SVM-based robust methodologies. See [5]
for alternative robust classiﬁers under label noise.

In this paper we propose a novel binary supervised classiﬁcation method,
called Optimal Classiﬁcation Tree with Support Vector Machines (OCTSVM),
that proﬁts both from the ideas of SVM and OCT to build classiﬁcation rules.
Speciﬁcally, our method uses the hierarchical structure of OCTs, which leads to
easily interpretable rules, but splits are based on SVM hyperplanes, maximizing
the margin between the two classes at each node of the tree. The fact that the
combination of SVM and classiﬁcation tree tools provides enhanced classiﬁers is
not new. A similar approach can be found in [3]. Nevertheless, in that paper
the authors analyze the greedy CART strategy by incorporating, sequentially the
maximization of the margin, over known assignments of observations to the leaves
of the tree. Opposite to that, OCTSVM does not assume those assumptions
and it performs an exact optimization approach. Moreover, this new method
also incorporates decisions on relabeling observations in the training dataset,
making it specially suitable for datasets where adversary attacks are suspected.
The results of our experiments show that OCTSVM outperforms other existing
methods under similar testing conditions.

The rest of the paper is organized as follows.

In Section 2 we recall the
main ingredients of our approach, in particular, SVM, RE-SVM and OCTs, as
well as the notation used through the paper. Section 3 is devoted to introduce
our methodology, and presents a valid Mixed Integer Non Linear Progamming
(MINLP) formulation. In Section 4 we report the results obtained in our com-
putational experience, in particular, the comparison of our method with OCT,
OCT-H and the greedy CART. Finally, some conclusions and further research
on the topic are drawn in Section 5.

2. Preliminaries

In this section we recall the main ingredients in the approach that will be
presented in Section 3 which allows us to construct robust classiﬁers under la-
bel noises, namely, Support Vector Machines with Relabeling (RE-SVM) and
Optimal Classiﬁcation Trees with oblique splits (OCT-H).

All through this paper, we consider that we are given a training sample X =
{(x1, y1) , . . . (xn, yn), } ⊆ Rp×{−1, +1}, in which p features have been measured

5

for a set of n individuals (x1, . . . , xn) as well as a ±1 label is also known for each
of them (y1, . . . , yn). The goal of supervised classiﬁcation is, to derive, from X ,
a decision rule DX : Rp → {−1, 1} capable to accurately predict the right label
of out-sample observations given only the values of the features. We assume,
without loss of generality that the features are normalized, i.e., xi ∈ [0, 1]p.

One of the most used optimization-based method to construct classiﬁers for
binary labels is SVM [15]. This classiﬁer is constructed by means of a separating
hyperplane in the feature space, H = {z ∈ Rp : ω(cid:48)z + ω0 = 0}, such that the
decision rule becomes:

DSV M
X

(x) =

(cid:40)

−1 if ω(cid:48)z + ω0 ≤ 0,
if ω(cid:48)z + ω0 ≥ 0.
1

To construct such a hyperplane, SVM chooses the one that simultaneously max-
imizes the separation between classes and minimizes the errors of misclassiﬁed
observations. These errors are measures proportional to the distances (in the
feature space) from the observations to their label half-space. SVM can be for-
mulated as a convex non linear programming (NLP) problem. This approach
allows one for the use of a kernel function as a way of embedding the original
data in a higher dimension space where the separation may be easier without
increasing the diﬃculty of solving the problem (the so-called kernel trick).

On the other hand, SVM has also been studied in the context of robust classiﬁ-
cation. In [8] three new models derived from SVM are developed to be applicable
to datasets in which the observations may have wrong labels in the training sam-
ple. This characteristic is incorporated into the models by allowing some of the
labels to be swapped (relabelled) at the same time that the separating hyper-
plane is built. Two of these models combine SVM with cluster techniques in
a single optimization problem while the third method, the so-called RE-SVM,
relabels observations based on misclassiﬁcation errors without using clustering
techniques, what makes it easier to train. The RE-SVM problem can be formu-
lated as:

min

1
2

(cid:107)ω(cid:107)2

2 + c1

n
(cid:88)

i=1

ei + c2

n
(cid:88)

i=1

ξi

s.t.

(1 − 2ξi)yi(ω(cid:48)xi + ω0) ≥ 1 − ei,
ω ∈ Rp, ω0 ∈ R,
ei ∈ R+, ξi ∈ {0, 1} ,

(RE − SVM)

∀i = 1, . . . , n,

(1)

∀i = 1, . . . , n,

6

V. BLANCO, A. JAP ´ON and J. PUERTO

where ξi takes value 1 if the ith observation of the training sample is relabelled,
and 0 otherwise and ei is the misclassifying error deﬁned as the hinge loss:

(cid:40)

ei =

max{0, 1 − yi(ω(cid:48)xi + ω0)} if observation i is not relabelled
max{0, 1 + yi(ω(cid:48)xi + ω0)} if observation i is relabelled

,

for i = 1, . . . , n. The costs parameters c1 and c2 (unit cost per misclassifying
error and per relabelled observation) allows one to ﬁnd a trade-oﬀ between large
separation between classes: c1 and c2 are parameters modelling the unit cost of
misclassiﬁed errors and relabelling, respectively. ((cid:107) · (cid:107)2 stands for the Euclidean
norm in Rp.) Constraints (1) assures the correct deﬁnition of the hinge loss and
relabelling variables.

The problem above can be reformulated as a Mixed Integer Second Order Cone
Optimization (MISOCO) problem, for which oﬀ-the-shelf optimization solvers as
CPLEX or Gurobi are able to solve medium size instances in reasonable CPU
time.

Classiﬁcation Trees (CT) are a family of classiﬁcation methods based on a
hierarchical relation among a set of nodes. The decision rule for CT methods is
built by recursively partitioning the feature space by means of hyperplanes. At
the ﬁrst stage, a root node for the tree is considered where all the observations
belongs to. Branches are sequentially created by splits on the feature space,
creating intermediate nodes until a leaf node is reached. Then, the predicted
label for an observation is given by the majority class of the leaf node where it
belongs to.

Speciﬁcally, at each node, t, of the tree a hyperplane Ht = {z ∈ Rp : ω(cid:48)

tz +
ωt0 = 0} is constructed and the splits are deﬁned as ω(cid:48)
tz + ωt0 < 0 (left branch)
and ω(cid:48)
In Fig. 1 we show a simple classiﬁcation
tree with depth two, for a small dataset with 6 observations, that are correctly
classiﬁed on the leaves.

tz + ωt0 ≥ 0 (right branch).

The most popular method to construct Classiﬁcation Trees from a training
dataset is CART, introduced by Brieman et. al [13]. CART is a greedy heuris-
tic approach, which myopically constructs the tree without further foreseen to
deeper nodes. Starting at the root node, it decides the splits by means of hyper-
planes minimizing an impurity function in each node. Each split results in two
new nodes, and this procedure is repeated until a stopping criterion is reached
(maximal depth, minimum number of observations in the same class, etc). CART
produces deep trees, which may lead to overﬁtting in out-of-sample observations.
Also, trees are normally subject to a prune process based on the trade-oﬀ be-
tween the impurity function reduction and a cost-complexity parameter. The
main advantage of CART is that it is easy to implement and fast to train.

7

Figure 1. Decision tree of depth two.

On the other hand, Bertsimas and Dunn [4] have recently proposed an optimal
approach to build CTs by solving a mathematical programming problems which
builds the decision tree in a compact model considering its whole structure and
at the same time making decisions on pruning or not pruning the branches.

Given a maximum depth, D, for the Classiﬁcation Tree it can have at most

T = 2D+1 − 1 nodes. These nodes are diﬀerentiated in two types:

• Branch nodes: τB = {1, . . . , (cid:98)T /2(cid:99)} are the nodes where the splits are

applied.

• Leaf nodes: τL = {(cid:98)T /2(cid:99), . . . , T } are the nodes where predictions for

observations are performed.

We use the following notation concerning the hierarchical structure of a tree:

• p(t): parent node of node t, for t = 1, . . . , T .
• τbl: set of nodes that follow the left branch on the path from their parent
nodes. Analogously, we deﬁne τbr as the set of nodes whose right branch
has been followed on the path from their parent nodes.

• u: set of nodes that have the same depth inside the tree. We represent
by U the whole set of levels. The root node is the zero-level, u0, hence,
for a given depth D we have D + 1 levels, being uD the set of leaf nodes.

OCTs are constructed by minimizing the following objective function:

(cid:88)

t∈τL

Lt + α

(cid:88)

t∈τB

dt,

8

V. BLANCO, A. JAP ´ON and J. PUERTO

where Lt stands for the misclassiﬁcation errors at the leaf t (measured as the
number of wrongly classiﬁed observations in the leaf), and dt is a binary variable
that indicates if a split is produced at t. Therefore, the constant α is used to
regulate the trade-oﬀ between the complexity (depth) and the accuracy (mis-
In its simplest version,
classifying errors of the training sample) of the tree.
motivated by what it is done in CART, the splits are deﬁned by means of a
single variable, i.e., in the form ωtjxj + ωt0 ≤ 0. Nevertheless, OCT can be
extended to a more complex version where the splits are hyperplanes deﬁned
by their normal vector, a ∈ Rp which is known as OCT-H. Moreover, a robust
version of OCT has also been studied under the noise label scenario in [5].

3. Optimal Classification Trees with SVM splits and Relabeling

(OCTSVM)

This section is devoted to introduce our new classiﬁcation methodology, namely
OCTSVM. The rationale of this approach is to combine the advantage of hierar-
chical classiﬁcation methods such as Classiﬁcation Trees, with the beneﬁts from
using distance-based classiﬁcation errors, by means of hyperplanes maximizing
the margin between classes (SVM paradigm). Therefore, this new model rests
on the idea of constructing an optimal classiﬁcation tree in which the splits of
the nodes are performed by following the underling ideas of model (RE − SVM):
1) the splits are induced by hyperplanes in which the positive and the negative
class are separated maximizing the margin between classes, 2) minimizing the
classiﬁcation errors, and 3) allowing observations to be relabeled along the train-
ing process. In contrast to what it is done in other Classiﬁcation Tree methods,
OCTSVM does not make a distinction (beyond the hierarchical one) between
branch and leaf nodes, in the sense that RE-SVM based splits are sequentially
applied in each node, and the ﬁnal classiﬁcation for any observation comes from
the hyperplanes resulting at the last level of the tree, in case there are no pruned
branches, or at the last node where a split was made in case of a pruned branch.
As it has been pointed out before, OCT-H is a classiﬁcation tree that allows
the use of general (oblique) hyperplane splits, which is built by solving a single
optimization problem that takes into account the whole structure of the tree.
Nevertheless, despite the good results this method has proven to obtain in real
classiﬁcation problems, a further improvement is worth to be considered. In Fig.
2 we see a set of points in the plane diﬀerentiated by colors in two classes. Look-
ing at the left picture, one can see one of the optimal solutions of OCT-H for
depth equal to two, where the red hyperplane is the split applied at the root node
and the black ones are applied on the left and right descendants, which deﬁne the
four leaves. This solution is optimal, for a certain value of the cost-complexity

9

parameters, since it does not make any mistakes on the ﬁnal classiﬁcation. Nev-
ertheless, since this method does not have any kind of control on the distances
from points to the hyperplanes, one can observe that the blue class has very
tiny margins at the leaves, and hence, for this class, misclassiﬁcation errors are
more likely to occur in out-of-sample observations. On the other hand, on the
right side of Fig. 2 one sees another possible optimal solution for the OCTSVM
model with depth equal to one. Again, the red hyperplane is the split applied at
the root node and the black ones are the classiﬁcation splits applied at the two
leaves. Despite these two methods are obtaining a perfect classiﬁcation on the
training sample, Fig. 2 shows that OCTSVM provides a more balanced solution
than OCT-H since it has wider margins between both classes, what could be
translated into a higher accuracy for out of sample observations.

Figure 2. Optimal solutions for OCT-H with D = 2 (left) and
OCTSVM with D = 1 (right).

In order to formulate the OCTSVM as a MINLP, we will start describing the
rationale of its objective function that must account for the margins induced by
the splits at all nodes, the classiﬁcation errors, the penalty paid for relabelling
observations and the cost complexity of the ﬁnal classiﬁcation tree. To formulate
the above concepts we need diﬀerent sets of variables. First of all, we consider
continuous variables: ωt ∈ Rp, ωt0 ∈ R, t = 1, . . . , T , which represent the coeﬃ-
cients and the intercept of the hyperplane performing the split at node t. Taking
2
into account that the margin of a hyperplane is given by
||ωt|| , maximizing the
minimum margin between classes induced by the splits can be done introducing
an auxiliary variable δ ∈ R (that will be minimized in the objective function)
which is enforced by the following constraints:

1
2

||ωt||2 ≤ δ

∀t = 1, . . . , T.

(2)

Once the maximization of the margin is set, we have to model the minimization
of the errors at the nodes, whereas at the same time we minimize the number
of relabelled observations. These two tasks are accomplished by the variables

10

V. BLANCO, A. JAP ´ON and J. PUERTO

eit ∈ R, i = 1, . . . , n, t = 1, . . . , T , that account for the misclassiﬁcation error
of observation i at node t, and ξit ∈ {0, 1} binary variables modelling whether
observation i is relabeled or not at node t. If c1 and c2 are the unit costs of
misclassiﬁcation and relabelling, respectively, our goal is achieved adding to the
objective function the following two terms:

n
(cid:88)

T
(cid:88)

c1

eit + c2

n
(cid:88)

T
(cid:88)

ξit.

i=1

t=1

i=1

t=1

The correct meaning of these two sets of variables must be enforced by some fam-
ilies of constraints that we describe next. Nevertheless, for the sake of readability
before describing those constraints modeling these eit and ξit, we must introduce
another family of variables the βit ∈ Rp, βi0 ∈ R, i = 1, . . . , n, t = 1, . . . , T ,
which are continuous variables equal to the coeﬃcients of the separating hyper-
plane at node t when observation i is relabelled, and equal to zero otherwise.
In addition, we consider binary variables zit ∈ {0, 1} needed to control whether
observation i belongs to node t of the tree. Now, putting all these elements
together, as it is done in RE-SVM, we can properly deﬁne the splits and their
errors at each node of the tree using the following constraints:
(cid:40)

yi(ω(cid:48)

txi + ωt0) − 2yi(β(cid:48)

txi + βit0) ≥ 1 − eit − M (1 − zit),

∀i = 1, . . . , N,
t = 1, . . . , T,

βitj = ξitωtj,

∀i = 1, . . . , N, t = 0, . . . , T, j = 0, . . . , p.

(4)

(3)

Constraints (4), which can be easily linearized, are used to deﬁne the βit vari-
ables: they equal the ωt variables when the observation i is relabelled (ξi = 1),
and are equal to zero otherwise (ξi = 0). On the other hand, constraints (3)
control the relabelling at each node of the tree. If an observation i is in node t
(zit = 1), and ξi = 0, we obtain the standard SVM constraints for the separating
hyperplane. Nevertheless, if ξi = 1, then the separating constraints are applied
for the observation i as if its class were the opposite to its actual one, i.e., as
if observation i is relabeled. Moreover, since M is a big enough constant, these
constraints do not have any kind of impact in the error variables of node t if
observation i does not belong to this node (zit = 0).
The ﬁnal term to be included in the objective function of the problem is the
complexity of the resulting tree. Following the approach in OCT and OCT-H,
we consider binary variables dt ∈ {0, 1} , t = 1, . . . , T , that account whether a
split is applied at node t. Thus, to control the tree complexity resulting of the
process, we minimize the sum of these variables multiplied by a cost penalty c3.

Gathering all the components together, the objective function to be minimized
in our problem results in

11

n
(cid:88)

T
(cid:88)

δ + c1

eit + c2

n
(cid:88)

T
(cid:88)

ξit + c3

i=1

t=1

i=1

t=1

T
(cid:88)

t=1

dt.

According to this, we need to consider the following constraints so as to obtain
a suitable modeling for the dt variables:

(cid:107)ωt(cid:107)2 ≤ M dt,

∀t = 1, . . . , T.

(5)

By imposing this, and being M a big enough constant, dt = 0 only if (cid:107)ωt(cid:107)2 = 0,
that is, if there is no split at node t. On the other hand, in case a split is
applied at node t, (cid:107)ωt(cid:107)2 > 0, and hence dt is forced to assume the value one.
Another point that is important to remark about the dt variables is that once a
non-leaf node does not split (that is, the corresponding branch is pruned at this
node), the successors of node t can not make splits either to maintain the correct
hierarchical structure of the tree. Recalling that p(t) is the parent node of node
t, we can guarantee this relationship throughout the following constraints

dt ≤ dp(t),

∀t = 1, . . . , T.

(6)

There are still some details left that must be imposed to make the model work
as required. In the decision tree, observations start at the root node and they
advance descending through the levels of the tree until they reach a leaf or a
pruned node. Hence, we have to guarantee that observations must belong to
one, and only one, node per level. By means of the zit variables, this can be
easily done by the usual assignment constraints applied in each level, u ∈ U , of
the tree:

(cid:88)

t∈u

zit = 1,

∀i = 1, . . . , N, u ∈ U.

(7)

Moreover, for consistency in the relation between a node and its ancestor, it is
clear that if observation i is in node t (zit = 1), then, observation i must be
also in the parent of node t (zip(t) = 1), with the only exception of the root
node. Besides, if observation i is not in node t (zit = 0), then i can not be in
its successors, and this is modeled by adding the following constraints to the
problem:

zit ≤ zip(t),

∀i = 1, . . . , N, t = 2, . . . , T.

(8)

So far, the OCTSVM model has everything it needs to properly perform the
splits by following the RE-SVM rationale described in (RE − SVM), taking into
consideration the tree complexity, and maintaining the hierarchical relationship
among nodes. The last element that we need to take care of, to assure the

12

V. BLANCO, A. JAP ´ON and J. PUERTO

correct performance of the whole model, is to deﬁne how observations follow
their paths inside the tree. We get from constraints (8) that observations move
from parent to children (nodes), but every non terminal node has a left and
a right child node, and we need to establish how observations take the left or
the right branch. Since the splits are made by the separating hyperplane, we
force observations that lie on the positive half space of a hyperplane to follow
the right branch of the parent node, and observations that lie con the negative
one to take the left branch. This behavior is modeled with the binary variables
θit ∈ {0, 1}, that are used to identify if observation i lies in the positive half
space of the separating hyperplane at node t, θit = 1, or if observation i lies on
the negative half space, θit = 0. By considering M a big enough constant, the
correct behavior of path followed by the observations is enforced by the following
constraints:

ω(cid:48)

txi + ωt0 ≥ −M (1 − θit),

ω(cid:48)

txi + ωt0 ≤ M θit,

∀i = 1, . . . , N, t = 1, . . . , T,

∀i = 1, . . . , N, t = 1, . . . , T.

(9)

(10)

Hence, by making use of these θit variables, and distinguishing between nodes
that come from left splits, τbl (nodes indexed by even numbers), and right splits,
τbr (nodes indexed by odd numbers), we control that the observations follow
the paths through the branches in the way we described above throughout the
following constraints:

zip(t) − zit ≤ θip(t),

zip(t) − zit ≤ 1 − θip(t),

∀i = 1, . . . , N, t ∈ τbl

∀i = 1, . . . , N, t ∈ τbr

(11)

(12)

According to constraints (11), if an observation i is on the parent node of an even
node t (zip(t) = 1), and i lies on the negative half space of the hyperplane deﬁning
the split on p(t) (θip(t) = 0), then zit is forced to be 1. Hence, θip(t) = 0 implies
that observation i takes the left branch to the child node t ∈ τbl. Moreover,
we can see that this constraint is consistent since if zip(t) = 1, but observation
i is not in the left child node, zit = 0, t ∈ τbl, then θip(t) equals 1, and which
means that observation i lies on the positive half space of the hyperplane of p(t).
On the other hand, constraints 12 are similar but for the right child nodes, τbr.
If an observation i is in the parent node of an odd node t ∈ τbr, and i lies on
the positive half space of the hyperplane of p(t) (θip(t) = 1), then, zit = 1 what
means that observation i has to be on node t.

Gathering all the constraints together, the OCTSVM model is obtained by

solving the following MINLP:

13

(OCTSVM)

∀i = 1, . . . , N, t = 1, . . . , T,

min δ + c1

n
(cid:88)

T
(cid:88)

eit + c2

n
(cid:88)

T
(cid:88)

ξit + c3

i=1

t=1

i=1

t=1

T
(cid:88)

t=1

dt

s.t.

||ωt||2 ≤ δ,

∀t = 1, . . . , T,

1
2
yi(ω(cid:48)

txi + ωt0) − 2yi(β(cid:48)

txi + βit0) ≥ 1 − eit − M (1 − zit),

βitj = ξitωtj,

∀i = 1, . . . , N, t = 0, . . . , T, j = 0, . . . , p,

(cid:107)ωt(cid:107)2 ≤ M dt,

∀t = 1, . . . , T,

dt ≤ dp(t),
(cid:88)

zit = 1,

∀t = 1, . . . , T,

∀i = 1, . . . , N, u ∈ U,

t∈u
zit ≤ zip(t),
ω(cid:48)

∀i = 1, . . . , N, t = 2, . . . , T,

txi + ωt0 ≥ −M (1 − θit),

∀i = 1, . . . , N, t = 1, . . . , T,

ω(cid:48)

txi + ωt0 ≤ M θit,

∀i = 1, . . . , N, t = 1, . . . , T,

zip(t) − zit ≤ θip(t),

∀i = 1, . . . , N, t ∈ τbl,

zip(t) − zit ≤ 1 − θip(t),
∀i = 1, . . . , N, t ∈ τbr,
eit ∈ R+, βit ∈ Rp, βit0 ∈ R, ξit, zit, θit ∈ {0, 1} , ∀i = 1, . . . , N, t = 1, . . . , T,
ωt ∈ Rp, ωt0 ∈ R, dt ∈ {0, 1} , ∀t = 1, . . . , T.

4. Experiments

In this section we present the results of our computational experiments. Four
diﬀerent Classiﬁcation Tree-based methods are compared, CART, OCT, OCT-H
and OCTSVM, on eight popular real-life datasets from UCI Machine Learning
Repository [18]. The considered datasets together with their dimensions (n:
number of observations and p: number of features) are shown in Table 1.

Dataset
Australian
BreastCancer 683
Heart
Ionosphere
MONK’s
Parkinson
Sonar
Wholesale

n
p
690 14
9
270 13
351 34
415
7
240 40
208 60
7
440

Table 1. Datasets used in our computational experiments.

14

V. BLANCO, A. JAP ´ON and J. PUERTO

Our computational experience focuses on the analysis of the accuracy of the
diﬀerent classiﬁcation trees-based methods. This analysis is based in four dif-
ferent experiments per each one of the eight considered dataset. Our goal is to
analyze the robustness of the diﬀerent methods against label noise in the data.
Therefore, in our experiments we use, apart from the original datasets, three
diﬀerent modiﬁcations where in each one of them a percentage of the labels in
the sample are randomly ﬂipped. The percentages of ﬂipped labels range in
{0%, 20%, 30%, 40%}, where 0% indicates that the original training data set is
used to construct the tree.

We perform a 4-fold cross-validation scheme, i.e., datasets are split into four
random train-test partitions. One of the folds is used for training the model
while the rest are used for testing. When testing the classiﬁcation rules, we
compute the accuracy, in percentage, on out of sample data:

ACC = #Well Classiﬁed Test Observations

#Test Observations

· 100.

The CART method was coded in R using the rpart library. OCT, OCT-H and
OCTSVM mathematical programming models were coded in Python and solved
using Gurobi 8.1.1 on a PC Intel Core i7-7700 processor at 2.81 GHz and 16GB
of RAM. A time limit of 30 seconds was set for training.

The calibration of the parameters of the diﬀerent optimization-based models

compared in these experiments was set as follows:

• For OCTSVM we used a grid on (cid:8)10i : i = −5, . . . , 5(cid:9) for the constants

c1 and c2, and a smaller one (cid:8)10i : i = −2, . . . , 2(cid:9) for c3.

• For CART, OCT and OCT-H we used the same grid (cid:8)10i : i = −5, . . . , 5(cid:9)
for the cost-complexity constant, and a threshold on the minimum num-
ber of observations per leaf equal to a 5% of the training sample size.

Last to mention, the depth, D, considered in these experiments was equal to
three for CART, OCT and OCT-H, whereas for OCTSVM we ﬁxed depth equal
to two, creating consequently trees with 3 levels, to set a fair comparison among
the diﬀerent methods.

For each dataset, we replicate each experiment four times.

In Table 2 we
show the average accuracies for each one of the methods and datasets. The best
average accuracies are highlighted (boldfaced).

Observe that when 0% of the training labels are ﬂipped (i.e., the original
dataset), a general trend in terms of accuracy can be observed: CART < OCT
< OCT-H < OCTSVM, with the only exception of Wholesale in which CART
obtains a non-meaningful slightly larger average accuracy. We would like to
emphasize that these results are not surprising. Indeed, on the one hand, OCT is
an optimal classiﬁcation version of the CART algorithm, and hence better results
should be expected from this model, as already shown in the computational

15

Heart

Australian

Ionosphere

BreastCancer

% Flipped CART OCT OCT-H OCTSVM
86.34
85.44
82.99
84.55
83.45
74.85
80.24
79.15
66.87
73.89
71.34
56.93
81.26
79.85
70.41
96.25
93.18
92.22
91.57
90.47
90.92
87.98
83.29 90.87
81.92
77.75 86.50
89.43
85.93 90.37
84.13
73.88
73.66
82.61
71.63
72.98
80.51
70.18
68.22
76.19
64.52
62.36
80.86
70.05
69.31
85.51
81.80
83.08
80.51
79.59
75.65
77.79
75.20
70.02
75.71
70.65
60.17
79.88
76.81
72.23
61.26
59.03
60.67
60.48
59.59
57.11
60.09
57.87
57.19
60.12
55.25
54.01
60.49
57.94
57.25
81.52
74.38
74.73
76.24
70.82
67.03
73.01
71.39
63.68
68.73
59.72
56.79
74.88
69.08
65.56
74.76
66.90
65.36
69.92
60.38
57.41
67.30
58.97
54.64
63.84
57.77
55.16
68.96
58.14
61.01
89.75
90.01
90.28
82.06
83.79 87.84
78.13
78.54 84.58
71.72
69.52 75.79
80.42
80.53 84.56
77.02
73.71
69.92
Table 2. Averaged accuracy results of the computational experiments.

0
20
30
40
Average
0
20
30
40
Average
0
20
30
40
Average
0
20
30
40
Average
0
20
30
40
Average
0
20
30
40
Average
0
20
30
40
Average
0
20
30
40
Average

85.16
79.88
71.55
64.25
75.21
94.21
89.38
84.83
76.35
86.19
78.86
74.22
70.88
65.30
72.32
84.40
75.08
71.93
65.87
74.32
61.63
59.33
59.13
57.01
59.28
74.69
66.66
62.81
58.46
65.66
71.63
65.45
62.53
60.33
64.99
90.51
85.43
77.91
72.56
81.60
72.44

Total Averages

Wholesale

Parkinson

MONK’s

Sonar

experience in [4], and also evidenced in our experiments. Moreover, OCT is
just a restricted version of OCT-H, in that in the latter, the set of admissible

16

V. BLANCO, A. JAP ´ON and J. PUERTO

hyperplanes is larger than in the former. Also, as already pointed out in Fig. 2,
OCTSVM goes one step further and, apart from allowing oblique hyperplanes
based trees, has a more solid structure due to the margin maximization, the
distance based errors and the possibility of relabeling points. All together turns
into higher accuracy results as pointed out in our experiments. In some datasets
the comparison of the diﬀerent methods gives rather similar results, however in
some others OCTSVM is above a 5% percent of accuracy with respect to the
other three methods.

Turning to the results on datasets with ﬂipped labels on the training dataset,
OCTSVM clearly outperforms the rest of methods and consistently keeps its
advantage in terms of accuracy with respect to the other three methods. OCT
and OCT-H, which are both above CART, alternate higher-lower results among
the diﬀerent datasets. Our method, namely OCTSVM, clearly is capable to
capture the wrongly labeled observations and constructs classiﬁcation rules able
to reach higher accuracies, even when 40% of the labels were ﬂipped, while other
methods, give rise to classiﬁers that signiﬁcantly worsen their performance in
terms of accuracy in the test sample.

The global average accuracy in the case that labels are not ﬂipped using
OCTSVM is 82.44%, being 2% better than OCT-H, 3% better than OCT and
10% better than CART. If 40% of the labels are ﬂipped the situation is similar,
being OCTSVM also 10% better than CART, and then, clearly showing the
robustness of our method under label noises.

5. Conclusions and Further Research

Supervised classiﬁcation is a fruitful ﬁeld that has attracted the attention of
researchers for many years. One of the methods that has experienced a more in
depth transformation in the last years is classiﬁcation trees. Since the pioneer
contribution by Breiman et al. [13], where CART was proposed, this technique
has included tools from mathematical optimization giving rise to the so called
OCT [4, 5], methods that are optimal in some sense. In spite of that, there is
still some more room for further improvements, in particular making classiﬁers
more robust against perturbed datasets. Our contribution is this paper goes in
that direction and it augments the catalogue of classiﬁcation tree methods able
to handle noise in the labels of the dataset.

We have proposed a new optimal classiﬁcation tree approach able to handle
label noise in the data. Two main elements support our approach: the splitting
rules for the classiﬁcation trees are designed to maximize the separation margin
between classes and wrong labels of the training sample are detected and changed
at the same time that the classiﬁer is built. The method is encoded on a Mixed
Integer Second Order Order Cone Optimization problem so that one can solve

17

medium size instances by any of the nowadays available oﬀ-the-shelf solvers. We
report intensive computational experiments on a battery of datasets taken from
UCI Machine Learning repository showing the eﬀectiveness and usefulness of our
approach.

Future research lines on this topic include the analysis of nonlinear splits when
branching in OCTSVM, both using kernel tools derived from SVM classiﬁers or
speciﬁc families of nonlinear separators. This approach will result into more
ﬂexible classiﬁers able to capture the nonlinear trends of many real-life datasets.
Additionally, we also plan to address the design of math-heuristic algorithms
which keep the essence of OCTs but capable to train larger datasets.

References

[1] Agarwal, N., Balasubramanian, V. N., and Jawahar, C. Improving multiclass clas-
siﬁcation by deep networks using dagsvm and triplet loss. Pattern Recognition Letters 112
(2018), 184–190.

[2] Benati, S., Puerto, J., and Rodr´ıguez-Ch´ıa, A. M. Clustering data that are graph

connected. European Journal of Operational Research 261, 1 (2017), 43–53.

[3] Bennett, K. P., and Blue, J. A support vector machine approach to decision trees. In
1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World
Congress on Computational Intelligence (Cat. No. 98CH36227) (1998), vol. 3, IEEE,
pp. 2396–2401.

[4] Bertsimas, D., and Dunn, J. Optimal classiﬁcation trees. Machine Learning 106, 7

(2017), 1039–1082.

[5] Bertsimas, D., Dunn, J., Pawlowski, C., and Zhuo, Y. D. Robust classiﬁcation.

INFORMS Journal on Optimization 1, 1 (2019), 2–34.

[6] Bertsimas, D., and Dunn, J. W. Machine learning under a modern optimization lens.

Dynamic Ideas LLC, 2019.

[7] Blanco, V., Jap´on, A., Ponce, D., and Puerto, J. On the multisource hyperplanes
location problem to ﬁtting set of points. Computers & Operations Research (2020), 105124.
[8] Blanco, V., Jap´on, A., and Puerto, J. A mathematical programming approach to
binary supervised classiﬁcation with label noise. arXiv preprint arXiv:2004.10170 (2020).
[9] Blanco, V., Jap´on, A., and Puerto, J. Optimal arrangements of hyperplanes for svm-
based multiclass classiﬁcation. Advances in Data Analysis and Classiﬁcation 14, 1 (2020),
175–199.

[10] Blanco, V., Puerto, J., and Salmer´on, R. Locating hyperplanes to ﬁtting set of
points: A general framework. Computers & Operations Research 95 (2018), 172–193.
[11] Blanquero, R., Carrizosa, E., Jim´enez-Cordero, A., and Mart´ın-Barrag´an, B.
Selection of time instants and intervals with support vector regression for multivariate
functional data. Computers & Operations Research 123 (2020), 105050.

[12] Blanquero, R., Carrizosa, E., Ram´ırez-Cobo, P., and Sillero-Denamiel, M. R.
A cost-sensitive constrained lasso. Advances in Data Analysis and Classiﬁcation (2020),
1–38.

[13] Breiman, L., Friedman, J., Stone, C. J., and Olshen, R. A. Classiﬁcation and

regression trees. CRC press, 1984.

18

V. BLANCO, A. JAP ´ON and J. PUERTO

[14] Carrizosa, E., Molero-R´ıo, C., and Romero Morales, D. Mathematical opti-
mization in classiﬁcation and regression trees, 12 2020. https://www.researchgate.
net/publication/346922645_Mathematical_optimization_in_classification_and_
regression_trees.

[15] Cortes, C., and Vapnik, V. Support-vector networks. Machine learning 20, 3 (1995),

273–297.

[16] Cover, T., and Hart, P. Nearest neighbor pattern classiﬁcation. IEEE transactions on

information theory 13, 1 (1967), 21–27.

[17] Drucker, H., Burges, C. J., Kaufman, L., Smola, A. J., and Vapnik, V. Support
vector regression machines. In Advances in neural information processing systems (1997),
pp. 155–161.

[18] Dua, D., and Graff, C. UCI machine learning repository, 2017.
[19] Fr´enay, B., and Verleysen, M. Classiﬁcation in the presence of label noise: a survey.
IEEE transactions on neural networks and learning systems 25, 5 (2013), 845–869.
[20] Friedman, J., Hastie, T., and Tibshirani, R. The elements of statistical learning,

vol. 1. Springer series in statistics New York, 2001.

[21] Gaudioso, M., Gorgone, E., Labb´e, M., and Rodr´ıguez-Ch´ıa, A. M. Lagrangian
relaxation for svm feature selection. Computers & Operations Research 87 (2017), 137–145.
[22] G¨unl¨uk, O., Kalagnanam, J., Menickelly, M., and Scheinberg, K. Optimal deci-
sion trees for categorical data via integer programming. arXiv preprint arXiv:1612.03225
(2018).

[23] Lewis, D. D. Naive (bayes) at forty: The independence assumption in information re-

trieval. In European conference on machine learning (1998), Springer, pp. 4–15.

[24] Quinlan, J. Machine learning and id3. Los Altos: Morgan Kauﬀman (1996).
[25] Quinlan, R. C4. 5. Programs for machine learning (1993).
[26] Tang, X., and Xu, A. Multi-class classiﬁcation using kernel density estimation on k-

nearest neighbours. Electronics Letters 52, 8 (2016), 600–602.

[27] Weerasinghe, S., Erfani, S. M., Alpcan, T., and Leckie, C. Support vector machines
resilient against training data integrity attacks. Pattern Recognition 96 (2019), 106985.

