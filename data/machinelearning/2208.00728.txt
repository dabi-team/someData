Performance Comparison of Deep RL Algorithms
for Energy Systems Optimal Scheduling

Hou Shengren1, Edgar Mauricio Salazar2, Pedro P. Vergara1, Peter Palensky1
1Intelligent Electrical Power Grids, Delft University of Technology, The Netherlands
2Electrical Energy Systems (EES) Group, Eindhoven University of Technology, The Netherlands
emails: {H.Shengren, P.P.VergaraBarrios, P.Palensky}@tudelft.nl, E.M.Salazar.Duque@tue.nl

2
2
0
2

g
u
A
1

]

Y
S
.
s
s
e
e
[

1
v
8
2
7
0
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—Taking advantage of their data-driven and model-
free features, Deep Reinforcement Learning (DRL) algorithms
have the potential to deal with the increasing level of uncertainty
due to the introduction of renewable-based generation. To deal
simultaneously with the energy systems’ operational cost and
technical constraints (e.g, generation-demand power balance)
DRL algorithms must consider a trade-off when designing the re-
ward function. This trade-off introduces extra hyperparameters
that impact the DRL algorithms’ performance and capability
of providing feasible solutions. In this paper, a performance
comparison of different DRL algorithms, including DDPG, TD3,
SAC, and PPO, are presented. We aim to provide a fair
comparison of these DRL algorithms for energy systems optimal
scheduling problems. Results show DRL algorithms’ capability
of providing in real-time good-quality solutions, even in unseen
operational scenarios, when compared with a mathematical
programming model of the energy system optimal scheduling
problem. Nevertheless, in the case of large peak consumption,
these algorithms failed to provide feasible solutions, which can
impede their practical implementation.

Index Terms—Energy management, Machine learning, Deep

learning, Reinforcement learning,

Sets:
G

B

L

V

S

A

R

T

Indexes:
i

j

m

k

NOTATION

Set of (DGs) distributed generators
Set of ESSs
Set of Loads
Set of PVs
Set of states of the RL algorithms
Set of actions
Set of rewards
Set of time steps

DG unit i ∈ G
ESS j ∈ B
PV unit m ∈ V
Load demand m ∈ L
Time-step t ∈ T

t
Parameters:
ai bi ci Quadratic, linear and constant parameters associated

∆t

to the i-th DG operation cost
Length for the discretization of the operational time

978-1-6654-8032-1/22/$31.00 ©2022 European Union

P

E

P

β

B
j EB
C

ηB

σ1 σ2

λ

P

Discount factor

G
t P G

i Maximum and minimum generation limit of the DG

units

RUi RDi Ramping up and ramping down ability of the DG

units

B
j P B

j Maximum and minimum charging/discharging limit

of the ESSs

j Maximum and minimum level of SOC of the ESSs

Maximum main grid export/import limit
Electricity sell coefﬁcient
Energy exchange efﬁciency for ESSs
Reward re-scale and constrain penalty coefﬁcients
Electricity price for time slot t
Active power of PV systems
Active power demand

ρt
P V
m,t
P L
k,t
Continuous Variables:
P G
i,t
P B
j,t
SOC B
P N
t
∆Pt
Binary Variables:

Active power output of DG units
Active power discharge/charge of ESSs

j,t State of charge for ESSs

Active power exported/imported of main grid
Active power unbalance

ui,t

Operational state of the DG units

I. INTRODUCTION

The massive integration of renewable-based resources inher-
ently increases the complexity of the energy systems’ schedul-
ing problem [1]. Existing scheduling approaches are not ad-
equate to deal with a large number of distributed generation
(DG) units and the increasing level of uncertainty. Innovative
approaches are urgently needed, capable of providing in real-
time good-quality solutions while still enforcing operational
and technical constraints. In the technical literature, two main
approaches are available to deal with the energy system’s un-
certainty, namely model-based and model-free approaches [2].
In model-based approaches, uncertainty is considered either
by using a probability distribution function or by leveraging
a set of representative scenarios, constructing a complex
mathematical formulation considering the system’s operational
constraints. This formulation leads to a stochastic multi-stage

 
 
 
 
 
 
mathematical programming model that can be solved using
commercial mixed-integer nonlinear programming (MINLP)
solvers [3], [4], such as CPLEX. Nevertheless, although capa-
ble of providing good quality solutions, existing model-based
approaches are not adequate for real-time operation due to the
large computational time required.

To overcome this, model-free approaches have been intro-
duced as an alternative solution. The most promising model-
free approach is based on the use of reinforcement learning
(RL) [5]. In this, by modeling the decision-making problem as
a Markov Decision Process (MDP), RL algorithms can learn
the system’s dynamics by interaction, providing good-quality
solutions guided by a reward value used as a performance
indicator [6]. The main advantage of RL-based algorithms
is that, if properly designed and trained, they can capture
energy systems’ uncertainty by leveraging historical data,
providing good-quality solutions in real-time, and avoiding any
computational burden during operation.

Recently, Deep reinforcement learning (DRL), using Deep
Neural Networks (DNN) as function approximators, has shown
good performance on solving different energy-related schedul-
ing problems with continuous actions, including home appli-
ances [7] and microgrids scheduling [8]. Unfortunately, these
works rely on value-based RL algorithms, which do not scale
well. Instead, policy-based DRL algorithms, by integrating an
actor-critic structure, have shown outstanding performance on
complex MDP tasks such as robot control and video games [9].
The current DRL state-of-the-art includes algorithms such as
Proximal Policy Optimization (PPO) [10], Soft Actor-Critic
(SAC), Policy optimization [11], Deep Deterministic Policy
Gradient (DDPG) [12] and derivation (TD3) [13].

Unlike general MDP tasks,

the energy system optimal
scheduling problem, formulated as an MDP, must enforce a
rigorous set of operational constraints to ensure a reliable
operation. For instance, the power balance constraint, modeled
as an equality constraint, must always be met during real-time
operation. Enforcing equality constraints in DRL algorithms
is currently an unresolved challenge. Several works indirectly
enforced the power balance constraint, for example, by deﬁn-
ing a speciﬁc DG unit as an inﬁnite source of power [14]
or by adding a penalty term to the reward function [15],
[16]. Nevertheless, this introduces extra hyperparameters that
impact the DRL algorithms’ performance and capability of
providing feasible solutions. Given the challenge mentioned
above, in this paper, we aim to provide a fair performance
comparison of DRL algorithms for energy systems optimal
scheduling problems. To do this, we compared DDPG, TD3,
SAC, and PPO algorithms’ performance and performed a
sensibility analysis of hyperparameters on the algorithms’ ca-
pabilities of providing good-quality solutions, even in unseen
operational scenarios.

II. MATHEMATICAL PROGRAMMING FORMULATION

The energy systems optimal scheduling problem can be
modeled as the the MINLP model given by (1)-(12). The
objective function in (1) aims at minimizing the operational

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

(9)

cost for the whole time horizon T , which comprises the
operational cost of the DG units, as presented in (2) and the
cost of buying/selling electricity from/to the main grid, as in
(3). Given the output power of DG units P G
i,t, the operational
cost can be estimated by using a quadratic model as in (2).
The transaction cost between the energy system and the main
grid is settled according Time-of-Use (TOU) prices, in which
it is assumed that selling prices are lower than the purchasing
prices. In (3), ρt is the TOU price at time slot t, while P N
t
refers to the exported/imported power transaction to/from the
main grid.

min

CG

i,t = ai ·

t∈T
X
P G
i,t

(CG

i,t + CE

t )∆t

i∈G
X
2

+ bi · P G

i,t + ci,

i ∈ G.

(cid:0)
t =

CE

(cid:1)
ρtP N
t
βρtP N
t

(

P N
P N

t > 0,
t < 0.

Subject to:
P G

i,t +

i∈G
X

m∈V
X

m,t + P N
P V

t +

P B

j,t =

P L

k,t, ∀t ∈ T

j∈B
X

k∈L
X

G
i

· ui,t ≤ P G

· ui,t

i,t ≤ P
i,t−1 ≤ RUi
i,t+1 ≤ RDi
B
j ≤ P B
j,t ≤ P
j
j,t = SOCj,t−1 + ηBP B

P G
i
P G
i,t − P G
P G
i,t − P G
− P B
SOCB
EB

B
j,t ≤ E
j
C

t ≤ P

C

j ≤ SOCB
≤ P N
− P
ui,t ∈ {0, 1}

j,t∆t

∀i ∈ G, ∀t ∈ T

∀i ∈ G, ∀t ∈ T

∀i ∈ G, ∀t ∈ T

∀j ∈ B, ∀t ∈ T

∀j ∈ B, ∀t ∈ T

∀j ∈ B, ∀t ∈ T (10)

∀t ∈ T (11)
∀i ∈ G, ∀t ∈ T (12)

Expression (4) deﬁnes the power balance constrain. A
binary variable is used to model the DG unit’s commitment
status, i.e., ui,t = 1 represent that the i-th DG unit is operating.
Expression (5) deﬁnes the DG units generation power limits
while (6) and (7) enforce the DG unit’s ramping up and down
constraints, respectively. Energy storage systems (ESSs) are
modeled using (8)-(10). In this model, the operation cost of
ESSs is not considered, while ESSs are allowed to schedule
their discharge and charge power in advance. Expression
(8) deﬁnes the charging and discharging power limits, while
expression (9) models the state of charge (SOC) as a function
of the charging and discharging power. Expression in (10)
limits the energy stored in the ESSs, avoiding the impacts
caused by over-charging and over-discharging. Finally, main
grid export/import power limit is modeled by the expression
in (11).

III. MDP FORMULATION

The above-presented problem can be formulated as a MDP,
which can be represented as a 5-tuple (S, A, P, R, λ), where
S represents the set of system states, A the set of actions,
P the state transition probability, R the reward function, and

t , P G

t , P L

λ the discount factor. In this formulation, the energy system
operator can be modeled as an RL agent. The state information
provides an important basis for the operator to dispatch units.
We deﬁne a state as st = (P V
st ∈ S,
while the actions, deﬁning the scheduling of the DG units and
the ESSs, as at = (P G
at ∈ A. Notice that the RL
agent does not directly control the transaction between the
energy system and the main grid. Instead, after any action is
executed, power is exported/imported from the main grid to
maintain the power balance. Nevertheless, a maximum power
capacity constraint exits and must be enforced i.e., (11).

t−1, SOCt),

i,t, P B

t ),

Given the state st and action at at time step t, the energy

system transit to the next state st+1 deﬁned by

ss′ = Pr {st+1 = s′ | st = s, at = a}
P a

(13)

where P a
ss′ corresponds to the transition probability, which
models the energy system’s dynamics and uncertainty in-
volved. In model-based algorithms, the uncertainty is predicted
by a determined value or sampling from a prior probability
distribution. Instead, DRL is a model-free approach, capable
of learning the uncertainty and dynamics from historical data
and interactions.

The reward rt is given by the environment as an indicator
to guide update direction of the policy. In the energy systems
optimal scheduling problem, the reward function should guide
the RL agent to take actions that minimize the operational
cost while enforcing the power balance constraint. This can
be done by using the next reward function

rt (st, at) = −σ1

"

CG

i,t + CE
t

#

− σ2∆Pt,

(14)

i∈G
X

in which ∆P corresponds to the power unbalance at time-step
t and is deﬁned as,

∆Pt = |

P G

i,t +

P V
m,t +P N

t +

P B

j,t −

P L

k,t| (15)

i∈G
X

m∈V
X

j∈B
X

k∈L
X

while σ2 is used to control the trade-off between the cost
minimization and the penalty incurred in case of power
unbalance. The goal of the RL algorithms is to ﬁnd an optimal
scheduling policy π∗(stochastic or deterministic) to maximize
the total expected discounted rewards for the formulated MDP

π∗ = arg max

π

E

(st,at)∼ρπ

R (st, at)

#

.

(16)

"

t∈T
X

IV. DRL POLICY-BASED ALGORITHMS

The formulated MDP have a continuous state and action
spaces, which can be hard to solve by using classical RL
algorithms, such as Q learning, due to their poor scalabil-
ity features [6]. By leveraging the generalization and ﬁtting
capability of DNNs, DRL algorithms have shown good per-
formance when dealing with this challenge. In valued-based
the action-state function Q is iteratively
DRL algorithms,
updated to indirectly deﬁne a deterministic policy, for which
the foundation is Bellman optimality equation:

Q∗(s, a) = R(s, a)+γ

P (s′ | s, a) max
a′∈A

Q∗ (s′, a′) (17)

If

s′∈S
X
the optimal value function Q∗(s, a)

is estimated,
then the optimal policy can be derived as π∗(s) =
arg maxa∈A Q∗(s, a). Value-based DRL algorithms use DNNs
approximate the Q-function, dealing with continuous state
spaces. However, for continuous action MDP problems, it
requires a full scan of the action space when executing
policy improvement i.e., arg maxa∈A Qπ(s, a), leading to a
dimensionality problem. Instead, policy-based DRL algorithms
directly search for the optimal policy. The policy is usually
modeled with a parametric function, denoted as πθ(s|a). Based
on the policy gradient theorem, the policy gradient is expressed
as ∇θJ(θ) = Eπ [Qπ(s, a)∇θ ln πθ(a | s)], where J(θ) is
the reward function. Using gradient ascent, θ can be updated
towards the direction suggested by ∇θJ(θ) to ﬁnd the policy
πθ that lead the highest total discounted rewards.

In this paper, we will compare the performance of state-
of-the-art DRL algorithms, including DDPG, TD3, SAC, PPO
when solving the MDP described in Sec. III, using a penalty to
enforce the power balance constrain, as showed in (14). DDPG
and TD3 are off-policy, deterministic algorithms, while PPO
and SAC are stochastic algorithms. In general, DRL algorithms
interact with the environment to collect a reward. This reward
is then used to update a critic DNNs parameters based on the
temporal difference (TD) algorithm. Then, the critic network
is used to update actor DNNs parameters based on policy
gradient theory. A more detailed explanation of policy-based
algorithms can be found in [5].

V. EXPERIMENTAL RESULTS

A. Experimental Setting

Three DG units, using the parameters shown in Table I,
are deﬁned. For the ESSs, the charging and discharging limits
are set as 100 kW, the nominal capacity as 500 kW, while
the energy efﬁciency is set as ηB = 0.9. We assume that
the grid’s maximum export/import limit is deﬁned as 100
kW. To encourage the use of renewable energies, we set
selling prices as half of the current electricity prices, i.e.,
β = 0.5. One-hour resolution data proﬁles including solar
generation, demand consumption, and electricity prices for a
period of one year are used. The original data is divided into
training and testing sets. The training set contains the ﬁrst
three weeks of each month, while the testing sets contain the
remaining data. This strategy allows the DRL algorithm to
consider seasonal changes in the PV generation and demand
consumption. During training, the EESs’ initial SOC were
randomly set. All algorithms were implemented in Python
using PyTorch and trained for 1000 episodes. Unless otherwise
mentioned, default settings were used. The implemented DRL
algorithms and the environment are openly available at [17].
Hyperparameters σ1 and σ2 were deﬁned as 0.01 and 50,
respectively, as default values. Each experiment is run with
ﬁve random seeds to eliminate randomness from the code
implementation part during the simulation process. To evaluate

Table I
DG UNITS INFORMATION

Units

DG1

DG2

DG3

a[$/kW2]

b[$/kW]

c[$]

P G[kW]

G

P

[kW] RU [kW] RD[kW]

0.0034

0.001

0.001

3

10

15

30

40

70

10

50

100

150

375

500

100

100

200

100

100

200

the DRL algorithms’ performance, the total operational cost,
as in (1), and the power unbalance as in (15), are used as
metrics. To validate and compare the performance of the tested
DRL algorithms, we have also solved the MIQP formulation
presented in Sec. II using Pyomo [18].

B. Performance on the Training Set

Fig. 1 shows the average reward, losses, operational cost,
and power unbalance for all the DRL algorithms during the
training process. Reward increased rapidly after training for
100 episodes, while loss decreased to a small value after 200
episodes. This trend is due to the fact that the parameters
of all algorithms are initialized randomly, leading to random
actions. As a result, a high power unbalance is observed. As
the training continues, all DRL agents learn how to meet the
power unbalance constraint due to the penalty term used in
the reward function. The comparison of the four algorithms
shows that the PPO and SAC algorithms are more stable
when compared with the DDPG and TD3 algorithms. Not
only do PPO and SAC algorithms have a higher reward after
training, but they also have a lower variance. In the end, all
algorithms converged to similar values. However, DDPG and
TD3 algorithms showed a signiﬁcant performance decrease
after training for 800 episodes. For PPO and SAC algorithms,
the power unbalance is mitigated notably after training for
100 episodes and nearly disappeared at 1000 episodes, while
DDPG and TD3 algorithms show that power unbalances
decrease notably after 200 episodes and then remain after
training 1000 episodes, at around 500-700 kW.

Fig. 2(a) shows the cumulative operational cost for 10 days
in the training set. All tested DRL algorithms have a similar
performance compared to MIQP. The solution provided by the
MIQP formulation is treated as the optimal solution. Notice
that some of the DRL algorithms have outperformed the solu-
tion provided by the MIQP formulation in some days in terms
of the operational cost. Nevertheless, this is because they did
not strictly meet the power balance constraint. Fig. 2(b) shows
that the cumulative power unbalances of 10 days is less than
1200 kW, accounting for no more than 5% of the total demand.
Notice that the PPO and the TD3 algorithms maintained the
lowest and largest power unbalance, respectively, among all
algorithms.

C. Performance on the Test Set

Table II and Table III shows the operational cost and the
power unbalance, respectively, with 95% conﬁdence intervals,
tested on 10 days on the test set. As can be seen, DRL algo-
rithms show similar performance compared with the optimal
solutions provided by the MIQP formulation. Among these,

the PPO and TD3 algorithms over-perform the DDPG and
SAC algorithms regarding the operational cost. Notice that
all DRL algorithms were able to operate with a low power
unbalance on most of the days from the test set. Nevertheless,
all these algorithms failed to maintain the power unbalance
test day due to the signiﬁcantly high demand
in the last
in
consumption around peak time. This result shows that
the case of extreme events (e.g., high demand consumption,
low renewable generation, etc.), perhaps not captured on the
trained data, DRL algorithms fail to provide a feasible solu-
tion. These results are important as larger power unbalances
can lead to an outage, especially if an export limit is deﬁned
with the main grid.

Fig. 3 show the ESSs’ SOC and the output power of the
DG units, ESSs, and the export/import power from the grid
deﬁned using the PPO algorithm and the optimal solution
provided by the MIQP formulation. As can be seen, when
the electricity price is high and the net power is low, the PPO
algorithm dispatches the ESSs in charging mode, while around
peak-time, the ESSs is dispatched in discharging mode. A
similar operational schedule is deﬁned by the optimal solution
provided by the MIQP formulation. As can be seen in Fig. 3,
at 12h, the DG unit 1 and the ESSs operating in discharge
mode play the most critical role in meeting all the demand.
A different operational schedule was deﬁned by the PPO
algorithms for the same time step, in which the DG unit 2
and the main grid supply all the demand consumption. In this
regard, as the schedule deﬁned by the PPO algorithm did not
prioritize the use of the ESSs, a higher operational cost was
observed.

D. Sensitivity Analysis

Enforcing technical constraints using a penalty term in the
reward function is one of the most common approaches used
to provide feasible solutions. As previously mentioned, this
approach introduces extra hyperparameters, in this case, to
balance the numeric relationship between the operational cost
and the power balance constraint. Additionally, as the grid ex-
port/import power limit provide ﬂexibility for the actions taken
by DRL algorithms, this can signiﬁcantly impact the feasibility
of the provided solutions. In this section, a sensitivity analysis
of the penalty coefﬁcient, σ2, as used in expression (14), is
presented.

C

Fig. 4 shows the convergence performance of all the tested
DRL algorithms for different values of the coefﬁcient σ2 con-
sidering P
= 100 kW. As can be seen, the operational cost
increased signiﬁcantly when increasing σ2 from 20 to 50 and
then reduced after increasing it from 50 to 100. On the other
hand, as expected, increasing σ2 notably reduced the power
unbalance. In this case, if σ2 is set as 100, the PPO and SAC
algorithms were able to almost completely mitigate the power
unbalance. Nevertheless, the DDPG and TD3 algorithms were
able to minimize the power unbalance for all values of σ2,
which allows concluding that such algorithms are less sensitive
to this hyperparameter when compared with the PPO and SAC
algorithms.

]
-
[
d
r
a
w
e
R

-

-50

-75

-100

-125

-150

Figure 1. Average (a) reward, (b) operational cost, and (c) power unbalance, during 1000 episode training for all the tested DRL algorithms.

Table II
MEAN AND 95% CONFIDENCE BOUNDS FOR COST[$]

Test day
1
2
3
4
5
6
7
8
9
10

DDPG
21142 (20670, 21613)
11364 (10784, 11943)
9539 (9162, 9915)
3292 (1886, 4697)
23128 (21024, 25232)
11266 (10774, 11758)
4046 (2608, 5484)
11219 (10345, 12092)
7349 (5838, 8860)
26285 (24625, 27944)

TD3
19810 (19332, 20287)
9824 (9306, 10342)
8868 (8442, 9293)
5120 (4742, 5496)
23038 (22247, 23829)
15631 (15169, 16092)
4901 (4635, 5166)
9761 (9424, 10097)
7662 (7190, 8134)
24050 (23857, 24242)

SAC
21907 (21445, 22368)
11084 (8246, 13921)
10362 (8932, 11792)
2502 (1469, 3534)
20146 (17179, 23112)
13345 (12197, 14491)
2766 (1636, 3894)
12019 (8803, 15233)
7943 (6017, 9869)
23303 (22167, 24439)

PPO
17637 (14385, 20888)
10833 (9714, 11924)
10214 (8980, 11446)
2478 (1947, 3009)
19045 (17399, 20690)
12844 (11018, 14669)
3076 (2254, 3839)
10778 (8762, 12792)
7250 (5881, 8619)
22614 (20087, 25141)

MIQP
18145
8358
7417
1046
14971
8085
1652
7126
5053
23452

Table III
MEAN AND 95% CONFIDENCE BOUNDS FOR UNBALANCE POWER [KW]

Test day
1
2
3
4
5
6
7
8
9
10

DDPG
189.89 (189.65, 190.14)
5.45 (0, 23.46)
74.16 (34.41, 113.92)
2.27 (0, 12.64)
10.85 (8.31, 13.39)
32.34 (32.34, 32.34)
0.07 (0, 0.48)
64.61 (33.82, 95.41)
33.88 (10.52, 57.24)
517.19 (506.65, 527.74)

TD3
246.14 (245.52, 246.76)
12.63 (12.63, 12.63)
49.43 (49.43, 49.43)
37.42 (31.28, 43.57)
73.40 (73.18, 73.62)
0.67 (0, 3.59)
0.00 (0, 0)
107.24 (107.24, 107.24)
19.81 (19.74, 19.89)
436.86 (433.39, 440.33)

SAC
225.56 (224.08, 227.04)
8.30 (0, 39.89)
25.73 (0, 51.79)
0.01 (0, 0.03)
100.75 (70.38, 131.11)
50.05 (35.37, 64.73)
37.72 (37.72, 37.72)
32.38 (22.84, 41.91)
0.09 (0, 0.63)
450.06 (434.73, 465.39)

PPO
171.27 (29.19, 313.35)
27.94 (0, 66.88)
36.12 (0, 110.21)
36.70 (0.75, 72.65)
67.81 (0, 185.54)
20.52 (0, 84.23)
3.38 (0, 13.12)
66.00 (14.92, 117.08)
28.51 (0, 78.86)
410.11 (331.57, 488.66)

(a)

(b)

Figure 2. Cumulative operational cost and power unbalance for 10 days

VI. CONCLUSION

In this paper, the performance of policy-based DRL al-
gorithms (DDPG, TD3, SAC, PPO) has been evaluated for
the energy systems optimal scheduling problem. We provided
a detailed sensitivity analysis for the penalty terms used
to enforce the power balance constraint and minimize the
operational cost
in the reward function deﬁnition. Results
showed that DRL algorithms are able to provide good quality
solutions in real-time by exploiting the good generalization
capabilities of deep learning models. In general, we observed
that the PPO algorithm outperforms DDPG, SAC, and TD3
algorithms. The penalty coefﬁcient used to balance the rela-

tionship between the operational cost and enforce the power
balance constraint signiﬁcantly impacted the performance of
all DRL algorithms. Additionally, we observed that increasing
the grid export/import power limit increases the ﬂexibility of
actions taken by algorithms (capability of providing feasible
actions solutions). Nevertheless, all the tested DRL algorithms
lack safety guarantees compared to model-based approaches,
which could impede their practical implementation. In this
regard, new approaches to enforce operational constraints in
DRL algorithms are urgently needed.

ACKNOWLEDGMENT

This work made use of the Dutch national e-infrastructure
with the support of the SURF Cooperative (grant no. EINF-
2684) and was supported by the Chinese Scholarship Council
(CSC) (grant no. 202106660002).

REFERENCES

[1] D. E. Olivares, C. A. Cañizares, and M. Kazerani, “A centralized optimal
energy management system for microgrids,” in 2011 IEEE Power and
Energy Society General Meeting.

IEEE, 2011, pp. 1–6.

[2] M. F. Zia, E. Elbouchikhi, and M. Benbouzid, “Microgrids energy
management systems: A critical review on methods, solutions, and
prospects,” Applied energy, vol. 222, pp. 1033–1055, 2018.

 
Price

MIQP

PPO

e
c
i
r
P

Netload
DG 1
DG 2
DG 3
ESS Charge
ESS Discharge
Import 
Export

C
O
S

]

W
k
[

r
e
w
o
P

Netload
DG 1
DG 2
DG 3
ESS Charge
ESS Discharge
Import 
Export

]

W
k
[

r
e
w
o
P

Time [h]
(a)

Time [h]
(b)

Time [h]
(c)

Figure 3. Operational schedule of all DG units and ESSs: (a) and (b) PPO algorithm, (b) and (c) MIQP formulation.

3(cid:0)(cid:1)

200

1(cid:2)(cid:3)

PSfrag replacements
ρ2 = 20
ρ2 = 50
ρ2 = 100

Figure 4. Sensitive analysis for σ2 = (20, 50, 100), (a, b, c) and (d, e, f) refer to the cost and power unbalance corresponding to 20, 50, 100, respectively.

[3] F. Haﬁz, A. R. de Queiroz, P. Fajri, and I. Husain, “Energy management
and optimal storage sizing for a shared community: A multi-stage
stochastic programming approach,” Applied energy, vol. 236, pp. 42–
54, 2019.

[4] P. P. Vergara, M. Salazar, J. S. Giraldo, and P. Palensky, “Optimal
dispatch of pv inverters in unbalanced distribution systems using rein-
forcement learning,” International Journal of Electrical Power & Energy
Systems, vol. 136, p. 107628, 2022.

[5] X. Chen, G. Qu, Y. Tang, S. Low, and N. Li, “Reinforcement learning
for decision-making and control in power systems: Tutorial, review, and
vision,” arXiv preprint arXiv:2102.01168, 2021.

[6] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press, 2018.

[7] T. A. Nakabi and P. Toivanen, “Deep reinforcement learning for energy
management in a microgrid with ﬂexible demand,” Sustainable Energy,
Grids and Networks, vol. 25, p. 100413, 2021.

[8] Y. Ji, J. Wang, J. Xu, X. Fang, and H. Zhang, “Real-time energy ma-
nagement of a microgrid using deep reinforcement learning,” Energies,
vol. 12, no. 12, p. 2291, 2019.

[9] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-
dimensional continuous control using generalized advantage estimation,”
arXiv preprint arXiv:1506.02438, 2015.

[10] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,
2017.

[11] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic
actor,” in International conference on machine learning. PMLR, 2018,
pp. 1861–1870.

[12] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,

D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” arXiv preprint arXiv:1509.02971, 2015.

[13] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approxi-
mation error in actor-critic methods,” in International conference on
machine learning. PMLR, 2018, pp. 1587–1596.

[14] J. R. Vázquez-Canteli, S. Dey, G. Henze, and Z. Nagy, “Citylearn:
Standardizing research in multi-agent
learning for
demand response and urban energy management,” arXiv preprint
arXiv:2012.10504, 2020.

reinforcement

[15] Y. Ji, J. Wang, J. Xu, and D. Li, “Data-driven online energy scheduling
of a microgrid based on deep reinforcement learning,” Energies, vol. 14,
no. 8, p. 2120, 2021.

[16] S. Zhou, Z. Hu, W. Gu, M. Jiang, M. Chen, Q. Hong, and C. Booth,
“Combined heat and power system intelligent economic dispatch: A deep
journal of electrical
reinforcement
power & energy systems, vol. 120, p. 106016, 2020.

learning approach,” International

[17] H.

Shengren,

2022,

https://github.com/ShengrenHou/

DRL-for-Energy-Systems-Optimal-Scheduling.

[18] W. E. Hart, C. D. Laird, J.-P. Watson, D. L. Woodruff, G. A. Hackebeil,
B. L. Nicholson, J. D. Siirola et al., Pyomo-optimization modeling in
python. Springer, 2017, vol. 67.

 
 
