0
2
0
2

l
u
J

6

]

G
L
.
s
c
[

1
v
6
4
0
3
0
.
7
0
0
2
:
v
i
X
r
a

Machine Learning with the Sugeno Integral:
The Case of Binary Classiﬁcation

Sadegh Abbaszadeh and Eyke H¨ullermeier

Paderborn University
Heinz Nixdorf Institute and Department of Computer Science
Intelligent Systems and Machine Learning Group
sadegh.abbaszadeh@uni-paderborn.de, eyke@upb.de

Abstract. In this paper, we elaborate on the use of the Sugeno integral in the context of
machine learning. More speciﬁcally, we propose a method for binary classiﬁcation, in which
the Sugeno integral is used as an aggregation function that combines several local evalua-
tions of an instance, pertaining to diﬀerent features or measurements, into a single global
evaluation. Due to the speciﬁc nature of the Sugeno integral, this approach is especially
suitable for learning from ordinal data, that is, when measurements are taken from ordinal
scales. This is a topic that has not received much attention in machine learning so far. The
core of the learning problem itself consists of identifying the capacity underlying the Sugeno
integral. To tackle this problem, we develop an algorithm based on linear programming.
The algorithm also includes a suitable technique for transforming the original feature values
into local evaluations (local utility scores), as well as a method for tuning a threshold on
the global evaluation. To control the ﬂexibility of the classiﬁer and mitigate the problem
of overﬁtting the training data, we generalize our approach toward k-maxitive capacities,
where k plays the role of a hyper-parameter of the learner. We present experimental studies,
in which we compare our method with competing approaches on several benchmark data sets.

Key words: Machine learning, binary classiﬁcation, Sugeno integral, aggregation, non-additive
measures

1 Introduction

The idea of combining models and aggregation functions from the ﬁeld of (multi-criteria)
decision making with data-driven approaches for model identiﬁcation from the ﬁeld of ma-
chine learning has attracted increasing attention in recent years. Examples of such combi-
nations include methods for learning the majority rule model [17], the non-compensatory
sorting model [18], or the Choquet integral [21]. In contrast to many other machine learn-
ing approaches, corresponding models are interpretable and meaningful from of decision
making point of view, a property that has gained increasing attention in the recent past
[6,1]. Besides, they often guarantee other properties that might be desirable, such as mono-
tonicity.

The general structure of such models, sketched in Fig. 1, is as follows: Given a choice
alternative described in terms of an attribute vector x = (x1, . . . , xm), each attribute xi is
ﬁrst evaluated by means of a local utility function, and thereby turned into a local utility
degree ui = ui(xi) ∈ R — it corresponds to what is called a “criterion” in multi-criteria
decision analysis. In a second step, the local utility degrees u1, . . . , um are aggregated into
a global utility U = U (u1, . . . , um). Finally, a decision or an action y is taken based on
this utility. We refer to this setting as multi-criteria machine learning (MCML).

 
 
 
 
 
 
Fig. 1: Structure of a multi-criteria machine learning model.

Our work is motivated by recent contributions, in which the aggregation step is accom-
plished by means of the (dicrete) Choquet integral [21,23,12,22]. As a versatile aggregation
function, the Choquet integral has a number of properties that are quite appealing from
a machine learning point of view. For example, it allows for combining non-linearity with
monotonicity, i.e., to model nonlinear yet monotone dependencies between input attributes
(criteria) and outcomes (global utilities, decisions). Besides, it is able to capture complex
interactions between diﬀerent input attributes.

In this paper, we consider the MCML setting with the Sugeno integral [20] instead of the
Choquet integral as an aggregation function. The former can be seen as the qualitative
counterpart of the latter: As it operates on a purely ordinal (instead of a numerical) scale,
it appears to be speciﬁcally suitable for learning from ordinal data. Moreover, whereas
machine learning methods based on the Choquet integral can be seen as generalizations of
learning linear models, methods based on the Sugeno integral are conceptually similar to
symbolic, logic-based model classes such as decision trees and rule-based models, in which
predictions are obtained by testing properties of the attribute values xi of an instance
x, typically comparing them with certain thresholds, but without doing any numerical
computations with them.

In the next section, we recall the deﬁnition of the Sugeno integral and some of its basic
properties. In Section 3, we introduce a class of binary classiﬁers, which are based on
thresholding the Sugeno integral, and analyze the ﬂexibility of this model class in terms
of its VC dimension. A method for learning binary classiﬁers of that kind is introduced
in Section 4. Section 5 addresses the idea of controlling the degree of maxitivity of the
Sugeno integral as a means for adjusting the model ﬂexibility to the complexity of the
data, thereby avoiding the problem of overﬁtting the training data. Section 6 presents the
results of an experimental study, prior to concluding the paper in Section 7.

AGGREGATIONlocalutilityfunctionsglobal	utilityDECISIONlocalutilityfunctions2 The Sugeno Integral

Recall that a set function on [m] ..= {1, . . . , m} is a real-valued function deﬁned on all
subsets of [m]. A capacity on [m] is a set function µ : 2[m] → R that preserves monotonicity,
i.e., such that µ(A) ≤ µ(B) for all A, B ∈ 2[m] with A ⊆ B. The capacity µ is normalized
if µ([m]) = 1, and additive if µ(A ∪ B) = µ(A) + µ(B) for any disjoint subsets A, B ⊆ [m].
Likewise, it is called “maxitive” if µ(A ∪ B) = µ(A) ∨ µ(B) = max(µ(A), µ(B)) for any
subsets A, B ⊆ [m] [9, p. 173].

2.1 Deﬁnition of the Sugeno Integral

The Sugeno integral is an aggregation function deﬁned with respect to a capacity, i.e., it
combines a set of values u1, . . . , um pertaining to m criteria into a single representative
value, accounting for the importance of subsets of criteria as speciﬁed by the capacity. Since
the values ui are measured on an ordinal scale, the Sugeno integral relies on disjunctive
and conjunctive operations (instead of addition and multiplication). Formally, it can be
expressed in various ways [9]. A common deﬁnition of the Sugeno integral with respect to
the capacity µ on [m] is as follows:

Sµ(u) = Sµ(u1, . . . , um) =

m
(cid:95)

j=1

(cid:0)uσ(j) ∧ µ (cid:0)Aσ(j)

(cid:1)(cid:1) ,

(1)

where σ is a permutation on [m] such that uσ(1) ≤ uσ(2) ≤ . . . ≤ uσ(m) and Aσ(j) =
{σ(j), σ(j + 1), . . . , σ(m)}. Since the Sugeno integral is a particular weighted lattice poly-
nomial function, its disjunctive normal representation is given by

Sµ(u) =

(cid:95)





(cid:94)

A⊆[m]

j∈A



uj ∧ µ(A)

 .

(2)

Considering disjunction and conjunction as mathematical formalizations of existential and
universal quantiﬁcation, respectively, this representation suggests the following interpre-
tation: The Sugeno integral Sµ(u) is high if (and only if) there exists a subset of criteria
A, such that A has high importance and all values ui on these criteria are high. In other
words, µ(A) is a measure of suﬃciency of the criteria A: Satisfying all criteria in A (i.e.,
achieving high utilities) is enough to achieve an overall high utility.

Another important representation of the Sugeno integral, which we will exploit later on,
is a deﬁnition in terms of a median [9, p. 213]:

Sµ(u) = Med

(cid:16)

u1, . . . , um, µ (cid:0)Aσ(2)

(cid:1) , . . . , µ (cid:0)Aσ(m)

(cid:1) (cid:17)

Thus, the Sugeno integral can be obtained by sorting the 2m − 1 values, which are given
as arguments to Med in the above expression, from smallest to largest, and then taking
the value at position m in this sorted list.

2.2 The k-maxitive Sugeno Integral

With a measure (capacity) µ, interactions and dependencies between criteria can be mod-
eled in a very ﬂexible way. An obvious drawback, however, is the exponential complexity
implied by this approach: The speciﬁcation of a capacity requires a value µ(A) for each
A ⊆ [m]. In the case of the Choquet integral, it has therefore been suggested to trade
complexity against expressivity by working with k-additive measures, which essentially
means capturing interactions between criteria up to a degree of k [21,12]. Practically, the
full expressivity of a capacity is indeed rarely needed — on the contrary, from a machine
learning point of view it may even be harmful, due to an increased danger of overﬁtting
the training data [21]. Instead, values such as k = 2 or k = 3 will typically suﬃce. The
case k = 2 is especially interesting, and the jump in performance from k = 1 to k = 2 is
often the highest. This is because, whereas k = 1 is not able to capture any dependencies,
k = 2 is able to capture pairwise dependencies, and thus dependencies of higher order at
least indirectly.

The qualitative analogue of k-additivity is k-maxitivity [14,3]. Formally, the notions of k-
maxitive capacity and k-maxitive aggregation function are deﬁned as follows: A capacity
µ is called k-maxitive if for any subset U ⊆ [m] with |U | > k, there exists a proper subset
V of U such that µ(V ) = µ(U ). For k > 1, µ is called proper k-maxitive if it is k-maxitive
but not (k − 1)-maxitive. Note that the k-maxitivity of a capacity µ can be characterized
equivalently by the following condition:

µ(U ) =

(cid:95)

V ⊂U

µ(V ) whenever

|U | > k

A Sugeno integral Sµ is k-maxitive, if the underlying measure µ is k-maxitive. Regarding
the issue of complexity, note that a k-maxitive capacity requires the speciﬁcation of

k
(cid:88)

i=1

(cid:19)

(cid:18)m
i

values µ(A), which, for small to moderate k, is substantially less than the 2m − 2 values
needed for the general case, and remains polynomial in k.

3 The Sugeno Integral for Binary Classiﬁcation

A binary classiﬁer is a map h : X → {0, 1}, where X is a so-called instance space; here,
we make the common assumption that instances are described in terms of attributes or
features, i.e., we assume X = X1 × . . . × Xm, where Xi is the domain of the ith attribute.
Thus, a binary classiﬁer accepts any instance x ∈ X as an input, and either assigns it to
the negative (h(x) = 0) or to the positive class (h(x) = 1). The learning task essentially
consists of choosing an appropriate classiﬁer h from an underlying hypothesis space H,
given a set of training data — we will come back to this task in Section 4 below.

We are interested in hypotheses that are expressed in terms of the Sugeno integral. More
speciﬁcally, we consider a hypothesis space H consisting of threshold classiﬁers of the

following form, which we simply refer to as “Sugeno classiﬁers”:

(cid:16)
h(x) = h(x1, . . . , xm) = I

Sµ(f (x)) ≥ β

(cid:17)

,

(3)

where I(·) is the indicator function and

f (x) = (cid:0)f1(x1), . . . , fm(xm)(cid:1) = (u1, . . . , um) ∈ [0, 1]m .

Thus, given an instance x = (x1, . . . , xm), a classiﬁcation is accomplished in three steps:

– First, each feature xi is turned into a local utility ui using the transformation fi.
– The local utilities, considered as criteria, are combined into an overall utility using the

Sugeno integral with capacity µ.

– The class assignment is done via thresholding, i.e., by comparing the overall utility

with a threshold β.

Note that a hypothesis h ∈ H is thus speciﬁed by three components: the transformation
f , the capacity µ, and the threshold β. The corresponding hypothesis space H is relatively
rich and allows for modeling classiﬁcation functions in a very ﬂexible way, especially if
the capacity µ can be chosen without any restrictions. In fact, we can prove the following
result about the VC dimension1 of H as deﬁned above (see Appendix A).

Theorem 1. The VC dimension of the hypothesis space H comprised of threshold classi-
ﬁers of the form (3) grows asymptotically at least as fast as 2m/

m.

√

It is also interesting to note that the class of Sugeno classiﬁers covers several types of
classiﬁers, which are commonly used in machine learning, as special cases. This includes,
for example, k-of-m classiﬁers, which assign an instance to the positive class if at least k
of the set of m criteria are fulﬁlled, and to the negative class otherwise. More speciﬁcally,
(3) can be specialized to this type of classiﬁer as follows:

– Features xi are transformed into binary utilities via fi : Xi → {0, 1}; thus, fi simply

distinguishes between “good” and “bad” values xi.

– The capacity µ is deﬁned by µ(A) = |A|/m.
– The threshold β takes the value k/m.

Another interesting setting is as follows:

– Again, features xi are transformed into binary utilities ui ∈ {0, 1}, suggesting that a

criterion is either satisﬁed or not.

– The capacity µ is speciﬁed by a set of subsets A1, . . . , AJ ⊆ [m] as follows: µ(A) = 1

if Aj ⊆ A for some j ∈ [J], and µ(A) = 0 otherwise.

– The threshold β takes the value 1/2.

1 The Vapnik-Chervonenkis (VC) dimension is an measure of ﬂexibility of a hypothesis space, which plays

an important role in generalization and statistical learning theory [24].

Here, each Aj = {i1, . . . , ij} can be thought of as a rule of the following form:

IF (ui1 = 1) AND . . . AND (uij = 1) THEN h = 1

The overall classiﬁcation is positive if at least one of the rules applies, and negative other-
wise. Models of this kind are closely related to monotone decision rules [5] and monotone
decision trees [15].

More generally, a Sugeno classiﬁer with threshold β can be interpreted as a logical formula,
namely as a disjunctive normal form:

h(x) =

(cid:95)

(cid:94)

I(cid:0)fi(xi) ≥ β(cid:1) ,

boundary sets A

j∈A

(4)

where a boundary set A is a subset A ⊆ [m] such that µ(A) ≥ β and µ(A(cid:48)) < β for all
A(cid:48) (cid:40) A, i.e., a “minimal” subset reaching the threshold β. Note that, for a given measure
µ, the set of boundary sets of [m] forms an antichain, which means that these sets are
non-redundant. Also note that, in the case of k-maxitive measures, the size of boundary
sets is at most k. Considering the threshold β as a kind of aspiration level, the condition
I(fi(xi) ≥ β) in (4) can be interpreted as “feature xi is satisfactory”. Correspondingly, the
Sugeno classiﬁer assigns the positive class if x is satisfactory on all features in at least one
boundary set of features A.

Proposition 1. The Sugeno classiﬁer (3) coincides with (4).

Proof. Suppose that h(x) = 1 according to (4). Thus, there exists a boundary set A such
that I(fj(xj) ≥ β), i.e., uj ≥ β for all j ∈ A. Now, consider the representation (2) of the
Sugeno integral. Since minj∈A uj ≥ β and µ(A) ≥ β, we have Sµ(x) ≥ β and h(x) = 1
according to (3).

Suppose that h(x) = 1 according to (3). Thus, according to (2), there exists at least one
subset A(cid:48) ⊆ [m] such that uj ≥ β for all j ∈ A(cid:48) and µ(A(cid:48)) ≥ β. Therefore, A(cid:48) is either a
boundary set or a superset of a boundary set, so that there exists a boundary set A ⊆ A(cid:48).
Moreover, since minj∈A uj ≥ minj∈A(cid:48) uj ≥ β, we conclude that h(x) = 1 according to (4).

4 Learning a Sugeno Classiﬁer

Consider the class of Sugeno classiﬁers, i.e., the hypothesis space H, as introduced in
the previous section. More speciﬁcally, following the idea of structural risk minimization,
we structure the space as follows: H1 ⊂ H2 ⊂ · · · ⊂ Hm, where Hk denotes the class
of Sugeno classiﬁers restricted to k-maxitive capacities. The parameter k will serve as a
hyper-parameter, i.e., as a parameter of the learning algorithm.

Given a set of training data of the form

D =

(cid:110)(cid:16)

x(i), y(i)(cid:17)(cid:111)n

i=1

⊂ (X × {0, 1})n ,

(5)

supposed to be generated i.i.d. (independent and identically distributed) according to an
underlying (though unknown) probability measure P on X ×{0, 1}, the task of the learning

(cid:90)

algorithm (or learner for short) is to ﬁnd a Sugeno classiﬁer h ∈ Hk with low risk (expected
loss)

R(h) =

(cid:96) (y, h(x)) d P(x, y) ,

(6)

X ×{0,1}

where (cid:96) is a loss function such as 0/1 loss ((cid:96)(y, ˆy) = 0 if y = ˆy and = 1 otherwise). Recall
that a classiﬁer is identiﬁed by the feature transformation f , the capacity µ, and the
threshold β, which all have to be determined on the basis of the data D. In the following,
we discuss these components one by one.

4.1 Feature Transformation

As already explained, feature transformation is meant to turn each feature (predictor
variable) into a criterion that is measured in the unit interval, that is, to replace each
feature value xi by a local utility score ui. More speciﬁcally, feature transformation is
assumed to assure monotonicity in the sense that higher values ui are better (more likely
to produce the positive class), as well as commensurability between the criteria.

Here, we make the assumption that monotonicity already holds for the original features, so
that the transformations fi can be monotone as well. More precisely, we assume that higher
values xi are better. If the opposite is the case (lower values are better), all values xi can
simply be replaced by −xi. Sometimes, the direction might not be be known beforehand,
and instead must be determined on the basis of the data. In Appendix B, we propose a
method for that purpose.

In our current approach, the transformations fi are determined in an unsupervised man-
ner, i.e., using the feature information x(1), . . . , x(n) in the training data D but not the
observed class labels y(1), . . . , y(n). Starting from “the higher the better” feature values,
transformation essentially comes down to normalizing the data, i.e., mapping the feature
values (isotonically) to the unit interval. To this end, we make use of a quantile-based
approach, that is, the idea to replace a value x by the probability P (X ≤ x), where P
is the underlying distribution of the feature. Since P is not known, it is replaced by the
empirical distribution function.
More precisely, the transformation fi of the ith feature is determined as follows. For a
suitable permutation σ on [n], denote by xi,σ(1) ≤ xi,σ(2) ≤ · · · ≤ xi,σ(n) the sorted list
of values that are observed for the ith feature in the training data D. We then deﬁne the
empirical distribution function fi : R → [0, 1] in terms of a piecewise linear interpolation
of the points (xi,σ(j), aj), where

aj =

1
2

(cid:16)(cid:12)
(cid:12){l | xi,σ(l) < xi,σ(j)}(cid:12)

(cid:12) + (cid:12)

(cid:17)
(cid:12){l | xi,σ(l) ≤ xi,σ(j)}(cid:12)
(cid:12)

Note that the number of data points to be interpolated is not necessarily n, because tied
x-values xi,σ(j) = xi,σ(j+1) only contribute a single point. Averaging the cases of strict
inequality < and ≤ in the computation of aj is important for exactly those ties, which
occur especially often on ordinal scales. To make the deﬁnition of fi complete, we set
fi(x) = 0 for x < xi,σ(1) and fi(x) = 1 for x > xi,σ(n).

4.2 Learning the Capacity

To learn the capacity µ, we follow the principle of empirical risk minimization. Thus, we
consider the problem of minimizing the 0/1 loss of the classiﬁer (3) on the training data
(5). This problem is provably NP-hard, so we cannot expect to ﬁnd an optimal solution
eﬃciently. Therefore, we opt for an approximate solution. To this end, we construct a linear
program LP, the solution of which will determine the capacity µ. In spite of the theoretical
(worst-case) complexity of linear programming, this approach leads to a practically eﬃcient
learning algorithm, thanks to the availability of modern solvers that are able to handle
programs with thousands of variables and inequalities within seconds.

The inequalities of LP are coming from the monotonicity of the capacity µ. For A ⊆ [m],
let cA denote the value µ(A), i.e., the value assigned by the capacity µ to A. Thus, the
set C = {cA | A ⊂ [m]} corresponds to the parameters that need to be determined. Since
c∅ = 0 and c[m] = 1, the following inequalities need to be added to the optimization
problem:

∀A ⊆ [m], b ∈ [m]/A : cA ≤ cA∪{b}

(7)

When ﬁtting a k-maxitive capacity, some of these inequalities may actually turn into
equalities.
Let (cid:0)x(i), y(i)(cid:1) be an instance in the training data (5). For simplicity, we subsequently
drop the superscript and simply write x = (x1, . . . , xm). Let σ be a permutation such that
xσ(1) ≤ xσ(2) ≤ . . . ≤ xσ(m). Using the median-representation of the Sugeno integral, we
have

Sµ(x) = Med

x1, . . . , xm, µ (cid:0)Aσ(2)

(cid:1) , . . . , µ (cid:0)Aσ(m)

(cid:16)

(cid:1) (cid:17)

,

where Aσ(i) := {σ(i), . . . , σ(m)}.

Suppose that x is a positive example, i.e., y = 1. According to the classiﬁer (3) for a
given β, we must guarantee that Sµ(x) ≥ β, which is equivalent to guaranteeing that
at least m among the values x1, . . . , xm, cA2, . . . , cAm ≥ β. Inspired by the of margin
maximization [16], we would even like to guarantee Sµ(x) ≥ β+ = β + ρ, where ρ ≥ 0 is
a margin. Let p = min{j | xj ≥ β+}. If p = m, then the above condition is automatically
fulﬁlled, and Sµ(x) ≥ β+. In this case, no constraint on the measure µ needs to be added.
Suppose that p < m. In this case, to satisfy the condition Sµ ≥ β+, at least m − p of the
values cA2, . . . , cAm must be ≥ β+. In light of the order relations cA2 ≥ · · · ≥ cAm and
cA1 = · · · = cAm−k , which guarantees the k-maxitivity of µ, we set p = m − k + 2 for all
1 < p < m − k + 2. Thus, based on the value of p, the constraint cAp−1 ≥ β+ is added. We
ignore the particular case of p = 1.

Now, suppose that x is a negative example, i.e., y = 0. To classify x correctly (with a
margin ρ), we must guarantee that Sµ < β− = β − ρ, which is equivalent to guaranteeing
that at least m among the values x1, . . . , xm, cA2, . . . , cAm < β−. Let p = max{j | xj < β−}.
If p = m, then the above condition is automatically fulﬁlled, and Sµ(x) < β−. In this case,
no constraint on the measure µ needs to be added. Let p < m. In this case, to satisfy the
condition Sµ < β−, at least m − p of the values cA2, . . . , cAm must be < β−. In light of the
order relations cA2 ≥ . . . ≥ cAm, we set p = m − k for all p < m − k. Therefore, based on
the value of p, the constraint cAp+1 < β− is added.

For each example (x(i), y(i)) ∈ D, a constraint can be derived according to the procedure
outlined above. Obviously, it will not always be possible to satisfy all these constraints
simultaneously, i.e., to ﬁt the training data without any error. Therefore, to account for
unavoidable mistakes, we introduce slack variables ξi. When x(i) is a positive example,
the corresponding inequality becomes cApi−1 + ξi ≥ β+. Likewise, when x(i) is a negative
sample, the relaxed condition is cApi+1 − ξi < β−. Noting that 2y(i) − 1 = +1 for positive
and = −1 for negative examples, both conditions can be expressed as follows:

− (2y(i) − 1)(cApi−yi

− β) − ξi + ρ ≤ 0

(8)

Finally, the LP consists of minimizing the sum of slack variables ξi subject to the above
constraints, that is:

minimize
C,ξ1,...,ξn

n
(cid:88)

i=1

ξi

subject to

(7) and (8)

(9)

A slight modiﬁcation of the above program is required for the case of learning a k-maxitive
capacity. In this case, the variables cA for |A| > k are implicitly deﬁned by the condition
cA = max{cB | B ⊂ A}, and hence not part of the program. In the case of a positive
example, to ensure Sµ(x) ≥ β+, we need to guarantee that cA ≥ β+ for at least one subset
A ⊆ B = {xp, . . . , xm}. If |B| ≤ k, we can simply add the constraint cB ≥ β+. Otherwise,
we need to assure that cA ≥ β+ for at least one proper k-subset A ⊂ B. This “existential”
constraint is of disjunctive nature, and therefore diﬃcult to formalize in terms of an LP. To
deal with this problem, we add a single constraint cA ≥ β+ for a randomly chosen k-subset
of B. Obviously, this condition is suﬃcient though not necessary, i.e., our program might
be slightly more constrained than necessary. This, however, appears acceptable in light of
our idea of using k-maxitivity for the purpose of regularization. The case of a negative
example can be handled more easily. Here, we need to satisfy cA ≤ β− for all k-subsets
A ⊆ B = {xp, . . . , xm}.

Just like the order of maxitivity k, the margin ρ is a hyper-parameter of the algorithm
that needs to be ﬁxed, for example, through an internal validation procedure. As one ad-
vantage in comparison to other margin classiﬁers, let us mention that our Sugeno classiﬁer
produces normalized predictions in the unit interval. Obviously, this simpliﬁes the search
for “reasonable” values of the margin, which we should expect to be found close to 0, for
example in the range [0, 0.1] or [0, 0.2].

4.3 Learning the Threshold

The linear programming approach to learning the capacity µ, as outlined in the previous
section, assumes the threshold β to be given — by treating both µ and β as variables, the
program would no longer be linear. Correspondingly, the threshold needs to be determined
ﬁrst, prior to solving (9).

To this end, we again formulate a linear program, which seeks to ﬁnd a β that is optimal
in the sense of minimizing the number of classiﬁcation errors on the training data D. Yet,

since the capacity µ is not (yet) known, we have to replace the Sugeno classiﬁer by a
“surrogate” classiﬁer. Here, we suggest to classify an instance x as positive if

Med(u1, . . . , um) = Med(cid:0)f1(x1), . . . , fm(xm)(cid:1) ≥ β ,

and as negative otherwise. This classiﬁer follows the same principle as the Sugeno classiﬁer,
namely the median rule, though without using the values of the capacity µ. The basic idea
is to ﬁnd a threshold that is well attuned to the feature values, so that the capacity can
be used later on to optimally “reﬁne” or correct the classiﬁcations.

In summary, we end up with the following linear program, in which the slack variables ζi
are again used to account for those mistakes that cannot be avoided:

n
(cid:88)

i=1

ζi

minimize
β,ζ1,...,ζn

subject to

(cid:16)
− (2y(i) − 1)

Med

(cid:16)

f1(x(i)

(cid:17)
1 ), . . . , fm(x(i)
m )

(cid:17)

− β

− ζi ≤ 0

(10)

for i = 1, . . . , n.

5 Complexity reduction

As discussed before, restricting the Sugeno integral to k-maxitive capacities and estimating
the measure µ on subsets of size at most k may be advantageous from a learning point
of view, as it reduces the danger of poor generalization due to over-ﬁtting the training
data. In this section, we elaborate on how to eﬃciently determine the hyper-parameter
k in the most favorable way. A practical and quite obvious approach is to determine an
optimal k in a data-driven way through cross-validation (on the training data), i.e., trying
diﬀerent values for k and adopting the one that leads to the best estimated generalization
performance. However, from a theoretical point of view, one may wonder what ensures
the existence of such k, and how signiﬁcant it is to push the k-maxitivity property to an
arbitrary capacity µ.

Once an SI model has been ﬁt, µ can be considered as an approximately k-maxitive
capacity, for some suitable k ∈ [m]. In mathematical analysis, the signiﬁcance of this sort
of approximation, i.e., the question of how an approximate object can be estimated by an
exact object, is studied under the notion of stability [8,13].

When we constrain the Sugeno integral by k-maxitivity, we can still have a signiﬁcant
approximation of that Sugeno integral according to the following stability result. For an
arbitrary measure µ on 2[m] and a small real number ε ∈ [0, 1), we deﬁne the following
subset of [m]:

Gµ,ε =




k ∈ [m] | µ(B) −

(cid:95)

µ(A) ≤ ε, ∀B ∈ 2[m] \ ∅



A⊆B,|A|≤k






.

Theorem 2. Let µ be a capacity on [m]. For a given ε ∈ [0, 1), if the set Gµ,ε attains the
minimum at k∗, there exists a k∗-maxitive measure µ∗ such that

0 ≤ Sµ(x) − Sµ∗(x) ≤ ε

(11)

for all (x, y) ∈ D.

Proof. For each ε ∈ (0, 1), the set Gµ,ε is non-empty, because m ∈ Gµ,ε for any ε ∈ (0, 1). It
means that Gµ,ε is a non-empty ﬁnite subset of integer numbers, so it takes its minimum for
some k ∈ [m], say k∗. Corresponding to k∗, we deﬁne the measure µ∗ for each B ∈ 2[m] \ ∅
as follows:

µ∗(B) =






µ(B)
(cid:87)
A⊆B,|A|=k∗

|B| ≤ k∗,
otherwise.

µ(A)

One can easily check k∗-maxitivity properties (cf. Sect. 2) for µ∗.

Assume that (x, y) is an instance in the training set D and σ is a permutation of [m] such
that xσ(1) ≤ xσ(2) ≤ . . . ≤ xσ(m). From k∗-maxitivity of µ∗, it follows that

µ∗ (cid:0)Aσ(m)

(cid:1) ≤ µ∗ (cid:0)Aσ(m−1)
= µ∗ (cid:0)Aσ(m−k∗)

(cid:1) ≤ · · · ≤ µ∗ (cid:0)Aσ(m−(k∗−1))
(cid:1) = · · · = µ∗ (cid:0)Aσ(1)

(cid:1) ,

(cid:1) =

(12)

where Aσ(i) := {σ(i), . . . , σ(m)}. It should be noted that Sµ(x) is either equal to xσ(p)
or to µ (cid:0)Aσ(p)
(cid:1) for some p ∈ [m] (see [9, Prop. 5.65]). Indeed, p corresponds to the place
where the values xσ(j) cross the values µ (cid:0)Aσ(j)
If p ≥ m − (k∗ − 1), then by the deﬁnition of µ∗, we have µ (cid:0)Aσ(p)
(cid:1), and
consequently Sµ(x) = Sµ∗(x). Now, assume that p ≤ m − k∗. We can distinguish the
following two cases:

(cid:1) = µ∗ (cid:0)Aσ(p)

(cid:1).

(1) If xσ(p) ≤ µ (cid:0)Aσ(p)

(cid:1), then

Sµ(x) = xσ(p)
according to the median-description of the Sugeno integral. Since µ∗ is k∗-maxitive
and k∗ ∈ Gµ,ε, it is not diﬃcult to deduce that

(13)

µ (cid:0)Aσ(p)

(cid:1) ≤ µ∗ (cid:0)Aσ(p)

(cid:1) + ε ,

(14)

and therefore, xσ(p) ≤ µ∗ (cid:0)Aσ(p)
Sµ(x), otherwise xσ(p) > µ∗ (cid:0)Aσ(p)
case, from (13) and (14), we conclude that Sµ(x) − Sµ∗(x) ≤ ε.
(cid:1) ≤ xσ(p), then we have Sµ(x) = µ (cid:0)Aσ(p)

(cid:1) + ε. If xσ(p) ≤ µ∗ (cid:0)Aσ(p)
(cid:1). Thus, it follows that Sµ∗(x) = µ∗ (cid:0)Aσ(p)

(cid:1), then Sµ∗(x) = xσ(p) =
(cid:1). In this

(cid:1). From the deﬁnition, µ∗ (cid:0)Aσ(p)
(cid:1) ≤
(cid:1). Using again the inequality (14), we

(2) If µ (cid:0)Aσ(p)
µ (cid:0)Aσ(p)
conclude that Sµ(x) − Sµ∗(x) ≤ ε.

(cid:1) ≤ xσ(p), and so Sµ∗(x) = µ∗ (cid:0)Aσ(p)

This concludes the proof.

According to Theorem 2, any Sugeno integral Sµ(x) can be approximated by a k∗-maxitive
Sugeno integral Sµ∗(x), where k∗ can be chosen optimally from [m].

data set

# instances # attributes

source

Table 1: Summary of data sets.

Dagstuhl-15512 ArgQuality Corpus (DGS)
Den Bosch (DBS)
Mammographic (MMG)
Auto MPG
Employee Rejection/Acceptance (ERA)
Employee Selection (ESL)
Breast Cancer (BCC)
Breast Cancer Wisconsin (BCW)
Lecturers Evaluation (LEV)
Haberman’s Survival Data (HAB)
Indian Liver Patient (ILP)

960
120
961
392
1000
488
286
699
1000
306
583

14
8
5
7
4
4
7
9
4
3
9

Wachsmuth et al. [25]
Daniels and Kamp [4]
UCI
UCI
UCI
WEKA
UCI
UCI
WEKA
UCI
UCI

6 Experimental Results

In this section, we present results of some experimental studies that we conducted to
assess the practical performance of the Sugeno classiﬁer. To this end, we collected a set
of suitable benchmark data sets, mostly from the UCI2 and the WEKA machine learning
repositories [11]. In particular, these are data sets for which a monotonicity assumption is
plausible, and which have already been used in previous studies on monotone classiﬁcation
[21]. An overview of the data sets is given in Table 1; for a detailed description of the data,
we refer to Section C in the appendix.

6.1 Sensitivity Analysis for Threshold Learning

As we explained in Section 4, the capacity µ and the threshold β of the Sugeno classiﬁer
cannot be learned simultaneously (at least not eﬃciently). Instead, we determine β ﬁrst,
using the linear program (10), and ﬁt the capacity µ afterwards. Therefore, as a natural
question, one may ask what is lost by decomposing the learning task into two parts, instead
of optimizing µ and β jointly. To study this question, we conducted a kind of sensitivity
analysis.

More speciﬁcally, instead of only learning the Sugeno classiﬁer with the threshold deter-
mined according to (10), we learned a classiﬁer (capacity) for all thresholds in the unit
interval (i.e., for 0.01, 0.02, . . . , 1) and compared the performances of the classiﬁers thus
obtained. To this end, we estimated the generalization error of each classiﬁer in terms of
its error on hold-out data, averaged over several random splits of the data for training and
testing.

Figure 2 shows results for the LEV data set. As can be seen, the threshold determined ac-
cording to (9) always leads to optimal overall performance. From these results, which look
very similar for other data sets, we conclude that our approach to learning the threshold is
well suited, with little scope for further improvement due to more sophisticated methods.

2 http://archive.ics.uci.edu/ml/

(a)

(b)

(c)

Fig. 2: Performance (average 0/1 loss on test data) of the Sugeno classiﬁer on the LEV
data, depending on the choice of the threshold. From left to right: 20%, 50%, and 80%
of the data used for training. The threshold determined by (9) is depicted by the vertical
line.

As additional evidence, we “alternately” optimized the threshold β as follows: Once the
capacity µ (i.e., the set of parameters {cA | A ⊂ [m]}) has been found based on the original
threshold β determined by (9), we again optimized the threshold given the capacity µ. As
before, this can be done by solving an LP. Figure 3 compares the threshold β determined
by (9) and the threshold β0 determined in this way. As can be seen, both values are very
close to each other.

(a)

(b)

(c)

Fig. 3: Performance (average 0/1 loss on test data) of the Sugeno classiﬁer on the MMG
data, depending on the choice of the threshold. From left to right: 20%, 50%, and 80% of
the data used for training. The red vertical line depicts the threshold β determined by (9),
and the green vertical line depicts the threshold β0 determined by alternate optimization.

00.10.20.30.40.50.70.80.91Threshold value0.10.20.30.40.50.60.70.80/1 loss00.10.20.30.40.50.70.80.91Threshold value0.10.20.30.40.50.60.70.80/1 loss00.10.20.30.40.50.70.80.91Threshold value0.10.20.30.40.50.60.70.80/1 loss00.10.20.30.40.50.70.80.91Threshold value0.10.20.30.40.50.60.70.80/1 loss00.10.20.30.40.50.70.80.91Threshold value0.10.20.30.40.50.60.70.80/1 loss00.10.20.30.40.50.70.80.91Threshold value0.10.20.30.40.50.60.70.80/1 loss6.2 Classiﬁcation Performance

In this section, we investigate the performance of the Sugeno classiﬁer in terms of its
classiﬁcation accuracy. In all experiments, the data is randomly split into two parts, one
for training and one for testing. Performance is then reported in terms of the 0/1 loss on
the test data, averaged over 100 random splits. The Sugeno classiﬁer is determined on the
training data as described in Section 4. The degree k of “maxitivity” is a hyper-parameter
of the learning method, which is determined in an internal cross-validation: To estimate
the performance that can be achieved with a certain k, the learner conducts a 10-fold
cross-validation on the training data. Trying diﬀerent values for k, it picks the presumably
best one for training the classiﬁer to be used for prediction on the test data.

As a baseline to compare with, we used a 2-additive “Choquistic regression” (CR) as
proposed in [21]. As already explained in the beginning of the paper, this approach is
very close to ours and essentially diﬀers in using the Choquet instead of the Sugeno
integral as an aggregation function. As additional baselines, we included standard logistic
regression (LR) and decision trees (DT). DT refers to a binary decision tree, ﬁtted for
binary classiﬁcation, and implemented in Statistics and Machine Learning Toolbox of
Matlab. We also include a rule-based method which is monotone and ﬂexible, namely the
MORE algorithm for learning rule ensembles under monotonicity constraints [5].

The results in terms of misclassiﬁcation rate (0/1 loss) and rank statistics are shown in
Table 2 for diﬀerent amounts of training data. Moreover, Table 3 provides a summary
of pairwise win/loss statistics. As can be seen, the Sugeno classiﬁer is very competitive
and performs quite strongly, often even the best. There are no truly signiﬁcant diﬀerences
between the classiﬁers, however, except that decision trees are clearly outperformed. By
and large, LR, CR, MORE, and SI perform the same. This result is nevertheless inter-
esting and encouraging, as it shows that one can take advantage of the representational
and algorithmic beneﬁts of the Sugeno classiﬁer without the need to accept a drop in
performance.

As we discussed before, we exploit the restriction to k-maxitive capacities as a means
for regularization of the Sugeno classiﬁer, i.e., to prevent overﬁtting eﬀects that might be
expected when ﬁtting a fully maxitive measure. Table 4 shows a comparison between the
fully maxitive classiﬁer and the k-maxitive classiﬁer, which treats k as a hyper-parameter of
the learning algorithm. As can be seen, consistent improvements can indeed be achieved
through regularization with k. Mostly, these improvements are not very big, however,
which also shows that even the unregularized Sugeno classiﬁer is not too susceptible to
overﬁtting.

7 Conclusion

In this paper, we proposed a novel method for binary classiﬁcation that builds upon the
Sugeno integral as a means for aggregating feature information in supervised machine
learning. Due to the speciﬁc properties of the Sugeno integral, this approach is especially
suitable for learning monotone models from ordinal data, although it can of course also be

Table 2: Classiﬁcation performance in terms of mean ± standard deviation of 0/1 loss for 20% (top),
50% (middle), and 80% (bottom) training set. Average ranks comparing signiﬁcantly worse with SI at the
90% conﬁdence level (according to a Friedman-Nemenyi test) are put in bold font.

data set

DT

LR

CR

MORE

SI

DGS1
DGS2
DBS
MMG
MPG
ESL
ERA
BCC
BCW
LEV
HAB
ILP

.106±.012(5) .095±.009(3) .093±.009(2)
.096±.013(5) .088±.008(1) .091±.008(3)
.206±.052(5) .199±.060(4) .183±.046(3)
.199±.024(5) .171±.011(2) .171±.011(3)
.125±.025(5) .110±.019(3) .107±.017(2)
.122±.016(5) .077±.013(1) .080±.012(2)
.312±.018(5) .291±.012(2) .291±.011(1)
.307±.046(5) .285±.028(4) .281±.036(3)
.065±.014(5) .046±.014(3) .046±.012(2)
.177±.018(5) .165±.010(3) .165±.011(2)
.320±.043(5) .270±.025(3) .265±.020(2)
.337±.025(5) .303±.019(4) .301±.014(3)

.098±.012(4)
.094±.007(4)
.171±.041(1)
.172±.010(4)
.102±.015(1)
.094±.011(3)
.304±.016(3)
.269±.025(1)
.043±.010(1)
.169±.017(4)
.273±.016(4)
.301±.023(2)

.093±.009(1)
.089±.009(2)
.180±.035(2)
.169±.013(1)
.113±.020(4)
.104±.012(4)
.308±.017(4)
.270±.029(2)
.050±.014(4)
.165±.012(1)
.265±.022(1)
.286±.010(1)

avg. rank

5.00

2.75

2.33

2.67

2.25

DGS1
DGS2
DBS
MMG
MPG
ESL
ERA
BCC
BCW
LEV
HAB
ILP

.094±.012(5) .091±.010(4) .087±.010(1) .0898±.013(2) .090±.011(3)
.088±.012(5) .078±.010(1) .080±.009(2) .0881±.012(4) .085±.011(3)
.181±.058(5) .167±.048(4) .161±.044(3) .1391±.045(1) .159±.036(2)
.197±.018(5) .165±.016(2) .166±.015(3) .1671±.012(4) .162±.019(1)
.096±.024(4) .095±.015(3) .098±.017(5) .0868±.016(1) .095±.018(2)
.097±.019(4) .069±.014(2) .068±.013(1) .0876±.017(3) .105±.018(5)
.302±.019(5) .291±.019(2) .290±.017(1) .2973±.015(3) .299±.015(4)
.291±.034(5) .263±.029(3) .269±.032(4) .2459±.032(1) .263±.031(2)
.052±.011(5) .037±.008(3) .037±.008(2) .0362±.010(1) .044±.012(4)
.149±.016(1) .163±.012(5) .162±.011(3) .1622±.017(4) .159±.014(2)
.316±.036(5) .260±.030(2) .252±.030(1) .2984±.047(4) .264±.025(3)
.339±.024(5) .304±.023(4) .297±.021(3) .2904±.019(2) .285±.017(1)

avg. rank

4.50

2.92

2.42

2.50

2.67

DGS1
DGS2
DBS
MMG
MPG
ESL
ERA
BCC
BCW
LEV
HAB
ILP

.089±.018(4) .094±.018(5) .088±.018(3)
.086±.018(4) .079±.018(2) .081±.017(3)
.166±.090(5) .140±.076(2) .142±.069(3)
.198±.031(5) .164±.024(2) .164±.024(3)
.086±.012(2) .098±.027(4) .099±.026(5)
.083±.028(3) .067±.023(2) .063±.020(1)
.297±.027(5) .279±.029(1) .288±.026(2)
.296±.053(5) .270±.052(3) .252±.047(1)
.049±.018(5) .034±.013(3) .034±.013(2)
.143±.021(1) .166±.023(5) .161±.024(4)
.317±.056(5) .260±.052(3) .260±.052(2)
.332±.038(5) .295±.039(3) .304±.037(4)

.081±.016(1)
.093±.026(5)
.138±.076(1)
.168±.022(4)
.079±.025(1)
.086±.024(4)
.291±.028(3)
.279±.047(4)
.033±.015(1)
.157±.024(3)
.289±.035(4)
.292±.010(2)

.085±.023(2)
.076±.017(1)
.155±.066(4)
.157±.028(1)
.092±.029(3)
.104±.030(5)
.297±.026(4)
.266±.052(2)
.041±.019(4)
.156±.020(2)
.258±.051(1)
.282±.038(1)

avg. rank

4.08

2.92

2.75

2.75

2.50

applied to learning from numerical data. We analyzed theoretical properties of the Sugeno
classiﬁer, proposed a learning algorithm based on linear programming, and assessed the
performance of the classiﬁer in an experimental study.

Our empirical results are promising and show that the Sugeno classiﬁer is competitive in
terms of predictive accuracy, in spite of its seemingly restricted expressiveness compared to
more powerful models like the (numerical) Choquet integral. Combined with its “symbolic”
nature, this makes it quite an appealing approach to data-driven model construction,

Table 3: Win/loss statistics (number of data sets that the ﬁrst method beats the second one), for
20%|50%|80% training data.

data set DT

LR CR MORE SI

total wins total rank

–
DT
LR
12|11|9
CR 12|10|10 9|8|6

0|1|3 0|2|2
3|4|6
–

–

MORE 12|11|9 5|7|6 5|5|6
12|10|9 7|7|8 8|6|7

SI

0|1|3
7|6|6
7|7|6
–
6|5|6

0|6|11

0|2|3
5|5|4 27|26|25
4|6|5 32|31|27
6|7|6 28|29|27
33|28|30

–

5|5|4
4|4|3
2|1|2
3|2|2
1|3|1

Table 4: Comparing the classiﬁcation performance of fully maxitive and k-maxitive Sugeno integral for
20%, 50% and 80% training data: average 0/1 loss and average k chosen by the SI classiﬁer.

data set 20% training 20% training average 50% training 50% training average 80% training 80% training average
fully maxitive k-maxitive

fully maxitive k-maxitive

fully maxitive k-maxitive

k

k

k

DGS1
DGS2
DBS
MMG
MPG
ESL
ERA
BCC
BCW
LEV
HAB
ILP

.094±.010
.088±.009
.183±.032
.169±.011
.115±.021
.105±.012
.308±.019
.275±.037
.053±.016
.163±.011
.290±.031
.314±.024

.093±.009
.089±.009
.180±.035
.169±.013
.113±.020
.104±.012
.308±.017
.270±.029
.050±.014
.165±.012
.265±.022
.286±.010

4.02
4.08
6.50
4.12
6.15
3.88
3.47
5.35
7.49
3.08
1.82
3.76

.090±.010
.085±.012
.166±.040
.165±.013
.098±.016
.107±.016
.300±.016
.269±.028
.044±.013
.158±.012
.293±.031
.308±.021

.090±.011
.085±.011
.159±.036
.162±.013
.095±.018
.105±.018
.299±.015
.263±.031
.044±.012
.159±.014
.264±.025
.285±.017

3.86
3.84
6.11
3.79
5.86
3.59
3.56
5.79
6.88
2.96
1.73
3.46

.088±.017
.080±.019
.154±.067
.169±.024
.097±.031
.108±.027
.303±.028
.283±.048
.039±.016
.161±.024
.277±.055
.300±.037

.085±.023
.076±.017
.156±.066
.157±.028
.092±.029
.104±.030
.297±.026
.266±.052
.041±.019
.156±.020
.258±.051
.282±.038

3.57
3.58
5.93
3.29
5.34
3.00
3.16
5.70
6.81
2.75
1.52
3.41

especially from the point of view of interpretable machine learning and explainable AI
[2,10]. There are various directions of future work:

– Our approach is limited to binary classiﬁcation and should be extended toward other
machine learning problems, such as multinomial and ordinal classiﬁcation (akin to the
extension of the Choquet classiﬁer to ordinal classiﬁcation [23]).

– Even if the true dependency between the predictor variables and the target is mono-
tone, the training data may violate monotonicity (e.g., due to noise and errors). Never-
theless, our learning algorithm enforces monotonicity by solving a constrained otimiza-
tion problem. Another idea, which has been put forward in the literature on mono-
tonic classiﬁcation [7], is to “monotonize” the training data ﬁrst, and then to ﬁt an
unconstrained model to the pre-processed data. This approach appears to be a viable
alternative for the Sugeno classiﬁer, too, and is certainly worth an investigation.

– Last but not the least, it is tempting to further elaborate on the connection between
the Sugeno classiﬁer and other types of classiﬁers. As we pointed out, by specifying
the underlying capacity in a suitable way, well-known models like the k-of-m classiﬁer
can be obtained as special cases of the Sugeno classiﬁer. Thus, our approach may
provide a unifying framework of a broader class of classiﬁers, suggest new methods
for training such classiﬁers, reveal interesting relationships between them, and perhaps
even suggest new classiﬁers as speciﬁc instantiations.

A VC Dimension of the Sugeno Classiﬁer

Theorem 3. The VC dimension of the hypothesis space H comprised of threshold classi-
ﬁers of the form (3) grows asymptotically at least as fast as 2m/

m.

√

√

Proof. We show the model class H can shatter a suﬃciently large data set D of the size
2m/
m. We construct the set D by using the binary attribute values, which means that
xi ∈ {0, 1} for all 1 ≤ i ≤ m. Accordingly, each instance x = (x1, . . . , xm) ∈ {0, 1}m can
be identiﬁed with a subset of indices Sx ⊆ X, namely its indicator set Sx = {i|xi = 1}.

We recall a well-known result of Sperner [19], who showed that the maximum cardinality of
(cid:1). Antichain is a notion
any antichains of the set [m] is the so-called Sperner number (cid:0) m
in combinatorics regarding the incomparable subsets of an arbitrary set. An antichain A of
the set [m] is a nonempty proper subset of 2[m] such that, for all A, B ∈ A, neither A ⊆ B
nor B ⊆ A. The Sperner number is obviously restricted due to the above non-inclusion
constraint on pairs of subsets. Sperner showed that the corresponding antichain A is given
by the family of all q-subsets of X with q = (cid:98)m/2(cid:99), that is, all subsets A ⊆ X such that
|A| = q.

(cid:98)m/2(cid:99)

From a decision making perspective, each attribute can be interpreted as a criterion.
Using this fact, we are going to deﬁne the speciﬁc dataset D in terms of the collection
of all instances x = (x1, . . . , xm) ∈ {0, 1}m whose indicator set Sx is a q-subset of X.
Naturally, the instances in D are therefore maximally incomparable; indeed, each instance
in D satisﬁes exactly q of the m criteria, and there is not a single dominance relation in
the sense that the set of criteria satisﬁed by one instance is a superset of those satisﬁed by
another instance. This substantial property will help us to show that D can be shattered
by H.

It should be noted that the set D can be shattered by a model class H if, for each subset
P ⊆ D, there is a model H ∈ H such that H(x) = 1 for all x ∈ P and H(x) = 0 for all
(x, y) ∈ D \ P. Now, we consider any such subset P from the data set D as constructed
above, and the Segeno integral given by (2). We deﬁne the measure µ as follows:

µ(E) =

(cid:40)
1

if E ⊇ Sx for some x ∈ P,

0 otherwise.

Obviously, this deﬁnition is feasible and yields a proper capacity µ.

Now, consider expression (2) for any (x, y) ∈ P. For E = Sx, we have mini∈E xi = 1 and
µ(E) = 1, and hence Sµ(x) = 1. Likewise, consider expression (2) for any (x(cid:48), y(cid:48)) ∈ D \ P.
From the construction of µ and the fact that, for each pair x (cid:54)= x(cid:48) in training set, neither
Sx ⊆ Sx(cid:48) nor Sx(cid:48) ⊆ Sx, it follows that either mini∈E x(cid:48)
i = 0 or µ(E) = 0, regardless of
E. More speciﬁcally, we can distinguish three cases according to the size of E: If |E| < q,
then µ(E) = 0. If |E| > q, then mini∈E x(cid:48)
i = 0, because Sx(cid:48) is a q-subset of X. If |E| = q,
then either E = Sx for some x ∈ P, in which case mini∈E x(cid:48)
i = 0, or E (cid:54)= Sx for all x ∈ P,
in which case µ(E) = 0. Consequently, the Sugeno integral is given as follows:

SIµ(x) =

(cid:40)

1 x ∈ P,

0

otherwise.

Thus, with β = 1/2, the classiﬁer (3) behaves exactly as required, that is, it classiﬁes all
x ∈ P as positive and all x /∈ P as negative.

We make use of Sterling’s approximation of large factorials (and hence binomial coeﬃ-
cients) for the asymptotic analysis. For the sequence (b1, b2, . . .) of the so-called central
binomial coeﬃcients bn, it is known that
(cid:19)

bn =

(cid:18)2n
n

=

(2n)!
(n!)2 ≥

1
2

√

4n
π · n

.

By setting n = m/2 and ignoring constant terms, we conclude that the VC dimension of
the model class H grows asymptotically at least as fast as 2m/

m.

√

B Determining the Direction of Features

The Sugeno classiﬁer assumes monotonicity in the sense that the dependency between
predictor and target variables is either “the higher the better” or “the lower the better”.
In many cases, the direction is known beforehand and provided to the learner as part of the
prior knowledge. Otherwise, if this is not the case, we suggest to determine the direction
as a pre-processing step in a data-driven way as follows:

– We ﬁt a ﬂexible model, such as a neural network, to the training data. This model

does neither require nor impose any monotonicity condition.

– For every variable xj, we determine the output of the model for each original training
m ) and for the same instance with a slightly increased value

1 , . . . , x(i)

instance x(i) = (x(i)
for xj, i.e., with x(i)
j

replaced by x(i)

j + δ.

– We count the number of cases in which the output of the model decreases and increases,
respectively, and have a guess on the direction of the variable’s inﬂuence depending on
which of the cases prevails.

C Data Sets

For a description of the following data sets, we refer to [21]: Bosch (DBS), Mammographic
(MMG), Auto MPG, Employee Selection (ESL), Employee Rejection/Acceptance (ERA),
Employee Rejection/Acceptance (ERA), Breast Cancer (BCC), Breast Cancer Wisconsin
(BCW), Lecturers Evaluation (LEV), Haberman’s Survival Data (HAB), Indian Liver
Patient (ILP).

The Dagstuhl-15512 ArgQuality Corpus (DGS) is a data set that consists of 25 to 35
textual debate portal arguments for two postures on 16 issues, such as christianity vs.
atheism and is the school uniform a good or bad idea. For each posture pair, the ﬁve
ﬁrst-rate texts are taken and also ﬁve further are chosen via stratiﬁed sampling. So, both
high-level arguments and diﬀerent lower-level qualities are covered and ﬁnally, 320 texts
(20 argumentative comments × 16 issues) are chosen. Then, three annotators (two females,
one male, from three countries, who work at two universities and one company) discuss
all 320 texts. They assess 15 quality dimensions (including its overall quality) in the

taxonomy for each comment using three ordinal scores: 1 (low), 2 (average) and 3 (high).
In our experiments, we consider the overall quality as the target attribute and binarize it by
distinguishing between low-level argument (score 1) and high-level argument (scores 2 and
3). In addition, to improve the accuracy of the classiﬁcation and predicting based on more
related subgroup of attributes, we distinguish two subsets of the attributes, one related to
emotional and relevancy criteria, and one related to the logical and reasonableness criteria.
The ﬁrst group includes local acceptability, cogency, eﬀectiveness, global relevance, and
emotional appeal. The second group consists of arrangement, cogency, global suﬃciency,
reasonableness, credibility, and suﬃciency.

References

1. Sadegh Abbaszadeh, Alireza Tavakoli, Marjan Movahedan, and Peide Liu. Fuzzy aggregation opera-
tors with application in the energy performance of buildings. International Journal for Uncertainty
Quantiﬁcation, 8(6), 2018.

2. J.M. Alonso, C. Castiello, and C. Mencar. Interpretability of fuzzy systems: Current research trends
and prospects. In J. Kacprzyk and W. Pedrycz, editors, Springer Handbook of Computational Intelli-
gence, pages 219–237. Springer, Berlin, Heidelberg, 2015.

3. Q. Brabant and M. Couceiro. k-maxitive sugeno integrals as aggregation models for ordinal preferences.

Fuzzy Sets Syst., 343:65–75, 2018.

4. Hennie Daniels and B Kamp. Application of mlp networks to bond rating and house pricing. Neural

Computing & Applications, 8(3):226–234, 1999.

5. K. Dembczynski, W. Kotlowski, and R. Slowinski. Learning rule ensembles for ordinal classiﬁcation

with monotonicity constraints. Fundamenta Informaticae, 94(2):163–178, 2009.

6. Tufan Demirel, Sultan Ceren ¨Oner, Serhat T¨uz¨un, Muhammet Deveci, Mahir ¨Oner, and Nihan C¸ etin
Demirel. Choquet integral-based hesitant fuzzy decision-making to prevent soil erosion. Geoderma,
313:276–289, 2018.

7. A. Feelders. Monotone relabeling in ordinal classiﬁcation. In Prod. ICDM-2010, 10th IEEE Int. Conf.

on Data Mining, pages 803–808, Sydney, Australia, 2010.

8. Madjid Eshaghi Gordji and Sadegh Abbaszadeh. Theory of Approximate Functional Equations: In

Banach Algebras, Inner Product Spaces and Amenable Groups. Academic Press, 2016.

9. Michel Grabisch, Jean-Luc Marichal, Radko Mesiar, and Endre Pap. Aggregation functions, volume

127. Cambridge University Press, 2009.

10. M. Guo, Q. Zhang, X. Liao, and Y. Chen. An interpretable machine learning framework for modelling

human decision behavior. abs/1906.01233, 2019.

11. Mark Hall, Eibe Frank, Geoﬀrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten.
The weka data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10–18, 2009.
12. E. H¨ullermeier and A. Fallah Tehrani. Eﬃcient learning of classiﬁers based on the 2-additive Choquet
integral. In C. Moewes and A. N¨urnberger, editors, Computational Intelligence in Intelligent Data
Analysis, Studies in Computational Intelligence, pages 17–30. Springer, 2012.

13. Donald H Hyers. On the stability of the linear functional equation. Proceedings of the National

Academy of Sciences of the United States of America, 27(4):222, 1941.

14. R. Mesiar and A. Koles´arov´a. k-maxitive aggregation functions. Fuzzy Sets and Systems, 346:127–137,

2018.

15. R. Potharst and A. Feelders. Classiﬁcation trees for problems with monotonicity constraints. SIGKDD

Explorations, 4(1):1–10, 2002.

16. B. Sch¨olkopf and AJ. Smola. Learning with Kernels: Support Vector Machines, Regularization, Opti-

mization, and Beyond. MIT Press, 2001.

17. Olivier Sobrie, Vincent Mousseau, and Marc Pirlot. Learning a majority rule model from large sets of
assignment examples. In Proc. ADT, 3rd International Conference on Algorithmic Decision Theory,
pages 336–350, Bruxelles, Belgium, 2013.

18. Olivier Sobrie, Vincent Mousseau, and Marc Pirlot. Learning the parameters of a non compensatory
sorting model. In Proc. ADT, 4th International Conference on Algorithmic Decision Theory, pages
153–170, Lexington, KY, USA, 2015.

19. E. Sperner. Ein satz¨uber untermengen einer endlichen menge. Math. Z., 27:544–548, 1928.
20. M. Sugeno. Theory of Fuzzy Integrals and its Application. PhD thesis, Tokyo Institute of Technology,

1974.

21. A. Fallah Tehrani, W. Cheng, K. Dembczynski, and E. H¨ullermeier. Learning monotone nonlinear

models using the choquet integral. Machine Learning, 89:183–211, 2012.

22. A. Fallah Tehrani, W. Cheng, and E. H¨ullermeier. Preference learning using the Choquet integral:
The case of multipartite ranking. IEEE Transactions on Fuzzy Systems, 20(6):1102–1113, 2012.
23. A. Fallah Tehrani and E. H¨ullermeier. Ordinal Choquistic regression. In J. Montero, G. Pasi, and
D. Ciucci, editors, Proceedings EUSFLAT–2013, 8th International Conference of the European Society
for Fuzzy Logic and Technology, Milano, Italy, 2013. Atlantis Press.

24. V.N. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1998.
25. Henning Wachsmuth, Nona Naderi, Yufang Hou, Yonatan Bilu, Vinodkumar Prabhakaran, Tim Al-
berdingk Thijm, Graeme Hirst, and Benno Stein. Computational argumentation quality assessment
in natural language. In Proceedings of the 15th Conference of the European Chapter of the Association
for Computational Linguistics, 2017.

