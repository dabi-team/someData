1
2
0
2

y
a
M
1
2

]
h
p
-
p
e
h
[

1
v
2
1
5
0
1
.
5
0
1
2
:
v
i
X
r
a

Reframing Jet Physics with New Computational Methods

Kyle Cranmer1, Matthew Drnevich1, Sebastian Macaluso1, and Duccio Pappadopulo

1New York University, USA

Abstract

We reframe common tasks in jet physics in probabilistic terms, including jet re-
construction, Monte Carlo tuning, matrix element – parton shower matching for large
jet multiplicity, and eﬃcient event generation of jets in complex, signal-like regions of
phase space. We also introduce Ginkgo, a simpliﬁed, generative model for jets, that
facilitates research into these tasks with techniques from statistics, machine learning,
and combinatorial optimization. We review some of the recent research in this direction
that has been enabled with Ginkgo. We show how probabilistic programming can be
used to eﬃciently sample the showering process, how a novel trellis algorithm can be
used to eﬃciently marginalize over the enormous number of clustering histories for the
same observed particles, and how dynamic programming, A* search, and reinforcement
learning can be used to ﬁnd the maximum likelihood clusterinng in this enormous
search space. This work builds bridges with work in hierarchical clustering, statistics,
combinatorial optmization, and reinforcement learning.

Contents

1 Introduction

1.1 Reframing jet physics in probabilistic terms . . . . . . . . . . . . . . . . . .
Jet clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.1
1.1.2 Tuning the parameters of the shower model
. . . . . . . . . . . . . .
1.1.3 Event Generation for events with large jet multiplicity . . . . . . . .
Simulating jet backgrounds in signal-rich regions of phase space . . .
1.1.4

2 Ginkgo: A simpliﬁed generative model for jets

2.1 The generative process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Reconstruction: The Likelihood for a Proposed Jet Clustering . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
2.3 Greedy and beam search algorithms

2
2
3
4
5
6

6
7
8
9

1

 
 
 
 
 
 
3 Examples of Research Enabled with Ginkgo

Sparse Cluster Trellis

3.1.1
3.1.2 Results

3.1 Hierarchical Cluster Trellis Algorithm . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Hierarchical clustering through reinforcement learning . . . . . . . . . . . .
Jet clustering as a Markov Decision Process . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Hierarchical clustering using A* . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 Probabilistic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.1
3.2.2 Results

3.3.1 Results

4 Conclusion

1

Introduction

10
10
11
11
13
13
14
14
15
15

16

Jets are the most copiously produced objects at the LHC and the subject of intense
experimental and theoretical study. Improvements to our understanding and treatment of
jets can have a signiﬁcant impact on the physics program of the LHC; however, various
computational bottlenecks appear in this quest. Below we will discuss a few areas that
show such computational bottlenecks and identify emerging computational techniques that
may be able to address them. We hope that this may challenge some assumptions about
the computational demands of simulation, reconstruction, and analysis of LHC data when
jets are involved.

1.1 Reframing jet physics in probabilistic terms

Monte Carlo event generators (e.g. simulators like PYTHIA [1], Herwig [2], and Sherpa
[3]) encode a physics model for the fragmentation and hadronization of quarks and gluons
produced at colliders. In statistical and machine learning language, they are generative
models for jets. Following the notation of Ref. [4, 5], we denote the parameters of the
(Monte Carlo) simulation θ, the observable output of the simulator x, and latent variables
(aka Monte Carlo truth record or showering history) z. The simulators typically evolve
the latent state sequentially as a Markov process and model the physics of each splitting,
clustering, etc. In the original parton showers, based on successive 1
2 splittings, the
joint likelihood for the parton shower can be expressed as:

→

p(x, z

θ) = p(x
|

zleaves, θ)
|

(cid:89)

s∈splittings

p(zs,L, zs,R

zs,P , θ) ,
|

(1)

where zs,P , zs,L (zs,R), are respectively the data needed to encode the state of the parent and
left (right) children for the sth splitting and zleaves are the terminal leaves of the showering

2

process. The hadronization and detector simulation ﬁt in this framing as well, but we do
not discuss it explicitly in this work.

We ﬁnd it elucidating to reframe the following concepts in jet physics in probabilistic

terms:

• Joint likelihood for latent shower and observed constituents p(x, z
θ) = (cid:82) dz p(x, z

• Marginal likelihood for observed constituents p(x
|

θ)
|
θ)
|

• Maximum likelihood showering history ˆz = argmaxz p(x
• Maximum likelihood parameters for the model ˆθ = argmaxθ p(x
• Posterior distribution on showering histories p(z

z, θ)
|

x, θ)

|

θ) = argmaxθ
|

(cid:82) dz p(x, z

θ)
|

A few challenges present themselves in this framing of jet physics.

First of all, the joint likelihood p(x, z

θ) and the likelihood of individual splittings
|
p(zs,L, zs,R
zs,P , θ) is not exposed in a way that is convient to access. The joint likelihood
|
corresponds to what is coded in PYTHIA [1], Herwig [2], and Sherpa [3], but often in
terms of accept-reject sampling and procedural code that does not explicitly expose the
probabilities themselves. This motivates Ginkgo, which provides convenient access to these
quantities in a simpliﬁed parton shower.
Secondly, the joint likelihood p(x, z

θ) is not immediately of interest to experimentalists
|
since the (latent) showering history z is not observed. Quantities such as the marginal
θ) and the maximum likelihood parameter ˆθ involve integration (sums) over
likelihood p(x
|
all possible showering histories. The number of possible showering histories grows factorially
with the number of jet constituents. This super-exponential growth in the number of
showering histories is at the heart of many computational bottlenecks in jet physics, making
the marginalization and maximization over the latent space z of showering histories typically
intractable.

This paper is organized as follows. Below, we will review some common tasks in jet
physics framed in these probabilistic terms. We will identify the computational challenges
and the potential for emerging computational techniques to address them. In Section 2
we will describe Ginkgo’s simpliﬁed probabilistic model for the parton shower. Finally,
in Section 3 we will review some of the recent research into these problems enabled with
Ginkgo.

1.1.1 Jet clustering

Jet reconstruction can be thought of as estimating the latent state (showering history)
z from the observed particles x. Traditionally, given a set of ﬁnal state particles, jets
are reconstructed using one of the generalized kt clustering algorithms [6, 7, 8, 9]. These
algorithms sequentially cluster jet constituents by merging the closest pair based on the

3

distance measure dα
ij,

ti , p2α
tj )

ij = min(p2α
dα

∆R2
ij
R2
, R is a ﬁxed value corresponding to
where ∆Rij is the angular distance for the pair
the jet radius and α =
speciﬁes the anti-kt, Cambridge/Aachen (C/A) and kt
algorithms, respectively. This traditional approach does not make explicit reference to the
probability model for the jet, but intuitively the kt and C/A algorithms cluster together
two constituents that are likely to have emerged from the same parent.1 We can formulate
the intuition for the kt and C/A algorithms as saying that the distance measure dα
i,j is
monotonically decreasing with the splitting likelihood p(zs,L, zs,R

1, 0, 1

{−

(2)

i, j

}

(cid:104)

(cid:105)

In the probabilistic language, the natural goal is to ﬁnd the most likely clustering ˆz. In
that light, the generalized-kt algorithms are greedy algorithms for ﬁnding the maximum
likelihood clustering. Greedy algorithms are not guaranteed to ﬁnd the maximum likelihood
clustering because they do not consider the tree globally, i.e. they ﬁnd the best solution
locally step by step. More sophisticated algorithms like beam search, which is used widely in
natural language processing, look more than one step ahead and consider multiple possible
clusterings in memory as they proceed. They are guaranteed to recover jet clusterings that
are at least as good as the greedy algorithm, and can be expected to improve upon them.
But to do this, one needs a way to score the combination of multiple clusterings. It is not
clear how one would combine the distance measure dα
i,j for two splittings, but there is a
zs,P , θ) (i.e. their product).
natural rule for combining the splitting likelihood p(zs,L, zs,R
|
This is one of the advantages of the probabilistic formulation: it allows us to recast the
objective of existing greedy clustering algorithms like kt as well as extend them to more
sophisticated algorithms.

zs,P , θ).
|

In more general terms, jet clustering can be framed as a hierarchical clustering task.
In that framing, the generalized-kt algorithms are considered (bottom-up) hierarchical
agglomerative clustering (HAC) algorithms. In Section 3 we will review the research in
jet clustering enabled by Ginkgo, including a novel trellis data structure and dynamic
programming model, a novel A∗ search algorithm that makes use of a modiﬁed trellis data
structure and a heuristic, as well as a few reinforcement learning algorithms including
Monte Carlo Tree Search and Behavioral Cloning.

1.1.2 Tuning the parameters of the shower model

Monte Carlo tuning, can be thought of as estimating θ given a dataset of
. Ideally,
}
if we wanted to ﬁt (tune) the parameters θ of PYTHIA [1], Herwig [2], and Sherpa [3]
(and we had inﬁnite computing power), then we would compute the maximum likelihood
ˆθ based on the high dimensional jet data x. Since we do not, we resort to tools like
Professor [10], which compare projections of complicated events to individual variables

xi

{

1In contrast, the anti-kt algorithm focuses more on having desirable global properties for the jets than

reconstructing a physically motivated showering history.

4

Figure 1: Schematic representation of the tree
structure of a jet generated with Ginkgo and the
resulting tree for some clustering algorithm. For
a given algorithm, z labels the latent structure of
the tree. The tree leaves x are labeled in red and
the inner nodes in green.

(marginal distributions), which is blind to various forms of mismodelling in the high-
dimensional structure of the jets. The marginalizaton over the latent space is implicit when
forming the histograms of these individual diﬀerential distributions. The fact that tuning
the generators is itself a bottleneck suppresses the motivation to add even more ﬂexibility
and parameters to the shower models, even if they might lead to more accurate description
of the jets.

The ﬁrst emerging technique in this direction is likelihood-free inference or simulation-
based inference [4, 11, 12]. Recent progress in this direction includes likelihood-free inference
methods [4, 5, 11, 12]. These methods approximate the intractable p(x
θ) using machine
|
learning and bypass an explicit marginalization over the latent state z. The techniques can
exploit the joint likelihood p(x, z
θ) if it is available. An implementation of these techniques
|
for events simulated with Pythia was introduced in Refs. [13]. A closely related approach
was outlined in Ref. [14]

An alternate approach to this problem would be to use probabilistic programming
techniques to eﬃciently approximate the intractable integrals. The ﬁrst prototypes of
integrating probabilistic programming with the Monte Carlo generators (speciﬁcally Sherpa)
was performed in Refs. [15, 16]

1.1.3 Event Generation for events with large jet multiplicity

The enormous number of possible showering histories is a bottleneck in the simulation
of multijets events [17, 18] and shower deconstruction [19, 20, 21]. When implementing
the CKKW-L matching algorithm [17, 18], parton ﬁnal states need to be reweighted with
the corresponding Sudakov form factors of each history, p(x, z
θ). The standard algorithm
typically becomes infeasible for parton level conﬁgurations that exceed the complexity of
W/Z+ 6 jet ﬁnal state [22] due to the super-exponential growth in the number of clustering
histories.

|

To ameliorate these bottlenecks we introduced in [23], a novel data structure and
algorithms, called hierarchical cluster trellis, that can be used to eﬃciently represent the
distribution over trees. The trellis can be used to compute the marginal likelihood p(x
θ) or
|
the exact maximum likelihood showering history ˆz in time and memory proportional to the
signiﬁcantly smaller powerset of the number of jet constituents, i.e. 2N . In particular, we
showed that the trellis allows us to perform these operations for larger values of N where
3)!! trees is impractical. Thus far the implementation is
the naive iteration over the (2N

−

5

based on binary trees, 1
to consider 2

→

→

2 splittings; however, it is possible to extend the cluster trellis

3 splittings required in the CKKW-L algorithm.

The trellis data structure also provides an eﬃcient dynamic programming algorithm to
ﬁnd the maximum likelihood shower history ˆz, which provides a principled alternative to
the generalized kt algorithms, that are based on a greedy sequential clustering algorithm as
described in Sec. 1.1.1.

1.1.4 Simulating jet backgrounds in signal-rich regions of phase space

Simulating suﬃcient numbers of multijet background events is a computational challenge due
to the enormous rate of multijet events and their steeply falling spectra. The experimental
collaborations have traditionally sliced the phase space into exclusive regions (eg. based
on the pT of the leading jet at parton level). This is an eﬀective strategy for populating
the tails of that distribution, but it is not eﬀective for populating complicated phase space
regions (eg. QCD events that satisfy the cut on a boosted top-tagger). Generating enough
background Monte Carlo events in these signal-like regions of phase space is one of the
computational challenges for the LHC and HL-LHC.

Denote passing an event selection cut with the indicator function 1(x). In our probabilis-
1(x), θ)
tic language, we are interested in eﬃciently sampling the showering histories z
|
so that we do not waste computing resources on the expensive detector simulation for events
that won’t satisfy the cuts. When the phase space regions are not aligned with parton level
quantities, then we must perform importance sampling in the parton shower itself, and
1(x), θ), which is diﬃcult to
the ideal importance distribution would be the unfolded p(z
estimate when working with cuts based on complicated jet observables.

p(z

∼

|

Recent developments in probabilistic programming systems oﬀer a potential way to
address these challenges. Probabilistic programming systems provide tools for inferring
the latent state of a simulator based on some observations (e.g., p(z
x, θ)), and they
use the simulator directly during inference. As mentioned above, the pyprob probabilistic
programming system was integrated with the Sherpa event generator via the ppx protocol [15,
16]. By instrumenting Sherpa with ppx, the pyprob system is able to bias the control ﬂow of
the event generator to perform advanced forms of importance sampling. In Refs. [15, 16] a
large recurrent neural network learned an eﬃcient importance sampling distribution q(z
x);
however, the target was τ lepton decay instead of jet physics. More recently, we have
instrumented our Ginkgo generator with pyprob, which we will describe below.

|

|

2 Ginkgo: A simpliﬁed generative model for jets

At present, it is very hard to access the joint likelihood in state-of-the-art parton shower
generators in full physics simulations, e.g. PYTHIA [1], Herwig [2], and Sherpa [3].
Also, typical implementations of parton showers involve sampling procedures that destroy

6

the analytic control of the joint likelihood. Thus, to aid in machine learning research
for jet physics, a python package for a toy generative model of a parton shower, called
Ginkgo, was introduced in [24]. Ginkgo has a tractable joint likelihood, and is as simple and
easy to describe as possible but at the same time captures essential ingredients of parton
shower generators in full physics simulations. It also ensures permutation invariance and
momentum conservation. Ginkgo was designed to enable implementations of probabilistic
programming, diﬀerentiable programming, dynamic programming and variational inference.
Within the analogy between jets and NLP, Ginkgo can be thought of as ground-truth parse
trees with a known language model.

Ginkgo implements a recursive algorithm to generate a binary tree, where each node is
represented by an energy-momentum vector and the leaves are the jet constituents. We
want our model to represent the following features:

• Momentum conservation: the total momentum of the jet (root of the tree) is obtained

from adding the momentum of all of its constituents.

• Running of the splitting scale: each splitting is characterized by a scale t that decreases
when evolving down the tree from root to leaves (t is the invariant squared mass,
t = m2).

We also want our model to lead to a natural analogue of the generalized kt clustering

algorithms [6, 7, 8, 9] for the generated jets. These algorithms are characterized by

• Permutation invariance: the jet momentum should be invariant with respect to the

order in which we cluster its constituents.

• Distance measure: the angular separation between two jet constituents is typically
used as a distance measure among them. In particular, traditional jet clustering
algorithms are based on a measure given by dij
ij where ∆Rij is the angular
separation between two particles.

∆R2

∝

2.1 The generative process

The generative process depends on the following input parameters:

0 : four-momentum of the jet (input value for the root node of the tree).

• pµ
• t0: initial squared mass.
• tcut: cut-oﬀ squared mass to stop the showering process.
• λ: decaying rate for the exponential distribution.

During the generative process, starting from the root of the tree, each parent node
is split, generating a left (L) and a right (R) child. At each splitting we sample squared
invariant masses for the children, tL, tR from a decaying exponential. We require the
constraint √tL + √tR < √tP , where √tP is the parent mass. Then we implement a 2-body
decay in the parent center-of-mass frame. The children direction is obtained by uniformly

7

sampling a unit vector on the 2-sphere (in the parent center-of-mass frame the children
move in opposite directions). Finally, we apply a Lorentz boost to the lab frame, to obtain
the 4 dimensional vector pµ = (E, px, py, pz) that characterizes each node. This prescription
ensures momentum conservation and permutation invariance. Next, we outline the splitting
of a node as follows.

(3)

(4)

1
e−λ
λ

−

(√tP

−

1. Draw tL and tR from an exponential distribution,
λ
tP

λ, tP) =

f (t
|

tL

∼

1

− λ
e
tP

t

tR

f (t
|

∼

λ, tP, tL) =

1
e−λ

−

1

−

√tL)2 e

√
(

λ
√
tP−

tL)2 t

max

and deﬁne mL = √tL and mR = √tR. We apply a veto on sampled values where
tL (cid:62) tP and tR (cid:62) (√tP
√tL)2. For inference, given two particles, we assign
min
and tR
tL, tR
tL, tR
tL
{

−
→
2. Compute a 2-body decay in the parent rest frame.
3. Apply a Lorentz boost to each of the children, with γ = Ep√
tP
4. If tL (tR) is greater than tcut repeat the process.

and γβ =

(cid:126)pp

/√tP .
|

|

→

}

{

}

.

The algorithm is outlined in Algorithm 1. After running, the ﬁnal binary tree for the jet is
obtained.

Algorithm 1: Toy Parton Shower Generator
1 function NodeProcessing (pµ
p, tP, tcut, λ, tree)

Input

: parent momentum (cid:126)pp, parent mass squared tp, cut-oﬀ mass squared tcut,
rate for the exponential distribution λ, binary tree tree

2

3

4

5

6

7

8

9

Add parent node to tree.
if tp > tcut then

Sample tL and tR from the decaying exponential distribution.
Sample a unit vector from a uniform distribution over the 2-sphere.
Compute the 2-body decay of the parent node in the parent rest frame.
Apply a Lorentz boost to the lab frame to each child.
NodeProcessing (pµ
p, tL, tcut, λ, tree)
NodeProcessing (pµ
p, tR, tcut, λ, tree)

2.2 Reconstruction: The Likelihood for a Proposed Jet Clustering

In addition to the generative model described above, which is used for generating data
with Monte Carlo, we also need to be able to assign a likelihood value to a proposed jet
clustering. To do this we use the same general form for the jet’s likelihood based on a
product of likelihoods over each splitting as in Eq. 1. In order to evaluate this we need

8

Figure 2: 2D heat clustermap visualizations of Ginkgo jets, where the leaves ordering corresponds
to the order to access them when traversing the truth tree (columns) and each clustering algorithm
(rows), where we show beam search (left) and kt (right).

to ﬁrst reconstruct the parent from the left and right children. Then we use the same
equations described above (Eq. 3 and 4) for the splitting probabilities that are used in the
generative model. The Ginkgolibrary provides functions to evaluate the joint likelihood
λ, tcut) of any proposed hierarchical clustering of the observed ﬁnal state particles.
p(x, z

|

2.3 Greedy and beam search algorithms

As described in Sec. 1.1.1, we can reframe the goal of jet clustering as ﬁnding the maximum
likelihood estimate (MLE) for the latent structure of a jet, given a set of constituents
(leaves). Diﬀerent algorithms will return diﬀerent tree-like hierarchical clusterings zshower,
and we can compare the performance of various algorithms. We study approximate solutions
for bottom-up agglomerative clustering like the generalized-kt algorithms (which are a class
of greedy algorithms that locally maximize the likelihood at each step in the clustering
process) and beam search (which maximizes the likelihood of multiple steps before choosing
what to cluster).

We provide implementations of these algorithms on Ginkgo jets in [25]. We also
developed a visualization package in [26] and show examples below. In Fig. 2 we show 2D
heat clustermaps where the color scale speciﬁes the total number of steps needed to connect
any two leaves through their closest common ancestor using the truth-level jet tree. The
better the truth tree latent structure is reconstructed, the more the heat map structure
looks block diagonal.

9

3 Examples of Research Enabled with Ginkgo

In this section we highlight some of recent research that has been enabled with Ginkgo.

3.1 Hierarchical Cluster Trellis Algorithm

Jet clustering in high-energy physics is a siloed sub-ﬁeld of research, which is ironic given
that hierarchical clustering is a common task in many areas of science and can be eﬀectively
abstracted. Hierarchical clustering is often used to discover meaningful structures, such as
phylogenetic trees of organisms [27], taxonomies of concepts [28], subtypes of cancer [29].
We deﬁne a hierarchical clustering as a recursive splitting of a dataset of elements,
N
i=1 into subsets until reaching singletons, e.g. leaves of a binary tree. This can
X =
}
equivalently be viewed as starting with the set of singletons and repeatedly taking the union
of sets until reaching the entire dataset.

xi

{

The authors of Ref. [23] consider an energy-based probabilistic model for hierarchical
clustering. The model is based on measuring the compatibility of each pair of sibling nodes,
R+. We also denote the potential function
described by a potential function ψ : 2X
for a hierarchical clustering H and dataset X as φ(X
H). Then, the probability of H for the
|
X), is equal to the unnormalized potential of H normalized by the partition
dataset X, P (H
|
function (marginal likelihood), Z(X):

2X

→

×

X) =
P (H
|

φ(X
H)
|
Z(X)

with

φ(X

H) =
|

(cid:89)

ψ(XL, XR)

(5)

XL,XR∈siblings(H)

where the partition function is given by Z(X) = (cid:80)

H∈H(X) φ(X

H).
|

Next, they deﬁne MAP hierarchy as the maximum likelihood hierarchical clustering given
a dataset X. Exactly performing inference on the MAP hierarchy and ﬁnding the partition
function by enumerating all hierarchical clusterings over N elements is exceptionally diﬃcult
because the number of hierarchies grows extremely rapidly, namely (2N

3)!! [30, 31].

To overcome the computational burden, a cluster trellis data structure for hierarchical
(3N ) time,
clustering was introduced in [23]. The trellis computes these quantities in the
without having to iterate over each possible hierarchy. While still exponential, this is
feasible in regimes where enumerating all possible trees would be infeasible, and is to our
knowledge the fastest exact MAP/partition function result, making practical exact inference
for datasets on the order of 20 points (

1022 trees) or fewer.

109 operations vs

O

3

−

∼

×

∼

We brieﬂy review novel dynamic-programming algorithms for exact (and approx.) infer-
ence in hierarchical clustering introduced in [23]. The trellis allows us to compute the parti-
tion function Z(X) and MAP inference, i.e. ﬁnd the maximum likelihood tree structure.
The Cluster Trellis package is available at github.com/SebastianMacaluso/ClusterTrellis.
Each node in the trellis corresponds to all subsets of elements (jet constituents). A schematic
representation and the assignment between nodes in a binary tree and nodes in the trellis
is shown in Fig. 6.

10

Figure 3: Schematic representation of
the trellis and node assignment between
the trellis and a binary tree.

Computing the Partition Function. The partition function, Z(X), for every node
in the trellis is computed in order (in a bottom-up approach), memoizing the partial value
at each node.

Computing the Maximum Likelihood Hierarchical Clustering. The MAP hier-
X) = argmaxH∈H(X) φ(H). This

archy for dataset X, H(cid:63)(X), is H(cid:63)(X) = argmaxH∈H(X) P (H
|
is also computed in order in a bottom-up approach.

Sampling from the Posterior Distribution. Drawing samples from the true pos-
terior distribution P (H
X) is also diﬃcult because of the extremely large number of trees.
|
However, there is a sampling procedure implemented using the trellis which gives samples
from the exact true posterior without enumerating all possible hierarchies.

3.1.1 Sparse Cluster Trellis

The authors of Ref. [23] also introduced a sparse trellis data structure, which allows the
algorithms to scale to larger datasets by controlling the sparsity index, i.e. the fraction of
the total number of possible clusterings being considered. Most clusterings have likelihood
values orders of magnitude smaller than the MAP clustering making their contribution to
the partition function negligible. As a result, if we build a sparse trellis that considers the
most relevant hierarchies, we could ﬁnd approximate solutions for inference in datasets
where implementing the full trellis is not feasible. The sparse trellis can be constructed from
samples (e.g., ground truth from a simulator, greedy, or beam search trees) or randomly
sampling pairwise splittings for the children of a node.

3.1.2 Results

In Fig. 4 (left) we show the partition function versus the MAP hierarchy for each set
of leaves from a Ginkgo dataset. Figure 4 (right) shows the results from sampling 105
hierarchies (black dots) and the expected distribution. Figure 5 shows the performance of
the sparse trellis to calculate the MAP values on a set of 100 Ginkgo jets with 9 leaves.
Even though beam search has a good performance for trees with a small number of leaves,
we see that both sparse trellises quickly improve over beam search, with a sparsity index of
only about 2%.

11

Figure 4: Left: scatter plot of the partition function Z vs. the maximum log likelihood for a Ginkgo
dataset, with up to 10 jet constituents. The color indicates the number of leaves of each hierarchical
clustering. Right: comparison of the posterior distribution for a speciﬁc jet with ﬁve leaves from
sampling 105 hierarchies (black dots with small error bars) and expected posterior distribution (in
green). The log likelihood for the ground truth tree is a vertical dashed red line.

Figure 5: Trellises MAP hierarchy log likelihood
(values are relative to the greedy algorithm) vs
their sparsity. Each value corresponds to the
mean over 100 trees of a test dataset. We show
the Simulator (Sim.) and the Beam Search (BS)
trellises.

12

−60−50−40−30‘=logp(x,HMAP)−50−45−40−35−30−25logZ=log∑p(x,H)5678910Numberofleaves−36−34−32−30‘=logp(x,H)0.000.250.500.751.001.251.501.75p(‘|x)GroundTruthSampledExpected10−410−310−210−1100Sparsity−0.50.00.51.01.5logp(x,H∗)relativetogreedyFullTrellisBSGreedyBSTrellis(pT)Sim.Trellis(pT)3.2 Hierarchical clustering through reinforcement learning

In this section we review results from [32] that cast hierarchical clustering as a Markov
Decision Process (MDP) and adapted reinforcement learning algorithms to solve it. In
particular, Monte-Carlo Tree Search (MCTS) guided by a neural network policy was
adapted to the problem of jet clustering. This approach closely follows the AlphaZero
algorithm [33, 34, 35], which achieved superhuman performance in a range of board games,
demonstrating its ability to eﬃciently search large combinatorial spaces. While (model-free)
RL methods have been used in the context of jet grooming, i.e. pruning an existing tree to
remove certain backgrounds [36], they have not yet been used for clustering, that is, the
construction of the binary tree itself.

3.2.1 Jet clustering as a Markov Decision Process

The authors of Ref. [32] used the ingredients of Ginkgo to recast the problem of clustering
as an MDP, which is deﬁned by the quartet (

, P, R):

,

S

A

is given by all possible particle sets at any given point during the

• The state space

clustering process, s = zt.

S

• The actions
merged.

A

are the choice of two particles a = (i, j) with 1

i < j

≤

≤

nt to be

• The state transitions P are deterministic and update zt to zt−1 by replacing the
particles pt,i and pt,j with a parent pt−1,i = pt,i + pt,j. All other particles are left
unchanged, each state transition thus reduces the number of particles by one.

• The rewards R are the splitting probabilities, R(s = zt, a = (i, j)) = log ps(zt

zt−1(i, j)).
|

• The MDP is episodic and terminates when only a single particle is left.

An agent solves the jet clustering problem by ﬁrst considering the state of all observed,
ﬁnal-state particles and choosing which two to merge into a parent. It receives the log
likelihood of this splitting as reward. Next, it considers the reduced set of particles where
the two chosen particles have been replaced by their proposed parent, chooses the next pair
of particles to merge, and so on. Rolling out an episode leads to a proposed clustering tree
, with the total received reward being equal to the log likelihood of this
z =
z1, . . . , zN
}
{
tree following Eq. (1).

The formulation of jet clustering as an MDP allows us to use any (model-free) rein-
forcement learning (RL) algorithm to tackle it. Since the state transition model is known
(and deterministic), they instead use in [32], a model-based planning approach to leverage
this knowledge. They chose Monte Carlo Tree Search (MCTS) [33], which builds a search
tree over possible clusterings z by rolling out a number of clusterings. In addition, the
authors considered a clustering algorithm based on imitation learning, speciﬁcally Behavioral
Cloning (BC): a policy π is trained to imitate the actions that reconstruct the true trees,
which we can extract from the generative model, by maximizing log π(s, atruth).

13

Figure 6: Mean log likelihood of clustered jets (larger is better). Left: against the
computational cost, measured as the number of evaluations of the splitting likelihood ps
required by the diﬀerent algorithms. Right: as a function of the number of ﬁnal-state
particles (leaves of the tree), using the best-performing (and most computationally expensive)
hyperparameter setup for each algorithm. MCTS (solid, red) gives the highest-quality jet
clusterings.
3.2.2 Results

We present a comparison of the diﬀerent clustering algorithms on a dataset of Ginkgo
jets, taken from [32]. They compare MCTS and BC agents to a greedy algorithm, a
beam search algorithm that maintains the b = 1000 most likely clusterings, and a random
policy. For jets with a small number of ﬁnal-state particles we also compute the trellis exact
maximum likelihood tree (MLE) following Ref. [23]. Fig. 6 shows the log likelihood of the
clustering against its computational cost (left) and against the number of ﬁnal-state particles
(right). While the greedy and beam search baselines lead to a robust performance at low
computational cost, MCTS planning can generate hierarchical clusterings of a markedly
higher likelihood. This advantage is more pronounced at larger number of ﬁnal-state
particles, showing that MCTS can explore large combinatorial spaces better than these
baselines.

3.3 Hierarchical clustering using A*

In this section we review an algorithm introduced in Ref. [37], that combines A* search with
a novel trellis data structure to overcome the prohibitively large search space of hierarchical
clusterings, namely (2N-3)!! hierarchies for N elements. This method can be applied to
the same type of energy-based probabilistic models for hierarchical clustering introduced in
Ref. [23].

A* Search. A* is a best-ﬁrst search algorithm for ﬁnding the minimum cost path between
a starting node and a goal node in a weighted graph. Following canonical notation, the
function f (n) = g(n) + h(n) determines the most promising node to search next, where

14

103104105Cost (splitting likelihood evaluations)96959493Tree log likelihoodGreedyBeam searchMCTS1015Number of leaves01234Tree log likelihood relative to greedyMLERandomGreedyBeam searchMCTSBC1015150100500g(n) is the cost of the path up from the start to node n and h(n) is a heuristic estimating
the cost of traveling from n to a goal. Also, when the heuristic h(
) is admissible (always
·
under estimates cost), A* is admissible and provides an optimal solution.

In order to redeﬁne clustering as a search problem, we need to deﬁne the search space,
the graph being searched over, and the goal states. Each state in the search space is deﬁned
as a partial hierarchical clustering, which is a subset of some hierarchical clustering of
the elements, e.g. a sub-tree. The fundamental challenge of applying A* to clustering is
the massive state space. Na¨ıve representations of the A* state space and frontier require
explicitly storing every tree structure, potentially growing to be at least the number of
3)!!. To overcome this, in Ref. [37] an approach was proposed using a
binary trees (2n
cluster trellis to (1) compactly encode states in the space of hierarchical clusterings (as
paths from the root to the leaves of the trellis), and (2) compactly represent the search
frontier (as nested priority queues). This method, reduces the super-exponential space and
time required for A* to ﬁnd the exact MAP hierarchy to exponential in the worst case.

−

Aproximate A*. An approx. version of A*, feasible for a much greater number of
elements (jet constituents), was also implemented in Ref. [37]. In this case, a sparse trellis
(one with missing nodes and/or edges) is initialized using beam search. Then, (1) A* is
run over this sparse trellis, (2) the sparse trellis is extended by adding nodes, and edges
corresponding to child / parent relationships while running A* at exploration time for each
node explored during A* search, (3) A* is run iteratively, obtaining a solution and then
running A* again, extending the trellis further between or during subsequent iterations.

3.3.1 Results

Figure 7 shows a comparison of the MAP values of the proposed exact and approximate
algorithms using A* with benchmark algorithms (greedy, beam search, MCTS, and exact
trellis), on Ginkgo jets 2. Approximate versions of A* allow the algorithm to handle jets
with many more constituents while signiﬁcantly improving over beam search and greedy
baselines. MCTS provides a strong baseline for small datasets, where it is feasible to
implement it. However, A* shows an improvement over MCTS while also being feasible for
jets with more constituents.

3.4 Probabilistic Programming

The Monte Carlo simulators implicitly describe the complicated distribution p(x, z
θ) and
|
implement sampling through random number generators. Probabilistic programming
extends this functionality with the ability to condition on the values of some of the random
variables x or z [38], and it achieves this by hijacking the random number generators. This
controlling inference algorithm uses those hooks to bias the simulator towards the desired
output (e.g. importance sampling [39]) or through Monte Carlo sampling. In particular, it

2These algorithms can be run from github.com/SebastianMacaluso/HCmanager.

15

Figure 7: Jet Physics Results. Cost (Neg. log. likelihood) for the MAP hierarchy vs number of
elements (jet constituents) of each algorithm on Ginkgo jets minus the cost for greedy (lower values
are better solutions). We see that both exact and approx. A* greatly improve over beam search and
greedy. Though MCTS provides a strong baseline for small jets where it is feasible to implement it
(left), A* still shows an improvement over it.

∼

1(x), θ).
|

x, θ),
provides the ability to sample latent variables conditioned on observations, i.e. z
and observations conditioned on latent variables, i.e. x
z, θ). For example, this
|
technique can be used to eﬃciently sample the tails of backgrounds in signal-rich regions of
phase space z

p(x

p(z

p(z

∼

∼

|

As a proof of concept, we chose to use the PyProb framework [40] (applied to the
Sherpa event generator in Refs. [41]) to implement probabilistic programming in Ginkgo.
After integrating PyProb and Ginkgo, we successfully sampled jets while conditioning on
variables, such as the jet transverse momentum and the number of constituents. The
histogram in Fig. 8 demonstrates one simple example of the eﬀectiveness of this framework
for sampling tails of distributions. Using PyProb, we are able to force Ginkgo to only
produce samples of jets with fewer than eight or more than twenty-six constituents. We can
see that importance sampling accurately samples the desired regions of the distribution, i.e.
with the same relative probabilities as the original distribution. Though this is a simple
example, this method is powerful enough to allow us to condition on any sampled value
within the generator, including latent variables, and condition on those values or arbitrary
combinations of them.

4 Conclusion
This paper introduces a framing of common tasks in jet physics in probabilistic terms.
We present Ginkgo, a simpliﬁed generative model for jets designed to facilitate research
into new computational techniques for jet physics. A novel trellis data structure and
dynamic programming algorithms that have been developed for hierarchical clustering were
motivated by this area of research. The Ginkgo library has been interfaced with both

16

102030Number of elements6543210Cost - Greedy CostGreedyBeam SearchMCTSApprox. A*Exact A*Exact Trellis80100120140Number of elements120100806040200Cost - Greedy CostGreedyBeam SearchApprox. A*Figure 8: Histogram of the number of jet
constituents (leaves) for jets generated with
Ginkgo. We show the distribution of the num-
ber of constituents with no constraints (blue)
and the one when using importance sampling
with the limits on the number of leaves de-
ﬁned by the red dashed lines (orange).

an implementation of the trellis and A* algorithms and the Open AI Gym reinforcement
learning library. We presented comparisons of jet clustering using greedy, beam search,
Monte Carlo Tree Search, the sparse and full cluster trellis, and exact and approx. A*
search algorithms. These new algorithms provide a principled alternative to the generalized
kt algorithms, which are based on a greedy sequential clustering algorithm. Additionally, we
show that the trellis allows to marginalize and sample from the true posterior distribution
of clustering histories for a set of jet constituents. This could ameliorate bottlenecks
when implementing the CKKW-L matching algorithm for events with large jet multiplicity.
Finally, we presented how complicated regions of phase space could be sampled using
probabilistic programming.

Acknowledgements

We would like to thank Craig S. Greenberg, Nicholas Monath, Ji-Ah Lee, Patrick Flaherty,
Andrew McGregor, and Andrew McCallum for their collaboration on eﬃcient maximum
likelihood estimation on Ginkgo with a trellis. We also would like to thank Craig S.
Greenberg, Nicholas Monath, Avinava Dubey, Patrick Flaherty, Manzil Zaheer, Amr
Ahmed, and Andrew McCallum for their collaboration on exact and approximate hierarchical
clustering using A*. We are grateful for the support of the National Science Foundation
under the awards ACI-1450310, OAC-1836650, and OAC-1841471, the Moore-Sloan data
science environment at NYU, as well as the NYU IT High Performance Computing resources,
services, and staﬀ expertise.

References

[1] T. Sj¨ostrand, S. Ask, J. R. Christiansen, R. Corke, N. Desai, P. Ilten, S. Mrenna,

S. Prestel, C. O. Rasmussen, and P. Z. Skands, “An introduction to PYTHIA 8.2,”
Comput. Phys. Commun. 191 (2015) 159–177, arXiv:1410.3012 [hep-ph].

[2] J. Bellm et al., “Herwig 7.0/Herwig++ 3.0 release note,” Eur. Phys. J. C 76 no. 4,

(2016) 196, arXiv:1512.01178 [hep-ph].

17

[3] T. Gleisberg, S. Hoeche, F. Krauss, M. Schonherr, S. Schumann, F. Siegert, and

J. Winter, “Event generation with SHERPA 1.1,” JHEP 02 (2009) 007,
arXiv:0811.4622 [hep-ph].

[4] J. Brehmer, K. Cranmer, G. Louppe, and J. Pavez, “A Guide to Constraining

Eﬀective Field Theories with Machine Learning,” Phys. Rev. D 98 no. 5, (2018)
052004, arXiv:1805.00020 [hep-ph].

[5] K. Cranmer, J. Brehmer, and G. Louppe, “The frontier of simulation-based inference,”

Proceedings of the National Academy of Sciences (2020) ,
https://www.pnas.org/content/early/2020/05/28/1912789117.full.pdf.
https://www.pnas.org/content/early/2020/05/28/1912789117.

[6] M. Cacciari, G. P. Salam, and G. Soyez, “The anti-kt jet clustering algorithm,” JHEP

04 (2008) 063, arXiv:0802.1189 [hep-ph].

[7] S. Catani, Y. L. Dokshitzer, M. H. Seymour, and B. R. Webber, “Longitudinally

invariant kt clustering algorithms for hadron hadron collisions,” Nucl. Phys. B 406
(1993) 187–224.

[8] Y. L. Dokshitzer, G. D. Leder, S. Moretti, and B. R. Webber, “Better jet clustering

algorithms,” JHEP 08 (1997) 1, arXiv:9707323 [hep-ph].

[9] S. D. Ellis and D. E. Soper, “Successive combination jet algorithm for hadron
collisions,” Phys. Rev. D 48 (1993) 3160–3166, arXiv:9305266 [hep-ph].

[10] A. Buckley, H. Hoeth, H. Lacker, H. Schulz, and J. E. von Seggern, “Systematic event

generator tuning for the LHC,” Eur. Phys. J. C 65 (2010) 331–357,
arXiv:0907.2973 [hep-ph].

[11] J. Brehmer, K. Cranmer, G. Louppe, and J. Pavez, “Constraining Eﬀective Field
Theories with Machine Learning,” Phys. Rev. Lett. 121 no. 11, (2018) 111801,
arXiv:1805.00013 [hep-ph].

[12] J. Brehmer, G. Louppe, J. Pavez, and K. Cranmer, “Mining gold from implicit models

to improve likelihood-free inference,” arXiv:1805.12244 [stat.ML].

[13] A. Andreassen and B. Nachman, “Neural Networks for Full Phase-space Reweighting
and Parameter Tuning,” Phys. Rev. D 101 no. 9, (2020) 091901, arXiv:1907.08209
[hep-ph].

[14] G. Louppe, J. Hermans, and K. Cranmer, “Adversarial Variational Optimization of

Non-Diﬀerentiable Simulators,” arXiv:1707.07113 [stat.ML].

[15] A. G. Baydin et al., “Etalumis: Bringing Probabilistic Programming to Scientiﬁc

Simulators at Scale,” 7, 2019. arXiv:1907.03382 [cs.LG].

18

[16] A. G. Baydin et al., “Eﬃcient Probabilistic Inference in the Quest for Physics Beyond

the Standard Model,” arXiv:1807.07706 [cs.LG].

[17] S. Catani, F. Krauss, R. Kuhn, and B. Webber, “QCD matrix elements + parton

showers,” JHEP 11 (2001) 063, arXiv:hep-ph/0109231.

[18] L. Lonnblad, “Correcting the color dipole cascade model with ﬁxed order matrix

elements,” JHEP 05 (2002) 046, arXiv:hep-ph/0112284.

[19] D. E. Soper and M. Spannowsky, “Finding physics signals with shower deconstruction,”

Phys. Rev. D 84 (2011) 074002, arXiv:1102.3480 [hep-ph].

[20] D. E. Soper and M. Spannowsky, “Finding top quarks with shower deconstruction,”

Phys. Rev. D 87 (2013) 054012, arXiv:1211.3140 [hep-ph].

[21] D. Ferreira de Lima, P. Petrov, D. Soper, and M. Spannowsky, “Quark-Gluon tagging
with Shower Deconstruction: Unearthing dark matter and Higgs couplings,” Phys.
Rev. D 95 no. 3, (2017) 034001, arXiv:1607.06031 [hep-ph].

[22] S. H¨oche, S. Prestel, and H. Schulz, “Simulation of Vector Boson Plus Many Jet Final

States at the High Luminosity LHC,” Phys. Rev. D 100 no. 1, (2019) 014024,
arXiv:1905.05120 [hep-ph].

[23] C. S. Greenberg, S. Macaluso, N. Monath, J.-A. Lee, P. Flaherty, K. Cranmer,

A. McGregor, and A. McCallum, “Data Structures
in Hierarchical Clustering,” arXiv:2002.11661 [cs.DS].

& Algorithms for Exact Inference
\

[24] K. Cranmer, S. Macaluso, and D. Pappadopulo, “Toy Generative Model for Jets

Package,” 2019. https://github.com/SebastianMacaluso/ToyJetsShower.

[25] Cranmer, Kyle and Macaluso, Sebastian and Pappadopulo, Duccio, “Greedy and

Beam Search clustering algorithms for jet physics,” 2019.
https://github.com/SebastianMacaluso/StandardHC.

[26] Cranmer, Kyle and Macaluso, Sebastian and Pappadopulo, Duccio, “Visualize Binary
Trees Package,” 2019. https://github.com/SebastianMacaluso/VisualizeBinaryTrees.

[27] A. Kraskov, H. St¨ogbauer, R. G. Andrzejak, and P. Grassberger, “Hierarchical

clustering using mutual information,” EPL (Europhysics Letters) 70 no. 2, (2005) 278.

[28] P. Cimiano and S. Staab, “Learning concept hierarchies from text with a guided

agglomerative clustering algorithm,” in Proceedings of the ICML 2005 Workshop on
Learning and Extending Lexical Ontologies with Machine Learning Methods. 2005.

19

[29] T. Sørlie, C. M. Perou, R. Tibshirani, T. Aas, S. Geisler, H. Johnsen, T. Hastie, M. B.
Eisen, M. Van De Rijn, S. S. Jeﬀrey, et al., “Gene expression patterns of breast
carcinomas distinguish tumor subclasses with clinical implications,” Proceedings of the
National Academy of Sciences 98 no. 19, (2001) 10869–10874.

[30] D. Callan, “A combinatorial survey of identities for the double factorial,” 2009.

[31] E. Dale and J. Moon, “The permuted analogues of three Catalan sets,” 1993.

[32] J. Brehmer, S. Macaluso, D. Pappadopulo, and K. Cranmer, “Hierarchical clustering
in particle physics through reinforcement learning,” in 34th Conference on Neural
Information Processing Systems. 11, 2020. arXiv:2011.08191 [cs.AI].

[33] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,

J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, and Others,
“Mastering the game of go with deep neural networks and tree search,” nature 529
no. 7587, (2016) 484–489.

[34] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez,

T. Hubert, L. Baker, M. Lai, A. Bolton, and Others, “Mastering the game of go
without human knowledge,” nature 550 no. 7676, (2017) 354–359.

[35] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot,

L. Sifre, D. Kumaran, T. Graepel, and Others, “Mastering chess and shogi by self-play
with a general reinforcement learning algorithm,” arXiv:1712.01815 (2017) .

[36] S. Carrazza and F. A. Dreyer, “Jet grooming through reinforcement learning,” Phys.

Rev. D100 no. 1, (2019) 014014, arXiv:1903.09644 [hep-ph].

[37] C. S. Greenberg, S. Macaluso, N. Monath, A. Dubey, P. Flaherty, M. Zaheer,

A. Ahmed, K. Cranmer, and A. Mccallum, “Exact and Approximate Hierarchical
Clustering Using A*,” arXiv:2104.07061 [cs.LG].

[38] A. D. Gordon, T. A. Henzinger, A. V. Nori, and S. K. Rajamani, “Probabilistic

programming,” in Proceedings of the on Future of Software Engineering, pp. 167–181.
2014.

[39] T. A. Le, A. G. Baydin, and F. Wood, “Inference compilation and universal

probabilistic programming,” in Artiﬁcial Intelligence and Statistics, pp. 1338–1348.
2017.

[40] A. G. Baydin, L. Shao, W. Bhimji, L. Heinrich, S. Naderiparizi, A. Munk, J. Liu,

B. Gram-Hansen, G. Louppe, L. Meadows, et al., “Eﬃcient probabilistic inference in
the quest for physics beyond the standard model,” in Advances in neural information
processing systems, pp. 5459–5472. 2019.

20

[41] A. G. Baydin, L. Shao, W. Bhimji, L. Heinrich, L. Meadows, J. Liu, A. Munk,

S. Naderiparizi, B. Gram-Hansen, G. Louppe, et al., “Etalumis: Bringing probabilistic
programming to scientiﬁc simulators at scale,” in Proceedings of the International
Conference for High Performance Computing, Networking, Storage and Analysis,
pp. 1–24. 2019.

21

