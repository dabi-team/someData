1
2
0
2

p
e
S
9

]

G
L
.
s
c
[

1
v
1
5
3
4
0
.
9
0
1
2
:
v
i
X
r
a

PREPRINT

ABSTRACT

Josef Kircher
Chair of Mechatronics
Augsburg University
josef.kircher@student
.uni-augsburg.de

Lars Mikelsons
Chair of Mechatronics
Augsburg University
lars.mikelsons@informatik
.uni-augsburg.de

Tobias Thummerer
Chair of Mechatronics
Augsburg University
tobias.thummerer@informatik
.uni-augsburg.de

NEURALFMU: TOWARDS STRUCTURAL INTEGRATION OF
FMUS INTO NEURAL NETWORKS

This paper covers two major subjects: First, the presentation of a new open-source library called
FMI.jl for integrating FMI into the Julia programming environment by providing the possibility
to load, parameterize and simulate FMUs. Further, an extension to this library called FMIFlux.jl
is introduced, that allows the integration of FMUs into a neural network topology to obtain a
NeuralFMU. This structural combination of an industry typical black-box model and a data-driven
machine learning model combines the different advantages of both modeling approaches in one single
development environment. This allows for the usage of advanced data driven modeling techniques
for physical effects that are difÔ¨Åcult to model based on Ô¨Årst principles.

P R E P RIN T

Models inside closed simulation tools make hybrid model-
ing difÔ¨Åcult, because for training data-driven model parts,
determination of the loss gradient through the Neural Net-
work (NN) and the model itself is needed. Nevertheless,
the structural integration of models inside machine learning
topologies like NNs is a research topic that gathered more
and more attention. When it comes to learning system
dynamics, the structural injection of algorithmic numerical
solvers into NNs lead to large improvements in perfor-
mance, memory cost and numerical precision over the use
of residual neural networks [3], while offering a new range
of possibilities, e.g. Ô¨Åtting data that was observed at irreg-
ular time steps [8]. The result of integrating a numerical
solver for ordinary differential equations (ODEs) into a
NN is known as NeuralODE. For the Julia programming
language (from here on simply referred to as Julia), a ready-
to-use library for building and training NeuralODEs named
DiffEqFlux.jl1 is already available [12]. Probably the most
mentioned point of criticism regarding NeuralODEs is the
difÔ¨Åcult porting to real world applications (s. section 3.1.2
and 3.1.3).

A different approach for hybrid modeling, as in Raissi,
Perdikaris, and Karniadakis [15], is the integration of the
physical model into the machine learning process by eval-
uating (forward propagating) the physical model as part
of the loss function during training in so called Physics-
informed Neural Networks (PINNs). In contrast, this con-
tribution focuses on the structural integration of Functional
Mock-up Units (FMUs) into the NN itself and not only
the cost function, allowing much more Ô¨Çexibility with re-
spect to what can be learned and inÔ¨Çuenced. However, it is
also possible to build and train PINNs with the presented
library.

Finally, another approach are Bayesian Neural Stochastic
Differential Equations (BNSDE) as is Haussmann et al. [5],
which use bayesian model selection together with Probably
Approximately Correct (PAC) bayesian bounds during the
NN training to improve hybrid model accuracy on basis of
noisy prior knowledge. For an overview on the growing
Ô¨Åeld of hybrid modeling see e.g. [17] or [14].

To conclude, hybrid modeling with its different facets is
an emerging research Ô¨Åeld, but still chained to academic
use-cases. It seems a logical next step to open up these
auspicious ML-technologies, besides many more not men-
tioned, to industrial applications.

Keywords NeuralFMU, FMI, FMU, Julia, NeuralODE

Introduction

1

1https://github.com/SciML/DiffEqFlux.jl

 
 
 
 
 
 
PREPRINT

2 Presenting the Libraries

‚Ä¢ FMI.jl: load, instantiate, parameterize and simu-
late FMUs seamlessly inside the Julia program-
ming language

‚Ä¢ FMIFlux.jl: place FMUs simply inside any feed-
forward NN topology and still keep the resulting
hybrid model trainable with a standard Automatic
Differentiation (AD) training process

Because the result of integrating a numerical solver into
a NN is known as NeuralODE, we suggest to pursue this
naming convention by presenting the integration of a FMU,
NN and a numerical solver as NeuralFMU.

This paper is further structured into four topics: The presen-
tation of our libraries FMI.jl and FMIFlux.jl, an example
handling a NeuralFMU setup and training, the explanation
of the methodical background and Ô¨Ånally a short conclu-
sion with future outlook.

the modeling tool. An exported model container, that ful-
Ô¨Ålls the FMI-requirements is called FMU. FMUs can be
used in other simulation environments or even inside of
entire co-simulations like System Structure and Parameter-
ization (SSP) [10]. FMUs are subdivided into two major
classes: model exchange (ME) and co-simulation (CS).
The different use-cases depend on the FMU-type and the
availability of standardized, but optional implemented
FMI-functions.

Combining physical and data-driven models inside a sin-
gle industry tool is currently not possible, therefore it is
necessary to port models to a more suitable environment.
An industry typical model exchange format is needed. Be-
cause the Functional Mock-up Interface (FMI) is an open
standard and widely used in industry as well as in research
applications, it is a suitable candidate for this aim. Fi-
nally, a software interface that integrates FMI into the
ML-environment is necessary. Therefore, we present two
open-source software libraries, which offer all required
features:

By providing the libraries FMI.jl (https://github.com/
ThummeTo/FMI.jl) and FMIFlux.jl (https://github.com/
ThummeTo/FMIFlux.jl), we want to open the topic Neu-
ralODEs for industrial applications, but also lay a founda-
tion to bring other state-of-the-art ML-technologies closer
to production.
In the following two subsections, short
style explanations of the involved tools and techniques are
given.

Our Julia-library FMI.jl provides high-level commands to
unzip, allocate, parameterize and simulate entire FMUs,
as well as plotting the solution and parsing model meta
data from the model description. Because FMI has already
two released speciÔ¨Åcation versions and is under ongoing
development4, one major goal was to provide the ability to
simulate different version FMUs with the same user front-
end. To satisfy users who prefer close-to-speciÔ¨Åcation
programming, as well as users that are new to the topic
and favor a smaller but more high-level command set, we
provide high-level Julia commands, but also the possibility
to use the more low-level commands speciÔ¨Åed in the FMI-
standard [9].

P R E P RIN T

In this section, it is shortly explained and motivated why
the authors picked the Julia programming language for
the presented task. Julia is a dynamic typing language
developing since 2009 and Ô¨Årst published in 2012 [1],
with the aim to provide fast numerical computations in a
platform-independent, high-level programming language
[2]. The language and interpreter was originally invented
at the Massachusetts Institute of Technology, but since
today many other universities and research facilities joined
the development of language expansions, which mirrors in
many contributions from different countries and even in its
own conference, the JuliaCon2. In Elmqvist, Neumayr, and
Otter [4], the library expansion Modia.jl3 was introduced.
Modia.jl allows object-orientated white-box modeling of
mechanical and electrical systems, syntactically similar to
Modelica, in Julia.

using FMI
myFMU = fmiLoad ( " path / to / myFMU . fmu " )
fmiInstantiate !( myFMU )
simData = fmiSimulate ( myFMU , 0.0 ,
10.0; recordValues =[ " mass . s " ])

The shortest way to load a FMU with FMI.jl, simulate it
for t ‚àà {0, 10}, gather and plot simulation data for the
example variable mass.s and free the allocated memory is
implemented only by a few lines of code as follows:

The FMI-standard [9] allows the distribution and exchange
of models in a standardized format and independent of

Listing 1: Simulating FMUs with FMI.jl (high-level).

fmiPlot ( simData )
fmiUnload ( myFMU )

1.2 Functional Mock-up Interface (FMI)

Figure 1: Logo of the library FMI.jl.

Julia Programming Language

4The current version is 2.0.2, but an alpha version 3.0 is

2.1 Simulating FMUs

1.1

2http://www.juliacon.org
3https://github.com/ModiaSim/Modia.jl

already available.

2

PREPRINT

10.0)

2.2.1 ME-FMUs

fmiDoStep ( fmuComp , t , dt )

Please note, that these six lines are not only a code snippet,
but a fully runnable Julia program.

‚Ä¢ ... where to place FMUs inside the NN topology,
as long as all signals are traceable via AD (no
signal cuts).

learning frameworks, a deep NN in Julia using Flux.jl6
is conÔ¨Ågured by chaining multiple neural layers together.
Probably the most intuitive way of integrating a FMU into
this topology, is to simply handle the FMU as a network
layer. In general, FMIFlux.jl does not make restrictions to
...

end
fmiTerminate ( fmuComp )
fmiFreeInstance !( fmuComp )
fmiUnload ( myFMU )

Dependent on the FMU-type, ME or CS, different setups
for NeuralFMUs should be considered. In the following,
two possibilities are presented.

f m i E n t e r I n i t i a l i z a t i o n M o d e ( fmuComp )
f m i E x it I n i t i a l i z a ti o n M o d e ( fmuComp )
dt = 0.1
ts = 0.0: dt :10.0
for t in ts

For users, that prefer more control over memory and per-
formance, the original C-language command set from the
FMI-speciÔ¨Åcation is wrapped into low-level commands
and is available, too. A code snippet, that simulates a
CS-FMU, looks like this:

Listing 2: Simulating CS-FMUs with FMI.jl (low-level).
using FMI
myFMU = fmiLoad ( " path / to / myFMU . fmu " )
fmuComp = fmiInstantiate !( myFMU )
fm iS et upE xperiment ( fmuComp , 0.0 ,

‚Ä¢ ... which FMU-signals can be used as layer
inputs and outputs.
It is possible to use any
variable that can be set via fmiSetReal or
fmiSetContinuousStates as input and any vari-
able that can be retrieved by fmiGetReal or
fmiGetDerivatives as output.

P R E P RIN T

Different interfaces between the FMU layer and NN are
possible. For example, the number of FMU layer inputs
could equal the number of FMU layer outputs and be sim-
ply the number of model states. In Figure 3, the visual-
ization of the suggested structure is given. The top NN
is fed by the current system state (cid:126)xnn. The NN is able to
learn and compensate state-dependent modeling failures
like measurement offsets or physical displacements and
thresholds. After that, the corrected state vector (cid:126)xme is
passed to the ME-FMU, and the current state derivative
Àô(cid:126)xme is retrieved. The bottom NN is able to learn additional
physical effects, like friction or other forces, from the state
derivative vector. Finally the corrected state derivatives
Àô(cid:126)xnn are integrated by the numerical solver, to retrieve the
next system state (cid:126)xnn(t + h). Note, that the time step
size h can be determined by a modern numerical solver
like Tsit5 [16], with different advantages like dynamic step
size and order adaption. This is a signiÔ¨Åcant advantage in
performance and memory cost over the use of recurrent
NNs for numerical integration.

Note, that these function calls are not dependent on the
FMU-Version, but are inspired by the command set of
FMI 2.0.2. The underlying FMI-Version is determined in
the call fmiLoad. Because the naming convention could
change in future versions of the standard, version-speciÔ¨Åc
function calls like fmi2DoStep (the "2" stands for the FMI-
versions 2.x) are available, too. Readers that are famil-
iar with FMI will notice, that the functions fmiLoad and
fmiUnload are not mentioned in the standard deÔ¨Ånition.
The function fmiLoad handles the creation of a temporary
directory for the unpacked data, unpacking of the FMU-
archive, as well as the loading of the FMU-binary and its
model description. In fmiUnload, all FMU-related mem-
ory is freed and the temporary directory is deleted. Beside
CS-FMUs, ME-FMUs are supported, too. The numerical
solving and event handling for ME-FMUs is performed via
the library DifferentialEquations.jl5, the standard library
for solving different types of differential equations in Julia
[13].

For most common applications, the use of ME-FMUs will
be the Ô¨Årst choice. Because of the absence of an integrated
numerical solver inside the FMU, there are much more pos-
sibilities when it comes to learning dynamic processes. A
mathematical view on a ME-FMU leads to the state space
equation (Equation 1) and output equation (Equation 2),
meaning a ME-FMU computes the state derivative Àô(cid:126)xme
and output values (cid:126)yme for the current time step t from a
given state (cid:126)xme and optional input (cid:126)ume:
Àô(cid:126)xme = (cid:126)fme((cid:126)xme, (cid:126)ume, t)
(cid:126)yme = (cid:126)gme((cid:126)xme, (cid:126)ume, t)

The open-source library extension FMIFlux.jl allows for
the fusion of a FMU and a NN. As in many other machine

Note, that many other conÔ¨Ågurations for setting up the
NeuralFMU are thinkable, e.g.:

Figure 2: Logo of the library extension FMIFlux.jl.

Integrating FMUs into NNs

(1)
(2)

2.2

5https://diffeq.sciml.ai/stable/

6https://fluxml.ai/Flux.jl/stable/

3

PREPRINT

‚Ä¢ the bottom NN could learn from the FMU out-
put (cid:126)yme or other model variables, that can be
retrieved by fmiGetReal

‚Ä¢ of course,

2.2.2 CS-FMUs

net = Chain (

Listing 3: A NeuralFMU (ME) in Julia.

To implement the presented ME-NeuralFMU in Julia, the
following code is sufÔ¨Åcient:

there is no restriction to fully-
connected (dense) layers, other feed-forward lay-
ers or even drop-outs are possible

‚Ä¢ there could be a bypass around the FMU between
both NNs to forward state-dependent signals from
the top NN to the bottom NN

Dense ( length ( x_nn ) , ...) ,
... ,
Dense (... , length ( x_me ) ) ,
x_me -> fmiDoStepME ( myFMU , x_me ) ,
Dense ( length ( dx_me ) , ...) ,
... ,
Dense (... , length ( dx_nn ) ) )
nfmu = ME_NeuralFMU ( net , ...)

P R E P RIN T

This prevents the manipulation of system dynamics at the
most effective point: Between the FMU state derivative
output and the numerical integration. However, other tasks
like learning an error correction term, are still possible
to implement. The presence of a numerical solver leads
to a different mathematical description compared to ME-
FMUs: The CS-FMU computes the next state (cid:126)xcs(t + h)
and output (cid:126)ycs(t+h) dependent on its internal current state
(cid:126)xcs (Eq. 3 and 4). Unlike for ME, the state and derivative
values of a CS-FMU are not necessarily known (disclosed
via FMI).

In case of CS-FMUs, the number of layer inputs could
be based on the number of FMU-inputs and the number
of outputs in analogy. As for ME-NeuralFMUs, this is
just one possible setup for a CS-NeuralFMU. Figure 4
shows the topology of the considered NeuralFMU. The
top NN retrieves an input signal (cid:126)unn, which is corrected
into (cid:126)ucs. Note, that here training of the top NN is only
possible if the FMU output is sensitive to the FMU input.
This is often not the case for physical simulations. Inside
the CS-FMU, the input (cid:126)ucs is set, an integrator step with
step size h is performed and the FMU output (cid:126)ycs(t + h)
is forwarded to the bottom NN. Here, a simple error cor-
rection into (cid:126)ynn(t + h) is performed, meaning the error is

Beside ME-FMUs, it is also possible to use CS-FMUs as
part of NeuralFMUs. For CS-FMUs, a numerical solver
like CVODE [7] is already integrated and compiled as part
of the FMU itself.

‚Ä¢ the bottom NN could learn from states (cid:126)xme or
derivatives Àô(cid:126)xme, for a targeted expansion of the
model by additional model equations

(cid:126)xcs(t + h) = (cid:126)fcs((cid:126)xcs, (cid:126)ucs, t, h)
(cid:126)ycs(t + h) = (cid:126)gcs((cid:126)xcs, (cid:126)ucs, t, h)

‚Ä¢ the top NN could additionally generate a FMU

Figure 3: Example for a NeuralFMU (ME).

input (cid:126)ume

(3)
(4)

4

ME-FMUFF-NN·à∂‘¶ùë•ùëöùëíùë¢ùëöùëíùë°‘¶ùë•ùëöùëí·à∂‘¶ùë•ùëõùëõNumericalSolver‡∂±‘¶ùë•ùëõùëõ(ùë°+‚Ñé)FF-NN‘¶ùë•ùëõùëõ‘¶ùë¶ùëöùëíPREPRINT

‚Ä¢ the step size h could be learned by an additional

NN to optimize simulation performance

net = Chain (

myFMU , h , u_cs ) ,

Listing 4: A NeuralFMU (CS) in Julia.

nfmu = CS_NeuralFMU ( net , ...)

The software implementation of the considered CS-
NeuralFMU looks as follows:

Dense ( length ( y_cs ) , ...) ,
... ,
Dense (... , length ( y_nn ) ) )

‚Ä¢ there could be a bypass around the FMU between
both NNs to forward input-dependent signals
from the top NN to the bottom NN

Dense ( length ( u_nn ) , ...) ,
... ,
Dense (... , length ( u_cs ) ) ,
u_cs -> fm i In pu tDo St e pC S O utp u t (

First, the function fmiInputDoStepCSOutput sets all FMU-
inputs to the values u, respectively the output of the pre-
vious net layer. After that, a fmiDoStep with step size h
is performed and Ô¨Ånally the FMU output is retrieved and
passed to the next layer. Because of the integration over
time inside the CS-FMU to retrieve new system states, it is
necessary to reset the FMU for every training run, similar
to the training of recurrent NNs.

P R E P RIN T

High-performance machine learning frameworks like
Flux.jl are using AD for reverse-mode differentiation
through the NN topology. Because all mathematical op-
erations inside the NN are known (white-box), this is a
very efÔ¨Åcient way to derive the gradient and train NNs.
On the other hand, the jacobian over a black-box FMU is
unknown from the view of an AD framework, because the
model structure is (in general) hidden as compiled machine
code. This jacobian is part of the loss gradient AD-chain
and needs to be determined.

Note that for CS, even if the macro step size h must be
determined by the user, it does not need to be constant if
the numerical solver inside the FMU supports varying step
sizes. If so, the internal solver step size may vary from h, in
fact h acts as a upper boundary for the internal micro step
size. As a result, if the FMU is compiled with a variable-
step solver, unnecessarily small values for h will have
negative inÔ¨Çuence on the internal solver performance, but
large values will not destabilize the numerical integration.

Beside others, the most common and default AD frame-
work in Julia is Zygote.jl7. A remarkable feature of Zy-
gote.jl is, that it provides the ability to deÔ¨Åne custom ad-
joints, meaning the gradient of a custom function can be
deÔ¨Åned to be an arbitrary function. This renders the possi-
bility to pass a custom gradient for a FMU-representation
to the AD-framework, which will be later used to derive
the total loss function gradient during the training process.

For the efÔ¨Åcient training of NNs, the gradient of the loss
function according to the net parameters (weights and bi-
ases) is needed. In the following, three methods to derive

determined and compensated without necessarily learning
the mathematical representation of the underlying physical
effect.

Other conÔ¨Ågurations for setting up the hybrid structure are
interesting, e.g.:

Figure 4: Example for integrating a CS-FMU into a NN.

‚Ä¢ the bottom NN could learn from the system state

3 Methodical Background

3.1 Gradient of the Loss Function

(cid:126)xcs(t + h) or state derivative Àô(cid:126)xcs(t + h)

7https://fluxml.ai/Zygote.jl/latest/

5

CS-FMUFF-NN‘¶ùë•ùëêùë†(ùë°+‚Ñé)ùë¢ùëêùë†ùë°‘¶ùë¶ùëõùëõ(ùë°+‚Ñé)FF-NNùë¢ùëõùëõ‘¶ùë¶ùëêùë†(ùë°+‚Ñé)‡∂±‚ÑéPREPRINT

¬∑

¬∑

¬∑

¬∑

¬∑

=

+

=

(9)

(5)

(10)

d
dt

‚àÇ(cid:126)y
‚àÇ(cid:126)u

(cid:126)Jme =

(cid:126)Jf mu =

‚àÇ(cid:126)xnn
‚àÇpi

‚àÇ(cid:126)xnn
‚àÇpi
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:126)gtop_i

‚àÇ(cid:126)xme
‚àÇ(cid:126)xnn
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:126)Jtop

‚àÇ(cid:126)xme
‚àÇ(cid:126)xnn
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:126)Jtop

‚àÇ Àô(cid:126)xnn
‚àÇ(cid:126)xnn
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:126)Jnn

‚àÇ Àô(cid:126)xme
‚àÇ(cid:126)xme
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:126)Jme

‚àÇ Àô(cid:126)xme
‚àÇ(cid:126)xme
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:126)Jme

‚àÇ Àô(cid:126)xnn
‚àÇ Àô(cid:126)xme
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:126)Jbottom

‚àÇ Àô(cid:126)xnn
‚àÇpi
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:126)gbottom_i

3.1.1 Automatic Differentiation (AD)

Inserting Equation 9 into Equation 8 yields:

the loss gradient will be discussed: AD, forward sensitiv-
ity analysis and backward adjoints. In the following, only
ME-NeuralFMUs are considered and the loss function
l((cid:126)xnn((cid:126)p)) is expected to depend explicitly on the system
state (cid:126)xnn (the NeuralFMU output) and only implicitly on
the net parameters (cid:126)p.

The jacobian of the entire NeuralFMU (cid:126)Jnn can be de-
scribed via chain-rule as a product of the three jacobians
(cid:126)Jbottom (over the bottom part of the NN), (cid:126)Jme (over the
ME-FMU) and (cid:126)Jtop (over the top part of the NN):
‚àÇ Àô(cid:126)xnn
‚àÇ Àô(cid:126)xme
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:126)Jbottom

For white-box systems, like native implemented numerical
solvers, one possible approach to provide the gradient is
AD [12]. In general, the mathematical operations inside a
FMU are not known (compiled binary), meaning despite
AD being a very common technique, it is only suitable for
determining the gradient of the NN, but not the jacobian
over a FMU. The unknown jacobian (cid:126)Jf mu over the FMU
layer with layer inputs (cid:126)u and outputs (cid:126)y is noted as follows:

Inserting (cid:126)u = (cid:126)xme and (cid:126)y = Àô(cid:126)xme results in the jacobian
(cid:126)Jme for the FMU from the example in subsection 2.2.1
(ME), inserting (cid:126)u = (cid:126)ucs and (cid:126)y = (cid:126)ycs(t + h) results in the
jacobian matrix (cid:126)Jcs for subsection 2.2.2 (CS):
‚àÇ Àô(cid:126)x
‚àÇ(cid:126)x
‚àÇ(cid:126)y(t + h)
‚àÇ(cid:126)u(t)

Retrieving the jacobian (cid:126)Jme is handled in subsection 3.2,
the jacobians (cid:126)Jbottom and (cid:126)Jtop are determined by AD, be-
cause the NN is modeled as white-box and all mathemati-
cal operations are known. The remaining gradients, (cid:126)gtop_i
and (cid:126)gbottom_i can be determined building an AD-chain,
dependent on the parameter locations inside the NN (top
or bottom part), the jacobian (cid:126)Jme is needed.
As mentioned,
the poor scalability with parameter
count makes forward sensitivities unattractive for ML-
applications with large parameter spaces, but it remains
an interesting option for small NNs. To decide which sen-
sitivity approach to pick for a speciÔ¨Åc NN size, a useful
comparison according to performance of forward and other
sensitivity estimation techniques, dependent on the number
of involved parameters, can be found in [11].

P R E P RIN T

To retrieve the partial derivative (sensitivity) of the sys-
tem state according to a net parameter pi ‚àà (cid:126)p and thus
in straight forward manner also the gradient of the loss
function, another common approach is Forward Sensitivity
Analysis. Sensitivities can be estimated by extending the
system state by additional sensitivity equations in form of
ODEs. Dependent on the number of parameters |(cid:126)p|, this
leads to large ODE systems of size (1 + |(cid:126)p|) ¬∑ |(cid:126)x| [6, p. 21]
and therefore worsens the overall computation and mem-
ory performance. Computations can be reduced, but at a
higher memory cost [12, p. 15]. For a ME-NeuralFMU, the
sensitivity equation for a parameter pi can be formulated
as in Hindmarsh and Serban [6, p. 19]:

The simpliÔ¨Åcation in Equation 7 does not lead to problems
for small step sizes h, because in practice, a small error in
the jacobian only negatively affects the optimization perfor-
mance (convergence speed) and not the convergence itself.
However, the quantity of the mentioned error is dependent
on the the optimization algorithm and parameterization
and h should be selected on the basis of expert knowledge
about the model and optimizer or - if not available - as part
of hyper parameter tuning.

The performance disadvantage of Forward Sensitivity
Analysis motivates the search for a method, that scales
better with a high parameter count. Retrieving the direc-
tional derivatives over a black-box FMU sounds similar
to the reverse-mode differentiation over a black-box nu-
merical solver as in Chen et al. [3]. The name Backward
Adjoints results from solving the ODE adjoint problem
backwards in time:

The jacobian (cid:126)Jnn can be retrieved like in Equation 9. The
searched gradient of the loss function is then given as in
Hindmarsh and Serban [6, p. 22]:

3.1.2 Forward Sensitivities

3.1.3 Backward Adjoints

‚àÇ Àô(cid:126)xnn
‚àÇ(cid:126)xnn
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:126)Jnn

‚àÇ(cid:126)xnn
‚àÇ(cid:126)p
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:126)gtop

dl((cid:126)xnn)
d(cid:126)xnn

dl((cid:126)xnn)
d(cid:126)p

‚àÇ Àô(cid:126)xnn
‚àÇpi

‚àÇ Àô(cid:126)xnn
‚àÇ(cid:126)p

(cid:124) (cid:123)(cid:122) (cid:125)
(cid:126)gbottom

= (cid:126)aT (t0) ¬∑

‚àÇ(cid:126)y(t)
‚àÇ(cid:126)u(t)

‚àÇ(cid:126)xnn
‚àÇpi

‚àÇ(cid:126)xnn
‚àÇpi

= ‚àí(cid:126)aT ¬∑

(cid:126)Jcs =

(t0) +

(cid:126)aT (t)

d(cid:126)a
dt

d
dt

(12)

(11)

(cid:126)a =

(cid:90) t1

(8)

(6)

(7)

(t)

dt

+

‚âà

=

t0

¬∑

‚àÇ Àô(cid:126)xnn
‚àÇ(cid:126)xnn
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:126)Jnn

(13)

6

PREPRINT

3.2

4 Example

Jacobian of the FMU

3.2.1 Finite Differences

Listing 5: Training NeuralFMUs in Julia.

nfmu = NeuralFMU ( net , ...)
p_net = Flux . params ( nfmu )
Flux . train !(... , p_net , ...)

Finally, we provide a seamless link to the ML-library
Flux.jl, meaning NeuralFMUs can be trained the same
way as a convenient NN in Julia:

As a Ô¨Ånal note, the presented methodical procedure, inte-
grating FMUs into the Julia machine learning environment,
can be transferred to other AD-frameworks in other pro-
gramming languages like Python.

Independent of the chosen method, the jacobian over the
FMU (cid:126)Jme is needed to keep the NeuralFMU trainable, but
is unknown and must be determined. In the following,
we suggest two possibilities to retrieve the gradient over
a FMU: Finite Differences and the use of the built-in
function fmi2GetDirectionalDerivative.

Here (cid:126)gtop and (cid:126)gbottom can be determined again using AD
and (cid:126)Jme. To conclude, the backward adjoint ODE system
with dimension |(cid:126)x| has to be solved only once independent
of the number of parameters and therefore requires less
computations for large parameter spaces compared to for-
ward sensitivities. On the other hand, backward adjoints
are only suitable, if the loss function gradient is smooth and
bounded [6, p. 22], which limits the possible use for this
technique to continuous systems and therefore to almost
only research applications.

When modeling physical systems, it‚Äôs often not practical
to model solely based on Ô¨Årst principle and parameter-
ize every physical aspect. For example, when modeling
mechanical, electrical or hydraulic systems, a typical mod-
eling assumption is the negligence of friction or the use
of greatly simpliÔ¨Åed friction models. Even when using
friction models, the parameterization of these is a difÔ¨Åcult
and error prone task. Therefore, we decided to show the
beneÔ¨Åts of the presented hybrid modeling technique on an
easy to understand example from this set of problems.

The jacobian can be derived by selective input modiÔ¨Åca-
tion, sampling of additional trajectory points and estimat-
ing the partial derivatives via Ô¨Ånite differences. Note, that
this approach is an option for ME-FMUs, for CS-FMUs
only if the optional functions to store and set previous
FMU states, fmi2GetState and fmi2SetState, are avail-
able. Otherwise, sampling would require to setup a new
simulation for every FMU layer input and every consid-
ered time step, if the system state vector is unknown. This
would be an unacceptable effort for most industrial appli-
cations with large models.

P R E P RIN T

The preferred approach in this paper is different and ben-
eÔ¨Åts from a major advantage of the FMI-standard: Fully
implemented FMUs provide the partial derivatives between
any variable pair, thus the partial derivative between the
systems states and derivatives (ME) or the FMU inputs and
its outputs (CS) is known at any simulation time step and
does not need to be estimated by additional methods. In
FMI 2.0.2, the partial derivatives can be retrieved by call-
ing the optional function fmi2GetDirectionalDerivative
[9, p. 26]. Depending on the underlying implementation
of this function, which can vary between exporting tools,
this can be a fast and reliable way to gather directional
derivatives in fully implemented FMUs.

The parameter s0 describes the absolute position of the
Ô¨Åxed anchor point, allowing to model a system displace-
ment or a constant position measurement offset. Further,
the friction force ff ric between the pendulum body and
the underlying ground is implemented with the non-linear,
discontinuous friction model from MassWithStopAndFric-
tion8 as part of the Modelica Standard Library (MSL). The
friction term for positive v denotes:

To conclude, the key step is to forward the directional
derivatives over the FMU to the AD-framework Zy-
gote.jl. As mentioned, Zygote.jl provides a feature
In
to deÔ¨Åne a custom gradient over any function.
this case, the gradients for the functions fmiDoStepME
and fmiInputDoStepCSOutput are wrapped to calls to
fmi2GetDirectionalDerivative.

As in Figure 5, the reference system is modeled as one
mass oscillator (horizontal spring-pendulum) with mass m,
spring constant c and relaxed spring length srel, deÔ¨Åned
by the differential equation:

c ¬∑ (s0 + srel ‚àí s) ‚àí ff ric(v)
m

ff ric(v) = fcoulomb + fprop ¬∑ v + fstribeck ¬∑ e‚àífexp¬∑v

8Modelica.Mechanics.Translational.Components.

Figure 5: The reference system in Modelica.

3.2.2 Built-in Directional Derivatives

¬®s = Àôv = a =

4.1 Model

(14)

(15)

MassWithStopAndFriction (MSL 3.2.3)

7

fixedspringc=10 N/mmassm=1 kgPREPRINT

Unit

Value

Parameter

4.3 Results

4.3.1 Training

0.05
0.25
0.5
2.0
1.0
10.0
1.0
0.0

Value
ref. model FMU model

fprop
fcoulomb
fstribeck
fexp
mass.m
spring.c
spring.srel
f ixed.s0

0.0 N ¬∑s/m
N
0.0
N
0.0
s/m
0.0
kg
1.0
N/m
10.0
m
1.0
m
0.1

The corresponding code is available online as part of the
library repository9.

Table 1: Parameterization of the reference and FMU
model.

This friction model consists of parameters for the con-
stant Coulomb-friction fcoulomb, fprop for the velocity-
proportional (viscous) friction and fstribeck for the expo-
nential Stribeck-friction. The FMU (white box) model on
the other hand, only covers the modeling of a continuous,
frictionless spring pendulum, therefore with ff ric(v) = 0.

The aim here is to learn a generalized representation of
the parameterized friction-model in Equation 15 from mea-
surements of the pendulum‚Äôs movement over time. Further,
a displacement of s0 = 0.1 m is added to the FMU model
(modeling failure), which should be detected and compen-
sated. Both systems are parameterized as in Table 1.

After a short training10 of 2500 runs on 400 data points
(each position and velocity), the hybrid model is able to
imitate the reference system on training data, as can be
seen in Fig. 6 for position and 7 for velocity. The training
has not converged yet, further training will lead to a im-
proved Ô¨Åt. For the training case, the system was initialized
with mass.s0 = 0.5 m (the pendulum equilibrium is at
1.0 m) and mass.v0 = 0 m/s. Please keep in mind that the
NeuralFMU was only trained by data gathered from one
single simulation scenario.

P R E P RIN T

We will show, that with a NeuralFMU-structure as in Fig-
ure 3, it is possible to learn a simpliÔ¨Åed friction model
as well as the constant system displacement (modeling
failure) with a simple fully-connected feed-forward NN as
in Table 2. The network topology results from a sim-
ple random search hyper parameter optimization for a
NeuralFMU model with a maximum of 150 net parameters
and 8 layers. All weights are initialized with standard-
normal distributed random values and all biases with zeros,
except the weights of layer #1 are initialized as identity
matrix, to start training with a neutral setup and keep the
system closer to the preferred intuitive solution. The loss
function is deÔ¨Åned as simple mean squared error between
equidistant sample points of the NeuralFMU and the refer-
ence system.

The bottom part of the NN learned the physical effect dis-
continuous friction in a generalized way, because the net
was trained based on the state derivatives instead of the
states themselves. A comparison of the friction model
of the reference system, the FMU and the learned fric-
tion model, extracted from the bottom part NN of the
NeuralFMU, is shown in Figure 10. The learned friction
model is a simpliÔ¨Åcation of the reference friction model,

Even if the deviation between NeuralFMU and reference
system is larger for testing then for training data, the hy-
brid model performs well on the test case with a different
(untrained) initial system state (Figure 8 and 9). For test-
ing, the system is initialized with mass.s0 = 1.0 m and
mass.v0 = ‚àí1.5 m/s.

Figure 6: Comparison of the mass position of the FMU,
reference system and the NeuralFMU after 2500 and 5000
training steps on training data.

9https://github.com/ThummeTo/FMIFlux.jl/blob/

Table 2: Topology of the example NeuralFMU.

main/example/modelica_conference_2021.jl

Inputs Outputs Activation

4.2 NeuralFMU Setup

10Training was performed single-core on a desktop CPU (In-
tel¬Æ CoreTM i7-8565U) and took about 22.5 minutes. GPU train-
ing is under development.

#1 (input)
#2 (FMU)
#3 (hidden)
#4 (hidden)
#5 (output)

identity
none
identity
tanh
identity

4.3.2 Testing

2
2
8
8
2

2
2
2
8
8

Layer

8

PREPRINT

because of the small net layout and a lack of data at the
discontinuity near v = 0. Finally, also the displacement
modeling failure of the white-box model (FMU) was can-
celed out by the small top NN as can be seen in Figure 11.

Figure 7: Comparison of the mass velocity of the FMU,
reference system and the NeuralFMU after 2500 and 5000
training steps on training data.

Figure 10: Comparison of the friction models of the FMU,
reference system and the NeuralFMU (bottom part NN)
after 2500 and 5000 training steps on testing data.

P R E P RIN T

The presented open source library FMI.jl (https://
github.com/ThummeTo/FMI.jl) allows the easy and seam-
less integration of FMI-models into the Julia programming
language. FMUs can be loaded, parameterized and sim-
ulated using the abilities of the FMI-standard. Optional
functions like retrieving the partial derivatives or manip-
ulating the FMU state are available if supported by the
FMU. The library release version 0.1.4 is compatible with
FMI 2.0.x (the common version at the time of release),
supporting upcoming standard updates like FMI 3.0 is

Figure 11: Comparison of the displacements of the FMU,
reference system and the NeuralFMU (top part NN) after
2500 and 5000 training steps on testing data.

Figure 8: Comparison of the mass position of the FMU,
reference system and the NeuralFMU after 2500 and 5000
training steps on testing data.

Figure 9: Comparison of the mass velocity of the FMU,
reference system and the NeuralFMU after 2500 and 5000
training steps on testing data.

5 Conclusion

9

PREPRINT

[2]

planned. The library currently supports ME- as well as CS-
FMUs, running on Windows and Linux operation systems.
Event-handling to simulate discontinuous ME-FMUs is
supported.

abs/1209.5145 (2012). arXiv: 1209 . 5145. URL:
http://arxiv.org/abs/1209.5145.
Jeff Bezanson et al. ‚ÄúJulia: A Fresh Approach to
Numerical Computing‚Äù. In: CoRR abs/1411.1607
(2015). arXiv: 1411.1607. URL: http://arxiv.
org/abs/1411.1607.

[3] Tian Qi Chen et al. ‚ÄúNeural Ordinary Differen-
tial Equations‚Äù. In: CoRR abs/1806.07366 (2018).
arXiv: 1806.07366. URL: http://arxiv.org/
abs/1806.07366.

[4] Hilding Elmqvist, Andrea Neumayr, and Martin
Otter. ‚ÄúModia - Dynamic Modeling and Simulation
with Julia‚Äù. In: Juliacon 2018. 2018. URL: https:
//elib.dlr.de/124133/.

FMUs contain the model as compiled binary, therefore
FMU related computations must be performed on the CPU.
On the other hand, deploying NNs on optimized hardware
like GPUs often results in a better training performance.
Currently, the training of the NeuralFMU is completely
done on the CPU. A hybrid hardware training loop with the
FMU on the CPU and NN on the GPU may lead to perfor-
mance improvements for wider and deeper NN-topologies.

Current and future work covers the implementation of a
more general custom adjoint, meaning despite Zygote.jl,
other AD-frameworks will be supported. Further, we are
working on different fall-backs if the optional function
fmi2GetDirectionalDerivatives is not available. The
Ô¨Ånite differences approach for ME-FMUs is already im-
plemented, sampling via fmi2GetState and fmi2SetState
for CS-FMUs will be supported soon.

The library extension FMIFlux.jl (https://github.com/
ThummeTo/FMIFlux.jl) makes FMUs differentiable and
opens the possibility to setup and train NeuralFMUs, the
structural combination of a FMU, NN and a numerical
solver. Proper event-handling during back-propagation
whilst training of NeuralFMUs is under development, even
if there were no problems during training with the dis-
continuous model from the paper example. A cumulative
publication is planned, focusing on a real industrial use-
case instead of a methodical presentation.

[5] Manuel Haussmann et al. ‚ÄúLearning Partially
Known Stochastic Dynamics with Empirical PAC
Bayes‚Äù. In: (2021). arXiv: 2006.09914 [cs.LG].
[6] Alan C. Hindmarsh and Radu Serban. User
v2.5.0. Tech.
for CVODES
Documentation
rep. Nov. 2006. URL: https : / / www .
researchgate . net / profile / Radu -
Serban / publication / 239581887 _ User _
Documentation _ for _ CVODES _ v210 /
links / 00b7d534f0be2a496f000000 / User -
Documentation-for-CVODES-v210.pdf.
[7] Alan C. Hindmarsh et al. User Documentation for
CVODE v5.7.0 (sundials v5.7.0). Tech. rep. Feb.
2021. URL: https : / / computing . llnl . gov /
sites/default/files/cv_guide-5.7.0.pdf.
[8] Mike Innes et al. ‚ÄúA Differentiable Programming
System to Bridge Machine Learning and Scien-
tiÔ¨Åc Computing‚Äù. In: CoRR abs/1907.07587 (2019).
arXiv: 1907.07587. URL: http://arxiv.org/
abs/1907.07587.

P R E P RIN T

[10] Modelica Association. System Structure and
1.0.
Parameterization. Document
Tech.
rep. Link√∂ping: Modelica Associa-
tion, Mar. 2019. URL: https : / / ssp -
standard . org / publications / SSP10 /
SystemStructureAndParameterization10
.
pdf.

Beside NeuralFMUs, FMIFlux.jl paves the way for other
hybrid modeling techniques and new industrial use-cases
by making FMUs differentiable in an AD-framework. The
authors are excited about any assistance they can get to ex-
tend the library repositories by new features and maintain
them for the upcoming technology progress. Contributors
are welcome.

[9] Modelica Association. Functional Mock-up Inter-
faceforModel Exchange and Co-Simulation. Docu-
ment version: 2.0.2. Tech. rep. Link√∂ping: Modelica
Association, Dec. 2020. URL: https://github.
com / modelica / fmi - standard / releases /
download / v2 . 0 . 2 / FMI - Specification - 2 .
0.2.pdf.

The authors like to thank Andreas Heuermann for his hints
and nice feedback during the development of the library.
Further, we thank Florian Schl√§ffer for designing the beau-
tiful library logos. This work has been partly supported
by the ITEA 3 cluster programme for the project UPSIM -
Unleash Potentials in Simulation.

[11] Christopher Rackauckas et al. ‚ÄúA Comparison of
Automatic Differentiation and Continuous Sensitiv-
ity Analysis for Derivatives of Differential Equa-
tion Solutions‚Äù. In: (2018). arXiv: 1812 . 01892
[cs.NA].

An extension of the library to the CS-standard SSP [10],
including the necessary machine learning back-end, is near
completion. This will allow the integration of complete
CSs into a NN topology and retrieve a NeuralSSP.

[12] Christopher Rackauckas et al. ‚ÄúDiffEqFlux.jl - A
Julia Library for Neural Differential Equations‚Äù. In:
CoRR abs/1902.02376 (2019). arXiv: 1902.02376.
URL: http://arxiv.org/abs/1902.02376.

Jeff Bezanson et al. ‚ÄúJulia: A Fast Dynamic
Language for Technical Computing‚Äù. In: CoRR

Acknowledgments

References

version:

[1]

10

PREPRINT

[17]

[13] Christopher Rackauckas et al. SciML/DifferentialE-
quations.jl: v6.17.2. Version v6.17.2. July 2021.
DOI: 10.5281/zenodo.5069045. URL: https:
//doi.org/10.5281/zenodo.5069045.
[14] Rahul Rai and Chandan K. Sahu. ‚ÄúDriven by Data
or Derived Through Physics? A Review of Hybrid
Physics Guided Machine Learning Techniques With
Cyber-Physical System (CPS) Focus‚Äù. In: IEEE Ac-
cess 8 (2020), pp. 71050‚Äì71073. DOI: 10.1109/
ACCESS.2020.2987324.

[15] M. Raissi, P. Perdikaris, and G.E. Karniadakis.
‚ÄúPhysics-informed neural networks: A deep learning
framework for solving forward and inverse prob-
lems involving nonlinear partial differential equa-
tions‚Äù. In: Journal of Computational Physics 378
(2019), pp. 686‚Äì707. ISSN: 0021-9991. DOI: https:
/ / doi . org / 10 . 1016 / j . jcp . 2018 . 10 . 045.
URL: https : / / www . sciencedirect . com /
science/article/pii/S0021999118307125.

[16] Ch. Tsitouras. ‚ÄúRunge‚ÄìKutta pairs of order 5(4)
satisfying only the Ô¨Årst column simplifying as-
sumption‚Äù. In: Computers & Mathematics with Ap-
plications 62.2 (2011), pp. 770‚Äì775. ISSN: 0898-
1221. DOI: https : / / doi . org / 10 . 1016 / j .
camwa . 2011 . 06 . 002. URL: https : / / www .
sciencedirect.com/science/article/pii/
S0898122111004706.
Jared Willard et al. ‚ÄúIntegrating Physics-Based Mod-
eling with Machine Learning: A Survey‚Äù. In: (2020).
arXiv: 2003.04919 [physics.comp-ph].

P R E P RIN T

11

