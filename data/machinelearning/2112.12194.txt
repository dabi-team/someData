Surrogate Likelihoods for Variational Annealed Importance Sampling

Martin Jankowiak 1 Du Phan 2

2
2
0
2

l
u
J

2
1

]
L
M

.
t
a
t
s
[

2
v
4
9
1
2
1
.
2
1
1
2
:
v
i
X
r
a

Abstract

Variational inference is a powerful paradigm for
approximate Bayesian inference with a number
of appealing properties, including support for
model learning and data subsampling. By contrast
MCMC methods like Hamiltonian Monte Carlo
do not share these properties but remain attractive
since, contrary to parametric methods, MCMC
is asymptotically unbiased. For these reasons re-
searchers have sought to combine the strengths of
both classes of algorithms, with recent approaches
coming closer to realizing this vision in practice.
However, supporting data subsampling in these
hybrid methods can be a challenge, a shortcoming
that we address by introducing a surrogate likeli-
hood that can be learned jointly with other vari-
ational parameters. We argue theoretically that
the resulting algorithm allows an intuitive trade-
off between inference ﬁdelity and computational
cost. In an extensive empirical comparison we
show that our method performs well in practice
and that it is well-suited for black-box inference
in probabilistic programming frameworks.

1. Introduction

Bayesian modeling and inference is a powerful approach
to making sense of complex datasets. This is especially
the case in scientiﬁc applications, where accounting for
prior knowledge and uncertainty is essential. For motivat-
ing examples we need only look to recent efforts to study
COVID-19, including e.g. epidemiological models that in-
corporate mobility data (Miller et al., 2020; Monod et al.,
2021) or link differentiable transmissibility of SARS-CoV-2
lineages to viral mutations (Obermeyer et al., 2022).

Unfortunately for many application areas the broader use
of Bayesian models is hampered by the difﬁculty of for-

1Broad Institute, Cambridge, MA, USA 2Google Research,
Cambridge, MA, USA. Correspondence to: Martin Jankowiak
<mjankowi@broadinstitute.org>.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

mulating scalable inference algorithms. One regime that
has proven particularly challenging is the large data regime.
This is essentially because powerful MCMC methods like
Hamiltonian Monte Carlo (HMC) (Duane et al., 1987; Neal
et al., 2011) are, at least on the face of it, incompatible with
data subsampling (i.e. mini-batching) (Betancourt, 2015).
While considerable effort has gone into developing MCMC
methods that accommodate data subsampling,1 developing
generic MCMC methods that yield high ﬁdelity posterior
approximations while scaling to very large datasets remains
challenging.

For this reason, among others, recent years have seen exten-
sive development of approximate inference methods based
on variational inference (Jordan et al., 1999; Blei et al.,
2017). Besides ‘automatic’ support for data subsampling
(Hoffman et al., 2013; Ranganath et al., 2014), at least for
suitable model classes, variational inference has several ad-
ditional favorable properties, including support for amortiza-
tion (Dayan et al., 1995), log evidence estimates, and model
learning, motivating researchers to combine the strengths
of MCMC and variational inference (Salimans et al., 2015;
Hoffman, 2017; Caterini et al., 2018; Ruiz & Titsias, 2019).

Recent work from Geffner & Domke (2021) and Zhang et al.
(2021) offers a particularly elegant formulation of a varia-
tional inference method that leverages (unadjusted) HMC as
well as annealed importance sampling (AIS) (Neal, 2001).
Although these are fundamentally variational methods, since
they make use of HMC, which does not itself readily ac-
commodate data subsampling, these methods likewise do
not automatically inherit support for data subsampling. This
is unfortunate because these methods do automatically in-
herit many of the other nice features of variational inference,
including support for amortization and model learning.

In this work we set out to extend the approach in Geffner &
Domke (2021) and Zhang et al. (2021) to support data sub-
sampling, thus making it applicable to the large data regime.
Our basic strategy is simple and revolves around introduc-
ing a surrogate log likelihood that is cheap to evaluate. The
surrogate log likelihood is used to guide HMC dynamics, re-
sulting in a ﬂexible variational distribution that is implicitly
deﬁned via the gradient of the surrogate log likelihood. The

1See e.g. Dang et al. (2019) and Zhang et al. (2020) for recent

work.

 
 
 
 
 
 
Surrogate Likelihoods for Variational Annealed Importance Sampling

corresponding variational objective accommodates unbiased
mini-batch estimates, at least for the large class of models
with appropriate conditional independence structure that we
consider. As we show in experiments in Sec. 7, our method
performs well in practice and is well-suited for black-box
inference in probabilistic programming frameworks.

2. Problem setting

We are given a dataset

and a model of the form

D
, z) = pθ(z)

pθ(

D

N
(cid:89)

n=1

pθ(yn

z, xn)
|

(1)

RD is governed by a prior
where the latent variable z
∈
pθ(z) and the likelihood factorizes into N terms, one for
each data point (yn, xn)
, and where θ denotes any
∈ D
additional (non-random) parameters in the model. Eqn. 1
encompasses a large and important class of models and
includes e.g. a wide variety of multi-level regression models.
The log density corresponding to Eqn. 1 is given by

log pθ(

D

, z) = log pθ(z) + Σn log pθ(yn

z, xn)

|

(2)

Ψ0(z) + ΨL(

, z)

D

≡

where we deﬁne the log prior Ψ0(z) and log likelihood
ΨL(

, z).2

D

D

We are interested in the regime where N is large or individ-
ual likelihood terms are costly to evaluate so that computing
the full log likelihood ΨL(
, z) and its gradients is imprac-
tical. We aim to devise a ﬂexible variational approximation
) that can be ﬁt with an algorithm
to the posterior pθ(z
that supports data subsampling. Additionally we would like
our method to be generic in nature so that it can readily be
incorporated into a probabilistic programming framework
as a black-box inference algorithm. Finally we would like a
method that supports model learning, i.e. one that allows us
to learn θ in conjunction with the approximate posterior.

|D

For simplicity in this work we primarily focus on global
latent variable models with the structure in Eqn. 1. The
approach we describe can also be extended to models with
local latent variables, i.e. those local to each data point. For
more discussion see Sec. 7.6 and Sec. A.1 in the appendix.

3. Background

Before describing our method in Sec. 4, we ﬁrst review
relevant background.

3.1. Variational inference

The simplest variants of variational inference introduce a
parametric variational distribution qφ(z) and proceed to

2We

also

use

the

notation ΨL(DI, z)

≡

Σn∈I log pθ(yn|z, xn) for a set of indices I.

choose the parameters φ to minimize the Kullback-Leibler
(KL) divergence between the variational distribution and
the posterior pθ(z
)). This can be
), i.e. KL(qφ(z)
done by maximizing the Evidence Lower Bound or ELBO

pθ(z
|

|D

|D

ELBO

≡

Eqφ(z) [log pθ(

, z)

D

−

log qφ(z)]

(3)

D

≤

which satisﬁes ELBO
). Thanks to this inequal-
log pθ(
ity the ELBO naturally enables joint model learning and
inference, i.e. we can maximize Eqn. 3 w.r.t. both model
parameters θ and variational parameters φ simultaneously.
A potential shortcoming of the fully parametric approach de-
scribed here is that it can be difﬁcult to specify appropriate
parameterizations for qφ(z).

3.2. Annealed Importance Sampling

Annealed importance sampling (AIS) (Neal, 2001) is a
) that leverages a
method for estimating the evidence pθ(
D
K
sequence of K bridging densities
fk(z)
k=1 that connect
}
a simple base distribution q0(z) to the posterior. In more
detail, AIS can be understood as importance sampling on an
extended space. That is we can write

{

(cid:90)

) =

pθ(

D

dz pθ(

D

, z) = Eqfwd(z0:K )

(cid:20) qbwd(z0:K)
qfwd(z0:K)

(cid:21)

(4)

where the proposal distribution qfwd and the (un-normalized)
target distribution qbwd are given by

1(z1
qfwd(z0:K) = q0(z0)
T
, zK) ˜
T

qbwd(z0:K) = pθ(

D

z0)

|
· · · T
K(zK−1

K(zK

zK)
|

· · ·

zK−1)
|
˜
1(z0
T

z1)

|

T

Here each
k is a MCMC kernel that leaves the bridging
density fk(z) invariant. While there is considerable free-
dom in the choice of q0(z) and
, it is natural to let
}
, z)βk where
q0(z) = pθ(z) and fk(z)
are inverse temperatures that satisfy 0 < β1 < β2 <
βk
{
... < βK = 1. Additionally

fk(z)
q0(z)1−βk pθ(

∝

D

{

}

(5)

k(zk

˜
k(zk−1
T

zk) =
|

zk−1)fk(zk−1)/fk(zk)
|

k

T
k. Concep-
is the reverse MCMC kernel corresponding to
move samples from q0(z) towards
tually, the kernels
the posterior via a sequence of moves, each of which is
βk
and sufﬁciently large
‘small’ for appropriately spaced
{
K. Indeed AIS is consistent as K
(Neal, 2001) and
has been shown to achieve accurate estimates of log pθ(
)
empirically (Grosse et al., 2015).

}
→ ∞

{T

D

T

}

3.3. Hamiltonian Monte Carlo

Hamiltonian Monte Carlo (HMC) is a powerful gradient-
based MCMC method (Duane et al., 1987; Neal et al.,
2011). HMC proceeds by introducing an auxiliary mo-
RD and a log joint (un-normalized) density
mentum v

∈

Surrogate Likelihoods for Variational Annealed Importance Sampling

−

H(z, v) where the Hamiltonian is deﬁned
log π(z, v) =
, z) is the
log pθ(
as H(z, v) = T (v) + V (z), V (z) =
potential energy, T (v) = 1
2 v(cid:62)M−1v is the kinetic energy,
and M is the mass matrix. Hamiltonian dynamics is then
deﬁned as:

−

D

dz
dt

=

∂H
∂v

dv
dt

=

∂H
∂z

−

(6)

Evolving trajectories according to Eqn. 6 for any time τ
yields Markovian transitions that target the stationary distri-
bution π(z, v). By alternating Hamiltonian evolution with
momentum updates

vt+1

(γvt, (1

−

∼ N

γ2)M)

(7)

|D

). Here γ

and disregarding the momentum yields a MCMC chain tar-
[0, 1) controls the
geting the posterior pθ(z
level of momentum refreshment; e.g. the γ
1 regime sup-
presses random-walk behavior (Horowitz, 1991). In general
we cannot simulate Hamiltonian trajectories exactly and in-
stead use a symplectic integrator like the so-called ‘leapfrog’
integrator. Combined with momentum refreshment a single
HMC step (zk−1, vk−1)

(zk, vk) is given by

≈

∈

→
2 M−1vk−1

zk−1 + η
ˆzk + η

2 M−1 ˆvk

ˆzk
zk

←

←

ˆvk
vk

←

∼ N

vk−1

η

V (ˆzk)

(8)

−
∇
(γ ˆvk, (1

γ2)M)

−

DAIS restores differentiability by removing the accept/reject
step, which makes it straightforward to optimize the vari-
ational bound using gradient-based methods. While drop-
ping the Metropolis-Hastings correction invalidates detailed
balance, AIS and thus the resulting variational bound re-
main intact. Moreover, since DAIS employs a HMC kernel
and since we expect HMC moves to have high acceptance
probabilities—at least for a well-chosen step size η and mass
matrix M—we expect the K DAIS transitions to efﬁciently
move samples from q0(z) towards the posterior.

In more detail DAIS operates on an extended space
(z0, ..., zK, v0, ..., vK), with proposal and target distribu-
tions given by

qfwd(z0:K, v0:K) = q0(z0)q0(v0)

(cid:81)K

k=1T
qbwd(z0:K, v0:K) = pθ(
×
D
˜
k(zk−1, vk−1
T

(cid:81)K

k=1

k(zk, vk
, zK)

|

×
zk−1, vk−1)

zk, vk)

|

(9)

(10)

where q0(v0) =
N
Here each kernel
T
Eqn. 8 using the annealed potential energy

(v0
0, M) is the momentum distribution.
k performs a single leapfrog step as in

|

Vk(z) =
=

(1

(1

−

−

−

−

βk) log q0(z)
βk) log q0(z)

−

−

βk log pθ(
βk (Ψ0(z) + ΨL(

, z)

D

(11)
, z))

D

where η is the step size. Since the leapfrog integrator is
inexact, a Metropolis-Hastings accept/reject step is used
to ensure asymptotic correctness. For a comprehensive
introduction to HMC see e.g. (Betancourt, 2017).

and without including a Metropolis-Hastings correction.
Notably, Geffner & Domke (2021) and Zhang et al. (2021)
show that the DAIS variational objective is easy to compute,
as clariﬁed by the following lemma.

3.4. Differentiable Annealed Importance Sampling
a.k.a. Uncorrected Hamiltonian Annealing

Lemma 1 The DAIS bound given by proposal and target
distributions as in Eqn. 9 is differentiable and is given by

We describe recent work, (UHA; Geffner & Domke (2021))
and (DAIS; Zhang et al. (2021)), that combines HMC and
AIS in a variational inference framework. For simplicity we
refer to the authors’ algorithm as DAIS and disregard any
differences between the two (contemporaneous) references.3

DAIS

L

(cid:104)
E
≡
(cid:104)
= E

log qbwd(z0:K, v0:K)

(cid:105)
log qfwd(z0:K, v0:K)

−
log q0(z0)+

log pθ(

, zK)

(cid:80)K

D
log

−
(ˆvk, M)

k=1 {

N

log

(vk−1, M)
}

N

−

(cid:105)

|

(z1, v1

A notable feature of HMC is that

the HMC kernel
z0, v0) can generate large moves in z-space with
T
large acceptance probabilities, which makes it an attrac-
tive kernel choice for AIS (Sohl-Dickstein & Culpepper,
2012). Moreover, as an importance sampling framework
AIS naturally gives rise to a variational bound, since ap-
plying Jensen’s inequality to the log of Eqn. 4 immediately
). Geffner & Domke (2021)
yields a lower bound to log p(
D
and Zhang et al. (2021) note that the utility of such an
AIS variational bound is severely hampered by the fact
that typical MCMC kernels include a Metropolis-Hastings
accept/reject step that makes the bound non-differentiable.

L

where the expectation is w.r.t. qfwd. Moreover, gradient
estimates of
DAIS w.r.t. θ and φ can be computed using
reparameterized gradients provided the base distribution
q0(z) is reparameterizable. For a proof see Sec. A.3 in the
supplemental materials, Geffner & Domke (2021) or Zhang
et al. (2021).

Note that
instead it sufﬁces to compute kinetic energy differences.

k do not appear explicitly in Lemma 1;

T

k and ˜
T

4. Surrogate Likelihood DAIS

3A closely related approach that utilizes unadjusted over-

damped Langevin steps is described in (Thin et al., 2021).

The DAIS variational objective in Lemma 1 can lead to tight
), especially
bounds on the model log evidence log pθ(

D

Surrogate Likelihoods for Variational Annealed Importance Sampling

L

for large K. However, for a model like that in Eqn. 1,
DAIS can be prohibitively expensive, with a
optimizing
(N K) cost per optimization step for a dataset with N data
O
points. This is because sampling qfwd requires computing
, z), K
the gradient of the full log likelihood, i.e.
times, which is expensive whenever N is large or individual
likelihood terms are costly to compute.

ΨL(

∇

D

|D

Vk(z)

In order to speed-up training and sampling in this regime
we ﬁrst observe that the proof of Lemma 1 holds for any
.4 The annealed ansatz
choice of potential energies
}
{
in Eqn. 11 is a natural choice, but any other choice that
efﬁciently moves samples from q0(z) towards the posterior
pθ(z
) can lead to tight variational bounds. This observa-
tion naturally leads to two scalable variational bounds that
are tractable in the large data regime. In the ﬁrst method,
a variant of which was considered theoretically in Zhang
et al. (2021) and which we refer to as NS-DAIS, we replace
Vk(z) with a stochastic estimate that depends on a mini-
batch of data. In the second method, which we refer to as
SL-DAIS, we replace the full log likelihood ΨL(
, z) with
a surrogate log likelihood ˆΨL(z) that is cheap to evaluate.
Below we argue theoretically and demonstrate empirically
that SL-DAIS is superior to NS-DAIS.

D

4.1. NS-DAIS: Naive Subsampling DAIS

J ⊂ {
D

1, ..., N
with
}
, z) as N
B ΨL(

=
For any mini-batch of indices
|J |
B we can deﬁne an estimator of ΨL(
J , z).
D
In NS-DAIS we plug this estimator into the potential en-
ergy in Eqn. 11. With this choice of Vk(z) sampling from
(N ). Moreover, by replacing the
qfwd is
O
log pθ(
DAIS with an estimator computed
using an independent mini-batch of indices
with
instead of
distributions qfwd and qbwd in Eqn. 9 are replaced with

}
(B)
(N ). More formally, the proposal and target

= B we obtain an unbiased estimator that is

(B) instead of
, zK) term in

1, ..., N

I ⊂ {

O
D

|I|

O

O

L

qfwd

qbwd

→

→

q0(z0)q0(v0)(cid:81)K
pθ(zK

k(zk, vk
k=1T
I)N/Bpθ(zK)(cid:81)K

k=1

|D

)

|

zk−1, vk−1,
J
˜
k(zk−1, vk−1
T

)
|·

where we omit the common factor of q(
) for brevity.
See Algorithm 2 and Sec. A.5 in the supplement for details.

)q(

J

I

4.2. SL-DAIS: Surrogate Likelihood DAIS

We proceed as in NS-DAIS except we plug in a ﬁxed non-
stochastic surrogate log likelihood ˆΨL(z) into the potential
energy in Eqn. 11. More formally, the proposal and target
distributions qfwd and qbwd in Eqn. 9 are replaced with

qfwd

qbwd

→

→

q(

I
q(

I

)q0(z0)q0(v0)(cid:81)K
)pθ(

k(zk, vk
k=1T
zK)N/Bpθ(zK)(cid:81)K
|

I
D

k=1

|

zk−1, vk−1, ˆΨL)
˜
)
k(zk−1, vk−1
|·
T

4See Sec. A.3 in the supplement for details.

Algorithm 1 SL-DAIS: Surrogate Likelihood Differentiable
Annealed Importance Sampling. We highlight in blue
where the algorithm differs from DAIS. To recover DAIS
we substitute ˆΨL(z)
N . Note that ˆΨL
is only used to guide HMC dynamics and that a stochastic
estimate of ΨL(

, z) still appears on the ﬁnal line.

, z) and B

ΨL(

→

→

D

D

D

Input: model log density Ψ0(z) + ΨL(
, z), surrogate
log likelihood ˆΨL(z), base variational distribution q0(z),
number of steps K, inverse temperatures
, step
}
size η, momentum refresh parameter γ, mass matrix M,
dataset
Initialize: z0
∼
∼ N
for k = 1 to K do
zk−1 + η
2 M−1vk−1
(cid:16)
(cid:110)
Ψ0(z)+ ˆΨL(z)
βk

of size N , mini-batch size B

βk) log q0(z)

log q0(z0)

(0, M),

L ← −

βk
{

q0, v0

+(1

gk

ˆzk

←

D

(cid:17)

z
← ∇

−

(cid:111)(cid:12)
(cid:12)
(cid:12)
(cid:12)z=ˆzk

ˆvk
vk−1 + ηgk
ˆzk + η
zk
if k < K then

←
←

2 M−1 ˆvk

γ ˆvk + (cid:112)1

vk
end if

←

γ2ε, ε

−

∼ N

(0, M)

+ log

(ˆvk, M)

N

L ← L

end for
Sample mini-batch indices
+ Ψ0(zK) + N

Return:

L

log

N

(vk−1, M)

−

1, ..., N

}
I, zK)

I ⊂ {
B ΨL(

D

with

|I|

= B

I

I
D

where q(
) encodes sampling mini-batches of B indices
without replacement, and data subsampling occurs solely in
the pθ(
zK) term. Like NS-DAIS the SL-DAIS ELBO
|
admits a simple unbiased gradient estimator. In contrast to
NS-DAIS, the HMC dynamics in SL-DAIS targets a ﬁxed
target distribution. We expect the reduced stochasticity of
SL-DAIS as compared to NS-DAIS to result in superior
empirical performance.5 See Algorithm 1 for the complete
algorithm and Sec. A.6 in the supplement for a more com-
plete formal description.

There are multiple possibilities for how to parameterize
ˆΨL(z). We brieﬂy describe the simplest possible recipe,
which we refer to as RAND, leaving a detailed ablation study
to Sec. 7.1. In RAND we randomly choose Nsurr
N sur-
and introduce a Nsurr-
(˜yn, ˜xn)
rogate data points
{
dimensional vector of learnable (positive) weights ω, where
ω can be learned jointly with other variational parameters.

} ⊂ D

(cid:28)

5This expectation is closely related to a comment in Zhang
et al. (2021): [DAIS] relies on all intermediate distributions, and
so the error induced by stochastic gradient noise accumulates over
the whole trajectory. Simply taking smaller steps fails to reduce
the error. We conjecture that AIS-style algorithms are inherently
fragile to gradient noise.

Surrogate Likelihoods for Variational Annealed Importance Sampling

The surrogate log likelihood is then given by

ˆΨL(z) =

(cid:88)

n

ωn log pθ(˜yn

z, ˜xn)

|

(12)

Evidently computing ˆΨL(z) is
(Nsurr). Besides its sim-
plicity, an appealing feature of this parameterization is that
it leverages the known functional form of the likelihood.6

O

4.3. Discussion

Before taking a closer look at the theoretical properties of
NS-DAIS and SL-DAIS in Sec. 5, we make two simple
observations. First, as we should expect from any bona
ﬁde variational method, maximizing the variational bound
does indeed lead to tighter posterior approximations, as
formalized in the following proposition:

Proposition 1 The NS-DAIS and SL-DAIS approximate
posterior distributions, each of which is given by the
marginal qfwd(zK), both satisfy the inequality

log pθ(

)

− L ≥
D
NS−DAIS or

pθ(zK
KL(qfwd(zK)
|
SL−DAIS, respectively.

|D

))

0

≥

where

is

L

L

L

L

See Sec. A.4 for a proof. Thus as
increases for ﬁxed
θ, the KL divergence decreases and qfwd(zK) becomes a
better approximation to the posterior. Second, sampling
from qfwd in the case of NS-DAIS requires the entire dataset
. Conveniently in the case of SL-DAIS we only require
D
the surrogate log likelihood ˆΨL(z) so that the dataset can
be discarded after training.

5. Convergence analysis

As in Zhang et al. (2021) to make our analysis tractable we
(µ0, Λ−1
consider linear regression with a prior pθ(z) =
0 )
and a likelihood (cid:81)
R
RD. Furthermore we work under the following
and xn
set of simplifying assumptions:

obs), where each yn

xn, σ2

n N

(yn

z
|

N

∈

∈

·

Assumption 1 We use γ = 0, equally spaced inverse tem-
K −1/4, and
peratures
, a step size that varies as η
}
the prior as the base distribution (i.e. q0(z) = pθ(z)).

βk

∼

{

5.1. NS-DAIS

What kind of posterior approximation do we expect from
NS-DAIS? As clariﬁed by the following proposition, NS-
DAIS does not directly target the posterior pθ(z

).

|D

Proposition 2 The approximate posterior for linear regres-
sion given by running NS-DAIS with K steps under As-
sumption 1 converges to the ‘aggregate pseudo-posterior’

6Chen et al. (2022) use the same ansatz in concurrent work

focused on Bayesian coresets.

pAgg
θ

(z

)

≡

|D
. Here pθ(z

→ ∞

as K
|D
sponding to the data subset
likelihood term, i.e. pθ(z

J )]

Eq(J ) [pθ(z
J ) denotes the posterior corre-
J with appropriately scaled

(13)

|D

D
J )

|D

pθ(

J

D

∝

z)N/Bpθ(z).
|

See Sec. A.5 for a proof and additional details. Thus we
generally expect NS-DAIS to provide a good posterior ap-
proximation when the aggregate pseudo-posterior is a good
approximation to the posterior. The poor performance of
NS-DAIS in experiments in Sec. 7 suggests that this is not
case for moderate mini-batch sizes in typical models.

5.2. SL-DAIS

We now analyze SL-DAIS assuming the surrogate log likeli-
hood ˆΨL(z) differs from the full log likelihood. As is well
known the exact posterior for linear regression is given by
post(Λ0µ0 + 1
X(cid:62)y)
σ2
X(cid:62)X. The gradient of the full log

post) where µpost = Λ−1

(µpost, Λ−1

obs

N
and Λpost = Λ0 + 1
σ2
likelihood is given by

obs

zΨL(

D

∇

, z) =

1
σ2

obs

(cid:0)y(cid:62)X

−

X(cid:62)Xz(cid:1) = a

Bz (14)

−

1
X(cid:62)X.
where we have deﬁned a
σ2
We suppose that ˆΨL(z) is likewise a quadratic7 function of
z but that ˆΨL(z)

, z) so that we can write

y(cid:62)X and B

= ΨL(

1
σ2

≡

≡

obs

obs

D

z ˆΨL(z) = a + δa

∇

−

(B + δB)z

(15)

z ˆΨL(z) as in Eqn. 15 we can prove the following:

For

∇

Proposition 3 Running SL-DAIS for linear regression with
z ˆΨL(z) as in
K steps under Assumption 1 and using
Eqn. 15 results in a variational gap that can be bounded as

∇

log p(

D

)

− L

SL-DAIS

(K −1/2)
≤ O
(cid:124)
(cid:125)
(cid:123)(cid:122)
DAIS error

+

TrΛ−1
postδB
|
|
(cid:123)(cid:122)
(cid:125)
(cid:124)
surrogate likelihood error

where we have dropped higher order terms in δa and δB.8
Furthermore the KL divergence between qfwd(zK) and the
posterior pθ(zK

) is bounded by the same quantity.

|D

This intuitive result can be proven by suitably adapting the
results in Zhang et al. (2021); see Sec. A.6 in the supple-
mental materials for details.

Proposition 3 suggests that SL-DAIS offers an intuitive
trade-off between inference speed and ﬁdelity. For example,

7Note that ˆΨL(z) will be precisely of this form if we use the
RAND parameterization. In other words Eqn. 15 follows directly
from Eqn. 12, which is agnostic to the particular likelihood that
appears in the model.

8δa ﬁrst appears at second order. Note that we drop higher
order terms simply to make the bound more readily interpretable.

(cid:54)
Surrogate Likelihoods for Variational Annealed Importance Sampling

for a ﬁxed computational cost, SL-DAIS can accommodate
N/B larger than DAIS. For
a value of K that is a factor
∼
a sufﬁciently good surrogate log likelihood, the beneﬁts of
larger K can more than compensate for the error introduced
by an imperfect ˆΨL(z). See Sec. 7.3 for concrete examples.

to choose the Nsurr surrogate data points. In CS-FIX we
likewise use a coreset algorithm to choose the surrogate data
points but instead of learning ω we use the weights provided
by the coreset algorithm. Finally in NN we parameterize
ˆΨL(z) as a neural network. See Table 1 for the results.

6. Related work

Many variational objectives that leverage importance sam-
pling (IS) have been proposed. These include the impor-
tance weighted autoencoder (IWAE) (Burda et al., 2015;
Cremer et al., 2017), the thermodynamic variational objec-
tive (Masrani et al., 2019), and approaches that make use
of Sequential Monte Carlo (Le et al., 2017; Maddison et al.,
2017; Naesseth et al., 2018). For a general discussion of IS
in variational inference see Domke & Sheldon (2018).

An early combination of MCMC methods with variational
inference was proposed by Salimans et al. (2015) and Wolf
et al. (2016). A disadvantage of these approaches is the
need to learn reverse kernels, a shortcoming that was later
addressed by Caterini et al. (2018).

Bayesian coresets enable users to run MCMC on large
datasets after ﬁrst distilling the data into a smaller num-
ber of weighted data points (Huggins et al., 2016; Campbell
& Broderick, 2018; 2019). These methods do not support
model learning. Finally a number of authors have explored
MCMC methods that enable data subsampling (Maclaurin
& Adams, 2015; Quiroz et al., 2018; Zhang et al., 2020), in-
cluding those that leverage stochastic gradients (Welling &
Teh, 2011; Chen et al., 2014; Ma et al., 2015). See Sec. A.2
for an extended discussion of related work.

7. Experiments

Next we compare the performance of SL-DAIS and
NL-DAIS to various MCMC and variational base-
lines. Our experiments are implemented using JAX
(Bradbury et al., 2020) and NumPyro (Phan et al.,
2019; Bingham et al., 2019). An open source im-
plementation of our method will be made available at
https://num.pyro.ai/en/stable/autoguide.html.

7.1. Surrogate log likelihood comparison
We compare four ans¨atze for the surrogate log likelihood ˆΨL
used in SL-DAIS. For concreteness we consider a logistic
z, xn)
regression model with a bernoulli likelihood p(yn
|
governed by logits σ(xn
) is the logis-
z), where σ(
·
In RAND we randomly choose Nsurr sur-
tic function.
, introduce a Nsurr-
rogate data points
dimensional vector of learnable weights ω and let ˆΨL(z) =
(cid:80)
z, ˜xn). In CS-INIT we proceed similarly
|
but use a Bayesian coreset algorithm (Huggins et al., 2016)

(˜yn, ˜xn)
{

n ωn log p(˜yn

} ⊂ D

·

We can read off several conclusions from Table 1. First,
using a neural ansatz works extremely poorly. This is not
surprising, since a neural ansatz does not leverage the known
likelihood function. Second, for the three methods that rely
(˜yn, ˜xn)
on surrogate data points
, results improve as we
}
{
increase Nsurr, although the improvements are somewhat
modest for the two inference problems we consider. Third,
the simplest ansatz, namely RAND, works quite well. This
is encouraging because this ansatz is simple, generic, and
easily automated, which makes it an excellent candidate for
probabilistic programming frameworks. Consequently we
use RAND in all subsequent experiments. Finally, the poor
performance of CS-FIX makes it clear that the notion of
Bayesian coresets, while conceptually similar, is not congru-
ent with our use case for surrogate likelihoods. In particular
the coreset notion in (Huggins et al., 2016) aims to approxi-
, z) across latent space as a whole. However, the
mate ΨL(
zero-avoiding behavior of variational inference implies that
reproducing the precise tail behavior of ΨL(
, z) is less
important than accurately representing the vicinity of the
posterior mode. Additionally the Hamiltonian ‘integration
error’ resulting from using ﬁnite η and K can be partially
corrected for by learning ω jointly with q0(z), something
that a two-stage coreset-based approach is unable to do.

D

D

Dataset
Nsurr

RAND
CS-INIT
CS-FIX
NN

Higgs
64

211.1
209.6
83.0
16.8

1024

222.4
218.5
192.7

SUSY
64

627.5
625.9
501.0
2.6

1024

637.2
633.7
555.5

Table 1: We compare four ans¨atze for the surrogate log
likelihood ˆΨL on two logistic regression datasets. For the
three strategies that make use of surrogate data points, we
64, 1024
vary Nsurr
. We report ELBO improvements
}
(in nats) above a mean-ﬁeld Normal baseline and average
results across 20 replications. See Sec. 7.1 for details and
Table 6 in the supplement for expanded results, including
uncertainties and additional parameterizations.

∈ {

7.2. Classifying imbalanced data

To better understand the limitations of NS-DAIS and SL-
DAIS we consider a binary classiﬁcation problem with im-
balanced data. We expect both methods to ﬁnd this regime
, z) exhibits
challenging, since the full log likelihood ΨL(

D

Surrogate Likelihoods for Variational Annealed Importance Sampling

elevated sensitivity to the small number of terms involving
the rare class. See Fig. 1 for the results. Test accuracies
decrease substantially as the class imbalance increases; this
is expected, since there is considerable overlap between
the two classes in feature space. Strikingly, SL-DAIS with
the RAND surrogate ansatz outperforms NS-DAIS across
the board,9 with the gap increasing as the class imbalance
increases. Indeed NS-DAIS barely outperforms a mean-
ﬁeld baseline (not shown for legibility). This suggests that
SL-DAIS + RAND can be a viable strategy even in difﬁcult
regimes like the one explored here.

Figure 1: We depict test accuracy w.r.t. the rare class label
on an imbalanced binary classiﬁcation dataset. We compare
DAIS to SL-DAIS with Nsurr = 256 and NS-DAIS with
B = 256. The horizontal axis encodes the ratio between
the number of non-rare and rare class labels. Accuracies
are averaged over 20 (3) independent replications for SL-
DAIS and NS-DAIS (DAIS), respectively, and shaded bands
denote 90% conﬁdence intervals.

7.3. Logistic regression

×

We use ﬁve logistic regression datasets to systematically
compare eight variational methods and two MCMC methods.
For each dataset we consider a somewhat moderate number
104) so that we can include baseline
of training data (N = 5
methods that are impractical for larger N . Our baselines in-
clude three variational methods that use parametric distribu-
tions: mean-ﬁeld Normal (MF); multivariate Normal (MVN);
and a block neural autoregressive ﬂow (Flow) (De Cao
et al., 2020). They also include DAIS-MF, NS-DAIS-MF,
and SL-DAIS-MF (all of which use a mean-ﬁeld base dis-
tribution) and SL-DAIS-MVN, which uses a MVN base
distribution. NS-DAIS and SL-DAIS use B = 256 and
Nsurr = 256, respectively. The two MCMC methods are
HMCECS (Dang et al., 2019), a variant of HMC that consid-
ers subsets of data in each iteration, and NUTS-CS, where
we ﬁrst form a Bayesian coreset consisting of 1024 weighted

9We observe similar behavior for the ELBO.

data points, and then run NUTS (Hoffman et al., 2014). We
emphasize that these two MCMC methods do not offer
some of the beneﬁts of variational methods (e.g. support for
model learning) but we include them so that we can better
assess the quality of the approximate posterior predictive
distributions returned by the variational methods.

See Fig. 2 and Table 2 for results. We ﬁnd that SL-DAIS
consistently outperforms NS-DAIS and in many cases it
matches the performance of DAIS. Among the scalable
methods Flow and SL-DAIS perform best, although SL-
DAIS can be substantially faster than Flow, see Fig. 3.
Fig. 3 also clariﬁes the computational beneﬁts of SL-DAIS:
SL-DAIS with K = 8 steps handily outperforms DAIS with
K = 2 at about 2% of the computational cost. Finally we
ﬁnd that the best variational methods yield predictive log
likelihoods that are competitive with the MCMC baselines.

7.4. Robust Gaussian process regression

We model a geospatial precipitation dataset (Lyon, 2004;
Lyon & Barnston, 2005) considered in (Pleiss et al., 2020)
using a Gaussian process regressor with a heavy-tailed Stu-
dent’s t likelihood; see Sec. A.7.5 for details on the modeling
setup. We compare SL-DAIS with K = 4, Nsurr = 256,
and a multivariate Normal (MVN) base distribution q0(z)
to a baseline with a MVN variational distribution. Note that
the dimension of the latent space D is equal to the number
of inducing points. See Table 3 for the results.

There is signiﬁcant variability in the data and consequently
the posterior function values exhibit considerable uncer-
tainty. Due to the non-conjugate likelihood, the posterior
over the inducing points features signiﬁcant non-gaussianity,
especially as the number of inducing points increases and
the inducing point locations become closer together. Con-
sequently we see the largest ELBO and test log likelihood
improvements for the largest number of inducing points.

latent dim. D = 64
∆ELBO
∆LL

127.3 ± 26.6
0.014 ± 0.003

D = 128
187.1 ± 26.6
0.015 ± 0.003

D = 256
529.5 ± 17.3
0.039 ± 0.006

Table 3: We report ELBO and test log likelihood (LL) im-
provements in nats for the regression experiment in Sec. 7.4,
comparing SL-DAIS to a baseline with a multivariate Nor-
mal variational distribution. Test log likelihoods are normal-
ized by the number of test points. Results are averaged over
seven train/test splits.

7.5. Gaussian process classiﬁcation

We train Gaussian process classiﬁers on the classiﬁcation
datasets considered in Sec. 7.3. We compare SL-DAIS with
K = 4, Nsurr = 512, and a MVN base distribution to

124816Classimbalance2030405060AccuracyNS-DAISSL-DAISDAISSurrogate Likelihoods for Variational Annealed Importance Sampling

Figure 2: We report ELBO improvements and test log likelihoods for the logistic regression experiment in Sec. 7.3. ELBO
improvements are with respect to the mean-ﬁeld (MF) Normal baseline. Circles denote variational methods and squares
denote MCMC methods. Blue methods are ours, red methods are mini-batchable variational methods, and green methods
are everything else. Metrics are averaged over 7 independent replications and error bars denote standard errors (we do 3
independent replications for the most expensive methods, namely DAIS and HMCECS). See Sec. A.7.3 in the supplement
for test accuracies. Note that numerals in method names indicate the number of HMC steps K used.

ELBO
Log likelihood
Opt. time

MF MVN Flow
5.36
6.60
2.08
5.88
6.16
2.16
3420.3
58.1
38.8

NS-DAIS-MF
5.00
4.60
113.1

SL-DAIS-MF NS-DAIS-MVN SL-DAIS-MVN
2.60
2.44
83.1

4.16
4.36
152.1

2.20
2.40
131.6

Table 2: We report performance ranks w.r.t. ELBO and test log likelihood across 5 train/test splits and 5 datasets for the
logistic regression experiment in Sec. 7.3. We also report time per optimization step in milliseconds as in Fig. 3. Lower is
better for all metrics. The rank satisﬁes 1

7, since we compare 7 scalable variational methods.

rank

≤

≤

a baseline with a MVN variational distribution. We use
128 inducing points so the latent dimension is D = 128.
See Table 4 for results. We ﬁnd that SL-DAIS leads to
consistently higher ELBOs and that this translates to small
but non-negligible gains in test accuracy for the datasets
with the largest ELBO gains.

We emphasize that this and the preceding Gaussian process
experiments represent difﬁcult inference problems, since the
latent dimensionality is moderately high (with D as large
as 256) and since high-dimensional model parameters θ
(including D inducing point locations) are learned jointly
with the approximate posterior.

Dataset
SUSY
Higgs
MiniBooNE
CovType-Fir
CovType-Pine

∆ELBO
34.0 ± 2.2
225.5 ± 7.3
163.9 ± 5.4
471.7 ± 42.2
616.6 ± 71.8

∆Accuracy
0.002 ± 0.019
0.072 ± 0.044
0.121 ± 0.011
0.399 ± 0.080
0.425 ± 0.045

Table 4: We report ELBO and test accuracy improvements
for the classiﬁcation experiment in Sec. 7.5, comparing SL-
DAIS to a baseline with a multivariate Normal variational
distribution. Results are averaged over ﬁve train/test splits.

7.6. Local latent variable models

What about models with global and local latent variables?
For simplicity we evaluate the performance of the simplest
DAIS-like inference procedure that can simultaneously ac-

0500∆ELBOSL-DAIS-MVN-8SL-DAIS-MF-8NS-DAIS-MF-8DAIS-MF-8DAIS-MF-2FlowMVNSUSY(D=18)100200∆ELBOHiggs(D=28)0500∆ELBOMiniBooNE(D=51)250500750∆ELBOCovType-Fir(D=55)250500750∆ELBOCovType-Pine(D=55)−0.49−0.48LoglikelihoodSL-DAIS-MVN-8SL-DAIS-MF-8NS-DAIS-MF-8FlowMVNMFDAIS-MF-8DAIS-MF-2HMCECSNUTS-CSSUSY(D=18)−0.65−0.64LoglikelihoodHiggs(D=28)−0.32−0.30LoglikelihoodMiniBooNE(D=51)−0.495−0.490−0.485LoglikelihoodCovType-Fir(D=55)−0.55−0.54LoglikelihoodCovType-Pine(D=55)Surrogate Likelihoods for Variational Annealed Importance Sampling

8. Discussion

In this work we have focused on models with global latent
variables. The experiment in Sec. 7.6 only scratches the
surface of what is possible for the richer and more complex
case of models that contain global and local latent variables.
Exploring hybrid scalable inference strategies for this class
of models that combine gradient-based MCMC with varia-
tional methods is an interesting direction for future work.

Acknowledgements

We thank Tomas Geffner for answering questions about
Geffner & Domke (2021). We thank Ola Rønning for con-
tributing the open source NumPyro-based HMCECS imple-
mentation we used in our experiments.

References

Asuncion, A. and Newman, D. Uci machine learning repos-

itory, 2007.

Bachem, O., Lucic, M., and Krause, A. Practical core-
set constructions for machine learning. arXiv preprint
arXiv:1703.06476, 2017.

Baldi, P., Sadowski, P., and Whiteson, D. Searching for
exotic particles in high-energy physics with deep learning.
Nature communications, 5(1):1–9, 2014.

Betancourt, M. The fundamental incompatibility of scalable
hamiltonian monte carlo and naive data subsampling. In
International Conference on Machine Learning, pp. 533–
540. PMLR, 2015.

Betancourt, M. A conceptual introduction to hamiltonian
monte carlo. arXiv preprint arXiv:1701.02434, 2017.

Bingham, E., Chen, J. P., Jankowiak, M., Obermeyer, F.,
Pradhan, N., Karaletsos, T., Singh, R., Szerlip, P., Hors-
fall, P., and Goodman, N. D. Pyro: Deep universal proba-
bilistic programming. The Journal of Machine Learning
Research, 20(1):973–978, 2019.

Blackard, J. A. and Dean, D. J. Comparative accuracies
of artiﬁcial neural networks and discriminant analysis in
predicting forest cover types from cartographic variables.
Computers and electronics in agriculture, 24(3):131–151,
1999.

Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. Varia-
tional inference: A review for statisticians. Journal of
the American statistical Association, 112(518):859–877,
2017.

Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,
C., Maclaurin, D., and Wanderman-Milne, S. Jax: com-

Figure 3: We report the time per optimization step for each
variational method in Fig. 2 on the CovType-Fir dataset.
Runtimes are for a CPU with 24 cores (Intel Xeon Gold
5220R 2.2GHz).

commodate local latent variables and data subsampling. In
brief we introduce a parametric distribution for the global
latent variable and use DAIS to deﬁne a distribution over
the local latent variables. Assuming the local latent vari-
ables are conditionally independent once we condition on
the global latent variable, this results in N non-interacting
DAIS chains. Consequently the ELBO is amenable to data
subsampling; see Sec. A.1 for an extended discussion

To evaluate this approach we consider a robust linear regres-
sion model that uses a Student’s t likelihood. We use the
well-known representation of this likelihood as a continuous
mixture of Normal distributions. This yields a model with
local Gamma variates where the local latent variables can
be integrated out exactly.

We compare two variational approaches, both of which
use a mean-ﬁeld Normal distribution for the global latent
variable. To deﬁne an oracle baseline we integrate out the
local latent variables before performing variational infer-
ence. This oracle represents an upper performance bound
on the semi-parametric approach deﬁned above, which we
refer to as Semi-DAIS. See Table 5 for results. We ﬁnd
that for K = 16 Semi-DAIS ‘recovers’ about half of the
gap between a fully mean-ﬁeld baseline and the oracle. We
expect this gap would decrease further for larger K.

Dataset
Pol
Elevators

Semi-DAIS-8
91.4 ± 3.3
186.3 ± 5.3

Semi-DAIS-16 Oracle
136.4 ± 3.6
366.8 ± 5.5

360.2 ± 2.6
758.0 ± 2.5

Table 5: We report results for the experiment in Sec. 7.6. In
each case the reported ELBO improvement is above a mean
ﬁeld baseline. Results are averaged over 10 replications.

0.1110100Gradientsteptime(ms)SL-DAIS-MVN-8SL-DAIS-MF-8NS-DAIS-MF-8DAIS-MF-8DAIS-MF-2FlowMVNMFSurrogate Likelihoods for Variational Annealed Importance Sampling

posable transformations of python+ numpy programs,
2018. URL http://github. com/google/jax, 4:16, 2020.

Burda, Y., Grosse, R., and Salakhutdinov, R. Importance
weighted autoencoders. arXiv preprint arXiv:1509.00519,
2015.

Campbell, T. and Broderick, T. Bayesian coreset construc-
tion via greedy iterative geodesic ascent. In International
Conference on Machine Learning, pp. 698–706. PMLR,
2018.

Grosse, R. B., Ghahramani, Z., and Adams, R. P. Sandwich-
ing the marginal likelihood using bidirectional monte
carlo. arXiv preprint arXiv:1511.02543, 2015.

Hoffman, M. D. Learning deep latent gaussian models with
markov chain monte carlo. In International conference
on machine learning, pp. 1510–1519. PMLR, 2017.

Hoffman, M. D., Blei, D. M., Wang, C., and Paisley, J.
Stochastic variational inference. Journal of Machine
Learning Research, 14(5), 2013.

Campbell, T. and Broderick, T. Automated scalable
bayesian inference via hilbert coresets. The Journal of
Machine Learning Research, 20(1):551–588, 2019.

Hoffman, M. D., Gelman, A., et al. The no-u-turn sampler:
adaptively setting path lengths in hamiltonian monte carlo.
J. Mach. Learn. Res., 15(1):1593–1623, 2014.

Caterini, A. L., Doucet, A., and Sejdinovic, D. Hamil-
arXiv preprint

tonian variational auto-encoder.
arXiv:1805.11328, 2018.

Chen, N., Xu, Z., and Campbell, T. Bayesian infer-
arXiv preprint

ence via sparse hamiltonian ﬂows.
arXiv:2203.05723, 2022.

Chen, T., Fox, E., and Guestrin, C. Stochastic gradient
hamiltonian monte carlo. In International conference on
machine learning, pp. 1683–1691. PMLR, 2014.

Cremer, C., Morris, Q., and Duvenaud, D. Reinterpret-
ing importance-weighted autoencoders. arXiv preprint
arXiv:1704.02916, 2017.

Dang, K.-D., Quiroz, M., Kohn, R., Minh-Ngoc, T., and Vil-
lani, M. Hamiltonian monte carlo with energy conserving
subsampling. Journal of machine learning research, 20,
2019.

Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. The
helmholtz machine. Neural computation, 7(5):889–904,
1995.

De Cao, N., Aziz, W., and Titov, I. Block neural autore-
gressive ﬂow. In Uncertainty in Artiﬁcial Intelligence, pp.
1263–1273. PMLR, 2020.

Ding, X. and Freedman, D. J. Learning deep generative mod-
els with annealed importance sampling. arXiv preprint
arXiv:1906.04904, 2019.

Domke, J. and Sheldon, D. Importance weighting and varia-
tional inference. arXiv preprint arXiv:1808.09034, 2018.

Duane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D.
Hybrid monte carlo. Physics letters B, 195(2):216–222,
1987.

Geffner, T. and Domke, J. Mcmc variational inference via
uncorrected hamiltonian annealing. Advances in Neural
Information Processing Systems, 34, 2021.

Horowitz, A. M. A generalized guided monte carlo algo-

rithm. Physics Letters B, 268(2):247–252, 1991.

Huggins, J., Campbell, T., and Broderick, T. Coresets for
scalable bayesian logistic regression. Advances in Neural
Information Processing Systems, 29:4080–4088, 2016.

Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul,
L. K. An introduction to variational methods for graphical
models. Machine learning, 37(2):183–233, 1999.

Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Le, T. A., Igl, M., Rainforth, T., Jin, T., and Wood, F.
Auto-encoding sequential monte carlo. arXiv preprint
arXiv:1705.10306, 2017.

Li, Y., Turner, R. E., and Liu, Q. Approximate inference
with amortised mcmc. arXiv preprint arXiv:1702.08343,
2017.

Lyon, B. The strength of el ni˜no and the spatial extent of
tropical drought. Geophysical Research Letters, 31(21),
2004.

Lyon, B. and Barnston, A. G. Enso and the spatial extent of
interannual precipitation extremes in tropical land areas.
Journal of Climate, 18(23):5095–5109, 2005.

Ma, Y.-A., Chen, T., and Fox, E. B. A complete
recipe for stochastic gradient mcmc. arXiv preprint
arXiv:1506.04696, 2015.

Maclaurin, D. and Adams, R. P. Fireﬂy monte carlo: Exact
mcmc with subsets of data. In Twenty-Fourth Interna-
tional Joint Conference on Artiﬁcial Intelligence, 2015.

Maddison, C. J., Lawson, D., Tucker, G., Heess,
N., Norouzi, M., Mnih, A., Doucet, A., and Teh,
Y. W. Filtering variational objectives. arXiv preprint
arXiv:1705.09279, 2017.

Surrogate Likelihoods for Variational Annealed Importance Sampling

Masrani, V., Le, T. A., and Wood, F. The thermodynamic
variational objective. arXiv preprint arXiv:1907.00031,
2019.

Rezende, D. and Mohamed, S. Variational inference with
normalizing ﬂows. In International conference on ma-
chine learning, pp. 1530–1538. PMLR, 2015.

Miller, A. C., Foti, N. J., Lewnard, J. A., Jewell, N. P.,
Guestrin, C., and Fox, E. B. Mobility trends provide a
leading indicator of changes in sars-cov-2 transmission.
MedRxiv, 2020.

Monod, M., Blenkinsop, A., Xi, X., Hebert, D., Bershan,
S., Tietze, S., Baguelin, M., Bradley, V. C., Chen, Y.,
Coupland, H., et al. Age groups that sustain resurging
covid-19 epidemics in the united states. Science, 371
(6536):eabe8372, 2021.

Naesseth, C., Linderman, S., Ranganath, R., and Blei, D.
Variational sequential monte carlo. In International con-
ference on artiﬁcial intelligence and statistics, pp. 968–
977. PMLR, 2018.

Neal, R. M. Annealed importance sampling. Statistics and

computing, 11(2):125–139, 2001.

Neal, R. M. et al. Mcmc using hamiltonian dynamics. Hand-

book of markov chain monte carlo, 2(11):2, 2011.

Obermeyer, F., Jankowiak, M., Barkas, N., Schaffner,
S. F., Pyle, J. D., Yurkovetskiy, L., Bosso, M., Park,
D. J., Babadi, M., MacInnis, B. L., Luban, J., Sa-
beti, P. C., and Lemieux, J. E. Analysis of 6.4 mil-
lion sars-cov-2 genomes identiﬁes mutations associ-
Science, 2022.
ated with ﬁtness.
doi: 10.1126/
science.abm1208. URL https://www.science.
org/doi/abs/10.1126/science.abm1208.

Phan, D., Pradhan, N., and Jankowiak, M. Composable
effects for ﬂexible and accelerated probabilistic program-
ming in numpyro. arXiv preprint arXiv:1912.11554,
2019.

Pleiss, G., Jankowiak, M., Eriksson, D., Damle, A., and
Gardner, J. Fast matrix square roots with applications to
gaussian processes and bayesian optimization. Advances
in Neural Information Processing Systems, 33:22268–
22281, 2020.

Quinonero-Candela, J. and Rasmussen, C. E. A unify-
ing view of sparse approximate gaussian process regres-
sion. The Journal of Machine Learning Research, 6:
1939–1959, 2005.

Quiroz, M., Kohn, R., Villani, M., and Tran, M.-N. Speeding
up mcmc by efﬁcient data subsampling. Journal of the
American Statistical Association, 2018.

Ranganath, R., Gerrish, S., and Blei, D. Black box varia-
tional inference. In Artiﬁcial intelligence and statistics,
pp. 814–822. PMLR, 2014.

Roe, B. P., Yang, H.-J., Zhu, J., Liu, Y., Stancu, I., and
McGregor, G. Boosted decision trees as an alternative
to artiﬁcial neural networks for particle identiﬁcation.
Nuclear Instruments and Methods in Physics Research
Section A: Accelerators, Spectrometers, Detectors and
Associated Equipment, 543(2-3):577–584, 2005.

Ruiz, F. and Titsias, M. A contrastive divergence for combin-
ing variational inference and mcmc. In International Con-
ference on Machine Learning, pp. 5537–5545. PMLR,
2019.

Salimans, T., Kingma, D., and Welling, M. Markov chain
monte carlo and variational inference: Bridging the gap.
In International Conference on Machine Learning, pp.
1218–1226. PMLR, 2015.

Snelson, E. and Ghahramani, Z. Sparse gaussian processes
using pseudo-inputs. Advances in neural information
processing systems, 18:1257, 2006.

Sohl-Dickstein, J. and Culpepper, B. J. Hamiltonian an-
nealed importance sampling for partition function estima-
tion. arXiv preprint arXiv:1205.1925, 2012.

Thin, A., Kotelevskii, N., Doucet, A., Durmus, A.,
Moulines, E., and Panov, M. Monte carlo variational
auto-encoders. In International Conference on Machine
Learning, pp. 10247–10257. PMLR, 2021.

Tran, M.-N., Kohn, R., Quiroz, M., and Villani, M.
The block pseudo-marginal sampler. arXiv preprint
arXiv:1603.02485, 2016.

Welling, M. and Teh, Y. W. Bayesian learning via stochastic
gradient langevin dynamics. In Proceedings of the 28th
international conference on machine learning (ICML-11),
pp. 681–688. Citeseer, 2011.

Wolf, C., Karl, M., and van der Smagt, P. Variational in-
ference with hamiltonian monte carlo. arXiv preprint
arXiv:1609.08203, 2016.

Zhang, G., Hsu, K., Li, J., Finn, C., and Grosse, R. B. Dif-
ferentiable annealed importance sampling and the perils
of gradient noise. Advances in Neural Information Pro-
cessing Systems, 34, 2021.

Zhang, R., Cooper, A. F., and De Sa, C. M. Asymptotically
optimal exact minibatch metropolis-hastings. Advances
in Neural Information Processing Systems, 33:19500–
19510, 2020.

Surrogate Likelihoods for Variational Annealed Importance Sampling

A. Appendix

The appendix is organized as follows. In Sec. A.1 we discuss local latent variable models. In Sec. A.2 we provide an
extended discussion of related work. In Sec. A.3 we provide a proof of Lemma 1. In Sec. A.4 we discuss general properties
of NS-DAIS and SL-DAIS. In Sec. A.5 we discuss NS-DAIS and in Sec. A.6 we discuss SL-DAIS. In Sec. A.7 we provide
details about our experimental setup. In Sec. A.8 we present additional ﬁgures and tables that accompany those in the main
text.

A.1. Local Latent Variable Models

Another important class of models includes local latent variables
with joint densities of the form:

wn
{

}

in addition to a global latent variable z, i.e. models

pθ(

D

, W, z) = pθ(z)

N
(cid:89)

n=1

pθ(wn

|

z)pθ(yn

wn, z, xn)
|

(16)

A viable DAIS-like inference strategy for this class of models that would be scalable to large N is to adopt a semi-parametric
approach. First, we introduce a ﬂexible, parametric variational distribution qφ(z) using, for example, a normalizing ﬂow.
qφ(z) the N inference problems w.r.t.
After conditioning on a sample z
effectively decouple. Consequently we can
apply DAIS to each subproblem, resulting in an ELBO that accommodates unbiased mini-batch estimates.

wn

∼

{

}

In more detail we proceed as follows. Introduce a parametric variational distribution qφ(z) and a mean-ﬁeld distribution
qφ(W) that factorizes as qφ(W) = (cid:81)
n qφ(wn). Then write (in this section we suppress momenta terms in the MCMC
kernels for brevity)

1(W1
qfwd(z, W0:K) = qφ(z)
T
, WK, z) ˜
T
= Eqfwd [log qbwd

qbwd(z, w0:K) = pθ(

W0, z)
|

K(WK

WK−1, z)
|
· · · T
˜
1(W0
WK, z)
K(WK−1
T
|
log qfwd]

· · ·

D

L

−

where the forward MCMC kernels target a log density of the form (here for β = 1)

N
(cid:88)

n=1

(log pθ(wn

|

z) + log pθ(yn

wn, z, xn))
|

W1, z)

|

(17)

(18)

where z is kept ﬁxed throughout HMC dynamics. Since this density splits up into a sum over N turns, we actually have N
, WK, z) term that
independent DAIS chains (assuming our mass matrix is appropriately factorized). Since the log pθ(
also splits up into such a sum, this ELBO admits mini-batch sampling. That is, we choose a mini-batch of
enters into
size B
are not instantiated. This of
course is exactly how subsampling works with vanilla parametric mean-ﬁeld variational distributions.

L
N and run B DAIS chains forward at each optimization step so that e.g. most

wn

(cid:28)

D

}

{

We can follow the same basic strategy to make this variational distribution more fully DAIS-like while still supporting
subsampling. The basic idea is that we will have N + 1 DAIS chains. The N DAIS chains for each wn will be as above.
But we will also introduce a DAIS chain that is responsible for producing the z sample. To enable subsampling this z-chain
must target a surrogate likelihood.

In more detail we proceed as follows. To simplify the equations we assume K = 1.

qfwd(˜z0:1, ˜W0:1, W0:1) = qφ(˜z0, ˜W0)qφ(W0)
qbwd(˜z0:1, ˜W0:1, W0:1) = pθ(

, W1, ˜z1) ˜
T

D

surr(˜z1, ˜W1

˜z0, ˜W0)
T
(W0

|
˜z1, ˜W1) ˜
T

|

|

T
surr(˜z0, ˜W0
log qfwd]

= Eqfwd [log qbwd

L

−

(W1

W0, ˜z1)

|
W1, ˜z1)

(19)

Here qφ(˜z0, ˜W0) and qφ(W0) are (distinct) simple parametric variational distributions.
surr is a DAIS chain that targets
a surrogate likelihood with learnable weights that includes Nsurr data points and also Nsurr local latent variables ˜W. As
W0, ˜z1) consists of N independent DAIS chains, all of which are conditioned on ˜z1. Note that since these N
above
|
DAIS chains factorize there is no need for a surrogate here. We note that the output of the N + 1 DAIS chains is ˜z1, ˜W1 and
W1. ˜W1 is a Nsurr-dimensional ‘auxiliary variable’ and so we essentially throw it out (nothing depends on it directly)—its

(W1

T

T

Surrogate Likelihoods for Variational Annealed Importance Sampling

D

only purpose is to ‘integrate out’ W-uncertainty in the surrogate likelihood.10 By contrast W1 is the ‘actual’ W sample
, W1, ˜z1), i.e. our approximate posterior sample is (W1, ˜z1). As above we can still subsample the N
that enters into pθ(
DAIS chains and get an ELBO that supports data subsampling. Basically we’ve replaced the parametric distribution for z
above with a single DAIS chain that targets a surrogate likelihood.
Yet another (somewhat simpler) variant of this approach would forego ˜W transitions in building up the z sample and instead
parameterize the surrogate likelihood using a learnable point parameter ˜W. As should now be evident, the space of models
with mixed local/global latent variables opens up lots of possibilities for DAIS-like inference procedures. For this reason we
leave a detailed empirical exploration of the algorithms described above to future work.

A.2. Related Work (extended)

In this section we provide an extended discussion of related work. Many variational objectives that leverage importance
sampling (IS) have been proposed. These include the importance weighted autoencoder (IWAE) (Burda et al., 2015; Cremer
et al., 2017), the thermodynamic variational objective (Masrani et al., 2019), and approaches that make use of Sequential
Monte Carlo (Le et al., 2017; Maddison et al., 2017; Naesseth et al., 2018). For a general discussion of IS in variational
inference see Domke & Sheldon (2018).

As discussed in Sec. 3 in the main text our work builds on recent work, namely (UHA; Geffner & Domke (2021)) and
(DAIS; Zhang et al. (2021)), which can be understood as utilizing unadjusted underdamped Langevin steps. A closely
related approach that utilizes unadjusted overdamped Langevin steps is described in (Thin et al., 2021). We note that earlier
attempts to incorporate AIS into a variational framework like Ding & Freedman (2019) do not beneﬁt from a single, uniﬁed
ELBO-based objective.

An early combination of MCMC methods with variational inference was proposed by Salimans et al. (2015) and Wolf et al.
(2016). A disadvantage of these approaches is the need to learn reverse kernels, a shortcoming that was later addressed by
Caterini et al. (2018). Additional work that explores combinations of MCMC and variational methods includes Li et al.
(2017); Hoffman (2017); Ruiz & Titsias (2019)

Bayesian coresets enable users to run MCMC on large datasets after ﬁrst distilling the data into a smaller number of weighted
data points (Huggins et al., 2016; Bachem et al., 2017; Campbell & Broderick, 2018; 2019). These methods do not support
model learning, since they rely on a two-stage approach (i.e. coreset learning and inference are done separately). Finally
a number of authors have explored MCMC methods that enable data subsampling (Maclaurin & Adams, 2015; Quiroz
et al., 2018; Dang et al., 2019; Zhang et al., 2020), including those that leverage stochastic gradients (Welling & Teh, 2011;
Chen et al., 2014; Ma et al., 2015). Finally we note that learning ﬂexible variational distributions parameterized by neural
networks, i.e. normalizing ﬂows, is another popular black-box approach for improving variational approximations (Rezende
& Mohamed, 2015).

A.3. Computing

DAIS

L

We provide a proof of Lemma 1 in the main text. We reproduce the argument from Zhang et al. (2021); see Geffner &
Domke (2021) for a similar derivation. The forward kernel

k in Eqn. 9 can be decomposed as

k(zk, vk

zk−1, vk−1) =
|

k

T

T

zk−1, vk−1)
T
|

refresh(vk

k

ˆvk)
|

T
leap(zk, ˆvk

Similarly the backward kernel ˜
T

k in Eqn. 9 can be decomposed as

˜
k(zk−1, vk−1
T

zk, vk) = ˜
k
T

|

refresh(ˆvk

vk) ˜
k
T
|

leap(zk−1, vk−1

zk,
|

−

ˆvk)

where we ﬂip the sign of ˆvk to account for time reversal. Since we have

refresh(vk

k

T

ˆvk) =

|

(vk

|

N

γ ˆvk, (1

−

γ2)M)

refresh(ˆvk

˜
k
T

vk) =
|

N

(ˆvk

γvk, (1
|

−

γ2)M)

and

vk

||

−

γ ˆvk

2
||

ˆvk

γvk

2 = (1
||

−

−

− ||

γ2) (cid:0)

vk

||

2
||

ˆvk

2(cid:1)

||

− ||

10For this reason we will probably need to pretrain qφ(˜z0, ˜W0)—perhaps introducing an auxiliary loss function—since ˜W1 will have

a weak learning signal.

(20)

(21)

(22)

(23)

Surrogate Likelihoods for Variational Annealed Importance Sampling

it follows that

k
T
˜
k
T
Furthermore, since the leapfrog step is volume preserving and reversible we also have that

= N
N

(vk
(ˆvk

0, M)
|
0, M)
|

refresh(vk
refresh(ˆvk

ˆvk)
|
vk)
|

leap(zk, ˆvk

k

T

zk−1, vk−1) = ˜
k
T
|

leap(zk−1, vk−1

zk,
|

−

ˆvk)

(24)

(25)

From these equations the only tricky part of Lemma 1, i.e. the derivation of the kinetic energy difference correction, follows.
leap; in particular using a surrogate
Importantly, Eqn. 25 is true regardless of the target density used to deﬁne
log likelihood leaves Lemma 1 intact.

leap and ˜
k
T

T

k

A.4. General properties of NS-DAIS and SL-DAIS

We prove Proposition 1 in the main text. In particular we show that the NS-DAIS and SL-DAIS approximate posteriors
qfwd(zK) satisfy the inequality

log pθ(

)

D

+ KL(qfwd(zK)
|

pθ(zK

))

|D

≥ L

(26)

where
corresponding to qbwd(z0:K, v0:K) so that log qbwd(z0:K, v0:K) = log pθ(

NS−DAIS or

L

L

L

is

SL−DAIS. First consider SL-DAIS and let ˜qbwd(z0:K, v0:K) be the normalized distribution

) + log ˜qbwd(z0:K, v0:K) and write

D

log pθ(

D

) = Eqfwd

(cid:104)

log qbwd(z0:K, v0:K)

−

(cid:105)
log qfwd(z0:K, v0:K)

+

KL(qfwd(z0:K, v0:K)
˜qbwd(z0:K, v0:K))
|
SL−DAIS + KL(qfwd(z0:K, v0:K)
˜qbwd(z0:K, v0:K))
|
pθ(zK
SL−DAIS + KL(qfwd(zK)
|
Eqfwd(zK ) [KL(qfwd(z0:K, v0:K

˜qbwd(z0:K, v0:K

)) +

=

=

L

L

|D
zK)
|
|

zK))]
|

where we appealed to the chain rule of KL divergences which reads

p(a, b)) = KL(q(a)
KL(q(a, b)
|

p(a)) + Eq(b) [KL(q(a
|
|

p(a
b)
|
|

b))]

(27)

(28)

(29)

(30)

The result then follows since the second KL divergence in Eqn. 29 is non-negative. The result for NS-DAIS follows by
the same argument, with the difference that
index
plays no role in this proof, since its role is to facilitate unbiased mini-batch estimates, i.e. it has no impact on the value of

is now one of the ‘auxiliary’ variables

z0:K, v0:K
{

. Note that the

J

I

}

NS−DAIS or

L

SL−DAIS (see the next section for details).

L

A.5. NS-DAIS

We ﬁrst elaborate how NS-DAIS is formulated. The forward and backward kernels are deﬁned as:

qfwd(z0:K, v0:K,

I
qbwd(z0:K, v0:K,

,

J
,

I

J

) = q(

I
) = q(

)q(

)q(

I

J

J

)q0(z0)q0(v0)(cid:81)K
)pθ(zK

k(zk, vk
k=1T
I)N/Bpθ(zK)(cid:81)K

k=1

|D

)

|

zk−1, vk−1,
J
˜
k(zk−1, vk−1
T

zk, vk,

|

)

J

controls the potential energy used to guide the HMC dynamics and the corresponding ELBO is given by

NS−DAIS

L

(cid:104)

(cid:104)

Eqfwd

≡
= Eq(cid:48)

fwd

log qbwd(z0:K, v0:K,

I

,

log q(cid:48)

bwd(z0:K, v0:K,

J

J
)

−
log q(cid:48)

−

)

log qfwd(z0:K, v0:K,

fwd(z0:K, v0:K,

(cid:105)
)

J

,
I
(cid:105)
)

J

where

J

where

q(cid:48)
fwd(z0:K, v0:K,
q(cid:48)
bwd(z0:K, v0:K,

J

J

) = q(

) = q(

J

J

)q0(z0)q0(v0)(cid:81)K
)pθ(zK

k=1T
)pθ(zK)(cid:81)K

|D

zk−1, vk−1,
|

k(zk, vk
˜
k(zk−1, vk−1
T

J
zk, vk,
|

k=1

)

)

J

(31)

(32)

Surrogate Likelihoods for Variational Annealed Importance Sampling

Algorithm 2 NS-DAIS: Naive Subsampling Differentiable Annealed Importance Sampling. We highlight in blue where the
algorithm differs from DAIS. To recover DAIS we substitute N
, zK),
N (or in other words remove all mini-batch sampling).
and B

B ΨL(

B ΨL(

, z), N

I, zK)

J , z)

ΨL(

ΨL(

→

→

D

D

D

D

, step size η, momentum refresh parameter γ, mass matrix M, dataset
}

D

, z), base variational distribution q0(z), number of
of size N ,

D

→

Input: model log prior density Ψ0(z), model log likelihood ΨL(
βk
steps K, inverse temperatures
{
mini-batch size B
Initialize: z0
∼ N
∼
Sample mini-batch indices
for k = 1 to K do
zk−1 + η
2 M−1vk−1
(cid:16)
(cid:110)
Ψ0(z)+ N
βk

log q0(z0)
with

L ← −
}

βk) log q0(z)

(0, M),

1, ..., N

q0, v0

J ⊂ {

J , z)

= B

+(1

|J |

gk

ˆzk

←

(cid:17)

B ΨL(

D

−

z
← ∇

(cid:111)(cid:12)
(cid:12)
(cid:12)
(cid:12)z=ˆzk

ˆvk
vk−1 + ηgk
ˆzk + η
zk
if k < K then

←
←

2 M−1 ˆvk

γ ˆvk + (cid:112)1

vk
end if

←

γ2ε, ε

−

∼ N

(0, M)

+ log

(ˆvk, M)

N

L ← L

end for
Sample mini-batch indices
+ Ψ0(zK) + N

Return:

L

log

N

(vk−1, M)

−

1, ..., N

with

}

|I|

= B

I ⊂ {
B ΨL(

D

I, zK)

are the forward and backward kernels without the
is the momentum distribution. To be more speciﬁc, for each

I

mini-batch index.11 Note that here and elsewhere q0(v0) =

the potential energy that guides each

T

J

Vk(z

) =

(1

−

−

|J

βk) log q0(z)

βk

−

(cid:0)Ψ0(z) + N

B ΨL(

J , z)(cid:1)

D

See Algorithm 2 for the complete procedure and see Algorithm 3 to make a direct comparison to DAIS.

(v0
k is given by

N

0, M)
|

(33)

Now we prove Proposition 2 in the main text. As in the main text let q(
to sampling mini-batches of B indices
corresponds to the data subset

}
J with appropriately scaled likelihood term, i.e.

1, ..., N

J ⊂ {

J

without replacement. Let pθ(z

|D

) denote the distribution that corresponds
J ) denote the posterior that

D

Further denote the ‘aggregate pseudo-posterior’ by

pθ(z

J )

|D

∝

pθ(

J

D

z)N/Bpθ(z)

|

We have (appealing to the convexity of the KL divergence)

pAgg
θ

(z

)

|D

≡

Eq(J ) [pθ(z

J )]

|D

KL(qfwd(zK)
|

pAgg
θ

(z

|D

)) = KL(Eq(J ) [qfwd(zK
Eq(J ) [KL(qfwd(zK

≤
=

(K −1/2)

O

)]

Eq(J ) [pθ(zK
|
J ))]
pθ(zK
|

|D

)

|J

|J

(34)

(35)

(36)

(37)

(38)

J )])

|D

where in the last line we used that q(
et al. (2021) to conclude that for each
known, convergence in KL divergence implies convergence w.r.t. the total variation distance.

) has ﬁnite support and where we have appealed to the convergence result in Zhang
(K −1/2). We note that, as is well
we have that KL(qfwd(zK

J )) =

pθ(zK

J
J

)
|

|D

|J

O

11Note that these are the kernels that were used in the proof in Sec. A.4.

Surrogate Likelihoods for Variational Annealed Importance Sampling

Algorithm 3 DAIS: Differentiable Annealed Importance Sampling. We provide a complete description of DAIS (Geffner &
Domke, 2021; Zhang et al., 2021) in our notation.

Input: model log prior density Ψ0(z), model log likelihood ΨL(
D
βk
steps K, inverse temperatures
, step size η, momentum refresh parameter γ, mass matrix M,
}
(0, M),
Initialize: z0
∼
for k = 1 to K do
zk−1 + η

log q0(z0)

L ← −

q0, v0

ˆzk

{

, z), base variational distribution q0(z), number of

∼ N
2 M−1vk−1
βk(Ψ0(z)+ΨL(
{

←

gk

z
← ∇

, z))+(1

βk) log q0(z)
}

−

D

(cid:12)
(cid:12)
(cid:12)
(cid:12)z=ˆzk

ˆvk
vk−1 + ηgk
ˆzk + η
zk
if k < K then

←
←

2 M−1 ˆvk

γ ˆvk + (cid:112)1

vk
end if

←

γ2ε, ε

−

∼ N

(0, M)

+ log

N

(ˆvk, M)

log

N

−

(vk−1, M)

L ← L

end for
Return:

+ Ψ0(zK) + ΨL(

D

L

, zK)

A.6. SL-DAIS

Before we turn to a proof of Proposition 3 in the main text, we give a more formal description of SL-DAIS. The forward and
backward kernels are deﬁned as:

qfwd(z0:K, v0:K,

I
qbwd(z0:K, v0:K,

) = q(

I
) = q(

I

)q0(z0)q0(v0)(cid:81)K
)pθ(zK

k(zk, vk
k=1T
I)N/Bpθ(zK)(cid:81)K

k=1

|D

zk−1, vk−1, ˆΨL)
|
˜
k(zk−1, vk−1
T

|

zk, vk, ˆΨL)

and the corresponding ELBO is given by

SL−DAIS

L

where

Eqfwd

≡
= Eq(cid:48)

fwd

I

(cid:104)

(cid:104)

log qbwd(z0:K, v0:K,

I

log q(cid:48)

bwd(z0:K, v0:K)

−

)

log qfwd(z0:K, v0:K,
−
(cid:105)
log qfwd(z0:K, v0:K)

(cid:105)
)

I

(39)

fwd(z0:K, v0:K) = q0(z0)q0(v0)(cid:81)K
q(cid:48)
q(cid:48)
bwd(z0:K, v0:K) = pθ(zK

k=1T
)pθ(zK)(cid:81)K

k(zk, vk
˜
k(zk−1, vk−1
|D
T
mini-batch index.

k=1

|

zk−1, vk−1, ˆΨL)

zk, vk, ˆΨL)
|

are the forward and backward kernels without the

I

We now provide a proof of Proposition 3 in the main text. First we restate some of the equations and notation from the main
obs),
text. We consider Bayesian linear regression with a prior pθ(z) =
X(cid:62)y)
where each yn
and Λpost = Λ0 + 1
σ2

N
X(cid:62)X. Throughout we work under the following set of simplifying assumptions:

0 ) and a likelihood (cid:81)
post) where µpost = Λ−1

RD. The exact posterior is given by

(yn
post(Λ0µ0+ 1
σ2

(µ0, Λ−1
N
(µpost, Λ−1

R and xn

xn, σ2

n N

z
|

∈

∈

obs

·

obs

Assumption 1 We use full momentum refreshment (γ = 0), equally spaced inverse temperatures
varies as η

K −1/4, and the prior as the base distribution (i.e. q0(z)

pθ(z)).

, a step size that

βk

{

}

∼

→

(˜µpost, ˜Λpost) that corresponds to the surrogate log likelihood
Next we deﬁne the parameters of the ‘surrogate posterior’
ˆΨL(z) speciﬁed by δa and δB, where δa and δB control the degree to which ˆΨL(z) is incorrectly speciﬁed. Given our
parameterization

N

it is easy to verify that

∇

z ˆΨL(z) = a + δa

(B + δB)z

−

˜Λpost = Λ0 + B + δB = Λpost + δB
post (Λpostµpost + δa)

post (Λ0µ0 + a + δa) = ˜Λ−1

˜µpost = ˜Λ−1

(40)

(41)

Surrogate Likelihoods for Variational Annealed Importance Sampling

Expanding these expressions in powers of δa and δB we have:

These expressions also imply that

˜µpost

≈

˜Λ−1

post ≈
µpost + Λ−1

Λ−1

post −

postδa

Λ−1
Λ−1

postδBΛ−1
post
postδBµpost

−

det ˜Λpost = det Λpost det (cid:0)1D

Λ−1

postδB(cid:1)

−

≈

det Λpost

(cid:0)1 + TrΛ−1

postδB(cid:1)

which yields

Λ−1
post

˜Λpost

log

|

| ≈

TrΛ−1

postδB.

(42)

(43)

(44)

Next we restate some results from Sec. 4.1 and the supplementary materials in Zhang et al. (2021), suitably adapting them to
our setting and notation. Where appropriate we use subscripts/superscripts to distinguish DAIS and SL-DAIS terms.

Lemma 2 In the case of Bayesian linear regression under Assumption 1 each of the K + 1 random variables
zk
{
enter DAIS are normally distributed as zk
k ). If we run DAIS for K steps the ELBO gap is given by

(µk, Λ−1

∼ N

log p(

)

DAIS =

− L

D
1
2 (cid:107)
(cid:124)

µK

+

2
Λpost
(cid:125)

(cid:107)

1
2
(cid:124)

−

µpost
(cid:123)(cid:122)
(I)

Tr(ΛpostΛ−1
K )

(cid:123)(cid:122)
(II)

−

+

D
2
(cid:125)

1
2
(cid:124)

Λ−1

postΛ0

log

|

| −

Eqfwd

(cid:34) K
(cid:88)

k=1

(ˆvk, M)
(vk−1, M)

log N
N

(cid:35)

(cid:125)

(cid:123)(cid:122)
(III)

that

}

(45)

||

−

µK

µpost

2
Λpost
||

where
in Zhang et al. (2021), this equation also holds for the gap log p(
kernel qfwd into the expectation in (III) and µk and Λk are deﬁned in terms of the
SL-DAIS12 we have that

µpost). Moreover, as is evident from the derivation in Sec. B.2
SL−DAIS if we substitute the SL-DAIS forward
that enter SL-DAIS. Additionally for

µpost)TΛpost(µK

= (µK

zk
{

− L

−

−

D

}

)

µSL−DAIS
(cid:107)

K

−

˜µpost

(cid:107)

=

(K −1/2)

O
1
2

log

˜Λ−1

postΛ0

|

| −

Eqfwd

(ΛSL−DAIS
K
(cid:107)
(cid:34) K
(cid:88)

log N
N

k=1

)−1

˜Λ−1

post(cid:107)
(cid:35)

−
(ˆvk, M)
(vk−1, M)

(K −1/2)

(K −1/2)

=

=

O

O

With these ingredients we can now proceed with the proof. From Eqn. 42 and Eqn. 46 we have

µSL−DAIS
(cid:107)

K

−

µpost

µSL−DAIS

K

(cid:107) ≤ (cid:107)

˜µpost

+

(cid:107)

(cid:107)

˜µpost

−

−

µpost

=

(cid:107)

O

(K −1/2) +

(
(cid:107)

) +
δa
(cid:107)

(

).
δB
(cid:107)
(cid:107)

O

O

Hence for SL-DAIS we have that

(I) =

Once again appealing to Eqn. 42 and Eqn. 46 we have

(K −1/2) +

δa
(
(cid:107)

2 +
(cid:107)

2).
δB
(cid:107)
(cid:107)

O

O

(46)

(47)

(48)

Tr(Λpost(ΛSL−DAIS

K

)−1)

−

D = Tr(Λpost((ΛSL−DAIS
= Tr(Λpost((ΛSL−DAIS
(K −1/2) + TrΛ−1

K

K

postδB

≈ O

)−1
)−1

Λ−1
˜Λ−1

post))
post)) + Tr(Λpost( ˜Λ−1

post −

−

−

so that we get the following estimate for the second term of Eqn. 45:

(II) =

O

(K −1/2) +

1
2

TrΛ−1

postδB

Finally, from Eqn. 44 and Eqn. 46 we have

(III) =

1
2

log

Λ−1
post
|

˜Λpost

+

|

O

(K −1/2) =

1
2

TrΛ−1

postδB +

(K −1/2)

O

Λ−1

post))

(49)

(50)

(51)

which ends the ﬁrst part of the proof for Proposition 3. The fact that the KL divergence between qfwd(zK) and the posterior
pθ(zK
) is bounded by the same quantity easily follows from standard arguments, see e.g. the proof of Proposition 1 in
|D
Sec. A.4.

12This is evident from the proof of Lemma 1 in Zhang et al. (2021). The analogous result is also true for DAIS but we do not require

this for the proof.

Surrogate Likelihoods for Variational Annealed Importance Sampling

A.7. Experimental details

All experiments use 64-bit ﬂoating point precision. We use the Adam optimizer with default momentum hyperparameters in
105 optimization steps with an initial learning rate
all experiments (Kingma & Ba, 2014). In all optimization runs we do 3
of 10−3 that drops to 10−4 and 10−5 at iteration 105 and 2
105, respectively. Similar to Geffner & Domke (2021) we
parameterize the step size ηk in the kth iteration of DAIS/NS-DAIS/SL-DAIS as

×

×

ηk = clip(˜η + κβk, min = 0, max = ηmax)

(52)

where ˜η and κ are learnable parameters and we choose ηmax = 0.25. The inverse temperatures
are parameterized
using the exponential transform to enforce positivity and a cumulative summation to enforce monotonicity. For SL-DAIS the
learnable weights ω are uniformly initialized so that their total weight is equal to the number of data points, i.e. (cid:80)
n ωn = N .
10−2, and initialize κ to
In all experiments we use a diagonal mass matrix M. We initialize ˜η to be small, e.g. ˜η
κ = 0. We initialize γ to γ = 0.9. Unless noted otherwise (see GP models), we learn all parameters that deﬁne the model
and variational distribution jointly in a single optimization run, i.e. without any kind of pre-training (in contrast to Geffner &
Domke (2021)). We found that this generally worked better, although in a few cases pre-training led to better results when
the base distribution was a multivariate Normal.

βk
{

10−4

−

∼

}

A.7.1. SURROGATE LOG LIKELIHOODS

All methods use K = 8. We use 50k MC samples to estimate ELBOs after training. The training set has 50k data points and
all replications are for the same training set. We consider eight different strategies for deﬁning a surrogate log likelihood in
the case of logistic regression:

1. RAND: We randomly choose Nsurr surrogate data points

(˜yn, ˜xn)

, introduce a Nsurr-dimensional vector of

learnable weights ω, and let ˆΨL(z) = (cid:80)

n ωn log p(˜yn

{
z, ˜xn).
|

} ⊂ D

2. RAND

: We randomly choose Nsurr surrogate covariates
learnable weights, ω− and ω+. We then write ˆΨL(z) = (cid:80)

±

˜xn
{
z, ˜xn) + (cid:80)
n ω+
n log p(yn = 1
|

and introduce two Nsurr-dimensional vectors of
n ω−
z, ˜xn).
n log p(yn = 0
|

}

3. CS-INIT: We use a Bayesian coreset algorithm (Huggins et al., 2016) to choose Nsurr surrogate data points, introduce

a Nsurr-dimensional vector of learnable weights ω, and let ˆΨL(z) = (cid:80)

n ωn log p(˜yn

z, ˜xn).
|

4. CS-INIT

: We proceed as in CS-INIT but ignore the

˜yn

±
two Nsurr-dimensional vectors of learnable weights, ω− and ω+. We then write ˆΨL(z) = (cid:80)
z, ˜xn) + (cid:80)
1
|

z, ˜xn).
n log p(yn = 0
|

n ω−

}

{

returned by the coreset algorithm and instead introduce
n log p(yn =

n ω+

5. CS-FIX: As in CS-INIT we use a coreset algorithm to choose the surrogate data points but instead of learning ω we

use the weights provided by the coreset algorithm.

6. KM-INIT

: We use k-means++ to extract Nsurr cluster centroids from the set of N covariates in
±
two Nsurr-dimensional vectors of learnable weights, ω− and ω+. We then write ˆΨL(z) = (cid:80)
z, ˜xn) + (cid:80)
1
|

z, ˜xn) and treat the surrogate covariates
n log p(yn = 0
|

D
n ω+
as learnable parameters.

˜xn
{

n ω−

}

and introduce
n log p(yn =

7. KM-FIX
ﬁxed.

±

: We proceed as in KM-INIT

±

except the surrogate covariates returned by the clustering algorithm remain

8. NN: We parameterize ˆΨL(z) as a neural network with two hidden layers, ELU non-linearities, and 100 neurons per

hidden layer.

A.7.2. CLASSIFYING IMBALANCED DATA

We set Nsurr = B = 256 and K = 8. We use the SUSY dataset with 20k data points held out for testing. The rare class
corresponds to signal events. For each ratio of rare to non-rare class we keep the dataset ﬁxed so that replications only differ
with respect to the random number seed that controls optimization.

Surrogate Likelihoods for Variational Annealed Importance Sampling

Dataset
Nsurr

Higgs
64

RAND
211.1 ± 1.1
RAND±
214.3 ± 1.0
CS-INIT
209.6 ± 1.2
CS-INIT± 213.5 ± 0.9
CS-FIX
83.0 ± 12.3
KM-INIT± 184.0 ± 2.5
KM-FIX±
218.3 ± 1.3
NN
16.8 ± 0.9

256

1024

214.2 ± 1.3
217.0 ± 1.5
213.8 ± 1.1
214.5 ± 1.4
136.6 ± 12.6
187.4 ± 3.0
217.7 ± 1.4

222.4 ± 1.1
217.8 ± 1.5
218.5 ± 1.8
219.0 ± 1.7
192.7 ± 5.5
182.4 ± 2.8
217.4 ± 1.8

SUSY
64

627.5 ± 2.1
635.0 ± 3.0
625.9 ± 2.2
633.2 ± 3.7
501.0 ± 36.2
222.3 ± 24.0
635.5 ± 3.1
2.6 ± 1.4

256

1024

638.9 ± 9.7
637.9 ± 4.0
645.3 ± 2.7
634.8 ± 5.5
538.5 ± 14.2
188.3 ± 11.4
633.9 ± 2.6

637.2 ± 2.2
632.2 ± 3.8
633.7 ± 3.8
637.4 ± 3.6
555.5 ± 15.8
169.7 ± 13.9
632.1 ± 2.5

Table 6: This table is an expanded version of Table 1 in Sec 7.1. We compare eight different parameterizations for the
surrogate log likelihood ˆΨL. We consider two logistic regression datasets. For the seven strategies that make use of surrogate
64, 256, 1024
data points, we vary Nsurr
. We report ELBO improvements (in nats) above a mean-ﬁeld Normal baseline.
}
See Sec. 7.1.

∈ {

A.7.3. LOGISTIC REGRESSION

When running HMCECS (Dang et al., 2019), we use the ‘perturbed’ approach with a NUTS kernel and use a control variate
based on a second-order Taylor approximation around the maximum a posteriori estimate. In addition, to facilitate mixing
we use block pseudo-marginal updates for data subsample indices with 25 blocks (Tran et al., 2016). We run for a total of
103 samples by a factor of 10, and
15
use these 103 thinned samples to compute posterior predictive distributions on held-out test data. For NUTS we use the
default settings in NumPyro, in particular a diagonal mass matrix that is adapted during warm-up and a maximum tree depth
of 10.

103 samples for burn-in, thin the remaining 10

103 iterations, discard the ﬁrst 5

×

×

×

For NUTS-CS we identify 1024 coresets using the coreset algorithm described in Huggins et al. (2016). Note that in most
cases the total number of coresets returned by the algorithm is somewhat less than 1024, e.g. 1009 or 1017. We use the
settings recommended in Huggins et al. (2016). In particular we use kmeans++ to construct a clustering with k = 6 clusters.
We set the hyperparameter a to a = 6. We then run NUTS Hoffman et al. (2014) on a logistic regression model conditioned
on the weighted coresets using the same number of NUTS iterations, mass matrix, etc., as for HMCECS.

The Higgs and SUSY datasets are described in (Baldi et al., 2014) and available from the UCI repository (Asuncion &
Newman, 2007). The MiniBooNE dataset is from (Roe et al., 2005) and is likewise available from UCI. For the two CovType
datasets (Blackard & Dean, 1999), which are also vailable from UCI, we reduce the 7-way classiﬁcation problem to two
binary classiﬁcations problems: i) ﬁr versus rest; and ii) pine versus rest.

A.7.4. GAUSSIAN PROCESS CLASSIFICATION

We consider a Gaussian process model for binary classiﬁcation with a logistic link function. The covariates are assumed to
Rd. To make GP inference scalable we use the FITC approximation (Snelson & Ghahramani, 2006;
be d-dimensional, xn
Quinonero-Candela & Rasmussen, 2005), which results in a model density of the following form:

∈

p(

0, KZZ)
, f , u) = p(u
|

D

(cid:89)

n

p(yn

|

σ(fn))p(fn

mn(u), vn)

|

∈

RM ×d are the M inducing point locations, K refers to the RBF kernel parameterized by a (scalar) kernel scale
where Z
RM
and a d-dimensional vector of length scales, σ(
are the inducing point values, fn is the Gaussian process function value corresponding to covariate xn, and mn(u) and vn
are given by:

) is the sigmoid function, KZZ
·

RM ×M is the prior covariance, u

∈

∈

mn(u) = kT

nZK−1

ZZu

vn = k(xn, xn)

nZK−1
kT

ZZknZ

−

(53)

RM with (knZ)m = k(xn, zm) for m = 1, ..., M , where k(
·
are uncorrelated with one another, we can approximately integrate out

) is the kernel function. Since by construction
Here knZ
·
fn
the
using Gauss-Hermite quadrature using
{
Q quadrature points. In all our experiments we use Q = 16, M = 128, and Nsurr = 512. This results in a model density of

fn
{

∈

}

}

,

Surrogate Likelihoods for Variational Annealed Importance Sampling

Figure 4: In this companion ﬁgure to Fig. 2 we report full results for the logistic regression experiment in Sec. 7.3. Circles
denote variational methods and squares denote MCMC methods. Metrics are averaged over 7 independent replications
(except for DAIS and HMCECS where we do 3 independent replications), and error bars denote standard errors. Note that
these ﬁgures include an additional method, namely NS(cid:48)-DAIS. This method is identical to NS-DAIS except that it utilizes
independently drawn mini-batches of data for each evaluation of the annealed potential energy Vk in Eqn. 33. We note that
NS(cid:48)-DAIS outperforms NS-DAIS but is inferior to SL-DAIS.

the form

p(

D

, u) = p(u
|

0, KZZ)

(cid:89)

n

p(yn

|

u, xn)

(54)

(N M 2 + M 3 + N Q). We use variational inference to infer
where the cost of computing N likelihoods p(yn
M and the cost of computing the full
an approximate posterior over u for the model density in Eqn. 54. In practice N
(N M 2) term. In all our Gaussian process experiments we initialize the inducing point
likelihood is dominated by the
locations Z with k-means and treat them as learnable parameters. We use a mini-batch size B = 128. After training ELBOs
are estimated with 20k MC samples. The MVN base distribution of SL-DAIS is initialized using the result from the MVN
baseline.

u, xn) is

(cid:29)

O

O

|

A.7.5. ROBUST GAUSSIAN PROCESS REGRESSION

We proceed exactly as in Sec. A.7.4 except we use a Student’s t likelihood p(yn
ν, fn, σobs).
|
Here ν > 0 is the overdispersion parameter, with small ν corresponding to large overdispersion. We constrain ν to satisfy
ν > 2, which is equivalent to requiring that the observation noise have ﬁnite variance.

ν, fn, σobs) = StudentT(yn
|

The Precipitation dataset is available from the IRI/LDEO Climate Data Library.13 This spatio-temporal dataset represents
the ‘WASP’ index (Weighted Anomaly Standardized Precipitation) at various latitudes and longitudes. Each data point

13http://iridl.ldeo.columbia.edu/maproom/Global/Precipitation/WASP_Indices.html

0500∆ELBOSL-DAIS-MVN-8SL-DAIS-MF-8NS’-DAIS-MF-8NS-DAIS-MF-8DAIS-MF-8DAIS-MF-2FlowMVNSUSY(D=18)100200∆ELBOHiggs(D=28)0500∆ELBOMiniBooNE(D=51)250500750∆ELBOCovType-Fir(D=55)250500750∆ELBOCovType-Pine(D=55)−0.49−0.48LoglikelihoodSL-DAIS-MVN-8SL-DAIS-MF-8NS’-DAIS-MF-8NS-DAIS-MF-8FlowMVNMFDAIS-MF-8DAIS-MF-2HMCECSNUTS-CSSUSY(D=18)−0.65−0.64LoglikelihoodHiggs(D=28)−0.32−0.30LoglikelihoodMiniBooNE(D=51)−0.495−0.490−0.485LoglikelihoodCovType-Fir(D=55)−0.55−0.54LoglikelihoodCovType-Pine(D=55)7778AccuracySL-DAIS-MVN-8SL-DAIS-MF-8NS’-DAIS-MF-8NS-DAIS-MF-8FlowMVNMFDAIS-MF-8DAIS-MF-2HMCECSNUTS-CSSUSY(D=18)6264AccuracyHiggs(D=28)8788AccuracyMiniBooNE(D=51)77.578.078.5AccuracyCovType-Fir(D=55)77.578.078.5AccuracyCovType-Pine(D=55)Surrogate Likelihoods for Variational Annealed Importance Sampling

ELBO
Log likelihood
Accuracy

MF MVN Flow NS-DAIS-MF
6.60
2.08
6.16
2.16
5.48
1.96

5.36
5.88
5.24

5.00
4.60
4.52

SL-DAIS-MF NS-DAIS-MVN SL-DAIS-MVN
2.60
2.44
3.40

2.20
2.40
3.28

4.16
4.36
4.12

Table 7: We report performance ranks w.r.t. three metrics across 5 train/test splits and 5 datasets for the logistic regression
experiment in Sec. 7.3. Lower is better for all metrics. The rank satisﬁes 1
7, since we compare 7 scalable
variational methods.

rank

≤

≤

corresponds to the WASP index for a given year (which itself is the average of monthly WASP indices). We use the data
from 2010, resulting in a dataset with 10127 data points. In each train/test split we randomly choose N = 8000 data points
for training and use the remaining 2127 data points for testing.

We use a mini-batch size B = 128. We estimate the ﬁnal ELBO using 5000 samples and test log likelihoods and accuracies
using 1000 posterior samples. We use Nsurr = 256 surrogate data points with the RAND surrogate parameterization (see
Sec. A.7.1). We use an initial step size ηinit = 10−4 and initialize SL-DAIS using the model and variational parameters
obtained with a MVN variational distribution.

Figure 5: We report the time per optimization step for each variational method in Fig. 2 on the CovType-Fir dataset. We
compare CPU runtime (24 cores; Intel Xeon Gold 5220R 2.2GHz) to GPU runtime (NVIDIA Tesla K80). We note that the
runtime comparison against DAIS would be increasingly favorable to NS-DAIS and SL-DAIS as the size of the dataset
increases.

A.7.6. LOCAL LATENT VARIABLE MODELS

We use the Pol UCI dataset with 15k training and 15k test data points. We use 10k MC samples to estimate ELBOs after
training. We set B = 256.

A.8. Additional ﬁgures and tables

For additional results pertaining to the logistic regression experiment in Sec. 7.3 see Fig. 4, Fig. 5, and Table 7. For an
expanded version of the table in Sec. 7.1 see Table 6.

0.1110100Gradientsteptime(ms)SL-DAIS-MVN-8SL-DAIS-MF-8NS-DAIS-MF-8DAIS-MF-8DAIS-MF-2FlowMVNMFCPU0.1110100Gradientsteptime(ms)GPU