1
2
0
2

p
e
S
8
2

]

G
L
.
s
c
[

1
v
5
9
8
3
1
.
9
0
1
2
:
v
i
X
r
a

Symbolic Regression by Exhaustive Search –
Reducing the Search Space using Syntactical
Constraints and Efﬁcient Semantic Structure
Deduplication.

Lukas Kammerer and Gabriel Kronberger and Bogdan Burlacu and Stephan M.
Winkler and Michael Kommenda and Michael Affenzeller

Abstract Symbolic regression is a powerful system identiﬁcation technique in in-
dustrial scenarios where no prior knowledge on model structure is available. Such
scenarios often require speciﬁc model properties such as interpretability, robust-
ness, trustworthiness and plausibility, that are not easily achievable using standard
approaches like genetic programming for symbolic regression. In this chapter we
introduce a deterministic symbolic regression algorithm speciﬁcally designed to ad-
dress these issues. The algorithm uses a context-free grammar to produce models
that are parameterized by a non-linear least squares local optimization procedure. A
ﬁnite enumeration of all possible models is guaranteed by structural restrictions as
well as a caching mechanism for detecting semantically equivalent solutions. Enu-
meration order is established via heuristics designed to improve search efﬁciency.
Empirical tests on a comprehensive benchmark suite show that our approach is com-
petitive with genetic programming in many noiseless problems while maintaining
desirable properties such as simple, reliable models and reproducibility.

Key words: symbolic regression, grammar enumeration, graph search

e-mail:

lukas.kammerer@fh-hagenberg.at

Lukas Kammerer1,2,3
· Gabriel
Kronberger1,3 · Bogdan Burlacu1,3 · Stephan M. Winkler1,2 · Michael Kommenda1,3 · Michael
Affenzeller1,2
1 Heuristic and Evolutionary Algorithms Laboratory (HEAL), University of Applied Sciences
Upper Austria, Softwarepark 11, 4232 Hagenberg, Austria
2 Department of Computer Science, Johannes Kepler University, Altenberger Straße 69, 4040
Linz, Austria
3 Josef Ressel Center for Symbolic Regression, University of Applied Sciences Upper Austria,
Softwarepark 11, 4232 Hagenberg, Austria

The ﬁnal publication is available at https://link.springer.com/chapter/10.

1007%2F978-3-030-39958-0_5.

1

 
 
 
 
 
 
2

1 Introduction

Kammerer et al.

Symbolic regression is a task that we can solve with genetic programming (GP)
and a common example where GP is particularly effective in practical applications.
Symbolic regression is a machine learning task whereby we try to ﬁnd a mathemat-
ical model represented as a closed-form expression that captures dependencies of
variables from a dataset. Genetic programming has been proven to be well-suited
for this task especially when there is little knowledge about the data-generating pro-
cess. Even when we have a good understanding of the underlying process, GP can
identify counterintuitive or unexpected solutions.

1.1 Motivation

GP has some practical limitations when used for symbolic regression. One limita-
tion is that—as a stochastic process—it might produce highly dissimilar solutions
even for the same input data. This can be very helpful to produce new “creative”
solutions. However, it is problematic when we try to integrate symbolic regression
in carefully engineered solutions (e.g. for automatic control of production plants).
In such situations we would hope that there is an optimal solution and the solu-
tion method guarantees to identify the optimum. Intuitively, if the data changes only
slightly, we expect that the optimal regression solution also changes only slightly.
If this is the case we know that the solution method is trustworthy (cf. [15, 31]) and
we can rely on the fact that the solutions are optimal at least with respect to the
objective function that we speciﬁed. Of course this is only wishful thinking because
of three fundamental reasons: (1) the symbolic regression search space is huge and
contains many different expressions which are algebraically equivalent, (2) GP has
no guarantee to explore the whole search space with reasonable computational re-
sources and (3) the ”optimal solution” might not be expressible as a closed-form
mathematical expressions using the given building blocks. Therefore, the goal is to
ﬁnd an approximately optimal solution.

1.2 Prior Work

Different methods have been developed with the aim to improve the reliability of
symbolic regression. Currently, there are several off-the-shelf software solutions
which use enhanced variants of GP and are noteworthy in this context: the Data-
Modeler package1 [16] provides extensive capabilities for symbolic regression on
top of Mathematica™. Eureqa™ is a commercial software tool2 for symbolic re-

1 http://www.evolved-analytics.com/
2 https://www.nutonian.com/products/eureqa/

Symbolic Regression by Exhaustive Search

3

gression based on research described in [27, 28, 29]. The open-source framework
HeuristicLab3 [36] is a general software environment for heuristic and evolution-
ary algorithms with extensive functionality for symbolic regression and white-box
modeling.

In other prior work, several researchers have presented non-evolutionary solution
methods for symbolic regression. Fast function extraction (FFX) [22] is a determin-
istic method that uses elastic-net regression [39] to produce symbolic regression
solutions orders of magnitudes faster than GP for many real-world problems. The
work by Korns toward “extremely accurate” symbolic regression [12, 13, 14] high-
lights the issue that baseline GP does not guarantee to ﬁnd the optimal solution even
for rather limited search spaces. They give a useful systematic deﬁnition of increas-
ingly larger symbolic regression search spaces using abstract expression grammars
[10] and describes enhancements to GP to improve it’s reliability. The work by
Worm and Chiu on prioritized grammar enumeration [38] is closely related. They
use a restricted set of grammar rules for deriving increasingly complex expressions
and describe a deterministic search algorithm, which enumerates the search space
for limited symbolic regression problems.

1.3 Organization of this Chapter

Our contribution is conceptually an extension of prioritized grammar enumeration
[38], although our implementation of the method deviates signiﬁcantly. The most
relevant extensions are that we cut out large parts of the search space and provide
a general framework for integrating heuristics in order to improve the search efﬁ-
ciency. Section 2 describes how we reduce the size of the search space which is
deﬁned by a context-free grammar:

1. We restrict the structure of solution to prevent too complicated solutions.
2. We use grammar restrictions to prevent semantic duplicates—solutions with dif-
ferent syntax but same semantics, such as algebraic transformations. With these
restrictions, most solutions can only be generated in exactly one way.

3. We efﬁciently identify remaining duplicates with semantic hashing, so that

(nearly) all solutions in the search space are semantically unique.

In Section 3, we explain the algorithm that iterates all these semantically unique
solutions. The algorithm sequentially generates solutions from the grammar and
keeps track of the most accurate one. For very small problems, it is even feasible to
iterate the whole search space [19]. However, our goal in larger problems is to ﬁnd
accurate and concise solutions early during the search and to stop the algorithm after
a reasonable time. The search order is determined with heuristics, which estimate the
quality of solutions and prioritize promising ones in the search. A simple heuristic
is proposed in Section 4. Modeling results in Section 5 show that this ﬁrst version
of our algorithm can already solve several difﬁcult noiseless benchmark problems.

3 https://dev.heuristiclab.com

4

Kammerer et al.

2 Deﬁnition of the Search Space

The search space of our deterministic symbolic regression algorithm is deﬁned by a
context-free grammar. Production rules in the grammar deﬁne the mathematical ex-
pressions that can be explored by the algorithm. The grammar only speciﬁes possi-
ble model structures whereby placeholders are used for numeric coefﬁcients. These
are optimized separately by a curve-ﬁtting algorithm (e.g. optimizing least squares
with an gradient-based optimization algorithm) using the available training data.

In a general grammar for mathematical expressions—as it is common in sym-
bolic regression with GP for example—the same formula can be derived in several
forms. These duplicates inﬂate the search space. To reduce them, our grammar is
deliberately restricted regarding the possible structure of expressions. Remaining
duplicates that cannot be prevented by a context-free grammar are eliminated via a
hashing algorithm. Using both this grammar and hashing, we can generate a search
space with only semantically unique expressions.

2.1 Grammar for Mathematical Expressions

In this work we consider mathematical expressions as list of symbols which we call
phrases or sentences. A phrase can contain both terminal and non-terminal symbols
and a sentence only terminal symbols. Non-terminal symbols can be replaced by
other symbols as deﬁned by a grammar’s production rules while terminal symbols
represent parts of the ﬁnal expression like functions or variables in our case.

Our grammar is very similar to the one by Kronberger et al. [19]. It produces
only rational polynomials which may contain linear and nonlinear terms, as outlined
conceptually in Equation 1. The basic building blocks of terms are linear and non-
linear functions {+, ×, inv, exp, log, sin, square root, cube root}. Recursion in the
production rules represents a strategy for generating increasingly complex solutions
by repeated nesting of expressions and terms.

Expr = c1Term1 + c2Term2 + . . . + cn
Term = Factor0 × Factor1 × . . .

(1)

Factor ∈ {variable, log(variable), exp(variable), sin(variable)}

We explicitly disallow nested non-linear functions, as we consider such solu-
tions too complex for real-world applications. Otherwise, we allow as many differ-
ent structures as possible to keep accurate and concise models in the search space.
We prevent semantic duplicates by generating just one side of mathematical equality
relations in our grammar, e.g. we allow xy + xz but not x(y + z). Since each function
has different mathematical identities, many different production rules are necessary
to cover all special cases. Because we scale every term including function argu-

Symbolic Regression by Exhaustive Search

5

ments, we also end up with many placeholders for coefﬁcients in the structures. All
production rules are detailed in Listing 1 and described in the following.

Listing 1 Context-free grammar for generating mathematical expressions
G(Expr):
// Expressions and terms for polynomial structure

Expr

-> "const" "*" Term "+" Expr

"const" "*" Term "+" "const"

Term

-> RecurringFactors "*" Term

RecurringFactors
OneTimeFactors

|

|
|

RecurringFactors -> VarFactor | LogFactor |

ExpFactor | SinFactor

VarFactor
LogFactor
ExpFactor
SinFactor

-> <variable>
-> "log" "(" SimpleExpr ")"
-> "exp" "(" "const" "*" SimpleTerm ")"
-> "sin" "(" SimpleExpr ")"

// Factors which can occur at most once per term

OneTimeFactors -> InvFactor "*" SqrtFactor "*" CbrtFactor |
|
InvFactor "*" SqrtFactor
CbrtFactor |
InvFactor "*"
SqrtFactor "*" CbrtFactor |
|
|

SqrtFactor

InvFactor

CbrtFactor

-> "1/" "(" InvExpr ")"

InvFactor
SqrtFactor -> "sqrt" "(" SimpleExpr ")"
CbrtFactor -> "cbrt" "(" SimpleExpr ")"

// Function arguments

SimpleExpr -> "const" "*" SimpleTerm "+" SimpleExpr

|

"const" "*" SimpleTerm "+" "const"

SimpleTerm -> VarFactor "*" SimpleTerm | VarFactor

InvExpr -> "const" "*" InvTerm "+" InvExpr |

"const" "*" InvTerm "+" "const"

InvTerm -> RecurringFactors "*" InvTerm

|

RecurringFactors "*" SqrtFactor "*" CbrtFactor |
RecurringFactors "*" SqrtFactor
|
CbrtFactor |
RecurringFactors "*"
SqrtFactor "*" CbrtFactor |
|
|

RecurringFactors

SqrtFactor

We use a polynomial structure as outlined in Equation 1 to prevent a factored
form of solutions. The polynomial structure is enforced with the production rules

CbrtFactor

6

Kammerer et al.

Expr and Term. We restrict the occurrence of the multiplicative inverse (= 1
... ), the
square root and cube root function to prevent a factored form such as 1
1
x+z . This is
x+y
necessary since we want to allow sums of simple terms as function arguments (see
non-terminal symbol SimpleExpr). Therefore, these three functions can occur
at most once time per term. This is deﬁned with symbol OneTimeFactors and
one production rule for each combination. The only function in which we do not
allow sums as argument is exponentiation (see ExpFactor), since this form is
substituted by the overall polynomial structure (e.g. we allow exey but not ex+y).
Equation 2 shows some example identities and which forms are supported.

in the search space:

not in the search space:

c1xy + c2xz + c3 ≡ x(c4y + c5z) + c6

c1

1
c2x + c3xx + c4xy + c5y + c6

+ c7 ≡ c8

1
c9x + c10

1
c11x + c12y + c13

+ c14

(2)

c1 exp(c2x) exp(c3y) + c4 ≡ c5 exp(c6x + c7y) + c8

We only allow (sums of) terms of variables as function arguments, which we
express with the production rules SimpleExpr and SimpleTerm. An exception
is the multiplicative inverse, in which we want to include the same structures as
in ordinary terms. However, we disallow compound fractions like in Equation 3.
Again, we introduce separate grammar rules InvExpr and InvTerm which cover
the same rules as Term except the multiplicative inverse.

in the search space:

not in the search space:

c1

1
c2 log(c3x + c4) + c5

+ c6 ≡ c7

1

c8

1
c9 log(c10x+c11)+c12

+ c13

(3)

+ c14

In the simplest case, the grammar produces an expression E0 = c0x + c1, where
x is a variable and c0 and c1 are coefﬁcients corresponding to the slope and inter-
cept. This expression is obtained by considering the simplest possible Term which
corresponds to the derivation chain Expr → Term → RecurringFactors →
VarFactor → x. Further derivations could lead for example to the expression
E1 = c0x + (c1x + c2), produced by nesting E0 into the ﬁrst part of the production
rule for Expr, where the Term is again substituted with the variable x.

However, duplicate derivations can still occur due to algebraic properties like as-
sociativity and commutativity. These issues cannot be prevented with a context-free
grammar because a context-free grammar does not consider surrounding symbols of
the derived non-terminal symbol in its production rules. For example the expression
E1 = c0x + (c1x + c2) contains two coefﬁcients c0 and c1 for variable x which could
be folded into a new coefﬁcient cnew = c0 + c1. This type of redundancy becomes
even more pronounced when VarFactor has multiple productions (corresponding
to multiple input variables), as it becomes possible for multiple derivation paths to

Symbolic Regression by Exhaustive Search

7

produce different expressions which are algebraically equivalent, such as c1x + c2y,
c3x + c4x + c5y, c6y + c7x for corresponding values of c1...c7. Another example are
c1xy and c2yx which are both equivalent but derivable from the grammar.

To avoid re-visiting already explored regions of the search space, we implement a
caching strategy based on expression hashing for detecting algebraically equivalent
expressions. The computed hash values are the same for algebraically equivalent ex-
pressions. In the search algorithm we keep the hash values of all visited expressions
and prevent re-evaluations of expressions with identical hash values.

2.2 Expression Hashing

We employ expression hashing by Burlacu et al. [3] to assign hash values to subex-
pressions within phrases and sentences. Hash values for parent expressions are ag-
gregated in a bottom-up manner from the hash values of their children using any
general-purpose hash function. We then simplify such expressions according to
arithmetic properties such as commutativity, associativity, and applicable mathe-
matical identities. The resulting canonical minimal form and associated hash value
are then cached in order to prevent duplicated search effort.

Root = “ + ”
H = ⊕(Root, H0, H1)

N0 = “ × ”
H0 = ⊕(N0, H0,0, H0,1)

N1 = “ × ”
H1 = ⊕(N1, H1,0, H1,1)

N0,0 = c1
H0,0 = ⊕(N0,0)

N0,1 = x1
H0,1 = ⊕(N0,1)

N1,0 = c2
H1,0 = ⊕(N1,0)

N1,1 = x2
H1,1 = ⊕(N1,1)

Fig. 1 Hash tree example, in which the hash values of all nodes are calculated from both their own
node content and the has value of their children [3].

Expression hashing builds on the idea of Merkle trees [23]. Figure 1 shows how
hash values propagate towards the tree root (the topmost symbol of the expression)
using hash function ⊕ to aggregate child and parent hash values. Expression hashing
considers an internal node’s own symbol, as well as associativity and commutativity
properties. To account for these properties, each hashing step must be accompanied
by a corresponding sorting step, where child subexpressions are reordered according
to their type and hash value. Algorithm 1 ensures that child nodes are sorted and
hashed before parent nodes, such that calculated hash values are consistent towards
the root symbol.

8

Kammerer et al.

Algorithm 1: Expression hashing [3]

input : An expression E
output: The corresponding sequence of hash values

1 hashes ← empty list of hash values;
2 symbols ← list of symbols in E;
3 foreach symbol s in symbols do
4

H(s) ← an initial hash value;
if s is a terminal function symbol then

5

6

7

8

9

if s is commutative then

Sort the child nodes of s;

child hashes ← hash values of s’s children;
H(n) ← ⊕(child hashes, H(s));

10

hashes.append(H(n));

11 return hashes;

An expression’s hash value is then given by the hash value of its root symbol.
After sorting, sub-expressions with the same hash value are considered isomorphic
and are simpliﬁed according to arithmetic rules. The simpliﬁcation procedure is
illustrated in Figure 2 and consists of the following steps:

1. Fold: Apply associativity to eliminate nested symbols of the same type. For ex-
ample, postﬁx expression a b + c + consists of two nested additions where
each addition symbol has arity 2. Folding ﬂattens this expression to the equiva-
lent form a b c + where the addition symbol has arity 3.

2. Simplify: Apply arithmetic rules and mathematical identities to further simplify
the expressions. Since expression already include placeholders for numerical co-
efﬁcients, we eliminate redundant subexpressions such as a a b + which be-
comes a b +, or a a + which becomes a.

3. Repeat steps 1 and 2 until no further simpliﬁcation is possible.

Nested + and × symbols in Figure 2 are folded in the ﬁrst step, simplifying
the tree structure of the expression. Arithmetic rules are then applied for further
simpliﬁcation. In this example, the product of exponentials

exp(c1 × x1) × exp(c2 × x1) ≡ exp((c1 + c2) × x1)

is simpliﬁed since from a local optimization perspective, optimizing the coefﬁcients
of the expression yields better results for a single coefﬁcient c3 = c1 + c2, thus it
makes no sense to keep both original factors. Finally, the sum c4x1 + c5x1 is also
simpliﬁed since one term in the sum is redundant.

After simpliﬁcation, the hash value of the simpliﬁed tree is returned as the hash
value of the original expression. Based on this computation we are able to identify
already explored search paths and avoid duplicated effort.

Symbolic Regression by Exhaustive Search

9

+

fold

+

c

×

×

c

×

×

fold

×

c

+

×

+

×

c

×

×

exp

exp

c

x1

c

x1

c

x1

c

x1

c

exp

exp

c

x1

c

exp

×

×

c

x1

c

x1

×

×

c

x1

c

x1

×

c

x1

(a) Original expression

(b) Folded expression

(c) Minimal form

Fig. 2 Simpliﬁcation to canonical minimal form during hashing

3 Exploring the Search Space

By limiting the size of expressions, the grammar and the hashing scheme produce
a large but ﬁnite search space of semantically unique expressions. In an exhaustive
search, we iterate all these expressions and search for the best ﬁtting one. Thereby,
we derive sentences with every possible derivation path. An expression is rejected
if another expression with the same semantic—according to hashing—has already
been generated during the search. When a new, previously unseen sentence is de-
rived, the placeholders for coefﬁcients are replaced with real values and optimized
separately. The best ﬁtting sentence is stored.

Algorithm 2 outlines how all unique expressions are derived: We store unﬁn-
ished phrases—expressions with non-terminal symbols—in a data structure such as
a stack or queue. We fetch phrases from this data structure one after another, derive
new phrases, calculate their hash values and compare these hash values to previously
seen ones. To derive new phrases, we always replace the leftmost non-terminal sym-
bol in the old phrase with the production rules of this non-terminal symbol. If a
derived phrase becomes a sentence with only terminal symbols, its coefﬁcients are
optimized and its ﬁtness is evaluated. Otherwise, if it still contains derivable non-
terminal symbols, it is put back on the data structure.

We restrict the length of a phrase by its number of variable references—e.g. xx
and log(x) + x have two variable references. Phrases that exceed this limit are dis-
carded in the search. Since every non-terminal symbol is eventually derived to at
least one variable reference, non-terminal symbols count as variable references. In
our experiments, a limit on the complexity has been found to be the most intuitive
way to estimate an appropriate search space limit. Other measures, e.g. the number
of symbols are harder to estimate since coefﬁcients, function symbols and the non-
factorized representation of expression quickly inﬂate the number of symbols in a
phrase.

10

Kammerer et al.

Algorithm 2: Iterating the Search Space

input : Data set ds, max. number of variable references maxVariableRefs
output: Best ﬁtting expression

1 openPhrases ← empty data structure;
2 seenHashes ← empty set;
3 Add StartSymbol to openPhrases;
4 bestExpression ← constant symbol;

5 while openPhrases is not empty do
6

oldPhrase ← fetch and remove from openPhrases;
nonTerminalSymbol ← leftmost nonterminal symbol in oldPhrase;
foreach production prod of nonTerminalSymbol do

7

8

9

10

11

12

13

14

15

16

17

18

19

20

newPhrase ← apply prod on copy of oldPhrase;
if VariableRefs(newPhrase) ≤ maxVariableRefs then

hash ← Hash(newPhrase);
if seenHashes not contains hash then

Add hash to seenHashes;
if newPhrase is sentence then

Fit coefﬁcients of newPhrase to ds;
Evaluate newPhrase on ds;
if newPhrase is better than bestExpression then

bestExpression ← newPhrase;

else

Add newPhrase to openPhrases;

21 return bestExpression

3.1 Symbolic Regression as Graph Search Problem

Without considering the semantics of an expression, we would end up exploring a
search tree like in Figure 3, in which semantically equivalent expressions are derived
multiple times (e.g. c1x+c2x and c1x+c2x+c3x). However, hashing turns the search
tree into a directed search graph in which nodes (derived phrases) are reachable via
one or more paths, as shown in Figure 4. Thus, hashing prevents the search in a
graph region that was already visited. From this point of view, Algorithm 2 is very
similar to simple graph search algorithms such as depth-ﬁrst or breadth-ﬁrst search.

3.2 Guiding the Search

In Algorithm 2, the order in which expressions are generated is determined by the
data structure used. A stack or a queue would result in a depth-ﬁrst or a breadth-ﬁrst
search respectively. However, as the goal is to ﬁnd well-ﬁtting expressions quickly
and efﬁciently, we need to guide the traversal of a search graph towards promising
phrases.

Symbolic Regression by Exhaustive Search

11

Fig. 3 Search tree of expression generation without semantic hashing.

Our general framework for guiding the search is very similar to the idea used
in the A* algorithm [5]. We use a priority queue as data structure and assign a
priority value to each phrase, indicating the expected quality of sentences which are
derivable from that phrase. Phrases with high priority are derived ﬁrst in order to
discover well-ﬁtting sentences, steering the algorithm towards good solutions.

Similar to the A* algorithm, we cannot make a deﬁnite statement about a phrase’s
priority before actually deriving all possible sentences from it. Therefore, we need
to estimate this value with problem-speciﬁc heuristics. The calculation of phrase
priorities provides us a generic point for integrating heuristics for improving the
search efﬁciency and extending the algorithm’s capabilities in future work.

4 Steering the Search

We introduce a simple heuristic for guiding the search and leave more complex
and efﬁcient heuristics for future work. The proposed heuristic makes a pessimistic

ExprTerm + ExprExpr→Term+ExprTermExpr→TermFactor + ExprTerm→FactorFactor * Term + ExprTerm→Factor*TermFactor * TermTerm→Factor*TermFactorTerm→FactorX + ExprFactor→XX * Term + ExprFactor→XX * TermFactor→X X  Factor→XX + Term + ExprExpr→Term+ExprX + TermExpr→TermX + Factor + ExprTerm→Factortoo longTerm→Factor*TermX + Factor * TermTerm→Factor*TermX + FactorTerm→FactorX + X + ExprFactor→XX + X + TermExpr→Termtoo longExpr→Term+ExprX + X + FactorTerm→Factortoo longTerm→Factor*TermX + X + XFactor→XX + X * TermFactor→XX + XFactor→XX + X * FactorTerm→Factortoo longTerm→Factor*TermX * Factor + ExprTerm→Factortoo longTerm→Factor*TermX * Factor * TermTerm→Factor*TermX * FactorTerm→FactorX * X + ExprFactor→XX * X * TermFactor→XX * XFactor→XX * X + TermExpr→Termtoo longExpr→Term+ExprX * X + FactorTerm→Factortoo longTerm→Factor*TermX * X + XX * X * FactorTerm→Factortoo longTerm→Factor*TermX * X * XFactor→XFactor→XX + X * XFactor→X12

Kammerer et al.

Fig. 4 Search graph with loops caused by semantic hashing.

estimation of the quality of a phrase’s derivable sentences. This is done by evaluating
phrases before they are derived to sentences. With the goal of ﬁnding short and
accurate sentences quickly, the priority value considers both the expected quality
and the length of a phrase.

4.1 Quality Estimation

Estimating the expected quality of an unﬁnished phrase is possible due to the poly-
nomial structure of sentences and the derivation of the leftmost non-terminal sym-
bol in every phrase. Since expressions are sums of terms (c1Term1 + c2Term2 + ...),
repeated expansion of the leftmost non-terminal symbol derives one term after an-
other. This results in phrases such as in Equation 4, in which the ﬁrst two terms
c1 log(c2x + c3) and c4xx contain only terminal symbols and the last non-terminal
symbol is Expr.

ﬁnishedTerm1

c1 log(c2x + c3) +

ﬁnishedTerm2
c4xx

+

Expr
(cid:124)(cid:123)(cid:122)(cid:125)
Treat as coefﬁcient

(4)

Phrases where the only non-terminal symbol is Expr are evaluated as if they
were full sentences by treating Expr as a coefﬁcient during the local optimization
phase. We get a pessimistic estimate of the quality of derivable sentences, since de-

ExprTerm + ExprExpr→Term+ExprTermExpr→TermFactor + ExprTerm→FactorFactor * Term + ExprTerm→Factor*TermFactor *TermTerm→Factor*TermFactorTerm→FactorX + ExprFactor→XX * Term + ExprFactor→XX * TermFactor→XXFactor→XX + Term + ExprExpr→Term+ExprX + TermExpr→TermX + Factor + ExprTerm→Factortoo longTerm→Factor*TermX + Factor * TermTerm→Factor*TermX + FactorTerm→FactorFactor→XX + X * TermFactor→XFactor→XX + X * FactorTerm→Factortoo longTerm→Factor*TermX * Factor + ExprTerm→Factortoo longTerm→Factor*TermX * Factor * TermTerm→Factor*TermX * FactorTerm→FactorX * X + ExprFactor→XX * X * TermFactor→XX * XFactor→XX * X + TermExpr→Termtoo longExpr→Term+ExprX * X + FactorTerm→Factortoo longTerm→Factor*TermTerm→Factortoo longTerm→Factor*TermX + X * XFactor→XFactor→XX * X * FactorX * X * XFactor→XSymbolic Regression by Exhaustive Search

13

rived sentences with more terms can only have better quality. The quality can only
improve with more terms because of separate coefﬁcient optimization and one scal-
ing coefﬁcient per term, as shown in Equation 5. If a term which does not improve
the quality is derived, the optimization of coefﬁcients will cancel it out by setting
the corresponding scaling coefﬁcient to zero (e.g. c5 in Equation 5).

ﬁnishedTerm1 + ﬁnishedTerm2 +

c5Term
(cid:124) (cid:123)(cid:122) (cid:125)
Can only improve quality

(5)

This heuristic works only for phrases in which Expr is the only non-terminal
symbol. For sentences with different non-terminal symbols, we reuse the estimated
quality from the last evaluated parent phrase. The estimate is updated when a new
term with only terminal symbols is derived and again only one Expr remains. For
now, we do not have a reliable estimation method for terms that contain non-terminal
symbols and leave this topic for future work.

4.2 Priority Calculation

To prevent arbitrary adding of badly-ﬁtting terms that are eventually scaled down to
zero, our priority measure considers both a phrase’s length and its expected accu-
racy. To balance these two factors, these two measures need to be in the same scale.
We use the normalized mean squared error (NMSE) as quality measure which is in
the range [0, 1] for properly scaled solutions. This measure corresponds to 1 − R2
(coefﬁcient of determination). As length measure we use the number of symbols
relative to the maximum sentence length.

Since we limit the search space to a maximum number of variable references of a
phrase, we cannot exactly calculate the maximum possible length of a phrase. There-
fore, we estimate this maximum length with a greedy procedure: Starting with the
grammar’s start symbol Expr, we iteratively derive a new phrase using the longest
production rule. If two production rules have the same length, we take the one with
least non-terminal symbols and variable references.

Phrases with lower priority values are expanded ﬁrst during the search. The pri-
ority for steering the search from Section 3 is the phrase’s NMSE value minus its
weighted relative length, as shown in Equation 6. The weight w controls the greed-
iness and allows corrections of over- or underestimations of the maximum length.
However, in practice this value is not critical.

priority(p) = NMSE(p) − w

len(p)
lengthmax

(6)

14

Kammerer et al.

Table 1 OSGP experiment settings

Parameter

Setting

500
300
200 000
15%
Gender-speciﬁc selection (random and proportional)
Subtree swapping
Point mutation, tree shaking, changing single symbols, replac-
ing/removing branches
Number of nodes: 30, depth: 50
+, −, ×, ÷, exp, log, sin, cos, square, sqrt, cbrt

Population size
Max. selection pressure
Max. evaluated solutions
Mutation probability
Selection
Crossover operator
Mutation operator

Max. tree size
Function set

5 Experiments

We run our algorithm on several synthetic benchmark datasets to show that the
search space deﬁned by our restricted grammar is powerful enough to solve many
problems in feasible time. As benchmark datasets, we use noiseless datasets from
physical domains [4] and Nguyen-, Vladislavleva- and Keijzer-datasets [37] as de-
ﬁned and implemented in the HeuristicLab framework.

The search space was restricted in the experiments to include only sentences with
at most 20 variable references. We evaluate at most 200 000 sentences. Coefﬁcients
are randomly initialized and then ﬁtted with the iterative gradient-based Levenberg-
Marquardt algorithm [20, 21] with at most 100 iterations. For each model structure,
we repeat the coefﬁcient ﬁtting process ten times with differently initialized values
to reduce the chance of ﬁnding bad local optima.

As a baseline, we also run symbolic regression with GP on the same benchmark
problems. Therefore, we execute GP with strict offspring selection (OSGP) [1] and
explicit optimization of coefﬁcients [9]. The OSGP settings are listed in Table 1.
The OSGP experiments were executed with the HeuristicLab software framework4
[36]. Since this comparison focuses only on particular weaknesses and strengths
of our proposed algorithm over state of the art-techniques, we use the same OSGP
settings for all experiments and leave out problem-speciﬁc hyper parameter-tuning.

5.1 Results

Both the exhaustive search and OSGP were repeated ten times on each dataset. All
repetitions of the exhaustive search algorithm led to the exact same results. This un-
derlines the determinism of the proposed methods, even though we rely on stochas-
ticity when optimizing coefﬁcients. Also the OSGP results do not differ much. Ta-

4 https://dev.heuristiclab.com

Symbolic Regression by Exhaustive Search

15

Table 2 Median NMSE results for Keijzer instances.

Problem

0.3x sin(2πx); x ∈ [−1, 1]
0.3x sin(2πx); x ∈ [−2, 2]
0.3x sin(2πx); x ∈ [−3, 3]
x3 exp(−x) cos(x) sin(x)(sin(x)2 cos(x) − 1)
(30xz)/((x − 10)y2)

1
i

1 [6, 7]
2 [6]
3 [6]
4 [6, 26]
5 [6]
6 [6, 32] ∑x
i=1
ln(x)
7 [6, 32]
√
x
8 [6, 32]
9 [6, 32]
10 [6, 32] xy
11 [6, 33] xy + sin((x − 1)(y − 1))
12 [6, 33] x4 − x3 + y2/2 − y
13 [6, 33] 6 sin(x) cos(y)
14 [6, 33] 8/(2 + x2 + y2)
15 [6, 33] x3/5 + y3/2 − y − x

arcsinh(x) i.e. ln(x +

√

x2 + 1)

Exhaustive Search
Train.

Test

OSGP

Train.

Test

3e-27
5e-22
6e-32
1e-04
3e-08
8e-13
2e-31
2e-14
5e-14
4e-04
7e-04
5e-32
2e-32
4e-32
1e-22

2e-27
5e-22
3e-31
2e-04
3e-08
6e-09
3e-31
8e-10
1e-05
1e-01
7e-01
1e-31
2e-31
2e-31
2e-21

1e-30
5e-18
4e-30
1e-06
3e-20
5e-14
1e-30
5e-21
5e-17
6e-32
2e-22
7e-22
3e-32
1e-17
2e-11

8e-31
4e-18
3e-30
1e-06
3e-20
5e-13
2e-30
1e-21
6e-16
2e-04
9e-02
8e-18
3e-32
1e-17
6e-10

bles 2-5 show the achieved NMSE values for the exhaustive search and the median
NMSE values of all OSGP repetitions. NMSE values in the Tables 2-5 smaller than
10−8 are considered as exact or good-enough approximations and emphasized in
bold. The exhaustive search found a good solution (NMSE < 10−8) within ten min-
utes for all datasets. If no such solution was found, the algorithm runs until it reaches
the max. number of evaluated solutions, which can take days for larger datasets.

The experimental results show, that our algorithm struggles with problems with
complex terms—for example with Keijzer data sets 4, 5 and 11 in Table 2. This is
probably because our heuristic works ”term-wise”—our algorithm searches com-
pletely broad without any guidance within terms which still contain non-terminal
symbols. This issue becomes even more pronounced when we have to ﬁnd long
and complex function arguments. It should also be noted that our algorithm only
ﬁnds non-factorized representations of such arguments, which are even longer and
therefore even harder to ﬁnd in a broad search.

For the Nguyen datasets in Table 3 and the Keijzer datasets 12-15 in Table 2,
we ﬁnd the exact or good approximations in most cases with our exhaustive search.
Especially for simpler datasets, the results of our algorithm surpasses the one of
OSGP. This is likely due to the datasets’ low number of training instances, which
makes it harder for OSGP to ﬁnd good approximations.

Some problems are not contained in the search space, thus we do not ﬁnd any
good solution for them. This is the case for Keijzer 6, 9 and 10 in Table 2, for
which we do not support the required function symbols in our grammar. Also all
Vladislavleva datasets except 6 and 7 in Table 4 and the problems ”Fluid Flow” and
”Pagie-1” in Table 5 are not in the hypothesis space as they are too complex.

Another issue is the optimization of coefﬁcients. Although several problems have
a simple structure and are in the search space, we do not ﬁnd the right coefﬁcients

16

Kammerer et al.

for arguments of non-linear functions, for example in Nguyen 5-7. The issue hereby
is that we iterate over the actually searched model structure but determine bad co-
efﬁcients. As we do never look again at the same model structure, we can only
ﬁnd an approximation. This is a big difference to symbolic regression with genetic
programming, in which we might ﬁnd the same structure again in next generations.

6 Discussion

Among the nonlinear system identiﬁcation techniques, symbolic regression is char-
acterized by its ability to identify complex nonlinear relationships in structured nu-
merical data in the form of interpretable models. The combination of the power of
nonlinear system identiﬁcation without a priori assumptions about the model struc-
ture with the white-box ability of mathematical formulas represents the unique sell-
ing point of symbolic regression. If tree-based GP is used as search method, the
ability to interpret the found models is limited due to the stochasticity of the GP
search. Thus, at the end of the modeling phase, several similarly complex models of
approximately the same quality can be produced, which have completely different
structures and use completely different subsets of features. These last-mentioned
limitations due to ambiguity can be countered using a deterministic approach in
which only semantically unique models may be used. This approach, however, re-
quires a lot of restrictions regarding search space complexity in order to specify a
subspace in which an exhaustive search is feasible. On the other hand, the exhaus-
tive claim enables the approach to generate extensive model libraries already in the
ofﬂine phase, through which as soon as a concrete task is given in the online phase,
it is only necessary to navigate in a suitable way.

Table 3 Median NMSE results for Nguyen instances.

Problem

x3 + x2 + x
x4 + x3 + x2 + x
x5 + x4 + x3 + x2 + x
x6 + x5 + x4 + x3 + x2 + x
sin(x2) cos(x) − 1
sin(x) + sin(x + x2)
log(x + 1) + log(x2 + 1)
√
x

1 [34]
2 [34]
3 [34]
4 [34]
5 [34]
6 [34]
7 [34]
8 [34]
9 [34]
10 [34] 2 sin(x) cos(y)
11 [34] xy
12 [34] x4 − x3 + y2/2 − y

sin(x) + sin(y2)

Exhaustive Search
Train.

Test

OSGP

Train.

Test

5e-34
3e-33
1e-33
6e-12
9e-14
2e-17
4e-13
6e-32
2e-13
5e-32
2e-06
2e-31

3e-33
4e-33
7e-33
6e-11
3e-13
2e-12
5e-12
2e-31
2e-12
1e-31
1e-02
2e-31

8e-30
5e-30
2e-16
2e-12
3e-18
6e-14
5e-13
7e-32
8e-31
1e-28
6e-30
7e-18

2e-29
1e-28
2e-15
3e-08
4e-18
6e-08
1e-09
1e-31
8e-31
8e-29
3e-30
5e-17

Symbolic Regression by Exhaustive Search

17

Table 4 Median NMSE results for Vladislavleva instances.

Problem

Exhaustive Search
Train.

Test

OSGP

Train.

Test

exp(−(x1 − 1)2)/(1.2 + (x2 − 2.5)2)
exp(−x)x3 cos(x) sin(x)(cos(x) sin(x)2 − 1)
f2(x1)(x2 − 5)

1 [30]
2 [26]
3 [35]
4 [35] 10/(5 + ∑5
i=1(xi − 3)2)
5 [35] 30((x1 − 1)(x3 − 1))/(x2
6 [35] 6 sin(x1) cos(x2)
7 [35]
8 [35]

3e-03
3e-04
1e-02
1e-01
2e-03
8e-32
(x1 − 3)(x2 − 3) + 2 sin((x1 − 4)(x2 − 4))
1e-30
((x1 − 3)4 + (x2 − 3)3 − (x2 − 3))/((x2 − 2)4 + 10) 1e-03

2(x1 − 10))

3e-01
1e-02
2e-01
2e-01
9e-03
4e-31
9e-31
2e-01

1e-09
3e-06
3e-05
7e-03
8e-16
6e-31
5e-29
5e-05

9e-07
2e-03
6e-04
1e-02
9e-15
3e-19
4e-29
2e-02

Table 5 Median NMSE results for other instances.

Problem

Poly-10 [25]
x1x2 + x3x4 + x5x6 + x1x7x9 + x3x6x10
Pagie-1 (Inverse Dynamics) [24]
1/(1 + x−4) + 1/(1 + y−4)
Aircraft Lift Coefﬁcient [4]
CLα (α − α0) +CLδe δeSHT /Sref
Fluid Flow [4]
V∞r sin(θ )(1 − R2/r2) + Γ /(2π) ln(r/R)
Rocket Fuel Flow [4]
p0A(cid:63)/
T0

γ/R(2/(γ + 1))(γ+1)/(γ−1)

(cid:112)

√

Exhaustive Search
Train.

Test

OSGP

Train.

Test

2e-32

1e-32

7e-02

1e-01

1e-03

6e-01

9e-07

5e-05

3e-31

3e-31

2e-17

2e-17

3e-04

4e-04

9e-06

2e-05

3e-31

3e-31

1e-19

1e-19

In a very reduced summary, one could characterize the classical tree-based sym-
bolic regression using GP and the approach of deterministically and exhaustively
generating models in such a way that the latter enables a complete search in an in-
complete search space while the classical approach performs an incomplete search
in a rather complete search space.

6.1 Limitations

The approach we have described in this contribution also has several limitations. For
the identiﬁcation of optimal coefﬁcient values we rely on the Levenberg-Marquardt
method for least squares, which is a local search routine using gradient information.
Therefore, we can only hope to ﬁnd global optima for coefﬁcient values. Finding bad
local optima for coefﬁcients is less of a concern when using GP variants with a sim-
ilar local improvement scheme because there are implicitly many restarts through
the evolutionary operations of recombination and mutation. In the proposed method

18

Kammerer et al.

we visit each structure only once and therefore risk to discard a good solution when
we are unlucky to ﬁnd good coefﬁcients.

We have worked only with noiseless problem instances yet. We observed in ﬁrst
experiments with noisy problems instances that the algorithm might get stuck try-
ing to improve non-optimal partial solutions due to its greedy nature. Therefore, we
need further investigations before we move on with the development of our algo-
rithm to noisy real-world problems.

Another limitation is the poor scalability of grammar enumeration when increas-
ing the number of features or the size of the search space. When increasing these
parameters we can not expect to explore a signiﬁcant part of the complete search
space and must increasingly rely on the power of heuristics to hone in on relevant
subspaces. Currently, we have only integrated a single heuristic which evaluates
terms in partial solutions and prioritizes phrase which include well-ﬁtting terms.
However, the algorithm has no way to prioritize incomplete terms and is inefﬁcient
when trying to ﬁnd complex terms.

7 Outlook

Even when considering the above mentioned limitations of the currently imple-
mented algorithm we still see signiﬁcant potential in the approach of more system-
atic and deterministic search for symbolic regression and we already have several
ideas to improve the algorithm and overcome some of the limitations.

The integration of improved heuristics for guided search is our top-priority. An
advantage of the concept is that it is extremely general and allows to experiment with
many different heuristics. Heuristics can be as simple as prioritizing shorter expres-
sions or less complex expressions. More elaborate schemes which guide the search
based on prior knowledge about the data-generating process are easy to imagine.
Heuristics could incorporate syntactical information (e.g. which variables already
occur within the expression) as well as information from partial evaluation of ex-
pressions. We also consider dynamic heuristics which are adjusted while the algo-
rithm is running and learning about the problem domain. Potentially, we could even
identify and learn heuristics which are transferable to other problem instances and
would improve efﬁciency in a transfer learning setting.

Getting trapped in local optima is less of a concern when we apply global search
algorithms for coefﬁcient values such as evolution strategies, differential evolution,
or particle swarm optimization (cf. [11]). Another approach would be to reduce
the ruggedness of the objective function through regularization of the coefﬁcient
optimization step. This could be helpful to reduce the potential of overﬁtting and
getting stuck in sub-optimal subspaces of the search space.

Generally, we consider grammar enumeration to be effective only when we limit
the search space to relatively short expressions—which is often the case in our in-
dustrial applications. Therein lies the main potential compared to the more general
approach of genetic programming. In this context we continue to explore potential

Symbolic Regression by Exhaustive Search

19

for segmentation of the search space [19] in combination with grammar enumera-
tion in an ofﬂine phase for improving later search runs. Grammar enumeration with
deduplication of structures could also be helpful to build large ofﬂine libraries of
sub-expressions that could be used by GP [2, 8, 17, 18].

Acknowledgements The authors gratefully acknowledge support by the Christian Doppler Re-
search Association and the Federal Ministry for Digital and Economic Affairs within the Josef
Ressel Center for Symbolic Regression.

References

1. Affenzeller, M., Winkler, S., Wagner, S., Beham, A.: Genetic Algorithms and Genetic Pro-
gramming - Modern Concepts and Practical Applications, Numerical Insights, vol. 6. CRC
Press, Chapman & Hall (2009)

2. Angeline, P.J., Pollack, J.: Evolutionary module acquisition. In: Proceedings of the Second
Annual Conference on Evolutionary Programming, pp. 154–163. La Jolla, CA, USA (1993)
3. Burlacu, B., Kammerer, L., Affenzeller, M., Kronberger, G.: Hash-based Tree Similarity and
Simpliﬁcation in Genetic Programming for Symbolic Regression. In: Computer Aided Sys-
tems Theory, EUROCAST 2019 (2019)

4. Chen, C., Luo, C., Jiang, Z.: A multilevel block building algorithm for fast modeling general-

ized separable systems. Expert Systems with Applications 109, 25–34 (2018)

5. Hart, P.E., Nilsson, N.J., Raphael, B.: A formal basis for the heuristic determination of min-
IEEE Transactions on Systems Science and Cybernetics 4(2), 100–107

imum cost paths.
(1968)

6. Keijzer, M.: Improving symbolic regression with interval arithmetic and linear scaling. In:
Genetic Programming, Proceedings of EuroGP’2003, LNCS, vol. 2610, pp. 70–82. Springer-
Verlag, Essex (2003)

7. Keijzer, M., Babovic, V.: Genetic programming, ensemble methods and the bias/vari-
In: Genetic Programming, Proceedings of Eu-

ance tradeoff - introductory investigations.
roGP’2000, LNCS, vol. 1802, pp. 76–90. Springer-Verlag, Edinburgh (2000)

8. Keijzer, M., Ryan, C., Murphy, G., Cattolico, M.: Undirected training of run transferable li-
braries. In: Proceedings of the 8th European Conference on Genetic Programming, Lecture
Notes in Computer Science, vol. 3447, pp. 361–370. Springer, Lausanne, Switzerland (2005)
9. Kommenda, M., Kronberger, G., Winkler, S., Affenzeller, M., Wagner, S.: Effects of constant
In: Proceed-
optimization by nonlinear least squares minimization in symbolic regression.
ings of the 15th Annual Conference Companion on Genetic and Evolutionary Computation,
GECCO ’13 Companion, pp. 1121–1128. ACM (2013)

10. Korns, M.F.: Symbolic regression using abstract expression grammars.

In: GEC ’09: Pro-
ceedings of the ﬁrst ACM/SIGEVO Summit on Genetic and Evolutionary Computation, pp.
859–862. ACM, Shanghai, China (2009)

11. Korns, M.F.: Abstract expression grammar symbolic regression.

In: Genetic Programming
Theory and Practice VIII, Genetic and Evolutionary Computation, vol. 8, chap. 7, pp. 109–
128. Springer, Ann Arbor, USA (2010)

12. Korns, M.F.: Extreme accuracy in symbolic regression. In: Genetic Programming Theory and
Practice XI, Genetic and Evolutionary Computation, chap. 1, pp. 1–30. Springer, Ann Arbor,
USA (2013)

13. Korns, M.F.: Extremely accurate symbolic regression for large feature problems. In: Genetic
Programming Theory and Practice XII, Genetic and Evolutionary Computation, pp. 109–131.
Springer, Ann Arbor, USA (2014)

20

Kammerer et al.

14. Korns, M.F.: Highly accurate symbolic regression with noisy training data. In: Genetic Pro-
gramming Theory and Practice XIII, Genetic and Evolutionary Computation, pp. 91–115.
Springer, Ann Arbor, USA (2015)

15. Kotanchek, M., Smits, G., Vladislavleva, E.: Trustable symbolic regression models: using
ensembles, interval arithmetic and pareto fronts to develop robust and trust-aware models.
In: Genetic Programming Theory and Practice V, Genetic and Evolutionary Computation,
chap. 12, pp. 201–220. Springer, Ann Arbor (2007)

16. Kotanchek, M.E., Vladislavleva, E., Smits, G.: Symbolic Regression Is Not Enough: It Takes

a Village to Raise a Model, pp. 187–203. Springer New York, New York, NY (2013)

17. Krawiec, K., Pawlak, T.: Locally geometric semantic crossover. In: GECCO Companion ’12:
Proceedings of the fourteenth international conference on Genetic and evolutionary computa-
tion conference companion, pp. 1487–1488. ACM, Philadelphia, Pennsylvania, USA (2012)
18. Krawiec, K., Swan, J., O’Reilly, U.M.: Behavioral program synthesis: Insights and prospects.
In: Genetic Programming Theory and Practice XIII, Genetic and Evolutionary Computation,
pp. 169–183. Springer, Ann Arbor, USA (2015)

19. Kronberger, G., Kammerer, L., Burlacu, B., Winkler, S.M., Kommenda, M., Affenzeller, M.:
Cluster analysis of a symbolic regression search space. In: Genetic Programming Theory and
Practice XVI. Springer, Ann Arbor, USA (2018)

20. Levenberg, K.: A method for the solution of certain non-linear problems in least squares.

Quarterly of Applied Mathematics 2(2), 164–168 (1944)

21. Marquardt, D.W.: An algorithm for least-squares estimation of nonlinear parameters. Journal

of the Society for Industrial and Applied Mathematics 11(2), 431–441 (1963)

22. McConaghy, T.: FFX: Fast, scalable, deterministic symbolic regression technology. In: Ge-
netic Programming Theory and Practice IX, Genetic and Evolutionary Computation, chap. 13,
pp. 235–260. Springer, Ann Arbor, USA (2011)

23. Merkle, R.C.: A digital signature based on a conventional encryption function. In: Advances
in Cryptology — CRYPTO ’87, pp. 369–378. Springer Berlin Heidelberg, Berlin, Heidelberg
(1988)

24. Pagie, L., Hogeweg, P.: Evolutionary consequences of coevolving targets. Evolutionary Com-

putation 5(4), 401–418 (1997)

25. Poli, R.: A simple but theoretically-motivated method to control bloat in genetic program-
ming. In: Genetic Programming, Proceedings of EuroGP’2003, LNCS, vol. 2610, pp. 204–
217. Springer-Verlag, Essex (2003)

26. Salustowicz, R.P., Schmidhuber, J.: Probabilistic incremental program evolution. Evolutionary

Computation 5(2), 123–141 (1997)

27. Schmidt, M., Lipson, H.: Co-evolving ﬁtness predictors for accelerating and reducing evalua-
tions. In: Genetic Programming Theory and Practice IV, Genetic and Evolutionary Computa-
tion, vol. 5, pp. 113–130. Springer, Ann Arbor (2006)

28. Schmidt, M., Lipson, H.: Symbolic regression of implicit equations. In: Genetic Programming
Theory and Practice VII, Genetic and Evolutionary Computation, chap. 5, pp. 73–85. Springer,
Ann Arbor (2009)

29. Schmidt, M., Lipson, H.: Age-ﬁtness pareto optimization.

In: Genetic Programming The-
ory and Practice VIII, Genetic and Evolutionary Computation, vol. 8, chap. 8, pp. 129–146.
Springer, Ann Arbor, USA (2010)

30. Smits, G., Kotanchek, M.: Pareto-front exploitation in symbolic regression. In: Genetic Pro-

gramming Theory and Practice II, chap. 17, pp. 283–299. Springer, Ann Arbor (2004)

31. Stijven, S., Vladislavleva, E., Kordon, A., Kotanchek, M.: Prime-time: Symbolic regression
In: Genetic Programming Theory and Practice XIII,

takes its place in industrial analysis.
Genetic and Evolutionary Computation, pp. 241–260. Springer, Ann Arbor, USA (2015)
32. Streeter, M.J.: Automated discovery of numerical approximation formulae via genetic pro-
gramming. Master’s thesis, Computer Science, Worcester Polytechnic Institute, MA, USA
(2001)

33. Topchy, A., Punch, W.F.: Faster genetic programming based on local gradient search of nu-
meric leaf values. In: Proceedings of the Genetic and Evolutionary Computation Conference
(GECCO-2001), pp. 155–162. Morgan Kaufmann, San Francisco, California, USA (2001)

Symbolic Regression by Exhaustive Search

21

34. Uy, N.Q., Hoai, N.X., O’Neill, M., McKay, R.I., Galvan-Lopez, E.: Semantically-based
crossover in genetic programming: application to real-valued symbolic regression. Genetic
Programming and Evolvable Machines 12(2), 91–119 (2011)

35. Vladislavleva, E.J., Smits, G.F., den Hertog, D.: Order of nonlinearity as a complexity mea-
IEEE

sure for models generated by symbolic regression via Pareto genetic programming.
Transactions on Evolutionary Computation 13(2), 333–349 (2009)

36. Wagner, S., Affenzeller, M.: HeuristicLab: A generic and extensible optimization environ-

ment. In: Adaptive and Natural Computing Algorithms, pp. 538–541. Springer (2005)

37. White, D.R., McDermott, J., Castelli, M., Manzoni, L., Goldman, B.W., Kronberger, G.,
Ja´skowski, W., O’Reilly, U.M., Luke, S.: Better GP benchmarks: community survey results
and proposals. Genetic Programming and Evolvable Machines 14(1), 3–29 (2013)

38. Worm, T., Chiu, K.: Prioritized grammar enumeration: symbolic regression by dynamic pro-
gramming. In: GECCO ’13: Proceeding of the ﬁfteenth annual conference on Genetic and
evolutionary computation conference, pp. 1021–1028. ACM, Amsterdam, The Netherlands
(2013)

39. Zou, H., Hastie, T.: Regularization and variable selection via the elastic net. Journal of the

royal statistical society: series B (statistical methodology) 67(2), 301–320 (2005)

