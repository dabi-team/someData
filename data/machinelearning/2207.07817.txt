On Scheduling Ring-All-Reduce Learning Jobs in Multi-Tenant
GPU Clusters with Communication Contention
Menglu Yu,1 Bo Ji,2 Hridesh Rajan,1 and Jia Liu3,1
1Department of Computer Science, Iowa State University
2Department of Computer Science, Virginia Tech
3Department of Electrical and Computer Engineering, The Ohio State University

2
2
0
2

g
u
A
4
1

]

C
D
.
s
c
[

2
v
7
1
8
7
0
.
7
0
2
2
:
v
i
X
r
a

ABSTRACT
Powered by advances in deep learning (DL) techniques, machine
learning and artiï¬cial intelligence have achieved astonishing suc-
cesses. However, the rapidly growing needs for DL also led to communication-
and resource-intensive distributed training jobs for large-scale DL
training, which are typically deployed over GPU clusters. To sus-
tain the ever-increasing demand for DL training, the so-called â€œring-
all-reduceâ€ (RAR) technologies have recently emerged as a favor-
able computing architecture to eï¬ƒciently process network commu-
nication and computation load in GPU clusters. The most salient
feature of RAR is that it removes the need for dedicated parameter
servers, thus alleviating the potential communication bottleneck.
However, when multiple RAR-based DL training jobs are deployed
over GPU clusters, communication bottlenecks could still occur
due to contentions between DL training jobs. So far, there remains
a lack of theoretical understanding on how to design contention-
aware resource scheduling algorithms for RAR-based DL training
jobs, which motivates us to ï¬ll this gap in this work. Our main con-
tributions are three-fold: i) We develop a new analytical model that
characterizes both communication overhead related to the worker
distribution of the job and communication contention related to
the co-location of diï¬€erent jobs; ii) Based on the proposed ana-
lytical model, we formulate the problem as a non-convex integer
program to minimize the makespan of all RAR-based DL training
jobs. To address the unique structure in this problem that is not
amenable for optimization algorithm design, we reformulate the
problem into an integer linear program that enables provable ap-
proximation algorithm design called SJF-BCO (Smallest Job First
with Balanced Contention and Overhead); and iii) We conduct ex-
tensive experiments to show the superiority of SJF-BCO over ex-
isting schedulers. Collectively, our results contribute to the state-
of-the-art of distributed GPU system optimization and algorithm
design.

ACM Reference Format:
Menglu Yu,1 Bo Ji,2 Hridesh Rajan,1 and Jia Liu3,1 . 2022. On Scheduling
Ring-All-Reduce Learning Jobs in Multi-Tenant GPU Clusters with Com-
munication Contention. In The Twenty-third International Symposium on
Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks
and Mobile Computing (MobiHoc â€™22), October 17â€“20, 2022, Seoul, Republic of
Korea. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3492866.3549716

1 INTRODUCTION
Background and Motivation: In recent years, the rise of deep
learning has driven an ever-increasing need for large-scale dis-
tributed training in GPU clusters, which leverages massive paral-
lelism to speed up the training processes. This has been evidenced
by the popularity of several prevailing distributed deep learning
(DDL) frameworks (e.g., TensorFlow [1] and PyTorch [12]). In these
DDL frameworks, the traditional and most widely adopted computing-
networking structure is based on the sever-worker (SW) architec-
ture, where DDL training jobs are decomposed into and executed
in parallel by a set of workers under the coordination of a param-
eter server. However, as the number of workers increases, the SW
architecture suï¬€ers from serious scalability limitations since the
server acts as a communication bottleneck and a single-point-of-
failure. To address the scalability limitations of the SW architec-
ture, the ring-all-reduce (RAR) [13] architecture has attracted in-
creasing attention in recent years. The key idea of RAR is that, by
forming a ring and working collaboratively, the workers can up-
date the learning model parameters without needing any parame-
ter server, thus removing the communication bottleneck and alle-
viating the single point of failure. Moreover, it can be shown that
the RAR architecture enjoys the highly desirable â€œbandwidth opti-
malityâ€ in the sense that, as the number of workers increases, the
amount of information exchanged in the network is asymptotically
independent of the number of workers (see Section 3 for details).

CCS CONCEPTS
â€¢ Computing methodologies
works

â†’

Network performance analysis.

â†’

Distributed algorithms; â€¢ Net-

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full cita-
tion on the ï¬rst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciï¬c permission
and/or a fee. Request permissions from permissions@acm.org.
MobiHoc â€™22, October 17â€“20, 2022, Seoul, Republic of Korea
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9165-8/22/10. . . $15.00
https://doi.org/10.1145/3492866.3549716

However, despite all these salient features, the performance of
deploying RAR-based training jobs in multi-tenant GPU clusters re-
mains far from being satisfactory in practice [19]. The fundamen-
tal reason is that the bandwidth optimality of RAR architecture
only happens when there is only a single training job in the system
(i.e., a contention-free environment). In a multi-tenant GPU clus-
ter, however, such an ideal contention-free condition is rarely satis-
ï¬ed. As a result, signiï¬cant communication bottleneck links could
occur when deploying RAR-based training jobs in the system. For
example, researchers in [19] have found that on a cluster of four-
GPU servers connected by 10 Gbps Ethernet, when only one RAR
training job is executed with four GPUs in the cluster, the job com-
pletion time is 295 seconds. In comparison, when four jobs of the
same type are executed simultaneously with each job still using

 
 
 
 
 
 
MobiHoc â€™22, October 17â€“20, 2022, Seoul, Republic of Korea

Yu and Liu, et al.

four GPUs but scheduled across GPU servers, each jobâ€™s comple-
tion time dramatically increases to 675 seconds due to the exten-
sive communication contention. These empirical performance re-
sults of RAR indicate that developing eï¬ƒcient and eï¬€ective sched-
uling for RAR-based DDL training jobs is well warranted to miti-
gate contention-induced communication bottlenecks. However, in
the literature so far, there remains a lack of theoretical understand-
ing on how to design contention-aware resource scheduling algo-
rithms for RAR-based DDL training jobs. In light of the rapidly
growing importance of RAR-based DDL deployment, our goal in
this paper is to ï¬ll this gap and develop contention-aware sched-
uling algorithms for RAR-based training jobs in multi-tenant GPU
clusters.

Technical Challenges: We note, however, that due to a num-
ber of technical diï¬ƒculties, developing contention-aware sched-
uling algorithms for RAR-based DDL jobs in multi-tenant GPU
clusters is highly challenging. First and foremost, just as any net-
work optimization problems that deal with contentions and inter-
ferences, the completion time of an RAR-based training job de-
pends not only on its resource allocation decisions (i.e., the num-
ber of ring-forming workers and their locality), but also on the
number of concurrent RAR-based DDL jobs that (partially or com-
pletely) share the communication links of this job. The complex
communication coupling between concurrent RAR-based training
jobs renders it intractable to compute the per-iteration execution
time of an RAR-based DDL job in closed-form. Second, there exists
a fundamental trade-oï¬€ in terms of job locality. On one hand, co-
locating all workers of an RAR-based DDL job on the same server
enjoys a faster intra-server communication speed, but could lead
to resource fragmentation. On the other hand, spreading the ring
of an RAR job over multiple servers could also result in more con-
tentions of communication links and overhead in establishing con-
nections between servers. Last but not least, due to the resource
constraints of each server and the iterative nature of DDL train-
ing workload, the resource allocation decision for each RAR-based
training job is subject to a mix of packing and covering types of
constraints, both of which are known to be NP-hard.

Our Contributions: In this paper, we overcome the above chal-
lenges and design a suite of scheduling algorithmic techniques for
eï¬ƒcient RAR-based DDL training in multi-tenant GPU clusters
with theoretical makespan performance guarantees. The key idea
of our algorithmic design is to transfer the structural complexity
of the intractable per-iteration running evaluation in the original
scheduling problem to the dimensional complexity of an equiva-
lent reformulated problem, which has a much cleaner integer lin-
ear program structure to work with. Our main results and technical
contributions are summarized as follows:

â€¢

â€¢

We ï¬rst propose a new analytical framework for RAR-based
DDL training resource allocation and scheduling that charac-
terizes both communication contention and overhead under
the RAR architecture in a multi-tenant GPU cluster. This ana-
lytical modeling serves as the foundation to enable us to formu-
late the scheduling optimization framework to minimize the
makespan of all RAR-based training jobs.
As mentioned earlier, due to the complex resource contentions
and couplings between RAR-based DDL jobs, it is intractable

to determine the closed-form expression for the per-iteration
execution time for each DDL job. To address this challenge, we
further reformulate the original problem into an equivalent in-
teger problem, which has a cleaner problem structure. Doing
so allows us to convert the structural complexity of the origi-
nal problem to the exponential dimensionality complexity in
the reformulated problem, which is more amenable for low-
complexity search-based optimization algorithm design.
Based on the above problem reformulation, we propose an ef-
ï¬cient scheduling algorithm called SJF-BCO (smallest job first
with balanced contention and overhead) with theoretical ap-
proximation ratio guarantee. We conduct extensive experiments
to verify the performance of our proposed SJF-BCO algorithm
and compare with existing scheduling policies to show the su-
periority of SJF-BCO over these baselines.

â€¢

Collectively, our results contribute to a comprehensive and fun-
damental understanding of RAR-based DDL resource scheduling
optimization. The roadmap of the rest of the paper is as follows.
In Section 2, we review the related literature. Section 3 present
preliminaries to familiar readers with the necessary background.
Section 4 introduces the system model and problem formulation.
Section 5 demonstrates our algorithms and Section 6 provides their
performance analysis. Section 7 presents numerical results and Sec-
tion 8 concludes this paper.

2 RELATED WORK
As mentioned in Section 1, DDL training job scheduling algorithms
have received growing interest recently. Research in this area aims
to schedule DDL jobs and manage computing resources eï¬ƒciently
in multi-tenant GPU computing clusters. Early attempts in this
ï¬eld were mostly heuristic approaches based on empirical observa-
tions and models to conduct the resource scheduling (e.g., [3, 7, 10,
11]). For example, Gandiva [20] considered GPU time-slicing and
job scheduling by predicting DDL training jobs characteristics. Op-
timus [14] leveraged performance models through online-ï¬tting to
guide the job scheduling aiming to minimize training completion
time. Rather than using prediction models, another line of research
is to take advantage of the model-less data-riven learning methods
for DDL job scheduling (e.g., [2, 8, 18]). For instance, Harmony [2],
a deep-reinforcement-learning-based scheduler considered mini-
mizing the job completion time. Hu. et al. [8] designed a new sched-
uling framework called Spear to minimize the makespan of jobs
by leveraging the deep reinforcement learning techniques. How-
ever, these works do not provide theoretical performance guar-
antee. Also, none of these works considered RAR-based DDL job
scheduling.

The most related work to this paper is GADGET [22], which
characterized RAR-based DDL job scheduling based on the assump-
tion that the bandwidth of each job is reserved. As a result, there
is no need to consider communication contention in [22]. We note
that a limitation of the reserved bandwidth assumption is that it
could lead to resource under-utilization. In contrast, this paper con-
siders communication contention to avoid this limitation. This, how-
ever, renders the scheduling problem far more challenging. Lastly,
Wang et al. [19] also considered contention under various all-reduce

On Scheduling Ring-All-Reduce Learning Jobs in Multi-Tenant GPU Clusters with Communication Contention

MobiHoc â€™22, October 17â€“20, 2022, Seoul, Republic of Korea

Worker 1
c
a
b

c

Worker 3
b c
a

a

Worker 2
a b c

Step 1
(Shareâˆ’Reduce)

b

Worker 3
b
a

a

Worker 3
a

b

Initial State

Worker 1
b

c

State 2

Worker 1
a
b c

b

Step 3
(Shareâˆ’Only)

c

Worker 2
c

Worker 3
c
a

Worker 1
c
b
c

a

State 1

Worker 1
a b

c

Worker 2
c
a

Step 2
(Shareâˆ’Reduce)

a

Worker 2
b c

Step 4
(Shareâˆ’Only)

a
State 3

Worker 1

Worker 3
cb
a

Worker 2
a
b c

Effectively

Worker 3

Worker 2

n

State 4

Gradient subvector n of Worker 1

(n = a, b, c)
n Gradient Subvector n of Worker 3
(n = a, b, c)

b

n

Gradient subvector 2 summation
for Workers 2 and 3
Gradient subvector n summation
for Workers 1, 2, and 3 (n = a, b, c)

Final State

Gradient subvector n of Worker 2

(n = a, b, c)

Gradient subvector 1 summation for Workers 1 and 2

Gradient subvector 3 summation for Workers 1 and 3

n

a

c

Sum of full gradient vectors of Workers 1, 2, and 3

Figure 1: A three-worker illustrative example of the ring-all-
reduce (RAR) process.

architectures, including RAR. However, they also relied on a system-
dependent online-ï¬tting model to predict the execution time and
did not explicitly formulate any scheduling optimization problem.
Their solution was based on heuristics without theoretical perfor-
mance guarantee. In contrast, we develop an analytical model to
facilitate the job scheduling as a rigorous optimization problem,
which in turn entails approximation algorithm design with theo-
retical performance guarantee.

= ğ‘„, and gğ‘–
ğ‘˜ is a stochastic gradient based on a random sam-
|Qğ‘˜ |
ple ğ›¿ğ‘–
âˆˆ Qğ‘˜ . The ï¬nite-sum and mini-batch structure of SGD nat-
urally lends itself to a distributed implementation in a ğ‘„-worker
DDL system coordinated by a parameter server as follows: First,
the dataset is partitioned by ğ‘„ workers. In each iteration ğ‘˜, each
worker retrieves the current model parameters from the server and
randomly draws a sample from its local dataset, and then computes
a stochastic gradient (e.g., using the backpropagation method). Then,
all workers send their gradients to the server to be aggregated.

2) The Ring-All-Reduce (RAR) Architecture: It can be seen
from the above discussions that SGD-based distributed learning
naturally implies a server-worker (SW) architecture. However, as
mentioned in Section 1, the SW architecture suï¬€ers from scalabil-
ity limitations as the number of workers increases. This is because
all workers need to communicate with the server, which creates
a bottleneck. Speciï¬cally, a ğ‘¤-worker SW system that solves a ğ‘‘-
dimensional ERM problem requires 2ğ‘¤ğ‘‘ amount of data exchange
per iteration (each worker sends and receives a ğ‘‘-dimensional vec-
tors per iteration), which scales linearly with respect to ğ‘¤.

)

1

ğ‘¤
(

To address this scalability limitation, the RAR [13] has been pro-
posed to remove the server. Under RAR, the workers form a ring
to exchange and aggregate data collaboratively. For a ğ‘¤-worker
RAR system, each worker splits its stochastic gradient into ğ‘¤ sub-
vectors (see Fig. 1 for an example with ğ‘¤ = 3). Each iteration of
steps that can be divided into two phases. In
RAR has 2
âˆ’
1), workers perform gradients re-
the ï¬rst phase (steps 1, . . . , ğ‘¤
duction (i.e., summation), where each worker receives a gradient
subvector from its upstream worker and sends its local reduction
result to its downstream worker (Share-Reduce phase). In the sec-
ond phase (steps ğ‘¤, . . . , 2ğ‘¤
2), each worker circulates its resultant
sub-vectors following the same token to obtain its ï¬nal resultant
gradients vector (Share-Only phase). Since each worker sends ğ‘‘
ğ‘¤
1
ğ‘¤
amount of data for 2
times, the total amount of data any
âˆ’
(
)
worker receives is 2ğ‘‘
1
ğ‘¤
, which is asymptotically independent
)ğ‘¤
âˆ’
(
of ğ‘¤ as ğ‘¤ increases (also referred to as being bandwidth-optimal in
the literature).

âˆ’

âˆ’

3 RING-ALL-REDUCE (RAR)-BASED

DISTRIBUTED LEARNING: A PRIMER

4 SYSTEM MODEL AND PROBLEM

FORMULATION

In this section, we provide a quick overview on the RAR-based dis-
tributed learning to familiarize readers with necessary background
and ï¬x the terminologies that are useful in the rest of the paper.

In this section, we ï¬rst introduce our system model in Section 4.1
and then present the problem formulation for RAR-based DDL sched-
uling optimization in multi-tenant GPU clusters in Section 4.2.

(

âˆˆ

w
)

Rğ‘‘ Â¯ğ¿

, 1
ğ‘ƒ

1) SGD-Based Distributed Learning: The training of many
ML problems is typically in the form of an empirical risk minimiza-
ğ‘ƒ
tion (ERM) problem: minw
, where w
ğ‘–=1 ğ¿
(
is a loss
contains the model parameters to be learned, ğ¿
(
function, and ğ‘ƒ is the total number of samples. Due to the high-
dimensionality and the large dataset size of many ERM problems
(e.g., in deep learning), the stochastic gradient descent (SGD) method
has become the most widely adopted method. The SGD method
1 = wğ‘˜ âˆ’
can be written as the following iterative process: wğ‘˜
âˆˆQğ‘˜ gğ‘–
ğ‘„
ğœ‚ğ‘˜ /
ğ‘˜ , where ğœ‚ğ‘˜ denotes the learning rate in the ğ‘˜-th
ğ‘–
(
Qğ‘˜ represents the mini-batch in the ğ‘˜-th iteration with
iteration,
Ã

w, ğ›¿ğ‘–
)
w, ğ›¿ğ‘–
)

Ã

+

)

4.1 System Model
1) Scheduling Model: Consider a multi-tenant GPU cluster that
contains a set of servers
. Each server is equipped with a set of
homogeneous (i.e., of equal computation speed) and synchronized
GPUs. The servers in
are connected by a network and the net-
work topology can be modeled as a connected graph. In the be-
= ğ‘‡ time-slots,
ginning of a scheduling horizon
there is a set of RAR-based DDL jobs
waiting to be scheduled
J
is associated with a number
for training over

. Each job ğ‘—

of length

|T |

S

S

T

T

âˆˆ J

MobiHoc â€™22, October 17â€“20, 2022, Seoul, Republic of Korea

Yu and Liu, et al.

No concurrent jobs

Server 1

Server 2

Two concurrent jobs

Server 1

Server 2

Finally, to ensure that no workers should be allocated for non-
active jobs and positive integer number of workers should be as-
signed to active jobs, we have:

Job 1

Job 2

Job 1

Job 2

(a) Without communication contention

(b) With communication contention

Figure 2: An example of worker placement.

of GPUs ğº ğ‘— and a total number of training iterations ğ¹ ğ‘— from its
users, both of which are requested by its users.1

In this paper, we consider the â€œgang-schedulingâ€ discipline that
is widely adopted in practical large-scale GPU clusters [7, 10, 19].
Under gang scheduling, all workers (i.e., GPUs) of an RAR-based
DDL job should be allocated simultaneously. Moreover, once a job
is scheduled to start, all GPUs allocated for this job will run to the
jobâ€™s completion and no preemption/migration is allowed.2 Upon
the jobâ€™s completion, the occupied resource will also be released
simultaneously. Each GPU can only be occupied by one worker
of some job at any given time. As shown in Fig. 2, the workers
of a job can be allocated within a single server or across multiple
servers, as long as there exists a path in the underlying network
that connects these workers and forms a ring topology to perform
the RAR process. Note that Fig. 2(a) allocates the workers in the
same server for each job, thus having no communication overhead.
On the contrary, Fig. 2(b) allocates workers across diï¬€erent servers
for each job, which introduces communication contention when
the two jobs happen to perform RAR communication concurrently.
In this system, the control decisions of the scheduler are: i) de-
termine a feasible scheduling for all jobs in
subject to network
resource capacity; and ii) determine each jobâ€™s starting time. Specif-
ically, consider an RAR-based DDL job ğ‘— scheduled with ğ‘¤ ğ‘— work-
Z+ denote the number
ers and its gradient size is ğ‘š ğ‘— . Let ğ‘¦ ğ‘—ğ‘ 
of GPUs scheduled for job ğ‘— on server ğ‘  in time-slot ğ‘¡
. Then, a
scheduling decision in time-slot ğ‘¡ can be fully deï¬ned by the vec-
. Let ğ‘ ğ‘— = arg minğ‘¡
tor y
be the
starting time of job ğ‘— (to be determined) by the scheduling and let
ğ‘‡ğ‘— be the resultant completion time of job ğ‘—. Let
âˆˆ
represent the set of active jobs (jobs being executed) in
ğ‘ ğ‘— ,ğ‘‡ğ‘—
[
time slot ğ‘¡. Clearly, to satisfy the ğº ğ‘— number of GPUs requested
for job ğ‘— during its training time, we have:

ğ‘¦ ğ‘—ğ‘ 
{

ğ‘¡
J [

ğ‘¦ ğ‘—ğ‘ 
[

> 0,

âˆˆ T

ğ‘ 
âˆ€

] âˆˆ

ğ‘¡
[

ğ‘¡
[

ğ‘¡
[

ğ‘¡
[

ğ‘—, ğ‘ 

]}

J

,

,

âˆ€

}

{

]

]

]

]

]

ğ‘¡

ğ‘—

,

|

ğ‘¦ ğ‘—ğ‘ 

ğ‘¡
[

]

= ğº ğ‘— ,

ğ‘—

âˆ€

ğ‘¡
âˆˆ J [

]

, ğ‘¡

.

âˆˆ T

(1)

Ã•ğ‘ 
âˆˆS

Also, scheduling decisions y
ğ‘¡ are subject to GPU resource con-
âˆ€
straints. Let ğ‘‚ğ‘  represent the GPU capacity of server ğ‘ . To ensure
that the allocated GPUs do not exceed each serverâ€™s limit, we have:

ğ‘¡
[

]

,

ğ‘¦ ğ‘—ğ‘ 

ğ‘¡
[

] â‰¤

ğ‘‚ğ‘ ,

ğ‘ 
âˆ€

, ğ‘¡

.

âˆˆ T

âˆˆ S

Ã•ğ‘—
ğ‘¡
âˆˆ J [

]

Also, under the non-preemptive gang scheduling, we have:

ğ‘¦ ğ‘—ğ‘ 

ğ‘¡
[

]

= ğ‘¦ ğ‘—ğ‘ 

ğ‘¡
[

1

,

]

âˆ’

ğ‘ 
âˆ€

, ğ‘—

ğ‘¡
âˆˆ J [

]

âˆˆ S

, ğ‘ ğ‘— < ğ‘¡

ğ‘‡ğ‘— .

â‰¤

(2)

(3)

1In practice, to prevent spending excessively long time waiting for the training pro-
cess of a DDL job to converge, a maximum number of training iterations is usually
given.
2Besides the overhead and complication added for both software and hardware, it
has been shown that frequent job preemption and migration may lead to signiï¬cant
performance degradation [7].

ğ‘¦ ğ‘—ğ‘ 
ğ‘¦ ğ‘—ğ‘ 

]

ğ‘¡
[
ğ‘¡
[

= 0,

Z++,

ğ‘ 
âˆ€

, ğ‘— âˆ‰

, ğ‘¡

ğ‘¡
J [

]

ğ‘¡
âˆˆ J [

, ğ‘—

âˆˆ S
ğ‘ 
âˆ€

âˆˆ T
, ğ‘¡

.

,

(4)

(5)

]

] âˆˆ

âˆˆ T

âˆˆ S
2) Communication Contention Modeling: With the above
scheduling model, we are now in a position to present our com-
munication contention model. We assume that no communication
contention will be introduced if at most one server is used for the
job. For example, in Fig. 2(a), jobs 1 and 2 both use intra-server
communication and does not incur any communication contention.
By contrast, in Fig. 2(b), jobs 1 and 2 induce communication con-
tention since they both compete for inter-server link bandwidth
between servers 1 and 2. We let ğ‘ ğ‘—
denote the largest number
of concurrently running jobs that share an inter-server communi-
cation link with job ğ‘— in time slot ğ‘¡, which can be computed as:

ğ‘¡
[

]

ğ‘ ğ‘—

ğ‘¡
[

]

= max
ğ‘ 
âˆˆS

1

{

(cid:26)

0 < ğ‘¦ ğ‘—ğ‘ 

< ğº ğ‘—

ğ‘¡
[

]

} Ã•ğ‘—â€² âˆˆ J [

ğ‘¡

]

{

1

0 <ğ‘¦ ğ‘—â€²ğ‘  [
ğ‘¡
, ğ‘¡
ğ‘¡
âˆˆ J [

]
ğ‘—

,

< ğº ğ‘—â€² }(cid:27)
(6)
.

]

{

ğ‘¡
[

âˆˆ T

< ğº ğ‘—

0 < ğ‘¦ ğ‘—ğ‘ 

}
0 < ğ‘¦ ğ‘—â€²ğ‘  [
ğ‘¡

âˆ€
In (6), the ï¬rst term 1
indicates that only active
]
jobs using inter-server communication on server ğ‘  will be consid-
1
ered. The second term
represents
]
the number of diï¬€erent jobs that compete for inter-server commu-
nication on server ğ‘ . Since job ğ‘— may not be transmitting at all
times (due to switching between communication and computation
modes), we let ğ‘˜ ğ‘—
be the actual largest number of contending
jobs on average with job ğ‘— in time-slot ğ‘¡, which can be assumed to
be statistically linearly proportional to ğ‘ ğ‘—

< ğº ğ‘—â€² }

ğ‘¡
ğ‘—â€² âˆˆ J [

, i.e.,

ğ‘¡
[

Ã

{

]

]

ğ‘˜ ğ‘—

= ğœ‰1ğ‘ ğ‘—

ğ‘¡
ğ‘¡
[
[
is a positive constant.

âˆ€

]

]

ğ‘—

,

ğ‘¡
[
ğ‘¡
âˆˆ J [

]

]
, ğ‘¡

,

âˆˆ T

(7)

where ğœ‰1

0, 1

]

âˆˆ (

3) RAR-Based DDL Training Completion Time Modeling:
To evaluate the job completion time ğ‘‡ğ‘— of job ğ‘—, we need to ï¬rst
characterize the RAR training speed. Note that the per-iteration
RAR operation time of each DDL job can be decomposed into three
parts: i) information exchange time, ii) computation time, and iii)
communication overhead. Next, we will model the operation time
of each part individually.

]

y

])

ğ‘¡
[

ğ‘¡
[

2-1) Information Exchange Time: We use ğµ {

ğœ” ğ‘—,1,ğœ” ğ‘—,2 } (
to de-
note the bandwidth between two successive workers ğœ” ğ‘—,1 and ğœ” ğ‘—,2
in job ğ‘—â€™s ring in time-slot ğ‘¡ under a scheduling decision y
, where
ğœ” ğ‘—,2 is the downstream worker of ğœ” ğ‘—,1. Note that, unlike [22], we
do not reserve bandwidth for each job in this paper, and this band-
width is determined by communication contention with other jobs
,
under the scheduling decisions y
ğ‘¡
[
]
ğœ” ğ‘—,1,ğœ” ğ‘—,2 } (
min
ğ‘¡
])
[
bottleneck link of job ğ‘— under scheduling decision y
ğ‘—
L
is the set of all links of job ğ‘—. Recall from Section 3 that the amount
of information exchanged in each time-slot can be computed as
2ğ‘š ğ‘—
. Thus, the number of time-slots for information ex-
ğ‘¤ğ‘— (
change can be computed as

(see Fig. 2(b)). We let ğµ ğ‘—
represent the bandwidth of the

ğ‘¤ ğ‘—
Clearly, the bottleneck link of job ğ‘— occurs in those links that are
shared by the largest number of other concurrently running jobs.

ğœ” ğ‘—,1,ğœ” ğ‘—,2) âˆˆL ğ‘— ğµ {

2ğ‘š ğ‘—
ğ‘¤ğ‘— (

, where

ğ‘¤ ğ‘—

ğ‘¡
[

ğ‘¡
[

ğ‘¡
[

ğµ ğ‘—

)/

])

])

âˆ’

âˆ’

y

y

y

1

1

)

]

(

(

.

(

On Scheduling Ring-All-Reduce Learning Jobs in Multi-Tenant GPU Clusters with Communication Contention

MobiHoc â€™22, October 17â€“20, 2022, Seoul, Republic of Korea

)

(

(

)

]

]

(

(

1

y

y

âˆˆ

])

])

])

])

ğ‘˜ ğ‘—

â‰«

ğ‘¡
[

ğ‘¡
[

ğ‘¡
[

ğ‘¡
[

ğ‘¡
[

ğ‘¡
[

] +

ğ›¼
ğ‘¡
[

ğ‘˜ ğ‘—
/

ğœ¶ , ğ‘˜ ğ‘—

ğœ¶ , ğ‘˜ ğ‘—

] +
ğ‘˜ ğ‘—
(

(
] âˆ’

= 1 and ii) ğ‘“

share of bandwidth if ğ‘˜ ğ‘—

ğ‘ğ‘’ in practice [16, 23]. Recall that ğ‘˜ ğ‘—

We let ğ‘ğ‘’ and ğ‘ğ‘– be the link bandwidth between and within servers,
respectively, where ğ‘ğ‘–
ğ‘¡
]
[
denotes the actual largest number of contending jobs on average
with job ğ‘— in time-slot ğ‘¡. Ideally, each job on this bottleneck link
has an equal share of bandwidth ğ‘ğ‘’
under communication
contention. In practice, however, the bandwidth performance of-
ten degrades when multiple jobs compete for a link, which results
in each job having less than ğ‘ğ‘’
ğ‘¡
ğ‘˜ ğ‘—
] â‰¥
[
/
to rep-
2 [19]. To model this eï¬€ect, we use a function ğ‘“
(
resent the â€œbandwidth sharing degradation factorâ€ under commu-
Rğ‘‘ captures all parameters that
nication contention, where ğœ¶
satisï¬es the
could lead to degradation. We assume that ğ‘“
ğ‘¡
[
])
ğœ¶ , 1
ğœ¶ , ğ‘˜ ğ‘—
following properties: i) ğ‘“
is an in-
ğ‘¡
])
(
[
(
ğœ¶ , ğ‘˜ ğ‘—
. For example, if ğ‘“
is a linear
ğ‘¡
ğ‘¡
creasing function of ğ‘˜ ğ‘—
])
[
[
]
ğœ¶ , ğ‘˜ ğ‘—
= ğ‘ğ‘’
=
function ğ‘˜ ğ‘—
ğ‘¡
ğ‘“
, then ğµ ğ‘—
ğ‘¡
ğ‘¡
] âˆ’
[
[
[
(
/
ğ‘ğ‘’
.
1
ğ›¼
ğ‘˜ ğ‘—
))

/(
Recall that in the special case where all workers of a job ğ‘— are
co-located within a single server, there is no contention. Further,
intra-server communication is typically enabled by fast intercon-
= ğ‘ğ‘– .
nect techniques (e.g., NVLink [4]). Hence, we have ğµ ğ‘—
2-2) Computation Time: To characterize the computation time
in the RAR operation, we use ğ¶ to denote the computational speed
of a GPU unit (deï¬ned as the amount of data processed in each
time-slot). Since there are
amount of data for reduction
âˆ’
in each RAR operation, the number of time-slots to complete all
ğ‘š ğ‘—
reductions can be computed as
ğ¶. In addition to the
ğ‘¤ğ‘— (
all-reduce operation time, the computation time also includes the
forward pass (FP) time and the backward pass (BP) time to compute
a stochastic gradient. We let Î”ğ‘“
ğ‘— (Î”ğ‘
ğ‘— ) denote the duration of one FP
(BP) of job ğ‘—. Note that the FP time is proportional to the mini-batch
size ğ‘€ğ‘— , which can be calculated as Î”ğ‘“
ğ‘— ğ‘€ğ‘— (the size of a mini-batch
multiplied by the FP processing time of one sample). Meanwhile,
the BP time Î”ğ‘
ğ‘— is usually not relevant to the mini-batch size ğ‘€ğ‘—
and is typically ï¬xed. Thus, the total number of time-slots for per-
iteration computation can be computed as
Î”ğ‘
ğ‘— .
2-3) Communication Overhead: In practice, it has been observed
that typically, the more servers an RAR-based DDL job uses to
perform the training, the larger the latency due to communica-
tion overhead (e.g., ACK time for message transmission, negotia-
tion time among all workers before conducting all-reduce [15]) can
be [19]. In this paper, we use ğ›¾ ğ‘—
to denote the latency of job
yğ‘—
ğ‘— caused by communication overhead in time-slot ğ‘¡. We assume
that the latency is linear proportional to the number of servers in
>
use, i.e., ğ›¾ ğ‘—
ğ‘¡
ğ‘¦ ğ‘—ğ‘ 
[
{
is a positive constant.
ğ‘ 
0,
âˆ€
Lastly, putting 2-1) â€“ 2-3) together, we can compute the RAR
as follows:

operation time of job ğ‘— under scheduling decision y

, where yğ‘—

(
and ğœ‰2

ğ‘¡
[
âˆˆ (

ğ‘š ğ‘—
ğ‘¤ğ‘— (

ğ‘š ğ‘—
ğ‘¤ğ‘— (

ğ‘¦ ğ‘—ğ‘ 
[

ğ‘ 
Ã

])
0, 1

= ğœ‰2

ğ‘— ğ‘€ğ‘—

> 0

ğ‘¤ ğ‘—

ğ‘¤ ğ‘—

ğ‘¤ ğ‘—

Î”ğ‘“

ğ‘¡
[

ğ‘¡
[

ğ‘¡
[

yğ‘—

)/

)/

])

1

âˆ’

âˆ’

ğ¶

=

+

+

1

1

1

}

)

]

]

]

]

(

]

ğ‘¡
[

]

=

ğœ ğ‘—

ğ‘¡
[

]

ğ‘š ğ‘—
ğ‘¤ğ‘—

Â·
ğµ ğ‘—

2

ğ‘¤ ğ‘—
(
y

ğ‘¡
[

1

)

âˆ’

ğ‘š ğ‘—
ğ‘¤ğ‘—

+

ğ‘¤ ğ‘—

1

)

âˆ’

Â· (
ğ¶

(

])
(i.e., the number of mini-
Hence, the RAR training speed ğœ™ ğ‘—
batch iterations completed by job ğ‘—) in time-slot ğ‘¡ can be computed
. Recall that ğ¹ ğ‘— is the requested number of
as ğœ™ ğ‘—

ğ‘¡
[

ğœ ğ‘—

,

]

1

ğ‘¡
[

]

ğ‘¡
[

])âˆ’

âŒ‹

âŒŠ(

ğ›¾ ğ‘—
+

(

yğ‘—

ğ‘¡
[

])+

Î”ğ‘“

ğ‘— ğ‘€ğ‘—

+

Î”ğ‘
ğ‘— .

(8)

Table 1: Notation.

]

ğº ğ‘—
T/
/
N
S
ğ‘¡
J [
]
ğ‘˜ ğ‘— [
ğ‘¡
ğ‘¡
ğœğ‘— [
]
ğ‘¡
ğ‘¦ ğ‘—ğ‘  [
]
ğ‘‚ğ‘ 
ğ‘‡ğ‘—
ğ‘ ğ‘— /
Y
yğ‘˜
ğ‘—
yğ‘˜
ğ‘— )
yğ‘˜
ğ‘— )
yğ‘˜
ğ‘— )
G (
ğ‘¥ğ‘˜
ğ‘—
ğ‘Š ğ‘˜
ğ‘—ğ‘”
ğ‘ˆ ğ‘”
ğ‘ 

ğœŒ
Ë†ğœŒ

(
(

Scheduling time horizon/# of GPUs requested by job j
Set of servers/GPUs in the cluster
The set of active jobs in time-slot t
Actual largest number of contending jobs on average with
job j in time-slot ğ‘¡
Per-iteration training time of job j in time-slot t
# of GPUs scheduled on server ğ‘  for job j in time-slot ğ‘¡
GPU capacity of server ğ‘ 
Starting/completion time slot of job ğ‘—
The set of feasible scheduling schemes over
A schedule of job ğ‘— indexed with ğ‘˜
Actual execution time of job ğ‘— when schedule yğ‘˜
Estimated execution time of job ğ‘— when schedule yğ‘˜
Set of GPUs allocated for job ğ‘— when schedule yğ‘˜
Indicate whether job ğ‘— follows schedule yğ‘˜
ğ‘— or not
Execution time added to GPU ğ‘” by job ğ‘— if job j follows yğ‘˜
ğ‘—
The accumulative execution time of worker ğ‘” on server ğ‘ 

ğ‘— is used

ğ‘— is used

T

ğ‘— is used

ğ‘¡
[

] â‰¥

ğ‘¡
âˆˆ J [

]

âˆ€

+

ğ‘¡

ğ‘¡
(cid:8) Ã•

iterations for training job ğ‘—. Thus, job ğ‘—â€™s completion time can be
calculated as:
ğ‘‡ğ‘— = ğ‘ ğ‘—

arg min

(9)

ğœ™ ğ‘—

ğ¹ ğ‘—

ğ‘—

,

.

(cid:9)

âˆˆT
4.2 Problem Statement
In this paper, our goal is to determine the scheduling decisions
y
to minimize the makespan (i.e., maxğ‘— ğ‘‡ğ‘— ), which is one of the
most useful metrics to measure the eï¬ƒciency of multi-tenant GPU
clusters [5, 6]. Putting all modeling constraints and the objective
together, the RAR-based DDL job scheduling problem (RAR-DDLS)
can be formulated as the following optimization problem:

ğ‘¡
[

]

RAR-DDLS:

min
,
ğ‘¡
ğ‘¦ ğ‘—ğ‘  [
âˆ€
]

ğ‘—,ğ‘ ,ğ‘¡

ğ‘‡ğ‘—

max
ğ‘—
âˆˆ J

subject to Constraints

1
) âˆ’ (

(

9

.

)

We note that Problem RAR-DDLS is an integer non-convex pro-
gram with packing and covering constraints, which is NP-Hard. In
addition, the non-convex constraint in (6) involves indicator func-
tions and the max operator, which cannot be written in a closed-
from expression and hence is not amenable to conventional opti-
mization techniques. Due to these challenges, we will pursue an
approximation algorithmic approach in Section 5 that oï¬€ers prov-
able approximation ratio guarantee. To conclude this section, we
summarize the key notations in this paper in Table 1.

5 SOLUTION APPROACH
As mentioned in Section 4, a key challenge to solve Problem RAR-
DDLS is that, due to the mixed covering- and packing-type con-
straints, the number of job scheduling combinations grows expo-
nentially as the number of servers/jobs increases. Thus, it is com-
putationally prohibitive to enumerate all possible combinations be-
fore the scheduler decides when to start and which GPU(s) should
be allocated to achieve the optimal scheduling. Exacerbating the
problem is the fact that communication contention renders a mixed-
integer bilinear structure in (6), making it intractable to express
ğ‘ ğ‘—
in closed-form. Due to these challenges, it is diï¬ƒcult to di-
rectly solve Problem RAR-DDLS based on its original formulation.

ğ‘¡
[

]

MobiHoc â€™22, October 17â€“20, 2022, Seoul, Republic of Korea

Yu and Liu, et al.

No

Search for a
y
schedule 

Ï„j[t], âˆ€t

Evaluate                to 
 estimate makespan

 A â€˜â€˜good enoughâ€™â€™
      schedule ?

Return
y

Yes

Figure 3: Basic idea for solving Problem RAR-DDLS.

To overcome this challenge, we propose the following â€œindirectâ€
approach to solve Problem RAR-DDLS.

(

]

]

y

=

ğ‘¡
[

ğ‘¡
[

}
is determined by ğµ ğ‘—

]
increases as ğ‘˜ ğ‘—
> 0

1) Basic Idea: First, we note that, although not in closed-form
expressions, the per-iteration time ğœ ğ‘—
for each job can be com-
puted in polynomial time according to (6)-(8) once a schedule (i.e.,
ğ‘—, ğ‘ 
) is given. Speciï¬cally, we note that the per-
y
,
ğ‘¦ ğ‘—ğ‘ 
ğ‘¡
âˆ€
{
]
[
ğ‘¡
ğ‘¡
yğ‘—
iteration time ğœ ğ‘—
. More-
ğ‘¡
[
])
[
[
ğœ¶ , ğ‘˜ ğ‘—
in-
ğ‘¡
yğ‘—
gets larger, and ğ›¾ ğ‘—
ğ‘¡
over, ğ‘“
[
(
])
[
])
1
creases as
can be
ğ‘¡
grows. Thus, the range of ğœ ğ‘—
ğ‘¦ ğ‘—ğ‘ 
ğ‘ 
[
{
estimated. The largest number of ğ‘˜ ğ‘—
is maxğ‘  ğ‘‚ğ‘  , i.e., the worst
Ã
case would be each job places one of its workers into the server
with the biggest capacity and they all compete for the bandwidth.
ğ‘ğ‘’
Thus, we can have ğµ ğ‘—
. In addition,
ğ‘“
/
1
we have
1, ğº ğ‘—
ğ‘ 
1
and
ğ‘¦ ğ‘—ğ‘ 
Ã
{
respectively, we can attain the lower and upper bounds.

ğ‘¡
y
(
[
> 0
])
with their lower and upper bounds in Eqn. (8),

ğœ¶ , maxğ‘  ğ‘‚ğ‘ 
(
)
. Then by plugging ğµ ğ‘—
]

ğ‘¦ ğ‘—ğ‘ 
{
ğ‘¡
[

ğ‘¡
]
[
> 0
}

]) âˆˆ [
} âˆˆ [

and ğ›¾ ğ‘—

, ğ‘ğ‘–

ğ‘¡
[

ğ‘¡
[

ğ‘¡
[

ğ‘¡
[

])

y

}

]

(

]

]

(

]

]

(

]

ğ‘ 
Ã

,

]

âˆ€

ğ‘¡
[

The above insight suggests that we can solve Problem RAR-
DDLS via the following search-based approach to circumvent the
structural diï¬ƒculty in (6)-(8) . We can ï¬rst search for a schedule y,
then ğœ ğ‘—
ğ‘¡ can be eï¬ƒciently evaluated to estimate the makespan.
Then, we repeat the process until we ï¬nd a â€œgood enoughâ€ sched-
ule. Therefore, we can have the algorithmic framework as shown
in Fig. 3 to obtain an approximate makespan if the search space is
given. Clearly, the search space of y remains huge and diï¬ƒcult to
sample. Nonetheless, in what follows, we show that Problem RAR-
DDLS can be reformulated to facilitate this search-based approach.
2) Problem Reformulation: In order to enable the search of a
schedule, we ï¬rst reformulate Problem RAR-DDLS by introducing
following notations. We let
be the set of all GPUs
be the set of feasible schedul-
in the cluster. Let
ing schemes for the jobs to be scheduled, where yğ‘˜ =
yğ‘˜
1, . . . , yğ‘˜
ğ½ }
{
and yğ‘˜
ğ‘¦ğ‘˜
. Note that, with a slight
, ğ‘¡
ğ‘¡
ğ‘—
ğ‘—ğ‘  [
{
âˆˆ T } âˆˆ
abuse of notation, we use ğ‘¦ğ‘˜
here as a constant (not a variable)
ğ‘¡
ğ‘—ğ‘  [
to denote the number of workers allocated for job ğ‘— on server ğ‘ 
in time-slot ğ‘¡ if schedule yğ‘˜ is used. We also use ğœŒ ğ‘—
to de-
note the execution time of job ğ‘— if schedule yğ‘˜ is used. Also, we
,
denote the starting time of job ğ‘— under schedule yğ‘˜ as ğ‘ ğ‘—
. Let ğ‘¥ğ‘˜
arg min
ğ‘ 
be the binary variable to
âˆƒ
indicate whether job ğ‘— follows schedule yğ‘˜ (ğ‘¥ğ‘˜
= 1) or not (ğ‘¥ğ‘˜
= 0).
ğ‘—
ğ‘—
Then Problem RAR-DDLS can be reformulated as the following in-
teger linear program (ILP):

=
N
y1, . . . , y |Y | }
Zğ‘†
+

ğ‘¦ğ‘˜
ğ‘¡
ğ‘—ğ‘  [
|

1, . . . , ğ‘

ğ‘— âˆˆ {

> 0,

âˆˆ S

ğ‘ 
âˆ€

0, 1

ğ‘¡
{

yğ‘˜

yğ‘˜

Y

=

=

Ã—

{

}

{

}

}

ğ‘‡

]

]

(

)

]

)

(

,

min
ğ‘¥ğ‘˜
ğ‘—,ğ‘˜
ğ‘— ,

âˆ€

max
ğ‘—

ğ‘¥ğ‘˜
ğ‘—

ğ‘ ğ‘—
(cid:0)

(

yğ‘˜

yğ‘˜

ğœŒ ğ‘—

(

) +

)

(cid:1)

subject to.

|Y | }
,

Ã•ğ‘˜
1,...,
âˆˆ{
= ğ‘¥ğ‘˜
ğ‘¥ğ‘˜
ğ‘—
ğ‘—â€²
ğ‘¥ğ‘˜
0, 1
ğ‘— âˆˆ {

,

}

ğ‘¥ğ‘˜
ğ‘— = 1,

ğ‘—

âˆ€

,

âˆˆ J

âˆ€

, ğ‘˜

ğ‘—, ğ‘— â€² âˆˆ J
, ğ‘˜
ğ‘—

âˆ€

âˆˆ J

1, . . . ,

âˆˆ {

1, . . . ,

âˆˆ {

,

|Y|}
.

|Y|}

(10)

(11)

(12)

(13)

Constraint (11) ensures that exactly one schedule is chosen. Con-
straint (12) ensures that all jobs use the same schedule yğ‘˜ . We note
that, although Problem (10) has a simpler structure compared to
Problem RAR-DDLS, it hides the complexity in the dimensionality
, which is intractable to explore.
of the exponential search space
Y
However, based on this reformulated problem, we will show next
that it is possible for one to identify a â€œgood enoughâ€ schedule such
that the makespan can be upper bounded.

Unfortunately, Problem (10) remains an NP-hard problem. We
state this formally in Theorem 1, which can be proved based on
the reduction to the vertex coloring problem (VCP).

Theorem 1. Let ğ‘›ğ‘” = maxğ‘— ğº ğ‘— . Solving Problem (10) to within
-approximate ratio is NP-hard even when the exact

an ğ‘‚

log ğ‘›ğ‘”
2âˆšlog log ğ‘›ğ‘” )

(

processing time of each job is available.

(

)

yğ‘˜

Proof. Here, we consider the special case with all jobs having a
= 1). We ï¬rst show that VCP can be re-
unit processing time (ğœŒ
duced to the job scheduling problem in Problem (14) in polynomial
time. Given an instance ğ¼ of VCP, i.e., given a graph ğº =
with
bounded degree ğ‘›ğ‘”, we can construct our job scheduling problem
, where it
as follows: i) For each node ğ‘£
=
has only one schedule ğ‘¦ ğ‘—ğ‘£
ğ¸, we add a
worker ğ‘¤ğ‘¢,ğ‘£ to
ğ‘¤ğ‘¢,ğ‘£
}
and ğ‘¦ ğ‘—ğ‘£
. If the graph ğº â€²ğ‘  maximum degree is no
greater than ğ‘›ğ‘”, then the maximum number of workers that can
be allocated to each job is also ğ‘›ğ‘”.

ğ‘‰ , we create a job ğ‘—ğ‘£
ğ‘¢, ğ‘£
(
. Also, update the scheduling as ğ‘¦ ğ‘—ğ‘¢

âˆˆ J
) âˆˆ
= ğ‘¦ ğ‘—ğ‘¢ âˆª{

âˆˆ
; ii) For each edge

S
= ğ‘¦ ğ‘—ğ‘£ âˆª {

ğ‘‰ , ğ¸
(

ğ‘¤ğ‘¢,ğ‘£

âˆ…

}

)

With this reduction, we next show the solution of VCP can be
translated to the solution of Problem (14), and vice verse. First, re-
call that each job has one unit processing time. Thus, all jobs should
be executed inside a unit time interval (
, . . .). If we have
the solution to VCP, then we can schedule jobs with the same color
in the same interval. Also, if we have the solution to the job sched-
uling problem, then the jobs in the same interval can be marked as
the same color. Hence, ï¬nding an optimal solution of Problem (14)
is equivalent to ï¬nd an optimal solution of VCP.

0, 1

1, 2

)

[

[

)

,

Similarly, given an instance of job scheduling, we can construct
an instance of VCP, where the makespan equals to the number of
colors. Therefore, if we have an ğ›¼-approximate solution to the job
scheduling problem, we have an ğ›¼-approximate solution to VCP.
However, with the graph of degree at most ğ‘›ğ‘”, it is known that
coloring a 2âˆšlog log ğ‘›ğ‘” -colorable graph with ğ‘‚
colors is NP-
(cid:3)
Hard. This completes the proof.

log ğ‘›ğ‘”

(

)

The hardness result in Theorem 1 suggests that solving Prob-
lem (10) necessitates the design of approximation algorithms, which
is our goal in algorithm development next.

ğ‘—

)

)

(

(

yğ‘˜

ğ‘— ğœŒ ğ‘—

= ğ‘¥ğ‘˜

3) Identify a Scheduling with Bounded Makespan: We let
be the set of GPUs allocated for job ğ‘— when schedule yğ‘˜ is
yğ‘˜
G
used. We use ğ‘Š ğ‘˜
to denote the execution time added
ğ‘—ğ‘”
to GPU ğ‘” by job ğ‘— if job ğ‘— follows schedule yğ‘˜ . Since each job ğ‘—
only chooses one schedule, the total execution time of GPU ğ‘” can
be computed as ğ‘Šğ‘” =
ğ‘—ğ‘”. However, due to communication
is hard to evaluate in
contention, the exact processing time ğœŒ ğ‘—
computing ğ‘Š ğ‘˜
yğ‘˜
(
)
1 and
can be bounded as Ë†ğœŒ ğ‘—

ğ‘—ğ‘”. Fortunately, the estimated processing time Ë†ğœŒ ğ‘—

for some ğ‘™

ğ‘˜ ğ‘Š ğ‘˜

, ğ‘¢ğœŒ ğ‘—

ğ‘™ ğœŒ ğ‘—

yğ‘˜

yğ‘˜

yğ‘˜

yğ‘˜

Ã

Ã

(

)

ğ‘—

(

) âˆˆ [

(

)

(

)]

â‰¤

On Scheduling Ring-All-Reduce Learning Jobs in Multi-Tenant GPU Clusters with Communication Contention

MobiHoc â€™22, October 17â€“20, 2022, Seoul, Republic of Korea

:

Y

(14)

â‰¥

1, since ğœ ğ‘—

ğ‘¢
replace ğœŒ ğ‘—
ğœ‹ that solves the following ILP to choose one schedule from

ğ‘¡
[
when computing ğ‘Š ğ‘˜

yğ‘˜
to
)ğ‘¢
ğ‘—ğ‘”. Consider a search algorithm

is bounded. Here, we use

Ë†ğœŒ ğ‘— (

yğ‘˜

yğ‘˜

ğœŒ ğ‘—

â‰¤

)

]

)

(

(

min

subject to. Ë†ğ‘Š ğ‘˜
ğ‘—ğ‘”

1

âˆ’
= ğ‘¥ğ‘˜
ğ‘—

Ë†ğœŒ ğ‘—

yğ‘˜
(
ğ‘¢

)

,

âˆ€
Ë†ğ‘Š ğ‘˜

ğ‘—

, ğ‘˜

âˆˆ {

âˆˆ J

1, . . . ,

ğ‘—ğ‘” â‰¤

ğœƒğ‘¢,

ğ‘”
âˆ€

âˆˆ N

, ğ‘”

âˆˆ N

,

(15)

(16)

|Y|}
,

Ã•ğ‘—
Ã•ğ‘˜
1,...,
âˆˆ J
âˆˆ{
Constraints

|Y | }
11

(

.

13
)

) âˆ’ (

Note that Problem (14) has no objective function to be optimized
since we are only interested in whether a feasible solution no greater
than a given maximum execution time limit ğœƒğ‘¢ exists (ğœƒğ‘¢ depends
on parameter ğ‘¢). Constraints (15)â€“(16) ensure that no GPUâ€™s exe-
cution time would exceed ğœƒğ‘¢ . Let ğ‘Š ğœ‹
ğ‘Šğ‘” be the max-
imum execution time of all GPUs returned by algorithm ğœ‹. Due to

max = maxğ‘”

âˆˆN

, the solution of ğœ‹ ï¬nds a lower bound
max, which is also a lower bound of the makespan under ğœ‹

the use of estimated
of ğ‘Š ğœ‹
(due to potential idling resulted from synchronization barrier).

Ë†ğœŒ ğ‘— (

yğ‘˜
)ğ‘¢

Note that for any feasible scheduling with the upper bound ğœƒğ‘¢
for Problem (14), we can ï¬nd a corresponding feasible solution for
Problem (10) by setting ğ‘¥ğ‘˜
= 1 if job ğ‘— follows schedule yğ‘˜ ; other-
ğ‘—
wise, set ğ‘¥ğ‘˜
= 0. Thus, the challenge of solving Problem (10) be-
ğ‘—
comes ï¬nding a tightest execution time limit ğœƒğ‘¢ for Problem (14),
which is relatively easy since there is no need to explore the expo-
nential search space of schedules

Y
It is insightful to understand the choice of ğœƒğ‘¢ in Problem (14).
On one hand, if ğœƒğ‘¢ is too small, Problem (14) could be infeasible,
and no scheduling for Problem (10) can be found. On the other
hand, when ğœƒğ‘¢ is too large, then all schedulings can be considered,
and the gap between the optimal maximum execution time and the
optimal makespan can be large, thus no meaningful lower bound of
ğ‘Š ğœ‹
max can be found. Fortunately, since determining an appropriate
ğœƒğ‘¢ is a univariate search, we can simply use the bisection method
to eï¬ƒciently ï¬nd the minimum ğœƒğ‘¢ feasible to Problem (14).

.

]

âˆˆ [

1, ğ‘›ğ‘”

4) Algorithm Description: We next present our scheduling al-
gorithm based on bisection to search ğœƒğ‘¢ and the smallest job ï¬rst
strategy to solve Problem (14) for a given ğœƒğ‘¢ . Note that if a jobâ€™s
ring of workers is scheduled over a large number of servers, it
may potentially worsen communication contention with concur-
rent jobs and its communication overhead could be large. There-
fore, to control the number of active servers, we use a threshold
to control the number of maximum servers
parameter ğœ…
for scheduling jobs. We summarize our scheduling approach in Al-
gorithm 1. The intuition behind Algorithm 1 is that: 1) When the
job is small (i.e., ğº ğ‘—
ğœ…), we prefer to pack the job into servers
whose GPUs are already occupied by some other jobs rather than
opening new server(s) to host its workers. Since the job is small, the
induced contention is mild by using the shared servers. Further, by
packing its workers to these servers, we can avoid fragmentation
introduced by a small job and save space for larger jobs that will be
scheduled next. 2) If ğº ğ‘— > ğœ…, we prefer to allocate the jobâ€™s work-
ers to new server(s). This is because shared servers may only have
limited available GPU(s), and in order to gang-schedule a large job,

â‰¤

(

ğ‘  , Ë†ğœŒ ğ‘—

Algorithm 1: Smallest Job First with Balanced Contention
and Overhead (SJF-BCO).
, ğ‘ˆ ğ‘”
yğ‘˜
1 Input:
, ğ‘¢, ğœ† ğ‘— ;
J
)
2 Initialization: Let ğ‘ˆ ğ‘”
0,
ğ‘  â†
3 Sort jobs by ğº ğ‘— in non-decreasing order, and denote as
4 ğ‘š
1, ğ‘Ÿğ‘–ğ‘”â„ğ‘¡
â†
5 while ğ‘™ğ‘’ ğ‘“ ğ‘¡ <= ğ‘Ÿğ‘–ğ‘”â„ğ‘¡ do
ğ‘Ÿğ‘–ğ‘”â„ğ‘¡
6

â†
ğ‘‡ , yğœƒ â† âˆ…
2, ğ‘šğœƒ â†
ğœƒğ‘¢
)/
+
for ğœ… = 1, 2, . . . , maxğ‘— ğº ğ‘— do
, mğ‘˜

, ğ‘™ğ‘’ ğ‘“ ğ‘¡

â† âˆ…

â† (

ğ‘‡ , y

ğ‘™ğ‘’ ğ‘“ ğ‘¡

ğ‘”, ğ‘ ;

â†

ğ‘‡ ;

âˆ€

;

ğ‘  ;

J

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

yğ‘˜
ğœƒ â† âˆ…
for ğ‘— = 1, 2, . . . ,

1;
ğ‘ 

ğœƒ â† âˆ’
|J
ğœ… then

if ğº ğ‘—

â‰¤

do

|

Return yğ‘— , ğ‘‡ğ‘— using Algorithm 2;

else

Return yğ‘— , ğ‘‡ğ‘— using Algorithm 3;

, ğ‘šğ‘˜

ğœƒ â†

max

mğ‘˜

ğœƒ , ğ‘‡ğ‘—

{

;

}

}

then

if yğ‘— ==
âˆ…
break;
yğ‘˜
yğ‘˜
yğ‘—
ğœƒ â†
ğœƒ âˆª {
< ğ‘šğœƒ then
if ğ‘šğ‘˜
ğœƒ
ğ‘šğ‘˜
ğ‘šğœƒ â†
ğœƒ , yğœƒ â†
< ğ‘š then
ğ‘šğœƒ , y
ğœƒğ‘¢

yğœƒ ;
1;

â†
âˆ’

â†

if ğ‘šğœƒ
ğ‘š
â†
ğ‘Ÿğ‘–ğ‘”â„ğ‘¡

else

yğ‘˜
ğœƒ ;

ğ‘™ğ‘’ ğ‘“ ğ‘¡

23
24 return m, y;

â†

ğœƒğ‘¢

1;

+

Algorithm 2: Fragment-Aware First Fit Packing (FA-FFP).

1 Input: A given job ğ‘—,

yğ‘˜
)
available GPUs with execution time not exceed ğœƒğ‘¢ ;

, ğ‘¢, ğœƒğ‘¢ ;

ğ‘  , Ë†ğœŒ ğ‘—

S

(

, ğ‘ˆ ğ‘”

2

G
3 if

4

5

6

ğº ğ‘— then

ğœƒğ‘¢
ğ‘–ğ‘‘ğ‘™ğ‘’ â†
ğœƒğ‘¢
ğ‘–ğ‘‘ğ‘™ğ‘’ | â‰¥
|G
Pick top-ğº ğ‘— workers with least ğ‘ˆ ğ‘”
arg maxğ‘¡
ğ‘‡ğ‘—
ğ‘¦ ğ‘—ğ‘ 
{
â†
ğ‘ˆ ğ‘”
ğ‘ˆ ğ‘”
yğ‘˜
Ë†ğœŒ ğ‘—
ğ‘  +
ğ‘  â†
(
return yğ‘— , ğ‘‡ğ‘— ;

> 0
ğ‘¦ ğ‘—ğ‘ 
|
ğ‘”, ğ‘ 

ğ‘¡
] âˆˆ
[
yğ‘— ;
) âˆˆ

ğ‘¡
[
]
ğ‘¢,
)/

âˆ€(

7
8 if there exists running jobs then
9
10 return

, ğ‘‡ ;
âˆ…

ğ‘  from
yğ‘— ,

ğœƒğ‘¢
ğ‘–ğ‘‘ğ‘™ğ‘’ as yğ‘— ;
G
ğ‘ , ğ‘¡

;

âˆ€

}

Waiting for some job to exit and then goes to Line 2;

a large number of shared servers may be used, which leads to a
high communication overhead.

In Algorithm 1, ğ‘ˆ ğ‘”

1, ğ‘‡

ğ‘  denotes the accumulative execution time of
worker ğ‘” on server ğ‘ . We ï¬rst sort jobs in non-decreasing order of
their sizes ğº ğ‘— in Line 3. We search ğœƒğ‘¢ using the bisection method
in the range
to perform scheduling
(Lines 5-7). We then iterate through each job (Line 9). If its size is
not greater than the threshold ğœ… (Line 10), Algorithm 2 will be used
to do the scheduling (Line 11); otherwise, Algorithm 3 will be called
(Line 13). If no feasible scheduling of job ğ‘— is returned, then we quit
the current loop and update ğœ… (Line 14); otherwise, we will update

, and use the pair

ğœƒğ‘¢, ğœ…

]

[

)

(

MobiHoc â€™22, October 17â€“20, 2022, Seoul, Republic of Korea

Yu and Liu, et al.

;

}

Lemma 2 (Maximum Execution Time Upperbound). Algorithm 1

produces a schedule with the maximum execution time Ë†ğ‘Š Alg1

max = Ëœğœƒğ‘¢ .

yğ‘˜
(
ğ‘” ğ‘ˆ ğ‘”
ğ‘  /
Ã

Algorithm 3: Least Busy Server-GPU First (LBSGF).
1 Input: A given job ğ‘—, ğ‘ˆ ğ‘”
by
2 Sort the server set

, ğ‘¢, ğœ† ğ‘— ;
)
ğ‘‚ğ‘  in non-decreasing order,

ğ‘  , Ë†ğœŒ ğ‘—

S

and choose the top ğ‘š-servers s.t.
denote the selected server set as
ğœƒğ‘¢
ğ‘–ğ‘‘ğ‘™ğ‘’ â† âˆ…

;

ğ‘š
ğ‘ =1 ğ‘‚ğ‘ 
â‰¥
Sğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ ;
Ã

ğœ† ğ‘—ğº ğ‘— , and

âˆˆ Sğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ do
Sort GPUs whose ğ‘ˆ ğ‘”
yğ‘˜
ğœƒğ‘¢ by execution
(
time in non-decreasing order, then append them to

ğ‘¢
)/

ğ‘  +

Ë†ğœŒ ğ‘—

â‰¤

ğœƒğ‘¢
ğ‘–ğ‘‘ğ‘™ğ‘’

;

ğº ğ‘— then

G
ğœƒğ‘¢
ğ‘–ğ‘‘ğ‘™ğ‘’ | â‰¥
|G
Pick top-ğº ğ‘— workers with least ğ‘ˆ ğ‘”
arg maxğ‘¡
ğ‘‡ğ‘—
ğ‘¦ ğ‘—ğ‘ 
{
â†
ğ‘ˆ ğ‘”
ğ‘ˆ ğ‘”
yğ‘˜
Ë†ğœŒ ğ‘—
ğ‘  +
ğ‘  â†
(
return yğ‘— ,ğ‘‡ğ‘— ;

> 0
ğ‘¦ ğ‘—ğ‘ 
|
ğ‘”, ğ‘ 

ğ‘¡
[
] âˆˆ
yğ‘— ;
) âˆˆ

ğ‘¡
[
]
ğ‘¢,
)/

âˆ€(

ğ‘  as yğ‘— ;
yğ‘— ,

ğ‘ , ğ‘¡

âˆ€

10
11 if there are running jobs then
12
13 return

, ğ‘‡ ;
âˆ…

Waiting for some job to exit and then goes to Line 2;

3
G
4 for ğ‘ 

5

6 if

7

8

9

)

(

ğœƒğ‘¢, ğœ…

the scheduling and makespan given the current
(Line 16).
Upon ï¬nishing scheduling all jobs, we will update the schedule and
makespan for the given ğœƒğ‘¢ if it has a smaller makespan (Lines 17-
18). After exhausting all values of ğœ… for a given ğœƒğ‘¢ , we will update
the global makespan and the schedule if the current input ğœƒğ‘¢ has a
better performance (Lines 19-20). Also, it indicates that we can fur-
ther decrease the value of ğœƒğ‘¢ to ï¬nd a potentially better schedule.
Thus, we search for the left half space by moving the right pointer
(Line 21); otherwise, we should increase the value of ğœƒğ‘¢ by mov-
ing the left pointer (Line 23). By scheduling workers as described
in Algorithm 1, no workerâ€™s execution time will exceed the given
limit ğœƒğ‘¢ . We denote the tightest execution time limit returned as
Ëœğœƒğ‘¢ .

â‰¤

Algorithm 2 is based on the idea of â€œfragment-aware ï¬rst ï¬t
packing,â€ where we ï¬rst add all available GPUs whose ğ‘ˆ ğ‘”
ğ‘¢
ğ‘  +
)/
ğœƒğ‘¢ (Line 2). If there are enough available GPUs to schedule for
job ğ‘—â€™s workers (Line 3), we choose top-ğº ğ‘— GPUs with least exe-
cution time ï¬rst (Line 4). We then evaluate the completion time of
job ğ‘— (Line 5) and update the corresponding GPUsâ€™ execution time
(Line 6); otherwise, we wait for some job to ï¬nish (Lines 8-9).

yğ‘˜

Ë†ğœŒ ğ‘—

(

Algorithm 3 is based on the idea of â€œleast busy server-GPU ï¬rst,â€
where we sort the servers by its GPUâ€™s average accumulative exe-
cution time (Line 2) and add the available GPUs whose execution
time does not exceed ğœƒğ‘¢ in a non-decreasing order (Lines 4-5). Here,
1 as a tuning parameter. The smaller the ğœ† ğ‘— is,
we introduce ğœ† ğ‘—
the fewer number of servers can be used. If enough idle workers
can be found, we schedule the job, evaluate its completion time,
update the execution time of the chosen GPUs, and return the
schedule (Lines 6-10); otherwise, we wait for some job to ï¬nish
(Lines 11-12). If there is no running job left, then return schedule
âˆ…
and timespan ğ‘‡ (as makespan) to indicate the scheduling is infea-
sible (Line 13).

â‰¥

6 PERFORMANCE ANALYSIS
In this section, we analyze the theoretical performance of SJF-BCO.
Speciï¬cally, we will establish the approximation ratio guarantee of
our proposed SJF-BCO algorithm as follows:

1) We ï¬rst show in Lemma 2 that the maximum execution time
max ) returned by our algorithm is equal to Ëœğœƒğ‘¢ .

(i.e., Ë†ğ‘Š Alg1

Ë†ğ‘Š Alg1
max )
(

2) We then prove that the makespan is ğ‘‚
3) We further show that the gap between Ëœğœƒğ‘¢ and the tightest exe-
cution time limit ğœƒ âˆ—ğ‘¢ returned by some oï¬„ine optimal algorithm
in the right-hand-side (RHS) of (16) is bounded in Lemma 4.
Finally, by putting all these lemmas together, we arrive at the ap-
proximation ratio result stated in Theorem 5.

in Lemma 3.

ğ‘—

Ë†ğœŒ ğ‘— (

ğ‘˜ ğ‘¥ğ‘˜
ğ‘—

yğ‘˜
)ğ‘¢

Proof. Note that in Algorithm 1, we can obtain a schedule such
that the execution time of every worker will not exceed Ëœğœƒğ‘¢ , i.e.,
ğ‘” (cf. Line 2 in Algorithm 2 and Line 5
in Algorithm 3). Note that Ëœğœƒğ‘¢ is the tightest value found by Alg. 1
Ã
since we will keep decreasing its value in the RHS of (16) until it
becomes equal to Ë†ğ‘Š Alg1

max in the LHS of (16). It then follows that:

Ëœğœƒğ‘¢ ,

Ã

â‰¤

âˆ€

Ë†ğ‘Š Alg1

max = max

ğ‘”

ğ‘¥ğ‘˜
ğ‘—

Ë†ğœŒ ğ‘—

yğ‘˜
(
ğ‘¢

)

= Ëœğœƒğ‘¢ .

âˆˆN Ã•ğ‘—
âˆˆ J
Thus, we can have the maximum execution time Ë†ğ‘Š Alg1
Ëœğœƒğ‘¢ , and the proof is complete.

Ã•ğ‘˜
1,...,
âˆˆ{

|Y | }

max is equal to
(cid:3)

Lemma 3 (Makespan Upperbound). Algorithm 1 achieves a makespan

at most ğ‘›ğ‘” Ë†ğ‘Š Alg1

max , where ğ‘›ğ‘” is deï¬ned as in Theorem 1.

Proof. To bound the makespan, we need to attain upper bounds
of the total busy and idle time for each worker. Recall that due
to the synchronous gang scheduling for training, the worker may
wait for other workers to ï¬nish executing other jobs before it could
start processing the current job, which may result in idling. First,
we can have the total busy time ğ‘‡ busy
Ëœğœƒğ‘¢ . Next, we
work on bounding the total idle time ğ‘‡ idle

Ë†ğ‘Š Alg1
max
.

Lem. 2=

ğ‘”

â‰¤
ğ‘”

âˆˆ N

For any worker ğ‘”

, we use ğ‘”ğ‘— to denote the last job ğ‘— on ğ‘”.
Suppose job ğ‘— follows schedule yğ‘˜ . At any time slot ğ‘¡ before worker
ğ‘” processes job ğ‘—, there are two cases: i) worker ğ‘” is occupied by
other jobs (i.e., ğ‘” is busy); ii) worker ğ‘” is idle, but at least one worker
is busy with executing other jobs. Since we consider
ğ‘”â€² âˆˆ G
the gang-scheduling discipline, the job cannot be delayed if there is
a suï¬ƒcient number of GPUs available as requested. Thus we have:

yğ‘˜

)

(

ğ‘—

a

)

(

ğ‘‡ idle
ğ‘”

â‰¤ Ã•ğ‘”â€² âˆˆGğ‘— (

yğ‘˜

) |

ğ‘”â€²â‰ ğ‘”

ğ‘‡ busy
ğ‘”â€²

Ë†ğ‘Š Alg1
max

â‰¤ Ã•ğ‘”â€² âˆˆGğ‘— (

yğ‘˜

) |

ğ‘”â€²â‰ ğ‘”

b

(
)
â‰¤ (

ğº ğ‘—

1

Ë†ğ‘Š Alg1
max ,
)

âˆ’

yğ‘˜

where (a) follows from the fact that in any time slot ğ‘¡ that worker ğ‘”
is idle (case ii), we must be able to ï¬nd at least one busy worker ğ‘”â€² âˆˆ
. To calculate the idle time of worker ğ‘”, we can calculate the
G
yğ‘˜
instead, and the limit of each
busy time of worker(s) ğ‘”â€² âˆˆ G
workerâ€™s busy time is Ë†ğ‘Š Alg1
max . Also, (b) follows from the fact that at

(

(

)

)

ğ‘—

ğ‘—

On Scheduling Ring-All-Reduce Learning Jobs in Multi-Tenant GPU Clusters with Communication Contention

MobiHoc â€™22, October 17â€“20, 2022, Seoul, Republic of Korea

most ğº ğ‘—
can upper bound the makespan ğ‘‡ total as:

âˆ’

1 number of GPUs (except worker ğ‘”) are busy. Then, we

ğ‘”

ğ‘‡ busy
ğ‘”
(
+
ğº ğ‘— Ë†ğ‘Š Alg1

ğ‘‡ total = max
âˆˆN
= max
ğ‘—
âˆˆ J
and the proof is complete.

ğ‘‡ idle
ğ‘”

) â‰¤

max
ğ‘—
âˆˆ J
max = ğ‘›ğ‘” Ë†ğ‘Š Alg1
max ,

Ë†ğ‘Š Alg1
(cid:18)

max + (

ğº ğ‘—

1

Ë†ğ‘Š Alg1
max (cid:19)
)

âˆ’

(cid:3)

Next, we characterize the gap between the maximum execution
time limit Ëœğœƒğ‘¢ and the optimal execution time ğœƒ âˆ—ğ‘¢ in the RHS of (16).
Lemma 4. The maximum execution time Ëœğœƒğ‘¢ returned by Algo-

rithm 1 satisï¬es Ëœğœƒğ‘¢

ğœ‘ ğ‘¢
ğ‘™

Â·

â‰¤

ğœƒ âˆ—ğ‘¢ , where ğœ‘ = maxğ‘—

ğœŒ ğ‘— (
ğœŒ ğ‘— (

yğ‘˜1
yğ‘˜2

,

ğ‘˜1, ğ‘˜2.
âˆ€

)
)

Proof. Let ğ‘˜âˆ— and Ëœğ‘˜ be the schedule indices chosen by solving
yğ‘˜

Problem (14) optimally and Algorithm 1, respectively. Let
be the set of selected GPUs if schedule yğ‘˜ is used. We have

G(

)

Lem. 2=

Ëœğœƒğ‘¢

max
y Ëœğ‘˜

âˆˆG (

ğ‘”

b

(
)
â‰¤

max
yğ‘˜âˆ—

âˆˆG (

ğ‘”

) Ã•ğ‘—
âˆˆ J

) Ã•ğ‘—
âˆˆ J
ğœ‘ ğ‘¢

Ë†ğœŒ ğ‘—

Ëœğ‘˜
y
(
ğ‘¢

)

a

(
)
â‰¤

max
y Ëœğ‘˜

âˆˆG (

ğ‘”

ğ‘™ Ë†ğœŒ ğ‘—
(
ğ‘¢

yğ‘˜âˆ—

)

Eq. (16)

â‰¤

ğœ‘

ğœ‘ ğ‘¢

ğ‘™ Ë†ğœŒ ğ‘—
(
ğ‘¢

yğ‘˜âˆ—

)

) Ã•ğ‘—
âˆˆ J
ğ‘¢
ğ‘™

ğœƒ âˆ—ğ‘¢ .

(

(

)

(

yğ‘˜

yğ‘˜

yğ‘˜

ğ‘™ ğœŒ ğ‘—

, ğ‘¢ğœŒ ğ‘—

) âˆˆ [

To see why (a) holds, recall that for any schedule yğ‘˜ , we have
Ë†ğœŒ ğ‘—
. Then, for any two diï¬€erent sched-
)]
ules yğ‘˜1 and yğ‘˜2 , we can calculate the worst-case ratio as

)
) â‰¤
ğ‘¢ğœŒ ğ‘— (
ğ‘™ . The inequality in (b) can be established as fol-
ğ‘™ ğœŒ ğ‘— (
lows. First, note that Ëœğ‘˜ is chosen using the least execution time
ï¬rst scheduling strategy in Algorithm 2 (Line 4). Then, we have

)
) â‰¤

Ë†ğœŒ ğ‘— (
Ë†ğœŒ ğ‘— (

yğ‘˜1
yğ‘˜2

yğ‘˜1
yğ‘˜2

ğœ‘ ğ‘¢

Ë†ğœŒ ğ‘— (

y Ëœğ‘˜
)ğ‘¢

Ë†ğœŒ ğ‘— (

yğ‘˜
)ğ‘¢

ğ‘”

ğ‘—

ğ‘—

)

,

â‰¤

yğ‘˜

y Ëœğ‘˜

âˆˆ J

âˆˆG (

âˆˆG (

) Ã

max
ğ‘˜, which
maxğ‘”
ğ‘”
can be proved by contradiction as follows. Suppose there exists
yğ‘˜
ğ‘”â€² âˆˆ
âˆ€
. However, we know that Ëœğ‘˜ chooses the GPUs with the least
, which contradicts
(cid:3)

G(
execution time ï¬rst, i.e., ğ‘” should be in
G(
our assumption. This completes the proof.

âˆˆ G(
Ëœğ‘˜
y

such that

y Ëœğ‘˜
)ğ‘¢

yğ‘˜
)ğ‘¢

) \ G(

Ë†ğœŒ ğ‘— (

Ë†ğœŒ ğ‘— (

Ëœğ‘˜
y

Ëœğ‘˜
y

âˆˆ J

âˆˆ J

âˆˆ J

Ã

Ã

Ã

â‰¤

âˆ€

)

)

)

,

ğ‘—

ğ‘—

Finally, by putting everything together, we have the following

approximation ratio for our proposed approach:

Theorem 5 (Approximation Ratio). Alg. 1 is ğ‘›ğ‘”ğœ‘ ğ‘¢

ğ‘™ -approximate.

Proof. We use ğ‘‡ âˆ— to denote the optimal makespan that pro-

duced by some oï¬„ine optimal algorithm. It then follows that

ğ‘‡ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ Lem.3
â‰¤

ğ‘›ğ‘” Ë†ğ‘Š Alg1
max

Lem.2= ğ‘›ğ‘” Ëœğœƒğ‘¢

Lem.4
â‰¤

ğ‘›ğ‘”ğœ‘

ğ‘¢
ğ‘™

ğœƒ âˆ—ğ‘¢

a

(
)
â‰¤

ğ‘›ğ‘”ğœ‘

ğ‘¢
ğ‘™

ğ‘‡ âˆ—,

yğ‘˜
)ğ‘¢

where (a) is due to Problem (14) estimates the processing time as
Ë†ğœŒ ğ‘— (
nization barrier), which implies ğœƒ âˆ—ğ‘¢ â‰¤

without considering potential idling (caused by synchro-
ğ‘‡ âˆ—. This completes the proof.
(cid:3)

Remark 1. Note that the result in Theorem 5 does not depend
explicitly on the parameter ğœ… in SJF-BCO. This is because Theo-
rem 5 is only a worst-case upper bound that depends on Ëœğœƒğ‘¢ , which
in turn depends on ğœ…. Hence, ğœ… is implicitly captured in Theorem 5.

(

ğ‘›ğ‘”

|J |

SJF-BCO is ğ‘‚

ğ‘ log ğ‘ logğ‘‡

Theorem 6 (Polynomial Running Time). Time complexity of
, where ğ‘›ğ‘” is deï¬ned as in Thm. 1.

)
Proof. The sorting operation plays a dominant role in the to-
ğœ…, we
tal running time in Algorithm 1. For each job ğ‘—, if ğº ğ‘—
need to sort all GPUs in the cluster, which takes ğ‘‚
time
in order to choose top-ğº ğ‘— workers with least execution time ï¬rst
in Algorithm 2 (Line 4). Otherwise, we only need to sort servers,
which takes ğ‘‚
time in order to choose top-ğ‘š servers as in
ğ‘† log ğ‘†
time to schedule
Algorithm 3 (Line 2). Thus, it takes ğ‘‚
each job since ğ‘ > ğ‘†. Then, for all the jobs to be scheduled given
(ğœƒğ‘¢, ğœ…), it has ğ‘‚
time complexity. Recall that we use
bisection to search ğœƒğ‘¢ , where each iteration contains an inner loop
. This implies a total of ğ‘›ğ‘” logğ‘‡ trials. Thus,
indexed by ğœ…
âˆˆ [
(cid:3)
the overall time complexity is ğ‘‚

ğ‘ log ğ‘ logğ‘‡

â‰¤
ğ‘ log ğ‘

ğ‘ log ğ‘

ğ‘ log ğ‘

1, ğ‘›ğ‘”

(|J |

]

(

(

)

)

(

)

)

.

ğ‘›ğ‘”

(

|J |

)

7 NUMERICAL RESULTS
In this section, we conduct simulation studies to evaluate the per-
formance of our proposed SJF-BCO algorithm.

]

(

]

}

{

âˆ€

ğ‘¡
[

âˆˆ [

50, 300

1000, 6000

]
) âˆˆ [

[21], and ğœ† ğ‘— = 1,

0.01, 0.05
yğ‘˜

uniformly at random.

(evaluated from the product of ğœ ğ‘—

1) Experiment Settings: Similar to the setting in [19], the work-
load is generated based on the Microsoft job trace [9]. We generate
160 DDL jobs by scaling down the original job trace [9] following
the job-type distribution, where there are 80 single-GPU jobs, 14
2-GPU jobs, 26 4-GPU jobs, 30 8-GPU jobs, 8 16-GPU jobs, and 2
32-GPU jobs. We set ğ¹ ğ‘—
. The extra time cost brought
by communication contention and overhead is within 15% of the
total actual execution time. We let ğœ‰1 = ğœ‰2 (cf. Sec. 4.1) to make
communication contention and overhead cost comparable. We set
ğœ ğ‘—
ğ‘—. We set the estimated exe-
] âˆˆ [
cution time Ë†ğœŒ
ğ‘¡
]
[
and ğ¹ ğ‘— ). The GPU cluster has 20 servers. The number of GPUs on
4, 8, 16, 32
each server is chosen from
2) Baselines for Comparison: We compare our algorithm with
three representative job scheduling algorithms: First-Fit (FF) [17],
List-Scheduling (LS) [17], and Random (RAND) [19]. Here, we de-
ï¬ne ğœƒ ğ‘“
ğ‘¢ as the maximum execution time limit returned by the sched-
uling policy ğ‘“ . Given a job ğ‘—, FF picks the ï¬rst ğº ğ‘— available GPUs
such that their accumulative execution time does not exceed the
limit ğœƒ ğ¹ ğ¹
ğ‘¢ , from server to server. This policy tends to pack diï¬€erent
jobs into the fewest number of servers to avoid fragmentation in-
troduced by small jobs, which can save space for large jobs to be
scheduled next. LS selects top-ğº ğ‘— GPUs with least execution time
ï¬rst, so that the accumulative execution time does not exceed the
limit ğœƒğ¿ğ‘†
ğ‘¢ . Note that this policy may introduce high communication
overhead since it may choose GPUs from a large number of servers.
Further, LS tries to balance the execution time between GPUs by
always selecting the one with the least execution time. RAND ran-
domly chooses servers and GPUs to schedule jobs. In this policy,
we allocate GPUs to a job as long as it does not exceed ğ‘‡ , i.e., we
set ğœƒğ‘…ğ´ğ‘ ğ·
= ğ‘‡ , to avoid the long running time in order to ï¬nd a
ğ‘¢
feasible schedule.

MobiHoc â€™22, October 17â€“20, 2022, Seoul, Republic of Korea

Yu and Liu, et al.

Average
Bounds
Makespan

)
t
o
s

l

e
m

i
t
(

s
e
m

i
t
n
o
i
t
e
p
m
o
C

l

800

750

700

650

600

550

)
t
o
s

l

e
m

i
t
(

n
a
p
s
e
k
a
M

Turning point 2

Turning point 1

Scheduling policies

500

0

5

10

15
The value of

20

25

30

)
t

o
s

l

e
m

i
t
(

n
a
p
s
e
k
a
M

1500

1000

500

0

SJF-BCO
LS
FF
RAND

10

15
Number of servers

20

1100

1000

900

800

700

600

)
t
o
s

l

e
m

i
t
(
n
a
p
s
e
k
a
M

500

1

2

3

4

5

6

7

8

The value of

Figure 4: Makespan compari-
son under diï¬€erent policies.

Figure 5: Impact of value of ğœ…
on makespan.

Figure 6: Makespan as the
number of servers increases.

Figure 7: Impact of the value
of ğœ† on makespan.

3) Experiment Results: First, we compare the makespan per-
formance achieved by our SJF-BCO algorithm with those of the
baseline policies. We set ğ‘‡ = 1200. As shown in Fig. 4, SJF-BCO
outperforms other scheduling policies both in terms of makespan
and average job completion times, implying that SJF-BCO is also
superior in terms of total job completion time. Note that SJF-BCO
tends to open new server(s) for large jobs to avoid the large com-
munication overhead and use shared servers for small jobs to avoid
the fragmentation, thus achieving better average completion time
and makespan than FF and RAND. Note that SJF-BCO has more
prominent advantages over these baselines when the cluster has
limited GPU resources.

â‰¤

Then, we examine the impact of ğœ… on the makespan in our pro-
posed SJF-BCO algorithm. We set ğ‘‡ = 1200, and select ğœ… from 1 to
32. As indicated in Fig. 5, as the value of ğœ… increases, the makespan
ï¬rst drops and then increases and then drops again. Recall that in
Algorithm 1, FA-FFP is used when the number of requested GPUs
ğº ğ‘—
ğœ…; otherwise LBSGF is used. Note that before Turning point
1 in Fig. 5, as ğœ… increases, the makespan drops since more small
jobs are packed into the fewest number of shared servers, result-
ing in decrement of communication contention and overhead in-
troduced by larger jobs to be scheduled later. However, as ğœ… con-
tinues to grow, communication contention becomes more notice-
able since more large jobs are scheduled to the shared servers, lead-
ing to the increase of makespan. Finally, as ğœ… becomes suï¬ƒciently
large, then the majority or even all jobs use shared servers to sched-
ule their workers, which can slightly decrease the communication
overhead due to the smaller resultant ring-span (see Turning point
2 in Fig. 5).

Next, we investigate the inï¬‚uence of communication contention
by reducing the number of servers. We setğ‘‡ = 1500. Intuitively, the
larger number of servers, the less communication contention. As
we can see from Fig. 6, as we increase the number of servers from
10 to 20, the makespan of FF, LS and SJF-BCO decrease due to the
degradation of contention level. Note that, if enough resources are
available in the cluster, then each job will have a separate set of
servers using SJF-BCO, i.e., its performance will become better as
number of GPUs increases. In this case, no communication con-
tention will be introduced using SJF-BCO. The intuition that FF
has the largest makespan reduction is that the average idle time
for workers drops dramatically since a smaller execution time limit
could be set as the number of servers increases.

Lastly, we inspect the inï¬‚uence of ğœ† on the makespan for SJF-
and ğœ… = 1. As we can see from Fig. 7, the

1, 2, 4, 8

BCO with ğœ†

âˆˆ {

}

makespan monotonically decreases as the ğœ† increases. Recall that a
larger ğœ†-value implies a larger number of servers could be selected.
Then, the job has a higher chance to open new servers to sched-
ule its workers, resulting in less communication contention and a
smaller communication overhead. Interestingly, ğœ† plays a similar
role as ğœ…, with the aim to balance communication overhead and
contention. Speciï¬cally, ğœ… aï¬€ects the overall balance between all
jobs since it determines the portion of jobs to use either FA-FFP or
LBSGF, while ğœ† focuses more on the balance between communica-
tion contention and overhead for a speciï¬c job that uses LBSGF to
schedule.

8 CONCLUSION
In this paper, we studied resource scheduling for DDL jobs in a
multi-tenant GPU cluster, where we considered the communica-
tion contention and overhead determined by the distribution of
workers. We showed that this problem can be formulated as a highly
non-trivial non-linear integer program with nonconvex and mixed
packing-covering constraints. We then converted the problem into
a tractable integer linear program, which enables the design of
approximation algorithms. Speciï¬cally, we developed a new an-
alytical model that jointly considers the placements and starting
times of the workers of each DDL job. Through careful reformula-
tion, we then transformed the problem into an integer linear pro-
gram with a more tractable structure, and proposed an approxima-
tion algorithm with an approximation ratio performance guaran-
tee. We provided rigorous theoretical analysis and conducted ex-
periments to demonstrate the eï¬ƒcacy of our algorithms. Collec-
tively, our results contribute to a fundamental understanding on
resource scheduling for DDL jobs in multi-tenant GPU clusters.

9 ACKNOWLEDGEMENTS
J. Liuâ€™s work has been supported in part by NSF grants CAREER
CNS-2110259, CNS-2112471, CNS-2102233, CCF-2110252, and a Cisco
Systems Research Grant GR127298. B. Jiâ€™s work has been supported
in part by NSF CNS-2112694. H. Rajanâ€™s work has been supported
in part by NSF 21-20448 and NSF 19-34884.

REFERENCES
[1] Abadi, M., Barham, P., et al. TensorFlow: A system for large-scale machine

learning. In Proc. of USENIX OSDI (2016).

[2] Bao, Y., Peng, Y., and Wu, C. Deep learning-based job placement in distributed

machine learning clusters. In in IEEE INFOCOM (2019).

 
 
 
 
On Scheduling Ring-All-Reduce Learning Jobs in Multi-Tenant GPU Clusters with Communication Contention

MobiHoc â€™22, October 17â€“20, 2022, Seoul, Republic of Korea

[3] Chau, V., Chu, X., Liu, H., and Leung, Y.-W. Energy eï¬ƒcient job scheduling
with dvfs for cpu-gpu heterogeneous systems. In Proceedings of the Eighth Inter-
national Conference on Future Energy Systems (2017), pp. 1â€“11.

[4] Foley, D., and Danskin, J. Ultra-performance pascal gpu and nvlink intercon-

nect. In IEEE Micr (2017), vol. 37, pp. 7â€“17.

[5] Grandl, R., Chowdhury, M., Akella, A., and Ananthanarayanan, G. Altru-

istic scheduling in multi-resource clusters. In OSDI (2016).

[6] Grandl, R., Kandula, S., Rao, S., Akella, A., and Kulkarni, J. Graphene:
Packing and dependency-aware scheduling for data-parallel clusters. In OSDI
(2016).

[7] Gu, J., Chowdhury, M., Shin, K. G., Zhu, Y., Jeon, M., Qian, J., Liu, H., and
Guo, C. Tiresias: A gpu cluster manager for distributed deep learning. In NSDI
19 (2019), pp. 485â€“500.

[8] Hu, Z., Tu, J., and Li, B. Spear: Optimized dependency-aware task scheduling
with deep reinforcement learning. In in 2019 IEEE 39th International Conference
on Distributed Computing Systems (ICDCS) (2019).

[9] Jeon, M., Venkataraman, S., Phanishayee, A., Qian, J., Xiao, W., and Yang,
F. Analysis of large-scale multi-tenant gpu clusters for dnn training workloads.
In 2019 USENIX Annual Technical Conference (USENIX ATC 19) (2019).

[10] Mahajan, K., Balasubramanian, A., Singhvi, A., Venkataraman, S., Akella,
A., Phanishayee, A., and Chawla, S. Themis: Fair and eï¬ƒcient gpu cluster
scheduling. In 17th USENIX Symposium on Networked Systems Design and Imple-
mentation (NSDI 20) (2020), pp. 289â€“304.

[11] Mei, X., Chu, X., Liu, H., Leung, Y., and Li, Z. Energy eï¬ƒcient real-time task
scheduling on cpu-gpu hybrid clusters. In IEEE INFOCOM 2017 - IEEE Conference
on Computer Communications (2017), pp. 1â€“9.

[12] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., KÃ¶pf, A., Yang, E., De-
Vito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai,
J., and Chintala, S. Pytorch: an imperative style, high-performance deep learn-
ing library. In NeurIPS (2019).

[13] Patarasuk, P., and Yuan, X. Bandwidth optimal all-reduce algorithms for clus-
ters of workstations. In Journal of Parallel and Distributed Computing (2009).

[14] Peng, Y., Bao, Y., Chen, Y., Wu, C., and Guo, C. Optimus: An eï¬ƒcient dynamic

resource scheduler for deep learning clusters. In Proc. of ACM EuroSys (2018).

[15] Sergeev, A., and Balso, M. D. Horovod: Fast and easy distributed deep learning

in tensorï¬‚ow. In arXiv preprint arXiv:1802.05799 (2018).

[16] Shi, S., Qiang, W., and Chu, X. Performance modeling and evaluation of dis-
tributed deep learning frameworks on gpus. In The 4th International Conference
on Big Data Intelligence and Computing (DataCom) (2018), pp. 949â€“957.

[17] Stavrinides, G. L., and Karatza, H. D. Scheduling multiple task graphs in het-
erogeneous distributed real-time systems by exploiting schedule holes with bin
packing techniques. In Simulation Modelling Practice and Theory (2011), vol. 19,
pp. 540â€“552.

[18] Wang, L., Weng, Q., Wang, W., Chen, C., and Li, B. Metis: Learning to schedule
long-running applications in shared container clusters at scale. In SC20: Inter-
national Conference for High Performance Computing, Networking, Storage and
Analysis (2020), pp. 1â€“17.

[19] Wang, Q., Shi, S., Wang, C., and Chu, X. Communication contention aware

scheduling of multiple deep learning training jobs. In arXiv:2002.10105 (2020).

[20] Xiao, W., Bhardwaj, R., Ramjee, R., Sivathanu, M., Kwatra, N., Han, Z., Patel,
P., Peng, X., Zhao, H., Zhang, Q., Yang, F., and Zhou, L. Gandiva: Introspective
cluster scheduling for deep learning. In in 13th USENIX Symposium on Operating
Systems Design and Implementation (OSDI 18) (2018), pp. 595â€“610.

[21] Yu, M., Liu, J., Wu, C., Ji, B., and Bentley, E. S. Toward eï¬ƒcient online sched-
uling for distributed machine learning systems. IEEE Transactions on Network
Science and Engineering (TNSE) (2021).

[22] Yu, M., Tian, Y., Ji, B., Wu, C., Rajan, H., and Liu, J. Gadget: Online resource
In IEEE INFOCOM

optimization for scheduling ring-all-reduce learning jobs.
(2022).

[23] Zhang, H., Zheng, Z., Xu, S., Dai, W., Ho, Q., Liang, X., Hu, Z., Wei, J., Xie,
P., and Xing, E. P. Poseidon: An eï¬ƒcient communication architecture for dis-
tributed deep learning on gpu clusters.
In in 2017 USENIX Annual Technical
Conference (USENIX ATC 17) (2017), pp. 181â€“193.

