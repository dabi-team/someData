2
2
0
2

r
a

M
2

]

C
O
.
h
t
a
m

[

2
v
7
4
9
5
0
.
5
0
1
2
:
v
i
X
r
a

A New Perspective on Low-Rank Optimization

Dimitris Bertsimas · Ryan Cory-Wright · Jean Pauphilet

Abstract A key question in many low-rank problems throughout optimization, machine learning,
and statistics is to characterize the convex hulls of simple low-rank sets and judiciously apply these
convex hulls to obtain strong yet computationally tractable convex relaxations. We invoke the
matrix perspective function — the matrix analog of the perspective function — and characterize
explicitly the convex hull of epigraphs of simple matrix convex functions under low-rank constraints.
Further, we combine the matrix perspective function with orthogonal projection matrices–the matrix
analog of binary variables which capture the row-space of a matrix–to develop a matrix perspective
reformulation technique that reliably obtains strong relaxations for a variety of low-rank problems,
including reduced rank regression, non-negative matrix factorization, and factor analysis. Moreover,
we establish that these relaxations can be modeled via semideﬁnite constraints and thus optimized
over tractably. The proposed approach parallels and generalizes the perspective reformulation
technique in mixed-integer optimization and leads to new relaxations for a broad class of problems.

Keywords Low-rank matrix · Semideﬁnite optimization · Matrix perspective function · Perspective
reformulation technique

Mathematics Subject Classiﬁcation (2010) 90C22 · 90C25 · 90C26 · 15A03 · 26B25

1 Introduction

Over the past decade, a considerable amount of attention has been devoted to low-rank optimization,
resulting in theoretically and practically eﬃcient algorithms for problems as disparate as matrix
completion, reduced rank regression, or computer vision. In spite of this progress, almost no equivalent
progress has been made on developing strong lower bounds for low-rank problems. Accordingly, this
paper proposes a procedure for obtaining novel and strong lower bounds.

D. Bertsimas
Sloan School of Management, Massachusetts Institute of Technology, Cambridge, MA 02139
ORCID: 0000-0002-1985-1003 E-mail: dbertsim@mit.edu

R. Cory-Wright
Operations Research Center, Massachusetts Institute of Technology, Cambridge, MA 02139
ORCID: 0000-0002-4485-0619 E-mail: ryancw@mit.edu

J. Pauphilet
London Business School, London, UK
ORCID: 0000-0001-6352-0984 E-mail: jpauphilet@london.edu

 
 
 
 
 
 
2

D. Bertsimas, R. Cory-Wright, J. Pauphilet

We consider the following low-rank optimization problem:

min
X∈S n
+

(cid:104)C, X(cid:105) + Ω(X) + µ · Rank(X) s.t. (cid:104)Ai, X(cid:105) = bi ∀i ∈ [m], X ∈ K, Rank(X) ≤ k,

(1)

where C, A1, . . . Am ∈ S n are n × n symmetric matrices, b1, . . . bm ∈ R are scalars, [n] denotes the
+ denotes the n × n positive semideﬁnite cone, and µ ∈ R+, k ∈ N
set of running indices {1, ..., n}, S n
are parameters which controls the complexity of X by respectively penalizing and constraining its
rank. The set K is a proper—i.e., closed, convex, solid and pointed—cone [c.f. 15, Section 2.4.1],
and Ω(X) = tr(f (X)) for some matrix convex function f ; see formal deﬁnitions and assumptions
in Section 3.

For optimization problems with logical constraints, strong relaxations can be obtained by formu-
lating them as mixed-integer optimization (MIO) problems and applying the so-called perspective
reformulation technique [see 36, 41]. In this paper, we develop a matrix analog of the perspective
reformulation technique to obtain strong yet computationally tractable relaxations of low-rank
optimization problems of the form (1).

1.1 Motivating Example

In this section, we illustrate the implications of our results on a statistical learning example. To
emphasize the analogy with the perspective reformulation technique in MIO, we ﬁrst consider the
best subset selection problem and review its perspective relaxations. We then consider a reduced-rank
regression problem – the rank-analog of best subset selection – and provide new relaxations that
naturally arise from our Matrix Perspective Reformulation Technique (MPRT).

Best Subset Selection: Given a data matrix X ∈ Rn×p and a response vector y ∈ Rn, the (cid:96)0 − (cid:96)2
regularized best subset selection problem is to solve [c.f. 64, 7, 6, 9, 75, 3]:

min
w∈Rp

1
2n

(cid:107)y − Xw(cid:107)2

2 +

1
2γ

(cid:107)w(cid:107)2

2 + µ(cid:107)w(cid:107)0,

(2)

where µ, γ > 0 are parameters which control w’s sparsity and sensitivity to noise respectively.

Early attempts at solving Problem (2) exactly relied upon weak implicit or big-M formulations
of logical constraints which supply low-quality relaxations and therefore do not scale well [see 14, 43,
for discussions]. However, very similar algorithms now solve these problems to certiﬁable optimality
with millions of features. Perhaps the key ingredient in modernizing these (previously ineﬃcient)
algorithms was invoking the perspective reformulation technique—a technique for obtaining high-
quality convex relaxations of non-convex sets—ﬁrst stated in Stubbs [71] PhD thesis [see also 72, 21]
and popularized by Frangioni and Gentile [36], Akt¨urk et al. [1], G¨unl¨uk and Linderoth [41] among
others.

A New Perspective on Low-Rank Optimization

3

Relaxation via the Perspective Reformulation Technique: By applying the perspective reformulation
technique [36, 1, 41] to the term µ(cid:107)w(cid:107)0 + 1

2, we obtain the following reformulation:

2γ (cid:107)w(cid:107)2

min
w,ρ∈Rp,z∈{0,1}p

1
2n

(cid:107)y − Xw(cid:107)2

2 +

1
2γ

e(cid:62)ρ + µ · e(cid:62)z s.t.

ziρi ≥ w2
i

∀i ∈ [p],

(3)

where e denotes a vector of all ones of appropriate dimension.

Interestingly, this formulation can be represented using second-order cones [41, 64] and optimized
over eﬃciently using projected subgradient descent [9]. Moreover, it reliably supplies near-exact
relaxations for most practically relevant cases of best subset selection [64, 6]. In instances where it
is not already tight, one can apply a reﬁnement of the perspective reformulation technique to the
term (cid:107)y − Xw(cid:107)2

2 and thereby obtain the following (tighter yet more expensive) relaxation [25]:

min
w∈Rp,z∈[0,1]p,W ∈Sp
+

1
2n

(cid:107)y(cid:107)2

2 −

1
n

(cid:104)y, Xw(cid:105) +

1
2

(cid:104)W ,

1
γ

I +

1
n

X (cid:62)X(cid:105) + µe(cid:62)z

(4)

s.t. W (cid:23) ww(cid:62), ziWi,i ≥ w2

i ∀i ∈ [p].

Recently, a class of even tighter relaxations were developed by Atamt¨urk and Gomez [3], Han
et al. [42], Frangioni et al. [38]. As they were developed by considering multiple binary variables
simultaneously and therefore do not, to our knowledge, generalize readily to the low-rank case (where
we often have one low-rank matrix), we do not discuss (or generalize) them here.

Reduced Rank Regression: Given m observations of a response vector Yj ∈ Rn and a predictor
Xj ∈ Rp, an important problem in high-dimensional statistics is to recover a low-complexity
model which relates X, Y . A popular choice for doing so is to assume that X, Y are related via
Y = Xβ + E, where β ∈ Rp×n is a coeﬃcient matrix which we assume to be low-rank, E is a matrix
of noise and we require that the rank of β is small in order that the linear model is parsimonious
[56]. Introducing Frobenius regularization gives rise to the problem:

min
β∈Rp×n

1
2m

(cid:107)Y − Xβ(cid:107)2

F +

1
2γ

(cid:107)β(cid:107)2

F + µ · Rank(β),

(5)

where γ, µ > 0 control the robustness to noise and the complexity of the estimator respectively and
we normalize the ordinary least squares loss by dividing by m, the number of observations.

Existing attempts at solving this problem generally involve replacing the low-rank term with a
nuclear norm term [56], which succeeds under some strong assumptions on the problem data but
not in general. Recently, we proposed a new framework to model rank constraints, using orthogonal
projection matrices which satisfy Y 2 = Y instead of binary variables which satisfy z2 = z [11].
By building on this work, in this paper we propose a generalization of the perspective function to
matrix-valued functions with positive semideﬁnite arguments and develop a matrix analog of the
perspective reformulation technique from MIO which uses projection matrices instead of binary
variables.

4

D. Bertsimas, R. Cory-Wright, J. Pauphilet

Relaxations via the Matrix Perspective Reformulation Technique: By applying the matrix perspective
reformulation technique (Theorem 1) to the term 1
F + µ · Rank(β), we will prove that the
following problem is a valid—and numerically high-quality—relaxation of (5):

2γ (cid:107)β(cid:107)2

min

β∈Rp×n,W ∈S n

+,θ∈Sp

+

1
2m

(cid:107)Y − Xβ(cid:107)2

F +

1
2γ

tr(θ) + µ · tr(W )

s.t. W (cid:22) I,

(cid:33)

(cid:32)

θ β
β(cid:62) W

(cid:23) 0.

(6)

The analogy between problems (2)-(5) and their relaxations (3)-(6) is striking. The goal of the
present paper is to develop the corresponding theory to support and derive the relaxation (6).
Interestingly, the main argument that led [25] to the improved relaxation (4) for (2) can be extended
to reduced-rank regression. Combined with our MPRT, it leads to the relaxation:

min

θ∈S n

+,β∈Rp×n,B∈S n

+,W ∈S n
+

s.t.

1
2m
(cid:32)

(cid:33)

(cid:23) 0, W (cid:22) I.

B β
β W

(cid:107)Y (cid:107)2

F −

1
m

(cid:104)Y , Xβ(cid:105) +

1
2

(cid:104)B,

1
γ

I +

1
m

X (cid:62)X(cid:105) + µ · tr(W )

(7)

It is not too hard to see that this is a valid semideﬁnite relaxation: if W is a rank-k projection
matrix then, by the Schur complement lemma [see 16, Equation 2.41], β = βW , and thus the rank
of β is at most k. Moreover, if we let B = ββ(cid:62) in a solution, we recover a low-rank solution to the
original problem1. Actually, as we show in Section 3.3, a similar technique can be applied to any
instance of Problem (1), for which the applications beyond matrix regression are legion.

1.2 Literature Review

Three classes of approaches have been proposed for solving Problem (1): (a) heuristics, which
prioritize computational eﬃciency and obtain typically high-quality solutions to low-rank problems
eﬃciently but without optimality guarantees [see 58, for a review]; (b) relax-and-round approaches,
which balance computational eﬃciency and accuracy concerns by relaxing the rank constraint and
rounding a solution to the relaxation to obtain a provably near-optimal low-rank matrix [11, Section
1.2.2]; and (c) exact approaches, which prioritize accuracy over computational eﬃciency and solve
Problem (1) exactly in exponential time [11, Section 1.2.1].

Of the three classes of approaches, heuristics currently dominate the literature, because their
superior runtime and memory usage allows them to address larger-scale problems. However, recent
advances in algorithmic theory and computational power have drastically improved the scalability of
exact and approximate methods, to the point where they can now solve moderately sized problems
which are relevant in practice [11]. Moreover, relaxations of strong exact formulations often give rise
to very eﬃcient heuristics (via tight relaxations of the exact formulation) which outperform existing

1 Observe that the constraints in Problem (4) are equivalent to the block matrix constraint

(cid:32)

Diag(z) Diag(w)
Diag(w) W

(cid:33)

(cid:23) 0.

This veriﬁes that the reduced rank regression formulation is indeed a generalization of [25]’s formulation for sparse
regression.

A New Perspective on Low-Rank Optimization

5

heuristics. This suggests that heuristic approaches may not maintain their dominance going forward,
and motivates the exploration of tight yet aﬀordable relaxations of low-rank problems.

1.3 Contributions and Structure

The main contributions of this paper are twofold. First, we propose a general reformulation technique
for obtaining high-quality relaxations of low-rank optimization problems: introducing an orthogonal
projection matrix to model a low-rank constraint, and strengthening the formulation by taking the
matrix perspective of an appropriate substructure of the problem. This technique can be viewed
as a generalization of the perspective reformulation technique for obtaining strong relaxations of
sparse or logically constrained problems [36, 41, 10, 42]. Second, by applying this technique, we
obtain explicit characterizations of convex hulls of low-rank sets which frequently arise in low-rank
problems. As the interplay between convex hulls of indicator sets and perspective functions has
engineered algorithms which outperform state-of-the-art heuristics in sparse linear regression [6, 43]
and sparse portfolio selection [77, 10], we hope that this work will empower similar developments
for low-rank problems.

The rest of the paper is structured as follows: In Section 2 we supply some background on
perspective functions and review their role in developing tight formulations of mixed-integer problems.
In Section 3, we introduce the matrix perspective function and its properties, extend the function’s
deﬁnition to allow semideﬁnite in addition to positive deﬁnite arguments, and propose a matrix
perspective reformulation technique (MPRT) which successfully obtains high-quality relaxations for
low-rank problems which commonly arise in the literature.We also connect the matrix perspective
function to the convex hulls of epigraphs of simple matrix convex functions under rank constraints.
In Section 4, we illustrate the utility of this connection by deriving tighter relaxations of several
low-rank problems than are currently available in the literature. Finally, in Section 5, we numerically
verify the utility of our approach on reduced rank regression, D-optimal design and non-negative
matrix factorization problems.

Notation: We let nonbold face characters such as b denote scalars, lowercase bold faced characters
such as x denote vectors, uppercase bold faced characters such as X denote matrices, and calligraphic
uppercase characters such as Z denote sets. We let [n] denote the set of running indices {1, ..., n}
and N denote the set of positive integers. We let e denote a vector of all 1’s, 0 denote a vector of all
0’s, and I denote the identity matrix. We let S n denote the cone of n × n symmetric matrices, S n
+
denote the cone of n × n positive semideﬁnite matrices, S n
denote the cone of n × n doubly
non-negative matrices, and Cn
+ } denote the cone of n × n completely positive
matrices. Finally, we let X † denote the Moore-Penrose pseudoinverse of a matrix X; see Horn and
Johnson [46], Bhatia [13] for general theories of matrix operators. Less common matrix operators
will be deﬁned as they are needed.

+ := {U U (cid:62) : U ∈ Rn×n

+ ∩ Rn×n

+

6

D. Bertsimas, R. Cory-Wright, J. Pauphilet

2 Background on Perspective Functions

In this section, we review perspective functions and their interplay with tight formulations of logically
constrained problems. This prepares the ground for and motivates our study of matrix perspective
functions and their interplay with tight formulations of low-rank problems. Many of our subsequent
results can be viewed as (nontrivial) generalizations of the results in this section, since a rank
constraint is a cardinality constraint on the singular values.

2.1 Preliminaries

Consider a proper closed convex function f : X → R, where X is a convex subset of Rn. The
perspective function of f is commonly deﬁned for any x ∈ Rn and any t > 0 as (x, t) (cid:55)→ tf (x/t). Its
closure is deﬁned by continuity for t = 0 and is equal to [c.f. 45, Proposition IV.2.2.2 ]:

gf (x, t) =






tf (x/t)

if t > 0, x/t ∈ X ,

0

if t = 0, x = 0,

f∞(x)

if t = 0, x (cid:54)= 0,

+∞

otherwise,

where f∞ is the recession function of f , as originally stated in [67, p. 67] which is given by

f∞(x) = lim
t→0

tf

(cid:16)

x0 − x +

(cid:17)

x
t

= lim

t→+∞

f (x0 + tx) − f (x0)
t

,

for any x0 in the domain of f . That is, f∞(x) is the asymptotic slope of f in the direction of x.

The perspective function was ﬁrst investigated by Rockafellar [67], who made the important
observation that f is convex in x if and only if gf is convex in (x, t). Among other properties, we
have that, for any t > 0, (x, t, s) ∈ epi(gf ) if and only if (x/t, s/t) ∈ epi(f ) [45, Proposition IV.2.2.1].
We refer to the review by Combettes [23] for further properties of perspective functions.

Throughout this work, we refer to gf as the perspective function of f –although it technically is

the closure of the perspective. We also consider a family of convex functions f which satisfy:

Assumption 1 The function f : X → R is proper, closed, and convex. 0 ∈ X and for any x (cid:54)= 0,
f∞(x) = +∞.

The condition f∞(x) = +∞, ∀x (cid:54)= 0 is equivalent to limx→∞ f (x)/(cid:107)x(cid:107) = +∞, and means that,
asymptotically, f increases to inﬁnity faster than any aﬃne function. In particular, it is satisﬁed if
the domain of f is bounded or if f is strictly convex. Under Assumption 1, the deﬁnition of the
perspective function of f simpliﬁes to

gf (x, t) =






tf (x/t)

if t > 0,

0

+∞

if t = 0, x = 0,

otherwise.

(8)

A New Perspective on Low-Rank Optimization

7

2.2 The Perspective Reformulation Technique

A number of authors have observed that optimization problems over binary and continuous variables
admit tight reformulations involving perspective functions of appropriate substructures of the
problem, since Ceria and Soares [21], building upon the work of Rockafellar [67, Theorem 9.8],
derived the convex hull of a disjunction of convex constraints. To motivate our study of the matrix
perspective function in the sequel, we now demonstrate that a class of logically-constrained problems
admit reformulations in terms of perspective functions. We remark that this development bears
resemblance to other works on perspective reformulations including [10, 42, 38].

Consider a logically-constrained problem of the form

min
z∈Z,x∈Rn

c(cid:62)z + f (x) + Ω(x)

s.t. xi = 0 if zi = 0 ∀i ∈ [n],

(9)

where Z ⊆ {0, 1}n, c ∈ Rn is a cost vector, f (·) is a generic convex function which possibly models
convex constraints x ∈ X for a convex set X ⊆ Rn implicitly—by requiring that g(x) = +∞ if
x /∈ X , and Ω(·) is a regularization function which satisﬁes the following assumption:

Assumption 2 (Separability) Ω(x) = (cid:80)

i∈[n] Ωi(xi), where each Ωi satisﬁes Assumption 1.

Since zi is binary, imposing the logical constraint “xi = 0 if zi = 0” plus the term Ωi(xi) in the
objective is equivalent to gΩ(xi, zi) + (1 − zi)Ωi(0) in the objective, where gΩi is the perspective
function of Ωi, and thus Problem (9) is equivalent to:

min
z∈Z,x∈Rn

c(cid:62)z + f (x) +

n
(cid:88)

(cid:18)

i=1

gΩi(xi, zi) + (1 − zi)Ωi(0)

.

(10)

(cid:19)

Notably, while Problems (9)-(10) have the same feasible regions, (10) often has substantially stronger
relaxations, as frequently noted in the perspective reformulation literature [36, 41, 35, 10].

For completeness, we provide a formal proof of equivalence between (9)-(10); note that a related

(although dual, and weaker as it requires Ω(0) = 0) result can be found in [10, Thm. 2.5]:

Lemma 1 Suppose (9) attains a ﬁnite optimal value. Then, (10) attains the same value.

Proof It suﬃces to establish that the following equality holds:

gΩi(xi, zi) + (1 − zi)Ωi(0) = Ωi(xi) +

(cid:40)

0

if xi = 0 or zi = 1,

+∞ otherwise.

Indeed, this equality shows that any feasible solution to one problem is a feasible solution to the
other with equal cost. We prove this by considering the cases where zi = 0, zi = 1 separately.

– Suppose zi = 1. Then, gΩi(xi, zi) = ziΩi(xi/zi) = Ωi(xi) and xi = zi · xi, so the result holds.
– Suppose zi = 0. If xi = 0 we have gΩi(0, 0) + Ωi(0) = Ωi(0), and moreover the right-hand-side of
(cid:117)(cid:116)

the equality is certainly Ωi(0). Alternatively, if xi (cid:54)= 0 then both sides equal +∞.

8

D. Bertsimas, R. Cory-Wright, J. Pauphilet

In Table 1, we present examples of penalties Ω for which Assumption 1 holds and the perspective
reformulation technique is applicable. We remind the reader that the exponential cone is [c.f. 22]:

Kexp = {x ∈ R3 : x1 ≥ x2 exp(x2/x3), x2 > 0} ∪ {(x1, 0, x3) ∈ R3 : x1 ≥ 0, x3 ≤ 0},

while the power cone is deﬁned for any α ∈ (0, 1) as [c.f. 22]:

Kα

pow = {x ∈ R3 : xα

1 x1−α

2 ≥ |x3|}.

Table 1: Convex substructures which frequently arise in MIOs and their perspective reformulations.
For conciseness, we give gΩ(x, z) for z > 0 only, i.e., the ﬁrst case in (8), gΩ(x, z) for z = 0 being
deﬁned as in Equation (8).

Penalty

Big-M

Ω(x)

(cid:40)

0

if |x| ≤ M,

gΩ(x, z) if z > 0
(cid:40)

0

if |x| ≤ M z

+∞ otherwise

+∞ otherwise

Ridge

1
2γ x2

x2/2γz

Ridge + Big-M

1
2γ x2, if |x| ≤ M

x2/2γz, if |x| ≤ M z

Power

|x|p, p>1

|x|pz1−p

Formulation

|x| ≤ M z

min θ

s.t.

θz ≥

1
2γ

x2

min θ

s.t.

θz ≥

1
2γ

min θ

x2, |x| ≤ M z

s.t.

(θ, z, x) ∈ K1/p
pow

min θ

Log(cid:15) + Big-M

− log(x + (cid:15)), if 0 ≤ x ≤ M −z log(x/z + (cid:15)), if x ≤ M z

s.t.

(x + z(cid:15), z, −θ) ∈ Kexp,

Entropy

x log x

x log(x/z), if x > 0

s.t.

(z, x, −θ) ∈ Kexp,

x ≤ M z

min θ

Softplus+Big-M log(1 + exp(x)), if |x| ≤ M z log(1 + exp(x/z)), if |x| ≤ M z

x ≤ M z

min θ

s.t.

z ≥ u + v, |x| ≤ M z,

(u, z, −θ) ∈ Kexp,

(v, z, x − θ) ∈ Kexp

A New Perspective on Low-Rank Optimization

9

2.3 Perspective Cuts

Another computationally useful application of the perspective reformulation technique has been to
derive a class of cutting-planes for MIOs with logical constraints [36]. To motivate our generalization
of these cuts to low-rank problems, we now brieﬂy summarize their main result.

Consider the following problem:

min
z∈Z

min
x∈Rn

c(cid:62)z + f (x) +

n
(cid:88)

i=1

Ωi(xi)

s.t. Aixi ≤ bizi ∀i ∈ [n],

(11)

where {xi : Aixi ≤ 0} = {0}, which implies the set of feasible x is bounded, Ωi(xi) is a closed
convex function, we take Ωi(0) = 0 as in [36] for simplicity, and f (x) is a convex function. Then,
letting ρi model the epigraph of Ωi(xi) + cizi and si be a subgradient of Ωi at ¯xi, i.e., si ∈ ∂Ωi(¯xi),
we have the following result [36, 41]:

Proposition 1 The following cut

ρi ≥ (ci + Ωi(¯xi))zi + si(xi − ¯xizi)

(12)

is valid for the equivalent MINLO:

min
z∈Z

min
x,ρ∈Rn

f (x) +

n
(cid:88)

ρi

i=1
s.t. Aixi ≤ bizi ∀i ∈ [n],

Remark 1 In the special case where Ωi(xi) = x2
ρi ≥ 2xi ¯xi − ¯x2

i , the cut reduces to:

i zi + cizi ∀¯xi.

ρi ≥ Ωi(xi) + cizi ∀i ∈ [n].

(13)

The class of cutting planes deﬁned in Proposition 1 are commonly referred to as perspective cuts,
because they deﬁne a linear lower approximation of the perspective function of Ωi(xi), gΩi(xi, zi).
Consequently, Proposition 1 implies that a perspective reformulation of (11) is equivalent to adding
all (inﬁnitely many) perspective cuts (12). This may be helpful where the original problem is
nonlinear, as a sequence of linear MIOs can be easier to solve than one nonlinear MIO [see 37, for a
comparison].

3 The Matrix Perspective Function and Its Applications

In this section, we generalize the perspective function from vectors to matrices, and invoke the matrix
perspective function to propose a new technique for generating strong yet eﬃcient relaxations of a
diverse family of low-rank problems, which we call the Matrix Perspective Reformulation Technique
(MPRT). Selected background on matrix analysis [see 13, for a general theory] and semideﬁnite
optimization [see 74, for a general theory] which we use throughout this section can be found in
Appendix A.

10

D. Bertsimas, R. Cory-Wright, J. Pauphilet

3.1 A Matrix Perspective Function

To generalize the ideas from the previous section to low-rank constraints, we require a more expressive
transform than the perspective transform, which introduces a single (scalar) additional degree of
freedom and cannot control the eigenvalues of a matrix. Therefore, we invoke a generalization from
quantum mechanics—the matrix perspective function deﬁned in [26, 27], building upon the work
of [28]; see also [53, 54, 55, 24] for a related generalization of perspective functions to perspective
functionals.

Deﬁnition 1 For a matrix-valued function f : X → S n
perspective function of f , gf , is deﬁned as

+ where X ⊆ S n is a convex set, the matrix

gf (X, Y ) =




Y 1

2 f

(cid:16)



∞

Y − 1

2 XY − 1

2

(cid:17)

Y 1

2

if Y − 1

2 XY − 1

2 ∈ X , Y (cid:31) 0,

otherwise.

Remark 2 If X and Y commute and f is analytic, then Deﬁnition 1 simpliﬁes into Y f (cid:0)Y −1X(cid:1),
which is the analog of the usual deﬁnition of the perspective function originally stated in [28].
Deﬁnition 1, however, generalizes this deﬁnition to the case where X and Y do not commute by
ensuring that Y − 1
2 is nonetheless symmetric, in a manner reminiscent of the development of
interior point methods [see, e.g., 2]. In particular, if Y is a projection matrix such that X = Y X–as
occurs for the exact formulations of the low-rank problems we consider in this paper–then it is
safe to assume that X, Y commute. However, when Y is not a projection matrix, this cannot be
assumed in general.

2 XY − 1

The matrix perspective function generalizes the deﬁnition of the perspective transformation to

matrix-valued functions and satisﬁes analogous properties:

Proposition 2 Let f be a matrix-valued function and gf its matrix perspective function. Then:

(a) f is matrix convex, i.e.,

tf (X) + (1 − t)f (W ) (cid:23) f (tX + (1 − t)W ) ∀X, W ∈ S n, t ∈ [0, 1],

(14)

if and only if gf is matrix convex in (X, Y ).

(b) gf is a positive homogeneous function, i.e., for any µ>0 we have

gf (µX, µY ) = µgf (X, Y ).

(c) Let Y (cid:31) 0 be a positive deﬁnite matrix. Then, letting the epigraph of f be denoted by

epi(f ) := {(X, θ) : X ∈ dom(f ), f (X) (cid:22) θ},

(15)

(16)

we have (X, Y , θ) ∈ epi(gf ) if and only if (Y − 1

2 XY − 1

2 , Y − 1

2 θY − 1

2 ) ∈ epi(f ).

Proof We prove the claims successively:

A New Perspective on Low-Rank Optimization

11

(a) This is precisely the main result of Ebadian et al. [26, Theorem 2.2].
(b) For µ > 0, gf (µX, µY ) = µY 1
(µY )− 1
2 f
(c) By generalizing the main result in [15, Chapter 3.2.6], for any Y (cid:31) 0 we have that

2 = µgf (X, Y ).

2 µX(µY )− 1

Y 1

(cid:16)

(cid:17)

2

(X, Y , θ) ∈ epi(gf ) ⇐⇒ Y

1

2 f (Y − 1

2 XY − 1

⇐⇒ f (Y − 1
⇐⇒ (Y − 1

2 XY − 1
2 XY − 1

1
2 (cid:22) θ,
2 θY − 1
2 ,

2 )Y
2 ) (cid:22) Y − 1
2 , Y − 1

2 θY − 1

2 ) ∈ epi(f ). (cid:117)(cid:116)

We now specialize our attention to matrix-valued functions deﬁned by a scalar convex function,

as suggested in the introduction.

3.2 Matrix Perspectives of Operator Functions

From any function ω : R → R, we can deﬁne its extension to the set of symmetric matrices,
fω : S n → S n as

fω(X) = U Diag(ω(λx

1 ), . . . , ω(λx

n))U (cid:62),

(17)

1 , . . . , λx

where X = U Diag(λx
n)U (cid:62) is an eigendecomposition of X. Functions of this form are
called operator functions [see 13, for a general theory]. In particular, one can show that fω(X) is
well-deﬁned (does not depend explicitly on the eigenbasis of X, U ). Among other examples, taking
ω(x) = exp(x) (resp. log(x)) provides a matrix generalization of the exponential (resp. logarithm)
function; see Appendix A.1.

Central to our analysis is that we can explicitly characterize the closure of the matrix perspective
of fω under some assumptions on ω, i.e., deﬁne by continuity gfω (X, Y ) for rank-deﬁcient matrices
Y :

Proposition 3 Consider a function ω : R → R satisfying Assumption 1. Then, the closure of the
matrix perspective of fω is, for any X ∈ S n, Y ∈ S n
+,

(cid:40)

gfω (X, Y ) =

Y 1

2 fω(Y − 1

2 XY − 1

2 )Y 1

2

if Span(X) ⊆ Span(Y ), Y (cid:23) 0,

∞

otherwise,

where Y − 1

2 denotes the pseudo-inverse of the square root of Y .

Remark 3 Note that in the expression of gfω above, the matrix Y − 1
2 is unambiguously
deﬁned if and only if Span(X) ⊆ Span(Y ) (otherwise, its value depends on how we deﬁne the
pseudo-inverse of Y 1
2 outside of its range). Accordingly, in the remainder of the paper, we omit
the condition Span(X) ⊆ Span(Y ) whenever the analytic expression for gfω explicitly involves
Y − 1

2 XY − 1
2 .

2 XY − 1

12

D. Bertsimas, R. Cory-Wright, J. Pauphilet

The proof of Proposition 3 is deferred to Appendix B.1. In the appendix, we also present an
immediate extension where additional constraints, X ∈ X , are imposed on the argument of fω. As
in our prior work Bertsimas et al. [11], we reformulate the rank constraints in (1) by introducing a
projection matrix Y to encode for the span of X. Naturally, Y should be rank-deﬁcient. Hence,
Proposition 3 ensures that having tr(gfω (X, Y )) < ∞ is a suﬃcient condition for Y to indeed
control Span(X).

To gain intuition on how the matrix perspective function transforms X and Y , we now provide
an interesting connection between the matrix perspective of fω and the perspective of ω in the case
where X and Y commute.

Proposition 4 Consider two matrices X ∈ S n, Y ∈ S n
+ that commute and such that Span(X) ⊆
Span(Y ). Hence, there exists an orthogonal matrix U which jointly diagonalizes X and Y . Let
λx
1 , . . . , λx
n denote the eigenvalues of X and Y respectively, ordered according to this
basis U . Consider an operator function fω with ω satisfying Assumption 1. Then, we have that:

n and λy

1, . . . , λy

gfω (X, Y ) = U Diag (gω(λx

1 , λy

1), . . . , gω(λx

n, λy

n)) U (cid:62)

Proof By simultaneously diagonalizing X and Y , we get

Y − 1

2 XY − 1
2 = U Diag (λx
(cid:17)

1 /λy
= U Diag (ω(λx

1, . . . , λx
1 /λy

n/λy
1), . . . , ω(λx

n) U (cid:62),
n/λy

(cid:16)

Y − 1

n)) U (cid:62),

fω
(cid:16)

2

2 XY − 1
(cid:17)

Y

1

2 fω

Y − 1

2 XY − 1

2

Y

1

2 = U Diag (λy

1ω(λx

1 /λy

1), . . . , λy

nω(λx

n/λy

n)) U (cid:62). (cid:117)(cid:116)

Note that if Y is a projection matrix such that Span(X) ⊆ Span(Y ) then we necessarily have

that X = Y X = XY and the assumptions of Proposition 4 hold.

In the general case where X and Y do not commute, we cannot simultaneously diagonalize
them. However, we can still project Y onto the space of matrices that commute with X. We show in
Appendix B.2 that this is a trace preserving operation that can only reduce the value of tr (gfω (X, ·)).

3.3 The Matrix Perspective Reformulation Technique

Deﬁnition 1 and Proposition 3 supply the necessary language to lay out our Matrix Perspective
Reformulation Technique (MPRT). Therefore, we now state the technique; details regarding its
implementation will become clearer throughout the paper.

Let us revisit Problem (1), and assume that the term Ω(X) satisﬁes the following properties:

Assumption 3 Ω(X) = tr (fω(X)), where ω is a function satisfying Assumption 1 and whose
associated operator function, fω, is matrix convex.

A New Perspective on Low-Rank Optimization

13

Assumption 3 implies that the regularizer can be rewritten as operating on the eigenvalues of
X, λi(X), directly: Ω(X) = (cid:80)
i∈[n] ω(λi(X)). As we discuss in the next section, a broad class of
functions satisfy this property. For ease of notation, we refer to fω as f in the remainder of the
paper (and accordingly denote by gf its matrix perspective function).

After letting an orthogonal projection matrix Y model the rank of X—as per [11]—Problem (1)

admits the equivalent mixed-projection reformulation:

min
Y ∈Y k
n

min
X∈S n

(cid:104)C, X(cid:105) + µ · tr(Y ) + tr(f (X))

(18)

s.t.

(cid:104)Ai, X(cid:105) = bi ∀i ∈ [m], X = Y X, X ∈ K,

where Y ∈ Y k

n is the set of n × n orthogonal projection matrices with trace at most k:

n := (cid:8)Y ∈ S n
Y k
Note that for k ∈ N, the convex hull of Y k
which is a well-studied object in its own right [59, 60, 51, 61].

+ : Y 2 = Y , tr(Y ) ≤ k(cid:9) .
n is given by Conv(Y k

n) = {Y ∈ S n

+ : Y (cid:22) I, tr(Y ) ≤ k},

Since Y is an orthogonal projection matrix, imposing the nonlinear constraint X = Y X and
introducing the term Ω(X) = tr(f (X)) in the objective is equivalent to introducing the following
term in the objective:

where gf is the matrix perspective of f , and thus Problem (18) is equivalent to:

tr(gf (X, Y )) + (n − tr(Y ))ω(0),

min
Y ∈Y k
n

min
X∈S n

(cid:104)C, X(cid:105) + µ · tr(Y ) + tr(gf (X, Y )) + (n − tr(Y ))ω(0)

(19)

s.t.

(cid:104)Ai, X(cid:105) = bi ∀i ∈ [m], X ∈ K,

Let us formally state and verify the equivalence between Problems (18)-(19) via:

Theorem 1 Problems (18)-(19) attain the same optimal objective value.

Proof It suﬃces to show that for any feasible solution to (18) we can construct a feasible solution to
(19) with an equal or lower cost, and vice versa:

– Let (X, Y ) be a feasible solution to (18). Since X = Y X ∈ S n, X and Y commute. Hence, by

Proposition 4, we have (using the same notation as in Proposition 4):

tr (gf (X, Y )) =

(cid:88)

i∈[n]

gω (λx

i , λy

i ) =

1{λy

i > 0}ω(λx

i ),

(cid:88)

i∈[n]

i > 0} is an indicator function which denotes whether the ith eigenvalue of Y (which

where 1{λy
is either 0 or 1) is strictly positive. Moreover, since X = Y X, λy
(cid:88)

(cid:88)

tr (f (X)) =

ω(λx

i ) = tr (gf (X, Y )) +

i = 0 =⇒ λx
1{λy

i = 0}ω(0)

i = 0 and

i∈[n]

i∈[n]

= tr (gf (X, Y )) + (n − tr(Y ))ω(0).

(20)

This establishes that (X, Y ) is feasible in (19) with the same cost.

14

D. Bertsimas, R. Cory-Wright, J. Pauphilet

– Let (X, Y ) be a feasible solution to (19). Then, it follows that X ∈ Span(Y ), which implies that
X = Y X since Y is a projection matrix. Therefore, (20) holds, which establishes that (X, Y )
(cid:117)(cid:116)
is feasible in (18) with the same cost.

Eventually, relaxing Y ∈ Y k

n in Problem (19) supplies as strong—and sometimes signiﬁcantly

stronger—relaxations than by any other technique we are aware of, as we explore in Section 4.

Remark 4 Note that, based on the proof of Theorem 1, we could replace gf (X, Y ) in (19) by any
function ˜g(X, Y ) such that gf (X, Y ) = ˜g(X, Y ) for X, Y that commute, with no impact on the
objective value. However, it might impact tractability if ˜g(X, Y ) is not convex in (X, Y ).

Remark 5 Under Assumption 3, the regularization term Ω(X) penalizes all eigenvalues of fω(X)
equally. The MPRT can be extended to a wider class of regularization functions that penalize the
largest eigenvalues more heavily, at the price of (a signiﬁcant amount of) additional notation. For
brevity, we lay out this extension in Appendix C.

Theorem 1 only uses the fact that f is an operator function with ω satisfying Assumption 1, not
the fact that f is matrix convex. In other words, (19) is always an equivalent reformulation of (18).
An interesting question is to identify the set of necessary conditions for the objective of (19) to be
convex in (X, Y )–f being matrix convex is clearly suﬃcient. The objective in (19) is convex only as
long as tr (gf ) is. Interestingly, this is not equivalent to the convexity of tr(f ). See Appendix B.3 for
a counter-example. It is, however, an open question whether a weaker notion than matrix convexity
could ensure the joint convexity of tr(gf ). It would also be interesting to investigate the beneﬁts and
the tractability of non-convex penalties (either by having f not matrix convex or ω non-convex),
given the successes of non-convex penalty functions in sparse regression problems [76, 29].

3.4 Convex Hulls of Low-Rank Sets and the MPRT

We now show that, for a general class of low-rank sets, applying the MPRT is equivalent to taking
the convex hull of the set. This is signiﬁcant, because we are not aware of any general-purpose
techniques for taking convex hulls of low-rank sets. Formally, we have the following result:

Theorem 2 Consider an operator function f = fω satisfying Assumption 3. Let

T = {X ∈ S n : tr(f (X)) + µ · Rank(X) ≤ t, Rank(X) ≤ k}

(21)

be a set where t ∈ R, k ∈ N are ﬁxed. Then, an extended formulation of the convex hull of T is given
by:

T c = (cid:8)(X, Y ) ∈ S n × Conv(Y k

n) : tr(gf (X, Y )) + µ · tr(Y ) + (n − tr(Y ))ω(0) ≤ t(cid:9) .

(22)

n) = {Y ∈ S n
Where Conv(Y k
and gf is the matrix perspective function of f .

+ : Y (cid:22) I, tr(Y ) ≤ k} is the convex hull of trace-k projection matrices,

A New Perspective on Low-Rank Optimization

15

Proof We prove the two directions sequentially:

– Conv (T ) ⊆ T c: let X ∈ T . Then, since the rank of X is at most k, there exists some Y ∈ Y k
n
such that X = Y X and tr(Y ) = Rank(X). Moreover, by the same argument as in the proof of
Theorem 1, it follows that (20) holds and tr(gf (X, Y )) + µ · tr(Y ) + (n − tr(Y ))ω(0) ≤ t, which
conﬁrms that (X, Y ) ∈ T c. Since T c is a convex set, we therefore have Conv (T ) ⊆ T c.

– T c ⊆ Conv (T ): let (X, Y ) ∈ T c. Our proof uses Proposition 4, which requires X and Y to
commute. Let X denote the set of matrices that commute with X: X := {M : XM = M X}.
Denote Y|X the projection of Y onto X . By Lemma 4, we have that Y|X ∈ Conv(Y k
n), and
tr (cid:0)gf (X, Y|X )(cid:1) ≤ tr (gf (X, Y )) < ∞ so (X, Y|X ) ∈ T c as well. Hence, without loss of generality,
by renaming Y ← Y|X , we can assume that X and Y commute. Then, it follows from Proposition
4 that the vectors of eigenvalues of X and Y (ordered according to a shared eigenbasis U ),
(λ(X), λ(Y )) belong to the set

(cid:40)

(x, y) ∈ Rn × [0, 1]n :

(cid:88)

i

yi ≤ k,

n
(cid:88)

i=1

yiω

(cid:17)

(cid:16) xi
yi

(cid:88)

+ µ

yi + (n −

(cid:41)

yi)ω(0) ≤ t

,

(cid:88)

i

which, by [41, Lemma 6], is the convex hull of

(cid:40)

U c :=

(x, y) ∈ Rn × {0, 1}n :

(cid:88)

i

yi ≤ k,

n
(cid:88)

i=1

ω (xi) + µ

i

(cid:88)

i

yi ≤ t, xi = 0 if yi = 0 ∀i ∈ [n]

.

(cid:41)

Let us decompose (λ(X), λ(Y )) into λ(X) = (cid:80)
(cid:80)
k αk = 1, and (x(k), y(k)) ∈ U c. By deﬁnition,

k αkx(k), λ(Y ) = (cid:80)

k αky(k), with αk ≥ 0,

T (k) := U Diag(x(k))U (cid:62) ∈ T

and X = (cid:80)

k αkT (k). Therefore, we have that X ∈ Conv(T ), as required.

(cid:117)(cid:116)

Remark 6 Since linear optimization problems over convex sets admit extremal optima, Theorem
2 demonstrates that unconstrained low-rank problems with spectral objectives can be recast as
linear semideﬁnite problems, where the rank constraint is dropped without loss of optimality. This
suggests that work on hidden convexity in low-rank optimization, i.e., deriving conditions under
which low-rank linear optimization problems admit exact relaxations where the rank constraint is
omitted [see, e.g., 61, 73, 12], could be extended to incorporate spectral functions.

3.5 Examples of the Matrix Perspective Function

Theorem 2 demonstrates that, for spectral functions under low-rank constraints, taking the matrix
perspective is equivalent to taking the convex hull. To highlight the utility of Theorems 1-2, we
therefore supply the perspective functions of some spectral regularization functions which frequently
arise in the low-rank matrix literature, and summarize them in Table 2. We also discuss how these
functions and their perspectives can be eﬃciently optimized over. Note that all functions introduced
in this section are either matrix convex or the trace of a matrix convex function, and thus supply
valid convex relaxations when used as regularizers for the MPRT.

16

D. Bertsimas, R. Cory-Wright, J. Pauphilet

Spectral constraint: Let ω(x) = 0 if |x| ≤ M , +∞ otherwise. Then,

f (X) =

(cid:40)

0

if (cid:107)X(cid:107)σ ≤ M,

+∞ otherwise,

for X ∈ S n, where (cid:107) · (cid:107)σ denotes the spectral norm, i.e., the largest eigenvalue in absolute magnitude
of X. Observe that the condition (cid:107)X(cid:107)σ ≤ M can be expressed via semideﬁnite constraints−M I (cid:22)
X (cid:22) M I. The perspective function gf can then be expressed as

gf (X, Y ) =

(cid:40)

0

if − M Y (cid:22) X (cid:22) M Y ,

+∞ otherwise.

If X and Y commute, gf (X, Y ) requires that |λj(X)| ≤ M λj(Y ) ∀j ∈ [n]–the spectral analog of a
big-M constraint. This constraint can be modeled using two semideﬁnite cones, and thus handled
by semideﬁnite solvers.

Convex quadratic: For ω(x) = x2, f (X) = X (cid:62)X. Then, the perspective function gf is

gf (X, Y ) =

(cid:40)

X (cid:62)Y †X if Y (cid:23) 0,

+∞

otherwise.

Observe that this function’s epigraph is semideﬁnite-representable. Indeed, by the Schur complement
lemma [16, Equation 2.41], minimizing the trace of gf (X, Y ) is equivalent to solving

min
θ∈S n,Y ∈S n,X∈S n

tr(θ)

s.t.

(cid:33)

(cid:32)

θ X
X (cid:62) Y

(cid:23) 0.

Interestingly, this perspective function allows us to rewrite the rank-k SVD problem

min
X∈Rn×m

(cid:107)X − A(cid:107)2

F : Rank(X) ≤ k

as a linear optimization problem over the set of orthogonal projection matrices, which implies that
the orthogonal projection constraint can be relaxed to its convex hull without loss of optimality (since
some extremal solution will be optimal for the relaxation). This is signiﬁcant, because while rank-k
SVD is commonly thought of as a non-convex problem which “surprisingly” admits a closed-form
solution, the MPRT shows that it actually admits an exact convex reformulation:

min
X,Y ,θ

1
2

tr(θ) − (cid:104)A, X(cid:105) +

1
2

(cid:107)A(cid:107)2

F s.t. Y (cid:22) I, tr(Y ) ≤ k,

(cid:33)

(cid:32)

θ X
X (cid:62) Y

(cid:23) 0.

Note that, in the above formulation, we extended our results for symmetric matrices to rectangular
matrices X ∈ Rn×m without justiﬁcation. We rigorously derive this extension for f (X) = X (cid:62)X in
Appendix D and defer the study of the general case to future research.

A New Perspective on Low-Rank Optimization

17

Spectral plus convex quadratic: Let

f (X) =

(cid:40)

X (cid:62)X if (cid:107)X(cid:107)σ ≤ M,
+∞ otherwise,

for X ∈ S n. Then, the perspective function gf is

gf (X, Y ) =

(cid:40)

X (cid:62)Y †X if − M Y (cid:22) X (cid:22) M Y ,

+∞

otherwise.

This can be interpreted as the spectral analog of combining a big-M and a ridge penalty.

Convex quadratic over completely positive cone: Consider the following optimization problem

min
X∈S n

X (cid:62)X s.t. X ∈ Cn
+,

+ = {X : X = U U (cid:62), U ∈ Rn×n

where Cn
+ denotes the completely positive cone. Then, by
denoting f (X) = X (cid:62)X and gf its perspective function we obtain a valid relaxation by minimizing
tr(gf ), which, by the Schur complement lemma [see 16, Equation 2.41], can be reformulated as

+ } ⊆ S n

min
θ∈S n,Y ∈S n,X∈S n

tr(θ)

s.t.

(cid:33)

(cid:32)

θ X
X (cid:62) Y

∈ S 2n

+ , X ∈ Cn
+.

Unfortunately, this formulation cannot be tractably optimized over, since separating over the
completely positive cone is NP-hard. However, by relaxing the completely positive cone to the doubly
non-negative cone—S n
+ —we obtain a tractable and near-exact relaxation. Indeed, as we
shall see in our numerical experiments, combining this relaxation with a state-of-the-art heuristic
supplies certiﬁably near-optimal solutions in both theory and practice.

+ ∩ Rn×n

Note that we could have obtained an alternative relaxation by instead considering the perspective

of

f (X) =

(cid:40)

X (cid:62)X if X ∈ Cn
+,

+∞ otherwise.

Remark 7 One can obtain a nearly identical formulation over the copositive cone [c.f. 17].

Power: Let2 f (X) = X α for α ∈ [0, 1] and X ∈ S n

+. The matrix perspective function is3

gf (X, Y ) =

(cid:40)

Y

1−α

2 X αY

1−α
2

if Y

−1
2 XY

+∞

otherwise.

−1

2 ∈ S n

+, Y (cid:23) 0,

2 Note that f (X) and its perspective are concave functions; hence we model their hypographs, not epigraphs.
3 We only consider the PSD case for notational convenience. However, the symmetric case follows in much the

same manner, after splitting X = X+ − X− : X+, X− (cid:23) 0, (cid:104)X+, X−(cid:105) = 0 and replacing X with X+ + X−.

18

D. Bertsimas, R. Cory-Wright, J. Pauphilet

Remark 8 (Matrix Power Cone) This function’s epigraph, the matrix power cone, i.e.,

Kpow,α

mat = {(X1, X2, X3) ∈ S n

+ × S n

+ × S n : X

1−α
2

2 X α

1 X

1−α
2

2

(cid:23) X3,+ + X3,−}

is a closed convex cone which is semideﬁnite representable for any rational α [31]. Consequently, it
is a tractable object which successfully models the matrix power function (and its perspective) and
we shall make repeated use of it when we apply the MPRT to several important low-rank problems
in Section 3.5.

Logarithm: Let f (X) = − log(X) be the matrix logarithm function. We have that

gf (X, Y ) =




−Y 1

2 log

(cid:16)



+∞

Y − 1

2 XY − 1

2

(cid:17)

Y 1

2

if X, Y (cid:31) 0,

otherwise.

Observe that when X and Y commute, gf (X, Y ) can be rewritten as Y (log(Y ) − log(X)), which
is the quantum relative entropy function [see 32, for a general theory]. We remark that the domain
of log(X) requires that X is full-rank, which at a ﬁrst glance makes the use of this function
problematic for low-rank optimization. Accordingly, we consider the (cid:15)−logarithm function, i.e.,
log(cid:15)(X) = log(X + (cid:15)I) for (cid:15) > 0, as advocated by Fazel et al. [34] in a diﬀerent context. Note that
background on the matrix exponential and logarithm functions can be found in Appendix A.

Observe that tr(log(X)) = log det(X) while tr(gf ) = tr(X(log(X) − log(Y )). Thus, the matrix
logarithm and its trace verify the concavity of the logdet function—which has numerous applications
in low-rank problems [34] and interior point methods [66] among others—while the perspective of
the matrix logarithm provides an elementary proof of the convexity of the quantum relative entropy:
a task for which perspective-free proofs are technically demanding [28].

Von Neumann entropy: Let f (X) = X log(X) denote the von Neumann quantum entropy of a
density matrix X. Then, its perspective function is gf (X, Y ) = XY − 1
2 . When
X and Y commute, this perspective can be equivalently written as

2 log(Y − 1

2 XY − 1

2 )Y 1

(cid:40)

gf (X, Y ) =

X 1

2 log(Y − 1

2 XY − 1

2 )X 1

2

+∞

if X, Y (cid:31) 0,

otherwise.

which is referred to as the Umegaski relative entropy or the matrix Kullback-Leibler divergence
in the literature. Note that various generalizations of the relative entropy for matrices have been
proposed in the quantum physics literature [44]. However, these diﬀerent deﬁnitions agree on the set
of commuting matrices, hence can be used interchangeably for optimization purposes (see Remark
4).

Remark 9 (Quantum relative entropy cone) Note the epigraph of gf , namely,

Kop, rel

mat = {(X1, X2, X3) ∈ S n × S n

++ × S n

++ : X1 (cid:23) −X

1
2

2 log(X − 1

2 X3X − 1

2

2

2

)X

1
2

2 },

A New Perspective on Low-Rank Optimization

19

is a convex cone which can be approximated using semideﬁnite cones and optimized over using
either the Matlab package CVXQuad (see [32]), or optimized over directly using an interior point
method for asymmetric cones [48]4. Consequently, this is a tractable object which models the matrix
logarithm and Von Neumann entropy (and their perspectives).

Finally, Table 2 relates the matrix perspectives discussed above with their scalar analogs.

Table 2: Analogy between perspectives of scalars and perspectives of matrix convex functions.

Perspective of function

Matrix perspective of function

Type

f (x) : R → R

gf (x, t)

Ref.

f

gf

Quadratic

Power

Log

Entropy

x2/t

x2
−xα : 0 < α < 1 −xαt1−α
−t log( x
t )
− log(x)
x log( x
t )

x log(x)

[4]

[15]

[15]

X (cid:62)X
−X α
− log(X)

[15] X log(X) X

X (cid:62)Y †X

1−α

1−α
2

−Y

−Y
1

2 X αY
(cid:16)
X − 1
2 XY − 1

1
2 log
2 log(Y − 1

2 Y X − 1

2

2 )X

1
2

(cid:17)

1
2

Y

Ref.

[11]

Prop. 3

[32]

[52, 28]

3.6 Matrix Perspective Cuts

We now generalize the perspective cuts of [36, 41] from vectors to matrices and cardinality to rank
constraints. Let us reconsider the previously deﬁned mixed-projection optimization problem:

min
Y ∈Y k
n

min
X∈S n
+

(cid:104)C, X(cid:105) + µ · tr(Y ) + tr(f (X)) s.t. (cid:104)Ai, X(cid:105) = bi ∀i ∈ [m], X = Y X, X ∈ K,

where similarly to [36] we assume that f (0) = 0 to simplify the cut derivation procedure. Letting θ
model the epigraph of f via θ (cid:23) f (X) and S be a subgradient of f at ¯X, we have:

θ (cid:23) f ( ¯X)Y + S(cid:62)(X − ¯XY ),

(23)

which if f (X) = X 2 —as discussed previously—reduces to

θi (cid:23) ¯X(2X − ¯XY ),

which is precisely the analog of perspective cuts in the vector case. Note however that these cuts
require semideﬁnite constraints to impose, which suggests they may not be as practically useful. For
instance, our prior work [11]’s outer-approximation scheme for low-rank problems has a non-convex
QCQOP master problem, which can only be currently solved using Gurobi, while Gurobi currently
does not support semideﬁnite constraints.

4 Speciﬁcally, if we are interested in quantum relative entropy problems where we minimize the trace of X1, as
occurs in the context of the MPRT, we may achieve this using the domain-driven solver developed by [48]. However,
we are not aware of any IPMs which can currently optimize over the full quantum relative entropy cone.

20

D. Bertsimas, R. Cory-Wright, J. Pauphilet

We remark however that the inner product of Equation (23) with an arbitrary PSD matrix
supplies a valid linear inequality. Two interesting cases of this observation arise when we take the
inner product of the cut with either a rank-one matrix or the identity matrix.
Taking an inner product with the identity matrix supplies the inequality:

tr(θ) ≥ (cid:104)f ( ¯X), Y (cid:105) + (cid:104)S, X − ¯XY (cid:105) ∀Y ∈ Y k
n.

(24)

Moreover, by analogy to [10, Section 3.4], if we “project out” the X variables by decomposing the
problem into a master problem in Y and subproblems in X then this cut becomes the Generalized
Benders Decomposition cuts derived in our prior work [11, Equation (17)].

Alternatively, taking the inner product of the cut with a rank-one matrix bb(cid:62) gives:

b(cid:62)θb ≥ b(cid:62) (cid:0)f ( ¯X)Y + S(cid:62)(X − ¯XY )(cid:1) b.

A further improvement is actually possible: rather than requiring that the semideﬁnite inequality
is non-negative with respect to one rank-one matrix, we can require that it is simultaneously
non-negative in the directions v1 and v2. This supplies the second-order cone [63, Eqn. (8)] cut:

(cid:33)(cid:62)

(cid:32)

v1
v2

(cid:0)θ − f ( ¯X)Y − S(cid:62)(X − ¯XY )(cid:1)

(cid:33)(cid:62)

(cid:32)

v1
v2

(cid:23)

(cid:32)

(cid:33)

0 0
0 0

.

The analysis in this section suggests that applying a perspective cut decomposition scheme
out-of-the-box may be impractical, but leaves the door open to adaptations of the scheme which
account for the projection matrix structure.

4 Examples and Perspective Relaxations

In this section, we apply the MRPT to several important low-rank problems, in addition to the
previously discussed reduced-rank regression problem (Section 1.1). We also recall Theorem 2 to
demonstrate that applying the MPRT to spectral functions which feature in these problems actually
gives the convex hull of relevant substructures.

4.1 Matrix Completion

Given a sample (Ai,j : (i, j) ∈ I ⊆ [n] × [n]) of a matrix A ∈ S n
+, the matrix completion problem is
to reconstruct the entire matrix, by assuming A is approximately low-rank [19]. Letting µ, γ > 0 be
penalty multipliers, this problem admits the formulation:

min
X∈S n
+

(cid:88)

(i,j)∈I

(Xi,j − Ai,j)2 +

1
2γ

(cid:107)X(cid:107)2

F + µ · Rank(X).

(25)

A New Perspective on Low-Rank Optimization

21

Applying the MPRT to the (cid:107)X(cid:107)2

F = tr(X (cid:62)X) term demonstrates that this problem is equivalent

to the mixed-projection problem:

min

X,θ∈S n

+,Y ∈Y n
n

(cid:88)

(i,j)∈I

(Xi,j − Ai,j)2 +

1
2γ

tr(θ) + µ · tr(Y )

s.t.

(cid:33)

(cid:32)

Y X
X θ

(cid:23) 0,

n ) = {Y ∈ S n : 0 (cid:22) Y (cid:22) I} supplies a valid relaxation. We
and relaxing Y ∈ Y n
now argue that this relaxation is often high-quality, by demonstrating that the MPRT supplies the
convex envelope of t ≥ 1

F + µ · Rank(X), via the following corollary to Theorem 2:

n to Y ∈ Conv(Y n

2γ (cid:107)X(cid:107)2

Corollary 1

Let S = (cid:8)(Y , X, θ) ∈ Y k

n × S n

+ × S n : θ (cid:23) X (cid:62)X, uY (cid:23) X (cid:23) (cid:96)Y (cid:9)

be a set where (cid:96), u ∈ R+. Then, this set’s convex hull is given by:

(cid:40)

S c =

(Y , X, θ) ∈ S n

+ × S n

+ × S n : Y (cid:22) I, tr(Y ) ≤ k, uY (cid:23) X (cid:23) (cid:96)Y ,

(cid:32)

Y X
X (cid:62) θ

(cid:33)

(cid:41)

(cid:23) 0

.

4.2 Tensor Completion

A central problem in machine learning is to reconstruct a d-tensor X given a subsample of its
entries (Ai1,...id : (i1, . . . id) ∈ I ⊆ [n1] × [n2] × . . . × [nd]), by assuming that the tensor is low-
rank. Since even evaluating the rank of a tensor is NP-hard [49], a popular approach for solving
this problem is to minimize the reconstruction error while constraining the ranks of diﬀerent
unfoldings of the tensor [see, e.g., 39]. After imposing Frobenius norm regularization and letting
(cid:107) · (cid:107)HS =
i1,...,id
norm of a tensor, this leads to optimization problems of the form:

denote the (second-order cone representable) Hilbert-Schmidt

i1=1 . . . (cid:80)nd

id=1 X 2

(cid:113)(cid:80)n1

min
X∈Rn1×...×nd

(cid:88)

(Ai1,...id − Xi1,...id )2 +

(i1,...id)∈I

n
(cid:88)

i=1

(cid:107)X(i)(cid:107)2
F

s.t. Rank(X(i)) ≤ k ∀i ∈ [n]. (26)

Similarly to low-rank matrix completion, it is tempting to apply the MRPT to model the X (cid:62)
(i)X(i)
term for each mode-n unfolding. We now demonstrate this supplies a tight approximation of the
convex hull of the sum of the regularizers, via the following lemma (proof omitted, follows in the
spirit of [41, Lemma 4]):

Lemma 2

(cid:40)

Let Q =

(ρ, Y1, . . . , Ym, X1, . . . , Xm, θ1, . . . , θm) : ρ ≥

qitr(θi), (Xi, Yi, θi) ∈ S i ∀i ∈ [m]

(cid:41)

m
(cid:88)

i=1

be a set where li, ui, qi ∈ Rn
li, ui. Then, an extended formulation of this set’s convex hull is given by:

+ ∀i ∈ [m], and Si is a set of the same form as S, but l, u are replaced by

(cid:40)

Qc =

(ρ, Y1, . . . , Ym, X1, . . . , Xm, θ1, . . . , θm) : ρ ≥

qitr(θi), (Xi, Yi, θi) ∈ S c

i ∀i ∈ [m]

.

(cid:41)

m
(cid:88)

i=1

22

D. Bertsimas, R. Cory-Wright, J. Pauphilet

Lemma 2 suggests that the MPRT may improve algorithms which aim to recover tensors of low
slice rank. For instance, in low-rank tensor problems where (26) admits multiple local solutions,
solving the convex relaxation coming from Qc and greedily rounding may give a high-quality initial
point for an alternating minimization method such as the method of [30], and indeed allow such a
strategy to return better solutions than if it were initialized at a random point.

Note however that Lemma 2 does not necessarily give the convex hull of the sum of the regularizers,
since the regularization terms involve diﬀerent slices of the same tensor and thus interact; see also
[68] for a related proof that the tensor trace norm does not give the convex envelope of the sum of
ranks of slices.

4.3 Low-Rank Factor Analysis

An important problem in statistics, psychometrics and economics is to decompose a covariance
matrix Σ ∈ S n
+, as explored by
Bertsimas et al. [8] and references therein. This corresponds to solving:

+ into a low-rank matrix X ∈ S n

+ plus a diagonal matrix Φ ∈ S n

min
X,Φ∈S n
+

(cid:107)Σ − Φ − X(cid:107)q

q s.t. Rank(X) ≤ k, Φi,j = 0, ∀i, j ∈ [n] : i (cid:54)= j, (cid:107)X(cid:107)σ ≤ M

(27)

where q ≥ 1, (cid:107)X(cid:107)q = ((cid:80)n
i=1 λi(X)q)
norm of X via a big-M constraint for the sake of tractability.

1
q denotes the matrix q-norm, and we constrain the spectral

This problem’s objective involves minimizing tr (Σ − Φ − X)q, and it is not immediately obvious
how to either apply the technique in the presence of the Φ variables or alternatively seperate out the
Φ term and apply the MPRT to an appropriate (Φ-free) substructure. To proceed, let us therefore
ﬁrst consider its scalar analog, obtaining the convex closure of the following set:

T = {(x, y, z, t) ∈ R × R × {0, 1} × R+ : t ≥ |x + y − d|q, |x| ≤ M, x = 0 if z = 0},

where d ∈ R and q ≥ 1 are ﬁxed constants, and we require that |x| ≤ M for the sake of tractability.
We obtain the convex closure via the following proposition (proof deferred to Appendix B):

Proposition 5 The convex closure of the set T , T c, is given by:

(cid:26)

T c =

(x, y, z, t) ∈ R × R × [0, 1] × R+ : ∃β ≥ 0 : t ≥

|y − β − d(1 − z)|q
(1 − z)q−1

+

|x + β − dz|q
zq−1

(cid:27)

, |x| ≤ M z

.

Remark 10 To check that this set is indeed a valid convex relaxation, observe that if z = 0 then
x = 0 and x = −β =⇒ β = 0 and t ≥ |y − d|q, while if z = 1 then y = β and t ≥ |x + y − d|q.

Observe that T c can be modeled using two power cones and one inequality constraint.
Proposition 5 suggests that we can obtain high-quality convex relaxations for low-rank factor
analysis problems via a judicious use of the matrix power cone. Namely, introduce an epigraph

A New Perspective on Low-Rank Optimization

23

matrix θ to model the eigenvalues of (Σ − Φ − X)q and an orthogonal projection matrix Y2 to
model the span of X. This then leads to the following matrix power cone representable relaxation:

min

X,Φ,θ,Y1,Y2∈S n

+,β∈S n

tr(θ)

1−q
2

1
2

1
2

s.t. θ (cid:23) Y
1

(Y

1 ΣY

1 − β − Φ)Y
Y1 + Y2 = I, tr(Y ) ≤ k, Φi,j = 0, ∀i, j ∈ [n] : i (cid:54)= j,
Φ (cid:22) X, X (cid:22) M Y2, −X (cid:22) M Y2.

+ Y

(Y

1

2

1−q
2

1−q
2

1
2

1
2

2 ΣY

2 + β − X)Y
2

1−q
2

,

4.4 Optimal Experimental Design

Letting A ∈ Rn×m where m ≥ n be a matrix of linear measurements of the form yi = a(cid:62)
i β + (cid:15)i from
an experimental setting, the D-optimal experimental design problem (a.k.a. the sensor selection
problem) is to pick k ≤ m of these experiments in order to make the most accurate estimate of β
possible, by solving [see 47, 69, for a modern approach]:

max
z∈{0,1}n:e(cid:62)z≤k

log det

(cid:15)



ziaia(cid:62)
i

 ,





(cid:88)

i∈[n]

(28)

where we deﬁne log det(cid:15)(X) = log det(X + (cid:15)I) for (cid:15) > 0 to be the pseudo log-determinant of a
rank-deﬁcient PSD matrix, which can be thought of as imposing an uninformative prior of importance
(cid:15) on the experimental design process. Since log det(X) = tr(log(X)), a valid convex relaxation is
given by:

max
z∈[0,1]n,θ∈S n
+

tr(θ)

s.t.

log (cid:0)ADiag(z)A(cid:62) + (cid:15)I(cid:1) (cid:23) θ,

which can be modeled using the quantum relative entropy cone, via (−θ, I, ADiag(z)A(cid:62) + (cid:15)I) ∈
Krel, op
. This is equivalent to perhaps the most common relaxation of D-optimal design, as proposed
mat
by Boyd and Vandenberghe [15, Eqn. 7.2.6]. By formulating in terms of the quantum relative entropy
cone, the identity term suggests this relaxation leaves something “on the table”.
i∈[n] ziaia(cid:62)
i

is a rank-k matrix
and thus at an optimal solution to the original problem there is some orthogonal projection matrix
Y such that X = Y X. Therefore, we can take the perspective function of f (X) = log(X+(cid:15)I), and
thereby obtain the following valid—and potentially much tighter when k < n—convex relaxation:

In this direction, let us apply the MPRT. Observe that X := (cid:80)

max
z∈[0,1]n,θ,Y ∈S n
+

tr(θ) + (n − tr(Y )) log((cid:15))

(29)

s.t. Y

1
2 log

(cid:16)

Y − 1

2 ADiag(z)A(cid:62)Y − 1

2 + (cid:15)I

(cid:17)

Y

1

2 (cid:23) θ, Y (cid:22) I, tr(Y ) ≤ k,

which can be modeled via the quantum relative entropy cone: (−θ, Y , ADiag(z)A(cid:62) + (cid:15)Y ) ∈ Krel, op
.
We now argue that this relaxation is high-quality, by demonstrating that the MPRT supplies the
convex envelope of t ≥ − log det(cid:15)(X) under a low-rank constraint, via the following corollary to
Theorem 2:

mat

24

Corollary 2

Let S =

(cid:110)

X ∈ S n

+ : t ≥ − log det
(cid:15)

D. Bertsimas, R. Cory-Wright, J. Pauphilet

(X), Rank(X) ≤ k

(cid:111)

be a set where (cid:15), k, t are ﬁxed. Then, this set’s convex hull is:

(cid:26)

S c =

(Y , X) ∈ S n

+ × S n

+ :0 (cid:22) Y (cid:22) I, tr(Y ) ≤ k,

t ≥ −tr(Y

1

2 log(cid:15)(Y − 1

2 XY − 1

2 )Y

1
2 ) − (n − tr(Y )) log((cid:15))

(cid:27)

.

Remark 11 Observe that (29)’s relaxation is not useful in the over-determined regime where k ≥ n,
since setting Y = I recovers (28)’s Boolean relaxation, which is considerably cheaper to optimize
over. Accordingly, we only consider the under-determined regime in our experiments.

4.5 Non-Negative Matrix Optimization

Many important problems in combinatorial optimization, statistics and computer vision [see, e.g.,
17] reduce to optimizing over the space of low-rank matrices with non-negative factors. An important
special case is when we would like to ﬁnd the low-rank completely positive matrix X which best
approximates (in a least-squares sense) a given matrix A ∈ S n
+, i.e., perform non-negative principal
component analysis. Formally, we have the problem:

min

X∈Cn

+:Rank(X)≤k

(cid:107)X − A(cid:107)2
F ,

(30)

where Cn

+ := {U U (cid:62) : U ∈ Rn×n

+ } denotes the cone of n × n completely positive matrices.

Applying the MPRT to the strongly convex 1

2 (cid:107)X(cid:107)2

F term in the objective therefore yields the

following completely positive program:

min
+,Y ,θ∈S n

X∈Cn

1
2

tr(θ) − (cid:104)X, A(cid:105) +

1
2

(cid:107)A(cid:107)2

F s.t. Y (cid:22) I, tr(Y ) ≤ k,

(cid:33)

(cid:32)

Y X
X (cid:62) θ

∈ S2n
+ .

(31)

Interestingly, since (31)’s reformulation has a linear objective, some extreme point in its relaxation
is optimal, which means we can relax the requirement that Y is a projection matrix without loss
of optimality and the computational complexity of the problem is entirely concentrated in the
completely positive cone. Unfortunately however, completely positive optimization itself is intractable.
Nonetheless, it can be approximated by replacing the completely positive cone with the doubly
non-negative cone, S n

+ ∩ Rn×n

min
+∩Rn×n
+ ,Y ,θ∈S n

X∈S n

1
2

+ . Namely, we instead solve
(cid:32)

tr(θ) − (cid:104)X, A(cid:105) +

(cid:107)A(cid:107)2

F s.t.

1
2

Y X
X (cid:62) θ

(cid:33)

∈ S2n

+ , Y (cid:22) I, tr(Y ) ≤ k.

(32)

Unfortunately, rounding a solution to (32) to obtain a completely positive X is non-trivial.
Indeed, according to Ge and Ye [40], there is currently no eﬀective mechanism for rounding doubly

A New Perspective on Low-Rank Optimization

25

non-negative programs. Nonetheless, as we shall see in our numerical results, there are already
highly eﬀective heuristic methods for completely positive matrix factorization, and combining our
relaxation with such a procedure oﬀers certiﬁcates of near optimality in a tractable fashion.

Remark 12 If X = DΠ is a monomial matrix, i.e., decomposable as the product of a diagonal
matrix D and a permutation matrix Π, as occurs in binary optimization problems such as k-means
clustering problems among others [c.f. 62], then it follows that (X (cid:62)X)† ≥ 0 [see 65] and thus
Y := X(X (cid:62)X)†X (cid:62) is elementwise non-negative. In this case, the doubly non-negative relaxation
(32) should be strengthened by requiring that Y ≥ 0.

5 Numerical Results

In this section, we evaluate the algorithmic strategies derived in the previous section, implemented in
Julia 1.5 using JuMP.jl 0.21.6 and Mosek 9.1 to solve the conic problems considered here. Except
where indicated otherwise, all experiments were performed on a Intel Xeon E5—2690 v4 2.6GHz
CPU core using 32 GB RAM. To bridge the gap between theory and practice, we have made our
code freely available on Github at github.com/ryancorywright/MatrixPerspectiveSoftware.

5.1 Reduced Rank Regression

In this section, we compare our convex relaxations for reduced rank regression developed in the
introduction and laid out in (6)-(7)—which we refer to as “Persp” and “DCL” respectively—against
the nuclear norm estimator proposed by [56] (“NN”), who solve

min
β∈Rp×n

1
2m

(cid:107)Y − Xβ(cid:107)2

F +

1
2γ

(cid:107)β(cid:107)2

F + µ(cid:107)β(cid:107)∗.

(33)

Similarly to [56], we attempt to recover rank−ktrue estimators βtrue = U V (cid:62), where each entry
of U ∈ Rp×ktrue, V ∈ Rn×ktrue is i.i.d. standard Gaussian N (0, 1), the matrix X ∈ Rm×p contains
i.i.d. standard Gaussian N (0, 1) entries, Y = Xβ + E, and Ei,j ∼ N (0, σ) injects a small amount of
i.i.d. noise. We set n = p = 50, k = 10, γ = 106, σ = 0.05 and vary m. To ensure a fair comparison,
we cross-validate µ for both of our relaxations and [56]’s approach so as to minimize the MSE on a
validation set. For each m, we evaluate 20 diﬀerent values of µ which are distributed uniformly in
logspace between 10−4 and 104 across 50 random instances for our convex relaxations and report on
100 diﬀerent random instances with the “best” µ for each method and each p.

Rank recovery and statistical accuracy: Figures 1a-1c report the relative accuracy ((cid:107)βest−βtrue(cid:107)F /(cid:107)βtrue(cid:107)F ),
the rank (i.e., number of singular values of βest which exceed 10−4), and the out-of-sample MSE5
(cid:107)Xnewβest−ynew(cid:107)2
F ).
Results are averaged over 100 random instances per value of m. We observe that—even though we

F (normalized by the out-of-sample MSE of the ground truth (cid:107)Xnewβtrue−ynew(cid:107)2

5 Evaluated on m = 1000 new observations of Xj , Yk generated from the same distribution.

26

D. Bertsimas, R. Cory-Wright, J. Pauphilet

did not supply the true rank of the optimal solution in our formulation—Problem (7)’s relaxation
returns solutions of the correct rank (ktrue = 10) and better MSE/accuracy, while our more “naive”
perspective relaxation (6) and the nuclear norm approach (33) return solutions of a higher rank
and lower accuracy. This suggests that (7)’s formulation should be considered as a more accurate
estimator for reduced rank problems, and empirically conﬁrms that the MPRT can lead to signiﬁcant
improvements in statistical accuracy.

Scalability w.r.t. m: Figure 1d reports the average time for Mosek to converge6 to an optimal solution
(over 100 random instances per m). Surprisingly, although (7) is a stronger relaxation than (6), it is
one to two orders of magnitude faster than (6) and (33)’s formulations. The relative scalability of
(7)’s formulation as m—the number of observation— increases can be explained by the fact that
(7) considers a linear inner product of the Gram matrix X (cid:62)X with a semideﬁnite matrix B (the
size of which does not vary with m) while Problems (6) and (33) have a quadratic inner product
(cid:104)ββ(cid:62), X (cid:62)X(cid:105) which must be modeled using a rotated second-order cone constraint (the size of
which depends on m), since modern conic solvers such as Mosek do not allow quadratic objective
terms and semideﬁnite constraints to be simultaneously present (if they did, we believe all three
formulations would scale similarly).

Scalability w.r.t p: Next, we evaluate the scalability of all three approaches in terms of their solve
times and peak memory usage (measured using the slurm command MaxRSS), as n = p increases.
Fig. 2 depicts the average time to converge to an optimal solution (a) and peak memory consumption
(b) by each method as we vary n = p with m = n, k = 10, γ = 106, each µ ﬁxed to the average
cross-validated value found in the previous experiment, a peak memory budget of 120GB, a runtime
budget of 12 hours, and otherwise the same experimental setup as previously (averaged over 20
random instances per n). We observe (7)’s relaxation is dramatically more scalable than the other two
approaches considered, and can solve problems of nearly twice the size (4 times as many variables),
and solves problems of a similar size in substantially less time and with substantially less peak
memory consumption (40s vs. 1000s when n = 100). All in all, the proposed relaxation (7) seems to
be the best method of the three considered.

5.2 Non-Negative Matrix Factorization

In this section, we benchmark the quality of our dual bound for non-negative matrix factorization
laid out in Section 4.5 by using the non-linear reformulation strategy proposed by [18] (alternating
least squares or ALS) to obtain upper bounds. Namely, we obtain upper bounds by solving for local

6 We model the convex quadratic (cid:107)Xβ − Y (cid:107)2

F using a rotated second order cone for formulations (6) and (33) (the
quadratic term doesn’t appear directly in (7)), model the nuclear norm term in (33) by introducing matrices U , V

such that

(cid:23) 0 and minimizing tr(U ) + tr(V ), use default Mosek parameters for all approaches.

(cid:32)

(cid:33)

U β
β(cid:62) V

A New Perspective on Low-Rank Optimization

27

(a) Accuracy

(b) Rank

(c) Relative MSE

(d) Runtime

Fig. 1: Comparative performance, as the number of samples m increases, of formulations (6) (Persp,
in blue), (7) (DCL, in orange) and (33) (NN, in green), averaged over 100 synthetic reduced rank
regression instances where n = p = 50, ktrue = 10. The hyperparameter µ was ﬁrst cross-validated
for all approaches separately.

minima of the problem

min
U ∈Rn×k
+

(cid:107)U U (cid:62) − A(cid:107)2
F .

(34)

In our implementation of ALS, we obtain a local minimum by introducing a dummy variable V
which equals U at optimality and alternating between solving the following two problems

Ut+1 = arg min

U ∈Rn×k

+

Vt+1 = arg min

V ∈Rn×k

+

(cid:107)U V (cid:62)

t − A(cid:107)2

F + ρt(cid:107)U − Vt(cid:107)2
F ,

(cid:107)UtV (cid:62) − A(cid:107)2

F + ρt(cid:107)Ut − V (cid:107)2
F ,

(35)

(36)

where we set ρt = min(10−4 × 2t−1, 105) at the tth iteration in order that the ﬁnal matrix is positive
semideﬁnite, as advocated in [5, Section 5.2.3] (we cap ρt to avoid numerical instability). We iterate
over solving these two problems from a random initialization point V0—where each V0,i,j is i.i.d.

28

D. Bertsimas, R. Cory-Wright, J. Pauphilet

(a) Runtime

(b) Peak Memory

Fig. 2: Average time to compute an optimal solution (left panel) and peak memory usage (right
panel) vs. dimensionality n = p for Problems (6) (Persp, in blue), (7) (DCL. in orange) and (33)
(NN, in green) over 20 synthetic reduced rank regression instances where ktrue = 10.

standard uniform—until either the objective value between iterations does not change by 10−4 or
we exceed the maximum number of allowable iterations, which we set to 100.

To generate problem instances, we let A = U U (cid:62) + E where U ∈ Rn×ktrue, each Ui,j is uniform
on [0, 1], Ei,j ∼ N (0, 0.0125ktrue), and set Ai,j = 0 if Ai,j < 0. We set n = 50, ktrue = 10. We
use the ALS heuristic to compute a feasible solution X and an upper-bound on the problem’s
objective value. By comparing it with the lower bound derived from our MPRT, we can assess the
sub-optimality of the heuristic solution, which previously lacked optimality guarantees.

Figure 3 depicts the average relative in-sample MSE of the heuristic ((cid:107)X − A(cid:107)F /(cid:107)A(cid:107)F ) and the
relative bound gap—(UB-LB)/UB— as we vary the target rank, averaged over 100 random synthetic
instances. We observe that the method is most accurate and has the lowest MSE when k is set to
ktrue = 10, which conﬁrms that the method can recover solutions of the correct rank. In addition, by
combining the solution from OLS with our lower-bound, we can compute a duality gap and assert
that the heuristic solution is 0% − 3%-optimal, with the gap peaking at k = ktrue and stabilizing as
k → n. This echoes similar ﬁndings in k-means clustering and alternating current optimal power
ﬂow problems, where the SDO relaxation need not be near-tight in theory but nonetheless is nearly
exact in practice [62, 50]. Further, this suggests our convex relaxation may be a powerful weapon
for providing gaps for heuristics for non-negative matrix factorization, and particularly detecting
when they are performing well or can be further improved.

Figure 4 reports the time needed to compute both the upper bound and a lower bound solution

as we vary the target rank.

A New Perspective on Low-Rank Optimization

29

(a) Relative MSE

(b) Bound gap

Fig. 3: Average relative MSE and duality gap vs. target rank k using the ALS heuristic (UB) and
the MPRT relaxation (LB). Results are averaged over 100 synthetic completely positive matrix
factorization instances where n = 50, ktrue = 10.

Fig. 4: Computational time to compute a feasible solution (ALS) and solve the relaxation (Semideﬁnite
bound) vs. target rank k, averaged over 100 synthetic completely positive matrix factorization
instances where n = 50, ktrue = 10.

5.3 Optimal Experimental Design

In this section, we benchmark our dual bound for D-optimal experimental design (29) against the
convex relaxation (28) and a greedy submodular maximization approach, in terms of both bound
quality and the ability of all three approaches to generate high-quality feasible solutions. We round
both relaxations to generate feasible solutions greedily, by setting the k largest zi’s in a continuous
relaxation to 1, while for the submodular maximization approach we iteratively set the jth index of

30

D. Bertsimas, R. Cory-Wright, J. Pauphilet

z to 1, where S is initially an empty set and we iteratively take

S ← S ∪ {j} : j ∈ arg max
i∈[n]\S

log det

(cid:15)

(cid:40)

(cid:33)(cid:41)

zlala(cid:62)

l + aia(cid:62)
i

.

(cid:32)

(cid:88)

l∈S

Interestingly, the greedy rounding approach enjoys rigorous approximation guarantees [see 47, 69],
while the submodular maximization approach also enjoys strong guarantees [see 57].

We benchmark all methods in terms of their performance on synthetic D-optimal experimental
design problems, where we let A ∈ Rn×m be a matrix with i.i.d. N (0, 1√
n ) entries. We set n =
20, m = 10, (cid:15) = 10−6 and vary k < m over 20 random instances. Table 3 depicts the average relative
bound gap, objective values, and runtimes for all 3 methods (we use the lower bound from (28)’s
relaxation to compute the submodular bound gap). Note that all results for this experiment were
generated on a standard Macbook pro laptop with a 2.9GHZ 6-core Intel i9 CPU using 16GB
DDR4 RAM, CVX version 1.22, Matlab R2021a, and Mosek 9.1. Moreover, we optimize over (29)’s
relaxation using the CVXQuad package developed by [32].

Table 3: Average runtime in seconds and relative bound gap per approach, over 20 random instances
where n = 10, m = 20.

Problem (28)+round

Submodular

Problem (29)+round

k Time(s) Gap (%) Time(s) Gap (%) Time(s)

Gap (%)

1
2
3
4
5
6
7
8
9

0.52
0.63
0.59
0.63
0.53
0.53
0.55
0.60
0.54

88.8
93.7
97.1
100.2
103.8
109.0
117.7
136.9
260.9

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

88.9
93.7
97.0
100.2
103.9
109.0
117.7
138.5
287.5

347.0
338.5
320.8
338.7
331.1
287.5
255.1
236.1
235.9

0.00
0.01
0.06
0.18
0.37
1.40
2.39
5.25
28.43

Relaxation quality: We observe that (29)’s relaxation is dramatically stronger than (28), oﬀering
bound gaps on the order of 0% − 3% when k ≤ 7, rather than gaps of 90% or more. This conﬁrms
the eﬃcacy of the MPRT, and demonstrates the value of taking low-rank constraints into account
when designing convex relaxations, even when not obviously present.

Scalability: We observe that (29)’s relaxation is around two orders of magnitude slower than the
other proposed approaches, largely because semideﬁnite approximations of quantum relative entropy
are expensive, but is still tractable for moderate sizes. We believe, however, that the relaxation would
scale signiﬁcantly better if it were optimized over using an interior point method for non-symmetric
cones [see, e.g., 70, 48], or an alternating minimization approach [see 33]. As such, (29)’s relaxation

A New Perspective on Low-Rank Optimization

31

is potentially useful at moderate problem sizes with oﬀ-the-shelf software, or at larger problem sizes
with problem-speciﬁc techniques such as alternating minimization.

6 Conclusion

In this paper, we introduced the Matrix Perspective Reformulation Technique (MPRT), a new
technique for deriving tractable and often high-quality relaxations of a wide variety of low-rank
problems. We also invoked the technique to derive the convex hulls of some frequently-studied
low-rank sets, and provided examples where the technique proves useful in practice. This is signiﬁcant
and potentially useful to the community, because substantial progress on producing tractable upper
bounds for low-rank problems has been made over the past decade, but until now almost no progress
on tractable lower bounds has followed.

Future work could take three directions: (1) automatically detecting structures where the MPRT
could be applied, as is already done for perspective reformulations in the MIO case by CPLEX and
Gurobi, (2) developing scalable semideﬁnite-free techniques for solving the semideﬁnite relaxations
proposed in this paper, and (3) combining the ideas in this paper and in our prior work [11] with
custom branching strategies to solve low-rank problems to optimality at scale.

Acknowledgments: We are very grateful to two anonymous referees for useful and constructive
comments. In particular, we would like to thank reviewer #1 for a very helpful reﬁnement of our
deﬁnition of the matrix perspective function, and reviewer #2 for suggesting the name matrix
perspective function and supplying some new references on perspective operator functions.

References

1. M. S. Akt¨urk, A. Atamt¨urk, and S. G¨urel. A strong conic quadratic reformulation for machine-job
assignment with controllable processing times. Operations Research Letters, 37(3):187–191, 2009.
2. F. Alizadeh. Interior point methods in semideﬁnite programming with applications to combina-

torial optimization. SIAM Journal on Optimization, 5(1):13–51, 1995.

3. A. Atamt¨urk and A. Gomez. Rank-one convexiﬁcation for sparse regression. arXiv:1901.10334,

2019.

4. A. Ben-Tal and A. Nemirovski. Lectures on modern convex optimization: Analysis, algorithms,

and engineering applications, volume 2. SIAM Philadelphia, PA, 2001.

5. D. P. Bertsekas. Nonlinear programming. Athena Scientiﬁc Belmont MA, 3rd edition, 2016.
6. D. Bertsimas and B. Van Parys. Sparse high-dimensional regression: Exact scalable algorithms

and phase transitions. The Annals of Statistics, 48(1):300–323, 2020.

7. D. Bertsimas, A. King, and R. Mazumder. Best subset selection via a modern optimization lens.

The Annals of Statistics, pages 813–852, 2016.

8. D. Bertsimas, M. S. Copenhaver, and R. Mazumder. Certiﬁably optimal low rank factor analysis.

Journal of Machine Learning Research, 18(1):907–959, 2017.

32

D. Bertsimas, R. Cory-Wright, J. Pauphilet

9. D. Bertsimas, J. Pauphilet, and B. Van Parys. Sparse regression: Scalable algorithms and

empirical performance. Statistical Science, 35(4):555–578, 2020.

10. D. Bertsimas, R. Cory-Wright, and J. Pauphilet. A uniﬁed approach to mixed-integer opti-
mization problems with logical constraints. SIAM Journal on Optimization, 31(3):2340–2367,
2021.

11. D. Bertsimas, R. Cory-Wright, and J. Pauphilet. Mixed-projection conic optimization: A new
paradigm for modeling rank constraints. Operations Research, Articles in Advance, 2021.
12. D. Bertsimas, R. Cory-Wright, and J. Pauphilet. Solving large-scale sparse PCA to certiﬁable

(near) optimality. Journal of Machine Learning Research, 23(13):1–35, 2022.

13. R. Bhatia. Matrix analysis, volume 169. Springer Science & Business Media New York, 2013.
14. D. Bienstock. Eigenvalue techniques for convex objective, nonconvex optimization problems.
In International Conference on Integer Programming and Combinatorial Optimization, pages
29–42. Springer, 2010.

15. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge,

UK, 2004.

16. S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan. Linear matrix inequalities in system and
control theory, volume 15. Studies in Applied Mathematics, Society for Industrial and Applied
Mathematics, Philadelphia, PA, 1994.

17. S. Burer. On the copositive representation of binary and continuous nonconvex quadratic

programs. Mathematical Programming, 120(2):479–495, 2009.

18. S. Burer and R. D. Monteiro. A nonlinear programming algorithm for solving semideﬁnite

programs via low-rank factorization. Mathematical Programming, 95(2):329–357, 2003.

19. E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of

Computational Mathematics, 9(6):717, 2009.

20. E. Carlen. Trace inequalities and quantum entropy: an introductory course. Entropy and the

quantum, 529:73–140, 2010.

21. S. Ceria and J. Soares. Convex programming for disjunctive convex optimization. Mathematical

Programming, 86(3):595–614, 1999.

22. R. Chares. Cones and interior-point algorithms for structured convex optimization involving

powers and exponentials. PhD thesis, UCL-Universit´e Catholique de Louvain, 2009.

23. P. L. Combettes. Perspective functions: Properties, constructions, and examples. Set-Valued

and Variational Analysis, 26(2):247–264, 2018.

24. B. Dacorogna and P. Mar´echal. The role of perspective functions in convexity, polyconvexity,

rank-one convexity and separate convexity. Journal of Convex Analysis, 15(2):271–284, 2008.
25. H. Dong, K. Chen, and J. Linderoth. Regularization vs. relaxation: A conic optimization

perspective of statistical variable selection. arXiv:1510.06083, 2015.

26. A. Ebadian, I. Nikoufar, and M. E. Gordji. Perspectives of matrix convex functions. Proceedings

of the National Academy of Sciences, 108(18):7313–7314, 2011.

27. E. Eﬀros and F. Hansen. Non-commutative perspectives. Annals of Functional Analysis, 5(2):

74–79, 2014.

A New Perspective on Low-Rank Optimization

33

28. E. G. Eﬀros. A matrix convexity approach to some celebrated quantum inequalities. Proceedings

of the National Academy of Sciences, 106(4):1006–1008, 2009.

29. J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties.

Journal of the American statistical Association, 96(456):1348–1360, 2001.

30. V. F. Farias and A. A. Li. Learning preferences with side information. Management Science, 65

(7):3131–3149, 2019.

31. H. Fawzi and J. Saunderson. Lieb’s concavity theorem, matrix geometric means, and semideﬁnite

optimization. Linear Algebra and its Applications, 513:240–263, 2017.

32. H. Fawzi, J. Saunderson, and P. A. Parrilo. Semideﬁnite approximations of the matrix logarithm.

Foundations of Computational Mathematics, 19(2):259–296, 2019.

33. L. Faybusovich and C. Zhou. Self-concordance and matrix monotonicity with applications to
quantum entanglement problems. Applied Mathematics and Computation, 375:125071, 2020.
34. M. Fazel, H. Hindi, and S. P. Boyd. Log-det heuristic for matrix rank minimization with
applications to Hankel and Euclidean distance matrices. In Proceedings of the 2003 American
Control Conference, 2003., volume 3, pages 2156–2162. IEEE, 2003.

35. M. Fischetti, I. Ljubi´c, and M. Sinnl. Redesigning Benders decomposition for large-scale facility

location. Management Science, 63(7):2146–2162, 2016.

36. A. Frangioni and C. Gentile. Perspective cuts for a class of convex 0–1 mixed integer programs.

Mathematical Programming, 106(2):225–236, 2006.

37. A. Frangioni and C. Gentile. A computational comparison of reformulations of the perspective

relaxation: SOCP vs. cutting planes. Operations Research Letters, 37(3):206–210, 2009.

38. A. Frangioni, C. Gentile, and J. Hungerford. Decompositions of semideﬁnite matrices and
the perspective reformulation of nonseparable quadratic programs. Mathematics of Operations
Research, 45(1):15–33, 2020.

39. S. Gandy, B. Recht, and I. Yamada. Tensor completion and low-n-rank tensor recovery via

convex optimization. Inverse Problems, 27(2):025010, 2011.

40. D. Ge and Y. Ye. On doubly positive semideﬁnite programming relaxations. Optimization

Online, 2010.

41. O. G¨unl¨uk and J. Linderoth. Perspective reformulations of mixed integer nonlinear programs

with indicator variables. Mathematical Programming, 124(1-2):183–205, 2010.

42. S. Han, A. G´omez, and A. Atamt¨urk. 2x2 convexiﬁcations for convex quadratic optimization

with indicator variables. arXiv:2004.07448, 2020.

43. H. Hazimeh, R. Mazumder, and A. Saab. Sparse regression at scale: Branch-and-bound rooted
in ﬁrst-order optimization. Mathematical Programming, articles in advance, pages 1–42, 2021.
44. F. Hiai and D. Petz. The proper formula for relative entropy and its asymptotics in quantum

probability. Communications in Mathematical Physics, 143(1):99–114, 1991.

45. J.-B. Hiriart-Urruty and C. Lemar´echal. Convex analysis and minimization algorithms I:

Fundamentals, volume 305. Springer Science & Business Media Berlin, 2013.

46. R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge University Press, New York, 1985.

34

D. Bertsimas, R. Cory-Wright, J. Pauphilet

47. S. Joshi and S. Boyd. Sensor selection via convex optimization. IEEE Transactions on Signal

Processing, 57(2):451–462, 2008.

48. M. Karimi and L. Tun¸cel. Domain-driven solver (DDS): a MATLAB-based software package for
convex optimization problems in domain-driven form. arXiv preprint arXiv:1908.03075, 2019.
49. T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):

455–500, 2009.

50. J. Lavaei and S. H. Low. Zero duality gap in optimal power ﬂow problem. IEEE Transactions

on Power Systems, 27(1):92–107, 2011.

51. A. S. Lewis. Convex analysis on the Hermitian matrices. SIAM Journal on Optimization, 6(1):

164–177, 1996.

52. E. H. Lieb and M. B. Ruskai. Proof of the strong subadditivity of quantum-mechanical entropy.

with an appendix by B. Simon. Journal of Mathematical Physics, 14:1938–1941, 1973.

53. P. Mar´echal. On the convexity of the multiplicative potential and penalty functions and related

topics. Mathematical Programming, 89(3):505–516, 2001.

54. P. Mar´echal. On a functional operation generating convex functions, part 1: duality. Journal of

Optimization Theory and Applications, 126(1):175–189, 2005.

55. P. Mar´echal. On a functional operation generating convex functions, part 2: algebraic properties.

Journal of Optimization Theory and Applications, 126(2):357–366, 2005.

56. S. Negahban and M. J. Wainwright. Estimation of (near) low-rank matrices with noise and

high-dimensional scaling. The Annals of Statistics, pages 1069–1097, 2011.

57. G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing

submodular set functions—i. Mathematical Programming, 14(1):265–294, 1978.

58. L. T. Nguyen, J. Kim, and B. Shim. Low-rank matrix completion: A contemporary survey.

IEEE Access, 7:94215–94237, 2019.

59. M. L. Overton and R. S. Womersley. On the sum of the largest eigenvalues of a symmetric

matrix. SIAM Journal on Matrix Analysis and Applications, 13(1):41–45, 1992.

60. M. L. Overton and R. S. Womersley. Optimality conditions and duality theory for minimizing
sums of the largest eigenvalues of symmetric matrices. Mathematical Programming, 62(1-3):
321–357, 1993.

61. G. Pataki. On the rank of extreme matrices in semideﬁnite programs and the multiplicity of

optimal eigenvalues. Mathematics of Operations Research, 23(2):339–358, 1998.

62. J. Peng and Y. Wei. Approximating K-means-type clustering via semideﬁnite programming.

SIAM Journal on Optimization, 18(1):186–205, 2007.

63. F. Permenter and P. Parrilo. Partial facial reduction: simpliﬁed, equivalent SDPs via approxi-

mations of the PSD cone. Mathematical Programming, 171(1-2):1–54, 2018.

64. M. Pilanci, M. J. Wainwright, and L. El Ghaoui. Sparse learning via Boolean relaxations.

Mathematical Programming, 151(1):63–87, 2015.

65. R. Plemmons and R. Cline. The generalized inverse of a nonnegative matrix. Proceedings of the

American Mathematical Society, pages 46–50, 1972.

A New Perspective on Low-Rank Optimization

35

66. J. Renegar. A mathematical view of interior-point methods in convex optimization, volume 3.

Society for Industrial and Applied Mathematics, 2001.

67. R. T. Rockafellar. Convex analysis. Number 28. Princeton university press, 1970.
68. B. Romera-Paredes and M. Pontil. A new convex relaxation for tensor completion. arXiv

preprint arXiv:1307.4653, 2013.

69. M. Singh and W. Xie. Approximation algorithms for D-optimal design. Mathematics of

Operations Research, 45:1193–1620, 2020.

70. A. Skajaa and Y. Ye. A homogeneous interior-point algorithm for nonsymmetric convex conic

optimization. Mathematical Programming, 150(2):391–422, 2015.

71. R. A. Stubbs. Branch-and-cut methods for mixed 0-1 convex programming. PhD thesis,

Northwestern University, 1996.

72. R. A. Stubbs and S. Mehrotra. A branch-and-cut method for 0-1 mixed convex programming.

Mathematical Programming, 86(3):515–532, 1999.

73. A. L. Wang and F. Kılın¸c-Karzan. On the tightness of SDP relaxations of QCQPs. Mathematical

Programming, Articles in Advance, pages 1–41, 2021.

74. H. Wolkowicz, R. Saigal, and L. Vandenberghe. Handbook of semideﬁnite programming: theory,

algorithms, and applications, volume 27. Springer Science & Business Media, 2012.

75. W. Xie and X. Deng. Scalable algorithms for the sparse ridge regression. SIAM Journal on

Optimization, 30(4):3359–3386, 2020.

76. C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of

Statistics, 38(2):894–942, 2010.
77. X. Zheng, X. Sun, and D. Li.

Improving the performance of MIQP solvers for quadratic
programs with cardinality and minimum threshold constraints: A semideﬁnite program approach.
INFORMS Journal on Computing, 26(4):690–703, 2014.

A Background on Operator Functions

In this work, we make repeated use of operator functions, i.e., functions deﬁned from the spectral
decomposition of a matrix. Namely, for any function ω : R → R, its corresponding operator function
fω : S n → S n is deﬁned as

where X = U Diag(λx
some common examples and useful properties of operator functions.

1 , . . . , λx

fω(X) = U Diag(ω(λx

n))U (cid:62)
n)U (cid:62) is an eigendecomposition of X. In this appendix, we present

1 ), . . . , ω(λx

A.1 Examples: Matrix exponential and logarithm

For self-consistency of the paper, we now deﬁne the matrix exponential and logarithm functions
and summarize their properties. These results are well known and can be found in modern matrix
analysis textbooks [see, e.g., 13]

36

D. Bertsimas, R. Cory-Wright, J. Pauphilet

Deﬁnition 2 (Matrix exponential) Let X ∈ S n be a symmetric matrix with eigendecomposition
X = U ΛU (cid:62). Letting exp(Λ) = diag(eλ1 , eλ2, . . . , eλn ), we deﬁne exp(X) := U exp(Λ)U (cid:62).

Proposition 6 The matrix exponential, exp : S n → S n

+, satisﬁes the following properties:

– Power series expansion: exp(X) = I + (cid:80)∞
i=1
– Trace monotonicity: X (cid:22) Y =⇒ tr(exp(X)) ≤ tr(exp(Y )).
– Golden-Thompson-inequality: tr(exp(X + Y )) ≤ tr(exp(X)) + tr(exp(Y )).

i! X i.

1

Remark 13 The matrix exponential is not monotone: X (cid:22) Y (cid:54)=⇒ exp(X) (cid:22) exp(Y ) [13, Ch.V].

Deﬁnition 3 (Matrix logarithm) Let X ∈ S n be a symmetric matrix with eigendecomposition
X = U ΛU (cid:62). Letting log(Λ) = diag(log(λ1), log(λ2), . . . , log(λn)), we have log(X) := U log(Λ)U (cid:62).

Proposition 7 The matrix logarithm, log(X) : S n

++ → S n, satisﬁes the following properties:

– Operator monotonicity: X (cid:22) Y =⇒ log(X) (cid:22) log(Y ).
– Functional inversion: log(exp(X)) = X ∀X ∈ S n.
– Jacobi formula I: tr(log(X)) = log det(X).
– Jacobi formula II: exp (cid:0) 1

n tr log(X)(cid:1) = det(X) 1
n .

A.2 Properties of operator functions

Among other properties, one can show that the trace of operator functions is invariant under an
orthogonal rotation, i.e., tr(fω(X)) = tr(fω(U (cid:62)XU )) for any orthogonal rotation U . Also, if ω is
analytical, then fω is also analytical with the same Taylor expansion.

In our analysis (in particular the proof of Proposition 3), we will use this simple bound on

v(cid:62)fω(A)v in the case where ω is convex:

Lemma 3 Consider a convex function ω : R → R and a symmetric matrix A ∈ S n. Consider a
unit vector v. Then,

v(cid:62)fω(A)v ≥ ω (cid:0)v(cid:62)Av(cid:1) .

Proof Consider a spectral decomposition of A, A = (cid:80)n
and

i=1 λiuiu(cid:62)

i . Then, fω(A) = (cid:80)n

i=1 ω(λi)uiu(cid:62)
i

v(cid:62)fω(A)v =

n
(cid:88)

i=1

ω(λi)v(cid:62)uiu(cid:62)

i v ≥ ω

λiv(cid:62)uiu(cid:62)
i v

(cid:33)

= ω (cid:0)v(cid:62)Av(cid:1) ,

(cid:32) n
(cid:88)

i=1

where the inequality comes from the convexity of ω since v(cid:62)uiu(cid:62)
v(cid:62) (cid:0)(cid:80)n

(cid:1) v = (cid:107)v(cid:107)2 = 1.

i=1 uiu(cid:62)
i

i v = (u(cid:62)

i v)2 ≥ 0 and (cid:80)n

i=1 v(cid:62)uiu(cid:62)

i v =

A New Perspective on Low-Rank Optimization

37

B Omitted Proofs

In this section, we supply all omitted proofs, in the order the results were stated.

B.1 Proof of Proposition 3

Proof Fix X ∈ S n. For Y (cid:31) 0, the perspective of fω is well-deﬁned according to Deﬁnition 1. Now,
consider an arbitrary Y (cid:23) 0 and deﬁne P as the orthogonal projection onto the kernel of Y , which
is orthogonal to Span(Y ). Then, Yε := Y + εP for ε > 0 is invertible. The closure of the matrix
for ε → 0.
perspective of fω is deﬁned by continuity as the limit of Mε := Y

ε XY − 1
Y − 1
Since the ranges of Y and P are orthogonal (Y P = P Y = 0), we have Y − 1

2 + ε− 1

ε = Y − 1

ε fω

2 P ,

1
2
ε

Y

(cid:17)

(cid:16)

1
2

ε

2

2

2

and

ε XY − 1
Y − 1

ε = Y − 1

2

2

2 XY − 1

2 + ε− 1

2 P XY − 1

2 + ε− 1

2 Y − 1

2 XP + ε−1P XP .

Note that lim
ε→0

Y

1
2

ε = Y

1
2 but lim
ε→0

Y − 1

2

ε

(cid:54)= Y − 1

2 . We now distinguish two cases.

Case 1: If span(X) ⊆ span(Y ), XP = P X = 0 so

ε XY − 1
Y − 1

ε = Y − 1

2

2

2 XY − 1
2 ,
(cid:16)

Mε = Y

ε fω

1
2

Y − 1

2 XY − 1

2

(cid:17)

1
2

Y

ε →ε→0 Y

(cid:16)

1

2 fω

Y − 1

2 XY − 1

2

(cid:17)

Y

1
2 .

Case 2:

If span(X) (cid:54)⊆ span(Y ), consider an orthonormal basis of Rn such that u1, . . . , uk
k) and uk+1, . . . , un is a basis of
j Xuj (cid:54)= 0. Without
n Xun (cid:54)= 0. We show that the matrix Mε goes to inﬁnity as

is an eigenbasis of Span(Y ) (with respective eigenvalues λy
Span(Y )⊥ = Ker(Y ). By assumption, k < n and there exists j > k such that u(cid:62)
loss of generality, we shall assume u(cid:62)
ε → 0 by showing that u(cid:62)

1, . . . λy

n Mεun diverges.

Since Y ± 1

ε un = ε± 1

2

2 un, we have

u(cid:62)

n Mεun = ε u(cid:62)

n fω

(cid:16)

ε XY − 1
Y − 1

ε

2

2

(cid:17)

un ≥ ε ω

(cid:16)

n Y − 1
u(cid:62)

ε XY − 1

ε un

2

2

(cid:17)

= ε ω (cid:0)ε−1u(cid:62)

n Xun

(cid:1) ,

where the inequality follows from the convexity of ω and Lemma 3. By Assumption 1,

εω (cid:0)ε−1u(cid:62)

n Xun

(cid:1) = ω∞(u(cid:62)

n Xun) = +∞,

lim
ε→0

because u(cid:62)

n Xun (cid:54)= 0 and ω is coercive.

(cid:117)(cid:116)

We now provide a simple extension of Proposition 3 that will prove useful later in our exposition.

38

D. Bertsimas, R. Cory-Wright, J. Pauphilet

Corollary 3 Consider a function ω : R → R satisfying Assumption 1 and denote its associated
operator function fω. Consider a closed set X ⊆ S n and deﬁne

f (X) =

(cid:40)

fω(X)

if X ∈ X ,

+∞

otherwise.

Then, the closure of the matrix perspective of f is, for any X ∈ S n, Y ∈ S n
+,

gf (X, Y ) =

(cid:40)

Y 1

2 fω(Y − 1

2 XY − 1

2 )Y 1

2

if Span(X) ⊆ Span(Y ), Y (cid:23) 0, Y − 1

2 XY − 1

2 ∈ X ,

∞

otherwise,

where Y − 1

2 denotes the pseudo-inverse of the square root of Y .

Proof Fix X ∈ S n and Y ∈ S n
+. From Proposition 3, we know that gf (X, Y ) = +∞ if Span(X) (cid:54)⊆
Span(Y ). Let us assume that Span(X) ⊆ Span(Y ). Following the same construction as in the
proof of Proposition 3, we obtain a sequence Yε that converges to Y as ε → 0 and such that
ε XY − 1
Y − 1
(cid:117)(cid:116)

2 , which concludes the proof.

ε = Y − 1

2 XY − 1

2

2

B.2 Perspective functions with non-commuting matrices

In contrast with Proposition 4, in the general case where X and Y do not commute, we cannot
simultaneously diagonalize them and connect gfω with gω. However, we can still project Y onto the
space of matrices that commute with X and obtain the following result when gfω is matrix convex:

Lemma 4 Let X ∈ S n and Y ∈ S n
+ be matrices, and deﬁne X := {M : M X = XM } as the set
of matrices which commute with X. For any matrix M , denote M|X the orthogonal projection of
M onto X . Then, since M (cid:55)→ M|X is a projection operator, we have that

Y|X ∈ S n

+, and tr (cid:0)Y|X

(cid:1) = tr (Y ) .

Moreover, if Y (cid:55)→ gfω (X, Y ) is matrix convex, then we have

tr (cid:2)gfω (X, Y|X )(cid:3) ≤ tr [gfω (X, Y )] .
Proof First, let us observe that X is a closed subset of S n, contains the identity, and is closed under
multiplication and transposition, also know as a Von Neumann subalgebra [see 20, Section 4 for
a detailed treatment of projections onto subalgebras]. The orthogonal projection of a semideﬁnite
matrix onto X is also semideﬁnite and has the same trace [20, Theorem. 4.13], so

tr (cid:0)Y|X

(cid:1) = tr (Y ) .

Furthermore, since Y (cid:55)→ gfω (X, Y ) is matrix convex, Carlen [20, Theorem 4.16] yields

gfω (X, Y|X ) (cid:22) gfω (X, Y )|X .
Taking the trace on both sides and using that tr (cid:0)gfω (X, Y )|X
proof.

(cid:1) = tr (gfω (X, Y )) concludes the
(cid:117)(cid:116)

A New Perspective on Low-Rank Optimization

39

In other words, taking the projection of Y onto the commutant of X is a trace preserving operation
that can only reduce the value of tr (gfω (X, ·)). In this paper, we invoke the projection onto X (a
non-convex set) for theoretical purposes, not computational ones. So we are not interested in how to
compute Y|X in practice. Note that, according to Proposition 2(a), Lemma 4 holds if fω is matrix
convex.

B.3 Counterexample to joint convexity of trace of matrix perspective of cube

In this section, we demonstrate by counterexample that if ω is a convex and continuous function
then, even though the trace of its matrix extension, tr(fω), is convex [c.f. 20, Theorem 2.10], the
trace of its matrix perspective need not be convex.

Speciﬁcally, let us consider ω(x) = x3. In this case, ω is convex on R+, fω is not matrix convex,

but tr(fω) is matrix convex. We have that

tr(gfω (X, Y )) = tr (cid:0)XY †XY †X(cid:1)

for X ∈ Span(Y ), X, Y ∈ S n

+. Let us now consider

Y1 =

(cid:32)

0.160378 0.343004
0.343004 0.764592

(cid:33)

, Y2 =

(cid:32)

0.0859208 0.181976
0.181976 0.52666

X1 =

(cid:32)

0.242865 0.543321
0.543321 1.26604

(cid:33)

, X2 =

(cid:32)

0.0595215 0.241702
1.0596
0.241702

(cid:33)

(cid:33)

,

.

Then, some elementary algebra reveals that

tr (cid:2)gfω

while

(cid:0) 1
2 X1 + 1

2 X2, 1

2 Y1 + 1

2 Y2

(cid:1)(cid:3) = 6.248327,

1

2 tr [gfω (X1, Y1)] + 1

2 tr [gfω (X2, Y2)] = 6.23977,

which veriﬁes that tr(gfω (X, Y )) is not midpoint convex in (X, Y ), despite tr(fω) being convex.

B.4 Proof of Proposition 5

Proof We use the proof technique laid out in [42, Section 3.1], namely writing T as the disjunction
of two convex sets driven by whether z is active and applying Fourier-Motzkin elimination. That is,
we have T = T 1 ∪ T 2 where:

T 1 = {(0, y1, 0, t1) : t1 ≥ |y1 − d|q} ,
T 2 = {(x2, y2, 1, t2) : t2 ≥ |x2 − y2 − d|q, |x2| ≤ M } .

40

D. Bertsimas, R. Cory-Wright, J. Pauphilet

Moreover, a point (x, y, z, t) is in the convex hull T c if and only if it can be written as a convex
combination of points in T 1, T 2. Letting λ1, λ2 denote the weight of points in this system, we then
have that (x, y, z, t) ∈ T c if and only if the following system admits a solution:

λ1 + λ2 = 1,

x = λ2x2,

y = λ1y1 + λ2y2,

t = λ1t1 + λ2t2,

z = λ2,
t1 ≥ |y1 − d|q,
t2 ≥ |x2 + y2 − d|q,
λ1, λ2 ≥ 0,

|x2| ≤ M.

(37)

For ease of computation, we now eliminate variables. First, one can substitute t1, t2 for their lower
bounds in the deﬁnition of t and replace λ2 with z to obtain

λ1 + z = 1,

x = zx2,

y = λ1y1 + zy2,
t ≥ λ1|y1 − d|q + z|x2 + y2 − d|q,
λ1, z ≥ 0,

|x2| ≤ M.

Next, we substitute x/z for x2 and (y − zy2)/λ1 for y1 to obtain

λ1 + z = 1, λ1, z ≥ 0, |x| ≤ M z

t ≥

1
λq−1
1

|y − y2z − d(1 − z)|q +

1
zq−1 |x + y2z − dz|q.

(38)

(39)

Finally, we let zy2 be the free variable β and set λ1 = 1 − z to obtain the required convex set. (cid:117)(cid:116)

C Generalizing the Matrix Perspective Reformulation Technique to Functions

We now demonstrate the MPRT can be extended to incorporate a diﬀerent separability of eigenvalues
assumption, at the price of (a possibly signiﬁcant amount of) additional notations. For any symmetric
matrix X, let us denote λ↓
i (X) the ith largest eigenvalue of X. Before proceeding any further, we
recall the following result, due to [4, Example 18.c], which provides a semideﬁnite representation of
the sum of the k largest eigenvalues:

A New Perspective on Low-Rank Optimization

41

Lemma 5 (Representability of sums of largest eigenvalues) Let Sk(X) := (cid:80)k
i (X)
denote the sum of the k largest eigenvalues of a symmetric matrix X ∈ S n. Then, the epigraph of
Sk, Sk(X) ≤ tk, admits the following semideﬁnite representation:

i=1 λ↓

tk ≥ ksk + tr(Zk), Zk + skI (cid:23) X, Zk (cid:23) 0.

Based on this result, we can relax the assumption that the penalty term Ω(X) corresponds to

the trace of an operator function. Instead, we can assume:

Assumption 4 Ω(X) = (cid:80)
satisfying Assumption 1 and whose associated operator function, fω, is matrix convex.

i (fω(X)), where p1 ≥ . . . ≥ pn ≥ 0 and where ω is a function

i∈[n] piλ↓

This assumption is particularly suitable for Markov Chain problems [see, e.g., 15, Chapter 4.6],
where we are interested in controlling the behaviour of the largest eigenvalue (which always equals 1)
plus the second largest eigenvalue of a matrix. However, it might appear to be challenging to model,
since, e.g., λ↓
2(X) is a non-convex function. By applying a telescoping sum argument reminiscent of
the one in [4, Prop. 4.2.1], namely

Ω(X) =

n
(cid:88)

i=1

piλ↓

i (f (X)) =

n
(cid:88)

(pi − pi+1)Si(f (X))

i=1

with the convention pn+1 = 0, Lemma 5 allows us to rewrite low-rank problems where Ω(X) satisﬁes
Assumption 4 in the form:

min
Y ∈Y k
n

min
X∈S n
+,
+,si,ti∈R+ ∀i∈[n]

Zi∈S n

(cid:104)C, X(cid:105) + µ · tr(Y ) +

n
(cid:88)

(pi − pi+1)ti

i=1

(40)

s.t.

(cid:104)Ai, X(cid:105) = bi ∀i ∈ [m], X = Y X, X ∈ K,
ti ≥ isi + tr(Zi), Zi + siI (cid:23) f (X), Zi (cid:23) 0 ∀i ∈ [n],

where ti models the sum of the i largest eigenvalues of f (X). Applying the MPRT then yields the
following extension to Theorem 1:

Proposition 8 Suppose Problem (40) attains a ﬁnite optimal value. Then, the following problem
attains the same value:

min
Y ∈Y k
n

min
X∈S n
+,
+,si,ti∈R+ ∀i∈[n]

Zi∈S n

(cid:104)C, X(cid:105) + µ · tr(Y ) +

n
(cid:88)

(pi − pi+1)ti

i=1

(41)

s.t.

(cid:104)Ai, X(cid:105) = bi ∀i ∈ [m], Y − 1
ti ≥ isi + i − tr(Y ) + tr(Zi) ∀i ∈ [n],
Zi + siI (cid:23) gf (X, Y )+ω(0)(I − Y ), Zi (cid:23) 0 ∀i ∈ [n].

2 XY − 1

2 ∈ K,

42

D. Bertsimas, R. Cory-Wright, J. Pauphilet

The proof of this reformulation is almost identical to the proof of Theorem 1, after observing that
(20) holds not only for the traces but for the matrices directly, i.e., if X and Y ∈ Y k
n commute, we
have

f (X) = gf (X, Y ) + ω(0)(I − Y ).

Problem (41) involves n times as many variables as Problem (18) and therefore supplies substantially
less tractable relaxations. Nonetheless, it could be useful in speciﬁc instances. In the aforementioned
Markov Chain mixing problem, pi − pi+1 = 0 ∀i ≥ k with k = 2, so we can omit the variables which
model the eigenvalues larger than 2 .

D Extension to the rectangular case

In this section, we extend the MPRT to the case where X is a generic n × m matrix and f (X) is the
convex quadratic penalty f (X) = X (cid:62)X. In this case, tr(f (X)) = (cid:107)X(cid:107)2
F is the squared Frobenius
norm of X.

First, observe that f : Rn×m → S m

+ . Alternatively, one could have considered g(X) = XX (cid:62) ∈ S n
+
and obtain the same penalty, i.e., tr(f (X)) = tr(g(X)). In other words, one can arbitrarily choose
whether f preserves the row or the column space of X. By the Schur complement lemma, the
epigraph is semideﬁnite representable via

(cid:40)

epi(f ) :=

(X, θ) ∈ Rn×m × S m

+ :

(cid:32)

θ X (cid:62)
X I

(cid:33)

(cid:41)

(cid:23) 0

,

so f is matrix convex.

In the symmetric case, we considered the matrix perspective of f at (X, Y ), where Y (cid:23) 0 is
a matrix controlling the range of X. When X is no longer symmetric, it is natural to consider a
matrix perspective function which involves two projection matrices, one of which models the row
space and one which models the column space, as proposed in our prior work [11]. More precisely,
for Y , Z (cid:31) 0 we deﬁne a perspective of f as

gf (X, Y , Z) = Z

1

2 f (Y − 1

2 XZ− 1

2 )Z

1
2 .

(42)

For f (X) = X (cid:62)X, this function actually does not depend on Z. Hence, we consider

˜gf (X, Y ) = gf (X, Y , Z) = X (cid:62)Y −1X.

Extending this function to positive semideﬁnite Y using the same proof technique as in Proposition
3, we then obtain

(cid:40)

˜gf (X, Y ) =

X (cid:62)Y †X if Y (cid:23) 0, Span(X) ⊆ Span(Y ),

∞

otherwise.

A New Perspective on Low-Rank Optimization

43

Proof Fix X ∈ S n and Y (cid:23) 0. As in the proof of Proposition 3 denote P the orthogonal projection
onto the kernel of Y , and deﬁne Yε := Y + εP for ε > 0. Hence,

X (cid:62)Y −1

ε X = X (cid:62)Y †X + ε−1X (cid:62)P X.

The right-hand side admits a ﬁnite limit if and only if

X (cid:62)P X = 0 ⇐⇒ Span(X) ⊆ Ker(P ) = Span(Y ). (cid:117)(cid:116)

Furthermore, using the Schur complement lemma as in [11], one can show that ˜gf is SDP-

representable:

(cid:40)

epi(˜gf ) =

(X, Y , θ) ∈ Rn×m × S n

+ × S m :

(cid:32)

θ X (cid:62)
X Y

(cid:33)

(cid:41)

(cid:23) 0

,

and hence matrix convex.

Finally, we can easily check that Theorem 1 still holds in the symmetric case because (20) –which

simpliﬁes to tr(f (X)) = tr(˜gf (X)) in this case– holds for any Y ∈ Y k

n such that X = Y X.

Remark 14 We believe the approach outlined above could be generalized to a broader class of
function that generalizes operator functions to the non-symmetric case. Namely, we could consider
functions of the form

1 , . . . , σx

fω(X) = V Diag (ω(σx

m)) V (cid:62)
m) V (cid:62) is a singular value decomposition of X and ω is a convex
where X = U Diag (σx
function satisfying Assumption 1. Again, fω could arbitrarily be deﬁned as preserving U or V . For
these functions, the perspective gfω (X, Y , X) is well deﬁned for Y , Z (cid:31) 0. Unlike in the quadratic
case, however, its value will depend on both Y and Z. Developing the theoretical tools necessary to
extend the MPRT to rectangular matrices, is therefore a question for future research.

1 ), . . . , ω(σx

