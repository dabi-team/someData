1

Reliability-Performance Trade-offs
in Neuromorphic Computing

Twisha Titirsha and Anup Das
Electrical and Computer Engineering Drexel University, Philadelphia, PA, USA
Email: {tt624,anup.das}@drexel.edu

0
2
0
2

p
e
S
6
2

]
E
N
.
s
c
[

1
v
2
7
6
2
1
.
9
0
0
2
:
v
i
X
r
a

Abstract—Neuromorphic architectures built with Non-Volatile
Memory (NVM) can signiﬁcantly improve the energy efﬁciency of
machine learning tasks designed with Spiking Neural Networks
(SNNs). A major source of voltage drop in a crossbar of these
architectures are the parasitic components on the crossbar’s
bitlines and wordlines, which are deliberately made longer to
achieve lower cost-per-bit. We observe that the parasitic voltage
drops create a signiﬁcant asymmetry in programming speed and
reliability of NVM cells in a crossbar. Speciﬁcally, NVM cells that
are on shorter current paths are faster to program but have lower
endurance than those on longer current paths, and vice versa.
This asymmetry in neuromorphic architectures create reliability-
performance trade-offs, which can be exploited efﬁciently using
SNN mapping techniques. In this work, we demonstrate such
trade-offs using a previously-proposed SNN mapping technique
with 10 workloads from contemporary machine learning tasks
for a state-of-the art neuromoorphic hardware.

Index Terms—Neuromorphic Computing, Non-Volatile Mem-

ory (NVM), Phase-Change Memory (PCM), Endurance

I. INTRODUCTION

Spiking Neural Networks (SNNs) are emerging machine
learning models with spike-based computation and bio-inspired
learning algorithms. Event-driven neuromorphic hardware such
as TrueNorth [1], Loihi [2], and DYNAP-SE [3] implements
biological neurons and synapses to execute SNN-based machine
learning tasks in an energy-efﬁcient manner. This makes neu-
romorphic hardware suitable for energy-constrained platforms
such as the embedded systems [4] and edge devices of the
Internet-of-Things (IoTs) [5].

A neuromorphic hardware is implemented as a tile-based
architecture, the tiles are interconnected using a shared intercon-
nect such as the Network-on-Chip (NoC) [6] and Segmented
Bus [7]. Each tile consists of a crossbar, which can implement
a ﬁxed number of neurons and synapses. A crossbar in a
neuromorphic hardware is an n × n organization, with n bitlines
(columns) and n worklines (rows). A silicon neuron is mapped
along each wordline of a crossbar, while a synaptic cell is
placed at the cross-section of each bitline and wordline using
an access device such as a transistor or a diode [8].

Recently, Non-Volatile Memory (NVM) such as Phase-
Change Memory (PCM), Oxide-based Resistive RAM (OxR-
RAM), and and Spin-Transfer Torque Magnetic or Spin-Orbit-
Torque RAM (STT- and SoT-MRAM) are used as synaptic cells
to increase integration density and reduce energy consumption
of crossbars in neuromorphic hardware [9]–[11].

A major source of voltage drops in a crossbar are the parasitic
resistance and capacitance on its bitlines and wordlines, which

are deliberately made longer to achieve lower cost-per-bit. In
fact, for a PCM-based crossbar, each neuron is approximately
18x the size of a PCM cell [12]. To amortize this large size,
systems designers implement larger crossbars, e.g., 128 × 128
for DYNAP-SE and 256 × 256 for TrueNorth. For such large
crossbar sizes, the current on the longest path in a crossbar
becomes signiﬁcantly lower than the current on its shortest
path for the same spike voltage generated from a neuron and
the same conductance programmed on the enabled synaptic
cell in these paths.1

Current asymmetry leads to a difference in performance
and reliability of NVM cells. Higher current through an NVM
cell can lead to faster programming of the cell. This means
that NVM cells on shorter current paths are faster to access
and program. However, NVMs also have limited endurance,
ranging from 105 (for Flash) to 1010 (for OxRRAM), with PCM
somewhere in between (≈ 107). An NVM cell’s endurance is
strongly dependent on the programming current. We build
the case for PCM, where the conductance change is induced
by Joule heating of the chalcogenide material in the cell.
The endurance of the material depends on the self-heating
temperature, which is dependent on the programming current.
Therefore, the NVM cells on shorter current paths have higher
self-heating temperature, and therefore lower endurances.

In recent years, many approaches are proposed to map SNNs
to neuromorphic hardware. This includes the performance-
oriented SNN mapping technique of [13], [14], the dataﬂow-
based mapping technique of [15], [16], the energy-aware
mapping technique of [17]–[19], the circuit aging-aware map-
ping technique of [20]–[23], and the run-time SNN mapping
technique of [24]. Unfortunately, none of these approaches
exploit the reliability and performance trade-offs of NVM
cells in neuromorphic computing. In this paper, we take
one such mapping approach – SpiNeMap, and show the
signiﬁcant variations in endurance and speed during its mapping
explorations.

The remainder of this paper is organized as follows. We
provide a background of PCM and neuromorphic architectures
in Section II. Next, we formulate the endurance-access speed
trade-offs for a single PCM cell and integrate such trade-
offs at the crossbar-level in Section III. Next, we discuss the
mapping exploration of SpiNeMap in Section IV. We present
our evaluation in Section V and conclusion in Section VI.

1The length of a current path in a crossbar is measured in terms of the

number of parasitic components that are encountered on the path.

 
 
 
 
 
 
II. BACKGROUND

In this section, we discuss the background on Phase-Change
Memory (PCM) to aid the understanding of the trade-offs
in neuromorphic computing. We also provide a discussion
on machine learning approaches using SNNs, and how such
approaches can be mapped to hardware consisting of PCM
cells organized into crossbars.

A PCM cell is built with chalcogenide alloy, e.g., Ge2Sb2Te5
(GST) [25], and is connected to a bitline and a wordline using
an access device. The GST alloy can either be in an amorphous
(high resistance) state, or in one of the partially crystallized
(low resistance) states. PCM is recently explored as scalable
DRAM alternative for conventional computing [26]–[31]. This
work explores PCM for neuromorphic computing. For such
computing architectures, the weight of a synaptic connection is
programmed as conductance of a PCM cell by driving current
and inducing Joule heating in the cell.

In many machine learning approaches such as online learning
using Spike-Timing Dependent Plasticity (STDP) [32], one-
shot learning [33], life-long learning [34], and reinforcement
learning [35], it is necessary to update synaptic weights based
on the input excitation. To facilitate such synaptic updates, a
PCM cell’s state must be switched by driving current through
it using the spikes generated from neurons. However, frequent
switching of a PCM cell’s state may lead to endurance issues,
where the cell fails to be programmed correctly, leading to a
degradation of machine learning performance. Furthermore, a
key requirement in such online learning use-cases is their real-
time performance, i.e., the weight updates must be completed
within a small time interval.

To understand the workload-dependent performance and
endurance trade-offs associated with PCM cells in a neuro-
morphic architecture, Figure 1a shows a simple SNN with
two input and one output neurons. Figure 1b illustrates the
mapping of this SNN to a crossbar. As seen from this ﬁgure, the
synaptic weights w1 and w2 are programmed as conductances.
The spike voltages are multiplied with the conductances to
generate current, which gets integrated along the columns.
The current strength guides the update of conductance of
the PCM cell enabled along the current path. Clearly, the
weight update frequency depends on the spikes generated
from the hardware neurons mapped along the rows of a
crossbar. The latter depends on how neurons and synapses
of a machine learning model, e.g., Figure 1a are mapped
to the corresponding resources on a crossbar. To elaborate
on this, Figure 1c illustrates the utilization of a different set
of PCM cells to realize the SNN of Figure 1a. If the PCM
cells in a crossbar have different performance and endurance
characteristics (which we demonstrate in this work), then the
mapping of neurons and synapses of a machine learning model
plays a critical role in system-level performance and reliability.

III. ENDURANCE-PERFORMANCE TRADE-OFFS IN
NEUROMORPHIC HARDWARE

We formulate the endurance-performance trade-offs for a
single PCM cell. To establish the relationship, we consider the
GST material of a PCM cell to be in a crystalline state. The

2

Fig. 1.
a crossbar, (c) A different mapping of the network to the hardware.

(a) A simple spiking neural network, (b) Mapping of the network to

amorphization process, i.e., the crystalline-to-amorphous state
transition involves driving a very high current through the cell
for a short duration. This high current raises the temperature of
the GST material through Joule heating in the heater attached to
the GST, which transitions the material to its amorphous state.
The crystalline fraction (Vc) is computed using the Johnson-
Mehl-Avrami (JMA) equation [36] as

Vc = exp

(cid:20)

−α ×

(TSH − Tamb)
Tm

(cid:21)

× t

,

(1)

where t is the time, Tm is the melting temperature of the GST
material, and α is a ﬁtting constant. The exponential decay of Vc
in Equation 1 implies that, higher the self-heating temperature
(TSH), faster is the reduction of the crystalline volume, i.e.,
faster is the amorphization process.

The self-heating temperature is related to the square of

programming current (Iprog) as

TSH = k · I 2

prog

(2)

where k is a constant.

From Equations 1 and 2 we can conclude that higher the
programming current, higher is the self-heating temperature,
and hence, faster is the programming of the cell.

However, with increase in self-heating temperature, the
endurance of a PCM cell reduces. Using the phenomenological
endurance model [37], endurance of a PCM cell can be
expressed as

Endurance ≈ exp

,

(3)

(cid:18) γ

(cid:19)

TSH

where γ is a ﬁtting parameter.

From Equations 2 and 3, we conclude that higher the
programming current, higher is the self-heating temperature
and therefore, lower is the endurance.

Figure 2 shows the current through the PCM cells in a
128x128 PCM crossbar. This current variation is due to the
difference in the length of current paths from pre-synaptic
neurons to post-synaptic neurons in the crossbar, where the
length of a current path is measured in terms of the number
of parasitic elements on the path. These current values are
obtained for a 65nm technology node and at 300K temperature
corner. As can be clearly seen from the ﬁgure, current through
PCM cells on the top-right corner of the crossbar is lower than
through PCM cells located at the bottom-left corner. Therefore,
cells at the top-right corner are slower to program and have
higher endurance, while those at the bottom-left corner are

Post-synaptic neurons123(a)(b)Pre-synaptic neuronsPost-synaptic neurons(c)Pre-synaptic neurons3

where t is the iteration number, ϕ1, ϕ2 are constants and Pbest
(and Gbest) is the particles own (and neighbors) experience.
In Figure 3, we illustrate the iterative approach to ﬁnd an
optimal solution using PSO. The PSO algorithm starts with an
initial neighborhood of swarms. In this example, we illustrate
3 swarms, each with 3 particles (see Figure 3a). Each particle
jumps to a new location with a velocity determined as a function
of local best (within swarms), and global best. This continues
until the sub-swarms converge (see Figure 3b). In the third
step, the swarm regroups and the position and velocity update
steps are repeated (see Figure 3c). We continue these iterations
until a predeﬁned convergence criteria is reached.

SpiNeMap uses PSO to minimize the number of spikes
communicated on the global interconnect, which leads to a
reduction in the energy consumption.

Fig. 3.

Illustrating the iterative steps of PSO.

Figure 4 illustrates the performance-endurance trade-offs ob-
tained during the mapping exploration of SpiNeMap. The ﬁgure
plots the performance and endurance obtained for different
design solutions generated during the design-space exploration
using the PSO. The ﬁgure also shows two solutions – one
with highest endurance and one with the highest performance.
We note that the highest performance mapping is generated
by DFSynthesizer [13], which maps neurons and synapses to
hadware, minimizing the execution time of applications.

Fig. 4. Performance-endurance trade-offs using SpiNeMap.

V. EVALUATION

A. Evaluation Framework

We evaluated 10 machine learning applications that are
representative of three most commonly used neural network
classes — convolutional neural network (CNN), multi-layer
perceptron (MLP), and recurrent neural network (RNN). These
applications are 1) LeNet [39] based handwritten digit recogni-
tion with 28 × 28 images of handwritten digits from the MNIST
dataset [40]; 2) AlexNet [41] for Imagenet classiﬁcation [42];
3) VGG16 [43], also for Imagenet classiﬁcation [42]; 4) ECG-
based heart-beat classiﬁcation (HeartClass) [44], [45] using
electrocardiogram (ECG) data from the Physionet database [46];

Fig. 2. Current map in a 128x128 crossbar.

faster to program and have lower endurance. Table I summarizes
these ﬁndings.

TABLE I
SUMMARY OF PERFORMANCE-ENDURANCE TRADE-OFFS.

Location

Performance Endurance

Top-right corner

Low

Bottom-left corner

High

High

Low

IV. MAPPING EXPLORATIONS

In this section, we present the mapping exploration of
SpiNeMap [18] and show the performance-endurance trade-offs
that are obtained during its design-space exploration.

SpiNeMap uses an instance of the Particle Swarm Optimiza-
tion (PSO) [38], a meta-heuristic approach to map neurons and
synapses to the hardware. To this end, SpiNeMap ﬁrst partitions
a spiking neural network based application into clusters, where
each cluster can ﬁt onto the resources of a crossbar. The
clusters are then mapped to the crossbars using the PSO. In
general, PSO ﬁnds the optimum solution to a ﬁtness function
F . Each solution is represented as a particle in the swarm.
Each particle has a velocity with which it moves in the search
space to ﬁnd the optimum solution. During the movement, a
particle updates its position and velocity according to its own
experience (closeness to the optimum) and also experience of
its neighbors. We introduce the following notations for PSO.

D = dimensions of the search space
np = number of particles in the swarm

(4)

Θ = {θl ∈ RD}
V = {vl ∈ RD}

np−1
l=0 = positions of particles in the swarm
np−1
l=0 = velocity of particles in the swarm

Position and velocity updates are performed according to

the following equation.

Θ(t + 1) = Θ(t) + V(t + 1)

V(t + 1) = V(t) + ϕ1 ·

(cid:16)

Pbest − Θ(t)

(cid:17)

+ ϕ2 ·

(cid:16)

Gbest − Θ(t)

(5)

(cid:17)

0326496128Post-synapticneurons1289664320Pre-synapticneurons200220240260280300320PCMcurrent(µA)(a) Initial neighborhoods (3 swarms, 9 particles)(a) sub-swarms converge(a) regrouping and repeat0.097937050.254663410.097937053.926751830.630823383.788240720.028059350.198631370.028059355.034451630.641988672.598036540.162722640.680545420.162722641.469409640.948765463.54262690.91251090.060249350.912510916.59769050.717786811.268812730.822896240.235079190.822896244.253885680.497731971.012011890.43363120.907447910.43363121.101991637.7809E-055.788442110.286115950.247175340.286115954.04571110.409514211.291378130.699914710.33056380.699914713.025134650.162776021.530682740.647633880.245835350.647633884.067763290.5874456831.61169550.870045730.650775910.870045731.536627240.386618178.676284090.748229030.773494850.748229031.292833420.979794773.123931130.715662050.953661080.715662051.048590550.273989531.442337130.585175690.953882930.585175691.048346680.772989971.318651190.696240830.959860710.696240831.041817830.918547441.070099660.250842740.535855450.250842741.866174930.438202142.016227350.262913530.313819980.262913533.186540210.565632881.936013150.897043060.941948160.897043061.061629550.640381922.440450330.039138950.783202160.039138951.276809560.785844992.877757790.88548780.001668940.8854878599.1815850.3464980950.00858760.704951450.566134370.704951451.76636510.225429832.052756940.947174890.219244040.947174894.561127320.003785951.995014240.687556580.662021230.687556581.510525580.178830111.176464180.105100540.896652890.105100541.115258770.07970552.942433240.263749390.170864740.263749395.852582480.587784384.663857270.937461790.393778690.937461792.53949750.252646563.93175580.553415230.398191150.553415232.511356660.37844181.006089040.688727440.101377940.688727449.864078730.286829161.913296730.959712730.867861120.959712731.152258090.971903741.392297930.062002980.228177320.062002984.382556560.3414274115.88126620.417314550.296263890.417314553.37536920.483707782.531863440.142488440.596972880.142488441.675117980.34238591.08068450.352641450.053419640.3526414518.71970760.528263541.130891710.048834720.139517380.048834727.167565770.556707261.694672680.078237670.2558450.078237673.908616490.701831153.862031120.931948910.713856290.931948911.400842180.1726758928.36935780.797142310.96643930.797142311.034726130.225839241.06283650.94318110.350758110.94318112.850967570.562553563.739053430.423998840.050141630.4239988419.94350730.904415611.488236630.811719710.355920250.811719712.809618150.89578241.562371280.637799370.571187330.637799371.75073910.566514763.591131070.426463810.629406910.426463811.588797310.78287113.395325220.206083410.926781960.206083411.079002440.978108782.280831150.081334170.412655450.081334172.423329210.772999483.632734480.400467820.338784290.400467822.951730730.4592274416.58825660.120362640.923193080.120362641.083197020.647535122.00615730.416774960.445007880.416774962.247151230.680454372.679156610.038013050.854559910.038013051.170192970.199720361.104772550.080860430.023776080.0808604342.059070.873576553.114499930.867456560.475803250.867456562.101709040.839014841.157373630.102546380.622336610.102546381.606847470.11526534.622140510.34671390.861970960.34671391.160131890.072609671.700356010.433036180.650121060.433036181.538175070.437567791.288652710.530208490.878624920.530208491.138142090.200923691.344056510.442621450.817377230.442621451.223425330.649127291.162535880.75497530.03376690.754975329.61479970.193430572.176693990.52496420.253953980.52496423.937721280.90715091.470520080.042984180.686242190.042984181.457211470.689502411.827508470.051562150.108617520.051562159.206618270.526031262.387577450.413410780.705694690.413410781.41704340.476354072.524925550.873068910.772811040.873068911.293977370.333528120.52520430.709816920.549582490.709816921.819563060.375704163.689638270.315013130.598521990.315013131.670782390.450228111.446482030.85856910.320049030.85856913.124521250.79358411.30558560.655658550.422682380.655658552.365842670.541381247.776737930.242642680.870690930.242642681.148513170.053157953.146786150.448408890.6932940.448408891.442389530.002612161.177135940.895942310.599038780.895942311.669341010.97937881.259057390.229419640.266863860.229419643.747228980.44621971.888583050.022734580.510005640.022734581.960762650.224133431.245856030.424519710.785940280.424519711.272361310.341872216.228565860.299273470.487490580.299273472.051321670.628019019.602288380.294256420.998627270.294256421.001374620.091762051.137736750.894350950.029605150.8943509533.77790990.418968463.568180780.26494030.117441530.26494038.514875390.893631424.474260610.793955030.287447140.793955033.478900560.2707548913.65939050.727330680.730774870.727330681.368410490.819778361.468836680.05924860.769084210.05924861.300247730.924174532.801984310.137418160.310389560.137418163.221757850.10551721.007409430.146469480.477282250.146469482.095196310.689284251.214728840.065838190.464675030.065838192.152041630.682927661.023529250.834866830.998761140.834866831.001240390.42142084122.7835070.850008670.163346430.850008676.121958230.810818652.208466180.973510470.741528020.973510471.348566710.708781051.595833970.206451890.714919370.206451891.398759130.825040112.607017780.479857860.092772550.4798578610.77905050.788443471.029091770.622609180.106414750.622609189.397193480.050827692.049388140.826967920.20664410.826967924.839238010.725221631.996937160.713237350.991997940.713237351.008066610.203731241.285699360.94678990.989857140.94678991.010246790.084072510.6930390.021249580.658643340.021249581.518272390.045999222.067070470.571345060.272496190.571345063.669776080.187686627.321543760.530857180.542683780.530857181.842693730.470179882.846240060.6084340.432624860.6084342.311471440.596005033.286061170.251393360.186365950.251393365.365787110.522996072.158539770.168704360.764816290.168704361.307503530.242030211.061143990.609894750.12915350.609894757.742724830.151845771.106273840.360963270.959082460.360963271.042663220.677471151.890969580.843518160.706325430.843518161.415777990.298543921.182841670.476246580.907382480.476246581.102071090.845476555.089577490.313703320.006244160.31370332160.149530.4541107113.6694345a0510152000.20.40.60.81EndurancePerformanceHighest EnduranceHighest Performance4

5) multi-layer perceptron (MLP)-based handwritten digit recog-
nition (MLP-MNIST) [47] using the MNIST database; 6)
edge detection (EdgeDet) [48] on 64 × 64 images using
difference-of-Gaussian; 7) image smoothing (ImgSmooth) [48]
on 64 × 64 images; 8) heart-rate estimation (HeartEstm) [49]
using ECG data; 9) RNN-based predictive visual pursuit
(VisualPursuit) [50]; and 10) recurrent digit recognition (R-
DigitRecog) [47]. Table II summarizes the topology, the number
of neurons and synapses of these applications, and their baseline
accuracy. To demonstrate the trade-offs, we enable STDP-based
weight updates [32] in each of these applications.2 But our
approach is not limited to STDP.

where ai,j is the number of spikes propagating through
the PCM cell in a given SNN workload and Ei,j is its
endurance.

B. Performance

Figure 5 compares the performance of DFSynthesizer, a
performance-oriented technique to map SNNs to neuromorphic
hardware and SpiNeMap, which minimizes the number of
spikes on the shared interconnect. We observe that compared to
DFSynthesizer, the performance using SpiNeMap is an average
10% lower for these applications.

TABLE II
EVALUATED APPLICATIONS.

Class

CNN

MLP

RNN

Applications
LeNet [39]
AlexNet [41]
VGG16 [43]
HeartClass [45]
DigitRecogMLP
EdgeDet [48]
ImgSmooth [48]
HeartEstm [49]
VisualPursuit [50]
R-DigitRecog [47]

Synapses Neurons Topology
282,936

20,602 CNN
38,730,222 230,443 CNN
99,080,704 554,059 CNN
1,049,249
153,730 CNN
79,400
114,057
9,025
66,406
163,880
11,442

884
6,120
4,096
166
205
567

FeedForward (784, 100, 10)
FeedForward (4096, 1024, 1024, 1024)
FeedForward (4096, 1024)
Recurrent Reservoir
Recurrent Reservoir
Recurrent Reservoir

Accuracy
85.1%
90.7%
69.8 %
63.7%
91.6%
100%
100%
100%
47.3%
83.6%

We model the DYNAP-SE neuromorphic hardware [3] with

the following conﬁgurations.

• A tiled array of 4 tiles, each with a 128x128 crossbar.

There are 65,536 memristors per crossbar.

• Spikes are digitized and communicated between cores
through a mesh routing network using the Address Event
Representation (AER) protocol.

• Each synaptic element is a PCM-based memristor.

Table III reports the hardware parameters of DYNAP-SE.

TABLE III
MAJOR SIMULATION PARAMETERS EXTRACTED FROM [3].

Neuron technology

65nm CMOS

Synapse technology

Supply voltage

PCM

1.0V

Energy per spike

50pJ at 30Hz spike frequency

Energy per routing

147pJ

Switch bandwidth

1.8G. Events/s

We evaluate the following metrics.

• Performance: This is the time it takes to execute an

application on the hardware model.

• Effective lifetime: This is the minimum effective life-
time of all PCM cells in the hardware. The effective
lifetime (Li,j), deﬁned for the PCM cell connecting the
ith pre-synaptic neuron with jth post-synaptic neuron in a
memristive crossbar as

Li,j = Ei,j /ai,j ,

(6)

2Spike-Timing Dependent Plasticity (STDP) [51] is a learning mechanism in
SNNs, where the synaptic weight between a pre- and a post-synaptic neuron is
updated based on the timing of pre-synaptic inputs relative to the post-synaptic
spike.

Fig. 5. Performance normalized to DFSynthesizer.

C. Effective Lifetime

Figure 6 plots the normalized lifetime of DFSynthesizer
and SpiNeMap for the evaluated applications. Lifetime results
are normalized to the lifetime obtained using the mapping
that generates the highest effective lifetime (see Figure 4). We
observe that lifetime using the mapping of DFSynthesizer is on
average 30% lower, while that using SpiNeMap is 19% lower
than the highest lifetime.

Fig. 6. Lifetime normalized to mapping with highest lifetime.

VI. CONCLUSIONS

In this work, we show the trade-offs between performance
and lifetime of neuromorphic hardware with PCM-based
crossbars. Speciﬁcally, we show that in a PCM-based crossbar,
the PCM cells that are located on the bottom-left corner
are faster to access but have lower lifetime than PCM cells
on the top-right corner, which are slower but have higher
lifetime. Existing SNN-mapping techniques do not explore this
trade-offs in mapping neurons and synapses to hardware. The
design space exploration of these mapping techniques often
select mapping that generate high performance or optimize for
energy consumption. Therefore, the lifetime obtained using
these techniques is signiﬁcantly lower than the highest lifetime.
A possible future direction is therefore, to explore the trade-
offs during the design-space exploration. This will enable
generating SNN mapping that are balanced in terms of lifetime,
performance, and energy consumption.

LeNetAlexNetVGG16HeartClassMLP-MNISTEdgeDetImgSmoothHeartEstmVisualPursuitR-DigitRecogAVERAGE01NormalizedperformanceDFSynthesizerSpiNeMapLeNetAlexNetVGG16HeartClassMLP-MNISTEdgeDetImgSmoothHeartEstmVisualPursuitR-DigitRecogAVERAGE01NormalizedlifetimeDFSynthesizerSpiNeMapACKNOWLEDGMENT

This work is supported by the National Science Founda-
tion Award CCF-1937419 (RTML: Small: Design of System
Software to Facilitate Real-Time Neuromorphic Computing).

REFERENCES

[1] M. V. Debole et al., “TrueNorth: Accelerating from zero to 64 million

neurons in 10 years,” Computer, 2019.

[2] M. Davies et al., “Loihi: A neuromorphic manycore processor with

on-chip learning,” IEEE Micro, 2018.

[3] S. Moradi et al., “A scalable multicore architecture with heterogeneous
memory structures for dynamic neuromorphic asynchronous processors
(DYNAPs),” TBCAS, 2017.

[4] E. A. Lee et al., Introduction to embedded systems: A cyber-physical

systems approach. Mit Press, 2016.

[5] W. Shi et al., “Edge computing: Vision and challenges,” IOTJ, 2016.
[6] L. Benini et al., “Networks on chip: A new paradigm for systems on

chip design,” in DATE, 2002.

[7] A. Balaji et al., “Exploration of segmented bus as scalable global

interconnect for neuromorphic computing,” in GLSVLSI, 2019.

[8] F. Catthoor et al., “Very large-scale neuromorphic systems for biological
signal processing,” in CMOS Circuits for Biological Sensing and
Processing, 2018.

[9] G. W. Burr et al., “Neuromorphic computing using non-volatile memory,”

Advances in Physics: X, 2017.

[10] A. Mallik et al., “Design-technology co-optimization for OxRRAM-based

synaptic processing unit,” in VLSIT, 2017.

[11] P. Wijesinghe et al., “An all-memristor deep spiking neural computing
system: A step toward realizing the low-power stochastic brain,” TETCI,
2018.

[12] G. Indiveri, “A low-power adaptive integrate-and-ﬁre neuron circuit,” in

ISCAS, 2003.

[13] S. Song et al., “Compiling spiking neural networks to neuromorphic

hardware,” in LCTES, 2020.

[14] A. Balaji et al., “Enabling resource-aware mapping of spiking neural
networks via spatial decomposition,” Embedded Systems Letters, 2020.
[15] A. Das et al., “Dataﬂow-based mapping of spiking neural networks on

neuromorphic hardware,” in GLSVLSI, 2018.

[16] A. Balaji et al., “A framework for the analysis of throughput-constraints

of snns on neuromorphic hardware,” in ISVLSI, 2019.

[17] A. Balaji et al., “PyCARL: A PyNN interface for hardware-software

co-simulation of spiking neural network,” in IJCNN, 2020.

[18] A. Balaji et al., “Mapping spiking neural networks to neuromorphic

hardware,” TVLSI, 2020.

[19] A. Das et al., “Mapping of local and global synapses on spiking

neuromorphic hardware,” in DATE, 2018.

[20] A. Balaji et al., “A framework to explore workload-speciﬁc performance
and lifetime trade-offs in neuromorphic computing,” CAL, 2019.
[21] S. Song et al., “Improving dependability of neuromorphic computing

with non-volatile memory,” in EDCC, 2020.

[22] S. Song et al., “A case for lifetime reliability-aware neuromorphic

computing,” in MWSCAS, 2020.

[23] T. Titirsha et al., “Thermal-aware compilation of spiking neural networks

to neuromorphic hardware,” in LCPC, 2020.

5

[24] A. Balaji et al., “Run-time mapping of spiking neural networks to

neuromorphic hardware,” JSPS, 2020.

[25] S. Ovshinsky, “Reversible electrical switching phenomena in disordered

structures,” Physical Review Letters, 1968.

[26] B. C. Lee et al., “Architecting phase change memory as a scalable dram

alternative,” in ISCA, 2009.

[27] M. K. Qureshi et al., “Scalable high performance main memory system

using phase-change memory technology,” in ISCA, 2009.

[28] S. Song et al., “Enabling and exploiting partition-level parallelism (palp)

in phase change memories,” TECS, 2019.

[29] S. Song et al., “Exploiting inter-and intra-memory asymmetries for data

mapping in hybrid tiered-memories,” in ISMM, 2020.

[30] S. Song et al., “Improving phase change memory performance with data

content aware access,” in ISMM, 2020.

[31] S. Song et al., “Aging Aware Request Scheduling for Non-Volatile Main

Memory,” in ASP-DAC, 2021.

[32] S. R. Kheradpisheh et al., “STDP-based spiking deep convolutional
neural networks for object recognition,” Neural Networks, 2018.
[33] L. Fei-Fei et al., “One-shot learning of object categories,” TPAMI, 2006.
[34] J. M. Allred et al., “Stimulating stdp to exploit locality for lifelong learn-
ing without catastrophic forgetting,” Purdue University West Lafayette
United States, Tech. Rep., 2019.

[35] M. S. Shim et al., “Biologically inspired reinforcement learning for

mobile robot collision avoidance,” in IJCNN, 2017.

[36] M. Avrami, “Granulation, phase change, and microstructure kinetics of

phase change. III,” The Journal of Chemical Physics, 1941.

[37] D. B. Strukov, “Endurance-write-speed tradeoffs in nonvolatile memories,”

Applied Physics A: Materials Science and Processing, 2016.

[38] J. Kennedy, “Particle swarm optimization,” Encyclopedia of machine

learning, 2010.

[39] Y. LeCun et al., “Lenet-5, convolutional neural networks,” URL:

http://yann. lecun. com/exdb/lenet, 2015.

[40] L. Deng, “The mnist database of handwritten digit images for machine

learning research,” IEEE Signal Processing Magazine, 2012.

[41] A. Krizhevsky et al., “Imagenet classiﬁcation with deep convolutional
neural networks,” in Advances in neural information processing systems
(NeurIPS), 2012.

[42] J. Deng et al., “Imagenet: A large-scale hierarchical image database,” in
Conference on Computer Vision and Pattern Recognition (CVPR), 2009.
[43] K. Simonyan et al., “Very deep convolutional networks for large-scale

image recognition,” arXiv, 2014.

[44] A. Das et al., “Heartbeat classiﬁcation in wearables using multi-layer
perceptron and time-frequency joint distribution of ecg,” in CHASE,
2018.

[45] A. Balaji et al., “Power-accuracy trade-offs for heartbeat classiﬁcation

on neural networks hardware,” JOLPE, 2018.

[46] G. B. Moody et al., “Physionet: a web-based resource for the study
of physiologic signals,” IEEE Engineering in Medicine and Biology
Magazine, 2001.

[47] P. U. Diehl et al., “Unsupervised learning of digit recognition using spike-
timing-dependent plasticity,” Frontiers in Computational Neuroscience,
2015.

[48] T. Chou et al., “CARLsim 4: An open source library for large
scale, biologically detailed spiking neural network simulation using
heterogeneous clusters,” in International Joint Conference on Neural
Networks (IJCNN), 2018.

[49] A. Das et al., “Unsupervised heart-rate estimation in wearables with
Liquid states and a probabilistic readout,” Neural Networks, 2018.
[50] H. J. Kashyap et al., “A recurrent neural network based model of
predictive smooth pursuit eye movement in primates,” in International
Joint Conference on Neural Networks (IJCNN), 2018.

[51] Y. Dan et al., “Spike timing-dependent plasticity of neural circuits,”

Neuron, vol. 44, pp. 23–30, 2004.

