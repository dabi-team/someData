0
2
0
2

y
a
M
3
1

]
L
M

.
t
a
t
s
[

2
v
2
4
1
9
0
.
2
0
0
2
:
v
i
X
r
a

Learning Optimal Classiﬁcation Trees:

Strong Max-Flow Formulations

Sina Aghaei1, Andrés Gómez2, Phebe Vayanos1

2Department of Industrial and Systems Engineering, Viterbi School of Engineering

1Center for Artiﬁcial Intelligence in Society

1,2University of Southern California

{saghaei,gomezand,phebe.vayanos}@usc.edu

Abstract

We consider the problem of learning optimal binary classiﬁcation trees. Literature on the

topic has burgeoned in recent years, motivated both by the empirical suboptimality of heuristic

approaches and the tremendous improvements in mixed-integer programming (MIP) technology.

Yet, existing approaches from the literature do not leverage the power of MIP to its full extent.

Indeed, they rely on weak formulations, resulting in slow convergence and large optimality gaps.

To ﬁll this gap in the literature, we propose a ﬂow-based MIP formulation for optimal binary

classiﬁcation trees that has a stronger linear programming relaxation. Our formulation presents

an attractive decomposable structure. We exploit this structure and max-ﬂow/min-cut duality to

derive a Benders’ decomposition method, which scales to larger instances. We conduct extensive

computational experiments on standard benchmark datasets on which we show that our proposed

approaches are 50 times faster than state-of-the art MIP-based techniques and improve out of

sample performance up to 13.8%.

1.

Introduction

1.1. Motivation & Related Work

Since their inception over 30 years ago, decision trees have become among the most popular

techniques for interpretable machine learning (classiﬁcation and regression), see Breiman (1984).

A decision tree takes the form of a binary tree. In each internal node of the tree, a binary test is

performed on a speciﬁc feature. Two branches emanate from each internal node, with each branch

representing the outcome of the test. If a datapoint passes (resp. fails) the test, it is directed to

the left (resp. right) branch. A predicted label is assigned to all leaf nodes. Thus, each path from

root to leaf represents a classiﬁcation rule that assigns a unique label to all datapoints that reach

1

 
 
 
 
 
 
that leaf. The goal in the design of optimal decision trees is to select the tests to perform at each

internal node and the labels to assign to each leaf to maximize prediction accuracy (classiﬁcation)

or to minimize prediction error (regression). Not only are decision trees popular in their own right;

they also form the backbone for more sophisticated machine learning models. For example, they

are the building blocks for random forests, one of the most popular and stable machine learning

techniques available, see e.g., Liaw and Wiener (2002). They have also proved useful to provide

explanations for the solutions to optimization problems, see e.g., Bertsimas and Stellato (2018).

The problem of learning optimal decision trees is an N P-hard problem, see Hyaﬁl and Rivest

(1976) and Breiman (2017). It can intuitively be viewed as a combinatorial optimization problem

with an exponential number of decision variables: at each internal node of the tree, one can select

what feature to branch on (and potentially the level of that feature), guiding each datapoint to the

left or right using logical constraints.

Traditional Methods. Motivated by these hardness results, traditional algorithms for learning

decision trees have relied on heuristics that employ very intuitive, yet ad-hoc, rules for constructing

the decision trees. For example, CART uses the Gini Index to decide on the splitting, see Breiman

(1984); ID3 employs entropy, see Quinlan (1986); and C4.5 leverages normalized information gain,

see Quinlan (2014). The high quality and speed of these algorithms combined with the availability of

software packages in many popular languages such as R or Python has facilitated their popularization,

see e.g., Kuhn et al. (2018), Therneau et al. (2015). They are now routinely used in commercial,

medical, and other applications.

Mathematical Programming Techniques. Motivated by the heuristic nature of traditional ap-

proaches, which provide no guarantees on the quality of the learned tree, several researchers have

proposed algorithms for learning provably optimal trees based on techniques from mathematical

optimization. Approaches for learning optimal decision-trees rely on enumeration coupled with

rules to prune-out the search space. For example, Nijssen and Fromont (2010) use itemset mining

algorithms and Narodytska et al. (2018) use satisﬁability (SAT) solvers. Verhaeghe et al. (2019)

propose a more elaborate implementation combining several ideas from the literature, including

branch-and-bound, itemset mining techniques and caching. Hu et al. (2019) use analytical bounds (to

aggressively prune-out the search space) combined with a tailored bit-vector based implementation.

The Special Case of MIP. As an alternative approach to conducting the search for optimal trees,

Bertsimas and Dunn (2017) recently proposed to use mixed-integer programming (MIP) to learn

optimal classiﬁcation trees. Following this work, using MIP to learn decision trees gained a lot of

traction in the literature with the works of Günlük et al. (2018), Aghaei et al. (2019), and Verwer

and Zhang (2019). This is no coincidence. First, MIP comes with a suit of oﬀ-the shelf solvers and

2

algorithms that can be leveraged to eﬀectively prune-out the search space. Indeed, solvers such as

CPLEX (2009) and Gurobi Optimization (2015) have beneﬁted from decades of research, see Bixby

(2012), and have been very successful at solving a broad class of MIP problems. Second, MIP comes

with a highly expressive language that can be used to tailor the objective function of the problem

or to augment the learning problem with constraints of practical interest. For example, Aghaei

et al. (2019) leverage the power of MIP to incorporate fairness and interpretability constraints into

learned classiﬁcation and regression trees. They also show how MIP technology can be exploited to

learn decision trees with more sophisticated structure (linear branching and leaﬁng rules). Similarly,

Günlük et al. (2018) use MIP to solve classiﬁcation trees with combinatorial branching decisions.

MIP formulations have also been leveraged to design decision trees for decision- and policy-making

problems, see Azizi et al. (2018) and Ciocan and Miši´c (2018), and for optimizing decisions over

tree ensembles, see Mišic (2017).

Discussion & Motivation. The works of Bertsimas and Dunn (2017), Günlük et al. (2018), Aghaei

et al. (2019), and Verwer and Zhang (2019) have served to showcase the modeling power of using

MIP to learn decision trees and the potential suboptimality of traditional algorithms. Yet, we argue

that they have not leveraged the power of MIP to its full extent. A critical component for eﬃciently

solving MIPs is to pose good formulations, but determining such formulations is no simple task.

The standard approach for solving MIP problems is the branch-and-bound method, which partitions

the search space recursively and solves Linear Programming (LP) relaxations for each partition

to produce lower bounds for fathoming sections of the search space. Thus, since solving a MIP

requires solving a large sequence of LPs, small and compact formulations are desirable as they

enable the LP relaxation to be solved faster. Moreover, formulations with tight LP relaxations,

referred to as strong formulations, are also desirable, as they produce higher quality lower bounds

which lead to a faster pruning of the search space, ultimately reducing the number of LPs to be

solved. Unfortunately, these two goals are at odds with one another, with stronger relaxations

often requiring additional variables and constraints than weak ones. For example, in the context

of decision trees, Verwer and Zhang (2019) propose a MIP formulation with signiﬁcantly fewer

variables and constraints than the formulation of Bertsimas and Dunn (2017), but in the process

weaken the LP relaxation. As a consequence, neither method consistently dominates the other.

We note that in the case of MIPs with large numbers of decision variables and constraints,

classical decomposition techniques from the Operations Research literature may be leveraged to

break the problem up into multiple tractable subproblems of benign complexity. A notable example

of a decomposition algorithm is Benders’ (Benders 1962). Bender’s decomposition exploits the

structure of mathematical programming problems with so-called complicating variables which couple

constraints with one another and which, once ﬁxed, result in an attractive decomposable structure

that is leveraged to speed-up computation and alleviate memory consumption, allowing the solution

3

of large-scale MIPs. To the best of our knowledge, existing approaches from the literature have

not sought explicitly strong formulations, neither have they attempted to leverage the potentially

decomposable structure of the problem. This is precisely the gap we ﬁll with the present work.

1.2. Proposed Approach & Contributions

Our approach and main contributions in this paper are:

(a) We propose an intuitive ﬂow-based MIP formulation for learning optimal classiﬁcation trees

with binary data. Notably, our proposed formulation does not use big-M constraints, which

are known to lead to weak LP relaxations. We also show that the resulting LP relaxation is

stronger than existing alternatives.

(b) Our proposed formulation is amenable to Bender’s decomposition. In particular, binary tests

are selected in the master problem and each subproblem guides each datapoint through the

tree via a max-ﬂow formulation. We leverage the max-ﬂow structure of the subproblems to

solve them eﬃciently via min-cut procedures.

(c) We present the ﬁrst polyhedral results concerning the convex hull of the feasible region of

decision trees: we show that all cuts added in our proposed Benders method are facets of this

decision tree polytope.

(d) We conduct extensive computational studies, showing that our formulations improve upon the

state-of-the-art MIP algorithms, both in terms of in-sample solution quality (and speed) and

out-of-sample performance.

The proposed modeling and solution paradigm can act as a building block for the faster and more

accurate learning of more sophisticated trees. Continuous data can be discretized and binarized

to address problems with continuous labels, see Breiman (2017). Regression trees can be obtained

via minor modiﬁcations of the formulation, see e.g., Verwer and Zhang (2017). Fairness and

interpretability constraints can naturally be incorporated into the problem, see Aghaei et al. (2019).

We leave these studies to future work.

The rest of the paper is organized as follows. We introduce our ﬂow-based formulation and

our Bender’s decomposition method in §2 and §3, respectively. We report in §5 computational

experiments with popular benchmark datasets.

2. Decision Tree Formulation

2.1. Problem Formulation

We are given a training dataset T := {xi, yi}i∈I consisting of datapoints indexed in the set I.
Each row i ∈ I of this dataset consists of F binary features indexed in the set F and collected

4

in the vector xi ∈ {0, 1}F and a label yi drawn from the ﬁnite set K of classes. We consider the

problem of designing an optimal decision tree that minimizes the misclassiﬁcation rate based on

MIP technology.

The key idea behind our model is to augment the decision tree with a single source node s that

is connected to the root node (node 1) of the tree and a single sink node t connected to all nodes

of the tree, see Figure 1. This modiﬁcation enables us to think of the decision tree as a directed

acyclic graph with a single source and sink node. Datapoints ﬂow from source to sink through a

single path and only reach the sink if they are correctly classiﬁed (they will face a “road block” if

incorrectly classiﬁed which will prevent the datapoint from traversing the graph at all). Similar to

traditional algorithms for learning decision trees, we allow labels to be assigned to internal nodes of

the tree. In that case, correctly classiﬁed datapoints that reach such nodes are directly routed to

the sink node (as if we had a “short circuit”).

Next, we introduce our notation and conventions that will be useful to present our model. We

denote by N and L the sets of all internal and leaf nodes in the tree, respectively. For each node

n ∈ N ∪ L, we let a(n) be the direct ancestor of n in the graph. For n ∈ N , let ‘(n) (resp. r(n))

∈ N ∪ L represent the left (resp. right) direct descendant of node n in the graph. In particular,

we have a(1) = s. We will say that we branch on feature f ∈ F at node n ∈ N if the binary test
performed at n asks “Is xi
f = 0”? Datapoint i will be directed left (right) if the answer is aﬃrmative

(negative).

The decision variables for our formulation are as follows. The variable bnf ∈ {0, 1} indicates
if we branch on (i.e., perform a binary test on) feature f ∈ F at node n ∈ N . If P
f ∈F bnf = 0
for some node n ∈ N , no feature is selected to branch on at that node, and a class is assigned to

node n. We let the variable wnk ∈ {0, 1} indicate if the predicted class for node n ∈ N ∪ L is k ∈ K.
A datapoint i is correctly classiﬁed iﬀ it reaches some node n such that wnk = 1 with k = yi. Points
that arrive at that node and that are correctly classiﬁed are directed to the sink. For each node
n ∈ N and for each datapoint i ∈ I, we introduce a binary valued decision variable zi
a(n),n which
equals 1 if and only if the ith datapoint is correctly classiﬁed (i.e., reaches the sink) and traverses
the edge between nodes a(n) and n. We let zi

n,t be deﬁned accordingly for each edge between node

n ∈ N ∪ L and sink t.

5

Figure 1: A classiﬁcation tree of depth 2 viewed as a directed acyclic graph with a single source and
sink.

The ﬂow-based formulation for decision trees reads

max (1 − λ) X
i∈I

X

n,t − λ X
zi

X

bnf

n∈N ∪L

n∈N

f ∈F

s.t. X
f ∈F
X

bnf + X

wnk = 1

k∈K

wnk = 1

n,‘(n) + zi

n,r(n) + zi
n,t

∀n ∈ N

∀n ∈ L

∀n ∈ N , i ∈ I

∀i ∈ I, n ∈ L

∀i ∈ I

∀n ∈ N , i ∈ I

∀n ∈ N , i ∈ I

∀i ∈ I : yi = k, n ∈ N ∪ L

∀n ∈ N , f ∈ F

∀n ∈ N ∪ L, k ∈ K

∀n ∈ N ∪ L, i ∈ I

∀n ∈ L, i ∈ I,

bnf

bnf

k∈K
a(n),n = zi
zi
a(n),n = zi
zi
n,t
zi
s,1 ≤ 1
n,‘(n) ≤ X
zi

f ∈F :xi

f =0

n,r(n) ≤ X
zi

f ∈F :xi

f =1

zi
n,t ≤ wnk

bnf ∈ {0, 1}

wnk ∈ {0, 1}
zi
a(n),n ∈ {0, 1}
zi
n,t ∈ {0, 1}

(1.1)

(1.2)

(1.3)

(1.4)

(1.5)

(1.6)

(1.7)

(1.8)

(1.9)

(1.10)

(1.11)

(1.12)

(1.13)

where λ ∈ [0, 1] is a regularization weight. The objective (1.1) maximizes the total number of
correctly classiﬁed points P
f ∈F bnf .
Thus, λ controls the trade-oﬀ between these competing objectives, with larger values of lambda

n,t while minimizing the number of splits P

n∈N ∪L zi

n∈N

i∈I

P

P

6

s<latexit sha1_base64="t5Z8j6uw1dDrul9BiPoeucvIxm0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipqQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6perVpr1ir12zyOIpzBOVyCB9dQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A4OuM/g==</latexit>1<latexit sha1_base64="RSOBfsC0h9n0lPM3VbiR0pceZig=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip6Q3KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W9WrXWrFXqt3kcRTiDc7gED66hDvfQgBYwQHiGV3hzHp0X5935WLYWnHzmFP7A+fwBfOOMvA==</latexit>2<latexit sha1_base64="XW0qcPrccSfI7onjbbF2xR2gGy4=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4Kkkp6LHoxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzUrA5KZbfiLkDWiZeTMuRoDEpf/WHM0gilYYJq3fPcxPgZVYYzgbNiP9WYUDahI+xZKmmE2s8Wh87IpVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNkUbgrf68jppVyterVJr1sr12zyOApzDBVyBB9dQh3toQAsYIDzDK7w5j86L8+58LFs3nHzmDP7A+fwBfmeMvQ==</latexit>3<latexit sha1_base64="LmvAoiNgRcYEisiHXakVSuvpF8g=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokW9Fj04rEF+wFtKJvtpF272YTdjVBCf4EXD4p49Sd589+4bXPQ1gcDj/dmmJkXJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJssFrHqBFSj4BKbhhuBnUQhjQKB7WB8N/PbT6g0j+WDmSToR3QoecgZNVZqXPVLZbfizkFWiZeTMuSo90tfvUHM0gilYYJq3fXcxPgZVYYzgdNiL9WYUDamQ+xaKmmE2s/mh07JuVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNkUbgrf88ippXVa8aqXaqJZrt3kcBTiFM7gAD66hBvdQhyYwQHiGV3hzHp0X5935WLSuOfnMCfyB8/kDf+uMvg==</latexit>4<latexit sha1_base64="YyGWf7FHWY1YzP9SDwB8wuM+bzs=">AAAB5HicbVBNS8NAEJ3Urxq/qlcvi0XwVBIp2GPRi8cK9gPaUDbbSbt2swm7G6GE/gIvHhSv/iZv/hu3bQ7a+mDg8d4MM/PCVHBtPO/bKW1t7+zulffdg8Oj45OKe9rRSaYYtlkiEtULqUbBJbYNNwJ7qUIahwK74fRu4XefUWmeyEczSzGI6VjyiDNqrPRQH1aqXs1bgmwSvyBVKNAaVr4Go4RlMUrDBNW673upCXKqDGcC5+4g05hSNqVj7FsqaYw6yJeHzsmlVUYkSpQtachS/T2R01jrWRzazpiaiV73FuJ/Xj8zUSPIuUwzg5KtFkWZICYhi6/JiCtkRswsoUxxeythE6ooMzYb14bgr7+8STrXNb9eq1ebt0UYZTiHC7gCH26gCffQgjYwQHiBN3h3npxX52PVWHKKiTP4A+fzBxb9i5U=</latexit>5<latexit sha1_base64="RTnT9GACZjSEkfi58lGJCASK3Us=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOl5mW/XHGr7hxklXg5qUCORr/81RvELI1QGiao1l3PTYyfUWU4Ezgt9VKNCWVjOsSupZJGqP1sfuiUnFllQMJY2ZKGzNXfExmNtJ5Ege2MqBnpZW8m/ud1UxNe+xmXSWpQssWiMBXExGT2NRlwhcyIiSWUKW5vJWxEFWXGZlOyIXjLL6+S9kXVq1VrzVqlfpPHUYQTOIVz8OAK6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8fgvOMwA==</latexit>6<latexit sha1_base64="TxpuHov6zkv80qcfRPVyYFgoy0A=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOl5mW/XHGr7hxklXg5qUCORr/81RvELI1QGiao1l3PTYyfUWU4Ezgt9VKNCWVjOsSupZJGqP1sfuiUnFllQMJY2ZKGzNXfExmNtJ5Ege2MqBnpZW8m/ud1UxNe+xmXSWpQssWiMBXExGT2NRlwhcyIiSWUKW5vJWxEFWXGZlOyIXjLL6+S9kXVq1VrzVqlfpPHUYQTOIVz8OAK6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8fhHeMwQ==</latexit>7<latexit sha1_base64="taWcOcC06x3gohqXrSs+/h1ZC7M=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokU6rHoxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzUrA1KZbfiLkDWiZeTMuRoDEpf/WHM0gilYYJq3fPcxPgZVYYzgbNiP9WYUDahI+xZKmmE2s8Wh87IpVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNkUbgrf68jppX1e8aqXarJbrt3kcBTiHC7gCD2pQh3toQAsYIDzDK7w5j86L8+58LFs3nHzmDP7A+fwBhfuMwg==</latexit>t<latexit sha1_base64="oHGXHZo5G8TdNmPNtJfxRQRlOrM=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsN+3azSbsToQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNM7uZ+54lrI2L1gNOE+xEdKREKRtFKTRyUK27VXYCsEy8nFcjRGJS/+sOYpRFXyCQ1pue5CfoZ1SiY5LNSPzU8oWxCR7xnqaIRN362OHRGLqwyJGGsbSkkC/X3REYjY6ZRYDsjimOz6s3F/7xeiuGNnwmVpMgVWy4KU0kwJvOvyVBozlBOLaFMC3srYWOqKUObTcmG4K2+vE7aV1WvVq01a5X6bR5HEc7gHC7Bg2uowz00oAUMODzDK7w5j86L8+58LFsLTj5zCn/gfP4A4m+M/w==</latexit>N<latexit sha1_base64="+x3984mTXf253KD79FQEYgaEHRw=">AAAB8nicbVDLSsNAFL3xWeur6tLNYBFclUQKuiy6cSUV7APaUCbTSTt0MgkzN0IJ/Qw3LhRx69e482+ctFlo64GBwzn3MueeIJHCoOt+O2vrG5tb26Wd8u7e/sFh5ei4beJUM95isYx1N6CGS6F4CwVK3k00p1EgeSeY3OZ+54lrI2L1iNOE+xEdKREKRtFKvX5EccyozO5ng0rVrblzkFXiFaQKBZqDyld/GLM04gqZpMb0PDdBP6MaBZN8Vu6nhieUTeiI9yxVNOLGz+aRZ+TcKkMSxto+hWSu/t7IaGTMNArsZB7RLHu5+J/XSzG89jOhkhS5YouPwlQSjEl+PxkKzRnKqSWUaWGzEjammjK0LZVtCd7yyaukfVnz6rX6Q73auCnqKMEpnMEFeHAFDbiDJrSAQQzP8ApvDjovzrvzsRhdc4qdE/gD5/MHhlWRaw==</latexit>L<latexit sha1_base64="kSBvzVtw9bZt7iC3lTpdZutzqs0=">AAAB8nicbVDLSsNAFL3xWeur6tLNYBFclUQKuiy6ceGign1AG8pkOmmHTiZh5kYooZ/hxoUibv0ad/6NkzYLbT0wcDjnXubcEyRSGHTdb2dtfWNza7u0U97d2z84rBwdt02casZbLJax7gbUcCkUb6FAybuJ5jQKJO8Ek9vc7zxxbUSsHnGacD+iIyVCwShaqdePKI4Zldn9bFCpujV3DrJKvIJUoUBzUPnqD2OWRlwhk9SYnucm6GdUo2CSz8r91PCEsgkd8Z6likbc+Nk88oycW2VIwljbp5DM1d8bGY2MmUaBncwjmmUvF//zeimG134mVJIiV2zxUZhKgjHJ7ydDoTlDObWEMi1sVsLGVFOGtqWyLcFbPnmVtC9rXr1Wf6hXGzdFHSU4hTO4AA+uoAF30IQWMIjhGV7hzUHnxXl3Phaja06xcwJ/4Hz+AINLkWk=</latexit>corresponding to greater regularization. An interpretation of the constraints is as follows. Con-

straint (1.2) ensures that at each node we either branch on a feature or assign a class label to it

(but not both, the label is only used if we do not branch at that node). Constraint (1.3) guarantees

that each leaf has a unique predicted class label. Constraint (1.4) is a ﬂow conservation constraint

for each datapoint i and node n ∈ N : it ensures that if a datapoint arrives at a node, it must also

leave the node through one of its descendants, or be correctly classiﬁed and routed to t. Similarly,

constraint (1.5) enforces ﬂow conservation for each node n ∈ L. The inequality constraint (1.6)

ensures that at most one unit of ﬂow can enter the graph through the source. Constraints (1.7)

and (1.8) ensure that if a datapoint is routed to the left (right) at node n, then one of the features
such that xi
f = 1) must have been selected for branching at the node. Constraint (1.9)

f = 0 (xi

ensures that datapoints routed to the sink node t are correctly classiﬁed.

Given a choice of branching and labeling decisions, b and w, each datapoint is allotted one unit

of ﬂow which it attempts to guide through the graph from the source node to the sink node. If

the datapoint cannot be correctly classiﬁed, the ﬂow that will reach the sink (and by extension

enter the source) will be zero. In particular note that once the b and w variables have been ﬁxed,

optimization of the ﬂows can be done separately for each datapoint. This implies that the problem

can be decomposed to speed-up computation, an idea that we leverage in Section 3. In particular,

note that the optimization over ﬂow variables can be cast as a max-ﬂow problem for each datapoint,

implying that the integrality constraint on the z variables can be relaxed to yield an equivalent

formulation. We leverage this idea in our computational experiments.

Formulation (1) has several distinguishing features relative to existing MIP formulations for

training decision trees

i) It does not use big-M constraints.

ii) It includes ﬂow variables indicating whether each datapoint is directed to the left or right at

each branching node.

iii) It only tracks datapoints that are correctly classiﬁed.

The number of variables and constraints in formulation (1) is O(cid:0)2d(|I| + |F|)(cid:1), where d is the
tree depth. Thus, its size is of the same order as the one proposed by Bertsimas and Dunn (2017).

Nonetheless, as we discuss in §2.2, the LP relaxation of formulation (1) is tighter, and therefore

results in a more aggressive pruning of the search space without incurring in signiﬁcant additional

costs.

2.2. Strength of the Flow-Based Formulation

We now argue that formulation (1), which we henceforth refer to as ﬂow-based formulation, is

stronger than existing formulations from the literature. The BinOCT formulation of Verwer and

7

Zhang (2019) is obtained by aggregating constraints from the OCT formulation of Bertsimas and

Dunn (2017) (using big-M constants). As a consequence, its relaxation is weaker. Thus, it suﬃces

to argue that the proposed formulation is stronger than OCT.

Proposition 1. If λ = 0, then formulation (1) has a stronger relaxation than OCT.

A formal proof of Proposition 1 is given in online companion C. In the following, we provide some

intuition in how formulation (1) is stronger. We work with a simpliﬁed version of the formulation of

Bertsimas and Dunn (2017) specialized to the case of binary data. We provide this formulation in

the online companion B.

2.2.1. No big-M s

In this section, we argue that the absence of big-M constraints in our formulation induces a stronger

formulation. In the OCT formulation, for i ∈ I and n ∈ L, there are binary variables ζ such that
ζ i
a(n),n = 1 if datapoint i is assigned to leaf node n (regardless of whether that point is correctly
classiﬁed or not), and ζ i
a(n),n = 0 otherwise. In addition, the authors introduce a variable Ln
that represents the number of missclassiﬁed points at leaf node n, and this variable is deﬁned via

constraints Ln ≥ 0 and

Ln ≥ X

i∈I

a(n),n − X
ζ i
i∈I:
yi=k

ζ i
a(n),n − |I|(1 − wnk) ∀k ∈ K.

Thus, the number of correctly classiﬁed points is |I| − P
with M = |I|, which is activated or deactivated depending on whether wnk = 1 or not.

n∈L Ln. Note that this is a big-M constraint,

The LP relaxation induced from counting correctly classiﬁed points can be improved. The

number of such points, using the variables above, is

|I| − X
n∈L

Ln = X

X

ζ i
a(n),nwnyi.

n∈L

i∈I

(2)

The right hand side of (2) is nonlinear (quadratic). Nonetheless, the quadratic function is supermod-

ular, see Nemhauser et al. (1978), and its concave envelop can be described by introducing variables
a(n),n := ζ i
zi

a(n),nwnyi via the system

|I| − X
n∈L
a(n),n ≤ ζ i
zi

Ln ≤ X

n∈L
a(n),n, zi

zi
a(n),n

X

i∈I

a(n),n ≤ wnyi ∀n ∈ N , i ∈ I.

The additional variables z are precisely the variables used in formulation (1). Note that a simple

application of this idea would require the introduction of additional variables for each pair (i, n).

8

However, by noting that the desired tree structure can be enforced using the new variables z only,

and the original variables ζ can be dropped, we achieve this strengthening without incurring the

cost of a larger formulation.

2.2.2.

Improved branching constraints

To correctly enforce the branching structure of the decision-tree, Bertsimas and Dunn (2017) use

(after specializing their formulation to the case of binary data) constraints of the form

zi
a(m),m ≤ 1 − bnf ∀i ∈ I, m ∈ L, n ∈ AL(m), f ∈ F : xi

f = 1,

(3)

where AL(m) denotes the set of ancestors of m whose left branch was followed on the path from

the root to m. An intrepretation of this constraint is as follows: if datapoint i reaches leaf node

m, then for all nodes in the path where i took the left direction, no branching decision bnf can be
made that would cause the point to go right. Instead, we use constraint (1.7).

We now show that (1.7) induces a stronger LP relaxation. First, we focus on the left hand side

of (1.7): due to ﬂow conservation constraints (1.4), we ﬁnd that

n,‘(n) = X
zi

zi
a(m),m

m∈L:m∈LD(n)

where, following the notation of Bertsimas and Dunn (2017), LD(n) is the set of left descendants of

n. In particular, the left hand side of constraint (1.7) is larger than the left hand side of (3). Now,

we focus on the right hand side: from constraints (1.2), we ﬁnd that

X

bnf = 1 − X

f ∈F :xi

f =0

k∈K

ynk − X
f ∈F :xi

f =1

bnf .

In particular, the right hand side of (1.7) is smaller than the right hand side of (3). Similar

arguments can be made for constraint (1.8). As a consequence, the linear inequalities for branching

induced from formulation (1) dominate those proposed by Bertsimas and Dunn (2017).

2.2.3. Further Strengthening of the Formulation

Formulation (1) can be strengthened even more through the addition of cuts.

subset of the rows such that: a) i ∈ H ⇒ xi

Let n ∈ N be any node such that ‘(n) and r(n) ∈ L. Also, let f ∈ F and deﬁne H ⊆ I as any
f = 1, and b) i, j ∈ H ⇒ yi 6= yj. Intuitively, H is a set
of points belonging to diﬀerent classes that would all be assigned to the right branch if feature f is

9

selected for branching. Then, the constraint

zi
n,‘(n) ≤ 1 − bn,f

X

i∈H

(4)

is valid: indeed, if bn,f = 1, then none of the points in H can be assigned to the left branch; and, if
bn,f = 0, then at most one of the points in H can be correctly classiﬁed.

None of the constraint in (1) implies (4). As a matter of fact, if all constraints (4) are added for

all possible combinations of sets H, nodes n and features f , then variables wnk with n ∈ L could be
dropped from the formulation, along with constraints (1.9) and (1.3). Naturally, we do not add all

constraints (4) a priori, but instead use cuts to enforce them as needed.

3. A Benders’ Decomposition Approach

The ﬂow-based formulation (1) is eﬀective at reducing the number of branch-and-bound nodes

required to prove optimality when compared with existing formulations, and results in a substantial

speedup in small- and medium-sized instances, see §5. However, in larger instances, the computational

time required to solve the LP relaxations may become prohibitive, impairing its performance in

branch-and-bound.

Recall from §2 that, if variables b and w are ﬁxed, then the problem decomposes into |I|

independent subproblems, one for each datapoint. Additionally, each problem is a maximum

ﬂow problem, for which specialized polynomial-time methods exist. Due to these characteristics,

formulation (1) can be naturally tackled using Benders’ decomposition, see Benders (1962). In what

follows, we describe the Benders’ decomposition approach.

Observe that problem (1) can be written in an equivalent fashion by making the subproblems

explicit as follows:

max (1 − λ) X
i∈I

gi(b, w) − λ X
n∈N

X

f ∈F

bnf

s.t. X
f ∈F
X

k∈K

bnf + X

wnk = 1

k∈K

wnk = 1

bnf ∈ {0, 1}

wnk ∈ {0, 1}

∀n ∈ N

∀n ∈ L

∀n ∈ N , f ∈ F

∀n ∈ N ∪ L, k ∈ K,

(5.1)

(5.2)

(5.3)

(5.4)

(5.5)

where, for any ﬁxed i ∈ I, w and b, gi(b, w) is deﬁned as the optimal objective value of the max-ﬂow

10

problem

max X

zi
n,t

s.t. zi

n,‘(n) + zi

n,r(n) + zi
n,t

n∈N ∪L
a(n),n = zi
zi
a(n),n = zi
n,t
s,1 ≤ ci
zi
zi
n,‘(n) ≤ ci
n,r(n) ≤ ci
zi
n,t ≤ ci
zi
zi
a(n),n ≥ 0
zi
n,t ≥ 0

s,1(b, w)

n,‘(n)(b, w)

n,r(n)(b, w)

n,t(b, w)

∀n ∈ N

∀n ∈ L

∀n ∈ N

∀n ∈ N

∀n ∈ N ∪ L

∀n ∈ N ∪ L

∀n ∈ N ∪ L.

(6.1)

(6.2)

(6.3)

(6.4)

(6.5)

(6.6)

(6.7)

(6.8)

(6.9)

In formulation (6) we use the shorthand cnn0(b, w) to represent upper bounds on the decision
variables z. These values can be interpreted as edge capacities in the ﬂow problem, and are given as
ci
s,1(b, w) := 1 for all n ∈ N , ci
f =1 bnf for
n,t(b, w) := wnyi. Note that gi(b, w) = 1 if point i is correctly classiﬁed
all n ∈ N ∪ L, and ﬁnally ci

n,r(n)(b, w) := P

n,‘(n)(b, w) := P

f =0 bnf and ci

f ∈F :xi

f ∈F :xi

given the tree structure and class labels induced by (b, w).

From the well-known max-ﬂow/min-cut duality, we ﬁnd that gi(b, w) also equals the optimal

value of the dual of the above max-ﬂow problem, which is expressible as

min ci

s,1(b, w)qs,1 + X

n,‘(n)(b, w)qn,‘(n) + X
ci

n,r(n)(b, w)qn,r(n) + X
ci

ci
n,t(b, w)qn,t

(7.1)

n∈N

n∈N

n∈N ∪L

s.t. qs,1 + p1 ≥ 1

qn,‘(n) + p‘(n) − pn ≥ 0

qn,r(n) + pr(n) − pn ≥ 0

qn,t − pn ≥ 0

qs,1 ≥ 0

qn,‘(n), qn,r(n) ≥ 0

qn,t ≥ 0

(7.2)

∀n ∈ N (7.3)

∀n ∈ N (7.4)

∀n ∈ N ∪ L (7.5)

(7.6)

∀n ∈ N (7.7)

∀n ∈ N ∪ L .(7.8)

Problem (7) is a minimum cut problem, where variable pn is one if and only if node n is in the
source set (we implicitly ﬁx ps = 1), and variable qi,j is one if and only if arc (i, j) is part of the
minimum cut. Note that the feasible region (7.2)-(7.8) of the minimum cut problem does not depend

on the variables (b, w); we denote this feasible region by P.

11

We can now reformulate the master problem (5) as follows:

max (1 − λ) X
i∈I

gi − λ X
n∈N

s.t. gi ≤ ci

s,1(b, w)qs,1 + X

f ∈F
n,‘(n)(b, w)qn,‘(n) + X
ci

X

bnf

(8.1)

+ X

n∈N
ci
n,t(b, w)qn,t

n∈N ∪L

n∈N

X

bnf + X

wnk = 1

f ∈F
X

k∈K

wnk = 1

k∈K
gi ≤ 1

bnf ∈ {0, 1}

wnk ∈ {0, 1}

ci
n,r(n)(b, w)qn,r(n)

∀q : (p, q) ∈ P

(8.2)

∀n ∈ N

(8.3)

∀n ∈ L

(8.4)

∀i ∈ I

∀n ∈ N , f ∈ F

∀n ∈ N ∪ L, k ∈ K.

(8.5)

(8.6)

(8.7)

In the above formulation, we have added constraint (8.5) to make sure we get bounded solutions

in the relaxed master problem. Note that constraint (8.2) can be relaxed to only hold ∀q :

(p, q) ∈ ext(P), where ext(P) denotes the extreme points of P. These extreme points correspond

to cuts induced by (7.2)-(7.8) in the graph. Moreover, observe that equalities (8.3) and (8.4)

f ∈F bnf + P

can be relaxed to inequalities without loss of generality. Indeed, in any feasible solution where
P

k∈K wnk < 1 for some n ∈ N , it is possible to set any wnk to unity to obtain a feasible
solution with identical objective value and where (8.3) is satisﬁed at equality. We deﬁne H= as
the set of (b, w, g) satisfying constraints (8.2)-(8.7), and deﬁne H≤ as the set of points satisfying
the inequality version of (8.2)-(8.7). In the next section we discuss eﬀective implementations of

problem (8).

4. Generating Strong Cuts on the Fly

Formulation (8) contains an exponential number of inequalities (8.2), and needs to be implemented

using row generation, wherein constraints (8.2) are initially dropped and added as cuts on the ﬂy

during optimization. Row generation can be implemented in modern MIP optimization solvers via

callbacks, by adding lazy constraints at relevant nodes of the branch-and-bound tree. Identifying

which constraint (8.2) to add can in general be done by solving a minimum cut problem, and could

in principle be solved via well-known algorithms, such as Goldberg and Tarjan (1988) and Hochbaum

(2008).

12

Row generation methods for integer programs may require a long time to converge to an optimal

solution if each cut added is weak for the feasible region of interest, as illustrated for example by

the poor performance of the pure cutting plane algorithm of Gomory (1958). Nonetheless, cutting

planes have been extremely successful at solving integer programs when the cuts added are strong

or, ideally, “facet-deﬁning” for the convex hull of the feasible region. Formally, facet-deﬁning cuts

are those cuts which are necessary to describe the convex hull. For example, integer programming

formulations for traveling salesman problems contain an exponential number of “subtour elimination”

constraints that are added on the ﬂy as cuts. Nonetheless, all such inequalities are facet deﬁning for

the convex hull of the feasible region, see Grötschel et al. (1985), and cutting plane methods are able

to ﬁnd provably optimal tours to problems with tens of thousands of variables or more Applegate

et al. (2009). Unfortunately, as illustrated by the following example, several of the inequalities (8.2)

may actually be weak for conv(H=) and conv(H≤), where conv(H) denotes the convex hull of H.

Example 1. Consider an instance of Problem (8) with a depth d = 1 decision-tree (i.e., N = {1}

and L = {2, 3}) and a dataset involving a single feature (F = {1}). Consider datapoint i such that
1 = 0 and yi = k. Suppose that the solution to the master problem is such that we branch on (the
xi
unique) feature at node 1 and predict class k0 6= k at node 2. Then, datapoint i is routed left at node

1 and is misclassiﬁed. A valid min-cut for the resulting graph includes all arcs incoming into the

sink,

i.e., qn,n0 = 1 iﬀ n0 = t. The associated cut (8.2) reads

gi ≤ w1k + w2k + w3k.

(9)

Intuitively, (9) states that datapoint i can be correctly classiﬁed if its class label is assigned to at least

one node, and is valid for conv(H=) and conv(H≤). However, since datapoint i cannot be routed to
node 3, the stronger inequality

is valid for conv(H=), conv(H≤) and dominates (9).

gi ≤ w1k + w2k

(10)

(cid:4)

Therefore, an implementation of formulation (8) using general purpose min-cut algorithms to

identify constraints to add may perform poorly. This motivates us to develop a tailored algorithm

that exploits the structure of the graph induced by capacities c(b, w). As we will show, our algorithm

exhibits substantially faster runtimes than general purpose min-cut methods and returns inequalities

that are never dominated, resulting in faster convergence of the Benders’ decomposition approach.

Algorithm 1 shows the proposed procedure, which can be called at integer nodes of the branch-

and-bound tree. For notational convenience, we deﬁne bn,‘(n) = bn,r(n) = 0 for leaf nodes n ∈ L.
Since at each iteration in the main loop (lines 5-23), the value of n is updated to a descendant

of n, the algorithm terminates in a most O(d) iterations, where d is the depth of the tree – since
|N ∪ L| is O(2d), the complexity is logarithmic in the size of the tree. Figure 2 illustrates graphically

13

Algorithm 1. We now prove that Algorithm 1 is indeed a valid separation algorithm.

Algorithm 1 Separation procedure
Input: (b, w, g) satisfying (8.3)-(8.7);

i ∈ I : datapoint used to generate the cut.
Output: −1 if all constraints (8.2) are satisﬁed;

values for min-cut q otherwise.

1: if gi = 0 return −1
2: Initialize q ← 0
3: Initialize n ← 1
4: Initialize S ← {s}
5: loop
6:

S ← S ∪ {n}
if ci

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

n,‘(n)(b, w) = 1 then
qn,r(n) ← 1
qn,t ← 1
n ← ‘(n)

else if ci

n,r(n)(b, w) = 1 then

qn,‘(n) ← 1
qn,t ← 1
n ← r(n)

else if ci

n,t(b, w) = 0 then

qn,‘(n) ← 1
qn,r(n) ← 1
qn,t ← 1
return q

else

return −1

. No arcs in the cut
. Current node=root
. S is the source set of the cut

. Arcs to the right are in the cut
. Arcs to the sink are in the cut
. Datapoint i is routed left

. Arcs to the left are in the cut
. Arcs to the sink are in the cut
. Datapoint i is routed right

. Arcs to the left are in the cut
. Arcs to the right are in the cut
. Datapoint i is misclassiﬁed

. ci

n,t(b, w) = 1 in this case
. i is correctly classiﬁed

end if
22:
23: end loop

Proposition 2. Given i ∈ I and (b, w, g) satisfying (8.3)-(8.7), Algorithm 1 either ﬁnds a violated

inequality (8.2) or proves that all such inequalities are satisﬁed.

Proof. Note that the right hand side of (8.2), which corresponds to the capacity of a cut in the
graph, is nonnegative. Therefore, if gi = 0 (line 1), all inequalities are automatically satisﬁed. Since

(b, w) is integer, all arc capacities in formulations (6) and (7) are either 0 or 1. Moreover, since
gi ≤ 1, we ﬁnd that either the value of a minimum cut is 0 and there exists a violated inequality, or

the value of a minimum cut is at least 1 and there is no violated inequality. Finally, there exists a

0-capacity cut if and only if s and t belong to diﬀerent connected components in the graph induced

by c(b, w).

The component connected to s can be found using depth-ﬁrst search. For any ﬁxed n ∈ N ∪ L,
constraints (8.3)-(8.4) and the deﬁnition of c(b, w) imply that at most one arc (n, n0) outgoing
from n can have capacity 1. If arc (n, n0) has capacity 1 and n0 6= t (lines 7-14), then n0 can be

14

Figure 2: Pictorial description of Algorithm 1. Green arcs (n, n0) have capacity ci
n,n0(b, w) = 1 (and
others capacity 0). Red arcs are those in the minimum cut. On the left, examples with minimum cut
value equal to 1 (constraint (8.2) is satisﬁed). On the right, examples with minimum cut values of 0
(new constraints added).

added to the component connected to s (set S) and all other outgoing arcs from n (which have

capacity of 0) can be added to the min-cut (at zero cost). If all outgoing arcs from n have capacity

0, they can be added to the min-cut. In that case, the connected components to s end at node n.

If the unique outgoing arc from node n that has capacity 1 is (n, t), then s and t are in the same

connected component and the value of the minimum cut is at least 1. Therefore, the connected

component S to s corresponds to a path from s to a node n where no branching is performed: if
ci
n,t = 1 then t is also in this connected component and no cut is added (line 21): otherwise, a
(cid:4)
violated cut has been found (line 19).

In addition to providing a very fast method for generating cuts at integer nodes of a branch-and-

bound tree, Algorithm 1 is also guaranteed to generate facet-deﬁning cuts of conv(H≤). Such cuts
are never dominated.

Theorem 1. All violated inequalities found by Algorithm 1 are facet-deﬁning for conv(H≤).

We defer the proof of Theorem 1 to the supplemental material A.

Example 2 (Example 1 Continued). In the instance considered in Example 1, if b1f = 1 and w2k = 0,
then the cut generated by the algorithm (q1,r(1) = q1,t=q2,t = 1) is precisely (10). If b1f = 0 and

15

s<latexit sha1_base64="t5Z8j6uw1dDrul9BiPoeucvIxm0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipqQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6perVpr1ir12zyOIpzBOVyCB9dQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A4OuM/g==</latexit>1<latexit sha1_base64="RSOBfsC0h9n0lPM3VbiR0pceZig=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip6Q3KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W9WrXWrFXqt3kcRTiDc7gED66hDvfQgBYwQHiGV3hzHp0X5935WLYWnHzmFP7A+fwBfOOMvA==</latexit>2<latexit sha1_base64="XW0qcPrccSfI7onjbbF2xR2gGy4=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4Kkkp6LHoxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzUrA5KZbfiLkDWiZeTMuRoDEpf/WHM0gilYYJq3fPcxPgZVYYzgbNiP9WYUDahI+xZKmmE2s8Wh87IpVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNkUbgrf68jppVyterVJr1sr12zyOApzDBVyBB9dQh3toQAsYIDzDK7w5j86L8+58LFs3nHzmDP7A+fwBfmeMvQ==</latexit>3<latexit sha1_base64="LmvAoiNgRcYEisiHXakVSuvpF8g=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokW9Fj04rEF+wFtKJvtpF272YTdjVBCf4EXD4p49Sd589+4bXPQ1gcDj/dmmJkXJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJssFrHqBFSj4BKbhhuBnUQhjQKB7WB8N/PbT6g0j+WDmSToR3QoecgZNVZqXPVLZbfizkFWiZeTMuSo90tfvUHM0gilYYJq3fXcxPgZVYYzgdNiL9WYUDamQ+xaKmmE2s/mh07JuVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNkUbgrf88ippXVa8aqXaqJZrt3kcBTiFM7gAD66hBvdQhyYwQHiGV3hzHp0X5935WLSuOfnMCfyB8/kDf+uMvg==</latexit>4<latexit sha1_base64="YyGWf7FHWY1YzP9SDwB8wuM+bzs=">AAAB5HicbVBNS8NAEJ3Urxq/qlcvi0XwVBIp2GPRi8cK9gPaUDbbSbt2swm7G6GE/gIvHhSv/iZv/hu3bQ7a+mDg8d4MM/PCVHBtPO/bKW1t7+zulffdg8Oj45OKe9rRSaYYtlkiEtULqUbBJbYNNwJ7qUIahwK74fRu4XefUWmeyEczSzGI6VjyiDNqrPRQH1aqXs1bgmwSvyBVKNAaVr4Go4RlMUrDBNW673upCXKqDGcC5+4g05hSNqVj7FsqaYw6yJeHzsmlVUYkSpQtachS/T2R01jrWRzazpiaiV73FuJ/Xj8zUSPIuUwzg5KtFkWZICYhi6/JiCtkRswsoUxxeythE6ooMzYb14bgr7+8STrXNb9eq1ebt0UYZTiHC7gCH26gCffQgjYwQHiBN3h3npxX52PVWHKKiTP4A+fzBxb9i5U=</latexit>5<latexit sha1_base64="RTnT9GACZjSEkfi58lGJCASK3Us=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOl5mW/XHGr7hxklXg5qUCORr/81RvELI1QGiao1l3PTYyfUWU4Ezgt9VKNCWVjOsSupZJGqP1sfuiUnFllQMJY2ZKGzNXfExmNtJ5Ege2MqBnpZW8m/ud1UxNe+xmXSWpQssWiMBXExGT2NRlwhcyIiSWUKW5vJWxEFWXGZlOyIXjLL6+S9kXVq1VrzVqlfpPHUYQTOIVz8OAK6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8fgvOMwA==</latexit>6<latexit sha1_base64="TxpuHov6zkv80qcfRPVyYFgoy0A=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOl5mW/XHGr7hxklXg5qUCORr/81RvELI1QGiao1l3PTYyfUWU4Ezgt9VKNCWVjOsSupZJGqP1sfuiUnFllQMJY2ZKGzNXfExmNtJ5Ege2MqBnpZW8m/ud1UxNe+xmXSWpQssWiMBXExGT2NRlwhcyIiSWUKW5vJWxEFWXGZlOyIXjLL6+S9kXVq1VrzVqlfpPHUYQTOIVz8OAK6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8fhHeMwQ==</latexit>7<latexit sha1_base64="taWcOcC06x3gohqXrSs+/h1ZC7M=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokU6rHoxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzUrA1KZbfiLkDWiZeTMuRoDEpf/WHM0gilYYJq3fPcxPgZVYYzgbNiP9WYUDahI+xZKmmE2s8Wh87IpVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNkUbgrf68jppX1e8aqXarJbrt3kcBTiHC7gCD2pQh3toQAsYIDzDK7w5j86L8+58LFs3nHzmDP7A+fwBhfuMwg==</latexit>t<latexit sha1_base64="oHGXHZo5G8TdNmPNtJfxRQRlOrM=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsN+3azSbsToQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNM7uZ+54lrI2L1gNOE+xEdKREKRtFKTRyUK27VXYCsEy8nFcjRGJS/+sOYpRFXyCQ1pue5CfoZ1SiY5LNSPzU8oWxCR7xnqaIRN362OHRGLqwyJGGsbSkkC/X3REYjY6ZRYDsjimOz6s3F/7xeiuGNnwmVpMgVWy4KU0kwJvOvyVBozlBOLaFMC3srYWOqKUObTcmG4K2+vE7aV1WvVq01a5X6bR5HEc7gHC7Bg2uowz00oAUMODzDK7w5j86L8+58LFsLTj5zCn/gfP4A4m+M/w==</latexit>(Datapoint1)<latexit sha1_base64="dC9ZZ4TOzYj0VK8PArae4MKZ1us=">AAAB/XicbVDLSgNBEJz1GeNrfdy8DAYhXsKuBPQY1IPHCOYByRJmJ5NkyOzMMtMrxiX4K148KOLV//Dm3zhJ9qCJBQ1FVTfdXWEsuAHP+3aWlldW19ZzG/nNre2dXXdvv25UoimrUSWUbobEMMElqwEHwZqxZiQKBWuEw6uJ37hn2nAl72AUsyAifcl7nBKwUsc9bAN7gLR4TYDEikvA/um44xa8kjcFXiR+RgooQ7XjfrW7iiYRk0AFMablezEEKdHAqWDjfDsxLCZ0SPqsZakkETNBOr1+jE+s0sU9pW3Z/VP190RKImNGUWg7IwIDM+9NxP+8VgK9iyDlMk6ASTpb1EsEBoUnUeAu14yCGFlCqOb2VkwHRBMKNrC8DcGff3mR1M9KfrlUvi0XKpdZHDl0hI5REfnoHFXQDaqiGqLoET2jV/TmPDkvzrvzMWtdcrKZA/QHzucP3+qU2Q==</latexit>(Datapoint2)<latexit sha1_base64="Z5BjckWynNKf8z2AAQ/qhJdMwx4=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5CRahbkpSCros6sJlBfuANpTJdNIOncyEmRuxhuKvuHGhiFv/w51/47TNQlsPXDiccy/33hPEnGlw3W8rt7K6tr6R3yxsbe/s7tn7B00tE0Vog0guVTvAmnImaAMYcNqOFcVRwGkrGF1N/dY9VZpJcQfjmPoRHggWMoLBSD37qAv0AdLSNQYcSybAqZxNenbRLbszOMvEy0gRZaj37K9uX5IkogIIx1p3PDcGP8UKGOF0UugmmsaYjPCAdgwVOKLaT2fXT5xTo/SdUCpTZv9M/T2R4kjrcRSYzgjDUC96U/E/r5NAeOGnTMQJUEHmi8KEOyCdaRROnylKgI8NwUQxc6tDhlhhAiawggnBW3x5mTQrZa9art5Wi7XLLI48OkYnqIQ8dI5q6AbVUQMR9Iie0St6s56sF+vd+pi35qxs5hD9gfX5A+FwlNo=</latexit>(Datapoint3)<latexit sha1_base64="1hnlI4dfArrssEkq19rAhnR/x98=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSxC3ZREC7os6sJlBfuANpTJdNIOnWTCzI1YQ/FX3LhQxK3/4c6/cdpmoa0HLhzOuZd77/FjwTU4zreVW1peWV3Lrxc2Nre2d+zdvYaWiaKsTqWQquUTzQSPWB04CNaKFSOhL1jTH15N/OY9U5rL6A5GMfNC0o94wCkBI3Xtgw6wB0hL1wRILHkE+Oxk3LWLTtmZAi8SNyNFlKHWtb86PUmTkEVABdG67ToxeClRwKlg40In0SwmdEj6rG1oREKmvXR6/RgfG6WHA6lMmf1T9fdESkKtR6FvOkMCAz3vTcT/vHYCwYWX8ihOgEV0tihIBAaJJ1HgHleMghgZQqji5lZMB0QRCiawggnBnX95kTROy26lXLmtFKuXWRx5dIiOUAm56BxV0Q2qoTqi6BE9o1f0Zj1ZL9a79TFrzVnZzD76A+vzB+L2lNs=</latexit>(Datapoint4)<latexit sha1_base64="AdtNK+sTLmmXsy6NV/yDbSGHavM=">AAAB/XicbVDLSsNAFJ34rPUVHzs3wSLUTUkkoMuiLlxWsA9oQ5lMJ+3QyUyYuRFrKP6KGxeKuPU/3Pk3TtsstPXAhcM593LvPWHCmQbX/baWlldW19YLG8XNre2dXXtvv6FlqgitE8mlaoVYU84ErQMDTluJojgOOW2Gw6uJ37ynSjMp7mCU0CDGfcEiRjAYqWsfdoA+QFa+xoATyQQ4/um4a5fcijuFs0i8nJRQjlrX/ur0JEljKoBwrHXbcxMIMqyAEU7HxU6qaYLJEPdp21CBY6qDbHr92DkxSs+JpDJl9k/V3xMZjrUexaHpjDEM9Lw3Ef/z2ilEF0HGRJICFWS2KEq5A9KZROH0mKIE+MgQTBQztzpkgBUmYAIrmhC8+ZcXSeOs4vkV/9YvVS/zOAroCB2jMvLQOaqiG1RDdUTQI3pGr+jNerJerHfrY9a6ZOUzB+gPrM8f5HyU3A==</latexit>s<latexit sha1_base64="t5Z8j6uw1dDrul9BiPoeucvIxm0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipqQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6perVpr1ir12zyOIpzBOVyCB9dQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A4OuM/g==</latexit>1<latexit sha1_base64="RSOBfsC0h9n0lPM3VbiR0pceZig=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip6Q3KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W9WrXWrFXqt3kcRTiDc7gED66hDvfQgBYwQHiGV3hzHp0X5935WLYWnHzmFP7A+fwBfOOMvA==</latexit>2<latexit sha1_base64="XW0qcPrccSfI7onjbbF2xR2gGy4=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4Kkkp6LHoxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzUrA5KZbfiLkDWiZeTMuRoDEpf/WHM0gilYYJq3fPcxPgZVYYzgbNiP9WYUDahI+xZKmmE2s8Wh87IpVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNkUbgrf68jppVyterVJr1sr12zyOApzDBVyBB9dQh3toQAsYIDzDK7w5j86L8+58LFs3nHzmDP7A+fwBfmeMvQ==</latexit>3<latexit sha1_base64="LmvAoiNgRcYEisiHXakVSuvpF8g=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokW9Fj04rEF+wFtKJvtpF272YTdjVBCf4EXD4p49Sd589+4bXPQ1gcDj/dmmJkXJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJssFrHqBFSj4BKbhhuBnUQhjQKB7WB8N/PbT6g0j+WDmSToR3QoecgZNVZqXPVLZbfizkFWiZeTMuSo90tfvUHM0gilYYJq3fXcxPgZVYYzgdNiL9WYUDamQ+xaKmmE2s/mh07JuVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNkUbgrf88ippXVa8aqXaqJZrt3kcBTiFM7gAD66hBvdQhyYwQHiGV3hzHp0X5935WLSuOfnMCfyB8/kDf+uMvg==</latexit>4<latexit sha1_base64="YyGWf7FHWY1YzP9SDwB8wuM+bzs=">AAAB5HicbVBNS8NAEJ3Urxq/qlcvi0XwVBIp2GPRi8cK9gPaUDbbSbt2swm7G6GE/gIvHhSv/iZv/hu3bQ7a+mDg8d4MM/PCVHBtPO/bKW1t7+zulffdg8Oj45OKe9rRSaYYtlkiEtULqUbBJbYNNwJ7qUIahwK74fRu4XefUWmeyEczSzGI6VjyiDNqrPRQH1aqXs1bgmwSvyBVKNAaVr4Go4RlMUrDBNW673upCXKqDGcC5+4g05hSNqVj7FsqaYw6yJeHzsmlVUYkSpQtachS/T2R01jrWRzazpiaiV73FuJ/Xj8zUSPIuUwzg5KtFkWZICYhi6/JiCtkRswsoUxxeythE6ooMzYb14bgr7+8STrXNb9eq1ebt0UYZTiHC7gCH26gCffQgjYwQHiBN3h3npxX52PVWHKKiTP4A+fzBxb9i5U=</latexit>5<latexit sha1_base64="RTnT9GACZjSEkfi58lGJCASK3Us=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOl5mW/XHGr7hxklXg5qUCORr/81RvELI1QGiao1l3PTYyfUWU4Ezgt9VKNCWVjOsSupZJGqP1sfuiUnFllQMJY2ZKGzNXfExmNtJ5Ege2MqBnpZW8m/ud1UxNe+xmXSWpQssWiMBXExGT2NRlwhcyIiSWUKW5vJWxEFWXGZlOyIXjLL6+S9kXVq1VrzVqlfpPHUYQTOIVz8OAK6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8fgvOMwA==</latexit>6<latexit sha1_base64="TxpuHov6zkv80qcfRPVyYFgoy0A=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOl5mW/XHGr7hxklXg5qUCORr/81RvELI1QGiao1l3PTYyfUWU4Ezgt9VKNCWVjOsSupZJGqP1sfuiUnFllQMJY2ZKGzNXfExmNtJ5Ege2MqBnpZW8m/ud1UxNe+xmXSWpQssWiMBXExGT2NRlwhcyIiSWUKW5vJWxEFWXGZlOyIXjLL6+S9kXVq1VrzVqlfpPHUYQTOIVz8OAK6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8fhHeMwQ==</latexit>7<latexit sha1_base64="taWcOcC06x3gohqXrSs+/h1ZC7M=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokU6rHoxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzUrA1KZbfiLkDWiZeTMuRoDEpf/WHM0gilYYJq3fPcxPgZVYYzgbNiP9WYUDahI+xZKmmE2s8Wh87IpVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNkUbgrf68jppX1e8aqXarJbrt3kcBTiHC7gCD2pQh3toQAsYIDzDK7w5j86L8+58LFs3nHzmDP7A+fwBhfuMwg==</latexit>t<latexit sha1_base64="oHGXHZo5G8TdNmPNtJfxRQRlOrM=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsN+3azSbsToQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNM7uZ+54lrI2L1gNOE+xEdKREKRtFKTRyUK27VXYCsEy8nFcjRGJS/+sOYpRFXyCQ1pue5CfoZ1SiY5LNSPzU8oWxCR7xnqaIRN362OHRGLqwyJGGsbSkkC/X3REYjY6ZRYDsjimOz6s3F/7xeiuGNnwmVpMgVWy4KU0kwJvOvyVBozlBOLaFMC3srYWOqKUObTcmG4K2+vE7aV1WvVq01a5X6bR5HEc7gHC7Bg2uowz00oAUMODzDK7w5j86L8+58LFsLTj5zCn/gfP4A4m+M/w==</latexit>s<latexit sha1_base64="t5Z8j6uw1dDrul9BiPoeucvIxm0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipqQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6perVpr1ir12zyOIpzBOVyCB9dQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A4OuM/g==</latexit>1<latexit sha1_base64="RSOBfsC0h9n0lPM3VbiR0pceZig=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip6Q3KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W9WrXWrFXqt3kcRTiDc7gED66hDvfQgBYwQHiGV3hzHp0X5935WLYWnHzmFP7A+fwBfOOMvA==</latexit>2<latexit sha1_base64="XW0qcPrccSfI7onjbbF2xR2gGy4=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4Kkkp6LHoxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzUrA5KZbfiLkDWiZeTMuRoDEpf/WHM0gilYYJq3fPcxPgZVYYzgbNiP9WYUDahI+xZKmmE2s8Wh87IpVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNkUbgrf68jppVyterVJr1sr12zyOApzDBVyBB9dQh3toQAsYIDzDK7w5j86L8+58LFs3nHzmDP7A+fwBfmeMvQ==</latexit>3<latexit sha1_base64="LmvAoiNgRcYEisiHXakVSuvpF8g=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokW9Fj04rEF+wFtKJvtpF272YTdjVBCf4EXD4p49Sd589+4bXPQ1gcDj/dmmJkXJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJssFrHqBFSj4BKbhhuBnUQhjQKB7WB8N/PbT6g0j+WDmSToR3QoecgZNVZqXPVLZbfizkFWiZeTMuSo90tfvUHM0gilYYJq3fXcxPgZVYYzgdNiL9WYUDamQ+xaKmmE2s/mh07JuVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNkUbgrf88ippXVa8aqXaqJZrt3kcBTiFM7gAD66hBvdQhyYwQHiGV3hzHp0X5935WLSuOfnMCfyB8/kDf+uMvg==</latexit>4<latexit sha1_base64="YyGWf7FHWY1YzP9SDwB8wuM+bzs=">AAAB5HicbVBNS8NAEJ3Urxq/qlcvi0XwVBIp2GPRi8cK9gPaUDbbSbt2swm7G6GE/gIvHhSv/iZv/hu3bQ7a+mDg8d4MM/PCVHBtPO/bKW1t7+zulffdg8Oj45OKe9rRSaYYtlkiEtULqUbBJbYNNwJ7qUIahwK74fRu4XefUWmeyEczSzGI6VjyiDNqrPRQH1aqXs1bgmwSvyBVKNAaVr4Go4RlMUrDBNW673upCXKqDGcC5+4g05hSNqVj7FsqaYw6yJeHzsmlVUYkSpQtachS/T2R01jrWRzazpiaiV73FuJ/Xj8zUSPIuUwzg5KtFkWZICYhi6/JiCtkRswsoUxxeythE6ooMzYb14bgr7+8STrXNb9eq1ebt0UYZTiHC7gCH26gCffQgjYwQHiBN3h3npxX52PVWHKKiTP4A+fzBxb9i5U=</latexit>5<latexit sha1_base64="RTnT9GACZjSEkfi58lGJCASK3Us=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOl5mW/XHGr7hxklXg5qUCORr/81RvELI1QGiao1l3PTYyfUWU4Ezgt9VKNCWVjOsSupZJGqP1sfuiUnFllQMJY2ZKGzNXfExmNtJ5Ege2MqBnpZW8m/ud1UxNe+xmXSWpQssWiMBXExGT2NRlwhcyIiSWUKW5vJWxEFWXGZlOyIXjLL6+S9kXVq1VrzVqlfpPHUYQTOIVz8OAK6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8fgvOMwA==</latexit>6<latexit sha1_base64="TxpuHov6zkv80qcfRPVyYFgoy0A=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOl5mW/XHGr7hxklXg5qUCORr/81RvELI1QGiao1l3PTYyfUWU4Ezgt9VKNCWVjOsSupZJGqP1sfuiUnFllQMJY2ZKGzNXfExmNtJ5Ege2MqBnpZW8m/ud1UxNe+xmXSWpQssWiMBXExGT2NRlwhcyIiSWUKW5vJWxEFWXGZlOyIXjLL6+S9kXVq1VrzVqlfpPHUYQTOIVz8OAK6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8fhHeMwQ==</latexit>7<latexit sha1_base64="taWcOcC06x3gohqXrSs+/h1ZC7M=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokU6rHoxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzUrA1KZbfiLkDWiZeTMuRoDEpf/WHM0gilYYJq3fPcxPgZVYYzgbNiP9WYUDahI+xZKmmE2s8Wh87IpVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNkUbgrf68jppX1e8aqXarJbrt3kcBTiHC7gCD2pQh3toQAsYIDzDK7w5j86L8+58LFs3nHzmDP7A+fwBhfuMwg==</latexit>t<latexit sha1_base64="oHGXHZo5G8TdNmPNtJfxRQRlOrM=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsN+3azSbsToQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNM7uZ+54lrI2L1gNOE+xEdKREKRtFKTRyUK27VXYCsEy8nFcjRGJS/+sOYpRFXyCQ1pue5CfoZ1SiY5LNSPzU8oWxCR7xnqaIRN362OHRGLqwyJGGsbSkkC/X3REYjY6ZRYDsjimOz6s3F/7xeiuGNnwmVpMgVWy4KU0kwJvOvyVBozlBOLaFMC3srYWOqKUObTcmG4K2+vE7aV1WvVq01a5X6bR5HEc7gHC7Bg2uowz00oAUMODzDK7w5j86L8+58LFsLTj5zCn/gfP4A4m+M/w==</latexit>s<latexit sha1_base64="t5Z8j6uw1dDrul9BiPoeucvIxm0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipqQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6perVpr1ir12zyOIpzBOVyCB9dQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A4OuM/g==</latexit>1<latexit sha1_base64="RSOBfsC0h9n0lPM3VbiR0pceZig=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip6Q3KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W9WrXWrFXqt3kcRTiDc7gED66hDvfQgBYwQHiGV3hzHp0X5935WLYWnHzmFP7A+fwBfOOMvA==</latexit>2<latexit sha1_base64="XW0qcPrccSfI7onjbbF2xR2gGy4=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4Kkkp6LHoxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzUrA5KZbfiLkDWiZeTMuRoDEpf/WHM0gilYYJq3fPcxPgZVYYzgbNiP9WYUDahI+xZKmmE2s8Wh87IpVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNkUbgrf68jppVyterVJr1sr12zyOApzDBVyBB9dQh3toQAsYIDzDK7w5j86L8+58LFs3nHzmDP7A+fwBfmeMvQ==</latexit>3<latexit sha1_base64="LmvAoiNgRcYEisiHXakVSuvpF8g=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokW9Fj04rEF+wFtKJvtpF272YTdjVBCf4EXD4p49Sd589+4bXPQ1gcDj/dmmJkXJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJssFrHqBFSj4BKbhhuBnUQhjQKB7WB8N/PbT6g0j+WDmSToR3QoecgZNVZqXPVLZbfizkFWiZeTMuSo90tfvUHM0gilYYJq3fXcxPgZVYYzgdNiL9WYUDamQ+xaKmmE2s/mh07JuVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNkUbgrf88ippXVa8aqXaqJZrt3kcBTiFM7gAD66hBvdQhyYwQHiGV3hzHp0X5935WLSuOfnMCfyB8/kDf+uMvg==</latexit>4<latexit sha1_base64="YyGWf7FHWY1YzP9SDwB8wuM+bzs=">AAAB5HicbVBNS8NAEJ3Urxq/qlcvi0XwVBIp2GPRi8cK9gPaUDbbSbt2swm7G6GE/gIvHhSv/iZv/hu3bQ7a+mDg8d4MM/PCVHBtPO/bKW1t7+zulffdg8Oj45OKe9rRSaYYtlkiEtULqUbBJbYNNwJ7qUIahwK74fRu4XefUWmeyEczSzGI6VjyiDNqrPRQH1aqXs1bgmwSvyBVKNAaVr4Go4RlMUrDBNW673upCXKqDGcC5+4g05hSNqVj7FsqaYw6yJeHzsmlVUYkSpQtachS/T2R01jrWRzazpiaiV73FuJ/Xj8zUSPIuUwzg5KtFkWZICYhi6/JiCtkRswsoUxxeythE6ooMzYb14bgr7+8STrXNb9eq1ebt0UYZTiHC7gCH26gCffQgjYwQHiBN3h3npxX52PVWHKKiTP4A+fzBxb9i5U=</latexit>5<latexit sha1_base64="RTnT9GACZjSEkfi58lGJCASK3Us=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOl5mW/XHGr7hxklXg5qUCORr/81RvELI1QGiao1l3PTYyfUWU4Ezgt9VKNCWVjOsSupZJGqP1sfuiUnFllQMJY2ZKGzNXfExmNtJ5Ege2MqBnpZW8m/ud1UxNe+xmXSWpQssWiMBXExGT2NRlwhcyIiSWUKW5vJWxEFWXGZlOyIXjLL6+S9kXVq1VrzVqlfpPHUYQTOIVz8OAK6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8fgvOMwA==</latexit>6<latexit sha1_base64="TxpuHov6zkv80qcfRPVyYFgoy0A=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOl5mW/XHGr7hxklXg5qUCORr/81RvELI1QGiao1l3PTYyfUWU4Ezgt9VKNCWVjOsSupZJGqP1sfuiUnFllQMJY2ZKGzNXfExmNtJ5Ege2MqBnpZW8m/ud1UxNe+xmXSWpQssWiMBXExGT2NRlwhcyIiSWUKW5vJWxEFWXGZlOyIXjLL6+S9kXVq1VrzVqlfpPHUYQTOIVz8OAK6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8fhHeMwQ==</latexit>7<latexit sha1_base64="taWcOcC06x3gohqXrSs+/h1ZC7M=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokU6rHoxWML9gPaUDbbSbt2swm7G6GE/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHM03Qj+hI8pAzaqzUrA1KZbfiLkDWiZeTMuRoDEpf/WHM0gilYYJq3fPcxPgZVYYzgbNiP9WYUDahI+xZKmmE2s8Wh87IpVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNkUbgrf68jppX1e8aqXarJbrt3kcBTiHC7gCD2pQh3toQAsYIDzDK7w5j86L8+58LFs3nHzmDP7A+fwBhfuMwg==</latexit>t<latexit sha1_base64="oHGXHZo5G8TdNmPNtJfxRQRlOrM=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48t2A9oQ9lsN+3azSbsToQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNM7uZ+54lrI2L1gNOE+xEdKREKRtFKTRyUK27VXYCsEy8nFcjRGJS/+sOYpRFXyCQ1pue5CfoZ1SiY5LNSPzU8oWxCR7xnqaIRN362OHRGLqwyJGGsbSkkC/X3REYjY6ZRYDsjimOz6s3F/7xeiuGNnwmVpMgVWy4KU0kwJvOvyVBozlBOLaFMC3srYWOqKUObTcmG4K2+vE7aV1WvVq01a5X6bR5HEc7gHC7Bg2uowz00oAUMODzDK7w5j86L8+58LFsLTj5zCn/gfP4A4m+M/w==</latexit>w1k = 0 (which is feasible in conv(H≤)) in the solution to the master problem used to generate the
cut, then the cut returned by Algorithm 1 (q1,‘(1) = q1,t = 1) is

For all other possible values of (b, w), Algorithm 1 does not ﬁnd a violated cut.

gi ≤ w1k + b1f .

5. Experiments

Approaches and Datasets. We evaluate our two approaches on eight publicly available datasets.

The number of rows (I), number of one-hot encoded features (F), and number of class labels (K)

for each dataset are given in Table 1. We compare the ﬂow-based formulation (FlowOCT) and its

Benders’ decomposition (Benders) against the formulations proposed by Bertsimas and Dunn (2017)

(OCT) and Verwer and Zhang (2019) (BinOCT). As the code used for OCT is not publicly available,

we implemented the corresponding formulation (adapted for the case of binary data). The details of

this implementation are given in the online companion B.

Table 1: Datasets used in the experiments.

|I|
Dataset
122
monk3
124
monk1
169
monk2
232
house-votes-84
625
balance-scale
958
tic-tac-toe
car_evaluation 1728
3196
kr-vs-kp

|F|
15
15
15
16
20
27
20
38

|K|
2
2
2
2
3
2
4
2

Experimental Setup. Each dataset is split into three parts: the training set (50%), the validation

set (25%), and the testing set (25%). The training and validation sets are used to tune the value

of the hyperparameter λ. We repeat this process 5 times with 5 diﬀerent samples. We test values

of λ = 0.1j for j = 0, . . . , 9. Finally, we use the best λ to train a tree using the training and

evaluation sets from the previous step, which we then evaluate against the testing set to determine

the out-of-sample accuracy. All approaches are implemented in Python programming language and

solved by the Gurobi 8.1 solver. All problems are solved on a single core of SL250s Xeon CPUs by

HPE and 4gb of memory with a one hour time limit.

16

(a) Performance proﬁle with λ = 0.

(b) Performance proﬁle with λ > 0.

(c) Optimality gaps as a function of the size= 2d × |I|.

Figure 3: Summary of optimization performance.

In-Sample (Optimization) Performance. Figure 3 summarizes the in-sample performance, i.e.,

how good the methods are at solving the optimization problems. Detailed results are provided in

the online companion B. From Figure 3(a), we observe that for λ = 0, BinOCT is able to solve 79

instances within the time limit (and outperforms OCT), but Benders solves the same quantity of

17

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll02550751000100020003000Time (s)Number of Instances SolvedApproachlBendersFlowOCTBinOCTOCTlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll02505007500100020003000Time (s)Number of Instances SolvedApproachlBendersFlowOCTOCTllllllllllllllllllllllllllllllll0255075100103104SizeOptimality Gap (%)ApproachlBendersFlowOCTBinOCTOCTinstances in only 140 seconds, resulting in a 30× speedup. Similarly, from Figure 3(b), it can be seen
that for λ > 0, OCT is able to solve 666 instances within the time limit1, while Benders requires
only 70 seconds to do so, resulting in a 50× speedup. Finally, Figure 3(c) shows the optimality

gaps proven as a function of the dimension. We observe that all methods result in a gap of 0%

in small instances. As the dimension increases, BinOCT (which relies on weak formulations but

fast enumeration) yields 100% optimality gaps in most cases. OCT and BinOCT prove better gaps,

but the performance degrades substantially as the dimension increases. Benders results in the best

performance, proving optimality gaps of 20% or less regardless of dimension.

Out-of-Sample (Statistical) Performance. Table 2 reports the out-of-sample accuracy after cross-

validation. Each row represents the average over the ﬁve samples. We observe that the better

optimization performance translates to superior statistical properties as well: OCT is the best method

in two instances (excluding ties), BinOCT in six, while the new formulations FlowOCT and Benders

are better in 13 (of which Benders accounts for 10, and is second after FlowOCT in an additional

two).

1BinOCT does not include the option to have a regularization parameter, and is omitted.

18

Table 2: Out of sample accuracy

Dataset
monk3
monk3
monk3
monk3
monk1
monk1
monk1
monk1
monk2
monk2
monk2
monk2
house-votes-84
house-votes-84
house-votes-84
house-votes-84
balance-scale
balance-scale
balance-scale
balance-scale
tic-tac-toe
tic-tac-toe
tic-tac-toe
tic-tac-toe
car_evaluation
car_evaluation
car_evaluation
car_evaluation
kr-vs-kp
kr-vs-kp
kr-vs-kp
kr-vs-kp

Depth OCT BinOCT FlowOCT Benders

2
3
4
5
2
3
4
5
2
3
4
5
2
3
4
5
2
3
4
5
2
3
4
5
2
3
4
5
2
3
4
5

92.3
83.2
91
87.1
71
83.2
100
93.5
56.7
62.3
59.5
63.3
79.3
97.2
96.9
95.2
68.7
69
68.5
65.7
66.7
68.1
70.4
69.7
76.5
73.3
75.2
74.8
73.7
69.3
64.7
62.7

92.3
91
85.2
87.7
72.3
82.6
99.4
96.8
49.8
58.1
60.5
55.8
96.2
94.1
94.8
93.1
67.9
71.5
73.9
75.3
65.9
72.2
80.3
78.9
76.5
78.4
80.3
81.3
87.2
87.8
90.8
87.1

92.3
91
92.3
92.3
72.3
81.3
100
100
56.7
63.7
58.6
62.3
97.2
97.2
96.9
96.9
68.7
69.8
73.2
71.6
66.7
68.5
68.7
66.3
76.5
76.7
71.6
61.6
70.5
61.2
54.3
45.8

92.3
91
91
91.6
71
81.3
100
100
56.7
63.3
64.2
61.9
97.2
97.2
95.5
97.2
68.7
71
71.7
76.8
66.7
72.6
77.1
79.3
76.5
79.1
79.7
80.5
87.2
89.9
91
86.7

19

References

Sina Aghaei, Mohammad Javad Azizi, and Phebe Vayanos. Learning optimal and fair decision trees for

non-discriminative decision-making. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,

volume 33, pages 1418–1426, 2019.

David L Applegate, Robert E Bixby, Vašek Chvátal, William Cook, Daniel G Espinoza, Marcos Goycoolea,

and Keld Helsgaun. Certiﬁcation of an optimal tsp tour through 85,900 cities. Operations Research

Letters, 37(1):11–15, 2009.

Mohammad Javad Azizi, Phebe Vayanos, Bryan Wilder, Eric Rice, and Milind Tambe. Designing fair,

eﬃcient, and interpretable policies for prioritizing homeless youth for housing resources. In International

Conference on the Integration of Constraint Programming, Artiﬁcial Intelligence, and Operations

Research, pages 35–51. Springer, 2018.

Jacques F Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische

mathematik, 4(1):238–252, 1962.

Dimitris Bertsimas and Jack Dunn. Optimal classiﬁcation trees. Machine Learning, 106(7):1039–1082, 2017.

Dimitris Bertsimas and Bartolomeo Stellato. The voice of optimization. arXiv preprint arXiv:1812.09991,

2018.

Robert E Bixby. A brief history of linear and mixed-integer programming computation. Documenta

Mathematica, pages 107–121, 2012.

Leo Breiman. Classiﬁcation and regression trees. Technical report, 1984.

Leo Breiman. Classiﬁcation and regression trees. Routledge, 2017.

Dragos Ciocan and Velibor Miši´c. Interpretable optimal stopping. 2018.

IBM ILOG CPLEX. V12. 1: UserâŁ™s manual for cplex. International Business Machines Corporation, 46

(53):157, 2009.

Andrew V Goldberg and Robert E Tarjan. A new approach to the maximum-ﬂow problem. Journal of the

ACM (JACM), 35(4):921–940, 1988.

Ralph E Gomory. Outline of an algorithm for integer solutions to linear programs. Bulletin of the American

Mathematical Society, 64(5):275–278, 1958.

Martin Grötschel, Manfred W Padberg, et al. Polyhedral theory. The traveling salesman problem, pages

251–305, 1985.

Oktay Günlük, Jayant Kalagnanam, Matt Menickelly, and Katya Scheinberg. Optimal decision trees for

categorical data via integer programming. arXiv preprint arXiv:1612.03225, 2018.

Incorporate Gurobi Optimization. Gurobi optimizer reference manual. URL http://www. gurobi. com, 2015.

Dorit S Hochbaum. The pseudoﬂow algorithm: A new algorithm for the maximum-ﬂow problem. Operations

research, 56(4):992–1009, 2008.

Xiyang Hu, Cynthia Rudin, and Margo Seltzer. Optimal sparse decision trees. arXiv preprint arXiv:1904.12847,

2019.

Laurent Hyaﬁl and Ronald L Rivest. Constructing optimal binary search trees is NP complete. Information

Processing Letters, 1976.

20

Max Kuhn, Steve Weston, Mark Culp, Nathan Coulter, and Ross Quinlan. Package âŁ˜c50âŁ™, 2018.

Andy Liaw and Matthew Wiener. Classiﬁcation and regression by randomforest. R news, 2(3):18–22, 2002.

Velibor V Mišic. Optimization of tree ensembles. arXiv preprint arXiv:1705.10883, 2017.

Nina Narodytska, Alexey Ignatiev, Filipe Pereira, and Joao Marques-Silva. Learning optimal decision trees

with SAT. In IJCAI, pages 1362–1368, 2018.

George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for

maximizing submodular set functionsâŁ”i. Mathematical programming, 14(1):265–294, 1978.

Siegfried Nijssen and Elisa Fromont. Optimal constraint-based decision tree induction from itemset lattices.

Data Mining and Knowledge Discovery, 21(1):9–51, 2010.

John Ross Quinlan. Induction of decision trees. Machine learning, 1(1):81–106, 1986.

John Ross Quinlan. C4. 5: programs for machine learning. Elsevier, 2014.

Terry Therneau, Beth Atkinson, Brian Ripley, and Maintainer Brian Ripley. Package âŁ˜rpartâŁ™. 2015.

Hélene Verhaeghe, Siegfried Nijssen, Gilles Pesant, Claude-Guy Quimper, and Pierre Schaus. Learning optimal

decision trees using constraint programming. In The 25th International Conference on Principles and

Practice of Constraint Programming (CP2019), 2019.

Sicco Verwer and Yingqian Zhang. Learning decision trees with ﬂexible constraints and objectives using

integer optimization. In International Conference on AI and OR Techniques in Constraint Programming

for Combinatorial Optimization Problems, pages 94–103. Springer, 2017.

Sicco Verwer and Yingqian Zhang. Learning optimal classiﬁcation trees using a binary linear program

formulation. In 33rd AAAI Conference on Artiﬁcial Intelligence, 2019.

21

A. Proof of Theorem 1

The proof proceeds in three steps. We ﬁx i ∈ I. We derive the speciﬁc structure (p, q) ∈ P of the

cuts associated with datapoint i generated by our procedure. We then provide |N × F| + |L × K| + |I|

points that lie in conv(H≤) and at each of which the cut generated holds with equality. Since the
choice of i ∈ I is arbitrary and since the cuts generated by our procedure are valid (by construction),

this will conclude the proof.

To minimize notational overhead, we assume throughout this proof that λ = 0. In this case, an

optimal solution to the master problem where P
k∈K wnk = 0 for all n ∈ N can always be obtained.
Given a set A and a point a ∈ A, we use A \ a as a shorthand for A \ {a}. Finally, we let eij = 1
be a vector (whose dimensions will be clear from the context) with a 1 in coordinates (i, j) and 0

elsewhere.

Fix i ∈ I. Let (¯b, ¯w, ¯g) be optimal in the (relaxed) master problem and assume P

k∈K ¯wnk = 0
for all n ∈ N . Given j ∈ I, let n(j) ∈ L be the leaf of the tree deﬁned by (¯b, ¯w) that datapoint j is
assigned to. Given n ∈ N , let f (n) ∈ F be the feature selected for testing at node n under (¯b, ¯w),
i.e., ¯bnf (n) = 1.

We now derive the structure of the cuts (8.2) generated by Algorithm 1 (see also the proof of
Proposition 2) when (¯b, ¯w, ¯g) is input. A minimum cut is returned by Algorithm 1 if and only if s
and t belong to diﬀerent connected components in the tree induced by (¯b, ¯w). Under this assumption,
since P
k∈K ¯wnk = 0 for all n ∈ N , the connected component S constructed in Algorithm 1 forms
a path from s to nd = n(i) ∈ L, i.e., S = {s, n1, n2, . . . , nd}. The minimum cut q obtained from
Algorithm 1 then corresponds to the arcs adjacent to nodes in S that do not belong to the path

formed by S. Therefore, qs,1 = 0, qn,t = 1 iﬀ n = n(i), and for each n ∈ N ,

qn,‘(n) = 1 iﬀ n ∈ p and X
f ∈F :xi

f =1

¯bnf = 1, and

qn,r(n) = 1 iﬀ n ∈ p and X
f ∈F :xi

f =0

¯bnf = 1.

Therefore, the cut (8.2) returned by Algorithm 1 reads

gi ≤ wn(i)yi + X

X

bnf .

n∈S

f ∈F :
xi
f 6=xi

f (n)

(11)

Next, we give |N × F| + |L × K| + |I| aﬃnely independent points in H≤ for which (11) holds
with equality. Given a vector b ∈ {0, 1}|N |×|F|, we let bS (resp. bN \S) collect those elements of b
whose ﬁrst index n ∈ S (resp. n /∈ S). We now describe the points, which are also summarized in

22

Table 3.

Table 3: Companion table for the proof of Theorem 1: list of aﬃnely independent points that live on
the cut generated by inputing i ∈ I and (¯b, ¯w, ¯g) in Algorithm 1.

# condition

dim=
sol=

|S| × |F|
bS

|N \ S| × |F|
bN \S

|L| × |K|
w

1

2
3
4

5

6
7

8

“baseline” point

n ∈ L, k ∈ K \ yi
n ∈ L \ n(i)
n = n(i)

n ∈ N \ S, f ∈ F

n ∈ S
n ∈ S, f ∈ F : f 6= f (n), xi
n ∈ S, f ∈ F : f 6= f (n), xi

f = xi
f 6= xi

f (n)

f (n)

¯bS

¯bS
¯bS
¯bS

¯bS

¯bS − enf (n)
¯bS − enf (n) + enf
¯bS − enf (n) + enf

9
10
11

j ∈ I \ i : yj 6= yi
j ∈ I \ i : yj = yi, n(j) 6= n(i)
j ∈ I \ i : yj = yi, n(j) = n(i)

¯bS
¯bS
¯bS

0

0
0
0

enf

0
0

0

¯bN \S
¯bN \S
¯bN \S

|I|
g

0

0
0
ei

0

0
0

ei

0

enk
enyi
en(i)yi

0

0
0
X

n∈L:n6=n(i)

enyi

en(j)yj
en(j)yj
en(i)yi

ej
ej
ei + ej

1 One point that is a “baseline” point; all other points are variants of it. It is given by bS = ¯bS,
bN \S = 0, w = 0 and g = 0 and corresponds to selecting the features to branch on according
to ¯b for nodes in S and setting all remaining variables to 0. The baseline point belongs to H≤
and constraint (11) is active at this point.

2-4 |L| × |K| points obtained from the baseline point by varying the w coordinates and adjusting

g as necessary to ensure (11) remains active: 2: |L| × (|K| − 1) points, each associated with
a leaf n ∈ L and class k ∈ K : k 6= yi, where the label of leaf n is changed to k. 3: |L| − 1
points, each associated with a leaf n ∈ L : n 6= n(i), where the class label of n is changed to yi.
4: One point where the class label of leaf n(i) is set to yi, allowing for correct classiﬁcation
of datapoint i; in this case, the value of the rhs of (11) is 1, and we set gi = 1 to ensure the

cut (11) remains active.

5

|N \ S| × |F| points obtained from the baseline point by varying the bN \S coordinates. Each
point is associated with a node n ∈ N \S and feature f ∈ F and is obtained by changing the

decision to branch on feature f and node n to 1. As those branching decisions do not impact

the routing of datapoint i the value of the rhs of inequality (11) remains unchanged and the

inequality stays active.

23

6-8 |S| × |F| points, obtained from the baseline point by varying the bS coordinates and adjusting w
and g as necessary to guarantee feasibility of the resulting point and to ensure that (11) stays

active. 6: |S| points, each associated with a node n ∈ S obtained by not branching on feature

f (n) at node n (nor on any other feature), resulting in a “dead-end” node. The value of the

rhs of (11) is unchanged in this case and the inequality remains active. 7-8: |S| points, each

associated with a node n ∈ S and feature f 6= f (n). 7: If the branching decision f (n) at

f = xi

node n is replaced with a branching decision that results in the same path for datapoint i, i.e.,
if xi
f (n), it is possible to swap those decisions without aﬀecting the value of the rhs in
inequality (11). 8: If a feature that causes i to change paths is chosen for branching, i.e., if
f (n), then the value of the rhs of (11) is increased by 1, and we set gi = 1 to ensure the
f 6= xi
xi
inequality remains active; to guarantee feasibility of the resulting point, we label each leaf
node except for n(i) with the class yi, which does not aﬀect inequality (11).

9-11 |I| − 1 points, obtained from the baseline point by letting bN \S = ¯bN \S and adjusting w and g
as necessary. Each point is associated with a datapoint j ∈ I \ i which we allow to be correctly
classiﬁed. 9: If datapoint j has a diﬀerent class than datapoint i (yj 6= yi), we label the leaf

node where j is routed to with the class of j, i.e., wn(j)yj = 1. The value of the rhs of (11) is
unaﬀected the inequality remains active. 10: If datapoint j has the same class as datapoint i

but is routed to a diﬀerent leaf than i, an argument paralleling that in 9 can be made. 11: If

datapoint j has the same class as datapoint i and is routed to the same leaf n(i), we label
n(i) with the class of yi = yj and set gj = 1; the value of the rhs of (11) increases by 1. Thus,
we set also correctly classify datapoint i by setting gi = 1 to ensure that (11) is active.

The |N × F| + |L × K| + |I| points constructed above, see also Table 3, are aﬃnely independent.

Indeed, each diﬀers from the previously introduced points in at least one coordinate. All these

points also belong to H≤. This concludes the proof.

B. OCT

In this section, we provide a simpliﬁed version of the formulation of Bertsimas and Dunn (2017)

specialized to the case of binary data.

We start with introducing the notation that is used for the formulation. Let N and L denote

the sets of all internal and leaf nodes in the tree structure. For each node n ∈ N ∪ L\{1}, a(n)

refers to the direct ancestor of node n. AL(n) is the set of ancestors of n whose left branch has

been followed on the path from the root node to n, and similarly AR(n) is the set of right-branch

ancestors, such that A(n) = AL(n) ∪ AR(n)

Let bnf be a binary decision variable where bnf = 1 iﬀ at node n, feature f is branched upon. For
f < vn is performed where vn ∈ R is a decision

each datapoint i at node n ∈ N a test P

f ∈F bnf xi

24

Figure 4: A classiﬁcation tree of depth 2

variable represeinting the cut-oﬀ value of the test. If datapoint i passes the test it follows the left

branch otherwise it follows the right one. Let dn = 1 iﬀ node n applies a split, to allow having this
option not to split at a node. To track each datapoint i through the tree, the decision variable
a(n),n is introduced where ζ i
ζ i

a(n),n = 1 iﬀ datapoint i is in node n.

Let Pnk to be the number of datapoints of class k assigned to leaf node n and Pn to be the
total number of datapoints in leaf node n. Let wnk denote the prediction of each leaf node n,
where wnk = 1 iﬀ the predicted label of node n is k ∈ K. At the end, let Ln denote the number of

25

N<latexit sha1_base64="XTAmHD3+uebqRV9EN3uWWn2e2mM=">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclZkq6LLoxpVUsA9sh5JJ77ShmcyQZIRS+hduXCji1r9x59+YaWehrQcCh3PuJeeeIBFcG9f9dlZW19Y3Ngtbxe2d3b390sFhU8epYthgsYhVO6AaBZfYMNwIbCcKaRQIbAWjm8xvPaHSPJYPZpygH9GB5CFn1FjpsRtRM2RUkLteqexW3BnIMvFyUoYc9V7pq9uPWRqhNExQrTuemxh/QpXhTOC02E01JpSN6AA7lkoaofYns8RTcmqVPgljZZ80ZKb+3pjQSOtxFNjJLKFe9DLxP6+TmvDKn3CZpAYlm38UpoKYmGTnkz5XyIwYW0KZ4jYrYUOqKDO2pKItwVs8eZk0qxXvvFK9vyjXrvM6CnAMJ3AGHlxCDW6hDg1gIOEZXuHN0c6L8+58zEdXnHznCP7A+fwBFJ6Qhg==</latexit>L<latexit sha1_base64="n2ZkyHRQfuxyMWApcvw1zS/q5hw=">AAAB8XicbVC7SgNBFL3rM8ZX1NJmMAhWYTcKWgZtLCwimAcmS5id3E2GzM4uM7NCCPkLGwtFbP0bO//G2WQLTTwwcDjnXubcEySCa+O6387K6tr6xmZhq7i9s7u3Xzo4bOo4VQwbLBaxagdUo+ASG4Ybge1EIY0Cga1gdJP5rSdUmsfywYwT9CM6kDzkjBorPXYjaoaMCnLXK5XdijsDWSZeTsqQo94rfXX7MUsjlIYJqnXHcxPjT6gynAmcFrupxoSyER1gx1JJI9T+ZJZ4Sk6t0idhrOyThszU3xsTGmk9jgI7mSXUi14m/ud1UhNe+RMuk9SgZPOPwlQQE5PsfNLnCpkRY0soU9xmJWxIFWXGllS0JXiLJy+TZrXinVeq9xfl2nVeRwGO4QTOwINLqMEt1KEBDCQ8wyu8Odp5cd6dj/noipPvHMEfOJ8/EZaQhA==</latexit>1<latexit sha1_base64="3uKJJTIbJdE2WmM72S+tEthm1Lc=">AAAB6HicbVDJSgNBEK2JW4xb1KOXxiB4CjMqLregF48JmAWSIfR0apI2PQvdPUII+QIvHhTx6id582/smQyixgcFj/eqqKrnxYIrbdufVmFpeWV1rbhe2tjc2t4p7+61VJRIhk0WiUh2PKpQ8BCbmmuBnVgiDTyBbW98k/rtB5SKR+GdnsToBnQYcp8zqo3UcPrlil21M5BF4uSkAjnq/fJHbxCxJMBQM0GV6jp2rN0plZozgbNSL1EYUzamQ+waGtIAlTvNDp2RI6MMiB9JU6EmmfpzYkoDpSaBZzoDqkfqr5eK/3ndRPuX7pSHcaIxZPNFfiKIjkj6NRlwiUyLiSGUSW5uJWxEJWXaZFPKQrhKcf798iJpnVSd0+pp46xSu87jKMIBHMIxOHABNbiFOjSBAcIjPMOLdW89Wa/W27y1YOUz+/AL1vsXk2WM7A==</latexit>3<latexit sha1_base64="IHtO9p1P8liIpvcwLcwcOMd35ys=">AAAB6HicbVDJSgNBEK2JW4xb1KOXxiB4ChMjLregF48JmAWSIfR0apLWnoXuHiEM+QIvHhTx6id582/smQyixgcFj/eqqKrnRoIrbdufVmFpeWV1rbhe2tjc2t4p7+51VBhLhm0WilD2XKpQ8ADbmmuBvUgi9V2BXff+OvW7DygVD4NbPY3Q8ek44B5nVBupVR+WK3bVzkAWSS0nFcjRHJY/BqOQxT4GmgmqVL9mR9pJqNScCZyVBrHCiLJ7Osa+oQH1UTlJduiMHBllRLxQmgo0ydSfEwn1lZr6run0qZ6ov14q/uf1Y+1dOAkPolhjwOaLvFgQHZL0azLiEpkWU0Mok9zcStiESsq0yaaUhXCZ4uz75UXSOanW6tV667TSuMrjKMIBHMIx1OAcGnADTWgDA4RHeIYX6856sl6tt3lrwcpn9uEXrPcvlm2M7g==</latexit>6<latexit sha1_base64="MmOck5ijdazcZFYIXF+O9mWPeYw=">AAAB6HicbVBNS8NAEN3Ur1q/qh69LBbBU0msVL0VvXhswX5AG8pmO2nXbjZhdyOU0F/gxYMiXv1J3vw3btIgan0w8Hhvhpl5XsSZ0rb9aRVWVtfWN4qbpa3tnd298v5BR4WxpNCmIQ9lzyMKOBPQ1kxz6EUSSOBx6HrTm9TvPoBULBR3ehaBG5CxYD6jRBupVR+WK3bVzoCXiZOTCsrRHJY/BqOQxgEITTlRqu/YkXYTIjWjHOalQawgInRKxtA3VJAAlJtkh87xiVFG2A+lKaFxpv6cSEig1CzwTGdA9ET99VLxP68fa//STZiIYg2CLhb5Mcc6xOnXeMQkUM1nhhAqmbkV0wmRhGqTTSkL4SpF/fvlZdI5qzq1aq11Xmlc53EU0RE6RqfIQReogW5RE7URRYAe0TN6se6tJ+vVelu0Fqx85hD9gvX+BZr5jPE=</latexit>7<latexit sha1_base64="jAAP9qrfjx8oxJMVdCVx6ggBAmU=">AAAB6HicbVBNS8NAEN3Ur1q/qh69LBbBU0msWL0VvXhswX5AG8pmO2nXbjZhdyOU0F/gxYMiXv1J3vw3btIgan0w8Hhvhpl5XsSZ0rb9aRVWVtfWN4qbpa3tnd298v5BR4WxpNCmIQ9lzyMKOBPQ1kxz6EUSSOBx6HrTm9TvPoBULBR3ehaBG5CxYD6jRBupVR+WK3bVzoCXiZOTCsrRHJY/BqOQxgEITTlRqu/YkXYTIjWjHOalQawgInRKxtA3VJAAlJtkh87xiVFG2A+lKaFxpv6cSEig1CzwTGdA9ET99VLxP68fa//STZiIYg2CLhb5Mcc6xOnXeMQkUM1nhhAqmbkV0wmRhGqTTSkL4SrFxffLy6RzVnVq1VrrvNK4zuMooiN0jE6Rg+qogW5RE7URRYAe0TN6se6tJ+vVelu0Fqx85hD9gvX+BZx9jPI=</latexit>Xf2Fb1fxif>v1<latexit sha1_base64="ezrIiahubgXFaVinWgcsNF5qHqE=">AAACE3icbVDLSsNAFJ3UV62vqEs3g0UQFyWRgq6kKIjLCvYBTQyT6aQdOpmEmUmxhPyDG3/FjQtF3Lpx5984abPQ1gMXDufcy733+DGjUlnWt1FaWl5ZXSuvVzY2t7Z3zN29towSgUkLRywSXR9JwignLUUVI91YEBT6jHT80VXud8ZESBrxOzWJiRuiAacBxUhpyTNPoCOT0EsD6FAOnRCpIUYsvc4y30vtIHu4p14AL+DYsz2zatWsKeAisQtSBQWanvnl9COchIQrzJCUPduKlZsioShmJKs4iSQxwiM0ID1NOQqJdNPpTxk80kofBpHQxRWcqr8nUhRKOQl93ZkfLee9XPzP6yUqOHdTyuNEEY5ni4KEQRXBPCDYp4JgxSaaICyovhXiIRIIKx1jRYdgz7+8SNqnNbteq9/Wq43LIo4yOACH4BjY4Aw0wA1oghbA4BE8g1fwZjwZL8a78TFrLRnFzD74A+PzBw7QnaM=</latexit>2<latexit sha1_base64="P9jSdtNLjbzdB95seFPXyVC/O2o=">AAAB6HicbVDLSsNAFJ3UV62vqks3g0VwVZJWfOyKbly2YB/QhjKZ3rRjJ5MwMxFK6Be4caGIWz/JnX/jJA2i1gMXDufcy733eBFnStv2p1VYWV1b3yhulra2d3b3yvsHHRXGkkKbhjyUPY8o4ExAWzPNoRdJIIHHoetNb1K/+wBSsVDc6VkEbkDGgvmMEm2kVm1YrthVOwNeJk5OKihHc1j+GIxCGgcgNOVEqb5jR9pNiNSMcpiXBrGCiNApGUPfUEECUG6SHTrHJ0YZYT+UpoTGmfpzIiGBUrPAM50B0RP110vF/7x+rP1LN2EiijUIuljkxxzrEKdf4xGTQDWfGUKoZOZWTCdEEqpNNqUshKsU598vL5NOrerUq/XWWaVxncdRREfoGJ0iB12gBrpFTdRGFAF6RM/oxbq3nqxX623RWrDymUP0C9b7F5TpjO0=</latexit>4<latexit sha1_base64="44ElfCVPVtD1lZxTdJwmuX5TeZ8=">AAAB6HicbVDLSsNAFJ3UV62vqks3g0VwVRJbfOyKbly2YB/QhjKZ3rRjJ5MwMxFK6Be4caGIWz/JnX/jJA2i1gMXDufcy733eBFnStv2p1VYWV1b3yhulra2d3b3yvsHHRXGkkKbhjyUPY8o4ExAWzPNoRdJIIHHoetNb1K/+wBSsVDc6VkEbkDGgvmMEm2kVn1YrthVOwNeJk5OKihHc1j+GIxCGgcgNOVEqb5jR9pNiNSMcpiXBrGCiNApGUPfUEECUG6SHTrHJ0YZYT+UpoTGmfpzIiGBUrPAM50B0RP110vF/7x+rP1LN2EiijUIuljkxxzrEKdf4xGTQDWfGUKoZOZWTCdEEqpNNqUshKsU598vL5POWdWpVWuteqVxncdRREfoGJ0iB12gBrpFTdRGFAF6RM/oxbq3nqxX623RWrDymUP0C9b7F5fxjO8=</latexit>5<latexit sha1_base64="I4UEM/juM7rVSGwZnTVTbzhGuFU=">AAAB6HicbVDLSsNAFJ3UV62vqks3g0VwVRLrc1d047IF+4A2lMn0ph07mYSZiVBCv8CNC0Xc+knu/BsnaRC1HrhwOOde7r3HizhT2rY/rcLS8srqWnG9tLG5tb1T3t1rqzCWFFo05KHsekQBZwJammkO3UgCCTwOHW9yk/qdB5CKheJOTyNwAzISzGeUaCM1zwblil21M+BF4uSkgnI0BuWP/jCkcQBCU06U6jl2pN2ESM0oh1mpHyuICJ2QEfQMFSQA5SbZoTN8ZJQh9kNpSmicqT8nEhIoNQ080xkQPVZ/vVT8z+vF2r90EyaiWIOg80V+zLEOcfo1HjIJVPOpIYRKZm7FdEwkodpkU8pCuEpx/v3yImmfVJ1atdY8rdSv8ziK6AAdomPkoAtUR7eogVqIIkCP6Bm9WPfWk/Vqvc1bC1Y+s49+wXr/Apl1jPA=</latexit>Xf2Fb1fxifv1<latexit sha1_base64="ZvttfATJ74bdlOIL84NUn6YJx5A=">AAACFnicbVDLSsNAFJ3UV62vqEs3g0VwY0mkoMuiIC4r2Ac0MUymk3boZBJnJsUS8hVu/BU3LhRxK+78GydtFtp64MLhnHu59x4/ZlQqy/o2SkvLK6tr5fXKxubW9o65u9eWUSIwaeGIRaLrI0kY5aSlqGKkGwuCQp+Rjj+6zP3OmAhJI36rJjFxQzTgNKAYKS155okjk9BLA+hQDp0QqSFGLL3KMt9L7SCDD3fU0yYj93Ds2Z5ZtWrWFHCR2AWpggJNz/xy+hFOQsIVZkjKnm3Fyk2RUBQzklWcRJIY4REakJ6mHIVEuun0rQweaaUPg0jo4gpO1d8TKQqlnIS+7swPl/NeLv7n9RIVnLsp5XGiCMezRUHCoIpgnhHsU0GwYhNNEBZU3wrxEAmElU6yokOw519eJO3Tml2v1W/q1cZFEUcZHIBDcAxscAYa4Bo0QQtg8AiewSt4M56MF+Pd+Ji1loxiZh/8gfH5A83knyE=</latexit>missclassiﬁed datapoints at node n.

max (1 − λ)

!

|I| − X
n∈L

Ln

− λ X
n∈N

dn

s.t. Ln ≥ Pn − Pnk − |I|(1 − wnk)

Ln ≤ Pn − Pnk + |I|wnk
Pnk = X
ζ i
a(n),n

i∈I:
yi=k
ζ i
a(n),n

Pn = X

i∈I

ln = X

wnk

k∈K

ζ i
a(n),n ≤ ln
X
ζ i
a(n),n = 1

bmf xi

f ≥ vm + ζ i

a(n),n − 1

bmf xi

f ≤ vm − 2ζ i

a(n),n + 1

n∈L
X

f ∈F
X

f ∈F
X

f ∈F

∀k ∈ K, n ∈ L

∀k ∈ K, n ∈ L

∀k ∈ K, n ∈ L

(12.1)

(12.2)

(12.3)

(12.4)

∀n ∈ L

(12.5)

∀n ∈ L

(12.6)

∀n ∈ L

∀i ∈ I

(12.7)

(12.8)

∀i ∈ I, n ∈ L, m ∈ AR(n)

(12.9)

∀i ∈ I, n ∈ L, m ∈ AL(n)

(12.10)

bnf = dn

∀n ∈ N

(12.11)

0 ≤ vn ≤ dn

dn ≤ da(n)
zi
n, ln ∈ {0, 1}

bnf , dn ∈ {0, 1}

∀n ∈ N

∀n ∈ N \{1}

∀i ∈ I, n ∈ L

∀f ∈ F, n ∈ N ,

(12.12)

(12.13)

(12.14)

(12.15)

where λ ∈ [0, 1] is a regularization term. The objective (12.1) maximizes the total number

of correctly classiﬁed datapoints |I| − P
n∈N dn.
Constraints (12.2) and (12.3) deﬁnes the number of missclassiﬁed datapoints at each node n.

n∈L Ln while minimizing the number of splits P

Constraints (12.4) and (12.5) give the deﬁnitions of Pnk and Pn respectively. constraints (12.6)-
(12.7), enforce that if a leaf n does not have an assigned class label, no datapoint should end up at

that leaf. Constraint (12.8) makes sure that each datapoint i is assigned to exactly one of the leaf

nodes. Constraint (12.9) implies that if datapoint i is assigned to node n, it should take the right

branch for all ancestors of n belonging to AR(n). Respectively, constraint (12.10) implies that if

datapoint i is assigned to node n, it should take the left branch for all ancestors of n belonging

to AL(n). Constraint (12.11) enforces that if node n splits, it should split on exatcly one of the

26

 
features f ∈ F. Constraint (12.12) implies that if a node does not apply a split, all datapoints going

through this node would take the right branch. At the end constraint (12.13) makes sure that if

node n does not split, none of its descendants cannot split.

In the main formulation of Bertsimas and Dunn (2017), they have parameter Nmin which denotes
the minimum number of points at each leaf. We set this parameter to zero as we do not have a

similar notion in our formulation.

C. Comparison with OCT

In formulation (12), vn can be ﬁxed to dn for all nodes (for the case of binary data) leading to the
simpliﬁed formulation

max (1 − λ)

!

|I| − X
n∈L

Ln

− λ X
n∈N

dn

s.t. Ln ≥ Pn − Pnk − |I|(1 − wnk)

Ln ≤ Pn − Pnk + |I|wnk
Pnk = X
ζ i
a(n),n

i∈I:
yi=k
ζ i
a(n),n

Pn = X

i∈I

ln = X

wnk

k∈K

ζ i
a(n),n ≤ ln
X
ζ i
a(n),n = 1

∀k ∈ K, n ∈ L

∀k ∈ K, n ∈ L

∀k ∈ K, n ∈ L

∀n ∈ L

∀n ∈ L

∀n ∈ L

∀i ∈ I

n∈L

X

f ∈F :xi

f =1

X

f ∈F :xi

f =1

bmf ≥ dm + ζ i

a(n),n − 1

∀i ∈ I, n ∈ L, m ∈ AR(n)

bmf ≤ dm − 2ζ i

a(n),n + 1

∀i ∈ I, n ∈ L, m ∈ AL(n)

X

f ∈F

bnf = dn

dn ≤ da(n)
zi
n, ln ∈ {0, 1}

bnf , dn ∈ {0, 1}

∀n ∈ N

∀n ∈ N \{1}

∀i ∈ I, n ∈ L

∀f ∈ F, n ∈ N ,

Note that ﬁxing vn is a simpliﬁcation due to the assumption of binary data, rather than an actual
strengthening. Moreover, note that OCT and FlowOCT have diﬀerent conventions for nodes where

27

 
branching is not performed: in FlowOCT, a label (encoded by wnk) is directly assigned to that node,
while in OCT all points go right by convention. This diﬀerent convention creates a slight change

in the feasible region of both formulations. To be able to directly compare the formulations, we

consider the case of “full" trees where branching is performed at all internal nodes N . For FlowOCT

formulation, this corresponds to setting wnk = 0 for all n ∈ N , k ∈ K, while for OCT it corresponds
to setting dn = 1 for all n ∈ S. Moreover, using the identity P
f =0 bmf
and and noting that ln can be ﬁxed to 1 in the formulation, we obtain the simpliﬁed OCT formulation

f =1 bmf = 1 − P

f ∈F :xi

f ∈F :xi

max

!

|I| − X
n∈L

Ln

s.t. Ln ≥ Pn − Pnk − |I|(1 − wnk)

Ln ≤ Pn − Pnk + |I|wnk
Pnk = X
ζ i
a(n),n

i∈I:
yi=k
ζ i
a(n),n

Pn = X

i∈I

wnk = 1

ζ i
a(n),n = 1

X

k∈K
X

n∈L

∀k ∈ K, n ∈ L

∀k ∈ K, n ∈ L

∀k ∈ K, n ∈ L

(13.1)

(13.2)

(13.3)

(13.4)

∀n ∈ L

(13.5)

∀n ∈ L

(13.6)

∀i ∈ I

(13.7)

X

bmf ≥ ζ i

a(n),n

∀i ∈ I, n ∈ L, m ∈ AR(n)

(13.8)

f ∈F :xi

f =1

X

bmf ≥ 2ζ i

a(n),n − 1

∀i ∈ I, n ∈ L, m ∈ AL(n)

(13.9)

f ∈F :xi

f =0

X

bnf = 1

f ∈F
ζ i
a(n),n ∈ {0, 1}

bnf ∈ {0, 1}

∀n ∈ N

(13.10)

∀i ∈ I, n ∈ L

∀f ∈ F, n ∈ N .

(13.11)

(13.12)

C.1. Strengthening

We now show how formulation (13) can be strengthened. Observe that the validity of the steps

below is guaranteed by the validity of FlowOCT, thus we do not focus on validity below.

28

 
Bound tightening for (13.9) Adding the quantity 1 − ζ i

a(n),n ≥ 0 to the right hand side of (13.9),

we obtain the stronger constraints

X

f ∈F :xi

f =0

bmf ≥ ζ i

a(n),n ∀i ∈ I, n ∈ L, m ∈ AL(n).

Improved branching constraints Constraints (13.8) can be strengthened to

X

bmf ≥ X

ζ i
a(n),n ∀i ∈ I, m ∈ N .

f ∈F :xi

f =1

n∈L:m∈AR(n)

(14)

(15)

Observe that constraints (15), in addition to being stronger than (13.8), also reduce the number

of constraints require to represent the LP relaxation. Similarly, constraint (14) can be further

improved to

X

bmf ≥ X

ζ i
a(n),n ∀i ∈ I, m ∈ N .

f ∈F :xi

f =0

n∈L:m∈AL(n)

(16)

Improved missclassiﬁcation formulation Deﬁne for all i ∈ I and n ∈ L additional variables
zi
a(n),n ≤ ζa(n),nwn,yi. Note that zi
a(n),n = 1)
and the class of i is assigned to n (wnyi = 1), hence zi
a(n),n = 1 only if i is correctly classiﬁed at leaf
n. Upper bounds of zi

a(n),n = 1 implies that datapoint i is routed to leaf n (ζ i

a(n),n = 1 can be imposed via the linear constraints

a(n),n ≤ ζ i
zi

a(n),n, zi

a(n),n ≤ wnyi ∀n ∈ L, i ∈ I.

(17)

In addition, since Ln corresponds to the number of missclassiﬁed points at leaf n ∈ L and P
we ﬁnd that constraints

n∈L Ln,

Ln ≥ X

(ζ i

n,a(n) − zi

n,a(n)).

Note that constraints (18) and (13.7) imply that

i∈I

X

Ln ≥ |I| − X

X

zi
n,a(n).

n∈L

i∈I

n∈L

(18)

(19)

C.2. Simpliﬁcation

The linear programming relaxation of the formulation obtained in §C.1, given by constraints (13.2)-

(13.7), (13.10)-(13.12), (15), (16), (17) and (18), is certainly stronger than the relaxation of OCT,

as either constraints where tightened or additional constraints were added. We now show how

the resulting formulation can be simpliﬁed without loss of relaxation quality, ultimately obtaining

FlowOCT.

29

Upper bound on missclassiﬁcation Variable Ln has a negative objective coeﬃcient and only
appears on constraints (13.2)-(13.3) and (18), it will always be set to a lower bound. Therefore,

constraint (13.3) is redundant and can be dropped without aﬀecting the relaxation of the problem.

Lower bound on missclassiﬁcation Substituting variables according to (13.4) and (13.5), we ﬁnd

that for a given k ∈ K and n ∈ L, (13.2) is equivalent to

Ln ≥ X

i∈I

a(n),n − X
ζ i
i∈I:
yi=k

ζ i
a(n),n − |I|(1 − wnk)

⇔ Ln ≥ X

(wnk − 1) + X

(ζ i

a(n),n − 1 + wnk).

i∈I
yi=k

i∈I
yi6=k

Observe that wnk − 1 ≤ 0 ≤ ζ i
k ∈ K \ {yi},

a(n),n − zi

a(n),n. Moreover, we also have that for any i ∈ I and

zi
a(n),n ≤ wnyi ≤ 1 − wnk,

(20)

where the ﬁrst inequality follows from (17) and the second inequality follows from (13.6). Therefore
from (20) we conclude that ζ i
a(n),n and inequalities (18) dominate
inequalities (13.2). Since inequalities (13.4)-(13.5) only appeared in inequalities (13.2)-(13.3), which

a(n),n − 1 + wnk ≤ ζ i

a(n),n − zi

where shown to be redundant, they can be dropped as well. Finally, as inequalities (18) deﬁne the

unique the lower bounds of Ln in the simpliﬁed formulation, they can be changed to equalities
without loss of generalities, and the objective can be updated according to (19). After all the

30

(21.1)

(21.2)

(21.3)

(21.4)

(21.5)

(21.6)

(21.7)

(21.8)

(21.9)

changes outlined so far, the formulation reduces to

X

zi
n,a(n)

n∈L

max X
i∈I
s.t. X
k∈K
X

wnk = 1

ζ i
a(n),n = 1

∀n ∈ L

∀i ∈ I

n∈L

X

bmf ≥ X

ζ i
a(n),n

∀i ∈ I, m ∈ N

f ∈F :xi

f =1

n∈L:m∈AR(n)

X

bmf ≥ X

ζ i
a(n),n

∀i ∈ I, m ∈ N

n∈L:m∈AL(n)

f ∈F :xi

f =0

X

bnf = 1

a(n),n

f ∈F
a(n),n ≤ ζ i
zi
zi
a(n),n ≤ wnyi
ζ i
a(n),n ∈ {0, 1}

bnf ∈ {0, 1}

∀n ∈ N

∀n ∈ L, i ∈ I

∀n ∈ L, i ∈ I

∀i ∈ I, n ∈ L

∀f ∈ F, n ∈ N .

(21.10)

C.3. Projection

We now project out the ζ variables, obtaining a more compact formulation with the same LP

relaxation. Speciﬁcally, consider the formulation

max X
i∈I
s.t. X
k∈K

X

zi
n,a(n)

n∈L

wnk = 1

X

bmf ≥ X

zi
a(n),n

f ∈F :xi

f =1

n∈L:m∈AR(n)

X

bmf ≥ X

zi
a(n),n

n∈L:m∈AL(n)

f ∈F :xi

f =0

X

bnf = 1

f ∈F
zi
a(n),n ≤ wnyi
zi
a(n),n ∈ {0, 1}

bnf ∈ {0, 1}

∀n ∈ L

∀i ∈ I, m ∈ N

∀i ∈ I, m ∈ N

∀n ∈ N

∀n ∈ L, i ∈ I

∀i ∈ I, n ∈ L

∀f ∈ F, n ∈ N ..

(22.1)

(22.2)

(22.3)

(22.4)

(22.5)

(22.6)

(22.7)

(22.8)

Proposition 3. Formulations (21) and (22) are equivalent, i.e., their LP relaxations have the same

31

optimal objective value.

Proof. Let ν1 and ν2 be the optimal objective value of the LP relaxations of (21) and (22). Note
that (22) is a relaxation of (21), obtained by dropping constraint (21.3) and replacing ζ with a lower

bound in constraints (21.4)-(21.5). Therefore, it follows that ν2 ≥ ν1. We now show that ν2 ≤ ν1.
Let (b∗, w∗, z∗) be an optimal solution of (22) and let i ∈ I. For any given i ∈ I, by summing

constraints (22.3) and (22.4) for the root node m = 1, we ﬁnd that

1 = X
f ∈F :xi

f =1

1f + X
b∗
f ∈F :xi

f =0

1f ≥ X
b∗

(zi

a(n),n)∗.

n∈L

(23)

Now let ζ = z∗. If the inequality in (23) holds at equality, then (b∗, w∗, z∗, ζ) satisﬁes all constraints

in (21) and the proof is complete. Otherwise, it follows that either (22.3) or (22.4) is strict at node

m = 1, and without loss of generality assume (22.3) is strict. Summing up inequalities (22.3) and

(22.4) for node m = r(1), we ﬁnd that

1 = X
f ∈F

b∗
r(1)f >

X

(zi

a(n),n)∗,

n∈L:r(1)∈AR(n)∪AL(n)

(24)

where the strict inequality holds since the right hand side of (24) is no greater than the right hand

side of (23). By applying this process recursively, we obtain a path from node 1 to a leaf h ∈ L
such that all inequalities (22.3)-(22.4) corresponding to this path are strict. The value ζ i
a(h),h can
be then increased by the minimum slack in the constraints, and the overall process can be repeated
(cid:4)

until inequality (21.3) is tight.

C.4. Substitution

Finally, to recover the FlowOCT formulation, for all m ∈ L, substitute variables

m,r(m) := X
zi

zi
a(n),n, and

n∈L:m∈AR(n)

m,‘(m) := X
zi

zi
a(n),n.

n∈L:m∈AL(n)

32

and for all n ∈ L introduce variables zn,t = za(n),n. Constraints (22.3)-(22.4) reduce to P
m,r(m) and P
zi
m,‘(m). Finally, since

f ∈F :xi

f =0 bmf ≥ zi

f ∈F :xi

f =1 bmf ≥

za(m),m =

X

za(n),n

n∈L:m∈AR(n)∪AL(n)

= X

za(n),n + X

za(n),n

n∈L:m∈AR(n)

n∈L:m∈AL(n)

= zm,r(m) + zm,‘(m),

we recover the ﬂow conservation constraints.

D. Extended Results

In Table 4, for each dataset and depth, we show the in sample results for each approach. In this

table, for λ = 0, we average the in sample results including the training accuracy, optimality gap

and solving time across ﬁve diﬀerent samples trained over 50% of the data when λ is ﬁxed to be

zero. Out of 32 instances, OCT has the best training accuracy in 0 instances (excluding ties) and

BinOCT in 7 instances while FlowOCT and Benders have the best accuracy in 11 instances. In terms

of solving time, BinOCT achieves a smaller solving time in 7 instances while Benders achieves a

smaller solving time in 13 instances (excluding ties). In terms of optimality gap, OCT achieves a

smaller gap time only in one of the instances while Benders achieves a smaller gap time in 15

instances (excluding ties).

Similarly for λ > 0 we show similar results but this time for a given instance and a λ ∈ [0.1, 0.9]

with step size of 0.1 we solve 5 diﬀerent samples and report the average results across all 45 samples.

As BinOCT does not have any regularization term, we have excluded it from this section. We observe

that Benders outperform OCT in both optimality gap and solving time for all instances.

33

s
t
l
u
s
e
r

e
l

p
m
a
s

n
I

:
4

l

e
b
a
T

0
>
λ

0
=
λ

e
m
T

i

p
a
G

e
m
T

i

p
a
G

e
m
T

i

p
a
G

e
m
T

i

p
a
G

c
c
a
-
n

i
a
r
T

e
m
T

i

p
a
G

c
c
a
-
n

i
a
r
T

e
m
T

i

p
a
G

c
c
a
-
n
i
a
r
T

e
m
T

i

p
a
G
c
c
a
-
n
i
a
r
T

s
r
e
d
n
e
B

T
C
O
w
o
l
F

T
C
O

s
r
e
d
n
e
B

T
C
O
w
o
l
F

T
C
O
n
B

i

T
C
O

h
t
p
e
D

t
e
s
a
t
a
D

8
.
0

0
.
4

0
.
1
3

2
.
5
3

0
.
1

3
.
4

2
.
6

4
.
8

5
.
3

7
.
3
7
3

2
.
8
1
8
2

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

2
.
9

9
.
5

7
.
3
2

1
.
4
4
2

5
.
2
6
2

6
.
8

6
.
5
2

1
.
3
6

5
.
4
5
1

4
.
5
1

7
.
9
1
6

9
.
6
8
8
2

3
.
9
2
8
2

1
.
3
1

8
.
7
8
9
2

0
.
1

9
.
0
1

2
.
2
2

7
.
5
2

4
.
6

1
.
9
5
3

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
8

5
.
6
7

6
.
4
6
1

7
.
0
1
3

1
.
2
9

4
.
2
4
3
2

2
.
0
4
3
3

1
.
0
1

3
.
1
1
6
3

1
.
0
0
6
3

4
.
4
2

8
.
2
2
6
3

8
.
5
1
1

0
.
0

7
.
3
9
1
1

2
.
0
0
6
3

4
.
8
1

6
.
0
1
6
3

2
.
0
0
6
3

8
.
4
2

7
.
1
2
6
3

2
.
0
0
6
3

9
.
4
2

6
.
0
5
6
3

1
.
5
5

2
.
1
7
5
3

0
.
0

7
.
6

6
.
9
7
6

9
.
4
1
6
3

1
.
0
0
6
3

1
.
0
2

6
.
0
3
6
3

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
1
1

4
.
4
1

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

1
.
0

1
.
2
2

3
.
9
2

0
.
0

1
.
9
2

2
.
7
2

1
.
5
3

8
.
0

2
.
2
2

2
.
8
2

8
.
2

7
.
3
1
3

3
.
7
9
1
1

3
.
1
6
6
1

0
.
4

3
.
6
9
0
1

1
.
2
4
0
1

3
.
7
0
6
2

6
.
5
1

0
.
0

0
.
0

5
.
0

7
.
0

0
.
0

3
.
0

3
.
0

3
.
3

0
.
0

9
.
0

1
.
6
1

3
.
3
7

9
.
5

2
.
1

8
.
4

3
.
5

7
.
2

1
.
2
2

1
.
6
7
0
3

5
.
8
1

0
.
3
9
6

1
.
1
1
2
3

4
.
7
2

0
.
0
0
6
3

2
.
4
2
2
3

0
.
1
3

4
.
5
0
5
3

3
.
4

5
.
7
0
4

3
.
8
8
7

0
.
4
3
3
1

2
.
2
4
1

0
.
0

0
.
0

0
.
0

2
.
0

0
.
0

5
.
1

1
.
7
6

4
.
3
1

8
.
8
1

0
.
7

4
.
3
1
6
3

8
.
7
3

4
.
8
4
5

0
.
5
3
6
3

2
.
3
4

0
.
0
0
6
3

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
8

2
.
3

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

7
.
9

8
.
9
8
6
3

3
.
2
5

0
.
0
0
6
3

9
.
9
1

7
.
5
6
3
2

8
.
2

3
.
8
1
1

0
.
0

5
.
6
2
6
3

6
.
3
3

0
.
0
0
6
3

8
.
8
1

0
.
0
7
6
3

9
.
8
3

0
.
0
0
6
3

5
.
0
2

4
.
8
7
7
3

4
.
1
4

0
.
0
0
6
3

4
.
3
1

7
.
0
1
6
1

0
.
0

4
.
6
6

8
.
4
3
6
3

0
.
6
3

0
.
0
0
6
3

0
.
0

3
.
9

9
.
1
9
6
3

9
.
7
3

0
.
0
0
6
3

6
.
9
1

1
.
0
0
6
3

9
.
8
1

2
.
3
5
7
3

3
.
2
7
0
2
6
1

4
.
8
2
8
3

3
.
7
3

0
.
0
0
6
3

3
.
7
1

1
.
2
5
6

0
.
0

5
.
4
4
5
3

2
.
0
0
6
3

7
.
1
1

3
.
1
7
6
3

4
.
8
1

2
.
9
4

2
.
1
4
6
3

4
.
4
2

2
.
1
7
6

0
.
0

9
.
1
2
7
3

1
.
0
4

0
.
0
0
6
3

7
.
0
1

2
.
0
0
6
3

7
.
9

5
.
9
7
4
4

6
.
7
4
9
9
1
4

7
.
3
2
9
3

6
.
9
4

0
.
0
0
6
3

8
.
1
1

3
.
0
0
6
3

5
.
2
1

0
.
9
8
7
4

8
.
2
2
0
7
7
6

7
.
0
0
4
4

5
.
0
5

0
.
0
0
6
3

7
.
0
1

0
.
4
9

0
.
8
9

0
.
0
0
1

0
.
0
0
1

8
.
5
8

5
.
5
9

0
.
0
0
1

0
.
0
0
1

4
.
1
7

2
.
1
8

5
.
9
8

9
.
6
9

1
.
7
9

0
.
9
9

0
.
0
0
1

0
.
0
0
1

2
.
0
7

6
.
6
7

7
.
9
7

9
.
2
8

7
.
2
7

7
.
8
7

0
.
3
8

2
.
8
8

5
.
8
7

7
.
1
8

6
.
3
8

2
.
5
8

9
.
6
8

4
.
0
9

8
.
9
8

5
.
0
9

7
.
6

1
.
9
8

6
.
1
4
1

7
.
7
1
1

0
.
8

7
.
5
4

0
.
9
6

3
.
4
1
2

8
.
2
1

6
.
6
0
1
1

5
.
2
0
6
3

5
.
5
0
6
3

9
.
2
1

9
.
2
9
1

9
.
6
7
2

9
.
6
0
2

5
.
9
4

8
.
0
2
3
3

3
.
1
1
6
3

0
.
3
2
6
3

7
.
8
9
1
1

6
.
0
1
6
3

1
.
3
2
6
3

1
.
9
6
7
3

8
.
4
1
2
1

9
.
4
1
6
3

8
.
0
3
6
3

6
.
2
4
7
3

7
.
1
8
4
3

0
.
1
0
7
3

5
.
0
6
5
4

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

4
.
8

0
.
7

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

5
.
0

5
.
0
3

0
.
7
2

0
.
0

5
.
1
3

0
.
6
2

2
.
2
4

7
.
7

2
.
6
2

1
.
9
3

9
.
9
3

3
.
4
1

0
.
5
6

1
.
8
5

7
.
0
9
7
3

0
0
0
0
1

0
.
4
9

0
.
8
9

0
.
0
0
1

0
.
0
0
1

8
.
5
8

5
.
5
9

0
.
0
0
1

0
.
0
0
1

4
.
1
7

2
.
1
8

5
.
9
8

6
.
3
9

1
.
7
9

0
.
9
9

0
.
0
0
1

0
.
0
0
1

3
.
0
7

6
.
6
7

4
.
6
7

8
.
8
7

8
.
2
7

0
.
6
7

5
.
9
7

4
.
0
7

9
.
6
7

4
.
9
7

9
.
1
7

6
.
1
7

1
.
2
8

1
.
1
6

8
.
3
6

5
.
1
5

3
.
0

2
.
2
3

3
.
4
5
6

6
.
6
5
1

5
.
1

8
.
4
4

9
.
1
6

3
.
5

7
.
5

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0
0
6
3

8
.
9
9

0
.
0
0
6
3

0
.
0
0
1

0
.
0
0
6
3

0
.
0
0
1

6
.
0

4
.
0
8
1

0
.
0

0
.
0

6
.
5
0
1
1

0
.
0
2

6
.
4
5
4

8
.
6

0
.
0

0
.
0

0
.
0
0
6
3

3
.
5
9

0
.
0
0
6
3

0
.
0
0
1

0
.
0
0
6
3

0
.
0
0
1

7
.
9
8

0
.
0

0
.
0
0
6
3

0
.
0
0
1

0
.
0
0
6
3

0
.
0
0
1

0
.
0
0
6
3

0
.
0
0
1

7
.
7
2

0
.
0

0
.
0
0
6
3

0
.
0
0
1

0
.
0
0
6
3

0
.
0
0
1

0
.
0
0
6
3

0
.
0
0
1

6
.
2
0
1

0
.
0

0
.
0
0
6
3

0
.
0
0
1

0
.
0
0
6
3

0
.
0
0
1

0
.
0
0
6
3

0
.
0
0
1

0
.
4
9

0
.
8
9

0
.
0
0
1

0
.
0
0
1

8
.
5
8

5
.
5
9

0
.
0
0
1

0
.
0
0
1

4
.
1
7

5
.
0
8

7
.
6
8

8
.
4
9

1
.
7
9

0
.
9
9

8
.
9
9

0
.
0
0
1

2
.
0
7

5
.
6
7

8
.
8
7

7
.
1
8

7
.
2
7

8
.
8
7

0
.
5
8

2
.
8
8

5
.
8
7

9
.
1
8

9
.
3
8

0
.
5
8

9
.
6
8

6
.
2
9

5
.
2
9

6
.
3
9

1
.
2

6
.
3
8
2

2
.
1
9
3

8
.
3
0
2

9
.
2

1
.
2
7
6

5
.
9
4

9
.
6
1
1

3
.
3
1

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

9
.
2
0
6
3

1
.
2
2

3
.
7
0
6
3

4
.
3
1

9
.
7
1
6
3

8
.
5

1
.
8
9
2

6
.
2
9

1
.
3
8

4
.
9
5
1

1
.
8

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0

4
.
3
1
6
3

6
.
3
3

1
.
5
3
6
3

7
.
2
3

4
.
6
8
6
3

1
.
9
3

8
.
9
2
6
2

4
.
1

6
.
6
2
6
3

1
.
9
2

0
.
0
7
6
3

6
.
3
3

4
.
4
7
7
3

1
.
0
4

8
.
7
4
0
1

0
.
0

7
.
4
3
6
3

9
.
9
2

3
.
2
9
6
3

0
.
1
3

6
.
8
2
8
3

1
.
9
3

1
.
1
4
6
3

0
.
8
1

7
.
5
2
7
3

6
.
5
3

1
.
3
2
9
3

2
.
7
4

2
.
8
9
3
4

9
.
0
5

0
.
4
9

0
.
8
9

0
.
0
0
1

0
.
0
0
1

8
.
5
8

5
.
5
9

0
.
0
0
1

0
.
0
0
1

4
.
1
7

0
.
1
8

3
.
8
8

6
.
2
9

1
.
7
9

0
.
9
9

0
.
0
0
1

0
.
0
0
1

2
.
0
7

0
.
5
7

5
.
5
7

9
.
1
7

7
.
2
7

5
.
7
7

9
.
4
7

4
.
1
7

5
.
8
7

2
.
7
7

5
.
6
7

1
.
2
7

7
.
4
8

8
.
3
7

1
.
8
6

6
.
6
6

2

3

4

5

2

3

4

5

2

3

4

5

2

3

4

5

2

3

4

5

2

3

4

5

2

3

4

5

2

3

4

5

3
k
n
o
m

3
k
n
o
m

3
k
n
o
m

3
k
n
o
m

1
k
n
o
m

1
k
n
o
m

1
k
n
o
m

1
k
n
o
m

2
k
n
o
m

2
k
n
o
m

2
k
n
o
m

2
k
n
o
m

e
s
u
o
h

e
s
u
o
h

e
s
u
o
h

e
s
u
o
h

e
c
n
a
l
a
b

e
c
n
a
l
a
b

e
c
n
a
l
a
b

e
c
n
a
l
a
b

e
o
t
-
c
a
t
-
c
i
t

e
o
t
-
c
a
t
-
c
i
t

e
o
t
-
c
a
t
-
c
i
t

e
o
t
-
c
a
t
-
c
i
t

l
a
v
e
_
r
a
c

l
a
v
e
_
r
a
c

l
a
v
e
_
r
a
c

l
a
v
e
_
r
a
c

p
k
-
s
v
-
r
k

p
k
-
s
v
-
r
k

p
k
-
s
v
-
r
k

p
k
-
s
v
-
r
k

34

