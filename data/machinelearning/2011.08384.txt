0
2
0
2

v
o
N
7
1

]
T
S
.
h
t
a
m

[

1
v
4
8
3
8
0
.
1
1
0
2
:
v
i
X
r
a

Optimal Sub-Gaussian Mean Estimation in R

Jasper C.H. Lee

Paul Valiant

Brown University
jasperchlee@brown.edu

IAS & Purdue University
pvaliant@gmail.com

November 18, 2020

Abstract

We revisit the problem of estimating the mean of a real-valued distribution, presenting a novel
estimator with sub-Gaussian convergence: intuitively, “our estimator, on any distribution, is as
accurate as the sample mean is for the Gaussian distribution of matching variance.” Crucially,
in contrast to prior works, our estimator does not require prior knowledge of the variance, and
works across the entire gamut of distributions with ﬁnite variance, including those without any
higher moments. Parameterized by the sample size n, the failure probability δ, and the variance
σ2, our estimator is accurate to within σ
, tight up to the 1 + o(1) factor. Our
estimator construction and analysis gives a framework generalizable to other problems, tightly
analyzing a sum of dependent random variables by viewing the sum implicitly as a 2-parameter
ψ-estimator, and constructing bounds using mathematical programming and duality techniques.

(1 + o(1))

2 log 1
δ
n

q

·

1

Introduction

We revisit one of the most fundamental problems in statistics: estimating the mean of a real-valued
distribution, using as few independent samples from it as possible. Our proposed estimator has
convergence that is optimal not only in a big-O sense (i.e. “up to multiplicative constants”), but
tight to a 1 + o(1) factor, under the minimal (and essentially necessary, see below) assumption of
the ﬁniteness of the variance. Previous works, discussed further in Section 2, are either only big-O
tight [12, 20, 1], or require additional strong assumptions such as the variance being known to the
estimator [4] or assumptions that allow for accurate estimates of the variance, such as the kurtosis
(fourth moment) being ﬁnite [4, 7].

1.1 The Model and Main Result

Given a set of i.i.d. samples from a real-valued distribution, the goal is to return, with extremely
high probability, an accurate estimate of the distribution’s mean. Speciﬁcally, given a sample set
X of size n consisting of independent draws from a real-value distribution D, an (ǫ, δ)-estimator
of the mean is a function ˆµ : Rn
δ, the estimate
ˆµ(X) is within ǫ of the true mean µ(D). Namely,

R such that, except with failure probability

→

≤

P(
ˆµ(X)
|

−

µ(D)

| ≤

ǫ)

1

δ

−

≥

(1)

The goal is to ﬁnd the optimal tradeoﬀ between the sample size n, and the error parameters
ǫ and δ, for the distribution D. Fixing any two of the three parameters and minimizing the third

1

 
 
 
 
 
 
−

yields essentially equivalent reformulations of the problem: we can ﬁx ǫ, δ and minimize the sample
complexity n; we can ﬁx δ, n and minimize error ǫ; or we can ﬁx ǫ, n and minimize the failure
probability δ (maximizing the robustness 1

δ).

Perhaps the most standard and well-behaved setting for mean estimation is when the distribu-
tion D is a Gaussian. The sample mean (the empirical mean) is a provably optimal estimator in
our sense when D is Gaussian: for any ǫ, δ > 0, the sample mean µ(X) is an (ǫ, δ)-estimator when
log 1
δ

(all logarithms will be base e); and there is no
given a sample set of size n = (2 + o(1))
(ǫ, δ)- estimator for Gaussians if the constant 2 in the previous expression for the sample size is
changed to any smaller number.

σ2(D)
·
ǫ2

The main result of this paper is an estimator that performs as well, on any distribution with
ﬁnite variance, as the sample mean does on a Gaussian, without knowledge of the distribution or
its variance:

Theorem 1. Estimator 1, given δ, n > 0, deﬁnes a function ˆµ such that with probability at least
1

δ, given a sample set X of size n, yields an estimate with error

−

ˆµ(X)
|

−

µ(D)

| ≤

σ(D)

·

(1 + o(1))

s

2 log 1
δ
n

(0, 0). Furthermore, as evidenced by the Gaussian
Here, the o(1) term tends to 0 as
case, there is no estimator which, under the same settings, produces an error that improves on our
guarantees by more than a 1 + o(1) multiplicative factor.

→

(cid:17)

(cid:16)

log 1
n , δ
δ

We have parameterized the above theorem in terms of ﬁxing the sample size n and the robust-
ness parameter δ and asking for the minimum error ǫ; however, because of the simple functional
form of the bounds of Theorem 1, we can equivalently rephrase it as saying that, for any ǫ, δ,
(a reparameterized) Algorithm 1 is an (ǫ, δ) estimator using (2 + o(1)) σ(D)2
δ samples; or for
ǫ2
σ2(D) )-robust. For each of these
any n, ǫ, Algorithm 1 gives an estimate that is δ = exp(
formulations, the performance is optimal up to the 1 + o(1) factor, as evidenced by the well-known
Gaussian case, as explained above.

nǫ2
(2+o(1))
·

log 1

−

We make the following observations regarding the main (minimal) assumption in the theorem,
namely the ﬁniteness of the variance of the unknown distribution. First, imposing further assump-
tions about the ﬁniteness of higher moments will not yield any improvements to the result, since
matching lower bounds are provided by Gaussians, for which all moments are ﬁnite. Second, as
shown by Devroye et al. [7], relaxing the ﬁnite variance assumption by only assuming, say, the
ﬁniteness of the (1 + β)th moment for some β < 1 will yield strictly worse sample complexity. In
particular, the sample complexity will have an ǫ-dependence that is ω(1/ǫ2). Thus, our result shows
that mean estimation can be performed at a sub-Gaussian rate, with the optimal multiplicative
constant of 2 in the sample complexity, if and only if the variance of the underlying distribution is
ﬁnite.

We also contrast with previous works that attain optimal sub-Gaussian convergence but make
additional assumptions such as the ﬁniteness of the kurtosis (4th moment) [4, 7]. The gap in
assumptions between those works and this work is not only theoretical, but also of practical con-
sequence: power law distributions are known to be good models of certain real-world phenomena,
and for exponents in the range (3, 5], the variance exists, but not the kurtosis.

2

1.2 Our Approach

We brieﬂy describe the main features of our estimator, as a setting for what follows, and to dis-
tinguish it from prior work. At the highest level:
in order to return a δ-robust estimate of the
mean, our estimator “throws out the 1
δ most extreme points in the samples”, and returns the
mean of what remains. More speciﬁcally, outliers are thrown out in a weighted manner, where we
throw out a fraction of each data point, with the fraction proportional to the square of its distance
from a median-of-means initial guess for the mean, where the fraction is capped at 1, and the
proportionality constant is chosen so that the total weight thrown out equals exactly 1
δ . See
Estimator 1 for full details, but we stress here that the estimator is simple to implement—it may
be computed in linear time—and therefore applicable in practice.

3 log 1

3 log 1

The above description is rather diﬀerent from the typical M-estimator/ψ-estimator approach
of Catoni [4] and other works in this area. However, as we see in Section 3, our estimator can be
reinterpreted as a 2-parameter ψ-estimator, and the proof of our main result will crucially rely on
this reformulation.

1.3 Motivation: 3rd-order corrections of the empirical mean

Perhaps the most non-obvious part of our estimator is throwing out exactly 1
δ many samples.
We motivate this quantity in this section, by considering the special case of estimating the mean
of asymmetric—very biased—Bernoulli distributions, which is in some sense an extremal case for
our setting.

3 log 1

−

p). The interesting regime for us is when p is very small, and thus 1

Example 1. Consider the mean estimation problem, given n samples from a Bernoulli distribution
supported on 0 and 1, where the probability of drawing 1 equals some parameter p. Thus the
number of 1s observed is distributed as the Binomial distribution Bin(n, p), of mean np and variance
np(1
1, and the Binomial
distribution is essentially the Poisson distribution P oi(np) of mean and variance λ = np. In this
setting, the mean estimation problem becomes: given a sample k from P oi(np), and the parameters
n and δ, return an estimate that, except with failure probability δ, is as close as possible to p (or
equivalently np). Given a Poisson sample k
P oi(np), returning simply k is a natural estimate
of np; however, since Poisson distributions are slightly skewed, it turns out that one should instead
return the correction k

←

−

≈

p

1

(k

Explicitly, the Poisson distribution has pmf poi(λ, k) = λke−λ

, whose logarithm, using Stirling’s
approximation for the factorial, expanding to 3rd order in k, and dropping lower-order terms in
. The 2nd-order term here corresponds to a Gaussian centered at k = λ of
λ is
variance λ, which is a standard approximation for the Poisson distribution. However, crucially,
the 3rd order term, corresponding to the positive skewness of the Poisson distribution, increases the
pmf to the right of k = λ and decreases it by an essentially symmetric factor to the left.

λ)2
2λ + (k

−
6λ

λ)3

−

k!

−

3 log 1
δ .

−

Seeking a δ-robust estimation of λ from a single sample of k, we are concerned, essentially,
with the interval where the Poisson pmf is greater than δ, or equivalently, where the log pmf is
in the ﬁrst term of the above approximation equals log δ
greater than log δ. The quadratic

λ)2

(k

2λ log 1

±

q

when k = λ
δ , and this interval is centered at the λ. However, crucially, when we
take into account the 3rd-order term, the interval where poi(λ, k)
δ essentially shifts to become
k = 1
δ . Thus, given a single sample, one can δ-robustly estimate the mean of
a Poisson distribution similarly well as the Gaussian of same mean and variance, but only if one
returns the sample minus 1

3 log 1

2λ log 1

δ + λ

q

≥

±

3 log 1
δ .

−
2λ

−

3

3 log 1

Thus, the 1

δ term in our estimator arises essentially from a 3rd order correction to the
sample mean, at least in the special case of Bernoulli distributions. For additional intuition and
motivation about the “3rd order correction” in our estimator, please refer to Appendix A.

1.4 Key Contributions in Our Construction and Analysis

In addition to settling the fundamental sample complexity question of mean estimation, we point out
that the estimator construction and analysis may also be of independent interest. In particular, the
analysis framework—as described below—is generalizable to other problem settings and estimator
constructions.

Our overall analysis framework may be viewed as a Chernoﬀ bound—showing exponentially
small probability of estimation error via bounds on a moment generation function (expectation of
an exponentiated real-valued random variable). However, since we seek to analyze our estimator to
sub-constant accuracy, many standard approaches fail to yield the required resolution. We point
out three crucial components of our approach.

i ψµ(xi, ˆµ, ˆα) and ψα(ˆµ, ˆα) =

First, our estimator (Estimator 1) is not a sum of independent terms, which is fundamental
to standard Chernoﬀ bound approaches, and thus we instead reformulate our estimator as a 2-
parameter ψ-estimator (see Deﬁnition 2). This technique rewrites our estimate ˆµ as the ﬁrst
coordinate of the root (ˆµ, ˆα) of a system of 2 equations ψµ(ˆµ, ˆα) = 0 and ψα(ˆµ, ˆα) = 0, where
i ψα(xi, ˆµ, ˆα) are explicitly sums of a
the functions ψµ(ˆµ, ˆα) =
corresponding function applied to each of the n independent data points in the sample set. Thus we
have bought independence at the price of making the estimator an implicit function, introducing
two new variables. One-dimensional estimators of this form are standard: for example, Catoni’s [4]
mean estimator in the case of known variance is a (1 parameter) ψ-estimator for which he proves
ﬁnite sample concentration. However, adding another dimension—ˆα, a new implicit variable whose
value the estimator will ultimately discard—is less standard, without standard analysis techniques,
yet signiﬁcantly increases the expressive power of such estimators [24]. Our high-level approach is
to ﬁnd carefully chosen linear combinations of the functions ψµ and ψα, each of which is now a sum
of independent terms, and prove Chernoﬀ bounds about these linear combinations.

P

P

Second, even after identifying these linear combinations of ψ functions, the corresponding Cher-
noﬀ bound analysis is diﬃcult to directly tackle. The Chernoﬀ bound analysis, as it turns out,
is essentially equivalent to bounding a max-min optimization problem where the maximization is
over the set of real-valued probability measures with mean 0 and variance 1. In other words, the
max-min optimization problem can be interpreted as having uncountably inﬁnitely many variables.
In order to drastically simplify the problem and make it amenable to analysis, we use convex-
concave programming and linear programming duality techniques to reduce the problem to a pure
minimization problem with a small ﬁnite number of variables, which we can analyze tightly.

We believe that the above two ideas—1) reformulating an estimator as a multi-parameter ψ-
estimator, so as to ﬁnd a proxy of the estimator that is a sum of independent variables, and
2) viewing the corresponding Chernoﬀ bound analysis as an optimization problems and applying
relevant duality techniques—form a general analysis framework which expands the space of possible
estimators that are amenable to tight analysis.

2 Related Work

There is a long history of work on real-valued mean estimation in a variety of models.
In the
problem setting we adopt, where the sole assumption is on the ﬁniteness of the second moment,
the median-of-means algorithm [12, 20, 1] has long been known to have sample complexity tight

4

to within constant multiplicative factors, albeit with a sub-optimal constant. Catoni [4] improved
this sample complexity to essentially optimal (tight up to a 1 + o(1) factor), by focusing on the
special cases where the variance of the underlying distribution is known or the 4th moment is ﬁnite
and bounded (in which case the second moment can be accurately estimated). We stress however
that the ﬁniteness of the 4th moment is nonetheless a much stronger assumption than our minimal
assumption on the ﬁniteness of the variance (see the discussion at the end of Section 1.1).

Moving beyond the original problem formulation, Devroye et al. [7] drew the distinction be-
tween a single-δ estimator, which takes in the robustness parameter δ as input, versus a multiple-δ
estimator, which does not take any δ as input, but still provides guarantees across a wide range
of δ values. In their work, making the same ﬁnite kurtosis assumption as Catoni, they achieved a
multiple-δ estimator with essentially optimal sample complexity, for a wide range of δ values. It
is thus natural and prudent to ask whether a multiple-δ estimator can exist for the entire class of
distributions with ﬁnite variance, for a meaningful range of δ values. Unfortunately, Devroye et
al. [7] showed strong lower bounds answering the question in the negative. Hence, in this work, our
proposed estimator is (and must be) a single-δ estimator, taking δ as input.

Many applications have arisen from the success of sub-Gaussian mean estimation, showing how
to leverage or extend Catoni-style estimators to new settings, achieving sub-Gaussian performance
on problems such as regression, empirical risk minimization, and online learning (bandit settings):
for example see [18, 2, 5, 3].

A separate but closely related line of work is on high dimensional mean estimation. While
estimators generalizing the “median-of-means” construction were found to have statistical con-
vergence tight to multiplicative constants, until recently, such estimators took super-polynomial
time to compute [17]. A recent line of work [10, 6, 15], started by Hopkins [10], thus focuses on
the computational aspect, and brought the computation time ﬁrst down to polynomial time, with
subsequent work bringing it further down to quadratic time using spectral methods.

A recent comprehensive survey by Lugosi and Mendelson [16] explains much of the above works

in greater detail.

Other works have focused on mean estimation in restrictive settings, for example, with diﬀer-
ential privacy constraints. For example, Kamath et al. [14] studied the diﬀerentially private mean
estimation problem in the constant probability regime, and showed strong sample complexity sepa-
rations from our unrestricted setting. Duchi, Jordan and Wainright [8, 9] also studied the problem
under the stricter constraint of local diﬀerential privacy. See the work of Kamath et al. [14] for a
more comprehensive literature review on diﬀerentially private mean estimation.

Part of our tight analysis relies on insights from mathematical programming and duality; see [22]
for a detailed discussion of prior works that use such mathematical programming and duality tools
to either design or analyze statistical estimators [21, 19, 26, 27, 13, 25].

3 Our Estimator

In this section, we present our estimator (Estimator 1), as well as its reformulation as a 2-parameter
ψ-estimator. We then present some perspective and basic structural properties of the estimator
that will serve as a foundation for the analysis to follow.

3.1 Meaning of the Estimator

Consider the expression in Step 3 for the ﬁnal returned value of the estimator, ˆµ = κ + 1
n
κ)(1
computes exactly the sample mean. The factor (1

i(xi −
1
κ)
·
κ)2, 1)) may be thought of as a weight

κ)2, 1)). Without the ﬁnal min expression, the expression κ + 1
n

min(α(xi −

i(xi −
P

−

P

min(α(xi−

−

5

Estimator 1 The Main Estimator

Inputs:

• n independent samples

ﬁnite variance)

xi}
{

• Conﬁdence parameter δ

from the unknown underlying distribution D (guaranteed to have

1. Compute the median-of-means estimate κ: evenly partition the data into log 1

δ groups and

let κ be the median of the set of means of the groups.

2. Find the solution α to the monotonic, piecewise-linear equation

i min(α(xi−

3. Output: ˆµ = κ + 1
n

i(xi −

κ)(1

min(α(xi −

−

κ)2, 1))

P

P

κ)2, 1) = 1

3 log 1

δ

on the ith element, between 0 and 1, where a weight of 1 leaves that element as is, but a weight
towards 0 essentially throws out part of the sample xi and instead defaults to the median-of-means
κ)2, 1)
estimate κ. Thus, rather than either keeping or discarding each entry, the weight min(α(xi −
speciﬁes what fraction of the ith sample to discard.

3 log 1

The condition in Step 2 of Estimator 1 picks α so that the total, weighted, number of discarded
samples equals 1
κ)2, 1) specifying what fraction of each xi to
discard says, essentially, that this fraction should be proportional to the square of the deviation
of xi from the mean estimate κ, capped at 1 so that we do not discard “more than 100% of” any
sample xi.

δ . The expression min(α(xi −

3.2 Structural Properties of the Estimator

→

First, the estimator is “aﬃne invariant” in the sense that, if its input samples

We point out three basic structural properties of Estimator 1 that both shed light on the estimator
itself, and will be crucial to its analysis. We formally state and prove these properties in Appendix B.
undergo an
aﬃne map x
ax+b then its output will be mapped correspondingly. Second, as is well known, the
median-of-means estimate κ of Step 1, while not as accurate as what we will eventually return, is
δ/2, the median-of-means estimate has additive
robust in the sense that, with probability at least 1

−
log 1
n )—proportional to the eventual guarantees of
error from the true mean that is at most O(σ
δ
our estimator, but with somewhat worse proportionality constant. Third, if we temporarily ignore
Step 1, treating κ as a free parameter, we show that the ﬁnal output of the algorithm, ˆµ, varies
very little with κ. Combined with the accuracy guarantees of the median-of-means estimate, the
diﬀerence in the ﬁnal estimate between using the median-of-means as κ versus using the true mean
as κ is inconsequential (a o(1) factor) compared to the total additive error we aim for. Therefore,
for the purposes of analysis, it suﬃces to assume that κ takes the value of the true mean (though
an algorithm could not do this in practice, as the true mean is unknown).

xi}
{

q

·

These structural properties allow us to drastically simplify the analysis: the aﬃne invariance
means it is suﬃcient to show our estimator works for the special case of distributions with mean
0 and variance 1; the second and third properties mean that errors in κ eﬀectively do not matter,
and, for distributions with mean 0, it is suﬃcient to omit Step 1 and instead just analyze the case
where κ = 0.

We point out that Estimator 1 when modiﬁed to set κ = 0 (independently of the samples) is

6

no longer aﬃne invariant, nor is its reformulation as a ψ-estimator in Section 3.3. The structural
properties in this section show that, instead of analyzing the actual estimator (Estimator 1 which is
aﬃne invariant), it suﬃces to analyze this artiﬁcially simpliﬁed, although no longer aﬃne invariant,
estimator which sets κ = 0, on distributions with mean 0 and variance 1. Explicitly, in the rest
of the paper we will show Proposition 3 (Section 4), which analyzes the mean-0 variance-1 case
of the ψ-estimator deﬁned below in Deﬁnition 2; the discussion of this section—made formal in
Appendix B—shows that this proposition implies our main result, Theorem 1.

3.3 Representing a Special Case of Estimator 1 as a ψ-Estimator

As discussed in Section 1.4, our estimator, even its simpliﬁed version with κ = 0, is not a sum
of independent terms, making it diﬃcult to tightly bound its moment generating function, and
hence also diﬃcult to prove its concentration around the true mean using a Chernoﬀ-style bound.
Our solution is to reformulate Estimator 1, with the simplifying assumption that κ = 0, as a
2-parameter ψ-estimator, as deﬁned in Deﬁnition 2. This reformulation deﬁnes our estimate ˆµ
implicitly in terms of two new functions ψµ and ψα that are indeed sums of n independent terms,
each term depending on a single xi. We will use this representation crucially for the concentration
analysis of the estimator.

Deﬁnition 2. Consider Estimator 1 but with Step 1 replaced with “κ = 0”. The estimator can
be equivalently expressed as follows:

1. Input: n independent samples X = x1, . . . , xn

2. Solve for the (unique) pair (ˆµ, ˆα) satisfying ψµ = 0 and ψα = 0, where the functions are

deﬁned as follows:

ψµ(X, ˆµ, ˆα) =

n

Xi=1
n

(cid:0)

xi

1

ˆµ

−

−

min

ˆαx2

i , 1

(cid:0)
ˆαx2

i , 1

(cid:0)
1
3n

−

log

(cid:0)

(cid:1)

(cid:1)(cid:1)(cid:1)
1
δ

(cid:19)

ψα(X, ˆµ, ˆα) =

min

Xi=1 (cid:18)

(Note that ˆα > 0 always)

3. Output: ˆµ from the previous step

We will sometimes omit ˆµ from the arguments of ψα since ˆµ is not used in the deﬁnition of the
function. We will often refer to the pair (ψµ, ψα) as a 2-element vector ψ.

For convenience in the rest of the paper, we deﬁne ˆv

3n ˆα , which we refer to as the
“truncated empirical variance”; this is because, if we modify the ψα = 0 condition by removing
the “truncation” of taking the min with 1, then the resulting condition, when expressed in terms
of ˆv = log(1/δ)
n
and rearranged, is exactly the condition that ˆv is the empirical variance: 1
i=1 x2
i .
n
Thus ˆα may be thought of as a proxy for the empirical variance, as ˆv = log(1/δ)
equals the empirical
variance, except in cases when samples are far enough from 0 that they are “truncated” by the
“min”.

P

3n ˆα

3n ˆα

≡

log(1/δ)

Interestingly, in the case that none of the samples are “truncated”, (and κ = 0), the overall
i , namely, ˆµ is “the empirical
δ times the ratio of the empirical 3rd moment over the

output of the estimator becomes 1
n
mean, corrected by subtracting 1
empirical 2nd moment.”

3n log 1
P

3nˆv x3

i xi −

i xi −

i = 1
n

log(1/δ)

αx3

P

7

Proof that Deﬁnition 2 is equivalent to Estimator 1 when κ is set to 0. Fix a set of samples X =
. We observe that Estimator 1, with the additional simplifying assumption that κ = 0, can be
xi}
{
represented by the following 2 equations.

min(α x2

i , 1) =

1
3

log

1
δ

Xi
ˆµ =

1
n

Xi

xi(1

−

min(α x2

i , 1))

(2)

Estimator 1 solves for α in the ﬁrst line, and uses this α value to compute the estimate ˆµ in the
second line. The two conditions of Equation 2 are equivalent to the two conditions ψα = 0, ψµ = 0
respectively, and thus the two estimators are equivalent.

4 Analyzing our estimator

In this section, we outline the proof of our main theorem, restated as follows.

Theorem 1. Estimator 1, given δ, n > 0, and a sample set X of n independent samples from
δ over the sampling process, yield an estimate ˆµ
distribution D, will, with probability at least 1

ˆµ(X)
|

−

µ(D)

| ≤

σ(D)

·

(1 + o(1))

q

−

2 log 1
δ
n

. Here, the o(1) term tends to 0 as

with error at most

log 1
n , δ
δ

(cid:16)

(cid:17)

(0, 0).

→

The discussion of the structural properties of Estimator 1 in Section 3.2 shows that it is suﬃcient
to instead show that, for any distribution of mean 0 and variance 1, the ψ-estimator of Deﬁnition 2
will return an estimate ˆµ that is close to 0, except with tiny probability. (See Appendix B for
the formal statements of the claims of Section 3.2.) Recall also that, since the ψ-estimator solves
for (ˆµ, ˆα) such that ψ(X, ˆµ, ˆα) = 0 (where X is the sample set) and returns ˆµ, the claim that
the returned estimate will be close to 0 is equivalent to saying that every (ˆµ, ˆα) pair with ˆµ far
from 0 must violate the equation, namely ψ(X, ˆµ, ˆα)
= 0. We thus prove the following proposition
(Proposition 3), to yield Theorem 1. Note that the failure probability in Proposition 3 is δ/2
(instead of δ, as in Theorem 1), accounting for an additional δ/2 probability that the median-of-
means estimate in Step 1 of Estimator 1 fails.

c log log 1
δ
log 1
δ

,

2 log 1
δ
n
(cid:17) q
δ
2 over
−

Proposition 3. There exists a universal constant c > 0 such that, ﬁxing ǫ′ =

1 +

we have that for all distributions D with mean 0 and variance 1, with probability at least 1
the set of samples X, for all ˆµ, ˆα where

> ǫ′ and ˆα > 0, the vector ψ(X, ˆµ, ˆα)

= 0.

(cid:16)

ˆµ
|

|

ˆµ
|

Proposition 3 asks us to show that, with high probability, ψ(X, ˆµ, ˆα) is not at the origin for
any choice of
> ǫ′, ˆα; instead, as a proof strategy, we choose a ﬁnite bounded mesh of ˆµ, ˆα and
show that the function ψ(X, ˆµ, ˆα) is 1) not just nonzero, but far from the origin on this set, 2)
Lipschitz in between mesh elements, and 3) monotonic (in an appropriate sense) outside the mesh
bounds. Step 1), discussed below, contains the most noteworthy part of the proof, a mathematical
programming-inspired bound to help complete a delicate Chernoﬀ bound argument.

|

For simplicity, we reparameterize to work with ˆv

(the “truncated empirical variance”)
instead of ˆα: the mesh we analyze, covering the most delicate region for analysis, will span the
[0.05, 55.5], namely, where the truncated empirical variance ˆv is within a constant factor
interval ˆv
of the true variance of 1. Note that this should not be taken to imply that ˆv
[0.05, 55.5] with high
probability—the truncated empirical variance is not designed to be a good estimate of the variance,

≡

∈

∈

log(1/δ)
3n ˆα

8

6
6
3n ˆα

that we analyze (from the ﬁnite mesh):

merely as a step in robustly estimating the mean; and further, accurate estimates of the variance
are simply impossible in general without further assumptions such as bounds on the distribution’s
3rd or 4th moments. We also want to distinguish our estimator from Catoni’s [4]: Catoni’s estimator
relies on having a high-precision estimate of the variance (to within a 1 + o(1) factor) in order to
achieve the desired performance. By contrast, our estimator is robust against wild inaccuracies of
the (truncated) empirical variance ˆv compared to the true variance of 1. In short, the approach of
our estimator should be viewed as distinct from Catoni’s, since, while Catoni’s estimator relies on
an initial good guess at the variance, ours thrives in the inevitable situations where ˆv is far from 1.
We return to describing our strategy for analyzing the performance of our estimator. For each
ˆµ, ˆv = log(1/δ)
instead of directly showing that, with
δ
2 probability, ψ(X, ˆµ, ˆα) is far from the origin in some direction, we instead linearize this
≥
δ
claim; we prove the stronger claim that there exists a speciﬁc direction d(ˆv) such that with
2
probability, ψ(X, ˆµ, ˆα) is more than
log(1/δ) distance from the origin in direction d (speciﬁcally we
ψ(ˆµ, ˆα), while we upper bound each coordinate of d inversely
lower bound the dot product d(ˆv)
with the Lipschitz coeﬃcients of ψ). The crucial advantage of this reformulation is that, since each
of ψµ, ψα is a sum of n terms, that are each a function of an independent sample xi from D, the
ψ(X, ˆµ, ˆα) is thus also a sum of n independent terms, and thus we ﬁnish the
dot product d(ˆv)
proof with a Chernoﬀ bound, Lemma 4. The Chernoﬀ bound argument itself is standard; however,
to bound the resulting expression requires an extremely delicate analysis that we pull out into a
separate 4-variable inequality expressed as Lemma 7—see the discussion around the lemma for
more details and for motivation of the analysis from a mathematical programming perspective.

−

≥

−

1

1

1

·

·

We state the crucial Chernoﬀ bound (Lemma 4) and the Lipschitz bounds (Lemma 5), and then
use them to prove Proposition 3. We prove Lemmas 4 and 5 in the next section, along with the
statement and proof of the delicate component that is Lemma 7.

Lemma 4. Consider an arbitrary distribution D with mean 0 and variance 1. There exists a

universal constant c where the following claim is true. Fixing ˆµ = ǫ′ =
then for all δ smaller than some universal constant, and for all ˆv
d(ˆv) where dµ ≥

n
log(1/δ) |

0, and both

dα|
|

,
dµ|

∈

are bounded by a universal constant, such that

1 +

c log log 1
δ
log 1
δ

2 log 1
δ
n

,

[0.05, 55.5], there exists a vector

(cid:16)

(cid:17) q

P
Dn  

←

X

d(ˆv)

q

ψ

·

(cid:18)

X, ˆµ = ǫ′, ˆα =

log(1/δ)
3nˆv

>

(cid:19)

1
log 1

δ ! ≥

1

−

δ
log4 1
δ

Furthermore, for ˆv = 0.05 we have dµ =
dα < 0.

q

3.75 log(1/δ)

n

, dα = √3; and for ˆv = 55.5 we have dµ = 0,

Lemma 5. Consider an arbitrary set of n samples X. Consider the expressions ψµ(X, ˆµ, ˆα), ψα(X, ˆα),
reparameterized in terms of ˆv

in place of ˆα. Suppose the equation ψα(X, ˆα) = 0 has a

log(1/δ)
3n ˆα

≡

solution in the range ˆv
∈
Lipschitz with respect to ˆv on the entire interval ˆv
some universal constant c.

[0.05, 55.5]. Then the functions

∈

log(1/δ)

[0.05, 55.5], with Lipschitz constant c log 1

n ψµ(X, ˆµ, ˆα) and ψα(X, ˆα) are
δ for

q

We now prove Proposition 3, which per our previous discussion, implies our main result, The-

orem 1.

Proof of Proposition 3. As in Lemma 4, we ﬁx ǫ′ = (1+
constant.

c log log 1
δ
log 1
δ

2 log 1
δ
n

)
q

, where c is some universal

9

−

−

at least 1

δ
Θ(log 1
δ )
set of n samples X, for all ˆv
log(1/δ)
3nˆv

By symmetry, instead of considering positive and negative ˆµ, it suﬃces to consider the case

−

ˆµ > ǫ′ (as opposed to ˆµ <

ǫ′) and show that this case succeeds with probability at least 1

To prove the claim, we ﬁrst prove a stronger statement on a restricted domain, that with
δ
[0.05, 55.5] there
4 over the randomness of the sample set X, for each ˆv
∈
0 throughout, and,
) > 0, with dµ ≥

probability at least 1
exists a vector d = (dµ, dα) such that d
3.75 log(1/δ)

·
, dα = √3; and for ˆv = 55.5 we have dµ = 0, dα < 0.

for ˆv = 0.05 we have dµ =

ψ(X, ǫ′, ˆα

log(1/δ)
3nˆv

≡

−

We will ﬁrst apply Lemma 4 to each ˆv in a discrete mesh: let M consist of evenly spaced points

between 0.05 and 55.5 with spacing 1/ log3 1

δ (thus with Θ(log3 1

δ ) many points).

By Lemma 4 and a union bound over these Θ(log3 1
(which is at least 1

δ ) points, we have that with probability
δ
4 for δ smaller than some universal constant) over the

q

n

δ
4 .

−

M , there exists a vector d(ˆv) such that d(ˆv)

ψ(X, ˆµ = ǫ′, ˆα

≡
) > 1/ log 1
δ , where d further satisﬁes the desired positivity and boundary conditions, and
n
are bounded by a universal constant. For the rest of the proof, we
log(1/δ) |

dα|
|

,
dµ|

∈

·

where both

will only consider sets of samples X satisfying the above condition.

q

to conclude that both
interval ˆv′ ∈

Now consider an arbitrary ˆv′ ∈
log(1/δ)
3nˆv′

. We wish to extend the dot product inequality to hold also for ˆv′. If ψα 6

M and consider the vector ψ evaluated at ˆα′ =
= 0 then there is
nothing to prove: set dµ = 0 and dα = sign(ψα); otherwise, ψα = 0 means we may apply Lemma 5
n ψµ(X, ˆµ, ˆα′) and ψα(X, ˆµ, ˆα′) are Lipschitz with respect to ˆv′ on the

[0.05, 55.5]

log(1/δ)

\

q

[0.05, 55.5], with Lipschitz constant c log 1

δ for some universal constant c.
M to ˆv′, which by deﬁnition of M is at most 1/ log3 1

0 and both

Consider the closest ˆv

δ away. By
∈
ψ(X, ˆµ = ǫ′, ˆα = log(1/δ)
assumption on X, there exists a vector d such that d
δ , with
,
dµ|
are bounded by a universal constant. Because of the Lipschitz
dµ ≥
bounds on ψ, combined with the bounds on the size of the dµ, dα, we conclude that the Lipschitz
constant of the dot product (treating the vector d as ﬁxed) is O(log 1
δ ). Thus, the large positive
ψ(X, ˆµ = ǫ′, ˆv′) >
dot product at ˆv implies at least a positive dot product nearby at ˆv′: d

n
log(1/δ) |

) > 1/ log 1

dα|
|

q

3nˆv

·

1
log 1

O(log 1
δ )

1
log3 1
δ

> 0, for suﬃciently small δ as given in the proposition statement.

δ −
Having shown the stronger version of the claim for the restriction ˆµ = ǫ′ and ˆv

[0.05, 55.5]
we now extend to the entire domain via three monotonicity arguments. Explicitly, assume the set
of samples X satisﬁes the dot product inequality above with the vector function d(ˆv), where d(ˆv)
satisﬁes the boundary conditions at ˆv = 0.05 and 55.5 speciﬁed in Lemma 4. From this assumption,
we will show that ψ

= 0 for any positive ˆv = log(1/δ)
3n ˆα , and for any ˆµ
First consider ˆv > 55.5 (still ﬁxing ˆµ = ǫ′). The function ψα =
is an increasing function of ˆα, and thus a decreasing function of ˆv
the dot product d
same choice of d as we increase ˆv from 55.5.

ǫ′.
≥
n
3n log 1
i , 1
i=1
δ
log(1/δ)
3n ˆα . Since for ˆv = 55.5,
(cid:0)
(cid:1)
(cid:1)
ψ > 0 with dµ = 0, dα < 0, the dot product will thus remain positive for this

ˆαx2

min

P
≡

−

∈

(cid:0)

1

·

·

≡

log(1/δ)
3nˆv

ψ(X, ǫ′, ˆα
the sums deﬁning ψµ or ψα depends on ˆα (and thus ˆv) only in the factor min(ˆαx2
there is no dependence unless the ﬁrst term attains the min, namely
upper bounded by
in the dot product which have ˆα dependent are simply equal to dµ ˆαx3

Next, for ˆv < 0.05 (again still ﬁxing ˆµ = ǫ′), we analogously show that the dot product of
) with the ﬁxed vector d(0.05) will increase as we decrease ˆv. The ith term in
i , 1). Further,
1/ˆα, which in turn is
log(1/δ) because of our assumption that ˆv < 0.05. Thus, the only ith terms
i (dα + xidµ).
and dα(0.05) = √3 from Lemma 4, the expression
√0.15√3.75 is thus always non-negative, and thus the overall dot product

By our choice of dµ(0.05) =
(dα + xidµ)

i + dα ˆαx2

3.75 log(1/δ)

i = ˆαx2

xi| ≤
|

0.15

√3

p

q

q

n

n

≥

−

10

6
cannot decrease as we send ˆα to

—equivalently, sending ˆv to 0—as desired.

∞
We have thus shown that, for all non-negative ˆα = log(1/δ)

0
whose dot product with ψ(X, ǫ′, ˆα) is greater than 0. We complete the proof by noting that the
only dependence on ˆµ in ψ is that ψµ is (trivially) increasing in ˆµ. Since dµ ≥
0, increasing ˆµ
from ǫ′ will only increase the dot product, and thus the dot product remains strictly greater than
0, implying that ψ(X, ˆµ, ˆα)

, there is a vector d with dµ ≥

= 0 as desired.

3nˆv

5 Proofs of Lemmas 4 and 5

The main purpose of this section is to present and motivate the proof of Lemma 4—since our results
are tight across such a wide parameter space, the resulting inequalities are somewhat subtle. After,
we also present the short proof of Lemma 5.

Lemma 4. Consider an arbitrary distribution D with mean 0 and variance 1. There exists a

universal constant c where the following claim is true. Fixing ˆµ = ǫ′ =
then for all δ smaller than some universal constant, and for all ˆv
d(ˆv) where dµ ≥

n
log(1/δ) |

0, and both

dα|
|

,
dµ|

∈

are bounded by a universal constant, such that

1 +

c log log 1
δ
log 1
δ

2 log 1
δ
n

,

[0.05, 55.5], there exists a vector

(cid:16)

(cid:17) q

P
Dn  

←

X

d(ˆv)

q
ψ

·

(cid:18)

X, ˆµ = ǫ′, ˆα =

log(1/δ)
3nˆv

>

(cid:19)

1
log 1

δ ! ≥

1

−

δ
log4 1
δ

(3)

Furthermore, for ˆv = 0.05 we have dµ =
dα < 0.

q

3.75 log(1/δ)

n

, dα = √3; and for ˆv = 55.5 we have dµ = 0,

We start the analysis via standard Chernoﬀ bounds on the complement of the probability in
Equation 3 via Lemma 6, before pausing to discuss how mathematical programming and duality
insights lead to the formulation of the crucial Lemma 7; we then complete the proof.

Lemma 6. Consider an arbitrary distribution D with mean 0 and variance 1. For all suﬃciently
small δ, for any ˆµ, ˆα and vector d = (dµ, dα), we have

P
Dn  

←

d

·

X

ψ (X, ˆµ, ˆα)

1
log 1

δ ! ≤

≤

2

e−

dµ ˆµ+dα

(cid:18)

1

3n log 1

δ E
x

←

D

(edµx(1
−

min(ˆαx2,1))

−

dα min(ˆαx2,1))

n

(cid:19)

Proof. We upper-bound the probability by exponentiating the negation of both sides of the expres-
sion inside the probability, and then using Markov’s inequality:

P
Dn  

←

X

d(ˆv)

·

ψ(X, ˆµ, ˆα)

1
log 1

δ !

≤

d(ˆv)
·

e−

ψ(X,ˆµ, ˆα)

1
log 1
δ

e−

≥

(cid:19)

(cid:18)

Dn
(cid:16)
(e−

Dn

= P
X
←
2 E
≤
X
←
= 2 E
D
x
←
e−

= 2

(cid:18)

dµ ˆµ+dα

d(ˆv)
·

ψ(X,ˆµ, ˆα)

e−

by Markov’s inequality; and e

1
log(1/δ)

≤

2 for suﬃciently small δ

d(ˆv)
·

ψ(x,ˆµ, ˆα))n by independence

(cid:17)

1

3n log 1

δ E
x

←

D

(edµx(1
−

min(ˆαx2,1))

−

dα min(ˆαx2,1))

n

substituting the deﬁnition of ψ

(cid:19)

(4)

11

6
5.1 Mathematical Programming and Duality Analysis

In order to show Lemma 4, we aim to ﬁnd bounds on the failure probability that are as strong
as possible. Appealing to Lemma 6 that we have just proven, recall that, as in the standard
Chernoﬀ bound methodology, we are still free to choose the parameters dµ, dα, which we do so as
to minimize the resulting bound on the failure probability. Phrased abstractly, the goal is, for the
ˆµ, ˆα of Lemma 4, to show that, for any distribution D of mean 0 and variance 1, there is a choice
d = (dµ, dα) that makes Equation 4 suﬃciently small. Phrased as an optimization problem, our
goal is to evaluate (or tightly bound):

max
D

min
d=(dµ,dα)

e−

dµ ˆµ+dα

1

3n log 1

δ E
x

←

D

(edµx(1
−

min(ˆαx2,1))

−

dα min(ˆαx2,1))

(5)

where D ranges over distributions of mean 0 and variance 1.

We will use convex-concave programming and linear programming duality to signiﬁcantly sim-
plify the max-min program in Equation 5 before we dive into the part of analysis that is ad hoc
for this problem. We wish to emphasize here again that the steps of 1) writing an estimator
as a multi-parameter ψ-estimator and ﬁnding an analogous lemma to our Lemma 4, then 2) us-
ing mathematical programming duality to simplify the Chernoﬀ bound analysis, are a framework
generalizable for tightly analyzing other estimators.

For simplicity of exposition, assume that we restrict the support of D to some suﬃciently ﬁne-
grained ﬁnite set, meaning that the maximization in Equation 5 is now ﬁnite-dimensional, albeit
an arbitrarily large ﬁnite number. For each support element x, let Dx be a variable representing
the probability of choosing x under distribution D. The expectation component of Equation 5 may
now be expressed as sum that is a linear function in the variables Dx:

max
D

min
d=(dµ,dα)

e−

dµ ˆµ+dα

1

3n log 1

δ

edµx(1
−

min(ˆαx2,1))

−

dα min(ˆαx2,1)

(6)

Dx ·

x
X

Using the standard max-min inequality (a form of weak duality in optimization), we have
that Equation 6 is upper bounded by swapping the maximization and minimization (Equation 7),
meaning that the vector d no longer depends on the distribution D.

min
d=(dµ,dα)

max
D

e−

dµ ˆµ+dα

1

3n log 1

δ

edµx(1
−

min(ˆαx2,1))

−

dα min(ˆαx2,1)

(7)

Dx ·

x
X

Crucially, however, Equation 7 is not just an upper bound on Equation 6, but is in fact equal to it,
due to Sion’s minimax theorem [23]. To apply Sion’s minimax theorem, it suﬃces to check that 1)
both d and D are constrained to be in convex sets, at least one of which is compact, 2) the objective
is convex in d and 3) concave in the variables Dx. For the ﬁrst condition, we note that the set of
distributions on a ﬁnite domain is compact. The objective is convex in d since the objective is the
sum of exponentials that are each linear in d. And the objective is concave in Dx because it is in
fact a linear function of D.

The guarantee of Sion’s minimax theorem means that we may work with Equation 7 instead of
Equation 6 without sacriﬁcing tightness in our analysis. This justiﬁes why we are free to choose
d = (dµ, dα) in Lemma 4 that does not depend on the distribution D.

To further simplify the problem in Equation 7, we note again that both the objective and the
constraints on D are linear in the variables Dx, meaning that the inner maximization is in fact
a linear program. We can then apply linear programming (strong) duality to yield the following
equivalent optimization (Equation 8). We note that, as above, for the purposes of upper bounding

12

Equation 5, it suﬃces to only use weak duality. Strong duality however guarantees that this step
does not introduce slack into the analysis.

The three variables V, M, S in the inner minimization below are the dual variables corresponding
to the three constraints on distribution D originally: that D has variance 1, mean 0, and total
probability mass 1.

min
d=(dµ,dα)

min
V,M,S

V + S

for all x: V x2 + M x + S

dµ ˆµ+dα

e−

1

3n log 1

δ +dµx(1

−

≥

min(ˆαx2,1))

−

dα min(ˆαx2,1)

(8)

∈

We have thus reduced the inﬁnite-dimensional optimization problem of Equation 5 to the ﬁve-
dimensional problem of Equation 8 (or six dimensions, if we include the universal quantiﬁcation
R), a signiﬁcant simpliﬁcation. We bound Equation 8 by explicitly choosing values for
for x
d = (dµ, dα), V, M, S as functions of ˆα, n, log 1
δ , and showing that they jointly satisfy the constraint
of Equation 8, for all x. We factor out the terms in the exponential that do not depend on x; we
δ with
make the variable substitutions y
≡
dependence on the single variable ˆv; taking the log of both sides (and swapping sides) yields an
expression that is recognizable in the following lemma, where the multipliers of 1, y, y2 respectively
on the right hand side are essentially our choices of S, M, V :

to replace dependence on ˆα, n, log 1

√ ˆαx and ˆv

log(1/δ)
3n ˆα

≡

Lemma 7. For all ˆv

R : ay

1

y

∀

∈

∈

−

[0.05, 55.5], there exist a > 0 and b such that

min

y2, 1

min

y2, 1

b

·

−

log

≤

1 + ay + y2ˆv(

−

3 +

a√6
√ˆv −

b)

!

(cid:0)

[C, C ′] and b
where a
a = 0.75, b = √3 works.

∈

(cid:0)

[
−

∈

(cid:1)(cid:1)

(cid:0)

(cid:1)

C ′, C ′] for positive constants C, C ′. Further, for ˆv = 0.05, the pair

We emphasize that the application of Lemma 7 in the proof of Lemma 4 below is straightforward,
though ﬁnding the particular form of Lemma 7 is not. Further, one would not seek a result of the
form of Lemma 7 without the guarantees of this section, derived via duality and mathematical
programming, showing that “results of the form of Lemma 7 encompass the full power of the
Chernoﬀ bounds of Equation 4.” See the end of Section 5.2 for the proof of Lemma 7.

5.2 Proof of Lemma 4

We now prove Lemma 4 by combining the Chernoﬀ bound analysis of Lemma 6 with the inequality
from Lemma 7. We point out that the proof below is direct, without any reference to duality or
mathematical programming; however, the discussion of Section 5.1 was crucial to discovering the
right formulation for Lemma 7. We prove Lemma 7 at the end of the section.

Lemma 4. Consider an arbitrary distribution D with mean 0 and variance 1. There exists a

universal constant c where the following claim is true. Fixing ˆµ = ǫ′ =
then for all δ smaller than some universal constant, and for all ˆv
d(ˆv) where dµ ≥

n
log(1/δ) |

0, and both

dα|
|

,
dµ|

∈

are bounded by a universal constant, such that

1 +

c log log 1
δ
log 1
δ

2 log 1
δ
n

,

[0.05, 55.5], there exists a vector

(cid:16)

(cid:17) q

P
Dn  

←

X

d(ˆv)

q
ψ

·

(cid:18)

X, ˆµ = ǫ′, ˆα =

log(1/δ)
3nˆv

>

(cid:19)

1
log 1

δ ! ≥

1

−

δ
log4 1
δ

Furthermore, for ˆv = 0.05 we have dµ =
dα < 0.

q

3.75 log(1/δ)

n

, dα = √3; and for ˆv = 55.5 we have dµ = 0,

13

 
Proof. Start with the bound on the probability of failure given by Lemma 6:

2

e−

dµ ˆµ+dα

(cid:18)

1

3n log 1

δ E
x

←

D

(edµx(1
−

min(ˆαx2,1))

−

dα min(ˆαx2,1))

n

(cid:19)

∈

For ˆv

[0.05, 55.5) we bound the exponential inside the expectation via the exponential of
Lemma 7; we also use Lemma 7 to choose dµ, dα for us (the ˆv = 55.5 case is covered at the end).
Namely, in Lemma 7 use ˆv as given, substitute x√ˆα
as always), and choose
dµ ≡

≡
b—in particular, for ˆv = 0.05 this gives dµ(0.05) = 0.75√ ˆα = 0.75

a√ ˆα, and dα ≡

y (where ˆα

log(1/δ)
3nˆv

3nˆv =

log(1/δ)

≡

. Thus the failure probability is bounded by

q

3.75 log(1/δ)

n

q

dµ ˆµ+dα

e−

2





= 2

e−

dµ ˆµ+dα

≤

≤

≤

dµq

2e−

δ
log4 1
δ

1

3n log 1

δ E
x
D
←
y=x√ ˆα

1 + ay + y2ˆv

3 +

 −

a√6
√ˆv −

b

1

3n log 1

δ

1 +

log 1
δ
3n  −

3 + 3dµ

2n

log(1/δ) −

s

n

!!


n

dα

!!!

since D has mean 0, variance 1

dµ ˆµ+dα

2

e−

1

3n log 1

δ +

log 1
δ
3n (cid:16)−

3+3dµq

2n
log(1/δ) −

dα(cid:17)

n

!

since 1 + z

≤

ez for any z

2n

log(1/δ) c log log 1

δ −

log 1
δ

substituting ˆµ = ǫ′ =

1 +

2 log 1
δ
n

c log log 1
δ
log 1
δ ! s
a
√3ˆv

=

n
log(1/δ)

as desired, for large enough c, since dµ

r

is greater than some positive constant.

We prove the ˆv = 55.5 case now. We choose dµ = 0 and dα =

of Equation 4 to yield

4, substituting into the bound

−

2

e−

(cid:18)

4

3n log 1

δ E
x

←

D

(e4 min(ˆαx2,1))

n

(cid:19)

(1 + 54ˆαx2)n

2δ4/3 E
≤
D
←
= 2δ4/3(1 + 55.5ˆα)n

x

since for y

[0, 1], e4y

∈

1 + 54y

≤

since D has variance 1

2δ4/3en
·
≤
= 2δ4/3δ−

54 log(1/δ)
3·55.5n

since 1 + z

54
3·55.5

2δ1.009

≤

ez, substituting def. of ˆα

≤

which is bounded as desired for small enough δ.

We now prove Lemma 7.

Lemma 7. For all ˆv

R : ay

1

y

∀

∈

(cid:0)

∈

−

where a
[C, C ′] and b
a = 0.75, b = √3 works.

∈

∈

(cid:0)

[
−

[0.05, 55.5], there exist a > 0 and b such that

min

y2, 1

min

y2, 1

b

·

−

log

≤

1 + ay + y2ˆv(

−

(cid:1)(cid:1)

(cid:0)

(cid:1)

3 +

a√6
√ˆv −

b)

!

(9)

C ′, C ′] for positive constants C, C ′. Further, for ˆv = 0.05, the pair

14

 
 
 
 
 
 
Proof. We ﬁrst prove the special case of 1) ˆv = 0.05, before moving to the general case of 2)
ˆv
(0.05, 55.5]. We note that our choice of a(ˆv), b(ˆv) is not continuous in ˆv at 0.05, but the
usage of the lemma does not require any continuity. We choose a, b at the edge case ˆv = 0.05 for
convenience.

∈

1) For ˆv = 0.05, we choose a = 0.75, b = √3. This special case of Equation 9 simpliﬁes to:

−
(cid:1)(cid:1)
3+ a√6
√ˆv −
[
−
∈

R : 0.75y

1

y

∀

∈

min

−

y2, 1

√3

min

y2, 1

·

≤

log

1 + 0.75y + 0.174y2

−

(cid:0)

(cid:0)
(where 0.174 is a lower bound on ˆv(
−
analyzed in many ways. For the range y
which in this range is at least .75y
expression that the left hand side reduces to in this range, 0.75y
range, y /
1, 1], the left hand side is the constant
∈
in the argument of the right hand side, 1 + 0.75y + 0.174y2, always exceeds e−

b) ). This is a 1-dimensional bound and can be easily
1, 1] : the right hand side is at least log(1 + 0.75y),
.75y2, which is easily shown to be greater than the polynomial
0.75y3. For the remaining
√3y2
√3, and it is easy to check that the quadratic

[
−

−

−

−

−

(cid:1)

(cid:1)

(cid:0)

(cid:0)

2) To show Equation 9 for the rest of the range of ˆv

positive root of the quadratic equation √ˆv(a2
the motivation for this choice shortly. For now, note that the deﬁnition of a implies a
otherwise √ˆv(a2

12) + √6a would be greater than 0.

12) + √6a = 0 and let b = 3

−

−

≤

√3.
(0.05, 55.5], we choose a to be the
a2/2—we will see
√12, for

∈

Our proof will analyze the sign of the derivative with respect to y of the diﬀerence between the

right and left hand sides of Equation 9. For the critical region

y
|

| ≤

1 this derivative equals:

a + 2yˆv(

−
1 + ay + y2ˆv(

3 + a√6

b)

√ˆv −
3 + a√6

−

√ˆv −

a + 3ay2 + 2by

b) −

The crucial step is to choose a to be the positive root of the quadratic equation √ˆv(a2

√6a = 0 and let b = 3

−

a2/2, after which Equation 10 miraculously factors as

(10)

12) +

−

1
3a2 ·

y(y + 2

a )(y + 2

a

a −
3 )y + ( 4

a
3 )2
a2 −

1
3 )

y2 + ( 4

a −

9 (a2

From this expression for the derivative, it is straightforward to read oﬀ its sign. The discriminant
of the quadratic in the denominator is 1
12) > 0, meaning the denominator is always positive.
The squared term in the numerator cannot aﬀect the overall sign. Thus the sign of the derivative
equals the sign of y(y + 2
a ), meaning that the diﬀerence between the right and left side of Equation 9
is monotonically increasing for y > 0, and unimodal for y < 0, having non-positive derivative for
[ 2
a , 0] and nonnegative derivative for smaller y. Thus to show the inequality holds for all
y
1.
1, 1] it suﬃces to check it at y = 0 and y =
y

[
−
The y = 0 case is trivial as both sides of Equation 9 equal 0.
For y =

1, Equation 9, after expressing both √ˆv and b in terms of a becomes

∈
∈

−

−

−

a2
2 −

3

≤

log

2 + a

−

(cid:18)

36

−

a2

12

(cid:19)

−

(11)

For a

∈
approximation, √12
−
√12
becomes the claim that 6(1

[0, √12), the inverse of the rational expression inside the log is bounded by its linear
, which is between 0 and 1, Equation 11

. Calling this a new variable z = √12
−
√12
log z, which is easily veriﬁed for z

(0, 1].

a

a

z)2
3
−
Lastly, we show Equation 9 for
y
|
|
of the inequality is the constant value

−

≤ −
> 1. Reexpressing b and √ˆv in terms of a, the left hand side
2 (independent of y), while the right hand side

3 + a2

b =

∈

−

−

15

a2 y2). Analyzing the quadratic inside the log shows that the right hand side has

is log(1 + ay + 3a2
12
−
a minimum of a2
12 , attained at y =
When the location of this minimum, y =

a2
6a .
−

−

12

12

a2

1, 1], then because
this quadratic is monotonic to either side of the minimum, the fact that we have already proven
1 implies the inequality holds for all y further from 0.
Equation 9 for y =

6a , is inside the interval [
−

−

−

The remaining case is when the minimum is not in [

1, 1], namely,

−
a < 1.59; since a is monotonic in ˆv, a is at least its value when ˆv = 0.05, namely a
log a2
Equation 9 thus reduces to showing that, for a
trivially implied, substituting z = a2
the claim.

−
[1.003, 1.59] we have a2
log z for z

12 , by the inequality 6z

2 −
∈

≤

−

−

∈

3

a2
6a <
−

1, meaning
1.003.
12 , which is
3
[0.083, 0.22], yielding

≤

≥

±

12

5.3 Proof of Lemma 5

Lemma 5. Consider an arbitrary set of n samples X. Consider the expressions ψµ(X, ˆµ, ˆα), ψα(X, ˆα),
reparameterized in terms of ˆv

in place of ˆα. Suppose the equation ψα(X, ˆα) = 0 has a

log(1/δ)
3n ˆα

≡

solution in the range ˆv
∈
Lipschitz with respect to ˆv on the entire interval ˆv
some universal constant c.

[0.05, 55.5]. Then the functions

∈

log(1/δ)

[0.05, 55.5], with Lipschitz constant c log 1

n ψµ(X, ˆµ, ˆα) and ψα(X, ˆα) are
δ for

q

Proof. Consider the ˆv derivative of ψα(X, ˆµ, ˆα

n
i=1
(cid:16)
(cid:17)
1
ˆv ˆαx2
i or 0, depending on which term
i , 1
The ˆv derivative of min
P
−
in the min is the smallest, and in either case has magnitude at most 1
ˆv min(ˆαx2
(cid:17)
i , 1). Thus the overall
ˆv derivative of ψα(X, ˆµ, ˆα) has magnitude at most 1
i , 1). Since, we are guaranteed that
ˆv
3 log 1
[0.05, 55.5], we thus have that the derivative is within a

log(1/δ)
3nˆv
log(1/δ)
3nˆv2 x2

δ for some ˆv

i min(ˆαx2

i , 1
(cid:17)

n
i=1 min

3n log 1

3nˆv x2

3nˆv x2

is either

log(1/δ)

log(1/δ)

= 1

ˆαx2

i =

i , 1

min

) =

−

−

≡

(cid:16)

(cid:16)

1

.

δ

constant factor of this across the entire range, as desired.
P
Similarly, consider the ˆv derivative of ψµ(X, ˆµ, ˆα) =

(cid:1)

(cid:0)

term of this is the ˆv derivative of min(ˆαx3
xi ≤
Since

1/ˆα, and thus the magnitude of this derivative may be bounded by 1
ˆαx2
ˆv√ ˆα
n
is bounded by a constant times log 1
δ (as in the last paragraph), and 1
i=1 min
ˆv√ˆα

ˆαx2

i , 1

P

p

P

−

(cid:0)

1

−

min

ˆαx2

. The ith
xi
i , 1
−
1
ˆv ˆαx3
i or 0 depending on whether
(cid:0)
(cid:1)(cid:1)(cid:1)
n
.
i=1 min

i , 1

(cid:0)

n
i=1
i , xi), which is either
(cid:0)

P

is
(cid:1)

∈

ˆµ

P

bounded by a constant times
is bounded by a constant times log 1
δ , as desired.
q

=

1
√ˆv ˆα

3n

(cid:0)

(cid:1)

log(1/δ) , the magnitude of the derivative of

log(1/δ)

n ψµ(X, ˆµ, ˆα)

q

16

References

[1] Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the

frequency moments. J. Comput. Syst. Sci, 58(1):137–147, 1999.

[2] Christian Brownlees, Emilien Joly, G´abor Lugosi, et al. Empirical risk minimization for heavy-

tailed losses. Ann. Stat., 43(6):2507–2536, 2015.

[3] S´ebastien Bubeck, Nicolo Cesa-Bianchi, and G´abor Lugosi. Bandits with heavy tail. IEEE

Trans. Inf. Theory, 59(11):7711–7717, 2013.

[4] Olivier Catoni. Challenging the empirical mean and empirical variance: a deviation study.

Ann. I. H. Poincare-PR, 48(4):1148–1185, 2012.

[5] Olivier Catoni and Ilaria Giulini. Dimension-free pac-bayesian bounds for matrices, vectors,

and linear least squares regression. arXiv:1712.02747, 2017.

[6] Yeshwanth Cherapanamjeri, Nicolas Flammarion, and Peter L Bartlett. Fast mean estimation

with sub-gaussian rates. In Proc. COLT ’20, pages 786–806, 2019.

[7] Luc Devroye, Matthieu Lerasle, Gabor Lugosi, and Roberto I. Oliveira. Sub-gaussian mean

estimators. Ann. Stat, 44(6):2695–2725, 2016.

[8] John C Duchi, Michael I Jordan, and Martin J Wainwright. Local privacy and statistical

minimax rates. In Proc. FOCS ’13, pages 429–438, 2013.

[9] John C Duchi, Michael I Jordan, and Martin J Wainwright. Minimax optimal procedures for

locally private estimation. J. Am. Stat. Assoc, 113(521):182–201, 2018.

[10] Samuel B Hopkins et al. Mean estimation with sub-gaussian rates in polynomial time.

Ann. Stat., 48(2):1193–1213, 2020.

[11] Daniel Hsu and Sivan Sabato. Loss minimization and parameter estimation with heavy tails.

J. Mach. Learn. Res, 17(1):543–582, 2016.

[12] Mark R Jerrum, Leslie G Valiant, and Vijay V Vazirani. Random generation of combinatorial

structures from a uniform distribution. Theor. Comput. Sci, 43:169–188, 1986.

[13] Jiantao Jiao, Kartik Venkat, Yanjun Han, and Tsachy Weissman. Minimax estimation of

functionals of discrete distributions. IEEE Trans. Inf. Theory, 61(5):2835–2885, 2015.

[14] Gautam Kamath, Vikrant Singhal, and Jonathan Ullman. Private mean estimation of heavy-
tailed distributions. volume 125 of Proc. COLT ’20, pages 2204–2235. PMLR, 09–12 Jul 2020.

[15] Zhixian Lei, Kyle Luh, Prayaag Venkat, and Fred Zhang. A fast spectral algorithm for mean

estimation with sub-gaussian rates. In Proc. COLT ’20, pages 2598–2612, 2020.

[16] Gabor Lugosi and Shahar Mendelson. Mean estimation and regression under heavy-tailed

distributions–a survey, 2019.

[17] G´abor Lugosi, Shahar Mendelson, et al. Sub-Gaussian estimators of the mean of a random

vector. The annals of statistics, 47(2):783–794, 2019.

[18] Stanislav Minsker. Uniform bounds for robust mean estimators. arXiv:1812.03523, 2018.

17

[19] Ankur Moitra and Michael Saks. A polynomial time algorithm for lossy population recovery.

In Proc. FOCS’13, pages 110–116, 2013.

[20] A.S. Nemirovsky and D.B. Yudin. Problem Complexity and Method Eﬃciency in Optimization.

Wiley, 1983.

[21] Yury Polyanskiy, Ananda Theertha Suresh, and Yihong Wu. Sample complexity of population

recovery. In Proc. COLT’17, volume 65, 2017.

[22] Yury Polyanskiy and Yihong Wu. Dualizing Le Cam’s method, with applications to estimating

the unseens. arXiv:1902.05616, 2019.

[23] Maurice Sion. On general minimax theorems. Pac. J. Math, 8(1):171–176, 1958.

[24] Leonard A. Stefanski and Dennis D. Boos. The calculus of M-estimation. The American

Statistician, 56(1):29–38, 2002.

[25] Gregory Valiant and Paul Valiant. The power of linear estimators. In Proc. FOCS’11, pages

403–412, 2011.

[26] Yihong Wu and Pengkun Yang. Minimax rates of entropy estimation on large alphabets via

best polynomial approximation. IEEE Trans. Inf. Theory, 62(6):3702–3720, 2016.

[27] Yihong Wu, Pengkun Yang, et al. Chebyshev polynomials, moment matching, and optimal

estimation of the unseen. Ann. Stat, 47(2):857–883, 2019.

18

A Additional “3rd Order” Motivation for Our Estimator

In this appendix, we give additional motivation of our estimator as a “3rd order correction” to the
sample mean.

Suppose (for this section only), as in [4], that one knows the variance σ2(D) of the distribution

in question, or has a good estimate of it.

n

i=1(xi+c(xi)) for some function c : R

Example 2. Given samples x1, . . . , xn from a distribution of mean 0 and variance 1 and bounded
higher moments, suppose our goal is to construct a slight variant of the empirical mean that will
robustly return an estimate that is close to 0, the true mean; we consider estimates of the form
R. Explicitly, given a bound b, we want our estimate to
1
n
b, with as high probability as possible. For simplicity we will consider the positive case,
be between
namely, bounding Px1,...,xn( 1
b). With a view towards deriving a Chernoﬀ bound,
n
we rearrange, multiply by an arbitrary positive constant α, and exponente inside the probability to
yield that this probability equals Px1,...,xn(exp(α
1); by Markov’s inequality,
this probability is at most Ex1,...,xn(exp(α
b))), for our choice of α > 0. We set
α = b. Since each xi is independent, this probability becomes Ex(exp(b(x + c(x)

i(xi + c(xi)
P

i(xi + c(xi))

i(xi + c(xi)

b)))n.

b))

P

P

→

−

≥

≥

±

−

Considering the empirical estimator, where c(xi) = 0, we thus have that the probability the
b2)), where this expression

empirical mean estimate exceeds b is at most the nth power of Ex(exp(bx
can be expanded to 3rd order as

−

P

−

b2

e−

1 + b E(x) +

(cid:18)

b2 E(x2) +

1
2

1
6

b3 E(x3) + O(x4)

(cid:19)

As we assumed the data distribution has mean 0 and variance 1, we can simplify the above expression
to

b2

e−

1 +

b2 +

b3 E(x3) + O(b4)

1
2

1
6

(cid:18)

(cid:19)

Ignoring, for the moment, the 3rd or higher-order terms, this expression is e−
b2/2, whose nth power equals e−

≈
b2n/2, which is exactly the bound one would expect for the standard
e−
Gaussian case, of the probability that the empirical mean of n samples is more than b from the true
value. However, the 3rd order term is a crucial obstacle here, as the third moment E(x3) could be
of either sign, skewing either the left tail or right tail to have substantially more mass than in our
benchmark of the Gaussian case.

(1 + 1

2 b2)

b2

We thus choose a correction function c(xi) so as to cancel out this 3rd-order term and improve
6 b3 E(x3) in the 3rd-order expansion of our
1
6 x3b2, yielding a bound on the failure

to cancel out the term 1
b))), we replace x by x

the estimate in this regime:
Chernoﬀ bound Ex(exp(b(x
−
probability of the nth power of

−

b2

e−

1 +

(cid:18)

1
2

b2 + O(b4)

= e−

b2/2+O(b4)

(cid:19)

as desired.

b2n/2 equal δ, and thus the correction

For the sake of clarity, we can change variables, letting the leading term of our probability
1
6 x3b2 becomes c(x)
δ , meaning the
bound e−
−
correction amounts essentially to a 3rd moment correction, split n ways and scaled by the same
3 log 1
1

δ of our main algorithm.

3 log 1

n x3 1

≡ −

We explicitly relate this estimator to Estimator 1 by pointing out that, when none of the samples
3nˆv may be expressed in

xi are “truncated” by Estimator 1 (namely, ˆαx2

1 always), then ˆα

log(1/δ)

1

≡

i ≤

19

terms of the empirical variance ˆv; taking κ = 0 for simplicity, the returned estimate will be 1
n
αx3
the empirical variance is the true variance, 1.

i xi−
δ , which equals the above-derived “3rd-order corrected estimator” when

i xi −

3 log 1

i = 1
n

1
ˆvn x3
i

P

1

P

In the above example we showed that Chernoﬀ bounds for the empirical mean deteriorate for
distributions with large 3rd moments (skew), and that adding a 3rd-order correction to the empirical
mean corrects for this, leaving essentially “Gaussian-like” performance. These calculations motivate
several features of Estimator 1—including the 1
δ parameter, and the 3rd-order terms in the
expression for ˆµ—even though the overall form of Estimator 1 is rather diﬀerent, as it must work
in all regimes and not just in the cartoon asymptotic regime considered in this example.

3 log 1

B Proposition 3 implies Theorem 1

For completeness’ sake, we explicitly state and prove the properties described in Section 3.2, which
combine to show that Proposition 3 implies Theorem 1.

Lemma 8. Suppose X is a set of samples in R. Then for any δ > 0 and any scale a > 0 and shift
b,

ˆµ(aX + b, δ) = a ˆµ(X, δ) + b

where ˆµ denotes the output of Estimator 1.

The above lemma follows trivially from the fact that the median-of-means estimate also respects
shift and scale in the input samples, and that α is chosen in Step 2 of Estimator 1 so that min(α(xi −
κ)2, 1) does not depend on the aﬃne parameters a, b.

Fact 9 ([11]). For any distribution D with mean µ and standard deviation σ, the median-of-means
estimate κ, on input n samples, satisﬁes

P

κ
|



µ

|

−

> O

σ



s

log 1
δ
n 

δ



≤


Lemma 10. Consider a ﬁxed sample set X of size n, and a conﬁdence parameter δ. Let e(X, δ, κ)
denote Estimator 1 but where Step 1 is omitted and κ is instead considered as an input. Then,







d e(X, δ, κ)
d κ

= O



s

log 1
δ
n 

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Fact 9 shows that, except with δ probability, the median-of-means estimate is within O

(cid:12)
(cid:12)
(cid:12)
(cid:12)





σ

log 1
δ
n

(cid:18)

q

(cid:19)

of the true mean, and multiplying this by the Lipschitz constant O

log 1
δ
n

from Lemma 10 shows

that the change in output of Algorithm 1, between using the median-of-means versus setting κ = 0,

(cid:18)q

(cid:19)

= o

σ

log 1
δ
n

. This discrepancy is therefore a o(1) fraction of the

has magnitude O

σ

2

log 1
δ
n

q
additive error guaranteed by Theorem 1.

q

(cid:18)

!

We now prove Lemma 10.

(cid:19)

20

 
Proof. We compute the derivatives with respect to α and κ of the ˆµ (computed in Step 3 of
Estimator 1), and the expression on the left hand side of Step 2, which we denote ν
i min(α(xi−
κ)2, 1) = 1, all derivatives are 0, so we adopt
κ)2, 1). We note that for terms where min(α(xi −
κ)2 < 1. Thus we
the notation “Σ<” to denote summing only over those indices i for which α(xi −
have

P

≡

α(xi −

κ)

κ)2

dν
dκ

dν
dα

dˆµ
dκ

dˆµ
dα

<
X
(xi −
1
n

= 2

=

<
X
= 1 +

=

1
n

−

<
X

<
X
(xi −

κ)3

(
−

1 + 3α(xi −

κ)2)

Recall that α is deﬁned implicitly so as to make the expression ν = 1

if we change κ at a rate of 1, then α also changes at rate
overall derivative of the estimate with respect to changing κ equals dˆµ
from the derivatives computed above.

−

(cid:14)

dκ −

dν
dκ

3 log 1

δ ; thus in Estimator 1,
dν
dα to keep ν unchanged. Thus, the
dν
dα . We bound this

dˆµ
dα

dν
dκ

(cid:14)
<” is at most 1

3 log 1

To bound dˆµ

dκ , we note that the number of indices not in the sum “

δ because
each such i contributes 1 to the left hand side of the condition in Step 2 of Estimator 1 and the right
hand side equals 1
δ . The
remaining part of dˆµ
3 log 1
1
itself, and thus is at most 1

dκ are bounded as 1+ 1
n times the corresponding terms in ν

δ . Thus the initial terms of dˆµ
κ)2 is 3

3 log 1
dκ , namely 1
n log 1

dκ = O( 1

3n log 1

n log 1

< 3α(xi −
δ . Thus dˆµ
P

We now bound the remaining term
√α , we may bound dˆµ

xi −
|
2/√α. Combining this, with the other derivatives and the bound α
κ
|
the previous paragraph yields:

dα , involving a 3rd moment term, by the simpler
3 log 1

dν
dα . Since for each index i in “
dˆµ
dα | ≤
P
<(xi −

<” we have
1
xi −
< |
n
κ)2 from
P

δ ).

dˆµ
dα

| ≤

<(

dν
dκ

P

P

1)

≤

−

≤

−

≤

(cid:14)

κ

n

n

1

1

1

|

δ

δ

(cid:14) P

dˆµ
dα

dν
dκ

dν
dα

2√α
n

≤

<(xi −

κ)
<(xi −
P

<(xi −
κ)2

κ)2

2

1

3 log 1
n

δ

≤

q

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:30)

P
pP
where the last inequality is Cauchy-Schwarz applied to the sequence (xi −

P

P

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

4 log 1
δ
3n

<(xi −
<(xi −
κ) and the all-1s sequence.

κ)
κ)2 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ s

21

