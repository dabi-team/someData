1
2
0
2

t
c
O
8
1

]

R
C
.
s
c
[

1
v
5
3
6
9
0
.
0
1
1
2
:
v
i
X
r
a

A ground-truth dataset of real security patches

Soﬁa Reis
INESC-ID & IST/U.Lisbon, Portugal
sofia.o.reis@tecnico.ulisboa.pt

Rui Abreu
INESC-ID & FEUP, Portugal
rui@computer.org

Abstract

Training machine learning approaches for vulnerability identiﬁcation and pro-
ducing reliable tools to assist developers in implementing quality software—free
of vulnerabilities—is challenging due to the lack of large datasets and real data.
Researchers have been looking at these issues and building datasets. However,
these datasets usually miss natural language artifacts and programming language
diversity. We scraped the entire CVE details database for GitHub references and
augmented the data with 3 security-related datasets. We used the data to create
a ground-truth dataset of natural language artifacts (such as commit messages,
commits comments, and summaries), meta-data and code changes. Our dataset in-
tegrates a total of 8057 security-relevant commits—the equivalent to 5942 security
patches—from 1339 different projects spanning 146 different types of vulnerabil-
ities and 20 languages. A dataset of 110k non-security-related commits is also
provided. Data and scripts are all available on GitHub. Data is stored in a .CSV
ﬁle. Codebases can be downloaded using our scripts. Our dataset is a valuable
asset to answer research questions on different topics such as the identiﬁcation of
security-relevant information using NLP models; software engineering and security
best practices; and, vulnerability detection and patching; and, security program
analysis.

1

Introduction

Research surrounding security patches understanding [11], identiﬁcation [12, 14, 19, 20, 3, 7, 4, 9, 21]
and automated repair [8, 13] are hot topics in software security. However, empirically validating
and reproducing these approaches is challenging due to the lack of widely accepted and easy-to-use
databases of real vulnerabilities [10].

Accessing this kind of data is not an easy task for several reasons: 1) many security patches are not
publicly available; 2) developers do not use MITRE’s convention (e.g., CVE-2017-4971) to identify
vulnerabilities in commit messages; 3) commit messages are poorly written and do not provide
enough/any information. Previous research has shown that it is possible to ﬁnd more vulnerability-
ﬁxing commits either using keywords on commits messages [17, 16] or training and applying machine
learning algorithms [21, 7, 18].

With our data, we aim to provide a ground-truth dataset—real security patches previously validated
by experts—capable of supporting research using natural language, code changes, and codebases. We
scraped the Common Vulnerabilities Exposures (CVE) Details database [1] for references to GitHub
commits, i.e., code changes used to patch CVEs. For each CVE, we collected CVE ID, CWE ID,
Vulnerability Type, CVE Severity Score, Summaries and GitHub references. We augmented this
data with other 3 datasets that also contain vulnerabilities and the URL links to security patches:
Secbench [17, 16], Pontas et al. [15] and Big-Vul [5]. We crossed the data from datasets with the
descriptive data obtained from CVE Details to complete the information regarding each CVE. After
merging and cleaning the data (e.g., remove duplicates and links for branches already commits ahead

35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.

 
 
 
 
 
 
of the patch version), we collected the metadata of each GitHub commit: message commit, author,
date, comments, ﬁles, code changes and commit parents.

Our dataset integrates a total of 8057 security-relevant commits—the equivalent to 5942 secu-
rity patches—from 1339 different projects spanning 146 different types of vulnerabilities (CWEs)
and 20 languages. We provide natural language artifacts and code changes. Codebases can be
pulled/downloaded using our scripts. For machine learning purposes, we also provide a dataset of
110161 security-unrelated commits. The negative dataset can be extended or even built from scratch
with our scripts. Our dataset is a valuable asset to answer research questions on different topics
such as detection of security-relevant information; commits classiﬁcation; software engineering and
security best practices; understand security patches, their impact, and risks; vulnerability detection
and patching; and generic security program analysis.

Why is our dataset important? Our dataset is different from previous work in the following points:

Secbench (2017) [17, 16] is a dataset of 676 single-commit security patches—the patch and vulnera-
ble version are available for each case. Patches span 51 different classes (CWEs) and 18 languages.
Data was collected by crawling 113 different open-source projects and applying regular expressions
on commit messages to detect vulnerabilities ﬁxes (e.g., fix xss). Our dataset integrates more
security patches, natural language artifacts, code changes and a negative dataset for machine learning
purposes.

VulniOSS (2018) [6] provides results of code metrics and release data of 17738 software vulnerabili-
ties. We do not use this dataset because they do not provide commit id/sha(s). Our dataset instead
provides commit data, code changes, and natural language artifacts.

Pontas et al. (2019) [15] is a manually curated dataset of 624 Java vulnerabilities. The authors
tracked the CVE updates and mapped the CVE IDs manually to the code versions on GitHub. The
dataset only provides the commit IDs, projects, and CVE IDs. We provide a dataset with more
programming languages and more features regarding the CVEs, commits, and code changes.

Big-Vul (2020) [5] is a dataset of code changes and CVE summaries for 3755 vulnerabilities in
C/C++. Authors scraped the CVE Details database and searched the resulting CVE entries on
open-source repositories to map the respective codebases. We scraped the CVE Details database
as well and found more 2224 security-relevant commits. Our dataset also provides cases for other
programming languages and natural language artifacts.

The contributions of our work are the following ones:

Ground-truth Dataset: We organized, cleaned and extended previous research work by producing
a larger dataset of vulnerabilities—validated by experts—with new features: ﬁles extension and
programming languages. The dataset contains the details of each CVE, code changes information
and natural language artifacts (CVEs summaries, commits messages and commits comments).

Toolset: Both the dataset and tools implemented to scrape, clean, and manipulate the data are
available in the GitHub repository [2].

2 Data Collection

Our dataset was built using the following methodology:

(1) CVE Details Scraping: We scraped the entire CVE Details [1] website by year. For each year,
we obtained the entire list of published CVEs and scraped the webpage of each CVE in the list. We
saved all the data provided on the webpage. However, we only integrate the following features; CVE
ID, CWE ID, CVE Severity, Summary and GitHub references. We ﬁltered the results to only entries
with links/references to GitHub. References not pointing for speciﬁc versions of the repository were
removed (e.g., https://github.com/branch_x_x) because we were not able to ensure that the
branch was still on the patch version. The scripts are available in the scraper/ folder.

(2) Fusing Existing Datasets: We fused 3 datasets of security patches with the CVEs obtained in
the previous step (CVE Details Scraping): Secbench [17, 16], Pontas et al. [15] and Big-Vul [5].
Table 1 shows the initial distribution of commits and patches of each dataset and their license. After
fusing the data, duplicates and entries for other source code hosting websites were removed. We

2

crossed the fused data with the CVE details data to complete the CVE entries with CWEs and
summaries. The scripts are available in the scripts/ folder.

Table 1: Distribution of commits before and after phase (2)

Dataset

Before (2)

After (2)

License

CVE details [1]
Secbench [17]
Pontas et al. [15]
Big-Vul [5]

Commits
4529
676
1282
4529

Patches Commits

4183
676
624
3755

2224
659
1127
4047

Patches
1816
659
565
3433

-
MIT
Apache 2.0
MIT

(3) Extracting Meta-Data: We used the GitHub API to retrieve the meta-data of each commit:
message, author, date, changed ﬁles, comments, and parents. Each comment has an author, date,
and body text. Each ﬁle has a path, number of additions, number of deletions, number of changes,
and status. We built a script to add 2 features to the dataset: extension of the ﬁles and programming
languages. We inferred programming languages from the extension ﬁles.

(4) Generate Negative Dataset: We used the GitHub API to retrieve 50 random security-unrelated
commits per project of the positive dataset. We cleaned the commits using security-related keywords
such as “secur”, “patch”, “cve”, “vuln”, “attack”. This dataset integrates only two columns: commit
link and commit message. The scripts are available in the negative/ folder.

Table 2: Description of the positive data collected (security patches)
CVE Details

Column Name (.CSV)
cve_id
project
sha
cwe_id
score

Column Name (.CSV)
ﬁles

language
github
parents
date
author
ext_ﬁles
lang

Description
The common vulnerabilities and exposures identiﬁer.
GitHub project name.
Commit key or identiﬁer of the version in the project repository.
The common weakness enumeration identiﬁer.
Severity score of the vulnerability.
Code Changes

Description
Set of ﬁles changed by the patch. Information schema: path,
additions, deletions, changes, status.
Programming Language.
Commit link.
Commit keys for previous software version.
Date of the changes.
Author of the changes.
Extension of the ﬁles.
Programming Language.

Natural Language Artifact

Column Name (.CSV)
summary
message
comments

Description
Summary of the vulnerability.
Commit message.
Comments set. Information schema: author, date and body.

3 Dataset Description

In total, we collected 8057 security-relevant commits—the equivalent to 5942 security patches—from
1339 different projects spanning 146 different types of vulnerabilities (CWEs) and 20 languages. The
dataset integrates 16 different columns of data for each commit. Table 2 describes in more detail
each feature of the dataset. We provide scripts to retrieve the CVE Details data (scraper folder); to
fuse, clean and manipulate the data (scripts folder); to augment data by adding features such as
the extension ﬁles and the programming language in which the commits were implemented the data
(scripts/add_features.py script); and, scripts to generate the negative dataset, i.e., the dataset

3

Figure 1: Distribution of commits per programming language. The top-3 of most prevalent program-
ming languages are C/C++ (3944 commits), Java (1369 commits) and PHP (1350 commits).

with non-security related commits (negative folder). Jupyter notebooks are also provided to show
the statistics presented in this section (notebooks folder).

Figure 1 shows the number of commits implemented in each language for the 20 languages available
in our dataset. The top-3 of most prevalent programming languages are C/C++ (3944 commits), Java
(1369 commits) and PHP (1350 commits). Security-relevant commits are less prevalent amongst
Erlang (3), Lua (5) and Vala (3)—less than 15 commits were obtained per language.

Figure 2 shows the amount of commits distributed by the top-20 of vulnerabilities (CWEs). Our
dataset comprises commits to patch 146 different types of CWEs. The most common commits patch
vulnerabilities such as the CWE-79: Improper Neutralization of Input during Web (870 commits),
CWE-20: Improper Input Validation (712 commits), CWE-119: Improper Restriction of Operations
within the Bound of a Memory Buffer (705 commits), CWE-200: Exposure of Sensitive Information
to an Unauthorized Actor (419 commits), CWE-125: Out-of-bounds Read (380 commits). These
represent 38.1% of the entire dataset.

Figure 3 shows the number of commits used to patch software vulnerabilities per year. The security-
relevant commits of this dataset were published from 1999 until 2021. The number of security-relevant
commits between 2016 and 2019 are the most predominant ones.

Code ﬁles
Test ﬁles
Changelogs/News
Readme
Total of Files

Table 3: Files and code changes descriptive data
Files
26032
6770
933
131
33866
Code Churn
1113450
379203
1492653

Additions (+++)
Deletions (- - -)
Total of Changes

76.87%
19.99%
2.75%
0.39%
100%

74.60%
25.40%
100%

4

Figure 2: Distribution of commits for the top-20 of vulnerabilities (CWEs). The dataset comprises
commits to patch 146 different types of CWEs.

Figure 3: Distribution of commits per year (from 1999 to 2021). The number of commits between
2016 and 2019 are the most predominant ones.

5

(a) CVE Summary

(b) Commit Message

(c) Developers Comments

Figure 4: Wordclouds for each type of natural language artifact: (a) CVE Summary (col: summary),
(b) Commit Message (col: message), and, (3) Developers Comments (col: comments)

Table 3 shows the distribution of ﬁles and code changes of the positive dataset. A total of 8057
security-related commits were performed across 33866 different ﬁles: 76.87% are code ﬁles, 19.99%
are test ﬁles, 2.75% are changelogs and news ﬁles; and, 0.39% are readme ﬁles. To patch the total
of 5942 security patches integrated into this dataset, 1113450 lines of code were added, and 379203
lines of code were deleted. In total, more than 1M of changes were performed.

Figure 4 shows wordclouds for the different natural language artifacts: CVE summaries (Figure 4a),
messages (Figure 4b) and comments (Figure 4c). Each wordcloud shows the importance of the
words used in our artifacts. CVE summaries are brief explanations of vulnerabilities and possible
attacks—words such as “attacker”, “remote”, “service”, “allows” and “user” are the most common.
While security-relevant commit messages usually describe an action such as ﬁxing a vulnerability.
Figure 4b supports this by showing words such as “ﬁx”, “issue”, “review”, and “change”. Comments
are usually performed by developers, project contributors or users asking for information on the
vulnerability/code. The most important words are “issue”, “commit”, “CVE” “thank” and “github”.
Overall, wordclouds reﬂect the natural language artifacts goals.

A dataset of security-unrelated commits (negative dataset) was generated from the projects integrated
in the dataset of security patches. For each project in the positive dataset, we collected 50 random
commits. Non-security related keywords were leveraged to clean the dataset. In total, we selected
110161 random non-security related commits from the projects in the positive dataset. Each entry
of the negative dataset represents a non-security-related commit. For each commint, the dataset
integrates 1) the github link for the code changes and 2) the commit message of the change.

Both datasets are available on our GitHub repository in the comma-separated values (CSV) format
(dataset folder). Scripts to download the codebases of the commits are provided. The jupyter
notebooks used to report this analysis are also available.

4 Future research and Improvements

One of the issues in the ﬁeld of security program analysis is the lack of easy-to-use, well-organized
and diverse datasets of security patches. Our dataset provides a larger dataset considering commits for
20 programming languages, 146 types of vulnerabilities and 3 different types of artifacts: meta-data,
natural language data and codebases. This dataset can be used in several vulnerability related research
topics such as:

Identiﬁcation and classiﬁcation of security-relevant commits. This is a recent hot topic in the
software engineering ﬁeld. Machine learning models to identify and classify automatically this kind
of commits are being created [21, 18]. These models aim to fasten software releases and notify
developers when new vulnerabilities are reported. Our natural language artifacts can be used to train
deep learning approaches based on text mining and natural language processing to detect this type of
commits.

Vulnerability detection and patching. Over the past 2 decades, a lot of research as been developed
in the vulnerability detection ﬁeld [12, 14, 19, 20, 3, 7, 4, 9, 21]. More recently, research in
vulnerability patching has emerged [8, 13]. However, many of these techniques are not well tested

6

due to the lack of code samples and do not cover some types of vulnerabilities. Deep learning
approaches have a huge potential to tackle the later issue, but due to the lack of data to well train and
test the models, these approaches do not succeed. We provide a large dataset of real data capable of
boosting research in these ﬁelds.

Understand the impact and risks of security patches. Researchers performed a large scale empiri-
cal study to understand security patches characteristics [11]. However, there is a low understanding
on the impact and risks of security patches on software. Our scripts allow the user to download the
codebases of the commits, previous commit(s), and the diff between both. With code-centric infor-
mation, researchers can better understand security patches and their features. New neural networks
can be trained with this data to detect and patch vulnerabilities. This kind of data can also be used
to create models using software properties to predict the risk of patches on companies and software
development.

Explore if the best practices of software engineering and security are being used. Code evolves
and teams change. Assessing the quality of commits and reports may be crucial to make development
easier for everyone. Our natural language artifacts can be used to explore if best practices are
being used. Code changes data can also be leveraged to better understand if the best practices of
development are being used.

5 Limitations and Challenges

Our dataset integrates security patches without a CVE assigned. Thus, we were not able to get
the CVE Details meta-data for these cases: 10% of the rows are missing descriptive data, e.g., the
vulnerability identiﬁer (CVE ID), summary, type of vulnerability (CWE ID), and score. On the other
hand, our methodology to infer the programming language from the extension ﬁles involved in the
commits failed on classifying less than 5% of the entire dataset. Thus, some commits might not have
a programming language associated. Both issues will be improved/addressed in the future.

6 Conclusions

We fused 4 different security-related data sources and created a ground-truth dataset of natural
language artifacts, meta-data, and code changes. Our dataset integrates a total of 8057 security-
relevant commits—the equivalent to 5942 security patches—from 1339 different projects spanning
146 different types of vulnerabilities and 20 languages. In addition, this dataset provides a negative
dataset of 110k non-security-related commits. Data and scripts are available on GitHub. Data is
stored in a .CSV ﬁle and codebases can be download using our scripts. Our goal is to extend this
dataset in the future by crossing the entire CVEs list with github information. An interactive website
to navigate the data is under development. Our dataset is a valuable asset to answer research questions
on different topics such as the identiﬁcation of security-relevant commits; software engineering
and security best practices; understand security patches, their impact, and risks; and, vulnerability
detection and patching.

7 Acknowledgments

This material is based upon work supported by the scholarship number SFRH/BD/143319/2019 from
Fundação para a Ciência e Tecnologia (FCT).

References

[1] Cve details: The ultimate security vulnerability datasource. https://www.cvedetails.com/.

Accessed: October 20, 2021.

[2] Github repository. https://github.com/TQRG/security-patches-dataset. Accessed:

October 20, 2021.

[3] CIFUENTES, C., AND SCHOLZ, B. Parfait: Designing a scalable bug checker. In Proceedings
of the 2008 Workshop on Static Analysis (New York, NY, USA, 2008), SAW ’08, Association
for Computing Machinery, p. 4–11.

7

[4] EVANS, D., AND LAROCHELLE, D. Improving security using extensible lightweight static

analysis. IEEE Software 19, 1 (2002), 42–51.

[5] FAN, J., LI, Y., WANG, S., AND NGUYEN, T. N. A c/c++ code vulnerability dataset with code
changes and cve summaries. In Proceedings of the 17th International Conference on Mining
Software Repositories (New York, NY, USA, 2020), MSR ’20, Association for Computing
Machinery, p. 508–512.

[6] GKORTZIS, A., MITROPOULOS, D., AND SPINELLIS, D. Vulinoss: A dataset of security
vulnerabilities in open-source systems. In 2018 IEEE/ACM 15th International Conference on
Mining Software Repositories (MSR) (2018), pp. 18–21.

[7] HOVEMEYER, D., AND PUGH, W. Finding bugs is easy. SIGPLAN Not. 39, 12 (Dec. 2004),

92–106.

[8] HUANG, Z., LIE, D., TAN, G., AND JAEGER, T. Using safety properties to generate vulnera-
bility patches. In 2019 IEEE Symposium on Security and Privacy (SP) (2019), pp. 539–554.

[9] JOVANOVIC, N., KRUEGEL, C., AND KIRDA, E. Pixy: a static analysis tool for detecting web
application vulnerabilities. In 2006 IEEE Symposium on Security and Privacy (S P’06) (2006),
pp. 6 pp.–263.

[10] JUST, R., JALALI, D., AND ERNST, M. D. Defects4j: A database of existing faults to enable
controlled testing studies for java programs. In Proceedings of the 2014 International Symposium
on Software Testing and Analysis (New York, NY, USA, 2014), ISSTA 2014, Association for
Computing Machinery, p. 437–440.

[11] LI, F., AND PAXSON, V. A large-scale empirical study of security patches. In Proceedings of
the 2017 ACM SIGSAC Conference on Computer and Communications Security (New York,
NY, USA, 2017), CCS ’17, Association for Computing Machinery, p. 2201–2215.

[12] LI, Z., ZOU, D., XU, S., OU, X., JIN, H., WANG, S., DENG, Z., AND ZHONG, Y. Vuldeep-
ecker: A deep learning-based system for vulnerability detection. Proceedings 2018 Network
and Distributed System Security Symposium (2018).

[13] LIN, Z., JIANG, X., XU, D., MAO, B., AND XIE, L. Autopag: Towards automated software
patch generation with source code root cause identiﬁcation and repair. In Proceedings of the
2nd ACM Symposium on Information, Computer and Communications Security (New York, NY,
USA, 2007), ASIACCS ’07, Association for Computing Machinery, p. 329–340.

[14] PONTA, S. E., PLATE, H., AND SABETTA, A. Beyond metadata: Code-centric and usage-based

analysis of known vulnerabilities in open-source software, 2018.

[15] PONTA, S. E., PLATE, H., SABETTA, A., BEZZI, M., AND DANGREMONT, C. A manually-
curated dataset of ﬁxes to vulnerabilities of open-source software. In 2019 IEEE/ACM 16th
International Conference on Mining Software Repositories (MSR) (2019), pp. 383–387.

[16] REI, S., AND ABREU, R. A database of existing vulnerabilities to enable controlled testing

studies. Int. J. Secur. Softw. Eng. 8, 3 (July 2017), 1–23.

[17] REIS, S., AND ABREU, R. Secbench: A database of real security vulnerabilities. In SecSE@

ESORICS (2017), pp. 69–85.

[18] SAWADOGO, A. D., BISSYANDÉ, T. F., MOHA, N., ALLIX, K., KLEIN, J., LI, L., AND

TRAON, Y. L. Learning to catch security patches, 2020.

[19] YAMAGUCHI, F., GOLDE, N., ARP, D., AND RIECK, K. Modeling and discovering vulnera-
bilities with code property graphs. In 2014 IEEE Symposium on Security and Privacy (2014),
pp. 590–604.

[20] ZHOU, Y., LIU, S., SIOW, J., DU, X., AND LIU, Y. Devign: Effective vulnerability identiﬁca-

tion by learning comprehensive program semantics via graph neural networks, 2019.

[21] ZHOU, Y., AND SHARMA, A. Automated identiﬁcation of security issues from commit
messages and bug reports. In Proceedings of the 2017 11th Joint Meeting on Foundations
of Software Engineering (New York, NY, USA, 2017), ESEC/FSE 2017, Association for
Computing Machinery, p. 914–919.

8

