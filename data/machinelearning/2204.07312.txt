2
2
0
2

r
p
A
5
1

]

C
O
.
h
t
a
m

[

1
v
2
1
3
7
0
.
4
0
2
2
:
v
i
X
r
a

Structural Analysis of Branch-and-Cut and the Learnability of
Gomory Mixed Integer Cuts

Maria-Florina Balcan∗

Siddharth Prasad†

Tuomas Sandholm‡

Ellen Vitercik§

Abstract

The incorporation of cutting planes within the branch-and-bound algorithm, known as
branch-and-cut, forms the backbone of modern integer programming solvers. These solvers
are the foremost method for solving discrete optimization problems and thus have a vast ar-
ray of applications in machine learning, operations research, and many other ﬁelds. Choosing
cutting planes eﬀectively is a major research topic in the theory and practice of integer pro-
gramming. We conduct a novel structural analysis of branch-and-cut that pins down how every
step of the algorithm is aﬀected by changes in the parameters deﬁning the cutting planes added
to the input integer program. Our main application of this analysis is to derive sample com-
plexity guarantees for using machine learning to determine which cutting planes to apply during
branch-and-cut. These guarantees apply to inﬁnite families of cutting planes, such as the family
of Gomory mixed integer cuts, which are responsible for the main breakthrough speedups of
integer programming solvers. We exploit geometric and combinatorial structure of branch-and-
cut in our analysis, which provides a key missing piece for the recent generalization theory of
branch-and-cut.

1 Introduction

Integer programming (IP) solvers are the most widely-used tools for solving discrete optimization
problems. They have numerous applications in machine learning, operations research, and many
other ﬁelds, including MAP inference [22], combinatorial auctions [33], natural language process-
ing [23], neural network veriﬁcation [11], interpretable classiﬁcation [37], training of optimal decision
trees [9], and optimal clustering [30], among many others.

Under the hood, IP solvers use the tree-search algorithm branch-and-bound [26] augmented
with cutting planes, known as branch-and-cut (B&C). A cutting plane is a linear constraint that
is added to the LP relaxation at any node of the search tree. With a carefully selected cutting
plane, the LP guidance can more eﬃciently lead B&C to the globally optimal integral solution.
Cutting planes, speciﬁcally the family of Gomory mixed integer cuts which we study in this paper,
are responsible for breakthrough speedups of modern IP solvers [14].

Successfully employing cutting planes can be challenging because there are inﬁnitely many cuts
to choose from and there are still many open questions about which cuts to employ when. A growing
body of research has studied the use of machine learning for cut selection [7, 8, 19, 34]. In this
paper, we analyze a machine learning setting where there is an unknown distribution over IPs—for

∗Computer Science Department, Machine Learning Department, Carnegie Mellon University. ninamf@cs.cmu.edu
†Computer Science Department, Carnegie Mellon University. sprasad2@cs.cmu.edu
‡Computer Science Department, Carnegie Mellon University, Optimized Markets, Inc., Strategic Machine, Inc.,

Strategy Robot, Inc. sandholm@cs.cmu.edu

§Department of Electrical Engineering and Computer Sciences, UC Berkeley. vitercik@berkeley.edu

1

 
 
 
 
 
 
example, a distribution over a shipping company’s routing problems. The learner receives a training
set of IPs sampled from this distribution which it uses to learn cut parameters with strong average
performance over the training set (leading, for example, to small search trees). We provide sample
complexity bounds for this procedure, which bound the number of training instances suﬃcient to
ensure that if a set of cut parameters leads to strong average performance over the training set,
it will also lead to strong expected performance on future IPs from the same distribution. These
guarantees apply no matter what procedure is used to optimize the cut parameters over the training
set—optimal or suboptimal, automated or manual. We prove these guarantees by analyzing how
the B&C tree varies as a function of the cut parameters on any IP. By bounding the “intrinsic
complexity” of this function, we are able to provide our sample complexity bounds.

Figure 1: Facility location with 40 locations and 40 clients. Samples generated by perturbing a
base facility location IP.

Figure 2: Facility location with 80 locations, 80 clients, and random Euclidean distance costs.

Figures 1 and 2 illustrate the need for distribution-dependent policies for choosing cutting
planes. We plot the average number of nodes expanded by B&C as a function of a parameter µ
that controls its cut-selection policy, as we detail in Appendix A. In each ﬁgure, we draw a training
set of facility location integer programs from two diﬀerent distributions. In Figure 1, we deﬁne the
distribution by starting with a uniformly random facility location instance and perturbing its costs.
In Figure 2, the costs are more structured: the facilities are located along a line and the clients

2

have uniformly random locations. In Figure 1, a smaller value of µ leads to small search trees, but
in Figure 2, a larger value of µ is preferable. These ﬁgures illustrate that tuning cut parameters
according to the instance distribution at hand can have a large impact on the performance of
B&C, and that for one instance distribution, the best parameters for cut evaluation can be very
diﬀerent—in fact opposite—than the optimal parameters for another instance distribution.

The key challenge we face in developing a theory for cutting planes is that a cut added at
the root remains in the LP relaxations stored in each node all the way to the leaves, thereby im-
pacting the LP guidance that B&C uses to search throughout the whole tree. Tiny changes to
any cut can thus completely change the entire course of B&C. At its core, our analysis therefore
involves understanding an intricate interplay between the continuous and discrete components of
our problem. The ﬁrst, continuous component requires us to characterize how the solution the LP
relaxation changes as a function of its constraints. This optimal solution will move continuously
through space until it jumps from one vertex of the LP tableau to another. We then use this char-
acterization to analyze how the B&C tree—a discrete, combinatorial object—varies as a function
of its LP guidance.

1.1 Our contributions

Our ﬁrst main contribution (Section 3) addresses a fundamental question: how does an LP’s solution
change when new constraints are added? As the constraints vary, the solution will jump from vertex
to vertex of the LP polytope. We prove that one can partition the set of all possible constraint
vectors into a ﬁnite number of regions such that within any one region, the LP’s solution has a
clean closed form. Moreover, we prove that the boundaries deﬁning this partition have a speciﬁc
form, deﬁned by degree-2 polynomials.

We build on this result to prove our second main contribution (Section 4), which analyzes how
the entire B&C search tree changes as a function of the cuts added at the root. To prove this result,
we analyze how every aspect of B&C—the variables branched on, the nodes selected to expand, and
the nodes fathomed—changes as a function of the LP relaxations that are computed throughout
the search tree. We prove that the set of all possible cuts can be partitioned into a ﬁnite number
of regions such that within any one region, B&C builds the exact same search tree.

This result allows us to prove sample complexity bounds for learning high-performing cutting
planes from the class of Gomory mixed integer (GMI) cuts, our third main contribution (Section 5).
GMI cuts are one of the most important families of cutting planes in the ﬁeld of integer program-
ming. Introduced by Gomory [16], they dominate most other families of cutting planes [15], and
are perhaps most directly responsible for the realization that a branch-and-cut framework is nec-
essary for the speeds now achievable by modern IP solvers [3]. A historical account of these cuts is
provided by Cornu´ejols [14]. The structural results from Section 4 allow us to understand the “in-
trinsic complexity” of B&C’s performance as a function of the GMI cuts it uses. We quantify this
notion of intrinsic complexity using pseudo-dimension [32], which then implies a sample complexity
bound.

1.2 Related research

Learning to cut. This paper helps develop a theory of generalization for cutting plane selection.
This line of inquiry began with a paper by Balcan et al. [8], who studied Chv´atal-Gomory cuts
for (pure) integer programs (IPs). Unlike that work, which exploited the fact that there are only
ﬁnitely many distinct Chv´atal-Gomory cuts for a given IP, our analysis of GMI cuts is far more
involved.

3

The main distinction between our analysis in this paper and the techniques used in previous
papers on generalization guarantees for integer programming [5, 7, 8] can be summarized as follows.
Let µ be a (potentially multidimensional) parameter controlling some aspect of the IP solver (e.g.
a mixture parameter between branching rules or a cutting-plane parameter). In previous works, as
µ varied, there were only a ﬁnite number of states each node of branch-and-cut could be in. For
example, in the case of branching/variable selection, µ controls the additional branching constraint
added to the IP at any given node of the search tree. There are only ﬁnitely many possible
branching constraints, so there are only ﬁnitely many possible “child” IPs induced by µ. Similarly,
if µ represents the parameterization for Chv´atal-Gomory cuts [12, 17], since Balcan et al. [8] showed
that there are only ﬁnitely many distinct Chv´atal-Gomory cuts for a given IP, as µ varies, there
are only ﬁnitely many possible child IPs induced by µ at any stage of the search tree. However, in
many settings, this property does not hold. For example if µ = (α, β) controls the normal vector
and oﬀset of an additional feasible constraint αT x ≤ β, there are inﬁnitely many possible IPs
corresponding to the choice of (α, β). Similarly, if µ controls the parameterization of a GMI cut,
there are inﬁnitely many IPs corresponding to the choice of µ (unlike Chv´atal-Gomory cuts). In
this paper, we develop a new structural understanding of B&C that is signiﬁcantly more involved
than the structural results in prior work.

This paper ties in to a broader line of research that provides sample complexity bounds for

algorithm conﬁguration [e.g., 6, 18]. A chapter by Balcan [4] provides a comprehensive survey.

There have also been several papers that study how to use machine learning for cut selection
from an applied perspective [19, 34]. In contrast, the goal of this paper is to provide theoretical
guarantees.

Sensitivity analysis of integer and linear programs. A related line of research studied the
sensitivity of LPs, and to a lesser extent IPs, to changes in their parameters. Mangasarian and
Shiau [28] and Li [27], for example, show that the optimal solution to an LP is a Lipschitz function
of the right-hand-side of its constraints but not of its objective. Cook et al. [13] study how the
set of optimal solutions to an IP changes as the objective function varies and the right-hand-side
of the constraints varies. This paper ﬁts in to this line of research as we study how the solution
to an LP varies as new rows are added. This function is not Lipschitz, but we show that it is
well-structured.

2 Notation and branch-and-cut background

Integer and linear programs. An integer program (IP) is deﬁned by an objective vector c ∈ Rn,
a constraint matrix A ∈ Zm×n, and a constraint vector b ∈ Zm, with the form

max{cT x : Ax ≤ b, x ≥ 0, x ∈ Zn}.

The linear programming (LP) relaxation is formed by removing the integrality constraints:

max{cT x : Ax ≤ b, x ≥ 0}.

(1)

(2)

We denote the optimal solution to (1) by x∗

and its objective value by z∗
optimum of (2) subject to these additional constraints (similarly deﬁne z∗

LP. If σ is a set of constraints, we let x∗

LP = cT x∗

IP. We denote the optimal solution to (2) by x∗
LP
LP(σ) denote the LP
IP(σ)).

LP(σ) and x∗

4

Polyhedra and polytopes. A set P ⊆ Rn is a polyhedron if there exists an integer m, A ∈ Rm×n,
and b ∈ Rm such that P = {x ∈ Rn : Ax ≤ b}. P is a rational polyhedron if there exists A ∈ Zm×n
and b ∈ Zm such that P = {x ∈ Rn : Ax ≤ b}. A bounded polyhedron is called a polytope.
The feasible regions of all IPs considered in this paper are assumed to be rational polytopes 1.
Let P = {x ∈ Rn : aix ≤ bi, i ∈ M } be a nonempty polyhedron. For any I ⊆ M , the set
FI := {x ∈ Rn : aix = bi, i ∈ I, aix ≤ bi, i ∈ M \ I} is a face of P. Conversely, if F is a
nonempty face of P, then F = FI for some I ⊆ M . Given a set of constraints σ, let P(σ) denote
the polyhedron that is the intersection of P with all inequalities in σ.

Cutting planes. A cutting plane is a linear constraint αT x ≤ β. Let P be the feasible region of
the LP relaxation in Equation (2) and PI = P ∩ Zn be the feasible set of the IP in Equation (1).
A cutting plane is valid if it is satisﬁed by every integer-feasible point: αT x ≤ β for all x ∈ PI.
A valid cut separates a point x ∈ P \ PI if αT x > β. We interchangeably refer to a cut by its
parameters (α, β) ∈ Rn+1 and the halfspace αT x ≤ β in Rn it deﬁnes.

An important family of cuts that we study in this paper is the set of Gomory mixed integer

(GMI) cuts.

Deﬁnition 2.1 (Gomory mixed integer cut). Suppose the feasible region of the IP is in equality
form Ax = b, x ≥ 0 (which can be achieved by adding slack variables). For u ∈ Rm, let fi denote
the fractional part of (uT A)i and let f0 denote the fractional part of uT b. That is, (uT A)i =
((cid:98)uT A(cid:99))i + fi and uT b = (cid:98)uT b(cid:99) + f0. The Gomory mixed integer (GMI) cut parameterized by u
is given by

(cid:88)

i:fi≤f0

fixi +

f0
1 − f0

(cid:88)

i:fi>f0

(1 − fi)xi ≥ f0.

LP = (x∗

LP[1], . . . , x∗

Branch-and-cut. We provide a high-level overview of branch-and-cut (B&C) and refer the reader
to the textbook by Nemhauser and Wolsey [31] for more details. Given an IP, B&C searches through
the IP’s feasible region by building a binary search tree. B&C solves the LP relaxation of the input
IP and then adds any number of cutting planes. It stores this information at the root of its binary
search tree. Let x∗
LP[n]) be the solution to the LP relaxation with the addition
of the cutting planes. B&C next uses a variable selection policy to choose a variable xi to branch
on. This means that it splits the IP’s feasible region in two: one set where xi ≤ (cid:98)x∗
LP[i](cid:99) and the
other where xi ≥ (cid:100)x∗
LP[i](cid:101). The left child of the root now corresponds to the IP with a feasible
region deﬁned by the ﬁrst subset and the right child likewise corresponds to the second subset.
B&C then chooses a leaf using a node selection policy and recurses, adding any number of cutting
planes, branching on a variable, and so on. B&C fathoms a node—which means that it will never
branch on that node—if 1) the LP relaxation at the node is infeasible, 2) the optimal solution to the
LP relaxation is integral, or 3) the optimal solution to the LP relaxation is no better than the best
integral solution found thus far. Eventually, B&C will fathom every leaf, and it can be veriﬁed that
it has found the globally optimal integral solution. We assume there is a bound κ on the size of the
tree we allow B&C to build before we terminate, as is common in prior research [5, 7, 8, 20, 24, 25].
Every step of B&C—including node and variable selection and the choice of whether or not to
fathom—depends crucially on guidance from LP relaxations. To give an example, this is true of
the product scoring rule [1], a popular variable selection policy that our results apply to.

1This assumption is not a restrictive one. The Minkowski-Weyl theorem states that any polyhedron can be
decomposed as the sum of a polytope and its recession cone. All results in this paper can be derived for rational
polyhedra by considering the corresponding polytope in the Minkowski-Weyl decomposition.

5

LP be the solution to the LP relaxation at a node and z∗
Deﬁnition 2.2. Let x∗
The product scoring rule branches on the variable i ∈ [n] that maximizes: max{z∗
(cid:98)x∗

LP[i](cid:99)), 10−6} · max{z∗

LP[i](cid:101)), 10−6}.

LP(xi ≥ (cid:100)x∗

LP − z∗

LP = cT x∗
LP − z∗

LP.
LP(xi ≤

The tighter the LP relaxation, the more valuable the LP guidance, highlighting the importance

of cutting planes.

Polynomial arrangements in Euclidean space. Let p ∈ R[y1, . . . , yk] be a polynomial of
degree at most d. The polynomial p partitions Rk into connected components that belong to either
Rk \ {(y1, . . . , yk) : p(y1, . . . , yk) = 0} or {(y1, . . . , yk) : p(y1, . . . , yk) = 0}. When we discuss the
connected components of Rk induced by p, we include connected components in both these sets.
We make this distinction because previous work on sample complexity for data-driven algorithm
design oftentimes only needed to consider the connected components of the former set. The number
of connected components in both sets is O(dk) [29, 35, 36].

3 Linear programming sensitivity

Our main result in this section characterizes how an LP’s optimal solution is aﬀected by the
addition of one or more new constraints. In particular, ﬁxing an LP with m constraints and n
LP(αT x ≤ β) ∈ Rn denotes the new LP optimum when the constraint αT x ≤ β is
variables, if x∗
added, we pin down a precise characterization of x∗
LP(αT x ≤ β) as a function of α and β. We
LP(αT x ≤ β) has a piece-wise closed form: there are surfaces partitioning Rn+1 such
show that x∗
LP(αT x ≤ β) has a closed
that within each connected component induced by these surfaces, x∗
form. While the geometric intuition used to establish this piece-wise structure relies on the basic
property that optimal solutions to LPs are achieved at vertices, the surfaces deﬁning the regions
are perhaps surprisingly nonlinear: they are deﬁned by multivariate degree-2 polynomials in α, β.
In Appendix B.1 we illustrate these surfaces for an example two-variable LP.

There are two main steps of our proof: (1) tracking the set of edges of the LP polytope inter-
sected by the new constraint, and once that set of edges is ﬁxed, (2) tracking which edge yields the
vertex with the highest objective value.

Let M = [m] denote the set of m constraints. For E ⊆ M , let AE ∈ R|E|×n and bE ∈ R|E|
denote the restrictions of A and b to E. For α ∈ Rn, β ∈ R, and E ⊆ M with |E| = n − 1, let
E,α,β ∈ Rn×n be
AE,α ∈ Rn×n denote the matrix obtained by adding row vector α to AE and let Ai
the matrix AE,α with the ith column replaced by (bE, β)T .

Theorem 3.1. Let (c, A, b) be an LP and let x∗
LP denote the optimal solution. There is a set
of at most mn hyperplanes and at most m2n degree-2 polynomial hypersurfaces partitioning Rn+1
into connected components such that for each component C, one of the following holds: either (1)
LP(αT x ≤ β) = x∗
x∗

LP or (2) there is a set of constraints E ⊆ M with |E| = n − 1 such that

LP(αT x ≤ β) =
x∗

(cid:32) det(A1

E,α,β)
det(AE,α)

, . . . ,

(cid:33)

det(An
E,α,β)
det(AE,α)

for all (α, β) ∈ C.

Proof. First, if αT x ≤ β does not separate x∗
LP. The set of all such
cuts is the halfspace in Rn+1 given by {(α, β) ∈ Rn+1 : αT x∗
LP ≤ β}. All other cuts separate x∗
LP
and thus pass through P = {x ∈ Rn : Ax ≤ b, x ≥ 0}, and the new LP optimum is achieved at

LP(αT x ≤ β) = x∗

LP, then x∗

6

a vertex created by the cut. We consider the new vertices formed by the cut, which lie on edges
(faces of dimension 1) of P. Letting M denote the set of m constraints that deﬁne P, each edge e
of P can be identiﬁed with a subset E ⊂ M of size n − 1 such that the edge is precisely the set of
all points x such that

aT
aT

i x = bi
i x ≤ bi

∀ i ∈ E

∀ i ∈ M \ E,

where ai is the ith row of A. Let AE ∈ Rn−1×n denote the restriction of A to only the rows in E,
and let bE ∈ R|E| denote the entries of b corresponding to constraints in E. Drop the inequality
constraints deﬁning the edge, so the equality constraints deﬁne a line in Rn. The intersection of
the cut αT x ≤ β and this line is precisely the solution to the system of n linear equations in n
variables: AEx = bE, αT x = β. By Cramer’s rule, the (unique) solution x = (x1, . . . , xn) to this
det(Ai
system is given by xi =
det(AE,α) . To ensure that the intersection point indeed lies on the edge of
the polytope, we simply stipulate that it satisﬁes the inequality constraints in M \ E. That is,

E,α,β )

n
(cid:88)

j=1

aij ·

det(Aj
E,α,β)
det(AE,α)

≤ bi

(3)

for every i ∈ M \E (note that if α, β satisfy any of these constraints, it must be that det(AE,α) (cid:54)= 0,
which guarantees that AEx = bE, αT x = β indeed has a unique solution). Multiplying through
by det(AE,α) shows that this constraint is a halfspace in Rn+1, since det(AE,α) and det(Ai
E,α,β)
are both linear in α and β. The collection of all the hyperplanes deﬁning the boundaries of these
halfspaces over all edges of P induces a partition of Rn+1 into connected components such that
for all (α, β) within a given connected component, the (nonempty) set of edges of P that the
hyperplane αT x = β intersects is invariant.

Now, consider a single connected component, denoted by C for brevity. Let e1, . . . , ek denote
the edges intersected by cuts in C, and let E1, . . . , Ek ⊂ M denote the sets of constraints that are
binding at each of these edges, respectively. For each pair ep, eq, consider the surface

n
(cid:88)

i=1

ci ·

det(Ai
Ep,α,β)
det(AEp,α)

=

n
(cid:88)

i=1

ci ·

det(Ai
Eq,α,β)
det(AEq,α)

.

(4)

Clearing the (nonzero) denominators shows this is a degree-2 polynomial hypersurface in α, β in
Rn+1. This hypersurface is the set of all (α, β) for which the LP objective value achieved at
the vertex on edge ep is equal to the LP objective value achieved at the vertex on edge eq. The
collection of these surfaces for each p, q partitions C into further connected components. Within
each of these connected components, the edge containing the vertex that maximizes the objective
LP(αT x ≤ β) has the closed form
is invariant. If this edge corresponds to binding constraints E, x∗
x∗
LP(αT x ≤ β)[i] =
for all (α, β) within this component. We now count the number of
surfaces used to obtain our decomposition. P has at most (cid:0) m
(cid:1) ≤ mn−1 edges, and for each edge
n−1
E we ﬁrst considered at most |M \ E| ≤ m hyperplanes representing decision boundaries for cuts
intersecting that edge (Equation (3)), for a total of at most mn hyperplanes. We then considered
a degree-2 polynomial hypersurface for every pair of edges (Equation (4)), of which there are at
most (cid:0)mn
2

det(Ai
E,α,β )
det(AE,α)

(cid:1) ≤ m2n.

In Appendix B.2, we generalize Theorem 3.1 to understand x∗

LP as a function of any K con-
In this case, we show that the piecewise structure is given by degree-2K multivariate

straints.
polynomials.

7

4 Structure and sensitivity of branch-and-cut

We now use Theorem 3.1 to answer a fundamental question about B&C: how does the B&C tree
change when cuts are added at the root? Said another way, what is the structure of the B&C tree
as a function of the set of cuts? We prove that the set of all possible cuts can be partitioned into
a ﬁnite number of regions where by employing cuts from any one region, the B&C tree remains
exactly the same. Moreover, we prove that the boundaries between regions are deﬁned by constant-
degree polynomials. As in the previous section, we focus on a single cut added to the root of the
B&C tree. We provide an extension to multiple cuts in Appendix C.2.

We outline the main steps of our analysis:

1. In Lemma 4.2 we use Theorem 3.1 to understand how the LP optimum at any node in the

B&C tree behaves as a function of cuts added at the root.

2. In Lemma 4.3, we analyze how the branching decisions of B&C are impacted by variations

in the cuts.

3. In Lemma 4.4, we analyze how cuts aﬀect which nodes are fathomed due to the integrality of

the LP relaxation.

4. In Theorem 4.5, we analyze how the LP estimates based on cuts can lead to pruning nodes
of the B&C tree, which gives us a complete description of when two cutting planes lead to
the same B&C tree.

The full proofs from this section are in Appendix C.
Given an IP, let τ = (cid:100)maxx∈P (cid:107)x(cid:107)∞(cid:101) be the maximum magnitude coordinate of any LP-feasible
solution, rounded up. The set of all possible branching constraints is contained in BC := {x[i] ≤
(cid:96), x[i] ≥ (cid:96)}0≤(cid:96)≤τ,i∈[n] which is a set of size 2n(τ + 1). Na¨ıvely, there are at most 22n(τ +1) subsets of
branching constraints, but the following observation allows us to greatly reduce the number of sets
we consider.

Lemma 4.1. Fix an IP (c, A, b). Deﬁne an equivalence relation on pairs of branching-constraint
LP(αT x ≤ β, σ2) for all possible cutting
LP(αT x ≤ β, σ1) = x∗
sets σ1, σ2 ⊆ BC, by σ1 ∼ σ2 ⇐⇒ x∗
planes αT x ≤ β. The number of equivalence classes of ∼ is at most τ 3n.

By Cramer’s rule, τ ≤ | det( (cid:101)A)|, where (cid:101)A is any square submatrix of A. This is at most annn/2
by Hadamard’s inequality, where a is the maximum absolute value of any entry of A. However, τ
can be much smaller in various cases. For example, if A contains even one row with only positive
entries, then τ ≤ (cid:107)b(cid:107)∞.

We will use the following notation in the remainder of this section. Let Aσ and bσ denote the
augmented constraint matrix and vector when the constraints in σ ⊆ BC are added. For E ⊆ M ∪σ,
let AE,σ ∈ R|E|×n and bE ∈ R|E| denote the restrictions of Aσ and bσ to E. For α ∈ Rn, β ∈ R
and E ⊆ M ∪ σ with |E| = n − 1, let AE,α,σ ∈ Rn×n denote the matrix obtained by adding row
E,α,β,σ ∈ Rn×n be the matrix AE,α,σ with the ith column replaced by
vector α to AE,σ and let Ai
(bE,σ, β)T .

Lemma 4.2. For any LP (c, A, b), there are at most (m + 2n)nτ 3n hyperplanes and at most (m +
2n)2nτ 3n degree-2 polynomial hypersurfaces partitioning Rn+1 into connected components such that
LP(αT x ≤
for each component C and every σ ⊂ BC, either: (1) x∗
β, σ) = z∗
LP(σ), or (2) there is a set of constraints E ⊆ M ∪ σ with |E| = n − 1 such that
LP(αT x ≤ β, σ)[i] =
x∗

LP(αT x ≤ β, σ) = x∗

for all (α, β) ∈ C.

LP(σ) and z∗

det(Ai
E,α,β,σ)
det(AE,α,σ)

8

Proof sketch. The same reasoning in the proof of Theorem 3.1 yields a partition with the desired
properties.

Next, we reﬁne the decomposition obtained in Lemma 4.2 so that the branching constraints
added at each step of B&C are invariant within a region. Our results apply to the product scoring
rule (Def. 2.2), which is used, for example, by the leading open-source solver SCIP [10].

Lemma 4.3. There are at most 3(m + 2n)nτ 3n hyperplanes, 3(m + 2n)3nτ 4n degree-2 polynomial
hypersurfaces, and (m + 2n)6nτ 4n degree-5 polynomial hypersurfaces partitioning Rn+1 into con-
nected components such that within each component, the branching constraints used at every step
of B&C are invariant.

LP(αT x ≤ β, σ) = x∗

LP(σ) or there exists E ⊆ M ∪ σ such that x∗

Proof sketch. Fix a connected component C in the decomposition established in Lemma 4.2. Then,
for each σ, either x∗
LP(αT x ≤
det(Ai
E,α,β,σ)
β, σ)[i] =
for all (α, β) ∈ C and all i ∈ [n]. Now, if we are at a stage in the branch-
det(AE,α,σ)
and-cut tree where σ is the list of branching constraints added so far, and the ith variable is
LP(αT x ≤ β, σ)[i](cid:5) and xi ≥
being branched on next, the two constraints generated are xi ≤ (cid:4)x∗
LP(αT x ≤ β, σ)[i](cid:7), respectively. If C is a component where x∗
(cid:6)x∗
LP(σ), then
there is nothing more to do, since the branching constraints at that point are trivially invariant
over (α, β) ∈ C. Otherwise, in order to further decompose C such that the right-hand-side of these
constraints are invariant for every σ and every i = 1, . . . , n, we add the two decision boundaries
given by

LP(αT x ≤ β, σ) = x∗

k ≤

det(Ai
E,α,β,σ)
det(AE,α,σ)

≤ k + 1

for every i, σ, and every integer k = 0, . . . , τ − 1. This ensures that within every connected
LP(αT x ≤
component of C induced by these boundaries (hyperplanes), (cid:98)x∗
β, σ)[i](cid:101) are invariant. A careful analysis of the deﬁnition of the product scoring rule provides the
appropriate reﬁnement of this partition.

LP(αT x ≤ β, σ)[i](cid:99) and (cid:100)x∗

We now move to the most critical phase of branch-and-cut: deciding when to fathom a node.
One reason a node might be fathomed is if the LP relaxation of the IP at that node has an integral
solution. We derive conditions that ensure that nearby cuts have the same eﬀect on the integrality
of the original IP at any node in the search tree. Recall that PI = P ∩ Zn is the set of integer
points in P. Let V ⊆ Rn+1 denote the set of all valid cuts for the input IP (c, A, b). The set V is
a polyhedron since it can be expressed as

V =

(cid:92)

x∈PI

{(α, β) ∈ Rn+1 : αT x ≤ β},

and PI is ﬁnite as P is bounded. For cuts outside V, we assume the B&C tree takes some special
form denoting an invalid cut. Our goal now is to decompose V into connected components such
that 1 (cid:2)x∗

LP(αT x ≤ β, σ) ∈ Zn(cid:3) is invariant for all (α, β) in each component.

Lemma 4.4. For any IP (c, A, b), there are at most 3(m + 2n)nτ 4n hyperplanes, 3(m + 2n)3nτ 4n
degree-2 polynomial hypersurfaces, and (m + 2n)6nτ 4n degree-5 polynomial hypersurfaces parti-
tioning Rn+1 into connected components such that for each component C and each σ ⊆ BC,
1 (cid:2)x∗
LP

(cid:0)αT x ≤ β, σ(cid:1) ∈ Zn(cid:3) is invariant for all (α, β) ∈ C.

9

Proof sketch. Fix a connected component C in the decomposition that includes the facets deﬁning
V and the surfaces obtained in Lemma 4.3. For all σ, xI ∈ PI, and i ∈ [n], consider the surface

LP(αT x ≤ β, σ)[i] = xI[i].
x∗

(5)

By Lemma 4.2, this surface is a hyperplane. Clearly, within any connected component of C induced
LP(αT x ≤ β, σ) = xI] is invariant. Finally, if
by these hyperplanes, for every σ and xI ∈ PI, 1[x∗
LP(αT x ≤ β, σ) ∈ Zn for some cut αT x ≤ β within a given connected component, x∗
x∗
LP(αT x ≤
LP(αT x ≤ β, σ) = xI ∈ Zn for all cuts
β, σ) = xI for some xI ∈ PI(σ) ⊆ PI, which means that x∗
αT x ≤ β in that connected component.

Suppose for a moment that a node is fathomed by B&C if and only if either the LP at that node
is infeasible, or the LP optimal solution is integral—that is, the “bounding” of B&C is suppressed.
In this case, the partition of Rn+1 obtained in Lemma 4.4 guarantees that the tree built by branch-
Indeed, since the branching constraints
and-cut is invariant within each connected component.
at every node are invariant, and for every σ the integrality of x∗
LP(αT x ≤ β, σ) is invariant,
the (bounding-suppressed) B&C tree (and the order in which it is built) is invariant within each
connected component in our decomposition. Equipped with this observation, we now analyze the
full behavior of B&C.
Theorem 4.5. Given an IP (c, A, b), there is a set of at most O(14n(m + 2n)3n2τ 5n2) polynomial
hypersurfaces of degree ≤ 5 partitioning Rn+1 into connected components such that the branch-and-
cut tree built after adding the cut αT x ≤ β at the root is invariant over all (α, β) within a given
component.

Proof sketch. Fix a connected component C in the decomposition induced by the set of hyperplanes
and degree-2 hypersurfaces established in Lemma 4.4. Let

Q1, . . . , Qi1, I1, Qi1+1, . . . , Qi2, I2, Qi2+1, . . .

(6)

denote the nodes of the tree branch-and-cut creates, in order of exploration, under the assumption
that a node is pruned if and only if either the LP at that node is infeasible or the LP optimal solution
is integral (so the “bounding” of branch-and-bound is suppressed). Here, a node is identiﬁed by
the list σ of branching constraints added to the input IP. Nodes labeled by Q are either infeasible
or have fractional LP optimal solutions. Nodes labeled by I have integral LP optimal solutions and
are candidates for the incumbent integral solution at the point they are encountered. (The nodes
are functions of α and β, as are the indices i1, i2, . . ..) By Lemma 4.4 and the observation following
it, this ordered list of nodes is invariant over all (α, β) ∈ C.

Now, given an node index (cid:96), let I((cid:96)) denote the incumbent node with the highest objective value
encountered up until the (cid:96)th node searched by B&C, and let z(I((cid:96))) denote its objective value. For
each node Q(cid:96), let σ(cid:96) denote the branching constraints added to arrive at node Q(cid:96). The hyperplane

LP(αT x ≤ β, σ(cid:96)) = z(I((cid:96)))
z∗
(which is a hyperplane due to Lemma 4.2) partitions C into two subregions. In one subregion,
LP(αT x ≤ β, σ(cid:96)) ≤ z(I((cid:96))), that is, the objective value of the LP optimal solution is no greater
z∗
than the objective value of the current incumbent integer solution, and so the subtree rooted at
Q(cid:96) is pruned. In the other subregion, z∗
LP(αT x ≤ β, σ(cid:96)) > z(I((cid:96))), and Q(cid:96) is branched on further.
Therefore, within each connected component of C induced by all hyperplanes given by Equation 16
for all (cid:96), the set of node within the list (15) that are pruned is invariant. Combined with the surfaces
established in Lemma 4.4, these hyperplanes partition Rn+1 into connected components such that
as (α, β) varies within a given component, the tree built by branch-and-cut is invariant.

(7)

10

5 Sample complexity bounds for B&C

In this section, we show how the results from the previous section can be used to provide sample
complexity bounds for conﬁguring B&C. Our results will apply to families of cuts parameterized by
vectors u from a set U , such as the family of GMI cuts from Deﬁnition 2.1. We assume there is an
unknown, application-speciﬁc distribution D over IPs. The learner receives a training set S ∼ DN
of N IPs sampled from this distribution. A sample complexity guarantee bounds the number of
samples N suﬃcient to ensure that for any parameter setting u ∈ U , the B&C tree size on average
over the training set S is close to the expected B&C tree size. More formally, let gu(c, A, b) be
the size of the tree B&C builds given the input (c, A, b) after applying the cut deﬁned by u at the
root. Given (cid:15) > 0 and δ ∈ (0, 1), a sample complexity guarantee bounds the number of samples N
suﬃcient to ensure that with probability 1 − δ over the draw S ∼ DN , for every parameter setting
u ∈ U ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

(cid:88)

(c,A,b)∈S

gu(c, A, b) − E [gu(c, A, b)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ (cid:15).

(8)

To derive our sample complexity guarantee, we use the notion of pseudo-dimension [32]. Let
G = {gu : u ∈ U }. The pseudo-dimension of G, denoted Pdim(G), is the largest integer N for
which there exist N IPs (c1, A1, b1), . . . , (cN , AN , bN ) and N thresholds r1, . . . , rN ∈ R such that
for every binary vector (σ1, . . . , σN ) ∈ {0, 1}N , there exists gu ∈ G such that gu(ci, Ai, bi) ≥ ri
if and only if σi = 1. The number of samples suﬃcient to ensure that Equation (8) holds is
N = O( κ2
δ )) [32]. Equivalently, for a given number of samples N , the left-hand-
δ ).

side of Equation (8) can be bounded by κ

(cid:15)2 (Pdim(G) + log 1

N (Pdim(G) + log 1

(cid:113) 1

So far, α, β are parameters that do not depend on the input instance c, A, b. Suppose now
that they do: α, β are functions of c, A, b and a parameter vector u (as they are for GMI cuts).
Despite the structure established in the previous section, if α, β can depend on (c, A, b) in arbitrary
ways, one cannot even hope for a ﬁnite sample complexity, illustrated by the following impossibility
result. The full proofs of all results from this section are in Appendix D.
Theorem 5.1. There exist functions αc,A,b : U → Rn and βc,A,b : U → R such that

where U is any set with |U | = |R|.

Pdim ({gu : u ∈ U }) = ∞,

However, in the case of GMI cuts, we show that the cutting plane coeﬃcients parameterized by
u are highly structured. Combining this structure with our analysis of B&C allows us to derive
polynomial sample complexity bounds.

Lemma 5.2. Consider the family of GMI cuts parameterized by u ∈ [−U, U ]m. There is a set of
at most O(nU 2 (cid:107)A(cid:107)1 (cid:107)b(cid:107)1) hyperplanes partitioning [−U, U ]m into connected components such that
(cid:98)uT ai(cid:99), (cid:98)uT b(cid:99), and 1[fi ≤ f0] are invariant, for every i, within each component.
Proof sketch. We have fi = uT ai − (cid:98)uT ai(cid:99), f0 = uT b − (cid:98)uT b(cid:99), and since u ∈ [−U, U ]m, (cid:98)uT ai(cid:99) ∈
[−U (cid:107)ai(cid:107)1 , U (cid:107)ai(cid:107)1] and (cid:98)uT b(cid:99) ∈ [−U (cid:107)b(cid:107)1 , U (cid:107)b(cid:107)1]. For all i, ki ∈ [−U (cid:107)ai(cid:107)1 , U (cid:107)ai(cid:107)1] ∩ Z and
k0 ∈ [−U (cid:107)b(cid:107)1 , U (cid:107)b(cid:107)1] ∩ Z, hyperplanes deﬁne the two regions

(cid:4)uT ai

(cid:5) = ki ⇐⇒ ki ≤ uT ai < ki + 1

and the hyperplanes deﬁning the two halfspaces

(cid:4)uT b(cid:5) = k0 ⇐⇒ k0 ≤ uT b < k0 + 1.

11

In addition, for each i, consider the hyperplane

uT ai − ki = uT b − k0.

(9)

Within any connected component of Rm determined by these hyperplanes, (cid:98)uT ai(cid:99) and (cid:98)uT b(cid:99) are
constant. Also, 1[fi ≤ f0] is invariant within each component, since if (cid:98)uT ai(cid:99) = ki and (cid:98)uT b(cid:99) = k0,
fi ≤ f0 ⇐⇒ uT ai − ki ≤ uT b − k0, which is the hyperplane from Equation 9. The lemma follows
by counting the hyperplanes.

Let α : [−U, U ]m → Rn denote the function taking GMI cut parameters u to the corresponding
vector of coeﬃcients determining the resulting cutting plane, and let β : [−U, U ]m → R denote the
oﬀset of the resulting cutting plane. So (after multiplying through by 1 − f0),

α(u)[i] =

(cid:40)

fi(1 − f0)
f0(1 − fi)

if fi ≤ f0
if fi > f0

and β(u) = f0(1 − f0) (of course f0 and each fi are functions of u, but we suppress this dependence
for readability).

The next lemma allows us to transfer the polynomial partition of Rn+1 from Theorem 4.5 to a

polynomial partition of [−U, U ]m, incurring only a factor 2 increase in degree.

Lemma 5.3. Let p ∈ R[y1, . . . , yn+1] be a polynomial of degree d. Let D ⊆ [−U, U ]m be a connected
component from Lemma 5.2. Deﬁne q : D → R by q(u) = p(α(u), β(u)). Then q is a polynomial
in u of degree 2d.

Proof. By Lemma 5.2, there are integers k0, ki for i ∈ [n] such that (cid:98)uT ai(cid:99) = ki and (cid:98)uT b(cid:99) = k0
for all u ∈ D. Also, the set S = {i : fi ≤ f0} is ﬁxed over all u ∈ D.

A degree-d polynomial p in variables y1, . . . , yn+1 can be written as (cid:80)

i∈T yi
for some coeﬃcients λT ∈ R, where T (cid:118) [n + 1] means that T is a multiset of [n + 1]. Evaluating
at (α(u), β(u)), we get

T (cid:118)[n+1],|T |≤d λT

(cid:81)

(cid:88)

λT

(cid:89)

|T |≤d

i∈T ∩S
i(cid:54)=n+1

fi(1 − f0)

(cid:89)

i∈T \S
i(cid:54)=n+1

f0(1 − fi)

(cid:89)

i∈T
i=n+1

f0(1 − f0).

Now, fi = uT ai − ki and f0 = uT b − k0 are linear in u. The sum is over all multisets of size at
most d, so each monomial consists of the product of at most d degree-2 terms of the form fi(1 − f0),
f0(1 − fi), or f0(1 − f0). Thus, deg(q) ≤ 2d, as desired.

Applying Lemma 5.3 to every polynomial hypersurface in the partition of Rn+1 established in

Theorem 4.5 yields our main structural result for GMI cuts.

Lemma 5.4. Consider the family of GMI cuts parameterized by u ∈ [−U, U ]m. For any IP
(c, A, b), there are at most O(nU 2 (cid:107)A(cid:107)1 (cid:107)b(cid:107)1) hyperplanes and 2O(n2)(m + 2n)O(n3)τ O(n3) degree-10
polynomial hypersurfaces partitioning [−U, U ]m into connected components such that the B&C tree
built after adding the GMI cut deﬁned by u is invariant over all u within a single component.

Bounding the pseudo-dimension of the class of tree-size functions {gu : u ∈ [−U, U ]m} is a
direct application of the main theorem of Balcan et al. [6] along with standard results bounding
the VC dimension of polynomial boundaries [2].

12

Theorem 5.5. The pseudo-dimension of the class of tree-size functions {gu : u ∈ [−U, U ]m} on
the domain of IPs with (cid:107)A(cid:107)1 ≤ a and (cid:107)b(cid:107)1 ≤ b is

O (cid:0)m log(abU ) + mn3 log(m + n) + mn3 log τ (cid:1) .

We generalize the analysis of this section to multiple GMI cuts at the root of the B&C tree
in Appendix D. The analysis there is more involved since GMI cuts can be applied in sequence,
re-solving the LP relaxation after each cut. In particular, GMI cuts applied in sequence have one
more parameter than the next, so the hyperplane deﬁned by each GMI cut depends (polynomially)
on the parameters deﬁning all GMI cuts before it. We show that if K GMI cuts are sequentially
applied at the root, the resulting partition of the parameter space is induced by polynomials of
degree O(K2).

6 Conclusions

In this paper, we investigated fundamental questions about linear and integer programs: given
an integer program, how many possible branch-and-cut trees are there if one or more additional
feasible constraints can be added? Even more speciﬁcally, what is the structure of the branch-
and-cut tree as a function of a set of additional constraints? Through a detailed geometric and
combinatorial analysis of how additional constraints aﬀect the LP relaxation’s optimal solution, we
showed that the branch-and-cut tree is piecewise constant and precisely bounded the number of
pieces. We showed that the structural understandings that we developed could be used to prove
sample complexity bounds for conﬁguring branch-and-cut.

Acknowledgements

This material is based on work supported by the National Science Foundation under grants CCF-
1733556, CCF-1910321, IIS-1901403, and SES-1919453, the ARO under award W911NF2010081,
the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003,
a Simons Investigator Award, an AWS Machine Learning Research Award, an Amazon Research
Award, a Bloomberg Research Grant, and a Microsoft Research Faculty Fellowship.

References

[1] Tobias Achterberg. Constraint Integer Programming. PhD thesis, Technische Universit¨at

Berlin, 2007.

[2] Martin Anthony and Peter Bartlett. Neural Network Learning: Theoretical Foundations. Cam-

bridge University Press, 2009.

[3] Egon Balas, Sebastian Ceria, G´erard Cornu´ejols, and N Natraj. Gomory cuts revisited. Oper-

ations Research Letters, 19(1):1–9, 1996.

[4] Maria-Florina Balcan. Data-driven algorithm design. In Tim Roughgarden, editor, Beyond

Worst Case Analysis of Algorithms. Cambridge University Press, 2020.

[5] Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch.

In International Conference on Machine Learning (ICML), 2018.

13

[6] Maria-Florina Balcan, Dan DeBlasio, Travis Dick, Carl Kingsford, Tuomas Sandholm, and
Ellen Vitercik. How much data is suﬃcient to learn high-performing algorithms? Gener-
alization guarantees for data-driven algorithm design.
In Annual Symposium on Theory of
Computing (STOC), 2021.

[7] Maria-Florina Balcan, Siddharth Prasad, Tuomas Sandholm, and Ellen Vitercik. Improved

learning bounds for branch-and-cut. arXiv preprint arXiv:2111.11207, 2021.

[8] Maria-Florina Balcan, Siddharth Prasad, Tuomas Sandholm, and Ellen Vitercik. Sample
complexity of tree search conﬁguration: Cutting planes and beyond. In Annual Conference on
Neural Information Processing Systems (NeurIPS), 2021.

[9] Dimitris Bertsimas and Jack Dunn. Optimal classiﬁcation trees. Machine Learning, 106(7):

1039–1082, 2017.

[10] Ksenia Bestuzheva, Mathieu Besan¸con, Wei-Kun Chen, Antonia Chmiela, Tim Donkiewicz,
Jasper van Doornmalen, Leon Eiﬂer, Oliver Gaul, Gerald Gamrath, Ambros Gleixner,
Leona Gottwald, Christoph Graczyk, Katrin Halbig, Alexander Hoen, Christopher Hojny,
Rolf van der Hulst, Thorsten Koch, Marco L¨ubbecke, Stephen J. Maher, Frederic Mat-
ter, Erik M¨uhmer, Benjamin M¨uller, Marc E. Pfetsch, Daniel Rehfeldt, Steﬀan Schlein,
Franziska Schl¨osser, Felipe Serrano, Yuji Shinano, Boro Sofranac, Mark Turner, Stefan
Vigerske, Fabian Wegscheider, Philipp Wellner, Dieter Weninger, and Jakob Witzig. The
SCIP Optimization Suite 8.0. Technical report, Optimization Online, December 2021. URL
http://www.optimization-online.org/DB_HTML/2021/12/8728.html.

[11] Rudy Bunel, Ilker Turkaslan, Philip H.S. Torr, Pushmeet Kohli, and M. Pawan Kumar. A
uniﬁed view of piecewise linear neural network veriﬁcation. In Annual Conference on Neural
Information Processing Systems (NeurIPS), 2018.

[12] Vaˇsek Chv´atal. Edmonds polytopes and a hierarchy of combinatorial problems. Discrete

mathematics, 4(4):305–337, 1973.

[13] William Cook, Albertus MH Gerards, Alexander Schrijver, and ´Eva Tardos. Sensitivity theo-
rems in integer linear programming. Mathematical Programming, 34(3):251–264, 1986.

[14] G´erard Cornu´ejols. Revival of the Gomory cuts in the 1990’s. Annals of Operations Research,

149(1):63–66, 2007.

[15] G´erard Cornu´ejols and Yanjun Li. Elementary closures for integer programs. Operations

Research Letters, 28(1):1–8, 2001.

[16] Ralph Gomory. An algorithm for the mixed integer problem. resreport RM-2597, The Rand

Corporation, 1960.

[17] Ralph E. Gomory. Outline of an algorithm for integer solutions to linear programs. Bulletin

of the American Mathematical Society, 64(5):275 – 278, 1958.

[18] Rishi Gupta and Tim Roughgarden. A PAC approach to application-speciﬁc algorithm selec-

tion. SIAM Journal on Computing, 46(3):992–1017, 2017.

[19] Zeren Huang, Kerong Wang, Furui Liu, Hui-Ling Zhen, Weinan Zhang, Mingxuan Yuan, Jianye
Hao, Yong Yu, and Jun Wang. Learning to select cuts for eﬃcient mixed-integer programming.
Pattern Recognition, 123:108353, 2022.

14

[20] Frank Hutter, Holger H Hoos, Kevin Leyton-Brown, and Thomas St¨utzle. ParamILS: An
automatic algorithm conﬁguration framework. Journal of Artiﬁcial Intelligence Research, 36
(1):267–306, 2009. ISSN 1076-9757.

[21] Robert G Jeroslow. Trivial integer programs unsolvable by branch-and-bound. Mathematical

Programming, 6(1):105–109, 1974.

[22] J¨org Hendrik Kappes, Markus Speth, Gerhard Reinelt, and Christoph Schn¨orr. Towards
eﬃcient and exact map-inference for large scale discrete computer vision problems via combi-
natorial optimization. In Conference on Computer Vision and Pattern Recognition (CVPR),
pages 1752–1758. IEEE, 2013.

[23] Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Peter Clark, Oren Etzioni, and Dan Roth.
Question answering via integer programming over semi-structured knowledge. In International
Joint Conference on Artiﬁcial Intelligence (IJCAI), 2016.

[24] Robert Kleinberg, Kevin Leyton-Brown, and Brendan Lucier. Eﬃciency through procrasti-
nation: Approximately optimal algorithm conﬁguration with runtime guarantees. In Interna-
tional Joint Conference on Artiﬁcial Intelligence (IJCAI), 2017.

[25] Robert Kleinberg, Kevin Leyton-Brown, Brendan Lucier, and Devon Graham. Procrastinating
with conﬁdence: Near-optimal, anytime, adaptive algorithm conﬁguration. Annual Conference
on Neural Information Processing Systems (NeurIPS), 2019.

[26] Ailsa H Land and Alison G Doig. An automatic method of solving discrete programming

problems. Econometrica, pages 497–520, 1960.

[27] Wu Li. The sharp lipschitz constants for feasible and optimal solutions of a perturbed linear

program. Linear algebra and its applications, 187:15–40, 1993.

[28] Olvi L Mangasarian and T-H Shiau. Lipschitz continuity of solutions of linear inequalities,
programs and complementarity problems. SIAM Journal on Control and Optimization, 25(3):
583–595, 1987.

[29] John Milnor. On the Betti numbers of real varieties. Proceedings of the American Mathematical

Society, 15(2):275–280, 1964.

[30] Atsushi Miyauchi, Tomohiro Sonobe, and Noriyoshi Sukegawa. Exact clustering via integer
programming and maximum satisﬁability. In AAAI Conference on Artiﬁcial Intelligence, 2018.

[31] George Nemhauser and Laurence Wolsey. Integer and Combinatorial Optimization. John Wiley

& Sons, 1999.

[32] David Pollard. Convergence of Stochastic Processes. Springer, 1984.

[33] Tuomas Sandholm. Very-large-scale generalized combinatorial multi-attribute auctions:
In Zvika Neeman, Alvin Roth, and Nir

Lessons from conducting $60 billion of sourcing.
Vulkan, editors, Handbook of Market Design. Oxford University Press, 2013.

[34] Yunhao Tang, Shipra Agrawal, and Yuri Faenza. Reinforcement learning for integer program-

ming: Learning to cut. International Conference on Machine Learning (ICML), 2020.

[35] Ren´e Thom. Sur l’homologie des varietes algebriques reelles. In Diﬀerential and combinatorial

topology, pages 255–265. Princeton University Press, 1965.

15

[36] Hugh E Warren. Lower bounds for approximation by nonlinear manifolds. Transactions of the

American Mathematical Society, 133(1):167–178, 1968.

[37] Jiaming Zeng, Berk Ustun, and Cynthia Rudin. Interpretable classiﬁcation models for recidi-
vism prediction. Journal of the Royal Statistical Society: Series A (Statistics in Society), 180
(3):689–722, 2017.

16

A Further details about plots

The version of the facility location problem we study involves a set of locations J and a set of
clients C. Facilities are to be constructed at some subset of the locations, and the clients in C are
served by these facilities. Each location j ∈ J has a cost fj of being the site of a facility, and a
cost sc,j of serving client c ∈ C. Finally, each location j has a capacity κj which is a limit on the
number of clients j can serve. The goal of the facility location problem is to arrive at a feasible set
of locations for facilities and a feasible assignment of clients to these locations that minimizes the
overall cost incurred.

The facility location problem can be formulated as the following 0, 1 IP:

minimize

subject to

(cid:88)

j∈J
(cid:88)

j∈J
(cid:88)

fjxj +

(cid:88)

(cid:88)

j∈J

c∈C

sc,jyc,j

yc,j = 1

∀ c ∈ C

yc,j ≤ κjxj

∀ j ∈ J

c∈C
yc,j ∈ {0, 1}
xj ∈ {0, 1}

∀ c ∈ C, j ∈ J
∀ j ∈ J

We consider the following two distributions over facility location IPs.

First distribution Facility location IPs are generated by perturbing the costs and capacities of
a base facility location IP. We generated the base IP with 40 locations and 40 clients by choosing
the location costs and client-location costs uniformly at random from [0, 100] and the capacities
uniformly at random from {0, . . . , 39}. To sample from the distribution, we perturb this base IP
by adding independent Gaussian noise with mean 0 and standard deviation 10 to the cost of each
location, the cost of each client-location pair, and the capacity of each location.

Second distribution Facility location IPs are generated by placing 80 evenly-spaced locations
along the line segment connecting the points (0, 1/2) and (1, 1/2) in the Cartesian plane. The
location costs are all uniformly set to 1. Then, 80 clients are placed uniformly at random in the
unit square [0, 1]2. The cost sc,j of serving client c from location j is the distance between j and c.
Location capacities are chosen uniformly at random from {0, . . . , 43}.

In our experiments, we add ﬁve cuts at the root of the B&C tree. These ﬁve cuts come from the
set of Chv´atal-Gomory and Gomory mixed integer cuts derived from the optimal simplex tableau
of the LP relaxation. The ﬁve cuts added are chosen to maximize a weighting of cutting-plane
scores:

µ · score1 + (1 − µ) · score2.
score1 is the parallelism of a cut, which intuitively measures the angle formed by the objective vec-
tor and the normal vector of the cutting plane—promoting cutting planes that are nearly parallel
with the objective direction. score2 is the eﬃcacy, or depth, of a cut, which measures the perpen-
dicular distance from the LP optimum to the cut—promoting cutting planes that are “deeper”, as
measured with respect to the LP optimum. More details about these scoring rules can be found
in Balcan et al. [8] and references therein. Given an IP, for each µ ∈ [0, 1] (discretized at steps of
0.01) we choose the ﬁve cuts among the set of Chv´atal-Gomory and Gomory mixed integer cuts
that maximize (10). Figures 1 and 2 display the average tree size over 1000 samples drawn from the
respective distribution for each value of µ used to choose cuts at the root. We ran our experiments
using the C API of IBM ILOG CPLEX 20.1.0, with default cut generation disabled.

(10)

17

B Omitted results and proofs from Section 3

B.1 Example in two dimensions

Consider the LP

max{x + y : x ≤ 1, y ≥ 0, y ≤ x}.

The optimum is at (x∗, y∗) = (1, 1). Consider adding an additional constraint α1x + α2y ≤ 1. Let
h denote the hyperplane α1x + α2y = 1. We derive a description of the set of parameters (α1, α2)
such that h intersects the hyperplanes x = 1 and y = x. The intersection of h and x = 1 is given
by

(x, y) =

1,

(cid:18)

(cid:19)

,

1 − α1
α2

which exists if and only if α2 (cid:54)= 0. This intersection point is in the LP feasible region if and only if
0 ≤ 1−α1
≤ 1 (which additionally ensures that α2 (cid:54)= 0). Similarly, h intersects y = x at
α2

(x, y) =

(cid:18)

1
α1 + α2

,

1
α1 + α2

(cid:19)

,

which exists if and only if α1 +α2 (cid:54)= 0. This intersection point is in the LP feasible region if and only
if 0 ≤ 1
≤ 1. Now, we put down an “indiﬀerence” curve in (α1, α2)-space that represents the
set of (α1, α2) such that the value of the objective achieved at the two aforementioned intersection
points is equal. This surface is given by

α1+α2

2
α1 + α2

= 1 +

1 − α1
α2

.

Since α1 + α2 (cid:54)= 0 and α2 (cid:54)= 0 (for the relevant α1, α2 in consideration), this is equivalent to
1 − α2
α2
2 − α1 + α2 = 0, which is a degree-2 curve in α1, α2. The left-hand-side can be factored to
write this as (α1 − α2)(α1 + α2 − 1) = 0. Therefore, this curve is given by the two lines α1 = α2
and α1 + α2 = 1. Figure 3 illustrates the resulting partition of (α1, α2)-space.

It turns out that when n = 2 the indiﬀerence curve can always be factored into a product of
linear terms. Let the objective of the LP be (c1, c2), and let s1x + s2y = u1 and t1x + t2y = v1 be
two intersecting edges of the LP feasible region. Let α1x + α2y = β be an additional constraint.
The intersection points of this constraint with the two lines, if they exist, are given by

(cid:18) s2β − uα2
s2α1 − s1α2

,

s1β − uα1
s1α2 − s2α1

(cid:19)

and

(cid:18) t2β − vα2
t2α1 − t1α2

,

t2β − vα1
t1α2 − t2α1

(cid:19)

.

The indiﬀerence surface is thus given by

c1

s2β − uα2
s2α1 − s1α2

+ c2

s1β − uα1
s1α2 − s2α1

= c1

t2β − vα2
t2α1 − t1α2

+ c2

t2β − vα1
t1α2 − t2α1

.

For α1, α2 such that s2α1 − s1α2 (cid:54)= 0 and t2α1 − t1α2 (cid:54)= 0, clearing denominators and some
manipulation yields

(c1α2 − c2α1)((ut1 − vs1)α2 − (ut2 − vs2)α1 + (s2t2 − t1s2)β) = 0.

This curve consists of the two planes c1α2 − c2α1 = 0 and (ut1 − vs1)α2 − (ut2 − vs2)α1 + (s2t2 −
t1s2)β = 0.

18

Figure 3: Decomposition of the parameter space: the blue region contains the set of (α1, α2) such
that the constraint intersects the feasible region at x = 1 and x = y. The red lines consist of all
(α1, α2) such that the objective value is equal at these intersection points. The red lines partition
the blue region into two components: one where the new optimum is achieved at the intersection
of h and x = y, and one where the new optimum is achieved at the intersection of h and x = 1.

This is however not true if n > 2. For example, consider an LP in three variables x, y, z with
the constraints x + y ≤ 1, x + z ≤ 1, x ≤ 1, z ≤ 1. Writing out the indiﬀerence surface (assuming
the objective is c = (1, 1, 1)T ) for the vertex on the intersection of {x + y = 1, x = 1} and the
vertex on {x + z = 1, z = 1} yields

α1α2 − α2β − α2

3 + α3β = 0.

Setting β = 1, we can plot the resulting surface in α1, α2, α3 (Figure 4).

B.2 Linear programming sensitivity for multiple constraints

Lemma B.1. Let (c, A, b) be an LP and let M denote the set of its m constraints. Let x∗
LP and z∗
LP
denote the optimal solution and its objective value, respectively. For F ⊆ M , let AF ∈ R|F |×n and
bF ∈ R|F | denote the restrictions of A and b to F . For k ≤ n, α1, . . . , αk ∈ Rn, β1, . . . , βk ∈ R, and
F ⊆ M with |F | = n − k, let AF,α1,...,αk ∈ Rn×n denote the matrix obtained by adding row vectors
∈ Rn×n be the matrix AF,α1,...,αk ∈ Rn×n with the ith
α1, . . . , αk to AF and let Ai
(cid:3)T . There is a set of at most K hyperplanes, nKnmn degree-K
column replaced by (cid:2)bF β1
polynomial hypersurfaces, and nKnm2n degree-2K polynomial hypersurfaces partitioning RK(n+1)
into connected components such that for each component C, one of the following holds: either (1)
x∗
Kx ≤ βK) = x∗
LP(αT
LP, or (2) there is a subset of cuts indexed by (cid:96)1, . . . , (cid:96)k ∈ [K]
and a set of constraints F ⊆ M with |F | = n − k such that

F,α1,β1,...,αk,βk
· · · βk

1 x ≤ β1, . . . , αT

LP(αT
x∗

1 x ≤ β1, . . . , αT

Kx ≤ βK) =

(cid:32) det(A1

F,α(cid:96)1 ,β(cid:96)1 ,...,α(cid:96)k
det(AF,α(cid:96)1 ,...,α(cid:96)k

)

,β(cid:96)k
)

, . . . ,

det(An

F,α(cid:96)1 ,β(cid:96)1 ,...,α(cid:96)k
det(AF,α(cid:96)1 ,...,α(cid:96)k

(cid:33)

)

,

,β(cid:96)k
)

for all (α1, β1, . . . , αK, βK) ∈ C.

Proof. First, if none of αT
LP and z∗
βK) = x∗

LP(αT

1 x ≤ β1, . . . , αT

Kx ≤ βK separate x∗

1 x ≤ β1, . . . , αT

Kx ≤ βK) = z∗

LP, then x∗

LP(αT
Kx ≤
LP. The set of all such cuts is given by the

1 x ≤ β1, . . . , αT

19

Figure 4: Indiﬀerence surface for two edges of the feasible region of an LP in three variables.

intersection of halfspaces in RK(n+1) given by

(cid:110)

(α1, β1, . . . , αk, βk) ∈ RK(n+1) : αT

j x∗

LP ≤ βj

(cid:111)

.

(11)

K
(cid:92)

j=1

All other vectors of K cuts contain at least one cut that separates x∗
LP, and those cuts therefore
pass through P = {x ∈ Rn : Ax ≤ b, x ≥ 0}. The new LP optimum is thus achieved at a vertex
created by the cuts that separate x∗
LP. As in the proof of Theorem 3.1, we consider all possible
new vertices formed by our set of K cuts. In the case of a single cut, these new vertices necessarily
were on edges of P, but now they may lie on higher dimensional faces.

1 x ≤ β1, . . . , αT

Consider a subset of k ≤ n cuts that separate x∗

LP. Without loss of generality, denote these
cuts by αT
k x ≤ βk. We now establish conditions for these k cuts to “jointly” form
a new vertex of P. Any vertex created by these cuts must lie on a face f of P with dim(f ) = k
(in the case that k = n, the relevant face f with dim(f ) = n is P itself). Letting M denote the set
of m constraints that deﬁne P, each dimension-k face f of P can be identiﬁed with a (potentially
empty) subset F ⊂ M of size n − k such that f is precisely the set of all points x such that

aT
aT

i x = bi
i x ≤ bi

∀ i ∈ F

∀ i ∈ M \ F,

where ai is the ith row of A. Let AF ∈ Rn−k×n denote the restriction of A to only the rows in F ,
and let bF ∈ Rn−k denote the entries of b corresponding to the constraints in F . Consider removing
the inequality constraints deﬁning the face. The intersection of the cuts αT
k x ≤ βk
and this unbounded surface (if it exists) is precisely the solution to the system of n linear equations

1 x ≤ β1, . . . , αT

AF x = bF
αT
1 x = β1
...
k x = βk.

αT

20

Let AF,α1,...,αk ∈ Rn×n denote the matrix obtained by adding row vectors α1, . . . , αk to AF , and
let Ai

∈ Rn×n denote the matrix AF,α1,...,αk where the ith column is replaced by

F,α1,β1,...,αk,βk








∈ Rn.








bF
β1
...
βk

By Cramer’s rule, the solution to this system is given by

x =

(cid:32) det(A1

F,α1,β1,...,αk,βk
det(AF,α1,...,αk )

)

, . . . ,

det(An

F,α1,β1,...,αk,βk
det(AF,α1,...,αk )

(cid:33)

)

,

and the value of the objective at this point is

cT x =

n
(cid:88)

i=1

ci ·

det(Ai

F,α1,β1,...,αk,βk
det(AF,α1,...,αk )

)

.

Now, to ensure that the unique intersection point x (1) exists and (2) actually lies on f (or simply
lies in P, in the case that F = ∅) , we stipulate that it satisﬁes the inequality constraints in M \ F .
That is,

n
(cid:88)

j=1

aij

det(A1

F,α1,β1,...,αk,βk
det(AF,α1,...,αk )

)

≤ bi

(12)

If α1, β1 . . . , αk, βk satisﬁes any of these constraints,

it must be that
for every i ∈ M \ F .
det(AF,α1,...,αk ) (cid:54)= 0, which guarantees that AF x = bF , αT
k x = βk indeed has a
unique solution. Now, det(AF,α1,...,αk ) is a polynomial in α1, . . . , αk of degree ≤ k, since it is mul-
tilinear in each coeﬃcient of each α(cid:96), (cid:96) = 1, . . . , k. Similarly, det(A1
) is a polynomial
in α1, β1, . . . , αk, βk of degree ≤ k, again because it is multilinear in each cut parameter. Hence,
the boundary each constraint of the form given by Equation 12 is a polynomial of degree at most
k.

1 x = β1, . . . , αT

F,α1,β1,...,αk,βk

The collection of these polynomials for every k, every subset of {αT

Kx ≤ βK}
of size k, and every face of P of dimension k, along with the hyperplanes determining separa-
tion constraints (Equation 11), partition RK(n+1) into connected components such that for all
(α1, β1, . . . , αK, βK) within a given connected component, there is a ﬁxed subset of K and a ﬁxed
set of faces of P such that the cuts with indices in that subset intersect every face in the set at a
common vertex.

1 x ≤ β1, . . . , αT

Now, consider a single connected component, denoted by C. Let f1, . . . , f(cid:96) denote the faces
intersected by vectors of cuts in C, and let (without loss of generality) 1, . . . , k denote the subset of
cuts that intersect these faces. Let F1, . . . , F(cid:96) ⊂ M denote the sets of constraints that are binding
at each of these faces, respectively. For each pair fp, fq, consider the surface

n
(cid:88)

i=1

ci ·

det(Ai

Fp,α1,β1,...,αk,βk
det(AFp,α1,...,αk )

)

=

n
(cid:88)

i=1

ci ·

det(Ai

Fq,α1,β1,...,αk,βk
det(AFq,α1,...,αk )

)

,

which can be equivalently written as

n
(cid:88)

i=1

ci ·det(Ai

Fp,α1,β1,...,αk,βk

) det(AFq,α1,...,αk ) =

n
(cid:88)

i=1

21

ci ·det(Ai

Fq,α1,β1,...,αk,βk

) det(AFp,α1,...,αk ). (13)

This is a degree-2k polynomial hypersurface in (α1, β1, . . . , αK, βK) ∈ RK(n+1). This hypersurface
is precisely the set of all cut vectors for which the LP objective achieved at the vertex on face fp is
equal to the LP objective value achieved at the vertex on face fq. The collection of these surfaces
for each p, q partitions C into further connected components. Within each of these connected
components, the face containing the vertex that maximizes the objective is invariant, and the
subset of cuts passing through that vertex is invariant. If F ⊆ M is the set of binding constraints
representing this face, and (cid:96)1, . . . , (cid:96)k ∈ [K] represent the subset of cuts intersecting this face,
LP(αT
x∗

Kx ≤ βK) have the closed forms:

Kx ≤ βK) and z∗

1 x ≤ β1, . . . , αT

1 x ≤ β1, . . . , αT

LP(αT
(cid:32) det(A1

F,α(cid:96)1 ,β(cid:96)1 ,...,α(cid:96)k
det(AF,α(cid:96)1 ,...,α(cid:96)k

)

,β(cid:96)k
)

, . . . ,

det(An

F,α(cid:96)1 ,β(cid:96)1 ,...,α(cid:96)k
det(AF,α(cid:96)1 ,...,α(cid:96)k

(cid:33)

)

,

,β(cid:96)k
)

LP(αT
x∗

1 x ≤ β1, . . . , αT

Kx ≤ βK) =

and

LP(αT
z∗

1 x ≤ β1, . . . , αT

Kx ≤ βK) =

n
(cid:88)

i=1

ci ·

det(Ai

F,α(cid:96)1 ,β(cid:96)1 ,...,α(cid:96)k
det(AF,α(cid:96)1 ,...,α(cid:96)k

)

.

,β(cid:96)k
)

for all (α1, β1, . . . , αK, βK) within this component. We now count the number of surfaces used to
obtain our decomposition. First, we added K hyperplanes encoding separation constraints for each
of the K cuts (Equation 11). Then, for every subset S ⊆ K of size ≤ n, and for every face F of P
with dim(F ) = |S|, we ﬁrst considered at most |M \ F | ≤ m degree-≤ K polynomial hypersurfaces
representing decision boundaries for when cuts in S intersected that face (Equation 12). The
number of k-dimensional faces of P is at most (cid:0) m
(cid:1) ≤ mn−k ≤ mn−1, so the total number of
n−k
(cid:1))mn ≤ nKnmn. Finally, we considered a degree-2K
these hypersurfaces is at most ((cid:0)K
0
polynomial hypersurface for every subset of cuts and every pair of faces with degree equal to the
size of the subset, of which there are at most nKn(cid:0)mn
2

(cid:1) + · · · + (cid:0)K
n

(cid:1) ≤ nKnm2n.

C Omitted results and proofs from Section 4

LP(αT x ≤ β, σ1) = x∗

Proof of Lemma 4.1. Consider as an example σ1 = {x[1] ≤ 1, x[1] ≤ 5} and σ2 = {x[1] ≤ 1}. We
LP(αT x ≤ β, σ2) for any cut αT x ≤ β, because the constraint x[1] ≤ 5
have x∗
is redundant in σ1. More generally, any σ ⊆ BC can be reduced by preserving only the tightest
≤ constraint and tightest ≥ constraint without aﬀecting the resulting LP optimal solutions. The
number of such unique reduced sets is at most ((τ + 2)2)n < τ 3n (for each variable, there are
τ + 2 possibilities for the tightest ≤ constraint: no constraint or one of x[i] ≤ 0, . . . , x[i] ≤ τ , and
similarly τ + 2 possibilities for the ≥ constraint).

Proof of Lemma 4.2. We carry out the same reasoning in the proof of Theorem 3.1 for each reduced
(cid:1) ≤ (m + |σ|)n−1. For each edge E, we considered
σ. The number of edges of P(σ) is at most (cid:0)m+|σ|
n−1
at most |(M ∪ σ) \ E| ≤ m + |σ| hyperplanes, for a total of at most (m + |σ|)n halfspaces. Then,
we had a degree-2 polynomial hypersurface for every pair of edges, of which there are at most
(cid:0)(m+|σ|)n
(cid:1) ≤ (m + |σ|)2n. Summing over all reduced σ (of which there are at most τ 3n), combined
2
with the fact that if σ is reduced then |σ| ≤ 2n, we get a total of at most (m + 2n)nτ 3n hyperplanes
and at most (m + 2n)2nτ 3n degree-2 hypersurfaces, as desired.

Proof of Lemma 4.4. Fix a connected component C in the decomposition that includes the facets
deﬁning V and the surfaces obtained in Lemma 4.3. For all σ ∈ BC, xI ∈ PI, and i = 1, . . . , n,
consider the surface

LP(αT x ≤ β, σ)[i] = xI[i].
x∗

(14)

22

This surface is a hyperplane, since by Lemma 4.2, either x∗
LP(σ)[i] or
det(Ai
E,α,β,σ)
x∗
LP(αT x ≤ β, σ)[i] =
det(AE,α,σ) , where E ⊆ M ∪ σ is the subset of constraints correspond-
ing to σ and C. Clearly, within any connected component of C induced by these hyperplanes, for
LP(αT x ≤ β, σ) ∈ Zn
every σ and xI ∈ PI, 1[x∗
for some cut αT x ≤ β within a given connected component, x∗
LP(αT x ≤ β, σ) = xI for some
LP(αT x ≤ β, σ) = xI ∈ Zn for all cuts αT x ≤ β in that
xI ∈ PIH(σ) ⊆ PI, which means that x∗
connected component.

LP(αT x ≤ β, σ) = xI] is invariant. Finally, if x∗

LP(αT x ≤ β, σ)[i] = x∗

(cid:1) ≤
We now count the number of hyperplanes given by Equation 14. For each σ, there are (cid:0)m+|σ|
n−1
(m + 2n)n−1 binding edge constraints E ⊆ M ∪ σ deﬁning the formula of Lemma 4.2, and we have
n|PI| hyperplanes for each E. Since τ = maxx∈PI (cid:107)x(cid:107)∞, |PI| ≤ τ n. So the total number of
hyperplanes given by Equation 14 is at most τ 3n(m + 2n)n−1nτ n ≤ (m + 2n)nτ 4n. The number of
facets deﬁning V is at most |PIH| ≤ |PI| ≤ τ n. Adding these to the counts obtained in Lemma 4.3
yields the ﬁnal tallies in the lemma statement.

Proof of Theorem 4.5. Fix a connected component C in the decomposition induced by the set of
hyperplanes and degree-2 hypersurfaces established in Lemma 4.4. Let

Q1, . . . , Qi1, I1, Qi1+1, . . . , Qi2, I2, Qi2+1, . . .

(15)

denote the nodes of the tree branch-and-cut creates, in order of exploration, under the assumption
that a node is pruned if and only if either the LP at that node is infeasible or the LP optimal solution
is integral (so the “bounding” of branch-and-bound is suppressed). Here, a node is identiﬁed by
the list σ of branching constraints added to the input IP. Nodes labeled by Q are either infeasible
or have fractional LP optimal solutions. Nodes labeled by I have integral LP optimal solutions and
are candidates for the incumbent integral solution at the point they are encountered. (The nodes
are functions of α and β, as are the indices i1, i2, . . ..) By Lemma 4.4 and the observation following
it, this ordered list of nodes is invariant over all (α, β) ∈ C.

Now, given an node index (cid:96), let I((cid:96)) denote the incumbent node with the highest objective value
encountered up until the (cid:96)th node searched by B&C, and let z(I((cid:96))) denote its objective value. For
each node Q(cid:96), let σ(cid:96) denote the branching constraints added to arrive at node Q(cid:96). The hyperplane

(16)

LP(αT x ≤ β, σ(cid:96)) = z(I((cid:96)))
z∗
(which is a hyperplane due to Lemma 4.2) partitions C into two subregions. In one subregion,
LP(αT x ≤ β, σ(cid:96)) ≤ z(I((cid:96))), that is, the objective value of the LP optimal solution is no greater
z∗
than the objective value of the current incumbent integer solution, and so the subtree rooted at
Q(cid:96) is pruned. In the other subregion, z∗
LP(αT x ≤ β, σ(cid:96)) > z(I((cid:96))), and Q(cid:96) is branched on further.
Therefore, within each connected component of C induced by all hyperplanes given by Equation 16
for all (cid:96), the set of node within the list (15) that are pruned is invariant. Combined with the
surfaces established in Lemma 4.4, these hyperplanes partition Rn+1 into connected components
such that as (α, β) varies within a given component, the tree built by branch-and-cut is invariant.
Finally, we count the total number of surfaces inducing this partition. Unlike the counting stages
of the previous lemmas, we will ﬁrst have to count the number of connected components induced by
the surfaces established in Lemma 4.4. This is because the ordered list of nodes explored by branch-
and-cut (15) can be diﬀerent across each component, and the hyperplanes given by Equation 16
depend on this list. From Lemma 4.4 we have 3(m + 2n)nτ 4n hyperplanes, 3(m + 2n)3nτ 4n degree-2
polynomial hypersurfaces, and (m + 2n)6nτ 4n degree-5 polynomial hypersurfaces. To determine
the connected components of Rn+1 induced by the zero sets of these polynomials, it suﬃces to
consider the zero set of the product of all polynomials deﬁning these surfaces. Denote this product

23

polynomial by p. The degree of the product polynomial is the sum of the degrees of 3(m +
2n)nτ 4n degree-1 polynomials, 3(m + 2n)3nτ 4n degree-2 polynomials, and (m + 2n)6nτ 4n degree-5
polynomials, which is at most 3(m+2n)nτ 4n+2·3(m+2n)3nτ 4n+5·(m+2n)6nτ 4n < 14(m+2n)3nτ 4n.
By Warren’s theorem, the number of connected components of Rn+1 \ {(α, β) : p(α, β) = 0} is
O((14(m + 2n)3nτ 4n)n−1), and by the Milnor-Thom theorem, the number of connected components
of {(α, β) : p(α, β) = 0} is O((14(m + 2n)3nτ 4n)n−1) as well. So, the number of connected
components induced by the surfaces in Lemma 4.4 is O(14n(m + 2n)3n2τ 4n2). For every connected
LP(αT x ≤ β, σ(cid:96)) is already determined due to
component C in Lemma 4.4, the closed form of z∗
Lemma 4.2, and so the number of hyperplanes given by Equation 16 is at most the number of
possible σ ⊆ BC, which is at most τ 3n. So across all connected components C, the total number
of hyperplanes given by Equation 16 is O(14n(m + 2n)3n2τ 5n2). Finally, adding this to the surface-
counts established in Lemma 4.4 yields the lemma statement.

C.1 Product scoring rule for variable selection

Let σ be the set of branching constraints added thus far. The product scoring rule branches on the
variable i ∈ [n] that maximizes:

max{z∗

LP(σ) − z∗

LP(xi ≤ (cid:98)x∗

LP(σ)[i](cid:99), σ), γ} · max{z∗

LP(σ) − z∗

LP(xi ≥ (cid:100)x∗

LP(σ)[i](cid:101), σ), γ},

where γ = 10−6.

Lemma C.1. There is a set of of at most 3(m + 2n)nτ 3n hyperplanes and (m + 2n)2nτ 3n degree-
2 polynomial hypersurfaces partitioning Rn+1 into connected components such that for any con-
LP(αT x ≤ β, σ)[i](cid:5) , xi ≥
nected component C and any σ, the set of branching constraints {xi ≤ (cid:4)x∗
(cid:6)x∗

LP(αT x ≤ β, σ)[i](cid:7) | i ∈ [n]} is invariant across all (α, β) ∈ C.

Proof. Fix a connected component C in the decomposition established in Lemma 4.2. By Lemma 4.2,
LP(αT x ≤
for each σ, either x∗
det(Ai
E,α,β,σ)
for all (α, β) ∈ C. Fix a variable i ∈ [n], which corresponds to two branching
det(AE,α,σ)

LP(σ) or there exists E ⊆ M ∪ σ such that x∗

LP(αT x ≤ β, σ) = x∗

β, σ)[i] =
constraints

xi ≤ (cid:4)x∗

LP(αT x ≤ β, σ)[i](cid:5) and xi ≥ (cid:6)x∗

LP(αT x ≤ β, σ)[i](cid:7) .

(17)

If C is a component where x∗
LP(σ), then these two branching constraints are
trivially invariant over (α, β) ∈ C. Otherwise, in order to further decompose C such that the
right-hand-sides of these constraints are invariant for every σ, we add the two decision boundaries
given by

LP(αT x ≤ β, σ) = x∗

k ≤

det(Ai
E,α,β,σ)
det(AE,α,σ)

≤ k + 1

for every i, σ, and every integer k = 0, . . . , τ − 1, where τ = maxx∈P∩Zn (cid:107)x(cid:107)∞. This ensures that
within every connected component of C induced by these boundaries (hyperplanes),
(cid:38) det(Ai

(cid:36) det(Ai

(cid:37)

(cid:39)

(cid:4)x∗

LP(αT x ≤ β, σ)[i](cid:5) =

and (cid:6)x∗

LP(αT x ≤ β, σ)[i](cid:7) =

E,α,β,σ)
det(AE,α,σ)

E,α,β,σ)
det(AE,α,σ)

are invariant, so the branching constraints from Equation (17) are invariant. For a ﬁxed σ, there
are two hyperplanes for every E ⊆ M ∪ σ corresponding to an edge of P(σ) and i = 1, . . . , n, for
a total of at most 2n(cid:0)m+|σ|
(cid:1) ≤ 2n(m + |σ|)n−1 hyperplanes. Summing over all reduced σ, we get a
n−1
total of 2n(m + 2n)n−1τ 3n < 2(m + 2n)nτ 3n hyperplanes. Adding these hyperplanes to the set of
hyperplanes established in Lemma 4.2 yields the lemma statement.

24

Proof of Lemma 4.3. Fix a connected component C in the decomposition established in Lemma C.1.
We know that for each set of branching constraints σ:

• By Lemma 4.2, either x∗
det(Ai
E,α,β,σ)
LP(αT x ≤ β, σ)[i] =
x∗
det(AE,α,σ)

LP(αT x ≤ β, σ) = x∗

LP(σ) or there exists E ⊆ M ∪ σ such that

for all (α, β) ∈ C and all i ∈ [n], and

• The set of branching constraints {xi ≤ (cid:4)x∗
[n]} is invariant across all (α, β) ∈ C.

LP(αT x ≤ β, σ)[i](cid:5) , xi ≥ (cid:6)x∗

LP(αT x ≤ β, σ)[i](cid:7) | i ∈

Suppose that σ is the list of branching constraints added so far. For any variable k ∈ [n], let

k = (xk ≤ (cid:4)x∗
σ−
So long as (α, β) ∈ C, σ−
rule as

LP(αT x ≤ β, σ)[k](cid:5) , σ) and σ+
k and σ+

k = (xk ≥ (cid:6)x∗

LP(αT x ≤ β, σ)[k](cid:7) , σ).

k are ﬁxed. With this notation, we can write the product scoring

max{z∗

LP(αT x ≤ β, σ) − z∗

LP(αT x ≤ β, σ−

k ), γ} · max{z∗

LP(αT x ≤ β, σ) − z∗

LP(αT x ≤ β, σ+

k ), γ},

where γ = 10−6.

By Lemma 4.2, we know that across all (α, β) ∈ C, either z∗

LP(αT x ≤ β, σ+

k ) = z∗

LP(σ+

k ) or

there exists E+

k ⊆ M ∪ σ+

k such that

z∗
LP

(cid:0)αT x ≤ β, σ+
k

(cid:1) =

n
(cid:88)

i=1

ci ·

det

(cid:16)

Ai
(cid:16)

E+

k ,α,β,σ+

k

det

AE+

k ,α,σ+

k

(cid:17)

(cid:17) ,

and similarly for σ−
k . Therefore, for each k ∈ [n],
there is a single degree-2 polynomial hypersurface partitioning C into connected components such
that within each connected component, either

k , deﬁned according to some edge set E−

k ⊆ M ∪ σ−

LP(αT x ≤ β, σ) − z∗
z∗

LP(αT x ≤ β, σ−

k ) ≥ γ

(18)

or vice versa, and similarly for σ+
forms:

k . In particular, the former hypersurface will have one of four

1. z∗

LP(σ) − z∗

LP(σ−

k ) ≥ γ, which is uniformly satisﬁed or not satisﬁed across all (α, β) ∈ C,

2. z∗

LP(σ) − (cid:80)n

i=1 ci ·

(cid:19)

,α,β,σ−
(cid:19) ≥ γ, which is a hyperplane,
k

E−
k

(cid:18)

det

Ai

(cid:18)

det

A

E−
k

,α,σ−
k

3. (cid:80)n

i=1 ci ·

E,α,β,σ)
det(Ai
det(AE,α,σ)

− z∗

LP(σ−

k ) ≥ γ, which is a hyperplane, or

4. (cid:80)n

i=1 ci





det(Ai
E,α,β,σ)
det(AE,α,σ)

−

(cid:18)

det

Ai

(cid:18)

det

A

E−
k

E+
k

,α,σ−
k

(cid:19)



,α,β,σ−
k
(cid:19)

 ≥ γ, which is a degree-2 polynomial hypersurface.

Simply said, these are all degree-2 polynomial hypersurfaces.

Within any region induced by these hypersurfaces, the comparison between any two variables

xk and xj will have the form

max{z∗

LP(αT x ≤ β, σ) − z∗

LP(αT x ≤ β, σ−

k ), γ} · max{z∗

LP(αT x ≤ β, σ) − z∗

LP(αT x ≤ β, σ+

k ), γ}

25

≥ max{z∗

LP(αT x ≤ β, σ) − z∗

LP(αT x ≤ β, σ−

j ), γ} · max{z∗

LP(αT x ≤ β, σ) − z∗

LP(αT x ≤ β, σ+

j ), γ}

which at its most complex will equal












n
(cid:88)

i=1

ci

≥

n
(cid:88)

i=1

ci

(cid:17)

(cid:16)

Ai

det

E,α,β,σ
det (AE,α,σ)

(cid:17)

(cid:16)

Ai

det

E,α,β,σ
det (AE,α,σ)

(cid:16)

Ai
(cid:16)

det

−

det

E−

k ,α,β,σ−
k
(cid:17)

AE−

k ,α,σ−

k

(cid:17)




 ·






n
(cid:88)

i=1

ci

(cid:17)

(cid:16)

Ai

det

E,α,β,σ
det (AE,α,σ)

det

−

(cid:16)

Ai
(cid:16)

E+

k ,α,β,σ+
k
(cid:17)

AE+

k ,α,σ+

k

(cid:17)






(19)


(cid:19)

det

det

(cid:18)

Ai

(cid:18)

det

Ai

−

(cid:16)

det

(cid:19)



E−

j ,α,β,σ−
(cid:17)

j

AE−

j ,α,σ−

j





·

n
(cid:88)

i=1

ci







(cid:17)

(cid:16)

Ai

det

E,α,β,σ
det (AE,α,σ)

−

(cid:16)

det

E+

j ,α,β,σ+
(cid:17)

j

AE+

j ,α,σ+

j

.





This inequality can be written as a degree-5 polynomial hypersurface. In any region induced by
these hypersurfaces, the variable that branch-and-cut branches on will be ﬁxed.

We now count the total number of hypersurfaces. First, we count the number of degree-2
polynomial hypersurfaces from Equation (18): there is a hypersurface deﬁned by each variable
xk, set of branching constraints σ, cutoﬀ t ∈ [τ ] such that σ−
k = (xk ≤ t, σ), set E ⊆ M ∪ σ
k ⊆ M ∪ σ−
corresponding to an edge of P(σ), and set E−
k ). For a
(cid:1) ≤ 2nτ (m + |σ| + 1)2(n−1) hypersurfaces. Summing
ﬁxed σ, this amounts to 2nτ (cid:0)m+|σ|
n−1
over all τ 3n reduced σ, we have 2nτ 3n+1(m + 2n + 1)2(n−1) degree-2 polynomial hypersurfaces.

k (and similarly for σ+

(cid:1)(cid:0)m+|σ|+1
n−1

k and E+

Next, we count the number of degree-5 polynomial hypersurfaces from Equation (19): there
is a hypersurface deﬁned by each pair of variables xk, xj, set of branching constraints σ, cutoﬀs
tk, tj ∈ [τ ] such that σ−
j , E+
j
corresponding to edges of P(σ), P(σ−
j ). For a ﬁxed σ, this amounts to
(cid:1)4
n2τ 2(cid:0)m+|σ|
≤ n2τ 2(m + |σ| + 1)5(n−1) hypersurfaces. Summing over all τ 3n reduced σ,
n−1
we have n2τ 3n+2(m + 2n + 1)5(n−1) degree-5 polynomial hypersurfaces.

j = (xj ≤ tj, σ), and sets E, E−

k = (xk ≤ tk, σ) and σ−

(cid:1)(cid:0)m+|σ|+1
n−1

k ), P(σ+

k ), P(σ−

j ), P(σ+

k , E+

k , E−

Adding these hypersurfaces to those from Lemma C.1, we get the lemma statement.

C.2 Extension to multiple cutting planes

LP(αT

1 x ≤ β1, . . . , αT

We can similarly derive a multi-cut version of Lemma 4.2 that controls x∗
Kx ≤
βK, σ) for any set of branching constraints. We use the following notation. Let (c, A, b) be an LP
and let M denote the set of its m constraints. For F ⊆ M ∪ σ, let AF,σ ∈ R|F |×n and bF,σ ∈ R|F |
denote the restrictions of Aσ and bσ to F . For α1, . . . , αk ∈ Rn, β1, . . . , βk ∈ R, and F ⊆ M ∪ σ
with |F | = n − k, let AF,α1,...,αk,σ ∈ Rn×n denote the matrix obtained by adding row vectors
F,α1,β1,...,αk,βk,σ ∈ Rn×n be the matrix AF,α1,...,αk,σ ∈ Rn×n with the
α1, . . . , αk to AF,σ and let Ai
ith column replaced by (cid:2)bF,σ β1
Corollary C.2. Fix an IP (c, A, b). There is a set of at most K hyperplanes, nKn(m + 2n)nτ 3n
degree-K polynomial hypersurfaces, and nKn(m + 2n)2nτ 3n degree-2K polynomial hypersurfaces
partitioning RK(n+1) into connected components such that for each component C and every σ ⊆ BC,
one of the following holds: either (1) x∗
LP(σ), or (2) there is
a subset of cuts indexed by (cid:96)1, . . . , (cid:96)k ∈ [K] and a set of constraints F ⊆ M ∪ σ with |F | = n − k
such that

Kx ≤ βK, σ) = x∗

1 x ≤ β1, . . . , αT

LP(αT

· · · βk

(cid:3)T .

LP(αT
x∗

1 x ≤ β1, . . . , αT

Kx ≤ βK, σ) =

(cid:32) det(A1

F,α(cid:96)1 ,β(cid:96)1 ,...,α(cid:96)k
det(AF,α(cid:96)1 ,...,α(cid:96)k

,σ)

,β(cid:96)k
,σ)

, . . . ,

det(An

F,α(cid:96)1 ,β(cid:96)1 ,...,α(cid:96)k
det(AF,α(cid:96)1 ,...,α(cid:96)k

(cid:33)

,σ)

,

,β(cid:96)k
,σ)

26

for all (α1, β1, . . . , αK, βK) ∈ C.

Proof. The exact same reasoning in the proof of Lemma B.1 applies. We still have K hyperplanes.
Now, for each σ, for each subset S ⊆ K with |S| ≤ n, and for every face F of P(σ) with dim(F ) =
|S|, we have at most m degree-K polynomial hypersurfaces. The number of k-dimensional faces
of P(σ) is at most (cid:0)m+|σ|
(cid:1) ≤ (m + 2n)n−1, so the total number of these hypersurfaces is at most
n−k
nKn(m + 2n)nτ 3n. Finally, for every σ, we considered a degree-2K polynomal hypersurfaces for
every subset of cuts and every pair of faces with degree equal to the size of the subset, of which
there are at most nKn(m + 2n)2nτ 3n, as desired.

We now reﬁne the decomposition obtained in Lemma 4.2 so that the branching constraints
added at each step of branch-and-cut are invariant within a region. For ease of exposition, we
assume that branch-and-cut uses a lexicographic variable selection policy. This means that the
variable branched on at each node of the search tree is ﬁxed and given by the lexicographic ordering
x1, . . . , xn. Generalizing the argument to work for other policies, such as the product scoring rule,
can be done as in the single-cut case.

Lemma C.3. Suppose branch-and-cut uses a lexicographic variable selection policy. Then, there
is a set of of at most K hyperplanes, 3n2Kn(m + 2n)nτ 3n degree-K polynomial hypersurfaces, and
nKn(m + 2n)2nτ 3n degree-2K polynomial hypersurfaces partitioning Rn+1 into connected compo-
nents such that within each connected component, the branching constraints used at every step of
branch-and-cut are invariant.

Proof. Fix a connected component C in the decomposition established in Corollary C.2. Then, by
1 x ≤ β1, . . . , αT
Corollary C.2, for each σ, either x∗
LP(σ) or there exists cuts
(without less of generality) labeled by indices 1, . . . , k ∈ [K] and there exists F ⊆ M ∪ σ such that

Kx ≤ βK, σ) = x∗

LP(αT

LP(αT
x∗

1 x ≤ β1, . . . , αT

Kx ≤ βK, σ)[i] =

det(Ai

F,α1,β1,...,αk,βk,σ)

det(AF,α1,...,αk,σ)

for all (α, β) ∈ C and all i ∈ [n]. Now, if we are at a stage in the branch-and-cut tree where σ is
the list of branching constraints added so far, and the ith variable is being branched on next, the
two constraints generated are

xi ≤ (cid:4)x∗

LP(αT

1 x ≤ β1, . . . , αT

Kx ≤ βK, σ)[i](cid:5) and xi ≥ (cid:6)x∗

LP(αT

1 x ≤ β1, . . . , αT

Kx ≤ βK, σ)[i](cid:7) ,

respectively. If C is a component where x∗
LP(σ), then there
is nothing more to do, since the branching constraints at that point are trivially invariant over
(α1, β1, . . . , αK, βK) ∈ C. Otherwise, in order to further decompose C such that the right-hand-
side of these constraints are invariant for every σ and every i = 1, . . . , n, we add the two decision
boundaries given by

Kx ≤ βK, σ) = x∗

1 x ≤ β1, . . . , αT

LP(αT

k ≤

det(Ai

F,α1,β1,...,αk,βk,σ)

det(AF,α1,...,αk,σ)

≤ k + 1

for every i, σ, and every integer k = 0, . . . , τ − 1, where τ = (cid:100)maxx∈P (cid:107)x(cid:107)∞(cid:101). This ensures
that within every connected component of C induced by these boundaries (degree-K polynomial
hypersurfaces),

(cid:4)x∗

LP(αT x ≤ β, σ)[i](cid:5) =

(cid:36) det(Ai

F,α1,β1,...,αk,βk,σ)

(cid:37)

det(AF,α1,...,αk,σ)

27

and

(cid:6)x∗

LP(αT x ≤ β, σ)[i](cid:7) =

(cid:38) det(Ai

F,α1,β1,...,αk,βk,σ)

(cid:39)

det(AF,α1,...,αk,σ)

are invariant, so the branching constraints added by, for example, a lexicographic branching
rule, are invariant. For a ﬁxed σ, there are two hypersurfaces for every subset S ⊆ [K], every
F ⊆ M ∪ σ corresponding to a |S|-dimensional face of P(σ), and every i = 1, . . . , n, for a total
of at most 2n2Kn(cid:0)m+|σ|
(cid:1) ≤ 2n2Kn(m + 2n)n. Summing over all reduced σ, we get a total of
|S|
2n2Kn(m + 2n)nτ 3n hypersurfaces. Adding these hypersurfaces to the set of hypersurfaces estab-
lished in Corollary C.2 yields the lemma statement.

Now, as in the single-cut case, we consider the constraints that ensure that all cuts are valid.
Let V ⊆ RK(n+1) denote the set of all vectors of valid K cuts. As before, V is a polyhedron, since
we may write

K
(cid:92)

(cid:92)

V =

k=1

xIH∈PIH

(cid:110)

(α1, β1, . . . , αK, βk) ∈ RK(n+1) : αT

k xIH ≤ βk

(cid:111)

.

We now reﬁne our decomposition further to control the integrality of the various LP solutions

at each node of branch-and-cut.

Lemma C.4. Given an IP (c, A, b), there is a set of at most 2Kτ n hyperplanes, 4n2Kn(m+2n)nτ 4n
degree-K polynomial hypersurfaces, and nKn(m + 2n)2nτ 3n degree-2K polynomial hypersurfaces
partitioning RK(n+1) into connected components such that for each component C, and each σ ⊆ BC,

1 (cid:2)x∗
LP

(cid:0)αT

1 x ≤ β1, . . . , αT

Kx ≤ βK, σ(cid:1) ∈ Zn(cid:3)

is invariant for all (α1, β1, . . . , αK, βK) ∈ C.

Proof. Fix a connected component C in the decomposition that includes the facets deﬁning V and
the surfaces obtained in Lemma C.3. For all σ ∈ BC, xI ∈ PI, and i = 1, . . . , n, consider the surface

x∗
LP

(cid:0)αT

1 x ≤ β1, . . . , αT

Kx ≤ βK, σ(cid:1) [i] = xI[i].

(20)

LP(αT

This surface is a polynomial hypersurface of degree at most K, due to Corollary C.2. Clearly,
within any connected component of C induced by these hyperplanes, for every σ and xI ∈ PI,
LP(αT
Kx ≤ βK, σ) = xI] is invariant. Finally, if x∗
1 x ≤ β1, . . . , αT
1[x∗
Kx ≤
βK, σ) ∈ Zn for some K cuts αT
Kx ≤ βK within a given connected component,
x∗
LP(αT
1 x ≤
β1, . . . , αT
Kx ≤ βK in that
connected component.

Kx ≤ βK, σ) = xI for some xI ∈ PIH(σ) ⊆ PI, which means that x∗

Kx ≤ βK, σ) = xI ∈ Zn for all vectors of K cuts αT

1 x ≤ β1, . . . , αT

1 x ≤ β1, . . . , αT

1 x ≤ β1, . . . , αT

1 x ≤ β1, . . . , αT

LP(αT

We now count the number of hyperplanes given by Equation 20. For each σ, there are nKn
possible subsets of cut indices and at most (m+2n)n−1 binding face constraints F ⊆ M ∪σ deﬁning
the formula of Corollary C.2. For each subset-face pair, there are n|PI| ≤ nτ n degree-K polynomial
hypersurfaces given by Equation 20. So the total number of such hypersurfaces over all σ is at
most τ 3nn2Kn(m + 2n)n−1τ n. The number of facets deﬁning V is at most K|PI| ≤ Kτ n. Adding
these to the counts obtained in Lemma C.3 yields the ﬁnal tallies in the lemma statement.

At this point, as in the single-cut case, if the bounding aspect of branch-and-cut is suppressed,
our decomposition yields connected components over which the branch-and-cut tree built is invari-
ant. We now prove our main structural theorem for B&C as a function of multiple cutting planes
at the root.

28

Theorem C.5. Given an IP (c, A, b), there is a set of at most O(12nn2nK2n2(m+2n)2n2τ 5n2) poly-
nomial hypersurfaces of degree at most 2K partitioning RK(n+1) into connected components such
that the branch-and-cut tree built after adding the K cuts αT
k x ≤ βk at the root is in-
variant over all (α1, β1, . . . , αK, βK) within a given component. In particular, fc,A,b(α1, β1, . . . , αK, βK)
is invariant over each connected component.

1 x ≤ β1, . . . , αT

Proof. Fix a connected component C in the decomposition induced by the set of hyperplanes,
degree-K hypersurfaces, and degree-2K hypersurfaces established in Lemma C.4. Let

Q1, . . . , Qi1, I1, Qi1+1, . . . , Qi2, I2, Qi2+1, . . .

(21)

denote the nodes of the tree branch-and-cut creates, in order of exploration, under the assumption
that a node is pruned if and only if either the LP at that node is infeasible or the LP optimal solution
is integral (so the “bounding” of branch-and-bound is suppressed). Here, a node is identiﬁed by
the list σ of branching constraints added to the input IP. Nodes labeled by Q are either infeasible
or have fractional LP optimal solutions. Nodes labeled by I have integral LP optimal solutions and
are candidates for the incumbent integral solution at the point they are encountered. (The nodes
are functions of α1, β1, . . . , αK, βK, as are the indices i1, i2, . . ..) By Lemma C.4, this ordered list
of nodes is invariant for all (α1, β1, . . . , αK, βk) ∈ C.

Now, given an node index (cid:96), let I((cid:96)) denote the incumbent node with the highest objective value
encountered up until the (cid:96)th node searched by B&C, and let z(I((cid:96))) denote its objective value. For
each node Q(cid:96), let σ(cid:96) denote the branching constraints added to arrive at node Q(cid:96). The hyperplane

z∗
LP

(cid:0)αT

1 x ≤ β1, . . . , αT

Kx ≤ βK, σ(cid:96)

(cid:1) = z(I((cid:96)))

(22)

1 x ≤ β1, . . . , αT

(which is a hyperplane due to Corollary C.2) partitions C into two subregions. In one subregion,
LP(αT
z∗
k x ≤ βk, σ(cid:96)) ≤ z(I((cid:96))), that is, the objective value of the LP optimal solution
is no greater than the objective value of the current incumbent integer solution, and so the subtree
rooted at Q(cid:96) is pruned. In the other subregion, z∗
k x ≤ βk, σ(cid:96)) > z(I((cid:96))), and
Q(cid:96) is branched on further. Therefore, within each connected component of C induced by all hyper-
planes given by Equation 22 for all (cid:96), the set of node within the list (21) that are pruned is invariant.
Combined with the surfaces established in Lemma C.4, these hyperplanes partition RK(n+1) into
connected components such that as (α1, β1 . . . , αK, βK) varies within a given component, the tree
built by branch-and-cut is invariant.

1 x ≤ β1, . . . , αT

LP(αT

Finally, we count the total number of surfaces inducing this partition. Unlike the counting stages
of the previous lemmas, we will ﬁrst have to count the number of connected components induced by
the surfaces established in Lemma C.4. This is because the ordered list of nodes explored by branch-
and-cut (21) can be diﬀerent across each component, and the hyperplanes given by Equation 22
depend on this list. From Lemma C.4 we have 6n2Kn(m + 2n)2nτ 4n polynomial hypersurfaces of
degree ≤ 2K. The set of all (α1, β1, . . . αK, βk) ∈ RK(n+1) such that (α1, β1, . . . , αK, βK) lies on
the boundary of any of these surfaces is precisely the zero set of the product of all polynomials
deﬁning these surfaces. Denote this product polynomial by p. The degree of the product polynomial
is the sum of the degrees of 6n2Kn(m + 2n)2nτ 4n polynomials of degree ≤ 2K, which is at most
2K · 6Kn2Kn(m + 2n)2nτ 4n = 12n2Kn+2(m + 2n)2nτ 4n. By Warren’s theorem, the number of
connected components of Rn+1 \ {(α, β) : p(α, β) = 0} is O((12n2Kn+2(m + 2n)2nτ 4n)n−1), and
by the Milnor-Thom theorem, the number of connected components of {(α, β) : p(α, β) = 0} is
O((12n2Kn+2(m + 2n)2nτ 4n)n−1) as well. So, the number of connected components induced by
the surfaces in Lemma C.4 is O(12nn2nK2n2(m + 2n)2n2τ 4n2). For every connected component C
LP(αT x ≤ β, σ(cid:96)) is already determined due to Corollary C.2,
in Lemma C.4, the closed form of z∗

29

and so the number of hyperplanes given by Equation 22 is at most the number of possible σ ⊆ BC,
which is at most τ 3n. So across all connected components C, the total number of hyperplanes
given by Equation 22 is O(12nn2nK2n2(m + 2n)2n2τ 5n2). Finally, adding this to the surface-counts
established in Lemma C.4 yields the theorem statement.

D Omitted results from Section 5

Proof of Theorem 5.1. For a set X , X <N denotes the set of ﬁnite sequences of elements from
X . There is a bijection between the set of IPs (c, A, b) ∈ I := Rn × Zm×n × Zm and R, so
IPs can be uniquely represented as real numbers (and vice versa). Now, consider the set of all
ﬁnite sequences of pairs of IPs and ±1 labels of the form ((c1, A1, b1), ε1), . . . , ((cN , AN , bN ), εN ),
ε1, . . . , εN ∈ {−1, 1}, that is, the set (I × {−1, 1})<N. There is a bijection between this set and
(R×{−1, 1})<N, and in turn there is a bijection between (R×{−1, 1})<N and R. Hence, there exists
a bijection between U and (I × {−1, 1})<N. Fix such a bijection ϕ : U → (I × {−1, 1})<N, and let
ϕ−1 : (I × {−1, 1})<N → U denote the inverse of ϕ, which is well deﬁned and also a bijection.

Let n be odd. For c ∈ R, let IPc ∈ I denote the IP

maximize
subject to 2x1 + · · · + 2xn = n

c

x ∈ {0, 1}n.

(23)

Since n is odd, IPc is infeasible, independent of c. Jeroslow [21] showed that without the use of
cutting planes or heuristics, branch-and-bound builds a tree of size 2(n−1)/2 before determining
infeasibility and terminating. The objective c is irrelevant, but is important in generating distinct
IPs with this property. Consider the cut x1 + · · · + xn ≤ (cid:98)n/2(cid:99), which is a valid cut for IPc (this is in
fact a Chv´atal-Gomory cut [8]). In particular, since n is odd, x1 + · · · + xn ≤ (cid:98)n/2(cid:99) =⇒ x1 + · · · +
xn ≤ (n − 1)/2 < n/2, so the equality constraint of IPc is violated by this cut. Thus, the feasible
region of the LP relaxation after adding this cut is empty, and branch-and-bound will terminate
immediately at the root (building a tree of size 1). Denote this cut by (α(−1), β(−1)) = (1, (cid:98)n/2(cid:99)).
On the other hand, let (α(1), β(1)) = (0, 0) be the trivial cut 0 ≤ 0. Adding this cut to the
IP constraints does not change the feasible region, so branch-and-bound will build a tree of size
2(n−1)/2.

We now deﬁne αc,A,b and βc,A,b. Let

(αc,A,b(u), βc,A,b(u)) =






(α(1), β(1))
(α(−1), β(−1))
(0, 0)

if ((c, A, b), 1) ∈ ϕ(u) and ((c, A, b), −1) /∈ ϕ(u)
if ((c, A, b), −1) ∈ ϕ(u) and ((c, A, b), 1) /∈ ϕ(u)
otherwise

.

The choice to use (0, 0) in the case that either ((c, A, b), ε) /∈ ϕ(u) for each ε ∈ {−1, 1}, or
((c, A, b), −1) ∈ ϕ(u) and ((c, A, b), 1) ∈ ϕ(u) is arbitrary and unimportant. Now, for any in-
teger N > 0, constructing a set of N IPs and N thresholds that is shattered is almost imme-
diate. Let c1, . . . , cN ∈ R be distinct reals, and let 1 < r1, . . . , rN < 2(n−1)/2. Then, the set
{(IPc1, r1), . . . , (IPcN , rN )} can be shattered. Indeed, given a sign pattern (ε1, . . . , εN ) ∈ {−1, 1}N ,
let

Then, if εi = 1, (αIPci
If εi = −1, (αIPci

(u), βIPci

(u), βIPci

u = ϕ−1 ((IPc1, ε1), . . . , (IPcN , εN )) .
(u)) = (α(1), β(1)), so gu(IPci) = 2(n−1)/2 and sign(gu(IPci)−ri) = 1.
(u)) = (α(−1), β(−1)), so gu(IPci) = 1 and sign(gu(IPci) − ri) = −1. So

30

for any N there is a set of IPs and thresholds that can be shattered, which yields the theorem
statement.

Proof of Lemma 5.2. We have fi = uT ai − (cid:98)uT ai(cid:99), f0 = uT b − (cid:98)uT b(cid:99), and since u ∈ [−U, U ]m,
(cid:98)uT ai(cid:99) ∈ [−U (cid:107)ai(cid:107)1 , U (cid:107)ai(cid:107)1] and (cid:98)uT b(cid:99) ∈ [−U (cid:107)b(cid:107)1 , U (cid:107)b(cid:107)1]. Now, for all i, ki ∈ [−U (cid:107)ai(cid:107)1 , U (cid:107)ai(cid:107)1]∩
Z and k0 ∈ [−U (cid:107)b(cid:107)1 , U (cid:107)b(cid:107)1] ∩ Z, put down the hyperplanes deﬁning the two halfspaces

(cid:4)uT ai

(cid:5) = ki ⇐⇒ ki ≤ uT ai < ki + 1

and the hyperplanes deﬁning the two halfspaces

(cid:4)uT b(cid:5) = k0 ⇐⇒ k0 ≤ uT b < k0 + 1.

In addition, consider the hyperplane

uT ai − ki = uT b − k0

(24)

(25)

(26)

for each i. Within any connected component of Rm determined by these hyperplanes, (cid:98)uT ai(cid:99) and
(cid:98)uT b(cid:99) are constant. Furthermore, 1[fi ≤ f0] is invariant within each connected component, since
if (cid:98)uT ai(cid:99) = ki and (cid:98)uT b(cid:99) = k0, fi ≤ f0 ⇐⇒ uT ai − ki ≤ uT b − k0, which is the hyperplane
given by Equation 26. The total number of hyperplanes of type 24 is O(nU (cid:107)A(cid:107)1), the total
number of hyperplanes of type 25 is O(U (cid:107)b(cid:107)1), and the total number of hyperplanes of type 26 is
nU 2 (cid:107)A(cid:107)1 (cid:107)b(cid:107)1. Summing yields the lemma statement.
Proof of Lemma 5.4. Let C ⊆ Rn+1 be a connected component in the partition established in
Theorem 4.5, so C can be written as the intersection of at most 14n(m + 2n)3n2τ 5n2 polynomial
constraints of degree at most 5. Let D ⊆ [−U, U ]m be a connected component in the partition
established in Lemma 5.2. By Lemma 5.3, there are at most 14n(m + 2n)3n2τ 5n2 polynomials of
degree at most 10 partitioning D into connected components such that within each component,
1[(α(u), β(u)) ∈ C] is invariant. If we consider the overlay of these polynomial surfaces over all
components C, we will get a partition of [−U, U ]m such that for every C, 1[(α(u), β(u)) ∈ C] is
invariant over each connected component of [−U, U ]m. Once we have this we are done, since all u
in the same connected component of [−U, U ]m will be sent to the same connected component of
Rn+1 by (α(u), β(u)), and thus by Theorem 4.5 the behavior of branch-and-cut will be invariant.
We now tally up the total number of surfaces. The number of connected components C was given
by Warren’s theorem and the Milnor-Thom theorem to be O(14n(n+1)(m + 2n)3n2(n+1)τ 5n2(n+1)),
so the total number of degree-10 hypersurfaces is 14n(m + 2n)3n2τ 5n2 times this quantity, which
yields the lemma statement.

D.1 Multiple GMI cuts at the root

In this section we extend our results to allow for multiple GMI cuts at the root of the B&C tree.
These cuts can be added simultaneously, sequentially, or in rounds. If GMI cuts u1, u2 are added
simultaneously, both of them have the same dimension and are deﬁned in the usual way. If GMI
cuts u1, u2 are added sequentially, u2 has one more entry than u1. This is because when cuts are
added sequentially, the LP relaxation is re-solved after the addition of the ﬁrst cut, and the second
cut has a multiplier for all original constraints as well as for the ﬁrst cut (this ensures that the
second cut can be chosen in a more informed manner). If K cuts are made at the root, they can be
added in sequential rounds of simultaneous cuts. In the following discussion, we focus on the case
where all K cuts are added sequentially—the other cases can be viewed as instantiations of this.
We refer the reader to the discussion in Balcan et al. [8] for more details.

31

To prove an analogous result for multiple GMI cuts (in sequence, that is, each successive GMI
cut has one more parameter than the previous), we combine the reasoning used in the single-GMI-
cut case with some technical observations in Balcan et al. [8].

Lemma D.1. Consider the family of K sequential GMI cuts parameterized by u1 ∈ [−U, U ]m, u2 ∈
[−U, U ]m+1, . . . , uK ∈ [−U, U ]m+K−1. For any IP (c, A, b), there are at most

O (cid:0)nK(1 + U )2K (cid:107)A(cid:107)1 (cid:107)b(cid:107)1

(cid:1)

degree-K polynomial hypersurfaces and

2O(n2)KO(n3)(m + 2n)O(n3)τ O(n3)

degree-4K2 polynomial hypersurfaces partitioning [−U, U ]m × · · · × [−U, U ]m+K−1 connected com-
ponents such that the B&C tree built after sequentially adding the GMI cuts deﬁned by u1, . . . , uK
is invariant over all (u1, . . . , uK) within a single component.

Proof. We start with the setup used by Balcan et al. [8] to prove similar results for sequential
Chv´atal-Gomory cuts. Let a1, . . . , an ∈ Rm be the columns of A. We deﬁne the following aug-
i ∈ Rm+K−1 for each i ∈ [n], and the augmented constraint vectors
mented columns (cid:101)a1
(cid:101)b1 ∈ Rm, . . . , (cid:101)bK ∈ Rm+K−1 via the following recurrences:

i ∈ Rm, . . . , (cid:101)aK

(cid:101)a1
i = ai
(cid:20)
(cid:101)ak−1
(cid:101)ak
i =
k−1(cid:101)ak−1
uT

i

i

(cid:21)

and

(cid:101)b1 = b
(cid:34)

(cid:101)bk =

(cid:101)bk−1
k−1(cid:101)bk−1
uT

(cid:35)

for k = 2, . . . , K. In other words, (cid:101)ak
is the ith column of the constraint matrix of the IP and (cid:101)bk
is the constraint vector after applying cuts u1, . . . , uk−1. An identical induction argument to that
of Balcan et al. [8] shows that for each k ∈ [K],

i

(cid:4)uT

k (cid:101)ak

i

(cid:104)

(cid:5) ∈

− (1 + U )k (cid:107)ai(cid:107)1 , (1 + U )k (cid:107)ai(cid:107)1

(cid:105)

and

(cid:4)uT

k (cid:101)bk(cid:5) ∈

(cid:104)
− (1 + U )k (cid:107)b(cid:107)1 , (1 + U )k (cid:107)b(cid:107)1

(cid:105)

.

Now, as in the single-GMI-cut setting, consider the surfaces

(cid:4)uT

k (cid:101)ak

i

(cid:5) = (cid:96)i ⇐⇒ (cid:96)i ≤ uT

k (cid:101)ak

i < (cid:96)i + 1

(27)

and

(28)
for every i, k, and every integer (cid:96)i ∈ [−(1 + U )k (cid:107)ai(cid:107)1 , (1 + U )k (cid:107)ai(cid:107)1] ∩ Z and every integer
(cid:96)0 ∈ [−(1 + U )k (cid:107)b(cid:107)1 , (1 + U )k (cid:107)b(cid:107)1] ∩ Z. In addition, consider the surfaces

(cid:4)uT

k (cid:101)bk(cid:5) = (cid:96)0 ⇐⇒ (cid:96)i ≤ uT

k (cid:101)bk < (cid:96)0 + 1

k (cid:101)ak
uT

i − (cid:96)i = uT

k (cid:101)bk − (cid:96)0

32

(29)

i is a polynomial in u1[1], . . . , u1[m], u2[1], . . . , u2[m+

k (cid:101)ak

for each i, k, (cid:96)i, (cid:96)0. As observed by Balcan et al. [8], uT
1], . . . , uk[1], . . . , uk[m + k − 1] of degree at most k (as is uT
k (cid:101)bk), so surfaces 27, 28, and 29 are all
degree-K polynomial hypersurfaces for all i, k. Within any connected component of [−U, U ]m ×
k (cid:101)bk(cid:99) are constant. Furthermore
· · ·×[−U, U ]m+K−1 induced by these hypersurfaces, (cid:98)uT
1[f k
0 ] is invariant for every i, k, where f k
k (cid:101)ak
k (cid:101)bk − (cid:98)uT

i ≤ f k
Now, ﬁx a connected component D ⊆ [−U, U ]m × · · · × [−U, U ]m+K−1 induced by the above
hypersurfaces, and let C ⊆ RK(n+1) be the intersection of q polynomial inequalities of degree at
most d. Consider a single degree-d polynomial inequality in K(n + 1) variables y1, . . . , yK(n+1),
which can be written as

i (cid:99) and (cid:98)uT
k (cid:101)ak

k (cid:101)ak
i − (cid:98)uT

i (cid:99) and f k

i = uT

0 = uT

k (cid:101)bk(cid:99).

(cid:88)

T (cid:118)[K(n+1)]
|T |≤d

λT

(cid:89)

j∈T

yj =

(cid:88)

λT1,...,TK

(cid:89)

yj1 · · ·

(cid:89)

yjK ≤ γ.

T1,...,TK (cid:118)[n+1]
|T1|+···+|TK |≤d

j1∈T1

jK ∈TK

Now, the sets S1, . . . , SK deﬁned by Sk = {i : f k

i ≤ f k

0 } are ﬁxed within D, so we can write this as

(cid:88)

λT1,...,TK

T1,...,TK (cid:118)[n+1]
|T1|+···+|TK |≤d

(cid:34)

K
(cid:89)

(cid:89)

k=1

j∈Tk∩Sk
j(cid:54)=n+1

j (1 − f k
f k
0 )

(cid:89)

0 (1 − f k
f k
j )

j∈Tk\Sk
j(cid:54)=n+1

(cid:35)
0 (1 − f k
f k
0 )

≤ γ.

(cid:89)

j∈Tk
j=n+1

j and f k

We have that f k
0 are degree-k polynomials in u1, . . . , uk. Since the sum is over all multisets
T1, . . . , TK such that |T1| + · · · + |TK| ≤ d, there are at most d terms across the products, each of
0 (1 − f0)k. Therefore, the left-hand-side is a polynomial of
the form f k
degree at most 2dK, and if C ⊆ RK(n+1) is the intersection of q polynomial inequalities each of
degree at most d, the set

j (1 − f0)k, f k

j ), or f k

0 (1 − f k

{(u1, . . . , uK) ∈ D : (α (u1, . . . , uK) , β (u1, . . . , uK)) ∈ C} ⊆ [−U, U ]m × · · · × [−U, U ]m+K−1

can be expressed as the intersection of q degree-2dK polynomial inequalities.

(cid:16)

To ﬁnish, we run this process for every connected component C ⊆ RK(n+1) in the partition estab-
lished by Theorem C.5. This partition consists of O(12nn2nK2n2(m + 2n)2n2τ 5n2) degree-2K poly-
nomials over RK(n+1). By Warren’s theorem and the Milnor-Thom theorem, these polynomials par-
tition RK(n+1) into O(12n(n+1)n2n(n+1)K2n2(n+1)(m + 2n)2n2(n+1)τ 5n2(n+1)) connected components.
Running the above argument for each of these connected components of RK(n+1) yields a total
of O
=
2O(n2)KO(n3)(m + 2n)O(n3)τ O(n3) polynomials of degree 4K2. Finally, we count the surfaces of
the form (27),
(28), and (29). The total number of degree-K polynomials of type 27 is at most
O(nK(1 + U )K (cid:107)A(cid:107)1), the total number of degree-k polynomials of type 28 is O(K(1 + U )K (cid:107)b(cid:107)1),
and the total number of degree-K polynomials of type 29 is O(nK(1 + U )2K (cid:107)A(cid:107)1 (cid:107)b(cid:107)). Summing
these counts yields the desired number of surfaces in the lemma statement.

12n(n+1)n2n(n+1)K2n2(n+1)(m + 2n)2n2(n+1)τ 5n2(n+1)(cid:17)

12nn2nK2n2(m + 2n)2n2τ 5n2(cid:17)

· O

(cid:16)

In any connected component of [−U, U ]m determined by these surfaces, 1[(α(u), β(u)) ∈ C] is
invariant for every connected component C ⊆ RK(n+1) in the partition of RK(n+1) established in
Theorem C.5. This means that the tree built by branch-and-cut is invariant, which concludes the
proof.

Finally, applying the main result of Balcan et al. [6] to Lemma D.1, we get the following

pseudo-dimension bound for the class of K sequential GMI cuts at the root of the B&C tree.

33

Theorem D.2. For u1 ∈ [−U, U ]m, u2 ∈ [−U, U ]m+1, . . . , uK ∈ [−U, U ]m+K−1, let gu1,...,uK (c, A, b)
denote the number of nodes in the tree B&C builds given the input (c, A, b) after sequentially apply-
ing the GMI cuts deﬁned by u1, . . . , uK at the root. The pseudo-dimension of the set of functions
{gu1,...,uK : (u1, . . . , uK) ∈ [−U, U ]m × · · · × [−U, U ]m+K−1} on the domain of IPs with (cid:107)A(cid:107)1 ≤ a
and (cid:107)b(cid:107)1 ≤ b is

O (cid:0)mK3 log U + mn3K2 log(mnKτ ) + mK2 log(ab)(cid:1) .

34

