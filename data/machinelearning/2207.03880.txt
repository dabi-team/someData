Constrained Training of Neural Networks via
Theorem Proving

Mark Chevallier ! (cid:18)
School of Informatics, University of Edinburgh, UK

Matthew Whyte !
School of Informatics, University of Edinburgh, UK

Jacques D. Fleuriot ! (cid:18)
School of Informatics, University of Edinburgh, UK

Abstract

We introduce a theorem proving approach to the speciﬁcation and generation of temporal logical
constraints for training neural networks. We formalise a deep embedding of linear temporal logic over
ﬁnite traces (LTLf ) and an associated evaluation function eval characterising its semantics within
the higher-order logic of the Isabelle theorem prover. We then proceed to formalise a loss function
L that we formally prove to be sound, and diﬀerentiable to a function dL. We subsequently use
Isabelle’s automatic code generation mechanism to produce OCaml versions of LTLf , L and dL that
we integrate with PyTorch via OCaml bindings for Python. We show that, when used for training
in an existing deep learning framework for dynamic movement, our approach produces expected
results for common movement speciﬁcation patterns such as obstacle avoidance and patrolling. The
distinctive beneﬁt of our approach is the fully rigorous method for constrained training, eliminating
many of the risks inherent to ad-hoc implementations of logical aspects directly in an “unsafe"
programming language such as Python.

2012 ACM Subject Classiﬁcation Theory of computation → Modal and temporal logics; Computing
methodologies → Neural networks

Keywords and phrases Linear temporal logic, neural networks, theorem proving, Isabelle/HOL

1

Introduction

The usefulness and general applicability of neural networks is well understood, but there are
several ways in which their behaviour could be improved. In particular, it can be hard to
understand the behaviour of a neural network, as they take a black box approach in which it
is usually diﬃcult to establish the reason why a given result was achieved.

Additionally, a lot of training is often needed before a neural network behaves as desired,
and uncertainty in the behaviour of a neural network can make it entirely unsuited to certain
safety-critical tasks such as robot movement.

If we can use some form of logical speciﬁcations or constraints as part of the training of a

neural network then this can have beneﬁts such as:

The network’s output can be interpreted in light of the speciﬁcations passed to it. A
logical speciﬁcation is clear and well-deﬁned, and if it is violated by the output, this can
provide a way of assessing the network.
The volume of data required to train the network to ensure that it does not breach the
constraints could be reduced e.g. where a speciﬁcation may be simple to express but need
many sets of training data for a neural network to learn through imitation [23].

Of particular relevance to the current work is the approach by Innes and Ramamoorthy [13],
which builds on work by Fischer [7] and aims to improve the training speed of learning robotic
movement by mimicking a demonstrator. The suggestion is that given some logical criterion
speciﬁed in linear temporal logic (e.g. “don’t tip the cup until you are above the bowl”),
the network learning the movements will need less demonstrator data to have conﬁdence

2
2
0
2

l
u
J

8

]
I

A
.
s
c
[

1
v
0
8
8
3
0
.
7
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Constrained Training of Neural Networks via Theorem Proving

that it will not breach the rule, even on unseen inputs. We extend this work by taking
a fully-rigorous, theorem-proving based approach to the logical underpinnings of the loss
function and its derivative.

In particular, by formalising a deep embedding of linear temporal logic over ﬁnite traces
(LTLf ) and its semantics in the higher-order logic (HOL) of the proof-assistant Isabelle, we
formulate a loss function L that measures whether a statement is satisﬁed. Moreover, we
formally prove that L is diﬀerentiable, with an explicit derivative dL that can be used as
part of the gradient-descent minimisation of the loss.

We use Isabelle to automatically generate OCaml code from our provably correct formal
speciﬁcations, and integrate it into a PyTorch neural network via a library that provides
OCaml bindings for Python. We use both the loss function and its derivative in combination
with the usual training process so that the neural network can learn from both training data
and our speciﬁed constraints. The loss function we generate could in general be applied
to any neural network with a notion of time-sequences – for the purposes of our paper, we
demonstrate this using a speciﬁc network that predicts paths of motion.

More speciﬁcally, we demonstrate experimentally that the neural network learns con-
strained behaviour when given a wide variety of logical constraints in LTLf . Because our
code for the logical implementations of the speciﬁcations, the loss function and its derivative
was generated automatically by Isabelle, we argue that our approach provides enhanced
guarantees about the correctness of the whole pipeline.

The paper is organised as follows: we brieﬂy review some related work next and provide
some background to our work. Then in Section 2, we discuss LTLf and its formalisation. In
Section 3 we build our loss function and its derivative, showing both are correct. Section 4
discusses how we generated and implemented the OCaml code in PyTorch. In Section 5 we
demonstrate that our code works as expected. We conclude with some thoughts about the
work’s potential impact and future direction in Section 6.

1.1 Related work

There has been some work aimed at unifying propositional logical constraints with neural
networks that have probabilistic outputs [29]. Hu et al. trained a network to follow a
rules-based “teacher” [11] while work by Li et al. incorporates ﬁrst-order logic rules directly
in the network design, with the aim of guiding training and prediction [17]. Other authors
modify the loss function based on the satisfaction of logical constraints to train a neural
network [1, 7, 18]. On the reinforcement learning front, there has been recent work, e.g.
on specifying policy learning via LTL instructions [16, 27]. None of the above approaches
involves any theorem proving like the current work.

A distinct but related strand of work centres around the formal veriﬁcation of neural
networks. This involves using SMT solvers to formally verify properties of neural networks
[6, 12, 14, 26]. These SMT solvers typically verify simple propositional logic constraints over
boolean variables, and in any case are not used to train the network but to check it. This is
distinct from our objective, where we are formally verifying the logical system used to train
a neural network and automatically generating code for the actual training.

1.1.1 Comparison with Innes and Ramamoorthy’s work

As mentioned already, our work is motivated by that of Innes and Ramamoorthy [13]. However,
our examination of their approach, and in particular, the ﬁdelity of the code that implements

M. Chevallier, M. Whyte and J. D. Fleuriot

3

their experiments with the paper’s descriptions, uncovered several weaknesses that, we believe,
support the need for a more formal neurosymbolic pipeline.

The Python code for their logical formulation does not always match that given in their
paper: the “6=” comparison, for example, is given in the paper as an indicator function,
but in the Python script, it is deﬁned as a 6= b ⇐⇒ (a < b) ∨ (b < a). Moreover, although
presented in their paper, they do not encode a component of the loss function for the LTL
Until operator, meaning they were not able to cover any tests involving it – something
which we can do in our work (see Section 2.2).

There are also some aspects of LTL in Innes and Ramamoorthy’s paper that remain
implicit and are not discussed – notably its use of LTL is over ﬁnite traces, which has a
distinct semantics from the more usual formulation of LTL over inﬁnite traces.

The primary advantage of our work in comparison to theirs resides in our fully rigorous
speciﬁcations and proofs, with the training constraints guaranteed via systematic code
generation from our speciﬁcations. This guarantee means we know that the code generated
will have the properties that were established for it during the theorem proving stage of our
process.

1.2 Isabelle

We brieﬂy review a few aspects of Isabelle [21] that are relevant to this work. Mathematical
theories written in Isabelle are a collection of formal deﬁnitions of various kinds (algebraic
objects, types, functions, etc.), and associated theorems about their properties. Proofs can
be written in a pen-and-paper like, structured proof language [28].

When deﬁning a constant or function, it must be assigned a type (“T::τ ” states that
T is of type τ ). Function types are stated using “⇒”. We provide some quick illustrative
examples:

1. state :: "int ⇒ real" tells us that a state is a function from the int type, the

integers, to the real numbers. Note also that functions are usually curried.

2. path ::

"state list" tells us that a path is a ﬁnite list of such states i.e. a list of

functions.

Note that in Isabelle, a list can be written as s#ss denoting the element s being consed onto
the (possibly empty) list ss and that the lambda abstraction, or anonymous function, is
denoted by λx. M so e.g. λx. x2 denotes the function that squares its argument x.

It is possible to generate computable code from the formal speciﬁcation of functions in
Isabelle into Standard ML (PolyML), OCaml, Haskell and Scala [9]. This mechanism provides
a rigorous link between Isabelle concepts, e.g. our L and dL functions, and their automatically-
generated counterparts, whose computational behaviour (modulo implementational details
such as translating reals to ﬂoats) can then be expected to respect the properties that were
formally proven as theorems in Isabelle. This is a vital component of our pipeline and will
be discussed further in Section 4.

2

Formalising linear temporal logic

As already mentioned, We mechanise linear temporal logic over ﬁnite paths in Isabelle and
use it to formulate a loss function for training a neural network under rigorously speciﬁed
logical constraints. We review some of the salient aspects of our formalisation next.

4

Constrained Training of Neural Networks via Theorem Proving

Figure 1 A path of states with values: time proceeds from left to right and j indexes the values

each state can measure

2.1 States and paths

Recall the deﬁnition of state from the previous section. In the context of our work, the state
function at a particular time-step i tracks several values in a learning problem that might be
compared: a constant, or some measurement that a neural network uses in its training e.g.
how far a robotic hand is from a barrier at a given moment in time. The integers passed to
a state function are simply the indices to those values of interest. In almost all cases, a state
function only needs to represent some ﬁnite number K of values. By only considering indices
j over this range, the state function for a given time-step can be considered as a vector, as
can be seen in Fig. 1.1

With each state function encoding information about a system at a speciﬁc time-step, a
path of length N encodes the evolution of a system over N time-steps. In the case of Fig.
1, increasing i represents the forward temporal progression of the system. It should also
be clear from this ﬁgure that the values encoded by a path of states can equivalently be
represented in matrix form, which will be of vital importance to our PyTorch integration
(see Section 4.2).

2.2 LTL and ﬁnite traces of states

Our formalisation uses a variant of LTL [24] known as LTLf [5], which is interpreted over
ﬁnite traces (of states) and is often viewed as a more natural choice for applications in AI,
e.g. planning [2], where processes are usually of ﬁnite duration. Note that in what follows,
any reference to LTL means LTLf unless otherwise stated.

As we are using LTL to evaluate conditions during the training of a neural network, we
take comparisons of real-valued terms (each term corresponding to measurements in the
environment or constants), namely t < t0, t ≤ t0, t = t0, t 6= t0 as our atomic propositions ρ [7].
Thus we divide constraints into these comparisons (comp) and those constraints (constraint)
arising from LTL’s operators (see further down for their Isabelle implementations).

The LTLf formulae are thus: ρ, ρ1 ∧ρ2, ρ1 ∨ρ2, N ρ (Next), (cid:3)ρ (Always), ♦ρ (Eventually),
ρ1Uρ2 (Weak Until), ρ1Rρ2 (Strong Release). In LTLf these have the usual semantics of
LTL, except at the end of a path. In particular, for a sequence of length i, ¬(N ρ) holds at
the ﬁnal step i [5]. As a consequence, when dealing with a ﬁnite time-sequence of length i, it
is important to recognise that ¬(N ρ) ⇐⇒ N (¬ρ) is not true. ¬(N ρ) is true at the last step

1 Strictly speaking, we could have used natural numbers as our indices. However, we chose integers

because OCaml, the target language for our speciﬁcations, has no native natural number type.

M. Chevallier, M. Whyte and J. D. Fleuriot

5

in a time-sequence and N (¬ρ) is false at that step. We refer the reader to Appendix B for
some more details about LTLf .

We formalise negation via a Not function which, given any constraint, returns a constraint
that is provably logically equivalent to its negation. This reduces the number of primitive
operators that one needs to specify for LTLf , thereby simplifying reasoning about its
properties.

We proceed by deﬁning Isabelle datatypes comp and constraint for the comparison and
constraints respectively. This approach to specifying the language in (higher-order) logic is
known as a deep embedding [3]. Doing so will enable us to prove that the loss function is
sound and, importantly, generate fully self-contained code for the speciﬁcation language that
will be used as part of the training of our neural net. Note also our formulation of state
and path which, as per the previous section, creates a ﬁnite time-sequence over which LTLf
can be evaluated.

type_synonym state = " int ⇒ real "
type_synonym path = " state list "

datatype comp = Less int int | Lequal int int | Equal int int

| Nequal int int

datatype constraint = Comp comp | And constraint constraint

| Or constraint constraint | Next constraint | Always constraint
| Eventually constraint | Until constraint constraint
| Release constraint constraint

Listing 1 Datatypes for our LTLf implementation

Next, we formalise our eval function, which characterises the semantics of LTLf by
recursively evaluating the truth-value of a constraint over a path. We give an extract of the
Isabelle deﬁnition (see Appendix A for the full deﬁnition):

function eval :: " constraint ⇒ path ⇒ bool " where

" eval c [] = False "

| " eval ( Comp ( Lequal v1 v2 )) ( s # ss ) = (( s v1 ) ≤ ( s v2 ))"

...

| " eval ( Always c ) ( s # ss ) = (( eval c ( s # ss ))

∧ ( if ss = [] then True else ( eval ( Always c ) ss )))"

...

Listing 2 An extract of the LTLf evaluation function

Given the complexity of LTLf compared to propositional logic, we also prove a number
of LTLf equivalences, which conﬁrms that our eval behaves as expected. For example, we
show (amongst other examples) that, (cid:3)((cid:3)ρ) = (cid:3)ρ and that N (ρ1 ∨ ρ2) = (N ρ1) ∨ (N ρ2).

3

A LTL-based loss function and its derivative

The loss function L – which takes a constraint ρ, a path P and a relaxation factor γ, and
returns a real value – needs to satisfy several important properties:
1. L(ρ, P, γ) ≥ 0;
2. L is diﬀerentiable with respect to any of the terms that the constraint compares;
3. (Soundness) limγ→0 L(ρ, P, γ) = 0 ⇐⇒ ρ(P ), where ρ(P ) is the truth value of ρ on P .

3.1 Soft functions and their derivatives

When formalising L, in order for it to be diﬀerentiable, it is necessary to use soft versions
of various functions. Thus, based on the work by Cuturi and Blondel [4], we deﬁne binary

6

Constrained Training of Neural Networks via Theorem Proving

softmax and softmin functions, maxγ and minγ, respectively, as:

max
γ

a1 a2 =

min
γ

a1 a2 =

(

max a1 a2
γ log(ea1/γ + ea2/γ) otherwise

γ ≤ 0

(

min a1 a2
−γ log(e−a1/γ + e−a2/γ) otherwise

γ ≤ 0

Each of these soft functions takes an additional parameter γ. The intention behind these
is that as γ → 0, maxγ → max, minγ → min, and that they are diﬀerentiable for γ > 0. In
Isabelle, these are easily formalised and proven as correct – we give Max_gamma as an example
(see Appendix A for Min_gamma):

fun Max_gamma :: " real ⇒ real ⇒ real ⇒ real " where

" Max_gamma γ a b = ( if γ ≤ 0 then max a b

else γ * ln ( exp ( a /γ ) + exp ( b /γ )))"

lemma Max_gamma_lim : "(λγ . Max_gamma γ a b ) −0→ max a b "

Listing 3 The soft gamma Max function and its correctness theorem

where (λx.f x) −0→ L denotes limx→0 f (x) = L in Isabelle [8].

For Max_gamma, the derivatives with respect to a and b are built separately before being
combined to give the dMax_gamma_ds function (as deﬁned in Appendix C). We show that
this is the expected derivative using Isabelle’s has_real_derivative relation by proving
the following theorem (for γ > 0):

theorem dMax_gamma_chain :

assumes "γ > 0" and "( f has _real_de riva t ive Df ) ( at x )"

and "( g has_real_derivativ e Dg ) ( at x )"

shows "((λy . Max_gamma γ ( f y ) ( g y )) has_real_deri vati v e
( dMax_gamma_ds γ ( f x ) Df ( g x ) Dg )) ( at x )"

Listing 4 The derivative’s correctness stated in Isabelle

Likewise, we use other soft functions to capture losses from the Nequal comparison, again
using γ as a parameter. We note here an important distinction between our approach and
that of Fischer [7] and Innes and Ramamoorthy [13]: in their respective work, the Nequal
and Lequal comparisons were not deﬁned using soft-functions and were not diﬀerentiable.
They relied on the implicit auto-diﬀerentiation machinery of PyTorch to handle these for
backpropagation purposes. In our case, though, we provide an explicit derivative of the
loss function to PyTorch thereby giving us guarantees that backpropagation is using the
desired function for gradient descent (see Sections 3.3 and 4.2). We therefore require the
loss functions for all our comparisons to be provably diﬀerentiable. The interested reader is
referred to Appendix C for more detail.

3.2 Formalising the loss function

We proceed to deﬁne L recursively over the constraint, given a path and a relaxation factor
γ. We give an extract of the deﬁnition:

M. Chevallier, M. Whyte and J. D. Fleuriot

7

function L :: " constraint ⇒ path ⇒ real ⇒ real " where

" L c [] γ = 1"

| " L ( Comp ( Lequal v1 v2 )) ( s # ss ) γ = Max_gamma γ ( s v1 - s v2 ) 0"
| " L ( Comp ( Nequal v1 v2 )) ( s # ss ) γ = Nequal_gamma γ ( s v1 ) ( s v2 )"

...

| " L ( Always c ) ( s # ss ) γ = Max_gamma γ ( L c ( s # ss ) γ )

( if ss = [] then 0 else ( L ( Always c ) ss ) γ )"

...

| " L ( Until c1 c2 ) ( s # ss ) γ = Min_gamma γ ( L c2 ( s # ss ) γ )

( Max_gamma γ ( L c1 ( s # ss ) γ )

( if ss = [] then 0 else ( L ( Until c1 c2 ) ss γ )))"

...

Listing 5 An extract of the L function, see Appendix A for the full deﬁnition

In the above formulation, the deﬁning equation for the Lequal shows that if the ﬁrst
state-value is smaller or equal to the ﬁrst, L produces 0, equivalent to logical truth. This
works in a similar way to a soft rectiﬁcation function – its limit as γ tends to zero is identical
and proven in Isabelle. As another remark, our comparison operators for L are deﬁned in
terms of ≤ and 6=, with the other two comparisons, < and = deﬁned using them.

For all the LTL operators, the L function, in common with our eval function, recurses
over the constraint from the outside in, and recurses down the path as required for temporal
operators. Innes and Ramamoorthy do not use a recursive deﬁnition [13], which though
ﬁne for relatively simple LTL operators such as Always, leads to a much more involved
formulation for more complicated ones such as Until.

In particular, our recursive deﬁnition of L against the Until operator is signiﬁcantly
simpler than that given in their paper because in our case Until is logically equivalent to:

ρ1Uρ2(i) = (ρ2(i) ∨ (ρ1(i) ∧ (ρ1Uρ2(i + 1))

where the ρ1Uρ2(i + 1) clause is assumed to be true at the path’s termination.

Note also that when we are evaluating over an empty path, L takes value 1, which is
equivalent to any constraint on the empty path evaluating as false. This means that our
Next constraint matches our expectation (as discussed in Section 2.2) at the end of a ﬁnite
trace. However, this understanding of how L treats the empty path means we must specify a
slightly diﬀerent behaviour for how L handles ﬁnite paths for the Always and Weak Until
constraints. As L recurses down the path for these two, if it reaches the end of the path, it
should return a 0 value (equivalent to true) if every state it has recursed through meets the
speciﬁed constraints.

Once we have formulated L via a series of lemmas and an inductive proof on the
constraint datatype, we show that it has the expected property with respect to the eval
function. The proof is not straightforward and depends on being able to show continuity of
our various soft gamma functions when γ = 0.

theorem L_sound : "((λγ . L c ss γ ) −0→ 0) ←→ ( eval c ss )"

Listing 6 Soundness of L stated in Isabelle

We have now formally deﬁned an LTL-based, soft loss function L that, for any constraint
and ﬁnite trace, tends to 0 as its gamma parameter tends to 0, if and only if the constraint
is satisﬁed over that trace.

3.3 Derivative of the loss function

We next construct a derivative dL for the L function, to be used for gradient-based methods
in PyTorch (see Section 4.2). The derivative must be deﬁned with respect to each time-step

8

Constrained Training of Neural Networks via Theorem Proving

i and state-value index j at that time-step along the ﬁnite trace. The full formulation is
extensive, so for brevity, we only present an extract here:

function dL :: " constraint ⇒ path ⇒ real ⇒ int ⇒ int ⇒ real " where

" dL c [] γ i j = 0"

...

| " dL ( Comp ( Nequal v1 v2 )) ( s # ss ) γ i j = ( if ( i 6= 0) then 0

else ( dNequal_gamma_ds γ ( s v1 ) ( if j = v1 then 1 else 0) ( s v2 )

( if j = v2 then 1 else 0)))"

...

| " dL ( Until c1 c2 ) ( s # ss ) γ i j = dMin_gamma_ds γ ( L c2 ( s # ss ) γ )

( dL c2 ( s # ss ) γ i j ) ( Max_gamma γ ( L c1 ( s # ss ) γ )
( if ( ss = []) then 0 else ( L ( Until c1 c2 ) ss γ )))
( dMax_gamma_ds γ ( L c1 ( s # ss ) γ ) ( dL c1 ( s # ss ) γ i j )
( if ( ss = []) then 0 else ( L ( Until c1 c2 ) ss γ ))

( if ( ss = []) then 0 else ( dL ( Until c1 c2 ) ss γ (i -1) j )))"

...

Listing 7 An extract of the dL function, see Appendix A for the full deﬁnition

The dL function formalisation is structured similarly to our formalisation of the L function,
deﬁned recursively over the components of the LTLf constraint passed to it and essentially
follows from repeated applications of the chain rule. In deﬁning it, we make extensive use of
the derivatives of the soft-functions we described in Section 3.1 (and in Appendix C).

3.3.1 Correctness of dL

While the formulation of dL may look intricate, we formally prove that it is indeed the correct
derivative for our loss function and thus guarantee that when used for backpropagation it
will achieve the desired results.

In order to facilitate this, we must turn a path into a function of a single variable
representing a single state-value at a speciﬁc time-step on the path. We do so by formalising
the state-update function update_state and the recursive function update_path, which
allow us to specify the value at a particular i and j:

fun update_state :: " state ⇒ int ⇒ real ⇒ state " where
" update_state s j x = (λn . if n = j then x else ( s n ))"

primrec update_path :: " path ⇒ int ⇒ int ⇒ real ⇒ path " where

" update_path [] i j = (λx . [])"
| " update_path ( s # ss ) i j = ( if i = 0 then

(λx . ( update_state s j x ) # ss ) else ( if i < 0 then (λx . ( s # ss ))
else (λx . s # ( update_path ss (i -1) j x ))))"

Listing 8 The update functions

Using this mechanism, we prove that dL is indeed the derivative of the L function, with
respect to the value at any i and j. In Isabelle, the theorem formalising this is as follows:

theorem L_has_derivative :

assumes "γ > 0"
shows "((λy . L c ( update_path pth i j y ) γ ) h as_fi eld _derivative

( dL c ( update_path pth i j x ) γ i j )) ( at x )"

Listing 9 Correctness of dL stated in Isabelle

4

A PyTorch-compatible LTL loss function

With L and dL formalised in Isabelle, what remains is to integrate them into the PyTorch
environment. Unfortunately, there does not exist a mechanism for generating Isabelle

M. Chevallier, M. Whyte and J. D. Fleuriot

9

functions as Python code. Instead, we choose to generate intermediate representations of L
and dL in OCaml since our recursive Isabelle functions can be straightforwardly translated
to type-safe OCaml ones and, moreover, there exists a Python library that can be used to
call OCaml functions from within Python code [19].

4.1 OCaml code generation

In order to produce computable code, we need to map the real numbers of Isabelle to ﬂoating
points. As this is an approximation, it naturally has some scope for machine arithmetic errors,
although the code generated for the various functions is fully faithful to their deﬁnitions in
Isabelle. So, assuming that the ﬂoating point computations are well-behaved, we expect the
OCaml functions to satisfy the properties that were proven for their Isabelle counterparts
(e.g. that the OCaml-generated dL is the derivative of the OCaml-generated L).

We make use of code generation machinery of Isabelle, which provides code printing
instructions for translating between real numbers in Isabelle to ﬂoating point numbers in
OCaml [10] to generate an OCaml module LTL_Loss.

4.2 PyTorch integration

Python bindings for the OCaml deﬁnitions of L and dL are incorporated into a PyTorch
autograd.Function object through the forward and backward methods, respectively. These
methods are required for the loss function to form part of a computational graph in PyTorch
and enable training based on gradient descent. Each object is parameterised by an LTL
constraint, represented as an OCaml expression; constants for comparison; and a value for γ.
Consequently, our LTL_Loss module, implemented as a subclass of autograd.Function, is
functionally identical to a diﬀerentiable PyTorch operation on tensors. Paths are represented
as tensors, with each row containing the values related to a particular state. This is a
transpose of the arrangement depicted in Fig. 1.

Signiﬁcantly, we know exactly how LTL_Loss should behave when computing gradients
with autograd, as it is solely characterised by its formalisation in Isabelle. For a more
detailed explanation on the construction of the class and the mapping of a path to a PyTorch
tensor, the reader is referred to Appendix D.

5

Experiments

With our Isabelle-formalised loss function L and its explicit derivative dL fully available as
a generic autograd function in PyTorch, we now verify that we can achieve experimental
results that include and extend those of Innes and Ramamoorthy [13].

We take our experimental models from their work and extend them to show the improve-
ments provided by our approach. Speciﬁcally, after replicating some of the main results from
their work using our method, we also demonstrate an application of the Until constraint
whose loss evaluation was not implemented in their code.

5.1 Domain setup

Each of the tests takes place in a 2-dimensional planar environment with Cartesian co-
ordinates. The training data follow a spline-shaped curve consisting of N=100 sequenced
points in the plane following the curve with small random perturbations, simulating a
demonstrator moving via some trajectory from the origin to some destination in the plane.

10

Constrained Training of Neural Networks via Theorem Proving

We train a feed-forward neural network to learn a Dynamic Movement Primitive (DMP) [25]
to follow this trajectory.

5.2 Unconstrained training

Let D denote the trajectory of the demonstrator along the spline and Q denote the trajectory
of the DMP learned by the neural network. Both Q and D are N × 2 matrices. Moreover, let
Qi denote the row vector at index i of Q (likewise for Di and D). The per-sample imitation
loss, Ld, for this sample pair is given by Ld(Q, D) = 1
N

i=0 kQi − Dik2.

PN −1

Intuitively, Ld penalises the learned trajectory for deviating from the demonstrator. For
a batch of samples, we compute the average imitation loss. Ld is used as the sole loss in
the training of the neural network over 200 epochs using the Adam optimizer [15] with a
learning rate of 10−3.

5.3 Constrained training with LTLf

First, it is important to distinguish that for a given sample, its trajectory Q is not necessarily
the same as the corresponding path P to be reasoned over by LTL. While Q encodes the x
and y co-ordinates at each point, P encodes values about the trajectory (which may include
intermediate functions of x and y) as well as any constants for comparison, that are reasoned
over by the LTL constraint.

With this in mind, consider a diﬀerentiable function g which maps Q to a path P , an
LTL constraint ρ which reasons over that path, and a relaxation factor γ > 0. We can
incorporate this constraint into the learning process by augmenting the per-sample loss
function to be minimised to give Lf ull = Ld(Q, D) + ηL(ρ, g(Q), γ), where η is a positive
constant representing the weighting of the constraint violation against the imitation loss.
This weighting can be adjusted to reﬂect the priority in satisfying the constraints relative to
following the demonstrator accurately.

We now repeat the same training procedure as for the unconstrained case, but with
this new loss function and with γ = 0.005. In PyTorch, the loss L is implemented by an
instantiation of LTL_Loss (mentioned in Section 4.2) with ρ and γ as arguments.

We lay out 4 diﬀerent problems. Tests 1, 2, and 4 are similar to those of Innes and
Ramamoorthy [13], while Test 3 evaluates the Until constraint, which is a non-trivial
extension to their available constraints:

1. Avoid: The trajectory (always) avoids an open ball of radius 0.1 around the point
o = (0.4, 0.4). At each time-step, we compute the Euclidean distance between the
trajectory and o, which we denote as pdo. The LTL constraint becomes: (cid:3)(0.1 ≤ pdo).
2. Patrol: The trajectory eventually reaches o1 = (0.2, 0.4) and o2 = (0.85, 0.6) in the plane.
With Euclidean distances pdo1, pdo2, this constraint becomes: (♦(pdo1 ≤ 0))∧(♦(pdo2 ≤ 0)).
Note that we do not use the comparison = as both pdo1 and pdo2 are non-negative by
construction and this formulation has a lower computational cost.

3. Until: The y co-ordinate, py, of the trajectory cannot exceed 0.4 until its x co-ordinate,

px, is at least 0.6: (py ≤ 0.4) U (0.6 ≤ px).

4. Compound: A more complicated test combining several conditions. The trajectory
should avoid an open ball of radius 0.1 around the point o1 = (0.5, 0.5), while eventually
touching the point o2 = (0.7, 0.5). Further, the y co-ordinate of the trajectory should
not exceed 0.8. With pdo1 and pdo2 deﬁned in the same way as before, this compound
constraint is represented as: ((cid:3)(0.1 ≤ pdo1)) ∧ (♦(pdo2 ≤ 0)) ∧ ((cid:3)(py ≤ 0.8)).

M. Chevallier, M. Whyte and J. D. Fleuriot

11

Test

Unconstrained

Ld

L

Constrained
L
Ld

Avoid
Patrol
Until
Compound, η = 1
Compound, η = 4

0.0047
0.0052
0.0036
0.0044
0.0055

0.0789
0.1238
0.0970
0.1612
0.1611

0.0146
0.0312
0.0158
0.0318
0.0358

0.0239
0.0049
0.0207
0.0291
0.0269

Table 1 The losses associated with each test, averaged over 5 iterations

For a more concrete explanation of the role of function g, consider the Avoid test. Here, g
is deﬁned to act row-wise on a trajectory Q, producing new row vectors whose elements are
the Euclidean distance pdo and the constant 0.1, as these are the only quantities reasoned
over by the LTL constraint.

5.4 Results

When we run our tests, we ﬁrst train the neural network ignoring the logical constraint, then
we use the latter as part of its loss calculations. The diﬀerences between the two sets of
results demonstrates the eﬀectiveness of the logical constraint as used in the loss function for
the training. The loss ﬁgures are shown in Table 1.

The losses in the table show that when we do perform unconstrained training, we
understandably have high constraint losses. These are substantially improved when we
constrain the training, although the imitation losses increase as the trajectory outputted by
the neural network no longer follows the training trajectory as closely.

Diﬀerent constraints respond with diﬀerent loss values based on the speciﬁc deﬁnitions
used, so we should be careful when making comparisons between the diﬀerent constrained
tests (save for the two diﬀerently weighted compound tests). Even though in most of our
tests the constraints are satisﬁed, the constraint losses are not zero. Given that we are
dealing with a soft L function with a positive γ parameter, this is expected. By the soundness
theorem (Listing 6), the constraint losses would be reduced further with smaller values of γ.
Visually, we can conﬁrm that the constrained training is actually satisfying the spe-
ciﬁcations of the Avoid, Patrol, and Compound (for η = 4) tests in Fig. 2. The Until and
Compound (for η = 1) tests are not fully satisﬁed (Until starts crossing the line y = 0.4
slightly early, and Compound brieﬂy enters the open ball and doesn’t quite reach its goal at
o2), but clearly exhibit altered behaviour towards meeting the expected constraints. The
slight violations happen because, in addition to the constraint loss, the trajectory loss plays
a part in forming the neural network’s training. These losses work against each other in
our tests. Overall, though, these results demonstrate the clear eﬀectiveness of the logical
constraint in changing the learned behaviour of the DMP.

Examining the Compound test (for η = 1) more closely, we note that the trajectory
slightly wanders within the open ball and does not quite reach the point we want it to. To
verify how weighting aﬀects this, we ran the test again with an increased value, namely η = 4,
applied to the constraint loss and the eﬀects are obvious – the trajectory now properly avoids
the ball and gets closer to the point required. The same approach can be taken with the
Until test to ensure satisfaction of its constraint. However, due to space considerations, we
will not be considering this here.

At the conclusion of our experiments it is clear that the certainty of our approach comes

12

Constrained Training of Neural Networks via Theorem Proving

Figure 2 Avoid, Patrol and Until tests; and the Compound test, with η = 1 and η = 4

at a cost in performance: running a full test can take anywhere from a few minutes to a few
hours for the more complicated constraints, and the performance cannot be enhanced by
running on a GPU as our OCaml functions cannot take advantage of this. There are several
possible approaches to resolving this and improving performance as discussed next.

6

Conclusion

We demonstrate that using a theorem proving approach, we can formulate a deep embedding
of LTLf in higher-order logic and use this to fully formalise logical constraints for training, as
well as the loss function and its derivative. We can then generate code for the whole logical
framework and integrate it with PyTorch. Furthermore, our experimental results show that
these constraints can successfully change the training process to match the desired behaviour.
Thus, we believe that our work provides much stronger guarantees of correctness than one
could expect from an ad-hoc implementation of logical operators made directly in Python.
There are some practical limitations to the current work. In particular, training the
neural network can be slow because OCaml functions are used to compute the loss and its
derivative with respect to each possible input, and these functions run outside the highly
optimised PyTorch infrastructure. We are exploring some potential ways of addressing this
shortcoming, which include adopting a tensor formulation within Isabelle to take advantage
of OCaml bindings for PyTorch tensors [20] and producing PyTorch-compatible code by
extending Isabelle’s existing functionality for code generation.

Our approach is generic, so in principle a diﬀerent formalism, e.g. a continuous-time logic,
could be used instead of LTLf . Moreover, by formalising the derivative of the loss function, we
unlock the potential to reason formally about the traversal of the loss surface during gradient
descent. Given our results, we believe this work opens the way to a tighter integration
between fully-formal symbolic reasoning in a theorem prover and machine learning.

M. Chevallier, M. Whyte and J. D. Fleuriot

13

References

1

2

Samy Badreddine, Artur d’Avila Garcez, Luciano Seraﬁni, and Michael Spranger. Logic
tensor networks. Artiﬁcial Intelligence, 303:103649, 2022. URL: https://www.sciencedirect.
com/science/article/pii/S0004370221002009, doi:https://doi.org/10.1016/j.artint.2021.
103649.
Jorge Baier and Sheila Mcilraith. Planning with First-Order Temporally Extended Goals
using Heuristic Search. In Proceedings of the National Conference on Artiﬁcial Intelligence,
volume 1, 01 2006.

3 Richard J Boulton, Andrew D Gordon, Michael JC Gordon, John Harrison, John Herbert,
and John Van Tassel. Experience with embedding hardware description languages in hol. In
TPCD, volume 10, pages 129–156. Citeseer, 1992.

4 Marco Cuturi and Mathieu Blondel. Soft-DTW: a diﬀerentiable loss function for time-series.

arXiv preprint arXiv:1703.01541, 2017.

5 Giuseppe De Giacomo and Moshe Y Vardi. Linear temporal logic and linear dynamic logic on
ﬁnite traces. In IJCAI’13 Proceedings of the Twenty-Third international joint conference on
Artiﬁcial Intelligence, pages 854–860. Association for Computing Machinery, 2013.

6 Ruediger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In
International Symposium on Automated Technology for Veriﬁcation and Analysis, pages 269–
286. Springer, 2017.

8

9

7 Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce Zhang, and Martin
Vechev. DL2: Training and querying neural networks with logic. In International Conference
on Machine Learning, pages 1931–1941, 2019.
Jacques D. Fleuriot. On the mechanization of real analysis in Isabelle/HOL. In Mark Aagaard
and John Harrison, editors, Theorem Proving in Higher Order Logics, pages 145–161. Springer,
2000.
Florian Haftmann and Lukas Bulwahn. Code generation from Isabelle/HOL theories. Isabelle
documentation, 2021. URL: https://isabelle.in.tum.de/doc/codegen.pdf.
Florian Haftmann, Johannes Hölzl, and Tobias Nipkow. Code_Real_Approx_By_Float.
HOL_Library, 2021. URL: https://isabelle.in.tum.de/library/HOL/HOL-Library/Code_
Real_Approx_By_Float.html.
Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep
neural networks with logic rules. In Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pages 2410–2420, Berlin, Germany,
August 2016. Association for Computational Linguistics. URL: https://aclanthology.org/
P16-1228, doi:10.18653/v1/P16-1228.

10

11

12 Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety veriﬁcation of deep
In International conference on computer aided veriﬁcation, pages 3–29.

neural networks.
Springer, 2017.
Craig Innes and Ram Ramamoorthy. Elaborating on Learned Demonstrations with Temporal
Logic Speciﬁcations.
In Marc Toussaint, Antonio Bicchi, and Tucker Hermans, editors,
Proceedings of Robotics: Science and System XVI, 2020. doi:10.15607/RSS.2020.XVI.004.

13

14 Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex:
An eﬃcient smt solver for verifying deep neural networks. In International conference on
computer aided veriﬁcation, pages 97–117. Springer, 2017.

15 Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL:
http://arxiv.org/abs/1412.6980.

16 Yen-Ling Kuo, Boris Katz, and Andrei Barbu. Encoding formulas as deep networks: Rein-
forcement learning for zero-shot execution of LTL formulas. In 2020 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pages 5604–5610. IEEE, 2020.

14

Constrained Training of Neural Networks via Theorem Proving

17 Tao Li and Vivek Srikumar. Augmenting neural networks with ﬁrst-order logic. In Anna
Korhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings of the 57th Conference
of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August
2, 2019, Volume 1: Long Papers, pages 292–302. Association for Computational Linguistics,
2019. doi:10.18653/v1/p19-1028.

18 Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, and Marco Gori. Lyrics: A
general interface layer to integrate logic inference and deep learning. In Ulf Brefeld, Elisa
Fromont, Andreas Hotho, Arno Knobbe, Marloes Maathuis, and Céline Robardet, editors,
Machine Learning and Knowledge Discovery in Databases, pages 283–298, Cham, 2020. Springer
International Publishing.
Laurent Mazare. Using Python and OCaml in the same Jupyter notebook. December 2019.
URL: https://blog.janestreet.com/using-python-and-ocaml-in-the-same-jupyter-notebook.
Laurent Mazare. OCaml-Torch. GitHub, 2021. URL: https://github.com/LaurentMazare/
ocaml-torch.
Tobias Nipkow, Lawrence C Paulson, and Markus Wenzel. Isabelle/HOL: a proof assistant for
higher-order logic, volume 2283. Springer Science & Business Media, 2002.

19

21

20

22 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. Advances in neural information processing
systems, 32:8026–8037, 2019.
Lerrel Pinto and Abhinav Gupta. Supersizing self-supervision: Learning to grasp from 50k
tries and 700 robot hours. In 2016 IEEE International Conference on Robotics and Automation
(ICRA), pages 3406–3413, 2016. doi:10.1109/ICRA.2016.7487517.

23

24 Amir Pnueli. The temporal logic of programs. In 18th Annual Symposium on Foundations of

25

Computer Science (sfcs 1977), pages 46–57, 1977. doi:10.1109/SFCS.1977.32.
Stefan Schaal, Jan Peters, Jun Nakanishi, and Auke Ijspeert. Learning movement primitives.
In Robotics research. the eleventh international symposium, pages 561–572. Springer, 2005.

26 Karsten Scheibler, Leonore Winterer, Ralf Wimmer, and Bernd Becker. Towards veriﬁcation

27

of artiﬁcial neural networks. In MBMV, pages 30–40, 2015.
Pashootan Vaezipoor, Andrew C Li, Rodrigo A Toro Icarte, and Sheila A. Mcilraith.
LTL2Action: Generalizing LTL instructions for multi-task RL. In Marina Meila and Tong
Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume
139 of Proceedings of Machine Learning Research, pages 10497–10508. PMLR, 18–24 Jul 2021.
28 Markus Wenzel. Isar—a generic interpretative approach to readable formal proof documents.
In International Conference on Theorem Proving in Higher Order Logics, pages 167–183.
Springer, 1999.
Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Broeck. A Semantic Loss
Function for Deep Learning with Symbolic Knowledge. In Jennifer Dy and Andreas Krause,
editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of
Proceedings of Machine Learning Research, pages 5502–5511, Stockholmsmässan, Stockholm
Sweden, 10–15 Jul 2018. PMLR. URL: http://proceedings.mlr.press/v80/xu18h.html.

29

M. Chevallier, M. Whyte and J. D. Fleuriot

15

A

Isabelle speciﬁcations

function eval :: " constraint ⇒ path ⇒ bool " where

" eval c [] = False "

| " eval ( Comp ( Lequal v1 v2 )) ( s # ss ) = (( s v1 ) ≤ ( s v2 ))"
| " eval ( Comp ( Nequal v1 v2 )) ( s # ss ) = (( s v1 ) 6= ( s v2 ))"
| " eval ( Comp ( Less v1 v2 )) ( s # ss ) = (( s v1 ) < ( s v2 ))"
| " eval ( Comp ( Equal v1 v2 )) ( s # ss ) = (( s v1 ) = ( s v2 ))"
| " eval ( And c1 c2 ) ( s # ss ) = (( eval c1 ( s # ss )) ∧ ( eval c2 ( s # ss )))"
| " eval ( Or c1 c2 ) ( s # ss ) = (( eval c1 ( s # ss )) ∨ ( eval c2 ( s # ss )))"
| " eval ( Next c ) ( s # ss ) = eval c ss "
| " eval ( Always c ) ( s # ss ) = (( eval c ( s # ss )) ∧ ( if ss = [] then True

else ( eval ( Always c ) ss )))"

| " eval ( Eventually c ) ( s # ss ) = (( eval c ( s # ss ))

∨ ( eval ( Eventually c ) ss ))"

| " eval ( Until c1 c2 ) ( s # ss ) = (((( eval c1 ( s # ss ))

∧ ( if ss = [] then True else ( eval ( Until c1 c2 ) ss ))))

∨ eval c2 ( s # ss ))"

| " eval ( Release c1 c2 ) ( s # ss ) = ((((( eval c2 ( s # ss ))

∧ ( eval ( Release c1 c2 ) ss )))

∨ eval ( And c1 c2 ) ( s # ss )) ∧ eval ( Eventually c1 ) ( s # ss ))"

Listing 10 The LTLf evaluation function, described in Section 2.2

fun Min_gamma :: " real ⇒ real ⇒ real ⇒ real " where

" Min_gamma γ a b = ( if γ ≤ 0 then min a b

else -γ * ln ( exp ( - a /γ ) + exp ( - b /γ )))"

lemma Min_gamma_lim : "(λγ . Min_gamma γ a b ) −0→ min a b "

Listing 11 The soft gamma Min function and its correctness theorem, described in Section 3.1

function L :: " constraint ⇒ path ⇒ real ⇒ real " where

" L c [] γ = 1"

| " L ( Comp ( Lequal v1 v2 )) ( s # ss ) γ = Max_gamma γ ( s v1 - s v2 ) 0"
| " L ( Comp ( Nequal v1 v2 )) ( s # ss ) γ = Nequal_gamma γ ( s v1 ) ( s v2 )"
| " L ( Comp ( Less v1 v2 )) ( s # ss ) γ = Max_gamma γ

( L ( Comp ( Lequal v1 v2 )) ( s # ss ) γ )
( L ( Comp ( Nequal v1 v2 )) ( s # ss ) γ )"

| " L ( Comp ( Equal v1 v2 )) ( s # ss ) γ = Max_gamma γ

( L ( Comp ( Lequal v1 v2 )) ( s # ss ) γ )
( L ( Comp ( Lequal v2 v1 )) ( s # ss ) γ )"

| " L ( And c1 c2 ) ( s # ss ) γ = Max_gamma γ

( L c1 ( s # ss ) γ )
( L c2 ( s # ss ) γ )"

| " L ( Or c1 c2 ) ( s # ss ) γ = Min_gamma γ

( L c1 ( s # ss ) γ )
( L c2 ( s # ss ) γ )"

| " L ( Next c ) ( s # ss ) γ = L c ss γ "
| " L ( Always c ) ( s # ss ) γ = Max_gamma γ ( L c ( s # ss ) γ )

( if ss = [] then 0 else ( L ( Always c ) ss ) γ )"

| " L ( Eventually c ) ( s # ss ) γ = Min_gamma γ ( L c ( s # ss ) γ )

( L ( Eventually c ) ss γ )"

| " L ( Until c1 c2 ) ( s # ss ) γ = Min_gamma γ ( L c2 ( s # ss ) γ )

( Max_gamma γ ( L c1 ( s # ss ) γ ) ( if ss = [] then 0

else ( L ( Until c1 c2 ) ss γ )))"
| " L ( Release c1 c2 ) ( s # ss ) γ = Max_gamma γ

( L ( Eventually c1 ) ( s # ss ) γ )
( Min_gamma γ

( Max_gamma γ ( L c1 ( s # ss ) γ ) ( L c2 ( s # ss ) γ ))
( Max_gamma γ ( L c2 ( s # ss ) γ ) ( L ( Release c1 c2 ) ss γ )))"

Listing 12 The L function, described in Section 3.2

16

Constrained Training of Neural Networks via Theorem Proving

function dL :: " constraint ⇒ path ⇒ real ⇒ int ⇒ int ⇒ real " where

" dL c [] γ i j = 0"

| " dL ( Comp ( Lequal v1 v2 )) ( s # ss ) γ i j = ( if i 6= 0

then 0 else ( dLequal_gamma_ds γ ( s v1 )

( if eqint j v1 then (1:: real ) else 0) ( s v2 )
( if eqint j v2 then (1:: real ) else 0)))"

| " dL ( Comp ( Nequal v1 v2 )) ( s # ss ) γ i j = ( if ( i 6= 0) then 0

else ( dNequal_gamma_ds γ ( s v1 )

( if j = v1 then 1 else 0) ( s v2 )
( if j = v2 then 1 else 0)))"

| " dL ( Comp ( Less v1 v2 )) ( s # ss ) γ i j = dMax_gamma_ds γ

( L ( Comp ( Lequal v1 v2 )) ( s # ss ) γ )

( dL ( Comp ( Lequal v1 v2 )) ( s # ss ) γ i j )

( L ( Comp ( Nequal v1 v2 )) ( s # ss ) γ )

( dL ( Comp ( Nequal v1 v2 )) ( s # ss ) γ i j )"

| " dL ( Comp ( Equal v1 v2 )) ( s # ss ) γ i j = dMax_gamma_ds γ

( L ( Comp ( Lequal v1 v2 )) ( s # ss ) γ )

( dL ( Comp ( Lequal v1 v2 )) ( s # ss ) γ i j )

( L ( Comp ( Lequal v2 v1 )) ( s # ss ) γ )

( dL ( Comp ( Lequal v2 v1 )) ( s # ss ) γ i j )"
| " dL ( And c1 c2 ) ( s # ss ) γ i j = dMax_gamma_ds γ

( L c1 ( s # ss ) γ )

( dL c1 ( s # ss ) γ i j )

( L c2 ( s # ss ) γ )

( dL c2 ( s # ss ) γ i j )"

| " dL ( Or c1 c2 ) ( s # ss ) γ i j = dMin_gamma_ds γ

( L c1 ( s # ss ) γ )

( dL c1 ( s # ss ) γ i j )

( L c2 ( s # ss ) γ )

( dL c2 ( s # ss ) γ i j )"

| " dL ( Next c ) ( s # ss ) γ i j = dL c ss γ (i -1) j "
| " dL ( Always c ) ( s # ss ) γ i j = dMax_gamma_ds γ

( L c ( s # ss ) γ )

( dL c ( s # ss ) γ i j )

( if ss = [] then 0 else ( L ( Always c ) ss ) γ )

( if ss = [] then 0 else ( dL ( Always c ) ss ) γ (i -1) j )"

| " dL ( Eventually c ) ( s # ss ) γ i j = dMin_gamma_ds γ

( L c ( s # ss ) γ )

( dL c ( s # ss ) γ i j )
( L ( Eventually c ) ss γ )

( dL ( Eventually c ) ss γ (i -1) j )"

| " dL ( Until c1 c2 ) ( s # ss ) γ i j = dMin_gamma_ds γ ( L c2 ( s # ss ) γ )

( dL c2 ( s # ss ) γ i j ) ( Max_gamma γ ( L c1 ( s # ss ) γ )
( if ( ss = []) then 0 else ( L ( Until c1 c2 ) ss γ )))
( dMax_gamma_ds γ ( L c1 ( s # ss ) γ ) ( dL c1 ( s # ss ) γ i j )

( if ( ss = []) then 0 else ( L ( Until c1 c2 ) ss γ ))
( if ( ss = []) then 0 else ( dL ( Until c1 c2 ) ss γ (i -1) j )))"

| " dL ( Release c1 c2 ) ( s # ss ) γ i j = dMax_gamma_ds γ

( L ( Eventually c1 ) ( s # ss ) γ )

( dL ( Eventually c1 ) ( s # ss ) γ i j )

( Min_gamma γ ( Max_gamma γ ( L c1 ( s # ss ) γ ) ( L c2 ( s # ss ) γ ))
( Max_gamma γ ( L c2 ( s # ss ) γ ) ( L ( Release c1 c2 ) ss γ )))

( dMin_gamma_ds γ ( Max_gamma γ ( L c1 ( s # ss ) γ ) ( L c2 ( s # ss ) γ ))

( dMax_gamma_ds γ
( L c1 ( s # ss ) γ ) ( dL c1 ( s # ss ) γ i j )
( L c2 ( s # ss ) γ ) ( dL c2 ( s # ss ) γ i j ))

( Max_gamma γ ( L c2 ( s # ss ) γ ) ( L ( Release c1 c2 ) ss γ ))

( dMax_gamma_ds γ
( L c2 ( s # ss ) γ ) ( dL c2 ( s # ss ) γ i j )
( L ( Release c1 c2 ) ss γ ) ( dL ( Release c1 c2 ) ss γ (i -1) j )))"

Listing 13 The dL function, described in Section 3.3

M. Chevallier, M. Whyte and J. D. Fleuriot

17

B

A brief introduction to LTLf

Linear temporal logic uses time-based modalities with which propositions can be held to be
true or false at particular steps of a sequence in discrete time. This truth-value may change
for a given proposition as the time-sequence progresses.

The LTLf formulae are: ρ, ρ1 ∧ ρ2, ρ1 ∨ ρ2, N ρ (Next), (cid:3)ρ (Always), ♦ρ (Eventually),
ρ1Uρ2 (Weak Until), ρ1Rρ2 (Strong Release). The ﬁrst three are understood as per the
semantics of propositional logic connectives. Informally, the rest are to be understood as
follows:

N ρ (Next ρ) is true if at the next step along the time-sequence, ρ holds. Note that as we
are working in ﬁnite paths, we must consider how to evaluate this at the termination of a
path – we discuss this below.
(cid:3)ρ (Always ρ) holds if at the current step and all subsequent steps along the time-sequence,
ρ holds.
♦ρ (Eventually ρ) holds if at the current or at least one subsequent step along the
time-sequence, ρ holds.
ρ1Uρ2 (ρ1 Until ρ2) means that ρ1 holds at least for all steps until ρ2 holds. ρ2 need
not hold at any future point – this is a Weak Until.
ρ1Rρ2 (ρ1 Release ρ2) means that ρ2 holds at least until and including the point when
ρ1 holds. ρ1 must hold at some point in the path – this is a Strong Release.

As discussed in Section 2.2, we must consider how N is intended to behave at the end of a
time-sequence. As per the usual treatment of LTL on a ﬁnite trace, that with a sequence of
length i, we assume ¬(N ρ) holds at the ﬁnal step i [5].

C

An extended description of soft functions

The soft maximum function has a derivative function that we brieﬂy mentioned in Section
3.1. We give some more details of its construction here.

fun dMax_gamma_da :: " real ⇒ real ⇒ real ⇒ real " where

" dMax_gamma_da γ a b = exp ( a /γ )/( exp ( a /γ ) + exp ( b /γ ))"

fun dMax_gamma_ds :: " real ⇒ real ⇒ real ⇒ real ⇒ real ⇒ real " where

" dMax_gamma_ds γ a da b db = ( dMax_gamma_da γ a b ) * da

+ ( dMax_gamma_db γ a b ) * db "

Listing 14 The derivative for our soft max function, as discussed in Section 3.1

For Max_gamma, the derivatives with respect to a and b are built separately before being
combined to give the dMax_gamma_ds function. In the above, dMax_gamma_da is deﬁned and
then proven to be the partial derivative of the softmax function Max_gamma γ a b w.r.t. a
when γ > 0. The formulation of the derivative with respect to b is similar and hence
omitted. We only consider the case γ > 0 as this is when our soft functions are diﬀerentiable.
dMax_gamma_ds is the derivative w.r.t. both paramaters.

In addition to the soft maximum and minimum functions we discuss in Section 3.1, there
are a number of other soft functions we use. We discuss their Isabelle speciﬁcation and their
proofs of correctness here.

We use a function Bell_gamma to capture losses from the Nequal comparison. In Isabelle,

we have:

fun Nzero :: " real ⇒ real " where

" Nzero x = ( if x =0 then 1 else 0)"

18

Constrained Training of Neural Networks via Theorem Proving

fun Bell_gamma :: " real ⇒ real ⇒ real " where

" Bell_gamma γ x = ( if γ≤0 then ( Nzero x ) else 1/ exp ( x ^2/(2*γ )))"

fun Nequal_gamma :: " real ⇒ real ⇒ real ⇒ real " where

" Nequal_gamma γ a b = Bell_gamma γ (a - b )"

Listing 15 The Nequal_gamma function

where we have a derivative function, dBell_gamma_dx deﬁned as:

fun dBell_gamma_dx :: " real ⇒ real ⇒ real " where
" dBell_gamma_dx γ x = exp ( -( x ^2)/(2*γ ))*( - x /γ )"

Listing 16 Derivative for the Bell_gamma function

We also prove that limγ→0 Bell_gamma γ 0 = 1 (and is 0 for any other value) and that

dBell_gamma_dx is the expected derivative when γ > 0:

theorem B e l l _ g a m m a _ d e r i v _ g a m m a _ g t _ z e r o :

assumes gamma_gt_zero : "γ >0"
shows "((λy . Bell_gamma γ y ) has_ fi eld_derivative

dBell_gamma_dx γ x ) ( at x )"

Listing 17 Correctness of the Bell_gamma derivative

As can be seen above, Nequal_gamma is deﬁned by passing the diﬀerence of two values
a and b to Bell_gamma. We can thus leverage the above properties to deﬁne and prove its
derivative dNequal_gamma_ds, in a similar way as was done for Max_gamma. For brevity, this
is omitted.

D PyTorch integration

PyTorch is a Python library for deep learning [22]. We use it for testing whether our
generated code enables the intended training within the framework developed by Innes and
Ramamoorthy [13] (see Section 5). Crucial to PyTorch (and other similar libraries) is the
tensor datatype that is used to represent inputs and outputs over a network of mathematical
operations in a typically multi-dimensional array-like form.

A sequence of tensors and operations performed thereon are recorded in a directed acyclic
graph known as a computational graph. Traversing this graph from a root scalar tensor,
PyTorch’s automatic diﬀerentiation engine torch.autograd computes the gradients of the
root tensor with respect to each of the elements of the tensors in the graph, via successive
application of the chain rule of diﬀerentiation. This algorithm enables a fundamental approach
to neural network training, backpropagation, where a network’s weights are iteratively
adjusted according to the gradient of a computed loss with respect to themselves.

In order for our LTL loss function to form part of a computational graph in PyTorch, it
must be implemented as a subclass of autograd.Function. We call this class LTL_Loss as
per our OCaml module, and it is parameterised by an LTL constraint, represented as an
OCaml expression, constants for comparison, and a value for γ.

As noted in Section 2.1, an instantiation of a path can be represented by a matrix:

P =




 =






s0
...
sN −1






t00
...
t(N −1)0

. . .
. . .
. . .

t0(K−1)
...
t(N −1)(K−1)






M. Chevallier, M. Whyte and J. D. Fleuriot

19

Here, each row is a representation of a state along the path. This matrix is useful as it can
be implemented as a PyTorch tensor.

To interface our OCaml code for L and dL with autograd, two methods are deﬁned:

1. forward: this applies the loss function to a 2-dimensional input tensor representing a path,
using the OCaml bindings to apply the OCaml representation of L. If a 3-dimensional
input tensor is given, with the leading dimension indexing separate paths, an average loss
is computed.

2. backward: this computes the derivative of the scalar loss output with respect to the input
tensor. Iterating over each element of the tensor, we call the OCaml representation of dL
to compute the derivative of the loss with respect to it.

With this, our class is functionally identical to a diﬀerentiable PyTorch operation on

tensors, as discussed in Section 4.2.

