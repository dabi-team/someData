0
2
0
2

p
e
S
9
1

]

G
L
.
s
c
[

2
v
9
7
3
6
0
.
7
0
0
2
:
v
i
X
r
a

Rule Covering for Interpretation and Boosting

¸S. ˙Ilker Birbil∗
Econometric Institute
Erasmus University Rotterdam
3000 DR Rotterdam, The Netherlands
birbil@ese.eur.nl

Mert Edali
Department of Medicine
University of Chicago
Chicago, IL 60637, USA
meadli@medicine.bsd.uchicago.edu

Birol Yüceo˘glu
Migros T.A. ¸S
Ata¸sehir, 34758, ˙Istanbul, Turkey
biroly@migros.com.tr

Abstract

We propose two algorithms for interpretation and boosting of tree-based ensemble
methods. Both algorithms make use of mathematical programming models that
are constructed with a set of rules extracted from an ensemble of decision trees.
The objective is to obtain the minimum total impurity with the least number of
rules that cover all the samples. The ﬁrst algorithm uses the collection of decision
trees obtained from a trained random forest model. Our numerical results show
that the proposed rule covering approach selects only a few rules that could be
used for interpreting the random forest model. Moreover, the resulting set of rules
closely matches the accuracy level of the random forest model. Inspired by the
column generation algorithm in linear programming, our second algorithm uses
a rule generation scheme for boosting decision trees. We use the dual optimal
solutions of the linear programming models as sample weights to obtain only those
rules that would improve the accuracy. With a computational study, we observe that
our second algorithm performs competitively with the other well-known boosting
methods. Our implementations also demonstrate that both algorithms can be
trivially coupled with the existing random forest and decision tree packages.

1

Introduction

Although their prediction performances are remarkable, tree-based ensemble methods are difﬁcult to
interpret due to large number of trees in the trained model. Among these methods, Random Forest
algorithm is probably the most frequently used alternative for solving classiﬁcation and regression
problems arising in different domains [2]. This performance versus interpretability trade-off leads
to the following two main questions in our work: Given a trained random forest, can we obtain an
interpretable representation with mathematical programming that shows a performance close-enough
to the overall forest? Using a similar mathematical programming model, can we come up with a
learning algorithm, which generates only those rules that improve the performance of a base tree?

To answer these questions, we focus on the rules constituting the trees in the forest and propose two
algorithms based on mathematical programming models. The model in the ﬁrst algorithm extracts
the rules corresponding to the leaves of the forest that result in the minimum total impurity while
covering all the samples. With this algorithm, we are able to mimick the performance of the original
forest with a signiﬁcantly few number of rules (most of the time even less than the number of rules in

∗Corresponding author.

Preprint. Under review.

 
 
 
 
 
 
a regular decision tree). Being encouraged with these results, we then propose our second algorithm
based on selective generation of rules. This algorithm improves the total impurity gradually by
solving a series of linear programming problems. This approach is a variant of the column generation
algorithm in mathematical optimization. In our scheme, we use the dual optimal solutions of the
linear programs as the sample weights. We also observe that this scheme has close ties to well-known
ideas in boosting methods [13, 14, 17]. Thus, rule extraction and boosting are the two keywords that
help us to review the related literature.

Friedman and Popescu [9] propose RuleFit method to extract rules from ensembles. The authors
use decision trees as base learners, where each node of the decision tree corresponds to a rule. They
present an ensemble generation algorithm to obtain a rule set. Then, they solve a linear regression
problem that minimizes the loss with a lasso penalty for selecting rules. Meinshausen [15] presents a
similar approach, called Node Harvest, which relies on rules generated by a tree ensemble. Node
Harvest minimizes the quadratic loss function by partitioning the samples and relaxing the integrality
constraints on the weights. Even though the algorithm does not constrain the number of rules, the
author observes that only a small number of rules are used. Mashayekhi and Gras [11] propose
coverage of the samples in the training set by the rules generated by Random Forest algorithm. They
create a score for each rule by considering the percentage of correctly and incorrectly classiﬁed
samples as well as the rule length. By using a hill climbing algorithm, they are able to obtain slightly
lower accuracy with much fewer rules than random forest. Extending this work, Mashayekhi and
Gras [12] propose three different algorithms to extract if-then rules from decision tree ensembles.
The authors then compare their algorithms with both RuleFit and Node Harvest. All these approaches
use accuracy values to construct the objective functions. Interpretability in terms of reducing the
number of rules is achieved by using lasso penalties. Our ﬁrst algorithm, on the other hand, reduces
the number of rules directly through the objective function.

Gradient boosting algorithms use a sequence of weak learners and build a model iteratively [7, 13, 8].
At each iteration the boosting algorithm focuses on the misclassiﬁed samples by adjusting their
weights with respect to the errors of the previous iterations. A boosting method that uses linear
programming is called LPBoost [5]. This algorithm uses weak learners to maximize the margins
separating the different classes. Like us, the authors also use column generation to create the weak
learners iteratively. The ﬁnal ensemble then becomes a weighted combination of the weak learners.
The approach can accept decision stumps or decision trees as weak learners. However, decision trees
can perform badly in terms of convergence and solution accuracy, if the dual variables associated with
margin classiﬁcation are not constrained. Unlike LPBoost, we use the duals as sample weights in our
column generation routine. Another boosting method based on mathematical programming is called
IPBoost [16]. This approach uses binary decision variables corresponding to misclassiﬁcation by a
predetermined margin. Due to binary decision variables, the authors resort to an integer programming
solution technique called branch-and-bound-and-price. Dash et al. [4] also use column generation to
learn Boolean decision rules in either disjunctive normal form or conjunctive normal form. They solve
a sequence of integer programming problems for pricing the columns. They choose to restrict the
number of selected clauses as a user-deﬁned hyperparameter for improving interpretability. Bertsimas
and Dunn [1] propose an integer programming formulation to create optimal decision trees for multi-
class classiﬁcation. They present two approaches with respect to node splitting. The ﬁrst approach is
more general but less interpretable. The second more interpretable model considers the values of
one feature for node splitting. One advantage of their model is that it does not rely on an assumption
for the nature of the values of the features. Günlük et al. [10] propose a binary classiﬁcation tree
formulation for only categorical features. The idea can be extended to numerical variables by using
thresholding. The approach is less general than the approach presented by Bertsimas and Dunn
[1], but has some advantages like the number of integer variables being independent of the training
dataset size. Firat et al. [6] propose a column generation based heuristic for learning decision tree.
The approach is based on generating decision paths and can be used for solving instances with tens
of thousands of observations.

In the light of this review, we make the following contributions to the literature: We propose a
new mathematical programming approach for interpretation and boosting of random forests and
decision trees, respectively. Unlike other work in the literature, the objective function in our models
aims at minimizing both the total impurity and the number of selected rules. When applied to a
trained random forest for interpretation, our ﬁrst algorithm obtains signiﬁcantly few number of rules
with an accuracy level close to the random forest model. Our second algorithm is a new boosting

2

method based on rule generation and linear programming. As a novel approach, the algorithm uses
dual information to weigh samples in order to increase the coverage with less impure rules. In
our computational study, we demonstrate that both algorithms are remarkably easy to implement
within a widely-used machine learning package.2 Without any ﬁne tuning, we obtain quite promising
numerical results with both algorithms showing the potential of the rule covering for interpretation
and boosting.

2 Minimum rule cover: interpretation of forests

Let (xi, yi), i ∈ I be the set of samples, where the vector xi ∈ X and the scalar yi ∈ K denote
the input sample and the output class, respectively. We shall assume in the subsequent part that the
learning problem at hand is a classiﬁcation problem. However, our discussion here can be extended
to regression problems as well (we elaborate on this point in Section S.3.3).

Suppose that a Random Forest algorithm is trained on this dataset and a collection of trees is grown.
Given one of these trees, we can easily generate the path j from the root node to a leaf node that
results in a subset of samples xi, i ∈ I(j) ⊆ I. Actually, these paths constitute the rules that are later
used for classiﬁcation with majority voting. Each rule corresponds to a sequence of if-then clauses
and it is likely that some of these sequences may appear multiple times. We denote all such rules in
all trees by set J . Clearly, the size of J is quite large. Therefore, we next construct a mathematical
programming model that aims at selecting the minimum number of rules from the trained forest while
preserving the performance. We also make sure that all samples are covered with the selected rules.

As rule j ∈ J corresponds to a leaf node, we can also evaluate the node impurity, wj. Two most
common measures for evaluating the node impurity are known as Gini and Entropy criteria. We
further introduce the binary decision variables zj that mark whether the corresponding rules j ∈ J
are selected or not. The cost of wj is incurred, when zj = 1. Note that if we use only these impurity
evaluations as the objective function coefﬁcients (costs), then the model would tend to select many
rules that cover only a few samples, which are likely to be pure nodes. However, we want to advocate
the use of fewer rules in the model so that the resulting set of rules is easy to interpret. In other
words, we also aim at minimizing the number of rules along with the impurity cost. Therefore, we
replace the cost coefﬁcient so that selecting excessive number of rules is penalized. Our mathematical
programming model then becomes:

minimize

subject to

(cid:80)

(cid:80)

j∈J (1 + wj)zj
j∈J (i) zj ≥ 1,

zj ∈ {0, 1},

i ∈ I,

j ∈ J ,

(1)

where J (i), i ∈ I is the set of rules prescribing all the leaves involving sample xi. In other words,
sample i is covered with the rules in subset J (i) ⊆ J .

The mathematical programming model (1) is a standard weighted set covering formulation, where sets
and items correspond to rules and samples, respectively. Although set covering problem is NP-hard,
current off-the-shelf solvers are capable of providing optimal solutions to moderately large instances.
In case the integer programming formulation becomes difﬁcult to solve, there are also powerful
heuristics that return approximate solutions very quickly. One such approach is the well-known
greedy algorithm proposed by Chvatál [3]. Algorithm S.1 shows the steps of this heuristic using our
notation. Recently, Chvatál’s greedy algorithm, as we implement here, has been shown to perform
quite well on a set of test problems when compared against several more recent heuristics [18]. Thus,
we have reported our results also with this algorithm in Section 4. Clearly, this particular heuristic
can be replaced with any other approach solving the set covering problem. Since these heuristics
provide an approximate solution, we denote the resulting set of rules by ˆJ , and by deﬁnition, the
cardinality of set ˆJ is larger than the cardinality of the optimal set of rules obtained by solving (1).

Algorithm 1 gives the steps of our minimum rule cover algorithm (MIRCO). The algorithm takes as
input the trained random forest model and the training dataset. Note that after running Algorithm 1,
we obtain a subset of rules ˆJ that can also be used for classifying out-of-sample points (test set) with

2(GitHub page) – https://github.com/sibirbil/RuleCovering
3All cross references starting with letter “S” refer to the supplementary document.

3

Figure 1: A toy example illustrating that MIRCO does not necessarily correspond to a tree. Rectangles
denote the regions obtained with the rules in ˆJ . The numbers in the rectangles depict the classiﬁcation
results corresponding to three classes shown with three different markers.

majority voting. If we denote the predicted class of test sample x0 with C(x0, ˆJ ), then

C(x0, ˆJ ) = arg max
k∈K

(cid:8) (cid:88)

njkLj(x0)(cid:9),

j∈ ˆJ

(2)

where njk stands for the number of samples from class k in leaf j, and Lj(x0) is an operator showing
whether x0 satisﬁes the sequence of clauses corresponding to leaf j. In fact, testing in this manner
would be a natural approach for evaluating the performance of MIRCO. Since our main objective is
to interpret the underlying model obtained with Random Forest algorithm, we hope that this subset of
rules would obtain a classiﬁcation performance closer to the performance obtained with all the rules
in set J . We show with our computational study in Section 4 that this is indeed the case.

Algorithm 1: MIRCO

Input :Random forest model RF; training dataset (xi, yi), i ∈ I

J ← J ∪ (rules in Tree);

1 J = ∅;
2 for Tree in RF do
3
4 end
5 Evaluate impurities wj, j ∈ J using (xi, yi), i ∈ I;
6 Construct sets J (i), i ∈ I ;
7 Solve model (1) with Algorithm S.1;
8 Return ˆJ ;

Figure 1 shows an example set of of rules obtained with MIRCO on a small data set. Clearly, the
resulting set of rules does not correspond to a decision tree. On one hand, this implies that we cannot
simply select one of the trees grown by the Random Forest algorithm and replace its leaves with the
set of rules obtained with MIRCO. Consequently, multiple rules may cover the same test sample, in
which case we can again use majority voting to determine its class. Region A in Figure 1 shows that
any test sample in that region will be classiﬁed with two overlapping rules. On the other hand, there
is also the risk that the entire feature space is not covered by the subset of rules. In Figure 1 such a
region is marked as B, where none of the rules cover this region. Therefore, if one decides to use
only the rules in ˆJ as a classiﬁer, then some of the samples in the test set may not be classiﬁed at
all. This is an anticipated behaviour, since MIRCO guarantees to cover only those samples that are
used to train the Random Forest algorithm. Our numerical results show that the percentages of test
samples that MIRCO fails to classify are quite low.

Notice that the entire set of rules used by MIRCO does not have to be constructed after training a
Random Forest algorithm. If there is an oracle that provides many rules to construct the set J , then
we can still (approximately) solve the mathematical programming model (1) and obtain the resulting
set of rules. In fact, the development of this oracle decouples our discussion from training a forest. In
the next section, we propose such an oracle that generates the necessary rules on the ﬂy.

4

x1x2llllllllll 2 1 1 3l123AB3 RCBoost: rule cover boosting

The main difﬁculty with the set covering problem is the integrality condition on the decision variables.
When this condition is ignored, the resulting problem is a simple linear programming problem that
can be solved quite efﬁciently. For the same reason, many approximation methods devised for the set
covering problem are based on considering the linear programming relaxation of (1). Formally, the
relaxed problem simply becomes:

minimize

subject to

(cid:80)

(cid:80)

j∈J (1 + wj)zj
j∈J (i) zj ≥ 1,

zj ≥ 0,

i ∈ I,

j ∈ J ,

(λi)

(3)

where λi, i ∈ I denote the dual variables corresponding to the constraints. After solving this problem,
we obtain both the primal and the dual optimal solutions. As we will discuss shortly, the optimal
dual variables bear important information about the sample points. Since we relax the integrality
restrictions, one suggestion could be rewriting the objective as solely the minimization of total
impurity. We refrain from such an approach because when there are many pure leaves, then the
corresponding rule does not contribute to the objective function value (wj = 0) and the corresponding
zj variable becomes free to attain any value. This implies that the primal problem has multiple
optimal solutions, and hence causing degeneracy in the dual.

We have seen in the previous section that problem (3) may easily have quite many variables (columns).
One standard approach to solve this type of linear programming problems is based on iterative
generation of only necessary variables that would lead to an improvement in the objective function
value. This approach is known as column generation. The main idea is to start with a set of columns
(column pool) and form a linear programming problem, called the restricted master problem (RMP).
After solving the restricted master problem, the dual optimal solution is obtained. Then using this
dual solution, a pricing subproblem is solved to ﬁnd the columns with negative reduced costs. These
columns are the only candidates for improving the objective function value when they are added to
the column pool. The next iteration continues with obtaining the optimal dual solution of the new
restricted master problem with the extended column pool. Again, the pricing subproblem is solved to
ﬁnd the columns with negative reduced costs. If there is no such column, then the column generation
algorithm terminates.

Suppose that we denote the column pool in the restricted master problem at iteration t of column
generation algorithm by Jt. Then, we solve the following linear programming problem:

minimize

subject to

(cid:80)

(cid:80)

(1 + wj)zj
j∈Jt
j∈Jt(i) zj ≥ 1,

zj ≥ 0,

i ∈ I,

j ∈ Jt,

(λt
i)

(RMP(Jt))

where λt
for the rules in Jt that cover sample i. The reduced cost of rule j ∈ Jt is simply given by

i denotes the dual variable corresponding to constraint i ∈ I at iteration t, and Jt(i) stands

¯wj = (1 + wj) −

(cid:88)

λt
i.

i∈I(j)

(4)

The necessary rules that would improve the objective function value in the next iteration are the ones
with negative reduced costs. In all this discussion, the key point is, in fact, the pricing subproblem,
which would generate the rules with negative reduced costs. Recall that the columns in this approach
are constructed by the rules in our setting. Thus, in the remaining part of this section, we shall use
the term “rule pool” instead of “column pool.”

Algorithm 2 shows the training process of the proposed rule cover boosting (RCBoost) algorithm.
The steps of the algorithm is also given in Figure 2. Here, DecisionTree routine plays the role of
the pricing subproblem. The parameter of this routine is a vector, which is used to assign different
weights to the samples. To obtain rules with negative reduced costs, the dual optimal solution of
the restricted master problems is passed as the sample weight vector (line 6). If there are rules with
negative reduced costs (line 7), then these are added to the rule pool J ∗ in line 11. Otherwise, the
algorithm terminates (line 9). Note that a bound on the maximum number of RMP calls can also be
considered as a hyperparameter. The resulting set or rules in the ﬁnal pool constitute the classiﬁer

5

Figure 2: Flowchart of RCBoost

(line 13). Like in Random Forest algorithm, a test point is classiﬁed using the majority voting system
among all the rules (if-then clauses) that are satisﬁed. Note that there is no danger of failing to
classify a test sample like MIRCO, since the initial rule pool is constructed with a decision tree (line
2). The output set of rules J ∗ is then used for classifying out-of-sample point with majority voting
as in (2). That is, C(x0, J ∗) is the predicted class for x0.

Algorithm 2: RCBoost

Input :Training data (xi, yi), i ∈ I

1 ¯λ = (1, 1, . . . , 1);
2 J ∗ ← DecisionTree(¯λ);
3 for t = 1, 2, . . . do
λt ← RMP(J ∗);
4
¯λ ← ¯λ + λt;
¯J ← DecisionTree(¯λ);
¯J − = {j ∈ ¯J : ¯wj < 0};
if ¯J − = ∅ then
break;

6

7

5

9

8

10

end
J ∗ ← J ∗ ∪ ¯J −;

11
12 end
13 Return J ∗;

As line 5 shows, we accumulate the dual vectors at every iteration. This is because many of the
dual variables quickly become zero as either the corresponding primal constraints are inactive or
the dual optimal solution becomes degenerate. So, we make sure with the accumulation that all
the sample weights remain nonzero. We should also note that calling the decision tree algorithm
with (cumulative) sample weights is a proxy to the pricing subproblem. Because, the actual pricing
subproblem is supposed to search for the rules with negative reduced costs. However, we argue
that using such a proxy is still sensible. We appeal to the theory of sensitivity analysis in linear
programming and the role of sample weights in boosting algorithms. An optimal dual variable
shows the change in the optimal objective function, when the right-hand side of this constraint (all
ones in our case) is slightly adjusted. Since each constraint corresponds to the samples in (1), the
corresponding optimal dual variables show us the role of these samples for changing the objective
function value. In fact the reduced cost calculation in (4) also bears the same intuition and promotes
having a rule with low impurity and large dual variables. Thus, if the samples with large dual values
(weights) belong to the same class, then they are more likely to be classiﬁed within the same leaf
(rule) to decrease the impurity. On the other hand, if these samples belong to different classes, then
putting them into separate leaves also yields a decrease in impurity. This is precisely what the sample
weights try to achieve for boosting methods, where the weights for the incorrectly classiﬁed samples

6

SolveDONEYESNOInitial Column PoolPricing Subproblemare increased. For instance, let wλ

j denote the weighted Gini impurity for leaf j ∈ J given by

wλ

j = 1 −

(cid:88)

k∈K

(cid:32) (cid:80)

i∈I(j)

1k(i) + (cid:80)
|I(j)| + (cid:80)

1k(i) (cid:80)t
(cid:80)t
s=1 λs
i

i∈I(j)

i∈I(j)

s=1 λs
i )

(cid:33)2

.

where |I(j)| denotes the cardinality of set I(j) and the indicator operator 1k(i) is one when sample
i belongs to class k; otherwise, it is zero. This weighted impurity is minimized whenever the samples
with large accumulated duals from the same class are classiﬁed within the same leaf. This choice is
likely to trigger generation of leaves with negative reduced costs, since the reduced cost value (4) is
also minimized when the same class of samples (lower Gini impurity) with large duals are covered
by rule j.

Using sample weights with decision trees also has a clear advantage from implementation point of
view. Almost all existing tools available for efﬁciently training decision trees admit weights for the
samples so that boosting algorithms, like the well-known AdaBoost [7], can make use of this feature.
Those algorithms increase the weights of misclassiﬁed samples in an iterative manner and obtain a
sequence of classiﬁers. Then, these classiﬁers are further weighed according to their classiﬁcation
performances. RCBoost, however, only produces a sequence of rules by increasing weights and each
rule is evaluated by its impurity contribution to the objective function through its reduced cost. Thus,
the reduced costs play the role of the partial derivatives like in gradient boosting algorithms. These
observations naturally beg for a discussion about the relationship between our algorithm RCBoost
and the other boosting algorithms. We reserve Section S.2 to elaborate on this point.

4 Numerical results

We next present our computational study with MIRCO and RCBoost algorithms on 15 datasets with
varying characteristics. We have implemented both algorithms in Python 4. The details of these
datasets are given in Table S.1. While most of these datasets correspond to binary classiﬁcation
problems, there are also problems with three classes (seeds and wine), six classes (glass) and
eight classes (ecoli). In the literature, mammography, diabetes, oilspill, phoneme, glass and
ecoli are considered as imbalanced datasets.

MIRCO is compared against Decision Tree (DT) and Random Forest (RF) algorithms in terms of
accuracy and the number of rules (interpretability). We also compare results of RCBoost (RCB)
against RF, AdaBoost (ADA) and Gradient Boosting (GB). For the latter two boosting methods,
we use decision trees as base estimators. We apply 10 × 4 nested stratiﬁed cross validation in
all experiments with the following options for hyperparameter tuning: maxdepth ∈ {5, 10, 20},
numberoftrees ∈ {10, 50, 100}. We also use numberofRMPcalls ∈ {5, 10, 50, 100, 200} option
to set a bound on the number of RMP calls.

We ﬁrst report our results with DT, RF and MIRCO algorithms in Figure 3. As the accuracy values
(average over 10 replications) in Figure 3a show, MIRCO performs on par with RF on most of the
datasets (see Table S.2 for the details). Recall that MIRCO is not guaranteed to classify all the
test samples. Thus, we also provide the fraction of the test samples missed by MIRCO (Figure
3b). However, we still classify such a sample by applying the rules that have the largest fraction of
accepted clauses. Therefore, the reported comparisons against RF and DT in Figure 3a are carried on
all samples. We observe that the fraction of the missed test samples is below 6% for all datasets but
one. This shows that MIRCO does not only approximate the RF in terms of accuracy, but also covers
a very high fraction of the feature space. Figure 3c depicts the numbers of rules in log-scale to give
an indication about the interpretability. In all but one dataset, MIRCO has generated less number of
rules than DT. Furthermore, the standard deviation ﬁgures in Table S.2 show that MIRCO shows less
variability than DT in all but two of the datasets.

To compare RCB, we present the average accuracy values of RF, ADA, GB in Figure 4 (see Table S.3
for the details). We also report the results with the initial Decision Tree (iniDT) used in RCB algorithm
(see line 2 in Algorithm 2) so that we can observe how rule generation improves performance.
The number in parentheses above each dataset shows the average number of RMP calls by RCB.
These results demosntrate that RCB exhibits a quite competitive performance. In particular, RCB

4(GitHub page) – https://github.com/sibirbil/RuleCovering

7

(b)

(c)

(a)

Figure 3: (a) Accuracy values for RF, DT, and MIRCO. (b) Average percentage of missed samples by
MIRCO. (c) Average number of rules (in logarithmic scale) generated by RF, DT, and MIRCO.

Figure 4: RCBoost results

outperforms both boosting algorithms, ADA and GB in four problems. One critical point here is
that RCB makes only a small number of RMP calls. In all but one datasets, RCB improves the
performance of the base decision tree, iniDT by using the proposed rule generation.

8

BANKNOTEILPDIONOSPHERETRANSFUSIONLIVERTICTACTOEWDBCMAMMOGRAPHYDIABETESOILSPILLPHONEMESEEDSWINEGLASSECOLIMIRCORandom ForestDecision TreeAccuracy Values0.00.20.40.60.81.0BANKNOTEILPDIONOSPHERETRANSFUSIONLIVERTICTACTOEWDBCMAMMOGRAPHYDIABETESOILSPILLPHONEMESEEDSWINEGLASSECOLIMissed Test Samples (%)02468BANKNOTEILPDIONOSPHERETRANSFUSIONLIVERTICTACTOEWDBCMAMMOGRAPHYDIABETESOILSPILLPHONEMESEEDSWINEGLASSECOLIDecision TreeRandom ForestMIRCOlog(Number of Rules)0246810BANKNOTEILPDIONOSPHERETRANSFUSIONLIVERTICTACTOEWDBCMAMMOGRAPHYDIABETESOILSPILLPHONEMESEEDSWINEGLASSECOLIRFADAGBiniDTRCBAccuracy Values(8.80)(26.00)(11.10)(6.20)(19.90)(13.00)(9.30)(27.90)(41.90)(10.10)(12.50)(4.30)(2.90)(9.80)(9.40)0.00.20.40.60.81.05 Conclusion

We have proposed two algorithms for interpretation and boosting. Both algorithms are based on
mathematical programming approaches for covering the rules in decision trees. The ﬁrst algorithm
is used for interpreting a model trained with the Random Forest algorithm. The second algorithm
focuses on the idea of generating necessary rules to boost an initial model trained with the Decision
Tree algorithm. Both algorithms are quite straightforward to implement within the existing packages.
Even with our ﬁrst implementation attempt, we have obtained promising results. The ﬁrst algorithm
has successfully extracted much less number of rules than the random forest model leading to better
interpretability. Moreover, we have observed that the accuracy levels obtained with the extracted
rules of our ﬁrst algorithm closely follow the accuracy levels of the trained random forest model.
When compared against other boosting algorithms, our second algorithm has shown a competitive
performance. As a new boosting algorithm, we have also provided a discussion about the relationship
between our approach and other boosting algorithms. The idea of selecting rules for interpretation or
boosting leads to a plethora of extensions that could be further studied. Therefore, we have prepared
a separate supplementary discussion on extensions in Section S.3. This section complements our
discussion as well as lists several future research paths.

9

References

[1] D. Bertsimas and J. Dunn. Optimal classiﬁcation trees. Machine Learning, 106(7):1039–1082, 2017.

[2] L. Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.

[3] V. Chvatal. A greedy heuristic for the set-covering problem. Mathematics of Operations Research, 4(3):

233–235, 1979.

[4] S. Dash, O. Gunluk, and D. Wei. Boolean decision rules via column generation. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information
Processing Systems, pages 4655–4665. 2018.

[5] A. Demiriz, K. P. Bennett, and J. Shawe-Taylor. Linear programming boosting via column generation.

Machine Learning, 46(1-3):225–254, 2002.

[6] M. Firat, G. Crognier, A. F. Gabor, C. Hurkens, and Y. Zhang. Column generation based heuristic for

learning classiﬁcation trees. Computers & Operations Research, 116:104866, 2020.

[7] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to

boosting. Journal of Computer and System Sciences, 55(1):119 – 139, 1997.

[8] J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5):

1189–1232, 2001.

[9] J. H. Friedman and B. E. Popescu. Predictive learning via rule ensembles. The Annals of Applied Statistics,

2(3):916–954, 2008.

[10] O. Günlük, J. Kalagnanam, M. Menickelly, and K. Scheinberg. Optimal decision trees for categorical data

via integer programming. arXiv preprint arXiv:1612.03225, 2018.

[11] M. Mashayekhi and R. Gras. Rule extraction from random forest: the rf+hc methods. In D. Barbosa and
E. Milios, editors, Advances in Artiﬁcial Intelligence, pages 223–237, Cham, 2015. Springer International
Publishing.

[12] M. Mashayekhi and R. Gras. Rule extraction from decision trees ensembles: New algorithms based on
heuristic search and sparse group lasso methods. International Journal of Information Technology &
Decision Making, 16(06):1707–1727, 2017.

[13] L. Mason, J. Baxter, P. Bartlett, and M. Frean. Boosting algorithms as gradient descent. In S. Solla, T. Leen,
and K. Müller, editors, Advances in Neural Information Processing Systems, pages 512–518. 1999.

[14] L. Mason, , P. L. Bartlett, and J. Baxter. Improved generalization through explicit optimization of margins.

Machine Learning, 38:243–255, 2000.

[15] N. Meinshausen. Node harvest. The Annals of Applied Statistics, 4(4):2049–2072, 2010.

[16] M. E. Pfetsch and S. Pokutta. IPBoost–non-convex boosting via integer programming. arXiv preprint

arXiv:2002.04679, 2020.

[17] R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. Boosting the margin: A new explanation for the

effectiveness of voting methods. The Annals of Statistics, 26(5):1651–1686, 1998.

[18] F. J. Vasko, Y. Lu, and K. Zyma. What is the best greedy-like heuristic for the weighted set covering

problem? Operations Research Letters, 44:366–369, 2016.

10

Supplementary Material for
“Rule Covering for Interpretation and Boosting”

Note that most of the cross references in this supplementary ﬁle refer to the original manuscript. Therefore,
clicking on those references may not take you to the desired location.

S.1 Chvatál’s greedy algorithm

We restate the well-known heuristic of Chvatál in our own notation. Recall that the sets I and J stand for the
set of samples and the set of rules, respectively. Algorithm S.1 gives the steps of the greedy heuristic, where
IR(j) represents those samples that are in R covered by rule j ∈ J . In line 3, the cost of selecting rule j is
1 + wj, and the operation |IR(j)| denotes the cardinality of the set IR(j). Removal of the redundant rules in
line 7 is done in two steps: First, the rules in ¯J are sorted in ascending order of their costs in a list. Then, the
rules at the end of this list are removed as long as the remaining rules cover all the samples.

Algorithm S.1: Greedy
Result: Set of rules ˆJ

1 R = I, ˆJ = ∅;
2 while R (cid:54)= ∅ do

3

4

j∗ = arg min
j∈J \ ˆJ
R ← R\IR(j∗);
ˆJ ← ˆJ ∪ j∗;

5
6 end
7 Remove redundant columns from ˆJ ;
8 Return ˆJ ;

(cid:110) 1+wj

(cid:111)
|IR(j)| : IR(j) (cid:54)= ∅
;

S.2 Relation to other boosting algorithms

The use of sample weights and reduced costs shows a similar trait among RCBoost and other boosting algorithms.
The reduced cost of rule j can be considered as the partial derivative of the objective function with respect to
the corresponding variable zj. Thus, rules that have negative reduced costs are added to the rule pool. This
resembles a descent iteration. To establish the relationship between RCBoost and other boosting methods from
the gradient descent point of view [13], we ﬁrst deﬁne the Lagrangean function

L(z, λ) =

(cid:0)1 + wj

(cid:1)zj +

(cid:88)

j∈J

(cid:88)

i∈I

(cid:0)1 −

λi

(cid:88)

(cid:1),

zj

j∈J (i)

where z and λ denote the primal and dual vectors. Suppose that the optimal solution pair for (RMP(Jt)) is given
by zt and λt. Then, we have

j∈Jt

j∈J /Jt

j∈J /Jt

¯wjzt

¯wjzt
j

i∈I λt
i∈I λt

j + (cid:80)
¯wjzc
j ,

L((zt, zc), λt) = (cid:80)
= (cid:80)

i + (cid:80)
i + (cid:80)
where ¯wj is the reduced cost of rule j as in (4) and the tuple (zt, zc) shows the variables in Jt and J /Jt,
respectively. The last equality is a direct consequence of optimality conditions, since zt and λt are comple-
mentary solution vectors. Thus, taking the steepest descent step for j ∈ J /Jt simply implies involving those
rules with negative reduced costs in the next iteration. From this perspective, RCBoost is similar to gradient
boosting algorithms. Nonetheless, unlike those algorithms RCBoost does not ﬁt a weak-classiﬁer but instead
obtains a larger set Jt+1 by adding a subset of rules with negative reduced costs (line 11 of Algorithm 2). Then,
solving the linear programming problem over Jt+1 yields the parameters zt+1 and λt+1. Suppose we denote
the optimal objective function of (RMP(Jt)) by Φ(Jt), which is the total impurity that we use to evaluate the
training loss of our classiﬁer. With each iteration we improve this loss, since Φ(Jt) ≥ Φ(Jt+1) due to our
linear programming formulation. If we further deﬁne the subset

∂Jt ⊆ {j ∈ J : ¯wj < 0},

11

then Φ(Jt+1) = Φ(Jt ∪ ∂Jt).

There has also been a focus on the margin maximization idea to discuss the generalization performance of
boosting methods [14, 17]. Here, one of the fundamental points is to observe that a sample with a large margin
is likely to be classiﬁed correctly. Thus, margin maximization is about assigning larger weights to those samples
with small margins. RCBoost also implicitly aims at obtaining large margins through selection of rules with less
impurities (classiﬁcation errors decrease). In other words, minimizing the objective function of the restricted
master problem promotes large margins. Formally, we can rewrite the optimal objective function value of
(RMP(Jt)) as

(cid:88)

(1 + wj)zt

j =

(cid:88)

(cid:88)

µjzt

j =

(cid:88)

λt
i,

j∈Jt

i∈I

j∈Jt(i)

i∈I

j∈Jt(i) zt

where µj = 1+wj
(cid:80)

|I(j)| , j ∈ J . Using complementary slackness condition, we know that λt

i > 0 only if
j is the convex combination of margins obtained with different rules for
sample i. The larger the obtained impurity, the smaller the corresponding margin. Like other boosting methods,
RCBoost also assigns larger weights to those samples with smaller margins.

j = 1. Thus, (cid:80)

j∈Jt(i) µjzt

S.3 Some extensions

We have used classiﬁcation for our presentation. Nonetheless, both proposed algorithms can also be adjusted to
solve regression problems. To interpret random forest regressors with MIRCO, the ﬁrst step is to use a proper
criterion, like mean squared error, for evaluating wj, j ∈ J values. Then, MIRCO can be used as it is given in
Algorithm 1. The same criteria can also be used in RCBoost to construct the restricted master problem. When a
regression problem is solved, one needs to pay attention to two potential caveats: (i) The two components of
the objective function coefﬁcients in optimization model (1) or (3) may not be balanced well, since the criteria
values wj, j ∈ J are not bounded above by one. This could be overcome by scaling the criteria values. If
scaling is done with a multiplier, then this multiplier becomes a new hyperparameter which may require tuning.
(ii) Regression trees tend to have too many leaves. Even after applying MIRCO, the cardinality of the resulting
set of rules ˆJ may be quite large, and hence, interpreting the random forest regressor may be difﬁcult. Similarly,
RCBoost may make too many calls to the restricted master problem and end up with a large number of rules in
the ﬁnal rule pool J ∗.

Many boosting algorithms use decision trees as their weak estimators. Thus, it seems that MIRCO can also
be used with those algorithms to interpret their results. However, one needs to pay attention to the fact that
boosting algorithms assign different weights to their weak estimators and MIRCO does not take these weights
into account. Therefore, if rule covering is used for interpretation of boosting algorithms, Algorithm 1 should be
adjusted to incorporate the estimator weights.

Recall that we use a decision tree with sample weights in RCBoost as a proxy to an actual pricing subproblem
(line 6, Algorithm 2). An alternative could be using the impurity and the negative reduced cost together while
constructing the decision tree. In that case, splitting can be done from non-dominated bi-criteria values; i.e.,
impurity and negative reduced cost values.

Clearly, we can also use a random forest or any ensemble of trees with sample weights as a proxy to the pricing
subproblem. Even further, one may even model a separate combinatorial problem to generate many different
rules. At this point, it is important to keep in mind that considering all possible rules with negative reduced costs
could prove to be a daunting job.

In its current stage, RCBoost adds all the rules with negative reduced costs to the rule pool. An alternative
approach could be to include only the rule with the most negative reduced cost. Along the same vein, we may
also remove those rules that have high (positive) reduced costs from the column pool. Actually, these are typical
approaches in column generation to keep the size of the column pool manageable. As the size of the rule pool
becomes large, RCBoost also becomes less interpretable. Fortunately, one can easily apply MIRCO or other rule
extraction approaches from the literature such as the RuleFit algorithm [9] to a trained RCBoost. In that case,
Algorithm 1 can be applied starting from line 5 with J ← J ∗. As a ﬁnal remark, we note that RCBoost treats
the rules equally when a test point is classiﬁed with the satisﬁed rules in J ∗. An alternative could be giving
different weights to different rules. These rule weights could be evaluated by making use of the optimal values
of zj, j ∈ J ∗ obtained from the ﬁnal restricted master problem. However, all these changes should be carefully
contemplated, since the resulting rule pool after shrinking may not necessarily cover the feature space as we
have discussed in Section 2 (see also Figure 1).

12

S.4 Details of numerical results

Table S.1: Characteristics of the datasets

Dataset

banknote
ILDP
ionosphere
transfusion
liver
tic-tac-toe
WDBC
mammography
diabetes
oilspill
phoneme
seeds
wine
glass
ecoli

Number of Number of Number of
Samples
Classes
Features

4
10
34
4
6
9
30
6
8
48
5
7
13
10
7

2
2
2
2
2
2
2
2
2
2
2
3
3
6
8

1372
583
351
748
345
958
569
11183
768
937
5427
210
178
214
336

Table S.2: MIRCO results

DT

RF

MIRCO

Dataset

Accuracy

Accuracy

Accuracy % of Missed Pts

banknote
ILDP
ionosphere
transfusion
liver
tic-tac-toe
WDBC
mammography
diabetes
oilspill
phoneme
seeds
wine
glass
ecoli
∗ Numbers in parantheses show the standard deviations

0.99 (0.02)
0.66 (0.07)
0.87 (0.07)
0.79 (0.04)
0.65 (0.09)
0.94 (0.02)
0.93 (0.03)
0.98 (0.00)
0.74 (0.04)
0.96 (0.02)
0.88 (0.01)
0.91 (0.06)
0.91 (0.07)
0.67 (0.05)
0.81 (0.09)

0.99 (0.01)
0.71 (0.06)
0.92 (0.03)
0.77 (0.04)
0.73 (0.06)
0.99 (0.01)
0.96 (0.02)
0.99 (0.00)
0.77 (0.04)
0.96 (0.01)
0.91 (0.01)
0.91 (0.09)
0.98 (0.03)
0.79 (0.07)
0.88 (0.06)

0.98 (0.01)
0.69 (0.03)
0.91 (0.04)
0.76 (0.01)
0.66 (0.06)
0.97 (0.02)
0.95 (0.03)
0.98 (0.00)
0.73 (0.04)
0.96 (0.01)
0.88 (0.02)
0.91 (0.05)
0.91 (0.07)
0.71 (0.07)
0.76 (0.08)

0.01 (0.01)
0.06 (0.04)
0.02 (0.03)
0.01 (0.01)
0.06 (0.05)
0.02 (0.02)
0.01 (0.02)
0.00 (0.00)
0.05 (0.04)
0.01 (0.01)
0.03 (0.01)
0.02 (0.03)
0.04 (0.04)
0.09 (0.06)
0.03 (0.03)

13

Table S.3: RCBoost results

Dataset

RF
Accuracy

ADA
Accuracy

GB
Accuracy

iniDT
Accuracy

RCB
Accuracy

banknote
ILDP
ionosphere
transfusion
liver
tic-tac-toe
WDBC
mammography
diabetes
oilspill
phoneme
seeds
wine
glass
ecoli
∗ Numbers in parantheses show the standard deviations

0.99 (0.01)
0.71 (0.06)
0.92 (0.03)
0.77 (0.04)
0.73 (0.06)
0.99 (0.01)
0.96 (0.02)
0.99 (0.00)
0.77 (0.04)
0.96 (0.01)
0.91 (0.01)
0.91 (0.09)
0.98 (0.03)
0.79 (0.07)
0.88 (0.06)

1.00 (0.00)
0.70 (0.06)
0.94 (0.05)
0.74 (0.04)
0.68 (0.07)
1.00 (0.01)
0.97 (0.03)
0.99 (0.00)
0.74 (0.04)
0.97 (0.01)
0.92 (0.01)
0.92 (0.07)
0.92 (0.08)
0.77 (0.08)
0.84 (0.07)

0.99 (0.01)
0.69 (0.03)
0.92 (0.04)
0.77 (0.03)
0.68 (0.06)
0.99 (0.01)
0.96 (0.03)
0.99 (0.00)
0.76 (0.04)
0.96 (0.02)
0.91 (0.01)
0.92 (0.07)
0.93 (0.05)
0.73 (0.10)
0.83 (0.06)

0.99 (0.01)
0.66 (0.07)
0.88 (0.03)
0.78 (0.03)
0.64 (0.10)
0.95 (0.02)
0.92 (0.03)
0.98 (0.00)
0.70 (0.06)
0.95 (0.02)
0.88 (0.01)
0.91 (0.09)
0.89 (0.07)
0.67 (0.11)
0.82 (0.09)

0.98 (0.01)
0.70 (0.04)
0.89 (0.05)
0.78 (0.04)
0.71 (0.05)
0.99 (0.01)
0.94 (0.02)
0.99 (0.00)
0.75 (0.03)
0.96 (0.02)
0.90 (0.01)
0.94 (0.06)
0.94 (0.04)
0.71 (0.09)
0.83 (0.05)

14

