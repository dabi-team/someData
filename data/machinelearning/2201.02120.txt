Treehouse: A Case For Carbon-Aware Datacenter Software

Thomas Anderson1, Adam Belay2, Mosharaf Chowdhury3, Asaf Cidon4, and Irene Zhang1,5
1University of Washington, 2MIT, 3University of Michigan, 4Columbia University, 5Microsoft Research

2
2
0
2

n
a
J

6

]

C
D
.
s
c
[

1
v
0
2
1
2
0
.
1
0
2
2
:
v
i
X
r
a

Abstract

The end of Dennard scaling and the slowing of Moore‚Äôs
Law has put the energy use of datacenters on an unsus-
tainable path. Datacenters are already a signiÔ¨Åcant frac-
tion of worldwide electricity use, with application de-
mand scaling at a rapid rate. We argue that substantial re-
ductions in the carbon intensity of datacenter computing
are possible with a software-centric approach: by making
energy and carbon visible to application developers on a
Ô¨Åne-grained basis, by modifying system APIs to make
it possible to make informed trade offs between perfor-
mance and carbon emissions, and by raising the level
of application programming to allow for Ô¨Çexible use of
more energy efÔ¨Åcient means of compute and storage. We
also lay out a research agenda for systems software to
reduce the carbon footprint of datacenter computing.

1 Introduction

The pressing need for society to address global cli-
mate change has caused many large organizations to
begin to track and report their aggregate greenhouse
gas emissions, both directly caused by their operations
and indirectly caused through energy use and by sup-
ply chains [5]. However, there are no standard software
mechanisms in place to track and and control emissions
from information technology (IT). This lack of visibil-
ity is especially acute where multiple applications share
the same physical hardware, such as datacenters, since
carbon emissions today can only be accounted for at the
server or processor chip level, not at the software and ap-
plication level.

In aggregate, datacenters represent a large and grow-
ing source of carbon emissions; estimates place dat-
acenters as responsible for 1-2% of aggregate world-
wide electricity consumption [31, 51]. Given rapidly-
increasing demand for computing and data analysis [46,
59], continual improvements are needed in the carbon
efÔ¨Åciency of computing to keep the climate impact of
computing from skyrocketing [31, 51, 52]. The end of
Dennard scaling means that exponential improvements
in energy efÔ¨Åciency are no longer an automatic conse-
quence of Moore‚Äôs Law. Over the past few years, various

Figure 1: Application demands for energy is growing faster
than energy efÔ¨Åciency improvements we can achieve. Tree-
house takes a software-centric approach to reduce this gap.

technologies have been introduced to improve matters‚Äî
for example, server consolidation and improvements in
power distribution. However, these steps will not be
enough going forward (Figure 1).

For cloud datacenter operators, a popular option is to
construct datacenters in locations with inexpensive, re-
newable power generation. Although a step forward, this
is unlikely to be a complete solution for several rea-
sons. First, hardware manufacturing, assembly and trans-
portation, as well as the construction and maintenance
of the datacenter itself, are all energy and greenhouse
gas intensive. In fact, chip manufacturing alone is re-
sponsible for about a third of the lifecycle greenhouse
gas emissions of a modern datacenter [27]. Second, edge
computing‚Äîplacing computing near customers‚Äîis in-
creasingly popular as a way to improve application re-
sponsiveness; these smaller scale datacenters are often
located in or near cities without access to dedicated green
power sources.1 Power is often much slower to provision
than other parts of IT; for example, provisioning inter-
state power lines to access remote green energy often re-
quires many years of advance planning. Finally, many
companies continue to operate their own on-premise dat-
acenters; any solution must work for those deployments
as well.

We propose Treehouse, a project whose goal is to

1For example, about a half acre of solar panels, plus batteries, are

needed to fully power a single 24x7 server rack [3].

1

 
 
 
 
 
 
build the foundations for a new software infrastructure
that treats energy and carbon as a Ô¨Årst-class resource,
alongside traditional computing resources like compute,
memory, and storage. Today, developers have almost no
way to know how their engineering decisions affect the
climate. The goal of Treehouse is to enable develop-
ers and operators to understand and reduce greenhouse
gases from datacenter sources. We target all datacenter
environments, including cloud, edge computing, and on-
premise environments.

We identify three new foundational abstractions nec-
essary to enable developers to optimize their carbon foot-
print: (1) energy provenance, a mechanism to track en-
ergy usage, (2) an interface for expressing applications‚Äô
service-level agreements (SLAs) to allow operators to
trade off performance and carbon consumption, and (3)
¬µfunctions, a fungible Ô¨Åne-grained unit of execution that
enables more efÔ¨Åcient hardware utilization. We also lay
out a research agenda to develop mechanisms for reduc-
ing carbon footprint by: (1) reducing software bloat, (2)
interchanging computational resources, (3) interchang-
ing memory resources, and (4) energy-aware scheduling
policies.

Beyond our direct research agenda, we hope our ef-
forts can inspire the broader software systems commu-
nity to focus much more on datacenter carbon reduction.

2 Foundations for Energy-Aware

Datacenter Software

Application developers today have few tools at their dis-
posal to write energy and carbon-efÔ¨Åcient applications.
First, they have no good way to account for the amount
of carbon their applications are emitting. In addition, it
is not clear what the carbon implications would be of
particular design choices (e.g., shifting their application
from a dedicated server to a shared server, or moving
their storage from disk to Ô¨Çash). While many cloud users
do optimize for lower cloud costs, cost does not equate
to energy usage. For example, while an HDD is much
cheaper to run than an SSD, it is far more energy in-
tensive. Similarly, for computationally intensive appli-
cations, FPGAs can often provide only a small integer
factor speedup relative to CPUs, but a factor of 10-70
improvement in energy efÔ¨Åciency [50].

Second, from the standpoint of the operator (e.g., the
cloud provider or a devops engineer in an on-premises
data center), even if they have some understanding of
the energy consumption of particular hardware resources
(e.g., servers), reducing the carbon footprint of a work-
load will often reduce performance. The datacenter op-
erator does not typically know when it would be appro-
priate to make that tradeoff, and so the common prac-
tice is to optimize infrastructure energy use only when

it would have negligible impact on performance, regard-
less of whether the performance matters to a particular
application.

Third, software applications today are typically provi-
sioned in a static set of bundled resources, which make
it difÔ¨Åcult to optimize for lower energy usage. For exam-
ple, virtual machines or containers typically come pre-
allocated with a set of CPU cores, memory capacity, and
network and disk bandwidth. As modern datacenter ap-
plications typically exhibit bursty and unpredictable pat-
terns at the microsecond-scale, this bundling of resources
causes applications to be inefÔ¨Åcient and energy-wasteful.
In this section, we introduce a set of abstractions that
we believe will lay the foundations for solving these
problems, to allow developers to track and optimize the
energy and carbon footprints of their applications.

2.1 Energy Provenance

In order to track and account for carbon emissions at the
software level, we need the ability to measure the en-
ergy provenance of each application. We use the term
provenance to denote both the direct and indirect energy
usage of a particular application. For example, an appli-
cation not only directly consumes energy when it is run-
ning user-level code, but it also consumes energy in the
operating system, in storage devices, and in the network
interface and switches along its path when it is commu-
nicating with a remote server, as well as the energy used
on its behalf at the remote server.

Since it is difÔ¨Åcult to directly measure the lifecycle en-
ergy provenance of individual applications through hard-
ware mechanisms alone, we believe it will be necessary
to construct a supervised machine learning model to es-
timate the energy provenance of the application, given
its resource usage. The input (or features) of the model
will be metrics that are easily measured in software, in-
cluding the network bandwidth (for switches and net-
work interface cards), bytes of storage and storage band-
width (for memory and persistent storage) and acceler-
ator cycles, as well as the type and topology of hard-
ware the application runs on. The model could be trained
and validated by carefully measuring in a lab environ-
ment how these performance metrics affect system-level
energy usage. Armed with accurate single-node energy
provenance estimates, we plan to annotate data center
communication, such as remote procedure calls (RPCs),
much as cloud providers annotate RPCs with debugging
information today [21]. These lifecycle per-application
energy estimates, combined with estimates of the carbon
intensity of power generation in each location, would
give developers the needed visibility into the impact of
their design decisions. This is a necessary Ô¨Årst step to
enlisting the developer community in achieving compu-
tational energy and carbon efÔ¨Åciency.

2

2.2 Exposing Application-Level SLAs

Another barrier to energy-efÔ¨Åcient computing is that op-
timizations that improve energy efÔ¨Åciency often hurt per-
formance. Disabling processor boost mode; moving less
frequently used data from high power memory to much
lower power non-volatile memory or solid-state storage;
turning off underutilized memory chips; moving compu-
tation from power-hungry general purpose processors to
more efÔ¨Åcient dedicated hardware accelerators; power-
ing down a fraction of the network core when it is not
needed‚Äîthese steps save energy but very often come at
the cost of worse system and application performance.

For application code, provided we address energy
provenance, the application developer can decide on
the right tradeoff that meets user performance expecta-
tions in the most energy-efÔ¨Åcient manner possible. These
types of optimizations are harder for systems code be-
cause it currently lacks any direct knowledge of applica-
tion intent. Traditionally, system designs have been eval-
uated in terms of response times and throughput, but to
achieve this designers have been willing to use all avail-
able resources regardless of the energy cost. Thus, while
these designs are often optimal for performance, they
sacriÔ¨Åce carbon efÔ¨Åciency.

To address this challenge, we aim to provide a way
for application developers to convey to systems code
their tolerance (and/or desire) for energy-saving opti-
mizations. This is the equivalent of eco-mode when driv-
ing a car. Together with provenance data to track the
energy impact of using different resources, the system
designer and operator can make informed choices as to
how to schedule and place workloads. Once system code
can optimize its behavior along the energy-performance
Pareto curve, application developers can make informed
choices to meet their users‚Äô carbon reduction goals.

To this end, we believe we need to develop a new
interface to expose application-level performance con-
straints (Service Level Agreements, or SLAs) to sys-
tems software. This will enable a new class of energy-
aware systems-level optimizations. For highly latency-
sensitive operations, it may still make sense to use the
highest-performance solutions, even at high energy cost.
But where there is available slack in user expectations,
we can use that Ô¨Çexibility to choose the most energy-
efÔ¨Åcient solution consistent with meeting user needs.

There is a large body of work on shifting long-running
batch jobs (e.g., MapReduce-style analytics) to a cleaner
sources of energy [15‚Äì17, 34, 35, 42, 47]. These type of
tasks are the extreme end of the SLA spectrum (depicted
in Figure 2), and typically operate at time scales of hours
or even days. This provides enough slack to shift them to
geographically-remote datacenters or to different times
of the day, to take advantage of spatial or temporal avail-
ability of clean energy (e.g., wind and solar). Our focus

Figure 2: Treehouse focuses on reducing the carbon footprint
for tasks with sub-second SLAs. Due to these latency con-
straints, the energy optimizations available are those within the
same datacenter. Prior work has considered relocating batch
jobs (e.g., analytics) to datacenters with greener sources of
power, or to greener periods of the day.

is on energy optimizations that can also apply to appli-
cations with much tighter SLAs, at the millisecond and
even single-digit microsecond scale. For these applica-
tions, it isn‚Äôt feasible to move the work to remote data-
centers or to periods of off-peak electricity generation.

2.3 ¬µfunctions

Despite the fact that many applications have highly dy-
namic resource usage, cloud applications today are often
provisioned for peak resource usage in coarse-grained
and static ways. For example, a virtual machine, con-
tainer, or even serverless compute engine will be provi-
sioned statically, with, say, 4 cores, 32 GB of memory,
etc., for seconds, minutes and hours at a time, while ap-
plication demand varies at much Ô¨Åner time-scales.

This leads to a high degree of resource stranding‚Äî
compute, memory, and storage that is only lightly uti-
lized, but which cannot be used for other applica-
tions. Although many hardware devices have low power
modes, these are only of partial beneÔ¨Åt. Even at low
load, power consumption is often half of the high load
case [11], in addition to the environmental impact of fab-
ricating devices that on average sit idle. Power efÔ¨Åciency
per unit of application work is maximized when system
software keeps resources fully utilized.

Further, the most energy efÔ¨Åcient option is to avoid
doing work that wasn‚Äôt needed in the Ô¨Årst place. Exist-
ing datacenter software stacks are bloated, with layers of
functionality added over time and kept for programmer
speed and convenience rather than refactored down to
their essential purpose. In the old era of Dennard scaling,
inefÔ¨Åcient layering could be addressed with time‚Äîevery
year, faster and more energy-efÔ¨Åcient computers would
become available to hide the impact of software bloat.
With the end of Dennard scaling, however, keeping old,
inefÔ¨Åcient software layers adds up.

We believe we need a new abstraction to address both
software bloat and resource stranding. First, we need
a lightweight way to provision resources at much Ô¨Åner
time scales, choosing the most energy efÔ¨Åcient option
that meets each application‚Äôs SLA. Second, to achieve

3

microseconds       milliseconds             seconds            minutes                 hours             days           Real-time workloads;OLTP;Streaming;AI/ML InferenceInteractive analytics;OLAP;Batch analytics;Background jobs;AI/ML TrainingOur focushigh utilization, we need to aggregate application re-
source demands more effectively.

A New Abstraction for Fungible Compute Modern
datacenter applications are distributed at extremely Ô¨Åne
granularities. For example, each user-facing HTTP re-
quest received by Facebook or Twitter spawn requests
to dozens of microservices that lead to thousands of in-
dividual RPCs to servers. As datacenter networks get
faster and in-memory microservices become more efÔ¨Å-
cient (e.g., by using kernel-bypass), datacenter servers
can increasingly process and respond to requests in mi-
croseconds [12].

To accommodate microsecond-scale datacenter appli-
cations, we need a new programming model with Ô¨Åne-
grained resource allocation and low provisioning over-
heads. It must be efÔ¨Åcient enough to make adjustments at
the microsecond-scale, so it can respond to sudden work-
load changes [29, 44]. ReÔ¨Çecting its microsecond scale,
we call this abstraction for general-purpose Ô¨Åne-grained
application provisioning microfunctions. Microfunctions
represent a large enough time scale to do useful work
(i.e., a few thousand cycles), while being Ô¨Åne-grained
enough to balance resource usage quickly as shifts in
load occur.

We plan to use an RPC-based API for ¬µfunctions,
including an interface for the user to deÔ¨Åne SLAs, as
well as an energy or carbon budget. We also see fore-
see opportunities to further increase efÔ¨Åciency through
computation shipping between ¬µfunctions, allowing us
to improve locality and reduce data movement [36,
61]. Dynamically deciding when to move data or com-
putation will also enable new efÔ¨Åciency vs. perfor-
mance tradeoffs. Building upon the recent trend to-
ward microservices, we envision that full applications
can be constructed by partitioning their components
into Ô¨Åne-grained units and running them as independent
¬µfunctions.

FaaS (Function-as-a-Service) or serverless frame-
works, such as AWS Lambda [1] share a similar no-
tion by allowing developers to express their jobs and
get billed at the granularity of individual function in-
vocations. However, FaaS still operates on top of stat-
ically allocated resource containers, making it difÔ¨Åcult
to bin pack the right combination of functions‚Äîin the
face of variable resource usage‚Äîto achieve high utiliza-
tion. Some cloud providers compensate by overcommit-
ting functions to containers, but this leads to inconsistent
per-function performance [60]. In addition, FaaS suffers
from software bloat and high function startup times. The
‚Äúcold start‚Äù problem, in particular, can cause FaaS to
take hundreds of milliseconds or more to invoke a func-
tion [55]. This timescale is many orders of magnitude too
coarse to achieve balance during Ô¨Åne-grained shifts in re-
source demand, while incurring far higher energy over-

head than is necessary. Finally, FaaS is only designed
to operate on a speciÔ¨Åc type of compute and memory
(namely, CPU and DRAM), and cannot take advantage of
more energy efÔ¨Åcient options such as accelerators (e.g.,
GPUs, FPGAs, NICs) and heterogeneous forms of mem-
ory (e.g., persistent memory).

Our goal for ¬µfunctions is to provide a lightweight
function abstraction, which is decoupled from any static
grouping of resources, such as a container or a VM.
Instead, we aim to make ¬µfunctions completely fungi-
ble, consuming resources on-demand as they are needed,
with the ability to run on heterogeneous computing re-
sources.

Micro-Second Scale Performance In order to exploit
Ô¨Åne-grained variations in resource usage and concur-
rency, we plan to support microsecond-scale invoca-
tions of ¬µfunctions, an improvement of several orders
of magnitude over existing serverless systems. We must
tackle two research challenges to spawn ¬µfunctions this
quickly.

First, the cold start problem must be addressed to
speed up invocations on machines that have not recently
executed a particular function. One barrier is the high ini-
tialization cost of existing isolation mechanisms. For ex-
ample, even after sophisticated optimizations, Amazon‚Äôs
Firecracker still requires at least 125 milliseconds to start
executing a function environment [6].

Second, we must ensure that ¬µfunction invocations
themselves can start extremely quickly. A major barrier
to fast function invocation in existing FaaS systems is
that they rely on inefÔ¨Åcient RPC protocols built on top of
HTTP. In addition, existing FaaS systems require a com-
plex tier of dedicated load balancing servers [6], which
leads to signiÔ¨Åcant delays.

Resource Disaggregation Resource disaggregation
poses a solution to the Ô¨Åxed bundling of resources
(in servers, virtual machines or containers). While
microsecond resource allocation helps to minimize the
resources stranded by overprovisioning,
it does not
solve the problem of bin packing application resource
allocations onto servers, leaving some resources still
stranded. Disaggregating resources reduces resource
stranding at the cost of added latency. For applications
whose SLAs are designed to tolerate slightly longer
latencies, disaggregation enables the system to allocate
exactly the amount of compute, memory and storage
each application requires at the moment, from a shared
pool. This allows idle resources to be powered off to
save energy without compromising application-level
SLAs.

There has been some progress on disaggregating re-
sources, particularly on disaggregating storage [9,10,23].
However, some resources, such as memory and CPU, are

4

3 Research Agenda

We now describe a speciÔ¨Åc agenda that builds upon the
Treehouse foundational abstractions to reduce datacen-
ter energy consumption, by allowing software systems to
make energy-aware decisions.

3.1 Minimizing Software Bloat

InefÔ¨Åcient software layers can be found in system-level
building blocks shared across applications, including
data movement, data (un)marshalling, memory alloca-
tion, and remote procedure call handling. In a cluster-
wide proÔ¨Åling study at Google, it was found that these
common building blocks consume about 30% of all cy-
cles; the Linux kernel, including thread scheduling and
network packet processing, consumes an additional 20%
of all cycles [32]. In other words, shared software infras-
tructure is signiÔ¨Åcant enough to account for almost half
of all CPU cycles available in a typical datacenter.

We propose two steps to address software bloat. The
Ô¨Årst step is to continue optimizing the many layers of the
IT software stack that we have inherited. Many of these
layers were designed for systems where I/O took mil-
liseconds to complete. We need a fundamental redesign
of the software stack for fast I/O (networking and stor-
age) devices.

One direction is to use Linux as a control plane for
backward compatibility, but allow applications efÔ¨Åcient
direct access to I/O [13, 53]. Widely-used bypass tech-
nologies include RDMA and DPDK [58] for network
bypass as well as Optane and SPDK [28] for storage by-
pass. Although more work is needed to understand how
best to integrate these technologies with the kernel, stud-
ies have shown that operating system overheads can be
slashed while still providing traditional kernel functions
such as centralized scheduling, Ô¨Åle system semantics,
and performance isolation [37,49,62]. A complementary
approach is to move user-deÔ¨Åned functions written in a
type-safe language into the Linux kernel, to allow cus-
tomization closer to the hardware [18, 24, 48, 64].

A longer-term solution is to ofÔ¨Çoad parts of the data
path to more powerful and lower-energy I/O hardware.
For example, both Amazon and Microsoft Azure ofÔ¨Çoad
to hardware the packet re-writing needed for cloud virtu-
alization [20, 33]. This minimizes the energy cost of the
added abstraction. We need to extend this approach to
other layers of the systems stack to truly reduce the soft-
ware energy drain from management systems. For ex-
ample, we are designing an open-source, reconÔ¨Ågurable
hardware networking stack to reduce energy use of fre-
quently used operating system and runtime functions.

Ultimately, we believe we will need a new energy-
optimized operating system kernel and runtime system
for datacenters architected to take advantage of energy-

Figure 3: Depiction of the Treehouse scheduler. The scheduler
takes as its input the energy provenance of each function, the
state of the different hardware resources as well as the func-
tion‚Äôs SLA. It then schedules the functions in the most energy-
efÔ¨Åcient way, while still meeting their SLAs, across the differ-
ent clusters of disaggregated resources.

still primarily consumed locally on monolithic servers.
While there is a large body of research on trying to dis-
aggregate these resources [7, 8, 22, 26, 38, 54, 56], signiÔ¨Å-
cant challenges remain for real-world adoption, includ-
ing: security [57], isolation [63], synchronization [43]
and fault tolerance [40]. These challenges are exacer-
bated especially in low-latency (i.e., microsecond-scale)
settings that are our focus.

Design Questions A key design question is whether
to build ¬µfunctions on top of Linux, and whether
¬µfunctions need to be able to support POSIX. While
running ¬µfunctions on top of Linux may make it eas-
ier for existing applications to transition to ¬µfunctions,
it comes at a high cost. In particular, Linux adds signif-
icant overhead to I/O operations, and it is not the natu-
ral interface for writing a distributed application across
disaggregated resources. We plan to pursue in parallel
both research directions: (a) incrementally adapt Linux
to be more lightweight, as well as (b) pursue a clean-
slate non-POSIX OS design. We describe these efforts in
Section 3.1.

2.4 Summary

To conclude, our three foundational abstractions would
allow developers to deÔ¨Åne ¬µfunctions that can operate
on fungible resources at microsecond time-scales. Devel-
opers would deÔ¨Åne SLAs for these ¬µfunctions, allowing
the cloud operator to navigate the energy-performance
Pareto curve. Finally, the energy provenance of these
¬µfunctions would be tracked and accounted for at all
times.

This process is depicted in Figure 3, where the Tree-
house scheduler (described in ¬ß3.4) collects as input the
energy provenance and the SLA of the ¬µfunctions, and
schedules them on the resource at a time that would still
meet their SLA while minimizing overall energy usage.

5

ùúáfunction schedulerApplication SLA andperformance metricsCPUCPUCPUCPUCPUCPUCPUFPGACPUCPUCPUDRAMCPUCPUCPUNVMEnergy provenanceDisaggregated computeDisaggregated memory/storageSchedule memory ùúáfunctionSchedule compute ùúáfunction 3.3

Interchangeable Memory

Similar to interchangeable compute devices, DRAM,
NVRAM, SSD, and HDD can all be interchanged to
some degree: while DRAM is volatile, in many use cases
non-volatility is not a strict requirement. Each offers a
different operating point in the tradeoff between energy
efÔ¨Åciency and tail latency, as shown schematically in Fig-
ure 4. Even within a particular technology, there are of-
ten energy tradeoffs, such as in the choice between single
and multi-level cell encodings on SSDs.

Another trend is towards microsecond-scale networks,
such as CXL and RDMA. This can allow memory re-
sources to be more effectively disaggregated, reduc-
ing both the cost and energy waste of resource strand-
ing. Combined with high-performance storage technolo-
gies, such as 3D XPoint (e.g., Intel Optane SSD [2])
or SLC NAND (e.g., Samsung Z-SSD [4]) which offer
microsecond-scale access times, signiÔ¨Åcant amounts of
energy (and carbon) can be saved by shifting data that is
currently stored on DRAM to lower-power nearby stor-
age.

We propose to design a general-purpose system that
interchanges memory for lower power storage, without
affecting the application‚Äôs SLAs while staying within
an energy budget. Such a system would need to auto-
matically identify which data should sit in DRAM, and
which part in storage, based on the ¬µfunction‚Äôs timeli-
ness constraint, its read and write access patterns, and its
access granularity. In addition, we can also employ intel-
ligent caching and prefetching to mask reduced DRAM
use [45].

3.4 Energy-Aware Scheduling

So far, we have separately considered interchangeable
compute and interchangeable memory resources. For
the most part, we have also assumed that the total en-
ergy consumption is given as a constraint for those op-
timizations. However, any realistic application requires
both computation and storage. We need to consider how
to Ô¨Ånd the Pareto frontier of an application‚Äôs energy-
performance curve by co-optimizing both sets of inter-
changeable resources in a disaggregated environment,
while taking energy sources and ¬µfunction SLAs into ac-
count.

Given that a ¬µfunction can run on multiple inter-
changeable compute devices and the computation de-
vice may have choices to use one of the many storage
mediums, one direction would be extending well-known
multi-commodity Ô¨Çow-based resource allocation formu-
lations [19, 30, 39] for determining the best combination
of interchangeable resources to use. Figure 5 gives a sim-
ple example. There are ¬µfunctions from three applica-
tions: A1, A2, and A3 (three commodities with different
colors), each of which can run on one of the Ô¨Åve compute

Figure 4: Pareto frontier of energy usage-vs-tail latency for in-
terchangeable memory options. For example, DRAM and SSD
are located at the opposite corners in this tradeoff space, but the
relationships between alternatives may not always be linear.

efÔ¨Åcient hardware acceleration. This may be either as a
clean-slate design or by incrementally replacing parts of
the Linux kernel [41]. By raising the level of abstraction
from POSIX to ¬µfunctions, we make it easier to support
these more radical designs.

3.2

Interchangeable Compute

Datacenter applications are often designed to take advan-
tage of a speciÔ¨Åc type of compute engine. Traditional ap-
plications typically assume they are running on CPUs,
while many machine learning applications rely on accel-
erators like GPUs, TPUs, and FPGAs, with new options
emerging every month. In many cases, an application‚Äôs
energy consumption can be signiÔ¨Åcantly reduced, while
still meeting its SLA, if the application used a different
less energy-intensive computing resource.

For example, FPGAs are often much more energy efÔ¨Å-
cient than CPUs on the same computation. However, for
highly dynamic workloads with tight timing limits, CPUs
are often used instead because they can be quickly con-
Ô¨Ågured and/or reallocated as demand changes. We be-
lieve we can obtain the best of both worlds by making it
possible to run ¬µfunctions in hybrid mode‚Äîusing CPUs
to meet transient and short-term bursts with FPGAs used
to meet the more stable and predictable portion of the
workload. Because FPGAs, like CPUs, are at their peak
energy efÔ¨Åciency at full utilization, this means transpar-
ently scaling FPGAs up and down much like we do today
for CPUs. To reduce engineering costs of maintaining
multiple implementations, we aim to develop an inter-
mediate representation (IR) that can be converted to run
on a broad spectrum of accelerators (e.g., similar to what
TVM [14] does for machine learning); cloud customers
will then be able to tradeoff between agility and energy
efÔ¨Åciency as they see Ô¨Åt.

6

4 Conclusion
The end of Dennard scaling and the slowing of Moore‚Äôs
Law has led to an inÔ¨Çection point with respect to the
impact the computing industry on the world‚Äôs ecology.
Computing is still a small fraction of global energy use,
but we can no longer count on automatic advances in
the energy-efÔ¨Åciency of computing to compensate for the
rapid upward spiral in computing demand. To continue to
reap the beneÔ¨Åts of computing without endangering the
planet, we need to treat energy efÔ¨Åciency as a Ô¨Årst class
design goal. The Treehouse project aims to address this
challenge by building tools that help application develop-
ers understand the implications of their design decisions
on energy and carbon use, by adapting interfaces to make
timeliness requirements explicit to allow for informed
system-level tradeoffs of energy versus time, and by re-
ducing the energy cost of commonly used abstractions.
More broadly, we believe that the systems software re-
search community can and must play a constructive role
in reducing the impact of computing on the planet, as we
make the transition to abundant carbon-free energy over
the next few decades.

Acknowledgments
We would like to thank Simon Peter for suggesting Fig-
ure 1. This work is supported by grants from the National
Science Foundation (2104243, 2104292, 2104398, and
2104548), VMware, and Cisco Systems.

References
[1] AWS Lambda. https://aws.amazon.com/

lambda/.

[2] Intel Optane SSD 9 Series.

https:

//www.intel.com/content/www/
us/en/products/memory-storage/
solid-state-drives/consumer-ssds/
optane-ssd-9-series.html.

[3] National renewable energy laboratory: Land use
by system technology. https://www.nrel.
gov/analysis/tech-size.html.

[4] Samsung Z-SSD.

https://www.samsung.

com/semiconductor/ssd/z-ssd/.

[5] Greenhouse gas corporate accounting and report-
ing standard. https://ghgprotocol.org/
corporate-standard, 2021.

[6] A. Agache, M. Brooker, A. Iordache, A. Liguori,
R. Neugebauer, P. Piwonka, and D. Popa. Fire-
cracker: Lightweight virtualization for serverless
In USENIX NSDI, pages 419‚Äì434,
applications.
2020.

Figure 5: An example multi-commodity Ô¨Çow-based formula-
tion for cost-performance optimization at time t.

resources (R1 . . . R5) with different ‚Äì already-proÔ¨Åled
and known ‚Äì speedups. At time t, each ¬µfunction can
read and write pertinent data (e.g.,, A1 needs three ob-
jects A11‚ÄìA13) from/to two interchangeable storage de-
vices (the availability of data for reading can be captured
by the presence/absence of edges between a compute de-
vice and corresponding data in that storage medium).
Now one can represent the problem of optimizing for
total energy consumption for these simple ¬µfunctions
as the sum of all edge costs for each ¬µfunction (with
appropriate constraints to avoid oversubscribing each
compute device) ‚Äì minimizing the total cost across all
¬µfunctions will ensure that the overall energy consump-
tion is minimized. By appropriately setting the costs
of the edges and objective functions, we can consider
trading off energy consumption for application perfor-
mance and vice versa. The primary challenge of such
optimization-based approaches is the speed at which we
can determine placements‚Äîa few microseconds may not
be enough. Approximation- and/or memoization-based
are more likely to succeed.

What we highlighted so far deals only with assign-
ments of ¬µfunctions to interchangeable compute and
memory/storage at a particular time instant. However,
one-shot device assignment is just the beginning of the
scheduling problem; we must also schedule ¬µfunctions
over time without violating their SLAs. The key here
will likely be to take advantage of deadline-based and
altruistic scheduling solutions [25] to effectively lever-
age available slack. We can consider dividing time
into Ô¨Åxed-length scheduling windows, pack ¬µfunctions
with smaller slack within the current window, and push
¬µfunctions with larger slack into future windows. This
would maximize our ability to convert application Ô¨Çexi-
bility over timeliness into lower energy and carbon use.

7

A1A2A3R1R2R3R4R5A11A12A21A13A31DRAMA11A12A21A13A31SSDInterchangeableCompute[7] M. K. Aguilera, N. Amit, I. Calciu, X. Deguillard,
J. Gandhi, S. Novakovi¬¥c, A. Ramanathan, P. Sub-
rahmanyam, L. Suresh, K. Tati, R. Venkatasubra-
manian, and M. Wei. Remote Regions: A simple
abstraction for remote memory. In USENIX ATC,
2018.

[8] E. Amaro, C. Branner-Augmon, Z. Luo, A. Ouster-
hout, M. K. Aguilera, A. Panda, S. Ratnasamy, and
S. Shenker. Can far memory improve job through-
put? In ACM EuroSys, 2020.

[9] Amazon. Amazon Elastic Block Store. https:

//aws.amazon.com/ebs/.

[10] Amazon. Amazon Web Services. https://

aws.amazon.com/s3/.

[11] L. A. Barroso and U. H¬®olzle. The case for energy-
proportional computing. Computer, 40(12):33‚Äì37,
2007.

[12] L. A. Barroso, M. Marty, D. A. Patterson, and
P. Ranganathan. Attack of the killer microseconds.
Commun. ACM, 60(4):48‚Äì54, 2017.

[13] A. Belay, G. Prekas, A. Klimovic, S. Grossman,
C. Kozyrakis, and E. Bugnion.
IX: A protected
dataplane operating system for high throughput and
low latency. In USENIX OSDI, pages 49‚Äì65, 2014.

[14] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan,
H. Shen, M. Cowan, L. Wang, Y. Hu, L. Ceze, et al.
TVM: An automated end-to-end optimizing com-
piler for deep learning. In OSDI, pages 578‚Äì594,
2018.

[15] Y. Chen, S. Alspaugh, D. Borthakur, and R. Katz.
Energy efÔ¨Åciency for large-scale MapReduce work-
loads with signiÔ¨Åcant interactive analysis. In ACM
EuroSys, pages 43‚Äì56, 2012.

[16] Y. Chen, A. Ganapathi, and R. H. Katz. To com-
press or not to compress-compute vs. io tradeoffs
In ACM SIG-
for mapreduce energy efÔ¨Åciency.
COMM Workshop on Green networking, pages 23‚Äì
28, 2010.

[17] D. Cheng, P. Lama, C. Jiang, and X. Zhou. Towards
energy efÔ¨Åciency in heterogeneous Hadoop clus-
ters by adaptive task assignment. In IEEE ICDCS,
pages 359‚Äì368, 2015.

[18] P. Enberg, A. Rao, and S. Tarkoma. Partition-aware
packet steering using XDP and eBPF for improv-
In Proceedings
ing application-level parallelism.
of the 1st ACM CoNEXT Workshop on Emerging

in-Network Computing Paradigms, pages 27‚Äì33,
2019.

[19] S. Even, A. Itai, and A. Shamir. On the complexity
of time table and multi-commodity Ô¨Çow problems.
In 16th Annual Symposium on Foundations of Com-
puter Science, pages 184‚Äì193. IEEE, 1975.

[20] D. Firestone, A. Putnam, S. Mundkur, D. Chiou,
A. Dabagh, M. Andrewartha, H. Angepat,
V. Bhanu, A. CaulÔ¨Åeld, E. Chung, H. K. Chan-
drappa, S. Chaturmohta, M. Humphrey, J. Lavier,
N. Lam, F. Liu, K. Ovtcharov, J. Padhye, G. Pop-
uri, S. Raindel, T. Sapre, M. Shaw, G. Silva,
M. Sivakumar, N. Srivastava, A. Verma, Q. Zuhair,
D. Bansal, D. Burger, K. Vaid, D. A. Maltz, and
A. Greenberg.
Azure accelerated networking:
SmartNICs in the public cloud. In USENIX NSDI,
2018.

[21] R. Fonseca, G. Porter, R. H. Katz, and S. Shenker.
X-trace: A pervasive network tracing framework.
In USENIX NSDI, 2007.

[22] P. X. Gao, A. Narayan, S. Karandikar, J. Carreira,
S. Han, R. Agarwal, S. Ratnasamy, and S. Shenker.
Network requirements for resource disaggregation.
In OSDI, 2016.

[23] Y. Gao, Q. Li, L. Tang, Y. Xi, P. Zhang, W. Peng,
B. Li, Y. Wu, S. Liu, L. Yan, F. Feng, Y. Zhuang,
F. Liu, P. Liu, X. Liu, Z. Wu, J. Wu, Z. Cao, C. Tian,
J. Wu, J. Zhu, H. Wang, D. Cai, and J. Wu. When
cloud storage meets RDMA. In NSDI, 2021.

[24] Y. Ghigoff, J. Sopena, K. Lazri, A. Blin, and
G. Muller. Bmc: Accelerating memcached using
safe in-kernel caching and pre-stack processing. In
USENIX NSDI, pages 487‚Äì501, 2021.

[25] R. Grandl, M. Chowdhury, A. Akella, and G. Anan-
thanarayanan.
Altruistic scheduling in multi-
resource clusters. In USENIX OSDI, pages 65‚Äì80,
2016.

[26] J. Gu, Y. Lee, Y. Zhang, M. Chowdhury, and K. G.
Shin. EfÔ¨Åcient memory disaggregation with InÔ¨Ån-
iswap. In NSDI, 2017.

[27] U. Gupta, Y. G. Kim, S. Lee, J. Tse, H.-H. S. Lee,
G.-Y. Wei, D. Brooks, and C.-J. Wu. Chasing car-
bon: The elusive environmental footprint of com-
puting, 2020.

[28] Intel Corporation. Storage performance develop-

ment kit. http://www.spdk.io.

8

[29] C. Iorgulescu, R. Azimi, Y. Kwon, S. Elnikety,
M. Syamala, V. R. Narasayya, H. Herodotou,
P. Tomita, A. Chen, J. Zhang, and J. Wang. PerfIso:
Performance isolation for commercial
latency-
sensitive services. In H. S. Gunawi and B. Reed,
editors, USENIX ATC, pages 519‚Äì532, 2018.

[30] M. Isard, V. Prabhakaran, J. Currey, U. Wieder,
K. Talwar, and A. Goldberg. Quincy: fair schedul-
In ACM
ing for distributed computing clusters.
SOSP, pages 261‚Äì276, 2009.

[31] N. Jones. How to stop data centres from gobbling
up the world‚Äôs electricity. Nature, 561(7722):163‚Äì
167, 2018.

[40] Y. Lee, H. A. Maruf, M. Chowdhury, A. Cidon, and
K. G. Shin. Mitigating the performance-efÔ¨Åciency
tradeoff in resilient memory disaggregation. CoRR,
abs/1910.09727, Oct 2020.

[41] J. Li, S. Miller, D. Zhuo, A. Chen, J. Howell, and
T. Anderson. An incremental path towards a safer
In Proceedings of the Workshop on
os kernel.
Hot Topics in Operating Systems, HotOS ‚Äô21, page
183‚Äì190, 2021.

[42] Z. Liu, M. Lin, A. Wierman, S. H. Low, and L. L.
Andrew. Greening geographical load balancing.
ACM SIGMETRICS Performance Evaluation Re-
view, 39(1):193‚Äì204, 2011.

[32] S. Kanev, J. P. Darago, K. M. Hazelwood, P. Ran-
ganathan, T. Moseley, G. Wei, and D. M. Brooks.
In D. T.
ProÔ¨Åling a warehouse-scale computer.
Marr and D. H. Albonesi, editors, ACM ISCA,
pages 158‚Äì169, 2015.

[43] T. Ma, M. Zhang, K. Chen, Z. Song, Y. Wu, and
X. Qian. AsymNVM: An efÔ¨Åcient framework for
implementing persistent data structures on asym-
metric nvm architecture. In ACM ASPLOS, pages
757‚Äì773, 2020.

[33] A. Kaufmann, S. Peter, N. K. Sharma, T. Anderson,
and A. Krishnamurthy. High performance packet
processing with FlexNIC. In ACM ASPLOS, pages
67‚Äì81, 2016.

[34] K. Kim, F. Yang, V. M. Zavala, and A. A. Chien.
Data centers as dispatchable loads to harness
stranded power. IEEE Transactions on Sustainable
Energy, 8(1):208‚Äì218, 2016.

[35] A. Krioukov, C. Goebel, S. Alspaugh, Y. Chen,
D. E. Culler, and R. H. Katz. Integrating renewable
energy using data analytics systems: Challenges
IEEE Data Engineering Bul-
and opportunities.
letin, 34(1):3‚Äì11, 2011.

[36] C. Kulkarni, S. Moore, M. Naqvi, T. Zhang,
R. Ricci, and R. Stutsman. Splinter: Bare-metal ex-
In
tensions for multi-tenant low-latency storage.
A. C. Arpaci-Dusseau and G. Voelker, editors,
USENIX OSDI, pages 627‚Äì643, 2018.

[37] Y. Kwon, H. Fingler, T. Hunt, S. Peter, E. Witchel,
and T. Anderson. Strata: A cross media Ô¨Åle system.
In ACM SOSP, Oct. 2017.

[38] A. Lagar-Cavilla, J. Ahn, S. Souhlal, N. Agarwal,
R. Burny, S. Butt, J. Chang, A. Chaugule, N. Deng,
J. Shahid, G. Thelen, K. A. Yurtsever, Y. Zhao, and
P. Ranganathan. Software-deÔ¨Åned far memory in
warehouse-scale computers. In ASPLOS, 2019.

[39] T. N. Le, X. Sun, M. Chowdhury, and Z. Liu. Al-
loX: Compute allocation in hybrid clusters. In ACM
EuroSys, pages 31:1‚Äì31:16, 2020.

[44] M. Marty, M. de Kruijf, J. Adriaens, C. Alfeld,
S. Bauer, C. Contavalli, M. Dalton, N. Dukkipati,
W. C. Evans, S. Gribble, N. Kidd, R. Kononov,
G. Kumar, C. Mauer, E. Musick, L. E. Olson,
E. Rubow, M. Ryan, K. Springborn, P. Turner,
V. Valancius, X. Wang, and A. Vahdat. Snap: a mi-
In ACM
crokernel approach to host networking.
SOSP, pages 399‚Äì413, 2019.

[45] H. A. Maruf and M. Chowdhury.

Prefetching Remote Memory with Leap.
USENIX ATC, 2020.

Effectively
In

[46] E. Masanet, A. Shehabi, N. Lei, S. Smith, and
Recalibrating global data center
Science, 367(6481):984‚Äì

J. Koomey.
energy-use estimates.
986, 2020.

[47] L. Mashayekhy, M. M. Nejad, D. Grosu, Q. Zhang,
and W. Shi. Energy-aware scheduling of mapre-
IEEE
duce jobs for big data applications.
transactions on Parallel and distributed systems,
26(10):2720‚Äì2733, 2014.

[48] S. Miller, K. Zhang, M. Chen, R. Jennings,
A. Chen, D. Zhuo, and T. Anderson. High velocity
kernel Ô¨Åle systems with Bento. In USENIX FAST,
pages 65‚Äì79, Feb. 2021.

[49] A. Ousterhout, J. Fried, J. Behrens, A. Belay, and
H. Balakrishnan. Shenango: Achieving high CPU
efÔ¨Åciency for latency-sensitive datacenter work-
loads. In USENIX NSDI, pages 361‚Äì378, 2019.

[50] P. Patel, K. Lim, A. Martinez, T. Anderson, J. Nel-
son, and I. Zhang. Fungible computing as a service.
https://treehouse-research.github.io/.

9

[51] F. Pearce. Energy hogs: can world‚Äôs huge data cen-
ters be made more efÔ¨Åcient? Yale Environment,
360, 2018.

[52] M. Pesce. Cloud Computing‚Äôs Coming Energy Cri-

sis. IEEE Spectrum, 2021.

[53] S. Peter, J. Li, I. Zhang, D. R. K. Ports, D. Woos,
A. Krishnamurthy, T. Anderson, and T. Roscoe. Ar-
rakis: The operating system is the control plane. In
USENIX OSDI, pages 1‚Äì16, 2014.

[54] Z. Ruan, M. Schwarzkopf, M. K. Aguilera, and
A. Belay. AIFM: High-performance, application-
In USENIX OSDI, pages
integrated far memory.
315‚Äì332, Nov. 2020.

[55] M. Shahrad, R. Fonseca, ¬¥I. Goiri, G. Chaudhry,
P. Batum, J. Cooke, E. Laureano, C. Tresness,
M. Russinovich, and R. Bianchini. Serverless in the
wild: Characterizing and optimizing the serverless
workload at a large cloud provider. arXiv preprint
arXiv:2003.03423, 2020.

[56] Y. Shan, Y. Huang, Y. Chen, and Y. Zhang. Le-
goOS: A disseminated, distributed OS for hardware
resource disaggregation. In USENIX OSDI, 2018.

[57] A. K. Simpson, A. Szekeres, J. Nelson, and
I. Zhang. Securing RDMA for high-performance
datacenter storage systems. In USENIX HotCloud,
July 2020.

[58] The Linux Foundation Projects. Data plane devel-
opment kit. https://www.dpdk.org/.

[59] A. Vahdat. Coming of Age in the Fifth Epoch
https://www.

of Distributed Computing.
youtube.com/watch?v=27zuReojDVw.

[60] L. Wang, M. Li, Y. Zhang, T. Ristenpart, and M. M.
Swift. Peeking behind the curtains of serverless
platforms. In USENIX ATC, pages 133‚Äì146, 2018.

[61] J. You, J. Wu, X. Jin, and M. Chowdhury. Ship
compute or ship data? why not both? In USENIX
NSDI, pages 633‚Äì651, 2021.

[62] I. Zhang, J. Liu, A. Austin, J. Stephenson, and
A. Badam. I‚Äôm not dead yet! the role of the operat-
ing system in a kernel-bypass era. In ACM HotOS,
April 2019.

[63] Y. Zhang, Y. Tan, B. Stephens, and M. Chowd-
hury. Justitia: Software multi-tenancy in hardware
kernel-bypass networks. In USENIX NSDI, 2022.

[64] Y. Zhong, H. Wang, Y.

J. Wu, A. Cidon,
R. Stutsman, A. Tai, and J. Yang. BPF for Storage:
An Exokernel-Inspired Approach. In ACM HotOS,
2021.

10

