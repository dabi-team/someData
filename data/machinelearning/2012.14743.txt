BayesCard: Revitalizing Bayesian Networks for Cardinality
Estimation
Ziniu Wu1, Amir Shaikhha2, Rong Zhu1, Kai Zeng1, Yuxing Han1, Jingren Zhou1
1Alibaba Group, 2University of Edinburgh
1{ziniu.wzn, red.zr, zengkai.zk, yuxing.hyx, jingren.zhou}@alibaba-inc.com, 2{amir.shaikhha}@ed.ac.uk

1
2
0
2

b
e
F
2

]

B
D
.
s
c
[

2
v
3
4
7
4
1
.
2
1
0
2
:
v
i
X
r
a

ABSTRACT
Cardinality estimation (CardEst) is an essential component in query
optimizers and a fundamental problem in DBMS. A desired CardEst
method should attain good algorithm performance, be stable to
varied data settings, and be friendly to system deployment. However,
no existing CardEst method can fulfill the three criteria at the
same time. Traditional methods often have significant algorithm
drawbacks such as large estimation errors. Recently proposed deep
learning based methods largely improve the estimation accuracy
but their performance can be greatly affected by data and often
difficult for system deployment.

In this paper, we revitalize the Bayesian networks (BN) for
CardEst by incorporating the techniques of probabilistic program-
ming languages. We present BayesCard, the first framework that
inherits the advantages of BNs, i.e., high estimation accuracy and
interpretability, while overcomes their drawbacks, i.e. low structure
learning and inference efficiency. This makes BayesCard a perfect
candidate for commercial DBMS deployment. Our experimental
results on several single-table and multi-table benchmarks indicate
BayesCardâ€™s superiority over existing state-of-the-art CardEst meth-
ods: BayesCard achieves comparable or better accuracy, 1â€“2 orders
of magnitude faster inference time, 1â€“3 orders faster training time,
1â€“3 orders smaller model size, and 1â€“2 orders faster updates. Mean-
while, BayesCard keeps stable performance when varying data with
different settings. We also deploy BayesCard into PostgreSQL. On
the IMDB benchmark workload, it improves the end-to-end query
time by 13.3%, which is very close to the optimal result of 14.2%
using an oracle of true cardinality.

1 INTRODUCTION
Cardinality estimation (CardEst), which aims at predicting the re-
sult size of a SQL query without its actual execution, is a longstand-
ing and fundamental problem in DBMS. It is the core component of
query optimizers [26, 28, 42] to produce high-quality query plans.
Although a variety of CardEst methods have been proposed in
the last several decades, it remains to be a notoriously challenging
problem in the DB community.
Status and challenges of CardEst. Given a table ğ‘‡ on attributes
{ğ‘‡1, . . . ,ğ‘‡ğ‘› } and a query ğ‘„, CardEst is equivalent to estimating the
probability of tuples in ğ‘‡ satisfying ğ‘„. Therefore, the core problem
of CardEst is how to model the distribution of ğ‘‡ to estimate the
probability of ğ‘„. Based on existing work [49], we believe that an ap-
plicable CardEst method should satisfy criteria from three aspects,
namely A(Algorithm), D(Data) and S(System). (A): the CardEst al-
gorithm itself should have high estimation accuracy, fast inference
(and training) time, lightweight storage cost, and efficient updating
process, in order to generate high-quality query plans [36, 58]. (D):

Table 1: Status of CardEst methods according to ADS criteria.

CardEst
Methods

Algorithm

Data

System

y
c
a
r
u
c
c
A

y
c
n
e
t
a
L

i

g
n
n
i
a
r
T

e
z
i
S

l
e
d
o
M

g
n

i
t
a
d
p
U

n
o
i
t
u
b
i
r
t
s
i
D

n
o
i
t
a
l
e
r
r
o
C

n

i
a
m
o
D

e
l
a
c
S

g
u
b
e
D

t
e
r
p
r
e
t
n
I

t
c
i
d
e
r
P

e
c
u
d
o
r
p
e
R

âˆ’ âœ“ âœ“ âœ“ âˆ’ âœ“ âˆ’
âˆ’ âœ“ âˆ’

Histogram âˆ’ âœ“ âœ“ âœ“ âœ“ âœ“ âˆ’ âœ“ âˆ’ âœ“ âœ“ âœ“ âœ“
âˆ’ âœ“ âˆ’ âœ“ âˆ’
âˆ’
Sampling
âœ“ âˆ’ âœ“ âœ“ âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
Naru
âˆ’ âœ“ âœ“
âœ“ âœ“ âœ“ âœ“ âˆ’ âœ“ âˆ’ âœ“ âˆ’
DeepDB
âˆ’ âœ“ âœ“
âœ“ âœ“ âœ“ âœ“ âˆ’ âœ“ âœ“ âœ“ âˆ’
FLAT
âˆ’ âœ“ âˆ’ âœ“ âˆ’ âœ“ âœ“ âœ“ âˆ’
âˆ’ âœ“
âˆ’
MSCN
âˆ’ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“
âœ“ âˆ’
BN
BayesCard âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“

âˆ’
âˆ’
âˆ’
âˆ’

the CardEst method should maintain stable performance for differ-
ent data with varied distribution, attribute correlation, domain size,
and number of attributes. (S): the CardEst method should be friendly
for system deployment with interpretable model, predictable be-
haviors, reproducible results, and easy for debugging [49].

The simplest CardEst method assumes that all attributes are mu-
tually independent and builds a histogram on each ğ‘‡ğ‘– . Its estimation
latency is low but the error is high since correlations between at-
tributes are ignored. Another class of methods samples tuples from
ğ‘‡ for CardEst. They can be inaccurate on high-dimensional data or
queries with small cardinality. These traditional CardEst methods
have significant algorithm drawbacks and unstable performance
w.r.t. varied data but friendly for system deployment.

Recently, numerous works attempt to utilize machine learning
(ML), especially deep learning (DL) techniques for CardEst. They
either build supervised models mapping featurized query ğ‘„ to its
cardinality [1, 21] or learn unsupervised models of ğ‘ƒğ‘‡ , the joint
probability distribution of table ğ‘‡ , to support computing the proba-
bility of any query ğ‘„ on ğ‘‡ [20, 53, 58]. DL-based CardEst methods
greatly improve the estimation accuracy but often sacrifice other
algorithm aspects. More importantly, their performance can be
greatly affected by data and often difficult for system deployment,
such as the hyper-parameter tuning and the â€œblack-boxâ€ property.
Table 1 summarizes the status of existing CardEst methods ac-
cording to the ADS criteria. We can clearly see that no existing
solution satisfactorily addresses this problem.

Our motivation. Recently, a classical method Bayesian networks
(BNs) have re-attracted numerous attentions in the ML community
to overcome the drawbacks of deep models [25, 54, 57], and they
are naturally suitable for CardEst [16, 18, 48]. In comparison with
other methods, BNs have significant advantages in terms of the ADS
criteria. First, from the algorithm perspective, BNs are very compact
and easy to update. Second, BNs reflect the intrinsic causal relations
between attributes, which are robust to the data changes. Thus,

 
 
 
 
 
 
they tend to maintain stable performance as the data varies in terms
of correlation, distribution, and etc. Third, BNs are interpretable,
easy to predict, maintain, validate and improve with expert knowl-
edge, thus friendly for system deployment. These attractive models
have been proposed decades ago [16, 48], but the BNsâ€™ NP-hard
model construction process and intractable probability inference
algorithm make them impractical for DBMS.

In summary, as long as we can overcome the inefficiency of model
construction and probability inference of BNs, we can obtain a desir-
able method for CardEst satisfying the ADS criteria simultaneously.

Our contributions. In this paper, we try to resolve the CardEst
challenges by revitalizing BNs with new equipments. We propose
BayesCard, a unified Bayesian framework for CardEst. The key
idea of BayesCard is to build an ensemble of BNs to model the
distributions of tables in a database, and use the constructed BNs
to estimate the cardinality of any query. BayesCard incorporates
the recent advances in probabilistic programming languages (PPLs)
for building BNs [2, 3, 33, 38, 40, 46]. PPLs allow for a declarative
specification of probabilistic models, within which each variable
is defined as a probability distribution influenced by others. Based
on PPLs, we can easily define BNs to support various structure
learning, parameter learning, and inference algorithms. Therefore,
BayesCard provides a user-friendly framework of building different
BNs suitable for various data and system settings.

The key techniques of BayesCard overcome the deficiency of ex-
isting BNs. First, based on PPLs, BayesCard designs the progressive
sampling and compiled variable elimination probability inference
algorithms, which significantly accelerate the traditional BNâ€™s infer-
ence process. Moreover, BayesCard adapts its inference algorithms
to efficiently handle multi-table join queries. Second, BayesCard
designs an efficient model construction algorithm for building an
ensemble of BNs. Furthermore, using PPLs, BayesCard can pre-
specify constraints on the learned BN structure with prior knowl-
edge to speed up the structure learning process. An accurate and
lightweight BN structure could be obtained efficiently.

By our benchmark evaluation results, in comparison with DL-
based CardEst methods, BayesCard achieves comparable or better
accuracy, 1â€“2 orders of magnitude lower inference latency (near
histogram) and update time, and 1â€“3 orders faster training time
and smaller model size. Meanwhile, BayesCard keeps stable perfor-
mance when varying data with different settings. We also integrate
BayesCard into PostgreSQL. On the benchmark workload, it im-
proves the end-to-end query time by 13.3%, which is very close to
the optimal result of 14.2% using the true cardinality.

In summary, the main contributions of this paper are as follows:
â€¢ We analyze the existing CardEst methods in terms of the ADS
criteria to evaluate a good and practical CardEst method. (Section 2)
â€¢ We propose BayesCard, a general framework that unifies the
efforts behind PPLs for constructing BNs for CardEst. (Section 3)
â€¢ We develop algorithms and techniques in BayesCard using PPLs
to improve inference latency and reduce the model construction
cost, which help BayesCard attain the desired properties of CardEst
methods. (Section 4 and 5)

â€¢ We conduct extensive experiments on benchmarks and inte-
grate BayesCard into real-world system to demonstrate its superi-
ority from ADS criteria. (Section 7)

Ziniu Wu

1

, Amir Shaikhha

2

, Rong Zhu

1

1
, Kai Zeng

, Yuxing Han

1

, Jingren Zhou

1

2 PROBLEM DEFINITION AND ANALYSIS
In this section, we first formally define the CardEst problem from
both database and statistical perspectives and then exhaustively
examine the existing traditional methods and state-of-the-art DL-
based methods for CardEst from the ADS criteria.
CardEst problem. Let ğ‘‡ be a table with ğ‘› attributes ğ‘‡1, Â· Â· Â· ,ğ‘‡ğ‘›.
For each 1 â‰¤ ğ‘– â‰¤ ğ‘›, let ğ·ğ‘– denote the domain (all unique values) of
attribute ğ‘‡ğ‘– . Any selection query ğ‘„ on ğ‘‡ can be represented in a
canonical form1 as ğ‘„ = {ğ‘‡1 âˆˆ ğ‘…ğ‘„ (ğ‘‡1) âˆ§ ğ‘‡2 âˆˆ ğ‘…ğ‘„ (ğ‘‡2) âˆ§ Â· Â· Â· âˆ§ ğ‘‡ğ‘› âˆˆ
ğ‘…ğ‘„ (ğ‘‡ğ‘›)}, where ğ‘…ğ‘„ (ğ‘‡ğ‘– ) âŠ† ğ· (ğ‘‡ğ‘– ) is the region specified by ğ‘„ over
attribute ğ‘‡ğ‘– . Without loss of generality, we have ğ‘…ğ‘„ (ğ‘‡ğ‘– ) = ğ· (ğ‘‡ğ‘– ) if
ğ‘„ has no constraint on attribute ğ‘‡ğ‘– .

Let ğ¶ğ‘„ denote the cardinality, i.e., the number of tuples in ğ‘‡
satisfying query ğ‘„. From a statistical perspective, we can also regard
all tuples in ğ‘‡ as points sampled according to the joint distribution
ğ‘ƒğ‘‡ = ğ‘ƒğ‘‡ (ğ‘‡1,ğ‘‡2, . . . ,ğ‘‡ğ‘›) of all attributes. Let ğ‘ƒğ‘‡ (ğ‘„) = ğ‘ƒğ‘‡ (ğ‘‡1 âˆˆ
ğ‘…ğ‘„ (ğ‘‡1),ğ‘‡2 âˆˆ ğ‘…ğ‘„ (ğ‘‡2), Â· Â· Â· ,ğ‘‡ğ‘› âˆˆ ğ‘…ğ‘„ (ğ‘‡ğ‘›) be the probability specified
by the region of ğ‘„. Then, we have ğ¶ğ‘„ = ğ‘ƒğ‘‡ (ğ‘„) Â· |ğ‘‡ |. Thus, the
CardEst problem can essentially be reduced to model the probability
density function (PDF) ğ‘ƒğ‘‡ of table ğ‘‡ . In this paper, we focus on data-
driven CardEst methods, which try to model ğ‘ƒğ‘‡ directly. For query-
driven CardEst methods, they implicitly model ğ‘ƒğ‘‡ by building
functions mapping ğ‘„ to ğ‘ƒğ‘‡ (ğ‘„).
Existing CardEst Methods. We review the two traditional meth-
ods widely used by commercial DBMS and four state-of-the-art
(SOTA) DL-based methods.

1). Histogram [32] method assumes all attributes in ğ‘‡ are inde-

pendent, and thus ğ‘ƒğ‘‡ can be estimated as the (cid:206)ğ‘›

2). Sampling is a model-free method, which fetches tuples from

ğ‘‡ on-the-fly to estimate the probability of ğ‘„ on the samples.

ğ‘–=1 ğ‘ƒğ‘‡ (ğ‘‡ğ‘– ).

3). Naru [53], based on deep auto-regression models (DAR) [15],
ğ‘–=2 ğ‘ƒğ‘‡ (ğ‘‡ğ‘– |ğ‘‡1, . . . ,ğ‘‡ğ‘›âˆ’1) and approximate

factorizes ğ‘ƒğ‘‡ as ğ‘ƒğ‘‡ (ğ‘‡1) âˆ— (cid:206)ğ‘›
each conditional PDF by a deep neural network (DNN).

4). DeepDB [20], based on sum-product networks (SPN) [37],
approximates ğ‘ƒğ‘‡ by recursively decomposing it into local and sim-
pler PDFs. Specifically, the tree-structured SPN contains sum node
to split ğ‘ƒğ‘‡ to multiple ğ‘ƒğ‘‡ â€² on tuple subset ğ‘‡ â€² âŠ† ğ‘‡ , product node
to decompose ğ‘ƒğ‘‡ â€² to ğ‘ƒğ‘‡ â€² (ğ‘‡ğ‘– ) Â· ğ‘ƒğ‘‡ â€² (ğ‘‡ğ‘— ) if attributes ğ‘‡ğ‘– and ğ‘‡ğ‘— are
independent and leaf node if ğ‘ƒğ‘‡ is a univariate PDF.

5). FLAT [58], based on factorized-split-sum-product networks
(FSPN) [51], improves over SPN by adaptively decomposing ğ‘ƒğ‘‡
according to the attribute dependence level. It adds the factorize
node to split ğ‘ƒğ‘‡ as ğ‘ƒğ‘‡ (ğ‘Š ) Â· ğ‘ƒğ‘‡ (ğ» |ğ‘Š ) where ğ» and ğ‘Š are highly
and weakly correlated attributes in ğ‘‡ . ğ‘ƒğ‘‡ (ğ‘Š ) is modeled in the
same way as SPN. ğ‘ƒğ‘‡ (ğ» |ğ‘Š ) is decomposed into small PDFs by the
split nodes until ğ» is locally independent of ğ‘Š . Then, the multi-leaf
node is used to model the multivariate PDF ğ‘ƒğ‘‡ (ğ» ) directly.

6). MSCN [21], is a query-driven method, which uses the set-
convolutional DNN to learn the mapping functions between the
input query ğ‘„ and its probability ğ‘ƒğ‘‡ (ğ‘„).
Analysis Results. We elaborate the ADS criteria for CardEst prob-
lem and analyze the aforementioned methods in details. The results
are summarized in Table 1.

1Handling pattern matching queries or string predicates (e.g., â€œLIKEâ€ queries) require
extensions (such as q-grams [6]), which we do not consider in this paper.

BayesCard: Revitalizing Bayesian Networks for Cardinality Estimation

â€¢ Algorithm. From the algorithmâ€™s perspective, we consider five
important metrics that are widely used in existing work [20, 58] to
evaluate the performance of CardEst methods.

â€¢ Estimation accuracy is one of the priorities for CardEst since
inaccurate estimation typically leads to sub-optimal and slow query
plan [26]. Unfortunately, the traditional methods frequently incur
poor estimations: Histogram can cause large estimation error in
presence of attributes correlations and Sampling may be inaccurate
on high-dimensional data with limited sampling size. Query-driven
methods, such as MSCN, also have poor accuracy if the target query
does not follow the same distribution of the query workload that the
model is trained on. By existing evaluations [20, 53, 58], DL-based
CardEst methods can produce accurate results.

â€¢ Inference latency is crucial since CardEst method needs to be
executed numerous times in query optimization [27, 42]. As a re-
sult, slow latency may degrade the end-to-end query time on plan
generation and execution. The inference latency of Naru is high be-
cause of its large underlying DNN models and repetitive sampling
process. Sampling is also not efficient when the sample size is large.
â€¢ Training cost refers to CardEst model construction time for a
given database. Query-driven based methods, such as MSCN, are
in general slow for training, since an enormous amount of queries
need to be executed to learn the models.

â€¢ Model size is related to the storage cost of models. In nowadays
DBMS, the space costs of all these CardEst methods are affordable.
â€¢ Update time is also important since table data frequently changes.
Traditional methods are easy to update while no existing DL-based
method can keep up with the fast data updates [49].

â€¢ Data. Generally, a DBMS will process various data with different
settings. Therefore, we analyze whether the CardEst methods have
a stable performance on four typical variations of data settings,
namely data distribution, attribute correlation, attribute domain size,
and the number of attributes (scale).

For traditional methods, Histogramâ€™s estimation error grows
exponentially when data are highly correlated. Samplingâ€™s accuracy
degrades on high-dimensional data with larger domain size and
more attributes. In addition, for highly skewed data, the fetched
samples tend to miss the query ranges with small probability, which
also degrades its accuracy.

For DL-based methods, the poor performance stability of Naru,
DeepDB and MSCN is demonstrated in a recent benchmark study [49].
In a nutshell, their accuracy decreases while inference and train-
ing cost increases with more attributes. Naru is also sensitive to
data distribution and domain size since skewed or large PDF is
more difficult to model. DeepDB has the intrinsic drawback that
tends to generate large and inaccurate SPNs on highly correlated
attributes [31]. FLAT overcomes the drawback of DeepDB but its
performance also degrades severely with more attributes.

â€¢ System. The CardEst method should satisfy the following prop-
erties for friendly system deployment [49].

â€¢ Debuggability and easy to tune are crucial to the DB experts. The
DL-based methods with â€œblack-boxâ€ components may fail silently
and contain high risks of missing a bug [49].

â€¢ Interpretability is necessary when system developers would
like to explain and validate the learned component, which is not
satisfied by the DL-based methods [5].

â€¢ Predictability is important since the system developers would
like to predict the performance before actual deployment. As Naru
and MSCN contain DNNs with illogical behaviors [49], their per-
formance is hard to predict.

â€¢ Reproducibility is necessary to locate system issues. As Sam-
pling and Naru involve stochastic processes, their results cannot be
reproduced by estimating the same query one more time.
Summary. From Table 1, we observe that no existing CardEst
method is satisfactory in all criteria. Our detailed experimental
evaluation in Section 7 also verifies this observation. Therefore,
we design a new CardEst framework BayesCard that successfully
satisfies all criteria for the first time.

3 BAYESCARD OVERVIEW
In this section, we briefly review the background knowledge on
BN and PPL in Section 3.1, which are the foundations of BayesCard.
Then we overview our new framework BayesCard for CardEst in
Section 3.2.

3.1 Background Knowledge
Bayesian networks specifies a probability distribution ğ‘ƒğ‘‡ of table
ğ‘‡ , whose attributes form a directed acyclic graph (DAG), such as
Image (2.ii) in Figure 1. Each node of the DAG corresponds to an
attribute and each edge defines the causal dependency between two
nodes. An attribute is dependent on its parents (the source nodes
with edges directing to this attribute) and conditionally independent
of all other attributes given its parents [22]. Thus, the ğ‘ƒğ‘‡ can be
compactly represented as ğ‘ƒğ‘‡ (ğ‘‡1, Â· Â· Â· ,ğ‘‡ğ‘›) = (cid:206)ğ‘›
ğ‘–=1 ğ‘ƒğ‘‡ (ğ‘‡ğ‘– |ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– )),
where ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– ) denotes the set of parents of ğ‘‡ğ‘– in the defined DAG.
Probabilistic programming languages are general-purpose pro-
gramming paradigm to specify probabilistic models and perform
inference on the models automatically. Unlike in traditional pro-
gramming languages (TPLs), each variable in PPLs is defined as
a probability distribution, whose value can condition on a set of
other variables. The compilers of PPLs are optimized to efficiently
learn parameters of variable distribution and sample from these
distributions. PPLs have been applied to various ML domains, such
as computer vision [23], with remarkable performance.

To define a BN, for each attribute ğ‘‡ğ‘– , the PPLs can define a vari-
able whose distribution is conditioned on variables in ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– ). For
example, the first seven lines in the PPL program on the right side
of Image (2.ii) in Figure 1 sufficiently defines the BN on the left as
seven variables. PPLs have the following properties. First, PPLs can
define variables of any general distribution, including tabular and
continuous distributions, which helps to build BNs with continuous
attributes. Whereas, existing BNs for CardEst problems [16, 18, 48]
only support discrete variables. Second, PPLs can efficiently learn
the parameters using maximum likelihood estimation (MLE) [33];
e.g. the parameters of the example BN in Image (2.ii) can be derived
by simply executing the last two lines of code. Third, PPLs [41] also
incorporates several main-stream algorithms for learning the BNsâ€™
structure, which captures the causal pattern of attributes in the data.
The structure learning procedure of PPLs supports pre-specifying
sub-structures. Forth, PPLs can efficiently generate samples from
the distribution of each variable.

Ziniu Wu

1

, Amir Shaikhha

2

, Rong Zhu

1

1
, Kai Zeng

, Yuxing Han

1

, Jingren Zhou

1

Figure 1: An example workflow of BayesCard.

3.2 BayesCard framework
In this paper, we propose BayesCard, a framework for CardEst. The
key idea of BayesCard is to build an ensemble of BNs to model
the distributions of tables in a database and use the constructed
BNs to estimate the cardinality of any query. This framework, in-
cluding model construction and probability inference of BNs, is
implemented using PPLs in order to leverage its compiler and exe-
cution advantages of presenting probability distribution.

Specifically, the inputs of BayesCard are a DB D containing
ğ‘› tables and its join schema J . Following prior workâ€™s assump-
tion [52, 56, 58], BayesCard only considers the join schema to be a
tree, i.e. without self joins or cyclic joins. In the join tree J , each
node represents a table and each edge represents a join relation
between two tables. For example, Figure 1-(1) illustrates a DB with
11 tables and the join tree schema on the tables.

Given D and J , BayesCard constructs an ensemble of ğ‘š BNs.
Each BN models the joint distribution of a subset of connected
tables in J . For example in Figure 1-(1), BayesCard builds 5 BNs
(ğµğ‘1, . . . , ğµğ‘5 in the red circles) to characterize the distributions
of tables in the DB, where ğµğ‘4 is built to represent the joint distri-
bution of tables ğ» and ğ¾.

To accurately model the joint distribution of multiple tables T ,
BayesCard uses the fanout method as in prior works [20, 52, 58],
by creating a BN on the full outer join results of T , along with
additional fanout attributes. For example, as shown in Figure 1-(2.i),
ğµğ‘4 models Î©, the full outer join of ğ» and ğ¾ (shown in Figure 1-
(2.iii)), along with the added fanout attributes: ğ¹ğ»âˆ’â†’Î©, indicating
how many tuples in Î© does a particular tuple in ğ» fanouts to;
ğ¹ğ¾âˆ’â†’Î©, indicating how many tuples in Î© does a particular tuple in
ğ¾ fanouts to; ğ¹Î©âˆ’â†’{ğ´,ğ· }, indicating how many tuples in the outer
join table Î© âŠ²âŠ³ ğ´ âŠ²âŠ³ ğ· does a particular tuple in Î© fanouts to.

Each BN can be represented as a PPL program, such as ğµğ‘4 in
Figure 1-(2.ii). The probability ğ‘ƒ T (ğ‘„) of any query ğ‘„ on a subset
of tables T can be estimated based on the combination of multiple
BNs containing tables covered in T . The process of estimating the
probability of a given query ğ‘ƒ T (ğ‘„) is called probability inference.

Challenges. Existing PPLs are not optimized for CardEst tasks in
terms of probability inference and model construction, which are
all addressed and optimized in BayesCard.
Probability inference. After the PPL program is successfully de-
clared to represent a BN, existing PPLs do not support using this
program for efficient probability inference, which is the key to
CardEst problem. Therefore, BayesCard tailors existing PPLs and
designs two efficient inference algorithms. Using PPLsâ€™ extremely
efficient sampling process, BayesCard proposes the progressive sam-
pling algorithm, which guarantees to run in linear time complex-
ity for estimating any query (Section 4.1). In addition, BayesCard
invents compiled variable elimination to further accelerate the in-
ference algorithm (Section 4.2). Furthermore, BayesCard adapts its
inference algorithms for the fanout method to efficiently combine
results from multiple BNs to estimate the probability of join queries
(Section 4.3).
Model construction. A database generally contains multiple ta-
bles and deciding which ensemble of BNs corresponding to the
partition of tables to learn significantly affects the CardEst accu-
racy and efficiency. Therefore, BayesCard designs the ensemble
construction algorithm to explore the optimal partition of all tables
in the DB and optimizes the CardEst quality (Section 5.1). Further-
more, Existing PPLs do not explore how to accelerates the structure
learning algorithms in DB scenarios. BayesCard tailors and speeds
up these algorithms by exploring and exploiting functional depen-
dencies and other user-defined expert knowledge (Section 5.2).

4 PROBABILITY INFERENCE IN BAYESCARD
In this section, we address the probability inference in BayesCard.
Specifically, we first propose two novel inference algorithms based
on PPLs for a single BN model, namely progressive sampling (Sec-
tion 4.1), which guarantees to return an approximate probability
estimation in linear time, and complied variable elimination (Sec-
tion 4.2), which returns the exact probability with two orders of
magnitude acceleration. Next, we present how to extend these two
algorithms on multiple BNs to support join queries (Section 4.3).

(1)Join Tree of Tables in DB (2.ii)Defining a single BN with PPL program(2.i) Adding fanout attributesH.KeyH1H2FHâ†’Î©110301120201220102K.KeyK1K2FKâ†’Î©115-52225-61220-81330101H.KeyH1H2K.KeyK1K2FHâ†’Î©FKâ†’Î©FÎ©â†’{A, D}11030115-512312020115-512422010225-621122010220-8212âˆ…âˆ…âˆ…33010014ABDCFGEIJHKvvBN1BN2BN3BN4BN5H1K1FHâ†’Î©FKâ†’Î©K2H2FÎ©	â†’{A,D}CodeListing1Codeexamples1#Definingvariablescorrespondingtoattrsin{H,K}2H1=dist.categorical();3K1=dist.categorical();4K2=dist.gaussian().condition_on(H1,K1);5F_HO=dist.categorical().condition_on(H1);6H2=dist.gaussian().condition_on(K2);7F_KO=dist.categorical().condition_on(K1);8F_OAD=dist.categorical().condition_on(F_KO,H2,K1);9#Learntheparameterofvariabledistributions10parameters=MLE(H1,K1,K2,F_HO,H2,F_KO,F_OAD);11parameters.estimate_from_data(T);Listing1:Pythonexample1(3) Probability inferenceEnsembleof BNsQuery QProbability InferenceCardinality    of QH.KeyH1H2FHâ†’Î©110301120201220102K.KeyK1K2FKâ†’Î©115-52225-61220-81330101H.KeyH1H2K.KeyK1K2FHâ†’Î©FKâ†’Î©FÎ©â†’{A, D}11030115-512312020115-512422010225-621122010220-8212âˆ…âˆ…âˆ…33010014Outer join table Î©(2.iii) Table H and KBayesCard: Revitalizing Bayesian Networks for Cardinality Estimation

Algorithm 1 Progressive Sampling Inference Algorithm
Input: a table ğ‘‡ with ğ‘› attributes, a query ğ‘„ with region ğ‘…ğ‘„ and a PPL
program defining the BN on ğ‘ƒğ‘‡

1: Align the attributes in topological order ğ‘‡1, . . . ,ğ‘‡ğ‘›
2: ğ‘ â† 1, ğ‘† â† [0]ğ‘˜Ã—ğ‘›, an ğ‘˜ Ã— ğ‘› dimension matrix of samples
3: for ğ‘– âˆˆ {1, . . . , ğ‘› } do
4:

Take ğ‘† [ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– ) ], the columns in ğ‘† corresponding to attributes in

(cid:205)ğ‘‘ âˆˆğ‘† [ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– ) ] ğ‘ƒğ‘‡ (ğ‘‡ğ‘– |ğ‘‘)

Ë†ğ‘ƒğ‘– (ğ‘‡ğ‘– ) â† 1
ğ‘˜
ğ‘ â† ğ‘ âˆ— Ë†ğ‘ƒğ‘– (ğ‘‡ğ‘– âˆˆ ğ‘…ğ‘„ (ğ‘‡ğ‘– ))
Define a PPL variable ğ‘ƒ â€²
ğ‘† [ğ‘– ] â† ğ‘˜ points sampled from ğ‘ƒ â€²
ğ‘–

ğ‘– by normalizing Ë†ğ‘ƒğ‘– (ğ‘¡ğ‘– |ğ‘¡ğ‘– âˆˆ ğ‘…ğ‘„ (ğ‘‡ğ‘– ))

ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– )

5:

6:

7:
8:
9: end for
10: return ğ‘

4.1 Progressive sampling
We define the inference procedure of a simple case, where we
have a query ğ‘„ on tables ğ‘‡ in a DB and a single BN that exactly
models ğ‘ƒğ‘‡ on the full outer join of tables ğ‘‡ . In this case, estimating
the cardinality of ğ‘„, ğ‘ƒğ‘‡ (ğ‘„) can be derived directly on this BN. As
defined in Section 2, a query ğ‘„ takes the form of {ğ‘‡1 âˆˆ ğ‘…ğ‘„ (ğ‘‡1)âˆ§ğ‘‡2 âˆˆ
ğ‘…ğ‘„ (ğ‘‡2) âˆ§ Â· Â· Â· âˆ§ ğ‘‡ğ‘› âˆˆ ğ‘…ğ‘„ (ğ‘‡ğ‘›)}, where ğ‘…ğ‘„ is the region defined by ğ‘„
over attributes in ğ‘‡ .

Thus, we can represent the probability of ğ‘„ as: ğ‘ƒğ‘‡ (ğ‘„) = (cid:206)ğ‘›

(ğ‘‡ğ‘– âˆˆ ğ‘…ğ‘„ (ğ‘‡ğ‘– )|ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– ) âˆˆ ğ‘…ğ‘„ (ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– ))) = (cid:206)ğ‘›
denotes the query region over the set of parent attributes ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– )
and we can denote each term as ğ‘ƒğ‘– , for simplicity. Therefore, to
compute ğ‘ƒğ‘‡ (ğ‘„), we only need to compute or estimate each ğ‘ƒğ‘– .

ğ‘–=1 ğ‘ƒğ‘– , where ğ‘…ğ‘„ (ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– ))

ğ‘–=1 ğ‘ƒğ‘‡

In PPLs, accessing the probability ğ‘ƒğ‘‡ (ğ‘‡ğ‘– |ğ‘ ) for each fixed value
assignment ğ‘  âˆˆ ğ‘…ğ‘„ (ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– )) takes constant time complexity. How-
ever, computing ğ‘ƒğ‘– is generally intractable, as there can be exponen-
tial or infinite number of unique values in ğ‘…ğ‘„ (ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– )). Specifically,
for large BNs with complex structures, the PPLsâ€™ existing inference
algorithms can not have an efficiency guarantee, which is required
for CardEst in practical DBMS. Therefore, BayesCard designs the
progressive sampling inference algorithm, which uses the Monte
Carlo approximation of ğ‘ƒğ‘– based on a sample ğ‘† of ğ‘…ğ‘„ (ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– )) to
ensure the computation efficiency, i.e., ğ‘ƒğ‘– â‰ˆ 1
(cid:205)ğ‘  âˆˆğ‘† ğ‘ƒğ‘‡ (ğ‘…ğ‘„ (ğ‘‡ğ‘– )|ğ‘ ).
|ğ‘† |
The default sampling procedure in PPLs only supports sampling
values from a variableâ€™s domain, which are not like to fail in the
query range ğ‘…ğ‘„ . Naively using this sampling algorithm will re-
sult in enormous ineffective points. Therefore, we can leverage
the learned model, create variables to materialize the distribu-
tion ğ‘ƒ (ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– )|ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– ) âˆˆ ğ‘…ğ‘„ (ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– ))), and progressively sample
points from ğ‘…ğ‘„ (ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– )) accordingly, which greatly improves the
sample effectiveness.

Algorithm description. We present the details in Algorithm 1.
Specifically, we first align the attributes from ğ‘‡ in topological order
as ğ‘‡1, . . . ,ğ‘‡ğ‘›, where ğ‘‡1 is the root of the BNâ€™s DAG structure (line 1).
We can directly obtain from the PPL ğ‘ƒğ‘‡ (ğ‘‡1) as it does not depend
on any other attribute, and compute ğ‘ƒ1 = ğ‘ƒğ‘‡ (ğ‘…ğ‘„ (ğ‘‡1)). Then, we
can define a new variable in PPLs to represent the distribution
ğ‘ƒğ‘‡ (ğ‘¡1|ğ‘¡1 âˆˆ ğ‘…ğ‘„ (ğ‘‡1)) and generate sample ğ‘†1 of ğ‘…ğ‘„ (ğ‘‡1) from this
variable. Next, for each of the rest attributes ğ‘‡ğ‘– , the samples of
its parents ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– ) must have already been generated because the

attributes are aligned in topological order (line 5). We can derive a
new distribution Ë†ğ‘ƒğ‘– approximating ğ‘ƒğ‘‡ (ğ‘‡ğ‘– |ğ‘…ğ‘„ (ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– ))) using these
samples (line 6). This distribution Ë†ğ‘ƒğ‘– will be used to estimate ğ‘ƒğ‘–
(line 7) and generate samples from ğ‘…ğ‘„ (ğ‘‡ğ‘– ) (line 8). At last, after we
achieve the estimated value for each ğ‘ƒğ‘– , ğ‘ƒğ‘‡ (ğ‘„) can be computed as
their product (line 10).
Analysis. Sampling |ğ‘† | points and evaluating the probability with
each fixed point takes ğ‘‚ (|ğ‘† |) time complexity to approximate each
ğ‘ƒğ‘– . Thereafter, time complexity of progressive sampling on BN with
any structure is guaranteed to be ğ‘‚ (|ğ‘† | âˆ— ğ‘›). This inference algo-
rithm is very efficient because generally, a small sample ğ‘† would
suffice to make a very accurate estimation and the sampling process
is extremely efficient in PPL. The progressive sampling algorithm
in PPL resembles the one in the DAR model, proposed by Naru [53].
Our method is different from theirs in the following aspects: 1)
Efficient sampling is naturally supported in PPL for various con-
tinuous distributions, whereas the sampling procedure in DAR is
post-equipped for categorical distributions only. 2) The progressive
sampling in BayesCard estimates each ğ‘ƒğ‘– using sample ğ‘† during
the sampling process, whereas in DAR, the samples ğ‘† are used to
directly compute the ğ‘ƒğ‘‡ , which is less effective.
Graph reduction optimization. To further accelerate the pro-
gressive sampling algorithm, BayesCard proposes the graph reduc-
tion optimization, which significantly speeds up the inference la-
tency for datasets with large amount of attributes.
Main idea. In fact, the progressive sampling algorithm involves a
large amount of redundant computation. For example, for an at-
tribute ğ‘‡ğ‘– , which is not constrained by predicates in ğ‘„, i.e. ğ‘…ğ‘„ (ğ‘‡ğ‘– ) =
ğ· (ğ‘‡ğ‘– ), the estimation of ğ‘ƒğ‘– should equal to 1. If all the decedents ğ‘‡ğ‘—
of ğ‘‡ğ‘– are not constrained in ğ‘„, there is no need to sample ğ‘‡ğ‘– since
each ğ‘ƒ ğ‘— should equal to 1 regardless of the samples. Therefore, we
can reduce the larger BN model to a much smaller one by removing
these redundant attributes, and perform probability inference on it
without affecting the estimation accuracy.
Formulation. First, we make the following rigorous definition
of reduced graph ğº â€². Intuitively, ğº â€² only contains all constrained
attributes in the query and other necessary attributes to connect
them to form a minimal BN. An example of a reduced graph can be
found in Figure 2.

Definition 1. Given a BN representing a table ğ‘‡ with attributes
ğ‘‰ = {ğ‘‡1, Â· Â· Â·ğ‘‡ğ‘›}, its defined DAG ğº = (ğ‘‰ , ğ¸), and a query ğ‘„ = (ğ‘‡ â€²
1 =
ğ‘– âˆˆ ğ‘‰ . We define the reduced graph ğº â€² =
ğ‘¡ â€²
1 âˆ§ Â· Â· Â· âˆ§ ğ‘‡ â€²
(ğ‘‰ â€², ğ¸ â€²) to be a sub-graph of ğº where ğ‘‰ â€² equals (cid:208)1â‰¤ğ‘– â‰¤ğ‘˜ ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ (ğ‘‡ â€²
ğ‘– ),
and ğ¸ â€² equals all edges in ğ¸ with both endpoints in ğ‘‰ â€². ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ (ğ‘‡ â€²
ğ‘– )
includes all parent nodes of ğ‘‡ â€²
ğ‘– and their parent nodes recursively.

ğ‘˜ ) where ğ‘‡ â€²

ğ‘˜ = ğ‘¡ â€²

Based on this definition, we can reduce the original BN model
(i.e. PPL program with variables ğ‘‰ ) into a much smaller one (i.e.
PPL program with variable ğ‘‰ â€²), and perform inference on it. The
correctness of the graph reduction optimization is stated in Theo-
rem 1. Due to space limits, we put the proof of all theorems in the
Appendix A of the accompanied technical report [50].

Theorem 1. Given a BN ğµ defining ğº, a query ğ‘„ and the reduced
BN ğµâ€² defining ğº â€² on ğ‘„, computing ğ‘ƒğ‘‡ (ğ‘„) on ğµâ€² is equivalent to
computing ğ‘ƒğ‘‡ (ğ‘„) on ğµ.

Ziniu Wu

1

, Amir Shaikhha

2

, Rong Zhu

1

1
, Kai Zeng

, Yuxing Han

1

, Jingren Zhou

1

An example program showing the JIT compilation of VE on the
same query ğ‘„ is shown in Figure 2. Specifically, for each variable
ğ‘‡ğ‘– of PPLs in the reduced graph ğº â€², the JIT program first extract
the parameters of its distribution ğ‘ƒğ‘‡ (ğ‘‡ğ‘– |ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– )). Since VE only
supports categorical distributions, the extracted parameters of ğ‘‡ğ‘–
. Next, based on the query region ğ‘…ğ‘„ , the JIT
forms a matrix ğ‘€ğ‘‡ğ‘–
program can further reduce ğ‘€ğ‘‡ğ‘–
by keeping only useful information,
i.e. slicing its rows with ğ‘…ğ‘„ (ğ‘‡ğ‘– ) and its columns with ğ‘…ğ‘„ (ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– ))
(lines 2-6 of the code in Figure 2). This reduction not only eliminates
the redundant computation but also enables a close-form linear
algebra equation.

Then, BayesCard can determine an elimination order for these
variables using the reversed topological order or standard proce-
dure [11]. A fixed program containing only linear algebra opera-
tions can be derived, like the one in line 8, where â€œmatmalâ€ refers
to matrix multiplication, â€œcolsumâ€ refers to column sum, and â€œ.Tâ€
refers to the transpose. At last, this generated static program can
execute efficiently, thanks to the batch processing of the tensor
operations with various performance tuning techniques (e.g., loop
tiling, parallelization, and vectorization). By our evaluation, such
program can achieve up to two orders of magnitude speed-ups over
the original VE algorithm.

4.3 Probability inference for fanout method
Previous sections discuss the process of inferring the probability
ğ‘ƒğ‘‡ (ğ‘„) of a query ğ‘„ on the table(s) ğ‘‡ , represented by exactly a single
BN. For a database with multiple tables, this process needs to be
modified for the following two types of queries: (1) a query ğ‘„ on
tables, that cover many BNs (i.e. ğ‘„ on ğ‘‡ = {ğ´, ğ·, ğ», ğ¾ } in Figure 1)-
(1); (2) a query on tables, that only cover a subset of a single BN
(i.e. ğ‘„ on ğ‘‡ = {ğ» }). In these cases, the BayesCard does not contain
an exact BN representing ğ‘ƒğ‘‡ to estimate this query ğ‘„. Fortunately,
based on the fanout method explained earlier in Section 3.2, we can
use the following theorem to calculate ğ‘ƒğ‘‡ (ğ‘„), which is proposed
and proved in [58].

Theorem 2. Given a query ğ‘„, let ğ‘‰ = {ğ‘‰1, ğ‘‰2, . . . , ğ‘‰ğ‘‘ } denote
all vertices (nodes) in the join tree touched by ğ‘„ and let V de-
notes the full outer join of all tables in ğ‘‰ . On each node ğ‘‰ğ‘– , let ğ¹ =
{ğ¹ğ´1,ğµ1, ğ¹ğ´2,ğµ2, . . . , ğ¹ğ´ğ‘›,ğµğ‘› }, where each (ğ´ğ‘— , ğµ ğ‘— ) is a distinct join
where ğµ ğ‘— is not in ğ‘„. Let ğ‘“ = (ğ‘“1, ğ‘“2, . . . , ğ‘“ğ‘›) where ğ¹ğ´ ğ‘— ,ğµ ğ‘— = ğ‘“ğ‘— for all
1 â‰¤ ğ‘– â‰¤ ğ‘›, denote an assignment to ğ¹ and dlm(ğ‘“ ) = (cid:206)ğ‘›
ğ‘—=1 max{ğ‘“ğ‘— , 1}.
Let

ğ‘ğ‘– =

(cid:18)

|Vğ‘– |
|V |

âˆ‘ï¸

Â·

ğ‘“ ,ğ‘£

ğ‘ƒVğ‘– (ğ‘„ğ‘– âˆ§ ğ¹ = ğ‘“ âˆ§ ğ¹ğ‘‰ğ‘– ,ğ‘‰ = ğ‘£) Â·

max{ğ‘£, 1}
dlm(ğ‘“ )

(cid:19)

.

(1)

Then, the cardinality of ğ‘„ is |V | Â· (cid:206)ğ‘‘

ğ‘–=1 ğ‘ğ‘– .

In short, since all the fanout attributes involved in this compu-
tation are pre-stored in the table ğ‘‰ğ‘– and there exists a BN for ğ‘ƒVğ‘–
,
BayesCard can directly use this theorem for probability inference
of multi-table join queries.
Efficient summation computation in BayesCard. We can com-
max{ğ‘£,1}
pute the summation (cid:205)ğ‘“ ,ğ‘£ (ğ‘ƒVğ‘– (ğ‘„ğ‘– âˆ§ ğ¹ = ğ‘“ âˆ§ ğ¹ğ‘‰ğ‘–,ğ‘‰ = ğ‘£) Â·
dlm(ğ‘“ ) )
over all assignments of ğ‘“ and ğ‘£ as efficiently as computing the
probability ğ‘ƒVğ‘– (ğ‘„ğ‘– ) for any query. We will explain the detailed pro-
cedure for calculating (cid:205)ğ‘“ âˆˆğ· (ğ¹ ) ğ‘ƒğ‘‡ (ğ‘„, ğ¹ = ğ‘“ ) âˆ— ğ‘“ using progressive

Figure 2: Graph reduction and the compiled program with
JIT. The left image shows the graph reduction for query ğ¾2 âˆˆ
{10, 20}, ğ»2 âˆˆ {âˆ’1, 10}. The red nodes refer to the attributes in
the query. All red, green nodes and the red edges form the
reduced graph ğº â€².

4.2 Compiled variable elimination
Progressive sampling works for general PPL programs with any
distribution type. For programs restricted to categorical distribu-
tions, we can further accelerate the inference algorithm using an
alternative approach: compiled variable elimination. Inspired by the
impressive results of compilation for query processing [34, 43, 44],
we investigate the usage of just-in-time compilation (JIT) and com-
piler optimizations to improve inference latency.

Observation. Let us revisit the example ğµğ‘4 built on tables ğ»
and ğ¾, in the left image of Figure 2. Consider a query ğ‘„ = (ğ¾1 âˆˆ
{10, 20} âˆ§ ğ»2 âˆˆ {âˆ’1, 10}), where we remove all â€œblackâ€ attributes
by the graph reduction technique, based on Theorem 1. For the
â€œgreenâ€ attributes, we have ğ‘…ğ‘„ (ğ»1) = ğ· (ğ»1), ğ‘…ğ‘„ (ğ¾2) = ğ· (ğ¾2), and
ğ‘…ğ‘„ (ğ¹ğ»âˆ’â†’Î©) = ğ· (ğ¹ğ»âˆ’â†’Î©). The variable elimination algorithm (VE)
compute the probability ğ‘ƒğ‘‡ (ğ‘„) based on the following equation.

ğ‘ƒğ‘‡ (ğ‘„) =

âˆ‘ï¸

Â· Â· Â·

âˆ‘ï¸

ğ‘ƒğ‘‡ (â„1) âˆ— ğ‘ƒğ‘‡ (ğ‘˜1) âˆ— Â· Â· Â· âˆ— ğ‘ƒğ‘‡ (â„2|, ğ‘“ğ» âˆ’â†’Î©, ğ‘˜2)

â„1 âˆˆğ‘…ğ‘„ (ğ»1)

â„2 âˆˆğ‘…ğ‘„ (ğ»2)

This computation can be very inefficient in PPLs and repeated
for estimating multiple queries. However, we observe that the VE
algorithm only involves sum and product over attributes. If each
variable in PPL (attribute in BN) is defined as categorical conditional
distribution, they can be materialized as vectors or matrices. Thus,
the VE algorithm essentially defines a program of linear algebra
operations, whose execution time can be significantly enhanced by
nowadays computing resource. Furthermore, we observe that the
linear algebra program computing VE is fixed for a target query as
long as the elimination order is fixed.

JIT of VE. For any query, the BayesCard can first decide an optimal
variable elimination order and then compile the learned BN from the
PPL program into a static program containing only matrix or tensor
operations to maximize the execution efficiency. Furthermore, this
program can be re-used to infer other queries with the same reduced
graph by only changing the input query regions ğ‘…ğ‘„ (as shown in
Figure 2). Therefore, JIT can remember the execution pattern for
this query and will re-use this pattern to infer the probability of
future queries for further speed-up.

H1K1FHâ†’Î©FKâ†’Î©K2H2FÎ©	â†’{A,D}H1K1FHâ†’Î©K2H2GraphReductionJITCodeListing1Codeexamples1#Definingvariablescorrespondingtoattrsin{H,K}2H1=dist.categorical();3K1=dist.categorical();4K2=dist.gaussian().condition_on(H1,K1);5F_HO=dist.categorical().condition_on(H1);6H2=dist.gaussian().condition_on(K2);7F_KO=dist.categorical().condition_on(K1);8F_OAD=dist.categorical().condition_on(F_KO,H2,K1);9#Learntheparameterofvariabledistributions10parameters=MLE(H1,K1,K2,F_HO,H2,F_KO,F_OAD);11parameters.estimate_from_data(T);Listing1:Pythonexample1defJIT_exact_for_BN_4(R_Q):2#extractdistparametersandmatrixandslicewithR_Q3M_H1=H1.params.as_mat()[R_Q(H1)];4M_K1=K1.params.as_mat()[R_Q(K1)];5M_K2=K2.params.as_mat()[R_Q(K2),R_Q(H1),R_Q(K1)];6M_F_HO=F_HO.params.as_mat()[R_Q(F_HO),R_Q(H1)];7M_H2=H2.params.as_mat()[R_Q(H2),R_Q(K2),R_Q(F_HO)];8#Usethereversetopologicalorderaseliminationorder9exec(matmal(colsum(matmal(colsum(M_H2),M_F_HO)*10matmal(M_K2,M_K1)),M_H1))Listing2:Pythonexample1BayesCard: Revitalizing Bayesian Networks for Cardinality Estimation

Figure 3: BayesCard ensemble learning algorithm demo.
sample and complied variable elimination, where ğ· (ğ¹ ) denotes the
domain of unique values in ğ¹ . Then, this procedure can naturally
generalize to more complex cases.

Our calculation procedure is motivated by the Bayesian rule,
that ğ‘ƒğ‘‡ (ğ‘„, ğ¹ = ğ‘“ ) = ğ‘ƒğ‘‡ (ğ¹ = ğ‘“ |ğ‘„) âˆ— ğ‘ƒğ‘‡ (ğ‘„). We observe that ğ‘ƒğ‘‡ (ğ‘„)
is a fixed value independent of ğ¹ because the fanout attributes are
artificial attributes that will not be involved in ğ‘„. Furthermore, by
property of BN, we know that ğ‘ƒğ‘‡ (ğ¹ |ğ‘„) = ğ‘ƒğ‘‡ (ğ‘“ |ğ‘…ğ‘„ (ğ‘ƒğ‘ğ‘Ÿ (ğ¹ ))), so
can derive the following equation. It spots a common term ğ‘ƒğ‘‡ (ğ‘„)
so the calculation can avoid repeatedly computing ğ‘ƒğ‘‡ (ğ‘„).

âˆ‘ï¸

âˆ‘ï¸

ğ‘“ âˆˆğ· (ğ¹ )

ğ‘ƒğ‘‡ (ğ‘„, ğ¹ = ğ‘“ )âˆ—ğ‘“ = ğ‘ƒğ‘‡ (ğ‘„)âˆ—(cid:169)
ğ‘ƒğ‘‡ (ğ‘“ |ğ‘…ğ‘„ (ğ‘ƒğ‘ğ‘Ÿ (ğ¹ ))) âˆ— ğ‘“ (cid:170)
(cid:173)
(cid:174)
ğ‘“ âˆˆğ· (ğ¹ )
(cid:171)
(cid:172)
Progressive sampling. Recall in Section 4.2, BayesCard estimates
ğ‘ƒğ‘– = ğ‘ƒğ‘‡ (ğ‘‡ğ‘– |ğ‘…ğ‘„ (ğ‘ƒğ‘ğ‘Ÿ (ğ‘‡ğ‘– ))) by making progressive samples of ğ‘…ğ‘„
and approximate the ğ‘ƒğ‘‡ (ğ‘„) as (cid:206) ğ‘ƒğ‘– . After finishing estimating
ğ‘ƒğ‘‡ (ğ‘„) with sample ğ‘†, BayesCard can directly estimate (cid:205)ğ‘“ âˆˆğ· (ğ¹ )
ğ‘ƒğ‘‡ (ğ‘“ |ğ‘…ğ‘„ (ğ‘ƒğ‘ğ‘Ÿ (ğ¹ ))) âˆ— ğ‘“ using the same sample ğ‘†, i.e. as (cid:205)ğ‘“ âˆˆğ‘† [ğ¹ ]
Ë†ğ‘ƒğ‘‡ (ğ‘“ |ğ‘† [ğ‘ƒğ‘ğ‘Ÿ (ğ¹ )])âˆ—ğ‘“ . The final result can be achieved by multiplying
these two terms together.
Compiled variable elimination. Recall in Section 4.2, BayesCard
can specify a particular elimination order by choosing the fanout
variable ğ¹ as the last variable to eliminate. Using PPL, the intermedi-
ate result after each elimination step is materialized as a distribution.
Therefore, before the last elimination step of VE algorithm for com-
puting ğ‘ƒğ‘‡ (ğ‘„), BayesCard can store the intermediate result, which
represents the conditional distribution ğ‘ƒğ‘‡ (ğ¹ |ğ‘„). Then, the sum-
mation (cid:205)ğ‘“ âˆˆğ· (ğ¹ ) ğ‘ƒğ‘‡ (ğ‘“ |ğ‘…ğ‘„ (ğ‘ƒğ‘ğ‘Ÿ (ğ¹ ))) âˆ— ğ‘“ equals to ğ‘ƒğ‘‡ (ğ¹ |ğ‘„) Â· ğ· (ğ¹ ),
where Â· denotes the vector dot product. Therefore, similar to com-
puting ğ‘ƒğ‘‡ (ğ‘„), this process only involves linear algebra operations,
which can be compiled and efficiently calculated using JIT.

5 MODEL CONSTRUCTION OF BAYESCARD
In this section, we explain how BayesCard constructs an ensemble
of BNs for a multi-table database. Specifically, Section 5.1 first intro-
duces the BN ensemble construction method with budget, which
clusters all tables in the database into several groups and builds
a single BN on each group of tables. Then, Section 5.2 introduces
some optimizations for building a single BN using PPLs. Finally,
Section 5.3 shows how to incrementally update the BN model.

5.1 Ensemble construction with budget
Main idea. Consider the example database in Figure 3 with 11
tables ğ´, ğµ, . . . , ğ¾ forming a join tree, where each node represents a
table and each edge represents a possible join between two tables. A
previous approach [20] suggests to create every possible two-table
join results, examine the level of dependence between attributes
across the two, and determine whether to create one large model on
their full outer join table or two separate models. Since generating
the full outer join of multiple tables could require exponential

Algorithm 2 BN Ensemble Construction Algorithm
Input: a DB schema with n tables ğ‘‡1, Â· Â· Â· ,ğ‘‡ğ‘› and a budget ğ‘˜
1: Create the join tree T = (ğ‘‰ , ğ¸) for the schema
2: Generate unbiased samples ğ‘† for full outer join of the entire schema
3: Initialize a dependence matrix ğ‘€ âˆˆ Rğ‘›Ã—ğ‘›
4: for Each pair of tables ğ‘’ = (ğ‘‡ğ‘–,ğ‘‡ğ‘— ) do
5:

Calculate the RDC dependence level scores between all attributes

in ğ‘‡ğ‘– and attributes in ğ‘‡ğ‘—

ğ‘¤ğ‘’ â† average RDC scores

return T and learn a single PRM for each table

6:
7: end for
8: if ğ‘˜ = 1 then
9:
10: end if
11: for ğ‘˜â€² â† 2, Â· Â· Â· , ğ‘˜ do
12:
13:
14:
15:
16:
17:
18: end for
19: return T and learn a single PRM for each node in T

Sort ğ¸ in decreasing order based on ğ‘¤ğ‘’ .
for ğ‘’ = (ğ‘¢, ğ‘£) âˆˆ ğ¸ do

end for

if ğ‘¢ and ğ‘£ contain exactly ğ‘˜â€² tables in total then

end if

Update ğ‘‡ by contracting nodes ğ‘¢, ğ‘£ to a single node {ğ‘¢, ğ‘£ }

memory, this approach normally can not explore the possibility of
creating a model on the join of more than three tables.

Another approach [52] generates an unbiased sample ğ‘† on the
full outer join of all tables in the schema and builds a single large
model on ğ‘† directly. As the resulting model is built on all attributes
in the database, the model construction and the probability infer-
ence can be very inefficient. Moreover, the size of ğ‘† is relatively
small with respect to the full outer join size, suggesting a large
amount of information loss, so the learned model on ğ‘† might not
accurately represent the actual data distribution.

In order to balance the estimation accuracy and inference effi-
ciency, we want to explore the full possibility of learning different
BN ensembles such that the number of joined tables in each BN
is no more than a threshold. Therefore, the resulting ensemble
should capture as much dependence between tables as possible and
simultaneously keep each BN in this ensemble as small as possible.
Algorithm description. The details of the ensemble construction
algorithm is given in Algorithm 2. First, we define the budget ğ‘˜
such that a single BN model can only be constructed on (a sample
of) the full outer join of no more than ğ‘˜ tables. The budget ğ‘˜ is a
hyper-parameter decided by the dataset, system, and computing
resource. The algorithm generally works as follows:

1) Computing dependency between tables (lines 1-7). Given a tree-
structured join schema T , we first generate the unbiased sample ğ‘†
of the full outer join of all tables according to [56]. Specifically, the
join tree is regarded as a rooted tree and samples ğ‘† are obtained by
scanning all tables in T in a bottom-up manner. Then, we calcu-
late the randomized dependence coefficient, i.e., RDC value [29],
between each pair of join tables using ğ‘†. The detailed computation
method is given in Appendix B of our technical report [50]. In
Figure 3, the RDC value is shown as red numbers on each edge.

2) Contracting nodes (lines 8-18). Intuitively, we would like to
build a model on the full outer join of tables with high dependency.
We can iteratively contract the nodes (tables) with high RDC value

ABCDEFGHIJK0.10.30.50.80.70.80.90.60.60.71234(a). kâ€™= 2A,DB,EC,FH,KGIJ0.10.40.45120.60.750.7(b). kâ€™= 3A,DB,E,IC,F,GH,KJ(c). Resultin T in a greedy manner. Let ğ‘˜ â€² = 2 at the beginning. In each
iteration, if ğ‘˜ â€² â‰¤ ğ‘˜, we first sort all edges ğ‘’ = (ğ‘¢, ğ‘£) (joins) in a
descending order based on their RDC values. According to this edge
order, we aggregate ğ‘¢, ğ‘£, i.e. two endpoints of edge ğ‘’, into a single
node if they contain exactly ğ‘˜ â€² tables in total and update the RDC
values of ğ‘’ accordingly, whose details is given in Appendix B of
our technical report [50]. We iterate this process until ğ‘˜ â€² = ğ‘˜, and
in the end, we obtain a tree where each node contains at most ğ‘˜
tables. For example, in Figure 3, let the budget ğ‘˜ = 3. In the first
iteration where ğ‘˜ â€² = 2, the algorithm considers joining two tables
together. The edge (ğµ, ğ¸) has the highest RDC value, so ğµ and ğ¸
are aggregated in the first step ( 1â—‹ in Figure 3(a)). After the first
iteration, the join schema T has been transformed into a new tree
in Figure 3(b). Similarly, in the second iteration where ğ‘˜ â€² = 3, the
node {ğµ, ğ¸} is first merged with the node ğ¼ . Finally, the join tree is
transformed to a tree in Figure 3(c).

3) Building BNs (line 19). In the end, BayesCard will construct a
single BN model on (a sample of) the full outer join of tables within
each node and fanout attributes will be added accordingly.

Time Complexity analysis. As shown in [56], creating the sam-
ples ğ‘† on the full outer join of tables ğ‘‡1, Â· Â· Â· ,ğ‘‡ğ‘› takes ğ‘‚ ((cid:205)ğ‘›
ğ‘–=1 |ğ‘‡ğ‘– |)
time. Let ğ‘š be the attribute number in the full outer join of the
tables. Calculating the pairwise RDC values takes ğ‘‚ (ğ‘š2|ğ‘† | log |ğ‘† |).
The rest of Algorithm 2 takes ğ‘‚ (ğ‘˜ğ‘›2) time since the algorithm termi-
nates in ğ‘˜ iterations and in each iteration we only need to check the
tables defined by two endpoints of each edge, which is at most ğ‘›2.
Thus, the whole time complexity is ğ‘‚ ((cid:205)ğ‘›
ğ‘–=1 |ğ‘‡ğ‘– | +ğ‘š2|ğ‘† | log |ğ‘† | +ğ‘˜ğ‘›2).

5.2 Single model construction optimizations
The structure learning process, i.e., learning the causal structure
from data of a single BN, is an NP-hard combinatorial optimization
problem [7]. Current structure learning algorithms supported by
PPLs either produce a general DAG structure or a simplified tree
structure. We show optimization techniques for them as follows:

Optimization for DAG structure learning algorithms. The ex-
act DAG structure learning algorithms explore the super-exponential
searching space of all possible DAGs and select the best candi-
date [4, 9, 19, 55]). The learned structure is accurate but ineffi-
cient, which only scales to tens of attributes. Approximate meth-
ods limit the searching space with local heuristic (i.e. greedy algo-
rithms [8, 14, 39]), but they may produce inaccurate results. Based
on PPLs, BayesCard supports pre-specifying sub-structures before
running the exact and greedy structure learning algorithms, which
limits the DAG searching space and makes the structure learn-
ing much more efficient. Specifically, practical databases gener-
ally exist attributes with functional dependencies [13] or obvious
causal relations between attributes, such as oneâ€™s â€œageâ€ determining
oneâ€™s â€œschool levelâ€. First, users of BayesCard can use their â€œexpert
knowledgeâ€ to pre-specify certain causal structures for subsets of
attributes. Then, the PPLs within BayesCard can define the vari-
ables corresponding to these attributes, and condition the variables
with each other according to the pre-specified structure. At last,
BayesCard can rely on the existing algorithms to construct the re-
maining causal structure on these variables. Since the algorithms
are forced to maintain these sub-structures, the number of qualified

Ziniu Wu

1

, Amir Shaikhha

2

, Rong Zhu

1

1
, Kai Zeng

, Yuxing Han

1

, Jingren Zhou

1

DAG candidates is significantly curtailed, making the structure
learning process more efficient without loss in accuracy.
Optimization for tree structure learning algorithms. The tree
structure learning algorithm learns a tree structure such as Chow-
Liu tree [10], which sacrifices accuracy for efficiency. BayesCard can
also improve the accuracy of a learned structure using the afore-
mentioned â€œexpert knowledgeâ€ after running the Chow-Liu tree
algorithm. This efficient algorithm forces the learned BN structure
to be a tree, which could contain â€œfalseâ€ causality or miss important
attribute dependence. For example, intuitively we know that the
number of â€œchildrenâ€ raised by someone is largely dependent on
oneâ€™s â€œincomeâ€ and oneâ€™s â€œmarital statusâ€, which can not be cap-
tured simultaneously by the tree BN, since one node is only allowed
to have one parent. Thus, after the structure is learned, BayesCard
can add the edge from â€œIncomeâ€ to â€œChildrenâ€ to improve its accu-
racy. With PPLs, only the parameters of the affected sub-structure
(the â€œChildrenâ€ variable in this example) need to be updated.

5.3 Model updates
Most of the practical databases update their data frequently, requir-
ing the cardinality estimators to adjust their underlying models
dynamically [49]. When the data distribution changes, BayesCard
can update its underlying BNs very efficiently. Specifically, the
learned structure of BN captures the intrinsic causal pattern of the
attributes, which is not likely to change even in the case of massive
data updates. Therefore, in most cases, BayesCard can preserve the
original BN structure and only incrementally update its distribution
parameters. Such parameter updates are extremely efficient using
MLE in PPLs. By our testing, it generally takes less than one second
for an insertion or deletion of a thousand tuples. In some rare cases
involving the insertion or deletion of attributes, a new BN struc-
ture should be constructed. Even in this case, the causal pattern of
the original attributes is largely preserved. Therefore, BayesCard
can pre-specify some sub-structures and learn the new structure
efficiently using the methods stated in the previous section.

6 ANALYSIS OF BAYESCARD
In this section, we analyze and demonstrate that BayesCard satisfies
the ADS criteria from all aspects, as shown in Table 1.
Algorithm. A BN with exact learned structure can losslessly cap-
ture the data distribution, a.k.a. near-perfect estimation accuracy for
all queries. We show empirically that even with an approximate tree
structure, BayesCard can achieve comparable or better accuracy
than the current SOTA methods. The inference latency of BayesCard
is roughly 1ms per query (close to Histogram method), thanks to
our novel inference algorithms. Furthermore, as explained in Sec-
tion 4, BayesCard can learn a compact structure of small model size
with fast training and update time.
Data. Every dataset contains an inherent causal pattern, which
can be discovered by BayesCard. Building upon this structure,
BayesCard can represent its PDF accurately and efficiently. Specifi-
cally, the variables in PPL can characterize most data distribution
types with varied domain size. Attribute correlation is merely a man-
ifestation of the underlying causal pattern, which can be accurately
represented. Moreover, for data with more attributes (larger scale),
the proposed graph reduction inference technique can reduce a

BayesCard: Revitalizing Bayesian Networks for Cardinality Estimation

larger graph into a much smaller one for efficient inference. There-
fore, the inference latency is also stable for various data settings.
System. Both the structure and the distribution parameters of
BayesCard model are interpretable and debuggable. Specifically, a
DB expert can verify a learned structure based on his prior knowl-
edge of data causality (functional dependency in DBs), and validate
the learned parameter using basic probability rules (non-negative
and sum to one). Since the probability inference of BayesCard fol-
lows the Bayesian rule, its performance is logical and predictable.
Furthermore, the compiled VE does not contain any stochasticity,
so the usersâ€™ error is reproducible.

7 EXPERIMENTAL RESULTS
In this section, we empirically demonstrate the superiority of our
BayesCard over other CardEst methods. In the following, Section 7.1
first introduces the experimental setups. Next, Section 7.2 thor-
oughly compares different CardEst methods in terms of the ADS
criteria on single table datasets. Then, Section 7.3 evaluates the
performance and end-to-end query plan execution time on multi-
table datasets. At last, Section 7.4 performs ablation studies on our
proposed algorithms and optimizations in BayesCard method.

7.1 Experimental setups
CardEst methods to compare with. We compare our BayesCard
framework with the following CardEst methods, including both
traditional methods widely used in DBMS and four existing SOTA
DL-based methods. For each ML-based CardEst method, we adopt
the authorsâ€™ source code and apply the same hyper-parameters as
used in the original paper.

1). Histogram is the simplest CardEst method widely used in

DBMS such as Postgres [17].

2). Sampling has been used in DBMS such as MySQL [35]. In our

testing, we randomly sample 1% of all tuples for CardEst.

3). Naru/NeuroCard [52, 53] are DAR-based CardEst methods for

single table and multi-table join queries, respectively.
4). DeepDB [20] is a SPN-based CardEst method.
5). FLAT [58] is an FSPN-based CardEst method.
6). MSCN [21] is the SOTA query-driven CardEst method. For
each dataset, we train it with 105 queries generated in the same
way as the workload.

Our BayesCard framework subsumes BNs with various combi-
nation of structure learning and inference algorithms as described
in previous sections. In Section 7.2 and 7.3, we use an exemplary
BN with Chow-Liu tree structure learning algorithm and compiled
variable elimination inference algorithm with graph reduction opti-
mizations. The comparison of different BNs realizable in BayesCard
and controlled ablation studies are deferred to Section 7.4.
Datasets and query workloads. Our single table experiments are
performed on three datasets:
1).DMV dataset is a real-world dataset consisting of 11,575,483
tuples of vehicle registration information in New York. We use the
same attributes as in [49, 53].
2). CENSUS dataset contains population survey by U.S. Census
Bureau conducted in 1990. This dataset has 2,458,285 tuples and
68 attributes, containing highly correlated attributes. Based on
RDC test [29], we find that more half of the attributes are highly

correlated with at least one other attribute. This dataset is very
large in scale and has very complicated distribution.
3) SYNTHETIC datasets are a collection of human-generated datasets
with varied data distribution skewness, attributes correlation, do-
main size and number of attributes. We generated these datasets
using the similar approach as a recent benchmark study [49]. They
are used to evaluate modelsâ€™ stability w.r.t. changes in data.

For each dataset, we generate 1, 500 selection queries as work-
load. For each query ğ‘„, first we select a subset of attributes as filter
attributes of ğ‘„. For each selected attribute ğ‘, if it represents a con-
tinuous variable, we uniformly generate two values (ğ‘£1, ğ‘£2) from
its value domain and then add the filter predicate â€œğ‘£1 â‰¤ ğ‘ â‰¤ ğ‘£2â€ to
ğ‘„. Otherwise, if ğ‘ is a categorical variable, we uniformly sample ğ‘˜
unique values{ğ‘£1, ğ‘£2, Â· Â· Â· , ğ‘£ğ‘˜ } from its domain and place a predicate
â€œğ‘ IN {ğ‘£1, Â· Â· Â· , ğ‘£ğ‘˜ }â€ in ğ‘„.
4). Multi-table IMDB: We conduct the multi-table experiment on
international movie database (IMDB) benchmark. Prior work [26]
claims that this DB contains complicated data structure and estab-
lishes it to be a good test benchmark for cardinality estimators. We
use JOB-light benchmark query workload with 70 queries proposed
in the original paper [26] and create another workload of 1500
JOB-comp with more comprehensive and complicated queries.

JOB-lightâ€™s IMDB schema contains six tables (title, cast_info,
movie_info, movie_companies, movie_keyword, movie_info_idx) and
five join operations in total where every other tables can only join
with the primary table â€œtitleâ€. Each JOB-light query involves 3-6
tables with 1-4 filter predicates. The filter variety is not very diverse
with equality filters on all attributes but the â€œtitle.production_yearâ€
attribute only. In addition, JOB-lightâ€™s workload only contains 70
queries, which is not enough to account for the variance in model
prediction. Thus, we synthesize 1,500 JOB-comp queries based on
the schema of JOB-light with more number of filter predicates
per query. Each JOB-comp query involves 4-6 tables with 2-7 filter
predicates. The queries are uniformly distributed to each join of
4-6 tables. After determining the join graph, the filter predicates
selection process is similar as in single table cases.

Evaluation metric: We use the Q-error as our evaluation metrics,
which is define as follow:

Q-error = ğ‘šğ‘ğ‘¥ (

Estimated Cardinality
True Cardinality

,

True Cardinality
Estimated Cardinality

)

This evaluation metric is well recognized in DBMS community
and widely used in recent papers on cardinality estimation [16, 20,
47, 52, 53]. We report the 50%(median), 90%, 95% and 100%(worst)
Q-error quantiles as evaluation of estimation accuracy.
Experimental environment: All models are evaluated on Intel(R)
Xeon(R) Platinum 8163 CPU with 64 cores, 128GB DDR4 main
memory, and 1TB SSD. For a fair comparison, we compare the
model inference latency on CPU only since apart from the DAR
model (Naru and NeuroCard) and MSCN, the rest methodsâ€™ inference
algorithms do not support GPU.

7.2 Model evaluation on single tables
In this section, we compare the performance of CardEst methods
in terms of Algorithm and Data criteria.

Dataset

Table 2: Performance of CardEst algorithms on single tables.
Latency (ms)
90%
2.1
1.024
12.32
0.1
79
1.052
86
1.026
5.1
1.124
1.028
0.6
3.4
2.263

50%
Method
1.001
BayesCard
Histogram 1.318
1.004
Sampling
1.003
Naru
1.006
DeepDB
FLAT
1.001
1.210
MSCN

100%
7.641
1 Â· 104
143.0
5.500
108.1
11.37
151.8

95%
1.049
143.6
1.140
1.035
1.193
1.066
4.507

DMV

CENSUS

1.063
BayesCard
Histogram 5.561
Sampling
1.130
1.229
Naru
1.469
DeepDB
1.452
FLAT
2.700
MSCN

1.484
259.8
1.412
2.210
6.295
6.326
15.83

2.052
5 Â· 104
374.2
7.156
178.21
174.93
1 Â· 104

227.5
5 Â· 105
1703
1095
1 Â· 104
1 Â· 104
1 Â· 105

2.4
0.2
113
129
25
25
4.8

Algorithm criteria. We evaluate the CardEst methods from four
aspects: estimation accuracy, inference latency, model size and
training time, and updating effects.
Estimation accuracy: The estimation accuracy on two real-world
single table datasets is reported in Table 2, where the color shade
in each cell corresponds to the rank among different CardEst meth-
ods. When compared with traditional models (Histogram and Sam-
pling), BayesCard achieves 1â€“3 order of magnitude higher accuracy
than both models. When compared with DL-based methods (Naru,
DeepDB and FLAT ), BayesCard has comparable or better estimate
accuracy on DMV dataset, but significantly more accurate on CEN-
SUS dataset. This is because these DL models can accurately repre-
sent the data distribution of DMV, which contains relatively less
attribute correlation and fewer number of attributes. CENSUS, how-
ever, contains seven times larger number of attributes with more
complex attribute correlations. As the learning space grows expo-
nentially with the number of attributes, Naruâ€™s accuracy dropped
significantly. For DeepDB and FLAT, their SPN or FSPN structure
can not well capture the data distribution in presence of a large
number of highly correlated attributes, so their performance also
heavily degrades.
Inference latency: As shown in Table 2, apart from Histogram,
which leverages the attribute independence assumption for fast
inference, BayesCard generally attains the best (1â€“2 orders of mag-
nitude) inference latency among the result methods. Worth noticing
that we observe significant increase in latency from DMV to CEN-
SUS datasets for all methods except for BayesCard. BayesCardâ€™s
inference time appears to be insensitive to the number of attributes,
mainly because the novel graph reduction technique can reduce a
large CENSUS attribute graph to a much smaller one for inference.
Model size and training time: As shown in Figure 4, apart from
the traditional methods, BayesCard achieves the smallest model size
with the fastest training time because the causal pattern of datasets
enables a compact representation of data distribution. Worth notic-
ing that Sampling is a model-free method that does not have model
size or training time, so we do not include it in the figure.
Updating time: We evaluate each methodâ€™s updating effects by
following a similar experimental setup of prior work [49]. Specif-
ically, we create a copy of the original DMV dataset and sort the
tuples based on the value of each column in an ascending order.
Then, we take the first 20% of the data to train a stale model and use

Ziniu Wu

1

, Amir Shaikhha

2

, Rong Zhu

1

1
, Kai Zeng

, Yuxing Han

1

, Jingren Zhou

1

Figure 4: Model storage and training time.

Table 3: Performance of model updates of different CardEst
methods on DMV. The baseline q-error is the 95% q-error
quoted from Table 2 for comparison.

Method
baseline q-error
95% q-error
Update time (s)

BayesCard Histogram Naru DeepDB FLAT
1.066
1.451
257

1.049
1.049
103

1.193
18.83
142

1.035
14.79
1980

143.6
143.6
25

the rest 80% as data insertion updates. This procedure will make
sure that the training dataset has different data distribution than
the testing dataset; otherwise, the stale model would perform well
without model updates. Then, after the model finishes the updating
process, we test the model using the original query workload same
as in Table 2 and report their 95% q-errors and total update time in
Figure 3. Here, we refrain from comparing with the query-driven
method MSCN because it requires a new query workload to update
its model, which is unavailable in our experimental settings.

BayesCard, Histogram, and DeepDB all preserve the original struc-
ture and only incrementally update the parameters, so in general,
they have the fastest update time. Among them, Histogram has the
least amount of parameters to update, so it has the best update time.
We use the method described in the original paper [58] to update
FLAT, which generates new sub-structures to fit the inserted data
distribution, so it is slightly slower than the previous three. Naru
uses the incoming data to fine-tune its pre-trained DNNs for three
epochs, which is significantly slower than others.

After the model updates, we observe that BayesCard has no drop
in estimation accuracy, whereas the deep probabilistic models have
degraded performance. The reasons can be summarized as follow:
(1) BayesCardâ€™s structure captures the data causal pattern, which
often does not change after update; (2) DeepDBâ€™s preserved structure
is not robust against data distribution changes; (3) fine-tuning the
Naruâ€™s underlying DAR model overfits the information from the
20% previously trained data, leading to degraded performance.
Summary: BayesCard attains comparable or better estimation ac-
curacy, lower inference latency, smaller model size, less training and
update time than DL-based models. In addition, BayesCard is 1-3
orders of magnitude more accurate than traditional methods.
Data criteria. We evaluate the stability of CardEst methods in
terms of Data criteria from four aspects: data distribution, attribute
correlation, domain size, and number of attributes.

SYNTHETIC datasets are generated using the similar approach
in a recent benchmark study [49]. Specifically, suppose we would
like to generate a table ğ‘‡ with attributes {ğ‘‡1, . . . ,ğ‘‡ğ‘› } and 106 tu-
ples, where is the ğ‘› denotes the number of attributes (scale). We
generate the first column for ğ‘‡1 using a Pareto distribution (us-
ing scipy.stats.pareto function), with a controlled skewness ğ‘  and
domain size ğ‘‘. For each of the rest attribute ğ‘‡ğ‘– , we generate a col-
umn based on a previous attribute ğ‘‡ğ‘— with ğ‘— < ğ‘–, to control the
correlation ğ‘. For each tuple (ğ‘¡1, . . . , ğ‘¡ğ‘›) in ğ‘‡ , we set ğ‘¡ğ‘– to ğ‘¡ ğ‘— with a

IMDBCENSUSModel training time (Sec)Model size (KB)DMVBayesCard: Revitalizing Bayesian Networks for Cardinality Estimation

Figure 5: Comparing BayesCard and DeepDBâ€™s stability.

probability of ğ‘, and set ğ‘¡ğ‘– to a random value drawn from the Pareto
distribution with the probability of 1 âˆ’ ğ‘.

The experimental results on SYNTHETIC are shown in Figure 5.
Due to space limit, we only plot the comparison results between
BayesCard and DeepDB on the estimation accuracy metric. The
additional experimental results are reported in the appendix of the
technical report [50]. We summarize our observations as follows.
Distribution (s): Similar to the previous study [49], we find that
increasing the Pareto distribution skewness severely degrades the
performance of Naru and Sampling methods, but has only mild
effect on BayesCard and other methods. This is because BayesCard,
Histogram, FLAT, and DeepDB all use (multi-)histograms to repre-
sent distributions, which are robust against distribution changes.
Correlation (c): The increase in ğ‘ has no impact on BayesCard,
mild impact on Sampling, Naru, FLAT and MSCN, and severe im-
pact on Histogram and DeepDB, which make local or global attribute
independence assumptions. BayesCard is able to capture the causal
pattern of the datasets, and thus can represent any attribute corre-
lation accurately.
Domain (d): The increase in the domain size degrades the estima-
tion accuracy for all methods, because increasing ğ‘‘ may increase
the data complexity exponentially as there are ğ‘‘ğ‘› possible values
that a tuple can take. Fortunately, except for Naru, the degrades in
accuracy are within a reasonable range for all other methods.
Scale (n): Similar to domain size, increasing the number of at-
tributes also increases the data complexity exponentially, and thus
we expect to see a decrease in accuracy for all methods. Surpris-
ingly, the performance of BayesCard was not affected by ğ‘› at all.
This is owing to the graph reduction technique, which significantly
reduces the number of attributes involved during inference. This
technique not only improves the inference latency but also increases
the estimation accuracy as potential modeling errors on the reduced
attributes are also eliminated.

Apart from estimation accuracy, BayesCard also maintains very
stable and robust performance in terms of inference latency, model
size, and training time, which is analyzed in Appendix C [50].
Summary: BayesCard is much more stable and robust than other
CardEst methods for datasets with various settings of data.

7.3 Model performance on multi-table dataset
As reported in Table 4 and Figure 4, BayesCard achieves comparable
performance with the current SOTAs on the two query workloads
of the IMDB dataset and preserves its superior inference latency,
lightweight model storage, and fast training. Specifically, the esti-
mation accuracy of BayesCard is comparable to NeuroCard, slightly

JOB-light

Workload

90%
3.534
1006
55.29
4.545
2.500
1.819
19.70

50%
Method
BayesCard
1.300
Histogram 7.318
2.464
Sampling
1.580
NeuroCard
1.318
DeepDB
FLAT
1.150
2.750
MSCN

Table 4: Performance of cardinality estimation algorithms
on IMDB datasets with two query workloads.
100%
19.13
1 Â· 107
4 Â· 104
8.510
39.60
10.86
661.0
4 Â· 104
1 Â· 108
8 Â· 106
1 Â· 105
1 Â· 105
1 Â· 104
1 Â· 105

1.271
BayesCard
Histogram 15.78
3.631
Sampling
1.538
NeuroCard
1.930
DeepDB
FLAT
1.202
4.961
MSCN

Latency (ms)
5.4
0.1
63
673
49
6.8
6.7

95%
4.836
5295
276.1
5.910
3.161
2.247
97.60
86.3
4 Â· 104
1374
81.23
248.0
57.23
447.0

9.053
7480
102.7
9.506
28.30
6.495
45.7

6.2
0.2
101
73
55
10.1
6.6

JOB-Comp

Figure 6: End-to-End evaluation of BayesCard.

better than DeepDB, and slightly worse than FLAT, but with up to
60Ã— smaller model size, and 10Ã— faster training and inference.
End-to-End evaluation on PostgreSQL. Furthermore, we use the
IMDB dataset to demonstrate BayesCardâ€™s behavior in terms of
System criteria. The four aspects of System criteria are rather con-
ceptual and hard to compare quantitatively in experiment, so we
incorporate BayesCard into a commercial DBMS, Postgres 9.6.6, to
show that it can improve the query optimization process of a real
system. Specifically, we evaluate the end-to-end query process-
ing time for JOB-light queries as shown in Figure 6, and compare
BayesCard with the Postgres baseline, FLAT, and optimal result
derived by inserting the true cardinality during query optimization.
We do not compare with other methods since FLAT has established
its SOTA performance in the same experiment, as reported in the
original paper [58]. We observe that:

1) BayesCard improves the Postgres baseline by 13.3%, suggesting
that with more accurate CardEst results, the query optimizer can
generate better query plans with lower execution cost.

2) The improvement of BayesCard is very close to the method
using true cardinality in query compiling (14.2%). This verifies that
the accuracy of BayesCard is sufficient to generate high-quality
query plans. Besides, even though BayesCard has a slightly worse
estimation accuracy, it still marginally outperforms FLAT. Both
methods produce similar execution plans and the marginal gain of
BayesCard over FLAT mainly credits to its faster inference latency.
3) The improvement of BayesCard and FLAT becomes more sig-
nificant on queries joining more tables because the execution plan
for a query joining 2 or 3 is almost fixed. Whereas, for queries
joining more tables, the inaccurate Postgres baseline results may

Top 20% estimation q-error distributionDistribution Skewness s(c = 0.4, d=100, n=10)Attribute Correlation c(s = 1.0, d=100, n=10)Domain Size d(s = 1.0, c = 0.4, n=10)Number of attributes n(s = 1.0, c = 0.4, d=100)2345OverallNumber of tables in query0100200300400Average execution time (s)+1.2%-0.6%-26.5%-10.3%-12.9%+0.3%-1.7%-26.8%-10.1%-13.3%-1.7%-2.3%-28.5%-11.1%-14.2%PostgresFLATBayesCardTrue_cardTable 5: Comparing different structure learning algorithms
of BayesCard on CENSUS.
Infer.

95%

Algorithms

q-error Time (s)

Exact
Greedy
Chow-Liu

1.24
1.88
2.05

16.5
2.45
0.78

Model

Update
Train
Size (mb) Time (min) Time (s)
298
62.1
19.8

1391
442
103

43.7
2.53
0.08

lead to a sub-optimal query plan, while BayesCard and FLAT pro-
viding more accurate CardEst results can find a better plan. This
phenomenon has also observed and explained in [36, 58].
Summary: The integration of BayesCard into Postgres validates it
as a practical counterpart of the CardEst component in Postgres and
also verifies that BayesCard is a system-friendly CardEst method.

7.4 Comparing algorithms within BayesCard
In this section, we compare different BayesCardâ€™s structure learning
algorithms, perform ablation studies on the inference algorithms
and summarize the take-home messages for using BayesCard.
Comparing structure learning algorithms. We report the esti-
mation accuracy, inference latency without any proposed tech-
niques, training time, model size, and update time on CENSUS
dataset for various structure learning algorithms in Table 5. For ex-
act and greedy algorithms, we incorporated the â€œexpert knowledgeâ€
as described in Section 4.3; otherwise, these algorithms become
intractable and can not generate the BNâ€™s structure. We observe
that with a more accurate structure learning algorithm (exact), the
estimate accuracy has a significant improvement, but it sacrifices
the other four dimensions to a great extent. We did not report the
result for DMV and IMDB datasets with a much fewer number of
attributes because their data causal patterns are much simpler and
different structure learning algorithms have similar performance.

Ablation study of inference algorithms. We compare the novel
inference optimizations of BayesCard with the original variable
elimination (VE) and belief propagation (BP) algorithms on a model
learned with Chow-Liu tree algorithm on the CENSUS dataset,
shown in Table 6. We have the following observations: (1) the
latency of original algorithms, VE and BP is unaffordable (780 ms per
query) for practical systems; (2) the graph reduction (GR) and just-
in-time compilation (JIT) optimization do not affect the estimation
accuracy; (3) the GR and JIT alone improve the inference latency
by 5 and 30 times respectively, and 325 times when combined for
VE; (4) the progressive sampling algorithm (PS) produces 4 times
larger estimation error but with significant improvement in latency.
Worth noticing that the inference latency of PS and PS+GR can
be much faster than VE+GR+JIT for BayesCard with a complex
structure (e.g. learned by exact structure learning algorithm).

Take-home messages for BayesCard users. (1) The Chow-Liu
tree structure learning algorithm can efficiently generate a compact
model, which has improved inference latency and stable perfor-
mance over other structure learning algorithms. The degrades in
accuracy can be compensated using â€œexpert knowledgeâ€ described
in Section 4.3. (2) The VE+GR+JIT inference algorithm efficiently
produces exact estimation for BNs with discrete attributes, which

Ziniu Wu

1

, Amir Shaikhha

2

, Rong Zhu

1

1
, Kai Zeng

, Yuxing Han

1

, Jingren Zhou

1

Table 6: Ablation study of different inference algorithms of
BayesCard on CENSUS.

Algorithms
95% q-error
Latency (ms)

VE
2.05
780

BP
2.05
685

VE+GR VE+JIT VE+GR+JIT

2.05
190

2.05
21.9

2.05
2.4

PS
7.47
8.8

PS+GR
7.47
3.5

is debuggable, predictable, reproducible, and very friendly for sys-
tem development. However, PS+GR is a general approach that has
guaranteed efficiency for any complex DAG-structured BN, and
support continuous attributes with any distribution. (3) BayesCard
provides a general CardEst framework for users to explore different
trade-offs to suit their data and system settings.

8 RELATED WORK
We will briefly revisit the existing CardEst methods based on BN
and the supervised CardEst methods.
BN-based methods have been explored decades ago for CardEst.
Getoor et al. [16] used a greedy algorithm for BN structure learning,
the variable elimination for probability inference, and referential in-
tegrity assumption for join estimation. Tzoumas et al. [47] learned
an exact-structured BN and used belief propagation for inference.
Halford et al. [18] adopted the Chow-Liu tree structure learning
algorithm, the VE inference algorithm, and the uniformity assump-
tion for join estimation. However, none of the practical DBMSes
incorporates these methods due to their impractical structure learn-
ing process, intractable inference latency, or inaccurate estimation
for join queries due to over-simplified assumptions.
Supervised CardEst methods use the feedback of past queries to
train ML models, which maps the featurized query ğ‘„ to its actual
cardinality. The first approach using neural networks on cardinal-
ity estimation was published for UDF predicates [24]. Later on, a
regression-based model [1] and a semi-automatic alternative [30]
were presented. Recently, supervised DL-based approaches, used
multi-set convolutional network (MSCN ) [21], tree-LSTM [45], and
lightweight XG-boost model [12] for CardEst. However, the super-
vised learning approaches have two major drawbacks as mentioned
in [20]: (1) Their models neglect the data itself and are not robust
to changes in query workload. (2) Collecting the training data can
be very expensive and training data has to be recollected when
the workload changes. Therefore, in general, query-driven super-
vised ML methods on cardinality estimation are not as flexible and
accurate as data-driven unsupervised ML methods.

9 CONCLUSION
This paper proposes BayesCard, the first framework that unifies the
existing efforts on PPLs and BNs and optimizes them for CardEst in
different data and system settings. BayesCard revitalizes BNs with
new equipments in model construction and probability inference,
which make it a desirable CardEst method satisfying the algorithm,
data and system criteria at the same time. Extensive experimental
studies and end-to-end system deployment establish BayesCardâ€™s
superiority over existing CardEst methods.

Furthermore, BayesCard captures the underlying data causality,
which benefits other data-related tasks. In future work, we plan to
explore the possibility of using BayesCard for other tasks, such as
data cleaning, entity matching, and approximate query processing.

BayesCard: Revitalizing Bayesian Networks for Cardinality Estimation

REFERENCES
[1] Mert Akdere and Ugur Cetintemel. 2012. Learning-based query performance

modeling and prediction. ICDE (2012).

[2] Ankur Ankan and Abinash Panda. 2015. pgmpy: Probabilistic graphical models
using python. In Proceedings of the 14th Python in Science Conference (SCIPY 2015).
Citeseer.

[3] Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj
Pradhan, Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and
Noah D. Goodman. 2019. Pyro: deep universal probabilistic programming. Journal
or Machine learning research (2019).

[4] Remco R Bouckaert. 1993. Probabilistic network construction using the minimum

description length principle. ECSQURU (1993).

[5] Supriyo Chakraborty, Richard Tomsett, Ramya Raghavendra, Daniel Harborne,
Moustafa Alzantot, Federico Cerutti, Srivastavam Mani, Alun Preece, Simon Julier,
Raghuveer M. Rao, Troy D. Kelley, Dave Braines, Murat Sensoyk, Christopher J.
Willis, and Prudhvi Gurram. 2017. Interpretability of deep learning models: A
survey of results. In 2017 IEEE SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI.
1â€“6.

[6] Surajit Chaudhuri, Venkatesh Ganti, and Luis Gravano. 2004. Selectivity es-
timation for string predicates: Overcoming the underestimation problem. In
Proceedings. 20th International Conference on Data Engineering. IEEE, 227â€“238.
[7] David Maxwell Chickering. 1996. Learning Bayesian networks is NP-complete.

In Learning from Data: Artificial Intelligence and Statistics (1996).

[8] David Maxwell Chickering. 2002. Optimal structure identification with greedy

search. JMLR (2002).

[9] David Maxwell Chickering and David Heckerman. 1997. Efficient approximations
for the marginal likelihood of Bayesian networks with hidden variables. ML 29
(1997).

[10] C. K. Chow and C. N. Liu. 1968. Approximating discrete probability distributions
with dependence trees. IEEE transactions on Information Theory (1968).

[11] Adnan Darwiche. 2009. Modeling and reasoning with Bayesian networks. Cam-

bridge university press.

[12] Anshuman Dutt, Chi Wang, Azade Nazi, Srikanth Kandula, Vivek Narasayya,
and Surajit Chaudhuri. 2019. Selectivity estimation for range predicates using
lightweight models. Proceedings of the VLDB Endowment 12, 9 (2019), 1044â€“1057.
[13] Wenfei Fan, Floris Geerts, Jianzhong Li, and Ming Xiong. 2010. Discovering
conditional functional dependencies. IEEE Transactions on Knowledge and Data
Engineering 23, 5 (2010), 683â€“698.

[14] Fei Fu and Qing Zhou. 2013. Learning sparse causal Gaussian networks with
experimental intervention: regularization and coordinate descent. JASA (2013).
[15] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. 2015. MADE:
International Conference on

Masked autoencoder for distribution estimation.
Machine Learning (2015), 881â€“889.

[16] Lise Getoor and Daphne Taskar, Ben andKoller. 2001. Selectivity estimation using

probabilistic models. SIGMOD (2001).

[17] The PostgreSQL Global Development Group. 2018. Documentation PostgreSQL

[18] Max Halford, Philippe Saint-Pierre, and Franck Morvan. 2019. An approach based
on bayesian networks for query selectivity estimation. DASFAA 2 (2019).
[19] David Heckerman, David Geiger, and David Maxwell Chickering. 1995. Learning
Bayesian networks: The combination of knowledge and statistical data. ML 20
(1995).

[20] Benjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina, Ker-
sting Kristian, and Carsten Binnig. 2020. DeepDB: Learn from Data, not from
Queries! PVLDB (2020).

[21] Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter Boncz, and
Alfons Kemper. 2019. Learned Cardinalities: Estimating correlated joins with
deep learning. CIDR (2019).

[22] Dophan Koller and Nir Friedman. 2009. Probabilistic Graphical Models Principles

and Techniques. MIT Press.

[23] Tejas D Kulkarni, Pushmeet Kohli, Joshua B Tenenbaum, and Vikash Mansinghka.
2015. Picture: A probabilistic programming language for scene perception. In
Proceedings of the ieee conference on computer vision and pattern recognition.
4390â€“4399.

[24] Seetha Lakshmi and Shaoyu Zhou. 1998. Selectivity estimation in extensible

databases â€“ A neural network. VLDB (1998).

[25] Hao-Chih Lee, Matteo Danieletto, Riccardo Miotto, Sarah T Cherng, and Joel T
Dudley. 2019. Scaling structural learning with NO-BEARS to infer causal tran-
scriptome networks. arXiv e-prints (2019), 1911.00081.

[26] Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter Boncz, Alfons Kemper, and
Thomas Neumann. 2015. How Good Are Query Optimizers, Really? Proc. VLDB
Endow. 9 (2015), 204â€“215.

[27] Viktor Leis, Bernhard Radke, Andrey Gubichev, Atanas Mirchev, Peter Boncz,
Alfons Kemper, and Thomas Neumann. 2018. Query optimization through the
looking glass, and what we found running the join order benchmark. The VLDB
Journal (2018).

[28] Guy Lohman. 2014. Is query optimization a solved problem? SIGMOD (2014).

10.3.

[29] David Lopez-Paz, Philipp Hennig, and Bernhard SchÃ¶lkopf. 2013. The randomized

dependence coefficient. In NIPS. 1â€“9.

[30] Tanu Malik, Randal Burns, and Nitesh Chawla. 2007. A black-box approach to

query cardinality estimation. CIDR (2007).

[31] James Martens and Venkatesh Medabalimi. 2014. On the expressive efficiency of

sum product networks. arXiv preprint arXiv:1411.7717 (2014).

[32] Yoss Matias, Jeffery Scott Vitter, and Min Wang. 1998. Wavelet-based histograms

for selectivity estimation. SIGMOD (1998).

[33] T. Minka, J.M. Winn, J.P. Guiver, Y. Zaykov, D. Fabian, and J. Bronskill. 2018.
/Infer.NET 0.3. Microsoft Research Cambridge. http://dotnet.github.io/infer.
[34] Thomas Neumann. 2011. Efficiently Compiling Efficient Query Plans for Modern

Hardware. PVLDB 4, 9 (2011), 539â€“550.

[35] Corporation Oracle. 2020. MySQL 8.0 Reference Manual.
[36] Matthew Perron, Zeyuan Shang, Tim Kraska, and Michael Stonebraker. 2019.
How I learned to stop worrying and love re-optimization. In ICDE. 1758â€“1761.
[37] Hoifung Poon and Pedro Domingos. 2011. Sum-product networks: A New Deep
Architecture. IEEE International Conference on Computer Vision Workshops (2011).
[38] John Salvatier, Thomas V. Wiecki, and Christopher Fonnesbeck. 2016. Proba-
bilistic programming in Python using PyMC3. PeerJ Computer Science (2016).
https://doi.org/10.7717/peerj-cs.55

[39] Mauro Scanagatta, Giorgio Corani, Cassio P De Campos, and Marco Zaffalon.
2016. Optimal structure identification with greedy search. NeurIPS (2016).
[40] Jacob Schreiber. 2018. Pomegranate: fast and flexible probabilistic modeling in

python. Journal of Machine Learning Research 18, 164 (2018), 1â€“6.

[41] Jacob Schreiber. 2018. pomegranate: Fast and Flexible Probabilistic Modeling in

Python. jmlr (2018).

[42] P. G. Selinger, M. M. Astrahan, D. D. Chamberlin, R. A. Lorie, and T. G. Price.
1979. Access path selection in a relational database management system. ACM
SIGMOD international conference on Management of data (1979).

[43] Amir Shaikhha, Yannis Klonatos, and Christoph Koch. 2018. Building Efficient
Query Engines in a High-Level Language. ACM Transactions on Database Systems
43, 1, Article 4 (April 2018), 45 pages.

[44] Amir Shaikhha, Yannis Klonatos, Lionel Parreaux, Lewis Brown, Mohammad
Dashti, and Christoph Koch. 2016. How to Architect a Query Compiler. In
Proceedings of the 2016 International Conference on Management of Data (San
Francisco, California, USA) (SIGMODâ€™16). ACM, New York, NY, USA, 1907â€“1922.
[45] Ji Sun and Guoliang Li. 2019. An end-to-end learning-based cost estimator. arXiv

preprint arXiv:1906.02560 (2019).

[46] Dustin Tran, Alp Kucukelbir, Adji B. Dieng, Maja Rudolph, Dawen Liang, and
David M. Blei. 2016. Edward: A library for probabilistic modeling, inference, and
criticism. arXiv preprint arXiv:1610.09787 (2016).

[47] Kostas Tzoumas, Amol Deshpande, and Christian S Jensen. 2011. Lightweight
graphical models for selectivity estimation without independence assumptions.
Proceedings of the VLDB Endowment 4, 11 (2011), 852â€“863.

[48] Kostas Tzoumas, Amol Deshpande, and Christian S Jensen. 2013. Efficiently
adapting graphical models for selectivity estimation. Proceedings of the VLDB
Endowment 1, 22 (2013).

[49] Xiaoying Wang, Changbo Qu, Weiyuan Wu, Jiannan Wang, and Qingqing
Are We Ready For Learned Cardinality Estimation?

Zhou. 2020.
arXiv:2012.06743 [cs.DB]

[50] Ziniu Wu and Amir Shaikhha. 2020. BayesCard: Revitalizing Bayesian Networks

for Cardinality Estimation. arXiv:2012.14743 [cs.DB]

[51] Ziniu Wu, Rong Zhu, Andreas Pfadler, Yuxing Han, Jiangneng Li, Zhengping
Qian, Kai Zeng, and Jingren Zhou. 2020. FSPN: A New Class of Probabilistic
Graphical Model. arXiv:2011.09020 [cs.AI]

[52] Zongheng Yang, Amog Kamsetty, Sifei Luan, Eric Liang, Yan Duan, Xi Chen, and
Ion Stoica. 2020. NeuroCard: One Cardinality Estimator for All Tables. arxiv
(2020).

[53] Zongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Xi Chen,
Pieter Abbeel, Joseph M. Hellerstein, Sanjay Krishnan, and Ion Stoica. 2019. Deep
unsupervised cardinality estimation. Proceedings of the VLDB Endowment (2019).
[54] Qiaoling Ye, Arash Amini, and Qing Zhou. 2020. Optimizing regularized cholesky
score for order-based learning of bayesian networks. IEEE transactions on pattern
analysis and machine intelligence (2020).

[55] Changhe Yuan, Brandon Malone, and Xiaojian Wu. 1984. Learning Optimal
Bayesian Networks Using A* Search. Wadsworth and Brooks, Monterey, CA
(1984).

[56] Zhuoyue Zhao, Robert Christensen, Feifei Li, Xiao Hu, and Ke Yi. 2018. Random
sampling over joins revisited. In Proceedings of the International Conference on
Management of Data (2018).

[57] Rong Zhu, Andreas Pfadler, Ziniu Wu, Yuxing Han, Xiaoke Yang, Feng Ye, Zhen-
ping Qian, Jingren Zhou, and Bin Cui. 2021. Efficient and Scalable Structure
Learning for Bayesian Networks: Algorithms and Applications. ICDE (2021).
[58] Rong Zhu, Ziniu Wu, Yuxing Han, Kai Zeng, Andreas Pfadler, Zhengping Qian,
Jingren Zhou, and Bin Cui. 2020. FLAT: Fast, Lightweight and Accurate Method
for Cardinality Estimation. arXiv preprint arXiv:2011.09022 (2020).

ğ‘˜ = ğ‘¡ â€²

A PROOF OF THEOREM 1:
Theorem 1. Given a BN and its defined DAG ğº = (ğ‘‰ , ğ¸), representing
a table ğ‘‡ with attributes ğ‘‰ = {ğ‘‡1, Â· Â· Â·ğ‘‡ğ‘›}, and a query ğ‘„ = (ğ‘‡ â€²
1 =
ğ‘– âˆˆ ğ‘‰ . Let ğº â€² = (ğ‘‰ â€², ğ¸ â€²) be a sub-graph of
ğ‘˜ ) where ğ‘‡ â€²
1 âˆ§ Â· Â· Â· âˆ§ğ‘‡ â€²
ğ‘¡ â€²
ğº where ğ‘‰ â€² equals (cid:208)1â‰¤ğ‘– â‰¤ğ‘˜ ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ (ğ‘‡ â€²
ğ‘– ), and ğ¸ â€² equals all edges in
ğ¸ with both endpoints in ğ‘‰ â€². ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ (ğ‘‡ â€²
ğ‘– ) includes all parent nodes
of ğ‘‡ â€²
ğ‘– and all parents of parent node recursively. Then, performing VE
of BN on full graph G is equivalent to running VE on reduced graph
Gâ€™.

1 , Â· Â· Â· ,ğ‘‡ â€²

Proof of Theorem 1: Given the probability query ğ‘„ on original
graph ğº and the reduced graph ğº â€² defined above, we define ğ‘„ğ‘‰ =
{ğ‘‡ â€²
1 , Â· Â· Â·ğ‘‡ â€²â€²
. In this proof, we will only
ğ‘›âˆ’ğ‘˜
show that running VE on ğº is equivalent to running VE on ğº â€². Then
the proof for progressive sampling naturally follows as it is directly
approximating the computation of VE.

ğ‘˜ } and ğ‘‰ /ğ‘„ğ‘‰ = ğ‘‡ â€²â€²

First, recall that by law of total probability, we have the following

Equation 2.

ğ‘ƒğ‘‡ (ğ‘‡ â€²

1 = ğ‘¡ â€²

1, Â· Â· Â· ,ğ‘‡ â€²

ğ‘˜ = ğ‘¡ â€²

ğ‘˜ ) =

âˆ‘ï¸

Â· Â· Â·

âˆ‘ï¸

ğ‘¡ â€²â€²
1 âˆˆğ· (ğ‘‡ â€²â€²
1 )

ğ‘›âˆ’ğ‘˜ âˆˆğ· (ğ‘‡ â€²â€²
ğ‘¡ â€²â€²

ğ‘›âˆ’ğ‘˜ )

(cid:34)

(cid:214)

ğ‘‡ â€²
ğ‘– âˆˆğ‘„ğ‘‰

(cid:214)

ğ‘ƒğ‘‡ (ğ‘‡ â€²

ğ‘– = ğ‘¡ â€²

ğ‘– |ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²

ğ‘– ))âˆ—

ğ‘ƒğ‘‡ (ğ‘‡ â€²â€²

ğ‘– = ğ‘¡ â€²â€²
ğ‘–

|ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²â€²

ğ‘– ))

(cid:35)

(2)

ğ‘‡ â€²â€²
ğ‘– âˆˆğ‘‰ /ğ‘„ğ‘‰

ğ‘– and ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²â€²
where ğ· (ğ‘‡ â€²â€²
ğ‘– ) denotes the domain of attribute ğ‘‡ â€²â€²
ğ‘– )
denotes the parents of node ğ‘‡ â€²â€²
in graph ğº. For simplicity, here
ğ‘–
we refer to ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²â€²
ğ‘— âˆˆ ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²
ğ‘— = ğ‘¡ â€²â€²
ğ‘– ) as (ğ‘‡ â€²â€²
ğ‘– )). The
VE algorithm are essentially computing Equation 2 by summing
out one attribute from ğ‘‰ /ğ‘„ğ‘‰ at a time until all ğ‘‡ â€²â€²
ğ‘– âˆˆ ğ‘‰ /ğ‘„ğ‘‰ are
eliminated [22].

ğ‘— , âˆ€ ğ‘‡ â€²â€²

Alternatively, we can derive the following Equation 3 by law of

total probability and conditional independence assumption.

Ziniu Wu

1

, Amir Shaikhha

2

, Rong Zhu

1

1
, Kai Zeng

, Yuxing Han

1

, Jingren Zhou

1

includes all parent nodes of ğ‘‡ â€²
cursively. Let |ğ‘‰ â€²| = ğ‘›â€² and ğ‘‰ â€²/ğ‘„ğ‘‰ = ğ‘‡ â€²â€²â€²
sively write out ğ‘ƒğ‘‡
result in Equation 4.

(cid:16) (cid:208) ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²

ğ‘— )1â‰¤ ğ‘— â‰¤ğ‘˜

ğ‘– and all parents of parent node re-
1 , Â· Â· Â· ,ğ‘‡ â€²â€²â€²
. We can recur-
ğ‘›â€²âˆ’ğ‘˜
(cid:17) using Equation 3 and

ğ‘ƒğ‘‡ (ğ‘‡ â€²

1 = ğ‘¡ â€²

1, Â· Â· Â· ,ğ‘‡ â€²

ğ‘˜ = ğ‘¡ â€²

ğ‘˜ ) =

âˆ‘ï¸

Â· Â· Â·

âˆ‘ï¸

1 âˆˆğ· (ğ‘‡ â€²â€²â€²
ğ‘¡ â€²â€²â€²
1 )

ğ‘›â€²âˆ’ğ‘˜ âˆˆğ· (ğ‘‡ â€²â€²â€²
ğ‘¡ â€²â€²â€²

ğ‘›âˆ’ğ‘˜ )

(cid:34)

(cid:214)

ğ‘‡ â€²
ğ‘– âˆˆğ‘„ğ‘‰

(cid:214)

ğ‘ƒğ‘‡ (ğ‘‡ â€²

ğ‘– = ğ‘¡ â€²

ğ‘– |ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²

ğ‘– ))âˆ—

ğ‘ƒğ‘‡ (ğ‘‡ â€²â€²â€²

ğ‘– = ğ‘¡ â€²â€²â€²

ğ‘–

|ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²â€²â€²

ğ‘–

))

(cid:35)

(4)

ğ‘‡ â€²â€²â€²
ğ‘– âˆˆğ‘‰ /ğ‘„ğ‘‰

Equation 4 has the same form as Equation 2 with less attributes in
the summation. Thus the VE algorithm [22] can compute Equation 4
by eliminating one attribute from ğ‘‰ â€²/ğ‘„ğ‘‰ at a time. Thus running
VE on ğº is equivalent to running VE on ğº â€².

B COMPUTING THE DEPENDENCE LEVEL

BETWEEN TABLES

We use the randomized dependence coefficient (RDC) [29] as a
measure of dependence level between two attributes. RDC is invari-
ant with respect to marginal distribution transformations and has
low computational cost and it is widely used in many statistical
methods [20, 51]. The complexity of RDC is roughly ğ‘‚ (ğ‘› âˆ— ğ‘™ğ‘œğ‘”(ğ‘›))
where n is the sample size for the two attributes.

Figure 7: PRM Ensemble learning algorithm demonstration

ğ‘ƒğ‘‡ (ğ‘‡ â€²

1 = ğ‘¡ â€²

ğ‘˜ = ğ‘¡ â€²
ğ‘˜ )

1, Â· Â· Â· ,ğ‘‡ â€²
âˆ‘ï¸

âˆ‘ï¸

B.1 Calculating the pairwise RDC score

between two tables

=

=

(cid:34)

ğ‘– âˆˆ(cid:208) ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²
ğ‘‡ â€²â€²

ğ‘— )1â‰¤ ğ‘— â‰¤ğ‘˜

ğ‘– âˆˆğ· (ğ‘‡ â€²â€²
ğ‘¡ â€²â€²
ğ‘– )

(cid:34)

ğ‘ƒğ‘‡

(cid:16)
1 = ğ‘¡ â€²
ğ‘‡ â€²

1, Â· Â· Â· ,ğ‘‡ â€²

ğ‘˜ = ğ‘¡ â€²
ğ‘˜ |

(cid:216)

(ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²

ğ‘— )1â‰¤ ğ‘— â‰¤ğ‘˜ )

(cid:17)

âˆ—

ğ‘ƒğ‘‡

(cid:16) (cid:216) ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²

ğ‘— )1â‰¤ ğ‘— â‰¤ğ‘˜

(cid:35)

(cid:17)

âˆ‘ï¸

âˆ‘ï¸

ğ‘– âˆˆ(cid:208) ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²
ğ‘‡ â€²â€²

ğ‘— )1â‰¤ ğ‘— â‰¤ğ‘˜

ğ‘– âˆˆğ· (ğ‘‡ â€²â€²
ğ‘¡ â€²â€²
ğ‘– )

(cid:214)

ğ‘ƒğ‘‡ (ğ‘‡ â€²

ğ‘— = ğ‘¡ â€²

ğ‘— |ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²

ğ‘— )) âˆ— ğ‘ƒğ‘‡

ğ‘‡ â€²
ğ‘— âˆˆğ‘„ğ‘‰

(cid:16) (cid:216) ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²

ğ‘— )1â‰¤ ğ‘— â‰¤ğ‘˜

(cid:35)

(cid:17)

(3)

where ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  (ğ‘‡ â€²â€²

which is the same as parents of nodeğ‘‡ â€²â€²
reduced graph ğº â€² where ğ‘‰ â€² = (cid:208)1â‰¤ğ‘– â‰¤ğ‘˜ ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ (ğ‘‡ â€²

ğ‘– ) denotes the parents of node ğ‘‡ â€²â€²
in graph ğº,
ğ‘–
in graph ğº â€². By definition of
ğ‘–
ğ‘– ). ğ´ğ‘›ğ‘ğ‘’ğ‘ ğ‘¡ğ‘œğ‘Ÿ (ğ‘‡ â€²
ğ‘– )

Recall Figure 7, we have a DB schema with 11 tables ğ´, Â· Â· Â· , ğ¾
and their join tables are defined as a tree T on the left image. In
addition, we have unbiased samples S of the full outer join of all
tables in T using the previously mentioned approach [56]. Now
consider ğ‘‡ , ğ‘… âˆˆ ğ´, Â· Â· Â· , ğ¾ as two random tables in this schema with
attributes ğ‘‡1, Â· Â· Â· ,ğ‘‡ğ‘› and ğ‘…1, Â· Â· Â· , ğ‘…ğ‘š respective. We can compute
the pairwise RDC score between attributes ğ‘‡ğ‘– and ğ‘… ğ‘— , ğ‘…ğ·ğ¶ğ‘– ğ‘— based
on S, as described in [29]. Then we take the average as the level of
dependence between ğ‘‡ and ğ‘… in the following Equation 5.
âˆ‘ï¸

âˆ‘ï¸

ğ‘…ğ·ğ¶ğ‘–,ğ‘— /(ğ‘› âˆ— ğ‘š)

(5)

1â‰¤ğ‘– â‰¤ğ‘›

1â‰¤ ğ‘— â‰¤ğ‘š

Thus, we can compute the dependence level matrix ğ‘€ of size
11 Ã— 11 with each entry specifying the dependence level between
two tables in the schema. Then the edge weights of original T on
the left image can be directly taken from ğ‘€. The complexity of
calculating ğ‘€ is thus ğ‘‚ (ğ‘š2 âˆ— |S| âˆ— ğ‘™ğ‘œğ‘”(|S|)) where m is the total
number attributes in all tables.

ABCDEFGHIJK0.10.30.50.80.70.80.90.60.60.71234(a). kâ€™= 2A,DB,EC,FH,KGIJ0.10.40.45120.60.750.7(b). kâ€™= 3A,DB,E,IC,F,GH,KJ(c). ResultCorrelation (c): For the inference latency and model size, the in-
crease of ğ‘ has negative impact on DeepDB and FLAT, as their models
become larger on more correlated data. However, FLAT behaves
well on very highly correlated data since it can split them with other
attributes to reduce the model size. The impact of ğ‘ on BayesCard is
mild, whose model size is still affordable. For the training time, the
increase of ğ‘ has impact on algorithms except Histogram and Sam-
pling. This is also reasonable as they need more time to model the
complex distribution. Note that, for each setting of ğ‘, the training
time of our BayesCard is still much less than them.
Domain (d): For the inference latency, model size and training time,
the increase of ğ‘‘ has significant impact on Naru, DeepDB and FLAT.
This is increasing the number of attributes would increases the data
complexity exponentially, so they need more neurons or nodes to
model the distribution. Whereas, the impact on our BayesCard is
much mild.
Scale (n): Similar to domain size, increasing the number of at-
tributes also increases the data complexity exponentially, and thus
we expect to see an increase in latency, model size and traininbg
time for almost all methods. However, in comparison with Naru,
DeepDB, FLAT and MSCN, the impact on BayesCard is not very
significant.
Summary: In comparison with DL-based CardEst methods, our
BayesCard attains very stable and robust performance in terms of
inference latency, model size and training time.

BayesCard: Revitalizing Bayesian Networks for Cardinality Estimation

B.2 Calculating the pairwise RDC score

between two set of tables

During the PRM ensemble construction procedure, we sometimes
need to calculate the dependence level between two sets of tables,
such as the dependence level of ğ´, ğ· and ğ», ğ¾ as in the right image
of Figure 7. Similarly to the previous cases in Section B.1, this value
can be directly computed from ğ‘€.

Take ğ´ğ‘¡ğ‘¡ (ğ‘‡ ) denotes the set of attributes in table T. Same as Equa-
tion 5, the level of dependence between ğ´, ğ· and ğ», ğ¾ is defined as
Equation 7.

âˆ‘ï¸

âˆ‘ï¸

ğ‘…ğ·ğ¶ğ‘ğ‘‘,â„ğ‘˜ /

ğ‘ğ‘‘ âˆˆğ´ğ‘¡ğ‘¡ğ‘Ÿ ( {ğ´,ğ· })

â„ğ‘˜ âˆˆğ´ğ‘¡ğ‘¡ğ‘Ÿ ( {ğ»,ğ¾ })

(cid:16)

(cid:32)

|ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ´) + ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ·)| âˆ— |ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ» ) + ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ¾)|

(cid:17)

âˆ‘ï¸

âˆ‘ï¸

ğ‘…ğ·ğ¶ğ‘,â„

ğ‘ âˆˆğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ´)
âˆ‘ï¸

â„ âˆˆğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ» )

âˆ‘ï¸

ğ‘…ğ·ğ¶ğ‘,ğ‘˜

ğ‘ âˆˆğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ´)
âˆ‘ï¸

ğ‘˜ âˆˆğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ¾)
âˆ‘ï¸

ğ‘‘ âˆˆğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ·)

â„ âˆˆğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ» )

âˆ‘ï¸

âˆ‘ï¸

ğ‘‘ âˆˆğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ·)

ğ‘˜ âˆˆğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ¾)

ğ‘…ğ·ğ¶ğ‘‘,â„

(cid:33)

ğ‘…ğ·ğ¶ğ‘‘,ğ‘˜

=

+

+

+

(cid:16)

/

|ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ´) + ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ·)| âˆ— |ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ» ) + ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ¾)|

(cid:16)

=

ğ‘€ [ğ´, ğ» ] âˆ— |ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ´)| âˆ— |ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ» )|

+ ğ‘€ [ğ´, ğ¾] âˆ— |ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ´)| âˆ— |ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ¾)|
+ ğ‘€ [ğ·, ğ» ] âˆ— |ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ·)| âˆ— |ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ» )|
(cid:17)

+ ğ‘€ [ğ·, ğ¾] âˆ— |ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ·)| âˆ— |ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ¾)|

(cid:16)

/

|ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ´) + ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ·)| âˆ— |ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ» ) + ğ´ğ‘¡ğ‘¡ğ‘Ÿ (ğ¾)|

(cid:17)

(cid:17)

(6)

(7)

Thus the weight of the edge can be updated quickly knowing

the pre-computed ğ‘€ and the number of attributes in each table.

C ADDITIONAL EXPERIMENTAL RESULTS
The addition experiments on SYNTHETIC comparing BayesCard
with other methods are reported in Table 7 and Table 8. Please
note that we do not fine-tune the hyper-parameters of the DL-
based methods since the training time on all the datasets take so
long that we can not afford to explore different hyper-parameters.
But we believe that the experimental results are enough to show
the insights. On the other hand, BayesCard do not have hyper-
parameters to fine-tune, which is another advantage of our methods.
We summarize our observations as follows.
Distribution (s): For the inference latency and model size, we find
that increasing the Pareto distribution skewness would degrade the
performance of DeepDB and FLAT, but has little affect on all other
methods. This is because DeepDB and FLAT tend to generate larger
models on more complex data. As a result, their training time also
improves w.r.t. the skewness level. The training time of Naru and
MSCN also grows w.r.t. skewness level, as their underlying DNNs
need more time to model the complex distributions.

Table 7: Stability performance of different CardEst method w.r.t. changes in data distribution skewness and correlation.

Ziniu Wu

1

, Amir Shaikhha

2

, Rong Zhu

1

1
, Kai Zeng

, Yuxing Han

1

, Jingren Zhou

1

CardEst
Methods

Algorithm
Criteria

Distribution Skewness (s)
c=0.4, d=100, n=10

BayesCard

Histogram

Sampling

Naru

DeepDB

FLAT

MSCN

Accuracy (95% q-error)
Latency (ms)
Model size (kb)
Training time (s)
Accuracy (95% q-error)
Latency (ms)
Model size (kb)
Training time (s)
Accuracy (95% q-error)
Latency (ms)
Model size (kb)
Training time (s)
Accuracy (95% q-error)
Latency (ms)
Model size (kb)
Training time (s)
Accuracy (95% q-error)
Latency (ms)
Model size (kb)
Training time (s)
Accuracy (95% q-error)
Latency (ms)
Model size (kb)
Training time (s)
Accuracy (95% q-error)
Latency (ms)
Model size (kb)
Training time (s)

s=0
1.06
3.5
534
17.5
136
0.1
11.5
3.1
1.03
52
-
-

1.03
58
3670
4460
4.51
4.4
701
131
1.06
0.6
76
91
55.1
1.1
3200
1203

s=0.3

1.09
2.8
530
18.2
112
0.1
9.7
3.0
1.67
54
-
-
1.22
67
3670
4910
4.89
4.6
597
133
1.15
0.9
101
93
160
1.1
3200
1208

s=0.6
1.25
3.7
538
17.3
195
0.1
8.2
3.1
2.87
52
-
-
1.78
58
3670
4879
15.1
7.5
834
197

1.23
0.6
80
127
105
1.0
3200
959

s=1.0

1.49
2.4
529
19.2
240
0.1
8.8
4.4
2.21
60
-
-
3.62
62
3670
5503
14.0
7.3
1107
247
1.76
0.5
75
142
94
1.1
3200
1430

s=1.5
2.39
2.2
403
16.8
219
0.1
7.3
3.9
55.9
52
-
-
28.8
60
3670
5908
14.2
5.8
1104
271

2.25
1.5
430
240
129
1.3
3200
871

s=2.0
2.28
3.0
514
14.4
277
0.1
7.5
4.1
142.1
49
-
-
21.4
66
3670
6371
19.0
9.2
1315
280

2.11
1.7
580
253
340
1.0
3200
1770

c=0

1.32
0.1
9.5
5.2

1.32
0.1
9.5
3.0
1.78
51
-
-
1.76
73
3670
1702
1.32
0.1
9.5
5.5
1.32
0.1
9.5
5.5
51.3
1.0
3200
922

c=0.8

c=0.6
2.13
1.6
478
37.9
1403
0.1
9.7
3.0
4.07
47
-
-

Attribute Correlation (c)
s=1.0, d=100, n=10
c=0.4
1.48
2.4
508
17.3
240
0.1
8.8
3.4
2.21
61
-
-
3.62
62
3670
5503
15.1
7.3
1198
244.2
1.76
0.5
75
244.2
94
1.2
3200
1432

1.49
3.3
605
21.3
2 Â· 104
0.1
8.4
3.3
2.10
52
-
-
1.71
66
3670
4962
663
16.4
5532
2570
1.73
17.8
1889
1370
544
1.3
3200
1831

2.09
61
3670
9915
117
10.6
1864
421
2.11
4.1
1201
629
145
1.0
3200
955

c=0.2
1.28
2.9
525
19.2
73.2
0.1
8.3
2.9
1.62
53
-
-

1.23
65
3670
4702
3.58
4.3
570
131
1.27
0.7
103
133
54.0
1.4
3200
935

c=1.0

1.00
2.1
396
45.6
9 Â· 104
0.1
9.1
3.1
1.03
52
-
-
1.10
59
3670
1308
108
13.2
1907
830

1.00
0.2
4.7
17.0
620
1.1
3200
880

Table 8: Scalability performance of different CardEst method w.r.t. changes in data domain size and number of attributes.

CardEst
Methods

Algorithm
Criteria

Domain Size (d)
s=1.0, c=0.4, n=10

BayesCard

Histogram

Sampling

Naru

DeepDB

FLAT

MSCN

Accuracy (95% q-error)
Latency (ms)
Model size (kb)
Training time (s)
Accuracy (95% q-error)
Latency (ms)
Model size (kb)
Training time (s)
Accuracy (95% q-error)
Latency (ms)
Model size (kb)
Training time (s)
Accuracy (95% q-error)
Latency (ms)
Model size (kb)
Training time (s)
Accuracy (95% q-error)
Latency (ms)
Model size (kb)
Training time (s)
Accuracy (95% q-error)
Latency (ms)
Model size (kb)
Training time (s)
Accuracy (95% q-error)
Latency (ms)
Model size (kb)
Training time (s)

d=10
1.29
1.0
19.3
11.1
15.2
0.1
0.8
1.1
1.95
59
-
-

1.02
32
3140
1032
1.08
1.3
29.5
25
1.08
0.5
16.1
15.5
53.0
0.9
3200
1179

d=100

d=500

d=1000

1.49
2.4
542
14.9
240
0.1
9.8
3.0
2.21
55
-
-
3.62
62
3670
5503
10.0
7.5
834
197
1.76
0.6
75.3
142
94.4
1.1
3200
1430

1.05
2.8
982
33.4
498
0.2
14.0
4.3
2.09
60
-
-
7.30
81
6135
5607
15.1
8.8
1234
769
1.35
1.5
310
198
106
1.1
3200
1329

1.04
11.5
1280
48.7
1066
0.4
12.8
6.7
4.93
63
-
-
89.4
115
8747
6489
107
11.9
1910
2310
1.17
18.0
2701
2670
188
1.1
3200
1398

d=5000
25.3
2.2
168
7.3
2 Â· 104
0.8
13.2
11.5

21.2
57
-
-
292
154
2 Â· 104
5980
61
10.7
1781
4155
27.6
15.9
1980
1535
173
1.0
3200
1530

d=10000
49.1
3.6
417
6.5
6 Â· 104
0.1
14.3
15.0
57.9
59
-
-
1783
260
4 Â· 104
1 Â· 104
213
19.0
3974
1 Â· 104
44.0
49.7
5732
9721
290
1.2
3200
1230

n=2
1.04
0.4
64.2
2.01
19.3
0.1
1.5
0.8
1.04
45
-
-
1.39
33
2447
1157
1.04
0.6
35.0
21

1.02
0.4
15.0
9.7
18.8
0.4
2430
719

n=5
1.12
1.5
236
4.73
103
0.1
4.0
1.6
1.26
60
-
-

1.09
37
3050
4201
1.17
1.4
129
65

1.09
0.5
49.9
48.6
40.5
0.9
2871
821

Number of Attributes (n)
s=1.0, c=0.4, d=100
n=10

n=50

n=100

1.49
2.4
542
14.5
240
0.1
10.1
3.1
2.21
77
-
-
3.62
62
3670
5503
15.1
7.5
834
197
1.76
0.5
75.3
142
94.4
1.1
3200
1430

2.58
4.7
2820
113
3 Â· 104
0.5
48.8
16.4
342
109
-
-
121.0
225
6673
6930
257
25.4
6710
3698
255
25.9
6908
4017
1783
1.3
3328
1600

1.97
11.3
5400
576
1 Â· 105
0.4
173
44.8
2 Â· 104
123
-
-
475
473
8010
1 Â· 104
1490
67.1
3 Â· 104
8930
2015
66.0
3 Â· 104
1 Â· 104
8084
1.8
3609
2031

n=200

3.02
15.1
1 Â· 104
1907
9 Â· 105
0.7
308
90.7
3 Â· 104
135
-
-
1098
726
1 Â· 104
2 Â· 104
1 Â· 104
109
9 Â· 104
2 Â· 104
1 Â· 104
110
9 Â· 104
2 Â· 104
3 Â· 104
2.0
3827
2299

