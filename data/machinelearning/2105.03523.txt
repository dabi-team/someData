Test Suites as a Source of Training Data for Static
Analysis Alert Classiﬁers

1st Lori Flynn
Software Engineering Institute
Carnegie Mellon University
Pittsburgh, USA
lﬂynn@sei.cmu.edu

2nd William Snavely
(former) Software Engineering Institute
Carnegie Mellon University
Pittsburgh, USA
will.snavely@gmail.com

3rd Zachary Kurtz
(former) Software Engineering Institute
Carnegie Mellon University
Pittsburgh, USA
zkurtz@pm.me

1
2
0
2

y
a
M
7

]
E
S
.
s
c
[

1
v
3
2
5
3
0
.
5
0
1
2
:
v
i
X
r
a

Abstract—Flaw-ﬁnding static analysis tools typically generate
large volumes of code ﬂaw alerts including many false positives.
To save on human effort to triage these alerts, a signiﬁcant
body of work attempts to use machine learning to classify and
prioritize alerts. Identifying a useful set of training data, however,
remains a fundamental challenge in developing such classiﬁers
in many contexts. We propose using static analysis test suites
(i.e., repositories of “benchmark” programs that are purpose-
built to test coverage and precision of static analysis tools) as
a novel source of training data. In a case study, we generated
a large quantity of alerts by executing various static analyzers
on the Juliet C/C++ test suite, and we automatically derived
ground truth labels for these alerts by referencing the Juliet
test suite metadata. Finally, we used this data to train classiﬁers
to predict whether an alert is a false positive. Our classiﬁers
obtained high precision (90.2%) and recall (88.2%) for a large
number of code ﬂaw types on a hold-out test set. This preliminary
result suggests that pre-training classiﬁers on test suite data could
help to jumpstart static analysis alert classiﬁcation in data-limited
contexts.

Index Terms—static, analysis, alert, classiﬁcation, rapid, pre-

cise, test suite, Juliet

I. INTRODUCTION

Flaw-ﬁnding static analysis (FFSA) tools typically gener-
ate large volumes of code ﬂaw alerts including many false
positives. To save on human effort to triage these alerts, a
signiﬁcant body of work attempts to use machine learning to
classify and prioritize alerts. Identifying a useful set of training
data, however, remains a fundamental challenge in developing
such classiﬁers in many contexts. We propose using static
analysis test suites (i.e., repositories of “benchmark” programs
that are purpose-built to test FFSA tools) as a novel source of
training data for a wide range of “conditions” (types of code
ﬂaws). In a case study, we generated a large quantity of alerts
by executing various FFSA tools on the Juliet test suite, and
we automatically derived ground truth labels for these alerts
by referencing the Juliet test suite metadata. Finally, we used
this data to train classiﬁers to predict whether an alert is a
false positive, and tested the classiﬁers on hold-out data.

This paper focuses on warnings from FFSA tools that look
for security ﬂaws. A checkerID is a unique string or regular
expression in a tool’s alerts for that ﬂaw type. We use “coding
taxonomy” to mean a named set of coding rules, weaknesses,
standards, or guidelines (e.g., CERT coding rule [13] or

Common Weakness Enumeration (CWE [21])). Each rule or
weakness is considered a single condition. An FFSA tool’s
alert may be mapped to conditions in one or more coding
taxonomies (e.g., [22] lists many FFSA tools that provide
CWE output). “Alert fusion” refers to the practice of unifying
alert information from different tools which map to the same
condition in the same part of the code (e.g., same line of
same ﬁle). Fusion may be imprecise, e.g., a particular CWE
may occur in two different parts of the same line of code. 1

Term
SA tool

FFSA tool
SA alert

Checker

CheckerID
Test suites

Condition

Alert fusion

Coding taxonomy

Deﬁnition
static analysis tool that analyzes code without
running it
ﬂaw-ﬁnding static analysis tool
static analysis alert (warning) about a particu-
lar type of ﬂaw
analysis for a particular type of code ﬂaw, by
a particular FFSA tool
checker name
repositories of “benchmark” programs that are
purpose-built to test FFSA tools
a constraint or property of validity with which
code should comply. FFSA tools try to detect
if code violates conditions.
unifying alert information from different tools
which map to the same condition and code
location
a named set of coding rules, weaknesses, stan-
dards, or guidelines

TABLE I
TERMINOLOGY

Table I summarizes key terminology used in this paper.

A. Related Work

Our work uses the Juliet C/C++ v1.2 test suite [12]. The
Juliet test suites [12] provide example test programs with

1AST 2021, May 20-21, 2021. This is the authors’ version of the work.
This material is based upon work funded and supported by the Department
of Defense under Contract No. FA8702-15-D-0002 with Carnegie Mellon
University for the operation of the Software Engineering Institute, a federally
funded research and development center. References herein to any speciﬁc
commercial product, process, or service by trade name, trade mark, manufac-
turer, or otherwise, does not necessarily constitute or imply its endorsement,
recommendation, or favoring by Carnegie Mellon University or its Software
Engineering Institute. Carnegie Mellon® and CERT® are registered in the
U.S. Patent and Trademark Ofﬁce by Carnegie Mellon University. DM18-
0144

 
 
 
 
 
 
multiple subtypes of each CWE addressed (e.g., separate
variant programs instantiating the ﬂaw using an integer or a
string), and for most of those subtypes also provides variant
test programs involving variants of control, data, and/or type
ﬂow (e.g., a simple example with the ﬂaw completely within a
function, versus an example with control and data ﬂow through
multiple function calls and involving data pointers). Test suites
are usually used to test and compare vulnerability analysis
tools [10], [20] or test system defenses [5], but our work uses
test suites in a new way: to rapidly build labeled data for SA
alert classiﬁer development.

NIST provides over 600,000 cost-free, open-source test suite
programs (including the Juliet test suite) in its Software Ref-
erence Dataset (SARD) [24], along with metadata identifying
each test’s known ﬂaw conditions and the ﬂaw locations. Our
method could be applied even more broadly with additional
test suites including the others hosted by NIST.

Delaitre et al. tested security ﬂaw-ﬁnding SA tools on the
Juliet test suite (their research mostly focused on the Java tests)
and on average those tools found about 20% of weaknesses
in basic test cases (no added complexity). They found that
complex control ﬂow or data ﬂow constructs signiﬁcantly
reduced the tools’ success rates, and identify ﬂaws with highest
and fewest ﬁndings across a set of anonymized FFSA tools [9].
Since single FFSA tools have different coverage of code ﬂaws
(warning about some but not others) [6], [9], multiple FFSA
tools may be used to ﬁnd more code ﬂaws [25]. This approach,
however, compounds the problem of generating too many
alerts to deal with, including too many false positives. Our
work uses multiple FFSA tools and addresses the issue of
handling the alerts. Our work builds on the ﬁndings in [9] by
gathering data on FFSA tool performance for variant ﬂows of
control, type, and data for CWE sub-types and then using that
for classiﬁer development.

Pugh and Ayewah found a mean time of 117 seconds per SA
alert review, from analyzing data from 282 Google engineers
that made over 10,000 manual SA alert determinations [3].
One example of insufﬁcient labeled data as a barrier comes
from our previous work with 3 large organizations that do
software development, where lack of data covering more types
of ﬂaws resulted in classiﬁer incorporation being impractical
for them [2]. Even when an organization has large audit
archives, if the auditors have not used a consistent set of audit
rules and a well-deﬁned auditing lexicon, the data may not
be useful (and most organizations don’t have a well-deﬁned
auditing lexicon and auditing rules) [28]. Data-rich Google
developed 85% accurate classiﬁer models predicting FindBugs
false positives [27]. Cross-project classiﬁer prediction is an
area of research developed to address insufﬁcient labeled data
for a code project. Our work is intended to rapidly develop a
large quantity of labeled data archives using labeled alerts on
test suite, creating classiﬁers to do cross-project defect pre-
diction on production code alerts. Nam and Kim [23] discuss
issues related to cross-project defect prediction: some features
in different projects may correlate to predictions in different
ways, but the rationale for doing cross-project prediction is a

hypothesis that some features are helpful for developing clas-
siﬁers that work well across projects. They use the magnitude
of metric values to do cross-project defect prediction. Their
work focuses on determining a subset of labeled data and
features within that data to use, while our work focuses on
development of a labeled dataset covering many conditions
that develops precise classiﬁers. Zhang et al. [32] did cross-
project prediction, showing improvement of classiﬁcation re-
sults using a connectivity-based unsupervised classiﬁer. A
connectivity-based unsupervised classiﬁer could possibly be
used on data from the labeled audit archives produced from
test suites and alerts from production code. Jing et al. [16]
does cross-company defect prediction including cross-project
prediction, using features that include the name of company
that developed code for the line/function/ﬁle/class/program
related to the alert. A labeled dataset from our system could
be used in conjunction with their techniques.

Bessey et al. [6] found many issues with using SA in prac-
tice, including tools ignoring constructs and thus producing
many false positive alerts, users wrongly labeling diagnostics
they ﬁnd confusing as false, and user difﬁculty dealing with
many alerts resulting in tools producing less (possibly-true)
alerts about possible defects than they could. Beller et al. found
that, in practice, few open-source projects have FFSA tools
integrated closely with their workﬂows, and most of those
projects do not mandate that a codebase should be warning-
free [4]. The goal of our work is to make it easier to use FFSA
tools to effectively ﬁnd and ﬁx prioritized defects, including
for open-source projects.

Heckman and Williams [14] did an extensive survey of
methods that classify and prioritize actionable alerts, detail-
ing 21 peer-reviewed studies. Our method uses 5 of the
approaches (alert type selection, contextual information, data
fusion, machine learning, and mathematical and statistical
models) discussed in the paper, and doesn’t use the other 3
(dynamic detection, graph theory, and model checking). Our
method uses 2 of the artifact characteristics categories (alert
characteristics, code characteristics), and doesn’t use the other
3 (source code repository metrics, bug database metrics, and
dynamic analysis metrics). No previous work in their survey
(nor in any research publication we have found since then)
involves using test suites to automatically label data to create
classiﬁers.

The approach by Kremenek et al. [18] does not merge sets of
alerts from different tools, so alerts from a tool are prioritized
in a set according to that tool’s ranking rather than individually.
Alerts are ranked by correlating data per FFSA tool, using
features from tools and from codebases [19]. Sets of alerts
from different tools are ordered relatively (all alerts from one
tool are prioritized below all alerts from another tool, never
interspersed), with no alert fusion. The labeled data archives
developed in our work could be used in combination with
labeled production alerts in a similar adaptive heuristic.

Our own earlier research [1], [2] developed classiﬁers in
many ways similar to the current work: labeled audit archives
were used, with fused alerts from different tools that map to

the same condition, line number, and ﬁlepath. However, that
work was only able to develop accurate classiﬁers for 3 CERT
C coding rules with single rule data, despite using a signiﬁcant
quantity of audit archives. Those audit archives include data
from 8 years of CERT analysis on 26 codebases, plus new
audit data provided by three collaborating organizations over
the course of a year where the collaborators audited SA alerts
for their own codebases using an auditing lexicon and auditing
rules we developed [28]. Our current work addresses that
labeled-data quantity problem.

II. NEW METHOD FOR RAPIDLY GENERATING LABELED
ALERT ARCHIVES

To quickly generate a large quantity of labeled data for
classiﬁer development, we developed a software system that
uses test suites, as shown in Fig. 1. Data for classiﬁer de-
velopment uses results for each FFSA tool on variant ﬂows
of control,
type, and data for CWE sub-types. Resultant
classiﬁer development potentially can use this information
about individual FFSA tool precision (percent correct or true
positive) in combination with type of ﬂow complexity, CWE
sub-type, and the set of FFSA tools that alert to more precisely
predict if a given alert is true or false.

We ran 8 FFSA tools on the 61,387 tests (covering 118
CWEs) in the Juliet Test Suite for C/C++ v1.2 [12], including
popular proprietary tools and open-source tools. We developed
scripts that parse the FFSA tool output and upload alert
data to a database, including the checkerID plus the ﬁlepath
and line number of the possible defect. Due to licensing
restrictions, we cannot name proprietary tools we used nor
their performance. The open-source FFSA tools we used are
Cppcheck [30], Rosecheckers [29], and GCC [26]. GCC is a
compiler, but like all compilers it statically analyzes the code,
plus it outputs warnings (when run with parameters such as
these: −Wall − Wextra − Wpointer − arith − Wstrict −
prototypes − Wformat − security) that can be mapped (as
regular expressions) to code ﬂaw taxonomies [8].

We generated additional features for the classiﬁers to train
on by running three code metrics tools on the test suite:
(Lizard [31], CCSM [7], and a proprietary tool (anonymous)).
lines of code,
All
complexity, and cohesion. Lizard provided the fewest metrics
and CCSM provided the most. Our database correlates each
alert with applicable code metrics.

the tools provide counts of signiﬁcant

A. Deriving Ground Truth Alert Verdicts

The Juliet Test Suite contains two kinds of metadata that

are relevant for determining the validity of alerts:

• a manifest ﬁle: This is an XML ﬁle that provides pre-
cise ﬂaw information including line number, CWE, and
ﬁlepath.

• function names: Documentation for the test suite says
that if the function name includes the string GOOD then
the particular CWE does not occur in it, but if it includes
the string BAD then the CWE does occur in the function.
We gathered information about ﬁlepath and line numbers

covered by each function name that contains GOOD or
BAD, as well as the CWE indicated (usually by ﬁlename).

Note that both the manifest ﬁle and the function names
provide only CWE-speciﬁc ﬂaw information. In general, a line
of code marked BAD for one code ﬂaw type could be ﬂawless
with respect to all other code ﬂaw types. Thus, we can use the
metadata ﬂaw information to determine the validity of an alert
only when we can establish that the alert’s checkerID is for a
ﬂaw of the same type as a ﬂaw referenced in the metadata. The
test suite metadata does not identify every CWE weakness in
the Juliet code nor all locations of the CWE weaknesses, so
an alert that doesn’t map to the test suite metadata cannot be
automatically labeled using the metadata. In other words, if
an alert’s CWE doesn’t match the test suite metadata’s CWE,
the metadata can’t be used to label the alert true or false.
Publicly-available mappings between checkerIDs

and
CWEs are available for many of the FFSA tools that we tested.
We fused alerts from this set of tools, producing a set of fused
alerts with known CWE designations. We then determined
verdicts (i.e. classiﬁer ground truth labels) for each fused alert
as follows:

• If the manifest

includes a record of a ﬂaw for the
same ﬁlepath, line number, and CWE as the alert, then
set verdict=True, indicating that the alert is a true
positive.

• If the defect alert matches any line within a function
name with GOOD and the alert’s CWE matches the CWE
associated with the function, then set verdict=False,
indicating a that the alert is a false positive.

Applying the above procedure resulted in 36,968 TP fused
alerts, 84,269 FP fused alerts, and fused alerts with no determi-
nation. Note that the procedure deﬁned above conservatively
refrains from assigning a verdict for an alert when (a) the alert
falls within a function labeled BAD but (b) the metadata ﬂaw
line number does not match the line number of the alert.

Automatically generating this volume of alert verdicts is
potentially a nontrivial cost savings. As per Section I-A),
manually auditing alerts typically takes on the order of 117
seconds per alert. Manually generating our 121,237 alert
verdicts at this rate might have taken 3,940 hours. However,
manual auditing and natural code would likely not cover many
of the conditions in the Juliet test suite, since many conditions
rarely exist in natural code. Even if FFSA tools were run on a
large number of natural codebases (which would take a lot
of time and computation) and alerts were found for many
conditions, it might take many manual audits before both true
and false determinations could be made. Realistically, it would
take an enormous amount of manual auditing time (and money
to pay for it), to develop that much data.

In comparison, incorporating a new FFSA tool into our
system takes approximately 24 hours of developer effort, to
code a parser for the FFSA tool output (unless it uses the
new SARIF standard output format /citesarif) and otherwise
integrate into the system. After the FFSA tool is run on the
test suite, the script the developer wrote is run on the output,

Fig. 1. System for automated alert labeling and classiﬁer training

and then labeling the alerts automatically and rapidly happens.
Adding a new test suite to our system similarly involves
the system to integrate
using standard interfaces to adapt
another test suite. However, adding a new test suite takes
more manual effort because each test suite has new unique
compile and test requirements, and for non-SARD test suites
the metadata format must be adapted to the SARD metadata
format. Considering the effort required to add more FFSA
tools and test suites, the new automated system clearly scales
to generate much more labeled data at much lower cost than
manual analysis.

B. Speculative Mapping Method

For FFSA tools that did not have publicly-available checker
mappings to CWEs, we developed a speculative mapping
method to estimate these mappings. Since FFSA tools often
ﬁnd different sets of defects, we hoped to obtain greater CWE
coverage without the cost of manual mapping between the
FFSA tools’ checkers and CWEs.

Our speculative mapping method treats a co-occurrence of
a test suite metadata CWE designation for a line of code and
an alert on that same line of code as evidence that the alert’s
checkerID was designed to detect the kind of ﬂaw indicated
by that CWE. Such a co-occurence is a checker-CWE match.
We summarize this evidence by counting all checker-CWE
matches. That is, for every checker-CWE pair, we count how
many times an alert issued by that checker falls on the same
line as a code ﬂaw of that CWE. A checker may match
multiple CWEs, however, so it’s not always obvious which
CWE a checker is the best match for.

We deﬁne two checker-CWE match rate percentages to
guide the speculative mapping of checkerIDs to CWEs. Let
mij denote the count of checker-CWE matches between
checker i and CWE j; let mi = (cid:80)
j mij , the total count
of all checker-CWE matches involving checker i; and let
mj = (cid:80)
i mij , the total count of all checker-CWE matches
(for the tool being speculatively mapped) involving CWE j.

Finally, we deﬁne the

forward match percentage = 100

mij
mi

and the

backward match percentage = 100

mij
mj

.

Conceptually,
these match rate percentages are alternative
ways to measure the relative frequencies by which alerts from
each checker fall on the lines of particular conditions (CWEs)
that are recorded in the test suite manifest.

For each checker, we establish a preliminary assignment
of that checker to the CWE that has the greatest forward
(backward) match rate percentage with that checker. Finally,
we ﬁx a threshold greater than 0. All preliminary assignments
for which the forward (backward) match rate exceeds the
threshold become speculative mappings. (Section III-D details
the speciﬁc thresholds used.)

Proceeding with the assumption that the speculative map-
pings are correct allows us to increase the number of alerts
with ground truth verdicts. Every alert from a speculatively-
mapped checker has an associated CWE (via the speculative
mapping), and we assign verdicts for such alerts just as
described in Section II-A.

1) Errors in Speculative Mappings: Some speculative map-
pings are wrong. For example, the unmapped tool’s checker
may be alerting about a type of code ﬂaw on that code
line that
the test suite manifest does not mention. Other
mappings are only partially correct. That is the case when
a CWE and a tool checker do not have an EQUALS rela-
tionship, but instead have a SUBSET OF , SUPERSET OF ,
or PARTIALLY OVERLAPPING relationship. Some map-
pings will be missed by speculative mapping. The unmapped
tool might detect the code ﬂaw speciﬁed in the test suite
manifest, but assign the ﬂaw location to code location X while
the manifest assigns it to code location Y . Fault and failure
location may differ. For example, if a failure is caused by an
out of bound array index, the tool and manifest might locate

Data PreparationTrain and EvaluateDefect MetadataMappings between each checker and conditions (e.g., CWE IDs) it detectsSource Code MetricsGenerate Ground Truth Alert LabelsDatabaseCreate Classifiers  random forest lasso regression Xgboost LightGBMTrainingDataTestData70%30%Set of Alerts from each FFSA ToolTool A: checker A1, checker A2, ...Tool B: checker B1, checker B2, ......Labeled Ground Truth DataApply ClassifiersSource Code Structure MetadataFilesFunctionCompare Classifier Labels to Ground Truth Labelsthe fault differently. One could deﬁne it in the very line of the
access or in a function that did not sanitize before that.

If

the actual

2) Test Mappings Rationale and Alternatives: We tested
forward and backward speculative matching with varying
thresholds,
to gather empirical data related to checker to
CWE mappings. We tried to identify generally-useful thresh-
relationship is
olds or mapping directions.
EQUALS then we would expect close to a 100% match. If
the actual relationship is SUBSET OF or SUPERSET OF
then we would expect the forward and backward specula-
tive matching results to be different. If the relationship is
PARTIALLY OVERLAPPING the forward and backward
speculative matching results could be similar or different.
Alternative formulas not tested in this work might produce
useful results, such as: combined match percentage = 0.5 ∗
forward + 0.5 ∗ backward .

C. Automated Speculative Mapping with Manual Veriﬁcation

We identiﬁed an alternative to using the automated spec-
ulative mappings alone. In this alternative, we start with
speculative mappings and then perform manual veriﬁcation
of the mapping. There are approximately 700 CWE and static
analysis tools may have hundreds of checker IDs. Validating
70, 000 possible checker mappings (in this example, for a tool
with only 100 checker IDs) would be very time-consuming, as
it requires a precise understanding of both the checker deﬁni-
tion (which takes on average time(D checker )) and the CWE
deﬁnition (which takes on average time(D CWE )), and after
that determination of the relationship between checker and
CWE (which takes on average time(Relationship)) for a
time in our example of 70, 000 ∗ time(D CWE ) ∗
total
time(D checker )∗time(Relationship). However, speculative
mapping reduces the candidate mappings to a far smaller
number, with each candidate mapping by deﬁnition having
evidence supporting the possible mapping. Three different
speculative CWE mappings per each tool checker ID is far
more than we actually observed. Using that overestimate,
for our example the amount of time required to do such
mapping would be 300∗time(D CWE )∗time(D checker )∗
time(Relationship), which is a factor of 233 less than 70, 000.

III. BUILDING AND TESTING CLASSIFIERS

The classiﬁer task is to determine whether a fused alert is
a True Positive (TP) or a False Positive (FP) on the basis of
various code metrics and other features associated with each
alert. Section III-A explains how we divided the labeled alert
archives into a training set and a test set. Section III-B intro-
duces the classiﬁers. Section III-C describes the performance
of the classiﬁers for fused alerts excluding all speculatively
mapped alerts and analyzes the importance of speciﬁc features.
Section III-D shows how overall performance differs after
including speculatively mapped alerts in the training data.

A. Training data versus test data

We deﬁned a non-speculative training data set and nu-
merous speculative training data sets based on the inclusion

or exclusion of alerts with speculatively-mapped checkerIDs.
The hold-out test set, by contrast, involved only alerts with
non-speculative mappings to ensure a consistent comparison
between a classiﬁer that is alternatively trained on the two
training sets.

To be precise about the test train splits, deﬁne the following

sets of raw (pre-fused) alerts and fused alerts:

• Amapped : the raw alerts with checkerIDs that have known

CWE mappings.

• A(T, d): the raw alerts in Amapped together with the
alerts whose checkerID that is speculatively mapped to a
CWE above the threshold T on the match percentage in
direction d ∈ {forward, backward}, as per Section II-C.
• Apure : the raw alerts that do not share a line with any
other alert that has a speculatively-mapped checkerID (for
every threshold and match percentage direction).
• AF mapped : the fused alerts derived from Amapped .
• AF (T, d): the fused alerts derived from A(T, d).
• AF pure : the fused alerts derived from Apure .
• AF test : a stratiﬁed (on verdict and CWE) random
sample of the fused alerts in AF pure . This represents a
“pure” test set not intertwined with speculative mappings.
• AF non−speculative : the fused alerts in AF mapped exclud-

ing those in AF test .

• AF speculative (T, d): the fused alerts in AF (T, d), exclud-

ing those in AF test .

To summarize, we started by deﬁning our single test set
AF test which included 36,445 fused alerts, or about 30%
of the fused alerts excluding speculative mappings. We then
deﬁned the training set AF non−speculative to include all the re-
maining fused alerts, excluding speculative mappings. Finally,
we deﬁned a series of training sets AF (T, d) that include all
of the alerts in AF non−speculative but also include varying
subsets of the alerts with speculatively-mapped checkerIDs.

Table II shows the number of fused alerts in the training set

AF speculative (T, d) for each combination of threshold
T ∈ {0%, 5%, 25%, 50%, 75%, 100%} and match direction
d. At the most permissive setting (with a threshold of 0),
the number of fused alerts in the training data is 464,887,
approximately twice the 121,237 fused alerts in the main
training set.

B. Classiﬁers and performance metrics

We used the R statistical programming language to run four
different classiﬁers: LightGBM, XGBoost, the H2O.ai imple-
mentation of random forests, and the glmnet implementation
of lasso-regularized logistic regression. All of these except for
the lasso regression are based on decision trees. LightGBM
and XGBoost won acclaim on Kaggle [17] and are both
examples of regularized Gradient Boosting Machines. Random
forest and lasso regression are both generic algorithms; we
used the the random forest implementation from h2o.ai and
the lasso implementation in the glmnet package.

Our purpose for trying several classiﬁers is to identify
at least one one that generates highly accurate predictions
on a hold-out test set. In particular, inference regarding the

Match direction
backward
forward

0%
464887
464887

5%
308038
426927

25%
169370
296259

50%
149926
222108

75%
137128
193622

100%
134374
134374

TABLE II
NUMBER OF FUSED ALERTS IN THE TRAINING DATA FOR EACH SPECULATIVE MAPPINGS THRESHOLD

importance of individual features (including multicollinearity-
induced variance in feature effects) was not a primary focus.
Although we did some experimentation to choose reasonable
hyperparameters for each classiﬁer, we largely accepted de-
fault settings, treating each algorithm as a black-box. All four
of these algorithms are sophisticated in the sense that they
tend to perform well on nearly arbitrary sets of features.

For each of these classiﬁers, a prediction for an alerts
is a number between 0 and 1 that represents an estimated
probability that the alert has verdict=True. We round
these predictions to generate binary classiﬁcation output as
needed to compute metrics like precision and recall.

Our primary metric for classiﬁer performance is the area
under the receiver operating characteristic curve (AUROC).
The AUROC is useful for comparing the overall performance
of classiﬁers when the classiﬁer output is probabilistic. The
AUROC gives partial credit to probabilistic predictions: When
the true label is 1, a prediction of 0.4 scores much better than
a prediction of 0, for example. The AUROC penalizes false
positive probability mass and false negative probability mass
equally.

In addition to the AUROC, we compute the precision, recall,
and accuracy, since these standard metrics are relatively easy
to interpret. The precision is the fraction of predicted TP
alerts that were indeed TP, and the recall is the fraction of
TP alerts that were successfully predicted to be TP. Note that
precision is meaningless when the classiﬁer does not classify
any issues as TP, and so precision values for certain subsets
of the test set are missing. Similarly, recall is meaningless
when the labeled alerts contain no TP alerts, as there is nothing
to be “recalled”.

C. Results without speculative mappings

This section summarizes performance of the classiﬁers on
the test set AF test after training on the main training data
AF non−speculative (as per Section III-A).

Table III summarizes the average performance of the dif-
ferent classiﬁers over the entire test set. Table IV summarizes
the performance of LightGBM, our best classiﬁer, on all CWE
IDs for which at least one test data point was available. Note
that that test count is the number of fused alerts available
for testing for each CWE ID, and TP rate is the fraction of
the testing alerts that were TP.

Table V shows the same performance statistics on groups
of alerts associated with selected CERT rules. For each CERT
secure coding rule, we identiﬁed the set of relevant CWE test
programs based on previously-established mappings between
the taxonomies, documentation about the test suite, and man-
ual inspection of the test suite programs. We subsequently
identiﬁed the set of fused alerts applicable to each CERT

rule via their CWE designations. These sets are not mutually
disjoint in general; the CWE of each fused alert can correspond
to no CERT rule, one CERT rule, or multiple CERT rules.
Thus, the values of test count need not correspond to the
counts in Table IV.

A cursory examination of accuracy in Tables IV and
V suggests that the classiﬁer is often accurate across many
different kinds of rules. Some of these results stand out. The
ﬁrst CWE (457) and second CERT Rule (ARR30-C) both have
very low TP rates; such low TP rates are the source of a
great deal of wasted triaging effort. Our classiﬁer, however,
performed at nearly 100% accuracy on these sets of alerts,
correctly classifying them as FP.

There are limitations, however,

to how broadly we can
interpret this success. Juliet Test Suite is not a representative
sample of code in general. The kinds of code artifacts that trig-
ger alerts from various checkers on the test suite could differ
signiﬁcantly from other code bases, degrading the performance
of our classiﬁer outside of the test suite.

Classiﬁer
rf
lasso
xgboost
lightgbm

Accuracy
0.947
0.880
0.949
0.958

Precision
0.898
0.891
0.922
0.917

Recall
0.902
0.628
0.882
0.928

AUPRC
0.979
0.833
0.974
0.985

TABLE III
AVERAGE PERFORMANCE OF DIFFERENT CLASSIFIERS

We calculated the top 10 features in terms of the total gain of
each feature’s splits in the underlying LightGBM trees, shown
in Table VI (metrics from a proprietary tool anonymized). This
importance ranking is not guaranteed to identify all signiﬁcant
features, and the precise ordering of features is not very
meaningful (in the presence of multicollinearity, for example).
Such a ranking, however, is useful for identifying at least a
sample of the moderate-to-highly-signiﬁcant features. We use
this to provide an example of the patterns that the classiﬁer
detects.

The feature with the highest information gain is the ﬁeld
FUNC_CALLED_BY_LOCAL, from the CCSM metrics tool,
representing the “number of local functions calling this func-
tion”. Fig. 3 shows that alerts are almost always true positives
when the function that contains the alerted line of code is
called by at least one local function.

D. Results using speculatively mapped alerts

We re-trained LightGBM – our best classiﬁer – separately
on each of the training data sets AF speculative (T, d) (as
per Sections II-C, III-A). Experimenting with setting various
thresholds T on the d =forward (or backward) match per-
centages is a way to include speculatively mapped alerts of

Fig. 2. Assessing the value of training a classiﬁer on speculatively mapped alerts

alerts and (2) expected performance to degrade with the
inclusion of alerts at the lowest threshold, since many of
the speculative mappings are presumably incorrect and might
”poison” the training data.

Looking past the overall average classiﬁer performance to
the performance on individual CWEs, there is a possibility that
speculative mappings may improve the classiﬁer for individual
CWEs that are otherwise severely underrepresented in the
training data. At
inclusive speculative mappings
conﬁguration (threshold 0), there are 102 CWEs in the train-
ing data, signiﬁcantly more than the 82 CWEs in the non-
speculative data.

the most

We observed, however,

the classiﬁer accuracy for
that
well-represented CWEs was not signiﬁcantly higher than the
accuracy on underrepresented ones. For instance, the mean
accuracy for the top 20 most-common CWEs in our training
data was 0.946, only slightly greater than the accuracy of 0.942
obtained for the 20 least common CWEs. This suggests that
there is not a strong relationship between the number of fused
alerts in the training data for CWEs of a particular type and
the resulting classiﬁer performance for that CWE. That is, it
appears that signals that the classiﬁer learns are not particularly
CWE-speciﬁc, such that the classiﬁer may do well even for
new alerts corresponding to CWEs that were not represented
in the training data.

IV. DATA

We have published the open-source data used to develop
classiﬁers and make mappings in this work, within the
“RC Data” dataset [11] That dataset includes Juliet Java test
suite data that is not part of this paper’s case study, but that
data was developed using the auto-labeling system discussed
in this paper.

V. CONCLUSIONS, LIMITS, FUTURE WORK

We developed a novel method that uses test suites to
automatically generate a large quantity of labeled data for SA
alert classiﬁer development. We implemented this in a software

Fig. 3. Labeled alerts vs. function caller counts

varying levels of ”speculativeness” in the training data for the
classiﬁers. This section summarizes these alternative training
data sets in terms of the performance of the resulting trained
classiﬁer on test set AF test .

Figure 2 shows performance metrics for LightGBM (our
best classiﬁer) at each matching threshold. The red (blue
dashed) lines show the performance at each threshold applied
to the backward (forward) match percentage. To understand
whether inclusion of speculatively-mapped alerts in the train-
ing set improves the performance of the classiﬁer, compare the
red and blue lines against the black point at the right-hand side
of each graph, which marks the performance of LightGBM
trained on data that excludes all speculative mappings (i.e. a
threshold just greater than 1).

From Figure 2, we conclude that the addition of the specu-
lative alerts did not substantially affect the performance of the
classiﬁers. This was surprising in two ways, since we had (1)
expected performance to be slightly better with (as opposed to
without) the inclusion of high-threshold speculatively-mapped

0204060801000.9570.9580.9590.960Match percentage threshold (lower is more inclusive)Accuracyllbackward match %forward match %no speculative match0204060801000.98200.98300.98400.9850Match percentage threshold (lower is more inclusive)AUPRCllbackward match %forward match %no speculative matchFUNC_CALLED_BY_LOCAL = 0FUNC_CALLED_BY_LOCAL > 0False PositiveTrue PositiveNumber of fused alerts (thousands)05101520system and then in a case study, we generated a large quantity
of labeled data for many different conditions, using the Juliet
test suite. Initial tests of the resulting classiﬁers on partitioned
alerts from the test suite data show high accuracy for a
large number of code ﬂaw types. We developed an automated
method to map checkerIDs to code ﬂaw taxonomy conditions
speciﬁed in test suites. We also created a mised automated-
and-manual method that can greatly reduce mapping effort
compared to fully-manual mapping while being more correct
than fully-automated mappings.

With incorporation of test suites that cover more conditions
(e.g., more CWEs), we expect that the methods demonstrated
here would lead to successful classiﬁers for those additional
conditions. In general, the method and software system devel-
oped can be used with any FFSA and code metrics tools to
rapidly develop labeled data archives with standard additions
to the system: Incorporating a new tool requires a parser and
uploads to the database, while any new FFSA tool requires
checkerID mappings. The general method can use additional
artifact characteristics (source code repository metrics, bug
database metrics, and dynamic analysis metrics), but
the
software system we developed would need to be extended.

The Juliet test suite programs are small (generally consisting
of 1-3 short ﬁles) and are synthetically constructed. Our future
work includes testing the classiﬁers on natural codebases,
including both widely-used publicly-available cost-free open-
source codebases and non-public proprietary codebases be-
longing to collaborators. Also, we have started to include
test suites like STONESOUP [15] with much larger test
programs (in the case of STONESOUP, widely-used natural
test programs injected with ﬂaws) that also have more complex
control, data, and type ﬂows than the Juliet test suite.

REFERENCES

alerts

ﬁx
[1] Prioritizing
https://insights.sei.cmu.edu/sei blog/2016/06/
code
prioritizing-alerts-from-static-analysis-to-ﬁnd-and-ﬁx-code-ﬂaws.html,
2016. Accessed: 2016-06-27.

from static

analysis

ﬂaws.

ﬁnd

and

to

[2] Prioritizing security alerts: A dod case study.

https://insights.sei.
cmu.edu/sei blog/2017/01/prioritizing-security-alerts-a-dod-case-study.
html, January 2017.

[3] Nathaniel Ayewah and William Pugh. The google ﬁndbugs ﬁxit.

In
Proceedings of the 19th international symposium on Software testing
and analysis, pages 241–252. ACM, 2010.

[4] Moritz Beller, Radjino Bholanath, Shane McIntosh, and Andy Zaidman.
Analyzing the state of static analysis: A large-scale evaluation in open
In 2016 IEEE 23rd International Conference on
source software.
Software Analysis, Evolution, and Reengineering (SANER), volume 1,
pages 470–481. IEEE, 2016.

[5] Azzedine Benameur, Nathan S Evans, and Matthew C Elder. Minestrone:

Testing the soup. In CSET, 2013.

[6] Al Bessey, Ken Block, Ben Chelf, Andy Chou, Bryan Fulton, Seth
Hallem, Charles Henri-Gros, Asya Kamsky, Scott McPeak, and Dawson
Engler. A few billion lines of code later: using static analysis to ﬁnd
bugs in the real world. Communications of the ACM, 53(2):66–75, 2010.
[7] bright-tools open source project on github. C code source metrics (ccsm).

https://github.com/bright-tools/ccsm.

[8] CERT.

Cert c coding standard (wiki).

https://wiki.sei.cmu.edu/

conﬂuence/display/c/GCC.

[9] Aurelien Delaitre, Vadim Okun, and Elizabeth Fong. Of massive static
analysis data. In Software Security and Reliability-Companion (SERE-
C), 2013 IEEE 7th International Conference on, pages 163–167. IEEE,
2013.

[10] Aurelien Delaitre, Bertrand Stivalet, Elizabeth Fong, and Vadim Okun.
Evaluating bug ﬁnders–test and measurement of static code analyzers. In
Complex Faults and Failures in Large Software Systems (COUFLESS),
2015 IEEE/ACM 1st International Workshop on, pages 14–20. IEEE,
2015.

[11] Lori

Flynn,

Ebonie McNeil, Matt

Sisk,

Open

Snavely.
dataset RC data
https://wiki.sei.cmu.edu/conﬂuence/display/seccode/Open+Dataset+
RC Data+for+Classiﬁer+Research, August 2020.

classiﬁer

for

and William
research.

[12] NSA Center for Assured Software.

Juliet test suite c/c++ v1.2 user

guide. https://samate.nist.gov/SARD/resources/Juliet Test Suite v1.2
for C Cpp - User Guide.pdf.

[13] CERT Secure Coding group. Sei cert coding standards (wiki). https:
//wiki.sei.cmu.edu/conﬂuence/display/seccode, 2018. Accessed March
9, 2018.

[14] Sarah Heckman and Laurie Williams. A systematic literature review
of actionable alert identiﬁcation techniques for automated static code
analysis. Information and Software Technology, 53(4):363–387, 2011.
Iarpa

[15] Intelligence Advanced Research Projects Activity (IARPA).
stonesoup. https://samate.nist.gov/SRD/view.php?tsID=102.

[16] Xiaoyuan Jing, Fei Wu, Xiwei Dong, Fumin Qi, and Baowen Xu.
Heterogeneous cross-company defect prediction by uniﬁed metric rep-
resentation and cca-based transfer learning. In Proceedings of the 2015
10th Joint Meeting on Foundations of Software Engineering, pages 496–
507. ACM, 2015.

[17] kaggle. Kaggle website. https://www.kaggle.com/, 2018. Accessed

March 19, 2018.

[18] Ted Kremenek, Ken Ashcraft, Junfeng Yang, and Dawson Engler.
Correlation exploitation in error ranking. In ACM SIGSOFT Software
Engineering Notes, volume 29, pages 83–93. ACM, 2004.

[19] Ted Kremenek and Dawson Engler. Z-ranking: Using statistical analysis
to counter the impact of static analysis approximations. In International
Static Analysis Symposium, pages 295–315. Springer, 2003.

[20] Gregory Larsen, EK Fong, David A Wheeler, and Rama S Moorthy.
State-of-the-art resources (soar) for software vulnerability detection,
test, and evaluation. Technical report, INSTITUTE FOR DEFENSE
ANALYSES ALEXANDRIA VA, 2014.

[21] MITRE. Common weakness enumeration: A community-developed
dictionary of software weakness types. https://cwe.mitre.org. Accessed
June 22, 2016.

[22] MITRE. Cwe-compatible products and services. https://cwe.mitre.org/

compatible/compatible.html, 2018. Accessed March 9, 2018.

[23] Jaechang Nam and Sunghun Kim. Clami: Defect prediction on unlabeled
In Automated Software Engineering (ASE), 2015 30th
datasets (t).
IEEE/ACM International Conference on, pages 452–463. IEEE, 2015.
[24] National Institute of Standards and Technology (NIST). Sard test suites.

https://samate.nist.gov/SRD/testsuite.php.

[25] Daniel Plakosh, Robert Seacord, Robert W Stoddard, David Svoboda,
and David Zubrow. Improving the automated detection and analysis of
secure coding violations. 2014.

[26] The GNU Project. Gnu compiler collection (gcc). https://gcc.gnu.org/.
[27] Joseph R Ruthruff, John Penix, J David Morgenthaler, Sebastian El-
baum, and Gregg Rothermel. Predicting accurate and actionable static
In Proceedings of the
analysis warnings: an experimental approach.
30th international conference on Software engineering, pages 341–350.
ACM, 2008.

[28] David Svoboda, Lori Flynn, and William Snavely.

alert audits: Formal lexicon and rules.
Cybersecurity Development (SecDev) 2016. ieee, 2016.

Static analysis
In Proceedings of the IEEE

[29] CERT Secure Coding Team. Cert rosecheckers static analysis tool for

c code. http://rosecheckers.sourceforge.net.

[30] Cppcheck Team. cppcheck static source code analysis tool for c and

c++ code. https://sourceforge.net/projects/cppcheck/, 2021.

[31] Terry Yin. Lizard. https://github.com/terryyin/lizard. Accessed January

29, 2018.

[32] Feng Zhang, Quan Zheng, Ying Zou, and Ahmed E Hassan. Cross-
project defect prediction using a connectivity-based unsupervised clas-
siﬁer. In Proceedings of the 38th International Conference on Software
Engineering, pages 309–320. ACM, 2016.

recall
1.00
1.00
0.87
1.00
1.00
1.00
0.95
0.95
0.80
0.82
0.74
0.80
0.90
1.00
1.00
0.99
1.00
0.73
1.00
0.93
0.99
0.99
1.00
1.00
0.91
1.00
0.79
0.97
0.79
0.98
1.00
0.98
0.97
1.00
1.00
1.00
1.00
1.00
1.00
0.95
0.90
0.78
0.97
1.00

precision
1.00
1.00
0.85
1.00
1.00
1.00
0.73
0.72
0.83
0.82
0.84
0.82
0.82
1.00
1.00
1.00
1.00
0.94
1.00
0.73
0.99
0.99
1.00
1.00
0.83
1.00
0.97
0.98
0.77
1.00
1.00
0.98
0.96
1.00
1.00
1.00
1.00
1.00
1.00
0.93
0.98
0.81
0.93
1.00

CWE-ID
824
457
681
665
908
758
195
194
196
676
426
78
704
253
762
404
761
197
401
680
476
690
190
20
188
672
134
775
191
129
590
628
369
606
119
252
456
909
125
121
122
843
126
563
377
415
468
469
398
192
480
783
127
327
416
688
569
787
120
131
170
328
467
587
570
367
465
597
483
338
478
482
484
123
561
571
466
562
835

test count
3239
2906
2871
2467
2451
2317
1485
1286
1265
994
982
982
866
853
850
734
698
693
663
618
513
501
461
446
424
417
381
376
361
336
300
279
234
232
225
167
148
139
135
118
116
115
71
65
64
37
36
35
34
33
32
32
27
25
23
22
20
20
19
19
19
19
19
18
16
13
13
13
11
7
7
7
7
6
5
4
1
1
1

TP rate
0.09
0.06
0.25
0.03
0.03
0.00
0.22
0.25
0.24
0.44
0.43
0.43
0.40
0.27
0.67
0.56
0.60
0.44
0.53
0.17
0.23
0.17
0.16
0.64
0.33
0.60
0.44
0.18
0.29
0.49
1.00
0.88
0.47
0.31
0.73
0.74
0.54
0.49
0.56
0.82
0.83
0.23
0.83
0.17
0.00
0.73
0.36
0.37
0.38
0.00
0.97
0.97
0.96
1.00
0.56
0.00
0.95
0.60
1.00
1.00
1.00
1.00
1.00
0.39
1.00
1.00
1.00
1.00
0.00
1.00
1.00
1.00
1.00
0.83
1.00
1.00
1.00
1.00
1.00
TABLE IV
PERFORMANCE OF LIGHTGBM ON TEST SETS COMPOSED OF META
ALERTS FOR SPECIFIC CWE IDS.

accuracy
1.00
1.00
0.93
1.00
1.00
1.00
0.91
0.89
0.92
0.84
0.83
0.83
0.88
1.00
1.00
1.00
1.00
0.86
1.00
0.93
1.00
1.00
1.00
1.00
0.91
1.00
0.90
0.99
0.87
0.99
1.00
0.97
0.97
1.00
1.00
1.00
1.00
1.00
1.00
0.90
0.90
0.90
0.92
1.00
1.00
0.97
1.00
1.00
1.00
1.00
1.00
1.00
0.93
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
0.83
1.00
1.00
1.00
0.00
1.00

1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00

1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00

1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
0.00
1.00

1.00
1.00
1.00
1.00
0.83
1.00
1.00
1.00

1.00
1.00
0.96
1.00
1.00

1.00
1.00
0.96
1.00
1.00

0.96
1.00
1.00
1.00

1.00
1.00
1.00
1.00

1.00

CERT rule
ARR30-C
ARR36-C
ARR38-C
ARR39-C
CON43-C
DCL30-C
ENV33-C
ERR33-C
ERR34-C
EXP33-C
EXP34-C
EXP37-C
EXP45-C
EXP46-C
FIO30-C
FIO42-C
FIO47-C
FLP32-C
FLP34-C
INT30-C
INT31-C
INT32-C
INT33-C
INT36-C
MEM30-C

test count
2123
2015
973
631
615
372
371
271
259
240
234
197
186
140
106
93
57
48
35
31
18
13
7
1
1

TP rate
0.30
0.06
0.16
0.03
0.12
0.42
0.16
0.07
1.00
0.00
0.47
1.00
0.40
0.12
0.85
1.00
0.75
0.31
0.37
0.26
0.39
1.00
1.00
1.00
1.00

precision
0.81
1.00
0.84
1.00
0.81
0.97
0.98
1.00
1.00

0.96
1.00
0.81
1.00
0.97
1.00
0.93
0.93
1.00
1.00
1.00
1.00
1.00

1.00

recall
0.84
1.00
0.85
0.94
0.85
0.78
0.97
1.00
1.00

0.97
1.00
0.95
1.00
0.92
1.00
0.93
0.93
1.00
1.00
1.00
1.00
1.00
0.00
1.00

accuracy
0.89
1.00
0.95
1.00
0.96
0.90
0.99
1.00
1.00
1.00
0.97
1.00
0.89
1.00
0.91
1.00
0.89
0.96
1.00
1.00
1.00
1.00
1.00
0.00
1.00

TABLE V
PERFORMANCE OF LIGHTGBM ON TEST SETS COMPOSED OF FUSED
ALERTS THAT CORRESPOND TO SPECIFIC CERT RULES.

Rank
1
2
3
4
5
6
7
8
9
10

Feature
CCSM FUNC_CALLED_BY_LOCAL
checkerID
CCSM RAW_KW_LONG_CNT
Tool X Metric 1
Tool X Metric 2
CWE
Lizard Parameters
Tool X Metric 3
Lizard Number_of_Tokens
Tool X Metric 4
TABLE VI
FEATURE IMPORTANCE IN LIGHTGBM: TOP 10.

Importance
0.423
0.294
0.047
0.031
0.026
0.025
0.015
0.012
0.008
0.006

