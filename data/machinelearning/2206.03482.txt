Parametric Chordal Sparsity for SDP-based Neural Network Veriﬁcation

Anton Xue, Lars Lindemann, and Rajeev Alur

2
2
0
2

n
u
J

7

]

G
L
.
s
c
[

1
v
2
8
4
3
0
.
6
0
2
2
:
v
i
X
r
a

Abstract— Many future technologies rely on neural networks,
but verifying the correctness of their behavior remains a major
challenge. It is known that neural networks can be fragile in
the presence of even small input perturbations, yielding unpre-
dictable outputs. The veriﬁcation of neural networks is therefore
vital to their adoption, and a number of approaches have been
proposed in recent years. In this paper we focus on semideﬁnite
programming (SDP) based techniques for neural network
veriﬁcation, which are particularly attractive because they
can encode expressive behaviors while ensuring a polynomial-
time decision. Our starting point is the DeepSDP framework
proposed by Fazlyab et al, which uses quadratic constraints to
abstract the veriﬁcation problem into a large-scale SDP. When
the size of the neural network grows, however, solving this
SDP quickly becomes intractable. Our key observation is that
by leveraging chordal sparsity and speciﬁc parametrizations
of DeepSDP, we can decompose the primary computational
bottleneck of DeepSDP — a large linear matrix inequality
(LMI) — into an equivalent collection of smaller LMIs. Our
parametrization admits a tunable parameter, allowing us to
trade-off efﬁciency and accuracy in the veriﬁcation procedure.
We call our formulation Chordal-DeepSDP, and provide exper-
imental evaluation to show that it can: (1) effectively increase
accuracy with the tunable parameter and (2) outperform
DeepSDP on deeper networks.

I. INTRODUCTION

[4], [5], meaning that

The success of neural networks is well documented in the
literature for various applications, e.g., in Go [1], in hand-
written character recognition [2], and in autonomous driving
[3]. Neural networks are, however, notoriously opaque. The
key challenges in analyzing their behavior are two-fold:
ﬁrst, one must appropriately model the nonlinear activation
functions; second, the neural network may be very large.
In addition, neural networks are often sensitive to input
perturbations
test-driven methods
may insufﬁciently cover the input domain. It is hence often
unclear what a neural network exactly learns, and how certain
desirable properties can be veriﬁed. This is of particular
concern in safety-critical applications like autonomous driv-
ing, where the lack of formal veriﬁcation guarantees poses a
serious barrier to adoption. In this work we focus on safety
veriﬁcation: given a neural network f : Rn1 → Rm and a
speciﬁcation S ⊆ X × Rm where X ⊆ Rn1 is the admissible
input domain, does the neural network f satisfy S? In other
words, is it the case that

(x, f (x)) ∈ S for all x ∈ X ?

Anton Xue and Rajeev Alur are with the Department of Computer
and Information Science, University of Pennsylvania, PA, USA (email:
antonxue@seas.upenn.edu and alur@seas.upenn.edu).

Lars Lindemann is with the Department of Electrical and Sys-
tems Engineering, University of Pennsylvania, PA, USA (email:
larsl@seas.upenn.edu).

Our starting point is the DeepSDP veriﬁcation framework
presented in [6], with a focus on improving scalability.
DeepSDP uses quadratic constraints to abstract nonlinear
activation functions, yielding a convex relaxation of the
safety veriﬁcation problem as a large SDP. When the neural
network grows larger, however, solving DeepSDP becomes
intractable — thereby motivating our work.

We observe that although DeepSDP [6] itself does not
exploit any sparsity to improve scalability, it was recently
noted in [7] that a particular instantiation of DeepSDP for
neural networks with ReLU action functions admits chordal
sparsity [8], [9], [10] in its LMI constraint — the key
computational bottleneck. This insight allows the authors of
[7] to then decompose and efﬁciently solve this version of
DeepSDP. This approach improves scalability, but lacks two
features critical to the original DeepSDP work. First, the
formulation presented in [7] is the sparsest parametrization
of DeepSDP using a restrictive abstraction of the activation
function behavior — which cannot capture the interaction of
neurons across nonadjacent layers. Moreover, this approach
cannot verify speciﬁcations with input-output coupling, e.g.
the L2 gain κ of a network f posed as (cid:107)f (x)(cid:107)2 ≤ κ(cid:107)x(cid:107)2.
In this work we present an alternative parametrization
of DeepSDP that overcomes both limitations. We present a
tunable parametrization of DeepSDP that leads to different
levels of chordal sparsity, allowing us to trade-off sparsity
and accuracy in the veriﬁcation result. Importantly, our
tunable parametrization allows us to recover a form similar
to [7] as well as the original DeepSDP formulation at the ex-
treme ends of the efﬁciency-accuracy spectrum. Furthermore,
our approach retains the full expressive power of DeepSDP’s
speciﬁcations, in particular the ability to encode arbitrary
quadratic input-output speciﬁcations — a distinguishing fea-
ture of SDP-based neural network veriﬁcation techniques.
Particularly, our contributions are as follows:

• We propose the Chordal-DeepSDP method for SDP-
based veriﬁcation of neural networks with general ac-
tivation functions. We particularly show that speciﬁc
parametrizations of DeepSDP admit chordally sparse
decompositions, allowing us to formulate the equivalent
Chordal-DeepSDP problem using smaller constraints.
The primary beneﬁt is scalability, as the main computa-
tional bottleneck of DeepSDP — a large LMI — is now
decomposed into an equivalent collection of smaller
LMIs without incurring any accuracy loss.

• We present techniques for bolstering the accuracy and
scalability of Chordal-DeepSDP. We show how addi-
tional constraints can further tighten the veriﬁcation ac-
curacy while retaining the same chordal decomposition

 
 
 
 
 
 
structure. Moreover, we observe that a second level of
chordal decomposition is possible, allowing us to further
accelerate the run time.

• We provide numerical evaluations to show the effective-
ness of Chordal-DeepSDP. We show on a set of deep
random networks that Chordal-DeepSDP outperforms
DeepSDP in terms of efﬁciency without inducing con-
servatism. Our open source implementation is available
at https://github.com/AntonXue/nn-sdp.

A. Related Work

The safety veriﬁcation of neural networks has found broad
interest over the past years and there have been various de-
partures to address the problem [11], [12]. Great success was
achieved with Reluplex [13], which is a specialized satisﬁa-
bility modulo theories (SMT) based veriﬁcation method that
interleaves the simplex procedure with boolean satisﬁability
solving. While Reluplex only applies to feedforward neural
networks with ReLU activation functions,
the technique
was extended to fully connected and convolutional neural
networks with arbitrary piecewise-linear activation functions
with Marabou [14]. Other works in SMT-based veriﬁcation
include [15], [16]. Related to SMT-solving has been the
use of mixed-integer programming such as in [17], [18],
[19]. These methods give accurate results for various types
of neural networks and are in fact complete for ReLU
activation functions, but they are fundamentally hard-to-solve
combinatorial problems.

Another way of approaching safety veriﬁcation is by
reformulating it as a reachability problem, see e.g., [20], [21],
[22], [23], [24], [25] which has been of particular interest for
the safety veriﬁcation of closed-loop dynamical systems [26].
While heuristics for reachability problems over dynamical
systems exist, these methods are typically computationally
expensive. Safety veriﬁcation methods based on abstract
interpretations [27], [28], [29], [22], [30] work by soundly
over-approximating the input set into a convenient abstract
domain, e.g. polytopes or zonotopes, and then propagating
the abstracted representation through the network. Then,
properties veriﬁed on the abstract output domain imply that
the same properties hold on the original system. These
approaches generally offer good scalability, but one must
carefully choose good abstract domains and transformers to
preserve accuracy.

Broadly related to this work is the idea to formulate neural
network veriﬁcation as a convex optimization problem. For
scalability,
linear approximations have been presented in
[31], [32] where particularly [31] is amendable to paral-
lelization via ADMM [33]. Techniques for interval analysis
like β-CROWN [34] are also GPU-parallelizable. Further
related are convex optimization-based techniques [35], which
commonly use convex over-approximations of the activation
functions in order to formulate a convex veriﬁcation problem.
As mentioned previously, we investigate the scalability of
semideﬁnite programming methods [36] in this work. The
authors in [37] present an early work in this direction,
encoding ReLU activations via semideﬁnite constraints. This

was improved upon in DeepSDP [6], which can handle
arbitrary activations — so long as they are encodable via
quadratic constraints. The work of [7] recognized that cer-
tain restricted parameterizations of DeepSDP admit sparsity
structures amenable to chordal decomposition, but such
formulations tend to be conservative and also limiting in
their expressiveness. The notion of chordal sparsity has
proven useful in decomposing large-scale optimization prob-
lems, and are extensively studied [8], [9], [10], [38], [39]
with application in [40], [41], [42]. Some recent directions
on SDP-based neural network veriﬁcation have examined
ﬁnding tighter abstractions, especially for networks with
ReLU activations [43], [44]. Also related are works based
on polynomial optimization [45], [46], [47], [48], which
similarly use semideﬁnite programming as the underlying
computational technique.

II. BACKGROUND AND PROBLEM FORMULATION
We consider feedforward neural networks f : Rn1 → Rm
with K ≥ 2 layers, i.e., K − 1 hidden layers. For an input
x1 ∈ Rn1, the output of the neural network is recursively
computed for k = 1, . . . , K − 1 as

f (x1) := WKxK + bK,

xk+1 := φ(Wkxk + bk),

where Wk and bk are the weight matrices and bias
vectors of the kth layer that are of appropriate dimen-
sions. We denote the dimensions of x1, . . . , xK by pos-
itive integers n1, . . . , nK ∈ Z+. The function φ(u) :=
vcat(ϕ(u1), ϕ(u2) . . .) is the vector-valued result of the
activation function ϕ : R → R applied element-wise to
the input u = vcat(u1, u2 . . .). We assume throughout the
paper that the same type of activation function is used for
all neurons. Let N := n1 + · · · + nK and also deﬁne the
stacked vectors

x := vcat(x1, . . . , xK) ∈ RN ,

z := vcat(x, 1) ∈ RN +1,

with selectors Eaz = 1 and Ekz = xk for k = 1, . . . , K.

A. Neural Network Veriﬁcation with DeepSDP

Towards verifying if a network f satisﬁes a speciﬁcation
S, the authors in [6] have presented DeepSDP. The main
idea is to use quadratic constraints (QCs) to abstract the
input set X ,
the activation functions φ, and the safety
speciﬁcation S. We then use these QCs to set up the safety
veriﬁcation problem as a large semideﬁnite program (SDP)
whose satisﬁability implies that the network is safe.

1) Abstracting the Input Set via QCs: To abstract the
input set X into a QC, let P (γin) ∈ Sn1+1 be a symmetric
indeﬁnite matrices parametrized by a vector γin ∈ Γin where
Γin is the permissible set. Each such matrix P (γin) now has
to satisfy the following QC

z(cid:62)Zinz ≥ 0, Zin :=

(cid:21)(cid:62)

(cid:20)E1
Ea

P (γin)

(cid:21)

(cid:20)E1
Ea

∈ SN +1

(1)

where recall that E1z = x1 and Eaz = 1. The vector γin
appears linearly in P (γin) and will be a decision variable
in DeepSDP. The dimension of γin depends on the speciﬁc

choice of P (γin) (see [6, Section 3.A]): for instance, the
polytope X := {x1 ∈ Rn : Hx ≤ h} can be abstracted into
the QC (1) with

P (γin) :=

(cid:21)
(cid:20) H (cid:62)ΛH −H (cid:62)Λh
−h(cid:62)ΛH h(cid:62)Λh

where Λ is a symmetric and nonnegative matrix whose
entries are those of γin. As the entries of Λ are nonnegative,
in this case Γin is the set of nonnegative vectors.

2) Abstracting the Activation Functions via QCs: To
abstract the activation function φ : Rn → Rn, let Q(γac) ∈
S2n+1 be a set of symmetric indeﬁnite matrices parametrized
by a vector γac ∈ Γac where Γac is the permissible set. Each
such matrix Q(γac) must now satisfy the following QC





(cid:62)











(2)

Q(γac)

u
φ(u)
1

 ≥ 0 for all u ∈ U

u
φ(u)
1
where U ⊆ Rn is some input set of φ. The dimension of
γac depends on the particular activation function and on
the speciﬁc abstraction that is chosen. In particular, it was
shown in [6, Proposition 2] that many activations ϕ are [a, b]-
sector-bounded 1 for ﬁxed scalars a ≤ b. In such cases, the
stacked activation φ(u) = vcat(ϕ(u1), ϕ(u2), . . .) admits the
abstraction

Qsec :=





−2abT
(a + b)T
q(cid:62)
13

(a + b)T q13
q23
q33

−2T
q(cid:62)
23


 ∈ S1+2n,

T :=

n
(cid:88)

i=1

λiieie(cid:62)

i +

(cid:88)

λij∆ij∆(cid:62)
ij,

(i,j)∈Iβ

Iβ := {(i, j) : 1 ≤ i < j ≤ n, j − i ≤ β}

(3)

where ∆ij = ei − ej, and q13, q23, q33, λij are linearly
parametrized by γac. Here Iβ is a β-banded index set, which
means that T is a β-banded symmetric matrix. Importantly,
this abstraction for activations can be done the entire multi-
layered network and not just for a single layer’s activation.
By deﬁning b := vcat(b1, . . . , bK−1),








A :=




W1
...
0

· · ·
. . .
· · · WK−1

0
...

0
...

 , B :=
0

0
...
0




In2
...
0

· · ·
. . .
· · ·

0
...
InK


 ,

we may write Bx = φ(Ax + b) ∈ Rn2+···+nK , such that
(2) then becomes

z(cid:62)Zacz ≥ 0, Zac :=


(cid:63)



(cid:62)



Q(γac)


 ∈ SN +1, (4)





A b
B 0
1
0

where we use [(cid:63)] to denote identical terms as on the RHS
and we assume that Q(γac) = Qsec. The use of the integer-
valued β parameter in T serves to induce sparsity 2 in Zac so

1A function ϕ is [a, b]-sector-bounded if a ≤ ϕ(u1)−ϕ(u2)

≤ b for all
u1, u2 ∈ R. For instance, the ReLU, sigmoid, and tanh activations are all
[0, 1]-sector bounded.

u1−u2

2In the original formulation of Qsec in [6, Section 3.C] the authors do
not assume a banded index set. This is equivalent to the case of taking
β = N − n1 − 1 under our formulation.

that we may apply chordal decomposition later. Intuitively,
ﬁxing larger values of β permits more λij terms, thereby
increasing slack (and thus accuracy) in the abstraction at
the cost of sparsity (and thus scalability). Our formulation
therefore allows one to use β as a tunable sparsity parameter
to navigate this accuracy-scalability trade-off.

We remark that the DeepSDP framework is not restricted
to abstractions of form Qsec. However, we will ﬁrst present
our main results under the assumptions of (2) with Q(γac) =
Qsec in Section III before showing in Section IV how other
abstractions may be incorporated while preserving sparsity.
3) Abstracting the Safety Speciﬁcation via QCs: To ab-
stract the safety speciﬁcation S into a QC, let S ∈ Sn1+m+1
be a symmetric matrix. We assume that S encodes safety by
the inequality z(cid:62)Zoutz ≥ 0 where
(cid:62)


(cid:63)






I
0
0
0 WK bK

1
0
0









E1
EK
Ea

S


 ∈ SN +1,

(5)

Zout :=

which captures a quadratic relation between the input x1
and output y = f (x1). For instance, to bound the local
L2 gain via S = {(x1, y) : (cid:107)y(cid:107)2 ≤ κ(cid:107)x1(cid:107)2}, take S =
diag(−κI, I, 0).

4) DeepSDP: Finally, we combine the input, activation,
and output QCs and deﬁne DeepSDP as the following
semideﬁnite program:

ﬁnd γ := (γin, γac) ∈ Γin × Γac
subject to Z(γ) := Zin + Zac + Zout (cid:22) 0,

(6)

We recall the formal safety guarantees of DeepSDP next.

Lemma 1 ([6], Theorem 2). Suppose DeepSDP (6) is
feasible, then f satisﬁes the speciﬁcation S.

This result ensures that feasibility of DeepSDP implies that
the desired safety condition S holds. Moreover, one can also
derive a reachability variant of DeepSDP by parametrizing
some components of S e.g. the L2 gain κ in the above
example, in order to then optimize for the tightest ﬁt. This
notion of reachability is demonstrated in [6, Section 5] and
also in Section V. A computational bottleneck of DeepSDP,
however, is the large semideﬁnite constraint. In particular, the
dimension of Z(γ) grows with the size of the neural network
f , so that solving DeepSDP quickly becomes intractable.

B. Chordal Sparsity

The notion of chordal sparsity connects chordal graph
theory and sparse matrix decomposition [49], [9], and can
be used to efﬁcently solve large-scale semideﬁnite programs.
In the context of this paper, if Z(γ) is chordally sparse, then
the LMI constraint Z(γ) (cid:22) 0 within DeepSDP can be split
into an equivalent set of smaller matrix constraints.

1) Chordal Graphs and Sparse Matrices: Let G(V, E) be
:= {1, . . . , n} and edges
a graph with vertices V = [n]
E ⊆ V × V. We assume that G is undirected, meaning
that E is symmetric, i.e., (i, j) ∈ E implies (j, i) ∈ E. A
subset of vertices C ⊆ V forms a clique if u, v ∈ C implies
that (u, v) ∈ E, and let C(i) be the ith vertex of C under

the natural ordering. If there is no other clique that strictly
contains C, then C is a maximal clique. A cycle of length k is
a sequence of vertices v1, . . . , vk, v1 ∈ V with (vk, v1) ∈ E
and adjacent connections (vi, vi+1) ∈ E. A chord is an edge
that connects two nonadjacent vertices in a cycle. We say
that a graph is chordal if every cycle of length ≥ 4 has at
least one chord [9, Chapter 2].

Dually, an edge set E may also describe the sparsity of
a symmetric matrix. Given G(V, E), the set of symmetric
matrices of size n with sparsity pattern E is deﬁned as
Sn(E) := {X ∈ Sn : Xij = Xji = 0 if (i, j) (cid:54)∈ E}.
(7)
If X ∈ Sn(E), then we say that X has sparsity pattern E.
We say that the entry Xij is sparse if (i, j) (cid:54)∈ E, and that it
is dense otherwise. Moreover, if G(V, E) is chordal, then we
say that X has chordal sparsity or is chordally sparse.

2) Chordal Decomposition of Sparse Matrices: For a
chordally sparse matrix X ∈ Sn(E), one can deduce whether
X (cid:23) 0 by analyzing the maximal cliques of G(V, E). To
do this, given a clique C ⊆ V, deﬁne its projection matrix
EC ∈ R|C|×n such that (EC)ij = 1 iff C(i) = j, and
(EC)ij = 0 otherwise. An exact condition for X (cid:23) 0 is
then stated as follows.

Lemma 2 ([10] Theorem 2.10). Let G(V, E) be a chordal
and {C1, . . . , Cp} its maximal cliques. Then X ∈ Sn(E) and
X (cid:23) 0 iff there are Xk ∈ S|Ck| such that each Xk (cid:23) 0 and
X = (cid:80)p

XkECk .

k=1 E(cid:62)
Ck

We call this summation of Xk a chordal decomposition of
X. The importance of this result is that we may now solve
a large LMI using an equivalent collection of smaller ones.
Because solving X (cid:23) 0 is typically at least n3 in time [50],
this trade-off for smaller Xk (cid:23) 0 is often desirable.

III. NEURAL NETWORK VERIFICATION WITH
CHORDAL-DEEPSDP

In this section, we construct a chordally sparse variant of
DeepSDP that we call Chordal-DeepSDP. We assume here
that activation functions are abstracted as sector bounded
functions with Q(γac) = Qsec, and show in Section IV
how other abstractions can be used while preserving sparsity.
The main technical challenge is to precisely characterize the
sparsity of Z(γ), and to gain intuition we show in Figure 1
different patterns as the value of β varies.

Fig. 1. The sparsity of Z(γ) ∈ SN +1 for β = 0, 2, 4 for networks with
depth K = 6 and nk = 3 for each k. Each plot corresponds to a sparsity
Eβ where (i, j) ∈ Eβ iff (i, j) is a colored square. Each Eβ is the disjoint
union of: EM (blue), Ea (green), and E1,K (orange).

We see that Eβ is an overlapping block-diagonal matrix
with arrow structure [9]. For each increment of β, we observe

that the blocks on the diagonal grow one unit downwards
and rightwards. From these plots it is straightforward to
conjecture the structure of Eβ. First, let us deﬁne the function
S(k) := n1 + · · · + nk to express summation over nk, where
S(0) = 0, S(K) = N , and let nK+1 = m. The sparsity
patterns Eβ observed in Figure 1 are then characterized in
the following lemma.
Lemma 3. It holds that Z(γ) ∈ SN +1(Eβ), where

Eβ := EM ∪ Ea ∪ E1,K
EM := EM,1 ∪ · · · ∪ EM,K−1,
Ea := {(i, j) ∈ [N + 1]2 : i = N + 1 or j = N + 1},

with each EM,k ⊆ [N ]2 and the E1,K ⊆ [N ]2 deﬁned as

EM,k := {(i, j) : S(k − 1) + 1 ≤ i, j ≤ S(k + 1) + β},
E1,K := {(i, j) : (1 ≤ i ≤ n1 and S(K − 1) + 1 ≤ j ≤ N )
or (1 ≤ j ≤ n1 and S(K − 1) + 1 ≤ i ≤ N )}.

However, a chordal decomposition of Z(γ) given Eβ is
not obvious. This is because each G([N + 1], Eβ) is not
chordal: the “wing tips” at the lower-left and upper-right
corners allows for a chord-less cycle of length ≥ 4 between
vertices 1 and N + 1, akin to the “wheel” shape seen in [9,
Figure 3.1 (right)].

Crucially, however, the notion of chordal sparsity is tied to
graphs rather than to matrices. For instance, given Z(γ) ∈
SN +1(Eβ), if there exists Fβ ⊇ Eβ, then Z(γ) ∈ SN +1(Fβ)
as well. Moreover, if G([N + 1], Fβ) is chordal, then one
may decompose Z(γ) with respect to the maximal cliques
of G([N + 1], Fβ) as in Lemma 2. Such Fβ is known as a
chordal extension of Eβ, and the ﬂexibility of permitting a
matrix to have multiple valid sparsity patterns means that we
may pick whichever sparsity is convenient (i.e. chordal) in
order to apply chordal decomposition.

Fig. 2. A chordal extension of each Eβ into Fβ , such that each G([N +
1], Fβ ) is now chordal.

One such chordal extension of each Eβ is shown in
Figure 2, where the idea is to ﬁll in certain rows and columns
(denoted by red (cid:63)) in order to chordally extend each Eβ into
its corresponding Fβ. This chordal extension in Figure 2 can
be described by Fβ := Eβ ∪ EK, where EK ⊆ [N + 1]2 with

EK := {(i, j) : S(K − 1) + 1 ≤ i ≤ N
or S(K − 1) + 1 ≤ j ≤ N }.

The sparsity patterns Fβ are central to our sparsity analysis
of Z(γ) and chordal decomposition of DeepSDP. We next de-
scribe the chordality and maximal cliques of G([N +1], Fβ).

Theorem 1. The graph G([N + 1], Fβ) is chordal and the
number of its maximal cliques is

p := min{k : S(k + 1) + β ≥ S(K − 1)}.

Furthemore, the kth maximal clique for k < p is

Ck := {v : S(k − 1) + 1 ≤ v ≤ S(k + 1) + β
or S(K − 1) + 1 ≤ v ≤ N + 1},

and the ﬁnal clique is Cp := {v : S(p−1)+1 ≤ v ≤ N +1}.
The above results establish that Z(γ) ∈ SN +1(Fβ)
and that G([N + 1], Fβ) is chordal with maximal cliques
{C1, . . . , Cp}. These are the sufﬁcient conditions
for
Lemma 2, allowing us to form a chordally sparse variant
of DeepSDP below that we call Chordal-DeepSDP.

ﬁnd γ := (γin, γac) ∈ Γin × Γac,

Z1, . . . , Zp (cid:22) 0

subject to Z(γ) =

p
(cid:88)

k=1

E(cid:62)
Ck

ZkECk

Moreover, by Lemma 2 we have that DeepSDP and Chordal-
DeepSDP are in fact equivalent problems.

Theorem 2. DeepSDP (6) and Chordal-DeepSDP (8) are
equivalent: one is feasible iff the other is, and γ is a solution
to (6) iff γ and some Z1, . . . , Zp is a solution to (8).

Remark 1. If Chordal-DeepSDP (8) is feasible, then the
safety condition in Lemma 1 holds.

In summary, we have decomposed the large semideﬁnite
constraint Z(γ) (cid:22) 0 present in DeepSDP into a large equality
constraint and a collection of smaller Zk (cid:22) 0 constraints.
The primary beneﬁt of Chordal-DeepSDP is computational,
since the cost of solving an LMI is usually at least cubic in
its size [50]. This means that solving Z(γ) (cid:22) 0 is at least
cubic in N , whereas the cost of each Zk (cid:22) 0 is at least cubic
in |Ck| — which may be signiﬁcantly smaller than N . This
is especially preferable for deeper networks, as the number
of cliques grows with the number of layers.

Moreover, our formulation of Chordal-DeepSDP admits
any symmetric S, meaning that
this method can handle
arbitrary quadratically-coupled input-output speciﬁcations.
This is a key advantage over the more conservative approach
of [7], which can only handle output constraints.

IV. BOLSTERING CHORDAL-DEEPSDP WITH DOUBLE
DECOMPOSITION AND ADJACENT-LAYER ABSTRACTIONS

In Section III, we have built-up a basic version of Chordal-
DeepSDP. In Section IV-A, we show that our formulation
of Fβ from Eβ admits yet another level of chordal decom-
position that we found useful for performance and that we
call Chordal-DeepSDP-2. In addition, our construction until
this point has only assumed Q(γ) = Qsec for abstracting
activation functions φ. This simpliﬁes the presentation of our
theoretical results, as parametrization of Qsec by different β
is the primary difﬁculty in the sparsity analysis of Z(γ). In
practice, however, we found additional abstractions useful to

more tightly bound the activation behavior — in Section IV-
B we show how to incorporate other activation abstractions
that target adjacent-layer connections while preserving the
same sparsity Eβ analyzed in Theorem 1.

A. Double Decomposition

In Section III, we presented a chordal decomposition of
Z(γ) with respect to the maximal cliques of G([N + 1], Fβ).
Recall, however, that we obtained Fβ from Eβ by treating
certain zero entries of Z(γ) as dense, see also Figure 2. This
means that certain Zk terms in the equality constraint of
Chordal-DeepSDP will in fact have well-structured sparsity,
which we state next.

Lemma 4. Each matrix Zk in Chordal-DeepSDP (8) for
indices 2 ≤ k ≤ p − 1 has structure




(8)

Zk =



(Zk)11
0
(Zk)(cid:62)
13

0
(Zk)22
(Zk)(cid:62)
23

(Zk)13
(Zk)23
(Zk)33

 ,

where (Zk)11 ∈ Snk+nk+1+β, (Zk)22 ∈ SnK , (Zk)33 ∈ R.

The structure of these Zk in Lemma 4 is a block-arrow
shape [9, Section 8.2], which are known to be chordal. In
particular for 2 ≤ k ≤ p − 1 we may express a chordal
decomposition of Zk as

Zk = E(cid:62)

Dk,1

Yk,1EDk,1 + E(cid:62)
D2

Yk,2ED2 ∈ S|Ck|(Dk,1 ∪ Dk,2)

where the two maximal cliques for this block-arrow shape
are Dk,1, Dk,2 ⊆ [|Ck|] with

Dk,1 := {v : 1 ≤ v ≤ nk + nk+1 + β} ∪ {|Ck|},
Dk,2 := {v : nk + nk+1 + 1 ≤ v ≤ |Ck|},

where Dk,1 covers the (11), (13), (33) blocks of Zk, and
Dk,2 covers the (22), (23), (33) blocks. Applying Lemma 2
again means that Zk (cid:22) 0 iff Yk,1, Yk,2 (cid:22) 0. This additional
level of decomposition on Zk means that we may fur-
ther chordally decompose Chordal-DeepSDP. For simplicity,
when k = 1 let us deﬁne Y1,1 = Z1 with D1,1 = C1,
and Y1,2 = 0 with D1,2 = ∅ (and similarly for k = p).
This doubly decomposed problem, which we call Chordal-
DeepSDP-2, is formulated as:

ﬁnd γ := (γin, γac) ∈ Γin × Γac,
Y1,1, Y1,2, . . . , Yp,1, Yp,2 (cid:22) 0

subject to Z(γ) =

p
(cid:88)

k=1

E(cid:62)

Dk,1

Yk,1EDk,1 + E(cid:62)
D2

Yk,2ED2

(9)

Remark 2. DeepSDP (6), Chordal-DeepSDP (8), and
Chordal-DeepSDP-2 (9) are equivalent problems in the sense
of Theorem 2 due to Lemma 2.

B. Adjacent-Layer Abstractions

Often it is desirable to use multiple activation abstrac-
tions in order to tightly bound the network behavior. More
generally, we say that Qk ∈ S1+2nk+1 is an abstraction for

the adjacent-layer connection xk+1 = φ(Wkxk + bk) if the
following inequality holds:


(cid:63)



(cid:62)



Qk





Wk
0
0

0
Ink+1
0





bk
0
1





Ek
Ek+1
Ea



 z ≥ 0

(10)

for all xk ∈ Xk, where Xk includes the reachable set of
xk from x1 ∈ X1. Such Qk can encode useful information
about the activations at each layer. For instance, if one knows
from x1 ∈ X that xk always satisﬁes xk ≤ xk ≤ xk, then
Qk−1 can encode the fact that xk lies within this box, similar
to [6, Section 3.C.4]. Using interval bounds is popular in
practice because they are typically cheap to compute, and
we use the LiRPA library [51] for this, though other interval
preprocessing choices are also valid.

Note that the nonnegativity in (10) means we may add

abstractions to yield new ones. Thus, the inequality

K−1
(cid:88)

k=1



(cid:62)


(cid:63)







Wk
0
0

Qk

0
Ink+1
0





bk
0
1





Ek
Ek+1
Ea



 z ≥ 0

(11)

for all x1 ∈ X , would be an abstraction that accounts for all
K−1 adjacent-layer connections in the network. Importantly,
the formulation of (11) permits us to use a combination of
arbitrary abstractions Qk for φ between adjacent layers, so
long as they each satisfy the appropriate QC. In addition,
we may also rewrite (11) into a form resembling that of (4),
which we show in Lemma 5

Lemma 5. There exists Qadj linearly parametrized by the
Q1, . . . , QK−1 such that the inequality

z(cid:62)



(cid:62)





A b
B 0
1
0



Qadj







A b
B 0
1
0

 z ≥ 0 for all x1 ∈ X

holds iff the inequality (11) holds.

Applying again the nonnegavity condition of activation
QCs, we can redeﬁne Q(γac) := Qsec+Qadj. Intuitively Qsec
is able to enforce cross-layer dependencies, while Qadj can
only enforce adjacent-layer dependencies. Although different
abstractions come with their respective trade-offs, combining
them into a single Q(γac) for DeepSDP means that we
can more tightly approximate the activation functions of
the entire multi-layered network. Conveniently, using such
a Qadj does not change the overall sparsity of Z(γ) as we
will show next.

Theorem 3. Let Q(γac) := Qsec + Qadj and similarly deﬁne
Z(γ) as before, then Z(γ) ∈ SN +1(Eβ).

This means that the formulations of Chordal-DeepSDP and
Chordal-DeepSDP-2 are still equivalent to DeepSDP under
the deﬁnition of Q(γac) in Theorem 3.

V. NUMERICAL EXPERIMENTS

We now evaluate the effectiveness of our approach, where
we aim to answer the following: (Q1) What is the scalability
of our methods vs DeepSDP? (Q2) What
is the impact

of β on veriﬁcation accuracy? We refer to [6] for other
experiments on the accuracy of DeepSDP, and recall that
given a ﬁxed β, the formulations of DeepSDP, Chordal-
DeepSDP, and Chordal-DeepSDP-2 are equivalent.

a) QC Conﬁguration: For ReLU functions, we use the
sector-bounded QC proposed in [6, Lemma 4] and for general
sector-bounded activations we use that of [6, Lemma 2]. Both
of these have the same sparsity as Qsec described in (3). We
use Qadj to encode interval propagation information that we
obtain from the LiRPA library [51], similar to the bounded
nonlinearities presented in [6, Section 3.C.4]. All of these
activation QCs induce a Z(γ) with sparsity Eβ.

b) Dataset: As our primary focus is on improving
the scalability of DeepSDP under parametrized sparsity, we
use two randomly generated datasets in our experiments:
one batch for scalability experiments, and another batch for
reachability experiments. We generate a collection of net-
works of widths = 10, 20 and depths (K −1) = 5, 10, . . . , 50
with Gaussian random weights with σ := 2/
w log w for
scalability experiments and σ := 1/
2 for reachability
experiments. We consider only ReLU activations. For ease
of plotting, each of these networks have input and output
dimensions n1 := m := 2.

√

√

c) System: All experiments were run on machines with
Intel Xeon Gold 6148 CPU @ 2.40 GHz with 80 cores and
692 GB of RAM. Our codebase was implemented with Julia
1.7.2 and we used MOSEK 9.3 as our convex solver.

A. (Q1): Scalability

For each random network we ran DeepSDP, Chordal-
DeepSDP, and Chordal-DeepSDP-2 with β = 0, . . . , 7 to
optimize a reachability ellipsoid when the initial input is
constrained to the box X := {x : (cid:107)1 − x(cid:107)∞ ≤ 0.5} ⊆ R2,
where 1 ∈ R2 is the vector of ones.

Our plots are shown in Figure 3. We observe that Chordal-
DeepSDP-2 consistently outperforms both DeepSDP and
Chordal-DeepSDP, especially as the network depth increases.
While DeepSDP with β = 0 is competitive, its performance
quickly degrades as one increases β — whereas the decay for
Chordal-DeepSDP and Chordal-DeepSDP-2 is more gradual,
meaning that we can achieve more accuracy without a
signiﬁcant performance penalty.

B. (Q2): Reachability

We next investigate the impact of β on the veriﬁcation
accuracy. We ﬁrst ﬁx an initial set to be the box X := {x :
(cid:107)1 − x(cid:107)∞ ≤ 0.5} ⊆ R2, where 1 ∈ R2 is the vector of ones.
Then, we uniformly sample 105 points to approximate the
center yc ∈ R2 and shape P ∈ S2
+ — which is a regularized
positive deﬁnite covariance matrix. The reachability problem
is to then minimize ρ subject to (cid:107)P −1(y − yc)(cid:107)2 ≤ ρ, for
which S takes form described in [6, Section 5.A]. Note that
we ﬁx P and yc in contrast to the technique presented in [52],
which allows for one to optimize over P and yc directly at
the cost of a larger semideﬁnite program.

We show the reach ellipsoids in Figure 4 along with
uniformly sampled outputs. As the value of β increases, we
see that the reach-set estimation becomes more accurate.

Fig. 3.
Runtimes (secs) for DeepSDP (dualized), Chordal-LipSDP,
and Chordal-LipSDP-2 on networks with widths 10, 20 and depths
10, 20, . . . , 100. Each curve of the same color denote the performance
of a different β = 0, 1, . . . , 7, with the intermediate region shaded. We
observed that dualizing DeepSDP yields a signiﬁcant performance increase
compared to the primal (often over ×10), but this beneﬁt was not observed
for Chordal-DeepSDP nor Chordal-DeepSDP-2.

C. Discussion

Our numerical experiments show that Chordal-DeepSDP-2
outperforms both DeepSDP and Chordal-DeepSDP on deep
networks. Moreover, both Chordal-DeepSDP and Chordal-
DeepSDP-2 exhibit better scaling properties than DeepSDP
as the value of β increases, with the performance of
DeepSDP signiﬁcantly degrading.

We found it helpful

to run the dual formulation of
DeepSDP, with help of the Dualization.jl library 3
— this alone achieves a very signiﬁcant speedup (often
over ×10) compared to its primal formulation. This same
dualization trick does not seem to beneﬁt Chordal-DeepSDP
likely because of the
nor Chordal-DeepSDP-2, however,
large equality constraint that is not present in DeepSDP.

VI. CONCLUSIONS

We present Chordal-DeepSDP, a SDP-based method for
the safety veriﬁcation of feedforward neural networks. Using

3As of this writing, Dualization.jl does not yet support dualizing
a feasibility primal problem. We therefore add the simple objective of
minimizing (cid:107)γ(cid:107)1 for feasibility problems.

Fig. 4. Ellipsoid reachability for W10-D10 (top) and W20-D10 (bottom).
As β increases, the ellipsoid tightens about the sampled points.

the concept of chordal sparsity, Chordal-DeepSDP presents
a scalable chordal decomposition of DeepSDP. We show
how to parametrically tune the sprasity of Chordal-SDP,
allowing us to operate between the extremes of efﬁciency and
accuracy. We provide numerical evaluations showcasing this
trade-off in efﬁciency and accuracy and illustrate the com-
putational advantages of Chordal-DeepSDP over DeepSDP.

A. Results for Lemma 3

APPENDIX

Proof of Lemma 3. By the Lemmas below, we have that

Zin ∈ SN +1(EM ∪ Ea),
Zac ∈ SN +1(EM ∪ Ea),
Zout ∈ SN +1(EM ∪ Ea ∪ E1,K),

(Lemma 6)

(Lemma 7)

(Lemma 8)

and so the sum Zin + Zac + Zout = Z(γ) ∈ SN +1(Eβ).

Remark 3. Suppose that a graph has n = 4 vertices and
C = {1, 2, 4}, then the projection matrix EC has the useful
algebraic property for any X ∈ S3:

E(cid:62)

C XEC =







X11 X12
X21 X22
0
X41 X42

0







.

0 X14
0 X24
0
0
0 X44

This is convenient for quickly determining which entries of
Zin, Zac, Zout are dense.
Lemma 6. It holds that Zin ∈ SN +1(EM ∪ Ea).

Proof. Recall (1) and observe that (Zin)ij is dense iff

i, j ∈ {v : 1 ≤ v ≤ n1}
(cid:125)

(cid:124)

(cid:123)(cid:122)
B1

∪ {v : v = N + 1}
(cid:123)(cid:122)
(cid:125)
Ba

(cid:124)

where B1, Ba ⊆ [N + 1] are disjoint. Equivalently, (Zin)ij
is dense iff any of the following conditions hold:

(1) i ∈ B1 and j ∈ B1
(2) i ∈ B1 and j ∈ Ba (or the symmetric case)
(3) i ∈ Ba and j ∈ Ba

If (1) holds, then (i, j) ∈ EM,1; if any of (2) or (3) holds,
then (i, j) ∈ Ea. Thus (i, j) dense implies (i, j) ∈ EM ∪ Ea,
and so Zin ∈ SN +1(EM ∪ Ea).

Lemma 7. It holds that Zac ∈ SN +1(EM ∪ Ea).

Proof. Recall (4) and observe that we may express Zac as:

Zac =

=

=











(cid:62) 





A b
B 0
1
0
(cid:21)(cid:62) (cid:20)a11T a12T
(cid:20)A
a12T a22T
B
(cid:63)

a11T a12T q13
a12T a22T q23
q33
(cid:63)
(cid:21) (cid:20)A
B

(cid:21)

(cid:63)

(cid:63)

(cid:63)













A b
B 0
1
0





(cid:21)

(cid:20) M p12
p22
(cid:63)

∈ SN +1

where a11 := −2ab, a12 := a + b, a22 := −2 and where
p12 and p13 follow by straightforward calculation. From
Lemma 9 (which is provided below) we know that M ∈
SN (EM ), and since p12, p22 occupy the (N + 1)th column
and row of Zac, it follows that Zac ∈ SN +1(EM ∪ Ea).

Lemma 8. It holds that Zout ∈ SN +1(EM ∪ Ea ∪ E1,K).

Proof. Recall (5) and observe that (Zout)ij is dense iff

i, j ∈ {v : 1 ≤ v ≤ n1}
(cid:125)

(cid:124)

(cid:123)(cid:122)
B1

(cid:124)

∪ {v : S(K − 1) + 1 ≤ v ≤ N }
(cid:123)(cid:122)
(cid:125)
BK
,
∪ {v : v = N + 1}
(cid:125)
(cid:123)(cid:122)
Ba

(cid:124)

where B1, BK, Ba ⊆ [N + 1] are disjoint. Equivalently,
(Zout)ij is dense iff any of the following holds:

(1) i ∈ B1 and j ∈ B1
(2) i ∈ B1 and j ∈ BK (or the symmetric case)
(3) i ∈ B1 and j ∈ Ba (or the symmetric case)
(4) i ∈ BK and j ∈ BK
(5) i ∈ BK and j ∈ Ba (or the symmetric case)
(6) i ∈ Ba and j ∈ Ba

If (1) holds, then (i, j) ∈ EM,1; if (4) holds, then (i, j) ∈
EM,K−1; if (2) holds, then (i, j) ∈ E1,K; if any of (3), (5), or

(6) holds, then (i, j) ∈ Ea. Thus (i, j) dense implies (i, j) ∈
EM ∪ Ea ∪ E1,K, and so Zout ∈ SN +1(EM ∪ Ea ∪ E1,K).
Lemma 9. For all a11, a12, a22 ∈ R it holds that
(cid:21)(cid:62) (cid:20)a11T a12T
a12T a22T

∈ SN (EM ).

(cid:21) (cid:20)A
B

(cid:20)A
B

M :=

(cid:21)

Proof. This follows from [53, Theorem 1], in which the
summation S(k) is deﬁned in the same manner, and in which
the Zα matrix from [53, Equation 3] has the same sparsity
pattern as M .

B. Results for Theorem 1

Proof of Theorem 1. The deﬁnition of p lets us partition
Fβ = EA ∪ ER with EA ∩ ER = ∅, such that

EA := EA,1 ∪ · · · ∪ EA,p,
ER := {(i, j) : S(K − 1) + 1 ≤ i or S(K − 1) + 1 ≤ j}.

EA,k := EM,k ∩ [S(K − 1)]2

with ER ⊆ [N + 1]2. In other words, ER = EK ∪ Ea and
EA = Fβ \ER. Our strategy is to show that G([S(K −1)], EA)
is chordal and iteratively reconstruct G([N + 1], Fβ) using
the edge set ER while preserving chordal sparsity at every
step. To begin, we claim that the G([S(K −1), EA) is chordal
with maximal cliques A1, . . . , Ap where

Ak = {v ∈ [S(K − 1)] : S(k − 1) + 1 ≤ v ≤ S(k + 1) + β}.

This claim follows from [53, Theorem 2] if we identify the
variable and notational substitutions

[S(K − 1)] (cid:55)→ V, S(K − 1) (cid:55)→ N, EA,k (cid:55)→ Ek, β (cid:55)→ τ.

Intuitively, EA is the same sparsity as in [53, Theorem 2] if
one applies these mappings.

We now construct G([N + 1], Fβ) from G([S(K − 1), EA)
by iteratively adding each vertex v ∈ [N + 1] \ [S(K − 1)]
and its corresponding edges of ER. By the deﬁnition of ER,
this iterative method means that each newly added v will
have an edge to all the vertices already present. Inductively
by Lemma 10 (which is provided below), this implies that
G([N + 1], Fβ) is chordal and that its maximal cliques are

Ck = Ak ∪ {S(K − 1) + 1, . . . , N + 1}.

Lemma 10. Let G(V, E) be a chordal graph and C1, . . . , Cp
its maximal cliques, and let G(cid:48)(V (cid:48), E (cid:48)) be an extension of G
with a new vertex v(cid:48) that is connected to every other vertex,
i.e. V (cid:48) = V ∪ {v(cid:48)} and E (cid:48) = E ∪ {(v(cid:48), v) : v ∈ V}. Then
G(cid:48) is chordal and its maximal cliques are C(cid:48)
p where
each C(cid:48)

1, . . . , C(cid:48)

k = Ck ∪ {v(cid:48)}.

Proof. Let u1, . . . , uk, u1 be a cycle in G(cid:48) with k ≥ 4. If
this cycle lies strictly in G, then there exists a chord because
G is chordal. Now suppose that u1 = v(cid:48) without loss of
generality, then the edge (ui, v(cid:48)) ∈ E (cid:48) for all vertices of this
cycle by construction of G(cid:48) — meaning that there exists a
chord. This shows that every cycle in G(cid:48) of length k ≥ 4 has
a chord, and thus G(cid:48) is chordal.

To show that each C(cid:48)

by contradiction that some C(cid:48)

k is a maximal clique of G(cid:48), suppose
k is strictly contained in another

clique C(cid:48) of G(cid:48). But this means that C(cid:48) \ {v(cid:48)} is a clique
in G which strictly contains some Ck = C(cid:48)
k \ {v(cid:48)}, which
contradicts the maximality assumption of each Ck in G.

To show that the C(cid:48)

k are the only maximal cliques of G(cid:48),
suppose by contradiction that there exists another maximal
clique C(cid:48) of G(cid:48). Now observe that C(cid:48) \ {v(cid:48)} is a clique in
G, and so by assumption it must be contained in some Ck.
However, this would mean that C(cid:48) ⊆ C(cid:48)
k, which contradicts
the assumption that C(cid:48) is maximal in G(cid:48).

C. Results for Theorem 2

Proof of Theorem 2. Since Z(γ) ∈ SN +1(Fβ) and G([N +
1], Fβ) is chordal with maximal cliques {C1, . . . , Cp}, by
Lemma 2 we have thatZ(γ) (cid:22) 0 iff each Zk (cid:22) 0 and Z(γ) =
(cid:80)p

k=1 E(cid:62)
Ck

ZkECk .

D. Results for Double Decomposition

Proof of Lemma 4. First, recall that Z1<k<p has dimension
nk + nk+1 + β + nK + 1 because:

E. Results for Adjacent-Layer Connections

Proof of Lemma 5. We proceed by rewriting (11) into the
form of Lemma 5.









Ek
Ek+1
Ea



 z



(cid:62)


(cid:63)



Qk

K−1
(cid:88)

k=1







(cid:62)


(cid:63)



Qk



bk
0
1




0
Ink+1
0

Wk
0
0


Wkxk + bk
xk+1
1




(cid:62)


(cid:63)





(cid:62)


(cid:63)



P (cid:62)

k QkPk

P (cid:62)

k QkPk

















W1x1 + b1
...
WK−1xK−1 + bK−1
x2
...
xK
1

















 z

A b
B 0
1
0

A b
B 0
1
0

(cid:62) (cid:32)K−1
(cid:88)





P (cid:62)

k QkPk

k=1

(cid:124)

(cid:123)(cid:122)
Qadj

(cid:33)

(cid:125)



 z





A b
B 0
1
0

=

K−1
(cid:88)

k=1

=

K−1
(cid:88)

k=1

K−1
(cid:88)

=

k=1


Ck = {v : S(k − 1) + 1 ≤ v ≤ S(k + 1) + β}
(cid:125)

(cid:123)(cid:122)
nk+nk+1+β elements

(cid:124)

= z(cid:62)



∪ {v : S(K − 1) + 1 ≤ v ≤ N + 1}
(cid:123)(cid:122)
(cid:125)
nK +1 elements

(cid:124)

.

To see why (Zk)12 = 0, we show that its entries do not
contribute any terms to Z(γ). Let Xk be a dense matrix the
same dimension as (Zk)12 and consider the expression:

E(cid:62)
Ck





0mk×mk
X (cid:62)
k
0

Xk
0nK ×nK
0

0
0
01×1


 ECk ∈ SN +1,

where Pk is a projection matrix acting on vcat(Ax+b, Bz, 1)
that selects out the vector vcat(Wkxk + bk, xk+1, 1).

Proof of Theorem 3. It sufﬁces to show that each summation
term of (11) has sparsity Eβ. Consider the expression



(cid:62)


(cid:63)







Wk
0
0

0
Ink+1
0





bk
0
1

Qk



 ,





Ek
Ek+1
Ea

where mk = nk + nk+1 + β, whose (i, j) entry is dense iff

(cid:0)S(k − 1) + 1 ≤ i ≤ S(k + 1) + β and S(K − 1) + 1 ≤ j ≤ N (cid:1)

(cid:0)S(k − 1) + 1 ≤ j ≤ S(k + 1) + β and S(K − 1) + 1 ≤ i ≤ N (cid:1).

or

It will then sufﬁce to show that (i, j) (cid:54)∈ Eβ — which by
Lemma 3 is a sparsity of Z(γ) — because this allows us
to conclude that Z(γ) does not depend on (Zk)12. As this
disjunctive condition is symmetric, we may consider some
(i, j) satisfying the ﬁrst case without loss of generality. First,
(i, j) (cid:54)∈ EM because our choice of k < p means that

S(k − 1) + 1 ≤ i ≤ S(k + 1) + β < S(K − 1) + 1 ≤ j,

and so (i, j) does not meet the inclusion criteria of any EM,k.
Furthermore, our choice of k < p means that we cannot have
i = N + 1 nor j = N + 1, and so (i, j) (cid:54)∈ Ea. Finally, since
k > 1, we have that i, j ≥ n1 + 1, and so (i, j) (cid:54)∈ E1,K.
Consequently, it follows that (Zk)12 = 0.

whose (i, j) entry is dense iff

(cid:124)

i, j ∈ {v : S(k − 1) + 1 ≤ v ≤ S(k)}
(cid:125)

(cid:123)(cid:122)
Bk
∪ {v : S(k) + 1 ≤ v ≤ S(k + 1)}
(cid:123)(cid:122)
(cid:125)
Bk1
,
∪ {v : v = N + 1}
(cid:123)(cid:122)
(cid:125)
Ba

(cid:124)

(cid:124)

where Bk, Bk+1, Ba are disjoint. Equivalently, the (i, j) entry
is dense iff any of the following conditions holds:

(1) i ∈ Bk and j ∈ Bk
(2) i ∈ Bk and j ∈ Bk+1 (or the symmetric case)
(3) i ∈ Bk and j ∈ Ba (or the symmetric case)
(4) i ∈ Bk+1 and j ∈ Bk+1
(5) i ∈ Bk+1 and j ∈ Ba (or the symmetric case)
(6) i ∈ Ba and j ∈ Ba

If (1), (2), or (4) holds, then (i, j) ∈ EM,k; if (3), (5), or (6)
holds, then (i, j) ∈ Ea. Thus the (i, j) entry is dense implies
that (i, j) ∈ EM ∪ Ea, and so (i, j) ∈ Eβ.

REFERENCES

[1] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,
M. Lanctot, L. Sifre, D. Kumaran, T. Graepel et al., “A general
reinforcement learning algorithm that masters chess, shogi, and go
through self-play,” Science, vol. 362, no. 6419, pp. 1140–1144, 2018.
[2] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278–2324, 1998.

[3] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “Carla:
An open urban driving simulator,” in Conference on robot learning.
PMLR, 2017, pp. 1–16.

[4] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harness-
ing adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.
[5] J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling deep
neural networks,” IEEE Transactions on Evolutionary Computation,
vol. 23, no. 5, pp. 828–841, 2019.

[6] M. Fazlyab, M. Morari, and G. J. Pappas, “Safety veriﬁcation and
robustness analysis of neural networks via quadratic constraints and
semideﬁnite programming,” IEEE Transactions on Automatic Control,
2020.

[7] M. Newton and A. Papachristodoulou, “Exploiting sparsity for neural
network veriﬁcation,” in Learning for Dynamics and Control. PMLR,
2021, pp. 715–727.

[8] J. Agler, W. Helton, S. McCullough, and L. Rodman, “Positive
semideﬁnite matrices with a given sparsity pattern,” Linear algebra
and its applications, vol. 107, pp. 101–149, 1988.

[9] L. Vandenberghe and M. S. Andersen, “Chordal graphs and semidef-
inite optimization,” Foundations and Trends in Optimization, vol. 1,
no. 4, pp. 241–433, 2015.

[10] Y. Zheng, “Chordal sparsity in control and optimization of large-scale

systems,” Ph.D. dissertation, University of Oxford, 2019.

[11] S. Bak, C. Liu, and T. Johnson, “The second international veriﬁcation
of neural networks competition (vnn-comp 2021): Summary and
results,” arXiv preprint arXiv:2109.00498, 2021.

[12] C. Liu, T. Arnon, C. Lazarus, C. Strong, C. Barrett, and M. J.
Kochenderfer, “Algorithms for verifying deep neural networks,” arXiv
preprint arXiv:1903.06758, 2019.

[13] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, “Re-
luplex: An efﬁcient smt solver for verifying deep neural networks,” in
International Conference on Computer Aided Veriﬁcation. Springer,
2017, pp. 97–117.

[14] G. Katz, D. A. Huang, D. Ibeling, K. Julian, C. Lazarus, R. Lim,
P. Shah, S. Thakoor, H. Wu, A. Zelji´c et al., “The marabou framework
for veriﬁcation and analysis of deep neural networks,” in International
Conference on Computer Aided Veriﬁcation. Springer, 2019, pp. 443–
452.

[15] X. Song, E. Manino, L. Sena, E. Alves, I. Bessa, M. Lujan, L. Cordeiro
et al., “Qnnveriﬁer: A tool for verifying neural networks using smt-
based model checking,” arXiv preprint arXiv:2111.13110, 2021.
[16] L. Sena, X. Song, E. Alves, I. Bessa, E. Manino, L. Cordeiro
et al., “Verifying quantized neural networks using smt-based model
checking,” arXiv preprint arXiv:2106.05997, 2021.

[17] A. Lomuscio and L. Maganti, “An approach to reachability analysis for
feed-forward relu neural networks,” arXiv preprint arXiv:1706.07351,
2017.

[18] V. Tjeng, K. Xiao, and R. Tedrake, “Evaluating robustness of
neural networks with mixed integer programming,” arXiv preprint
arXiv:1711.07356, 2017.

[19] M. Fischetti and J. Jo, “Deep neural networks and mixed integer linear
optimization,” Constraints, vol. 23, no. 3, pp. 296–309, 2018.
[20] R. Ivanov, J. Weimer, R. Alur, G. J. Pappas, and I. Lee, “Verisig:
verifying safety properties of hybrid systems with neural network
controllers,” in Proceedings of the 22nd ACM International Conference
on Hybrid Systems: Computation and Control, 2019, pp. 169–178.

[21] R. Ivanov, T. Carpenter, J. Weimer, R. Alur, G. Pappas, and I. Lee,
“Verisig 2.0: Veriﬁcation of neural network controllers using taylor
model preconditioning,” in International Conference on Computer
Aided Veriﬁcation. Springer, 2021, pp. 249–262.

[22] H.-D. Tran, X. Yang, D. M. Lopez, P. Musau, L. V. Nguyen, W. Xiang,
S. Bak, and T. T. Johnson, “Nnv: The neural network veriﬁcation
tool for deep neural networks and learning-enabled cyber-physical
systems,” in International Conference on Computer Aided Veriﬁcation.
Springer, 2020, pp. 3–17.

[23] J. A. Vincent and M. Schwager, “Reachable polyhedral marching
(rpm): A safety veriﬁcation algorithm for robotic systems with deep
neural network components,” arXiv preprint arXiv:2011.11609, 2020.
[24] W. Xiang, H.-D. Tran, and T. T. Johnson, “Reachable set computation
and safety veriﬁcation for neural networks with relu activations,” arXiv
preprint arXiv:1712.08163, 2017.

[25] ——, “Output reachable set estimation and veriﬁcation for multilayer
neural networks,” IEEE transactions on neural networks and learning
systems, vol. 29, no. 11, pp. 5777–5783, 2018.

[26] M. Everett, “Neural network veriﬁcation in control,” arXiv preprint

arXiv:2110.01388, 2021.

[27] M. Mirman, T. Gehr, and M. Vechev, “Differentiable abstract in-
terpretation for provably robust neural networks,” in International
Conference on Machine Learning. PMLR, 2018, pp. 3578–3586.

[28] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri,
and M. Vechev, “Ai2: Safety and robustness certiﬁcation of neural
networks with abstract interpretation,” in 2018 IEEE Symposium on
Security and Privacy (SP).

IEEE, 2018, pp. 3–18.

[29] G. Singh, T. Gehr, M. P¨uschel, and M. Vechev, “An abstract domain for
certifying neural networks,” Proceedings of the ACM on Programming
Languages, vol. 3, no. POPL, pp. 1–30, 2019.

[30] S. Bak, H.-D. Tran, K. Hobbs, and T. T. Johnson, “Improved geometric
path enumeration for verifying relu neural networks,” in International
Conference on Computer Aided Veriﬁcation. Springer, 2020, pp. 66–
96.

[31] S. Chen, E. Wong, J. Z. Kolter, and M. Fazlyab, “Deepsplit: Scalable
veriﬁcation of deep neural networks via operator splitting,” arXiv
preprint arXiv:2106.09117, 2021.

[32] E. Wong and Z. Kolter, “Provable defenses against adversarial ex-
amples via the convex outer adversarial polytope,” in International
Conference on Machine Learning. PMLR, 2018, pp. 5286–5295.

[33] S. Boyd, N. Parikh, and E. Chu, Distributed optimization and statisti-
cal learning via the alternating direction method of multipliers. Now
Publishers Inc, 2011.

[34] S. Wang, H. Zhang, K. Xu, X. Lin, S. Jana, C.-J. Hsieh, and
J. Z. Kolter, “Beta-CROWN: Efﬁcient bound propagation with per-
neuron split constraints for complete and incomplete neural network
veriﬁcation,” Advances in Neural Information Processing Systems,
vol. 34, 2021.

[35] K. Dvijotham, R. Stanforth, S. Gowal, T. A. Mann, and P. Kohli,
“A dual approach to scalable veriﬁcation of deep networks.” in UAI,
vol. 1, no. 2, 2018, p. 3.

[36] M. Fazlyab, M. Morari, and G. J. Pappas, “An introduction to neural
network analysis via semideﬁnite programming,” in 2021 60th IEEE
Conference on Decision and Control (CDC).
IEEE, 2021, pp. 6341–
6350.

[37] A. Raghunathan, J. Steinhardt, and P. Liang, “Semideﬁnite relaxations
for certifying robustness to adversarial examples,” arXiv preprint
arXiv:1811.01057, 2018.

[38] Y. Zheng and G. Fantuzzi, “Sum-of-squares chordal decomposition
of polynomial matrix inequalities,” Mathematical Programming, pp.
1–38, 2021.

[39] Y. Zheng, G. Fantuzzi, A. Papachristodoulou, P. Goulart, and A. Wynn,
“Fast admm for semideﬁnite programs with chordal sparsity,” in 2017
American Control Conference (ACC).
IEEE, 2017, pp. 3335–3340.
[40] R. P. Mason and A. Papachristodoulou, “Chordal sparsity, decom-
posing sdps and the lyapunov equation,” in 2014 American Control
Conference.

IEEE, 2014, pp. 531–537.

[41] L. P. Ihlenfeld and G. H. Oliveira, “A faster passivity enforcement
method via chordal sparsity,” Electric Power Systems Research, vol.
204, p. 107706, 2022.

[42] H. Chen, H.-T. D. Liu, A. Jacobson, and D. I. Levin, “Chordal decom-
position for spectral coarsening,” arXiv preprint arXiv:2009.02294,
2020.

[43] B. Batten, P. Kouvaros, A. Lomuscio, and Y. Zheng, “Efﬁcient neural
network veriﬁcation via layer-based semideﬁnite relaxations and linear
cuts,” in International Joint Conference on Artiﬁcial Intelligence
(IJCAI21), 2021, pp. 2184–2190.

[44] J. Lan, A. Lomuscio, and Y. Zheng, “Tight neural network veriﬁcation
via semideﬁnite relaxations and linear reformulations,” in Proccedings
of the 36th AAAI Conference on Artiﬁcial Intelligence (AAAI22), 2021.
[45] T. Chen, J. B. Lasserre, V. Magron, and E. Pauwels, “Semialgebraic
optimization for lipschitz constants of relu networks,” Advances in
Neural Information Processing Systems, vol. 33, pp. 19 189–19 200,
2020.

[46] R. A. Brown, E. Schmerling, N. Azizan, and M. Pavone, “A uniﬁed
view of sdp-based neural network veriﬁcation through completely
positive programming,” in International Conference on Artiﬁcial In-
telligence and Statistics. PMLR, 2022, pp. 9334–9355.

[47] M. Newton and A. Papachristodoulou, “Neural network veriﬁcation
using polynomial optimisation,” in 2021 60th IEEE Conference on
Decision and Control (CDC).

IEEE, 2021, pp. 5092–5097.

[48] ——, “Sparse polynomial optimisation for neural network veriﬁca-

tion,” arXiv preprint arXiv:2202.02241, 2022.

[49] A. Griewank and P. L. Toint, “On the existence of convex decompo-
sitions of partially separable functions,” Mathematical Programming,
vol. 28, no. 1, pp. 25–49, 1984.

[50] J. Lofberg, “Pre-and post-processing sum-of-squares programs in
practice,” IEEE transactions on automatic control, vol. 54, no. 5, pp.
1007–1011, 2009.

[51] K. Xu, Z. Shi, H. Zhang, Y. Wang, K.-W. Chang, M. Huang,
B. Kailkhura, X. Lin, and C.-J. Hsieh, “Automatic perturbation analy-
sis for scalable certiﬁed robustness and beyond,” Advances in Neural
Information Processing Systems, vol. 33, pp. 1129–1141, 2020.
[52] M. Fazlyab, M. Morari, and G. J. Pappas, “Probabilistic veriﬁcation
and reachability analysis of neural networks via semideﬁnite program-
ming,” in 2019 IEEE 58th Conference on Decision and Control (CDC).
IEEE, 2019, pp. 2726–2731.

[53] A. Xue, L. Lindemann, A. Robey, H. Hassani, G. J. Pappas, and
R. Alur, “Chordal sparsity for lipschitz constant estimation of deep
neural networks,” arXiv preprint arXiv:2204.00846, 2022.

