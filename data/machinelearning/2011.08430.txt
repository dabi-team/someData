Deep Reinforcement Learning for Stochastic
Computation Ofﬂoading in Digital Twin Networks

Yueyue Dai, Member, IEEE, Ke Zhang, Sabita Maharjan, Senior Member, IEEE,
and Yan Zhang, Fellow, IEEE

1

0
2
0
2

v
o
N
8
1

]

G
L
.
s
c
[

2
v
0
3
4
8
0
.
1
1
0
2
:
v
i
X
r
a

Abstract—The rapid development of Industrial Internet of
Things (IIoT) requires industrial production towards digitaliza-
tion to improve network efﬁciency. Digital Twin is a promising
technology to empower the digital transformation of IIoT by cre-
ating virtual models of physical objects. However, the provision
of network efﬁciency in IIoT is very challenging due to resource-
constrained devices, stochastic tasks, and resources heterogeneity.
Distributed resources in IIoT networks can be efﬁciently exploited
through computation ofﬂoading to reduce energy consumption
while enhancing data processing efﬁciency. In this paper, we
ﬁrst propose a new paradigm Digital Twin Networks (DTN) to
build network topology and the stochastic task arrival model
in IIoT systems. Then, we formulate the stochastic computation
ofﬂoading and resource allocation problem to minimize the long-
term energy efﬁciency. As the formulated problem is a stochastic
programming problem, we leverage Lyapunov optimization tech-
nique to transform the original problem into a deterministic per-
time slot problem. Finally, we present Asynchronous Actor-Critic
(AAC) algorithm to ﬁnd the optimal stochastic computation of-
ﬂoading policy. Illustrative results demonstrate that our proposed
scheme is able to signiﬁcantly outperforms the benchmarks.

Index Terms—Digital twin, Industrial Internet of Things, Deep

reinforcement learning, Computation ofﬂoading.

I. INTRODUCTION

The Industrial Internet of Things (IIoT) is an enabling
technology of Cyber-Physical Systems (CPSs) that can equip
the industrial units, such as sensors, instruments, and devices
with the ability to communicate and interact with each other.
The IIoT has undergone rapid technological development
in recent years. According to the report from International
Data Corporation (IDC) [1], the number of connected devices
will reach 41.6 billion and these devices are predicted to
generate nearly 80 zettabytes of data by 2025. The high spread
of IIoT requires industrial production towards network and
digitalization.

Digital twin is a powerful technology to enable the digital
transformation by creating virtual models of physical objects
in the digital way, as shown in Fig. 1. The virtual models can
understand the state of the physical entities through sensing
data, so as to predict, estimate, and analyse the dynamic

This research was supported in part by the National Natural Science
Foundation of China under Grant No. 61941102 and in part by the Xi’an
Key Laboratory of Mobile Edge Computing and Security, under Grant No.
201805052ZD3CG36. (Corresponding author: Yan Zhang)

Y. Dai and K. Zhang are with the School of Information and Communica-
tion Engineering, University of Electronic Science and Technology of China,
Chengdu 611731, China (email:yueyuedai@ieee.org; zhangke@uestc.edu.cn).
S. Maharjan and Y. Zhang are with Department of Informatics, University
of Oslo, Norway, and also with Simula Metropolitan Center for Digital
Engineering, Norway. (email: sabita@iﬁ.uio.no, yanzhang@ieee.org).

Fig. 1: Concept of digital twin

twin,

twin is ﬁrst proposed in
changes. The concept of digital
[2] and applied by NASA to comprehensive diagnosis and
maintenance of ﬂight systems. Recently, digital twin has been
expanded to smart cities, manufacturing and IIoT. Exploiting
the network topology and physical elements
digital
in IIoT can be well mirrored and we can make system
management based on the mirrored models. However, there
are many technical challenges in applying digital twins to
IIoT. First, massive data collected from various IIoT devices
needs to be processed timely. But
the limited computing
resources available at the local servers cannot support fast
data processing and digital twin modelling in IIoT networks
[3], [4]. Second, the interaction between the virtual models
and the physical objects in a digital twin-enabled network
requires frequent communication between them. Moreover,
since the communication is wireless, the stochastic associated
with the wireless channel may result in a poor transmission
link, correspondingly longer service delay.

The IIoT applications, such as data analytics and smart man-
ufacturing, involve plenty of computation tasks. To improve
data/task processing efﬁciency and prolong battery lifetime of
IIoT devices, computation ofﬂoading is a promising approach
which ofﬂoads the collected data and computation tasks to
distributed servers to process, such as base stations, access
points, and road-side units in an IIoT network [5], [6]. There
has been considerable amount of work focusing on computa-
tion ofﬂoading in wireless networks and vehicular networks.
The authors in [7] proposed to ofﬂoad computation tasks to
lightweight and distributed road-side units to minimize task
processing latency in vehicular networks. The authors in [8]
proposed a joint computation ofﬂoading, power allocation, and
channel assignment problem to maximize the achievable sum
rate for 5G-enabled trafﬁc management systems. The authors
in [9] proposed to ofﬂoad computation tasks to multiple
distributed Small-cell Base Stations (SBS) and Macro-cell

 
 
 
 
 
 
2

Fig. 2: The digital twin network

Base Stations (MBS) to minimize energy consumption in 5G
networks. The above works typically assumed that each device
executes a single computation task without considering the
randomness of task arrivals [10], [11]. Such assumptions make
the computation ofﬂoading design not practical for IIoT net-
works. Since devices in IIoT networks continuously generate
data, stochastic task arrival model is more reasonable and
long-term computation performance needs to be considered.
Moreover, in an IIoT network with heterogeneous resources,
it is challenging to jointly optimize ofﬂoading decision, trans-
mission power, bandwidth and computation resource while
also incorporating time-varying channel condition.

Deep Reinforcement Learning (DRL) is an emerging tech-
nique to address problems with characterized with time-
varying feature [12], [13]. State-of-the-art works have utilized
DRL for optimizing computation ofﬂoading in wireless net-
works and vehicular networks. For instance, the authors in
[14] proposed a Deep-Q Network (DQN) based task ofﬂoading
scheme to select the optimal edge server and the optimal
transmission mode to maximize task ofﬂoading utility in
vehicular networks. The authors in [15] proposed the double
DQN based backscatter-aided hybrid data ofﬂoading scheme
to reduce power consumption for data transmission. The
authors in [16] proposed a Deep Deterministic Policy Gradient
(DDPG) based computation ofﬂoading and resource allocation
scheme to minimize system energy consumption in wireless
networks. These works however mainly focus on to choosing
whether to execute tasks locally or ofﬂoad to edge servers
with a deterministic task arrival model. These solutions are
not directly applicable to IIoT networks since the task arrival
model is stochastic.

In this paper, we ﬁrst propose a Digital Twin Network
(DTN) which utilizes digital twin to establish an efﬁcient
mapping between IIoT and digital systems. In the proposed
DTN network, virtual models of IIoT entities can be created
by monitoring real-time states of devices and base stations.
Then, we formulate stochastic computation ofﬂoading problem
as an optimization problem. Based on the virtual models and
monitored information of digital twin, we design a DRL-
based algorithm to solve the formulated problem. Our main
contributions in this paper are summarized as follows:

• We propose an architecture to integrate digital twin with

the IIoT network to model network topology, physical
devices and base stations.

• We formulate stochastic computation ofﬂoading problem
as an optimization problem, and utilize Lyapunov opti-
mization technique to equivalently transform the original
problem to a deterministic per-time slot problem.

• We adopt Asynchronous Actor-Critic (AAC), a DRL-
based algorithm, to solve the computation ofﬂoading and
resource allocation problem. Numerical results demon-
strate that our proposed algorithm signiﬁcantly outper-
forms the benchmark policies.

II. SYSTEM MODEL DESCRIBED BY DIGITAL TWIN

A. The Digital Twin Network

Our digital twin network architecture is shown in Fig. 2,

which consists of physical IIoT network and digital twin.

The physical IIoT network has three major components,
i.e., distributed IIoT devices, SBSs, and centralized MBS.
Each device collects the data from sensors and on-device
applications, and they need to analyse the collected data.
As data analysis is computation-intensive, devices with lim-
ited computation capability and battery, may not be able to
complete them timely. So they have to ofﬂoad these tasks
to edge servers for a high level of quality of computation
experience. SBSs are equipped with edge servers and they
can provide devices computation resources. However, since an
SBS often serves several devices, to ensure the requirements of
all devices are satisﬁed, SBSs need to optimize computation
resources, bandwidth and transmission power. The MBS is
equipped with an edge server and an DRL agent, thus the MBS
has sufﬁcient communication, computation and AI-enabled
processing capabilities.

The digital twin is a virtual model of physical elements
and a digital representation of the physical system. Different
from a virtual prototype, digital twin not only mirrors the
characteristic of physical elements/system but also makes
prediction, simulates the system, and can play a crucial role
in optimizing the resources. In our digital twin network ,
we can utilize digital
the network
topology of physical IIoT, (2) monitor network parameters
and models, i.e., dynamic changes of resources and stochastic
task arrival processes, (3) optimize ofﬂoading and resource

twin to (1) construct

allocation policy. Speciﬁcally, we deploy different functions
on devices, SBSs and the MBS to build digital twin network.
Devices run two DT functions: data collection and parameter
synchronization. SBSs also run two DT functions: building
local virtual models of devices and SBSs, and synchronizing
parameters. The main functions of the MBS are to construct
the network topology of the physical network and to design
ofﬂoading and scheduling policy.

B. Network Model

Based on digital twin, the digital representation (i.e., virtual
models) of the physical network is created (i.e., virtual world).
The digital models here contain wireless network topology,
communication model between devices and base stations, and
the stochastic task queueing model.

1) Network Topology and Communication Model for DTN
Digital twin ﬁrstly models the wireless edge network as a
discrete time-slotted system. A directed graph G = (U, B, ε)
is used to represent the network, where U = {u1, .., uN }
and B = {b0, b1, ..., bM } denote the set of devices and base
stations (b0 is the index for MBS) respectively. ε denotes
the association between devices and base stations. That is,
if device ui is connected to SBS bj, the link will be recorded
in edge set ε. Then, the digital twin uses a 3-tuple DTi(t) to
characterize devices, i.e.,

DTi(t) = {pi,max(t), li(t), f l
i }

(1)

where pi,max(t) denotes the maximal transmission power at
time slot t, li(t) denotes the current location of ui, and f l
i
denotes the computation resources of local server. Similarly,
the digital twin uses a 3-tuple DTj(t) to characterize base
stations, i.e.,

DTj(t) = {lj(t), wj, f e
j }

(2)

where lj(t) denotes the current location of bj, wj denotes the
channel bandwidth of bj, f e
j denotes the computation resource.
The task ofﬂoading between devices and base stations is
facilitated through wireless communication. According to [9],
wireless data rate is related to spectrum,
interference and
to utilize spectrum efﬁ-
bandwidth. In wireless networks,
ciently, SBSs reuse the MBS’s frequency resource and Orthog-
onal Frequency Division Multiple Access (OFDMA) is often
adopted to suppress the interference. Thus, the interference
between the MBS and SBSs can be ignored. Here, we consider
devices communicate with the nearest base station to perform
computation ofﬂoading. γs
j is deﬁned as the coverage radius of
SBS bj. If the distance between device ui and SBS bj is less
than γs
j ), device ui can communicate with
bj. The wireless communication data rate between device ui
and SBS bj can be expressed as

ij(t) < γs

j (i.e., rs

Rs

ij(t) = wij(t) log(1 +

pi(t)hs

ij(t)−α

ij(t)rs
σ2 + I

),

(3)

where wij(t) (wij(t) ≤ wj) is the bandwidth that SBS
bj allocates to device ui at time slot t, hs
ij(t) is the cur-
rent channel gain, α is path loss exponent, σ2 is noise
power, and rs
ij(t) is calculated based on the location of
li(t) and lj(t) (i.e., rs
ij(t) =(cid:107) li(t) − lj(t) (cid:107)). I =

3

(cid:80)

i(cid:48)∈U /{i},j(cid:48)∈B/{j} pi(cid:48)(t)hs

i(cid:48)j(cid:48)(t)rs

i(cid:48)j(cid:48)(t)−α is the interference

from other SBSs.

If device ui does not lie within the coverage of any SBS, it
will communicate with the MBS. The wireless communication
data rate between device ui and the MBS is

Rm

i0(t) = wi0(t) log(1 +

pi(t)hm

i0 (t)−α

i0(t)rm
σ2

),

(4)

where wi0(t) (wi0(t) ≤ w0) is the channel bandwidth between
device ui and the MBS at time slot t, hm
i0(t) is the channel
gain between device ui and the MBS at time slot t, rm
i0 (t) =(cid:107)
li(t) − l0(t) (cid:107) is the distance between device ui and the MBS.

2) Stochastic Task Queueing Model for DTN
At the beginning of time slot t, device ui generates and
stores λi(t) (bits/slot) of computation tasks into the local
dataset. Without loss of generality, we assume λi(t) in dif-
ferent time slots are independent, and E[λi(t)] = λ. Due to
the limitation of computation resources, each device executes
part of the computation tasks at its local server and ofﬂoads
part of them to the associated base station. The rest will be
queueing in the local task buffer and we consider the buffer
has sufﬁcient capacity. We denote the size of the computation
tasks that is executed locally as Dl
i(t) and the size of the
computation tasks ofﬂoaded to base station bj(j ∈ B) as
De
ij(t). The queue length of local task buffer at the beginning
of time slot t on device uj is denoted as Ql
i(t) and the queue
length is dynamically updated with the following equation:

Ql

i(t + 1) = max{Ql

i(t) − Ψi(t), 0} + λi(t)

(5)

where Ψi(t) = Dl
ij(t) is the size of the computation
tasks that leaves the task buffer of device ui during time slot
t.

i(t) + De

Each edge server also has a task buffer to store the ofﬂoaded
but not yet executed task. We denote the queue length of edge
task buffer at the beginning of time slot t on base station
bj(j ∈ B) as Qe
j(t). The queue length is dynamically updated
by:

Qe

j(t + 1) = max{Qe

j(t) − Ψj(t), 0} +

De

ij(t)

(6)

(cid:88)

i∈U

i∈U De

where (cid:80)
ij(t) is the amount of tasks ofﬂoaded from
devices during time slot t. Ψj(t) is the size of the computation
tasks that departs edge task buffer (i.e., executed by edge
server). According to the deﬁnition of stability in [17], task
queue is stable if all computation tasks satisfy the following
constraints:

lim
T →∞

lim
T →∞

1
T

1
T

T −1
(cid:88)

(cid:88)

t=0

i∈U

T −1
(cid:88)

(cid:88)

t=0

j∈B

E{Ql

i(t)} < ∞

E{Qe

j(t)} < ∞

(7a)

(7b)

C. Task Ofﬂoading Model

During time slot t, device ui executes Dl

i(t) locally and
ij(t) to base station bj. Next, we will calculate

ofﬂoads De

the energy consumption of local execution and computation
ofﬂoading.

1) Local Execution:
Let f l

i (t) denote the computation resource (i.e., CPU cycles
per second) of device ui during time slot t. c denotes the
required number of CPU cycles for executing one bit of
computation task. Thus, the size of computation tasks executed
locally will be

Dl

i(t) =

τ f l
i (t)
c

,

(8)

where τ is duration of the time slot.

The energy consumption of unit computation resource is
ς(f l
i )2, where ς is the effective switched capacitance depend-
ing on the chip architecture [9]. We denote local energy
consumption for computing task Dl
i(t), which can
be deﬁned as

i(t) as El

El

i(t) = ςτ f l

i (t)3.

(9)

2) Edge Server Execution:
Devices ofﬂoad their tasks to base stations via wireless
communication. Since devices are associated with different
base stations, the ofﬂoaded tasks of device ui during time
slot t can be expressed as
(cid:40)

De

ij(t) =

Rs
Rm

ij(t)τ
i0(t)τ

j ∈ B/{b0}
j = b0

(10)

The energy consumption in this case consists of three parts.
The ﬁrst one is the energy consumption of uplink wireless
transmission for ofﬂoading. The second one is the computation
energy consumption which is related to the allocated com-
putation resources. The third one is the energy consumption
of downlink wireless transmission for ofﬂoading computation
result to the devices. Since the size of the result is very small,
we ignore the energy consumption for downlink transmission.
Thus, the energy consumption for executing task De
ij(t) on
base station bj is given by

Ee

ij(t) = pi(t)τ +

De

ij(t) ∗ c
f e
ij(t)

∗ ε,

(11)

where f e
ij(t) is the computation resource that bj allocates to
device ui at time slot t, ε is the energy consumption for unit
computation on edge servers.

The total energy consumption is consisted of the energy
consumption of task execution in the local and edge servers, as
well as the transmission energy consumption for computation
ofﬂoading. Therefore, the total energy consumption can be
expressed as

Etol(t) =

El

i(t) +

(cid:88)

i∈U

(cid:88)

(cid:88)

i∈U

j∈B

Ee

ij(t),

(12)

III. PROBLEM FORMULATION

In this section, we ﬁrst formulate the stochastic computation
ofﬂoading problem of DTN as an optimization problem, and
then transform the formulated problem based on Lyapunov
optimization.

4

A. Stochastic Computation Ofﬂoading Problem

The objective of stochastic computation ofﬂoading problem
is to minimize network efﬁciency ηEE. ηEE is deﬁned as the
ratio of long-term total energy consumption to the correspond-
ing long-term aggregate accomplished computation tasks, i.e.,

(cid:80)T −1
t=0

E{Etol(t)}

1
T
(cid:80)

limT →∞
1
T

(cid:80)T −1
t=0

ηEE =

(cid:80)

E{Dt

i∈U

j∈B

i(t) + De

limT →∞

The system operation at

ij(t)}
(13)
time slot t can be denoted
as a(t) = [w(t), p(t), Ψ(t), f l(t), f e(t)], where w(t) =
[w10(t), ..., wNM (t)] is bandwidth allocation vector, p(t) =
is transmission power vector, Ψ(t) =
[p1(t), ..., pN (t)]
[Ψ0(t), ..., ΨM (t)] is the vector associated with the compu-
tation task that leaves edge servers, f l(t) = [f l
N (t)]
and f e(t) = [f e
NM (t)] are the vector of computation
resource that edge servers allocate to devices. Taking network
stability constraint into account, the optimization problem for
minimizing ηEE can be formulated as:

10(t), ..., f e

1(t), ..., f l

s.t.

≤ 1, wij(t) ≥ 0

ηEE

P 1 : min
a(t)
wij(t)
wj

(cid:88)

i∈U
0 (cid:54) pi(t) (cid:54) pi,max(t),
i (t) (cid:54) f l
0 (cid:54) f l
i ,
(cid:88)
j , f e
ij(t) ≤ f e
f e

ij(t) ≥ 0

i∈U
Ψj(t) ∗ c ≤ f e
(7a) − (7b)

j τ, Ψj(t) ≥ 0

(14a)

(14b)

(14c)

(14d)

(14e)

Constraint (14a) is the bandwidth allocation constraint.
Constraints (14b) and (14c) denote the transmission power
and computation resource constraints, respectively. Constraint
(14d) ensures that the sum of the computation resource of
each base station allocated to all devices does not exceed
the total amount of computation resource the base station
has. Constraint (14e) implies that the amount of computation
resource for processing task Ψj cannot exceed the available
computation resources. Constraints (7a) and (7b) guarantee
that the stability of all task queues.

Since computation resource, transmission power and band-
width need to be determined at each time slot, P1 is a
stochastic optimization problem, which is challenging to solve
by applying classic convex optimization algorithms such as
interior-point method and Lagrangian duality theory. From
Eq. (10) and Eq. (11), wireless communication rate and
allocated computation resource jointly determine the energy
consumption of edge server execution. Thus, the radio re-
source management problem is coupled with the computation
resource allocation problem. Moreover, in radio resource man-
agement problem, bandwidth and transmission power are also
highly coupled. The complex coupling among optimization
variables and mixed combinatorial feature makes it difﬁcult to
solve P1. Further, the stochastic task arrival, dynamic channel

state information and dynamic task buffer make designing an
efﬁcient resource management policy for devices and edge
servers quite challenging.

Lyapunov optimization is a powerful methodology for
solving optimization problems with long-term objective and
constraints, which requires less prior information on the task
arrival, channel state information, and task buffer. The prin-
cipal idea behind Lyapunov optimization is to transform the
optimization problem with long-term objective into a series
of subproblems with short-term objective, and to transform
the long-term constraints into constraints with queue stability.
Besides, Lyapunov optimization is of low computational com-
plexity by optimizing subproblem through an online algorithm.
In this paper, we exploit Lyapunov optimization to transform
the original stochastic optimization problem as a deterministic
per-time block problem and propose a stochastic computation
ofﬂoading algorithm to solve P1.

Algorithm 1 The Stochastic Computation Ofﬂoading Algo-
rithm with Digital Twin-predicted Perturbation for DTN

5

1: At

the beginning of each time slot, digital

twin ﬁrst
predicts perturbation vector β based on local task queue
and Eq. (19),

2: Digital twin observes Θ(t) and λi(t) and determines a(t)

by solving the following problem in each time slot,

P 2 : min
a(t)

V [Etol(t) − ηEE(t)

(cid:88)

(cid:88)

(Dt

i (t) + De

ij(t))] +

(cid:88)

j∈B

{Qe

j (t)[

(cid:88)

De

ij(t) − Ψj(t)]} −

[Ql

i(t) − βi][Ψi(t) − λi(t)]

i∈U

j∈B
(cid:88)

i∈U
i∈U
(7a) − (7b), (14a) − (14e)

s.t.

(20)

3: Updates Ql
4: t = t + 1.

i(t) and Qe

j(t) based on Eq. (5) and (6),

B. Lyapunov-based Problem Transformation and Digital twin-
predicted Perturbation

To construct a Lyapunov optimization framework, we add a
perturbation vector β = [β1, ..., βN ] in Lyapunov function to
keep the value of this function always small. The perturbation
parameters are simulated in the digital twin and it will be given
in the following. We deﬁne the quadratic Lyapunov function
as the sum of squared queue backlogs,

optimal solution of P1. According to [18], perturbation vector
β is very important as it
inﬂuences the performance of
optimization of the designed algorithm. Based on the deﬁnition
of perturbation vector in [18], β is the maximal lower bound
of local task queue. Since digital twin is a mirror of physical
network, it can easy get any information of the network and
predicts each one of β based on

βi = V η(cid:48)

EE(t) + Ψmax

(19)

L(Θ(t)) =

1
2

{

(cid:88)

[Ql

i(t) − βi]2 +

i∈U

Qe

j(t)2}

(cid:88)

j∈B

(15)

where Ψmax = max(Ψi(t)).

where Θ(t) = [Ql(t), Qe(t)] represents current task queue
lengths of devices and edge servers. Further, we deﬁne the
conditional drift as

(cid:52)L(Θ(t)) = E[L(Θ(t + 1)) − L(Θ(t))|Θ(t)]

(16)

By minimizing (cid:52)L(Θ(t)), we can short the length of task
queues towards a smaller value.

Accordingly, the Lyapunov drift-plus-penalty function can

be expressed as

(cid:52)V L(Θ(t)) = (cid:52)L(Θ(t)) + V E[ηEE(t)|Θ(t)]
i(t) + De

where ηEE(t) = Etol(t)/ (cid:80)
ij(t)), V is
i∈U
a non-negative weight parameter. By minimizing (cid:52)V L(Θ(t)),
we can ensure network stability, and meanwhile minimize
network EE. We derive the upper bound of (cid:52)V L(Θ(t)) as,
i(t) − βi]E[Ψi(t) − λi(t)|Θ(t)]

(cid:52)V L(Θ(t)) ≤ C −

j∈B(Dt

(17)

[Ql

(cid:88)

(cid:80)

−

(cid:88)

j∈B

i∈U
j(t)E[Ψj(t) −

Qe

(cid:88)

De

ij(t)|Θ(t)]} + V E[ηEE(t)|Θ(t)].

i∈U

(18)

1
2

{(cid:80)

i∈U [Ψ2

i,max + λ2

where C =
((cid:80)
i∈U De
are the upper bounds of Ψi(t), λi(t), Ψj(t) and De
spectively.

ij,max)2]} and Ψi,max, λi,max, Ψj,max and De

j∈B[Ψ2

i,max] + (cid:80)

j,max +

ij,max
ij(t), re-

Based on the Lyapunov optimization theory, we can min-
imize the right side of the inequality in (18) to obtain the

Thus, we ﬁrst utilize digital twin to simulate the perturbation
parameter of devices and then optimize the right side of the
inequality in (18) in each time slot. The proposed stochastic
computation ofﬂoading algorithm for DTN is shown in Al-
gorithm 1, where P2 needs to be solved per-time slot. The
traditional method to solve P2 is to decompose it into several
sub-problems and alternatively solving sub-problems until it
converges to the global optimal solution. However, when wire-
less channels change or task queues update, each sub-problem
needs to recalculate the optimal solution. Frequent operations
to solve sub-problems will inﬂuence the convergence. DRL is
an emerging technique which can ﬁnd a near optimal solution
in a real-time manner. Thus, we design a DRL-based algorithm
to ﬁnd the optimal solution of P2.

IV. DRL-EMPOWERED STOCHASTIC COMPUTATION
OFFLOADING ALGORITHM FOR DTN

The framework of the digital twin enabled DRL algorithm
is illustrated in Fig. 3. Digital
twin mirrors the network
topology and parameters of physical wireless network and
transmits network state to DRL. DRL derives the best strategy
to minimize network energy efﬁciency.

A. Digital Twin-simulated Network Environment

To solve P2, the system ﬁrst constructs Markov decision
process, i.e., M = (S, A, P, R), and then use DRL algorithm
to explore actions. From Fig. 3, the network state s(t) is
constructed by digital twin and outputted to DRL agent.

6

Fig. 3: Digital twin enabled DRL algorithm

twin can adopt

To gather network environment information, digital twin
needs to predict location, energy and the generated task ﬂow
of devices and base stations. Digital
the
existing K-Nearest Neighbors (KNN) classiﬁcation method
and position prediction algorithm in [19] to predict users’
location. The maximal current transmission power pi,max(t)
time slot t is the summarize of the predicted energy
at
and the pi,max(t − 1). The generated task ﬂow is based on
the application running on each device. After gathering the
network information, digital twin updates network topology,
channel condition and queueing model. The main operation in
the update of network topology is to make user association
decision, which decides the connection between devices and
base stations. Here we adopt online user association method in
[20]. Then, digital twin generates current state and transmits
it to the DRL agent.

Thus, at

the beginning of time slot t,

the DRL agent
construct system state which includes transmission data rates
between devices and base stations, available bandwidth, com-
putation resources, transmission power, and queue length. We
can deﬁne system state s(t) ∈ S at time slot t as

• Ψ(t) = [Ψ0(t), ..., ΨM (t)]: 1 × (M + 1) vector where
j τ /c is the computation task that leaves

0 ≤ Ψj(t) ≤ f e
edge server j;

• f l(t) = [f l

1(t), ..., f l

allocation vector where f l

• f e(t) = [f e

10(t), ..., f e

N (t)]: N × 1 computation resource
i (t) ∈ [0, f l
i ];
NM (t)] : N × (M + 1) computation

resource allocation matrix where f e

ij(t) ∈ [0, f j
e ];

It is worth noting that all variables in action a(t) are con-
tinuous. Thus, we will utilize a policy gradient-based DRL
algorithm to explore policy.

After executing action a(t), digital twin updates system
state and estimates immediate reward Rimm(s(t), a(t)). In
a traditional Markov decision process, the system updates
its state based on the given transition probability P r(s(t +
1)|s(t), a(t)). However, in DRL, the distribution of transition
probability is often unknown. The DRL agent utilizes deep
neural network to approximate it.

The immediate reward function is deﬁned as the objective

of P2 problem, i.e.,

Rimm(s(t), a(t)) = −V [Etol(t) − ηEE(t)

(cid:88)

(cid:88)

(Dt

i (t) + De

ij(t))]

s(t) = {R(t), F, pmax(t), w, Θ(t)}.

(21)

+

(cid:88)

{Qe

j (t)[Ψj(t) −

The state space S is as follows:

i∈U

De

ij(t)]} +

(cid:88)

i∈U

j∈B

(cid:88)

i∈U
[Ql

i(t) − βi][Ψi(t) − λi(t)]

i∈U

(22)

• R(t) = {[Rs
10(t), ..., Rm
[Rm
matrix where Rs

11(t), ..., Rs
NM (t)],
N 0(t)]}: N × (M + 1) wireless data rate

1M (t)], ..., [Rs

N 1(t), ..., Rs

ij(t) ≥ 0 and Rm

i0(t) ≥ 0;

• F = [f l

1, ..., f l

0 , .., f e
resource vector where f l

N , f e

M ]: 1×(N +M +1) computation
i ≥ 0 and f e
j ≥ 0;

• pmax(t) = [p1,max(t), ..., pN,max(t)]: N ×1 transmission

power vector at time slot t;

• w = [w0, ..., wj]: 1 × (M + 1) bandwidth vector;
• Θ(t) = [Ql(t), Qe(t)], where Ql(t) = [Ql

N (t)]
indicates the queue length of the local task buffer and
Qe(t) = [Qe
M (t)] is the queue length of the
task buffer on edge servers.

0(t), ..., Qe

1(t), ..., Ql

Since a(t) = [w(t), p(t), Ψ(t), f l(t), f e(t)] in P2 denotes
system operation at time slot t, we deﬁne it as the output
from the DRL agent (i.e., action). The action space A includes
following ﬁelds:

• w(t) = [w10(t), ..., wNM (t)]: N × (M + 1) bandwidth

allocation matrix where wij(t) ∈ [0, wj];

• p(t) = [p1(t), ..., pN (t)]: represents N × 1 transmission

power vector where pi(t) ∈ [0, pi,max(t)];

After computing immediate reward, the system updates its
state from s(t) to s(t + 1) based on action a(t).

The objective of the DRL agent is to maximize the cumu-

lative reward,

R = max E

(cid:34)T −1
(cid:88)

(cid:35)
δtRimm(s(t), a(t))

,

(23)

t=0

is the discount factor. If all

where δ ∈ [0, 1]
tasks are
satisfying the constraints of P2, DRL agent gets a total reward.
Otherwise, the agent receives a penalty, which is a negative
constant.

B. Asynchronous Actor-Critic Algorithm

The DRL algorithm is classiﬁed into value-based and policy
gradient-based. Value-based DRL algorithms, such as DQN
and double DQN, estimate Q-values and (cid:15)-greedy strategy
to explore policy with discrete action space. But value-based
DRL algorithms are of limited value for problems with con-
tinuous action space. Policy gradient-based DRL can learn

stochastic policies effectively for tackling problems with con-
tinuous action space problems. The main idea of this method is
to optimize a parameterized stochastic policy by estimating the
gradient of the expected reward of the policy and then updating
the parameters of the policy in the gradient direction. We
deploy AAC algorithm in digital twin to optimize cumulative
reward R.

AAC is an asynchronous learning algorithm which utilizes
multiple agents to interact with its own environment and each
agent contains a replica of the environment [21]. A speciﬁc
AAC agent is Actor-Critic based, where Actor is used to
generate actions and Critic is used to evaluate and criticize
the current policy by processing the reward obtained from the
environment.

1) Actor-Critic based policy gradient training:
a(t) = π(s(t)|θπ) denotes the policy learned from current
state, where π(s(t)|θπ) is the explored ofﬂoading and resource
allocation policy produced by deep neural network of actor
network. The network parameter of actor network is denoted
as θπ and it is trained through the policy gradient method [21].
The gradient of the expected cumulative discounted reward is
calculated by,

∞
(cid:88)

Eπ[

(cid:53)θπ

δtRimm(t)] = Eπ[(cid:53)θπ log π(s(t)|θπ)Aπ(s, a)]

t=0

(24)
where Aπ(s, a) is the difference between the expected cu-
mulative discounted reward starting from state s when agent
chooses action a and follows policy π. Here, Aπ(s, a) is called
the advantage function which indicates whether things get
better or worse than expected. Aπ(s, a) is calculated using,
Aπ(s, a) = Rimm(s(t), a(t)) + δvθv (s(t + 1)) − vθv (s(t)),
(25)

The parameter θπ is updated based on:

θπ = θπ + απ

(cid:88)

t

(cid:53)θπ log π(s(t)|θπ)Aπ(s, a)],

(26)

where απ is the learning rate of the actor network. We use
critic network to estimate the cumulative discounted reward of
each state following the current actor network’s policy, which
is also expressed as the value of each state, vθv (s(t)). θv is
the network parameter of critic network. The parameter θv is
updated as follows:

θv = θv + αv

(cid:88)

t

(cid:53)θv (Rimm(s(t), a(t))+

(27)

δvθv (s(t + 1)) − vθv (s(t)))2

where αv is the learning rate of the critic network.

After the value function approximation vθv (s(t)) and pa-
rameter θv are updated by critic process, actor network then
uses the advantage function Aπ(s, a) outputted from the critic
process to update its policy parameters.

2) Asynchronous Learning with Experience Replay:
In DQN algorithm, there is an important component, i.e.,
replay memory, which disrupts the correlation between the
experiences such that the sequence in DRL meets the inde-
pendent and identical distribution. However, replay memory
needs an off-policy learning algorithm to generate experiences

Algorithm 2 Asynchronous Actor-Critic algorithm for Each
Learning Agent

7

π and θ(cid:48)
v

1: Assume agent parameter θ(cid:48)
2: Initialize learning step counter t = 1;
3: repeat
4:
5:

π = θπ and θv

Synchronize agent parameter θ(cid:48)
Update global shared counter T
tstart = t
Use digital twin to construct current state s(t)
repeat

Perform action a(t) for problem P2 based on

policy π(s(t)|θ(cid:48)

π)

Transfer to new state s(t + 1) and calculate imme-

diate reward Rimm(s(t), a(t))

t ← t + 1
T ← T + 1

until t − tstart == tmax
R = vθ(cid:48)
(s(t))
for i ∈ {t − 1, ..., tstart} do

v

reward.append(Rimm(s(t), a(t)) + δR)

16:
17:
18:
19: until T > Tmax

end for
send reward to global agent

Algorithm 3 Asynchronous Actor-Critic algorithm for Global
Agent

1: Assume global shared parameter θπ and θv, and global

shared counter T (cid:48) = 0

2: while receive reward from agent do
for i ∈ {t − 1, ..., tstart} do
3:
4:
5:

Accumulate gradients with respect to θ(cid:48)
(cid:80)
dθπ ← dθπ + απ
(cid:80)
dθv ← dθv + αv

log π(s(t)|θ(cid:48)
A2
π(s, a)

i (cid:53)θ(cid:48)
i (cid:53)θ(cid:48)

π

v

π and θ(cid:48)
v:
π)Aπ(s, a)]

end for
perform asynchronous update of θπ using dθπ and θv

6:
7:
8:
9:

10:

11:
12:
13:
14:
15:

6:
7:
8:

9:
10:

using dθv

if T > Tmax then

break

end if
11:
12: end while

and needs large amount of memory to store the generated
experience. AAC is an online DRL algorithm that can re-
duce correlation between adjacent samples by asynchronous
learning with considerably less amount of computation. To
implement AAC, DTN sets up a global agent and multiple
learning agents.The algorithms to be carried by learning agent
and global agent are given in Algorithm 2 and Algorithm 3,
respectively. Learning agent is deployed at SBS and it can
interact with its own environment and the environment of all
agents has same settings and structures. The learning agents
parallelly explore and accumulate ofﬂoading and resource
allocation policy. After every tmax learning steps, agents will
send the accumulated updates to the global agent. The global
agent is deployed in MBS and it asynchronously updates θπ
and θv. Tmax represents the max training episodes.

8

Fig. 4: System cost under different schemes.

Fig. 5: System cost with respect to the number of SBSs

V. NUMERICAL RESULTS

We consider a network topology with one MBS, M = 3
SBSs, and N = 20 devices. We consider Rayleigh fading
channels. The maximum transmission power of devices is
set to 100 mW. The noise power is σ2 = 10−11 mW. The
bandwidth of the MBS and each SBSs are 10 MHz and 5 MHz,
respectively. In addition, τ = 100 ms, c = 100 cycles/bit. The
CPU computation capabilities of the devices, the SBSs and the
MBS, are 0.5, 10, and 50 GHz, respectively. The actor network
of AAC has three fully-connected hidden layers each with
128 neurons whose activation function is ReLU and an output
layer with 8 neurons using softmax function as the activation
function. The critic network has three fully-connected hidden
layers each with 128 neurons whose activation function is
ReLU and one linear neuron as output. We use Python and
TensorFlow to evaluate the performance of our proposed
stochastic computation ofﬂoading algorithm. Based on the
deﬁnition of immediate reward (22), the minimization of P2
is equivalent to the maximization of DRL reward. For ease
of observation, we deﬁne the objective of P2 as system cost
and the system cost equals to the negative value of the DRL
cumulative reward.

Fig. 4 illustrates the relationship between system cost and
training episodes under different schemes. The blue curve
represents joint optimization of computation ofﬂoading, band-
width and transmission power, but without computation re-
source allocation. The green curve shows the performance of
the proposed scheme with computation ofﬂoading and compu-
tation resource allocation optimization but without bandwidth
and transmission power allocation. From Fig. 4 we can see the
performance of the red curve outperforms the two benchmarks,
since it can concurrently optimize computation ofﬂoading,
bandwidth and transmission power, and computation resource
the system cost of the blue curve is
allocation. Besides,
lower than the system cost of the green curve. This means,
compared with the optimization of computation resource, the
optimization of bandwidth and transmission power has a
greater inﬂuence on the performance.

Fig. 6: System cost with respect to the number of devices
under different schemes.

to the number of SBSs. We observe that when N = 40, the
value of system cost decreases with the increase of the number
of SBS. When N = 20, the value of system cost does not
change much with the increase of the number of SBS. This
indicates, when the number of devices is large, increasing the
number of SBSs can reduce system cost. When the number of
devices is small, increasing the number of base stations has
little effect on reducing system cost.

Fig. 6 shows the comparison of the system cost with respect
to the number of devices under different schemes. The number
of devices ranges from 10 to 40. From Fig. 6, we can draw
several observations. First, the system cost of three different
schemes respectively increases as the number of devices
becomes large. The reason is that the growth of devices leads
to more ofﬂoading requests, which results in the consumption
of more communication and computation resource. Second,
the performance of the proposed algorithm outperforms two
benchmarks by jointly optimizing computation ofﬂoading,
bandwidth and transmission power, and computation resource
allocation.

Fig. 5 plots the comparison of the system cost with respect

Fig. 7 illustrates the comparison of system cost with respect

9

allocation problem to jointly optimize ofﬂoading decision,
transmission power, bandwidth and computation resource.
As the formulated problem is a non-convex stochastic pro-
gramming problem, we leverage the Lyapunov optimization
technique to equivalently transform the original problem to a
deterministic per-time slot problem. Finally, we utilized AAC
algorithm to solve the computation ofﬂoading and resource
allocation problem. Numerical results demonstrate that our
proposed algorithm signiﬁcantly outperforms the benchmarks.

REFERENCES

[1] IDC, “The growth in connected iot devices is expected to generate 79.4
zb of data in 2025, according to a new idc forecast,” https://www.idc.
com/getdoc.jsp?containerId=prUS45213219, 2019.

[2] E. Glaessgen and D. Stargel, “The digital twin paradigm for future
nasa and us air force vehicles,” in 53rd AIAA/ASME/ASCE/AHS/ASC
structures, 2012, p. 1818.

[3] K. Zhang, S. Leng, Y. He, S. Maharjan, and Y. Zhang, “Mobile edge
computing and networking for green and low-latency internet of things,”
IEEE Commun. Mag., vol. 56, no. 5, pp. 39–45, 2018.

Fig. 7: System cost with respect to the number of devices
under different DRL algorithms.

[4] Y. Lu, X. Huang, K. Zhang, S. Maharjan,

“Communication-efﬁcient
networks
in industrial
doi:10.1109/TII.2020.3010798, 2020.

federated learning for digital
iot,”

IEEE Trans.

Ind.

and Y. Zhang,
twin edge
,

Inform., no.

[5] T. Taleb, K. Samdanis, B. Mada, H. Flinck, S. Dutta, and D. Sabella,
“On multi-access edge computing: A survey of the emerging 5g network
edge cloud architecture and orchestration,” IEEE Commun. Surv. Tutor.,
vol. 19, no. 3, pp. 1657–1681, 2017.

[6] X. Li, J. Wan, H. Dai, M. Imran et al., “A hybrid computing solution
and resource scheduling strategy for edge computing in smart manufac-
turing,” IEEE Trans. Ind. Inform., vol. 15, no. 7, pp. 4225–4234, 2019.
[7] Y. Dai, D. Xu, S. Maharjan, and Y. Zhang, “Joint load balancing and
ofﬂoading in vehicular edge computing and networks,” IEEE Internet
Things J., vol. 6, no. 3, pp. 4377–4387, 2019.

[8] Z. Ning, X. Wang, F. Xia et al., “Joint computation ofﬂoading, power
allocation, and channel assignment for 5g-enabled trafﬁc management
systems,” IEEE Trans. Ind. Inform., vol. 15, no. 5, pp. 3058–3067, 2019.
[9] Y. Dai, D. Xu, S. Maharjan, and Y. Zhang, “Joint computation ofﬂoading
and user association in multi-task mobile edge computing,” IEEE Trans.
Veh. Technol., vol. 67, no. 12, pp. 12 313–12 325, 2018.

[10] Y. Mao, J. Zhang, S. Song, and K. B. Letaief, “Stochastic joint radio
and computational resource management for multi-user mobile-edge
computing systems,” IEEE Trans. Wirel. Commun., vol. 16, no. 9, pp.
5994–6009, 2017.

[11] S. Mao, S. Leng, S. Maharjan, and Y. Zhang, “Energy efﬁciency and
delay tradeoff for wireless powered mobile-edge computing systems
with multi-access schemes,” IEEE Trans. Wirel. Commun., vol. 19, no. 3,
pp. 1855–1867, 2020.

[12] K. Ahmed, H. Tabassum, and E. Hossain, “Deep learning for radio
resource allocation in multi-cell networks,” IEEE Netw., vol. 33, no. 6,
pp. 188–195, 2019.

[13] Y. Dai, D. Xu, K. Zhang, S. Maharjan, and Y. Zhang, “Deep rein-
forcement learning and permissioned blockchain for content caching
in vehicular edge computing and networks,” IEEE Trans. Veh. Technol.,
vol. 69, no. 4, pp. 4312–4324, 2020.

[14] K. Zhang, Y. Zhu, S. Leng, Y. He, S. Maharjan, and Y. Zhang, “Deep
learning empowered task ofﬂoading for mobile edge computing in urban
informatics,” IEEE Internet Things J., vol. 6, no. 5, pp. 7635–7647, 2019.
[15] Y. Xie, Z. Xu, J. Xu, S. Gong, and Y. Wang, “Backscatter-aided hybrid
data ofﬂoading for mobile edge computing via deep reinforcement learn-
ing,” in International Conference on Machine Learning and Intelligent
Communications. Springer, 2019, pp. 525–537.

[16] Y. Dai, K. Zhang, S. Maharjan, and Y. Zhang, “Edge intelligence for
energy-efﬁcient computation ofﬂoading and resource allocation in 5g be-
yond,” IEEE Trans. Veh. Technol., no. , doi: 10.1109/TVT.2020.3013990,
2020.

[17] M. J. Neely, “Stochastic network optimization with application to
communication and queueing systems,” Synthesis Lectures on Commu-
nication Networks, vol. 3, no. 1, pp. 1–211, 2010.

[18] L. Huang and M. J. Neely, “Utility optimal scheduling in energy-
harvesting networks,” IEEE/ACM Trans. Netw., vol. 21, no. 4, pp. 1117–
1130, 2012.

Fig. 8: Impact of the learning rate on performance.

to number of devices under different DRL algorithms. The
number of devices ranges from 5 to 20. We can observe that
the system cost increases with the increase in the number of
devices. For given amount of computation resources, a large
number of devices results in a low task execution rate and high
energy consumption. This inevitably increases system cost.
Moreover, our AAC based algorithm performs considerably
better compared to the DQN. The main reason is that action
discretization in DQN may lead to skipping better actions.
Fig. 8 shows the impact of learning rate on the performance
of the proposed algorithm. We can see, when learning rate is
0.001, the system cost of the proposed algorithm converges to
the lowest value. Thus, 0.001 is the best learning rate for the
proposed algorithm.

VI. CONCLUSIONS

In this paper, we proposed a DTN architecture for IIoT,
which utilizes digital twin to construct the network topology
and stochastic task arrival model in IIoT networks. Then, we
formulated the stochastic computation ofﬂoading and resource

[19] Y. Li, L. Lei, and M. Yan, “Mobile user location prediction based on
IEEE, 2019, pp.

user classiﬁcation and markov model,” in IJCIME.
440–444.

[20] W. C. Ao and K. Psounis, “Approximation algorithms for online user
association in multi-tier multi-cell mobile networks,” IEEE/ACM Trans.
Netw., vol. 25, no. 4, pp. 2361–2374, 2017.

[21] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep
reinforcement learning,” in ICML, 2016, pp. 1928–1937.

10

