1
2
0
2

t
c
O
5
2

]
E
S
.
s
c
[

2
v
5
0
1
4
0
.
8
0
1
2
:
v
i
X
r
a

Towards better data discovery and collection with
ﬂow-based programming

Andrei Paleyes
Department of Computer Science
University of Cambridge
ap2169@cam.ac.uk

Christian Cabrera
Department of Computer Science
University of Cambridge
chc79@cam.ac.uk

Neil D. Lawrence
Department of Computer Science
University of Cambridge
ndl21@cam.ac.uk

Abstract

Despite huge successes reported by the ﬁeld of machine learning, such as voice
assistants or self-driving cars, businesses still observe very high failure rate when
it comes to deployment of ML in production. We argue that part of the reason
is infrastructure that was not designed for data-oriented activities. This paper
explores the potential of ﬂow-based programming (FBP) for simplifying data
discovery and collection in software systems. We compare FBP with the currently
prevalent service-oriented paradigm to assess characteristics of each paradigm
in the context of ML deployment. We develop a data processing application,
formulate a subsequent ML deployment task, and measure the impact of the task
implementation within both programming paradigms. Our main conclusion is that
FBP shows great potential for providing data-centric infrastructural beneﬁts for
deployment of ML. Additionally, we provide an insight into the current trend that
prioritizes model development over data quality management.

1

Introduction

After achieving considerable success as an academic discipline, machine learning (ML) components
are now increasingly deployed in production systems. Government agencies, private companies and
individuals all apply ML to solve practical tasks. McKinsey has reported a 25% year to year growth
of ML adoption by businesses, with nearly half of respondents reporting revenue increase [Cam et al.,
2019].

Deployment of machine learning typically happens on top of the existing data processing infras-
tructure. Companies aim to speed up their processing workﬂows, gain additional insights to aid
their decision making, improve detection of anomalous behavior, or provide customers with new
functionality based on historical data. Machine learning models are often a centerpiece of such
projects. Unfortunately, ML deployment projects face difﬁcult challenges, with companies reporting
up to 50% failure rate [Wiggers, 2019]. We believe that one of the reasons for these failures lies in the
fact that the majority of ML projects are being deployed on top of existing software solutions which
were built to fulﬁll goals that are important but unrelated to ML, such as high availability, robustness,
low latency. However, machine learning poses a new set of challenges that the majority of existing
software architectures are not designed for [Paleyes et al., 2020]. Data processing is mentioned as one
of the areas that cause most concern [Polyzotis et al., 2018], especially in high-scale service-oriented
software environments, such as Twitter [Lin and Ryaboy, 2013] or Conﬂuent [Stopford, 2016].

Preprint. Under review.

 
 
 
 
 
 
As more businesses seek to convert the data they manage into value, it seems reasonable to explore
software architectures that could better ﬁt that purpose. In this work we consider the potential of
ﬂow-based programming (FBP, Morrison [1994]) as a paradigm for building business applications
with ML, and compare it with currently prevalent control-ﬂow paradigms, namely service-oriented
architecture (SOA, Perrey and Lycett [2003], O’Reilly [2020]).

There have already been some attempts to enhance SOA with better data handling capabilities [Götz
et al., 2018, Gluck, 2020, Saﬁna et al., 2016, Dehghani, 2019]. In our work we wanted to explore
radically different approach towards better data management in software systems. So, rather than
incrementally improving SOA, we consider FBP due to a range of useful properties that are particular
to the paradigm, e.g. access to a dataﬂow graph and data coupling between the components. We
anticipate that they make data-related tasks, such as data discovery and collection, simpler to perform.
We illustrate this idea with a simple experiment. We develop an example application separately with
each paradigm. We then carry out an ML deployment procedure within both implementations, and
analyze how each deployment stage affects the complexity of the codebase. Our conclusions show
that while there are a number of trade-offs to consider, FBP has potential to simplify deployment of
ML in data-driven applications.

Data ﬂow paradigms are not new in software engineering, a duality of control ﬂow and data ﬂow
for building software has long been explored by the computer science community [Treleaven, 1982,
Lauer and Needham, 1979, Hasselbring et al., 2021]. FBP’s potential to improve software quality
and maintenance has been shown in comparison with other paradigms and design principles, such as
OOP [Morrison, 2010], functional programming Roosta [2000], and SOA in IoT context [Lobunets
and Krylovskiy, 2014]. But to our knowledge FBP has never been compared to SOA in the context of
ML deployment before. Some of the high level ideas that motivated this paper were ﬁrst introduced
by Diethe et al. [2019] and further developed by Lawrence [2019] and Borchert [2020] under the
name of Data Oriented Architectures (DOA). Our work can be seen as the ﬁrst step towards applying
DOA to practical tasks.

2 Experiment setup

To explore the different software paradigms we developed an example application to study properties
of FBP and compare it against the more widespread SOA approach. Concretely, we implemented a
prototype of a taxi ride allocation system described by Lawrence [2019]. The application receives
data about currently available drivers and incoming ride requests, and outputs the allocated rides.
The application also processes updates of each allocated ride, and keeps track of factual passenger
wait times. We formulated a business problem that can be solved with ML: provide user with an
estimated wait time, in addition to the allocated driver. Training data for the ML model can be
collected based on historical wait times. This type of additional functionality has been shown to be
among the major contributors to project’s technical debt [Molnar and Motogna, 2020]. As a result we
focus our evaluation on changes in code quality.

Two separate implementations of the described application were created: one with FBP using
ﬂowpipe1 and one with SOA using Flask2. Detailed description of the application and implementation
details speciﬁc to each paradigm can be found in Appendix B. Full source code of the application can
be found at https://github.com/mlatcl/fbp-vs-soa/tree/ride-allocation.

In order to enable structured approach towards evaluation of codebase changes over the course of ML
deployment, we deﬁned three stages of the implementation:

• Stage 1: minimal code to provide basic functionality. The stage is denoted in the code and

this paper by sufﬁx min.

• Stage 2: same as Stage 1 plus dataset collection. A complete dataset required collecting data
from two locations within the application. Inputs, which we considered to be ride requests
and driver locations, are available at the time ride allocation is done. Output, which is the
actual waiting time, becomes available later in different part of the app, when passenger
pickup happens. Denoted by sufﬁx data.

1ﬂowpipe is available at https://github.com/PaulSchweizer/ﬂowpipe. It is considered to be an FBP-inspired

framework, but provides all FBP features critical for our work and is easy to read and understand.

2https://ﬂask.palletsprojects.com/

2

Table 1: List of all created versions of the Ride Allocation app. First column gives the key by which
a particular version is referred to in the codebase. Explanation of each metric used can be found in
Appendix C.

Key
f b_app_min
f b_app_data
f b_app_ml
soa_app_min
soa_app_data
soa_app_ml

Paradigm Stage Description
FBP
FBP
FBP
SOA
SOA
SOA

Basic functionality
Same as f b_app_min plus dataset collection
Same as f b_app_data plus estimated wait time output
Basic functionality
Same as soa_app_min plus dataset collection
Same as soa_app_data plus estimated wait time output

1
2
3
1
2
3

• Stage 3: same as Stage 2 plus the new output of estimated wait time produced via a deployed
ML model. The ML model is trained on the dataset collected at the previous stage. The
application has to load an already trained ofﬂine and serialized ML model, perform the
prediction at the time ride allocation is done, and output estimated wait time in addition to
the allocation information. Denoted by sufﬁx ml.

Overall we ended up with 6 versions of the Ride Allocation system, which are listed in Table 1.

We use a number of software metrics to assess the impact of each subsequent stage on the overall
quality of the codebase. These metrics measure size, complexity and maintainability of the code.
This metric-based evaluation approach was chosen to enable objective evaluation of codebase quality
and how the ML deployment process affected it at each stage.

3 Experiment Results

In this section we discuss our observations from the experiment, summing up our observations from
each stage. Detailed discussion of the evaluation, as well as our subjective impressions of those
development aspects that were harder to measure, can be found in Appendix C.

Size and complexity metrics and our own observations suggest that initial cost of developing the
FBP solutions is higher. That is likely the consequence of the fact that SOA is a highly evolved
and widely deployed programming paradigm. Therefore the majority of people with experience
of modern industrial software development, which authors of this work consider themselves to
be, can iterate and make progress within this paradigm at a quicker pace. In contrast, FBP is not
nearly as widespread. This paradigm requires a conceptual shift in the way a developer thinks about
the application, because instead of customary control-ﬂow one needs to adopt data-ﬂow mindset.
However, cognitive complexity metric suggest that FBP programs are easier to read and comprehend
once they are written (see Figure 8 in the Appendix C).

Dataset collection stage turned out to be the most critical for surfacing differences between the
paradigms in the ML deployment context. FBP programs allow programmatic access to the whole
dataﬂow graph, with nodes representing business logic of the application, and edges representing
ﬂows of data, meaning that it is possible to programmatically access data ﬂowing to or from any
node. Consequently, changes were made in single location of the codebase, even though we needed
to collect data from multiple internal sources. In contrast, changes to the SOA application had to be
introduced in multiple places, which poorly affects code intepretability and introduces challenges for
long term support.

Unlike the previous two stages, the model hosting stage yielded no additional insight into difference
between the paradigms considered. Nevertheless it is important to see the conﬁrmation of the fact
that both paradigms can support hosting ML model for predictions without signiﬁcant impact on the
rest of the system.

Comparing behavior of multiple code complexity metrics we have realized that the data collection
stage was far more impactful change than the model deployment (Figure 1). This might uncover
additional reason for the trend in modern ML community to focus on model research rather then data
research [Lawrence, 2017]. If making changes to deployed model is easier and less error-prone than
making changes to data engineering pipeline, it is easy to understand why developers and researches

3

are motivated to seek improvements in model iterations rather than over data quality. Nevertheless
we believe data management is equally important part of machine learning process, especially since
data scientists spend most of their time working with data [Nazabal et al., 2020]. With this work we
aim to make a step towards simplifying data-oriented tasks in software systems.

(a) Code size metrics combined. The
trends intersect between stages min and
data, while being nearly parallel
between stages data and ml. This
suggests that data collection on data
stage had different scale of impact on
metrics for FBP and SOA
implementations.

(b) Code complexity metrics combined.
The distance between points that
represent stages min and data is
signiﬁcantly bigger than between points
for stages data and ml. This shows how
big was an impact on code complexity
by the implementation of data
collection.

Figure 1: Combinations of multiple code metrics. All values are normalized to fall within [0, 1] range.
Text next to a point is the app version key this point corresponds to.

4 Conclusions and Future Work

In this paper we illustrated the potential of using FBP to ease the pain of deploying ML and improve
data management. In a software system designed according to FBP principles the tasks of data
discovery and collection become more straightforward, thus simplifying consequent deployment of
ML. We believe better tooling that allows developers to deﬁne dataﬂow graphs at a higher level of
abstraction would help ﬁll some of the current gaps and leverage that potential.

Additionally, we showed that data collection code caused much more signiﬁcant impact to metrics of
both codebases, compared to model deployment. This could be an explanation of modern trends of
seeking performance improvements through models rather than through data.

We observed that when developing an application with FBP paradigm, a lot of effort is spent in
deﬁning and manipulating the dataﬂow graph. On the other hand, once such a graph is deﬁned, all
data ﬂows in the system become explicit, thus making data discovery task simpler. Any framework
that allows software developers to abstract away from the boilerplate code and focus on actual
application domain, business logic and entities, would streamline the development process and reduce
the complexity of the codebase. There are tools that might serve this purpose, such as Google
DataﬂowKrishnan and Gonzalez [2015], KubeﬂowBisong [2019] and Apache NiFi3, although they
are usually seen as very speciﬁc to particular applications. Understanding commonalities of these
frameworks is a promising starting point for building general purpose development tools.

In the future we would like to further scale the experiment described in this paper. For instance,
the same ML deployment perspective can be considered in the distributed context, where data
streaming platforms such as Apache Kafka would have to be used. Long-term experiments can also
be informative to observe the code evolution over a longer period, e.g. a year. Other paradigms, such
as the Actor model [Hewitt, 2010], might be considered for comparison.

3https://niﬁ.apache.org/

4

0.00.20.40.60.81.0FBP-based0.00.20.40.60.81.0SOA-basedmindatamlmindatamlLogical Lines of CodeHalstead Volume0.00.20.40.60.81.0FBP-based0.00.20.40.60.81.0SOA-basedmindatamlmindatamlmindatamlHalstead DifficultyCyclomatic ComplexityCognitive ComplexityAcknowledgments and Disclosure of Funding

We would like to thank our colleagues Pierre Thodoroff, Markus Kaiser, Eric Meissner, Jessica
Montgomery, Diana Robinson for many insightful discussions.

References

Arif Cam, Michael Chui, and Bryce Hall. Global AI survey: AI proves its worth, but few scale

impact. McKinsey Analytics, 2019.

Kyle Wiggers.

IDC: For 1 in 4 companies, half of all AI projects fail, 2019. Available
at https://venturebeat.com/2019/07/08/idc-for-1-in-4-companies-half-of-all-
ai-projects-fail/.

Andrei Paleyes, Raoul-Gabriel Urma, and Neil D. Lawrence. Challenges in deploying machine

learning: a survey of case studies. arXiv preprint arXiv:2011.09926, 2020.

Neoklis Polyzotis, Sudip Roy, Steven Euijong Whang, and Martin Zinkevich. Data lifecycle chal-
lenges in production machine learning: a survey. ACM SIGMOD Record, 47(2):17–28, 2018.

Jimmy Lin and Dmitriy Ryaboy. Scaling big data mining infrastructure: the Twitter experience. ACM

SIGKDD Explorations Newsletter, 14(2):6–19, 2013.

Ben Stopford. The data dichotomy: Rethinking the way we treat data and services, 2016.
Available at https://www.confluent.io/blog/data-dichotomy-rethinking-the-way-
we-treat-data-and-services/.

J. Paul Morrison. Flow-based programming. In Proc. 1st International Workshop on Software

Engineering for Parallel and Distributed Systems, pages 25–29, 1994.

Randall Perrey and Mark Lycett. Service-oriented architecture. In 2003 Symposium on Applications

and the Internet Workshops, 2003. Proceedings., pages 116–119. IEEE, 2003.

O’Reilly. Microservices adoption in 2020: A survey, 2020.

Available at https://

www.oreilly.com/radar/microservices-adoption-in-2020/.

Benjamin Götz, Daniel Schel, Dennis Bauer, Christian Henkel, Peter Einberger, and Thomas Bauern-

hansl. Challenges of production microservices. Procedia CIRP, 67:167–172, 2018.

Adam Gluck. Introducing domain-oriented microservice architecture. Uber Engineering Blog, 2020.

Larisa Saﬁna, Manuel Mazzara, Fabrizio Montesi, and Victor Rivera. Data-driven workﬂows for
microservices: Genericity in jolie. In 2016 IEEE 30th International Conference on Advanced
Information Networking and Applications (AINA), pages 430–437. IEEE, 2016.

Zhamak Dehghani. How to move beyond a monolithic data lake to a distributed data mesh. Martin

Fowler’s Blog, 2019.

Philip C. Treleaven. Towards a decentralised general-purpose computer. In Programmiersprachen

und Programmentwicklung, pages 21–31. Springer, 1982.

Hugh C Lauer and Roger M. Needham. On the duality of operating system structures. ACM SIGOPS

Operating Systems Review, 13(2):3–19, 1979.

Wilhelm Hasselbring, Maik Wojcieszak, and Schahram Dustdar. Control ﬂow versus data ﬂow in
distributed systems integration: Revival of ﬂow-based programming for the industrial internet of
things. IEEE Internet Computing, 25(4):5–12, 2021.

J. Paul Morrison. Flow-Based Programming: A new approach to application development. CreateS-

pace, 2010.

Seyed H. Roosta. Data ﬂow and functional programming. In Parallel Processing and Parallel

Algorithms, pages 411–437. Springer, 2000.

5

O. Lobunets and A. Krylovskiy. Applying ﬂow-based programming methodology to data-driven

applications development for smart environments, 2014.

Tom Diethe, Tom Borchert, Eno Thereska, Borja Balle, and Neil D. Lawrence. Continual learning in

practice. arXiv preprint arXiv:1903.05202, 2019.

Neil D. Lawrence. Modern data oriented programming, 2019.

Available at http://

inverseprobability.com/talks/notes/modern-data-oriented-programming.html.

Tom Borchert. Milan: An evolution of data-oriented programming, 2020. Available at https:

//tborchertblog.wordpress.com/2020/02/13/28/.

Arthur-Jozsef Molnar and Simona Motogna. Long-term evaluation of technical debt in open-source
software. In Proceedings of the 14th ACM/IEEE International Symposium on Empirical Software
Engineering and Measurement (ESEM), pages 1–9, 2020.

Neil D. Lawrence. Data readiness levels. arXiv preprint arXiv:1705.02245, 2017.

Alfredo Nazabal, Christopher K. I. Williams, Giovanni Colavizza, Camila Rangel Smith, and Angus
Williams. Data engineering for data analytics: a classiﬁcation of the issues, and case studies. arXiv
preprint arXiv:2004.12929, 2020.

S. P. T. Krishnan and Jose L. Ugia Gonzalez. Google Cloud Dataﬂow. In Building Your Next Big

Thing with Google Cloud Platform, pages 255–275. Springer, 2015.

Ekaba Bisong. Kubeﬂow and kubeﬂow pipelines. In Building Machine Learning and Deep Learning

Models on Google Cloud Platform, pages 671–685. Springer, 2019.

Carl Hewitt. Actor model of computation: scalable robust information systems. arXiv preprint

arXiv:1008.1459, 2010.

Michael P. Papazoglou and Dimitrios Georgakopoulos. Service-oriented computing. Communications

of the ACM, 46(10):25–28, 2003.

Christian Cabrera, Fan Li, Vivek Nallur, Andrei Palade, MA Razzaque, Gary White, and Siobhán
Clarke. Implementing heterogeneous, autonomous, and resilient services in iot: An experience
report. In A World of Wireless, Mobile and Multimedia Networks (WoWMoM), 2017 IEEE 18th
International Symposium on, pages 1–6. IEEE, 2017.

Tomasz Szydlo, Robert Brzoza-Woch, Joanna Sendorek, Mateusz Windak, and Chris Gniady. Flow-
based programming for IoT leveraging fog computing. In 2017 IEEE 26th International Conference
on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE), pages 74–79.
IEEE, 2017.

Samuel Lampa, Jonathan Alvarsson, and Ola Spjuth. Towards agile large-scale predictive modelling
in drug discovery with ﬂow-based programming design principles. Journal of Cheminformatics, 8
(1):1–12, 2016.

Jesse Zaman, Lode Hoste, and Wolfgang De Meuter. A ﬂow-based programming framework for
mobile app development. In Proceedings of the 3rd International Workshop on Programming for
Mobile and Touch, pages 9–12, 2015.

H. Bergius. NoFlo–ﬂow-based programming for Javascript. URL: http: // noflojs .org , 2015.

Diego Clerissi, Maurizio Leotta, Gianna Reggio, and Filippo Ricca. Towards an approach for
developing and testing node-red iot systems. In Proceedings of the 1st ACM SIGSOFT International
Workshop on Ensemble-Based Software Engineering, pages 1–8, 2018.

Zenon Chaczko and Robin Braun. Learning data engineering: Creating iot apps using the node-red
and the rpi technologies. In 2017 16th International Conference on Information Technology Based
Higher Education and Training (ITHET), pages 1–8. IEEE, 2017.

Dave Mason and Kruti Dave. Block-based versus ﬂow-based programming for naive programmers.

In 2017 IEEE Blocks and Beyond Workshop (B&B), pages 25–28. IEEE, 2017.

6

Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies Liu,
Jeremy Freeman, D. B. Tsai, Manish Amde, Sean Owen, et al. Mllib: Machine learning in Apache
Spark. The Journal of Machine Learning Research, 17(1):1235–1241, 2016.

Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi, and Kostas Tzoumas.
Apache Flink: Stream and batch processing in a single engine. Bulletin of the IEEE Computer
Society Technical Committee on Data Engineering, 36(4), 2015.

Jay Kreps, Neha Narkhede, Jun Rao, et al. Kafka: A distributed messaging system for log processing.

In Proceedings of the NetDB, volume 11, pages 1–7, 2011.

Maurice H. Halstead. Elements of software science. 1977.

Thomas J. McCabe. A complexity measure.

IEEE Transactions on software Engineering, (4):

308–320, 1976.

G. Ann Campbell. Cognitive complexity: An overview and evaluation. In Proceedings of the 2018

international conference on technical debt, pages 57–58, 2018.

Appendix A Background on relevant paradigms

In this section we brieﬂy introduce key concepts and paradigms used throughout the paper.

A.1 Service-oriented architecture

Service-oriented architecture (SOA) is a paradigm for development of software applications Perrey
and Lycett [2003]. Under this paradigm the application is broken down into several components
called services, which interact between each other via a pre-deﬁned protocol to provide application’s
users with necessary functionality. Services only interact on API level, thus hiding details of each
service’s implementation, which results in a set of loosely coupled components. In recent years
SOA and its derivatives, such as microservices, gained substantial popularity and can be considered
de-facto standard distributed software design approach O’Reilly [2020]. Service orientation gives
developers a range of important beneﬁts: incapsulation, loose coupling, modularity, scalability and
ease of integration with other systems, so it is a reasonable choice for those who need to build scalable
software Papazoglou and Georgakopoulos [2003], Cabrera et al. [2017].

A.2 Flow-based programming

Flow-based programming was created by Morrison [1994], and can be considered a special case
of the more general dataﬂow programming paradigm. It deﬁnes software applications as a set of
processes which exchange data via connections that are external to those processes. FBP exhibits
“data coupling”, which is considered in computing to be the loosest form of coupling between
software components, thus promoting a ﬂexible component-oriented software architecture. FBP has
a reputation as a paradigm that optimizes speed, bandwidth and processing power in multi-tasking,
parallel processing applications. For example Szydlo et. al. consider FBP’s application to IoT Szydlo
et al. [2017], Lampa et.al. explored FBP’s potential in the context of drug discovery Lampa et al.
[2016], Zaman et. al. presented an FBP-based framework for mobile development Zaman et al.
[2015]. Recent years saw birth of several general-purpose projects built on ﬂow-based programming
principles, such as NoFlo Bergius [2015] and Node-RED4. Node-RED in particular became popular
in the IoT community Clerissi et al. [2018], Chaczko and Braun [2017], as the FBP model was found
to be a good ﬁt for building data processing pipelines in IoT. Developing this idea further, in this
paper we argue that there is potential in a wider use of FBP.

A notable feature of FBP is the ability to present the whole program visually as a graph of data
ﬂow. This feature has two important implications. First, the graph-like structure allows to reason
about the complete programs in a unique way that is often impossible in case of object oriented
programming (OOP) or SOA. We leverage this aspect in this work. Second, it allows for visual
no-code programming which in some cases may aid adoption of FBP. In particular it is believed to be
useful for beginners who have little or no prior coding experience [Mason and Dave, 2017].

4https://nodered.org/

7

A.3 Data streams

In this work we make use of data streams as connectors in ﬂow-based programs. A data stream is a
sequence of data records that are made available over time. Machine learning on data streams is not a
new concept. Data processing platforms such as Apache Spark [Meng et al., 2016], Apache Flink
[Carbone et al., 2015] or Google Cloud Dataﬂow [Krishnan and Gonzalez, 2015] are widely used for
manipulating large datasets and executing machine learning tasks. AWS Kinesis5 and Apache Kafka
[Kreps et al., 2011] are some of the most commonly used data streaming services.

Appendix B Ride Allocation application

In this section we discuss the gory details of implementing Ride Allocation application in SOA and
FBP paradigms, starting with the general description of the application itself.

B.1 Ride Allocation application

We have chosen Ride Allocation application as described by Lawrence [2019] for our case study. On
one hand, the idea of assigning taxi drivers to incoming ride requests is simple enough to describe and
it does not require much background knowledge. On the other hand, it provides enough non-trivial
functionality and data processing to make it worth studying in the context of ML deployment.

At the highest level our implementation consists of two parts: the allocation system itself and the
code that simulates events happening in the outside world. This overview can be seen in Figure 2.

The allocation system provides several functions. First, it assigns taxi drivers to incoming ride
requests. Second, it keeps track of all allocated rides and updates them according to the incoming
events. We recognize several types of events: ride starts, ride ﬁnishes, location updates, cancellations.
Third, it calculates factual wait times for passengers, deﬁned as a difference between the moment a
ride was allocated and the passenger’s pickup.

The simulation part is implemented as a discrete-event simulation and is responsible for generating
events that would be happening if our system was deployed as a part of a real life taxi application.
In addition to creating initial data like a list of available drivers, it generates new ride requests and
events for rides that were previously allocated.

To ensure fair comparison we have created functionally equivalent systems using FBP and SOA
development paradigms. To further ensure equivalency we have maximized code reuse. In particular,
both implementations share the same data types as well as the simulation code. All data types used in
the system are shown in Figure 3.

Figure 2: Overview of the Ride Allocation system high level mechanics. Simulation part generates
new data, which is handled by Ride Allocation app to allocate new rides or update information about
existing ones. Output of the app is sent back to the simulation. All FBP and SOA implementations of
the Ride Allocation app implement same interface to ensure fair comparison.

5https://aws.amazon.com/kinesis/

8

Figure 3: All data record types deﬁned in the Ride Allocation system. Each table shows the name of
the class and a list of its ﬁelds. Arrows signify "has a" relationship between entities. For simplicity
separate types for driver and user were not deﬁned, and instead just IDs were used to distinguish
these entities. Although such types would be necessary in real software application, in our example
they are not needed for any function.

B.2 Implementation notes: SOA paradigm

Figure 4 shows the software implementation of the allocation app based on the SOA paradigm. We
use the lightweight Flask6 framework to develop the SOA based applications because of its ﬂexibility
and popularity among the micro-services community. We implement a set of services that offer the
required capabilities for each implementation stage. The Ride Allocation App communicates with
the services using REST APIs to save, retrieve and process rides, drivers, and waiting times data. All
services are hosted locally, and the communication is happening via HTTP requests.

Two services were implemented for the Stage 1 as they provide the basic functionalities of the Ride
Allocation application.

Ride Service implements the capabilities to save, retrieve, and process rides information accord-
ing to the simulation events. Three APIs are implemented to store the rides information (i.e.,
add_all_rides, add_ride_events, and add_ride_infos). Four APIs are implemented to retrieve rides
information (i.e., get_ride_infos, get_ride_wait_infos, get_all_rides, and get_rides_by_state). And
the update_ride_allocation API is implemented to update rides’ information once they are allocated.

Driver Service implements the capabilities to save, retrieve and process drivers information. We
implement the
add_all_drivers_statuses and the add_ride_drivers_locations APIs to store drivers information. Three
APIs are implemented to retrieve drivers data (i.e., get_all_drivers, get_driver_data_to_save, and
get_driver_by_id). The allocate_driver and the release_driver APIs are implemented to update
drivers availability once a ride is assigned or ﬁnished.

6Flask Framework - https://palletsprojects.com/p/flask/

9

Figure 4: SOA implementation of the Ride Allocation app. New services deﬁned within each stage
are shown. App code interacts with each service via HTTP requests.

The services above fulﬁll all functionality required by Stage 1 of the experiment. For Stage 2 we imple-
mented an additional service responsible for collecting the dataset. Speciﬁcally, Data Manager Ser-
vice implements the functionality of collecting the data that the learning model uses in the next stage.
These functions include data formatting (i.e., get_ride_data_to_save, and get_wait_times_to_save)
and data storing (i.e., save_data_to_ﬁle). In addition, changes were made to Ride and Driver services
to retrieve necessary data.

One more service was implemented in order to host the trained ML model and do predictions with it,
as required by Stage 3 of the experiment. Learning Service provides the API get_estimated_times,
which predicts user’s waiting times based on the driver’s and user’s locations using the trained model.

Furthermore, we have also implemented data access layer (DAL) that stored and retrieved data as
necessary from the local SQLite database. For the purposes of fair comparison we have excluded
DAL code from further analysis presented in Section C, since FBP implementation described in the
next section does not use a database.

B.3 Implementation notes: FBP paradigm

The FBP implementation of the Ride Allocation system is built with two major building blocks: data
streams and stateless processing nodes. It follows an example implemented in the Milan package7.
For the purposes of this work we use the lightweight Python FBP framework ﬂowpipe8.

A data stream is a collection of data records of the same type that is updated whenever a new record
arrives. Each data stream within the application belongs to one of three categories:

• Input streams, that receive data from the outside world;

• Output streams, that hold data produced by the application;

7https://github.com/amzn/milan/tree/master/milan/milan-samples/milan-sample-gen/

src/main/scala/com/amazon/milan/samples/bodaboda
8https://github.com/PaulSchweizer/flowpipe

10

Ride Allocation AppFlask FrameworkRide ServiceDriver ServiceData Manager ServiceLearningServiceREST APIREST APIREST APIREST APISTAGE 1STAGE 2STAGE 3• Internal streams, that hold intermediate data within the application. These streams are

necessary because processing nodes by deﬁnition are not allowed to have state.

A processing node takes one or more streams as an input, performs some operations on them, and
then puts the result into one or more output or internal data streams. Within the design approach
we are considering such nodes do not carry internal state, and do not make external calls to outside
services or databases. All data inﬂuencing a processing node should be registered in the system, so if
such additional input is necessary, it should be represented as a data stream.

As can be seen in Figure 5, our system deﬁnes four granular operations, some of which produce data
that is only observed within the application itself, such as joined information about a driver. This data
is put on a separate stream before being consumed by the AllocateRide operation.

Figure 5: Flow diagram of the Ride Allocation system implemented via ﬂow-based programming
paradigm. Here rectangles show data streams, circles show processing nodes, arrows show ﬂow of
data records. Left-most streams (highlighted red) represent input data, right-most streams (green)
represent system outputs, and other streams (yellow) are internal to the system.

Implementation of the dataset collection task in Stage 2 exploits the graph-like nature of FBP
programs. In order to gain access to necessary data records, we traversed the dataﬂow graph and
intercept data in corresponding nodes.

New output required by Stage 3 was implemented as one more output stream preceded by a new
processing node. The processing node EstimateRideWaitTime consumed RideAllocationStream as
well as DriverInformationStream, loaded trained ML model to do necessary inference, and produced
EstimatedRideWaitTime output stream.

Appendix C Evaluation

We employed two approaches towards evaluating pros and cons of each of the programming paradigms
used in our experiment: metric-based and empirical comparison.

There are many possible aspects to consider when comparing software engineering approaches. Even
though it is possible to draw some general conclusions about each paradigm from the comparisons
outlined below, within the scope of this paper we only limited ourselves to the task of comparing FBP
and SOA in the context of data management and ML. Therefore in this section we mostly consider
visible impact on the overall codebase caused by implementation of tasks required by Stages 2 and 3
of the project.

11

C.1 Metric-based comparison

We use a number of software evaluation metrics in order to measure the impact of each subsequent
stage on the overall quality of the codebase. Here is the complete list of metrics we used, with brief
discussion of why we felt it is relevant to our experiment. We used the Python package Radon9 and
the Flake8 plugin10 to carry out measurements.

• Logical Lines of Code, which only counts executable statements and ignores comments

and blank lines, as a simple ﬁrst measure of the codebase size.

• Halstead Volume Halstead [1977], which deﬁnes code volume via the number of operations

and operands. This is used as a more advanced measure of the codebase size.

• Cyclomatic Complexity McCabe [1976], which measures the number of independent paths
through the code. We use it to assess how the complexity of the codebase grows with added
functionality. Since cyclomatic complexity is measured separately for each code block, such
as a function, we consider average cyclomatic complexity across the application.

• Cognitive complexity Campbell [2018], which measures the complexity of the control ﬂow
of code. We use it to assess how easy it is to understand the code. As with cyclomatic
complexity, we consider average cognitive complexity across the application.

• Halstead Difﬁculty Halstead [1977], which measures how difﬁcult it is to read the code.
Again, this Halstead metric is formulated via the number operations and operands in code.
This metric is used for a different perspective on code understanding.

• Maintainability Index, as deﬁned in the Radon package11, is a composite metric that is
calculated using a number of other metrics as operands. We use it to assess how maintainable
is the codebase.

Figure 6: Metrics to compare sizes of different implementations of the Ride Allocation app. Higher
value corresponds to bigger codebase.

Figure 6 presents the size comparison of all implementations. Despite being developed with different
paradigms and different tools, the code of each implementation turned out to be of comparable size,
both in logical lines of code and in terms of operations/operands. Across all stages in both paradigms
the SOA implementation turned out to be marginally bigger. While this fact does not inform the
comparison on its own, it simpliﬁes further reasoning. If we had observed signiﬁcant, say orders of
magnitude, difference in size of the codebases, it would have complicated fair comparison of other
metrics that scale with size, such as Halstead Difﬁculty.

The cyclomatic complexity metric comparison is presented on Figure 7. It is clear that graph traversing
logic that was required to collect the dataset in FBP-driven application has noticeable impact on the
computational complexity of the solution. No other big increases are visible, indicating that no other
changes either in FBP or in SOA codebase required complex logic to implement.

9https://github.com/rubik/radon
10https://github.com/Melevir/cognitive_complexity
11https://radon.readthedocs.io/en/latest/intro.html#maintainability-index

12

MinDataMLApplication Stage275300325350375400425450Logical Lines of CodeApplication TypeFBP-basedSOA-basedMinDataMLApplication Stage7008009001000Halstead VolumeApplication TypeFBP-basedSOA-basedFigure 7: Comparison of average cyclomatic complexity of all implementations of the Ride Allocation
app. Higher value means higher complexity.

Figure 8 presents two metrics that address understanding of the code by humans. It is interesting
to observe that Halstead Difﬁculty and Cognitive Complexity metrics seemingly contradict: while
the former shows SOA is a much simpler codebase, the latter clearly favors FBP. We believe this
contradiction reﬂects key charasteristics of each development approach. FBP requires a lot of
operations on edges and vertices in order to build and manipulate the data ﬂow graph. That explains
its worse score in Halstead Difﬁculty, which is calculated with operations and operands. However
deﬁning the graph is a linear process, and once the graph is deﬁned control ﬂow on it becomes trivial,
thus FPB’s lower score on Cognitive Complexity. On the contrary, SOA-based approaches require
relatively fewer operations to setup, partially due to a richer software framework underlying them.
However, the control ﬂow of sending requests and handling responses (on top of the boilerplate already
provided by Flask) isn’t trivial, hence worse performance in terms of the Cognitive Complexity.

Figure 8: Metrics to compare how difﬁcult it is to understand and follow the code of different
implementations of the Ride Allocation app. Average value of the cognitive complexity is used. For
both metrics higher value means code is harder to comprehend.

Nevertheless the Maintainability Index, as shown on the Figure 9, is unaffected by aforementioned
contradiction, and declines at approximately same rate for both paradigms.

Lastly, we combine multiple metrics together, to gain an additional insight. Figure 1 combines
size-related metrics on one plot, and complexity-related metrics on the other. Interestingly, these
plots make it very clear how data collection stage critically impacts all metrics of the codebase. For
size metrics, the trends intersect between Stage 1 and Stage 2, while being nearly parallel between

13

MinDataMLApplication Stage2.152.202.252.302.352.402.45Cyclomatic ComplexityApplication TypeFBP-basedSOA-basedMinDataMLApplication Stage4.04.55.05.56.06.57.0Halstead DifficultyApplication TypeFBP-basedSOA-basedMinDataMLApplication Stage0.81.01.21.41.61.82.0Cognitive ComplexityApplication TypeFBP-basedSOA-basedFigure 9: Comparison of Maintainability Index metric of all implementations of the Ride Allocation
app. Higher value means better maintainability.

Stage 2 and Stage 3. This suggests that data collection on stage 2 had different scale of impact on
different metrics for FBP and SOA implementations. For complexity metrics, the distance between
points that represent Stage 1 and Stage 2 is signiﬁcantly bigger than between points for Stage 2 and
Stage 3. This again clearly shows how big was an impact on code complexity by the implementation
of data collection.

C.2 Empirical comparison

Here we share more subjective impressions acquired in process of implementing the code for the
experiment in both paradigms, that were hard to measure with objective metrics.

In Stage 1 the SOA application turned out quicker and more straightforward to develop than its FBP
counterpart. SOA solution could be built with higher level tools, while FBP required some low level
piping and additional abstractions that were not available out of the box. SOA paradigm is also more
common, while FBP required a different way of thinking about the application logic. Overall, for
someone who has experience with developing software services, the cost of developing Stage 1 code
was lower for the SOA-based approach.

As expected, both approaches did not provide the dataset out of the box, and changes were necessary
for dataset collection task in Stage 2. However the code needed for FBP solution was more localized.
Essentially the whole data collection operation could be implemented at a single location. On the
contrary, SOA implementation required changes in three distinct services in order to collect the
complete dataset, which is more intrusive and prone to errors, especially if the code is ever anticipated
to be in long term maintenance mode.

Both approaches performed well for deploying ML model in Stage 3: FBP exposed model predictions
via separate processing node followed by an output stream, SOA encapsulated the model within a
separate service with a simple API. We believe this has shown that both paradigms are well equipped
for the usage of trained ML models.

14

MinDataMLApplication Stage262830323436Maintainability IndexApplication TypeFBP-basedSOA-based