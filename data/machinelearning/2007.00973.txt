1
2
0
2

b
e
F
7
1

]

G
L
.
s
c
[

2
v
3
7
9
0
0
.
7
0
0
2
:
v
i
X
r
a

Learning to search efﬁciently for causally
near-optimal treatments

Samuel H˚akansson∗
University of Gothenburg
samuel.hakansson@gu.se

Viktor Lindblom
Chalmers University of Technology
viklindb@student.chalmers.se

Omer Gottesman†
Brown University
omer gottesman@brown.edu

Fredrik D. Johansson
Chalmers University of Technology
fredrik.johansson@chalmers.se

Abstract

Finding an effective medical treatment often requires a search by trial and error.
Making this search more efﬁcient by minimizing the number of unnecessary trials
could lower both costs and patient suffering. We formalize this problem as learning
a policy for ﬁnding a near-optimal treatment in a minimum number of trials using
a causal inference framework. We give a model-based dynamic programming
algorithm which learns from observational data while being robust to unmeasured
confounding. To reduce time complexity, we suggest a greedy algorithm which
bounds the near-optimality constraint. The methods are evaluated on synthetic and
real-world healthcare data and compared to model-free reinforcement learning. We
ﬁnd that our methods compare favorably to the model-free baseline while offering
a more transparent trade-off between search time and treatment efﬁcacy.

1

Introduction

Finding a good treatment for a patient often involves trying out different options before a satisfactory
one is found (Murphy et al., 2007). If the ﬁrst-line drug is ineffective or has severe side-effects,
guidelines may suggest it is replaced by or combined with another drug (Singh et al., 2016). These
steps are repeated until an effective combination of drugs is found or all options are exhausted, a
process which may span several years (NCCMH, 2010). A long search adds to patient suffering and
postpones potential relief. It is therefore critical that this process is made as time-efﬁcient as possible.

We formalize the search for effective treatments as a policy optimization problem in an unknown
decision process with ﬁnite horizon (Garcia and Ndiaye, 1998). This has applications also outside of
medicine: For example, in recommendation systems, we may sequentially propose new products or
services to users with the hope of ﬁnding one that the user is interested in. Our goal is to perform as
few trials as possible until the probability that there are untried actions which are signiﬁcantly better
is small—i.e., a near-optimal action has been found with high probability. Historical observations
allow us to transfer knowledge and perform this search more efﬁciently for new subjects. As more
actions are tried and their outcomes observed, our certainty about the lack of better alternatives
increases. Importantly, even a failed trial may provide information that can guide the search policy.

In this work, we restrict our attention to actions whose outcomes are stationary in time. This implies
both that repeated trials of the same action have the same outcome and that past actions do not
causally impact the outcome of future actions. The stationarity assumption is justiﬁed, for example,

∗This work was completed while the author was afﬁliated with Chalmers University of Technology.
†This work was completed while the author was afﬁliated with Harvard University.

 
 
 
 
 
 
for medical conditions where treatments manage symptoms but do not alter the disease state itself, or
where the impact of sequential treatments is known to be additive. In such settings, past actions and
outcomes may help predict the outcomes of future actions without having a causal effect on them.

We formalize learning to search efﬁciently for causally effective treatments as off-policy optimization
of a policy which ﬁnds a near-optimal action for new contexts after as few trials as possible. Our set-
ting differs from those typical of reinforcement or bandit learning (Sutton et al., 1998): (i) Solving the
problem relies on transfer of knowledge from observational data. (ii) The stopping (near-optimality)
criterion depends on a model of unobserved quantities. (iii) The number of trials in a single sequence
is bounded by the number of available actions. We address identiﬁcation of an optimal policy using a
causal framework, accounting for potential confounding. We give a dynamic programming algorithm
which learns policies that satisfy a transparent constraint on near-optimality for a given level of
conﬁdence, and a greedy approximation which satisﬁes a bound on this constraint. We show that
greedy policies are sub-optimal in general, but that there are settings where they return policies with
informative guarantees. In experiments, including an application derived from antibiotic resistance
tests, our algorithms successfully learn efﬁcient search policies and perform favorably to baselines.

2 Related work

Our problem is related to the bandit literature, which studies the search for optimal actions through
trial and error (Lattimore and Szepesv´ari, 2020), and in particular to contextual bandits (Abe et al.,
2003; Chu et al., 2011). In our setting, a very small number of actions is evaluated, with the goal
of terminating search as early as possible. This is closely related to the ﬁxed-conﬁdence variant of
best-arm identiﬁcation (Lattimore and Szepesv´ari, 2020, Chapter 33.2), in which only exploration is
performed. To solve this problem without trying each action at least once, we rely on transferring
knowledge from previous trials. This falls within the scope of transfer and meta learning. Liao et al.
(2020) explicitly tackled pooling knowledge across patients data to determine an optimal treatment
policy in an RL setting and Maes et al. (2012) devised methods for meta-learning of exploration
policies for contextual bandits. A notable difference is that we assume that outcomes of actions are
stationary in time. We leverage this both in model identiﬁcation and policy optimization.

Experiments continue to be the gold standard for evaluating adaptive treatment strategies (Nahum-
Shani et al., 2012). However, these are not always feasible due to ethical or practical constraints.
We approach our problem as causal estimation from observational data (Rosenbaum et al., 2010;
Robins et al., 2000), or equivalently, as off-policy policy optimization and evaluation (Precup, 2000;
Kallus and Santacatterina, 2018). Unlike many works, we do not fully rely on ignorability—that all
confounders are measured and may be adjusted for. Zhang and Bareinboim (2019) recently studied
the non-ignorable setting but allowed for limited online exploration. In this work we aim to bound
the effect of unmeasured confounding rather than to eliminate it using experimental evidence.

Our problem is closely related to active learning (Lewis and Gale, 1994), which has been used to
develop testing policies that minimize the expected number of tests performed before an under-
lying hypothesis is identiﬁed. For a known distribution of hypotheses, ﬁnding an optimal policy
is NP-hard (Chakaravarthy et al., 2007), but there exists greedy algorithms with approximation
guarantees (Golovin et al., 2010). In our case, (i) the distribution is unknown, and (ii) hypotheses
(outcomes) are only partially observed. Our problem is also related to optimal stopping (Jacka, 1991)
of processes but differs in that the process is controlled by our decision-making agent.

3 Learning to search efﬁciently for causally near-optimal treatments

We consider learning policies π ∈ Π that search over a set of actions A := {1, ..., k} to ﬁnd an action
a ∈ A such that its outcome Y (a) ∈ Y is near-optimal. When such an action is found, the search
should be terminated as early as possible using a special stop action, denoted a = STOP. Throughout,
a high outcome is assumed to be preferred and we often refer to actions as “treatments”. The potential
outcome Y (a) may vary between subjects (contexts) depending on baseline covariates X ∈ X ⊆ Rd
and unobserved factors. When a search starts, all potential outcomes {Y (a) : a ∈ A} are unobserved,
but are successively revealed as more actions are tried, see the illustration in Figure 1. To guide
selection of the next action, we learn from observational data of previous subjects.

2

Figure 1: Illustration of the observed sequence of treatments at = (a1, ..., at) and outcomes yt =
(y1, ..., yt) for a patient, and the problem of estimating the outcome of possible future treatments.

Historical searches are observed through covariates X and a sequence of T action-outcome pairs
(A1, Y1), ..., (AT , YT ). Note the distinction between interventional and observational outcomes;
Ys(a) represents the potential outcome of performing action a at time s (Rubin, 2005). We assume
that y ∈ Y are discrete, although our results may be generalized to the continuous case. Sequences
of s random variables (A1, ..., As) are denoted with a bar and subscript, As ∈ As, and Hs =
(X, As, Y s) ∈ Hs := X × {As × Y s} denotes the history up-to time s, with H0 = (X, ∅, ∅). With
slight abuse of notation, a ∈ h means that a was used in h and (a, y) ∈ h that it had the outcome y.
|h| denotes the number of trials in h. The set of histories of at most k actions is denoted H = ∪k
s=1Hs.
Termination of a sequence is indicated by the sequence length T and may either be the result of
ﬁnding a satisfactory treatment or due to censoring. Hence, the full set of potential outcomes is not
observed for most subjects. Observations are distributed according to p(X, T, AT , Y T ).
We optimize a deterministic policy π which suggests an action a following observed history h,
starting with (x, ∅, ∅), or terminates the sequence. Formally, π ∈ Π ⊆ {H → A ∪ {STOP}}. Taking
the action π(hs) = STOP at a time point s implies that T = s. Let pπ(X, A, Y , T ) be the distribution
in which actions are drawn according to the policy π. For a given slack parameter (cid:15) ≥ 0 and a
conﬁdence parameter δ ≥ 0, we wish to solve the following problem.
[T ]

E

minimize
π∈Π

X,Y ,A,T ∼pπ

(cid:20)

subject to Pr

max
a(cid:54)∈At

Y (a) > max
(a,y)∈h

y + (cid:15)

(cid:12)
(cid:12)
(cid:12) Ht = h, T = t

(cid:21)

≤ δ, ∀t ∈ N, h ∈ Ht

(1)

In (1), the objective equals the expected search length under π and the constraint enforces that
termination occurs only when there is low probability that a better action will be found among the
unused alternatives. Note that if maxy∈Y y is known and is in Y s, the constraint is automatically
satisﬁed at s. To evaluate the constraint, we need a model of unobserved potential outcomes. This is
dealt with in Section 4. We address optimization of (1) for a known model in Section 5.

4 Causal identiﬁcation and estimation of optimality conditions

Our assumed causal model for observed data is illustrated graphically in Figure 2a. Most notably, the
graph deﬁnes the causal structure between actions and outcomes—previous actions A1, ..., As−1 and
outcomes Y1, ..., Ys−1 are assumed to have no direct causal effect on future outcomes Ys, ..., YT . To
allow for correlations between outcomes, we posit the existence of counfounders X (observed) and
U (unobserved) and an unobserved moderator Z. All other variables are assumed exogenous.

To evaluate the near-optimality constraint and solve (1), we must identify the probability

ρ(h) := Pr[max
a(cid:54)∈h

Y (a) > max
(a,y)∈h

y + (cid:15) | H = h] ,

(2)

with the convention that max(a,y)∈h0 y = −∞ if |h0| = 0. Henceforth, let A−h = {a ∈ A : a (cid:54)∈ hs}
denote the set of untried actions at h and let S(A−h) be all permutations of the elements in A−h.
We state assumptions sufﬁcient for identiﬁcation of ρ(h) below. Throughout this work we assume that
consistency, stationarity of outcomes and positivity always hold, and provide identiﬁability results
both when ignorability holds (Section 4.1) and when it is violated (Section 4.2).
Identifying assumptions. Deﬁne As+1:k = (As+1, ..., Ak). Under the observational distribution p,
and evaluation distribution pπ, for all π ∈ Π, h ∈ Hs, and s, r ∈ N, we assume

3

𝑎!,𝑎"𝑦!𝑎#,𝑌𝑎∶𝑎∉(𝑎!𝑦#,𝑦"⋯Observed	outcomesUnobservedPatient	at	baseline⋯𝑋(a) Assumed causal structure. Arrows between boxes
indicate connections between all variables in the
boxes: X is a cause of every As and Ys. Past actions
are assumed not to be direct causes of future outcomes.
Z is a moderator of treatment effects. Dashed outlines
indicate unobserved variables.

(b) Efﬁcacy (fraction of subjects for which optimal
action is found) and search length for varying amounts
of samples, with trade-off parameters δ = 0.4, (cid:15) = 0
(CDP, CG), λ = 0.35 (NDP). The sufﬁx H indicates
historical smoothing and F function approximation.
Error bars indicate standard errors over 71 realizations.

Figure 2: Assumed causal structure (left) and results from synthetic experiments (right).

1. Consistency: Ys = Ys(As)

2. Stationarity: Ys(a) = Yr(a) =: Y (a)

3. Positivity:

∃a ∈ S(A−hs ) : pπ(Hs = hs) > 0 =⇒ p(As+1:k = a | Hs = hs) > 0

4. Ignorability: Ys(a) ⊥⊥ As | Hs−1

Ignorability follows from the backdoor criterion applied to the causal model of Figure 2a when U
is empty (Pearl, 2009). We expand on this setting next. In contrast to conventions typically used in
the literature, positivity is speciﬁed w.r.t. the considered policy class. This ensures that every action
could be observed at some point after every history h that is possible under policies in Π. Under
Assumption 2 (stationarity), there is no need to try the same treatment twice, since the outcome is
already determined by the ﬁrst trial. We can restrict our attention to non-repeating policies,

Π ⊆ {π : H → A ∪ {STOP} ; π(h) (cid:54)∈ h} .

Non-repeating policies such as these take the form of a decision tree of depth at most k = |A|.
Remark 1 (Assumptions 1–4 in practice). Only the positivity assumption may be veriﬁed empirically;
stationarity, consistency and ignorability must be justiﬁed by domain knowledge. Readers experienced
with causal estimation will be familiar with the process of establishing ignorability and consistency
through graphical arguments or reasoning about statistical independences. Stationarity is more
speciﬁc to our setting and without it, the notion of a near-optimal action is not well-deﬁned—the
best action could change with time. This phenomenon occurs is settings where outcomes naturally
increase or decrease over time, irrespective of interventions. For example, the cognitive function
of patients with Alzheimer’s disease tends to decrease steadily over time (Arevalo-Rodriguez et al.,
2015). As a result, measures of cognitive function Yt(a) for patients on a medication a will be
different depending on the stage t of progression that the patient is in. As a rule-of-thumb, stationarity
is better justiﬁed over small time-frames or for more stable conditions.

4.1

Identiﬁcation without unmeasured confounders

Our stopping criterion ρ(h) is an interventional quantity which represents the probability that an
unused action would be preferable to previously tried ones. In general, this is not equal to the rate at
which such an action was preferable in observed data. Nevertheless, we prove that ρ(h) is identiﬁable
from observational data in the case that U does not exist (ignorability holds w.r.t. H). First, the
following lemma shows that the order of history does not inﬂuence the probability of future outcomes.

Lemma 1. Let I be a permutation of (1, ..., s). Under stationarity, for all a ∈ As and b (cid:54)∈ a,

p(Y (b) | X, As = a, Y s = y) = p(Y (b) | X, As = (aI(1), ..., aI(s)), Y s = (yI(1), ..., yI(s))) (3)

4

				=	𝐴$	𝐴%	𝐴&…	𝑌$	𝑌%	𝑌&	𝑍	𝑋Treatments…OutcomesModeratorConfounders	𝑈1.21.41.61.82.0Mean number of trials0.6750.7000.7250.7500.7750.8000.8250.850EfficacyHigh-data regimeLow-data regimeCDP_HCDP_FCG_HCG_FNDP_HNDP_FLemma 1 is proven in Appendix A.1. As a consequence, we may treat two histories with the same
events in different order as equivalent when estimating p(Y (a) | H).

We can now state the following result about identiﬁcation of the near-optimality constraint of (1).

Theorem 1. Under Assumptions 1–4, the stopping criterion ρ(h) in (2) is identiﬁable from the
observational distribution p(X, T, A, Y ). For any time step s with history hs, let h(I)s =
(x, aI(1), ..., aI(s), yI(1), ..., yI(s)) be an arbitrary permutation of hs. Then, for any sequence of
untried actions as+1:k = (as+1, ..., ak) ∈ S(A−hs ) with h(I)r the (hypothetical) continued history
at time r > s corresponding to as+1:k and ys+1:k, and with µ(hs) = max(a,y)∈hs y,

ρ(hs) =

(cid:88)

1 [max(y) > µ(hs) + (cid:15)]

ys+1:k∈Y k−s

k
(cid:89)

r=s+1

p(Yr = yr | Ar = ar, Hr = h(I)r−1) .

(4)

A proof of Theorem 1 is given in Appendix A.2. Equation (4) gives a concrete means to estimate ρ(h)
from observational data by constructing a model of p(Ys | As, Hs−1 = h). Due to Assumption 2
(stationarity), this model can be invariant to permutations of h. Another important consequence of
this result is that, because Theorem 1 holds for any future sequence of actions, (4) holds also over any
convex combination for different future action sequences, such as the expectation over the empirical
distribution. Using likely sequences under the behavior policy will lead to lower-variance estimates.

Remark 2. In the fully discrete case, we may estimate p(Y | A, H) using a probability table, and
we do so in some experiments in Section 6. However, this becomes increasingly difﬁcult for both
statistical and computational reasons when A and Y grow larger or when any of the variables are
continuous. The permutation invariance given by Theorem 1 provides some relief but, nevertheless,
the number of possible combinations (histories) grows exponentially with the number of actions.
As a result, it is very probable that certain pairs of histories and actions (h, a) are never observed
in practical applications. We consider two remedies to this. In Appendix B, we give methods for
leveraging observations of similar histories h(cid:48) ≈ h in the estimation of p(Y | H = h, A), one
based on historical kernel-smoothing in the tabular case, and one based on function approximation.
These are compared empirically in Section 6. In Section 5.2, we give bounds to use in place of the
probability of unobserved potential outcomes which further mitigate the curse of dimensionality.

4.2 Accounting for unobserved confounders

If Assumption 4 (ignorability) does not hold with respect to observed variables, the stopping criterion
ρ(hs) may not be identiﬁed from observational data without further assumptions. A natural relaxation
of ignorability is that the same condition holds w.r.t. an expanded adjustment set (Hs, U ), where
U ∈ U is an unobserved set of variables. This is the case in our assumed causal model, see Figure 2a.
We require additionally that U has bounded inﬂuence on treatment propensity. For all u ∈ U, h ∈ H,
with s = |h| and a ∈ S(A−h), assume that there is a sensitivity parameter, α ≥ 1, such that

1
α

≤

Pr[As+1:k = a | Hs = h]
Pr[As+1:k = a | U = u, Hs = h]

≤ α ,

(5)

where As+1:k is deﬁned as in Assumption 3. Like ignorability, this assumption must be justiﬁed from
external knowledge since U is unobserved. We arrive at the following result.

Theorem 2. Assume that (5) and Assumptions 1–4 hold with respect to (Hs, U ) for all s ∈ [k] with
sensitivity parameter α ≥ 1. Then, for any h ∈ Hs, a ∈ S(A−h) and ν = µ(h) + (cid:15), we have

Pr[

k
max
r=s

Yr > ν | As+1:k = a, Hs = h] ≤

δ
α

=⇒ ρ(h) = Pr[max
a∈a

Y (a) > ν | Hs = h] ≤ δ .

A proof of Theorem 2 is given in Appendix A.4. To achieve near-optimality with conﬁdence level of δ
in the presence of unobserved confounding with propensity inﬂuence α, we must require a conﬁdence
level of at most δ/α. Unlike classical approaches to sensitivity analysis, as well as more recent
results (Kallus and Zhou, 2018), this argument does not rely on importance (propensity) weighting.

5

5 Policy optimization

We give two algorithms for policy optimization under the assumption that a model of the stopping
criterion ρ(h) is known. As noted previously, this problem is NP-hard due to the exponentially
increasing number of possible histories (Rivest, 1987). Nevertheless, for moderate numbers of
actions, we may solve (1) exactly using dynamic programming, as shown next. Then we propose a
greedy approximation algorithm and discuss model-free reinforcement learning as alternatives.

5.1 Exact solutions with dynamic programming

Let X, A, Y be discrete. For sufﬁciently small numbers of actions, we can solve (1) exactly in this
setting. Let h(cid:48) = h ∪ {(a, y)} denote the history where (a, y) follows h and recall the convention
maxa∈∅ Y (a) = −∞. Now deﬁne Q to be the expected cumulative return—see e.g., Sutton et al.
(1998) for an introduction—of taking action a in a state with history h ∈ H,
(cid:88)

Q(h, a) = r(h, a) + 1[a (cid:54)= STOP]

p(Y (a) = y | h)

Q(h ∪ {(a, y)}, a(cid:48)) ,

max
a(cid:48)∈A∪{STOP}

(6)

y∈Y

where r(h, a) is a reward function deﬁned below. The value function V at a history h is deﬁned in
the usual way, V (h) = maxa Q(h, a). To satisfy the near-optimality constraint of (1), we use an
estimate of the function ρ(h), see (2), to deﬁne γ(cid:15),δ,α(h) := 1[ρ(h) < δ/α] for parameters (cid:15), δ ≥ 0,
α ≥ 1. The function γ(cid:15),δ,α(h) represents whether an (cid:15), δ/α-optimum has been found. We deﬁne

r(cid:15),δ,α(h, a) =

(cid:40) −∞,
0,
−1,

if a = STOP, γ(cid:15),δ,α(h) = 0
if a = STOP, γ(cid:15),δ,α(h) = 1
if a (cid:54)= STOP

.

(7)

With this, given a model of p(Ys(a) | Hs−1, As), the Q-function of (6) may be computed using
dynamic programming, analogous to the standard algorithm for discrete-state reinforcement learning.
Theorem 3. Recall that H0 = (X, ∅, ∅). The policy maximizing (6), π(h) = arg maxa Q(h, a),
with reward given by (7) is an optimal solution to (1) with objective Epπ [T ] = EX [−V (H0)].

Theorem 3 follows from Bellman optimality and the deﬁnition of r in (7), see Appendix A.5.

5.2 A greedy approximation algorithm

We propose a greedy policy as an approximate solution to (1) in high-dimensional settings where
exact solutions are infeasible to compute. We then discuss sub-optimality and approximation ratios
of greedy algorithms. First, consider the greedy policy πG, which chooses the treatment with the
highest probability of ﬁnding a best-so-far outcome, weighted by its value, according to

f (h, a) = E[1[Y (a) > max
(·,y)∈h

y]Y (a) | Hs = h]

until the stopping criterion is satisﬁed,

πG(h) :=

(cid:26) STOP,

γ(cid:15),δ,α(h) = 1

arg maxa(cid:54)∈h f (h, a), otherwise

,

(8)

(9)

where γ(cid:15),δ,α is deﬁned as in Section 5.1. While using πG avoids solving the costly dynamic
programming problem of the previous section, it still requires evaluation of γ(h). Even for short
histories, |h| ≈ 1, computing γ(h) involves modeling the distribution of maximum-length sequences
over potentially |Y||A| conﬁgurations. To increase efﬁciency, we bound the stopping statistic ρ, and
approximate γ, using conditional distributions of the potential outcome of single actions.

ρ(h) := p

(cid:18)

max
a(cid:54)∈h

Y (a) > µ(h) + (cid:15) | h

≤

(cid:19)

(cid:88)

a(cid:54)∈h

p (Y (a) > µ(h) + (cid:15) | h) .

(10)

A proof is given in Appendix A.3. Using the upper bound in place of ρ(h) leads to a feasible solution
of (1) with more conservative stopping behavior and better outcomes but worse expected search time.
In the case δ = 0, the exact statistic and the upper bound lead to identical policies. Representing
the upper bound as a function of all possible histories still requires exponential space in the worst
case, but only a small subset of histories will be observed for policies that terminate early. We use
the bound on ρ(h) in experiments with both dynamic programming and greedy policies in Section 6.
The general problem of learning bounds on potential outcomes was studied by (Makar et al., 2020).

6

Example 1. In the following example, the greedy policy does identify a near-optimal action after
the smallest expected number of trials, for δ = 0, (cid:15) = 0. Let X = 0, Y ∈ {0, 1} and A ∈ {1, 2, 3},
Z = {1, ..., 4}, p(Z) = [0.20, 0.15, 0.20, 0.45]T and let C be the matrix with elements cij such that

p(Y (j) = 1 | Z = i) = cij, with C (cid:62) =

(cid:34)1
0
1

0
1
0

1
0
0

(cid:35)

0
1
1

.

In this scenario, p(Y (·) = 1) = [0.4, 0.6, 0.65](cid:62). The greedy strategy would thus start with π(∅) = 3,
followed by π((3)) = 2 and then π((3, 2)) = 1 to guarantee successful treatment. An optimal strategy
is to start with A1 = 2 and then A2 = 1. The expected time E[T ] is 1.5 under the greedy policy and
1.4 under the optimal one. The worst-case time under the greedy strategy is 3 and 2 under the optimal.

In Appendix A.6, we show that our problem is equivalent to a variant of active learning once a model
for p(Y (a1), ..., Y (ak), X) is known. In general, it is NP-hard to obtain an approximation ratio better
than a logarithmic factor of the number of possible combinations of potential outcomes (Golovin et al.,
2010; Chakaravarthy et al., 2007). However, for instances with additional structure, e.g., through
correlations induced by the moderator Z, this ratio may be signiﬁcantly smaller than |A| log |Y|.

5.3 A model-free approach

In off-policy evaluation, it has been noted that for long-term predictions, model-free approaches may
be preferable to, and suffer less bias than, their model-based counterparts (Thomas and Brunskill,
2016). They are therefore natural baselines for solving (1). We construct such a baseline below.

Let max(·,y)∈h y represent the best outcome so far in history h, with s = |h| and let λ > 0 be a
parameter trading off early termination and high outcome. Now, consider a reward function r(h, a)
which assigns a reward at termination equal to the best outcome found so far. A penalty −λ is
awarded for each step of the sequence until termination, a common practice for controlling sequence
length in reinforcement learning, see e.g, (Pardo et al., 2018). Let

rλ(h, a) = {0, if a (cid:54)= STOP ; max
(·,y)∈h

y − λ|h|, if a = STOP} .

(11)

The policy πλ which optimizes this reward, using dynamic programming as in Section 5.1, is used as
a baseline in experiments in Section 6.

While this approach has the advantage of not requiring a model of future outcomes, without a
model, the stopping criterion ρ(h) cannot be veriﬁed and the advantage of being able to specify
an interpretable certainty level is lost. This is because the trade-off parameter λ does not have a
universal interpretation—the value of λ which achieves a given rate of near-optimality will vary
In contrast, the conﬁdence parameter δ directly represents a bound on the
between problems.
probability that there is a better treatment available when stopping. Additionally, in Appendix A.7,
we prove that there are instances of the main problem (1), for a given value of δ, such that no setting
of λ results in an optimal solution.

6 Experiments

We evaluate our proposed methods using synthetic and real-world healthcare data in terms of the
quality of the best action found, and the number of trials in the search.3 In particular, we study the
efﬁcacy of policies, deﬁned as the fraction of subject for which a near-optimal action has been found
when the choice to stop trying treatments is made. Models of potential outcomes are estimated using
either a table with historical smoothing (labeled with sufﬁx H) or using function approximation using
random forests (sufﬁx F), see Appendix B. Following each estimation strategy, we compare policies
learned using constrained dynamic programming (CDP), the constrained greedy approximation (CG)
and the model-free RL variant, referred to as as na¨ıve dynamic programming (NDP), see Section 5.
Establishing near-optimality is infeasible in most observational data as only a subset of actions are
explored. However, as we will see, in our particular application, it may be determined exactly.

3Implementations can be found at: https://github.com/Healthy-AI/TreatmentExploration

7

6.1 Synthetic experiments: Effect of sample size and algorithm choice

To investigate the effects of data set size, number of actions, dimensionality of baseline covariates and
the uncertainty parameter δ on the quality of learned policies, we designed a synthetic data generating
process (DGP). This DGP parameterizes probabilities of actions and outcomes as log-linear functions
of a permutation-invariant vector representation of history and of (X, Z), respectively. For the results
here, A = {1, ..., 5}, X = {0, 1}, Y = {0, 1, 2}, Z = {0, 1}3. Due to space limitations, we give the
full DGP and more results of these experiments in Appendix C.1.

We compare the effect of training set size for the different policy optimization algorithms (CDP, CG,
NDP and model estimation schemes ( F, H). Here, CDP and CG use δ = 0.4, (cid:15) = 0 and the upper
bound of (10) and NDP λ = 0.35. We consider training sets in a low-data regime with 50 samples
and a high-data regime of 75000, with ﬁxed test set size of 3000 samples. Results are averaged over
71 realizations. In Figure 2b, we see that the value of all algorithms converge to comparable points
in the high-data regime but vary signiﬁcantly in the low-data regime. In particular, CG and CDP
improve on both metrics as the training set grows. The time-efﬁcacy trade-off is more sensitive to the
amount of data for NDP than for the other algorithms, and while additional data signiﬁcantly reduces
the mean number of actions taken, this comes at a small expense in terms of efﬁcacy. This highlights
the sensitivity of the na¨ıve RL-based approach to the choice of reward: the scale of the parameter λ
determines a trade-off between the number of trials and efﬁcacy, the nature of which is not known in
advance. In contrast, CDP and CG are preferable in that δ and (cid:15) have explicit meaning irrespective
of the sample and result in a subject-speciﬁc stopping criterion, rather than an average-case one.

6.2 Optimizing search for effective antibiotics

Antibiotics are the standard treatment for bacterial infections. However, infectious organisms can
develop resistance to speciﬁc drugs (Spellberg et al., 2008) and patterns in organism-drug resistance
vary over time (Kanjilal et al., 2018). Therefore, when treating patients, it is important that an
antibiotic is selected to which the organism is susceptible. For conditions like sepsis, it is critical that
an effective antibiotic is found within hours of diagnosis (Dellinger et al., 2013).

As a proof-of-concept, we consider the task of selecting effective antibiotics by analyzing a cohort of
intensive-care-unit (ICU) patients from the MIMIC-III database (Johnson et al., 2016). We simplify
the real-world task by taking effective to mean that the organism is susceptible to the antibiotic.
When treating patients for infections in the ICU, it is common that microbial cultures are tested
for resistance. This presents a rare opportunity for off-policy policy evaluation, as the outcomes of
these tests may be used as the ground truth potential outcomes of treatment (Boominathan et al.,
2020). In practice, the results of these tests are not always available at the time of treatment. For this
reason, we learn models based on the test outcomes only of treatments actually given to patients. To
simplify further, we interpret concurrent treatments as sequential; their outcomes are not conﬂated
here since they are taken from the culture tests. We stress that this task is not meant to accurately
reﬂect clinical practice, but to serve as a benchmark based on a real-world distribution. Although a
patient’s condition may change as a response to treatment, bacteria typically do not develop resistance
during a particular ICU stay, and so the stationarity assumption is valid.

Baseline covariates X of a patient represent their age group (4 groups), whether they had infectious
or skin diseases (2 × 2 groups), and the identity of the organism, e.g., Staphylococcus aureus. These
were found to be important predictors of resistance by Ghosh et al. (2019). In total, X comprised
12 binary indicators. From the full set of microbial events in MIMIC-III, we restricted our study to
a subset of 4 microorganisms and 6 antibiotics, selected based on overall prevalence and the rate
of co-occurrence in the data. There were three distinct ﬁnal outcomes of culture tests, resistant,
intermediate, susceptible, encoded as Y = 0, 1, 2, respectively, where higher is better. The resulting
cohort restricted to patients treated using only the selected antibiotics consisted of n = 1362 patients
which had cultures tested for resistance against all antibiotics. The cohort was split randomly into a
training and test set with a 70/30 ratio and experiments were repeated over ﬁve such splits. Patients
treated for multiple organisms were split into different instances. A full list of variables, the selected
antibiotics and organisms, and additional statistics are given in Appendix C.2.

We compare our learned policies to the policy used to select antibiotics in practice. However, due to
censoring, e.g., from mortality, the sequence length of observed patients may not be representative of
the expected number of trials used by the observed policy before an effective treatment is found. In

8

(a) Mean best-at-termination or best-so-far outcome
found after a given number of trials, across all subjects,
for different policies. δ = 0, λ = 0.35.

(b) Efﬁcacy of antibiotics vs the mean number of trials
for different values of δ and λ (one value per marker)
and different model estimation schemes.

Figure 3: Results from the antibiotics experiment. Average best-found outcome of different policies
across patients at different stages of the search (a) and efﬁcacy and search time (number of treatment
trials) as functions of δ (b). In plot (a), at a given number of trials, the best-so-far outcome is used for
ongoing sequences, and the best-at-termination is used for terminated ones. Efﬁcacy refers to the
rate at which a near-optimal treatment is found at the given δ. Sufﬁxes F and H indicates model
estimation using function approximation and historical smoothing respectively. (cid:15) = 0.

other words, the average outcome for patients who went through t treatments is a biased estimate of
the value of the observed policy. Therefore, for direct comparison with current practice (“Doctor”),
only the mean outcome following the ﬁrst treatment point is displayed (star marker) in Figure 3a. For
an approximate comparison with current practice, as used in multiple treatment trials, we created a
baseline dubbed “Emulated doctor”. It uses a tabular estimate of the observed policy to imitate the
choices made by doctors in the dataset in terms of the history H = (X, A, Y ), i.e., it operates on the
same information as the other algorithms. We compare this to CDP, CG and NDP, and evaluate all
policies using culture tests for held-out observations. We sweep all hyperparameters uniformly over
10 values; for CDP, CG, δ ∈ [0, 1], for NDP H, λ ∈ [0, 0.5] and for NDP F, λ ∈ [0, 1].

In Figure 3a, we see that CG, CDP and NDP, with function approximation, all learn comparable
policies that are preferable to the estimated behavior policy. The mean search length was 1.26 for
CDP and NDP, 1.28 for CG and 1.38 for Emulated doctor. We see that the best treatment found after
a single trial is slightly better in the raw data (star marker). This may be because more information
is available to the physician than to our algorithms. The physician could (1) take into account the
original value of continuous variables, such as age, instead of using age groups and (2) use more
features of the patient in order to ﬁnd the right treatment. Using more covariates in this instance
would make the problem impractical to solve without further approximations since the table generated
by the dynamic programming algorithm grows exponentially. The current variable set was restricted
for this reason. In Figure 3b, we see that across different values of δ, λ, all algorithms achieve
near-optimal efﬁcacy (almost 1), but vary in their search time. CDP is equal or preferable to CG,
with the model-free baseline NDP achieving the worst results. A much more noticeable difference is
that between policies learned using the model estimated with function approximation (sufﬁx F) and
those with a (smoothed) tabular representation (sufﬁx H).

7 Conclusion

We have formalized the problem of learning to search efﬁciently for causally effective treatments.
We have given conditions under which the problem is solvable by learning from observational data,
and proposed algorithms that estimate a causal model and perform policy optimization. Our solution
using constrained dynamic programming (CDP) in an exponentially large state space illustrates the
associated computational difﬁculties and prompted our investigation of two approximations, one
based on greedy search and one on model-free reinforcement learning. We found that the greedy
search algorithm performed comparably to the exact solution in experiments and was less sensitive to
sample size. Determining conditions under which greedy algorithms are preferable statistically is an

9

123456Number of tried treatments1.751.801.851.901.95Mean treatment effectCDP_FCG_FNDP_FEmulated doctorMax outcomeDoctor1.241.261.281.301.321.341.36Mean number of trials0.9880.9900.9920.9940.9960.9981.000EfficacyFunction approximationHistorical smoothingCDP_HCG_HNDP_HCDP_FCG_FNDP_Finteresting open question. We believe that our work will have the largest impact in settings where a)
the assumption of potential outcome stationarity is justiﬁed, b) even a small reduction in search time
is valuable and c) a transparent trade-off between efﬁcacy and search time is valuable in itself.

Broader impact

Personalized and partially automated selection of medical treatments is a long-standing goal for
machine learning and statistics with the potential to improve the lives of patients and reduce the
workload on physicians. This task is not without risk however, as poor decisions may fail to reduce or
even increase suffering. It is important that implementations of such ideas is guided by strong domain
knowledge, thorough evaluation and that checks and balances are in place. Many previous works in
this ﬁeld aim to identify new policies for treatment or doses with the goal of improving treatment
response itself. This goal is not always feasible to achieve—some conditions are fundamentally hard
to treat with available medications and procedures. In contrast, we focus on conditions where a
good enough treatment would be identiﬁed by an existing policy given enough time, with the goal of
reducing this search time as much as possible. The trade-off between a good outcome and time is
made transparent using a model of patient outcomes and a certainty parameter. With this, we hope to
contribute towards making machine learning methods more suitable for clinical implementation.

Funding disclosure

This work was supported in part by the Wallenberg AI, Autonomous Systems and Software Program
(WASP) funded by the Knut and Alice Wallenberg Foundation.

References

Abe, N., A. W. Biermann, and P. M. Long

2003. Reinforcement learning with immediate rewards and linear hypotheses. Algorithmica,
37(4):263–293.

Arevalo-Rodriguez, I., N. Smailagic, M. R. i Figuls, A. Ciapponi, E. Sanchez-Perez, A. Giannakou,

O. L. Pedraza, X. B. Cosp, and S. Cullum
2015. Mini-mental state examination (mmse) for the detection of alzheimer’s disease and other
dementias in people with mild cognitive impairment (mci). Cochrane Database of Systematic
Reviews, 2015(3).

Boominathan, S., M. Oberst, H. Zhou, S. Kanjilal, and D. Sontag

2020. Treatment policy learning in multiobjective settings with fully observed outcomes. arXiv
preprint arXiv:2006.00927.

Chakaravarthy, V. T., V. Pandit, S. Roy, P. Awasthi, and M. Mohania

2007. Decision trees for entity identiﬁcation: Approximation algorithms and hardness results.
In Proceedings of the twenty-sixth ACM SIGMOD-SIGACT-SIGART symposium on Principles of
database systems, Pp. 53–62.

Chu, W., L. Li, L. Reyzin, and R. Schapire

2011. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth Interna-
tional Conference on Artiﬁcial Intelligence and Statistics, Pp. 208–214.

Dellinger, R. P., M. M. Levy, A. Rhodes, D. Annane, H. Gerlach, S. M. Opal, J. E. Sevransky, C. L.

Sprung, I. S. Douglas, R. Jaeschke, et al.
2013. Surviving sepsis campaign: international guidelines for management of severe sepsis and
septic shock, 2012. Intensive care medicine, 39(2):165–228.

Garcia, F. and S. M. Ndiaye

1998. A learning rate analysis of reinforcement learning algorithms in ﬁnite-horizon. In Proceed-
ings of the 15th International Conference on Machine Learning (ML-98. Citeseer.

Ghosh, D., S. Sharma, E. Hasan, S. Ashraf, V. Singh, D. Tewari, S. Singh, M. Kapoor, and D. Sengupta
2019. Machine learning based prediction of antibiotic sensitivity in patients with critical illness.
medRxiv, P. 19007153.

10

Golovin, D., A. Krause, and D. Ray

2010. Near-optimal bayesian active learning with noisy observations. In Advances in Neural
Information Processing Systems, Pp. 766–774.

Guillory, A. and J. Bilmes

2009. Average-case active learning with costs. In International conference on algorithmic learning
theory, Pp. 141–155. Springer.

Jacka, S. .

1991. Optimal stopping and the american put. Mathematical Finance, 1(2):1–14.

Johnson, A. E., T. J. Pollard, L. Shen, H. L. Li-wei, M. Feng, M. Ghassemi, B. Moody, P. Szolovits,

L. A. Celi, and R. G. Mark
2016. Mimic-iii, a freely accessible critical care database. Scientiﬁc data, 3:160035.

Kallus, N. and M. Santacatterina

2018. Optimal balancing of time-dependent confounders for marginal structural models. arXiv
preprint arXiv:1806.01083.

Kallus, N. and A. Zhou

2018. Confounding-robust policy improvement. In Advances in Neural Information Processing
Systems 31, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
eds., Pp. 9269–9279. Curran Associates, Inc.

Kanjilal, S., M. R. A. Sater, M. Thayer, G. K. Lagoudas, S. Kim, P. C. Blainey, and Y. H. Grad

2018. Trends in antibiotic susceptibility in staphylococcus aureus in boston, massachusetts, from
2000 to 2014. Journal of clinical microbiology, 56(1):e01160–17.

Kosaraju, S. R., T. M. Przytycka, and R. Borgstrom

1999. On an optimal split tree problem. In Workshop on Algorithms and Data Structures, Pp. 157–
168. Springer.

Lattimore, T. and C. Szepesv´ari

2020. Bandit algorithms. Cambridge University Press.

Lewis, D. D. and W. A. Gale

1994. A sequential algorithm for training text classiﬁers. In SIGIR’94, Pp. 3–12. Springer.

Liao, P., K. Greenewald, P. Klasnja, and S. Murphy

2020. Personalized heartsteps: A reinforcement learning algorithm for optimizing physical activity.
Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(1):1–22.

Maes, F., L. Wehenkel, and D. Ernst

2012. Meta-learning of exploration/exploitation strategies: The multi-armed bandit case. In
International Conference on Agents and Artiﬁcial Intelligence, Pp. 100–115. Springer.

Makar, M., F. D. Johansson, J. Guttag, and D. Sontag

2020. Estimation of utility-maximizing bounds on potential outcomes. In International Conference
on Machine Learning. PMLR.

Murphy, S. A., L. M. Collins, and A. J. Rush

2007. Customizing treatment to the patient: Adaptive treatment strategies. Drug and alcohol
dependence, 88(Suppl 2):S1.

Nahum-Shani, I., M. Qian, D. Almirall, W. E. Pelham, B. Gnagy, G. A. Fabiano, J. G. Waxmonsky,

J. Yu, and S. A. Murphy
2012. Experimental design and primary data analysis methods for comparing adaptive interventions.
Psychological methods, 17(4):457.

NCCMH

2010. National Collaborating Centre for Mental Health (UK). Depression: the treatment and
management of depression in adults (updated edition).

11

Pardo, F., A. Tavakoli, V. Levdik, and P. Kormushev

2018. Time limits in reinforcement learning. In International Conference on Machine Learning,
Pp. 4045–4054.

Pearl, J.

2009. Causality. Cambridge university press.

Precup, D.

2000. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty
Publication Series, P. 80.

Rivest, R. L.

1987. Learning decision lists. Machine learning, 2(3):229–246.

Robins, J. M., M. A. Hernan, and B. Brumback

2000. Marginal structural models and causal inference in epidemiology.

Rosenbaum, P. R. et al.

2010. Design of observational studies, volume 10. Springer.

Rubin, D. B.

2005. Causal inference using potential outcomes: Design, modeling, decisions. Journal of the
American Statistical Association, 100(469):322–331.

Singh, J. A., K. G. Saag, S. L. Bridges Jr, E. A. Akl, R. R. Bannuru, M. C. Sullivan, E. Vaysbrot,

C. McNaughton, M. Osani, R. H. Shmerling, et al.
2016. 2015 american college of rheumatology guideline for the treatment of rheumatoid arthritis.
Arthritis & rheumatology, 68(1):1–26.

Spellberg, B., R. Guidos, D. Gilbert, J. Bradley, H. W. Boucher, W. M. Scheld, J. G. Bartlett,

J. Edwards Jr, and I. D. S. of America
2008. The epidemic of antibiotic-resistant infections: a call to action for the medical community
from the infectious diseases society of america. Clinical infectious diseases, 46(2):155–164.

Sutton, R. S., A. G. Barto, et al.

1998. Introduction to reinforcement learning, volume 2. MIT press Cambridge.

Thomas, P. and E. Brunskill

2016. Data-efﬁcient off-policy policy evaluation for reinforcement learning. In International
Conference on Machine Learning, Pp. 2139–2148.

WHO

1978. International classiﬁcation of diseases : Ninth revision, basic tabulation list with alphabetic
index.

Zhang, J. and E. Bareinboim

2019. Near-optimal reinforcement learning in dynamic treatment regimes. In Advances in Neural
Information Processing Systems, Pp. 13401–13411.

12

Supplementary material for: Learning to search
efﬁciently for causally near-optimal treatments

Samuel H˚akansson
University of Gothenburg
samuel.hakansson@gu.se

Viktor Lindblom
Chalmers University of Technology
viklindb@student.chalmers.se

Omer Gottesman
Brown University
omer gottesman@brown.edu

Fredrik D. Johansson
Chalmers University of Technology
fredrik.johansson@chalmers.se

A Proofs of theorems

A.1 Proof of Lemma 1 (Stationarity)

Lemma S1 (Lemma 1 restated). Let I be a permutation of the sequence (1, ..., s). Then, for our
causal graph under Assumption 2, for b ∈ A,

p(Y (b) | X, As = a, Y s = y) = p(Y (b) | X, As = (aI(1), ..., aI(s)), Y s = (yI(1), ..., yI(s)))

Proof. Let h = (x, (a1, y1), ..., (as, ys)). Let π be a permutation of 1, ..., s and π(r) the index
assigned to r. We use the short-hands p(a) = p(A = a), p(A | b) = p(A | B = b), etc.

p(Y (a) | Hs = hs, As = as)

stationarity

=

=

=

=

=

=

=

p(Ys(a), hs, as)
p(hs, as)
z p(Ys(a), hs, as, z)
(cid:80)
z p(hs, as, z)

(cid:80)

r p(yr | hr, ar, z)p(ar | hr, z)p(z)

(cid:80)

(cid:80)

(cid:80)

(cid:80)

(cid:80)

(cid:80)

z p(Ys(a) | hs, as, z)p(as | hs, z)p(hs | z)p(z)
z p(as | hs, z)p(hs | z)p(z)
z p(Ys(a) | hs, as, z)p(as | hs, z) (cid:81)
(cid:81)

(cid:80)
z p(Ys(a) | as, z)p(as | hs) (cid:81)
z p(as | hs) (cid:81)

z

r p(yr | hr, ar, z)p(ar | hr, z)p(z)

r p(yr | ar, z)p(ar | hr)p(z)

r p(yr | ar, z)p(ar | hr)p(z)

(cid:80)
z p(Ys(a) | z) (cid:81)
(cid:81)
z p(Y (a) | z) (cid:81)
(cid:81)

(cid:80)
z

(cid:80)
z

r p(yr(ar) | z)p(z)

r p(yr(ar) | z)p(z)

r p(Yπ(r)(ar) = yr | z)p(z)

r p(Yπ(r)(ar) = yr | z)p(z)

prob. laws

expand

expand history

As ⊥⊥ Z | Hs

cancel terms

stationarity

Since the last expression is invariant to π, the result follows.

A.2 Proof of Theorem 1 (Identiﬁability)

Theorem S1 (Theorem 1 restated). Under Assumptions 1–4, the stopping statistic ρ(h) in (2) and
(cid:15), δ-optimality are identiﬁable from the observational distribution p(X, T, A, Y ). In particular, for

13

any time step s with history hs, let h(I)s = (x, aI(1), ..., aI(s), yI(1), ..., yI(s)) be an arbitrary
permutation of hs. Then, for any sequence of untried (future) actions as+1:k = (as+1, ..., ak) ∈
S(A−hs ) with h(I)r the continued history at time r > s corresponding to as+1:k and ys+1:k,

ρ(hs) =

(cid:88)

1 [max(y) > µ(hs) + (cid:15)]

ys+1:k∈Y k−s
where µ(hs) = max(a,y)∈hs y.

k
(cid:89)

r=s+1

p(Yr = yr | Ar = ar, Hr = h(I)r−1) . (S1)

Proof. Fix any history h = (x, (a1, y1), ..., (as, ys)) ∈ H with s = |h|, any time points q, r ∈ [k],
any a ∈ A and let a ∈ S(A) such that the subsequence as = (a1, ..., as) coincides with h. Then, by
Assumption 2, we have

Yr(a) = Yq(a) = Y (a) and max
a(cid:54)∈h

Y (a) =

k
max
r=s+1

Yr(ar) .

Below, we sum over sequences of outcomes ys+1:k = (ys+1, ..., yk) ∈ Y k−s and refer to the history
hr for r > s. Here, hr = (x, (a1, y1), ..., (ar, yr)) is a sequence of both observed actions and
outcomes (corresponding to the sub-sequence hs ⊆ hr) and unobserved ones. By deﬁnition, we have
for any sequence of actions a ∈ S(A) according to the above, for any µ ∈ Y

ρµ(hs) =

(cid:88)

ys+1:k∈Y k−s

p([Y (as+1), . . . , Y (ak)] = ys+1:k | Hs = hs)1[max(ys+1:k) > µ]

=

=

(cid:88)

1 (cid:2)max(ys+1:k) > µ(cid:3)

ys+1:k∈Y k−s

(cid:88)

1 (cid:2)max(ys+1:k) > µ(cid:3)

ys+1:k∈Y k−s

k
(cid:89)

r=s+1

k
(cid:89)

r=s+1

p(Yr(ar) = yr | Hr−1 = hr−1)

p(Yr = yr | Ar = ar, Hr−1 = hr−1) .

In the second step we apply Assumption 2 (stationarity) and in the third Assumptions 1–Assumptions 4
(consistency, sequential ignorability). Finally, from ignorability and stationarity, we have for any
permutations h(I)s,

ρµ(hs) =

(cid:88)

1 (cid:2)max(ys+1:k) > µ(cid:3)

ys+1:k∈Y k−s

k
(cid:89)

r=s+1

p(Yr = yr | Ar = ar, Hr−1 = h(I)r−1) .

Doing so, we obtain the result in (4). In solving (1), we only need to evaluate ρ(hs) for histories with
positive support under pπ. Assumption 3 (positivity) ensures that there exists at least one permutation
a ∈ S(A−hs) such that p(As+1:k = a | Hs = hs). This in turn implies identiﬁability.

A.3 Bounds on stopping criterion

Theorem S2. For any threshold µ ∈ Y and history h ∈ H, we have under Assumption 2,

[p (Y (a) > µ | h)]

max
a(cid:54)∈h
(cid:125)
(cid:123)(cid:122)
(cid:124)
Used for less conservative stopping

≤ p

(cid:124)

(cid:18)

max
a(cid:54)∈h

(cid:19)

Y (a) > µ | h

≤

(cid:123)(cid:122)
=: ρµ(h)

(cid:125)

(cid:88)

p (Y (a) > µ | h)

a(cid:54)∈h
(cid:124)

(cid:123)(cid:122)
Used for more conservative stopping

(cid:125)

(S2)

Proof. Let A−h = {a ∈ A : a (cid:54)∈ h}. We start with the upper bound. By deﬁnition
{y ∈ Y |A−h| : ya > µ}

{y ∈ Y |A−h| : max(y) > µ} =

(cid:91)

Hence, by Boole’s inequality,

a∈A−h

(cid:18)

p

max
a(cid:54)∈h

Y (a) > µ | H = h

≤

(cid:19)

(cid:88)

(cid:88)

p (Y (A−h) = y | H = h)

a∈A−h
(cid:88)

a∈A−h

=

y∈Y |A−h | : y(a)>µ

p (Y (a) > µ | h) .

14

For the lower bound, the argument is equally straight-forward.

(cid:18)

p

max
a(cid:54)∈h

Y (a) > µ | H = h

=

(cid:19)

(cid:88)

y : max(y)>µ

p (Y (A−h) = y | H = h)

≥ max
a∈A−h

= max
a∈A−h

(cid:88)

p (Y (A−h) = y | H = h)

y : y(a)>µ
p (Y (a) > µ | H = h) .

A.4 Proof of Theorem 2

We restate the following assumption and Theorem 2 for convenience.
Assumption S1. A random variable U has α-bounded propensity sensitivity relative to H if for all
u ∈ U, h ∈ H, with s = |h| and a ∈ Ak−s, for some α ≥ 1, with As+1:k = (As+1, ..., Ak),

1
α

≤

Pr[As+1:k = a | H = h]
Pr[As+1:k = a | U = u, H = h]

≤ α .

Theorem S3 (Theorem 2 restated). Given is that Assumption S1 (bounded propensity) holds for
H, U with sensitivity parameter α ≥ 1 and Assumption 4 (ignorability) holds for all s ∈ [k] w.r.t.
confounders (Hs, U ). Let Yr be the (hypothetical) outcome of treatment Ar at time r = s + 1, ..., k.
Then, for any history h ∈ Hs and the set of treatments a = A \ A(h), it holds that

Pr[

k
max
r=s+1

Yr > µ | As+1:k = a, Hs = h] ≤

δ
α

=⇒ Pr[max
a∈a

Y (a) > µ | Hs = h] ≤ δ .

Proof. We have by deﬁnition, where y > µ applies element-wise,

Pr[max
a∈a

Pr[max

i

Y (a) > µ | H = h] =

(cid:88)

y:y>µ

Pr[Y (a) = y | H = h]

Yi > µ | A = a, H = h] =

(cid:88)

y:y>µ

Pr[Y = y | A = a, H = h]

Then, marginalizing over the unobserved confounder U and conditioning on H,

Pr[max
a∈a

Y (a) > µ | H = h] =

=

(cid:88)

y:y>µ
u∈U
(cid:88)

y:y>µ
u∈U

Pr[Y (a) = y | h, U = u]p(U = u | h)

Pr[Y = y | h, u, a]p(u | h)

where the last equality follows from ignorability w.r.t. H, U . Applying the same steps to
Pr[maxi Yi > µ | A = a], we get

Pr[max

i

Yi > µ | h, A = a] =

(cid:88)

y:y>µ
u∈U

Pr[Y = y | h, u, a]p(u | a, h)

We ﬁnd that

Y (a) > µ | h] − Pr[max

i

Yi > µ | h, A = a]

Pr[y | h, u, a] (p(u | h) − p(u | a, h))

Pr[y | h, u, a]p(u | a, h)

(cid:18) p(u | h)
p(u | a, h)

(cid:19)

− 1

= (∗)

Pr[max
a∈a
(cid:88)

=

y:y>µ
u∈U

(cid:88)

y:y>µ
u∈U

=

15

By Bayes rule, we have

and so,

p(u | h)
p(u | a, h)

=

p(u | h)p(a | h)
p(a | u, h)p(u | h)

=

p(a | h)
p(a | u, h)

(∗) =

(cid:88)

y:y>µ
u∈U

Pr[y | h, u, a]p(u | a, h)

(cid:18) p(a | h)
p(a | u, h)

(cid:19)

− 1

The result follows immediately from our Assumption S1, that 1
upper bound is needed.

α ≤ p(a|h)

p(a|u,h) ≤ α. In fact, only the

A.5 Proof of Theorem 3 (Correctness of dynamic programming)

Theorem S4 (Theorem 3 restated). Recall that H0 = (X, ∅, ∅). The policy maximizing (6), π(h) =
arg maxa Q(h, a), is an optimal solution to (1) and its expected search time is E[T ] = EX [−V (H0)].

Proof sketch. Recall that

γ(cid:15),δ,α(h) := 1[Pr[max
a(cid:48)(cid:54)∈h

Y (a(cid:48)) > µ(h) + (cid:15) | H = h] < δ/α]

(S3)

Q(h, a) = r(h, a) + 1[a (cid:54)= STOP]

(cid:88)

y∈Y

p(Y (a) = y | h)

max
a(cid:48)∈A∪{STOP}

Q(h ∪ {(a, y)}, a(cid:48)) ,

r(cid:15),δ,α(h, a) =

(cid:40) −∞,
0,
−1,

if a = STOP, γ(cid:15),δ,α(h) = 0
if a = STOP, γ(cid:15),δ,α(h) = 1
if a (cid:54)= STOP

.

(S4)

(S5)

and V (h) = maxa,c Q(h, a).
By deﬁnition, any policy that achieves a ﬁnite expected reward EH0 [V (H0)] satisﬁes the stopping
criterion, and is therefore a feasible solution to (1). Furthermore, any time search is terminated
(a = STOP or A−h = ∅), the expected sum of rewards for a sequence is equal to minus the number
of steps spent until the sequence terminates. The sequence is optimal if it terminates as soon
as an (cid:15), δ-optimal treatment is found. Thus, a policy with ﬁnite expected return that maximizes
(cid:3)
V (H0) = maxa Q(H0, a) is an optimally efﬁcient search policy for effective treatments.

A.6 Approximation ratio of greedy algorithms

The active learning problem concerns identiﬁcation of a hypothesis g ∈ G by iteratively performing
tests suggested by a policy (Guillory and Bilmes, 2009). The problem then amounts to ﬁnding a
policy π which selects tests A = A1, ..., AT , the results Y (A1), ..., Y (AT ) of which identify g with
probability 1, p(G = g | Y (A1), ..., Y (AT )) = 1. We consider now the case were a prior distribution
p(G, Y (1), ..., Y (k)) is known, as studied by (Guillory and Bilmes, 2009). A sequence of tests A
which identiﬁes g is associated with a cost c(A, G), and the objective is to ﬁnd π which minimizes
the expected cost over p,

We have the following result from the literature.

c(π) = EG,A∼π[c(A, G)] .

Theorem S5 (Adapted from Theorem 4 of (Kosaraju et al., 1999)). There exists a greedy policy π
such that for any p such that Y (a) : a ∈ A are deterministic given G,

where π∗ = arg minπ(cid:48) c(π(cid:48)) .

c(π) ≤ c(π∗)O(log |G|)

16

This bound is matched by a lower bound by (Chakaravarthy et al., 2007) which states that it is
NP-hard to achieve an approximation ratio better than o(log |G|).

In the setting with δ = 0, our problem may posed as active learning where the hypothesis corresponds
to the maximum value of potential outcomes, G = min{g ∈ Y : p(maxa Y (a) > g) ≤ 0}. Once
this quantity is identiﬁed, the stopping criterion may be determined immediately. However, under
this hypothesis, Y (a) are not deterministic given G and the results above do not apply. Golovin et al.
(2010) study the noisy case under the assumption that non-determinism in Y (a) is controlled by a
noise variable Θ, i.e., that Y (A) = f (G, Θ) for some deterministic function f .
Theorem S6 (Adapted from Theorem 3 in (Golovin et al., 2010) with uniform costs). Fix hypotheses
G, tests A and outcomes in Y, Fix a prior p(G, Θ) and a function f : G × supp(Θ) → Y |A| which
deﬁne the probabilistic noise model. Let c(π) denote the expected cost of π incurs to identify which
equivalence class G the outcome vector Y (AT ) belongs to. Let π∗ denote the policy minimizing c(·),
and let π denote the adaptive policy implemented by the greedy algorithm EC2. Then,

c(π) ≤ c(π∗)O(log |A| + log |supp(Θ)|) .

In the case that all combinations of outcomes are feasible, log |supp(Θ)| = |A| log |Y| and the bound
above is vacuous, since a trivial bound on the search time is |A|. When there is structure in potential
outcomes, supp(Θ) may be much smaller. For example, if the moderating variable Z controls all
uncertainty in Y (a), given X, the bound reduces to O(log |ZX |) where ZX = {z ∈ Z : p(Z | X) >
0}, which may be signiﬁcantly smaller than |A| log |Y|.

A.7 Model-free RL and CDP are not equivalent

Let max(·,y)∈h y represent the best outcome so far at history h, with s = |h| and λ > 0 a parameter
trading off early termination and high outcome. Now, consider the reward function rmodel-free
(h, a)
following history h ∈ H deﬁned below.

λ

rmodel-free
λ

(h, a) =

(cid:26) 0,

a (cid:54)= STOP
max(·,y)∈h y − λ|h|, a = STOP.

and the policy maximizing the expected sum of rewards
(cid:34) k

π∗,model-free
λ

= arg max

π

Eh,a∼π

(cid:35)

rmodel-free
λ

(hs, as)

.

(cid:88)

s=1

(S6)

(S7)

Now consider the greedy policy maximizing the Q-function deﬁned by

λ

(h, a) +

Q(h, a) = Eh(cid:48)|h,a[rmodel-free

max
a(cid:48)∈A−h∪{STOP}
For readers familiar with reinforcement learning, it is easy to see that policy maximizing Q deﬁned
above also maximizes the expected sum of rewards given by (S6). Below, we prove that this algorithm
does not in general solve (1).
Theorem S7. There are instances of (1) (main problem), speciﬁed by a distribution p and parameters
(cid:15), δ, such that the solutions to (1) and (S7) are distinct for every choice of λ > 0.

Q(h(cid:48), a(cid:48)) | Hs = h, As = a] .

(S8)

Proof. Consider a context-less setting with two actions A = {a, b} with the following potential
outcomes: p(Y (a) = 1.0) = 1/2, p(Y (a) = 0.5) = 1/2 and p(Y (b) = 0.5 + (cid:15)) = 1. In this
scenario, having observed nothing, the probability that action b yields a higher outcome than a is 1/2.
Hence, for δ = 0.5, CDP always prefers to start with action b and end immediately. Now, consider
NDL, which minimizes the expected return with the reward function,

(cid:26) 0,

r(h, a) =

a (cid:54)= STOP
max(·,y)∈h y − λ|h|, a = STOP

where s indicates the stop action and max(·,y)∈h y represents the best outcome so far at history h
and λ > 0. The Q-function is in (S8). NDP computes this recursively and uses the policy which
maximizes it. Under the version of this problem with (cid:15) < 0.25, we can show that there is no λ > 0
such that Q(∅, b) > Q(∅, a). We give the map of Q below under this assumption.

For λ > (cid:15), Q(∅, a) = 0.75 − λ and Q(∅, b) = max(0.5 + (cid:15) − λ, 0.75 + (cid:15)/2 − 2λ) < Q(∅, a). For
0 < λ ≤ (cid:15), we have Q(∅, a) = 0.75 − 1.5λ + (cid:15)/2 > Q(∅, b) by the assumption (cid:15) < 0.25. Hence,
NDL would, for any λ prefer action a. However, for δ = 0.5, CDP would prefer action b. Thus, for
δ = 0.5, there is no λ which make these equivalent.

17

h

A2
–
–
–
b
b
–
–
–
–
–

Y1
1.0
0.5
0.5 + (cid:15)
1.0
0.5
1.0
0.5
0.5 + (cid:15)
–
–

A1
a
a
b
a
a
a
a
b
–
–

Y2
–
–
–
0.5 + (cid:15)
0.5 + (cid:15)
–
–
–
–
–

a

STOP

STOP

STOP

STOP

STOP
b
b
a
a
b

Q(h, a)

1.0 − λ
0.5 − λ
0.5 + (cid:15) − λ
1.0 − 2λ
0.5 + (cid:15) − 2λ
1.0 − 2λ
0.5 + (cid:15) − 2λ
(1.0−2λ)+(0.5+(cid:15)−2λ)
2
(1.0−λ)+max(0.5−λ,0.5+(cid:15)−2λ)
2
max(0.5 + (cid:15) − λ, (1.0−2λ)+(0.5+(cid:15)−2λ)

2

)

B Historical smoothing and function approximation

The number of possible combinations (histories) grows exponentially with the number of actions,
k = |A|. As a result, it is very probably that certain combinations of histories h and actions a are
never observed in practice. We consider two solutions to this: historical smoothing and function
approximation. Historical smoothing is used in the discrete case

by estimating the probability p(Y (a) = y | Hs−1 = h) using a weighted average of outcomes
for observations (h, a, y) and observations for subsequences (h(cid:48), a, y) where h(cid:48) ⊆ h. Function
approximation imputes ˆp(Y (a) = y | Hs−1 = h) using a regression estimator trained on all
observations. We expand on these approaches in Appendix B.

B.1 Historical smoothing

Consider estimating the function p(Y (a) | H = h) in the discrete case. Under the stationarity
assumption, Assumption 2, it is sufﬁcient to represent the history in terms of indicators for tried
treatments, {Ba : a ∈ A} such that Ba ∈ {0, 1}, and observed outcomes of these actions. Hence,
p(Y (a) | H = h) may be represented by a table of dimensions |Y| × ({0, 1} × |Y|)|A|. Clearly,
even under this representation, the number of possible histories grows exponentially with the number
of actions. For this reason, for moderate to high numbers of actions, it will be unlikely to observe
samples for each cell of this table.

To obtain an estimate even in cases with high dimensionality, we use historical smoothing based on
a prior. In the discrete case, we may view the distribution of the outcomes Y (a) for a treatment a
following history h as a categorical distribution. We impose a Dirichlet prior on this distribution
and use the posterior distribution in estimating the stopping statistic ρ and in policy optimization.
A Dirichlet prior for p(Y (a) | H = h) is speciﬁed by pseudo-counts β1(a, h), ..., β|Y|(a, h). The
ny(a,h)+βy(a,h)
y(cid:48) ny(cid:48) (a,h)+βy(cid:48) (a,h) , where ny(a, h) is equal to the number of samples
posterior parameters are then
where Y (a) = y following history h. In this work, we consider two different priors β.

(cid:80)

Historical prior (kernel smoothing) The historical prior assumes that the conditional outcome
distribution changes slowly with the number of past observations. The prior itself is a weighted
average of the outcome probability at all possible previous histories,

βy(a, h) =

(cid:88)

h(cid:48)⊂h

w(h, h(cid:48)) · ˆp(Y (a) | H = h(cid:48)),

(S9)

where the weight of the probability given by a shorter history is determined by its similarity to h,

w(h, h(cid:48)) =

e−(|h|−|h(cid:48)|−1)2
|h| · 2|h−hi|−1

.

(S10)

Uninformed prior The uninformed prior assigns a small uniform value to all β.

18

B.2 Function approximation

Observations for the ith subject are denoted x(i), a(i)
s . To use function approximation, we
t
ﬁt a single function f , acting on a representation of history φ(h) to estimate p(Y (a) | H = h) by
solving the following problem,

, y(i)
t

, a(i)

min
f ∈F

n
(cid:88)

ti(cid:88)

i=1

s=1

L(f (h(i)

s , a(i)

s ), y(i)

s ) ,

(S11)

for an appropriately chosen function class F and loss function L. In the discrete settings considered
in the paper, we use the logistic (cross-entropy) loss which leaves the solution to (S11) a probabilistic
classiﬁer, or estimate of p(Y (a) | H = h) for all a, h.

C Additional experimental results

Below follow additional details and results from the experiments. All experiments were implemented
in Python and run on standard laptop computers. Each experiment on the synthetic DGP took less
than a handful of hours to ﬁnish. For the antibiotics experiment, the overall time to produce the
results for all values of δ was 2 days.

C.1 Synthetic data generating process

We describe the datagenerating process (DGP) for the synthetic dataset used in Figure 2b and
additional results described below. Let oa(h) = 1[a ∈ h] and o(h) = [o1(h), ..., ok(h)](cid:62). The
moderator Z ∈ {0, 1}d and covariates X ∈ {0, 1}v are drawn according to

1. Z ∼ Bernoulli(α)
2. X ∼ Bernoulli(max(min(βZ, 0.98), 0.02)).

given a set of parameters α ∈ [0, 1]d, β ∈ [0, 1]v×d drawn element-wise uniformly at random.

The action STOP is drawn at any point following the ﬁrst treatment with probability pSTOP = 0.1. To
emulate a closer-to-realistic policy, if not stopped, the next action is drawn according to a categorical
distribution with probabilities determined by the variable X and the dissimilarity of the new action A
to previous actions in H. Outcomes are drawn according to a categorical distribution with parameters
given by the pdf of a Cauchy random variable, itself with parameters depending on the variables X,
Z and A. For a full description of the data generating distribution, see Algorithm 1.

C.1.1 Additional results for the synthetic DGP

We present additional results for CDP, CG and NDP applied to the synthetic DGP described above.
Unless otherwise speciﬁed, δ = 0.4, (cid:15) = 0, λ = 0.35 and CDP and CG use the upper bound
approximation of the stopping criterion described in Appendix A.3 with historical smoothing ( H), as
described in Appendix B.

In Figure S1, we illustrate the mean efﬁcacy and search time (number of trials) as a function dataset
size, varying logarithmically from n = 50 to n = 75000 samples. We include the variance across
(cid:80)m
i=1 (xi − ¯x)2. This Figure is a different view of
m random seeds for the experiment, ˆσ2 = 1
Figure 2b, where we clearly see that the efﬁcacy for most algorithms go up as data set size grows and
search time decreases. For NDP, as noted in Section 6, we see the opposite trend, however.

m−1

Figure S2 shows the trade-off between search time (number of trials) for different algorithms and
40 different values of δ ∈ [0, 1] with λ = δ for n = 15000 samples, in the setting corresponding to
Figure 2b. In Figure S3, we give the corresponding comparison for using lower or upper bounds
in the estimation of the stopping criterion ρ, as described in Appendix A.3. Here, U refers to the
upper bound, L to the lower bound and E is “exact” estimator, i.e. the empirical estimator of the
exact expression for the stopping criterion, ρ. At ﬁrst glance, the output of the different algorithms
using different bounds appear very similar. However, as we see in Figure S4, the trade-off induced by
a speciﬁc value of δ varies greatly depending on the estimation strategy. This is discussed also in
Section 6, where we note that the policy learned by NDP is very sensitive to the setting of λ.

19

(a)

(b)

Figure S1: Efﬁcacy and time over different sized training sets for the synthetic DGP. Interval widths
represent the variance across 50 realizations.

Figure S2: Efﬁcacy and search time (number of trials) for different policy optimization methods
operating the same model (historical smoothing, upper bound).

(a) Constrained Dynamic Programming algorithm.

(b) Constrained Greedy algorithm.

Figure S3: Results using estimates of the stopping criterion based on the upper ( U) and lower bounds
( L) described in Appendix A.3, as well as the no-bound (exact) estimate ( E) for the CDP and CG
algorithms with δ varying linearly in [0, 1].

20

102103104105Data set size0.650.700.750.800.85EfficacyCDP_HCDP_FCG_HCG_FNDP_HNDP_F102103104105Data set size1.21.41.61.82.02.2Mean search timeCDP_HCDP_FCG_HCG_FNDP_HNDP_F1.001.251.501.752.002.252.502.75Mean time0.700.750.800.850.900.951.00EfficacyCDPCGNGNDP1.01.21.41.61.82.02.2Mean time0.40.50.60.70.80.91.0EfficacyCDP_UCDP_LCDP_E1.01.21.41.61.82.02.2Mean time0.700.750.800.850.900.951.00EfficacyCG_UCG_LCG_E(a) Efﬁcacy and search time (number of trials) for vary-
ing approximations used in estimating the stopping
criterion, with the upper bound, in the CDP algorithm.
U stands for using a uniform prior to ﬁll in missing
valus. H is the historical kernel smoothing described
in Appendix B. F refers to function approximation
and T the result for using the true model.

(b) Efﬁcacy and search time (number of trials) when
using different bounds on the stopping criterion ρ in
the CDP algorithm. U stands for using the upper
bound, L for the lower bound and E for the exact
(no bound) estimate of ρ.

Figure S4: Efﬁcacy and mean search time (number of trials), varying δ in [0, 1].

C.2 Antibiotic resistance dataset

Below, we give additional information on the antibiotic resistance dataset compiled from MIMIC-III.

To gather a cohort for which a consistent set of culture tests had all been performed for every patient,
the set of organisms were restricted to a small subset. This selection was made based on overall
prevalence in the data as well as the co-occurrence with common antibiotic culture tests. The selected
organisms and antibiotics are listed below.

Selected (bacterial) microorganisms:

• Escherichia Coli (E. coli)
• Pseudomonas aeruginosa
• Klebsiella pneumoniae
• Proteus mirabilis

Selected antibiotics:

• Ceftazidime
• Piperacillin/Tazo
• Cefepime

21

(a)

(b)

Figure S5: Efﬁcacy and mean search time over different values of δ on the antibiotic resistance data
set. The width of the plots represent the unbiased empirical sample variance across random splits.

(a)

(b)

Figure S6: Efﬁcacy and mean number of trials over different values of λ for the Naive Dynamic
Programming algorithm. Variance is unbiased sample variance across random splits of the data. λ is
perturbed by 0.0001 in order to avoid division by zero for λ = 0.

• Tobramycin
• Gentamicin
• Meropenem

pending was also an “result” in MIMIC-III, there were few of these instances and they were removed.
Covariates X: Ages are divided into the four groups [0, 15], (15, 31], (31, 60], and (60, ∞). The two
diseases are Infectious And Parasitic Diseases and Diseases Of The Skin And Subcutaneous Tissue as
classiﬁed by ICD (WHO, 1978). The data was split in training and test 70/30 from 1362 patients
and patients with multiple organisms were not split between the sets. Patients who had taken any
antibiotic other than our chosen ones were not included in the data. Figure S5 uses the same data as
Figure 3b but is split by δ and variance is shown.

# of treatments
1
2
3
4
5

# of patients
860
340
137
22
3

22

0.00.20.40.60.81.00.960.970.980.991.00EfficacyCDPCGCG_FCDP_FNDPNDP_F0.00.20.40.60.81.01.151.201.251.301.35Mean search timeCDPCGCG_FCDP_FNDPNDP_F0.000.250.500.751.001.251.501.752.000.650.700.750.800.850.900.951.00EfficacyNDP_HNDP_F0.000.250.500.751.001.251.501.752.001.01.52.02.53.03.54.0Mean number of trialsNDP_HNDP_FAlgorithm 1: Generating distribution of actions and potential outcomes
Input: Weight parameter wx (default value 1)
Input: Number of outcomes ny
Input: Uniform stopping probability pSTOP

Generating parameters:
u1, u2 ∼ N (0k×(1+v+d), 1)
u2 ← |u2|
for i ← 2 to v + 1 do

u1(·, i) ← u1(·, i) · wx
u2(·, i) ← u2(·, i) · wx

end
η ∼ N (0k×(1+v+k), 1)
for a ← 1 to k do

i=1

1 (a) ← (cid:80)1+v+d
u−
2 (a) ← (cid:80)1+v+d
u−
1 (a) ← (cid:80)1+v+d
u+
2 (a) ← (cid:80)1+v+d
u+

i=1

i=1

i=1

1[u−
1[u−
1[u−
1[u−

1 (a, i) < 0]u−
2 (a, i) < 0]u−
1 (a, i) > 0]u−
2 (a, i) > 0]u−

1 (a, i)
2 (a, i)
1 (a, i)
2 (a, i)

end

Generating distribution of actions:
p(A = STOP) = pSTOP
for a, a(cid:48) ∈ {1, ..., k} do

∆(a, a(cid:48)) ← (cid:107)u1(a) − u1(a(cid:48))(cid:107)2

2 + (cid:107)u2(a) − u2(a(cid:48))(cid:107)2
2

end
for h ∈ H do

v = [1; x; o(h)]
for a ∈ {1, ..., k} do

˜p(a) ← eη(a,·)(cid:62)v for a(cid:48) ∈ h do
˜p(a) ← ˜p(a) · ∆(a, a(cid:48))

end

end
for a ∈ {1, ..., k} do

p(A = a | h, A (cid:54)= STOP) ←

(cid:80)

˜p(a)
a∈{1,...,k} ˜p(a)

end

end

Generating distribution of potential outcomes:
for x ∈ X , z ∈ Z do
for a ← 1 to k do
v ← [1; x; z]
y0(a) ← u1(a, ·)(cid:62)v
y0(a) ← (ny−1)(y0(a)−u−
(u+
1 (a))
γ(a) ← u1(a, ·)(cid:62)v
γ(a) ← (γ(a)−u−
2 (a)−u−
(u+
for y ← 1 to ny do

2 (a))
2 (a))

1 (a)−u−

1 (a))

˜p(a, y) ← fcauchy(y; y0(a), γ(a))

end
for y ← 1 to ny do

p(Y (a) = y | x, z) ← ˜p(a,y)

(cid:80)ny

y=1 ˜p(a,y)

end

end

end

23

