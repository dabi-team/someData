1
2
0
2

b
e
F
7
1

]

G
L
.
s
c
[

2
v
3
7
9
0
0
.
7
0
0
2
:
v
i
X
r
a

Learning to search efï¬ciently for causally
near-optimal treatments

Samuel HËšakanssonâˆ—
University of Gothenburg
samuel.hakansson@gu.se

Viktor Lindblom
Chalmers University of Technology
viklindb@student.chalmers.se

Omer Gottesmanâ€ 
Brown University
omer gottesman@brown.edu

Fredrik D. Johansson
Chalmers University of Technology
fredrik.johansson@chalmers.se

Abstract

Finding an effective medical treatment often requires a search by trial and error.
Making this search more efï¬cient by minimizing the number of unnecessary trials
could lower both costs and patient suffering. We formalize this problem as learning
a policy for ï¬nding a near-optimal treatment in a minimum number of trials using
a causal inference framework. We give a model-based dynamic programming
algorithm which learns from observational data while being robust to unmeasured
confounding. To reduce time complexity, we suggest a greedy algorithm which
bounds the near-optimality constraint. The methods are evaluated on synthetic and
real-world healthcare data and compared to model-free reinforcement learning. We
ï¬nd that our methods compare favorably to the model-free baseline while offering
a more transparent trade-off between search time and treatment efï¬cacy.

1

Introduction

Finding a good treatment for a patient often involves trying out different options before a satisfactory
one is found (Murphy et al., 2007). If the ï¬rst-line drug is ineffective or has severe side-effects,
guidelines may suggest it is replaced by or combined with another drug (Singh et al., 2016). These
steps are repeated until an effective combination of drugs is found or all options are exhausted, a
process which may span several years (NCCMH, 2010). A long search adds to patient suffering and
postpones potential relief. It is therefore critical that this process is made as time-efï¬cient as possible.

We formalize the search for effective treatments as a policy optimization problem in an unknown
decision process with ï¬nite horizon (Garcia and Ndiaye, 1998). This has applications also outside of
medicine: For example, in recommendation systems, we may sequentially propose new products or
services to users with the hope of ï¬nding one that the user is interested in. Our goal is to perform as
few trials as possible until the probability that there are untried actions which are signiï¬cantly better
is smallâ€”i.e., a near-optimal action has been found with high probability. Historical observations
allow us to transfer knowledge and perform this search more efï¬ciently for new subjects. As more
actions are tried and their outcomes observed, our certainty about the lack of better alternatives
increases. Importantly, even a failed trial may provide information that can guide the search policy.

In this work, we restrict our attention to actions whose outcomes are stationary in time. This implies
both that repeated trials of the same action have the same outcome and that past actions do not
causally impact the outcome of future actions. The stationarity assumption is justiï¬ed, for example,

âˆ—This work was completed while the author was afï¬liated with Chalmers University of Technology.
â€ This work was completed while the author was afï¬liated with Harvard University.

 
 
 
 
 
 
for medical conditions where treatments manage symptoms but do not alter the disease state itself, or
where the impact of sequential treatments is known to be additive. In such settings, past actions and
outcomes may help predict the outcomes of future actions without having a causal effect on them.

We formalize learning to search efï¬ciently for causally effective treatments as off-policy optimization
of a policy which ï¬nds a near-optimal action for new contexts after as few trials as possible. Our set-
ting differs from those typical of reinforcement or bandit learning (Sutton et al., 1998): (i) Solving the
problem relies on transfer of knowledge from observational data. (ii) The stopping (near-optimality)
criterion depends on a model of unobserved quantities. (iii) The number of trials in a single sequence
is bounded by the number of available actions. We address identiï¬cation of an optimal policy using a
causal framework, accounting for potential confounding. We give a dynamic programming algorithm
which learns policies that satisfy a transparent constraint on near-optimality for a given level of
conï¬dence, and a greedy approximation which satisï¬es a bound on this constraint. We show that
greedy policies are sub-optimal in general, but that there are settings where they return policies with
informative guarantees. In experiments, including an application derived from antibiotic resistance
tests, our algorithms successfully learn efï¬cient search policies and perform favorably to baselines.

2 Related work

Our problem is related to the bandit literature, which studies the search for optimal actions through
trial and error (Lattimore and SzepesvÂ´ari, 2020), and in particular to contextual bandits (Abe et al.,
2003; Chu et al., 2011). In our setting, a very small number of actions is evaluated, with the goal
of terminating search as early as possible. This is closely related to the ï¬xed-conï¬dence variant of
best-arm identiï¬cation (Lattimore and SzepesvÂ´ari, 2020, Chapter 33.2), in which only exploration is
performed. To solve this problem without trying each action at least once, we rely on transferring
knowledge from previous trials. This falls within the scope of transfer and meta learning. Liao et al.
(2020) explicitly tackled pooling knowledge across patients data to determine an optimal treatment
policy in an RL setting and Maes et al. (2012) devised methods for meta-learning of exploration
policies for contextual bandits. A notable difference is that we assume that outcomes of actions are
stationary in time. We leverage this both in model identiï¬cation and policy optimization.

Experiments continue to be the gold standard for evaluating adaptive treatment strategies (Nahum-
Shani et al., 2012). However, these are not always feasible due to ethical or practical constraints.
We approach our problem as causal estimation from observational data (Rosenbaum et al., 2010;
Robins et al., 2000), or equivalently, as off-policy policy optimization and evaluation (Precup, 2000;
Kallus and Santacatterina, 2018). Unlike many works, we do not fully rely on ignorabilityâ€”that all
confounders are measured and may be adjusted for. Zhang and Bareinboim (2019) recently studied
the non-ignorable setting but allowed for limited online exploration. In this work we aim to bound
the effect of unmeasured confounding rather than to eliminate it using experimental evidence.

Our problem is closely related to active learning (Lewis and Gale, 1994), which has been used to
develop testing policies that minimize the expected number of tests performed before an under-
lying hypothesis is identiï¬ed. For a known distribution of hypotheses, ï¬nding an optimal policy
is NP-hard (Chakaravarthy et al., 2007), but there exists greedy algorithms with approximation
guarantees (Golovin et al., 2010). In our case, (i) the distribution is unknown, and (ii) hypotheses
(outcomes) are only partially observed. Our problem is also related to optimal stopping (Jacka, 1991)
of processes but differs in that the process is controlled by our decision-making agent.

3 Learning to search efï¬ciently for causally near-optimal treatments

We consider learning policies Ï€ âˆˆ Î  that search over a set of actions A := {1, ..., k} to ï¬nd an action
a âˆˆ A such that its outcome Y (a) âˆˆ Y is near-optimal. When such an action is found, the search
should be terminated as early as possible using a special stop action, denoted a = STOP. Throughout,
a high outcome is assumed to be preferred and we often refer to actions as â€œtreatmentsâ€. The potential
outcome Y (a) may vary between subjects (contexts) depending on baseline covariates X âˆˆ X âŠ† Rd
and unobserved factors. When a search starts, all potential outcomes {Y (a) : a âˆˆ A} are unobserved,
but are successively revealed as more actions are tried, see the illustration in Figure 1. To guide
selection of the next action, we learn from observational data of previous subjects.

2

Figure 1: Illustration of the observed sequence of treatments at = (a1, ..., at) and outcomes yt =
(y1, ..., yt) for a patient, and the problem of estimating the outcome of possible future treatments.

Historical searches are observed through covariates X and a sequence of T action-outcome pairs
(A1, Y1), ..., (AT , YT ). Note the distinction between interventional and observational outcomes;
Ys(a) represents the potential outcome of performing action a at time s (Rubin, 2005). We assume
that y âˆˆ Y are discrete, although our results may be generalized to the continuous case. Sequences
of s random variables (A1, ..., As) are denoted with a bar and subscript, As âˆˆ As, and Hs =
(X, As, Y s) âˆˆ Hs := X Ã— {As Ã— Y s} denotes the history up-to time s, with H0 = (X, âˆ…, âˆ…). With
slight abuse of notation, a âˆˆ h means that a was used in h and (a, y) âˆˆ h that it had the outcome y.
|h| denotes the number of trials in h. The set of histories of at most k actions is denoted H = âˆªk
s=1Hs.
Termination of a sequence is indicated by the sequence length T and may either be the result of
ï¬nding a satisfactory treatment or due to censoring. Hence, the full set of potential outcomes is not
observed for most subjects. Observations are distributed according to p(X, T, AT , Y T ).
We optimize a deterministic policy Ï€ which suggests an action a following observed history h,
starting with (x, âˆ…, âˆ…), or terminates the sequence. Formally, Ï€ âˆˆ Î  âŠ† {H â†’ A âˆª {STOP}}. Taking
the action Ï€(hs) = STOP at a time point s implies that T = s. Let pÏ€(X, A, Y , T ) be the distribution
in which actions are drawn according to the policy Ï€. For a given slack parameter (cid:15) â‰¥ 0 and a
conï¬dence parameter Î´ â‰¥ 0, we wish to solve the following problem.
[T ]

E

minimize
Ï€âˆˆÎ 

X,Y ,A,T âˆ¼pÏ€

(cid:20)

subject to Pr

max
a(cid:54)âˆˆAt

Y (a) > max
(a,y)âˆˆh

y + (cid:15)

(cid:12)
(cid:12)
(cid:12) Ht = h, T = t

(cid:21)

â‰¤ Î´, âˆ€t âˆˆ N, h âˆˆ Ht

(1)

In (1), the objective equals the expected search length under Ï€ and the constraint enforces that
termination occurs only when there is low probability that a better action will be found among the
unused alternatives. Note that if maxyâˆˆY y is known and is in Y s, the constraint is automatically
satisï¬ed at s. To evaluate the constraint, we need a model of unobserved potential outcomes. This is
dealt with in Section 4. We address optimization of (1) for a known model in Section 5.

4 Causal identiï¬cation and estimation of optimality conditions

Our assumed causal model for observed data is illustrated graphically in Figure 2a. Most notably, the
graph deï¬nes the causal structure between actions and outcomesâ€”previous actions A1, ..., Asâˆ’1 and
outcomes Y1, ..., Ysâˆ’1 are assumed to have no direct causal effect on future outcomes Ys, ..., YT . To
allow for correlations between outcomes, we posit the existence of counfounders X (observed) and
U (unobserved) and an unobserved moderator Z. All other variables are assumed exogenous.

To evaluate the near-optimality constraint and solve (1), we must identify the probability

Ï(h) := Pr[max
a(cid:54)âˆˆh

Y (a) > max
(a,y)âˆˆh

y + (cid:15) | H = h] ,

(2)

with the convention that max(a,y)âˆˆh0 y = âˆ’âˆ if |h0| = 0. Henceforth, let Aâˆ’h = {a âˆˆ A : a (cid:54)âˆˆ hs}
denote the set of untried actions at h and let S(Aâˆ’h) be all permutations of the elements in Aâˆ’h.
We state assumptions sufï¬cient for identiï¬cation of Ï(h) below. Throughout this work we assume that
consistency, stationarity of outcomes and positivity always hold, and provide identiï¬ability results
both when ignorability holds (Section 4.1) and when it is violated (Section 4.2).
Identifying assumptions. Deï¬ne As+1:k = (As+1, ..., Ak). Under the observational distribution p,
and evaluation distribution pÏ€, for all Ï€ âˆˆ Î , h âˆˆ Hs, and s, r âˆˆ N, we assume

3

ğ‘!,ğ‘"ğ‘¦!ğ‘#,ğ‘Œğ‘âˆ¶ğ‘âˆ‰(ğ‘!ğ‘¦#,ğ‘¦"â‹¯Observed	outcomesUnobservedPatient	at	baselineâ‹¯ğ‘‹(a) Assumed causal structure. Arrows between boxes
indicate connections between all variables in the
boxes: X is a cause of every As and Ys. Past actions
are assumed not to be direct causes of future outcomes.
Z is a moderator of treatment effects. Dashed outlines
indicate unobserved variables.

(b) Efï¬cacy (fraction of subjects for which optimal
action is found) and search length for varying amounts
of samples, with trade-off parameters Î´ = 0.4, (cid:15) = 0
(CDP, CG), Î» = 0.35 (NDP). The sufï¬x H indicates
historical smoothing and F function approximation.
Error bars indicate standard errors over 71 realizations.

Figure 2: Assumed causal structure (left) and results from synthetic experiments (right).

1. Consistency: Ys = Ys(As)

2. Stationarity: Ys(a) = Yr(a) =: Y (a)

3. Positivity:

âˆƒa âˆˆ S(Aâˆ’hs ) : pÏ€(Hs = hs) > 0 =â‡’ p(As+1:k = a | Hs = hs) > 0

4. Ignorability: Ys(a) âŠ¥âŠ¥ As | Hsâˆ’1

Ignorability follows from the backdoor criterion applied to the causal model of Figure 2a when U
is empty (Pearl, 2009). We expand on this setting next. In contrast to conventions typically used in
the literature, positivity is speciï¬ed w.r.t. the considered policy class. This ensures that every action
could be observed at some point after every history h that is possible under policies in Î . Under
Assumption 2 (stationarity), there is no need to try the same treatment twice, since the outcome is
already determined by the ï¬rst trial. We can restrict our attention to non-repeating policies,

Î  âŠ† {Ï€ : H â†’ A âˆª {STOP} ; Ï€(h) (cid:54)âˆˆ h} .

Non-repeating policies such as these take the form of a decision tree of depth at most k = |A|.
Remark 1 (Assumptions 1â€“4 in practice). Only the positivity assumption may be veriï¬ed empirically;
stationarity, consistency and ignorability must be justiï¬ed by domain knowledge. Readers experienced
with causal estimation will be familiar with the process of establishing ignorability and consistency
through graphical arguments or reasoning about statistical independences. Stationarity is more
speciï¬c to our setting and without it, the notion of a near-optimal action is not well-deï¬nedâ€”the
best action could change with time. This phenomenon occurs is settings where outcomes naturally
increase or decrease over time, irrespective of interventions. For example, the cognitive function
of patients with Alzheimerâ€™s disease tends to decrease steadily over time (Arevalo-Rodriguez et al.,
2015). As a result, measures of cognitive function Yt(a) for patients on a medication a will be
different depending on the stage t of progression that the patient is in. As a rule-of-thumb, stationarity
is better justiï¬ed over small time-frames or for more stable conditions.

4.1

Identiï¬cation without unmeasured confounders

Our stopping criterion Ï(h) is an interventional quantity which represents the probability that an
unused action would be preferable to previously tried ones. In general, this is not equal to the rate at
which such an action was preferable in observed data. Nevertheless, we prove that Ï(h) is identiï¬able
from observational data in the case that U does not exist (ignorability holds w.r.t. H). First, the
following lemma shows that the order of history does not inï¬‚uence the probability of future outcomes.

Lemma 1. Let I be a permutation of (1, ..., s). Under stationarity, for all a âˆˆ As and b (cid:54)âˆˆ a,

p(Y (b) | X, As = a, Y s = y) = p(Y (b) | X, As = (aI(1), ..., aI(s)), Y s = (yI(1), ..., yI(s))) (3)

4

				=	ğ´$	ğ´%	ğ´&â€¦	ğ‘Œ$	ğ‘Œ%	ğ‘Œ&	ğ‘	ğ‘‹Treatmentsâ€¦OutcomesModeratorConfounders	ğ‘ˆ1.21.41.61.82.0Mean number of trials0.6750.7000.7250.7500.7750.8000.8250.850EfficacyHigh-data regimeLow-data regimeCDP_HCDP_FCG_HCG_FNDP_HNDP_FLemma 1 is proven in Appendix A.1. As a consequence, we may treat two histories with the same
events in different order as equivalent when estimating p(Y (a) | H).

We can now state the following result about identiï¬cation of the near-optimality constraint of (1).

Theorem 1. Under Assumptions 1â€“4, the stopping criterion Ï(h) in (2) is identiï¬able from the
observational distribution p(X, T, A, Y ). For any time step s with history hs, let h(I)s =
(x, aI(1), ..., aI(s), yI(1), ..., yI(s)) be an arbitrary permutation of hs. Then, for any sequence of
untried actions as+1:k = (as+1, ..., ak) âˆˆ S(Aâˆ’hs ) with h(I)r the (hypothetical) continued history
at time r > s corresponding to as+1:k and ys+1:k, and with Âµ(hs) = max(a,y)âˆˆhs y,

Ï(hs) =

(cid:88)

1 [max(y) > Âµ(hs) + (cid:15)]

ys+1:kâˆˆY kâˆ’s

k
(cid:89)

r=s+1

p(Yr = yr | Ar = ar, Hr = h(I)râˆ’1) .

(4)

A proof of Theorem 1 is given in Appendix A.2. Equation (4) gives a concrete means to estimate Ï(h)
from observational data by constructing a model of p(Ys | As, Hsâˆ’1 = h). Due to Assumption 2
(stationarity), this model can be invariant to permutations of h. Another important consequence of
this result is that, because Theorem 1 holds for any future sequence of actions, (4) holds also over any
convex combination for different future action sequences, such as the expectation over the empirical
distribution. Using likely sequences under the behavior policy will lead to lower-variance estimates.

Remark 2. In the fully discrete case, we may estimate p(Y | A, H) using a probability table, and
we do so in some experiments in Section 6. However, this becomes increasingly difï¬cult for both
statistical and computational reasons when A and Y grow larger or when any of the variables are
continuous. The permutation invariance given by Theorem 1 provides some relief but, nevertheless,
the number of possible combinations (histories) grows exponentially with the number of actions.
As a result, it is very probable that certain pairs of histories and actions (h, a) are never observed
in practical applications. We consider two remedies to this. In Appendix B, we give methods for
leveraging observations of similar histories h(cid:48) â‰ˆ h in the estimation of p(Y | H = h, A), one
based on historical kernel-smoothing in the tabular case, and one based on function approximation.
These are compared empirically in Section 6. In Section 5.2, we give bounds to use in place of the
probability of unobserved potential outcomes which further mitigate the curse of dimensionality.

4.2 Accounting for unobserved confounders

If Assumption 4 (ignorability) does not hold with respect to observed variables, the stopping criterion
Ï(hs) may not be identiï¬ed from observational data without further assumptions. A natural relaxation
of ignorability is that the same condition holds w.r.t. an expanded adjustment set (Hs, U ), where
U âˆˆ U is an unobserved set of variables. This is the case in our assumed causal model, see Figure 2a.
We require additionally that U has bounded inï¬‚uence on treatment propensity. For all u âˆˆ U, h âˆˆ H,
with s = |h| and a âˆˆ S(Aâˆ’h), assume that there is a sensitivity parameter, Î± â‰¥ 1, such that

1
Î±

â‰¤

Pr[As+1:k = a | Hs = h]
Pr[As+1:k = a | U = u, Hs = h]

â‰¤ Î± ,

(5)

where As+1:k is deï¬ned as in Assumption 3. Like ignorability, this assumption must be justiï¬ed from
external knowledge since U is unobserved. We arrive at the following result.

Theorem 2. Assume that (5) and Assumptions 1â€“4 hold with respect to (Hs, U ) for all s âˆˆ [k] with
sensitivity parameter Î± â‰¥ 1. Then, for any h âˆˆ Hs, a âˆˆ S(Aâˆ’h) and Î½ = Âµ(h) + (cid:15), we have

Pr[

k
max
r=s

Yr > Î½ | As+1:k = a, Hs = h] â‰¤

Î´
Î±

=â‡’ Ï(h) = Pr[max
aâˆˆa

Y (a) > Î½ | Hs = h] â‰¤ Î´ .

A proof of Theorem 2 is given in Appendix A.4. To achieve near-optimality with conï¬dence level of Î´
in the presence of unobserved confounding with propensity inï¬‚uence Î±, we must require a conï¬dence
level of at most Î´/Î±. Unlike classical approaches to sensitivity analysis, as well as more recent
results (Kallus and Zhou, 2018), this argument does not rely on importance (propensity) weighting.

5

5 Policy optimization

We give two algorithms for policy optimization under the assumption that a model of the stopping
criterion Ï(h) is known. As noted previously, this problem is NP-hard due to the exponentially
increasing number of possible histories (Rivest, 1987). Nevertheless, for moderate numbers of
actions, we may solve (1) exactly using dynamic programming, as shown next. Then we propose a
greedy approximation algorithm and discuss model-free reinforcement learning as alternatives.

5.1 Exact solutions with dynamic programming

Let X, A, Y be discrete. For sufï¬ciently small numbers of actions, we can solve (1) exactly in this
setting. Let h(cid:48) = h âˆª {(a, y)} denote the history where (a, y) follows h and recall the convention
maxaâˆˆâˆ… Y (a) = âˆ’âˆ. Now deï¬ne Q to be the expected cumulative returnâ€”see e.g., Sutton et al.
(1998) for an introductionâ€”of taking action a in a state with history h âˆˆ H,
(cid:88)

Q(h, a) = r(h, a) + 1[a (cid:54)= STOP]

p(Y (a) = y | h)

Q(h âˆª {(a, y)}, a(cid:48)) ,

max
a(cid:48)âˆˆAâˆª{STOP}

(6)

yâˆˆY

where r(h, a) is a reward function deï¬ned below. The value function V at a history h is deï¬ned in
the usual way, V (h) = maxa Q(h, a). To satisfy the near-optimality constraint of (1), we use an
estimate of the function Ï(h), see (2), to deï¬ne Î³(cid:15),Î´,Î±(h) := 1[Ï(h) < Î´/Î±] for parameters (cid:15), Î´ â‰¥ 0,
Î± â‰¥ 1. The function Î³(cid:15),Î´,Î±(h) represents whether an (cid:15), Î´/Î±-optimum has been found. We deï¬ne

r(cid:15),Î´,Î±(h, a) =

(cid:40) âˆ’âˆ,
0,
âˆ’1,

if a = STOP, Î³(cid:15),Î´,Î±(h) = 0
if a = STOP, Î³(cid:15),Î´,Î±(h) = 1
if a (cid:54)= STOP

.

(7)

With this, given a model of p(Ys(a) | Hsâˆ’1, As), the Q-function of (6) may be computed using
dynamic programming, analogous to the standard algorithm for discrete-state reinforcement learning.
Theorem 3. Recall that H0 = (X, âˆ…, âˆ…). The policy maximizing (6), Ï€(h) = arg maxa Q(h, a),
with reward given by (7) is an optimal solution to (1) with objective EpÏ€ [T ] = EX [âˆ’V (H0)].

Theorem 3 follows from Bellman optimality and the deï¬nition of r in (7), see Appendix A.5.

5.2 A greedy approximation algorithm

We propose a greedy policy as an approximate solution to (1) in high-dimensional settings where
exact solutions are infeasible to compute. We then discuss sub-optimality and approximation ratios
of greedy algorithms. First, consider the greedy policy Ï€G, which chooses the treatment with the
highest probability of ï¬nding a best-so-far outcome, weighted by its value, according to

f (h, a) = E[1[Y (a) > max
(Â·,y)âˆˆh

y]Y (a) | Hs = h]

until the stopping criterion is satisï¬ed,

Ï€G(h) :=

(cid:26) STOP,

Î³(cid:15),Î´,Î±(h) = 1

arg maxa(cid:54)âˆˆh f (h, a), otherwise

,

(8)

(9)

where Î³(cid:15),Î´,Î± is deï¬ned as in Section 5.1. While using Ï€G avoids solving the costly dynamic
programming problem of the previous section, it still requires evaluation of Î³(h). Even for short
histories, |h| â‰ˆ 1, computing Î³(h) involves modeling the distribution of maximum-length sequences
over potentially |Y||A| conï¬gurations. To increase efï¬ciency, we bound the stopping statistic Ï, and
approximate Î³, using conditional distributions of the potential outcome of single actions.

Ï(h) := p

(cid:18)

max
a(cid:54)âˆˆh

Y (a) > Âµ(h) + (cid:15) | h

â‰¤

(cid:19)

(cid:88)

a(cid:54)âˆˆh

p (Y (a) > Âµ(h) + (cid:15) | h) .

(10)

A proof is given in Appendix A.3. Using the upper bound in place of Ï(h) leads to a feasible solution
of (1) with more conservative stopping behavior and better outcomes but worse expected search time.
In the case Î´ = 0, the exact statistic and the upper bound lead to identical policies. Representing
the upper bound as a function of all possible histories still requires exponential space in the worst
case, but only a small subset of histories will be observed for policies that terminate early. We use
the bound on Ï(h) in experiments with both dynamic programming and greedy policies in Section 6.
The general problem of learning bounds on potential outcomes was studied by (Makar et al., 2020).

6

Example 1. In the following example, the greedy policy does identify a near-optimal action after
the smallest expected number of trials, for Î´ = 0, (cid:15) = 0. Let X = 0, Y âˆˆ {0, 1} and A âˆˆ {1, 2, 3},
Z = {1, ..., 4}, p(Z) = [0.20, 0.15, 0.20, 0.45]T and let C be the matrix with elements cij such that

p(Y (j) = 1 | Z = i) = cij, with C (cid:62) =

(cid:34)1
0
1

0
1
0

1
0
0

(cid:35)

0
1
1

.

In this scenario, p(Y (Â·) = 1) = [0.4, 0.6, 0.65](cid:62). The greedy strategy would thus start with Ï€(âˆ…) = 3,
followed by Ï€((3)) = 2 and then Ï€((3, 2)) = 1 to guarantee successful treatment. An optimal strategy
is to start with A1 = 2 and then A2 = 1. The expected time E[T ] is 1.5 under the greedy policy and
1.4 under the optimal one. The worst-case time under the greedy strategy is 3 and 2 under the optimal.

In Appendix A.6, we show that our problem is equivalent to a variant of active learning once a model
for p(Y (a1), ..., Y (ak), X) is known. In general, it is NP-hard to obtain an approximation ratio better
than a logarithmic factor of the number of possible combinations of potential outcomes (Golovin et al.,
2010; Chakaravarthy et al., 2007). However, for instances with additional structure, e.g., through
correlations induced by the moderator Z, this ratio may be signiï¬cantly smaller than |A| log |Y|.

5.3 A model-free approach

In off-policy evaluation, it has been noted that for long-term predictions, model-free approaches may
be preferable to, and suffer less bias than, their model-based counterparts (Thomas and Brunskill,
2016). They are therefore natural baselines for solving (1). We construct such a baseline below.

Let max(Â·,y)âˆˆh y represent the best outcome so far in history h, with s = |h| and let Î» > 0 be a
parameter trading off early termination and high outcome. Now, consider a reward function r(h, a)
which assigns a reward at termination equal to the best outcome found so far. A penalty âˆ’Î» is
awarded for each step of the sequence until termination, a common practice for controlling sequence
length in reinforcement learning, see e.g, (Pardo et al., 2018). Let

rÎ»(h, a) = {0, if a (cid:54)= STOP ; max
(Â·,y)âˆˆh

y âˆ’ Î»|h|, if a = STOP} .

(11)

The policy Ï€Î» which optimizes this reward, using dynamic programming as in Section 5.1, is used as
a baseline in experiments in Section 6.

While this approach has the advantage of not requiring a model of future outcomes, without a
model, the stopping criterion Ï(h) cannot be veriï¬ed and the advantage of being able to specify
an interpretable certainty level is lost. This is because the trade-off parameter Î» does not have a
universal interpretationâ€”the value of Î» which achieves a given rate of near-optimality will vary
In contrast, the conï¬dence parameter Î´ directly represents a bound on the
between problems.
probability that there is a better treatment available when stopping. Additionally, in Appendix A.7,
we prove that there are instances of the main problem (1), for a given value of Î´, such that no setting
of Î» results in an optimal solution.

6 Experiments

We evaluate our proposed methods using synthetic and real-world healthcare data in terms of the
quality of the best action found, and the number of trials in the search.3 In particular, we study the
efï¬cacy of policies, deï¬ned as the fraction of subject for which a near-optimal action has been found
when the choice to stop trying treatments is made. Models of potential outcomes are estimated using
either a table with historical smoothing (labeled with sufï¬x H) or using function approximation using
random forests (sufï¬x F), see Appendix B. Following each estimation strategy, we compare policies
learned using constrained dynamic programming (CDP), the constrained greedy approximation (CG)
and the model-free RL variant, referred to as as naÂ¨Ä±ve dynamic programming (NDP), see Section 5.
Establishing near-optimality is infeasible in most observational data as only a subset of actions are
explored. However, as we will see, in our particular application, it may be determined exactly.

3Implementations can be found at: https://github.com/Healthy-AI/TreatmentExploration

7

6.1 Synthetic experiments: Effect of sample size and algorithm choice

To investigate the effects of data set size, number of actions, dimensionality of baseline covariates and
the uncertainty parameter Î´ on the quality of learned policies, we designed a synthetic data generating
process (DGP). This DGP parameterizes probabilities of actions and outcomes as log-linear functions
of a permutation-invariant vector representation of history and of (X, Z), respectively. For the results
here, A = {1, ..., 5}, X = {0, 1}, Y = {0, 1, 2}, Z = {0, 1}3. Due to space limitations, we give the
full DGP and more results of these experiments in Appendix C.1.

We compare the effect of training set size for the different policy optimization algorithms (CDP, CG,
NDP and model estimation schemes ( F, H). Here, CDP and CG use Î´ = 0.4, (cid:15) = 0 and the upper
bound of (10) and NDP Î» = 0.35. We consider training sets in a low-data regime with 50 samples
and a high-data regime of 75000, with ï¬xed test set size of 3000 samples. Results are averaged over
71 realizations. In Figure 2b, we see that the value of all algorithms converge to comparable points
in the high-data regime but vary signiï¬cantly in the low-data regime. In particular, CG and CDP
improve on both metrics as the training set grows. The time-efï¬cacy trade-off is more sensitive to the
amount of data for NDP than for the other algorithms, and while additional data signiï¬cantly reduces
the mean number of actions taken, this comes at a small expense in terms of efï¬cacy. This highlights
the sensitivity of the naÂ¨Ä±ve RL-based approach to the choice of reward: the scale of the parameter Î»
determines a trade-off between the number of trials and efï¬cacy, the nature of which is not known in
advance. In contrast, CDP and CG are preferable in that Î´ and (cid:15) have explicit meaning irrespective
of the sample and result in a subject-speciï¬c stopping criterion, rather than an average-case one.

6.2 Optimizing search for effective antibiotics

Antibiotics are the standard treatment for bacterial infections. However, infectious organisms can
develop resistance to speciï¬c drugs (Spellberg et al., 2008) and patterns in organism-drug resistance
vary over time (Kanjilal et al., 2018). Therefore, when treating patients, it is important that an
antibiotic is selected to which the organism is susceptible. For conditions like sepsis, it is critical that
an effective antibiotic is found within hours of diagnosis (Dellinger et al., 2013).

As a proof-of-concept, we consider the task of selecting effective antibiotics by analyzing a cohort of
intensive-care-unit (ICU) patients from the MIMIC-III database (Johnson et al., 2016). We simplify
the real-world task by taking effective to mean that the organism is susceptible to the antibiotic.
When treating patients for infections in the ICU, it is common that microbial cultures are tested
for resistance. This presents a rare opportunity for off-policy policy evaluation, as the outcomes of
these tests may be used as the ground truth potential outcomes of treatment (Boominathan et al.,
2020). In practice, the results of these tests are not always available at the time of treatment. For this
reason, we learn models based on the test outcomes only of treatments actually given to patients. To
simplify further, we interpret concurrent treatments as sequential; their outcomes are not conï¬‚ated
here since they are taken from the culture tests. We stress that this task is not meant to accurately
reï¬‚ect clinical practice, but to serve as a benchmark based on a real-world distribution. Although a
patientâ€™s condition may change as a response to treatment, bacteria typically do not develop resistance
during a particular ICU stay, and so the stationarity assumption is valid.

Baseline covariates X of a patient represent their age group (4 groups), whether they had infectious
or skin diseases (2 Ã— 2 groups), and the identity of the organism, e.g., Staphylococcus aureus. These
were found to be important predictors of resistance by Ghosh et al. (2019). In total, X comprised
12 binary indicators. From the full set of microbial events in MIMIC-III, we restricted our study to
a subset of 4 microorganisms and 6 antibiotics, selected based on overall prevalence and the rate
of co-occurrence in the data. There were three distinct ï¬nal outcomes of culture tests, resistant,
intermediate, susceptible, encoded as Y = 0, 1, 2, respectively, where higher is better. The resulting
cohort restricted to patients treated using only the selected antibiotics consisted of n = 1362 patients
which had cultures tested for resistance against all antibiotics. The cohort was split randomly into a
training and test set with a 70/30 ratio and experiments were repeated over ï¬ve such splits. Patients
treated for multiple organisms were split into different instances. A full list of variables, the selected
antibiotics and organisms, and additional statistics are given in Appendix C.2.

We compare our learned policies to the policy used to select antibiotics in practice. However, due to
censoring, e.g., from mortality, the sequence length of observed patients may not be representative of
the expected number of trials used by the observed policy before an effective treatment is found. In

8

(a) Mean best-at-termination or best-so-far outcome
found after a given number of trials, across all subjects,
for different policies. Î´ = 0, Î» = 0.35.

(b) Efï¬cacy of antibiotics vs the mean number of trials
for different values of Î´ and Î» (one value per marker)
and different model estimation schemes.

Figure 3: Results from the antibiotics experiment. Average best-found outcome of different policies
across patients at different stages of the search (a) and efï¬cacy and search time (number of treatment
trials) as functions of Î´ (b). In plot (a), at a given number of trials, the best-so-far outcome is used for
ongoing sequences, and the best-at-termination is used for terminated ones. Efï¬cacy refers to the
rate at which a near-optimal treatment is found at the given Î´. Sufï¬xes F and H indicates model
estimation using function approximation and historical smoothing respectively. (cid:15) = 0.

other words, the average outcome for patients who went through t treatments is a biased estimate of
the value of the observed policy. Therefore, for direct comparison with current practice (â€œDoctorâ€),
only the mean outcome following the ï¬rst treatment point is displayed (star marker) in Figure 3a. For
an approximate comparison with current practice, as used in multiple treatment trials, we created a
baseline dubbed â€œEmulated doctorâ€. It uses a tabular estimate of the observed policy to imitate the
choices made by doctors in the dataset in terms of the history H = (X, A, Y ), i.e., it operates on the
same information as the other algorithms. We compare this to CDP, CG and NDP, and evaluate all
policies using culture tests for held-out observations. We sweep all hyperparameters uniformly over
10 values; for CDP, CG, Î´ âˆˆ [0, 1], for NDP H, Î» âˆˆ [0, 0.5] and for NDP F, Î» âˆˆ [0, 1].

In Figure 3a, we see that CG, CDP and NDP, with function approximation, all learn comparable
policies that are preferable to the estimated behavior policy. The mean search length was 1.26 for
CDP and NDP, 1.28 for CG and 1.38 for Emulated doctor. We see that the best treatment found after
a single trial is slightly better in the raw data (star marker). This may be because more information
is available to the physician than to our algorithms. The physician could (1) take into account the
original value of continuous variables, such as age, instead of using age groups and (2) use more
features of the patient in order to ï¬nd the right treatment. Using more covariates in this instance
would make the problem impractical to solve without further approximations since the table generated
by the dynamic programming algorithm grows exponentially. The current variable set was restricted
for this reason. In Figure 3b, we see that across different values of Î´, Î», all algorithms achieve
near-optimal efï¬cacy (almost 1), but vary in their search time. CDP is equal or preferable to CG,
with the model-free baseline NDP achieving the worst results. A much more noticeable difference is
that between policies learned using the model estimated with function approximation (sufï¬x F) and
those with a (smoothed) tabular representation (sufï¬x H).

7 Conclusion

We have formalized the problem of learning to search efï¬ciently for causally effective treatments.
We have given conditions under which the problem is solvable by learning from observational data,
and proposed algorithms that estimate a causal model and perform policy optimization. Our solution
using constrained dynamic programming (CDP) in an exponentially large state space illustrates the
associated computational difï¬culties and prompted our investigation of two approximations, one
based on greedy search and one on model-free reinforcement learning. We found that the greedy
search algorithm performed comparably to the exact solution in experiments and was less sensitive to
sample size. Determining conditions under which greedy algorithms are preferable statistically is an

9

123456Number of tried treatments1.751.801.851.901.95Mean treatment effectCDP_FCG_FNDP_FEmulated doctorMax outcomeDoctor1.241.261.281.301.321.341.36Mean number of trials0.9880.9900.9920.9940.9960.9981.000EfficacyFunction approximationHistorical smoothingCDP_HCG_HNDP_HCDP_FCG_FNDP_Finteresting open question. We believe that our work will have the largest impact in settings where a)
the assumption of potential outcome stationarity is justiï¬ed, b) even a small reduction in search time
is valuable and c) a transparent trade-off between efï¬cacy and search time is valuable in itself.

Broader impact

Personalized and partially automated selection of medical treatments is a long-standing goal for
machine learning and statistics with the potential to improve the lives of patients and reduce the
workload on physicians. This task is not without risk however, as poor decisions may fail to reduce or
even increase suffering. It is important that implementations of such ideas is guided by strong domain
knowledge, thorough evaluation and that checks and balances are in place. Many previous works in
this ï¬eld aim to identify new policies for treatment or doses with the goal of improving treatment
response itself. This goal is not always feasible to achieveâ€”some conditions are fundamentally hard
to treat with available medications and procedures. In contrast, we focus on conditions where a
good enough treatment would be identiï¬ed by an existing policy given enough time, with the goal of
reducing this search time as much as possible. The trade-off between a good outcome and time is
made transparent using a model of patient outcomes and a certainty parameter. With this, we hope to
contribute towards making machine learning methods more suitable for clinical implementation.

Funding disclosure

This work was supported in part by the Wallenberg AI, Autonomous Systems and Software Program
(WASP) funded by the Knut and Alice Wallenberg Foundation.

References

Abe, N., A. W. Biermann, and P. M. Long

2003. Reinforcement learning with immediate rewards and linear hypotheses. Algorithmica,
37(4):263â€“293.

Arevalo-Rodriguez, I., N. Smailagic, M. R. i Figuls, A. Ciapponi, E. Sanchez-Perez, A. Giannakou,

O. L. Pedraza, X. B. Cosp, and S. Cullum
2015. Mini-mental state examination (mmse) for the detection of alzheimerâ€™s disease and other
dementias in people with mild cognitive impairment (mci). Cochrane Database of Systematic
Reviews, 2015(3).

Boominathan, S., M. Oberst, H. Zhou, S. Kanjilal, and D. Sontag

2020. Treatment policy learning in multiobjective settings with fully observed outcomes. arXiv
preprint arXiv:2006.00927.

Chakaravarthy, V. T., V. Pandit, S. Roy, P. Awasthi, and M. Mohania

2007. Decision trees for entity identiï¬cation: Approximation algorithms and hardness results.
In Proceedings of the twenty-sixth ACM SIGMOD-SIGACT-SIGART symposium on Principles of
database systems, Pp. 53â€“62.

Chu, W., L. Li, L. Reyzin, and R. Schapire

2011. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth Interna-
tional Conference on Artiï¬cial Intelligence and Statistics, Pp. 208â€“214.

Dellinger, R. P., M. M. Levy, A. Rhodes, D. Annane, H. Gerlach, S. M. Opal, J. E. Sevransky, C. L.

Sprung, I. S. Douglas, R. Jaeschke, et al.
2013. Surviving sepsis campaign: international guidelines for management of severe sepsis and
septic shock, 2012. Intensive care medicine, 39(2):165â€“228.

Garcia, F. and S. M. Ndiaye

1998. A learning rate analysis of reinforcement learning algorithms in ï¬nite-horizon. In Proceed-
ings of the 15th International Conference on Machine Learning (ML-98. Citeseer.

Ghosh, D., S. Sharma, E. Hasan, S. Ashraf, V. Singh, D. Tewari, S. Singh, M. Kapoor, and D. Sengupta
2019. Machine learning based prediction of antibiotic sensitivity in patients with critical illness.
medRxiv, P. 19007153.

10

Golovin, D., A. Krause, and D. Ray

2010. Near-optimal bayesian active learning with noisy observations. In Advances in Neural
Information Processing Systems, Pp. 766â€“774.

Guillory, A. and J. Bilmes

2009. Average-case active learning with costs. In International conference on algorithmic learning
theory, Pp. 141â€“155. Springer.

Jacka, S. .

1991. Optimal stopping and the american put. Mathematical Finance, 1(2):1â€“14.

Johnson, A. E., T. J. Pollard, L. Shen, H. L. Li-wei, M. Feng, M. Ghassemi, B. Moody, P. Szolovits,

L. A. Celi, and R. G. Mark
2016. Mimic-iii, a freely accessible critical care database. Scientiï¬c data, 3:160035.

Kallus, N. and M. Santacatterina

2018. Optimal balancing of time-dependent confounders for marginal structural models. arXiv
preprint arXiv:1806.01083.

Kallus, N. and A. Zhou

2018. Confounding-robust policy improvement. In Advances in Neural Information Processing
Systems 31, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
eds., Pp. 9269â€“9279. Curran Associates, Inc.

Kanjilal, S., M. R. A. Sater, M. Thayer, G. K. Lagoudas, S. Kim, P. C. Blainey, and Y. H. Grad

2018. Trends in antibiotic susceptibility in staphylococcus aureus in boston, massachusetts, from
2000 to 2014. Journal of clinical microbiology, 56(1):e01160â€“17.

Kosaraju, S. R., T. M. Przytycka, and R. Borgstrom

1999. On an optimal split tree problem. In Workshop on Algorithms and Data Structures, Pp. 157â€“
168. Springer.

Lattimore, T. and C. SzepesvÂ´ari

2020. Bandit algorithms. Cambridge University Press.

Lewis, D. D. and W. A. Gale

1994. A sequential algorithm for training text classiï¬ers. In SIGIRâ€™94, Pp. 3â€“12. Springer.

Liao, P., K. Greenewald, P. Klasnja, and S. Murphy

2020. Personalized heartsteps: A reinforcement learning algorithm for optimizing physical activity.
Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(1):1â€“22.

Maes, F., L. Wehenkel, and D. Ernst

2012. Meta-learning of exploration/exploitation strategies: The multi-armed bandit case. In
International Conference on Agents and Artiï¬cial Intelligence, Pp. 100â€“115. Springer.

Makar, M., F. D. Johansson, J. Guttag, and D. Sontag

2020. Estimation of utility-maximizing bounds on potential outcomes. In International Conference
on Machine Learning. PMLR.

Murphy, S. A., L. M. Collins, and A. J. Rush

2007. Customizing treatment to the patient: Adaptive treatment strategies. Drug and alcohol
dependence, 88(Suppl 2):S1.

Nahum-Shani, I., M. Qian, D. Almirall, W. E. Pelham, B. Gnagy, G. A. Fabiano, J. G. Waxmonsky,

J. Yu, and S. A. Murphy
2012. Experimental design and primary data analysis methods for comparing adaptive interventions.
Psychological methods, 17(4):457.

NCCMH

2010. National Collaborating Centre for Mental Health (UK). Depression: the treatment and
management of depression in adults (updated edition).

11

Pardo, F., A. Tavakoli, V. Levdik, and P. Kormushev

2018. Time limits in reinforcement learning. In International Conference on Machine Learning,
Pp. 4045â€“4054.

Pearl, J.

2009. Causality. Cambridge university press.

Precup, D.

2000. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty
Publication Series, P. 80.

Rivest, R. L.

1987. Learning decision lists. Machine learning, 2(3):229â€“246.

Robins, J. M., M. A. Hernan, and B. Brumback

2000. Marginal structural models and causal inference in epidemiology.

Rosenbaum, P. R. et al.

2010. Design of observational studies, volume 10. Springer.

Rubin, D. B.

2005. Causal inference using potential outcomes: Design, modeling, decisions. Journal of the
American Statistical Association, 100(469):322â€“331.

Singh, J. A., K. G. Saag, S. L. Bridges Jr, E. A. Akl, R. R. Bannuru, M. C. Sullivan, E. Vaysbrot,

C. McNaughton, M. Osani, R. H. Shmerling, et al.
2016. 2015 american college of rheumatology guideline for the treatment of rheumatoid arthritis.
Arthritis & rheumatology, 68(1):1â€“26.

Spellberg, B., R. Guidos, D. Gilbert, J. Bradley, H. W. Boucher, W. M. Scheld, J. G. Bartlett,

J. Edwards Jr, and I. D. S. of America
2008. The epidemic of antibiotic-resistant infections: a call to action for the medical community
from the infectious diseases society of america. Clinical infectious diseases, 46(2):155â€“164.

Sutton, R. S., A. G. Barto, et al.

1998. Introduction to reinforcement learning, volume 2. MIT press Cambridge.

Thomas, P. and E. Brunskill

2016. Data-efï¬cient off-policy policy evaluation for reinforcement learning. In International
Conference on Machine Learning, Pp. 2139â€“2148.

WHO

1978. International classiï¬cation of diseases : Ninth revision, basic tabulation list with alphabetic
index.

Zhang, J. and E. Bareinboim

2019. Near-optimal reinforcement learning in dynamic treatment regimes. In Advances in Neural
Information Processing Systems, Pp. 13401â€“13411.

12

Supplementary material for: Learning to search
efï¬ciently for causally near-optimal treatments

Samuel HËšakansson
University of Gothenburg
samuel.hakansson@gu.se

Viktor Lindblom
Chalmers University of Technology
viklindb@student.chalmers.se

Omer Gottesman
Brown University
omer gottesman@brown.edu

Fredrik D. Johansson
Chalmers University of Technology
fredrik.johansson@chalmers.se

A Proofs of theorems

A.1 Proof of Lemma 1 (Stationarity)

Lemma S1 (Lemma 1 restated). Let I be a permutation of the sequence (1, ..., s). Then, for our
causal graph under Assumption 2, for b âˆˆ A,

p(Y (b) | X, As = a, Y s = y) = p(Y (b) | X, As = (aI(1), ..., aI(s)), Y s = (yI(1), ..., yI(s)))

Proof. Let h = (x, (a1, y1), ..., (as, ys)). Let Ï€ be a permutation of 1, ..., s and Ï€(r) the index
assigned to r. We use the short-hands p(a) = p(A = a), p(A | b) = p(A | B = b), etc.

p(Y (a) | Hs = hs, As = as)

stationarity

=

=

=

=

=

=

=

p(Ys(a), hs, as)
p(hs, as)
z p(Ys(a), hs, as, z)
(cid:80)
z p(hs, as, z)

(cid:80)

r p(yr | hr, ar, z)p(ar | hr, z)p(z)

(cid:80)

(cid:80)

(cid:80)

(cid:80)

(cid:80)

(cid:80)

z p(Ys(a) | hs, as, z)p(as | hs, z)p(hs | z)p(z)
z p(as | hs, z)p(hs | z)p(z)
z p(Ys(a) | hs, as, z)p(as | hs, z) (cid:81)
(cid:81)

(cid:80)
z p(Ys(a) | as, z)p(as | hs) (cid:81)
z p(as | hs) (cid:81)

z

r p(yr | hr, ar, z)p(ar | hr, z)p(z)

r p(yr | ar, z)p(ar | hr)p(z)

r p(yr | ar, z)p(ar | hr)p(z)

(cid:80)
z p(Ys(a) | z) (cid:81)
(cid:81)
z p(Y (a) | z) (cid:81)
(cid:81)

(cid:80)
z

(cid:80)
z

r p(yr(ar) | z)p(z)

r p(yr(ar) | z)p(z)

r p(YÏ€(r)(ar) = yr | z)p(z)

r p(YÏ€(r)(ar) = yr | z)p(z)

prob. laws

expand

expand history

As âŠ¥âŠ¥ Z | Hs

cancel terms

stationarity

Since the last expression is invariant to Ï€, the result follows.

A.2 Proof of Theorem 1 (Identiï¬ability)

Theorem S1 (Theorem 1 restated). Under Assumptions 1â€“4, the stopping statistic Ï(h) in (2) and
(cid:15), Î´-optimality are identiï¬able from the observational distribution p(X, T, A, Y ). In particular, for

13

any time step s with history hs, let h(I)s = (x, aI(1), ..., aI(s), yI(1), ..., yI(s)) be an arbitrary
permutation of hs. Then, for any sequence of untried (future) actions as+1:k = (as+1, ..., ak) âˆˆ
S(Aâˆ’hs ) with h(I)r the continued history at time r > s corresponding to as+1:k and ys+1:k,

Ï(hs) =

(cid:88)

1 [max(y) > Âµ(hs) + (cid:15)]

ys+1:kâˆˆY kâˆ’s
where Âµ(hs) = max(a,y)âˆˆhs y.

k
(cid:89)

r=s+1

p(Yr = yr | Ar = ar, Hr = h(I)râˆ’1) . (S1)

Proof. Fix any history h = (x, (a1, y1), ..., (as, ys)) âˆˆ H with s = |h|, any time points q, r âˆˆ [k],
any a âˆˆ A and let a âˆˆ S(A) such that the subsequence as = (a1, ..., as) coincides with h. Then, by
Assumption 2, we have

Yr(a) = Yq(a) = Y (a) and max
a(cid:54)âˆˆh

Y (a) =

k
max
r=s+1

Yr(ar) .

Below, we sum over sequences of outcomes ys+1:k = (ys+1, ..., yk) âˆˆ Y kâˆ’s and refer to the history
hr for r > s. Here, hr = (x, (a1, y1), ..., (ar, yr)) is a sequence of both observed actions and
outcomes (corresponding to the sub-sequence hs âŠ† hr) and unobserved ones. By deï¬nition, we have
for any sequence of actions a âˆˆ S(A) according to the above, for any Âµ âˆˆ Y

ÏÂµ(hs) =

(cid:88)

ys+1:kâˆˆY kâˆ’s

p([Y (as+1), . . . , Y (ak)] = ys+1:k | Hs = hs)1[max(ys+1:k) > Âµ]

=

=

(cid:88)

1 (cid:2)max(ys+1:k) > Âµ(cid:3)

ys+1:kâˆˆY kâˆ’s

(cid:88)

1 (cid:2)max(ys+1:k) > Âµ(cid:3)

ys+1:kâˆˆY kâˆ’s

k
(cid:89)

r=s+1

k
(cid:89)

r=s+1

p(Yr(ar) = yr | Hrâˆ’1 = hrâˆ’1)

p(Yr = yr | Ar = ar, Hrâˆ’1 = hrâˆ’1) .

In the second step we apply Assumption 2 (stationarity) and in the third Assumptions 1â€“Assumptions 4
(consistency, sequential ignorability). Finally, from ignorability and stationarity, we have for any
permutations h(I)s,

ÏÂµ(hs) =

(cid:88)

1 (cid:2)max(ys+1:k) > Âµ(cid:3)

ys+1:kâˆˆY kâˆ’s

k
(cid:89)

r=s+1

p(Yr = yr | Ar = ar, Hrâˆ’1 = h(I)râˆ’1) .

Doing so, we obtain the result in (4). In solving (1), we only need to evaluate Ï(hs) for histories with
positive support under pÏ€. Assumption 3 (positivity) ensures that there exists at least one permutation
a âˆˆ S(Aâˆ’hs) such that p(As+1:k = a | Hs = hs). This in turn implies identiï¬ability.

A.3 Bounds on stopping criterion

Theorem S2. For any threshold Âµ âˆˆ Y and history h âˆˆ H, we have under Assumption 2,

[p (Y (a) > Âµ | h)]

max
a(cid:54)âˆˆh
(cid:125)
(cid:123)(cid:122)
(cid:124)
Used for less conservative stopping

â‰¤ p

(cid:124)

(cid:18)

max
a(cid:54)âˆˆh

(cid:19)

Y (a) > Âµ | h

â‰¤

(cid:123)(cid:122)
=: ÏÂµ(h)

(cid:125)

(cid:88)

p (Y (a) > Âµ | h)

a(cid:54)âˆˆh
(cid:124)

(cid:123)(cid:122)
Used for more conservative stopping

(cid:125)

(S2)

Proof. Let Aâˆ’h = {a âˆˆ A : a (cid:54)âˆˆ h}. We start with the upper bound. By deï¬nition
{y âˆˆ Y |Aâˆ’h| : ya > Âµ}

{y âˆˆ Y |Aâˆ’h| : max(y) > Âµ} =

(cid:91)

Hence, by Booleâ€™s inequality,

aâˆˆAâˆ’h

(cid:18)

p

max
a(cid:54)âˆˆh

Y (a) > Âµ | H = h

â‰¤

(cid:19)

(cid:88)

(cid:88)

p (Y (Aâˆ’h) = y | H = h)

aâˆˆAâˆ’h
(cid:88)

aâˆˆAâˆ’h

=

yâˆˆY |Aâˆ’h | : y(a)>Âµ

p (Y (a) > Âµ | h) .

14

For the lower bound, the argument is equally straight-forward.

(cid:18)

p

max
a(cid:54)âˆˆh

Y (a) > Âµ | H = h

=

(cid:19)

(cid:88)

y : max(y)>Âµ

p (Y (Aâˆ’h) = y | H = h)

â‰¥ max
aâˆˆAâˆ’h

= max
aâˆˆAâˆ’h

(cid:88)

p (Y (Aâˆ’h) = y | H = h)

y : y(a)>Âµ
p (Y (a) > Âµ | H = h) .

A.4 Proof of Theorem 2

We restate the following assumption and Theorem 2 for convenience.
Assumption S1. A random variable U has Î±-bounded propensity sensitivity relative to H if for all
u âˆˆ U, h âˆˆ H, with s = |h| and a âˆˆ Akâˆ’s, for some Î± â‰¥ 1, with As+1:k = (As+1, ..., Ak),

1
Î±

â‰¤

Pr[As+1:k = a | H = h]
Pr[As+1:k = a | U = u, H = h]

â‰¤ Î± .

Theorem S3 (Theorem 2 restated). Given is that Assumption S1 (bounded propensity) holds for
H, U with sensitivity parameter Î± â‰¥ 1 and Assumption 4 (ignorability) holds for all s âˆˆ [k] w.r.t.
confounders (Hs, U ). Let Yr be the (hypothetical) outcome of treatment Ar at time r = s + 1, ..., k.
Then, for any history h âˆˆ Hs and the set of treatments a = A \ A(h), it holds that

Pr[

k
max
r=s+1

Yr > Âµ | As+1:k = a, Hs = h] â‰¤

Î´
Î±

=â‡’ Pr[max
aâˆˆa

Y (a) > Âµ | Hs = h] â‰¤ Î´ .

Proof. We have by deï¬nition, where y > Âµ applies element-wise,

Pr[max
aâˆˆa

Pr[max

i

Y (a) > Âµ | H = h] =

(cid:88)

y:y>Âµ

Pr[Y (a) = y | H = h]

Yi > Âµ | A = a, H = h] =

(cid:88)

y:y>Âµ

Pr[Y = y | A = a, H = h]

Then, marginalizing over the unobserved confounder U and conditioning on H,

Pr[max
aâˆˆa

Y (a) > Âµ | H = h] =

=

(cid:88)

y:y>Âµ
uâˆˆU
(cid:88)

y:y>Âµ
uâˆˆU

Pr[Y (a) = y | h, U = u]p(U = u | h)

Pr[Y = y | h, u, a]p(u | h)

where the last equality follows from ignorability w.r.t. H, U . Applying the same steps to
Pr[maxi Yi > Âµ | A = a], we get

Pr[max

i

Yi > Âµ | h, A = a] =

(cid:88)

y:y>Âµ
uâˆˆU

Pr[Y = y | h, u, a]p(u | a, h)

We ï¬nd that

Y (a) > Âµ | h] âˆ’ Pr[max

i

Yi > Âµ | h, A = a]

Pr[y | h, u, a] (p(u | h) âˆ’ p(u | a, h))

Pr[y | h, u, a]p(u | a, h)

(cid:18) p(u | h)
p(u | a, h)

(cid:19)

âˆ’ 1

= (âˆ—)

Pr[max
aâˆˆa
(cid:88)

=

y:y>Âµ
uâˆˆU

(cid:88)

y:y>Âµ
uâˆˆU

=

15

By Bayes rule, we have

and so,

p(u | h)
p(u | a, h)

=

p(u | h)p(a | h)
p(a | u, h)p(u | h)

=

p(a | h)
p(a | u, h)

(âˆ—) =

(cid:88)

y:y>Âµ
uâˆˆU

Pr[y | h, u, a]p(u | a, h)

(cid:18) p(a | h)
p(a | u, h)

(cid:19)

âˆ’ 1

The result follows immediately from our Assumption S1, that 1
upper bound is needed.

Î± â‰¤ p(a|h)

p(a|u,h) â‰¤ Î±. In fact, only the

A.5 Proof of Theorem 3 (Correctness of dynamic programming)

Theorem S4 (Theorem 3 restated). Recall that H0 = (X, âˆ…, âˆ…). The policy maximizing (6), Ï€(h) =
arg maxa Q(h, a), is an optimal solution to (1) and its expected search time is E[T ] = EX [âˆ’V (H0)].

Proof sketch. Recall that

Î³(cid:15),Î´,Î±(h) := 1[Pr[max
a(cid:48)(cid:54)âˆˆh

Y (a(cid:48)) > Âµ(h) + (cid:15) | H = h] < Î´/Î±]

(S3)

Q(h, a) = r(h, a) + 1[a (cid:54)= STOP]

(cid:88)

yâˆˆY

p(Y (a) = y | h)

max
a(cid:48)âˆˆAâˆª{STOP}

Q(h âˆª {(a, y)}, a(cid:48)) ,

r(cid:15),Î´,Î±(h, a) =

(cid:40) âˆ’âˆ,
0,
âˆ’1,

if a = STOP, Î³(cid:15),Î´,Î±(h) = 0
if a = STOP, Î³(cid:15),Î´,Î±(h) = 1
if a (cid:54)= STOP

.

(S4)

(S5)

and V (h) = maxa,c Q(h, a).
By deï¬nition, any policy that achieves a ï¬nite expected reward EH0 [V (H0)] satisï¬es the stopping
criterion, and is therefore a feasible solution to (1). Furthermore, any time search is terminated
(a = STOP or Aâˆ’h = âˆ…), the expected sum of rewards for a sequence is equal to minus the number
of steps spent until the sequence terminates. The sequence is optimal if it terminates as soon
as an (cid:15), Î´-optimal treatment is found. Thus, a policy with ï¬nite expected return that maximizes
(cid:3)
V (H0) = maxa Q(H0, a) is an optimally efï¬cient search policy for effective treatments.

A.6 Approximation ratio of greedy algorithms

The active learning problem concerns identiï¬cation of a hypothesis g âˆˆ G by iteratively performing
tests suggested by a policy (Guillory and Bilmes, 2009). The problem then amounts to ï¬nding a
policy Ï€ which selects tests A = A1, ..., AT , the results Y (A1), ..., Y (AT ) of which identify g with
probability 1, p(G = g | Y (A1), ..., Y (AT )) = 1. We consider now the case were a prior distribution
p(G, Y (1), ..., Y (k)) is known, as studied by (Guillory and Bilmes, 2009). A sequence of tests A
which identiï¬es g is associated with a cost c(A, G), and the objective is to ï¬nd Ï€ which minimizes
the expected cost over p,

We have the following result from the literature.

c(Ï€) = EG,Aâˆ¼Ï€[c(A, G)] .

Theorem S5 (Adapted from Theorem 4 of (Kosaraju et al., 1999)). There exists a greedy policy Ï€
such that for any p such that Y (a) : a âˆˆ A are deterministic given G,

where Ï€âˆ— = arg minÏ€(cid:48) c(Ï€(cid:48)) .

c(Ï€) â‰¤ c(Ï€âˆ—)O(log |G|)

16

This bound is matched by a lower bound by (Chakaravarthy et al., 2007) which states that it is
NP-hard to achieve an approximation ratio better than o(log |G|).

In the setting with Î´ = 0, our problem may posed as active learning where the hypothesis corresponds
to the maximum value of potential outcomes, G = min{g âˆˆ Y : p(maxa Y (a) > g) â‰¤ 0}. Once
this quantity is identiï¬ed, the stopping criterion may be determined immediately. However, under
this hypothesis, Y (a) are not deterministic given G and the results above do not apply. Golovin et al.
(2010) study the noisy case under the assumption that non-determinism in Y (a) is controlled by a
noise variable Î˜, i.e., that Y (A) = f (G, Î˜) for some deterministic function f .
Theorem S6 (Adapted from Theorem 3 in (Golovin et al., 2010) with uniform costs). Fix hypotheses
G, tests A and outcomes in Y, Fix a prior p(G, Î˜) and a function f : G Ã— supp(Î˜) â†’ Y |A| which
deï¬ne the probabilistic noise model. Let c(Ï€) denote the expected cost of Ï€ incurs to identify which
equivalence class G the outcome vector Y (AT ) belongs to. Let Ï€âˆ— denote the policy minimizing c(Â·),
and let Ï€ denote the adaptive policy implemented by the greedy algorithm EC2. Then,

c(Ï€) â‰¤ c(Ï€âˆ—)O(log |A| + log |supp(Î˜)|) .

In the case that all combinations of outcomes are feasible, log |supp(Î˜)| = |A| log |Y| and the bound
above is vacuous, since a trivial bound on the search time is |A|. When there is structure in potential
outcomes, supp(Î˜) may be much smaller. For example, if the moderating variable Z controls all
uncertainty in Y (a), given X, the bound reduces to O(log |ZX |) where ZX = {z âˆˆ Z : p(Z | X) >
0}, which may be signiï¬cantly smaller than |A| log |Y|.

A.7 Model-free RL and CDP are not equivalent

Let max(Â·,y)âˆˆh y represent the best outcome so far at history h, with s = |h| and Î» > 0 a parameter
trading off early termination and high outcome. Now, consider the reward function rmodel-free
(h, a)
following history h âˆˆ H deï¬ned below.

Î»

rmodel-free
Î»

(h, a) =

(cid:26) 0,

a (cid:54)= STOP
max(Â·,y)âˆˆh y âˆ’ Î»|h|, a = STOP.

and the policy maximizing the expected sum of rewards
(cid:34) k

Ï€âˆ—,model-free
Î»

= arg max

Ï€

Eh,aâˆ¼Ï€

(cid:35)

rmodel-free
Î»

(hs, as)

.

(cid:88)

s=1

(S6)

(S7)

Now consider the greedy policy maximizing the Q-function deï¬ned by

Î»

(h, a) +

Q(h, a) = Eh(cid:48)|h,a[rmodel-free

max
a(cid:48)âˆˆAâˆ’hâˆª{STOP}
For readers familiar with reinforcement learning, it is easy to see that policy maximizing Q deï¬ned
above also maximizes the expected sum of rewards given by (S6). Below, we prove that this algorithm
does not in general solve (1).
Theorem S7. There are instances of (1) (main problem), speciï¬ed by a distribution p and parameters
(cid:15), Î´, such that the solutions to (1) and (S7) are distinct for every choice of Î» > 0.

Q(h(cid:48), a(cid:48)) | Hs = h, As = a] .

(S8)

Proof. Consider a context-less setting with two actions A = {a, b} with the following potential
outcomes: p(Y (a) = 1.0) = 1/2, p(Y (a) = 0.5) = 1/2 and p(Y (b) = 0.5 + (cid:15)) = 1. In this
scenario, having observed nothing, the probability that action b yields a higher outcome than a is 1/2.
Hence, for Î´ = 0.5, CDP always prefers to start with action b and end immediately. Now, consider
NDL, which minimizes the expected return with the reward function,

(cid:26) 0,

r(h, a) =

a (cid:54)= STOP
max(Â·,y)âˆˆh y âˆ’ Î»|h|, a = STOP

where s indicates the stop action and max(Â·,y)âˆˆh y represents the best outcome so far at history h
and Î» > 0. The Q-function is in (S8). NDP computes this recursively and uses the policy which
maximizes it. Under the version of this problem with (cid:15) < 0.25, we can show that there is no Î» > 0
such that Q(âˆ…, b) > Q(âˆ…, a). We give the map of Q below under this assumption.

For Î» > (cid:15), Q(âˆ…, a) = 0.75 âˆ’ Î» and Q(âˆ…, b) = max(0.5 + (cid:15) âˆ’ Î», 0.75 + (cid:15)/2 âˆ’ 2Î») < Q(âˆ…, a). For
0 < Î» â‰¤ (cid:15), we have Q(âˆ…, a) = 0.75 âˆ’ 1.5Î» + (cid:15)/2 > Q(âˆ…, b) by the assumption (cid:15) < 0.25. Hence,
NDL would, for any Î» prefer action a. However, for Î´ = 0.5, CDP would prefer action b. Thus, for
Î´ = 0.5, there is no Î» which make these equivalent.

17

h

A2
â€“
â€“
â€“
b
b
â€“
â€“
â€“
â€“
â€“

Y1
1.0
0.5
0.5 + (cid:15)
1.0
0.5
1.0
0.5
0.5 + (cid:15)
â€“
â€“

A1
a
a
b
a
a
a
a
b
â€“
â€“

Y2
â€“
â€“
â€“
0.5 + (cid:15)
0.5 + (cid:15)
â€“
â€“
â€“
â€“
â€“

a

STOP

STOP

STOP

STOP

STOP
b
b
a
a
b

Q(h, a)

1.0 âˆ’ Î»
0.5 âˆ’ Î»
0.5 + (cid:15) âˆ’ Î»
1.0 âˆ’ 2Î»
0.5 + (cid:15) âˆ’ 2Î»
1.0 âˆ’ 2Î»
0.5 + (cid:15) âˆ’ 2Î»
(1.0âˆ’2Î»)+(0.5+(cid:15)âˆ’2Î»)
2
(1.0âˆ’Î»)+max(0.5âˆ’Î»,0.5+(cid:15)âˆ’2Î»)
2
max(0.5 + (cid:15) âˆ’ Î», (1.0âˆ’2Î»)+(0.5+(cid:15)âˆ’2Î»)

2

)

B Historical smoothing and function approximation

The number of possible combinations (histories) grows exponentially with the number of actions,
k = |A|. As a result, it is very probably that certain combinations of histories h and actions a are
never observed in practice. We consider two solutions to this: historical smoothing and function
approximation. Historical smoothing is used in the discrete case

by estimating the probability p(Y (a) = y | Hsâˆ’1 = h) using a weighted average of outcomes
for observations (h, a, y) and observations for subsequences (h(cid:48), a, y) where h(cid:48) âŠ† h. Function
approximation imputes Ë†p(Y (a) = y | Hsâˆ’1 = h) using a regression estimator trained on all
observations. We expand on these approaches in Appendix B.

B.1 Historical smoothing

Consider estimating the function p(Y (a) | H = h) in the discrete case. Under the stationarity
assumption, Assumption 2, it is sufï¬cient to represent the history in terms of indicators for tried
treatments, {Ba : a âˆˆ A} such that Ba âˆˆ {0, 1}, and observed outcomes of these actions. Hence,
p(Y (a) | H = h) may be represented by a table of dimensions |Y| Ã— ({0, 1} Ã— |Y|)|A|. Clearly,
even under this representation, the number of possible histories grows exponentially with the number
of actions. For this reason, for moderate to high numbers of actions, it will be unlikely to observe
samples for each cell of this table.

To obtain an estimate even in cases with high dimensionality, we use historical smoothing based on
a prior. In the discrete case, we may view the distribution of the outcomes Y (a) for a treatment a
following history h as a categorical distribution. We impose a Dirichlet prior on this distribution
and use the posterior distribution in estimating the stopping statistic Ï and in policy optimization.
A Dirichlet prior for p(Y (a) | H = h) is speciï¬ed by pseudo-counts Î²1(a, h), ..., Î²|Y|(a, h). The
ny(a,h)+Î²y(a,h)
y(cid:48) ny(cid:48) (a,h)+Î²y(cid:48) (a,h) , where ny(a, h) is equal to the number of samples
posterior parameters are then
where Y (a) = y following history h. In this work, we consider two different priors Î².

(cid:80)

Historical prior (kernel smoothing) The historical prior assumes that the conditional outcome
distribution changes slowly with the number of past observations. The prior itself is a weighted
average of the outcome probability at all possible previous histories,

Î²y(a, h) =

(cid:88)

h(cid:48)âŠ‚h

w(h, h(cid:48)) Â· Ë†p(Y (a) | H = h(cid:48)),

(S9)

where the weight of the probability given by a shorter history is determined by its similarity to h,

w(h, h(cid:48)) =

eâˆ’(|h|âˆ’|h(cid:48)|âˆ’1)2
|h| Â· 2|hâˆ’hi|âˆ’1

.

(S10)

Uninformed prior The uninformed prior assigns a small uniform value to all Î².

18

B.2 Function approximation

Observations for the ith subject are denoted x(i), a(i)
s . To use function approximation, we
t
ï¬t a single function f , acting on a representation of history Ï†(h) to estimate p(Y (a) | H = h) by
solving the following problem,

, y(i)
t

, a(i)

min
f âˆˆF

n
(cid:88)

ti(cid:88)

i=1

s=1

L(f (h(i)

s , a(i)

s ), y(i)

s ) ,

(S11)

for an appropriately chosen function class F and loss function L. In the discrete settings considered
in the paper, we use the logistic (cross-entropy) loss which leaves the solution to (S11) a probabilistic
classiï¬er, or estimate of p(Y (a) | H = h) for all a, h.

C Additional experimental results

Below follow additional details and results from the experiments. All experiments were implemented
in Python and run on standard laptop computers. Each experiment on the synthetic DGP took less
than a handful of hours to ï¬nish. For the antibiotics experiment, the overall time to produce the
results for all values of Î´ was 2 days.

C.1 Synthetic data generating process

We describe the datagenerating process (DGP) for the synthetic dataset used in Figure 2b and
additional results described below. Let oa(h) = 1[a âˆˆ h] and o(h) = [o1(h), ..., ok(h)](cid:62). The
moderator Z âˆˆ {0, 1}d and covariates X âˆˆ {0, 1}v are drawn according to

1. Z âˆ¼ Bernoulli(Î±)
2. X âˆ¼ Bernoulli(max(min(Î²Z, 0.98), 0.02)).

given a set of parameters Î± âˆˆ [0, 1]d, Î² âˆˆ [0, 1]vÃ—d drawn element-wise uniformly at random.

The action STOP is drawn at any point following the ï¬rst treatment with probability pSTOP = 0.1. To
emulate a closer-to-realistic policy, if not stopped, the next action is drawn according to a categorical
distribution with probabilities determined by the variable X and the dissimilarity of the new action A
to previous actions in H. Outcomes are drawn according to a categorical distribution with parameters
given by the pdf of a Cauchy random variable, itself with parameters depending on the variables X,
Z and A. For a full description of the data generating distribution, see Algorithm 1.

C.1.1 Additional results for the synthetic DGP

We present additional results for CDP, CG and NDP applied to the synthetic DGP described above.
Unless otherwise speciï¬ed, Î´ = 0.4, (cid:15) = 0, Î» = 0.35 and CDP and CG use the upper bound
approximation of the stopping criterion described in Appendix A.3 with historical smoothing ( H), as
described in Appendix B.

In Figure S1, we illustrate the mean efï¬cacy and search time (number of trials) as a function dataset
size, varying logarithmically from n = 50 to n = 75000 samples. We include the variance across
(cid:80)m
i=1 (xi âˆ’ Â¯x)2. This Figure is a different view of
m random seeds for the experiment, Ë†Ïƒ2 = 1
Figure 2b, where we clearly see that the efï¬cacy for most algorithms go up as data set size grows and
search time decreases. For NDP, as noted in Section 6, we see the opposite trend, however.

mâˆ’1

Figure S2 shows the trade-off between search time (number of trials) for different algorithms and
40 different values of Î´ âˆˆ [0, 1] with Î» = Î´ for n = 15000 samples, in the setting corresponding to
Figure 2b. In Figure S3, we give the corresponding comparison for using lower or upper bounds
in the estimation of the stopping criterion Ï, as described in Appendix A.3. Here, U refers to the
upper bound, L to the lower bound and E is â€œexactâ€ estimator, i.e. the empirical estimator of the
exact expression for the stopping criterion, Ï. At ï¬rst glance, the output of the different algorithms
using different bounds appear very similar. However, as we see in Figure S4, the trade-off induced by
a speciï¬c value of Î´ varies greatly depending on the estimation strategy. This is discussed also in
Section 6, where we note that the policy learned by NDP is very sensitive to the setting of Î».

19

(a)

(b)

Figure S1: Efï¬cacy and time over different sized training sets for the synthetic DGP. Interval widths
represent the variance across 50 realizations.

Figure S2: Efï¬cacy and search time (number of trials) for different policy optimization methods
operating the same model (historical smoothing, upper bound).

(a) Constrained Dynamic Programming algorithm.

(b) Constrained Greedy algorithm.

Figure S3: Results using estimates of the stopping criterion based on the upper ( U) and lower bounds
( L) described in Appendix A.3, as well as the no-bound (exact) estimate ( E) for the CDP and CG
algorithms with Î´ varying linearly in [0, 1].

20

102103104105Data set size0.650.700.750.800.85EfficacyCDP_HCDP_FCG_HCG_FNDP_HNDP_F102103104105Data set size1.21.41.61.82.02.2Mean search timeCDP_HCDP_FCG_HCG_FNDP_HNDP_F1.001.251.501.752.002.252.502.75Mean time0.700.750.800.850.900.951.00EfficacyCDPCGNGNDP1.01.21.41.61.82.02.2Mean time0.40.50.60.70.80.91.0EfficacyCDP_UCDP_LCDP_E1.01.21.41.61.82.02.2Mean time0.700.750.800.850.900.951.00EfficacyCG_UCG_LCG_E(a) Efï¬cacy and search time (number of trials) for vary-
ing approximations used in estimating the stopping
criterion, with the upper bound, in the CDP algorithm.
U stands for using a uniform prior to ï¬ll in missing
valus. H is the historical kernel smoothing described
in Appendix B. F refers to function approximation
and T the result for using the true model.

(b) Efï¬cacy and search time (number of trials) when
using different bounds on the stopping criterion Ï in
the CDP algorithm. U stands for using the upper
bound, L for the lower bound and E for the exact
(no bound) estimate of Ï.

Figure S4: Efï¬cacy and mean search time (number of trials), varying Î´ in [0, 1].

C.2 Antibiotic resistance dataset

Below, we give additional information on the antibiotic resistance dataset compiled from MIMIC-III.

To gather a cohort for which a consistent set of culture tests had all been performed for every patient,
the set of organisms were restricted to a small subset. This selection was made based on overall
prevalence in the data as well as the co-occurrence with common antibiotic culture tests. The selected
organisms and antibiotics are listed below.

Selected (bacterial) microorganisms:

â€¢ Escherichia Coli (E. coli)
â€¢ Pseudomonas aeruginosa
â€¢ Klebsiella pneumoniae
â€¢ Proteus mirabilis

Selected antibiotics:

â€¢ Ceftazidime
â€¢ Piperacillin/Tazo
â€¢ Cefepime

21

(a)

(b)

Figure S5: Efï¬cacy and mean search time over different values of Î´ on the antibiotic resistance data
set. The width of the plots represent the unbiased empirical sample variance across random splits.

(a)

(b)

Figure S6: Efï¬cacy and mean number of trials over different values of Î» for the Naive Dynamic
Programming algorithm. Variance is unbiased sample variance across random splits of the data. Î» is
perturbed by 0.0001 in order to avoid division by zero for Î» = 0.

â€¢ Tobramycin
â€¢ Gentamicin
â€¢ Meropenem

pending was also an â€œresultâ€ in MIMIC-III, there were few of these instances and they were removed.
Covariates X: Ages are divided into the four groups [0, 15], (15, 31], (31, 60], and (60, âˆ). The two
diseases are Infectious And Parasitic Diseases and Diseases Of The Skin And Subcutaneous Tissue as
classiï¬ed by ICD (WHO, 1978). The data was split in training and test 70/30 from 1362 patients
and patients with multiple organisms were not split between the sets. Patients who had taken any
antibiotic other than our chosen ones were not included in the data. Figure S5 uses the same data as
Figure 3b but is split by Î´ and variance is shown.

# of treatments
1
2
3
4
5

# of patients
860
340
137
22
3

22

0.00.20.40.60.81.00.960.970.980.991.00EfficacyCDPCGCG_FCDP_FNDPNDP_F0.00.20.40.60.81.01.151.201.251.301.35Mean search timeCDPCGCG_FCDP_FNDPNDP_F0.000.250.500.751.001.251.501.752.000.650.700.750.800.850.900.951.00EfficacyNDP_HNDP_F0.000.250.500.751.001.251.501.752.001.01.52.02.53.03.54.0Mean number of trialsNDP_HNDP_FAlgorithm 1: Generating distribution of actions and potential outcomes
Input: Weight parameter wx (default value 1)
Input: Number of outcomes ny
Input: Uniform stopping probability pSTOP

Generating parameters:
u1, u2 âˆ¼ N (0kÃ—(1+v+d), 1)
u2 â† |u2|
for i â† 2 to v + 1 do

u1(Â·, i) â† u1(Â·, i) Â· wx
u2(Â·, i) â† u2(Â·, i) Â· wx

end
Î· âˆ¼ N (0kÃ—(1+v+k), 1)
for a â† 1 to k do

i=1

1 (a) â† (cid:80)1+v+d
uâˆ’
2 (a) â† (cid:80)1+v+d
uâˆ’
1 (a) â† (cid:80)1+v+d
u+
2 (a) â† (cid:80)1+v+d
u+

i=1

i=1

i=1

1[uâˆ’
1[uâˆ’
1[uâˆ’
1[uâˆ’

1 (a, i) < 0]uâˆ’
2 (a, i) < 0]uâˆ’
1 (a, i) > 0]uâˆ’
2 (a, i) > 0]uâˆ’

1 (a, i)
2 (a, i)
1 (a, i)
2 (a, i)

end

Generating distribution of actions:
p(A = STOP) = pSTOP
for a, a(cid:48) âˆˆ {1, ..., k} do

âˆ†(a, a(cid:48)) â† (cid:107)u1(a) âˆ’ u1(a(cid:48))(cid:107)2

2 + (cid:107)u2(a) âˆ’ u2(a(cid:48))(cid:107)2
2

end
for h âˆˆ H do

v = [1; x; o(h)]
for a âˆˆ {1, ..., k} do

Ëœp(a) â† eÎ·(a,Â·)(cid:62)v for a(cid:48) âˆˆ h do
Ëœp(a) â† Ëœp(a) Â· âˆ†(a, a(cid:48))

end

end
for a âˆˆ {1, ..., k} do

p(A = a | h, A (cid:54)= STOP) â†

(cid:80)

Ëœp(a)
aâˆˆ{1,...,k} Ëœp(a)

end

end

Generating distribution of potential outcomes:
for x âˆˆ X , z âˆˆ Z do
for a â† 1 to k do
v â† [1; x; z]
y0(a) â† u1(a, Â·)(cid:62)v
y0(a) â† (nyâˆ’1)(y0(a)âˆ’uâˆ’
(u+
1 (a))
Î³(a) â† u1(a, Â·)(cid:62)v
Î³(a) â† (Î³(a)âˆ’uâˆ’
2 (a)âˆ’uâˆ’
(u+
for y â† 1 to ny do

2 (a))
2 (a))

1 (a)âˆ’uâˆ’

1 (a))

Ëœp(a, y) â† fcauchy(y; y0(a), Î³(a))

end
for y â† 1 to ny do

p(Y (a) = y | x, z) â† Ëœp(a,y)

(cid:80)ny

y=1 Ëœp(a,y)

end

end

end

23

