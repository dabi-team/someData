Machine Learning manuscript No.
(will be inserted by the editor)

Incorporating Symbolic Domain Knowledge into Graph
Neural Networks

Tirtharaj Dash · Ashwin Srinivasan ·
Lovekesh Vig

1
2
0
2

b
e
F
9
1

]

G
L
.
s
c
[

2
v
0
0
9
3
1
.
0
1
0
2
:
v
i
X
r
a

the date of receipt and acceptance should be inserted later

Abstract Our interest is in scientiﬁc problems with the following characteristics:
(1) Data are naturally represented as graphs; (2) The amount of data available is
typically small; and (3) There is signiﬁcant domain-knowledge, usually expressed
in some symbolic form (rules, taxonomies, constraints and the like). These kinds
of problems have been addressed eﬀectively in the past by symbolic machine learn-
ing methods like Inductive Logic Programming (ILP), by virtue of 2 important
characteristics: (a) The use of a representation language that easily captures the
relation encoded in graph-structured data, and (b) The inclusion of prior infor-
mation encoded as domain-speciﬁc relations, that can alleviate problems of data
scarcity, and construct new relations. Recent advances have seen the emergence
of deep neural networks speciﬁcally developed for graph-structured data (Graph-
based Neural Networks, or GNNs). While GNNs have been shown to be able to
handle graph-structured data, less has been done to investigate the inclusion of
domain-knowledge. Here we investigate this aspect of GNNs empirically by em-
ploying an operation we term vertex-enrichment and denote the corresponding
GNNs as V EGN N s. Using over 70 real-world datasets and substantial amounts
of symbolic domain-knowledge, we examine the result of vertex-enrichment across
5 diﬀerent variants of GNNs. Our results provide support for the following: (a)
Inclusion of domain-knowledge by vertex-enrichment can signiﬁcantly improve the
performance of a GNN. That is, the performance of V EGN N s is signiﬁcantly
better than GN N s across all GNN variants; (b) The inclusion of domain-speciﬁc
relations constructed using ILP improves the performance of V EGN N s, across all
GNN variants. Taken together, the results provide evidence that it is possible to
incorporate symbolic domain knowledge into a GNN, and that ILP can play an

T. Dash · A. Srinivasan
APP Centre for Artiﬁcial Intelligence Research
Department of Computer Science & Information Systems
BITS Pilani, K.K. Birla Goa Campus, Goa
E-mail: {tirtharaj, ashwin}@goa.bits-pilani.ac.in

L. Vig
TCS Innovation Labs, New Delhi

 
 
 
 
 
 
2

Tirtharaj Dash et al.

important role in providing high-level relationships that are not easily discovered
by a GNN.

1 Introduction

Industrialising scientiﬁc discovery, in the manner demonstrated by the Robot Sci-
entist Project [1] uses machine learning programs as scientiﬁc assistants. At the
very least, this would appear to require machine learning methods that are able
to (a) cope with data that have some inherent structure, in the form of entities
and relations; and (b) construct good predictive models by eﬀectively drawing on
any existing scientiﬁc knowledge thought to be relevant. A really useful assistant
would have to do more. A wish-list would include identifying the best explanation
for a prediction based on what is known; suggesting hidden variables or mecha-
nisms which could improve the prediction; and proposing experiments to test the
hypotheses. The Robot Scientist Project showed ways to achieve each of these
in some measure with Inductive Logic Programming (ILP). Recent rapid gains
in neural-network technology suggest that deep networks could form the basis of
extremely powerful predictive models, which is clearly relevant to the construction
of an eﬀective scientiﬁc assistant. Here we investigate the performance of state-
of-the-art deep networks speciﬁcally designed to analyse graph-structured data. A
substantial number of applications addressed by ILP belong to this category of
data (see, for example, [2, 3, 4]). There are at least three good reasons to investi-
gate if graph neural networks, or GNNs, are able to incorporate domain-knowledge.
First, studies with ILP have repeatedly shown that inclusion of domain-knowledge
can make substantial diﬀerence to predictive performance. Furthermore, a recent
report on Artiﬁcial Intelligence (AI) for Science identiﬁes incorporating domain-
knowledge in AI as one of the three Grand Challenges facing the application of
AI [5]. Deep learning methods based on neural networks have not focused on this,
relying instead on their internal computational machinery to construct higher-level
concepts automatically from the raw data. The ILP experience suggests otherwise,
and we would like to know if this applies to GNNs. Second, symbolic encodings of
domain knowledge are both natural and ﬂexible ways of encoding prior knowledge.
ILP systems implemented as logic programs have been the pre-eminent form of
machine learning for using such knowledge. Despite extremely eﬃcient implemen-
tations of logic programming, the signiﬁcant world-wide eﬀort into the develop-
ment of deep learning tools that have resulted in highly eﬃcient implementations
that exploit the processing capabilities of graphics processing units (GPUs). A
GNN capable of including symbolic domain knowledge could provide an eﬃcient
way of constructing predictive models. Thirdly, to the best of our knowledge, GNN
applications to date have been restricted to simple node-and-edge features, and
have not attempted to encode any signiﬁcant domain-knowledge. The real-world
problems we examine in this paper have very extensive amounts of domain infor-
mation, resulting from many years of academic and industrial eﬀort into the use
of ILP.

In this paper, we restrict the investigation to the problem of prediction, which
we see as a necessary ﬁrst step in the development of automated scientiﬁc assis-
tants. Facilities for explanations and experiment-proposal using GNN models are
conceptually harder, are deferred to future work. We assess the use of domain-

Incorporating Symbolic Domain Knowledge into Graph Neural Networks

3

knowledge using a sample of over 70 datasets, containing over 200,000 data in-
stances. The datasets refer to problems in a broad category known as structure-
activity prediction. Each data instance is, therefore, a molecule, which is naturally
represented as a graph. 1 For this class of problems, we now have a suﬃciently
large body of domain-knowledge encoded in human-understandable symbolic rela-
tions. This allows us to perform a case-study on the inclusion of symbolic relations
by GNNs. The principal contributions of the paper are as follows:

– To the ﬁeld of graph neural networks, the paper presents a large-scale empirical
study using real-world datasets on the inclusion of domain-knowledge. To the
best of our knowledge, the number of graphs used and the number of relations
encoding domain-knowledge are the most extensive to date.

– To the ﬁeld of inductive logic programming, the paper demonstrates a con-
tinuing case for the usefulness of ILP on relational learning tasks, despite the
development of very eﬃcient deep neural networks speciﬁcally designed for a
speciﬁc form of relational data.

– To the ﬁeld of neuro-symbolic modelling, the technique of vertex-enrichment
described in the paper provides a simple but eﬀective way of incorporating
symbolic relations into graph-based neural networks.

The rest of the paper is organised as follows. In Section 2, we describe vertex-
enrichment in graphs and a set of practical considerations arising from the de-
veloped algorithms. In Section 3, we describe our aims, data and background
knowledge, the speciﬁcs of the methodology, and the obtained results. Section 4
lists related works, and Section 5 concludes the paper.

2 Graph Neural Networks (GNNs)

GNNs are primarily developed for learning from data represented as graphs. For
completeness, we include some basic deﬁnitions ﬁrst.

Deﬁnition 1 (Graphs) A graph G is a pair (V, E) where V is a set of vertices,
E is a set of edges and a subset of V × V . A graph is said to be undirected if for
every (vi, vj) ∈ E, (vj , vi) in E.

We will be concerned in this paper with undirected graphs. We note that for
such graphs, E can be represented more compactly as a set consisting of 1- or 2-
element subsets of V . We will return to this later, as we extend the consideration
to hypergraphs. For molecular graphs, of the kind considered here, self-loops do
not occur.

Example 1 Molecules as graphs. A benzene ring (shown below) can be represented
as a graph, in which vertices correspond to atoms and edges correspond to bonds [7].

1 In fact, GNNs were originally tested with molecular datasets [6].

4

Tirtharaj Dash et al.

The graph-representation of the molecule on the left is:

({1, 2, 3, 4, 5, 6}, {(1, 2), (2, 1), (2, 3), (3, 2), (3, 4), (4, 3), (4, 5), (5, 4), (5, 6), (6, 5),

(6, 1), (1, 6)})

We will need the concept of the neighbourhood of a vertex in an undirected graph:2

Deﬁnition 2 (Neighbourhood) Given a graph G = (V, E), σ is a neighbourhood
function from V to 2V .

Example 2 One obvious deﬁnition of σ for an undirected graph (V, E) is σ(v) = {vi :
vi ∈ V, (v, vi) ∈ E}. For the graph in Example 1, σ(1) = {2, 6}, σ(2) = {1, 3}.

For the GNNs in this paper, we will need labelled undirected graphs.

Deﬁnition 3 (Graph Labellings) Let V be a set of vertex labels and E be a set of
edge labels. Then a vertex-labelling of a graph G = (V, E) is a function ψ : V → 2V
and an edge-labelling is a function (cid:15) : E → 2E .3

Example 3 The vertex labels of the graph given in Example 1 can be the atom-types
(Carbon, C), and edge labels can be the bond-types (single bond: 1, double bond: 2).
The label for the vertex 1 is ψ(1) = · · · = ψ(6) = {C}. The labelling for the edges are
(cid:15)((1, 2)) = (cid:15)((2, 1)) = {2}, (cid:15)((2, 3)) = (cid:15)((3, 2)) = {1} and so on.

Although not evident in this example, vertex- and edge-labels can have more than
one element (hence the mapping to 2V and 2E ). This will be necessary later.

We will use the term graph interchangeably to denote the tuple (V, E) or the
tuple (V, E, σ, ψ, (cid:15)). We are interested here in classifying graphs. That is, given a
set of class labels Y, we want to construct a function that maps a graph of the
form (V, E, σ, ψ, (cid:15)) to Y. A GNN is one such function that employs 2 higher-order
functions.

Deﬁnition 4 (Relabel) Given a graph (V, E, σ, ψ, (cid:15)). Let Relabel be a function
that returns a graph (V, E, σ, ψ(cid:48), (cid:15)(cid:48)), where the functions ψ(cid:48) and (cid:15)(cid:48) may be diﬀerent
to ψ and (cid:15).

A vectorisation function is used to map a graph as a real-valued vector.

Deﬁnition 5 (Vectorise) Let G denote the set of graph-tuples of the form (V, E, σ, ψ, (cid:15)).
A vectorisation of the graph-tuple is the result of applying a function V ec : G → (cid:60)d
(d ≥ 1).

A GNN is the composition of these functions, and some prediction function as

implemented by a neural network.

Deﬁnition 6 (GNN) Let N N : (cid:60)d → Y denote a neural network that maps a
real-valued vector to a set of class labels. Given a G = (V, E, σ, ψ, (cid:15)), GN N (G) =
N N (V ec(Relabel(G))).

Variations of GNNs result from changing the deﬁnitions of N N, V ec and Relabel.
Many diﬀerent deﬁnitions of the Relabel function have been proposed recently. We
defer the speciﬁc details of the GNN variants used here to Section 2.2.

2 Henceforth, by “graph” we will mean an undirected graph.
3 We do not commit here to any speciﬁc data structure that should be used to implement

the label set. This could be, for example, a Boolean-valued array of size |V|.

Incorporating Symbolic Domain Knowledge into Graph Neural Networks

5

2.1 Encoding n-ary Relations

GNNs, as we have described them so far, deal with node- and edge-labels in
an undirected graph, in which edges are sets of vertex-pairs. That is, the edges
represent a symmetric binary relation. However, for many real-world problems—
including the ones considered in this paper—we have access to domain-knowledge
which relate more than just pairs of vertices. For example, if a molecule is repre-
sented as a graph (with atoms as vertices, and an edge denoting a bond between
a pair of vertices), then a benzene-ring is a relation amongst 6 distinct vertices,
with some speciﬁc constraints on the vertices and edges. Here, we will consider
domain-knowledge to be a set of relations, each of which can be expressed as a
hypergraph.

Deﬁnition 7 (Hypergraphs) A hypergraph H is the pair (V, E(cid:48)), where V is a
set of vertices and E(cid:48) is a non-empty subset of 2V . Each element of E(cid:48) is called a
hyperedge.

Example 4 A hypergraph of the molecular graph given in Example 1 can be H =
({1, 2, 3, 4, 5, 6}, {{1, 2}, {3, 4, 5, 6}, {2, 4, 5}, {1, 2, 3, 4, 5, 6}}).

We note that since hyperedges are sets, there is no distinction between permu-
tations of vertices in a hyperedge. So, as deﬁned here, we will take hyperedges
as being undirected. Hypergraph labellings can be deﬁned similarly as before,
using a pair of functions for vertex- and edge-labels. We will reuse the nota-
tion ψ and (cid:15) for these functions, with annotations to clarify what is meant.
The neighboorhood relation σ is left unspeciﬁed here (one obvious deﬁnition is
σ(vi) = {vj : h ∈ E(cid:48), {vi, vj } ⊆ h}). In this paper, we are interested in n-ary
relations that can be expressed as hypergraphs.

Deﬁnition 8 (n-ary Relation as a Labelled Hypergraph) A n-ary relation R
deﬁned over vertices of a graph G = (V, E) is a hypergraph H = (V, E(cid:48)), and every
hyperedge h ∈ E(cid:48) has n elements from V . We will denote this as R(G) = H. Let
ψG denote a vertex-labelling over G and R/n denote the predicate-symbol for R.
With some abuse of notation, the vertex-labelling function for R(G) = H = (V, E(cid:48))
is as follows:

ψH (v) =

(cid:40)

ψG(v) ∪ {R/n}
∅

if ∃h ∈ E(cid:48)s.t. v ∈ h
otherwise

and the hyperedge-labelling function is:

(cid:15)H (h) = {R/n}

(h ∈ E(cid:48))

That is, the vertex-labelling of a vertex v in the hypergraph H is a set containing
the existing vertex-label of v in G augmented by the predicate-symbol R/n vertex-
label.

6

Tirtharaj Dash et al.

Example 5 Consider a relation for a Benzene ring:

Benzene(a1, a2, a3, a4, a5, a6) ←

Cycle(a1, a2, a3, a4, a5, a6) ∧
Aromatic(a1, a2, a3, a4, a5, a6).

One possible vertex-labelling is:

ψH (1) = · · · = ψH (6) = {C, Benzene/6}

(here, C denotes “carbon”). A hyperedge-labelling may contain:

(cid:15)H ({1, 2, 3, 4, 5, 6}) = {Benzene/6}

The extension to multiple relations, not all of the same arity, is straightforward.

Deﬁnition 9 (Multiple Relations as a Labelled Hypergraph) Let R1, . . . , Rk
(cid:48)). Then
be relations deﬁned on vertices of a graph G = (V, E), s.t. Ri(G) = (V, Ei
(cid:83) Ri(G) is the hypergraph H = (V, E(cid:48)) where E(cid:48) = (cid:83) Ei
(cid:48). The corresponding
labelling functions are:

and

ψH (v) =

(cid:15)H (v) =

(cid:91)

(cid:91)

ψHi (v)

(cid:15)Hi (v)

Example 6 In the molecular graph given below, there are two relations: Benzene/6
and P yrrole/5.

One possible vertex-labelling for this graph is:

ψH (1) = ψH (4) = ψH (5) = ψH (6) = {C, Benzene/6}
ψH (8) = ψH (9) = {C, P yrrole/5}
ψH (7) = {N, P yrrole/5}
ψH (2) = ψH (3) = {C, Benzene/6, P yrrole/5}

and a hyperedge-labelling is:

(cid:15)H ({1, 2, 3, 4, 5, 6}) = {Benzene/6}
(cid:15)H ({2, 7, 8, 9, 3}) = {P yrrole/5}

In principle, provided we are able to deﬁne a neighbourhood function σ for hyper-
graphs, the deﬁnition of GNNs in Defn. 6 does not change. We would however like
to use one of the standard GNN implementations described in the previous sec-
tion, which restricts graphs with 2-vertex edges, and edge-labels to singleton sets.
With some loss of information, we extract a suitable graph from a hypergraph.

Incorporating Symbolic Domain Knowledge into Graph Neural Networks

7

Deﬁnition 10 (Vertex-Enriched Graphs) Let G = (V, E) be a graph, with
neighbourhood function σ, vertex-labelling function ψ, and edge-labelling func-
tion (cid:15). Here, E is a subset of V × V . Let R = {R1, . . . , Rk} be a set of relations
deﬁned on G, and (cid:83) Ri(G) be the hypergraph H = (V, E(cid:48)) with vertex-labelling
function ψ(cid:48) as in Defn. 9. Then G(cid:48) = (V, E, σ, ψ(cid:48), (cid:15)) is called a vertex-enriched form
of G = (V, E, σ, ψ, (cid:15)). We denote this by V E(G, R) = G(cid:48).

Example 7 The molecular graph G for Example 6 is

G = ({1, 2, 3, 4, 5, 6, 7, 8, 9}, {(1, 2), (2, 1), · · · , (1, 6), (6, 1), (2, 7), (7, 2), · · · ,

(9, 3)(3, 9)})

A vertex-labelling of G is:

ψ(1) = · · · = ψ(6) = ψ(8) = ψ(9) = {C}

ψ(7) = {N }

The vertex-labelling of the vertex-enriched graph G(cid:48), after the inclusion of the relations
in Example 6 is:

ψ(cid:48)(1) = ψ(cid:48)(4) = ψ(cid:48)(5) = ψ(cid:48)(6) = {C, Benzene/6}
ψ(cid:48)(8) = ψ(cid:48)(9) = {C, P yrrole/5}
ψ(cid:48)(7) = {N, P yrrole/5}
ψ(cid:48)(2) = ψ(cid:48)(3) = {C, Benzene/6, P yrrole/5}

The edge-labelling and neighborhood functions do not change after relation-enrichment.

The vertex-enriched graph thus extends the vertex-labelling of a graph G, with
the vertex-labels from the hypergraph H obtained from relations R1, . . . , Rk de-
ﬁned on G. The resulting graph can be used directly by the implementations of
GNNs described in the appendix. We note that the process of vertex-enrichment
is a simpliﬁcation of the full relational information available. For example, in the
example above, if an atom (represented by a vertex in the molecular graph) is part
of more than 1 benzene ring, then its vertex-enrichment will only contain a single
entry for Benzene/6, indicating that it is part of 1 or more benzene rings.

Deﬁnition 11 (Vertex-Enriched GNN) Let G = (V, E, σ, ψ, (cid:15)), and Relabel, V ec
and N N be as before. Then, a Vertex Enriched GNN is V EGN N (G) = N N (V ec(Relabel(V E(G, R)))).

2.2 Practical Considerations

The GNN variants in this paper diﬀer in the Relabel operation, based on the
convolution procedure employed. In this work, we employ the following diﬀerent
convolution procedures:

1. Localised approximation to spectral graph convolution [8]: This is a spectral
method for graph convolution that uses convolutional aggregator. This is a
simple and well-behaved layer-wise propagation rule for neural network models
which operate directly on graphs.

8

Tirtharaj Dash et al.

2. Multi-scale graph convolution [9]: This convolution method can perform con-
volution operations using multiple-sized neighbourhoods (the authors call this
“higher order” graph convolution).

3. Graph convolution with attention [10]: This is a spatial method of graph con-
volution that uses an “attention” mechanism, that estimates the importance
of vertices in the neighbourhood of a vertex.

4. Sample-and-aggregate graph convolution [11]: Here the convolution procedure
samples from a distribution that is constructed from feature-vectors of vertices
in the neighbourhood of a vertex.

5. Graph convolution based on auto-regressive moving average [12]: This is a
convolution method that employs a polynomial function of the feature-vectors
in the neighbourhood of a vertex.

The Relabel operation also includes a pooling step after each convolution oper-
ation. Additional details are in Appendix A. In all cases, we have used a ﬁxed
vectorisation function V ec that is based on a readout mechanism, and N N refers
to a standard multi-layer perceptron (MLP).

We now elaborate on three practical issues arising from the use of Vertex-

Enriched GNNs:

1. The vertex-enriched graphs we obtain allow us to use standard forms of GNNs
(see Procedure 1). However, this comes with the limitation that we only change
the vertex-labellings. A GNN deﬁned directly on hypergraphs would have ac-
cess to more information than the vertex-enriched GNN, since the former would
retain the edge-labelling on hyperedges, and can have a richer deﬁnition of the
neighbourhood function. Recently, there have been some proposals of GNNs for
hypergraphs [13, 14, 15]. It is possible that these forms of GNNs may perform
better than Vertex-Enriched GNNs. We expect that the results in Section 3.4
will act as baseline for such comparisons.

2. Procedure 1 requires identiﬁcation of subgraphs of the original graph. That
is: for every relation Ri ∈ R, the corresponding hyperedge Hi is a subset of
vertices {v1, . . . , vn} ∈ V , such that (v1, . . . , vn) ∈ Ri. This step requires the
identiﬁcation of all subsets of vertices of the graph constituting hyperedge as
above. For a graph (V, E), this can, in the worst case require an examination of
(|V |
n ) combinations. Therefore, for arbitrary sized graphs and subgraphs, this is
computationally hard. In practice, we will be forced to impose bounds on the
size of Vs and on the size of the subgraph.

3. We have not described how the relations in R themselves are obtained. There
are two possibilities here. First, they are provided as prior information (back-
ground knowledge in ILP terminology). Secondly, the R provided as prior infor-
mation can be augmented by relations constructed automatically (see Proce-
dure 2). In this paper, the construction of new relations is done using an ILP
engine, by adapting the usual clause-construction procedure (see Appendix B.1
for an ILP-based implementation of LearnRels in Procedure 2).4

4 Usually, clauses constructed by an ILP engine are either used as part of a hypothesis, or
as features to construct a Boolean-vector representation of the data (“propositionalisation”).
Here, the clauses are not used in either of these roles, but as relations that augment the prior
knowledge available to the GNN.

Incorporating Symbolic Domain Knowledge into Graph Neural Networks

9

Procedure 1: (EnrichGraph) Vertex-Enrichment of a graph G, given a
set of relations R. The new label of a vertex includes all the relations of
which the vertex is part.

Data: Graph G = (V, E, σ, ψ, (cid:15)), a set of relations R = {R1, . . . , Rk}
Result: Vertex-Enriched Graph, G(cid:48) = (V, E, σ, ψ(cid:48), (cid:15))
Let ψ(cid:48) := ψ;
for Ri ∈ R do

Let Ri ⊆ V n;
Hi = {{v1, . . . , vn} : (v1, . . . , vn) ∈ Ri};
Let Vs = (cid:83)
Hj ;
for vj ∈ Vs do

Hj ∈Hi

ψ(cid:48)(vj ) := ψ(cid:48)(vj ) ∪ {Ri/n};

end

end
return (V, E, σ, ψ(cid:48), (cid:15));

Procedure 2: (AugmentRels) Augmentation of a set of relations R by
learning new relations from data.

Data: A graph G = (V, E, σ, ψ, (cid:15)); a set pre-classiﬁed instances E = {(Gi, yi) :

Gi = (Vi, Ei, σ, ψi, (cid:15)i) and yi ∈ Y}; a set of relations R = {R1, . . . , Rk}; and a
bound n on the number of new relations

Result: A vertex-enriched graph G(cid:48) = (V, E, σ, ψ(cid:48), (cid:15)) obtained from an augmentation

of R by at most n new relations obtained using E

Let R(cid:48) = LearnRels(R, E, n);
return EnrichGraph(G, R ∪ R(cid:48));

3 Empirical Evaluation

3.1 Aims

Our aims in this paper is to investigate the incorporation of background knowledge
by GNNs. Speciﬁcally, using the term Vertex-Enriched GNNs (VEGNNs) to denote
the inclusion of relations into GNNs (See Procedure 1), the experiments attempt
to answer to the following questions:

1. How do VEGNNs perform against standard GNNs? This compares GNNs with

and without the inclusion of domain-knowledge.

2. Can the performance of VEGNNs be improved by using symbolic learner with
access to the same domain-knowledge? This tests whether the computational
machinery of a GNN is suﬃcient to construct (representations of) the high-level
relationships needed for good prediction.

3.2 Materials

3.2.1 Data

The datasets are classiﬁcation problems arising in the ﬁeld of drug-discovery. We
have evaluated our GNNs on 73 real-world binary classiﬁcation datasets. Each

10

Tirtharaj Dash et al.

dataset represents an extensive drug evaluation eﬀort at the National Cancer In-
stitute (NCI)5. The datasets represent experimentally determined eﬀectiveness of
anti-cancer activity of a compound against a number of cell lines [16]. The datasets
correspond to the concentration parameter GI50, which is the concentration that
results in 50% growth inhibition. Some of the datasets have been used in vari-
ous data mining studies such as in a study involving the use of graph kernels in
machine learning [17].

# of Datasets

73

Avg. # of Molecules per
dataset (Graphs)
3032

Avg. # of Atoms per
molecule (Vertices)
24

Avg. # of Bonds per
molecule (Edges)
51

Fig. 1: Summary of datasets (Total number of instances is 221306)

3.2.2 Background Knowledge

The initial version of the background knowledge in this paper here was used in
[18, 19]. It is a collection of logic programs deﬁning almost 100 relations for var-
ious functional groups and ring structures in a chemical compound.6 The back-
ground knowledge consists of multiple hierarchies. However, we modiﬁed some
of the predicate deﬁnitions to avoid redundant computation and for tractability
to trade-oﬀ completeness for eﬃciency. For proprietary reasons, we are only able
to show the results of using the deﬁnitions, which are functional groups repre-
sented as functional group(CompoundID, Atoms, Length, Type) and rings described
as ring(CompoundID, RingID, Atoms, Length, Type). For eﬃciency, we have restricted
the deﬁnition of the ring relation to produce rings of maximum length 8. The ﬁrst
use of this new version of the background knowledge is reported in [20] where
we had also deﬁned three higher level relations to infer the presence of composite
structures from the presence of functional groups and rings in a compound. These
are: the presence of fused rings, connected rings and substructures. These relations
are deﬁned below.

has struc(CompoundId, Atoms, Length, Struc) This relation is T RU E if a compound
identiﬁed by CompoundId contains a structure Struc of length Length containing
a set of atoms in Atoms.

fused(CompoundId, Struc1, Atoms1, Struc2, Atoms2) This relation is T RU E if a com-
pound identiﬁed by CompoundId contains a pair of fused structures Struc1 and
Struc2 with Atoms1 and Atoms2 respectively (that is, there is at least 1 pair of
common atoms).

connected(CompoundId, Struc1, Atoms1, Struc2, Atoms2) This relation is T RU E if a
compound identiﬁed by CompoundId contains a pair structures Struc1 and
Struc2 that with Atoms1 and Atoms2 respectively that are not fused but con-
nected by a bond between an atom in Struc1 and an atom in Struc2.

5 https://www.cancer.gov/
6 The deﬁnitions used were originally developed for tackling industrial-strength problems by

the biotechnology company PharmaDM.

Incorporating Symbolic Domain Knowledge into Graph Neural Networks

11

The level of abstraction in the background knowledge is shown in Fig. 2. The
hierarchy available in functional groups and rings is shown in Fig. 3 and Fig. 4.

Fig. 2: Levels of abstraction in the background knowledge [20]

Fig. 3: Functional group hierarchy

3.2.3 Algorithms and Machines

The data used for this work and the set of symbolic relations (R) described in
Section 3.2.2 are written as Prolog facts. For generating the additional set of ILP
relations (R(cid:48)), we use Aleph [21] that takes the data and the background-knowledge
as input. This additional set of relations R(cid:48) further augments the existing relations
in R for our V EGN N (cid:48) studies. A logic program extracts a set of vertices in a graph
for which any symbolic relation Ri (∈ R or ∈ R(cid:48)) is T RU E. We use YAP compiler
for execution of this logic program.

12

Tirtharaj Dash et al.

Fig. 4: Ring hierarchy

The GNN variants used here are described in Appendix A. All the experiments
are conducted in Python environment. The GNN models have been implemented
by using the PyTorch Geometric library [22], which is a geometric deep learning
extension for PyTorch [23] and it provides graph pre-processing routines and makes
the deﬁnition of graph convolution easier to implement.

For all the experiments, we use a machine with Ubuntu (16.04 LTS) operating
system, and hardware conﬁguration such as: 64GB of main memory, 16-core Intel
Xeon processor, a NVIDIA P4000 graphics processor with 8GB of video memory.

3.3 Method

In all experiments, we refer to GNN variants as GN N 1,...,5. The correspond-
ing vertex-enriched versions are V EGN N 1,...,5. The GNN variants have 1 hyper-
parameter that determines the structure of the GNN (see Appendix A). We will
denote this by m and assume that it takes values from a ﬁxed-set of values M .

Experiment 1: GNNs vs. VEGNNs

For constructing the VEGNNs, we assume that we have access to a set of

domain relations R. The method used is as follows.

For each dataset D:
1. Let T r, V al, T e denote a train-validation-test split of the data D
2. For each of GN N 1,...,5 and V EGN N 1,...,5:

(a) Find the best value m∗ ∈ M using the performance on T r and V al
(b) Record the predictive performance on T e of the model constructed

using m∗

3. Compare the performance of GN N i against that of V EGN N i (i = 1, . . . , 5).

The following additional details are relevant:

– The relations in R are those described in Section 3.2.2.
– In our implementation, we use three graph convolution blocks and three pooling

blocks interleaving each other.

Incorporating Symbolic Domain Knowledge into Graph Neural Networks

13

– The convolution blocks can be of one of the ﬁve convolution variants listed
in Section 2.2. Due to the large-scale experimentation (number of datasets,
number of GNN variants), the various hyperparameters in convolution blocks
are set to default values in PyTorch Geometric library.

– The graph pooling block uses self-attention pooling [24] with pooling ratio of
0.5. We use a hierarchical pooling architecture that uses the readout mechanism
proposed by Cangea et al. [25]. The readout block aggregates node features to
produce a ﬁxed size intermediate representation for the graph. The ﬁnal ﬁxed-
size representation for the graph is obtained by element-wise addition (⊕) of
the three readout representations.

– The ﬁnal representation is then fed as input to a 3-layered MLP. We use a
dropout layer with ﬁxed dropout rate of 0.5 after ﬁrst layer of MLP. The
loss function is negative log-likelihood between the targets and the predictions
from the model. Further detail on the GNN architectures is provided in Ap-
pendix A.4.

– We select amongst two possible values of the structure hyperparameter m (8
and 128), corresponding to small and large amounts of convolution in the
convolutional-layers of the GNNs and VEGNNs;

– We use Adam [26] optimiser for training the GNNs (GN N1,...,5) and VEGNNs
(V EGN N1,...,5). The learning rate is 0.0005, weight decay parameter is 0.0001,
momentum factors are the default values of β1,2 = (0.9, 0.999).

– Maximum number of training epochs is 1000. The batch size is 128.
– We use an early-stopping mechanism [27] to obtain the optimal model after
training that can be used for evaluation on T e. The patience period for early
stopping is 50.

– Comparison of performance is done using the Wilcoxon signed-rank test, using

the standard implementation within MATLAB (R218b).

Experiment 2: VEGNNs with ILP-constructed Relations

Given a set of generic relations R, and some data, a VEGNN should, in princi-
ple, be able to construct new (domain-speciﬁc) relations across its internal layers.
That is, it may not be necessary to provide a VEGNN with anything more than
R. In this experiment, we investigate the extent to which this holds in practice, by
evaluating the eﬀects of augmenting R with higher-level relations learned by ILP.
The ILP procedure used to obtain these relations has been described elsewhere
(see Procedure 2). Our method is as follows.

For each dataset D:
1. Let T r, V al, T e denote the train-validation-test split of the data D
2. Let R(cid:48) denote a set of new relations obtained using an ILP engine with

access to R and T r ∪ V al

3. Let V EGN N 1,...,5 denote the VEGNNs obtained with R and V EGN N (cid:48)

1,...,5

denote the VEGNNs with R ∪ R(cid:48).
For each of V EGN N 1,...,5 and V EGN N (cid:48)
(a) Find the best value m∗ for the structure hyperparameter m, using T r

1,...,5:

and V al

(b) Record the predictive performance on T e of the model constructed

using m∗

14

Tirtharaj Dash et al.

4. Compare the performance of V EGN N i against that of V EGN N (cid:48)

i (i =

1, . . . , 5).

The following additional details are relevant:

– The relations in R are those described in Section 3.2.
– The construction of the V EGN N s is as in Experiment 1.
– The relations in R(cid:48) are obtained using the ILP engine Aleph [21] with hide-

and-seek sampling [28].

– We repeat the comparisons for |R(cid:48)| = 100, |R(cid:48)| = 500, and |R(cid:48)| = 1000.
– ILP-constructed relations can be complex, and involve several vertices. To en-
sure tractability, we restrict the computation to detecting a single hyperedge
(and not all hyperedges) corresponding to the ILP-constructed relation. This
results in a loss of information.

– As in Experiment 1, comparisons will be in the form of a Wilcoxon signed-rank

test, implemented within MATLAB (R2018b).

3.4 Results

The main results from the experiments are shown qualitatively in Fig. 5. The prin-
cipal ﬁndings from the tabulations are these: (a) Inclusion of domain-knowledge
into GNNs (that is, the use of vertex-enriched GNNs) results in an improvement
in predictive accuracy for all variants of GNN; and (b) The performance of vertex-
enriched GNNs can be improved further by augmenting the domain-relations with
additional relations constructed by an ILP engine.

We now examine the results in more detail: From Fig. 5, it is evident that
the performance of graph-based networks improves with the inclusion of domain-
knowledge. A quantitative tabulation of wins, losses and draws is in Fig. 6. These
results provide suﬃcient grounds to answer positively the primary research ques-
tion addressed in this paper, namely: do GNNs beneﬁt from the inclusion of
domain-knowledge?

Assuming that it is useful to provide a GNN with domain-knowledge, we can
then ask: are vertex-enriched GNNs suﬃciently powerful to compute automatically
any additional information needed for high predictive performance? The results in
Fig. 5 suggest that the answer to this is “no”, since it appears that the inclusion of
ILP-constructed relations makes a signiﬁcant diﬀerence. To understand this better,
we tabulate quantitative diﬀerences obtained as the number of ILP relations added
is increased. This is shown in Fig. 7. The plot in Fig. 5 uses 1000 ILP-relations
(the corresponding quantitative diﬀerences are the last column in Fig. 7).

Since the inclusion of even small numbers of ILP relations (100) seems to
improve performance of the VEGNN, it would appear that the internal represen-
tations within a VEGNN are of limited expressivity when compared to those con-
structed by ILP. In turn, the complete tabulation suggests that a hybrid VEGNN-
ILP learner is very likely to be better than just a VEGNN learner (and in turn, a
GNN learner).

We note that vertex-enrichment is only a vertex-related operation. It is rele-
vant to ask if there are any edge-related operations associated with the addition

Incorporating Symbolic Domain Knowledge into Graph Neural Networks

15

(a) GNN variant: GN N 1

(b) GNN variant: GN N 2

(c) GNN variant: GN N 3

(d) GNN variant: GN N 4

(e) GNN variant: GN N 5

Fig. 5: Qualitative comparison of graph-based neural networks. Here GNN refers
to the performance of the graph-based neural network without domain relations;
VEGNN refers to the performance of the network vertex-enriched with generic
domain relations shown in Section 3.2.2; and VEGNN(cid:48) refers to the performance
of the network vertex-enriched with the generic domain-relations and domain-
speciﬁc relations constructed by an ILP engine. Performance refers to predictive
(holdout-set) accuracy, and all performances are normalised against that of the
GNN. Further, the compounds are arranged in order of increasing GNN perfor-
mance: the apparent trend of high-to-low gains for VEGNN and VEGNN(cid:48) from
left to right are artifacts of this ordering. No signiﬁcance should also be attached
to the line joining the data points: this is only for visual clarity.

of domain-relations. Since these relations result in hyperedges, a natural edge-
operation is one of clique-expansion [29] of the domain-relations. That is, the orig-
inal graph is transformed to a new graph by the inclusion of all pairwise edges
between vertices in hyperedges entailed by the relations. We have investigated
this, but for reasons of space, do not include the results here. A summary of the
eﬀect of clique-expansion is: (a) By itself, clique-expansion of domain-relations is
not helpful; (b) Clique-expansion, in combination with vertex-enrichment does not
yield any clear advantage over vertex-enrichment alone across the GNN variants.

Datasets0.60.70.80.911.11.21.31.4GNNVEGNNVEGNN'Datasets0.60.70.80.911.11.21.31.4GNNVEGNNVEGNN'Datasets0.60.70.80.911.11.21.31.4GNNVEGNNVEGNN'Datasets0.60.70.80.911.11.21.31.4GNNVEGNNVEGNN'Datasets0.60.70.80.911.11.21.31.4GNNVEGNNVEGNN'16

Tirtharaj Dash et al.

GNN
Variant
GN N 1
GN N 2
GN N 3
GN N 4
GN N 5

Accuracy (V EGN N vs. GN N )
Higher/Lower/Equal (p-value)
48/14/11 (< 0.001)
48/19/6 (0.005)
53/11/9 (< 0.001)
54/12/7 (< 0.001)
43/19/11 (0.002)

Fig. 6: Quantitative comparison of GNN performance. Here GN N refers to the
graph-based neural network without domain-knowledge, and V EGN N refers to
the network vertex-enriched with the generic domain-knowledge described in Sec-
tion 3.2.2. The tabulations are the number of datasets on which V EGN N has
higher, lower or equal predictive accuracy on a holdout-set. Statistical signiﬁcance
is assessed by the Wilcoxon signed-rank test.

Accuracy (V EGN N (cid:48) vs. V EGN N )
Higher/Lower/Equal (p-value)
|R(cid:48)| = 500
46/19/8 (< 0.001)
55/13/5 (< 0.001)
49/16/8 (< 0.001)
46/23/4 (0.013)
49/14/10 (< 0.001)

|R(cid:48)| = 100
45/17/11 (< 0.001)
46/20/7 (< 0.001)
47/17/9 (< 0.001)
40/27/6 (0.055)
39/20/14 (0.026)

|R(cid:48)| = 1000
55/10/8 (< 0.001)
54/17/2 (< 0.001)
55/12/6 (< 0.001)
53/16/4 (< 0.001)
51/13/9 (< 0.001)

GNN
Variant
GN N 1
GN N 2
GN N 3
GN N 4
GN N 5

Fig. 7: Quantitative comparison of performance after augmenting domain-relations
with ILP-constructed relations. Here V EGN N (cid:48) denotes the vertex-enriched GNN
obtained after augmenting the generic domain relations (R) with domain-speciﬁc
relations constructed by an ILP engine (R(cid:48)); and V EGN N denotes the vertex-
enriched GNN with R. The tabulations are the number of datasets on which
V EGN N (cid:48) has higher, lower or equal predictive accuracy on a holdout-set. Statis-
tical signiﬁcance is assessed by the Wilcoxon signed-rank test.

4 Related Work

GNN-like models were ﬁrst proposed in [30, 6]. In these studies, the features from
the graph data was extracted using neural networks. Gori et al. [31] and Scarselli
et al. [32] proposed new graph-based learning methods that used recursive aggre-
gation of information. They called these models ‘graph neural networks (GNNs)’.
The major boost to the ﬁeld of GNNs followed the introduction of graph con-
volution [8] and the notion of graph embedding [33, 34]. Many such embedding
methods are based on iterative processing of the neighborhood information of any
vertex. One such vertex embedding method was formulated by generalising the
convolution operation to graphs. The convolution operation computes “hidden”
states (essentially vector-representations) of the vertices in the graph. There are a
wide variety of convolution-based GNNs most of which are classiﬁed into spectral-
or non-spectral (spatial) approaches. Two methodical and comprehensive surveys
over a series of variants of graph neural networks can be found in [35] and [36].
We have already seen that for practical problems the data cannot eﬀectively be

Incorporating Symbolic Domain Knowledge into Graph Neural Networks

17

modelled by pairwise associations. Methods have been proposed to deﬁne convo-
lutions for higher-order graphs or hypergraphs [13, 14, 15], although none of these
have considered the problem of inclusion of domain-knowledge. To the extent that
we consider a vertex-enriched graph to be a result of a hypergraph representation
of the data, the work proposed in this paper loosely falls under the category of
Hypergraph-based neural networks.

Notwithstanding the convolution operation used in GNNs, one drawback that
has been identiﬁed is that the representations learned by them could be poor if
the amount of training data (number of graphs) is small, which would lead to poor
generalisation [37]. The usual solution to this problem is to overcome data scarcity
by the use of prior knowledge, a feature that is at the heart of Inductive Logic Pro-
gramming. In almost all applications of ILP to date, the use of prior or background
knowledge is central (see [38]). In contrast, the position taken in the neural-network
literature, especially those dealing with networks with large numbers of hidden
layers, is that provided suﬃcient data are available, representations of relevant
domain-concepts can be computed automatically from data. But when data are
scarce, this assumption breaks down. The area of neuro-symbolic modelling [39]
has been concerned with ways of combining symbolic and neural learning. A simple
way of doing this has been studied under the category of “propositionalisation”
in ILP [40, 41, 42, 43, 44]. Although, propositionalisation approaches have been suc-
cessfully applied to various problems but are still considered as ad hoc approaches.
These approaches are studied in the larger context of macro-operators [45], which
are approaches to improve the heuristic search in ILP systems and extract higher-
level or meta-rules [46]. Pioneering work on the combination of neural-networks
and symbolic features has been done by d’Avila Garcez and Zaverucha [47] and
extended in Fran¸ca et al. [43, 48]. There are several studies that report that the
relational features constructed using propositionalisation-based approach can sub-
stantially improve predictive performance of statistical machine learning models,
see for example: [49, 50]. Recently, ILP-based feature-construction for deep multi-
layer perceptrons (a special case of Deep Relational Machines, or DRMs [51]) was
shown to yield surprisingly good results on the datasets used here, albeit with
very large numbers of features [20, 28]. At the other end of the spectrum, methods
are now being developed that include “neural” predicates (predicates whose deﬁ-
nitions are implemented by neural networks) as part of the background knowledge
available to a symbolic learner [52].

Domain-knowledge is often available as knowledge graphs (or semantic net-
works) rather than as a set of relations deﬁned in logic. Knowledge graph embed-
ding [53, 54] is a technique that is mostly applied to construct a vector represen-
tation for the knowledge graph, which can then be infused into some form into
a neural network. In recent reports, it is proposed that the latent representation
learned by a neural network can be coupled with the representation of the knowl-
edge graph that may improve the predictive performance of the neural network
model [55, 56].

5 Conclusions

Our focus in this paper has been on the use of graph-based neural networks (GNNs)
on scientiﬁc data. Scientiﬁc understanding is largely an incremental process that

18

Tirtharaj Dash et al.

builds on knowledge that is already known. It is natural therefore to expect that
automatic techniques intended for scientiﬁc data analysis will similarly be able
to utilise such knowledge. The results here clearly show the beneﬁt of having
mechanisms to incorporate domain-knowledge into GNNs. They also show the
beneﬁts of ILP as a mechanism for identifying relationships that appear not to
be within the practical reach of the GNN variants we have considered. An ILP-
purist could well ask: why then should we use GNNs at all? There are several
reasons to persist, chief amongst which are reasons of implementation eﬃciency
and widespread availability of packaged libraries. Assuming GNNs are useful, our
goal has been to show that they can be more useful if they use domain-speciﬁc
relations, and yet more so if they include results from an ILP engine.7

To the best of our knowledge, the experiments in this paper constitute some of
the most extensive applications of GNNs to large-scale real-world scientiﬁc data. It
has not been the focus of this paper to construct a GNN-based benchmark for the
data, but to investigate the use of domain-knowledge. There is undoubtedly room
in the future for comparative studies against other techniques that may or may not
utilise the domain-knowledge available. More immediately, the process of vertex-
enrichment can create very large vectors at each vertex (the result of a many-hot
encoding of the relations in the vertex’s label). We conjecture that this situation
can be improved by performing some dimensionality-reduction at each vertex. A
straightforward option is to include some form of auto-encoder at each vertex,
before re-labelling. Vertex-enriched GNNs can probably be signiﬁcantly improved
by directly working with Hypergraph GNNs (HGNNs). In principle, HGNNs will
have more information (like hyperedge labels). Will HGNNs also beneﬁt from the
use of ILP? We do not know the answer to this as yet.

Despite the recent empirical successes in various ﬁelds, recent studies highlight
some of the theoretical limitations of GNNs. For instance, GNNs cannot distinguish
between some pairs of graphs that are indistinguishable by the 1-WL test [37, 9],
that is, a GNN with any parameter setting cannot distinguish two graphs unless the
labels of the graphs are same. A recent study on GNNs [57] has shown that the class
of aggregate-combine GNNs cannot be logically more expressible than a fragment
of two-variable ﬁrst-order logic with counting quantiﬁers (Logic FOC2), which is
a form of description logic. In a diﬀerent report, various theoretical limitations of
GNNs are studied, speciﬁcally, in terms of approximation ratios of combinatorial
algorithms [58]. We have already indicated that the vertex-enrichment procedure
described in this paper may not capture fully the relational information present in
the data. We believe this limitation can be overcome by adopting a diﬀerent form
of graph representation, that is nevertheless still amenable to the use of GNNs.
We intend to explore this as future work.

At the outset of this paper, we motivated the use of machine learning in devel-
oping an automated scientiﬁc assistant. While high predictive power is expected
from an ML-based scientiﬁc assistant, it is not suﬃcient. It is evident that this pa-
per’s focus is on how prediction can improve by the inclusion of domain-knowledge.
An Understandable explanation of the models constructed by GNNs remains a
challenge.

7 The use of ILP would seem to undermine the motivation just given for using GNNs.
However, this is not so. First, once the ILP relations are constructed, the main modelling
eﬀort is still done using GNNs. Secondly, the construction of relations is task that can be
implemented by a specialised library.

Incorporating Symbolic Domain Knowledge into Graph Neural Networks

19

Data and Code Availability

Data, background-knowledge and codes used in our experiments are available at:
https://github.com/tirtharajdash/VEGNN.

Acknowledgements

This work is supported by DST-SERB Grant EMR/2016/002766, Government
of India. The second author is a Visiting Professorial Fellow at School of CSE,
UNSW Sydney. We sincerely thank Ing. Gustav ˇSourek, Czech Technical Univer-
sity, Prague for providing the dataset information; and researchers at the DTAI,
University of Leuven, for suggestions on how to use the background knowledge
within DMAX. We also thank Dr. Oghenejokpeme I. Orhobor and Professsor Ross
D. King for providing us with initial set of background-knowledge deﬁnitions.

References

1. King, R.D., Whelan, K.E., Jones, F.M., Reiser, P.G., Bryant, C.H., Muggleton, S.H., Kell,
D.B., Oliver, S.G.: Functional genomic hypothesis generation and experimentation by a
robot scientist. Nature 427(6971), 247–252 (2004)

2. King, R.D., Muggleton, S.H., Srinivasan, A., Sternberg, M.: Structure-activity relation-
ships derived by machine learning: The use of atoms and their bond connectivities to pre-
dict mutagenicity by inductive logic programming. Proceedings of the National Academy
of Sciences 93(1), 438–442 (1996)

3. Srinivasan, A., King, R.D.: Feature construction with inductive logic programming: A
study of quantitative predictions of biological activity aided by structural attributes. Data
Mining and Knowledge Discovery 3(1), 37–57 (1999)

4. Faruquie, T.A., Srinivasan, A., King, R.D.: Topic models with relational features for drug
design. In: International Conference on Inductive Logic Programming. pp. 45–57. Springer
(2012)

5. Stevens, R., Taylor, V., Nichols, J., Maccabe, A.B., Yelick, K., Brown, D.: Ai for science.

Tech. rep., Argonne National Lab.(ANL), Argonne, IL (United States) (2020)

6. Baskin, I.I., Palyulin, V.A., Zeﬁrov, N.S.: A neural device for searching direct correlations
between structures and properties of chemical compounds. Journal of chemical information
and computer sciences 37(4), 715–721 (1997)

7. McNaught, A.D., Wilkinson, A., et al.: Compendium of chemical terminology, vol. 1669.

Blackwell Science Oxford (1997)

8. Kipf, T.N., Welling, M.: Semi-supervised classiﬁcation with graph convolutional networks.
In: 5th International Conference on Learning Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings (2017)

9. Morris, C., Ritzert, M., Fey, M., Hamilton, W.L., Lenssen, J.E., Rattan, G., Grohe, M.:
Weisfeiler and leman go neural: Higher-order graph neural networks. In: Proceedings of
the AAAI Conference on Artiﬁcial Intelligence. vol. 33, pp. 4602–4609 (2019)

10. Veli˘ckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., Bengio, Y.: Graph
attention networks. In: International Conference on Learning Representations (2018),
https://openreview.net/forum?id=rJXMpikCZ

11. Hamilton, W., Ying, Z., Leskovec, J.: Inductive representation learning on large graphs.

In: Advances in neural information processing systems. pp. 1024–1034 (2017)

12. Bianchi, F.M., Grattarola, D., Alippi, C., Livi, L.: Graph neural networks with convolu-

tional arma ﬁlters. arXiv preprint arXiv:1901.01343 (2019)

13. Feng, Y., You, H., Zhang, Z., Ji, R., Gao, Y.: Hypergraph neural networks. In: Proceedings

of the AAAI Conference on Artiﬁcial Intelligence. vol. 33, pp. 3558–3565 (2019)

14. Jiang, J., Wei, Y., Feng, Y., Cao, J., Gao, Y.: Dynamic hypergraph neural networks. In:
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence
(IJCAI). pp. 2635–2641 (2019)

20

Tirtharaj Dash et al.

15. Yadati, N., Nimishakavi, M., Yadav, P., Nitin, V., Louis, A., Talukdar, P.: Hypergcn: A
new method for training graph convolutional networks on hypergraphs. In: Advances in
Neural Information Processing Systems. pp. 1509–1520 (2019)

16. Marx, K.A., O’Neil, P., Hoﬀman, P., Ujwal, M.: Data mining the nci cancer cell line com-
pound gi50 values: identifying quinone subtypes eﬀective against melanoma and leukemia
cell classes. Journal of chemical information and computer sciences 43(5), 1652–1667 (2003)
17. Ralaivola, L., Swamidass, S.J., Saigo, H., Baldi, P.: Graph kernels for chemical informatics.

Neural networks 18(8), 1093–1110 (2005)

18. Van Craenenbroeck, E., Vandecasteele, H., Dehaspe, L.: Dmax’s functional group and ring

library. https://dtai.cs.kuleuven.be/software/dmax/ (2002)

19. Ando, H.Y., Dehaspe, L., Luyten, W., Van Craenenbroeck, E., Vandecasteele, H.,
Van Meervelt, L.: Discovering h-bonding rules in crystals with inductive logic program-
ming. Molecular pharmaceutics 3(6), 665–674 (2006)

20. Dash, T., Srinivasan, A., Vig, L., Orhobor, O.I., King, R.D.: Large-scale assessment of
deep relational machines. In: International Conference on Inductive Logic Programming.
pp. 22–37. Springer (2018)

21. Srinivasan,

aleph manual.
The
programinduction/Aleph/aleph.html (2001)

A.:

https://www.cs.ox.ac.uk/activities/

22. Fey, M., Lenssen, J.E.: Fast graph representation learning with PyTorch Geometric. In:

ICLR Workshop on Representation Learning on Graphs and Manifolds (2019)

23. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,
Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-performance deep
learning library. In: Advances in Neural Information Processing Systems. pp. 8024–8035
(2019)

24. Lee, J., Lee, I., Kang, J.: Self-attention graph pooling. In: International Conference on

Machine Learning. pp. 3734–3743 (2019)

25. Cangea, C., Veliˇckovi´c, P., Jovanovi´c, N., Kipf, T., Li`o, P.: Towards sparse hierarchical

graph classiﬁers. arXiv preprint arXiv:1811.01287 (2018)

26. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980 (2014)

27. Prechelt, L.: Early stopping-but when? In: Neural Networks: Tricks of the trade, pp. 55–69.

Springer (1998)

28. Dash, T., Srinivasan, A., Joshi, R.S., Baskar, A.: Discrete stochastic search and its appli-
cation to feature-selection for deep relational machines. In: International Conference on
Artiﬁcial Neural Networks. pp. 29–45. Springer (2019)

29. Zhou, D., Huang, J., Sch¨olkopf, B.: Learning with hypergraphs: Clustering, classiﬁcation,
and embedding. In: Advances in neural information processing systems. pp. 1601–1608
(2007)

30. Sperduti, A., Starita, A.: Supervised neural networks for the classiﬁcation of structures.

IEEE Transactions on Neural Networks 8(3), 714–735 (1997)

31. Gori, M., Monfardini, G., Scarselli, F.: A new model for learning in graph domains. In:
Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005. vol. 2,
pp. 729–734 vol. 2 (2005)

32. Scarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M., Monfardini, G.: The graph neural

network model. IEEE Transactions on Neural Networks 20(1), 61–80 (2008)

33. Cui, P., Wang, X., Pei, J., Zhu, W.: A survey on network embedding. IEEE Transactions

on Knowledge and Data Engineering 31(5), 833–852 (2018)

34. Zhang, D., Yin, J., Zhu, X., Zhang, C.: Network representation learning: A survey. IEEE

transactions on Big Data (2018)

35. Zhou, J., Cui, G., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., Sun, M.: Graph neural
networks: A review of methods and applications. arXiv preprint arXiv:1812.08434 (2018)
36. Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., Philip, S.Y.: A comprehensive survey
on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems
(2020)

37. Xu, K., Hu, W., Leskovec, J., Jegelka, S.: How powerful are graph neural networks? In:
International Conference on Learning Representations (2019), https://openreview.net/
forum?id=ryGs6iA5Km

38. Muggleton, S., De Raedt, L., Poole, D., Bratko, I., Flach, P., Inoue, K., Srinivasan, A.: Ilp

turns 20. Machine learning 86(1), 3–23 (2012)

39. Besold, T.R., Garcez, A.d., Bader, S., Bowman, H., Domingos, P., Hitzler, P., K¨uhnberger,
K.U., Lamb, L.C., Lowd, D., Lima, P.M.V., et al.: Neural-symbolic learning and reasoning:
A survey and interpretation. arXiv preprint arXiv:1711.03902 (2017)

Incorporating Symbolic Domain Knowledge into Graph Neural Networks

21

40. Lavraˇc, N., Dˇzeroski, S., Grobelnik, M.: Learning nonrecursive deﬁnitions of relations with

linus. In: European Working Session on Learning. pp. 265–281. Springer (1991)

41. Kramer, S., Lavraˇc, N., Flach, P.: Propositionalization approaches to relational data min-

ing. In: Relational data mining, pp. 262–291. Springer (2001)

42. Krogel, M.A., Rawles, S., ˇZelezn`y, F., Flach, P.A., Lavraˇc, N., Wrobel, S.: Comparative
evaluation of approaches to propositionalization. In: International Conference on Inductive
Logic Programming. pp. 197–214. Springer (2003)

43. Fran¸ca, M.V., Zaverucha, G., Garcez, A.S.d.: Fast relational learning using bottom clause
propositionalization with artiﬁcial neural networks. Machine learning 94(1), 81–104 (2014)
44. Fran¸ca, M.V.M., Zaverucha, G., Garcez, A.S.d.: Neural relational learning through semi-

propositionalization of bottom clauses. In: 2015 AAAI Spring Symposium Series (2015)

45. Castillo, L.P., Wrobel, S.: Macro-operators in multirelational learning: a search-space re-
duction technique. In: European Conference on Machine Learning. pp. 357–368. Springer
(2002)

46. Alphonse, ´E.: Macro-operators revisited in inductive logic programming. In: International

Conference on Inductive Logic Programming. pp. 8–25. Springer (2004)

47. d’Avila Garcez, A.S., Zaverucha, G.: The connectionist inductive learning and logic pro-

gramming system. Appl. Intell. 11(1), 59–77 (1999)

48. Fran¸ca, M.V.M., d’Avila Garcez, A.S., Zaverucha, G.: Relational knowledge extraction
from neural networks. In: Proceedings of the NIPS Workshop on Cognitive Computation:
Integrating Neural and Symbolic Approaches co-located with the 29th Annual Conference
on Neural Information Processing Systems (NIPS 2015), Montreal, Canada, December
11-12, 2015. (2015)

49. Ramakrishnan, G., Joshi, S., Balakrishnan, S., Srinivasan, A.: Using ilp to construct fea-
tures for information extraction from semi-structured text. In: International Conference
on Inductive Logic Programming. pp. 211–224. Springer (2007)

50. Saha, A., Srinivasan, A., Ramakrishnan, G.: What kinds of relational features are useful
for statistical learning? In: International Conference on Inductive Logic Programming. pp.
209–224. Springer (2012)

51. Lodhi, H.: Deep relational machines. In: International Conference on Neural Information

Processing. pp. 212–219. Springer (2013)

52. De Raedt, L., Manhaeve, R., Dumancic, S., Demeester, T., Kimmig, A.: Neuro-symbolic=
neural+ logical+ probabilistic. In: NeSy’19@ IJCAI, the 14th International Workshop on
Neural-Symbolic Learning and Reasoning. pp. 1–4 (2019)

53. Ding, B., Wang, Q., Wang, B., Guo, L.: Improving knowledge graph embedding using sim-
ple constraints. In: Proceedings of the 56th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers). pp. 110–121. Association for Computa-
tional Linguistics, Melbourne, Australia (Jul 2018), https://www.aclweb.org/anthology/
P18-1011

54. Ziegler, K., Caelen, O., Garchery, M., Granitzer, M., He-Guelton, L., Jurgovsky, J., Portier,
P.E., Zwicklbauer, S.: Injecting semantic background knowledge into neural networks using
graph embeddings. In: 2017 IEEE 26th International Conference on Enabling Technologies:
Infrastructure for Collaborative Enterprises (WETICE). pp. 200–205. IEEE (2017)
55. Gaur, M., Kursuncu, U., Wickramarachchi, R.: Shades of knowledge-infused learning for

enhancing deep learning. IEEE Internet Computing 23(6), 54–63 (2019)

56. Kursuncu, U., Gaur, M., Sheth, A.: Knowledge infused learning (k-il): Towards deep in-
corporation of knowledge in deep learning. arXiv preprint arXiv:1912.00512 (2019)
57. Barcel´o, P., Kostylev, E.V., Monet, M., P´erez, J., Reutter, J., Silva, J.P.: The logical
expressiveness of graph neural networks. In: International Conference on Learning Repre-
sentations (2020), https://openreview.net/forum?id=r1lZ7AEKvB

58. Sato, R.: A survey on the expressive power of graph neural networks. arXiv preprint

arXiv:2003.04078 (2020)

59. Muggleton, S.: Inverse entailment and progol. New generation computing 13(3-4), 245–286

(1995)

60. Joshi, S., Ramakrishnan, G., Srinivasan, A.: Feature construction using theory-guided
sampling and randomised search. In: International Conference on Inductive Logic Pro-
gramming. pp. 140–157. Springer (2008)

61. Plotkin, G.: Automatic Methods of Inductive Inference. Ph.D. thesis, Edinburgh University

(August 1971)

22

Tirtharaj Dash et al.

A Graph Neural Networks

A.1 Implementation

In a graph G = (V, E), let Xv denote a vector that represents the labelling of a vertex v ∈ V .
This is called the feature vector of the vertex v. In a GNN, the Relabel function is implemented
by a neighbourhood aggregation mechanism[37]. It updates the representation of a vertex, hv
iteratively. That is, in kth iteration (or kth layer), the representation of a vertex v, h(k)
can
be computed using two procedures: AGGREGATE and COMBINE.

v

v = AGGREGATE(k) (cid:16)(cid:110)
a(k)
v = COMBINE(k) (cid:16)
h(k)

h(k−1)
v

(cid:17)

, a(k)
v

h(k−1)
u

: u ∈ N (v)

(cid:111)(cid:17)

,

(1)

(2)

where, N (v) denotes the set of vertices adjacent to v. Initially (at k = 0), h(0)

v = Xv.

The V ectorise function constructs a vector representation of the entire graph. This step is
carried out after the representations of all the vertices are relabelled by some iterations. The
vectorised representation of the entire graph can be obtained using a READOUT function that
aggregates vertex features from the ﬁnal iteration (k = K):

hG = READOUT

(cid:16)(cid:110)

h(K)
v

(cid:111)(cid:17)

| v ∈ G

(3)

There are diﬀerent variants of AGGREGATE-COMBINE procedures available in the litera-
ture on GNNs. These are mostly implemented using the methods known as graph convolution
and graph pooling (refer [35, 36]). The READOUT procedure is usually implemented using a
global or hierarchical pooling operation. The convolution operations of various GNNs used in
our work are brieﬂy described in Appendix A.2. Further, we use an additional pooling layer
called structural-attention pooling after each of the convolution layer. This is brieﬂy described
in Appendix A.3.

A.2 Graph Convolutions

A.2.1 Variant 1

The ﬁrst variant of GNN used in our work is based on spectral-based graph convolutional
network proposed by Kipf and Welling [8]. It uses a layer-wise (or iteration-wise) propagation
rule for a graph with N vertices as:

H(k) = σ

(cid:16) ˜D− 1

2 ˜A ˜D− 1

2 H(k−1)Θ(k−1)(cid:17)

(4)

where, H (k) ∈ RN ×D denotes the matrix of vertex representations of length D, ˜A = A + I
is the adjacency matrix representing an undirected graph G with added self-connections, A ∈
RN ×N is the graph adjacency matrix, IN is the identity matrix, ˜Dii = (cid:80)
˜Aij , and Θ(k−1)
is the iteration-speciﬁc trainable parameter matrix, σ(·) denotes the activation function e.g.
ReLU(·) = max(0, ·), H(0) = X, X is the matrix of vertex feature vectors Xis.

j

A.2.2 Variant 2

The second variant is based on the graph neural network proposed by Morris et al. [9] that
passes messages directly between subgraph structures inside the graph. At iteration k, the
feature representation of a vertex is computed by using

h(k)
u = σ


h(k−1)

u

· Θ(k)

1 +

(cid:88)

h(k−1)
v

· Θ(k)
2



v∈N (u)



(5)

where, σ is a non-linear transfer function applied component wise to the function argument,
Θs are the layer-speciﬁc learnable parameters of the network.

Incorporating Symbolic Domain Knowledge into Graph Neural Networks

23

A.2.3 Variant 3

The third variant is an attention-based model, which is popularly known as Graph Attention
Network (GAT) [10]. This network assumes that the contributions of neighboring vertices
to the central vertex are not pre-determined which is the case in the Graph Convolutional
Network [8]. This adopts attention mechanisms to learn the relative weights between two
connected vertices. The graph convolutional operation at iteration k is thereby deﬁned as:

h(k)
u = σ





(cid:88)

uv Θ(k)h(k−1)
α(k)

u





v∈N (u)∪u

(6)

where, h(0)
called attention weight, which is deﬁned as

u = Xu. The connective strength between the vertex u and its neighbor vertex v is

α(k)

uv = softmax

(cid:16)

LeakyReLU

aT (cid:104)
(cid:16)

Θ(k)h(k−1)
u

(cid:107) Θ(k)h(k−1)

v

(cid:105)(cid:17)(cid:17)

(7)

where, a is the set of learnable parameters of a single layer feed-forward neural network.

A.2.4 Variant 4

The fourth variant is called GraphSAGE and it is a framework for inductive representation
learning on large graphs [11]. It is done in two steps: local neighborhood sampling and then ag-
gregation of generating the embeddings of the sampled nodes. GraphSAGE is used to generate
low-dimensional vector representations for nodes, and is especially useful for graphs that have
rich node attribute information. The following is an iterative update of the node embedding:

h(k)
u = σ


h(k−1)

u

· Θ(k)

1 +

1
|N (u)|



(cid:88)

h(k−1)
v

· Θ(k)
2



v∈N (u)

(8)

where, σ is a non-linear transfer function applied component wise to the function argument,
Θs are the layer-speciﬁc learnable parameters of the network.

A.2.5 Variant 5

This variant of GNN is inspred by the auto-regressive moving avarage (ARMA) ﬁlters that
are considered to be more robust than polynomial ﬁlters [12]. The ARMA graph convolutional
operator is deﬁned as follows:

H(k) =

1
M

M
(cid:88)

m=1

H(K)
m

(9)

where, M is the number of parallel stacks, K is the number of layers; and H(K)
deﬁned as

(cid:16) ˆLH(k)
where, ˆL = I − L is the modiﬁed Laplacian. The Θ parameters are learnable parameters.

2 + H(0)Θ(k)

m Θ(k)

H(k+1)
m

= σ

(cid:17)

2

m is recursively

(10)

A.3 Graph Pooling

Graph pooling methods apply downsampling mechanisms to graphs. In this work, we use a
recently proposed graph pooling method based on self-attention [24]. It uses graph convolution
deﬁned in Eq. (4) to obtain a self-attention score as given in Eq. 11 with the trainable parameter
replaced by Θatt ∈ RN ×1, which is a set of trainable parameters in the pooling layer.
(cid:16) ˜D− 1

2 ˜A ˜D− 1

Z = σ

(11)

(cid:17)

2 XΘatt

Here, σ(·) is the activation function e.g. tanh.

24

Tirtharaj Dash et al.

A.4 Structure of the GNNs

The structure of the GNNs closely follows the structure used in [24]. A schematic diagram
of our implemented architecture is shown in Fig. 8. As shown in the diagram, the output of
the hierarchical pooling is fed as input to a multilayer perceptron (MLP). So, the input layer
of the MLP contains 2m units, followed by two hidden layers with m units and (cid:98)m/2(cid:99) units
respectively. The activation function used in the hidden layers is relu. The output layer size
is |Y| (in this work, 2) with logsoftmax activation.

Fig. 8: Graph classiﬁcation architecture used in this work. We perform our exper-
iments with ﬁve diﬀerent types of graph convolution methods, each resulting in a
diﬀerent kind of GNN architecture.

Incorporating Symbolic Domain Knowledge into Graph Neural Networks

25

B ILP Speciﬁcs

B.1 Extending Domain-Knowledge

We assume that a set of relations R are provided as part of the background knowledge B
available to an ILP engine.8 Given B and data E consisting of a set of positive and negative
instances (here representing molecules with or without the property of interest), and ILP
engine can construct new clauses deﬁned in terms of the relations in R. These clauses can
be additionally be ordered in terms of some utility function (for example, a clause encoding
a relation that holds for large number of positive instances may have a high utility). The so-
called technique of ILP-based “propositionalisation”, for example, identiﬁes high-utility clauses
(for example, see [49, 60, 20]). The procedure used to draw “new” relations using ILP-derived
techniques is in Procedure 3.

Procedure 3: (LearnRels) Procedure to construct new relations using
ILP. We assume the domain-knowledge consists of some relations R ∈ B.
The construction of ⊥ is as described in [59], and (cid:23)θ refers to Plotkin’s θ-
subsumption [61]. The redundancy test used is subsumption-equivalence.
The distribution DC is deliberately left unspeciﬁed here. In the exper-
iments in the paper, DC is either uniform (resulting in simple random
construction of new relations), or a non-uniform selection based on clause-
utility, as described in [28]. N ewR is a new relation name that does
not occur in B; Cp denotes a conjunction of literals; x is shorthand for
x1, x2, . . . , xn, the variables in Cp.

Data: Domain knowledge B, A set of examples E, and M axDraws
Result: A set of relations R(cid:48)
R(cid:48) := ∅;
draws := 0;
i := 1;
Drawn := ∅;
while draws ≤ M axDraws do

Randomly draw an example ei ∈ E with replacement;
Let ⊥(B, ei) be the most speciﬁc rule that entails ei, given B;
Let DC be a distribution over clauses;
Draw a clause Ci using DC s.t. Ci (cid:23)θ ⊥d(B, ei);
if Ci is not redundant given Drawn then
Let Ci = (Class((x, c) ← Cpi(x)));
Let Ri = (N ewR(x) ← Cpi(x));
Drawn := Drawn ∪ {Ci};
R(cid:48) = R(cid:48) ∪ {Ri};
increment i;

end
increment draws;

end
return R(cid:48);

8 Besides R, B will usually contain additional ILP-speciﬁc content like mode declarations

(see [59], along with search constraints and ancillary predicates).

26

B.2 Input

Tirtharaj Dash et al.

We use the ILP engine Aleph to construct the most-speciﬁc rule above. Aleph requires the
speciﬁcation of a mode language, specifying the predicates in R. The mode-language used for
the experiments in the paper is given below:

:- modeb(*,bond(+mol,-atomid,-atomid,#atomtype,#atomtype,#bondtype)).
:- modeb(*,has_struc(+mol,-atomids,-length,#structype)).
:- modeb(*,connected(+mol,#structype,-atomids,#structype,-atomids)).
:- modeb(*,fused(+mol,#structype,-atomids,#structype,-atomids)).

The ‘#’-ed arguments in the mode declaration refers to type, that is, #atomtype refers to the
type of atom, #bondtype refers to the type of bond, and #structype refers to the type of the
structure (functional group or ring) associated with the molecule.

Each data instance (a molecule) is represented by a set of ground facts of the following

kind:

bond(m1,27,24,o2,car,1).
...

Here bond(m1,27,24,o2,car,1) denotes that in instance m1 there is an oxygen atom (id 27),
and a carbon atom (id 24) connected by a single bond (car denotes a carbon atom in an
aromatic ring).

Given the molecular structure additional facts like functional group/4 and ring/4 are pre-
computed for eﬃciency using the generic relations in R (which contain the symbolic deﬁnitions
of benzene rings, oxide groups, etc.). This results in facts like the following:

functional_group(m1,[27],1,oxide).
ring(m1,[25,28,30,29,26,23],6,benzene_ring).
...

We note that these predicates result in a reiﬁcation of the predicates in R (that is, the
predicate symbols are converted to terms). The predicates has struc/4, connected/5 and
fused/5 are deﬁned over these predicates. For example (in Prolog format):

has_struc(Mol,Atoms,Length,Type):-

ring(Mol,Atoms,Length,Type).

has_struc(Mol,Atoms,Length,Type):-

functional_group(Mol,Atoms,Length,Type).

...

We reiterate that these predicates are deﬁned directly on the relations in R: the use of
functional group/4 and ring/4 is for compactness and eﬃciency.

B.3 Output

Given the mode language, and data consisting of the molecular structure, the ILP engine ﬁnds
clauses like these (shown as Prolog clauses):

class(A,pos):-

has_struc(A,D,E,ester_car),
bond(A,F,G,c1,c1,3).

class(A,pos):-

connected(A,benzene_ring,D,benzene_ring,E),
connected(A,keton,F,non_hetero_non_aromatic,G).

class(A,pos):-

fused(A,benzene_ring,D,imidazole_ring,E),
connected(A,oxide,F,oxide,G).

...

Each such clause is converted to an n-ary relation using the steps in Procedure 2.

