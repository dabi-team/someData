The Role of Lookahead and Approximate Policy
Evaluation in Reinforcement Learning with Linear
Value Function Approximation

Anna Winnicki
Department of Electrical and Computer Engineering and Coordinated Science Laboratory, University of Illinois at
Urbana-Champaign, Urbana, IL 61801, annaw5@illinois.edu

Joseph Lubars
Sandia National Laboratories, 1515 Eubank Blvd SE, Albuquerque, NM 87123, lubars2@illinois.edu

Michael Livesay
Sandia National Laboratories, 1515 Eubank Blvd SE, Albuquerque, NM 87123, mlivesa@sandia.gov

R. Srikant
Department of Electrical and Computer Engineering and Coordinated Science Laboratory, University of Illinois at
Urbana-Champaign, Urbana, IL 61801, rsrikant@illinois.edu. R. Srikant is also aﬃliated with c3.ai DTI.

Function approximation is widely used in reinforcement learning to handle the computational diﬃculties

associated with very large state spaces. However, function approximation introduces errors which may lead to

instabilities when using approximate dynamic programming techniques to obtain the optimal policy. There-

fore, techniques such as lookahead for policy improvement and m-step rollout for policy evaluation are used

in practice to improve the performance of approximate dynamic programming with function approximation.

We quantitatively characterize, for the ﬁrst time, the impact of lookahead and m-step rollout on the per-

formance of approximate dynamic programming (DP) with function approximation: (i) without a suﬃcient

combination of lookahead and m-step rollout, approximate DP may not converge, (ii) both lookahead and

m-step rollout improve the convergence rate of approximate DP, and (iii) lookahead helps mitigate the eﬀect

of function approximation and the discount factor on the asymptotic performance of the algorithm. Our

results are presented for two approximate DP methods: one which uses least-squares regression to perform

function approximation and another which performs several steps of gradient descent of the least-squares

objective in each iteration.

Key words : Markov Decision Processes, Dynamic Programming

2
2
0
2

l
u
J

2
1

]

G
L
.
s
c
[

6
v
9
1
4
3
1
.
9
0
1
2
:
v
i
X
r
a

1

 
 
 
 
 
 
2

1. Introduction

In many applications of reinforcement learning, such as playing chess and Go, the underlying model

is known and so the main challenge is in solving the associated dynamic programming problem in an

eﬃcient manner. Policy iteration and variants of policy iteration Bertsekas (2019, 2011), Bertsekas

and Tsitsiklis (1996) that solve dynamic programming problems rely on computations that are

infeasible due to the sizes of the state and action spaces in modern reinforcement learning problems.

As a remedy to this “curse of dimensionality,” several state-of-the-art algorithms Silver et al.

(2017a,b), Mnih et al. (2016) employ function approximation, lookahead for policy improvement,

m-step rollout for policy evaluation, and gradient descent to compute the function approximation,

see Section 2 for a deﬁnition of these terms.

Our goal in this paper is to understand the role of multi-step lookahead for policy improvement

(i.e., repeatedly applying the Bellman operator multiple times) and m-step rollout (which is a tech-

nique to approximately evaluate a policy by rolling out the dynamic programming tree for a certain

number of steps m) on the accuracy of approximate policy iteration techniques. The algorithms

we study in this paper are closely related to least-squares policy iteration (LSPI) Lagoudakis and

Parr (2003) and approximate policy iteration (PI), see Bertsekas and Tsitsiklis (1996), Bertsekas

(2019). In the analysis of approximate PI, it is assumed that the policy evaluation and improvement

steps have bounded errors, and using these, an error bound is obtained for the algorithm which

repeatedly uses approximate policy evaluation and improvement. LSPI is an algorithm that builds

on approximate PI where the policy evaluation step uses a least-squares algorithm to estimate the

value function for the entire state space using the value function evaluated at a few states. However,

the bounds presented in Lagoudakis and Parr (2003) are simply a special case of the bounds for

generic approximate PI, and do not explicitly take into account the details of the implementation

of least-squares-based policy evaluation. When such details are taken into account, it turns out

the roles of the depth of lookahead (H) and rollout (m) become important, and their impact on

the error bounds on the performance of approximate value iteration has not been characterized

3

in prior work. In this paper, on the other hand, we assume that policies are evaluated at a few

states using an m-step rollout and as a result, convergence of the algorithm is not guaranteed in

general. Additionally, we show that the eﬀect of function approximation can be mitigated using

lookahead in the policy improvement step. The use of a partial rollout in our algorithm also makes

our work similar to modiﬁed policy iteration Puterman and Shin (1978), which is also called opti-

mistic policy iteration Bertsekas and Tsitsiklis (1996). To the best of our knowledge, none of these

prior works consider the impact of using gradient descent to implement an approximate version of

least-squares policy evaluation within approximate PI. Thus, our algorithm and analysis can be

viewed as a detailed look at approximate PI and modiﬁed PI when linear function approximation,

least-squares policy evaluation and gradient descent are used to evaluate policies.

Our contributions are as follows:

• We examine the impact of lookahead and m-step rollout on approximate policy iteration

with linear function approximation. As is common in practice, we assume that we evaluate an

approximate value function only for some states at each iteration. We obtain performance bounds

for our algorithm under the assumption that the sum of the lookahead and the number of steps in

the m-step rollout is suﬃciently large. We demonstrate through an extension of a counterexample

in Tsitsiklis and van Roy (1994) that such a condition is necessary, in general, for convergence with

function approximation unlike the tabular setting in Efroni et al. (2019). See Appendix E for our

counterexample.

• For ease of exposition, we ﬁrst present the case where one solves a least-squares problem at

each iteration to obtain the weights associated with the feature vectors in the function approxima-

tion of the value function. Our performance bounds in this case strictly generalize the bounds in

Lagoudakis and Parr (2003) and Bertsekas (2019) for approximate PI.

• We then consider a more practical and widely-used scheme where several steps of gradient

descent are used to update the weights of the value function approximation at each iteration.

Obtaining performance bounds for the gradient descent algorithm is more challenging and these

bounds can be found in Section 4.

4

• Our results show that the suﬃcient conditions on the hyperparameters (such as the amount of

lookahead, rollout, gradient descent parameters) of the algorithm required for convergence either

do not depend on the size of the state space or depend only logarithmically on the size of the state

space.

• From a theoretical perspective, our analysis shows that one can improve the upper bound

on the error in approximate policy iteration from 1/(1 − α)2 (see Lagoudakis and Parr (2003),

Bertsekas (2019)) to 1/(1 − αH)(1 − α) by using lookahead, where α is the discount factor and H

is the amount of lookahead used.

• In addition to asymptotic performance bounds, we also provide ﬁnite-time guarantees for our

algorithm. Our ﬁnite-time bounds show that our algorithm converges exponentially fast in the case

of least-squares as well as the case where a ﬁxed number of gradient descent steps are performed

in each iteration of the algorithm.

1.1. Other Related Work

The recent work in Efroni et al. (2019) considers a variant of policy iteration that utilizes lookahead

and approximate policy evaluation using an m-step rollout (see Section 2 for deﬁnitions of these

terms). As stated in the motivation in Efroni et al. (2019), it is well known that Monte Carlo Tree

Search (MCTS) Kocsis and Szepesv´ari (2006), Browne et al. (2012) works well in practice even

though the worst-case complexity can be exponential Shah et al. (2020b); see Munos (2014) for

some analysis of MCTS in MDPs where the number of states that can be visited from a given state

is bounded. Motivated by policy iteration, the algorithm in Efroni et al. (2019) estimates the value

function associated with a policy and aims to improve the policy at each step. Policy improvement

is achieved by obtaining the “greedy” policy in the case of policy iteration or a lookahead policy in

the work of Efroni et al. (2019), which involves applying the Bellman operator several times to the

current iterate before obtaining the greedy policy. The idea is that the application of the Bellman

operator several times gives a more accurate estimate of the optimal value function. Then, similarly

to policy iteration, the algorithm in Efroni et al. (2019) aims to evaluate the new policy. The

5

algorithm in Efroni et al. (2019) uses an m-step rollout to compute the value function associated

with a policy, i.e., it applies the Bellman operator associated with the policy m times. The work of

Efroni et al. (2019) establishes that a lookahead can signiﬁcantly improve the rate of convergence

if one uses the value function computed using lookahead in the approximate policy evaluation

step. However, their paper does not study the use of function approximation which is critical to

handling large state spaces, nor does it quantify the eﬀects of varying m in the convergence of their

algorithm.

Now, we compare our work to other papers in the literature. The role of lookahead and rollout

in improving the performance of RL algorithms has also been studied in a large number of papers

including Shah et al. (2020a), Moerland et al. (2020), Efroni et al. (2020), Tomar et al. (2020),

Efroni et al. (2018b), Springenberg et al. (2020), Deng et al. (2020). The works of Baxter et al.

(1999), Veness et al. (2009), Lanctot et al. (2014) explore the role of tree search in RL algorithms.

However, to the best of our knowledge, the amount of lookahead and rollout needed as a function

of the feature vectors has not been quantiﬁed in prior works.

The works of Bertsekas (2011) and Bertsekas (2019) also study a variant of policy iteration

wherein a greedy policy is evaluated approximately using feature vectors at each iteration. These

papers also provide rates of convergence as well as a bound on the approximation error. However,

our main goal is to understand the relations between function approximation and lookahead/rollout

which are not considered in these other works.

2. Preliminaries

We consider a Markov Decision Process (MDP), which is deﬁned to be a 5-tuple (S, A, P, r, α). The

ﬁnite set of states of the MDP is S. There exists a ﬁnite set of actions associated with the MDP

A. Let Pij(a) be the probability of transitioning from state i to state j when taking action a ∈ A.

We denote by sk the state of the MDP and by ak the corresponding action at time k. We associate

with state sk and action ak a non-deterministic reward r(sk, ak) ∈ [0, 1]∀sk ∈ S, ak ∈ A. We assume

that the rewards are uniformly bounded. Our objective is to maximize the cumulative discounted

reward with discount factor α ∈ (0, 1).

6

Towards this end, we seek to ﬁnd a deterministic policy µ which associates with each state s ∈ S

an action µ(s) ∈ A. For every policy µ and every state s ∈ S we deﬁne J µ(s) as follows:

J µ(s) := E[

∞
(cid:88)

i=0

αkr(sk, µ(sk))|s0 = s].

We deﬁne the optimal reward-to-go J ∗ as J ∗(s) := max

µ

J µ(s). The objective is to ﬁnd a policy

µ that maximizes J µ(s) for all s ∈ S. Towards the objective, we associate with each policy µ a

function Tµ : R|S| → R|S| where for J ∈ R|S|, the sth component of TµJ is

(TµJ)(s) = r(s, µ(s)) + α

|S|
(cid:88)

j=1

Psj(µ(s))J(j),

for all s ∈ S. If function Tµ is applied m times to vector J ∈ R|S|, then we say that we have

performed an m-step rollout of the policy µ and the result T m

µ J of the rollout is called the return.

Similarly, we deﬁne the Bellman operator T : R|S| → R|S| with the sth component of T J being

(cid:40)

(T J)(s) = max
a∈A

r(s, a) + α

Psj(a)J(j)

(cid:41)
.

|S|
(cid:88)

j=1

(1)

The policy corresponding to the T operator is deﬁned as the greedy policy. If operator T is applied

H times to vector J ∈ R|S|, we call the result - T HJ - the H-step “lookahead” corresponding to J.

The greedy policy corresponding to T HJ is called the H-step lookahead policy, or the lookahead

policy, when H is understood. More precisely, given an estimate J of the value function, the

lookahead policy is the policy µ such that Tµ(T H−1J) = T (T H−1J).

It is well known that each time the Bellman operator is applied to a vector J to obtain T J, the

following holds:

(cid:107)T J − J ∗(cid:107)∞ ≤ α (cid:107)J − J ∗(cid:107)∞ .

Thus, applying T to obtain T J gives a better estimate of the value function than J.

The Bellman equations state that the vector J µ is the unique solution to the linear equation

J µ = TµJ µ.

(2)

Algorithm 1 Least-Squares Function Approximation Algorithm
Input: J0, m, H, feature vectors {φ(i)}i∈S, φ(i) ∈ Rd and subsets Dk ⊆ S, k = 0, 1, . . . . Here Dk is

the set of states at which we evaluate the current policy at iteration k.

7

1: Let k = 0.

2: Let µk+1 be such that (cid:13)

(cid:13)T HJk − Tµk+1T H−1Jk

(cid:13)
(cid:13)∞

≤ εLA.

3: Compute ˆJ µk+1(i) = T m

µk+1

T H−1(Jk)(i) + wk+1(i) for i ∈ Dk.

4: Choose θk+1 to solve

min
θ

(cid:88)

(cid:16)

i∈Dk

(Φθ)(i) − ˆJ µk+1(i)

(cid:17)2

,

(5)

where Φ is a matrix whose rows are the feature vectors.

5: Jk+1 = Φθk+1.

6: Set k ← k + 1. Go to 2.

Additionally, we have that J ∗ is a solution to

J ∗ = T J ∗.

Note that every greedy policy w.r.t. J ∗ is optimal and vice versa Bertsekas and Tsitsiklis (1996).

We will now state several useful properties of the operators T and Tµ. See Bertsekas and Tsitsiklis

(1996) for more on these properties. Consider the vector e ∈ R|S| where e(i) = 1∀i ∈ 1, 2, . . . , |S|.

We have:

T (J + ce) = T J + αce, Tµ(J + ce) = TµJ + αce.

Operators T and Tµ are also monotone:

J ≤ J (cid:48) =⇒ T J ≤ T J (cid:48), TµJ ≤ TµJ (cid:48).

3. Least-Squares Function Approximation Algorithm

(3)

(4)

Our algorithm is described in Algorithm 1. We now explain our algorithm and the associated

notation in detail. Due to the use of function approximation, our algorithm is an approximation

8

to policy iteration with lookahead. At each iteration index, say, k, we have an estimate of the

value function, which we denote by Jk. To obtain Jk+1, we perform a lookahead to improve the

value function estimate at a certain number of states (denoted by Dk) which can vary with each

iteration. For example, Dk could be chosen as the states visited when performing a tree search

to approximate the lookahead process. During the lookahead process, we note that we will also

obtain an H-step lookahead policy, which we denote by µk+1. As noted in the Introduction, the

computation of T H−1(Jk)(i) for i ∈ Dk in Step 3 of Algorithm 1 may be computationally infeasible;

however, as noted in Efroni et al. (2019), techniques such as Monte Carlo tree search (MCTS)

are employed in practice to approximately estimate T H−1(Jk)(i). In this paper, we model the fact

that lookahead cannot be performed exactly due to the associated computational complexity by

allowing an error in the lookahead process which we denote by εLA in Step 2 of the algorithm.

We obtain estimates of J µk+1(i) for i ∈ Dk, which we call ˆJ µk+1(i). To obtain an estimate

of J µk+1(i), we perform an m-step rollout with policy µk+1, and obtain a noisy version of

T m

µk+1

T H−1Jk(i) for i ∈ Dk. We also model the approximation error in the rollout by adding noise

(denoted by wk+1(i) in Step 3 of the algorithm) to the return (result of the rollout - see Section

2) computed at the end of this step. In order to estimate the value function for states not in Dk,

we associate with each state i ∈ S a feature vector φ(i) ∈ Rd where typically d << |S|. The matrix

comprised of the feature vectors as rows is denoted by Φ. We use those estimates to ﬁnd the best

ﬁtting θ ∈ Rd, i.e.,

min
θ

(cid:88)

(cid:16)

i∈Dk

(Φθ)(i) − ˆJ µk+1(i)

(cid:17)2

.

The solution to the above minimization problem is denoted by θk+1. The algorithm then uses θk+1

to obtain Jk+1 = Φθk+1. The process then repeats. Note that to compute ˆJ µk+1(i), we obtain noisy

estimates of T m

µk+1

T H−1Jk(i) for i ∈ Dk. Another alternative is to instead obtain noisy estimates

of T m

µk+1

Jk(i) for i ∈ Dk. It was shown in Efroni et al. (2019) that the former option is preferable

because it has a certain contraction property. Thus, we have chosen to use this computation in our

algorithm as well. However, we have shown in Appendix C that the algorithm also has bounded

error which becomes small if m is chosen to be suﬃciently large.

9

Remark 1. We note that µk+1(i) in Step 2 of Algorithm 1 does not have to be computed for all

states i ∈ S. The actions µk+1(i) have to be computed only for those i ∈ S that are encountered in

the rollout step of the algorithm (Step 3).

To analyze Algorithm 1, we make the following assumption which states that we explore a

suﬃcient number of states during the policy evaluation phase at each iteration.

Assumption 1. For each k ≥ 0, rank {φ(i)}i∈Dk = d.

We assume that the noise wk is bounded.

Assumption 2. For some εP E > 0, the noise in policy evaluation satisﬁes (cid:107)wk(cid:107)∞ ≤ εP E∀k.

We also assume that the rewards are bounded.

Assumption 3. r(s, u) ∈ [0, 1] ∀s ∈ S, u ∈ A.

Using Assumption 1, Jk+1 can be written as

Jk+1 = Φθk+1 = Φ(Φ(cid:62)
Dk

(cid:124)

ΦDk )−1Φ(cid:62)
Dk
(cid:123)(cid:122)
=:Mk+1

Pk
(cid:125)

ˆJ µk+1,

(6)

where ΦDk is a matrix whose rows are the feature vectors of the states in Dk and Pk is a matrix of

zeros and ones such that Pk

ˆJ µk+1 is a vector whose elements are a subset of the elements of ˆJ µk+1

corresponding to Dk. Note that ˆJ µk+1(i) for i /∈ Dk does not aﬀect the algorithm, so we can deﬁne

ˆJ µk+1(i) = T m

µk+1

T H−1Jk(i) for i /∈ Dk.

Written concisely, our algorithm is as follows:

Jk+1 = Mk+1(T m

µk+1

T H−1Jk + wk),

(7)

where µk+1 is deﬁned in Step 2 of the algorithm. Since wk(i) for i /∈ Dk does not aﬀect the algorithm,

we deﬁne wk(i) = 0 for i /∈ Dk.

Now we will state our theorem which characterizes the role of lookahead (H) and return (m) on

the convergence of approximate policy iteration with function approximation.

10

Theorem 1. Suppose that m and H satisfy m + H − 1 > log(δF V )/ log(1/α), where

δF V := sup

k

(cid:107)Mk(cid:107)∞ = sup
k

(cid:13)
(cid:13)Φ(Φ(cid:62)
Dk

ΦDk )−1Φ(cid:62)
Dk

Pk

(cid:13)
(cid:13)∞

.

Then, under Assumptions 1-3, the following holds:

(cid:107)J µk − J ∗(cid:107)∞ ≤

αk(H)
1 − α
(cid:124)

+

2αH (cid:107)J µ0 − J0(cid:107)∞
1 − α

(cid:123)(cid:122)
ﬁnite-time component

k max(αH, β)k−1

+

2αH τ
1−β + εLA
(1 − αH)(1 − α)
(cid:125)
(cid:123)(cid:122)
(cid:124)
asymptotic component

.

(8)

(cid:125)

where

and

τ :=

αm + αm+H−1
1 − α

δF V + δapp + δF V εP E,

β := αm+H−1δF V ,

δapp := sup
k,µk

(cid:107)MkJ µk − J µk (cid:107)∞ .

The proof of Theorem 1 can be found in Appendix B. We now make several comments about the

implications of Theorem 1:

1. In conjunction with the counterexample in Appendix E, Theorem 1 shows that while

(cid:107)J µk − J ∗(cid:107)∞ depends on the function approximation error (δapp) and the feature vectors (δF V ),

the eﬀect of these terms diminishes exponentially with increased H, with the exception of the tree

search error (εLA).

2. It is useful to compare our asymptotic bound with the asymptotic bound for approximate

PI in Bertsekas (2019). There, it assumed that m = ∞, εP E = 0 and H = 1, in which case our

bound is identical to the one in Bertsekas (2019). However, when H > 1, our asymptotic error is

proportional to 1/(1 − α)(1 − αH) which is much better than the 1/(1 − α)2 bound for approximate

policy iteration Bertsekas (2019). When α → 1, the expected discounted reward is of the order of

1/(1 − α), thus the 1/(1 − α)2 bound on the error is generally considered to be very loose. Our

result shows that the use of lookahead signiﬁcantly improves this error bound. Additionally, our

bound is able to capture situations where a full rollout (i.e., m = ∞) is impossible to perform.

11

The proof of Theorem 1 is closely related to the proofs of Theorems 2-3. The proofs of The-

orems 2-3 are presented in the next section while we defer the proof of Theorem 1 to Appendix

B. We note that the above result is fundamentally diﬀerent from the conclusion in Theorem 4 in

Efroni et al. (2019) where one requires a condition on m and H for convergence when one uses Jk

instead of using T H−1Jk in Step 2 of the algorithm. Here, we have shown that even when one uses

T H−1Jk, one may need large m + H for convergence due to the use of function approximation.

We can additionally characterize the approximation error of our iterates, Jk, by computing

bounds on the asymptotic error lim supk→∞ (cid:107)Jk − J ∗(cid:107)∞ . The bounds along with their derivations

can be found in Appendix D. The corresponding ﬁnite-time bounds can be easily obtained from

the proof of Proposition 4 in Appendix D. It is important to note that the upper bounds on

(cid:107)J µk − J ∗(cid:107)∞ and (cid:107)Jk − J ∗(cid:107)∞ illustrate that J µk approximates J ∗ much better than Jk does. Thus,

algorithms need not wait for the value function estimates to converge before the corresponding

policies reach near optimality. In Bertsekas (2021), it is noted that, in reinforcement learning to

play computer games or board games, it is not uncommon during training to get a relatively crude

estimate of the value function, which is improved by lookahead and m-step return during actual

game play. Our analysis would also apply to this situation – we have not explicitly diﬀerentiated

between training and game play in our analysis.

Theorem 1 can be used to make the following observation: how close J µk is to J ∗ depends on

four factors – the representation power of the feature vectors and the feature vectors themselves

(δapp, δF V ), the amount of lookahead (H), the extent of the rollout (m) and the approximation in

the policy determination and policy evaluation steps (εLA and εP E). Further, it is easy to see that

lookahead and rollout help mitigate the eﬀect of feature vectors and their ability to represent the

value functions.

4. Gradient Descent Algorithm

Solving the least-squares problem in Algorithm 1 involves a matrix inversion, which can be com-

putationally diﬃcult. So we propose an alternative algorithm which performs ηk steps of gradient

12

Algorithm 2 Gradient Descent Algorithm
Input: θ0, m, H, feature vectors {φ(i)}i∈S, φ(i) ∈ Rd, and Dk, which is the set of states for which

we evaluate the current policy at iteration k.

1: k = 0, J0 = Φθ0.

2: Let µk+1 be such that (cid:13)

(cid:13)T HJk − Tµk+1T H−1Jk

(cid:13)
(cid:13)∞

≤ εLA.

3: Compute ˆJ µk+1(i) = T m

µk+1

T H−1Jk(i) + wk+1(i) for i ∈ Dk.

4: θk+1,0 := θk. For (cid:96) = 1, 2, . . . , ηk+1, iteratively compute the following:

θk+1,(cid:96) = θk+1,(cid:96)−1 − γ∇θc(θ; ˆJ µk+1)|θk+1,(cid:96)−1,

(9)

where

c(θ; ˆJ µk+1) :=

1
2

(cid:88)

(cid:16)

i∈D

(Φθ)(i) − ˆJ µk+1(i)

(cid:17)2

,

and Φ is a matrix whose rows are the feature vectors.

5: Deﬁne

and set

θk+1 = θk+1,ηk+1,

Jk+1 = Φθk+1.

6: Set k ← k + 1. Go to 2.

descent with stepsize γ at each iteration k, where the gradient refers to the gradient of the least-

squares objective in (5).

The gradient descent-based algorithm is presented in Algorithm 2.

In order to present our main result for the gradient descent version of our algorithm, we deﬁne

˜θµk for any policy µk which will be used in the proof of the theorem:

˜θµk := arg min

θ

1
2

(cid:13)
(cid:13)ΦDk θ − Pk(T m
µk

T H−1Jk−1 + wk)(cid:13)
2
(cid:13)
2

.

Note that

Φ˜θµk = Mk(T m
µk

T H−1Jk−1 + wk),

13

(10)

where Mk is deﬁned in (6). Thus, ˜θµk represents the function approximation of the estimate of

J µk obtained from the m-step return.

We now present two propositions which will be used in the subsequent theorems to obtain bounds

on the convergence of approximate policy iteration with function approximation when gradient

descent is employed to estimate the least-squares objective in each iteration.

Proposition 1.

where

and

(cid:107)J µk+1 − J ∗(cid:107)∞ ≤ αH (cid:107)J µk − J ∗(cid:107)∞ +

2αH (cid:107)J µk − Jk(cid:107)∞ + εLA
1 − α

,

(cid:107)J µk − Jk(cid:107)∞ ≤ ak (cid:107)Jk−1 − J µk−1(cid:107)∞ + bk,
(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

αηk
GD,γ(αm+H−1δF V + 1)

ak := αm+H−1δF V +

(11)

(12)

bk := (1 +

(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

αηk

GD,γ)(

αm + αm+H−1
1 − α

δF V + δapp + δF V εP E) +

(cid:112)|S| (cid:107)Φ(cid:107)∞
(1 − α)σmin,Φ

αηk

GD,γ,

where σmin,Φ is the smallest singular value in the singular value decomposition of Φ and αGD,γ :=

supk maxi |1 − γλi(Φ(cid:62)
Dk

ΦDk )|, where λi denotes the i-th largest eigenvalue of a matrix.

The proof of Proposition 1 is presented later in this section. Using Proposition 1 and iterating

in the special case where ak is constant or upper bounded by a constant a, where 0 < a < 1, and

bk is constant or upper bounded by a constant b, where 0 < b, we get the following:

Proposition 2. When ak ≤ a for all k where 0 < a < 1, and bk ≤ b, for all k, where 0 < b, the

following holds:

(cid:107)J µk − J ∗(cid:107)∞ ≤

αk(H)
1 − α

+

2αH
1 − α

k max(αH, a)k−1 (cid:107)J µ0 − J0(cid:107)∞ +

2αH b
1−a + εLA
(1 − α)(1 − αH)

.

14

Using Proposition 2, we can get Theorems 2 and 3 which give us ﬁnite-time and asymptotic

bounds for the cases where ηk is constant and ηk is increasing to inﬁnity, respectively:

Theorem 2. Suppose that γ, m, and H satisfy

and

γ <

d inf k

1
(cid:13)
(cid:13)Φ(cid:62)
Dk

,

ΦDk

(cid:13)
(cid:13)

2

∞

m + H > 1 + log(2δF V )/ log(1/α).

(13)

(14)

Furthermore, consider the case where ηk is a constant, which we call η, where

η > log(

3(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

)/ log(1/αGD,γ).

Then, under Assumptions 1-3, the following holds:

(cid:107)J µk − J ∗(cid:107)∞ ≤

αk(H)
1 − α
(cid:124)

+

2αH
(1 − α)2

k max(αH, aη)k−1

+

(cid:123)(cid:122)
ﬁnite-time component

(cid:125)

2αH bη
+ εLA
1−aη
(1 − α)(1 − αH)
(cid:125)
(cid:123)(cid:122)
(cid:124)
asymptotic component

,

(15)

aη := αm+H−1δF V +

(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

αη
GD,γ(αm+H−1δF V + 1)

where

and

bη := (1 +

(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

αη

GD,γ)(

αm + αm+H−1
1 − α

δF V + δapp + δF V εP E) +

(cid:112)|S| (cid:107)Φ(cid:107)∞
(1 − α)σmin,Φ

αη

GD,γ.

Taking limits on both sides as k → ∞, we have the following asymptotic bound:

lim sup
k→∞

(cid:107)J µk − J ∗(cid:107)∞ ≤

2αH bη
+ εLA
1−aη
(1 − α)(1 − αH)

.

Theorem 3. Consider ηk where ηk is increasing and ηk → ∞. Suppose that γ, m, and H satisfy

(13) and (14). Then, under Assumptions 1-3, the following holds:

lim sup
k→∞

(cid:107)J µk − J ∗(cid:107)∞ ≤

2αH b∗
1−a∗ + εLA
(1 − α)(1 − αH)

,

(16)

15

where

and

a∗ := αm+H−1δF V

b∗ :=

αm + αm+H−1
1 − α

δF V + δapp + δF V εP E.

Before we present the proofs of Propositions 1-2 and Theorems 2-3, we make the following

remarks:

In the case where ηk is constant, i.e., ηk = η, when γ is suﬃciently small and η is suﬃciently

large, we have exponential rate of convergence to the asymptotic error, assuming that m and H

are suﬃciently large. When we increase η, our asymptotic error becomes smaller until it reaches

the asymptotic error of the least-squares algorithm, i.e., when η → ∞, we recover the asymptotic

error of Algorithm 1.

If it is diﬃcult to ascertain whether η is suﬃciently large, one can consider an increasing sequence

ηk such that ηk → ∞. For such a sequence, our asymptotic error term is the same as that of the

least-squares problem.

Proof of Proposition 1 We break the proof of the the proposition into three steps.

Step 1: In this step, since θk is obtained by taking ηk steps of gradient descent towards ˜θµk beginning

from θk−1, we show that the following holds:

(cid:13)
(cid:13)θk − ˜θµk
(cid:13)

(cid:13)
(cid:13)
(cid:13)2

≤ αηk

GD,γ

(cid:13)
(cid:13)θk−1 − ˜θµk
(cid:13)

(cid:13)
(cid:13)
(cid:13)2

,

where αGD,γ := supk maxi |1 − γλi(Φ(cid:62)
Dk

ΦDk )|, where λi denotes the i-th largest eigenvalue of a

matrix.

We note that since

0 < λi(Φ(cid:62)
Dk

ΦDk ) ≤ (cid:13)

(cid:13)Φ(cid:62)
Dk

ΦDk

(cid:13)
2
(cid:13)
2

≤ d (cid:13)

(cid:13)Φ(cid:62)
Dk

ΦDk

(cid:13)
2
(cid:13)

∞

≤ d sup

k

(cid:13)
(cid:13)Φ(cid:62)
Dk

ΦDk

(cid:13)
2
(cid:13)

∞

,

αGD,γ < 1 when γ <

1
(cid:13)
(cid:13)Φ(cid:62)
(cid:13)
Dk

ΦDk

(cid:13)
(cid:13)
(cid:13)

2

∞

.

d supk

16

Proof of Step 1: Recall that the iterates in Equation (9) can be written as follows:

θk,(cid:96) = θk,(cid:96)−1 − γ∇θc(θ; ˆJ µk )|θk,(cid:96)−1 = θk,(cid:96)−1 − γ

(cid:16)

Φ(cid:62)

Dk−1

ΦDk−1θk,(cid:96)−1 − Φ(cid:62)

Dk−1

Pk−1(T m
µk

T H−1Jk + wk−1)

(cid:17)

.

Since

0 = ∇θc(θ; ˆJ µk )|˜θµk = Φ(cid:62)

Dk−1

ΦDk−1

˜θµk − Φ(cid:62)

Dk−1

Pk−1(T m
µk

T H−1Jk + wk−1),

we have the following:

θk,(cid:96) = θk,(cid:96)−1 − γ

(cid:16)

Φ(cid:62)

Dk−1

+ Φ(cid:62)

Dk−1

Pk−1(T m
µk

ΦDk−1θk,(cid:96)−1 − Φ(cid:62)
(cid:17)

T H−1Jk + wk−1)

ΦDk−1

˜θµk − Φ(cid:62)

Dk−1

Pk−1(T m
µk

T H−1Jk + wk−1)

Dk−1

= θk,(cid:96)−1 − γΦ(cid:62)

Dk−1

ΦDk−1(θk,(cid:96)−1 − ˜θµk ).

Subtracting ˜θµk from both sides gives:

θk,(cid:96) − ˜θµk = θk,(cid:96)−1 − ˜θµk − γΦ(cid:62)

Dk−1

ΦDk−1(θk,(cid:96)−1 − ˜θµk )

= (I − γΦ(cid:62)

Dk−1

ΦDk−1)(θk,(cid:96)−1 − ˜θµk ).

Thus,

(cid:13)
(cid:13)θk,(cid:96) − ˜θµk
(cid:13)

(cid:13)
(cid:13)
(cid:13)2

=

≤

Dk−1

(cid:13)
(cid:13)
(cid:13)(I − γΦ(cid:62)
(cid:13)
(cid:13)
(cid:13)I − γΦ(cid:62)

ΦDk−1

Dk−1

≤ max
i

≤ max
i

|λi(I − γΦ(cid:62)

|1 − γλi(Φ(cid:62)

(cid:13)
(cid:13)
(cid:13)2

(cid:13)
ΦDk−1)(θk,(cid:96)−1 − ˜θµk )
(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)θk,(cid:96)−1 − ˜θµk
(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)θk,(cid:96)−1 − ˜θµk
(cid:13)
(cid:13)
(cid:13)θk,(cid:96)−1 − ˜θµk
(cid:13)
(cid:13)
(cid:13)θk,(cid:96)−1 − ˜θµk
(cid:13)

ΦDk−1)|

ΦDk−1)|

Dk−1

Dk−1

(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)2

,

max
i

≤ sup
k

(cid:124)

|1 − γλi(Φ(cid:62)
Dk

(cid:123)(cid:122)
=:αGD,γ

ΦDk )|
(cid:125)

where λi denotes the i-th largest eigenvalue of a matrix.

Iterating over k, the following holds:

(cid:13)
(cid:13)θk − ˜θµk
(cid:13)

(cid:13)
(cid:13)
(cid:13)2

=

(cid:13)
(cid:13)
(cid:13)θk,ηk − ˜θµk
(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)θk,0 − ˜θµk
(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)θk−1 − ˜θµk
(cid:13)
(cid:13)
(cid:13)2

GD,γ

GD,γ

.

≤ αηk

= αηk

Step 2 : Using Step 1 and matrix norm properties, we obtain the following bound on (cid:13)

(cid:13)Jµk − Jk

(cid:107)J µk − Jk(cid:107)∞ ≤ ak (cid:107)Jk−1 − J µk−1(cid:107)∞ + bk,
(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

αηk
GD,γ(αm+H−1δF V + 1)

ak := αm+H−1δF V +

17

(cid:13)
(cid:13)∞

:

(17)

and

bk := (1 +

(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

αηk

GD,γ)(

αm + αm+H−1
1 − α

δF V + δapp + δF V εP E) +

(cid:112)|S| (cid:107)Φ(cid:107)∞
(1 − α)σmin,Φ

αηk

GD,γ.

Proof of Step 2: Using equivalence and sub-multiplicative properties of matrix norms, we have

the following:

1
(cid:107)Φ(cid:107)∞

(cid:13)
(cid:13)Φθk − Φ˜θµk
(cid:13)

(cid:13)
(cid:13)
(cid:13)∞

≤

≤

≤ αηk

GD,γ

(cid:13)
(cid:13)
(cid:13)θk − ˜θµk
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)θk − ˜θµk
(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)θk−1 − ˜θµk
(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)Φθk−1 − Φ˜θµk
(cid:13)
(cid:13)
(cid:13)Φθk−1 − Φ˜θµk
(cid:13)
(cid:13)
(cid:13)Jk−1 − Φ˜θµk
(cid:13)

1
σmin,Φ
(cid:112)|S|
αηk
σmin,Φ
(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)∞

αηk

αηk

GD,γ

GD,γ

GD,γ

(cid:13)
(cid:13)
(cid:13)∞

,

≤

≤

≤

=⇒

(cid:13)
(cid:13)Jk − Φ˜θµk
(cid:13)

(cid:13)
(cid:13)
(cid:13)∞

where σmin,Φ is the smallest singular value in the singular value decomposition of Φ and the last

line follows from the fact that Jk := Φθk.

The above implies the following:

(cid:107)J µk − Jk(cid:107)∞ ≤

(cid:13)
(cid:13)Φ˜θµk − J µk
(cid:13)

(cid:13)
(cid:13)
(cid:13)∞

+

(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

αηk

GD,γ

= (cid:13)

(cid:13)Mk(T m
µk

T H−1Jk−1 + wk) − J µk (cid:13)
(cid:13)∞

where the equality follows from (10).
(cid:13)
(cid:13)Jk−1 − Φ˜θµk
(cid:13)

Now we bound

(cid:13)
(cid:13)
(cid:13)∞

as follows:

(cid:13)
(cid:13)Jk−1 − Φ˜θµk
(cid:13)
(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

+

(cid:13)
(cid:13)
(cid:13)∞

αηk

GD,γ

(cid:13)
(cid:13)Jk−1 − Φ˜θµk
(cid:13)

(cid:13)
(cid:13)
(cid:13)∞

,

(18)

(cid:13)
(cid:13)Jk−1 − Φ˜θµk
(cid:13)

(cid:13)
(cid:13)
(cid:13)∞

≤ (cid:107)Jk−1 − J µk−1(cid:107)∞ + (cid:107)J µk−1 − J µk (cid:107)∞ +

(cid:13)
(cid:13)J µk − Φ˜θµk
(cid:13)

(cid:13)
(cid:13)
(cid:13)∞

≤ (cid:107)Jk−1 − J µk−1(cid:107)∞ +

≤ (cid:107)Jk−1 − J µk−1(cid:107)∞ +

1
1 − α
1
1 − α

+

(cid:13)
(cid:13)J µk − Φ˜θµk
(cid:13)

(cid:13)
(cid:13)
(cid:13)∞

+ (cid:13)

(cid:13)J µk − Mk(T m
µk

T H−1Jk−1 + wk)(cid:13)
(cid:13)∞

,

(19)

18

where

last

the
line
follows
T H−1Jk−1 + wk) − J µk (cid:13)
(cid:13)∞

(cid:13)
(cid:13)Mk(T m
µk

from (10). We

introduce Lemma

1

to upper bound

as follows:

Lemma 1.

(cid:13)
(cid:13)Mk(T m
µk

T H−1Jk−1 + wk) − J µk (cid:13)
(cid:13)∞

≤ αm+H−1δF V (cid:107)Jk−1 − J µk−1(cid:107)∞ +

αm + αm+H−1
1 − α

δF V + δapp + δF V εP E.

The proof of Lemma 1 is in Appendix A.

Putting (18), (19), and Lemma 1 together, we get the following:

(cid:107)J µk − Jk(cid:107)∞ ≤ ak (cid:107)Jk−1 − J µk−1(cid:107)∞ + bk,

ak := αm+H−1δF V +

(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

αηk
GD,γ(αm+H−1δF V + 1)

and

bk := (1 +

(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

αηk

GD,γ)(

αm + αm+H−1
1 − α

δF V + δapp + δF V εP E) +

(cid:112)|S| (cid:107)Φ(cid:107)∞
(1 − α)σmin,Φ

αηk

GD,γ.

Step 3: We will establish the following bound on Tµk+1T H−1J µk using the contraction property of

Bellman operators and property in (3):

−Tµk+1T HJ µk ≤ 2αHfke − T HJ µk + εLAe.

Using properties in (3) and monotonicity, we will repeatedly apply Tµk+1 to both sides and take

limits to obtain the following:

J µk+1 − T HJ µk ≥ −

2αH (cid:107)J µk − Jk(cid:107)∞ + εLA
1 − α

e.

Proof of Step 3: We begin by noting that

−Tµk+1T HJ µk ≤ −Tµk+1T H−1J µk − Tµk+1T H−1Jk + Tµk+1T H−1Jk

(a)

≤ αH (cid:107)J µk − Jk(cid:107)∞ e − Tµk+1T H−1Jk

≤ αH (cid:107)J µk − Jk(cid:107)∞ e − T HJk + εLAe

= αH (cid:107)J µk − Jk(cid:107)∞ e − T HJk − T HJ µk + T HJ µk + εLAe

(b)

≤ 2αH (cid:107)J µk − Jk(cid:107)∞ e − T HJ µk + εLAe,

19

where e is the vector of all 1s, (a) and (b) follow from the contraction property of T H and Tµk+1.

The rest of the proof is similar to the arguments in Bertsekas and Tsitsiklis (1996), Bertsekas

(2019); however, we have to explicitly incorporate the role of lookahead (H) in the remaining steps

of the proof. Suppose that we apply the Tµk+1 operator (cid:96) − 1 times to both sides. Then, due to

monotonicity and the fact Tµ(J + ce) = Tµ(J) + αce, for any policy µ, we have the following:

T (cid:96)

µk+1

T HJ µk ≤ α(cid:96)(2αH (cid:107)J µk − Jk(cid:107)∞ + εLA)e + T (cid:96)+1
µk+1

T HJ µk .

Using a telescoping sum, we get the following inequality:

T j

µk+1

T HJ µk − T HJ µk ≥ −

j
(cid:88)

(cid:96)=1

α(cid:96)−1(2αH (cid:107)J µk − Jk(cid:107)∞ + εLA)e.

Taking limits as j → ∞, we obtain the following:

J µk+1 − T HJ µk ≥ −

2αH (cid:107)J µk − Jk(cid:107)∞ + εLA
1 − α

e.

The rest of proof is straightforward. Subtracting J ∗ from both sides of the previous inequality

and using the contraction property of T , we get:

αH (cid:107)J ∗ − J µk (cid:107)∞ e +

2αH (cid:107)J µk − Jk(cid:107)∞ + εLA
1 − α

e.

≥ J ∗ − T HJ µk +

2αH (cid:107)J µk − Jk(cid:107)∞ + εLA
1 − α

e

≥ J ∗ − J µk

≥ 0,

which implies that

(cid:107)J µk+1 − J ∗(cid:107)∞ ≤ αH (cid:107)J µk − J ∗(cid:107)∞ +

2αH (cid:107)J µk − Jk(cid:107)∞ + εLA
1 − α

,

(20)

since J ∗ ≥ J µ for all policies µ. The above, together with the inequality in (17) gives us Proposition

1.

(cid:3)

We now prove Proposition 2 by iterating over k using Proposition 1.

20

Proof of Proposition 2 First, noting our assumptions in Proposition 1 that ak ≤ a for all k

where 0 < a < 1, and bk ≤ b for all k, where 0 < b, we iterate over k using (12) to obtain a bound

on (cid:107)J µk − Jk(cid:107)∞ as follows:

(cid:107)J µk − Jk(cid:107)∞ ≤ ak (cid:107)J0 − J µ0(cid:107)∞ + b

k−1
(cid:88)

k−1
(cid:89)

j=0

m=j

am.

(21)

Now, we iterate over k in (11) to get the following:

(cid:107)J µk − J ∗(cid:107)∞ ≤ αk(H−1) (cid:107)J µ0 − J ∗(cid:107)∞ +

2αH
1 − α

k−1
(cid:88)

(cid:96)=0

α(k−(cid:96)−1)(H−1) (cid:107)J µ(cid:96) − J(cid:96)(cid:107)∞ +

εLA
(1 − α)(1 − αH−1)

≤

αk(H−1)
1 − α

+

2αH
1 − α

k−1
(cid:88)

(cid:96)=0

α(k−(cid:96)−1)(H−1) (cid:107)J µ(cid:96) − J(cid:96)(cid:107)∞ +

εLA
(1 − α)(1 − αH−1)

.

(22)

Combining the inequalities in (21) and (22) gives us the following:

(cid:107)J µk − J ∗(cid:107)∞ ≤

+

≤

≤

≤

=

αk(H−1)
1 − α

+

2αH
1 − α

k−1
(cid:88)

(cid:96)=0

εLA
(1 − α)(1 − αH−1)

αk(H−1)
1 − α

+

2αH
1 − α

k−1
(cid:88)

(cid:96)=0

α(k−(cid:96)−1)(H−1)(cid:104)

a(cid:96) (cid:107)J0 − J µ0(cid:107)∞ + b

(cid:96)−1
(cid:88)

(cid:96)−1
(cid:89)

am(cid:105)

j=0

m=j

α(k−(cid:96)−1)(H−1)(cid:104)

a(cid:96) (cid:107)J0 − J µ0(cid:107)∞ +

(cid:105)

b
1 − a

+

εLA
(1 − α)(1 − αH−1)

αk(H−1)
1 − α

+

2αH
1 − α

αk(H−1)
1 − α

αk(H−1)
1 − α

+

+

2αH
1 − α

2αH
1 − α

(cid:107)J0 − J µ0(cid:107)∞

(cid:107)J0 − J µ0(cid:107)∞

k−1
(cid:88)

(cid:96)=0
k−1
(cid:88)

(cid:96)=0

α(k−(cid:96)−1)(H−1)a(cid:96) +

2αH b

1−a + εLA

(1 − α)(1 − αH−1)

max(αH−1, a)k−1 +

2αH b

1−a + εLA

(1 − α)(1 − αH−1)

k max(αH−1, a)k−1 (cid:107)J µ0 − J0(cid:107)∞ +

2αH b

1−a + εLA

(1 − α)(1 − αH−1)

,

where the last inequality follows from the assumption that 0 < a < 1. (cid:3)

We now use Proposition 2 to prove Theorems 2-3.

Proof of Theorem 2 When ηk = η, the following holds:

ak = aη := αm+H−1δF V +

(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

αη
GD,γ(αm+H−1δF V + 1)

and

bk = bη := (1 +

(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

αη

GD,γ)(

αm + αm+H−1
1 − α

δF V + δapp + δF V εP E) +

(cid:112)|S| (cid:107)Φ(cid:107)∞
(1 − α)σmin,Φ

αη

GD,γ.

Note that from our assumptions on m, γ, and H, 0 < aη < 1 and 0 < bη. To see that aη < 1, observe

from our assumptions in Theorem 2 that

αm+H−1δF V <

1
2

(23)

21

and

(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

αη
GD,γ(αm+H−1δF V + 1) <

(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

αη

GD,γ

3
2

<

1
2

.

Putting the above two lines together gives us
(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

αm+H−1δF V +

αη
GD,γ(αm+H−1δF V + 1) < 1.

So, we can directly use Proposition 2 to obtain (15). (cid:3)

Proof of Theorem 3 Take any constant 0 < c∗ < 1 − αm+H−1δF V , where c∗ is a margin of error.

Deﬁne k(c∗) to be the smallest value k(c∗) such that:

ηk(c∗) > log

(cid:16) 1
c∗

(cid:112)|S| (cid:107)Φ(cid:107)∞
σmin,Φ

(

αm + αm+H−1
1 − α

δF V + δapp + δF V εP E +

1
1 − α

(cid:17)

)

/ log(1/αGD,γ).

(24)

We know such a k(c∗) exists since ηk → ∞. We deﬁne ac∗ and bc∗ as follows:

ac∗ := αm+H−1δF V + c∗,

bc∗ :=

αm + αm+H−1
1 − α

δF V + δapp + δF V εP E + c∗.

It is easy to see from (24) and the deﬁnitions of ak and bk in Proposition 1 that ak ≤ ac∗ and

bk ≤ bc∗ when k > k(c∗), since 0 < αGD,γ < 1 from our assumption on γ in Theorem 3.

From our assumptions on c∗, m, γ, and H, 0 < ac∗ < 1 and 0 < bc∗, where we use a similar

technique as in the proof of Theorem 2 with αm+H−1δF V < 1 − c∗ in place of (23) to show that

ac∗ < 1. So, we can use Proposition 2 and begin iterating at k(c∗) to obtain ﬁnite time bounds on

(cid:107)J µk − J ∗(cid:107)∞ as follows:

(cid:107)J µk − J ∗(cid:107)∞

≤

α(k−k(c∗))(H)
1 − α

(cid:124)

+

2αH
1 − α

(k − k(c∗)) max(ac∗, αH)k−k(c∗)−1 (cid:13)
(cid:123)(cid:122)
ﬁnite-time component

(cid:13)J µk(c∗) − Jk(c∗)

+

(cid:13)
(cid:13)∞
(cid:125)

2αH bc∗
+ εLA
1−ac∗
(1 − αH)(1 − α)
(cid:125)
(cid:123)(cid:122)
(cid:124)
asymptotic component

.

(25)

22

Using Proposition 1, we can upper bound (cid:13)

(cid:13)
(cid:13)J µk(c∗) − Jk(c∗)

(cid:13)
(cid:13)∞

≤

where ak and bk are deﬁned in Proposition 1.

i=1

Taking limits on both sides of (25), we get the following:

(cid:13)
(cid:13)∞

as follows:

(cid:13)J µk(c∗) − Jk(c∗)
k(c∗)
(cid:89)

ai (cid:107)J0 − J µ0(cid:107)∞ +

k(c∗)
(cid:88)

bj

k(c∗)
(cid:89)

ak,

j=1

k=j+1

lim sup (cid:107)J µk − J ∗(cid:107)∞ ≤

2αH bc∗
+ εLA
1−ac∗
(1 − αH)(1 − α)

.

Since the above holds for all c∗ > 0, we get Theorem 3.

(cid:3)

Looking through the steps of our proof, one can obtain ﬁnite-time bounds for the case where ηk

is increasing. However, the algorithm consists of two loops: one corresponding to policy iteration

and the other corresponding to gradient descent within each policy iteration step. It is hard to

compare the relative complexities of each step within these loops. Therefore, a ﬁnite-time analysis

does not shed much light into the amount of computations needed to execute the algorithm with

an increasing sequence ηk. However, it is interesting to note that for all k > k(c∗), the algorithm

converges exponentially fast in the number of policy iteration steps although the number of gradient

descent steps within each policy iteration step is increasing.

5. Conclusion

Practical RL algorithms that deal with large state spaces implement some form of approximate

policy iteration. In traditional analyses of approximate policy iteration, for example in Bertsekas

(2019), it is assumed that there is an error in the policy evaluation step and an error in the policy

improvement step. In this paper, we seek to understand the role of function approximation in the

policy evaluation step and the associated changes that one has to make to the approximate policy

iteration algorithm (such as lookahead) to counteract the eﬀect of function approximation. Our

main conclusion is that lookahead provides two beneﬁts: (i) it mitigates the eﬀects of function

approximation, rollout and the choice of speciﬁc feature vectors, and (ii) from a theoretical per-

spective, it improves the upper bound on the error in approximate policy iteration from 1/(1 − α)2

to 1/(1 − αH)(1 − α).

23

Possible directions for future work include the following:

• For problems with a terminal state, it would be interesting to consider cases where the value

function of a given policy is estimated using a full rollout which provides an unbiased estimate as

in Tsitsiklis (2002).

• In game playing applications, gradient descent is commonly used to estimate the value function,

but temporal-diﬀerence learning is used in other applications. It would be interesting to extend

our results to the case of TD learning-based policy evaluation.

• While neural networks are not linear function approximators, recent results on the NTK

analysis of neural networks suggest that they can be approximated as linear combinations of basis

functions Jacot et al. (2018), Du et al. (2018), Arora et al. (2019), Ji and Telgarsky (2019), Cao

and Gu (2019). Thus, to the extent that the NTK approximation is reasonable, our results can

potentially shed light on why the combination of the representation capability of neural networks

and tree-search methods work well in practice, although further work is necessary to make this

connection precise.

Appendix A: Proof of Lemma 1

Proof of Lemma 1

+ δF V εP E

+ (cid:107)Mkwk(cid:107)∞

+ (cid:107)Mk(cid:107)∞ (cid:107)wk(cid:107)∞

T H−1Jk−1 + wk) − J µk (cid:13)
(cid:13)
(cid:13)Mk(T m
(cid:13)∞
µk
T H−1Jk−1 − J µk (cid:13)
≤ (cid:13)
(cid:13)MkT m
(cid:13)∞
µk
T H−1Jk−1 − J µk (cid:13)
≤ (cid:13)
(cid:13)MkT m
(cid:13)∞
µk
T H−1Jk−1 − J µk (cid:13)
≤ (cid:13)
(cid:13)MkT m
(cid:13)∞
µk
T H−1Jk−1 − MkJ µk + MkJ µk − J µk (cid:13)
= (cid:13)
(cid:13)MkT m
(cid:13)∞
µk
T H−1Jk−1 − MkJ µk (cid:13)
≤ (cid:13)
+ (cid:107)MkJ µk − J µk (cid:107)∞ + δF V εP E
(cid:13)MkT m
(cid:13)∞
µk
T H−1Jk−1 − J µk (cid:13)
(cid:13)
(cid:13)T m
(cid:13)∞
µk
(cid:13)T H−1Jk−1 − J µk (cid:13)
(cid:13)
≤ αm (cid:107)Mk(cid:107)∞
(cid:13)∞
(cid:13)T H−1Jk−1 − J ∗ + J ∗ − J µk (cid:13)
(cid:13)
(cid:13)∞
(cid:13)T H−1Jk−1 − J ∗(cid:13)
(cid:13)
(cid:13)∞
≤ αm+H−1 (cid:107)Mk(cid:107)∞ (cid:107)Jk−1 − J ∗(cid:107)∞ +

+ (cid:107)MkJ µk − J µk (cid:107)∞ + δF V εP E

≤ αm (cid:107)Mk(cid:107)∞

≤ αm (cid:107)Mk(cid:107)∞

+ δapp + δF V εP E

(cid:107)Mk(cid:107)∞ + δapp + δF V εP E

≤ (cid:107)Mk(cid:107)∞

+ sup
k,µk

+ δF V εP E

αm
1 − α

(cid:107)MkJ µk − J µk (cid:107)∞ + δF V εP E

+ αm (cid:107)Mk(cid:107)∞ (cid:107)J ∗ − J µk (cid:107)∞ + δapp + δF V εP E

24

(cid:3)

≤ αm+H−1 (cid:107)Mk(cid:107)∞ (cid:107)Jk−1 − J µk−1(cid:107)∞ +

≤ αm+H−1 (cid:107)Mk(cid:107)∞ (cid:107)Jk−1 − J µk−1 + J µk−1 − J ∗(cid:107)∞ +
αm + αm+H−1
1 − α
αm + αm+H−1
1 − α

≤ αm+H−1δF V (cid:107)Jk−1 − J µk−1(cid:107)∞ +

αm
1 − α

(cid:107)Mk(cid:107)∞ + δapp + δF V εP E

(cid:107)Mk(cid:107)∞ + δapp + δF V εP E

δF V + δapp + δF V εP E.

Appendix B: Proof of Theorem 1

Remark 2. The proof of Theorem 1 is somewhat simpler than that of Theorems 2-3 but uses

many of the same ideas. The proof of Theorem 1 skips Steps 1 and 2 in the proof of Proposition

1 and instead uses Lemma 1 to obtain an analogous result to Step 2. The rest of the proof of

Proposition 1 also applies to the proof of Theorem 1.

Proof of Theorem 1 Consider policies µk and µk+1, where µ0 can be taken to be any arbitrary

policy. We have the following:

−Tµk+1T HJ µk ≤ −Tµk+1T H−1J µk − Tµk+1T H−1Jk + Tµk+1T H−1Jk

(a)

≤ αH (cid:107)J µk − Jk(cid:107)∞ e − Tµk+1T H−1Jk

≤ αH (cid:107)J µk − Jk(cid:107)∞ e − T HJk + εLAe

= αH (cid:107)J µk − Jk(cid:107)∞ e − T HJk − T HJ µk + T HJ µk + εLAe

(b)

≤ 2αH (cid:107)J µk − Jk(cid:107)∞ e − T HJ µk + εLAe,

(26)

where e is the vector of all 1s, (a) and (b) follow from the contraction property of T H and Tµk+1.
from (7), we can further bound

Since (cid:107)J µk − Jk(cid:107)∞ = (cid:13)

T H−1Jk−1 + wk) − J µk (cid:13)
(cid:13)∞

(cid:13)Mk(T m
µk

(cid:107)J µk − Jk(cid:107)∞ using Lemma 1. We now rewrite our recursion from Lemma 1:

Suppose that we deﬁne β ∈ (0, 1) as follows:

β := αm+H−1δF V ,

(27)

where we note from our assumption in Theorem 1 that αm+H−1δF V < 1. Furthermore, we denote

τ as follows:

τ :=

αm + αm+H−1
1 − α

δF V + δapp + δF V εP E.

Then, our bound from Lemma 1 can be rewritten as the following:

(cid:107)J µk − Jk(cid:107)∞ ≤ β (cid:107)Jk−1 − J µk−1(cid:107)∞ + τ.

(28)

(29)

Iterating over k, we get that for any k,

(cid:107)J µk − Jk(cid:107)∞ ≤ (β)k (cid:107)J µ0 − J0(cid:107)∞ +

(cid:124)

(cid:123)(cid:122)
=:fk

k−1
(cid:88)

i=0

βiτ

.

(cid:125)

Putting (26) and (30) together, we have the following:

−Tµk+1T HJ µk ≤ 2αHfke − T HJ µk + εLAe.

25

(30)

(31)

The rest of the proof is similar to the arguments in Bertsekas and Tsitsiklis (1996), Bertsekas

(2019); however, we have to explicitly incorporate the role of lookahead (H) in the remaining steps

of the proof.

Suppose that we apply the Tµk+1 operator (cid:96) − 1 times. Then, due to monotonicity and the fact

that Tµ(J + ce) = Tµ(J) + αce, for any policy µ, we have the following:

−T (cid:96)+1
µk+1

T HJ µk ≤ α(cid:96)(2αHfk + εLA)e − T (cid:96)

µk+1

T HJ µk .

Using a telescoping sum, we get the following inequality:

T j

µk+1

T HJ µk − T HJ µk ≥ −

j
(cid:88)

(cid:96)=1

α(cid:96)−1(2αHfk + εLA)e.

Taking the limit as j → ∞ on both sides, we have the following:

J µk+1 − T HJ µk ≥ −

2αHfk + εLA
1 − α

e.

Rearranging terms and subtracting J ∗ from both sides, we get the following:

αH (cid:107)J ∗ − J µk (cid:107)∞ e +

2αHfk + εLA
1 − α

e ≥ J ∗ − T HJ µk +

2αHfk + εLA
1 − α

e ≥ J ∗ − J µk+1 ≥ 0,

which implies that

(cid:107)J µk+1 − J ∗(cid:107)∞ ≤ αH (cid:107)J µk − J ∗(cid:107)∞ +

2αHfk + εLA
1 − α

,

(32)

since J ∗ ≥ J µ for all policies µ.

We iterate over k > 0 to get the following:

(cid:107)J µk − J ∗(cid:107)∞ ≤ αk(H) (cid:107)J µ0 − J ∗(cid:107)∞ +

k−1
(cid:88)

(cid:96)=0

α(k−(cid:96)−1)(H) 2αHf(cid:96) + εLA

1 − α

≤

αk(H)
1 − α

+

k−1
(cid:88)

(cid:96)=0

α(k−(cid:96)−1)(H) 2αHf(cid:96) + εLA

1 − α

.

26

where

f(cid:96) := (β)(cid:96) (cid:107)J µ0 − J0(cid:107)∞ +

k−1
(cid:88)

i=0

βiτ,

and β := αm+H−1δF V and τ := αm+αm+H−1

1−α

δF V + δapp + δF V εP E.

We will now obtain ﬁnite-time bounds of (cid:107)J µk − J ∗(cid:107)∞ using the above results. The following

holds:

(cid:107)J µk − J ∗(cid:107)∞ ≤

αk(H)
1 − α

+

=

αk(H)
1 − α

+

k−1
(cid:88)

(cid:96)=0

k−1
(cid:88)

(cid:96)=0

α(k−(cid:96)−1)(H) 2αHf(cid:96) + εLA

1 − α

α(k−(cid:96)−1)(H)

2αH(cid:16)

β(cid:96) (cid:107)J µ0 − J0(cid:107)∞ + τ

1−β

(cid:17)

+ εLA

1 − α

≤

≤

≤

αk(H)
1 − α

+

2αH (cid:107)J µ0 − J0(cid:107)∞
1 − α

αk(H)
1 − α

αk(H)
1 − α

+

+

2αH (cid:107)J µ0 − J0(cid:107)∞
1 − α

2αH (cid:107)J µ0 − J0(cid:107)∞
1 − α

k−1
(cid:88)

(cid:96)=0
k−1
(cid:88)

(cid:96)=0

α(k−(cid:96)−1)(H)β(cid:96) +

2αH τ
1−β + εLA
(1 − αH)(1 − α)

max(αH, β)k−1 +

2αH τ
1−β + εLA
(1 − αH)(1 − α)

k max(αH, β)k−1 +

2αH τ
1−β + εLA
(1 − αH)(1 − α)

,

(33)

where the second to last to last inequality holds due to the assumption in Theorem 1. (cid:3)

Appendix C: A Modiﬁed Least Squares Algorithm

Suppose Step 3 of Algorithm 1 is changed to ˆJ µk+1(i) = T m

µk+1

(Jk)(i) + wk+1(i) for i ∈ Dk. Then,

it is still possible to get bounds on the performance of the algorithm when m is suﬃciently large.

With this modiﬁcation to the algorithm, we have the following:

Proposition 3. Suppose that m satisﬁes m > log(δF V )/ log(1/α), where

δF V := sup

k

(cid:107)Mk(cid:107)∞ = sup
k

(cid:13)
(cid:13)Φ(Φ(cid:62)
Dk

ΦDk )−1Φ(cid:62)
Dk

Pk

(cid:13)
(cid:13)∞

.

Then, under Assumptions 1-3, the following holds:

(cid:107)J µk − J ∗(cid:107)∞ ≤

αk(H)
1 − α
(cid:124)

+

where

and

2αH (cid:107)J µ0 − J0(cid:107)∞
1 − α

(cid:123)(cid:122)
ﬁnite-time component

k max(αH, β(cid:48))k−1

+

2αH τ (cid:48)
1−β(cid:48) + εLA
(1 − αH)(1 − α)
(cid:125)
(cid:123)(cid:122)
(cid:124)
asymptotic component

.

(cid:125)

τ (cid:48) := αmδF V ,

β(cid:48) :=

αmδF V
1 − α

+ δapp + δF V εP E,

δapp := sup
k,µk

(cid:107)MkJ µk − J µk (cid:107)∞ .

Proof of Proposition 3 The proof of Theorem 3 is identical to the proof of Theorem 1 except

for the iteration in (29). We thus give the following iteration which can be substituted in our proof

of Theorem 1:

27

(cid:107)Jk − J µk (cid:107)∞ = (cid:13)
= (cid:13)
≤ (cid:13)
≤ (cid:13)
≤ (cid:13)
= (cid:13)
≤ (cid:13)

+ (cid:107)Mkwk(cid:107)∞

Jk−1 + wk) − J µk (cid:13)
(cid:13)Mk(T m
(cid:13)∞
µk
Jk−1 + wk) − J µk (cid:13)
(cid:13)Mk(T m
(cid:13)∞
µk
Jk−1 − J µk (cid:13)
(cid:13)MkT m
(cid:13)∞
µk
Jk−1 − J µk (cid:13)
(cid:13)MkT m
(cid:13)∞
µk
Jk−1 − J µk (cid:13)
(cid:13)MkT m
(cid:13)∞
µk
Jk−1 − MkJ µk + MkJ µk − J µk (cid:13)
(cid:13)MkT m
(cid:13)∞
µk
Jk−1 − MkJ µk (cid:13)
+ (cid:107)MkJ µk − J µk (cid:107)∞ + δF V εP E
(cid:13)MkT m
(cid:13)∞
µk
Jk−1 − J µk (cid:13)
(cid:13)∞

+ (cid:107)Mk(cid:107)∞ (cid:107)wk(cid:107)∞

+ δF V εP E

+ δF V εP E

(cid:107)Mk(cid:107)∞

(cid:13)
(cid:13)T m
µk

+ sup
k,µk

≤ sup
k

(cid:107)MkJ µk − J µk (cid:107)∞ + δF V εP E

≤ αmδF V (cid:107)Jk−1 − J µk (cid:107)∞ + δapp + δF V εP E

= αmδF V (cid:107)Jk−1 − J µk−1 + J µk−1 − J µk (cid:107)∞ + δapp + δF V εP E

≤ αmδF V (cid:107)Jk−1 − J µk−1(cid:107)∞ + αmδF V (cid:107)J µk−1 − J µk (cid:107)∞ + δapp + δF V εP E

≤ αmδF V (cid:107)Jk−1 − J µk−1(cid:107)∞ +

αmδF V
1 − α

+ δapp + δF V εP E.

Substituting

and

β(cid:48) := αmδF V

τ (cid:48) :=

αmδF V
1 − α

+ δapp + δF V εP E,

in place of β and τ , respectively, in Theorem 1 and the proof of Theorem 1, we obtain Proposition

3.

Appendix D: Bounds on Iterates In Algorithm 1

In the following proposition, we present a bound on the diﬀerence between Jk and J ∗.

Proposition 4. When αm+H−1δF V < 1,

lim sup
k→∞

(cid:107)Jk − J ∗(cid:107)∞ ≤

(cid:0)1 + δF V αm(cid:1)(cid:104) 2αH τ

(cid:105)

1−β +εLA
(1−αH )(1−α)
1 − δF V αm+H−1

+ δapp + δF V εLA

,

where β and τ are deﬁned in Theorem 1.

The proof is as follows.

28

Proof of Proposition 4

(cid:107)Jk+1 − J ∗(cid:107)∞ = (cid:107)Jk+1 − J µk+1 + J µk+1 − J ∗(cid:107)∞

≤ (cid:107)Jk+1 − J µk+1(cid:107)∞ + (cid:107)J µk+1 − J ∗(cid:107)∞

≤

(cid:13)
(cid:13)
(cid:13)Mk+1T m

µk+1
+ (cid:107)J µk+1 − J ∗(cid:107)∞ + δF V εLA

T H−1Jk − J µk+1

(cid:13)
(cid:13)
(cid:13)∞

+ δF V εLA

=

(cid:13)
(cid:13)
(cid:13)Mk+1T m

µk+1
+ (cid:107)J µk+1 − J ∗(cid:107)∞ + δF V εLA

T H−1Jk − Mk+1J µk+1 + Mk+1J µk+1 − J µk+1

(cid:13)
(cid:13)
(cid:13)∞

T H−1Jk − Mk+1J µk+1

(cid:13)
(cid:13)
(cid:13)∞

+ (cid:107)Mk+1J µk+1 − J µk+1(cid:107)∞

T H−1Jk − J µk+1

(cid:13)
(cid:13)
(cid:13)∞

+ (cid:107)Mk+1J µk+1 − J µk+1(cid:107)∞

≤

(cid:13)
(cid:13)
(cid:13)Mk+1T m

µk+1

µk+1
+ (cid:107)J µk+1 − J ∗(cid:107)∞ + δF V εLA
(cid:13)
(cid:13)
(cid:13)T m
≤ (cid:107)Mk+1(cid:107)∞
+ (cid:107)J µk+1 − J ∗(cid:107)∞ + δF V εLA
(cid:13)T H−1Jk − J µk+1(cid:13)
≤ δF V αm (cid:13)
(cid:13)∞
+ (cid:107)J µk+1 − J ∗(cid:107)∞ + δF V εLA
= δF V αm (cid:13)
≤ δF V αm (cid:13)

+ δapp

+ δF V αm (cid:107)J ∗ − J µk+1(cid:107)∞ + δapp

+ δapp + (cid:107)J µk+1 − J ∗(cid:107)∞ + δF V εLA

(cid:13)T H−1Jk − J ∗ + J ∗ − J µk+1(cid:13)
(cid:13)∞
(cid:13)T H−1Jk − J ∗(cid:13)
(cid:13)∞
+ (cid:107)J µk+1 − J ∗(cid:107)∞ + δF V εLA
≤ δF V αm+H−1 (cid:107)Jk − J ∗(cid:107)∞ + δF V αm (cid:107)J ∗ − J µk+1(cid:107)∞ + δapp
+ (cid:107)J µk+1 − J ∗(cid:107)∞ + δF V εLA
= δF V αm+H−1 (cid:107)Jk − J ∗(cid:107)∞ + (cid:0)1 + δF V αm(cid:1) (cid:107)J ∗ − J µk+1(cid:107)∞ + δapp + δF V εLA.

From Theorem 1, we have that

lim sup
k→∞

(cid:107)J µk − J ∗(cid:107)∞ ≤

2αH τ
1−β + εLA
(1 − αH)(1 − α)

.

Thus, for every ε(cid:48) > 0, there exists a k(ε(cid:48)) such that for all k > k(ε(cid:48)),

(cid:107)J µk − J ∗(cid:107)∞ ≤

2αH τ
1−β + εLA
(1 − αH)(1 − α)

+ ε(cid:48).

Thus, for all k > k(ε(cid:48)), we have:

(cid:107)Jk+1 − J ∗(cid:107)∞ ≤ δF V αm+H−1 (cid:107)Jk − J ∗(cid:107)∞ + (cid:0)1 + δF V αm(cid:1)(cid:104) 2αH τ

1−β + εLA
(1 − αH)(1 − α)

+ ε(cid:48)(cid:105)

+ δapp + δF V εLA.

Iterating over k gives us:

lim sup
k→∞

(cid:107)Jk − J ∗(cid:107)∞ ≤

(cid:0)1 + δF V αm(cid:1)(cid:104) 2αH τ

1−β +εLA

(1−αH )(1−α) + ε(cid:48)(cid:105)
1 − δF V αm+H−1

+ δapp + δF V εLA

.

Figure 1

An example illustrating the necessity of the condition in Theorem 1

(a) µa

(b) µb

Since the above holds for all ε(cid:48):

lim sup
k→∞

(cid:107)Jk − J ∗(cid:107)∞ ≤

Appendix E: Counterexample

(cid:0)1 + δF V αm(cid:1)(cid:104) 2αH τ

(cid:105)

1−β +εLA
(1−αH )(1−α)
1 − δF V αm+H−1

+ δapp + δF V εLA

29

.

Even though, in practice, J µk is what we are interested in, the values Jk computed as part of

our algorithm should not go to ∞ since the algorithm would be numerically unstable otherwise.
In Appendix D, we provide a bound on (cid:107)Jk − J ∗(cid:107)∞ when m + H − 1 is suﬃciently large as in
Theorem 1. In this subsection, we show that, when this condition is not satisﬁed, Jk can become

unbounded.

The example we use is depicted in Figure 1. There are two policies, µa and µb and the transitions

are deterministic under the two policies. The rewards are deterministic and only depend on the

states. The rewards associated with states are denoted by r(x1) and r(x2), with r(x1) > r(x2).
Thus, the optimal policy is µa. We assume scalar features φ(x1) = 1 and φ(x2) = 2.

We ﬁx H = 1. The MDP follows policy µa when:

Jk(x1) > Jk(x2) =⇒ θk > 2θk.

Thus, as long as θk > 0, the lookahead policy will be µb.

We will now show that θk increases at each iteration when δF V αm+H−1 > 1. We assume that
5 . At iteration k + 1,

θ0 > 0 and Dk = {1, 2} ∀k. A straightforward computation shows that δF V = 6
suppose µk+1 = µb, our ˆJ µk+1(i) for i = 1, 2 are as follows:

ˆJ µk+1(1) = r(x1) +

m−1
(cid:88)

i=1

r(x1)αi + 2αmθk,

ˆJ µk+1(2) = r(x2) +

m−1
(cid:88)

i=1

r(x2)αi + 2αmθk.

Thus, from Step 5 of Algorithm 1:

θk+1 = arg min

2
(cid:88)

(cid:16)

(Φθ)(i) − ˆJ µk+1(i)

(cid:17)2

θ

i=1
(cid:80)m−1

i=1 αir(x1)
5
αmθk.

=⇒ θk+1 =

=⇒ θk+1 >

6
5

+

i=1 αir(x2)

2 (cid:80)m−1
5

+

6αmθk
5

Thus, since θ0 > 0 and H = 1, when 6

5 αm+H−1θk = δF V αm+H−1 > 1, θk goes to ∞.

30

Appendix F: Numerical Results

In this appendix, we test our algorithms on a grid world problem, using the same grid world

problem as in Efroni et al. (2018a) and Efroni et al. (2019).

For our simulations, we assume a deterministic grid world problem played on an N × N grid.

The states are the squares of the grid and the actions are {’up’, ’down’, ’right’, ’left’, and ’stay’},

which move the agent in the prescribed direction, if possible. In each experiment, a goal state is

chosen uniformly at random to have a reward of 1, while each other state has a ﬁxed reward drawn

uniformly from [−0.1, 0.1]. Unless otherwise mentioned, for the duration of this section, N = 25

and α = 0.9.

In order to perform linear function approximation, we prescribe a feature vector for each state.

In this section, we focus on three particular choices:

1. Random feature vectors: each entry of the matrix Φ is an independent N (0, 1) random variable

2. Designed feature vectors: the feature vector for a state with coordinates (x, y) is [x, y, d, 1]T ,

where d is the number of steps required to reach the goal from state (x, y)

3. Indicator vectors: the feature vector for each state i is a N 2-dimensional indicator vector

where only the i-th entry is nonzero

Recall that our theorems suggest that the amount of lookahead and return depends on the choice

of the feature vectors. Our experiments support this observation as well. The amount of lookahead

and m-step return required is high (often over 30) for random feature vectors, but we are able

to signiﬁcantly reduce the amount required by using the designed feature vectors which better

represent the states.

We test Algorithm 1 in each of our experiments, using a starting state of J0 = θ0 = 0. All plots

in this section graph an average over 20 trials, where each trial has a ﬁxed random choice of Dk,

the set of states used for policy evaluation. Error bars show the standard deviation of the mean.

The code used to produce these graphs is included in the Supplementary Material.

F.1. The eﬀect of m and H on convergence

In Figure 2, we showed how H and m aﬀect convergence of the iterates Jk to J ∗. When m and H are

small, the value of Jk sometimes diverges. If the value diverges for even one trial, then the average

over trials of (cid:107)Jk − J ∗(cid:107)∞ also increases exponentially with k. However, if the average converges

for all trials, then the plot is relatively ﬂat. The m or H required for convergence depends on the

parameter δF V deﬁned in Theorem 1. Over 20 trials, the average value of δF V for each of our choices

of feature vectors are 30.22, 16.29, and 1.0, respectively. As showed through a counter-example in

the main body of the paper, in general, one needs m + H − 1 > log(δF V )/ log(1/α) for convergence.

However, in speciﬁc examples, it is possible for convergence to for smaller values of m + H. For

31

Figure 2

(Top) For random feature vectors, as m and H increase, the value Jk eventually stops diverging. (Bot-

tom) For designed feature vectors, a smaller amount of lookahead and m-step return are needed to

prevent Jk from diverging.

example, in our grid word model, log(16.29)

log(1/0.9) ≈ 26.5, but we will observe that such a large amount of

m + H is not required for convergence.

In Figure 2, it is diﬃcult to see how H and m aﬀect the probability of divergence, as a function

of the representative states chosen to be sampled. Therefore, we introduce Figure 3. These plots

show the proportion of trials in which the distance (cid:107)Jk − J ∗(cid:107)∞ exceeded 105 after 30 iterations of

32

Figure 3 We plot the probability that (cid:107)Jk − J ∗(cid:107)∞ diverges as a function of H and m. For the ﬁrst plot, m = 3

(a) Varying H

(b) Varying m

and for the second plot, H = 3. In both cases, the algorithm never diverges once H + m is large enough,

though a smaller amount of lookahead or m-step return are needed for the designed feature vectors.

Figure 4 We plot the ﬁnal value of (cid:107)J µk − J ∗(cid:107)∞ after 30 iterations. For the ﬁrst plot, m = 3 and for the second

(a) Varying H

(b) Varying m

plot, H = 3. As H increases, the ﬁnal policy improves. With large enough H, we obtain the optimal

policy. However, past a certain point, increasing m is not helpful for ﬁnding a better policy.

our algorithm. As expected, the algorithm never diverges for indicator vectors, as our algorithm is

then equivalent to the tabular setting. The designed feature vectors clearly require a much smaller

amount of lookahead or m-step return, well below the amount predicted by the average δF V of

16.29. However, no matter the choice of feature vectors, we will eventually prevent our algorithm

from diverging with a large enough value of H + m.

F.2. Convergence to the optimal policy

In Theorem 1, we show that as H increases, we converge to a policy µk that is closer to the

optimal policy. In this section, we experimentally investigate the role of m and H on the ﬁnal

value of (cid:107)J µk − J ∗(cid:107)∞. The results can be found in Figure 4. As predicted by theory, we do get

closer to the optimal policy as H increases. However, increasing m does not help past a certain

point, which is also consistent with the theory. Indeed, although µk is approaching the optimal
policy µ∗ as H increases, the iterates Jk are not converging to J ∗ due to error induced by function

approximation. Increasing m improves the policy evaluation, but cannot correct for this inherent

33

error from approximating the value function.

Acknowledgments

The research presented here was supported in part by a grant from Sandia National Labs and the NSF Grants

CCF 1934986, CCF 2207547, CNS 2106801, ONR Grant N00014-19-1-2566, and ARO Grant W911NF-

19-1-0379. Sandia National Laboratories is a multimission laboratory managed and operated by National

Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International

Inc., for the U.S. Department of Energy’s National Nuclear Security Administration under contract DE-

NA0003525. This paper describes objective technical results and analysis. Any subjective views or opinions

that might be expressed in the paper do not necessarily represent the views of the U.S. Department of Energy

or the United States Government.

References

Arora S, Du S, Hu W, Li Z, Wang R (2019) Fine-grained analysis of optimization and generalization for

overparameterized two-layer neural networks. International Conference on Machine Learning, 322–332

(PMLR).

Baxter J, Tridgell A, Weaver L (1999) Tdleaf(lambda): Combining temporal diﬀerence learning with game-

tree search. CoRR cs.LG/9901001, URL https://arxiv.org/abs/cs/9901001.

Bertsekas D (2011) Approximate policy iteration: a survey and some new methods. Journal of Control Theory

and Applications 9:310–335.

Bertsekas D (2021) Lessons from alphazero for optimal, model predictive, and adaptive control.

Bertsekas D, Tsitsiklis J (1996) Neuro-dynamic Programming (Athena Scientiﬁc), ISBN 9781886529106.

Bertsekas DP (2019) Reinforcement learning and optimal control (Athena Scientiﬁc Belmont, MA).

Browne C, Powley E, Whitehouse D, Lucas S, Cowling P, Rohlfshagen P, Tavener S, Perez Liebana D,

Samothrakis S, Colton S (2012) A survey of monte carlo tree search methods. IEEE Transactions on

34

Computational Intelligence and AI in Games 4:1:1–43, URL http://dx.doi.org/10.1109/TCIAIG.

2012.2186810.

Cao Y, Gu Q (2019) Generalization bounds of stochastic gradient descent for wide and deep neural networks.

Advances in Neural Information Processing Systems 32:10836–10846.

Deng H, Yin S, Deng X, Li S (2020) Value-based algorithms optimization with discounted multiple-step

learning method in deep reinforcement learning. 2020 IEEE 22nd International Conference on High

Performance Computing and Communications; IEEE 18th International Conference on Smart City;

IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS), 979–984,

URL http://dx.doi.org/10.1109/HPCC-SmartCity-DSS50907.2020.00131.

Du SS, Zhai X, Poczos B, Singh A (2018) Gradient descent provably optimizes over-parameterized neural

networks. International Conference on Learning Representations.

Efroni Y, Dalal G, Scherrer B, Mannor S (2018a) Beyond the one step greedy approach in reinforcement

learning. CoRR abs/1802.03654, URL http://arxiv.org/abs/1802.03654.

Efroni Y, Dalal G, Scherrer B, Mannor S (2018b) Multiple-step greedy policies in online and approximate

reinforcement learning.

Efroni Y, Dalal G, Scherrer B, Mannor S (2019) How to combine tree-search methods in reinforcement

learning.

Efroni Y, Ghavamzadeh M, Mannor S (2020) Online planning with lookahead policies. Advances in Neural

Information Processing Systems 33.

Jacot A, Gabriel F, Hongler C (2018) Neural tangent kernel: Convergence and generalization in neural

networks. arXiv preprint arXiv:1806.07572 .

Ji Z, Telgarsky M (2019) Polylogarithmic width suﬃces for gradient descent to achieve arbitrarily small test

error with shallow relu networks. International Conference on Learning Representations.

Kocsis L, Szepesv´ari C (2006) Bandit based monte-carlo planning. Machine Learning: ECML, volume 2006,

282–293, ISBN 978-3-540-45375-8, URL http://dx.doi.org/10.1007/11871842_29.

Lagoudakis MG, Parr R (2003) Least-squares policy iteration. The Journal of Machine Learning Research

4:1107–1149.

35

Lanctot M, Winands MHM, Pepels T, Sturtevant NR (2014) Monte carlo tree search with heuristic evalua-

tions using implicit minimax backups.

Mnih V, Badia AP, Mirza M, Graves A, Lillicrap TP, Harley T, Silver D, Kavukcuoglu K (2016) Asynchronous

methods for deep reinforcement learning. CoRR abs/1602.01783, URL http://arxiv.org/abs/1602.

01783.

Moerland TM, Broekens J, Jonker CM (2020) A framework for reinforcement learning and planning.

Munos R (2014) From bandits to monte-carlo tree search: The optimistic principle applied to optimization

and planning. Foundations and Trends in Machine Learning 7, URL http://dx.doi.org/10.1561/

2200000038.

Puterman M, Shin MC (1978) Modiﬁed policy iteration algorithms for discounted markov decision problems.

Management Science 24:1127–1137.

Shah D, Somani V, Xie Q, Xu Z (2020a) On reinforcement learning for turn-based zero-sum markov games.

CoRR abs/2002.10620, URL https://arxiv.org/abs/2002.10620.

Shah D, Xie Q, Xu Z (2020b) Non-asymptotic analysis of monte carlo tree search.

Silver D, Hubert T, Schrittwieser J, Antonoglou I, Lai M, Guez A, Lanctot M, Sifre L, Kumaran D, Graepel

T, Lillicrap TP, Simonyan K, Hassabis D (2017a) Mastering chess and shogi by self-play with a general

reinforcement learning algorithm. CoRR abs/1712.01815, URL http://arxiv.org/abs/1712.01815.

Silver D, Schrittwieser J, Simonyan K, Antonoglou I, Huang A, Guez A, Hubert T, Baker L, Lai M, Bolton

A, et al. (2017b) Mastering the game of go without human knowledge. Nature 550(7676):354–359.

Springenberg JT, Heess N, Mankowitz D, Merel J, Byravan A, Abdolmaleki A, Kay J, Degrave J, Schrit-

twieser J, Tassa Y, et al. (2020) Local search for policy iteration in continuous control. arXiv preprint

arXiv:2010.05545 .

Tomar M, Efroni Y, Ghavamzadeh M (2020) Multi-step greedy reinforcement learning algorithms.

Tsitsiklis JN (2002) On the convergence of optimistic policy iteration. Journal of Machine Learning Research

3(Jul):59–72.

Tsitsiklis JN, van Roy B (1994) Feature-based methods for large scale dynamic programming. Machine

Learning, 59–94.

36

Veness J, Silver D, Blair A, Uther W (2009) Bootstrapping from game tree search. Bengio Y, Schuur-

mans D, Laﬀerty J, Williams C, Culotta A, eds., Advances in Neural Information Processing Systems,

volume 22 (Curran Associates, Inc.), URL https://proceedings.neurips.cc/paper/2009/file/

389bc7bb1e1c2a5e7e147703232a88f6-Paper.pdf.

