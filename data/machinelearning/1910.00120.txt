0
2
0
2

r
p
A
3
1

]

G
L
.
s
c
[

3
v
0
2
1
0
0
.
0
1
9
1
:
v
i
X
r
a

September 2019 (Revised April 2020)

Multiagent Rollout Algorithms and

Reinforcement Learning

Dimitri Bertsekas†

Abstract

We consider ﬁnite and inﬁnite horizon dynamic programming problems, where the control at each stage

consists of several distinct decisions, each one made by one of several agents. We introduce an approach,

whereby at every stage, each agent’s decision is made by executing a local rollout algorithm that uses a base
policy, together with some coordinating information from the other agents. The amount of local computation

required at every stage by each agent is independent of the number of agents, while the amount of total
computation (over all agents) grows linearly with the number of agents. By contrast, with the standard

rollout algorithm, the amount of total computation grows exponentially with the number of agents. De-
spite the drastic reduction in required computation, we show that our algorithm has the fundamental cost

improvement property of rollout: an improved performance relative to the base policy. We also discuss pos-
sibilities to improve further the method’s computational eﬃciency through limited agent coordination and

parallelization of the agents’ computations. Finally, we explore related approximate policy iteration algo-
rithms for inﬁnite horizon problems, and we prove that the cost improvement property steers the algorithm

towards convergence to an agent-by-agent optimal policy.

1. MULTIAGENT PROBLEM FORMULATION - FINITE HORIZON PROBLEMS

We consider a standard form of an N -stage dynamic programming (DP) problem (see [Ber17], [Ber19]),

which involves the discrete-time dynamic system

xk+1 = fk(xk, uk, wk),

k = 0, 1, . . . , N − 1,

(1.1)

where xk is an element of some (possibly inﬁnite) state space, the control uk is an element of some ﬁnite

control space, and wk is a random disturbance, which is characterized by a probability distribution Pk(· |

xk, uk) that may depend explicitly on xk and uk, but not on values of prior disturbances wk−1, . . . , w0. The

control uk is constrained to take values in a given subset Uk(xk), which depends on the current state xk.

The cost of the kth stage is denoted by gk(xk, uk, wk); see Fig. 1.1.

We consider policies of the form

π = {µ0, . . . , µN −1},

† McAfee Professor of Engineering, MIT, Cambridge, MA, and Fulton Professor of Computational Decision

Making, ASU, Tempe, AZ.

1

 
 
 
 
 
 
Control uk
Random Transition

Future Stages Terminal Cost

Future Stages Terminal Cost gN (xN )

...

+1 xN

) x0

...

Random Transition xk+1 = fk(xk, uk, wk) Random cost

) xk

) Random Cost
) Random Cost gk(xk, uk, wk)

xk+1

Stage k

k Future Stages

Figure 1.1 Illustration of the N -stage stochastic optimal control problem. Starting from state

xk, the next state under control uk is generated according to a system equation

xk+1 = fk(xk, uk, wk),

where wk is the random disturbance, and a random stage cost gk(xk, uk, wk) is incurred.

where µk maps states xk into controls uk = µk(xk), and satisﬁes a control constraint of the form µk(xk) ∈

Uk(xk) for all xk. Given an initial state x0 and a policy π = {µ0, . . . , µN −1}, the expected cost of π starting

at x0 is

Jπ(x0) = E

(

N −1

gN (xN ) +

gk

xk, µk(xk), wk

k=0
X

(cid:0)

,

)
(cid:1)

where the expected value operation E{·} is over all the random variables wk and xk. The optimal cost is

the function J * of the initial state x0, deﬁned by

J *(x0) = min
π∈Π

Jπ(x0),

where Π is the set of all policies, while an optimal policy π∗ is one that attains the minimal cost for every

initial state; i.e.,

Jπ∗(x0) = min
π∈Π

Jπ(x0).

Since J * and π∗ are typically hard to obtain by exact DP, we consider reinforcement learning (RL) algorithms

for suboptimal solution, and focus on rollout, which we describe next.

1.1. The Standard Rollout Algorithm

The aim of rollout is policy improvement.

In particular, given a policy π = {µ0, . . . , µN −1}, called base

policy, with cost-to-go from state xk at stage k denoted by Jk,π(xk), k = 0, . . . , N , we wish to use rollout

to obtain an improved policy, i.e., one that achieves cost that is at most Jk,π(xk) starting from each xk.

The standard rollout algorithm provides on-line control of the system as follows (see the textbooks [BeT96],

[Ber17], [Ber19]):

2

Standard One-Step Lookahead Rollout Algorithm:

Start with the initial state x0, and proceed forward generating a trajectory

{x0, ˜u0, x1, ˜u1, . . . , xN −1, ˜uN −1, xN }

according to the system equation (1.1), by applying at each state xk a control ˜uk selected by the

one-step lookahead minimization

˜uk ∈ arg min

uk∈Uk(xk)

E

gk(xk, uk, wk) + Jk+1,π

fk(xk, uk, wk)

(1.2)

n

(cid:0)

.
(cid:1)o

The one-step minimization (1.2), which uses Jk+1,π in place of the optimal cost-to-go function, deﬁnes

a policy ˜π = {˜µ0, . . . , ˜µN −1}, where for all xk and k, ˜µk(xk) is equal to the control ˜uk obtained from Eq.

(1.2). This policy is referred to as the rollout policy. The fundamental cost improvement result here is that

the rollout policy improves over the base policy in the sense that

Jk,˜π(xk) ≤ Jk,π(xk),

∀ xk, k,

(1.3)

where Jk,˜π(xk), k = 0, . . . , N , is the cost-to-go of the rollout policy starting from state xk ([Ber17], Section

6.4, or [Ber19], Section 2.4.2).

The expected value in Eq. (1.2) is the Q-factor of the pair (xk, uk) corresponding to the base policy:

Qk,π(xk, uk) = E

gk(xk, uk, wk) + Jk+1,π

fk(xk, uk, wk)

.

In the “standard” implementation of rollout, at each encountered state xk, the Q-factor Qk,π(xk, uk) is

n

(cid:0)

(cid:1)o

computed by some algorithm separately for each control uk ∈ Uk(xk) (often by Monte Carlo simulation).

Unfortunately, in the multiagent context to be discussed shortly, the number of controls in Uk(xk), and the

attendant computation of Q-factors, grow rapidly with the number of agents, and can become very large.

The purpose of this paper is to introduce a modiﬁed rollout algorithm for the multiagent case, which requires

much less computation while maintaining the cost improvement property (1.3).

1.2. The Multiagent Case

Let us assume a special structure of the control space, corresponding to a multiagent version of the problem.†

In particular, we assume that the control uk consists of m components u1

k, . . . , um
k ,

† While we focus on multiagent problems, our methodology applies to any problem where the control uk consists

uk = (u1

k, . . . , um

k ),

of m components, uk = (u1

k, . . . , um

k ).

3

with the component uℓ

k, ℓ = 1, . . . , m, chosen by agent ℓ at stage k, from within a given set U ℓ

k(xk). Thus

the control constraint set is the Cartesian product†

Uk(xk) = U 1

k (xk) × · · · × U m

k (xk).

(1.4)

Then the minimization (1.2) involves as many as sm Q-factors, where s is the maximum number of elements

of the sets U i

k(xk) [so that sm is an upper bound to the number of controls in Uk(xk), in view of its Cartesian
product structure (1.4)]. Thus the computation required by the rollout algorithm is of order O(sm) per stage.

In this paper we propose an alternative rollout algorithm that achieves the cost improvement property

(1.3) at much smaller computational cost, namely of order O(sm) per stage. A key idea here is that the

computational requirements of the rollout one-step minimization (1.2) are proportional to the number of

controls in the set Uk(xk) and are independent of the size of the state space. This motivates a reformulation

of the problem, ﬁrst suggested in the neuro-dynamic programming book [BeT96], Section 6.1.4, whereby

control space complexity is traded oﬀ with state space complexity by “unfolding” the control uk into its m

components, which are applied one agent-at-a-time rather than all-agents-at-once. We discuss this idea next

within the multiagent context.

1.3. Trading oﬀ Control Space Complexity with State Space Complexity

We noted that a major issue in rollout is the minimization over uk ∈ Uk(xk) in Eq. (1.2), which may be very

time-consuming when the size of the control constraint set is large. In particular, in the multiagent case

where uk = (u1

k ), the time to perform this minimization is typically exponential in m. In this case,
we can reformulate the problem by breaking down the collective decision uk into m individual component

k, . . . , um

decisions, thereby reducing the complexity of the control space while increasing the complexity of the state

space. The potential advantage is that the extra state space complexity does not aﬀect the computational

requirements of some RL algorithms, including rollout.

To this end, we introduce a modiﬁed but equivalent problem, involving one-agent-at-a-time control

selection. At the generic state xk, we break down the control uk into the sequence of the m controls
k, u2
u1
mediate “states” (xk, u1

k , and between xk and the next state xk+1 = fk(xk, uk, wk), we introduce artiﬁcial inter-
), and corresponding transitions. The choice

k, . . . , um

k, . . . , um−1

k

k), (xk, u1

of the last control component um

) marks the transition to the next state

k, u2
k), . . . , (xk, u1
k at “state” (xk, u1

k, . . . , um−1

k

xk+1 = fk(xk, uk, wk) according to the system equation, while incurring cost gk(xk, uk, wk); see Fig. 1.2.

† The Cartesian product structure of the constraint set is adopted here for simplicity of exposition, particularly

when arguing about computational complexity. The idea of trading oﬀ control space complexity and state space

complexity (cf. Section 1.3), on which this paper rests, does not depend on a Cartesian product constraint structure.

Of course when this structure is present, it simpliﬁes the computations of the methods of this paper.

4

1 Control um
k

Random Transition

, u1
k

) xk

, u2
k

xk, u1
k

m
k u3
k

k xk, u1
1

k, u2
k

Random Transition xk+1 = fk(xk, uk, wk) Random cost
k um−1
3
...
k2
k xk, u1

k, . . . , um−1

xk+1

k

) Random Cost
) Random Cost gk(xk, uk, wk)

Stage k

Figure 1.2 Equivalent formulation of the N -stage stochastic optimal control problem for the
case where the control uk consists of m components u1

k, u2

k, . . . , um
k :

uk = (u1

k, . . . , um

k ) ∈ U 1

k (xk) × · · · × U m

k (xk).

The ﬁgure depicts the kth stage transitions. Starting from state xk, we generate the intermediate
k, . . . , um−1
), using the respective controls u1
k), . . . , (xk, u1
states (xk, u1
.
The ﬁnal control um leads from (xk, u1
) to xk+1 = fk(xk, uk, wk), and a random stage
cost gk(xk, uk, wk) is incurred.

k, . . . , um−1
k, . . . , um−1

k), (xk, u1

k, u2

k

k

k

It is evident that this reformulated problem is equivalent to the original, since any control choice that

is possible in one problem is also possible in the other problem, while the cost structure of the two problems

is the same. In particular, every policy

(µ1

k, . . . , µm

k ) | k = 0, . . . , N − 1

π =

(cid:8)

(cid:9)

of the original problem, including a base policy in the context of rollout, is admissible for the reformulated

problem, and has the same cost function for the original as well as the reformulated problem.†

† It would superﬁcially appear that the reformulated problem contains more policies than the original prob-

lem, because its form is more general:

in the reformulated problem the policy at the kth stage applies the control

components

k(xk), µ2
µ1

k(xk, u1

k), . . . , µm

k (xk, u1

k, . . . , um−1

k

),

where u1

k = µ1

k(xk) and for ℓ = 2, . . . , m, uℓ

k is deﬁned sequentially as

uℓ
k = µℓ

k(xk, u1

k, . . . , uℓ−1

k

).

Still, however, this policy is equivalent to the policy of the original problem that applies the control components

where for ℓ = 2, . . . , m, ˆµℓ

k(xk) is deﬁned sequentially as

k(xk), ˆµ2
µ1

k(xk), . . . , ˆµm

k (xk),

ˆµℓ
k(xk) = µℓ
k

xk, µ1

k(xk), ˆµ2

k(xk), . . . , ˆµℓ−1

k

(cid:0)

5

(xk)

.

(cid:1)

The motivation for the reformulated problem is that the control space is simpliﬁed at the expense

of introducing m − 1 additional layers of states, and corresponding m − 1 cost-to-go functions J 1

k (xk, u1
J 2
of the state space does not adversely aﬀect the operation of rollout, since the Q-factor minimization (1.2)

(xk, u1

k),
), in addition to Jk(xk). On the other hand, the increase in size

k), . . . , J m−1

k, . . . , um−1

k (xk, u1

k, u2

k

k

is performed for just one state at each stage. Moreover, in a diﬀerent context, the increase in size of the

state space can be dealt with by using function approximation, i.e., with the introduction of cost-to-go

approximations

˜J 1
k (xk, u1

k, r1

k), ˜J 2

k (xk, u1

k, u2

k, r2

k), . . . , ˜J m−1

k

(xk, u1

k, . . . , um−1

k

, rm−1
k

),

in addition to ˜Jk(xk, rk), where rk, r1

k, . . . , rm−1

k

are parameters of corresponding approximation architectures

(such as feature-based architectures and neural networks).

2. MULTIAGENT ROLLOUT

Consider now the standard rollout algorithm applied to the reformulated problem shown in Fig. 1.2, with

k, . . . , µm

k ), with each µℓ

a given base policy π = {µ0, . . . , µN −1}, which is also a policy of the original problem [so that µk =
(µ1
k, ℓ = 1, . . . , m, being a function of just xk]. The algorithm generates a rollout
policy ˜π = {˜µ0, . . . , ˜µN −1}, where for each stage k, ˜µk consists of m components ˜µℓ
k ),
and is obtained for all xk according to

k, i.e., ˜µk = (˜µ1

k, . . . , ˜µm

˜µ1
k(xk) ∈ arg min
∈U 1
k

u1
k

(xk )

˜µ2
k(xk) ∈ arg min
∈U 2
k

u2
k

(xk )

E

gk

n

E

gk

n

(cid:0)

(cid:0)

xk, ˜µ1

k(xk), u2

k, . . . , µm

k (xk), wk

· · ·

· · ·

(cid:1)

(cid:1)

xk, u1

k, µ2

k(xk), . . . , µm

k (xk), wk

+ Jk+1,π

fk

xk, u1

k, µ2

k(xk), . . . , µm

k (xk), wk

(cid:16)

(cid:0)

+ Jk+1,π

fk

xk, ˜µ1

k(xk), u2

k, . . . , µm

k (xk), wk

˜µm
k (xk) ∈ arg

min
∈Um
k

(xk)

um
k

E

gk

xk, ˜µ1

k(xk), ˜µ2

k(xk), . . . , um

k , wk

xk, ˜µ1

k(xk), ˜µ2

k(xk), . . . , um

k , wk

n

(cid:0)

(cid:1)

(cid:16)

(cid:0)

(cid:16)

(cid:0)

· · ·

+ Jk+1,π

fk

,

(cid:1)(cid:17)o

,

(cid:1)(cid:17)o

.

(cid:1)(cid:17)o

(2.1)

via a sequence of m minimizations, once over each of the agent controls u1

Thus, when applied on-line, at xk, the algorithm generates the control ˜µk(xk) =
k, . . . , um

k(xk), . . . , ˜µm
˜µ1
k (xk)
k , with the past controls
(cid:1)
determined by the rollout policy, and the future controls determined by the base policy; cf. Eq. (2.1). Assuming

(cid:0)

a maximum of s elements in the constraint sets U i

k(xk), the computation required at each stage k is of order

O(n) for each of the “states” xk, (xk, u1

k), . . . , (xk, u1

k, . . . , um−1

k

), for a total of order O(sm) computation.

In the “standard” implementation of the algorithm, at each (xk, u1

k, . . . , uℓ−1

k

) with ℓ ≤ m, and for

each of the controls uℓ

k, we generate by simulation a number of system trajectories up to stage N , with all
future controls determined by the base policy. We average the costs of these trajectories, thereby obtaining

6

the Q-factor corresponding to (xk, u1

minimal Q-factor, with the controls u1

k, . . . , uℓ−1
k, . . . , uℓ−1

k

k

, uℓ

k). We then select the control uℓ
held ﬁxed at the values computed earlier.

k that corresponds to the

Prerequisite assumptions for the preceding algorithm to work in an on-line multiagent setting are:

(a) All agents have access to the current state xk.

(b) There is an order in which agents compute and apply their local controls.

(c) There is intercommunication between agents, so agent ℓ knows the local controls u1

k, . . . , uℓ−1

k

computed

by the predecessor agents 1, . . . , ℓ − 1 in the given order.

Note that the rollout policy (2.1), obtained from the reformulated problem is diﬀerent from the rollout

policy obtained from the original problem [cf. Eq. (1.2)]. Generally, it is unclear how the two rollout policies

perform relative to each other in terms of attained cost. On the other hand, both rollout policies perform no

worse than the base policy, since the performance of the base policy is identical for both the reformulated

problem and for the original problem. This is shown formally in the following proposition.

Proposition 2.1: Let π be a base policy and let ˜π be a corresponding rollout policy generated by

the multiagent rollout algorithm (2.1). We have

Jk,˜π(xk) ≤ Jk,π(xk),

for all xk and k.

(2.2)

Proof: We will show Eq. (2.2) by induction, and for simplicity, we will give the proof for the case of just

two agents, i.e., m = 2. Clearly Eq. (2.2) holds for k = N , since JN,˜π = JN,π = gN . Assuming that it holds

for index k + 1, i.e., Jk+1,˜π ≤ Jk+1,π, we have for all xk,

Jk,˜π(xk) = E

gk

≤ E

n

gk

xk, ˜µ1

k(xk), ˜µ2

k(xk), wk

(cid:0)
xk, ˜µ1

k(xk), ˜µ2

k(xk), wk

+ Jk+1,˜π

fk

(cid:1)

+ Jk+1,π

(cid:16)

fk

xk, ˜µ1

k(xk), ˜µ2

k(xk), wk

(cid:0)
xk, ˜µ1

k(xk), ˜µ2

k(xk), wk

(cid:1)(cid:17)o

E

gk(xk, ˜µ1

(cid:0)
(cid:1)
k(xk), u2
k, wk) + Jk+1,π

(cid:16)

fk

xk, ˜µ1

k(xk), u2

k, wk

(cid:1)(cid:17)o

n
k(xk), µ2
xk, ˜µ1

k(xk), wk

+ Jk+1,π

fk

(cid:16)
(cid:0)
k(xk), µ2
xk, ˜µ1

k(xk), wk

(cid:1)(cid:17)o

(2.3)

E

gk(xk, u1

k, µ2

(cid:0)
k(xk), wk) + Jk+1,π

(cid:1)

(cid:16)

fk

xk, u1

k, µ2

k(xk), wk

(cid:1)(cid:17)o

n
(cid:0)
= min
∈U 2
u2
k (xk)
k

≤ E

gk

n
(cid:0)
= min
∈U 1
u1
k (xk)
k

≤ E

gk

n
k(xk), µ2
xk, µ1

k(xk), wk

+ Jk+1,π

fk

(cid:16)
(cid:0)
k(xk), µ2
xk, µ1

k(xk), wk

(cid:1)(cid:17)o

(cid:16)

(cid:0)

(cid:1)(cid:17)o

(cid:1)

7

n
= Jk,π(xk),

(cid:0)

where in the preceding relation:

(a) The ﬁrst equality is the DP equation for the rollout policy ˜π.

(b) The ﬁrst inequality holds by the induction hypothesis.

(c) The second equality holds by the deﬁnition of the rollout algorithm as it pertains to agent 2.

(d) The third equality holds by the deﬁnition of the rollout algorithm as it pertains to agent 1.

(e) The last equality is the DP equation for the base policy π.

The induction proof of the cost improvement property (2.2) is thus complete for the case m = 2. The proof

for an arbitrary number of agents m is entirely similar. Q.E.D.

Note the diﬀerence in the proof argument between the all-agents-at-once and one-agent-at-a-time rollout

algorithms. In the former algorithm, the second equality in Eq. (2.3) would be over both u1

k (xk) and
k ∈ U 2
u2
k (xk), and the second inequality and third equality would be eliminated. Still the proof of the cost
improvement property (2.2) goes through in both cases. Note also that if the base policy were optimal, Eq.

k ∈ U 1

(2.3) would hold as an equality throughout for both rollout algorithms, while the rollout policy ˜π would also

be optimal.

On the other hand, there is an important situation where the all-agents-at-once rollout algorithm can

improve the base policy but the one-agent-at-a-time algorithm will not. This possibility may arise when

the base policy is “agent-by-agent-optimal,” i.e., each agent’s control component is optimal, assuming that

the control components of all other agents are kept ﬁxed at some known values.† Such a policy may not

be optimal, except under special conditions. Thus if the base policy is agent-by-agent-optimal, multiagent

rollout will be unable to improve strictly the cost function, even if this base policy is strictly suboptimal.

However, we speculate that a situation where a base policy is agent-by-agent-optimal is unlikely to occur

in rollout practice, since ordinarily a base policy must be reasonably simple, readily available, and easily

simulated.

Let us also note that the qualitative diﬀerence between all-agents-at-once versus one-agent-at-a-time

rollout is reminiscent of the context of value iteration (VI) algorithms, which involve minimization of a

Bellman equation-like expression over the control constraint. In such algorithms one may choose between

† This is a concept that has received much attention in the theory of team optimization, where it is known

as person-by-person optimality. It has been studied in the context of somewhat diﬀerent problems, which involve

imperfect state information that may not be shared by all the agents; see Marschak [Mar55], Radner [Rad62],

Witsenhausen [Wit71], Ho [Ho80]. For more recent works, see Nayyar, Mahajan, and Teneketzis [NMT13], Nayyar

and Teneketzis [NaT19], Li et al. [LTZ19], the book by Zoppoli, Parisini, Baglietto, and Sanguineti [ZPB19], and the

references quoted there.

8

Gauss-Seidel methods, where the cost of a single state (and the control at that state) is updated at a time,

while taking into account the results of earlier state cost computations, and Jacobi methods, where the cost

of all states is updated at once. The tradeoﬀ between Gauss-Seidel and Jacobi methods is well-known in

the VI context: generally, Gauss-Seidel methods are faster, while Jacobi methods are also valid, as well as

better suited for distributed asynchronous implementation; see [BeT89], [Ber12]. Our context in this paper is

quite diﬀerent, however, since we are considering updates of agent controls, and not cost updates at diﬀerent

states.

Let us provide an example that illustrates how the size of the control space may become intractable

for even moderate values of the number of agents m.

Example 2.1 (Spiders and Fly)

Here there are m spiders and one ﬂy moving on a 2-dimensional grid. During each time period the ﬂy moves to

a some other position according to a given state-dependent probability distribution. The spiders, working as

a team, aim to catch the ﬂy at minimum cost. Each spider learns the current state (the vector of spiders and

ﬂy locations) at the beginning of each time period, and either moves to a neighboring location or stays where

it is. Thus each spider i has as many as ﬁve choices at each time period (with each move possibly incurring
a diﬀerent location-dependent cost). The control vector is u = (u1, . . . , um), where ui is the choice of the ith
spider, so there are about 5m possible values of u. However, if we view this as a multiagent problem, as per

the reformulation of Fig. 1.2, the size of the control space is reduced to ≤ 5 moves per spider.

To apply multiagent rollout, we need a base policy. A simple possibility is to use the policy that directs

each spider to move on the path of minimum distance to the current ﬂy position. According to the multiagent

rollout formalism, the spiders choose their moves in a given order, taking into account the current state, and

assuming that future moves will be chosen according to the base policy. This is a tractable computation,

particularly if the rollout with the base policy is truncated after some stage, and the cost of the remaining

stages is approximated using a certainty equivalence approximation in order to reduce the cost of the Monte

Carlo simulation. The problem can be made more complicated by introducing terrain obstacles, travel costs,

or multiple ﬂies.

Sample computations with this example indicate that the multiagent rollout algorithm of this section

performs about as well as the standard rollout algorithm. Both algorithms perform much better than the

base policy, and exhibit some “intelligence” that the base policy does not possess. In particular, in the rollout

algorithms the spiders attempt to “encircle” the ﬂy for faster capture, rather that moving straight towards the

ﬂy along a shortest path.

The following example is similar to the preceding one, but involves two ﬂies and two spiders moving

along a line, and admits an exact analytical solution. It illustrates how the multiagent rollout policy may

exhibit intelligence and agent coordination that is totally lacking from the base policy. The behavior described

in the example has been supported by computational experiments with larger two-dimensional problems of

9

Figure 2.1 Illustration of the 2-dimensional spiders-and-ﬂy problem. The state is the
pair of distances between spiders and ﬂy. At each time period, each spider moves to a
neighboring location or stays where it is. The spiders make moves with perfect knowledge

of the locations or each other and of the ﬂy. The ﬂy moves randomly, regardless of the

position of the spiders.

the type described in the preceding example.

Example 2.2 (Spiders and Flies)

This is a spiders-and-ﬂies problem that admits an analytical solution. There are two spiders and two ﬂies

moving along integer locations on a straight line. For simplicity we will assume that the ﬂies’ positions are

ﬁxed at some integer locations, although the problem is qualitatively similar when the ﬂies move randomly. The

spiders have the option of moving either left or right by one unit; see Fig. 2.2. The objective is to minimize the

time to capture both ﬂies. The problem has essentially a ﬁnite horizon since the spiders can force the capture

of the ﬂies within a known number of steps.

Here the optimal policy is to move the two spiders towards diﬀerent ﬂies, the ones that are initially closest

to them (with ties broken arbitrarily). The minimal time to capture is the maximum of the two initial distances

of the two optimal spider-ﬂy pairings.

Let us apply multiagent rollout with the base policy that directs each spider to move one unit towards

the closest ﬂy position (and in case of a tie, move towards the ﬂy that lies to the right). The base policy is

poor because it may unnecessarily move both spiders in the same direction, when in fact only one is needed

to capture the ﬂy. This limitation is due to the lack of coordination between the spiders: each acts selﬁshly,

ignoring the presence of the other. We will see that rollout restores a signiﬁcant degree of coordination between

the spiders through an optimization that takes into account the long-term consequences of the spider moves.

According to the multiagent rollout mechanism, the spiders choose their moves one-at-a-time, optimizing

10

Spider 1 Spider 2 Fly 1 Fly 2

Spider 1 Spider 2 Fly 1 Fly 2

Spider 1 Spider 2 Fly 1 Fly 2

Spider 1 Spider 2 Fly 1 Fly 2

+ 1, . . . , k

+ 1, . . . , k

+ 1, . . . , k

Spider 1 Spider 2 Fly 1 Fly 2 n − 1

1 n n

n n + 1

Figure 2.2 Illustration of the two-spiders and two-ﬂies problem. The spiders move along

integer points of a line. The two ﬂies stay still at some integer locations. The optimal policy

is to move the two spiders towards diﬀerent ﬂies, the ones that are initially closest to them.

The base policy directs each spider to move one unit towards the nearest ﬂy position.

Multiagent rollout with the given base policy starts with spider 1 at location n,
and calculates the two Q-factors that correspond to moving to locations n − 1 and n + 1,
assuming that the remaining moves of the two spiders will be made using the go-towards-
the-nearest-ﬂy base policy. The Q-factor of going to n − 1 is smallest because it saves
in unnecessary moves of spider 1 towards ﬂy 2, so spider 1 will move towards ﬂy 1. The

trajectory generated by multiagent rollout is to move spiders 1 and 2 towards ﬂies 1 and

2, respectively, then spider 2 ﬁrst captures ﬂy 2, and then spider 1 captures ﬂy 1. Thus
multiagent rollout generates the optimal policy.

over the two Q-factors corresponding to the right and left moves, while assuming that future moves will be

chosen according to the base policy. Let us consider a stage, where the two ﬂies are alive while the spiders

are at diﬀerent locations as in Fig. 2.2. Then the rollout algorithm will start with spider 1 and calculate two

Q-factors corresponding to the right and left moves, while using the base heuristic to obtain the next move of

spider 2, and the remaining moves of the two spiders. Depending on the values of the two Q-factors, spider 1

will move to the right or to the left, and it can be seen that it will choose to move away from spider 2 even if

doing so increases its distance to its closest ﬂy contrary to what the base heuristic will do; see Fig. 2.2. Then

spider 2 will act similarly and the process will continue. Intuitively, spider 1 moves away from spider 2 and ﬂy

2, because it recognizes that spider 2 will capture earlier ﬂy 2, so it might as well move towards the other ﬂy.

Thus the multiagent rollout algorithm induces implicit move coordination, i.e., each spider moves in a

way that takes into account future moves of the other spider. In fact it can be veriﬁed that the algorithm

will produce an optimal sequence of moves starting from any initial state. It can also be seen that ordinary

rollout (both ﬂies move at once) will also produce an optimal move sequence. Moreover, the example admits a

two-dimensional generalization, whereby the two spiders, starting from the same position, will separate under

the rollout policy, with each moving towards a diﬀerent spider, while they will move in unison in the base policy

whereby they move along the shortest path to the closest surviving ﬂy. Again this will typically happen for

both standard and multiagent rollout.

The preceding example illustrates how a poor base policy can produce a much better rollout policy,

something that can be observed in many other problems. Intuitively, the key fact is that rollout is “farsighted”

in the sense in can beneﬁt from control calculations that reach far into future stages.

11

3. ROLLOUT VARIANTS FOR FINITE HORIZON PROBLEMS

It is worth noting a few variants of the rollout algorithm for the reformulated ﬁnite horizon problem of Fig.

1.2.

(a) Instead of selecting the agent controls in a ﬁxed order, it is possible to change the order at each stage

k (the preceding cost improvement proof goes through again by induction). In fact it is possible to

optimize over multiple orders at the same stage, or to base the order selection on various features of

the state x (for instance in the case of Example 2.1, with multiple spiders and ﬂies, giving priority to

the spiders “closest” to some ﬂy may make sense).

(b) We can use at each stage k, a base policy {µ1

state xk, but also on the preceding controls, i.e., consists of functions µℓ

k } that selects controls that depend not just on the
k, . . . , uℓ−1
k of the form µℓ
).
This can exploit intuition into the problem’s structure, but from a theoretical viewpoint, it is not more

k(xk, u1

k, . . . , µm

k

general. The reason is that policies where control component selections uℓ

k depend on the previously
in addition to the current state xk, can be equivalently rep-

selected control components u1

k, . . . , uℓ−1

k

resented by policies where the selection of uℓ

k depends on just xk.

(c) The algorithm can be applied to a partial state information problem (POMDP), after it has been trans-

formed to a perfect state information problem, using a belief state formulation, where the conditional

probability distribution of the state given the available information plays the role of xk (note here

that we have allowed the state space to be inﬁnite, thereby making our methodology applicable to the

POMDP/belief state formulation).

(d) We may use rollout variants involving multistep lookahead, truncated rollout, and terminal cost function

approximation, in the manner described in the RL book [Ber19]. Of course, in such variants the cost

improvement property need not hold strictly, but it holds within error bounds, some of which are given

in [Ber19], Section 5.1, for the inﬁnite horizon discounted problem case.

We may also consider multiagent rollout algorithms that are asynchronous in the sense that the agents

may compute their rollout controls in parallel or in some irregular order rather than in sequence, and they

may also communicate these controls asynchronously with some delays. Algorithms of this type are discussed

in generality in the book [BeT89], and also in the papers [BeY10], [BeY12], [YuB13], within the present DP

context [see also the books [Ber12] (Section 2.6), and [Ber18] (Section 2.6)]. An example of such an algorithm

is obtained when at a given stage, agent ℓ computes the rollout control ˜uℓ
k(xk), . . . , µℓ−1

of some of the agents 1, . . . , ℓ − 1, and uses the controls µ1

k

k before knowing the rollout controls
(xk) of the base policy in their place.

While such an algorithm is likely to work well for many problems, it may not possess the cost improvement

property. In fact we can construct a simple example involving a single state, two agents, and two controls

12

per agent, where the 2nd agent does not take into account the control applied by the 1st agent, and as a

result the rollout policy performs worse than the base policy.

Example 3.1 (Cost Deterioration in the Absence of Adequate Agent Coordination)

Consider a problem with two agents (m = 2) and a single state. Thus the state does not change and the costs

of diﬀerent stages are decoupled (the problem is essentially static). Each of the two agents has two controls:
k ∈ {0, 1} and u2
u1
equal to 2 if u1

k ∈ {0, 1}. The cost per stage gk is equal to 0 if u1
k = 1. Suppose that the base policy applies u1

k = 0. Then it can be seen that when

k, is equal to 1 if u1

k 6= u2
k = u2

k = 0, and is

k = u2

k = u2

executing rollout, the ﬁrst agent applies u1

k = 1, and in the absence of knowledge of this choice, the second

agent also applies u2

k = 1 (thinking that the ﬁrst agent will use the base policy control u1

k = 0). Thus the cost

of the rollout policy is 2 per stage, while the cost of the base policy is 1 per stage. By contrast the rollout

algorithm that takes into account the ﬁrst agent’s control when selecting the second agent’s control applies
k = 1 and u2
u1

k = 0, thus resulting in a rollout policy with the optimal cost of 0 per stage.

The diﬃculty here is inadequate coordination between the two agents.

In particular, each agent uses

rollout to compute the local control, each thinking that the other will use the base policy control. If instead

the two agents were to coordinate their control choices, they would have applied an optimal policy.

The simplicity of the preceding example also raises serious questions as to whether the cost improvement

property (2.2) can be easily maintained by a distributed rollout algorithm where the agents do not know

the controls applied by the preceding agents in the given order of local control selection, and use instead

the controls of the base policy. Still, however, such an algorithm is computationally attractive in view of its

potential for eﬃcient distributed implementation, and may be worth considering in a practical setting. A

noteworthy property of this algorithm is that if the base policy is optimal, the same is true of the rollout

policy. This suggests that if the base policy is nearly optimal, the same is true of the rollout policy.

One may also speculate that if agents are naturally “weakly coupled” in the sense that their choice of

control has little impact in the desirability of various controls of other agents, then a more ﬂexible inter-agent

communication pattern may be suﬃcient for cost improvement.† A computational comparison of various

multiagent rollout algorithms with ﬂexible communication patterns may shed some light on this question.

The key question is whether and under what circumstances agent coordination is essential, i.e., there is a

signiﬁcant performance loss when the computations of diﬀerent agents are done to some extent concurrently

† In particular, one may divide the agents in “coupled” groups, and require coordination of control selection only

within each group, while the computation of diﬀerent groups may proceed in parallel. For example, in applications

where the agents’ locations are distributed within some geographical area, it may make sense to form agent groups

on the basis of geographic proximity, i.e., one may require that agents that are geographically near each other (and

hence are more coupled) coordinate their control selections, while agents that are geographically far apart (and hence

are less coupled) forego any coordination.

13

rather than sequentially with intermediate information exchange.

4. MULTIAGENT PROBLEM FORMULATION - INFINITE HORIZON DISCOUNTED

PROBLEMS

The multiagent rollout ideas that we have discussed so far can be modiﬁed and generalized to apply to

inﬁnite horizon problems. In this context, we may consider multiagent versions of value iteration (VI for

short) and policy iteration (PI for short) algorithms. We will focus on discounted problems with ﬁnite

number of states and controls, so that the Bellman operator is a contraction mapping, and the strongest

version of the available theory applies (the solution of Bellman’s equation is unique, and strong convergence

results hold for VI and PI methods); see [Ber12], Chapters 1 and 2, and [Ber18], Chapter 2. However, a

qualitatively similar methodology can be applied to undiscounted problems involving a termination state

(e.g., stochastic shortest path problems, see [BeT96], Chapter 2, [Ber12], Chapter 3, and [Ber18], Chapters

3 and 4).

In particular, we consider a standard Markovian decision problem (MDP for short) inﬁnite horizon

discounted version of the ﬁnite horizon m-agent problem of Section 1.2, where m > 1. The control u consists

of m components uℓ, ℓ = 1, . . . , m,

u = (u1, . . . , um),

(for the MDP notation adopted for this section, we switch for convenience to subscript indexing for control

components, and reserve superscript indexing for policy iterates). At state x and stage k, a control u is

applied, and the system transitions to a next state y with transition probabilities pxy(y) and cost g(x, u, y).

When at stage k the transition cost is discounted by αk, where α ∈ (0, 1) is the discount factor. Each control

component uℓ is separately constrained to lie in a given ﬁnite set Uℓ(x) when the system is at state x. Thus

the control constraint is u ∈ U (x), where U (x) is the ﬁnite Cartesian product set

U (x) = U1(x) × · · · × Um(x).

The cost function of a stationary policy µ that applies control µ(x) ∈ U (x) at state x is denoted by Jµ(x),

and the optimal cost [the minimum over µ of Jµ(x)] is denoted J *(x).

An equivalent version of the problem, involving a reformulated/expanded state space is depicted in

Fig. 4.1 for the case m = 3. The state space of the reformulated problem consists of

x, (x, u1), . . . , (x, u1, . . . , um−1),

where x ranges over the original state space, and each uℓ, ℓ = 1, . . . , m, ranges over the corresponding

constraint set Uℓ(x). At each stage, the agents choose their controls sequentially in a ﬁxed order: from state

x agent 1 applies u1 ∈ U1(x) to go to state (x, u1), then agent 2 applies u2 ∈ U2(x) to go to state (x, u1, u2),

14

Agent 1 Agent 2 Agent 3

1 x, u1, u2

2 u3

1 x, u1, u2

1 x, u1, u2

1 x, u1, u2

Cost 0 Cost g(x, u, w)
Control constraint:

1 u2

3 x, u1
Agent 1 Agent 2 Agent 3

3 Cost 0 Cost

3 x, u1

3 x, u1

3 x, u1

u1

Agent 1 Agent 2 Agent 3

3 Cost 0 Cost

x x, u

x x, u

x x, u

x x, u

Base Heuristic Minimization Possible Path Reformulated State Space

Base Heuristic Minimization Possible Path Reformulated State Space

Base Heuristic Minimization Possible Path Reformulated State Space

Base Heuristic Minimization Possible Path Reformulated State Space

Base Heuristic Minimization Possible Path Reformulated State Space

Base Heuristic Minimization Possible Path Reformulated State Space

Base Heuristic Minimization Possible Path Reformulated State Space

Base Heuristic Minimization Possible Path Reformulated State Space

Figure 4.1 Illustration of how to transform an m-agent inﬁnite horizon problem into a stationary
inﬁnite horizon problem with fewer control choices available at each state (in this ﬁgure m = 3).
At the typical stage and state x, the ﬁrst agent chooses u1 at no cost leading to state (x, u1).
Then the second agent applies u2 at no cost leading to state (x, u1, u2). Finally, the third agent
applies u3 leading to some state y at cost g(x, u, y), where u is the combined control of the three
agents, u = (u1, u2, u3). The ﬁgure shows the ﬁrst three transitions of the trajectories that start
from the states x, (x, u1), and (x, u1, u2), respectively.

and so on, until ﬁnally at state (x, u1, . . . , um−1), agent m applies um ∈ Um(x), completing the choice of

control u = (u1, . . . , um), and eﬀecting the transition to state y at a cost g(x, u, y), appropriately discounted.

Note that this reformulation involves the type of tradeoﬀ between control space complexity and state

space complexity that we discussed in Section 1.3. The reformulated problem involves m cost-to-go functions

J 0(x), J 1(x, u1), . . . , J m−1(x, u1, . . . , um−1),

(4.1)

with corresponding sets of Bellman equations, but a much smaller control space. Moreover, the existing

analysis of rollout algorithms, including implementations, variations, and error bounds, applies to the refor-

mulated problem; see Section 5.1 of the author’s RL textbook [Ber19]. Similar to the ﬁnite horizon case, the

implementation of the rollout algorithm involves one-agent-at-a-time policy improvement, and is much more

economical for the reformulated problem, while maintaining the basic cost improvement and error bound

properties of rollout, as they apply to the reformulated problem.

4.1. Agent-by-Agent Policy Iteration for Inﬁnite Horizon Discounted Problems

A PI algorithm generates a sequence of policies {πk}, and can be viewed as repeated or perpetual rollout,

i.e., πk+1 is the rollout policy obtained when πk is the base policy. For the reformulated problem just

described, the policy evaluation step of the PI algorithm requires the calculation of m cost-to-go functions

of the form (4.1), so the policy evaluation must be done over a much larger space than the original state

space. Moreover, the policies generated by this algorithm are also deﬁned over a larger space and have the

15

form

µ0(x), µ1(x, u1), . . . , µm−1(x, u1, . . . , um−1).

(4.2)

Motivated by this fact, we may consider a multiagent PI algorithm that operates over the simpler class of

policies of the form

µ(x) =

µ0(x), µ1(x), . . . , µm−1(x)

,

(4.3)

i.e., the policies for the original inﬁnite horizon problem.

(cid:0)

(cid:1)

We assume n states, denoted by 1, . . . , n, and we introduce the Bellman operator T , which maps a

vector J =

J(1), . . . , J(n)

to the vector T J =

(T J)(1), . . . , (T J)(n)

according to

(cid:0)

(cid:1)

(T J)(x) = min
u∈U(x)

n

(cid:0)
pxy(u)

g(x, u, y) + αJ(y)

,

x = 1, . . . , n,

(4.4)

(cid:1)

y=1
X

(cid:0)

(cid:1)

and for each policy µ, the corresponding Bellman operator Tµ deﬁned by

n

(TµJ)(x) =

pxy

µ(x)

y=1
X

(cid:0)

g
(cid:1)(cid:16)

x, µ(x), y

+ αJ(y)

,

x = 1, . . . , n.

(4.5)

(cid:0)

(cid:1)

(cid:17)

It is well known that T and Tµ are contraction mappings of modulus α with respect to the sup norm, and

their unique ﬁxed points are J * and Jµ, respectively, i.e., J * = T J * and Jµ = TµJµ.

In the preceding expressions, we will often expand u to write it in terms of its components. In particular,

we may write pxy(u1, . . . , um) and g(x, u1, . . . , um, y) in place of pxy(u) and g(x, u, y), respectively. Similarly,

we will denote the components of a policy µ as µ1, . . . , µm.

The Standard Policy Iteration Algorithm

For each policy µ, we introduce the subset of policies

M(µ) = {˜µ | T˜µJµ = T Jµ}.

(4.6)

Equivalently, we have ˜µ ∈ M(µ) if ˜µ is obtained from µ by using the standard policy improvement operation,

˜µ(x) ∈ arg min
u∈U(x)

n

y=1
X

pxy(u)

g(x, u, y) + αJµ(y)

,

x = 1, . . . , n,

(4.7)

(cid:0)

(cid:1)

or

where Qµ(x, u) is the Q-factor of the state-control pair (x, u) corresponding to µ, given by

˜µ(x) ∈ arg min
u∈U(x)

Qµ(x, u),

x = 1, . . . , n,

n

Qµ(x, u) =

pxy(u)

g(x, u, y) + αJµ(y)

.

y=1
X

(cid:0)

16

(cid:1)

(4.8)

(4.9)

The standard form of PI generates a sequence {µk} of policies, starting from a given policy µ0 (see e.g.,

[Ber12]). Given the current policy µk, it generates a new policy from the set of “improved” policies M(µk)

of Eq. (4.6):

µk+1 ∈ M(µk).

(4.10)

Thus the kth iteration of the standard PI algorithm can be separated into two phases:

(a) Policy evaluation, which computes Jµk .

(b) Policy improvement , which computes a new policy µk+1 ∈ M(µk) by the minimization over u ∈ U (x)

of the Q-factor Qµk (x, u); cf. Eq. (4.7)-(4.8).

The Multiagent Policy Iteration Algorithm

Our proposed one-agent-at-a-time PI algorithm uses a modiﬁed form of policy improvement, whereby the

control u = (u1, . . . , um) is optimized one-component-at-a-time, similar to Section 2. In particular, given the

current policy µk, the next policy is obtained as

where for a given µ, we denote by

M(µ) the set of policies ˜µ = (˜µ1, . . . , ˜µm) satisfying for all x = 1, . . . , n,

f

µk+1 ∈

M(µk),

(4.11)

˜µ1(x) ∈ arg min

u1∈U1(x)

n

n

f

pxy

u1, µ2(x), . . . , µm(x)

g

x, u1, µ2(x), . . . , µm(x), y

+ αJµ(y)

,

y=1
X

(cid:0)

(cid:1)(cid:16)

(cid:0)

(cid:1)

(cid:17)

˜µ2(x) ∈ arg min

u2∈U2(x)

pxy

˜µ1(x), u2, µ3(x), . . . , µm(x)

g

x, ˜µ1(x), u2, µ3(x), . . . , µm(x), y

+ αJµ(y)

,

y=1
X

(cid:0)

n

· · ·

· · ·

(cid:1)(cid:16)

(cid:0)

· · ·

(cid:1)

(cid:17)

˜µm(x) ∈ arg min

pxy

˜µ1(x), . . . , ˜µm−1(x), um

g

x, ˜µ1(x), . . . , ˜µm−1(x), um, y

+ αJµ(y)

. (4.12)

um∈Um(x)

y=1
X

(cid:1)(cid:16)
Note that each of the m minimizations (4.12) can be performed for each state x independently, i.e.,

(cid:17)

(cid:0)

(cid:1)

(cid:0)

the computations for state x do not depend on the computations for other states, thus allowing the use

of parallel computation over the diﬀerent states. On the other hand, the computations corresponding to

individual components must be performed in sequence (in the absence of special structure related to coupling

of the control components through the transition probabilities and the cost per stage). It will also be clear

from the subsequent analysis that the ordering of the components may change from one policy improvement

operation to the next.

Similar to the ﬁnite horizon case of Sections 2 and 3, the salient feature of the one-agent-at-a-time policy

improvement operation (4.12) is that it is far more economical than the standard policy improvement:

it

17

requires a sequence of m minimizations, once over each of the control components u1, ..., um. In particular, for

the minimization over the typical component uℓ, the preceding components u1, . . . , uℓ−1 have been computed

earlier by the minimization that yielded the policy components ˜µ1, . . . , ˜µℓ−1, while the following controls

uℓ+1, . . . , um are determined by the current policy components µℓ+1, . . . , µm. Thus, if the number of controls

within each component constraint set Uℓ(x) is bounded by a number s, the modiﬁed policy improvement

phase requires at most smn calculations of a Q-factor of the generic form (4.9). By contrast, since the number

of elements in the constraint set U (x) is bounded by sm, the corresponding number of Q-factor calculations

in the standard policy improvement is bounded by smn. Thus the one-agent-at-a-time policy improvement

where the number of Q-factors grows linearly with m, as opposed to the standard policy improvement, where

the number of Q-factor calculations grows exponentially with m.

We say that a policy µ = {µ1, . . . , µm} is agent-by-agent optimal if it satisﬁes µ ∈

M(µ), or equivalently,

for all x = 1, . . . , n, and ℓ = 1, . . . , m, we have

n

pxy

µ1(x), . . . , µm(x)

g

x, µ1(x), . . . , µm(x), y

+ αJµ(y)

f

y=1
X

(cid:0)

(cid:1)(cid:16)

(cid:0)

= min

uℓ∈Uℓ(x)

n

y=1
X

(cid:1)

(cid:17)

pxy

µ1(x), . . . , µℓ−1(x), uℓ, µℓ+1(y), . . . , µm(y)

(cid:0)

·

g

µ1(y), . . . , µℓ−1(y), uℓ, µℓ+1(y), . . . , µm(y)

+ αJµ(y)

.

(cid:1)

(cid:16)

(cid:0)

(cid:1)

(4.13)
(cid:17)

To interpret this deﬁnition, let a policy µ = {µ1, . . . , µm} be given, and consider for every ℓ = 1, . . . , m the
single agent DP problem where for all ℓ′ 6= ℓ the ℓ′th policy components are ﬁxed at µℓ′, while the ℓth policy

component is subject to optimization. We view the agent-by-agent optimality deﬁnition as the optimality

condition for all the single agent problems [Eq. (4.13) can be written as Tµ,ℓJµ = TℓJµ, where Tℓ and Tµ,ℓ

are the Bellman operators (4.4) and (4.5) that correspond to the single agent problem of agent ℓ]. We can

then conclude that µ = {µ1, . . . , µm} is agent-by-agent optimal if each component µℓ is optimal for the

ℓth single agent problem, where it is assumed that the remaining policy components remain ﬁxed; in other

words by using µℓ, each agent ℓ acts optimally, assuming all other agents ℓ′ 6= ℓ use the corresponding policy

components µℓ′.

In the terminology of team theory such a policy may also be called “person-by-person

optimal.”

Note that an (overall) optimal policy is agent-by-agent optimal, but the reverse is not true as the

following example shows. This is well-known from the aforementioned research on team theory; see [Mar55],

[Rad62], [Wit71], [Ho80], [NMT13], [NaT19], [LTZ19], [ZPB19].

Example 4.1 (Counterexample for Agent-by-Agent Optimality)

Consider an inﬁnite horizon problem, which involves two agents (m = 2) and a single state x. Thus the state

does not change and the costs of diﬀerent stages are decoupled (the problem is essentially static). Each of the

18

two agents has two controls: u1 ∈ {0, 1} and u2 ∈ {0, 1}. The cost per stage g is equal to 2 if u1 6= u2, is

equal to 1 if u1 = u2 = 0, and is equal to 0 if u1 = u2 = 1. The unique optimal policy is to apply µ1(x) = 1

and µ2(x) = 1. However, it can be seen that the suboptimal policy that applies µ1(x) = 0 and µ2(x) = 0 is

agent-by-agent optimal.

The preceding example is representative of an entire class of DP problems where an agent-by-agent

optimal policy is not overall optimal. Any static multivariable optimization problem where there are nonop-

timal solutions that cannot be improved upon by coordinate descent can be turned into an inﬁnite horizon

DP example where these nonoptimal solutions deﬁne agent-by-agent optimal policies that are not overall op-

timal. Conversely, one may search for problem classes where an agent-by-agent optimal policy is guaranteed

to be (overall) optimal among the type of multivariable optimization problems where coordinate descent is

guaranteed to converge to an optimal solution; for example positive deﬁnite quadratic problems or prob-

lems involving diﬀerentiable strictly convex functions (see [Ber16], Section 3.7). Generally, agent-by-agent

optimality may be viewed as an acceptable form of optimality for many types of problems.

Our main result is that the agent-by-agent PI algorithm just described converges to an agent-by-agent

optimal policy in a ﬁnite number of iterations. For the proof, we use a special rule for breaking ties in the

policy improvement operation in favor of the current policy component. This rule is easy to enforce, and

guarantees that the algorithm cannot cycle between policies. Without this tie-breaking rule, the following

proof can be modiﬁed to show that while the generated policies may cycle, the corresponding cost function

values converge to the cost function value of some agent-by-agent optimal policy.

Proposition 4.1: (PI Convergence to an Agent-by-Agent Optimal Policy) Let {µk} be

a sequence generated by the agent-by-agent PI algorithm (4.11) assuming that ties in the policy

improvement operation of Eq. (4.12) are broken as follows: If for any ℓ = 1, . . . , m and x = 1, . . . , n,

the control component µℓ(x) attains the minimum in Eq. (4.12), we choose ˜µℓ(x) = µℓ(x) [even if

there are other control components within Uℓ(x) that attain the minimum in addition to µℓ(x)]. Then

for all x and k, we have

Jµk+1 (x) ≤ Jµk (x),

and after a ﬁnite number of iterations, we have µk+1 = µk, in which case the policies µk+1 and µk are

agent-by-agent optimal.

Proof:

In the following proof and later all vector inequalities are meant to be componentwise, i.e., for any

two vectors J and J ′, we write J ≤ J ′ if J(x) = J ′(x) for all x. The critical step of the proof is the following

19

monotone decrease inequality:

T˜µJ ≤ TµJ ≤ J,

for all ˜µ ∈

M(µ) and J with TµJ ≤ J,

(4.14)

which yields as a special case T˜µJµ ≤ Jµ, since TµJµ = Jµ. This parallels a key inequality for standard PI,

f

namely that T˜µJµ ≤ Jµ, for all ˜µ ∈ M(µ), which lies at the heart of its convergence proof. Once Eq. (4.14)

is shown, the monotonicity of the operator T˜µ implies the cost improvement property J˜µ ≤ Jµ, and by using

the ﬁniteness of the set of policies, the ﬁnite convergence of the algorithm will follow.

We will give the proof of the monotone decrease inequality (4.14) for the case m = 2. The proof for an

arbitrary number of components m > 2 is entirely similar. Indeed, if TµJ ≤ J, we have for all x,

n

(T˜µJ)(x) =

pxy

˜µ1(x), ˜µ2(x)

g

x, ˜µ1(x), ˜µ2(x), y

+ αJ(y)

y=1
X

(cid:0)

n

= min

u2∈U2(x)

n

pxy

y=1
X

(cid:0)

(cid:1)(cid:16)
(cid:0)
˜µ1(x), u2

≤

pxy

˜µ1(x), µ2(x)

g

y=1
X

(cid:0)

n

= min

u1∈U1(x)

n

pxy

y=1
X

(cid:0)

(cid:1)(cid:16)
(cid:0)
u1, µ2(x)

≤

pxy

µ1(x), µ2(x)

g

y=1
X

(cid:0)
= (TµJ)(x)

≤ J(x),

(cid:1)(cid:16)

(cid:0)

(cid:1)
x, ˜µ1(x), u2, y

g
(cid:1)(cid:16)
x, ˜µ1(x), µ2(x), y

(cid:0)

(cid:1)
x, u1, µ2(x), y

g
(cid:1)(cid:16)
x, µ1(x), µ2(x), y

(cid:0)

(cid:17)

+ αJ(y)

(cid:17)

(cid:1)

+ αJ(y)

(cid:17)

(cid:1)

+ αJ(y)

(cid:1)

(cid:17)

(cid:17)

+ αJ(y)

(4.15)

where:

(1) The ﬁrst and fourth equalities use the deﬁnition of the Bellman operator T˜µ.

(2) The second and third equalities hold by the deﬁnition of policies ˜µ ∈

M(µ).

(3) The ﬁrst and second inequalities are evident.

f

(4) The last inequality is the assumption TµJ ≤ J.

By letting J = Jµk in the monotone decrease inequality (4.14), we have Tµk+1Jµk ≤ Jµk . In view of

the monotonicity of Tµk+1, we also have T t+1

µk+1Jµk ≤ T t

µk+1Jµk for all t ≥ 1, so that

Jµk+1 = lim
t→∞

T t
µk+1 Jµk ≤ Tµk+1Jµk ≤ Jµk .

It follows that either Jµk+1 = Jµk , or else we have strict policy improvement, i.e., Jµk+1 (x) < Jµk (x) for
at least one state x. As long as strict improvement occurs, no generated policy can be repeated by the

20

algorithm. Since there are only ﬁnitely many policies, it follows that within a ﬁnite number of iterations,

we will have Jµk+1 = Jµk . Once this happens, equality will hold throughout in Eq. (4.15) when µ = µk,
˜µ = µk+1, and J = Jµk . This implies that

n

pxy

µk+1
1

(x), µk+1

2

(x)

g

x, µk+1
1

(x), µk+1

2

(x), y

y=1
X

(cid:0)

(cid:1)(cid:16)

(cid:0)

= min

u2∈U2(x)

n

=

pxy

and

n

y=1
X

(cid:0)

+ αJµk (y)
(cid:17)

n

pxy

µk+1
1

(cid:1)
(x), u2

g

x, µk+1
1

(x), u2, y

y=1
X
µk+1
1

(cid:0)
(x), µk

2 (x)

g
(cid:1)(cid:16)

(cid:1)(cid:16)
(cid:0)
x, µk+1
1

(cid:0)

(x), µk

2 (x), y

(cid:1)

+ αJµk (y)
(cid:17)

(cid:1)
+ αJµk (y)
(cid:17)

,

pxy

µk+1
1

(x), µk

2 (x)

g

x, µk+1
1

(x), µk

2 (x), y

y=1
X

(cid:0)

(cid:1)(cid:16)

(cid:0)

= min

u1∈U1(x)

n

=

pxy

n

pxy

(cid:1)
u1, µk

2(x)

y=1
X
(cid:0)
1(x), µk
µk
2 (x)

+ αJµk (y)
(cid:17)

2(x), y

x, u1, µk

g
(cid:1)(cid:16)
(cid:0)
1(x), µk
x, µk

2 (x), y

+ αJµk (y)
(cid:17)

(cid:1)
+ αJµk (y)
(cid:17)

.

g
(cid:1)(cid:16)

(cid:0)
In view of our tie breaking rule, Eq. (4.17) implies that µk+1
µk+1
2 = µk
optimal. Q.E.D.

1, and then Eq. (4.16) implies that
2. Thus we have µk+1 = µk, and from Eqs. (4.16) and (4.17), µk+1 and µk are agent-by-agent

= µk

(cid:0)

(cid:1)

1

y=1
X

(4.16)

(4.17)

As Example 4.1 shows, there may be multiple agent-by-agent optimal policies, with diﬀerent cost

functions. This illustrates that the policy obtained by the multiagent PI algorithm may depend on the

starting policy. It turns out that the same example can be used to show that the policy obtained by the

algorithm depends also on the order in which the agents select their controls.

Example 4.2 (Dependence of the Final Policy on the Agent Iteration Order)

Consider the problem of Example 4.1.
optimal policy µ∗ where µ∗
Let the starting policy be µ0 where µ0

1(x) = 1 and µ∗

In this problem there are two agent-by-agent optimal policies: the

2(x) = 1, and the suboptimal policy ˆµ where ˆµ1(x) = 0 and ˆµ2(x) = 0.

2(x) = 0. Then if agent 1 iterates ﬁrst, the algorithm will
terminate with the suboptimal policy, µ1 = ˆµ, while if agent 2 iterates ﬁrst, the algorithm will terminate with
the optimal policy, µ1 = µ∗.

1(x) = 1 and µ0

Generally, it may not be easy to escape from a suboptimal agent-by-agent optimal policy. This is

similar to trying to escape from a local minimum in multivariable optimization. Of course, one may try

minimization over suitable subsets of control components, selected by some heuristic, possibly randomized,

mechanism. However, such a approach is likely to be problem-dependent, and may not oﬀer meaningful

guarantees of success.

21

We note that the line of proof based on the monotone decrease inequality (4.14) given above can be

used to establish the validity of some variants of agent-by-agent PI. One such variant, which we will not

pursue further, enlarges the set

M(µ) to allow approximate minimization over the control components in

Eq. (4.12). In particular, we require that in place of Eq. (4.15), each control ˜µℓ(x), ℓ = 1, . . . , m, satisﬁes

f

n

pxy

˜µ1(x), . . . , ˜µℓ−1(x), ˜µℓ(x), µℓ+1(x), . . . , µm(x)

y=1
X

(cid:0)

(cid:1)

g

x, ˜µ1(x), . . . , ˜µℓ−1(x), ˜µℓ(x), µℓ+1(x), . . . , µm(x), y

+ αJµ(y)

n

<

pxy

(cid:0)

(cid:16)
˜µ1(x), . . . , ˜µℓ−1(x), µℓ(x), µℓ+1(x), . . . , µm(x)

(cid:1)

(cid:17)

y=1
X

(cid:0)

g

x, ˜µ1(x), . . . , ˜µℓ−1(x), µℓ(x), µℓ+1(x), . . . , µm(x), y

+ αJµ(y)

,

(cid:16)

(cid:0)

(cid:1)

(cid:17)

(cid:1)

whenever there exists uℓ ∈ Uℓ(x) that can strictly reduce the corresponding minimized expression in Eq.

(4.15). It can be seen that even with this approximate type of minimization over control components, the

convergence proof of Prop. 4.1 still goes through.

Another important variant of agent-by-agent PI is an optimistic version, whereby policy evaluation is

performed by using a ﬁnite number of agent-by-agent value iterations. Moreover, there are many possibilities

for approximate agent-by-agent PI versions, including the use of value and policy neural networks.

In

particular, the multiagent policy improvement operation (4.12) may be performed at a sample set of states

xs, s = 1, . . . , q, thus yielding a training set of state-rollout control pairs

xs, ˜µ(xs)

, s = 1, . . . , q, which can

be used to train a (policy) neural network to generate an approximation ˆµ to the policy ˜µ. The policy ˆµ can

(cid:0)

(cid:1)

be used in turn to train a feature-based architecture or a neural network that approximates its cost function

Jˆµ, and the approximate multiagent PI cycle can be continued. Thus in this scheme, the diﬃculty with a

large control space is mitigated by agent-by-agent policy improvement, while the diﬃculty with a large state

space is overcome by training value and policy networks. A further discussion of this type of approximate

schemes is beyond the scope of the present paper.

Finally, we note that the issues relating to parallelization of the policy improvement (or rollout) step

that we discussed at the end of Section 3 for ﬁnite horizon problems, also apply to inﬁnite horizon problems.

Moreover, the natural partition of the state space illustrated in Fig. 4.1 suggests a distributed implementation

(which may be independent of any parallelization in the policy improvement step). In particular, distributed

asynchronous PI algorithms based on state space partitions are proposed and analyzed in the work of

Bertsekas and Yu [BeY10] [see also [BeY12], [YuB13], and the books [Ber12] (Section 2.6), and [Ber18]

(Section 2.6)]. These algorithms are relevant for distributed implementation of the multiagent PI ideas of

the present paper.

22

5. CONCLUDING REMARKS

We have shown that in the context of multiagent problems, an agent-by-agent version of the rollout algorithm

has greatly reduced computational requirements, while still maintaining the fundamental cost improvement

property of the standard rollout algorithm. There are many variations of rollout algorithms for multiagent

problems, which deserve attention, despite the potential lack of strict cost improvement in the case of a

suboptimal base policy that is agent-by-agent optimal. Computational tests in some practical multiagent

settings will be helpful in comparatively evaluating some of these variations.

We have primarily focused on the cost improvement property, and the practically important fact that

it can be achieved at a much reduced computational cost. However, it is useful to keep in mind that the

agent-by-agent rollout algorithm is simply the standard all-agents-at-once rollout algorithm applied to the

(equivalent) reformulated problem of Fig. 1.2 (or Fig. 4.1 in the inﬁnite horizon case). As a result, all

known insights, results, error bounds, and approximation techniques for standard rollout apply in suitably

reformulated form.

In this paper, we have assumed that the control constraint set is ﬁnite in order to argue about the

computational eﬃciency of the agent-by-agent rollout algorithm. The rollout algorithm itself and its cost

improvement property are valid even in the case where the control constraint set is inﬁnite, including the

model predictive control context (cf. Section 2.5 of the RL book [Ber19]), and linear-quadratic problems.

However, it may be unclear that agent-by-agent rollout oﬀers an advantage in the inﬁnite control space case.

We have also discussed an agent-by-agent version of PI for inﬁnite horizon problems, which uses one-

component-at-a-time policy improvement. While this algorithm may terminate with a suboptimal policy

that is agent-by-agent optimal, it may produce comparable performance to the standard PI algorithm,

which however may be computationally intractable even for a moderate number of agents. Moreover, our

multiagent PI convergence result of Prop. 4.1 can be extended beyond the ﬁnite-state discounted context to

more general inﬁnite horizon DP contexts, where the PI algorithm is well-suited for algorithmic solution.

Other extensions include agent-by-agent variants of VI, optimistic PI, and other related methods. The

analysis of such extensions is reported separately; see [Ber20a].

We ﬁnally mention that the idea of agent-by-agent rollout also applies within the context of challenging

deterministic discrete/combinatorial optimization problems, which involve constraints that couple the con-

trols of diﬀerent stages. We discuss the corresponding constrained multiagent rollout algorithms separately

in the paper [Ber20b].

6. REFERENCES

[BeT89] Bertsekas, D. P., and Tsitsiklis, J. N., 1989. Parallel and Distributed Computation: Numerical

Methods, Prentice-Hall, Englewood Cliﬀs, NJ; republished in 1996 by Athena Scientiﬁc, Belmont, MA.

23

[BeT96] Bertsekas, D. P., and Tsitsiklis, J. N., 1996. Neuro-Dynamic Programming, Athena Scientiﬁc, Bel-

mont, MA.

[BeY10] Bertsekas, D. P., and Yu, H., 2010. “Asynchronous Distributed Policy Iteration in Dynamic Pro-

gramming,” Proc. of Allerton Conf. on Communication, Control and Computing, Allerton Park, Ill, pp.

1368-1374.

[BeY12] Bertsekas, D. P., and Yu, H., 2012. “Q-Learning and Enhanced Policy Iteration in Discounted

Dynamic Programming,” Math. of OR, Vol. 37, pp. 66-94.

[Ber12] Bertsekas, D. P., 2012. Dynamic Programming and Optimal Control, Vol. II, 4th edition, Athena

Scientiﬁc, Belmont, MA.

[Ber16] Bertsekas, D. P., 2016. Nonlinear Programming, 3rd edition, Athena Scientiﬁc, Belmont, MA.

[Ber17] Bertsekas, D. P., 2017. Dynamic Programming and Optimal Control, Vol. I, 4th edition, Athena

Scientiﬁc, Belmont, MA.

[Ber18] Bertsekas, D. P., 2018. Abstract Dynamic Programming, Athena Scientiﬁc, Belmont, MA.

[Ber19] Bertsekas, D. P., 2019. Reinforcement Learning and Optimal Control, Athena Scientiﬁc, Belmont,

MA.

[Ber20a] Bertsekas, D. P., 2020. “Multiagent Value Iteration Algorithms in Dynamic Programming and

Reinforcement Learning,” in preparation.

[Ber20b] Bertsekas, D. P., 2020. “Constrained Multiagent Rollout and Multidimensional Assignment with

the Auction Algorithm,” arXiv preprint, arXiv:2002.07407.

[Ho80] Ho, Y. C., 1980. “Team Decision Theory and Information Structures,” Proceedings of the IEEE, Vol.

68, pp. 644-654.

[LTZ19] Li, Y., Tang, Y., Zhang, R., and Li, N., 2019. “Distributed Reinforcement Learning for De-

centralized Linear Quadratic Control: A Derivative-Free Policy Optimization Approach,” arXiv preprint

arXiv:1912.09135.

[Mar55] Marschak, J., 1975. “Elements for a Theory of Teams,” Management Science, Vol. 1, pp. 127-137.

[NMT13] Nayyar, A., Mahajan, A. and Teneketzis, D., 2013. “Decentralized Stochastic Control with Partial

History Sharing: A Common Information Approach,” IEEE Transactions on Automatic Control, Vol. 58,

pp. 1644-1658.

[NaT19] Nayyar, A. and Teneketzis, D., 2019. “Common Knowledge and Sequential Team Problems,” IEEE

Transactions on Automatic Control, Vol. 64, pp. 5108-5115.

[Rad62] Radner, R., 1962. “Team Decision Problems,” Ann. Math. Statist., Vol. 33, pp. 857-881.

24

[Wit71] Witsenhausen, H., 1971. “Separation of Estimation and Control for Discrete Time Systems,” Pro-

ceedings of the IEEE, Vol. 59, pp. 1557-1566.

[YuB13] Yu, H., and Bertsekas, D. P., 2013. “Q-Learning and Policy Iteration Algorithms for Stochastic

Shortest Path Problems,” Annals of Operations Research, Vol. 208, pp. 95-132.

[ZPB19] Zoppoli, R., Parisini, T., Baglietto, M., and Sanguineti, M., 2019. Neural Approximations for

Optimal Control and Decision, Springer.

25

