0
2
0
2

v
o
N
1
2

]

G
L
.
s
c
[

3
v
9
1
8
0
1
.
5
0
9
1
:
v
i
X
r
a

Learning to Optimize Computational Resources:
Frugal Training with Generalization Guarantees

Ellen Vitercik
Carnegie Mellon University
vitercik@cs.cmu.edu

Maria-Florina Balcan
Carnegie Mellon University
ninamf@cs.cmu.edu

Tuomas Sandholm
Carnegie Mellon University
Optimized Markets, Inc.
Strategic Machine, Inc.
Strategy Robot, Inc.
sandholm@cs.cmu.edu

November 24, 2020

Abstract

Algorithms typically come with tunable parameters that have a considerable impact on the
computational resources they consume. Too often, practitioners must hand-tune the parameters,
a tedious and error-prone task. A recent line of research provides algorithms that return nearly-
optimal parameters from within a ﬁnite set. These algorithms can be used when the parameter
space is inﬁnite by providing as input a random sample of parameters. This data-independent
discretization, however, might miss pockets of nearly-optimal parameters: prior research has
presented scenarios where the only viable parameters lie within an arbitrarily small region. We
provide an algorithm that learns a ﬁnite set of promising parameters from within an inﬁnite
set. Our algorithm can help compile a conﬁguration portfolio, or it can be used to select
the input to a conﬁguration algorithm for ﬁnite parameter spaces. Our approach applies to
any conﬁguration problem that satisﬁes a simple yet ubiquitous structure: the algorithm’s
performance is a piecewise constant function of its parameters. Prior research has exhibited this
structure in domains from integer programming to clustering.

1 Introduction

Similar combinatorial problems often arise in seemingly unrelated disciplines. Integer programs,
for example, model problems in ﬁelds ranging from computational biology to economics. To fa-
cilitate customization, algorithms often come with tunable parameters that signiﬁcantly impact
the computational resources they consume, such as runtime. Hand-tuning parameters can be time
consuming and may lead to sub-optimal results. In this work, we develop the foundations of au-
tomated algorithm conﬁguration via machine learning. A key challenge we face is that in order
to evaluate a conﬁguration’s requisite computational resources, the learning algorithm itself must
expend those resources.

To frame algorithm conﬁguration as a machine learning problem, we assume sample access to
an unknown distribution over problem instances, such as the integer programs an airline solves day
to day. The learning algorithm uses samples to determine parameters that, ideally, will have strong
expected performance. Researchers have studied this conﬁguration model for decades, leading to
advances in artiﬁcial intelligence [43], computational biology [19], and many other ﬁelds. This
approach has also been used in industry for tens of billions of dollars of combinatorial auctions [37].

1

 
 
 
 
 
 
Recently, two lines of research have emerged that explore the theoretical underpinnings of
algorithm conﬁguration. One provides sample complexity guarantees, bounding the number of
samples suﬃcient to ensure that an algorithm’s performance on average over the samples generalizes
to its expected performance on the distribution [5, 6, 22]. These sample complexity bounds apply
no matter how the learning algorithm operates, and these papers do not include learning algorithms
that extend beyond exhaustive search.

The second line of research provides algorithms for ﬁnding nearly-optimal conﬁgurations from
a ﬁnite set [29, 30, 39, 40]. These algorithms can also be used when the parameter space is inﬁnite:
for any γ ∈ (0, 1), ﬁrst sample ˜Ω(1/γ) conﬁgurations, and then run the algorithm over this ﬁnite
set. The authors guarantee that the output conﬁguration will be within the top γ-quantile.
If
there is only a small region of high-performing parameters, however, the uniform sample might
completely miss all good parameters. Algorithm conﬁguration problems with only tiny pockets
of high-performing parameters do indeed exist: Balcan et al. [6] present distributions over integer
programs where the optimal parameters lie within an arbitrarily small region of the parameter
space. For any parameter within that region, branch-and-bound—the most widely-used integer
programming algorithm—terminates instantaneously. Using any other parameter, branch-and-
bound takes an exponential number of steps. This region of optimal parameters can be made so
small that any random sampling technique would require an arbitrarily large sample of parameters
to hit that region. We discuss this example in more detail in Section 5.

This paper marries these two lines of research. We present an algorithm that identiﬁes a ﬁnite
set of promising parameters within an inﬁnite set, given sample access to a distribution over problem
instances. We prove that this set contains a nearly optimal parameter with high probability. The
set can serve as the input to a conﬁguration algorithm for ﬁnite parameter spaces [29, 30, 39, 40],
which we prove will then return a nearly optimal parameter from the inﬁnite set.

An obstacle in our approach is that the loss function measuring an algorithm’s performance as
a function of its parameters often exhibits jump discontinuities: a nudge to the parameters can
trigger substantial changes in the algorithm’s behavior. In order to provide guarantees, we must
tease out useful structure in the conﬁguration problems we study.

The structure we identify is simple yet ubiquitous in combinatorial domains: our approach
applies to any conﬁguration problem where the algorithm’s performance as a function of its param-
eters is piecewise constant. Prior research has demonstrated that algorithm conﬁguration problems
from diverse domains exhibit this structure. For example, Balcan et al. [6] uncovered this structure
for branch-and-bound algorithm conﬁguration. Many corporations must regularly solve reams of
integer programs, and therefore require highly customized solvers. For example, integer programs
are a part of many mesh processing pipelines in computer graphics [14]. Animation studios with
thousands of meshes require carefully tuned solvers which, thus far, domain experts have hand-
crafted [15]. Our algorithm can be used to ﬁnd conﬁgurations that minimize the branch-and-bound
tree size. Balcan et al. [5] also exhibit this piecewise-constant structure in the context of linkage-
based hierarchical clustering algorithms. The algorithm families they study interpolate between
the classic single-, complete-, and average-linkage procedures. Building the cluster hierarchy is
expensive: the best-known algorithm’s runtime is ˜O(n2) given n datapoints [34]. As with branch-
and-bound, our algorithm ﬁnds conﬁgurations that return satisfactory clusterings while minimizing
the hierarchy tree size.

We now describe our algorithm at a high level. Let (cid:96) be a loss function where (cid:96)(ρ, j) measures
the computational resources (running time, for example) required to solve problem instance j
using the algorithm parameterized by the vector ρ. Let OP T be the smallest expected loss1

1As we describe in Section 2, we compete with a slightly more nuanced benchmark than OP T , in line with prior

2

Ej∼Γ[(cid:96)(ρ, j)] of any parameter ρ, where Γ is an unknown distribution over problem instances. Our
algorithm maintains upper conﬁdence bound on OP T , initially set to ∞. On each round t, the
algorithm begins by drawing a set St of sample problem instances. It computes the partition of
the parameter space into regions where for each problem instance in St, the loss (cid:96), capped at 2t, is
a constant function of the parameters. On a given region of this partition, if the average capped
loss is suﬃciently low, the algorithm chooses an arbitrary parameter from that region and deems it
“good.” Once the cap 2t has grown suﬃciently large compared to the upper conﬁdence bound on
OP T , the algorithm returns the set of good parameters. We summarize our guarantees informally
below.

Theorem 1.1 (Informal). The following guarantees hold:

1. The set of output parameters contains a nearly-optimal parameter with high probability.
√
2. Given accuracy parameters (cid:15) and δ, the algorithm terminates after O (cid:0)ln (cid:0) 4

1 + (cid:15) · OP T /δ(cid:1)(cid:1)

rounds.

3. On the algorithm’s ﬁnal round, let P be the size of the partition the algorithm computes. The

√
number of parameters it outputs is O (cid:0)P · ln (cid:0) 4

1 + (cid:15) · OP T /δ(cid:1)(cid:1).

4. The algorithm’s sample complexity on each round t is polynomial in 2t (which scales linearly

with OP T ), log P , the parameter space dimension, 1

δ , and 1
(cid:15) .

We prove that our sample complexity can be exponentially better than the best-known uni-
form convergence bound. Moreover, it can ﬁnd strong conﬁgurations in scenarios where uniformly
sampling conﬁgurations will fail.

2 Problem deﬁnition

The algorithm conﬁguration model we adopt is a generalization of the model from prior research [29,
30, 39, 40]. There is a set Π of problem instances and an unknown distribution Γ over Π. For
example, this distribution might represent the integer programs an airline solves day to day. Each
algorithm is parameterized by a vector ρ ∈ P ⊆ Rd. At a high level, we assume we can set a
budget on the computational resources the algorithm consumes, which we quantify using an integer
τ ∈ Z≥0. For example, τ might measure the maximum running time we allow the algorithm.
There is a utility function u : P × Π × Z≥0 → {0, 1}, where u(ρ, j, τ ) = 1 if and only if the
algorithm parameterized by ρ returns a solution to the instance j given a budget of τ . We make
the natural assumption that the algorithm is more likely to ﬁnd a solution the higher its budget:
u(ρ, j, τ ) ≥ u(ρ, j, τ (cid:48)) for τ ≥ τ (cid:48). Finally, there is a loss function (cid:96) : P ×Π → Z≥0 which measures the
minimum budget the algorithm requires to ﬁnd a solution. Speciﬁcally, (cid:96)(ρ, j) = ∞ if u(ρ, j, τ ) = 0
for all τ , and otherwise, (cid:96)(ρ, j) = argmin {τ : u(ρ, j, τ ) = 1}. In Section 2.1, we provide several
examples of this problem deﬁnition instantiated for combinatorial problems.

The distribution Γ over problem instances is unknown, so we use samples from Γ to ﬁnd a

parameter vector ˆρ ∈ P with small expected loss. Ideally, we could guarantee that

E
j∼Γ

[(cid:96) ( ˆρ, j)] ≤ (1 + (cid:15)) inf
ρ∈P

(cid:26)

E
j∼Γ

(cid:27)

[(cid:96) (ρ, j)]

.

(1)

Unfortunately, this ideal goal is impossible to achieve with a ﬁnite number of samples, even in the
extremely simple case where there are only two conﬁgurations, as illustrated below.

research.

3

Figure 1: Fix a parameter vector ρ. The ﬁgure is a hypothetical illustration of the cumulative
density function of (cid:96)(ρ, j) when j is sampled from Γ. For each value τ along the x-axis, the solid
line equals Prj∼Γ [(cid:96)(ρ, j) ≤ τ ]. The dotted line equals the constant function 1 − δ. Since 100 is the
largest integer such that Prj∼Γ [(cid:96)(ρ, j) ≥ 100] ≥ δ, we have that tδ(ρ) = 100.

Example 2.1. [Weisz et al. [40]] Let P = {1, 2} be a set of two conﬁgurations. Suppose that the
loss of the ﬁrst conﬁguration is 2 for all problem instances: (cid:96)(1, j) = 2 for all j ∈ Π. Meanwhile,
suppose that (cid:96)(2, j) = ∞ with probability δ for some δ ∈ (0, 1) and (cid:96)(2, j) = 1 with probability
1 − δ. In this case, Ej∼Γ[(cid:96)(1, j)] = 2 and Ej∼Γ[(cid:96)(2, j)] = ∞. In order for any algorithm to verify
that the ﬁrst conﬁguration’s expected loss is substantially better than the second’s, it must sample
at least one problem instance j such that (cid:96)(2, j) = ∞. Therefore, it must sample Ω(1/δ) problem
instances, a lower bound that approaches inﬁnity as δ shrinks. As a result, it is impossible to give a
ﬁnite bound on the number of samples suﬃcient to ﬁnd a parameter ˆρ that satisﬁes Equation (1).

The obstacle that this example exposes is that some conﬁgurations might have an enormous
loss on a few rare problem instances. To deal with this impossibility result, Weisz et al. [39, 40],
building oﬀ of work by Kleinberg et al. [29], propose a relaxed notion of approximate optimality.
To describe this relaxation, we introduce the following notation. Given δ ∈ (0, 1) and a parameter
vector ρ ∈ P, let tδ(ρ) be the largest cutoﬀ τ ∈ Z≥0 such that the probability (cid:96)(ρ, j) is greater
than τ is at least δ. Mathematically, tδ(ρ) = argmaxτ ∈Z {Prj∼Γ[(cid:96)(ρ, j) ≥ τ ] ≥ δ}. The value tδ(ρ)
can be thought of as the beginning of the loss function’s “δ-tail.” We illustrate the deﬁnition of
tδ(ρ) in Figure 1. We now deﬁne the relaxed notion of approximate optimality by Weisz et al. [39].

Deﬁnition 2.2 (((cid:15), δ, P)-optimality). A parameter vector ˆρ is ((cid:15), δ, P)-optimal if

E
j∼Γ

[min {(cid:96) ( ˆρ, j) , tδ ( ˆρ)}] ≤ (1 + (cid:15)) inf
ρ∈P

(cid:26)

E
j∼Γ

(cid:27)
(cid:2)min (cid:8)(cid:96) (ρ, j) , tδ/2(ρ)(cid:9)(cid:3)

.

In other words, a parameter vector ˆρ is ((cid:15), δ, P)-optimal if its δ-capped expected loss is within a
(1 + (cid:15))-factor of the optimal δ/2-capped expected loss.2 To condense notation, we write OP Tcδ :=
inf ρ∈P {Ej∼Γ [min {(cid:96) (ρ, j) , tcδ(ρ)}]}. If an algorithm returns an (cid:0)(cid:15), δ, ¯P(cid:1)-optimal parameter from
within a ﬁnite set ¯P, we call it a conﬁguration algorithm for ﬁnite parameter spaces. Weisz et al.
[40] provide one such algorithm, CapsAndRuns.

2.1 Example applications

In this section, we provide several instantiations of our problem deﬁnition in combinatorial domains.

2The fraction δ/2 can be replaced with any cδ for c ∈ (0, 1). Ideally, we would replace δ/2 with δ, but the resulting

property would be impossible to verify with high probability [39].

4

Tree search. Tree search algorithms, such as branch-and-bound, are the most widely-used tools
for solving combinatorial problems, such as (mixed) integer programs and constraint satisfaction
problems. These algorithms recursively partition the search space to ﬁnd an optimal solution,
organizing this partition as a tree. Commercial solvers such as CPLEX, which use tree search
under the hood, come with hundreds of tunable parameters. Researchers have developed machine
learning algorithms for tuning these parameters [4, 6, 20, 23, 24, 27, 28, 32, 42]. Given parameters
ρ and a problem instance j, we might deﬁne the budget τ to cap the size of the tree the algorithm
In that case, the utility function is deﬁned such that u(ρ, j, τ ) = 1 if and only if the
builds.
algorithm terminates, having found the optimal solution, after building a tree of size τ . The loss
(cid:96)(ρ, j) equals the size of the entire tree built by the algorithm parameterized by ρ given the instance
j as input.

Clustering. Given a set of datapoints and the distances between each point, the goal in clustering
is to partition the points into subsets so that points within any set are “similar.” Clustering
algorithms are used to group proteins by function, classify images by subject, and myriad other
applications. Typically, the quality of a clustering is measured by an objective function, such as
the classic k-means, k-median, or k-center objectives. Unfortunately, it is NP-hard to determine
the clustering that minimizes any of these objectives. As a result, researchers have developed a
wealth of approximation and heuristic clustering algorithms. However, no one algorithm is optimal
across all applications.

Balcan et al. [5] provide sample complexity guarantees for clustering algorithm conﬁguration.
Each problem instance is a set of datapoints and there is a distribution over clustering problem
instances. They analyze several inﬁnite classes of clustering algorithms. Each of these algorithms
begins with a linkage-based step and concludes with a dynamic programming step. The linkage-
based routine constructs a hierarchical tree of clusters. At the beginning of the process, each
datapoint is in a cluster of its own. The algorithm sequentially merges the clusters into larger
clusters until all elements are in the same cluster. There are many ways to build this tree: merge
the clusters that are closest in terms of their two closest points (single-linkage), their two farthest
points (complete-linkage), or on average over all pairs of points (average-linkage). These linkage
procedures are commonly used in practice [3, 36, 41] and come with theoretical guarantees. Balcan
et al. [5] study an inﬁnite parameterization, ρ-linkage, that interpolates between single-, average-,
and complete-linkage. After building the cluster tree, the dynamic programming step returns the
pruning of this tree that minimizes a ﬁxed objective function, such as the k-means, k-median, or
k-center objectives.

Building the full hierarchy is expensive because the best-known algorithm’s runtime is O(n2 log n),
where n is the number of datapoints [34]. It is not always necessary, however, to build the entire
tree: the algorithm can preemptively terminate the linkage step after τ merges, then use dynamic
programming to recover the best pruning of the cluster forest. We refer to this variation as τ -capped
ρ-linkage. To evaluate the resulting clustering, we assume there is a cost function c : P ×Π×Z → R
where c(ρ, j, τ ) measures the quality of the clustering τ -capped ρ-linkage returns, given the instance
j as input. We assume there is a threshold θj where the clustering is admissible if and only if
c(ρ, j, τ ) ≤ θj, which means the utility function is deﬁned as u(ρ, j, τ ) = 1{c(ρ,j,τ )≤θj }. For example,
c(ρ, j, τ ) might measure the clustering’s k-means objective value, and θj might equal the optimal
k-means objective value (obtained only for the training instances via an expensive computation)
plus an error term.

5

3 Data-dependent discretizations of inﬁnite parameter spaces

We begin this section by proving an intuitive fact: given a ﬁnite subset ¯P ⊂ P of parameters that
contains at least one “suﬃciently good” parameter, a conﬁguration algorithm for ﬁnite parameter
spaces, such as CapsAndRuns [40], returns a parameter that’s nearly optimal over the inﬁnite
set P. Therefore, our goal is to provide an algorithm that takes as input an inﬁnite parameter
space and returns a ﬁnite subset that contains at least one good parameter. A bit more formally, a
parameter is “suﬃciently good” if its δ/2-capped expected loss is within a
1 + (cid:15)-factor of OP Tδ/4.
We say a ﬁnite parameter set ¯P is an ((cid:15), δ)-optimal subset if it contains a good parameter.

√

Deﬁnition 3.1 (((cid:15), δ)-optimal subset). A ﬁnite set ¯P ⊂ P is an ((cid:15), δ)-optimal subset if there is a
vector ˆρ ∈ ¯P such that Ej∼Γ

(cid:2)min (cid:8)(cid:96) ( ˆρ, j) , tδ/2 ( ˆρ)(cid:9)(cid:3) ≤

1 + (cid:15) · OP Tδ/4.

√

We now prove that given an ((cid:15), δ)-optimal subset ¯P ⊂ P, a conﬁguration algorithm for ﬁnite

parameter spaces returns a nearly optimal parameter from the inﬁnite space P.

√
Theorem 3.2. Let ¯P ⊂ P be an ((cid:15), δ)-optimal subset and let (cid:15)(cid:48) =
(cid:0)(cid:15)(cid:48), δ, ¯P(cid:1)-optimal. Then Ej∼Γ [min {(cid:96) ( ˆρ, j) , tδ ( ˆρ)}] ≤ (1 + (cid:15)) · OP Tδ/4.
Proof. Since the parameter ˆρ is (cid:0)(cid:15)(cid:48), δ, ¯P(cid:1)-optimal, we know that Ej∼Γ [min {(cid:96) ( ˆρ, j) , tδ ( ˆρ)}] ≤
√
(cid:2)min (cid:8)(cid:96) (ρ, j) , tδ/2(ρ)(cid:9)(cid:3)(cid:9). (We use a minimum instead of an inﬁmum because
¯P is a ﬁnite set by Deﬁnition 3.1.) The set ¯P is an ((cid:15), δ)-optimal subset of the parameter space P, so
there exists a parameter vector ρ(cid:48) ∈ ¯P such that Ej∼Γ
1 + (cid:15) · OP Tδ/4.
Therefore,

(cid:2)min (cid:8)(cid:96) (ρ(cid:48), j) , tδ/2 (ρ(cid:48))(cid:9)(cid:3) ≤

1 + (cid:15) − 1. Suppose ˆρ ∈ ¯P is

1 + (cid:15)·minρ∈ ¯P

(cid:8)Ej∼Γ

√

E
j∼Γ

[min {(cid:96) ( ˆρ, j) , tδ ( ˆρ)}] ≤

≤

√

√

(cid:26)

E
j∼Γ

(cid:27)
(cid:2)min (cid:8)(cid:96) (ρ, j) , tδ/2(ρ)(cid:9)(cid:3)

(cid:2)min (cid:8)(cid:96) (cid:0)ρ(cid:48), j(cid:1) , tδ/2

(cid:0)ρ(cid:48)(cid:1)(cid:9)(cid:3)

1 + (cid:15) · min
ρ∈ ¯P
1 + (cid:15) · E
j∼Γ

so the theorem statement holds.

≤ (1 + (cid:15)) · OP Tδ/4,

4 Our main result: Algorithm for learning ((cid:15), δ)-optimal subsets

We present an algorithm for learning ((cid:15), δ)-optimal subsets for conﬁguration problems that satisfy
a simple, yet ubiquitous structure: for any problem instance j, the loss function (cid:96)(·, j) is piecewise
constant. This structure has been observed throughout a diverse array of conﬁguration problems
ranging from clustering to integer programming [5, 6]. More formally, this structure holds if for
any problem instance j ∈ Π and cap τ ∈ Z≥0, there is a ﬁnite partition of the parameter space
P such that in any one region R of this partition, for all pairs of parameter vectors ρ, ρ(cid:48) ∈ R,
min {(cid:96)(ρ, j), τ } = min {(cid:96)(ρ(cid:48), j), τ }.

To exploit this piecewise-constant structure, we require access to a function Partition that
takes as input a set S of problem instances and an integer τ and returns this partition of the
parameters. Namely, it returns a set of tuples (P1, z1, τ1) , . . . , (Pk, zk, τk) ∈ 2P × [0, 1] × Z|S| such
that:

1. The sets P1, . . . , Pk make up a partition of P.

2. For all subsets Pi and vectors ρ, ρ(cid:48) ∈ Pi, 1
|S|

(cid:80)

j∈S 1{(cid:96)(ρ,j)≤τ } = 1
|S|

(cid:80)

j∈S 1{(cid:96)(ρ(cid:48),j)≤τ } = zi.

6

Algorithm 1 Algorithm for learning ((cid:15), δ)-optimal subsets
Input: Parameters δ, ζ ∈ (0, 1), (cid:15) > 0.
1: Set η ← min (cid:8) 1
8
2: while 2t−3δ < T do
3:

1 + (cid:15) − 1(cid:1) , 1
9

√
(cid:0) 4

(cid:9), t ← 1, T ← ∞, and G ← ∅.

(cid:113) 2d ln|Partition(St,2t)|
|St|

Set St ← {j}, where j ∼ Γ.
+
while ηδ <
Compute tuples (P1, z1, τ1) , . . . , (Pk, zk, τk) ← Partition (cid:0)St, 2t(cid:1).
for i ∈ {1, . . . , k} with zi ≥ 1 − 3δ/8 do

|St| ln 8(2t|St|t)2

(cid:113)

8

ζ

Set G ← G ∪ {Pi}.
Sort the elements of τi: τ1 ≤ · · · ≤ τ|St|.
Set T (cid:48) ← 1
|St|
if T (cid:48) < T then Set T ← T (cid:48).

m=1 min (cid:8)τm, τ(cid:98)|St|(1−3δ/8)(cid:99)

(cid:80)|St|

(cid:9) .

4:

5:

6:

7:

8:

9:

10:

do Draw j ∼ Γ and add j to St.

t ← t + 1.

11:
12: For each set P (cid:48) ∈ G, select a vector ρP (cid:48) ∈ P (cid:48).
Output: The ((cid:15), δ)-optimal set {ρP (cid:48) | P (cid:48) ∈ G}.

3. For all subsets Pi, all ρ, ρ(cid:48) ∈ Pi, and all j ∈ S, min {(cid:96)(ρ, j), τ } = min {(cid:96)(ρ(cid:48), j), τ } = τi[j].

We assume the number of tuples Partition returns is monotone: if τ ≤ τ (cid:48), then |Partition(S, τ )| ≤
|Partition(S, τ (cid:48))| and if S ⊆ S (cid:48), then |Partition(S, τ )| ≤ |Partition(S (cid:48), τ )|.

As we describe in Appendix C, results from prior research imply guidance for implementing
Partition in the contexts of clustering and integer programming. For example, in the clustering
application we describe in Section 2.1, the distribution Γ is over clustering instances. Suppose n is
an upper bound on the number of points in each instance. Balcan et al. [5] prove that for any set
S of samples and any cap τ , in the worst case, |Partition(S, τ )| = O (cid:0)|S|n8(cid:1), though empirically,
|Partition(S, τ )| is often several orders of magnitude smaller [9]. Balcan et al. [5] and Balcan
et al. [9] provide guidance for implementing Partition.

High-level description of algorithm. We now describe our algorithm for learning ((cid:15), δ)-optimal
subsets. See Algorithm 1 for the pseudocode. The algorithm maintains a variable T , initially set
to ∞, which roughly represents an upper conﬁdence bound on OP Tδ/4. It also maintains a set G of
parameters which the algorithm believes might be nearly optimal. The algorithm begins by aggres-
sively capping the maximum loss (cid:96) it computes by 1. At the beginning of each round, the algorithm
doubles this cap until the cap grows suﬃciently large compared to the upper conﬁdence bound T .
At that point, the algorithm terminates. On each round t, the algorithm draws a set St of samples
(cid:2)min (cid:8)(cid:96)(ρ, j), 2t(cid:9)(cid:3)
(Step 4) that is just large enough to estimate the expected 2t-capped loss Ej∼Γ
for every parameter ρ ∈ P. The number of samples it draws is a data-dependent quantity that
depends on empirical Rademacher complexity [11, 31].

Next, the algorithm evaluates the function Partition (cid:0)St, 2t(cid:1) to obtain the tuples

(P1, z1, τ1) , . . . , (Pk, zk, τk) ∈ 2P × [0, 1] × Z|St|.

By deﬁnition of this function, for all subsets Pi and parameter vector pairs ρ, ρ(cid:48) ∈ Pi, the fraction
of instances j ∈ St with (cid:96)(ρ, j) ≤ 2t is equal to the fraction of instances j ∈ St with (cid:96)(ρ(cid:48), j) ≤ 2t. In
other words,
1{(cid:96)(ρ(cid:48),j)≤2t} = zi. If this fraction is suﬃciently high
(at least 1 − 3δ/8), the algorithm adds Pi to the set of good parameters G (Step 7). The algorithm

1{(cid:96)(ρ,j)≤2t} = 1
|St|

1
|St|

j∈St

j∈St

(cid:80)

(cid:80)

7

estimates the δ/4-capped expected loss of the parameters contained Pi, and if this estimate is
smaller than the current upper conﬁdence bound T on OP Tδ/4, it updates T accordingly (Steps 8
through 10). Once the cap 2t has grown suﬃciently large compared to the upper conﬁdence bound
T , the algorithm returns an arbitrary parmeter from each set in G.

Algorithm analysis. We now provide guarantees on Algorithm 1’s performance. We denote the
values of t and T at termination by ¯t and ¯T , and we denote the state of the set G at termination by
¯G. For each set P (cid:48) ∈ ¯G, we use the notation τP (cid:48) to denote the value τ(cid:98)|St|(1−3δ/8)(cid:99) in Step 9 during
the iteration t that P (cid:48) is added to G.

Theorem 4.1. With probability 1 − ζ, the following conditions hold, with

η = min

√
(cid:26) 4

(cid:27)

1 + (cid:15) − 1
8

,

1
9

and

c =

√
16 4

1 + (cid:15)
δ

:

1. Algorithm 1 terminates after ¯t = O (cid:0)log (cid:0)c · OP Tδ/4

(cid:1)(cid:1) iterations.

2. Algorithm 1 returns an ((cid:15), δ)-optimal set of parameters of size at most

¯t
(cid:88)

t=1

(cid:12)
(cid:12)Partition (cid:0)St, c · OP Tδ/4

(cid:1)(cid:12)
(cid:12) .

3. The sample complexity on round t ∈ [¯t], |St|, is
d ln (cid:12)

(cid:12)Partition (cid:0)St, c · OP Tδ/4

(cid:32)

˜O

η2δ2

(cid:1)(cid:12)
(cid:12) + c · OP Tδ/4

(cid:33)

.

Proof. We split the proof into separate lemmas. Lemma 4.4 proves Part 1. Lemma 4.6 as well as
Lemma B.11 in Appendix B prove Part 2. Finally, Part 3 follows from classic results in learning
theory on Rademacher complexity. In particular, it follows from an inversion of the inequality in
Step 4 and the fact that 2t ≤ 2¯t ≤ c · OP Tδ/4, as we prove in Lemma 4.4.

Theorem 4.1 hinges on the assumption that the samples S1, . . . , S¯t Algorithm 1 draws in Step 4

are suﬃciently representative of the distribution Γ, formalized as follows:
Deﬁnition 4.2 (ζ-representative run). For each round t ∈ [¯t], denote the samples in St as St =
(cid:110)
. We say that Algorithm 1 has a ζ-representative run if for all rounds t ∈ [¯t], all

(cid:111)

: i ∈ [|St|]

j(t)
i

integers b ∈ [|St|], all caps τ ∈ Z≥0, and all parameters ρ ∈ P, the following conditions hold:
(cid:110)

(cid:111)

1. The average number of instances j(t)

1 , . . . , j(t)
j(t)

b

the probability that (cid:96)(ρ, j) ≤ τ :

(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1 1(cid:110)
(cid:96)

(cid:16)
ρ,j(t)
i

(cid:17)

≤τ

i ∈
(cid:80)b

1
b

with loss smaller than τ nearly matches

(cid:12)
(cid:12)
(cid:111) − Prj∼Γ [(cid:96)(ρ, j) ≤ τ ]
(cid:12)
(cid:12)

≤ γ(t, b, τ ), and

2. The average τ -capped loss of the instances j(t)

1 , . . . , j(t)

b nearly matches the expected τ -capped

loss:

where

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
b

b
(cid:88)

i=1

min

(cid:16)

(cid:110)
(cid:96)

ρ, j(t)
i

(cid:17)

(cid:111)

, τ

(cid:12)
(cid:12)
(cid:12)
[min {(cid:96)(ρ, j), τ }]
(cid:12)
(cid:12)

− E
j∼Γ

≤ τ · γ(t, b, τ ),

(cid:118)
(cid:117)
(cid:117)
(cid:116)

2d ln

(cid:12)
(cid:12)
(cid:12)

Partition

γ(t, b, τ ) =

(cid:111)

, τ

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:115)

+ 2

2
b

ln

8 (τ bt)2
ζ

.

(cid:16)(cid:110)

1 , . . . , j(t)
j(t)

b

b

8

In Step 4, we ensure St is large enough that Algorithm 1 has a ζ-representative run with

probability 1 − ζ.

Lemma 4.3. With probability 1 − ζ, Algorithm 1 has a ζ-representative run.

Lemma 4.3 is a corollary of a Rademacher complexity analysis which we include in Appendix B
(Lemma B.1). Intuitively, there are only |Partition(S, τ )| algorithms with varying τ -capped losses
over any set of samples S. We can therefore invoke Massart’s ﬁnite lemma [35], which guarantees
that each set St is suﬃciently large to ensure that Algorithm 1 indeed has a ζ-representative run.
The remainder of our analysis will assume that Algorithm 1 has a ζ-representative run.

Number of iterations until termination. We begin with a proof sketch of the ﬁrst part of
Theorem 4.1. The full proof is in Appendix B.

Lemma 4.4. Suppose Algorithm 1 has a ζ-representative run. Then 2¯t ≤ 16
δ

√
4

1 + (cid:15) · OP Tδ/4.

Proof sketch. By deﬁnition of OP Tδ/4, for every γ > 0, there exists a vector ρ∗ ∈ P whose δ/4-
(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4 (ρ∗)(cid:9)(cid:3) ≤ OP Tδ/4 +γ.
capped expected loss is within a γ-factor of optimal: Ej∼Γ
We prove this vector’s δ/4-capped expected loss bounds ¯t:

2¯t ≤

√
16 4

1 + (cid:15)
δ

· E
j∼Γ

(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4 (ρ∗)(cid:9)(cid:3) .

(2)

This implies the lemma statement holds. We split the proof of Inequality (2) into two cases: one
where the vector ρ∗ is contained within a set P (cid:48) ∈ ¯G, and the other where it is not. In the latter
(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4 (ρ∗)(cid:9)(cid:3), which implies Inequality (2)
case, Lemma 4.5 bounds 2¯t by 8
holds. We leave the other case to the full proof in Appendix B.

δ · Ej∼Γ

In the next lemma, we prove the upper bound on 2¯t that we use in Lemma 4.4. The full proof

is in Appendix B.

Lemma 4.5. Suppose Algorithm 1 has a ζ-representative run. For any parameter vector ρ (cid:54)∈
(cid:83)

P (cid:48)∈ ¯G P (cid:48), 2¯t ≤ 8

δ · Ej∼Γ

(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/4(ρ)(cid:9)(cid:3) .

Proof sketch. The last round that Algorithm 1 adds any subset to the set G is round ¯t − 1. For
ease of notation, let ¯S = S¯t−1. Since ρ is not an element of any set in G, the cap 2¯t−1 must
be too small compared to the average loss of the parameter ρ. Speciﬁcally, it must be that
1
8 . Since Algorithm 1 had a ζ-representative run, the probability
| ¯S|
the loss of ρ is smaller than 2¯t−1 converges to the fraction of samples with loss smaller than 2¯t−1:
Prj∼Γ
< 1 − (3/8 − η)δ.

j∈ ¯S 1{(cid:96)(ρ,j)≤2¯t−1} < 1 − 3δ

(cid:96)(ρ, j) ≤ 2¯t−1(cid:105)
(cid:104)

(cid:96)(ρ, j) ≤ 2¯t−1(cid:105)

(cid:80)

(cid:80)

(cid:104)

≤ 1
| ¯S|

j∈ ¯S 1{(cid:96)(ρ,j)≤2¯t−1} + ηδ, so Prj∼Γ
(cid:96)(ρ, j) ≥ 2¯t−1(cid:105)

(cid:104)

Since η ≤ 1/9, it must be that Prj∼Γ
that 2¯t−1 ≤ tδ/4(ρ). Therefore, Ej∼Γ

≥ δ/4, so by deﬁnition of tδ/4(ρ), we have

(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/4(ρ)(cid:9)(cid:3) ≥ δ

4 · tδ/4(ρ) ≥ 2¯t−3δ.

Optimality of Algorithm 1’s output. Next, we provide a proof sketch of the second part of
Theorem 4.1, which guarantees that Algorithm 1 returns an ((cid:15), δ)-optimal subset. The full proof is
in Appendix B. For each set P (cid:48) ∈ ¯G, τP (cid:48) denotes the value of τ(cid:98)|St|(1−3δ/8)(cid:99) in Step 9 of Algorithm 1
during the iteration t that P (cid:48) is added to G.

Lemma 4.6. If Algorithm 1 has a ζ-representative run, it returns an ((cid:15), δ)-optimal subset.

9

Proof sketch. By deﬁnition of OP Tδ/4, for every γ > 0, there is a vector ρ∗ ∈ P such that
(cid:2)min (cid:8)(cid:96)(ρ∗, j), tδ/4(ρ∗)(cid:9)(cid:3) ≤ OP Tδ/4 + γ. Let P ∗ be the output of Algorithm 1. We claim
Ej∼Γ
there exists a parameter ρ(cid:48) ∈ P ∗ such that

(cid:2)min (cid:8)(cid:96) (cid:0)ρ(cid:48), j(cid:1) , tδ/2

(cid:0)ρ(cid:48)(cid:1)(cid:9)(cid:3) ≤

E
j∼Γ

√

1 + (cid:15) · E
j∼Γ

(cid:2)min (cid:8)(cid:96)(ρ∗, j), tδ/4(ρ∗)(cid:9)(cid:3) ,

(3)

which implies the lemma statement. There are two cases: either the vector ρ∗ is contained within
a set P (cid:48) ∈ ¯G, or it is not. In this sketch, we analyze the latter case.

By Lemma 4.5, we know that Ej∼Γ

(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4(ρ∗)(cid:9)(cid:3) ≥ 2¯t−3δ. When Algorithm 1 ter-

minates, 2¯t−3δ is greater than the upper conﬁdence bound ¯T , which means that

(cid:2)min (cid:8)(cid:96)(ρ∗, j), tδ/4(ρ∗)(cid:9)(cid:3) > ¯T .

E
j∼Γ

We next derive a lower bound on ¯T : we prove that there exists a set P (cid:48) ∈ ¯G and parameter vector
√
1 + (cid:15) · ¯T ≥ Ej∼Γ [min {(cid:96) (ρ, j) , τP (cid:48)}]. Our upper bound on ¯T implies that
ρ ∈ P (cid:48) such that 4
√
(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4(ρ)(cid:9)(cid:3). Finally, we prove that there is
Ej∼Γ [min {(cid:96) (ρ, j) , τP (cid:48)}] ≤ 4
1 + (cid:15) · Ej∼Γ
√
a parameter ρ(cid:48) ∈ P (cid:48) ∩ P ∗ whose δ/2-capped expected loss is within a 4
1 + (cid:15)-factor of the expected
loss Ej∼Γ [min {(cid:96) (ρ, j) , τP (cid:48)}]. This follows from a proof that τP (cid:48) approximates tδ/4(ρ(cid:48)) and the
fact that ρ and ρ(cid:48) are both elements of P (cid:48). Stringing these inequalities together, we prove that
Equation (3) holds.

The second part of Theorem 4.1 also guarantees that the size of the set Algorithm 1 returns is
bounded (see Lemma B.11 in Appendix B). Together, Lemmas 4.4 and 4.6, as well as Lemma B.11
in Appendix B, bound the number of iterations Algorithm 1 makes until it returns an ((cid:15), δ)-optimal
subset.

5 Comparison to prior research

We now provide comparisons to prior research on algorithm conﬁguration with provable guarantees.
Both comparisons revolve around branch-and-bound (B&B) conﬁguration for integer programming
(IP), overviewed in Section 2.1.

Uniformly sampling conﬁgurations. Prior research provides algorithms for ﬁnding nearly-
If the parameter space is inﬁnite and
optimal conﬁgurations from a ﬁnite set [29, 30, 39, 40].
their algorithms optimize over a uniformly-sampled set of ˜Ω(1/γ) conﬁgurations, then the output
conﬁguration will be within the top γ-quantile, with high probability. If the set of good parameters
is small, however, the uniform sample might not include any of them. Algorithm conﬁguration
problems where the high-performing parameters lie within a small region do exist, as we illustrate
in the following theorem.

Theorem 5.1 (Balcan et al. [6]). For any 1
distributions Γ over IPs with n variables and a B&B parameter3 with range [0, 1] such that:

2 and n ≥ 6, there are inﬁnitely-many

3 < a < b < 1

1. If ρ ≤ a, then (cid:96)(ρ, j) = 2(n−5)/4 with probability 1

2 and (cid:96)(ρ, j) = 8 with probability 1
2 .

2. If ρ ∈ (a, b), then (cid:96)(ρ, j) = 8 with probability 1.

3As we describe in Appendix C.2, ρ controls the variable selection policy. The theorem holds for any node selection

policy.

10

3. If ρ ≥ b, then (cid:96)(ρ, j) = 2(n−4)/2 with probability 1

2 and (cid:96)(ρ, j) = 8 with probability 1
2 .

Here, (cid:96)(ρ, j) measures the size of the tree B&B builds using the parameter ρ on the input integer
program j.

In the above conﬁguration problem, any parameter in the range (a, b) has a loss of 8 with
probability 1, which is the minimum possible loss. Any parameter outside of this range has an
abysmal expected loss of at least 2(n−6)/2. In fact, for any δ ≤ 1/2, the δ-capped expected loss of
any parameter in the range [0, a]∪[b, 1] is at least 2(n−6)/2. Therefore, if we uniformly sample a ﬁnite
set of parameters and optimize over this set using an algorithm for ﬁnite parameter spaces [29, 30,
39, 40], we must ensure that we sample at least one parameter within (a, b). As a and b converge,
however, the required number of samples shoots to inﬁnity, as we formalize below. This section’s
omitted proofs are in Appendix C.2.

Theorem 5.2. For the B&B conﬁguration problem in Theorem 5.1, with constant probability over
the draw of m = (cid:98)1/(b − a)(cid:99) parameters ρ1, . . . , ρm ∼ Uniform[0, 1], {ρ1, . . . , ρm} ∩ (a, b) = ∅.

Meanwhile, Algorithm 1 quickly terminates, having found an optimal parameter, as we describe

below.

Theorem 5.3. For the B&B conﬁguration problem in Theorem 5.1, Algorithm 1 terminates after
˜O(log 1/δ) iterations, having drawn ˜O((δη)−2) sample problem instances where

η = min

√
(cid:26) 4

(cid:27)

1 + (cid:15) − 1
8

,

1
9

and returns a set containing an optimal parameter in (a, b).

Similarly, Balcan et al. [8] exemplify clustering conﬁguration problems—which we overview in
Section 2.1—where the optimal parameters lie within an arbitrarily small region, and any other
parameter leads to signiﬁcantly worse performance. As in Theorem 5.2, this means a uniform
sampling of the parameters will fail to ﬁnd optimal parameters.

Uniform convergence. Prior research has provided uniform convergence sample complexity
bounds for algorithm conﬁguration. These guarantees bound the number of samples suﬃcient to
ensure that for any conﬁguration, its average loss over the samples nearly matches its expected
loss.

We prove that in the case of B&B conﬁguration, Algorithm 1 may use far fewer samples to ﬁnd
a nearly optimal conﬁguration than the best-known uniform convergence sample complexity bound.
Balcan et al. [6] prove uniform convergence sample complexity guarantees for B&B conﬁguration.
They bound the number of samples suﬃcient to ensure that for any conﬁguration in their inﬁnite
parameter space, the size of the search tree B&B builds on average over the samples generalizes to
the expected size of the tree it builds. For integer programs over n variables, the best-known sample
complexity bound guarantees that (2n/(cid:15)(cid:48))2 samples are suﬃcient to ensure that the average tree size
B&B builds over the samples is within an additive (cid:15)(cid:48) factor of the expected tree size [6]. Meanwhile,
as we describe in Theorem 5.3, there are B&B conﬁguration problems where our algorithm’s sample
complexity bound is signiﬁcantly better: our algorithm ﬁnds an optimal parameter using only
˜O((δη)−2) samples.

11

6 Conclusion

We presented an algorithm that learns a ﬁnite set of promising parameters from an inﬁnite param-
eter space. It can be used to determine the input to a conﬁguration algorithm for ﬁnite parameter
spaces, or as a tool for compiling an algorithm portfolio. We proved bounds on the number of itera-
tions before our algorithm terminates, its sample complexity, and the size of its output. A strength
of our approach is its modularity: it can determine the input to a conﬁguration algorithm for ﬁnite
parameter spaces without depending on speciﬁcs of that algorithm’s implementation. There is an
inevitable tradeoﬀ, however, between modularity and computational eﬃciency. In future research,
our approach can likely be folded into existing conﬁguration algorithms for ﬁnite parameter spaces.

Acknowledgments

This material is based on work supported by the National Science Foundation under grants IIS-
1718457, IIS-1617590, IIS-1618714, IIS-1901403, CCF-1535967, CCF-1910321, SES-1919453, and
CCF-1733556, the ARO under award W911NF-17-1-0082, an Amazon Research Award, an AWS
Machine Learning Research Award, a Bloomberg Data Science research grant, a fellowship from
Carnegie Mellon University’s Center for Machine Learning and Health, and the IBM PhD Fellow-
ship.

References

[1] Tobias Achterberg. SCIP: solving constraint integer programs. Mathematical Programming

Computation, 1(1):1–41, 2009.

[2] Daniel Alabi, Adam Tauman Kalai, Katrina Ligett, Cameron Musco, Christos Tzamos, and
Ellen Vitercik. Learning to prune: Speeding up repeated computations. In Proceedings of the
Conference on Learning Theory (COLT), 2019.

[3] Pranjal Awasthi, Maria-Florina Balcan, and Konstantin Voevodski. Local algorithms for in-

teractive clustering. The Journal of Machine Learning Research, 18(1):75–109, 2017.

[4] Amine Balafrej, Christian Bessiere, and Anastasia Paparrizou. Multi-armed bandits for adap-
tive constraint propagation. Proceedings of the International Joint Conference on Artiﬁcial
Intelligence (IJCAI), 2015.

[5] Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. Learning-
theoretic foundations of algorithm conﬁguration for combinatorial partitioning problems. In
Proceedings of the Conference on Learning Theory (COLT), 2017.

[6] Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch.

In Proceedings of the International Conference on Machine Learning (ICML), 2018.

[7] Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm
design, online learning, and private optimization. In Proceedings of the IEEE Symposium on
Foundations of Computer Science (FOCS), 2018.

[8] Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. Learning-
theoretic foundations of algorithm conﬁguration for combinatorial partitioning problems. arXiv
preprint arXiv:1611.04535, 2018.

12

[9] Maria-Florina Balcan, Travis Dick, and Manuel Lang. Learning to link. arXiv preprint

arXiv:1907.00533, 2019.

[10] Maria-Florina Balcan, Travis Dick, and Wesley Pegden. Semi-bandit optimization in the

dispersed setting. CoRR, abs/1904.09014, 2019.

[11] Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds

and structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.

[12] James S Bergstra, R´emi Bardenet, Yoshua Bengio, and Bal´azs K´egl. Algorithms for hyper-
In Proceedings of the Annual Conference on Neural Information

parameter optimization.
Processing Systems (NeurIPS), pages 2546–2554, 2011.

[13] Felix Berkenkamp, Angela P Schoellig, and Andreas Krause. No-regret bayesian optimization
with unknown hyperparameters. Journal of Machine Learning Research, 20:1–24, 2019.

[14] David Bommes, Henrik Zimmer, and Leif Kobbelt. Mixed-integer quadrangulation. ACM

Transactions On Graphics (TOG), 28(3):77, 2009.

[15] David Bommes, Henrik Zimmer, and Leif Kobbelt. Practical mixed-integer optimization for
In International Conference on Curves and Surfaces, pages 193–206,

geometry processing.
2010.

[16] Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of ex-
pensive cost functions, with application to active user modeling and hierarchical reinforcement
learning. arXiv preprint arXiv:1012.2599, 2010.

[17] R. C. Buck. Partition of space. Amer. Math. Monthly, 50:541–544, 1943. ISSN 0002-9890.

[18] Vincent Cohen-Addad and Varun Kanade. Online Optimization of Smoothed Piecewise Con-
stant Functions. In Proceedings of the International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), 2017.

[19] Dan DeBlasio and John D Kececioglu. Parameter Advising for Multiple Sequence Alignment.

Springer, 2018.

[20] John P. Dickerson and Tuomas Sandholm. Throwing darts: Random sampling helps tree search
when the number of short certiﬁcates is moderate. In Proceedings of the Annual Symposium
on Combinatorial Search (SoCS), 2013. Also in AAAI-13 Late-Breaking paper track.

[21] Andrew Gilpin and Tuomas Sandholm.

Information-theoretic approaches to branching in

search. Discrete Optimization, 8(2):147–159, 2011. Early version in IJCAI-07.

[22] Rishi Gupta and Tim Roughgarden. A PAC approach to application-speciﬁc algorithm selec-

tion. SIAM Journal on Computing, 46(3):992–1017, 2017.

[23] He He, Hal Daume III, and Jason M Eisner. Learning to search in branch and bound algo-
rithms. In Proceedings of the Annual Conference on Neural Information Processing Systems
(NeurIPS), 2014.

[24] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. ParamILS: an automatic algorithm
conﬁguration framework. Journal of Artiﬁcial Intelligence Research, 36(1):267–306, 2009.

13

[25] Frank Hutter, Holger Hoos, and Kevin Leyton-Brown. Bayesian optimization with censored
response data. In NIPS workshop on Bayesian Optimization, Sequential Experimental Design,
and Bandits (BayesOpt’11), 2011.

[26] Frank Hutter, Holger Hoos, and Kevin Leyton-Brown. Sequential model-based optimization
for general algorithm conﬁguration. In International Conference on Learning and Intelligent
Optimization (LION), pages 507–523, 2011.

[27] Elias Boutros Khalil, Pierre Le Bodic, Le Song, George Nemhauser, and Bistra Dilkina. Learn-
In Proceedings of the AAAI Conference on

ing to branch in mixed integer programming.
Artiﬁcial Intelligence, 2016.

[28] Elias Boutros Khalil, Bistra Dilkina, George Nemhauser, Shabbir Ahmed, and Yufen Shao.
Learning to run heuristics in tree search. In Proceedings of the International Joint Conference
on Artiﬁcial Intelligence (IJCAI), 2017.

[29] Robert Kleinberg, Kevin Leyton-Brown, and Brendan Lucier. Eﬃciency through procrastina-
tion: Approximately optimal algorithm conﬁguration with runtime guarantees. In Proceedings
of the International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2017.

[30] Robert Kleinberg, Kevin Leyton-Brown, Brendan Lucier, and Devon Graham. Procrastinating
with conﬁdence: Near-optimal, anytime, adaptive algorithm conﬁguration. Proceedings of the
Annual Conference on Neural Information Processing Systems (NeurIPS), 2019.

[31] Vladimir Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Trans-

actions on Information Theory, 47(5):1902–1914, 2001.

[32] Markus Kruber, Marco E L¨ubbecke, and Axel Parmentier. Learning when to use a decompo-
sition. In International Conference on AI and OR Techniques in Constraint Programming for
Combinatorial Optimization Problems, pages 202–210. Springer, 2017.

[33] Jeﬀ T Linderoth and Martin WP Savelsbergh. A computational study of search strategies for

mixed integer programming. INFORMS Journal on Computing, 11(2):173–187, 1999.

[34] Christopher Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. Introduction to information

retrieval. Natural Language Engineering, 16(1):100–103, 2010.

[35] Pascal Massart. Some applications of concentration inequalities to statistics. In Annales de la

Facult´e des sciences de Toulouse: Math´ematiques, volume 9, pages 245–303, 2000.

[36] Mehreen Saeed, Onaiza Maqbool, Haroon Atique Babri, Syed Zahoor Hassan, and S Mansoor
Sarwar. Software clustering techniques and the use of combined algorithm. In Proceedings of
the European Conference on Software Maintenance and Reengineering, pages 301–306. IEEE,
2003.

[37] Tuomas Sandholm. Very-large-scale generalized combinatorial multi-attribute auctions:
In Zvika Neeman, Alvin Roth, and Nir

Lessons from conducting $60 billion of sourcing.
Vulkan, editors, Handbook of Market Design. Oxford University Press, 2013.

[38] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of ma-
chine learning algorithms. In Proceedings of the Annual Conference on Neural Information
Processing Systems (NeurIPS), pages 2951–2959, 2012.

14

[39] Gell´ert Weisz, Andr´es Gy¨orgy, and Csaba Szepesv´ari. LeapsAndBounds: A method for
approximately optimal algorithm conﬁguration. In Proceedings of the International Conference
on Machine Learning (ICML), 2018.

[40] Gell´ert Weisz, Andr´es Gy¨orgy, and Csaba Szepesv´ari. CapsAndRuns: An improved method
for approximately optimal algorithm conﬁguration. Proceedings of the International Conference
on Machine Learning (ICML), 2019.

[41] James R White, Saket Navlakha, Niranjan Nagarajan, Mohammad-Reza Ghodsi, Carl Kings-
ford, and Mihai Pop. Alignment and clustering of phylogenetic markers-implications for mi-
crobial diversity studies. BMC bioinformatics, 11(1):152, 2010.

[42] Wei Xia and Roland Yap. Learning robust search strategies using a bandit-based approach.

In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2018.

[43] Lin Xu, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. SATzilla: Portfolio-based
algorithm selection for SAT. Journal of Artiﬁcial Intelligence Research, 32:565–606, June 2008.

A Review of Rademacher complexity

At a high level, Rademacher complexity measures the extent to which a class of functions ﬁt random
noise.
Intuitively, this measures the richness of a function class because more complex classes
should be able to ﬁt random noise better than simple classes. Empirical Rademacher complexity
can be measured on the set of samples and implies generalization guarantees that improve based
on structure exhibited by the set of samples. Formally, let F ⊆ [0, H]X be an abstract function
class mapping elements of a domain X to the interval [0, H]. Given a set S = {x1, . . . , xN } ⊆ X ,
the empirical Rademacher complexity of F with respect to S is deﬁned as

(cid:34)

(cid:98)RS (F ) = E
σ

sup
f ∈F

(cid:35)

σi · f (xi)

,

1
N

N
(cid:88)

i=1

where σi ∼ Uniform ({−1, 1}). Let D be a distribution over X . Classic results from learning
theory [11, 31] guarantee that with probability 1 − δ over the draw S ∼ DN , for every function
f ∈ F ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

(cid:88)

x∈S

f (x) − E
x∼D

(cid:12)
(cid:12)
(cid:12)
[f (x)]
(cid:12)
(cid:12)

≤ (cid:98)RS (F ) + 2H

(cid:114)

2 ln (4/δ)
N

.

Massart [35] proved the following Rademacher complexity bound for the case where the set

{(f (x1) , . . . , f (xN )) : f ∈ F } is ﬁnite.

Lemma A.1. [Massart [35]] Let F |S = {(f (x1) , . . . , f (xN )) : f ∈ F }.
(cid:98)RS (F ) ≤

, where r = maxa∈F |S ||a||2.

2 log|F |S |

√

r

N

If |F |S | is ﬁnite, then

B Additional proofs and lemmas about Algorithm 1

The following Rademacher complexity bound guarantees that with probability 1 − ζ, Algorithm 1
has a ζ-representative run.

15

Lemma B.1. For any τ ∈ Z≥0, deﬁne the function classes Fτ = {j (cid:55)→ min{(cid:96)(ρ, j), τ } | ρ ∈ P}
and Hτ = (cid:8)j (cid:55)→ 1{(cid:96)(ρ,j)≤τ } | ρ ∈ P(cid:9). Let S ⊆ Π be a set of problem instances. Then (cid:98)RS (Fτ ) ≤
(cid:113) 2d ln|Partition(S,τ )|
(cid:113) 2d ln|Partition(S,τ )|
τ
|S|
|S|

and (cid:98)RS (Hτ ) ≤

.

Proof. This lemma follows from Lemma A.1.

Lemma B.2. Suppose Algorithm 1 has a ζ-representative run. For any set P (cid:48) ∈ ¯G and all param-
eters ρ ∈ P (cid:48), tδ/2(ρ) ≤ τP (cid:48) ≤ tδ/4(ρ).

Proof. We begin by proving that tδ/2(ρ) ≤ τP (cid:48).
Claim B.3. Suppose Algorithm 1 has a ζ-representative run. For any set P (cid:48) ∈ G and all parameters
ρ ∈ P (cid:48), tδ/2(ρ) ≤ τP (cid:48).

Proof of Claim B.3. Let t be the round that P (cid:48) is added to G. We know that for all parameter
vectors ρ ∈ P (cid:48),

1
|St|

(cid:88)

j∈St

1{(cid:96)(ρ,j)≤τP(cid:48) } ≥

(cid:98)|St| (1 − 3δ/8)(cid:99)
|St|

≥ 1 −

3δ
8

−

1
|St|

.

(4)

Since Algorithm 1 had a ζ-representative run, we know that

Pr
j∼Γ

[(cid:96)(ρ, j) ≤ τP (cid:48)] ≥

≥

1
|St|

1
|St|

(cid:88)

j∈St

(cid:88)

j∈St

1{(cid:96)(ρ,j)≤τP(cid:48) } −

1{(cid:96)(ρ,j)≤τP(cid:48) } −

(cid:115)

(cid:115)

2d ln |Partition (St, τP (cid:48))|
|St|

(cid:115)

− 2

2
|St|

ln

8 (τP (cid:48) |St| t)2
ζ

2d ln |Partition (St, 2t)|
|St|

(cid:115)

− 2

2
|St|

ln

8 (2t |St| t)2
ζ

,

where the second inequality follows from the fact that τP (cid:48) ≤ 2t and monotonicity. Based on Step 4
of Algorithm 1, we know that

Pr
j∼Γ

[(cid:96)(ρ, j) ≤ τP (cid:48)] ≥

1
|St|

(cid:88)

j∈St

1{(cid:96)(ρ,j)≤τP(cid:48) } − ηδ.

Moreover, by Equation (4), Prj∼Γ [(cid:96)(ρ, j) ≤ τP (cid:48)] ≥ 1− 3δ
we also know that

≤ ηδ. Therefore, Prj∼Γ [(cid:96)(ρ, j) ≤ τP (cid:48)] ≥ 1 − (cid:0) 3

|St| −ηδ. Based on Step 4 of Algorithm 1,
8 + η2 + η(cid:1) δ. Finally, since

8 − 1

1√

|St|

η ≤ 1

9 , we have that Prj∼Γ [(cid:96)(ρ, j) ≤ τP (cid:48)] > 1 − δ/2, which means that

Pr
j∼Γ

[(cid:96)(ρ, j) > τP (cid:48)] = Pr
j∼Γ

[(cid:96)(ρ, j) ≥ τP (cid:48) + 1] <

δ
2

.

(5)

We claim that Equation (5) implies that tδ/2(ρ) ≤ τP (cid:48). For a contradiction, suppose tδ/2(ρ) >
τP (cid:48), or in other words, tδ/2(ρ) ≥ τP (cid:48) + 1. Since tδ/2(ρ) = argmaxτ ∈Z {Prj∼Γ[(cid:96)(ρ, j) ≥ τ ] ≥ δ/2},
(cid:2)(cid:96)(ρ, j) ≥ tδ/2(ρ)(cid:3) ≤ Prj∼Γ [(cid:96)(ρ, j) ≥ τP (cid:48) + 1] < δ/2, which is a
this would mean that δ/2 ≤ Prj∼Γ
contradiction. Therefore, the claim holds.

Next, we prove that τP (cid:48) ≤ tδ/4(ρ).

Claim B.4. Suppose Algorithm 1 has a ζ-representative run. For any set P (cid:48) ∈ G and all parameters
ρ ∈ P (cid:48), τP (cid:48) ≤ tδ/4(ρ).

16

Proof of Claim B.4. Let t be the round that P (cid:48) is added to G. We know that for all parameter
vectors ρ ∈ P (cid:48),

1 −

1
|St|

(cid:88)

j∈St

1{(cid:96)(ρ,j)≤τP(cid:48) −1} = 1 −

1
|St|

(cid:88)

j∈St

1{(cid:96)(ρ,j)<τP(cid:48) }

=

=

≥

≥

1
|St|

1
|St|

(cid:88)

j∈St
(cid:88)

j∈St

(cid:0)1 − 1{(cid:96)(ρ,j)<τP(cid:48) }

(cid:1)

1{(cid:96)(ρ,j)≥τP(cid:48) }

|St| − (cid:98)|St| (1 − 3δ/8)(cid:99)
|St|

3δ
8

.

Therefore,
know that

1
|St|

(cid:80)

j∈St

1{(cid:96)(ρ,j)≤τP(cid:48) −1} ≤ 1 − 3δ

8 . Since Algorithm 1 had a ζ-representative run, we

Pr
j∼Γ

1
|St|

≤

≤ 1 −

< 1 −

[(cid:96)(ρ, j) ≤ τP (cid:48) − 1]

(cid:88)

j∈St

1{(cid:96)(ρ,j)≤τP(cid:48) −1} +

(cid:115)

2d ln |Partition (St, τP (cid:48) − 1)|
|St|

+ 2

(cid:115)

2
|St|

ln

8 ((τP (cid:48) − 1) |St| t)2
ζ

(cid:115)

(cid:115)

3δ
8

3δ
8

+

+

2d ln |Partition (St, τP (cid:48) − 1)|
|St|

+ 2

(cid:115)

2
|St|

ln

8 ((τP (cid:48) − 1) |St| t)2
ζ

2d ln |Partition (St, 2t)|
|St|

+ 2

(cid:115)

2
|St|

ln

8 (2t |St| t)2
ζ

because τP (cid:48) − 1 < τP (cid:48) ≤ 2t and monotonicity. Based on Step 4 of Algorithm 1,

Pr
j∼Γ

[(cid:96)(ρ, j) ≤ τP (cid:48) − 1] < 1 −

3δ
8

+ ηδ.

Since η ≤ 1/9, we have that Prj∼Γ [(cid:96)(ρ, j) ≤ τP (cid:48) − 1] < 1 − δ/4. Therefore, Prj∼Γ [(cid:96)(ρ, j) ≥ τP (cid:48)] =
Prj∼Γ [(cid:96)(ρ, j) > τP (cid:48) − 1] > δ/4. Since

tδ/4(ρ) = argmaxτ ∈Z

(cid:26)

Pr
j∼Γ

[(cid:96)(ρ, j) ≥ τ ] ≥ δ/4

,

(cid:27)

we have that τP (cid:48) ≤ tδ/4(ρ).

The lemma statement follows from Claims B.3 and B.4.

Corollary B.5. Suppose Algorithm 1 has a ζ-representative run. For every set P (cid:48) ∈ ¯G and any
parameter vector ρ ∈ P (cid:48), τP (cid:48)δ/4 ≤ Ej∼Γ [min {(cid:96)(ρ, j), τP (cid:48)}] .

Proof. By Lemma B.2, we know that τP (cid:48) ≤ tδ/4(ρ), so Prj∼Γ [(cid:96)(ρ, j) ≥ τP (cid:48)] ≥ δ
Ej∼Γ [min {(cid:96)(ρ, j), τP (cid:48)}] ≥ τP (cid:48) Prj∼Γ [(cid:96)(ρ, j) ≥ τP (cid:48)] ≥ τP(cid:48) δ
4 .

4 . Therefore,

17

Lemma 4.5. Suppose Algorithm 1 has a ζ-representative run. For any parameter vector ρ (cid:54)∈
(cid:83)

P (cid:48)∈ ¯G P (cid:48), 2¯t ≤ 8

δ · Ej∼Γ

(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/4(ρ)(cid:9)(cid:3) .

(cid:80)

j∈ ¯S 1{(cid:96)(ρ,j)≤2¯t−1} < 1 − 3δ

Proof. The last round that Algorithm 1 adds any subset to the set G is round ¯t − 1. For ease
let ¯S = S¯t−1. Since ρ is not an element of any set in G, the cap 2¯t−1 must
of notation,
be too small compared to the average loss of the parameter ρ. Speciﬁcally, it must be that
1
8 . Otherwise, the algorithm would have added a parameter set
| ¯S|
containing ρ to the set G on round ¯t − 1 (Step 6). Since Algorithm 1 had a ζ-representative run,
we know that the probability the loss of ρ is smaller than 2¯t−1 converges to the fraction of samples
with loss smaller than 2¯t−1. Speciﬁcally,
(cid:96)(ρ, j) ≤ 2¯t−1(cid:105)
(cid:104)

Pr
j∼Γ

1{(cid:96)(ρ,j)≤2¯t−1} +

(cid:115)

2d ln (cid:12)

(cid:12)Partition (cid:0) ¯S, 2¯t−1(cid:1)(cid:12)
(cid:12)
(cid:12) ¯S(cid:12)
(cid:12)
(cid:12)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

+ 2

2
(cid:12) ¯S(cid:12)
(cid:12)
(cid:12)

ln

8 (cid:0)2¯t−1 (cid:12)

(cid:12) ¯S(cid:12)
(cid:12) (¯t − 1)(cid:1)2
ζ

1{(cid:96)(ρ,j)≤2¯t−1} + ηδ,

≤

≤

1
(cid:12) ¯S(cid:12)
(cid:12)
(cid:12)
1
(cid:12) ¯S(cid:12)
(cid:12)
(cid:12)

(cid:88)

j∈ ¯S
(cid:88)

j∈ ¯S

where the second inequality follows from Step 4 of Algorithm 1. Using our bound of 1 − 3δ/8
on the fraction of samples with loss smaller than 2¯t−1, we have that Prj∼Γ
<

(cid:96)(ρ, j) ≤ 2¯t−1(cid:105)
(cid:104)

1 − (3/8 − η)δ. Since η ≤ 1/9, it must be that Prj∼Γ

(cid:96)(ρ, j) ≤ 2¯t−1(cid:105)
(cid:104)

< 1 − δ/4, or conversely,

Prj∼Γ

(cid:104)

(cid:96)(ρ, j) ≥ 2¯t−1(cid:105)

≥ Prj∼Γ

(cid:104)
(cid:96)(ρ, j) > 2¯t−1(cid:105)

> δ/4. Since

tδ/4(ρ) = argmaxτ ∈Z

(cid:26)

Pr
j∼Γ

[(cid:96)(ρ, j) ≥ τ ] ≥ δ/4

,

(cid:27)

we have that 2¯t−1 ≤ tδ/4(ρ). Therefore,

E
j∼Γ

(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/4(ρ)(cid:9)(cid:3) ≥ tδ/4(ρ) Pr

j∼Γ

(cid:2)(cid:96)(ρ, j) ≥ tδ/4(ρ)(cid:3)

· tδ/4(ρ)

≥

δ
4
≥ 2¯t−3δ.

Lemma B.6. Suppose Algorithm 1 has a ζ-representative run. For any parameter vector ρ (cid:54)∈
(cid:83)

(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/4(ρ)(cid:9)(cid:3) ≥ ¯T .

P (cid:48)∈ ¯G P (cid:48), Ej∼Γ

Proof. From Lemma 4.5, we know that Ej∼Γ
(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/4(ρ)(cid:9)(cid:3) ≥ ¯T .
of Algorithm 1, ¯T ≤ 2¯t−3δ, so Ej∼Γ
Lemma B.7. Suppose Algorithm 1 has a ζ-representative run. There exists a set P (cid:48) ∈ ¯G and a
parameter vector ρ ∈ P (cid:48) such that ¯T ≥ 1
4√

(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/4(ρ)(cid:9)(cid:3) ≥ 2¯t−3δ. Moreover, from Step 2

· Ej∼Γ [min {(cid:96)(ρ, j), τP (cid:48)}].

1+(cid:15)

18

Proof. By deﬁnition of the upper conﬁdence bound ¯T (Steps 8 through 10 of Algorithm 1),
there is some round t, some set P (cid:48) ∈ ¯G, and some parameter vector ρ ∈ P (cid:48) such that ¯T =
1
|St|

min {(cid:96) (ρ, j) , τP (cid:48)}. Since Algorithm 1 had a ζ-representative run,

j∈St

(cid:80)

¯T =

1
|St|

(cid:88)

j∈St

min {(cid:96) (ρ, j) , τP (cid:48)}

≥ E
j∼Γ

[min {(cid:96) (ρ, j) , τP (cid:48)}] − τP (cid:48)





(cid:115)

2d ln |Partition (St, τP (cid:48))|
|St|

+ 2

(cid:115)

2
|St|

ln

8 (τP (cid:48) |St| t)2
ζ



 .

By Step 6, we know that at least a (1 − 3δ/8)-fraction of the problem instances j ∈ St have a loss
(cid:96)(ρ, j) that is at most 2t. Therefore, by deﬁnition of τP (cid:48) = τ(cid:98)|St|(1−3δ/8)(cid:99), it must be that τP (cid:48) ≤ 2t.
By monotonicity, this means that

¯T ≥ E
j∼Γ

[min {(cid:96) (ρ, j) , τP (cid:48)}] − τP (cid:48)





(cid:115)

2d ln |Partition (St, 2t)|
|St|

+ 2

(cid:115)

2
|St|

ln

8 (2t |St| t)2
ζ



 .

Based on Step 4 of Algorithm 1,

¯T ≥ E
j∼Γ

[min {(cid:96) (ρ, j) , τP (cid:48)}] − τP (cid:48)ηδ.

From Corollary B.5, τP (cid:48)δ/4 ≤ Ej∼Γ [min {(cid:96)(ρ, j), τP (cid:48)}], which means that

¯T ≥ (1 − 4η) E
j∼Γ

[min {(cid:96)(ρ, j), τP (cid:48)}] .

Finally, the lemma statement follows from the fact that

η = min

(cid:0) 4√

(cid:26) 1
8

1 + (cid:15) − 1(cid:1) ,

(cid:27)

1
9

≤

1
4

(cid:18)

1 −

(cid:19)

.

√
4

1
1 + (cid:15)

Lemma B.8. Suppose Algorithm 1 has a ζ-representative run. For every set P (cid:48) ∈ ¯G and every pair
√
of parameter vectors ρ1, ρ2 ∈ P (cid:48), Ej∼Γ [min {(cid:96) (ρ1, j) , τP (cid:48)}] ≤ 4

1 + (cid:15) · Ej∼Γ [min {(cid:96) (ρ2, j) , τP (cid:48)}].

Proof. Let t be the round that the interval P (cid:48) was added to G. Since Algorithm 1 had a ζ-
representative run,

E
j∼Γ

[min {(cid:96) (ρ1, j) , τP (cid:48)}]

≤

1
|St|

(cid:88)

j∈St

min {(cid:96) (ρ1, j) , τP (cid:48)} + τP (cid:48)





(cid:115)

2d ln |Partition (St, τP (cid:48))|
|St|

+ 2

(cid:115)

2
|St|

ln

8 (τP (cid:48) |St| t)2
ζ



 .

By deﬁnition of the set P (cid:48), for all problem instances j ∈ St, min {(cid:96) (ρ1, j) , τP (cid:48)} = min {(cid:96) (ρ2, j) , τP (cid:48)} .
Therefore,

E
j∼Γ

[min {(cid:96) (ρ1, j) , τP (cid:48)}]

19

≤

1
|St|

(cid:88)

j∈St

min {(cid:96) (ρ2, j) , τP (cid:48)} + τP (cid:48)





(cid:115)

2d ln |Partition (St, τP (cid:48))|
|St|

+ 2

(cid:115)

2
|St|

ln

8 (τP (cid:48) |St| t)2
ζ



 .

Again, since Algorithm 1 had a ζ-representative run,

E
j∼Γ

[min {(cid:96) (ρ1, j) , τP (cid:48)}]

≤ E
j∼Γ

[min {(cid:96) (ρ2, j) , τP (cid:48)}] + 2τP (cid:48)





(cid:115)

2d ln |Partition (St, τP (cid:48))|
|St|

+ 2

(cid:115)

2
|St|

ln

8 (τP (cid:48) |St| t)2
ζ



 .

Since τP (cid:48) ≤ 2t,

E
j∼Γ

[min {(cid:96) (ρ1, j) , τP (cid:48)}]

≤ E
j∼Γ

[min {(cid:96) (ρ2, j) , τP (cid:48)}] + 2τP (cid:48)





(cid:115)

2d ln |Partition (St, 2t)|
|St|

+ 2

(cid:115)

2
|St|

ln

8 (2t |St| t)2
ζ



 .

Based on Step 4 of Algorithm 1, this means that

E
j∼Γ

[min {(cid:96) (ρ1, j) , τP (cid:48)}] ≤ E
j∼Γ

[min {(cid:96) (ρ2, j) , τP (cid:48)}] + 2τP (cid:48)ηδ.

By Corollary B.5, Ej∼Γ [min {(cid:96) (ρ1, j) , τP (cid:48)}] ≤ (1+8η) Ej∼Γ [min {(cid:96) (ρ2, j) , τP (cid:48)}]. The lemma state-
√
ment follows from the fact that η ≤ (cid:0) 4

1 + (cid:15) − 1(cid:1) /8.

Lemma 4.6. If Algorithm 1 has a ζ-representative run, it returns an ((cid:15), δ)-optimal subset.

Proof. Since OP Tδ/4 := inf ρ∈P
exists a parameter vector ρ ∈ P such that Ej∼Γ
there exists a parameter ρ(cid:48) ∈ P ∗ such that

(cid:8)Ej∼Γ

(cid:2)min (cid:8)(cid:96) (ρ, j) , tδ/4(ρ)(cid:9)(cid:3)(cid:9), we know that for any γ > 0, there
(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/4(ρ)(cid:9)(cid:3) ≤ OP Tδ/4 + γ. We claim

(cid:2)min (cid:8)(cid:96) (cid:0)ρ(cid:48), j(cid:1) , tδ/2

(cid:0)ρ(cid:48)(cid:1)(cid:9)(cid:3) ≤

E
j∼Γ

√

1 + (cid:15) · E
j∼Γ

(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/4(ρ)(cid:9)(cid:3) ,

(6)

and thus the lemma statement holds (see Lemma B.9).

First, suppose ρ is contained in a set P (cid:48) ∈ ¯G. By Lemmas B.2 and B.8, there exists a parameter

ρ(cid:48) ∈ P (cid:48) ∩ P ∗ such that

(cid:2)min (cid:8)(cid:96) (cid:0)ρ(cid:48), j(cid:1) , tδ/2

E
j∼Γ

(cid:0)ρ(cid:48)(cid:1)(cid:9)(cid:3) ≤ E
j∼Γ
≤ 4√
≤ 4√

(cid:2)min (cid:8)(cid:96) (cid:0)ρ(cid:48), j(cid:1) , τP (cid:48)

(cid:9)(cid:3)

1 + (cid:15) · E
j∼Γ
1 + (cid:15) · E
j∼Γ

[min {(cid:96) (ρ, j) , τP (cid:48)}]
(cid:2)min (cid:8)(cid:96) (ρ, j) , tδ/4(ρ)(cid:9)(cid:3) .

√
Since 4

√

1 + (cid:15) ≤

1 + (cid:15), Equation (6) holds in this case.

Otherwise, suppose ρ (cid:54)∈ (cid:83)

(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/4(ρ)(cid:9)(cid:3) ≥
¯T . Moreover, by Lemma B.7, there exists a set P (cid:48) ∈ ¯G and parameter vector ρ∗ ∈ P (cid:48) such that
√
1 + (cid:15) · ¯T ≥ Ej∼Γ [min {(cid:96) (ρ∗, j) , τP (cid:48)}]. Finally, by Lemma B.8, there exists a parameter vector
4

P (cid:48)∈ ¯G P (cid:48). By Lemma B.6, we know that Ej∼Γ

ρ(cid:48) ∈ P (cid:48) ∩ P ∗ such that

E
j∼Γ

(cid:2)min (cid:8)(cid:96) (cid:0)ρ(cid:48), j(cid:1) , tδ/2

(cid:0)ρ(cid:48)(cid:1)(cid:9)(cid:3) ≤ E
j∼Γ

(cid:2)min (cid:8)(cid:96) (cid:0)ρ(cid:48), j(cid:1) , τP (cid:48)

(cid:9)(cid:3)

20

1 + (cid:15) · E
j∼Γ

[min {(cid:96) (ρ∗, j) , τP (cid:48)}]

≤ 4√
√

≤

<

√

1 + (cid:15) · ¯T
1 + (cid:15) · E
j∼Γ

(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/4(ρ)(cid:9)(cid:3) .

Therefore, Equation (6) holds in this case as well.

Lemma B.9. Let P ∗ be the set of parameters output by Algorithm 1. Suppose that for ev-
(cid:2)min (cid:8)(cid:96)(ρ(cid:48), j), tδ/2(ρ(cid:48))(cid:9)(cid:3) ≤
ery γ > 0, there exists a parameter vector ρ(cid:48) ∈ P ∗ such that Ej∼Γ
√
√
(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/2(ρ)(cid:9)(cid:3)(cid:9) ≤

1 + (cid:15) (cid:0)OP Tδ/4 + γ(cid:1) . Then minρ∈P ∗

1 + (cid:15) · OP Tδ/4.

(cid:8)Ej∼Γ

Proof. For a contradiction, suppose that minρ∈P ∗
(cid:8)Ej∼Γ
and let γ(cid:48) = minρ∈P ∗
there exists a parameter vector ρ(cid:48) ∈ P ∗ such that

(cid:8)Ej∼Γ
√
(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/2(ρ)(cid:9)(cid:3)(cid:9)−

(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/2(ρ)(cid:9)(cid:3)(cid:9) >
1 + (cid:15)·OP Tδ/4. Setting γ = γ(cid:48)

1 + (cid:15)·OP Tδ/4
, we know
√

2

1+(cid:15)

√

(cid:2)min (cid:8)(cid:96)(ρ(cid:48), j), tδ/2(ρ(cid:48))(cid:9)(cid:3) ≤

E
j∼Γ

=

√

√

(cid:18)

1 + (cid:15)

1 + (cid:15) · OP Tδ/4 +

(cid:19)

γ(cid:48)
1 + (cid:15)

OP Tδ/4 +

√
2
γ(cid:48)
2
(cid:27)
(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/2(ρ)(cid:9)(cid:3)
(cid:27)
(cid:2)min (cid:8)(cid:96)(ρ, j), tδ/2(ρ)(cid:9)(cid:3)

−

γ(cid:48)
2

,

= min
ρ∈P ∗

< min
ρ∈P ∗

(cid:26)

(cid:26)

E
j∼Γ

E
j∼Γ

which is a contradiction.

Lemma 4.4. Suppose Algorithm 1 has a ζ-representative run. Then 2¯t ≤ 16
δ

√
4

1 + (cid:15) · OP Tδ/4.

Proof. For each set P (cid:48) ∈ ¯G, let tP (cid:48) be the round where P (cid:48) is added to the set G, let SP (cid:48) = StP(cid:48) , and
let ρP (cid:48) be an arbitrary parameter vector in P (cid:48). Since no set is added to G when t = ¯t, it must be that
(cid:2)min (cid:8)(cid:96) (ρ, j) , tδ/4(ρ)(cid:9)(cid:3)(cid:9),
for all sets P (cid:48) ∈ ¯G, tP (cid:48) ≤ ¯t − 1. Moreover, since OP Tδ/4 := inf ρ∈P
we know that for every γ > 0, there exists a parameter vector ρ∗ such that

(cid:8)Ej∼Γ

(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4 (ρ∗)(cid:9)(cid:3) ≤ OP Tδ/4 + γ.

E
j∼Γ

Below, we prove that 2¯t ≤ 16 4√
holds (see Lemma B.10).

1+(cid:15)
δ

· Ej∼Γ

(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4 (ρ∗)(cid:9)(cid:3) and thus the lemma statement

Case 1: ρ∗ (cid:54)∈ (cid:83)
Therefore, 2¯t ≤ 8

P (cid:48)∈ ¯G P (cid:48). By Lemma 4.5, we know that 2¯t−3δ ≤ Ej∼Γ
δ · Ej∼Γ

(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4 (ρ∗)(cid:9)(cid:3) ≤ 16 4√

· Ej∼Γ

1+(cid:15)
δ

(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4 (ρ∗)(cid:9)(cid:3).

(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4 (ρ∗)(cid:9)(cid:3).

Case 2: ρ∗ is an element of a set P (cid:48) ∈ ¯G and tP (cid:48) ≤ ¯t − 2. Let T (cid:48) be the value of T at the
beginning of round ¯t − 1. Since the algorithm does not terminate on round ¯t − 1, it must be that
2¯t−4δ < T (cid:48). By deﬁnition of T (cid:48),

2¯t−4δ < T (cid:48) = min

¯P:t ¯P ≤¯t−2

1
|S ¯P |

(cid:88)

j∈S ¯P

min {(cid:96) (ρ ¯P , j) , τ ¯P } ≤

1
|SP (cid:48)|

(cid:88)

j∈SP(cid:48)

min {(cid:96) (ρ∗, j) , τP (cid:48)} .

21

Since Algorithm 1 had a ζ-representative run, 2¯t−4δ is upper-bounded by

[min {(cid:96) (ρ∗, j) , τP (cid:48)}] + τP (cid:48)



E
j∼Γ



(cid:115)

2d ln |Partition (SP (cid:48), τP (cid:48))|
|SP (cid:48)|

+ 2

(cid:115)

2
|SP (cid:48)|

ln

8 (τP (cid:48) |SP (cid:48)| tP (cid:48))2
ζ



 .

Since τP (cid:48) ≤ 2tP(cid:48) and f is monotone, 2¯t−4δ is at most

Ej∼Γ [min {(cid:96) (ρ∗, j) , τP (cid:48)}] + τP (cid:48)





(cid:115)

2d ln |Partition (SP (cid:48), 2tP(cid:48) )|
|SP (cid:48)|

+ 2

(cid:115)

2
|SP (cid:48)|

ln

8 (2tP(cid:48) |SP (cid:48)| tP (cid:48))2
ζ



 .

By Step 4 of Algorithm 1, 2¯t−4δ ≤ Ej∼Γ [min {(cid:96) (ρ∗, j) , τP (cid:48)}] + τP (cid:48)ηδ. Finally, by Corollary B.5,
√
2¯t−4δ ≤ (1 + 4η) Ej∼Γ [min {(cid:96) (ρ∗, j) , τP (cid:48)}] . Since η < (cid:0) 4

1 + (cid:15) − 1(cid:1) /4,

2¯t−4δ < 4√

1 + (cid:15) · E
j∼Γ

[min {(cid:96) (ρ∗, j) , τP (cid:48)}] .

Recalling that τP (cid:48) ≤ tδ/4 (ρ∗) by Lemma B.2, we conclude that

2¯t ≤

√
16 4

1 + (cid:15)
δ

· E
j∼Γ

(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4 (ρ∗)(cid:9)(cid:3) .

Case 3: ρ∗ is not an element of any set ¯P ∈ ¯G with t ¯P ≤ ¯t − 2, but ρ∗ is an element of a
set P (cid:48) ∈ ¯G with tP (cid:48) = ¯t − 1. Let S (cid:48) = S¯t−2 and let ¯P be the set containing ρ∗ in Step 5 on round
¯t−2. Since ¯P was not added to G on round ¯t−2, we know that fewer than a (cid:0)1 − 3δ
(cid:1)-fraction of the
8
instances in S (cid:48) have a loss of at most 2¯t−2 when run with any parameter vector ρ ∈ ¯P (including
ρ∗). In other words,

1
|S (cid:48)|

(cid:88)

j∈S (cid:48)

1{(cid:96)(ρ∗,j)≤2¯t−2} < 1 −

3δ
8

.

Since Algorithm 1 had a ζ-representative run,

(cid:96) (ρ∗, j) ≤ 2¯t−2(cid:105)
(cid:104)

(cid:88)

j∈S (cid:48)

1{(cid:96)(ρ∗,j)≤2¯t−2} +

Pr
j∼Γ

1
|S (cid:48)|

≤

< 1 −

(cid:115)

2d ln (cid:12)

(cid:12)Partition (cid:0)S (cid:48), 2¯t−2(cid:1)(cid:12)
(cid:12)

(cid:115)

+ 2

2
|S (cid:48)|

ln

8 (cid:0)2¯t−2 |S (cid:48)| (¯t − 2)(cid:1)2
ζ

(cid:115)

3δ
8

+

2d ln (cid:12)

(cid:12)Partition (cid:0)S (cid:48), 2¯t−2(cid:1)(cid:12)
(cid:12)

|S (cid:48)|

2
|S (cid:48)|

ln

8 (cid:0)2¯t−2 |S (cid:48)| (¯t − 2)(cid:1)2
ζ

.

|S (cid:48)|

+ 2

(cid:115)

Based on Step 4 of Algorithm 1, Prj∼Γ

(cid:104)

(cid:96) (ρ∗, j) ≤ 2¯t−2(cid:105)

Prj∼Γ
≥
δ/4. Since tδ/4(ρ∗) = argmaxτ ∈Z {Prj∼Γ[(cid:96)(ρ∗, j) ≥ τ ] ≥ δ/4}, we have that 2¯t−2 ≤ tδ/4 (ρ∗). There-
fore,

< 1 − δ/4. Therefore, Prj∼Γ

≥ Prj∼Γ

(cid:96) (ρ∗, j) ≤ 2¯t−2(cid:105)
(cid:104)
(cid:104)

< 1 − (3/8 − η)δ. Since η ≤ 1/9,
(cid:104)

(cid:96) (ρ∗, j) > 2¯t−2(cid:105)

(cid:96) (ρ∗, j) ≥ 2¯t−2(cid:105)

(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4 (ρ∗)(cid:9)(cid:3) ≥ tδ/4 (ρ∗) Pr (cid:2)(cid:96) (ρ∗, j) ≥ tδ/4 (ρ∗)(cid:3)

E
j∼Γ

≥ 2¯t−2 Pr (cid:2)(cid:96) (ρ∗, j) ≥ tδ/4 (ρ∗)(cid:3)
≥ δ2¯t−4,

which means that 2¯t ≤ 16

δ · Ej∼Γ

(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4 (ρ∗)(cid:9)(cid:3).

22

1+(cid:15)
δ

· Ej∼Γ

(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4 (ρ∗)(cid:9)(cid:3) ≤ 16 4√

Lemma B.10. Suppose that for all γ > 0, there exists a parameter vector ρ∗ ∈ P such that
2¯t ≤ 16 4√
· OP Tδ/4.
Proof. For a contradiction, suppose 2¯t > 16 4√
γ = γ(cid:48)δ
32 4√

, we know there exists a parameter vector ρ∗ ∈ P such that

(cid:0)OP Tδ/4 + γ(cid:1). Then 2¯t ≤ 16 4√

·OP Tδ/4 and let γ(cid:48) = 2¯t − 16 4√

·OP Tδ/4. Letting

1+(cid:15)
δ

1+(cid:15)
δ

1+(cid:15)
δ

1+(cid:15)
δ

1+(cid:15)

2¯t ≤

≤

=

√
16 4

1 + (cid:15)
δ
√
16 4

1 + (cid:15)
δ
√
16 4
1 + (cid:15)
δ
γ(cid:48)
2

= 2¯t −

(cid:2)min (cid:8)(cid:96) (ρ∗, j) , tδ/4 (ρ∗)(cid:9)(cid:3)

· E
j∼Γ

(cid:18)

OP Tδ/4 +

· OP Tδ/4 +

(cid:19)

γ(cid:48)δ
√
32 4
1 + (cid:15)
γ(cid:48)
2

< 2¯t,

which is a contradiction. Therefore, the lemma statement holds.

Lemma B.11. Suppose Algorithm 1 has a ζ-representative run. The size of the set P ∗ ⊂ P that
(cid:12)
(cid:12)Partition (cid:0)St, 16
Algorithm 1 returns is bounded by (cid:80)¯t
Proof. Based on Step 12 of Algorithm 1, the size of P ∗ equals the size of the set ¯G. Algorithm 1
only adds sets to G on Step 7, and on each round t, the number of sets it adds is bounded by
St, 2¯t(cid:17)(cid:12)
(cid:12). By monotonicity, we know that (cid:12)
(cid:12)Partition (cid:0)St, 2t(cid:1)(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤
√
(cid:12)
(cid:1)(cid:12)
(cid:12)Partition (cid:0)St, 16
4
(cid:12). Therefore,
1 + (cid:15) · OP Tδ/4

(cid:12)Partition (cid:0)St, 2t(cid:1)(cid:12)

1 + (cid:15) · OP Tδ/4

Partition

(cid:1)(cid:12)
(cid:12).

(cid:12) ≤

√
4

t=1

(cid:12)
(cid:12)
(cid:12)

(cid:16)

δ

δ

(cid:12) ¯G(cid:12)
(cid:12)

(cid:12) = |P ∗| ≤

¯t
(cid:88)

t=1

(cid:12)
(cid:12)
Partition
(cid:12)
(cid:12)

(cid:18)

St,

4√

16
δ

1 + (cid:15) · OP Tδ/4

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

C Additional information about related research

In this section, we begin by surveying additional related research. We then describe the bounds
on |Partition(S, τ )| that prior research has provided in the contexts of integer programming and
clustering. We also describe an integer programming algorithm conﬁguration problem where the
best-known uniform convergence bound is exponential in the number of variables [6], whereas our
algorithm only requires ˜O

(δη)−2(cid:17)

samples.

(cid:16)

C.1 Survey of additional related research

A related line of research [2, 7, 10, 18, 22] studies algorithm conﬁguration in online learning settings,
where the learner encounters a sequence—perhaps adversarially selected—of problem instances over
a series of timesteps. The learner’s goal is to select an algorithm on each timestep so that the
learner has strong cumulative performance across all timesteps (as quantiﬁed by regret, typically).
In contrast, this paper is focused on the batch learning setting, where the problem instances are
not adversarially generated but come from a ﬁxed distribution.

23

A number of papers have explored Bayesian optimization as a tool for parameter optimization.
These algorithms have strong performance in practice [12, 25, 26, 38], but often do not come with
theoretical guarantees, which is the focus of our paper. Those papers that do include provable guar-
antees typically require that the loss function is smooth, as quantiﬁed by its Lipschitz constant [16]
or its RKHS norm [13], which is not the case in our setting.

C.2 Integer programming

In this section, to match prior research, we use the notation Q to denote an integer program (rather
than the notation j, as in the main body).

Balcan et al. [6] study mixed integer linear programs (MILPs) where the goal is to maximize an
objective function c(cid:62)x subject to the constraints that Ax ≤ b and that some of the components of
x are contained in {0, 1}. Given a MILP Q, we use the notation ˘xQ = (˘xQ[1], . . . ˘xQ[n]) to denote
an optimal solution to the MILP’s LP relaxation. We denote the optimal objective value to the
MILP’s LP relaxation as ˘cQ, which means that ˘cQ = c(cid:62) ˘xQ.

(resp., Q−

The most popular algorithm for solving MILPs is called branch-and-bound (B&B), which we
now describe at a high level. Let Q(cid:48) be a MILP we want to solve. B&B builds a search tree T with
Q(cid:48) at the root. At each round, the algorithm uses a node selection policy (such as depth- or best-
ﬁrst search) to choose a leaf of T . This leaf node corresponds to a MILP we denote as Q. Using
a variable selection policy, the algorithm then chooses one of that MILP’s variables. Speciﬁcally,
let Q+
i ) equal the MILP Q after adding the constraint xi = 1 (resp., xi = 0). The
i
algorithm deﬁnes the right (resp., left) child of the leaf Q to equal Q+
i ). B&B then tries
to “fathom” these leafs. At a high level, B&B fathoms a leaf if it can guarantee that it will not
ﬁnd any better solution by branching on that leaf than the best solution found so far. See, for
example, the research by Balcan et al. [6] for the formal protocol. Once B&B has fathomed every
leaf, it terminates. It returns the best feasible solution to Q(cid:48) that it found in the search tree, which
is provably optimal.

i (resp., Q−

Balcan et al. [6] focus on variable selection policies, and in particular, score-based variable

selection policies, deﬁned below.

Deﬁnition C.1 (Score-based variable selection policy [6]). Let score be a deterministic function
that takes as input a partial search tree T , a leaf Q of that tree, and an index i, and returns a real
value score(T , Q, i) ∈ R. For a leaf Q of a tree T , let NT ,Q be the set of variables that have not
yet been branched on along the path from the root of T to Q. A score-based variable selection
policy selects the variable argmaxxi∈NT ,Q

{score(T , Q, i)} to branch on at the node Q.

Score-based variable selection policies are extremely popular in B&B implementations [1, 21, 33].
See the research by Balcan et al. [6] for examples. Given d arbitrary scoring rules score1, . . . , scored,
Balcan et al. [6] provide guidance for learning a linear combination ρ1score1 + · · · + ρdscored that
leads to small expected tree sizes. They assume that all aspects of the tree search algorithm except
the variable selection policy, such as the node selection policy, are ﬁxed. In their analysis, they
prove the following lemma.

Lemma C.2. [Balcan et al. [6]] Let score1, . . . , scored be d arbitrary scoring rules and let Q be
an arbitrary MILP over n binary variables. Suppose we limit B&B to producing search trees of size
τ . There is a set H of at most n2(τ +1) hyperplanes such that for any connected component R of
[0, 1]d \ H , the search tree B&B builds using the scoring rule ρ1score1 + · · · + ρdscored is invariant
across all (ρ1, . . . , ρd) ∈ R.

24

Balcan et al. [6] observe that in practice, the number of hyperplanes is signiﬁcantly smaller.
Given a set S of MILP instances, there are |S|n2(τ +1) relevant hyperplanes H∗. The number of
connected components of the set [0, 1]d \ H∗ is at most (cid:0)|S|n2(τ +1) + 1(cid:1)d
[17]. Therefore, in our
context, |Partition(S, τ )| ≤ (cid:0)|S|n2(τ +1) + 1(cid:1)d
. When d = 2, Balcan et al. [6] provide guidance for
ﬁnding the partition of [0, 1] into intervals I where the search tree B&B builds using the scoring
rule ρ · score1 + (1 − ρ) · score2 is invariant across all ρ ∈ I. These intervals correspond to the
output of the function Partition. An important direction for future research is extending the
implementation to multi-dimensional parameter spaces.

Uniformly sampling conﬁgurations.

Theorem 5.2. For the B&B conﬁguration problem in Theorem 5.1, with constant probability over
the draw of m = (cid:98)1/(b − a)(cid:99) parameters ρ1, . . . , ρm ∼ Uniform[0, 1], {ρ1, . . . , ρm} ∩ (a, b) = ∅.
Proof. We know that the probability {ρ1, . . . , ρm}∩(a, b) = ∅ is (1−(b−a))m ≥ (1−(b−a))1/(b−a) ≥
1
3 since b − a ≤ 1
6 .

Uniform convergence versus Algorithm 1. We now describe an integer programming algo-
rithm conﬁguration problem where the best-known uniform convergence bound is exponential in
the number of variables [6], whereas our algorithm only requires ˜O
samples. We use a
family of MILP distributions introduced by Balcan et al. [6]:

(δη)−2(cid:17)

(cid:16)

Theorem C.3 (Balcan et al. [6]). For any MILP Q, let

and

score1(T , Q, i) = min

score2(T , Q, i) = max

(cid:110)

˘cQ − ˘cQ+

i

(cid:111)

, ˘cQ − ˘cQ−

i

(cid:110)

˘cQ − ˘cQ+

i

, ˘cQ − ˘cQ−

i

(cid:111)

.

Deﬁne (cid:96)(ρ, Q) to be the size of the tree B&B produces using the scoring rule ρ·score1+(1−ρ)·score2
given Q as input. For every a, b such that 1
2 and for all even n ≥ 6, there exists
an inﬁnite family of distributions D over MILP instances with n variables such that the following
conditions hold:

3 < a < b < 1

1. If ρ ≤ a, then (cid:96)(ρ, Q) = 2(n−5)/4 with probability 1

2 and (cid:96)(ρ, Q) = 8 with probability 1
2 .

2. If ρ ∈ (a, b), then (cid:96)(ρ, Q) = 8 with probability 1.

3. If ρ ≥ b, then (cid:96)(ρ, Q) = 2(n−4)/2 with probability 1

2 and (cid:96)(ρ, Q) = 8 with probability 1
2 .

This holds no matter which node selection policy B&B uses.

(cid:1)2

As we describe in Section 5, the sample complexity bound Balcan et al. [6] provide implies that
at least (cid:0) 2n
samples are required to ensure that the average tree size branch-and-bound builds
(cid:15)(cid:48)
over the samples is within an additive (cid:15)(cid:48) factor of the expected tree size. Even for (cid:15)(cid:48) = 2n/2, this
sample complexity bound is exponential in n.

Theorem 5.3. For the B&B conﬁguration problem in Theorem 5.1, Algorithm 1 terminates after
˜O(log 1/δ) iterations, having drawn ˜O((δη)−2) sample problem instances where

η = min

√
(cid:26) 4

(cid:27)

1 + (cid:15) − 1
8

,

1
9

and returns a set containing an optimal parameter in (a, b).

25

(cid:16)

(δη)−2(cid:17)

Proof. In the proof of Theorem C.3, Balcan et al. [6] show that for any distribution D in this family
and any subset S from the support of D, |Partition(S, τ )| = 3, and the partition Partition
returns is [0, a], (a, b), and [b, 1]. Therefore, for the ﬁrst three iterations, our algorithm will use
˜O
samples. At that point, t = 3, and the tree-size cap is 8. Algorithm 1 will discover that
for all of the samples Q ∈ S3, when ρ ∈ (a, b), (cid:96)(ρ, Q) = 8. Therefore, it add (a, b) to G and it will
set T = 8. It will continue drawing ˜O
samples at each round until 2t−3δ ≥ T = 8, or in
(δη)−2(cid:17)

(δη)−2(cid:17)
other words, until t = O(log(1/δ)). Therefore, the total number of samples it draws is ˜O
The set it returns will contain a point ρ ∈ (a, b), which is optimal.

(cid:16)

(cid:16)

.

C.3 Clustering

We begin with an overview of agglomerative clustering algorithms. A clustering instance (V, d)
consists of a set V of n points and a distance metric d : V × V → R≥0 specifying all pairwise
distances between these points. The goal is to partition the points into groups such that distances
within each group are minimized and distances between each group are maximized. Typically, the
quality of a clustering is measured by an objective function, such as the classic k-means, k-median,
or k-center objectives. Unfortunately, it is NP-hard to determine the clustering that minimizes any
of these objectives.

An agglomerative clustering algorithm is characterized by a merge function ξ(A, B) → R≥0,
which deﬁnes the distance between any two sets of points A, B ⊆ V . The algorithm builds a
cluster tree T , starting with n singleton leaf nodes, each of which contains one point from V .
The algorithm iteratively merges the two sets with minimum distance until there is a single node
remaining, consisting of the set V . The children of any node N in this tree correspond to the
two sets of points that were merged to form N . Common choices for the merge function ξ include
mina∈A,b∈B d(a, b) (single-linkage),
a∈A,b∈B d(a, b) (average-linkage) and maxa∈A,b∈B d(a, b)
(complete-linkage). The linkage procedure is followed by a dynamic programming step, which
returns the pruning of the tree that minimizes a ﬁxed objective function, such as the k-means,
k-median, or k-center objectives. If the linkage procedure is terminated early, we will be left with
a forest, rather than a single tree. The dynamic programming procedure can easily be adjusted
to ﬁnd the best pruning of the forest (for example, by completing the hierarchy arbitrarily and
enforcing that the dynamic programming algorithm only return clusters contained in the original
forest).

1
|A|·|B|

(cid:80)

Balcan et al. [5] deﬁne three inﬁnite families of merge functions. The families A1 and A2 consist
of merge functions ξ(A, B) that depend on the minimum and maximum of all pairwise distances
between A and B. The second family, denoted by A3, depends on all pairwise distances between
A and B. All classes are parameterized by a single value ρ.

(cid:40)

A1 =

ξ1,ρ : (A, B) (cid:55)→

(cid:18)

min
u∈A,v∈B

(d(u, v))ρ + max

u∈A,v∈B

(d(u, v))ρ

ρ ∈ R ∪ {∞, −∞}

,

(cid:41)

(cid:19)1/ρ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:26)

A2 =

ξ2,ρ : (A, B) (cid:55)→ ρ min

u∈A,v∈B

A3 =






ξ3,ρ : (A, B) (cid:55)→






1
|A||B|

(cid:88)

u∈A,v∈B

(cid:27)

ρ ∈ [0, 1]

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)






.

u∈A,v∈B
1/ρ (cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)



(d(u, v))ρ

ρ ∈ R ∪ {∞, −∞}

d(u, v) + (1 − ρ) max

d(u, v)

The classes A1 and A2 deﬁne spectra of merge functions ranging from single-linkage (ξ1,−∞

26

and ξ2,1) to complete-linkage (ξ1,∞ and ξ2,0). The class A3 includes average-, complete-, and
single-linkage.

Lemma C.4. Let (V, d) be an arbitrary clustering instance over n points. There is a partition of R
into k = O(n8) intervals I1, . . . , Ik such that for any interval Ii and any two parameters ρ, ρ(cid:48) ∈ Ii,
the sequences of merges the agglomerative clustering algorithm makes using the merge functions
ξ1,ρ and ξ1,ρ(cid:48) are identical. The same holds for the set of merge functions A2.

Given a set S of clustering instances, there are O (cid:0)|S|n8(cid:1) relevant intervals. Therefore, in
our context, |Partition(S, τ )| = O(|S|n8). Balcan et al. [5] provide guidance for ﬁnding these
intervals, which correspond to the output of the function Partition. They also prove the following
guarantee for the class A3.

Lemma C.5. Let (V, d) be an arbitrary clustering instance over n points. There is a partition
of R into k = O (cid:0)n232n(cid:1) intervals I1, . . . , Ik such that for any interval Ii and any two parameters
ρ, ρ(cid:48) ∈ Ii, the sequences of merges the agglomerative clustering algorithm makes using the merge
functions ξ3,ρ and ξ3,ρ(cid:48) are identical.

Similarly, given a set S of clustering instances, there are O (cid:0)|S|n232n(cid:1) relevant intervals. There-
fore, in our context, |Partition(S, τ )| = O (cid:0)|S|n232n(cid:1). Again, Balcan et al. [5] provide guidance
for ﬁnding these intervals.

27

