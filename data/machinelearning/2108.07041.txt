Implicitly Regularized RL with Implicit Q-values

2
2
0
2

y
a
M
1
3

]

G
L
.
s
c
[

2
v
1
4
0
7
0
.
8
0
1
2
:
v
i
X
r
a

Nino Vieillard1,2

Marcin Andrychowicz1

Anton Raichuk1

Olivier Pietquin1

Matthieu Geist1

1Google Research, Brain Team 2Université de Lorraine, CNRS, Inria, IECL, F-54000 Nancy, France

Abstract

The Q-function is a central quantity in many
Reinforcement Learning (RL) algorithms for
which RL agents behave following a (soft)-
greedy policy w.r.t. to Q. It is a powerful tool
that allows action selection without a model
of the environment and even without explic-
itly modeling the policy. Yet, this scheme can
only be used in discrete action tasks, with
small numbers of actions, as the softmax over
actions cannot be computed exactly other-
wise. More speciﬁcally, the usage of function
approximation to deal with continuous action
spaces in modern actor-critic architectures in-
trinsically prevents the exact computation of
a softmax. We propose to alleviate this issue
by parametrizing the Q-function implicitly,
as the sum of a log-policy and a value func-
tion. We use the resulting parametrization
to derive a practical oﬀ-policy deep RL algo-
rithm, suitable for large action spaces, and
that enforces the softmax relation between
the policy and the Q-value. We provide a
theoretical analysis of our algorithm: from an
Approximate Dynamic Programming perspec-
tive, we show its equivalence to a regularized
version of value iteration, accounting for both
entropy and Kullback-Leibler regularization,
and that enjoys beneﬁcial error propagation
results. We then evaluate our algorithm on
classic control tasks, where its results compete
with state-of-the-art methods.

1

INTRODUCTION

A large body of reinforcement learning (RL) algo-
rithms, based on approximate dynamic programming

(ADP) (Bertsekas and Tsitsiklis, 1996; Scherrer et al.,
2015), operate in two steps: a greedy step, where the
algorithm learns a policy that maximizes a Q-value,
and an evaluation step, that (partially) updates the
Q-values towards the Q-values of the policy. A com-
mon improvement to these techniques is to use regu-
larization, that prevents the new updated policy from
being too diﬀerent from the previous one, or from a
ﬁxed “prior” policy. For example, Kullback-Leibler
(KL) regularization keeps the policy close to the pre-
vious iterate (Vieillard et al., 2020a), while entropy
regularization keeps the policy close to the uniform
one (Haarnoja et al., 2018a). Entropy regularization,
often used in this context (Ziebart, 2010), modiﬁes
both the greedy step and the evaluation step so that
the policy jointly maximizes its expected return and its
entropy. In this framework, the solution to the policy
optimization step is simply a softmax of the Q-values
over the actions. In small discrete action spaces, the
softmax can be computed exactly: one only needs to de-
ﬁne a critic algorithm, with a single loss that optimizes
a Q-value. However, in large multi-dimensional – or
even continuous – action spaces, one needs to estimate
it. This estimation is usually done by adding an actor
loss, that optimizes a policy to ﬁt this softmax. It re-
sults in an (oﬀ-policy) actor-critic algorithm, with two
losses that are optimized simultaneously1 (Degris et al.,
2012). This additional optimization step introduces
supplementary errors to the ones already created by
the approximation in the evaluation step.

To remove these extraneous approximations, we
introduce the Implicit Q-values (IQ) algorithm, that
deviates from classic actor-critics, as it optimizes a
policy and a value in a single loss. The core idea is to
implicitly represent the Q-value as the sum of a value
function and a log-policy. This representation ensures
that the policy is the exact softmax of the Q-value,
despite the use of any approximation scheme. We use
this to design a practical model-free deep RL algorithm

Proceedings of the 25th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS) 2022, Valencia,
Spain. PMLR: Volume 151. Copyright 2022 by the au-
thor(s).

1We refer here speciﬁcally to oﬀ-policy actor-critics, built
on a value-iteration-like scheme. For on-policy actor-critics,
losses are optimized sequentially, and the policy is usually
fully evaluated.

 
 
 
 
 
 
Implicitly Regularized RL with Implicit Q-values

, a map-
through a stationary stochastic policy π ∈ ∆S
A
ping from states to distribution over actions. The
quality of a policy is quantiﬁed by the value func-
tion, Vπ(s) = Eπ[(cid:80)∞
t=0 γtr(st, at)|s0 = s]. The Q-
function is a useful extension, which notably allows
choosing a (soft)-greedy action in a model-free setting,
Qπ(s, a) = r(s, a) + Es(cid:48)|s,a[Vπ(s(cid:48))]. An optimal pol-
icy is one that achieve the highest expected return,
π∗ = argmaxπ Vπ.
A classic way to design practical algorithms beyond the
tabular setting is to adopt the Actor-Critic perspective.
In this framework, an RL agent parametrizes a policy
πθ and a Q-value Qψ with function approximation,
usually through the use of neural networks, and aims
at estimating an optimal policy. The policy and the
Q-function are then updated by minimizing two losses:
the actor loss corresponds to the greedy step, and the
critic loss to the evaluation step. The weights of the
policy and Q-value networks are regularly frozen into
target weights ¯ψ and ¯θ. With entropy regularization,
the greedy step amounts to ﬁnding the policy that max-
imizes Es∼S,a∼πθ [Q ¯ψ(s, a)−τ ln πθ(a|s)] (maximize the
Q-value with stochastic enough policy). The solution to
this problem is simply πθ(·|s) = softmax(Q ¯ψ(s, ·)/τ ),
which is the result of the greedy step of regularized
for
Value Iteration (VI) (Geist et al., 2019) and,
example, how the optimization step of Soft Actor-
Critic (Haarnoja et al., 2018a, SAC) is built.
In
a setting where the action space is discrete and
small, it amounts to a simple softmax computation.
However, on more complex action spaces (continuous,
and/or with a higher number of dimensions: as a
reference, the Humanoid-v2 environment from Openai
Gym (Brockman et al., 2016) has an action space
of dimension 17), it becomes prohibitive to use the
exact solution.
In this case, the common practice
is to resort to an approximation with a parametric
distribution model. In many actor critic algorithms
(SAC, MPO (Abdolmaleki et al., 2018), ...), the policy
is modelled as a Gaussian distribution over actions. It
introduces approximation errors, resulting from the
partial optimization process of the critic, and inductive
bias, as a Gaussian policy cannot represent an arbitrary
softmax distribution. We now turn to the description
of our core contribution: the Implicit Q-value (IQ)
algorithm, introduced to mitigate this discrepancy.

IQ implicitly parametrizes a Q-value via an explicit
parametrization of a policy and a value, as visualized
in Fig. 1. Precisely, from a policy network πθ and a
value network Vφ, we deﬁne our implicit Q-value as

Qθ,φ(s, a) = τ ln πθ(a|s) + Vφ(s).

(1)

Figure 1: view of the IQ parametrization.

that optimizes with a single loss a policy network and
a value network, built on this implicit representation
of a Q-value. To better understand it, we abstract
this algorithm to an ADP scheme, IQ-DP, and use
this point of view to provide a detailed theoretical
analysis. It relies on a key observation, that shows
an equivalence between IQ-DP and a speciﬁc form
of regularized Value Iteration (VI). This equivalence
explains the role of the components of IQ: namely, IQ
performs entropy and KL regularization. It also allows
us to derive strong performance bounds for IQ-DP.
In particular, we show that the errors made when
following IQ-DP are compensated along iterations.

Parametrizing the Q-value as a sum of a log-policy and
a value is reminiscent of the dueling architecture (Wang
et al., 2016), that factorizes the Q-value as the sum of
an advantage and a value. In fact, we show that it is a
limiting case of IQ in a discrete actions setting. This
link highlights the role of our policy, which calls for a
discussion on its necessary parametrization.

Finally, we empirically validate IQ. We evaluate our
method on several classic continuous control bench-
marks: locomotion tasks from Openai Gym (Brockman
et al., 2016), and hand manipulation tasks from the
Adroit environment (Rajeswaran et al., 2017). On
these environments, IQ reaches performances competi-
tive with state-of-the-art actor-critic methods.

2

IMPLICIT Q-VALUE
PARAMETRIZATION

We consider the standard Reinforcement Learning
(RL) setting, formalized as a Markov Decision Pro-
cess (MDP). An MDP is a tuple {S, A, P, r, γ}. S and
A are the ﬁnite state and action spaces2, γ ∈ [0, 1) is
the discount factor and r : S × A → [−Rmax, Rmax] is
the bounded reward function. Write ∆X the simplex
over the ﬁnite set X. The dynamics of an MDP are
,
deﬁned by a Markovian transition kernel P ∈ ∆S×A
where P (s(cid:48)|s, a) is the probability of transitioning to
state s(cid:48) after taking action a in s. An RL agent acts

S

2We restrict to ﬁnite spaces for the sake of analysis, but

our approach applies to continuous spaces.

Since πθ is constrained to be a distribution over
the actions, we have by construction that πθ(a|s) =

+...Nino Vieillard, Marcin Andrychowicz, Anton Raichuk, Olivier Pietquin, Matthieu Geist

softmax(Qθ,φ/τ ), the solution of the regularized greedy
step (see Appx. A.1 for a detailed proof). Hence, the
consequence of using such a parametrization is that
the greedy step is performed exactly, even in the func-
tion approximation regime. Compared to the classic
actor-critic setting, it thus gets rid of the errors cre-
ated by the actor. Note that calling Vφ a value makes
sense, since following the same reasoning we have that
Vφ(s) = τ ln (cid:80)
a(cid:48) exp(Qθ,φ(s, a(cid:48))/τ ), a soft version of
the value. With this parametrization in mind, one
could derive a deep RL algorithm from any value-based
loss using entropy regularization. We conserve the
ﬁxed-point approach of the standard actor-critic frame-
work, θ and φ are regularly copied to ¯θ and ¯φ, and
we design an oﬀ-policy algorithm, working on a replay
buﬀer of transitions (st, at, rt, st+1) collected during
training. Consider two hyperparameters, τ ∈ (0, ∞)
and α ∈ (0, 1) that we will show in Sec. 3 control
two forms of regularization. The policy and value are
optimized jointly by minimizing the loss

LIQ(θ, φ) = ˆE

(cid:104)(cid:0)rt + ατ ln π¯θ(at|st) + γV ¯φ(st+1)
− τ ln πθ(at|st) − Vφ(st)(cid:1)2(cid:105)
,

(2)

where ˆE denote the empirical expected value over a
dataset of transitions.
IQ consists then in a single
loss that optimizes jointly a policy and a value. This
brings a notable remark on the role of Q-functions
in RL. Indeed, Q-learning was introduced by Watkins
and Dayan (1992) – among other reasons – to make
greediness possible without a model (using a value only,
one needs to maximize it over all possible successive
states, which requires knowing the transition model),
and consequently derive practical, model-free RL
algorithms. Here however, IQ illustrates how, with
the help of regularization, one can derive a model-free
algorithm that does not rely on an explicit Q-value.

3 ANALYSIS

The Q-value of a policy is the unique ﬁxed point of
its Bellman operator Tπ deﬁned for any Q ∈ RS×A
as TπQ = r + γP (cid:104)π, Q(cid:105). We denote Q∗ = Qπ∗
the
optimal Q-value (the Q-value of the optimal policy).
When the MDP is entropy-regularized with a temper-
ature τ , a policy π admits a regularized Q-value Qτ
,
π
the ﬁxed point of the regularized Bellman operator
π Q = r + γP (cid:104)π, Q − τ ln π(cid:105). A regularized MDP ad-
T τ
mits an optimal regularized policy πτ
and a unique
∗
optimal regularized Q-value Qτ
∗

(Geist et al., 2019).

3.1

Ideal case

First, let us look at the ideal case, i.e. when LIQ is
exactly minimized at each iteration (tabular represen-
tation, dataset covering the whole state-action space,
expectation rather than sampling for transitions). In
this context, IQ can be understood as a Dynamic Pro-
gramming (DP) scheme that iterates on a policy πk+1
and a value Vk. They are respectively equivalent to
the target networks π¯θ
, while the next iter-
and V ¯φ
ate (πk+2, Vk+1) matches the solution (πθ, Vφ) of the
optimization problem in Eq. (2). We call the scheme
IQ-DP(α, τ ) and one iteration is deﬁned by choosing
(πk+2, Vk+1) such that the squared term in Eq. (2) is 0,

τ ln πk+2 + Vk+1 = r + ατ ln πk+1 + γP Vk.

(3)

This equation is well-deﬁned, due to the underlying con-
straint that πk+2 ∈ ∆S
(the policy must be a distribu-
A
tion over actions), that is (cid:80)
a∈A π(a|s) = 1 for all s ∈ S.
The basis for our discussion will be the equivalence of
this scheme to a version of regularized VI. Indeed, we
have the following result, proved in Appendix A.3.

Theorem 1. For any k ≥ 1, let (πk+2, Vk+1) be the
solution of IQ-DP(α, τ ) at step k. We have that


πk+2 = argmax(cid:104)π, r + γP Vk(cid:105) + (1 − α)τ H(π)

−ατ KL(π||πk+1)
Vk+1 = (cid:104)πk+2, r + γP Vk(cid:105) + (1 − α)τ H(πk+2)

−ατ KL(πk+2||πk+1)

In this section, we explain the workings of the IQ
algorithm deﬁned by Eq. (2) and detail the inﬂu-
ence of its hyperparameters. We abstract IQ into an
ADP framework, and show that, from that perspec-
tive, it is equivalent to a Mirror Descent VI (MD-VI)
scheme (Geist et al., 2019), with both entropy and
KL regularization. Let us ﬁrst introduce some useful
notations. We make use of the actions partial dot-
product notation: for u, v ∈ RS×A, we deﬁne (cid:104)u, v(cid:105) =
(cid:0) (cid:80)
s ∈ RS . For any V ∈ RS , we have
for any (s, a) ∈ S × A P V (s, a) = (cid:80)
s(cid:48) P (s(cid:48)|s, a)V (s(cid:48)).
We will deﬁne regularized algorithms, using the en-
tropy of a policy, H(π) = −(cid:104)π, ln π(cid:105), and the KL diver-
gence between two policies, KL(π||µ) = (cid:104)π, ln π − ln µ(cid:105).

a∈A u(s, a)v(s, a)(cid:1)

so IQ-DP(α, τ ) produces the same sequence of policies
as a value-based version of Mirror Descent VI, MD-
VI(ατ, (1 − α)τ ) (Vieillard et al., 2020a).

Discussion. The previous results shed a ﬁrst light
on the nature of the IQ method. Essentially, IQ-DP
is a parametrization of a VI scheme regularized with
both entropy and KL divergence, MD-VI(ατ, (1 − α)τ ).
This ﬁrst highlights the role of the hyperparameters, as
its shows the interaction between the two forms of reg-
ularization. The value of α balances between those two:
with α = 0, IQ-DP reduces to a classic VI regularized
with entropy; with α = 1 only the KL regularization
will be taken into account. The value of τ then controls

Implicitly Regularized RL with Implicit Q-values

∗

the amplitude of this regularization. In particular, in
the limit α = 0, τ → 0, we recover the standard VI
algorithm. This results also justiﬁes the soundness of
IQ-DP. Indeed, this MD-VI scheme is known to con-
verge to π(1−α)τ
the optimal policy of the regularized
MDP (Vieillard et al., 2020a, Thm. 2) and this results
readily applies to IQ3. Another consequence is that
it links IQ to Advantage Learning (AL) (Bellemare
et al., 2016). Indeed, AL is a limiting case of MD-VI
when α > 0 and τ → 0 (Vieillard et al., 2020b). There-
fore, IQ also generalizes AL, and the α parameter can
be interpreted as the advantage coeﬃcient. Finally, a
key observation is that IQ performs KL regularization
implicitly, the way it was introduced by Munchausen
RL (Vieillard et al., 2020b), by augmenting the reward
with the ατ ln πk+1 term (Eq. (3)). This observation
will have implications discussed next.

3.2 Error propagation result

Now, we are interested in understanding how errors
introduced by the function approximation used prop-
agate along iterations. At iteration k of IQ, denote
πk+1 and Vk the target networks. In the approximate
setting, we do not solve Eq. (3), but instead, we min-
imize L(θ, φ) with stochastic gradient descent. This
means that πk+2 and Vk+1 are the result of this op-
timization, and thus the next target networks. The
optimization process introduces errors, that come from
many sources: partial optimization, function approxi-
mation (policy and value are approximated with neural
networks), ﬁnite data, etc. We study the impact of
these errors on the distance between the optimal Q-
value of the MDP and the regularized Q-value of the
current policy used by IQ, Q(1−α)τ
. We insist right
away that Q(1−α)τ
is not the learned, implicit Q-value,
but the actual Q-value of the policy computed by IQ
in the regularized MDP. We have the following result
concerning the error propagation.

πk+1

πk+1

Theorem 2. Write πk+1 and Vk the kth update of
respectively the target policy and value networks. Con-
sider the error at step k, (cid:15)k ∈ RS×A, as the diﬀerence
between the ideal and the actual updates of IQ. For-
mally, we deﬁne the error as, for all k ≥ 1,

(cid:15)k = τ ln πk+2 + Vk+1 − (r + ατ ln πk+1 + γP Vk),

and the moving average of the errors as Ek = (1 −
α) (cid:80)k
j=1 αk−j(cid:15)j. We have the following results for two
diﬀerent cases depending on the value of α. Note that
when α < 1, we bound the distance to regularized opti-
mal Q-value.

3Vieillard et al. (2020a) show this for Q-functions, but

it can straightforwardly be extended to value functions.

1. General case: 0 < α < 1 and τ > 0, entropy and

KL regularization together:

(cid:107)Q(1−α)τ
∗


2
(1 − γ)2

− Q(1−α)τ
πk

(cid:107)∞ ≤

(1 − γ)

γk−j(cid:107)Ej(cid:107)∞

 + o



(cid:19)

.

(cid:18) 1
k

k
(cid:88)

j=1

2. Speciﬁc case α = 1, τ > 0, use of KL regulariza-

tion alone:

(cid:107)Q∗ − Qπk (cid:107)∞ ≤

2
1 − γ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
k

k
(cid:88)

j=1

(cid:15)j

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

+ O

(cid:19)

.

(cid:18) 1
k

Sketch of proof. The full proof
is provided in Ap-
pendix A.4. We build upon the connection we es-
tablished between IQ-DP and a VI scheme regularized
by both KL and entropy in Thm. 1. By injecting the
proposed representation into the classic MD-VI scheme,
we can build upon the analysis of Vieillard et al. (2020a,
Thm. 1 and 2) to provide these results.

Impact of KL regularization. The KL regulariza-
tion term, and speciﬁcally in the MD-VI framework, is
discussed extensively by Vieillard et al. (2020a), and we
refer to them for in-depth analysis of the subject. We
recall here the main interests of KL regularization, as
illustrated by the bounds of Thm 2. In the second case,
where it is the clearest (only KL is used), we observe
a beneﬁcial property of KL regularization: Averag-
ing of errors. Indeed, in a classic non-regularized VI
scheme (Scherrer et al., 2015), the error (cid:107)Q∗ − Qπθ (cid:107)
would depend on a moving average of the norms of
the errors (1 − γ) (cid:80)k
j=1 γk−j(cid:107)(cid:15)k(cid:107)∞, while with the KL
it depends on the norm of the average of the errors
(1/k)(cid:107) (cid:80)k
j=1 (cid:15)k(cid:107). In a simpliﬁed case where the errors
would be i.i.d. and zero mean, this would allow conver-
gence of approximate MD-VI, but not of approximate
VI. In the case α < 1, where we introduce entropy
regularization, the impact is less obvious, but we still
transform a sum of norm of errors into a sum of mov-
ing average of errors, which can help by reducing the
underlying variance.

Link to Munchausen RL. As stated in the
sketched proof, Thm. 2 is a consequence of (Vieillard
et al., 2020a, Thm. 1 and 2). A crucial limitation of this
work is that the analysis only applies when no errors are
made in the greedy step. This is possible in a relatively
simple setting, with tabular representation, or with a
linear parametrization of the Q-function. However, in
the general case with function approximation, exactly
solving the optimization problem regularized by KL is
not immediately possible: the solution of the greedy

Nino Vieillard, Marcin Andrychowicz, Anton Raichuk, Olivier Pietquin, Matthieu Geist

step of MD-VI(ατ, (1 − α)τ ) is πk+2 ∝ exp(Qk+1/τ )πα
k
(where Qk+1 = r + γP Vk), so computing it exactly
would require remembering every πj during the proce-
dure, which is not feasible in practice. A workaround to
this issue was introduced by Vieillard et al. (2020b) as
Munchausen RL: the idea is to augment the reward by
the log-policy, to implicitly deﬁne a KL regularization
term, while reducing the greedy step to a softmax. As
mentioned before, in small discrete action spaces, this
allows to compute the greedy step exactly, but it is
not the case in multidimensional or continuous action
spaces, and thus Munchausen RL loses its interest in
such domains. With IQ, we utilize the Munchausen
idea to implicitly deﬁne the KL regularization; but
with our parametrization, the exactness of the greedy
step holds even for complex action spaces: recall that
the parametrization deﬁned in Eq. (1) enforces that
the policy is a softmax of the (implicit) Q-value. Thus,
IQ can be seen as an extension of Munchausen RL to
multidimensional and continuous action spaces.

3.3 Link to the dueling architecture

Dueling Networks (DN) were introduced as a varia-
tion of the seminal Deep Q-Networks (DQN, Mnih
et al. (2015)), and have been empirically proven to be
eﬃcient (for example by Hessel et al. (2018)). The
idea is to represent the Q-value as the sum of a value
and an advantage. In this setting, we work with a no-
tion of advantage deﬁned over Q-functions (as opposed
to deﬁning the advantage as a function of a policy).
For any Q ∈ RS×A, its advantage AQ is deﬁned as
AQ(s, a) = Q(s, a) − maxa(cid:48)∈A Q(s, a(cid:48)). The advantage
encodes a sub-optimality constraint: it has negative
values and its maximum over actions is 0. Wang et al.
(2016) propose to learn a Q-value by deﬁning an ad-
vantage network FΘ and a value network VΦ, which in
turn deﬁne a Q-value QΘ,Φ as

QΘ,Φ(s, a) = FΘ(s, a) − max
a(cid:48)∈A

FΘ(s, a(cid:48)) + VΦ(s).

(4)

Subtracting the maximum over the actions ensures that
FΘ indeed represents an advantage (in red). Note that
dueling DQN was designed for discrete settings, where
computing the maximum over actions is not an issue.

In IQ, we need a policy network that represents a
distribution over the actions. There are several prac-
tical ways to represent the policy, discussed in Sec 4.
For the sake of simplicity, let us for now assume that
we are in a mono-dimensional discrete action space,
and that we use a common scaled softmax represen-
tation. Speciﬁcally, our policy is represented by a
neural network (eg.
fully connected) Fθ, that maps
state-action pairs to logits Fθ(s, a). The policy is
then deﬁned as πθ(·|s) = softmax(Fθ(s, ·)/τ ). Directly
from the deﬁnition of the softmax, we observe that

τ ln πθ(a|s) = Fθ(s, a) − τ ln (cid:80)
a(cid:48)∈A exp(Fθ(s, a(cid:48))/τ ).
The second term is a classic scaled logsumexp over the
actions, a soft version of the maximum: when τ → 0,
we have that τ ln (cid:80)(cid:48)
a exp(F (s, a(cid:48))/τ ) → maxa(cid:48) F (s, a(cid:48)).
Within the IQ parametrization, we have

Qθ,φ(s, a) = Fθ(s, a) − τ ln

(cid:88)

exp

a(cid:48)∈A

Fθ(s, a(cid:48))
τ

+ Vφ(s),

where we highlighted in red the soft advantage
component, analog to the advantage in Eq. (4),
which makes a clear link between IQ and DN. In
this case (scaled softmax representation), the IQ
parametrization generalizes the dueling architecture,
retrieved when τ → 0 (and with an additional AL
term whenever α > 0, see Sec. 3). In practice, Wang
et al. (2016) use a diﬀerent parametrization of the
advantage, replacing the maximum by a mean, deﬁning
QΘ,Φ(s, a) = AΘ(s, a)−|A|−1 (cid:80)
a(cid:48)∈A AΘ(s, a(cid:48))+VΦ(s).
We could use a similar trick and replace the logsumexp
by a mean in our policy parametrization, but in our
case this did not prove to be eﬃcient in practice.

We showed how the log-policy represents a soft ad-
vantage. While this explicits its role in the learning
procedure, it also raises questions about which repre-
sentation would be the most suited for optimization.

4 PRACTICAL CONSIDERATIONS

We now describe key practical issues encountered when
choosing a policy representation. The main one comes
from the delegation of the representation power of the
algorithm to the policy network. In a standard actor-
critic algorithm – take SAC for example, where the
policy is parametrized as a Gaussian distribution – the
goal of the policy is mainly to track the maximizing
action of the Q-value. Thus, estimation errors can
cause the policy to choose sub-optimal actions, but the
inductive bias caused by the Gaussian representation
may not be a huge issue in practice, as long as the
mean of the Gaussian policy is not too far from the
maximizing action. In other words, the representation
capacity of an algorithm such as SAC lies mainly in
the representation capacity of its Q-network. In IQ, we
have a parametrization of the policy that enforces it to
be a softmax of an implicit Q-value. By doing this, we
trade in estimation error – our greedy step is exact by
construction – for representation power. Indeed, as our
Q-value is not parametrized explicitly, but through the
policy, the representation power of IQ is in its policy
network, and a “simple” representation might not be
enough anymore. For example, if we parameterized the
policy as a Gaussian, this would amount to parametrize
an advantage as a quadratic function of the action:
this would drastically limit what the IQ policy could
represent (in terms of soft-advantage).

Implicitly Regularized RL with Implicit Q-values

j=1 πj

Multicategorical policies. To address this issue,
we turn to other, richer, distribution representations.
In practice, we consider a multi-categorical discrete
softmax distribution. Precisely, we are in the context
of a multi-dimensional action space A of dimension d,
each dimension being a bounded interval. We discretize
each dimension of the space uniformly in n values δj,
It eﬀectively deﬁnes a discrete
for 0 ≤ j ≤ n − 1.
action space A(cid:48) =×d
j=1 Aj, with Aj = {δ0, . . . δn−1}.
A multidimensional action is a vector a ∈ A(cid:48), and we
denote aj the jth component of the action a. Assuming
independence between actions conditioned on states, a
policy πθ can be factorized as the product of d marginal
mono-dimensional policies πθ(a|s) = (cid:81)d
θ(aj|s).
We represent each policy as the softmax of the output
of a neural network F j
, an thus we get the full rep-
θ
resentation πθ(a|s) = (cid:81)d
j=1 softmax(F j
θ (·|s))(aj). The
F j
functions can be represented as neural networks
θ
with a shared core, which only diﬀer in the last layer.
This type of multicategorical policy can represent any
distribution (with n high enough) that does not en-
compass a dependency between the dimensions. The
independence assumption is quite strong, and does not
hold in general. From an advantage point of view, it
assumes that the soft-advantage (i.e. the log-policy)
can be linearly decomposed along the actions. While
this limits the advantage representation, it is a much
weaker constraint than paramterizing the advantage
as a quadratic function of the action (which would be
the case with a Gaussian policy). In practice, these
types of policies have been experimented (Akkaya et al.,
2019; Tang and Agrawal, 2020), and have proven to be
eﬃcient on continuous control tasks.

Even richer policy classes can be explored. To account
for dependency between dimensions, one could envision
auto-regressive multicategorical representations, used
for example to parametrize a Q-value by Metz et al.
(2017). Another approach is to use richer continuous
distributions, such as normalizing ﬂows (Rezende and
Mohamed, 2015; Ward et al., 2019). In this work, we
restrict ourselves to the multicategorical setting, which
is suﬃcient to get satisfying results (Sec. 6), and we
leave the other options for future work.

IQ works around a ﬁxed-point scheme. Shortly, Trust-
PCL can be seen as a version of IQ without the target
value network V ¯φ
. These entropy-regularized residual
approaches are derived from the softmax temporal con-
sistency principle, which allows to consider extensions
to a speciﬁc form of multi-step learning (strongly rely-
ing on the residual aspect), but they also come with
drawbacks, such as introducing a bias in the optimiza-
tion when the environment is stochastic (Geist et al.,
2017). Dai et al. (2018) proposed SBEED (Smooth
Bellmann Error Embedding) to address this bias by
replacing the residual minimization with a saddle-point
problem. They provide an unbiased algorithm, but
that ultimately requires to solve a min-max optimiza-
tion, more complex than what we propose. Second,
Quinoa (Degrave et al., 2018) uses a similar loss to
Trust-PCL and IQ (without reference to the former
Trust-PCL), but does not propose any analysis, and
is evaluated only on a few tasks. Third, Normalized
Advantage Function (NAF, Gu et al. (2016)) is de-
signed with similar principles. In NAF, a Q-value is
parametrized as a value and and an advantage, the
latter being quadratic on the action. It matches the
special case of IQ with a Gaussian policy, where we
recover this quadratic parametrization. Finally, the ac-
tion branching architecture (Tavakoli et al., 2018) was
proposed to emulate Dueling Networks in the continu-
ous case, by using a similar Q-value parametrization
with a multicategorical policy representation. This
architecture is recovered by IQ in the case α = 0 and
τ → 0.

Regularization. Entropy and KL regularization are
used by many other RL algorithms. Notably, from a
dynamic programming perspective, IQ-DP(0, τ ) per-
forms the same update as SAC – an entropy regularized
VI. This equivalence is however not true in the function
approximation regime. Due to the empirical success of
SAC and its link to IQ, it will be used as a baseline on
continuous control tasks. Other algorithms also use KL
regularization, e.g. Maximum a posteriori Policy Opti-
mization (MPO, Abdolmaleki et al. (2018)). We refer
to Vieillard et al. (2020a) for an exhaustive review of
algorithms encompassed within the MD-VI framework.

5 RELATED WORK

6 EXPERIMENTS

Similar parametrizations. Other algorithms use
similar parametrization. First, Path Consistency Learn-
ing (PCL, (Nachum et al., 2017)) also parametrize the
Q-value as a sum of a log-policy and a value. Trust-
PCL (Nachum et al., 2018), builds on PCL by adding
a trust region constraint on the policy update, similar
to our KL regularization term. A key diﬀerence with
IQ is that (Trust-)PCL is a residual algorithm, while

Environments and metrics. We evaluate IQ ﬁrst
on the OpenAI Gym environment (Brockman et al.,
2016). It consists of 5 locomotion tasks, with action
spaces ranging from 3 (Hopper-v2) to 17 dimensions
(Humanoid-v2). We use a rather long time horizon
setting, evaluating our algorithm on 20M steps on each
environments. We also provide result on the Adroit
manipulation dataset (Rajeswaran et al., 2017), with a

Nino Vieillard, Marcin Andrychowicz, Anton Raichuk, Olivier Pietquin, Matthieu Geist

similar setting of 20M environment steps. Adroit is a
collection of 4 hand manipulation tasks, often used in
an oﬄine RL setting, but here we use it only as a direct
RL benchmark. Out of these 4 tasks, we only consider
3 of them: we could not ﬁnd any working algorithm
(baseline or new) on the “relocate” task. To summarize
the performance of an algorithm, we report the baseline-
normalized score along iterations: It normalizes the
score so that 0% corresponds to a random score, and
100% to a given baseline. It is deﬁned for one task as
score = scorealgorithm−scorerandom
, where the baseline is
scorebaseline−scorerandom
the best version of SAC on Mujoco and Adroit after
20M steps. We report aggregated results, showing the
mean and median of these normalized scores along the
tasks. Each score is reported as the average over 20
random seeds. For each experiment, the corresponding
standard deviation is reported in Appx. B.3.

implement

IQ algorithms. We
IQ with the
Acme (Hoﬀman et al., 2020) codebase. It deﬁnes two
deep neural networks, a policy network πθ and a value
network Vφ. IQ interacts with the environment through
πθ, and collect transitions that are stored in a FIFO
replay buﬀer. At each interaction, IQ updates θ and
φ by performing a step of stochastic gradient descent
with Adam (Kingma and Ba, 2015) on LIQ (Eq. (2)).
During each step, IQ updates a copy of the weights
θ, ¯θ, with a smooth update ¯θ ← (1 − λ)¯θ + λθ, with
λ ∈ (0, 1). It tracks a similar copy ¯φ of φ. We keep
almost all common hyperparameters (networks archi-
tecture, λ, etc.) the same as our main baseline, SAC.
We only adjust the learning rate for two tasks, Hu-
manoid and Walker, where we used a lower value: we
found that IQ beneﬁts from this, while for SAC we did
not observe any improvement (we provide more details
and complete results in Appx. B.3). Our value net-
work has the same architecture as the SAC Q-networks
except that the input size is only the state size (as it
does not depend on the action). The policy network
has the same architecture as the SAC policy network,
and diﬀers only by its output: IQ policy outputs a
multicategorical policy (so n · d values, where d is the
dimensionality of the action space and n is the number
of discrete action on each dimension), while SAC policy
outputs 2 d-dimensional vectors (mean and diagonal
covariance matrix of a Gaussian). We use n = 11 in our
experiments. IQ relies on two other hyperparameters,
α and τ . We selected a value of τ per task suite: 10−2
on Gym tasks and 10−3 on Adroit. To make the dis-
tinction between the cases when α = 0 and α > 0, we
denote IQ(α > 0) as M-IQ, for Munchausen-IQ, since
it makes use of the Munchausen regularization term.
For M-IQ, we found α = 0.9 to be the best performing
value, which is consistent with the ﬁndings of Vieillard
et al. (2020b). We report a empirical study of these

Figure 2: SAC-normalized mean scores on Gym (top)
and Adroit (bottom).

parameters in the next part of this Section. Extended
explanations are provided in Appx. B.2.

Baselines. On continuous control tasks, our main
baseline is SAC, as it reaches state-of-the-art perfor-
mance on Mujoco tasks. We compare to the version of
SAC that uses an adaptive temperature for reference,
but note that for IQ we keep a ﬁxed temperature (τ )
setting. To reach its best performance, SAC either uses
a speciﬁc temperature value per task, or an adaptive
scheme that controls the entropy of the policy. This
method could be extended to multicategorical policies,
but we leave this for future work, and for IQ we use
the same value of τ for all tasks of an environment. We
use SAC with the default parameters from Haarnoja
et al. (2018b) on Gym, and a speciﬁcally tuned version
of SAC on Adroit. Remarkably, SAC and IQ work with
similar hyperparameter ranges on both benchmarks.
We only found that using a learning rate of 3 · 10−5
(instead of 3 · 10−4) gave better performance on Adroit.
We also compare IQ to Trust-PCL. It is the closest al-
gorithm to IQ, with a similar parametrization (SBEED
also has this parametrization, but all the environments
we consider here have detreministic dynamics, and thus
there is no bias issue, see Sec. 5). To be fair, we com-
pare to our version of Trust-PCL, which is essentially a
residual version of IQ, where the target value network
is replaced by the online one. We use Trust-PCL
V ¯φ
with a ﬁxed temperature, and we tuned this temper-
ature to the environment. We found that Trust-PCL

050100150200steps (1e5)020406080100mean scores (%)IQM-IQPCLTrust-PCLIQ-GaussianM-IQ-GaussianSAC050100150200steps (1e5)020406080100mean scores (%)IQM-IQSACImplicitly Regularized RL with Implicit Q-values

Figure 3: Scores on Gym and Adroit. Vertical bars denote +/- empirical standard deviation over 20 seeds.

reaches its best performance with signiﬁcantly lower
values of τ compared to IQ. In the ablation (Fig. 2) we
used τ = 10−4 for PCL and Trust-PCL.

Comparison to baselines. We report aggregated
results of IQ and M-IQ on Gym and Adroit in Fig. 2
(median scores can be found in Appx. B.3). IQ reaches
competitive performance to SAC. It is less sample eﬃ-
cient on Gym (SAC reaches higher performance sooner),
but faster on Adroit, and IQ reaches a close ﬁnal per-
formance on both environments. Detailed scores in
Fig. 3 show how the performance varies across environ-
ments. Speciﬁcally, IQ outperforms SAC on 3 of the
8 considered environments: Ant, Hopper, and Door.
On the 5 others, SAC performs better, but IQ and
M-IQ still reach a reasonable performance. Moreover,
on almost all environments, all mean performances are
within the same conﬁdence interval (Humanoid, and to
a less extent HalfCheetah, being notable exceptions, in
favor of SAC). The diﬀerence in performance between
IQ and M-IQ represents the impact of the Munchausen
term (i.e KL regularization). It is never detrimental,
and can even bring some improvement on Gym, but
is clearly less useful than in discrete action. Indeed,
M-DQN (Vieillard et al., 2020b) is designed with the
same α parameter but is tailored for discrete actions,
and empirically clearly beneﬁts from using α > 0. We
conjecture that this discrepancy comes from the induc-
tive bias we introduce with the multicategorical policy,
that could lessen the eﬀect of the Munchausen trick.

(M)-IQ relies
Inﬂuence of the hyperparameters.
on three key hyperparameters: the temperature τ , the
Munchausen coeﬃcient α, and the number of bins n.
The inﬂuence of α can be understood as the diﬀerence
between IQ and M-IQ, discussed above. For the two
others parameters, we study their inﬂuence with two
sweeps, for which we report learning curves in Appx B.3.
IQ seems to be robust enough to the number of bins,

and n turns out to be rather easy to tune: values of
n between 7 and 15 give similar results. On the other
hand, we ﬁnd that τ needs to be selected carefully:
while it helps learning, too high values of τ can be
detrimental to the performance, and it highlights that
its optimal value might be dependant on the task. In
the end, τ has more inﬂuence on IQ than α or n, and
stands out to be the key element to tune within IQ; thus,
adapting the automatic scheduling of the temperature
from SAC looks like a promising research direction.

Ablation study. We perform an ablation on impor-
tant components of IQ in Fig. 2. (1) We replace the
target network by its online counterpart in Eq. (2),
which gives us Trust-PCL (PCL is obtained by setting
α = 0), a residual version of our method. IQ and M-
IQ both outperform Trust-PCL and PCL on Mujoco.
(2) We use a Gaussian parametrization of the policy
instead of a multicategorical distribution. We observe
on Fig. 2 that this causes the performance to drop
drastically. This validates the considerations about the
necessary complexity of the policy from Section 4.

7 CONCLUSION

We introduced IQ, a parametrization of a Q-value that
mechanically preserves the softmax relation between
a policy and an implicit Q-function. Building on this
parametrization, we derived an oﬀ-policy algorithm,
that learns a policy and a value by minimizing a single
loss. We provided insightful analysis that justiﬁes
our algorithm and creates meaningful
links with
the literature (notably with the dueling networks
architecture). Speciﬁcally, IQ performs entropy and
implicit KL regularization on the policy. This kind
of regularization was already used and analyzed in
RL, but was limited by the diﬃculty of estimating the
softmax of Q-function in continuous action settings.
IQ ends this limitation by avoiding any approximation

050001000015000scoreHalfCheetah-v2IQM-IQSAC02000400060008000Ant-v20200040006000Walker2d-v201000200030004000Hopper-v20.00.51.01.52.0steps1e70500010000scoreHumanoid-v20.00.51.01.52.0steps1e701000020000Hammer-v00.00.51.01.52.0steps1e70200040006000Pen-v00.00.51.01.52.0steps1e701000200030004000Door-v0Nino Vieillard, Marcin Andrychowicz, Anton Raichuk, Olivier Pietquin, Matthieu Geist

in this softmax, eﬀectively extending the analysis of
this regularization. This parametrization comes at
a cost: it shifts the representation capacity from the
Q-network to the policy, which makes the use of Gaus-
sian representation ineﬀective. We solved this issue by
considering multicategorical policies, which allowed IQ
to reach performance comparable to state-of-the-art
methods on classic continuous control benchmarks.
Yet, we envision that studying richer policy classes
may results in even better performance. In the end,
this work brings together theory and practice: IQ is a
theory-consistent manner of implementing an algorithm
based on regularized VI in continuous actions settings.

References

Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval
Tassa, Remi Munos, Nicolas Heess, and Martin Ried-
miller. Maximum a posteriori policy optimisation.
In International Conference on learning Representa-
tions (ICLR), 2018. 2, 6

Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej,
Mateusz Litwin, Bob McGrew, Arthur Petron, Alex
Paino, Matthias Plappert, Glenn Powell, Raphael
Ribas, et al. Solving rubik’s cube with a robot hand.
arXiv preprint arXiv:1910.07113, 2019. 6

Mohammad G Azar, Mohammad Ghavamzadeh,
Hilbert J Kappen, and Rémi Munos. Speedy Q-
learning. In Advances in Neural Information Pro-
cessing System (NeurIPS), pages 2411–2419, 2011.
15

Marc G Bellemare, Georg Ostrovski, Arthur Guez,
Philip S Thomas, and Rémi Munos. Increasing the
action gap: New operators for reinforcement learn-
ing. In AAAI Conference on Artiﬁcial Intelligence
(AAAI), 2016. 4

Dimitri P Bertsekas and John N Tsitsiklis. Neuro
dynamic programming. Athena Scientiﬁc Belmont,
MA, 1996. 1

Greg Brockman, Vicki Cheung, Ludwig Pettersson,
Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint
arXiv:1606.01540, 2016. 2, 6

Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He,
Zhen Liu, Jianshu Chen, and Le Song. Sbeed: Con-
vergent reinforcement learning with nonlinear func-
tion approximation. In International Conference on
Machine Learning (ICML), pages 1125–1134. PMLR,
2018. 6

Thomas Degris, Martha White, and Richard S Sutton.
Oﬀ-policy actor-critic. International Conference on
Machine Learning (ICML), 2012. 1

Matthieu Geist, Bilal Piot, and Olivier Pietquin. Is the
bellman residual a bad proxy? Advances in Neural
Information Processing Systems (NeurIPS), 2017. 6
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin.
A Theory of Regularized Markov Decision Processes.
In International Conference on Machine Learning
(ICML), 2019. 2, 3

Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and
Sergey Levine. Continuous deep q-learning with
model-based acceleration. In International Confer-
ence on Machine Learning (ICML), 2016. 6

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and
Sergey Levine. Soft actor-critic: Oﬀ-policy maximum
entropy deep reinforcement learning with a stochas-
tic actor. In International Conference on Machine
Learning (ICML), 2018a. 1, 2

Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen,
George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar,
Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al.
Soft actor-critic algorithms and applications. arXiv
preprint arXiv:1812.05905, 2018b. 7

Matteo Hessel, Joseph Modayil, Hado Van Hasselt,
Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David
Silver. Rainbow: Combining improvements in deep
reinforcement learning.
In AAAI Conference on
Artiﬁcial Intelligence (AAAI), 2018. 5

Jean-Baptiste Hiriart-Urruty and Claude Lemaréchal.
Fundamentals of convex analysis. Springer Science
& Business Media, 2004. 12

Matt Hoﬀman, Bobak Shahriari, John Aslanides,
Gabriel Barth-Maron, Feryal Behbahani, Tamara
Norman, Abbas Abdolmaleki, Albin Cassirer, Fan
Yang, Kate Baumli, Sarah Henderson, Alex Novikov,
Sergio Gómez Colmenarejo, Serkan Cabi, Caglar Gul-
cehre, Tom Le Paine, Andrew Cowie, Ziyu Wang,
Bilal Piot, and Nando de Freitas. Acme: A re-
search framework for distributed reinforcement learn-
ing. arXiv preprint arXiv:2006.00979, 2020. URL
https://arxiv.org/abs/2006.00979. 7, 15

Diederik P Kingma and Jimmy Ba. Adam: A method
for stochastic optimization.
In nternational Con-
ference for Learning Representations (ICLR), 2015.
7

Jonas Degrave, Abbas Abdolmaleki, Jost Tobias Sprin-
genberg, Nicolas Heess, and Martin Riedmiller.
Quinoa: a q-function you infer normalized over ac-
tions. Deep RL Workshop at NeurIPS, 2018. 6

Luke Metz, Julian Ibarz, Navdeep Jaitly, and James
Davidson. Discrete sequential prediction of con-
tinuous actions for deep rl.
arXiv preprint
arXiv:1705.05035, 2017. 6

Implicitly Regularized RL with Implicit Q-values

Patrick Nadeem Ward, Ariella Smofsky,

and
Avishek Joey Bose. Improving exploration in soft-
actor-critic with normalizing ﬂows policies. arXiv
preprint arXiv:1906.02771, 2019. 6

Christopher JCH Watkins and Peter Dayan. Q-learning.

Machine learning, 8(3-4):279–292, 1992. 3

Brian D Ziebart. Modeling Purposeful Adaptive Behav-
ior with the Principle of Maximum Causal Entropy.
PhD thesis, University of Washington, 2010. 1

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland,
Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529,
2015. 5

Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and
Dale Schuurmans. Bridging the gap between value
and policy based reinforcement learning. Advances in
Neural Information Processing Systems (NeurIPS),
2017. 6

Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and
Dale Schuurmans. Trust-pcl: An oﬀ-policy trust
region method for continuous control. International
Conference on Learning Representations (ICLR),
2018. 6

Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta,
Giulia Vezzani, John Schulman, Emanuel Todorov,
and Sergey Levine. Learning complex dexterous
manipulation with deep reinforcement learning and
demonstrations. arXiv preprint arXiv:1709.10087,
2017. 2, 6

Danilo Rezende and Shakir Mohamed. Variational
inference with normalizing ﬂows. In International
Conference on Machine Learning (ICML). PMLR,
2015. 6

Bruno Scherrer, Mohammad Ghavamzadeh, Victor
Gabillon, Boris Lesner, and Matthieu Geist. Approx-
imate modiﬁed policy iteration and its application
to the game of Tetris. Journal of Machine Learning
Research, 16:1629–1676, 2015. 1, 4

Yunhao Tang and Shipra Agrawal. Discretizing con-
tinuous action space for on-policy optimization. In
AAAI Conference on Artiﬁcial Intelligence (AAAI),
volume 34, pages 5981–5988, 2020. 6

Arash Tavakoli, Fabio Pardo, and Petar Kormushev.
Action branching architectures for deep reinforce-
ment learning. In AAAI Conference on Artiﬁcial
Intelligence (AAAI), volume 32, 2018. 6

Nino Vieillard, Tadashi Kozuno, Bruno Scherrer,
Olivier Pietquin, Rémi Munos, and Matthieu Geist.
Leverage the average: an analysis of kl regularization
in rl. In Advances in Neural Information Processing
Systems (NeurIPS), 2020a. 1, 3, 4, 6, 12, 14, 15
Nino Vieillard, Olivier Pietquin, and Matthieu Geist.
Munchausen reinforcement learning. Advances in
Neural Information Processing Systems (NeurIPS),
2020b. 4, 5, 7, 8, 21

Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt,
Marc Lanctot, and Nando Freitas. Dueling network
architectures for deep reinforcement learning. In In-
ternational conference on machine learning (ICML),
pages 1995–2003. PMLR, 2016. 2, 5

Nino Vieillard, Marcin Andrychowicz, Anton Raichuk, Olivier Pietquin, Matthieu Geist

Contents. This Appendix is organized as follows

• Appendix A completes the theoretical claims of the paper

– A.1 details the softmax consistency
– A.2 gives background on KL and entropy regularization
– A.3 proves Theorem 1 on equivalence between IQ-DP and regularized VI
– A.4 proves Theorem 2 on error propagation in IQ-DP
– A.5 details the relation between IQ and Munchausen DQN

• Appendix B provides details on experiments

– B.1 gives information on our technical setup, regarding software and computing infrastructure
– B.2 details the implementation of our method
– B.3.1 gives a complete view of our experimental results
– B.3.2 studies the inﬂuence of the hyperparameters

A Analysis

This Appendix provides details and proofs on the IQ paramterization.

Reminder on notations. Throughout the Appendix, we use the following notations. Recall that we deﬁned
the action dot product as, for any u and v ∈ RS×A,

(cid:104)u, v(cid:105) = (cid:0) (cid:88)

u(s, a)v(s, a)(cid:1)

s ∈ RS .

a∈A

We also slightly overwrite the + operator. Precisely, for any Q ∈ RS×A, V ∈ RS , we deﬁne Q + V ∈ RS×A as

Write 1 ∈ RS×A the constant function of value 1. For any Q ∈ RS×A, we deﬁne the softmax operator as

∀(s, a) ∈ S × A, (Q + V )(s, a) = Q(s, a) + V (s).

where the fraction is overwritten as the addition operator, that is for any state-action pair (s, a),

softmax(Q) =

exp(Q)
(cid:104)1, exp Q(cid:105)

∈ RS×A,

softmax(Q)(a|s) =

exp Q(s, a)
a(cid:48)∈A exp Q(s, a(cid:48))

(cid:80)

.

A.1 About the softmax consistency

First, we provide a detailed explanation of the consistency of the IQ parametrization. In Section 2, we claim
that parametrizing a Q-value as Q = τ ln π + V enforces the relation π = softmax(Q/τ ). This relation comes
mechanically from the constraint that π is a distribution over actions. For the sake of precision, we provide a
detailed proof of this claim as formalized in the following lemma.

Lemma 1. For any Q ∈ RS×A, π ∈ ∆S

A, V ∈ RS , we have

Q = τ ln π + V ⇔

(cid:40)

π = softmax( Q
τ )
V = τ ln(cid:104)1, exp Q
τ (cid:105)

.

(5)

Proof. Directly from the left hand side of Eq. (5), we have

π = exp

Q − V
τ

.

Implicitly Regularized RL with Implicit Q-values

Since π ∈ ∆S
A

(π is a distribution over the actions), we have

(cid:104)1, π(cid:105) = 1 ⇔(cid:104)1, exp

(cid:105) = 1

Q − V
τ
(cid:19)

(cid:18)

⇔

exp

−V
τ

(cid:104)1, exp

Q
τ

(cid:105) = 1

(V does not depend on the actions)

⇔V = τ ln(cid:104)1, exp

Q
τ

(cid:105).

And, for the policy, this gives

π = exp

Q − V
τ

= exp

Q − τ ln(cid:104)1, exp Q
τ (cid:105)
τ

=

exp( Q
τ )
(cid:104)1, exp Q
τ (cid:105)

= softmax

Q
τ

.

A.2 Useful properties of KL-entropy-regularized optimization

The following proofs relies on some properties of the KL divergence and of the entropy. Consider the greedy step
of MD-VI((1 − α)τ, ατ ), deﬁned in Thm. 1

πk+2 = argmax

(cid:104)π, r + γP Vk(cid:105) + (1 − α)τ H(π) − ατ KL(π||πk+1).

(6)

π∈∆S
A

Since the function π → (1 − α)τ H(π) − ατ KL(π||πk+1) is concave in π, this optimization problem can be tackled
using properties of the Legendre-Fenchel transform (see for example Hiriart-Urruty and Lemaréchal (2004, Chap.
E) for general deﬁnition and properties, and Vieillard et al. (2020a, Appx. A) for application to our setting). We
quickly state two properties that are of interest for this work in the following Lemma.
Lemma 2. Consider the optimization problem of Eq. (6). Write Qk+1 = r + γP Vk, we have that

πk+2 =

k+1 exp Qk+1
πα
k+1 exp Qk+1

1, πα

α

α

(cid:68)

(cid:69) .

We also get a relation between the maximizer and the maximum

(cid:104)πk+2, r + γP Vk(cid:105) + (1 − α)τ H(π) − ατ KL(π||πk+1) = τ ln

(cid:28)

πα
k+1, exp

(cid:29)

.

Qk+1
τ

Proof. See Vieillard et al. (2020a, Appx. A).

A.3 Equivalence to MD-VI: proof of Theorem 1

We turn to the proof of Thm 1. This result formalizes an equivalence in the exact case between the IQ-DP scheme
and a VI scheme regularized by entropy and KL divergence. Recall that we deﬁne the update of IQ-DP at step k
as

τ ln πk+2 + Vk+1 = r + ατ ln πk+1 + γP Vk

IQ-DP(α, τ ).

(7)

Note that we are for now considering the scenario where this update is computed exactly. We will consider errors
later, in Thm 2. Recall Thm. 1.

Theorem 1. For any k ≥ 1, let (πk+2, Vk+1) be the solution of IQ-DP at step k. We have that

(cid:40)

πk+2 = argmax(cid:104)π, r + γP Vk(cid:105) + (1 − α)τ H(π) − ατ KL(π||πk+1)
Vk+1 = (cid:104)πk+2, r + γP Vk(cid:105) + (1 − α)τ H(πk+2) − ατ KL(πk+2||πk+1)

so IQ-DP(α, τ ) produces the same sequence of policies as a value-based version of Mirror Descent VI, MD-
VI(ατ, (1 − α)τ ) (Vieillard et al., 2020a).

Nino Vieillard, Marcin Andrychowicz, Anton Raichuk, Olivier Pietquin, Matthieu Geist

Proof. Applying Lemma 1 to Eq. (7) gives

(cid:40)πk+2 = softmax r+ατ ln πk+1+γP Vk

Vk+1 = τ ln

τ
(cid:68)
1, exp r+ατ ln πk+1+γP Vk

(cid:69)

.

τ

For the policy, we have

πk+2 =

exp (α ln πk+1) exp r+γP Vk
(cid:68)
1, exp (α ln πk+1) exp r+γP Vk

α

α

(cid:69) =

k+1 exp r+γP Vk
πα
k+1 exp r+γP Vk

1, πα

α

α

(cid:68)

(cid:69) ,

and as direct consequence of Lemma 2

πk+2 = argmax(cid:104)π, r + γP Vk(cid:105) + (1 − α)τ H(π) − ατ KL(π||πk+1).

For the value, we have:

(cid:28)

Vk+1 = τ ln

1, exp(α ln πk+1) exp

r + γP Vk
τ

(cid:29)

(cid:28)

= τ ln

πα
k+1, exp

r + γP Vk
τ

(cid:29)

,

and again applying Lemma 2 gives

Vk+1 = (cid:104)πk+2, r + γVk(cid:105) + (1 − α)τ H(πk+2) − ατ KL(πk+2||πk+1).

A.4 Error propagation: proof of Theorem 2

Now we turn to the proof of Thm 2. This theorem handles the IQ-DP scheme in the approximate case, when
errors are made during the iterations. The considered scheme is

τ ln πk+2 + Vk+1 = r + ατ ln πk+1 + γP Vk + (cid:15)k+1.

(8)

Recall Thm. 2.

Theorem 2. Write πk+1 and Vk the kth update of respectively the target policy and value networks. Consider
the error at step k, (cid:15)k ∈ RS×A, as the diﬀerence between the ideal and the actual updates of IQ. Formally, we
deﬁne the error as, for all k ≥ 1,

and the moving average of the errors as

(cid:15)k+1 = τ ln πk+2 + Vk+1 − (r + ατ ln πk+1 + γP Vk),

Ek = (1 − α)

k
(cid:88)

j=1

αk−j(cid:15)j.

We have the following results for two diﬀerent cases depending on the value of α. Note that when α < 1, we bound
the distance to regularized optimal Q-value.

1. General case: 0 < α < 1 and τ > 0, entropy and KL regularization together:

(cid:107)Q(1−α)τ
∗

− Q(1−α)τ
πk

(cid:107)∞ ≤

2
(1 − γ)2



(1 − γ)

k
(cid:88)

j=1



γk−j(cid:107)Ej(cid:107)∞

 + o

(cid:19)

.

(cid:18) 1
k

2. Speciﬁc case α = 1, τ > 0, use of KL regularization alone:

(cid:107)Q∗ − Qπk (cid:107)∞ ≤

2
1 − γ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
k

k
(cid:88)

j=1

(cid:15)j

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

+ O

(cid:19)

.

(cid:18) 1
k

Implicitly Regularized RL with Implicit Q-values

Proof. To prove this error propagation result, we ﬁrst show an extension of Thm. 1, that links Approximate
IQ-DP with a Q-value based version of MD-VI. This new equivalence makes IQ-DP corresponds exactly to a
scheme that is extensively analyzed by Vieillard et al. (2020a). Then our result can be derived as a consequence
of (Vieillard et al., 2020a, Thm 1) and (Vieillard et al., 2020a, Thm 2).

Deﬁne a (KL-regularized) implicit Q-value as

so that now, the IQ-DP update (Eq. (8)) can be written

Qk = τ ln πk+1 − ατ ln πk + Vk,

We then use same method that for the proof of Thm. 1. Speciﬁcally, applying Lemma 1 to the deﬁnition of Qk
gives for the policy

Qk+1 = r + γP Vk + (cid:15)k+1.

(9)

πk+1 = softmax

(cid:18) Qk + ατ ln πk
τ

(cid:19)

(Lemma 1)

=

k exp Qk
πα
k exp Qk

1, πα

α

α

(cid:68)

(cid:69)

⇔ πk+1 = argmax(cid:104)π, Qk(cid:105) + (1 − α)τ H(π) − ατ KL(π||πk).

(Lemma 2)

For the value, we have from Lemma 1 on Qk

(cid:28)

Vk = τ ln

1, exp

Qk + ατ ln πk
τ

(cid:29)

(cid:28)

= τ ln

πα
k , exp

(cid:29)

,

Qk
τ

then, using Lemma 2, and the fact that πk+1 = softmax Qk
τ

, we have

Vk = (cid:104)πk+1, Qk(cid:105) + (1 − α)τ H(πk+1) − ατ KL(πk+1||πk).

Injecting this in Eq. (9) gives

Qk+1 = r + γP ((cid:104)πk+1, Qk(cid:105) + (1 − α)τ H(πk+1) − ατ KL(πk+1||πk)) .

Thus, we have proved the following equivalence between DP schemes

τ ln πk+2 + Vk+1 = r + ατ ln πk+1 + γP Vk + (cid:15)k+1
(cid:109)

(cid:40)

πk+1 = argmax(cid:104)π, Qk(cid:105) + (1 − α)τ H(π) − ατ KL(π||πk)
Qk+1 = r + γP ((cid:104)πk+1, Qk(cid:105) + (1 − α)τ H(πk+1) − ατ KL(πk+1||πk)) + (cid:15)k+1,

(10)

with

Qk = τ ln πk+1 − ατ ln πk + Vk.
The above scheme in Eq. (10) is exactly the MD-VI scheme studied by Vieillard et al. (2020a), where they
deﬁne β = α and λ = ατ . We now use their analysis of MD-VI to apply their result to IQ-DP, building on the
equivalence between the schemes. Note that transferring this type of analysis between equivalent formulations of
DP schemes is justiﬁed because the equivalences exist in terms of policies. Indeed, IQ-DP and MD-VI compute
diﬀerent (Q)-values, but produce identical series of policies. Since (Vieillard et al., 2020a, Thm 1) and (Vieillard
et al., 2020a, Thm. 2) bound the distance between the optimal (regularized) Q-value and the actual (regularized)
Q-values of the computed policy, the equivalence in terms of policies is suﬃcient to apply these theorems to
IQ-DP. Speciﬁcally, (Vieillard et al., 2020a, Thm 1) applied to the formulation of IQ in Eq. (10) proves point 1 of
Thm. 2, that is the case where α = 0. The second part is proven by applying (Vieillard et al., 2020a, Thm 2) to
this same formulation.

Nino Vieillard, Marcin Andrychowicz, Anton Raichuk, Olivier Pietquin, Matthieu Geist

In the case of a tabular representation (no function approximation needed),
Remark on sample complexity.
and with access to a generative model, we could derive a sample complexity bound from Theorem 2. Indeed, we
can follow a similar analysis to the one of Speedy Q-learning by Azar et al. (2011) by combining our results with
Azuma-Hoeﬀding concentration inequalities; it would show that IQ beneﬁts from the same sample complexity as
tabular MD-VI (Vieillard et al., 2020a), that is O

samples to get an (cid:15)-optimal policy.

(cid:17)

(cid:16) |S||A|
(cid:15)2(1−γ)4

A.5

IQ and Munchausen DQN

We claim in Section 3 that IQ is a form of Munchausen algorithm, speciﬁcally Munchausen-DQN (M-DQN). Here,
we clarify this link. Note that all of the information below is contained in Appx. A.3 and Appx. A.4. The point
of this section is to re-write it using notations used to deﬁned IQ as a deep RL agent, notations consistent with
how M-DQN is deﬁned.

IQ optimizes a policy πθ and a value Vφ by minimizing a loss LIQ (Eq. (2)). Recall that IQ implicitly deﬁnes a
Q-function as Qθ,φ = τ ln πθ + Vφ. Identifying this in LIQ makes the connection between Munchausen RL and IQ
completely clear. Indeed, the loss can be written as















ˆE

rt + ατ ln π¯θ(at|st) +

γV ¯φ(st+1)
(cid:124)
(cid:125)
(cid:123)(cid:122)
Q ¯θ, ¯φ (st+1,a)
τ ln (cid:80)
a exp
τ



− τ ln πθ(at|st) − Vφ(st)


(cid:125)
(cid:123)(cid:122)
Qθ,φ

(cid:124)

,








2

and since we have (Lemma 2, and using the fact that π¯θ = softmax(Q¯θ, ¯φ/τ ))

τ ln

(cid:88)

exp

a

Q¯θ, ¯φ(s, a)
τ

(cid:88)

=

a

π¯θ(a|s) (cid:0)Q¯θ(s, a) − τ ln π¯θ, ¯φ(a|s)(cid:1) ,

we get that the loss is



(cid:32)

ˆE



rt + ατ ln π¯θ(at|st) +

(cid:88)

a

π¯θ(a|st+1) (cid:0)Q¯θ, ¯φ(st+1, a) − τ ln π¯θ(a|st+1)(cid:1) − Qθ,φ(st, at)

(cid:33)2
 ,

which is exactly the Munchausen-DQN loss on Qθ,φ. Thus, in a mono-dimensional action setting (classic discrete
control problems for examle), IQ can really be seen as a re-parameterized version of M-DQN.

B Additional material on experiments

This Appendix provides additional detail on experiments, along with complete empirical results.

B.1 General information on experiments

Used assets.
under the Apache License (2.0).

IQ is implemented on the Acme library (Hoﬀman et al., 2020), distributed as open-source code

Compute resources. Experiments were run on TPUv2. One TPU is used for a single run, with one random
seed. To produce the main results (without the sweeps over parameters), we computed 780 single runs. One of
this run on a TPUv2 takes from 3 to 10 hours depending on the environment (the larger the action space, the
longer the run).

B.2 Details on algorithms

On the relation between α and τ . The equivalence result of Theorem 1 explains the role and the relation
between τ and α. In particular, it shows that IQ-DP(α, τ ) performs a VI scheme in an entropy-regularized MDP
(or in a max-entropy setting) where the temperature is not τ , but (1 − α)τ . Indeed, in this framework, the α
parameter balances between two forms of regularization: with α = 0, IQ-DP is only regularized with entropy,

Implicitly Regularized RL with Implicit Q-values

but with α > 0, IQ-DP is regularized with both entropy and KL. Thus, IQ-DP modiﬁes implicitly the intrinsic
temperature of the MDP it is optimizing for. To account for this discrepancy, every time we evaluate IQ with
α > 0 (that is, M-IQ), we report scores using τ /(1 − α), and not τ . For example, on Gym, we used a temperature
of 0.01 for IQ, and thus 0.1 for M-IQ (since, in our experiments, we took α = 0.9).

Discretization. We used IQ with policies that discretize the action space evenly. Here, we provide a precise
deﬁnition for our discretization method. Consider a multi-dimensional action space A of dimension d, each
dimension being a bounded interval [amin, amax], such that A = [amin, amax]d. We discretize each dimension of
the space uniformly in n values δj, for 0 ≤ j ≤ n − 1. The bins values are deﬁned as

and, for each j ∈ {1, . . . n − 1},

δ0 = amin +

amax − amin
2n

,

δj = δ0 + j

amax − amin
n

.

It eﬀectively deﬁnes a discrete action space

A(cid:48) =

d×

j=1

Aj, with Aj = {δ0, . . . δn−1}.

We use n = 11 in all of our experiments. The values of d, amin and amax depend on the environments speciﬁcations.

Evaluation setting. We evaluate our algorithms on Mujoco environements from OpenAI Gym and from the
Adroit manipulation tasks. On each enviroenment, we track performance for 20M environment steps. Every 10k
environment steps, we stop learning, and we evaluate our algorithm by reporting the average undiscounted return
over 10 episodes. We use deterministic evaluation, meaning that, at evaluation time, the algorithms interact by
choosing the expected value of the policy in one state, not by sampling from this policy (sampling is used during
training).

Pseudocode. We provide a pseudocode of IQ in Algorithm 1. This pseudocode describes a general learning
procedure that is followed by all agents. Replacing the IQ loss in Algorithm 1 by its residual version will give
the pseudocode for PCL, and replacing it by the actor and critic losses of SAC will give the pseudocode for this
method.

Algorithm 1 Implicit Q-values
Require: T ∈ N∗ the number of environment steps, λ ∈ (0, 1) the update coeﬃcient, γ ∈ [0, 1) the dicount factor,
τ ∈ (0, 1) the entropy temperature, α ∈ [0, 1) the implicit KL term, and hyperparameters detailed in Table 1.
Initialize θ, φ at random
B = {}
¯θ = θ
¯φ = φ
for t = 1 to T do

Collect a transition b = (st, at, rt, st+1) from πθ
B ← B ∪ {b}
On a random batch of transitions Bt ⊂ B, update (θ, φ) with one step of SGD on

ˆE(st,at,rt,st+1)∼Bt

(cid:104)(cid:0)rt + ατ ln π¯θ(at|st) + γV ¯φ(st+1) − τ ln πθ(at|st) − Vφ(st)(cid:1)2(cid:105)

,

¯θ ← λ¯θ + (1 − λ)θ
¯φ ← λ ¯φ + (1 − λ)φ

end for
return πθ

Nino Vieillard, Marcin Andrychowicz, Anton Raichuk, Olivier Pietquin, Matthieu Geist

HyperParameters. We Provide the hyperparameters used for our experiments in Tab. 1. If a parameter is
under “common parameters”, then it was used for all algorithms. We denote FCn a fully connected layer with an
output of n neurons. Recall that d is the dimension of the action space, and n is the number of bins we discretize
each dimension into.

Table 1: Parameters used for algorithms and ablations.

Common parameters

Parameter

λ (update coeﬃcient)
γ (discount)
|B| (replay buﬀer size)
|Bt| (batch size)
activations
optimizer
learning rate

Value

0.05
0.99
106
256
Relu
Adam
3 · 10−4

IQ speciﬁc parameters

τ (entropy temperature)
α (implicit KL term)
n (number of bins for the discretization)
π-network
V -network structure

0.01 on Gym, 0.001 on Adroit
0.9
11
(input: state) FC 512 − FC 512 − FC nd
(input: state) FC 512 − FC 512 − FC 1

(Trust)-PCL speciﬁc parameters

τ (entropy temperature)
π-network and V -network structures

1 · 10−4 on Gym
idem as IQ

SAC speciﬁc parameters

π-network structure
Q-network structure
learning rate

(input: state) FC 512 − FC 512 − FC 2d
(input: state and action) FC 512 − FC 512 − FC 1
3 · 10−4 on Gym, 3 · 10−5 on Adroit

Implicitly Regularized RL with Implicit Q-values

Figure 4: SAC-normalized aggregated scores on Gym environments. Best parameters: IQ uses a diﬀerent learning
rate for Humanoid-v2 and Walker2d-v2. Left: Mean scores. Right: Median scores.

B.3 Additional results

This Appendix section provides complete description of the scores and ablations in B.3.1, along with a study of
some hyperparameters in B.3.2.

B.3.1 Complete scores and ablations

Here, we provide detailed results in a more readable fashion in Fig. 4 and 5. We also give more details on the
aggregated scores (median scores, explanations on the hyperparmeter selection), and provide a more extensive
analysis of our experiments.

Aggregated scores. Aggregated scores on Gym and Adroit environments are reported in Figures 9 and 5. We
see from these results that (M)-IQ reaches performance close to SAC, but is still slightly below. On Gym, IQ is
slower (as in less sample eﬃcient) than SAC, but is faster on Adroit. One important thing to notice is that we
compare to the best version of SAC we found, that uses an adaptive temperature, while we use the same ﬁxed
temperature for all of the environments. Experimentally, it appears that IQ is more sensitive to hyperparameters
than SAC4. Notably, on Humanoid-v2 and Walker2d-v2, we show in Figure 6 that IQ can reach a much more
competitive score by using a speciﬁc hyperparameter per environment. To reﬂect this, we use a learning rate of
3 · 10−5 instead of 3 · 10−4 for Humanoid and Walker, while still comparing to the best version of SAC we found
(we also tested SAC with those parameters variations, but it did not improve the performance). This is reported
as best parameters results in Fig. 4; and for completeness, we report score using unique hyperparameters in Fig. 9.
On Adroit, we did not observe such variations, and thus we report scores with a single set of hyperparameters.

Detailed scores. We report detailed scores on Gym environments in Figure 7 and on Adroit in Figure 8.
On this ﬁgures, the thick line corresponds to the average over 20 seeds, and the error bars represents +/−
the empirical standard deviation over those seeds. It shows how the performance varies across environments.
Speciﬁcally, IQ outperforms SAC on 3 of the 8 considered environments: Ant, Hopper, and Door. On the 5
others, SAC performs better, but IQ and M-IQ still reach a reasonable performance. Moreover, on almost all
environments, all mean performances are within the same conﬁdence interval (Humanoid, and to a less extent
HalfCheetah, being notable exceptions, in favor of SAC).

Ablations. We report ablation scores, averaged over 20 seeds, comparing IQ, M-IQ, PCL, Trust-PCL and
(M)-IQ-Gaussian in Fig. 9. PCL (and Trust-PCL) are obtained by replacing V ¯φ
by Vφ in LIQ. (M)-IQ Gaussian is
obtained by parametrizing the policy as a diagonal normal distribution over the action. Results conﬁrm that the
ﬁxed-point approach is beneﬁcial wrt the residual one in this case, since IQ outperforms PCL and trust-PCL by a

4However, we note that SAC, when used with a ﬁxed temperature, is sensitive to the choice of this parameter. It
appears that IQ is less sensitive to this (we use the same temperature for all tasks), but a bit more to the learning rate. We
think this may be alleviated by adopting other policy representations, such as normalizing ﬂows, or by designing adaptive
schemes. We left this for future works.

0255075100125150175200steps (1e5)020406080100mean scores (%)IQM-IQSAC0255075100125150175200steps (1e5)020406080100median scores (%)IQM-IQSACNino Vieillard, Marcin Andrychowicz, Anton Raichuk, Olivier Pietquin, Matthieu Geist

Figure 5: SAC-Normalized aggregated scores on Adroit. Left: Mean scores. Right: Median scores.

Figure 6: Comparison of two value of the learning rate (lr) on IQ and M-IQ, on two environments. Each line
corresponds to the average score over 20 seeds.

050100150200steps (1e5)020406080100mean scores (%)IQM-IQSAC050100150200steps (1e5)020406080100median scores (%)IQM-IQSAC0.00.51.01.52.0steps1e7010002000300040005000scoreWalker2d-v2M-IQIQlr=3⋅10−4lr=3⋅10−50.00.51.01.52.0steps1e701000200030004000500060007000scoreHumanoid-v2M-IQIQlr=3⋅10−4lr=3⋅10−5Implicitly Regularized RL with Implicit Q-values

Figure 7: All individual scores on Gym. The vertical bars denote the empirical standard deviation over 20 seeds.
Best parameters: IQ uses a diﬀerent learning rate for Humanoid-v2 and Walker2d-v2.

Figure 8: All scores on Adroit. The vertical bars denote the empirical standard deviation over 20 seeds.

0.00.51.01.52.0steps1e7050001000015000scoreHalfCheetah-v2IQM-IQSAC0.00.51.01.52.0steps1e702000400060008000scoreAnt-v20.00.51.01.52.0steps1e70200040006000scoreWalker2d-v20.00.51.01.52.0steps1e701000200030004000scoreHopper-v20.00.51.01.52.0steps1e70500010000scoreHumanoid-v20.00.51.01.52.0steps1e701000020000scoreHammer-v0IQM-IQSAC0.00.51.01.52.0steps1e70200040006000scorePen-v00.00.51.01.52.0steps1e701000200030004000scoreDoor-v0Nino Vieillard, Marcin Andrychowicz, Anton Raichuk, Olivier Pietquin, Matthieu Geist

Figure 9: SAC-normalized ablation scores on Gym. Unique parameters: all algorithms use a single set of
hyperparmeters for the 5 environments.

clear margin in median and mean5. They also validate that the Gaussian parametrization is too limiting in IQ,
since IQ-Gaussian and M-IQ-Gaussian both totally collapse after 6 to 7 millions environment steps. Additionally,
this also highlights that the time frame usually considered on some of this environments (3M, or even 1M steps)
may be too short to actually observe the asymptotic behavior of many agents. Here, this time frame would have
prevented us to observe this collapsing phenomenon on the Gaussian IQ.

B.3.2

Inﬂuence of hyperparameters

In addition to the "deep" hyperparameters that control the function approximation (layer sizes, activations,
learning rate, ..), and that we take from our baselines, (M)-IQ relies on three key hyperparameters. Those are the
Munchausen coeﬃcient α, the temperature τ , and the number of bins n. The inﬂuence of α is represented by the
diﬀerence between M-IQ and IQ. Here, we study the inﬂuence of the two others hyperparameters, τ and n.

Inﬂuence of the temperature (τ ). We provide results for 3 values of τ and 2 values of α. We report scores
for IQ with α = 0 with diﬀerent values of τ in Fig. 10 and the same experiments on M-IQ (with α = 0.9) in
Fig. 11. We can draw two observations from these results. (1) They show that τ needs to be selected carefully: it
helps learning, but too high values can be highly detrimental to the performance. The temperature also depends
on the task. While we could ﬁnd a value that give reasonable results on all the considered environment, this
means that adapting the automatic scheduling of the temperature used in SAC could be a promising direction
for IQ. (2) The other observation is that clearly, τ has a much stronger inﬂuence on IQ than α. This is a key
empirical diﬀerence regarding the performance of M-DQN (Vieillard et al., 2020b), that has the same parameters,
but is evaluated on discrete actions settings. In these settings, the α parameters is shown to have a crucial
importance in terms of empirical results: M-DQN with α = 0.9 largely outperforms M-DQN with α = 0 on the
Atari benchmark. While this term still has eﬀect in IQ on some tasks, it is empirically less useful, even though it
is never detrimental. We conjecture that this discrepancy comes from the inductive bias we introduce with the
multicategorical policy, that could lessen the eﬀect of the Munchausen trick.

In Fig. 12, we show how the number of bins impact the performance of IQ
Inﬂuence of the number of bins.
on the Gym environments. The results indicate that this parameter is fairly easy to tune, as IQ is not too sensitive
to its variations. A remarkable observation is that with only 3 bins, IQ can achieve reasonable performance on
complex environments (see the results on Humanoid for example). This turns out to be a beneﬁcial feature of the
multicategorical policies. Indeed, the limiting factor of this parametrization, is that wee need a potentially large
number of bins in total, if the dimensions of the environment grows: the output size of the policy network is d ∗ n,
with d the dimension of the action space and n the number of bins. If we can keep the number of bins close to a
minimal value (as it seems to be the case here), this is promising for potential use of IQ on larger action spaces.

5We note that PCL was introduced only for discrete actions, and that Trust-PCL was proposed with a Gaussian policy,

on a much shorter time frame for training, and with many additional tricks.

050100150200steps (1e5)020406080100mean scores (%)IQM-IQPCLTrust-PCLIQ-GaussianM-IQ-GaussianSAC050100150200steps (1e5)020406080100median scores (%)IQM-IQPCLTrust-PCLIQ-GaussianM-IQ-GaussianSACImplicitly Regularized RL with Implicit Q-values

Figure 10: IQ (α = 0) with several values for τ on all Gym tasks. Each line is the average over 10 seeds.

Figure 11: M-IQ (α = 0.9) with several values for τ on all Gym tasks. Each line is the average over 10 seeds.

0.00.51.01.52.01e7050001000015000HalfCheetah-v20.00.51.01.52.01e70200040006000Ant-v20.00.51.01.52.01e7020004000Walker2d-v20.00.51.01.52.01e70100020003000Hopper-v20.00.51.01.52.01e7020004000Humanoid-v2τ=0.001τ=0.01τ=0.10.00.51.01.52.01e7050001000015000HalfCheetah-v20.00.51.01.52.01e70200040006000Ant-v20.00.51.01.52.01e7020004000Walker2d-v20.00.51.01.52.01e70100020003000Hopper-v20.00.51.01.52.01e7020004000Humanoid-v2τ=0.001τ=0.01τ=0.1Nino Vieillard, Marcin Andrychowicz, Anton Raichuk, Olivier Pietquin, Matthieu Geist

Figure 12: IQ with several number of bins on all Gym environments. Each line is the average over 10 seeds.

0.00.51.01.52.01e7050001000015000HalfCheetah-v20.00.51.01.52.01e7020004000Ant-v20.00.51.01.52.01e70200040006000Walker2d-v20.00.51.01.52.01e70100020003000Hopper-v20.00.51.01.52.01e70200040006000Humanoid-v2bins=3bins=7bins=11bins=15bins=19