ExactCombinatorialOptimizationwithGraphConvolutionalNeuralNetworksMaximeGasseMila,PolytechniqueMontréalmaxime.gasse@polymtl.caDidierChételatPolytechniqueMontréaldidier.chetelat@polymtl.caNicolaFerroniUniversityofBolognan.ferroni@specialvideo.itLaurentCharlinMila,HECMontréallaurent.charlin@hec.caAndreaLodiMila,PolytechniqueMontréalandrea.lodi@polymtl.caAbstractCombinatorialoptimizationproblemsaretypicallytackledbythebranch-and-boundparadigm.Weproposeanewgraphconvolutionalneuralnetworkmodelforlearningbranch-and-boundvariableselectionpolicies,whichleveragesthenaturalvariable-constraintbipartitegraphrepresentationofmixed-integerlinearprograms.Wetrainourmodelviaimitationlearningfromthestrongbranchingexpertrule,anddemonstrateonaseriesofhardproblemsthatourapproachproducespoliciesthatimproveuponstate-of-the-artmachine-learningmethodsforbranchingandgeneralizetoinstancessigniﬁcantlylargerthanseenduringtraining.Moreover,weimprovefortheﬁrsttimeoverexpert-designedbranchingrulesimplementedinastate-of-the-artsolveronlargeproblems.Codeforreproducingalltheexperimentscanbefoundathttps://github.com/ds4dm/learn2branch.1IntroductionCombinatorialoptimizationaimstoﬁndoptimalconﬁgurationsindiscretespaceswhereexhaustiveenumerationisintractable.Ithasapplicationsinﬁeldsasdiverseaselectronics,transportation,management,retail,andmanufacturing[42],butalsoinmachinelearning,suchasinstructuredpredictionandmaximumaposterioriinference[51;34;49].Suchproblemscanbeextremelydifﬁculttosolve,andinfactmostclassicalNP-hardcomputerscienceproblemsareexamplesofcombinatorialoptimization.Nonetheless,thereexistsabroadrangeofexactcombinatorialoptimizationalgorithms,whichareguaranteedtoﬁndanoptimalsolutiondespiteaworst-caseexponentialtimecomplexity[52].Animportantpropertyofsuchalgorithmsisthat,wheninterruptedbeforetermination,theycanusuallyprovideanintermediatesolutionalongwithanoptimalitybound,whichcanbeavaluableinformationintheoryandinpractice.Forexample,afteronehourofcomputation,anexactalgorithmmaygivetheguaranteethatthebestsolutionfoundsofarlieswithin2%oftheoptimum,evenwithoutknowingwhattheactualoptimumis.Thisqualitymakesexactmethodsappealingandpractical,andassuchtheyconstitutethecoreofmoderncommercialsolvers.Inpractice,mostcombinatorialoptimizationproblemscanbeformulatedasmixed-integerlinearprograms(MILPs),inwhichcasebranch-and-bound(B&B)[35]istheexactmethodofchoice.Branch-and-boundrecursivelypartitionsthesolutionspaceintoasearchtree,andcomputesrelaxation33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019),Vancouver,Canada.arXiv:1906.01629v3  [cs.LG]  30 Oct 2019boundsalongthewaytoprunesubtreesthatprovablycannotcontainanoptimalsolution.Thisiterativeprocessrequiressequentialdecision-making,suchasnodeselection:selectingthenextnodetoevaluate,andvariableselection:selectingthevariablebywhichtopartitionthenode’ssearchspace[41].Thisdecisionprocesstraditionallyfollowsaseriesofhard-codedheuristics,carefullydesignedbyexpertstominimizetheaveragesolvingtimeonarepresentativesetofMILPinstances[21].However,inmanycontextsitiscommontorepeatedlysolvesimilarcombinatorialoptimizationproblems,e.g.,day-to-dayproductionplanningandlot-sizingproblems[44],whichmaysigniﬁcantlydifferfromthesetofinstancesonwhichB&Balgorithmsaretypicallyevaluated.ItisthenappealingtousestatisticallearningfortuningB&Balgorithmsautomaticallyforadesiredclassofproblems.However,thislineofworkraisestwochallenges.First,itisnotobvioushowtoencodethestateofaMILPB&Bdecisionprocess[4],especiallysincebothsearchtreesandintegerlinearprogramscanhaveavariablestructureandsize.Second,itisnotclearhowtoformulateamodelarchitecturethatleadstoruleswhichcangeneralize,atleasttosimilarinstancesbutalsoideallytoinstanceslargerthanseenduringtraining.Inthisworkweproposetoaddresstheabovechallengesbyusinggraphconvolutionalneuralnetworks.Moreprecisely,wefocusonvariableselection,alsoknownasthebranchingproblem,whichliesatthecoreoftheB&Bparadigmyetisstillnotwelltheoreticallyunderstood[41],andadoptanimitationlearningstrategytolearnafastapproximationofstrongbranching,ahigh-qualitybutexpensivebranchingrule.Whilesuchanideaisnotnew[30;4;24],weproposetoaddressthelearningprobleminanovelway,throughtwocontributions.First,weproposetoencodethebranchingpoliciesintoagraphconvolutionalneuralnetwork(GCNN),whichallowsustoexploitthenaturalbipartitegraphrepresentationofMILPproblems,therebyreducingtheamountofmanualfeatureengineering.Second,weapproximatestrongbranchingdecisionsbyusingbehavioralcloningwithacross-entropyloss,alessdifﬁculttaskthanpredictingstrongbranchingscores[4]orrankings[30;24].WeevaluateourapproachonfourclassesofNP-hardproblems,namelysetcovering,combinatorialauction,capacitatedfacilitylocationandmaximumindependentset.WecompareagainstthepreviouslyproposedapproachesofKhaliletal.[30],Alvarezetal.[4]andHansknechtetal.[24],aswellasagainstthedefaulthybridbranchingruleinSCIP[20],amodernopen-sourcesolver.Theresultsshowthatourchoiceofmodel,stateencoding,andtrainingprocedureleadstopoliciesthatcanofferasubstantialimprovementovertraditionalbranchingrules,andgeneralizewelltolargerinstancesthanthoseusedintraining.InSection2,wereviewthebroaderliteratureofworksthatusestatisticallearningforbranching.InSection3,weformallyintroducetheB&Bframework,andformulatethebranchingproblemasaMarkovdecisionprocess.InSection4,wepresentourstaterepresentation,model,andtrainingprocedureforaddressingthebranchingproblem.Finally,wediscussexperimentalresultsinSection5.2RelatedworkFirststepstowardsstatisticallearningofbranchingrulesinB&BweretakenbyKhaliletal.[30],wholearnabranchingrulecustomizedtoasingleinstanceduringtheB&Bprocess,aswellasAlvarezetal.[4]andHansknechtetal.[24]wholearnabranchingruleofﬂineonacollectionofsimilarinstances,inafashionsimilartous.Ineachcaseabranchingpolicyislearnedbyimitationofthestrongbranchingexpert,althoughwithadifferentlyformulatedlearningproblem.Namely,Khaliletal.[30]andHansknechtetal.[24]treatitasarankingproblemandlearnapartialorderingofthecandidatesproducedbytheexpert,whileAlvarezetal.[4]treatitasaregressionproblemandlearndirectlythestrongbranchingscoresofthecandidates.Incontrast,wetreatitasaclassiﬁcationproblemandsimplylearnfromtheexpertdecisions,whichallowsimitationfromexpertsthatdon’trelyonbranchingscoresororderings.Theseworksalsodifferfromoursinthreeotherkeyaspects.First,theyrelyonextensivefeatureengineering,whichisreducedbyourgraphconvolutionalneuralnetworkapproach.Second,theydonotevaluategeneralizationabilitytoinstanceslargerthanseenduringtraining,whichweproposetodo.Finally,ineachcaseperformancewasevaluatedonasimpliﬁedsolver,whereaswecompare,fortheﬁrsttimeandfavorably,againstafull-ﬂedgedsolverwithprimalheuristics,cutsandpresolvingactivated.WecompareagainsttheseapproachesinSection5.Otherworkshaveconsideredusinggraphconvolutionalneuralnetworksinthecontextofapproximatecombinatorialoptimization,wheretheobjectiveistoﬁndgoodsolutionsquickly,withoutseekinganyoptimalityguarantees.TheﬁrstworkofthisnaturewasbyKhaliletal.[31],whoproposedaGCNNmodelforlearninggreedyheuristicsonseveralcollectionsofcombinatorialoptimization2problemsdeﬁnedongraphs.ThiswasfollowedbySelsametal.[47],whoproposedarecurrentGCNNmodel,NeuroSAT,whichcanbeinterpretedasanapproximateSATsolverwhentrainedtopredictsatisﬁability.SuchworksprovideadditionalevidencethatGCNNscaneffectivelycapturestructuralcharacteristicsofcombinatorialoptimizationproblems.Otherworksconsiderusingmachinelearningtoimprovevariableselectioninbranch-and-bound,withoutdirectlylearningabranchingpolicy.DiLibertoetal.[15]learnaclustering-basedclassiﬁertopickavariableselectionruleateverybranchingdecisionuptoacertaindepth,whileBalcanetal.[8]usethefactthatmanyvariableselectionrulesinB&Bexplicitlyscorethecandidatevariables,andproposetolearnaweightingofdifferentexistingscorestocombinetheirstrengths.Otherworkslearnvariableselectionpolicies,butforalgorithmslessgeneralthanB&B.Liangetal.[39]learnavariableselectionpolicyforSATsolversusingabanditapproach,andLedermanetal.[36]extendtheirworkbytakingareinforcementlearningapproachwithgraphconvolutionalneuralnetworks.Unlikeourapproach,theseworksarerestrictedtoconﬂict-drivenclauselearningmethodsinSATsolvers,andcannotbereadilyextendedtoB&Bmethodsforarbitrarymixed-integerlinearprograms.Inthesamevein,Balunovicetal.[9]learnbyimitationlearningavariableselectionprocedureforSMTsolversthatexploitsspeciﬁcaspectsofthistypeofsolver.Finally,researchershavealsofocusedonlearningotheraspectsofB&Balgorithmsthanvariableselection.Heetal.[25]learnanodeselectionheuristicbyimitationlearningoftheoracleprocedurethatexpandsthenodewhosefeasiblesetcontainstheoptimalsolution,whileSongetal.[48]learnnodeselectionandpruningheuristicsbyimitationlearningofshortestpathstogoodfeasiblesolutions,andKhaliletal.[32]learnprimalheuristicsforB&Balgorithms.Thoseapproachesarecomplementarywithourwork,andcouldinprinciplebecombinedtofurtherimprovesolverperformance.Moregenerally,manyauthorshaveproposedmachinelearningapproachestoﬁne-tuneexactoptimizationalgorithms,notnecessarilyforMILPsingeneral.ArecentsurveyisprovidedbyBengioetal.[10].3Background3.1ProblemdeﬁnitionAmixed-integerlinearprogramisanoptimizationproblemoftheformargminx(cid:8)c>x|Ax≤b,l≤x≤u,x∈Zp×Rn−p(cid:9),(1)wherec∈Rniscalledtheobjectivecoefﬁcientvector,A∈Rm×ntheconstraintcoefﬁcientmatrix,b∈Rmtheconstraintright-hand-sidevector,l,u∈Rnrespectivelytheloweranduppervariableboundvectors,andp≤nthenumberofintegervariables.Underthisrepresentation,thesizeofaMILPistypicallymeasuredbythenumberofrows(m)andcolumns(n)oftheconstraintmatrix.Byrelaxingtheintegralityconstraint,oneobtainsacontinuouslinearprogram(LP)whosesolutionprovidesalowerboundto(1),andcanbesolvedefﬁcientlyusing,forexample,thesimplexalgorithm.IfasolutiontotheLPrelaxationrespectstheoriginalintegralityconstraint,thenitisalsoasolutionto(1).Ifnot,thenonemaydecomposetheLPrelaxationintotwosub-problems,bysplittingthefeasibleregionaccordingtoavariablethatdoesnotrespectintegralityinthecurrentLPsolutionx?,xi≤bx?ic∨xi≥dx?ie,∃i≤p|x?i6∈Z,(2)whereb.candd.erespectivelydenotetheﬂoorandceilfunctions.Inpractice,thetwosub-problemswillonlydifferfromtheparentLPinthevariableboundsforxi,whichgetupdatedtoui=bx?icintheleftchildandli=dx?ieintherightchild.Thebranch-and-boundalgorithm[52,Ch.II.4],initssimplestformulation,repeatedlyperformsthisbinarydecomposition,givingrisetoasearchtree.Bydesign,thebestLPsolutionintheleafnodesofthetreeprovidesalowerboundtotheoriginalMILP,whereasthebestintegralLPsolution(ifany)providesanupperbound.Thesolvingprocessstopswheneverboththeupperandlowerboundsareequalorwhenthefeasibleregionsdonotdecomposeanymore,therebyprovidingacertiﬁcateofoptimalityorinfeasibility,respectively.3.2BranchingrulesAkeystepintheB&Balgorithmisselectingafractionalvariabletobranchonin(2),whichcanhaveaverysigniﬁcantimpactonthesizeoftheresultingsearchtree[2].Assuch,branchingrulesareat3stx7≤0x7≥1x1≤2x1≥3A(st)={1,3,4}at=4st+1x7≤0x7≥1x1≤2x1≥3x4≤−2x4≥−1Figure1:B&BvariableselectionasaMarkovdecisionprocess.Ontheleft,astatestcomprisedofthebranch-and-boundtree,withaleafnodechosenbythesolvertobeexpandednext(inpink).Ontheright,anewstatest+1resultingfrombranchingonthevariableat=x4.thecoreofmoderncombinatorialoptimizationsolvers,andhavebeenthefocusofextensiveresearch[40;43;1;17].Sofar,thebranchingstrategyconsistentlyresultinginthesmallestB&Btreesisstrongbranching[5].Itdoessobycomputingtheexpectedboundimprovementforeachcandidatevariablebeforebranching,whichunfortunatelyrequiresthesolutionoftwoLPsforeverycandidate.Inpractice,runningstrongbranchingateverynodeisprohibitive,andmodernB&Bsolversinsteadrelyonhybridbranching[3;1]whichcomputesstrongbranchingscoresonlyatthebeginningofthesolvingprocessandgraduallyswitchestosimplerheuristicssuchas:theconﬂictscore(intheoriginalarticle),thepseudo-cost[43]orahand-craftedcombinationofthetwo.ForamoreextensivediscussionofB&BbranchingstrategiesinMILP,thereaderisreferredtoAchterbergetal.[3].3.3MarkovdecisionprocessformulationAsremarkedbyHeetal.[25],thesequentialdecisionsmadeduringB&BcanbeassimilatedtoaMarkovdecisionprocess[26].Considerthesolvertobetheenvironment,andthebranchertheagent.Atthetthdecisionthesolverisinastatest,whichcomprisestheB&Btreewithallpastbranchingdecisions,thebestintegersolutionfoundsofar,theLPsolutionofeachnode,thecurrentlyfocusedleafnode,aswellasanyothersolverstatistics(suchas,forexample,thenumberoftimeseveryprimalheuristichasbeencalled).ThebrancherthenselectsavariableatamongallfractionalvariablesA(st)⊆{1,...,p}atthecurrentlyfocusednode,accordingtoapolicyπ(at|st).ThesolverinturnextendstheB&Btree,solvesthetwochildLPrelaxations,runsanyinternalheuristic,prunesthetreeifwarranted,andﬁnallyselectsthenextleafnodetosplit.Wearetheninanewstatest+1,andthebrancheriscalledagaintotakethenextbranchingdecision.Thisprocess,illustratedinFigure1,continuesuntiltheinstanceissolved,i.e.,untiltherearenoleafnodeleftforbranching.AsaMarkovdecisionprocess,B&Bisepisodic,whereeachepisodeamountstosolvingaMILPinstance.Initialstatescorrespondtoaninstancebeingsampledamongagroupofinterest,whileﬁnalstatesmarktheendoftheoptimizationprocess.Theprobabilityofatrajectoryτ=(s0,...,sT)∈Tthendependsonboththebranchingpolicyπandtheremainingcomponentsofthesolver,pπ(τ)=p(s0)T−1Yt=0Xa∈A(st)π(a|st)p(st+1|st,a).Anaturalapproachtoﬁndgoodbranchingpoliciesisreinforcementlearning,withacarefullydesignedrewardfunction.However,thisraisesseveralkeyissueswhichwecircumventbyadoptinganimitationlearningscheme,asdiscussednext.4MethodologyWenowdescribeourapproachfortacklingtheB&BvariableselectionprobleminMILPs,whereweuseimitationlearningandadedicatedgraphconvolutionalneuralnetworkmodel.AstheB&BvariableselectionproblemcanbeformulatedasaMarkovdecisionprocess,anaturalwayoftrainingapolicywouldbereinforcementlearning[50].However,thisapproachrunsintomanyissues.Notably,asepisodelengthisproportionaltoperformance,andrandomlyinitializedpoliciesperformpoorly,standardreinforcementlearningalgorithmsareusuallysoslowearlyintrainingastomaketotaltrainingtimeprohibitivelylong.Moreover,oncetheinitialstatecorrespondingtoaninstanceisselected,therestoftheprocessisinstance-speciﬁc,andsotheMarkovdecisionprocessestendtobeextremelylarge.Inthisworkwechooseinsteadtolearndirectlyfromanexpertbranchingrule,anapproachusuallyreferredtoasimitationlearning[27].4v1v2v3c1c2e1,1e1,3e1,2e2,3CEVC1V1C2V2π(x)n×dm×n×em×cn×64m×64m×64n×64n×1initialembeddingC-sideconvolutionV-sideconvolutionﬁnalembedding+softmaxFigure2:Left:ourbipartitestaterepresentationst=(G,C,E,V)withn=3variablesandm=2constraints.Right:ourbipartiteGCNNarchitectureforparametrizingourpolicyπθ(a|st).4.1ImitationlearningWetrainbybehavioralcloning[45]usingthestrongbranchingrule,whichsuffersahighcomputa-tionalcostbutusuallyproducesthesmallestB&Btrees,asmentionedinSection3.2.Weﬁrstruntheexpertonacollectionoftraininginstancesofinterest,recordadatasetofexpertstate-actionpairsD={(si,a?i)}Ni=1,andthenlearnourpolicybyminimizingthecross-entropylossL(θ)=−1NX(s,a∗)∈Dlogπθ(a∗|s).(3)4.2StateencodingWeencodethestatestoftheB&Bprocessattimetasabipartitegraphwithnodeandedgefeatures(G,C,E,V),describedinFigure2(Left).OnonesideofthegrapharenodescorrespondingtotheconstraintsintheMILP,oneperrowinthecurrentnode’sLPrelaxation,withC∈Rm×ctheirfeaturematrix.OntheothersidearenodescorrespondingtothevariablesintheMILP,oneperLPcolumn,withV∈Rn×dtheirfeaturematrix.Anedge(i,j)∈Econnectsaconstraintnodeiandavariablenodejifthelatterisinvolvedintheformer,thatisifAij6=0,andE∈Rm×n×erepresentsthe(sparse)tensorofedgefeatures.NotethatundermererestrictionsintheB&Bsolver(namely,byenablingcutsonlyattherootnode),thegraphstructureisthesameforallLPsintheB&Btree,whichreducesthecostoffeatureextraction.Theexactfeaturesattachedtothegrapharedescribedinthesupplementarymaterials.Wenotethatthisisreallyonlyasubsetofthesolverstate,whichtechnicallyturnstheprocessintoapartially-observableMarkovdecisionprocess[6],butalsothatexcellentvariableselectionpoliciessuchasstrongbranchingareabletodowelldespiterelyingonlyonasubsetofthesolverstateaswell.4.3PolicyparametrizationWeparametrizeourvariableselectionpolicyπθ(a|st)asagraphconvolutionalneuralnetwork[23;46;12].Suchmodels,alsoknownasmessage-passingneuralnetworks[19],areextensionsofconvolutionalneuralnetworksfromgrid-structureddata(asinimagesorsounds)toarbitrarygraphs.Theyhavebeensuccessfullyappliedtoavarietyofmachinelearningtaskswithgraph-structuredinputs,suchaspredictionofmolecularproperties[16;19],programveriﬁcation[38],anddocumentclassiﬁcationincitationnetworks[33].Graphconvolutionsexhibitmanypropertieswhichmakethemanaturalchoiceforgraph-structureddataingeneral,andMILPproblemsinparticular:1)theyarewell-deﬁnednomattertheinputgraphsize;2)theircomputationalcomplexityisdirectlyrelatedtothedensityofthegraph,whichmakesitanidealchoiceforprocessingtypicallysparseMILPproblems;and3)theyarepermutation-invariant,thatistheywillalwaysproducethesameoutputnomattertheorderinwhichthenodesarepresented.Ourmodeltakesasinputourbipartitestaterepresentationst=(G,C,V,E)andperformsasinglegraphconvolution,intheformoftwointerleavedhalf-convolutions.Indetail,becauseofthebipartitestructureoftheinputgraph,ourgraphconvolutioncanbebrokendownintotwosuccessivepasses,5onefromvariabletoconstraintsandonefromconstraintstovariables.Thesepassestaketheformci←fC(cid:16)ci,(i,j)∈EXjgC(ci,vj,ei,j)(cid:17),vj←fV(cid:16)vj,(i,j)∈EXigV(ci,vj,ei,j)(cid:17)(4)foralli∈C,j∈V,wherefC,fV,gCandgVare2-layerperceptronswithreluactivationfunctions.Followingthisgraph-convolutionlayer,weobtainabipartitegraphwiththesametopologyastheinput,butwithpotentiallydifferentnodefeatures,sothateachnodenowcontainsinformationfromitsneighbors.Weobtainourpolicybydiscardingtheconstraintnodesandapplyingaﬁnal2-layerperceptrononvariablenodes,combinedwithamaskedsoftmaxactivationtoproduceaprobabilitydistributionoverthecandidatebranchingvariables(i.e.,thenon-ﬁxedLPvariables).TherightsideofFigure2providesanoverviewofourarchitecture.PrenormlayersIntheliteratureofGCNN,itiscommontonormalizeeachconvolutionoperationbythenumberofneighbours[33].AsnotedbyXuetal.[53]thismightresultinalossofexpressive-ness,asthemodelthenbecomesunabletoperformasimplecountingoperation(e.g.,inhowmanyconstraintsdoesavariableappears).Thereforeweoptforun-normalizedconvolutions.However,thisintroducesaweightinitializationissue.Indeed,weightinitializationinstandardCNNsreliesonthenumberofinputunitstonormalizetheinitialweights[22],whichinaGCNNisunknownbeforehandanddependsonthedataset.Toovercomethisissueandstabilizethelearningprocedure,weadoptasimpleafﬁnetransformationx←(x−β)/σ,whichwecallaprenormlayer,appliedrightafterthesummationin(4).Theβandσparametersareinitializedwithrespectivelytheempiricalmeanandstandarddeviationofxonthetrainingdataset,andﬁxedonceandforallbeforetheactualtraining.Adoptingbothun-normalizedconvolutionsandthispre-trainingprocedureimprovesourgeneralizationperformanceonlargerproblems,aswillbeshowninSection5.5ExperimentsWenowpresentacomparativeexperimentagainstthreecompetingmachinelearningapproachesandSCIP’sdefaultbranchingruletoassessthevalueofourapproach,aswellasanablationstudytovalidateourarchitecturalchoices.Codeforreproducingalltheexperimentscanbefoundathttps://github.com/ds4dm/learn2branch.5.1SetupBenchmarksWeevaluateourapproachonfourNP-hardproblembenchmarks.OurﬁrstbenchmarkiscomprisedofsetcoveringinstancesgeneratedfollowingBalasandHo[7],with1,000columns.Wetrainandtestoninstanceswith500rows,andweevaluateoninstanceswith500(Easy),1,000(Medium)and2,000(Hard)rows.Oursecondbenchmarkiscomprisedofcombinatorialauctioninstances,generatedfollowingthearbitraryrelationshipsprocedureofLeyton-Brownetal.[37,Section4.3].Wetrainandtestoninstanceswith100itemsfor500bids,andweevaluateoninstanceswith100itemsfor500bids(Easy),200itemsfor1,000bids(Medium)and300itemsfor1,500bids(Hard).OurthirdbenchmarkiscomprisedofcapacitatedfacilitylocationinstancesgeneratedfollowingCornuejolsetal.[14],with100facilities.Wetrainandtestoninstanceswith100customers,andweevaluateoninstanceswith100(Easy),200(Medium)and400(Hard)customers.Finally,ourfourthbenchmarkiscomprisedofmaximumindependentsetinstancesonErd˝os-Rényirandomgraphs,generatedfollowingtheprocedureofBergmanetal.[11,4.6.4]withafﬁnitysetto4.Wetrainandtestoninstanceswithgraphsof500nodes,andweevaluateoninstanceswith500(Easy),1000(Medium)and1500nodes(Hard).Thesefourbenchmarkswerechosenbecausetheyarechallengingforstate-of-the-artsolvers,butalsorepresentativeofthetypesofintegerprogrammingproblemsencounteredinpractice.Inparticular,setcoveringproblemscapturethequintessenceofintegerlinearprogramming,sincecolumngenerationformulationscanbewrittenforvirtuallyanydifﬁcultdiscreteoptimizationproblem.ThroughoutallexperimentsweuseSCIP6.0.1asthebackendsolver,withatimelimitof1hour.FollowingKarzanetal.[29],FischettiandMonaci[17]andKhaliletal.[30]weallowcuttingplanegenerationattherootnodeonly,anddeactivatesolverrestarts.AllotherSCIPparametersarekepttodefaultsoastomakecomparisonsasfairandreproducibleaspossible.BaselinesWecompareagainstahuman-designedstate-of-the-artbranchingrule:reliabilitypseu-docost(RPB),avariantofhybridbranching[1]whichisusedbydefaultinSCIP.Forcompleteness,6Table1:Imitationlearningaccuracyonthetestsets.SetCoveringCombinatorialAuctionCapacitatedFacilityLocationMaximumIndependentSetmodelacc@1acc@5acc@10acc@1acc@5acc@10acc@1acc@5acc@10acc@1acc@5acc@10TREES51.8±0.380.5±0.191.4±0.252.9±0.384.3±0.194.1±0.163.0±0.497.3±0.199.9±0.030.9±0.447.4±0.354.6±0.3SVMRANK57.6±0.284.7±0.194.0±0.157.2±0.286.9±0.295.4±0.167.8±0.198.1±0.199.9±0.048.0±0.669.3±0.278.1±0.2LMART57.4±0.284.5±0.193.8±0.157.3±0.386.9±0.295.3±0.168.0±0.298.0±0.099.9±0.048.9±0.368.9±0.477.0±0.5GCNN65.5±0.192.4±0.198.2±0.061.6±0.191.0±0.197.8±0.171.2±0.298.6±0.199.9±0.056.5±0.280.8±0.389.0±0.1Table2:Policyevaluationonseparateinstancesintermsofsolvingtime,numberofwins(fastestmethod)overnumberofsolvedinstances,andnumberofresultingB&Bnodes(lowerisbetter).Foreachproblem,themodelsaretrainedoneasyinstancesonly.SeeSection5.1fordeﬁnitions.EasyMediumHardModelTimeWinsNodesTimeWinsNodesTimeWinsNodesFSB17.30±6.1%0/10017±13.7%411.34±4.3%0/90171±6.4%3600.00±0.0%0/0n/a±n/a%RPB8.98±4.8%0/10054±20.8%60.07±3.7%0/1001741±7.9%1677.02±3.0%4/6547299±4.9%TREES9.28±4.9%0/100187±9.4%92.47±5.9%0/1002187±7.9%2869.21±3.2%0/3559013±9.3%SVMRANK8.10±3.8%1/100165±8.2%73.58±3.1%0/1001915±3.8%2389.92±2.3%0/4742120±5.4%LMART7.19±4.2%14/100167±9.0%59.98±3.9%0/1001925±4.9%2165.96±2.0%0/5445319±3.4%GCNN6.59±3.1%85/100134±7.6%42.48±2.7%100/1001450±3.3%1489.91±3.3%66/7029981±4.9%SetCoveringFSB4.11±12.1%0/1006±30.3%86.90±12.9%0/10072±19.4%1813.33±5.1%0/68400±7.5%RPB2.74±7.8%0/10010±32.1%17.41±6.6%0/100689±21.2%136.17±7.9%13/1005511±11.7%TREES2.47±7.3%0/10086±15.9%23.70±11.2%0/100976±14.4%451.39±14.6%0/9510290±16.2%SVMRANK2.31±6.8%0/10077±15.0%23.10±9.8%0/100867±13.4%364.48±7.7%0/986329±7.7%LMART1.79±6.0%75/10077±14.9%14.42±9.5%1/100873±14.3%222.54±8.6%0/1007006±6.9%GCNN1.85±5.0%25/10070±12.0%10.29±7.1%99/100657±12.2%114.16±10.3%87/1005169±14.9%CombinatorialAuctionFSB30.36±19.6%4/10014±34.5%214.25±15.2%1/10076±15.8%742.91±9.1%15/9055±7.2%RPB26.55±16.2%9/10022±31.9%156.12±11.5%8/100142±20.6%631.50±8.1%14/96110±15.5%TREES28.96±14.7%3/100135±20.0%159.86±15.3%3/100401±11.6%671.01±11.1%1/95381±11.1%SVMRANK23.58±14.1%11/100117±20.5%130.86±13.6%13/100348±11.4%586.13±10.0%21/95321±8.8%LMART23.34±13.6%16/100117±20.7%128.48±15.4%23/100349±12.9%582.38±10.5%15/95314±7.0%GCNN22.10±15.8%57/100107±21.4%120.94±14.2%52/100339±11.8%563.36±10.7%30/95338±10.9%CapacitatedFacilityLocationFSB23.58±29.9%9/1007±35.9%1503.55±20.9%0/7438±28.2%3600.00±0.0%0/0n/a±n/a%RPB8.77±11.8%7/10020±36.1%110.99±24.4%41/100729±37.3%2045.61±18.3%22/422675±24.0%TREES10.75±22.1%1/10076±44.2%1183.37±34.2%1/474664±45.8%3565.12±1.2%0/338296±4.1%SVMRANK8.83±14.9%2/10046±32.2%242.91±29.3%1/96546±26.0%2902.94±9.6%1/186256±15.1%LMART7.31±12.7%30/10052±38.1%219.22±36.0%15/91747±35.1%3044.94±7.0%0/128893±3.5%GCNN6.43±11.6%51/10043±40.2%192.91±110.2%42/821841±88.0%2024.37±30.6%25/292997±26.3%MaximumIndependentSetwereportaswelltheperformanceoffullstrongbranching(FSB),ourslowexpert.Wealsocompareagainstthreemachinelearningbranchers:thelearning-to-scoreapproachofAlvarezetal.[4](TREES)basedonanExtraTrees[18]model,aswellasthelearning-to-rankapproachesfromKhaliletal.[30](SVMRANK)andHansknechtetal.[24](LMART),basedonanSVMrank[28]andaLambdaMART[13]model,respectively.TheTREESmodelusesvariable-wisefeaturesfromourbipartitestate,obtainedbyconcatenatingthevariable’snodefeatureswithedgeandconstraintnodefeaturesstatisticsoveritsneighborhood.TheSVMRANKandLMARTmodelsbothusetheoriginalfeaturesproposedbyKhaliletal.[30],whichwere-implementedwithinSCIP.Moretrainingdetailsforeachmachinelearningmethodcanbefoundinthesupplementarymaterials.TrainingWetrainthemodelsoneachbenchmarkseparately.Namely,foreachbenchmark,wegenerate100,000branchingsamplesextractedfrom10,000randomlygeneratedinstancesfortraining,20,000branchingsamplesfrom2,000instancesforvalidation,andsamefortest(seesupplementarymaterialsfordetails).WereportinTable1thetestaccuracyofeachmachinelearningmodeloverﬁveseeds,asthepercentageoftimesthehighestrankeddecisionofthemodel(acc@1),oneoftheﬁvehighest(acc@5)andoneofthetenhighest(acc@10)isavariablewhichisgiventhehigheststrongbranchingscore.7EvaluationEvaluationisperformedforeachproblemdifﬁculty(Easy,Medium,Hard)on20newinstancesusingﬁvedifferentseeds1,whichamountstoatotalof100solvingattemptspermethod.WereportstandardmetricsforMILPbenchmarking2,thatis:the1-shiftedgeometricmeanofthesolvingtimesinseconds,includingrunningtimesofunsolvedinstanceswithoutextrapenalization(Time);thehardware-independentﬁnalnodecountsofinstancesthataresolvedbyallbaselines(Nodes);andthenumberoftimeseachbranchingpolicyresultsinthefastestsolvingtime,overthenumberofinstancessolved(Win).PolicyevaluationresultsaredisplayedinTable2.Notethatwealsoreporttheaverageper-instancestandarddeviation,so“64±13.6%nodes”meansittookonaverage64nodestosolveaninstance,andwhensolvingoneofthoseinstancesthenumberofnodesvariedby13.6%onaverage.5.2ComparativeexperimentIntermsofpredictionaccuracy(Table1),GCNNclearlyoutperformsthebaselinecompetitorsonallfourproblems,whileSVMRANKandLMARTareonparwitheachotherandtheperformanceofTREESisthelowest.Atsolvingtime(Table2),theaccuracyofeachmethodisclearlyreﬂectedinthenumberofnodesrequiredtosolvetheinstances.Interestingly,thebestmethodintermsofnodesisnotnecessarilythebestintermsoftotalsolvingtime,whichalsotakesintoaccountthecomputationalcostofeachbranchingpolicy,i.e.,thefeatureextractionandinferencetime.TheSVMRANKapproach,despitebeingslightlybetterthanLMARTintermsofnumberofnodes,isalsoslowerduetoaworserunningtime/numberofnodestrade-off.OurGCNNmodelclearlydominatesoverall,exceptoncombinatorialauction(Easy)andmaximumindependentset(Medium)instances,whereLMARTandRPBarerespectivelyfaster.OurGCNNmodelgeneralizeswelltoinstancesofsizelargerthanseenduringtraining,andoutper-formsSCIP’sdefaultbranchingruleRPBintermsofrunningtimeinalmosteveryconﬁguration.Inparticularandstrikingly,itsigniﬁcantlyoutperformsRPBintermsofnodesonmediumandhardinstancesforsetcoverandcombinatorialauctionproblems.Asexpected,theFSBexpertbrancherisnotcompetitiveintermsofrunningtime,despiteproducingverysmallsearchtrees.Themaximumindependentsetproblemseemsparticularlychallengingforgeneralization,asallmachinelearningapproachesreportalowernumberofsolvedinstancesthanthedefaultRPBbrancher,andGCNN,despitebeingthefastestmachinelearningapproachoverall,exhibitsahighvariabilitybothintermsoftimeandnumberofnodes.Fortheﬁrsttimeintheliteratureamachine-learning-basedapproachiscomparedwithanessentiallyfull-ﬂedgedMILPsolver.Forthisreason,theresultsareparticularlyimpressive,andindicatethatGCNNisaveryseriouscandidatetobeimplementedwithinaMILPsolver,asanadditionaltooltospeedupmixed-integerlinearprogrammingsolvers.Also,theysuggestthatmorecouldbegainedfromatightintegrationwithinacomplexsoftware,likeanyMILPsolveris.5.3AblationstudyWepresentanablationstudyofourproposedGCNNmodelonthesetcoveringproblembycomparingthreevariantsoftheconvolutionoperationin(4):meanratherthansumconvolutions(MEAN),sumconvolutionswithoutourprenormlayer(SUM)andﬁnallysumconvolutionswithprenormlayers,whichisthemodelweusethroughoutourexperiments(GCNN).ResultsontestinstancesarereportedinTable3.ThesolvingperformanceofbothvariantsMEANandSUMisverysimilartothatofourbaselineGCNNonsmallinstances.Onlargeinstanceshowever,thevariantsperformsigniﬁcantlyworseintermsofbothsolvingtimeandnumberofnodes,especiallyonhardinstances.Thisempiricalevidencesupportsourhypothesisthatsum-convolutionsofferabetterarchitecturalpriorthanmean-convolutionfromthetaskoflearningtobranch,andthatourprenormlayerhelpsforstabilizingtrainingandimprovinggeneralization.1InadditiontoMLmodelswhicharere-trainedwithadifferentseed,allmajorMILPsolvershaveaparameter,seed,thatrandomizessometie-breakingrules,soastobeabletoreportaggregatedresultsoverthesameinstance.2Seee.g.http://plato.asu.edu/bench.html8Table3:AblationstudyofourGCNNmodelonthesetcoveringproblem.Sumconvolutionsgeneralizebettertolargerinstances,especiallywhencombinedwithaprenormlayer.AccuraciesEasyMediumHardModelacc@1acc@5acc@10timewinsnodestimewinsnodestimewinsnodesMEAN65.4±0.192.4±0.198.2±0.06.7±3%13/100134±6%43.7±3%19/1001894±4%1593.0±4%6/7062227±6%SUM65.5±0.292.3±0.298.1±0.16.6±3%27/100134±6%42.5±3%45/1001882±4%1511.7±3%22/7057864±4%GCNN65.5±0.192.4±0.198.2±0.06.6±3%60/100134±8%42.5±3%36/1001870±3%1489.9±3%42/7056348±5%6DiscussionTheobjectiveofbranch-and-boundistosolvecombinatorialoptimizationproblemsasfastaspossible.Branchingpoliciesmustthereforebalancethequalityofdecisionstakenwiththetimespenttotakeeachdecision.Anextremeexampleofthistradeoffisstrongbranching:thispolicytakesexcellentdecisionsleadingtolownumberofnodesoverall,buteverydecision-makingstepissoslowthattheoverallrunningtimeisnotcompetitive.EarlyexperimentsshowedthatwecouldtakebetterdecisionsanddecreasethenumberofnodesslightlyonaveragebytrainingaGCNNpolicywithmorelayersorwithalargerembeddingsize.However,thiswouldalsoleadtoincreasedcomputationalcostsforinferenceandslightlylargertimesateachdecision,andintheendincreasedsolvingtimesonaverage.Thepolicyarchitectureweproposeisthusacompromisebetweenlearningcapacityandinferencespeed,somethingthatisnottraditionallyaconcernwithinthemachinelearningcommunity.Anotherconcernamongthecombinatorialoptimizationcommunityistheabilityofpoliciestrainedonsmallinstancestogeneralizetolargerinstances.Wewereabletoshowthatmachinelearningmethods,andtheGCNNmodelinparticular,cangeneralizetofairlylargerinstances.However,ingeneralitisexpectedthattheimprovementinperformancedecreasesasourmodelisevaluatedonprogressivelylargerproblems,ascanalreadybeobservedfromTable2.Inearlyexperimentswithevenlargerinstances(huge),weobservedaperformancedropforthemodeltrainedonoursmallinstances.Thiscouldpresumablyberemediedbytrainingonlargerinstancesintheﬁrstplace,andindeedamodeltrainedonmediuminstancesdidperformwellonthosehugeinstancesagain.Inanycase,therearelimitsastothegeneralizationabilityofanylearnedbranchingpolicy,andsincethelimitislikelyverydependentontheproblemstructure,itisdifﬁculttogiveanyprecisequantitativeestimatesapriori.Thisdesirableabilitytogeneralizeoutsideofthetrainingdistribution,sometimestermedtransferlearning,isalsonotatraditionalconcerninthemachinelearningcommunity.7ConclusionWeformulatedbranch-and-bound,thestandardexactmethodforsolvingmixed-integerlinearpro-grams,asaMarkovdecisionprocess.Inthiscontext,weproposedandevaluatedanovelapproachfortacklingthebranchingproblem,byexpressingthestateofthebranch-and-boundprocessasabipartitegraph,whichreducestheneedforfeatureengineeringbynaturallyleveragingthevariable-constraintstructureofMILPproblems,andallowsfortheencodingofbranchingpoliciesasagraphconvolutionalneuralnetwork.WedemonstratedonfourNP-hardproblemsthat,byadoptingasimpleimitationlearningscheme,thepolicieslearnedbyaGCNNwereoutperformingpreviouslyproposedmachinelearningapproachesforbranching,andcouldalsooutperformthedefaultbranchingstrategyofSCIP,amodernopen-sourcesolver.Mostimportantly,wedemonstratedthatthelearnedpoliciescouldgeneralizetoinstancesizeslargerthanseenduringtraining.Thisisessentialsincecollectingstrongbranchingdecisions,hencetraining,canbecomputationallyprohibitiveonlargeinstances.OurworkindicatesthattheGCNNmodel,especiallyusingsumconvolutionswiththeproposedprenormlayer,isagoodarchitecturalpriorforthetaskofbranchinginMILP.Infuturework,wewouldliketoassesstheviabilityofourapproachonabroadersetoncombinatorialproblems,andalsoexperimentwithreinforcementlearningmethodsforimprovingoverthepolicieslearnedbyimitation.Also,webelievethatthereisplentyofroomforhydridapproachescombiningtraditionalmethodsandmachinelearningforbranching,andwewouldliketodigdeeperintothelearnedpoliciesinordertoextractsomeknowledgeofinterestfortheMILPcommunity.9AcknowledgementsWewouldliketothanktheanonymousreviewerswhosecontributionshelpedconsiderablyimprovethequalityofthispaper.WewouldalsoliketothankAmbrosGleixnerandBenjaminMüllerforenlighteningdiscussionsandtechnicalhelpregardingSCIP,aswellasGonzaloMuñoz,AleksandrKazachkovandGiuliaZarpellonforinsightfuldiscussionsonvariableselection.Finally,wethankJasonJo,MengQuandMikePieperfortheirhelpfulcommentsonthestructureofthepaper.ThisworkwassupportedbytheCanadaFirstResearchExcellenceFund(CFREF),IVADO,CIFAR,GERAD,andCanadaExcellenceResearchChairs(CERC).References[1]TobiasAchterbergandTimoBerthold.Hybridbranching.InIntegrationofAIandORTechniquesinConstraintProgrammingforCombinatorialOptimizationProblems,2009.[2]TobiasAchterbergandRolandWunderling.Mixedintegerprogramming:Analyzing12yearsofprogress.FacetsofCombinatorialOptimization,pages449–481,2013.[3]TobiasAchterberg,ThorstenKoch,andAlexanderMartin.Branchingrulesrevisited.OperationsResearchLetters,33:42–54,2005.[4]AlejandroM.Alvarez,QuentinLouveaux,andLouisWehenkel.Amachinelearning-basedapproximationofstrongbranching.INFORMSJournalonComputing,29:185–195,2017.[5]DavidApplegate,RobertBixby,VašekChvátal,andWilliamCook.FindingcutsintheTSP.Technicalreport,DIMACS,1995.[6]KarlJ.Åström.OptimalcontrolofMarkovprocesseswithincompletestateinformation.JournalofMathematicalAnalysisandApplications,10:174–205,1965.[7]EgonBalasandAndrewHo.Setcoveringalgorithmsusingcuttingplanes,heuristics,andsubgradientoptimization:acomputationalstudy.InCombinatorialOptimization,pages37–60.Springer,1980.[8]Maria-FlorinaBalcan,TravisDick,TuomasSandholm,andEllenVitercik.Learningtobranch.InProceedingsoftheInternationalConferenceonMachineLearning,2018.[9]MislavBalunovic,PavolBielik,andMartinVechev.LearningtosolveSMTformulas.InAdvancesinNeuralInformationProcessingSystems31,pages10338–10349,2018.[10]YoshuaBengio,AndreaLodi,andAntoineProuvost.Machinelearningforcombinatorialoptimization:amethodologicaltourd’horizon.arXiv:1811.06128,2018.[11]DavidBergman,AndreA.Cire,Willem-JanVanHoeve,andJohnHooker.Decisiondiagramsforoptimization.Springer,2016.[12]JoanBruna,WojciechZaremba,ArthurSzlam,andYannLeCun.Spectralnetworksandlocallyconnectednetworksongraphs.InProceedingsoftheSecondInternationalConferenceonLearningRepresentations,2014.[13]ChristopherJ.C.Burges.FromRankNettoLambdaRanktoLambdaMART:AnOverview.Technicalreport,MicrosoftResearch,2010.[14]GerardCornuejols,RamaswamiSridharan,andJeanMichelThizy.Acomparisonofheuristicsandrelaxationsforthecapacitatedplantlocationproblem.EuropeanJournalofOperationalResearch,50:280–297,1991.[15]GiovanniDiLiberto,SerdarKadioglu,KevinLeo,andYuriMalitsky.Dash:Dynamicapproachforswitchingheuristics.EuropeanJournalofOperationalResearch,248:943–953,2016.[16]DavidK.Duvenaud,DougalMaclaurin,JorgeAguilera-Iparraguirre,RafaelBombarell,TimothyHirzel,AlánAspuru-Guzik,andRyanP.Adams.Convolutionalnetworksongraphsforlearningmolecularﬁngerprints.InAdvancesinNeuralInformationProcessingSystems28,pages2224–2232,2015.10[17]MatteoFischettiandMicheleMonaci.Branchingonnonchimericalfractionalities.OperationsResearchLetters,40:159–164,2012.[18]PierreGeurts,DamienErnst,andLouisWehenkel.Extremelyrandomizedtrees.Machinelearning,63:3–42,2006.[19]JustinGilmer,SamuelS.Schoenholz,PatrickF.Riley,OriolVinyals,andGeorgeE.Dahl.Neuralmessagepassingforquantumchemistry.InProceedingsoftheThirty-FourthInternationalConferenceonMachineLearning,pages1263–1272,2017.[20]AmbrosGleixner,MichaelBastubbe,LeonEiﬂer,TristanGally,GeraldGamrath,RobertLionGottwald,GregorHendel,ChristopherHojny,ThorstenKoch,MarcoE.Lübbecke,StephenJ.Maher,MatthiasMiltenberger,BenjaminMüller,MarcE.Pfetsch,ChristianPuchert,DanielRehfeldt,FranziskaSchlösser,ChristophSchubert,FelipeSerrano,YujiShinano,JanMerlinViernickel,MatthiasWalter,FabianWegscheider,JonasT.Witt,andJakobWitzig.TheSCIPOptimizationSuite6.0.Zib-report,ZuseInstituteBerlin,July2018.[21]AmbrosGleixner,GregorHendel,GeraldGamrath,TobiasAchterberg,MichaelBastubbe,TimoBerthold,PhilippChristophel,KatiJarck,ThorstenKoch,JeffLinderoth,MarcoLübbecke,HansD.Mittelmann,DeryaOzyurt,TedK.Ralphs,DomenicoSalvagnin,andYujiShinano.MIPLIB2017:Data-DrivenCompilationofthe6thMixed-IntegerProgrammingLibrary.Technicalreport,OptimizationOnline,August2019.[22]XavierGlorotandYoshuaBengio.Understandingthedifﬁcultyoftrainingdeepfeedforwardneuralnetworks.InProceedingsoftheThirteenthInternationalConferenceonArtiﬁcialIntelligenceandStatistics,pages249–256,2010.[23]MarcoGori,GabrieleMonfardini,andFrancoScarselli.Anewmodelforlearningingraphdomains.InProceedingsofthe2005IEEEInternationalJointConferenceonNeuralNetworks,volume2,pages729–734,2005.[24]ChristophHansknecht,ImkeJoormann,andSebastianStiller.Cuts,primalheuristics,andlearningtobranchforthetime-dependenttravelingsalesmanproblem.arXiv:1805.01415,2018.[25]HeHe,HalIIIDaumé,andJasonEisner.Learningtosearchinbranch-and-boundalgorithms.InAdvancesinNeuralInformationProcessingSystems27,pages3293–3301,2014.[26]RonaldA.Howard.DynamicProgrammingandMarkovProcesses.MITPress,Cambridge,MA,1960.[27]AhmedHussein,MohamedM.Gaber,EyadElyan,andChrisinaJayne.Imitationlearning:Asurveyoflearningmethods.ACMComputingSurveys,50:21,2017.[28]ThorstenJoachims.Optimizingsearchenginesusingclickthroughdata.InProceedingsoftheEighthACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining,pages133–142,2002.[29]FatmaK.Karzan,GeorgeL.Nemhauser,andMartinW.P.Savelsbergh.Information-basedbranchingschemesforbinarylinearmixedintegerproblems.MathematicalProgrammingComputation,1:249–293,2009.[30]EliasB.Khalil,PierreLeBodic,LeSong,GeorgeNemhauser,andBistraDilkina.Learningtobranchinmixedintegerprogramming.InProceedingsoftheThirtiethAAAIConferenceonArtiﬁcialIntelligence,pages724–731,2016.[31]EliasB.Khalil,HanjunDai,YuyuZhang,BistraDilkina,andLeSong.Learningcombinatorialoptimizationalgorithmsovergraphs.InAdvancesinNeuralInformationProcessingSystems30,pages6348–6358,2017.[32]EliasB.Khalil,BistraDilkina,GeorgeL.Nemhauser,ShabbirAhmed,andYufenShao.Learningtorunheuristicsintreesearch.InProceedingsoftheTwenty-SixthInternationalJointConferenceonArtiﬁcialIntelligence,pages659–666,2017.11[33]ThomasN.KipfandMaxWelling.Semi-supervisedclassiﬁcationwithgraphconvolutionalnetworks.InProceedingsoftheFifthInternationalConferenceonLearningRepresentations,2017.[34]AlexKuleszaandFernandoPereira.Structuredlearningwithapproximateinference.InAdvancesinNeuralInformationProcessingSystems21,pages785–792,2008.[35]AilsaH.LandandAlisonG.Doig.Anautomaticmethodofsolvingdiscreteprogrammingproblems.Econometrica,28:497–520,1960.[36]GilLederman,MarkusN.Rabe,andSanjitA.Seshia.Learningheuristicsforautomatedreasoningthroughdeepreinforcementlearning.arXiv:1807.08058,2018.[37]KevinLeyton-Brown,MarkPearson,andYoavShoham.Towardsauniversaltestsuiteforcombinatorialauctionalgorithms.InProceedingsoftheSecondACMConferenceonElectronicCommerce,pages66–76,2000.[38]YujiaLi,DanielTarlow,MarcBrockschmidt,andRichardS.Zemel.Gatedgraphsequenceneuralnetworks.InYoshuaBengioandYannLeCun,editors,ProceedingsoftheFourthInternationalConferenceonLearningRepresentations,2016.[39]JiaHuiLiang,VijayGanesh,PascalPoupart,andKrzysztofCzarnecki.LearningratebasedbranchingheuristicforSATsolvers.InInternationalConferenceonTheoryandApplicationsofSatisﬁabilityTesting,pages123–140,2016.[40]JeffT.LinderothandMartinW.P.Savelsbergh.Acomputationalstudyofsearchstrategiesformixedintegerprogramming.INFORMSJournalonComputing,11:173–187,1999.[41]AndreaLodiandGiuliaZarpellon.Onlearningandbranching:asurvey.TOP,25:207–236,2017.[42]VangelisT.Paschos,editor.ApplicationsofCombinatorialOptimization.MathematicsandStatistics.Wiley-ISTE,secondedition,2014.[43]JagatPatelandJohnW.Chinneck.Active-constraintvariableorderingforfasterfeasibilityofmixedintegerlinearprograms.MathematicalProgramming,110:445–474,2007.[44]YvesPochetandLaurenceA.Wolsey.Productionplanningbymixedintegerprogramming.SpringerScienceandBusinessMedia,2006.[45]DeanA.Pomerleau.Efﬁcienttrainingofartiﬁcialneuralnetworksforautonomousnavigation.NeuralComputation,3:88–97,1991.[46]FrancoScarselli,MarcoGori,AhC.Tsoi,MarkusHagenbuchner,andGabrieleMonfardini.Thegraphneuralnetworkmodel.IEEETrans.NeuralNetworks,20(1):61–80,2009.[47]DanielSelsam,MatthewLamm,BenediktBünz,PercyLiang,LeonardodeMoura,andDavidL.Dill.Learningasatsolverfromsingle-bitsupervision.InProceedingsoftheSeventhInterna-tionalConferenceonLearningRepresentations,2019.[48]JialinSong,RaviLanka,AlbertZhao,YisongYue,andMasahiroOno.Learningtosearchviaretrospectiveimitation.arXiv:1804.00846,2018.[49]VivekG.K.SrikumarandDanRoth.Onamortizinginferencecostforstructuredprediction.InProceedingsofthe2012JointConferenceonEmpiricalMethodsinNaturalLanguageProcessingandComputationalNaturalLanguageLearning,pages1114–1124,2012.[50]RichardS.SuttonandAndrewG.Barto.ReinforcementLearning:AnIntroduction.MITPress,secondedition,2018.[51]MartinJ.Wainwright,TommiS.Jaakkola,andAlanS.Willsky.MAPestimationviaagreementontrees:message-passingandlinearprogramming.IEEETransactionsonInformationTheory,51:3697–3717,2005.[52]LaurenceA.Wolsey.IntegerProgramming.Wiley-Blackwell,1988.12[53]KeyuluXu,WeihuaHu,JureLeskovec,andStefanieJegelka.Howpowerfularegraphneuralnetworks?InProceedingsoftheSeventhInternationalConferenceonLearningRepresentations,2019.13ExactCombinatorialOptimizationwithGraphConvolutionalNeuralNetworksSupplementaryMaterialsMaximeGasseMila,PolytechniqueMontréalmaxime.gasse@polymtl.caDidierChételatPolytechniqueMontréaldidier.chetelat@polymtl.caNicolaFerroniUniversityofBolognan.ferroni@specialvideo.itLaurentCharlinMila,HECMontréallaurent.charlin@hec.caAndreaLodiMila,PolytechniqueMontréalandrea.lodi@polymtl.ca1DatasetcollectiondetailsForeachbenchmarkproblem,namely,setcovering,combinatorialauction,capacitatedfacilitylocationandmaximumindependentset,wegenerate10,000randominstancesfortraining,2,000forvalidation,and3x20fortesting(20easyinstances,20mediuminstances,and20hardinstances).Inordertoobtainourdatasetsofstate-actionpairs{(si,ai)}fortrainingandvalidation,wepickaninstancefromthecorrespondingset(trainingorvalidation),solveitwithSCIP,eachtimewithanewrandomseed,andrecordnewnodestatesandstrongbranchingdecisionduringthebranch-and-boundprocess.Wecontinueprocessingnewinstancesbysamplingwithreplacement,untilthedesirednumberofnodesamplesisreached,thatis,100,000samplesfortraining,and20,000forvalidation.Notethatthiswaythewholesetoftrainingandvalidationinstancesisnotnecessarilyusedtogeneratesamples.WereportthenumberoftraininginstancesactuallyusedforeachprobleminTable1.Table1:NumberoftraininginstancessolvedbySCIPforobtaining100,000trainingsamples(state-actionpairs).WereportboththetotalnumberofSCIPsolvesandthenumberofuniqueinstancessolved,sinceinstancesaresampledwithreplacement.SetCombinatorialCapacitatedMaximumCoveringAuctionFacilityLocationIndependentSettotal77711132271596198unique5335666150464349Notethatthestrongbranchingrule,asimplementedintheSCIPsolver,doesnotonlyprovidesbranchingdecisions,butalsotriggersside-effectswhichchangethestateofthesolveritself.Inordertousestrongbranchingasanoracleonly,whengeneratingourtrainingsamples,were-implementedavanillaversionofthefullstrongbranchingruleinSCIP,namedvanillafullstrong.Thisversionoffullstrongbranchingalsofacilitatestheextractionofstrongbranchingscoresfortrainingtheranking-basedandregression-basedmachinelearningcompetitors,andwillbeincludedbydefaultinthenextversionofSCIP.33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019),Vancouver,Canada.arXiv:1906.01629v3  [cs.LG]  30 Oct 20192Trainingdetails2.1GCNNAsdescribedinthemainpaper,forourGCNNmodelwerecordstrongbranchingdecisions(a)andextractbipartitestaterepresentations(s)duringbranch-and-boundonacollectionoftraininginstances.Thisyieldsatrainingdatasetofstate-actionpairs{(st,at)}.WetrainourmodelwiththeTensorﬂow[1]library,withthesameprocedurethroughoutallexperiments.Weﬁrstpretraintheprenormlayersasdescribedinthemainpaper.Wethenminimizeacross-entropylossusingAdam[4]withminibatchesofsize32andaninitiallearningrateof1e-3.Wedividethelearningrateby5whenthevalidationlossdoesnotimprovefor10epochs,andstoptrainingifitdoesnotimprovefor20.AllexperimentsareperformedonamachinewithanIntelXeonGold6126CPUat2.60GHzandanNvidiaTeslaV100GPU.AlistofthefeaturesincludedinourbipartitestaterepresentationisgivenasTable2.Formoredetailsthereaderisreferredtothesourcecodeathttps://github.com/ds4dm/learn2branch.Table2:Descriptionoftheconstraint,edgeandvariablefeaturesinourbipartitestaterepresentationst=(G,C,E,V).TensorFeatureDescriptionCobj_cos_simCosinesimilaritywithobjective.biasBiasvalue,normalizedwithconstraintcoefﬁcients.is_tightTightnessindicatorinLPsolution.dualsol_valDualsolutionvalue,normalized.ageLPage,normalizedwithtotalnumberofLPs.EcoefConstraintcoefﬁcient,normalizedperconstraint.VtypeType(binary,integer,impl.integer,continuous)asaone-hotencoding.coefObjectivecoefﬁcient,normalized.has_lbLowerboundindicator.has_ubUpperboundindicator.sol_is_at_lbSolutionvalueequalslowerbound.sol_is_at_ubSolutionvalueequalsupperbound.sol_fracSolutionvaluefractionality.basis_statusSimplexbasisstatus(lower,basic,upper,zero)asaone-hotencoding.reduced_costReducedcost,normalized.ageLPage,normalized.sol_valSolutionvalue.inc_valValueinincumbent.avg_inc_valAveragevalueinincumbents.2.2SVMrankandLambdaMARTForSVMrankandLambdaMART,ateachnodetwerecordstrongbranchingranksρiforeachcandidatevariablei,andextractthesamevariable-wisefeaturesφKhaliliasKhaliletal.[3],withtwomodiﬁcations.First,becauseofdifferencesbetweenCPLEXandSCIP,itisdifﬁculttoreimplementsingle/doubleinfeasibilitystatistics,butSCIPkeepstrackofleft/rightinfeasibilitystatistics,namelythenumberoftimesbranchingonthevariableledtoaleft(resp.right)infeasiblechild.Becausethesestatisticscapturesimilaraspectsofthebranch-and-boundtree,weswappedonefortheother.Second,sinceinoursettingattesttimethereisnoseparatedatacollectionstage,statisticsarecomputedon2pastbranchingdecisionsateachtimestept.Otherwisewefollow[3],whichsuggestazero-onefeaturenormalizationoverthecandidate,a.k.a.query-basednormalization,andabinarizationoftherankinglabelsaroundthe80thcentile.Thisyieldsatrainingdatasetofvariable-wisestate-rankpairs{(φKhalili(st),ρi,t)}.TheSVMrankmodelistrainedbyminimizingacostsensitivepairwiselosswitha2nd-orderpolyno-mialfeatureaugmentation,andregularizationonthevalidationsetamongC∈{0.001,0.01,0.1,1},asintheoriginalimplementation.TheLambdaMARTmodeluses500estimators,andistrainedbymaximizinganormalizeddiscountedcumulativegainwithdefaultparameters,withearlystoppingusingthevalidationset.NotethatintheirSVMrankimplementation,Khaliletal.[3]limitateachnodethecandidatevariablestothetenwithhighestpseudocost,bothfortrainingandtesting.Thismakessenseintheironlinesetting,asthemodelistrainedandappliedafterobservingthebehaviorofstrongbranchingonafewinitialnodes,whichinitializesthepseudocostsproperly.Inourcontext,however,thereisnoinitialstrongbranchingphase,andasistobeexpectedweobservedthatapplyingthispseudocostﬁlteringschemedegradedperformance.Consequently,wedonotﬁlter,andrathertrainandtestusingthewholesetofcandidatevariablesateverynode.Inaddition,boththeSVMrankandLambdaMARTapproachessufferfrompoorertrainingscalabilitythanstochasticgradientdescent-baseddeepneuralnetworks,andwefoundthattrainingonourentiredatasetwasprohibitive.Indeed,trainingwouldhavetakenmorethan500GBofRAMforSVMrankandmorethan1weekforLambdaMARTforeventhesmallestproblemclass.Consequently,wehadtolimitthesizeofthedataset,andchosetoreducethetrainingsetto250,000candidatevariables,andthevalidationsetto100,000.2.3ExtraTreesForExtraTrees,werecordstrongbranchingscoresσiforeachcandidatevariablei,andextractvariable-wisefeaturesφsimpleifromourbipartitestatesasfollows.Foreveryvariablewekeeptheoriginalvariablenodefeaturesvi,whichweconcatenatewiththecomponent-wiseminimum,meanandmaximumoftheedgeandconstraintnodefeatures(ei,j,cj)overitsneighborhood.Asaresult,weobtainatrainingdatasetofvariable-wisestate-scorepairs{(φsimplei(st),σi,t)}.Themodelistrainedbymean-squarederrorminimization,whileattesttimebranchingismadeonthevariablewiththehighestpredictedscore,i∗=argmaxiˆσ(φi(st)).AsAlvarezetal.[2,p.14]mention,withthismodeltheinferencetimeincreaseswiththetrainingsetsize.Additionally,eventhoughinpracticetheExtraTreesmodeltrainingscalestolargerdatasetsthantheSVMrankandLambdaMARTmodels,herealsoweranintomemoryissueswhentrainingonourentiredataset.Consequently,forExtraTreeswealsochosetolimitthetrainingdatasetto250,000candidatevariables,aﬁgureroughlyinlinewithAlvarezetal.[2].NotethatwedidnotusetheexpertfeaturesproposedbyKhaliletal.[3]forExtraTrees,asthosewereresultingindegradedperformance.3WhenaredecisionshardfortheGCNNpolicy?TounderstandwhatkindsofdecisionsaretakenbythetrainedGCNNpolicybeyondtheimitationlearningaccuracy,itisinsightfultolookatwhenitisconﬁdent.Inparticular,wecantaketheentropyofourpolicyasameasureofconﬁdence,andcomputeananalogousquantityfromthestrongbranchingscores,H(strongbranching)=logmformthenumberofcandidateswithmaximalstrongbranchingscore.Figure1showsthatentropiesateverydecisionpointontheeasyinstancesforthefourproblemsroughlycorrelate.ThissuggeststhatthedecisionswheretheGCNNhesitatesarethosethatareintrinsicallydifﬁcult,sinceonthosetheexperthesitatesmoreaswell.4TrainingsetsizesforthemachinelearningmethodsAsdiscussed,computationallimitationsmadetrainingofmachinelearningcompetitorsimpossibleonthefulldatasetthatweusedtotrainourGCNNpolicy.Itisalimitationofthosemethodsthattheycannotbeneﬁtfromasmuchtrainingdataastheproposeddeeppolicywiththesamecomputationalbudget,andconversely,thescalabilityofourproposedmethodtolargetrainingsetsisamajoradvantage.Sincethereislittlereasonforartiﬁciallyrestrictinganymodeltosmallerdatasetsthan30246GCNN entropy01234567Strong branching entropySet Covering123456GCNN entropy0123456Strong branching entropyCombinatorial Auction01234GCNN entropy0.00.51.01.52.02.53.03.54.0Strong branching entropyCapacitated Facility Location02468GCNN entropy02468Strong branching entropyMaximum Independent SetFigure1:EntropyoftheGCNNpolicyagainstthe“entropy”ofstrongbranching.Table3:Numberofbranchingnodes(state-actionpairs)usedfortrainingthemachinelearningcompetitors,thatis,forobtaining250,000trainingand100,000validationsamples(variable-scorepairs).Incomparison,thenumberofbranchingnodesinthecompletedatasetusedtotraintheGCNNmodelis100,000fortraining,and20,000forvalidation.SetCombinatorialCapacitatedMaximumCoveringAuctionFacilityLocationIndependentSettraining197917386201534validation7826962465213theycanhandle,especiallysincethestate-of-the-artbranchingrule(reliabilitypseudocost)isnotevenamachinelearningmethod,wedidnotdosointhepaper.Nonetheless,anaturalquestioniswhethertheimprovementsinperformanceprovidedbytheGCNNcamesolelyfromincreaseddatasetsize.Toanswerthisquestion,were-trainedaGCNNmodelusingthesameamountofdataasforthecompetitors,thatis,250,000candidatevariablesfortraining,and100,000forvalidation.Thesizeoftheresultingdatasets,fromthepointofviewofthenumberofbranchingnodesrecorded,isreportedinTable3.AscanbeseeninTable4,theresultingmodel(GCNN-SMALL)stillclearlyoutperformsthecompetitorsintermsofaccuracy.Thusalthoughusingmoretrainingsamplesimprovesperformance,theimprovementscannotbeexplainedonlybytheamountoftrainingdata.4Table4:Imitationlearningaccuracyonthetestsets.SetCoveringCombinatorialAuctionCapacitatedFacilityLocationMaximumIndependentSetmodelacc@1acc@5acc@10acc@1acc@5acc@10acc@1acc@5acc@10acc@1acc@5acc@10TREES51.8±0.380.5±0.191.4±0.252.9±0.384.3±0.194.1±0.163.0±0.497.3±0.199.9±0.030.9±0.447.4±0.354.6±0.3SVMRANK57.6±0.284.7±0.194.0±0.157.2±0.286.9±0.295.4±0.167.8±0.198.1±0.199.9±0.048.0±0.669.3±0.278.1±0.2LMART57.4±0.284.5±0.193.8±0.157.3±0.386.9±0.295.3±0.168.0±0.298.0±0.099.9±0.048.9±0.368.9±0.477.0±0.5GCNN-SMALL57.9±1.087.1±0.695.5±0.355.0±1.688.0±0.696.2±0.169.1±0.198.2±0.099.9±0.050.1±1.273.4±0.682.8±0.6GCNN65.5±0.192.4±0.198.2±0.061.6±0.191.0±0.197.8±0.171.2±0.298.6±0.199.9±0.056.5±0.280.8±0.389.0±0.1Table5:Trainingtimeforeachmachinelearningmethod,inhours.SetCombinatorialCapacitatedMaximumCoveringAuctionFacilityLocationIndependentSetTREES0.05±0.000.03±0.000.16±0.010.04±0.00SVMRANK1.21±0.011.17±0.061.04±0.031.19±0.02LMART2.87±0.232.47±0.261.38±0.152.16±0.53GCNN14.45±1.563.84±0.3318.18±2.984.73±0.855TrainingandinferencetimesForcompleteness,wereportinTable5thetrainingtimeofeachmachinelearningmethod,andinTable6theirinferencetimepernode.Ascanbeobserved,theGCNNmodelreliesonlesscomplicatedfeaturesthantheothermethods,andthereforerequireslesscomputationaltimeduringfeatureextraction,atthecostofahigherpredictiontime.Table6:Inferencetimepernodeforeachmachinelearningmodel,inmilliseconds.WereportboththetimerequiredtoextractandcomputefeaturesfromSCIP(feat.extract),andthetotalinferencetime(total)whichincludesfeatureextractionandmodelprediction.EasyMediumHardmodeltotalfeat.extracttotalfeat.extracttotalfeat.extractTREES23.8±4.011.2±2.028.4±4.515.7±3.254.9±14.242.7±13.8SVMRANK16.6±7.16.4±4.323.1±11.19.4±5.1150.4±19.355.6±12.6LMART7.0±3.56.0±3.310.1±4.88.8±4.655.7±12.951.9±12.6GCNN5.5±13.11.1±3.55.4±4.61.5±3.010.2±12.74.9±10.2SetCoveringTREES16.1±5.15.9±1.825.3±6.110.6±4.730.2±9.615.6±7.8SVMRANK14.7±10.43.9±4.831.6±20.28.3±5.755.4±39.213.5±9.4LMART4.5±2.63.6±2.39.6±5.78.2±5.415.4±9.113.5±8.6GCNN7.2±24.31.2±5.74.4±6.11.5±4.25.1±6.32.1±5.3CombinatorialAuctionTREES53.7±2.049.1±1.199.0±6.293.4±4.6205.3±4.9199.7±4.3SVMRANK13.3±6.59.3±5.821.8±14.517.9±14.080.4±70.974.1±70.5LMART9.7±6.29.3±6.118.4±14.117.9±13.983.5±80.482.9±80.3GCNN9.2±15.33.9±3.314.8±18.77.5±1.143.0±53.719.9±13.9CapacitatedFacilityLocationTREES41.3±7.812.0±5.856.9±12.425.0±11.183.8±17.545.4±15.5SVMRANK45.5±5.66.9±5.496.5±12.819.4±11.8200.2±17.835.4±17.5LMART8.3±5.06.6±5.022.1±11.219.2±11.238.8±16.034.8±15.9GCNN5.1±7.42.3±5.17.7±11.14.8±10.012.0±17.58.1±14.8MaximumIndependentSetReferences[1]MartínAbadi,AshishAgarwal,PaulBarham,EugeneBrevdo,ZhifengChen,CraigCitro,GregS.Corrado,AndyDavis,JeffreyDean,MatthieuDevin,SanjayGhemawat,IanGoodfellow,AndrewHarp,GeoffreyIrving,MichaelIsard,YangqingJia,RafalJozefowicz,LukaszKaiser,Manjunath5Kudlur,JoshLevenberg,DandelionMané,RajatMonga,SherryMoore,DerekMurray,ChrisOlah,MikeSchuster,JonathonShlens,BenoitSteiner,IlyaSutskever,KunalTalwar,PaulTucker,VincentVanhoucke,VijayVasudevan,FernandaViégas,OriolVinyals,PeteWarden,MartinWattenberg,MartinWicke,YuanYu,andXiaoqiangZheng.TensorFlow:Large-scalemachinelearningonheterogeneoussystems,2015.Softwareavailablefromtensorﬂow.org.[2]AlejandroM.Alvarez,QuentinLouveaux,andLouisWehenkel.Amachinelearning-basedapproximationofstrongbranching.INFORMSJournalonComputing,29:185–195,2017.[3]EliasB.Khalil,PierreLeBodic,LeSong,GeorgeNemhauser,andBistraDilkina.Learningtobranchinmixedintegerprogramming.InProceedingsoftheThirtiethAAAIConferenceonArtiﬁcialIntelligence,pages724–731,2016.[4]DiederikP.KingmaandJimmyBa.Adam:Amethodforstochasticoptimization.InProceedingsoftheThirdInternationalConferenceonLearningRepresentations,2015.6