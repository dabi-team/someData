The Softwarised Network Data Zoo

Manuel Peuster
Paderborn University
manuel.peuster@uni-paderborn.de

Stefan Schneider
Paderborn University
stefan.schneider@uni-paderborn.de

Holger Karl
Paderborn University
holger.karl@uni-paderborn.de

I. INTRODUCTION

To this end, we introduce the “softwarised network data zoo”
(SNDZoo), an open collection of software networking data sets
aiming to streamline and ease machine learning research in the
software networking domain. We present a general methodology
to collect, archive, and publish those data sets for use by other
researches and, as an example, eight initial data sets, focusing
on the performance of virtualised network functions.1

The softwarisation of networks is considered as the key
enabler for more agile, automated, and autonomous net-
work management concepts, including the use of DevOps
paradigms [1]. The two key technologies to achieve this
are software-deﬁned networking (SDN) and network function
virtualisation (NFV). Both play an important role as part
of an emerging trend, called zero touch network & service
management (ZSM), which aims to remove any manual steps
from network management tasks [2].

Abstract—More and more management and orchestration ap-
proaches for (software) networks are based on machine learning
paradigms and solutions. These approaches depend not only on
their program code to operate properly, but also require enough
input data to train their internal models. However, such training
data is barely available for the software networking domain and
most presented solutions rely on their own, sometimes not even
published, data sets. This makes it hard, or even infeasible, to
reproduce and compare many of the existing solutions. As a
result, it ultimately slows down the adoption of machine learning
approaches in softwarised networks.

metrics, we are still missing a publicly available collection
of open data sets that can be used to evaluate and compare
different ML solutions and algorithms with each other. Such
open data sets are a common tool in other communities, like
image recognition [6] or natural language processing [7], and
accelerated the adoption of ML solutions in those domains.

We argue that the software networking community also
needs open data sets to simplify and streamline ML research
and to improve the reproducibility of new ideas. To this end,
we introduce the “softwarised network data zoo” (SNDZoo)
– an open repository to collect, host, and share software
networking data sets. The SNDZoo is, to the best of our
knowledge, the ﬁrst effort to build such a central repository for
softwarised network data. It speciﬁcally focuses on NFV and
SDN performance data sets, collected through performance
measurements of real-world network setups. These data sets
go beyond existing collections of open networking data sets
such as topology data sets or trafﬁc traces [8], [9].

P R E P RIN T

As in many other domains, ML recently found its way
into the information and communications technology (ICT)
sector and networking domain [3]. The use of ML is espe-
cially appealing for network management and optimisation use
cases in softwarised networks. It is speciﬁcally well suited
for NFV/SDN scenarios, which shall be highly automated
to allow zero-touch network operations following DevOps
methods [1], [2]. Existing work focuses on, e.g., learning and
predicting of service metrics, such as response time and frame
rate [12], [13], scaling and resource dimensioning [4] as well
as placement decisions [5]. But all of this work relies on
custom data sets which might not even be publicly available.
The available public data sets, such as [12], are however not
available at a common repository and thus are often hard to
ﬁnd. Our work improves this situation by offering a common
repository to host and share data sets focusing on performance
measurements of softwarised networks as well as the involved

Our contributions are two-fold. After discussing related
work in Section II, we ﬁrst introduce the methodology and
workﬂow used to collect data sets for the SNDZoo, using our
open-source NFV benchmarking platform [10], in Section III.
Second, we present eight data sets, containing millions of
performance measurements collected from different real-world
VNFs from the security, web, and 5G vertical (Internet of
things) areas, in Section IV. We make all data sets available as
part of the SNDZoo project [11] for use by other researchers.

To automate network management and to realise ZSM, more
and more machine learning (ML) and artiﬁcial intelligence
(AI)-based network management solutions arise which claim
to be able to manage and optimise different aspects of our
networks [3]. Many of them focus on automated resource
dimensioning for NFV scenarios which try to optimise the
amount of resources assigned to each involved virtual network
function (VNF). To do so, they predict the upcoming network
load as well as the performance a VNF achieves using a given
amount of resources [4], [5].

But ML-based solutions are always data-driven and do not
only depend on programming code, which is a huge difference
to legacy management and automation approaches. This means
that ML approaches can only work efﬁciently if enough data
is available to train and test the involved models, before they
are put into production. Even if data is available in some
in form of volatile monitoring
custom environments, e.g.,

1Full paper will be published in: “2019 IEEE/IFIP 15th International
Conference on Network and Service Management (CNSM), Halifax,
Canada.”

II. RELATED WORK

9
1
0
2

g
u
A
6

]
I

N
.
s
c
[

2
v
2
6
9
4
0
.
5
0
9
1
:
v
i
X
r
a

 
 
 
 
 
 
platforms and components. This also complements and goes
beyond existing collections of open network data sets mainly
focusing on network topology graphs, like [8], [9], [14].

To collect the presented data sets, automated solutions to
benchmark and proﬁle NFV and SDN scenarios, including our
own work [10], [15], can be used [16]–[19]. Those benchmark-
ing approaches have not only been proposed by academia but
have also been in the focus of standardisation bodies [20],
[21]. They are complementary to the work presented in this
paper. In fact, we make use of these solutions to collect the
initial data sets available in the SNDZoo [11].

III. METHODOLOGY & WORKFLOW

Fig. 1: Example data collection setup and workﬂow. The dotted
notes point to possible implementation options.

used as input for the framework. The framework then on-
boards and deploys the ﬁrst conﬁguration to be tested on the
NFV platform by triggering the MANO system (2). Once the
SUT and the deﬁned probes are instantiated and conﬁgured,
the framework triggers the execution of the experiment, i.e.,
activating the probes and measuring the performance achieved
by the SUT (3). When the execution is done (e.g., after a
ﬁxed time limit), the SUT and the probes are terminated and
the results are collected by the framework (4), before the
next conﬁguration is selected (5) and a new measurement
cycle is started (2). Finally, after all conﬁgurations have been
successfully executed, the framework combines all collected
results as well as the conﬁguration parameters used for each
iteration and archives them in a table-based format (6).

Collecting data sets from softwarised network scenarios is
more than performing a handful of manual measurements on
a testbed running in a lab. In fact, manual measurements
should be avoided where possible to be able to (i) quickly
and objectively reproduce the measurements, (ii) run a given
measurement in a new environment, e.g., outside the lab, and
(iii) make use of the fact that software-based networking sce-
narios can be automatically deployed, allowing to completely
remove all manual steps from the process. On top of that,
software-based networks usually offer a much higher degree
of conﬁguration freedom, e.g., in terms of virtual resources
assigned to a VNF, resulting in many different conﬁgurations
for which measurements can be and need to be performed [22].
We introduce such a fully-automated data collection
methodology in Figure 1 using an NFV scenario as example.
A data collection setup consists of two main components.
First, an NFV benchmarking framework that is responsible to
control, manage, and automate the measurement and collection
process. Second, one or multiple NFV platforms, consisting of
management and orchestration (MANO) layer and NFV infras-
tructure. They are used to deploy and execute the experiment
setups, including the deployment and execution of the system
under test (SUT). This concept is independent of the technical
realisation of the different components. It is, for example,
possible to use different benchmarking frameworks, such as
tng-bench [10], Gym [16], or NFV Inspector [19], or multiple
NFV platforms, such as vim-emu [23], [24], OSM [25], or
SONATA-NFV [26], to perform measurements in different
environments.

P R E P RIN T

Using the setup shown in Figure 1, the following end-to-
end workﬂow is used to collect a full data set, containing
measurements for many different conﬁgurations of a given
SUT. First, the measurement experiment is deﬁned using the
description approach offered by the used benchmarking frame-
work (1). This description contains, e.g., a list of different
conﬁgurations that should be tested as well as a deﬁnition
of the probes that should be used. Probes are VNFs that
can be deployed together with and connected to the SUT
and are used to stimulate the SUT during the experiments.
Those stimuli can be, e.g., synthetic trafﬁc generators or
replayed trafﬁc traces. The ﬁnal description as well as the
SUT (e.g., given as an ETSI SOL004 VNF package) is then

the presented workﬂow using our own
NFV benchmarking framework, called tng-bench, which was
initially presented in [10] and is now available as actively-
developed open-source project [27]. We use vim-meu [23] as
NFV platform to execute container-based SUTs and probes.
There are two kinds of metrics collected by tng-bench. First,
the experiment metrics that are collected from the probes as
well as the SUT at the end of an experiment, e.g., total number
of processed packets. Those metrics are captured by collecting
log ﬁles from the involved containers before termination.
Second, we collect time series metrics using the Prometheus
time series database [28] controlled by tng-bench. Prometheus
periodically fetches all metrics, including the resource usage
of the involved containers (using cAdvisor [29]) as well as
SUT-speciﬁc metrics as we detail in the following section.

We collected eight initial data sets, focusing on the per-
formance of real-world VNFs under different conﬁgurations
to kick-off the SNDZoo project and to test the presented

IV. COLLECTING, ARCHIVING, AND PUBLISHING
THE FIRST DATA SETS

We implement

InputsNFV Benchmarking FrameworkConﬁguration ExplorerBenchmarkingControllerExp.Desc.SUT packageC1C2Cn...Platf. DrivercontrolmonitorOutputsDataExp.Param.1. Deﬁne6. Archive4. Collect2. On-board & Deploy5. Iterate  tng-bench (or Gym, NFV Inspector, etc.) NFV Platform NFV PlatformNFV PlatformMANOCiNFV InfrastructureProbeSUTProbecontrolDocker container (or VM)3. Execute & MeasurePrometheusvim-emu (or OpenStack)vim-emu (or OSM, SONATA-NFV, etc.)on-boardconﬁgurations, versions, and workloads are published along
with the data sets [11].

B. Data Collection

methodology, workﬂow, and tools. All initial data sets are
automatically collected and can be reproduced using our
benchmarking tool. We want to highlight that the scope of
the SNDZoo is not limited to performance data sets of single
VNFs. We plan to extend our efforts and also include measure-
ments of more complex network services, MANO performance
numbers, impact of different management approaches to the
performance of real world NFV systems, or even long-term
monitoring data into the project.

To collect

A. Experiment Setup

the data sets, we picked eight VNFs from
three different categories as shown in Table I: Security (IDS
systems), web (load balancers, proxies), and IoT (MQTT
brokers). This way, we not only have VNFs that transparently
forward the trafﬁc while passively analysing it (IDS systems),
but also active VNFs that can modify the trafﬁc (proxies). We
also have scenarios (MQTT broker) that can be considered
as examples for 5G vertical use cases, such as IoT, smart
manufacturing, or industry 4.0 [30].

In the ﬁrst category (SEC01 - SEC03), we benchmark three
IDS VNFs, namely Suricata 4.0 [31], Snort 2.9 [32], and
Snort 3.0 [32]. Each IDS is conﬁgured as transparent layer 2
bridge and passively monitors the incoming trafﬁc. To stim-
ulate the VNFs we use two publicly available trafﬁc traces
with small and big ﬂows that are continuously replayed at
maximum speed [33]. During the benchmarking experiment,
the VNFs are conﬁgured with different IDS rule sets, taken
from [34], and with different resource assignments, i.e., CPU
time (10 % to 100 %) and memory (256 MB and 1024 MB). As
a result, 80 different conﬁgurations are tested2 and each con-
ﬁguration is repeated 20 times, resulting in 1,600 experiment
runs, each testing a single conﬁguration.

To collect the presented data sets we used the setup pre-
sented in Section III using vim-emu [23] as NFV platform,
which is executed on a machine with Intel(R) Xeon(R) W-
2145 CPU at 3.70 GHz CPU, 32 GB of memory, running Linux
4.4.0-142-generic3. Vim-emu allows to deploy and control
NFV scenarios on a single physical machine using Docker
containers. In our experiments, the tested VNFs as well as
the probes used to stimulate them are deployed as Docker
containers, each of them always pinned to a single physical
CPU core to achieve isolation between VNFs and probes [10].
We collect a large number of different experiment metrics
for each tested conﬁguration as well as a large number of
time series metrics during experiment execution. More specif-
ically, we collect 268 to 281 experiment metrics after each
experiment and between 43 and 593 time series metrics during
each experiment. This results in up to 474,400 collected time
series records in data set SEC034, as shown in Table I. Each
of these records contains about 30 data points resulting from
a collection frequency of 0.5 Hz and an experiment runtime
of 60 s per conﬁguration. Those numbers highly depend on
the involved VNFs and the number of metrics they expose.
The used Snort 3.0 VNF, for example, exposes more than 400
metrics that can be collected, e.g., packet counters for different
protocol types.

P R E P RIN T

The use of tng-bench as experiment automation framework
allows to reproduce all presented experiments. To do so,
nothing more is required than two Linux machines on which
tng-bench and vim-emu are installed. All involved VNFs can
be downloaded from the SNDZoo repositories as pre-deﬁned
Docker containers [11] and used to re-run the experiments.
However, the absolute performance numbers in those data sets
obviously depend on the underlying hardware and will differ
for new measurements performed in different environments.
But, generic aspects like trends that can be identiﬁed between
different VNF conﬁgurations will still be visible [15].

The presented data sets contain between 2.5 and 14.5
million data points and are, for example, usable as training
and testing data sets for different kinds of prediction or
optimisation algorithms in the NFV domain. The collection
process of each data set took between 19.2 h and 79.8 h, as
shown in Table I. Even though the SEC03 data set has the
shortest runtime, it contains the most data points. The reason
for this is that the tested VNF, Snort 3.0, exposes more VNF-
speciﬁc metrics than any other tested VNF. The measurements
to collect IOT02 took more time than the others because the

In the third category (IOT01 - IOT02), the MQTT brokers
Mosquitto 1.6.2 [39] and Emqx 3.1.0 [40] are tested using
Malaria [41], an MQTT load generator. The broker is placed
between two probes running Malaria instances, one acting as
publisher and the other acting as subscriber, allowing us to
measure the end-to-end delay of MQTT messages. Besides
different resource assignments for the broker VNF, Malaria
is executed with different conﬁgurations, e.g., message sizes
between 10 and 1000 bytes as well as two different MQTT
QoS levels (1 and 2). Please note that the full details of all used

represents web
scenarios in which we test an Nginx 1.10.3 [35] and
a HAProxy 1.6.3 [36] load balancer VNF as well as a
Squid 3.5.12 [37] proxy. The VNFs are placed between a
source probe (user requests generated by Apache Bench [38])
and a target probe (web server running Apache 2.0 [38]). As
shown in Table I, 80 different conﬁgurations are executed,
including small and large requests and different resource
assignments, i.e., CPU time (10 % to 100 %) and memory
(64 MB, 128 MB, 256 MB, and 512 MB).

2There are only 40 conﬁgurations in the case of Snort 3.0 because of the

3The full hardware and software speciﬁcations of the used testbed are

4Considering conﬁgurations, repetitions, and collected metrics results in:

The second category (WEB01 - WEB03)

available as part of the SNDZoo repositories.

C. Resulting Data Sets

smaller rule set available for this version of Snort.

40·20·593=474,400 time series records.

TABLE I: Overview of the eight VNF benchmarking data sets initially published in the SNDZoo [11]

Name

Category

Class

VNF

Probe/Stimuli

Security

IDS Systems

SEC01
SEC02
SEC03

Suricata [31]
Snort 2.9 [32]
Snort 3.0 [32]

Traces [33]
Traces [33]
Traces [33]

Tested
Conﬁgu-
rations

80
80
40

Repetitions Experiment

Metrics

20
20
20

280
280
281

Time
Series
Metrics

Total
Exp.
Runtime

157
169
593

38.1 h
37.6 h
19.2 h

Total
data
points

7.9M
8.6M
14.5M

20
20
20

43
43
43

268
268
268

Load balancers

2.5M
2.5M
2.5M

38.3 h
39.4 h
39.9 h

WEB01 Web
WEB02
WEB03

Nginx [35]
HAProxy [36]
Squid [37]

AB/Apache [38]
AB/Apache [38]
AB/Apache [38]

80
80
80

IoT

20
20

80
80

90
90

Proxys

275
275

4.7M
4.7M

40.0 h
79.8 h

IOT01
IOT02

MQTT Broker Mosquitto [39] Malaria [41]
Malaria [41]
Emqx [40]

used VNF (Emqx) takes much longer to start up and to be
ready to process trafﬁc.

Figure 2 visualises a small subset of our data sets to give
the reader a brief example of the published data and to show
how researchers can make use of it. The ﬁgure shows the
comparison of two metrics (requests/s and request times) of
the WEB01-WEB03 data sets for different CPU and memory
conﬁgurations of the three SUTs (nginx, haproxy, and squid).
Each conﬁguration is represented by 20 points (measurement
repetitions). The lines indicate polynomial regressions (de-
gree=3) to demonstrate how the data can be used to train
performance models mapping different SUT conﬁgurations to
performance metrics. Those models could, e.g., be used for
scaling and placement optimisation algorithms [42].

(DVC) [43]. DVC focuses speciﬁcally on versioning ML data
sets and allows to store and version large ﬁles on external
storage solutions such as Amazon S3 while referencing them
from a Git repository. Users can then access and download
a speciﬁc data set by simply running two commands (git
clone and dvc pull) on their machine.

Each data set hosted in the SDNZoo is located in its
own Git/DVC repository with a connected Amazon S3 bucket
holding the data ﬁles (a total of 3.0 GB). Those repositories
not only contain the conﬁgurations and resulting data sets but
also meta data like the hardware and software speciﬁcations
of the machines on which the measurements have been per-
formed. Further, they contain license information as well as
all raw measurements, time series, and logs produced by tng-
bench. Besides the data set repositories, SNDZoo also provides
repositories containing the sources and descriptions of used
VNFs and services. All published data sets are indexed on
and linked from the SNDZoo website [11]. They are published
under creative commons CC-BY-SA 4.0 license.

P R E P RIN T

We plan to continue our efforts and add new data sets over
time. Our broader vision for the SNDZoo is to have a large
collection of different data sets for a wide variety of use cases,
scenarios, and deployments. Not only focusing on NFV and
SDN performance measurements but also on measurements
of MANO system and management performance. However,
this project will depend on community contributions and we
invite all community members to participate and use SNDZoo
as a platform to host and share their data sets. Contributors
are free to choose how they collect their data sets as long as
they ensure that the data sets come with enough information
such that the measurements can be reproduced in a fully
automated fashion. More details about the contribution process
are available online [11].

We introduced the SNDZoo project as the ﬁrst open col-
lection of NFV/SDN performance data sets. The SNDZoo
project aims to support the adoption of ML/AI in the software
networking community by enabling researchers to work with
common data sets simplifying comparisons and reproduction
of results. Along with this paper, we publish eight initial data
sets, containing performance measurements collected from a
set of real-world security, web, and IoT VNFs.

We version all experiment conﬁgurations that are used to
collect the presented data sets using Git so that we can repro-
duce old experiments even if they are updated or improved
over time. This also allows us to use GitHub as hosting
platform to publish and share the contents of the SNDZoo.
However, keeping large data sets, containing multiple ﬁles
with gigabytes of data, within a Git repository is bad practice
and results in poor performance. To solve this, we make use
of a recently introduced project called Data Version Control

Fig. 2: Example data set visualisations

D. Publishing the Data Sets

V. CONCLUSION

0.000.250.500.751.00CPU bandwidth (%)025005000750010000requests/smem = 64nginxhaproxysquid0.000.250.500.751.00CPU bandwidth (%)mem = 1280.00.20.40.60.81.0CPU bandwidth (%)0.000.250.500.751.00request times (ms)mem = 64nginxhaproxysquid0.00.20.40.60.81.0CPU bandwidth (%)mem = 128ACKNOWLEDGMENTS
This work has received funding from the European Union’s Horizon 2020 research
and innovation programme under grant agreement No. H2020-ICT-2016-2 761493
(5GTANGO), and the German Research Foundation (DFG) within the Collaborative
Research Centre “On-The-Fly Computing” (SFB 901).

REFERENCES

York, NY, USA: ACM, 2018, pp. 14:1–14:13. [Online]. Available:
http://doi.acm.org/10.1145/3185467.3185495

[19] M. G. Khan, S. Bastani, J. Taheri, A. Kassler, and S. Deng, “Nfv-
inspector: A systematic approach to proﬁle and analyze virtual network
functions,” in 2018 IEEE 7th International Conference on Cloud Net-
working (CloudNet).

IEEE, 2018, pp. 1–7.
[20] R. V. Rosa, C. E. Rothenberg, M. Peuster, and H. Karl, “Methodology
for VNF Benchmarking Automation,” IETF, Internet-Draft draft-rosa-
bmwg-vnfbench-02, Jul. 2018, work in Progress. [Online]. Available:
https://datatracker.ietf.org/doc/html/draft-rosa-bmwg-vnfbench-02

[1] H. Karl, S. Dr¨axler, M. Peuster, A. Galis, M. Bredel, A. Ramos, J. Mar-
trat, M. S. Siddiqui, S. van Rossem, W. Tavernier et al., “DevOps for
network function virtualisation: an architectural approach,” Transactions
on Emerging Telecommunications Technologies, vol. 27, no. 9, pp. 1206–
1215, 2016.

[9] S. Knight, H. X. Nguyen, N. Falkner, R. Bowden, and M. Roughan,
“The internet topology zoo,” IEEE Journal on Selected Areas in Com-
munications, vol. 29, no. 9, pp. 1765–1775, 2011.

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in 2009 IEEE conference on
computer vision and pattern recognition.

[3] M. Wang, Y. Cui, X. Wang, S. Xiao, and J. Jiang, “Machine learning
for networking: Workﬂow, advances and opportunities,” IEEE Network,
vol. 32, no. 2, pp. 92–99, March 2018.

[5] J. Sun, G. Huang, G. Sun, H. Yu, A. K. Sangaiah, and V. Chang,
“A q-learning-based approach for deploying dynamic service function
chains,” Symmetry, vol. 10, no. 11, 2018.
[Online]. Available:
http://www.mdpi.com/2073-8994/10/11/646

[2] J. Miao, G. He, X. Pei, K. Martiny, M. Klotz, A. Khan, G. K. nad
Ryosuke Kurebayashi, Y. Goto, S. Manning, F. Weaver, and D. Lopez.
(2017, Dec.) Zero-touch Network and Service Management. [Online].
Available: https://portal.etsi.org/TBSiteMap/ZSM/OperatorWhitePaper

[4] R. Mijumbi, S. Hasija, S. Davy, A. Davy, B. Jennings, and R. Boutaba,
“Topology-aware prediction of virtual network function resource re-
quirements,” IEEE Transactions on Network and Service Management,
vol. 14, no. 1, pp. 106–120, March 2017.

[7] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts,
“Learning word vectors for sentiment analysis,” in Proceedings of the
49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies. Portland, Oregon, USA: Association
for Computational Linguistics, June 2011, pp. 142–150.

[8] S. Orlowski, M. Pi´oro, A. Tomaszewski, and R. Wess¨aly, “SNDlib
1.0–Survivable Network Design Library,” in Proceedings of the 3rd
International Network Optimization Conference (INOC 2007), Spa,
Belgium, April 2007.

[Online].

[21] ETSI.

consortium.

consortium.

for NFVI.

[29] Google Inc.

[26] SONATA project

(2015) SONATA-NFV.

Ieee, 2009, pp. 248–255.

(2019) Google cAdvisor.

(2018) 5GTANGO SDK:

Available: http://sonata-nfv.eu

Available: https://prometheus.io/

[28] Linux Foundation. (2019) Prometheus Time Series Database. [Online].

[27] 5GTANGO project
sdk-benchmark.
tng-sdk-benchmark

tng-
[Online]. Available: https://github.com/sonata-nfv/

[23] M. Peuster, H. Karl, and S. van Rossem, “MeDICINE: Rapid Prototyp-
ing of Production-ready Network Services in Multi-PoP Environments,”
in 2016 IEEE Conference on Network Function Virtualization and
Software Deﬁned Networks (NFV-SDN), 2016, pp. 148–153.

[24] ETSI OSM, “OSM vim-emu documentation,” https://goo.gl/XZNLVw.
[25] ——. (2016) Open Source MANO: Open Source NFV Management
and Orchestration (MANO) software stack aligned with ETSI NFV.
[Online]. Available: https://osm.etsi.org

(2018) TST009v3.1.1: Network Functions Virtualisation
(NFV) Release 3; Testing; Speciﬁcation of Networking Benchmarks
and Measurement Methods
[Online]. Available:
https://docbox.etsi.org/ISG/NFV/Open/Publications pdf/Specs-Reports/
NFV-TST%20009v3.1.1%20-%20GS%20-%20NFVI Benchmarks.pdf
[22] M. Peuster and H. Karl, “Understand Your Chains and Keep Your
Deadlines: Introducing Time-constrained Proﬁling for NFV,” in 2018
14th International Conference on Network and Service Management
(CNSM), Nov 2018, pp. 240–246.

P R E P RIN T

[35] NGINX Inc. (2019) NGINX: High Performance Load Balancer, Web
Server, Reverse Proxy. [Online]. Available: https://www.nginx.com/
[36] HAProxy Project. (2019) HAProxy: The Reliable, High Performance
TCP/HTTP Load Balancer. [Online]. Available: http://www.haproxy.org/
[Online].

[42] S. Dr¨axler, H. Karl, and Z. ´A. Mann, “Jasper: Joint optimization of
scaling, placement, and routing of virtual network services,” IEEE
Transactions on Network and Service Management, vol. 15, no. 3, pp.
946–960, Sep. 2018.

[30] S. Schneider, M. Peuster, D. Behn, M. M¨uller, P.-B. B¨ok, and H. Karl,
“Putting 5G into Production: Realizing a Smart Manufacturing Verti-
cal Scenario,” in 2019 IEEE European Conference on Networks and
Communications (EuCNC), 2019.

[40] EMQ Technologies Co., Ltd. (2019) EMQ: Scalable and Realtime
MQTT Messaging for IoT in 5G Era. [Online]. Available: https:
//www.emqx.io/

[32] Cisco. (2016) Snort IDS/IPS. [Online]. Available: http://www.snort.org
[33] F. Klassen and AppNeta. (2019) Tcpreplay: Sample Captures. [Online].

[43] DVC Project. (2019) DVC: Open-source Version Control System for
Machine Learning Projects. [Online]. Available: https://dvc.org/

(2019) Malaria: Attacking MQTT systems
https://github.com/etactica/
[Online]. Available:

[38] The Apache Software Foundation. (2019) Apache Project. [Online].

[39] Eclipse Foundation. (2019) Eclipse Mosquitto: An open source MQTT

[31] OISFoundation. (2019) Suricata: Open Source IDS / IPS / NSM engine.

[34] ET Labs. (2019) Emerging Threats Open Ruleset. [Online]. Available:

Project.
with Mosquittos.
mqtt-malaria

Available: http://tcpreplay.appneta.com/wiki/captures.html

broker. [Online]. Available: https://mosquitto.org/

(2019) Squid: Optimising Web Delivery.

[Online]. Available: https://suricata-ids.org/

Available: http://www.squid-cache.org

Available: https://httpd.apache.org/

https://rules.emergingthreats.net/

//github.com/google/cadvisor

[Online]. Available: https:

IEEE, 2015, pp. 93–99.

[37] Squid Project.

[41] Malaria

IEEE, 2016.

[13] F. S. Samani and R. Stadler, “Predicting distributions of service metrics
using neural networks,” in 2018 14th International Conference on
Network and Service Management (CNSM), Nov 2018, pp. 45–53.
[14] J. Leskovec and A. Krevl. (2014, Jun.) SNAP Datasets: Stanford large
network dataset collection. [Online]. Available: http://snap.stanford.edu/
data

[16] R. V. Rosa, C. Bertoldo, and C. E. Rothenberg, “Take your vnf to the
gym: A testing framework for automated nfv performance benchmark-
ing,” IEEE Communications Magazine, vol. 55, no. 9, pp. 110–117,
2017.

[17] L. Cao, P. Sharma, S. Fahmy, and V. Saxena, “NFV-VITAL: A Frame-
work for Characterizing the Performance of Virtual Network Functions,”
in Network Function Virtualization and Software Deﬁned Network (NFV-
SDN), 2015 IEEE Conference on.

[12] R. Stadler, R. Pasquini, and V. Fodor, “Learning from network
device statistics,” Journal of Network and Systems Management,
vol. 25, no. 4, pp. 672–698, Oct 2017.
[Online]. Available:
https://doi.org/10.1007/s10922-017-9426-z

[10] M. Peuster and H. Karl, “Proﬁle Your Chains, Not Functions: Automated
Network Service Proﬁling in DevOps Environments,” in 2017 IEEE
Conference on Network Function Virtualization and Software Deﬁned
Networks (NFV-SDN), 2017.

[18] J. Nam, J. Seo, and S. Shin, “Probius: Automated approach for vnf
and service chain analysis in software-deﬁned nfv,” in Proceedings
of
New

the Symposium on SDN Research,

ser. SOSR ’18.

[15] M. Peuster and H. Karl, “Understand Your Chains: Towards Performance
Proﬁle-based Network Service Management,” in 5th European Workshop
on Software Deﬁned Networks (EWSDN’16).

[11] M. Peuster, S. Schneider, and H. Karl. (2019, Apr.) Software Network

Data Zoo. [Online]. Available: https://sndzoo.github.io

