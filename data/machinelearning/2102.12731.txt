2
2
0
2

r
a

M
3
2

]

G
L
.
s
c
[

2
v
1
3
7
2
1
.
2
0
1
2
:
v
i
X
r
a

Improving Approximate Optimal Transport Distances using Quantization

Gaspard Beugnot *1, 2

Aude Genevay1

Kristjan Greenwald3

Justin Solomon1

1MIT CSAIL
2INRIA
3IBM Watson AI Lab

Abstract

Optimal transport (OT) is a popular tool in ma-
chine learning to compare probability measures ge-
ometrically, but it comes with substantial computa-
tional burden. Linear programming algorithms for
computing OT distances scale cubically in the size
of the input, making OT impractical in the large-
sample regime. We introduce a practical algorithm,
which relies on a quantization step, to estimate
OT distances between measures given cheap sam-
ple access. We also provide a variant of our algo-
rithm to improve the performance of approximate
solvers, focusing on those for entropy-regularized
transport. We give theoretical guarantees on the
beneﬁts of this quantization step and display ex-
periments showing that it behaves well in practice,
providing a practical approximation algorithm that
can be used as a drop-in replacement for existing
OT estimators.

Optimal transport (OT) is a versatile component of the prob-
abilistic toolbox for machine learning. As an alternative
to conventional divergences between probability measures,
OT provides a means of measuring how distributions align
geometrically. OT has found application in parameter es-
timation [Bernton et al., 2019], robust learning [Esfahani
and Kuhn, 2018], and generative modeling [Salimans et al.,
2018, Genevay et al., 2018]—among other learning tasks.

When distributions are absolutely continuous or composed
of huge numbers of points, it becomes infeasible to compute
OT distances exactly. In this setting, a common approxima-
tion follows two steps: First, we draw k samples from both
distributions, and then we use linear programming to extract
the distance between empirical distributions. This plug-in
procedure produces a convergent approximation as k → ∞

*This work was conducted in large part during an internship at

MIT.

(by the Glivenko–Cantelli theorem, since the Wasserstein
distance metrizes weak convergence [Villani, 2003]), but
two challenges conspire to limit its scalability:

• Sample complexity bounds and related results show that
this approximation converges with rate k−1/d, where d is
the ambient dimension [Dudley, 1969, Weed and Bach,
2019]. These sharp asymptotic rates exhibit a curse of
dimensionality: we need a large number k of samples
(growing exponentially with d) before the approximation
is useful.

• The computational complexity of solving the linear pro-
gram is roughly cubic in k [Burkard et al., 2012], limiting
the maximum k we can take before this method becomes
unreasonably slow.

Together, these facts imply that the largest k for which solv-
ing the linear program is feasible may not be sufﬁcient for
extracting a usable distance estimate, i.e., the bottleneck
is not availability of samples/data (the classic statistical
setting), but computation budget.

Our work is motivated by a simple observation about the
methodology above. In machine learning, it is often straight-
forward to sample from the input measures for OT, e.g.
when they come from large datasets, generative models, or
easily-sampled smooth distributions. In this case, limited
approximation quality is a byproduct of the cubic com-
putational expense rather than a paucity of samples. The
algorithm above only draws O(k) samples—but it could
draw more without affecting the asymptotic runtime. That
is, we can improve approximation quality with little added
computational expense by drawing more than k samples,
cutting down to k representative (weighted) samples, and
then solving a smaller discrete problem.

We introduce a practical, easily-implemented improvement
to empirical OT. In our algorithm, the OT solver remains
either the linear program solver or the recently-popular reg-
ularized Sinkhorn algorithm [Cuturi, 2013]. As input to this
step, however, we “summarize” a superlinear number of
samples with k weighted samples through quantization. Our

Accepted for the 37th Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2021).

 
 
 
 
 
 
technique is seamless to implement given an implementation
of empirical OT and substantially improves approximation
quality given ﬁxed computational cost. It can be used as a
drop-in replacement for existing estimators. Beyond verify-
ing performance empirically, we provide theory predicting
the behavior we observe, in the low quantization error set-
ting. While it is impossible to overcome the asymptotic
curse of dimensionality associated to all ﬁnitely-supported
measures [Kloeckner, 2012], our method leverages better
convergence rates in the ﬁnite sample regime for “cluster-
able” distributions [Weed and Bach, 2019]. This leads to
substantial practical beneﬁt, with an improvement of the
exponent of the convergence rate by a factor 2 in the best
case (fast decaying tails) or at worst on par with the plug-in
estimator (close to uniform).

Related work. OT suffers from a severe curse of dimension-
ality. Effective approximation requires an exponential num-
ber of samples n in the ambient dimension. For an absolutely
continuous measure µ (w.r.t. Lebesgue), its Wasserstein dis-
tance to any measure supported on n points is asymptotically
lower-bounded by O(n−1/d) [Dudley, 1969]. This bound
can sometimes be circumvented, e.g., when the measures
have lower intrinsic dimension [Weed and Bach, 2019] or
when the support is discrete (convergence rate O((cid:112)1/n),
with constant depending on dimension) [Sommerfeld et al.,
2018]. To counter this curse of dimensionality, the best-
known workaround relies on entropic regularization, with
O((cid:112)1/n) convergence [Genevay et al., 2019]. Another es-
timator penalizes the rank of the transport plan [Forrow
et al., 2019], while [Goldfeld and Greenewald, 2020] pro-
poses a smoothed distance by convolving measures with
Gaussians. While these exhibit better convergence rates,
they only approximate the Wasserstein distance and do not
converge to its true value. The curse of dimensionality can
sometimes be mitigated for standard OT—[Weed and Bach,
2019] proves that for mixtures of Gaussians and clusterable
distributions, the p-th power of the p-Wasserstein distance
enjoys a O((cid:112)1/n) rate for small n—implying a O(n−1/4) rate
for W2.

While the curse of dimensionality requires many samples
to approximate transport reliably, in practice computational
complexity prevents us from doing so. OT between discrete
measures yields a large-scale linear program solvable using
network ﬂow solvers or the Hungarian algorithm, when both
measures have the same size and uniform weights [Burkard
et al., 2012]. These take O(n3 log n) time, where n is the
support size. As a faster alternative, entropy-regularized OT
can be solved with quadratic complexity using Sinkhorn’s
algorithm [Sinkhorn, 1967], but its convergence rate de-
cays when regularization goes to zero [Franklin and Lorenz,
1989].

For efﬁcient OT approximation, we oversample the input
measures and compute a summary via a quantization algo-
rithm like k-means; note quantization is equivalent to ﬁnding

the closest measure supported on k points in 2-Wasserstein
distance [Pollard, 1982, Canas and Rosasco, 2012]. The orig-
inal k-means algorithm [Lloyd, 1982] is prohibitive for large
sample sizes and often reaches local minima. With a care-
ful initialization, however, [Arthur and Vassilvitskii, 2006]
proved that k-means likely converges to near its global opti-
mum. This initialization, called k-means++, is obtained via
D2 sampling and is O(log k)-close to optimal in expectation.
This yields a cheap approximation in O(nk) time, since the
algorithm requires k passes through the data. Later variants
have lower computational complexity, among which [Bah-
mani et al., 2012] performs only a ﬁxed number of passes
on the data and [Bachem et al., 2016] uses an MCMC D2
sampler. These beneﬁt from bounds similar to k-means++
but have O(n) computational complexity.

Our approach has similarities with a line of work that uses a
multi-scale scheme to compute optimal transport efﬁciently
[Schmitzer and Schnörr, 2013, Gerber and Maggioni, 2017].
However, they focus on accelerating the exact computation
of optimal transport, while we target a fast approximation.
These multi-scale approaches also do not leverage a con-
nection between k-means and optimal transport to yield
quantitative analysis, and they are not applicable to entropy-
regularized transport.

Contributions. We propose efﬁcient OT estimators using
quantization, with theoretical analysis for two classes of OT
problems:

• (Unregularized) OT: We leverage the link between OT
and k-means [Pollard, 1982, Canas and Rosasco, 2012]
to quantify the bias and give precise bounds for Gaus-
sian mixtures and clusterable distributions in the non-
asymptotic regime.

• Entropy-regularized OT: Building on complexity re-
sults for Sinkhorn [Altschuler et al., 2017], we prove
that our pre-processing can yield ε-approximate OT with
better time/space complexity.

We compare our estimators to the plug-in estimator on toy
and real-world datasets.

Notation. Let µ and ν be probability measures on a com-
pact set X ⊆ Rd. The 2-Wasserstein distance between µ
and ν is

W2(µ, ν) def.=

(cid:18)

min
π∈Π(µ,ν)

(cid:90)

(cid:107)x − y(cid:107)2

2 dπ(x, y)

X ×X

(cid:19)1/2
,

(1)

where Π(µ, ν) is the set of couplings on X × X with
def.=
marginals µ, ν. Given n samples from each measure, Xn
def.= (y1, . . . , yn) ∼ ν ⊗n, the empir-
(x1, . . . , xn) ∼ µ ⊗n and Yn
ical plug-in estimator for W2 is

W2( ˆαn, ˆβn) =




 min
π1=1/n
πT 1=1/n

1
n2

n
∑
i, j=1



1/2

(cid:107)xi − y j(cid:107)2

2πi j




,

(2)

i=1 δxi and ˆβn
n ∑n
where ˆαn
measures from µ and ν, resp.

def.= 1

def.= 1

n ∑n

i=1 δyi are empirical

1 ALGORITHM OVERVIEW

We aim to improve the plug-in estimator W2( ˆµk, ˆνk), which
approximates W2(µ, ν) with O(k3 log k) computational com-
plexity (that of LP solvers) and O(k−α ) bias, given k sam-
ples from each measure. In the worst case (e.g., uniform
distributions), α = 1/d, but there exist regimes in which the
rate improves (see §2.2). Our idea is to oversample the mea-
sures, using n > k samples to construct approximations of µ
and ν of size k that yield an estimated OT value with better
bias while preserving computational complexity. To satisfy
these criteria, we need to ensure that pre-processing takes
O(k3 log k) time.
We denote by ˆSk(Xn) a stochastic map that inputs a sample
Xn = (x1, . . . , xn) ∼ µ ⊗n and outputs a k-point quantization.
For any ﬁnite S ⊆ X , use the function PS : X → S to de-
note the function that maps any point in X to its nearest
neighbor in S. Denoting by ˆµn (resp., ˆνn) the empirical mea-
sure associated to the n-sample Xn (resp., Yn) and f#(µ) the
pushforward of µ through f , our estimator is deﬁned as:
Est(k, n) def.= W2(PˆSk(Xn)#( ˆµn), PˆSk(Yn)#( ˆνn)).
That is, we replace ˆµk, ˆνk in the plug-in estimator (2) with
weighted k-point measures PˆSk(Xn)#( ˆµn) and PˆSk(Yn)#( ˆνn), the
centers of approximate k-means on Xn and Yn, resp. Each
center is weighted proportionally to the number of samples
in its Voronoi region. The plug-in estimator (2) corresponds
to n = k.

(3)

There are two steps in our pre-processing: (i) selecting k
points representative of the larger n samples and (ii) weight-
ing the resulting k points with the number of samples in
their Voronoi regions. For k-means++, (i) is O(nk) while
for [Bachem et al., 2016, Bahmani et al., 2012] it is O(k).
Regardless, the assignment in step (ii) requires O(nk) time.
To be consistent with the O(k3 log k) time complexity of the
OT solver, we thus set n = k2 log k.

Algorithm 1 summarizes our estimator. It takes four steps:
(1) sample k2 log k points from each measure, (2) run k-
means++ initialization, (3) project the k2 log k points onto
the k cluster centers, and (4) compute OT between these
new weighted point clouds. Steps (1) and (3) are seamless
to implement, while steps (2) and (4) have readily available
implementations in many languages, as they come from
well-known algorithms. Thus, the procedure is highly prac-
tical, and it can easily be implemented to improve the bias
of OT estimation with similar running times.

Algorithm 1: Approximation of W2(µ, ν)
Input :Two samplers µ, ν; number of anchor points k.
Output :Approximation of W2(µ, ν) with complexity

O(k3 log k)
/* Sample n points
Set n = k2 log k
Sample Xn = (x1, . . . , xn) i.i.d. from µ and Yn = (y1, . . . , yn)
i.i.d. from ν

*/

/* Subsample k anchor points
Compute ˆSk(Xn) = (c1, . . . , ck) with k-means++
Compute ˆSk(Yn) = (d1, . . . , dk) with k-means++
/* Compute weights
Set ai = ∑n
Set bi = ∑n
/* Cost matrix
Set Ci j = (cid:107)ci − d j(cid:107)2
/* Weighted Wasserstein distance
ˆνn) def.= LC(a, b)1/2
ˆµn, PˆSk(Yn)#
return W2(PˆSk(Xn)#

j=1 1i=arg minl (cid:107)x j−cl (cid:107)2
j=1 1i=arg minl (cid:107)x j−dl (cid:107)2

∀i ∈ {1, . . . , k}
∀i ∈ {1, . . . , k}

2 ∀i, j ∈ {1, . . . , n}

2

2

*/

*/

*/

*/

time.1 The estimator has O(k−2α ) bias in the best case and
O(k−α ) at worst, where the latter is the bias of the empirical
plug-in estimator.

Remark 1. The “best case” happens in the ﬁnite sample
regime, when distributions have low quantization error as
deﬁned in §2.2. For near-uniform distributions, we get the
asymptotic rate right away and cannot hope to improve on
the plug-in estimator.

This theorem predicts the performance observed in §4. In
short, with the same computational complexity, we improve
the bias by an exponent of 2 compared to the plug-in estima-
tor. Time complexity is a direct addition of pre-processing
and LP solver complexities. The bias bounds, on the other
hand, require more work and are the object of the next sec-
tion.

2 THEORETICAL ANALYSIS

2.1 BOUNDING BIAS

The bias of our estimator Est(k, n) deﬁned in (3) is

Bias(k, n) = |W2(µ, ν) − E [Est(k, n)]| .

(4)

By the triangle inequality on |·| and W2, we have that

Bias(k, n) ≤ E

Xn, ˆSk

+ E

Yn, ˆSk

(cid:105)
(cid:104)
W2(µ, PˆSk(Xn)#( ˆµn))
(cid:105)
(cid:104)
W2(ν, PˆSk(Yn)#( ˆνn))

,

The performance of our approach is summarized informally
in the theorem below; we bound bias in §2.

Theorem 1 (Informal). Algorithm 1 runs in O(k3 log k)

1This complexity assumes sampling is cheap, i.e., O(1). If
drawing samples requires complex operations, the number of
points we can sample will be below n = k2 log k; it is straight-
forward to adapt to this case.

so bounding bias amounts to controlling the two terms above.
This requires some deﬁnitions:

Deﬁnition 1 (Quantization error φS(C)). Let C ⊆ X be a
ﬁnite set of n elements. For any S ⊆ X , deﬁne the quantiza-
tion error of C w.r.t. S as

φS(C) = ∑
x∈C

d(x, S)2,

where d(x, S) = mins∈S d(x, s). For k ≤ n, denote by φ OPT
the optimal quantization error for a set of k elements, written
φ OPT
(C) = minS⊆X ,|S|=k φS(C), and Sk its minimizer.
k

k

We can relate the bias of our estimator to sample complexity
and quantization error as follows:

Lemma 1]. The second is the k-means++ optimality bound
of [Arthur and Vassilvitskii, 2006]. Jensen’s inequality com-
pletes the proof. Note having the optimal set Sk instead of
ˆSk would remove the log k factor in (6).

In our algorithm, we take n = k2 log k to get the following
bias for our estimator:

Corollary 1. In the setting of Theorem 2, with n = k2 log k,
our estimator (Algorithm 1) satisﬁes

E

Xn, ˆSk

(cid:105)
(cid:104)
W2(µ, PˆSk(Xn)#( ˆµn))

≤

(cid:16)
(k2 log k)−α + 1/k EXn

O

(cid:2)φ OPT
k

(Xn)(cid:3)1/2(cid:17)

.

(Bias

Suppose
Theorem 2
E [W2(µ, ˆµn)] ≤ O(n−α ), where α is the sample com-
plexity rate of µ. Then, for a sample Xn ∼ µ ⊗n,

estimator).

the

of

E

Xn, ˆSk

(cid:105)
(cid:104)
W2(µ, PˆSk(Xn)# ˆµn)

≤

(cid:16)
n−α + (cid:112)(log k)/n EXn

O

(cid:2)φ OPT
k

(Xn)(cid:3)1/2(cid:17)

.

The sample complexity here is not necessarily the asymp-
totic rate α = 1/d. Rather, we will see in §2.2 that our estima-
tor performs well in the ﬁnite sample regime for clusterable
distributions, with rate α = 1/4.

Proof. By the triangle inequality on W2, we can decompose
into two quantities A and B:

E

Xn, ˆSk
+ E
(cid:124)

(cid:105)
(cid:104)
W2(µ, PˆSk(Xn)# ˆµn)
W2( ˆµn, PˆSk(Xn)# ˆµn)
(cid:125)

≤

Xn, ˆSk

(cid:123)(cid:122)
B

.

(5)

EXn W2(µ, ˆµn)
(cid:123)(cid:122)
(cid:125)
(cid:124)
A

• A is the sample complexity rate of the empirical distribu-

tion, which we assume to be O(n−α ).

• B is the error made when projecting the n samples onto
k weighted points chosen by k-means++. If n = k, it van-
ishes and we recover the sample complexity of the empir-
ical estimator. Controlling B requires relating Wasserstein
distance to the optimal quantization [Canas and Rosasco,
2012].

Denoting Xn = (x1, . . . , xn) ∼ µ ⊗n, we write:

(cid:33)1/2


d(xi, ˆSk)2

B = E

Xn, ˆSk

≤ EXn



(cid:32)



n
∑
i=1

1
n
(cid:34)(cid:18) 8(log k + 2)
n

(cid:19)1/2(cid:35)

(Xn)

.

φ OPT
k

(6)

Corollary 1 tells us that at best, our estimator improves the
exponent in the bias bound by a factor of 2, going from
O(k−α ) to O(k−2α ) while keeping computational complex-
ity on par with the empirical plug-in estimator. To beneﬁt
from this improvement, we need to ensure quantization
error—the second term in the bound—is small enough so
that the ﬁrst dominates.

2.2 CONTROLLING THE QUANTIZATION

ERROR

To prove our estimator improves bias, we must make an
assumption on the behavior of the quantization error when
quantizing an n-sample from µ on k points. Intuitively,
the quantization error is small when the measure is well-
concentrated. In particular, we can upper bound quantization
error for Gaussian mixtures and measures supported on ﬁ-
nite numbers of balls.

Remark 2. We derive improved theoretical rates for these
two classes of functions, but our algorithm is better than the
plug-in estimator for any dataset whose quantization error
is smaller than the sample complexity. This is veriﬁed by sev-
eral real-world datasets (Fig.8, supplement), underscoring
the practical signiﬁcance of our proposed algoritm.

Deﬁnition 2 (Clusterable distribution). A distribution µ is
an (m, σ 2)-Gaussian mixture if it is a mixture of m Gaussian
distributions in Rd and the trace of the covariance matrix
of each mixture component is upper-bounded by σ 2. A dis-
tribution µ is (m, ∆)-clusterable if supp(µ) lies in the union
of m balls of radius at most ∆.

By writing down the deﬁnition of φ OPT
to prove that for k ≥ m, 1/n · E[φ OPT
(m, σ 2)-Gaussian mixture, and 1/n · E[φ OPT
is (m, ∆)-clusterable.

k

k

k

, it is straightforward
(Xn)] ≤ σ 2 if µ is a
(Xn)] ≤ ∆2 if µ

The ﬁrst equality comes from the equivalence between
W2 and the quantization error [Canas and Rosasco, 2012,

Incidentally, for such measures, better sample complexity
rates can be derived [Weed and Bach, 2019]:

Proposition 1 ([Weed and Bach, 2019]). If µ is a (m, σ 2)-
Gaussian mixture and log 1
σ ≥ 25/8, then for all n ≤
m(32σ 2 log 1
σ )−2,

E[W 2

2 (µ, ˆµn)] ≤ 84

(cid:112)m/n.

(7)

The same rate holds for (m, ∆)-clusterable distributions, for
all n ≤ m(2∆)−4.

This result can be extended to distributions that are mixtures
with fast decaying tails. This improved rate holds in the
small-sample regime, but asymptotically, the 1/d rate returns.
This rate is for squared W2, so in our analysis using W2 this
only implies α = 1/4 via Jensen’s inequality. Thus, these
improved rates for W2 are only relevant in dimension higher
than 4.

Further assumptions on σ 2 (resp. ∆) improve the conver-
gence rate of the bias from Theorem 2:

Proposition 2. If µ is an (m, σ 2)-Gaussian mixture (resp.
(m, ∆)-clusterable), then for all k ≥ m such that k2 log k ≤
m(32σ 2 log 1
σ )−2 (resp. k2 log k ≤ m(2∆)−4) our estimator
(Algorithm 1) satisﬁes

E[W2(µ, PˆSk(Xn)# ˆµn)] ≤

√

84

(cid:18) m

k2 log k

(cid:19)1/4

+Cσ

(cid:112)

log k,

(replacing σ by ∆ in the above bound for clusterable
distributions), where C is independent of k and σ . If
k2 log k ≤ m(32σ 2 log 1
σ )−2 (resp. k2 log k ≤ m(2∆)−4), then
σ ≤ O((log k)−1/4 k−1/2) (resp. ∆), and the rate becomes
O((log k)1/4 k−1/2).

Hence, we achieve an O((log k)1/4 k−1/2) rate in O(k3) com-
putation time, compared to the O(k−1/4) rate of the empir-
ical estimator. For the range of k we consider, we observe
in practice that the assumption 1/n · φ OPT
k ≤ 1/k often holds,
and hence our bound applies. Due to the curse of dimension-
ality, however, there is no guarantee for this to hold in the
asymptotic case.

Intuition on the ﬁnite sample regime. The intuition for
the bound of Proposition 1 is not simple. We provide an
informal explanation. From a high level, in the small sample
regime, we are looking at a coarse scale (e.g. from a distance,
Gaussians “look like” Diracs) so the bound behaves like
discrete optimal transport, which is n−1/2. However when
the number of samples grows, we are looking at a ﬁne scale;
in this regime, we suffer from the curse of dimensionality.
A second piece of intuition is simpler: when you have very
few samples, every new sample brings a lot of information,
but after a while, the information gain of each new sample
diminishes.

3 REGULARIZED TRANSPORT

Quantization can also improve approximate OT solvers, as
it introduces negligible error while improving the required
runtime and memory storage, at least in the discrete case.
We focus on entropic regularization, a popular approxima-
tion of OT obtainable in quadratic time with Sinkhorn’s
algorithm [Cuturi, 2013]. More precisely, the computational
complexity to obtain an ε-approximation of the unregular-
ized cost for discrete problems is bounded by O(k2ε −2), an
order of magnitude cheaper than the linear program [Lin
et al., 2019]. The oversampling strategy used previously
for absolutely continuous measures is irrelevant, however:
quantizing n points with k centroids takes at least O(nk)
time (because of weight assignment), which exceeds O(k2)
for n > k.

Instead, we consider the case where we are given two very
large discrete measures as input and rely on quantization to
design a more efﬁcient approximation procedure. In this set-
ting, the literature focuses on complexity bounds: given two
discrete distributions over n points and a target precision
ε, the aim is to provide an ε-approximation of unregular-
ized transport with bounded complexity [Altschuler et al.,
2017, Dvurechensky et al., 2018, Lin et al., 2019]. Build-
ing on this problem formulation, we propose a quantization
step with target precision ε as a preprocessing step. After-
wards, any approximate transport solver can be used on the
resulting quantized distribution. This provides the same the-
oretical guarantees and bounded computational complexity
as above, with potential computation time improvements.
Our algorithm is detailed in Algorithm 2.

Algorithm 2: ε-approximation of W2(µn, νn)
Input :Finite distributions µn, νn; target precision ε
Output :3ε-approximation of W2(µn, νn) with complexity

O(k2ε −2)

/* Quantize the point clouds
Sε = QUANTIZE (µn, ε); |Sε | = kε,µn
Tε = QUANTIZE (νn, ε); |Tε | = kε,νn
/* Compute weights and cost matrix
Set ai = ∑n
Set bi = ∑n
Set Ci j = (cid:107)ci − d j(cid:107)2
/* Regularized transport solver
return APPROXOT (C, a, b, ε)

j=1wµ, j1i=arg minl (cid:107)x j−cl (cid:107)2
j=1wν, j1i=arg minl (cid:107)y j−dl (cid:107)2

2 ∀ci, d j ∈ Sε × Tε

∀i ∈ {1, . . ., kε,µn }
∀i ∈ {1, . . ., kε,νn }

2

2

*/

*/

*/

Algorithm 2 relies on two subroutines: QUANTIZE and
APPROXOT. The former inputs a point cloud µn and a tol-
erance ε and outputs a (sub)set Sε , which is a quantized
version of µn. k-means++ can be adapted easily to do this.
An example is in Algorithm 3. APPROXOT yields an ε ap-
proximation of unregularized transport. The most used one
is probably the Sinkhorn algorithm, which has a complex-
ity bounded by O(k2ε −2); see [Altschuler et al., 2017] for
details. This is the one we use in our experiments.

Algorithm 3: QUANTIZE
Input :A ﬁnite distribution µn with support and weights

(xi, wi)1≤i≤n); target precision ε.

Output :Set Sε with kε elements, s.t.

2 (µn, PˆSε #µn) = ∑i wid(xi, Sε )2 < ε 2.
W 2

Sε ← xRAND(1,n)
D = (wid(xi, Sε )2)1≤i≤n
while ∑i Di > ε 2 do
Sε ← xarg maxi Di
D = (wid(xi, Sε )2)1≤i≤n

return Sε

Algorithm 3 is directly adapted from the original k-means++
algorithm. It is guaranteed to ﬁnish, as Sε = µn is a solution
for any ε. Denoting kε = |Sε | ≤ n, we have that the complex-
ity of Algorithm 3 is bounded by O(nkε ). Thus, Algorithm 2
ε ε −2) (cid:46) O(n2ε −2).
has a complexity bounded by O(nkε + k2
The fact that it outputs a 3ε approximation of OT relies on
Lemma 1 of [Canas and Rosasco, 2012]:

W2(µ, ν) ≤W2(PˆSε #µ, PˆTε #ν)

+W2(µ, PˆSε #µ) +W2(ν, PˆSε #ν)
term is approximated within ε thanks to

The ﬁrst
APPROXOT, the second/third thanks to Algorithm 3.

(8)

Overall, we have two options to obtain a 3ε approximation
of W2(µn, νn):

• Run APPROXOT(Cn, wµ , wν , 3ε) where Cn is the n × n

cost matrix between µn and νn.

• Run Algorithm 2.

Both have a complexity ≤ O(n2ε −2) and provide the same
theoretical guarantees; but the latter can provide a signiﬁcant
speed up. We compare both approaches in the next section,
measuring CPU-time vs. precision.

Space complexity. While Sinkhorn’s algorithm has space
complexity of O(n2), we highlight that alg. 2 has space
complexity of O(n + k2
ε ). Indeed, the QUANTIZE algorithm
only needs to keep track of the assignment of every point
to their nearest centroid: this is a vector of size n. Thus,
for huge datasets where storage is critical, quantization is
a natural way to downscale the point cloud while keeping
track of the precision loss.

Remark 3. Some remarks about Algorithm 2:

• The bound on the complexity of APPROXOT usually in-
volves (cid:107)C(cid:107)∞. It will be smaller for the cost between cen-
troids, providing additional speedup.

• This preprocessing step can be used for any p-Wasserstein
distance, by changing the exponent in QUANTIZE accord-
ingly (D = (wid(xi, Sε )p)1≤i≤n).

• We provide an algorithm with the same approximation
guarantees than the baseline, with lower or equal com-
putational complexity. A sharp bound on the output of
algorithm 2 would require studying ε (cid:55)→ kε .

4 EXPERIMENTS

Datasets. We test on discrete (mainly real-world data) and
continuous (synthetic) distributions. The latter tests theoreti-
cal bounds, while the former shows efﬁciency of Algorithm
1 on large point clouds. Fig. 5 (supplement) shows examples.
The discrete datasets are: DOT, Adult, and Sampled Mix-
tures. The ‘true’ distance is computed on the whole point
cloud; some datasets were downsampled to suit ground truth
computation on our machine. DOT [Schrieber et al., 2017]
contains grayscale images (i.e., ﬁxed discrete support in R2)
in various resolutions, a benchmark used e.g. in [Sommer-
feld et al., 2018], which uses the plug-in estimator. Adult
(UCI repository) is a point cloud in R6 with continuous fea-
tures for 35,000 individuals, split into two groups by income.
Sampled Mixtures (synthetic) contains 10,000 points from
a Gaussian mixture with covariance τ in R15, simulating
point clouds suited to k-means. The continuous distributions
are Gaussians and fragmented-hypercube [Forrow et al.,
2019],2 with closed-form W2; see Appendix 1 for details
and more experiments.

4.1 ALGORITHM 1

For each dataset, we compare the behavior of the plug-in
estimator and that of Algorithm 1. We plot the mean rel-
(cid:2)| Est(k, k2 log k) −W2(µ, ν)|(cid:3) /W2(µ, ν),
ative error E
estimating the expectation with 100 runs. We display two
types of plots: (i) mean relative error vs. k (size of the point
clouds passed to the LP) (Figures 1, 2) and (ii) mean relative
error vs. CPU time (Figure 3).

Xn, ˆSk

Results. Our estimator exhibits favorable behavior when
estimating W2 between large point clouds. In this case, the
sample complexity of the plug-in estimator W2(µ, ˆµk) de-
cays in O(k−1/2), independently of the dimension or number
of samples (these only affect the constant [Sommerfeld et al.,
2018]), but ours enjoys a faster decay rate exponent—up
to twice better. For continuous distributions, our results are
similarly advantageous in the ﬁnite-sample regime for clus-
terable distributions but tend to the sample complexity rate
in higher dimensions. They provide a way to verify The-
orem 2 and to illustrate the different regimes. We notice
in practice that oversampling enables the estimator to have
much lower variance (ﬁg. 7, supplement).

Discrete datasets. On the real-world datasets, the bias de-
cays 45% (DOT, ﬁg. 1a) to 65% (Adult, ﬁg. 1b) faster. A
simple analysis explains this: On a 100×100 image, with
k ≤ 100 samples the plug-in estimator will sample ∼ 1%
of the image, whereas our estimator processes all the pix-
els and then subsamples the 100 most relevant. Synthetic
experiments slightly qualify this analysis: When the data

2What they refer to as “k-means & OT” is not our Algorithm
1, since they set k = 4. Their x-axis does not relate to overall
computational complexity.

(a) DOT dataset.

(b) Adult dataset.

(c) Discrete mixture, large vari-
ance (τ = 0.1).

(d) Discrete mixture, low variance
(τ = 10−4).

Figure 1: Mean relative error vs. k on discrete datasets. Values in parentheses display the regression coefﬁcient computed
for the second half of the graph. In (a), we plot the average value of the 45 pairwise estimation on the DOT dataset
("Microscopy" images, 64 resolution).

(b) Gaussian with 10−4 diagonal
covariance.

(a) Gaussian with 10−1 diagonal
covariance.
Figure 2: Mean relative error vs. k on continuous distributions. Values in parentheses display the regression coefﬁcient
computed for the second half of the graph. Left: Gaussian in R5. When the clusterable assumption does not hold, the
improvement is negligible. However, when the ﬁnite sample rate is applicable, the improvement is striking (×2.1). Right:
Fragmented hypercube [Forrow et al., 2019]. In high dimension, it resembles the uniform distribution and we get no
improvement. In small dimension, the improvement is signiﬁcant (×1.8).

(c) Fragmented hypercube, d = 8. (d) Fragmented hypercube, d = 2.

(a) Spread Gaussian

(b) Peaked Gaussian
Figure 3: Mean relative error vs. CPU time (s) on (a) Gaussians with unit covariance, (b) Gaussians with 10−4 diagonal
covariance, (c) Adult dataset. One line corresponds to log-spaced values of k. Line’s transparency correspond to various
κ ∈ {1, 0.5, 0.1}, darkest for biggest value. Line’s color corresponds to various estimator. We compare the plug-in estimator
(orange) to two variants of our algorithm : k-means++ (green) or AFK-MC2 (blue) from [Bachem et al., 2016] as a
preprocessing step. For data with small quantization error, our approximate k-means pre-processing (even unoptimized)
provides a clear advantage.

(c) Adult Dataset

10010110210−1100Plug-in(-0.54)Ours(-0.83)10010110210−1100Plug-in(-0.28)Ours(-0.53)100101102100Plug-in(-0.39)Ours(-0.41)10010110210−210−1100Plug-in(-0.66)Ours(-0.95)100101102100Plug-in(-0.25)Ours(-0.20)10010110210−410−3Plug-in(-0.47)Ours(-1.00)10010110210−1Plug-in(-0.35)Ours(-0.37)10010110210−310−210−1Plug-in(-0.69)Ours(-1.28)10−310−210−110−1Plug-inOurs(KMeans++)Ours(AFKMC)10−310−210−110−410−3Plug-inOurs(KMeans++)Ours(AFKMC)10−310−210−110−1Plug-inOurs(KMeans++)Ours(AFKMC)is well-clustered the improvement is up to twice the de-
cay rate (ﬁg. 1d), as expected from Proposition 2; however,
when the point cloud is more spread out, the decay rate only
marginally improves over plug-in estimation.

Continuous distributions. The plug-in estimator on Gaus-
sian data recovers the expected −1/d rate exponent when
variance is high (ﬁg. 2a); when the variance is low, we ﬁnd
the better ﬁnite sample complexity rate of −1/2 predicted by
[Weed and Bach, 2019]. In this regime, our estimator beats
the plug-in estimator by a large margin (ﬁg. 2b). Asymp-
totically, both curves should reach the same slope of −1/d.
Similarly, we should expect our estimator to degrade on
the uniform distribution: for uniformly-spread data, quan-
tization error decays in k−1/d. The Fragmented Hypercube
example conﬁrms this: When d = 2, the distribution is clus-
terable (ﬁg. 2d), but as d increases the quantization error is
relatively high, eventually reaching the performance of the
plug-in estimator (ﬁg. 2c).

CPU time. Since our goal is to provide a faster W2 approxi-
mation, we check the decay of the bias against CPU time.
These experiments evaluate to what extent the theoretical
improvement of the bias may be cancelled by overhead
in k-means computation. The solver we use for OT [Fla-
mary and Courty, 2017] is thoroughly optimized, making
the comparison difﬁcult. However, our estimator is only
slower by a constant on spread out data (Figure 3(a)) and
provides a clear advantage on clustered (Figure 3(b)) and
real data (Figure 3(c)). To further improve, (i) our basic
implementation of k-means++ could be optimized and (ii)
we can use theoretically weaker minimizers of the quanti-
zation problem. In Figure 3, we use a faster approximate
quantizer, AFK-MC2 [Bachem et al., 2016] with ﬁxed chain
length on n = k2 log k points (blue), which has overall com-
plexity k2 log k but weaker guarantees on the quantization
error. Another alternative is to multiply the number of points
used to compute the anchors (we tested κ ∈ {1, 0.5, 0.1}) to
further decrease the complexity constant between the pre-
processing and the OT estimation steps. This can be used
as a hyper-parameter to balance faster execution with lower
bias improvement. For these experiments, we use an Intel(R)
Core(TM) i5-7200U CPU @ 2.50GHz processor, with 8 GB
memory. The k-means and OT solvers are implemented in
C and wrapped in Python.

quantization solution, for which no directly applicable re-
sults exist.

Lloyd’s algorithm. k-means++ is often used as an initial-
ization step for Lloyd’s algorithm. The latter converges to a
local minimizer of the quantization error, at the expense of
few more passes through the data, for an overall complexity
of O(nki), where i is the number of iterations. Theoretically,
this algorithm makes the quantization error decay by log k
at best. We verify experimentally that the improvement is
marginal in Figure 5.

(a) Gaussian with unit diagonal
covariance

(b) Gaussian with 10−4 diagonal
covariance

Figure 4: Empirical standard deviation of Algorithm 1 vs.
k. Sampling k2 log k samples instead of k, our estimator
manages a much lower standard deviation, independently of
the clusterability of the distribution.

Figure 5: Mean relative error vs. k on the Gaussian dataset
for Algorithm 1 (green), and the same algorithm succeeded
by Lloyd’s procedure (blue). The improvement of the latter
is marginal and comes at the expense of few O(nk) steps.

Variance of the estimator. Algorithm 1 relies on over-
sampling. Thus, we expect and conﬁrm experimentally that
it beneﬁts from much lower variance compared to the plug-
in estimator, as illustrated by the conﬁdence intervals in
Figures 1, 2 (plots are in log-log scale). For a more quanti-
tative analysis, we plot the empirical standard deviation
of Algorithm 1 on the Gaussian dataset on Figure 4. It is
worth noticing that it exhibits a much lower variance no mat-
ter how clusterable the underlying distribution is. However,
proving this requires bounding the stability of the optimal

4.2 ALGORITHM 2

To test the performance of Algorithm 2, we compare it to
do an approximate solver for entropy-regularized optimal
transport, which is arguably the most popular occurence
in machine learning applications. Speciﬁcally, for datasets
(µn, νn), we measure the CPU time to execute Algorithm
2 with input (µn, νn, ε) and APPROXOT(µn, νn, 3ε), which
are both guaranteed to output a 3ε−approximation of OT.

10010110210−210−1Plug-in(-0.47)Ours(-0.87)10010110210−410−3Plug-in(-0.52)Ours(-0.97)10010110210−410−310−2KMeans++(-1.10)LLoyd’s(-1.15)Here, APPROXOT is from [Altschuler et al., 2017], but any
other approximate solver satisfying the same constraints on
the input/output can be used. We display two types of plots:
(i) CPU time vs. precision ε and (ii) estimated transport cost
vs. precision ε. The former demonstrates efﬁciency while
the latter shows that the output is indeed at most ε away
from the unregularized cost.

Results. From the CPU time plots in ﬁg. 6 (left column)
the speedup introduced by our algorithm is unmistakable. It
only matches the performance of APPROXOT for low values
of ε, when QUANTIZE simply outputs the whole dataset to
have a small enough quantization error. That’s why it is most
useful for structured data, e.g. peaked distributions (ﬁg. 6.c)
or real-world datasets (ﬁg. 6.e) The error vs. ε plots (right
column) suggest that the bounds in [Altschuler et al., 2017]
are loose, since the error is often smaller than the guaranteed
ε. Quantization enables us to have maximum efﬁciency for
bounded inaccuracy.

5 CONCLUSION

Our algorithm is designed with practicality in mind: at best—
and in most of our experiments—we observe and expect
reduced bias for ﬁxed computational budget; at worst, it
behaves like plug-in estimation. Our bounds explain the
estimator’s good behavior by relating W2 to quantization
error. Even when we fall back to the −1/d rate asymptotically,
we have up to twice the decay rate in the ﬁnite sample case.
Quantization is also efﬁcient in aproximate OT solvers, as it
can match their error with improved time/space complexity.

Acknowledgements

The MIT Geometric Data Processing group acknowledges
the generous support of Army Research Ofﬁce grant
W911NF2010168, of Air Force Ofﬁce of Scientiﬁc Re-
search award FA9550-19-1-031, of National Science Foun-
dation grant IIS-1838071, from the CSAIL Systems that
Learn program, from the MIT–IBM Watson AI Labora-
tory, from the Toyota–CSAIL Joint Research Center, from
a gift from Adobe Systems, from an MIT.nano Immersion
Lab/NCSOFT Gaming Program seed grant, and from the
Skoltech–MIT Next Generation Program.

References

Jason Altschuler, Jonathan Weed, and Philippe Rigol-
let. Near-linear time approximation algorithms for
CoRR,
transport via sinkhorn iteration.
optimal
abs/1705.09634, 2017. URL http://arxiv.org/
abs/1705.09634.

David Arthur and Sergei Vassilvitskii. k-means++: The

(t, ε)

(error, ε)

(a) Gaussian, σ = 10−1 .

(b) Gaussian, σ = 10−1.

(c) Gaussian σ = 10−3 .

(d) Gaussian, σ = 10−3.

(e) Adult Dataset.

(f) Adult Dataset.

Figure 6: Left: CPU time (s) vs. ε, for Algorithm 2 and
APPROXOT [Altschuler et al., 2017]. Right: absolute error
vs. ε. The smallest precision for the range of ε is taken so
that APPROXOT requires nmax = 104 iterations. Algorithm
2 consistently provides an approximate solution an order of
magnitude faster than APPROXOT.

10−110010−410−310−210−1100FullQuantized10−110010−210−1100FullQuantized10−210−110−410−310−2FullQuantized10−210−110−310−210−1FullQuantized10110−410−310−210−1100101FullQuantized10110−1100FullQuantizedadvantages of careful seeding. Technical report, Stanford,
2006.

Olivier Bachem, Mario Lucic, Hamed Hassani, and Andreas
Krause. Fast and provably good seedings for k-means.
In Advances in Neural Information Processing Systems,
pages 55–63, 2016.

Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi
Kumar, and Sergei Vassilvitskii. Scalable k-means++.
Proceedings of the VLDB Endowment, 5(7), 2012.

Espen Bernton, Pierre E Jacob, Mathieu Gerber, and Chris-
tian P Robert. On parameter estimation with the Wasser-
stein distance. Information and Inference: A Journal of
the IMA, 8(4):657–676, 2019.

Rainer Burkard, Mauro Dell’Amico, and Silvano Martello.
Assignment Problems, revised reprint, volume 106.
SIAM, 2012.

Guillermo Canas and Lorenzo Rosasco. Learning proba-
bility measures with respect to optimal transport metrics.
In Advances in Neural Information Processing Systems,
pages 2492–2500, 2012.

Marco Cuturi. Sinkhorn distances: Lightspeed computation
of optimal transport. In Advances in Neural Information
Processing Systems, pages 2292–2300, 2013.

Richard Mansﬁeld Dudley. The speed of mean Glivenko–
Cantelli convergence. The Annals of Mathematical Statis-
tics, 40(1):40–50, 1969.

Pavel E. Dvurechensky, Alexander Gasnikov, and Alexey
Kroshnin. Computational optimal transport: Complex-
ity by accelerated gradient descent is better than by
sinkhorn’s algorithm. CoRR, abs/1802.04367, 2018. URL
http://arxiv.org/abs/1802.04367.

Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven
distributionally robust optimization using the Wasserstein
metric: Performance guarantees and tractable reformula-
tions. Mathematical Programming, 171(1-2):115–166,
2018.

Rémi Flamary and Nicolas Courty. POT: Python optimal
transport library, 2017. URL https://pythonot.
github.io/.

Aden Forrow, Jan-Christian Hütter, Mor Nitzan, Philippe
Rigollet, Geoffrey Schiebinger, and Jonathan Weed. Sta-
tistical optimal transport via factored couplings. In Inter-
national Conference on Artiﬁcial Intelligence and Statis-
tics, pages 2454–2465, 2019.

Joel Franklin and Jens Lorenz. On the scaling of multidi-
mensional matrices. Linear Algebra and Its Applications,
114:717–735, 1989.

Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning
generative models with sinkhorn divergences. In Interna-
tional Conference on Artiﬁcial Intelligence and Statistics,
pages 1608–1617, 2018.

Aude Genevay, Lénaïc Chizat, Francis Bach, Marco Cu-
turi, and Gabriel Peyré. Sample complexity of Sinkhorn
divergences. In International Conference on Artiﬁcial
Intelligence and Statistics, pages 1574–1583, 2019.

Samuel Gerber and Mauro Maggioni. Multiscale strate-
gies for computing optimal transport. arXiv preprint
arXiv:1708.02469, 2017.

Ziv Goldfeld and Kristjan Greenewald. Gaussian-smooth op-
timal transport: Metric structure and statistical efﬁciency.
AISTATS, 2020.

Benoit Kloeckner. Approximation by ﬁnitely supported
measures. ESAIM: Control, Optimisation and Calculus
of Variations, 18(2):343–359, 2012.

Tianyi Lin, Nhat Ho, and Michael I. Jordan. On the efﬁ-
ciency of the Sinkhorn and Greenkhorn algorithms and
their acceleration for optimal transport, 2019.

Stuart Lloyd. Least squares quantization in PCM. IEEE
Transactions on Information Theory, 28(2):129–137,
1982.

David Pollard. Quantization and the method of k-means.
IEEE Transactions on Information Theory, 28(2):199–
205, 1982.

Tim Salimans, Han Zhang, Alec Radford, and Dimitris
Metaxas. Improving GANs using optimal transport. In
International Conference on Learning Representations,
2018.

Bernhard Schmitzer and Christoph Schnörr. A hierarchical
approach to optimal transport. In International Confer-
ence on Scale Space and Variational Methods in Com-
puter Vision, pages 452–464. Springer, 2013.

Jorn Schrieber, Dominic Schuhmacher, and Carsten
Gottschlich.
DOTmark—A benchmark for dis-
IEEE Access, 5:271–282,
crete optimal
doi: 10.1109/access.2016.
2017.
2639065. URL http://dx.doi.org/10.1109/
ACCESS.2016.2639065.

ISSN 2169-3536.

transport.

Richard Sinkhorn. Diagonal equivalence to matrices with
prescribed row and column sums. The American Mathe-
matical Monthly, 74(4):402–405, 1967.

Max Sommerfeld, Jörn Schrieber, Yoav Zemel, and Axel
Munk. Optimal transport: Fast probabilistic approxima-
tion with exact solvers, 2018.

Cédric Villani. Topics in Optimal Transportation. Num-

ber 58. American Mathematical Society, 2003.

Jonathan Weed and Francis Bach. Sharp asymptotic and
ﬁnite-sample rates of convergence of empirical measures
in Wasserstein distance. Bernoulli, 25(4A):2620–2648,
2019.

Improving Approximations of Optimal Transport Distances with Quantization :
Supplementary Materials

Gaspard Beugnot ‡1, 2

Aude Genevay1

Kristjan Greenwald3

Justin Solomon1

1MIT CSAIL
2INRIA
3IBM Watson AI Lab

A ADDITIONAL INFORMATION ON THE EXPERIMENTS

A.1 ADDITIONAL INFORMATION ON THE SETUP

Implementation. We implemented the k-means++ subsampler using a combination of C++ and Python. Although the
complexity matches the bounds suggested in this article, our implementation was not designed with computational efﬁciency
in mind: later releases will provide faster implementation (i.e., lowering the constant in front of the O(k3 log k)). We used
the transport LP solver of Flamary and Courty [2017], but any other solver can be used in practice.

d.

√

Datasets. In the gaussian settings, we estimate the distance between a gaussian centered at 0d and one centered at 1d, both
with covariance τId. τ is a parameter to see the inﬂuence of the clusterability on the performance of our estimator. The
transport distance is known in closed form for gaussians, equal in this case to (cid:107)1d(cid:107)2 =
The fragmented-hypercube is an example from Forrow et al. [2019]. It consists of the uniform distribution µ on [0, 1]2,
which is pushed forward by the function T (X) = X + 2 sign(X) (cid:12) 12. T being the gradient of a convex function, the
transport distance between µ and T#µ can be computed in closed form as W2(µ, T#µ) =
8. This example is extended to
dimensions d > 2 by concatenating µ with U [0, 1]d−2; while this does not change the transport cost, it adds statistical
noise by mimicking a high-dimensional distribution with low-dimensional support. This deﬁnition enables a straightforward
interpretation between the two quantities at play in Theorem 2: in low dimension, our estimator has clear added value, but
this efﬁciency is lost in the quantization error in higher-dimensional settings.
The Sampled Mixtures synthetic dataset is produced as follows. We sample m1, m2 ∼ [0, 1]m×d and set Σ = τId. Then, we
draw ntot points from the mixture of gaussians {N (m1,i, Σ)}m
i=1 uniformly to obtain a point cloud X. We do likewise for
(cid:1)ntot . We stress that once we sample these two point clouds, we do not sample again from the mixture
Y ∼ (cid:0){N (m2,i, Σ)}m
of gaussians. The purpose of this experiment is to provide discrete point clouds in any dimension, with various shapes.

i=1

√

The only preprocessing step applied to the Adult dataset was centering and scaling.

CPU time simulations. Wall clock time was measured using the CPU clock and included the whole pipeline: Sample n
points, subsample k anchors, and run the linear program on the anchors. Each line in Figure 3 has multiple points marked:
each point corresponds to a different choice of k. For KMeans and AFK-MC2, there are 9 values of k evenly log-spaced
from 1 to 100. For the naïve estimator, there are 15 values of k ranging from 1 to 1000. An analogous procedure was used to
generate the plots in Figure 1, with a different x-axis.

*This work was conducted in large part during an internship at MIT.
‡This work was conducted in large part during an internship at MIT.

Accepted for the 37th Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2021).

Gaussian

Hypercube

UCI’s Adult dataset

Sampled Mixtures

Figure 7: First two components of some distributions tested in our experiments. Gaussian and Hypercube: continuous
distributions (we display samples). UCI and Sampled Mixtures: discrete point clouds. UCI is in R6, and Sampled Mixtures
are random point clouds of ntot = 104 points.

(a) Adult

(b) DOT, 128 Microscopy

(c) Gaussian in d = 5

(d) Hypercube in d = 2

(e) Hypercube in d = 8

Figure 8: Quantization error vs. k. Same settings as described in the article. For the Adult dataset, both distributions are
displayed (high and low income), and k-Means++ was ran 20 times for each. DOT plot’s is the average over the 10 available
distributions. The quantization error decays faster than the worst-case 1/d of the uniform distribution: 0.66 > 1/6 for adult,
and 1 > 1/2 for DOT. 1000 points were sampled from continuous distributions, and the output of k-Means was averaged
over 10 times. For Gaussians, the slope is the same for both low and high variance, but low variance yields a much smaller
quantization error, and thus better peformance of our method.

A.2 ADDITIONAL EXPERIMENTS

Quantized data assumption. We provide plots in Figure 8 suggesting that the low quantization error assumption made to
quantify the sample complexity is veriﬁed for the real-world datasets we use. Remember that quantization doesn’t improve
the rate when the distribution is close to uniform, or when the scale at which we process the data is below the signal’s scale.
Such situation is unlikely to appear in real-world settings, where we want to compute distances between signals rather than
noise.

Algorithm 2 on DOT. Due to lack of space, we report in Figure 9 the performance of Algorithm 2 on subsampled images
of DOT. Again, our estimator is a magnitude faster. The quantization step is well suited to the two dimensional support of
images.

100101102104LowHighreg_Low: -0.66x + 4.88reg_High: -0.66x + 4.53100101102102101reg_Res128: -1.02x + -0.771001011021051041031021011001011020.00010.1reg_Low: -0.41x + -4.20reg_High: -0.41x + 1.80100101102101102103104UnifCubereg_Unif: -1.14x + 3.03reg_Cube: -1.17x + 3.12100101102103104UnifCubereg_Unif: -0.37x + 3.63reg_Cube: -0.37x + 3.66(a) CPU times (s) vs ε

(b) Absolute error vs. ε

Figure 9: Comparison between Algorithm 2 and APPROXOT on DOT, a benchmark of gray-scaled images for optimal
transport solvers Schrieber et al. [2017]. APPROXOT comes from Altschuler et al. [2017]. For each ε, values are averaged
over the 45 pairwise estimation.

10210−410−310−210−1100FullQuantized102101FullQuantized