2
2
0
2

l
u
J

5

]
h
p
-
t
n
a
u
q
[

2
v
2
0
0
2
1
.
2
1
9
1
:
v
i
X
r
a

Quantum Logic Gate Synthesis as a Markov Decision Process

M. Sohaib Alam1,2,3, Noah F. Berthusen4,5,6, and Peter P. Orth5,7

1Rigetti Computing, 2919 Seventh Street, Berkeley, CA, 94710-2704 USA

2Quantum Artiﬁcial Intelligence Laboratory (QuAIL), NASA Ames Research Center, Moﬀett Field, CA, 94035, USA

3USRA Research Institute for Advanced Computer Science (RIACS), Mountain View, CA, 94043, USA

4Present address: Department of Computer Science, University of Maryland, College Park, MD, 20742, USA

5Ames Laboratory, Ames, Iowa 50011, USA

6Department of Electrical and Computer Engineering, Iowa State University, Ames, Iowa 50011, USA

7Department of Physics and Astronomy, Iowa State University, Ames, Iowa 50011, USA

Saturday 2nd July, 2022

Reinforcement learning has witnessed recent
applications to a variety of tasks in quantum pro-
gramming. The underlying assumption is that
those tasks could be modeled as Markov Deci-
sion Processes (MDPs). Here, we investigate
the feasibility of this assumption by exploring its
consequences for two fundamental tasks in quan-
tum programming: state preparation and gate
compilation. By forming discrete MDPs, focus-
ing exclusively on the single-qubit case (both
with and without noise), we solve for the optimal
policy exactly through policy iteration. We ﬁnd
optimal paths that correspond to the shortest
possible sequence of gates to prepare a state, or
compile a gate, up to some target accuracy. As
an example, we ﬁnd sequences of H and T gates
with length as small as 11 producing ∼ 99% ﬁ-
delity for states of the form (HT )n|0i with values
as large as n = 1010. In the presence of gate noise,
we demonstrate how the optimal policy adapts
to the eﬀects of noisy gates in order to achieve
a higher state ﬁdelity. Our work shows that one
can meaningfully impose a discrete, stochastic
and Markovian nature to a continuous, deter-
ministic and non-Markovian quantum evolution,
and provides theoretical insight into why rein-
forcement learning may be successfully used to
ﬁnd optimally short gate sequences in quantum
programming.

1 Introduction

Recent years have seen dramatic advances in the ﬁeld
of artiﬁcial intelligence [1] and machine learning [2, 3].
A long term goal is to create agents that can carry

M. Sohaib Alam: sohaib.alam@nasa.gov

out complicated tasks in an autonomous manner, rel-
atively free of human input. One of the approaches
that has gained popularity in this regard is reinforce-
ment learning. This could be thought of as referring
to a rather broad set of techniques that aim to solve
some task based on a reward-based mechanism [4]. For-
mally, reinforcement learning models the interaction of
an agent with its environment as a Markov Decision
Process (MDP). In many practical situations, the agent
may have limited access to the environment, whose dy-
In all such situa-
namics can be quite complicated.
tions, the goal of reinforcement learning is to learn or
estimate the optimal policy, which speciﬁes the (con-
ditional) probabilities of performing actions given that
the agent ﬁnds itself in some particular state. On the
other hand, in fairly simple environments such as the
textbook grid-world scenario [4], the dynamics can be
fairly simple to learn. Moreover, the state and action
spaces are ﬁnite and small, allowing for simple tabu-
lar methods instead of more complicated methods that
would, for example, necessitate the use of artiﬁcial neu-
ral networks [3]. In particular, one could use the dy-
namic programming method of policy iteration to solve
for the optimal policy exactly [5].

In recent times, reinforcement learning has met with
success in a variety of quantum programming tasks,
such as error correction [6], combinatorial optimization
problems [7], as well as state preparation [8–12] and gate
design [13, 14] in the context of noisy control. Here,
we investigate the question of state preparation and
gate compilation in the context of abstract logic gates,
and ask whether reinforcement learning could be suc-
cessfully applied to learn the optimal gate sequences to
prepare some given quantum state, or compile a spec-
iﬁed quantum gate.
Instead of exploring the eﬃcacy
of any one particular reinforcement method, we inves-
tigate whether it is even feasible to model these tasks
as MDPs. By discretizing state and action spaces in

1

 
 
 
 
 
 
this context, we circumvent questions and challenges
involving convergence rates, reward sparsity, and hyper-
paremeter optimization that typically show up in rein-
forcement learning scenarios. Instead, the discretization
allows us to exactly solve for and study quite explicitly
the properties of the optimal policy itself. This allows
us to test whether we can recover optimally short pro-
grams using reinforcement learning techniques in quan-
tum programming situations where we already have
well-established notions of what those optimally short
programs, or circuits, should look like.

There have been numerous previous studies in the
general problem of quantum compilation, including but
not limited to, the Solovay-Kitaev algorithm [15], quan-
tum Shannon decomposition [16], approximate compi-
lation [17, 18], as well as optimal circuit synthesis [19–
21]. Here, we aim to show that optimally short cir-
cuits could be found through solving discrete MDPs,
and that these circuits agree with independently cal-
culated shortest possible gate sequences for the same
tasks. Since the initial posting of this work, numerous
works have continued to explore the interface between
classical reinforcement learning and quantum comput-
ing. These include ﬁnding optimal parameters in varia-
tional quantum circuits [22–24], quantum versions of re-
inforcement learning and related methods [25–29], Bell
tests [30], as well as quantum control [31–34], state en-
gineering and gate compilation [35–40], the subject of
this paper.

In such studies, reinforcement learning is employed as
an approximate solver of some underlying MDP. This
raises the important question of how, and under what
conditions, can the underlying MDP be solved exactly,
and what kind of solution quality does it result in. Nat-
urally, such MDPs can only be solved exactly for rela-
tively small problem sizes. Our paper explores the an-
swer to this question in the context of single-qubit state
preparation and gate compilation, and demonstrates
the eﬀects of native gate choice, coordinate represen-
tation, discretization eﬀects as well as noise.

The organization of this paper is as follows. We ﬁrst
brieﬂy review the formalism of MDPs. We then in-
vestigate the problem of single-qubit state preparation
using a discretized version of the continuous {RZ, RY }
gates, as well as the discrete gateset {I, H, S, T }. We
then study this problem in the context of noisy quan-
tum channels. Finally, we consider the application to
the problem of single-qubit compilation into the {H, T }
gateset, and show, among other things, that learning
the MDP can be highly sensitive to the choice of coor-
dinates for the unitaries.

1.1 Brief Review of MDPs

Markov Decision Processes (MDPs) provide a conve-
nient framing of problems involving an agent interacting
with an environment. At discrete time steps t, an agent
receives a representation of the environment’s state st ∈
S, takes an action at ∈ A, and then receives a scalar re-
ward rt+1 ∈ R. The policy of the agent, describing the
conditional probability π(a|s) of taking action a given
the state s, is independent of the environment’s state at
previous time steps and therefore satisﬁes the Markov
property. The discounted return that an agent receives
from the environment after time step t is deﬁned as
Gt = P∞
k=0 γkrt+k+1 where γ ∈ [0, 1] is the discount
factor. The goal of the agent is then to ﬁnd the optimal
policy π∗(a|s) that maximizes the state-value function
(henceforth, “value function” for brevity), deﬁned as
the expectation value of the discounted return received
from starting in state st ∈ S and thereafter following the
policy π(a|s), and expressed as Vπ(s) = Eπ [Gt|st = s].
More formally then, the optimal policy π∗ satisﬁes the
inequality Vπ∗ (s) ≥ Vπ(s) for all s ∈ S and all policies π.
For ﬁnite MDPs, there always exists a deterministic op-
timal policy, which is not necessarily unique. The value
function for the optimal policy is then deﬁned as the
optimal value function V ∗(s) = Vπ∗ (s) = maxπVπ(s)
for all s ∈ S.

The value function satisﬁes a recursive relationship

known as the Bellman equation

Vπ(s) =

π(a|s)

X

a

X

s0,r

p(s0, r|s, a) [r + γVπ(s0)]

(1)

relating the value of the current state to that of its pos-
sible successor states following the policy π. Note that
the conditional probability of ﬁnding state s0 and re-
ceiving reward r having performed action a in state s
speciﬁes the environment dynamics, and also satisﬁes
the Markov property. This equation can be turned into
an iterative procedure known as iterative policy evalua-
tion

Vk+1(s) =

π(a|s)

X

a

X

s0,r

p(s0, r|s, a) [r + γVk(s0)]

(2)

which converges to the ﬁxed point Vk = Vπ in the k →
∞ limit, and can be used to obtain the value function
corresponding to a given policy π. In practice, we deﬁne
convergence as |Vk+1−Vk| < (cid:15) for some suﬃciently small
(cid:15).

Having found the value function, we could then
ask if the policy that produced this value function
could be further improved. To do so, we need the
state-action value function Qπ(s, a), deﬁned as the
expected return by carrying out action a in state s
and thereafter following the policy π, i.e. Qπ(s, a) =

2

Eπ [Gt|st = s, at = a]. According to the policy improve-
ment theorem, given deterministic policies π and π0, the
inequality Qπ(s, π0(s)) ≥ Vπ(s) implies Vπ0(s) ≥ Vπ(s)
where π0(s) = a (and in general π0(s) 6= π(s)) for all
s ∈ S.
In other words, having found the state-value
function corresponding to some policy, we can then im-
prove upon that policy by iterating through the action
space A while maintaining the next-step state-value
functions on the right hand side of Eq. (2) to ﬁnd a
better policy than the current one ((cid:15)-greedy algorithm
for policy improvement).

We can then alternate between policy evaluation and
policy improvement in a process known as policy itera-
tion to obtain the optimal policy [4]. Schematically, this
process involves evaluating the value function for some
given policy up to some small convergence factor, fol-
lowed by the improvement of the policy that produced
this value function. The process terminates when the
improved policy stops diﬀering from the policy in the
previous iteration. Of course, this procedure to identify
the optimal policy for an MDP relies on the ﬁniteness
of state and action spaces. As we will see below, by
discretizing the space of 1-qubit states (i.e. the surface
and interior of the Bloch sphere corresponding to pure
and mixed states), as well as identifying a ﬁnite gate
set, we create an MDP with the goal of state prepara-
tion for which optimal policies in the form of optimal
(i.e. shortest) quantum circuits may be found through
this method.

We note that one could view state evolution under
unitary operations or left multiplication of unitaries
by other unitaries as deterministic processes. These
could be thought of as trivially forming a Markov De-
cision Process where the probabilities p(s0|s, a) have a
δ-function support on some (point-like) state s0. Once
we impose discretization, this underlying determinism
implies that the dynamics of the discrete states are
strictly speaking non-Markovian, i.e.
the conditional
probability of landing in some discrete state s0 depends
not just on the previous discrete state and action, but
also on all the previous states and actions, since the un-
derlying continuous/point-like state evolves determinis-
tically. However, we shall see below that with suﬃcient
care, both the tasks of state preparation and gate com-
pilation can be modeled and solved as MDPs even with
discretized state spaces.

2 Preparation of single-qubit states

In this section, we will discuss the preparation of single-
qubit states as an MDP. In particular, we will focus on
preparing a discrete version of the |1i state. We will
do so using two diﬀerent gate sets, a discretized version
of the continuous RZ and RY gates, and the set of

naturally discrete gates I, H, S and T , and describe
probabilistic shuﬄing within discrete states to arrive at
optimal quantum programs via optimal policies. We
will also consider states of the form (HT )n|0i.

2.1 State and Action Spaces

We apply a fairly simple scheme for the discretization
of the space of pure 1-qubit states. As is well known,
this space has a one-to-one correspondence with points
on a 2-sphere, commonly known as the Bloch sphere.
With θ ∈ [0, π] denoting the polar angle and φ ∈ [0, 2π)
denoting the azimuthal angle, an arbitrary pure 1-qubit
state can be represented as

|ψi = cos

(cid:19)

(cid:18) θ
2

|0i + eiφsin

(cid:19)

(cid:18) θ
2

|1i

(3)

The discretization we adopt here is as follows. First,
we ﬁx some small number (cid:15) = π/k for some positive
integer k. Next, we identify polar caps around the north
(θ = 0) and south (θ = π) pole. The northern polar
cap is identiﬁed as the set of all 1-qubit (pure) states
for which θ < (cid:15) for some ﬁxed (cid:15), regardless of the value
of φ. Similarly, the southern polar cap is identiﬁed as
the set of all 1-qubit (pure) states for which θ > π − (cid:15),
independent of φ. Apart from these special regions, the
set of points n(cid:15) ≤ θ ≤ (n+1)(cid:15) and m(cid:15) ≤ φ ≤ (m+1)(cid:15) for
some positive integers 1 ≤ n ≤ k−2 and 0 ≤ m ≤ 2k−1
are identiﬁed as the same region. The polar caps thus
correspond to n = 0, k − 1, respectively.

We identify every region (n, m) as a “state” in the
MDP. As a result of this identiﬁcation, elements of the
space of 1-qubit pure states are mapped onto a dis-
crete set of states such that the 1-qubit states can now
only be identiﬁed up to some threshold ﬁdelity. For
instance, the |0i state is identiﬁed as the northern po-
lar cap with ﬁdelity cos2 (cid:0) π
(cid:1). Similarly, the |1i state
2k
is identiﬁed with the southern polar cap with ﬁdelity
sin2 (cid:16) (k−1) π
(cid:1). In other words, if we were
to try and obtain these states using this scheme, we
would only be able to obtain them up to these ﬁdeli-
ties.

= cos2 (cid:0) π

(cid:17)

2k

2k

Having identiﬁed a ﬁnite state space S composed of
discrete regions of the Bloch sphere, we next identify
single-qubit unitary operations, or gates, as the ac-
tion space A. There are some natural single-qubit gate
sets that are already discrete, such as {H, T }. Oth-
ers, such as the continuous rotation gates {RZ, RY },
require discretization similar to that of the continuous
state space of the Bloch sphere. We discretize the con-
tinuous gates RZ(β) and RY (γ) by discretizing the an-
gles β, γ ∈ [0, 2π]. The resolution δ = π/l must be
suﬃciently smaller than that of the state space (cid:15) = π/k
so that all states s ∈ S are accessible from all others

3

via the discretized gateset a ∈ A. In practice, a ratio
of (cid:15)/δ ∼ O(10) is usually suﬃcient, although the larger
this ratio, the better the optimal circuits we would ﬁnd.
Without loss of generality, and for illustrative pur-
poses, we identify the discrete state corresponding to
the |1i state (hereafter referred to as the “discrete |1i
state”) as the target state of our MDP. To prepare the
|1i state starting from any pure 1-qubit state using the
gates RZ and RY , it is well-known that we require at
most a single RZ rotation followed by a single RY ro-
tation. For states lying along the great circle through
the x and z axes, we need only a single RY rotation.
As a test of this discretized procedure, we investigate
whether solving this MDP would be able to reproduce
such optimally short gate sequences. We also consider
the gateset {I, H, T } below, where we include the iden-
tity gate to allow for the goal state to “do nothing” and
remain in its state. For simplicity and illustrative pur-
poses, we also include the S = T 2 gate in the case of
single-qubit state preparation.

2.2 Reward Structure and Environment Dynam-
ics

An obvious guess for a reward would be the ﬁdelity
|hφ|ψi|2 between the target state |ψi and the prepared
state |φi. However, here we consider an even simpler
reward structure of assigning +1 to the target state, and
0 to all other states. This allows us to directly relate
the length of optimal programs to the value function
corresponding to the optimal policy, as we show below.
To ﬁnish our speciﬁcation of the MDP, we also esti-
mate the environment dynamics p(s0, r|s, a). Since our
reward structure speciﬁes a unique reward r to every
state s0 ∈ S, these conditional probabilities reduce to
simply p(s0|s, a). The discretization of the Bloch sphere
implies that the action of a quantum gate a on a dis-
crete state s = (n, m) maps this state to other states
s0 = (n0, m0) according to a transition probability dis-
tribution p(s0|s, a). This non-determinism of the eﬀect
of the actions occurrs because the discrete states are
themselves composed of entire families of continuous
quantum states, which are themselves mapped deter-
ministically to other continuous quantum states. How-
ever, continuous states from the state discrete state re-
gion can land in diﬀerent discrete ﬁnal state regions.
A simple way to estimate these probabilities is to uni-
formly sample points on the 2-sphere, determine which
discrete state they land in, then perform each of the ac-
tions to determine the state resulting from this action.
We sample uniformly across the Bloch sphere by sam-
pling u, v ∼ U[0, 1], then setting θ = cos−1 (2u − 1) and
φ = 2πv. Although other means of estimating these
probabilities exist, we ﬁnd that this simple method

Figure 1: Optimal values for various states on the Bloch sphere
using the discrete RZ and RY gates, with a discount factor
γ = 0.8. The color of a state corresponds to its optimal value
function Vπ∗ , where lighter colors indicate a larger value. Those
colored in green are also exactly the states whose optimal cir-
cuits to prepare the discrete |1i state consist of a single RY
rotation, while those in blue are also exactly the ones whose
optimal circuits consist of an RZ rotation followed by an RY
rotation.

works well in practice for the particular problem of
single-qubit state preparation.

Note that for the target state |1i, the optimal policy
is to just apply the identity, i.e. RZ(0) or RY (0). This
action will continue to keep this state in the target state,
while yielding +1 reward at every time step. This yields
an inﬁnite series V ?(t) = P∞
k=0 γk, where V ?(s) := Vπ∗
and t is the target state, which we can trivially sum
to obtain (1 − γ)−1. This is the highest value of any
state on the discretized Bloch sphere. For γ = 0.8, we
obtain V ?(t) = 5.0. For some generic state s ∈ S, we
can show that with our reward structure, the optimal
value function is given by

V ?(s) =

∞
X

k=0

γk (cid:0)P k+1(cid:1)

t,s

(4)

where the elements of the matrix P are given by Ps0,s =
p(s0|s, π?(s)). From Eq. 4, it immediately follows that
V ?(s) ≤ V ?(t) for all s ∈ S. The Markov chain pro-
duced by the optimal policy has an absorbing state
given by the target state, and for some large enough
number of steps, all (discrete) states land in this absorb-
ing state. Indeed, the smallest K for which the Marko-

4

2.3 Optimal State Preparation Sequences

Using policy iteration allows for ﬁnding the optimal pol-
icy in an MDP. The optimal policy dictates the best
action to perform in a given state. We can chain the ac-
tions drawn from the optimal policy together to ﬁnd an
optimal sequence of actions, or gates, to reach the target
state. In our case, the actions are composed of unitary
operations, which deterministically evolve a quantum
state (note that we consider noise below in which case
the unitary gates are replaced by non-unitary quantum
channels). However, due to the discretization, this is
no longer true in our MDP, where the states evolve ac-
cording to the non-trivial probabilities p(s0|s, a). The
optimal policy is learned with respect to these stochas-
tic dynamics, and not with respect to the underlying
deterministic dynamics. In other words, we are impos-
ing a Markovian structure on essentially non-Markovian
dynamics. Therefore, if we simply start with some spe-
ciﬁc quantum state, and apply a sequence of actions
drawn from the optimal policy of the discrete states
that the evolving quantum states belong to, we might
not necessarily ﬁnd ourselves in the target (discrete)
state. For instance, the optimal policy in any one dis-
crete state may be to apply the Hadamard gate, and for
a subset of quantum states within that discrete state,
this may lead to another discrete state for which the
optimal policy is again the Hadamard gate. In such a
case, the evolution would be stuck in a loop.

s

U †

i|U †

1 U (1)

...U (n−1)
s

To circumvent this issue, in principle one may allow
“shuﬄing” of the quantum states within a particular
discrete state before evolving them under the optimal
policy. However, this may increase the length of the
gate sequence and moreover lead to poorer bounds on
f |ψf i = hψ0
the ﬁdelity, since hψ0
n ·
...U (2n−2)
UnU (n)
U1|ψii 6= hψ0
i|ψii, in general, where the
s
s
“shuﬄing” transformations are given by U (i)
: |ψi →
| ˜ψi such that |ψi ∼ | ˜ψi belong to the same discrete
state, while the Ui specify (unitary) actions sampled
from the optimal policy. On the other hand, with-
out such “shuﬄing”, the ﬁdelities in the target states
from sequences that only diﬀer in their starting states
is the same as the ﬁdelities of the starting states, i.e.
f |ψf i, where |ψii and |ψ0
hψ0
ii
are two diﬀerent initial pure states that belong to the
same initial discrete state, and U = Q
i Ui is the product
of the optimal policies Ui.

i|U †U |ψii = hψ0

i|ψii = hψ0

s

To avoid such shuﬄing while still producing conver-
gent paths, we sample several paths that lead from the
starting state and terminate in the target (discrete)
state, discarding sequences that are larger than some
acceptable value, e.g. the length K deﬁned by Eq. 5,
and report the one with the smallest length as the opti-
mal (i.e. shortest) program. Schematically, this can be

5

Figure 2: Optimal value landscape across the Bloch sphere
using the set of gates {I, H, S, T }, with a discount factor
γ = 0.95. The color of a state corresponds to its optimal
value function Vπ∗ , where darker colors indicate a larger value.
States distributed around the equator of the Bloch sphere are
especially advantageous to start from in order to reach the tar-
get |1i state, as their optimal circuits consist of short sequences
of S and H gates.

vian process converges to a steady state, such that

(cid:0)P K(cid:1)

s0,s = δs0,t

(5)

for all s, s0 ∈ S provides an upper bound for the
length of the gate sequence that leads from any one
discrete state s to the target discrete state t. Thus,
for the target state itself, K = 0. Since (cid:0)P k(cid:1)
s0,s ≤ 1,
for states that are one gate removed from the target
state s1, we have V ?(s1) ≤ V ?(t), and more generally
V ?(sk+1) ≤ V ?(sk). This intimately relates the length
of the optimal program to the optimal value function.
The optimal value landscape for the two gatesets are
shown in Figs. (1) and (2). Note that while in the case of
the discretized {RZ, RY } gates we have a distinguished
ring of states along the equator around the x-axis that
are only a single gate application away from the target
state, we have no such continuous patch on the Bloch
sphere for the {I, H, S, T } gateset, even though there
may be indidividual (continuous) states that are only a
single gate application away from the target state, e.g.
H|1i for the target state |1i. This shows that states
which are nearby on the Bloch sphere need not share
similar optimal paths to the target state, given such a
gateset.

described in pseudo-code as in Algorithm 1. This algo-
rithm can be used to generate optimal programs for any
given (approximately universal) single-qubit gateset. In
our experiments, we found M to be 2 for the discrete
{RZ, RY } gateset, and 88 for the {I, H, S, T } gateset,
and took K to be 100.

Algorithm 1 Optimal State Preparation Sequence

1: Inputs:-
2: Optimal-Policy, Target-State
3: State space S
4: K: length of largest acceptable program
5: M : number of sequences to sample
6:
7: Output:-
8: Optimal-Programs for each state
9:
10: Initialize empty array Optimal-Programs
11: for s ∈ S do
12:

13:
14:
15:
16:
17:

18:
19:
20:

21:
22:

23:
24:
25:
26:
27:

28:
29:

Initialize empty list Convergent-Programs
for i = 1 to M do

Converged ← F alse
while not Converged

State ← s
Prog ← Empty Program list
Counter ← 0
while Counter ≤ K
←

Action

Optimal-

Policy[State]

Prog.append(Action)
Next-State ← Env.Step(Action)
State ← Next-State
Counter ← Counter + 1
if State = Target-State
Converged ← True
Convergent-

Programs.append(Prog)

end for
Optimal-Prog ← Program with Min length in

Convergent-Programs

Optimal-Programs[s] ← Optimal-Prog

30:
31: end for

2.3.1 Discrete RZ and RY gateset

In the case of discrete RZ and RY gates, we ﬁnd what
we would expect at most a single RZ rotation followed
by a single RY rotation to get from anywhere on the
Bloch sphere to the (discrete) |1i state. For (discrete)
states lying along the equatorial ring around the Y -
axis, we need only apply a single RY rotation. Empir-
ically, we choose a state resolution of (cid:15) = π/16 so that
we would ﬁnd sequences generating the pure |1i state
from various discrete states across the Bloch sphere with

6

32

(cid:1) ∼ 99% ﬁdelity. The optimal programs we ﬁnd
cos2 (cid:0) π
via the preceding procedure for this gateset are com-
posed of programs with lengths either 1 or 2.

2.3.2 Discrete {I, H, T } gateset

We can also use the procedure described above to obtain
approximations to the states (HT )n |0i for integers n ≥
1. The unitary HT can be thought of as a rotation
about an axis n =
by an angle θ = 2 arccos
√
√
2(cid:1). The

(cid:0)5 − 2
(nx, ny, nz) =
angle θ has the continued fraction representation

(cid:16) cos(7π/8)
√

q 1
17

2, 5 − 2

2, 7 + 4

√

(cid:17)

2

θ = π +

(cid:1) q

cos (cid:0) π

8

− cos2( π

(cid:1)

2 − cos2 (cid:0) π
8 )b 1+k

8
2 c(−1+2b 1+k
1+2k

2 c)

1 + K∞

k=1

(6)

which is inﬁnite, and thus the angle θ is irrational. In
the above, we have used the Gaussian notation for the
continued fraction

K∞

n=1

an
bn

=

a1

b1 +

a2
b2+ a3

b3+...

(7)

and bxc is the ﬂooring operation x → n where n ∈ Z is
the closest integer where n ≤ x. The states (HT )n|0i
lie along an equatorial ring about the axis ~n, and no two
states (HT )n|0i and (HT )m|0i are equal for n 6= m. In-
creasing the value of n corresponds to preparing states
from among a ﬁnite set of states that span the equa-
torial ring about the axis of rotation. We choose to
investigate state preparation up to n = 1010. Although
as their form makes explicit, these states can be repro-
duced exactly using n many H and T gates, using our
procedure, they can only be obtained up to some ﬁdelity
controlled by the discretization as described above. The
advantage is that we can obtain good approximations
to these states with much fewer gates than n. This is
illustrated in Table 1 where short gate sequences can
reproduce states of the form (HT )n|0i for very large
values of n using only a few (between 3 and 17 gates).

3 Noisy state preparation

[13, 14].

Reinforcement learning has previously shown success
when applied in the presence of noise
In-
deed, the ability to learn the eﬀects of a noise channel
has apparent practical use when applied to the current
generation of noisy quantum computers. These devices
are often plagued by errors that severely limit the depth
of quantum circuits that can be executed. As full er-
ror correction procedures are too resource intensive to
be implemented on current hardware, error mitigation
methods have been developed to decrease the eﬀect of

Table 1: Gate sequences obtained from the optimal policy to
approximately produce target states |ψtargeti = (HT )n|0i. The
optimal policy and ﬁdelity are calculated for the noiseless case.
The ﬁdelity is deﬁned as F = hψtarget|ψi, where |ψi is obtained
from application of the shown gate sequences to the state |0i.
The sequences are to be read from right to left.

F
Gate sequence
n
102
0.987
TTHTHTHTH
103
0.998
TTTHTHTTTH
104
0.992
HTH
104
0.994
HTH
106
0.998
THTTH
107 HTTTTTHTHTHTHTTTH 0.998
108
0.999
I
109
0.996
I
1010
0.992
HTTTHTHTHTH

noise. Methods such as zero-noise extrapolation (ZNE),
Cliﬀord data regression (CDR), and probabilistic error
cancellation (PEC) involve post-processing of circuit re-
sults (with and without making use of knowledge about
the underlying type of noise) [41–44]. However, there
are also pre-processing error mitigation schemes that
aim to modify the input circuit in order to reduce the
impact of noise. Examples are quantum optimal control
methods and dynamical decoupling [45–47]. Such tech-
niques attempt to prepare a desired quantum state on a
noisy device using circuits (or sequence of pulses) that
are diﬀerent from the ones that would be optimal in the
absence of noise. This idea is immediately applicable in
our MDP framework as we now demonstrate.

3.1 State and Action Spaces

In the presence of noise, the quantum state becomes
mixed and is described by a density matrix, which for
a single qubit can generally be written as

ρ =

1
2

(I + rxX + ryY + rzZ) =

I + r · σ
2

.

(8)

Here, r = (rx, ry, rz) are real coeﬃcients called the
Bloch vector and σ = (X, Y, Z) are the Pauli matri-
ces. Since density matrices are semi-deﬁnite, it holds
that |r| ≤ 1. If ρ is a pure state, then |r| = 1, otherwise
|r| < 1. Pure states can thus be interpreted as points on
the surface of the Bloch sphere, whereas mixed states
correspond to points within the Bloch sphere. The max-
imally mixed state ρ = I/2 corresponds to the origin.
To ﬁnd r, one can calculate the expectation values of

each Pauli operator

(cid:16)

r =

Tr(ρX), Tr(ρY ), Tr(ρZ)

(cid:17)

= (r sin θ cos φ, r sin θ sin φ, r cos θ) ,

(9)

(10)

where r ≡ |r| ∈ [0, 1], θ ∈ [0, π], and φ ∈ [0, 2π).

We perform the state discretization analogously to
the previous section, but now need to discretize states
within the full Bloch ball. To this end, we ﬁx (cid:15) = π/k
and δ = 1/k for some positive integer k. Now the set of
points n(cid:15) ≤ θ ≤ (n + 1)(cid:15), m(cid:15) ≤ φ ≤ (m + 1)(cid:15), and lδ ≤
r ≤ (l + 1)δ for integers 1 ≤ n ≤ k − 2, 0 ≤ m ≤ 2k − 1,
and 0 ≤ l ≤ k − 1 constitute the same discrete state
s = (n, m, l) in the MDP. As before, the polar regions
n = 0, k − 1 are special as these regions are independent
of φ, i.e. they are described by the set of integers s =
(n, m = 0, l). This discretization corresponds to nesting
concentric spheres and setting the discrete MDP states
s to be the 3-dimensional regions between them.

Let us now introduce the action space A in the pres-
ence of noise. We model noisy gates using a composition
of a unitary gate U and a noisy quantum channel de-
scribed by a set of Kraus operators

E(ρ) =

EkρE†
k

X

k

(11)

k EkE†

with P
k = I. Application of a noisy quantum
channel can shrink the magnitude r of the Bloch vector
as the state becomes more mixed. Evolution under a
unitary gate U in this noisy channel results in

U E(ρ) =

U EkρE†

kU †

X

k

(12)

We here again consider the discrete gateset U ∈
{I, H, T }. Once we specify the type of noise via a set
of Kraus operators, its sole eﬀect on our description of
the MDP is to change the transition probability distri-
butions p(s0|s, a). While noise can change the optimal
policies, we may nevertheless solve for the optimal poli-
cies using the exact same procedure that we used in
the noiseless case. In the following, we compare the re-
sulting shortest gate sequences found by an agent that
was trained using the noisy transition probabilities p
and compare them to those found by an agent lacking
knowledge of the noise channel.

The noise observed in current quantum computers is
to a good approximation described by amplitude damp-
ing and dephasing channels. The amplitude damping
channel is described by the two Kraus operators

E0 =

(cid:18) 0
0

(cid:19)

√

γ
0

, E1 =

(cid:18) 1
0

√

0
1 − γ

(cid:19)

(13)

with 0 ≤ γ ≤ 1. Physically, we can interpret this
channel as causing a qubit in the |1i state to decay to

7

the |0i state with probability γ.
In current quantum
computing devices, the relaxation time, T1, describes
the timescale of such decay processes. For a given T1
time and a characteristic gate execution time τg, we
parametrize

γ = 1 − e−τg/T1 .

(14)

Note that application of the amplitude damping channel
also leads to dephasing of the oﬀ-diagonal elements of
the density matrix in the Z basis with timescales T2 =
2T1.

The dephasing channel takes the form

E(ρ) = (1 − p)ρ + pZρZ †

(15)

√

√

(speciﬁcally the

We use Pyquil

1 − p 1
and is described by the Kraus operators A0 =
and A1 =
pZ. This channel leads to pure phase damp-
ing of the oﬀ-diagonal terms of the density matrix in the
Z basis. It is described by a dephasing time, T2.
[48]

function
damping after dephasing from pyquil.noise)
to
construct a noise channels consisting of a composition of
these two noise maps by specifying the T1 and T2 times
as well as the gate duration τg. The amplitude damping
parameter γ is set by T1 using Eq. (14). Since this also
results in phase damping of the oﬀ-diagonal terms of the
density matrix (in the Z basis), the dephasing time T2
is upper limited by 2T1. We thus parametrize the de-
phasing channel parameter (describing any additional
pure dephasing) as

p =

1
2

(cid:0)1 − e−τg[1/T2−1/(2T1)](cid:1) .

(16)

The dephasing channel thus describes dephasing lead-
ing to T2 < 2T1 and acts trivially if T2 is at its upper
bound T2 = 2T1. In the following, we consider T1 = T2
such that the dephasing channel acts non-trivially on
the quantum state.

3.2 Reward Structure and Environment

For the reward structure of our noisy state preparation,
we consider the purity of the state when calculating
the reward. This is to account for the fact that there
may be no gate sequence that results in a state with a
high enough purity to land in the pure goal state. As
such, assigning a reward of +1 to the pure target state
and 0 to all other states can lead to poor convergence.
We assign the reward as follows: the pure target state
ρtarget is one of the MDP states (ntarget, mtarget, k − 1),
where r = 1 and thus l = k − 1. Considering a state
ρ in the MDP state (n0, m0, l0).
If n0 = ntarget and
m0 = mtarget, we assign a reward of l0/k. Otherwise, we
assign a reward of 0. With this construction, we reward
reaching a state in the target direction (i.e. with correct

angles θ, φ) using a reward amount that is proportional
to the purity of the state. We thus also reward gate
sequences that do not end up in the pure goal state,
while still encouraging states with higher purity.

One can expect the optimal value function for a noisy
evolution to take on smaller values due to the fact
Indeed, it can be eas-
that the rewards are smaller.
ily veriﬁed that for a simpliﬁed noise model consist-
ing of only a depolarizing quantum channel E(ρ) =
(1 − p)ρ + p
3 (XρX + Y ρY + ZρZ), the resulting optimal
value function is simply uniformly shrunk compared to
the optimal value function of a noiseless MDP. Since the
change is uniform across all values, the optimal policy is
unchanged from the noiseless setting. This is no longer
the case for realistic noise models such as described by
amplitude and dephasing quantum channels, in which
case we rederive the optimal policy using policy itera-
tion.

This requires updating the conditional probability
distributions p(s0|s, a) by performing Monte-Carlo sim-
ulations as before by drawing random continuous states
from within a given discrete states, applying determin-
istic noisy gates, and recording the obtained discrete
states. Now we ﬁnd transitions to states s0 with lower
purity than the initial state s. Note that the random-
ness of the probability distribution p arises solely from
the randomly sampled continuous quantum states to
which we apply the noisy gate actions. The random-
ness due to noise is fully captured within the mixed
state density matrix description of quantum states.

3.3 Optimal Noisy State Preparation Sequences

We now consider the task of approximating states of
the form (HT )n |0i for n (cid:29) 1, starting from the state
|0i, using a gate set {I, H, T } in the presence of noise.
We use the MDP formulation with transition probabili-
ties p(s0|s, a) obtained in the presence of amplitude and
dephasing noise channels. We ﬁnd the optimal policy
using policy iteration that yields optimal gate sequences
via Algorithm (1).

In Table 2, we present results up to n = 1010 that in-
cludes the shortest gate sequences found by the optimal
policies of noisy and noiseless MDP. We also compare
the ﬁnal state ﬁdelities produced by these optimal cir-
cuits. The ﬁdelities F that are listed in the table are
found by applying the optimal gate sequences for a given
n to the exact state |0i. Since the resulting states are
mixed, we calculate the ﬁdelity between the target state
σ and the state resulting from an optimal gate sequence
ρ as

F(ρ, σ) = (cid:0)tr

q√

√

ρσ

ρ(cid:1)2

.

(17)

We list both the gate sequences found by the noiseless
MDP, the agent whose underlying probability distribu-

8

n
102
103
104
105
106
107
108
109
1010

Noiseless MDP

F

Noisy MDP

F

0.774
0.652
0.843
0.824
0.735

TTHTHTHTH
TTTHTHTTTH
HTH
HTH
THTTH

0.882
0.820
0.820
0.869
0.863
HTTTTTHTHTHTHTTTH 0.600 HTHTTTTTTH 0.800
0.999
I
0.999
0.999
0.999
I
0.702 HTHITTTTTTH 0.806

HIIHTH
HTHTTTTHH
HTHTTTTHH
TTTTTTH
HTTHTIHTH

I
I
HTTTHTHTHTH

Table 2: Shortest gate sequences and noisy ﬁdelities F produced by the optimal policies π∗ of noiseless MDP (columns 2 and
3) and noisy MDP (columns 4 and 5). The gate sequences should be read right to left. The noise is characterized by T1 = T2
= 1 µs and the gate time is set to τg = 200 ns. While this corresponds to a noise level that is stronger than in current day
NISQ hardware, where T1, T2 ≈ 100µs, these parameters yield suﬃciently strong noise to highlight diﬀerences in the optimal gate
sequences. The ﬁdelities (with the target state) are obtained by preparing mixed states using the shown gate sequences applied to
|0i in the presence of noise. Note that the states generated by (HT )n |0i for n = 3, 4 have an overlap ﬁdelity of 0.99. This is also
the case for n = 7, 10. This explains the similarity of gate sequences found for these cases.

tion p(s0|s, a) is constructed from exact unitary gates,
and the noisy MDP, whose transition probabilities are
generated from noisy gates considering combined am-
plitude damping and dephasing error channels. We set
the relaxation and dephasing times to T1 = T2 = 1µs
and the gate time to τg = 200 ns. While the value for τg
is typical for present day superconducting NISQ hard-
ware, the values of T1, T2 are about two orders of magni-
tude shorter than typical values on today’s NISQ hard-
ware, where T1, T2 ’ 100µs. We choose such stronger
noise values in order to highlight the diﬀerence in gate
sequences (and resulting ﬁdelities) produced by the op-
timal policies π∗ for noisy and noiseless MDPs. We ex-
pect that this result is generic and robust when consid-
ering MDPs for multiple qubits, where two-qubit gate
errors are expected to lead to more pronounced noise
eﬀects.

The results in Table 2 demonstrate that even in the
presence of (strong) noise, the noisy MDP is able to
provide short gate sequences that approximate the tar-
get state reasonably well. Importantly, for all values of
n shown (except for n = 104), the optimal policy of the
noisy MDP π∗
noisy yields a gate sequence that results in
a higher ﬁdelity than the gate sequence obtained from
π∗
noiseless of the noiseless MDP (if applied in the pres-
ence of noise). This shows that noise can be mitigated
by adapting the gate sequence according to the noise ex-
perienced by the qubit. Solving for the optimal policies
of a noisy MDPs are a convenient approach to ﬁnding
such adapted quantum circuits.

In Fig. 3 we compare the gate sequences and ﬁdelities
obtained from the optimal policies of noisy and noise-
less MDPs for a ﬁxed value of n = 107 as a function
of T1 = T2. We observe the noisy MDP to outper-

form the noiseless MDP for all noise strengths. This
indicates that by learning the noise channel, the agent
can adapt to the noise and ﬁnd gate sequences that
yield higher ﬁdelities in that channel. Note that if we
applied the gate sequences found by the noisy MDP
in a noiseless setting, they would yield lower ﬁdelities
than gate sequences produced by the optimal policy of
a noiseless MDP. Based on these result, we conclude
that dynamic programming and reinforcement learning
methods provide a powerful and generic way to perform
pre-processing error mitigation by identifying optimal
gate sequences for qubit state preparation in the pres-
ence of noise. Future work should be directed towards
exploring these approaches for two and more coupled
qubits.

4 Compilation of single-qubit gates

In the previous sections, we considered an agent-
environment interaction in which we identiﬁed Hilbert
space as the state space, and the space of SU (2) gates
as the action space. Shifting our attention to the prob-
lem of quantum gate compilation, we now identify both
the state and action spaces with the space of SU (2) ma-
trices, where for convenience we ignore an overall U (1)
phase from the true group of single-qubit gates U (2).
We ﬁrst consider an appropriate coordinate system to
use, and discuss why the quaternions are better suited
to this task than Euler angles. We focus exclusively on
the gateset {I, H, T }, and modify the reward structure
slightly so that we now have to work with the proba-
bilities p(s0, r|s, a) instead of the simpler p(s0|s, a) as in
the previous section. We present empirical results for a
few randomly chosen (special) unitaries.

9

such that a2 + b2 + c2 + d2 = 1, we can write U =
RZ(α)RY (β)RZ(γ) with

α = α(a, b, c, d) = arctan (−b/a) + arctan (−d/c)

β = β(a, b, c, d) = 2 arccos(

p

a2 + b2)

(19)

γ = γ(a, b, c, d) = arctan(−b/a) − arctan(−d/c)

for some angles α, β and γ. Note that for β = 0, we have
a continuous degeneracy of choices in α and γ to specify
some RZ(δ) with α + γ = δ. However, the transforma-
tions above will conventionally ﬁx this to α = γ = δ/2.
Under the action of T , i.e. T : U → U 0 = T U =
RZ(α0)RY (β0)RZ(γ0), or equivalently T : (α, β, γ) →
(α0, β0, γ0), the ZYZ-coordinates transform rather sim-
ply as α0 = α + π/4, β0 = β, γ0 = γ. Under a similar
action of H however, the coordinates transform non-
trivially. The matrix entries, on which these parameters
depend, transform as
(cid:18) α − γ
(cid:20)
2
(cid:18) α − γ
2
(cid:18) α − γ
2
(cid:18) α − γ
2

(cid:18) α + γ
2
(cid:18) α + γ
2
(cid:18) α + γ
2
(cid:18) α + γ
2

(cid:18) β
2
(cid:18) β
2
(cid:18) β
2
(cid:18) β
2

1
√
2
−1
√
2
1
√
2
1
√
2

− cos

+ cos

− sin

+ sin

a0 =

d0 =

c0 =

b0 =

cos

cos

cos

cos

cos

cos

sin

sin

sin

sin

sin

sin

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:20)

(cid:20)

(cid:20)

(cid:19)(cid:21)

(cid:19)(cid:21)

(cid:18) β
2
(cid:18) β
2
(cid:19)(cid:21)
(cid:18) β
2
(cid:18) β
2
(20)

(cid:19)(cid:21)

This is a non-volume preserving operation for which

det(J) =

sin(β)
p1 − cos2(α) sin2(β)

(21)

where J denotes the Jacobian of the transformation
from (α, β, γ) to (α0, β0, γ0) under the action of H,
and which diverges for values of α and β such that
cos(α) sin(β) = ±1. This implies that for such patho-
logical values, a unit hypercube in the discretized
(α, β, γ) space gets mapped to a region that covers
indeﬁnitely many unit hypercubes in the discretized
(α0, β0, γ0) space.
In turn, this means that a single
state s gets mapped to an unbounded number of pos-
sible states s0, causing p(s0|s, a = H) to be arbitrary
small. This may prevent the agent from recognizing an
optimal path to valuable states, since even if the quan-
tity (r + γVπ(s0)) is particularly large for some states
s0, this quantity gets multiplied by the negligible factor
p(s0|s, a = H), and therefore has a very small contribu-
tion in an update rule such as Eq (2).

These problems can be overcome by switching to
using quaternions as our coordinate system. Unlike
the ZYZ-Euler angles, the space of quaternions is in
a one-to-one correspondence with SU (2). Given some
U ∈ SU (2) as in Eq (18), the corresponding quaternion

noisy (orange) and π∗

Figure 3: Fidelity F of the state σ prepared using optimal
gate sequences with target state ρtarget = (HT )n |0i for ﬁxed
n = 107 as a function of noise strength T1 = T2. The shortest
gate sequences (indicated in the ﬁgure) are produced by op-
timal policies π∗
noiseless (blue) of noisy and
noiseless MDPs, respectively. The noisy policy gives gate se-
quences that are diﬀerent from the noiseless case, which consis-
tently yield higher ﬁdelities. The optimal noisy gate sequence
is HT HT HT H for all times T1 = T2 ≥ 60µs. We ﬁx the gate
time to τg = 200 ns when generating the Kraus operators as
deﬁned by Eqs. (14) and (16). For each value of T1, T2, we
generate the transition probabilities p(s0|s, a) according to the
corresponding noise map and use policy iteration to ﬁnd the
optimal policy. The ﬁdelity is then calculated by applying the
gate sequence found by both the noisy and noiseless MDPs to
|0i in the error channel for that speciﬁc value of T1, T2. The
point at inﬁnity represents the noiseless case, and corresponds
to the transition probabilities learned by the noiseless MDP.

4.1 Coordinate system

We consider the gateset {I, H, T }. We include the iden-
tity in our gate set since we would like the target state to
possess the highest value, and have the agent do noth-
ing in the target state under the optimal policy. Be-
cause we would like to remain in the space of SU (2)
matrices, we deﬁne H = RY (π/2)RZ(π), which diﬀers
from the usual deﬁnition by an overall factor of i, and
T = RZ(π/4). Note that owing to our alternative gate
deﬁnitions, we have that H 2 = T 8 = −1 6= 1 so that we
may obtain up to 3 and 15 consecutive applications of
H and T respectively in the optimal program. Next, we
choose an appropriate coordinate system. One choice is
to parametrize an arbitrary U ∈ SU (2) using the ZYZ-
Euler angle decomposition. Under this parametrization,
given some U ∈ SU (2)

U = U (a, b, c, d) =

(cid:18) a + ib

c + id
−c + id a − ib

(cid:19)

(18)

10

is given simply as q = (a, b, c, d). Under the action of
T , its components transform as

a0 = a cos

b0 = b cos

c0 = c cos

d0 = d cos

(cid:17)

(cid:17)

(cid:17)

(cid:17)

(cid:16) π
8
(cid:16) π
8
(cid:16) π
8
(cid:16) π
8

+ b sin

− a sin

+ d sin

− c sin

(cid:17)

(cid:17)

(cid:17)

(cid:17)

(cid:16) π
8
(cid:16) π
8
(cid:16) π
8
(cid:16) π
8

(22)

while under the action of H, its components transform
as

a0 =

b0 =

c0 =

d0 = −

b + d
√
2
c − a
√
2
d − b
√
2
a + c
√
2

(23)

and det(J(T )) = det(J(H)) = 1 for the Jacobians asso-
ciated with both transformations, so that these opera-
tions are volume-preserving on this coordinate system.
In turn, this implies that a hypercube with unit volume
in the discretized quaternionic space gets mapped to a
region with unit volume.

For the purposes of the learning agent, this means
that the total number of states that can result from
acting with either T or H is bounded above. Suppose
we choose our discretization such that the grid spacing
along each of the 4 axes of the quaternionic space is
the same. Then, since a d-dimensional hypercube can
intersect with at most 2d equal-volume hypercubes, a
state s can be mapped to at most 16 possible states
s0. While this is certainly better than the pathological
case we noted previously using the ZYZ-Euler angles,
one could ask if it is possible to do better and design a
coordinate system such that a state gets mapped to at
most one other state.

is

3∆, n0

2∆, n0

One possible approach to make the environment
dynamics completely deterministic
to consider
a discretization q = (n1∆, n2∆, n3∆, n4∆) where
n1, n2, n3, n4 ∈ Z, and choose ∆ such that the trans-
formed quaternion can also be described similarly as
4∆), and try to ensure that n0
1∆, n0
q0 = (n0
1,
3, n0
2, n0
n0
4 are also integers. Essentially this would mean
that corners of hypercubes map to corners of hyper-
cubes, so that discretized states map uniquely to other
discretized states. However, consider the transforma-
tion under H, Eq. (23). For this transformation, re-
quiring a0 = (b + d)/
2 = (n2 + n4)∆/
1∆
√
2, for some k ∈ Z (and
in turn requires that n0
1 = k/
similarly for the other components). This implies that

2 to equal n0

√

√

n0
1 cannot be an integer, and so the map given with this
gateset over this discretized coordinate system cannot
be made deterministic in this manner. Nevertheless, we
ﬁnd that our construction is suﬃcient to solve the MDP
that we have set up.

4.2 Reward Structure and Environment Dynam-
ics

Some natural measures of overlap between two unitaries
include the Hilbert-Schmidt inner product tr(U †V ),
and since we work with quaternions,
the quater-
nion distance |q − q0|. However, neither does the
Hilbert-Schmidt inner product monotonically increase,
nor does the quaternion distance monotonically de-
crease, along the shortest {H, T } gate sequence. As
an example, consider the target quaternion q? =
[−0.52514, −0.38217, 0.72416, 0.23187] from Table (3)
with shortest compilation sequence HT T T T HT HHH
(read right to left) satisfying |q−q?| < 0.3, where q is the
prepared quaternion via the sequence. After the ﬁrst H
application, |q−q?| ∼ 1.34, which drops after the second
H application to |q − q?| ∼ 0.97, and then rises again
after the third H applciation to |q − q?| ∼ 1.49, before
eventually falling below the threshold error. Similarly,
the Hilbert-Schmidt inner product starts at ∼ 0.21, rises
to ∼ 1.05, then falls to ∼ −0.21 before eventually be-
coming ∼ 1.96. On the other hand, we showed previ-
ously how assigning a reward structure of +1 to some
target state, and 0 to all other states, made it possible
to relate the optimal value function to the length of the
optimal path.

Instead of specifying a reward of +1 in some target
state and 0 in every other state however, we now as-
sign a reward of +1 whenever the underlying unitary
has evolved to within an (cid:15)-net approximation of the
target unitary. Since we work with quaternions, we
specify this as obtaining a reward of +1 whenever the
evolved quaternion q satisﬁes |q−q?| < (cid:15), for some (cid:15) > 0
and q? is the target quaternion, and 0 otherwise. We
note that the Euclidean distance between two vectors
(a, b, c, d) and (a + ∆bin, b + ∆bin, c + ∆bin, d + ∆bin)
equals 2∆bin, however both those vectors cannot rep-
resent quaternions, since only either one of them can
have unit norm. Nevertheless, this sets a size of dis-
crete states, and we require that (cid:15) be comparable to
this scale, setting (cid:15) = 2∆bin in practice. This require-
ment comes from the fact that in general, the (cid:15)-net could
cover more than one state, so that we now need to es-
timate the probabilities p(s0, r|s, a), in contrast to the
scenario where a state uniquely speciﬁes the reward.
Demanding that (cid:15) ∼ ∆bin ensures that p(s0, r = 1|s, a)
does not become negligibly small.

We could estimate the dynamics by uniformly ran-

11

domly sampling quaternions, track which discrete state
the sampled quaternions belong to, evolve them un-
der the actions and track the resultant discrete state
and reward obtained as a result, just as we did in the
previous section. However, here we now estimate the
environment dynamics by simply rolling out gate se-
quences. Each rollout is deﬁned as starting from the
identity gate, then successively applying either an H or
T gate with equal probability until some ﬁxed number
K of actions have been performed. The probabilities
for the identity action p(s0, r|s, a = I) are simply es-
timated by recording that (s0, a = I) led to (s0, r) at
each step that we sample (s0, r) when performing some
other action a 6= I in some other state s 6= s0. The
number of actions per rollout K is set by the desired
accuracy, which the Solovay-Kitaev theorem informs us
is O(polylog(1/(cid:15))) [15], and in our case has an upper
bound given by Eq. 5. Estimating the environment dy-
namics in this manner is similar in spirit to oﬀ-policy
learning in typical reinforcement learning algorithms,
such as Q-learning [4].

Algorithm 2 Optimal Gate Compilation Sequence

1: Inputs:-
2: Optimal-Policy
3: Transition probabilities p(s0, r|s, a)
4: Reward structure
5: K: Largest acceptable length of gate sequence
6: Number of rollouts
7:
8: Output:-
9: Optimal-Sequence
10:
11: Initialize empty list Action-Rollouts
12: for each rollout do
13:
14:
15:
16:
17:

Initialize empty list Action-Sequence
Initialize State to Identity gate
Counter ← 0
while Counter < K

Action ← Optimal-Policy[State]
Action-Sequence.append(Action)
Sample (Next-State, Reward) from es-

timated p(s0, r|s, a)

State ← Next-State
if Reward = 1
break

if Action-Sequence is not empty

Action-Rollouts.append(Action-

Sequence)

18:
19:

20:
21:

22:
23:
24:

25: end for
26: Optimal-Sequence ← Minimum length Action-
Sequence in Action-Rollouts that satisﬁes pre-
cision bound

The accuracy with which we would obtain the mini-
mum length action sequence in Algorithm (2) need not
necessarily satisfy the bound (cid:15) set by the reward crite-
rion, r = 1 for |q − q?| < (cid:15), for reasoning similar to the
shuﬄing discussed in the context of state preparation
above. This is why we require Algorithm (2) to report
the minimum length action sequence that also satisﬁes
the precision bound. In practice, we found that this was
typically an unnecessary requirement and even when the
precision bound was not satisﬁed, the precision did not
stray too far from the bound. It should be emphasized
that due to the shuﬄing eﬀect, there is no a priori guar-
antee that optimal-sequence returned by Algorithm (2)
need even exist, since the precision bound is not guar-
anteed to exist, and the only bound we can safely set is
|q −q?| (cid:46) ∆bink, where k is the number of actions in the
sequence that prepares q. In practice however, we ﬁnd
the algorithm to work quite well in producing optimal
sequences that correspond to the shortest possible gate
sequences to prepare the target quaternions q?.

To benchmark the compilation sequences found by
this procedure, we ﬁnd shortest gate sequences for com-

4.3 Optimal Gate Compilation Sequences

Solving the constructed MDP through policy iteration,
we arrive at the optimal policy just as before. We
now chain the optimal policies together to form optimal
gate compilation sequences, accounting for the fact that
while the dynamics of our constructed MDP is stochas-
tic, the underlying evolution of the unitary states is
deterministic. The procedure we use for starting with
the identity gate and terminating, with some accuracy,
at the target state is outlined in pseudo-code in Algo-
rithm 2, where the length of the largest sequence K is
dictated by Eq. 5, and in our experiments we took 100
rollouts.

12

pilation to some speciﬁed precision using a brute-force
search that yields the smallest gate sequence that sat-
isﬁes |q − q?| < (cid:15) for some (cid:15) > 0 with the smallest value
of |q − q?|, where q is the prepared quaternion and q? is
the target quaternion. This brute-force procedure can
be described in pseudo-code as in Algorithm 3.

Algorithm 3 Shortest Gate Compilation Sequence

1: Inputs:-
2: Target unitary q?
3: Found = False
4: Target accuracy (cid:15)
5:
6: Output:-
7: Shortest-Sequence
8:
9: Found = False
10: while not Found
11:
12:
13:

Initialize empty list Quaternion-Distances
Sequences ← 2n sequences of {H, T }
for Seq in Sequences

Seq

14:

15:
16:
17:
18:

Evolve identity quaternion according to

Quaternion-Distances.append(|q−q?|)

if Min(Quaternion-Distances) < (cid:15)

Found ← True
Shortest-Sequence ← Seq with

Min(Quaternion-Distances)

As an experiment, we drew 30 (Haar) random SU (2)
matrices, and found their compilation sequences from
Algorithms (2) and (3). We set (cid:15) = 2∆bin = 0.3, es-
timated the environment dynamics using 1000 rollouts,
each rollout being 50 actions long, and each action being
a uniform draw between H and T . The ﬁndings are pre-
sented in Table (3), where the sequences are to be read
right to left. We ﬁnd that although the two approaches
sometimes yield diﬀerent sequences, the two sequences
agree in their length and produce quaternions that fall
within (cid:15) of the target quaternion. We expect in gen-
eral that the two approaches will produce comparable
length sequences and target ﬁdelities, though not nec-
essarily equal.

5 Conclusions

We have shown that the tasks of single-qubit state
preparation and gate compilation can be modeled as
ﬁnite MDPs yielding optimally short gate sequences to
prepare states or compile gates up to some desired ac-
curacy. These optimal sequences were found to be com-
parable with independently calculated shortest gate se-
quences for the same tasks, often agreeing with them

exactly. Additionally, we investigated state preparation
in the presence of amplitude damping and dephasing
noise channels. We found that an agent can learn infor-
mation about the noise and yield noise-adapted optimal
gate sequences that result in a higher ﬁdelity with the
target state. This work therefore provides strong ev-
idence that more complicated quantum programming
tasks can also be successfully modeled as MDPs. In sce-
narios where the state or action spaces grow too large
for dynamic programming to be applicable, or where the
environment dynamics cannot be accurately learned in
the simple manner described above, it is therefore highly
promising to apply reinforcement learning to ﬁnd opti-
mally short circuits for particular tasks. Future work
should be directed towards using dynamic programming
and reinforcement learning methods for noiseless and
noisy state preparation and gate compilation for sev-
eral coupled qubits.

We provide the required programs for qubit state
preparation as open-source software, and we make the
corresponding raw data of our results openly accessi-
ble [49].

6 Acknowledgments

M.S.A. was primarily supported by Rigetti Comput-
ing during this work, where he wrote an initial ver-
sion of this manuscript which was posted on the pre-
print repository arXiv. M.S.A. and P.P.O. were sup-
ported by the U.S. Department of Energy, Oﬃce of Sci-
ence, National Quantum Information Science Research
Centers, Superconducting Quantum Materials and Sys-
tems Center (SQMS) under the contract No. DE-AC02-
07CH11359. M.S.A. is currently supported under this
contract through NASA-DOE interagency agreement
SAA2-403602, and by USRA NASA Academic Mission
Services under contract No. NNA16BD14C.

M.S.A. would like to thank Erik Davis and Eric Peter-
son for valuable insights and useful feedback through-
out the development of this work. Previous work
with Keri McKiernan, Erik Davis, Chad Rigetti and
Nima Alidoust directly inspired this current investi-
Joshua Combes and Marcus da Silva pro-
gation.
vided early feedback and encouragement to explore
this work. P.P.O. acknowledges useful discussions with
Derek Brandt.

References

[1] P. Norvig and S. J. Russell, Artiﬁcial Intelligence:
A Modern Approach (Pearson Education Limited,
2016).

13

Table 3: Compilation Gate Sequences from MDP and Brute-Force (BF) using (cid:15) = 0.3

. Note the deﬁnitions of H = RY (π/2)RZ(π) and T = RZ(π/4), as described in the main text.

q?
[-0.54981 0.35852 0.41549 0.62972]
[-0.76688 0.32823 -0.37129 0.4078]
[-0.52514 -0.38217 0.72416 0.23187]
[-0.94809 0.13988 0.25424 -0.13006]
[-0.66457 -0.47827 0.45341 0.35218]
[-0.93392 -0.04759 -0.14279 -0.32426]
[-0.06813 -0.20031 0.97526 -0.06406]
[-0.52828 0.65335 -0.26856 0.47109]
[-0.62701 0.42767 -0.1176 0.64041]
[-0.27418 0.40672 -0.46718 0.73563]
[-0.09875 0.75277 0.50256 -0.41354]
[-0.04894 -0.00402 -0.83205 0.55252]
[-0.68691 0.36726 0.04274 -0.62566]
[-0.06072 0.76411 -0.12676 -0.62959]
[-0.62191 -0.0639 -0.76511 0.1541]
[-0.98674 0.06886 -0.1264 0.07503]
[-0.86814 0.26898 0.38056 0.17075]
[-0.2836 -0.03982 0.95045 -0.12098]
[-0.45815 -0.60513 -0.62792 0.17215]
[-0.60091 -0.54151 0.58106 0.08967]
[-0.3671 -0.15162 -0.40285 0.8246]
[-0.33288 0.42797 0.28725 0.78963]
[-0.84802 -0.1492 0.02132 0.50808]
[-0.88329 -0.28327 -0.28398 0.2427]
[-0.3926 -0.75829 0.34643 -0.38838]
[-0.85775 0.2746 0.42074 -0.10883]
[-0.27497 0.25412 0.69666 0.61195]
[-0.47217 0.0121 0.23258 0.85018]
[-0.67911 -0.46404 -0.35643 0.4432]

MDP Gate Sequence Brute-Force Gate Sequence

THTTH
HTHT
HTTTTHHHTH
HTTHTHHHTT
HTTTTHHHTH
THHHTHTHT
TTTTHTTTTH
HTHTT
HTTHT
HTTTHTT
TTHTTTTT
HTTTTHTT
TTHTTT
TTTTHTTT
TTTTH
HH
HTTTHTHHHTT
HTTTHTHTHTTT
TTTTTHHHTTH
HTHHTTHTH
HTHTHTHTTTTH
THTTTH
HTH
THTHTHHTTH
THTHHHTTTT
HTHTHTHHTT
TTHTTTH
HTTTH
HTTHTHHTTH

THTTH
HTHT
HTTTTHTHHH
THTTHHHTHT
HTTHHTHTTH
TTHHHTHTH
TTTTHTTTTH
HTHTT
HTTHT
HTTTHTT
TTHTTTTT
HTTTTHTT
TTHTTT
TTTTHTTT
TTTTH
HH
HTHHTHTHTTT
HTTTHTHTHTTT
TTTTHTHTHTH
HTTHHTHTH
HTHTHTHTTTTH
THTTTH
HTH
THTHHHTTTH
TTHTHTTTHH
HTHTHTHHTT
TTHTTTH
HTTTH
HTTHTHHTTH

|qM DP − q?|
0.19996
0.2483
0.18812
0.23144
0.29977
0.25982
0.22244
0.23627
0.22121
0.24486
0.28736
0.20474
0.25131
0.27854
0.19609
0.16286
0.22221
0.07442
0.2187
0.16617
0.15013
0.29693
0.21022
0.21036
0.26302
0.12494
0.0623
0.26015
0.27136

|qBF − q?|
0.19996
0.2483
0.18812
0.20043
0.26614
0.24801
0.22244
0.23627
0.22121
0.24486
0.28736
0.20474
0.25131
0.27854
0.19609
0.16286
0.09319
0.07442
0.19569
0.16617
0.15013
0.29693
0.21022
0.21036
0.22761
0.12494
0.0623
0.26015
0.27136

[2] S. Shalev-Schwartz and S. Ben-David, Understand-
ing Machine Learning: From Theory to Algorithms
(Cambridge University Press, 2014).

[3] I. Goodfellow, Y. Bengio, and A. Courville,
Deep Learning (MIT Press, 2016) http://www.
deeplearningbook.org.

[4] R. S. Sutton and A. G. Barto, Reinforcement
Learning: An Introduction (The MIT Press, 2017).
[5] R. Bellman, Proceedings of the National Academy

of Sciences (1952).

[10] M. August and J. M. Hern´andez-Lobato, High Per-
formance Computing. ISC High Performance 2018.
Lecture Notes in Computer Science, vol 11203.
(2018).

[11] F. Albarr´an-Arriagada, J. C. Retamal, E. Solano,
and L. Lamata, Phys. Rev. A 98, 042315 (2018).
[12] X.-M. Zhang, Z. Wei, R. Asad, X.-C. Yang, and
X. Wang, npj Quantum Information volume 5, Ar-
ticle number: 85 (2019).

[13] Z. An and D. L. Zhou, EPL, 126 (2019) 60002

[6] T. F¨osel, T. W. Petru Tighineanu, and F. Mar-

(2019).

quardt, Physical Review X. (2018).

[14] M. Y. Niu, S. Boixo, V. N. Smelyanskiy, and

[7] K. A. McKiernan, E. Davis, M. S. Alam, and

H. Neven, npj Quantum Inf 5, 33 (2019) (2019).

C. Rigetti, arXiv:1908.08054 (2019).

[15] C. M. Dawson and M. A. Nielsen, arXiv:quant-

[8] M. Bukov, A. G. Day, D. Sels, P. Weinberg,
A. Polkovnikov, and P. Mehta, Phys Rev X 8,
031086 (2018).

[9] M. Bukov, Phys. Rev. B 98, 224305 (2018).

ph/0505030 (2005).

[16] V. V. Shende, S. S. Bullock, and I. L. Markov,
IEEE Trans. on Computer-Aided Design, vol. 25,
no. 6, June 2006, pp.1000 - 1010 (2006).

14

[17] E. C. Peterson, G. E. Crooks, and R. S. Smith,

arXiv:1904.10541 (2019).

[18] V. Kliuchnikov, A. Bocharov, M. Roetteler, and

J. Yard, arXiv:1510.03888 (2015).

[19] V. Kliuchnikov, D. Maslov, and M. Mosca, Quan-
tum Information and Computation, Vol. 13, No.
7,8 pp. 607-630 (2013).

[20] F. Vatan and C. Williams, Phys. Rev. A 69, 032315

(2004).

[21] N. J. Ross and P. Selinger, Quantum Information
and Computation 16(11-12):901-953 (2016).
[22] J. Yao, M. Bukov, and L. Lin, in Proceedings of The
First Mathematical and Scientiﬁc Machine Learn-
ing Conference, Proceedings of Machine Learning
Research, Vol. 107, edited by J. Lu and R. Ward
(PMLR, 2020) pp. 605–634.

[23] M. M. Wauters, E. Panizon, G. B. Mbeng, and
G. E. Santoro, Phys. Rev. Research 2, 033446
(2020).

[24] O. Lockwood, Optimizing quantum variational cir-
cuits with deep reinforcement learning (2021).
[25] S. Jerbi, C. Gyurik, S. C. Marshall, H. J. Briegel,
and V. Dunjko, Parametrized quantum policies for
reinforcement learning (2021).

[26] A. Skolik, S. Jerbi, and V. Dunjko, Quantum
agents in the gym: a variational quantum algo-
rithm for deep q-learning (2021).

[27] D. Wang, X. You, T. Li, and A. M. Childs, Pro-
ceedings of the AAAI Conference on Artiﬁcial In-
telligence 35, 10102 (2021).

[28] D. Sutter, G. Nannicini, T. Sutter, and S. Woerner,
Quantum speedups for convex dynamic program-
ming (2020).

[29] J. Lumbreras, E. Haapasalo, and M. Tomamichel,
Multi-armed quantum bandits: Exploration versus
exploitation when learning properties of quantum
states (2021).

[30] A. A. Melnikov, P. Sekatski, and N. Sangouard,

Phys. Rev. Lett. 125, 160401 (2020).

[31] V. V. Sivak, A. Eickbusch, H. Liu, B. Royer,
I. Tsioutsios, and M. H. Devoret, Model-free quan-
tum control with reinforcement learning (2021).

[32] R. Porotti, D. Tamascelli, M. Restelli, and E. Prati,

Communications Physics 2, 61 (2019).

[33] M.-Z. Ai, Y. Ding, Y. Ban, J. D. Mart´ın-Guerrero,
J. Casanova, J.-M. Cui, Y.-F. Huang, X. Chen, C.-
F. Li, and G.-C. Guo, Experimentally realizing eﬃ-
cient quantum control with reinforcement learning
(2021).

[34] L. Giannelli, P. Sgroi, J. Brown, G. S. Paraoanu,
M. Paternostro, E. Paladino, and G. Falci, Physics
Letters A 434, 128054 (2022).

[35] J. Mackeprang, D. B. R. Dasari, and J. Wrachtrup,

Quantum Machine Intelligence 2, 5 (2020).

[36] A. Paler, L. M. Sasu, A. Florea, and R. Andonie,
Machine learning optimization of quantum circuit
layouts (2020).

[37] M. G. Pozzi, S. J. Herbert, A. Sengupta, and R. D.
Mullins, Using reinforcement learning to perform
qubit routing in quantum compilers (2020).

[38] L. Cincio, K. Rudinger, M. Sarovar, and P. J.

Coles, PRX Quantum 2, 010324 (2021).

[39] F. Marquardt, SciPost Phys. Lect. Notes , 29

(2021).

[40] L. Moro, M. G. A. Paris, M. Restelli, and E. Prati,

Communications Physics 4, 178 (2021).

[41] K. Temme, S. Bravyi, and J. M. Gambetta, Phys.

Rev. Lett. 119, 180509 (2017).

[42] Y. Li and S. C. Benjamin, Physical Review X 7,

10.1103/physrevx.7.021050 (2017).

[43] A. Mari, N. Shammah, and W. J. Zeng, Phys. Rev.

A 104, 052607 (2021).

[44] A. Lowe, M. H. Gordon, P. Czarnik, A. Arrasmith,
P. J. Coles, and L. Cincio, Phys. Rev. Research 3,
033098 (2021).

[45] L. Viola and E. Knill, Phys. Rev. Lett. 94, 060502

(2005).

[46] K. Khodjasteh and L. Viola, Phys. Rev. Lett. 102,

080501 (2009).

[47] M. Abdelhafez, D. I. Schuster, and J. Koch, Phys.

Rev. A 99, 052327 (2019).

[48] R. S. Smith, M. J. Curtis, and W. J. Zeng, A prac-
tical quantum instruction set architecture (2016),
arXiv:1608.03355 [quant-ph] .

[49] M. S. Alam, N. Berthusen, and P. P. Orth

10.6084/m9.ﬁgshare.19833436.v3 (2022).

A Optimal value function

Here, we prove Eq. 4. For a value function generated
by any policy, given that the reward is 1 in the target

15

state t and 0 in every other state

V (s) = p(t|s) [1 + γV (t)] +

p(s0|s)γV (s0)

X

s06=t

= p(t|s)V (t) +

X

s0

p(s0|s)γV (s0) − p(t|s)γV (t)

= p(t|s)V (t)(1 − γ) + γ

= p(t|s)V (t)(1 − γ) + γ

p(s0|s)V (s0)

p(s0|s) ×

X

s0
X

s0

"

p(t|s0)V (t) − p(t|s0)γV (t) +

#

p(s00|s0)γV (s00)

X

s00

= p(t|s)V (t)(1 − γ) + γ

p(t|s0)p(s0|s)V (t)

X

s0

−γ2 X

p(t|s0)p(s0|s)V (t) + γ2 X

p(s00|s0)p(s0|s)V (s00)

s0

s0,s00

= γ0(P )t,sV (t)(1 − γ) + γ(P 2)t,sV (t)(1 − γ)

+γ2 X

(P 2)s00,sV (s00)

s00

=

K
X

k=0

γk(P k+1)t,sV (t)(1 − γ)

+γK+1 X

(P K+1)s0,sV (s0)

s0

where in the ﬁrst equality, we have simply used
the fact that the reward is 1 in the target state
t and 0 in every other state,
in the 4th inequal-
ity we have expanded V (s0) using the same fact,
in the 6th equality we have used the fact that
P
s00 p(s00|s) and used the no-
tation (P k)s0,s = P
p(s0|sk−1) . . . p(s1|s), and
ﬁnally in the last equality we have recursively expanded
V (s00), just as in the preceding steps, a total of K times.
Noting that we can carry out this recursive expansion
arbitrarily many times, in the limit K → ∞, we ﬁnd

s0,s00 p(s00|s0)p(s0|s) = P

s1,...,sk−1

V (s) =

∞
X

k=0

γk(P k+1)t,sV (t)(1 − γ)

(25)

The above expression is valid for the value function cor-
responding to an arbitrary policy. Specializing to the
optimal policy, for which V (t) = (1 − γ)−1, we ﬁnd
precisely Eq. 4.

(24)

16

