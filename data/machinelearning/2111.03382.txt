1
2
0
2

v
o
N
5

]
E
S
.
s
c
[

1
v
2
8
3
3
0
.
1
1
1
2
:
v
i
X
r
a

Discerning Legitimate Failures From False Alerts: A Study of Chromium’s
Continuous Integration

GUILLAUME HABEN, SnT, University of Luxembourg, Luxembourg
SARRA HABCHI, SnT, University of Luxembourg, Luxembourg
MIKE PAPADAKIS, SnT, University of Luxembourg, Luxembourg
MAXIME CORDY, SnT, University of Luxembourg, Luxembourg
YVES LE TRAON, SnT, University of Luxembourg, Luxembourg

Flakiness is a major concern in Software testing. Flaky tests pass and fail for the same version of a program and mislead developers
who spend time and resources investigating test failures only to discover that they are false alerts. In practice, the defacto approach to
address this concern is to rerun failing tests hoping that they would pass and manifest as false alerts. Nonetheless, completely filtering

out false alerts may require a disproportionate number of reruns, and thus incurs important costs both computation and time-wise. As

an alternative to reruns, we propose Fair, a novel lightweight approach that classifies test failures into false alerts and legitimate

failures. Fair relies on a classifier and a set of features from the failures and test artefacts. To build and evaluate our machine learning

classifier, we use the continuous integration of the Chromium project. In particular, we collect the properties and artefacts of more

than 1 million test failures from 2,000 builds. Our results show that Fair can accurately distinguish legitimate failures from false alerts,

with an MCC up to 95%. Moreover, by studying different test categories: GUI, integration and unit tests, we show that Fair classifies

failures accurately even when the number of failures is limited. Finally, we compare the costs of our approach to reruns and show that

Fair could save up to 20 minutes of computation time per build.

Additional Key Words and Phrases: Continuous Integration, Test Failures, False Alerts

1 INTRODUCTION

Continuous Integration (CI) is a software engineering process that allows developers to frequently merge their changes

in a shared repository [25]. To ensure a fast and efficient collaboration, the CI automates parts —if not all— of the

development lifecycle. Regression testing is an important step of this cycle as it ensures that new changes do not break

existing features. Test suites are run for every commit and test results report if changes should be integrated to the

codebase or not. Deterministic tests, which have consistent results, are then primordial to keep a smooth automation

and a trouble-free CI. Unfortunately, some tests, commonly named flaky tests, exhibit a non-deterministic behavior as

they both pass and fail for the same version of the codebase. When flaky tests fail, they send false alerts to developers

who are evaluating their code changes. Indeed, developers spend time and effort investigating these flaky failures, only

to discover that they are not due to legitimate faults [9]. When these false alerts occur frequently, developers may lose

trust in their test suites and stop considering failures even if some of them are caused by real faults. In this way, false

alerts defy the purpose of software testing and hinder the flow of the CI.

Flakiness affects both open source and industrial projects [2, 15, 17]. Reports from Google show that almost 16% of

their 4.2 million tests have some level of flakiness. This leads them to spend between 2 to 16% of their computation

resources in rerunning flaky tests [21]. Many other companies reported having trouble dealing with flakiness, including

Microsoft [18], Spotify [22] and Firefox [24].

Authors’ addresses: Guillaume Haben, SnT, University of Luxembourg, Luxembourg, guillaume.haben@uni.lu; Sarra Habchi, SnT, University of Luxem-
bourg, Luxembourg, sarra.habchi@uni.lu; Mike Papadakis, SnT, University of Luxembourg, Luxembourg, michail.papadakis@uni.lu; Maxime Cordy, SnT,
University of Luxembourg, Luxembourg, maxime.cordy@uni.lu; Yves Le Traon, SnT, University of Luxembourg, Luxembourg, yves.letraon@uni.lu.

1

 
 
 
 
 
 
2

Haben et al.

In an attempt to better understand flaky tests, researchers studied the nature and root causes of flakiness [8, 16, 20, 29].

These studies showed that the causes differ depending on projects, frameworks, and programming languages. However,

they concurringly report that asynchronous waits, concurrency, and order-dependency are often the most prevalent

root causes [11, 26]. As for flakiness mitigation, rerunning failing tests is still the main strategy used by practitioners

to identify flaky tests [9]. If we can observe different outcomes of a test (passing or failing) when rerun on the same

version of a program, we know it is a flaky test. But the rerun approach can require a lot of time and compute resources.

Studies often re-execute test suites a large number of times, 100 times [23], 400 times [11] or even 10,000 times [1] and

are still able to uncover unseen flaky tests. Hence, other tools such as DeFlaker [2] and iDFlakies [17] were designed to

detect flaky tests with a minimal number of reruns. Recently, several approaches relied on machine-learning to predict

flaky tests based on code vocabulary, code coverage, and dynamic features [1, 12, 23], allowing flakiness detection

without reruns. Nevertheless, all these studies focus on distinguishing flaky tests from reliable tests and do not address
the distinction between false alerts (i.e., flaky failures) and legitimate test failures (i.e., real regressions in the code).

In this paper, we address the problem of flakiness by focusing on test executions. In particular, we propose Fair (for

FAIlure Recognizer), a novel lightweight approach for discerning legitimate failures from false alerts that are caused
by flaky tests. Our approach relies on dynamic features (e.g., stack-trace and run duration) and static features (e.g.,
test command and code) to discard false alerts while avoiding reruns and coverage computation. We then conduct an

empirical study on the Chromium project [6] to evaluate the effectiveness and efficiency of our approach. Our study

covers more than 1 million test executions, 2,000 builds, and 41 test suites that include unit, integration, and Graphical

User Interface (GUI) tests. With this study, we aim to answer the following research questions:

• RQ1: How effective is Fair at identifying legitimate failures?
• RQ2: How effective is Fair on different test categories?
• RQ3: What features are the most significant for Fair?
• RQ4: How efficient is Fair compared to reruns?

Our results show that:

• Fair can accurately predict if a failure is legit or a false alert with an MCC up to 95%.
• Fair shows great performances in detecting legitimate failures from GUI tests, with an MCC reaching 98%. For
other test categories (integration and unit), despite a low number of samples to train and test on, Fair is able to

decently predict failure classes.

• When investigating the most important features used in the model’s decisions, the run duration comes first,
followed by properties and keywords extracted from the artefacts (stack trace, test source, test command, etc).

• Compared to the computation time required by reruns, Fair is faster by an order of 105.

The remaining of this paper is organised as follows. Section 2 introduces the chromium CI and our data collection

process. Section 3 presents a preliminary analysis of the collected test failures. Section 4 explains our failure classification

approach Fair and Section 5 describes our evaluation protocol. Section 6 presents the evaluation results and Section 7

discusses the threats to validity. Finally, Section 8 discusses related works and Section 9 concludes with the main
findings. For replication purposes, we provide a comprehensive package that includes all the used datasets and scripts 1.

1https://github.com/GuillaumeHaben/FAIR-ReplicationPackage

Discerning Legitimate Failures From False Alerts

3

2 THE CHROMIUM PROJECT

2.1 Continuous integration environment

Started in 2008, with more than 2,000 contributors and 25 million lines of code, the Chromium web browser is one

of the biggest open-source projects currently existing. Google is one of the main maintainers, but other companies

and contributors also take part in its development. Chromium relies on LuCI as a CI platform [28]. It uses more than
900 parallelized builders, each one of them used to build with different settings (e.g., different compilers, instrumented
versions for memory error detection, fuzzing, etc) and to target different operating systems (e.g., Android, MacOS,
Linux, and Windows). Each builder is responsible for a list of builds triggered by each commit made to the project. In a
build, we find details about build properties, start and end times, status (i.e., success or failure), a listing of the steps and
links to the logs.

In the beginning, building and testing were sequential. Builders used to compile the project and zip the results to

builders responsible for tests. Testing was taking a lot of time, slowing developers productivity and testing Chromium

for several platforms was not conceivable, even by adding more computation power. A swarming infrastructure was

then introduced in order to scale according to the Chromium development team productivity, to keep getting the test

results as fast as possible and independently from the number of tests to run or the number of platforms to test. A fleet

of 14,000 build bots is currently available to run tasks in parallel. This setup helps running tests with low latency and

sustains hundreds of commits per day [6].

In this study, we focus on testers, i.e., builders only responsible for running tests. At the time of writing, we found
47 testers running Chromium test suites on different versions of operating systems. About 300,000 tests are divided
into 41 test suites, the biggest ones being blink_web_tests (testing the rendering engine) and base_unittests with more
than 60,000 tests each. For each build, we can obtain information about test results. The test is labelled as pass when
it successfully passed after one execution. In case of a failure, LuCI automatically reruns the test from 2 to 5 times
depending on configurations. If all reruns fail, the test is labelled as unexpected and will trigger a build failure. If a test

passes after having one or more failed executions during the same build, it is labelled as flaky and won’t prevent the

build from passing. In the remaining, we refer to unexpected failures as legitimate failures since their failing behaviour

persisted through several reruns. On the other hand, we refer to flaky failures as false alerts.

2.2 Data Collection

We focus our data collection on the two testers Linux Tests and Win10 Tests x64, which are responsible for executing
tests of the two main operating systems supported by Chromium. We collected the records of 1,000 builds for each
tester. This represents 45 days of development taken between February and March 2021. By querying LuCI ’s API, we
retrieved the details of each test failure that occurred within these builds. Table 1 reports the number of false alerts and

legitimate failures found in this dataset. We list the total number of runs that fall into each category and the number

of unique failing tests. Interestingly, we observe that the number of failures that turn out to be false alerts is much

larger than the legitimate ones, 969,417 and 225,762 respectively. That is, false alerts represent 81% of the failures in the

Chromium CI, whereas legitimate failures only represent 19%. We also observe that the number of tests responsible for

failures is very low compared to the number of failures. This indicates that the same tests keep failing across different

builds.

4

Haben et al.

Table 1. Data collected from the Chromium CI

Tester

#Builds

Linux Tests
Win10 Tests x64
Total

1,000
1,000
2,000

False alerts

#Tests
7,063
40,668
47,731

#Failures
191,886
777,531
969,417

Legitimate failures
#Failures
#Tests
175,904
2,855
49,858
1,482
225,762
4,337

False alerts represent more than 81% of the failures detected in the Chromium CI, whereas legitimate failures

only represent 19%.

3 PRELIMINARY ANALYSIS

This section presents our preliminary analysis of test failures in the Chromium CI. The objective is to study the

characteristics of test failures, both false alerts and legitimate failures, and pave the way for our approach design. In

accordance with the study objective, we formulate the following preliminary question:

• PQ: How do legitimate failures compare to false alerts?

To answer this question, we compare false alerts and legitimate failures in terms of:

• History: This represents the historical behaviour (pass, failure, flake) of tests responsible for failures.
• Duration: This represents the duration of test executions leading to legitimate failures and false alerts.

3.1 Test history

We analyse the history of tests responsible for legitimate failures and false alerts by relying on the metrics flakeRate
and failRate. For a failure of a test 𝑡 occurring at a commit 𝑐𝑛, we analyse all the commits from a time window 𝑤 (i.e.,
from 𝑐𝑛−𝑤 to 𝑐𝑛−1) to calculate the rates as follows:

𝑓 𝑙𝑎𝑘𝑒𝑅𝑎𝑡𝑒 (𝑡, 𝑛) =

(cid:205)𝑛−1

𝑥=𝑛−𝑤 𝑓 𝑙𝑎𝑘𝑒 (𝑡, 𝑥)
𝑤

(1)

𝑓 𝑎𝑖𝑙𝑅𝑎𝑡𝑒 (𝑡, 𝑛) =

(cid:205)𝑛−1

𝑥=𝑛−𝑤 𝑓 𝑎𝑖𝑙 (𝑡, 𝑥)
𝑤

(2)

where 𝑓 𝑙𝑎𝑘𝑒 (𝑡, 𝑥) = 1 if the test 𝑡 flakes in the commit 𝑐𝑥 and null otherwise, while 𝑓 𝑎𝑖𝑙 (𝑡, 𝑥) = 1 if the test 𝑡 fails
persistently (for five reruns) in the commit 𝑐𝑥 and null otherwise. These two metrics allow us to understand if the history
of a test can help in distinguishing false alerts from legitimate failures. The test execution history (a.k.a. heartbeat) has
been used in multiple studies (especially industrial ones [15, 19]) to detect flaky tests. These studies assume that many

flaky tests have distinguishable failure patterns over commits and hence can be detected by observing their history. In

this question, we check whether this assumption holds in the case of Chromium.

To illustrate differences in rates between false and legitimate failures, we plot the flakeRate() and failRate() for each

failure category in Figure 1. The flakiness and failure rates are computed using a window of 35 commits. For false alerts,

analyzing their history of failures shows that in most cases, the tests did not have a history of failures. The percentage
of false alerts having a 𝑓 𝑎𝑖𝑙𝑅𝑎𝑡𝑒 () > 0 is in fact 0.2%. In contrast, failures marked as false alerts were often witnessed in
tests that had false alerts in previous commits. Indeed, only 16% had a 𝑓 𝑙𝑎𝑘𝑒𝑅𝑎𝑡𝑒 () = 0 in the past commits. In the case

Discerning Legitimate Failures From False Alerts

5

Fig. 1. Flake rate (left) and failure rate (right) for runs marked as false alerts and legitimate failures by reruns

of legitimate failures, we observe that they generally occur in tests that did not fail or flake before. 60% of legitimate

failures never flaked in previous commits. From these observations, we may suggest that the flakeRate() can be used to

further distinguish between false alerts and legitimate failures. Nevertheless, there is still an important overlap between

the history of false alerts and legitimate failures. In particular, in the history of Linux and Windows testers, we observe

that:

• 16% of false alerts have 𝑓 𝑙𝑎𝑘𝑒𝑅𝑎𝑡𝑒 () = 0 𝑎𝑛𝑑 𝑓 𝑎𝑖𝑙𝑅𝑎𝑡𝑒 () = 0;
• And 19% of legitimate failures have 𝑓 𝑙𝑎𝑘𝑒𝑅𝑎𝑡𝑒 () = 0 𝑎𝑛𝑑 𝑓 𝑎𝑖𝑙𝑅𝑎𝑡𝑒 () = 0.

These two groups constitute a set of failures that have a clean history and thus are impossible to classify into false

alerts or legitimate failures based on flakiness and failure metrics. Hence, we conclude that the historical differences

between false alerts and legitimate failures are not sufficient for distinguishing them.

Another interesting observation from figure 1 is that there is a set of legitimate failures that occur in tests that
manifested false alerts (𝑓 𝑙𝑎𝑘𝑒𝑅𝑎𝑡𝑒 () > 0) in recent commits. We suspect that these outliers are false alerts that were
not properly classified by reruns (we call them unreliable failures). Indeed, while reruns can affirm that a failure is due
to flakiness by manifesting a test pass and fail for the same version, they do not allow us to affirm that a failure is

legitimate. A previous study showed that up to 10,000 reruns can be required to discover flaky tests that have a low

flake rate [1]. Hence, a legitimate failure in the case of Chromium can still be a false alert (flaky failure) that was not

rerun enough to manifest. This explains the behaviour of these tests which fail and flake intermittently in close time

windows.

3.2 Run duration

The duration of test failures is one of the features easily retrievable in the Chromium CI. Thus, as part of our preliminary

analysis, we compare false alerts and legitimate failures in terms of run duration. For this, we analyse the run duration of

6

Haben et al.

Table 2. Execution duration for test failures in seconds

False alerts
Legitimate failures

Q1 Median Q3
1.31
0.41
0.18
2.11
1.09
0.77

all failures from the Linux Tests tester. Table 2 shows the distribution of run duration for legitimate and false failures. We
observe that legitimate failures tend to take longer than false alerts. In particular, the median duration of test executions

is 1.09 seconds for legitimate failures, but only 0.41 seconds for false alerts. In the same vein, 25% of legitimate failures

lasted more than 2.11 seconds, whereas the same portion of false alerts only lasted more than 1.31 seconds. This suggests

that some failure properties such as run duration can help us distinguish legitimate failures from false alerts.

False alerts occur recurrently in the same tests, whereas legitimate failures occur in tests that have low chances

to fail or flake in the past. Yet, due to a large number of failures with a clean history, these differences are

not sufficient to discriminate legitimate failures from false alerts. With regard to run properties, false alerts

manifest a shorter run duration than legitimate failures.

4 FAIR

Our main objective is to distinguish legitimate failures from false alerts efficiently and effectively. To achieve this, we

propose a learning model that relies on dynamic and static properties to classify failures without performing any reruns.

In the following, we present our feature extraction process and model design.

4.1 Features

Data cleaning. Starting from the dataset retrieved from the Chromium CI, we applied two filters to ensure the quality
of our dataset. First, we excluded some failures that occur in a period where a big chunk of the test suite was failing

oddly. A commit undoing changes that introduce these test suite failures was performed after investigation. Specifically,
in Linux Tests, for 15 consecutive builds (98177 to 98192), 2,700 tests were failing repeatedly and this results in a total of
more than 170,000 failures. During the same period, in Win10 Tests x64, 1,300 tests were failing per build, resulting
in more than 45,000 failures spread across 9 builds (53760 to 53768). This irregular amount of failures only impacted

JavaScript tests and the root cause remains unexplained. Therefore, we decided not to consider these builds in our

experiments, to avoid a bias towards JavaScript tests. As a second cleaning step, we removed all the failures that were

deemed unreliable based on their history (cf. Section 3). This filter excluded 0.13% of the failures in our dataset.

Feature extraction. To construct our features, we analyse the failure properties, e.g., run duration, and all the available
artefacts. Depending on the test, different artefacts are potentially available for each failure. Crash logs, stack traces,

and the standard error provide various information about the raised exceptions and the execution of the program under

test. For some tests, we can also retrieve the command used to launch the test. To widen the feature space, we also
retrieve the source code of the test in its current commit using Google Git2. In the case of the test source code, we only

2https://chromium.googlesource.com/

Discerning Legitimate Failures From False Alerts

7

Table 3. Description of the features

Feature name
command
commandLength
crashlog
crashlogLength
runDuration
runStatus
runTagStatus

Feature description
The vocabulary of the command used to run the test
The number of characters present in the command artieact
The vocabulary of the crash log resulting from an error
The number of characters present in the crashlog artefact
The time spent for this run execution
0: ABORT, 1: FAIL, 2: PASS, 3: CRASH, 4: SKIP
0: CRASH, 1: PASS, 2: FAIL, 3: TIMEOUT, 4: SUCCESS,
5: FAILURE, 6: FAILURE_ON_EXIT, 7: NOTRUN, 8: SKIP, 9: UNKNOWN
The vocabulary of the stack trace resulting from an error

stackTrace
stackTraceLength The number of characters present in the stackTrace artefact
stderr
stderrLength
testSource
testSourceLength

The vocabulary of the standard error captured after the test execution
The number of characters present in the stderr artefact
The vocabulary of the source code for the test
The number of characters present in the testSource artefact

retrieved it when the test file was only containing one test. Chromium’s codebase — and tests — are written in different

languages, but the vast majority are HTML and JavaScript tests. As other tests are not easily parsable, we decided to

only add the test source to the runs of HTML and JavaScript tests. As all these artefacts are of textual nature (crashLog,

stackTrace, stderr, and testSource), we build a bag of words [10] representation for each of them and we consider them

as separate features. When counting words, we use Term Frequency-Inverse Document Frequency (TF-IDF), which

aims at lowering the weight on common words present in all documents. In order to denote the presence of artefacts

(or not) and their size, we add the length of all artefacts to the features. Additionally, we extract the run duration, run

status, and run tag status to complete our list of features. Run duration is the execution time needed to run the test.
Run status gives information about the run result (e.g., passing, failing, and skipped) and run tag status returns more
precise information about the result of a run depending on the type of test or test suite (e.g., timeout and failure on
exit). Table 3 provides a summary of all the extracted features.

4.2 Failure classifier

Following previous studies on flakiness prediction [12, 23] finding that Random Forest yields the best performances in

flakiness classification tasks, we rely on this model for our classification as well. Selecting the model that yields the

best performance is not in the scope of our study. We rather focus on understanding if the approach is feasible. With

the two sets of data collected from Linux and Windows testers, we proceed as follows. Each failure in our dataset is
denoted as an n-dimensional feature vector 𝑋 = (𝑥1, ..., 𝑥𝑛) where 𝑥𝑖 represents one feature. 𝑦 = {0, 1} indicates if the
failure is from the false alert class (0) or from the legitimate failure class (1). Once all vectors are created, we randomly

split our dataset by including 80% of it in the training set and 20% in the test set, conserving the class ratio in each

subset (stratified). Using the training set, we search for the optimal set of hyperparameters for our Random Forest

Classifier. To do so, we apply a 5-fold cross-validation technique using randomized search [3]. Considering our dataset,

this step takes between 5 to 30 minutes depending on the test categories considered (See RQ2) on a machine with Intel

Core 2.3GHz 8-core 9th-generation and 32-GB RAM. Once the optimal hyperparameters are tuned, we then search

8

Haben et al.

for the best threshold for precision and recall. We avoid using ROC curves which tend to be overly optimistic in their

view of algorithms’ performance in presence of a high level of imbalanced data. Instead, we use the precision-recall

curve recommended as an alternative in this case [7]. Tuning the behaviour of a model by calibrating its classification

threshold enables us to find the best trade-off between precision and recall. Finally, we refit the model on the whole

training set and we evaluate its performance on the test set. The classified failures are categorized as:

• True Positives (TP): failures that are correctly classified as legitimate failures;
• False Positives (FP): false alerts that are wrongfully classified as legitimate;
• False Negatives (FN): legitimate failures that are wrongfully classified as false alerts;
• True Negatives (TN): failures correctly classified as false alerts.

5 EVALUATION

This section presents our experimental protocol for evaluating Fair and answering our research questions.

5.1 RQ1: How effective is Fair at identifying legitimate failures?

In this question, we evaluate the performances of Fair at detecting legitimate failures. The objective is to investigate

the possibility of using a machine learning model and failure properties to perform this detection. To evaluate our

model, we rely on the following metrics:

𝑇 𝑃
𝑇 𝑃 +𝐹 𝑃 .
𝑇 𝑃
𝑇 𝑃 +𝐹 𝑁 .

• Precision: Given by:
• Recall: Given by:
• F1-score: Given by: 2 ∗
• MCC: The accuracy of a model is sensitive to the class imbalance. In particular, the precision and recall metrics
can easily be impacted when one class is underrepresented, which is the case for our dataset. To alleviate this

𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 ∗ 𝑟𝑒𝑐𝑎𝑙𝑙
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑟𝑒𝑐𝑎𝑙𝑙

issue, we report the Matthews Correlation Coefficient (MCC) which is a more reliable statistical rate to avoid

over-optimistic results in the case of an imbalanced dataset [5]. This metric takes into consideration all four

entries of the confusion matrix. MCC is given by:

𝑇 𝑁 × 𝑇 𝑃 − 𝐹 𝑃 × 𝐹 𝑁
√︁(𝑇 𝑁 + 𝐹 𝑁 )(𝑇 𝑃 + 𝐹 𝑃)(𝑇 𝑁 + 𝐹 𝑃)(𝐹 𝑁 + 𝑇 𝑃)

5.2 RQ2: How effective is Fair on different test categories?

In this question, we evaluate the performances of Fair on different test categories. In heavily tested projects, different

types of tests are used to assess the system quality. In the case of Chromium, test suites can be classified into three test

categories: Graphical User Interface (GUI), integration, and unit tests. Based on their purpose, these tests differ in terms
of scope and cost. In particular, unit tests target lower granularity components (e.g., classes or functions) and their
execution is generally cheap both computation and time-wise. On the other hand, GUI tests evaluate the system as a

whole (end-to-end) and consume more time and resources. Based on these differences, we expect these categories of

tests to manifest different execution characteristics, which may influence our false alert detection approach. Hence, we

take into account the test categories and evaluate the performances of Fair with two strategies:

• Cross-category evaluation: In this strategy, we train the model using data from the different categories of
tests, following their distribution in the original dataset. Then, we evaluate the model performances on three

Discerning Legitimate Failures From False Alerts

9

Table 4. Details about Chromium test suites for the considered builds

Test suite

Test type

unit
accessibility_unittests
unit
aura_unittests
unit
blink_platform_unittests
blink_web_tests
GUI
integration
browser_tests
unit
components_unittests
integration
content_browsertests
content_unittests
unit
integration
cronet_tests
elevation_service_unittests
unit
unit
events_unittests
unit
extensions_unittests
unit
gcp_unittests
GUI
interactive_ui_tests
unit
jingle_unittests
unit
media_blink_unittests
unit
mojo_unittests
unit
net_unittests
non_skia_renderer_swiftshader_blink_web_tests GUI
GUI
not_site_per_process_blink_web_tests
unit
perfetto_unittests
GUI
pixel_browser_tests
unit
remoting_unittests
unit
services_unittests
unit
setup_unittests
unit
shell_dialogs_unittests
integration
sync_integration_tests
unit
telemetry_unittests
unit
unit_tests
unit
views_unittests
GUI
vulkan_swiftshader_blink_web_tests
unit
webkit_unit_tests
unit
wm_unittests

#Tests
0
0
3
26,137
1,368
27
241
80
2
1
2
15
46
65
1
3
11
27
0
0
0
100
12
21
1
4
93
0
161
0
0
11,959
0

Win10 Tests x64

Linux Tests

#False alerts
0
0
81
190,489
11,228
990
6,380
209
206
968
164
5,018
3,889
182
327
11
131
452
0
0
0
633
259
698
135
1,922
133
0
2,703
0
0
528,703
0

#Legitimate failures
0
0
0
504
208
12
40
0
0
0
0
0
0
18
0
0
0
2
0
0
0
72
0
0
0
0
0
0
14
0
0
4
0

#Tests
1
74
3
1,389
2,529
100
268
3
0
0
0
0
0
144
0
3
0
11
12
1,486
19
0
0
10
0
0
91
2
154
307
14
75
59

#False alerts
961
318
204
74,566
20,297
601
583
6
0
0
0
0
0
486
0
428
0
881
210
60,684
410
0
0
386
0
0
103
7,283
2,285
673
2,898
10,350
298

#Legitimate failures
0
0
0
584
222
8
28
0
0
0
0
0
0
74
0
2
0
0
0
680
0
0
0
0
0
0
2
0
26
0
0
4
0

separate validation sets: GUI tests, integration tests and unit tests. This strategy allows a fine-grained evaluation

of our approach and shows its performances on the three test categories.

• Intra-category evaluation: We separately build and evaluate one model per test category. This allows us to

assess the performances of our strategy when only one test category is available.

Table 4 provides details about Chromium’s test suites. For each test suite, we report the number of collected false

alerts, legitimate failures, and the tests that are concerned by these failures. For the sake of readability, we only
present test suites for which at least 100 failures are collected. The two largest test suites are blink_web_tests and
webkit_unit_tests. Consequently, most failures come from these test suites.

5.3 RQ3: What features are the most significant for Fair?

In this question, we shed light on the main features used by our model to detect legitimate failures and false alerts. To

identify these features, we compute the information gain provided by each feature and we sort them accordingly. The

information gain is based on the Gini coefficient of the features. Afterwards, we report and analyse the ten features that

provide the most informational gain.

10

Haben et al.

5.4 RQ4: How efficient is Fair compared to reruns?

In the Chromium project, the current approach to deal with false alerts is to rerun tests to observe flakiness. In this

question, we compare our approach Fair with reruns as a baseline, in terms of execution time required to classify a

failure. To perform this comparison, we extract from LuCI:

• The average time required by reruns to identify a failure as a false alert;
• The average time required by reruns to identify a legitimate failure;

Based on these values, we compute the total time required to classify all failures in our dataset and estimate the rerun

cost per build. For the costs of Fair, we report the cost of model training and the average time consumed to classify a

failure. Thence, we deduce the overall cost for classifying all the failures of our dataset and the average cost of Fair per

build.

6 RESULTS

6.1 RQ1: How effective is Fair at identifying legitimate failures?

Table 5 presents the precision, recall, F1-score, and MCC score of Fair when evaluated with different strategies. In this
question, we focus on the results for All → All, which show the model performances when trained and tested on the
whole dataset, regardless of test categories. For the Linux tester, we observe that the model has great performances

with an MCC of 95%. Fair detects legitimate failures with a precision of 98% and a recall of 93%. Similar results are

observed in the Windows tester, with a precision of 95% and a recall of 92%. Furthermore, the MCC and F1-score are

also stable at 94%. Hence, we can conclude that Fair distinguishes accurately legitimate failures from false alerts.

Fair shows great performances in discriminating legitimate failures from false alerts, with a minimum MCC

and F1-score of 94%.

6.2 RQ2: How effective is Fair on different test categories?

The remaining of Table 5 reports the performance of Fair on specific test categories. First, we analyse the results of the

cross-category evaluation. This represents the cases where the model is trained using data from different categories but
tested on a specific test category, i.e., All → GUI, All → Integration, and All → Unit. The results show that training on
a diverse dataset still allows the model to accurately detect legitimate failures from different test categories. GUI and

integration tests have the best performances with an MCC score ranging between 87% and 98% in the Linux dataset,

and between 91% and 94% in the Windows dataset. Unit tests show slightly lower results with an MCC of 75% for both

Linux and Windows datasets. These positive results show that the observations of RQ1 were not solely based on the

performances on GUI tests (the predominant category in our datasets), and the model can distinguish legitimate failures

for the three test categories.

Secondly, we analyse the results of intra-category evaluation, i.e., GUI → GUI, Integration → Integration, and Unit
→ Unit. For GUI tests, we observe great performances in the two datasets. The minimum precision and recall are 96%
and 93% respectively, and the MCC and F1-score reach 98% in the Linux dataset. These performances decrease slightly

for the other two test categories, integration and unit, especially in the Linux dataset. Specifically, for integration tests,

Fair has an MCC and F1-score between 78% and 94%, and precision and recall between 76% and 96%. For unit tests,

the performances have more variations as MCC and F1-score range between 70% and 84%. The decrease and variation

Discerning Legitimate Failures From False Alerts

11

Table 5. Fair evaluation with different strategies

Metrics

Precision Recall

Dataset

Training strategies

Linux Tests

Win10 x64

All → All
All → GUI
All → Integration
All → Unit
GUI → GUI
Integration → Integration
Unit → Unit
All → All
All → GUI
All → Integration
All → Unit
GUI → GUI
Integration → Integration
Unit → Unit

98,0%
99,6%
78,6%
75,0%
99,5%
80,0%
100%
95,8%
93,7%
93,1%
100%
96,7%
96,6%
100%

F1
93,7% 95,8% 95,8%
97,4% 98,4% 98,4%
97,1% 86,8% 87,2%
75,0% 75,0% 75,0%
97,8% 98,7% 98,6%
76,9% 78,4% 78,3%
50,0% 66,7% 70,7%
92,9% 94,4% 94,4%
95,2% 94,4% 94,4%
90,0% 91,5% 91,5%
57,1% 72,7% 75,6%
93,5% 95,1% 95,1%
93,3% 94,9% 94,9%
71,4% 83,3% 84,5%

MCC False alerts
182,989
182,989
182,989
182,989
137,749
20,746
24,490
744,545
744,545
744,545
744,545
189,347
17,722
537,476

Classes
Legit failures
1,274
1,274
1,274
1,274
1,124
132
18
495
495
495
495
310
150
35

Total
184,263
184,263
184,263
184,263
138,873
20,878
24,508
745,040
745,040
745,040
745,040
189,657
17,872
537,511

for integration and unit tests are due to the limited number of legitimate failures from this category. In particular, the

number of legitimate unit test failures is 18 and 35 in the Linux and Windows datasets respectively. This means that the

training sets contain only 14 and 28 legitimate failures (80% of the total number). These limited samples do not allow

the model to observe enough examples of the target class (legitimate failures) and hinder its capabilities. However,

the overall decent results for all categories and the great results for GUI tests suggest that our approach is capable of

detecting legitimate failures from different test categories.

Fair can accurately detect legitimate failures that arise from GUI, integration, and unit tests. When provided

with large datasets, Fair can reach great performances with an MCC of 98%, and even with very limited

learning samples (only 14 legitimate failures), its performances remain decent with a minimum MCC of 70%.

6.3 RQ3: What features are the most significant for Fair?

Table 6 reports the most significant features for our model. Column Gain presents the informational gain provided by
each feature and column origin presents their sources. Overall, we observe that the model relies on features from both
run properties and artefacts. The run duration is the most important feature with an information gain of 0.06. This

confirms the results of our preliminary analysis where legitimate failures showed a tendency of longer run durations

than false alerts. Another run property that influences the model is the runTagStatus, with an informational gain of 0.01.

This property hints at the failure cause like crash, timeout, and failure on exit. From our analysis, we observed that false

alerts tend to be more related to timeouts than legitimate failures. Indeed, 15% of false alerts are caused by timeouts,

whereas only 4% of legitimate failures are related to timeouts. For features extracted from artefacts, the length of the

stack trace, the test source, and the execution command show an important influence on the model. This aligns with

previous industrial observations, which indicate that the test length is positively correlated to flakiness [1]. Besides the

length, the vocabulary of artefacts plays an important role in the classification, with five features in the list.

12

Haben et al.

Table 6. Most significant features

Feature
runDuration
stackTraceLength
elements
testSourceLength
update
next
commandLength
load
runTagStatus
portal

Artefacts

Origin
Run properties
Artefacts

Gain
0.061
0.027
0.019 Artefacts vocabulary
0.014
0.012 Artefacts vocabulary
0.011 Artefacts vocabulary
0.010
0.010 Artefacts vocabulary
0.010
0.008 Artefacts vocabulary

Run properties

Artefacts

Table 7. The cost of reruns used to detect legitimate failures

False Alerts

Legit Failures

Rerun cost

Average
0.41 seconds

#Instances
969,417

Average
1.09 seconds

#Instances
225,762

Per build
20 minutes

Total
686 hours

Table 8. The cost of building and using Fair

Training
Cost
<30 minutes

Prediction

Overall cost

Average
0.001 milliseconds

#Instances
1,195,179

Per build
1.1 milliseconds

Total
2.2 seconds

Fair capitalizes on dynamic properties like run duration and failure status to discern legitimate failures.

Properties and keywords from artefacts, like the stack trace and test command, also play a significant role in

the classification.

6.4 RQ4: How efficient is Fair compared to reruns?

Table 7 reports the time required by reruns to classify failures. On average, reruns take 2.3 seconds to observe flakiness

and confirm that a failure is a false alert, while it takes less than one second to identify a legitimate failure. Reruns

require 686 hours to classify all the failures in our dataset of 1,195,179 failures. Considering that these failures come

from 2,000 builds, this represents a cost of around 20 minutes per build.

Table 8 reports the time required by Fair to classify failures. Regardless of its type, Fair requires 0.001 milliseconds

to classify a failure. Hence, it takes 2.2 seconds to classify all the 1,195,179 failures and consumes on average 1.1

milliseconds per build. In comparison, Fair has a clear edge in terms of efficiency. Using Fair, the individual cost of

failure classification goes down from one second to 0.001 milliseconds (900,000 times faster), and the cost per build goes

from 20 minutes to 1.1ms.

Discerning Legitimate Failures From False Alerts

13

Fair is able to classify failures 900,000 times faster than reruns. Using Fair, the cost of failure classification per

build will go down from 20 minutes to one millisecond.

7 THREATS TO VALIDITY

7.1 External validity

One possible threat to our external validity is the generalizability to other software development infrastructures. We rely

solely on data from the Chromium project, which has a specific organization with a continuous integration setup that
may not generalize to other software projects. Thus, some of the features used in this study (e.g., run tag status) may be
difficult to obtain, impacting the model performances. We encourage future studies to investigate other dynamic features

and testing artefacts to further investigate their usability in discerning false alerts. Another threat to the generalizability

is the variety of test categories. To the best of our knowledge, this is the first study on false alert detection that considers

multiple test categories. Nonetheless, as our categories are limited to GUI, integration, and unit tests, investigations are

yet to be done to check if results hold for other categories of tests. Finally, the choice of programming languages and

testing environments may also affect the validity of our results. We perform our experiments on two builders responsible

for running test suites on two different operating systems and the analyzed tests are written in different programming

languages. Yet, to get better insights on the approach’s efficiency and effectiveness, we encourage investigations on

different configurations.

7.2 Construct validity

A potential threat to our study’s validity is the definitions of legitimate failures. We build our dataset from the

Chromium CI, which relies on reruns to classify failures. Consequently, the validity of our results may be hindered by

misclassifications present in the CI. To alleviate this threat, we performed a thorough cleaning step before using the

data collected from LuCI. In particular, we excluded failures that occur in a period where most of the test suite was

failing for 15 consecutive builds in Linux and 9 builds in Windows. Additionally, we excluded failures that were deemed

unreliable based on their history. This allowed us to filter out 0.13% of our initial dataset.

8 RELATED WORK

Flakiness is a well-known issue in Software Testing but research studies on this topic have only gained momentum in
the past few years. Luo et al. [20] conducted the first empirical study on the root causes of flakiness. They analyzed
commit fixes from 51 open-source projects and created the first taxonomy of flaky tests. Later on, several similar studies
followed with different settings. Lam et al. [16] conducted a study on flaky tests at Microsoft to identify and understand
root causes of flaky tests. They presented RootFinder, a framework that helps to debug flaky tests using logs and time
differences in their passing and failing runs. Romano et al. [26] focused their analysis on User Interface (UI) based
tests. They showcased flakiness root causes for UI tests and highlighted the conditions needed to fix them. In the
same vein, Gruber et al. [11] presented a large empirical analysis of more than 20,000 Python projects. They found test
order-dependency and infrastructure to be among the top reasons for flakiness in those projects.

In addition to empirical analyses, tools have been introduced by researchers to help debugging, reproducing, and
understanding flaky tests. Notably, DeFlaker [2] detects flaky tests across commits, without performing any reruns, by
checking for inconsistencies in test outcomes. More focused on test order-dependency, iDFlakies [17] detects flaky tests

14

Haben et al.

by rerunning test suites in various orders. To increase the chances of observing flakiness, Vysali et al. [30] introduced
Shaker, a technique that relies on stress testing while rerunning potential flaky tests. As for fixing tools, Shi et al. [27]
proposed iFixFlakies, a framework that recommends patches based on helpers present in other tests to automatically fix

order-dependent flaky tests.

While they remain scarce, the recent publication of datasets of flaky tests [2, 11, 17] enabled new lines of work.
Prediction models were suggested to differentiate flaky tests from non-flaky tests. King et al. [14] presented an approach
that leverages Bayesian networks to classify and predict flaky tests based on code metrics. Pinto et al. [23] used a bag
of words representation of each test to train a model able to recognize flaky tests based on the vocabulary from the

test code. This line of work has gained a lot of momentum lately as models achieved higher performances. Several

works were carried out to replicate those studies and ensure their validity in different contexts [4, 12]. More recently,
FlakeFlagger [1] has been introduced as another model using an extended set of features retrieved from the code under
test and test smells. Compared to FlakeFlagger, our approach uses a more lightweight set of features that are easily
accessible after test executions, without the requirements of computation overhead — e.g., the requirement of code
coverage. Moreover, compared to the existing works which distinguish flaky tests from non-flaky tests, Fair focuses on

test executions and discerns legitimate failures from flaky ones.

Closer to our work, Herzig et al. [13] used association rules to identify false alert patterns in the specific case of
system and integration tests that contain steps. They evaluated their approach during the development of Windows 8.1

and Microsoft Dynamics AX. They achieved a precision between 85% and 90% at detecting false testing alerts but had a
low recall between 34% and 48%. In our work, we cover a more varied set of tests, i.e., GUI, integration, and unit tests.
Besides, we investigate a new set of dynamic properties (run properties and artefacts) that allow us to reach better

performances (precision between 96% and 88% and a recall around 93%).

In an industrial context, Kowalczyk et al. [15] implemented a flakiness scoring service at Apple. Their model quantifies
the level of flakiness based on their historical flip rate and entropy (i.e., changes in test outcomes across different
revisions). Their goal was to identify and rank flaky tests to monitor and detect trends in flakiness. They were able
to reduce flakiness by 44% with less than 1% loss in fault detection. In their assessment of test transitions, Leong et
al. [19] showed that flakiness can impact regression testing techniques. In particular, they showed that the strategy of
transition-based prioritization, which reruns failing tests first, performs poorly because it automatically prioritizes

flaky tests. These findings highlighted the need for testing solutions that take flakiness into consideration.

9 CONCLUSION

This paper addresses the rising issue of false testing alerts that mislead software developers and hinder the flow of

continuous integration. Our objective is to propose an approach for discerning legitimate failures from the lots of false

alerts with high effectiveness and efficiency. Therefore, we proposed Fair, a novel approach for failure classification
using a lightweight set of features easily retrievable at runtime containing both dynamic properties (e.g., run duration,
run status, and logs) and static properties (e.g., test source and command). To build and evaluate our approach, we
relied on the continuous integration of the Chromium CI project. We collected test failures over a period of 1½ month

and analyzed over 1 million test failures. Our analysis shows that Fair can accurately detect legitimate failures even

when considering different categories of tests: GUI, integration, and unit tests. The distinction between false alerts

and legitimate failures capitalizes on failure properties like run duration and status but also uses the accompanying

artefacts and spots keywords that are linked to flaky behaviour. Finally, we assessed the cost of Fair as an alternative

to reruns for identifying legitimate failures. Once having a trained model — which took about four minutes in our case

Discerning Legitimate Failures From False Alerts

15

— Fair only requires 1 millisecond per build while reruns require 20 minutes. To facilitate the reuse of our dataset and
the replication of our study, we provide a comprehensive package including the used data and scripts3.

ACKNOWLEDGMENTS

This work is supported by the Facebook 2019 Testing and Verification research awards and PayPal.

REFERENCES

[1] Abdulrahman Alshammari, Christopher Morris, Michael Hilton, and Jonathan Bell. 2021. Flakeflagger: Predicting flakiness without rerunning tests.

In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 1572–1584.

[2] Jonathan Bell, Owolabi Legunsen, Michael Hilton, Lamyaa Eloussi, Tifany Yung, and Darko Marinov. 2018. DeFlaker: Automatically Detecting
Flaky Tests. In Proceedings of the 40th International Conference on Software Engineering - ICSE ’18. ACM Press, New York, New York, USA, 433–444.
https://doi.org/10.1145/3180155.3180164

[3] James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of machine learning research 13, 2 (2012).
[4] B. Camara, M. Silva, A. T. Endo, and S. Vergilio. 2021. What is the Vocabulary of Flaky Tests? An Extended Replication. In 2021 2021 IEEE/ACM 29th
International Conference on Program Comprehension (ICPC) (ICPC). IEEE Computer Society, Los Alamitos, CA, USA, 444–454. https://doi.org/10.
1109/ICPC52881.2021.00052

[5] Davide Chicco and Giuseppe Jurman. 2020. The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary

classification evaluation. BMC genomics 21, 1 (2020), 1–13.

[6] Chromium contributors. 2021. The Chromium Projects. https://www.chromium.org/. (Accessed on 08/17/2021).
[7] Jesse Davis and Mark Goadrich. 2006. The Relationship between Precision-Recall and ROC Curves. In Proceedings of the 23rd International
Conference on Machine Learning (Pittsburgh, Pennsylvania, USA) (ICML ’06). Association for Computing Machinery, New York, NY, USA, 233–240.
https://doi.org/10.1145/1143844.1143874

[8] Saikat Dutta, August Shi, Rutvik Choudhary, Zhekun Zhang, Aryaman Jain, and Sasa Misailovic. 2020. Detecting flaky tests in probabilistic and
machine learning applications. ISSTA 2020 - Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis (2020),
211–224. https://doi.org/10.1145/3395363.3397366

[9] Moritz Eck, Marco Castelluccio, Fabio Palomba, and Alberto Bacchelli. 2019. Understanding Flaky Tests: The Developer’s Perspective. arXiv (2019),

830–840.

[10] Yoav Goldberg. 2017. Neural network methods for natural language processing. Synthesis lectures on human language technologies 10, 1 (2017),

1–309.

[11] Martin Gruber, Stephan Lukasczyk, Florian Krois, and Gordon Fraser. 2021. An Empirical Study of Flaky Tests in Python. Proceedings - 2021 IEEE 14th
International Conference on Software Testing, Verification and Validation, ICST 2021 (2021), 148–158. https://doi.org/10.1109/ICST49551.2021.00026
[12] Guillaume Haben, Sarra Habchi, Mike Papadakis, Maxime Cordy, and Yves Le Traon. 2021. A Replication Study on the Usability of Code Vocabulary

in Predicting Flaky Tests. Proceedings of the International Conference on Mining Software Repositories (MSR) (2021).

[13] Kim Herzig and Nachiappan Nagappan. 2015. Empirically Detecting False Test Alarms Using Association Rules. Proceedings - International Conference

on Software Engineering 2 (2015), 39–48. https://doi.org/10.1109/ICSE.2015.133

[14] Tariq M King, Dionny Santiago, Justin Phillips, and Peter J Clarke. 2018. Towards a Bayesian Network Model for Predicting Flaky Automated Tests.
2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C) (2018), 100–107. https://doi.org/10.1109/QRS-
C.2018.00031

[15] Emily Kowalczyk, Karan Nair, Zebao Gao, Leo Silberstein, Teng Long, and Atif Memon. 2020. Modeling and Ranking Flaky Tests at Apple. In
Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice (Seoul, South Korea) (ICSE-SEIP
’20). Association for Computing Machinery, New York, NY, USA, 110–119. https://doi.org/10.1145/3377813.3381370

[16] Wing Lam, Patrice Godefroid, Suman Nath, Anirudh Santhiar, and Suresh Thummalapenta. 2019. Root Causing Flaky Tests in a Large-Scale
Industrial Setting. In Proceedings ofthe 28th ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA ’19). ACM Press, Beijing,
China, 101–111. https://doi.org/10.1145/3293882.3330570

[17] Wing Lam, Reed Oei, August Shi, Darko Marinov, and Tao Xie. 2019.

IDFlakies: A framework for detecting and partially classifying flaky
tests. Proceedings - 2019 IEEE 12th International Conference on Software Testing, Verification and Validation, ICST 2019 (2019), 312–322. https:
//doi.org/10.1109/ICST.2019.00038

[18] Wing Lam, Stefan Winter, Anjiang Wei, Tao Xie, Darko Marinov, and Jonathan Bell. 2020. A large-scale longitudinal study of flaky tests. Proceedings

of the ACM on Programming Languages 4, OOPSLA (2020), 1–29. https://doi.org/10.1145/3428270

[19] Claire Leong, Abhayendra Singh, Mike Papadakis, Yves Le Traon, and John Micco. 2019. Assessing transition-based test selection algorithms at
Google. In Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice, ICSE (SEIP) 2019, Montreal, QC,
Canada, May 25-31, 2019, Helen Sharp and Mike Whalen (Eds.). IEEE / ACM, 101–110. https://doi.org/10.1109/ICSE-SEIP.2019.00019

3https://github.com/GuillaumeHaben/FAIR-ReplicationPackage

16

Haben et al.

[20] Qingzhou Luo, Farah Hariri, Lamyaa Eloussi, and Darko Marinov. 2014. An empirical analysis of flaky tests. In Proceedings of the ACM SIGSOFT
Symposium on the Foundations of Software Engineering, Vol. 16-21-November-2014. Association for Computing Machinery, 643–653. https:
//doi.org/10.1145/2635868.2635920

[21] John Micco. 2017. The State of Continuous Integration Testing Google.
[22] Jason Palmer. 2019. Test Flakiness – Methods for identifying and dealing with flaky tests : Spotify Engineering. https://engineering.atspotify.com/

2019/11/18/test-flakiness-methods-for-identifying-and-dealing-with-flaky-tests/. (Accessed on 01/12/2021).

[23] Gustavo Pinto, Breno Miranda, Supun Dissanayake, Marcelo D’Amorim, Christoph Treude, and Antonia Bertolino. 2020. What is the Vocabulary
of Flaky Tests? Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020 (2020), 492–502. https:
//doi.org/10.1145/3379597.3387482

[24] M. Tajmilur Rahman and Peter C. Rigby. 2018. The impact of failing, flaky, and high failure tests on the number of crash reports associated with
firefox builds. ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering (2018), 857–862. https://doi.org/10.1145/3236024.3275529

[25] Max Rehkopf. [n.d.]. What is Continuous Integration | Atlassian. https://www.atlassian.com/continuous-delivery/continuous-integration. (Accessed

on 01/12/2021).

[26] Alan Romano, Zihe Song, Sampath Grandhi, Wei Yang, and Weihang Wang. 2021. An Empirical Analysis of UI-based Flaky Tests. In 2021 IEEE/ACM

43rd International Conference on Software Engineering (ICSE). IEEE, 1585–1597.

[27] August Shi, Wing Lam, Reed Oei, Tao Xie, and Darko Marinov. 2019. iFixFlakies : A Framework for Automatically Fixing Order-Dependent Flaky
Tests. In 27th ACM Joint European Software Engineering Conference and Symposium on the Foundations ofSoftware Engineering (ESEC/FSE ’19).
https://doi.org/10.1145/3338906.3338925

[28] The Chromium Development team. 2021. chromium/chromium: The official GitHub mirror of the Chromium source. https://github.com/chromium/

chromium. (Accessed on 07/06/2021).

[29] Swapna Thorve, Chandani Sreshtha, and Na Meng. 2018. An empirical study of flaky tests in android apps. Proceedings - 2018 IEEE International

Conference on Software Maintenance and Evolution, ICSME 2018 (2018), 534–538. https://doi.org/10.1109/ICSME.2018.00062

[30] Shivashree Vysali Vaidhyam Subramanian, Shane McIntosh, and Bram Adams. 2020. Quantifying, Characterizing, and Mitigating Flakily Covered

Program Elements. IEEE Transactions on Software Engineering (2020), 1–1. https://doi.org/10.1109/TSE.2020.3010045

