Fundamental Performance Limits for Sensor-Based
Robot Control and Policy Learning

Anirudha Majumdar and Vincent Pacelli
Department of Mechanical and Aerospace Engineering
Princeton University, Princeton, NJ, 08540
Emails: {ani.majumdar, vpacelli}@princeton.edu

2
2
0
2

y
a
M
5

]

O
R
.
s
c
[

2
v
9
2
1
0
0
.
2
0
2
2
:
v
i
X
r
a

Abstract—Our goal is to develop theory and algorithms for
establishing fundamental limits on performance for a given task
imposed by a robot’s sensors. In order to achieve this, we
deﬁne a quantity that captures the amount of task-relevant
information provided by a sensor. Using a novel version of
the generalized Fano inequality from information theory, we
demonstrate that this quantity provides an upper bound on the
highest achievable expected reward for one-step decision making
tasks. We then extend this bound to multi-step problems via
a dynamic programming approach. We present algorithms for
numerically computing the resulting bounds, and demonstrate
our approach on three examples: (i) the lava problem from
the literature on partially observable Markov decision processes,
(ii) an example with continuous state and observation spaces
corresponding to a robot catching a freely-falling object, and
(iii) obstacle avoidance using a depth sensor with non-Gaussian
noise. We demonstrate the ability of our approach to establish
strong limits on achievable performance for these problems by
comparing our upper bounds with achievable lower bounds
(computed by synthesizing or learning concrete control policies).

I. INTRODUCTION

Robotics is often characterized as the problem of transform-
ing “pixels to torques” [1]: how can an embodied agent convert
raw sensor inputs into actions in order to accomplish a given
task? In this paper, we seek to understand the fundamental
limits of this process by studying the following question: is
there an upper bound on performance imposed by the sensors
that a robot is equipped with?

As a motivating example, consider the recent debate around
the “camera-only” approach to autonomous driving favored
by Tesla versus the “sensor-rich” philosophy pursued by
Waymo [2]. Is an autonomous vehicle equipped only with
cameras fundamentally limited in terms of the performance or
safety it can achieve? By “fundamental limit”, we mean a bound
on performance (or safety) on a given task that holds regardless
of the form of control policy one utilizes (e.g., a neural network
with billions of parameters, a nonlinear model predictive control
scheme combined with a particle ﬁlter, etc.), how the policy is
synthesized (e.g., via model-free reinforcement learning, model-
based control, etc.), or how much computation is available to
the robot or software designer.

While there have been tremendous algorithmic advancements
in robotics over decades, we currently lack a “science” for
understanding such fundamental limits [3]. Current practice in
robotics is often empirical in nature (e.g., trying different
perception and control architectures with neural networks

Fig. 1. Our goal is to establish fundamental limits on performance for a given
task imposed by a robot’s sensors. We demonstrate our approach on examples
that include obstacle avoidance with a noisy depth sensor (ﬁg. left) using
motion primitives (ﬁg. right). We also show how we can use our approach to
establish the superiority of one sensor (Sensor 1: a dense depth sensor) over
another (Sensor 2: a sparse depth sensor) for a given task.

of growing size and varying architectures). Techniques for
establishing fundamental limits imposed by a sensor would
potentially allow us to glean important design insights (e.g.,
realizing that a particular sensor is not sufﬁcient for a task and
must be replaced). Further, such techniques could allow us to
establish the superiority of one suite of sensors over another
from the perspective of a given task, e.g., by synthesizing
a control policy for one sensor suite that achieves better
performance than the fundamental bound for another suite.

In this paper, we take a step towards this goal. We ﬁrst
observe that any technique for establishing fundamental bounds
on performance imposed by a given sensor must take into
account two factors: (i) the quality of the sensor (e.g., the
amount of information about the state of the robot and its
environment provided by the sensor), and, importantly, (ii) the
task that the robot is meant to accomplish. As an example,
consider a drone equipped with a (noisy) depth sensor (Fig. 1).
Depending on the nature of the task, the robot may need more
or less information from its sensors. For example, suppose
that the obstacle locations are highly constrained such that
a particular sequence of actions always succeeds in avoiding
them (i.e., there is a purely open-loop policy that achieves
good performance on the task); in this case, even an extremely
noisy or sparse depth sensor allows the robot to perform well.
However, if the distribution of obstacles is such that there is no

XYSensor 1Sensor 2Actions 
 
 
 
 
 
pre-deﬁned gap in the obstacles, then a noisy or sparse depth
sensor may fundamentally limit the achievable performance
on the task. The achievable performance is thus intuitively
inﬂuenced by the amount of task-relevant information provided
by the robot’s sensors.

Statement of contributions. Our primary contribution is to
develop theory and algorithms for establishing fundamental
bounds on performance imposed by a robot’s sensors for a
given task. Our key insight is to deﬁne a quantity that captures
the task-relevant information provided by the robot’s sensors.
Using a novel version of the generalized Fano inequality from
information theory, we demonstrate that this quantity provides
a fundamental upper bound on expected reward for one-step
decision making problems. We then extend this bound to multi-
step settings via a dynamic programming approach and propose
algorithms for computing the resulting bounds for systems with
potentially continuous state and observation spaces, nonlinear
and stochastic dynamics, and non-Gaussian sensor models (but
with discretized action spaces). We demonstrate our approach
on three examples: (i) the lava problem from the literature
on partially observable Markov decision processes (POMDPs),
(ii) a robot catching a freely-falling object, and (iii) obstacle
avoidance using a depth sensor (Fig. 1). We demonstrate the
strength of our bounds on these examples by comparing them
against the performance achieved by concrete control policies:
the optimal POMDP solution for the lava problem, a model-
predictive control (MPC) scheme for the catching example,
and a learned neural network policy for the obstacle avoidance
problem. We also present applications of our approach for
establishing the superiority of one sensor over another (from
the perspective of a given task). To our knowledge, the results
in this paper are the ﬁrst to provide general-purpose techniques
for establishing fundamental bounds on performance for sensor-
based control of robots.

A. Related Work

Domain-speciﬁc performance bounds. Prior work in
robotics has established fundamental bounds on performance
for particular problems. For example, [4, 5] consider high-
speed navigation through an (ergodic) forest consisting of
randomly-placed obstacles. Results from percolation theory [6]
are used to establish a critical speed beyond which there
does not exist (with probability one) an inﬁnite collision-free
trajectory. The work in [7] establishes limits on the speed at
which a robot can navigate through unknown environments in
terms of perceptual latency. Classical techniques from robust
control [8] have also been utilized to establish fundamental
limits on performance for control tasks (e.g., pole balancing)
involving linear output-feedback control and sensor noise
or delays [9]. The results in [10] demonstrate empirical
correlation of the complexity metrics presented in [9] with
sample-efﬁciency and performance of learned perception-based
controllers on a pole-balancing task. The approaches mentioned
above consider speciﬁc tasks (e.g., navigation in ergodic forests)
or relatively narrow classes of problems (e.g., linear output-
feedback control). In contrast, our goal is to develop a general

and broadly applicable theoretical and algorithmic framework
for establishing fundamental bounds on performance imposed
by a sensor for a given task.

Comparing sensors. The notion of a sensor lattice was
introduced in [11, 12] for comparing the power of different
sensors (see [13] for a similar approach for comparing robots).
The sensor lattice provides a partial ordering on different
sensors based on the ability of one sensor to simulate another.
However, most pairs of sensors are incomparable using such
a scheme. Moreover, the sensor lattice does not establish the
superiority of one sensor over another from the perspective of
a given task; instead, the partial ordering is based on the ability
of one sensor to perform as well as another in terms of ﬁltering
(i.e., state estimation). In this paper, we also demonstrate the
applicability of our approach for comparing different sensors.
However, this comparison is task-driven; we demonstrate how
one sensor can be proved to be fundamentally better than
another from the perspective of a given task.

Fano’s inequality and its extensions. In its original form,
Fano’s inequality [14] relates the lowest achievable error of
estimating a signal x from an observation y in terms of the
noise in the channel that produces observations from signals. In
recent years, Fano’s inequality has been signiﬁcantly extended
and applied for establishing fundamental limits for various
statistical estimation problems, e.g., lower bounding the Bayes
and minimax risks for different learning problems [15–17]. In
this paper, we build on generalized versions of Fano’s inequality
[16, 17] in order to obtain fundamental bounds on performance
for robotic systems with noisy sensors. On the technical front,
we contribute by deriving a stronger version of the generalized
Fano inequalities presented in [16, 17] by utilizing the inverse of
the KL divergence (Sec. III) and computing it using geometric
programming [18, Ch. 4.5]. The resulting inequality, which
may be of independent interest, allows us to derive fundamental
upper bounds on performance for one-step decision making
problems. We then develop a dynamic programming approach
for recursively applying the generalized Fano inequality in order
to derive bounds on performance for multi-step problems.

II. PROBLEM FORMULATION

A. Notation

We denote sequences by xi:j

k=i for i ≤ j.
Expectations are typically denoted as E[·] with the variable of
integration or its measure appearing below it for contextual
emphasis, e.g.: E
x

[·], E
p(x)

[·].

:= (xk)j

B. Problem Statement

We denote the state of the robot and its environment
at time-step t by st ∈ S. Let p0 denote the initial state
distribution. Let the robot’s sensor observation and control
action at time-step t be denoted by ot ∈ O and at ∈ A
respectively. Denote the (stochastic) dynamics of the state
by pt(st|st−1, at−1) and suppose that the robot’s sensor is
described by σt(ot|st). The robot’s task is prescribed using
reward functions r0, r1, . . . , rT −1 : S × A → R at each time-
step (up to a ﬁnite horizon).

Assumption 1 (Bounded rewards). We assume that rewards
are bounded, and without further loss of generality we assume
that rt(st, at) ∈ [0, 1], ∀st ∈ S, at ∈ A, t ∈ {0, . . . , T − 1}.

The robot’s goal

is to ﬁnd a (potentially time-varying
and history-dependent) control policy πt : Ot+1 → A that
maximizes the total expected reward:

R(cid:63) := sup
π0:T −1

(cid:34) T −1
(cid:88)

t=0

E
s0:T −1
o0:T −1

rt(st, πt(o0:t))

(cid:35)
.

(1)

Goal: Our goal is to upper bound the best achievable expected
reward R(cid:63) for a given sensor σ0:T −1. We note that we are
allowing for completely general policies that are arbitrary time-
varying functions of the entire history of observations received
up to time t (as long as the functions satisfy measurability
conditions that ensure the existence of the expectation in (1)).
An upper bound on R(cid:63) thus provides a fundamental bound
on achievable performance that holds regardless of how the
policy is parameterized (e.g., via neural networks, receding-
horizon control architectures, etc.) or synthesized (e.g., via
reinforcement learning, optimal control techniques, etc.).

III. BACKGROUND

In this section, we brieﬂy introduce some background

material that will be useful throughout the paper.

A. KL Divergence and Mutual Information

The Kullback-Leibler (KL) divergence between two distri-

butions is deﬁned as:

(cid:34)

D(p(x)(cid:107)q(x)) := E
p(x)

log

(cid:35)
.

p(x)
q(x)

(2)

The mutual information between two random variables x

and y is deﬁned as:

I(x; y) := D

(cid:16)

p(x, y)(cid:107)p(x)p(y)

(cid:17)

,

(3)

where p(x, y) is the joint distribution, and p(x) and p(y) are
the resulting marginal distributions. This quantity captures the
amount of information one obtains about one random variable
(e.g., the state st) by observing another random variable (e.g.,
sensor observations ot).

B. Inverting Bounds on the KL Divergence

Let Bp and Bq be Bernoulli distributions on {0, 1} with

mean p and q respectively. For p, q ∈ [0, 1], we deﬁne1:

DB(p(cid:107)q) := D(Bp(cid:107)Bq) = p log

p
q

+ (1 − p) log

1 − p
1 − q

.

In subsequent sections, we will obtain bounds on a quantity
p(cid:63) ∈ [0, 1] given by DB(p(cid:63)(cid:107)q) ≤ c for some q ∈ [0, 1] and
c ≥ 0. In order to upper bound p(cid:63), we will use the KL inverse:

D−1(q|c) := sup {p ∈ [0, 1] | DB(p(cid:107)q) ≤ c}.

(4)

1We adopt the commonly used convention when working with the KL
divergence of taking 0 · log(0/a) := 0, ∀a, and a · log(a/0) := ∞, ∀a > 0.

It is then easy to see that p(cid:63) ≤ D−1(q|c).

Since DB(·(cid:107)·) is (jointly) convex in both arguments, the
optimization problem in (4) is a convex problem. In particular,
one can compute the KL inverse efﬁciently using a geometric
program [18, Ch. 4.5] with a single decision variable p.

IV. PERFORMANCE BOUND FOR SINGLE-STEP PROBLEMS

In this section, we will derive an upper bound on the best
achievable reward R(cid:63) in the single time-step decision-making
setting. This bound will then be extended to the multi-step
setting in Sec. V.

When T = 1, our goal is to upper bound the following

quantity:

R(cid:63)(σ0; r0) := sup
π0
= sup
π0

E
s0,o0
E
p0(s0)

[r0(s0, π0(o0))]

E
σ0(o0|s0)

[r0(s0, π0(o0))].

(5)

(6)

The notation R(cid:63)(σ0; r0) highlights the dependence of the best
achievable reward in terms of the robot’s sensor and task (as
speciﬁed by the reward function). As highlighted in Sec. I, the
amount of information that the robot requires from its sensors in
order to obtain high expected reward depends on its task; certain
tasks may admit purely open-loop policies that obtain high
rewards, while other tasks may require high-precision sensing
of the state. We formally deﬁne a quantity that captures this
intuition and quantiﬁes the task-relevant information provided
by the robot’s sensors. We then demonstrate that this quantity
provides an upper bound on R(cid:63)(σ0; r0).
Deﬁnition 1 (Task-relevant information potential). Let I(o0; s0)
be the mutual information between the robot’s sensor observa-
tion and state. Deﬁne:

R⊥

0 := sup
a0

E
s0

[r0(s0, a0)]

(7)

as the highest achievable reward using an open-loop policy.
Then deﬁne the task-relevant information potential (TRIP) of a
sensor σ0 for a task speciﬁed by reward function r0 as:

τ (σ0; r0) := D−1(R⊥

0 |I(o0; s0)).

(8)

In order to interpret the TRIP, we state two useful properties

of the KL inverse.

Proposition 1 (Monotonicity of KL inverse). The KL inverse
D−1(q|c) is:

1) monotonically non-decreasing in c ≥ 0 for ﬁxed q ∈

[0, 1],

2) monotonically non-decreasing in q ∈ [0, 1] for ﬁxed

c ≥ 0.

Proof: The ﬁrst property follows from the fact
that
increasing c loosens the KL divergence constraint in the
optimization problem in (4). The proof of the second property
utilizes the envelope theorem [19, Corollary 5] and is provided
in Appendix A (Lemma 4).

The TRIP τ (σ0; r0) depends on two factors: the mutual
information I(o0; s0) (which depends on the robot’s sensor) and

the best reward R⊥
0 achievable by an open-loop policy (which
depends on the robot’s task). Using Proposition 1, we see that
as the sensor provides more information about the state (i.e., as
I(o0; s0) increases for ﬁxed R⊥
0 ), the TRIP is monotonically
non-decreasing. Moreover, the TRIP is a monotonically non-
0 for ﬁxed I(o0; s0). This qualitative
decreasing function of R⊥
dependence is intuitively appealing: if there is a good open-loop
policy (i.e., one that achieves high reward), then the robot’s
sensor can provide a small amount of information about the
state and still lead to good overall performance. The speciﬁc
form of the deﬁnition of TRIP is motivated by the result
below, which demonstrates that the TRIP upper bounds the
best achievable expected reward R(cid:63)(σ0; r0) in Eq. (5).

Theorem 1 (Single-step performance bound). The best achiev-
able reward is upper bounded by the task-relevant information
potential (TRIP) of a sensor:

τ (σ0; r0) ≥ R(cid:63)(σ0; r0) := sup
π0

E
s0,o0

[r0(s0, π0(o0))].

(9)

Proof: The proof is provided in Appendix A and is inspired
by the proof of the generalized Fano inequality presented in [17,
Proposition 14]. The bound (9) tightens the generalized Fano
inequality [16, 17] by utilizing the KL inverse (in contrast to
the methods in [16, 17], which may be interpreted as indirectly
bounding the KL inverse). The result presented here may thus
be of independent interest.

Theorem 1 provides a fundamental bound on performance
(in the sense of Sec. I) imposed by the sensor for a given
single-step task. This bound holds for any policy, independent
of its complexity or how it is synthesized or learned.

V. PERFORMANCE BOUND FOR MULTI-STEP PROBLEMS:
FANO’S INEQUALITY WITH FEEDBACK

In this section, we derive an upper bound on the best
achievable reward R(cid:63) deﬁned in (1) for the general multi time-
step setting. The key idea is to extend the single-step bound
from Theorem 1 using a dynamic programming argument.

Let πt

k : Ok−t+1 → A denote a policy that takes as input
the sequence of observations ot:k from time-step t to k (for
k ≥ t). Thus, a policy π0
k at time-step k utilizes all observations
received up to time-step k. Given an initial state distribution
p0 and and an open-loop action sequence a0:t−1, deﬁne the
reward-to-go from time t ∈ {0, . . . , T − 1} given a0:t−1 as:

Rt :=

E
st:T −1,ot:T −1|
a0:t−1

(cid:34) T −1
(cid:88)

k=t

rk(sk, πt

k(ot:k))

(cid:35)
,

(10)

where the expectation,

the open-loop sequence of actions from time-steps2 0 to t − 1,
and then applies the closed-loop policies πt
from time-steps t to T − 1. We further deﬁne RT := 0.

t+1, . . . , πt

t, πt

T −1

Now, for t ∈ {0, . . . , T − 1}, deﬁne:

(cid:34)

R⊥
t

:= sup
at

E
st|a0:t−1

(cid:104)

(cid:105)
rt(st, at)

+ Rt+1

(cid:35)
,

and

R⊥(cid:63)
t

:=

sup
t+1 ,...,πt+1
πt+1

T −1

R⊥
t .

(13)

(14)

The following result then leads to a recursive structure for

computing an upper bound on R(cid:63).

Proposition 2 (Recursive bound). For any t = 0, . . . , T − 1,
the following inequality holds for any open-loop sequence of
actions a0:t−1:

sup
t ,...,πt
πt

T −1

(cid:124)

Rt ≤ (T − t) · D−1

| I(ot; st)

(15)

(cid:32)

R⊥(cid:63)
t
T − t
(cid:123)(cid:122)
=:τt(σt:T −1;rt:T −1)

(cid:33)
.

(cid:125)

Proof: The proof follows a similar structure to Theorem 1

and is presented in Appendix A.

To see how we can use Proposition 2, we ﬁrst use (1) and

(10) to note that the LHS of (15) for t = 0 is equal to R(cid:63):

R(cid:63) = sup
0 ,...,π0
π0

T −1

R0 ≤ τ0(σ0:T −1; r0:T −1) .

(16)

The quantity τt(σt:T −1; rt:T −1) may be interpreted as a
multi-step version of the TRIP from Deﬁnition 1. This quantity
depends on the mutual information I(ot; st), which is computed
using the distribution pt(st|a0:t−1) over st that one obtains by
propagating p0 using the open-loop sequence of actions a0:t−1:

(cid:32)

I(ot; st) = D

pt(st|a0:t−1)σt(ot|st)

(cid:13)
(cid:13)
(cid:13)pt(st|a0:t−1)σt(ot)

(cid:33)

. (17)

t

In addition, τt(σt:T −1; rt:T −1) depends on R⊥(cid:63)

, which is
then divided by (T − t) to ensure boundedness between [0, 1]
(see Assumption 1). The quantity R⊥(cid:63)
can itself be upper
bounded using (15) with t + 1, as we demonstrate below. Such
an upper bound on R⊥(cid:63)
for t = 0 leads to an upper bound
on R(cid:63) using (16) and the monotonicity of the KL inverse
(Proposition 1). Applying this argument recursively leads to
Algorithm 1, which computes an upper bound on R(cid:63).

t

t

E
st:T −1,ot:T −1|
a0:t−1

(cid:104)

(cid:105)

·

(11)

2For t = 0, we use the convention that a0:−1 is the empty sequence and:

E
s0:T −1,o0:T −1|
a0:−1

(cid:105)

(cid:104)

·

:=

E
s0:T −1,o0:T −1

(cid:104)

(cid:105)

.

·

(12)

is taken with respect to the distribution of states st:T −1 and
observations ot:T −1 one receives if one propagates p0 using

Algorithm 1 Multi-Step Performance Bound
1: Initialize ¯RT (a0:T −1) = 0, ∀a0:T −1.
2: for t = T − 1, T − 2, . . . , 0 do
¯Rt(a0:t−1) = (T − t) · D−1(cid:16) ¯R⊥(cid:63)
where ¯R⊥(cid:63)
:= sup
at

(cid:12)
(cid:12)
t
(cid:12)
T −t
rt(st, at)

E
st|a0:t−1

3:

(cid:104)

t

(cid:17)
I(ot; st)
(cid:105)

, ∀a0:t−1,
+ ¯Rt+1(a0:t).

4: end for
5: return ¯R0 (bound on achievable expected reward).

Theorem 2 (Multi-step performance bound). Algorithm 1
returns an upper bound on the best achievable reward R(cid:63).

Proof: We provide a sketch of the proof here, which uses
(backwards) induction. In particular, Proposition 2 leads to the
inductive step. See Appendix A for the complete proof.

We prove that for all t = T − 1, . . . , 0,

sup
t ,...,πt
πt

T −1

Rt ≤ ¯Rt(a0:t−1), ∀a0:t−1.

(18)

Thus, in particular,

R(cid:63) = sup
0 ,...,π0
π0

T −1

R0 ≤ ¯R0.

(19)

We prove (18) by backwards induction starting from t =

T − 1. We ﬁrst prove the base step. Using (15), we obtain:

RT −1 ≤ D−1(cid:16)

(cid:17)
T −1 | I(oT −1; sT −1)

R⊥(cid:63)

.

(20)

sup
πT −1
T −1

T −1 = ¯R⊥(cid:63)
Using the fact that RT = 0, we can show that R⊥(cid:63)
T −1.
Combining this with (20) and the monotonicity of the KL
inverse (Proposition 1), we see:
RT −1 ≤ D−1(cid:16) ¯R⊥(cid:63)

(cid:17)
T −1 | I(oT −1; sT −1)

(21)

sup
πT −1
T −1

= ¯RT −1(a0:T −2).

(22)

In order to prove the induction step, suppose that for t ∈
{0, . . . , T − 2}, we have

sup
t+1 ,...,πt+1
πt+1

T −1

Rt+1 ≤ ¯Rt+1(a0:t).

(23)

We then need to show that

sup
πt
t ,...,πt

T −1

Rt ≤ ¯Rt(a0:t−1).

(24)

We can use the induction hypothesis (23) to show that R⊥(cid:63)
t ≤
¯R⊥(cid:63)
. Combining this with (15) and the monotonicity of the
t
KL inverse (Proposition 1), we obtain the desired result (24):

sup
t ,...,πt
πt

T −1

Rt ≤ (T − t) · D−1

= ¯Rt(a0:t−1).

(cid:32) ¯R⊥(cid:63)
t
T − t

(cid:33)

| I(ot; st)

(25)

(26)

VI. NUMERICAL IMPLEMENTATION

In order to compute the single-step bound using Theorem 1 or
the multi-step bound using Algorithm 1, we require the ability
to compute (or bound) three quantities: (i) the KL inverse, (ii)
the mutual information I(ot; st), and (iii) the quantity ¯R⊥(cid:63)
.
As described in Sec. III-B, we can compute the KL inverse
efﬁciently using a geometric program (GP) [18, Ch. 4.5] with
a single decision variable. There are a multitude of solvers for
GPs including Mosek [20] and the open-source solver SCS [21].
Next, we describe the computation of I(ot; st) and ¯R⊥(cid:63)
in
different settings.

t

t

A. Analytic Computation

In certain settings, one can compute I(ot; st) and ¯R⊥(cid:63)

t

exactly. We discuss two such settings of interest below.

Discrete POMDPs. In cases where the state space S, action
space A, and observation space O are ﬁnite, one can compute
I(ot; st) exactly by propagating the initial state distribution p0
forward using open-loop action sequences a0:t−1 and using
the expression (17) (which can be evaluated exactly since we
have discrete probability distributions). The expectation term
in ¯R⊥(cid:63)
can be computed similarly. In addition, the supremum
t
over actions can be evaluated exactly via enumeration.

Linear-Gaussian systems with ﬁnite action spaces. One
can also perform exact computations in cases where (i) the
state space S is continuous and the dynamics pt(st|st−1, at−1)
are given by a linear dynamical system with additive Gaussian
uncertainty, (ii) the observation space O is continuous and the
sensor model σt(ot|st) is such that the observations are linear
(in the state) with additive Gaussian uncertainty, (iii) the initial
state distribution p0 is Gaussian, and (iv) the action space A
is ﬁnite. In such settings, one can analytically propagate p0
forward through open-loop action sequences a0:t−1 using the
fact that Gaussian distributions are preserved when propagated
through linear-Gaussian systems (similar to Kalman ﬁltering
[22]). One can then compute I(ot; st) using (17) by leveraging
the fact that all the distributions involved are Gaussian, for
which KL divergences can be computed in closed form [23].
One can also compute ¯R⊥(cid:63)
exactly for any reward function that
permits the analytic computation of the expectation term using
a Gaussian (e.g., quadratic reward functions); the supremum
over actions can be evaluated exactly since A is ﬁnite.

t

B. Computation via Sampling and Concentration Inequalities

General settings. Next, we consider more general settings
with: (i) continuous state and observation spaces, (ii) arbi-
trary (e.g, non-Gaussian/nonlinear) dynamics pt(st|st−1, at−1),
which are potentially not known analytically, but can be
sampled from (e.g., as in a simulator), (iii) arbitrary (e.g., non-
Gaussian/nonlinear) sensor σt(ot|st), but with a probability
density function that can be numerically evaluated given any
particular state-observation pair, (iv) an arbitrary initial state
distribution p0 that can be sampled from, and (v) a ﬁnite action
space. Our bound is thus broadly applicable, with the primary
restriction being the ﬁniteness of A; we leave extensions to
continuous action spaces for future work (see Sec. VIII).

We ﬁrst discuss the computation of ¯R⊥(cid:63)

. Since the suprem-
ization over actions can be performed exactly (due to the
ﬁniteness of A), the primary challenge here is to evaluate the
expectation:

t

(cid:104)
rt(st, at)

(cid:105)
.

E
st|a0:t−1

(27)

t

We note that any upper bound on this expectation leads to an
upper bound on ¯R⊥(cid:63)
, and thus a valid upper bound on R(cid:63)
(due to the monotonicity of the KL inverse; Proposition 1).
One can thus obtain a high-conﬁdence upper bound on (27)
by sampling states st|a0:t−1, and using any concentration
inequality [24]. In particular, since we assume boundedness of
rewards (Assumption 1), we can use Hoeffding’s inequality.

Theorem 3 (Hoeffding’s inequality [24]). Let z be a random
variable bounded within [0, 1], and let z1, . . . , zn denote i.i.d.
samples. Then, with probability at least 1−δ (over the sampling
of z1, . . . , zn), the following bound holds with probability at
least 1 − δ:

E[z] ≤

(cid:114)

zi +

1
n

n
(cid:88)

i=1

log(1/δ)
2n

.

(28)

In our numerical examples (Sec. VII), we utilize a slightly

tighter version of Hoeffding’s inequality (see Appendix B).

Next, we discuss the computation of I(ot; st). Again, we
note that any upper bound on I(ot; st) yields a valid upper
bound on R(cid:63) due to the monotonicity of the KL inverse. In this
work, we utilize variational bounds on mutual information; in
particular, we use the “leave-one-out bound” [25]:

(cid:34)
I(ot; st) ≤ E

1
K

(cid:34)

K
(cid:88)

i=1

log

1
K−1

σt(o[i]
(cid:80)

t |s[i]
t )
j(cid:54)=i σt(o[i]

t |s[j]
t )

(cid:35)(cid:35)

,

(29)

t , o[i]

where the expectation is over size-K batches {(s[i]
t )}K
i=1
of sampled states st|a0:t−1 and observations sampled using
σt(ot|st). The quantity σt(o[i]
t |s[i]
t ) denotes (with slight abuse
of notation) the evaluation of the density function corresponding
to the sensor model. Since the bound (29) is in terms of an
expectation, one can again obtain a high-conﬁdence upper
bound by sampling state-observation batches and applying a
concentration inequality (e.g., Hoeffding’s inequality if the
quantity inside the expectation is bounded).

We note that the overall implementation of Algorithm 1 may
involve the application of multiple concentration inequalities
(each of which holds with some conﬁdence 1 − δi). One can
obtain the overall conﬁdence of the upper bound on R(cid:63) by
using a union bound: 1 − δ = 1 − (cid:80)

i δi.

C. Tightening the Bound with Multiple Horizons

We end this section by discussing a ﬁnal implementation
detail. Let T denote the time horizon of interest (as in Sec. II).
For any H ∈ {1, . . . , T }, one can deﬁne

R(cid:63)

H := sup
π0:H−1

(cid:34) H−1
(cid:88)

t=0

E
s0:H−1
o0:H−1

(cid:35)

rt(st, πt(o0:t))

(30)

Fig. 2. An illustration of the lava problem. The robot needs to navigate to
a goal without falling into the lava (using a noisy sensor).

Fig. 3. Results for the lava problem. We compare the upper bounds on
achievable expected rewards computed by our approach with the optimal
POMDP solution for different values of sensor noise.

T ≤ R(cid:63)

as the best achievable reward for a problem with horizon H
(instead of T ). One can then apply Algorithm 1 to compute
an upper bound on R(cid:63)
H . Since rewards are assumed to be
bounded within [0, 1] (Assumption 1), we can observe that
R(cid:63) = R(cid:63)
H + (T − H). In practice, we sometimes ﬁnd
that this bound provides a tighter bound on R(cid:63) for some H < T
(as compared to directly applying Algorithm 1 with a horizon
of T ). For our numerical examples, we thus sweep through
different values for the horizon H and report the lowest upper
bound R(cid:63)

H + (T − H).

VII. EXAMPLES

We demonstrate our approach on three examples: (i) the
lava problem from the POMDP literature, (ii) an example
with continuous state and observation spaces corresponding
to a robot catching a freely-falling object, and (iii) obstacle
avoidance using a depth sensor with non-Gaussian noise. We
illustrate the strength of our upper bounds on these examples by
comparing them against the performance achieved by concrete
control policies (i.e., lower bounds on achievable performance).
We also demonstrate the applicability of our approach for
establishing the superiority of one sensor over another (from
the perspective of a given task). Code for all examples can be
found at: https://github.com/irom-lab/performance-limits.

A. Lava Problem

The ﬁrst example we consider is the lava problem (Fig. 2)

[26–28] from the POMDP literature.

Dynamics. The setting consists of ﬁve discrete states (Fig. 2)
and two actions (left and right). If the robot falls into the
lava state, it remains there (i.e., the lava state is absorbing). If
the robot attempts to go left from state 1, it remains at state 1.
The initial state distribution p0 is chosen to be uniform over
the non-lava states.

Sensor. The robot is equipped with a sensor that provides a
(noisy) state estimate. The sensor reports the correct state (i.e.,

123: Goal45: LavaWall0.00.20.40.60.81.0pcorrect135RewardBoundOptimalot = st) with probability pcorrect (and a uniformly randomly
chosen incorrect state with probability 1 − pcorrect).

Rewards. The robot’s objective is to navigate to the goal
state (which is an absorbing state), within a time horizon of T =
5. This objective is encoded via a reward function rt(st, at),
which is purely state-dependent. The reward associated with
being in the lava is 0; the reward associated with being at the
goal is 1; the reward at all other states is 0.1.

Results. An interesting feature of this problem is that it
admits a purely open-loop policy that achieves a high expected
reward. In particular, consider the following sequence of actions:
left, right, right. No matter which state the robot starts
from, this sequence of actions will steer the robot to the
goal state (recall that the goal is absorbing). Given the initial
distribution and rewards above, this open-loop policy achieves
an expected reward of 3.425. Suppose we set pcorrect = 1/5
(i.e., the sensor just uniformly randomly returns a state estimate
and thus provides no information regarding the state). In this
case, Algorithm 1 returns an upper bound: 3.5 ≥ R(cid:63).

Next, we plot the upper bounds provided by Algorithm 1
for different values of sensor noise by varying pcorrect. The
resulting bounds are shown in Fig. 3. Since the lava problem is
a ﬁnite POMDP, one can compute R(cid:63) exactly using a POMDP
solver. Fig. 3 compares the upper bounds on R(cid:63) returned
by Algorithm 1 with R(cid:63) computed using the pomdp_py
package [29]. The ﬁgure illustrates that our approach provides
strong bounds on the best achievable reward for this problem.
We also note that computation of the POMDP solution (for
each value of pcorrect) takes ∼20s, while the computation of
the bound takes ∼0.2s (i.e., ∼ 100× faster).

B. Catching a Falling Object

Next, we consider a problem with continuous state and
observation spaces. The goal of the robot is to catch a freely-
falling object such as a ball (Fig. 4).

t , yrel

, vy,ball
t

t , vx,ball
t

Dynamics. We describe the four-dimensional state of the
] ∈ R4, where
t , yrel
robot-ball system by st := [xrel
[xrel
t ] is the relative position of the ball with respect to the
robot, and [vx,ball
] corresponds to the ball’s velocity. The
action at is the horizontal speed of the robot and can be chosen
within the range [−0.4, 0.4]m/s (discretized in increments of
0.1m/s). The dynamics of the system are given by:



, vy,ball
t

t



t + (vx,ball
xrel

t

− at)∆t

st+1 =










xrel

t+1
yrel
t+1
vx,ball
t+1
vy,ball
t+1

=
















t

t + ∆tvy,ball
yrel
vx,ball
t
vy,ball
t − g∆t

,









(31)

where ∆t = 1 is the time-step and g = 0.1m/s2 is chosen such
that the ball reaches the ground within a time horizon of T = 5.
The initial state distribution p0 is chosen to be a Gaussian with
mean [0.0m, 1.05m, 0.0m/s, 0.05m/s] and diagonal covariance
matrix diag([0.012, 0.12, 0.22, 0.12]).

Sensor. The robot’s sensor provides a noisy state esti-
mate ot = st + (cid:15)t, where (cid:15)t is drawn from a Gaussian

Fig. 4. An illustration of the ball-catching example with continuous state
and observation spaces. The robot is constrained to move horizontally along
the ground and can control its speed. Its goal is to track the position of the
falling ball using a noisy estimate of the ball’s state.

Fig. 5. Results for the ball-catching example. We compare the upper bounds on
achievable expected rewards with the expected rewards using MPC combined
with Kalman ﬁltering for different values of sensor noise (results for MPC are
averaged across ﬁve evaluation seeds; the std. dev. across seeds is too small
to visualize).

distribution with zero mean and diagonal covariance matrix
η · diag([0.52, 1.02, 0.752, 1.02]). Here, η is a noise scale that
we will vary in our experiments.

Rewards. The reward function rt(st, at) is chosen to
encourage the robot to track the ball’s motion. In particular,
we choose rt(st, at) = max(1 − 2|xrel
t |, 0). The reward is large
when xrel is close to 0 (with a maximum reward of 1 when
xrel
t = 0); the robot receives no reward if |xrel
t | ≥ 0.5. The
robot’s goal is to maximize the expected cumulative reward
over a time horizon of T = 5.

Results. Unlike the lava problem, this problem does not
admit a good open-loop policy since the initial distribution on
vx,ball
is symmetric about 0; thus, the robot does not have a
0
priori knowledge of the ball’s x-velocity direction (as illustrated
in Fig. 4). Fig. 5 plots the upper bound on the expected
cumulative reward obtained using Algorithm 1 for different
settings of the observation noise scale η. Since the dynamics
(31) are afﬁne, the sensor model is Gaussian, and the initial
state distribution is also Gaussian, we can apply the techniques
described in Sec. VI-A for analytically computing the quantities
of interest in Algorithm 1.

Fig. 5 also compares the upper bounds on R(cid:63) with achievable
lower bounds by applying a model-predictive control (MPC)
scheme combined with a Kalman ﬁlter for state estimation. We
estimate the expected reward achieved by the MPC controller
using 100 initial conditions sampled from p0. Fig. 5 plots
the average of these expected rewards across ﬁve random
seeds (the resulting std. dev. is too small to visualize). As the
ﬁgure illustrates, the MPC controller obeys the fundamental

XY0.00.51.01.52.02.53.0NoiseScale(η)135RewardBoundMPCbound on reward computed by our approach. Moreover,
the performance of the controller qualitatively tracks the
degradation of achievable performance predicted by the bound
as η is increased. Finally, we observe that sensors with noise
scales η = 1 and higher are fundamentally limited as compared
to a noiseless sensor. This is demonstrated by the fact that the
MPC controller for η = 0 achieves higher performance than
the fundamental limit on performance for η = 1.

C. Obstacle Avoidance with a Depth Sensor

For our ﬁnal example, we consider the problem of obstacle
avoidance using a depth sensor (Fig. 1). This is a more
challenging problem with higher-dimensional (continuous) state
and observation spaces, and non-Gaussian sensor models.

State and action spaces. The robot is initialized at the
origin with six cylindrical obstacles of ﬁxed radius placed
randomly in front of it. The state st ∈ R12 of this system
describes the locations of these obstacles in the environment.
In addition, we also place “walls” enclosing a workspace
[−1, 1]m × [−0.1, 1.2]m (these are not visualized in the ﬁgure
to avoid clutter). The initial state distribution p0 corresponds
to uniformly randomly choosing the x-y locations of the
cylindrical obstacles from the set [−1, 1]m × [0.9, 1.1]m. The
robot’s goal is to navigate to the end of the workspace by
choosing a motion primitive to execute (based on a noisy depth
sensor described below). Fig. 1 illustrates the set of ten motion
primitives the robot can choose from; this set corresponds to
the action space.

Rewards. We treat this problem as a one-step decision
making problem (Sec. IV). Once the robot chooses a motion
primitive to execute based on its sensor measurements, it
receives a reward of 0 if the motion primitive results in a
collision with an obstacle; if the motion primitive results
in collision-free motion, the robot receives a reward of 1.
The expected reward for this problem thus corresponds to the
probability of safe (i.e., collision-free) motion.

Sensor. The robot is equipped with a depth sensor which
provides distances along nrays = 10 rays. The sensor has a
ﬁeld of view of 90◦ and a maximum range of 1.5m. We use
the noise model for range ﬁnders described in [22, Ch. 6.3]
and consider two kinds of measurement errors: (i) errors due to
failures to detect obstacles, and (ii) random noise in the reported
distances. For each ray, there is a probability pmiss = 0.1 that
the sensor misses an obstacle and reports the maximum range
(1.5m) instead. In the case that an obstacle is not missed,
the distance reported along a given ray is sampled from a
Gaussian with mean equal to the true distance along that ray
and std. dev. equal to η. The noise for each ray is sampled
independently. Overall, this is a non-Gaussian sensor model
due to the combination of the two kinds of errors.

Results. We implement Algorithm 1 using the sampling-
based techniques described in Sec. VI-B. We sample 20K
obstacle environments for upper bounding the open-loop
rewards associated with each action. We also utilize 20K
batches (each of size K = 1000) for upper bounding the mutual
information using (29). We utilize a version of Hoeffding’s

Fig. 6. Results for the obstacle avoidance example. We compare the upper
bounds on achievable expected rewards with the expected rewards using a
learned neural network policy for different values of sensor noise (results for
the learned policy are averaged across ﬁve training seeds; the std. dev. is too
small to visualize).

inequality (Theorem 3) presented in Appendix B to obtain an
upper bound on R(cid:63) that holds with probability 1 − δ = 0.95.
Fig. 6 shows the resulting upper bounds for different values
of the observation noise std. dev. η. We compare these with
rewards achieved by neural network policies trained to perform
this task. For each η, we sample 5000 training environments
and corresponding sensor observations. For each environment,
we generate a ten-dimensional training label by recording
the minimum distance to the obstacles achieved by each
motion primitive and passing the vector of distances through
an (element-wise) softmax transformation. We use a cross-
entropy loss to train a neural network that predicts the label of
distances for each primitive given a sensor observation as input.
We use two fully-connected layers with a ReLu nonlinearity;
the output is passed through a softmax layer. At test-time,
a given sensor observation is passed as input to the trained
neural network; the motion primitive corresponding to the
highest predicted distance is then executed. We estimate the
expected reward achieved by the trained policy using 5000 test
environments (unseen during training). Fig. 6 plots the average
of these expected rewards across ﬁve training seeds for each
value of η (the std. dev. across seeds is too small to visualize).
We emphasize that the upper bound on R(cid:63) computed using
our approach is a fundamental limit on performance that holds
regardless of the size of the neural network, the network
architecture, or algorithm used for policy learning. We also
highlight the fact that a sensor with noise η = 0.1 achieves
higher performance than the fundamental limit for a sensor
with noise η = 0.4 or 0.5.

Finally, we apply our approach in order to compare two
sensors with varying number nrays of rays along which the depth
sensor provides distance estimates (Fig. 1). For this comparison,
we ﬁx η = 0.3 and pmiss = 0.05. We compare two sensors with
nrays = 50 (Sensor 1) and nrays = 5 (Sensor 2) respectively.
The upper bound on expected reward computed using our
approach (with conﬁdence 1 − δ = 0.95) for Sensor 2 is 0.79.
A neural network policy for Sensor 1 achieves an expected
reward of approximately 0.86, which surpasses the fundamental
limit on performance for Sensor 2.

VIII. DISCUSSION AND CONCLUSIONS

We have presented an approach for establishing fundamental
limits on performance for sensor-based robot control and policy

0.00.10.20.30.40.5NoiseScale(η)0.00.51.0RewardBoundLearnedPolicylearning. We deﬁned a quantity that captures the amount of task-
relevant information provided by a sensor; using a novel version
of the generalized Fano inequality, we demonstrated that this
quantity upper bounds the expected reward for one-step decision
making problems. We developed a dynamic programming
approach for extending this bound to multi-step problems.
The resulting framework has potentially broad applicability to
robotic systems with continuous state and observation spaces,
nonlinear and stochastic dynamics, and non-Gaussian sensor
models. Our numerical experiments demonstrate the ability of
our approach to establish strong bounds on performance for
such settings. In addition, we provided an application of our
approach for comparing different sensors and establishing the
superiority of one sensor over another for a given task.

Challenges and future work. There are a number of
challenges and directions for future work associated with
our approach. On the theoretical front, we expect that our
bounds in Theorems 1 and 2 can be extended to utilize
general f-divergences (instead of the KL divergence); this
could potentially lead to tighter bounds. In addition, it would
be interesting to handle settings where the sensor model is
inaccurate (in contrast to this paper, where we have focused on
establishing fundamental limits given a particular sensor model).
For example, one could potentially perform an adversarial
search over a family of sensor models in order to ﬁnd the model
that results in the lowest bound on achievable performance.

On the algorithmic front, the primary challenges are: (i)
efﬁcient computation of bounds for longer time horizons,
and (ii) extensions to continuous action spaces. As presented,
Algorithm 1 requires an enumeration over action sequences.
Finding more computationally efﬁcient versions of Algorithm 1
is thus an important direction for future work. The primary
bottleneck in extending our approach to continuous action
spaces is the need to perform a supremization over actions
when computing ¯R⊥(cid:63)
in Algorithm 1. However, we note that
any upper bound on ¯R⊥(cid:63)
also leads to a valid upper bound
on R(cid:63). Thus, one possibility is to use a Lagrangian relaxation
[18] to upper bound ¯R⊥(cid:63)
in settings with continuous action
spaces.

t

t

t

Our work also opens up a number of exciting directions for
longer-term research. While we have focused on establishing
fundamental limits imposed by imperfect sensing in this work,
one could envision a broader research agenda that seeks
to establish bounds on performance due to other limited
resources (e.g., onboard computation or memory). One concrete
direction is to combine the techniques presented here with
information-theoretic notions of bounded rationality [30–32].
Finally, another exciting direction is to turn the impossibility
results provided by our approach into certiﬁcates of robustness
against an adversary. Speciﬁcally, consider an adversary that can
observe our robot’s actions; if one could establish fundamental
limits on performance for the adversary due to its inability to
infer the robot’s internal state (and hence its future behavior)
using past observations, this provides a certiﬁcate of robustness
against any adversary. This is reminiscent of a long tradition
in cryptography of turning impossibility or hardness results

into robust protocols for security [33, Ch. 9].

Overall, we believe that the ability to establish fundamental
limits on performance for robotic systems is crucial for
establishing a science of robotics. We hope that the work
presented here along with the indicated directions for future
work represent a step in this direction.

ACKNOWLEDGMENTS

The authors were partially supported by the NSF CAREER
Award [#2044149] and the Ofﬁce of Naval Research [N00014-
21-1-2803]. The authors would like to thank Alec Farid and
David Snyder for helpful discussions on this work.

REFERENCES

[1] Oliver Brock. Is robotics in need of a paradigm shift?
In Berlin Summit on Robotics: Conference Report, pages
1–10, 2011.

[2] James Morris. Can Tesla really do without radar for full

self-driving? Forbes, Jul 2021.

[3] Daniel E. Koditschek. What is robotics? Why do we
need it and how can we get it? Annual Review of Control,
Robotics, and Autonomous Systems, 4:1–33, 2021.
[4] Sertac Karaman and Emilio Frazzoli. High-speed ﬂight
the IEEE
in an ergodic forest.
International Conference on Robotics and Automation
(ICRA), pages 2899–2906, 2012.

In Proceedings of

[5] Sanjiban Choudhury, Sebastian Scherer, and J. Andrew
Bagnell. Theoretical limits of speed and resolution for
kinodynamic planning in a Poisson forest. In Proceedings
of Robotics: Science and Systems (RSS), 2015.

[6] B´ela Bollob´as and Oliver Riordan. Percolation. Cam-

bridge University Press, 2006.

[7] Davide Falanga, Suseong Kim, and Davide Scaramuzza.
How fast is too fast? The role of perception latency in high-
speed sense and avoid. IEEE Robotics and Automation
Letters, 4(2):1884–1891, 2019.

[8] John C. Doyle, Bruce A. Francis, and Allen R. Tannen-
baum. Feedback Control Theory. Courier Corporation,
2013.

[9] Yoke Peng Leong and John C. Doyle. Understanding
robust control theory via stick balancing. In Proceedings
of the IEEE Conference on Decision and Control (CDC),
pages 1508–1514, 2016.

[10] Jingxi Xu, Bruce Lee, Nikolai Matni, and Dinesh Ja-
yaraman. How are learned perception-based controllers
impacted by the limits of robust control? In Learning for
Dynamics and Control, pages 954–966. PMLR, 2021.

[11] Steven M. LaValle.

Sensing and ﬁltering: A fresh
perspective based on preimages and information spaces.
Citeseer, 2012.

[12] Steven M. LaValle. Sensor lattices: Structures for com-
paring information feedback. In International Workshop
on Robot Motion and Control (RoMoCo), pages 239–246,
2019.

[13] Jason M. O’Kane and Steven M. LaValle. Comparing the
power of robots. The International Journal of Robotics
Research, 27(1):5–23, 2008.

[14] Thomas M. Cover. Elements of Information Theory. John

Wiley & Sons, 1999.

[15] John C. Duchi and Martin J. Wainwright. Distance-based
and continuum Fano inequalities with applications to
statistical estimation. arXiv preprint arXiv:1311.2669,
2013.

[16] Xi Chen, Adityanand Guntuboyina, and Yuchen Zhang.
On Bayes risk lower bounds. The Journal of Machine
Learning Research, 17(1):7687–7744, 2016.

[17] Sebastien Gerchinovitz, Pierre M´enard, and Gilles Stoltz.
Fano’s inequality for random variables. Statistical Science,
35(2):178–201, 2020.

[18] Stephen P. Boyd and Lieven Vandenberghe. Convex
Optimization. Cambridge University Press, 2004.
[19] Paul Milgrom and Ilya Segal. Envelope theorems for
arbitrary choice sets. Econometrica, 70(2):583–601, 2002.
for Python
9.0.84 (beta), 2019. URL https://docs.mosek.com/9.0/
pythonfusion/index.html.

[20] MOSEK ApS. MOSEK Fusion API

[21] B. O’Donoghue, E. Chu, N. Parikh, and S. Boyd. SCS:
Splitting conic solver, version 2.0.2. https://github.com/
cvxgrp/scs, November 2017.

[22] Sebastian Thrun, Wolfram Burgard, and Dieter Fox.

Probabilistic Robotics. MIT Press, 2005.

[23] John C. Duchi. Derivations for linear algebra and

optimization. 2016.

[24] St´ephane Boucheron, G´abor Lugosi, and Pascal Massart.
Concentration Inequalities: A Nonasymptotic Theory of
Independence. Oxford University Press, 2013.

[25] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex
Alemi, and George Tucker. On variational bounds of
In International Conference on
mutual information.
Machine Learning, pages 5171–5180. PMLR, 2019.
[26] Anthony R. Cassandra, Leslie P. Kaelbling, and Michael L.
Littman. Acting optimally in partially observable stochas-
In AAAI, volume 94, pages 1023–1028,
tic domains.
1994.

[27] Peter R. Florence. Integrated perception and control at
high speed. Master’s thesis, Massachusetts Institute of
Technology, 2017.

[28] Vincent Pacelli and Anirudha Majumdar. Learning task-
driven control policies via information bottlenecks. In
Proceedings of Robotics: Science and Systems (RSS),
2020.

[29] Kaiyu Zheng and Stefanie Tellex. Pomdp py: A frame-
work to build and solve POMDP problems. arXiv preprint
arXiv:2004.10099, 2020.

[30] Naftali Tishby and Daniel Polani. Information theory of
decisions and actions. In Perception-Action Cycle: Models,
Architectures, and Hardware, pages 601–636. Springer,
2011.

[31] Pedro A. Ortega, Daniel A. Braun, Justin Dyer, Kee-Eung
Kim, and Naftali Tishby. Information-theoretic bounded
rationality. arXiv preprint arXiv:1512.06789, 2015.
[32] Vincent Pacelli and Anirudha Majumdar. Robust control
under uncertainty via bounded rationality and differential
privacy. arXiv preprint arXiv:2109.08262, 2021.

[33] Jean-Philippe Aumasson.

Serious Cryptography: A
Practical Introduction to Modern Encryption. No Starch
Press, 2017.

[34] Wolfgang Mulzer. Five proofs of Chernoff’s bound with
applications. arXiv preprint arXiv:1801.03365, 2018.

APPENDIX A
PROOFS

Lemma 4 (Monotonicity of KL inverse). D−1(q|c) is monotonically non-decreasing in q ∈ [0, 1] for c ≥ 0.

Proof: Recall that the KL inverse is deﬁned using the following optimization problem with decision variable p:

D−1(q|c) := sup {p ∈ [0, 1] | DB(p(cid:107)q) ≤ c}.

We ﬁrst prove that:

D−1(q|c) ≥ q.

(32)

(33)

For contradiction, suppose that D−1(q|c) < q. Then, let p(cid:48) := q. Notice that p(cid:48) is a feasible point for the optimization problem
(32) since DB(p(cid:48)(cid:107)q) = DB(q(cid:107)q) = 0, and p(cid:48) = q ∈ [0, 1]. But, by assumption, p(cid:48) = q > D−1(q|c); this contradicts the statement
that D−1(q|c) is the optimal solution to (32).

This allows us to demonstrate the monotonicity of D−1(q|c) in q for the case when c = 0. In particular, when c = 0, we

have that D−1(q|c) = q. Thus, monotonicity of D−1(q|c) in q follows immediately.

Next, we demonstrate the monotonicity of D−1(q|c) in q for the case when c > 0. We note that:
• The objective function of (32) is continuous and concave (indeed, linear).
• The constraints of (32) can be written in the form g(p, q) ≥ 0 with g continuous and concave (since the KL divergence is

jointly convex in both arguments, and the other constraints on p are linear).

• There is a point (in particular, the point p = q) that is strictly feasible (i.e., g(p, q) > 0) for (32) for c > 0 and q ∈ [0, 1].
The optimization problem (32) thus satisﬁes the conditions for the envelope theorem [19, Corollary 5], which allows us to
characterize the derivative of (32) with respect to the parameter q:

∂[D−1(q|c)]
∂q

= sup

p∈X (cid:63)(q)

inf
λ∈Y (cid:63)(q)

∂L(p, q, λ)
∂q

,

(34)

where L(p, q, λ) is the Lagrangian for (32), X (cid:63)(q) are the set of optimizers for (32), Y (cid:63)(q) are the set of optimizers for the
Lagrange dual, and λ = [λ1, λ2, λ3] are the Lagrange multipliers corresponding to the three constraints:

c − DB(p(cid:107)q) ≥ 0,
p ≥ 0,

1 − p ≥ 0.

The Lagrangian is given by:

L(p, q, λ) = p + λ1(c − DB(p(cid:107)q)) + λ2p + λ3(1 − p).

Computing the partial derivative with respect to q:

∂DB(p(cid:107)q)
∂q
(cid:104)
p log

∂L(p, q, λ)
∂q

= −λ1

= −λ1

= −λ1

∂
∂q
(cid:104)

−

p
q

+

p
q
1 − p
1 − q

(cid:105)
.

+ (1 − p) log

(cid:105)

1 − p
1 − q

(35)

(36)

(37)

(38)

(39)

(40)

(41)

We note that the Lagrange multiplier λ1 must be nonnegative. In addition, we know from (33) that the optimal value p(cid:63) of the
optimization problem (32) must be greater than or equal to q. Thus:

1 − p(cid:63)
1 − q

≤ 1

≥ 1, and

p(cid:63)
q
1 − p(cid:63)
p(cid:63)
q
1 − q
∂L(p, q, λ)
∂q

≥

=⇒

=⇒

≥ 0.

(42)

(43)

(44)

Hence, the envelope theorem (34) demonstrates that D−1(q|c) is monotonically non-decreasing in q.

Theorem 1 (Single-step performance bound). The best achievable reward is upper bounded by the task-relevant information
potential (TRIP) of a sensor:

Proof: For a given policy π0, deﬁne:

τ (σ0; r0) ≥ R(cid:63)(σ0; r0) := sup
π0

E
s0,o0

[r0(s0, π0(o0))].

R := E

p0(s0)

E
σ0(o0|s0)

[r0(s0, π0(o0))],

(9)

(45)

and

E
q(o0)
The only difference between R and ˜R is that the observations o0 in ˜R are drawn from a state-independent distribution q.

[r0(s0, π0(o0))].

˜R := E

p0(s0)

(46)

Now, assuming bounded rewards (Assumption 1), we have:

DB(R(cid:107) ˜R) = DB

(cid:32)

E
σ0(o0|s0)

E
p0(s0)
(cid:32)

[r0(s0, π0(o0))]

≤ E

p0(s0)

≤ E

p0(s0)

DB

(cid:16)

D

E
σ0(o0|s0)

[r0(s0, π0(o0))]

σ0(o0|s0)

(cid:13)
(cid:17)
(cid:13)
(cid:13) q(o0)

.

(cid:33)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

E
p0(s0)

E
q(o0)

[r0(s0, π0(o0))]

(cid:33)

E
q(o0)

[r0(s0, π0(o0))]

(47)

(48)

(49)

The ﬁrst inequality above follows from Jensen’s inequality, while the second follows from the data processing inequality (see
[17, Corollary 2] for the speciﬁc version). From the well-known variational representation of the mutual information (see, e.g.,
[16]), we note that:

Thus, taking the inﬁmum of (49) with respect to q, we see:

I(o0; s0) = inf
q

D

E
p0(s0)

(cid:16)

σ0(o0|s0)

(cid:13)
(cid:17)
(cid:13)
(cid:13) q(o0)

We can then upper bound R using the KL inverse:

DB(R(cid:107) ˜R) ≤ I(o0; s0).

Thus,

(cid:17)
R ≤ D−1(cid:16) ˜R | I(o0; s0)

.

sup
π0

R ≤ sup
π0

(cid:17)
D−1(cid:16) ˜R | I(o0; s0)

.

From the monotonicity of the KL inverse (Lemma 4), we have:

Since by deﬁnition,

we have:

R ≤ D−1(cid:16)

sup
π0

sup
π0

(cid:17)
˜R | I(o0; s0)

.

R(cid:63)(σ0; r0) = sup
π0

R,

R(cid:63)(σ0; r0) ≤ D−1(cid:16)

(cid:17)
˜R | I(o0; s0)

.

sup
π0

Now, using the Fubini-Tonelli theorem, we see:

[r0(s0, π0(o0))]

[r0(s0, π0(o0))]

[r0(s0, π0(o0))]

sup
π0

˜R = sup
π0
= sup
π0
≤ E
q(o0)
= E
q(o0)
= sup
a0

E
p0(s0)
E
q(o0)
sup
π0
sup
a0
E
p0(s0)

E
q(o0)
E
p0(s0)
E
p0(s0)
E
p0(s0)
[r0(s0, a0)].

[r0(s0, a0)]

.

(50)

(51)

(52)

(53)

(54)

(55)

(56)

(57)

(58)

(59)

(60)

(61)

Since open-loop actions are special cases of policies, we also have:

sup
π0

˜R = sup
π0

E
p0(s0)

E
q(o0)

[r0(s0, π0(o0))] ≥ sup
a0

E
p0(s0)

[r0(s0, a0)].

As a result, we see that:

sup
π0

˜R = sup
a0

E
p0(s0)

[r0(s0, a0)] = R⊥
0 ,

where R⊥

0 is as deﬁned in (7). Combining this with (56), we obtain the desired result:
R(cid:63)(σ0; r0) ≤ D−1(cid:16)

(cid:17)
0 | I(o0; s0)

=: τ (σ0; r0).

R⊥

(62)

(63)

(64)

Proposition 2 (Recursive bound). For any t = 0, . . . , T − 1, the following inequality holds for any open-loop sequence of
actions a0:t−1:

sup
t ,...,πt
πt

T −1

Rt ≤ (T − t) · D−1

| I(ot; st)

(cid:32)

R⊥(cid:63)
t
T − t
(cid:123)(cid:122)
=:τt(σt:T −1;rt:T −1)

(cid:33)
.

(cid:125)

(cid:124)

(15)

Proof: The proof follows a similar structure to that of Theorem 1. First, note that Rt deﬁned in (10) can be written as:

Deﬁne:

Rt = E
st|
a0:t−1

E
ot|st

E
st+1:T −1,ot+1:T −1|
st,ot

˜Rt := E
st|
a0:t−1

E
q(ot)

E
st+1:T −1,ot+1:T −1|
st,ot

rk(sk, πt

(cid:35)
k(ot:k))

.

(cid:35)

rk(sk, πt

k(ot:k))

.

(cid:34) T −1
(cid:88)

k=t

(cid:34) T −1
(cid:88)

k=t

The only difference between ˜Rt and Rt is that the observations ot in ˜Rt are drawn from a state-independent distribution q.
For the sake of notational simplicity, we will assume that Rt and ˜Rt have been normalized to be within [0, 1] by scaling

with 1/(T − t). The desired result (15) then follows from the bound we prove below by simply rescaling with (T − t).

Now,

DB(Rt(cid:107) ˜Rt) = DB

(cid:32)

E
st|
a0:t−1

E
ot|
st
(cid:32)

E
st+1:T −1,ot+1:T −1|
st,ot

(cid:34) T −1
(cid:88)

k=t

(cid:35)

rk(sk, πt

k(ot:k))

(cid:13)
(cid:13)
(cid:13)

E
q(ot)

E
st+1:T −1,ot+1:T −1|
st,ot

(cid:35)(cid:33)

rk(sk, πt

k(ot:k))

(cid:34) T −1
(cid:88)

k=t

(cid:34) T −1
(cid:88)

k=t

rk(sk, πt

k(ot:k))

E
q(ot)

E
st+1:T −1,ot+1:T −1|
st,ot

(cid:35)(cid:33)

rk(sk, πt

k(ot:k))

(cid:34) T −1
(cid:88)

k=t

(65)

E
st|
a0:t−1
(cid:35)

(cid:13)
(cid:13)
(cid:13)

DB

(cid:16)

D

≤ E
st|
a0:t−1

≤ E
st|
a0:t−1

E
ot|
st

E
st+1:T −1,ot+1:T −1|
st,ot

σt(ot|st)

(cid:13)
(cid:17)
(cid:13)
(cid:13) q(ot)

.

The ﬁrst inequality above follows from Jensen’s inequality, while the second follows from the data processing inequality (see
[17, Corollary 2] for the speciﬁc version).

From the well-known variational representation of the mutual information [16], we note that:

I(ot; st) = inf
q

D

E
st|
a0:t−1

(cid:16)

σt(ot|st)(cid:107)q(ot)

(cid:17)

.

Thus, taking the inﬁmum of (65) with respect to q, we see:

DB(Rt(cid:107) ˜Rt) ≤ I(ot; st).

We can then upper bound Rt using the KL inverse:

Thus,

(cid:17)
Rt ≤ D−1(cid:16) ˜Rt | I(ot; st)

.

sup
t ,...,πt
πt

T −1

Rt ≤

sup
t ,...,πt
πt

T −1

D−1(cid:16) ˜Rt | I(ot; st)

(cid:17)

.

(66)

(67)

(68)

(69)

Notice that the LHS is precisely the quantity we are interested in upper bounding in Proposition 2. From the monotonicity of
the KL inverse (Lemma 4), we have:

where

Rt ≤ D−1(cid:16) ˜R(cid:63)

t | I(ot; st)

(cid:17)

,

sup
t ,...,πt
πt

T −1

˜Rt = sup
t ,...,πt
πt

T −1

E
st|
a0:t−1

E
q(ot)

E
st+1:T −1,ot+1:T −1|
st,ot

rk(sk, πt

k(ot:k))

(cid:35)
.

(cid:34) T −1
(cid:88)

k=t

˜R(cid:63)

t := sup
t ,...,πt
πt

T −1

Now, using the Fubini-Tonelli theorem:

sup
t ,...,πt
πt

T −1

E
st|
a0:t−1

E
q(ot)

E
st+1:T −1,ot+1:T −1|
st,ot

= sup
t ,...,πt
πt

T −1

E
q(ot)

E
st|
a0:t−1

E
st+1:T −1,ot+1:T −1|
st,ot

rk(sk, πt

(cid:35)
k(ot:k))

rk(sk, πt

(cid:35)
k(ot:k))

(cid:34) T −1
(cid:88)

k=t
(cid:34) T −1
(cid:88)

k=t

=

sup
t+1...,πt
πt

T −1

(cid:34)

(cid:34)

sup
πt
t

E
q(ot)

E
st|
a0:t−1

E
st+1:T −1,ot+1:T −1|
st,ot

≤

sup
t+1...,πt
πt

T −1

E
q(ot)

sup
πt
t

E
st|
a0:t−1

E
st+1:T −1,ot+1:T −1|
st,ot

rk(sk, πt

k(ot:k))

(cid:35) (cid:35)

(cid:35) (cid:35)

rk(sk, πt

k(ot:k))

.

(cid:34) T −1
(cid:88)

k=t
(cid:34) T −1
(cid:88)

k=t

(cid:35)

Notice that:

E
q(ot)

sup
πt
t

E
st|
a0:t−1

E
st+1:T −1,ot+1:T −1|
st,ot

(cid:34) T −1
(cid:88)

k=t

rk(sk, πt

k(ot:k))

= E
q(ot)

sup
πt
t

E
st|
a0:t−1

(cid:34)

(cid:34)

rt(st, πt

t(ot)) +

E
st+1,ot+1|
st,πt
t (ot)
(cid:34)

(cid:34)
rt+1(st+1, πt

t+1(ot:t+1)) + . . .

(cid:35)

(cid:35)

. . .

(cid:35)

(cid:35)

= E
q(ot)

sup
at

E
st|
a0:t−1

rt(st, at) +

E
st+1,ot+1|
st,at

rt+1(st+1, πt+1

t+1(ot+1)) + . . .

. . .

(cid:124)

(cid:34)

(cid:123)(cid:122)
Does not depend on ot

(cid:34)

= sup
at

E
st|
a0:t−1

rt(st, at) +

E
st+1,ot+1|
st,at

rt+1(st+1, πt+1

t+1(ot+1)) + . . .

(cid:125)

(cid:35)

(cid:35)

. . .

.

Here, (78) follows (77) since q is a ﬁxed distribution that does not depend on the state.

We thus see that (75) equals:

sup
πt
t+1...,πt

T −1

=

sup
t+1 ...,πt+1
πt+1

T −1

(cid:34)

(cid:124)
(cid:34)

(cid:34)

sup
at

E
st|
a0:t−1

(cid:34)

rt(st, at) +

(cid:34)

E
st+1,ot+1|
st,at

rt+1(st+1, πt+1

t+1(ot+1)) + . . .

. . .

(cid:35)

(cid:35)(cid:35)

(cid:34)

rt(st, at) +

sup
at

E
st|
a0:t−1

E
st+1,ot+1|
st,at

(cid:123)(cid:122)
Does not depend on ot
(cid:34)
rt+1(st+1, πt+1

t+1(ot+1)) + . . .

(cid:125)
(cid:35)(cid:35)

(cid:35)

. . .

(cid:35)(cid:35)

(cid:34)

(cid:35)

rt(st, at)

+

E
st+1:T −1,ot+1:T −1|
a0:t

(cid:34) T −1
(cid:88)

k=t+1

rk(sk, πt+1

k

(ot+1:k))

=

sup
t+1 ...,πt+1
πt+1

T −1

sup
at

E
st|
a0:t−1

R⊥
t

=

sup
t+1 ...,πt+1
πt+1
= R⊥(cid:63)
.
t

T −1

(70)

(71)

(72)

(73)

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

We have thus proved that ˜R(cid:63)
˜R(cid:63)
t
observation that ˜R(cid:63)

and thus ˜R(cid:63)

t ≥ R⊥(cid:63)

t ≤ R⊥(cid:63)

t = R⊥(cid:63)

t ≤ R⊥(cid:63)
t
implies:

t

t

). Since the RHS of (70) is a monotonically non-decreasing function of ˜R(cid:63)

(indeed, since open-loop policies are special cases of feedback policies, we also have
t (Lemma 4), the

Rt ≤ D−1(cid:16)

R⊥(cid:63)
t

(cid:17)
| I(ot; st)

.

sup
t ,...,πt
πt

T −1

Theorem 2 (Multi-step performance bound). Algorithm 1 returns an upper bound on the best achievable reward R(cid:63).

Proof: Using (backwards) induction, we prove that for all t = T − 1, . . . , 0,

Thus, in particular,

sup
t ,...,πt
πt

T −1

Rt ≤ ¯Rt(a0:t−1), ∀a0:t−1.

R(cid:63) = sup
0 ,...,π0
π0

T −1

R0 ≤ ¯R0.

(85)

(86)

(87)

We prove (86) by backwards induction starting from t = T − 1. In particular, Proposition 2 leads to the inductive step. We

ﬁrst prove the base step of induction using t = T − 1. Using (15), we see:

RT −1 ≤ D−1(cid:16)

(cid:17)
T −1 | I(oT −1; sT −1)

R⊥(cid:63)

.

sup
πT −1
T −1

By deﬁnition (see (14)),

R⊥(cid:63)

T −1 = sup
aT −1

E
sT −1|a0:T −2

(cid:104)

(cid:105)
rT −1(sT −1, aT −1)

E
sT −1|a0:T −2

(cid:104)

(cid:105)
rT −1(sT −1, aT −1)

= sup
aT −1
= ¯R⊥(cid:63)

T −1.

+ RT
(cid:124)(cid:123)(cid:122)(cid:125)
=0

Combining this with (88) and the monotonicity of the KL inverse (Proposition 1), we see:

RT −1 ≤ D−1(cid:16) ¯R⊥(cid:63)

(cid:17)
T −1 | I(oT −1; sT −1)

sup
πT −1
T −1

= ¯RT −1(a0:T −2).

Next, we prove the induction step. Suppose it is the case that for t ∈ {0, . . . , T − 2}, we have

We then need to show that

To prove this, we ﬁrst observe that

sup
t+1 ,...,πt+1
πt+1

T −1

Rt+1 ≤ ¯Rt+1(a0:t).

sup
πt
t ,...,πt

T −1

Rt ≤ ¯Rt(a0:t−1).

R⊥(cid:63)
t

:=

sup
t+1 ,...,πt+1
πt+1
(cid:34)

T −1

= sup
at

E
st|a0:t−1

(cid:34)

sup
at

E
st|a0:t−1

(cid:104)

(cid:105)
rt(st, at)

+ Rt+1

(cid:35)

(cid:104)
rt(st, at)

(cid:105)

+

sup
t+1 ,...,πt+1
πt+1

T −1

(cid:35)
.

Rt+1

Combining this with the induction hypothesis (91), we see

R⊥(cid:63)

t ≤ sup
at
=: ¯R⊥(cid:63)

t

(cid:104)

(cid:105)
rt(st, at)

(cid:35)
+ ¯Rt+1(a0:t)

E
st|a0:t−1

(cid:34)

.

(88)

(89)

(90)

(91)

(92)

(93)

(94)

Finally, combining this with (15) and the monotonicity of the KL inverse (Prop. 1), we obtain the desired result (92):

sup
t ,...,πt
πt

T −1

Rt ≤ (T − t) · D−1

= ¯Rt(a0:t−1).

(cid:32) ¯R⊥(cid:63)
t
T − t

(cid:33)

| I(ot; st)

(95)

(96)

In our numerical examples (Sec. VII), we utilize a slightly tighter version of Hoeffding’s inequality than the one presented in

Theorem 3. In particular, we use the following Chernoff-Hoeffding inequality (see [34, Theorem 5.1]).

APPENDIX B
CHERNOFF-HOEFFDING BOUND

Theorem 5 (Chernoff-Hoeffding inequality [34]). Let z be a random variable bounded within [0, 1], and let z1, . . . , zn denote
i.i.d. samples. Then, with probability at least 1 − δ (over the sampling of z1, . . . , zn), the following bound holds with probability
at least 1 − δ:

(cid:32)

DB

1
n

n
(cid:88)

i=1

zi

(cid:13)
(cid:13)
(cid:13)

(cid:33)

E[z])

≤

log(2/δ)
n

.

We can obtain an upper bound on E[z] using (97) as follows:

(cid:40)

E[z] ≤ sup

p ∈ [0, 1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

DB

(cid:16) 1
n

n
(cid:88)

i=1

zi

(cid:13)
(cid:17)
(cid:13)
(cid:13) p)

≤

log(2/δ)
n

(cid:41)

.

(97)

(98)

The optimization problem in the RHS of (98) is analogous to the KL inverse deﬁned in Sec. III-B, and can be thought of as
a “right” KL inverse (instead of a “left” KL inverse). Similar to the KL inverse in Sec. III-B, we can solve the optimization
problem in (98) using geometric programming.

