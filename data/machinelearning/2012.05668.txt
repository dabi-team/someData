0
2
0
2

c
e
D
0
1

]

O
C

.
t
a
t
s
[

1
v
8
6
6
5
0
.
2
1
0
2
:
v
i
X
r
a

Multilevel Delayed Acceptance MCMC with an
Adaptive Error Model in PyMC3

Mikkel B. Lykkegaard
Centre for Water Systems and
Institute for Data Science and Artiﬁcial Intelligence
University of Exeter
EX4 4QF, United Kingdom
m.lykkegaard@exeter.ac.uk

Grigorios Mingas
The Alan Turing Institute
NW1 2DB, United Kingdom
gmingas@turing.ac.uk

Robert Scheichl
Institute for Applied Mathematics and
Interdisciplinary Center for Scientiﬁc Computing
Ruprecht-Karls-Universität Heidelberg
69120 Heidelberg, Germany
r.scheichl@uni-heidelberg.de

Colin Fox
Department of Physics
University of Otago
Dunedin 9016, New Zealand
colin.fox@otago.ac.nz

Tim J. Dodwell
Institute for Data Science and Artiﬁcial Intelligence
University of Exeter
EX4 4QF, United Kingdom
t.dodwell@exeter.ac.uk

Abstract

Uncertainty Quantiﬁcation through Markov Chain Monte Carlo (MCMC) can be
prohibitively expensive for target probability densities with expensive likelihood
functions, for instance when the evaluation it involves solving a Partial Differential
Equation (PDE), as is the case in a wide range of engineering applications. Mul-
tilevel Delayed Acceptance (MLDA) with an Adaptive Error Model (AEM) is a
novel approach, which alleviates this problem by exploiting a hierarchy of mod-
els, with increasing complexity and cost, and correcting the inexpensive models
on-the-ﬂy. The method has been integrated within the open-source probabilistic
programming package PyMC3 and is available in the latest development version. In
this paper, the algorithm is presented along with an illustrative example.

1

Introduction

Sampling from an unnormalised posterior distribution π(·) using Markov Chain Monte Carlo
(MCMC) methods is a central task in computational statistics. This can be a particularly chal-
lenging problem when the evaluation of π(·) is computationally expensive and the parameter space θ
and data d deﬁning π(·) are high-dimensional. The sequential (highly) correlated nature of a Markov
chain and the slow converge rates of Monte Carlo sampling, means that many MCMC samples are
often required to obtain a sufﬁcient representation of a posterior distribution π(·). Examples of such
problems frequently occur in Bayesian inverse problems, image reconstruction and probabilistic
machine learning, where simulations of the measurements (required to calculate a likelihood) depend
on the evaluation of complex mathematical models (e.g. a system of partial differential equations) or
the evaluation of prohibitively large data sets.

Workshop on machine learning for engineering modeling, simulation and design @ NeurIPS 2020

 
 
 
 
 
 
In this paper a MCMC approach capable of accelerating existing sampling methods is proposed,
where a hierarchy (or sequence) π0(·), . . . , πL−1(·) of computationally cheaper approximations to
the ‘full’ posterior density π(·) ≡ πL(·) are available. As with the original delayed acceptance
algorithm, proposed by Christen and Fox [1], the idea is to generate MCMC proposals for the next
step in the chain from runs of MCMC subchains targeting the computationally cheaper, approximate
densities. The original DA method proposed the approach for just two levels. In this paper, the
approach is extended to recursively apply delayed acceptance across a complete hierarchy of model
approximations, a method termed multilevel delayed acceptance (MLDA). There are close connec-
tions to and similarities with multilevel variance reduction techniques, ﬁrst proposed by Giles [2],
widely studied for forward uncertainty propagation problems and importantly extended to Multilevel
Markov Chain Monte Carlo approach by Hoang et al. [3] and Dodwell et al. [4], and further to a
Multi-Index setting by Jasra et al. [5]. As in other multilevel approaches, the subchains in MLDA
can be exploited for variance reduction, but this is beyond the scope of this paper.

The increase in use of Bayesian probabilistic tools has naturally coincided with the development of
user-friendly computational packages, allowing users to focus on model development and testing,
rather than algorithm development of sampling methods and post-processing diagnostics. Various
high quality packages are available. Examples include: MUQ, STAN and Pyro.1 A guiding principle of
our work and of this contribution was to ensure that the MLDA implementation is easily accessible,
well supported and gives ﬂexibility to users to deﬁne complex models in a friendly language. To
achieve this we embed our sampler into the widely used open-source probabilistic programming
package PyMC3 [6]. The method and implementation have been accepted in the development version,
and will be made available with the next full release (version 3.9.4).

2 Adaptive Multilevel Delayed Acceptance (MLDA)

2.1 Preliminaries: Metropolis-Hastings MCMC Algorithms

Here, a typical Bayesian inverse problem is considered. Given are (limited) observations d ∈ RM of
a system and a mathematical model F(θ) : RR (cid:55)→ RM , which maps from a set of model parameters
θ ∈ RR to the space of model predictions of the data. The connection between model and data is
then, in the simplest case, described by the additive model

d = F(θ) + (cid:15)

(1)

(but it can also be more general). Here, (cid:15) is a random variable, which can depend on θ and captures
the uncertainty of the model’s reproduction of the data. It might include measurement uncertainty
of the recorded data, uncertainty due to model mis-speciﬁcation and/or uncertainties due to sing
in practice a numerical approximation of the mathematical model. The distribution of the random
variable (cid:15) deﬁnes the likelihood, i.e. the probability distribution L(d|θ). For simplicity it is assumed
to be Gaussian, i.e. (cid:15) ∼ N (µ(cid:15), Σ(cid:15)) and L(d|θ) ∼ N (d − F(θ) − µ(cid:15), Σ(cid:15)), but it does not have to be.

Given prior information π(θ) on the distribution of the model parameters θ, the aim is to condition
this distribution on the observations, i.e. to obtain samples from the posterior distribution π(θ|d).
Through Bayes’ theorem, it follows that

π(θ|d) =

L(d|θ)π(θ)
π(d)

∝ L(d|θ)π(θ).

(2)

Since the normalising constant π(d) (the evidence) is not typically known, the conditional distribution
π(θ|d) is generally intractable and exact sampling is not possible. There are various computational
strategies for generating samples from π(θ|d). This paper focuses on the Metropolis-Hastings MCMC
algorithm, described in Algorithm 1. It creates a Markov chain {θj}j∈N of correlated parameter
states θj that (in the limit) target the exact posterior distribution π(θ|d) (cf. e.g. [7]). The efﬁciency
of the algorithm is determined by the choice of the proposal distribution q(·|·).

Whilst MCMC methods are the gold-standard for sampling from complex posterior distributions, for
many types of models and data they come with signiﬁcant practical challenges. Firstly, each cycle
of Alg. 1 requires the evaluation of the model F(θ(cid:48)) which may be computationally very expensive.
Secondly, the samples generated in the chain are correlated, and therefore many cycles of Alg. 1 are

1MUQ: http://muq.mit.edu, STAN: https://mc-stan.org, Pyro: https://pyro.ai

2

Algorithm 1 (Metropolis-Hastings MCMC): Choose θ0. Then, for j = 0, . . . , J − 1:

1. Given θj, generate a proposal θ(cid:48) from a given proposal distribution q(θ(cid:48)|θj),
2. Accept proposal θ(cid:48) as the next sample with probability

α(θ(cid:48)|θj) = min

(cid:26)

1,

L(d|θ(cid:48)) π(θ(cid:48)) q(θj|θ(cid:48))
L(d|θj)π(θj)q(θ(cid:48)|θj)

(cid:27)

,

i.e. set θj+1 = θ(cid:48) with probability α, and θj+1 = θj with probability 1 − α.

often required to produce a sufﬁcient number of ”independent” (or effective) samples from π(θ|d).
The ideal proposal distribution generates cheap candidate proposals θ(cid:48), which have a high probability
of being accepted, and are independent of the previous sample θj.

In this paper, efﬁcient, Metropolis-style proposal strategies are developed that exploit a hierarchy of
approximations F(cid:96)(θ), for (cid:96) = 0, . . . , L − 1, to the full model FL := F, which are assumed to be
ordered according to increasing accuracy and computational cost.

2.2 Multilevel Delayed Acceptance

Delayed Acceptance (DA) is an approach ﬁrst introduced by Christen and Fox [1], exploiting a
simple, but highly effective idea. The original DA approach is a two-level method that assumes
a computationally cheaper approximation F ∗ for the forward map F is available. The idea is
that for any chosen proposal θ(cid:48), a standard Metropolis accept/reject step (as given in Alg. 1) is
performed with the approximate forward map F ∗(θ(cid:48)) before the expensive forward model F(θ(cid:48)) is
evaluated. Only if accepted, a second accept/reject step with the original forward map F(θ(cid:48)) and
(cid:110)
1, L(d|θ(cid:48))L∗(d|θj )
is carried out. Here, L∗(d|·) denotes the
with acceptance probability α = min
L(d|θj )L∗(d|θ(cid:48))
posterior distribution with the likelihood deﬁned by F ∗. The validity of this approach as a proposal
method, yielding a convergent MCMC algorithm, is provided in [1].

(cid:111)

The basic DA approach can be extended in two ways. First, instead of doing a single check for
the proposal that comes from the ﬁne level, a subchain of length J can be ran on the coarse level
[8, 9]. This does not affect the theory, but has the advantage of decorrelating samples passed back
as proposals to the ﬁne level. Second, and this is the main, novel algorithmic contribution, DA is
extended to a general multilevel setting, exploiting links to the Multilevel Markov Chain Monte Carlo
(MLMCMC) Method proposed by Dodwell et al. [4].

The subtle differences between the approaches are apparent when comparing the schematics of the
two multilevel proposal processes shown in Fig. 1. Algorithmically, Multilevel Delayed Acceptance
(MLDA) can be seen as a recursion of Delayed Acceptance over multiple levels (cid:96) = {0, 1, . . . , L}.
Crucially, if θi
(cid:96) is the current state at level (cid:96), and a proposal θ(cid:48) from the coarse subchain on level (cid:96) − 1
is rejected at level (cid:96), the coarse subchain to generate the subsequent proposal for level (cid:96) is again
initiated from θi
(cid:96). For MLMCMC, even if the coarse proposal is rejected, the coarse chain continues
independently of the ﬁne chain and does not revert to the state θi
(cid:96) (see Fig. 1, right). As a result,
coarse and ﬁne chains will detach, and only align once a coarse proposal is accepted at the ﬁne level.

Figure 1: Schematic for generating a proposal θ(cid:48) on level (cid:96) in MLDA (left) and in MLMCMC (right).

The new MLDA algorithm with subchain length J(cid:96) ∈ N on level 0 ≤ (cid:96) < L is described in
Algorithm 2.

3

Algorithm 2 (Multilevel Delayed Acceptance MCMC):
0 = . . . = θ0
Choose θ0 and set the states of all subchains θ0

1. Given θj and θj(cid:96)

L−1 = θ0. Then, for j = 0, . . . , J − 1:
(cid:96) such that j(cid:96) < J(cid:96) for all 1 ≤ (cid:96) < L, generate a subchain of length J0
0 ).

1 and using the transition kernel q(θ(cid:48)

0 = θj1

0|θj1

with Alg. 1 on level 0, starting from θ0
1 = θJ0
0 .

2. Let (cid:96) = 1 and θ(cid:48)
3. If (cid:96) = L go to Step 7. Otherwise compute the delayed acceptance probability on level (cid:96),

i.e.,

(cid:40)

α(cid:96) = min

1,

(cid:41)

.

L(cid:96)(d|θ(cid:48)
L(cid:96)(d|θj(cid:96)

(cid:96)) L(cid:96)−1(d|θj(cid:96)
(cid:96) )
(cid:96) )L(cid:96)−1(d|θ(cid:48)
(cid:96))
= θj(cid:96)

= θ(cid:48)

(cid:96) with probability α(cid:96) and θj(cid:96)+1
(cid:96)+1 = θJ(cid:96)

4. Set θj(cid:96)+1
(cid:96)
5. If j(cid:96) = J(cid:96) set θ(cid:48)
6. Otherwise set jk = 0 and θ0
7. Compute the delayed acceptance probability on level L, i.e.,

k = θj(cid:96)

(cid:96) , increment (cid:96) → (cid:96) + 1 and return to Step 3.

(cid:96) , for all 0 ≤ k < (cid:96), and return to Step 1.

(cid:96)

(cid:96) otherwise. Increment j(cid:96) → j(cid:96) + 1.

(cid:26)

αL = min

1,

L)L(cid:96)−1(d|θj)
L(cid:96)(d|θ(cid:48)
L(cid:96)(d|θj) L(cid:96)−1(d|θ(cid:48)
L)

(cid:27)

.

Set θj+1 = θ(cid:48)
8. Set j(cid:96) = 0 and θ0

L with probability αL and θj+1 = θj otherwise. Increment j → j + 1.

(cid:96) = θj, for all 0 ≤ (cid:96) < L, and return to Step 1.

2.3 Adaptive correction of the approximate posteriors

While the approach outlined above does guarantee sampling from the exact posterior, there are
situations when convergence can be prohibitively slow. When the model approximation is poor, the
delayed acceptance probability is low, and many proposals are rejected. This will result in suboptimal
acceptance rates and low effective sample sizes. The leftmost panel in Fig. 2 shows a contrived
example, where the approximate likelihoods (red/orange isolines) are offset from the likelihood on
the ﬁnest level (blue contours) and their scales, shapes and orientations are incorrect. Thus, as an
additional modiﬁcation, an Adaptive Error Model (AEM) is introduced to account for discrepancies
between model levels.

Figure 2: Effect of applying the Gaussian Adaptive Error Model (AEM). The ﬁrst panel shows the
initial state before adaptation, where the coarse likelihoods L(cid:96)(d|θ) (red/orange isolines) approximate
the ﬁne likelihood LL(d|θ) (blue contours) poorly. The second panel shows the effect of shifting the
likelihoods by the mean of the bias. The third panel shows the effect of additionaly incorporating
estimates of the covariance of the bias. (Adapted from [9].)

Let F(cid:96) denote a coarse forward map of level (cid:96) and FL denote the forward map on the ﬁnest level L.
To obtain a better approximation of the data d using F(cid:96), the two-level AEM suggested in [10, 11]
and analysed in [12] is extended by adding a telescopic sum of the differences in the forward model

4

output across all levels from (cid:96) to L:

d = FL(θ) + (cid:15) = F(cid:96)(θ) + B(cid:96)(θ) + (cid:15) with B(cid:96)(θ) :=

L−1
(cid:88)

k=(cid:96)

Fk+1(θ) − Fk(θ)
(cid:123)(cid:122)
(cid:125)
(cid:124)
:=Bk(θ)

,

(3)

denoting the bias on level (cid:96) at θ. The trick in the context of MLDA is that, since B(cid:96) is just a simple
sum, the individual bias terms Bk from pairs of adjacent model levels can be estimated independently,
so that new information can be exploited each time any set of adjacent levels are evaluated for
the same parameter value θ. Approximating each individual bias term Bk = Fk+1 − Fk with a
multivariate Gaussian B∗
k ∼ N (µk, Σk), the total bias B(cid:96) can be approximated by the Gaussian
(cid:96) ∼ N (µB,(cid:96), ΣB,(cid:96)) with µB,(cid:96) = (cid:80)
B∗
The bias-corrected likelihood function for level (cid:96) is then proportional to

k µk and ΣB,(cid:96) = (cid:80)

k Σk.

L∗

(cid:96) (d|θ) ∝ exp

(cid:18)

−

1
2

(cid:0)d − F(cid:96)(θ) − µ(cid:15) − µB,(cid:96)

(cid:1)T (cid:0)Σ(cid:15) + ΣB,(cid:96)

(cid:1)−1(cid:0)d − F(cid:96)(θ) − µ(cid:15) − µB,(cid:96)

(cid:1)

(cid:19)

.

(4)

One way to construct the AEM is ofﬂine, by sampling from the prior before running the MCMC, as
suggested in [10]. However, this approach requires a signiﬁcant overhead prior to sampling, and may
result in a suboptimal error model, since the bias in the posterior may differ substantially from the bias
in the prior. Instead, as suggested by [11], an estimate for the Bk can be constructed iteratively during
sampling, using the following recursive formulae for sample mean and sample covariance [13]:

µk,i+1 =

1
i + 1

(cid:16)

(cid:17)
iµk,i + Bk(θi+1)

and

Σk,i+1 =

i − 1
i

Σk,i +

(cid:16)

1
i

iµk,i µT

k,i − (i + 1)µk,i+1 µT

k,i+1 + Bk(θi+1) Bk(θi+1)T (cid:17)

(5)

(6)

While this approach in theory compromises ergodicity in the strict sense, the recursively constructed
sample moments exhibit diminishing adaptation [13].

3

Implementation and Demonstration

The Multilevel Delayed Acceptance MCMC algorithm (Alg. 2) has been implemented in PyMC3 [6],
an open-source probabilistic programming package for Python built on top of the Theano library [14].
The code is available in the development version of PyMC3.2. In the following section, we present
a numerical experiment, in which we compare the “vanilla” MLDA sampler to the AEM-activated
MLDA sampler. To demonstrate the effect of the AEM, we have chosen models of very low resolution
on the coarse levels. It is important to stress, however, that the AEM is not a strict requirement for
MLDA in cases, where the coarse models are better approximations of the ﬁne.

3.1 Example: Estimation of Soil Permeability in Subsurface Flow

In this example, a simple model problem arising in subsurface ﬂow modelling is considered. Proba-
bilistic uncertainty quantiﬁcation is of interest in various situations, for example in risk assessment
of radioactive waste repositories. Moreover, this simple PDE model is often used as a benchmark
for MCMC algorithms in the applied mathematics literature. The classical equations which govern
steady-state single-phase subsurface ﬂow are Darcy’s law coupled with an incompressibility constraint

w + k∇p = g

and ∇ · w = 0,

in D ⊂ Rd

(7)

for d = 1, 2 or 3, subject to suitable boundary conditions. Here p denotes the hydraulic head of the
ﬂuid, k the permeability tensor, w the ﬂux and g is the source term.

A typical approach to treat the inherent uncertainty in this problem is to model the permeability as
a random ﬁeld k = k(x, ω) on D × Ω, for some probability space (Ω, A, P). Therefore, (7) can be
written as the following PDE with random coefﬁcients:

−∇ · k(x, ω)∇p(x, ω) = f (x),

for all x ∈ D,

(8)

2https://github.com/pymc-devs/pymc3

5

where f := −∇ · g. As a synthetic example, consider the domain D := [0, 1]2 with f ≡ 0 and
deterministic boundary conditions

p|x1=0 = 0,

p|x1=1 = 1 and ∂np|x2=0 = ∂np|x2=1 = 0.

(9)

A widely used model for the prior distribution of the permeability in hydrology is a log-Gaussian
random ﬁeld, characterised by the mean of log k, here chosen to be 0, and by its covariance function,
here chosen to be
(cid:107)x − y(cid:107)2
2
2λ2
with σ = 2 and λ = 0.3. The log-Gaussian random ﬁeld is parametrised using a truncated Karhunen-
Loève (KL) expansion of log k, i.e., an expansion in terms of a ﬁnite set of independent, standard
Gaussian random variables θi ∼ N (0, 1), i = 1, . . . , R, given by

C(x, y) := σ2 exp

for x, y ∈ D,

(10)

−

(cid:18)

(cid:19)

,

log k(x, ω) =

R
(cid:88)

√

i=1

µiφi(x)θi(ω).

(11)

Here, {µi}i∈N are the sequence of strictly decreasing real, positive eigenvalues, and {φi}i∈N the
corresponding L2-orthonormal eigenfunctions of the covariance operator with kernel C(x, y). Thus,
the prior distribution on the parameter θ = (θi)R
i=1 in the stochastic PDE problem (8) is N (0, IR).
The aim is to infer the posterior distribution of θ, conditioned on measurements of p at M = 25
discrete locations xj ∈ D, j = 1, . . . , M , stored in the vector dobs ∈ RM . Thus, the forward operator
is F : RR → RM with Fj(θω) = p(xj, ω).

Figure 3: True log-conductivity ﬁeld of the coarsest model with m0 grid points (left) and the ﬁnest
model with m2 grid points (right).

All ﬁnite element (FE) calculations were carried out with FEniCS [15], using piecewise linear FEs on
a uniform triangular mesh. The coarsest mesh T0 consisted of m0 = 5 grid points in each direction,
while subsequent levels were constructed by two steps of uniform reﬁnement of T0, leading to
m(cid:96) = 4(cid:96)(m0 − 1) + 1 grid points in each direction on the three grids T(cid:96), (cid:96) = 0, 1, 2 (Fig. 3).
To demonstrate the excellent performance of MLDA with the AEM, synthetic data was generated by
drawing a sample θex from the prior distribution and solving (8) with the resulting realisation of k on
T2. To construct dobs, the computed discrete hydraulic head values at (xj)M
j=1, were then perturbed
by independent Gaussian random variables, i.e. by a sample (cid:15)∗ ∼ N (0, Σ(cid:15)) with Σ(cid:15) = 0.012IM .
To compare the “vanilla” MLDA approach to the AEM-enhanced version, we sampled the same
model using identical sampling parameters, with and without AEM activated. For each approach,
we sampled four independent chains, each initialised at a random point from the prior. For each
independent chain, we drew 5000 samples plus a burn-in of 2000. We used subchain lengths
J0 = J1 = 5, since that produced the best trade-off between computation time and effective sample
size for MLDA with the AEM. Note that the cost of computing the subchains on the coarser levels
only leads to about a 50% increase in the total cost for drawing a sample on level L. The PyMC3
non-blocked Random Walk Metropolis Hastings (RWMH) sampler was employed on the coarsest
level with automatic step-size tuning during burn-in to achieve an acceptance rate between 0.2 and
0.5. All other sampling parameters were maintained at the default setting of the MLDA method.

6

To assess the performance of the two approaches the Effective Sample Size (ESS) for each parameter
was computed [16]. Since the coarsest model was quite a poor approximation of the ﬁnest, running
MLDA without the Adaptive Error Model (AEM), yielded very poor results. None of the four chains
converged, there was poor mixing, a sub optimal acceptance rate of 0.019 on level L, and an ESS of
4 out of 20000 samples, meaning that each independent chain was only capable of producing a single
independent sample. When the AEM was employed and otherwise using the exact same sampling
parameters, we observed convergence for every chain, good mixing, an acceptance rate of 0.66 on
level L and an ESS of 3319 out of 20000 samples (Fig. 4). In comparison, a single-level non-blocked
RWMH sampler on grid T2 with automatic step-size tuning during burn-in produced an ESS of 19
out of 5000 samples with an acceptance rate of 0.26.

Figure 4: Traces of θ1 on level (cid:96) = 2, for MLDA without (left) and with AEM (right).

Note that the particular numerical experiment was chosen to demonstrate the dramatic effect that
employing the AEM can have in MLDA. Thus, making it possible to use multilevel sampling strategies
with very crude approximate models. A FE mesh with 25 degrees of freedom is extremely coarse
for a Gaussian random ﬁeld with correlation length λ = 0.3, yet using the AEM it still provides an
excellent surrogate for delayed acceptance. Typically much ﬁner models are used in real applications
with longer subchains on the coarser levels (cf. [4]). The AEM will be less critical in that case and
MLDA will also produce good ESS without the AEM. In a future journal paper, this topic will be
carefully studied along with a comparison with other samplers on the ﬁnest level and an analysis of
the multilevel variance reduction capabilities of MLDA.

Broader Impact

This research has the potential to make unbiased uncertainty quantiﬁcation of expensive models
available to a greater audience, including engineers employed in risk assessment and reliability engi-
neering. Since many engineering problems involve solving PDEs, multi-level hierarchies can easily
be introduced using grid reﬁnement, making this method exceptionally well suited for engineering
applications.

Acknowledgements

The work was funded by a Turing AI fellowship (2TAFFP\100007) and the Water Informatics Science
and Engineering Centre for Doctoral Training (WISE CDT) under a grant from the Engineering and
Physical Sciences Research Council (EPSRC), grant number EP/L016214/1.

References

[1] J. A. Christen and C. Fox. Markov chain Monte Carlo using an approximation. J. Comput.

Graph. Stat., 14(4):795–810, 2005.

[2] M. B. Giles. Multilevel Monte Carlo path simulation. Oper. Res., 56(3):607–617, 2008.

7

0100020003000400050000.250.300.350.400.450.501Without AEM0100020003000400050000.10.20.30.40.50.60.70.80.9With AEM[3] V. H. Hoang, C. Schwab, and A. M. Stuart. Complexity analysis of accelerated MCMC methods

for Bayesian inversion. Inverse Probl., 29(8):085010, 2013.

[4] T. J. Dodwell, C. Ketelsen, R. Scheichl, and A. L. Teckentrup. A hierarchical multilevel Markov
chain Monte Carlo algorithm with applications to uncertainty quantiﬁcation in subsurface ﬂow.
SIAM/ASA J. Uncertain. Q., 3(1):1075–1108, 2015.

[5] A. Jasra, K. Kamatani, K. Law, and Y. Zhou. A multi-index Markov chain Monte Carlo method.

Int. J. Uncertain. Quant., 8(1):61–73, 2018.

[6] J. Salvatier, T. V. Wiecki, and C. Fonnesbeck. Probabilistic programming in Python using

PyMC3. PeerJ. Comput. Sci., 2:e55, 2016.

[7] Gareth O. Roberts and Jeffrey S. Rosenthal. General state space Markov chains and MCMC

algorithms. Probability Surveys, 1(0):20–71, 2004.

[8] J. S. Liu. Monte Carlo Strategies in Scientiﬁc Comuputing. Springer International Publishing,

New York, 2004.

[9] M. B. Lykkegaard, T. J. Dodwell, and D. Moxey. Accelerating Uncertainty Quantiﬁcation
of Groundwater Flow Modelling Using Deep Neural Networks. Manuscript submitted for
publication. arXiv:2007.00400, 2020.

[10] J. Kaipio and E. Somersalo. Statistical inverse problems: Discretization, model reduction and

inverse crimes. J. Comput. Appl. Math., 198(2):493–504, 2007.

[11] T. Cui, C. Fox, and M. J. O’Sullivan. Bayesian calibration of a large-scale geothermal reservoir
model by a new adaptive delayed acceptance Metropolis Hastings algorithm. Water. Resour.
Res., 47:W10521, 2011.

[12] T. Cui, C. Fox, and M. J. O’Sullivan. A posteriori stochastic correction of reduced models in
delayed-acceptance MCMC, with application to multiphase subsurface inverse problems. Int. J.
Numer. Meth. Eng., 118(10):578–605, 2019.

[13] H. Haario, E. Saksman, and J. Tamminen. An adaptive Metropolis algorithm. Bernoulli,

7(2):223, 2001.

[14] Theano Development Team: Rami Al-Rfou et al. Theano: A Python framework for fast

computation of mathematical expressions. arXiv e-prints, abs/1605.02688, 2016.

[15] H. P. Langtangen and A. Logg. Solving PDEs in Python – The FEniCS tutorial Volume I. Simula

SpringerBriefs on Computing. Springer International Publishing, 2017.

[16] Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner.
Rank-normalization, folding, and localization: An improved R for assessing convergence of
MCMC. arXiv:1903.08008 [stat], 2020.

8

