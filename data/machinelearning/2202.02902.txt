Redactor: Targeted Disinformation Generation using
Probabilistic Decision Boundaries

Geon Heo
KAIST
Daejeon, Republic of Korea
geon.heo@kaist.ac.kr

Steven Euijong Whang
KAIST
Daejeon, Republic of Korea
swhang@kaist.ac.kr

2
2
0
2

b
e
F
7

]

G
L
.
s
c
[

1
v
2
0
9
2
0
.
2
0
2
2
:
v
i
X
r
a

ABSTRACT
Information leakage is becoming a critical problem as various in-
formation becomes publicly available by mistake, and machine
learning models train on that data to provide services. As a re-
sult, one’s private information could easily be memorized by such
trained models. Unfortunately, deleting information is out of the
question as the data is already exposed to the Web or third-party
platforms. Moreover, we cannot necessarily control the labeling
process and the model trainings by other parties either. In this set-
ting, we study the problem of targeted disinformation where the
goal is to lower the accuracy of inference attacks on a specific tar-
get (e.g., a person’s profile) only using data insertion. While our
problem is related to data privacy and defenses against exploratory
attacks, our techniques are inspired by targeted data poisoning
attacks with some key differences. We show that our problem is
best solved by finding the closest points to the target in the in-
put space that will be labeled as a different class. Since we do not
control the labeling process, we instead conservatively estimate
the labels probabilistically by combining decision boundaries of
multiple classifiers using data programming techniques. We also
propose techniques for making the disinformation realistic. Our
experiments show that a probabilistic decision boundary can be a
good proxy for labelers, and that our approach outperforms other
targeted poisoning methods when using end-to-end training on
real datasets.

1 INTRODUCTION
Information leakage is becoming a serious problem as personal
data is being used to train machine learning (ML) models. As a
striking example, Lee Luda [3] is an AI chat bot service that was
shut down soon after its release because of its hate speech towards
minorities and exposure of personal data including bank accounts
and addresses. Another example is the Web [2] where life insurers
can predict the life spans of their customers based on personal
information leaked on the Web through various online activities.
Recently, Flickr [1] has been in the news where photos uploaded in
its early days are now used as training data for various ML com-
puter vision models without author consent. Furthermore, there are
various privacy threats on ML models including inference attacks
[12, 26, 45] and reconstruction attacks [18, 19].

Unfortunately, one cannot simply delete the information if it is
published on the Web or uploaded on a third-party platform, but can
only dilute it. Even if the original data is deleted by request, there is
no way to prevent someone from extracting that information else-
where by attacking the model of the unknown third-party platform.
Moreover, there is also no control over the model training process
where anyone can train a model on the publicized data. Hence,

(a) Conventional Inference Attack Defense

(b) Redactor’s Defense

Figure 1: (a) Existing inference attack defenses (e.g., add
noise to model’s output) are not feasible when the victim
models are owned by third parties. (b) In contrast, we as-
sume the realistic setting where we can only add disinforma-
tion to the unlabeled training data, which is presumably la-
beled and used for model training by unknown model own-
ers.

conventional privacy techniques or inference attack defenses that
require ownership of the data or model are out of the question.
The only solution is to add more data to dilute one’s information
and reduce the model’s accuracy and confidence of predictions on
a certain target (e.g., personal profile) as shown in Figure 1. An
analogy is blacking out or redacting text where the reader knows
there is some information, but cannot read the text. We refer to this
problem as targeted disinformation.

The most relevant work to disinformation generation is targeted
poisoning [30, 44, 48, 53]. Recent techniques have been proposed
mainly for attacking image classification models. Given a target im-
age 𝑡 and a base image 𝑏 of a different class, the goal is to generate
an image 𝑝 that looks like 𝑏 according to a labeler, but is classified
the same as 𝑡 by the model. If transfer learning is used, this optimiza-
tion can be solved using a pre-trained model that is used to generate
features of images. Then 𝑝 can be made to be close to 𝑏 in the input

NoiseVictimAttackerAttack ModelVictim ModelOutputsUnlabeled data (e.g., the Web)Inaccessible by usInformation (e.g., membership)LabelingAttackerAttack ModelDisinformationUnlabeled data (e.g., the Web)Information (e.g., membership)Inaccessible by usVictimVictim Model(e.g. third-party platforms)OutputsLabeling 
 
 
 
 
 
space (i.e., the pixels are similar), but close to 𝑡 in the feature space.
The labeler would label 𝑝 to be the same as 𝑏, but this will confuse
the model training, which thinks that 𝑝 is the same as 𝑡. We would
like to utilize such techniques to change the output of the unknown
models (e.g., third-party platforms) and protect target instances
from indirect privacy attacks. However, most of these techniques
assume a transfer learning [33] scenario where a pre-trained model
is used to generate features in a fixed feature space. While transfer
learning benefits certain applications (e.g., NLP or vision tasks),
it is not always applicable, especially for structured data where
there is no efficient and generally accepted practice [7]. However,
structured data is important because most personal information is
stored in this format. In order to support structured data, we need
to assume end-to-end training where the feature space is no longer
fixed. As a result, existing targeted poisoning techniques cannot be
used as is for our purposes.

While our approach is inspired by targeted poisoning attacks,
the key difference is that we only utilize the input space to find the
best disinformation that is close to the target, but labeled differently.
How do we know the true label of the disinformation? Since we do
not have access to the labelers, our key idea is to conservatively
estimate human behavior using probabilistic decision boundaries
produced by combining multiple possible classifiers. We adapt data
programming [39–41], which combines multiple labeling functions
into a label model that produces probabilistic labels. In our setting,
we make the generative model produce the probability of an ex-
ample having a class that is different than 𝑡’s class. By limiting
this probability to be above a tolerance threshold, we now have a
conservative decision boundary. To illustrate our approach, Table 1
shows people records with attributes Education and Age. Let us say
𝑡 is the target, and 𝑏1 and 𝑏2 are other examples. There are three
binary surrogate models that produce the predictions ˆ𝑌1, ˆ𝑌2, and ˆ𝑌3.
Let us say the probabilistic decision boundary labels an example
as 0 (i.e., not in 𝑡’s class) only if at least 60% (intentionally low for
illustration purposes) of the surrogate models say so. As a result,
we can only safely say that 𝑏2 is not the same person as 𝑡. Given
this decision boundary, we can generate a disinformation 𝑑2 that is
labeled the same as 𝑏2, but has the closest distance to 𝑡 as shown
in Table 1 and Figure 2. We generate 𝑑2 between 𝑡 and 𝑏2 using
watermarking [10, 44] where a watermark of 𝑡 is added to 𝑏2 to
generate 𝑑2 using linear interpolation.

Our proposed system Redactor generates disinformation using
probabilistic decision boundaries and base examples as shown in Ta-
ble 1 and Figure 2. In addition, Redactor uses generative adversarial
networks [23] to produce more disinformation examples like 𝑑3 and
𝑑4. Here 𝑑4 happens to be even closer to 𝑡 than 𝑑2. Finally, Redactor
ensures the disinformation is realistic by avoiding patterns of data
that do not occur in the original data.

We evaluate Redactor on real datasets using end-to-end model
training. We show how the probabilistic decision boundaries are
valid proxies of labelers and how Redactor outperforms other tar-
geted poisoning attack baselines in terms of reducing the model’s
accuracy and confidence on the target and thus making the model’s
output more likely to change.

In the rest of the paper, we present the following:

• Overview of Redactor (Section 3) and its main components

2

ID Edu. Age

ˆ𝑌1

ˆ𝑌3 𝑃 (𝑌 = 0) Not the same as 𝑡?

ˆ𝑌2
Original Examples

𝑡
𝑏1
𝑏2

5
5
3

40
35
30

1
1
0

1
0
0

1
1
0

0
0.33
1

No
No
Yes

Generated Examples

0
0
0

0
0
0

4
7
6

0
0
1

35
40
40

Yes
Yes
Yes

1
1
0.67

𝑑2
𝑑3
𝑑4
Table 1: A table of personal records and three predictions ( ˆ𝑌1,
ˆ𝑌2, and ˆ𝑌3) of binary surrogate models. The last two columns
show how a conservative probabilistic decision boundary de-
termines which examples are different than 𝑡 by comparing
the probability of the true label 𝑌 being zero, i.e., 𝑃 (𝑌 = 0),
with a tolerance threshold of 0.6.

Figure 2: A labeler is approximated as a probabilistic model
that combines surrogate models. The decision boundaries of
surrogate models of Table 1 are shown as thin lines while the
probabilistic decision boundary is the thick gray line. Us-
ing 𝑏2, Redactor can generate the disinformation example
𝑑2 that is close to 𝑡, but still labeled differently. In addition,
Redactor can generate the fake realistic points 𝑑3 and 𝑑4 us-
ing a generative model where 𝑑4 happens to be even closer
to 𝑡.

– Probabilistic decision boundaries (Section 3.3).
– Disinformation generation (Section 3.4).
– Overall algorithm (Section 3.5).

• Experimental results (Section 4) where Redactor outperforms

existing targeted poisoning approaches.

• Related work (Section 5).

2 BACKGROUND
We clarify the difference between a transfer learning scenario and
end-to-end training scenario. We then explain why existing targeted
poisoning attacks, which rely on transfer learning, fail to perform
well in an end-to-end setting.

Comparison with Targeted Poisoning. Targeted poisoning attacks
have the goal of flipping the predictions on specific targets to a
certain class. A naïve approach is to add examples that are identi-
cal to the target, but with different labels. Unfortunately, such an
approach would not work if one does not have complete control
over the labeling process, which is unrealistic. Instead, the poison 𝑝
needs to be different enough from the target to be labeled differently
by any human. Yet, we also want 𝑝 to be close to the target as well.

td3b2d2d4Labeler (Latent)Surrogate Modelsb1(a) Transfer Learning Scenario (b) End-to-end Training Scenario

Figure 3: In a transfer learning scenario (a), the feature space
is fixed, making it possible to optimize on both the input and
feature spaces. In an end-to-end scenario (b), however, the
feature space may change after the model trains, so optimiz-
ing on the feature space may not be effective.

The state-of-the-art targeted poisoning attacks include Convex
Polytope Attack (CPA) [53] and its predecessors [10, 44, 48], which
also do not assume any control over the labeling and generate
poison examples that are similar to the base examples, but have the
same predictions as the target. Like our setting, these techniques
are not involved in the model training itself, but generate poisoned
examples that are presumably added to the training set. The goal is
to generate examples close to the target in the feature space while
being close to a base example in the input space as illustrated in
Figure 3a. The common optimization solved by these techniques is:

𝑝 = arg min

𝑥

||𝑓 (𝑥) − 𝑓 (𝑡)||2

2 + 𝛽 ||𝑥 − 𝑏 ||2
2

where 𝑓 maps inputs to features, 𝑡 is the target, 𝑏 is a base example
that is in a different class than 𝑡, 𝛽 balances the two objectives, and
𝑝 is the generated poison example.

End-to-end Training. In end-to-end training, all layers of the
model are trainable where any feature space that is not the input
space may change after model training. Therefore, CPA’s optimiza-
tion may not be effective because any distance on the feature space
corresponding to each layer can change arbitrarily. Figure 3b illus-
trates this point where the poison example 𝑝 can still be close to the
base example 𝑏 on a feature space that is not the input space even
after CPA’s optimization. To clearly demonstrate this point, we run
CPA on the AdultCensus dataset using a multilayer perceptron (see
more experimental settings in Section 4) and generate a poison
example 𝑝. We then observe how the 𝐿2 distances between 𝑝 and 𝑡
and between 𝑝 and 𝑏 change. As the model trains in a transfer learn-
ing scenario (Dotted), the feature distance from 𝑝 to 𝑡 decreases on
three different layers in the model while the input distance from
𝑝 to 𝑏 remains small. However, when the model trains end-to-end
(Solid), the feature distance from 𝑝 to 𝑡 increases rapidly, which
means that the model no longer classifies 𝑝 the same as 𝑡.

3 REDACTOR
3.1 Overview
We design an optimization problem of generating targeted disin-
formation for end-to-end training. We describe our objectives and
introduce the overall process of Redactor. In end-to-end training,

Figure 4: We run CPA [53] on the AdultCensus dataset and
observe the relative 𝐿2 distances from the initial points de-
picted in Figure 3. We measure distances from the poison
example 𝑝 to target 𝑡 on possible feature spaces (different
layers of a neural network). The transfer learning (TL) and
end-to-end training (ET) scenario results are shown as dot-
ted lines and solid lines, respectively. The TL results show
how CPA is effective in reducing the distances from 𝑝 to 𝑡,
while the ET results show how it fails to do so.

we can only utilize the input space and need to generate a disinfor-
mation that is as close as possible to the target example, but likely
to be labeled as a different class from the target. Suppose that a
human labeler has a mental decision boundary for labeling. In order
to satisfy both conditions, the disinformation must be the closest
point on the other side based on this decision boundary. Since we
do not have control of the labeling and thus do not know the de-
cision boundary, we propose to use surrogate models as a proxy
for human labeling. When combining these models, our objective
is not necessarily maximizing the overall accuracy, but instead
looking for a conservative decision boundary that can confidently
tell whether an example will be labeled differently than the target.
An additional challenge is to make the disinformation as realistic
as possible. For now, we assume there is a set 𝐶𝑟𝑒𝑎𝑙 ⊆ R𝐷 that
conceptually contains all possible realistic candidates where 𝐷 is
the number of features. In Section 3.4, we propose techniques for
generating realistic examples.

We now formulate our optimization problem as follows:

𝑁𝑑
∑︁

𝑗=1

min
{𝑑 𝑗 }

||𝑑 𝑗 − 𝑡 ||2

s.t. arg max

𝑐
max
𝑐≠𝑐𝑡

𝑀𝑐 (𝜙, 𝑑 𝑗 ) ≠ 𝑐𝑡

𝑀𝑐 (𝜙, 𝑑 𝑗 ) ≥ 𝛼

𝑑 𝑗 ∈ 𝐶𝑟𝑒𝑎𝑙 , ∀𝑗 ∈ [1 . . . 𝑁𝑑 ]

(1)

where 𝑡 ∈ R𝐷 is the target example, 𝑑 𝑗 ∈ R𝐷 is the 𝑗th disinfor-
mation among 𝑁𝑑 disinformations, 𝑐 ∈ [1...𝐶] is a class that is
not 𝑡’s class 𝑐𝑡 . 𝑀𝑐 (𝜙, 𝑥) is the probabilistic generative model that
combines surrogate models 𝜙 and returns the probability of an
example 𝑥 being in class 𝑐, and 𝛼 is the tolerance threshold for the
probabilistic decision boundary.

Redactor generates disinformation in three stages: training sur-
rogate models on the available data, generating a probabilistic de-
cision boundary, and generating disinformation examples. This
process is illustrated in Figure 5. In the next sections, we cover each
component and its techniques in more detail.

3

Feature extractor(fixed)Input spaceFeature spaceOutput spaceLinear layerPoisons’ labelTarget’s labeltbpInput spaceFeature spaceFeature extractorOutput spaceLinear layerPoisons’ labelTarget’s labeltbp010203040Number of Iterations202468Relative distancelayer1 (ET)layer2 (ET)layer3 (ET)layer1 (TL)layer2 (TL)layer3 (TL)follows:

𝑃𝑤 (Λ𝜙, 𝑌 ) = 𝑍 −1

𝑤 𝑒𝑥𝑝

(cid:32) 𝑙
∑︁

𝑤𝑇 𝐶𝑜𝑟𝑟𝑘 (Λ𝜙, 𝑦𝑘 )

(cid:33)

ˆ𝑤 = arg max

𝑤

log

∑︁

𝑌

𝑘=1
𝑃𝑤 (Λ𝜙, 𝑌 )

𝑀 (𝜙, 𝑑) = 𝑃 ˆ𝑤 (𝑌 | Λ𝜙,𝑑 ).

Here 𝐶𝑜𝑟𝑟 indicates all possible correlations between labeling func-
tions and latent 𝑌 , 𝑍 −1
𝑤 is the normalizing constant, and 𝑤 has the
weights of the generative model corresponding to each correlation.
We then use 𝑀 as the probabilistic decision boundary. For each
example 𝑑, 𝑀 returns a probability distribution of classes. Then 𝑑
is considered to be in a different class than the target 𝑡 if the class
with the maximum probability is not 𝑡’s class, and the maximum
probability is at least the tolerance threshold 𝛼.

3.4 Disinformation Generation
Given a target, we would like to find the closest possible points
that would be labeled differently. Obviously we cannot use the
target itself as it would not be labeled differently. Instead, we uti-
lize the probabilistic decision boundary to find the closest point
beyond the projected real decision boundary. We use watermark-
ing [10, 27, 38, 44] techniques where a watermark of the target is
added to the base example to generate disinformation using linear
interpolations. While this approach works naturally for image data
(i.e., the disinformation image is the same as the base image, but has
a glimpse of the target image overlaid), structured data consists of
numeric, discrete, and categorical features, so we need to perform
watermarking differently. For numeric features, we can take linear
interpolations. For discrete features that say require integer values,
we use rounding to avoid outputting real numbers as a result of the
interpolation. For categorical features, we choose the base’s value
or target’s value, whichever is closer. More formally:

𝑛𝑢𝑚𝑒𝑟𝑖𝑐 : 𝑑 (𝑖) = 𝛾𝑡 (𝑖) + (1 − 𝛾)𝑏 (𝑖)
𝑑𝑖𝑠𝑐𝑟𝑒𝑡𝑒 : 𝑑 (𝑖) = 𝑟𝑜𝑢𝑛𝑑 (𝛾𝑡 (𝑖) + (1 − 𝛾)𝑏 (𝑖) )
𝑐𝑎𝑡𝑒𝑔𝑜𝑟𝑖𝑐𝑎𝑙 : 𝑑 (𝑖) = 𝑟𝑜𝑢𝑛𝑑 (𝛾)𝑡 (𝑖) + 𝑟𝑜𝑢𝑛𝑑 (1 − 𝛾)𝑏 (𝑖)
where 𝑑 is the disinformation example, 𝑡 is the target, 𝑏 is a base
example, 𝑥 (𝑖) is 𝑥’s attributes corresponding to the feature index
set 𝑖, 𝑟𝑜𝑢𝑛𝑑 (𝑥) = ⌊𝑥 + 0.5⌋, and 0 ≤ 𝛾 ≤ 1.

In order to increase our chances of finding disinformation closer
to the target, we can use GANs to generate more bases that are
realistic and close to the decision boundary. Among possible GAN
techniques for tabular data [5, 11, 35, 46, 51, 52], we extend the con-
ditional tabular GAN (CTGAN) [51], which is the state-of-the-art
method for generating realistic, but fake tabular data. CTGAN’s key
techniques are using mode-specific normalization to learn compli-
cated column distributions and training-by-sampling to overcome
imbalanced training data.

Make Examples Realistic. CTGAN does not guarantee that all
constraints requiring domain knowledge are satisfied. For example,
in the AdultCensus dataset, the marital status “Wife” means that the
person is female, but we need to perform separate checking instead
of relying on CTGAN. Our solution is to avoid certain patterns

Figure 5: Redactor runs in three stages: surrogate model
training, probabilistic decision boundary generation, and
disinformation generation.

3.2 Training Surrogate Models
When choosing surrogate models, it is useful to have a variety of
models that can complement each other in terms of performance.
Similar strategies are used in data programming and ensemble
learning. However, our goal is not necessarily improving the over-
all accuracy of the combined model, but ensuring a conservative
probabilistic decision boundary. That is, there should be few false
positives where a disinformation that is predicted to be on the other
side of the target is actually labeled the same.

Another issue is that we may only have partial data for training
surrogate models. Indeed, if we are protecting personal information
on the Web, it is infeasible to train a model on the entire Web
data. However, we argue that we only need data that is in the
vicinity of the target and contains some examples in different classes
as well. We only require that the probabilistic decision boundary
approximates the decision making around the target. For example,
entity resolution [13, 17] also has this issue for large data, and there
are solutions for resolving subsets of data of interest [6] in order
to perform entity resolution in query time. We assume the same
setting, except that we are training models instead.

3.3 Probabilistic Decision Boundaries
We now explain how to generate a conservative probabilistic de-
cision boundary for identifying examples that will very likely not
be labeled the same as the target. We utilize multiple surrogate
models and combine them into a single probabilistic model using
data programming [39–41] techniques.

The data programming framework assumes that each labeling
function can output a positive prediction (1), negative prediction (0),
or abstained prediction (-1) if not confident enough. We implement
a labeling function using a surrogate model 𝜙𝑖 as follows where the
𝛽 parameter is used to determine when to abstain:

𝜆(𝜙𝑖, 𝑥) =

1
0
−1





𝜙𝑖 (𝑥) ≥ 0.5 + 𝛽
𝜙𝑖 (𝑥) ≤ 0.5 − 𝛽
|𝜙𝑖 (𝑥) − 0.5| < 𝛽

We train a probabilistic generative model 𝑀 with latent true
𝜙,𝑥 = 𝜆(𝜙𝑖, 𝑥 𝑗 ) as

labels 𝑌 using the label matrix Λ𝜙,𝑥 where Λ(𝑖,𝑗)

4

Available DataSurrogate ModelsDisinformation GenerationProbabilistic Decision boundaryCandidate GeneratorAlgorithm 1: Pseudo code for generating disinformation.

Input

: Target example 𝑡 , available data 𝐼 , trained surrogate
models 𝜙, trained generator model 𝐺, number of
disinformation examples 𝑁𝑑 , number of generated
samples 𝑁𝑔𝑒𝑛, tolerance threshold 𝛼, abstain threshold 𝛽

Output : Disinformation examples 𝑅
// Generate candidate examples

1 𝐶𝑟𝑒𝑎𝑙 ← [];
2 𝐵 ← NearestExamples(𝐼 , 𝑡 , 𝑁𝑑 ) 𝑠.𝑡 . 𝑐𝑏 ≠ 𝑐𝑡 ;
3 for 𝑖 in 0...𝑟 do
𝛾 ← 𝑖/𝑟 ;
4
𝐶𝑟𝑒𝑎𝑙 .append( WaterMarking(𝐵, 𝑡 , 𝛾 ) );

5
6 𝐶𝐺𝐴𝑁 ← G.generate(𝑁𝑔𝑒𝑛*𝑛);
7 𝐶𝐺𝐴𝑁 ← FilterUnrealisticRecord(𝐶𝐺𝐴𝑁 , 𝐼 );
8 𝐶𝑟𝑒𝑎𝑙 .append( NearestExamples(𝐶𝐺𝐴𝑁 , 𝑡 , 𝑁𝑔𝑒𝑛) );

// Decision boundary approximation

9 𝜙𝑡𝑜𝑝𝐾 ← SelectTopKmodels(𝐼 , 𝜙, 𝑘);
10 for 𝜙𝑖 ∈ 𝜙 do
11
12 Λ ← LabelMatrixTransform(Φ, 𝛽);
13 𝑀 ← TrainLabelModel(Λ, 𝐼 );

Φ𝑖 ← 𝜙𝑖 (𝐼 )

// Generate disinformation examples

14 𝑅 ← [];
15 for 𝑗 in 1...𝑁𝑑 do
16

𝑑 𝑗 ← arg min𝑥 ∈𝐶𝑟𝑒𝑎𝑙
𝑅.append(𝑑 𝑗 );
𝐶𝑟𝑒𝑎𝑙 .remove(𝑑 𝑗 );

17

18
19 return 𝑅;

| |𝑥 − 𝑡 | |2 𝑠.𝑡 . 𝑀𝑐 (𝑥) ≥ 𝛼 ;

that are never seen in the original data. In our example, there are
no examples where a Wife is a male, so we ignore all CTGAN-
generated examples with this combination. Instead of identifying
all these patterns manually, we automatically find them by looking
at the original data and identifying feature pairs that have very
low occurrences. In addition, we use clipping and quantization
techniques to further make sure the feature values are valid.

3.5 Putting Everything Together
Algorithm 1 shows the overall algorithm of Redactor. We first
select random base examples that are preferably close to the target,
but obviously have different labels according to our judgement
(Step 2). We then generate candidate disinformation examples using
watermarking and a CTGAN (Steps 3–8). We also construct the
probabilistic decision boundary by combining good-performing
surrogate models into a probabilistic model (Steps 9–13). Finally,
we return the disinformation examples that are on the other side
of the decision boundary from the target (Steps 14–19).

4 EXPERIMENTS
We evaluate Redactor and answer the following questions.

• Is a probabilistic decision boundary a good labeler proxy?
• How effective is Redactor’s disinformation in reducing model

accuracy and confidence?

• How realistic is Redactor’s disinformation to humans?
• Can Redactor scale to large data by using partial data?

5

4.1 Settings

Datasets. We use real tabular datasets for classification tasks. In
particular, we use two datasets that contain people records when
their information is leaked. We also use a large dataset to demon-
strate the scalability of our techniques.

• AdultCensus [31]: Contains 45,222 people examples and is
used to determine if one has a salary above a threshold of
$50K per year. The features consists of age, education, gender,
and occupation among others.

• COMPAS [4]: Contains 7,214 examples and is used to pre-
dict recidivism. The features include of name, gender, and
ethnicity among others.

• Diabetes [47]: A large dataset that contains 100,000 records
of diabetes patients in 130 US hospitals between 1999–2008.

Target and Base Examples. For each dataset, we choose 10 tar-
gets per dataset randomly. For each target, we choose 𝑘 nearest
examples with different labels as the base examples to generate 𝑘
watermarked disinformation examples.

Measures. To evaluate a probabilistic decision boundary, we use
precision, which is defined as the portion of examples that are
on the other side of the decision boundary from the target that
actually have different ground truth labels. To evaluate a model’s
performance, we measure the accuracy, which is the portion of
predictions that are correct, and use the confidence given by the
model. For all measures, we always report percentages.

Models. We use three types of models: surrogate models for prob-
abilistic decision boundaries, victim models to simulate inaccessible
black-box models, and attack models that are used to perform infer-
ence attacks (only used in Section 4.4; also see Figure 1).

We first explain the surrogate models and naming conventions:
• Seven neural networks that have different combinations of
the number of layers, the number of nodes per layer, and the
activation function. We use the naming format s_nn_A_X-
Y, which means a neural network that uses the activation
function 𝐴 (tanh, relu, log, and identity) and has 𝑋 layers
with 𝑌 nodes per layer.

• Two decision trees s_tree and two random forests (s_rf) using

the Gini and Entropy purity measures.

• Four SVM models (s_svm) using the radial basis function

(rbf), linear, polynomial, and sigmoid kernels.

• Three other models: gradient boosting (s_gb), AdaBoost

(s_ada), and logistic regression (s_logreg).

Table 2 shows a total of 18 surrogate models and their respective
train, test, and cross validation accuracies on the AdultCensus
dataset. We use the cross validation accuracies to select the top-𝑘
performing surrogate models without knowledge of the test accu-
racies. Note that the cross validation and test accuracies are similar.
We then use the following groups of surrogate models: g_all con-
tains all the models, g_top-k contains the top-performing surrogate
models, g_nn_only contains the neural network models, g_tree_only
contains the tree models, g_svm_only contains the SVM models,
and g_others contains the rest of the models.

For the victim models, we use a subset of Table 2 consisting 13
models (four neural networks, four trees and forests, two SVMs, and

Table 2: 18 surrogate model architectures and their individ-
ual Train, Test, and Cross Validation (CV) accuracies on the
AdultCensus dataset. The CV accuracies are needed to select
the top-𝑘 performing models.

Surrogate Model Train Acc. Test Acc. CV Acc.

tanh_5-2
relu_5-2
relu_50-25
relu_200-100
relu_25-10
log_5-2
identity_5-2

dt_gini
dt_entropy
rf_gini
rf_entropy

rbf
linear
polynomial
sigmoid

s_gb
s_ada
s_logreg

s_nn

s_tree

s_svm

others

86.46
86.57
90.33
95.55
87.93
85.63
84.84

85.28
85.29
85.08
85.16

85.83
84.78
85.13
81.24

85.70
86.22
84.90

85.07
85.24
82.44
81.56
84.22
85.26
84.74

85.56
85.38
85.21
85.37

84.92
85.03
83.16
82.11

85.99
86.30
84.86

84.24
84.92
82.67
81.64
83.64
84.50
84.79

84.73
84.84
84.93
84.96

84.53
84.63
82.79
82.22

86.12
86.12
84.76

three others), but with different numbers of layers and optimizers to
clearly distinguish them from the surrogate models. For the attack
models, we select 9 of the smallest models having the fewest layers,
depth, or number of tree estimators from Table 2. We choose small
models because attack models train on a victim model’s output and
loss and need to be small to perform well. We use the same naming
conventions as Table 2 except that the model names start with “a_”
instead of “s_” as shown in Table 5.

Methods. We compare Redactor with three baselines: (1) CPA is
the convex polytope attack described in Section 2; (2) GAN only is
Redactor using a CTGAN only; and (3) WM only is Redactor using
watermarking only.

Other Settings. For all models, we set the learning rate to 1e-
4 and the number of epochs to 1K. For CTGAN [51], we set the
input random vector size to 100. We use PyTorch [36] and Scikit-
learn [37], and all experiments are performed using Nvidia Titan
RTX GPUs. We evaluate all models on separate test sets.

4.2 Decision Boundary as a Labeler Proxy
We evaluate the probabilistic decision boundary precision in Ta-
ble 3, which shows the probabilistic decision boundary’s precision
for different 𝛼 tolerance threshold values. As 𝛼 increases, the pre-
cision tends to increase except for model groups with fewer than
five surrogate models. We observe that combining more surrogate
models improves the precision as well, but only to a certain extent.
Compared to taking a majority vote of surrogate models (MV), the
precision of a probabilistic decision boundary is usually higher. In
particular, using top-5 combined with 𝛼 = 0.95 results in the best
precision. We thus use this setting in the remaining sections.

Table 3: Precision for probabilistic decision boundaries with
different 𝛼 tolerance thresholds (0.5–0.99) and taking a ma-
jority vote of the surrogate models (MV).

Group

0.5

0.7

0.9

0.95

0.99

MV

g_all
g_top-15
g_top-12
g_top-10
g_top-7
g_top-5
g_top-3

g_nn-only
g_tree-only
g_svm-only
g_others

83.68
84.52
84.87
85.00
85.33
85.37
85.30

81.74
85.44
82.96
85.13

83.86
84.72
84.87
85.00
85.82
86.39
87.18

81.79
86.12
82.96
86.94

84.13
85.11
85.11
85.20
86.28
87.95
82.45

82.08
87.86
82.96
80.33

84.35
85.22
85.25
85.49
86.96
88.74
82.45

82.32
88.18
82.96
80.33

84.38
85.59
85.79
86.28
88.28
78.92
78.54

82.66
79.34
64.20
77.27

84.42
84.37
83.81
83.90
83.66
84.24
75.39

84.43
84.35
84.83
75.39

Table 4: Average performance changes of victim models on
targets when generating 500 and 50 disinformation exam-
ples on the AdultCensus and COMPAS datasets, respectively.
We average the performances of the 13 victim models.

Overall
Test Acc.

Target
Accuracy

Target
Confidence

AdultCensus

COMPAS

CPA

-2.78±8.08

-0.27±0.52

-1.97±8.60
GAN only -0.49±0.65 -16.67±13.72 -13.30±10.69
WM only -1.43±1.40 -28.89±12.78 -21.35±15.00
Redactor -1.99±1.73 -37.22±13.20 -26.23±14.44

CPA

-2.24±3.10
-0.26±0.80
GAN only -0.14±0.72
-2.30±3.31
WM only -2.31±2.08 -32.77±20.23 -21.68±13.26
Redactor -2.40±2.18 -33.89±18.83 -23.93±14.37

-0.56±5.39
-5.56±10.96

4.3 Disinformation Performance
We evaluate Redactor’s disinformation in terms of how it reduces a
victim model’s accuracy and confidence on the AdultCensus and
COMPAS datasets in Table 4. For each dataset, we train the 13 victim
models described in Section 4.1. We then generate disinformation
for the targets (500 examples for AdultCensus and 50 for COMPAS)
and re-train the victim models on the dataset plus disinformation.
We repeat each training four times. As a result, Redactor reduces
the performances more than the other baselines (especially CPA)
without reducing the test accuracy drastically.

Using the same victim model setting, we also analyze how the
number of disinformation examples and the distance between the
target and disinformation impacts the disinformation performance.
We first select 10 random target examples and vary the number
of disinformation examples generated. Figure 6a shows how the
average target accuracy and confidence of the 13 victim models
decrease further as more disinformation examples are generated,
but eventually plateaus. Next, we select 50 random target examples
and generate disinformation. Then we cluster the targets by their
average 𝐿2 distances to their disinformation examples. We then
plot each cluster in Figure 6b, which shows the average target
accuracy and confidence degradation of the 13 models against the

6

Table 5: Using Redactor’s disinformation to defend against
MIAs. For 10 target examples, a total of 200 disinformation
examples are generated. For each attack model, we show
how the disinformation changes its performances.

Without Disinfo. With Disinfo.

Attack
Model

a_tanh_5-2
a_relu_5-2
a_identity_5-2
a_dt_gini
a_dt_entropy
a_rf_gini
a_rf_entropy
a_ada
a_logreg

AVG

Overall Target Overall Target Target Acc.
F1 score Acc.

F1 score Acc.

Change

58.96
61.18
59.19
52.02
52.22
52.20
51.78
53.85
61.30

55.86

78.57
82.86
75.71
73.33
60.00
65.00
55.71
61.43
80.00

70.29

59.41
61.33
59.04
51.04
51.66
52.03
51.86
52.74
61.20

65.71
71.43
65.71
46.67
43.33
51.67
45.71
54.29
70.00

55.59

57.17

-12.86
-11.43
-10.00
-26.66
-16.67
-13.33
-10.00
-7.14
-10.00

-13.12

Figure 7: A comparison of average local surrogate model ac-
curacies when training the models on the Diabetes dataset.

4.6 Scalability
If a dataset is too large, we can still run Redactor on partial data as
explained in Section 3.2. We evaluate Redactor on the large Diabetes
dataset by first selecting 10 random targets and then training the
surrogate models on nearest neighbors of the targets where we
maintain of balance of different classes. As long as the surrogate
models are accurate, we know Redactor performs well. Figure 7
shows that, as the number of nearest neighbors increases, the aver-
age local accuracy on a separate test set of other nearest neighbors
rapidly increases. In particular, using 2,000 nearest neighbors (2% of
entire data) gives an average local accuracy comparable (within 1%)
to the average accuracy of training models on the entire dataset.

5 RELATED WORK
Redactor is related to multiple disciplines, and we explain why it
solves a novel problem. The objective of generating disinformation
is related to data privacy and deletion, although the problem setting
is different where there is a single target to protect, and there is
no control over the model or data. The techniques of Redactor are
closely related to data poisoning attacks, although we assume the
more common setting of end-to-end training instead of a transfer
learning setting. Finally, Redactor generates realistic disinformation
primarily for structured data.

Data Privacy, Deletion, and Disinformation. Data privacy is a
broad discipline of protecting one’s personal information within

7

(a)

(b)

Figure 6: (a) As the number of disinformation examples in-
creases, the target accuracy and confidence decrease signifi-
cantly while the overall test accuracy decreases only by 2%.
(b) As the distance to the target increases, we observe increas-
ing trends as opposed to (a).

average target-disinformation 𝐿2 distance of each cluster. As the
disinformation is further away from a target, it becomes difficult to
reduce the target’s accuracy and confidence.

4.4 Defense Against Inference Attacks
Redactor can also defend against membership inference attacks
(MIAs), which are the most popular inference attacks studied in the
literature. The goal of an MIA is to train an attack model that pre-
dicts if a specific example was used to train a victim model based on
its predictions and loss values. As explained in Section 4.1, we use
9 independent models in Table 2 with different hyperparameters
for attacking the trained victim models. We use the AdultCensus
dataset and select 10 target examples. Table 5 shows the MIA per-
formances with and without 200 disinformation examples using the
9 attack models. For each scenario, we specify the attack model’s
overall 𝐹1 score and average target accuracy. We use the 𝐹1 score
just for this experiment to address the class imbalance of member-
ship vs. non-membership. Each experiment is repeated seven times.
The less accurate the attack model, the better the privacy of the
target. As a result, the overall 𝐹1 score of the attack model does
not change much, but the target accuracy decreases significantly
(by up to 26%) due to the disinformation. Furthermore some target
accuracies drop to around 50%, which means the classification is
almost random.

4.5 Realistic Examples
We perform a comparison of our disinformation with real data to
see how realistic it is. Recall in Section 3.4 that we filter out ex-
amples that contain feature pair patterns that do not occur in the
original data. Table 6 shows a representative disinformation ex-
ample (among many others) that was generated using our method
along with the target and the target’s nearest examples. To see if
the disinformation is realistic, we conduct a poll asking 11 human
workers to correctly identify 5 disinformation and 5 real exam-
ples. As a result, the average accuracy is 53%, and the accuracies
for identifying disinformation and real examples are 40% and 65%,
respectively. We thus conclude that humans cannot easily distin-
guish our disinformation from real examples, and that identifying
disinformation examples is harder than identifying real examples.

02004006008001000# disinformation examples403020100Performance Change012345Avg. target-disinfo L2 distance6040200Test Acc.Target Acc.Target Conf.01000200030004000500060007000# nearest examples62646668Local AccuracyPartial dataAll dataTable 6: Comparison of disinformation with real data using the AdultCensus dataset.

Age Workclass Education Marital status

Occupation

Relationship Race Gender Capital gain Hrs/week Country Income

Target

38

Private

HS-grad

Never-married Machine-op-inspct Not-in-family White Male

0

Disinformation

43

Private

HS-grad

Never-married Machine-op-inspct Not-in-family White Male

7676

Nearest Examples to Target

41
37
52
36

Private
Private
Private
Private

HS-grad
HS-grad
HS-grad
HS-grad Married-civ-spouse Machine-op-inspct

Never-married Machine-op-inspct Not-in-family White Male
Never-married Machine-op-inspct Not-in-family White Male
Never-married Machine-op-inspct Not-in-family White Male
Husband White Male

0
0
0
7298

40

40

40
40
45
40

US

≤50K

US

>50K

US
US
US
US

≤50K
≤50K
>50K
>50K

data. The most popular approach is differential privacy [14–16]
where random records are added to a database to lower the chance
of information leakage. In comparison, our problem is a special
case of data privacy where there is no control over the data, and
the only way to improve one’s privacy is to add disinformation.

A related problem is data deletion where the goal is to make a
model forget about certain data. Data deletion has been recently
studied when using 𝑘-means clustering [20], non-iterative ML mod-
els [43], 𝐿2-regularized linear regression [25, 28], neural networks
(scrubbing [21, 22] and forgetting [8, 24, 50]). Most of these tech-
niques assume that the data or model can be changed or updated
at will. In comparison, we only assume that data can be added and
that models may be trained with the new data at some point. We
also do not assume label information, but do not control the end
model either.

The concept of disinformation is not new and has been studied
in different contexts. A work on data leakage detection [34] uses
disinformation to determine whether any information has been
leaked when data is distributed. A work on entity resolution [49]
proposes optimization techniques for lowering the entity resolution
accuracy while using a limited budget for generating disinformation.
In comparison, Redactor focuses on obfuscating information in ML
models for data privacy.

Data Poisoning. Targeted poisoning attacks [30, 44, 48, 53] have
the goal of flipping the prediction of specific targets to a certain
class. Clean-label attacks [44, 48] have been proposed for neural
networks to alter the model’s behavior on a specific test instance
by poisoning the training set without having any control over the
labeling. Convex Polytope Attack (CPA) [53] is another type of
clean-label targeted poisoning and has been proposed to generate
examples that are similar to the base examples, but have similar
predictions as a target. CPA is not involved in the model training,
but generates poisoned examples that are presumably added to the
training set. The goal is to generate examples close to the target
in the feature space while close to a base in the input space. CPA
performs well for image datasets where transfer learning is used.
Both of these techniques primarily target a transfer learning setting
whereas Redactor is designed for end-to-end training.

8

Exploratory attacks are used to extract information from mod-
els, and various defenses have been proposed. The dominant at-
tack most related to our work is the membership inference at-
tack, and many defenses [29, 32, 42] have been proposed. However,
most works assume access to the victim’s model. For example,
MemGuard [29] is a state-of-the-art defense that adds noise to the
model’s output to drop the attack model’s performance. Other tech-
niques include adding a regularizer to the model’s loss function [32]
and applying dropout or model stacking techniques [42]. However,
such model modifications are not possible in our setting where we
assume no access to the model.

Tabular Data Generation. Generating fake tabular data is be-
coming a major area in GAN research [9, 35, 51, 52]. CTGAN [51]
generates realistic data, but obviously does not necessarily sat-
isfy all necessary constraints as they require domain knowledge.
FakeTables [9] focuses on satisfying functional dependencies when
normalizing tables. LowProFool [5] generates adversarial examples
that are imperceptible by only modifying relatively unnoticeable
features. Redactor can utilize any of these techniques for generating
realistic base examples.

6 CONCLUSION
We proposed effective targeted disinformation methods for black-
box models on structured data where there is no access to the
labeling or model training. We explained why an end-to-end train-
ing setting is important and that existing poisoning attacks that
rely on a transferable learning setting do not perform well. We
then presented Redactor, which is designed for end-to-end training
where it generates a conservative probabilistic decision boundary
to emulate labeling and then generates realistic disinformation ex-
amples that reduce the target’s accuracy and confidence the most.
Our experiments showed that Redactor generates disinformation
more effectively than other poisoning attacks, defends against mem-
bership inference attacks, and generates realistic disinformation.

REFERENCES
[1] [n.d.].

How Photos of Your Kids Are Powering Surveillance Technol-
ogy. https://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-
recognition.html. Accessed Jan. 22nd, 2022.

[2] [n.d.]. Insurers Test Data Profiles to Identify Risky Clients. https://www.wsj.
com/articles/SB10001424052748704648604575620750998072986. Accessed Jan.
22nd, 2022.

[3] [n.d.]. South Korean AI chatbot pulled from Facebook after hate speech towards
minorities. https://www.theguardian.com/world/2021/jan/14/time-to-properly-
socialise-hate-speech-ai-chatbot-pulled-from-facebook. Accessed Jan. 22nd,
2022.

[4] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine
bias: There’s software used across the country to predict future criminals. And
its biased against blacks. ProPublica.

[5] Vincent Ballet, Xavier Renard, Jonathan Aigrain, Thibault Laugel, Pascal Frossard,
and Marcin Detyniecki. 2019. Imperceptible Adversarial Attacks on Tabular Data.
CoRR abs/1911.03274 (2019).

[6] Indrajit Bhattacharya, Lise Getoor, and Louis Licamele. 2006. Query-time entity

resolution. In SIGKDD. 529–534.

[7] Vadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawel-
czyk, and Gjergji Kasneci. 2021. Deep neural networks and tabular data: A survey.
arXiv preprint arXiv:2110.01889 (2021).

[8] Lucas Bourtoule, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hen-
grui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2019.
Machine Unlearning. CoRR abs/1912.03817 (2019).

[9] Haipeng Chen, Sushil Jajodia, Jing Liu, Noseong Park, Vadim Sokolov, and V. S.
Subrahmanian. 2019. FakeTables: Using GANs to Generate Functional Depen-
dency Preserving Tables with Bounded Real Data. In IJCAI. 2074–2080.

[10] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Targeted
Backdoor Attacks on Deep Learning Systems Using Data Poisoning. CoRR
abs/1712.05526 (2017).

[11] Edward Choi, Siddharth Biswal, Bradley A. Malin, Jon Duke, Walter F. Stewart,
and Jimeng Sun. 2017. Generating Multi-label Discrete Patient Records using
Generative Adversarial Networks. In MLHC, Vol. 68. PMLR, 286–305.

[12] Christopher A Choquette Choo, Florian Tramer, Nicholas Carlini, and Nicolas
Papernot. 2020. Label-Only Membership Inference Attacks. arXiv preprint
arXiv:2007.14321 (2020).

[13] Peter Christen. 2012. Data Matching - Concepts and Techniques for Record Linkage,

Entity Resolution, and Duplicate Detection. Springer.

[14] Cynthia Dwork. 2011. A firm foundation for private data analysis. Commun.

ACM 54, 1 (2011), 86–95.

[15] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. 2006. Cali-
brating Noise to Sensitivity in Private Data Analysis. In TCC, Vol. 3876. Springer,
265–284.

[16] Cynthia Dwork and Aaron Roth. 2014. The Algorithmic Foundations of Differen-

tial Privacy. Found. Trends Theor. Comput. Sci. 9, 3-4 (2014), 211–407.

[17] Ahmed K. Elmagarmid, Panagiotis G. Ipeirotis, and Vassilios S. Verykios. 2007.
Duplicate Record Detection: A Survey. IEEE Trans. Knowl. Data Eng. 19, 1 (2007),
1–16.

[18] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion
attacks that exploit confidence information and basic countermeasures. In Pro-
ceedings of the 22nd ACM SIGSAC Conference on Computer and Communications
Security. 1322–1333.

[19] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas
Ristenpart. 2014. Privacy in pharmacogenetics: An end-to-end case study of
personalized warfarin dosing. In 23rd {USENIX} Security Symposium ({USENIX}
Security 14). 17–32.

[20] Antonio Ginart, Melody Y. Guan, Gregory Valiant, and James Zou. 2019. Making
AI Forget You: Data Deletion in Machine Learning. In NeurIPS. 3513–3526.
[21] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Eternal Sunshine
of the Spotless Net: Selective Forgetting in Deep Networks. In CVPR. 9301–9309.
[22] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Forgetting Outside
the Box: Scrubbing Deep Networks of Information Accessible from Input-Output
Observations. In ECCV. 383–398.

[23] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-
Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Generative
Adversarial Nets. In NIPS. 2672–2680.

[24] Laura Graves, Vineel Nagisetty, and Vijay Ganesh. 2021. Amnesiac Machine

Learning. In AAAI. 11516–11524.

[25] Chuan Guo, Tom Goldstein, Awni Y. Hannun, and Laurens van der Maaten. 2020.

Certified Data Removal from Machine Learning Models. In ICML. 3832–3842.

[26] Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. 2019.
LOGAN: Membership inference attacks against generative models. Proceedings
on Privacy Enhancing Technologies 2019, 1 (2019), 133–152.

[27] Dorjan Hitaj and Luigi V Mancini. 2018. Have you stolen my model? evasion
attacks against deep neural network watermarking techniques. arXiv preprint
arXiv:1809.00615 (2018).

9

[28] Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Y. Zou. 2020.
Approximate Data Deletion from Machine Learning Models: Algorithms and
Evaluations. CoRR abs/2002.10077 (2020).

[29] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhenqiang
Gong. 2019. MemGuard: Defending against Black-Box Membership Inference
Attacks via Adversarial Examples. In CCS. ACM, 259–274.

[30] Mehran Mozaffari Kermani, Susmita Sur-Kolay, Anand Raghunathan, and Niraj K.
Jha. 2015. Systematic Poisoning Attacks on and Defenses for Machine Learning
in Healthcare. IEEE J. Biomed. Health Informatics 19, 6 (2015), 1893–1905.
[31] Ron Kohavi. 1997. Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-

Tree Hybrid. KDD (09 1997).

[32] Jiacheng Li, Ninghui Li, and Bruno Ribeiro. 2021. Membership Inference Attacks

and Defenses in Classification Models. In CODASPY. ACM, 5–16.

[33] Sinno Jialin Pan and Qiang Yang. 2010. A Survey on Transfer Learning. IEEE

Trans. Knowl. Data Eng. 22, 10 (2010), 1345–1359.

[34] Panagiotis Papadimitriou and Hector Garcia-Molina. 2011. Data Leakage Detec-

tion. IEEE Trans. Knowl. Data Eng. 23, 1 (2011), 51–63.

[35] Noseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu
Park, and Youngmin Kim. 2018. Data Synthesis based on Generative Adversarial
Networks. Proc. VLDB Endow. 11, 10 (2018), 1071–1083.

[36] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
2017. Automatic Differentiation in PyTorch. In NIPS Autodiff Workshop.

[37] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss,
Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. the
Journal of Machine Learning Research 12 (2011), 2825–2830.

[38] Erwin Quiring, Daniel Arp, and Konrad Rieck. 2018. Forgotten siblings: Unifying
attacks on machine learning and digital watermarking. In 2018 IEEE European
Symposium on Security and Privacy (EuroS&P). IEEE, 488–502.

[39] Alexander Ratner, Stephen H. Bach, Henry R. Ehrenberg, Jason Alan Fries, Sen
Wu, and Christopher Ré. 2017. Snorkel: Rapid Training Data Creation with Weak
Supervision. Proc. VLDB Endow. 11, 3 (2017), 269–282.

[40] Alexander Ratner, Stephen H. Bach, Henry R. Ehrenberg, Jason A. Fries, Sen
Wu, and Christopher Ré. 2020. Snorkel: rapid training data creation with weak
supervision. VLDB J. 29, 2-3 (2020), 709–730.

[41] Alexander J. Ratner, Stephen H. Bach, Henry R. Ehrenberg, and Christopher
Ré. 2017. Snorkel: Fast Training Set Generation for Information Extraction. In
SIGMOD. 1683–1686.

[42] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and
Michael Backes. 2019. ML-Leaks: Model and Data Independent Membership
Inference Attacks and Defenses on Machine Learning Models. In NDSS.
[43] Sebastian Schelter. 2020. "Amnesia" - Machine Learning Models That Can Forget

User Data Very Fast. In CIDR.

[44] Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
Tudor Dumitras, and Tom Goldstein. 2018. Poison Frogs! Targeted Clean-Label
Poisoning Attacks on Neural Networks. In NeurIPS. 6106–6116.

[45] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership inference attacks against machine learning models. In 2017 IEEE Sympo-
sium on Security and Privacy (SP). IEEE, 3–18.

[46] Akash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann, and Charles
Sutton. 2017. VEEGAN: Reducing Mode Collapse in GANs using Implicit Varia-
tional Learning. In NeurIPS. 3308–3318.

[47] Beata Strack, Jonathan P. DeShazo, Chris Gennings, Juan L. Olmo, Sebastian
Ventura, Krzysztof J. Cios, and John N. Clore. 2014. Impact of HbA1c Measurement
on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient
Records. BioMed Research International (2014).

[48] Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daumé III, and Tudor
Dumitras. 2018. When Does Machine Learning FAIL? Generalized Transferability
for Evasion and Poisoning Attacks. In 27th USENIX Security Symposium. 1299–
1316.

[49] Steven Euijong Whang and Hector Garcia-Molina. 2013. Disinformation tech-

niques for entity resolution. In CIKM. 715–720.

[50] Yinjun Wu, Edgar Dobriban, and Susan B. Davidson. 2020. DeltaGrad: Rapid

retraining of machine learning models. In ICML. 10355–10366.

[51] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni.

2019. Modeling Tabular data using Conditional GAN. In NeurIPS. 7333–7343.

[52] Lei Xu and Kalyan Veeramachaneni. 2018. Synthesizing Tabular Data using

Generative Adversarial Networks. CoRR abs/1811.11264 (2018).

[53] Chen Zhu, W. Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and
Tom Goldstein. 2019. Transferable Clean-Label Poisoning Attacks on Deep Neural
Nets. In ICML. 7614–7623.

