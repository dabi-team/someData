Orthogonal Matrices for MBAT Vector Symbolic 
Architectures, and a ‚ÄúSoft‚Äù VSA Representation for JSON  

Stephen I. Gallant 
sgallant@textician.com 

Textician 

Cambridge, MA 02138 
February 7, 2022 

Abstract 

Vector Symbolic Architectures (VSAs) give a way to represent a complex object as a single 
fixed-length vector, so that similar objects have similar vector representations.  These vector 
representations then become easy to use for machine learning or nearest-neighbor search.  We 
review a previously proposed VSA method, MBAT (Matrix Binding of Additive Terms), which 
uses multiplication by random matrices for binding related terms.  However, multiplying by such 
matrices introduces instabilities which can harm performance.  Making the random matrices be 
orthogonal matrices provably fixes this problem.  With respect to larger scale applications, we see 
how to apply MBAT vector representations for any data expressed in JSON.  JSON is used in 
numerous programming languages to express complex data, but its native format appears highly 
unsuited for machine learning.  Expressing JSON as a fixed-length vector makes it readily usable 
for machine learning and nearest-neighbor search.  Creating such JSON vectors also shows that a 
VSA needs to employ binding operations that are non-commutative.  VSAs are now ready to try 
with full-scale practical applications, including healthcare, pharmaceuticals, and genomics. 

Keywords:  MBAT (Matrix Binding of Additive Terms), VSA (Vector Symbolic Architecture), 
HDC (Hyperdimensional Computing), Distributed Representations, Binding, Orthogonal 
Matrices, Recurrent Connections, Machine Learning, Search, JSON, VSA Applications 

1  Introduction 

We are interested in Vector Symbolic Architectures (VSAs)1 which represent a complex object as 
a single fixed-length vector so that similar objects have similar vector representations.  VSAs use 
distributed representations that encode information over many or all vector dimensions, rather 
than localist representations that encode information over a few special words or vector positions.  
Arbitrarily complex objects, such as a sentence and its parse, can be represented as a single 
distributed vector.  VSAs give a way to represent similar objects, for example ‚ÄúThe large 
elephant‚Äù vs. ‚ÄúThe huge elephant‚Äù, as similar vectors, when measured by vector distance. 

By way of notation, we assume a fixed n-dimensional space, where a typical value for n might be 
300 dimensions or 1,000 dimensions.  Vectors, V, are column vectors of dimension n, with 
subscripts identifying particular vectors, such as VA or VB.  Matrices, M, are n by n, again with 
subscripts used to identify individual matrices.   

1 Also called Hyperdimensional Computing, HDC. 

Thanks to Phil Culliton and Dmitri Rachkovskij for helpful suggestions. 

1 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
For notational convenience, we incorporate a transpose into the dot or inner product:   

VA ‚Ä¢ VB represents VA

Transpose ‚Ä¢ VB. 

An important property of high dimensional vectors is that vector sums ‚Äúremember‚Äù which 
individual vectors were added.  For example, if we add 20 random vectors to form a vector sum 
V, then we can test whether any vector, VA (or a vector similar to VA) was among the 20 vectors:  
we merely compute VA ‚Ä¢ V.  A high positive value indicates that VA was in the sum.  This Vector 
Sum Memory property is counter-intuitive, because it doesn‚Äôt hold for scalars:  if we add 20 
numbers together and get 117, there is no way to tell whether one of those numbers was 14.  The 
proof of Vector Sum Memory is straightforward: 

VA ‚Ä¢ V  =    

||VA||2  +  noise   

if VA is in the sum for V 

0   +  noise 

otherwise. 

As dimensions increase, the signal dominates the noise.  See [Gallant & Okaywe (2013)] for 
further analysis and simulations.  To take an example (from Table 2 in that paper), suppose we 
sum 100 random vectors having entries ¬±1 to get vector V.  If we now take 100,000 other random 
vectors, the probability that all 100,000 have lower dot products with V than any of the original 
100 vectors is 98% if we are working in at least 7,000 dimensions. 

Previously we proposed a method for constructing VSAs called Matrix Binding of Additive 
Terms (MBAT) [Gallant & Okaywe (2013)].  The basic idea is to associate (bind) sets of objects 
by multiplying the sum of their corresponding vectors by a binding matrix.  For each type of 
binding, the binding matrix can be a fixed random matrix.  For example, if Mactor is the binding 
matrix (possibly randomly generated) associated with Actor in a sentence parse, and Vword is the 
vector for a given word, then we could generate the vector for Actor = ‚ÄúThe clever researcher‚Äù by 

V  =  Mactor ( VThe + Vclever + Vresearcher ) 

=  Mactor VThe + Mactor Vclever + Mactor Vresearcher. 

We can recursively apply this scheme to represent arbitrarily complex nested concepts.  
Moreover, we can use Vector Sum Memory to determine whether, for example, Mactor Vresearcher is 
likely to be a part of V by taking the dot product of the two vectors and noting whether it is large. 

2  Binding and Complex Structures 

Binding is useful for representing structure.  We need it to differentiate ‚ÄúThe clever researcher 
saw a large elephant‚Äù from ‚ÄúThe clever elephant saw a large researcher.‚Äù  Both sentences use the 
same words, so binding is needed to specify whether clever refers to researcher or to elephant. 

The motivation for MBAT was that the human brain has many recurrent connections; therefore 
neuron influences are better modeled by a matrix with many recurrent connections --- not a 
triangular matrix with only feedforward influences.  This suggested using matrix multiplication 
for binding, which works.  See [Gallant & Okaywe (2013)] for more details, including discussion 
of representing sequences and capacity simulations. 

This paper has two goals: 

‚Ä¢  The first goal is to repair an instability problem with MBAT representations, due to 

choosing random matrices for binding matrices.   

2 

 
 
   
 
 
 
‚Ä¢  The second goal is to give a practical example of using MBAT to represent highly-

structured data descriptions.  For this, we will show how to employ MBAT to convert 
JSON data descriptions to fixed-length vectors, which allows JSON to be easily used for 
machine learning and similarity search.  JSON is frequently used in computer science for 
specifying and representing things like data inputs to software applications.  Normally 
there is no notion of closeness between two JSON descriptions. 

3  A problem with MBAT 

MBAT representations can handle arbitrarily complex data relationships by binding additive 
terms, where terms may themselves be the results of other bindings.  For example, 

V = Msentence ( Mactor ( VThe + Vclever + Vresearcher ) + Mverb (Vsaw) + Mobject ( VThe + Vlarge + Velephant)). 

The problem is that when using random binding matrices, entries of Mactor might be much larger 
than entries of Mobject so that the object group is ignored.  Such instability becomes more 
problematic when multiplying random matrices, such as in  Msentence Mactor or in more deeply 
nested constructs that result in many matrix multiplications.  This problem can be particularly 
acute when representing ordered sequences by repeated matrix multiplications.  For example the 
vector for the sequence [A, B, C] becomes: 

Vsequence  =   MSEQ VA  +    MSEQ MSEQ VB  +    MSEQ  MSEQ MSEQ VC. 

One possible fix for this is to normalize vectors after each binding operation, but there is a better 
way:  use random orthogonal binding matrices. 

4  Random orthogonal matrices as binding matrices  

An orthogonal matrix is a matrix whose columns, M(*,i), are mutually orthogonal and have 
length 1.  Using orthogonal binding matrices solves the stability problem. 

We first review important properties of orthogonal matrices, which are defined by 

M(*,i) ‚Ä¢ M(*,j) =   

0 ùëìùëúùëü ùëñ  ‚â†  ùëó, and  

1 ùëìùëúùëü ùëñ  =  ùëó.  

A.  Orthogonal matrices preserve vector length:  || MV ||  =  || V ||.  Thus multiplication by 

an orthogonal matrix just gives a rotation in its high-dimensional space.  This property is 
well-known, but for completeness and to help with intuition we give the simple proof.  
Note that multiplying a matrix with a vector is the same as summing the columns of the 
matrix, with each column weighted by the corresponding vector component.2 

||MV||2  =  MV ‚Ä¢ MV  

=  ( ‚àëi M(*,i) Vi  )  ‚Ä¢  ( ‚àëj M(*,j) Vj  )  

=  ‚àëi‚â†j Vi Vj ( M(*,i) ‚Ä¢ M(*,j) )  +  ‚àëi Vi Vi ( M(*,i) ‚Ä¢ M(*,i) )  by rearranging 

=  0  +  ‚àëi Vi Vi (1)    by orthogonality 

=  V ‚Ä¢ V =  ||V||2 

2 This is not how we were taught to do matrix multiplication in school. 

3 

 
 
 
 
Because magnitudes are non-negative, having equal squares proves || MV ||  =  || V ||.  
This property, along with the associativity, in general, of matrix multiplication3, makes 
binding by many orthogonal matrices stable: 

|| MA MB MC V ||  = || MA (MB (MC V)) ||  =  || V ||. 

B.  For orthogonal matrices, their inverse is just their transpose:  M-1 = Mtranspose.  This 

follows from Mtranspose  M  =  I.   

Inverses can come in handy when computing which components are bound by a 
particular binding matrix.   For example, if we want to know whether MVA or MVB  is in 
resulting vector V, we can compute M-1 V and dot it with VA and VB, rather than having 
to compute both MVA and MVB . 

Tissera and McDonnell [2014] previously proposed using orthogonal matrices with a 
variation on MBAT, solely because of orthogonal matrices having readily available 
inverses. 

C.  It‚Äôs easy to generate random orthogonal matrices4.  For each column j, in order: 

a.  Start with a random vector, Vj. 

b.  For each previous column, Vk, use Gram-Schmidt elimination, replacing Vj by  

Vj ‚Äì (Vj ‚Ä¢ Vk) Vk, to make sure Vj is orthogonal to Vk.   

(We can also see by induction that the revised Vj remains orthogonal to all Vp for 
p<k, because we are subtracting from Vj a multiple of Vk which, by induction, is 
orthogonal to Vp.) 

c.  Normalize Vj.  (In the exceedingly unlikely case that Vj is the 0 vector, go back 

to step a.) 

(It can be shown that an orthogonal matrix must also have orthonormal rows, but we do 
not make use this property.) 

By property A, using length-preserving orthogonal matrices preserves the lengths of bound terms 
in MBAT representations, regardless of the complexity of the structure that is encoded.  This 
keeps complex structures from exploding in magnitude and solves our problem. 

5  Application:  representing JSON data by MBAT VSAs  

A tough test for representing complex objects is representing a JSON data description as a fixed-
length vector using MBAT. 

JSON (www.json.org) is a language independent way to express data with arbitrarily complex 
organization.  It uses objects, arrays, and values where: 

‚Ä¢  An object is an unordered set of name/value pairs. 
‚Ä¢  An array is an ordered collection of values. 
‚Ä¢  A value can be a string in double quotes, or a number, or true or false or null, or an object or 

an array. These structures can be nested. 

3 Matrix multiplication is associative, but not commutative. 
4 Or just use Python ortho_group.rvs. 

4 

 
 
 
 
 
For example (shortened from www.json.org): 

{ 

  "squadName": "Super hero squad", 
  "homeTown": "Metro City", 
  "active": true, 
  "members": [ 

    { 
      "name": "Molecule Man", 
      "age": 29, 
      "secretIdentity": "Dan Jukes", 
      "powers": [ 

        "Radiation resistance", 
        "Turning tiny", 
        "Radiation blast" 

      ] 
    }, 
    { 
      "name": "Madame Uppercut", 
      "age": 39, 
      "secretIdentity": "Jane Wilson", 
      "powers": [ 

        "Million tonne punch", 
        "Damage resistance", 
        "Superhuman reflexes" 

      ] 
    } 

  ] 

} 

Using native JSON objects directly for machine learning looks quite daunting, to say the least.  
Similarly, if we only want to find the closest matches from a collection of JSON descriptions, we 
could hack together some ad hoc search program but, again, this would be unappealing. 

However, using MBAT representations we can convert any JSON description to a fixed-length 
vector that is ‚Äúsoft‚Äù (preserves similarities), and then do machine learning or nearest-neighbor 
searches.  The conversion is simple: 

‚Ä¢  Represent a JSON object with MBAT by converting a name/value pair to a binding 

matrix times the corresponding value vector:  Mname ( Vvalue ). 

‚Ä¢  Represent a JSON array as a vector representing a sequence, as described in Section 3 

above.  The resulting sequence vector may optionally be normalized. 

‚Ä¢  Represent a JSON value by a vector where: 

o  Strings are represented by the sum of vectors for words in the string: 

V‚Äùthe smart researcher‚Äù  =   V‚Äùthe‚Äù + V‚Äùsmart‚Äù + V‚Äùresearcher‚Äù. 

To represent a vector for a word, we can use a normalized random vector or a 
vector from some precomputed dictionary such as Word2Vec [Mikolov (2013)] 
or Glove vectors [Pennington (2014)].  If order counts in the string, then we can 
use a sequence vector.  Optionally, we can  normalize vectors for strings. 

5 

 
o  Numbers are represented by a normalized random vector when there is no 

similarity between different numbers, such as ID numbers.  If we want numerical 
similarity, we can use a set of thresholds, sometimes called a thermometer code: 

VSum  =  normalized random vector Vnumber 

+ Vvery_low  if quantity ‚â• very low threshold 

+ Vlow  if quantity ‚â• low threshold 

+ ‚Ä¶ 

+ Vhigh  if quantity ‚â• high threshold 

Each of the individual threshold vectors is a normalized random vector, and we 
also normalize the resulting sum. 

o  For each of true, false and null, use fixed, normalized random vectors. 

Thus we can convert any JSON description into an MBAT fixed-length vector.  For example: 

V =    MSquadName (VSuper + Vhero + Vsquad)   +   MHomeTown (VMexico + VCity)   +   MActive (Vtrue) 

+  Mmembers ( 

MSEQ ( 

Mname (VMolecule + VMan)  +   

Mage (normalize(Vnumber + Volder_than_15 + Volder_than_25))  + 

MSecret_identity (VDan + VJukes)  + 

Mpowers ( 

MSEQ (VRadiation + Vresistance ) 

+ MSEQ MSEQ (VTurning + Vtiny ) 

+ MSEQ MSEQ MSEQ (VRadiation + Vblast ) 

) 

) 

+ MSEQ MSEQ ( 

Mname (VMadame + VUppercut) + 

‚Ä¶ etc. 

) 

Notice that some of the 29 added vectors used in computing the resulting vector may be complex, 
such as Mmembers MSEQ Mpowers MSEQ MSEQ MSEQ VRadiation, but it has the same length as just 

6 

 
VRadiation, and is simply a rotated version of VRadiation.  Note, also, that this term is differentiated 
from the term we get by shifting one of the MSEQ binding operators: Mmembers MSEQ MSEQ Mpowers 
MSEQ MSEQ VRadiation, which would place ‚Äúradiation‚Äù in the second power of Madame Uppercut 
rather than the third power of Molecule Man.  However, if binding were commutative, both terms 
would be equal.  This illustrates the necessity for complex objects that binding be non-
commutative.  Plate [2003] (Chapter 3.6.7) was aware of this issue and proposed several non-
commutative alternative formulations to his HRR system, including matrix multiplication. 

Presumably other VSA techniques having non-commutative binding operators can also represent 
JSON descriptions, but it must be verified that deeply embedded terms don‚Äôt cause problems. 

6  Prior research 

Vector Symbolic Architectures are an expanding field, with many theoretical and applied 
directions.  Some classic papers include Kanerva [1988, 2009], Plate [2003], Gayler [1998], Levy 
and Gayler [2008], and Rachkovskij [2001].  Luckily, there is an excellent two-part review by 
Kleyko, Rachkovskij, and associates [2021a,b] that is thorough, recent, and readily available.  
We refer the reader to these two papers for a better job presenting prior research than we are able 
to give. 

7  Discussion and applications 

MBAT in particular, and VSAs in general, can be viewed as similarity-preserving hash codes for 
complex structures, where resulting hashes are amenable to machine learning and to searching 
for nearest neighbors. 

With respect to neural information processing, the Vector Sum Memory principle gives a way to 
construct a reliable memory using many independent and unreliable components, such as 
neurons.  We would be astounded if something analogous were not used by brains.   

The JSON example shows that we can represent complex data structures, and that in order to 
represent them, we need to use VSA binding operations that are non-commutative.   

The ability to represent complex objects by a distributed fixed-length vector suggests many 
applications for MBAT, including: 

‚Ä¢  Healthcare:  We can represent important parts of the Electronic Health Record (EHR) as 
a fixed-length vector.  This would permit easy modeling of who is at risk for a particular 
condition, plus many other modeling tasks. 

‚Ä¢  Pharmaceutical Development:  Having fixed-length vector representations for individual 
EHRs would permit, for a particular treatment, modeling who would benefit and who 
would experience adverse events. 

‚Ä¢  Genomics:  We should be able to represent genomic sequences as fixed-length vectors 

for modeling, where the binding mechanism can specify ranges for sequence annotations.  
This would permit predictive modeling of which genome sequences are associated with 
particular conditions or diseases. 

In conclusion, VSAs appear ready to try with full-scale, practical applications. 

7 

 
 
 
 
 
 
References 

[Gallant & Okaywe (2013)] Gallant, S. I. & Okaywe, T. W. (2013)  Representing Objects, Relations, and 

Sequences.  Neural Computation 25, 2038‚Äì2078 [arXiv:1501.07627] 

[Gayler, 1998] Gayler, R. W. (1998). Multiplicative Binding, Representation Operators & Analogy. In 

Advances in Analogy Research: Integration of Theory and Data from the Cognitive, Computational, 
and Neural Sciences, pages 1‚Äì4. 

[Kanerva, 1988] Kanerva, P. (1988). Sparse Distributed Memory. The MIT Press. 

[Kanerva, 2009] Kanerva, P. (2009). Hyperdimensional Computing: An Introduction to Computing in 
Distributed Representation with High-Dimensional Random Vectors. Cognitive Computation, 
1(2):139‚Äì159. 

[Kleyko et al., 2021a] Kleyko, D., Rachkovskij, D. A., Osipov, E., and Rahimi, A.  A Survey on 

Hyperdimensional Computing aka Vector Symbolic Architectures, Part I: Models and Data 
Transformations. arXiv:2111.06077, pages 1‚Äì27.  

[Kleyko et al., 2021b] Kleyko, D., Rachkovskij, D. A., Osipov, E., and Rahimi, A.  A Survey on 

Hyperdimensional Computing aka Vector Symbolic Architectures, Part II: Applications, Cognitive 
Models, and Challenges . arXiv:2112.15424, pages 1‚Äì36. 

[Levy and Gayler, 2008] Levy, S. D. and Gayler, R. W. (2008). Vector Symbolic Architectures: A New 

Building Material for Artificial General Intelligence. In Artificial General Intelligence (AGI), pages 
414‚Äì418. 

[Mikolov (2013)] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word 

Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013. 

[Pennington (2014)] Pennington, Jeffrey, Richard Socher, Christopher Manning.  Glove: Global Vectors for 

Word Representation.  EMNLP 2014, 1532‚Äì1543. 

[Plate, 2003] Plate, T. A. (2003). Holographic Reduced Representations: Distributed Representation for 

Cognitive Structures. Stanford: Center for the Study of Language and Information (CSLI). 

[Rachkovskij, 2001] Rachkovskij, D. A., & Kussul, E. M. (2001). Binding and normalization of binary sparse 
distributed representations by context-dependent thinning. Neural Computation, 13, 411‚Äì452. 

[Tissera and McDonnell, 2014] Tissera, M. D. and McDonnell, M. D.  Enabling ‚ÄôQuestion Answering‚Äô in the 
MBAT Vector Symbolic Architecture by Exploiting Orthogonal Random Matrices. In IEEE 
International Conference on Semantic Computing (ICSC), pages 171‚Äì174. 

8 

 
 
 
