Autonomous Platoon Control with Integrated Deep
Reinforcement Learning and Dynamic Programming

Tong Liu, Lei Lei Senior Member, IEEE, Kan Zheng Senior Member, IEEE, Kuan Zhang Member, IEEE

1

2
2
0
2

n
u
J

5
1

]

Y
S
.
s
s
e
e
[

1
v
6
3
5
7
0
.
6
0
2
2
:
v
i
X
r
a

Abstract—Deep Reinforcement Learning (DRL) is regarded
as a potential method for car-following control and has been
mostly studied to support a single following vehicle. However, it
is more challenging to learn a stable and efﬁcient car-following
policy when there are multiple following vehicles in a platoon,
especially with unpredictable leading vehicle behavior. In this
context, we adopt an integrated DRL and Dynamic Programming
(DP) approach to learn autonomous platoon control policies,
which embeds the Deep Deterministic Policy Gradient (DDPG) al-
gorithm into a ﬁnite-horizon value iteration framework. Although
the DP framework can improve the stability and performance
of DDPG, it has the limitations of lower sampling and training
efﬁciency. In this paper, we propose an algorithm, namely Finite-
Horizon-DDPG with Sweeping through reduced state space using
Stationary approximation (FH-DDPG-SS), which uses three key
ideas to overcome the above limitations, i.e., transferring network
weights backward in time, stationary policy approximation for
earlier time steps, and sweeping through reduced state space.
In order to verify the effectiveness of FH-DDPG-SS, simulation
using real driving data is performed, where the performance
of FH-DDPG-SS is compared with those of the benchmark
algorithms. Finally, platoon safety and string stability for FH-
DDPG-SS are demonstrated.

Index Terms—Platoon Control; Deep Reinforcement Learning;

Dynamic Programming

I. INTRODUCTION

Autonomous vehicle platooning has progressed dramatically
thanks to the development of Cooperative Adaptive Cruise
Control (CACC) [1]–[3] and advanced Internet of Things tech-
nologies in various ﬁelds. Platoon control aims for determining
the control input of the following autonomous vehicles so that
all the vehicles move at the same speed while maintaining the
desired distances between each pair of preceding and following
vehicles. A well-designed platoon controller is able to increase
road capacity, reduce fuel consumption, as well as enhance
driving safety and comfort [4], [5].

Platoon controllers have been proposed based on classical
control theory, such as linear controller, H∞ controller, and
Sliding Mode Controller (SMC) [4], [6]. Meanwhile, platoon
control is essentially a Sequential Stochastic Decision Problem
(SSDP), where a sequence of decisions have to be made over
a speciﬁc time horizon for a dynamic system whose states
evolve in the face of uncertainty. To solve such an SSDP
problem, a few existing works relied on the Model Predictive
Control (MPC) method [5], [7]–[11], where the trajectories of
the leading vehicles are predicted by a model.

Although the MPC controller provides some safety guar-
antees to the control policy, the control performance is still
restricted by the accuracy of the model itself. As another
promising method, Reinforcement Learning (RL) provides

favorable model-free options to solve the optimal control
problems of dynamic systems. In RL, an agent can interact
with the environment and learn an optimal control policy by
trial and error without requiring the stochastic properties of
the underlying SSDP model [12]–[14]. Moreover, the more
powerful Deep Reinforcement Learning (DRL) methods can
deal with the curse-of-dimensionality problem of RL by ap-
proximating the value functions as well as policy functions
using deep neural networks [15], [16]. In recent years, research
on DRL has made signiﬁcant progress and many popular
DRL algorithms have been proposed, including value-based
methods such as Deep Q Network (DQN) [17] and Double
DQN [18]; and actor-critic methods such as Deep Determin-
istic Policy Gradient (DDPG) [19], Asynchronous Advantage
Actor-Critic (A3C) [20], and Trust Region Policy Optimization
(TRPO) [21]. The RL/DRL algorithms have been applied to
solve the platoon control problem in a few recent literature
[22]–[31].

To elaborate, most of the contributions have addressed the
car-following control problem of supporting a single following
vehicle. An adaptive Proportional-Integral (PI) controller is
presented in [23] whose parameters are tuned based on the
state of the vehicle according to the control policy learned by
actor-critic with kernel machines. However, the PI controller
needs to predeﬁne the candidate set of parameters before
learning. In order to avoid this problem, an adaptive controller
with parameterized batch actor-critic is proposed in [24]. A
few works improve the DRL-based control policy by mod-
elling/predicting the leading vehicle (leader)’s behavior [25],
[26]. In [25], a predictive controller based on the classical
DRL algorithm DDPG [19] is presented as an alternative to
the MPC controller, which uses advanced information about
future speed reference values and road grade changes. The
human driving data has been used in [27], [28] to help RL
achieve improved performance. In [29], the DDPG is applied
for car-following control problem taking into account
the
acceleration-related delay, where the leader is assumed to
drive at a constant speed. The proposed algorithm is used for
comparing the performance of DRL and MPC for Adaptive
Cruise Control (ACC)-based car-following control problems
in [30]. It is shown that DDPG has more advantages over
MPC in the presence of uncertainties. In [31], a deterministic
promotion RL method is proposed to improve training efﬁ-
ciency, where the direction of action exploration is evaluated
by a normalization-based function and the evaluated direction
works as a model-free search guide in return.

Meanwhile, DRL-based platoon control with multiple fol-
lowing vehicles has only been studied in a few recent works

 
 
 
 
 
 
[32]–[34]. Based on Predecessor-Leader Following (PLF)
topology, a CACC-based control algorithm using DDPG is
proposed in [32]. While DDPG is the most widely used
algorithm in the existing DRL-based car-following controllers
[25], [28]–[30], [32], it is shown that although DDPG performs
well in the single following vehicle system, it is more difﬁcult
to learn a stable control policy in a platoon system with
multiple following vehicles and unpredictable leading vehicle
behavior [33], [34]. To address this problem, the DDPG-based
technique is invoked in [33] for determining the parameters of
Optimal Velocity Model (OVM) instead of directly determin-
ing the accelerations. Meanwhile, [34] proposes a Hybrid Car-
Following Strategy (HCFS) that selects the best actions derived
from the DDPG controller and the linear controller, which is
used to determine vehicle acceleration in the platoon. By com-
bining with the classical control solutions, the performance
and convergence stability of DDPG are improved in [33], [34].
However, the classical controllers also limit the performance of
the above solutions, especially in complex driving environment
with random disturbance and non-linear system dynamics.

In this context, we adopt an integrated DRL and Dynamic
Programming (DP) approach to improve the stability and
performance of DDPG-based platoon control policy without
resorting to the help of the classical controllers. Speciﬁcally,
we propose an algorithm that builds upon the Finite-Horizon
DDPG (FH-DDPG) algorithm that was applied for the energy
management of microgrids [35]. FH-DDPG addresses the un-
stable training problem of DDPG in a ﬁnite-horizon setting by
using two key ideas: backward induction and time-dependent
actors/critics. The DDPG algorithm is embedded into a ﬁnite-
horizon value iteration framework, and a pair of actor and
critic networks are trained for each time step by backward
induction. It has been demonstrated in [35] that compared with
DDPG, FH-DDPG is much more stable and achieves better
performance.

However, FH-DDPG also suffers from some limitations that
can be considered as the "side-effects" of its DP framework.
Firstly, since FH-DDPG has to train K actor and critic
networks for a ﬁnite-horizon problem with K time steps, the
sample efﬁciency of FH-DDPG is 1/K that of DDPG. Speciﬁ-
cally, for E episodes of training experience, the actor and critic
networks of DDPG are trained with EK data entries, while
each pair of the K actor and critic networks in FH-DDPG is
only trained with E data entries at the corresponding time step.
Secondly, FH-DDPG has to sweep through the entire state
space when training the actor and critic networks at each time
step. As a result, the exhaustive sweeps approach considers a
large portion of the inconsequential states, resulting in many
wasted training updates.

To address the above two limitations in FH-DDPG, we use
three key ideas in our proposed DRL algorithm for platoon
control, namely FH-DDPG with Sweeping through reduced
state space using Stationary policy approximation (FH-DDPG-
SS). The contributions of this paper can be summarized as
follows:

• To overcome the ﬁrst limitation of FH-DDPG, i.e., the
low sampling efﬁciency, we propose two key ideas,
namely transferring network weights backward in time

2

and stationary policy approximation for earlier time steps.
The ﬁrst key idea is inspired by the parameter-transfer
approach in transfer learning [36], where we transfer
the trained actor and critic network weights at time step
k + 1 to the initial network weights at time step k. Thus,
the actor and critic networks at time step k are actually
trained from the experiences of the E(K −k) data entries
of the E episodes from time steps k to K, which improves
sampling efﬁciency. The second key idea is based on the
observation that the optimal policies are approximately
stationary for earlier time steps in a ﬁnite-horizon setting.
Therefore, the time step threshold m is ﬁrst determined so
the optimal policies and action-values are approximately
stationary and constant from time steps 1 to m. Next,
we use FH-DDPG to train the actors and critics from
time steps K to m + 1, and then train a single pair of
actor and critic networks using DDPG from time steps 1
to m, where the initial target network weights are set to
the trained actor and critic network weights at time step
m+1. The sampling efﬁciency is improved since the actor
and critic networks are trained from the experiences of
the Em data entries from time steps 1 to m.

• To address the second limitation of FH-DDPG, i.e., the
wasteful updates due to exhaustive sweeps, we propose
the third key idea, namely sweeping through reduced
state space. A good "kick-off" policy is ﬁrst
learned
by exhaustive sweeps, and then a reduced state space
is obtained by testing the "kick-off" policy. Finally, the
"kick-off" policy is trained by sweeping through the
reduced state space to further improve the performance.
This approach can help agents focus learning on the states
that good policies visit often, so as to improve training
efﬁciency.

• To implement the above three key ideas, the FH-DDPG-
SS algorithm is proposed to combine and integrate the
three improvements for FH-DDPG.

The remainder of the paper is organized as follows. The
system model is introduced in Section II. Section III for-
mulates the SSDP model for platoon control. The proposed
DRL algorithms to solve the SSDP model are presented in
Section IV. In Section V,
the performance of FH-DDPG-
SS is compared with those of the benchmark algorithms by
simulation. Moreover, platoon safety and string stability are
demonstrated. Section VI concludes the paper.

II. SYSTEM MODEL

We consider a platoon control problem with a number
of N > 2 vehicles, i.e., V = {0, 1, · · · , N − 1}, wherein
the position, velocity and acceleration of a following vehi-
cle (follower) i ∈ V\{0} at time t are denoted by pi(t),
vi(t), and acci(t), respectively. Here pi(t) represents the one-
dimensional position of the center of the front bumper of
vehicle i. Each follower i is manipulated by a distributed
car-following policy of a DRL controller with Vehicle-to-
Everything (V2X) communications.

Each vehicle i ∈ V obeys the dynamics model described by

A. State Space

3

a ﬁrst-order system.

˙pi(t) = vi(t),

˙vi(t) = acci(t),

˙acci(t) = −

1
τi

acci(t) +

1
τi

ui(t),

(1)

(2)

(3)

where τi is a time constant representing driveline dynamics
and ui(t) is the vehicle control input (commanded accelera-
tion) at time t. In order to ensure driving safety and comfort,
the following constraints are applied

accmin ≤ acci(t) ≤ accmax,

umin ≤ ui(t) ≤ umax,

(4)

(5)

where accmin and accmax are the acceleration limits, while
umin and umax are the control input limits.
time t,

i.e., bumper-to-
bumper distance between follower i and its preceding vehicle
(predecessor) i − 1, is denoted by di(t) with

The headway of follower i at

di(t) = pi−1(t) − pi(t) − Li−1,

(6)

where Li−1 is the the body length of vehicle i − 1.

We adopt Constant Time-Headway Policy (CTHP) in this
paper, i.e., follower i aims to maintain a desired headway
dr,i(t), which satisﬁes

At each time step k ∈ {1, 2, · · · , K}, the controller of
follower i determines the vehicle control input ui,k, based
on the observations of the system state. vi,k and acci,k can
be measured locally, while epi,k and evi,k can be measured
through a radar unit mounted at the front of the vehicle. Thus,
the state that the follower i can obtain locally is denoted by
xi,k = [epi,k, evi,k, acci,k]T.

Additionally, the follower i can obtain the driving status
xj,k and control input uj,k of the other vehicles j ∈ V\{i}
via V2X communications.

In this paper, we adopt the Predecessors Following (PF) in-
formation topology, i.e., acci−1,k and ui−1,k are transmitted to
the follower i ∈ V\{0}. Thus, the system state for the follower
i is denoted as: Si,k = [xi,k, acci−1,k, ui−1,k]T. The state
space S = {Si,k|epi,k, evi,k ∈ [−∞, ∞], acci,k, acci−1,k ∈
[accmin, accmax], ui−1,k ∈ [umin, umax]}.

B. Action Space

The control

input ui,k of the follower i ∈ V\{0} is
regarded as the action at the time step k1. The action space
A = {ui,k|ui,k ∈ [umin, umax]}.

C. System Dynamics Model

The system dynamics are derived in discrete time on the
basis of forward Euler discretization. Note that for the leader
0, ep0,k = ev0,k = 0, thus the system dynamics model evolves
in discrete time according to

where

x0,k+1 = A0x0,k + B0u0,k,


0
0

0

0
0
0

0
0
1 − T
τ0





 , B0 =





 .

0
0
T
τ0

(10)

(11)

dr,i(t) = ri + hivi(t),

(7)

A0 =

where ri is a standstill distance for safety of follower i and
hi is a constant time gap of follower i.

The control errors, i.e., gap-keeping error epi(t) and velocity

error evi(t) of follower i are deﬁned as

For the follower i ∈ V\{0} in the platoon, we have

xi,k+1 = Aixi,k + Biui,k + Ciacci−1,k,

(12)

epi(t) = di(t) − dr,i(t),

evi(t) = vi−1(t) − vi(t).

where

Ai =

(8)

(9)



1 T −hiT
−T
0

1 − T
0
τi

1
0





 , Bi =





 , Ci =

0
0
T
τi

D. Reward Function



 .

(13)





0
T
0

III. SSDP MODEL FOR PLATOON CONTROL

An SSDP can be formulated to determine the vehicle’s con-
trol action. The time horizon is discretized into time intervals
of length T seconds (s), and a time interval [(k − 1)T, kT )
amounts to a time step k, k = 1, 2, · · · , K, where K is the
total number of time steps. In the rest of the paper, we will
use xk := x((k − 1)T ) to represent any variable x at time
(k − 1)T .

In the following,

the state space, action space, system
dynamics model, and reward function of the SSDP model are
presented, respectively.

Reward function can guide the optimization objectives and
has an impact on the convergence of the DRL algorithm. Our
objective is to minimize gap-keeping error epi,k and velocity
error evi,k while penalizing control input ui,k and the jerk to
improve the driving comfort. Note that the jerk is the change
rate in acceleration, which is given by
acci,k+1 − acci,k
T

acci,k +

ji,k =

ui,k,

= −

(14)

1
τi

1
τi

1An action is normally denoted as a in the RL literature. In this paper, we
adopt the convention in the optimal control literature and denote the action
as u for consistency.

4

where the second equality is due to the forward Euler dis-
cretization of (3).

Although the quadratic cost function is normally adopted
in optimal control problems, it is found that it does not work
well for DDPG algorithm as the sudden large changes of
reward values often decrease its training stability. Therefore,
an absolute-value cost function is adopted for DDPG in
[29], [30], [34] to improve its performance. However, we
found that the absolute-value cost function could hinder the
further performance improvement when the control errors are
relatively small. Therefore, we design a Huber loss function
[37] as the reward function for each follower i ∈ V\{0}, which
is given by

R(Si,k, ui,k) =

(cid:26) rabs,
rqua,

if rabs < ε
if rabs ≥ ε

,

(15)

where

a copy of the actor and critic networks as target networks,
i.e., µ(cid:48)(Sk+1|θµ(cid:48)
) and Q(cid:48)(Sk+1, uk+1|θQ(cid:48)
), to calculate the
target values and uses soft target update. Experience replay is
adopted in DDPG to enable stable and robust learning. When
the replay buffer is full, the oldest sample will be discarded
before a new sample is stored in the buffer. A minibatch from
the buffer is sampled at each time step in order to update
the actor and critic networks. The critic network is trained
based on the Bellman Equation where the action-value is
deﬁned as the cumulative discounted reward from time step
k: Q(Sk, uk|θQ) = (cid:80)∞
k(cid:48)=k γk(cid:48)−kR(Sk(cid:48), uk(cid:48)) and the critic
network is updated by minimizing the Root Mean Square Error
(RMSE) Lk = R(Sk, uk) + γQ(cid:48)(Sk+1, µ(cid:48)(Sk+1|θµ(cid:48)
) −
Q(Sk, uk|θQ) using the sampled gradient descent with respect
to θQ. The actor network is updated by using the sampled
deterministic policy gradient ascent on Q(Sk, µ(Sk|θµ)|θQ)
with respect to θµ.

)|θQ(cid:48)

rabs = −{|

epi,k
ˆep,max

ji,k
2accmax/T
rqua = −λ{(epi,k)2 + a(evi,k)2 + b(ui,k)2 + c(ji,kT )2},

evi,k
ˆev,max

ui,k
umax

| + a|

| + c|

| + b|

|},

where ε is the reward threshold, ˆep,max and ˆev,max are the
nominal maximum control errors such that it is larger than
most possible control errors. λ is the reward scale. a, b and c
are the positive weights and can be adjusted to determine the
relative importance of minimizing the gap-keeping error, the
velocity error, the control input and the jerk.

Thus, the expected cumulative reward Jπi of the follower
i over the ﬁnite time horizon K under a policy πi can be
expressed as

Jπi = Eπi[

K
(cid:88)

k=1

γk−1R(Si,k, ui,k)],

(18)

where γ is the reward discount factor.

The ultimate objective is to ﬁnd the optimal policy π∗

i that

maximizes the expected cumulative reward Jπi, i.e.,

π∗
i = arg max
πi

Jπi.

(19)

IV. DRL ALGORITHMS

To solve the above SSDP, we propose a DRL algorithm
which improves on the FH-DDPG algorithm [35]. In the
following, we will ﬁrst provide a brief introduction to the
FH-DDPG algorithm, and then elaborate on the proposed
improvements.

A. FH-DDPG

FH-DDPG is a combination of DRL and DP, where the
DDPG algorithm is embedded into a ﬁnite-horizon value
iteration framework. It
is designed to solve ﬁnite-horizon
SSDP and improve the stability of the DDPG algorithm.

DDPG is a well-known DRL algorithm widely applied to
continuous control. It trains both a pair of actor and critic
networks,
to derive the
optimal policy µ∗(s|θµ) and the corresponding action-value
(Q-value) Q∗(Sk, uk|θQ), respectively [19]. DDPG creates

i.e., µ(Sk|θµ) and Q(Sk, uk|θQ),

DDPG is designed to solve the inﬁnite-horizion SSDPs,
where the actors and critics are the same for every time
step. On the other hand, the optimal policies and the cor-
responding action-values are normally time-dependent in a
ﬁnite-horizon setting [38]. Therefore, there are K actors and
critics in FH-DDPG for an SSDP with K time steps. As
shown in Fig.1, FH-DDPG starts by having the myopic policy
K(SK) = µmo(SK) as the optimal policy with the terminal
µ∗
reward RK for the ﬁnal time step K. And then, the ﬁnite
horizon value iteration starts from time step K − 1, and uses
backward induction to iteratively derive the optimal policy
µ∗
k(Sk, uk|θQk ) for each
k(Sk|θµk ) and the action-value Q∗
time step k, until it reaches the ﬁrst time step k = 1. In
each time step, an algorithm similar to DDPG is adopted
to solve a one-period MDP in which an episode only con-
sists of two time steps. However, different from DDPG,
the target actor network µ(cid:48)
k ) and critic network
Q(cid:48)
time step k are ﬁxed
and set as the trained actor network µk+1(Sk+1|θµk+1) and
critic network Qk+1(Sk+1, uk+1|θQk+1) of the next time step
k + 1. This greatly increases the stability and performance of
the algorithm. The pseudocode of the FH-DDPG algorithm
is given in Appendix A. Note that in each time step, the
DDPG − FT function is used to train the respective actor
and critic networks, where DDPG − FT is the abbreviation
for DDPG with ﬁxed targets.

k(Sk+1, uk+1|θQ(cid:48)

k ) of the current

k(Sk+1|θµ(cid:48)

B. Improving sampling efﬁciency

Compared with DDPG that is trained with EK data entries
for E episodes, the actor and critic networks per time step in
FH-DDPG are only trained with E data entries. The sample
efﬁciency of FH-DDPG is 1/K that of DDPG. To improve
sampling efﬁciency, two improvements are proposed in the
following.

1) Transferring network weights backward in time:

the actor µi,k(Si,k|θµi,k ) and the critic
In FH-DDPG,
Qi,k(Si,k, ui,k|θQi,k ) at each time step k are trained with
random initial parameters. Inspired by the parameter-transfer
approach in transfer learning [36], we transfer the trained actor
and critic network weights at time step k + 1, i.e., θµi,k+1

5

Algorithm 1 FH-DDPG-NB algorithm

1: Randomly initialize actor and critic network weights as

θµ0 and θQ0

i,K(Si,K) = µmo(Si,K) for the ﬁnal time step K

θµi = θµ0 and θQi = θQ0

2: Set µ∗
3: for k = K − 1, · · · , 1 do
if k = K − 1 then
4:
5:
6:
7:
8:
9:

else

θµi = θµi,k+1 and θQi = θQi,k+1

end if
θµi,k , θQi,k ← DDPG − FT(θµi , θQi, θµ(cid:48)
Update the target network:

i, θQ(cid:48)

i, k)

10:

θµ(cid:48)

i ← θµi,k , θQ(cid:48)

i ← θQi,k

11: end for
12: return {θµi,k , θQi,k }K−1
k=1

Even though the platoon control problem is not an LQR
problem in the strict sense, since both the system state Si,k and
action ui,k have constraints and there is non-Gaussian random
disturbances, we can observe similar trends in the learned
policy by FH-DDPG. This allows us to improve sampling
efﬁciency by ﬁrst obtaining the time step threshold m such
that the action-values and optimal policies are approximately
constant and stationary when k ≤ m. And then, we adopt a
single pair of actor and critic networks from time steps 1 to
m.

To elaborate, we ﬁrst obtain m by solving the LQR problem
for platoon control and analyzing the corresponding results,
ignoring the state/action constraints and random disturbances.
This enables us to determine the value of m in an efﬁcient
manner. Then FH-DDPG is trained from time steps K to
m + 1. Next, instead of training a separate pair of actor
and critic networks for each time step from 1 to m, we
train a single actor network µi(Si,k|θµi) and critic network
Qi(Si,k, ui,k|θQi) for all the time steps k ∈ {1, 2, ..., m}.
Speciﬁcally, the actor and critic networks are trained using
DDPG, where the initial values of the target networks are
set to those of the trained actor and critic networks at time
step m + 1, i.e., θµi,m+1 and θQi,m+1. The well-trained initial
values for the target networks can signiﬁcantly increase the
stability and performance of the DDPG algorithm. In this way,
the actor and critic networks are trained from the experiences
of the Em data entries of the E episodes from time steps
1 to m. The proposed algorithm is given in Algorithm 2,
namely FH-DDPG with Stationary policy Approximation for
earlier time steps (FH-DDPG-SA). The function FH − DDPG
is realized by the FH-DDPG algorithm given in Appendix A.
Note that FH-DDPG-SA can be combined with FH-DDPG-
NB, by adopting the latter algorithm instead of FH-DDPG to
train the actor and critic networks from time steps K to m + 1
in line 2 of the pseudocode. This will result in the FH-DDPG-
SA-NB algorithm.

Fig. 1. FH-DDPG framework.

and θQi,k+1 to the initial network weights at time step k, i.e.,
θµi,k and θQi,k respectively. Thus, although µi,k(Si,k|θµi,k )
and Qi,k(Si,k, ui,k|θQi,k ) are trained based on the E data
entries of the E episodes at time step k, the trainings are
built upon the initial weights θµi,k+1 and θQi,k+1, which were
in turn trained based on the E data entries of the E episodes
at time step k+1 and built upon the initial weights θµi,k+2 and
θQi,k+2. In this way, the actor and critic networks at time step
k are actually trained from the experiences of the E(K − k)
data entries of the E episodes from time steps k to K, instead
of only the E data entries as in FH-DDPG. The proposed
algorithm is given in Algorithm 1, namely FH-DDPG with
Network weights transferring Backward in time (FH-DDPG-
NB).

2) Stationary policy approximation for earlier time steps:
Although the action-values are generally time-dependent for
the ﬁnite-horizon control problems, there is a vanishing dif-
ference in action-values when the horizon length is sufﬁciently
large [39]. Taking the ﬁnite-horizon Linear Quadratic Regula-
tor (LQR) as an example, the action-value gradually converges
to the steady-state value as the time step k decreases from
the horizon K. Moreover, for k not close to horizon K, the
LQR optimal policy is approximately stationary [40], [41].

   End1?kK1?k1kkNYYNFinite-horizon value iteration mo   Calculate target value with          (,()) KKKRSSUse DDPG with fixed targets         to train (|)        and (,|)kkkkQkkkSQSu00Initialize actor and critic network               weights with         = , = kkQQSet 1kK11''  Set target networks with  ,  kkkkQQEndStartmofor t   Set he finathe myopic poll time stepicy as the optimal        policy                *( )() KKKKSSAlgorithm 2 FH-DDPG-SA-(NB) algorithm
1: Set the time horizon as {m + 1, · · · , K}
2: {θµi,k , θQi,k }K−1
3: Set the time horizon as {1, · · · , m}
4: Set the initial target networks weights with θµ(cid:48)

k=m+1 ← FH − DDPG(−NB)

i = θµi,m+1

and θQ(cid:48)

i = θQi,m+1
5: θµi, θQi ← DDPG

C. Sweeping through reduced state space

FH-DDPG embeds DRL under the framework of DP, while
the classical approach of DP is to sweep through the entire
state space at each time step k. This exhaustive sweeps
approach leads to many wasteful updates during training, since
many of the states are inconsequential as they are visited only
under poor policies or with very low probability. An alternative
approach is trajectory sampling which sweeps according to on-
policy distribution [12]. Although trajectory sampling is more
appealing, it is impossible to be adopted by FH-DDPG due to
the latter’s backward induction framework.

Inspired by trajectory sampling, we improve FH-DDPG
by sweeping through a reduced state space. Speciﬁcally, we
ﬁrst learn a relatively good "kick-off" policy by exhaustive
sweeps, and then obtain a reduced state space by testing the
"kick-off" policy, and ﬁnally continue to train the policy by
sweeping through the reduced state space to further improve
the performance. This approach can help agents focus learning
on the states that good policies visit often, which improves
training efﬁciency. For example in platoon control, the control
errors are normally small under good policies as the ends of
the training episodes are approached, and it is not necessary
to sweep through large control error states.

For platoon control, although the theoretical bounds of gap-
keeping error epi,k and velocity error evi,k are inﬁnity, it is
impossible to sweep through an inﬁnite range when training.
Therefore, we need to restrict sweeping to a ﬁnite range at
ﬁrst. In practice, there are some empirical limits to epi,k and
evi,k for a reasonable platoon control policy. Since we consider
that FH-DDPG is trained from scratch, some relatively large
control error states could be visited during training due to
the random initial policy and exploration. Therefore, we ﬁrst
sweep through a relatively large state space, i.e.,

S ls ={[epi,k, evi,k, acci,k]T|

epi,k ∈ [−ep,max, ep,max],
evi,k ∈ [−ev,max, ev,max],
acci,k ∈ [accmin, accmax]},

(20)

where ep,max and ev,max are the same for each time step and
are larger than most control errors during the training of FH-
DDPG. Thus, we ﬁrst train FH-DDPG in the state space S ls
to learn a "kick-off" policy ˆµi,k(Si,k|ˆθµi,k ) for the follower i
at time step k, and then obtain the upper and lower bounds of
a more reﬁned state space, i.e.,

i,k ={[epi,k, evi,k, acci,k]T|
S rs

epi,k ∈ [epi,k,min, epi,k,max],
evi,k ∈ [evi,k,min, evi,k,max],

6

acci,k ∈ [acci,k,min, acci,k,max]},

(21)
for the follower i at time step k by testing ˆµi,k(Si,k|ˆθµi,k ).
Next, the actor network ˆµi,k(Si,k|ˆθµi,k ) and critic network
ˆQi,k(Si,k, ui,k|ˆθQi,k ) are further trained by FH-DDPG, which
only sweeps through S rs
i,k.

Combining the above three improvements for FH-DDPG,
we propose a novel DRL algorithm, namely FH-DDPG with
Sweeping through reduced state space using Stationary policy
approximation (FH-DDPG-SS), which is given in Algorithm 3.
Note that the overall procedure of FH-DDPG-SS is the same
as that described in Section IV.C, except that in line 2 and line
12 , the FH-DDPG-SA-NB and FH-DDPG-SA algorithms are
adopted instead of the FH-DDPG algorithm to incorporate the
improvements in Algorithm 1 and Algorithm 2. The reason
why we use FH-DDPG-SA instead of FH-DDPG-SA-NB in
line 12 is that the initial actor and critic networks weights for
all the time steps are carried over from the previous training,
so we no longer need to transfer network weights backward
in time.

Algorithm 3 FH-DDPG-SS algorithm

k=1 ← FH − DDPG − SA − NB(S ls)

1: Set S ls according to (20)
2: {ˆθµi,k , ˆθQi,k }K−1
3: for g = 1, . . . , G do
4:

Test {ˆµi,k(Si,k|ˆθµi,k )}K−1
k=1
Store {x(g)
i,k }K−1
i,k}K−1
{Bt

k=1 = {[e(g)

k=1

5:

6: end for
7: for k = 1, . . . , K − 1 do
8:

9:
10: end for
11: Set

Find the upper and lower bounds epi,k,min, epi,k,max,
evi,k,min, evi,k,max, acci,k,min and acci,k,max in Bt
i,k
Set S rs

i,k according to (21)

the initial actor and critic network weights as

{ˆθµi,k , ˆθQi,k }K−1
k=1
12: {θµi,k , θQi,k }K−1
k=1 ← FH − DDPG − SA({S rs

i,k}K−1
k=1 )

pi,k, e(g)

vi,k, acc(g)

i,k ]T}K−1

k=1

in

V. EXPERIMENTAL RESULTS

In this section, we present the simulation results of the
proposed FH-DDPG-SS algorithm as well as benchmark DRL
algorithms, i.e., DDPG, FH-DDPG, and HCFS [34].

A. Experimental Setup

All the algorithms are trained/tested using the real leader
data u0,k extracted from the Next Generation Simulation
(NGSIM) dataset. 80%(800) of the data is used for training
and 20%(200) is used for testing. The platoon control environ-
ment and the DRL algorithms are implemented in Tensorﬂow
1.14 using Python [42]. We compare the performance of the
proposed FH-DDPG-SS algorithm with the benchmark algo-
rithms in terms of the average cumulative reward. Moreover,
the platoon safety and string stability performance for FH-
DDPG-SS are also demonstrated.

The technical constraints and operational parameters of
the platoon control environment are given in Table I. The

interval for each time step is set to T = 0.1 s, and each
episode is comprised of 100 time steps (i.e., K = 100) with
a duration of 10 s. The number of vehicles in the platoon
is N = 5. We initialize the states for each of 4 followers
with xi,0 = [1.5, −1, 0], ∀i ∈ {1, 2, 3, 4}. The nominal
maximum control errors in the reward function (15) are set
to ˆep,max = 15 m and ˆev,max = 10 m/s so that it is larger
than most possible control errors during training for all DRL
algorithms. For the FH-DDPG-SS algorithm, the time step
threshold m = 11. Moreover, the maximum gap-keeping error
ep,max and maximum velocity error ev,max in (20) are set to
2 m and 1.5 m/s, respectively. Additionally, to reduce large
oscillations in ui,k and acci,k, we set ji,k in FH-DDPG and
FH-DDPG-SS in the testing phase within [−0.3, 0.6] when
k > 11 by clipping ui,k.

TABLE I
TECHNICAL CONSTRAINTS AND OPERATIONAL PARAMETERS OF THE
PLATOON CONTROL ENVIRONMENT

Notations
Platoon environment
T
K
m
N
τi
hi
State & action
ep,max
ev,max
accmin
accmax
umin
umax
Reward function
a
b
c
ˆep,max
ˆev,max
ε

Description

Interval for each time step
Total time steps in each episode
Time step threshold
Number of vehicles
Driveline dynamics time constant
Time gap

Maximum gap-keeping error
Maximum velocity error
Minimum acceleration
Maximum acceleration
Minimum control input
Maximum control input

Values

0.1 s
100
11
5
0.1 s
1 s

2 m
1.5 m/s
−2.6 m/s2
2.6 m/s2
−2.6 m/s2
2.6 m/s2

Reward coefﬁcient
Reward coefﬁcient
Reward coefﬁcient
Nominal maximum gap-keeping error 15 m
Nominal maximum velocity error
Reward threshold

0.1
0.1
0.2

10 m/s
−0.4483

The hyper-parameters for training are summarized in Table
II. The values of all the hyper-parameters were selected by
performing a grid search as in [17], using the values reported
in [19] as a reference. DDPG has two hidden layers with
256 and 128 nodes, respectively; while FH-DDPG and FH-
DDPG-SS have three hidden layers with 400, 300, and 100
nodes, respectively. The sizes of the neural networks for each
algorithm are set to its best-performing network structures.
The sizes of input layers for all DRL algorithms are the same
and decided by the PF information topology. Moreover, an
additional 1-dimensional action input is fed to the second
hidden layer for each critic network. The total number of
training episodes E for all DRL algorithms is set to 5000. For
FH-DDPG-SS, we ﬁrst train the algorithm for 3000 episodes to
learn the "kick-off" policy in the ﬁrst phase, and then continue
to train 2000 episodes within the reduced state space in the
second phase. The replay buffer sizes for DDPG and FH-
DDPG are 250000 and 2500, respectively. This is because the

7

replay buffer for FH-DDPG only stores the data entries for
the corresponding time step. Since FH-DDPG-SS is trained
in two phases, the replay buffer sizes for the ﬁrst and second
phases are 2500 and 2000, respectively. Moreover, FH-DDPG-
SS leverages the FH-DDPG-SA-(NB) algorithm, which trains
the K −m−1 actors and critics for time steps K to m+1 using
FH-DDPG, and a single pair of actor and critic for time steps
1 to m using DDPG. The soft target update is implemented
with a parameter of 0.001 for DDPG. As FH-DDPG uses a
ﬁxed target network, there is no soft target update.

B. Comparison of FH-DDPG-SS with the benchmark algo-
rithms

1) Performance for testing data: The individual perfor-
mance of each follower i ∈ {1, 2, 3, 4} as well as the sum
performance of the 4 followers are reported in Table III for
DDPG, FH-DDPG, HCFS, and FH-DDPG-SS, respectively.
For each follower, the individual performance is obtained by
averaging the returns (cumulative rewards per episode) over
200 test episodes after training is completed. Note that the in-
dividual performance of the preceding vehicles are attenuated
by following vehicles upstream the platoon for each algorithm.
Moreover, we can observe that the ranking in terms of the sum
performance of all followers for the different algorithms is FH-
DDPG-SS>HCFS>FH-DDPG>DDPG, where FH-DDPG-SS
outperforms DDPG, FH-DDPG, and HCFS algorithms by
67.08%, 13.73%, and 10.60%, respectively. Note that FH-
DDPG performs better than DDPG as it applies backward
induction and time-dependent actors/critics to increase algo-
rithm stability. Moreover, FH-DDPG-SS further improves the
performance of FH-DDPG by implementing the three key
ideas proposed in Section IV.

2) Convergence properties: The performance of DRL al-
gorithms is evaluated periodically during training by testing
without exploration noise. Speciﬁcally, we run 10 test episodes
after every 100 training episodes, and average the returns
over the 10 test episodes as the performance for the latest
100 training episodes. The performance as a function of
the number of training episodes for each follower i with
DDPG, FH-DDPG, and FH-DDPG-SS is plotted in Fig. 2.
The convergence curve of HCFS is not plotted here since
HCFS combines the trained DDPG controller with the linear
controller, and thus the convergence property of HCFS is the
same as that of DDPG. Fig. 2a shows that DDPG exhibits
signiﬁcantly larger performance oscillation during training
compared to the other two algorithms, especially for follower
4. This demonstrates that both FH-DDPG and FH-DDPG-
SS are more stable than DDPG. It can be observed from
Fig. 2b and Fig. 2c that the convergence rates of FH-DDPG
and FH-DDPG-SS are similar. However, the performance of
FH-DDPG-SS is consistently better than that of FH-DDPG
during the 5000 training episodes. For the training episodes
e < 3000, the performance gain of FH-DDPG-SS is due to
the two key ideas of transferring network weights backward
in time and stationary policy approximation for earlier time
steps. Moreover, it can be observed that there is a sudden
performance improvement for all the vehicles of FH-DDPG-
SS when e ≥ 3000. This shows that salient performance gain

8

TABLE II
HYPER-PARAMETERS OF THE DRL ALGORITHMS FOR TRAINING

Parameter

Value

Actor network size

Critic network size

Actor activation function

Critic activation function
Actor learning rate α
Critic learning rate β
Total training episodes E
Batch size Nb
Replay buffer size
Reward scale λ
Reward discount factor γ

Soft target update

Noise type

Final layer weights/biases initialization

Other layer weights biases initialization

DDPG
256, 128

256, 128

FH-DDPG
400, 300, 100

400, 300, 100

FH-DDPG-SS
400, 300, 100

400, 300, 100

relu, relu, tanh

relu, relu, relu, tanh

relu, relu, relu, tanh

relu, relu, linear
1e−4
1e−3
5000

relu, relu, relu, linear
1e−4
1e−3
5000

relu, relu, relu, linear
1e−4
1e−3
3000, 2000

250000

2500

2500, 2000

64

5e−3

1

0.001

/

FH-DDPG: /, DDPG: 0.001

Ornstein-Uhlenbeck Process with θ = 0.15 and σ = 0.5
Random uniform distribution [−3 × 10−3, 3 × 10−3]

Random uniform distribution[− 1√
f
(f is the fan-in of the layer)

, 1√
f

]

TABLE III
PERFORMANCE AFTER TRAINING WIT NGSIM DATASET. EACH EPISODE HAS 100 TIME STEPS IN TOTAL. WE PRESENT BOTH THE AVERAGE OBSERVED
PERFORMANCE OF EACH FOLLOWER AND AVERAGE PERFORMANCE SUM OF 4 FOLLOWERS FOR DDPG, FH-DDPG, HCFS, AND FH-DDPG-SS.

Algorithm

DDPG
FH-DDPG
HCFS
FH-DDPG-SS

Follower 1
-0.0680
-0.0736
-0.0673
-0.0600

Individual performance
Follower 3
Follower 2
-0.0899
-0.0876
-0.0856
-0.0845
-0.0828
-0.0740
-0.0776
-0.0691

Follower 4
-0.2980
-0.0927
-0.1005
-0.0835

Sum performance

-0.8816
-0.3364
-0.3246
-0.2902

(a) DDPG

(b) FH-DDPG

(c) FH-DDPG-SS

Fig. 2. Performance during DRL algorithms training. The vertical axis corresponds to the average returns over 10 test episodes.

can be achieved by training the algorithm in a reduced state
space determined by the "kick-off" policy for the last 2000
episodes.

3) Testing results of one episode: We focus our attention
on a speciﬁc test episode having 100 time steps, and plot
driving status epi,k, evi,k, acci,k and control input ui,k along
with jerk ji,k of each follower i for all
the time steps
k ∈ {1, 2, · · · , 100}. Fig. 3a, Fig. 3b, Fig. 3c, and Fig. 3d show
the results of a speciﬁc test episode for DDPG, FH-DDPG,

HCFS, and FH-DDPG-SS, respectively. It can be observed
that the overall shapes of the corresponding curves of all
the algorithms look very similar except that the performance
curves for follower 4 using DDPG have large oscillations.
This observation is aligned with the results in Table III,
where follower 4 has signiﬁcantly worse performance when
using DDPG compared with using other algorithms. Fig. 3
shows that in general for each follower i ∈ {1, 2, 3, 4}, epi,k
has an initial value of 1.5 m and is reduced over time to

(cid:2)(cid:3)(cid:2)(cid:2)(cid:2)(cid:4)(cid:2)(cid:2)(cid:2)(cid:5)(cid:2)(cid:2)(cid:2)(cid:6)(cid:2)(cid:2)(cid:2)(cid:7)(cid:2)(cid:2)(cid:2)(cid:1)(cid:4)(cid:2)(cid:1)(cid:3)(cid:7)(cid:1)(cid:3)(cid:2)(cid:1)(cid:7)(cid:2)PerformanceTraining episode e Follower 1 Follower 2 Follower 3 Follower 4(cid:3)(cid:4)(cid:3)(cid:3)(cid:3)(cid:5)(cid:3)(cid:3)(cid:3)(cid:6)(cid:3)(cid:3)(cid:3)(cid:7)(cid:3)(cid:3)(cid:3)(cid:8)(cid:3)(cid:3)(cid:3)(cid:1)(cid:3)(cid:2)(cid:4)(cid:4)(cid:1)(cid:3)(cid:2)(cid:4)(cid:3)(cid:1)(cid:3)(cid:2)(cid:3)(cid:11)(cid:1)(cid:3)(cid:2)(cid:3)(cid:10)(cid:1)(cid:3)(cid:2)(cid:3)(cid:9)PerformanceTraining episode e Follower 1 Follower 2 Follower 3 Follower 4(cid:3)(cid:4)(cid:3)(cid:3)(cid:3)(cid:5)(cid:3)(cid:3)(cid:3)(cid:6)(cid:3)(cid:3)(cid:3)(cid:7)(cid:3)(cid:3)(cid:3)(cid:8)(cid:3)(cid:3)(cid:3)(cid:1)(cid:3)(cid:2)(cid:4)(cid:3)(cid:1)(cid:3)(cid:2)(cid:3)(cid:12)(cid:1)(cid:3)(cid:2)(cid:3)(cid:11)(cid:1)(cid:3)(cid:2)(cid:3)(cid:10)(cid:1)(cid:3)(cid:2)(cid:3)(cid:9)PerformanceTraining episode e Follower 1 Follower 2 Follower 3 Follower 49

(a) DDPG

(b) FH-DDPG

(c) HCFS

(d) FH-DDPG-SS

Fig. 3. Results of a speciﬁc test episode. The driving status epi,k, evi,k, and acci,k along with the control input ui,k and jerk ji,k of each follower i are
represented as different curves, respectively.

approximately 0 m; evi,k has an initial value of −1 m/s and
is increased to approximately 0 m/s; ui,k is relatively large
at the beginning of the episode to increase acci,k as fast as
possible, so that epi,k and evi,k can promptly converge to
approximately 0. Correspondingly, acci,k of each follower i
has an initial value of 0 m/s2 and is suddenly increased to a
relatively large value. Then both ui,k and acci,k are quickly
reduced to a negative value, and ﬁnally are increased over
time to approximately 0 m/s2. After the driving status and
control input converge to near 0, the values ﬂuctuate around 0
with ui,k trying to maximize the expected cumulative reward
in (19) without knowing the future control inputs ui−1,k(cid:48),
k < k(cid:48) < K, of the predecessor i − 1. Additionally, ji,k of
each follower i starts with a large positive value and is then
reduced to a negative value. After converging to near 0 m/s3,
the value of ji,k ﬂuctuates around 0 m/s3.

A closer examination of Fig. 3 reveals that the performance
differences of the algorithms are reﬂected in convergence
speed to steady-state and the oscillations of the driving status
and control input. Focusing on epi,k, it can be observed that
there are still positive gap-keeping errors for followers 2, 3,
and 4 in DDPG up to the end of the episode. Moreover, epi,k
of follower 1 in DDPG has the slowest convergence speed to 0
m among all the algorithms. Meanwhile, epi,k in FH-DDPG is
reduced to 0 m for each follower, but there are relatively large
oscillations after convergence. epi,k in HCFS also converges
to 0 m, but there are large oscillations near the end of the
episode for follower 4. epi,k in FH-DDPG-SS has the fastest
convergence speed to 0 m, and then remains around 0 m with
small oscillations. Now focusing on evi,k, the velocity error
in DDPG has the slowest convergence speed to 0 m/s among
all the algorithms. On the other hand, evi,k in FH-DDPG-SS
has the smallest oscillations around 0 m/s after convergence
to steady-state. Finally, compared with the other algorithms,
FH-DDPG-SS has the smallest jerk ji,k at the beginning of
the episode. Although ji,k in FH-DDPG-SS is not as small as
that in DDPG and HCFS in the later stage of the episode, it

is smaller than that in FH-DDPG and remains at a relatively
small level, which can ensure the driving comfort.

C. Platoon safety

In order to demonstrate that the platoon safety is ensured in
the proposed FH-DDPG-SS algorithm, Table IV summarizes
the average, maximum, and minimum returns as well as
the standard deviation across the 200 test episodes for each
follower i in FH-DDPG-SS. Additionally, epi,k per time step
k for the worst test episode among the 200 test episodes is
plotted in Fig. 4.

It can be observed from Table IV that the standard de-
viation of each follower i is small ranging from 0.0015 to
0.0017. Moreover, the differences between the maximum and
minimum returns are small for all followers. Speciﬁcally, the
minimum return is worse than the maximum return by 16.28%,
15.22%, 13.95% and 12.96% for the 4 followers, respectively.

TABLE IV
THE AVERAGE, MAXIMUM, AND MINIMUM RETURN AS WELL AS THE
STANDARD DEVIATION ACROSS THE 200 TEST EPISODES FOR EACH
FOLLOWER i OF FH-DDPG-SS

Return
Average
Maximum
Minimum
Standard deviation

Follower 1 Follower 2 Follower 3 Follower 4

-0.0600
-0.0559
-0.0650
0.0015

-0.0691
-0.0644
-0.0742
0.0016

-0.0776
-0.0731
-0.0833
0.0017

-0.0835
-0.0787
-0.0889
0.0017

test episode,

it can be observed in Fig. 4 that

To demonstrate that platoon safety is ensured even in the
the
worst
followers have an initial gap-keeping error epi,0 of 1.5 m and
the gap-keeping error is reduced over time to approximately
0 m. The most negative epi,k is −0.1014 m at k = 35, which
will not result in vehicle collision since the absolute value of
the position error is much smaller than the desired headway.

020406080100012020406080100-2-10020406080100-2-1012020406080100-2-1012epi,k (m)evi,k (m/s) Follower 1 Follower 2 Follower 3  Follower 4acci,k (m/s2)ui,k (m/s2)020406080100-8081624ji,k (m/s3)Time Step k020406080100012020406080100-2-10020406080100-2-1012020406080100-2-1012epi,k (m)evi,k (m/s) Follower 1 Follower 2 Follower 3  Follower 4acci,k (m/s2)ui,k (m/s2)020406080100-8081624ji,k (m/s3)Time Step k020406080100012020406080100-2-10020406080100-2-1012020406080100-2-1012epi,k (m)evi,k (m/s) Follower 1 Follower 2 Follower 3  Follower 4acci,k (m/s2)ui,k (m/s2)020406080100-8081624ji,k (m/s3)Time Step k020406080100012020406080100-2-10020406080100-2-1012020406080100-2-1012epi,k (m)evi,k (m/s) Follower 1 Follower 2 Follower 3  Follower 4acci,k (m/s2)ui,k (m/s2)020406080100-8081624ji,k (m/s3)Time Step k10

better convergence stability. Moreover, the platoon safety and
string stability for FH-DDPG-SS have been demonstrated.

A. Pseudocode of FH-DDPG Algorithm

APPENDIX

The pseudocode of FH-DDPG algorithm [35] is given below

as Function FH − DDPG.

Function 1 FH − DDPG

1: Randomly initialize actor and critic network weights as

θµ0 and θQ0

i,K(Si,K) = µmo(Si,K) for the ﬁnal time step K

2: Set µ∗
3: for k = K − 1, · · · , 1 do
4:

θµi = θµ0 and θQi = θQ0
θµi,k , θQi,k ← DDPG − FT(θµi , θQi, θµ(cid:48)
Update the target network:

5:
6:

i, θQ(cid:48)

i, k)

θµ(cid:48)

i ← θµi,k , θQ(cid:48)

i ← θQi,k

7: end for
8: return {θµi,k , θQi,k }K

k=1

Function 2 DDPG − FT(θµi, θQi, θµ(cid:48)
1: Initialize replay buffer R
2: Initialize a random process N for action exploration
3: for episode e = 1, . . . , E do
4:

i, θQ(cid:48)

i, k)

i,k and observe reward r(e)

Receive state S(e)
i,k
Select action u(e)
i,k according to the current policy and
exploration noise
Execute action u(e)
new state S(e)
Store transition (S(e)
a
Sample
i,k , r(n)
i,k , u(n)
(S(n)
if k = K − 1 then
i,k = r(n)

i,k , r(e)
random minibatch of Nb
i,k , S(n)

i,k + γrK(S(n)

i,k+1, µmo(S(n)

i,k and observe

i,k+1) from R

i,k+1) in R

i,k , S(e)

i,k , u(e)

Set y(n)

transitions

i,k+1))

i,k+1

else

Set y(n)

i,k = r(n)

i,k + γQ(cid:48)

i(S(n)

i,k+1, µ(cid:48)

i(S(n)

i,k+1|θµ(cid:48)

i)|θQ(cid:48)
i)

end if
Update critic by minimizing the loss:

5:

6:

7:
8:

9:
10:
11:

12:
13:
14:

L =

1
N

(cid:88)

i

(y(n)

i,k − Qi(S(n)

i,k , u(n)

i,k |θQi))

θQi ← θQi + β (cid:53)θQi L

15:

Update the actor using the sampled policy gradient:

(cid:53)θµi J ≈

(cid:53)uQi(s, u|θQi)|s=S(n)

i,k ,u=µ(S(n)
i,k )

(cid:88)
(

1
N
(cid:53)θµi µi(s|θµi )|S(n)

i

i,k

)

θµi ← θµi + α (cid:53)θµi J

16: end for
17: return θµi, θQi

Fig. 4.

epi,k of the worst test episode for FH-DDPG-SS.

D. String stability

The string stability of a platoon indicates whether oscilla-
tions are ampliﬁed upstream the trafﬁc ﬂow. The platoon is
called string stable if sudden changes in the velocity of a pre-
ceding vehicle are attenuated by following vehicles upstream
the platoon [43]. To show the string stability of the proposed
FH-DDPG-SS algorithm, we simulate the platoon where the
leader acceleration is set to 2 m/s2 when 20 < k ≤ 30,
and 0 m/s2 otherwise. The followers’ initial gap-keeping and
velocity errors are all set to 0.

Fig. 5. Results of a test episode for FH-DDPG-SS in a speciﬁc setting.

As shown in Fig.5, the amplitude of the oscillations in epi,k
and evi,k of each follower i ∈ {2, 3, 4} are both smaller than
those of its respective predecessor i − 1, demonstrating the
string stability of the platoon.

VI. CONCLUSION

This paper has studied how to solve the platoon control
problem using an integrated DRL and DP method. Firstly, the
SSDP model for platoon control has been formulated with
a Huber loss function for the reward. Then, the FH-DDPG-
SS algorithm has been proposed to improve sampling and
training efﬁciency over the baseline FH-DDPG algorithm with
three key ideas. Finally, the performance of FH-DDPG-SS
has been compared with DDPG, FH-DDPG and HCFS based
on real driving data extracted from the NGSIM. The results
have shown that FH-DDPG-SS has learned better policies for
platoon control, with signiﬁcantly improved performance and

020406080100-1012 Follower 1  Follower 2 Follower 3  Follower 4epi,k (m)Time Step k020406080100-0.4-0.20.00.20.40.602040608010001 Follower 1  Follower 2 Follower 3  Follower 4epi,k (m)Time Step kevi,k (m/s)REFERENCES

[1] V. Lesch, M. Breitbach, M. Segata, C. Becker, S. Kounev, and
C. Krupitzer, “An overview on approaches for coordination of platoons,”
IEEE Transactions on Intelligent Transportation Systems, pp. 1–17,
2021.

[2] Y. Ma, Z. Li, R. Malekian, S. Zheng, and M. A. Sotelo, “A novel mul-
timode hybrid control method for cooperative driving of an automated
vehicle platoon,” IEEE Internet of Things Journal, vol. 8, no. 7, pp.
5822–5838, 2021.

[3] L. Lei, T. Liu, K. Zheng, and L. Hanzo, “Deep reinforcement learning
aided platoon control relying on v2x information,” IEEE Transactions
on Vehicular Technology, pp. 1–1, 2022.

[4] S. E. Li, Y. Zheng, K. Li, Y. Wu, J. K. Hedrick, F. Gao, and
H. Zhang, “Dynamical modeling and distributed control of connected
and automated vehicles: Challenges and opportunities,” IEEE Intelligent
Transportation Systems Magazine, vol. 9, no. 3, pp. 46–58, 2017.
[5] H. Guo, J. Liu, Q. Dai, H. Chen, Y. Wang, and W. Zhao, “A distributed
adaptive triple-step nonlinear control for a connected automated vehicle
platoon with dynamic uncertainty,” IEEE Internet of Things Journal,
vol. 7, no. 5, pp. 3861–3871, 2020.

[6] T. Yang and C. Lv, “A secure sensor fusion framework for connected
and automated vehicles under sensor attacks,” IEEE Internet of Things
Journal, pp. 1–1, 2021.

[7] J. Lan and D. Zhao, “Min-max model predictive vehicle platooning with
communication delay,” IEEE Transactions on Vehicular Technology,
vol. 69, no. 11, pp. 12 570–12 584, 2020.

[8] Y. Lin and H. L. T. Nguyen, “Adaptive neuro-fuzzy predictor-based con-
trol for cooperative adaptive cruise control system,” IEEE Transactions
on Intelligent Transportation Systems, vol. 21, no. 3, pp. 1054–1063,
2020.

[9] C. Massera Filho, M. H. Terra, and D. F. Wolf, “Safe optimization of
highway trafﬁc with robust model predictive control-based cooperative
adaptive cruise control,” IEEE Transactions on Intelligent Transporta-
tion Systems, vol. 18, no. 11, pp. 3193–3203, 2017.

[10] E. van Nunen, J. Reinders, E. Semsar-Kazerooni, and N. van de Wouw,
“String stable model predictive cooperative adaptive cruise control for
heterogeneous platoons,” IEEE Transactions on Intelligent Vehicles,
vol. 4, no. 2, pp. 186–196, 2019.

[11] Y. Zheng, S. E. Li, K. Li, F. Borrelli, and J. K. Hedrick, “Dis-
tributed model predictive control for heterogeneous vehicle platoons
under unidirectional topologies,” IEEE Transactions on Control Systems
Technology, vol. 25, no. 3, pp. 899–910, 2017.

[12] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press, 2018.

[13] B. O’Donoghue, I. Osband, R. Munos, and V. Mnih, “The uncertainty
bellman equation and exploration,” in International Conference on
Machine Learning, 2018, pp. 3836–3845.

[14] N. Heess, G. Wayne, D. Silver, T. Lillicrap, T. Erez, and Y. Tassa,
“Learning continuous control policies by stochastic value gradients,” in
Advances in Neural Information Processing Systems, 2015, pp. 2944–
2952.

[15] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of
data with neural networks,” science, vol. 313, no. 5786, pp. 504–507,
2006.

[16] L. Lei, Y. Tan, K. Zheng, S. Liu, K. Zhang, and X. Shen, “Deep
reinforcement learning for autonomous internet of things: Model, ap-
plications and challenges,” IEEE Communications Surveys Tutorials,
vol. 22, no. 3, pp. 1722–1760, 2020.

[17] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
et al., “Human-level control
learning,”
Nature Publishing Group, vol. 518, no. 7540, pp. 529–533, 2015.
[18] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning
with double Q-learning,” in Thirtieth AAAI Conference on Artiﬁcial
Intelligence, 2016.

through deep reinforcement

[19] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” arXiv preprint arXiv:1509.02971, 2015.

[20] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-
forcement learning,” in International conference on machine learning,
2016, pp. 1928–1937.

[21] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust
region policy optimization,” in International Conference on Machine
Learning, 2015, pp. 1889–1897.

11

[22] C. Desjardins and B. Chaib-draa, “Cooperative adaptive cruise control:
A reinforcement learning approach,” IEEE Transactions on Intelligent
Transportation Systems, vol. 12, no. 4, pp. 1248–1260, 2011.

[23] J. Wang, X. Xu, D. Liu, Z. Sun, and Q. Chen, “Self-learning cruise
control using kernel-based least squares policy iteration,” IEEE Trans-
actions on Control Systems Technology, vol. 22, no. 3, pp. 1078–1087,
2014.

[24] Z. Huang, X. Xu, H. He, J. Tan, and Z. Sun, “Parameterized batch
reinforcement learning for longitudinal control of autonomous land ve-
hicles,” IEEE Transactions on Systems, Man, and Cybernetics: Systems,
vol. 49, no. 4, pp. 730–741, 2019.

[25] M. Buechel and A. Knoll, “Deep reinforcement learning for predictive
longitudinal control of automated vehicles,” in Proc. 21st Int. Conf.
Intelligent Transportation Systems (ITSC), 2018, pp. 2391–2397.
[26] Z. Li, T. Chu, I. V. Kolmanovsky, and X. Yin, “Training drift coun-
teraction optimal control policies using reinforcement
learning: An
adaptive cruise control example,” IEEE Transactions on Intelligent
Transportation Systems, vol. 19, no. 9, pp. 2903–2912, 2018.

[27] S. Wei, Y. Zou, T. Zhang, X. Zhang, and W. Wang, “Design and
experimental validation of a cooperative adaptive cruise control system
based on supervised reinforcement learning,” Applied sciences, vol. 8,
no. 7, p. 1014, 2018.

[28] M. Zhu, Y. Wang, Z. Pu, J. Hu, X. Wang, and R. Ke, “Safe, efﬁcient,
and comfortable velocity control based on reinforcement learning for
autonomous driving,” Transportation Research Part C: Emerging Tech-
nologies, vol. 117, p. 102662, 2020.

[29] Y. Lin, J. McPhee, and N. L. Azad, “Longitudinal dynamic versus
kinematic models for car-following control using deep reinforcement
learning,” in 2019 IEEE Intelligent Transportation Systems Conference
(ITSC).

IEEE, 2019, pp. 1504–1510.

[30] Y. Lin, J. McPhee, and N. Azad, “Comparison of deep reinforcement
learning and model predictive control for adaptive cruise control,” IEEE
Transactions on Intelligent Vehicles, vol. 6, no. 2, pp. 221–231, 2021.
[31] Y. Zhang, L. Guo, B. Gao, T. Qu, and H. Chen, “Deterministic pro-
motion reinforcement learning applied to longitudinal velocity control
for automated vehicles,” IEEE Transactions on Vehicular Technology,
vol. 69, no. 1, pp. 338–348, 2020.

[32] G. Wang, J. Hu, Y. Huo, and Z. Zhang, “A novel vehicle platoon follow-
ing controller based on deep deterministic policy gradient algorithms,”
in CICTP 2018: Intelligence, Connectivity, and Mobility. American
Society of Civil Engineers Reston, VA, 2018, pp. 76–86.
[33] T. Chu and U. Kalabi´c, “Model-based deep reinforcement

learning
for cacc in mixed-autonomy vehicle platoon,” in 2019 IEEE 58th
Conference on Decision and Control (CDC).
IEEE, 2019, pp. 4079–
4084.

[34] R. Yan, R. Jiang, B. Jia, J. Huang, and D. Yang, “Hybrid car-following
strategy based on deep deterministic policy gradient and cooperative
adaptive cruise control,” IEEE Transactions on Automation Science and
Engineering, pp. 1–9, 2021.

[35] L. Lei, Y. Tan, G. Dahlenburg, W. Xiang, and K. Zheng, “Dynamic
energy dispatch based on deep reinforcement learning in iot-driven smart
isolated microgrids,” IEEE Internet of Things Journal, p. 1, 2020.
[36] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transac-
tions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345–
1359, 2010.

[37] P. J. Huber, Robust statistics.
[38] M. L. Puterman, Markov decision processes: discrete stochastic dynamic

John Wiley & Sons, 2004, vol. 523.

programming.

John Wiley & Sons, 2014.

[39] T. W. Vossen, F. You, and D. Zhang, “Finite-horizon approximate linear
programs for capacity allocation over a rolling horizon,” Production and
Operations Management, 2022.

[40] S. P. Boyd, “Lecture 1 linear quadratic regulator: Discrete-time ﬁnite

horizon,” https://stanford.edu/class/ee363/lectures/dlqr, 2009.

[41] M. Shah, R. Ali, and F. M. Malik, “Control of ball and beam with lqr
control scheme using ﬂatness based approach,” in 2018 International
Conference on Computing, Electronic and Electrical Engineering (ICE
Cube), 2018, pp. 1–5.

[42] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for large-
scale machine learning,” in 12th {USENIX} symposium on operating
systems design and implementation ({OSDI} 16), 2016, pp. 265–283.

[43] G. J. L. Naus, R. P. A. Vugts, J. Ploeg, M. J. G. van de Molengraft,
and M. Steinbuch, “String-stable cacc design and experimental valida-
tion: A frequency-domain approach,” IEEE Transactions on Vehicular
Technology, vol. 59, no. 9, pp. 4268–4279, 2010.

