2
2
0
2

n
u
J

9
2

]

G
L
.
s
c
[

2
v
4
4
8
1
1
.
6
0
2
2
:
v
i
X
r
a

Quant-BnB: A Scalable Branch-and-Bound Method for Optimal
Decision Trees with Continuous Features

Rahul Mazumder∗

Xiang Meng†

Haoyue Wang‡

Abstract

Decision trees are one of the most useful and popular methods in the machine learning
toolbox. In this paper, we consider the problem of learning optimal decision trees, a combi-
natorial optimization problem that is challenging to solve at scale. A common approach in
the literature is to use greedy heuristics, which may not be optimal. Recently there has been
signiﬁcant interest in learning optimal decision trees using various approaches (e.g., based on
integer programming, dynamic programming)—to achieve computational scalability, most of
these approaches focus on classiﬁcation tasks with binary features. In this paper, we present
a new discrete optimization method based on branch-and-bound (BnB) to obtain optimal
decision trees. Diﬀerent from existing customized approaches, we consider both regression and
classiﬁcation tasks with continuous features. The basic idea underlying our approach is to
split the search space based on the quantiles of the feature distribution—leading to upper and
lower bounds for the underlying optimization problem along the BnB iterations. Our proposed
algorithm Quant-BnB shows signiﬁcant speedups compared to existing approaches for shallow
optimal trees on various real datasets.

1 Introduction

A decision tree is a classic machine learning predictive tool with a ﬂowchart-like structure
that allows users to derive interpretable decisions. Combined with its eﬀectiveness in solving
classiﬁcation and regression tasks, it is an immensely useful tool in domains where interpretability
is of great importance. Despite its appeal, the task of learning an optimal decision tree (with
smallest training error) is N P-hard [20] and thus computationally challenging. Therefore, greedy
methods such as CART [9] and ID3 [26] are popular choices. They construct decision trees
through a top-down approach. Starting from the root node, data is iteratively split into subsets
according to local objectives. In spite of high eﬃciency, greedy heuristics may not lead to an
optimal solution, possibly resulting in suboptimal predictive performance [6].

In recent years, there have been major advances in exploring optimization methods to construct
optimal 1 decision trees. [27, 17] explore mixed integer programming (MIP) approaches to learn
optimal trees with a ﬁxed depth. [3, 13] propose interesting dynamic programming (DP) approaches

∗MIT Sloan School of Management, Operations Research Center and MIT Center for Statistics (email: rahul-

maz@mit.edu).

†MIT Operations Research Center (email: mengx@mit.edu)
‡MIT Operations Research Center (email: haoyuew@mit.edu).
1In this paper, optimal refers to a global optimal solution to the optimization problem associated with learning a

decision tree.

1

 
 
 
 
 
 
for optimal decision trees. Despite impressive methodological advances, optimal tree-learning
approaches face the following challenges: (i) MIP formulations appear to have limited scalability.
[27] report that a MIP solver cannot solve an optimal tree of depth 2 with less than 1000
observations and 10 features in 10 minutes. (ii) Most state-of-the-art algorithms [13, 2, 22]
consider datasets with binary features (i.e, every feature is {0, 1}) rather than continuous ones.
Algorithms for optimal trees with continuous features are much less developed2—our goal in this
paper is to bridge this gap in the literature.

In this work we take a step towards addressing the aforementioned shortcomings by developing
a novel branch-and-bound (BnB) algorithm for the computation of shallow optimal trees (e.g.
depth=2, 3).
In contrast to earlier approaches, our algorithm can handle both classiﬁcation
and regression tasks and is designed to directly handle continuous features (including a mix of
continuous and binary features). In a nutshell, our proposed algorithm Quant-BnB utilizes the
quantiles of the feature distribution to decompose the search space into sub-regions—these are
subsequently used to generate lower bounds and upper bounds on the optimal value, and prune
sub-optimal regions of the search-space. To our knowledge, Quant-BnB is the ﬁrst standalone
method (i.e, does not rely on proprietary optimization solvers) for optimal classiﬁcation/regression
trees that directly applies to datasets with continuous features. We show that Quant-BnB computes
the optimal solution (cf. Section 4.3), and achieves signiﬁcant empirical improvements compared to
existing methods (cf Section 6). A Julia implementation of our code is open-sourced on GitHub3.

Paper
Carreira-Perpin´an and Tavallali [10]
Bertsimas and Dunn [7]
Verwer and Zhang [27]
Aglin et al. [3]
Demirovi´c et al. [13]
Lin et al. [21]
Aghaei et al. [2]
Our approach

Opt Feat Task
(cid:51)
R/C
(cid:51)
R/C
(cid:51)
C
(cid:55)
C
(cid:55)
C
(cid:55)
C
(cid:55)
R/C
(cid:51)
R/C

(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

Model
TAO
MIP
MIP
DP&BnB
DP
DP&BnB
MIP
BnB

Table 1: Related works on decision tree optimization. Opt indicates if the approach ﬁnds an optimal
tree. Feat indicates whether the method works for continuous features (without binarizing features). Task
indicates what tasks the method can handle: R for regression and C for classiﬁcation. TAO is an alternating
optimization-based heuristic. The optimal methods are based on BnB, MIP (optimization solvers) and DP.

Related Work: Various optimization techniques have been explored to learn high-quality
decision trees [5, 14, 24, 16, 25, 10]. A number of recent works explore MIP-approaches for optimal
decision trees. For example, [7] formulate learning optimal trees with a ﬁxed depth as a MIP
model. [17] design an improved model with much fewer binary decision variables for classiﬁcation
problems with binary features. [27] propose a model that works for numerical features with the
same order of binary variables as in [17]. [28] present a MIP approach for optimal decision trees
with hyperplane splits (aka oblique trees). Other MIP-based approaches have been proposed
in [1, 2]. In addition to the MIP approach, SAT solvers have been explored to learn optimal

2While it is possible to convert continuous features to binary features, this may result in a large number of
features, which leads to large computation times—see our experiments for details. To achieve scalability, it may be
more beneﬁcial to have tailored approaches for continuous features.

3https://github.com/mengxianglgal/Quant-BnB

2

decision trees [8, 23, 18].

Another line of work explores pruning techniques to improve the eﬃciency of DP-based
approaches.
[4, 11, 19] solve the optimal sparse decision tree using analytical bounds on the
optimal solution together with a customized bit-vector library. [21] improve the eﬃciency of earlier
approaches by using DP methods.
[3] utilize DP to compute better dual bounds during BnB
search. [13] show useful computational gains by using pre-computed information from sub-trees
and hash functions. [22] design smart guessing strategies to improve the performance of BnB-based
approaches.

As mentioned earlier, current approaches are unable to compute optimal classiﬁcation/regression
trees with continuous features at scale—a problem we address in this paper. Table 1 presents a
summary of the key characteristics of related existing approaches vis-a-vis our proposed method
Quant-BnB.

2 Preliminaries and Notations

2.1 Overview of optimal decision trees

Consider a supervised learning problem with n observations {(xi, yi)}i∈[n], each with p features
xi ∈ X ⊆ Rp and response yi ∈ Y. A decision tree recursively partitions the feature space X into
a number of hierarchical, disjoint regions, and makes a prediction for each region. In this paper,
we focus on binary decision trees (i.e., every non-terminal node splits into left and right children)
with axis-aligned splits. See [9] for further details.

For a decision tree T and feature vector x, let T (x) ∈ Y denote the corresponding prediction.
Given a loss function (cid:96)(·, ·) on Y × Y, and a family T of decision trees, an optimal decision tree is
a global optimal solution to the following optimization problem:

minT ∈T

(cid:96)(yi, T (xi)).

(cid:88)n

(1)

i=1
For regression problems with a scalar output, we can take Y ⊂ R and (cid:96) as the squared loss:
(cid:96)SE(y, ˆy) := (y − ˆy)2. We also consider extensions to multivariate continuous outcomes with
y ∈ Y ⊂ Rm, m ≥ 1 and (cid:96)SE(y, ˆy) := (cid:107)y − ˆy(cid:107)2. For classiﬁcation problems with C classes, one can
take Y = [C] := {1, 2, ..., C}, and (cid:96) to be the 0-1 (or mis-classiﬁcation) loss i.e., (cid:96)01(y, ˆy) = 1(y (cid:54)= ˆy).

2.2 Data types for the feature space

In this paper, we consider the general case where xi contains continuous and possibly binary
features. If a feature f is continuous, then {xi,f }n
i=1 can contain at most n diﬀerent values. Our
approach also applies to the setting where the number of distinct values of f is much smaller
than n. Recall that if f is a binary feature then xi,f ∈ {0, 1}, i ∈ [n]. As mentioned earlier,
when the features are all binary, the optimal decision tree can be computed quite eﬃciently as
shown in recent works (cf Section 1). To gather some intuition, note that, if all features are
binary, computing an optimal regression tree of depth d costs O(npd) operations by exhaustive
search—this can be acceptable even when n is large (e.g. n ≈ 104 ∼ 105) but d is small (e.g.
d = 2, 3). In contrast, if all features are drawn from a continuous distribution, then an exhaustive
search would cost O(ndpd) (almost surely)—this may be computationally intractable for the same
values of n, d. In this paper, our focus is to propose an eﬃcient BnB framework for the challenging

3

case when the features are continuous—a topic that seems to be less studied when compared to
the case where all features are binary.

2.3 Notations

t + wf

1 < wf

t := (wf

i=1—i.e., the

0 = −∞ and wf

For a feature f ∈ [p], let u(f ) ∈ [n] be the number of distinct elements in {xi,f }n
number of unique values assumed by feature f in the training data. We let wf
u(f )
denote these distinct values. In addition, we set wf
u(f )+1 = +∞. For any integer t
with 0 ≤ t ≤ u(f ), let ˜wf
t+1)/2. For Problem (1), it suﬃces to consider candidate
trees with all splitting thresholds located in the set { ˜wf
t }f ∈[p],0≤t≤u(f ) (Note that diﬀerent splitting
thresholds in the interval (wf
t+1) give the same routing decision on the training set, so we
choose the mid-point ˜wf
t as a candidate threshold). As we consider axis-aligned splits, each
splitting node of a tree can be described by a tuple (f, t), where f ∈ [p] is the splitting feature, and
0 ≤ t ≤ u(f ) is an integer indicating that the splitting threshold is ˜wt. We say (f, t) is the splitting
rule of this node. For a set I ⊆ [n], a feature f ∈ [p] and two integers a, b with 0 ≤ a ≤ b ≤ u(f ),
deﬁne

2 < · · · < wf

t , wf

a ≤ xi,f ≤ ˜wf
b }
a and ˜wf
to be the set of sample indices i for which xi,f lies between ˜wf
b .

:= {i ∈ I | ˜wf

I f
[a,b]

For A, B > 0, we use the notations A = O(B) or A (cid:46) B to denote that there exists a universal
constant C such that A ≤ CB; and use the notation A = (cid:101)O(B) if A = O(B log(n)), where n is
the number of samples.

2.4 Preliminaries for decision tree with depth 2

The algorithms we present in this paper are capable of solving shallow (e.g., d = 2 or 3)
optimal trees within practical runtimes. We present an in-depth description of our proposed
approach Quant-BnB for the case d = 2. Section 5 presents a sketch of how it can be extended to
deeper trees.

An optimal tree of depth 2 is a solution to the following problem

min
T ∈T2

n
(cid:88)

i=1

(cid:96)(yi, T (xi)),

(2)

where, T2 is the set of all decision trees with depth 2 whose splitting thresholds are in { ˜wf

t }f ∈[p],0≤t≤u(f ).

For a tree T ∈ T2, let (fO(T ), tO(T )), (fL(T ), tL(T )) and (fR(T ), tR(T )) denote the splitting
rules at the root node (O), the left child (L) and right child (R) of the root node respectively.
Given f0, f1, f2 ∈ [p] and two integers a, b with 0 ≤ a ≤ b ≤ u(f0), we deﬁne
(cid:12)
(cid:12)
(cid:12)fO(T ) = f0, tO(T ) ∈ [a, b], fL(T ) = f1, fR(T ) = f2

T2(f0, [a, b], f1, f2) :=

T ∈ T2

(cid:110)

(cid:111)

to be the set of trees in T2 whose root node, left child and right child splitting features are f0, f1
and f2 respectively, and the splitting threshold of the root node is between ˜wa and ˜wb. Given
F1, F2 ⊆ [p], deﬁne

T2(f0, [a, b], F1, F2) :=

T2(f0, [a, b], f1, f2).

(cid:91)

f1∈F1,f2∈F2

4

We adopt the convention that T2(f0, [a, b], F1, F2) = ∅ if F1 or F2 is empty. Note that here a and
b are integers indicating that the splitting threshold of f0 lies in { ˜wf0
b }. See the
appendix for an illustrative example.

a+1, , ...., ˜wf0

a , ˜wf0

For a given subset of samples I ⊆ [n], deﬁne

L0(I) := miny∈Y

(cid:88)

i∈I

(cid:96)(yi, y)

(3)

to be the loss of the best constant ﬁt to {yi}i∈I. As concrete examples, for regression problem
with Y = Rm and (cid:96) = (cid:96)SE, it holds

L0(I) =

(cid:88)

i∈I

(cid:13)
(cid:13)yi − (1/|I|)

(cid:88)

(cid:13)
2.
(cid:13)

yj

j∈I

For classiﬁcation problem with Y = [C] and loss (cid:96) = (cid:96)01, it holds

L0(I) = minc∈[C]

(cid:88)

i∈I

1{yi(cid:54)=c}.

(4)

(5)

Note that L0(I) can be viewed as the minimum loss of a “depth-0” decision tree with
observations in I. Similarly, we deﬁne a function L1 to be the best possible objective value for a
depth-1 decision tree (a “stump”) when using observations in I and a feature f ∈ [p]:

L1(I, f ) := min

0≤t≤u(f )

(cid:8)L0(I f

[0,t]) + L0(I f

[t,u(f )])(cid:9).

(6)

Note that for a given feature f and a set of indices I, if L0 is given by (4) or (5), then L1(I, f ) can
be computed within O(|I| log |I|) ≤ (cid:101)O(|I|) operations (the log-factor is from a sorting operation).
Let us consider f0, f1, f2 ∈ [p], integers a, b with 0 ≤ a ≤ b ≤ u(f0) and I ⊆ [n]. Extending
the deﬁnitions of L0 and L1 above, we now consider the best objective for a depth-2 decision tree.
Deﬁne

L2(I, f0, [a, b], f1, f2) := min
a≤t≤b

(cid:8)L1(I f0

[0,t], f1) + L1(I f0

[t,u(f0)], f2)(cid:9)

to be the minimum value of (cid:80)

i∈I (cid:96)(yi, T (xi)) over all depth-2 trees T ∈ T2(f0, [a, b], f1, f2).

To simplify the notation above, deﬁne the parameter space

(cid:110)

Φ :=

(f0, [a, b], f1, f2) | f0, f1, f2 ∈ [p],

a, b are integers with 0 ≤ a ≤ b ≤ u(f0)

(cid:111)
.

Then for any φ = (f0, [a, b], f1, f2) ∈ Φ, and I ⊆ [n], we use the short-hand notations

T2(φ) = T2(f0, [a, b], f1, f2),
L2(I, φ) = L2(I, f0, [a, b], f1, f2).

(7)

(8)

3 Upper and Lower Bounds for L2(I, φ)

For any φ ∈ Φ and I ⊆ [n], recall that L2(I, φ) is the best objective value of (2) across trees in
T2(φ) with samples in I. In this section, we discuss upper and lower bounds for L2(I, φ)—the
costs for computing these bounds are lower than directly computing L2(I, φ). Quant-BnB critically
makes use of these upper and lower bounds while performing BnB (cf Section 4).

5

3.1 Upper bounds for L2(I, φ)

Given φ = (f0, [a, b], f1, f2) ∈ Φ and I ⊆ [n], we compute an upper bound of L2(I, φ) by making
use of the quantiles of the features. For an integer s with 1 ≤ s ≤ b − a, we say that a set
of integers (t0, t1, . . . , ts) are almost s-equi-spaced in the interval [a, b] if t0 = a, ts = b and
tj = (cid:98)a + (j/s)(b − a)(cid:99) for 1 ≤ j ≤ s − 1. Given such a sequence (t0, t1, . . . , ts), we deﬁne

Vs(I, φ) := min
0≤j≤s

(cid:110)
L1(I f0

[0,tj ], f1) + L1(I f0

[tj ,u(f0)], f2)

(cid:111)
.

It follows that Vs(I, φ) ≥ L2(I, φ) for all φ = (f0, [a, b], f1, f2) ∈ Φ satisfying b − a ≥ s; and
hence Vs is an upper bound to L2. We note that quantile-based methods are commonly used as a
heuristic in decision tree algorithms (e.g., XGBoost [12]). Our work diﬀers—as discussed below,
we make use of this quantile-based approach to obtain an optimal decision tree.

3.2 Lower bounds for L2(I, φ)

We present some lower bounds for L2(I, φ). The lower bounds along with the upper bounds
discussed earlier, form the backbone of our BnB procedure.
Lower bound 1: The ﬁrst lower bound we consider is obtained by sorting the values of a feature,
and dropping the values lying in an interior sub-interval. Given any φ = (f0, [a, b], f1, f2) ∈ Φ and
I ⊆ [n], deﬁne

W0(I, φ) = W0(I, f0, [a, b], f1, f2) := L1(I f0

[0,a], f1) + L1(I f0

[b,u(f0)], f2).

(9)

[0,a], f1); the samples in I f0

We can interpret W0(I, φ) as follows: the samples in I f0
[0,a] are routed to the left subtree yielding a
loss L1(I f0
[b,u(f0)] are routed to the right subtree with a loss L1(I f0
[b,u(f0)], f2);
and the samples in I f0
[a,b] are “dropped” (i.e., do not enter any leaf node). Lemma 3.2 shows that
W0 is a lower bound for L2 i.e., W0(I, φ) ≤ L2(I, φ) for all φ ∈ Φ. Furthermore, W0(I, φ) is easier
to compute than L2(I, φ). Figure 1 presents a schematic diagram illustrating computation of the
lower bound W0.

Arranged in ascending order of feature f0

˜wf0
0

˜wf0
a

˜wf0
b

˜wf0

u(f0)

Figure 1: Figure showing sorting of the unique values in feature f0 into intervals [ ˜wf0
[ ˜wf0
a and ˜wf0
b
part in the ﬁgure) are dropped.

u(f0)]. In the deﬁnition of W0, samples with xi,f0 between ˜wf0

b ] and [ ˜wf0

b , ˜wf0

a , ˜wf0

0 , ˜wf0
a ],
(green

Lower bound 2: The second lower bound we consider is obtained by using a subset of samples
in the middle to ﬁt another depth-2 tree. Given any φ = (f0, [a, b], f1, f2) ∈ Φ and I ⊆ [n], deﬁne

W2(I, φ) = W2(I, f0, [a, b], f1, f2) := W0(I, φ) + L2(I f0

[a,b], φ).

(10)

Note that in the deﬁnition of W2, the samples in I f0
T2(φ), and yield an additional loss term L2(I f0

[a,b] are used to ﬁt another depth-2 tree in
[a,b], φ). It can be proved that W2(I, φ) ≤ L2(I, φ)

6

for all φ ∈ Φ (see Lemma 3.2). Note that W2(I, φ) is a better lower bound than W0(I, φ), but
has a higher computational cost—see Section 4.3 for details.
Lower bound 3: The third lower bound we introduce below combines the above ideas underlying
the computation of W0 and W2. To introduce this lower bound, we need some additional notations.
Given a φ = (f0, [a, b], f1, f2) ∈ Φ; and almost s(cid:48)-equi-spaced integers (t0, t1, . . . , t(cid:48)
s) in [a, b] (for
an integer s(cid:48) ≤ b − a), deﬁne

(cid:98)L2(I, φ, s(cid:48)) = (cid:98)L2(I, f0, [a, b], f1, f2, s(cid:48)) := min
1≤j≤s(cid:48)

(cid:8)L1(I f0

[0,tj−1], f1) + L1(I f0

[tj ,u(f0)], f2)(cid:9).

(11)

Note that in the j-th term of the minimum in (11), we drop the observations in I f0
following lemma shows that (cid:98)L2(·, ·, s(cid:48)) is a lower bound of L2(·, ·).

[tj−1,tj ]. The

Lemma 3.1. For any I ⊆ [n], any φ = (f0, [a, b], f1, f2) ∈ Φ, and any s ≤ b − a, it holds

(cid:98)L2(I, φ, s(cid:48)) ≤ L2(I, φ).

(12)

The proof of Lemma 3.1 is presented in the appendix. Note that the computation of (cid:98)L2(I, φ, s(cid:48))

is easier than the computation of L2(I, φ).

Using (cid:98)L2, we can introduce the third lower bound for L2(I, φ). Given any φ = (f0, [a, b], f1, f2) ∈

Φ and any integer s(cid:48) ≤ b − a, deﬁne

W1,s(cid:48)(I, φ) = W1,s(cid:48)(I, f0, [a, b], f1, f2) := W0(I, φ) + (cid:98)L2(I f0

[a,b], φ, s(cid:48)).

(13)

Note that in the deﬁnition above, the samples in I f0
[a,b], φ, s(cid:48)).
As a result, the value W1,s(cid:48)(I, φ) is larger than W0(I, φ) and smaller than W2(I, φ) (due to Lemma
3.1).

[a,b] are used to compute a term (cid:98)L2(I f0

The following lemma justiﬁes that W0, W1,s(cid:48) and W2 are indeed lower bounds of L2.

Lemma 3.2. For any I ⊆ [n], φ ∈ Φ and s(cid:48) ≤ b − a, it holds

W0(I, φ) ≤ W1,s(cid:48)(I, φ) ≤ W2(I, φ) ≤ L2(I, φ).

The proof of Lemma 3.2 is presented in the appendix. Lemma 3.2 shows that W0, W1,s(cid:48) and
W2 are lower bounds for L2; W0 is weakest and W2 is the tightest of the three bounds. See Section
4.3 for further discussions on the computational costs of these three lower bounds.

4 A Branch and Bound Method for Optimal Trees with Depth

2

In this section, we describe our algorithm Quant-BnB to solve problem (2) to optimality. Quant-BnB main-
tains and updates a search space represented as disjoint unions of sets of the form T2(f0, [a, b], F1, F2).
In Section 4.1, we ﬁrst present a proposition illustrating how we perform quantile-based pruning
on a set of the form T2(f0, [a, b], F1, F2).
In Section 4.2 we present the overall framework of
Quant-BnB. In Section 4.3 we discuss the computational cost of the algorithm.

7

4.1 A quantile-based pruning procedure

Suppose we are given f0 ∈ [p], integers a, b with 0 ≤ a ≤ b ≤ u(f0) and F1, F2 ⊆ [p]. We focus
on the subset of trees T2(f0, [a, b], F1, F2), and discuss a quantile-based method to replace this
collection by a smaller subset containing an optimal solution to (2).

Let (t0, ..., ts) be almost s-equi-spaced in [a, b]. The following proposition states a basic strategy

for pruning.

Proposition 4.1. Let W be a function on 2[n] × Φ with W (I, φ) ≤ L2(I, φ) for all φ ∈ Φ and
I ⊆ [n]; let U be an upper bound of the optimal value of problem (2). For each j ∈ [s], deﬁne

F1,j := (cid:8)f1 ∈ F1
F2,j := (cid:8)f2 ∈ F2

(cid:12)
(cid:12) min
f2∈F2
(cid:12)
(cid:12) min
f1∈F1

W ([n], φj

f1,f2

) ≤ U (cid:9),

W ([n], φj

f1,f2

) ≤ U (cid:9),

where φj

f1,f2

:= (f0, [tj−1, tj], f1, f2). Then any optimal solution of (2) is not in

T2(f0, [a, b], F1, F2) \

s
∪
j=1

T2(f0, [tj−1, tj], F1,j, F2,j).

(14)

(15)

(16)

The proof of Proposition 4.1 is presented in the appendix. Possible choices of the lower bound
W (I, φ) in Proposition 4.1 are W0(I, φ), W1,s(cid:48)(I, φ) and W2(I, φ), as designed in Section 3.2. If
the assumptions of Proposition 4.1 hold, we can replace the search space T2(f0, [a, b], F1, F2) by
a smaller space ∪s
j=1T2(f0, [tj−1, tj], F1,j, F2,j), or equivalently, prune all the feasible solutions in
(16). Note that it is possible that for some j ∈ [s], the set F1,j or F2,j is empty, and hence the
set T2(f0, [tj−1, tj], F1,j, F2,j) is also empty. In that case, we prune the trees in T2(f0, [a, b], F1, F2)
with splitting thresholds lying in [tj−1, tj].

4.2 Quant-BnB framework

We discuss the overall methodology of Quant-BnB to solve (2). The proposed algorithm maintains
and updates a set AL(k) (short for “alive”) over iterations k ≥ 0. AL(k) contains tuples of the form

(f0, [a, b], F1, F2),

where f0 ∈ [p]; a and b are integers with 0 ≤ a ≤ b ≤ u(f0); and F1, F2 ⊆ [p]. Initially (i.e., at
k = 0), all the trees in T2 are “alive”, so we set

AL(0) = ∪p

f0=1 {(f0, [0, u(f0)], [p], [p])} .

(17)

Intuitively speaking, as the iterations progress, we reduce the size of AL(k), by removing tuples
(f0, [a, b], F1, F2) that do not contain optimal solutions to (2). The algorithm also maintains and
updates the best objective value that it has found so far, denoted by U . Initially, we set U to be
the value at any feasible solution of (2).

At every iteration k ≥ 1, to update AL(k) from AL(k−1), we ﬁrst set AL(k) = ∅. The algorithm
then checks all elements in AL(k−1). For an element (f0, [a, b], F1, F2) in AL(k−1), if b − a is less than
a pre-speciﬁed integer s (i.e., the number of trees in the space is suﬃciently small), our algorithm
performs an exhaustive search to examine all candidate trees in the space T2(f0, [a, b], F1, F2).
Otherwise, the algorithm conducts the following 2 steps. Let (t0, ..., ts) be an almost s-equi-spaced
sequence in [a, b].

8

• (Step1: Update upper bound) Compute

U (cid:48) =

min
f1∈F1,f2∈F2

(cid:8)Vs([n], f0, [a, b], f1, f2)(cid:9).

If U (cid:48) < U , update U ← U (cid:48), and update the corresponding best tree.

• (Step2: Compute lower bound and prune) For a function W on 2[n] × Φ satisfying W (I, φ) ≤
L2(I, φ) for all φ ∈ Φ and I ⊆ [n], compute sets F1,j and F2,j as in (14) and (15) (for all
j ∈ [s]), and update

AL(k) = AL(k) (cid:91) (cid:16)

∪s

j=1 {(f0, [tj−1, tj], F1,j, F2,j)}

(cid:17)

.

Note that in Step 2 above, we need to compute values of W ([n], φj
) as in (14) and (15)–these
are lower bounds of L2([n], φj
). Function W can be taken as W0, W1,s(cid:48) or W2, as introduced
j=1{(f0, [tj−1, tj], F1,j, F2,j)} is added into AL(k);
in Section 3.2. At the end of Step 2, the set ∪s
this set replaces the tuple (f0, [a, b], F1, F2) in AL(k−1). In other words, all the trees that lie in
T2(f0, [a, b], F1, F2) but not in ∪s

j=1T2(f0, [tj−1, tj], F1,j, F2,j) are pruned.

f1,f2

f1,f2

The main steps of the algorithm above are summarized in Algorithm 1. For illustration, a

single iteration of Quant-BnB on a toy example is provided in appendix.

Algorithm 1 Quant-BnB for depth-2 decision trees

Input: data {(xi, yi)}n

i=1, an integer s ≥ 2, an initial upper bound U and a feasible solution ˆT .

Initialize AL(0) as in (17), and set k = 1.
repeat

Set AL(k) = ∅.
for each (f0, [a, b], F1, F2) in AL(k−1) do

if b − a ≤ s then

Use exhaustive search to get the bound
U (cid:48) = minf1∈F1,f2∈F2 L2([n], f0, [a, b], f1, f2).
Update U = min{U, U (cid:48)} and accordingly, the current best solution ˆT .
continue

end if
Let (t0, t1, . . . , ts) be almost s-equi-spaced in [a, b].
Perform Steps 1 and 2.

end for
Update k ← k + 1.
until AL(k) is empty
Output: The optimal decision tree ˆT and corresponding objective value U .

4.3 Correctness of Quant-BnB, computational cost

Theorem 4.2 (see Appendix for proof) establishes that Quant-BnB converges to the global optimum
of (2).

9

Theorem 4.2. Algorithm 1 terminates in at most (cid:100)logs(n)(cid:101) iterations and yields an optimal
solution of (2).

Computational cost: We discuss the computational cost of Steps 1 & 2 for a given (f0, [a, b], F1, F2).
To simplify the discussion, we consider the case when the number of unique values of feature f0
i.e., u(f0) is n.

Note that in Algorithm 1, a function W satisfying W (I, φ) ≤ L2(I, φ) for all φ ∈ Φ and
I ⊆ [n] is needed. Candidates of the lower bound W have been discussed in Section 3.2. Diﬀerent
choices of W have diﬀerent computational costs. The simplest choice is to set W = L2 directly,
in which case Step 2 of the algorithm reduces to an exhaustive search over T2(f0, [a, b], F1, F2),
which is expensive. By Lemma 3.2, it is also possible to take W to be W0, W1,s(cid:48) (for some proper
integer s(cid:48)) or W2. We compare the computational cost of Step 1 – Step 2 under these diﬀerent
choices, as shown below.

Lemma 4.3. Suppose L0 is given by (4) or (5). For a given (f0, [a, b], F1, F2), denote ˜p :=
|F1| + |F2|. Suppose u(f0) = n. Let s, s(cid:48) be positive integers with s(cid:48) · s ≤ b − a. The computational
cost of Steps 1-2 for diﬀerent choices of the lower-bound function W are as follows:
(1) If W = W0, the cost is bounded by (cid:101)O(n˜ps).
(2) If W = W1,s(cid:48), the cost is bounded by (cid:101)O(n˜ps + ˜ps(cid:48)(b − a)).
(3) If W = W2, the cost is bounded by (cid:101)O(n˜ps + (b−a)2 ˜p
s
(4) If W = L2, the cost is bounded by (cid:101)O(n˜p(b − a)).

).

Note that the assumption s · s(cid:48) ≤ b − a is necessary to make sure the equi-spaced sequences are
well-deﬁned. By this assumption, we have n˜ps ≤ n˜ps + ˜ps(cid:48)(b − a) ≤ n˜ps + (b − a)2 ˜p/s (cid:46) n˜p(b − a).
Lemma 4.3 implies that

W0 ≺ W1,s(cid:48) ≺ W2 ≺ L2,
where the notation “ ¯W ≺ ˜W ” means that the cost (of Steps 1 & 2) in using W = ¯W is bounded
by a constant multiple of the cost of using W = ˜W . On the other hand, by Lemma 3.2, it is
known that

(18)

W0(I, φ) ≤ W1,s(cid:48)(I, φ) ≤ W2(I, φ) ≤ L2(I, φ)

(19)

for all φ ∈ Φ. Therefore, among these four choices of W , there is a tradeoﬀ: the choice with lower
computational time for Step 1 – Step 2 will produce a less tight lower bound and may result in a
case where fewer trees are pruned in this iteration. Empirically, we ﬁnd that among these four
choices, choosing W = W1,s(cid:48) (for a proper s(cid:48)) has the best performance in most cases (see Section
6.1 for a comparison of these choices). A choice of s(cid:48) that appears to work well in practice is
s(cid:48) ≈ ns
b−a , in which case the cost in Lemma 4.3 (2) reduces to (cid:101)O(n˜ps). With this choice of s(cid:48), the
cost of Step 1–2 using W = W1,s(cid:48) is the same (up to a constant multiple) as the cost of choosing
W = W0, but the former always provides a better lower bound.

We note that when choosing W = W2 or W = L2, the cost of Algorithm 1 is not linear in n.
Indeed, initially (k = 0), for any (f0, [a, b], F1, F2) in AL(0), it holds b − a = n, and ˜p = 2p. So
by Lemma 4.3 (4) the cost of Steps 1–2 with W = L2 is (cid:101)O(n2 ˜p); and with W = W2 the cost is
(cid:101)O(nps + (n2p/s)) ≥ (cid:101)O(n3/2p).

10

5 Extension to Deeper Trees

Algorithm 1 can be generalized for the computation of optimal trees with a ﬁxed depth d
(d ≥ 3). We brieﬂy discuss the case when d = 3. To compute an optimal tree of depth 3,
Quant-BnB maintains and updates a set AL3 that contains tuples of the form

(f0, [a, b], Φ1, Φ2),

(20)

where f0 ∈ [p], 0 ≤ a ≤ b ≤ u(f0), and Φ1, Φ2 ⊆ Φ. Recall that Φ is deﬁned in (8), which contains
tuples corresponding to subsets of depth-2 trees. Each tuple (20) corresponds to a search space in
which the elements meet the following conditions: the root node (f0, t0) satisﬁes t0 ∈ [a, b]; also,
the left and right branch of the root node, which are decision trees of depth 2, are in T2(φ1) and
T2(φ2) for some φ1 ∈ Φ1 and φ2 ∈ Φ2 respectively.

To shrink the search space corresponding to (20), we set up an almost equi-spaced sequence of
integers and work with lower bounds for each smaller search space. Due space limits, we present
the details of the algorithm and related discussions in Section C.

For the case d ≥ 4, similar recursion can be applied to design BnB algorithms, but the
computational cost increases especially when p (# of features) is large. Therefore, we recommend
using our procedure for ﬁtting optimal decision trees with d ≤ 3.

So far, we only consider perfect binary trees (i.e., depth-d trees that has exactly 2d − 1 branch
nodes). In practice, it is preferable to optimize over non-perfect trees to enhance generalization
capability, especially for deeper trees. Note that we can modify Quant-BnB to handle non-perfect
trees by considering two cases for each node–it can be a branch node or a leaf–when calculating
upper and lower bounds. This modiﬁcation will not result in additional computation costs.

6 Numerical Experiments

In this section, we study the performance of Quant-BnB in terms of runtime and prediction
accuracy. In particular we study: (i) diﬀerences in the eﬃciency of Quant-BnB using various
methods to calculate the lower bound of L2(I, φ) (ii) computational cost of optimal trees (depths
2 and 3) on classiﬁcation tasks compared to state-of-the-art algorithms (iii) out-of-sample accuracy
compared to heuristic methods. We present details of experimental setup and results on regression
tasks in appendix Section D.

Datasets and Computing Environment: We collect 16 classiﬁcation (binary and multi-
class) and 11 regression datasets from UCI Machine Learning Repository [15]. All experiments
are carried out on MIT’s Engaging cluster on Intel Xeon 2.30GHz machine, with a single CPU
core and 25GB of RAM. Our algorithm implementation can be found on GitHub4.

6.1 Comparison of diﬀerent lower bounds

Recall that Quant-BnB requires a lower-bound function W such that W (I, φ) ≤ L2(I, φ) for all
φ ∈ Φ and I ⊆ [n]. In addition, the eﬃciency of the algorithm depends largely on the choice of
W . We have developed 4 possible lower bounds—W0, W1,s(cid:48), W2 and L2, and theoretically studied
the quality and computational cost of them (see (18) and (19)). In this experiment, we ﬁgure out
their practical performance.

4https://github.com/mengxianglgal/Quant-BnB

11

We set the parameter s in Algorithm 1 to be 3, and the parameter s(cid:48) is dynamically chosen as
(cid:98) 0.6ns
b−a (cid:99) for tuple φ = (f0, [a, b], f1, f2). As discussed in Section 4.3, the computational cost of W0
and W1,s(cid:48) is linear w.r.t n under such setting, while computing W2 and L2 in the ﬁrst iteration of
Quant-BnB costs O(n2).

We implement Quant-BnB with the lower-bound function W in Step 1-2 chosen as W0, W1,s(cid:48), W2
and L2, respectively. Table 2 displays the computation time of these four choices on UCI datasets
with number of data points n ≥ 104. Although W2 produces a tighter lower bound compared
to W0 and W1,s(cid:48) (which helps Quant-BnB prune more trees in every iteration), taking W = W2
still has a bad performance due to its expensive computational cost. In contrast, proposed lower
bounds W0 and W1,s(cid:48) result in signiﬁcant speedups compared to computing L2 directly. In the
following experiments, we always choose W = W1,s(cid:48) to reduce computational cost.

Name
avila
bean
eeg
htru
magic
skin
casp
energy
gas
news
query2

(n,p)
(10432,10)
(10888,16)
(11984,14)
(14318,8)
(15216,10)
(196045,3)
(36584,9)
(15788,28)
(29386,10)
(31715,59)
(159874,4)

W0 W1,s(cid:48) W2
-
4.5
5.1
-
3.4
3.0
-
2.9
2.9
244
1.3
1.4
-
1.0
1.2
-
2.1
2.0
-
4.2
6.7
-
14
18
-
1.5
1.5
-
349
301
-
9.8
15

L2
519
-
47
515
-
31
-
-
444
-
-

Table 2: Quant-BnB with four diﬀerent methods for lower bound computation on depth-2 trees. For each
dataset, the number of observations and the number of features are provided. Each entry denotes running
time in seconds. Symbol ‘-’ refers to time out (10min).

6.2 Comparison with state-of-the-art optimal methods

We compare our algorithm with the recently proposed methods for solving optimal classiﬁcation
trees: BinOCT [27], MurTree [13] and DL8.5 [3]. We also tested other competing algorithms, but
they all took substantially longer time to deliver optimal trees—see Appendix for details. Since
MurTree and DL8.5 apply to datasets with binary features, we adopt the equivalent-conversion
pre-processing used in [21] by encoding each continuous feature f to a set of u(f ) − 1 binary
features, using all possible thresholds.

The computation time for learning optimal shallow trees (depth = 2, 3) on classiﬁcation
tasks is presented in Table 3. Quant-BnB can solve depth-2 trees in a few seconds–a speedup
of several orders of magnitude compared to other methods. When the depth is 3, Quant-BnB
still outperforms competing algorithms by a large margin on 13 of 16 datasets. Although being
highly eﬀective on datasets with purely binary features, MurTree and DL8.5 can be expensive to
deliver optimal trees on datasets with continuous features. This is perhaps due to the increase
in number of features while converting the continuous features to binary features. On a related
note, it is worth pointing out that one may use approximate methods to convert continuous to

12

binary features [13]—however, such approximate schemes may result in a lossy compression of
the training data as shown in our experiments in the Appendix. Note also that we do not use
any compression of features for Quant-BnB. The numerical results illustrate the eﬀectiveness of
Quant-BnB in solving shallow optimal trees on large datasets with continuous features.

6.3 Comparison with heuristic methods

We study the test-accuracy of optimal decision trees. Earlier work [27, 13] in classiﬁcation with
binary features suggest that optimal trees can lead to better test-performance compared to
heuristics. We explore if similar empirical ﬁndings hold true for the tasks we consider herein. We
compare our approach with the well-known algorithm CART [9] and Tree Alternating Optimization
(TAO) [10]—both based on heuristics. Both CART, TAO consider the same models as Quant-BnB,
namely, axis-aligned trees with depth 2 or 3.

Q)/Lte

C − Lte

Q, where Lte

We compare the test error on a collection of 27 datasets: 16 classiﬁcation and 11 regression
tasks (see Appendix for details). Since the range of loss function of each test set varies, we study
the relative loss (Lte
C is the test error of
the competing algorithm (here, C is CART or TAO). The results are summarized in Fig 2. We
observe from the ﬁgure that Quant-BnB obtains depth−2 trees with lower test error in more than
66% datasets. When the depth is 3, Quant-BnB leads to better generalization in most of cases
compared to CART and TAO. The prediction performance of TAO is slightly better than CART,
at the expense of higher computational cost. The results indicate that optimal trees delivered by
Quant-BnB oﬀer an edge compared to heuristic methods, especially for deeper trees.

Q is test error of Quant-BnB, and Lte

Dataset
Name
avila
bank
bean
bidding
eeg
fault
htru
magic
occupancy
page
raisin
rice
room
segment
skin
wilt

(n,p)

(10430,10)
(1097,4)
(10888,16)
(5056,9)
(11984,14)
(1552,27)
(14318,8)
(15216,10)
(8143,5)
(4378,10)
(720,7)
(3048,7)
(8103,16)
(1848,18)
(196045,3)
(4339,5)

depth=2

depth=3

Quant-BnB BinOCT MurTree DL8.5 Quant-BnB BinOCT MurTree DL8.5
OoM
-
OoM
OoM
OoM
OoM
OoM
OoM
OoM
OoM
-
OoM
-
OoM
-
OoM

OoM
(100%)
OoM
(154%)
(13%)
(34%)
OoM
(14%)
(28%)
(35%)
(6.6%)
(11%)
(239%)
(250%)
(9.9%)
(56%)

(1.3%)
2963
(0%)
(1.1%)
(6.3%)
(9.1%)
(7.7%)
(2.6%)
(2.3%)
(0%)
9590
(3.0%)
(25%)
(0.5%)
(28%)
(0%)

4188
4.4
1014
30
4042
-
10303
1090
106
471
167
1340
180
153
350
67

OoM
142
OoM
6252
1783
-
OoM
OoM
1692
-
432
-
269
-
112
-

4.5
<0.1
3.4
0.2
2.9
1.6
1.3
1.0
0.3
0.4
0.1
0.4
1.0
1.1
2.3
0.2

OoM
8.4
OoM
345
288
530
OoM
OoM
193
155
13
591
18
389
37
653

3278
4.6
OoM
72
34
271
OoM
OoM
33
84
6.2
267
14
213
16
314

Table 3: Comparison of Quant-BnB aginst BinOCT, MurTree and DL8.5 on 16 classiﬁcation datasets. For
each dataset, the number of observations and the number of features are provided. Each entry denotes
running time in seconds. - refers to time out (4h), OoM refers to out of memory (25GB). If BinOCT times
out, we display the relative diﬀerence (LB − LQ)/LQ as a percentage instead, where LB and LQ are the
training errors of BinOCT and Quant-BnB, respectively.

13

7 Conclusions and Discussions

We present a novel BnB framework for optimal decision trees that applies to both regression and
classiﬁcation problems with continuous features. This extend the scope of optimal procedures in the
literature that have been developed for classiﬁcation problems with binary features. Our approach
is based on partitioning the feature values based on quantiles and using them to generate upper
and lower bounds. We discuss convergence guarantees of our procedure. Numerical experiments
suggest the eﬃciency of our approach for shallow decision trees.

Although a single optimal shallow tree appears to be somewhat restrictive in terms of prediction,
it can be useful for interpretability (it can be diﬃcult to interpret trees with depth much larger
than 3). Additionally, a heuristic procedure (e.g., CART) may require a larger depth to achieve
the same training/test error as an optimal tree with d = 3. To improve prediction performance of
a single shallow tree, one can use an ensemble (e.g., random forest, Boosting) of shallow trees.

Acknowledgements

We thank the reviewers for their comments that resulted in improvements in the paper. This
research is supported in part by grants from the Oﬃce of Naval Research (N00014-21-1-2841) and
Liberty Mutual Insurance.

(a) Comparison with CART, Depth= 2

(b) Comparison with TAO, Depth= 2

(c) Comparison with CART, Depth= 3

(d) Comparison with TAO, Depth= 3

Figure 2: Performance comparison of Quant-BnB against CART and TAO on 27 datasets. Lte
C is test error
of algorithm C (i.e., CART or TAO) and Lte
Q is test error of Quant-BnB. We summarize the relative diﬀerence
(Lte
Q -values between a heuristic method (C) and optimal decision trees delivered by Quant-BnB
(Q), shown in percentages using bar charts. A positive value of the x-axis means Quant-BnB performs
better than competing methods.

C − Lte

Q )/Lte

14

<-10-1000101020203030404050>50Difference in test error (in percentage)024681012Number of datasets<-10-1000101020203030404050>50Difference in test error (in percentage)02468101214Number of datasets<-10-1000101020203030404050>50Difference in test error (in percentage)0246810Number of datasets<-10-1000101020203030404050>50Difference in test error (in percentage)024681012Number of datasetsA Examples

A.1 An example of the space T2(f0, [a, b], F1, F2)

We consider a classiﬁcation dataset with n = 6, p = 3. The feature vectors and labels are provided
in Table 4.

Data
Feature 1
Feature 2
Feature 3
Label

x1 x2 x3 x4 x5 x6
5
1
5
0
5
0
2
1

4
4
5
1

3
3
3
2

3
2
3
1

2
1
0
2

Table 4: An classiﬁcation example with 6 samples. Each has 3 features and a label in {1, 2}.
.

Then we have u(1) = 5, and

w1

0 = −∞, w1

1 = 1, w1

2 = 2, w1

3 = 3, w1

4 = 4, w1

5 = 5, w1

6 = ∞,

˜w1

0 = −∞,

˜w1

1 = 1.5,

˜w1

2 = 2.5,

˜w1

3 = 3.5,

˜w1

4 = 4.5,

˜w1

5 = ∞.

(21)

(22)

As an example, the set T2(1, [1, 4], {2, 3}, {2, 3}) contains all trees whose splitting features at the
root node are 1; splitting thresholds at the root node are in {1.5, 2.5, 3.5, 4.5}; splitting features
at the left child and the right child are in {2, 3}.

A.2 An example of a single iteration of Quant-BnB

We follow the assumption on data given in the previous section. Now suppose at some iteration k,
the set AL(k−1) contains a single tuple (1, [1, 4], {2, 3}, {2, 3}). The current upper bound U equals
to 2, and the parameter s in Algorithm 1 is set to be 2. In addition, we choose W0(I, φ) deﬁned
in Eq.(9) as the lower bound required in Proposition 4.1.

To construct AL(k), Quant-BnB checks the tuple (1, [1, 4], {2, 3}, {2, 3}). Since s = 2 ≤ 4 − 1,
the algorithm computes (t0, t1, t2) = (1, 2, 4) being almost 2-equi-spaced in [1, 4] and conducts the
following 2 steps.

• (Step1: Update upper bound) Quant-BnB computes U (cid:48) by

U (cid:48) =

min
f1∈{2,3},f2∈{2,3}

(cid:8)V2([6], 1, [1, 4], f1, f2)(cid:9)
(cid:40)

(cid:110)

=

min
f1∈{2,3},f2∈{2,3}

min

L1({1}, f1) + L1({2, 3, 4, 5, 6}, f2),

L1({1, 2}, f1) + L1({3, 4, 5, 6}, f2), L1({1, 2, 3, 4, 5}, f1) + L1({6}, f2)

(cid:41)

(cid:111)

= 1.

Since U (cid:48) < U , Quant-BnB then updates U = U (cid:48) = 1.

15

• (Step2: Compute lower bound and prune) Quant-BnB computes W0([n], φj

) for any
j ∈ {1, 2}, f1, f2 ∈ {2, 3}. The results are (computation details are omitted for simplicity)

f1,f2

W0([n], φ1
W0([n], φ2

2,2) = 1, W0([n], φ1
2,2) = 0, W0([n], φ2

2,3) = 2, W0([n], φ1
2,3) = 1, W0([n], φ2

3,2) = 1, W0([n], φ1
3,2) = 0, W0([n], φ2

3,3) = 2,
3,3) = 1.

The algorithm then computes sets F1,j and F2,j according to (14) and (15) as

F1,1 := (cid:8)f1 ∈ F1
F1,2 := (cid:8)f1 ∈ F1
F2,1 := (cid:8)f2 ∈ F2
F2,2 := (cid:8)f2 ∈ F2

(cid:12)
(cid:12) min
f2∈F2
(cid:12)
(cid:12) min
f2∈F2
(cid:12)
(cid:12) min
f1∈F1
(cid:12)
(cid:12) min
f1∈F1

W ([n], φ1

W ([n], φ2

W ([n], φ1

W ([n], φ2

f1,f2) ≤ U (cid:9) = {2, 3},
f1,f2) ≤ U (cid:9) = {2, 3},
f1,f2) ≤ U (cid:9) = {2},
f1,f2) ≤ U (cid:9) = {2, 3}.

Finally, Quant-BnB updates

AL(k) = {(1, [1, 2], F1,1, F2,1)} ∪ {(1, [2, 4], F1,2, F2,2)}.

B Appendix for proofs

B.1 Auxiliary results

We ﬁrst prove a basic equality for L0.

Lemma B.1. For any disjoint sets I, J ⊆ [n], it holds

L0(I ∪ J ) ≥ L0(I) + L0(J ).

Proof. Note that

L0(I ∪ J ) = min
y∈Y

≥ min
y∈Y

(cid:88)

i∈I∪J
(cid:110) (cid:88)

i∈I

(cid:96)(yi, y) = min
y∈Y

(cid:111)

(cid:96)(yi, y)

+ min
y∈Y

i∈I
(cid:110) (cid:88)

j∈J

(cid:110) (cid:88)

(cid:96)(yi, y) +

(cid:111)

(cid:96)(yj, y)

(cid:88)

j∈J

(cid:111)

(cid:96)(yj, y)

= L0(I) + L0(J ).

Using the equality above, we prove a useful inequality for L1 presented below.

Lemma B.2. For any disjoint sets I, J ⊆ [n] and any f ∈ [p], it holds

L1(I ∪ J , f ) ≥ L1(I, f ) + L1(J , f ).

Proof. By the deﬁnition 6, there exists integer t∗ with 0 ≤ t∗ ≤ u(f ) such that

L1(I ∪ J , f ) = L0

(cid:16)

(I ∪ J )f

[0,t∗]

(cid:17)

(cid:16)

+ L0

(I ∪ J )f

[t∗,u(f )]

(cid:17)

.

16

(23)

(24)

(25)

(26)

Note that (I ∪ J )f

[0,t∗] = (I)f

[0,t∗] ∩ (J )f

[0,t∗] = ∅, so by Lemma B.1, we have

[0,t∗] ∪ (J )f
(cid:16)

[0,t∗] and (I)f
(cid:17)

L0

(I ∪ J )f

[0,t∗]

≥ L0(I f

[0,t∗]) + L0(J f

[0,t∗]).

By a similar argument we have

(cid:16)

L0

(I ∪ J )f

[t∗,u(f )]

(cid:17)

By (26), (27) and (28) we have

≥ L0(I f

[t∗,u(f )]) + L0(J f

[t∗,u(f )]).

(27)

(28)

L1(I ∪ J , f ) ≥ L0(I f

[0,t∗]) + L0(I f
(cid:8)L0(I f

[t∗,u(f )]) + L0(J f

[0,t∗]) + L0(J f

[0,t]) + L0(I f

[t,u(f )])(cid:9) + min

0≤t≤u(f )

[t∗,u(f )])
(cid:8)L0(J f

≥ min

0≤t≤u(f )

[0,t]) + L0(J f

[t,u(f )])(cid:9)

= L1(I, f ) + L1(J , f ).

This completes the proof.

A similar inequality also holds for L2, as shown below.

Lemma B.3. For any disjoint sets I, J ⊆ [n] and any φ ∈ Φ, it holds

L2(I ∪ J , φ) ≥ L2(I, φ) + L2(J , φ).

Proof. Given φ = (f0, [a, b], f1, f2) ∈ Φ, from the equality (7), there exists an integer t∗ ∈ [a, b]
such that

[t∗,u(f0)], f2).

(29)

Note that (I ∪ J )f0

[0,t∗] ∪ (J )f0

[0,t∗] and (I)f0

[0,t∗] = ∅, so by Lemma B.2, we have

L2(I ∪ J , φ) = L1((I ∪ J )f0

[0,t∗] = (I)f0
(cid:16)

L1

(I ∪ J )f0

[0,t∗], f1

[0,t∗], f1) + L1((I ∪ J )f0
[0,t∗] ∩ (J )f0

(cid:17)

≥ L1(I f0

[0,t∗], f1) + L1(J f0

[0,t∗], f1).

≥ L1(I f0

[t∗,u(f0)], f2) + L1(J f0

[t∗,u(f0)], f2).

(30)

(31)

By a similar argument we have

(cid:16)

L1

(I ∪ J )f0

[t∗,u(f0)], f2

(cid:17)

Combining (29), (30) and (31), we have

L2(I ∪ J , φ) ≥ L1(I f0

[t∗,u(f0)], f2) + L1(J f0

[0,t∗], f1) + L1(J f0

[t∗,u(f0)], f2)

≥

min
0≤t≤u(f0)

[0,t], f1) + L1(I f0

[t,u(f0)], f2)(cid:9) + min

(cid:8)L1(J f0

[0,t], f1) + L1(J f0

[t,u(f0)], f2)(cid:9)

0≤t≤u(f0)

[0,t∗], f1) + L1(I f0
(cid:8)L1(I f0

= L2(I, φ) + L2(J , φ).

This completes the proof.

17

B.2 Proof of Lemma 3.1

Proof. By deﬁnition (7) we have

L2(I, f0, [a, b], f1, f2) = min
a≤t≤b

Let t∗ be the integer in [a, b] such that

(cid:8)L1(I f0

[0,t], f1) + L1(I f0

[t,u(f0)], f2)(cid:9).

L2(I, f0, [a, b], f1, f2) = L1(I f0

[0,t∗], f1) + L1(I f0

[t∗,u(f0)], f2).

Since there exists j∗ ∈ [s(cid:48)] such that tj∗−1 ≤ t∗ ≤ tj∗, by Lemma B.2 we have

L1(I f0

[0,t∗], f1) ≥ L1(I f0

[0,tj∗−1], f1),

Combining (32) and (33) we have

and L1(I f0

[t∗,u(f0)], f2) ≥ L1(I f0

[tj∗ ,u(f0)], f2).

(32)

(33)

L2(I, f0, [a, b], f1, f2) ≥ L1(I f0

[0,tj∗−1], f1) + L1(I f0
(cid:8)L1(I f0

[0,tj−1], f1) + L1(I f0

[tj∗ ,u(f0)], f2)

[tj ,u(f0)], f2)(cid:9)

≥ min
j∈[s(cid:48)]

= (cid:98)L2(I, f0, [a, b], f1, f2, s(cid:48)).

B.3 Proof of Lemma 3.2

Proof. By deﬁnition it is trivial that W0(I, φ) ≤ W1,s(cid:48)(I, φ) for any s(cid:48) ≤ b − a; By Lemma 3.1, we
know that W1,s(cid:48)(I, φ) ≤ W2(I, φ). In the following, we prove that W2(I, φ) ≤ L2(I, φ). Suppose
φ = (f0, [a, b], f1, f2) with f0, f1, f2 ∈ [p] and 0 ≤ a ≤ b ≤ u(f0). Note that

L2(I, φ) = min
t∈[a,b]

(cid:110)
L1(I f0

[0,t], f1) + L1(I f0

[t,u(f0)], f2)

(cid:111)

≥ min
t∈[a,b]
= L1(I f0

(cid:110)
L1(I f0

[0,a], f1) + L1(I f0

[a,t], f1) + L1(I f0

[t,b], f2) + L1(I f0

(cid:111)
[b,u(f0)], f2)

[0,a], f1) + L1(I f0

[b,u(f0)], f2) + L2(I f0

[a,b], f0, [a, b], f1, f2) = W2(I, φ),

where the inequality follows from Lemma B.2.

B.4 Proof of Proposition 4.1

Proof. Note that

T2(f0, [a, b], F1, F2) =

s
(cid:91)

j=1

T2(f0, [tj−1, tj], F1, F2).

(34)

So we have

T2(f0, [a, b], F1, F2)\

s
(cid:91)

j=1

T2(f0, [tj−1, tj], F1,j, F2,j) =

s
(cid:91)

(cid:16)

j=1

T2(f0, [tj−1, tj], F1, F2)\T2(f0, [tj−1, tj], F1,j, F2,j)

(cid:17)

.

18

Suppose (for contradiction) that an optimal solution T ∗ is in the l.h.s. of the above set, then
there exists j ∈ [s] such that

T ∗ ∈ T2(f0, [tj−1, tj], F1, F2) \ T2(f0, [tj−1, tj], F1,j, F2,j).

(35)

Then we know fO(T ∗) = f0, fL(T ∗) ∈ F1, fR(T ∗) ∈ F2, and at least one of the following two cases
hold:

(i) fL(T ∗) ∈ F1 \ F1,j;

(ii) fR(T ∗) ∈ F2 \ F2,j.

If (i) holds, then we have

L2([n], f0, [tj−1, tj], fL(T ∗), fR(T ∗)) ≥ W ([n], f0, [tj−1, tj], fL(T ∗), fR(T ∗))

≥ min
f2∈F2

(cid:8)W ([n], f0, [tj−1, tj], fL(T ∗), f2)(cid:9) > U,

(36)

(37)

where the ﬁrst inequality is by the assumption that W (I, φ) ≤ L2(I, φ) for all φ ∈ Φ; the last
inequality is by the deﬁnition of F1,j. Note that L2([n], f0, [tj−1, tj], fL(T ∗), fR(T ∗)) is the optimal
value of (2), this is a contradiction to the assumption that U is an upper bound of the optimal
value of (2). If (ii) holds, by a similar argument as shown above we have a contradiction.

B.5 Proof of Theorem 4.2

Proof. Deﬁne

Ck :=

max
(f0,[a,b],F1,F2)∈AL(k)

{b − a}

as the length of the longest interval over all tuples in AL(k). As stated in Section 4.2, in each
iteration Algorithm 1 splits space T2(f0, [a, b], F1, F2) into at most s subsets

(cid:91)

j∈[s]

T2(f0, [tj−1, tj], F1,j, F2,j).

Using the deﬁnition of s-equi-spaced points in [a, b], and note that for any real numbers s1, s2 it
holds (cid:98)s1(cid:99) − (cid:98)s2(cid:99) ≤ (cid:100)s1 − s2(cid:101), we have

tj − tj−1 ≤ (cid:100)a + (j/s)(b − a) − a − ((j − 1)/s)(b − a)(cid:101) =

(cid:25)

(cid:24) b − a
s

for all j ∈ [s].

Namely, the length of the interval [a, b] in the tuple (f0, [a, b], F1, F2) reduces to 1/s of its original
(cid:108) Ck−1
value in every iteration. Therefore, Ck ≤
for any k ≥ 1. Applying this inequality
s
recursively, it holds that for any positive integers k0, m, Ck0 ≤ m as long as C0 ≤ msk0. Setting
k0 = (cid:100)logs(n)(cid:101) − 1 and m = (cid:100) C0

(cid:109)

sk0 (cid:101) yields

Ck0 ≤

(cid:109)

(cid:108) C0
sk0

≤

(cid:109)

(cid:108) n
sk0

≤ s.

If AL(k0) is not empty, then any tuple (f0, [a, b], F1, F2) in it satisﬁes b − a ≤ s. The algorithm will
then perform the exhaustive search method to each tuple in AL(k0) in the iteration k0 + 1. Hence,
AL(k0+1) = ∅, and the algorithm terminates in at most k0 + 1 = (cid:100)logs(n)(cid:101) iterations.

19

Now, we prove that the algorithm yields an optimal solution. Suppose (by contradiction)
that for some k0, AL(k0) becomes empty and the algorithm oﬀers an sub-optimal solution. This
indicates that in some iteration k ≤ k0, (at least one of if there exist multiple optimal solutions)
the optimal solution is discarded during Step2, otherwise the algorithm would examine the loss of
the optimal solution and record it since it is optimal. This contradicts to Proposition 4.1, which
guarantees that no optimal solution will be eliminated in Step2. Therefore, the algorithm will
give the optimal solution. The proof is completed.

B.6 Proof of Lemma 4.3

Proof. Note that for all these three choices of W , Step 1 is the same.

For the cost of Step 1, note that

U (cid:48) =

min
f1∈F1,f2∈F2,0≤j≤s

(cid:8)L1([n]f0

[0,tj ], f1) + L1([n]f0

[tj ,u(f0)], f2)(cid:9)

= min
0≤j≤s

(cid:110)

min
f1∈F1

{L1([n]f0

[0,tj ], f1)} + min
f2∈F2

{L1([n]f0

[tj ,u(f0)], f2)}

(cid:111)
.

(38)

Recall that L1(I, f ) can be computed within (cid:101)O(|I|) operations, so U (cid:48) can be computed within
(cid:101)O(n˜ps) operations.

The major cost of Step 2 lies in the computation of

min
f2∈F2

W ([n], φj

f1,f2

) = min
f2∈F2

(cid:8)W ([n], f0, [tj−1, tj], f1, f2)(cid:9)

for all f1 ∈ F1 and j ∈ [s],

(39)

and

min
f1∈F1

W ([n], φj

f1,f2

) = min
f1∈F1

(cid:8)W ([n], f0, [tj−1, tj], f1, f2)(cid:9)

for all f2 ∈ F2 and j ∈ [s].

(40)

Once (39) and (40) have been computed, the remaining cost of Step 2 can be bounded by O(˜ps).
Below we show the costs of (39) and (40), under diﬀerent choices of W .

(1) If W = W0, we have

min
f2∈F2

(cid:8)W ([n], f0, [tj−1, tj], f1, f2)(cid:9) = min
f2∈F2
= L1([n]f0

(cid:110)
L1([n]f0

[0,tj−1], f1) + L1([n]f0
(cid:8)L1([n]f0

(cid:111)
[tj ,n], f2)
[tj ,n], f2)(cid:9),

[0,tj−1], f1) + min
f2∈F2

(41)

where the ﬁrst equality makes use of the deﬁnition of W0 and the assumption that u(f0) = n. By
the expression above, we know that computing (39) requires at most (cid:101)O(n˜ps) operations. By a
similar argument, computing (40) also requires at most (cid:101)O(n˜ps) operations. Hence the cost of
Step 2 is bounded by (cid:101)O(n˜ps).

(2) If W = W1,s(cid:48) for some integer s(cid:48) ≤ b − a, we have

min
f2∈F2

= min
f2∈F2

(cid:8)W ([n], f0, [tj−1, tj], f1, f2)(cid:9)
(cid:110)
L1([n]f0

[0,tj−1], f1) + L1([n]f0

[tj ,n], f2) + (cid:98)L2([n]f0

(cid:111)
[tj−1,tj ], f0, [tj−1, tj], f1, f2, s(cid:48))
,

(42)

20

where we have used the assumption that u(f0) = n. Let (r0, ...., rs(cid:48)) be the almost s(cid:48)-equi-spaced
integers in [tj−1, tj], then by the deﬁnition of (cid:98)L2 we have

min
f2∈F2

(cid:8)W ([n], f0, [tj−1, tj], f1, f2)(cid:9)
(cid:110)
L1([n]f0

[0,tj−1], f1) + L1([n]f0
(cid:110)
L1([n]f0

= L1([n]f0

[0,tj−1], f1) + min
k∈[s(cid:48)]

=

min
f2∈F2,k∈[s(cid:48)]

:= J1(j, f1) + min
k∈[s(cid:48)]

(cid:110)

J3(j, f1, k) + min
f2∈F2

[tj−1,rk−1], f1) + L1([n]f0

[rk,tj ], f2) + L1([n]f0

[tj ,n], f2)

(cid:111)

[tj−1,rk−1], f1) + min
f2∈F2

(cid:8)L1([n]f0
(cid:8)J4(j, f2, k) + J2(j, f2)(cid:9)(cid:111)
.

[rk,tj ], f2) + L1([n]f0

[tj ,n], f2)(cid:9)(cid:111)

(43)

where J1(j, f1) := L1([n]f0
and J4(j, f2, k) := L1([n]f0

[0,tj−1], f1); J2(j, f2) := L1([n]f0
[tj−1,rk−1], f1)
[rk,tj ], f2), and we highlighted the dependence on j, f1, f2, k. Note that:

[tj ,n], f2); J3(j, f1, k) := L1([n]f0

• {J1(j, f1)}j∈[s],f1∈F1 can be computed with (cid:101)O(n˜ps) operations.

• {J2(j, f2)}j∈[s],f2∈F2 can be computed with (cid:101)O(n˜ps) operations.

• {J3(j, f1, k)}j∈[s],f1∈F1,k∈[s(cid:48)] can be computed with (cid:101)O(˜ps(cid:48)(tj − tj−1)s) = (cid:101)O(˜ps(cid:48)(b − a)) opera-

tions.

• {J4(j, f2, k)}j∈[s],f2∈F2,k∈[s(cid:48)] can be computed with (cid:101)O(˜ps(cid:48)(tj − tj−1)s) = (cid:101)O(˜ps(cid:48)(b − a)) opera-

tions.

After the values above have been computed and maintained in memory,

(cid:110)

•

minf2∈F2

(cid:8)J4(j, f2, k) + J2(j, f2)(cid:9)(cid:111)

j∈[s],k∈[s(cid:48)]

can be computed with O(˜pss(cid:48)) operations.

Based on this, we know

• Computing mink∈[s(cid:48)]

J3(j, f1, k) + minf2∈F2
f1 ∈ F1 requires at most O(˜pss(cid:48)) operations.

(cid:110)

(cid:8)J4(j, f2, k) + J2(j, f2)(cid:9)(cid:111)

for all j ∈ [s] and

With the analysis above, the computation of (39) requires at most (cid:101)O(n˜ps + ˜ps(cid:48)(b − a)) operations.
By a similar analysis, the computation of (40) also requires at most (cid:101)O(n˜ps + ˜ps(cid:48)(b − a)) operations.
(3) If W = W2, the analysis is the same as the analysis for W = W1,s(cid:48), with s(cid:48) being of the

same order as (b − a)/s.

(4) If W = L2, it holds

min
f ∈F2

W ([n], f0, [tj−1, tj], f1, f2) = min
f ∈F2

L2([n], f0, [tj−1, tj], f1, f2)

=

min
f2∈F2,tj−1≤t≤tj

(cid:110)
L1([n]f0

[0,t], f1) + L1([n]f0

[t,n], f2)

(44)

(cid:111)
.

For a ﬁxed j ∈ [s], computing the expression above requires (cid:101)O((tj − tj−1)˜pn) operations, so the
computation of (39) takes (cid:101)O(n˜p(b − a)) operations. By a similar analysis, the computation of
(40) also takes (cid:101)O(n˜p(b − a)) operations.

21

C Quant-BnB for depth-3 optimal regression trees

In the following we present details on Quant-BnB for depth-3 optimal regression trees.

C.1 Notations and preliminaries
Let T3 be the set of all decision trees with depth 3 whose splitting thresholds are in { ˜wf
The problem of optimal regression tree with depth 3 can be formulated as

t }f ∈[p],0≤t≤u(f ).

min
T ∈T3

n
(cid:88)

i=1

(cid:96)(yi, T (xi)).

(45)

We use the notations shown in Figure 3 to denote the nodes in a tree T ∈ T3. For S ∈
{O, L, R, LL, LR, RL, RR}, let (fS(T ), tS(T )) denote the splitting rule for NS(T ).

NO(T )

NL(T )

NR(T )

NLL(T )

NLR(T )

NRL(T )

NRR(T )

Leaf

Leaf

Leaf

Leaf

Leaf

Leaf

Leaf

Leaf

Figure 3: Decision tree with depth 3.

Given f0 ∈ [p], integers a and b with 0 ≤ a ≤ b ≤ u(f0) and φ1, φ2 ∈ Φ with φ1 =

(f1, [a1, b1], f1,1, f1,2) and φ2 = (f2, [a2, b2], f2,1, f2,2), deﬁne

T3(f0, [a0, b0], φ1, φ2)

(46)

to be the set of all trees T ∈ T3 satisfying: fO(T ) = f0, tO ∈ [a0, b0]; fL(T ) = f1, tL(T ) ∈ [a1, b1],
fLL(T ) = f1,1, fLR(T ) = f1,2;
For Φ1, Φ2 ⊆ Φ, deﬁne

fR(T ) = f2, tR(T ) ∈ [a2, b2], fRL(T ) = f2,1 and fRR(T ) = f2,2.

T3(f0, [a0, b0], Φ1, Φ2) :=

(cid:91)

φ1∈Φ1,φ2∈Φ2

T3(f0, [a0, b0], φ1, φ2).

(47)

For any φ = (f0, [a, b], f1, f2) ∈ Φ, and for a given positive integer s ≤ b − a, let (t0, ..., ts) be
almost equi-spaced in [a, b], and deﬁne φs,j := (f0, [tj−1, tj], f1, f2) for any j ∈ [s].

22

C.2 Quantile-based pruning

In this section, we focus on a subset of trees T3(f0, [a0, b0], Φ1, Φ2), and discuss how to replace it
with a smaller search space without missing the optimal solution.

Proposition C.1. Let W and V be two functions on 2[n] × Φ satisfying W (I, φ) ≤ L2(I, φ) ≤
V (I, φ) for all I ⊆ [n] and φ ∈ Φ. Let U be an upper bound of the optimal value of (45). Given a
subset T3(f0, [a0, b0], Φ1, Φ2) with f0 ∈ [p], 0 ≤ a0 ≤ b0 ≤ u(f0) and Φ1, Φ2 ⊆ Φ; let (t0, ..., ts) be
almost s-equi-spaced in [a0, b0]. For each j ∈ [s], deﬁne

Φ1,j :=

(cid:110)
φs,j(cid:48)
1

(cid:12)
(cid:12) φ1 ∈ Φ1, j(cid:48) ∈ [s], W ([n]f0
(cid:12)
[0,tj−1], φs,j(cid:48)
W ([n]f0

) + min
φ∈Φ2

1

1

[0,tj−1], φs,j(cid:48)
W ([n]f0

) ≤ min
φ∈Φ1

V ([n]f0

[0,tj ], φ),
(cid:111)

[tj ,u(f0)], φ) ≤ U

and

Φ2,j :=

(cid:110)
φs,j(cid:48)(cid:48)
2

(cid:12)
(cid:12) φ2 ∈ Φ2, j(cid:48)(cid:48) ∈ [s], W ([n]f0
(cid:12)
[tj ,u(f0)], φs,j(cid:48)(cid:48)
W ([n]f0

) + min
φ∈Φ1

2

[tj ,u(f0)], φs,j(cid:48)(cid:48)
W ([n]f0

2

[0,tj−1], φ) ≤ U

(cid:111)
.

) ≤ min
φ∈Φ2

V ([n]f0

[tj−1,u(f0)], φ),

(48)

(49)

Then any optimal solution of (45) is not in

T3(f0, [a0, b0], Φ1, Φ2) \

s
(cid:91)

j=1

T3(f0, [tj−1, tj], Φ1,j, Φ2,j).

(50)

Proof. Let T ∗ be any optimal solution of (50). It suﬃces to prove that: if T ∗ ∈ T3(f0, [a0, b0], Φ1, Φ2),
then there exists j ∈ [s] such that

T ∗ ∈ T3(f0, [tj−1, tj], Φ1,j, Φ2,j).

(51)

Suppose T ∗ ∈ T3(f0, [a0, b0], Φ1, Φ2), then tO(T ∗) ∈ [a0, b0], and there exist integers a1, b1, a2, b2
such that tL(T ∗) ∈ [a1, b1], tR(T ∗) ∈ [a2, b2], and

φ1 := (fL(T ∗), [a1, b1], fLL(T ∗), fLR(T ∗)) ∈ Φ1,
φ2 := (fR(T ∗), [a2, b2], fRL(T ∗), fRR(T ∗)) ∈ Φ2.

(52)

Since [a0, b0] = ∪s

j=1[tj−1, tj], there exists j ∈ [s] such that tO(T ∗) ∈ [tj−1, tj]. Let ((cid:96)0, ...., (cid:96)s)
i=1[(cid:96)i−1, (cid:96)i], so there exists j(cid:48) ∈ [s] such

be almost equi-spaced in [a1, b1]. tL(T ∗) ∈ [a1, b1] = ∪s
that tL(T ∗) ∈ [(cid:96)j(cid:48)−1, (cid:96)j(cid:48)]. Note that (by deﬁnition)

φs,j(cid:48)
1 = (fL(T ∗), [(cid:96)j(cid:48)−1, (cid:96)j(cid:48)], fLL(T ∗), fLR(T ∗)),

we have

W ([n]f0

(i)
≤ L2([n]f0

)

1

[0,tj−1], φs,j(cid:48)
L2([n]f0

[0,t0(T ∗)], φ)

1

)

[0,tj−1], φs,j(cid:48)
L2([n]f0

(iv)
≤ min
φ∈Φ1

(ii)
≤ L2([n]f0

[0,t0(T ∗)], φs,j(cid:48)
V ([n]f0

1

(v)
≤ min
φ∈Φ1

[0,tj ], φ)

[0,tj ], φ),

(iii)
= min
φ∈Φ1

(53)

(54)

)

23

where (i) is because W (I, φ) ≤ L2(I, φ) for any I ⊆ [n] and φ ∈ Φ; (ii) is because of tO(T ∗) ∈
[tj−1, tj] and Lemma B.3; (iii) is because T ∗ is the optimal solution of (45), and the left subtree
of T ∗ (rooted at NL(T ∗)) is in T2(φs,j(cid:48)
); (iv) is because of tO(T ∗) ∈ [tj−1, tj] and Lemma B.3; (v)
is because L2(I, φ) ≤ V (I, φ) for any I ⊆ [n] and φ ∈ Φ.

1

On the other hand,

W ([n]f0

[0,tj−1], φs,j(cid:48)

1

) + min
φ∈Φ2

W ([n]f0

[tj ,u(f0)], φ)

(i)
≤ W ([n]f0

(iii)
≤ L2([n]f0

1

[0,tj−1], φs,j(cid:48)
[0,t0(T ∗)], φs,j(cid:48)

1

) + W ([n][tj ,u(f0)], φ2)

(ii)
≤ L2([n]f0

) + L2([n]f0

[t0(T ∗),u(f0)], φ2)

[0,tj−1], φs,j(cid:48)
n
(cid:88)

1

(iv)
= min
T ∈T3

i=1

) + L2([n]f0

(cid:96)(yi, T (xi))

[tj ,u(f0)], φ2)
(v)
≤ U,

(55)

where (i) is because φ2 ∈ Φ2 (in (52)); (ii) is because W (I, φ) ≤ L2(I, φ) for any I ⊆ [n] and
φ ∈ Φ; (iii) is because of t0(T ∗) ∈ [tj−1, tj] and Lemma B.3; (iv) is because T ∗ is the optimal
solution of (45), the left subtree of T ∗ (rooted at NL(T ∗)) is in T2(φs,j(cid:48)
) and the right subtree of
T ∗ (rooted at NR(T ∗)) is in T2(φ2); (v) is because U is an upper bound of the optimal value of
(45).

1

Combining (54), (55) and note that φ1 ∈ Φ1 (by (52)) and j(cid:48) ∈ [s], we have

(fL(T ∗), [(cid:96)j(cid:48)−1, (cid:96)j(cid:48)], fLL(T ∗), fLR(T ∗)) = φs,j(cid:48)

1 ∈ Φ1,j.

(56)

Let (r0, ...., rs) be almost equi-spaced in [a2, b2]. Since tR(T ∗) ∈ [a2, b2] = ∪s
i=1[ri−1, ri], so there
exists j(cid:48)(cid:48) ∈ [s] such that tR(T ∗) ∈ [rj(cid:48)(cid:48)−1, rj(cid:48)(cid:48)]. By a similar argument as the proof of (56), it can
be proved that

(fR(T ∗), [rj(cid:48)(cid:48)−1, rj(cid:48)(cid:48)], fRL(T ∗), fRR(T ∗)) ∈ Φ2,j.

By (56) and (57), and recall that fO(T ∗) = f0 and tO(T ∗) ∈ [tj−1, tj], we have

This completes the proof of Proposition C.1.

T ∗ ∈ T3(f0, [tj−1, tj], Φ1,j, Φ2,j).

(57)

(58)

Note that in the deﬁnition of Φ1,j, two inequalities are needed to be satisﬁed. The ﬁrst
inequality corresponds to a pruning procedure when only considering the left depth-2 subtree
rooted at NL(T ) of a tree T ; the second inequality corresponds to a pruning procedure considering
the whole depth-3 tree. A similar argument holds for the deﬁnition of Φ2,j. Note that this is
slightly more intricate than the case for ﬁtting depth-2 trees, where only one inequality is imposed
(see Proposition 4.1).

C.3 Overall strategy

To solve (45), Quant-BnB maintains and updates a set AL3(k) (short for “alive”) (over iterations
k = 1, 2, ...) that contains tuples of the form

(f0, [a0, b0], Φ1, Φ2),

24

where f0 ∈ [p], 0 ≤ a0 ≤ b0 ≤ u(f0) and Φ1, Φ2 ⊆ Φ. A tuple (f0, [a0, b0], Φ1, Φ2) is in AL3(k) if
(based on the knowledge at iteration k) the optimal solution of (45) is possible to be in the set
T3(f0, [a0, b0], Φ1, Φ2). Initially (k = 0), all the trees in T3 are “alive”, so we set

where

AL3(0) =

p
(cid:91)

f0=1

(cid:8)(f0, [0, u(f0)], ¯Φ0, ¯Φ0)(cid:9) ,

¯Φ0 = {(f, [0, u(f )], f1, f2) | f, f1, f2 ∈ [p]}.

(59)

(60)

Quant-BnB also maintains and updates the best objective value that it has found so far, denoted
by U . Initially, we set U to be the value of any feasible solution of (45). At every iteration k ≥ 1,
to update AL3(k) from AL3(k−1), we ﬁrst set AL3(k) = ∅. The algorithm then checks all elements in
AL3(k−1). For an element (f0, [a0, b0], Φ1, Φ2) in AL3(k−1), if b0 − a0 is less than a ﬁxed integer s,
then the number of trees in the space is regarded as small, and the algorithm applies an exhaustive
search method to examine all candidate trees in the space T3(f0, [a0, b0], Φ1, Φ2). Otherwise, the
algorithm conducts the following steps.

Let (t0, ..., ts) be almost s-equi-spaced in [a0, b0]. Let W and V be two functions on 2[n] × Φ

satisfying W (I, φ) ≤ L2(I, φ) ≤ V (I, φ) for all I ⊆ [n] and φ ∈ Φ.

• (Step 1: Update upper bound) Compute

U (cid:48) =

min
0≤j≤s,φ1∈Φ1,φ2∈Φ2

(cid:110)

V ([n]f0

[0,tj ], φ1) + V ([n]f0

(cid:111)
.
[tj ,u(f0)], φ2)

If U (cid:48) < U , update U ← U (cid:48), and update the corresponding best tree.

• (Step 2: Compute lower bound and prune) Compute Φ1,j and Φ2,j as in (48) and (49), and

update:

AL3(k) = AL3(k) (cid:91) (cid:16)

∪s

j=1 {(f0, [tj−1, tj], Φ1,j, Φ2,j)}

(cid:17)

.

(61)

Above we have discussed the overall strategy which Quant-BnB uses for the computation
of optimal regression tree with depth 3. Additional attentions need to be paid for the the
implementation of the algorithm and the data structure. For example, to maintain the set Φ1, it
may be better to classify the elements in Φ1 into groups depending on the ﬁrst two components of
the elements, i.e., for (f1, [a1, b1], f1,1, f1,2) and (f (cid:48)
1,2) in Φ1, they are in the same
group if f1 = f (cid:48)
1 and [a1, b1] = [a(cid:48)
1]. These groupings can reduce the memory usage and make
the computations well-organized. A similar argument holds for Φ2. It is also important to make
use of the tree structure and reduce the computational costs of Step 1 and Step 2.

1,1, f (cid:48)

1], f (cid:48)

1, [a(cid:48)

1, b(cid:48)

1, b(cid:48)

25

D Experiments

D.1 Data pre-processing

We use a collection of 16 classiﬁcation and 11 regression datasets from UCI Machine Learning
Repository. Unless speciﬁed in the dataset, 80% of data are randomly selected as the training set,
and the rest are used for testing. We remove all features that do not assist prediction, i.e., unique
identiﬁers of samples and timestamps recording when data were collected. Table 5 presents a
summary of these datasets.

Name

Task

avila
bank
bean
bidding
eeg
fault
htru
magic
occupancy
page
raisin
rice
room
segment

C
C
C
C
C
C
C
C
C
C
C
C
C
C

number of
samples
10430
1097
10888
5056
11984
1552
14318
15216
8143
4378
720
3048
8103
1848

number of
features
10
4
16
9
14
27
8
10
5
10
7
7
16
18

class/dim Name

Task

skin
wilt
carbon
casp
concrete
energy
ﬁsh
gas
grid
news
qsar
query1
query2

C
C
R
R
R
R
R
R
R
R
R
R
R

12
2
7
2
2
7
2
2
2
5
2
2
4
7

number of
samples
196045
4339
8576
36584
824
15788
726
29386
8000
31715
436
8000
159874

number of
features
3
5
5
9
8
28
6
10
12
59
8
3
4

class/dim

2
2
3
1
1
1
1
1
1
1
1
1
3

Table 5: A summary of datasets used in experiments. C and R refer to classiﬁcation and regression
task, respectively. For classiﬁcation tasks class/dim refers to the number of classes; for regression
tasks, class/dim refers to the dimension of the target.

Since most state-of-the-art algorithms can only solve datasets with binary features, we perform
the same pre-processing procedure as presented in [21]. For a feature f ∈ [p], recall that
wf
2 < · · · < wf
1 < wf
i=1. We convert feature f to a set
of binary features {fj}u(f )−1

u(f ) denotes all unique values among {xi,f }n

deﬁned as

j=1

(cid:40)

xi,fj =

0 if xi,f < (wf
1 otherwise.

j + wf

j+1)/2,

(62)

j=1

for each f ∈ [p] yields a dataset with (cid:80)

Combining {fj}u(f )−1
f ∈[p](u(f ) − 1) binary features. The
conversion is equivalent, namely, an optimal tree on the pre-processed dataset can be converted to
an optimal tree on the original dataset and vice versa. In the worst case, the converted dataset
has O(np) binary features. It is often computationally challenging to solve the optimal decision
tree on such a dataset.

An alternative to equivalent-conversion is approximate conversion, which can greatly reduce
the number of binary features. However, approximate conversion weakens the characterization
capability of the tree model, which may result in non-negligible decrease in training accuracy.
Hence, there is a trade-oﬀ between training accuracy and computational cost.

26

We compare the equivalent-conversion conducted in our numerical experiments with an
approximate binarising method adopted in [13] (denoted by MDLP). MDLP uses a supervised
discretisation algorithm based on the minimum description length principle. We take the training
accuracy of CART [9] on original datasets as a benchmark. The size of converted datasets and
training accuracy are shown in Table 6. For dataset “avila”, “bean”,“fault” “room”, “segment”
and “skin”, the training accuracy on approximate datasets generated by MDLP is close to the
optimal accuracy. However, the accuracy on original datasets outperforms that on approximate
datasets by a large margin in rest cases. For several datasets, even the training accuracy of CART
is comparable to the training accuracy on approximate datasets. We thus conclude that using
equivalent-conversion is indispensable for obtaining high-quality trees.

Name

avila
bank
bean
bidding
eeg
fault
htru
magic
occupancy
page
raisin
rice
room
segment
skin
wilt

continuous
feature
10
4
16
9
14
27
8
10
5
10
7
7
16
18
3
5

equivalent
conversion
22176
4078
170481
10240
5239
16327
101412
120435
8339
8175
5032
19982
2879
13129
765
20329

MDLP
conversion
2122
26
428
44
118
244
92
122
122
50
18
28
144
145
108
7

Opt

depth=2
approx CART

depth=3
Opt
approx CART
54.2% 54.1% 50.7% 58.5% 58.5% 53.2%
92.5% 92.3% 90.9% 98.3% 97.3% 93.3%
66.3% 66.2% 65.7% 87.1% 86.9% 77.7%
98.1% 98.1% 98.1% 99.3% 98.1% 98.1%
66.5% 65.3% 62.5% 70.8% 68.8% 66.6%
58.3% 58.3% 54.0% 68.2% 67.3% 55.3%
97.9% 97.8% 97.7% 98.1% 97.9% 97.9%
80.5% 80.2% 79.4% 83.1% 81.1% 80.1%
98.9% 98.9% 98.9% 99.4% 99.1% 98.9%
95.4% 95.1% 95.4% 97.1% 96.6% 96.4%
87.4% 86.5% 86.8% 89.4% 87.5% 86.9%
93.3% 93.2% 93.0% 93.8% 93.4% 93.3%
94.6% 94.6% 93.2% 99.2% 99.2% 96.8%
57.5% 57.4% 43.0% 88.7% 88.1% 57.4%
92.7% 92.7% 90.7% 96.9% 96.8% 96.6%
99.1% 98.7% 99.1% 99.6% 98.7% 99.3%

Table 6: Training accuracy (in percentage) of optimal classiﬁcation trees with depth 2 and 3.
The third and forth columns provide the numbers of binary features of datasets processed by
equivalent-conversion and MDLP, respectively. Opt denotes the training accuracy of the optimal
classiﬁcation tree on original datasets, approx denotes the training accuracy of the optimal
classiﬁcation tree on datasets binarised using MDLP, and CART denotes the training accuracy of
CART on original datasets.

D.2 Optimization algorithms

Details and experimental settings of all comparison algorithms are stated below. Unless speciﬁed,
implementations of algorithms used in our experiments are obtained from their original authors.

• Quant-BnB: Our proposed algorithm is written in Julia programming language (v1.6).
The parameter s in Algorithm 1 is set to be 3. In Section 6.2 and Section 6.3, we choose
W1,s(cid:48) deﬁned in (13) as the lower bound required in Proposition 4.1. The parameter s(cid:48) is
dynamically selected as (cid:98) 0.6ns
b−a (cid:99) for tuple φ = (f0, [a, b], f1, f2).

• CART [9]: We utilize the implementation from Python package scikit-learn.

27

• TAO [10]: We implement TAO in Julia 1.6. TAO uses the solution generated by CART as

a warm start.

• OCT and ORT [7] : Since the original code is not available, we implement both methods in
Python and call Gurobi9 to solve MIP models. Both methods takes the solution generated
by CART as a warm start.

• BinOCT [27]: BinOCT is written in Python. We slightly modify the code so that the MIP
model is solved by Gurobi9. BinOCT uses the solution generated by CART as a warm start.

• DL8.5 [3]: DL8.5 is written in C++ and is run as an extension of Python.

• MurTree [13]: MurTree is written in C++ and run as an executable.

• FlowOCT and BenderOCT [2]: Both methods are implemented in Python. MIP models

are solved by Gurobi9.

• GOSDT [21]: GOSDT is written in C++ and run as an executable. GOSDT does not
force hard constraints on depth, but instead applies a sparsity coeﬃcient α to control the
complexity. As α decrease, GOSDT takes longer time to solve an optimal tree. To facilitate a
fair comparison with our algorithm on learning optimal depth-2 (or 3) tree, we test GOSDT
with

α ∈ {0.1, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001},

(63)

and select the smallest α with which GOSDT can learn an optimal tree with depth not
greater than 2 (or 3).

Other works for learning optimal trees (e.g., [1]) that do not show noticeable speed advantages
are not mentioned above. We do not consider the comparison with these algorithms as we focus
on the eﬃciency of solving optimal trees,

In addition to BinOCT, MurTree and DL8.5, we also run OCT, FlowOCT, BenderOCT and
GOSDT on collected classiﬁcation datasets. For FlowOCT, BenderOCT and GOSDT, we convert
original datasets to binary ones using equivalent-conversion described in Section D.1. None of
these methods is able to learn an optimal tree on any of 16 classiﬁcation datasets, so the results
are not displayed.

D.3 Results on regression tasks

We also compare our algorithm with ORT [7] on 11 regression datasets. To our best knowledge,
ORT is the one of the most eﬀective framework reported in the literature for solving optimal
decision trees on regression tasks. The results are displayed in Table 7. Quant-BnB successfully
solves trees of depth 2 in less than 10 seconds on datasets with thousands of instances, and
computes trees of depth 3 in a reasonable time for most cases. In contrast, ORT cannot optimally
solve any example in 4 hours. The experiment again conﬁrms the advantage of Quant-BnB for
solving shallow decision trees on relatively large-scale datasets. The scalability and versatility of
our proposed method contribute to the wide applications of optimal decision trees.

28

Name

(n,p)

carbon
casp
concrete
energy
ﬁsh
gas
grid
news
qsar
query1
query2

(8576,5)
(36584,9)
(824,8)
(15788,28)
(726,6)
(29386,10)
(8000,12)
(31715,59)
(436,8)
(8000,3)
(159874,4)

depth=2

depth=3

Quant-BnB
0.7
4.2
<0.1
14
<0.1
1.5
1.2
349
<0.1
<0.1
9.8

ORT
(20%)
(14%)
(1.2%)
(9.6%)
(4.5%)
(1358%)
(9.3%)
(22%)
(5.1%)
(38%)
(97%)

Quant-BnB
729
7609
125
-
34
421
1293
-
30
34
2896

ORT
(423%)
(12%)
(33%)
(7.6%)
(36%)
(510%)
(31%)
(29%)
(32%)
(73%)
OoM

Table 7: Comparison of Quant-BnB against ORT. For each dataset, the number of observations
and the number of features are provided. Each entry denotes running time in seconds. - refers to
time out (4h), OoM refers to out of memory (25GB). Since ORT times out in all cases, we display
the relative diﬀerences (LO − LQ)/LQ as a percentage instead, where LO and LQ are the training
errors of ORT and Quant-BnB, respectively.

References

[1] Sina Aghaei, Mohammad Javad Azizi, and Phebe Vayanos. Learning optimal and fair decision
trees for non-discriminative decision-making. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 33, pages 1418–1426, 2019.

[2] Sina Aghaei, Andr´es G´omez, and Phebe Vayanos. Strong optimal classiﬁcation trees. arXiv

preprint arXiv:2103.15965, 2021.

[3] Ga¨el Aglin, Siegfried Nijssen, and Pierre Schaus. Learning optimal decision trees using caching
branch-and-bound search. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 34, pages 3146–3153, 2020.

[4] Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia Rudin.
Learning certiﬁably optimal rule lists for categorical data. arXiv preprint arXiv:1704.01701,
2017.

[5] Kristin P Bennett and Jennifer A Blue. Optimal decision trees. Rensselaer Polytechnic

Institute Math Report, 214:24, 1996.

[6] Dimitris Bertsimas and Jack Dunn. Optimal classiﬁcation trees. Machine Learning,

106(7):1039–1082, 2017.

[7] Dimitris Bertsimas and Jack Dunn. Machine learning under a modern optimization lens.

Dynamic Ideas LLC, 2019.

[8] Christian Bessiere, Emmanuel Hebrard, and Barry O’Sullivan. Minimising decision tree size
as combinatorial optimisation. In International Conference on Principles and Practice of
Constraint Programming, pages 173–187. Springer, 2009.

[9] L Breiman, JH Friedman, R Olshen, and CJ Stone. Classiﬁcation and regression trees. 1984.
[10] Miguel A Carreira-Perpin´an and Pooya and Tavallali. Alternating optimization of decision

29

trees, with application to learning sparse oblique trees. Advances in Neural Information
Processing Systems, 31:1211–1221, 2018.

[11] Chaofan Chen and Cynthia Rudin. An optimization approach to learning falling rule lists.
In International Conference on Artiﬁcial Intelligence and Statistics, pages 604–612. PMLR,
2018.

[12] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings
of the 22nd acm sigkdd international conference on knowledge discovery and data mining,
pages 785–794, 2016.

[13] Emir Demirovi´c, Anna Lukina, Emmanuel Hebrard, Jeﬀrey Chan, James Bailey, Christopher
Leckie, Kotagiri Ramamohanarao, and Peter J. Stuckey. Murtree: Optimal decision trees via
dynamic programming and search. Journal of Machine Learning Research, 23(26):1–47, 2022.
[14] David Dobkin, Truxton Fulton, Dimitrios Gunopulos, Simon Kasif, and Steven Salzberg.
Induction of shallow decision trees. IEEE Trans. on Pattern Analysis and Machine Intelligence,
1997.

[15] Dheeru Dua and Casey Graﬀ. UCI machine learning repository, 2017.
[16] Alireza Farhangfar, Russell Greiner, and Martin Zinkevich. A fast way to produce near-
optimal ﬁxed-depth decision trees. In Proceedings of the 10th international symposium on
artiﬁcial intelligence and mathematics (ISAIM-2008), 2008.

[17] Oktay G¨unl¨uk, Jayant Kalagnanam, Minhan Li, Matt Menickelly, and Katya Scheinberg.
Optimal decision trees for categorical data via integer programming. Journal of Global
Optimization, pages 1–28, 2021.

[18] Hao Hu, Mohamed Siala, Emmanuel H´ebrard, and Marie-Jos´e Huguet. Learning optimal
decision trees with maxsat and its integration in adaboost. In IJCAI-PRICAI 2020, 29th
International Joint Conference on Artiﬁcial Intelligence and the 17th Paciﬁc Rim International
Conference on Artiﬁcial Intelligence, 2020.

[19] Xiyang Hu, Cynthia Rudin, and Margo Seltzer. Optimal sparse decision trees. Advances in

Neural Information Processing Systems (NeurIPS), 2019.

[20] Hyaﬁl Laurent and Ronald L Rivest. Constructing optimal binary decision trees is np-complete.

Information processing letters, 5(1):15–17, 1976.

[21] Jimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, and Margo Seltzer. Generalized and
scalable optimal sparse decision trees. In International Conference on Machine Learning,
pages 6150–6160. PMLR, 2020.

[22] Hayden McTavish, Chudi Zhong, Reto Achermann, Ilias Karimalis, Jacques Chen, Cynthia
Rudin, and Margo Seltzer. Fast sparse decision tree optimization via reference ensembles.
2022.

[23] Nina Narodytska, Alexey Ignatiev, Filipe Pereira, Joao Marques-Silva, and IS RAS. Learning

optimal decision trees with sat. In IJCAI, pages 1362–1368, 2018.

[24] Siegfried Nijssen and Elisa Fromont. Mining optimal decision trees from itemset lattices. In
Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and
data mining, pages 530–539, 2007.

[25] Siegfried Nijssen and Elisa Fromont. Optimal constraint-based decision tree induction from

itemset lattices. Data Mining and Knowledge Discovery, 21(1):9–51, 2010.

[26] J. Ross Quinlan. Induction of decision trees. Machine learning, 1(1):81–106, 1986.
[27] Sicco Verwer and Yingqian Zhang. Learning optimal classiﬁcation trees using a binary linear

30

program formulation.
volume 33, pages 1625–1632, 2019.

In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,

[28] Haoran Zhu, Pavankumar Murali, Dzung Phan, Lam Nguyen, and Jayant Kalagnanam. A
scalable mip-based method for learning optimal multivariate decision trees. In H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information
Processing Systems, volume 33, pages 1771–1781. Curran Associates, Inc., 2020.

31

