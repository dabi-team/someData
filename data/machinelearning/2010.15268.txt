Understanding the Pathologies of Approximate Policy Evaluation
when Combined with Greediﬁcation in Reinforcement Learning

Kenny Young 1 Richard S. Sutton 1

Abstract

1. Introduction

0
2
0
2

t
c
O
8
2

]

G
L
.
s
c
[

1
v
8
6
2
5
1
.
0
1
0
2
:
v
i
X
r
a

Despite empirical success, the theory of reinforce-
ment learning (RL) with value function approxi-
mation remains fundamentally incomplete. Prior
work has identiﬁed a variety of pathological be-
haviours that arise in RL algorithms that combine
approximate on-policy evaluation and greediﬁca-
tion. One prominent example is policy oscillation,
wherein an algorithm may cycle indeﬁnitely be-
tween policies, rather than converging to a ﬁxed
point. What is not well understood however is the
quality of the policies in the region of oscillation.
In this paper we present simple examples illus-
trating that in addition to policy oscillation and
multiple ﬁxed points— the same basic issue can
lead to convergence to the worst possible policy
for a given approximation. Such behaviours can
arise when algorithms optimize evaluation accu-
racy weighted by the distribution of states that
occur under the current policy, but greedify based
on the value of states which are rare or nonexis-
tent under this distribution. This means the values
used for greediﬁcation are unreliable and can steer
the policy in undesirable directions. Our observa-
tion that this can lead to the worst possible policy
shows that in a general sense such algorithms are
unreliable. The existence of such examples helps
to narrow the kind of theoretical guarantees that
are possible and the kind of algorithmic ideas that
are likely to be helpful. We demonstrate analyt-
ically and experimentally that such pathological
behaviours can impact a wide range of RL and dy-
namic programming algorithms; such behaviours
can arise both with and without bootstrapping,
and with linear function approximation as well as
with more complex parameterized functions like
neural networks.

1Department of Computing Science, University of Alberta,
Edmonton, Canada. Correspondence to: Kenny Young <kjy-
oung@ualberta.ca>.

Despite signiﬁcant empirical success, value-function based
algorithms for reinforcement learning (RL) control remain
poorly understood theoretically. When states are represented
exactly, Q-learning and Sarsa are known to converge to
the optimal action-value function under reasonable condi-
tions (Jaakkola, Jordan, and Singh, 1994; Singh, Jaakkola,
Littman, and Szepesv´ari, 2000; Tsitsiklis, 1994). Unfortu-
nately, if the action-value or state-value functions are repre-
sented by function approximation, even the relatively simple
case of linear function approximation, then the picture be-
comes less clear and pathological behaviours can result.

Gordon (1996) ﬁrst observed that Sarsa with an (cid:15)-greedy pol-
icy can oscillate indeﬁnitely, failing to converge. De Farias
and Van Roy (2000), generalized this to the dynamic pro-
gramming (DP) setting, by showing that approximate value
iteration (AVI) can also oscillate indeﬁnitely and may not
possess ﬁxed points. Bertsekas (2011) further highlights
that policy oscillation is a general property of a wide class of
DP algorithms which utilize on-policy evaluation combined
with greediﬁcation (see also Bertsekas (2010)).

What is not well understood however is to what extent such
policy oscillation is a problem. For example, Gordon (2001)
demonstrates that though SARSA(0) can oscillate between
policies, the weights nonetheless converge to a bounded re-
gion, but gives little insight into the quality of policies in this
region. Bertsekas (2011) states that “the full ramiﬁcations
of policy oscillation in practice are not fully understood
at present, but it is clear that they give serious reason for
concern”. In this report we demonstrate that the same basic
mechanism that leads to policy oscillations in a wide range
of RL and DP algorithms can also cause convergence to the
worst possible policy. The basic problem is that while the
evaluation accuracy is always optimized for the distribution
of states that arise under the current policy, greediﬁcation
may be based on states which are nonexistent or rare under
this distribution. Thus, the values used for greediﬁcation are
unreliable. In addition to policy oscillation we show that
even in cases where the policy does converge reliably, the
ﬁnal policy can be arbitrarily suboptimal. This can occur
even when the optimal policy is representable in the class
of greedy policies, and the on-policy evaluation error of the

 
 
 
 
 
 
Understanding the Pathologies of Approximate Policy Evaluation when Combined with Greediﬁcation

optimal policy can be made zero.

Our result on convergence to a suboptimal policy is related
to a result by Pendrith and McGarity (1998). They show that
although the optimal representable policy is always a ﬁxed
point of Monte Carlo (MC) policy-gradient, the same is not
true when the policy-gradient is estimated based on a value
function optimized by TD(λ). We highlight that this result
is a consequence a general issue with alternating between
evaluating and greedifying under function approximation
and can occur even if the policy is updated using a ﬁnite
look-ahead on a value function trained on full MC returns.
Thus, the issue is not exclusive to evaluation methods which
bootstrap, like TD(λ) .

The work of Lu, Schuurmans, and Boutilier (2018) is also
closely related. They highlight the inability of Q-learning
and AVI to converge to the optimal representable policy due
to what they refer to as delusional bias. Delusional bias
refers to bootstrapping values based on sequences of actions
which are not mutually realizable under a greedy policy in
the function approximation class. We can reframe this deﬁ-
nition by saying that optimizing approximate action-values
under the current policy can modify the approximated val-
ues of other actions such that, after greedifying, the learned
action-values are no longer optimized for the new policy.
Thus, delusional bias is closely related to the issues we
explore here. We take a somewhat different perspective
on the problem, which enables us to highlight how similar
problems can arises even in the absence of bootstrapping.

2. Background

RL refers to the problem faced by a goal directed agent
interacting with an initially unknown environment. We for-
malize this interaction as a Markov decision process (MDP).
An MDP M is deﬁned by a tuple M = (S, A, p). At each
time t an agent observes a state St ∈ S and based on this
information selects an action At ∈ A. Based on this action
and the current state, the environment then transitions to a
new state St+1 ∈ S, and outputs a reward Rt+1 ∈ R ac-
cording to the dynamics function p(s(cid:48), r|s, a) = P (St+1 =
s(cid:48), Rt+1 = r|St = s, At = a).

The agent’s behaviour is speciﬁed by a policy π(a|s), which
is a distribution over a ∈ A for each s ∈ S. We focus on the
episodic setting, where we assume that for every possible
policy the agent environment interaction eventually reaches
a terminal state from which no more reward is possible. Our
main conclusions can also be generalized to the cases of
discounted return and average reward. The termination time
T is the random time at which the terminal state is reached.

The sum of rewards obtained from some time t until T
T −1
(cid:80)
k=t

Rk+1. The agent’s goal is

is called the return Gt =

to learn, from experience, a policy which maximizes the
total expected return G0. We deﬁne the state-value function
under a particular policy π as vπ(s) = Eπ[Gt|St = s], that
is the expected return from time t given the agent starts in
state s and follows the policy π.

We use µπ(s) to represent the visitation frequency of a
state s under policy π. We refer to µπ(s) as the on-policy
distribution. In the continuing case, µπ(s) would be the
steady-state distribution. In the episodic case we consider,
we take it to be the expected fraction of total time-steps
spent in the state across all episodes, that is:

µπ(s) = Eπ

(cid:35)(cid:44)

1(St = s)

Eπ[T ] ,

(cid:34)T −1
(cid:88)

t=0

where T is the random time at which the terminal state is
reached and expectations are taken over episodes. In both
the episodic and continuing case µπ(s) represents the distri-
bution of states visited by an online RL agent running a ﬁxed
policy π. As such, it is also the distribution implicitly used
in the objective of most standard on-policy RL algorithms.

We will also discuss the DP setting. DP is similar to RL
except that we assume the agent has access to the dynamics
function p. In DP the goal is to compute a good policy
directly from p rather than learning from experience.

3. The Issue with Greediﬁcation based on

Approximate Evaluation

There is a basic issue with algorithms that alternate between
on-policy evaluation with function approximation and greed-
iﬁcation of the policy based on the resulting evaluation. The
issue is that when an approximate value function is opti-
mized to be accurate with respect to a particular on-policy
distribution, the resulting evaluation of states which are
rarely visited under the current policy can be essentially ar-
bitrary depending on the function class. Yet, when we greed-
ify the policy, we frequently rely on the evaluation of such
states. This issue is present even in the absence of bootstrap-
ping and affects a wide class of RL and DP algorithms that
use approximate value functions for policy improvement.
To illustrate how fundamental this issue is, we ﬁrst describe
how it occurs in a DP algorithm, without stochasticity, or
bootstrapping. We refer to this algorithm as approximate
policy iteration (API), its pseudocode is provided in Algo-
rithm 1. API learns an approximate state-value function
ˆv(·; θ) : S → R, which should be seen as a learned approxi-
mation to vπ(s), as an intermediate step to learning a good
policy. API alternates between optimizing ˆv(·; θ) for the
current policy by minimizing (cid:80)
µπ(s) (ˆv(s; θ) − vπ(s))2,
s

and updating the policy to be greedy1 with respect to a one-

1Assume ties are broken in some canonical manner.

Understanding the Pathologies of Approximate Policy Evaluation when Combined with Greediﬁcation

Algorithm 1 Approximate Policy Iteration
1: function API(π0, ˆv)
2:
3:
4:

π ← π0
while Not Converged do

θ ← argmin

(cid:80)
s
π ← Greedy(ˆv, θ)

ˆv(·;θ(cid:48)):θ(cid:48)∈Rn

µπ(s) (ˆv(s; θ(cid:48)) − vπ(s))2

5:
end while
6:
7: end function
8: function Greedy(ˆv, θ)
for s ∈ S do
9:
10:

ˆq(s, a) ˙= Eπ[Rt+1 + ˆv(St+1; θ)|St = s, At = a]
for a = argmax

ˆq(s, a)


1


11:

π(s, a) ←

a(cid:48)

0


otherwise

end for
return π

12:
13:
14: end function

step look-ahead on the current value function approxima-
tion. This seemingly simple algorithm displays a variety of
pathological behaviours depending on the way the function
approximator generalizes.

Our analysis will use the idea of ﬁxed points of API, and
later AVI. By ﬁxed point, we mean an approximate value
function ˆv(·; θ), such that ˆv(s; θ) remains unchanged for all
s after applying the greediﬁciation and evaluation steps of
the algorithm.

This report focuses speciﬁcally on on-policy evaluation.
More precisely, when the evaluation is optimized by mini-
mizing a sum of per-state errors weighted by the on-policy
distribution of the current policy. The issues we highlight
can perhaps be mitigated by focusing on optimizing the eval-
uation with respect to a distribution which is ﬁxed through-
out training (see for example the work of Maei, Szepesv´ari,
Bhatnagar, and Sutton (2010)). However, such methods are
limiting as the achievable performance can be limited by
the ﬁxed behaviour policy used. Optimizing evaluation with
respect to the on-policy distribution allows the behaviour
policy to be incrementally updated as an agent learns from
experience. For this reason, on-policy evaluation is a cru-
cially important case which is worth understanding better.

It is also possible to derive performance guarantees if we as-
sume the function approximator is such that, for any policy,
the value function approximation error for policy-evaluation
is uniformly bounded over all states (see chapter 6.2 of (Bert-
sekas and Tsitsiklis, 1996)). However, as noted by Munos
(2003), most practical algorithms do not allow us to bound
the maximum norm over all states. Munos (2003) also
identiﬁes certain conditions under which meaningful error
bounds are possible in the more realistic case where we
assume a bound on the error under some weighted quadratic

norm. Here, we highlight how pathological behaviours, in-
cluding convergence to the worst possible policy, can arise
even when on-policy evaluation error (the quadratic norm
weighted by the on-policy distribution) can be made zero
for each deterministic policy.

We begin by presenting three intuitive counterexamples us-
ing state-aggregation for function approximation. Note that
state-aggregation is a subclass of linear function approx-
imation. In each example there are two possible actions
A = {l, r} which only impact the state transition in the
initial state, where they take the agent along the left or right
path respectively. The three examples differ only in the
order in which rewards are presented along the two possible
paths. The differing placement of rewards leads to three dif-
ferent behaviours of API by modifying how optimizing the
evaluation accuracy under each of two possible determinis-
tic policies affects the evaluation of the other. We will use
πl and πr to refer to the two possible deterministic policies
based on which of the two actions is selected in the initial
state. The three counterexamples are illustrated in Figure 1.
We will discuss each counterexample in detail to see how it
leads to a certain pathological behaviour.

Oscillating Counterexample

In the ﬁrst counterexample, illustrated in Figure 1a , API
has no ﬁxed point. πl looks superior under the optimal
approximate state-value function for πr and visa-versa. This
leads to API oscillating between the two policies. To see this,
ﬁrst consider the approximate state-value function obtained
by the evaluation step of API (line 4) when the policy is
set to πl. Since µπl (C) = µπl (E) = 0, only the value of
B and D will impact the evaluation error. The true values
are vπ(B) = 0 and vπ(D) = 1. The optimal approximate
state-value function will match these values exactly. Due to
state aliasing, the optimal θ will correspond to ˆv(C; θ) = 1
and ˆv(E; θ) = 0. Now when we greedify the policy at A we
see a value of 1 for the right action and 0 for the left action.
Thus the policy switches to πr in the greediﬁcation step (line
5). Optimizing the state-value function for πr then leads to
ˆv(B; θ) = 3 and ˆv(C; θ) = 2. The next greediﬁcation step
will switch back to πl and so on.

When the policy is initialized to an arbitrary stochastic pol-
icy, greediﬁcation will take it to one of the two deterministic
policies, which one will depend on the relative probability
of the left and right actions. Thereafter, it will oscillate
as described above. Towards making this more precise,
let ρ = π(r|A), ˆv(A; θ) = θ0, ˆv(B; θ) = ˆv(E; θ) = θ1
and ˆv(C; θ) = ˆv(D; θ) = θ2, where θi is the ith ele-
ment of the parameter vector θ. For compactness deﬁne
µπ(s) (ˆv(s; θ(cid:48)) − vπ(s))2. It is straight-
θ(cid:63)
π = argmin

(cid:80)
s

θ(cid:48)

Understanding the Pathologies of Approximate Policy Evaluation when Combined with Greediﬁcation

(a) Oscillating example: API has
no ﬁxed point.

(b) Multiple example: API has
multiple ﬁxed points.

(c) Worst-case example: API con-
verges to the inferior policy.

Figure 1. Counterexamples which lead to pathological behaviours in approximate policy iteration. Circles are states, arrows are determin-
istic transitions associated with particular actions. Numbers next to arrows are rewards. States of the same color are aggregated under the
function approximator. In each example the right action in A is always optimal with a value of Vπr (A) = 2 compared to Vπl (A) = 0.
The examples differ only in the order the rewards are presented along each path, yet exhibit three different pathologies of API.

forward to show that:

θ(cid:63)
π = [ρvπ(C) + (1 − ρ)vπ(B), ρvπ(E)+

(1 − ρ)vπ(B), ρvπ(C) + (1 − ρ)vπ(D)].

(1)

Equation 1 will also hold for the other two counterexamples
to be outlined below. The true values of states B, C, D,
and E are policy independent, replacing each with their
appropriate numerical value in Equation 1 gives us:

θ(cid:63)
π = [2ρ, 3ρ, 1 + ρ] .

From here we can see that starting from a stochastic initial
policy, evaluation followed by greediﬁcation will produce
πl whenever ρ = π(r|A) > 1

2 and πr when ρ < 1
2 .

Multiple Fixed Point Counterexample

In the second counterexample, illustrated in Figure 1b, API
has multiple ﬁxed points. πl looks superior under it’s own
optimal approximate state-value function and πr looks su-
perior under it’s own. Thus, both value functions will be
ﬁxed points. Which ﬁxed point API converges to depends
only on the initial policy.

ˆv(B; θ) = −1 and ˆv(C; θ) = 2, thus again the greediﬁca-
tion step will leave the policy unchanged. Substituting state
values into Equation 1 gives:

θ(cid:63)
π = [2ρ, −ρ, 3ρ − 1] .

Hence, from an arbitrary policy π, evaluation followed by
greediﬁcation will produce πl whenever ρ ≤ 1
4 and πr
otherwise. So, when the policy is initialized arbitrarily,
the ﬁrst greediﬁcation step will take it to one of the two
deterministic policies, and it will remain there forever.

Worst-Case Counterexample

In the ﬁnal counterexample, illustrated in Figure 1c, API has
a single ﬁxed point, but it corresponds to the inferior of the
two deterministic policies. πl looks superior under it’s own
optimal approximate state-value functions as well as that of
πr. However, πr is in fact the optimal policy. Thus, API
will converge to the worse of the two possible deterministic
policies regardless of initialization. More precisely:

θ(cid:63)
π = [2ρ, 3ρ, 3ρ − 1] .

When the state-value function is optimized for πl we have
optimal values of ˆv(B; θ) = 0 and ˆv(C; θ) = −1. Since
ˆv(B; θ) > ˆv(C; θ), πl is the greedy policy and the greedi-
ﬁcation step of API will leave it unchanged. On the other
hand, when the evaluation is optimized for πr we have

So from an arbitrary stochastic policy π, evaluation followed
by greediﬁcation will produce πl whenever 3ρ ≥ 3ρ − 1.
This is always satisﬁed, so API will indeed converge to the
suboptimal policy from any initial policy in a single round
of evaluation and greediﬁcation. This occurs despite the fact

ACBDE00-1+3+1-1ACBDE00+3-1-1+1ACBDE00-1+3-1+1Understanding the Pathologies of Approximate Policy Evaluation when Combined with Greediﬁcation

that the optimal policy is representable as a greedy policy in
the class of approximate value functions, and the on-policy
evaluation error of the optimal policy can be made zero.

4. How General are these Behaviours?

To highlight that the pathologies in section 3 are not unique
to bootstrapping we introduced them within the API algo-
rithm. In this section we explain how these examples lead
to the same behaviours when bootstrapping is used, and in
the RL setting.

We deﬁne the Bellman operator Bπ by Bπv(s) =
Eπ[Rt+1 + v(St+1)|St = s] for every function v : S → R.
We similarly deﬁne the Bellman optimality operator B(cid:63) by
Eπ[Rt+1 + v(St+1)|St = s, At = a]. For
B(cid:63)v(s) = max
any policy π, vπ(s) is the unique solution to the Bellman
equation vπ = Bπvπ.

a

Consider the AVI algorithm in Algorithm 2, which updates
the approximate value function to minimize the error relative
to B(cid:63)ˆv(s; θ). Proposition 1 shows that AVI will behave
similarly to API on each of our counterexamples.
Proposition 1. For a given function approximator ˆv and a
ﬁxed MDP M, let it be the case that for any deterministic
policy π there exists a unique parameter setting θ∗
π such that
(cid:80)
s

π) − vπ(s))2 = 0. Then:

µπ(s) (ˆv(s; θ∗

(i) Every ﬁxed point of API is also a ﬁxed point of AVI.

(ii) If ˆv(s; θ) = θ · x(s) is a linear function of some features
x(s) ∈ Rn of the state s, then every ﬁxed point of AVI is
also a ﬁxed point of API

Proof. (i) Any ﬁxed point ˆv(·; ˜θ) of API must be a mini-
mizer of (cid:80)
µπ(s) (ˆv(s; θ) − vπ(s))2 under the determinis-
s
tic policy π = Greedy(ˆv, ˜θ). By assumption this minimum
error must be zero. The only way this can be true is if
µπ(s) = 0 for all s such that ˆv(s; ˜θ) (cid:54)= vπ(s). Hence:

0 =

=

=

(cid:88)

s
(cid:88)

s
(cid:88)

s

µπ(s) (vπ(s) − Bπvπ(s))2

µπ(s) (vπ(s) − B(cid:63)vπ(s))2

µπ(s)(vπ(s) − B(cid:63)ˆv(s; ˜θ))2,

these steps follow respectively from the Bellman equation,
the fact that π must be the greedy with respect to ˆv(s; ˜θ) at
the API ﬁxed point, and the fact that µπ(s) = 0 for every
s such that ˆv(s; ˜θ) (cid:54)= vπ(s). This means ˆv(·; ˜θ) is a ﬁxed
point of the evaluation step of AVI, since the error cannot
be reduced below zero. The greediﬁcation step will leave π
unchanged as the policy is already greedy. So ˆv(·; ˜θ) is also
a ﬁxed point of AVI.

µπ(s)(ˆv(s; θ(cid:48)) − B(cid:63)ˆv(s; θ))2

Algorithm 2 Approximate Value Iteration
1: function AVI(π0, ˆv, θ0)
π, θ ← π0, θ0
2:
while Not Converged do
3:
4:

θ ← argmin

(cid:80)
s
π ← Greedy(ˆv, θ)

ˆv(·;θ(cid:48)):θ(cid:48)∈Rn

5:
end while
6:
7: end function
8: function Greedy(ˆv, θ)
9:
10:

ˆq(s, a) ˙= Eπ[Rt+1 + ˆv(St+1, θ)|St = s, At = a]
for s ∈ S do


1


for a = argmax

ˆq(s, a)

a(cid:48)

0


otherwise

11:

π(s, a) ←

end for
return π

12:
13:
14: end function

(ii) Any ﬁxed point ˆv(·; ˜θ) of AVI must be a minimizer
of (cid:80)
µπ(s)(ˆv(·; θ) − Bπ ˆv(s; ˜θ))2 under the deterministic
s
policy π = Greedy(ˆv, ˜θ). We have used the fact that π must
be greedy at a ﬁxed point to replace B(cid:63) with Bπ.
Consider a new MDP M(cid:48) identical to M except with state
space S (cid:48) consisting of those states with µ(s) (cid:54)= 0, and all
transitions to states with µ(s) = 0 replaced with transitions
to the terminal state. Notice that vπ(s) for all s ∈ S (cid:48) is the
same for M(cid:48) as it was for M. Now consider the operator Ωπ
µπ(s)(ˆv(s; θ(cid:48))−Bπv)2
deﬁned by Ωπv = argmin

ˆv(·;θ(cid:48)):θ(cid:48)∈Rn

(cid:80)
s∈S (cid:48)

representing the evaluation step of AVI for which we know
ˆv(·; ˜θ) is a ﬁxed point. For linear ˆv(·; θ), Ωπ is a contraction
with respect to the norm (cid:107)v(cid:107) = (cid:80)
µπ(s)v(s)2, with a
s∈S (cid:48)

unique ﬁxed point (see for example Tsitsiklis and Van Roy
(1997)).

Next, note that ˆv(·; θ∗
π) is a ﬁxed point of Ωπ. To see this
π) = vπ(s) for all s ∈ S (cid:48),
note that, by assumption, ˆv(s; θ∗
π) = ˆv(s; θ∗
thus Bπ ˆv(s; θ∗
π). Hence, we can conclude that
ˆv(·; ˜θ) = ˆv(·; θ∗
π) is the unique ﬁxed point of Ωπ. We know
ˆv(·; θ∗
π) minimizes the evaluation step of API. This and the
fact that the policy is greedy, sufﬁces to show that ˆv(·; ˜θ) is
a ﬁxed point of API as well as AVI.

Each of the counterexamples in Figure 1 obey the assump-
tion of Proposition 1, with a linear ˆv. Thus we can conclude
that, for each counterexample, the ﬁxed points of AVI are
the same as those of API. So AVI has no ﬁxed points on the
oscillating counterexample, two ﬁxed points on the multiple
ﬁxed point example, and one ﬁxed point corresponding to
the suboptimal policy on the worst-case counterexample.
Proposition 1 does not preclude AVI possessing additional
cycles that are not possible for API or visa-versa. How-

Understanding the Pathologies of Approximate Policy Evaluation when Combined with Greediﬁcation

ever, for our particular examples, one can indeed show that
AVI will oscillate on the oscillating counterexample and
converge to a ﬁxed point on the other two examples.

One might hope that these issues would be less pronounced
when moving from the full evaluation and greediﬁcation
steps of API and AVI to the more incremental RL setting.
One might suspect that if the policy and value function
are more gradually adapted to each other, we won’t see
such pathological dynamics. However, we will demonstrate
that, for the most part, the general issue illustrated in the
counterexamples of Figure 3 still occur for standard RL
algorithms that make use of approximate evaluation.

It’s worth noting that MC policy-gradient (MC-PG), also
known as REINFORCE (Williams, 1992), does not display
the pathologies we highlight here, though Kakade and Lang-
ford (2002) have demonstrated that related issues with the
on-policy distribution can lead to slow convergence even
in the MC case. MC-PG makes updates according to an
unbiased estimate of the gradient of a well deﬁned objec-
tive, the expected return. MC-PG can have multiple local
minima when using function approximation, but the opti-
mal representable policy will always be the global minima,
and therefore a ﬁxed point. Assuming an appropriately
annealed step-size, MC-PG cannot experience cycles or a
repulsive optimal representable policy. However, there are
many reasons to prefer value based methods, variance reduc-
tion being the most obvious, hence it’s worth understanding
the underlying issues.

To understand why more incremental RL algorithms based
on approximate evaluation and greediﬁcation still lead to
similar pathological behaviour, consider an actor-critic (AC)
algorithm which concurrently trains a linear state-value func-
tion from MC returns, and a stochastic softmax policy which
represents each state independently and estimates the policy-
gradient using a one-step look-ahead on the value function.
The value function in this case is continuously updated to re-
duce a stochastic approximation of the same evaluation error
as the API algorithm. Meanwhile the policy is stochasti-
cally updated to be closer to the greedy policy of the current
value function. Despite updating the policy and value more
gradually than API, this procedure still maintains the same
expected direction of the value and policy update at each
point in parameter space. Thus, it’s easy to see how the
resulting dynamics are qualitatively similar to that of API
on the counterexamples in Figure 1.

One notable behavioural difference arises when using algo-
rithms with a continuous policy class, like the AC algorithm
described above. In particular, it has been shown that the
use of a continuous policy class sufﬁces to guarantee the
existence of ﬁxed points, but not the uniqueness. Thus, on
the oscillating example, algorithms which use a continu-
ous policy class will tend to converge to some intermediate

Figure 2. Pathological behaviours due to approximate evaluation
and greediﬁcation arise for a variety of RL algorithms. Columns
show results for different algorithms, rows show results on different
counterexamples. Each plot displays the result of 30 runs. Average
return is plotted in grey with error bars displaying standard error.
For Q-learning the action-value approximation for the left action
is shown in green, while the action-value approximation of the
right action is shown in red. For the two AC algorithms green
and red instead indicate the approximate state-value of B and C
respectively. For the oscillating counterexamples the solid lines
represent the mean with error bars displaying standard error. The
dotted line shows a single run to clearly illustrate the oscillating
behaviour of Q-learning. For the other two counterexamples we
simply display curves for each of the 30 runs.

stochastic policy rather than oscillating between policies.
See the work of De Farias & Van Roy (2000) for an analysis
of this point under a continuous policy variant of AVI and
the work of Perkins and Pendrith (2002) for an analysis of
Q-learning and Sarsa with continuous policy classes. The
proof in each case relies on Brouwer’s Fixed Point The-
orem (Brouwer, 1912), which states that any continuous
function from a compact convex set onto itself must pos-
sess a ﬁxed point. Continuous policy classes nonetheless
behave similarly on the multiple ﬁxed point and worst-case
counterexamples.

5. State-aggregation Experiments

Instead of analytically assessing the performance of a partic-
ular class of RL algorithms on these problems, we illustrate
the generality of these issues by running three RL algo-
rithms on the proposed counterexamples and empirically
demonstrate that similar behaviours result. The algorithms
tested are Q-learning, AC with a MC critic, and AC with a
TD(0) critic. The results are illustrated in Figure 2.

In these experiments, we formulate each counterexample as
having two actions available in each state, but the selected

Right ValueLeft ValueReturnQ-learningAC with MC criticAC with TD criticOscillatingExampleMultipleExampleWorst-caseExampleElapsed EpisodesUnderstanding the Pathologies of Approximate Policy Evaluation when Combined with Greediﬁcation

action only impacts the transition in state A. We represent
aggregated states by running each algorithm with a tabular
representation where aggregated states are treated as the
same. Hence for Q-learning, we learn 3 sets of 2 action-
values for state A, for state B and E and for state C and
D. For AC we learn 3 state-values along with 3 softmax-
paramaterized policies over 2 actions. A ﬁxed, reasonably
low step-size of 0.05 is used in each case. For Q-learning
we use (cid:15)-greedy exploration with (cid:15) = 0.05. All parameters
are randomly initialized from a unit normal distribution.

The results illustrate how different algorithms interact with
the three counterexamples. Q-learning shows policy and
value oscillation on the oscillating counterexample, while
both AC algorithms converge to some stochastic policy
where the value estimate of each state is approximately
equal. All algorithms show two distinct ﬁxed points on the
multiple ﬁxed point counterexample, though for AC with a
TD critic the bootstrapping seems to lead to somewhat more
chaotic behaviour, where some runs appear to converge to
neither ﬁxed point. All three algorithms consistently con-
verge to the inferior policy in the worst-case counterexample.
Interestingly, Q-learning initially tends toward the superior
policy, before suddenly dropping off as the evaluation is
optimized.

6. Neural Network Experiments

We’ve shown how the interaction of approximate policy
evaluation and greediﬁcation can lead to several pathologi-
cal behaviours with linear function approximation. In this
section we investigate how similar behaviours can arise
with neural network function approximation. It might not
be clear that with a universal function approximator like a
neural network, the issues highlighted here would still be
problematic. Given a network with large enough capacity,
one might hope we will be able to learn the values of every
state to sufﬁcient accuracy that we can approach a good
policy despite using approximate value functions for policy
improvement. However, the case of a network with large
enough capacity is not the only case of interest. As RL is
applied to solve increasingly sophisticated problems, the
worlds our agents act in will likely be much larger than the
agents’ representational capacity, though this is not the case
in many of the domains that are commonly used for evalu-
ation of deep RL algorithms. To give an example: ATARI
2600 games typically occupy 2-4kB (Bellemare, Naddaf,
Veness, and Bowling, 2013), while state of the art agents
can use more than one million parameters (Espeholt, Soyer,
Munos, Simonyan, Mnih, Ward, Doron, Firoiu, Harley, Dun-
ning, et al., 2018), or more than 4000kB assuming single
precision ﬂoats are used.

When the agent lacks sufﬁcient capacity to represent its
whole world, function approximation becomes a real issue,

and it is worth seriously thinking about how algorithms
learn to apply limited function approximation resources.
The basic issue we highlight in this report is not limited
to simple function approximators like state-aggregation or
linear function approximation. If the function class is not
rich enough to represent all state-values accurately, then
there may be a trade-off between evaluation accuracy under
one on-policy distribution and another. Even in the overpa-
rameterized regime, similar effects could impact the quality
of solutions obtained by RL algorithms, since the evalu-
ation is never fully optimized in practice. In these cases,
how the algorithm behaves will depend on how the function
approximator generalizes.

The state representation in Figure 3 illustrates how similar
issues can impact function approximation techniques like
neural networks. This representation involves no explicit
state aliasing, every state has an independent set of represen-
tations. However, the topology of the input space will force
an underparameterized single hidden layer neural network
with a sigmoid activation to face a trade-off similar to the
counterexamples in Figure 1.

Consider applying a single hidden layer neural network with
linear threshold activations with the state representation
in Figure 3. If the network has just 2 hidden units it is
underparameterized in the sense that it is unable to represent
the value of each state independently. Such a network is
limited to separating the input space into 3 intervals, each
of which can have an independent value. This is, however,
enough to exactly evaluate the three states along either one
of the left and right paths. To exactly evaluate the states
along the right path, the network must separate states A,
C and E. Due to the topology of the representation, this
means it must alias E and B. Similarly, to exactly evaluate
the states along the left path it must alias D and C. Thus, the
topology of the input space combined with the limitations
of the function approximator leads to a situation similar to
the examples given in Figure 1.

Unlike the linear threshold function, a differentiable sigmoid
activation can interpolate between states. To mitigate any
advantage gained by doing so, we draw the representation
of states from an interval, rather than a single deterministic
point. To minimize evaluation error along either path the
sigmoid hidden units must be close to ﬂat within each of 2
colored intervals of Figure 3. Thus their behaviour should
approximate that of linear thresholds when evaluation error
is minimized. State A is still represented deterministically
so that it’s evaluation can be learned unambiguously to
simplify interpretation of the results.

We trained Deep Q-network (DQN) agents on analogues
to each of the problems in Figure 3 with the representation
illustrated in Figure 3. We experimented with an overparam-
eterized network with 4 sigmoid units to establish that strong

Understanding the Pathologies of Approximate Policy Evaluation when Combined with Greediﬁcation

Figure 3. A state representation that illustrate how pathological
behaviours due to approximated evaluation and greediﬁcation can
arise even without state aliasing in an underparameterized neural
network. The input representation is one dimensional with the
representation of each state drawn uniformly at random on each
visit, from the region marked in the corresponding color on the line
on the right. In addition to changing the representation from state
aliasing, each of the above examples adds an additional −1 reward
after the ﬁrst state. This was done to make the value of the start
state distinct from the state that follows under a deterministic policy.
This forces the function approximator to represent them separately
to minimize the evaluation error. The other rewards marked with ∗
are identical to those in the associated counterexample in Figure 3.

performance was possible, after which we ran an underpa-
rameterized network with 2 sigmoid units to test whether
we would observe similar behaviour to that illustrated in
Figure 2 for the state-aggregation case.

In these experiments, we use experience replay with a batch
size of 32. We use RMSProp (Tieleman and Hinton, 2012)
for optimization. The step-size was selected from the set
{0.0025 · 2i : i ∈ {0, 1, .., 5}} to maximize performance
after 500,000 episodes with the overparameterized network
in the worst-case example, and the same step-size was
then used for the other problems. We use (cid:15)-greedy explo-
ration with (cid:15) = 0.1. We also add a small amount of L2-
regularization (a constant of 0.0001) to keep the network
parameters from growing inordinately large. All parameters
are randomly initialized from a unit normal distribution.

The results of our DQN experiments, presented in Figure 4
reﬂect some of the expected behaviour, though complicated
by a number of subtleties. With 4 hidden units the agent is
able to converge to good performance on each counterexam-

Figure 4. Pathological behaviours can arise with neural network
function approximation. Columns show results for different num-
bers of hidden layer units, rows show results on different coun-
terexamples. Each plot shows the evolution of the approximated
values of the left and right actions, plotted for each of 30 runs
individually. The average return is shown in grey with error bars
corresponding to standard error.

ple, though convergence was very slow on the worst-case
example. With 2 hidden units we see that the oscillating
example causes the action-values of the two actions to con-
verge close to the same value, consistently causing policy
oscillations and reduced expected return. For the multiple
ﬁxed point example many runs of the underparameterized
agent initially approach an approximate value function cor-
responding to the inferior policy. However, in the long-run,
every run was able to converge to the superior policy. It is
unclear whether this represents a general characteristic of
DQN which allows it to eventually escape from such inferior
ﬁxed points, or simply a quirk of our particular architecture
and problem setting. On the worst-case example, the un-
derparameterized agent usually converged to an evaluation
corresponding to the inferior policy, though 7 out of 30 runs
instead converged to a point where both actions had similar
estimated values, resulting in policy oscillations.

7. Conclusion

This report highlights a fundamental issue with RL methods
that combine on-policy evaluation and greediﬁcation. We
show for the ﬁrst time how this issue can cause convergence
to the worst possible policy, in addition to policy oscillation
and multiple ﬁxed points. By doing so, we hope to help
direct the search for potential solutions to this issue.

Of the three possible behaviours highlighted in the section 3,
we see the worst-case counterexample as the most prob-
lematic. The existence of ﬁxed points can be guaranteed
by using a continuous policy class (De Farias & Van Roy,

ACBDE-1∗∗∗∗-1+0.75+1.25-0.25-1.25-0.75+0.25Right ValueLeft ValueReturn4 UnitsOscillatingExampleMultipleExampleWorst-caseExampleElapsed Episodes2 UnitsUnderstanding the Pathologies of Approximate Policy Evaluation when Combined with Greediﬁcation

2000). Perkins and Precup (2003) further introduce a ver-
sion of API that is guaranteed to converge to a unique ﬁxed
point, albeit under fairly restrictive conditions.

Our results show that even when a unique ﬁxed point exists,
it can correspond to an arbitrarily bad policy. Even when the
optimal policy can be represented and evaluated exactly (in
the sense of on-policy evaluation error), it may be repulsive.
We consider this a problem with algorithms that combine
on-policy approximate policy evaluation and greediﬁcation.
Such a situation cannot occur in MC-PG, but can occur with
both action-value based methods like Q-learning, and AC
methods which use an approximate state-value function to
estimate the policy-gradient.

Despite the issues we highlight here, value-based methods
can be very powerful as a means to reduce the variance
from estimating the policy-gradient from returns alone, and
enabling learning immediately from single transitions. For
these reasons, it is important that we try to understand the
issues with the current standard approaches, and how we
might be able to improve upon them.

Methods for mitigating the issues we highlight could take a
variety of forms. One possibility is to develop better repre-
sentation learning methods which remove the kind of bad
generalization that leads to pathological behaviors. Another,
perhaps orthogonal, approach is to develop algorithms that
are more sensitive to the dynamics between the value func-
tion and policy. By developing both these directions we
can perhaps hope to design algorithms that make the most
out of the current representation, while also improving the
representation over time to further increase performance.

References

Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling,
M. The arcade learning environment: An evalua-
tion platform for general agents. Journal of Artiﬁcial
Intelligence Research, 47:253–279, 2013.

Bertsekas, D. P. Pathologies of temporal difference meth-
ods in approximate dynamic programming. In 49th
IEEE Conference on Decision and Control (CDC),
pp. 3034–3039. IEEE, 2010.

Bertsekas, D. P. Approximate policy iteration: A survey
and some new methods. Journal of Control Theory
and Applications, 9(3):310–335, 2011.

Bertsekas, D. P. and Tsitsiklis, J. N. Neuro-dynamic pro-

gramming. Athena Scientiﬁc, 1996.

Brouwer, L. E. J. ¨Uber abbildung von mannigfaltigkeiten.
Mathematische annalen, 71(1):97–115, 1912.
De Farias, D. P. and Van Roy, B. On the existence of ﬁxed
points for approximate value iteration and temporal-
difference learning. Journal of Optimization theory
and Applications, 105(3):589–608, 2000.

Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V.,
Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning,
I., et al. Impala: Scalable distributed deep-rl with im-
portance weighted actor-learner architectures. arXiv
preprint arXiv:1802.01561, 2018.

Gordon, G. J. Chattering in sarsa (lambda)-a cmu learning

lab internal report. 1996.

Gordon, G. J. Reinforcement learning with function approx-
imation converges to a region. In Advances in neu-
ral information processing systems, pp. 1040–1046,
2001.

Jaakkola, T., Jordan, M. I., and Singh, S. P. Convergence of
stochastic iterative dynamic programming algorithms.
In Advances in neural information processing systems,
pp. 703–710, 1994.

Kakade, S. and Langford, J. Approximately optimal approx-
imate reinforcement learning. In ICML, volume 2, pp.
267–274, 2002.

Lu, T., Schuurmans, D., and Boutilier, C. Non-delusional
q-learning and value-iteration. In Advances in Neu-
ral Information Processing Systems, pp. 9949–9959,
2018.

Maei, H. R., Szepesv´ari, C., Bhatnagar, S., and Sutton,
R. S. Toward off-policy learning control with function
approximation. In ICML, pp. 719–726, 2010.
Munos, R. Error bounds for approximate policy iteration.

In ICML, volume 3, pp. 560–567, 2003.

Pendrith, M. D. and McGarity, M. An analysis of direct
reinforcement learning in non-markovian domains. In
ICML, pp. 421–429, 1998.

Perkins, T. J. and Pendrith, M. D. On the existence of ﬁxed
points for q-learning and sarsa in partially observable
domains. In ICML, pp. 490–497, 2002.

Perkins, T. J. and Precup, D. A convergent form of approxi-
mate policy iteration. In Advances in neural informa-
tion processing systems, pp. 1627–1634, 2003.
Singh, S., Jaakkola, T., Littman, M. L., and Szepesv´ari,
C. Convergence results for single-step on-policy
reinforcement-learning algorithms. Machine learning,
38(3):287–308, 2000.

Tieleman, T. and Hinton, G. Lecture 6.5-RMSProp: Di-
vide the gradient by a running average of its recent
magnitude. Coursera: Neural networks for machine
learning, 4(2):26–31, 2012.

Tsitsiklis, J. N. Asynchronous stochastic approximation and
q-learning. Machine learning, 16(3):185–202, 1994.
Tsitsiklis, J. N. and Van Roy, B. Analysis of temporal-
diffference learning with function approximation. In
Advances in neural information processing systems,
pp. 1075–1081, 1997.

Williams, R. J. Simple statistical gradient-following al-
gorithms for connectionist reinforcement learning.
Machine learning, 8(3-4):229–256, 1992.

