9
1
0
2

p
e
S
0
1

]
L
P
.
s
c
[

1
v
6
7
0
5
0
.
9
0
9
1
:
v
i
X
r
a

Static Analysis for Probabilistic Programs

Ryan Bernstein

September 12, 2019

Abstract

Probabilistic programming is a powerful abstraction for statistical
machine learning. Applying static analysis methods to probabilistic
programs could serve to optimize the learning process, automatically
verify properties of models, and improve the programming interface
for users. This ﬁeld of static analysis for probabilistic programming
(SAPP) is young and unorganized, consisting of a constellation of tech-
niques with various goals and limitations. The primary aim of this work
is to synthesize the major contributions of the SAPP ﬁeld within an
organizing structure and context. We provide technical background
for static analysis and probabilistic programming, suggest a functional
taxonomy for probabilistic programming languages, and analyze the
applicability of major ideas in the SAPP ﬁeld. We conclude that,
while current static analysis techniques for probabilistic programs have
practical limitations, there are a number of future directions with high
potential to improve the state of statistical machine learning.

Contents

1 Introduction

Introduction to Probabilistic Programming . . . . . . . . . . .
1.1
1.2
. . . . . . . . . . . . . . . . .
Introduction to Static Analysis
1.3 Purposes of applying Static Analysis to Probabilistic Pro-
gramming . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.1 Optimization . . . . . . . . . . . . . . . . . . . . . . .
1.3.2 Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . .
1.3.3 Usability . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Technical background

2.1 Posterior inference . . . . . . . . . . . . . . . . . . . . . . . .
2.1.1 Markov-Chain Monte Carlo sampling . . . . . . . . . .

3
3
5

6
7
7
9

9
9
9

1

 
 
 
 
 
 
2.1.2 Variational methods . . . . . . . . . . . . . . . . . . .
2.2 Static analysis . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.1 Type systems . . . . . . . . . . . . . . . . . . . . . . .
2.2.2 Abstract Interpretation . . . . . . . . . . . . . . . . .
2.2.3 Monotone Framework . . . . . . . . . . . . . . . . . .
2.3 Probabilistic Programming from a Static Analysis and Pro-
gramming Languages perspective . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
2.3.1 Denotational Semantics
2.3.2 Operational Semantics . . . . . . . . . . . . . . . . . .

3 Properties of probabilistic programming languages salient

for static analysis
3.1 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.1 Discontinuous density functions . . . . . . . . . . . . .
3.1.2 Predicate query oriented . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
3.1.3
3.1.4 Dynamic parameter declaration . . . . . . . . . . . . .
. . . . . . . . . . . . .

3.2 Properties of representative languages

Sampling-based inference

4 Current techniques

4.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.1 Avoiding work . . . . . . . . . . . . . . . . . . . . . . .
4.2.2
Inference-aware optimization . . . . . . . . . . . . . .
4.2.3 Conclusions . . . . . . . . . . . . . . . . . . . . . . . .
4.3 Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3.1 Conclusions . . . . . . . . . . . . . . . . . . . . . . . .
4.4 Usability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4.1 Compute away restrictions to the interface . . . . . . .
4.4.2 Conclusions . . . . . . . . . . . . . . . . . . . . . . . .

5 Future directions

5.1 Extensions of current work . . . . . . . . . . . . . . . . . . . .
5.1.1 Optimization . . . . . . . . . . . . . . . . . . . . . . .
5.1.2 Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . .
5.1.3 Usability . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
Statistically-aware user feedback . . . . . . . . . . . .
5.2.1
5.2.2 Validation of programs against other abstractions . . .
5.2.3 Higher-level representations via transformation . . . .

5.2 Unexplored directions

2

13
14
14
17
18

21
22
24

26
26
26
27
28
28
30

30
30
30
30
35
38
38
42
42
42
45

45
45
45
46
47
47
48
48
49

5.2.4 Data-scalable and data-absent analyses . . . . . . . . .
5.2.5 Posterior inference failure prediction and diagnosis . .

6 Conclusions

7 Acknowledgements

1

Introduction

49
50

50

51

The idea of statically analyzing the source code of probabilistic programs
is relatively new, because probabilistic programming is relatively new. The
idea has cropped up in the form of a disparate set of approaches that have
been presented with varying purpose, technique and context of application.
Thus far, there has been no systematic study of this ﬁeld of static analysis of
probabilistic programs as a whole. This paper will attempt to ﬁll that role,
with the following goals in mind:

• To give a context and motivation for the application of static analysis
to probabilistic programs, such that the reader may understand and
appreciate the important ideas in the ﬁeld.

• To suggest some organization for the space of the ﬁeld, as a means to

locate ideas within the ﬁeld.

• To present, categorize and discuss some existing ideas that have been

explored in this space.

• To identify places where existing work could be fruitfully extended,
and to suggest directions of potentially high impact for the future.

This paper attempts to cover a representative set of probabilistic program-
ming languages, but uses the popular Stan language [7] as the primary ref-
erence point.

1.1

Introduction to Probabilistic Programming

Probabilistic programming provides a powerful means for specifying and
computing with probabilistic models.

A probabilistic model is a probability distribution over some known vari-
ables, called data or observations, and unknown variables, called model pa-
rameters (or just parameters). Probabilistic models underly statistical anal-
yses and allow the analyst to generalize conclusions about quantities of in-
terest beyond the observed data.

3

Often, the goal of writing a probabilistic model is to ﬁnd the posterior
probability distribution, also called the posterior distribution. The posterior
distribution is the probability distribution over the parameters conditioned
on the data, representing our updated beliefs about the model parameters
given the new information found in the data.

A probabilistic program expresses a probabilistic model by deﬁning a
joint distribution. A joint distribution maps each possible pair of intantia-
tions of data and parameters to a positive real number which corresponds to
how "likely" that pair is to occur according to the model. This mapping is
called the density function of the distribution1. When a distribution’s den-
sity sums (or integrates) to one, it is called a normalized distribution or a
probability distribution. The joint distribution represented by a probabilistic
program does not need to be normalized.

It is very convenient and ﬂexible to write a probabilistic model as an

unnormalized joint distribution. One can:

• Describe a process for generating the data in terms of the parameters,
along with a distribution representing one’s prior beliefs about the pa-
rameters. This process corresponds to constructing a joint distribution
by multiplying a likelihood and a prior.

• Simply specify a joint distribution directly as a function of the data

and model parameters.

When the data are plugged into the (potentially unnormalized) joint
distribution, we get an unnormalized distribution which is proportional to
the posterior probability distribution. This is a corollary of Bayes’ rule.

This task of recovering the posterior probability distribution from a pro-
portional unnormalized distribution is called posterior inference, and it is
the core computational challenge of probabilistic programming (and much
of computational Bayesian statistics). Unfortunately, exact posterior infer-
ence is computationally intractable for most real-world problems, since it
involves integrating over the whole space of parameters. We usually rely
instead on approximate posterior inference methods. Some common approx-
imate methods are described in section 2.1.

1Strictly speaking, the density function is a ratio between a distribution and a base
measure, and some distributions do not have densities. Except when we deal explicitly in
terms of measure theory, we will assume that every distribution has a density with respect
to the Lebesgue measure. In that case, it is reasonable to think of the density as being a
real number proportional to the probability.

4

The advantages of probabilistic programs are enabled by the separation
of concerns between the program, which is only concerned with represent-
ing probabilistic models, and the posterior inference engine, which is only
concerned with calculating the posterior. This separation has a number of
advantages over typical practices in applied statistics and machine learning:

• Probabilistic programs are freed of details of inference, letting users
operate at a high level of abstraction. This makes models easier to
write, debug, and iterate on. It also enables users to write programs
which clearly communicate their domain knowledge.

• Improvements to posterior inference algorithms propagate for free to

every probabilistic program, just by rerunning the model.

• As we will see, since probabilistic programs are written in standardized,
computer-readable, and semantically meaningful format, we can apply
static analysis for a number of additional beneﬁts.

1.2

Introduction to Static Analysis

The term "static analysis" refers to a set of techniques for analyzing the
source code of a program before it is run. The strategy is to enable as much
work as possible at the point of compiling a program in order to gain some
beneﬁt every time the program is run.

Static analysis is pervasive in software engineering. For example, some

common uses of static analysis are:

• When a C program is made faster by GCC’s -O3 option, shaving sec-
onds oﬀ the execution of a program that will be run a million times
[25].

• When NASA engineers prove that the Mars rover’s piloting software
will never divide by zero, ensuring the safety of a mission critical system
[6].

• When a programmer’s code editor points out a mistake and suggests

corrections in real time, saving hours of debugging time [27].

We will later see that these three examples are representative of three general
purposes for static analysis: optimization, veriﬁcation, and usability.

Approaches to static analysis vary, but one unifying theme is that they
work by viewing a program as a syntax tree. Syntax is the set of grammat-
ical rules that characterizes valid programs in a language, and allows each

5

Figure 1: On the left is shown the list of grammatical rules for writing
(a limited subset of) expressions in Stan. Expressions are pieces of the
language which evaluate to values, such as numbers or strings. On the right
is shown the list of rules for writing (a limited subset of) statements in Stan.
Statements are pieces that take some imperative action, such as setting the
value of a variable or running other statements in a loop. Not included
here are the rules for writing a whole Stan program, which would describe
the various blocks that are allowed, and how the blocks are made up of
statements. The rules are read as follows: to generate a value of the type
on the left hand side of the ::=, such as an E, pick one of the instances on
the right hand side (separated here by lines), such as c. This means that a
constant such as 1 is a valid Stan expression. Figure taken from [19].

program to be structured into a tree with the whole program as the root and
individual tokens as the leaves. For example, Figure 1 shows a subset of the
expression and statement syntax of Stan.

These motivations for static analysis are easily extended to probabilistic
programming.
Inferring the posterior of a model can take a lot of com-
puting time and power, and trusting that a model is correct can be expen-
sive and/or risky. While typical programming can be said to have compile-
time, run-time, and test-time phases, probabilistic programming can be said
to have analogous compile-time, inference-time, and model checking-time
phases. Each of these points could potentially beneﬁt from static analysis
using the new information available at each step.

1.3 Purposes of applying Static Analysis to Probabilistic

Programming

Static analysis can be applied to probabilistic programs for a variety of rea-
sons.

6

1.3.1 Optimization

The goal of optimization is, broadly, to get more done with fewer resources.
While optimization of traditional programs is done with respect to time
or memory for a single execution of a program, optimization of probabilis-
tic programs can consider the entire process of posterior inference, perhaps
with respect to the average time or memory it takes to converge on a pos-
terior distribution. This usually includes the traditional optimization of a
program as a subproblem, but also considers how the model can be analyzed
or transformed to better suit it for the particular inference method in use.

A second diﬀerence from optimization of traditional programs comes from
the diﬀerence in the program semantics that need to be preserved. In tradi-
tional programming, the optimized program needs to have the same behavior
when given the same inputs as the original. In probabilistic programming,
the program needs to represent the same posterior distribution as the origi-
nal. This diﬀerence leads to additional avenues for optimizing probabilistic
programs.

The performance of probabilistic program inference is an important con-
sideration. The performance advancements of the No-U-Turn Sampler al-
gorithm [20], for example, have led to a signiﬁcant number of practitioners
turning to probabilistic programming for their probabilistic modeling tasks.
The performance gains of variational inference, although often coming at
a large cost of precision, have further expanded probabilistic programming
into more applications. Fast model-to-posterior times directly enable users
to complete tasks that are otherwise infeasible, and lower barriers to users
expressing models they believe are most correct. If we could perfectly opti-
mize probabilistic programs, we could:

• Avoid all computation which is unnecessary, whether because it is ir-

relevant, overprecise, or redundant.

• Appropriately parallelize computation for hardware such as CPUs,

GPUs and TPUs.

• Make all useful information available to the inference engine.

1.3.2 Veriﬁcation

Software veriﬁcation is the task of proving properties of programs. Examples
of some typical types of properties are:

• A variable will never be negative;

7

• A loop will terminate;

• Some behavior is insensitive to perturbation in the program’s input.

Properties can be proven automatically by a variety of techniques, includ-
ing by type systems or abstract interpretation, which are discussed in the
technical background section.

When we extend veriﬁcation to probabilistic programs, we can use the
usual deterministic reasoning, but we can also extend both the properties
and the proofs to include probabilistic reasoning. We can include random
variables in the property, such as querying whether the expected value of a
random variable will always be positive. We could also consider the prob-
ability of the truth of a property, such as asking whether a safety property
holds with suﬃcient probability.

There are a number of types of errors in probabilistic programming that

we might want to catch using veriﬁcation.

• Depending on the probabilistic programming language, there might
be any of the usual programming errors:
inﬁnite loops, null pointer
references, buﬀer overﬂows and so on. This type of error can be caught
by implementing the techniques of standard compilers.

• There might be an accidental mismatch between the program and the
probabilistic model the user intended to represent. Software engineers
call this a logic error. This type of error might be caught by warning
the user of potential common errors or by checking the program against
additional assertions supplied by the user.

• Even if a program perfectly represents the intended probabilistic model,
the resulting posterior may still be incorrect if the model could not be
inferred using the chosen inference method. Catching this type of error
would be speciﬁc to the inference method in use.

Any progress toward catching these classes of error could automatically im-
prove the modeling process for users en masse.

The ability in software engineering to automatically verify aspects of
programs has become a vital safety feature for a variety of industries, such as
healthcare, ﬁnance, and aerospace engineering. Probabilistic programming
is in a unique position among machine learning and statistics tools to apply
an analog of these automated proofs in data science. Given the calls for
trustworthiness in machine learning and for robust statistical standards in
science, any practical results in this space could have signiﬁcant impact.

8

1.3.3 Usability

Static analysis can also be used to improve the human interface to proba-
bilistic programming. In software engineering, static analysis is used to au-
tomatically generate, transform, and give immediate feedback on programs,
and makes writing good programs easier and more accessible. This ease can
certainly be translated to probabilistic programming. The main diﬀerence
will be that interface improvements could potentially ease the entire model-
ing, inference and model criticism task, of which the programming task is a
subset.

Advances in usability correspond to broadening the availability of proba-
bilistic programming to a wider audience. It could also decrease the mental
overhead of all users, allowing them to spend more resources on the quality
of their models. Usability changes could also improve the interpretability of
programs, which increases their usefulness as communication devices.

2 Technical background

2.1 Posterior inference

Posterior inference algorithms take in a probabilistic program and observa-
tions of the known variables 2, and produce a representation of the posterior
distribution over the model parameters.

There are at least two popular ﬂavors of approximate inference: sampling-

based Markov chain Monte Carlo methods and variational methods.

2.1.1 Markov-Chain Monte Carlo sampling

In Markov chain Monte Carlo (MCMC) sampling methods, the goal is to
draw random samples from the normalized posterior distribution by using
some possibly unnormalized representation of the posterior. Sampling meth-
ods do not attempt to approximate the posterior as any closed-form function.
MCMC is the only practical method available that produces an unbiased es-
timate of complicated probabilistic models.

If it is run forever, an MCMC algorithm will produce an unbiased esti-
mate of the posterior distribution if the distribution is well-behaved. Some-

2Some literature assumes that the data is embedded inside the probabilistic program,
and so posterior inference would not need the data to be supplied separately. This paper
often treats them as being supplied separately for clarity and to agree with the usage of
languages like Stan. The two options are equivalent except when the data is needed for a
compile-time static analysis.

9

times distributions have disconnected regions, and so the process fails to
discover and sample from some regions. Additionally, MCMC algorithms
take some time to explore the space - initial samples will be highly corre-
lated with the initial sample and will not represent the distribution well.
Because of this, the sampling process is usually allowed to run for a certain
amount of time called a warm up period before the samples are recorded.

Metropolis-Hastings. Metropolis-Hastings (MH) algorithms are MCMC al-
gorithms which have a particular method of sampling. The idea is to draw
from a distribution called the proposal distribution, and then to either accept
or discard that draw according to a certain probability called the acceptance
probability [10]. The acceptance probability is a function of the unnormalized
distribution that is carefully chosen to accept the appropriate distribution of
draws according to the posterior distribution. Draws which move to higher
density are more likely to be accepted. The proposal distribution at each
step is a function of the previously accepted draw, making the sampling pro-
cess into a Markov chain that explores the parameter space. There are many
variants of MH, which diﬀer mainly in their choice of proposal distribution.
In practice, MH with naive proposal distributions (such as Gaussian
distributions centered at the last draw) can be very slow to explore high-
dimensional spaces or spaces of diﬃcult shapes, while more suﬃsticated vari-
ants like Hamiltonian Monte Carlo can achieve better performance.

Hamiltonian Monte Carlo and the No-U-Turn Sampler. Hamiltonian Monte
Carlo (HMC) is a variant of MH that is particularly eﬀective at quickly
exploring diﬃcult, high-dimensional distributions.

HMC works by using a connection between physics and probability the-
ory:
If a particle with random momentum is moving around a space, a
snapshot of the particle’s location will be distributed according to a prob-
ability distribution that is related to the potential energy function of the
space. So, to draw a random sample from a probability distribution, we set
up a space with the appropriate potential energy function, we simulate a
particle moving and sample the particle’s location. This ﬁts into the MCMC
framework: the proposal distribution for the next sample is deﬁned by the
physical simulation from the last accepted sample.

The No-U-Turn Sampler (NUTS) (Figure 2) is an extension of HMC
that automatically and dynamically selects runtime parameters of the HMC
process, such as the length of each simulation between samples, to avoid
ineﬃcient particle movements.

HMC/NUTS is remarkably fast and accurate for high-dimensional prob-

10

Figure 2: This shows the steps of a trajectory of a particle simulated by
NUTS for an underlying posterior distribution whose contour is shown in
blue. The axes represent two arbitrary dimensions of the same scale. The
trajectory is terminated approximately when the particle would start moving
back on itself. NUTS samples eﬃciently from the eccentric ellipse-shaped
distribution. Figure taken from [20].

11

Figure 3: Shown here are the Markov chain trajectories for two diﬀerent
MCMC variants given the same joint distribution (whose elliptical contour
is shown as dotted lines). The axes represent two arbitrary dimensions of the
same scale. Successive samples are shown as dots connected by lines. The
random-walk Metropolis algorithm has a Gaussian proposal distribution at
each step, where the Hamiltonian Monte Carlo moves each step according
to a particle simulated under Hamiltonian dynamics after an initial random
momentum. The posterior is a diﬃcult shape for the random-walk algorithm
to traverse, but the gradient-informed HMC easily takes steps across the
space. Figure taken from [14].

12

lems, and is one of the standard workhorses in probabilistic programming.
It tends to explore diﬃcult and high-dimensional distributions more reliably
than simpler variants of MH (Figure 3).

MCMC algorithms are approximate and stochastic, and as a such it can
be diﬃcult to assess the correctness of their posterior estimates. There are a
number of standard statistical tools for diagnosing problems, such as poste-
rior predictive checks, which check the posterior samples against the available
data [16], and Simulation-Based Calibration, which simulates data using the
generative distribution and checks the quality of the inferred posterior distri-
butions against the simulated parameters [35]. There is also exist diagnostic
metrics speciﬁcally for measuring the convergence of MCMC algorithms [37].
In order to work, HMC/NUTS requires the gradient of the density func-
tion. This means that probabilistic programming languages which support
HMC/NUTS usually implement automatic diﬀerentiation in their backend.
It also means that those languages are restricted to diﬀerentiable density
functions, which restricts the language semantics.

Other MCMC sampling algorithms require diﬀerent structure from the
density, such as the Hessian matrix at each point. Eﬃcient Gibbs sampling
requires a local statistical dependency set for each parameter called a Markov
blanket, which can either be gained from language design or by static or
dynamic analysis [38].

2.1.2 Variational methods

Variational inference (VI) attempts to ﬁnd an approximation to the pos-
terior distribution within a restricted solution space of distributions called
a variational family. Parameters in the variational family are optimized
to minimize a function called the Evidence Lower-Bound, which is a com-
putationally tractable way of minimizing the KL-divergence, a measure of
dissimilarity between the true posterior distributions and the approximation
from the variational family. Figure 4 shows example results of VI.

VI requires some variational family to be speciﬁed, and its accuracy can
depend heavily on the choice of family. VI will not converge to the true
posterior if it is not included in the family, and the resulting approxima-
tion will be biased depending on the exact deﬁnition of KL-divergence used.
However, VI often converges faster than sampling methods, and sometimes
scales better for large datasets in practice [3].

Like MCMC algorithms, VI is an approximate method, and so assessing
its correctness is also challenging. The standard posterior diagnostic tools
such as posterior predictive checks and Simulation-Based Calibration also

13

Figure 4: Shown here are the results of applying two variants of variational
inference (red) to an underlying bimodal posterior distribution (black). (a)
should VI with KL(P||Q), which tends to give an estimate more spread out
than the underlying distribution. (b) and (c) show two possible result of
VI with KL(Q||P), which tends to give more peaked estimates than the
underlying distribution.

apply to VI. In addition, since VI is a (typically non-convex) optimization,
converging to local optima is also a concern.

Some ﬂavors of VI require ﬁrst- or second- derivatives over the density,
and so are subject to the same restrictions as MCMC algorithms which
require the same.

2.2 Static analysis

A wide variety of techniques are available for static analysis. Exploring this
toolkit helps us understand the space of possibilities that can be applied in
a probabilistic setting. This section brieﬂy covers three topics that come up
repeatedly in the current approaches for probabilistic programs, and which
will likely be important for future advancements.

2.2.1 Type systems

Types are descriptions associated with components of a program’s syntax
tree. An component’s type describes properties of the value of the expression.
For example, int and ﬂoat are common names for types which correspond to
integral and ﬂoating-point values.

Types are useful for restricting the set of programs that a compiler will
allow. The compiler has a set of rules, called typing rules, which enumerate
the ways that typed expressions and statements are allowed to ﬁt together,

14

Figure 5: Shown are the types allowed by an example probabilistic program-
ming language [34]. In English, it says: "Types named A or B will be one
of: a real number R, a probability distribution over another type A, the unit
value (which is only ever one value), a pair of two values of types A and
B, or a type that can be any of the types Ai for i P I." Eﬀectively, this
language allows real numbers, distributions, a singleton type, and any pair
or disjunction of types. Figure taken from [34].

and what their combined type is if they do ﬁt together. If the compiler ﬁnds
a point in a program which does not match any rule, the compiler rejects the
program, and the program is said not to have type checked. For example,
most compilers will reject statements like int x “ 0.5; because they do not
have a rule allowing the assignment of 0.5 to a variable of integer type.

Figure 5 shows the set of types allowed in the example probabilistic
programming language from Staton [34], and Figure 6 shows the set of typing
rules from the same language. These rules are suﬃcient to check if a program
written in this language will type check and to infer the types of each part
of the program.

If a program type checks, the programmer can be reasonably sure that
the program is free of some class of bugs. Most compilers can promise that,
for example, there are no ﬂoats masquerading as integers and each function
is called with compatible parameters. The programmer can earn more peace
of mind from more powerful type systems, such as ones that can express if a
value falls within a certain range, or is a vector of a certain length. As type
systems become more powerful, they can encode more of the speciﬁcation of
the program, and any program that successfully type checks is more likely
to be correct. Encoding type systems so that they reject programs with
important classes of bugs is a rich ﬁeld of study.

If a program passes a type checker, the compiler has eﬀectively proven
something about the program. Writing type annotations into the program is
akin to writing a proof about the program that can be checked automatically.
Some compilers can infer some or all of the types of expressions based only on
their usage - this is akin to the compiler automatically writing and checking
a proof for the user. Compilers with advanced type systems often employ
software such as Satisﬁability Modulo Theory (SMT) solvers as workhorses
for automated theorem proving in this manner.

15

Figure 6: Shown are type rules suﬃcient to type-check the example proba-
bilistic programming language [34]. A rule asserts that the statements over
the line imply the statements below the line. Γ represents the context avail-
able from the rest of the program. Γ $x t : A means "the context implies
that an expression t has type A". The subscript x is used to distinguish be-
tween the implication of deterministic expressions with $d and probabilistic
expressions with $p. Non-probabilistic programs would only have $d, and
would not include rules for sample, score or normalize. (a) shows a non-
probabilistic example, which intuitively reads: "For some expression t with
type A from the context Γ, f ptq is of type B in the context Γ for some
measurable function f : A´ ą B." (b) shows the type rule for a sample
statement, which reads "For some expression t which is a distribution over
A, sampleptq is a probabilistic expression of type A in the context Γ. (c)
shows the type rule for score, which reads "For some real number-valued
expression t in the context Γ, scoreptq is a probablilistic expression of which
can only return one value (the unit value)." The other type rules deﬁne the
rest of the type system in this way. The semantics of this language are
discussed in a later section. Figure taken from [34].

16

Figure 7: This is adapted from the original presentation of consistency from
the 1977 Abstract Interpretation paper by Cousot & Cousot [13]. α is the
abstracting function, γ is the concretization function, Int is some program
with maps inputs (subscripts I) to outputs (subscripts O). A line over a
variable denotes that it is an abstracted version (the top part is abstracted).
The ě sign denotes consistency, because there can be more than one con-
cretization of an abstraction. An abstract interpretation is consistent if the
result of ﬂowing through the top, abstract path of the diagram produces
a consistent result with ﬂowing through the bottom, concrete part of the
diagram. This is akin to a commutative diagram.

Types can also be useful tools for compilers to take into account in order

to generate eﬃcient code.

2.2.2 Abstract Interpretation

Abstract Interpretation (AI) is a foundational and general framework for
reasoning about properties of programs and program parts. It provides a
vocabulary and some useful mathematical tools. Some work has already
been done to extend it into probabilistic settings.

In AI, the full meaning of a program is called the concrete semantics.
The concrete semantics captures the whole behavior of a program when it is

17

executed. The concrete domain is the space of possible concrete semantics
for programs.

An abstract domain is a simpliﬁcation of the concrete domain that only
captures some interesting aspect of the computation. The function that
maps from a concrete meaning to an abstract meaning is called an abstract-
ing function, and the reverse is called the concretization function. Given
an abstracting function, a concrete program with concrete semantics can
be transformed into an abstract program with abstract semantics. The ab-
stract program does away with all details from the concrete program that do
not aﬀect the abstract domain. This abstract program is called an abstract
interpretation of the concrete program.

An abstraction interpretation is called consistent if, for each possible
input, the output of the abstract program is consistent with the output of
the concrete program. A more formal statement is given in Figure 7.

For example ([13]), consider a program in an arithmetic language: ´1515˚
17, and suppose we care only about the sign of the result. We can use the
abstraction of signs: the concrete domain of integers is abstracted into the
abstract domain of signs. The abstract program is then ´p`q ˚ p`q, where
˚ is redeﬁned to work on signs, and it is now easy to compute that the out-
come will be p´q. This abstract interpretation is consistent since, for all
such arithmetic programs, the abstract result will have the sign of concrete
result.

This style of reasoning can be extended to reason about sophisticated
properties of programs, and serves as the foundation for other, more speciﬁc
approaches.

2.2.3 Monotone Framework

The Monotone Framework is a framework for applying the idea of abstract
interpretation to programs in a mechanical way. Most of the standard set of
program analyses ﬁt nicely into this framework.

The Monotone Framework [30] is a process for ﬁnding abstract properties
of the program at each point in its execution. The framework requires the
following inputs:

• A function from a syntactic element (usually a statement) to a trans-
formation of the abstract property. This is also called the transfer
function. The transfer function is usually described in terms of the
gen function, which describes what is added to a property, and the
kill function, which describes what is removed from a property.

18

Figure 8: This shows how the Monotone Framework to join two branches
of the control ﬂow graph (such as after an if ´ then ´ else statement) for
some general abstract property. The transfer function is written in terms of
genplq and killplq, which denote the additions and removals of items from
the abstract property set at the syntax element l. The A‚plq shows the
abstract property after executing the element l, and the A˝plq shows the
abstract property before the execution of the element l as the combination
of the output A‚s from previous nodes in the control ﬂow graph. When this
whole graph is drawn, the ﬁnal A‚ represents the ﬁnal value of the abstract
property for the whole program.

• A way of combining abstract properties from diﬀerent program branches
of control ﬂow (such as an if statement’s then and else branches). This
is pictured in Figure 8.

• The syntax tree of a program.

• The control ﬂow graph over the program, which speciﬁes the branching

structure of the statements.

The framework then produces a safe over- or under- approximation of the
program property that holds at each point in the execution of the program,
using the so-called Minimal Fixed Point Algorithm.

For example, consider the abstract property called the Reaching Deﬁni-
tions (RD) set. A reaching deﬁnition at a point is a deﬁnition (e.g. x “ 5)
which has not been overwritten up to this point in execution. The RD set is
a set containing each reaching deﬁnition. For some programming language,
we could describe how the syntax generates reaching deﬁnitions by changing
a variable deﬁnition (the gen function) or removes reaching deﬁnitions by
overwriting a variable deﬁnition (the kill function). We could deﬁne the
way that the RD property is joined together across branches as taking a
set union. We could then use the Monotone Framework to ﬁnd the RD set

19

Shown here are examples of the inputs that produce each of
Figure 9:
four common (and very useful) classical static analyses. L is the type of
the property. \ is the way that properties are rejoined after control-ﬂow
branches. Ď and K are the ordering relation and bottom element which
deﬁne a lattice over properties, which is necessary for the Minimal Fixed
Point Algorithm. fl shows the transfer function in terms of gen and kill
functions.

20

at each point in a given program. The RD property is a prerequisite for
building a dependence analysis graph.

Figure 9 shows similar Monotone Framework inputs for three other classic

analyses.

2.3 Probabilistic Programming from a Static Analysis and

Programming Languages perspective

In order to apply the rich theory of programming languages to probabilistic
programs, we should ﬁrst answer the fundamental question: what does a
probabilistic program mean? When we type a probabilistic program into
a text ﬁle, we know it represents an unnormalized probability distribution
- but how exactly are the symbols that make up a program mapped to
the mathematical object of a distribution? If we can answer this question
with suﬃcient rigor, we will have a good foundation to apply existing static
analysis tools to probabilistic programs.

A formal language semantics is a mapping from each valid program to

its meaning. The space of all meanings is called the semantic domain.

This level of rigor allows us to reason with mathematical rigor about a

number of questions about correctness in probabilistic programming:

• Language correctness: is the language speciﬁcation complete and cor-

rect?

• Correctness of posterior inference: will a posterior inference method
given a program produce results consistent with the semantics of that
program?

• Program transformation: When a transformation (such as a compiler
optimization) is applied to some program A resulting in program B,
will the semantics of A match the semantics of B?

• Program veriﬁcation: What property can we prove about a program

A that implies a property of the semantics of A?

For static analysis, we are especially interested in the help that formal
semantics gives us with program transformation and veriﬁcation. Keeping
this formal foundation in mind helps to develop and prove correctness of
static analyses.

There are two primary approaches3 that have been used most often to
build formal semantics for probabilistic programming languages: denota-

3Axiomatic semantics would be a third example.

21

tional semantics and operational semantics. The following sections introduce
them and give an example language and semantics for each.

2.3.1 Denotational Semantics

A denotational semantics assigns a mathematical object to each component
of the program, such that the meaning of the program is the composition
of the meanings of its components. A denotational semantics is deﬁned by
writing
“ V for each element S of the syntax, where V is the semantic
value of the element in the semantic domain. Denotational semantics is most
nature to deﬁne for expressions as opposed to imperative statements, and
works especially well for languages where everything is an expression with a
value.

S
(cid:74)

(cid:75)

1. Example: A measure theoretic semantics for a sample/score language

An example of an expression-only probabilistic programming language
is given in [34], whose type system is shown in Figure 5 in section 2.2.1.

Staton [34] gives a denotational semantics for this language is given in
terms of measure theory:

• A type A is interpreted in the semantics (written

) as a mea-
(cid:75)
surable space. A measurable space is a set along with a σ-algebra
ΣA, which is a set of subsets of the space that can be interpreted
as the set of subsets that can be formally assigned probabilities,
called events.

A
(cid:74)

• A deterministically-valued expression which depends on some sur-
rounding context is a measurable function from the context to the
expression type. A measurable function is a well-behaved func-
tion such that the preimage of a measurable set is also measurable.
. The context Γ is the
This can be written:
Γ
:
(cid:74)
(cid:75)
(cid:74)
set of variables bound in the surrounding program. A is the type
of the expression E.

Γ $d E
(cid:74)

A
(cid:74)

Ñ

(cid:75)

• A probabilistically-valued expression which depends on some sur-
rounding context is a measurable kernel from the context to the
expression type. A measurable kernel is a mapping from the con-
text to a measure on the output. This is written:
:
(cid:74)
Γ
(cid:75)
(cid:74)

, which is equivalent to
(cid:75)

• Since a whole program in this language is a probabilistic expres-
sion without a surrounding context, the whole program is a mea-
sure on the model variables.

Γ $p E
(cid:74)

Ñ r0, 8s.

A
(cid:74)

Γ
(cid:74)

ù

A
(cid:74)

Ś

Σ

(cid:75)

(cid:75)

22

(cid:75)

(cid:75)

Σ

“

Ś

ù

Γ
(cid:74)

A
(cid:74)

Γ
(cid:75)
(cid:74)

Figure 10: This is a standard way of representing the semantics of a language:
each element of the syntax is mapped, by the semantic bracket function, to
an object in the semantic domain on the right hand side. In this case, the
semantic domain is a probability kernel
Ñ r0, 8s.
Since the inputs (the program context γ and the event U ) are supplied as
subscripts on the left hand side, the right hand side is in r0, 8s. (a) shows
the semantics of sample, and is read as "The density of sampleptq with
context γ and event U is the meaning of t with context γ evaluated at
U ." Intuitively, the likelihood of drawing U from sampleptq is tpU q, since t
must be a distribution (from the typing rule marked (b) in Figure 6). (b)
shows the semantics of score, which is read as "The density of scoreptq with
context γ and event U is the value of t in the context γ if there is an event
U , otherwise 0." Intuitively, scoreptq has density t if it is evaluated to its
return type pq. The other statements deﬁne the rest of the semantics in a
similar way.

A
(cid:74)

(cid:75)

23

Figure 11: Shown here is the syntax of the untyped probabilistic lambda
calculus in the same style as Figure 1. The Score and Random features
are akin to the score and sample functions from the denotational semantics
example language in Figure 10.

The full denotational semantics for this language is shown in Figure 10.

If we repeatedly apply these rules, we reduce a program from a syn-
tactic tree to a mathematical object - in this case, a measure on the
model variables.

2. Example: Probabilistic graphical models as a semantic domain

Since Infer.NET programs correspond directly to directed, acyclic prob-
abilistic graphical models, it would be reasonable to deﬁne a denota-
tional semantics for Infer.NET with directed probabilistic graphical
models as the semantic domain. Drawing a variable from a distribu-
tion would correspond to a node with edges to the nodes representing
the distribution parameters. An example Infer.NET program is shown
in Figure 15.

2.3.2 Operational Semantics

An operational semantics assigns each component of the program to a math-
ematical object, but also describes how the component operates on some
global meaning of the program. Operational semantics tend to be conve-
nient to deﬁne for statements which can perform side-eﬀects that change the
meaning of the program, rather than just representing a value.

1. Example: Trace sampling semantics for a probabilistic lambda calculus

Borgström et al.
[4] deﬁne and provide an operational semantics for
a probabilistic programming language called the untyped probabilistic
lambda calculus. The syntax for the language is shown in Figure 11.
Roughly speaking, a lambda calculus is a simple language centered

24

Figure 12: Shown here are rules for a call-by-value operational semantics for
the untyped probabilistic lambda calculus. Rules are written in a similar
style to typing rules, where the statements over the line imply the statement
under the line. The A óC
D B syntax is read as "A evaluates to B, adding C
to the record of draws with density D". (a) shows the rule for score, which
reads "For some constant c P p0, 1s, scorepcq evaluates to true, does not add
any draws to the record, and has density c." Score is used to explicitly scale
the density at the present point in the distribution. (b) shows the rule for
drawing random values from distributions, and reads "If Dp(cid:126)cq has density w
at c, Dp(cid:126)cq evaluates to c with density w and adds c to the record of draws."
The other rules deﬁne the rest of semantics in this way.

25

around deﬁning and applying functions. The language in Figure 11
extends an untyped, call-by-value lambda calculus to include sampling
random variables from distributions.

The full operational semantics for this language is shown in Figure 12.
The semantic domain is a pair of: (1) a record of all of the random
samples drawn, and (2) the density of the unnormalized joint distri-
bution evaluated at those samples points. The operational semantics
deﬁnes how each element of the syntax contributes to this meaning by
adding onto it.

Borgström et al. go on to use this operational semantics to prove that
the posterior samples from running the Trace-MCMC algorithm on a
program is consistent with the semantics of the program. The ability
to give such a formal correctness proof is an example of the advantages
of formal semantics for probabilistic programming languages.

3 Properties of probabilistic programming languages

salient for static analysis

Not all static analysis techniques make sense to apply to all probabilistic pro-
gramming languages. This section enumerates a set of properties of prob-
abilistic programming languages which, taken together, determine the ap-
plicability of a given static analysis approach. The author does not claim
that this proposed set is complete or that it will suﬃce in the future, but at
least that they are useful to keep in mind when exploring static analysis ap-
proaches. The properties of a small, diverse set of probabilistic programming
languages are shown in Table 1.

3.1 Properties

3.1.1 Discontinuous density functions

Some languages and inference methods only work well (or at all) with dis-
tributions with densities that are continuous, diﬀerentiable, or diﬀerentiable
to a higher order (as is the case with some MCMC variants). This require-
ment is sometimes incompatible with static analysis techniques that utilize
discontinuous densities.

While it is possible to represent a Stan program with a discontinuous
density function (for example, a program containing with control ﬂow that
depends on a model parameter), Stan does not claim to handle this case well

26

Figure 13: This is an example query from the Church language. The expres-
sion reads as: "Deﬁne a query over the model parameters grass-is-wet, rain,
and sprinkler, and return the distribution over rain given day2 and condi-
tioned on the predicate (grass-is-wet ’day2). Conditioning on this predicate
almost always leads to a discontinuous density over the result of the query,
even if the model parameters are continuous (they likely are not in this case).
Figure taken from [4].

with NUTS. The particle trajectory that HMC simulates to draw samples
is diﬃcult to simulate with a discontinuous energy function. The language
Church, on the other hand, was built to support conditioning on predicates
over parameters (Figure 13), making discontinuities typical. Church gener-
ally uses non-HMC MCMC sampling.

Some static analysis techniques are not applicable to probabilistic pro-
gramming languages that can not handle discontinuity, often because they
utilize on parameter-dependent control ﬂow that introduces discontinuity.

3.1.2 Predicate query oriented

Some languages are designed to give a posterior probability of some predicate
query over the parameters, rather than estimating the full posterior distri-
bution of the parameters. One example of such a language is Uncertain<T>
(Figure 14).

Some static analysis techniques rely on this predicate query to struc-
ture the computation, and usually involve reducing the program to focus on
only answering the query. It is worth noting that languages which infer full
distributions can beneﬁt from these approaches in the context of veriﬁca-
tion, where a predicate query is speciﬁed or generated separately from the
program deﬁnition.

27

Figure 14: This shows an example usage of the C++ type Uncertain<T>.
Distance and speed are deﬁned to be random variables. They are used
to determine which branch of the if statement to follow, and so the only
distributions that need to be infered are the distributions over the predicate
queries (Speed ą 4 and pSpeed ă 4q.P rp0.9q). Figure taken from [5].

3.1.3 Sampling-based inference

Some languages specialize in sampling-based inference (MCMC), where other
languages focus on other methods (such as variational methods).

Stan is an example of language focused on its MCMC backend, while

Infer.NET (Figure 15) focuses on variational methods.

Some static analysis for probabilistic programming approaches speciﬁ-
cally target the process of drawing samples in some way, and would not
make sense to apply to languages focusing on other inference methods.

3.1.4 Dynamic parameter declaration

In most probabilistic programming languages, there is a constant set of model
parameters deﬁned over a program. This structure is not required in, for
example, Pyro (Figure 16). Pyro allows new model parameters to be declared
depending on the branch of execution.

Some probabilistic programming-speciﬁc static analysis approaches lack
the ﬂexibility to be compatible with this level of ﬂexibility, such as those
which build and analyze graphical models to transform the program. These
methods could potentially be extended to handle this ﬂexibility by making
conservative approximations.

28

Figure 15: This shows an example Infer.NET model which deﬁnes a graph-
ical model with variables x, y, and mu. Infer.NET’s primary backend uses
variational inference. Figure taken from [32].

Figure 16: This shows a Pyro distribution. Pyro is written in Python
The function
for the Keras computational graph software platform.
normal_product deﬁnes two new, locally-deﬁned model parameters. Stan,
for example, does not allow parameters to be declared dynamically inside
functions. Figure taken from [1].

29

Discontinuous
density
functions

Stan [7]
Uncertain<T> [5]
Infer.NET [32]
Church [17]
Pyro [1]
PSPs [26]

!

!
!
!

Sampling-
focused
inference
!
!

!

Predicate
query
oriented

Conditional
variable
declaration

!

!

!

!
!

Table 1: This shows the properties for each of a small, diverse set of proba-
bilistic programming languages.

3.2 Properties of representative languages

Table 1 shows examples of how the properties apply to real-world languages.

4 Current techniques

4.1 Overview

There have been important and interesting contributions to the ﬁeld of static
analysis for probabilistic programming. They vary in their goal, approach
and what they assume about the languages they target. It is not uncommon
for a technique to be presented alongside an entirely new language on which
to demonstrate it. This section attempts to categorize these contributions
ﬁrst according to their purpose and broad strategy, then to describe their
approach and enumerate the language properties that they assume.

4.2 Optimization

Perhaps the majority of current work focuses on the optimization of proba-
bilistic programs.

4.2.1 Avoiding work

The primary strategy for classical optimization is to avoid doing work dur-
ing program execution in some way. This idea extends to probabilistic pro-
gramming by avoiding work during posterior inference. The probabilistic
semantics do require some additional considerations.

30

1. Program slicing and dead code elimination

The idea of program slicing is to ﬁnd the "slice" of a program which
contributes to a value at some point. This task is highly related to
the goal of dead code elimination, which aims to remove parts of the
program which do not contribute to the ﬁnal outcome of the execution.
For our purposes, we can consider them to be the same problem, where
dead code elimination preserves only the "slice" which contributes to
the result of the program.

In the case of probabilistic programming, we seek to ﬁnd the slice which
contributes to the distribution that the program represents. We can
apply the usual dead code elimination techniques to non-probabilistic
portions of the program, but we can also sometimes eliminate prob-
abilistic portions like extraneous model parameters. The examples
below focus on eliminating unnecessary model parameters.

Example: Slicing probabilistic programs. Program slicing is made a
little bit more complex in the probabilistic setting because there can
be indirect dependencies between model parameters which are not re-
ﬂected directly by the syntax. Hur et al. [24] provide a technique for
program slicing which is aware of these indirect relationships, and can
eliminate code to compute certain variables based on their indepen-
dence from the model parameters being estimated.
Figure 17 shows how a program is transformed.

Example: Hakaru. Another example of program slicing is implemented
into the compiler of the probabilistic programming language Hakaru.
During Hakaru’s "disintegration" phase, a program representing a joint
measure is rewritten into a representation of a conditional distribution,
and during this phase Hakaru drops pieces of the joint measure expres-
sion that do not contribute to the conditional distribution. Figure 19
shows a Hakaru program before disintegration and Figure 20 shows the
representation after disintegration.

This technique is most eﬀective when the user speciﬁes a query that
does not require inferring the full posterior of all model parameters.
Stan does not have a facility for the user to specify queries, and so must
assume that all parameters are important. Church explicitly speciﬁes
its query, and so could easily take advantage of PPL-speciﬁc program
slicing.

This technique requires a static analysis pass to ﬁnd the dependency

31

Figure 17: On the left is shown a probabilistic program which computes the
distribution of a random variable l. Since only l is returned, we can eliminate
computations that assign to variables which do not eﬀect l. Figure 18 shows
the probabilistic graphical model corresponding to the conditional depen-
dence relationships between the variables in this program: using the usual
rules for graphical models, we see that the value s is independent of the
value of l. The program on the right has the s computations automatically
removed but preserves the semantics of the full program.

32

Figure 18: The directed graphical model corresponding to the program in
Figure 17. It encodes the following conditional independence: given i, s is
independent of l.

Figure 19: A Hakaru program. The ﬁrst three lines are the type of the
program. noiseT and noiseM are drawn from uniform distributions, t1, t2,
m1 and m2 are drawn from normal distributions. The ﬁnal line returns a
joint distribution where m1 and m2 are observed variables and noiseT and
noiseM are model parameters.

graph between variables. The dependency graph can be derived from
the Reaching Deﬁnitions property found from the Monotone Frame-
work. The dependency graph can be extracted from programs in any
probabilistic programming language with varying levels of diﬃculty
and speciﬁcity, and so program slicing can be applied in some form to
any language.

2. Partial evaluation

Rather than avoiding computation altogether, partial evaluation at-
tempts to compute as much as possible at compile-time. This is a very
general and eﬀective technique that is already applied as an optimiza-
tion step in some probabilistic programming languages.

33

Figure 20: This is a Hakaru expression representing the program in Figure 19
conditioned on m1 and m2 (named here x2 and x3). The program will also be
sliced with respect to the output variables t1 and t2 (now named x4 and x6).
This representation is confusing to read because it has been automatically
generated.

Figure 21: This is a Hakaru expression representing the conditional distribu-
tion in Figure 20 after being partially evaluated by Hakaru’s "Simpliﬁcation"
transformation, which applies algebraic reductions with the Maple software
package. Again, this representation is diﬃcult to read because it is automat-
ically generated.

34

For example, the Hakaru language implements partial evaluation by
translating pieces of the program into an algebraic language, passing it
to an algebraic simpliﬁer, and then tranlating back to the Hakaru rep-
resentation. Hakaru calls this the simpliﬁcation phase. A Hakaru rep-
resentation before simplication is shown in Figure 20 and after simpli-
ciﬁcation in Figure 21 The representation of a Hakaru program before
This aﬀords Hakaru extra eﬃciency depending heavily on the choices
of variable distributions and the strength of the solver.

The implementation of partial evaluation probabilistic languages is not
signiﬁcantly diﬀerent than its implementation for non-probabilistic lan-
guages. The approach is to search for patterns of subexpressions in the
program which can be reduced to some simpler form.

3. Simpliﬁcation by abstract interpretation

When a probabilistic program is only being used to answer some query,
it may be possible to simplify the program from the fully concrete
domain into a more abstract domain that can still represent the query.
This abstract program may allow for faster posterior inference [22].
This idea is covered in for the speciﬁc case of verifying a predicate
query by abstracting the program into a predicate domain. In theory,
it could be used to optimize programs for queries more complex than
predicates but less than full parameter densities.

4.2.2

Inference-aware optimization

Another avenue for optimization, which is a opportunity unique to proba-
bilistic programming, is to use program static analysis to improve the infer-
ence process. In some circumstances, probabilistic programs can be trans-
formed to be more eﬃcient for a particular posterior inference method. So
far work on this approach is limited to improving the eﬃciency of sampling
methods.

Example: Optimizing sampling with R2. Nori et al. [31] introduced an infer-
ence algorithm called R2 whose main innovation is overcoming a particular
ineﬃciency in drawing samples from a probabilistic program.

An example of the situation that R2 attempts to improve is shown in
Figure 22. The example is written in a language speciﬁed alongside R2. In
this program, some binary random variables are ﬁrst sampled from distribu-
tions. Observations of those samples are then speciﬁed (line 16) in the form
of a predicate assertion. A naive sampling process for this program would

35

Figure 22: An R2 program before transformation. The sampling statements
(e.g. line 6) are separated from the observation assertion (line 16), so some
samples will disagree with the observations.

generate the samples from the Bernoulli distributions, but they would very
often not satisfy the observed condition, and so they would be rejected and
work would be wasted.

Instead of this naive sampling process, R2 ﬁrst transforms the program
to specify the constraints on the samples at same the time they are drawn,
as shown in Figure 23. Then, instead of sampling from independent distribu-
tions and rejecting many of the samples, R2 directly samples from distribu-
tions conditioned on the constraints being true. Since the sampling process
itself is aware of the restrictions on the samples, no samples are rejected.

The program transformation is computed by cascading the logical im-
plications of the observations backward through the program by ﬁnding the
weakest precondition necessary for each successive command. This is called
the pre-image operator.

This technique relies heavily on restrictions in the demonstration lan-
guage. The pre-image operator is not necessarily computable for general
languages.
It is shown only for Bernoulli distributions, but the authors
suggest that the approach could be extended to other distributions by trun-
cating their support to match conditions on the samples. This approach
will probably only work with observations which reduce the support of the

36

Figure 23: The R2 program after transformation. Each sampling statement
is accompanied by the necessary conditions on the sample (for example, line
6 accompanied by line 7).

37

posterior.
Nonstandard interpretations. Non-standard interpretations are alternative
ways of reading a program which give a diﬀerent semantics to code that
is already written. A typical strategy for non-standard interpretation is to
replace an existing data type with a new data type that contains additional
information. A common example of this is automatic diﬀerentiation [8].

Since non-standard interpretations don’t necessarily happen at compile-
time, they aren’t strictly static analyses. However, since in the probabilistic
programming context they may happen before the inference phase, they may
eﬀectively work at as static analyses, and so they are mentioned brieﬂy here.
Non-standard interpretations have been used in the probabilistic program
setting to add structure to the deﬁnition of the density function in order
to enable an inference method. The most common example is automatic
diﬀerentiation, which automatically provides a density gradient for inference
methods such as HMC [38]. Another example is provenance analysis, which
is a sort of ad-hoc dependence analysis, which can enable alternative MCMC
algorithms [38]. A third example is a system called AutoConj [21], which
searches the code of a density deﬁnition for conjugacy structure to make
inference methods like Gibbs sampling simpler and more eﬃcient.

4.2.3 Conclusions

Depending on the language, probably the most practical source of optimiza-
tions will be classical optimizations applied to the non-probabilistic pro-
gramming speciﬁc portions of the pipeline, such as the density computation.
Many classical optimizations can be applied without major modiﬁcation.

The program slicing and partial evaluation ideas presented in this section
are direct extensions of classical optimizations to probabilistic programming.
Both of these ideas are likely to speed up any language they are applied to.
R2 represents interesting theoretical work on optimization that takes
advantage of the actual structure of the probabilistic programming pipeline.
However, it is unlikely to be practically useful until it is extended to be
compatible with popular languages and inference techniques.

4.3 Veriﬁcation

In probabilistic programming, veriﬁcation queries can be deﬁned over prob-
abilistic quantities. In such a case, we cannot necessarily answer the query
by proof of disproof, but rather by estimating (or bounding) the probabil-
ity that the query holds. One strategy for estimating the query probability

38

is to simply sample from the program and observe how often the query is
satisﬁed. This strategy can work, but the query may require rare events to
be estimated, and so it may take an unreasonable number of samples to get
suﬃcient certainty on the bound of the estimate. Most of the current work
relating to the veriﬁcation of properties of probabilistic programs deals with
making this estimate feasible.

1. Dividing the space of executions

One strategy is to divide up the space of possible executions by the
path taken through the program, and then evaluate each path sepa-
rately. This strategy will also allow us to ﬁnd formal bounds on the
probabilities of validation predicates, given that we use a fairly restric-
tive language to make the proofs easier.

Consider the probabilistic program in Figure 25. Our goal is to ﬁnd
the probabilities of the predicates on lines 11 and 12. Ideally, we could
sample executions of the this program inﬁnitely many times, taking
new samples of the random variables each time, and we would know
the predicate probabilities. Instead, the strategy will be:

(a) Sample some executions of the program, keeping track of the
paths of the executions. A path is deﬁned by the control-ﬂow
decisions that are made throughout the execution, such as the
branch taken at each if ´ then ´ else statement. Continue sam-
pling these paths until we are conﬁdence that, with high prob-
ability, the path of a newly sampled execution will fall into the
already observed set.

(b) Find a proven lower bound on the probability of a newly sampled

execution having a path in the observed set of paths.

(c) For each path in the observed set, compute upper and lower
bounds on the probability of each validation predicate using the
information that the path implies. For example, if the path in-
cludes the then branch of an if pXq statement, then X is known
about all executions that take this path, and X may change the
probabilities of the validation predicates. This extra information
lets us compute tighter upper- and lower-bounds on the validation
predicates.

(d) We can now compute upper- and lower- bounds on the predicates
over all paths by assuming the opposite extreme (certainly true or
certainly false) is taken with whatever probability is not certain
to be covered by the path set.

39

Figure 24: This is a deterministic part of the example program for the path
sampling method.

This strategy relies heavily on the restrictions of the language used
to demonstrate it. These restrictions include being limited to only
basic arithmetic operations on variables, only integral and ﬂoating-
point types, and limiting statements to assignments, conditionals and
while-loops. The conditionals and validation predicates are restricted
to linear assertions over reals and integers. Bounds on the probabilities
of the predicates given paths are possible to compute because of these
language restrictions. This strategy would be diﬃcult to translate
directly to more ﬂexible languages.

The strategy also depends heavily on the use of control-ﬂow with model
parameter predicates to break up the execution space. This advantage
would be lost in languages which do not support discontinuous density
functions, and thus have trouble representing such predicates at all.

2. Probabilistic abstract interpretation

The idea of abstract interpretation can be extended to probabilistic
programs. We can extend the original deﬁnition of abstraction con-
sistency (shown in Figure 7) to distributional consistency, which says
that the the probability of an abstraction of the results of the con-
crete program should be identical to the probability of an abstraction

40

Figure 25: This shows a probabilistic program in the imperative language
designed to demonstrate the path sampling method. The underlined state-
ments deﬁne veriﬁcation queries. The queries ask whether the sensativity of
the estimateLogEGF R function to its inputs meets a threshold. The def-
inition of the estimateLogEGF R function is shown in Figure 24. The key
point from that function is that its behavior depends on branching conditions
of its inputs.

resulting from the abstract program.

For the veriﬁcation settings, we consider abstract interpretation of
probabilistic programs into predicate domains. Predicate domains are
semantic domains containing only boolean variables which correspond
to the truth or falsity of predicates on variables in the concrete domain.
The validation query can then be described in this domain. If we can
construct an abstract program in the predicate domain which is con-
sistent with the original program, we can perform posterior inference
on the abstract program in order to perform the veriﬁcation.

Figure 26 shows an example of a probabilistic program before and after
abstracting the program. The veriﬁcation predicate we are considering
is z “ 0, so this is included as a variable in the abstracted program.
The abstract program encodes the observation that z is zero if either
x is zero or y rounds down to zero.
If the functions discrete_dist
or continuous_dist are expensive to compute, operating on the ab-
stracted program instead will be make this inference task feasible.

41

Figure 26: Example before (a) and after (b) abstracting a probabilistic pro-
gram into the predicate domain, considering the query z “ 0. Figure taken
from [22].

4.3.1 Conclusions

Veriﬁcation for probabilistic programming is still quite a young idea, and is
not yet especially practical - except in its classical form, which can still be
used to prove non-probabilistic statements about non-probabilistic variables
or probabilistic variables across a single evaluation of the density function.
Current probabilistic programming veriﬁcation techniques mostly rely on
restrictive demonstration languages for feasibility. Path sampling, for exam-
ple, is only eﬀective when the program includes many conditional branches
which relate to the veriﬁcation query, and can only handle very simple pro-
gramming constructs. Probabilistic abstract interpretation has the potential
to be a more general idea, but will likely require further development before
it can be applied with any reasonable generality to real world probabilistic
programming languages and scale to datasets.

4.4 Usability

4.4.1 Compute away restrictions to the interface

Probabilistic programming languages tend to be much more user-friendly
interfaces for machine learning than other approaches, since users can spec-
ify their models directly as programs. However, there are still major im-
provements to be made to the interface of the programs themselves. Some

42

probabilistic programming languages have made design choices which favor
a straightforward application their inference method of choice over ease of
programming, such as Stan programs enforcing a block structure. Static
analysis can sometimes oﬀer the best of both worlds by providing an easier
interface for users and translating it into the original programming language,
retaining its ease of inference. This is the goal of usability methods. Another
potential beneﬁt of this translation step is to optimize the workﬂow that is
automated away.

An example of this approach is SlicStan [19]. SlicStan is a probabilistic
programming language which is similar to Stan except that it relaxes some
of the restrictiveness of Stan’s syntax. SlicStan programs are compiled into
Stan by re-organizing statements into a Stan program.

Stan programs are written in predeﬁned blocks, such as the parameters
block for declaring model parameters and the model block and transformed
parameters block for deﬁning the log-density function. SlicStan’s primary
contribution is to not require these blocks. Instead, SlicStan allows users to
mix statements that would otherwise be in diﬀerent blocks, and sorts them
into their optimal block when the program is compiled to Stan. Figure 29
shows an example translation. By relaxing Stan’s block system, SlicStan
also extends Stan’s user-deﬁned function capabilities and makes composing
programs more straightforward. One potential downside is that a user might
lose some safety they would have by consciously structuring their program.
During the translation, SlicStan’s goal is to place each statement into the
Stan block where that statement will be executed the fewest times while still
executing correctly. Figure 28 shows the execution time for each block. For
example, since the model block will (typically) execute each time the NUTS
algorithm takes a step, SlicStan only assigns statements to model that must
be there, while any statement that can go into generated quantities, which
only runs once, should be placed there.

SlicStan works by augmenting Stan’s type system for expressions with
additional information (Figure 27). SlicStan then infers the level type (also
shown in Figure 27) for each statement using a set of typing rules. Statements
which must be in the model block, such as $„$-statements, are assigned the
MODEL level type. Statements which receive information from MODEL
level statements must be either MODEL or GENQUANT level statements,
and statements which provide information to MODEL level statements must
be either DATA or MODEL. Each statement is then assigned the least
compute-intensive level that is permitted by these rules. This way, each
statement is placed optimally without user input.

43

Figure 27: Enumerated here are the types of expressions in the SlicStan
language. Each SlicStan expression is given a type T which is a pair of a
base type τ (taken directly from Stan) and a level type (cid:96). The level type
indicates one of three options (DAT A, M ODEL and GEN QU AN T ), which
indicate their block placement requirements.

Figure 28: Each Stan block, along with the Stan phase during which it is
executed and the level of SlicStan statement that will be assigned to it. "Per
chain" means that the block is executed every time the NUTS algorithm is
restarted on a new starting point, which typically occurs many times during
posterior inference. "Per leapfrog" means that the block is executed every
time the NUTS algorithm takes a step, which occurs many times for each
chain. As a result, for example, it is less eﬃcient to place a statement in the
model block than in the transformed data block.

44

Figure 29: On the left is shown a SlicStan program, which allows users
to forgo blocks and deﬁne more ﬂexible functions. On the right is shown
the equivalent Stan program that will result from compilation. Only the
necessary statements are included in the model block of the resulting Stan
program.

4.4.2 Conclusions

SlicStan is the only major example of static analysis of probabilistic programs
for usability, perhaps since user experience can also be improved by writing
powerful libraries and documentation. However, presenting users with easier
and more powerful interfaces has potential impact across all programming
languages.

5 Future directions

5.1 Extensions of current work

This section enumerates some directions that would be natural extensions of
the current work.

5.1.1 Optimization

Automatic transformation of models to use parallel computing hardware.
With suﬃcient understanding of the information ﬂow within a probabilis-
tic program, it should be possible to automatically transform probabilistic

45

programs to take advantage of specialized computing hardware during pos-
terior inference. This could include parallelism with the CPU, and GPU or
TPU hardware. Some languages are already in a good position to imple-
ment this optimization, such as Edward and Pyro which target computation
graph backends (Tensorﬂow and Keras, respectively) that are already sup-
port specialized hardware. However, this optimization is applicable for most
any language.

For example, if a static analysis method could show that the iterations
of a f or loop in a Stan model block were independent, the loop could be
automatically parallelized. Parallelism within the computation of a density
would be the easiest to achieve, but parallelism across other parts of the
inference might also be possible for particular inference algorithms.

Passing additional structure to the inference algorithm. Non-standard in-
terpretation of source code can add to the semantics of a program, which
can provide more information to an inference algorithm. Current exam-
ples include automatic diﬀerentiation providing gradients to HMC in Stan
[8]; provenance analysis to enable alternative MCMC algorithms [38]; and
automatically discovered conjugacy structure to speed up various types of
inference [21].

Advances in inference techniques could be coupled with new static anal-
ysis techniques which provide additional information from the program. For
example, using dependence analysis on a Stan program to build a factor
graph could enable Stan’s backend to utilize the Sum-Product algorithm,
which might enable Stan to automatically marginalize out discrete valued
variables [2]. Dependence analysis could also be used to ﬁnd the Markov
blanket for each parameter, enabling eﬃcient Gibbs sampling.

Partial evaluation of automatic diﬀerentiation. Automatic diﬀerentiation is
typically much more convenient and robust than user-provided derivatives,
but it is sometimes less eﬃcient. Attempting to ﬁnd symbolic derivatives
for expressions using algebraic solvers, in a similar style to Hakaru in 4.2.1,
would sometimes speed up inference at the expense of compilation time and
complexity.

5.1.2 Veriﬁcation

Relaxing the restrictions of veriﬁcation methods. Many current approaches to
veriﬁcation rely heavily on the restrictions of their demonstration languages.
There are many potential ways to relax these restrictions which could allow

46

these approaches to be applied more broadly. For example, in the paper
introducing the R2 inference technique, Nori et al. suggest that R2’s ideas
could be applied to Stan - however, since R2 relies on observations in the
form of predicate assertions rather than data points, R2’s approach cannot
be directly applied to Stan without expanding its notion of observation.

New probabilistic abstract interpretations. Holtzen et al. demonstrated the
use of predicate domains for veriﬁcation (4.3) in a relatively simple environ-
ment. Their work could be extended by implementing more sophisticated
versions of the pre-image operator to infer the weakest precondition of more
types of predicates and statements. For instance, if an implementation of
the pre-image operator were to include suﬃcient statistical knowledge, then
perhaps statistical predicates could be included and reasoned about in the
abstract domain.

5.1.3 Usability

Restrictive mode. It could be useful to build restrictive subsets of existing
probabilistic programming languages, which make it harder for users to make
certain mistakes. This might be especially helpful to new users who might
not understand the pitfalls of advanced features. This approach contrasts
SlicStan, which aims to make the compilation process less restrictive.

For example, Stan includes a construct called reject(), which eliminates a
NUTS trajectory and eﬀectively sets the density at that point to zero. This
feature has the potential to introduce discontinuity into the density which
NUTS cannot eﬀectively estimate. A restrictive mode in Stan might disallow
reject() entirely.

In addition to being a helpful guide for usability, a restrictive mode might
also be helpful for implementing many other static analyses discussed in this
paper by disallowing problematic language features. For example, if some
static analysis didn’t work with recursive functions, a restrictive mode could
disallow recursive functions.

5.2 Unexplored directions

Listed here are directions that are of high potential and which are not directly
reﬂected in the current literature. They are meant more to be discussion
points than detailed plans.

47

5.2.1 Statistically-aware user feedback

There is potential for some system, supported by static analysis, to pro-
vide the user with feedback about the statistical model that their program
represents. This could fall under the general category of usability, because
statistical feedback may make probabilistic programming easier for users who
are not absolute experts in statistics. It could also fall under veriﬁcation,
because the feedback may be phrased in terms of evaluating the truth of
statistical properties on a program.

For example, the compiler could ﬁnd the conditional independencies en-
coded in a program and provide a representation such as a graphical model
to the user. The user could then check this model against their intent.

Other properties of a statistical model from the literature could also
potentially be provided, a compiler could attempt to determine the statistical
power of a model, letting a user understand if their dataset is likely to be
suﬃcient to evaluate a query to a suﬃcient level of certainty.

In the extreme, statistically-aware user feedback could allow a probabilis-
tic program compiler to take on the role of an interactive statistical assistant.
An assistant may not be able to supply the main ideas of a model, but it
could catch common errors and ﬁnd mismatches between the program and
the user’s intentions. A conversation about the model could take place as
an alternation of program iterations and compiler feedback.

5.2.2 Validation of programs against other abstractions

Typical program veriﬁcation involves evaluating the truth of a predicate on
the program variables.

When the user writes a probabilistic program, they may have some other
description of the model in mind, which are then made more concrete in
the form of the program. For example, a user might be trying to encode
a directed graphical model with some conditional independence structure,
or they have some knowledge of the causal relationships between the vari-
ables. If the program is written correctly, this additional information will be
encoded in the program.

These additional representations could be considered abstractions of the
probabilistic program akin to the abstract programs in probabilistic abstract
interpretation. They are not concrete enough to apply a posterior inference
method, but they can be checked for consistency against a concrete program.
A user would provide an abstraction of their probabilistic model, such as
a graphical model, in a speciﬁcation language alongside their probabilistic

48

program. A compiler could then check that the probabilistic program is
consistent with the provided abstraction.

5.2.3 Higher-level representations via transformation

SlicStan provides a slightly higher level alternative interface for Stan that
alleviates some of the programming complexities of writing Stan code. It
could be similarly useful to provide higher-level interfaces that alleviate some
of the complex statistical considerations for the user. Any piece of statistical
knowledge that can be encapsulated into a higher-level interface is likely to
be easier to write and more likely to be correct.

There could be domain-speciﬁc languages for a variety of ﬁelds which are
specialized to easily and precisely express domain knowledge in that ﬁeld.
For example, a domain-speciﬁc language for computational genetics might
include convenient manipulation of sequences-valued random variables, and
its compiler might translate that representation into a Hidden Markov Model
represented in Stan code.

Another example of could be automatically marginalizing discrete pa-

rameters.

5.2.4 Data-scalable and data-absent analyses

In the current literature, static analyses generally assume that observations
are integrated into the body of the program, and so this data is available to
be part of the analysis at compile time. There are two potential issues with
this assumption:

• Some analyses, especially veriﬁcation methods, will not scale well when

the data becomes very large.

• In some probabilistic programming languages such as Stan, the dataset
is not provided to the compiler along with the source code and is
instead fed into the program after compilation.

Static analysis methods could be developed with special attention to
these issues. For example, interval analysis is a form of abstract interpreta-
tion where numeric types are abstracted as interval ranges representing their
bounds. Interval analysis would be diﬃcult to implement on a probabilistic
program that has some values missing (in the case of absent data) or values
that are resource intensive to traverse (in the case of large datasets). An
alternative implementation of interval analysis could operate instead with
some user-provided assumptions or an eﬃcient process for summarizing the

49

relevant features of dataset. Examples of these summaries might be dimen-
sions, moments or bounds of a data variable.

5.2.5 Posterior inference failure prediction and diagnosis

One class of error discussed in section 1.3.2 is a failure of the posterior
inference algorithm to converge to a reasonable approximation of the true
posterior. Static analysis could do something to predict this type of error for
a given program, perhaps also informed by the dataset. Any such analysis
would be speciﬁc to the inference algorithm being run.
In general, implementation approaches could be:

• Searching for speciﬁc predictors of bad inference behavior. For exam-
ple, if the posterior inference algorithm were HMC, the compiler could
warn the user when the program is likely to have a discontinuous den-
sity or produce a non-Ergodic Markov chain.

• Searching for suﬃcient indicators of good inference behavior (which
will certainly be overly restrictive). For example, if the program is
found to match a particular class of model known be supported by the
inference engine, the compiler could be conﬁdent that the inference
task for that program is viable.

• Running posterior inference on automatically generated or subsampled
datasets and checking for indications of failure. This may be a static
analysis in the sense that it could be done before the real inference task,
but dynamic in the sense that it runs the program with an inference
engine.

6 Conclusions

The current state of the ﬁeld of static analysis for probabilistic program-
ming is quite limited in scope. There is a disconnect between the language
properties which theoretical static analyses assume and the languages which
tend to scale well and have the most users. As a result, most of the immedi-
ate gains available to probabilistic programming language compilers are the
same classical analyses available to other programming languages. However,
current work has shown signiﬁcant promise in the direction of probabilistic
programming-speciﬁc static analyses.

Static analysis methods are already applied successfully in software engi-
neering to provide high-level interfaces, automatic proofs-of-correctness, and

50

optimizations without mental overhead. With advances in this ﬁeld, ap-
plying static analysis methods to probabilistic programming has the unique
potential to do the same for probabilistic modeling.

7 Acknowledgements

I would like to thank Professor Jeannette Wing, Professor Andrew Gelman,
and Dr. Matthijs Vakar for their extensive feedback and guidance. I am also
very grateful to Professor Itsik Pe’er for generously providing funding that
enabled this work.

References

[1] Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer,
Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh, Paul A. Szerlip,
Paul Horsfall, and Noah D. Goodman. Pyro: Deep universal probabilis-
tic programming. Journal of Machine Learning Research, 20:28:1–28:6,
2019.

[2] Christopher M. Bishop. Pattern recognition and machine learning. New

York Springer, 2006.

[3] David M Blei, Alp Kucukelbir, and Jon D McAuliﬀe. Variational in-
ference: A review for statisticians. Journal of the American Statistical
Association, 112(518):859–877, 2017.

[4] Johannes Borgström, Ugo Dal Lago, Andrew D. Gordon, and Marcin
Szymczak. A lambda-calculus foundation for universal probabilistic pro-
gramming. In ICFP, 2016.

[5] James Bornholt, Todd Mytkowicz, and Kathryn S. McKinley. Uncer-

tain: a ﬁrst-order type for uncertain data. In ASPLOS, 2014.

[6] Guillaume Brat, Jorge A. Navas, Nija Shi, and Arnaud Venet.

A framework for static analysis based on abstract interpretation.
SEFM, 2014.

Ikos:
In

[7] Bob Carpenter, Andrew Gelman, Matthew D. Hoﬀman, Daniel Lee, Ben
Goodrich, Michael Betancourt, Marcus A. Brubaker, Jiqiang Guo, Peter
Li, and Allen Riddell. Stan: A probabilistic programming language.,
2017.

51

[8] Bob Carpenter, Matthew D Hoﬀman, Marcus Brubaker, Daniel Lee, Pe-
ter Li, and Michael Betancourt. The stan math library: Reverse-mode
automatic diﬀerentiation in c++. arXiv preprint arXiv:1509.07164,
2015.

[9] Arun Tejasvi Chaganty, Aditya V. Nori, and Sriram K. Rajamani. Ef-
ﬁciently sampling probabilistic programs via program analysis. In AIS-
TATS, 2013.

[10] Siddhartha Chib and Edward Greenberg.

Understanding the
metropolis-hastings algorithm. The american statistician, 49(4):327–
335, 1995.

[11] Guillaume Claret, Sriram K. Rajamani, Aditya V. Nori, Andrew D.
Gordon, and Johannes Borgström. Bayesian inference using data ﬂow
analysis. In ESEC/SIGSOFT FSE, 2013.

[12] Patrick Cousot and Radhia Cousot. Abstract interpretation: A uniﬁed
lattice model for static analysis of programs by construction or approx-
imation of ﬁxpoints. In POPL, 1977.

[13] Patrick Cousot and Michael Monerau. Probabilistic abstract interpre-

tation. In ESOP, 2012.

[14] S. Ermon. Variational inference, 2019.

[15] Andrew Gelman, Daniel Lee, and Jiqiang Guo. Stan: A probabilistic

programming language for bayesian inference and optimization, 2015.

[16] Andrew Gelman, Xiao-Li Meng, and Hal Stern. Posterior predictive
assessment of model ﬁtness via realized discrepancies. Statistica Sinica,
6(4):733–760, 1996.

[17] Noah D. Goodman, Vikash K. Mansinghka, Daniel M. Roy, Keith
Bonawitz, and Joshua B. Tenenbaum. Church: a language for gen-
erative models. In UAI, 2008.

[18] Andrew D. Gordon, Thomas A. Henzinger, Aditya V. Nori, and Sri-

ram K. Rajamani. Probabilistic programming. In FOSE, 2014.

[19] Maria I. Gorinova, Andrew D. Gordon, and Charles A. Sutton. Prob-
abilistic programming with densities in slicstan: eﬃcient, ﬂexible, and
deterministic. PACMPL, 3:35:1–35:30, 2019.

52

[20] Matthew D. Hoﬀman and Andrew Gelman. The no-u-turn sampler:
Adaptively setting path lengths in hamiltonian monte carlo. Journal of
Machine Learning Research, 15:1593–1623, 2014.

[21] Matthew D. Hoﬀman, Matthew J. Johnson, and Dustin Tran. Auto-
conj: Recognizing and exploiting conjugacy without a domain-speciﬁc
language. In NeurIPS, 2018.

[22] Steven Holtzen, Guy Van den Broeck, and Todd D. Millstein. Sound
In ICML,

abstraction and decomposition of probabilistic programs.
2018.

[23] Steven Holtzen, Todd D. Millstein, and Guy Van den Broeck. Proba-

bilistic program abstractions. CoRR, abs/1705.09970, 2017.

[24] Chung-Kil Hur, Aditya V. Nori, Sriram K. Rajamani, and Selva Samuel.

Slicing probabilistic programs. In PLDI, 2014.

[25] M Tim Jones. Optimization in gcc. Linux journal, 2005(131):11, 2005.

[26] Ashish Kapoor, Debadeepta Dey, and Shital Shah. Probabilistic safety

programs. CoRR, abs/1610.05376, 2016.

[27] Joomy Korkut. Edit-time tactics in idris. 2018.

[28] Praveen Narayanan, Jacques Carette, Wren Romano, Chung chieh
Shan, and Robert Zinkov. Probabilistic inference by program trans-
formation in hakaru (system description). In FLOPS, 2016.

[29] Radford M. Neal. Mcmc using hamiltonian dynamics, 2010.

[30] Nielson H. R. Nielson, F. and C. Hankin. Principles of program analysis.

Berlin Springer, 1999.

[31] Aditya V. Nori, Chung-Kil Hur, Sriram K. Rajamani, and Selva Samuel.
R2: An eﬃcient mcmc sampler for probabilistic programs. In AAAI,
2014.

[32] S S. J. Wang and M P. Wand. Using infer.net for statistical analyses.

The American Statistician, 65:115–126, 05 2011.

[33] Sriram Sankaranarayanan, Aleksandar Chakarov, and Sumit Gulwani.
inferring whole program

Static analysis for probabilistic programs:
properties from ﬁnitely many paths. In PLDI ’13, 2013.

53

[34] Sam Staton. Commutative semantics for probabilistic programming. In

ESOP, 2017.

[35] Sean Talts, Michael Betancourt, Daniel Simpson, Aki Vehtari, and An-
drew Gelman. Validating bayesian inference algorithms with simulation-
based calibration., 2018. cite arxiv:1804.06788Comment: 26 pages, 14
ﬁgures.

[36] Stan Development Team. Stan User’s Guide, Version 2.19., 2012.

[37] Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and
Paul-Christian Bürkner. Rank-normalization, folding, and localization:
An improved pR for assessing convergence of mcmc. arXiv preprint
arXiv:1903.08008, 2019.

[38] David Wingate, Noah D. Goodman, Andreas Stuhlmüller, and Jef-
frey Mark Siskind. Nonstandard interpretations of probabilistic pro-
grams for eﬃcient inference. In NIPS, 2011.

54

