GAP-Gen: Guided Automatic Python Code Generation

Junchen Zhao*
University of Calfornia, Irvine
Irvine, CA, United States
junchez3@uci.edu

Junlin Wang*
University of California, Irvine
Irvine, CA, United States
junliw1@uci.edu

Abstract

Automatic code generation from natural lan-
guage descriptions can be highly beneﬁcial
during the process of software development.
In this work, we propose GAP-Gen, an au-
tomatic code generation method guided by
Python syntactic constraints and semantic con-
straints. We ﬁrst introduce Python syntac-
tic constraints in the form of Syntax-Flow,
which is a simpliﬁed version of Abstract
Syntax Tree (AST) reducing the size and
high complexity of Abstract Syntax Tree but
maintaining the crucial syntactic information
In addition to Syntax-
of Python code.
Flow, we introduce Variable-Flow which ab-
stracts variable and function names consis-
tently throughout
In our work,
the code.
rather than pre-training, we focus on modi-
fying the ﬁne-tuning process which reduces
computational requirements but retains high
generation performance on automatic Python
code generation task. GAP-Gen ﬁne-tunes
the transformer-based language models T5 and
CodeT5 using the Code-to-Docstring datasets
CodeSearchNet, CodeSearchNet AdvTest and
Code-Docstring-Corpus from EdinburghNLP.
Our experiments show that GAP-Gen achieves
better results on automatic Python code gener-
ation task than previous works.1

1

Introduction

Software has become a crucial component of mod-
ern society, directly affecting billions of people’s
everyday work and life. Efﬁciently and effectively
designing the software has always been challeng-
ing. One possible solution to this challenge is to
use natural language description to generate cor-
responding source code. Early approaches to this
problem were rule-based, identifying syntactic pat-
terns in text and using handcrafted rules to map the
patterns to code. Methods used to recognize syntac-
tic structure include regular patterns (Gulwani and

1* denotes equal contribution.

Yurun Song*
Imperial College London
London, UK
songyr888@gmail.com

Ian G. Harris
University of California, Irvine
Irvine, CA, United States
harris@ics.uci.edu

Marron, 2014; Kate et al., 2005; Le et al., 2013)
and parse trees produced using context-free gram-
mars (Kate et al., 2005; Le et al., 2013; Ballard
and Biermann, 1979; Price et al., 2000). Several
previous approaches convert a sentence into a for-
mal statement by mapping verbs to functions in
the formal language, and mapping the objects of
the verb in the sentence to function arguments in
the formal language (Ballard and Biermann, 1979;
Price et al., 2000; Little and Miller, 2006). For
example, the sentence, “Add r1 to r2” might be
mapped to add(r1, r2) in a procedural language.
The problem of ﬁnding the objects of the verb to
use as function arguments is simple if the sentence
structure is strictly limited. Several approaches use
regular expressions (Le et al., 2013) or context-free
grammars (Kate et al., 2005) to identify the objects
in the sentence.

More recent approaches are data-driven and
leverage machine learning methods, e.g., (Desai
et al., 2016) uses a Naive Bayesian Classiﬁer to
map English words to a domain-speciﬁc language,
and (Quirk et al., 2015) learns production rules
for a semantic parser. (Rahit et al., 2019) uses a
Long-Short Term Memory (LSTM) Recurrent Neu-
ral Network (RNN) architecture to implement their
neural machine translation approach . Work pre-
sented in (Ling et al., 2016) introduces the Latent
Predictor Network (LPN) architecture which treats
code generation as a sequence-to-sequence model-
ing problem. (Yin and Neubig, 2017) builds upon
this approach by leveraging the grammar model of
the target language as prior knowledge.

Research presented in (Clement et al., 2020;
Feng et al., 2020; Lu et al., 2021) introduce
transformer-based language model pre-training
methods to map the natural language semantic
with the code. Although these works have rela-
tively good performance on source code generation
task, they usually require high computational re-
sources, which are difﬁcult to acquire. They also

2
2
0
2

n
a
J

9
1

]
L
P
.
s
c
[

1
v
0
1
8
8
0
.
1
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
Figure 1: An overview of the proposed approach Phase I has two language model generating Syntax-Flow and
Variable-Flow. In Phase II, another language model encode these two types of information as well as the docstring
to generate code.

usually consider the code as a sequence of tokens
(Feng et al., 2020; Kanade et al., 2020; Lu et al.,
2021) and ignore either the source code’s syntactic-
level or semantic-level information, which could
improve the language models’ code understanding
capability, during their pre-training process.

In this work, we present GAP-Gen, a method
to improve automatic Python source code genera-
tion from natural language description. Our GAP-
Gen is ﬁne-tuning of the pre-trained T5-English
(Raffel et al., 2020a) and CodeT5 (Wang et al.,
2021) language models that employ Syntax-Flow
and Variable-Flow as guidance and has shown on
being able to understand the relationship between
natural language description and Python code from
syntactic and semantic level of the Python code.

Our GAP-Gen training pipeline is summarized
under 2 Phases. As you can see in Figure 1, In
Phase I, we ﬁne-tune our pre-trained language
model for the purpose of generating Python code’s
syntactic constraints and semantic-level structure,
the Syntax-Flow and the Variable-Flow. In Phase
II, we ﬁne-tune a separate language model by en-
coding natural language description of the code,
the generated code syntactic constrains (Syntax-
Flow) and abstracted variable names (Variable-
Flow) from Phase I to generate Python code. By
doing so, language models ﬁne-tuned with GAP-
Gen training pipeline are able to surpass many pre-
vious works’ performances, which rely on the pre-
training process of language models without con-
sidering code’s syntactic and semantic information.

Our main contributions are:

• We introduce Syntax-Flow and demonstrate

the importance of the source code’s syntac-
tic information in the automatic Python code
generation task.

• We show that abstracting variable and func-
tion names through Variable-Flow is effective
in maintaining the naming semantics of the
code.

• We achieve high performance on automatic
Python source code generation task without
language model pre-training..

2 Related Works

Language Models
for Programming Lan-
guages. Transformer based language models that
utilize attention mechanism have been dominat-
ing NLP benchmarks (Vaswani et al., 2017; Wang
et al., 2018). The novel attention-based message
passing techniques plus multi-task pre-training (De-
vlin et al., 2019) have been through extensive stud-
ies. This leads to deeper understanding of the rep-
resentational power of transformer based models
(Ethayarajh, 2019; Kovaleva et al., 2019; Jain and
Wallace, 2019).

At

the same time,

transformer-based auto-
regressive language models consisting of en-
coder/decoder demonstrate stellar performances on
many NLP generative tasks (Radford et al., 2019;
Lewis et al., 2020; Raffel et al., 2020b). These tasks
include but not limited to story generation (See
et al., 2019), dialogue (Budzianowski and Vulic,
2019), summarization (Lewis et al., 2020), Entity
Retrieval (Cao et al., 2021), Question Answering
(Guu et al., 2020), and so on. Similar advances

PHASE IPHASE IISyntax-Flow Language ModelCode GenerationLanguage ModelVariable-Flow Language ModelDocstringSyntax-Flow Variable-Flow  Codehave also been made in Programming Language
relevant tasks.

Programming language tasks, although not con-
sidered as natural language, has been demonstrated
to have great results when it modeled similarly as
natural languages. (Feng et al., 2020) pre-trains on
MLM and replaced-token detection for code under-
standing takss. In (Liu et al., 2020), the authors de-
velop a code completion transformer-based model
by jointly predicting the probability and type of the
next token. For the task of code summarization,
transformer-based models outperforms the other
neural approaches (Yu et al., 2020; Ahmad et al.,
2020); Svyatkovskiy et al., 2020; Liu et al., 2020)
use GPT and UniLM respectively for code com-
pletion. More related to our work, (Husain et al.,
2019; Clement et al., 2020) explore pre-training
methodologies for learning better structural and
syntactical information for automatic code gener-
ation. Moreover, (Wang et al., 2021; Guo et al.,
2021) incorporates Variable-Flows and identiﬁer
information into their pre-training process for bet-
ter code generation performance.

Guided Text Generative Models. Generative
modeling is powerful but often falls short in many
conditions. The behavior of auto-regressive lan-
guage models cannot be explicitly controlled, and
was shown to be very easy to degenerate (Holtz-
man et al., 2020; Welleck et al., 2020; Meister et al.,
2020). This is also the case for code generation.
This prompts researchers to combat this issue by
looking at either the training time or during the de-
coding time. Work in (Fan et al., 2018) constrains
the sample space to top-k tokens in the softmax
logtis to avoid introducing highly unlikely tokens.
(Holtzman et al., 2020) instead restricts the sam-
pling space to the smallest set of space above some
probability mass. Using simple decoding variants
is light-weight to implement, but does not change
the predicted likelihood of each token. (Welleck
et al., 2020) argues that the likelihood objectives is
at fault, and proposes unlikelihood training objec-
tive, which forces lower probabilities on unlikely
generations.

Furthermore, practitioners have also injected
priors or structural information into the language
model for better generation. (Zhang et al., 2020;
Lagutin et al., 2021) utilizes policy learning to con-
trol model behaviors. However, this approach suf-
fers from high variance (Choshen et al., 2020). Re-
cently work in story generation (Yao et al., 2019;

Rashkin et al., 2020; Goldfarb-Tarrant et al., 2020)
uses a plotline/storyline as an intermediate state
for generation. This alleviates the language mod-
els tasks and sets up the model to better learn the
structure of the stories. Being motivated by story
generation, our work injects syntactic and semantic
structural information in a setup that is similar to
this line of works. For the code generation task,
we utilize our proposed Syntax-Flow and Variable-
Flow as the intermediate state to help language
model better understand code’s syntactic and se-
mantic structure information and improve its per-
formance.

3 Method

In this section, we describe our method by intro-
ducing Syntax-Flow and Variable-Flow. Then we
present the generation process of Syntax-Flow and
Variable-Flow. Finally, we present the Python code
generation process guided by Syntax-Flow and
Variable-Flow.

3.1 Syntax-Flow

Unlike other methods that generate code directly
from source input with pre-training, our approach
works in a more detailed way by generating the
structure of the code as an intermediate state ﬁrst,
and then generating the detailed code using the
code structure.

Procedural can be expressed as a formal struc-
ture - Abstract Syntax Tree (AST) for syntactical
correctness. An AST contains two major com-
ponents: STMT (Statement) and Expr (Expres-
sion). STMT describes the general structure of
code including the high-level Python code syntac-
tic constraints. Expr is the detailed content of the
code, mainly including the function variables and
operations. Additionally, there are some special
components in an AST such as the exceptional han-
dler, import alias, arguments, etc.

Due to AST’s formality and rich expressiveness
regarding the syntactic information of code, there
are works that generate AST ﬁrst and then use it
to aid code generation, such as (Yin and Neubig,
2017; Ling et al., 2016), but these works usually re-
quire composite model architecture changes. Also,
the AST is too complex for models to directly gen-
erate information. Due to the size of AST, it is
incompatible with many current transformer-based
language models. As a result, we propose a simpli-
ﬁed version of an AST, namely Syntax-Flow.

Figure 2: An overview of the Syntax-Flow and Variable-Flow Generation

Instead of using the entire tree structure of the
AST, we only extract crucial information including
Indentation, STMT and some parts of Expr. By
doing so, we reduce the complexity of AST but
retain its crucial syntactic structure of Python code,
and avoid the incompatibility issue between AST
and transformer-based language models.

In our proposed Syntax-Flow, there are three
critical components: Indentation, STMT and De-
fault Functions. These three components are
viewed as invariants which means that these com-
ponents should be kept unchanged for maintaining
code’s correct functionality.

3.2 Variable-Flow

Variable-Flow is an another indispensable compo-
nent in automatic code generation task. It can be
effectively applied for maintaining the naming se-
mantics of the code during the code generation
process. (Wang et al., 2021; Guo et al., 2021) use
Variable-Flow during their pre-training process and
achieve good performances on programming lan-
guage relevant tasks. In their works, they extract
function variables names as Variable-Flow which
is integrated into their pre-training process for im-
proving langauage models’ capability on under-
standing the code semantic structure.

In our work, rather than extract variable names
only as Variable-Flow, our Variable-Flow con-
tains: Indentation, variable names, and func-
tion names. Variable names and functions names
In other words,
are uniformly free to change.

Python code’s functionality remains correct regard-
less the changes of these two Variable-Flow’s com-
ponents. Therefore, comparing with Syntax-Flow,
our Variable-Flow contains variant components,
and is more dynamic.

3.3 Phase I - Generation of Syntax-Flow and

Variable-Flow

3.3.1 Generation of Syntax-Flow

Figure 2 shows the Syntax-Flow and Variable-Flow
generation pipeline. With regard to Syntax-Flow,
the numbers represent the number of Indentation
(4 spaces) required at the beginning of the com-
mand line. Then, a STMT (statement) is imme-
diately next to the Indentation, followed by sev-
eral built-ins Exprs (expressions). This feature is
extracted from Python code through AST syntax
visitor method, a method to go through every detail
of the AST nodes recursively and extract all nec-
essary nodes for use, such as FunctionDef, STMT,
exception handler etc, with the count of indentation
at the same time.

The simpliﬁed generation process of Syntax-
Flow is shown in Algorithm 1 Appendix A. For
each line of source code, we generate one line
of Syntax-Flow as you can see in Figure 2. For-
mally we denote the source code to be y =
(y1, y2, ..., yn), and let E = [e1, e2, ..., eL] be the
list of indexes of the newline character. Hence, ye1
would be the ﬁrst line break, and Y1 = (y1, ..., ye1)
would be the ﬁrst line of the source code, Y2 =

(ye1+1, ..., ye2) the second and so on. Then for
each such line of the code, we would generate a
pair a = (t, c). t is the indentation of the current
line of code or number of tabs, and c is the code
logic which includes control ﬂow or function deﬁ-
nitions. In other words, we are looking for

p(ti, ci|Yi) = p(ai|Yi)

(1)

where i denotes the ith line. Both properties
are derived from an AST, which is generated by a
standard toolkit. For more detailed steps, refer to
Algorithm 1 in Appendix A.

3.3.2 Syntax-Flow Language Model

To better learn and utilize the syntactical informa-
tion of the source code, we use the a Syntax-Flow
language model to ﬁrst encode docstrings and then
generate Syntax-Flow. Here we use a pre-trained
auto-regressive language model. We do not do any
additional pre-training, so computing resource is
restricted to a manageable amount. As you can
see in Algorithm 2 in Appendix A, to ﬁne-tune the
language model, we ﬁrst generate AST from the
ground golden source code |y = (y1, y2, . . . , yn).
Then we transform the AST of the source code to
Syntax-Flow in a deterministic process:

a = SYNPARSE(ASTPARSE(y))

(2)

where SYNPARSE stands for Syntax-Flow Parse.
Both SYNPARSE and ASTPARSE are deterministic
functions that generate the Syntax-Flow a. We take
this as our true reference and model the process as
a standard generative task PLMS (ˆai|x, ˆa1...ˆai−1),
namely

ˆa = LMS(x)

(3)

where x is the input (docstring for code gener-
ation). During inference, given a docstring, this
language model is able to generate Syntax-Flow
directly for latter use.

3.3.3 Generation of Variable-Flow

We deﬁne the format the format of Variable-Flow
in our work similar to that of Syntax-Flow as you
can see in Figure 2. For each line of code, it has an
indentation t followed by V = [v1, ..., vj]. V is the
list of Variable-Flow which can be either variable
names or function names. Multiple variable names
can exist in the same line. Its sequential nature
alleviates language model like T5 during the gener-
ation process. Similar to the setup of Syntax-Flow,

we are looking for

p(ti, Vi|Yi) = p(bi|Yi)

(4)

where Yi is the ith line of source code and bi =

(ti, Vi).

3.3.4 Variable-Flow Language Model
We generate Variable-Flow from source code y =
(y1, y2, . . . , yn). Then we can safely extract
Variable-Flow deterministically from AST:

b = VARPARSE(ASTPARSE(y))

(5)

where VARPARSE stands for Variable-Flow Parse.
We take this as our true reference and ﬁne-tune
the Variable-Flow Language Model on the source
code and Variable-Flow pairs. Speciﬁcally, we are
modeling PLMV (ˆbi|x, ˆb1...ˆbi−1). Hence,

ˆb = LMV (x)

(6)

for the ith line. At inference time, the model
is expected to generate Variable-Flow for latter
models to encode.

3.4 Phase II – Generation of Code

Code generation is built on the same language
model as Syntax-Flow and Variable-Flow language
model. However, unlike the generation of Syntax-
Flow or Variable-Flow, an issue in the code gener-
ation task is that the length of code description is
usually much shorter than the length of generated
code. For example, in the CodeSearchNet dataset,
many function code data length is over 128 tokens
while the description only has average length is
about 50 tokens per sequence. This means that
the input information is limited and not enough to
generate plausible code unless the language model
is available to have more prepared features dur-
ing code generation process. For this reason, we
use the information generated from Phase I as in-
termediate features to guide the language model
generating Python code in Phase II.

3.4.1 Guided Code Generation Language

Model

Having the docstring and the corresponding Syntax-
Flow and the Variable-Flow, this Code Generation
Language Model depends on these three sequences,
and generates source code. The language model
is obtained from a pre-trained auto-regressive lan-
guage model T5. In our work, we use the T5-based

language model as our guided Code Generation
Language Model LMG.

ˆy = LMG(x, LMS(x), LMV (x))

(7)

The Guided Code Generation Language Model
takes in the input docstrings x as well as the outputs
of the Syntax-Flow Language Model and Variable-
Flow Language Model.

4 Experiment

In this section, we present our our experiment in
detail. First, we introduce the datasets we use and
our data processing approach in our experiment.
Then, we present our experiment setup. Finally, we
introduce our evaluation metrics in the last subsec-
tions.

4.1 Datasets
Code Search Net (CSN)2 (Husain et al., 2019)
is collected from publicly available open-source
non-forked GitHub repositories. Only projects that
are referenced by at least one other project are
included. The original paper ﬁlters around 500k
code-documentation pairs for Python. They re-
moved pairs where either the documents are less
than 3 words or method less than 3 lines. They also
removed duplicate code, constructor and extension
methods. After processing, there are 412k training
data, 22k validation data and 22k test data.

Edinburgh

Code-to-Docstring

dataset
(CDC)3 (Barone and Sennrich, 2017) is a parallel
Python function-to-docstring corpus collected
The Edinburgh
and processed from Github.
Code-to-Docstring dataset contains 150,370 triples
of function declarations, docstrings and bodies in
the main parallel corpus. This parallel corpus is
partitioned into training/ validation/ testing data, in
which the training data contains 109,108 training
data, 2,000 validation data and 2000 testing data.
CodeSearchNet AdvTest (Adv)4 (Lu et al.,
2021) is a Python dataset derived from the Code-
SearchNet (CSN) corpus. The individual exam-
ple in CodeSearchNet AdvTest is designed for the
code search task. (Lu et al., 2021) took the ﬁrst
paragraph of the docstring as the query for the cor-
responding Python function. The function names
and variables are replaced by special tokens, which

2https://github.com/github/CodeSearchNet
3https://github.com/EdinburghNLP/code-docstring-

corpus

4https://github.com/microsoft/CodeXGLUE

we recover back with original variables name. The
CodeSearchNet Advtest dataset contains 251,820
training data, 9,640 validation data, and 19,210
testing data.

4.2 Data Processing

In our experiment, we process our data in 3 steps.
(1) Clean up Raw Code: All Python 2 code is con-
verted to Python 3 using package 2to35, and all
Python code styles remain consistent with package
pep86. We also remove all invalid code samples
that cannot be parsed to AST. After cleaning up
the raw code, 99.92% code data is remaining. (2)
Remove comments and docstrings: Comments and
docstrings are removed from the code, since these
will not be predicted. (3) Replace indentation and
newline: Indentation and newline is critical for gen-
eration a structured Python code. In our work, we
replace them with symbol § for Indentation and δ
for newline.

4.3 Experimental Set Up

In the both Phases, we use T5-based models. For
Phase I and II, the code description is the main
source inputs for the encoder.

Encoding Setup. We use the AdamW optimizer
for all the T5 models and assign learning rate 1e-
4 for Phase I and Phase II. The training step for
Phase I is kept at 75K and batch size at 32. The
training step for Phase II is kept at 100K and batch
size at 32.The learning scheduler is inverse root
square and has warm-up step of 5000 for phase I
and 10000 for Phase II.

Decoding Setup. Both Phase I and Phase II take
length 512 as input and have output length of 128
for phase I and 256 for Phase II. Beam size is 5 for
all Phases ﬁne-tuning. We add repetition penalty
of 2 for Syntax-Flow and Variable-Flow generation
considering the case that repeated statements occur
frequently. All the tasks are run on the two Nvidia
GeForce A6000 with 48GB GPU memory each.

Evaluation. For our experiment evaluation, we
use the metrics BLEU (Papineni et al., 2002),
ROUGE (Lin, 2004) and CodeBLUE (Ren et al.,
2020). BLUE and Rouge one of the most common
metrics to evaluate generated text. CodeBLUE is a
metric speciﬁcally designed for the evaluation of
generated programming languages. Apart from the
similarity of the tokens, it also considers the syntax
of commands and logic.

5https://pypi.org/project/2to3/
6https://pypi.org/project/autopep8/

CSN Syntax-Flow
CSN Variable-Flow
CDC Syntax-Flow
CDC Variable-Flow
Adv Syntax-Flow
Adv Variable-Flow
Avg Syntax-Flow
Avg Variable-Flow

Rouge1 Rouge2 RougeL BLEU
12.69
11.38
15.16
11.94
13.61
11.04
13.82
11.45

47.7
33.7
50.4
34.8
48.9
34.0
49.0
34.2

49.1
36.7
51.8
37.4
50.4
37.3
50.4
37.1

35.8
15.7
41.4
18.9
36.9
15.6
38.1
16.7

Table 1: The results of Syntax-Flow and Variable-Flow generation for all three datasets in Phase I.

Rouge1 Rouge2 RougeL BLEU CodeBLEU

CSN
CDC
Adv
Avg

30.8
32.3
29.8
31.0

11.9
15.7
11.0
12.8

27.6
29.3
26.7
27.8

20.86
22.62
20.74
21.41

21.64
22.39
20.91
21.65

Table 2: The results of Python code generation for CSN, CDC and Adv in Phase II.

5 Results and Analysis

ment.

In this section, we ﬁrst present our Phase I exper-
iment results, which contain the performance of
Syntax-Flow and Variable-Flow generation on the
CSN, CDC, and Adv Test datasets correspondingly.
Then, we present our Phase II experiment results
on CSN, CDC, and Adv Test datasets. We train our
models on each dataset’s training data, and run eval-
uations on the corresponding testing data. Finally,
we compare our approach’s performance on auto-
matic Python code generation task with previous
works.

5.1 Results of Phase I

Syntax-Flow Results. We ﬁrst show our results on
generating Syntax-Flow using T5 language model.
We evaluate the generated Syntax-Flow with Rouge
and BLEU metrics, as you can see in Table 1. The
Syntax-Flow performance of CSN, CDC, and Adv
is around 50% in Rouge-F1 and Rouge-F2, and
over 35% in Rouge-F2. These results are good
considering the real vocabulary used in Syntax-
Flow are relative small and syntax tokens are gener-
ally similar. When we make a comparison among
three corpus, results from CDC is slightly better
than that of Adv and CSN for all the metrics con-
sistently. CDC is a well organized dataset that’s
speciﬁcally designed for Python automatic code
generation task. Considering Adv is derived from
CSN thus more organized, there is only about 1.3%
in Rouge score and about 1% in BLEU improve-

Variable-Flow Results. We evaluate our gen-
erated Variable-Flow results from code docstrings
using the Rouge and BLEU metrics. Our evalua-
tion results regarding the generated Variable-Flow
is shown in the Table 1. Similar to the results in
Syntax-Flow, the performance of Variable-Flow in
CDC is slightly better than the other two datasets
for all the metrics scores. The average results of the
Variable-Flow is not as good as that of Syntax-Flow
because the generation of Variable-Flow variant
components is much more difﬁcult than the Syntax-
Flow invariant components. Moreover, over 95%
of Syntax-Flow samples’ length are shorter than
125 tokens. The Rouge F1 is over 35% and Rouge
F2 is over 15% on average.

5.2 Results of Phase II

From the Table 2, we can observe the performance
of ﬁnal Python code generation with Rouge, BLEU
and CodeBLEU. The result of CDC is the best
among three corpus because of its cleaner data as
well as the effect of better Phase I performance
(Syntax-Flow and Variable-Flow). CSN’s results
were slightly better than Adv’s since CSN had
about twice as much training data as Adv. As
we can see from the Figure 3, the performance
of GAP-Gen slightly outperforms the T5 model
that’s directly trained to generate Python code for
both Rouges and BLEU metrics. It indicates that
our pipeline approach is effective in improving
the Python code generation. Similar conclusion

Rouge1 Rouge2 RougeL BLEU CodeBLEU

GPT2 (Clement et al., 2020)
PyMT5 (Clement et al., 2020)
T5
GAP-Gen T5
CodeT5
GAP-Gen CodeT5

20.9
28.4
30.4
30.8
34.6
34.7

7.6
13.5
11.7
11.9
14.6
14.5

21.9
24.8
27.4
27.6
30.2
30.2

2.8
8.59
20.71
20.86
21.61
21.86

–
–
21.67
21.68
23.36
23.49

Table 3: The results of GAP-Gen with other models ﬁne-tuning on CSN datasets for Python code generation task.

can be proved by ﬁne-tuning the CodeT5 language
model with our GAP-Gen training pipeline. We
apply our training pipeline with CodeT5 in Phase II
and show that GAP-Gen CodeT5 achieves the best
Rouge, BLEU and CodeBLEU scores comparing
with other models on the same ﬁne-tuning task.

There is a large gap between GAP-Gen and
PyMT5 on BLEU and CodeBLEU, which is be-
cause PyMT5 generates sequence with max tokens
1024. But we let the max target length to be 256,
which covers about 75% of codes lengths. But
based on our comparison between T5 and GAP-
Gen, the results of GAP-Gen has improvement due
to the pre-requisite of Syntax-Flow and Variable-
Flow generation.

5.3 Discussion

Unlike other works focused on pre-training, we
design a pipeline approach to achieve a better ﬁne-
tuning result. Given the same training conﬁgura-
tion, our results prove that there is a certain im-
provement of the approach using docstring, Syntax-
Flow and Variable-Flow together than that of using
the docstring only. Code generation is a translation
task but has its own difﬁculties. First, our docstring
inputs are usually very short, while code outputs
are long. For example, there is about 85% of the
input sequences in CSN, CDC and Adv are less
than 128 tokens while over a half of codes that are
longer than 128 tokens. Moreover, code has stricter
syntax and less ambivalent semantics. Our pipeline,
by dividing the load of generating syntax and se-
mantic information to multiple language models,
bypasses the above difﬁculties and achieves better
generation results.

The data leaking issue exists in many previous
works using the pre-training technique on the auto-
matic code generation task. For example, in previ-
ous work (Clement et al., 2020), the dataset Code-
SearchNet used for ﬁne-tuning overlaps with their
data used for pre-training. Both of them are col-

lected from the public github repositories. Data
leaking will tend to result in high performance on
the ﬁne-tuning task but usually is dubious in prac-
tice because model should generalize on the unseen
data. In our work, we ﬁne-tune our model using
T5 which is not pre-trained on existing Code-to-
Docstring datasets. Hence, T5 does not have the
data leaking problem. However, CodeT5 is pre-
trained on the CSN dataset, which may lead to the
data leaking problem in code generation task. This
can be the reason that CodeT5 alone without us-
ing our training pipeline can achieve very good
results. However, after we ﬁne-tune CodeT5 using
our training pipeline, CodeT5 shows better perfor-
mance on the Python code generation task, as you
can see in Table 2.

At the same time, due to the computational re-
sources limitation, the maximum batch size we can
use is 32. Although we are limited by the compu-
tational resources, we still achieve very promising
results in the code generation task. In the future,
better computational resources would probably in-
crease the performance further.

6 Conclusions

In this work, we demonstrate the effectiveness of
injecting Python syntactic and semantic informa-
tion to the code generation tasks. We design and
implement two different types of information com-
ponents: Syntax-Flow and Variable-Flow. To in-
corporate these information, we encode them using
separate language models and then feed them along
with the docstring input into the ﬁnal language
model. Pre-trained language models ﬁne-tuned
with our proposed pipeline shows better perfor-
mances over state-of-the-art code generation mod-
els. For future directions, new strategies of incor-
porating those information can be explored.

References

Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi
Ray, and Kai-Wei Chang. 2020. A transformer-
based approach for source code summarization.
ArXiv, abs/2005.00653.

Bruce W. Ballard and Alan W. Biermann. 1979. Pro-
gramming in natural language: “nlc” as a prototype.
In ACM ’79.

Antonio Valerio Miceli Barone and Rico Sennrich.
2017. A parallel corpus of python functions and
documentation strings for automated code documen-
tation and code generation. In IJCNLP.

Paweł Budzianowski and Ivan Vulic. 2019. Hello, it’s
gpt-2 - how can i help you? towards the use of pre-
trained language models for task-oriented dialogue
systems. In EMNLP.

Nicola De Cao, Gautier Izacard, Sebastian Riedel, and
Fabio Petroni. 2021. Autoregressive entity retrieval.
ArXiv, abs/2010.00904.

Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri
Abend. 2020. On the weaknesses of reinforce-
ment learning for neural machine translation. ArXiv,
abs/1907.01752.

Colin B. Clement, Dawn Drain, Jonathan Timcheck,
Alexey Svyatkovskiy, and Neel Sundaresan. 2020.
Pymt5: Multi-mode translation of natural
lan-
guage and python code with transformers. ArXiv,
abs/2010.03150.

Aditya Desai, Sumit Gulwani, Vineeta Lokhande Hin-
gorani, Nidhi Jain, Amey Karkare, Mark Marron,
R Sailesh, and Subhajit Roy. 2016. Program synthe-
sis using natural language. 2016 IEEE/ACM 38th
International Conference on Software Engineering
(ICSE), pages 345–356.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In NAACL.

Kawin Ethayarajh. 2019. How contextual are contextu-
alized word representations? comparing the geome-
try of bert, elmo, and gpt-2 embeddings. In EMNLP.

Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-

erarchical neural story generation. In ACL.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-
bert: A pre-trained model for programming and nat-
ural languages. ArXiv, abs/2002.08155.

Seraphina Goldfarb-Tarrant,

Tuhin Chakrabarty,
Ralph M. Weischedel, and Nanyun Peng. 2020.
Content planning for neural story generation with
aristotelian rescoring. In EMNLP.

Sumit Gulwani and Mark Marron. 2014. Nlyze: inter-
active programming by natural language for spread-
sheet data analysis and manipulation. In SIGMOD
Conference, pages 803–814. ACM.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,
Duyu Tang, Shujie Liu, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano,
Shao Kun Deng, Colin Clement, Dawn Drain, Neel
Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou.
2021. Graphcodebert: Pre-training code representa-
tions with data ﬂow.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Ming-Wei Chang. 2020. Realm: Retrieval-
ArXiv,
augmented language model pre-training.
abs/2002.08909.

Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin
Choi. 2020. The curious case of neural text degener-
ation. ArXiv, abs/1904.09751.

Hamel Husain, Hongqi Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. Code-
searchnet challenge: Evaluating the state of seman-
tic code search. ArXiv, abs/1909.09436.

Sarthak Jain and Byron C. Wallace. 2019. Attention is

not explanation. In NAACL.

Aditya Kanade, Petros Maniatis, Gogul Balakrishnan,
and Kensen Shi. 2020. Learning and evaluating con-
textual embedding of source code.

Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to for-
mal languages. In AAAI, pages 1062–1068. AAAI
Press / The MIT Press.

Olga Kovaleva, Alexey Romanov, Anna Rogers, and
Anna Rumshisky. 2019. Revealing the dark secrets
of bert. ArXiv, abs/1908.08593.

Evgeny Lagutin, Daniil Gavrilov, and Pavel Kalaidin.
2021.
Improving
Implicit unlikelihood training:
neural text generation with reinforcement learning.
ArXiv, abs/2101.04229.

Vu Le, Sumit Gulwani, and Zhendong Su. 2013. Smart-
synth: synthesizing smartphone automation scripts
from natural language. In MobiSys ’13.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. Bart: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. ArXiv, abs/1910.13461.

Chin-Yew Lin. 2004. Rouge: A package for automatic

evaluation of summaries. In ACL 2004.

Wang Ling, Phil Blunsom, Edward Grefenstette,
Karl Moritz Hermann, Tomás Kociský, Fumin
Latent
Wang, and Andrew W. Senior. 2016.
ArXiv,
predictor networks for code generation.
abs/1603.06744.

Greg Little and Rob Miller. 2006. Translating keyword

commands into executable code. In UIST.

F. Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-
task learning based pre-trained language model for
code completion. 2020 35th IEEE/ACM Interna-
tional Conference on Automated Software Engineer-
ing (ASE), pages 473–485.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-
dong Zhou, Linjun Shou, Long Zhou, Michele Tu-
fano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-
daresan, Shao Kun Deng, Shengyu Fu, and Shujie
Liu. 2021. Codexglue: A machine learning bench-
mark dataset for code understanding and generation.
ArXiv, abs/2102.04664.

Clara Meister, Tim Vieira, and Ryan Cotterell. 2020. If
beam search is the answer, what was the question?
ArXiv, abs/2010.02650.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.

David Price, Ellen Riloff, Joseph L. Zachary, and Bran-
don Harvey. 2000. Naturaljava: a natural language
interface for programming in java. In IUI ’00.

Chris Quirk, Raymond J. Mooney, and Michel Galley.
2015. Language to code: Learning semantic parsers
for if-this-then-that recipes. In ACL.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020a. Exploring the limits
of transfer learning with a uniﬁed text-to-text trans-
former.

Colin Raffel, Noam M. Shazeer, Adam Roberts,
Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. 2020b. Ex-
ploring the limits of transfer learning with a uniﬁed
text-to-text transformer. ArXiv, abs/1910.10683.

K. M. Tahsin Hassan Rahit, Rashidul Hasan Nabil, and
Md Hasibul Huq. 2019. Machine translation from
natural language to code using long-short term mem-
ory. ArXiv, abs/1910.11471.

Hannah Rashkin, Asli Çelikyilmaz, Yejin Choi, and
Jianfeng Gao. 2020.
Plotmachines: Outline-
conditioned generation with dynamic plot state
tracking. In EMNLP.

Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shu-
jie Liu, Duyu Tang, M. Zhou, Ambrosio Blanco,
and Shuai Ma. 2020. Codebleu: a method for
ArXiv,
automatic evaluation of code synthesis.
abs/2009.10297.

A. See, Aneesh S. Pappu, Rohun Saxena, Akhila
Yerukola, and Christopher D. Manning. 2019. Do
massively pretrained language models make better
storytellers? ArXiv, abs/1909.10705.

Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
Intellicode compose:
and Neel Sundaresan. 2020.
In Proceed-
Code generation using transformer.
the 28th ACM Joint Meeting on Euro-
ings of
pean Software Engineering Conference and Sympo-
sium on the Foundations of Software Engineering,
ESEC/FSE 2020, page 1433–1443, New York, NY,
USA. Association for Computing Machinery.

Ashish Vaswani, Noam M. Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. 2017. Atten-
tion is all you need. ArXiv, abs/1706.03762.

Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2018.
Glue: A multi-task benchmark and analysis plat-
form for natural language understanding. ArXiv,
abs/1804.07461.

Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven
C. H. Hoi. 2021. Codet5: Identiﬁer-aware uniﬁed
pre-trained encoder-decoder models for code under-
standing and generation.

Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-
nan, Kyunghyun Cho, and Jason Weston. 2020. Neu-
ral text generation with unlikelihood training. ArXiv,
abs/1908.04319.

Lili Yao, Nanyun Peng, Ralph M. Weischedel, Kevin
Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-
and-write: Towards better automatic storytelling.
ArXiv, abs/1811.05701.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In ACL.

Xiaohan Yu, Quzhe Huang, Zongge Wang, Yansong
Feng, and Dongyan Zhao. 2020. Towards context-
aware code comment generation. In FINDINGS.

Yuhao Zhang, Derek Merck, Emily B Tsai, Christo-
pher D. Manning, and C. Langlotz. 2020. Optimiz-
ing the factual correctness of a summary: A study of
summarizing radiology reports. In ACL.

A Appendix

Algorithm 1 Generate Syntax-Flow & Variable-
Flow
Require:

x = (x1, x2, ..., xn) ∈ X: input docstring
LMS: Language Model being used for Syntax-
Flow
LMV : Language Model being used for
Variable-Flow

Ensure:

A = (a1, a2, ..., an): Syntax-Flow
B = (b1, b2, ..., bk): k Variable-Flow

1: Initialize A, B to be empty arrays
2: for each docstring:x ∈ X do
3:

4:

5:

a ← LMS(x)
b ← LMV (x)
Append(A,a)
Append(A,a)

6:
7: end for
8: return A,B

Algorithm 2 Training Syntax-Flow Language
Model
Require:

x = (x1, x2, ..., xn) ∈ X: input docstring
LMS: Pre-trained Language Model being used
for Syntax-Flow

Ensure:

LMS: Language Model ﬁne-tuned for gener-
ating Syntax-Flow

1: Initialize D to be empty array
2: for each docstring:x ∈ X do
3:

p ← ASTPARSE(X) AST parsed by stan-

4:

dard Python AST parser
d ← SYNPARSE(p)
Append(D,d)

5:
6: end for
7: for i = 1 to |X| do
8:

a(cid:48) ← LMS(X[i])
l = loss(D[i], a(cid:48))
LMS.backwards(l)

9:

10:
11: end for
12: return LMS

