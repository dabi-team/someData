Decomposing reverse-mode automatic diﬀerentiation

Roy Frostig, Matthew J. Johnson, Dougal Maclaurin, Adam Paszke, and Alexey Radul∗

1
2
0
2

y
a
M
0
2

]
L
P
.
s
c
[

1
v
9
6
4
9
0
.
5
0
1
2
:
v
i
X
r
a

Abstract

We decompose reverse-mode automatic dif-
ferentiation into (forward-mode) lineariza-
tion followed by transposition.
Doing
so isolates the essential diﬀerence between
forward- and reverse-mode AD, and simpli-
ﬁes their joint implementation.
In partic-
ular, once forward-mode AD rules are de-
ﬁned for every primitive operation in a source
language, only linear primitives require an
additional transposition rule in order to ar-
rive at a complete reverse-mode AD imple-
mentation. This is how reverse-mode AD is
written in JAX [BFH+18, FJL18] and Dex
[MRJV19, PJD+21].

Automatic diﬀerentiation (abbreviated AD or au-
todiﬀ) comes in several “modes”—algorithmic vari-
ants that present an eﬃciency tradeoﬀ. The most
common two are forward- and reverse-mode AD. At
their core, these compute slightly diﬀerent functions:
forward-mode AD carries out a Jacobian-vector prod-
uct (JVP), while reverse-mode AD carries out a vector-
Jacobian product (VJP), both for some vector taken
as input. To produce a Jacobian matrix, one might
invoke either mode over each standard basis vec-
tor.

Naturally, the two modes share apparent similarities,
and maintainers of AD tools might wonder about any
redundancy to avoid or to exploit in their implemen-
tation. Typically, implementing either mode requires
deﬁning a corresponding transfer rule for each base
primitive operation in the source language (e.g., for
element-wise array division, for matrix multiplication,
for array slicing, etc). Does maintaining both modes
require two complete, distinct collections of transfer
rules? The thought that it might has led industry-
grade tools such as PyTorch [PGM+19] to oﬀer only
one mode (reverse). JAX [BFH+18, FJL18] and Dex
[MRJV19, PJD+21] oﬀer both modes by reusing the
forward-mode transfer rules in reverse-mode AD ac-
cording to this note.

Our contribution is to decompose reverse-mode AD
into two steps:
linearization and transposition. Lin-
earization constructs a JVP function, by standard

∗Google Research

Presented at LAFI 2021 at POPL, 17 January 2021

forward-mode AD. It will be important that this JVP
consist of only linear primitives, which we ensure
with partial evaluation, a standard general-purpose
compiler technique. Transposition corresponds to the
mathematical notion of transposition of a linear map,
which we implement as a new program transforma-
tion that boils down to transposing linear primitives.
Transposition is an interesting operation in its own
right—the class of linear programs can be viewed as
a ﬂexible representation for structured sparsity of lin-
ear maps, and this program transformation achieves
mathematical transposition of the matrix without de-
stroying the sparsity pattern.

This decomposition underscores that transposition
is what fundamentally relates forward- and reverse-
mode, as the terms JVP and VJP suggest. A mono-
lithic VJP transformation must: (1) linearize primitive
operations, (2) compute the primal output while stor-
ing intermediate values, and (3) reverse the order of
operations. We instead treat each of these steps as a
standalone transformation. The linearization step en-
sures that transposition rules need only be deﬁned for
linear primitives in order to arrive at a complete AD
system.

1 Reverse from forward

For clarity, we will use a Haskell-like notation for types
in the sequel. We use the type constructor T for “tan-
gent space”, writing T a for the tangent space of a.
We do not distinguish tangent spaces from cotangent
spaces, because the ones that can occur in a realizable
computation are canonically isomorphic.

In this notation, the jvp and vjp transformations have
the types:

jvp :: (a -> b) -> (a, T a) -> (b, T b)
vjp :: (a -> b) -> (a, T b) -> (b, T a)

In both cases, the returned functions still compute
the primal mapping f :: a -> b (ﬁrst component
of the argument and result), but they are additionally
augmented with a linear transform over the tangent
types (given by the second input and result). While
jvp computes the directional derivative of the orig-
inal mapping, vjp computes the transpose, namely
directional gradients (as the name suggests). We
capture the similarity between jvp and vjp with a
new function transformation we call transpose, with

 
 
 
 
 
 
type:

transpose :: (a -o b) -> (b -o a)

Here the -o means a structurally linear function from
a to b. We do not give a precise deﬁnition of “struc-
turally linear” in this note, but the idea is that the
function should be implemented with only linear prim-
itives, and should preserve linearity through all sub-
computations by using the input exactly once in each
additive term of the output.

The operation of transpose is to accept a structurally
linear function that implements a linear map, and re-
turn a structurally linear function representing the
transpose of the same map. It has been shown that this
can be derived automatically, using techniques similar
to AD [Pip09]. To take advantage of this, we need
to isolate the transposable linear map from jvp, by
slightly reformulating as linearize:

linearize ::

(a -> b) -> a -> (b, T a -o T b)

The diﬀerence from jvp is that linearize does not
compute the primal and derivative functions at the
same time, but instead only computes the primal part
and returns the linear derivative function as a result.
Fortunately, the diﬀerence in type signatures between
jvp and linearize is just the eﬀect of partially eval-
uating jvp f with respect to the a argument, relying
on the fact that the primal result (of type b) depends
only on the primal input and not on the direction of
diﬀerentiation.

Partial evaluation is a well-studied technique used in
optimizing compilers. In brief, given the code of some
function g, and a known value for one of its inputs, we
can, at function transformation time, compute all the
intermediate values that depend on just the known in-
put (in this case, the primal point). The remainder of
the computation we stage into a new, generally simpler
and faster—and, in this case, linear—function that ac-
cepts the remaining arguments of g.

That last comment about linearity is important, and
is a critical thing we gain from true partial evaluation
as opposed to just closing over the a input. The par-
tial evaluation computes and stores everything that
depends only on the primal, and the nature of dif-
ferentiation implies that all the residuals are, in fact,
structurally linear in exactly the way they need to be
for transpose to work. Partial evaluation is of course
also critical for the performance of reverse-mode AD,
because it prevents potentially wasteful recomputation
of the primal values on the reverse pass. To summa-
rize:

linearize f =

let ft = jvp f

in partial_eval ft

With linearize in hand we can deﬁne vjp con-
cisely:

vjp f (x, ty) =

let (y, ft) = linearize f x

ftt

= transpose ft

in (y, ftt ty)

and we have achieved our goal of writing diﬀerentiation
rules for our primitives only once. We still need to pro-
vide a jvp rule for each basic operation in our system,
but the rule set necessary to implement transpose
is signiﬁcantly smaller, as it only considers the linear
operations. The partial eval transformation can be
implemented generically for all operations—the usual
interpretation is that the output is known if and only if
all inputs are known. The only place where care needs
to be taken is around control ﬂow constructs, but there
are usually very few in any language. In fact, many
tracing mechanisms used for AD erase control ﬂow en-
tirely.

2 Related work and discussion

The decomposition laid out in this note may be known
in, or implied by, folklore among some AD experts,
but we have not found it in writing, and neither have
many of our colleagues in the research community.
This has been the implementation approach in JAX
since before the project’s initial open-source release
in 2018. Several AD tools implement only one mode
[ABC+16, PGM+19], or implement both modes us-
ing two rule sets [BPS16, IEF+19]. Work by Elliott
touches peripherally on transposition in [Ell18] in its
discussion of duality and gradients. AD surveys and
textbooks [BPRS17, GW08] commonly observe the
fundamental bit that forward-mode AD computes a
JVP and that reverse-mode AD computes its trans-
pose (a VJP).

It is tempting to try to quantify the cost savings due
to a smaller rule set.
In JAX the number of trans-
position rules is roughly 40% of the JVP rule count.
Still, such a quantity should be read qualitatively: it
depends on the choice of language primitives, whose
design is determined by concerns extrinsic to AD. For
instance, JAX oﬀers just-in-time compilation, aﬀord-
ing it a minimal primitive set of roughly 120 operations
(compare to ∼400 primitives in PyTorch, and several
hundreds or more in TensorFlow [ABC+16]). Addi-
tionally, classes of similar primitives’ rules are merely
instances of a common rule template (e.g. for binary
operations that are linear in both operands); here we
have counted them individually. Some primitives (e.g.,
division) are only linear with respect to some of their
operands, obscuring the notion of a precise rule count.
Finally, fewer rules are not always simpler rules.

Dex:

Vytiniotis.
array programming
with typed indices. NeurIPS workshop:
for Machine
Program Transformations
Learning, 2019.

[PGM+19] Adam Paszke, Sam Gross, Francisco
Massa, Adam Lerer, James Bradbury,
Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Al-
ban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison,
Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. PyTorch: An imper-
ative style, high-performance deep learn-
ing library. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alch´e-Buc, E. Fox,
and R. Garnett, editors, Advances in Neu-
ral Information Processing Systems 32,
pages 8024–8035. Curran Associates, Inc.,
2019.

[Pip09] Dan Piponi. Two tricks for the price of
one: Linear ﬁlters and their transposes.
Journal of Graphics, GPU, and Game
Tools, 14(1):63–72, 2009.

[PJD+21] Adam Paszke, Daniel Johnson, David
Duvenaud, Dimitrios Vytiniotis, Alexey
Radul, Matthew Johnson,
Jonathan
and Dougal Maclaurin.
Ragan-Kelley,
Getting to the point.
index sets and
parallelism-preserving autodiﬀ for point-
ful array programming.
arXiv preprint
arXiv:2104.05372, 2021.

References

[ABC+16] Martin Abadi, Paul Barham, Jianmin
Chen, Zhifeng Chen, Andy Davis, Jeﬀrey
Dean, Matthieu Devin, Sanjay Ghemawat,
Geoﬀrey Irving, Michael Isard, Manjunath
Kudlur, Josh Levenberg, Rajat Monga,
Sherry Moore, Derek G. Murray, Benoit
Steiner, Paul Tucker, Vijay Vasudevan,
Pete Warden, Martin Wicke, Yuan Yu, and
Xiaoqiang Zheng. TensorFlow: A system
for large-scale machine learning.
In 12th
USENIX Symposium on Operating Sys-
tems Design and Implementation (OSDI
16), pages 265–283, 2016.

[BFH+18] James Bradbury, Roy Frostig, Peter
Hawkins, Matthew James
Johnson,
Chris Leary, Dougal Maclaurin, George
Jake Vander-
Necula, Adam Paszke,
Plas, Skye Wanderman-Milne, and Qiao
Zhang. JAX: composable transformations
of Python+NumPy
2018.
programs,
http://github.com/google/jax.

[BPRS17] Atılım G¨unes Baydin, Barak A Pearlmut-
ter, Alexey Andreyevich Radul, and Jef-
frey Mark Siskind. Automatic diﬀerentia-
tion in machine learning: a survey. The
Journal of Machine Learning Research
(JMLR), 18(1):5595–5637, 2017.

[BPS16] Atılım G¨une¸s Baydin, Barak A Pearlmut-
ter, and Jeﬀrey Mark Siskind. DiﬀSharp:
An AD library for .NET languages. arXiv
preprint arXiv:1611.03423, 2016.

[Ell18] Conal Elliott. The simple essence of auto-
matic diﬀerentiation. In Proceedings of the
ACM on Programming Languages (ICFP),
2018.

[FJL18] Roy Frostig, Matthew James Johnson, and
Chris Leary. Compiling machine learning
programs via high-level tracing. Machine
Learning and Systems (MLSys), 2018.

[GW08] Andreas Griewank and Andrea Walther.
Evaluating derivatives:
principles and
techniques of algorithmic diﬀerentiation.
SIAM, 2008.

[IEF+19] Mike Innes, Alan Edelman, Keno Fischer,
Chris Rackauckus, Elliot Saba, Viral B
Shah, and Will Tebbutt. A diﬀerentiable
programming system to bridge machine
learning and scientiﬁc computing. arXiv
preprint arXiv:1907.07587, 2019.

[MRJV19] Dougal Maclaurin,

Matthew J. Johnson,

Alexey

Radul,
and Dimitrios

