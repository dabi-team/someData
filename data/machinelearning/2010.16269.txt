0
2
0
2

v
o
N
4

]

G
L
.
s
c
[

3
v
9
6
2
6
1
.
0
1
0
2
:
v
i
X
r
a

The Combinatorial Multi-Bandit Problem
and its Application to Energy Management∗

Tobias Jacobs*, Mischa Schmidt*, S´ebastien Nicolas*, and Anett
Sch¨ulke*

* NEC Laboratories Europe, Heidelberg, Germany

Abstract

We study a Combinatorial Multi-Bandit Problem motivated by ap-
plications in energy systems management. Given multiple probabilistic
multi-arm bandits with unknown outcome distributions, the task is to op-
timize the value of a combinatorial objective function mapping the vector
of individual bandit outcomes to a single scalar reward. Unlike in single-
bandit problems with multi-dimensional action space, the outcomes of the
individual bandits are observable in our setting and the objective function
is known. Guided by the hypothesis that individual observability enables
better trade-oﬀs between exploration and exploitation, we generalize the
lower regret bound for single bandits, showing that indeed for multiple
bandits it admits parallelized exploration. For our energy management
application we propose a range of algorithms that combine exploration
principles for multi-arm bandits with mathematical programming. In an
experimental study we demonstrate the eﬀectiveness of our approach to
learn action assignments for 150 bandits, each having 24 actions, within
a horizon of 365 episodes.

1

Introduction

We study a cooperative multi-agent learning problem motivated by applica-
tions in energy systems management. Consider an interconnected system with
energy producers, consumers, and storage components. Management units are
responsible to steer the behavior of diﬀerent subsets of system components, con-
tributing to the overall goal to balance energy supply and demand while keeping
the system state within its operational constraints. Large-scale energy systems

∗This project has received funding from the European Union’s Horizon 2020 research and
innovation programme under grant agreement No 691735. The content of this article does
not reﬂect the oﬃcial opinion of the European Union. Responsibility for the information and
views expressed in the article lies entirely with the authors.

1

 
 
 
 
 
 
management is typically realized in a hierarchical fashion with higher-level man-
agement distributing and delegating targets.

From the viewpoint of a particular management unit, a multitude of n sys-
tem components, or subsystems, is to be operated. Each component can receive
a range of instructions (e.g. management policies), and the outcome of the in-
struction can be observed (e.g. by the load curve of the component). Among
the possible management goals are balancing supply and demand, reducing peak
load, and reducing consumption during a given target time interval. While the
level of fulﬁllment of the management goal can be calculated from the observed
behavior of the involved system components, the inﬂuence of the instructions
on the behavior follows a probability distribution that is unknown to the man-
agement unit and can only be learned by experience.

The above management task is an application of a general multi-bandit
problem. Each system component corresponds to a multi-arm bandit or actor,
the range of instructions corresponds to the arms or actions of the bandit, and
the observed behavior is the stochastic result of pulling an arm. The reward
is the level of fulﬁllment of the management goal, which can be computed by
a known function mapping the n behavior observations (e.g. load curves) to a
scalar.

Our contribution To the best of our knowledge we are the ﬁrst to study the
variant of the combinatorial bandit problem where the reward function com-
bines the output of several bandits. The formal problem deﬁnition is given in
Section 2. We generalize the lower regret bound of Lai and Robbins [9] to the
Combinatorial Multi-Bandit scenario. The generalized bound is established in
Section 3 by the requirement to apply suboptimal actions of all actors with a
distribution-dependent frequency, but with the possibility to do this in parallel
for diﬀerent actors. We describe how the problem is applied to energy systems
management in Section 4 and give a range of action selection algorithms com-
bining bandit exploration schemes with mathematical programming. In an ex-
perimental study presented in Section 5, the algorithms are applied to optimize
a load shifting objective in a setup with 150 energy consumers. The experiments
demonstrate the ability of our approach to parallelize the exploration process,
ﬁnding good action assignments within 365 episodes.

The Combinatorial Multi-Bandit Problem is applicable beyond energy man-
agement. Coordinating a multitude of unknown and heterogeneous actors is
an essential requirement for eﬀective systems management, including but not
limited to logistics, robotics, communication networks, and cloud computing
systems. Although such tasks can in principle also be addressed by traditional
Reinforcement Learning methods with multi-dimensional action space and a
scalar reward signal, doing so comes at the cost of discarding the individual
feedback of each managed component. Combinatorial bandits oﬀer a more ﬁne-
grained way to model these management problems, thus potentially speeding
up the learning and optimization process. Although the scope of this work is
restricted to stateless systems, the problem can also be generalized to contextual

2

bandits or even full MDPs.

Related work The authors of [5] have studied a multi-bandit problem where
in each episode only one bandit can apply an action. The challenge is to dis-
tribute a limited budget of exploration episodes among the bandits to determine
the best action for each of them with suﬃcient certainty.

The variant most similar our scenario is the Combinatorial Multi-Armed
Bandit problem introduced by Chen et al.
in [2]. Here multiple arms of a
single bandit can be played in each episode, where the set of admissible arm
combinations is limited by arbitrary constraints. Each arm, whether played or
not, produces some outcome in each episode, and the reward is only depending
on these outcomes. Among the diﬀerences to our Combinatorial Multi-Bandit
Problem are that (a) we consider multiple bandits each of which has to select
exactly one of its arms, (b) unlike [2] our analyses do not assume any properties
(like monotonicity, smoothness) of the reward function.

For the variant of the Combinatorial Multi-Armed Bandit Problem with full
observability of individual arm outcomes, Durand and Gagn´e have generalized
the Thompson sampling technique and applied it to Online feature selection
in [4]. In [3] Combes et al. study a version of the problem with linear reward
functions that was introduced as Combinatorial Bandits in [1], providing tight
lower bounds for the regret and providing action selection rules using the upper
conﬁdence bound method for exploration. Algorithms for the case of linear
reward functions have also been proposed in [6].

2 Problem deﬁnition and notation

In the Combinatorial Multi-Bandit Problem we are given a multitude of n actors,
indexed by i = 1 . . . n. Each actor i can perform a number ki of diﬀerent
actions, indexed by ai = 1 . . . ki. Selecting an action ai for actor i results
in a probabilistic output oi ∈ Oi. Unlike in most of the related work, the
outputs oi do not directly represent scalar rewards but can be elements of any
space Oi over which probability measures can be deﬁned. For each actor i the
distribution P(oi) over the possible outputs is unknown and only depends on
the most recent action ai that has been chosen for the actor. What is known
is the objective function r : O1 × . . . × On → IR which combines the individual
actor outputs into a scalar reward.

Time proceeds in discrete episodes t = 1 . . . T . In each episode an action
assignment at = (a1
t ) has to be made, assigning to each actor i an
action ai
t. We also use vector notation o = (o1, . . . , on) to denote the outputs of
the n actors observed at the end of each episode. The objective is to maximize
the total expected reward (cid:80)T

t , . . . , an

E[r(o) | at] .

As the outcomes only depend on the actions chosen in the same episode,
there exists an optimal action assignment ˜a = (˜a1, . . . , ˜an) that maximizes the
expected reward independently of the current episode t. If ˜a was known, then
a trivial optimal policy would be to select ˜a in each episode.

t=1

3

As ˜a and the outcome probabilities are unknown, an optimal or near-optimal
assignment of actions has to be learned from experience using a strategy that
balances exploration (choosing actions for the purpose of learning their outcome
distribution) and exploitation (choosing action assignments known to perform
well). When applying a particular action assignment a = (a1, . . . , an), the
expected gap compared to the optimal assignment ˜a is called the regret

ρ(a) = E[r(o) | ˜a] − E[r(o) | a] ,
(1)
and the normalized regret is deﬁned as ρ(a)/E[r(o) | ˜a]. Given action assign-
ments a1, . . . , at, the cumulative regret until episode t is deﬁned as

R(t) =

t
(cid:88)

t(cid:48)=1

ρ(at(cid:48)) = t · E[r(o) | ˜a] −

t
(cid:88)

t(cid:48)=1

E[r(o) | at(cid:48)] ,

(2)

and the cumulative normalized regret is deﬁned accordingly.

3 Lower regret bound

In [9] it was proven that for multi-arm bandit problems with action space A,
optimal action ˜a and reward r, the cumulative regret until episode t of any
action selection rule satisﬁes

lim
t→∞

R(t)
log t

≥

(cid:88)

a∈A

E[r | ˜a] − E[r | a]]
KL(a, ˜a)

,

(3)

under fairly non-restrictive conditions of the family of possible reward distribu-
tions. In the formula above, KL(a, ˜a) is the Kullback-Leibler divergence of the
reward distributions of a and ˜a.

This lower bound applies when interpreting the Combinatorial Multi-Bandit
Problem with n actors as a standard multi-arm bandit problem with a single
actor having an n-dimensional action space. Each action of the single actor
corresponds to an action assignment to the n original actors, and the result
of the reward function is used as the reward signal. Using this interpretation,
known algorithms for multi-arm bandit problems can be directly applied, but the
number of actions is exponential in n, and Equation 3 applies with exponentially
many summands.

Our generalization of Equation 3 is established following a similar argumen-
tation as the proof given in [9]. In that work it has been shown that any action
selection strategy either satisﬁes the bound in a trivial way or applies, with
probability 1, each suboptimal action a at least KL(a, ˜a)−1 log t times as t goes
to inﬁnity. An analog statement can be made for our multi-bandit problem,
introducing η(a, t) to deﬁne the number of times the action selection rule under
consideration has applied action assignment a in the ﬁrst t episodes. Accord-
ingly, ηi(ai, t) is the number of times actor i has applied action ai. We say that
action ai for actor i is suboptimal when it is not part of any optimal action as-
signment. As the proof of the following lemma follows the same argumentation
as the proof of Theorem 2 in [9], we only provide a proof sketch here.

4

Lemma 1 Assume that an action selection rules satisﬁes

E[η(a, t)] = o(tα)

(4)

for any α > 0 and each suboptimal action assignment a. Then, for any actor i
and any suboptimal action ai, it holds that

(cid:16)

P

ηi(ai, t) <

lim
t→∞

log t
KL(ai, ˜ai)

(cid:17)

= 0 ,

(5)

where ˜ai is the action of actor i in an optimal assignment.

Proof sketch Fix some suboptimal action ai and let ˜ai be the action as-
In the problem instance under
signed to actor i by an optimal assignment.
consideration, let Φ be the outcome probability distribution of ai. We deﬁne
a modiﬁed instance, where the outcome distribution of ai is set to a distribu-
tion Φ(cid:48) inﬁnitesimally close to the outcome distribution of ˜ai, but deﬁned in a
way that ai instead of ˜ai becomes part of the optimal assignment under Φ(cid:48).

For convenience of notation, deﬁne condition Ct as ηi(ai, t) < log t·KL(ai, ˜ai)−1.

Applying the precondition from Equation 4 to the modiﬁed instance with Φ(cid:48),
we obtain that EΦ(cid:48)[ηi(ai, t)] = t − o(tα) for any α > 0. From this lower bound
on the expectation of ηi(ai, t) under Φ(cid:48) we can apply the Markov inequality to
obtain an upper bound of roughly 1/t on PΦ(cid:48)(Ct).

For Equation 5 to hold for the original problem instance, it suﬃces to show

that

PΦ(Ct) → t1−(cid:15)PΦ(cid:48)(Ct) for t → ∞
(6)
for some (cid:15) > 0. To establish this, note that PΦ(Ct) and PΦ(cid:48)(Ct) are calculated
by integrating over the probability densities of all sequences of t episodes with Ct
satisﬁed. In any such sequence Y , let O be the multi-set of outputs observed by
actor i after action ai has been applied. Due to Ct, |O| < log t · KL(ai, ˜ai)−1. As
the only diﬀerence between PΦ(Y ) and PΦ(cid:48)(Y ) is in terms of the output proba-
bilities of the elements of O, it follows that PΦ(Y ) = PΦ(cid:48)(Y )·Πo∈OPΦ(o)/PΦ(cid:48)(o).
The divergence KL(ai, ˜ai) is the expected logarithm of PΦ(cid:48)(o)/PΦ(o); thus it can
be shown that the factor Πo∈OPΦ(o)/PΦ(cid:48)(o) almost surely converges to t1−(cid:15). (cid:3)
Lemma 1 establishes that, in order to attain sub-polynomial regret, the as-
signments have to be such that each suboptimal action ai is applied by its actor i
with a frequency that converges to log t · KL(ai, ˜ai)−1. The action ai can appear
in a range of diﬀerent suboptimal action assignments, possibly causing diﬀerent
regrets. The lower regret bound represents the best selection of assignments
that apply all suboptimal actions suﬃciently often.

Formally, for i = 1 . . . n, let Ai be the action set of actor i, and let A =
Ai × . . . × An be the set of all assignments. We deﬁne a mixed assignment as
a weight function w : A → IR+
0 . The weighting is called feasible if for each
action ai ∈ Ai of any actor i the total weight of all assignments assigning ai
to actor i is at least KL(ai, ˜ai)−1, that is, if (cid:80)
ai∈a w(a) ≥ KL(ai, ˜ai)−1. The

5

regret ρ(a) of an assignment a ∈ A is deﬁned as in Equation 1, and the regret
of a mixed assignment w is deﬁned as ρ(w) := (cid:80)

a∈A w(a)ρ(a). We deﬁne

w∗ := arg min
w feasible

ρ(w) .

(7)

Theorem 2 For any action selection rule for the Combinatorial Multi-Bandit
Problem the cumulative regret almost surely satisﬁes

lim
t→∞

R(t)
log t

≥ ρ(w∗) .

This lower bound is a generalization of the lower bound for single bandits as
stated in Equation 3. To see this, observe that, when n = 1, it holds that A =
A1 and the feasibility condition on w∗ implies that w∗(a) = KL(a, ˜a)−1 for
any a ∈ A.

4 Multi-bandit algorithms for energy systems

management

Our study of the Combinatorial Multi-Bandit Problem is motivated by an ap-
plication in the domain of energy management in a Smart City project. In this
project, an Energy Demand Management System is coordinating consumers of
energy with the goal to reduce consumption when required by the network. This
and other use cases of demand-side management are gaining importance in the
energy sector due to the increasing share of volatile renewable energy sources [7].
When modeling this application as a multi-bandit problem, each consumer
corresponds to an actor, and the action space consists of signals that can be
sent to energy consumers on a daily basis. The signals incentivize consumers to
re-schedule or curtail their energy consumption in various ways. At the end of
each day the load curves of the consumers are observed. A metric, deﬁned on the
aggregated load curve of all consumers, measures the extent to which the energy
management goal has been fulﬁlled. In this setting we consider the target to
obtain the maximal load reduction that can be sustained during a given target
time interval, measured against a given baseline curve. This speciﬁcation models
a common use case in energy demand management, where e.g. the owner of a
management unit agrees with the network operator to provide load reduction
as a service; see [12].

Formally, there are n actors, here each having the same number k of actions.
Having applied an action, the outcome is observed as an H-dimensional discrete
load reduction curve li = (li

H ). The objective function r is deﬁned as

1, . . . , li

r(l1, . . . , ln) = min
h=1...H

n
(cid:88)

i=1

li
h .

(8)

6

Algorithmic framework For the application just described we design a
range of algorithms that combine combinatorial optimization with exploration
strategies for bandit problems. In the next paragraphs we describe the opti-
mization module used by all algorithms, and subsequently we explain how the
diﬀerent algorithms apply the module.

Optimization module Assume that complete information about the envi-
ronments for N ≥ 1 sample episodes is available, that is, for each t = 1 . . . N
a known function (cid:96)t(i, j, h) speciﬁes the load reduction that will be achieved at
any daytime h when applying any action j to any actor i. Given this informa-
tion, the optimization module computes an action assignment maximizing the
average reward over the N sample episodes.

This computation is realized by formulating and solving the problem to
maximize the average result of Equation 8 as an integer linear problem. The
program uses n · k binary variables bij to specify whether actor i will apply
action j.

maximize

subject to

N
(cid:88)

t=1

Mt

n
(cid:88)

k
(cid:88)

i=1

j=1

(cid:96)t(i, j, h) · bij ≥ Mt

for h = 1 . . . H, t = 1, . . . , N

(9)

k
(cid:88)

bij = 1

j=1
bij ∈ {0, 1}

for i = 1 . . . n

for i = 1 . . . n, j = 1 . . . k .

Each summand Mt of the objective function represents the reward obtained
in one episode t. For any episode t and any daytime h within that episode,
an upper bound on Mt is given by the total load reduction of all actors at
that daytime. Thus, for each episode t there are H such upper bounds on Mt.
These N · H constraints are represented by the second line of the program. The
third line represents the n constraints that only one action can be chosen for
each actor i = 1 . . . n.

We remark that the mathematical program cannot be solved in polyno-
mial worst case runtime unless P = NP. To see this, consider the special case
with N = 1, k = 2 and (cid:96)1(i, j, h) ∈ {0, 1} for all i = 1 . . . n, j ∈ 1, 2, h = 1 . . . H.
The problem to decide whether there is an action assignment with reward at
least 1 is equivalent to the SAT problem with n variables and H clauses, which
is known to be NP-complete [8].

Nevertheless, state-of-the-art solvers for integer programs can solve fairly
large instances within reasonable runtime. When computation time runs out,
solvers can also return the best suboptimal solution found so far, including an
upper bound on the remaining optimality gap.

7

Single Episode algorithm We present a Single Episode (SE) algorithm us-
ing the optimization module with N = 1 to compute an optimal action assign-
ment for a single episode. This episode, speciﬁed by function (cid:96) := (cid:96)1, has to
represent the population of all possible episodes.

A simple way to realize such a function is to use (cid:96)(i, j, h) := ¯l(i, j, h),
where ¯l(i, j, h) is deﬁned as the average load reduction that has been previ-
ously obtained at daytime h by actor i after having applied action j. In case
no experience is available for action j of actor i an initial value is used, so
(cid:96)(i, j, h) := I(i, j, h) where I(i, j, h) is a user-deﬁned optimistic initial value. To
control the amount of exploration, we introduce a parameter called initial value
weight β ≥ 0, which determines how much the optimistic initial value is taken
into account in later episodes. In the computation of the average, each sample
from real experience is weighted with 1 while the initial value is weighted with
β.

As another means of exploration we use the well-known (cid:15)-greedy scheme and
apply it to each actor independently, that is, in each episode each actor ignores
with probability (cid:15) the assignment computed by the optimization module and
applies instead an action selected uniformly at random. A ﬁnal parameter driv-
ing exploration for SE is the number τ ≥ 0 of initial exploration episodes, during
which each actor applies random actions to obtain some initial experience.

Multi Episode algorithm A limitation of single episode algorithms is that
in general it is impossible to adequately represent the population of all episodes
by a single one. Solving the linear program using the expected values of (cid:96) as
parameters does in general not result in the action assignment maximizing the
expected reward due to the nonlinear objective function in Equation 8.

One way to deal with this issue is to compute an assignment that is optimal
for an ensemble of episodes. Given bias-free sample episodes 1 . . . N , the average
reward of assignment a is a bias-free and consistent estimator of the expected
reward of a. Thus, the assignment with maximal average reward among the N
samples almost surely converges against the optimal assignment.

Motivated by this observation, our Multi Episode (ME) algorithm constructs
a set of N sample episodes and applies the optimization module accordingly.
Let S(i, j) be the set of samples collected for actor i and action j, where
each sample represents an H-dimensional vector of load reduction values that
have been observed from actor i having applied action j.
If no such sam-
ple has been collected yet, the set is deﬁned using the (optimistic) initial val-
ues I(i, j, 1) . . . I(i, j, H).

A sample episode can be constructed by setting (cid:96)t(i, j, ·) to some value cho-
sen from the sample set S(i, j). Note that for any i, j, i(cid:48), j(cid:48) the values (cid:96)t(i, j, ·)
and (cid:96)t(i(cid:48), j(cid:48), ·) possibly have been collected at diﬀerent episodes. Thus, infor-
mation on the correlation of the two curves is discarded by this construction
method, and in general the sample episode is only bias-free under assumption
of stochastic independence of the outcomes of all actor/action combinations.

We use the number of sample episodes N as a parameter of the Multi

8

Episode algorithm, and we construct the N episodes by randomly picking from
each S(i, j). After an element of S(i, j) has been selected, it is temporarily
removed from the set until all other elements of S(i, j) have been selected (and
removed) as well. This way we ensure that the available sample curves are dis-
tributed among the N sample episodes as evenly as possible, which reduces the
correlation between the episodes and thus decreases the variance of the expected
reward estimators.

Oﬄine computation of optimal assignments To calculate the regret of
the above algorithms we implement a method to compute oﬄine an assign-
ment very close to the optimum. A true sample episode can be obtained from
simulated actors by querying their hypothetical behavior for each action. The
assignment is computed by the optimization module using a suﬃcient number
of such samples.

5 Experiments

Experimental setup The n actors in our setup are modeled as n energy
consumers, where the overall objective is to sustain the largest possible load
reduction over a given daily target time interval, which is discretized into day-
time slots 1, . . . , H; see Equation 8. The actors interpret each action as the
request to reduce energy usage during some request interval which is a sub-
interval of [1, H]. Out of the Ω(H 2) possible sub-intervals we deﬁne an action
set A of size O(H) as the smallest set with [1, H] ∈ A and [h1, h2] ∈ A =⇒
[h1, (cid:100) 1
2 (h1 +h2)(cid:101), h2] ∈ A. Our consumer model mixes three types
of loads. Assuming that for every consumer a baseline load curve has been ﬁxed,
we directly model load reductions against that hypothetical baseline.

2 (h1 +h2)(cid:101)], [(cid:100) 1

We model unconditional load reductions as a discrete Gaussian process that
is independent from applied actions. For each daytime h and actor i, the un-
conditional load reduction u(i, h) is a Normal-distributed random variable with
mean 0 and standard deviation σu. For some ﬁxed z ∈ [0, 1] and each day-
time h > 1, u(i, h) is generated by u(i, h) = z · u(i, h − 1) + (1 − z) · N (0, σ2
u).
From the linearity of mean and standard deviation of Gaussian variables follows
that u(i, h) ∼ N (0, σ2
u) for any h = 1, . . . , H. In our experiments we work with
ﬁxed z = 0.5 but with varying values of σu.

In addition, each consumer is assumed to have some curtailable load, which
represents energy consumption the consumer will suspend during the request
interval. This curtailable load is generated uniformly at random as a constant
value between 0 and 200W for each consumer.

Finally, every consumer has some shiftable load, which is speciﬁed as a load
with a speciﬁc daily starting time, length, and magnitude. The magnitude
is generated as a uniform random number between 0.5 kW and 1.0 kW. The
length L is a uniform random number between 0.25H and 0.5H, and the start
time is generated uniformly at random between time −0.5L and H − 0.5L,
such that up to 50% of the shiftable load can take place outside the target

9

Figure 1:
SE algorithm with β ∈
[0, 0.2], (cid:15) = 0, τ = 0; simulation
with µu = 500W .

Figure 2:
SE algorithm with β =
0, (cid:15) ∈ [0, 0.1], τ = 0; simulation
with µu = 500W .

interval [1, H]. This load can be freely shifted back and forth in time under the
constraint that it remains within the union of its original time window and the
target interval, and the consumer always tries to shift it such that its overlap
with the request interval is minimized. Each consumer is with 50% probability
cooperative enough to curtail its shiftable load when its overlap with the request
interval cannot be reduced.

Summarizing the model, each actor aggregates three distinct types of load
reduction potentials with diﬀerent characteristics. Note that the only prior
information about this model available to the learning agent is an upper bound
on the expected load reduction potential of 2kW per actor.

Results All experiments were performed with a horizon of 365 episodes, cor-
responding to one year in our application. Linear optimization was done via the
pulp [11] library for Python using COIN-OR [10] as the backend optimizer. A
runtime limit for the optimizer was set to 30s per episode, which turned out suf-
ﬁcient for a remaining optimality gap almost always under 5%. For computing
the optimal assignment oﬄine, 20 sample days were taken from the simulation
and the optimizer was given enough runtime for an optimality gap of less than
1%. The algorithms were assigned an initial value of 2 kW as the load reduction
potential for each actor, action, and daytime.

In a ﬁrst set of experiments the parameter spaces of the Single Episode
(Figure 1-3) and Multi Episode (Figure 4) algorithm are explored under the
condition of rather large daily load curve ﬂuctuations with µu = 500W , making
it diﬃcult for learners to determine optimal assignments.

Due to the optimistic initialization, all 24 actions are tried at least once by
each actor, resulting in a steep regret curve during that phase regardless of the
algorithm and its conﬁguration. The inﬂuence of parameter β, controlling for SE
the amount of remaining optimism after an action has been taken once, exhibits
the typical behavior of a parameter trading oﬀ exploration and exploitation, as
observable in Figure 1. Setting β = 0 is beneﬁcial in early episodes, while values

10

050100150200250300350episode020406080100120140cumulative normalized regretSE, β=0.00SE, β=0.05SE, β=0.10SE, β=0.15SE, β=0.20050100150200250300350episode020406080100120140cumulative normalized regretSE, †=0.00SE, †=0.05SE, †=0.10Figure 3:
SE algorithm with β =
0, (cid:15) = 0, τ ∈ [0, 50]; simulation
with µu = 500W .

Figure 4: ME algorithm with N ∈
[1, 30]; simulation with µu = 500W .

around 0.1 pay oﬀ later. As Figure 2 shows, the well-known (cid:15)-greedy strategy
does not improve exploration in the context of our experiments. Plausible rea-
sons are that (a) (cid:15)-greedy does not distinguish between slightly suboptimal and
clearly suboptimal actions, and (b) with 150 actors in each round the probabil-
ity is high that some of them apply an action which does not combine well with
the actions of the others. We set (cid:15) = 0 in the remaining experiments. The ﬁnal
parameter of SE is τ , the number of initial random exploration episodes; see
Figure 3. Also here it turns out that only for moderate values the initially larger
regret has the potential to pay oﬀ within a reasonable number of episodes.

For the Multi Episode algorithm we evaluate in Figure 4 the inﬂuence of
the number of sample episodes N on the result.
It turns out that the best
performance is achieved for N = 20. A possible reason why the performance
again deteriorates for N > 20 is the limited number of available samples per
action. Building too many episodes from the same sample set at some point
only increases the correlation among episodes and does not improve the reward
estimator anymore. Furthermore, with more sample episodes it becomes com-
putationally harder to determine a good action assignment within our runtime
limit of 30s, thus the overall solution quality decreases.

In a ﬁnal series of experiments we apply SE and ME using the most promis-
ing parameter conﬁgurations. The results for simulations with µu = 500W
and µu = 100W are visualized in Figure 5 and 6, respectively. The ﬁrst obser-
vation is that there is no fundamental advantage of using multiple episodes in
our application; combining SE with some initial exploration seems to lead to
a similar performance. An interesting observation can be made for the Single
Episode algorithm using optimism (β = 0.15) to drive exploration. While slow-
ing down convergence to the optimal assignment in case of low load ﬂuctuations
(µu = 100W ), it seems to be the only mechanism which continues to signiﬁcantly
improve the assignment over the course of all 365 episodes. For µu = 500W , at
the end of the curve in Figure 5, the curves with β = 0.15 have the least slopes
among all tested algorithms, i.e. they apply the action assignments closest to

11

050100150200250300350episode020406080100120140cumulative normalized regretSE, τ=0SE, τ=10SE, τ=20SE, τ=30SE, τ=50050100150200250300350episode020406080100120140cumulative normalized regretME, N=1ME, N=10ME, N=20ME, N=30Figure 5:
SE and ME algorithm
applied to simulation with µu =
500W .

Figure 6:
SE and ME algorithm
applied to simulation with µu =
100W .

the optimum. In general, Figure 6 shows that both ME and SE can ﬁnd action
assignments very close to the optimum in environments with little randomness.

6 Summary and open problems

The Combinatorial Multi-Bandit Problem is a Reinforcement Learning setup
with multiple actors. As our theoretical analysis has shown, the best trade-oﬀ
between exploration and exploitation for instances of this problem is limited by
regret Θ(log n), where the constant factor is determined by the regret of a com-
bination of action assignments that suﬃciently covers the suboptimal actions
of all actors. The algorithms designed for our energy management application
are based on a combination of bandit exploration schemes with mathematical
programming. With minimal prior knowledge about the 24 actions of 150 actors
in environments with high random ﬂuctuations, the implementations achieve a
cumulative normalized regret of around 115 in 365 episodes, which corresponds
to about 70% of the optimal reward on average.

Further theoretical analysis will be required to derive upper regret bounds.
The algorithms studied in our experiments have been tailored to a speciﬁc class
of applications, and it is an open question whether these or other action selec-
tion rules can match the lower bound in general. In addition, generalizing the
problem to stateful systems would substantially enhance its range of applica-
tions.

References

[1] Nicolo Cesa-Bianchi and G´abor Lugosi. Combinatorial bandits. Journal of

Computer and System Sciences, 78(5):1404–1422, 2012.

12

050100150200250300350episode020406080100120140cumulative normalized regretSE, β=0.00, τ=0SE, β=0.15, τ=0SE, β=0.00, τ=10SE, β=0.15, τ=10ME, N=20050100150200250300350episode020406080100120140cumulative normalized regretSE, β=0.00, τ=0SE, β=0.15, τ=0SE, β=0.00, τ=10SE, β=0.15, τ=10ME, N=20[2] Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed ban-
dit: General framework and applications. In International Conference on
Machine Learning, pages 151–159, 2013.

[3] Richard Combes, Mohammad Sadegh Talebi Mazraeh Shahi, Alexandre
Proutiere, et al. Combinatorial bandits revisited. In Advances in Neural
Information Processing Systems, pages 2116–2124, 2015.

[4] Audrey Durand and Christian Gagn´e. Thompson sampling for combinato-
rial bandits and its application to online feature selection. In Workshops
at the Twenty-Eighth AAAI Conference on Artiﬁcial Intelligence, 2014.

[5] Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric, and
S´ebastien Bubeck. Multi-bandit best arm identiﬁcation. In Advances in
Neural Information Processing Systems, pages 2222–2230, 2011.

[6] Yi Gai, Bhaskar Krishnamachari, and Rahul Jain. Combinatorial network
optimization with unknown variables: Multi-armed bandits with linear re-
wards and individual observations. IEEE/ACM Transactions on Network-
ing, 20:1466–1478, 2012.

[7] Haider Tarish Haider, Ong Hang See, and Wilfried Elmenreich. A review
of residential demand response of smart grid. Renewable and Sustainable
Energy Reviews, 59:166–178, 2016.

[8] Richard M Karp. Reducibility among combinatorial problems. In Com-

plexity of computer computations, pages 85–103. Springer, 1972.

[9] Tze Leung Lai and Herbert Robbins. Asymptotically eﬃcient adaptive

allocation rules. Advances in applied mathematics, 6(1):4–22, 1985.

[10] Robin Lougee-Heimer. The common optimization interface for operations
research: Promoting open-source software in the operations research com-
munity. IBM Journal of Research and Development, 47(1):57–66, 2003.

[11] Stuart Mitchell, Michael OSullivan, and Iain Dunning. Pulp: a linear pro-
gramming toolkit for python. The University of Auckland, Auckland, New
Zealand, http://www. optimization-online. org/DB FILE/2011/09/3178.
pdf, 2011.

[12] Pierluigi Siano. Demand response and smart grids—a survey. Renewable

and sustainable energy reviews, 30:461–478, 2014.

13

