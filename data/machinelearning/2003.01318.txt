CONVO: What does conversational
programming need? An exploration of machine
learning interface design

Jessica Van Brummelen
jess@csail.mit.edu
MIT

Kevin Weng
kweng@mit.edu
MIT

Phoebe Lin
phoebelin@gsd.harvard.edu
Harvard Graduate School of Design

Catherine Yeo
cyeo@college.harvard.edu
Harvard University

0
2
0
2

r
a

M
3

]

C
H
.
s
c
[

1
v
8
1
3
1
0
.
3
0
0
2
:
v
i
X
r
a

Abstract—Vast improvements in natural language understand-
ing and speech recognition have paved the way for conversational
interaction with computers. While conversational agents have
often been used for short goal-oriented dialog, we know little
about agents for developing computer programs. To explore the
utility of natural language for programming, we conducted a
study (n=45) comparing different input methods to a conversa-
tional programming system we developed. Participants completed
novice and advanced tasks using voice-based, text-based, and
voice-or-text-based systems. We found that users appreciated
aspects of each system (e.g., voice-input efﬁciency, text-input
precision) and that novice users were more optimistic about
programming using voice-input than advanced users. Our results
show that future conversational programming tools should be
tailored to users programming experience and allow users to
choose their preferred input mode. To reduce cognitive load,
future interfaces can incorporate visualizations and possess
custom natural language understanding and speech recognition
models for programming.

Index Terms—conversational programming, conversational AI,
interaction paradigms, voice interfaces, accessibility, education,
natural language processing, human-computer interaction

I. INTRODUCTION

With recent major advances in automatic speech recogni-
tion (ASR) and natural language processing (NLP) [1]–[3],
interacting with technology has become as easy as having
a conversation. Conversational agents have proliferated such
that it would be shocking for a smartphone not to be able to
transcribe and take action based on something someone said.
Programmers have begun to automate simple, few-turn tasks
using conversational artiﬁcial intelligence (AI), like turning
on lights, as well as longer, more complex tasks, such as
conversing with hairdressers to book clients’ appointments [4].
With such advances, this technology is positioned to be
leveraged in other spaces too. Speciﬁcally, it can increase
technology accessibility through question-answering (QA),
providing alternative input methods, and not requiring read-
ing/writing skills for
interaction. Computer programming
could especially beneﬁt from these advantages. Engaging
learners in straightforward conversations without syntax re-
quirements, could lower the barrier to entry to programming
and provide efﬁcient, alternative input methods. Nonetheless,
there are limitations to this technology. For example, ASR can
be frustrating, especially for those with nontraditional accents,

and NL is innately ambiguous, which could produce additional
errors (e.g., is the meaning of “say variable var”, say the value
of variable var or say the words “variable var”?).

Currently, little is known about the suitability of conversa-
tional agents for lowering the barrier to entry to programming.
There has been some work in single-turn program synthesis,
in which a NL utterance is converted to a program without
conversation [5], [6]; syntax- or keyword-dependent, voice-
based programming [7], [8]; conversational agents for control-
ling speciﬁc systems, such as Arduinos [9]; and for learning
linear tasks, such as sending emails [10]. However, optimal
interaction paradigms for NL, conversational systems remain
largely unstudied. We do not know whether it would be best
for end-users to conversationally interact through speech, text,
or a combination of multiple modalities. We also do not know
how programming conversationally affects cognitive load and
learning for novice programmers.

This paper will address questions about the usability, fea-
sibility, and cognitive load of a conversational programming
system. We completed a study (n=45) with our voice- and
text-based conversational programming tool, in which users
completed novice and advanced programming tasks using
only voice-input, only text-input, and voice-or-text systems.
They then answered Likert-scale and short answer questions
about usability, satisfaction, preference and overall experience.
Furthermore, we collected cognitive load indicator data, such
as time to completion, number of system resets, lengths of
utterances, and number of times users asked for help. We an-
alyzed these data through quantitative and thematic analyses.
The results will inform the development of future interactive
machine learning (ML) systems, especially conversational pro-
gramming agents, and begin to address the following research
questions:

RQ1: What is the preferred input modality for a conversa-

tional tool? Would multimodal input be useful?

RQ2: How do input modalities affect cognitive load?
RQ3: Do novice and advanced programmers’ conversa-

tional programming preferences differ?

RQ4: Are current ASR and NLU technologies adequate for

conversational coding?

RQ5: Is it better to have constrained or unconstrained NL?
RQ6: Can conversational programming teach computational

 
 
 
 
 
 
Fig. 1. CONVO’s architecture. The VUI passes spoken or typed input to the NLU module, which recognizes the intent. Given the intent, the DM might pass
a response like, “What do you want to call it?”, to the VUI to speak, or pass goals like, create procedure, to the PE.

thinking skills? How does it compare to visual programming?
RQ7: How can a conversational programming system fa-

cilitate project creation and computational action [11]?

This study is the ﬁrst in a series aimed to address these ques-
tions. It addresses voice- and text-input preferences (RQ1),
cognitive load effects of voice- and text-input (RQ2), ad-
vanced and novice user preferences (RQ3), and out-of-the-
box ASR and constrained NLU effectiveness (RQ4 and RQ5).
Future studies with updated systems (e.g., unconstrained NLU
system, system with visualizations) will address remaining
questions. Ultimately, our goal is to leverage state-of-the-art
ML technologies to empower learners to develop programs and
solve problems in their communities. To do so, we will explore
conversational AI design spaces with respect to lowering the
barrier to entry to programming.

This paper presents the following main contributions:
1) A formative study (n=45) examining cognitive load, in-
put modalities, and advanced and novice programmers’
performance with a conversational programming agent.
2) The system design of a conversational programming

agent, CONVO.

3) Design considerations for future systems based on quan-

titative and thematic analyses of user feedback.

II. BACKGROUND AND RELATED WORK

Our conversational programming system, CONVO, draws on
a number of research areas, including conversational AI, voice-
ﬁrst human-computer interaction (HCI), natural language pro-
gramming, and accessible programming.

A. Conversational AI, Voice-First HCI, and NL Programming

To create an effective conversational programming system,
we utilized conversational and voice-ﬁrst design principles.
Conversational AI design literature often references Grice’s
“conversational maxims” [12], which can be decomposed into
concise, correct, relevant, and natural principles [13]. For
example, to adhere to the natural principle, CONVO says “The
name, name, has already been used” instead of “Class name
already exists”.

Another important principle is NLU ﬂexibility. Design
guides suggest that conversational systems should understand
synonyms, over-answering and subtextual meaning [14], [15].
However, since we wanted to study the effects of constrain-
ing NL in a conversational programming system (RQ5), we

restricted CONVO to understanding particular NL phrases,
like “create a variable”, so that we can compare it to an
unconstrained NL system in future studies. We hypothesize
that constraining NL input will reduce potential for ambiguity
and increase the likelihood of CONVO understanding users
better; though at the same time, reducing the total number of
phrases CONVO understands could force users to think about
precise commands, thereby increasing their cognitive loads.

Previous studies support this hypothesis and illustrate how
unconstrained NL results in ambiguity and can cause mismatch
in human perception and reality of the system’s understand-
ing [16], [17]. Other research suggests, however, that such
ambiguities can be eliminated through conversational QA and
programming by demonstration [18], [19]. To the authors’
knowledge, however, there have been no studies directly com-
paring constrained and unconstrained NL programming sys-
tems with such ambiguity reduction methods. In this study, we
investigate users’ perception of constrained NL with CONVO.
Historically, voice-based programming systems have been
developed for advanced programmers using syntactically-
constrained vocabularies (largely due to limited ASR and
NLU technology) [8], [20]–[22]. Recently, however, a number
of voice-based, NL systems have appeared. TurtleTalk, for
instance, allows children to program the movement of a turtle
in a speech-based video game environment [23]. Other systems
allow for mechatronic system control or database queries [9],
[24], [25]. Nonetheless, each of these systems are limited in
scope. Though domain-speciﬁc ASR systems constrain the
vocabulary space which generally results in better speech
recognition, general-purpose ASR systems can provide the
ﬂexibility learning technology usually requires. This research
aims to determine the feasibility of a general-purpose voice-
or-text NL programming system using current state-of-the-art
ASR from Google Cloud Speech-to-Text API [26].

B. Accessibility and Cognitive Load

Conversational coding systems have the potential to increase
programming accessibility in three main areas: (1) for those
with visual- and motor-impairments, (2) for those who are un-
able to read or type, and (3) for those with little prior program-
ming knowledge. Particularly salient design principles for ac-
cessible auditory programming environments include describ-
ing code, not syntax; expressing logical context (e.g., nested
loop structure) over spatial context (e.g., line 484); providing

localization, querying and navigation cues; and intentionally
addressing voice-based ambiguity, like homonyms [27]–[32].
These principles guided the development of CONVO.

Despite these principles, voice-based programming systems
can still incite high cognitive load, especially when the vo-
cabulary or grammar deviates greatly from natural language
[20], [27]. Cognitive theory suggests that auditory and visual
information are processed via separate channels, and that
channels have limited capacity [33]. Thus, a voice-based
system without any scaffolding may overload the auditory
channel, but a combination of the two (e.g., a voice-or-text-
based system) may overcome the challenges of high cognitive
load [34]. Recent research also shows that student learning
is improved when students have verbal interaction back to a
conversational system [35].

III. SYSTEM DESIGN

CONVO is a voice-based system allowing users to develop
computer programs by conversing in natural language with
a conversational agent. For the user study, the system was
designed to support both voice- and text-based conversations.
Currently, the system supports three main tasks—(1) program
creation, (2) program editing, and (3) system feedback—
through natural language. This is illustrated in the following
system walk-through.

A. System Walk-through

Currently, CONVO is a constrained NL-based programming
system in which commands have to be stated exactly for
the system to understand. However, we are developing a less
constrained version for future studies to analyze NL constraint
effects on cognitive load. Here, we illustrate an example
scenario and conversation using the current system with Lisa,
a user, who wants to make a game for her little brother Chris
to help him learn what sounds animals make.

Constrained NL System (Current). Lisa starts up Convo
and says “Hey Convo, I want to make a game.” Since the
system doesn’t recognize the word “game”, it responds with “I
didn’t understand what you want to do. You can start making
a program by saying ‘create a program’.” Lisa says, “Okay,
create a program.” Convo responds by asking her, “What
do you want to name the program?”. Lisa replies, “Animal
Sounds.” She can now add actions to the program. Lisa wants
to make a loop but does not know how. She sees an example
phrase in the sidebar – “Create a loop” – and tries it out. The
system asks for the halting condition, to which Lisa responds,
“Until I say ‘stop’.” Now inside the loop, Lisa tells Convo
to ask for user input by saying, “Get user input and save it
as animal.” Next, Lisa makes conditionals for dog, cat, horse,
and cow sounds. For the dog sound, Lisa says “If animal is
dog, play the dog sound.” After going through each animal
sound, Lisa closes the loop by saying, “Close loop”, and tells
Convo she is ﬁnished by stating “Done.” Finally, Lisa and her
little brother Chris run the program by telling Convo to “Play
Animal Sounds.”

As illustrated by this scenario, CONVO only recognizes
exact commands like “create a program” or “make a program,”
limiting the possible conversations that users can have with
CONVO. In addition, CONVO has a constrained system for
providing feedback or assistance to the user. Below, we
illustrate the same scenario with a less constrained NL system.
Unconstrained NL System. Lisa starts up Convo and says
“Hey Convo, can I make a game called Animal Sounds?”
Convo does not know what a game is but is able to ask Lisa,
“What’s a game? Is it like a procedure or a variable?”. Convo
responds “It is a procedure.” With that, Convo is able to
equate “making a game” to making a procedure. In addition,
Convo also recognizes that “Animal Sounds” is the name that
Lisa wants to use. Convo proceeds to create a program called
“Animal Sounds”. Lisa wants to make a loop, but she has
never done it before with Convo. Instead of needing to consult
documentation, she directly asks “Convo, how do I make a
loop?”. Convo directs her through the process by responding,
“First, you need to have a stopping condition. What do you
want it to be?”. Lisa says, “Until I say ‘stop’,” and proceeds to
add actions to the loop. She says “Set animal to user input”.
Convo detects potential ambiguity and asks Lisa, “Do you
mean get user input and set ‘animal’ to the value of the input,
or set ‘animal’ to the words, ‘user input’?” Lisa indicates the
former and proceeds to create the rest of the program.

An unconstrained version of CONVO would be able to
recognize and detect user intents from a larger variety of
utterances. Because ambiguity exists in natural conversations,
the unconstrained version would also need to detect ambiguity
and ask for clariﬁcation. By implementing such a system, we
would be able to determine the suitability of both constrained
and unconstrained conversational NL for programming.

B. Technical Implementation

CONVO consists of four modules: the voice-user interface
(VUI), natural language understanding (NLU) module, dialog
manager (DM), and program editor (PE), as shown in Fig. 1.
Users interact with the system through the VUI. The VUI
receives and transcribes voice input into text and sends it to the
next module. The ASR is handled by Google’s Cloud Speech-
To-Text API [26] service. The transcribed output is sent to a
WebSocket server where the NLU and post-processing occurs,
and the DM generates appropriate responses. Responses are
voiced back to users using Google’s Speech Synthesis API
[36].

The second module performs NLU on the input, extracting
and recognizing users’ intents based on utterances provided by
the VUI. The NLU module is syntactically-constrained and
uses a regex expression-based semantic parser to determine
intent and extract semantic information. In future studies,
we will implement customized NL and speech models, and
compare this system with our current constrained system to
determine usability and cognitive load effects.

The third module is the DM, which is responsible for
keeping track of the conversation, goals between users and
the system, and the agent state. Agent states include program

creation, editing and execution states, and are managed by a
ﬁnite-state machine.

Given information extracted by the NLU, the DM creates a
“user goal.” The goal contains the user’s intent (e.g. intent to
create a variable) and necessary actions the system must take
to complete the goal. These actions are referred to as “agent
goals.” For example, when a user wants to create a variable, the
DM will ask for a name and an initial value if they weren’t
originally provided by the user. In this case, the user goal
is to create a variable, while the agent goals are to ask for
the name and initial value. The latter behavior is commonly
known as slot-ﬁlling. Completing agent goals leads to various
possible changes, including state changes and new additions to
the program. The agent goals and state determine the speciﬁc
response CONVO provides after a given user command.

The fourth module is the PE, which is responsible for
program-related tasks like editing and execution. The PE
interacts with the DM, receiving program-related commands
and actions and returning program context and state. During
program creation, the editor keeps a program representation in
memory. The representation is a list of actions that the agent
performs when executing the program, and can be exported to
other formats (e.g. JSON, Javascript, Python). During editing,
the PE keeps track of the program state, which contains
information such as deﬁned variables and the current action.

IV. USER STUDY

We conducted a user study to evaluate the effectiveness of
CONVO and to understand the user needs of a conversational
programming environment.

A. Participants

We recruited 45 participants through university mailing lists
and ﬂyers, with 27 males, 17 females, and 1 unspeciﬁed. Par-
ticipants ranged from local high school students to members of
local universities to community members from elsewhere. The
minimum age of participants was 14, maximum was 64, and
mean 25.3. Based on survey results, 12 users self-identiﬁed
as “novice” (users with little to no programming experience)
and 33 users self-identiﬁed as “advanced” (users who had
completed a programming course or had experience in object-
oriented programming). All 45 participants completed at least
one part of the user study. Participants were given a $20 or
$30 Amazon gift card depending on whether they identiﬁed
as novice or advanced.

B. Procedure

Upon arriving to the user study, participants were presented
with an informed consent form,
in which they agreed to
be audio- and keystroke-recorded. Participants were asked to
provide their own laptops, but were provided with earbuds for
the task. Before starting the study, participants read detailed
instructions about what to expect during the study and watched
a video on how to use the text-input, voice-input, and voice-
or-text systems, and ﬁlled out a demographics questionnaire.

Participants interacted with the CONVO programming envi-
ronment in three stages: the practice stage, the novice stage,
and the advanced stage, with the advanced stage for advanced
participants only. The practice stage was designed for partici-
pants to familiarize themselves with the environment. At each
stage, participants interacted with three systems. Each system
had a goal for the user to complete. Participants were shown
what success looked like for each goal through a video prior to
starting. Participants could not move on to the next goal until
they successfully completed the current goal. After interacting
with a system in a particular stage, participants ﬁlled out a
questionnaire about their experience.

We performed a mixed between- and within-subject test,
where the between-subject conditions were the novice and
advanced stages and the within-subject condition was input
modality type. We randomized the order of the systems and
introduced slight variations to the goals to account for learning
effects. Participants were given as much time as they needed
to complete all tasks, and could raise their hand to ask for
help if they had questions. Questions were addressed following
a strict protocol
to ensure participants received the same
advice for speciﬁc technical issues. After completing the study,
participants ﬁlled out a ﬁnal questionnaire.

C. Study Tasks

At every practice, novice, and advanced stage, participants
were given three tasks, or goals, to complete, one for each
system (voice-input, text-input and voice-or-text systems). The
goals varied slightly but were similar enough that participants
would produce similar actions while interacting with different
systems. Goals were randomly matched to systems. The set of
tasks participants were asked to complete were:

1) Practice Stage: Create a program where CONVO says
“hello world” in audio format. We varied the phrase
CONVO should say three times.

2) Novice Stage: Create a program where CONVO listens
for user input and plays two different animal sounds
(e.g. If I say “cat”, play “meow”). We varied the type
of animal sounds required three times.

3) Advanced Stage: Create a program where CONVO con-
tinuously listens for user input for a set number of times
and plays the corresponding animal sound. We varied
the number of times CONVO listens for user input and
the required animal sounds three times. Only advanced
participants completed the Advanced Stage.

The novice goals required participants to use conditionals and
variables. The advanced goals required participants to build
off of novice goals and use a loop to generate the speciﬁed
animal sounds multiple times. The user interface for the study
is shown in Fig. 2.

D. Data Collection

Each participant completed a questionnaire about their de-
mographic and programming background. All text- and voice-
input transcripts were recorded throughout the study. We also
recorded the number of times participants asked for help and

Novice participants made signiﬁcantly more incorrect utter-
ances with the voice-based system (M =17.38) compared to
the text-based (M =1.38, F1,38=17.79, p=0.0001) and voice-
(M =4.77, F1,38=11.39, p=0.0017),
or-text-based systems
whereas no signiﬁcant difference was observed for advanced
participants. In addition, novice participants were more satis-
ﬁed with the voice-or-text based system (M =2.61) than the
voice-based system (M =3.44, F1,35=15.90, p=0.0003), and
found the voice-or-text-based system (M =2.66) more efﬁcient
to use than the voice-based system (M =3.47, F1,35=14.18,
p=0.0006). There was no signiﬁcant difference in preference
observed by advanced participants.

Advanced participants perceived the voice-or-text-based
system (M =2.94) to be more difﬁcult to use compared to
the text-based system (M =3.5, F1,15=6.36, p=0.02);
there
was no signiﬁcant difference found for novice participants.
Overall, novice participants found the voice interaction of the
voice-based and voice-or-text-based systems to be useful and
enjoyable, whereas advanced participants tended to disagree
more with those statements (see Fig. 7).

Prior programming knowledge and gender did not have
a signiﬁcant effect on completion time for all participants.
Novice participants and advanced participants completed the
practice and novice stages in around the same time. There
was also no signiﬁcant difference between the number of
voice utterances and text utterances during the novice stage.
Advanced participants tended to use more text utterances than
voice utterances during the advanced stage.

To investigate cognitive load effects, we examined the
number of resets of the system (as participants mentioned they
reset due to forgetting where they were in the program they
were creating), time to goal completion, and number of times
users asked for help. Note that we only analyzed the advanced
stage for cognitive load, since the instructions were provided
line-by-line in the novice stage (i.e., minimal cognitive load
involved), whereas users needed to determine which steps to
take next on their own in the advanced stage (i.e., signiﬁcant
cognitive load involved). There was no observed signiﬁcant
difference in the number of times asked for help with the
voice-input, text-input, and voice-or-text system. The input
modality also did not have a signiﬁcant effect on the number
of resets or time to goal completion during the advanced stage.
2) Qualitative Analysis: We organized the free-form re-
sponses and analyzed for patterns using an inductive approach
[37] (i.e. open coding). We identiﬁed fourteen design themes,
which fell into two main categories, positive feedback and
recommendations (see Fig. 3).

We coded 651 occurrences of these themes and show repre-
sentative quotations below. Themes for the positive feedback
category follow:

Efﬁcient (49/651): “I liked how quick it was. Having to just

speak to program is far quicker than typing [...]”

Usable (48/651): “I liked how straight-forward and logical
it is because it translates the logic of the code into everyday
speak.”

Fig. 2. The advanced stage of the study using the voice-or-text-based system,
which shows both the record button and text box for input.

the number of resets they were given when they got stuck and
wanted to redo a particular goal. After every completed task,
we collected the following measures:

• Time: The total time duration for a participant to com-

plete a goal.

• Usability: Participants responded to “I found it difﬁcult
to complete the goal with the [input]-based system.”
and “I found programming with the [input]-based system
difﬁcult to use.” on a 5-point Likert scale.

• Satisfaction: Participants responded to “I am satisﬁed
programming with the [input]-based system.” on a 5-point
Likert scale.

• Efﬁciency: Participants responded to “I found program-
ming with the [input]-based system efﬁcient to use.” on
a 5-point Likert scale.

• Preferences: Participants answered free-form questions
about what they liked and disliked in using the system.
• Desired features: Participants answered free-form ques-

tions about what features they wished to add.

At the conclusion of the study, users ﬁlled out a question-
naire with 5-point Likert scale questions comparing the three
systems and free-form questions asking participants what they
would ask the conversational agent, challenges they ran into,
and questions they had about the system.

We used analysis of variance (ANOVA)

for between-
subjects analyses and repeated measures ANOVA for within-
subjects analyses.

E. Results

1) Quantitative Analysis: The type of input modality had
a signiﬁcant effect on participants’ perception of the system.
Our results show that both novice and advanced participants
strongly preferred the text-based system over the voice-based
system. Participants felt it was more difﬁcult to complete the
programming goals with the voice-based system, and were
generally more satisﬁed with the text-based system.

Improve speech-to-text

(190/651): “Differentiating be-
tween voices and then telling the difference with accents [was
a challenge for the system]”

Reduce NL constraints (66/651): “[...] Allowing more
variability in what I can say to the agent to get it to do the
same command would feel more natural.”

As shown in Fig. 5, six of the top seven themes for novice
and advanced users were the same, including improve speech
recognition, increase agent interaction, and add visualization.
Novice users emphasized increasing transparency over efﬁ-
ciency, and vice versa for advanced users.

Fig. 3. The theme hierarchy created during open coding.

Accessible (32/651): “I liked the availability of the text
option because it usually would take me a few attempts to
get the voice working.”

Effective coding features (9/651): “I liked that it tried to
catch cases like ‘not having a false condition’. I imagine this
will be super useful in recommending base cases for recursion
problems”

Interesting (6/651): “It feels cool to do this - I can imagine

coding while driving or doing housework.”

Themes for the recommendations category regarding im-

proving the agent’s output follow:

Increase agent interaction (91/651): “[I would add] a
spellchecker, like if a word is spelled incorrectly it could say
‘You said ‘dune’, did you mean ‘done’?”

Add visualization (72/651): “[I would add] some sort of
visualization of the function being built up as interaction
progresses”

Improve efﬁciency (30/651): “It also seems quite inefﬁcient
to ﬁgure out the right way to express a statement in actual
words that otherwise can be typed in a programming language
[...]”

Reduce cognitive load (12/651): “I can’t see my program
and I have to remember what’s going on, that will become
infeasible very quickly.”

Examples from the recommendation category regarding

improving users’ understanding follow:

Increase transparency (25/651): “[I would ask] ‘How do
you recognize the voices? Do you use any sort o [sic] machine
learning to recognize the accents?’ ”

Reduce ambiguity (12/651): “I’m interested in how does

the program differentiate similar commands.”

Convey system purpose (9/651): “Who is the intended
audience and what sort of programs do you imagine them
writing? [...]”

Examples from the recommendation category regarding

improving the agent’s recognition follow:

Fig. 4. Total number of occurrences for the top seven themes from advanced
user responses and top seven from novice user responses. Novice responses
emphasized transparency over efﬁciency. Note how the colors represent which
user group(s) the theme came from (e.g., pink represents a top theme from
novice users, dark blue represents a top theme from both novice and advanced
users).

Among input modalities, participants emphasized improve
speech recognition and increase agent interaction for all three
systems (voice-input, text-input, and voice-or-text) (see Fig.
5). The voice-input system responses emphasized efﬁciency;
the text-input system responses emphasized improving efﬁ-
ciency; and the voice-or-text system responses emphasized
accessibility. Both the voice- and text-input system responses
emphasized usability; both the voice-input and voice-or-text
system responses emphasized adding a visualization; and both
the text-input and voice-or-text systems emphasized reducing
the NL constraint.

Fig. 5. Total number of occurrences for the top ﬁve themes from each system
survey. The voice-input system responses emphasized efﬁciency; text-input,
a need to improve efﬁciency; and voice-or-text, accessibility. Note how the
bars’ colors represent which input system(s) the theme came from (e.g., pink
represents text-input system, and green represents voice-input and voice-or-
text systems).

V. DESIGN RECOMMENDATIONS
Through the quantitative and qualitative analyses, we iden-
tiﬁed six main design recommendations for future conversa-
tional programming systems.

Tailor to programming experience and task. Our results
suggest that conversational programming systems should be
tailored to their audiences due to differences in user pref-
erences and abilities. We found that novice users generally
found voice-input useful and enjoyable, whereas advanced
users tended to view it
less so, as shown in Fig. 6 and
7. Furthermore, although there was no signiﬁcant difference
between the overall number of voice- and text-inputs,
in
the advanced stage users tended to type rather than speak
(p=0.003). Advanced users also perceived voice-or-text to be
more difﬁcult than text (p=0.02), but there was no signiﬁcant
difference for novice users. Thus, for an advanced audience,
it may be more important to have a text-input option than for
a novice audience, and for an introductory audience, a voice-
input system may be more useful than for an advanced one.

Fig. 6. Novice user responses to Likert scale questions. Novices generally
found voice useful and enjoyable.

We also found that some advanced users found NL program-
ming cumbersome, likely because they were used to syntax-
restricted programming languages (e.g., “It also seems quite
inefﬁcient to ﬁgure out the right way to express a statement
in actual words that otherwise can be typed in a programming
language using very specialized characters.”), whereas novice
users tended to praise the naturalness of the language (e.g.,
“I liked the simplicity of using the normal talk, as in not
coding necessarily”). This is further reﬂected in how “improve
efﬁciency” was found in advanced users’ the top seven themes,
but not novice users’ (see Fig. 4). Thus, NL may be a better
ﬁt for an educational, introductory tool than an advanced tool.
Design a ﬂexible, accessible system. Our results suggest
conversational programming systems should be accessible
through both voice- and text-input. Participants found value in
both modalities, often citing voice as efﬁcient (see Fig. 5) and
text as accurate. Many participants had comments similar to,

Fig. 7. Advanced user responses to Likert scale questions. Advanced user
responses tended to be less favorable towards voice than novice responses.

“I liked being able to use the voice for longer commands, and
the text for shorter commands or misunderstood commands”.
This was supported by the signiﬁcant difference in number of
characters (p=0.004) and words (p=0.003) per voice utterance
over text utterance (i.e., longer voice utterances). Furthermore,
when using the voice-or-text system, participants used both
voice and text input, and there was no statistical evidence for a
difference in how many times participants spoke versus typed.
From an accessibility standpoint, it makes sense to provide
both input options, and allow each of them to stand alone
(such that the system is completely accessible by voice-only
and text-only). With current technologies, however, this may
be difﬁcult to achieve. The Google Cloud Speech-to-Text [26]
ASR system we used—which is often recognized as the gold
standard [38], [39]—did not seem sufﬁcient for programming.
Many participants commented on this (e.g., “Sometimes it had
problems understanding my speech, so I resorted to typing
things.”, “It seems like if speech recognition worked well,
it would be a better choice, but having this [text option] is
useful”) and we found that the most common theme was to
improve speech recognition. Thus, until speech recognition
systems improve, it may be infeasible to have a standalone
voice-input system.

Design a transparent system. Many participants described
how they would appreciate being able to ask the system how
it works. Some questions included:

• “What kind of nueral [sic] network do you run on?”
• “How do you understand what Im saying?”
• “How do you map my phrases to commands?”
• “What kind of voice recognition is used?”
• “Why didn’t the agent understand me?”
• “How do you register what I’m saying? Should I speak
slower/faster? How can I make it easier for you to
understand me?”

• “Do you use any sort o [sic] machine learning to recog-

nize the accents?”

Transparency was one of the top occurring themes for
novice users (see Fig. 4) and especially important when
developing AI systems for education.

Design with visualizations. A common theme in the free-
form responses was the desire for code visualizations. This
was in the top seven commonly occurring themes for both
novice and advanced users, and the top ﬁve themes for both
the voice- and text-based systems. Speciﬁcally, users asked
for ways to “visualize where [they] are in the program”,
view a “representation of the code [they were] making”, “see
[...] variable names or the name of the procedure”, see “the
current state of the program, or at
least [...] which level
[they]’re at”, and visually “modify [their] previous lines that
were misinterpreted”. This makes sense, as current technology
focuses heavily on visual systems and computer screens, and
voice-only systems can force high memorization requirements
on users. Nonetheless, depending on a system’s intended
audience, one may choose to avoid visualizations or make
them non-essential to the system for accessibility reasons.

Design to reduce cognitive load. In the thematic analysis,
some participants mentioned high cognitive load due to a
lack of visualizations (e.g., “I found it quite challenging to
the logic of the program entirely in my head;
ﬁgure out
like I had to ﬁgure it all out before entering
[...] it felt
anything.”). In future studies, we will analyze cognitive load
effects of integrating visualizations into CONVO. We expect
this will reduce the cognitive load for sighted users. Other
design features to potentially reduce cognitive load include
decreasing the constraint on the NL input such that users will
no longer have to remember speciﬁc phrases, and improving
the speech recognition model such that people don’t have to
repeat phrases as often, and are more likely to remember where
they are in the program.

For all cognitive load indicators (number of resets of the
system, time to goal completion, and number of times users
asked for help), we found no evidence for a signiﬁcant
difference between the voice-based, text-based, and voice-or-
text-based systems; thus, voice-based, text-based and voice-
or-text-based systems may be viable options when designing
for cognitive load.

Improve ASR and NLU. The most common theme in the
free-form responses was to improve speech recognition. As
mentioned previously, we used the Google Cloud Speech-to-
Text [26] ASR system—which is often recognized as the top
online ASR [38], [39]—for CONVO. Evidently, current ASR
systems are not sufﬁcient for fully standalone voice-based, NL
programming systems. One potential avenue for improvement
is to develop a custom NL programming ASR model that
incorporates common NL programming phrases, like “create a
variable”, to ensure recognition of those phrases. Nonetheless,
by training on speciﬁc phrases, this may cause the model to
be less robust to new phrases, which would somewhat defeat
the purpose of a generalizable NL system.

In addition to improved speech recognition, participants
desired reduced constraint on NL input (e.g., “It’s a very cool
idea, and with expanding the dictionary it could work better.”,

“I expect more natural-language input support such as ‘nope’,
‘no thanks’, etc. would be valuable as well.”). Reducing NL
constraint was a top theme in both the text-based and voice-
or-text-based systems, as well as in both novice and advanced
users’ responses (see Fig. 4 and 5).

We are currently developing an unconstrained NL version
of CONVO to understand whether this improves or reduces
performance, as there has been research questioning the
suitability of unconstrained NL for programming [16], [17].
Nevertheless, with additional ambiguity reduction techniques,
such as conversational QA and immediate feedback from
the agent, unconstrained NL may become suitable for intro-
ductory, educational NL programming, especially due to the
positive feedback in this area from the free-form responses
(e.g., “It gives feedback, which is really useful”, “The process
is pretty interactive and fun. The idea of using natural language
to code is great and the system reacts very fast.”, “Feedback
is immediate.”).

VI. CONCLUSIONS

In this study, we investigated the effectiveness of voice-
based, text-based, and voice-or-text-based systems in a con-
versational programming environment. We analyzed the sys-
tems in terms of difﬁculty, efﬁciency, and cognitive load
indicators through free-form responses, Likert scale questions,
and user activity during programming task completion. Our
results show a desire for and optimism about conversational
programming, especially in introductory programming sys-
tems. Future conversational and interactive ML systems should
consider the following six design recommendations: (1) Tailor
to programming experience and task, (2) Design a ﬂexible,
accessible system, (3) Design a transparent system, (4) Design
with visualizations, (5) Design to reduce cognitive load, and
(6) Improve ASR and NLU. Future iterations of CONVO will
include addressing questions about the effects of visualizations
and reducing NL constraints in terms of usability and cognitive
load, and the effectiveness of conversational programming for
learning computational thinking skills and taking computa-
tional action.

VII. ACKNOWLEDGEMENTS

We would like to thank Hal Abelson, Marisol Diaz, Selim
Tezel, and Ilaria Liccardi for their support, as well as the
participants in our study for their time.

REFERENCES

[1] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio,
“Attention-based models for speech recognition,” in Advances in neural
information processing systems, pp. 577–585, 2015.

[2] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg,
C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al., “Deep
speech 2: End-to-end speech recognition in english and mandarin,” in
International conference on machine learning, pp. 173–182, 2016.
[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of
deep bidirectional transformers for language understanding,” in NAACL-
HLT, 2019.

[4] D. E. O’Leary, “Google’s duplex: Pretending to be human,” Intelligent
Systems in Accounting, Finance and Management, vol. 26, no. 1, pp. 46–
53, 2019.

[28] S. Mealin and E. Murphy-Hill, “An exploratory study of blind software
developers,” in 2012 IEEE Symposium on Visual Languages and Human-
Centric Computing (VL/HCC), pp. 71–74, IEEE, 2012.

[29] A. Steﬁk, A. Haywood, S. Mansoor, B. Dunda, and D. Garcia, “SOD-
Beans,” in 2009 IEEE 17th International Conference on Program
Comprehension, pp. 293–294, IEEE, 2009.

[30] A. Steﬁk, C. Hundhausen, and R. Patterson, “An empirical investigation
into the design of auditory cues to enhance computer program compre-
hension,” International Journal of Human-Computer Studies, vol. 69,
no. 12, pp. 820 – 838, 2011.

[31] A. Steﬁk, On the design of program execution environments for non-
sighted computer programmers. PhD thesis, PhD thesis, Washington
State University, 2008.

[32] E. Schanzer, S. Bahram, and S. Krishnamurthi, “Accessible AST-based
programming for visually-impaired programmers,” in Proceedings of
the 50th ACM Technical Symposium on Computer Science Education,
pp. 773–779, ACM, 2019.

[33] R. E. Mayer, “The promise of multimedia learning: using the same
instructional design methods across different media,” Learning and
instruction, vol. 13, no. 2, pp. 125–139, 2003.

[34] R. Winkler, S. Hobert, A. Salovaara, M. S¨ollner, and J. M. Leimeis-
ter, “Sara, the lecturer: Improving learning in online education with
a scaffolding-based conversational agent,” ACM CHI Conference on
Human Factors in Computing Systems, April 2020.

[35] I. Lepadatu, “Use self-talking for learning progress,” Procedia-Social

[36] Google,

“Introduction

and Behavioral Sciences, vol. 33, pp. 283–287, 2012.
to

the
https://developers.google.com/web/updates/2014/01/Web-apps-that-
talk-Introduction-to-the-Speech-Synthesis-API, Last
accessed
2020-02-24.

synthesis

speech

api,”

2014.

on

[37] D. R. Thomas, “A general inductive approach for analyzing qualitative
evaluation data,” American Journal of Evaluation, vol. 27, no. 2,
pp. 237–246, 2006.

[38] J. Y. Kim, C. Liu, R. A. Calvo, K. McCabe, S. C. Taylor, B. W. Schuller,
and K. Wu, “A comparison of online automatic speech recognition
systems and the nonverbal responses to unintelligible speech,” arXiv
preprint arXiv:1904.12403, 2019.

[39] V. K¨epuska and G. Bohouta, “Comparing speech recognition systems
(microsoft api, google api and cmu sphinx),” Int. J. Eng. Res. Appl,
vol. 7, no. 03, pp. 20–24, 2017.

[5] M. Rabinovich, M. Stern, and D. Klein, “Abstract syntax networks
for code generation and semantic parsing,” in Proceedings of
the
55th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pp. 1139–1149, 2017.

[6] E. C. Shin, M. Allamanis, M. Brockschmidt, and A. Polozov, “Pro-
gram synthesis and semantic parsing with learned code idioms,” in
Advances in Neural Information Processing Systems 32 (H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett,
eds.), pp. 10825–10835, Curran Associates, Inc., 2019.

[7] A. Begel and S. L. Graham, “An assessment of a speech-based program-
ming environment,” in Visual Languages and Human-Centric Computing
(VL/HCC’06), pp. 116–120, IEEE, 2006.

[8] A. Nowogrodzki, “Writing code out loud,” Nature, vol. 559, pp. 141–

142, Jul 05 2018.

[9] Y. Kim, Y. Choi, D. Kang, M. Lee, T.-J. Nam, and A. Bianchi,
“Heyteddy: Conversational test-driven development for physical com-
puting,” Proceedings of the ACM on Interactive, Mobile, Wearable and
Ubiquitous Technologies, vol. 3, no. 4, pp. 1–21, 2019.

[10] T. J.-J. Li, I. Labutov, B. A. Myers, A. Azaria, A. I. Rudnicky, and
T. M. Mitchell, “An end user development approach for failure handling
in goal-oriented conversational agents,” Studies in Conversational UX
Design, 2018.

[11] M. Tissenbaum, J. Sheldon, and H. Abelson, “From computational
thinking to computational action,” Communications of the ACM, vol. 62,
no. 3, pp. 34–36, 2019.

[12] H. P. Grice, “Logic and conversation,” in Speech acts, pp. 41–58, Brill,

1975.

[13] J. Van Brummelen, “Tools to create and democratize conversational
artiﬁcial intelligence,” Master’s thesis, Massachusetts Institute of Tech-
nology, 2019.

[14] A. Inc., “Voice design guide,” 2019.

https://developer.amazon.com/

designing-for-voice/, Last accessed on 2020-02-24.

[15] Google, “Conversation design,” 2019. https://developers.google.com/

actions/design/, Last accessed on 2020-02-24.

[16] M. G. Helander, Handbook of human-computer interaction. Elsevier,

2014.

[17] J. Good and K. Howland, “Programming language, natural language?
supporting the diverse computational activities of novice programmers,”
Journal of Visual Languages & Computing, vol. 39, pp. 78–92, 2017.

[18] T. Quach, “Agent-based programming interfaces for children supporting
blind children in creative computing through conversation,” Master’s
thesis, Massachusetts Institute of Technology, 2019.

[19] T. J.-J. Li, M. Radensky, J. Jia, K. Singarajah, T. M. Mitchell, and
B. A. Myers, “Pumice: A multi-modal agent that learns concepts and
conditionals from natural language and demonstrations,” in Proceedings
of the 32nd Annual ACM Symposium on User Interface Software and
Technology, pp. 577–589, 2019.

[20] A. Desilets, “Voicegrip: a tool for programming-by-voice,” International
Journal of Speech Technology, vol. 4, no. 2, pp. 103–116, 2001.
[21] L. Rosenblatt, “Vocalide: An ide for programming via speech recog-
nition,” in Proceedings of the 19th International ACM SIGACCESS
Conference on Computers and Accessibility, pp. 417–418, 2017.
[22] J. Leggett and G. Williams, “An empirical investigation of voice as an
input modality for computer programming,” International Journal of
Man-Machine Studies, vol. 21, no. 6, pp. 493–520, 1984.

[23] H. Jung, H. J. Kim, S. So, J. Kim, and C. Oh, “Turtletalk: an educational
programming game for children with voice user interface,” in Extended
Abstracts of the 2019 CHI Conference on Human Factors in Computing
Systems, pp. 1–6, 2019.

[24] J. Thomason, A. Padmakumar, J. Sinapov, N. Walker, Y. Jiang, H. Yedid-
sion, J. Hart, P. Stone, and R. J. Mooney, “Jointly improving parsing
and perception for natural language commands through human-robot
dialog,” Journal of Artiﬁcial Intelligence Research, vol. 67, pp. 1–48,
2020.

[25] J. E. Godinez and H. M. Jamil, “Meet cyrus: the query by voice mobile
assistant for the tutoring and formative assessment of sql learners,” in
Proceedings of the 34th ACM/SIGAPP Symposium on Applied Comput-
ing, pp. 2461–2468, 2019.

[26] Google, “Google cloud speech-to-text,” 2020. https://cloud.google.com/

speech-to-text, Last accessed on 2020-02-24.

[27] A. Armaly, P. Rodeghero, and C. McMillan, “A comparison of program
comprehension strategies by blind and sighted programmers,” IEEE
Transactions on Software Engineering, vol. 44, no. 8, pp. 712–724, 2017.

