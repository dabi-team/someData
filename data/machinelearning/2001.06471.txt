1
2
0
2

n
u
J

6

]
L
M

.
t
a
t
s
[

2
v
1
7
4
6
0
.
1
0
0
2
:
v
i
X
r
a

Learning Sparse Classifiers

Learning Sparse Classiﬁers: Continuous and Mixed Integer
Optimization Perspectives

Antoine Dedieu
Operations Research Center
Massachusetts Institute of Technology
Cambridge, MA 02139, USA

Hussein Hazimeh
Operations Research Center
Massachusetts Institute of Technology
Cambridge, MA 02139, USA

Rahul Mazumder
Sloan School of Management
Operations Research Center
MIT Center for Statistics
Massachusetts Institute of Technology
Cambridge, MA 02142, USA

tonio.dedieu@gmail.com

hazimeh@mit.edu

rahulmaz@mit.edu

Abstract

We consider a discrete optimization formulation for learning sparse classiﬁers, where the
outcome depends upon a linear combination of a small subset of features. Recent work
has shown that mixed integer programming (MIP) can be used to solve (to optimality)
(cid:96)0-regularized regression problems at scales much larger than what was conventionally con-
sidered possible. Despite their usefulness, MIP-based global optimization approaches are
signiﬁcantly slower compared to the relatively mature algorithms for (cid:96)1-regularization and
heuristics for nonconvex regularized problems. We aim to bridge this gap in computation
times by developing new MIP-based algorithms for (cid:96)0-regularized classiﬁcation. We pro-
pose two classes of scalable algorithms: an exact algorithm that can handle p ≈ 50, 000
features in a few minutes, and approximate algorithms that can address instances with
p ≈ 106 in times comparable to the fast (cid:96)1-based algorithms. Our exact algorithm is based
on the novel idea of integrality generation, which solves the original problem (with p bi-
nary variables) via a sequence of mixed integer programs that involve a small number of
binary variables. Our approximate algorithms are based on coordinate descent and local
combinatorial search. In addition, we present new estimation error bounds for a class of
(cid:96)0-regularized estimators. Experiments on real and synthetic data demonstrate that our
approach leads to models with considerably improved statistical performance (especially,
variable selection) when compared to competing methods.

Keywords:

sparsity, sparse classiﬁcation, l0 regularization, mixed integer programming

1. Introduction

We consider the problem of sparse linear classiﬁcation, where the output depends upon a
linear combination of a small subset of features. This is a core problem in high-dimensional

1

 
 
 
 
 
 
Dedieu, Hazimeh, and Mazumder

statistics (Hastie et al., 2015) where the number of features p is comparable to or exceeds the
number of samples n. In such settings, sparsity can be useful from a statistical viewpoint
and can lead to more interpretable models. We consider the typical binary classiﬁcation
problem with samples (xi, yi), i = 1, . . . , n, features xi ∈ Rp, and outcome yi ∈ {−1, +1}. In
the spirit of best-subset selection in linear regression (Miller, 2002), we consider minimizing
the empirical risk (i.e., a surrogate for the misclassiﬁcation error) while penalizing the
number of nonzero coeﬃcients:

min
β∈Rp

1
n

n
(cid:88)

i=1

f ((cid:104)xi, β(cid:105), yi) + λ0(cid:107)β(cid:107)0,

(1)

where f : R × {−1, +1} → R is the loss function (for example, hinge or logistic loss). The
term (cid:107)β(cid:107)0 is the (cid:96)0 (pseudo)-norm of β which is equal to the number of nonzeros in β,
and λ0 > 0 is a regularization parameter which controls the number of nonzeros in β. We
ignore the intercept term in the above and throughout the paper to simplify the presenta-
tion. Problem (1) is known to be NP-Hard and poses computational challenges (Natarajan,
1995). In this paper, we introduce scalable algorithms for this optimization problem using
techniques based on both continuous and discrete optimization, speciﬁcally, mixed integer
programming (Wolsey and Nemhauser, 1999).

There is an impressive body of work on obtaining approximate solutions to Problem (1):
popular candidates include greedy (a.k.a. stepwise) procedures (Bahmani et al., 2013), prox-
imal gradient methods (Blumensath and Davies, 2009), among others. The (cid:96)1-norm (Tib-
shirani, 1996) is often used as a convex surrogate to the (cid:96)0-norm, leading to a convex
optimization problem. Nonconvex continuous penalties (such as MCP and SCAD) (Zhang,
2010) provide better approximations of the (cid:96)0-penalty but lead to nonconvex problems,
for which gradient-based methods (Gong et al., 2013; Parikh and Boyd, 2014; Li and Lin,
2015) and coordinate descent (Breheny and Huang, 2011; Mazumder et al., 2011) are often
used. These algorithms may not deliver optimal solutions for the associated nonconvex
problem. Fairly recently, there has been considerable interest in exploring Mixed Integer
Programming (MIP)-based methods (Wolsey and Nemhauser, 1999; Bertsimas et al., 2016;
Ustun and Rudin, 2016; Sato et al., 2016) to solve variants of Problem (1) to optimality.
MIP-based methods create a branch-and-bound tree that simultaneously leads to feasible
solutions and corresponding lower-bounds (a.k.a. dual bounds). Therefore, these meth-
ods deliver optimality certiﬁcates for the nonconvex optimization problem. Despite their
appeal in delivering nearly optimal solutions to Problem (1), MIP-based algorithms are
usually computationally expensive compared to convex relaxations or greedy (heuristic) al-
gorithms (Hazimeh and Mazumder, 2020; Hastie et al., 2020)—possibly limiting their use
in time-sensitive applications that arise in practice.

The vanilla version of best-subset selection is often perceived as a gold-standard for high-
dimensional sparse linear regression, when the signal-to-noise ratio (SNR) is high. However,
it suﬀers from overﬁtting when the SNR becomes moderately low (Friedman et al., 2001;
Mazumder et al., 2017; Hastie et al., 2020). A possible way to mitigate this shortcom-
ing is by imposing additional continuous regularization—see for example, Mazumder et al.
(2017); Hazimeh and Mazumder (2020) for studies in the (linear) regression setting. Thus,
we consider an extended family of estimators which combines (cid:96)0 and (cid:96)q (for q ∈ {1, 2})

2

Learning Sparse Classifiers

regularization:

min
β∈Rp

1
n

n
(cid:88)

i=1

f ((cid:104)xi, β(cid:105), yi) + λ0(cid:107)β(cid:107)0 + λq(cid:107)β(cid:107)q
q,

(2)

where the regularization parameter λ0 ≥ 0 explicitly controls the sparsity in β, and λq ≥ 0
controls the amount of continuous shrinkage on the nonzero coeﬃcients of β (for example,
the margin in linear SVM). In what follows, for notational convenience, we will refer to the
combination of regularizers λ0(cid:107)β(cid:107)0 and λq(cid:107)β(cid:107)q
q, as the (cid:96)0-(cid:96)q penalty. For ﬂexibility, our
framework allows for both choices of q ∈ {1, 2}, and the value of q needs to be speciﬁed
a-priori by the practitioner. When q = 2, Problem (2) seeks to deliver a solution β with
few nonzeros (controlled by the (cid:96)0-penalty) and a small (cid:96)2-norm (controlled by the ridge
penalty). Similarly, when q = 1, we seek a model β that has a small (cid:96)1-norm and a small
(cid:96)0-norm. If λ1 is large, the (cid:96)1-penalty may also encourage zeros in the coeﬃcients. Note
that the primary role of the (cid:96)0-penalty is to control the number of nonzeros in β; and that of
the (cid:96)1-penalty is to shrink the model coeﬃcients. In our numerical experiments we observe
that both choices of q ∈ {1, 2} work quite well, with no penalty uniformly dominating the
other. We refer the reader to Mazumder et al. (2017) for complementary discussions in the
regression setting.

A primary focus of our work is to propose new scalable algorithms for solving Prob-
lem (2), with certiﬁcates of optimality (suitably deﬁned). Problem (2) can be expressed
using MIP formulations. However, these formulations lead to computational challenges for
oﬀ-the-shelf commercial MIP solvers (such as Gurobi and CPLEX). To this end, we propose
a new MIP-based algorithm that we call “integrality generation”, which allows for solving
instances of Problem (2) with p ≈ 50, 000 (where n is small) to optimality within a few
minutes.1 This appears to be well beyond the capabilities of state-of-the-art MIP solvers,
including recent MIP-based approaches, as outlined below. To obtain high-quality solutions
for larger problem instances, in times comparable to the fast (cid:96)1-based solvers (Friedman
et al., 2010), we propose approximate algorithms based on coordinate descent (CD) (Wright,
2015) and local combinatorial optimization,2 where the latter leads to higher quality so-
lutions compared to CD. Our CD and local combinatorial optimization algorithms are
publicly available through our fast C++/R toolkit L0Learn: on CRAN at https://cran.
r-project.org/package=L0Learn and also at https://github.com/hazimehh/L0Learn.
From a statistical viewpoint, we establish new upper bounds on the estimation error
for solutions obtained by globally minimizing (cid:96)0-based estimators (2). These error bounds
(rates) appear to be better than current known bounds for (cid:96)1-regularization; and have rates
similar to the optimal minimax rates for sparse least squares regression (Raskutti et al.,
2011), achieved by (cid:96)0-based regression procedures.

Related Work and Contributions: There is a vast body of work on developing
optimization algorithms and understanding the statistical properties of various sparse esti-
mators (Hastie et al., 2015; B¨uhlmann and Van De Geer, 2011). We present a brief overview
of work that relates to our paper.

1. Empirically, we observe the runtime to depend upon the number of nonzeros in the solution. The runtime
can increase if the number of nonzeros in an optimal solution becomes large—see Section 5 for details.
2. The local combinatorial optimization algorithms are based on solving MIP problems over restricted

search-spaces; and are usually much faster to solve compared to the full problem (2).

3

Dedieu, Hazimeh, and Mazumder

Computation: An impressive body of work has developed fast algorithms for min-
imizing the empirical risk regularized with convex or nonconvex proxies to the (cid:96)0-norm,
e.g., Friedman et al. (2010); Breheny and Huang (2011); Mazumder et al. (2011); Nesterov
(2013); Shalev-Shwartz and Zhang (2012). Below, we discuss related work that directly
optimize objective functions involving an (cid:96)0 norm (in the objective or as a constraint).

Until recently, global optimization with (cid:96)0-penalization was rarely used beyond p = 30
as popular software packages for best-subset selection (for example, leaps and bestglm) are
unable to handle larger instances. Bertsimas et al. (2016) demonstrated that (cid:96)0-regularized
regression problems could be solved to near-optimality for p ≈ 103 by leveraging advances
in ﬁrst-order methods and the capabilities of modern MIP solvers such as Gurobi. Bert-
simas and King (2017); Sato et al. (2016) extend the work of Bertsimas et al. (2016) to
solve (cid:96)0-regularized logistic regression by using an outer-approximation approach that can
address problems with p in the order of a few hundreds. Bertsimas et al. (2020) propose a
cutting plane algorithm for the (cid:96)0-constrained least squares problem with additional ridge
regularization—they can handle problems with n ≈ p, when the feature correlations are
low and/or the amount of ridge regularization is taken to be suﬃciently large. Bertsimas
et al. (2017) adapt Bertsimas et al. (2020)’s work to solve classiﬁcation problems (e.g., with
logistic or hinge loss). The approach of Bertsimas et al. (2017) appears to require a fairly
high amount of ridge regularization for the cutting plane algorithm to work well.

A separate line of research investigates algorithms to obtain feasible solutions for (cid:96)0-
regularized problems. These algorithms do not provide dual bounds like MIP-based algo-
rithms, but can be computationally much faster. These include: (i) ﬁrst-order optimization
algorithms based on hard thresholding, such as Iterative Hard Thresholding (IHT) (Blumen-
sath and Davies, 2009) and GraSP (Bahmani et al., 2013), (ii) second-order optimization
algorithms inspired by the Newton method such as NTGP (Yuan and Liu, 2017), NHTP
(Zhou et al., 2021), and NSLR (Wang et al., 2019), and (iii) coordinate descent methods
based on greedy and random coordinate selection rules (Beck and Eldar, 2013; Patrascu
and Necoara, 2015).

Hazimeh and Mazumder (2020) present algorithms that oﬀer a bridge between MIP-
based global optimization and good feasible solutions for (cid:96)0-regularized problems, by using
a combination of CD and local combinatorial optimization. The current paper is similar in
spirit, but makes new contributions. We extend the work of Hazimeh and Mazumder (2020)
(which is tailored to the least squares loss function) to address the more general class of
problems in (2). Our algorithms can deliver solutions with better statistical performance
(for example, in terms of variable selection and prediction error) compared to the popular
fast algorithms for sparse learning (e.g., based on (cid:96)1 and MCP regularizers). Unlike heuris-
tics that simply deliver an upper bound, MIP-based approaches attempt to solve (2) to
optimality. They can (i) certify via dual bounds the quality of solutions obtained by our
CD and local search algorithms; and (ii) improve the solution if it is not optimal. However,
as oﬀ-the-shelf MIP-solvers do not scale well, we present a new method: the Integrality
Generation Algorithm (IGA) (see Section 3) that allows us to solve (to optimality) the MIP
problems for instances that are larger than current methods (Bertsimas and King, 2017;
Bertsimas et al., 2017; Sato et al., 2016). The key idea behind our proposed IGA is to solve
a sequence of relaxations of (2) by allowing only a subset of variables to be binary. On the

4

Learning Sparse Classifiers

contrary, a direct MIP formulation for (2) requires p many binary variables; and can be
prohibitively expensive for moderate values of p.

Statistical Properties: Statistical properties of high-dimensional linear regression
have been widely studied (Candes and Davenport, 2013; Raskutti et al., 2011; Bunea et al.,
2007; Candes and Tao, 2007; Bickel et al., 2009). One important statistical performance
measure is the (cid:96)2-estimation error deﬁned as (cid:107)β∗ − ˆβ(cid:107)2
2, where β∗ is the k-sparse vector used
in generating the true model and ˆβ is an estimator. For regression problems, Candes and
Davenport (2013); Raskutti et al. (2011) established a (k/n) log(p/k) lower bound on the (cid:96)2-
estimation error. This optimal minimax rate is known to be achieved by a global minimizer
of an (cid:96)0-regularized estimator (Bunea et al., 2007).
It is well known that the Dantzig
Selector and Lasso estimators achieve a (k/n) log(p) error rate (Candes and Tao, 2007;
Bickel et al., 2009) under suitable assumptions for the high-dimensional regression setting.
Compared to regression, there has been limited work in deriving estimation error bounds
for classiﬁcation tasks. Tarigan and Van De Geer (2006) study margin adaptation for (cid:96)1-
norm SVM. A sizable amount of work focuses on the analysis of generalization error and risk
bounds (Greenshtein, 2006; Van de Geer, 2008). Zhang et al. (2016) study variable selection
consistency of a nonconvex penalized SVM estimator, using a local linear approximation
method with a suitable initialization. Recently, Peng et al. (2016) proved a (k/n) log(p)
upper-bound for the (cid:96)2-estimation error of (cid:96)1-regularized support vector machines (SVM),
where k is the number of nonzeros in the estimator that minimizes the population risk.
Ravikumar et al. (2010) show consistent neighborhood selection for high-dimensional Ising
model using an (cid:96)1-regularized logistic regression estimator. Plan and Vershynin (2013) show
that one can obtain an error rate of k/n log(p/k) for 1-bit compressed sensing problems.
In this paper, we present (to our knowledge) new (cid:96)2-estimation error bounds for a (global)
minimizer of Problem (1)—our framework applies to a family of loss functions including
the hinge and logistic loss functions.

Our Contributions: We summarize our contributions below:

• We develop fast ﬁrst-order algorithms based on cyclic CD and local combinatorial
search to (approximately) solve Problem (2) (see Section 2). We prove a new result
which establishes the convergence of cyclic CD under an asymptotic linear rate. We
show that combinatorial search leads to solutions of higher quality than IHT and
CD-based methods. We discuss how solutions from the (cid:96)0-penalized formulation, i.e.,
Problem (2), can be used to obtain solutions to the cardinality constrained variant
of (2). We open source these algorithms through our sparse learning toolkit L0Learn.

• We propose a new algorithm: IGA, for solving Problem (2) to optimality. On some
problems, our algorithm reduces the time for solving a MIP formulation of Problem (2)
from the order of hours to seconds, and it can solve high-dimensional instances with
p ≈ 50, 000 and small n. The algorithm is presented in Section 3.

• We establish upper bounds on the squared (cid:96)2-estimation error for a cardinality con-
strained variant of Problem (2). Our (k/n) log(p/k) upper bound matches the optimal
minimax rate known for regression.

• On a series of high-dimensional synthetic and real data sets (with p ≈ 105), we show
that our proposed algorithms can achieve signiﬁcantly better statistical performance

5

Dedieu, Hazimeh, and Mazumder

in terms of prediction (AUC), variable selection accuracy, and support sizes, com-
pared to state-of-the-art algorithms (based on (cid:96)1 and local solutions to (cid:96)0 and MCP
regularizers). Our proposed CD algorithm compares favorably in terms of runtime
compared to current popular toolkits (Friedman et al., 2010; Breheny and Huang,
2011; Bahmani et al., 2013; Zhou et al., 2021) for sparse classiﬁcation.

1.1 Preliminaries and Notation

For convenience, we introduce the following notation:

g(β) def=

1
n

n
(cid:88)

i=1

f ((cid:104)xi, β(cid:105), yi)

and G(β) def= g(β) + λ1(cid:107)β(cid:107)1 + λ2(cid:107)β(cid:107)2
2.

Problem (2) is an instance of the following (more general) problem:

P (β) def= G(β) + λ0(cid:107)β(cid:107)0.

min
β∈Rp

(3)

In particular, Problem (2) with q = 1 is equivalent to Problem (3) with λ2 = 0. Similarly,
Problem (2) with q = 2 is equivalent to Problem (3) with λ1 = 0. Problem (3) will be the
focus in our algorithmic development.

We denote the set {1, 2, . . . , p} by [p] and the canonical basis for Rp by e1, . . . , ep. For
β ∈ Rp, we use Supp(β) to denote the support of β, i.e., the indices of its nonzero entries.
For S ⊆ [p], βS ∈ R|S| denotes the subvector of β with indices in S. Moreover, for a
diﬀerentiable function g(β), we use the notation ∇Sg(β) to refer to the subvector of the
gradient ∇g(β) restricted to coordinates in S. We let Z and Z+ denote the set of integers
and non-negative integers, respectively. A convex function g(β) is said to be µ-strongly
convex if β (cid:55)→ g(β) − µ(cid:107)β(cid:107)2
2/2 is convex. A function h(β) is said to be Lipschitz with
parameter L if (cid:107)h(β) − h(α)(cid:107)2 ≤ L(cid:107)β − α(cid:107)2 for all β, α in the domain of the function.

1.2 Examples of Loss Functions Considered

In Table 1, we give examples of popular classiﬁcation loss functions that fall within the
premise of our algorithmic framework and statistical theory. The column “FO & Local
Search” indicates whether these loss functions are amenable to our ﬁrst-order and local
search algorithms (discussed in Section 2).3 The column “MIP” indicates whether the
loss function leads to an optimization problem that can be solved (to optimality) via the
MIP methods discussed in Section 3. Finally, the column “Error Bounds” indicates if the
statistical error bounds (estimation error) discussed in Section 4 apply to the loss function.

2. First-Order and Local Combinatorial Search Algorithms

Here we present fast cyclic CD and local combinatorial search algorithms for obtaining high-
quality local minima (we make this notion precise later) for Problem (3). Our framework
assumes that g(β) is diﬀerentiable and has a Lipschitz continuous gradient. We ﬁrst present

3. That is, these algorithms are guaranteed to converge to a stationary point (or a local optimum) for the

corresponding optimization problems.

6

Learning Sparse Classifiers

Loss

f (ˆv, v)

FO & Local Search MIP Error Bounds

log(1 + e−ˆvv)

Logistic
Squared Hinge max(0, 1 − ˆvv)2
max(0, 1 − ˆvv)
Hinge

(cid:51)
(cid:51)
(cid:51)*

(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:55)
(cid:51)

Table 1: Examples of loss functions we consider. “*” denotes that our proposed ﬁrst-order
and local search methods apply upon using Nesterov (2012)’s smoothing on the
non-smooth loss function.

a brief overview of the key ideas presented in this section, before diving into the technical
details.

Due to the nonconvexity of (3), the quality of the solution obtained depends on the
algorithm—with local search and MIP-based algorithms leading to solutions of higher qual-
ity. The ﬁxed points of the algorithms considered satisfy certain necessary optimality condi-
tions for (3), leading to diﬀerent classes of local minima. In terms of solution quality, there is
a hierarchy among these classes. We show that for Problem (3), the minima corresponding
to the diﬀerent algorithms satisfy the following hierarchy:

MIP Minima ⊆ Local Search Minima ⊆ CD Minima ⊆ IHT Minima.

(4)

The ﬁxed points of the IHT algorithm contain the ﬁxed points of the CD algorithm. As we
move to the left in the hierarchy, the ﬁxed points of the algorithms satisfy stricter necessary
optimality conditions. At the top of the hierarchy, we have the global minimizers, which
can be obtained by solving a MIP formulation of (3).

Our CD and local search algorithms can run in times comparable to the fast (cid:96)1-regularized
approaches (Friedman et al., 2010). These algorithms can lead to high-quality solutions that
can be used as warm starts for MIP-based algorithms. The MIP framework of Section 3
can be used to certify the quality of these solutions via dual bounds, and to improve over
them (if they are sub-optimal).

In Section 2.1, we introduce cyclic CD for Problem (3) and study its convergence prop-
erties. Section 2.2 discusses how the solutions of cyclic CD can be improved by local search
and presents a fast heuristic for performing local search in high dimensions. In Section 2.3,
we discuss how our algorithms can be used to obtain high-quality (feasible) solutions to
the cardinality constrained counterpart of (3), in which the complexity measure (cid:107)β(cid:107)0 ap-
pears as a constraint and not a penalty as in (3). Finally, in Section 2.4, we brieﬂy present
implementation aspects of our toolkit L0Learn.

2.1 Cyclic Coordinate Descent: Algorithm and Computational Guarantees

We describe a cyclic CD algorithm for Problem (3) and establish its convergence to station-
ary points of (3).

Why cyclic CD? We brieﬂy discuss our rationale for choosing cyclic CD. Cyclic CD
has been shown to be among the fastest algorithms for ﬁtting generalized linear models with
convex and nonconvex regularization (e.g., (cid:96)1, MCP, and SCAD) (Friedman et al., 2010;

7

Dedieu, Hazimeh, and Mazumder

Mazumder et al., 2011; Breheny and Huang, 2011). Indeed, it can eﬀectively exploit spar-
sity and active-set updates, making it suitable for solving high-dimensional problems (e.g.,
with p ∼ 106 and small n). Algorithms that require evaluation of the full gradient at every
iteration (such as proximal gradient, stepwise, IHT or greedy CD algorithms) have diﬃcul-
ties in scaling with p (Nesterov, 2012). In an earlier work, Patrascu and Necoara (2015)
proposed random CD for problems similar to (3) (without an (cid:96)1-regularization term in the
objective). However, recent studies have shown that cyclic CD can be faster than random
CD (Beck and Tetruashvili, 2013; Gurbuzbalaban et al., 2017; Hazimeh and Mazumder,
2020). Furthermore, for (cid:96)0-regularized regression problems, cyclic CD is empirically seen to
obtain solutions of higher quality (e.g., in terms of optimization and statistical performance)
compared to random CD (Hazimeh and Mazumder, 2020).

Setup. Our cyclic CD algorithm for (3) applies to problems where g(β) is convex,
continuously diﬀerentiable, and non-negative. Moreover, we will assume that the gradient
of β (cid:55)→ g(β) is coordinate-wise Lipschitz continuous, i.e., for every i ∈ [p], β ∈ Rp and
s ∈ R, we have:

|∇ig(β + eis) − ∇ig(β)| ≤ Li|s|,

(5)

where Li > 0 is the Lipschitz constant for coordinate i. This assumption leads to the block
Descent Lemma (Bertsekas, 2016), which states that

g(β + eis) ≤ g(β) + s∇ig(β) +

1
2

Lis2.

(6)

Several popular loss functions for classiﬁcation fall under the above setup. For example,
logistic loss and squared hinge loss satisfy (5) with Li = (cid:107)X i(cid:107)2
2/n,
respectively (here, Xi denotes the i-th column of the data matrix X).

2/4n and Li = 2(cid:107)X i(cid:107)2

CD Algorithm. Cyclic CD (Bertsekas, 2016) updates one coordinate at a time (with
others held ﬁxed) in a cyclical fashion. Given a solution β ∈ Rp, we attempt to ﬁnd a new
solution by changing the i-th coordinate of β—i.e., we ﬁnd α such that αj = βj for all j (cid:54)= i
and αi minimizes the one-dimensional function: βi (cid:55)→ P (β). However, for the examples we
consider (e.g., logistic and squared hinge losses), there is no closed-form expression for this
minimization problem. This makes the algorithm computationally ineﬃcient compared to
g being the squared error loss (Hazimeh and Mazumder, 2020). Using (6), we consider a
quadratic upper bound ˜g(α; β) for g(α) as follows:

g(α) ≤ ˜g(α; β) def= g(β) + (αi − βi)∇ig(β) +

ˆLi
2

(αi − βi)2,

(7)

where ˆLi is a constant which satisﬁes ˆLi > Li. (For notational convenience, we hide the
dependence of ˜g on i.) Let us deﬁne the function

ψ(α) =

(cid:88)

i

ψi(αi) where ψi(αi) = λ01(αi (cid:54)= 0) + λ1|αi| + λ2α2
i .

Adding ψ(α) to both sides of equation (7), we get:

P (α) ≤ (cid:101)P ˆLi

(α; β) def= ˜g(α; β) + ψ(α).

(8)

8

Learning Sparse Classifiers

We can approximately minimize P (α) w.r.t. αi (with other coordinates held ﬁxed) by
minimizing its upper bound (cid:101)P ˆLi
(α; β) w.r.t. αi. A solution ˆαi for this one-dimensional
optimization problem is given by

ˆαi ∈ argmin

αi

(cid:101)P ˆLi

(α; β) = argmin

αi

(cid:18)

(cid:18)

αi −

βi −

ˆLi
2

1
ˆLi

(cid:19)(cid:19)2

∇ig(β)

+ ψi(αi).

(9)

Let ˆα be a vector whose i-th component is ˆαi, and ˆαj = βj for all j (cid:54)= i. Note that
P ( ˆα) ≤ P (β)—i.e., updating the i-th coordinate via (9) with all other coeﬃcients held
ﬁxed, leads to a decrease in the objective value P (β). A solution of (9) can be computed
in closed-form; and is given by the thresholding operator T : R → R deﬁned as follows:

T (c; λ, ˆLi) =

(cid:16)




ˆLi
ˆLi+2λ2
0


|c| − λ1
ˆLi

(cid:17)

sign(c)

(cid:16)

if

ˆLi
ˆLi+2λ2
otherwise

|c| − λ1
ˆLi

(cid:17)

≥

(cid:113) 2λ0

ˆLi+2λ2

(10)

where c = βi − ∇ig(β)/ ˆLi and λ = (λ0, λ1, λ2).

Algorithm 1 below, summarizes our cyclic CD algorithm.

Algorithm 1: Cyclic Coordinate Descent (CD)

• Input: Initialization β0 and constant ˆLi > Li for every i ∈ [p]

• Repeat for l = 0, 1, 2, . . . until convergence:

1. i ← 1 + (l mod p) and βl+1

j ← βl
j for all j (cid:54)= i
1, . . . , βi, . . . , βl
(βl
← argminβi (cid:101)P ˆLi

2. Update βl+1

i

c = βl

i − ∇ig(βl)/ ˆLi

p; βl) using (10) with

Computational Guarantees. The convergence of cyclic CD has been extensively
studied for certain classes of continuous objective functions, e.g., see Tseng (2001); Bertsekas
(2016); Beck and Tetruashvili (2013) and the references therein. However, these results do
not apply to our objective function due to the discontinuity in the (cid:96)0-norm. In Theorem 1, we
establish a new result which shows that cyclic CD (Algorithm 1) converges at an asymptotic
linear rate, and we present a characterization of the corresponding solution. Theorem 1 is
established under the following assumption:

Assumption 1 Problem (3) satisﬁes at least one of the following conditions:

1. Strong convexity of the continuous regularizer, i.e., λ2 > 0.

2. Restricted Strong Convexity: For some u ∈ [p], the function βS (cid:55)→ g(βS) is strongly
convex for every S ⊆ [p] such that |S| ≤ u. Moreover, λ0 and the initial solution β0
are chosen such that P (β0) < uλ0.

Theorem 1 Let {βl} be the sequence of iterates generated by Algorithm 1. Suppose that
Assumption 1 holds, then:

9

Dedieu, Hazimeh, and Mazumder

1. The support of βl stabilizes in a ﬁnite number of iterations, i.e., there exists an integer

N and support S ⊂ [p] such that Supp(βl) = S for all l ≥ N .

2. Let S be the support as deﬁned in Part 1. The sequence {βl} converges to a solution

β∗ with support S, satisfying:

β∗

S ∈ argmin

G(βS)

βS
(cid:115)

|β∗

i | ≥

2λ0
ˆLi + 2λ2

for i ∈ S

(11)

and

|∇ig(β∗)| − λ1 ≤

(cid:113)

2λ0( ˆLi + 2λ2)

for i ∈ Sc.

3. Let S be the support as deﬁned in Part 1. Let us deﬁne H(βS) def= g(βS) + λ2(cid:107)βS(cid:107)2
2.
Let σS be the strong convexity parameter of βS (cid:55)→ H(βS), and let LS be the Lipschitz
constant of βS (cid:55)→ ∇SH(βS). Denote ˆLmax = maxi∈S ˆLi and ˆLmin = mini∈S ˆLi. Then,
there exists an integer N (cid:48) such that the following holds for all t ≥ N (cid:48):

P (β(t+1)p) − P (β∗) ≤

(cid:18)

1 −

(cid:19)

σS
γ

(cid:0)P (βtp) − P (β∗)(cid:1) ,

(12)

where γ−1 = 2 ˆLmax(1 + |S|L2
S

ˆL−2

min).

We provide a proof of Theorem 1 in the appendix. The proof is diﬀerent from that
of CD for (cid:96)0-regularized regression (Hazimeh and Mazumder, 2020) since we use inexact
minimization for every coordinate update, whereas Hazimeh and Mazumder (2020) use
exact minimization. At a high level, the proof proceeds as follows. In Part 1, we prove a
suﬃcient decrease condition which establishes that the support stabilizes in a ﬁnite number
of iterations. In Part 2, we show that under Assumption 1, the objective function is strongly
convex when restricted to the stabilized support. After restriction to the stabilized support,
we obtain convergence from standard results on cyclic CD (Bertsekas, 2016). In Part 3, we
show an asymptotic linear rate of convergence for Algorithm 1. To establish this rate, we
extend the linear rate of convergence of cyclic CD for smooth strongly convex functions by
Beck and Tetruashvili (2013) to our objective function (note that due to the presence of
the (cid:96)1-norm, our objective is not smooth even after support stabilization).

Stationary Points of CD versus IHT: The conditions in (11) describe a ﬁxed point
of the cyclic CD algorithm and are necessary optimality conditions for Problem (3). We
now show that the stationary conditions (11) are strictly contained within the class of
stationary points arising from the IHT algorithm (Blumensath and Davies, 2009; Beck and
Eldar, 2013; Bertsimas et al., 2016). Recall that IHT can be interpreted as a proximal
gradient algorithm, whose updates for Problem (3) are given by:

βl+1 ∈ argmin

β

(cid:26) 1
2τ

(cid:107)β − (βl − τ ∇g(βl))(cid:107)2

2 + ψ(β)

(cid:27)

,

(13)

where τ > 0 is a step size. Let L be the Lipschitz constant of β (cid:55)→ ∇g(β), and let ˆL
be any constant satisfying ˆL > L. Update (13) is guaranteed to converge to a stationary

10

Learning Sparse Classifiers

point if τ = 1/ ˆL (e.g., see Lu 2014; Hazimeh and Mazumder 2020). Note that ˜β is a ﬁxed
point for (13) if it satisﬁes (11) with ˆLi replaced with ˆL. The component-wise Lipschitz
constant Li always satisﬁes Li ≤ L. For high-dimensional problems, we may have Li (cid:28) L
(see discussions in Beck and Eldar 2013; Hazimeh and Mazumder 2020 for problems where
L grows with p but Li is constant). Hence, the CD optimality conditions in (11) are more
restrictive than IHT—justifying a part of the hierarchy mentioned in (4). An important
practical consequence of this result is that CD may lead to solutions of higher quality than
IHT.

Remark 2 A solution β∗ that satisﬁes the CD or IHT stationarity conditions is a local
minimizer in the traditional sense used in nonlinear optimization.4 Thus, we use the terms
stationary point and local minimizer interchangeably in our exposition.

2.2 Local Combinatorial Search

We propose a local combinatorial search algorithm to improve the quality of solutions ob-
tained by Algorithm 1. Given a solution from Algorithm 1, the idea is to perform small
perturbations to its support in an attempt to improve the objective. This approach has
been recently shown to be very eﬀective (e.g, in terms of statistical performance) for (cid:96)0-
regularized regression (Hazimeh and Mazumder, 2020), especially under diﬃcult statistical
settings (high feature correlations or n is small compared to p). Here, we extend the ap-
proach of Hazimeh and Mazumder (2020) to general loss functions, discussed in Section 2.1.
As we consider a general loss function, performing exact local minimization becomes compu-
tationally expensive—we thus resort to an approximate minimization scheme. This makes
our approach diﬀerent from the least squares setting considered in Hazimeh and Mazumder
(2020).

Our local search algorithm is iterative.

It performs the following two steps at every

iteration t:

1. Coordinate Descent: We run cyclic CD (Algorithm 1) initialized from the current

solution, to obtain a solution βt with support S.

2. Combinatorial Search: We attempt to improve βt by making a change to its current
support S via a swap operation. In particular, we search for two subsets of coordinates
S1 ⊂ S and S2 ⊂ Sc, each of size at most m, such that removing coordinates S1 from
the support, adding S2 to the support, and then optimizing over the coeﬃcients in
S2, improves the current objective value.

To present an optimization formulation for the combinatorial search step (discussed above),
we introduce some notation. Let U S denote a p × p matrix whose i-th row is eT
if i ∈ S
i
and zero otherwise. Thus, for any β ∈ Rp, (U Sβ)i = βi if i ∈ S and (U Sβ)i = 0 if i /∈ S.
The combinatorial search step solves the following optimization problem:

min
S1,S2,β

P (βt − U S1βt + U S2β)

s.t.

S1 ⊂ S, S2 ⊂ Sc, |S1| ≤ m, |S2| ≤ m,

(14)

4. That is, given a stationary point β∗, there is a small (cid:15) > 0 such that any β lying in the set (cid:107)β − β∗(cid:107)2 ≤ (cid:15)

will have an objective that is at least as large as the current objective value P (β∗).

11

Dedieu, Hazimeh, and Mazumder

where the optimization variables are the subsets S1, S2 and the coeﬃcients of β restricted
to S2. If there is a feasible solution ˆβ to (14) satisfying P (ˆβ) < P (βt), then we move to ˆβ.
Otherwise, the current solution βt cannot be improved by swapping subsets of coordinates,
and the algorithm terminates. We summarize the algorithm below.

Algorithm 2: CD with Local Combinatorial Search

• Input: Initialization ˆβ

0

and swap subset size m.

• Repeat for t = 1, 2, . . . :

1. βt ← Output of cyclic CD initialized from ˆβ
2. Find a feasible solution ˆβ to (14) satisfying P (ˆβ) < P (βt).
t
3. If Step 2 succeeds, then set ˆβ

← ˆβ. Otherwise, if Step 2 fails, terminate.

t−1

. Let S ← Supp(βt).

Theorem 3 shows that Algorithm 2 terminates in a ﬁnite number of iterations and

provides a description of the resulting solution.

Theorem 3 Let {βt} be the sequence of iterates generated by Algorithm 2. Then, under
Assumption 1, βt converges in ﬁnitely many steps to a solution β∗ (say). Let S = Supp(β∗).
Then, β∗ satisﬁes the stationary conditions in (11) (see Theorem 1). In addition, for every
S1 ⊂ S and S2 ⊂ Sc with |S1| ≤ m, |S2| ≤ m, the solution β∗ satisﬁes the following
condition:

P (β∗) ≤ min
β

P (β∗ − U S1β∗ + U S2β).

(15)

Algorithm 2 improves the solutions obtained from Algorithm 1. This observation along with
the discussion in Section 2.1, establishes the hierarchy of local minima in (4). The choice of
m in Algorithm 2 controls the quality of the local minima returned—larger values of m will
lead to solutions with better objectives. For a suﬃciently large value of m, Algorithm 2 will
deliver a global minimizer of Problem (3). The computation time of solving Problem (14)
increases with m. We have observed empirically that small choices of m (e.g., m = 1)
can lead to a global minimizer of Problem (3) even for some challenging high-dimensional
problems where the features are highly correlated (see the experiments in Section 5).

Problem (14) can be formulated using MIP—this is discussed in Section 3, where we
also present methods to solve it for large problems. The MIP-based framework allows us
to (i) obtain good feasible solutions (if they are available) or (ii) certify (via dual bounds)
that the current solution cannot be improved by swaps corresponding to size m. Note that
due to the restricted search space, solving (14) for small values of m can be much easier
than solving Problem (3) using MIP solvers. A solution β∗ obtained from Algorithm 2 has
an appealing interpretation: being a ﬁxed point of (15), β∗ cannot be improved by locally
perturbing its support. This serves as a certiﬁcate describing the quality of the current
(locally optimal) solution β∗.

In what follows, we present a fast method to obtain a good solution to Problem (14) for

the special case of m = 1.

Speeding up Combinatorial Search when m = 1: For Problem (14), we ﬁrst check
if removing variable i, without adding any new variables to the support, improves the

12

Learning Sparse Classifiers

Algorithm 3: Fast Heuristic for Local Search when m = 1

• Input: Restricted set size q ∈ Z+ such that q ≤ p − |S|.

• For every i ∈ S:

1. If P (βt − eiβt
2. Compute ∇Scg(βt − eiβt

i ) < P (βt) then terminate and return βt − eiβt
i .

nents with the largest values of |∇jg(βt − eiβt

i )| for j ∈ Sc.

i ) and let J be the set of indices of the q compo-

3. For every j ∈ J:

Solve ˆβj ∈ argminβj ∈R G(βt − eiβt
thresholding operator in (10) (with ˆLi = Li and λ0 = 0). If P (βt − eiβt
ej ˆβj) < P (βt), terminate and return βt − eiβt

i + ejβj) by iteratively applying the
i +

i + ej ˆβj.

objective, i.e., P (βt − eiβt
Step 2 (Algorithm 2). Otherwise, we ﬁnd a feasible solution to Problem (14) by solving

i ) < P (βt). If the latter inequality holds, we declare a success in

min
βj

G(βt − eiβt

i + ejβj)

(16)

for every pair S1 = {i} and S2 = {j}. Performing the full minimization in (16) for every (i, j)
can be expensive—so we propose an approximate scheme that is found to work relatively
well in our numerical experience. We perform a few proximal gradient updates by applying
the thresholding operator deﬁned in (10) with the choice ˆLj = Lj, λ0 = 0, and using βj = 0
as an initial value, to approximately minimize (16)—this helps us identify if the inclusion
of coordinate j leads to a success in Step 2.

The method outlined above requires approximately solving Problem (16) for |S|(p − |S|)
many (i, j)-pairs (in the worst case). This cost can be further reduced if we select j from
a small subset of coordinates outside the current support S, i.e., j ∈ J ⊂ Sc, where
|J| < p − |S|. We choose J so that it corresponds to the q largest (absolute) values of the
gradient |∇jg(βt − eiβt
i )|, j ∈ Sc. As explained in Section A.4, this choice of J ensures
that we search among coordinates j ∈ Sc that lead to the maximal decrease in the current
objective with one step of a proximal coordinate update initialized from βj = 0. We
summarize the proposed method in Algorithm 3.

The cost of applying the thresholding operator in step 3 of Algorithm 3 is O(n).5 For
squared error loss, the cost can be improved to O(1) by reusing previously computed quan-
tities from CD (see Hazimeh and Mazumder 2020 for details). Our numerical experience
suggests that using the above heuristic with values of q ≈ 0.05 × p often leads to the same
solutions returned by full exhaustive search. Moreover, when some of the features are highly
correlated, Algorithm 2 with the heuristic above (or solving Problem 14 exactly) performs
better in terms of variable selection and prediction performance compared to state-of-the-art
sparse learning algorithms (see Section 5.2 for numerical results).

5. Assuming that the derivative of f (·, v) w.r.t the ﬁrst argument can be computed in O(1), which is the

case for common loss functions that we consider here.

13

Dedieu, Hazimeh, and Mazumder

2.3 Solutions for the Cardinality Constrained Formulation

Algorithms 1 and 2 deliver good solutions for the (cid:96)0-penalized problem (3). We now discuss
how they can be used to obtain solutions to the cardinality constrained version:

min
β∈Rp

G(β)

s.t.

(cid:107)β(cid:107)0 ≤ k,

(17)

where k controls the support size of β. While the unconstrained formulation (3) is amenable
to fast CD-based algorithms, some support sizes are often skipped as λ0 is varied. For
example, if we decrease λ0 to λ(cid:48)
0, the support size of the new solution can diﬀer by more
than one, even if λ(cid:48)
0 is taken to be arbitrarily close to λ0. On the other hand, formulation (17)
can typically return a solution with any desired support size,6 and it may be preferable over
the unconstrained formulation in some applications due to its explicit control of the support
size.

Suppose we wish to obtain a solution to Problem (17) for a support size k that is not
available from a sequence of solutions from (3). We propose to apply the IHT algorithm on
Problem (17), initialized by a solution from Algorithm 1 or 2. This leads to the following
update sequence

βl+1 ← argmin
(cid:107)β(cid:107)0≤k

(cid:26) 1
2τ

(cid:16)

(cid:13)
(cid:13)
(cid:13)β −

βl − τ ∇g(βl)

(cid:17)(cid:13)
2
(cid:13)
(cid:13)
2

+ λ1(cid:107)β(cid:107)1 + λ2(cid:107)β(cid:107)2
2

(cid:27)

,

(18)

for l ≥ 0, with initial solution β0 (available from the (cid:96)0-penalized formulation) and τ > 0 is
a step size (e.g., see Theorem 4). We note that update (18) typically returns a solution of
support size k but there can be degenerate cases where a support size of k is not possible
(see footnote 6).

Beck and Eldar (2013) has shown that greedy CD-like algorithms perform better than
IHT for a class of problems similar to (17) (without (cid:96)1-regularization). However, it is
computationally expensive to apply greedy CD methods to (17) for the problem sizes we
study here. Note that since we initialize IHT with a solution from Problem (3), it converges
rapidly to a high-quality feasible solution for Problem (17).

Theorem 4, which follows from Mazumder et al. (2017), establishes that IHT is guaran-

teed to converge, and provides a characterization of its ﬁxed points.

Theorem 4 Let {βl} be a sequence generated by the IHT algorithm updates (18). Let L
be the Lipschitz constant of ∇g(β) and let ˆL > L. Then, the sequence {βl} converges for a
step size τ = 1/ ˆL. Moreover, β∗ with support S is a ﬁxed point of (18) iﬀ (cid:107)β∗(cid:107)0 ≤ k,

β∗

S ∈ argmin

βS

G(βS)

and

|∇ig(β∗)| ≤ δ(k)

for i ∈ Sc,

where δj = | ˆLβ∗

j − ∇jg(β∗)| for j ∈ [p] and δ(k) is the kth largest value of {δj}p
1.

Lemma 5 shows that if a solution obtained by Algorithm 1 or 2 has a support size k, then
it is a ﬁxed point for the IHT update (18).

6. Exceptions can happen if for a subset T ⊂ [p] of size k, the minimum of βT (cid:55)→ G(βT ) has some

coordinates in βT exactly set to zero.

14

Learning Sparse Classifiers

Lemma 5 Let γ > 1 be a constant and β∗ with support size k be a solution for Problem (3)
obtained by using Algorithm 1 or 2 with ˆLi = γLi. Set ˆL = γL and τ = 1/ ˆL in (18). Then,
β∗ is a ﬁxed point of the update (18).

The converse of Lemma 5 is not true, i.e., a ﬁxed point of update (18) may not be a
ﬁxed point for Algorithm 1 or 2—see our earlier discussion around hierarchy (4).7 Thus,
we can generally expect the solutions returned by Algorithm 1 or 2 to be of higher quality
than those returned by IHT.

The following summarizes our procedure to obtain a path of solutions for Problem (17)

(here, we assume that (λ1, λ2) are ﬁxed and λ0 varies):

1. Run Algorithm 1 or 2 for a sequence of λ0 values to obtain a regularization path.

2. To obtain a solution to Problem (17) with a support size (say k) that is not available
in Step 1, we run the IHT updates (18). The IHT updates are initialized with a
solution from Step 1 having a support size smaller than k.

As the above procedure uses high-quality solutions obtained by Algorithm 1 or 2 as advanced
initializations for IHT, we expect to obtain notable performance beneﬁts as compared to
using IHT alone for generating the regularization path. Also, note that if a support size k
is available from Step 1, then there is no need to run IHT (see Lemma 5).

2.4 L0Learn: A Fast Toolkit for (cid:96)0-regularized Learning

We implemented the algorithms discussed above in L0Learn: a fast sparse learning toolkit
written in C++ along with an R interface. We currently support the logistic and squared-
hinge loss functions,8 but the toolkit can be expanded and we intend to incorporate addi-
tional loss functions in the future. Following Friedman et al. (2010); Hazimeh and Mazumder
(2020), we used several computational tricks to speed up the algorithms and improve the
solution quality—these include: warm starts, active sets, correlation screening, a (partially)
greedy heuristic to cycle through the coordinates, and eﬃcient methods for updating the
gradients by exploiting sparsity. Note that Problem (3) can lead to the same solution if two
values of λ0 are close to one another. To avoid this issue, we dynamically select a sequence
of λ0-values—this is an extension of an idea appearing in Hazimeh and Mazumder (2020)
for the least squares loss function.

3. Mixed Integer Programming Algorithms

We present MIP formulations and a new scalable algorithm: IGA, for solving Problem (3)
to optimality. Compared to oﬀ-the-shelf MIP solvers (e.g., the commercial solver Gurobi),
IGA leads to certiﬁably optimal solutions in signiﬁcantly reduced computation times. IGA
also applies to the local search problem of Section 2.2. We remind the reader that while
Algorithm 1 (and Algorithm 2 for small values of m) leads to good feasible solutions quite
fast, it does not deliver certiﬁcates of optimality (via dual bounds). The MIP framework
can be used to certify the quality of solutions obtained by Algorithms 1 or 2 and potentially
improve upon them.

7. Assuming that we obtain the same support sizes for both the constrained and unconstrained formulations.
8. This builds upon our earlier functionality for least squares loss (Hazimeh and Mazumder, 2020).

15

Dedieu, Hazimeh, and Mazumder

3.1 MIP Formulations

Problem (3) admits the following MIP formulation:

(cid:40)

min
β,z

G(β) + λ0

(cid:41)

p
(cid:88)

i=1

zi

s.t.

|βi| ≤ Mzi, zi ∈ {0, 1} , i ∈ [p]

(19)

where M is a constant chosen large enough so that some optimal solution β∗ to (2) satisﬁes
(cid:107)β∗(cid:107)∞ ≤ M. Algorithm 1 and Algorithm 2 (with m = 1) can be used to obtain good
estimates of M—see Bertsimas et al. (2016); Mazumder et al. (2017) for possible alternatives
on choosing M. In (19), the binary variable zi controls whether βi is set to zero or not. If
zi = 0 then βi = 0, while if zi = 1 then βi ∈ [−M, M] is allowed to be ‘free’. For the hinge
loss with q = 1, the problem is a Mixed Integer Linear Program (MILP). For q = 2 and the
hinge loss function, (19) becomes a Mixed Integer Quadratic Program (MIQP). Similarly,
the squared hinge loss leads to a MIQP (for both q ∈ {1, 2}). MILPs and MIQPs can be
solved by state-of-the-art MIP solvers (e.g., Gurobi and CPLEX) for small-to-moderate sized
instances. For other nonlinear convex loss functions such as logistic loss, two approaches are
possible: (i) nonlinear branch and bound (Belotti et al., 2013) or (ii) outer-approximation
in which a sequence of MILPs is solved until convergence (for example, see Bertsimas et al.
2017, 2020; Sato et al. 2016).9 We refer the reader to Lee and Leyﬀer (2011) for a review
of related approaches.

The local combinatorial search problem in (14) can be cast as a variant of (19) with

additional constraints; and is given by the following MIP:

G(θ) + λ0

p
(cid:88)

zi

min
β,z,θ

s.t. θ = βt −

i=1
(cid:88)

eiβt

i (1 − zi) +

i∈S
|βi| ≤ Mzi,
(cid:88)

zi ≥ |S| − m

i ∈ Sc

i∈S
(cid:88)

zi ≤ m

i∈Sc
zi ∈ {0, 1} ,

i ∈ [p]

(cid:88)

i∈Sc

eiβi

(20a)

(20b)

(20c)

(20d)

(20e)

(20f)

where S = Supp(βt). The binary variables zi, i ∈ [p] perform the role of selecting the
subsets S1 ⊂ S and S2 ⊂ Sc (described in (14)). Particularly, for i ∈ S, zi = 0 means
that i ∈ S1—i.e., variable i should be removed from the current support. Similarly, for
i ∈ Sc, zi = 1 means that i ∈ S2—i.e., variable j should be added to the support in (20).
Constraints (20d) and (20e) enforce |S1| ≤ m and |S2| ≤ m, respectively. The constraint
in (20b) forces the variable θ to be equal to βt − U S1βt + U S2β.

9. In this method, one obtains a convex piece-wise linear lower bound to a nonlinear convex problem. In
other words, this leads to a polyhedral outer approximation (aka outer approximation) to the epigraph
of the nonlinear convex function.

16

Learning Sparse Classifiers

Note that (20) has a smaller search space compared to the full formulation in (19)—there
are additional constraints in (20d) and (20e), and the number of free continuous variables is
|Sc|, as opposed to p in (19). This reduced search space usually leads to notable reductions
in the runtime compared to solving Problem (19).

3.2 Scaling up the MIP via the Integrality Generation Algorithm (IGA)

While state-of-the-art MIP solvers and outer-approximation based MILP approaches (Bert-
simas et al., 2017; Sato et al., 2016) lead to impressive improvements over earlier MIP-based
approaches, they often have long run times when solving high-dimensional classiﬁcation
problems with large p and small n (e.g., p = 50, 000 and n = 1000). Our proposed algo-
rithm, IGA, can solve (19) to global optimality, for high-dimensional instances that appear
to be beyond the capabilities of current MIP-based approaches. Loosely speaking, IGA
solves a sequence of MIP-based relaxations or subproblems of Problem (19); and exits upon
obtaining a global optimality certiﬁcate for (19). The aforementioned MIP subproblems
are obtained by relaxing a subset of the binary variables {zi}p
1 to lie within [0, 1], while the
remaining variables are retained as binary. Upon solving the relaxed problem and examin-
ing the integrality of the continuous zi’s, we create another (tighter) relaxation by allowing
more variables to be binary—we continue in this fashion till convergence. The algorithm is
formally described below.

The ﬁrst step in our algorithm is to obtain a good upper bound for Problem (19)—
this can be obtained by Algorithm 1 or 2. Let I denote the corresponding support of this
solution. We then consider a relaxation of Problem (19) by allowing the binary variables in
I c to be continuous:

min
β,z

s.t.

G(β) + λ0

p
(cid:88)

i=1

zi

|βi| ≤ Mzi,
zi ∈ [0, 1],
zi ∈ {0, 1} ,

i ∈ [p]

i ∈ I c
i ∈ I.

(21a)

(21b)

(21c)

(21d)

The relaxation (21) is a MIP (and thus nonconvex). The optimal objective of Problem (21)
is a lower bound to Problem (19).
In formulation (21), we place integrality constraints
on zi, i ∈ I—all remaining variables zi, i ∈ I c are continuous. Let βu, zu be the solution
obtained from the u-th iteration of the algorithm. Then, in iteration (u + 1), we set I ←
(cid:54)= 0, i ∈ I c} and solve Problem (21) (with warm-starting enabled). If at some
I ∪ {i | zu
i
iteration u, the vector zu is integral, then solution βu, zu must be optimal for Problem (19)
and the algorithm terminates. We note that along the iterations, we obtain tighter lower
bounds on the optimal objective of Problem (19). Depending on the available computational
budget, we can decide to terminate the algorithm at an early stage with a corresponding
lower bound. The algorithm is summarized below:

17

Dedieu, Hazimeh, and Mazumder

Algorithm 4: Integrality Generation Algorithm (IGA)

• Initialize I to the support of a solution obtained by Algorithm 1 or 2.

• For u = 1, 2, . . . perform the following steps till convergence:

1. Solve the relaxed MIP (21) to obtain a solution βu, zu.
2. Update I ← I ∪ {i | zu
i

(cid:54)= 0, i ∈ I c}.

it
As we demonstrate in Section 5, Algorithm 4 can lead to signiﬁcant speed-ups:
reduces the time to solve several sparse classiﬁcation instances from the order of hours to
seconds. This allows us to solve instances with p ≈ 50, 000 and n ≈ 1000 within reasonable
computation times. These instances are much larger than what has been reported in the
literature prior to this work (see for example, Sato et al. 2016; Bertsimas et al. 2017). A
main reason behind the success of Algorithm 4 is that Problem (21) leads to a solution z
with very few nonzero coordinates. Hence, a small number of indices are added to I in
step 2 of the algorithm. Since I is typically small, (21) can be often solved signiﬁcantly
faster than the full MIP in (19)—the branch-and-bound algorithm has a smaller number of
variables to branch on. Lemma 6 provides some intuition on why the solutions of (21) are
sparse.

Lemma 6 Problem (21) can be equivalently written as:

min
β,zI

s.t.

G(β) +

λ0
M

(cid:88)

i∈Ic

|βi| + λ0

(cid:88)

i∈I

zi

|βi| ≤ Mzi,
|βi| ≤ M,
zi ∈ {0, 1} ,

i ∈ I
i ∈ I c
i ∈ I.

(22a)

(22b)

(22c)

(22d)

We have observed empirically that in (22), the (cid:96)1-regularization term (cid:80)
i∈Ic |βi| encourages
sparse solutions, i.e., many of the components βi, i ∈ I c are set to zero. Consequently,
the corresponding zi’s in (21) are mostly zero at an optimal solution. The sparsity level
is controlled by the regularization parameter λ0/M—larger values will lead to more zi’s
being set to zero in (21). Thus, we expect Algorithm 4 to work well when λ0 is set to a
suﬃciently large value (to obtain suﬃciently sparse solutions). Note that Problem (22) is
diﬀerent from the Lasso as it involves additional integrality constraints.

Optimality Gap and Early Termination: Each iteration of Algorithm 4 provides
an improved lower bound to Problem (19). This lower bound, along with a good feasible
solution (e.g., obtained from Algorithm 1 or 2), leads to an optimality gap: given an upper
bound UB and a lower bound LB, the MIP optimality gap is deﬁned by (UB - LB)/LB. This
optimality gap serves as a certiﬁcate of global optimality. In particular, an early termination
of Algorithm 4 leads to a solution with an associated certiﬁcate of optimality.

Choice of I: The performance of Algorithm 4 depends on the initial set I. If the
initial I is close to the support of an optimal solution to (19), then our numerical experience
suggests that Algorithm 4 can terminate within a few iterations. Moreover, our experiments

18

Learning Sparse Classifiers

(see Section 5.2) suggest that Algorithm 2 can obtain an optimal or a near-optimal solution
to Problem (19) quickly, leading to a high-quality initialization for I.

In practice, if at iteration u of Algorithm 4, the set {i | zu
i

(cid:54)= 0} is large, then we add
only a small subset of it to I (in our implementation, we choose the 10 largest fractional
zi’s). Alternatively, while expanding I, we can use a larger cutoﬀ for the fractional zi’s,
i.e., we can take the indices {i | zu
i ≥ τ } for some value of τ ∈ (0, 1). This usually helps in
maintaining a small size in I, which allows for solving the MIP subproblem (21) relatively
quickly.

IGA for Local Combinatorial Search: While the discussion above was centered
around the full MIP (19)—the IGA framework (and in particular, Algorithm 4) extends, in
principle, to the local combinatorial search problem in (20) for m ≥ 2.

4. Statistical Properties: Error Bounds

We derive non-asymptotic upper bounds on the coeﬃcient estimation error for a family of (cid:96)0-
constrained classiﬁcation estimators (this includes the loss functions discussed in Section 1.2,
among others). For our analysis, we assume that: (xi, yi), i ∈ [n] are i.i.d. draws from an
unknown distribution P. Using the notation of Section 2, we consider a loss function f
and deﬁne its population risk L(β) = E (f ((cid:104)x, β(cid:105); y)), where the expectation is w.r.t. the
(population) distribution P. We let β∗ denote a minimizer of the risk, that is:

β∗ ∈ argmin

L(β) := E (f ((cid:104)x, β(cid:105); y)) .

(23)

β∈Rp

In the rest of this section, we let k = (cid:107)β∗(cid:107)0 and R = (cid:107)β∗(cid:107)2—i.e., the number of nonzeros
and the Euclidean norm (respectively) of β∗. We assume R ≥ 1. To estimate β∗, we
consider the following estimator:10

ˆβ ∈

argmin
β∈Rp
(cid:107)β(cid:107)0≤k, (cid:107)β(cid:107)2≤2R

1
n

n
(cid:88)

i=1

f ((cid:104)xi, β(cid:105); yi) ,

(24)

which minimizes the empirical loss with a constraint on the number of nonzeros in β and
a bound on the (cid:96)2-norm of β. The (cid:96)2-norm constraint in (24) makes β∗ feasible for Prob-
lem (24); and ensures that ˆβ lies in a bounded set (which is useful for the technical analysis).
Section 4.1 presents the assumptions we need for our analysis—see the works of Peng
et al. (2016), Ravikumar et al. (2010) and Belloni and Chernozhukov (2011) for related
assumptions in the context of (cid:96)1-based classiﬁcation and quantile regression procedures. In
Section 4.2, we establish a high probability upper bound on (cid:107)ˆβ − β∗(cid:107)2
2.

4.1 Assumptions

We ﬁrst present some assumptions for establishing the error bounds.

Loss Function. We list our basic assumptions on the loss function.

Assumption 2 The function t (cid:55)→ f (t; y) is non-negative, convex, and Lipschitz continuous
with constant L, that is |f (t1; y) − f (t2; y)| ≤ L|t1 − t2|, ∀t1, t2.

10. We drop the dependence of ˆβ on R, k for notational convenience.

19

Dedieu, Hazimeh, and Mazumder

We let ∂f (t; y) denote a subgradient of t (cid:55)→ f (t; y)—i.e., f (t2; y) − f (t1; y) ≥ ∂f (t1; y)(t2 −
t1), ∀t1, t2. Note that the hinge loss satisﬁes Assumption 2 with L = 1; and has a subgra-
dient given by ∂f (t; y) = 1 (1 − yt ≥ 0) y. The logistic loss function satisﬁes Assumption 2
with L = 1; and its subgradient coincides with the gradient.

Diﬀerentiability of the Population Risk. The following assumption is on the

uniqueness of β∗ and diﬀerentiability of the population risk L.

Assumption 3 Problem (23) has a unique minimizer. The population risk β (cid:55)→ L(β) is
twice continuously diﬀerentiable, with gradient ∇L(β) and Hessian ∇2L(β). In particular,
the following holds:

∇L(β) = E (∂f ((cid:104)x, β(cid:105); y) x) .

(25)

When f is the hinge loss, Koo et al. (2008) discuss conditions under which Assumption 3
holds. In particular, Assumption (A1) in Koo et al. (2008) requires that the conditional
density functions of x for the two classes are continuous and have ﬁnite second moments.
Under this assumption, the Hessian ∇2L(β) is well deﬁned and continuous in β. Under
Assumption (A4) (Koo et al., 2008), the Hessian is positive deﬁnite at an optimal solution,
therefore (23) has a unique solution. We refer the reader to Ravikumar et al. (2010); Van de
Geer (2008) for discussions pertaining to logistic regression.

Restricted Eigenvalue Conditions. Assumption 4 is a restricted eigenvalue condition
similar to that used in regression problems (Bickel et al., 2009; B¨uhlmann and Van De Geer,
2011). For an integer (cid:96) > 0, we assume that the quadratic forms associated with the Hessian
matrix ∇2L(β∗) and the covariance matrix n−1XT X are respectively lower-bounded and
upper-bounded on the set of 2(cid:96)-sparse vectors.

Assumption 4 Let (cid:96) > 0 be an integer. Assumption 4((cid:96)) is said to hold if there exists
constants κ((cid:96)), λ((cid:96)) > 0 such that almost surely the following holds:

κ((cid:96)) ≤

inf
z(cid:54)=0,(cid:107)z(cid:107)0≤2(cid:96)

(cid:26) zT ∇2L(β∗)z
(cid:107)z(cid:107)2
2

(cid:27)

and

λ((cid:96)) ≥

sup
z(cid:54)=0,(cid:107)z(cid:107)0≤2(cid:96)

(cid:26) (cid:107)Xz(cid:107)2
2
n(cid:107)z(cid:107)2
2

(cid:27)

.

In the rest of this section, we consider Assumption 4 with (cid:96) = k. Assumption (A4) in Peng
et al. (2016) for linear SVM is similar to our Assumption 4. For logistic regression, related
assumptions appear in the literature, e.g., Assumptions A1 and A2 in Ravikumar et al.
(2010) (in the form of a dependency and an incoherence condition on the population Fisher
information matrix).

Growth condition. As β∗ minimizes the population risk, we have ∇L(β∗) = 0. Under
the above regularity assumptions and when Assumption 4(k) is satisﬁed, the population risk
is lower-bounded by a quadratic function in a neighborhood of β∗. By continuity, we let
r(k) denote the maximal radius for which the following lower bound holds:

(cid:40)

r(k) = max

r > 0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

L(β∗ + z) ≥ L(β∗) +

1
4

κ(k)(cid:107)z(cid:107)2

2 ∀z s.t. (cid:107)z(cid:107)0 ≤ 2k, (cid:107)z(cid:107)2 ≤ r

(cid:41)

.

(26)

Below we make an assumption on the growth condition.

20

Learning Sparse Classifiers

Assumption 5 Let δ ∈ (0, 1/2). We say that Assumption 5(δ) holds if the parameters
n, p, k, R satisfy:

(cid:114)

24L
κ(k)

λ(k)
n

(k log (Rp/k) + log (1/δ)) < r(k),

and the following holds: (k/n) log(p/k) ≤ 1 and 7ne ≤ 3L(cid:112)λ(k)p log (p/k).

Assumption 5 is similar to the scaling conditions in Ravikumar et al. (2010)[Theorem 1] for
logistic regression. Belloni and Chernozhukov (2011) also makes use of a growth condition
for their analysis in (cid:96)1-sparse quantile regression problems. Note that although Assump-
tion 5 is not required to prove Theorem 7, we will need it to derive the error bound of
Theorem 8. We now proceed to derive an upper bound on coeﬃcient estimation error.

4.2 Main Result

We ﬁrst show (in Theorem 7) that the loss function f satisﬁes a form of restricted strong
convexity (Negahban et al., 2009) around β∗, a minimizer of (23).

Theorem 7 Let h = ˆβ − β∗, δ ∈ (0, 1/2), and τ = 6L
n (k log (Rp/k) + log (1/δ)).
If Assumptions 2, 3, 4(k) and 5(δ) are satisﬁed, then with probability at least 1 − δ, the
following holds:

(cid:113) λ(k)

n
(cid:88)

1
n

i=1

≥

1
4

f ((cid:104)xi, β∗ + h(cid:105); yi) −

f ((cid:104)xi, β∗(cid:105); yi)

n
(cid:88)

1
n

κ(k) (cid:8)(cid:107)h(cid:107)2

2 ∧ r(k)(cid:107)h(cid:107)2

(cid:9) − τ (cid:107)h(cid:107)2 ∨ τ 2.

i=1

(27)

Theorem 7 is used to derive the error bound presented in Theorem 8, which is the main
result in this section.

Theorem 8 Let δ ∈ (0, 1/2) and assume that Assumptions 2, 3, 4(k) and 5(δ) hold. Then
the estimator ˆβ deﬁned as a solution of Problem (24) satisﬁes with probability at least 1 − δ:

(cid:107)ˆβ − β∗(cid:107)2
2

(cid:46) L2λ(k)(cid:101)κ2

(cid:18) k log(Rp/k)
n

+

log(1/δ)
n

(cid:19)

(28)

where, (cid:101)κ = max{1/κ(k), 1}.
In (28), the symbol “(cid:46)” stands for “≤” up to a universal constant. The proof of Theorem 8 is
presented in Appendix A.7. The rate appearing in Theorem 8, of the order of k/n log(p/k),
is the best known rate for a (sparse) classiﬁer; and this coincides with the optimal scaling
in the case of regression. In comparison, Peng et al. (2016) and Ravikumar et al. (2010)
derived a bound scaling as k/n log(p) for (cid:96)1-regularized SVM (with hinge loss) and logistic
regression, respectively. Note that the bound in Theorem 8 holds for any suﬃciently small
δ > 0. Consequently, by integration, we obtain the following result in expectation.

Corollary 1 Suppose Assumptions 2, 3, 4(k) hold true and Assumption 5(δ) is true for δ
small enough, then:

E(cid:107)ˆβ − β∗(cid:107)2
2

(cid:46) L2λ(k)(cid:101)κ2 k log(Rp/k)

n

,

where (cid:101)κ is deﬁned in Theorem 8.

21

Dedieu, Hazimeh, and Mazumder

5. Experiments

In this section, we compare the statistical and computational performance of our proposed
algorithms versus the state of the art, on both synthetic and real data sets.

5.1 Experimental Setup

Data Generation. For the synthetic data sets, we generate a multivariate Gaussian data
matrix Xn×p ∼ MVN(0, Σ) and a sparse vector of coeﬃcients β† with k† nonzero entries,
such that β†
i = 1 for k† equi-spaced indices i ∈ [p]. Every coordinate yi of the outcome
vector y ∈ {−1, 1}n is then sampled independently from a Bernoulli distribution with
success probability: P (yi = 1|xi) = (1 + exp(−s(cid:104)β†, xi(cid:105)))−1, where xi denotes the i-th row
of X, and s is a parameter that controls the signal-to-noise ratio. Speciﬁcally, smaller values
of s increase the variance in the response y, and when s → ∞ the generated data becomes
linearly separable.

Algorithms and Tuning. We compare our proposal, as implemented in our package
L0Learn, with: (i) (cid:96)1-regularized logistic regression (glmnet package Friedman et al., 2010),
(ii) MCP-regularized logistic regression (ncvreg package Breheny and Huang, 2011), and
(iii) two packages for sparsity constrained minimization (based on hard thresholding): GraSP
(Bahmani et al., 2013) and NHTP (Zhou et al., 2021). For GraSP and NHTP, we use cardinality
constrained logistic regression with ridge regularization—that is, we optimize Problem (17)
with λ1 = 0. Tuning is done on a separate validation set under the ﬁxed design setting, i.e.,
we use the same features used for training but a new outcome vector y(cid:48) (independent of y).
The tuning parameters are selected so as to minimize the loss function on the validation
set (e.g., for regularized logistic regression, we minimize the unregularized negative log-
likelihood).

We use (cid:96)0-(cid:96)q as a shorthand to denote the penalty λ0(cid:107)β(cid:107)0 + λq(cid:107)β(cid:107)q

q for q ∈ {1, 2}. For
all penalties that involve 2 tuning parameters—i.e., (cid:96)0-(cid:96)q (for q ∈ {1, 2}), MCP, GraSP, and
NHTP, we sweep the parameters over a two-dimensional grid. For our penalties, we choose
100 λ0 values as described in Section 2.4. For GraSP and NHTP, we sweep the number of
nonzeros between 1 and 100. For (cid:96)0-(cid:96)1, and we choose a sequence of 10 λ1-values in [a, b],
where a corresponds to a zero solution and b = 10−4a. Similarly, for (cid:96)0-(cid:96)2, GraSP, and
NHTP, we choose 10 λ2 values between 10−4 and 100 for the experiment in Section 5.2; and
between 10−8 and 10−4 for that in Section 5.3. For MCP, the sequence of 100 λ values is
set to the default values selected by ncvreg, and we vary the second parameter γ over 10
values between 1.5 and 25. For the (cid:96)1-penalty, the grid of 100 λ values is set to the default
sequence chosen by glmnet.

Performance Measures. We use the following measures to evaluate the performance

of an estimator ˆβ:

• AUC: The area under the curve of the ROC plot.

• Recovery F1 Score: This is the F1 score for support recovery, i.e., it is the harmonic
mean of precision and recall: F1 Score = 2P R/(P + R), where P is the precision
given by |Supp(ˆβ) ∩ Supp(β†)|/|Supp(ˆβ)|, and R is the recall given by |Supp(ˆβ) ∩
Supp(β†)|/|Supp(β†)| . An F1 Score of 1 implies full support recovery; and a value of
zero implies that the supports of the true and estimated coeﬃcients have no overlap.

22

Learning Sparse Classifiers

High Correlation Setting (Σij = 0.9|i−j|)

Figure 1: Performance for varying n ∈ [100, 103] and Σij = 0.9|i−j|, p = 1000, k† = 25, s = 1. In
this high-correlation setting, our proposed algorithm (using local search) for the (cid:96)0-(cid:96)q
(for both q ∈ {1, 2}) penalized estimators—denoted as L0L2 (CD w. Local Search) and
L0L1 (CD w. Local Search) in the ﬁgure—seems to outperform state-of-the-art methods
(MCP, (cid:96)1, GraSP and NHTP) in terms of both variable selection and prediction. The
variable selection performance improvement (higher F1 score and smaller support size) is
more notable than AUC. Local search with CD shows beneﬁts compared to its variants
that do not employ local search, the latter denoted by L0L1 (CD) and L0L2 (CD) in the
ﬁgure.

• Support Size: The number of nonzeros in ˆβ.

• False Positives: This is equal to |Supp(ˆβ) \ Supp(β†)|.

5.2 Performance for Varying Sample Sizes

In this experiment, we ﬁx p and vary the number of observations n to study its eﬀect on
the performance of the diﬀerent algorithms and penalties. We hypothesize that when the
statistical setting is diﬃcult (e.g., features are highly correlated and/or n is small), good
optimization algorithms for (cid:96)0-regularized problems lead to estimators that can signiﬁcantly
outperform estimators obtained from convex regularizers and common (heuristic) algorithms
for nonconvex regularizers. To demonstrate our hypothesis, we perform experiments on the
following data sets:

• High Correlation: Σij = 0.9|i−j|, p = 1000, k† = 25, s = 1

• Medium Correlation: Σij = 0.5|i−j|, p = 1000, k† = 25, s = 1

For each of the above settings, we use the logistic loss function and consider 20 random
repetitions. We report the averaged results for the high correlation setting in Figure 1 and
for the medium correlation setting in Figure 2.

23

2004006008001000Number of Observations0.10.20.30.40.50.60.70.80.91.0Recovery F1 Score2004006008001000Number of Observations0.870.880.890.900.910.920.930.940.950.96AUC 2004006008001000Number of Observations020406080100120140Support SizeL1MCPL0L2 (CD)L0L1 (CD)L0L2 (CD w. Local Search)L0L1 (CD w. Local Search)NHTPGraSPDedieu, Hazimeh, and Mazumder

Medium Correlation Setting (Σij = 0.9|i−j|)

Figure 2: Performance for varying n and Σij = 0.5|i−j|, p = 1000, k† = 25, s = 1. In contrast
to Figure 1, this is a medium-correlation setting. Once again Algorithm 2 (CD
with local search) for the (cid:96)0-(cid:96)1 and (cid:96)0-(cid:96)2 penalties seems to perform quite well in
terms of variable selection and prediction performance, though its improvement
over the other methods appears to be less prominent compared to the high-
correlation setting in Figure 1. In this example, local search does not seem to
oﬀer much improvement over pure CD (i.e., Algorithm 1).

Figure 1 shows that in the high correlation setting, Algorithm 2 (with m = 1) for the
(cid:96)0-(cid:96)2 penalty and the (cid:96)0-(cid:96)1 penalty—denoted by L0L2 (CD w. Local Search) and L0L1 (CD
w. Local Search) (respectively) in the ﬁgure—achieves the best support recovery and AUC
across diﬀerent sample sizes n. We note that in this example, the best F1 score falls below
0.8 for n smaller than 600—suggesting that none of the algorithms can do full support
recovery. For larger values of n, the diﬀerence between Algorithm 2 and others become
more pronounced in terms of F1 score, suggesting an important edge in terms of variable
selection performance. Moreover, our algorithms select the smallest support size (i.e., the
most parsimonious model) for all n. In contrast, the (cid:96)1 penalty (i.e., L1) selects signiﬁcantly
larger support sizes (exceeding 100 in some cases) and suﬀers in terms of support recovery.
It is also worth mentioning that the other (cid:96)0-based algorithms, i.e., Algorithm 1, NHTP and
GraSP, outperform MCP and L1 in terms of support recovery (F1 score) and support sizes.
The performance of NHTP and GraSP appears to be similar in terms of support sizes and F1
score, though NHTP appears to be performing better compared to GraSP in terms of AUC.

In Figure 2, for the medium correlation setting, we see that Algorithms 1 and 2 per-
local search (with CD) performs similar to CD (without local search)—
form similarly:
Algorithm 1 can recover the correct support with around 500 observations. In this case, the
performance of MCP becomes similar to the (cid:96)0-(cid:96)q penalized estimators in terms of AUC,
though there are notable diﬀerences in terms of variable selection properties. Compared to
Figure 1, we observe that (cid:96)1 performs better in this example, but still appears to be gener-
ally outperformed by other algorithms in terms of all measures. We also observe that NHTP’s

24

2004006008001000Number of Observations0.20.40.60.81.0Recovery F1 Score2004006008001000Number of Observations0.880.900.920.940.96AUC 2004006008001000Number of Observations0255075100125150175200Support SizeL1MCPL0L2 (CD)L0L1 (CD)L0L2 (CD w. Local Search)L0L1 (CD w. Local Search)NHTPGraSPLearning Sparse Classifiers

Setting 1

Setting 2

Penalty/Loss

(cid:96)0-(cid:96)2/Logistic (Algorithm 1)
(cid:96)0-(cid:96)1/Logistic (Algorithm 1)
(cid:96)0-(cid:96)2/Logistic (Algorithm 2)
(cid:96)0-(cid:96)1/Logistic (Algorithm 2)
(cid:96)0-(cid:96)2/Sq. Hinge (Algorithm 1)
(cid:96)1/Logistic
MCP/Logistic
NHTP/Logistic
GraSP/Logistic

FP

0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0
2.8 ± 0.3
617.2 ± 8.3
0.0 ± 0.0
44.7 ± 5.6
50.5 ± 5.6

(cid:107)ˆβ(cid:107)0
30.0 ± 0.0
30.0 ± 0.0
30.0 ± 0.0
30.0 ± 0.0
32.8 ± 0.3
647.2 ± 8.3
30.0 ± 0.0
74.7 ± 5.6
80.5 ± 5.6

FP

21.6 ± 0.9
11.2 ± 0.7
11.5 ± 0.4
11.2 ± 0.4
23.9 ± 0.7
242.2 ± 4.8
80.1 ± 10.9
92.5 ± 0.6
45.3 ± 3.0

(cid:107)ˆβ(cid:107)0
26.2 ± 0.5
14.8 ± 0.3
14.6 ± 0.3
14.5 ± 0.3
27.0 ± 0.4
256.9 ± 5.3
91.5 ± 12.1
100.0 ± 0.0
54.4 ± 2.8

Table 2: Variable selection performance for diﬀerent penalty and loss combinations, under
high-dimensional settings. FP refers to the number of false positives. We consider
ten repetitions and report the averages (and standard errors) across the repetitions.

performance is comparable to the best methods in terms of F1 score and AUC. However, it
results in models that are more dense compared to L0L1 and L0L2 (both Algorithms 1 and
2). That being said, we will see in Section 5.5 that in terms of running time, Algorithm 1
has a signiﬁcant edge over NHTP. Furthermore, for larger problem instances (Section 5.3)
the performance of NHTP suﬀers in terms of variable selection properties compared to the
methods we propose in this paper. NHTP appears to perform better than GraSP across all
metrics in this setting.

Figures 1 and 2 suggest that when the correlations are high, local search with (cid:96)0 can
notably outperform competing methods (especially, in terms of variable selection). When
the correlations are medium to low, the diﬀerences across the diﬀerent nonconvex methods
become less pronounced. The performance of nonconvex penalized estimators (especially,
the (cid:96)0-based estimators) is generally better than (cid:96)1-based estimators in these examples.

5.3 Performance on Larger Instances

We now study the performance of the diﬀerent algorithms for some large values of p under
the following settings:

• Setting 1: Σ = I, n = 1000, p = 50, 000, s = 1000, and k† = 30

• Setting 2: Σij = 0.3 for i (cid:54)= j, Σii = 1, n = 1000, p = 105, s = 1000, and k† = 20.

Table 2 reports the results available from Algorithms 1 and 2 (m = 1) versus the other
methods, for diﬀerent loss functions. In Settings 1 and 2 above, all methods achieve an
AUC of 1 (approximately), and the main diﬀerences across the methods lie in variable
selection. For Setting 1, both Algorithms 1 and 2 applied to the logistic loss; and ncvreg
for the MCP penalty (with logistic loss) correctly recover the support.
In contrast, (cid:96)1
captures a large number of false positives, leading to large support sizes. Both NHTP and
GraSP have a considerable number of false positives and result in large support sizes.

25

Dedieu, Hazimeh, and Mazumder

In Setting 2, none of the algorithms correctly recovered the support. This setting is
more diﬃcult than Setting 1 as it has higher correlation and a larger p. In Setting 2, we
observe that CD with local search can have an edge over plain CD (see, logistic loss with
(cid:96)0-(cid:96)2 penalty). In terms of small FP and compactness of model size, CD with local search
appears to be the winner, with Algorithm 1 being quite close. Both Algorithms 1 and 2
appear to work better compared to earlier methods in this setting. We note that in Setting 2,
our proposed algorithms select supports with roughly 3 times fewer nonzeros than the MCP
penalized problem, and 10 times fewer nonzeros than those delivered by the (cid:96)1-penalized
problem. For both settings, our proposed methods oﬀer important improvements over NHTP
and GraSP as well.

5.4 Performance on Real Data Sets

We compare the performance of (cid:96)1 with (cid:96)0-(cid:96)q (q ∈ {1, 2}) regularization, using the logistic
loss function.11 We consider the following three binary classiﬁcation data sets taken from
the NIPS 2003 Feature Selection Challenge (Guyon et al., 2005):

• Arcene: This data set is used to identify cancer vs non-cancerous patterns in mass-
spectrometric data. The data matrix is dense with p = 10, 000 features. We used 140
observations for training and 40 observations for testing.

• Dorothea: This data set is used to distinguish active chemical compounds in a drug.
The data matrix is sparse with p = 100, 000 features. We used 805 observations for
training and 230 observations for testing.

• Dexter: The task here is to identify text documents discussing corporate acquisitions.
The data matrix is sparse with p = 20, 000 features. We used 420 observations for
training and 120 observations for testing.

We obtained regularization paths for (cid:96)1 using glmnet and for (cid:96)0-(cid:96)2 and (cid:96)0-(cid:96)1 using both
Algorithm 1 and Algorithm 2 (with m = 1). In Figure 3, we plot the support size versus
the test AUC for (cid:96)1 and Algorithm 2 (with m = 1) using (cid:96)0-(cid:96)2. To avoid overcrowded plots,
the results for our other algorithms and penalties are presented in Appendix B. For the
Arcene data set, (cid:96)0-(cid:96)2 with λ2 = 1 (i.e., the red curve) outperforms (cid:96)1 (the green curve)
for most of the support sizes, and it reaches a peak AUC of 0.9 whereas (cid:96)1 does not exceed
0.84. The other choices of λ2 also achieve a higher peak AUC than (cid:96)1 but do not uniformly
outperform it. A similar pattern occurs for the Dorothea data set, where (cid:96)0-(cid:96)2 with λ2 = 1
achieves a peak AUC of 0.9 at around 100 features, whereas (cid:96)1 needs around 160 features to
achieve the same peak AUC. For the Dexter data set, (cid:96)0-(cid:96)2 with λ2 = 10−2 (the blue curve)
achieves a peak AUC of around 0.98 using less than 10 features, whereas (cid:96)1 requires around
40 features to achieve a similar AUC. In this case, larger choices of λ2 do not lead to any
signiﬁcant gains in AUC. This phenomenon is probably due to a higher signal in this data

11. We tried MCP regularization using ncvreg, however it ran into convergence issues and did not terminate
in over an hour. We also tried NHTP, but it ran into convergence issues and took more than 2 hours to
solve for a single solution in the regularization path. Also, note that based on our experiment in Section
5.5, GraSP is slower than NHTP and cannot handle such high-dimensional problems. Therefore, we do
not include MCP, NHTP and GraSP in this experiment.

26

Learning Sparse Classifiers

Figure 3: Plots of the AUC versus the support size for the Arcene, Dorothea, and Dexter
data sets. The green curves correspond to logistic regression with (cid:96)1 regulariza-
tion. The other curves correspond to logistic regression with (cid:96)0-(cid:96)2 regularization
(using Algorithm 2 with m = 1) for diﬀerent values of λ2 (see legend).

set compared to the previous two. Overall, we conclude that for all the three data sets,
Algorithm 2 for (cid:96)0-(cid:96)2 can achieve higher AUC-values with (much) smaller support sizes.

5.5 Timings

In this section, we compare the running time of our algorithms versus several state-of-the-art
algorithms for sparse classiﬁcation.

5.5.1 Obtaining good solutions: Upper Bounds

We study the running time of ﬁtting sparse logistic regression models, using the following
packages: our package L0Learn, glmnet, ncvreg, and NHTP.12 In L0Learn, we consider both
Algorithm 1 (denoted by L0Learn 1) and Algorithm 2 with m = 1 (denoted by L0Learn
2). We generate synthetic data as described in Section 5.1, with n = 1000, s = 1, Σ = I,

12. We also considered GraSP, but we found it to be several orders of magnitude slower than the other
toolkits, e.g., it takes 3701 seconds at p = 105. All the other toolkits require less than 70 seconds at
p = 105. Therefore, we did not include GraSP in our timing experiments.

27

20406080100120140160Support Size0.700.730.750.780.800.830.850.880.90AUCArcene Dataset, n=140, p=10,000255075100125150175200Support Size0.700.730.750.780.800.830.850.880.90AUCDorothea Dataset, n=805, p=100,00020406080100120140Support Size0.800.830.850.880.900.930.950.981.00AUCDexter Dataset, n=420, p=20,000L0L2 - 2=101L0L2 - 2=100L0L2 - 2=101L0L2 - 2=102L1Dedieu, Hazimeh, and Mazumder

Figure 4: Runtimes to obtain good feasible solutions: Time (s) for obtaining a regularization
path (with 100 solutions) for diﬀerent values of p (as discussed in Section 5.5.1).
L0Learn 1 runs Algorithm 1 (CD), while L0Learn 2 runs Algorithm 2 (CD with
Local Search).

k† = 5, and we vary p. For all toolkits except NHTP,13 we set the convergence threshold to
10−6 and solve for a regularization path with 100 solutions. For L0Learn, we use a grid of
λ0-values varying between λmax
(the value which sets all coeﬃcients to zero) and 0.001λmax
.
For L0Learn and NHTP, we set λ2 = 10−7. For glmnet and ncvreg, we compute a path using
the default choices of tuning parameters. The experiments were carried out on a machine
with a 6-core Intel Core i7-8750H processor and 16GB of RAM, running macOS 10.15.7,
R 4.0.3 (with vecLib’s BLAS implementation), and MATLAB R2020b. The running times
are reported in Figure 4.

0

0

Figure 4 indicates that L0Learn 1 (Algorithm 1) and glmnet achieve the fastest runtimes
across the p range. For p > 40, 000, L0Learn 1 (Algorithm 1) runs slightly faster glmnet.
L0Learn 2 (Algorithm 2) is slower due to the local search, but it can obtain solutions in
reasonable times, e.g., less than a minute for 100 solutions at p = 105. It is important to
note that the toolkits are optimizing for diﬀerent objective functions, and the speed-ups
in L0Learn are partly due to the nature of (cid:96)0-regularization which selects fewer nonzeros
compared to those available from (cid:96)1 or MCP regularization.

5.5.2 Timings for MIP Algorithms: Global Optimality Certificates

We compare the running time to solve Problem (19) by Algorithm 4 (IGA) versus solv-
ing (19) directly i.e., without using IGA. We consider the hinge loss and take q = 2.
We use Gurobi’s MIP solver for our experiments. We set Σ = I and k† = 5 as de-
iβ† + (cid:15)i),
scribed in Section 5.1. We then generate (cid:15)i
where the signal-to-noise ratio SNR=Var(Xβ†)/σ2 = 10. We set n = 1000 and vary
p ∈ {104, 2 × 104, 3 × 104, 4 × 104, 5 × 104}. For a ﬁxed λ2 = 10, λ0 is chosen such that the
ﬁnal (optimal) solution has 5 nonzeros. The parameter M is set to 1.2(cid:107)˜β(cid:107)∞, where ˜β is

iid∼ N (0, σ2) and set yi = sign(x(cid:48)

13. NHTP does not have a parameter to control the tolerance for convergence.

28

Learning Sparse Classifiers

Tookit/p

10000

20000

30000

40000

50000

IGA (this paper)
Gurobi

35
4446

80
21817

117
-

169
-

297*
-

Table 3: Runtimes to certify global optimality: Time(s) for solving an (cid:96)0-regularized prob-
lem with a hinge loss function, to optimality. “-” denotes that the algorithm does
not terminate in a day. “*” indicates that the algorithm is terminated with a
0.05% optimality gap.

the warm start obtained from Algorithm 2. We note that in all cases, the optimal solution
recovers the support of the true solution β†.

For this experiment, we use a machine with a 12-core Intel Xeon E5 @ 2.7 GHz and
64GB of RAM, running OSX 10.13.6 and Gurobi v8.1. The timings are reported in Table 3.
The results indicate signiﬁcant speed-ups. For example, for p = 20, 000 our algorithm
terminates in 80 seconds whereas Gurobi takes 6 hours to solve (19) (to global optimality).
For larger values of p, Gurobi cannot terminate in a day, while our algorithm can terminate
to optimality in few minutes. It is worth noting that, empirically we observe IGA can attain
low running times when sparse solutions are desired (< 30 nonzeros in our experience). This
observation also applies to the state-of-the-art MIP solvers for sparse regression, e.g., see
Bertsimas et al. (2016, 2020); Hazimeh et al. (2020). For denser solutions, IGA can still be
useful for obtaining optimality gaps, but certifying optimality with a very small optimality
gap is expected to take longer.

6. Conclusion

We considered the problem of linear classiﬁcation regularized with a combination of the (cid:96)0
and (cid:96)q (for q ∈ {1, 2}) penalties. We developed both approximate and exact algorithms
for this problem. Our approximate algorithms are based on coordinate descent and lo-
cal combinatorial search. We established convergence guarantees for these algorithms and
demonstrated empirically that they can run in times comparable to the fast (cid:96)1-based solvers.
Our exact algorithm can solve to optimality high-dimensional instances with p ≈ 50, 000.
This scalability is achieved through the novel idea of integrality generation, which solves
a sequence of mixed integer programs with a small number of binary variables, until con-
verging to a globally optimal solution. We also established new estimation error bounds
for a class of (cid:96)0-regularized classiﬁcation problems and showed that these bounds compare
favorably with the best known bounds for (cid:96)1 regularization. We carried out experiments
on both synthetic and real datasets with p up to 105. The results demonstrate that our
(cid:96)0-based combinatorial algorithms can have a signiﬁcant statistical edge (in terms of vari-
able selection and prediction) compared to state-of-the-art methods for sparse classiﬁcation,
such as those based on (cid:96)1 regularization or simple greedy procedures for (cid:96)0 regularization.
There are multiple promising directions for future work. From a modeling perspective,
our work can be generalized to structured sparsity problems based on (cid:96)0 regularization
(Hazimeh et al., 2021). Recent work shows that specialized BnB solvers can be highly

29

Dedieu, Hazimeh, and Mazumder

scalable for (cid:96)0-regularized regression (Hazimeh et al., 2020). One promising direction is to
scale our proposed integrality generation algorithm further by developing specialized BnB
solvers for the corresponding MIP subproblems.

Acknowledgments

We would like to thank the anonymous reviewers for their comments that helped improve
the paper. The authors acknowledge research funding from the Oﬃce of Naval Research
[Grants ONR-N000141512342 and ONR-N000141812298 (Young Investigator Award)], and
the National Science Foundation [Grant NSF-IIS-1718258].

Appendix A. Proofs and Technical Details

A.1 Proof of Theorem 1

We ﬁrst present Lemma 9, which establishes that after updating a single coordinate, there
is a suﬃcient decrease in the objective function. The result of this Lemma will be used in
the proof of the theorem.

Lemma 9 Let {βl} be the sequence of iterates generated by Algorithm 1. Then, the follow-
ing holds for any l and i = 1 + (l mod p):

P (βl) − P (βl+1) ≥

ˆLi − Li
2

(βl+1

i − βl

i)2.

(29)

Proof Consider some l ≥ 0 and ﬁx i = 1 + (l mod p) (this corresponds to the coordinate
being updated via CD). Applying (6) to βl and βl+1 and adding ψ(βl+1) to both sides, we
have:

P (βl+1) ≤ g(βl) + (βl+1

i − βl

i)∇ig(βl) +

Li
2

(βl+1

i − βl

i)2 + ψ(βl+1).

By writing Li
2 (βl+1
regrouping the terms we get:

i − βl

i)2 as the sum of two terms: Li− ˆLi

2

(βl+1

i − βl

i)2 +

ˆLi
2 (βl+1

i − βl

i)2 and

P (βl+1) ≤ (cid:101)P ˆLi

(βl+1; βl) +

Li − ˆLi
2

(βl+1

i − βl

i)2,

(30)

where, in the above equation, we used the deﬁnition of (cid:101)P from (8). Recall from the
deﬁnition of Algorithm 1 that βl+1
p; βl) which implies
∈ argminβi (cid:101)P ˆLi
(βl; βl) = P (βl) (this follows directly from (8)).
(βl; βl) ≥ (cid:101)P ˆLi
that (cid:101)P ˆLi
(βl+1; βl). Plugging the latter inequality into (30) and
Therefore, we have P (βl) ≥ (cid:101)P ˆLi
rearranging the terms, we arrive to the result of the lemma.

(βl+1; βl). But (cid:101)P ˆLi

1, . . . , βi, , . . . , βl

(βl

i

Next, we present the proof of Theorem 1.
Proof

30

Learning Sparse Classifiers

• Part 1: We will show that Supp(βl) (cid:54)= Supp(βl+1) cannot happen inﬁnitely often.
Suppose for some l we have Supp(βl) (cid:54)= Supp(βl+1). Then, for i = 1 + (l mod p),
i (cid:54)= 0 = βl+1
one of the following two cases must hold: (I) βl
.
Let us consider case (I). Recall that the minimization step in Algorithm 1 is done
using the thresholding operator deﬁned in (10). Since βl+1
(cid:54)= 0, (10) implies that
|βl+1
i
to

. Plugging the latter bound into the result of Lemma 9, we arrive

i = 0 (cid:54)= βl+1

or (II) βl

(cid:113) 2λ0

ˆLi+2λ2

| ≥

i

i

i

P (βl) − P (βl+1) ≥

λ0,

(31)

ˆLi − Li
ˆLi + 2λ2

where the r.h.s. of the above is positive, due to the choice ˆLi > Li. The same argument
can be used for case (II) to arrive to (31). Thus, whenever the support changes,
(31) applies, and consequently the objective function decreases by a constant value.
Therefore, the support cannot change inﬁnitely often (as P (β) is bounded below).

• Part 2: First, we will show that under Assumption 1, the function βS (cid:55)→ G(βS) is
strongly convex. This holds for the ﬁrst case of Assumption 1, i.e., when λ2 > 0. Next,
we will consider the second case of Assumption 1 which states that P (β0) < λ0u and
g(β) is strongly convex when restricted to a support of size at most u. Since Algorithm
1 is a descent algorithm, we get P (βl) < λ0u for any l ≥ 0. This implies (cid:107)βl(cid:107)0 < u
for any l ≥ 0, and thus |S| < u. Consequently, the function βS (cid:55)→ g(βS) is strongly
convex (and so is βS (cid:55)→ G(βS)).
After support stabilization (from Part 1), the iterates generated by Algorithm 1 are
the same as those generated by CD for minimizing the strongly convex function G(βS),
which is guaranteed to converge (e.g., see Bertsekas 2016). Therefore, we conclude
that {βl} converges to a stationary solution β∗ satisfying β∗
G(βS) and
β∗

S ∈ argminβS

Sc = 0.

Finally, we will prove the two inequalities in (11). Fix some i ∈ S. Then, from the
deﬁnition of the thresholding operator in (10), the following holds for every l > N
(i.e., after support stabilization) at which coordinate i is updated:

(cid:115)

|βl

i| ≥

2λ0
ˆLi + 2λ2

.

(32)

Taking the limit as l → ∞ we arrive to the ﬁrst inequality in (11), which also implies
that Supp(β∗) = S. Now let us ﬁx some i ∈ Sc. For every l > N at which coordinate
i is updated, we have βl
i = 0 and the following condition holds by the deﬁnition of the
thresholding operator in (10):

ˆLi
ˆLi + 2λ2

(cid:16)(cid:12)
(cid:12)
(cid:12)

∇ig(βl)
ˆLi

(cid:12)
(cid:12)
(cid:12) −

λ1
ˆLi

(cid:17)

<

(cid:115)

2λ0
ˆLi + 2λ2

.

(33)

Taking the limit as l → ∞ in the above and simplifying, we arrive to the second
inequality in (11).

31

Dedieu, Hazimeh, and Mazumder

• Part 3: The support stabilization on S (from Part 1) and the fact that βl → β∗
where Supp(β∗) = S (from Part 2), directly imply that sign(βl
S) stabilizes in a ﬁnite
number of iterations. That is, there exists an integer N (cid:48) and a vector t ∈ {−1, 1}|S|
such that sign(βl
S) = t for all l ≥ N (cid:48). Therefore, for l ≥ N (cid:48), the iterates of Algorithm
1 are the same as those generated by running coordinate descent to minimize the
continuously diﬀerentiable function:

g(β) + λ0|S| + λ1(

(cid:88)

βi −

(cid:88)

i:ti>0

i:ti<0

βi) + λ2(cid:107)β(cid:107)2
2.

(34)

Beck and Tetruashvili (2013) established a linear rate of convergence for the case of
CD applied to a class of strongly convex and continuously diﬀerentiable functions,
which includes 34. Applying Beck and Tetruashvili (2013)’s result to (34) leads to
(12).

A.2 Proof of Theorem 3

Proof First, we will show that the algorithm terminates in a ﬁnite number of iterations.
Suppose the algorithm does not terminate after T iterations. Then, we have a sequence
{βt}T
0 of stationary solutions (since these are the outputs of Algorithm 1—see Theorem
1). Let St = Supp(βt). Each stationary solution βt
is a minimizer of the convex
function βSt (cid:55)→ G(βSt), and consequently P (βt) = min{P (β) | β ∈ Rp, Supp(β) = St}.
Moreover,
the deﬁnition of Step 2 and the descent property of cyclic CD imply
P (βT ) < P (βT −1) < · · · < P (β0). Therefore, the same support cannot appear more than
once in the sequence {βt}T
0 , and we conclude that the algorithm terminates in a ﬁnite
number of iterations with a solution β∗. Finally, we note that β∗ is the output of Algorithm
1 so it must satisfy the characterization given in Theorem 1. Moreover, the search in
Step 2 must fail at β∗ (otherwise, the algorithm does not terminate), and thus (15) holds.

A.3 Proof of Lemma 5
Proof First, we recall that δj = | ˆLβ∗
and ﬁx some j ∈ S. By Theorem 1, we have β∗
0 ∈ ∂G(β∗
2λ2|β∗

j |. Substituting this expression into δj, we get:

S). The zero subgradient condition directly implies that ∇jg(β∗

j − ∇jg(β∗)| for any j ∈ [p]. Let S = Supp(β∗)
G(βS), which is equivalent to
j )−

S) = −λ1sign(β∗

S ∈ argminβS

δj = |( ˆL + 2λ2)β∗
= ( ˆL + 2λ2)|β∗

j + λ1sign(β∗
j | + λ1

j )|

(cid:113)

2λ0( ˆL + 2λ2) + λ1,

≥

(35)

32

Learning Sparse Classifiers

where (35) follows from inequality |β∗
ˆL ≥ ˆLj. Now ﬁx some i /∈ S. Using Theorem 1 and ˆL ≥ ˆLi:

ˆLj +2λ2

j | ≥

(cid:113) 2λ0

(due to Theorem 1) and the fact that

(cid:113)

(cid:113)

δi = |∇ig(β∗)| ≤

2λ0( ˆLi + 2λ2) + λ1 ≤
(36)
Inequalities (35) and (36) imply that δj ≥ δi for any j ∈ S and i /∈ S. Since (cid:107)β∗(cid:107)0 = k,
we have δ(k) ≥ δi for any i /∈ S, which combined with the fact that β∗
G(βS)
(from Theorem 1) implies that β∗ satisﬁes the ﬁxed point conditions for IHT stated in
Theorem 4.

2λ0( ˆL + 2λ2) + λ1.

S ∈ argminβS

A.4 Choice of J: sorted gradients

Let β1
j denote the solution obtained after the ﬁrst application of the thresholding operator
to minimize the function in (16). Note that we are interested in coordinates with nonzero
β1
j (because in step 1 of Algorithm 3, we check whether βj should be zero). Coordinates
with nonzero β1
i )| − λ1 > 0 (this follows from (10) with λ0 = 0).
Using (6), it can be readily seen that we have the following lower bound on the improvement
in the objective if |∇jg(βt − eiβt

j must satisfy |∇jg(βt − eiβt

i )| − λ1 > 0:

P (βt − eiβt

i ) − P (βt − eiβt

i + ejβ1

j ) ≥

1
2(Lj + 2λ2)

(|∇jg(βt − eiβt

i )| − λ1)2 − λ0.

(37)

The choices of j ∈ Sc with larger |∇jg(βt − eiβt
in inequality (37)
and thus are expected to have lower objectives. Therefore, instead of searching across all
values of j ∈ Sc in Step 2 of Algorithm 2, we restrict j ∈ J.

i )| have a larger r.h.s.

A.5 Proof of Lemma 6

Proof Problem (21) can be rewritten as

(cid:104)

min
zIc

min
β,zI

G(β) + λ0

(cid:105)

zi

p
(cid:88)

i=1

|βi| ≤ Mzi,
zi ∈ [0, 1],
zi ∈ {0, 1} ,

i ∈ [p]

i /∈ I

i ∈ I.

Solving the inner minimization problem leads to zi = |βi|/M for every i ∈ I c. Plugging
the latter solution into the objective function, we arrive to the result of the lemma.

A.6 Proof of Theorem 7

A.6.1 A useful proposition

Proposition 1 (stated below) is an essential step in the proof of Theorem 7. This allows us
to control the supremum of a random variable (of interest) over a bounded set of 2k sparse
vectors.

33

Dedieu, Hazimeh, and Mazumder

Proposition 1 Let δ ∈ (0, 1/2), τ = 6L

(cid:113) λ(k)

n (k log (Rp/k) + log (1/δ)) and deﬁne

∆(z) =

1
n

n
(cid:88)

i=1

f ((cid:104)xi, β∗ + z(cid:105); yi) −

1
n

n
(cid:88)

i=1

f ((cid:104)xi, β∗(cid:105); yi) ,

∀z.

(38)

If Assumptions 2, 4(k) and 5(δ) hold then:



P




sup
z∈Rp
(cid:107)z(cid:107)0≤2k, (cid:107)z(cid:107)2≤3R

(cid:8)|∆(z) − E (∆(z))| − τ (cid:107)z(cid:107)2 ∨ τ 2(cid:9) ≥ 0




 ≤ δ.

Proof We divide the proof
First, we upper-bound the quantity
|∆(z) − E (∆(z))| for any 2k sparse vector with Hoeﬀding inequality. Second, we extend the
result to the maximum over an (cid:15)-net. We ﬁnally control the maximum over the compact
set and derive our proposition.

into 3 steps.

Step 1: We ﬁx z ∈ Rp such that (cid:107)z(cid:107)0 ≤ 2k and introduce the random variables Zi, ∀i

as follows

Zi = f ((cid:104)xi, β∗ + z(cid:105); yi) − f ((cid:104)xi, β∗(cid:105); yi) .

Assumption 2 guarantees that f (.; y) is Lipschitz with constant L, which leads to:

Note that ∆(z) = 1
n

n
(cid:80)
i=1

|Zi| ≤ L |(cid:104)xi, z(cid:105)| .

Zi. We introduce a small quantity η > 0 later explicited in the

proof. Using Hoeﬀding’s inequality and Assumption 4(k) it holds: ∀t > 0,

(cid:18)

(cid:12)
(cid:12)
|∆(z) − E (∆(z))| ≥ t ((cid:107)z(cid:107)2 ∨ η)
X
(cid:12)
(cid:12)

(cid:19)

P

(cid:32)

≤ 2 exp

−

(cid:32)

= 2 exp

−

(cid:32)

≤ 2 exp

−

(cid:18)

≤ 2 exp

−

2n2t2 ((cid:107)z(cid:107)2 ∨ η)2
(cid:80)n
i=1 L2(cid:104)xi, z(cid:105)2
2 ∨ η2(cid:1)

2n2t2 (cid:0)(cid:107)z(cid:107)2

(cid:33)

(cid:33)

L2(cid:107)Xz(cid:107)2
2

2nt2 (cid:0)(cid:107)z(cid:107)2

2 ∨ η2(cid:1)

(cid:33)

(39)

L2λ(k)(cid:107)z(cid:107)2
2
(cid:19)

2nt2
L2λ(k)

.

Note that in the above display, the r.h.s. bound does not depend upon X, the conditioning
event in the l.h.s. of display (39).

Step 2: We consider an (cid:15)-net argument to extend the result to any 2k sparse vector
satisfying (cid:107)z(cid:107)2 ≤ 3R. We recall that an (cid:15)-net of a set I is a subset N of I such that each
element of I is at a distance at most (cid:15) of N .

Lemma 1.18 in Rigollet (2015) proves that for any value (cid:15) ∈ (0, 1), the ball
. Consequently, we can

(cid:8)z ∈ Rd : (cid:107)z(cid:107)2 ≤ 3R(cid:9) has an (cid:15)-net of cardinality |N | ≤ (cid:0) 6R+1

(cid:1)d

(cid:15)

34

Learning Sparse Classifiers

ﬁx an (cid:15)-net Nk,R of the set Ik,R = {z ∈ Rp : (cid:107)z(cid:107)0 = 2k ; (cid:107)z(cid:107)2 ≤ 3R} with cardinality
(cid:0) p
2k

. This along with equation (39) leads to: ∀t > 0,

(cid:1) (cid:0) 6R+1

(cid:1)2k

(cid:15)

(cid:32)

P

sup
z∈Nk,R

(cid:0) |∆(z) − E (∆(z))| − t ((cid:107)z(cid:107)2 ∨ η) (cid:1) ≥ 0

(cid:33)

≤

(cid:18) p
2k

(cid:19) (cid:18) 6R + 1

(cid:19)2k

(cid:15)

(cid:18)

2 exp

−

2nt2
L2λ(k)

(cid:19)

.

(40)

Step 3: We ﬁnally extend the result to any vector in Ik,R. This is done by expressing
the supremum of the random variable |∆(z) − E (∆(z))| over the entire set Ik,R with respect
to its supremum over the (cid:15)-net Nk,R.

For z ∈ Ik,R, there exists z0 ∈ Nk,R such that (cid:107)z − z0(cid:107)2 ≤ (cid:15). With Assumption 2 and

Cauchy-Schwartz inequality, we obtain:

|∆(z) − ∆(z0)| ≤

1
n

n
(cid:88)

i=1

L |(cid:104)xi, z − z0(cid:105)| ≤

1
√
n

L(cid:107)X(z − z0)(cid:107)2 ≤ L(cid:112)λ(k)(cid:15).

(41)

For a ﬁxed value of t, we deﬁne the operator

ft(z) = |∆(z) − E (∆(z))| − t ((cid:107)z(cid:107)2 ∨ η) , ∀z.

Using the (reverse) triangle inequality, it holds that:

|∆(z) − ∆(z0)| ≥ |∆(z) − E (∆(z))| − |∆(z0) − E (∆(z0))| − |E (∆(z)) − E (∆(z0))| . (42)

In addition, note that:

((cid:107)z − z0(cid:107)2 ∨ η) + ((cid:107)z(cid:107)2 ∨ η) ≥ (cid:107)z0(cid:107)2 ∨ η.

(43)

Using (42) and (43) in ft(z) we have:

ft(z0) ≥ ft(z) − |∆(z) − ∆(z0)| − |E (∆(z) − ∆(z0))| − t ((cid:107)z − z0(cid:107)2 ∨ η) .

(44)

Now using (41) and Jensen’s inequality, we have:

|∆(z) − ∆(z0)| ≤ L(cid:112)λ(k)(cid:15)
|E (∆(z) − ∆(z0))| ≤ L(cid:112)λ(k)(cid:15).

Applying (45) and (cid:107)z − z0(cid:107)2 ≤ (cid:15) to the right hand side of (44), we have:

ft(z0) ≥ ft(z) − 2L(cid:112)λ(k)(cid:15) − t((cid:15) ∨ η).

(45)

(46)

Suppose we choose

(cid:114)

η = 4L

(cid:15) =

η2
4L(cid:112)λ(k)

k log (p/k)

λ(k)
n
= (cid:112)λ(k)

4Lk log (p/k)
n

35

Dedieu, Hazimeh, and Mazumder

This implies that:

t ≥ η/4.

(cid:15) ≤ η

(using k/n log(p/k) ≤ 1 by Assumption 5),

L(cid:112)λ(k)(cid:15) ≤ tη

(using t ≥ η/4 =⇒ tη ≥

η2
4

= L(cid:112)λ(k)(cid:15)).

Using the above, we obtain a lower bound to the r.h.s. of (46). This leads to the following
chain of inequalities:

ft(z) − 2L(cid:112)λ(k)(cid:15) − t((cid:15) ∨ η) ≥ ft(z) − 3tη

= |∆(z) − E (∆(z))| − t ((cid:107)z(cid:107)2 ∨ η) − 3tη
≥ |∆(z) − E (∆(z))| − 4t ((cid:107)z(cid:107)2 ∨ η)
= f4t(z).

Consequently, Equations (46) and (47) lead to:

∀t ≥ η/4, ∀z ∈ Ik,R, ∃z0 ∈ Nk,R : f4t(z) ≤ ft(z0),

which can be equivalently written as:

sup
z∈Ik,R

ft(z) ≤ sup

y∈Nk,R

ft/4(y), ∀t ≥ η.

(47)

(48)

Lemma 2.7 in Rigollet (2015) gives the relation (cid:0) p
2k
along with the union-bound (40), it holds that: ∀t ≥ η,

(cid:1) ≤ (cid:0) pe
2k

(cid:1)2k. Using equation (48)

(cid:32)

P

sup
z∈Ik,R

ft(z) ≥ 0

≤ P

(cid:33)

(cid:32)

(cid:33)

ft/4(z) ≥ 0

sup
z∈Nk,R
(cid:19) (cid:18) 6R + 1

(cid:15)
(cid:17)2k (cid:18) 6R + 1

(cid:19)2k

(cid:18)

2 exp

(cid:19)2k

2 exp

−

(cid:18)

(cid:15)
(cid:19)2k

7R
(cid:15)

(cid:18)

exp

−

2nt2
16L2λ(k)

.

(cid:19)

2nt2
16L2λ(k)
2nt2
16L2λ(k)
(cid:19)

−

≤

≤

(cid:18) p
2k

(cid:16) pe
2k
(cid:18) pe
2k

≤ 2

(49)

(cid:19)

By Assumption 5 we have 7en ≤ 3L(cid:112)λ(k)p log (p/k); and using the deﬁnition of (cid:15) (above),
it follows that:

p
k
where in the last inequality we used the assumption R ≥ 1. Using this in (49) we have:

Rp
k

7e
2(cid:15)

≤

≤

,

(cid:32)

P

sup
z∈Ik,R

(cid:33)

ft(z) ≥ 0

≤ 2

(cid:18) Rp
k

(cid:19)4k

(cid:18)

exp

−

(cid:19)

2nt2
16L2λ(k)

, ∀t ≥ η.

(50)

36

Learning Sparse Classifiers

We want the right-hand side of Equation (50) to be smaller than δ. To this end, we need to
select t2 ≥ 16L2λ(k)
and t ≥ η. A possible choice for t that
we use is:

+ log(2) + log (cid:0) 1
δ

(cid:104)
4k log

(cid:16) Rp
k

(cid:1)(cid:105)

2n

(cid:17)

τ = 6L

(cid:114)

λ(k)
n

(k log (Rp/k) + log (1/δ)).

We conclude that with probability at least 1 − δ:

sup
z∈Ik,R

fτ (z) ≤ 0.

Note that

fτ (z) = |∆(z) − E (∆(z))| − τ ((cid:107)z(cid:107)2 ∨ η) ≥ |∆(z) − E (∆(z))| − τ (cid:107)z(cid:107)2 ∨ τ 2.

It then holds with probability at least 1 − δ that:

sup
z∈Rp
(cid:107)z(cid:107)0≤2k, (cid:107)z(cid:107)2≤3R

(cid:8)|∆(z) − E (∆(z))| − τ (cid:107)z(cid:107)2 ∨ τ 2(cid:9) ≤ 0,

which concludes the proof.

A.6.2 Proof of Theorem 7

Proof The 2k sparse vector h = ˆβ − β∗ satisﬁes: (cid:107)h(cid:107)2 ≤ (cid:107)ˆβ(cid:107)2 + (cid:107)β∗(cid:107)2 ≤ 3R. Thus
applying Proposition 1 to ∆(h) (see the deﬁnition in Equation 38) it holds with probability
at least 1 − δ that:

∆(h) ≥ E (∆(h)) − τ (cid:107)h(cid:107)2 ∨ τ 2
(cid:32) n
(cid:88)

=

1
n

E

i=1

f ((cid:104)xi, β∗ + h(cid:105); yi) −

(cid:33)

f ((cid:104)xi, β∗(cid:105); yi)

− τ (cid:107)h(cid:107)2 ∨ τ 2

n
(cid:88)

i=1

(51)

= E [f ((cid:104)xi, β∗ + h(cid:105); yi) − f ((cid:104)xi, β∗(cid:105); yi)] − τ (cid:107)h(cid:107)2 ∨ τ 2
= L(β∗ + h) − L(β∗) − τ (cid:107)h(cid:107)2 ∨ τ 2.

To control the diﬀerence L(β∗ + h) − L(β∗) we consider the following two cases.
Case 1 ((cid:107)h(cid:107)2 ≤ r(k)): If (cid:107)h(cid:107)2 ≤ r(k) then by deﬁnition of r(k) in (26) it holds that

L(β∗ + h) − L(β∗) ≥

1
4

κ(k)(cid:107)h(cid:107)2
2.

(52)

37

Dedieu, Hazimeh, and Mazumder

Case 2: We consider the case when (cid:107)h(cid:107)2 > r(k). Since β (cid:55)→ L(β) is convex, the one-
dimensional function t → L (β∗ + tz) is convex and it holds:

L(β∗ + h) − L(β∗) ≥

≥

≥

=

(cid:107)h(cid:107)2
r(k)
(cid:107)h(cid:107)2
r(k)

(cid:107)h(cid:107)2
r(k)
1
4

κ(k)r(k)(cid:107)h(cid:107)2.

(cid:26)

(cid:18)

L

β∗ +

(cid:19)

h

r(k)
(cid:107)h(cid:107)2

(cid:27)

− L(β∗)

{L(β∗ + z) − L(β∗)}

(53)

inf
z: (cid:107)z(cid:107)0≤2k

(cid:107)z(cid:107)2=r(k)

κ(k)r(k)2

1
4

In (53), the ﬁrst inequality follows by observing that the one-dimensional function x (cid:55)→
(g(a + x) − g(a))/x deﬁned on x > 0 is increasing for any one-dimensional convex function
x (cid:55)→ g(x). The last inequality in display (53) follows from the deﬁnition of r(k) as in (26).
Combining Equations (51), (52) and (53), we obtain the following restricted strong

convexity property (which holds with probability at least 1 − δ):

∆(h) ≥

1
4

κ(k) (cid:8)(cid:107)h(cid:107)2

2 ∧ r(k)(cid:107)h(cid:107)2

(cid:9) − τ (cid:107)h(cid:107)2 ∨ τ 2.

A.7 Proof of Theorem 8
Proof Using the observation that ˆβ is a minimizer of Problem (24); and the representation
ˆβ = β∗ + h, it holds that:

1
n

n
(cid:88)

i=1

f ((cid:104)xi, β∗ + h(cid:105); yi) ≤

1
n

n
(cid:88)

i=1

f ((cid:104)xi, β∗(cid:105); yi) .

This relation is equivalent to saying that ∆(h) ≤ 0. Consequently, by combining this
relation with (27) (see Theorem 7), it holds with probability at least 1 − δ:

κ(k) (cid:8)(cid:107)h(cid:107)2

2 ∧ r(k)(cid:107)h(cid:107)2

(cid:9) ≤ τ (cid:107)h(cid:107)2 ∨ τ 2.

1
4

(54)

We now consider two cases.
Case 1: Let (cid:107)h(cid:107)2 ≤ τ . Then we have that

(cid:114)

(cid:107)h(cid:107)2 ≤ τ = 6L

λ(k)
n

(k log (Rp/k) + log (1/δ)).

Case 2: We consider the case where (cid:107)h(cid:107)2 > τ . With probability 1 − δ it holds (from
Inequality 54) that:

1
4

κ(k) (cid:8)(cid:107)h(cid:107)2

2 ∧ r(k)(cid:107)h(cid:107)2

(cid:9) ≤ τ (cid:107)h(cid:107)2,

38

Learning Sparse Classifiers

which can be simpliﬁed to:

(cid:107)h(cid:107)2 ∧ r(k) ≤

4τ
κ(k)

=

24L
κ(k)

(cid:114)

λ(k)
n

(k log (Rp/k) + log (1/δ)).

(55)

Note that by Assumption 5(δ), the r.h.s. of the above is smaller than r(k). The l.h.s.
of (55) is the minimum of two terms (cid:107)h(cid:107)2 and r(k); and since this is lower than r(k), we
conclude that

(cid:114)

(cid:107)h(cid:107)2 ≤

24L
κ(k)

λ(k)
n

(k log (Rp/k) + log (1/δ)).

Combining the results from Cases 1 and 2, we conclude that with probability at least

1 − δ, the following holds:

(cid:107)h(cid:107)2 ≤ CL max

1,

(cid:26)

(cid:27) (cid:114)

1
κ(k)

λ(k)
n

(k log (Rp/k) + log (1/δ)),

where C is an universal constant. This concludes the proof of the theorem.

A.8 Proof of Corollary 1:

Proof Let us deﬁne the random variable:

W =

1
(cid:101)κ2λ(k)
Since the diﬀerence ˆβ−β∗ is bounded, then W is upper-bounded by a constant. In addition,
because Assumption 5 is satisﬁed for δ > 0 small enough, Theorem 8 leads to the existence
of a constant C such that

(cid:107)ˆβ − β∗(cid:107)2
2.

L2

(cid:18)

P

W ≤

C
n

(k log(Rp/k) + log(1/δ))

≥ 1 − δ, ∀δ ∈ (0, 1).

(cid:19)

This relation can be equivalently expressed as

(cid:18)

P

W/C ≥

k log(p/k)
n

(cid:19)

+ t

≤ e−nt, ∀t ≥ 0.

Let us deﬁne H = (k/n) log(p/k). By integration it holds:

E(W ) =

≤

≤

(cid:90) +∞

0
(cid:90) +∞

0
(cid:90) +∞

0

CP (|W |/C ≥ t) dt

CP (|W |/C ≥ t + H) dt + CH

(56)

Ce−ntdt + CH =

C
n

+ CH ≤ 2CH

39

Dedieu, Hazimeh, and Mazumder

L0L2 (CD) vs. L1

Figure 5: Plots of AUC versus corresponding support sizes for the Arcene, Dorothea, and
Dexter data sets. The green curves correspond to logistic regression with (cid:96)1
regularization. The other curves correspond to logistic regression with (cid:96)0-(cid:96)2 reg-
ularization using Algorithm 1 for diﬀerent values of λ2 (see legend).

We ﬁnally conclude:

E(cid:107)ˆβ − β∗(cid:107)2
2

(cid:46) L2λ(k)(cid:101)κ2 k log(p/k)

n

.

Appendix B. Additional Experiments

Here we present additional experimental results complementing the real data set experi-
ments presented in Section 5.4. We compare the performance of our proposed algorithm
with (cid:96)1-regularization. In particular, Figure 5 considers the (cid:96)0-(cid:96)2 penalty with CD (Algo-
rithm 1); Figure 6 considers the (cid:96)0-(cid:96)1 penalty with local search (Algorithm 2 with m = 1);
and Figure 7 considers the (cid:96)0-(cid:96)1 penalty with CD (Algorithm 1).

40

20406080100120140160Support Size0.700.730.750.780.800.830.850.880.90AUCArcene Dataset, n=140, p=10,000255075100125150175200Support Size0.700.730.750.780.800.830.850.880.90AUCDorothea Dataset, n=805, p=100,00020406080100120140Support Size0.800.830.850.880.900.930.950.981.00AUCDexter Dataset, n=420, p=20,000L0L2 - 2=101L0L2 - 2=100L0L2 - 2=101L0L2 - 2=102L1Learning Sparse Classifiers

L0L1 (CD w. Local Search) vs. L1

Figure 6: Plots of the AUC versus the support size for the Arcene, Dorothea, and Dexter
data sets. The green curves correspond to logistic regression with (cid:96)1 regulariza-
tion. The other curves correspond to logistic regression with (cid:96)0-(cid:96)1 regularization
using Algorithm 2 (with m = 1) for diﬀerent values of λ2 (see legend).

41

20406080100120140160Support Size0.600.650.700.750.800.850.90AUCArcene Dataset, n=140, p=10,000255075100125150175200Support Size0.700.730.750.780.800.830.850.880.90AUCDorothea Dataset, n=805, p=100,00020406080100120140Support Size0.800.830.850.880.900.930.950.981.00AUCDexter Dataset, n=420, p=20,000L0L1 - 1=105*1L0L1 - 1=106*1L0L1 - 1=107*1L0L1 - 1=108*1L1Dedieu, Hazimeh, and Mazumder

L0L1 (CD) vs. L1

Figure 7: Plots of the AUC versus the support size for the Arcene, Dorothea, and Dexter
data sets. The green curves correspond to logistic regression with (cid:96)1 regulariza-
tion. The other curves correspond to logistic regression with (cid:96)0-(cid:96)1 regularization
using Algorithm 1 for diﬀerent values of λ2 (see legend).

42

20406080100120140160Support Size0.600.650.700.750.800.850.90AUCArcene Dataset, n=140, p=10,000255075100125150175200Support Size0.700.730.750.780.800.830.850.880.90AUCDorothea Dataset, n=805, p=100,00020406080100120140Support Size0.800.830.850.880.900.930.950.981.00AUCDexter Dataset, n=420, p=20,000L0L1 - 1=105*1L0L1 - 1=106*1L0L1 - 1=107*1L0L1 - 1=108*1L1Learning Sparse Classifiers

References

Sohail Bahmani, Bhiksha Raj, and Petros T Boufounos. Greedy sparsity-constrained opti-

mization. Journal of Machine Learning Research, 14(Mar):807–841, 2013.

Amir Beck and Yonina C. Eldar. Sparsity constrained nonlinear optimization: Optimality
conditions and algorithms. SIAM Journal on Optimization, 23(3):1480–1509, 2013. doi:
10.1137/120869778. URL https://doi.org/10.1137/120869778.

Amir Beck and Luba Tetruashvili. On the convergence of block coordinate descent
type methods. SIAM Journal on Optimization, 23(4):2037–2060, 2013. doi: 10.1137/
120887679. URL http://dx.doi.org/10.1137/120887679.

Alexandre Belloni and Victor Chernozhukov.

l1-penalized quantile regression in high-

dimensional sparse models. The Annals of Statistics, 39(1):82–130, 2011.

Pietro Belotti, Christian Kirches, Sven Leyﬀer, Jeﬀ Linderoth, James Luedtke, and
Ashutosh Mahajan. Mixed-integer nonlinear optimization. Acta Numerica, 22:1–131,
2013.

D.P. Bertsekas. Nonlinear Programming. Athena scientiﬁc optimization and computation
series. Athena Scientiﬁc, 2016. ISBN 9781886529052. URL https://books.google.com/
books?id=TwOujgEACAAJ.

Dimitris Bertsimas and Angela King. Logistic regression: From art to science. Statistical

Science, 32(3):367–384, 2017.

Dimitris Bertsimas, Angela King, and Rahul Mazumder. Best subset selection via a modern

optimization lens. The Annals of Statistics, 44(2):813–852, 2016.

Dimitris Bertsimas, Jean Pauphilet, and Bart Van Parys. Sparse classiﬁcation and phase
transitions: A discrete optimization perspective. arXiv preprint arXiv:1710.01352, 2017.

Dimitris Bertsimas, Bart Van Parys, et al. Sparse high-dimensional regression: Exact
scalable algorithms and phase transitions. The Annals of Statistics, 48(1):300–323, 2020.

Peter J Bickel, Ya’acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso

and dantzig selector. The Annals of Statistics, pages 1705–1732, 2009.

Thomas Blumensath and Mike E Davies. Iterative hard thresholding for compressed sensing.

Applied and computational harmonic analysis, 27(3):265–274, 2009.

Patrick Breheny and Jian Huang. Coordinate descent algorithms for nonconvex penal-
ized regression, with applications to biological feature selection. The annals of applied
statistics, 5(1):232, 2011.

Peter B¨uhlmann and Sara Van De Geer. Statistics for High-dimensional Data: Methods,

Theory and Applications. Springer Science & Business Media, 2011.

Florentina Bunea, Alexandre B Tsybakov, and Marten H Wegkamp. Aggregation for gaus-

sian regression. The Annals of Statistics, 35(4):1674–1697, 2007.

43

Dedieu, Hazimeh, and Mazumder

Emmanuel Candes and Mark A Davenport. How well can we estimate a sparse vector?

Applied and Computational Harmonic Analysis, 34(2):317–323, 2013.

Emmanuel J Candes and Terence Tao. The Dantzig selector: Statistical estimation when p

is much larger than n. The Annals of Statistics, pages 2313–2351, 2007.

Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learn-

ing. Springer series in statistics New York, NY, USA:, 2001.

Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Regularization paths for gener-
alized linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22,
2010. URL http://www.jstatsoft.org/v33/i01/.

Pinghua Gong, Changshui Zhang, Zhaosong Lu, Jianhua Huang, and Jieping Ye. A general
iterative shrinkage and thresholding algorithm for non-convex regularized optimization
problems. In International Conference on Machine Learning, pages 37–45, 2013.

Eitan Greenshtein. Best subset selection, persistence in high-dimensional statistical learning
and optimization under l1 constraint. The Annals of Statistics, 34(5):2367–2386, 2006.

Mert Gurbuzbalaban, Asuman Ozdaglar, Pablo A Parrilo, and Nuri Vanli. When cyclic
coordinate descent outperforms randomized coordinate descent. In Advances in Neural
Information Processing Systems, pages 6999–7007, 2017.

Isabelle Guyon, Steve Gunn, Asa Ben-Hur, and Gideon Dror. Result analysis of the nips
2003 feature selection challenge. In Advances in Neural Information Processing Systems,
pages 545–552, 2005.

Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical Learning with Spar-

sity: The Lasso and Generalizations. CRC Press, FL, 2015.

Trevor Hastie, Robert Tibshirani, and Ryan Tibshirani. Best subset, forward stepwise
or lasso? analysis and recommendations based on extensive comparisons. Statist. Sci.,
35(4):579–592, 11 2020. doi: 10.1214/19-STS733. URL https://doi.org/10.1214/
19-STS733.

Hussein Hazimeh and Rahul Mazumder. Fast best subset selection: Coordinate descent
and local combinatorial optimization algorithms. Operations Research, 68(5):1517–1537,
2020. doi: 10.1287/opre.2019.1919. URL https://doi.org/10.1287/opre.2019.1919.

Hussein Hazimeh, Rahul Mazumder, and Ali Saab. Sparse regression at scale: Branch-and-

bound rooted in ﬁrst-order optimization. arXiv preprint arXiv:2004.06152, 2020.

Hussein Hazimeh, Rahul Mazumder, and Peter Radchenko. Grouped variable selection
with discrete optimization: Computational and statistical perspectives. arXiv preprint
arXiv:2104.07084, 2021.

Ja-Yong Koo, Yoonkyung Lee, Yuwon Kim, and Changyi Park. A Bahadur representation
of the linear support vector machine. Journal of Machine Learning Research, 9(Jul):
1343–1368, 2008.

44

Learning Sparse Classifiers

Jon Lee and Sven Leyﬀer. Mixed Integer Nonlinear Programming. Springer Publishing

Company, Incorporated, 2011. ISBN 1461419263.

Huan Li and Zhouchen Lin. Accelerated proximal gradient methods for nonconvex pro-

gramming. Advances in Neural Information Processing Systems, 28:379–387, 2015.

Zhaosong Lu. Iterative hard thresholding methods for l0 regularized convex cone program-
ISSN 1436-4646. doi:

ming. Mathematical Programming, 147(1):125–154, Oct 2014.
10.1007/s10107-013-0714-4. URL https://doi.org/10.1007/s10107-013-0714-4.

R. Mazumder, P. Radchenko, and A. Dedieu. Subset Selection with Shrinkage: Sparse

Linear Modeling when the SNR is low. ArXiv e-prints, August 2017.

Rahul Mazumder, Jerome H. Friedman, and Trevor Hastie. Sparsenet: Coordinate descent
with nonconvex penalties. Journal of the American Statistical Association, 106(495):
1125–1138, 2011. doi: 10.1198/jasa.2011.tm09738. URL https://doi.org/10.1198/
jasa.2011.tm09738. PMID: 25580042.

Alan Miller. Subset selection in regression. CRC Press Washington, 2002.

Balas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM journal on

computing, 24(2):227–234, 1995.

Sahand Negahban, Bin Yu, Martin J Wainwright, and Pradeep K Ravikumar. A uniﬁed
framework for high-dimensional analysis of m-estimators with decomposable regularizers.
In Advances in Neural Information Processing Systems, pages 1348–1356, 2009.

Yu Nesterov. Eﬃciency of coordinate descent methods on huge-scale optimization problems.

SIAM Journal on Optimization, 22(2):341–362, 2012.

Yu Nesterov. Gradient methods for minimizing composite functions. Mathematical Pro-

gramming, 140(1):125–161, 2013.

Neal Parikh and Stephen Boyd. Proximal Algorithms. Now Foundations and Trends, 2014.
ISBN 9781601987167. doi: 10.1561/2400000003. URL http://ieeexplore.ieee.org/
xpl/articleDetails.jsp?arnumber=8187362.

A. Patrascu and I. Necoara. Random coordinate descent methods for (cid:96)0 regularized convex
IEEE Transactions on Automatic Control, 60(7):1811–1824, July 2015.

optimization.
ISSN 0018-9286. doi: 10.1109/TAC.2015.2390551.

Bo Peng, Lan Wang, and Yichao Wu. An error bound for l1-norm support vector machine
coeﬃcients in ultra-high dimension. Journal of Machine Learning Research, 17:1–26,
2016.

Yaniv Plan and Roman Vershynin. Robust 1-bit compressed sensing and sparse logistic
regression: A convex programming approach. IEEE Transactions on Information Theory,
59(1):482–494, 2013.

45

Dedieu, Hazimeh, and Mazumder

Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax rates of estimation for high-
dimensional linear regression over lq-balls. IEEE transactions on information theory, 57
(10):6976–6994, 2011.

Pradeep Ravikumar, Martin J Wainwright, and John D Laﬀerty. High-dimensional ising
model selection using l1-regularized logistic regression. The Annals of Statistics, 38(3):
1287–1319, 2010.

Philippe Rigollet. 18.s997: High dimensional statistics. Lecture Notes, Cambridge, MA,

USA: MIT OpenCourseWare, 2015.

Toshiki Sato, Yuichi Takano, Ryuhei Miyashiro, and Akiko Yoshise. Feature subset selection
for logistic regression via mixed integer optimization. Computational Optimization and
Applications, 64(3):865–880, 2016.

Shai Shalev-Shwartz and Tong Zhang. Proximal stochastic dual coordinate ascent. arXiv

preprint arXiv:1211.2717, 2012.

Bernadetta Tarigan and Sara A Van De Geer. Classiﬁers of support vector machine type

with l1 complexity regularization. Bernoulli, 12(6):1045–1076, 2006.

Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society. Series B (Methodological), pages 267–288, 1996.

P. Tseng. Convergence of a block coordinate descent method for nondiﬀerentiable min-
imization.
Journal of Optimization Theory and Applications, 109(3):475–494, 2001.
ISSN 1573-2878. doi: 10.1023/A:1017501703105. URL http://dx.doi.org/10.1023/A:
1017501703105.

Berk Ustun and Cynthia Rudin. Supersparse linear integer models for optimized medical

scoring systems. Machine Learning, 102(3):349–391, 2016.

Sara A Van de Geer. High-dimensional generalized linear models and the lasso. The Annals

of Statistics, pages 614–645, 2008.

Rui Wang, Naihua Xiu, and Shenglong Zhou. Fast newton method for sparse logistic

regression. arXiv preprint arXiv:1901.02768, 2019.

Laurence A Wolsey and George L Nemhauser.

Integer and combinatorial optimization,

volume 55. John Wiley & Sons, 1999.

Stephen J Wright. Coordinate descent algorithms. Mathematical Programming, 151(1):

3–34, 2015.

X. Yuan and Q. Liu. Newton-type greedy selection methods for (cid:96)0 -constrained mini-
IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12):

mization.
2437–2450, 2017. doi: 10.1109/TPAMI.2017.2651813.

Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The

Annals of statistics, 38(2):894–942, 2010.

46

Learning Sparse Classifiers

Xiang Zhang, Yichao Wu, Lan Wang, and Runze Li. Variable selection for support vector
machines in moderately high dimensions. Journal of the Royal Statistical Society: Series
B (Statistical Methodology), 78(1):53–76, 2016.

Shenglong Zhou, Naihua Xiu, and Hou-Duo Qi. Global and quadratic convergence of newton
hard-thresholding pursuit. Journal of Machine Learning Research, 22(12):1–45, 2021.

47

