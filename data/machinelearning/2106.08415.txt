Code to Comment Translation: A Comparative Study
on Model Effectiveness & Errors

Junayed Mahmud, Fahim Faisal, Raihan Islam Arnob,
Antonios Anastasopoulos, Kevin Moran
Department of Computer Science
George Mason University, USA
jmahmud,ffaisal,rarnob,antonis,kpmoran@gmu.edu

1
2
0
2

n
u
J

5
1

]
E
S
.
s
c
[

1
v
5
1
4
8
0
.
6
0
1
2
:
v
i
X
r
a

Abstract

Automated source code summarization is a
popular software engineering research topic
wherein machine translation models are em-
ployed to “translate” code snippets into rel-
evant natural language descriptions. Most
evaluations of such models are conducted us-
ing automatic reference-based metrics. How-
ever, given the relatively large semantic gap
between programming languages and natural
language, we argue that this line of research
would beneﬁt from a qualitative investigation
into the various error modes of current state-
of-the-art models. Therefore, in this work,
we perform both a quantitative and qualitative
comparison of three recently proposed source
In our quan-
code summarization models.
titative evaluation, we compare the models
based on the smoothed BLEU-4, METEOR,
and ROUGE-L machine translation metrics,
and in our qualitative evaluation, we perform
a manual open-coding of the most common
errors committed by the models when com-
pared to ground truth captions. Our investiga-
tion reveals new insights into the relationship
between metric-based performance and model
prediction errors grounded in an empirically
derived error taxonomy that can be used to
drive future research efforts.1

1

Introduction and Motivation

Proper documentation is an important component
of modern software development, and previous
studies have illustrated its advantages for tasks
ranging from program comprehension (Garousi
et al., 2015) to software maintenance (Chen and
Huang, 2009). However, manually documenting
software is a tedious task (McBurney and McMil-
lan, 2014) and modern agile development practices

1Our annotations and guidelines are publicly avail-
able on Github https://github.com/SageSELab/
CodeSumStudy and Zenodo: https://doi.org/10.
5281/zenodo.4904024.

tend to champion working code over extensive doc-
umentation (Beck et al., 2001). As such, a range
of important documentation activities are often ne-
glected (Zhi et al., 2015) leading to deﬁciencies in
carrying out development activities and contribut-
ing to technical debt. Because of this, researchers
have worked to develop automated code summa-
rization techniques wherein machine translation
models are employed to generate precise, seman-
tically accurate natural language descriptions of
source code (Haiduc et al., 2010). Due to the
promise and potential beneﬁts of effective auto-
mated source code summarization techniques, this
area of work has seen constant and growing atten-
tion at the intersection of the software engineering
and natural language processing research commu-
nities (Zhu and Pan, 2019).

Various techniques for automated source code
summarization have been explored extensively over
the past decade. Some of the earliest approaches
made use of a combination of structural code infor-
mation and text retrieval techniques for determin-
ing the most relevant terms (Haiduc et al., 2010),
with follow up work investigating the use of topic
modeling (Eddy et al., 2013). Techniques then
evolved from using information retrieval to canon-
ical machine learning techniques, with Ying and
Robillard (2013) using supervised Naive Bayes
and Support Vector Machine classiﬁers to iden-
tify code fragment lines that could be used as
suitable summaries. One of the ﬁrst appearances
of language modeling came from McBurney and
McMillan (2016) who proposed an approach com-
bining a software word usage model, natural lan-
guage generation systems, and the PageRank al-
gorithm (Langville and Meyer, 2006) to generate
summaries. Driven by the advent of deep learning,
current state-of-the-art techniques generally make
use of large-scale neural models and have signiﬁ-
cantly improved the performance of code summa-

 
 
 
 
 
 
rization tasks. For instance, Iyer et al. (2016) used
Long Short Term Memory (Hochreiter and Schmid-
huber, 1997) with attention (Bahdanau et al., 2015)
to generate summaries from a code snippet. Fol-
lowing this work, researchers have applied sev-
eral deep learning-based approaches to the task of
source code summarization (Zhang et al., 2020a;
Wan et al., 2018; LeClair et al., 2020).

In most works on automated code summariza-
tion, the performance of the generated natural lan-
guage descriptions is evaluated using reference-
based metrics adapted from machine translation,
e.g., BLEU (Papineni et al., 2002) and ME-
TEOR (Lavie and Agarwal, 2007), or text sum-
marization, e.g., ROUGE (Lin, 2004). As such,
most researchers make conclusions based on the
results obtained using these metrics. However, the
code summarization task is a difﬁcult one – due
in large part to the sizeable semantic gap between
the modalities of source code and natural language.
As such, while these metrics provide a general il-
lustration of model efﬁcacy, it can be difﬁcult to
determine the speciﬁc shortcomings of neural code
summarization techniques without a more exten-
sive qualitative investigation into their errors.

Few past studies have examined the failure
modes of neural code summarization models as
we outline in §6. Therefore, to further explore
this topic, in this paper we perform both a qualita-
tive and quantitative empirical comparison of three
neural code summarization models. Our quanti-
tative evaluation offers a comparison of three re-
cently proposed models (CodeBERT (Feng et al.,
2020), NeuralCodeSum (Ahmad et al., 2020), and
code2seq (Alon et al., 2019)) on the Funcom
dataset (LeClair and McMillan, 2019) using the
smoothed BLEU-4 (Lin and Och, 2004), ME-
TEOR (Lavie and Agarwal, 2007), and ROUGE-
L (Lin, 2004) metrics whereas our qualitative eval-
uation consists of a rigorous manual categorization
of model errors (compared to ground truth captions)
based on a procedure adapted from the practice of
open coding (Miles et al., 2013). In summary, this
paper makes the following contributions:

• We offer a quantitative comparative analysis of
the CodeBERT, NeuralCodeSum, and code2seq
models applied to the task of Java method sum-
marization in the Funcom dataset. The results of
this analysis illustrate that the CodeBERT model
performs best to a statistically signiﬁcant degree,
achieving a BLEU-4 score of 24.15, a METEOR

score of 30.34, and a ROUGE-L score of 35.65.
• We conduct a qualitative investigation into the
various prediction errors made by our three stud-
ied models and derive a taxonomy of error modes
across the various models. We also offer a dis-
cussion about differences in errors made across
models and suggestions for model improvements.
• We offer resources on GitHub2 and Zenodo3 for
replicating our experiments, including code and
trained models, in addition to all of the data
and examples used in our qualitative analysis
of model errors.

2 Background: Deep Learning for Code

Summarization

This section outlines necessary background re-
garding our chosen evaluation dataset as well as
the three neural code summarization models upon
which we focus our empirical investigation.

2.1 Dataset: Funcom

In this study we make use of the Funcom
dataset (LeClair and McMillan, 2019).4 We se-
lected this dataset primarily for three reasons: (i)
this dataset was speciﬁcally curated for the task of
code summarization, excluding methods more than
100 words and comments with >13 and <3 words
or which were auto-generated, (ii) it is currently
one of the largest datasets speciﬁcally tailored for
code summarization, containing over 2.1M Java
methods with paired JavaDoc comments, (iii) it
targets Java, one of the most popular program-
ming languages.5
In order to make for a feasi-
ble training procedure for our various model con-
ﬁgurations, and to keep the dataset size in line
with past work to which our studied models were
applied (e.g., the size of the CodeXGlue dataset
from Lu et al. (2021), containing approximately
180000 Java methods and JavaDoc pairs, to which
CodeBERT was applied) we chose to use the ﬁrst
500,000 method-comment pairs from the ﬁltered
Funcom dataset for our experiments. Note that
we did not use the tokenized version of the dataset
as provided by LeClair and McMillan (2019) as
each of our models has unique pre-processing con-
straints, described in detail in Appendix B.

2https://github.com/SageSELab/

CodeSumStudy

3https://doi.org/10.5281/zenodo.

4904024

4http://leclair.tech/data/funcom/
5https://octoverse.github.com

Figure 1: Code to text translation using CodeBERT.

2.2 Models

CodeBERT CodeBERT (Feng et al., 2020) is a
bimodal pre-trained model used in natural language
(NL) and programming language (PL) tasks. This
model supports six programming language tasks
in various downstream NL-PL applications, e.g.,
code search, code summarization, etc. The archi-
tecture of the model is based on BERT (Devlin
et al., 2019), speciﬁcally following the RoBERTa-
base (Liu et al., 2019) in using 125 million model
parameters. The objectives of training CodeBERT
are masked language modeling (MLM) and re-
placed token detection (RTD). Recently, Microsoft
Research Asia introduced the CodeXGLUE bench-
mark that consists of 14 datasets for ten diversiﬁed
code intelligence tasks (Lu et al., 2021). They
ﬁne-tuned CodeBERT in code-to-natural-language
generation tasks. CodeBERT was used as the en-
coder, with a six-layer self-attentive (Vaswani et al.,
2017) decoder. An architecture for code-to-text
translation using the CodeBERT encoder is shown
in Figure 1. The dataset Lu et al. (2021) used is
derived from CodeSearchNet (Husain et al., 2019).

NeuralCodeSum The second technique we
study is NeuralCodeSum (Ahmad et al., 2020).
Here, the authors explored a transformer-based ap-
proach to perform the task of code summarization,
using a self-attention mechanism to capture the
long-term dependencies that are common in source
code. In order to enable the model to both copy
from already seen source code and to generate new
words from its vocabulary, they employed a copy
mechanism (See et al., 2017). One important dis-
tinction of source code that this model takes into
account is that the absolute token position does not
necessarily assist in the process of learning effec-
tive source code representations (i.e., int a=b+c
and int a=c+b; both convey the same mean-
ing). To mitigate this problem, they used the rela-
tive positioning of tokens to encode pairwise token
relations. Additionally, the authors of this model
also explored the integration of an abstract syntax
tree (AST)-based source code representation. How-

ever, they found that the AST information did not
result in a marked improvement in model accuracy.

code2seq The third model we consider in our
study is code2seq (Alon et al., 2019), which is a
widely utilized technique that was originally de-
signed for the task of method name prediction. The
authors of this work focused on capturing the true
syntactic construction of source code by encoding
AST paths. They showed that code snippets which
exhibited differences in lines but that were designed
for similar functionality often have similar patterns
in their AST trees. To take advantage of this obser-
vation, code2seq uses an encoder-decoder architec-
ture that attends to the constructed AST encoding
to generate the resultant sequence. The authors
experimented with Java method name generation
as well as code captioning tasks. They compared
their code captioning approach to CodeNN (Iyer
et al., 2016) using BLEU score, against which it
illustrated improved performance.

3 Design of the Empirical Evaluation

To evaluate the performance of our three models
applied to the task of code summarization, we per-
form both a quantitative and qualitative evaluation
centered upon the following research questions:

RQ1: How effective is each model in terms of pre-
dicting natural language summaries from Java
methods?

RQ2: What types of errors do our studied models
make when compared to ground truth captions?

RQ3: What differences (if any) are there between
the errors made by different models?

3.1 Evaluation Methodology for RQ1
In this subsection, we discuss how we split the
dataset, the evaluation metrics we use, and how we
conﬁgure our studied models for training.

3.1.1 Dataset Preparation and Metrics
To adapt the Funcom dataset for our study, we
ﬁrst sampled the ﬁrst 500k function-comment pairs
from the ﬁltered Funcom dataset into training
(80%), validation (10%) and testing (10%) for our
experiment, ensuring that the method-comment
pairs between our training and testing datasets
came from separate software projects (i.e., split by
project), as suggested by the Funcom authors, in or-
der to avoid artiﬁcial inﬂation of performance due
to data snooping (LeClair and McMillan, 2019).

private void swap (int a, int b) {int temp = a;a = b;b = temp;}swap two integersEncoder (CodeBERT)Decoder (Transformers)Training

Dev

Testing

CodeXGlue

164923

5183

10955

Funcom

400000

50000

49997

Table 1: Data Statistics. We use the Funcom dataset.

As a comparison to past work, we illustrate the
training, validation and test dataset sizes between
the CodeXGLUE and Funcom datasets in Table 1.
As mentioned earlier we preprocess the sampled
dataset based on the requirements for each of our
chosen models, and provide details in Appendix B.
Prior work has explored the use of several
reference-based metrics, e.g., BLEU, METEOR,
and ROUGE-L for evaluating the performance of
code summarization. In our study we make use
of smoothed BLEU-4 as it was previously used to
evaluate the CodeBERT model (Feng et al., 2020).
BLEU is the geometric average of n-gram preci-
sions between the predicted and reference captions
multiplied by a brevity penalty that penalizes the
generation of short descriptions. We use the BLEU
metric applying a smoothing technique (Lin and
Och, 2004), which adds one count in the case of
n-gram hits to address hypotheses shorter than n.
In addition, we include METEOR (Lavie and Agar-
wal, 2007) and ROUGE-L (Lin, 2004) in our study.
METEOR computes the harmonic mean between
precision and recall based on unigram matches be-
tween the prediction from a model and reference,
also going beyond exact matches to include stem-
ming, synonyms, and lemmatization. ROUGE-L
computes the longest common subsequence-based
F-measure between the hypotheses and references.

3.1.2 Model Conﬁgurations and Training

We train, validate and test the three models de-
scribed in §2 for the task of summarizing Java
methods in natural language. A subset of model
hyperparameters for all three studied deep learning
models is shown in Table 2. We preprocess the
dataset for each of the models according to their
individual requirements and select the hyperparam-
eters for each of the models based on the optimal
settings from prior work.Additionally, we apply
some global preprocessing that is common to all
models, taken from recent work on language mod-
eling for code (Mastropaolo et al., 2021). Initially,
we remove all the comments that exist inside meth-
ods, as the commented code could lead to poor
predictions. Next, all the JavaDoc comments are

Hyper-
parameters

Batch Size
Beam Size
Optimizer
Learning Rate
#epochs

CodeBERT Neural-

code2seq

16
16
Adam
0.00005
15

CodeSum

64
4
Adam
0.0001
38

512
0
Momentum
0.01+decay
39

Table 2: Model Hyperparameters.

ﬁltered keeping only the description of the method.
Finally, we clean HTML and remove special char-
acters from the JavaDoc captions. We provide a
detailed account of our preprocessing and train-
ing techniques in Appendix B and in our publicly
available resources.

CodeBERT Model Conﬁgurations and Train-
ing: We use the open-source implementation6
made available by Microsoft to ﬁne-tune Code-
BERT using the Funcom dataset. We utilized the
optimal model conﬁgurations for this model used
to train on the CodeXGlue (Lu et al., 2021) dataset
with hyperparamters tuned on the Funcom dataset.

NeuralCodeSum Model Conﬁgurations and
Training: We use the open-source implementa-
tion of NeuralCodeSum7 to train the model in our
study. We performed one additional preprocessing
step than typical with this model, splitting camel-
case words. The dropout rate is set to 0.2 and we
train for a maximum of 1000 epochs. Additionally,
we stop training if validation does not improve after
20 iterations.

code2seq Model Conﬁgurations and Training:
We make use of the publicly available implementa-
tion of code2seq.8 To use the Funcom dataset, we
had to prepare the AST node representation using a
modiﬁed dataset build script.9 The original dataset
build script was designed to predict the method
name whereas we modify it to predict summaries.
One problem we faced representing Funcom meth-
ods as ASTs is that there were some code examples
which could not be parsed into an AST represen-
tation mainly because of the imposed minimum
code length threshold and the method not having

6https://github.com/microsoft/

CodeXGLUE/tree/main/Code-Text/
code-to-text

7https://github.com/wasiahmad/

NeuralCodeSum

8https://github.com/tech-srl/code2seq
9https://github.com/LRNavin/

AutoComments

any AST-Paths. As a result, we were able to train
code2seq on only a subset of the Funcom dataset
(40009/50000 ≈ 80.02%). To train the model we
made use of large batch sizes (e.g., 256 and 512)
as we noted smaller batch sizes resulted in instabil-
ity. As code2seq was originally designed to predict
method names, we also made some changes in the
model parameters to facilitate longer prediction
sequences, which we give in Appendix A.

3.2 Evaluation Methodology for RQ2 & RQ3
We performed a manual, qualitative analysis on the
output of the three models10 to answer RQ2 and
RQ3 in order to better understand and compare
the various types of errors each model makes. The
methodology we follow to categorize the model
prediction errors follows a procedure inspired by
open coding (Miles et al., 2013), which has been
used in prior studies to categorize large numbers of
software project artifacts (Linares-V´asquez et al.,
2017, inter alia). Initially, we randomly selected a
small number of samples from our validation split
of the Funcom dataset, and applied each of our
three models to generate captions. The four annota-
tors11 then met and discussed the samples to derive
an initial set of labels that described deviations
from the ground truth. We found that 15 meth-
ods (each with three predictions, one from each of
our studied models) were enough to reach an ini-
tial agreement on the labels. Note that we use the
ground truth captions as a “gold set” in order to ori-
ent our analysis to a shared understanding among
annotators and to limit potential subjectivity.

Next, we conducted two rounds of independent
labeling, wherein three annotators independently
coded a samples of method-comment pairs and
predicted comments, such that two annotators in-
dependently coded each sample. Here we deﬁne a
“sample” as a method ↔ gold-comment pair, and
the three resulting predictions from CodeBERT,
NeuralCodeSum, and code2seq respectively for the
method. During this process, annotators were free
to add additional labels outside of the initial set if
they deemed it necessary. The ﬁrst round of label-
ing consisted of 148 samples in total, amounting
to 148 × 3 = 444 predictions from our studied
models. After the independent labeling process,
the authors met to resolve the conﬂicts among the
labels. This initial round of coding resulted in a

10Some examples of the predictions are shown in Ap-

pendix C

11All annotators are also authors of this study.

disagreement on ≈ 82% of the samples wherein
author discussion was needed in order to derive a
common agreed upon label. There were two main
reasons for this relatively high rate of disagreement:
(i) the authors created some category labels with
similar semantic meanings, but different labels, and
(ii) some of the authors had different interpretations
of shared meanings. However, through an exten-
sive discussion, the conﬂicts were resolved and a
shared understanding reached. The second round
of independent labeling consisted of 50 samples,
and resulted in a disagreement rate of only ≈ 27%,
illustrating the stronger consensus among authors.
We derive the taxonomy presented in §4 from la-
bels present after both rounds of our open coding
procedure.

4 Evaluation Results

In this section, we will discuss the quantitative and
qualitative results from our empirical study in order
to answer our research questions.

4.1 RQ1 Results: Evaluation Based on

Reference-Based Metrics

To perform the evaluation on the Funcom dataset,
we use the optimal hyper-parameters shown in Ta-
ble 2 for the three deep learning models. Neural-
CodeSum could not predict natural language de-
scriptions for some examples (≈ 80). The most
likely reason for this situation is the errors in pro-
cessing code or docstring tokens. Table 3 shows
the quantitative results obtained based on smoothed
BLEU-4, METEOR, and ROUGE-L scores. The
results show that CodeBERT performs best among
the three models. We believe that the reason we
observe CodeBERT achieving this level of perfor-
mance is that this model is pre-trained on both
bimodal data and unimodal data (wherein bimodal
data refers to the coupled code and natural lan-
guage pairs and unimodal data refers to either nat-
ural language descriptions without code snippets
or code snippets without natural language descrip-
tions (Feng et al., 2020)).

Statistical signiﬁcance
In addition to calculat-
ing the evaluation scores (i.e. smoothed BLEU-4,
METEOR, ROUGE), we conducted statistical sig-
niﬁcance tests for all three metrics to assess the
validity of the obtained results. We took 19009
examples from the test dataset and used pairwise
bootstrap re-sampling (Koehn, 2004) between all

Models

Smoothed BLEU-4 METEOR ROUGE-L

CodeBERT
NeuralCodeSum
code2seq

24.15
21.50
18.61

30.34
27.78
27.31

35.65
33.71
33.52

Table 3: Evaluation Results with three metrics. CodeBERT is consistently better than the other two models.

In comparison to Neural-
3 model predictions.
CodeSum, we found CodeBERT performs better
with a mean score increase (BLEU-4 2.8, ME-
TEOR 2.9, ROUGE 2.2) at a 95% conﬁdence in-
terval, thus indicating a performance delta that is
statistically signiﬁcant.

4.2 RQ2 Results: Types of errors
In the ﬁrst round of our study that included 148 ×
3 = 444 samples, we were able to classify the
errors for 398 generated natural language descrip-
tions from the models from the validation dataset.
The remaining 46 descriptions that were not clas-
siﬁed as predictions were not made by the models
due to errors in parsing and one error in process-
ing code tokens. This singular error was due to
the fact an entire code snippet was commented out,
and our models do not process commented code.
Thus, we did not include the predictions for the
three different models for that code snippet in our
study. In the other 43 cases, the code2seq model
could not generate predictions because the model
was not able to parse the AST.

Our error taxonomy derived after both rounds
of the open coding process is shown in Fig-
ure 2. The taxonomy consists of seven high-
level categories with each consisting of mul-
tiple lower-level sub-categories.
To elabo-
rate, Semantically Unrelated to Code is a sub-
category of Incorrect Semantic Information.
Note that one category Consistent with Ground
Truth is dedicated to those captions that generally
matched the ground truth, which we include for
completeness. The numbers that are shown be-
side the name of the sub-categories illustrate the
number of errors for CodeBERT, NeuralCodeSum,
and code2seq respectively. The numbers shown
beside the categories’ names represent the cumula-
tive sum of the sub-categories. We provide a small
number of examples of these categorizations in Ap-
pendix C, and provide all labeled examples in our
public resources on GitHub and Zenodo. We make
the following notable observations resulting from
our derived taxonomy:

• Encouragingly, among the samples studied, the
largest category of samples did not display signif-
icant errors, falling into the Consistent with
Ground Truth category (162/535 ≈ 30.28%).
This category is the most frequent among all, but
we do see CodeBERT (unsurprisingly) exhibit
the largest number of reasonable summaries.
• The most prevalent error category exhib-
ited among our studied models was that of
Missing Information (148/535 ≈ 27.66%)
followed by the Incorrect Construction cate-
gory (110/535 ≈ 20.56%). This seems to indi-
cate that one of the biggest struggles for current
neural code summarization techniques is related
to the inclusion of various types of necessary
information in the summary itself, followed by
issues in properly constructing comment syntax.
• The models also either incorrectly recognized or
failed to recognize salient identiﬁers that were
needed to understand method functionality in
a non-negligible number of cases (71/535 ≈
13.2%). This suggests that mechanisms for iden-
tifying focal identiﬁers i.e., those that might
prominently contribute to describing the func-
tionality, could be beneﬁcial, similar to past work
on identifying focal methods (Qusef et al., 2010).
• Some of the models exhibited generated sum-
maries that over-generalized to the detriment
of the summary meaning (49/535 ≈ 9.15%) ,
whereas very few summaries contained extrane-
ous information.

• Further study is needed to gain a better under-
standing of the various facets of the critical infor-
mation and non-critical information that captions
were missing. For instance, we plan to explore
whether the necessary information is contained
within the code itself, or perhaps in semantically
related methods. We leave this for future work.

4.3 RQ3 Results: Comparison of three

different models

One advantage of the formulation of our empirical
study is that we are able to compare the various
shortcomings of our studied models as they relate

Figure 2: Taxonomy of the Errors Between the Generated Summaries and the Ground Truth

to our qualitative error analysis. To this end, we
make the following notable observations:

niques (Cohn et al., 2016; Mi et al., 2016) to
our task to mitigate this issue.

• The most frequent error categories for Code-
BERT and NeuralCodeSum are Consistent
but Missing Specific Information (Code-
BERT: 56/197 ≈ 28.42% and NeuralCodeSum:
35/197 ≈ 17.77%). However, for code2seq,
the most
frequent category is Repetition
(27/141 ≈ 19.15%).

• A non-negligible number of predictions from
CodeBERT fall into the focusing Only on the
Method Name category (20/197 ≈ 10.15%).
This may suggest a reliance of the model on
descriptive method names in order to produce
reasonable summaries.

• NeuralCodeSum and code2seq produce a small
number of predictions that are Semantically
Unrelated to Code. However, we did not ﬁnd
any such cases for CodeBERT.

• Similar to our quantitative evaluation, we ﬁnd
that CodeBERT performs best, but suffers from
a large number of errors related to Missing
Information.
In future work, we will inves-
tigate the adaptation of source coverage tech-

5 Discussion & Learned Lessons

Takeaway 1: The CodeBERT model illustrates
improved performance on the Funcom dataset
as compared to CodeXGLUE, likely due to
the ﬁltering steps undertaken in its construc-
tion. Previously, the CodeBERT model was ﬁne-
tuned on the CodeXGlue dataset and the smoothed
BLEU-4 score obtained on the Java dataset was
17.65 (Lu et al., 2021). However, we ﬁne-tuned
the model on the Funcom dataset and obtained a
smoothed BLEU-4 score of 24.15. We believe there
are two primary contributing factors to this obser-
vation: 1) A higher volume of data, and 2) ﬁltering
strategies. CodeXGLUE only provides 164923
training examples, whereas we used 400000 Java
Methods and Javadoc pairs during he ﬁne-tuning
process. Moreover, The CodeXGLUE dataset is
obtained from CodeSearchNet and the documents
that contain special tokens (e.g., <img> or https:)
are ﬁltered. In our preprocessing, we did not com-
pletely remove such data in the preprocessing; we
only remove the HTML and special characters from
the JavaDoc captions. We hypothesize that such
characters may contain important information and

Extraneous/Unnessecary Information Included (2, 3, 4)Missing Context  (1, 2, 2)Incorrect Construction (26, 31, 53)Consistent with Ground Truth (88, 57, 17)Over-Generalization (7, 21, 21)Incorrect Semantic Information (6, 27, 19)Missing Information (65,56,27)Missing Prog. Language Information (0, 0, 2)•Missing Attributes that refer to PL speciﬁc information.Missing Database Information (1, 2, 0)•Missing database attributes that provide needed context to method functionality.Consistent with Speciﬁc Info (30, 15, 5)•Comment matches ground truth well.Consistent but Missing Speciﬁc Info (56, 35, 12)•Comment matches ground truth mostly, but misses some important speciﬁc information.Improves upon Semantic Meaning (2, 6, 0)•The predicted comment matches the ground and improves capturing method meaning.Consistent but with Unnecessary Info (0, 1, 0)•Accurate but has some unnecessary info.Different Meaning (2, 3, 3)•Comment over-generalizes on the meaning of the code functionality.Algorithmically Incorrect (1, 6, 3)•Overgeneralizes to the point of incorrectnessMissing Attribute Speciﬁcation (4, 12, 15)•Uses generic names such as var.Partial Incorrect Information (6, 11, 3)•Semantically meaningful, with a few errors.Semantically Unrelated to Code (0, 11, 13)•Does not capture code context whatsoever.Algorithmically Incorrect (0, 5, 3)•Conveys a different algorithmic meaning as compared to the code.Incorrect Identiﬁer/Attribute (5, 19, 15)•Correctly identiﬁes a variable or attribute, but uses it incorrectly.Incomplete Sentence (1, 1, 10)•Predicted comment is grammatically incomplete.Repetition (0, 7, 27)•Comment contains unnecessary repetition of a word or fragment between 2-3 times.Extreme Repetition (0, 2, 1)•Comment contains unnecessary repetition of a word or fragment more than 2-3 times.Focusing Only on Method Name (20, 1, 0)•When comment focuses mostly on the method name, which provides an incomplete but partial description of the functionality.Grammatical Errors (0, 1, 0)•Grammatical Error is present in predicted caption.Missing Critical Information (21, 14, 7)•Comment is missing critical semantic information.Missing Task Elaboration (5, 2, 1)•Did not describe what code was doing properly.Missing Non-Critical Information (28, 19, 5)•Useful comment but non-critical info missing.Missing Web-Related Information (0, 1, 0)•Comment failed to mention web-related identiﬁer.Failed to Mention Identiﬁers (0, 11, 6)•Does not mention speciﬁc variable/attribute names, often using a generic identiﬁer.Missing Identiﬁer (5, 3, 7)•No identiﬁer mentioned at all.Missing Data Structure Information (2, 0, 1)•Does not capture relevant data structure infoMissing Syntax Information (2, 6, 0)•Important syntactic information (e.g. code ordering) is missing.Missing Exception (1, 0, 0)•Does not mention relevant exception infoMissing Conditional Information (1, 0, 0)•Misses code branching informationUnnecessary Data Structure Info (1, 0, 0)•Adds unnecessary data structure info to comment.Unnecessary File Information (0, 1, 1)•Adds unnecessary ﬁle information to comment.Unnecessary Incorrect Information (1, 2, 3)•Adds information to comment that is both incorrect and unnecessary.The numbers shown for each category illustrate the number of instances found for (CodeBERT, NeuralCodeSum, and code2seq) respectivelyas such lead to more effective predicted summaries.
Takeaway 2: Models that rely on statically pars-
ing source code can lead to high numbers of
missing/incomplete predictions. The preprocess-
ing for the code2seq model includes generating
strings from the AST node representation of each
method. Unfortunately, it is difﬁcult (or impos-
sible) to construct a suitable AST representation
for methods that fall under a certain token length
threshold. As a result, about 19.98% of the original
dataset could not be fed into the code2seq testing
module, and for which we could not generate any
prediction for these examples.
Takeaway 3: Some of the generated summaries
provide a semantic meaning similar to the
ground truth, despite exhibiting fewer n-gram
matches. Our studied models can generate sum-
maries that contain relevant semantic informa-
tion which can be useful for code comprehension
despite not perfectly matching the ground truth.
For instance, let’s consider the following example
ground truth for a Java method, “this method sets
the text for the heading on the component”. The
generated summary from the CodeBERT model is
“sets the heading caption”. Comparing these two
descriptions will not necessarily result in a high
BLEU-4 score. This suggests that a modiﬁcation
to the evaluation procedure for these models may
provide a more realistic characterization of model
performance in practice. For instance, measuring
BERTScore in addition to other metrics for eval-
uation (Zhang et al., 2020b)12 may help to better
capture semantic similarities compared to purely
symbolic similarities.
Takeaway 4: Future techniques for Neural
Code Summarization should carefully consider
techniques for mitigating potential errors re-
lated to Missing Information, and Incorrect
Construction as these are the most preva-
lent error types observed in our taxonomy.
Our error taxonomy provides concrete indica-
tors on where different types of models stand
to gain performance in order to make them
useful for downstream deployment.
In partic-
ular, we suggest
that future research focuses
on rectifying Missing Critical Information
and Missing Non-Critical Information rather
than Grammatical Errors or Unnecessary File
Information.
Takeway 5: Future studies should explore the

12https://github.com/Tiiiger/bert_score

combination of AST traversal based and self-
attention mechanism-based approaches to per-
form robust comment generation. AST-based
approach is useful to provide syntax level infor-
mation and it follows the structural tree traver-
sal method to capture the global information. At
the same time, we can see this approach is prone
to errors like Repetition and Semantically
Unrelated to Code. On the other hand, a self-
attention mechanism is useful to capture the local
information. So a multi-modal approach where
standard encoders can be utilized to combine both
AST-based and attention-based approaches can be
a viable direction to explore further.
Takeway 6: Robust evaluation metric(s) should
be developed that speciﬁcally focus on source
code - natural language translation. Source code
is fundamentally different from the natural lan-
guage from a number of perspectives. For instance,
it exhibits less signiﬁcant word order dependency,
the signiﬁcance of appropriate syntax naming and
mentioning, etc. So a robust code to natural lan-
guage translation evaluation metric should consider
assessment from both local and global levels. Stan-
dard machine translation metrics like BLEU, ME-
TEOR, ROUGE do not fully cover these factors.
As such, we encourage future work to study and de-
velop new forms of automated metrics for assessing
this special case of machine translation.

6 Related Work

6.1 Code to Comment Translation

Source code summarization is a topic of great inter-
est in software engineering research. The aim is to
automate a portion of the software documentation
process by automatically generating summaries of
a given granularity for a source code snippet (e.g.,
methods) to save developer effort. Techniques have
evolved from using more traditional Information
Retrieval (IR) and machine learning methods to
utilizing artiﬁcial neural networks.

One of the earliest deep-learning-based source
code summarization techniques is that by Iyer et al.
(2016). The authors used an attention-based neural
network to generate NL summaries from source
code. The approach was applied to the C# pro-
gramming language and SQL. Given the strong
syntax associated with programming languages, re-
searchers have also experimented with utilizing
AST information for source code summarization.
Hu et al. (2018) used an AST traversal method to

generate summaries. Additionally, LeClair et al.
(2019) utilized structural code information by en-
coding ASTs. Our goal in this study is to pro-
vide an overview on the performance of a vari-
ety of techniques, both sequence based (i.e., Code-
BERT, NeuralCodeSum), and structure-based (i.e.,
code2seq), in order to examine differences in quan-
titative and qualitative performance across differ-
ent types of models. Recently, a more complex
retrieval-augmented mechanism was introduced
that combines both retrieval and generation-based
methods for code to comment translation (Liu et al.,
2021). Finally, Bansal et al. (2021) recently pro-
posed a method that uses a vectorized represen-
tation of source code ﬁles. We plan to explore
additional techniques such as these in future work.

6.2 Empirical Studies of Code Summaries

and Code Summarization

Although many deep learning models are capable
of generating summaries from source code, very
few researchers have focused on evaluating the er-
rors made by the models from a human perspective.
During an early study on this topic, Ying and Ro-
billard (2013) tried to understand whether code
summaries achieved the same level of agreement
from multiple human perspectives. McBurney and
McMillan (2016) performed a comparison based
on the similarities of the summaries generated by
a newly proposed model which aimed at including
context in code summaries. However, most recent
work on code summarization models, e.g., (LeClair
et al., 2020; Bansal et al., 2021) depend on machine
translation metrics to measure the performance of
the code summarization task. However, a recent
study showed a necessity of revised metrics for
code summarization (Stapleton et al., 2020).

Perhaps the most closely related study to ours is
that conducted by Gros et al. (2020). In this study,
the authors question the validity of the formulation
of code summarization as a machine translation
task. In doing so, they apply code and natural lan-
guage summarization models to several recently
proposed code summarization datasets and one
natural language dataset. They found differences
between the natural language summarization and
code summarization datasets that suggests marked
semantic differences between the two task settings.
Additionally, the authors carried out experiments
which illustrate that reference-based metrics such
as BLEU score may not be well suited for mea-

suring the efﬁcacy of code summarization tasks.
Finally, the authors illustrate that IR techniques
perform reasonably well at code summarization.
While this study derives certain conclusions that
are similar to those in our work (e.g., the need for
better automated metrics) our study is differenti-
ated by our manually derived fault taxonomy.

To the best of our knowledge, no other study has
taken on a large-scale qualitative empirical study
with the objective of categorizing and understand-
ing errors between automatically generated and
ground truth code summaries. Thus, we believe
this is one of the ﬁrst papers to take a step toward
a grounded understanding of the errors made by
neural code summarization techniques – offering
empirically validated insights into how future code
summarization techniques might be improved.

7 Conclusion & Future Work

In this work we perform both quantitative and
qualitative evaluations of three popular neural
code summarization techniques. Based on our
quantitative analysis, we ﬁnd that the CodeBERT
model performs statistically signiﬁcantly better
than two other popular models (NeuralCodeSu, and
code2seq) achieving a smoothed-BLEU-4 score of
24.15, a METEOR score of 30.34, and a ROUGE-L
score of 35.65. Our qualitative analysis highlights
some the most common errors made by our studied
models and motivates follow-up work on improv-
ing speciﬁc model attributes.

In the future, we aim to expand our analysis to
additional retrieval-augmented summarization tech-
niques and to expand the scope and depth of our
neural code summarization model error taxonomy.

References

Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and
Kai-Wei Chang. 2020. A transformer-based ap-
proach for source code summarization. pages 4998–
5007.

Uri Alon, Shaked Brody, Omer Levy, and Eran Ya-
hav. 2019. code2seq: Generating sequences from
In 7th Inter-
structured representations of code.
national Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Neural machine translation by
CoRR,

Bengio. 2015.
jointly learning to align and translate.
abs/1409.0473.

Aakash Bansal, Sakib Haque, and Collin McMillan.
2021. Project-level encoding for neural source code
summarization of subroutines.

Kent Beck, Mike Beedle, Arie van Bennekum, Alis-
tair Cockburn, Ward Cunningham, Martin Fowler,
James Grenning, Jim Highsmith, Andrew Hunt, Ron
Jeffries, Jon Kern, Brian Marick, Robert C. Martin,
Steve Mellor, Ken Schwaber, Jeff Sutherland, and
Dave Thomas. 2001. Manifesto for agile software
development.

Jie-Cherng Chen and Sun-Jen Huang. 2009. An empir-
ical analysis of the impact of software development
problem factors on software maintainability. J. Syst.
Softw., 82(6):981–992.

Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-
molova, Kaisheng Yao, Chris Dyer, and Gholamreza
Haffari. 2016. Incorporating structural alignment bi-
ases into an attentional neural translation model. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 876–885, San Diego, California. Association
for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

B. P. Eddy, J. A. Robinson, N. A. Kraft, and J. C.
Carver. 2013. Evaluating source code summariza-
tion techniques: Replication and expansion. In 2013
21st International Conference on Program Compre-
hension (ICPC), pages 13–22.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-
BERT: A pre-trained model for programming and
In Findings of the Association
natural languages.
for Computational Linguistics: EMNLP 2020, pages
1536–1547, Online. Association for Computational
Linguistics.

Golara Garousi, Vahid Garousi-Yusifoglu, Guenther
Ruhe, Junji Zhi, Mahmoud Moussavi, and Brian
Smith. 2015. Usage and usefulness of technical soft-
In-
ware documentation: An industrial case study.
formation and Software Technology, 57:664 – 682.

David Gros, Hariharan Sezhiyan, Prem Devanbu, and
Zhou Yu. 2020. Code to comment ”translation”:
Data, metrics, baselining & evaluation. In Proceed-
ings of the 35th IEEE/ACM International Confer-
ence on Automated Software Engineering, ASE ’20,
page 746–757, New York, NY, USA. Association for
Computing Machinery.

Sonia Haiduc, Jairo Aponte, and Andrian Marcus.
Supporting program comprehension with
ICSE ’10, New York,

2010.
source code summarization.
NY, USA. Association for Computing Machinery.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural Comput.,

Long short-term memory.
9(8):1735–1780.

Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018.
In Proceedings
Deep code comment generation.
of the 26th Conference on Program Comprehension,
ICPC ’18, page 200–210, New York, NY, USA. As-
sociation for Computing Machinery.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. Code-
searchnet challenge: Evaluating the state of seman-
tic code search. CoRR, abs/1909.09436.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
Luke Zettlemoyer. 2016. Summarizing source code
In Proceedings
using a neural attention model.
of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 2073–2083, Berlin, Germany. Association for
Computational Linguistics.

Philipp Koehn. 2004.

Statistical signiﬁcance tests
In Proceed-
for machine translation evaluation.
ings of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing, pages 388–
395, Barcelona, Spain. Association for Computa-
tional Linguistics.

Amy N. Langville and Carl D. Meyer. 2006. Google’s
PageRank and Beyond: The Science of Search En-
gine Rankings. Princeton University Press, USA.

Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
In Proceed-
of correlation with human judgments.
ings of the Second Workshop on Statistical Machine
Translation, StatMT ’07, page 228–231, USA. Asso-
ciation for Computational Linguistics.

Alexander LeClair, Sakib Haque, Lingfei Wu, and
Collin McMillan. 2020. Improved code summariza-
tion via a graph neural network. In Proceedings of
the 28th International Conference on Program Com-
prehension, ICPC ’20, page 184–195, New York,
NY, USA. Association for Computing Machinery.

Alexander LeClair, Siyuan Jiang, and Collin McMil-
lan. 2019. A neural model for generating natural
In
language summaries of program subroutines.
Proceedings of the 41st International Conference
on Software Engineering, ICSE ’19, page 795–806.
IEEE Press.

Alexander LeClair and Collin McMillan. 2019. Rec-
ommendations for datasets for source code summa-
rization. CoRR, abs/1904.02660.

Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out, pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.

Matthew B. Miles, A. Michael Huberman, and Johnny
Salda˜na. 2013. Qualitative data analysis: a methods
sourcebook. Thousand Oaks, Califorinia: SAGE
Publications, Inc., [2014] ©2014.

Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics
In The 20th International
for machine translation.
Conference on Computational Linguistics (COLING
2004), pages 501–507. COLING.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. ACL ’02, page
311–318, USA. Association for Computational Lin-
guistics.

Mario Linares-V´asquez, Gabriele Bavota, Michele Tu-
fano, Kevin Moran, Massimiliano Di Penta, Christo-
pher Vendome, Carlos Bernal-C´ardenas, and Denys
Poshyvanyk. 2017. Enabling mutation testing for an-
droid apps. In Proceedings of the 2017 11th Joint
Meeting on Foundations of Software Engineering,
ESEC/FSE 2017, page 233–244, New York, NY,
USA. Association for Computing Machinery.

Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow,
and Yang Liu. 2021. Retrieval-augmented genera-
tion for code summarization via hybrid {gnn}.
In
International Conference on Learning Representa-
tions.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining ap-
proach. CoRR, abs/1907.11692.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-
dong Zhou, Linjun Shou, Long Zhou, Michele Tu-
fano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-
daresan, Shao Kun Deng, Shengyu Fu, and Shujie
Liu. 2021. Codexglue: A machine learning bench-
mark dataset for code understanding and generation.

Antonio Mastropaolo, Simone Scalabrino, Nathan
Cooper, David Nader Palacio, Denys Poshyvanyk,
Rocco Oliveto, and Gabriele Bavota. 2021. Study-
ing the usage of text-to-text transfer transformer to
support code-related tasks.

P. W. McBurney and C. McMillan. 2016. Automatic
source code summarization of context for java meth-
IEEE Transactions on Software Engineering,
ods.
42(2):103–119.

Paul W. McBurney and Collin McMillan. 2014. Au-
tomatic documentation generation via source code
In Proceedings
summarization of method context.
of the 22nd International Conference on Program
Comprehension, ICPC 2014, page 279–290, New
York, NY, USA. Association for Computing Machin-
ery.

Haitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe
Ittycheriah. 2016. Coverage embedding models for
In Proceedings of the
neural machine translation.
2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 955–960, Austin,
Texas. Association for Computational Linguistics.

Abdallah Qusef, Rocco Oliveto, and Andrea De Lucia.
2010. Recovering traceability links between unit
tests and classes under test: An improved method.
In 2010 IEEE International Conference on Software
Maintenance, pages 1–10.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers).

Sean Stapleton, Yashmeet Gambhir, Alexander LeClair,
Zachary Eberhart, Westley Weimer, Kevin Leach,
and Yu Huang. 2020. A human study of comprehen-
sion and code summarization. In Proceedings of the
28th International Conference on Program Compre-
hension, ICPC ’20, page 2–13, New York, NY, USA.
Association for Computing Machinery.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, undeﬁne-
dukasz Kaiser, and Illia Polosukhin. 2017. Atten-
tion is all you need. NIPS’17, page 6000–6010, Red
Hook, NY, USA. Curran Associates Inc.

Yao Wan, Zhou Zhao, Min Yang, Guandong Xu,
Haochao Ying, Jian Wu, and Philip S. Yu. 2018. Im-
proving automatic source code summarization via
In Proceedings of
deep reinforcement learning.
the 33rd ACM/IEEE International Conference on
Automated Software Engineering, ASE 2018, page
397–407, New York, NY, USA. Association for
Computing Machinery.

Annie T. T. Ying and Martin P. Robillard. 2013. Code
fragment summarization. ESEC/FSE 2013, page
655–658, New York, NY, USA. Association for
Computing Machinery.

Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun,
and Xudong Liu. 2020a. Retrieval-based neural
source code summarization. In Proceedings of the
ACM/IEEE 42nd International Conference on Soft-
ware Engineering, ICSE ’20, page 1385–1397, New
York, NY, USA. Association for Computing Machin-
ery.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020b. Bertscore:
Evaluating text generation with bert.

Junji Zhi, Vahid Garousi-Yusiﬂu, Bo Sun, Golara
Garousi, Shawn Shahnewaz, and Guenther Ruhe.
2015. Cost, beneﬁts and quality of software devel-
opment documentation. J. Sys. Sof.

Yuxiang Zhu and Minxue Pan. 2019. Automatic
code summarization: A systematic literature review.
CoRR, abs/1909.04352.

A Hyper-parameters

In Table 4, we show the hyper-parameters that are
used in our adapted models. Code2seq model could
not be trained using batch size 64 or 128 because of
the instability occurred from the longer comment
length. Originally, this model was designed to pre-
dict the method name. So we trained the model
using batch size 512 in our ﬁnal experiment and it
required 39 epochs to train the model.

Hyper-
parameters

Maximum
Source Length
Batch Size
Beam Size
Optimizer
Learning Rate

#epochs
Dropout rate
#Attention
heads
Early stopping
#layers

CodeBERT Neural-

Code2Seq

CodeSum

256

150

200

16
16
Adam
0.00005

15
0.1
12

True
6

64
4
Adam
0.0001

38
0.2
8

True
6

512
0
Momentum
0.01+exp.
decay
39
0.25
–

True
–

Table 4: Model Hyperparameters

B Data Prepossessing

We had to perform several preprocessing steps
to make the dataset ready for training. Among
all the three models, we removed comments in-
side methods, removed tags, clean HTML, low-
ercasing characters, removing special characters.
For the NeuralCodeSum model, we applied an
additional sub-tokenization step. For code2seq,
we needed to prepare the AST representation
of the code snippets. To do this, we used a
modiﬁed JavaExtractor13 which locates the
Java methods and put them in a ﬁle where each
line is for one method. Subtokenization is per-
formed in between to tokenize the CamelCase
attributes (i.e. ["ArrayList"->["Array",
"List"]]). The original dataset build script was
designed to put the method name in the prediction
window. The modiﬁed one puts the comment in-
stead of a method name. In Table 5, a Java code,
comment and the equivalent one line dataset in-
stance (AST representation) is presented. While
performing this step, some methods could not be
parsed as this AST representation mainly because
of the minimum method length threshold required
for the parsing. In total, we could transform 80.02%
of our training dataset on which we trained the
code2seq model. All the steps used in preprocess-
ing are shown in Table 6.

C Case Study

In Table 7, model predictions are given with the
ground truth and assigned error categories.

13https://github.com/LRNavin/

AutoComments

Original Method
Comment
AST represtation

public Type getType() { return m type; }
returns the type of this technical information
returns|the|type|of|this|technical|information
type,Cls0|Mth|Nm1,get|type type,Cls0|Mth|Bk|Ret|Nm0,m|type
get|type,Nm1|Mth|Bk|Ret|Nm0,m|type

Table 5: AST representation of java method for code2seq training

Preprocessing

CodeBERT Neural-

Code2Seq

CodeSum

removed comments
inside methods
removed tags for
comments
and
methods
HTML cleaning
Sub-tokenization
Lowercase
removing
characters

special

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

Table 6: Preprocessing

-
r
u
c

e
h
t

f
o

y
t
i
c
o
l
e
v

t
n
e
r
r
u
c

e
h
t

t
e
g

-
r
a
P
(

t
n
i
o
p
l
a
e
d

f
o

e
s
a
h
p

e
h
t

s
n
r
u
t
e
r

t
u
b

t
n
e
t
s
i
s
n
o
C

(

e
s
a
h
p
h
s
a
d

e
h
t

s
t
e
g

e
h
t

f
o

e
s
a
h
p

h
s
a
d

e
h
t

s
t
e
g

q
e
S
2
e
d
o
C

)
y
r
o
g
a
t
a
C

(
n
o
i
t
c
i
d
e
r
P

m
u
S
e
d
o
C
l
a
r
u
e
N

t
r
e
B
e
d
o
C

t
n
e
m
m
o
c
n
e
t
t
i
r
w
n
a
m
u
H

t
a
o
l
f

c
i
l
b
u
p

d
o
h
t
e

M
a
v
a
J

d
e
t
a
l
e
r
n
U
y
l
l
a
c
i
t
n
a
m
e
S
(

e
u
l
a
v

t
n
e
r

)
n
o
i
t
a
m
r
o
f
n
I

t
c
e
r
r
o
c
n
I

l
a
i
t

)
o
f
n
I

c
ﬁ
i
c
e
p
S
g
n
i
s
s
i

M

e
k
o
r
t
s
c
i
s
a
b

n
r
u
t
e
r

{

)
(
e
s
a
h
P
h
s
a
D
t
e
g

)
e
d
o
C
o
t

e
h
t

m
o
r
f

d
a
e
r

o
t

t
u
o
e
m

i
t

e
h
t

s
t
e
s

o
t

s
d
n
o
c
e
s
i
l
l
i

m

f
o

r
e
b
m
u
n

e
h
t

s
t
e
s

y
l
n
O
g
n
i
s
u
c
o
F
(

t
u
o
e
m

i
t
d
a
e
r
e
h
t

s
t
e
s

-
l
i

m
n
i
e
u
l
a
v
t
u
o
e
m

i
t
e
h
t

s
t
e
s

-
e
p
S
g
n
i
s
s
i

M

t
u
b
t
n
e
t
s
i
s
n
o
C

(

r
e
v
r
e
s

t
u
o
g
n
i
m

i
t
e
r
o
f
e
b
e
s
n
o
p
s
e
r
a
r
o
f

t
i
a
w

)
e
m
a
N
d
o
h
e
M
n
o

m
o
r
f

g
n
i
d
a
e
r

r
o
f

s
d
n
o
c
e
s
i
l

)
o
f
n
I

c
ﬁ
i
c

)
o
f
n
I

c
ﬁ
i
c
e
p
S
h
t
i

w

t
n
e
t
s
i
s
n
o
C

(

m
a
e
r
t
s

t
u
p
n
i

e
h
t

-
r
u
c

e
h
t

f
o

t
x
e
t

e
h
t

f
o

t
x
e
t

e
h
t

s
n
r
u
t
e
r

e
h
t

n
i

s
l
e
b
a
l

f
o

r
e
b
m
u
n

e
h
t

s
n
r
u
t
e
r

f
o

e
t
u
b
i
r
t
t
a

t
x
e
t
p
i
t
s
e
l
u
r
m
u
n
e
h
t

s
t
e
g

s
i
h
t

r
o
f

t
x
e
t

p
i
t

e
h
t

s
n
r
u
t
e
r

)
n
o
i
t
i
t
e
p
e
R

(

t
x
e
t

t
n
e
r

o
t

d
e
t
a
l
e
r
n
U
y
l
l
a
c
i
t
n
a
m
e
S
(

t
e
s

e
l
u
r

y
r
a
s
s
e
c
e
n
n
U

(

t
c
e
j
b
o
e
l
a
c
o
l
t
e
l
p
p
a
e
h
t

y
t
r
e
p
o
r
p

)
e
d
o
C

)
n
o
i
t
a
m
r
o
f
n
I

t
c
e
r
r
o
c
n
I

n
e
v
i
g

e
h
t

r
o
f

s
t
l
u
s
e
r

l
a
t
o
t

e
h
t

s
n
r
u
t
e
r

s
m
e
t
i

f
o

r
e
b
m
u
n

l
a
t
o
t

e
h
t

s
t
e
s

h
t
i

w

t
n
e
t
s
i
s
n
o
C

(

s
t
l
u
s
e
r

l
a
t
o
t
e
h
t

s
t
e
s

-
e
r
l
a
t
o
t

e
h
t

f
o
e
u
l
a
v
e
h
t

s
t
e
s

s
e
l
u
r

f
o

r
e
b
m
u
N
"

n
r
u
t
e
r

{

)
(
t
x
e
T
p
i
T
s
e
l
u
R
m
u
n

}

;
"
.
d
n
i
f

o
t

s
m
e
t
I

c
i
l
b
u
p

t
n
i
(
t
u
o
e
m
i
T
d
a
e
R
t
e
s

=

t
u
o
e
m
i
T
d
a
e
r
.
s
i
h
t

{

f
i

{

)
t
u
o
e
m
i
t

)
t
u
o
e
m
i
t

>

0
(

g
n
i
r
t
S

c
i
l
b
u
p

}

;
t
u
o
e
m
i
t

}
;
e
s
a
h
P
h
s
a
d

d
i
o
v

c
i
l
b
u
p

)
e
t
u
b
i
r
t
t

A

/
r
e
ﬁ

-
i
t
e
p
e
R

(

a
m
e
h
c
s

a
m
e
h
c
s

e
h
t

s
n
r
u
t
e
r

a
m
e
h
c
s

l

m
x

e
h
t

f
o

e
m
a
n

e
h
t

s
n
r
u
t
e
r

t
u
b

t
n
e
t
s
i
s
n
o
C

(

a
m
e
h
c
s

e
h
t

s
n
r
u
t
e
r

)
n
o
i
t

)
n
o
i
t
a
m
r
o
f
n
I

l
a
c
i
t
i
r

C
-
n
o
N
g
n
i
s
s
i

M

(

)
o
f
n
I

c
ﬁ
i
c
e
p
S
g
n
i
s
s
i

M

l

m
x

e
h
t

o
t

h
t
a
p

a

s
n
r
u
t
e
r

t
n
i
o
p

n
o
i
s
n
e
t
x
e

a

f
o

a
m
e
h
c
s

d
i

t
l
u
a
f
e
d
e
h
t

s
i

e
h
t

r
e
h
t
e
h
w
s
n
r
u
t
e
r

-
r
o
f
n
I
x
a
t
n
y
S
g
n
i
s
s
i

M

(
}
e
h
t

s
n
r
u
t
e
r

-
r
o
c
n
I

l
a
i
t
r
a
P
(

s
d
i
e
t
a
e
r
c

e
h
t

s
n
r
u
t
e
r

-
u
a

l
e
d
o
m
e
h
t

f
i

e
u
r
t

s
n
r
u
t
e
r

)
n
o
i
t
a
c
ﬁ
i
c
e
p
S
e
t
u
b
i
r
t
t

A
g
n
i
s
s
i

M

(

)
n
o
i
t
a
m

)
n
o
i
t
a
m
r
o
f
n
I

t
c
e
r

d
n
a

s
d
i

s
e
t
a
e
r
c

y
l
l
a
c
i
t
a
m
o
t

s
n
o
i
s
i
l
l
o
c

d
i

s
e
v
l
o
s
e
r

-
c
e
p
S
e
t
u
b
i
r
t
t

A
g
n
i
s
s
i

M

(

r
a
v

e
h
t

s
t
e
s

y
n
a

y
t
r
e
p
o
r
p

s
i
h
t

f
o

e
u
l
a
v

e
h
t

s
t
e
s

-
n
a
e

M

t
n
e
r
e
f
f
i

D

(

y
t
i
t
n
a
u
q

a

s
n
r
u
t
e
r

y
t
i
t
n
a
u
q
e
h
t

f
o
e
u
l
a
v
e
h
t

s
t
e
s

)
(
a
m
e
h
c
S
t
e
g

}
;
s
i
h
t

g
n
i
r
t
S

n
r
u
t
e
r

c
i
l
b
u
p

;
)
e
u
l
a
v
(
s
t
l
u
s
e
R
l
a
t
o
T
t
e
s

n
r
u
t
e
r

{

)
(
s
d
I
e
t
a
e
r
C
s
i

}

;
a
m
e
h
c
S
f

n
r
u
t
e
r

{

n
a
e
l
o
o
b

c
i
l
b
u
p

{

)
e
u
l
a
v

}

;
s
d
I
e
t
a
e
r
c

c
i
l
b
u
p

)
n
o
i
t
a
m
r
o
f
n
I

l
a
c
i
t
i
r

C
g
n
i
s
s
i

M

(

r
a
v

-
i
t
n
e
d
I

t
c
e
r
r
o
c
n
I
(

p
u
o
r
g

e
h
t

n
i

)
o
f
n
I

c
ﬁ
i
c
e
p
S

y
t
r
e
p
o
r
p

s
t
l
u
s

r
e
g
e
t
n
I
g
i
B
(
s
t
l
u
s
e
R
l
a
t
o
T
h
t
i
w

)
n
o
i
t
a
c
ﬁ
i

d
e
p
p
o
t
s

e
r
a

s
n
o
i
t
a
m
i
n
a

s
u
o
i
v
e
r
p

)
n
o
i
t
a
m
r
o
f
n
I

t
c
e
r
r
o
c
n
I
y
r
a
s
s
e
c
e
n
n
U

(

)
g
n
i

y
t
r
e
p
o
r
p

r
i
u
q
e
R
y
t
i
l
i
b
i
g
i
l
E
n
o
i
t
o
m
o
r
P

r
e
g
e
t
n
I
(
y
t
i
t
n
a
u
Q
h
t
i
w

t
n
e
m
e

-
r
o
c
n
I
(

e
g
a
m

i

e
h
t

f
o

t
h
g
i
e
h

e
h
t

)
e
t
u
b
i
r
t
t

A

/
r
e
ﬁ
i
t
n
e
d
I

s
t
e
s

t
c
e
r

-
r
o
c
n
I
(

e
g
a
m

i

e
h
t

f
o

t
h
g
i
e
h

e
h
t

)
e
t
u
b
i
r
t
t

A

/
r
e
ﬁ
i
t
n
e
d
I

s
t
e
s

t
c
e
r

-
n
o
C

(

r
e
n
i
a
t
n
o
c

e
h
t

f
o
t
h
g
i
e
h
e
h
t

s
t
e
s

-
i
n
i
m

e
h
t

s
t
e
s

d
o
h
t
e
m

s
i
h
t

)
o
f
n
I

c
ﬁ
i
c
e
p
S
g
n
i
s
s
i

M

t
u
b

t
n
e
t
s
i
s

n
i

e
l
b
a
t

e
h
t

f
o

t
h
g
i
e
h
m
u
m

y
t
r
e
p
o
r
p

e
h
t

f
o

r
e
t
t
e
g

e
h
t

r
o
f

t
s
e
t

r
e
v
r
e
s

e
h
t

o
t

n
o
i
t
c
e
n
n
o
c

e
h
t

s
t
s
e
t

t
u
b

t
n
e
t
s
i
s
n
o
C

(

n
o
i
t
c
e
n
n
o
c

a

r
o
f

t
s
e
t

)
n
o
i
t
a
c
ﬁ
i
c
e
p
S
e
t
u
b
i
r
t
t

A
g
n
i
s
s
i

M

(

c
ﬁ
i
c
e
p
S

g
n
i
s
s
i

M

t
u
b

t
n
e
t
s
i
s
n
o
C

(

)
o
f
n
I

c
ﬁ
i
c
e
p
S
g
n
i
s
s
i

M

)
o
f
n
I

n
o
i
t
c
e
n
n
o
c

a

r
o
f

t
s
e
t

o
t

y
r
t

e
l
b
a
n
u

f
i

n
o
i
t
p
e
c
x
e

s
w
o
r
h
t

n
o
i
t
c
e
n
n
o
c

a

t
e
g

o
t

s
l
e
x
i
p

h
t
a
p

e
h
t

f
o

h
t
a
p

e
h
t

o
t

h
t
a
p

e
h
t

s
t
e
s

g
n
i
s
s
i

M

t
u
b
t
n
e
t
s
i
s
n
o
C

(
h
t
a
p
e
h
t

s
t
e
s

g
n
i
s
s
i

M

t
u
b
t
n
e
t
s
i
s
n
o
C

(
h
t
a
p
e
h
t

s
t
e
s

”
h
t
a
p
”

e
h
t

f
o

e
u
l
a
v

e
h
t

s
t
e
s

e
h
t

f
o

h
t
a
p

e
h
t

o
t

e
v
i
t
a
l
e
r

t
o
n

e
r
a

)
n
o
i
t
i
t
e
p
e
R
e
m
e
r
t
x
E
(

h
t
a
p

)
o
f
n
I

c
ﬁ
i
c
e
p
S

)
o
f
n
I

c
ﬁ
i
c
e
p
S

e
t
u
b
i
r
t
t
a

h
t
u
r
t

d
n
u
o
r
g

h
t
i

w
s
n
o
i
t
c
i
d
e
r
p

l
e
d
o
m

f
o

y
d
u
t
s

e
s
a
c

d
e
l
i
a
t
e
D

:
7

e
l
b
a
T

t
n
i
(
t
h
g
i
e
H
t
e
s

d
i
o
v

}

;
s
i
h
t

n
r
u
t
e
r

c
i
l
b
u
p

;
)
e
u
l
a
v
(
y
t
i
t
n
a
u
Q
t
e
s
{

t
h
g
i
e
H
r
e
n
i
a
t
n
o
c

}

{

)
t
h
g
i
e
h

;
t
h
g
i
e
h

=

h
t
a
P
(
h
t
a
P
t
e
s

d
i
o
v

c
i
l
b
u
p

}

;
)
(
g
n
i
p
.
l
o
r
t
n
o
C
r
e
v
r
e
s

}

;
)
(
g
n
i
r
t
S
o
t
.
h
t
a
p

=

h
t
a
P
m

{

)
h
t
a
p

)
(
n
o
i
t
c
e
n
n
o
C
r
o
F
t
s
e
t

{

n
o
i
t
p
e
c
x
E

s
w
o
r
h
t

d
i
o
v

c
i
l
b
u
p

)
e
u
l
a
v

