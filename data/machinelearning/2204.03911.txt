manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

A posteriori learning for quasi-geostrophic turbulence
parametrization

Hugo Frezat1,2,3, Julien Le Sommer2, Ronan Fablet3, Guillaume Balarac1,4and

Redouane Lguensat5

1Univ. Grenoble Alpes, CNRS UMR LEGI, Grenoble, France

2Univ. Grenoble Alpes, CNRS UMR IGE, Grenoble, France

3IMT Atlantique, CNRS UMR Lab-STICC, Brest, France

4Institut Universitaire de France (IUF), Paris, France

5Institut Pierre Simon Laplace, IRD, Sorbonne Universit´e, Paris, France

Key Points:

• Subgrid parametrizations can be learned end-to-end with a posteriori criteria in-

volving model integration over several time-steps.

• Application of end-to-end learning to quasi-geostrophic turbulent ﬂows solves nu-

merical stability issues related to energy backscatter.

• Learned parametrizations outperform existing baselines for various evaluation met-

rics and apply to diﬀerent ﬂow conﬁgurations.

2
2
0
2

r
p
A
8

]
n
y
d
-
u
l
f
.
s
c
i
s
y
h
p
[

1
v
1
1
9
3
0
.
4
0
2
2
:
v
i
X
r
a

Corresponding author: Hugo Frezat, hugo.frezat@univ-grenoble-alpes.fr

–1–

 
 
 
 
 
 
manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

Abstract

The use of machine learning to build subgrid parametrizations for climate models is re-

ceiving growing attention. State-of-the-art strategies address the problem as a supervised

learning task and optimize algorithms that predict subgrid ﬂuxes based on information

from coarse resolution models. In practice, training data are generated from higher res-

olution numerical simulations transformed in order to mimic coarse resolution simula-

tions. By essence, these strategies optimize subgrid parametrizations to meet so-called

a priori criteria. But the actual purpose of a subgrid parametrization is to obtain good

performance in terms of a posteriori metrics which imply computing entire model tra-

jectories. In this paper, we focus on the representation of energy backscatter in two di-

mensional quasi-geostrophic turbulence and compare parametrizations obtained with dif-

ferent learning strategies at ﬁxed computational complexity. We show that strategies based

on a priori criteria yield parametrizations that tend to be unstable in direct simulations

and describe how subgrid parametrizations can alternatively be trained end-to-end in

order to meet a posteriori criteria. We illustrate that end-to-end learning strategies yield

parametrizations that outperform known empirical and data-driven schemes in terms of

performance, stability and ability to apply to diﬀerent ﬂow conﬁgurations. These results

support the relevance of diﬀerentiable programming paradigms for climate models in the

future.

Plain Language Summary

Climate projection and weather forecast heavily rely on computer simulations. But,

if the physical laws governing the evolution of the climate system are well known, their

simulation is still rather challenging. Fluid ﬂows being essentially turbulent, small de-

tails at ﬁne scale can have a tremendous impact on larger scales. Still, because of the

limitations in computing power, all these interactions across scales cannot be explicitly

resolved in computer simulations. Some of these interactions can only be represented ap-

proximately, and the design of these approximations is an active research area. Here we

describe a new method which leverages recent advances in artiﬁcial intelligence. We pro-

pose to run a small scale model jointly with climate models during the learning phase

in order to reduce their errors. Our method shows very promising results in toy exam-

ple ﬂow simulations, but its deployment at scale may seriously challenge the overall de-

sign of legacy climate models.

–2–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

1 Introduction

The representation of unresolved processes is a key source of uncertainty in weather

and climate models. Climate science and weather forecasting indeed heavily rely on nu-

merical simulations of the Earth’s atmosphere and oceans (Bauer et al., 2015; Neumann

et al., 2019). But even the most advanced applications are currently far from resolving

explicitly the wide variety of space-time scales and physical processes involved. This will

likely remain the case for the foreseeable future because of the non-linearity of ﬂuid dy-

namics and thermodynamics, and because of the ﬁnite nature of computational resources

(Schneider, Teixeira, et al., 2017; Fox-Kemper et al., 2014). Weather and climate mod-

els will therefore keep relying on approximated representations of the eﬀect of unresolved

processes in the form of subgrid parametrization schemes (Schneider, Lan, et al., 2017;

Fox-Kemper et al., 2019). Parametrization schemes accounting for the impact of turbu-

lence in the atmosphere and oceans at various scales will in particular remain essential

components of these models.

Parametrizations of unresolved turbulent motions are usually based on ﬁrst prin-

ciples, physics, idealized experiments, ﬁeld observations and high resolution simulations.

Their design involves a mixture of empirical and process-based modeling. Process-based

models are formulated, tested and calibrated with experiments performed in the ﬁeld,

in the laboratory or with computers (Stensrud, 2009). On this basis, the actual parametriza-

tion scheme estimates a tendency term for the target model from its resolved variables.

The underlying conceptual framework can rely on some ensemble averaging procedure,

so that the parametrization intends to capture the bulk statistical eﬀect of unresolved

processes, as for instance for turbulence models (Mellor, 1985). Alternatively, one may

consider a spatial ﬁltering procedure and the parametrization can then exploit the scale-

invariant properties of turbulence, as in Large Eddy Simulation (LES) models (Lesieur

et al., 2005). This latter framework is used for instance for the parametrizations of ocean

macro-turbulence in eddy-rich ocean models (Fox-Kemper & Menemenlis, 2008).

Recently, the use of machine learning (ML) for better parametrizing unresolved pro-

cesses in weather and climate models has gained momentum. Calibrating physics-based

parametrization schemes against observation with ML and emulators is for instance be-

coming common practice (Ollinaho et al., 2013; Schneider, Lan, et al., 2017; Couvreux

et al., 2021). Emulation approaches based on ML have also been proposed as a strat-

–3–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

egy for accelerating or regularizing existing schemes (Ukkonen et al., 2020; Chantry et

al., 2021; Meyer et al., 2021). ML also provides new means to design new subgrid parametriza-

tion schemes from high ﬁdelity simulations. In this context, ML may learn a mapping

which predicts the tendency term due to unresolved subgrid eﬀects from resolved quan-

tities available in a target model. In atmospheric models, these approaches have been

used to improve the representation of cloud micro-physics and moist processes (Krasnopolsky

et al., 2013; Rasp et al., 2018; O’Gorman & Dwyer, 2018; Brenowitz & Bretherton, 2018;

Seifert & Rasp, 2020). In ocean models, it is expected that the representation of macro-

turbulence could be improved with similar approaches (Bolton & Zanna, 2019; Zanna

& Bolton, 2021; Guillaumin & Zanna, 2021).

The design of parametrizations with ML builds on the rise of scientiﬁc machine learn-

ing and its broad application to physical sciences. Scientiﬁc machine learning is an emerg-

ing ﬁeld, which bridges scientiﬁc computing and machine learning. Some recent key de-

velopments in this ﬁeld have been motivated both from physical insights and for their

applications to physical sciences, especially in ﬂuid dynamics (Carleo et al., 2019; Thuerey

et al., 2021). The conceptual developments in ML motivated by applications to prob-

lems governed by partial diﬀerential equations (Long et al., 2018; Sirignano & Spiliopou-

los, 2018; Raissi et al., 2019) have for instance gradually freed ML from its black-box rep-

utation. The design of parametrization schemes now directly beneﬁts from ML approaches

for dynamical system identiﬁcation and equation discovery (Brunton et al., 2016; Zanna

& Bolton, 2020). The ability to embed symmetries and law invariances into neural net-

works (Cohen & Welling, 2016; Cranmer et al., 2020; Alet et al., 2021) will also likely

be important in the design of parametrization schemes (Frezat et al., 2021), and in ap-

plications of ML to ﬂuid mechanics in general (Brunton et al., 2020; Vinuesa & Brun-

ton, 2021).

But ML-based approaches to subgrid parametrizations are still mostly based on

a priori learning strategies, which could limit their performance and applicability. There

are indeed two diﬀerent sorts of evaluation metrics for measuring the precision of sub-

grid models in turbulent simulations (Pope, 2000). A priori metrics, on the one hand,

measure to what extent a given subgrid model is able to predict a tendency term due

to unresolved subgrid eﬀects at a ﬁxed time. A posteriori metrics, on the other hand,

require to perform simulations with the subgrid model, and measure its integrated im-

pact on the simulated ﬂows. The common strategy for learning subgrid parametrizations

–4–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

is to formulate a supervised learning task from a high-resolution reference simulation dataset.

In practice, learned parametrizations result from the minimization of a cost function based

on some a priori metrics measuring how well a mapping can predict unresolved ﬂuxes

from coarse-grain quantities. Still, with such strategy, what we really intend to optimize

is the ability of the parametrization to yield good solutions, when used a posteriori in

numerical simulations. In principle, the versatility of ML algorithms should allow us to

train parametrization schemes with learning criteria based on a posteriori metrics, adopt-

ing the so-called end-to-end learning framework (Glasmachers, 2017). But surprisingly,

there are very few published examples of end-to-end learning strategies in computational

ﬂuid dynamics (Sirignano et al., 2020; Kochkov et al., 2021; Stachenfeld et al., 2021).

It is therefore yet unclear how subgrid parametrizations trained with a priori and a pos-

teriori compare in terms of performance, stability and ability to apply to diﬀerent ﬂow

conditions.

Flows governed by quasi-geostrophic (QG) dynamics provide an interesting and chal-

lenging testbed to evaluate learning strategies for subgrid parametrizations. Quasi-geostrophic

theory indeed proposes a simple framework for studying geophysical ﬂows constrained

by earth rotation and stratiﬁcation, as for instance large scale atmospheric dynamics and

ocean macro-turbulence (Cushman-Roisin & Beckers, 2011). The two-dimensional tur-

bulence emerging from barotropic QG dynamics (Boﬀetta & Ecke, 2012; Majda & Wang,

2006) exhibits a dual cascade scenario with inverse energy transfers to larger scales and

direct enstrophy transfers to smaller scales (Kraichnan, 1967; Thuburn et al., 2014). Be-

cause of this inverse energy cascade, developing subgrid parametrizations for QG ﬂows

is a challenging task, as the stability of numerical integration schemes is directly con-

trolled by the rate of energy backscatter from small to large scales (Lilly, 1992; Carati

et al., 1995). As a consequence, a large number of subgrid parametrization schemes have

been proposed for two-dimensional turbulence (see e.g. Danilov et al. (2019) for a re-

view) and well documented ﬂow conﬁgurations with performance metrics for parametriza-

tion are readily available (Graham & Ringler, 2013). Unsurprisingly, attempts to learn

subgrid parametrizations for two-dimensional turbulence with ML have been less suc-

cessful than for other types of turbulent ﬂows (Maulik et al., 2019; Guan et al., 2022).

In particular, ad-hoc solutions had to be implemented in order to ensure the numerical

stability of the learned schemes under energy backscatter conditions. For instance, Maulik

et al. (2019) uses a clipping post-processing procedure to remove negative diﬀusivity while

–5–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

Guan et al. (2022) mitigates this problem in decaying turbulence by increasing the size

of the training dataset. Up to now, however, none of the published works investigates

the long-term statistical performance of learned schemes far beyond the decorrelation

horizon. Learning stable parametrizations for two-dimensional turbulence in QG ﬂows

is therefore still an open problem.

In this work, we compare parametrizations for two-dimensional turbulence obtained

with diﬀerent learning strategies, at ﬁxed computational complexity. In particular, we

show that we are able to train a model based on a posteriori metrics with an end-to-end

learning strategy. Through evaluation on three diﬀerent conﬁgurations (decay, wind-forcing

and beta-eﬀect), the end-to-end learning strategy is shown to yield stable parametriza-

tions that outperform previous physics-based and NN-based models without any explicit

postprocessing step. Statistical metrics on long-term spectral transfers are shown to be

in excellent agreement to direct numerical simulations (DNS), which is particularly en-

couraging for future climate models. The paper is organized as follows: In Sec. 2, we present

the a priori and a posteriori learning strategies and the type of metrics they are respec-

tively able to optimize. The application to quasi-geostrophic parametrizations is described

in Sec. 3 with the numerical setup and baselines used in the evaluation. Results are pre-

sented both for short-term and long-term statistics for three diﬀerent conﬁgurations in

Sec. 4. Finally, we discuss the limitations and implications of the described strategies

for realistic large-scale solvers.

2 Learning strategies

In this study, we address the simulation of the time evolution of geophysical quan-

tities y(t). We assume the underlying governing equations to be known. Let us denote

by f (y) these true dynamics. The numerical integration of this system being either im-

possible or expensive, we aim at solving the time evolution of reduced variables ¯y(t) such

that






∂y
∂t

∂ ¯y
∂t

= f (y),

y ∈ Ω

= g(¯y) + M(¯y),

¯y ∈ ¯Ω

(1)

T (y) = ¯y

–6–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

where ¯Ω ⊂ Ω, g a reduced-order operator, M a subgrid-scale parametrization and T

is a projection operator that maps true variables to reduced ones. The objective in reduced-

order modeling is to design operator g such that the evolution of the reduced variables

match the projection T (y) of the true variables y. We note that for some reduced or-

der problems, we identify f = g with variables existing on diﬀerent spaces or dimen-

sionalities.

Within a learning framework, one states the identiﬁcation of subgrid-scale term R(y) =

T (f (y))−g(T (y)) as a learning problem from reduced variables for a parametrization

M(¯y|θ) where θ are trainable model parameters. Under the assumption that projection

operator T commutes with partial derivatives, the most classic approach comes to train

parametrization M(¯y|θ) as a functional approximation of closure term R. This approach

has been widely explored in the recent literature (Vollant et al., 2017; Bolton & Zanna,

2019). It does not however constrain the trained parametrization to behave as expected

when implemented in the solver of the reduced-order system. In this respect, an end-to-

end framework would appear as an appealing approach to explicitly state the subgrid-

scale parametrization problem according to the best possible approximation of the true

reduced variables. Such end-to-end approaches have shown many advantages in the ap-

proximation of diﬀerential equation in general (Chen et al., 2018; Bakarji & Tartakovsky,

2021; Fablet et al., 2021). When applied to physical problems, they are often referred

as diﬀerentiable physics (de Avila Belbute-Peres et al., 2018; Um et al., 2020; Holl et al.,

2020), since they require the gradient of all the considered operators and solvers to be

available for the optimization algorithm. Overall, these two categories of learning ap-

proaches diﬀer in the space where the training is performed, similarly to the deﬁnition

of a priori and a posteriori metrics (Pope, 2000) for the benchmarking of SGS parametriza-

tions. This is the reason why we refer to a priori and a posteriori learning strategies as

detailed in the subsequent.

2.1 a priori learning

The a priori learning strategy comes to learn SGS parametrization using training

metrics deﬁned on instantaneous quantities, i.e. a direct measure of the accuracy of the

model based on the predicted SGS term R(y). The a priori loss Lprio has the form,

Lprio(M) := (cid:96)(R(y), M(¯y|θ))

(2)

–7–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

where M is a given SGS model to be evaluated. The most common a priori metrics (cid:96)

found in the ﬂuid dynamics community are the mean squared error (MSE) and the cor-

relation between true and predicted SGS terms. Training a NN-based parametrization

according to a a priori setting then comes to build a representative ground-truth dataset

{R(yi), ¯yi}n of paired SGS terms and reduced variables to solve the following minimiza-

tion problem w.r.t. model parameters θ

arg min
θ

Lprio(M) ≡ arg min

θ

(cid:96)({R(yi)}, M({¯yi}|θ)).

(3)

Solving for (3) requires evaluation of the partial derivative of the a priori loss Lprio with

respect to parameter θ, which only involves the gradient of M,

∂Lprio
∂θ

=

∂Lprio
∂R

∂R
∂θ

+

∂Lprio
∂M

∂M
∂θ

=

∂Lprio
∂M

∂M
∂θ

.

(4)

This approach has been applied to scalar (Vollant et al., 2017; Portwood et al., 2021; Frezat

et al., 2021) and momentum (Gamahara & Hattori, 2017; Beck et al., 2019; Xie et al.,

2020; Yuan et al., 2020) parametrizations of three-dimensional turbulence on diﬀerent

conﬁgurations. The two-dimensional case is also well documented in decaying (Maulik

et al., 2019; Pawar et al., 2020; Guan et al., 2022) and double-gyre (Bolton & Zanna, 2019;

Zanna & Bolton, 2020) conﬁgurations. We may emphasize that, by construction, the a

priori learning strategy shall lead to the best a priori results, which shall translate in

a good instantaneous prediction of the SGS term according to metrics Lprio.

2.2 a posteriori learning

The a posteriori learning strategy states the SGS parametrization problem as the

approximation of the true reduced variables according to some a posteriori metrics. This

is important since it is possible for a model to perform well a priori while failing a pos-

teriori, the most common factor being numerical instabilities due to the lack of small-

scale energy dissipation (Maulik et al., 2019; Guan et al., 2022). Let us denote by Φ the

ﬂow operator that advances the reduced system in time, i.e.,

ΦT

θ (¯y, M) =

(cid:90) T

0

g(¯y(t)) + M(¯y(t)|θ) dt.

(5)

Numerically-speaking, ﬂow operator Φ involves a time integration scheme (see Fig. 1).

Following recent advances in neural integration schemes (Chen et al., 2018; Ouala et al.,

2021), we may consider here both explicit and adaptive schemes. Let (cid:96)(y(t), ¯y(t)) be some

a posteriori metrics deﬁned in the true {y(t)}t∈[0,T ] and reduced {¯y(t)}t∈[0,T ] spaces and

–8–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

a representative dataset of trajectories over a time interval [0, T ]. The a posteriori learn-

ing strategy comes to minimize the resulting a posteriori training loss as follows,

Lpost(M) := (cid:96)(y(t), ΦT

θ (¯y(t0), M))

(6)

Now, the a posteriori minimization problem involves the time integration of Φ from a

set of initial reduced states, typically taken to be ¯y(t0) = T (y(t0)),

arg min
θ

Lpost(M) ≡ arg min

θ

(cid:96)({y(t)}t∈[0,T ] , ΦT

θ ({¯y(t0)}, M)).

(7)

Then from the Leibniz integral rule, updating model parameters θ requires the ﬂow par-

tial derivative, i.e.

∂Lpost
∂θ

=

∂Lpost
∂y

∂y
∂θ

+

∂Lpost
∂Φ

∂Φ
∂θ

=

∂Lpost
∂Φ

(cid:90) T

0

∂g(¯y(t))
∂θ

+

∂M(¯y(t)|θ)
∂θ

dt.

(8)

This equation makes explicit that the gradient-based optimization of the a posteriori cri-

terion involves the computation of the gradient with respect to all the components of the

forward model, i.e. dynamical operator g as well as the considered time integration scheme.

Assuming that one can run all components within a diﬀerentiable programming frame-

work (here, PyTorch (Paszke et al., 2019)), the embedded automatic diﬀerentiation tools

makes these computations direct with no additional programming cost. In our exper-

iments, this comes to performing an automatic diﬀerentiation for an explicit fourth-order

Runge-Kutta scheme with N discrete time-steps, which deﬁnes the temporal horizon T =

N ∆t.

The a posteriori strategy signiﬁcantly widens the range of metrics which can be

considered to calibrate the SGS parametrization. We illustrate this modeling ﬂexibility

for quasi-geostrophic turbulence in the next section. We may point out that this a pos-

teriori learning strategy has recently been explored for temporally-developing plane tur-

bulent jets (MacArt et al., 2021) and the short-term simulation of short-term two-dimensional

ﬂows (Kochkov et al., 2021). Here, we explore further its relevance for two-dimensional

geophysical ﬂows, including a benchmarking with the a priori setting for diﬀerent ﬂow

conﬁgurations.

3 Application to quasi-geostrophic turbulence

Geophysical turbulence is widely acknowledged to involve energy backscatter. This

makes SGS parametrization a key issue for the simulation of ocean and atmosphere dy-

namics (Graham & Ringler, 2013; Jansen et al., 2015; Juricke et al., 2020). As case-study

–9–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

y(t)

f (y(t))

y(t + ∆t)

· · ·

y(t + N ∆t)

f (y(t+N ∆t))

y(t + (N + 1)∆t)

T (y(t))

g(¯y(t))

Lpost

· · ·

Lpost

g(¯y(t + N ∆t))

Lpost

¯y(t)

+

¯y(t + ∆t)

· · ·

¯y(t + N ∆t)

+

¯y(t + (N + 1)∆t)

M(¯y(t)|θ)

Lprio

R(¯y(t))

M(¯y(t + N ∆t)|θ)

Figure 1.

Sketch of one learning step for the a priori and a posteriori strategies. The a priori

loss is computed at instantaneous time t (dashed, red), while the a posteriori loss involves several

states forward in time (dashed, blue).

framework, we consider quasi-geostrophic (QG) ﬂows. While providing an approximate

yet representative model for rotating stratiﬁed ﬂows found in atmosphere and ocean dy-

namics, they involve relatively complex SGS features that make the learning problem

non trivial. As such, QG ﬂows are regarded as an ideal playground to explore and as-

sess the relevance of a priori and a posteriori learning strategies for SGS parametriza-

tion in geophysical turbulence.

QG equations (Majda & Wang, 2006) are given by,

∂tω + J(ψ, ω) = ν∇2ω − µω − β∂xψ + F

(9)

which is equivalent to the transport of vorticity ω, obtained by taking the curl of the in-

compressible Navier-Stokes equations, i.e. ∇ · u = 0 and ω = ∇ × u and applying

beta-plane approximation, hydrostatic and geostrophic balances. In addition, we have,

u = (−∂yψ, ∂xψ)

ω = ∇2ψ

(10)

(11)

where ψ is the streamfunction, u the velocity and J(ψ, ω) = ∂xψ∂yω − ∂yψ∂xω is the

non-linear Jacobian operator. The model is parametrized by viscosity ν, linear drag co-

eﬃcient µ, Rossby parameter β and a source term F . The QG equations has two invari-

ants (Bouchet & Venaille, 2012), for energy

E =

(cid:90)

1
2

u2dr

–10–

(12)

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

and enstrophy

Z =

(cid:90)

1
2

ω2dr.

(13)

Also relevant for the long-term evolution of the SGS parametrizations the enstrophy trans-

fers, and in particular ﬂuxes, are deﬁned by

ΠM

Z (k) = −

(cid:90) k

0

ˆω∗(k(cid:48)) ˆM(k(cid:48)) − ˆω∗(k(cid:48)) ˆJ(ψ, ω)(k(cid:48))dk(cid:48)

(14)

where the Fourier transform is represented by ˆ· and complex conjugation by ·∗. Note that

the exact enstrophy ﬂux can be obtained from the SGS term ΠR

Z (k).

3.1 SGS parametrization for QG dynamics

The derivation of the reduced model for QG dynamics follows the same procedure

than described for ﬂuid dynamics in general. Assuming a known projection operator T

from Eq. (1) given as a spatial discretization D : Ω → ¯Ω and the convolution of y with

a kernel function G(x) (Leonard, 1975),

(cid:20)(cid:90)

T (y) := D

y(x(cid:48))G(x − x(cid:48))dx(cid:48)

(cid:21)

.

(15)

We can then derive the equations which govern the evolution of reduced vorticity ¯ω as,






∂tω + J(ψ, ω) = ν∇2ω − µω − β∂xψ + F,

ω ∈ Ω

∂t ¯ω + J( ¯ψ, ¯ω) = ν∇2 ¯ω − µ¯ω − β∂x

¯ψ + ¯F + J( ¯ψ, ¯ω) − J(ψ, ω)
(cid:125)

(cid:124)

(cid:123)(cid:122)
R(ψ,ω)

,

¯ω ∈ ¯Ω

(16)

T (ω) = ¯ω = D (cid:2)(cid:82) ω(x(cid:48))G(x − x(cid:48))dx(cid:48)(cid:3)

where R(ψ, ω) is the SGS term. For convenience, note that the reduced term can be ex-

pressed in a ﬂux formulation,

R(ψ, ω) = ∇ · (¯u ¯ω − u ω) .

(17)

In this context, ¯ω is only solved for the largest scales of the ﬂow, and R(ψ, ω) accounts

for the eﬀect of unresolved motions on the resolved scales. The SGS term is thus not known

from the reduced variables because of the non-linear interactions of small scales dynam-

ics J(ψ, ω). Following the notations introduced in Section 3, we aim to identify a QG

SGS parametrization M( ¯ψ, ¯ω|θ) given the parametrization for operator g by (9) using

both a priori and a posteriori learning strategies.

–11–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

3.2 Numerical solver

The equations (9) are solved using a pseudo-spectral diﬀerentiable code, with a clas-

sical fourth-order Runge-Kutta time integration scheme. The system is deﬁned in a squared

domain Ω ∈ [−π, π]2 with a Fourier basis, i.e. double-periodic boundary conditions ∂Ω

on Ntrue = (Nx, Ny) grid points with uniform spacing ∆true = ΩN −1

true. The reduced

states are obtained by projecting the true (or high-resolution) states through a convo-

lution with a spatial kernel Gδ(k) at spatial scale δ > 0 followed by a discretization on
the reduced grid ¯Ω, i.e. with larger spacing ∆reduced = δ∆true equivalent to a sharp cut-

oﬀ,

¯ω(k) := (ω ∗ Gδ)(|k| < π∆−1

reduced).

(18)

It has been shown previously that SGS parametrizations can perform diﬀerently depend-

ing on the type of ﬁlter used in the evaluations (Piomelli et al., 1988; Zhou et al., 2019).

We then aim to evaluate how the choice of the ﬁlter aﬀects the learning strategies and

we consider two common types of ﬁlters, deﬁned in spectral space as

Gaussian ﬁlter :
(cid:18)

k2∆2
24

true

(cid:19)

,

Gδ(k) = exp

−

Cut-oﬀ ﬁlter :

Gδ(k) = 0, ∀k > π∆−1

reduced.

(19)

(20)

Regarding numerical aspects, we can solve the time integration of the reduced system

g with a larger time-step by a factor corresponding to the grid size ratio (or ﬁlter scale

δ), i.e. ∆treduced = δ∆ttrue. To generate the corresponding datasets {R(yi), ¯yi}n and

{y(t)}t∈[0,T ], we subsample one true state every δ iterations performed by the true sys-

tem f .

3.3 Baseline parametrizations

For benchmarking purposes, we implement some physics-based baselines and fo-

cus on parametrizations based on functional eddy viscosity (Kraichnan, 1976), i.e. mod-

els that artiﬁcially dissipate energy at relevant scales to remain stable. This is to be con-

trasted with structural models that produce back-scatter and thus suﬀer from stability

issues and will not be considered here. One can state these parameterizations in a ﬂux

–12–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

formulation,

MP( ¯ψ, ¯ω) = νe∇2 ¯ω.

where the eddy viscosity coeﬃcient νe contains an arbitrary constant cP for which the

optimal value depends on the ﬂow conﬁguration,

νe = (cP∆)n|εP( ¯ψ, ¯ω)|

(21)

(22)

with n depending on the scaling law used to derive the model. We used the dynamic pro-

cedure proposed by Germano et al. (1991) and Lilly (1992) where the constant is com-

puted from a least-square minimization of the residual SGS term with a ﬁlter size larger
than δ, i.e. ˜ω = ω ∗ G˜δ, ˜δ > δ. We also apply spatial averaging in order to avoid lo-
cally negative constants cP(x, y) < 0, i.e. ensuring that the models are purely diﬀusive

and νe ≥ 0.

One of the most popular SGS model has been proposed by Smagorinsky (1963).

It derives from the assumption of direct cascade of energy, which is relevant for three-

dimensional ﬂows. However, this assumption is expected not to translate well to two-

dimensional or geophysical turbulence, even if it has been already employed in global cli-

mate models (Delworth et al., 2012). Following a similar derivation, the Leith model (Leith,

1996) is often referred as the two-dimensional counterpart of the Smagorinsky model,

assuming a direct cascade of enstrophy. The models are deﬁned as eddy viscosity coef-

ﬁcients proportional to the resolved strain rate ¯S and vorticity gradient ∇¯ω, respectively,

Smagorinsky model :

νe = (cS∆)2| ¯S|,

Leith model :

νe = (cL∆)3|∇¯ω|.

(23)

(24)

We will denote by MDynSmagorinsky and MDynLeith the dynamic versions of these two mod-

els where cP has been computed using the dynamic procedure mentioned above.

3.4 Neural architecture and training

Our main focus being here the impact of a priori and a posteriori strategies, we

consider the same neural-network-based parametrization M with the two learning set-

tings. We use a convolutional neural network (CNN) architecture, which is particularly

–13–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

{¯ω, ¯ψ}

Periodic pad

×9

Conv5×5
64
ReLU

Conv5×5

1

M

Figure 2.

Sketch of the fully convolutional architecture employed in this study. The model

applies 9 inner convolutional blocks, i.e. one 2D convolution layer ConvK

C and one non-linear

ReLU activation. The number of channels C for the inner convolutions and the kernel size K

are equal to 64 and 5, respectively. For the ﬁnal layer, the model uses a 2D convolution layer to

output the targeted real-valued SGS term. Since our computational domain is doubly periodic,

we also use a periodic padding as a ﬁrst layer.

relevant for translation-invariant problems and have been used with success to train SGS

parametrizations, e.g. (Beck et al., 2019; Bolton & Zanna, 2019; Lapeyre et al., 2019;

Mohan et al., 2020; Frezat et al., 2021; Guan et al., 2022).

As shown in Fig. 2, we use a simple ConvNet with 10 layers of convolutions with

non-linear ReLU activations. More involved architectures could further improve the per-

formance. We may point out that our goal is not to design an optimal NN-based archi-

tecture, but rather to evaluate the impact of diﬀerent learning strategies at similar com-

putational complexity for the SGS parameterization.

Regarding the learning phase, the training loss for the a priori strategy (3) com-

putes the MSE of the predicted term with respect to the true SGS term on a batch of

S samples,

Lprio(M) :=

1
S

S
(cid:88)

i=1

(R(ψ, ω)i − M( ¯ψi, ¯ωi))2.

(25)

For the a posteriori strategy (7), the choice of training loss is more ﬂexible, since we can

explore spatio-temporal metrics for a batch of N = T /∆t discrete integration steps. To

illustrate the basics of the strategy, we choose the MSE of the most important state of

–14–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

the QG system, i.e. the vorticity,

Lpost(M) :=

1
N

N
(cid:88)

(T (ω(i∆t)) − ¯ω(i∆t))2.

i=1

(26)

The models are trained with the Adam optimizer on 10 independent trajectories, i.e. sim-

ulations of 3000 snapshots each using diﬀerent initial conditions, which gives a dataset

of 30000 samples. Regarding the a posteriori strategy, training a model that performs

the time integration of a system of PDEs inside the minimization loop may lead to in-

stabilities and diﬃculties. To address these issues, we consider Algorithm 1. It involves

the following key steps:

• A gradual increase of the time horizon [0, T ] (Line 2). The time integration scheme

of reduced system with operators g and MNN may result in a very deep compu-

tational graph (typically, ConvNet with more than 8 layers with the considered

conﬁguration with 25 integration steps), which may in turn lead to the commonly

known vanishing gradient problem (Hochreiter et al., 2001) especially for the ﬁrst

epochs of the training process. To address this issue, we gradually increase the tem-

poral horizon from [0, T

e ] to [0, T ] where e corresponds to the current training epoch.

In this study, the increment is done using a simple linear function, but any increas-

ing heuristic should work as long as the ﬁrst critical epochs take a small number

of iterations.

• Withdrawing simulated data on the ﬂy for which the Courant–Friedrichs–Lewy

(CFL) is greater than some threshold (Line 8). Incorrect predictions from the NN

especially during the ﬁrst epochs of the training process for PDE problems at the

limit of numerical stability can lead to numerical blowups of the system and by

consequence exploding gradient for the minimization algorithm. We then discard

batches for which the Courant–Friedrichs–Lewy (CFL) is greater than some thresh-

old, commonly chosen to be 1.

4 Results

In order to evaluate the performance of a priori and a posteriori learning strate-

gies, we report numerical experiments for three diﬀerent conﬁgurations of QG ﬂows. First,

we study decaying turbulence and compare the a posteriori strategy with previous works

based on the a priori strategy (Maulik et al., 2019; Guan et al., 2022). Then, we assess

the performance of the proposed models in a more realistic wind-forced conﬁguration rep-

–15–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

Algorithm 1 Training algorithm for SGS model M using the a posteriori strategy.

True variables y are sampled randomly from dataset which is only required to con-

tain true variables. In practice, the projection can be applied beforehand and dataset

{T (y(t))}t∈[0,T ] can also be built from projected true states. Note that the outer loop

iterates over the entire trajectories for each epoch.
Require: dataset {y(t)}t∈[0,T ]

Require: reduced system g, training model M(¯y|θ)

Require: number of iterations n, number of epochs e, starting time t0

Require: loss function Lpost

1: for i ← 1 to e do
neﬀ ← i (cid:4) n

2:

(cid:5)

e

(cid:46) Deﬁne eﬀective number of iterations

3:

4:

5:

6:

7:

8:

9:

10:

11:

y(t) ← sample({y(t)} , neﬀ )

(cid:46) Randomly sample consecutive true states

¯y0 ← T (y(t0))

for j ← 1 to neﬀ do

(cid:46) Deﬁne initial state from true states

¯yj ← I(g(¯yj−1) + M(¯yj−1|θ))

(cid:46) Time integration with I (here RK4)

end for

if maxj∈neff CFL(¯yj) > 1 then

(cid:46) Discard batch if stability is not obtained

continue

end if

θ ← step(M, ∂Lpost(y,¯y)

∂θ

)

(cid:46) Optimize model parameters from a posteriori loss

12: end for

–16–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

Table 1. Parameters of the diﬀerent DNS ﬂow conﬁgurations. Details about simulation spin-

up, initialization and forcing parameters are described in more details in Sec. 4.2. Note that

reduced systems use the same parameters, except for grid resolution ( ¯Nx, ¯Ny) obtained from the

spatial ﬁlter scale (Nx/δ, Ny/δ) and time-step ∆treduced = δ∆ttrue. We consider the following

spatial and temporal transformations Ld(x) = 1.2 × 106s and Td(x) = 504 × 104/πm.

Name

Nx × Ny

Lx × Ly

∆t

km

s

Decay

Forced

2048 × 2048

104 × 104

2048 × 2048

104 × 104

Beta-plane

2048 × 2048

104 × 104

120

120

120

µ

m−1

0

1.25 × 10−8

1.25 × 10−8

ν

β

Re

m2s−1

m−1s−1

67.0

22.0

22.0

0

0

32 × 103

22 × 104

1.14 × 10−11

34 × 104

resentative of mesoscale oceanic simulations (Fox-Kemper & Menemenlis, 2008; Graham

& Ringler, 2013). Finally, we analyze the impact of planetary rotation through the beta-

plane eﬀect on a mid-latitude geophysical ﬂow.

For these three cases, we consider the following experimental setup. The training

and test data involve respectively 10 and 5 direct numerical simulations (DNS) corre-

sponding to the same conﬁgurations with diﬀerent initial conditions. The reduced sys-

tems are run with δ = 16, i.e. the reduced grid is 16 times smaller compared to the true

grid resolution in each direction. Reduced systems are integrated for 6000 iterations for

the non-stationary decay cases and 18000 iterations to determine long-term statistics of

the forced and beta-plane conﬁgurations. We may emphasize that the simulations used

for evaluation purposes are never seen during the training phase. The parameters of the

diﬀerent ﬂows are shown in Table 1 in dimensionalized units. Overall, for each QG con-

ﬁguration, we report a quantitative synthesis for the cutoﬀ and Gaussian ﬁlters and fur-

ther illustrate the key features of the diﬀerent learning strategies using a cutoﬀ ﬁlter.

Important quantities discussed in the evaluation are both;

• short term temporal evolution of quadratic invariants both for the energy (12)

and the enstrophy (13), typical in weather forecast.

• long term statistics from enstrophy spectrum and ﬂux (14) in spectral space, rel-

evant for climate predictions.

–17–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

Figure 3. True initial vorticity ﬁeld ω (left) and corresponding sub-grid contribution ω(cid:48) = ω −

¯ω (right) for a decaying turbulence trajectory.

Figure 4. Vorticity ﬁelds for the diﬀerent models at the end (6000 reduced system iterations

equivalent 96000 true system iterations) of one decaying turbulence evaluation trajectory.

4.1 Decaying turbulence

In the context of two-dimensional SGS parametrization with ML models, the de-

caying turbulence conﬁguration is one of the most studied (Maulik et al., 2019; Pawar

et al., 2020; Guan et al., 2022). This type of ﬂow is particularly interesting because of

its non-stationary nature, i.e. the system invariants are temporally varying. Similarly

to Guan et al. (2022), we sample the initial vorticity ﬁelds randomly from a gaussian dis-

tribution ω ∼ N (0, 1) at moderate wavenumbers k ∈ [10, 32] and integrate the sys-

tem for 10000 iterations before reaching spectrum self-similarity (Batchelor, 1969).

From this starting time t0, the vorticity ﬁelds exhibit an early turbulence behav-

ior with a lot of ﬁne structures (see Fig. 3). DNS and reduced models with SGS parametriza-

tions are run for 6000 iterations, which is longer than the temporal horizon used in the

training data by a factor of two. In two-dimensional decaying ﬂows, we expect to see vor-

tex pairing and emergence of larger structures as shown in Fig. 4. Due to their purely

–18–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

Figure 5. Evolution of domain-averaged energy (left) and enstrophy (right) computed in

non-dimensionalized time units in the decaying turbulence setting.

Figure 6. Final enstrophy spectrum (left) and time-averaged enstrophy ﬂux (right) describing

statistical performance of the models in decaying turbulence.

diﬀusive form, parameterizations MDynSmagorinsky and MDynLeith cannot perform well

on this conﬁguration and are indeed incorrectly dissipating relevant small scales. The

NN-based model trained with the a priori strategy has accumulated small scale enstro-

phy and is thus perturbed with noise coming from numerical instabilities. The model

trained with the a posteriori visually shows the expected stable dynamics with small scales

features, even outside the training regime, which supports some degree of generalization

(or extrapolation).

The evolution of domain-averaged quadratic integrals in Fig. 5 conﬁrms the ob-

servations from the velocity ﬁelds, since we can see a large energy and enstrophy decrease

for both MDynSmagorinsky and MDynLeith, while the a priori model correctly captures

–19–

0246810t0.050.060.070.080.0912Zu2drDNSMDynSmagorinskyMDynLeithMaprioriMN=25aposteriori(state)0246810t51015202512Zω2drDNSMDynSmagorinskyMDynLeithMaprioriMN=25aposteriori(state)100101k10-310-210-1Z(k)DNSMDynSmagorinskyMDynLeithMaprioriMN=25aposteriori(state)100101k0.00.20.40.60.81.01.2ΠZ(k)DNSMDynSmagorinskyMDynLeithMaprioriMN=25aposteriori(state)manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

Figure 7. True initial vorticity ﬁeld ω (left) and corresponding sub-grid contribution ω(cid:48) = ω −

¯ω (right) for a forced turbulence trajectory.

the energy decay but dissipates enstrophy too slowly compared to the DNS. Spectral statis-

tics shown in Fig. 6 are also in close agreement to the DNS for the a posteriori -trained

model, in particular for the large wavenumbers of the enstrophy spectrum Z(k) = k2E(k)

which particularly highlight the dynamic of the smallest resolved scales.

4.2 Forced turbulence

The second case-study involves QG ﬂows with a source term F designed to mimic

wind-stress. We study a particular conﬁguration inspired by Graham and Ringler (2013),

which evaluated the performance of a large number of physics-based parametrizations

in mesoscale ocean simulations. To reproduce these realistic equilibrium solutions, we

use a bottom drag (µ > 0) and initiate turbulent mixing from a wind-stress slowly vary-

ing in time at large-scale k = 4 with steady enstrophy rate injection CF such that,

F = CF (t)[ cos(4y + π sin(1.4t))−

cos(4x + π sin(1.5t))]

1
2

(cid:104)F 2(cid:105) = 3.

(27)

(28)

(29)

In order to converge to a stationary turbulent state, we initialize the simulation runs from

a few large-scale Fourier modes and spin-up on a smaller grid (10242) for over 1300 days.

The initial conditions for training and evaluation (see Fig. 7) are taken after energy and

enstrophy propagation to the smallest scales of the true grid (20482) in about 35 days.

–20–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

Figure 8. Vorticity ﬁelds for the diﬀerent models at the end (18000 reduced system iterations

equivalent 288000 true system iterations) of one forced turbulence evaluation trajectory.

Figure 9. Evolution of domain-averaged energy (left) and enstrophy (right) computed in

non-dimensionalized time units in the forced turbulence setting.

To evaluate the long-term performance of the forced conﬁguration in equilibrium,

we run simulations for 18000 time steps, which is at least 3 times longer than the com-

plete decorrelation time of the system due to chaos. We report the vorticity ﬁelds at the

end of the simulations in Fig. 8. The true vorticity state exhibits both large vortices gen-

erated by the wind forcing and small ﬁlaments in between. Overall, we draw conclusions

similar to the decaying turbulence regime. We note that the small structures are inac-

curately predicted for both MDynSmagorinsky and MDynLeith due to dissipation and for

the a priori model due to numerical instabilities. By contrast, the a posteriori model

is the only one to correctly capture both the large-scale and ﬁne-scale patterns in this

conﬁguration.

While domain-averaged integrals ﬂuctuate a lot on such long-term trajectories due

to the chaotic nature of the ﬂow, we expect those quantities to remain approximately

constant over time. This property is veriﬁed on the kinetic energy for both NN-based

–21–

051015202530t1.801.851.901.952.002.0512Zu2drDNSMDynSmagorinskyMDynLeithMaprioriMN=25aposteriori(state)051015202530t78910111212Zω2drDNSMDynSmagorinskyMDynLeithMaprioriMN=25aposteriori(state)manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

Figure 10. Time-averaged enstrophy spectrum (left) and enstrophy ﬂux (right) describing

statistical performance of the models in forced turbulence.

models, but the enstrophy of the a priori -trained model increases over time, which in-

dicates some accumulation of small-scale energy and may result in a potential future blow-

up of the simulation. The time-averaged statistical enstrophy spectrum shown in Fig.

10 demonstrates the ability of the a posteriori model to reproduce accurately both the

smallest scales and the largest scales of the simulation (small wavenumbers) compared

to the other models.

4.3 Beta-plane turbulence

The third case-study runs the same forced simulation as in the previous conﬁgu-

ration complemented by a beta-plane eﬀect to account for the meridional variation of

Coriolis force caused by a spherical shape. We take a Rossby parameter β correspond-

ing to Earth’s planetary rotation on mid latitudes (60°).

The beta-eﬀect has an important impact on the topology of the dynamics, as it cre-

ates high-velocity longitudinal jets as seen in Fig. 11. In this simple setting without to-

pography (ﬂat bottom layer), the system does not go through states transitions but re-

mains in a statistical equilibrium. The strong vorticity gradients in between jets are pre-

dicted more accurately (see Fig. 12) by the MDynLeith than the MDynSmagorinsky, which

still over-dissipates small scales. The model trained a priori is not stable at all in this

conﬁguration, while the a posteriori model remains stable to simulate visually-consistent

patterns.

–22–

100101k10-210-1100Z(k)DNSMDynSmagorinskyMDynLeithMaprioriMN=25aposteriori(state)100101k0.10.00.10.20.30.40.50.6ΠZ(k)DNSMDynSmagorinskyMDynLeithMaprioriMN=25aposteriori(state)manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

Figure 11. True initial vorticity ﬁeld ω (left) and corresponding sub-grid contribution

ω(cid:48) = ω − ¯ω (right) for a beta-plane turbulence trajectory.

Figure 12. Vorticity ﬁelds for the diﬀerent models at the end (18000 reduced system itera-

tions equivalent 288000 true system iterations) of one beta-plane turbulence evaluation trajec-

tory.

Figure 13. Evolution of domain-averaged energy (left) and enstrophy (right) computed in

non-dimensionalized time units in the beta-plane turbulence setting.

–23–

051015202530t4.04.55.05.56.012Zu2drDNSMDynSmagorinskyMDynLeithMaprioriMN=25aposteriori(state)051015202530t5010015020025030035012Zω2drDNSMDynSmagorinskyMDynLeithMaprioriMN=25aposteriori(state)manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

Figure 14. Time-averaged enstrophy spectrum (left) and enstrophy ﬂux (right) describing

statistical performance of the models in beta-plane turbulence.

The instabilities visible in the vorticity ﬁeld of the a priori model are explained

by the increasing energy and enstrophy in Fig. 13. This reveals a non-conservative be-

havior which leads to a simulation blowup. The a posteriori strategy performs extremely

well on this long-term simulation, predicting in particular a correct enstrophy evolution

compared to that of the DNS. The enstrophy spectrum and ﬂuxes (see Fig. 14) are sim-

ilar to those of the forced turbulence conﬁguration, except that the linear damping has

a stronger impact, due to the relative increase in velocity from the beta-eﬀect. Overall,

the conclusions are the same, with highest ﬁdelity small-scale dynamics is produced by

the model trained using the a posteriori strategy.

4.4 Quantitative synthesis

As a quantitative synthesis of our numerical experiments, we report for the three

case-studies and the diﬀerent models two metrics: an a priori metric given by Pearson

correlation coeﬃcient of the predicted SGS terms (Table. 3.) and an a posteriori met-

ric given by a variant of the error-landscape enstrophy ﬂux assessment presented by Meyers

(2011) (Table. 2). We report these performance metrics both for cutoﬀ and Gaussian

ﬁlters.

We ﬁrst notice that the type of learning strategy clearly impacts the correspond-

ing metrics, i.e. a priori learning performs best on a priori metrics and similarly for the

a posteriori learning on a posteriori metrics. Concerning the projection kernel used, al-

–24–

100101k10-210-1100101Z(k)DNSMDynSmagorinskyMDynLeithMaprioriMN=25aposteriori(state)100101k1.51.00.50.00.51.01.5ΠZ(k)DNSMDynSmagorinskyMDynLeithMaprioriMN=25aposteriori(state)manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

Table 2. Long-term performance of considered SGS parametrizations in the three diﬀerent

conﬁgurations with both cutoﬀ and gaussian projection kernels. We compute the L2 distance

between the reference enstrophy ﬂuxes and the ones simulated using the diﬀerent SGS parameter-

izations. The a posteriori learning strategy clearly leads to much better scores.

L2(ΠR

Z − ΠM
Z )

Decay

Forced

Beta-plane

Cutoﬀ Gaussian Cutoﬀ Gaussian Cutoﬀ Gaussian

MDynSmagorinsky

MDynLeith

Mapriori

1.95

1.64

0.74

1.31

1.02

0.60

0.49

0.16

0.36

0.16

0.11

0.40

2.83

1.66

8.73

1.75

0.98

0.26

Maposteriori (states)

0.13

0.09

0.02

0.02

0.30

0.05

Table 3.

Short-term performance of the considered SGS parametrizations in the three diﬀer-

ent conﬁgurations with both cutoﬀ and gaussian projection kernels. We compute the correlation

coeﬃcient between predicted and exact subgrid terms, which favors the a priori learning strategy.

ρR, M

Decay

Forced

Beta-plane

Cutoﬀ Gaussian Cutoﬀ Gaussian Cutoﬀ Gaussian

MDynSmagorinsky

MDynLeith

Mapriori

0.16

0.13

0.75

Maposteriori (states)

0.77

0.38

0.32

0.90

0.57

0.09

0.08

0.82

0.45

0.55

0.49

0.95

0.29

0.04

0.03

0.82

0.48

0.28

0.17

0.96

0.21

gebraic models and the NN-based model trained a priori perform better with the gaus-

sian ﬁlter. While the algebric models have good performance on the forced conﬁgura-

tion with gaussian ﬁlter, we note that the model trained using the a posteriori is con-

sistent in each conﬁguration.

5 Discussion

This study investigated diﬀerent learning strategies to train subgrid-scale (SGS)

parametrizations for two-dimensional quasi-geostrophic turbulent ﬂows. While the state-

of-the-art has mostly explored a priori learning schemes, our numerical experiments stress

the signiﬁcant improvement brought by the a posteriori learning strategy to better re-

–25–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

produce small scale dynamics on large temporal horizons with great accuracy. For all

the ﬂow conﬁgurations and coarsening schemes considered in this study, SGS parametriza-

tions trained according to an a posteriori training loss clearly outperform both physics-

based and machine learning baselines.

The a posteriori learning strategy introduced in this paper opens the possibility

to design stable subgrid parametrizations with more ﬂexibility than state-of-the-art ML-

based approaches. Indeed, we have here explored a relatively simple a posteriori train-

ing loss given by a vorticity-based MSE, the a posteriori learning scheme oﬀers a much

greater ﬂexibility for the exploitation and combination of diﬀerent a posteriori metrics

during the learning phase. Losses deﬁned from classic performance metrics such as en-

ergy transfers and distributions seem particularly appealing. One may also explore application-

speciﬁc metrics including among others boundary layers ﬂows. As the a posteriori learn-

ing strategy results in an improved stability of the trained SGS parametrizations, it may

also oﬀer means to explore more complex neural architectures for SGS terms. Here, we

considered a relatively simple ConvNet, but more complex and state-of-the-art neural

architectures including for instance ResNet, UNet and transformer networks could be

worth exploring. Another interesting avenue is the joint training of a posteriori mod-

els in the context of data assimilation such as described in Bonavita and Laloyaux (2020)

and Farchi et al. (2021).

An interesting connection can indeed be made between a posteriori learning and

variational data assimilation techniques. Our a posteriori learning algorithm formulates

a variational problem which is formally equivalent to the strong constraint 4D-Var scheme

(Blayo et al., 2015; Carrassi et al., 2018). But, in our case, the control vector is composed

of the parameters of the neural network and observations are assumed to be perfect. The

analogy between a posteriori learning and 4D-Var therefore brings the question of whether

parametrizations, or more generally corrections to existing models, could be learned di-

rectly from sparse and noisy observations (Schneider, Lan, et al., 2017). In this sense,

a posteriori learning is related to the bias correction methods that have been proposed

in data assimilation (Dee, 2005), and especially the schemes proposed to infer state-dependent

corrections to existing models (Griﬃth & Nichols, 2000; D’andrea & Vautard, 2000). In-

terestingly, this ﬁeld has received renewed attention over recent years with several au-

thors proposing to approach bias correction with ML (Bonavita & Laloyaux, 2020; Farchi

et al., 2021). In this context, we stress that our approach is very similar to the scheme

–26–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

introduced by Farchi et al. (2021), with the noticeable diﬀerence that we here learn a

correction through the 4D-Var scheme itself, and not from the increment of the assim-

ilation scheme.

By construction, parametrizations learned with the a posteriori strategy should im-

prove the short-term forecasting capabilities of the models. This is true in particular if

the training loss is deﬁned as the sum of forecasting errors over a given time horizon as

considered here. Interestingly, we noted here that for SGS parametrizations, this short-

term forecasting performance translates in a better long-term stability and representa-

tion of long-term ﬂow patterns where the long-term horizon being several order of mag-

nitudes greater than the time horizon in the training loss (18000 vs. 25 time steps). While

recent studies have explored neural models for the short-term forecasting of realistic geo-

physical ﬂows, especially for weather forecasting applications (Schultz et al., 2021; Weyn

et al., 2021), we believe our study opens new avenues for the exploitation of learning-

based components in climate-scale simulations, which remain an open challenge (Rasp

et al., 2018). In this respect, to account for the chaotic nature of turbulent ﬂows, a pos-

teriori training losses could also beneﬁt from statistical metrics as opposed to synoptic

ones as the MSE used in this work.

But a strong requirement of the proposed framework lies in the diﬀerentiability of

the considered dynamical model, which may question its practical applicability. Indeed,

most large-scale forward solvers in earth system models (ESM) rely on high-performance

languages that do not embed automatic diﬀerentiation (AD) capabilities. While it is gen-

erally recognised that adjoint models are very useful additional tools for these solvers

(Barkmeijer, 2009; Wunsch & Heimbach, 2013), adjoint operators are readily available

only for a small fraction of them (Heimbach et al., 2005; Vidard et al., 2015). We stress

that the emergence of a new generation of models written in diﬀerentiable programming

languages such as JAX and JuliaDiﬀ (H¨afner et al., 2021; Ramadhan et al., 2020; Srid-

har et al., 2021; Huang & Topping, 2021) naturally supports our contribution. Besides,

deep diﬀerentiable emulators (Nonnenmacher & Greenberg, 2021; Hatﬁeld et al., 2021;

Kasim et al., 2021) that learn a diﬀerentiable approximation of a non-diﬀerentiable for-

ward solver or of its adjoint may also open new avenues for the development of SGS parametriza-

tions for state-of-the-art ESMs with a posteriori learning strategies.

–27–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

Acknowledgments

The authors would like to thank Laure Zanna, Olivier Pannekoucke, Corentin Lapeyre

and Emmanuel Cosme for helpful discussions. This research was supported by the CNRS

through the 80 PRIME project and the ANR through the Melody, OceaniX, and HRMES

ANR-17-MPGA-0010 projects. Additional support was also provided by Schmidt Fu-

tures, a philanthropic initiative founded by Eric and Wendy Schmidt, as part of its Vir-

tual Earth System Research Institute (VESRI). Computations were performed using GPU

resources from GENCI-IDRIS.

References

Alet, F., Doblar, D., Zhou, A., Tenenbaum, J., Kawaguchi, K., & Finn, C.

(2021).

Noether networks: meta-learning useful conserved quantities. Advances in Neu-

ral Information Processing Systems, 34 .

Bakarji, J., & Tartakovsky, D. M.

(2021). Data-driven discovery of coarse-grained

equations. Journal of Computational Physics, 434 , 110219.

Barkmeijer, J. (2009). Adjoint diagnostics for the atmosphere and ocean.. Retrieved

from https://www.ecmwf.int/node/15243

Batchelor, G. K. (1969). Computation of the energy spectrum in homogeneous two-

dimensional turbulence. Physics of Fluids, 12 (12), II–233.

Bauer, P., Thorpe, A., & Brunet, G.

(2015).

The quiet revolution of numerical

weather prediction. Nature, 525 (7567), 47–55.

Beck, A., Flad, D., & Munz, C.-D. (2019). Deep neural networks for data-driven les

closure models. Journal of Computational Physics, 398 , 108910.

Blayo, ´E., Bocquet, M., Cosme, E., & Cugliandolo, L. F.

(2015).

Advanced data

assimilation for geosciences: Lecture notes of the les houches school of physics:

Special issue, june 2012. OUP Oxford.

Boﬀetta, G., & Ecke, R. E.

(2012). Two-dimensional turbulence. Annual review of

ﬂuid mechanics, 44 , 427–451.

Bolton, T., & Zanna, L.

(2019).

Applications of deep learning to ocean data in-

ference and subgrid parameterization.

Journal of Advances in Modeling Earth

Systems, 11 (1), 376–399.

Bonavita, M., & Laloyaux, P.

(2020). Machine learning for model error inference

and correction.

Journal of Advances in Modeling Earth Systems, 12 (12),

–28–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

e2020MS002232.

Bouchet, F., & Venaille, A.

(2012).

Statistical mechanics of two-dimensional and

geophysical ﬂows. Physics Reports, 515 (5), 227–295.

Brenowitz, N. D., & Bretherton, C. S. (2018). Prognostic validation of a neural net-

work uniﬁed physics parameterization. Geophysical Research Letters, 45 (12),

6289–6298.

Brunton, S. L., Noack, B. R., & Koumoutsakos, P. (2020). Machine learning for ﬂuid

mechanics. Annual Review of Fluid Mechanics, 52 , 477–508.

Brunton, S. L., Proctor, J. L., & Kutz, J. N.

(2016). Discovering governing equa-

tions from data by sparse identiﬁcation of nonlinear dynamical systems.

Pro-

ceedings of the national academy of sciences, 113 (15), 3932–3937.

Carati, D., Ghosal, S., & Moin, P.

(1995). On the representation of backscatter in

dynamic localization models. Physics of Fluids, 7 (3), 606–616.

Carleo, G., Cirac, I., Cranmer, K., Daudet, L., Schuld, M., Tishby, N., . . . Zde-

borov´a, L.

(2019). Machine learning and the physical sciences. Reviews of

Modern Physics, 91 (4), 045002.

Carrassi, A., Bocquet, M., Bertino, L., & Evensen, G.

(2018). Data assimilation in

the geosciences: An overview of methods, issues, and perspectives. Wiley Inter-

disciplinary Reviews: Climate Change, 9 (5), e535.

Chantry, M., Hatﬁeld, S., Dueben, P., Polichtchouk, I., & Palmer, T.

(2021). Ma-

chine learning emulation of gravity wave drag in numerical weather forecasting.

Journal of Advances in Modeling Earth Systems, 13 (7), e2021MS002477.

Chen, R. T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D.

(2018). Neural or-

dinary diﬀerential equations. In Advances in neural information processing sys-

tems (Vol. 31).

Cohen, T., & Welling, M. (2016). Group equivariant convolutional networks. In In-

ternational conference on machine learning (pp. 2990–2999).

Couvreux, F., Hourdin, F., Williamson, D., Roehrig, R., Volodina, V., Villefranque,

N., . . . others

(2021).

Process-based climate model development harness-

ing machine learning: I. a calibration tool for parameterization improvement.

Journal of Advances in Modeling Earth Systems, 13 (3), e2020MS002217.

Cranmer, M., Greydanus, S., Hoyer, S., Battaglia, P., Spergel, D., & Ho, S.

(2020).

Lagrangian neural networks. In Iclr 2020 workshop on integration of deep neu-

–29–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

ral models and diﬀerential equations.

Cushman-Roisin, B., & Beckers, J.-M.

(2011).

Introduction to geophysical ﬂuid dy-

namics: physical and numerical aspects. Academic press.

D’andrea, F., & Vautard, R. (2000). Reducing systematic errors by empirically cor-

recting model errors. Tellus A, 52 (1), 21–41.

Danilov, S., Juricke, S., Kutsenko, A., & Oliver, M.

(2019). Toward consistent sub-

grid momentum closures in ocean models.

In Energy transfers in atmosphere

and ocean (pp. 145–192). Springer.

de Avila Belbute-Peres, F., Smith, K., Allen, K., Tenenbaum, J., & Kolter, J. Z.

(2018). End-to-end diﬀerentiable physics for learning and control. In Advances

in neural information processing systems (Vol. 31, pp. 7178–7189).

Dee, D. P. (2005). Bias and data assimilation. Quarterly Journal of the Royal Mete-

orological Society, 131 (613), 3323–3343.

Delworth, T. L., Rosati, A., Anderson, W., Adcroft, A. J., Balaji, V., Benson, R.,

. . . others

(2012).

Simulated climate and climate change in the gfdl cm2. 5

high-resolution coupled climate model. Journal of Climate, 25 (8), 2755–2781.

Fablet, R., Chapron, B., Drumetz, L., M´emin, E., Pannekoucke, O., & Rousseau, F.

(2021). Learning variational data assimilation models and solvers. Journal of

Advances in Modeling Earth Systems, 13 (10), e2021MS002572.

Farchi, A., Laloyaux, P., Bonavita, M., & Bocquet, M. (2021). Using machine learn-

ing to correct model error in data assimilation and forecast applications. Quar-

terly Journal of the Royal Meteorological Society, 147 (739), 3067–3084.

Fox-Kemper, B., Adcroft, A., B¨oning, C. W., Chassignet, E. P., Curchitser, E., Dan-

abasoglu, G., . . . others (2019). Challenges and prospects in ocean circulation

models. Frontiers in Marine Science, 6 , 65.

Fox-Kemper, B., Bachman, S., Pearson, B., & Reckinger, S.

(2014). Principles and

advances in subgrid modelling for eddy-rich simulations.

Clivar Exchanges,

19 (2), 42–46.

Fox-Kemper, B., & Menemenlis, D.

(2008). Can large eddy simulation techniques

improve mesoscale rich ocean models? Ocean Modeling in an Eddying Regime,

177 , 319–337.

Frezat, H., Balarac, G., Le Sommer, J., Fablet, R., & Lguensat, R.

(2021). Physi-

cal invariance in neural networks for subgrid-scale scalar ﬂux modeling. Physi-

–30–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

cal Review Fluids, 6 (2), 024607.

Gamahara, M., & Hattori, Y.

(2017). Searching for turbulence models by artiﬁcial

neural network. Physical Review Fluids, 2 (5), 054604.

Germano, M., Piomelli, U., Moin, P., & Cabot, W. H.

(1991). A dynamic subgrid-

scale eddy viscosity model. Physics of Fluids, 3 (7), 1760–1765.

Glasmachers, T. (2017). Limits of end-to-end learning. In Asian conference on ma-

chine learning (pp. 17–32).

Graham, J. P., & Ringler, T.

(2013). A framework for the evaluation of turbulence

closures used in mesoscale ocean large-eddy simulations. Ocean Modelling, 65 ,

25–39.

Griﬃth, A. K., & Nichols, N. K. (2000). Adjoint methods in data assimilation for es-

timating model error. Flow, turbulence and combustion, 65 (3), 469–488.

Guan, Y., Chattopadhyay, A., Subel, A., & Hassanzadeh, P. (2022). Stable a poste-

riori les of 2d turbulence using convolutional neural networks: Backscattering

analysis and generalization to higher re via transfer learning. Journal of Com-

putational Physics, 111090.

Guillaumin, A. P., & Zanna, L.

(2021).

Stochastic-deep learning parameterization

of ocean momentum forcing. Journal of Advances in Modeling Earth Systems,

13 (9), e2021MS002534.

H¨afner, D., Nuterman, R., & Jochum, M. (2021). Fast, cheap, and turbulent—global

ocean modeling with gpu acceleration in python. Journal of Advances in Mod-

eling Earth Systems, 13 (12), e2021MS002717.

Hatﬁeld, S., Chantry, M., Dueben, P., Lopez, P., Geer, A., & Palmer, T.

(2021).

Building tangent-linear and adjoint models for data assimilation with neu-

ral networks.

Journal of Advances in Modeling Earth Systems, 13 (9),

e2021MS002521.

Heimbach, P., Hill, C., & Giering, R.

(2005). An eﬃcient exact adjoint of the par-

allel mit general circulation model, generated via automatic diﬀerentiation. Fu-

ture Generation Computer Systems, 21 (8), 1356–1371.

Hochreiter, S., Bengio, Y., Frasconi, P., Schmidhuber, J., et al.

(2001). Gradient

ﬂow in recurrent nets: the diﬃculty of learning long-term dependencies.

IEEE

Press.

Holl, P., Thuerey, N., & Koltun, V.

(2020). Learning to control pdes with diﬀeren-

–31–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

tiable physics. In International conference on learning representations.

Huang, L., & Topping, D. (2021). Jlbox v1. 1: a julia-based multi-phase atmospheric

chemistry box model. Geoscientiﬁc Model Development, 14 (4), 2187–2203.

Jansen, M. F., Held, I. M., Adcroft, A., & Hallberg, R. (2015). Energy budget-based

backscatter in an eddy permitting primitive equation model. Ocean Modelling,

94 , 15–26.

Juricke, S., Danilov, S., Koldunov, N., Oliver, M., & Sidorenko, D.

(2020). Ocean

kinetic energy backscatter parametrization on unstructured grids: Impact on

global eddy-permitting simulations.

Journal of Advances in Modeling Earth

Systems, 12 (1), e2019MS001855.

Kasim, M., Watson-Parris, D., Deaconu, L., Oliver, S., Hatﬁeld, P., Froula, D., . . .

others (2021). Building high accuracy emulators for scientiﬁc simulations with

deep neural architecture search. Machine Learning: Science and Technology,

3 (1), 015013.

Kochkov, D., Smith, J. A., Alieva, A., Wang, Q., Brenner, M. P., & Hoyer, S.

(2021). Machine learning-accelerated computational ﬂuid dynamics.

Pro-

ceedings of the National Academy of Sciences, 118 (21).

Kraichnan, R. H. (1967).

Inertial ranges in two-dimensional turbulence. Physics of

Fluids, 10 (7), 1417–1423.

Kraichnan, R. H. (1976). Eddy viscosity in two and three dimensions. Journal of At-

mospheric Sciences, 33 (8), 1521–1536.

Krasnopolsky, V. M., Fox-Rabinovitz, M. S., & Belochitski, A. A.

(2013).

Using

ensemble of neural networks to learn stochastic convection parameterizations

for climate and numerical weather prediction models from data simulated by a

cloud resolving model. Advances in Artiﬁcial Neural Systems, 2013 .

Lapeyre, C. J., Misdariis, A., Cazard, N., Veynante, D., & Poinsot, T. (2019). Train-

ing convolutional neural networks to estimate turbulent sub-grid scale reaction

rates. Combustion and Flame, 203 , 255–264.

Leith, C. (1996). Stochastic models of chaotic systems. Physica D: Nonlinear Phe-

nomena, 98 (2-4), 481–491.

Leonard, A.

(1975).

Energy cascade in large-eddy simulations of turbulent ﬂuid

ﬂows. Advances in Geophysics, 18 , 237–248.

Lesieur, M., M´etais, O., Comte, P., et al.

(2005). Large-eddy simulations of turbu-

–32–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

lence. Cambridge University Press.

Lilly, D. K.

(1992). A proposed modiﬁcation of the germano subgrid-scale closure

method. Physics of Fluids A: Fluid Dynamics, 4 (3), 633–635.

Long, Z., Lu, Y., Ma, X., & Dong, B. (2018). Pde-net: Learning pdes from data. In

International conference on machine learning (pp. 3208–3216).

MacArt, J. F., Sirignano, J., & Freund, J. B.

(2021). Embedded training of neural-

network subgrid-scale turbulence models.

Physical Review Fluids, 6 (5),

050502.

Majda, A., & Wang, X. (2006). Nonlinear dynamics and statistical theories for basic

geophysical ﬂows. Cambridge University Press.

Maulik, R., San, O., Rasheed, A., & Vedula, P.

(2019). Subgrid modelling for two-

dimensional turbulence using neural networks.

Journal of Fluid Mechanics,

858 , 122–144.

Mellor, G. L.

(1985). Ensemble average, turbulence closure.

In Advances in geo-

physics (Vol. 28, pp. 345–358). Elsevier.

Meyer, D., Hogan, R. J., Dueben, P. D., & Mason, S. L.

(2021). Machine learning

emulation of 3d cloud radiative eﬀects. Journal of Advances in Modeling Earth

Systems, e2021MS002550.

Meyers, J. (2011). Error-landscape assessment of large-eddy simulations: a review of

the methodology. Journal of Scientiﬁc Computing, 49 (1), 65–77.

Mohan, A. T., Lubbers, N., Livescu, D., & Chertkov, M.

(2020). Embedding hard

physical constraints in convolutional neural networks for 3d turbulence.

In Iclr

2020 workshop on integration of deep neural models and diﬀerential equations.

Neumann, P., D¨uben, P., Adamidis, P., Bauer, P., Br¨uck, M., Kornblueh, L., . . .

Biercamp, J.

(2019). Assessing the scales in numerical weather and climate

predictions: will exascale be the rescue?

Philosophical Transactions of the

Royal Society A, 377 (2142), 20180148.

Nonnenmacher, M., & Greenberg, D. S.

(2021).

Deep emulators for diﬀerentia-

tion, forecasting and parametrization in earth science simulators.

Journal of

Advances in Modeling Earth Systems, 13 (7), e2021MS002554.

O’Gorman, P. A., & Dwyer, J. G.

(2018). Using machine learning to parameter-

ize moist convection: Potential for modeling of climate, climate change, and

extreme events.

Journal of Advances in Modeling Earth Systems, 10 (10),

–33–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

2548–2563.

Ollinaho, P., Bechtold, P., Leutbecher, M., Laine, M., Solonen, A., Haario, H., &

J¨arvinen, H.

(2013). Parameter variations in prediction skill optimization at

ecmwf. Nonlinear processes in Geophysics, 20 (6), 1001–1010.

Ouala, S., Debreu, L., Pascual, A., Chapron, B., Collard, F., Gaultier, L., & Fablet,

R.

(2021). Learning runge-kutta integration schemes for ode simulation and

identiﬁcation. arXiv preprint arXiv:2105.04999 .

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., . . . others

(2019).

Pytorch: An imperative style, high-performance deep learning li-

brary.

In Advances in neural information processing systems (Vol. 32, pp.

8026–8037).

Pawar, S., San, O., Rasheed, A., & Vedula, P.

(2020).

A priori analysis on deep

learning of subgrid-scale parameterizations for kraichnan turbulence. Theoreti-

cal and Computational Fluid Dynamics, 34 (4), 429–455.

Piomelli, U., Moin, P., & Ferziger, J. H.

(1988). Model consistency in large eddy

simulation of turbulent channel ﬂows. Physics of Fluids, 31 (7), 1884–1891.

Pope, S. B. (2000). Turbulent ﬂows. Cambridge University Press.

Portwood, G. D., Nadiga, B. T., Saenz, J. A., & Livescu, D.

(2021).

Interpret-

ing neural network models of residual scalar ﬂux. Journal of Fluid Mechanics,

907 .

Raissi, M., Perdikaris, P., & Karniadakis, G. E.

(2019).

Physics-informed neural

networks: A deep learning framework for solving forward and inverse problems

involving nonlinear partial diﬀerential equations.

Journal of Computational

Physics, 378 , 686–707.

Ramadhan, A., Wagner, G., Hill, C., Campin, J.-M., Churavy, V., Besard, T., . . .

Marshall, J.

(2020).

Oceananigans. jl: Fast and friendly geophysical ﬂuid

dynamics on gpus. Journal of Open Source Software, 5 (53).

Rasp, S., Pritchard, M. S., & Gentine, P. (2018). Deep learning to represent subgrid

processes in climate models. Proceedings of the National Academy of Sciences,

115 (39), 9684–9689.

Schneider, T., Lan, S., Stuart, A., & Teixeira, J.

(2017).

Earth system modeling

2.0: A blueprint for models that learn from observations and targeted high-

resolution simulations. Geophysical Research Letters, 44 (24), 12–396.

–34–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

Schneider, T., Teixeira, J., Bretherton, C. S., Brient, F., Pressel, K. G., Sch¨ar, C.,

& Siebesma, A. P.

(2017). Climate goals and computing the future of clouds.

Nature Climate Change, 7 (1), 3–5.

Schultz, M., Betancourt, C., Gong, B., Kleinert, F., Langguth, M., Leufen, L., . . .

Stadtler, S.

(2021).

Can deep learning beat numerical weather prediction?

Philosophical Transactions of the Royal Society A, 379 (2194), 20200097.

Seifert, A., & Rasp, S.

(2020).

Potential and limitations of machine learning for

modeling warm-rain cloud microphysical processes.

Journal of Advances in

Modeling Earth Systems, 12 (12), e2020MS002301.

Sirignano, J., MacArt, J. F., & Freund, J. B.

(2020). Dpm: A deep learning pde

augmentation method with application to large-eddy simulation.

Journal of

Computational Physics, 423 , 109811.

Sirignano, J., & Spiliopoulos, K. (2018). Dgm: A deep learning algorithm for solving

partial diﬀerential equations.

Journal of Computational Physics, 375 , 1339–

1364.

Smagorinsky, J.

(1963). General circulation experiments with the primitive equa-

tions: I. the basic experiment. Monthly Weather Review , 91 (3), 99–164.

Sridhar, A., Tissaoui, Y., Marras, S., Shen, Z., Kawczynski, C., Byrne, S., . . . others

(2021).

Large-eddy simulations with climatemachine v0. 2.0: a new open-

source code for atmospheric simulations on gpus and cpus. Geoscientiﬁc Model

Development Discussions, 1–41.

Stachenfeld, K., Fielding, D. B., Kochkov, D., Cranmer, M., Pfaﬀ, T., Godwin, J.,

. . . Sanchez-Gonzalez, A.

(2021).

Learned coarse models for eﬃcient turbu-

lence simulation. arXiv preprint arXiv:2112.15275 .

Stensrud, D. J. (2009). Parameterization schemes: keys to understanding numerical

weather prediction models. Cambridge University Press.

Thuburn, J., Kent, J., & Wood, N.

(2014).

Cascades, backscatter and conserva-

tion in numerical models of two-dimensional turbulence. Quarterly Journal of

the Royal Meteorological Society, 140 (679), 626–638.

Thuerey, N., Holl, P., Mueller, M., Schnell, P., Trost, F., & Um, K.

(2021).

Physics-based deep learning.

WWW.

Retrieved from https://

physicsbaseddeeplearning.org

Ukkonen, P., Pincus, R., Hogan, R. J., Pagh Nielsen, K., & Kaas, E. (2020). Accel-

–35–

manuscript submitted to Journal of Advances in Modeling Earth Systems (JAMES)

erating radiation computations for dynamical models with targeted machine

learning and code optimization.

Journal of Advances in Modeling Earth Sys-

tems, 12 (12), e2020MS002226.

Um, K., Brand, R., Fei, Y. R., Holl, P., & Thuerey, N.

(2020).

Solver-in-the-loop:

Learning from diﬀerentiable physics to interact with iterative pde-solvers.

In

Advances in neural information processing systems (Vol. 33, pp. 6111–6122).

Vidard, A., Bouttier, P.-A., & Vigilant, F.

(2015). Nemotam: tangent and adjoint

models for the ocean modelling platform nemo. Geoscientiﬁc Model Develop-

ment, 8 (4), 1245–1257.

Vinuesa, R., & Brunton, S. L. (2021). The potential of machine learning to enhance

computational ﬂuid dynamics. arXiv preprint arXiv:2110.02085 .

Vollant, A., Balarac, G., & Corre, C.

(2017).

Subgrid-scale scalar ﬂux modelling

based on optimal estimation theory and machine-learning procedures. Journal

of Turbulence, 18 (9), 854–878.

Weyn, J. A., Durran, D. R., Caruana, R., & Cresswell-Clay, N. (2021). Sub-seasonal

forecasting with a large ensemble of deep-learning weather prediction models.

Journal of Advances in Modeling Earth Systems, 13 (7), e2021MS002502.

Wunsch, C., & Heimbach, P.

(2013).

Dynamically and kinematically consistent

global ocean circulation and ice state estimates.

In International geophysics

(Vol. 103, pp. 553–579). Elsevier.

Xie, C., Wang, J., & Weinan, E. (2020). Modeling subgrid-scale forces by spatial ar-

tiﬁcial neural networks in large eddy simulation of turbulence. Physical Review

Fluids, 5 (5), 054606.

Yuan, Z., Xie, C., & Wang, J. (2020). Deconvolutional artiﬁcial neural network mod-

els for large eddy simulation of turbulence. Physics of Fluids, 32 (11), 115106.

Zanna, L., & Bolton, T.

(2020). Data-driven equation discovery of ocean mesoscale

closures. Geophysical Research Letters, 47 (17), e2020GL088376.

Zanna, L., & Bolton, T.

(2021). Deep learning of unresolved turbulent ocean pro-

cesses in climate models. Deep Learning for the Earth Sciences: A Comprehen-

sive Approach to Remote Sensing, Climate Science, and Geosciences, 298–306.

Zhou, Z., He, G., Wang, S., & Jin, G.

(2019).

Subgrid-scale model for large-eddy

simulation of isotropic turbulent ﬂows using an artiﬁcial neural network. Com-

puters & Fluids, 195 , 104319.

–36–

