2
2
0
2

n
u
J

5

]

O
R
.
s
c
[

2
v
0
2
7
7
0
.
2
0
2
2
:
v
i
X
r
a

Active Uncertainty Reduction for Human-Robot
Interaction: An Implicit Dual Control Approach

Haimin Hu1 and Jaime F. Fisac1

Department of Electrical and Computer Engineering, Princeton University, USA,
{haiminh,jfisac}@princeton.edu

Abstract. The ability to accurately predict human behavior is central to the
safety and efﬁciency of robot autonomy in interactive settings. Unfortunately,
robots often lack access to key information on which these predictions may hinge,
such as people’s goals, attention, and willingness to cooperate. Dual control the-
ory addresses this challenge by treating unknown parameters of a predictive model
as stochastic hidden states and inferring their values at runtime using informa-
tion gathered during system operation. While able to optimally and automatically
trade off exploration and exploitation, dual control is computationally intractable
for general interactive motion planning, mainly due to the fundamental coupling
between robot trajectory optimization and human intent inference. In this paper,
we present a novel algorithmic approach to enable active uncertainty reduction for
interactive motion planning based on the implicit dual control paradigm. Our ap-
proach relies on sampling-based approximation of stochastic dynamic program-
ming, leading to a model predictive control problem that can be readily solved
by real-time gradient-based optimization methods. The resulting policy is shown
to preserve the dual control effect for a broad class of predictive human models
with both continuous and categorical uncertainty. The efﬁcacy of our approach is
demonstrated with simulated driving examples.

Keywords: human-robot interaction, dual control theory, stochastic MPC

1

Introduction

Computing robot plans that account for possible interactions with one or multiple hu-
mans is a challenging task, as the robotic system and human agents may have coupled
dynamics, limited communication capabilities, and conﬂicting interests. To achieve
safety and efﬁciency in human-robot interaction settings, the robot must competently
predict and seamlessly adapt to human behavior. Intent-driven behavior models are
widely used for such predictions: for example, the Boltzmann model of noisily rational
human decision-making [1, 2] assumes that the human is exponentially likelier to take
actions with a higher underlying utility. If the human’s intent is well captured by a given
utility function, the interaction can be modeled as a dynamic game in which the human’s
and the robot’s feedback strategies can be obtained via dynamic programming [3]. How-
ever, typical interaction settings may admit a plethora of a priori plausible intents (e.g.,
corresponding to distinct equilibrium solutions [4] or different human preferences [5]),
which in general cannot be fully modeled, let alone observed, by the robot [6]. The

 
 
 
 
 
 
2

Haimin Hu et al.

robot may seek to represent the human’s intent through a parametric model and then
infer the value of these parameters as hidden states under a Bayesian framework [6–8],
but doing so tractably while planning through interactions is an open problem.

Multi-stage trajectory optimization with closed-loop Bayesian inference can be gen-
erally cast as a stochastic optimal control problem. An important aspect of stochastic
control with hidden states is whether the computed policy generates the so-called dual
control effect [9–11]; that is, in the context of human-robot interaction, whether the
robot actively seeks to reduce the uncertainty about human’s hidden states. Solution
methods for dual stochastic optimal control problems can be categorized into explicit
approaches [5, 12, 13], which reformulate the problem with some form of heuristic
probing, and implicit approaches [10, 14, 15], which directly tackle the control prob-
lem with stochastic dynamic programming. While explicit dual control problems are
in general easier to formulate and solve than their implicit counterparts, designing the
probing term and tuning its weighting factor can be non-trivial and may lead to inconsis-
tent performance. For a comprehensive review of dual control methods we refer to [11].

Contribution: In this paper, we formulate a broad class of interactive planning prob-
lems in the framework of stochastic optimal control and present an approximate solution
method using implicit dual stochastic model predictive control (SMPC). The resulting
policy automatically trades off the cost of exploration and exploitation, allowing the
robot to actively reduce the uncertainty about the human’s hidden states without sac-
riﬁcing expected planning performance. Our proposed SMPC problem supports both
continuous and categorical human uncertainty and can be solved using off-the-shelf
real-time nonlinear optimization solvers. To our knowledge, this is the ﬁrst human-
robot interactive motion planning framework that performs active uncertainty reduction
without requiring an explicit information gathering strategy or objective.

2 Related Work

Human-Robot Interaction as a POMDP. Robotic motion planning problems that in-
volve identiﬁcation of human intentions and behaviors can be modeled as a Mixed
Observability Markov Decision Process [16], a variant of the well-known Partially
Observable Markov Decision Process (POMDP). Although intractable in the general
form, efﬁcient algorithms such as DESPOT [17] and POMCP [18] have been devel-
oped to approximately solve POMDPs. In [19], a dual SMPC method is proposed to
solve moderate-sized POMDPs. The resulting policy naturally exhibits system probing
and automatically trades off exploration and exploitation. When interactions between
the robot and the human are modeled, the POMDP formulation becomes a (usually
intractable) Partially-Observable Stochastic Game (POSG) [5]. Our approach can be
viewed as a new computationally efﬁcient framework for solving human-robot interac-
tion problems cast as POSGs.
Stochastic Model Predictive Control. SMPC has been widely used for robotic motion
planning under uncertainty due to its ability to handle safety-critical constraints and
general uncertainty models. In [20], an SMPC approach is proposed for lane change
assistance of the ego vehicle in the presence of other human-driven vehicles. In [21], a
scenario-based SMPC algorithm is proposed to capture multimodal reactive behaviors

Active Uncertainty Reduction for HRI

3

of uncontrolled human agents. In [8], a provably safe SMPC planner is developed for
human-aware robot planning, which proactively balances expected performance with
the risk of high-cost emergency safety maneuvers triggered by low-probability hu-
man behaviors. However, all those SMPC methods do not produce dual control effect.
In [15], an implicit dual SMPC is proposed for optimal control of nonlinear dynami-
cal systems with both parametric and structural uncertainty. Our paper builds on this
approach to enable active uncertainty reduction for human-robot interaction.
Active Information Gathering. To date, most human-in-the-loop planning methods
follow a “passively adaptive” paradigm. In [4], human-robot interaction is modeled as
a general-sum differential game with human’s equilibrium uncertainty. The robot ﬁrst
infers which equilibrium the humans are operating at and then aligns its own strategy
with the inferred human’s equilibrium solution. Recently, the notion of active infor-
mation gathering, which is conceptually very similar to dual control, has received at-
tention from the human-robot interaction community. In [5], an additional information
gathering term is added to the robot’s nominal objective in a trajectory optimization
framework for online estimating unknown parameters of the human’s state-action value
function. In fact, according to the categorization proposed by [11], the planning for-
mulation in [5] can be classiﬁed as an explicit dual control approach, which requires
heuristic design of a probing mechanism and weighing the relative importance between
optimizing the expected performance and reducing the uncertainty of human’s unknown
parameters. On the contrary, we propose an implicit dual control approach, which au-
tomatically balances performance with uncertainty reduction, thus not requiring design
of a heuristic information gathering mechanism.

3 Preliminaries

Human-Robot Joint System. We consider a class of discrete-time input-afﬁne dy-
namics that capture the interaction between a human individual or group and a robotic
system,

xt+1 = f (xt) + BR(xt)uR

t + BH (xt)uH
t + dt,
t ∈ U R ⊆ RmR and uH

(1)
t ∈ U H ⊆ RmH are
where xt ∈ Rn is the joint state vector, uR
the control vectors of the robot and the human, respectively, f : Rn → Rn is a nonlin-
ear function that describes the autonomous part of the dynamics, BR : Rn → Rn×mR
and BH : Rn → Rn×mH are control input matrices that can depend on the state, and dt
is an additive uncertainty term representing external disturbance inputs (e.g. wind) and
modeling error. For simplicity, we further assume that dt ∼ N (0, Σd) is a zero-mean
i.i.d. Gaussian random variable.
Human Action Parameterization. In this paper, we parametrize the human’s action at
each time t as a stochastic policy:

uH
t

:= (cid:80)nθ

i=1 θM

i uM
i

,

subject to

t ∈ U H ,
uH

(2)

which is a linear combination of stochastic basis policies uM
(θM
modes M that take values from a ﬁnite set M. We deﬁne each basis policy uM

) ∈ Rnθ . We further allow each basis policy uM
i

i with parameter θM :=
to have different
i with the

2 , . . . , θM
nθ

1 , θM

4

Haimin Hu et al.

Illustration of a scenario tree with N d = 2 dual control time steps and N e = 4 ex-
Fig. 1:
ploitation steps. The human-driven vehicle and autonomous car are plotted in white and yellow,
respectively. The hidden state θM is modeled as a 2D Gaussian random variable, which is then
projected onto a 1-simplex by a softmax operation. The contour plots display level sets of the
cumulative distribution function (CDF) of θM . Uncertainty is less signiﬁcantly reduced in the
case where the human prefers the left lane (upper branch), since their behavior is less inﬂuenced
by the robot’s (probing) actions.

“noisily rational” Boltzmann model from cognitive science [1]. Under this model, the
human picks actions according to a probability distribution:

i ∼ p (cid:0)uM
uM

i

| x, uR; M (cid:1) =

(cid:82)

i )
i (x,uR,uM

i (x,uR,˜uM

i )d˜uM
i

e−QM
i ∈U H e−QM
i (x, uR, uM

˜uM

,

(3)

for all i = 1, 2, . . . , nθ and M ∈ M. Here, QM
i ) is the human’s state-action
value (or Q-value) function associated with the i-th basis function and mode M . This
model assumes that, for a pair of ﬁxed (i, M ), the human is exponentially likelier to
pick an action that maximizes the state-action value function.

Remark 1. Our approach is agnostic to the concrete methods for determining the hu-
man’s state-action value function QM
i ), model parameter θM and mode M ,
i (x, uR, uM
which are usually speciﬁed by the system designer based on domain knowledge or
learned from prior data. Goal-driven models of human motion are well-established in
the literature. See for example [2, 5]. We provide two examples below.

Example 1. Consider the highway driving scenario depicted in Fig. 1, involving an au-
tonomous car (R) and a human-driven vehicle (H), whose action is parameterized as

D πM
D

(cid:0)xt, uR

t

(cid:1) + θM

F πM
F

(cid:0)xt, uR

t

(cid:1) ,

subject to

t ∈ U H ,
uH

where θM
F capture the level of distraction and focus of the human, respectively.
A focused human accounts for the safety of the joint system (e.g. avoiding and making

t = θM
uH
D and θM

Active Uncertainty Reduction for HRI

5

room for the robot when it attempts to merge in front of the human), while a distracted
human does not. Modeling the human’s level of distraction as a continuum is motivated
by recent work [22], which differs from ours in that they assume an additive structure
in the human’s reward (objective) function rather than their policy. Nonetheless, per the
maxim of “all models are wrong, but some are useful” [23], we will show in Section 6
that our choice of human predictive model is useful for planning purposes and leads
to a policy that outperforms the state-of-the-art baseline. Further, the human has two
distinct modes, namely preferring to drive in the left lane or in the right lane, i.e., M ∈
{l, r}. Given a joint state xt, for each i and M we compute the human’s game-theoretic
state-action value function using the iterative linear-quadratic game method [24], which
yields an approximate local feedback Nash equilibrium solution.

Example 2. In the second example, we use parameter θM := (θM
N C) , which cap-
tures the human’s level of cooperativeness. A cooperative (C) human also optimizes
for the robot’s objective while a non-cooperative (NC) human does not. We deﬁne the
discrete modes as different interactive behaviors of the human toward the robot, similar
to [7, 16]. Speciﬁcally, the human’s state-action value function is deﬁned as

C , θM

(cid:16)

(cid:16)

(cid:16)

QM

i =






QM
i

QM
i

QM
i
QM
i

xt, uR,Nash
t

(xt), uM
i

(cid:17)

(xt), uM
i
(cid:17)

(xt), uM
i

xt, uR,worst
t

xt, uR,best
t
(cid:1) ,
(cid:0)xH
t , uM
i

,

,

if M = N (Nash),

if M = p (protected),

(cid:17)

,

if M = w (wishful),

if M = o (oblivious).

is a local feed-
, and the
, respectively; the last mode follows the same as-

In the ﬁrst three modes, the human assumes that the robot’s control uR
t
back Nash equilibrium solution [24], the worst-case one that minimizes QM
i
best-case one that maximizes QM
i
sumption as in [6, 8] that the human ignores the presence of other agents.
Inferring Model Parameter and Mode. In general, parameter θM and mode M in hu-
man action model (2) are hidden states that are unknown to the robot. Therefore, they
can only be inferred from past observations. To address this, we deﬁne the informa-
tion vector It := (cid:2)xt, uR
(cid:3) as the collection of all information that is causally
observable by the robot at time t ≥ 0, with I0 = [x0]. We then deﬁne the belief
(cid:1) as the joint distribution of (θM , M ) conditioned on It, and
state bt := p (cid:0)θM , M | It
b0 := p (cid:0)θM , M (cid:1) is a given prior distribution. When the robot receives a new obser-
vation xt+1 ∈ It+1, the current belief state bt is updated using the recursive Bayesian
inference:

t−1, It−1

p(θM

− | It+1; M ) =

p(xt+1 | uR

t , It; θM , M )p(θM | It; M )
p(xt+1 | uR
t , It; M )

,

p(M− | It+1) =

b−
t+1 := p(θM
bt+1 = gt(b−

p(xt+1 | uR

p(xt+1 | uR
− , M− | It+1) = p(θM
t+1) := (cid:82) p(θM , M | ˜θM

t , It; M )p(M | It)
t , It)
− | It+1; M )p(M− | It+1),
− , ˜M−)p(˜θM

− , ˜M− | It+1)d˜θM

,

− d ˜M−.

(4a)

(4b)

(4c)

(4d)

6

Haimin Hu et al.

where p(θM , M | ˜θM
dynamics. Compactly, we can rewrite (4a)-(4d) as a dynamical system,

− , ˜M−) is a transition model and gt(·) is the belief state transition

bt+1 = g(bt, xt+1, uR

t ).

(5)

Unfortunately, system (5) in general does not adopt an analytical form beyond one-
step evolution. Even if the prior distribution bt is a Gaussian, the posterior bt+1 ceases
to be a Gaussian since the human’s action uH
t deﬁned by (2) and (3) (which in turn
affects the observation xt+1) is generally non-Gaussian, thus precluding the use of the
conjugate-prior properties of Gaussian distributions. In Section 5, we will introduce a
computationally efﬁcient method to propagate the belief state dynamics approximately.

4 Problem Statement

Canonical Human-in-the-Loop Planning Problem. We now deﬁne the central prob-
lem we want to solve in this paper: the canonical human-in-the-loop planning problem,
which is formulated as a stochastic ﬁnite-horizon optimal control problem as follows:

πR

min
[0:N −1]

E
(θM ,M )∼b[0:N −1],
[0:N −1]∼(2),d[0:N −1]
s.t. x0 = ˆxt, b0 = ˆbt,

uH

N −1
(cid:88)

k=0

(cid:96)R (cid:0)xk, πR

k (xk, bk)(cid:1) + (cid:96)R

F (xN )

(6a)

(6b)

xk+1 = f (xk) + BRπR
bk+1 = g (cid:0)bk, xk+1, πR
xk /∈ F,

k (xk, bk) + BH uH
k (xk, bk)(cid:1) ,

k + dk, ∀k = 0, . . . , N − 1 (6c)
∀k = 0, . . . , N − 1 (6d)
∀k = 0, . . . , N
(6e)

where ˆxt and ˆbt are the state measured and belief state maintained at time t, (cid:96)R :
Rn × U R → R≥0 and (cid:96)R
F : Rn → R≥0 are designer-speciﬁed stage and terminal cost
function, πk(xk, bk) is a causal feedback policy [10, 11] that leverages the (yet-to-be-
acquired) knowledge of future state xk and belief state bk, and F ⊆ Rn is a failure set
that the state is not allowed to enter.

In theory, problem (6) can be solved using stochastic dynamic programming [25].
(xk, bk) can be

An optimal robot’s value function Vk(xk, bk) and control policy πR,∗
obtained backwards in time using the Bellman recursion,

k

Vk(xk, bk) = min

πk(xk,bk)

(cid:96)R(xk, uR

k ) +

E
(θM ,M )∼bk,
uH
k ∼(2),dk

[Vk+1(xk+1, bk+1) | Ik]

(7)

s.t. (6c) − (6e)

with terminal condition VN (xN , bN ) = (cid:96)R
Dual Control Effect. Value function Vt(xt, bt) obtained by solving (7) depends on fu-
0 (ˆxt, ˆbt) the
ture belief states bk (k > t), thus giving the optimal policy uR,∗
ability to affect future uncertainty of the human quantiﬁed by the belief states. There-
fore, the optimal policy uR,∗
of (7) possesses the property of dual control effect, deﬁned

F (xN ).

:= πR

t

t

Active Uncertainty Reduction for HRI

7

formally in Deﬁnition 1. Due to the principle of optimality [25], the policy achieves an
optimal balance between optimizing the robot’s expected performance objective (6a)
and actively reducing its uncertainty about the human. In other words, the optimal pol-
icy of (7) automatically probes the human agents to reduce their associated uncertainty
only to the extent that doing so improves the robot’s expected closed-loop performance.

Deﬁnition 1 (Dual Control Effect [9–11, 26]). A control input has dual control effect
if it can affect, with nonzero probability, (a) at least one rth-order (r ≥ 2) central
moment of a hidden state variable in a metric space, or (b) the entropy of a categorical
hidden state variable.

Complication and Approximation of Dual Control. Unfortunately, (7) is computa-
tionally intractable in all but the simplest cases, mainly due to nested optimization of
robot’s action and and computing the conditional expectation. The expectation term in
(7) can be approximated to arbitrary accuracy with quantization of the belief states,
which, however, leads to exponential growth in computation, i.e. the issue of curse of
dimensionality [25]. It is for those reasons that approximate methods are mainly used to
solve dual control problems. Approximate dual control can be categorized into: explicit
approaches, e.g. [5, 12, 13] that simplify the original stochastic optimal control problem
by artiﬁcially introducing probing effect or information gathering objectives to the con-
trol policy, and implicit approaches, e.g. [10, 14, 15] that rely on direct approximation
of the Bellman recursion (7). The approach we take in this paper is a scenario-based
implicit dual control method, which is detailed in the next section. The main advan-
tage of using the implicit dual control approximation is that the automatic exploration-
exploitation trade-off of the policy is naturally preserved in an optimal sense [11, 19].

5 Active Uncertainty Reduction for Human-Robot Interaction

In this section, we describe an implicit dual control approach towards approximately
solving the canonical human-in-the-loop planning problem (6). We start by presenting
two approximation schemes for propagating the belief state dynamics and computing
the expectation in Bellman recursion (7), which are necessary for reformulating (7) as
a real-time solvable SMPC problem. The formulation and properties of our proposed
SMPC problem are shown towards the end of this section. See Fig. 1 for an example
solution of our proposed SMPC problem.
Tractable Reformulation of Belief State Dynamics. We start by deriving a tractable
recursive update rule for the belief state dynamics (5) by approximating the human’s
action model (2), for a given (M, θM ), as a Gaussian distribution. The technical tool
we rely on is the Laplace approximation [27]. Precisely, the conditional probability
distribution of each human’s basis policy uM

i deﬁned in (3) is approximated as:

i ∼ p (cid:0)uM
uM

i

| xt, uR

t ; M (cid:1) ≈ N (cid:0)µM

i (xt, uR

t ), ΣM

where the mean function is µM

i (xt, uR

covariance function is ΣM

i (xt, uR

t ) = ∇2

t ) = arg maxuM
(cid:0)xt, uR
QM
i

i ∈U H QM
t , uM
i

uM
i

(8)

t )(cid:1) ,

i (xt, uR
(cid:0)xt, uR
i
(cid:1)−1(cid:12)
(cid:12)
(cid:12)uM

t , uM
i

i =µM

i (xt,uR
t )

(cid:1) and the
. The

8

Haimin Hu et al.

intuition behind the above Laplace approximation scheme is that the Gaussian distri-
bution obtained in (8) centers around the mode of the original basis policy distribution
p (cid:0)uM
i (xt, uR
t ), which corresponds to the perfectly rational hu-
man action associated with θM
. Hence, the overall human action distribution, condi-
i
tioned on θM and M , is given by:

t , M (cid:1), i.e. µM

| xt, uR

i

uM
t ∼ N

(cid:16)(cid:80)nθ

i=1 θM

i µM

i (xt, uR

t ), (cid:80)nθ

i=1

(cid:0)θM

i

(cid:1)2

ΣM

i (xt, uR
t )

(cid:17)

.

(9)

We subsequently lift the requirement that uH
distributed during belief propagation, and we use projected uH
we can rewrite the human-robot joint system (1) as

t ∈ U H in order to keep uH

t normally
t for state evolution. Now,

xt+1 = BH (xt)U M (xt, uR

t )θM + f (xt) + BR(xt)uR

t ) := (cid:2)µM

t ) µM
where U M (xt, uR
state- and input-dependent matrix, and the combined disturbance term ¯dM
t
mean Gaussian, whose covariance is given by

t ) . . . µM
nθ

2 (xt, uR

1 (xt, uR

(xt, uR

t + ¯dM
t ,
(10)
t )(cid:3) is a deterministic
is a zero-

Σ

¯dM
t (xt, uR

t ; θM ) := Σd + (cid:80)nθ

i=1

(cid:0)θM

i

(cid:1)2

BH (xt)ΣM

i (xt, uR

t )BH (xt)

(cid:62)

.

(11)

Note that even if (10) is linear in θM , dependence of covariance matrix Σ ¯dM
t on θM , as
shown in (11), still prohibits updating the belief states in closed-form. To this end, we
approximate covariance Σ ¯dM
t by ﬁxing θM with some estimated value ¯θM , which can
be obtained, for example, from solutions of the last run or roll-out-based simulations.

Now, we can readily propagate approximate belief state dynamics (5) efﬁciently
in closed-form, leveraging the conjugate prior property of Gaussian distributions. Cru-
cially, the above approximate belief state updates preserve the dual control effect. To
verify this claim, we examine the covariance of the updated conditional distribution
p(θM

− | It+1; M ):

θM
t+1 = ΣθM
−

t +

Σ

(cid:20)
(cid:0)BH U M (xt, uR

t )(cid:1)(cid:62)

−1

¯dM
t

Σ

(xt, uR

t )BH U M (xt, uR
t )

(cid:21)−1

,

(12)

t

). From the above we can

, ΣθM
t
t affects the updated covariance matrix Σ

whose prior distribution is p(θM | It; M ) = N (µθM
θM
clearly see that the robot’s control uR
−
t+1, and
all future covariance matrices implicitly by affecting future states xk (k > t + 1) via
the Bellman recursion, hence producing dual control effect for θM according to Def-
inition 1. Dual control effect for M can be similarly veriﬁed via (4b). We denote the
approximate belief state dynamics as bt+1 = ˜g(bt, xt+1, uR
Implicit Dual Control using Scenario Trees. In this section, we propose an approx-
imate solution method for the canonical human-in-the-loop planning problem (6) us-
ing scenario-tree-based stochastic model predictive control (ST-SMPC) [11, 28], which
yields a control policy with dual control effect. The key idea of ST-SMPC is to ap-
proximate the expectation in Bellman recursion (7) based on uncertainty samples, i.e.
quantized belief states. This leads to a scenario tree that allows us to roll out (7) as a
deterministic ﬁnite-horizon optimal control problem, which can be readily solved by

t ).

Active Uncertainty Reduction for HRI

9

gradient-based algorithms. Since our approach hinges on directly approximating Bell-
man recursion (7), it can be understood as an implicit dual control method [11].

n , ¯dM

n | Ip(n); Mn)P ( ¯dM

n and Mn. Here, recall that ¯dM

We denote a node in the scenario tree as n, whose time, state and belief state are
denoted as tn, xn and bn, respectively. Similarly, the uncertainty samples of the node
are θM
n is the combined disturbance deﬁned in (11),
which implicitly incorporates a sample of the human’s action. The set of all nodes is de-
ﬁned as N . We deﬁne the transition probability from a parent node p(n) to its child node
(cid:1) . Subsequently, the
n as ¯Pn := P (θM
path transition probability of node n, i.e. the transition probability from the root node
n0 to node n can be computed recursively as Pn := ¯Pn · ¯Pp(n) · · · ¯Pn0 . In order to
quickly compute the conditional probabilities of (θM
n ) and avoid online sampling
(i.e. during the optimization), we use an ofﬂine sampling procedure, leveraging the
fact that they are Gaussian random variables, similar to what is done in [29]. We ﬁrst
generate samples ofﬂine from the standard Gaussian distribution. Then, during online
optimization, these samples are transformed using the analytical mean and covariance:

n | Ip(n); Mn)P (cid:0)Mn | Ip(n)

n , ¯dM

n = µθM
θM
(cid:16) ˜Σ

¯dM
n =

n (xn, uR

n ) +

(cid:16)

ΣθM

n (xn, uR
n )

(cid:17)1/2

¯dM
n (xn, uR
n )

(cid:17)1/2

¯dM,o
n .

θM,o
n

,

(13a)

(13b)

Unlike existing ST-SMPC methods for human-robot interaction, such as [8, 20], our
scenario tree has both state- and input-dependent uncertainty realizations (via trans-
formations (13a) and (13b)) and path transition probabilities (via Bayesian update of
M in (4b)). Therefore, the nodes can move in response to predicted states and inputs
during online optimization. Fundamentally, it is this feature that allows the robot to
actively reduce the human-related uncertainty via the dual control effect, and capture
mutual responses between the human and the robot.

Remark 2. In order to alleviate the exponential growth of complexity associated with
the scenario tree, only a small set of M , θM,o
are sampled. A scenario pruning
mechanism is introduced in [8] for static scenario trees, which can be used here as
a heuristic to prune branches based on initial guesses. Principled pruning for scenario
trees involving state- and input-dependent samples, however, remains an open question.

and ¯dM,o
n

n

Exploitation Steps. In order to alleviate the computation challenge caused by the ex-
ponential growth of nodes in the scenario tree, we can stop branching the tree at a stage
N d < N , which we refer to as the dual control horizon. Subsequently, the remaining
N e := N − N d stages become the exploitation horizon, where each scenario is ex-
tended without branching and the belief states are only propagated with the transition
dynamics gt(·) deﬁned in (4d), corresponding to a non-dual SMPC problem. Thanks
to the scenario tree structure, control inputs of the exploitation steps still preserve the
causal feedback property, allowing the robot to be cautious and “passively adaptive” to
future human’s uncertainty realizations.
Overall ST-SMPC Problem. Given a scenario tree deﬁned by node sets Nt, we can
roll out the Bellman recursion (7) based on uncertainty samples in the tree, leading to a

10

Haimin Hu et al.

ST-SMPC problem formulated as

min
UR
t

(cid:88)

˜n∈Nt\Lt

P˜n(cid:96)R(x˜n, uR

˜n ) +

(cid:88)

˜n∈Lt

P˜n(cid:96)R

F (x˜n, b˜n)

s.t. xn0 = ˆxt, bn0 = ˆbt,

x˜n = f (xp(˜n)) + BR(xp(˜n))uR

p(˜n) + BH (xp(˜n))uH

˜n + ¯dM
˜n ,

(cid:17)

,

p(˜n)

(cid:16)

bp(˜n), x˜n, uR
(cid:1) ,

b˜n = ˜g
b˜n = gt (cid:0)bp(˜n)
uH
˜n = U M ˜n (xp(˜n), uR
uR
˜n ∈ U R,
x˜n /∈ F,

p(˜n))θM

˜n , uH

˜n ∈ U H , (13a), (13b),

∀˜n ∈ Nt \ {n0}

∀˜n ∈ N d
t
∀˜n ∈ N e
t
∀˜n ∈ Nt \ {n0}

∀˜n ∈ Nt \ Lt
∀˜n ∈ Nt

(14)
where Lt is the set of all leaf nodes, (i.e. ones that do not have a descendant), N d
t
t are the set of dual control and exploitation nodes, respectively, and set UR
and N e
:=
t
˜n ∈ RmR : ˜n ∈ Nt \ Lt} is the collection of robot’s control inputs associated
{uR
with all non-leaf nodes. Problem (14) is a nonconvex trajectory optimization problem,
which can be solved using general-purpose nonconvex solvers such as SNOPT [30].
The optimal solution UR,∗
to (14) is implemented in a receding horizon fashion, i.e.
t
ID-SMPC(ˆxt, ˆbt) := uR,∗
πR
n0 .
Theorem 1. The policy πR

Proof. From (12), the robot’s control uR
Therefore, the policy πR
from (4b), it can be seen that uR
over M , and thereby its entropy, implying dual control effect for M per Deﬁnition 1.

ID-SMPC(·, ·) obtained by solving (14) has dual control effect.
t can affect the covariance of the belief over θM .
ID-SMPC has dual control effect for θM per Deﬁnition 1. Similarly,
t can affect all components of the categorical distribution

Safety and Feasibility. Since the ST-SMPC problem (14) is formulated as a standard
scenario program, safety assurances can be obtained based on existing safe control
methods. In the following, we brieﬂy discuss three ways to enforce safety, i.e. the
human-robot joint system in closed-loop with the policy of (14) satisfying x /∈ F.

– Robust. Our prior work [8] provides robust safety and recursive feasibility guaran-
tees for ST-SMPC based on shielding, a “least-restrictive” supervisory safety ﬁlter
designed purely based on sets F, U H , U R, and is agnostic to the speciﬁc predictive
model of human motion. As a special case, [29] provides closed-loop safety guar-
antees by projecting the ST-SMPC policy onto the set of backup control policies
associated with a robust controlled-invariant set. Note that an additional assumption
of bounded disturbance is needed in order to obtain robust safety guarantees.

– Probabilistic. We can replace (6e) with chance constraints to obtain probabilistic
safety guarantees, i.e. P [x ∈ F] ≤ β, where β is the tolerance level, provided
that x ∈ F can be written as a set of inequality constraints. An analytical bound
of β and probabilistic feasibility guarantee are established for chance-constrained
ST-SMPC problems with dynamic obstacles in [31].

Active Uncertainty Reduction for HRI

11

Fig. 2: Performance and safety trade-off of Examples 1 and 2. Each data point is obtained based
on a particular design of the planner and the statistical data of 100 trials with different random
seeds. The collision rate of ID-SMPC is always below 15% even for the least conservative design.

– Soft constrained. Soft constraint is a simple, yet effective technique widely used by
MPC to enforce safety of the closed-loop system. The advantages of soft constraints
are that they are easy to implement, efﬁcient to optimize, and feasibility can be
guaranteed. In this paper, we tailor the soft constrained MPC approach in [32] for
ST-SMPC, which relaxes the original hard constraints x˜n /∈ F in (14) with slack
variables for each node ˜n ∈ Nt. Although satisfaction of x /∈ F can no longer
be guaranteed for the closed-loop system, we show in Section 6 that our approach
signiﬁcantly reduces the collision rate comparing to the baselines (using the same
soft constrained MPC approach) due to active reduction of the human’s uncertainty.

6 Simulation Results

We evaluate our proposed implicit dual scenario-tree-based SMPC (ID-SMPC) planner
on simulated driving scenarios, where the human-driven vehicles are simulated using a
game-theoretic model synthesized with [24]. The open-source code is available online.1

Baselines. We compare our proposed ID-SMPC planner against four baselines:

– Explicit dual SMPC (ED-SMPC) planner, which augments the stage cost (cid:96)R in (6a)
with an information gain term λ(H(bk) − H(bk+1)) proposed by [5]. Here, λ > 0
is a ﬁne-tuned weighting factor.

– Non-dual scenario-tree-based SMPC (ND-SMPC) planner, which is based on solv-
ing (6) with a scenario tree that does not propagate belief states with an observation
model (so the resulting policy does not have dual control effect). A similar scenario
program is also considered in [8, 20, 28].

– Certainty-equivalent MPC (CE-MPC) planner, which is based on solving (6) with

the certainty-equivalence principle [11, 15].

1 https://github.com/SafeRoboticsLab/Dual Control HRI

0.60.811.21.41.61.822.21040%10%20%30%40%1.41.61.822.22.42.62.831040%10%20%30%40%50%12

Haimin Hu et al.

Fig. 3: Simulation snapshots of Example 1. Longitudinal positions are shown in relative coor-
dinates with pH
x = 0. The left, middle, and right columns display trajectories for t = [0, 3] s,
t = [3, 5] s, and the remainder of the trajectories, respectively. The bottom ﬁgure shows proba-
bility P (M = l) for all four planners overtime. Our proposed ID-SMPC planner yielded a clean
and sharp overtaking maneuver of the robot while the non-dual planners led to unsafe trajectories.

– Iterative linear-quadratic game with inference-based strategy alignment (ISA-iLQ)
planner, proposed by [4] and originally developed in [24]. An input projector is
used to enforce uH ∈ U H and uR ∈ U R.

ED-SMPC is a dual control planner while the other three do not generate dual control
effect. All planners use the same quadratic cost functions (cid:96)R and (cid:96)R
F .
Simulation Setup. All planners are equipped with the same human intent inference
scheme. Vehicle and pedestrian dynamics are described by the kinematic bicycle model
in [3] and the unicycle model in [24], respectively, both discretized with a time step
of ∆t = 0.2 s. The interactive human agents are simulated using the iterative linear-
quadratic game method [24] with a game horizon of 3 s. All simulations are performed
using MATLAB and YALMIP [33] on a desktop with an Intel Core i7-10700K CPU.
All nonlinear MPC problems are solved with SNOPT [30]. The average solving time
for ISA-iLQ, CE-MPC, ND-SMPC, ED-SMPC, and ID-SMPC at each time step are
1.92 s, 0.333 s, 0.184 s, 0.481 s, and 0.478 s, respectively, when the robot is interacting
with one human agent (Section 6.1 and 6.2), and 19.3 s, 0.775 s, 1.37 s, 4.17 s, and
4.03 s, respectively, in the presence of three human agents (Section 6.3). The increase
in solving time (for ST-SMPC problems) is due to the exponential growth of scenar-
ios when the number of agents increases. This highlights an opportunity to improve
time efﬁciency in multi-agent settings leveraging sparse scenario tree methods [8] and
distributed MPC techniques.

Active Uncertainty Reduction for HRI

13

Fig. 4: One trial of Example 1, where the robot successfully overtook the human-driven vehicle
in 6 s using ID-SMPC. CE-MPC and ISA-iLQ led to loss of safety and liveness, respectively.

Metrics. To measure the planning performance, we consider the following two metrics:

– Closed-loop cost, deﬁned as J R

cl := (cid:80)Tsim

t=0 (cid:96)R(xt, uR

t ), where Tsim is the simulation

horizon, and x[0:Tsim], u[0:Tsim] are the executed trajectories (with replanning).

– Collision rate, deﬁned as Ncoll/Ntrial × 100%, where Ncoll is the number of trials

that a collision happens, i.e. xt ∈ F, and Ntrial is the total number of trials.

Hypotheses. We make three hypotheses, which are supported by our simulation.

– H1 (Performance and Safety Trade-off). Dual-control planners result in a better

performance-safety trade-off than non-dual baselines.

– H2 (Safety and Liveness). Insufﬁcient knowledge about human hidden states (due
to lack of dual control effect) can cause loss of safety (e.g. collisions) and/or live-
ness (e.g. failing to overtake the human).

– H3 (Implicit vs Explicit Dual Control). Explicit dual control is less efﬁcient than

its implicit counterpart, even with ﬁne tuning.

6.1 Objective and Awareness Uncertainty (Example 1)

The performance-safety trade-off curve for Example 1 plotted in Fig. 2 validates H1.
Here, we design each planner with a set of ﬁne-tuned parameters, e.g. weight of the
collision avoidance cost and acceleration limits. For a given planner design, we sim-
ulate the scenario 100 times, each with a different random seed, which affects three
uncertainty sources: initial conditions, trajectories of the hidden states, and additive
disturbances. Although the ED-SMPC policy also manages to achieve a low collision
rate thanks to its ability to actively reduce the human uncertainty, its overall closed-loop
performance is consistently inferior to that of ID-SMPC, which validates H3.

Trajectory snapshots and evolution of P (M = l) of one simulation trial are shown
in Fig. 3. The priors are chosen as P (M = l) = 1 and θl, θr ∼ N ((0.5, 0.5), 5I). Un-
like non-dual control planners, ID-SMPC controlled the robot to approach the human-
driven vehicle along the center of the road, allowing the robot to informatively probe
the human—which resulted in a more accurate prediction of M (bottom)—and guiding
the robot through a region from which collisions can be avoided more easily. Indeed, as
the human’s hidden state M switched from r to l at t = 6 s, the robot using ID-SMPC
executed a sharp right turn and successfully avoided colliding with the human. The ED-
SMPC planner, although effective at reducing the uncertainty at the beginning, failed to
recognize that overtaking the human from the right would have resulted a more efﬁcient
trajectory. All non-dual control planners, even with replanning, caused a collision with

14

Haimin Hu et al.

Fig. 5: Simulation snapshots of Example 2 using the proposed ID-SMPC planner. The ground-
truth hidden states are ﬁxed throughout the simulation. In the orange blocks we display the robot’s
running belief p(M | It) and maximum a posteriori mean of θM with t = 2.6 s. The robot was
able to quickly identify the human’s hidden states and planned a collision-free trajectory in all
four trials, accounting for the anticipated uncertainty reduction and interactions with the human.

the human due to insufﬁcient knowledge about M . It is also worth noticing that even if
ID-SMPC uses ND-SMPC solutions for initialization, their closed-loop behaviors are
vastly different, which essentially comes from the dual control effect.

In Fig. 4, we examine another simulation trial of Example 1. Ground truth values
of the hidden states are θ = 1 and M = l, respectively. Using ID-SMPC, the robot
was able to safely overtake the human-driven car in 6 s. However, using the ISA-iLQ
planner, the robot failed to overtake the human within 10 s, which is the simulation
horizon. Due to lack of dual control effort, the robot was stuck behind the human,
unaware of the human’s willingness to make room for the robot. Fig. 4 also shows
an unsafe trajectory generated with the CE-MPC planner. Results shown by Fig. 3 and
Fig. 4 demonstrate that with dual control effort, the robot gains better safety and liveness
when interacting with the human, which supports our hypothesis H2.

6.2 Behavioral and Cooperative Uncertainty (Example 2)

Next, we consider an uncontrolled trafﬁc intersection scenario with the human uncer-
tainty introduced in Example 2. Trajectory snapshots of four simulation trials with dif-
ferent hidden states are shown in Fig. 5. We chose uninformative prior distributions
p(M | I0) = (cid:2)0.25 0.25 0.25 0.25(cid:3) and θM ∼ N ((0.5, 0.5), 5I) for all M ∈ M.

6.3 Multi-Agent Case Study

Finally, we apply ID-SMPC to the same intersection scenario as in Example 2 involv-
ing three human agents: two human-driven vehicles and a pedestrian. The uncertainty
for the human-driven vehicles and the pedestrian is modeled with Example 1 and 2,
respectively. Trajectory snapshots of one representative trial is shown in Fig. 6.

Active Uncertainty Reduction for HRI

15

Fig. 6: Simulation snapshots and estimated hidden states of a multi-agent interaction scenario
with a pedestrian (H1) and two human-driven vehicles (H2 and H3) using the ID-SMPC planner.

7 Discussion and Future Work

We have introduced an implicit dual control approach towards active uncertainty reduc-
tion for human-robot interaction. The resulting policy improves safety and efﬁciency
of human-in-the-loop motion planning via a tractable approximation to the Bellman re-
cursion of a dual control problem, automatically achieving an efﬁcient balance between
improving expected performance and eliciting information on future human behavior.
We see our work as an important step towards a broader class of methods that can han-
dle general parametrizations of human behavior, including the effect of robot decisions
on the human’s hidden state. While this paper focuses on the robot’s own performance,
our approach may be adapted to account for social coordination and altruism [22] in
cooperative human-robot settings. Finally, there is an open opportunity to derive formal
probabilistic or worst-case safety guarantees based on results established in [8, 31].

References

[1] R. D. Luce. “Individual Choice Behavior”. Oxford, England: John Wiley, 1959.
[2] B. D. Ziebart et al. “Maximum entropy inverse reinforcement learning.” AAAI. 2008.
[3]

J. F. Fisac et al. “Hierarchical game-theoretic planning for autonomous vehicles”. IEEE
International Conference on Robotics and Automation (ICRA). 2019.

[4] L. Peters et al. “Inference-based strategy alignment for general-sum differential games”.
19th International Conference on Autonomous Agents and Multi Agent Systems. 2020.
[5] D. Sadigh et al. “Planning for cars that coordinate with people: leveraging effects on
human actions for planning and active information gathering over human internal state”.
Autonomous Robots 42.7 (2018).
J. F. Fisac et al. “Probabilistically Safe Robot Planning with Conﬁdence-Based Human
Predictions”. Robotics: Science and Systems. 2018.

[6]

[7] R. Tian et al.

“Safety Assurances for Human-Robot Interaction via Conﬁdence-aware

Game-theoretic Human Models”. CoRR abs/2109.14700 (2021).

[8] H. Hu, K. Nakamura, and J. F. Fisac. “SHARP: Shielding-aware robust planning for safe

and efﬁcient human-robot interaction”. IEEE Robotics and Automation Letters (2022).

16

Haimin Hu et al.

[9] A. A. Feldbaum. “Dual control theory. I”. Avtomatika i Telemekhanika (1960).
[10] Y. Bar-Shalom and E. Tse. “Dual effect, certainty equivalence, and separation in stochastic

control”. IEEE Trans. Autom. Control 19.5 (1974).

[11] A. Mesbah. “Stochastic model predictive control with active uncertainty learning: A sur-

vey on dual control”. Annual Reviews in Control 45 (2018).

[12] T. A. N. Heirung, B. Foss, and B. E. Ydstie. “MPC-based dual control with online exper-

iment design”. Journal of Process Control 32 (2015).

[13] R. Tian et al. “Anytime game-theoretic planning with active reasoning about humans’
latent states for human-centered robots”. IEEE International Conference on Robotics and
Automation (ICRA). 2021.
[14] E. D. Klenske and P. Hennig.

“Dual control for approximate bayesian reinforcement

learning”. The Journal of Machine Learning Research 17.1 (2016).

[15] E. Arcari et al. “Dual stochastic MPC for systems with parametric and structural uncer-

tainty”. Learning for Dynamics and Control. 2020.

[16] T. Bandyopadhyay et al. “Intention-aware motion planning”. Algorithmic Foundations of

Robotics X. Springer, 2013.

[17] N. Ye et al. “DESPOT: Online POMDP Planning with Regularization”. Journal of Arti-

ﬁcial Intelligence Research 58.1 (2017).

[18] D. Silver and J. Veness. “Monte-Carlo planning in large POMDPs”. Neural Information

Processing Systems. 2010.

[19] M. A. Sehr and R. R. Bitmead. “Tractable dual optimal stochastic model predictive con-
trol: An example in healthcare”. IEEE Conference on Control Technology and Applica-
tions (CCTA). 2017.

[20] G. Schildbach and F. Borrelli. “Scenario model predictive control for lane change assis-

tance on highways”. IEEE Intelligent Vehicles Symposium (IV). 2015.

[21] Y. Chen et al. “Interactive Multi-Modal Motion Planning With Branch Model Predictive

Control”. IEEE Robotics and Automation Letters 7.2 (2022).

[22] B. Toghi et al. “Cooperative autonomous vehicles that sympathize with human drivers”.

IEEE Transactions on Intelligent Vehicles (IEEE TIV) (2021).

[23] G. E. Box. “All models are wrong, but some are useful”. Robustness in Statistics (1979).
[24] D. Fridovich-Keil et al. “Efﬁcient iterative linear-quadratic approximations for nonlinear
multi-player general-sum differential games”. IEEE International Conference on Robotics
and Automation (ICRA). 2020.

[25] R. Bellman. “Dynamic programming”. Science 153.3731 (1966).
[26] O. Hijab. “Entropy and dual control”. IEEE Conference on Decision and Control. 1984.
[27] A. D. Dragan and S. S. Srinivasa. “Formalizing Assistive Teleoperation”. Robotics: Sci-

ence and Systems VIII. 2012.

[28] D. Bernardini and A. Bemporad. “Stabilizing model predictive control of stochastic con-

strained linear systems”. IEEE Trans. Autom. Control 57.6 (2011).

[29] A. D. Bonzanini, J. A. Paulson, and A. Mesbah. “Safe learning-based model predictive
control under state-and input-dependent uncertainty using scenario trees”. IEEE Confer-
ence on Decision and Control (CDC). 2020.

[30] P. E. Gill, W. Murray, and M. A. Saunders. “SNOPT: An SQP algorithm for large-scale

constrained optimization”. SIAM Review 47.1 (2005).

[31] O. de Groot et al. “Scenario-Based Trajectory Optimization in Uncertain Dynamic Envi-

ronments”. IEEE Robotics and Automation Letters 6.3 (2021).

[32] M. N. Zeilinger, M. Morari, and C. N. Jones. “Soft constrained model predictive control

[33]

with robust stability guarantees”. IEEE Trans. Autom. Control (2014).
J. L¨ofberg. “YALMIP : A toolbox for modeling and optimization in MATLAB”. Proc. of
the CACSD Conference. Taipei, Taiwan, 2004.

A Recursive Bayesian Estimation Updates

Active Uncertainty Reduction for HRI

17

In this section, we write BR := BR(xt), BH := BH (xt), U M (xt, uR
Σ ¯dM
likelihood of state xt+1 as

t ) := U M and
for notational convenience. Based on (10), we can compute the

t ) := Σ ¯dM

t (xt, uR

t

P (xt+1 | uR

t , It; θM , M ) = N

(cid:16)

BH U M θM + f (xt) + BRuR

t , Σ ¯dM

t

(cid:17)

(xt, uR
t )

(15)

Recall that we can decompose the belief state as bt = P (cid:0)θM | It; M (cid:1) P (M | It).
Suppose the prior P (cid:0)θM |It; M (cid:1) ∼ N
µθM
is given as a Gaussian distribu-
t
tion. The conditional distribution p(θM
− | It+1; M ) can be computed by measurement
update (4a) in closed-form and remains a Gaussian, whose mean and covariance are
given by

, ΣθM
t

(cid:17)

(cid:16)

θM
−
t+1 = Σ
µ

θM
−
t+1

(cid:20)
(cid:0)BH U M (cid:1)(cid:62)

¯dM
t

Σ

−1 (cid:0)xt+1 − f (xt) − BRuR

t

(cid:1) +

(cid:17)−1

(cid:16)

ΣθM
t

(cid:21)

,

µθM
t

θM
−
t+1 =

Σ

(cid:20)(cid:16)

ΣθM
t

(cid:17)−1

+ (cid:0)BH U M (xt, uR

t )(cid:1)(cid:62)

¯dM
t

Σ

−1

(xt, uR

t )BH U M (xt, uR
t )

(cid:21)−1

.

Subsequently, measurement update (4b) for M can be readily computed by marginaliz-
ing state likelihood (15) with respect to θM , similar to [15].

B Scenario Tree Construction Procedure

Algorithm 1 Ofﬂine scenario tree construction
Input: Current state ˆxt and belief state ˆbt, horizon N > 0, dual control horizon 1 ≤ N d ≤ N ,

mode set M, branching number K > 0
Output: A scenario tree deﬁned by node set Nt
1: Initialization: xn0 ← ˆxt, bn0 ← ˆbt, tn0 ← 0, Nt ← {n0}

// Dual Control Steps:
2: for all t(cid:48) ← 0, 1, . . . , N d do
for all ˜n ← Nt do
3:
if ˜n.t = t(cid:48) then
4:
5:
6:
7:
8: end for

end for

end if

Branch out child nodes: Nt ← Nt ∪ BRANCH(˜n, M, K)

// Exploitation Steps:

for all ˜n ← Nt do
if ˜n.t = t(cid:48) then

9: for all t(cid:48) ← N d, N d + 1, . . . , N do
10:
11:
12:
13:
14:
15: end for

end for

end if

Extend to get one child node: Nt ← Nt ∪ EXTEND(˜n)

18

Haimin Hu et al.

Algorithm 2 Generate child nodes for dual control steps

1

Randomly sample a set {θM,o
Randomly sample a set { ¯dM,o
1
for all k ← 1, 2, . . . , K do

function BRANCH(n, M, K)
1: Initialize the set of child nodes: C ← ∅
2: for all M ← M do
3:
4:
5:
6:
7:
8:
9:
10: end for
11: return C

Create a node ˜n
t˜n = tn + 1, θM,o
C ← C ∪ {˜n}

˜n = θM,o

end for

k

, θM,o
2
, ¯dM,o
2

, . . . , θM,o
, . . . , ¯dM,o

K } from the standard Gaussian N (0, I)
K } from the standard Gaussian N (0, I)

, ¯dM,o

˜n = ¯dM,o

k

, M˜n = M

Algorithm 3 Generate a child node for exploitation steps

function EXTEND(n)
1: Create a node ˜n
2: t˜n = tn + 1, θM,o
3: return {˜n}

˜n = 0, ¯dM,o

˜n = 0, M˜n = Mn

C Practical Aspects

C.1 Computation of the Human’s Rational Actions

t , uM
i

i (xt, uR

The Laplace approximation used by (8) requires the mean function (human’s rational
(cid:1). In our paper, we use the
(cid:0)xt, uR
t ) as the maximizer of QM
action) µM
i
game-theoretic approach [24] to compute QM
i (·) as an approximate local Nash equi-
librium solution, which adopts an analytical maximizer. In case when the expression
of µM
i (·) cannot be computed beforehand, we use a numerical approach similar to [5],
which computes a local maximizer µM
i (·) during online optimization. Under the mild
assumption that QM
i (·) is a smooth function whose maximum can be attained, we can
i (·) with respect to uM
set the gradient of QM
to 0. This condition can be enforced either
i
as a hard constraint or a penalty cost in ST-SMPC problem (14).

C.2 Handling Unbounded Support of Predicted Human’s Action

Constraint uH
˜n ∈ U H in (14) may not be feasible, since the predicted human’s action uH
˜n
is given as a weighted sum of (unbounded) basis functions with (unbounded) normally
distributed weights. To reconcile this, we deﬁne, for each node ˜n, two separate decision
variables: ˜uH
˜n , which must equal the sampled linear combination of basis functions, and
˜n , which must satisfy uH
uH
˜n (cid:107)2 to (14), with
˜n to the nearest point in U H to the
some large C > 0 (we use C = 108), the solver sets uH
sample-consistent ˜uH
˜n enters the dynamics in (14).

˜n ∈ U H . By adding a cost term C(cid:107)˜uH

˜n . This feasible “projected” control uH

˜n − uH

Active Uncertainty Reduction for HRI

19

C.3 Parameter Estimation in Belief State Update

Recall that in order to derive the approximate belief state dynamics ˜g(·), we need to
estimate the value of θM in (11). In our paper, we use a roll-out-based approach by
setting the estimation ¯θM to the mean of the conditional distribution of θM computed
in Step 3 of the initialization pipeline, which is described in the next section.

C.4

Initialization Pipeline

Since problem (14) is in general a large-scale nonconvex optimization problem, ini-
tialization is crucial for solving it rapidly and reliably in real-time. In this paper, we
generate an initial guess for (14) using the following pipeline:

1. (Optional) Solve a certainty-equivalent MPC by setting θM and M to their maxi-

mum a posteriori estimated values based on the current belief state ˆbt.

2. Solve a non-dual SMPC with the same scenario tree structure as the dual-SMPC,
replacing belief state dynamics ˜g(·) with gt(·) for all dual control steps, and using
the certainty-equivalent MPC solution as the initial guess.

3. Forward-propagate belief states through ˜g(·) using the non-dual SMPC solution for

all dual control nodes in the scenario tree.

Step 1 is optional and is only needed when the non-dual SMPC in Step 2 cannot be read-
ily solved. In Section 6, we show that even if (14) uses results of the non-dual SMPC
as its initialization, the resulting closed-loop trajectories are signiﬁcantly different.

