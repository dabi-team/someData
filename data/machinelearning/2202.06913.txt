2
2
0
2

y
a
M
5
1

]

G
L
.
s
c
[

3
v
3
1
9
6
0
.
2
0
2
2
:
v
i
X
r
a

TPLP: Page 1–8. © The Author(s), 2021. Published by Cambridge University Press 2021

1

doi:10.1017/xxxxx

FOLD-RM: A Scalable, Efﬁcient, and Explainable
Inductive Learning Algorithm for Multi-Category
Classiﬁcation of Mixed Data

Huaduo Wang, Farhad Shakerin, Gopal Gupta
The University of Texas at Dallas, Richardson, USA
{huaduo.wang, farhad.shakerin, gupta}@utdallas.edu

Abstract

FOLD-RM is an automated inductive learning algorithm for learning default rules for mixed (numerical and
categorical) data. It generates an (explainable) answer set programming (ASP) rule set for multi-category
classiﬁcation tasks while maintaining efﬁciency and scalability. The FOLD-RM algorithm is competitive
in performance with the widely-used, state-of-the-art algorithms such as XGBoost and multi-layer per-
ceptrons (MLPs), however, unlike these algorithms, the FOLD-RM algorithm produces an explainable
model. FOLD-RM outperforms XGBoost on some datasets, particularly large ones. FOLD-RM also pro-
vides human-friendly explanations for predictions.

KEYWORDS: Explainable AI, Data Mining, Inductive Logic Programming, Machine Learning

1 Introduction

Dramatic success of machine learning has led to an avalanche of applications of Artiﬁcial Intelligence (AI).
However, the effectiveness of these systems is limited by the machines’ current inability to explain their
decisions to human users. That is mainly because statistical machine learning methods produce models that
are complex algebraic solutions to optimization problems such as risk minimization or geometric margin
maximization. Lack of intuitive descriptions makes it hard for users to understand and verify the underlying
rules that govern the model. Also, these methods cannot produce a justiﬁcation for a prediction they arrive
at for a new data sample. The problem of explaining (or justifying) a model’s decision to its human user is
referred to as the model interpretability problem. The sub-ﬁeld is referred to as Explainable AI (XAI).

The ILP learning problem is the problem of searching for a set of logic programming clauses from which
the training examples can be deduced. ILP provides an excellent solution for XAI. ILP is a thriving ﬁeld and
a large number of such clause search algorithms have been devised as described by Muggleton et al. (2012)
and Cropper and Dumancic (2020). The search in these ILP algorithms is performed either top down or
bottom-up. A bottom-up approach builds most-speciﬁc clauses from the training examples and searches the
hypothesis space by using generalization. This approach is not applicable to large-scale datasets, nor it can
incorporate negation-as-failure into the hypothesis, as explained in the book by Baral (2003). A survey of
bottom-up ILP systems and their shortcomings has been compiled by Sakama (2005). In contrast, top-down
approach starts with the most general clauses and then specializes them. A top-down algorithm guided by
heuristics is better suited for large-scale and/or noisy datasets, as explained by Zeng et al. (2014).

The FOIL algorithm by Quinlan is a popular top-down inductive logic programming algorithm that learns
a logic program. The FOLD algorithm by Shakerin et al. (2017) is a novel top-down algorithm that learns
default rules along with exception(s) that closely model human thinking. It ﬁrst learns default predicates
that cover positive examples while avoiding covering negative examples. Then it swaps the covered pos-
itive examples and negative examples and calls itself recursively to learn the exception to the default. It
repeats this process to learn exceptions to exceptions, exceptions to exceptions to exceptions, and so on.

 
 
 
 
 
 
2

Cambridge Author

The FOLD-R++ algorithm by Wang and Gupta (2022) is a new scalable ILP algorithm that builds upon
the FOLD algorithm to deal with the efﬁciency and scalability issues of the FOLD and FOIL algorithms.
It introduces the preﬁx sum computation and other optimizations to speed up the learning process while
providing human-friendly explanation for its prediction using the s(CASP) answer set programming system
of Arias et al. (2018). However, all these algorithms focus on binary classiﬁcation tasks, and cannot deal
with multi-category classiﬁcation tasks. Note that a binary classiﬁcation task checks whether a data record
is a member of a given class or not, e.g., does a given creature ﬂy or not ﬂy? In multi-category classiﬁca-
tion, there can be multiple membership classes, e.g., a given creature’s habitat can be predicted to be one of
desert, mountain, plain, salt water, or fresh water (see the textbook by Bishop (2006)).

In this paper we propose a new ILP learning algorithm called FOLD-RM for multi-category classiﬁca-
tion that builds upon the FOLD-R++ algorithm. FOLD-RM also provides native explanations for predic-
tion without external libraries or tools. Our experimental results indicates that the FOLD-RM algorithm
is comparable in performance to traditional, popular machine learning algorithms such as XGBoost by
Chen and Guestrin (2016) and Multi-Layer Perceptrons (MLP) described in the book by Aggarwal (2018).
In most cases, FOLD-RM outperforms them in execution efﬁciency. Of course, neither XGBoost nor MLP
are interpretable.

Note that the term model in the ﬁeld of machine learning and logic programming have different mean-
ings. We use the term model in this paper in machine learning sense. Thus, the answer set program generated
by our FOLD-RM algorithm is the model that we learn in the sense of machine learning. We use the term
answer set in this paper to refer to stable models of answer set programs, where a model means assignment
of truth values to program predicates that make the program true. Note also that that we use the terms clause
and rule interchangeably in this paper.

2 Background

2.1 Inductive Logic Programming

Inductive Logic Programming (ILP) as described in Muggleton (1991) is a subﬁeld of machine learning
that learns models in the form of logic programming clauses comprehensible to humans. This problem is
formally deﬁned as:
Given

1. A background theory B, in the form of an extended logic program, i.e., clauses of the form h ←
l1, ..., lm, not lm+1, ..., not ln, where h, l1, ..., ln are positive literals and not denotes negation-as-
failure (NAF) as described in Baral (2003). For reasons of efﬁciency, we restrict B to be stratiﬁed
(stratiﬁed logic programs are explained in the book by Gelfond and Kahl (2014)).

2. Two disjoint sets of ground target predicates E+, E− known as positive and negative examples,

respectively.

3. A hypothesis language of function free predicates L, and a reﬁnement operator ρ under θ-
subsumption described in Plotkin (1971) (for more details see the paper by Cropper and Dumancic
(2020)). The hypothesis language L is also assumed to be stratiﬁed.

Find a set of clauses H such that:

1. ∀e ∈ E+, B ∪ H |= e.
2. ∀e ∈ E−, B ∪ H 6|= e.
3. B ∧ H is consistent.

The target predicate is the predicate whose deﬁnition we want to learn as a stratiﬁed normal logic pro-
gram. The positive and negative examples are grounded target predicates, i.e., suppose we want to learn
the concept of which creatures can ﬂy, then we will give positive examples E+ = {fly(tweety),
fly(sam), ...} and negative examples E− = {fly(kitty), fly(polly), ...}, where
tweety, sam, . . . , are names of creatures that can ﬂy, and kitty, polly, . . . , are names of creatures
that cannot ﬂy.

Note that the reason for restricting to stratiﬁed normal logic programs is that we can realize a simple
and efﬁcient ASP interpreter in the FOLD-RM system code for the training process. If we allowed for

FOLD-RM

3

non-stratiﬁed programs, the training process will have to invoke a full-ﬂedged ASP interpreter during the
training and testing process, resulting in signiﬁcant inefﬁciency. Considering non-stratiﬁed programs is part
of our future research plan. We restrict ourselves to function-free predicates, i.e., we allow only Datalog
rules, again, for reasons of efﬁciency.

2.2 Default Rules

Default Logic proposed by Reiter (1980) is a non-monotonic logic to formalize commonsense reasoning. A
default D is an expression of the form

A : MB
Γ
which states that the conclusion Γ can be inferred if pre-requisite A holds and B is justiﬁed. MB stands for
“it is consistent to believe B” as explained in the book by Gelfond and Kahl (2014). Normal logic programs
can encode a default quite elegantly. A default of the form:

can be formalized as the following normal logic program rule:

α1 ∧α2 ∧ · · · ∧αn : M¬β1, M¬β2 . . . M¬βm
γ

γ :- α1,α2, . . . ,αn, not β1, not β2, . . . , not βm.
where α’s and β’s are positive predicates and not represents negation as failure (under the stable model se-
mantics as described in Baral (2003)). We call such rules default rules. Thus, the default bird(X ):M¬penguin(X )
will be represented as the following ASP-coded default rule:
fly(X) :- bird(X), not penguin(X).

f ly(X )

We call bird(X), the condition that allows us to jump to the default conclusion that X can ﬂy, as the
default part of the rule, and not penguin(X) as the exception part of the rule.

Default rules closely represent the human thought process (i.e., frequently used in commonsense rea-
soning). FOLD-R and FOLD-R++ learn default rules represented as answer set programs. Note that the
programs currently generated are stratiﬁed normal logic programs, however, we eventually hope to learn
non-stratiﬁed answer set programs too as in the work of Shakerin and Gupta (2018) and Shakerin (2020).
Hence, we continue to use the term answer set program for a normal logic program in this paper. An ad-
vantage of learning default rules is that we can distinguish between exceptions and noise as explained by
Shakerin et al. (2017) and Shakerin (2020). The introduction of (nested) exceptions, or abnormal predicates,
in a default rule increases coverage of the data by that default rule. A single rule can now cover more ex-
amples which results in reduced number of generated rules. The equivalent program without the abnormal
predicates will have many more rules if the abnormal predicates calls are fully expanded.

Classiﬁcation problems are either binary or multi-category.

2.3 Classiﬁcation Problems

1. Binary classiﬁcation is the task of classifying the elements of a set into two groups on the basis of a
classiﬁcation rule. For example, a speciﬁc patient (given a set of patients) has a particular disease or
not, or a particular manufactured article (in a set of manufactured articles) will pass quality control
or not. Details can be found in the book by Bishop (2006).

2. Multi-category or multinomial classiﬁcation is the problem of classifying instances into one of three
or more classes. For example, an animal can be predicted to have one of the following habitats: sea
water, fresh water, desert, mountain, or plains. Again, details can be found in the book by Bishop
(2006).

4

Cambridge Author

3 The FOLD-R++ Algorithm

The FOLD-R++ algorithm by Wang and Gupta (2022) is a new ILP algorithm for binary classiﬁcation that
is built upon the FOLD algorithm of Shakerin et al. (2017). Our FOLD-RM algorithm builds upon the
FOLD-R++ algorithm. FOLD-R++ increases the efﬁciency and scalability of the FOLD algorithm. The
FOLD-R++ algorithms divides features into two categories: categorical features and numerical features.
For a categorical feature, all the values in the feature would be considered as categorical values even though
some of them are numbers. For categorical features, the FOLD-R++ algorithm only generates equality or
inequality literals. For numerical features, the FOLD-R++ algorithm would try to read all the values as
numbers, converting them to categorical values if conversion to numbers fails. FOLD-R++ additionally
generates numerical comparison (≤ and >) literals for numerical values. For a mixed type feature that
contains both categorical values and numerical values, the FOLD-R++ algorithm treats them as numerical
features.

The FOLD-R++ algorithm employs information gain heuristic to guide literal selection during the learn-
ing process. It uses a simpliﬁed calculation process for information gain by using the number of true posi-
tive, false positive, true negative, and false negative examples that a literal can imply. The information gain
for a given literal is calculated as shown in Algorithm 1.

The goal of the ILP algorithm is to ﬁnd an answer set program whose answer set has all the positive
examples and none of the negative examples. Our algorithm incrementally learns this program using the
information gain heuristic. The Information gain heuristic allows us to reﬁne our program incrementally,
i.e., the answer set of the program after each reﬁnement step has more and more positive examples included
and fewer and fewer of the negative ones.

Algorithm 1 FOLD-R++ Algorithm: Information Gain function
Input: t p, f n, tn, f p: the number of Et p, E f n, Etn, E f p implied by literal
Output: information gain

1: function F(a, b)
if a = 0 then
2:
return 0

3:

4:

end if
return a · log2( a

a+b )

5:
6: end function
7: function IG(t p, f n,tn, f p)
8:

if f p + f n > t p + tn then

return −∞

9:

10:

end if
return

11:
12: end function

1

t p+ f p+tn+ f n ·(F(t p, f p) + F( f p,t p) + F(tn, f n) + F( f n,tn))

The comparison between two numerical values or two categorical values in FOLD-R++ is straightfor-
ward, as commonsense would dictate, i.e., two numerical (resp. categorical) values are equal if they are
identical, else they are unequal. However, a different assumption is made to compare a numerical value and
a categorical value in FOLD-R++. The equality between a numerical value and a categorical value is always
false, and the inequality between a numerical value and a categorical value is always true. Additionally, nu-
merical comparison (≤ and >) between a numerical value and a categorical value is always false. An exam-
ple is shown in Table 1 (Left), while an evaluation example for a given literal, literal(i, >, 4), based on the
comparison assumption is shown in Table 1 (Right). Given E+ = {1, 2, 2, 4, 5, x, x, y}, E− = {1, 3, 4, y, y, y, z},
and literal(i, >, 4), the true positive example Et p, false negative examples E f n, true negative examples Etn,
and false positive examples E f p implied by the literal are {5}, {1, 2, 2, 4, x, x, y}, {1, 3, 4, y, y, y, z}, Ø re-
spectively. Then, the information gain of literal(i, >, 4) is calculated IG(i,>,4)(1, 7, 7, 0) = −0.647 through
Algorithm 1.

comparison
5 = ‘k’
5 6= ‘k’
5 ≤ ‘k’
5 > ‘k’

evaluation
False
True
False
False

FOLD-RM

5

E+
E−
Etp(i,>,4)
Efn(i,>,4)
Etn(i,>,4)
Efp(i,>,4)

ith feature values
1 2 2 4 5 x x y
1 3 4 y y y z
5
1 2 2 4 x x y
1 3 4 y y y z
Ø

count
8
7
1
7
7
0

Table 1. Left: Comparisons between a numerical value and a categorical value. Right:
Evaluation and count for literal(i, >, 4).

ith feature values

E+ 1 2 2 4 5 x x y
E− 1 3 4 y y y z

value
pos

1 2 3 4 5
1 2 0 1 1

x
2

y
1

z
0

psum+ 1 3 3 4 5 N/A N/A N/A

neg

1 0 1 1 0

0

3

1

psum− 1 1 2 3 3 N/A N/A N/A

Table 2. Left: Examples and values on ith feature. Right: positive/negative count and preﬁx sum
on each value

The FOLD-R++ algorithm starts with the clause p(...) :- true., where p(...) is the target
predicate to learn. It specializes this clause by adding literals to its body during the inductive learning
process. It selects a literal to add that maximizes information gain (IG). The literal selection process is
summarized in Algorithm 2. In line 2, pos & neg are dictionaries that hold, respectively, the numbers of
positive & negative examples for each unique value. In line 3, xs & cs are lists that hold, respectively, the
unique numerical and categorical values. In line 4, xp & xn are the total number of, respectively, positive
& negative examples with numerical values; cp & cn are the same for categorical values. In line 11, the
information gain of literal(i, ≤, x) is calculated by taking the parameters pos[x] as the number of true positive
examples, xp − pos[x] + cp as the number of false negative examples, xn − neg[x] + cn as the number of
true negative examples, and neg[x] as the number of false positive examples. After computing the preﬁx
sum in line 6, pos[x] holds the total number of positive examples that has a value less than or equal to x.
Therefore, xp − pos[x] represents the total number of positive examples that have a value greater than x.
cp, the total number of positive examples that have a categorical value, is added to the number of false
negative examples because of the assumption that numerical comparison between a numerical value and a
categorical value is always false. The negative examples that have a value greater than x or a categorical
value would be evaluated as false by literal(i, ≤, x), so xn − neg[x] is added as true negative parameter. And,
cn, the total number of negative examples that has a categorical value, is added to true negative parameter.
The expression neg[x] means the number of negative examples that have the value less than or equal to x;
neg[x] is added as false positive parameter because the evaluations of these examples by literal(i, ≤, x) are
true. The information gain calculation processes of other literals also follows the comparison assumption
mentioned above. Finally, the best info gain function returns the best score on information gain and
the corresponding literal except the literals that have been used in current rule-learning process. For each
feature, we compute the best literal, then the find best literal function returns the best literal among
this set of best literals.

Example 1
Given positive and negative examples in Table 2, E+, E−, with mixed type of values on ith feature, the
target is to ﬁnd the literal with the best information gain on the given feature. There are 8 positive examples,
their values on ith feature are [1, 2, 2, 4, 5, x, x, y], and the values on ith feature of the 7 negative examples are
[1, 3, 4, y, y, y, z].

With the given examples and speciﬁed feature, the number of positive examples and negative examples for
each unique value are counted ﬁrst, which are shown as pos, neg on right side of Table 2. Then, the preﬁx
sum arrays are calculated for computing heuristic as psum+, psum−. Table 3 shows the information gain
for each literal, the literal(i, =, x) has been selected with the highest score.

6

Cambridge Author

Algorithm 2 FOLD-R++ Algorithm, Find Best Literal function
Input: E +: positive examples, E −: negative examples, Lused: used literals
Output: best lit: the best literal that provides the most information
1: function BEST INFO GAIN(E +, E −, i, Lused)
2:

pos, neg ← count classiﬁcation(E +, E −, i)
xs, cs ← collect unique values(E +, E −, i)
xp, xn, cp, cn ← count total(E +, E −, i)
xs ← couting sort(xs)
for j ← 1 to size(xs) do

⊲ compute preﬁx sum for E + & E − numerical values

pos[xs j] ← pos[xs j] + pos[xs j−1]
neg[xs j] ← neg[xs j] + neg[xs j−1]

end for
for x ∈ xs do

lit dict[literal(i, ≤, x)] ← IG(pos[x], xp − pos[x] + cp, xn − neg[x] + cn, neg[x])
lit dict[literal(i, >, x)] ← IG(xp − pos[x], pos[x] + cp, neg[x] + cn, xn − neg[x])

end for
for c ∈ cs do

lit dict[literal(i, =, x)] ← IG(pos[c], cp − pos[c] + xp, cn − neg[c] + xn, neg[c])
lit dict[literal(i, 6=, x)] ← IG(cp − pos[c] + xp, pos[c], neg[c], cn − neg[c] + xn)

⊲ return the best info gain and its corresponding literal

⊲ N is the number of features

end for
best, l ← best pair(lit dict, Lused)
return best, l

19:
20: end function
21: function FIND BEST LITERAL(E +, E −, Lused)
22:

best ig, best lit ← −∞, invalid
for i ← 1 to N do

ig, lit ← BEST INFO GAIN(E +,E −,i,Lused)
if best ig < ig then

best ig, best lit ← ig, lit

end if

end for
return best lit

29:
30: end function

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

23:

24:

25:

26:

27:

28:

value
1
≤ value −∞
> value
= value N/A
6= value N/A

-0.667 −∞
N/A
N/A

3

2

Info Gain
y
5
4
N/A
-0.655 -0.686 -0.688 -0.672 N/A
N/A
-0.682 -0.647 −∞
N/A
N/A -0.598 −∞
N/A
N/A
N/A
N/A
N/A
−∞

x

z
N/A
N/A
−∞

-0.631 -0.637

Table 3. The info gain on ith feature with given examples

4 The FOLD-RM Algorithm

The FOLD-R++ algorithm performs binary classiﬁcation. We generalize the FOLD-R++ algorithm to per-
form multi-category classiﬁcation. The generalized algorithm is called FOLD-RM. The FOLD-R++ algo-
rithm is summarized in Algorithm 3. The FOLD-R++ algorithm generates an answer set programming rule

FOLD-RM

7

set, in which all the rules have the same rule head. An example covered by any rule in the set would imply
the rule head is true. The FOLD-R++ algorithm generates a model by learning one rule at a time. Ruling
out the already covered example in line 9 after learning a rule would help select better literal for remaining
examples. In the rule learning process, the best literal would be selected according to the useful informa-
tion it can provide for current training examples (line 17) till the literal selection fails. If the ratio of false
positive examples to true positive examples drops below the threshold ratio in line 22, it would next learn
exceptions by swapping residual positive and negative examples and calling itself recursively (line 26). Any
examples that cannot be covered by the selected literals would be ruled out in line 20, 21. The ratio in line
22 represents the upper bound on the number of true positive examples to the number of false positive ex-
amples implied by the default part of a rule. It helps speed up the training process and reduces the number
of rules learned.

Generally, avoiding covering negative examples by adding literals to the default part of a rule will reduce
the number of positive examples the rule can imply. Explicitly activating the exception learning procedure
(line 26 in Algorithm 3) could increase the number of positive example a rule can cover while reducing
the total number of rules generated. As a result, the interpretability is increased due to fewer rules being
generated.

The FOLD-RM algorithm performs multi-category classiﬁcation. It generates rules that it can learn for
each category. If an example cannot be implied by any rule in the learned rule set, it means the model fails
to classify this example. The FOLD-RM algorithm, summarized in Algorithm 4, ﬁrst ﬁnds a target literal
that represents the category with most examples among the current training set (line 4). It next splits the
training set into positive and negative examples based on the target literal (line 5). Then, it learns a rule to
cover the target category (line 6) by calling the learn rule function of the FOLD-R++ Algorithm. The
already covered examples would be ruled out from the training set in line 11, and the rule head would be
changed to the target literal in line 12. However, there’s a difference between the outputs of FOLD-RM
and FOLD-R++. Unlike FOLD-R++, the output of FOLD-RM is a textually ordered answer set program,
which means a rule is checked only if all the rules before it did not apply. The FOLD-RM system is publicly
available at https://github.com/hwd404/FOLD-RM.

Note that for learning each rule, FOLD-RM (Algorithm 4) chooses the target predicate by ﬁnding the
label value with the most examples in the remaining training examples and sets it as the target predicate for
this rule. In other words, the target predicate is the “most popular” label value. The names of the predicates
are the names of features in the data. The head predicate and predicates in rule body each have exactly two
arguments. The ﬁrst argument is a reference to the data record itself. For the target predicate, the second
argument is the predicted label for that record, while for predicates in the body, the second argument is used
to extract the appropriate feature value for that record. The abnormal predicates only take one argument,
namely, the data record itself. For example, consider:

class(X,’2’) :- condition(X, ’s’), not ab5(X).
ab5(X) :- not steel(X,’r’), not enamelability(X,’2’).

The ﬁrst rule says that the predicted class of data record X is ‘2’ if the condition feature of X has value ‘s’,
and abnormal case ab5 does not apply. ab5(X) is an abnormal case predicate and has only one argument. It
says that the record X should not be predicted to have class value ‘2’, if the value of steel feature is not ‘r’,
and the value of enamelability feature is not ‘2’.

4.1 Algorithmic Complexity

Next, we analyze the complexity of the FOLD-RM algorithm. If M is the number of examples and N is
the number of features, it is easy to see that the time complexity of ﬁnding the best literal (Algorithm
2) is O(NM). We assume that counting sort (complexity O(M)) with a pre-sorted list is used at line 5 in
Algorithm 2. The worst case in the FOLD-RM algorithm arises when each generated rule only covers one
example and each literal only excludes one non-target example. Therefore, in the worst case there will be
O(M2) literals chosen in total. The worst case time complexity of the FOLD-RM algorithm (Algorithm 4)
can be calculated to be O(NM3). However, this is a theoretical upper bound. The actual learning process is
really efﬁcient because the heuristics we employ helps select very effective literals, reducing the number of
iterations in the algorithm.

3:

4:

5:

6:

7:

8:

9:

10:

16:

17:

18:

19:

20:

21:

22:

23:

24:

25:

26:

27:

28:

29:

8

Cambridge Author

Algorithm 3 FOLD-R++ Algorithm
Input: E +: positive examples, E −: negative examples

⊲ Global Parameters: target, B: background knowledge, ratio: exception ratio

Output: R = {r1, ..., rn}: a set of defaults rules with exceptions
1: function FOLD RPP(E +, E −, Lused)
2:

R ← Ø
while |E +| > 0 do

⊲ Lused: used literals, initially empty

r ← LEARN RULE(E +, E −, Lused)
EFN ← covers(r, E +, false)
if |EFN| = |E +| then

break

end if
E + ← EFN
R ← R ∪ {r}

⊲ EFN: false negative examples implied by rule r

⊲ rule out the already covered examples

11:

end while
return R
12:
13: end function
14: function LEARN RULE(E +, E −, Lused)
15:

L ← Ø
while true do

l ← FIND BEST LITERAL(E +, E −, Lused)
L ← L ∪ {l}
r ← set default(r, L)
E + ← covers(r, E +, true)
E − ← covers(r, E −, true)
if l is invalid or |E −| ≤ |E +| ∗ ratio then

if l is invalid then

⊲ set default part of rule r as L

r ← set default(r, L \ {l})

⊲ remove the invalid literal l from rule r

else

AB ← FOLD RPP(E −, E +, Lused + L)
r ← set exception(r, AB)

⊲ learn exception rules for r
⊲ set exception part of rule r as AB

end if
break

31:

30:

end if
end while
return r
32:
33: end function

⊲ the head of rule r is target

One can also prove that the FOLD-RM algorithm always terminates. The fold rm function calls the
learn rule function to induce a rule that can cover at least one ‘most popular’ remaining example till
all the examples have been covered or the learned rule fails to cover any ‘most popular’ example. The loop
in the fold rm function iterates at most |E| times while excluding the already covered examples. The
learn rule function reﬁnes the rule with a given target by adding the best literal to the rule body. By
adding literals to the rules, the numbers of true positive and false positive examples the rule implies can only
monotonically decrease. The learned valid literal excludes at least one false positive example that the rule
implies. So, the loop in the learn rule function iterates at most |E−| times. When the |E−| < |E+|∗ratio
condition is met, the fold rpp function is called to learn exception rules for the current default rule.

FOLD-RM

9

Similar to the fold rm function, the fold rpp function iterates at most |E+| times. Also, there are only
ﬁnite for-loops inside the find best literal function. Therefore, we can conclude that the FOLD-RM
algorithm will always terminate.

We illustrate FOLD-RM, next, with a simple example.

4.2 An Illustrative Example

Algorithm 4 FOLD-RM Algorithm
Input: E: examples, B: background knowledge, ratio: exception ratio
Output: R = {r1, ..., rn}: a set of defaults rules with exceptions

⊲ l: most popular target literal as the learning target

⊲ rule out the already covered examples

⊲ ﬁnd the most popular target literal

1: function FOLD RM(E)
2:

R ← Ø
while |E| > 0 do
l ← MOST(E)
E +, E − ← SPLIT BY LITERAL(E, l)
r ← LEARN RULE(E +, E −, Ø)
EFN ← covers(r, E +, f alse)
if |EFN| = |E +| then

break

end if
E ← E + ∪ EFN
r ← add head(r, l)
R ← R ∪ {r}

14:

end while
return R
15:
16: end function
17: function MOST(E)
18:

for e ∈ E do

count[labele] ← count[labele] + 1

end for
labelmost ← FIND MOST(count)
return literal(indexlabel, =, labelmost)

22:
23: end function
24: function SPLIT BY LITERAL(E, l)
25:

E +, E − ← Ø, Ø
for e ∈ E do

if EVALUATE(e, l) is true then

E + ← E + ∪ {e}

else

E − ← E − ∪ {e}

end if

end for
return E +, E −

33:
34: end function

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

19:

20:

21:

26:

27:

28:

29:

30:

31:

32:

10

Cambridge Author

Example 2
The target is to learn rules for habitat using the FOLD-RM algorithm. B, E are background knowledge
and training examples, respectively. There are 3 classiﬁcations: two explicit ones (land and water), and one
implicit one (neither land, nor water).

B: mammal(kitty).

mammal(john).
mammal(smoky).
mammal(charlie).
fish(nemo).

E: habitat(charlie,land).

habitat(smoky,land).
habitat(kitty,land).

cat(kitty).
whale(john).
bear(smoky).
dog(charlie).
clownfish(nemo).
habitat(john,water).
habitat(nemo,water).

the most useful

line 17 in Algorithms 3 because it provides

the target predicate {habitat(X,land):- true} is speciﬁed at

For the ﬁrst rule,
line 4 in
Algorithm 4 because ‘land’ is the majority label. The find best literal function selects lit-
eral mammal(X) as result and adds it to the clause r = {habitat(X,land):- mammal(X)}
at
information among literals
{cat,whale,bear,dog,fish,clownfish}. Then the training set rules out covered examples at
line 20-21 in Algorithm 3, E+ = Ø, E− ={john,nemo}. The default learning is ﬁnished at this point be-
cause the candidate literal cannot provide any further useful information. Therefore, the fold rpp func-
tion is called recursively with swapped positive and negative examples, E+ ={john,nemo}, E− = Ø,
to learn exceptions. In this case, an abnormal predicate {ab1(X):-whale(X)} is learned and added
to the previously generated clause as r = {habitat(X,land):- mammal(X), not ab1(X)}.
And the exception rule {ab1(X):- whale(X)} is added to the answer set program. FOLD-RM
next learns rules for target predicate {habitat(X,water):- true} and two rules are generated as
{habitat(X,water):- fish(X)} and {habitat(X,water):- whale(X)}. The generated
ﬁnal answer set program is:

habitat(X,land):- mammal(X), not ab1(X).
habitat(X,water):- fish(X).
habitat(X,water):- whale(X).
ab1(X):- whale(X).

The program above is a logic program, which means rules are not mutually exclusive. For correctness, a
rule should be checked only if all the earlier rules result in failure. FOLD-RM generates further rules to
make the learned rules mutually exclusive. The program above is transformed as shown below.

habitat(X,land):- habitat 1(X).
habitat(X,water):- habitat 2(X), not habitat 1(X).
habitat(X,water):- habitat 3(X), not habitat 2(X), not habitat 1(X).
habitat 1(X):- mammal(X), not ab1(X).
habitat 2(X):- fish(X).
habitat 3(X):- whale(X).
ab1(X):- whale(X).

5 Experimental Results

In this section, we present our experiments on standard UCI benchmarks. The XGBoost Classiﬁer is a well-
known classiﬁcation model and used as a baseline model in our experiments. The settings used for XGBoost
Classiﬁer is kept simple without limiting its performance. Multi-Layer Perceptron (MLP) is another widely-
used classiﬁcation model that can deal with generic classiﬁcation tasks. However, both XGBoost model and
MLP cannot take mixed type (numerical and categorical values in a row or a column) as training data without
pre-processing. For mixed type data, one-hot encoding—as explained in the book by Aggarwal (2018)—has
been used for data preparation. For binary classiﬁcation, we use accuracy, precision, recall, and F1 score
as evaluation metrics. For the multi-category classiﬁcation tasks, following convention, we use accuracy,

FOLD-RM

11

DataSet

XGBoost.Classiﬁer

FOLD-R++

1

1

1

1

1

1

1

1

1

0.97 0.98 0.98 0.97
0.95 0.97 0.96 0.96

Name
120
acute
704
autism
699
breast-w
1728
cars
690
credit-a
336
ecoli
270
heart
351
ionosphere
400
kidney
kr vs. kp
3196
mushroom 8124
435
voting
adult
32561
rain in aus 145460

#Rows #Cols Acc Prec Rec F1 T(ms) Acc Prec Rec F1 T(ms) #Rules
35
0.99
0.99 0.99
76
0.95 0.96 0.97 0.97
0.96 0.97 0.96 0.97
78
77
0.98
0.97 0.98
0.85 0.83 0.83 0.83
0.84 0.92 0.79 0.84
368
0.96 0.95 0.94 0.95
0.76 0.76 0.62 0.68
165
0.80 0.81 0.83 0.81
0.79 0.79 0.83 0.81
112
0.88 0.86 0.96 0.90 1,126 0.92 0.93 0.94 0.93
0.99
0.98 0.99
126
0.98 0.98 0.98 0.98
210
0.99 0.99 0.99 0.99
0.99 0.99 0.99 0.99
378
49

2.6
24.3
10.2
12.2
10.0
11.4
11.7
12.0
5.0
18.4
8.0
0.95 0.94 0.95 0.94
10.5
0.86 0.88 0.94 0.91 274,655 0.84 0.86 0.95 0.90 2,546
16.7
0.83 0.84 0.95 0.89 285,307 0.78 0.87 0.84 0.85 21,868 40.5

1.6
47
24
38
66
19
24
214
19
223
314
17

7
18
10
7
16
9
14
35
25
37
23
17
15
24

0.95 0.94 0.94 0.94

1

1

1

1

1

1

1

1

1

1

Table 4. Comparison of XGBoost and FOLD-R++ on various Datasets

weighted average of Macro precision, weighted average of Macro recall, and weighted average of Macro
F1 score to compare models as explained by Grandini et al. (2020). The average numbers of generated rules
are also reported for the FOLD-R++ and FOLD-RM algorithms in Table 4, Table 6, and Table 7, some of
them are not integers because of being averaged over a number of repeated experiments.

Both FOLD-R++ and FOLD-RM algorithms do not need any encoding for training. After specifying the
numerical features, they can deal with mixed type data directly, i.e., no one-hot encoding is needed. Even
missing values are handled and do not need to be provided. We implemented both algorithms with Python.
The hyper-parameter ratio is simply set as 0.5 for all the experiments. And all the learning processes have
been run on a desktop with Intel i5-10400 CPU @ 2.9 GHz and 32 GB RAM. To have good performance
test, we performed 10-fold cross-validation test on each dataset and average classiﬁcation metrics and exe-
cution time are shown. The best performer is highlighted in boldface.

The XGBoost Classiﬁer utilizes decision tree ensemble method to build model and provides good per-
formance. Performance comparison of FOLD-R++ and XGBoost is shown in Table 4. The FOLD-R++
algorithm is comparable to XGBoost classiﬁer for classiﬁcation, but it’s more efﬁcient in terms of execu-
tion time, especially on datasets with many unique feature values.

For multi-category classiﬁcation experiments, we collected 15 datasets for comparison with XGBoost
and MLP. The drug consumption dataset has many output attributes, we perform training on heroin, crack,
and semer attributes. The size and label distribution of the datasets used is shown in Table 5: number of rows
indicates the number of data records, while the number of columns indicates the number of features. We
ﬁrst compare the performance of FOLD-RM and XGBoost in Table 6. XGBoost performs much better on
datasets avila and yeast, and FOLD-RM performs much better on datasets ecoli, dry-bean, eeg, and weight-
lifting. After analyzing these dataset, FOLD-RM seems to perform better on more complicated datasets with
mixed type values. XGBoost seems to perform better on the datasets that have limited information. How-
ever, for those datasets for which FOLD-RM has similar performance with XGBoost, FOLD-RM is more
efﬁcient in terms of execution speed. In addition, FOLD-RM is explainable/interpretable, and XGBoost is
not.

The comparison with MLP is presented in Table 7. For most datasets, FOLD-RM can achieve equivalent
scores, similar to the comparison with XGBoost, FOLD-RM performs much better on datasets ecoli, dry-
bean, eeg, and weight-lifting, while MLP performs much better on datasets avila and yeast. MLP takes much
more time for training than XGBoost because of its algorithmic complexity. Like the XGBoost classiﬁer, for
complex datasets with mixed values, MLP also suffers from pre-processing complications such as having
to use one-hot encoding.

RIPPER algorithm by Cohen (1995) is a popular rule induction algorithm that generates conjunctive
normal form (CNF) formulas. Eight datasets have been used for comparison between RIPPER and FOLD-
RM. We did not ﬁnd the RIPPER algorithm implementation with multi class classiﬁcation. Therefore, we
have collected the accuracy data reported by Asadi and Shahrabi (2016) and performed the same experiment

Cambridge Author

#Rows #Cols Distribution

12

Dataset
anneal
avila

ecoli
drug-heroin
drug-crack
drug-semer
dry-bean

eeg
intention
nursery

pageblocks
parkinson
pendigits

wine
weight-lift
yeast

898
20867

336
1885
1885
1885
13611

14980
12330
12960

5473
756
10992

178
4024
1484

wall-robot

5456

ﬂags
glass
optidigits

194
214
3823

shuttle

58000

39
11

9
13
13
13
17

15
18
9

11
754
17

14
155
10

25

10
10
65

10

‘3’: 684, ‘U’: 40, ‘1’: 8, ‘5’: 67, ‘2’: 99
‘A’:8572,‘F’:3923,‘H’:1039,‘E’:2190,‘I’:1663,‘Y’:533
‘D’:705,‘X’:1044,‘G’:893,‘W’:89,‘C’:206,‘B’:10
‘cp’:143,‘im’:77,‘imS’:2,‘imL’:2,‘imU’:35,‘om’:20,‘omL’:5,‘pp’:52
‘CL0’:1605,‘CL1’:68,‘CL2’:94,‘CL3’:65,‘CL5’:16,‘CL6’:13,‘CL4’:24
‘CL0’:1627,‘CL1’:67,‘CL2’:112,‘CL3’:59,‘CL5’:9,‘CL4’:9,‘CL6’:2
‘CL0’: 1877, ‘CL2’: 3, ‘CL3’: 2, ‘CL4’: 1, ‘CL1’:2
‘SEKER’: 2027, ‘BARBUNYA’: 1322, ‘BOMBAY’: 522
‘CALI’: 1630, ‘HOROZ’: 1928, ‘SIRA’: 2636, ‘DERMASON’: 3546
‘0’: 8257, ‘1’: 6723
‘FALSE’: 10422, ‘TRUE’: 1908
‘recommend’: 2, ‘priority’: 4266
‘not recom’: 4320, ‘very recom’: 328, ‘spec prior’: 4044
‘1’: 4913, ‘2’: 329, ‘4’: 88, ‘5’: 115, ‘3’: 28
‘1’: 564, ‘0’: 192
‘8’: 1055, ‘2’: 1144, ‘1’: 1143, ‘4’: 1144, ‘6’: 1056
‘0’: 1143, ‘5’: 1055, ‘9’: 1055, ‘7’: 1142, ‘3’: 1055
‘1’: 59, ‘2’: 71, ‘3’: 48
‘E’: 1370, ‘A’: 1365, ‘D’: 276, ‘B’: 901, ‘C’: 112
‘MIT’: 244, ‘NUC’: 424, ‘CYT’: 463, ‘ME1’: 44, ‘EXC’: 35, ‘ME2’: 51
‘ME3’: 163, ‘VAC’: 30, ‘POX’: 20, ‘ERL’: 5, ‘0.18’: 2, ‘0.16’: 2, ‘0.37’: 1
‘Slight-Right-Turn’: 826, ‘Sharp-Right-Turn’: 2097
‘Move-Forward’: 2205, ‘Slight-Left-Turn’: 328
‘2’: 36, ‘6’: 15, ‘1’: 60, ‘0’: 40, ‘5’: 27, ‘3’: 8, ‘4’: 4, ‘7’: 4
‘1’: 70, ‘2’: 76, ‘3’: 17, ‘5’: 13, ‘6’: 9, ‘7’: 29
‘0’: 376, ‘7’: 387, ‘4’: 387, ‘6’: 377, ‘2’: 380
‘5’: 376, ‘8’: 380, ‘1’: 389, ‘9’: 382, ‘3’: 389
‘2’: 50, ‘4’: 8903, ‘1’: 45586, ‘5’: 3267, ‘3’: 171, ‘7’: 13, ‘6’: 10

Table 5. The size and label distribution of UCI datasets

FOLD-RM

XGBoost

1

1

1

1

17.9
33.6
42.3
12.0
17.6
10.3

Dataset
anneal
avila
ecoli

63
3,540
41
136
145
43

F1 Rules T(ms) Acc Prec Rec

F1
0.99 0.99 0.99 0.99

Acc Prec Rec
1
0.99
0.99 0.99
0.33 0.49 0.33 0.39
0.80 0.82 0.80 0.80
drug-heroin 0.84 0.74 0.84 0.78
drug-crack
0.85 0.75 0.85 0.80
drug-semer 0.99 0.99 0.99 0.99
dry-bean
eeg
intention
nursery
pageblocks
parkinson
pendigits
wine
weight-lift
yeast
wall-robot

0.42 0.79 0.42 0.50
0.84 0.77 0.84 0.79
0.85 0.76 0.85 0.81
0.99
0.91 0.91 0.91 0.91 185.7 13,415 0.29 0.87 0.29 0.37
0.78 0.78 0.78 0.77 164.5
0.50 0.70 0.50 0.54
0.90 0.89 0.90 0.90
0.90 0.89 0.90 0.89 114,161
78.3
0.97 0.97 0.97 0.96
24,100
0.88 0.93 0.88 0.89
59.8
0.97 0.97 0.97 0.96
81,416
0.95 0.94 0.95 0.94
72.3
527
0.84 0.84 0.84 0.83
15.9
0.81 0.80 0.81 0.79
0.96 0.96 0.96 0.96 219.2
54,102
0.91 0.92 0.91 0.91
0.93 0.96
0.94 0.97 0.94 0.95
0.93
7.6
49
0.51 0.81 0.51 0.57 224,140
14.0
0.45 0.45
0.45
8,629
8.7
403
0.99 0.99 0.99 0.99
30.5

3,914
1,621
643
929
8,503
2,447
17
1,879
146
2,402

T(ms)
295
4,897
806
1,266
1,116
393
3,458
340

0.08 0.15 0.08 0.10
0.99 0.99 0.99 0.99

0.99

0.5

1

1

1

1

1

1

1

Table 6. Comparison of FOLD-RM and XGBoost on UCI Datasets

FOLD-RM

13

FOLD-RM

MLP

Dataset
anneal
avila
ecoli

Acc Prec Rec
1
0.99
0.99 0.99
0.33 0.49 0.33 0.39
0.80 0.82 0.80 0.80
drug-heroin 0.84 0.74 0.84 0.78
0.85 0.75 0.85 0.80
drug-crack
drug-semer 0.99 0.99 0.99 0.99

17.9
33.6
42.3
12.0
17.6
10.3

63
3,540
41
136
145
43

F1 Rules T(ms) Acc Prec Rec

T(ms)
F1
462
0.99 0.99 0.99 0.99
0.90 0.90 0.90 0.90
73,610
0.52 0.91 0.52 0.61
411
0.82 0.77 0.82 0.79
1,093
1,061
0.84 0.77 0.84 0.80
518
0.99
11,292
0.91 0.91 0.91 0.91 185.7 13,415 0.57 0.92 0.57 0.66
0.78 0.78 0.78 0.77 164.5
0.49 0.68 0.49 0.54
5,946
0.90 0.89 0.90 0.90
0.84 0.76 0.84 0.78 218,087
78.3
0.97 0.97 0.97 0.96
0.91 0.94 0.91 0.91
59.8
0.97 0.97 0.97 0.96
0.93 0.91 0.93 0.92
72.3
0.82 0.82 0.82 0.81
0.81 0.80 0.81 0.79
15.9
0.99 0.99 0.99 0.99
0.96 0.96 0.96 0.96 219.2
0.97
0.97 0.98
7.6
0.94 0.97 0.94 0.95
0.54 0.89 0.54 0.58
14.0
0.41 0.49 0.41 0.38
8.7
0.88 0.88 0.88 0.88
30.5

3,914
1,621
643
929
8,503
2,447
17
1,879
146
2,402

943
6,452
1,416
6,732
189
52,643
3,750
8,141

0.08 0.15 0.08 0.10
0.99 0.99 0.99 0.99

dry-bean
eeg
intention
nursery
pageblocks
parkinson
pendigits
wine
weight-lift
yeast
wall-robot

0.99

1

1

1

1

1

1

1

Table 7. Comparison of FOLD-RM and MLP on UCI Datasets

Dataset
ecoli
glass
optidigits
pendigits

RIPPER Acc FOLD-RM Acc Dataset

RIPPER Acc FOLD-RM Acc

0.80
0.63
0.90
0.95

0.80
0.63
0.90
0.95

ﬂags
nursery
pageblocks
shuttle

0.61
0.72
0.97
0.99

0.58
0.96
0.96
1

Table 8. Comparison of RIPPER and FOLD-RM on UCI Datasets

with the same datasets with the FOLD-RM algorithm. Two-thirds of the dataset was used for training by
Asadi and Shahrabi (2016) and the remaining one-third used as the test set. We follow the same convention.
For each dataset, this process was repeated 50 times. The average of accuracy is shown in Table 8. Both
algorithms have similar accuracy on most datasets, though FOLD-RM outperforms on nursery dataset.
Ripper is explainable, as it outputs CNF formulas. However, the CNF formulae generated tend to have large
number of literals. In contrast, FOLD-RM rules are succinct due to use of negation as failure and they have
an operational semantics (that aligns with how humans reason) by virtue of being a normal logic program.

6 Prediction and Justiﬁcation

The FOLD-RM algorithm generates rules that can be interpreted by the human user to understand the
patterns and correlations that are implicit in the table data. These rules can also be used to make prediction
given new data input. Thus FOLD-RM serves as a machine learning algorithm in its own right. However,
making good predictions is not enough for critical tasks such as disease diagnosis and loan approval. FOLD-
RM comes with a built-in prediction and justiﬁcation facility. We illustrate this justiﬁcation facility via an
example.

Example 3
The “annealing” UCI dataset is a multi-category classiﬁcation task which contains 798 training examples
and 100 test examples and their classes based on features such as steel, carbon, hardness, condition, strength,
etc. FOLD-RM generates the following answer set program with 20 rules for 5 classes, which is pretty
concise and precise:

classes(X,’3’) :- not surface_quality(X,’?’), not ab1(X),

not ab2(X), not ab3(X), not ab4(X).

14

Cambridge Author

classes(X,’2’) :- condition(X,’s’), not ab5(X).
classes(X,’3’) :- not carbon(X,’00’), not ab6(X).
classes(X,’5’) :- family(X,’tn’).
classes(X,’u’) :- steel(X,’a’), not ab7(X).
classes(X,’2’) :- thick(X,N32), N32>0.8, not ab8(X),
not ab9(X), not ab10(X).

classes(X,’3’) :- not steel(X,’s’), not ab11(X), not ab6(X).
classes(X,’1’) :- family(X,’?’).
classes(X,’1’) :- family(X,’zs’).
ab1(X) :- hardness(X,’85’).
ab2(X) :- strength(X,’600’).
ab4(X) :- hardness(X,’80’), cbond(X,’?’).
ab5(X) :- not steel(X,’r’), not enamelability(X,’2’).
ab6(X) :- steel(X,’a’).
ab8(X) :- steel(X,’r’).
ab10(X) :- not temper_rolling(X,’?’).

ab7(X) :- carbon(X,’03’).
ab9(X) :- steel(X,’s’).
ab11(X) :- not family(X,’?’).

ab3(X) :- carbon(X,’10’).

The above generated rule set achieves 0.99 accuracy, 0.99 weighted Macro precision, 0.99 weighted
Macro recall, and 0.99 weighed Macro F1 score. The justiﬁcation tree generated by the FOLD-RM system
for the 8th test example is shown below:

Proof Tree for example number 8 :
the value of classes is 2 DOES HOLD because

the value of condition is ’s’ which should equal ’s’ (DOES HOLD)
exception ab5 DOES NOT HOLD because

the value of steel is ’r’ which should not equal ’r’ (DOES NOT HOLD)
the value of enamelability is ’?’ which should not equal ’2’ (DOES HOLD)

{’condition: S’, ’enamelability: ?’, ’steel: R’}

This justiﬁcation tree is also shown in another format: by showing which rules were involved in the
proof/justiﬁcation. For each call in each rule that was invoked, FOLD-RM shows whether it is true ([T])
or false ([F]). The head of each applicable rule is similarly annotated. We illustrate this for the 8th test
example:

[F]ab5(X) :- not [T]steel(X,’r’), not [F]enamelability(X,’2’).
[T]classes(X,’2’) :- [T]condition(X,’s’), not [F]ab5(X).
{’condition: S’, ’enamelability: ?’, ’steel: R’}

7 Conclusions and Related Work

In this paper we presented FOLD-RM, an efﬁcient and highly scalable algorithm for multi-category clas-
siﬁcation tasks. FOLD-RM can generate explainable answer set programs and human-friendly justiﬁcation
for predictions. Our algorithm does not need any encoding (such as one-hot encoding) for data preparation.
Compared to the well-known classiﬁcation models like XGBoost and MLP, our new algorithm has similar
performance in terms of accuracy, weighted macro precision, weighted macro recall, and weighted macro
F1 score. However, our new approach is much more efﬁcient and interpretable than these other approaches.
It is remarkable that an ILP system is comparable in accuracy to state-of-the-art traditional machine learning
systems.

ALEPH by Srinivasan (2001) is a well-known ILP algorithm that employs bottom-up approach to in-
duce rules for non-numerical data. Also, no automatic method is available for the specialization process. A
tree-ensemble based rule extraction algorithm is proposed by Takemura and Inoue (2021), its performance
relies on trained tree-ensemble model. It may also suffer from scalability issue because its running time is
exponential in the number of valid rules.

In practice, statistical Machine Learning models show good performance for classiﬁcation. Extracting
rules from statistical models is also a long-standing research topic. Rule extraction algorithms are of two
kinds: 1) Pedagogical (learning rules from black box models without looking into internal structures), such
as, TREPAN by Craven and Shavlik (1995), which learns decision trees from neural networks 2) Decom-
positional (learning rules by analysing the models inside out) such as, SVM+Prototypes by Nu˜nez et al.
(2006), which employs clustering algorithm to extract rules from SVM classiﬁers by utilizing support vec-
tors. RuleFit by Friedman and Popescu is another rule extraction algorithm that learns sparse linear models
with original feature decision rules from shallow tree ensemble model for both classiﬁcation and regression
tasks. However, its interpretability decreases when too many decision rules have been generated. Also, sim-

FOLD-RM

15

pler approaches that are a combination of statistical method with ILP have been extensively explored. The
kFOIL system by Landwehr et al. (2006) incrementally learns kernel for SVM FOIL style rule induction.
The nFOIL system by Landwehr et al. (2005) is an integration of Naive Bayes model and FOIL. TILDE by
Blockeel and De Raedt (1998) is another top-down rule induction algorithm based on C4.5 decision tree,
it can achieve similar performance with Decision Tree. However, it would suffer from scalability issue
when there are too many unique numerical values in the dataset. For most datasets we experimented with,
the number of leaf nodes in the trained C4.5 decision tree is much more than the number of rules that
FOLD-R++/FOLD-RM generate. The FOLD-RM algorithm outperforms the above methods in efﬁciency
and scalability due to (i) its use of learning defaults, exceptions to defaults, exceptions to exceptions, and
so on (i) its top-down nature, and (iii) its use of improved method (preﬁx sum) for heuristic calculation.

Acknowledgment

Authors acknowledge support from NSF grants IIS 1718945, IIS 1910131, IIP 1916206, US DoD, Atos
Corp and Amazon Corp. We thank our colleagues Joaquin Arias, Parth Padalkar, Kinjal Basu, Sarat Chandra
Varanasi, Elmer Salzar, Fang Li, Serdar Erbatur, and Doug DeGroot for discussions and help.

References

AGGARWAL, C. C. 2018. Neural Networks and Deep Learning - A Textbook. Springer.
ARIAS, J., CARRO, M., CHEN, Z., AND GUPTA, G. 2020. Justiﬁcations for goal-directed constraint answer

set programming. Electronic Proceedings in Theoretical Computer Science, 325, 59–72.

ARIAS, J., CARRO, M., SALAZAR, E., MARPLE, K., AND GUPTA, G. 2018. Constraint answer set pro-

gramming without grounding. Theory and Practice of Logic Programming, 18, 3-4, 337–354.

ASADI, S. AND SHAHRABI, J. 2016. Ripmc: Ripper for multiclass classiﬁcation. Neurocomputing, 191,

19–33.

BARAL, C. 2003. Knowledge representation, reasoning and declarative problem solving. Cambridge Uni-

versity Press.

BISHOP, C. M. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics).

Springer-Verlag, Berlin, Heidelberg.

BLOCKEEL, H. AND DE RAEDT, L. 1998. Top-down induction of ﬁrst-order logical decision trees. Artiﬁ-

cial Intelligence, 101, 1, 285–297.

CHEN, T. AND GUESTRIN, C. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM

SIGKDD 2016, KDD ’16, pp. 785–794.

COHEN, W. W. Fast effective rule induction. In Proceedings of the Twelfth International Conference on
International Conference on Machine Learning 1995, ICML’95, 115–123, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.

CRAVEN, M. W. AND SHAVLIK, J. W. Extracting tree-structured representations of trained networks.
In Proceedings of the 8th International Conference on Neural Information Processing Systems 1995,
NIPS’95, pp. 24–30, Cambridge, MA, USA. MIT Press.

CROPPER, A. AND DUMANCIC, S. 2020. Inductive logic programming at 30: a new introduction.

https://arxiv.org/abs/2008.07912.

DUA, D. AND GRAFF, C. 2017. UCI machine learning repository.
GELFOND, M. AND KAHL, Y. 2014. Knowledge representation, reasoning, and the design of intelligent

agents: The answer-set programming approach. Cambridge University Press.

GRANDINI, M., BAGLI, E., AND VISANI, G. 2020. Metrics for multi-class classiﬁcation: an overview.

arXiv 2008.05756.

GUNNING, D. 2015. Explainable artiﬁcial intelligence (xai), https://www.darpa.mil/program/explainable-artiﬁcial-intelligence.

Retrieved June 2018.

16

Cambridge Author

LANDWEHR, N., KERSTING, K., AND RAEDT, L. D. nFOIL: Integrating na¨ıve bayes and FOIL. In Proc.

AAAI 2005, pp. 795–800.

LANDWEHR, N., PASSERINI, A., RAEDT, L. D., AND FRASCONI, P. kFOIL: Learning simple relational

kernels. In Proc. AAAI 2006, pp. 389–394.

MUGGLETON, S. 1991. Inductive logic programming. New Gen. Comput., 8, 4, 295–318.
MUGGLETON, S., DE RAEDT, L., POOLE, D., BRATKO, I., FLACH, P., INOUE, K., AND SRINIVASAN,

A. 2012. ILP turns 20. Mach. Learn., 86, 1, 3–23.

NU ˜NEZ, H., ANGULO, C., AND CATAL `A, A. 2006. Rule-based learning systems for support vector ma-

chines. Neural Processing Letters, 24, 1–18.

PLOTKIN, G. D. A further note on inductive generalization. In Machine Intelligence 1971, volume 6, pp.

101–124. Edinburgh University Press.

QUINLAN, J. R. 1990. Learning logical deﬁnitions from relations. Machine Learning, 5, 239–266.
REITER, R. 1980. A logic for default reasoning. Artiﬁcial Intelligence, 13, 1-2, 81–132.
SAKAMA, C. 2005. Induction from answer sets in nonmonotonic logic programs. ACM Trans. Comput.

Log., 6, 2, 203–231.

SHAKERIN, F. 2020. Logic Programming-based Approaches in Explainable AI and Natural Language Pro-

cessing. PhD thesis. Department of Computer Science, The University of Texas at Dallas.

SHAKERIN, F. AND GUPTA, G. Heuristic based induction of answer set programs, from default theories
to combinatorial problems. In 28th International Conference on Inductive Logic Programming (ILP Up
and Coming Papers) 2018, volume 2206, pp. 36–51.

SHAKERIN, F., SALAZAR, E., AND GUPTA, G. 2017. A new algorithm to automate inductive learning of

default theories. TPLP, 17, 5-6, 1010–1026.

SRINIVASAN, A. 2001. The aleph manual. http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/.
TAKEMURA, A. AND INOUE, K. 2021. Generating explainable rule sets from tree-ensemble learning meth-
ods by answer set programming. Electronic Proceedings in Theoretical Computer Science, 345, 127–140.
WANG, H. AND GUPTA, G. FOLD-R++: A toolset for automated inductive learning of default theories from
mixed data. In Proceedings The First International Workshop on Combining Learning and Reasoning:
Programming Languages, Formalisms, and Representations (CLeaR) 2022.

ZENG, Q., PATEL, J. M., AND PAGE, D. 2014. Quickfoil: Scalable inductive logic programming. Proc.

VLDB Endow., 8, 3, 197–208.

