2
2
0
2

b
e
F
2
2

]
L
M

.
t
a
t
s
[

4
v
2
5
1
0
0
.
0
1
9
1
:
v
i
X
r
a

On the Complexity of Approximating
Multimarginal Optimal Transport

Tianyi Lin⋆,⋄ Nhat Ho⋆,‡ Marco Cuturi⊳,⊲ Michael I. Jordan⋄,†

Department of Electrical Engineering and Computer Sciences⋄
Department of Statistics†
University of California, Berkeley
Department of Statistics and Data Sciences, University of Texas, Austin‡
CREST - ENSAE⊳, Google Brain⊲

February 23, 2022

Abstract

≥

We study the complexity of approximating the multimarginal optimal transport (MOT)
distance, a generalization of the classical optimal transport distance, considered here be-
tween m discrete probability distributions supported each on n support points. First,
we show that the standard linear programming (LP) representation of the MOT prob-
lem is not a minimum-cost ﬂow problem when m
3. This negative result implies that
some combinatorial algorithms, e.g., network simplex method, are not suitable for approx-
imating the MOT problem, while the worst-case complexity bound for the deterministic
O(n3m). We then propose two simple and
interior-point algorithm remains a quantity of
deterministic algorithms for approximating the MOT problem. The ﬁrst algorithm, which
we refer to as multimarginal Sinkhorn algorithm, is a provably eﬃcient multimarginal
generalization of the Sinkhorn algorithm. We show that it achieves a complexity bound
(0, 1). This provides a ﬁrst near-linear time complex-
of
ity bound guarantee for approximating the MOT problem and matches the best known
complexity bound for the Sinkhorn algorithm in the classical OT setting when m = 2.
The second algorithm, which we refer to as accelerated multimarginal Sinkhorn algorithm,
achieves the acceleration by incorporating an estimate sequence and the complexity bound
O(m3nm+1/3ε−4/3). This bound is better than that of the ﬁrst algorithm in terms of
is
1/ε, and accelerated alternating minimization algorithm [Tupitsa et al., 2020] in terms
of n. Finally, we compare our new algorithms with the commercial LP solver Gurobi.
Preliminary results on synthetic data and real images demonstrate the eﬀectiveness and
eﬃciency of our algorithms.

O(m3nmε−2) for a tolerance ε

∈

e

e

e

1

Introduction

The multimarginal optimal transport (MOT) [Gangbo and Swiech, 1998, Pass, 2015], the
general problem of aligning or correlating m
2 probability measures so as to maximize
eﬃciency (with respect to a given cost function), is a generalization of the optimal transport
(OT) problem [Villani, 2003]. From the Kantorovich formulation [Kantorovich, 1942], we seek
to solve the following optimization problem,

≥

min
γ∈Π(µ1,µ2,...,µm)

ZM1×M2×···×Mm

c(x1, x2, . . . , xm) dγ(x1, x2, . . . , xm),

(1)

⋆ Tianyi Lin and Nhat Ho contributed equally to this work.

1

 
 
 
 
 
 
. . .

Mm whose marginals are

where Π(µ1, µ2, . . . , µm) denotes the set of positive joint measures on the product space M1 ×
) is a given cost function. In the discrete
µi}i∈[m], and c(
M2 ×
·
{
setting where each of µi is supported on n support points, the MOT problem is equivalent to
a linear programming (LP) problem with mn constraints and nm variables, which means that
any algorithm requires at least nm arithmetic operations in general.

×

The MOT problem has been recognized as the backbone of numerous important appli-
cations, such as matching in economics [Ekeland, 2005, Carlier and Ekeland, 2010a,b], den-
sity functional theory in physics [Seidl et al., 2007, Buttazzo et al., 2012, Cotar et al., 2013,
Mendl and Lin, 2013], generalized Euler ﬂow in ﬂuid dynamics [Brenier, 1989, 1999, 2008] and
ﬁnancial mathematics [Dolinsky and Soner, 2014, Galichon et al., 2014]. Over the past ﬁve
years, the MOT problem has begun to attract considerable attention, due in part to a wide va-
riety of emerging applications in machine learning, including generative adversarial networks
(GANs) [Choi et al., 2018, Cao et al., 2019], clustering [Mi and Bento, 2020], domain adapta-
tion [Hui et al., 2018, He et al., 2019] and Wasserstein barycenters [Agueh and Carlier, 2011,
Cuturi and Doucet, 2014, Benamou et al., 2015, Carlier et al., 2015, Srivastava et al., 2018].
Due to the space limit, we refer the interested readers to Pass [2015] for other applications of
the MOT problem and Peyr´e and Cuturi [2019] for more details of the MOT problem from a
computational point of view.

In order to further motivate the MOT problem, we brieﬂy describe two representative

application problems arising from machine learning.

Example 1. The multimarginal Wasserstein GANs [Cao et al., 2019] are proposed to opti-
mize a feasible MOT distance among diﬀerent domains. This approach is based on a new
dual formulation of the MOT distance and overcomes the limitations of existing methods by
alleviating the distribution mismatching issue and exploiting cross-domain correlations.

We consider m

parameterized by θk for all k
parameterized by w, we deﬁne the MOT distance in the dual form as follows,

→

=

∈

}

{Dk}k∈[m] and the associated generative models gk
be the class of discriminators
F

f : Rd
{

R

2 target domains
[m]. Let

≥

W (ˆPs, ˆPθ1, . . . , ˆPθm) = max

f

E

x∼ˆPs

[f (x)]

m

−

Xk=1

E

λ+
k

x∼ˆPθk

[f (x)],

s.t. ˆPθk ∈ Dk, f

∈

Ω,

where ˆPs is the real source distribution, ˆPθk is the distribution generated by gk for all k
ˆPs and ˆx(k)
and Ω =
for all k
and is set as 1/m in practice when no prior knowledge is available.

[m],
ˆPθk
k f (ˆx(k))
≤
∈ F |
[m] are samples. Note that λ+
k reﬂects the importance of the k-th target domain
P

c(x, ˆx(1), . . . , ˆx(m))
}

m
k=1 λ+

where x

f
{
∈

f (x)

∈
∈

−

∈

Example 2. The free-support Wasserstein barycenter [Agueh and Carlier, 2011] is deﬁned
µk}k∈[m] deﬁned on Rd according to the OT
as a weighted barycenter of input measures
{
distance. As shown by Agueh and Carlier [2011], the computation of barycenters of measures
can be computed by solving a MOT problem.

We consider the discrete setting where input measures are µk =

∆n, the support points
pk = (pk,1, . . . , pkn)
P
shown in [Agueh and Carlier, 2011] that the Wasserstein barycenter of
λ = (λ1, . . . , λm)
ground cost function c =

n
i=1 pk,iδxi with weights
Rd and the Dirac measure δ. It is
µk}k∈[m] with weights
{
∆m according to the OT distance with the quadratic Euclidean distance

xi}i∈[n] ⊆
{

2 is

∈

∈

k · k

µλ :=

γi1,...,imδAi1,...,im (x),

X1≤ik≤n,∀k∈[m]

2

where Ai1,...,im(x) =
multimarginal transportation plan that solves the MOT problem in the LP form of

m
k=1 λkxik is the Euclidean barycenter and γ

Rn×···×n is an optimal

∈

min
X∈Rn×···×nh

C, X

,
i

P
s.t.

Xi1,...,ik−1,j,ik+1,...,im = pkj for all (k, j)

[m]

×

∈

[n],

X1≤il≤n,l6=k,∀l∈[m]

∈

P

[n]

where C is deﬁned as Ci1,...,im =
In practice, we set λk = 1/m for all k

2 for (i1, . . . , im)
Ai1,...,im(x)
k
[m] when no prior knowledge is available.

m
xik −
k=1(λk/2)
k
It is worthy noting that the barycenter µλ is in general composed of more than m Diracs,
xi}i∈[n]. This is diﬀerent
and that these Diracs are not constrained to be on the support points
{
from the ﬁxed-support Wasserstein barycenter that must be on the same support points
xi}i∈[n]
{
of the input measures. To be speciﬁc, the free-support Wasserstein barycenter is the “true”
barycenter of measures, while the ﬁxed-support Wasserstein barycenter is an approximation
on the ﬁxed support points. But, on the ﬂip side of the coin, the ﬁxed-support Wasserstein
barycenter can be computed without solving any MOT problem and the complexity bound is
polynomial in m, n and 1/ε [Kroshnin et al., 2019, Lin et al., 2020] where ε is the desired
accuracy.

[n].

. . .

×

×

∈

Algorithms for the OT problem. The OT problem is a special instance of the MOT
problem with m = 2 and has been studied thoroughly during the past decade. To the best of
our knowledge, there are mainly two group of algorithms for solving the OT problem.

The ﬁrst line of algorithms are combinatorial graph algorithms [Klein, 1967, Edmonds and Karp,

1972, Hassin, 1983, Tardos, 1985, Galil and Tardos, 1988, Goldberg and Tarjan, 1990, Hassin,
In-
1992, Ervolina and McCormick, 1993b,a, Orlin, 1993, 1997, Goldberg and Rao, 1998].
deed, the OT problem is a minimum-cost ﬂow problem [Schrijver, 2003], which has graph
structure and leads to eﬃcient combinatorial algorithms mentioned before. Examples include
the primal-dual cost scaling algorithm [Goldberg and Tarjan, 1990] and the network simplex
algorithm [Orlin, 1997]; see also Daitch and Spielman [2008] and Lee and Sidford [2014] for
some recent progresses.

The second line of algorithms, initialized with the Sinkhorn algorithm [Cuturi, 2013], are
developed for solving the OT problem through either entropy regularization or continuous opti-
mization algorithmic frameworks. Examples include Greenkhorn algorithm [Altschuler et al.,
2017, Lin et al., 2019a], accelerated ﬁrst-order primal-dual algorithms [Dvurechensky et al.,
2018b], accelerated Sinkhorn algorithms [Lin et al., 2019b, Guminov et al., 2019], and some
other algorithms [Blanchet et al., 2018, Jambulapati et al., 2019, Lahn et al., 2019, Xie et al.,
2020]. Even though these algorithms are very eﬃcient, with easy to implement routines in
practice, the Sinkhorn algorithm and its accelerated variants remain as the state-of-the-art
approach for the OT problem and serve as the default solver in the celebrated POT pack-
age [Flamary and Courty, 2017].

Algorithms for the MOT problem. While the theory for computing the OT distance
has received ample attention, the theory for computing the MOT distance is still nascent.
Since the MOT problem has the LP representation with mn constraints and nm variables, it
can be solved by many LP algorithms, e.g., the interior-point algorithm, whose complexity
bounds are however not near-linear. That is to say, the dependence of n is linear in nm up to
the logarithmic factors.

Two specialized algorithms are avaliable for solving the MOT problem: multimarginal
Sinkhorn algorithm and accelerated alternating minimization algorithm. The former one gen-
eralizes the Sinkhorn algorithm to the MOT setting but only has the asymptotic convergence

3

analysis [Benamou et al., 2015, 2019, Peyr´e and Cuturi, 2019]; the latter one is proposed by
the concurrent work [Tupitsa et al., 2020] for solving the same dual entropic regularized MOT
O(m3nm+1/2ε−1) when applied to solve
problem as ours and achieves the complexity bound of
the MOT problem along with our rounding scheme. However, their algorithm is not a near-
linear time approximation algorithm and the dependence of n can be potentially improved.

e

Contribution:
In this paper, we study the complexity of approximating the MOT problem
between m discrete probability distributions with at most n points in their respective supports.
Our contributions can be summarized as follows:

1. We show that the standard LP representation of the MOT problem is not a minimum-
3. This implies the ineﬃciency of many combinatorial
cost ﬂow problem when m
algorithms, including network simplex method, as well as the worst-case complexity
bound of

O(n3m) for the standard deterministic interior-point algorithms.

≥

e

2. We propose two simple and deterministic algorithms for solving the entropic regularized
MOT problem. The ﬁrst one is named as multimarginal Sinkhorn algorithm which can
be also used to solve the MOT problem along with a new rounding scheme. The achieved
O(m3nmε−2), which is near-linear in terms of nm, demonstrating
complexity bound is
that our algorithm is unimprovable in terms of n in general setting. To the best of
our knowledge, this is a ﬁrst near-linear time approximation algorithm for solving the
MOT problem while the existing ones are either only shown convergent [Benamou et al.,
2015, 2019, Peyr´e and Cuturi, 2019] or not near-linear time [Tupitsa et al., 2020]. The
second one is named as accelerated multimarginal Sinkhorn algorithm and achieves the
O(m3nm+1/3ε−4/3) when applied to solve the MOT problem. This
complexity bound of
complexity bound is better than that of the ﬁrst algorithm in terms of 1/ε, and the
accelerated alternating minimization algorithm [Tupitsa et al., 2020] in terms of n.

e

e

3. We compare our algorithms with the commercial LP solver Gurobi. Preliminary results
on both synthetic data and real images demonstrate the eﬀectiveness and eﬃciency of
our algorithms in practice.

Organization. The remainder of the paper is organized as follows. In Section 2, we present
the background materials on the MOT problem and derive some important properties of the
objective function in the dual entropic regularized MOT problem. In Section 3, we show that
the standard LP representation of the MOT problem is not a minimum-cost ﬂow problem
3. In Sections 4 and 5, we propose the multimarginal Sinkhorn and accelerated
when m
multimarginal Sinkhorn algorithms for solving the entropic regularized MOT problem. We
also demonstrate that these algorithms can solve the MOT problem eﬃciently along with our
new rounding scheme. In Section 6, we present some numerical results which validate the
eﬃciency of our algorithms. We ﬁnally conclude this paper in Section 7.

≥

1, 2, . . . , n
{

Notation. We let [n] be the set
}
non-negative components. 1n ∈
∆n is denoted as the probability simplex in Rn
+: ∆n =
denote
as its cardinality. For a diﬀerentiable function f , we denote
full gradient of f and the gradient of f with respect to β. For a vector x
we denote

+ be the set of all vectors in Rn with
Rn refers to a vector with all of its components are 1 and
Rn
. For a set S, we
n u = 1
}
∇βf as the
f and
∇
Rn and 1
,
p
≤ ∞
∈
as its ℓ2-norm for simplicity. Let x and y be two

and Rn

+ : 1⊤

u
{

S
|

≤

∈

|

x
k

kp as its ℓp-norm and

x
k

k

4

x, y
vectors of same dimension, we denote min
{
For a tensor A = (Ai1,...,im)
∈
Ai1,...,im|
k1 =
A
1≤ik≤nk,∀k∈[m] |
k
and each component is deﬁned by

k∞ = max1≤ik≤nk,∀k∈[m] |
Ai1,...,im|
A
k
Rnk as its k-th marginal for k
, and denote rk(A)
∈
∈

as the component-wise minimum of x and y.
and
[m]

Rn1×...×nm, we write

}

P

[rk(A)]j :=

Ai1,...,ik−1,j,ik+1,...,im.

X1≤il≤nl,∀l6=k

Let A and B be two tensors of same dimension, we denote their Frobenius inner product as

A, B
h

i

:=

X1≤ik≤nk,∀k∈[m]

Ai1,...,imBi1,...,im.

Given the dimension n and accuracy ε, the notation a = O (b(n, ε)) stands for the upper
bound a
O(b(n, ε))
indicates the previous inequality where C depends on the logarithmic function of n and ε.

b(n, ε) where C > 0 is independent of n and ε, and the notation a =

≤

C

·

e

2 Preliminaries

In this section, we ﬁrst present the linear programming (LP) representation of the multi-
marginal optimal transport (MOT) problem as well as a formal speciﬁcation of an approx-
imate multimarginal transportation plan. Then, we describe the entropic regularized MOT
problem and derive the dual entropic regularized MOT problem where the objective function
is in the form of the logarithm of sum of exponents. Finally, we provide several properties of
this function which are useful for the subsequent analysis.

2.1 Linear programming representation

The linear programming representation of the OT problem between two discrete probability
distributions with n supports dates back to the seminar work by Kantorovich [1942], and can
be written as

min
X∈Rn×nh

C, X

i

s.t. X1n = r, X ⊤1n = c, X

0.

≥

denotes an
In the above formulation, X
nonnegative cost matrix, and r and c stand for two probability distributions lying in the
simplex ∆n. Approximately solving the OT problem amounts to ﬁnding an ε-approximate
transportation plan ˆX such that ˆX1n = r, ˆX ⊤1n = c and the following inequality holds true,

denotes a transportation plan, C

∈

∈

Rn×n
+

Rn×n
+

where X ⋆ is deﬁned as an optimal transportation plan of the OT problem.

C, ˆX
h

C, X ⋆

i ≤ h

+ ε.

i

As a straightforward generalization of the OT problem, the MOT problem is also a LP.
2 discrete probability

Indeed, the problem of computing the MOT distance between m
distributions with n supports is in the following form of

≥

min
X∈Rn×···×nh

C, X

,
i

s.t. rk(X) = rk for any k

[m], X

0.

≥

∈

(2)

In the above formulation, X denotes the multimarginal transportation plan, C
denotes a nonnegative cost tensor, and
all lying in ∆n.

Rn×···×n
rk}k∈[m] stand for a set of probability distributions
{

∈

+

5

We see from Eq. (2), that the MOT problem is a linear programming with mn equality con-
straints and nm variables. The solution we hope to achieve is an ε-approximate multimarginal
transportation plan which generalizes the notion of an ε-approximate transportation plan of
the OT problem. More speciﬁcally, we have the following deﬁnition of ε-approximate multi-
marginal transportation plan.

Deﬁnition 1. The nonnegative tensor
transportation plan if rk(

∈
X) = rk for any k

X

Rn×···×n

+

is called an ε-approximate multimarginal

[m] and the following inequality holds true,

∈

b

C,
h

C, X ⋆

i ≤ h

+ ε,

i

b
X

where X ⋆ is deﬁned as an optimal multimarginal transportation plan of the MOT problem.

b

With this deﬁnition in mind, one of the goals of this paper is to develop near-linear time
approximation algorithms for solving the MOT problem. In particular, we seek the algorithms
whose running time required to obtain an ε-approximate multimarginal transportation plan
is nearly linear in the number of unknown variables nm. These algorithms are favorable
in modern machine learning applications since they are unimprovable up to the logarithmic
has nm
factors in general. Indeed, for the general MOT problem, the tensor X
unknown entries. In order to solve the MOT problem, the number of arithmetic operations
required by any algorithms is at least nm.

Rn×···×n

∈

+

In the classical OT setting, Altschuler et al. [2017] has shown that the Sinkhorn algo-
rithm is near-linear time approximation algorithm. Benamou et al. [2015, 2019] generalized
the Sinkhorn algorithm to the MOT setting but did not provide any complexity bound guar-
antee for their algorithms. Thus, it is still unclear whether there exists a near-linear time
approximation algorithm for the general MOT problem.

2.2 Entropic regularized MOT and its dual form

Building on Cuturi’s entropic approach to the classical OT problem [Cuturi, 2013], we consider
a regularized version of the MOT problem in which we add an entropic penalty function to
the objective in Eq. (2). The resulting problem is in the following form:

min
X∈Rn×···×n
s.t.

ηH(X)

C, X
h
rk(X) = rk for any k

i −

[m], X

0,

≥

∈

(3)

where η > 0 denotes the regularization parameter and H(X) denotes the entropic regulariza-
tion term, which is given by:

H(X) :=

X, log(X)

−h

1n×···×ni
.

−

It is important to note that if η is large, the resulting optimal value of the entropic regularized
MOT problem (cf. Eq (3)) yields a poor approximation to the unregularized MOT problem.
Moreover, another issue of entropic regularization is that the sparsity of the solution is lost.
Even though an ε-approximate transportation plan can be found eﬃciently, it is not clear how
diﬀerent the resulting sparsity pattern of the obtained solution is with respect to the solution
of the actual OT problem. In contrast, as a special instance of the MOT distance, the actual
OT distance suﬀers from the curse of dimensionality [Dudley, 1969, Fournier and Guillin,
2015, Weed and Bach, 2019, Lei, 2020] and is signiﬁcantly worse than its entropic regularized

6

version in terms of the sample complexity [Genevay et al., 2019, Mena and Niles-Weed, 2019].
This statistical drawback also holds true for the unregularized MOT distance in general.

While there is an ongoing debate in the literature on the merits of solving the actual OT
problem versus. its entropic regularized version, we adopt here the viewpoint that reaching
an additive approximation of the actual MOT cost matters and therefore propose to scale η
as a function of the desired accuracy of the approximation.

Then we proceed to derive the dual form of the entropic regularized MOT problem in
Eq. (3). As in the usual 2-marginals OT case [Cuturi and Peyr´e, 2018], the dual form of the
MOT problem with m

3 remains an unconstrained smooth optimization problem.

By introducing the dual variables

≥

Lagrangian function of the entropic regularized MOT problem in Eq. (3) as follows:

λ1, . . . , λm} ⊆
{

∈

Rn and τ

R, we can deﬁne the

(X, λ1, . . . , λm) =

C, X
h

i −

ηH(X)

L

m

−

Xk=1

λ⊤
k (rk(X)

rk).

−

(4)

Note that the entropy function H(X) is not well deﬁned for any negative matrix X. Thus,
we can neglect the non-negative constraint X
whose
Rnm. In order to derive the smooth dual objective function, we consider
domain is Rn×...×n
the following minimization problem:

0 and deﬁne the above function

≥

×

L

+

min
X:kXk1=1h

C, X

i −

ηH(X)

m

−

Xk=1

λ⊤
k (rk(X)

rk).

−

In the above problem, the objective function is strongly convex. Thus, the optimal solution
is unique. After the simple calculations, the optimal solution ¯X = X(λ1, . . . , λm) has the
following form:

¯Xi1...im =

k=1 λkik −Ci1i2...im )

eη−1(Pm
1≤ik≤n,∀k∈[m] eη−1(Pm

k=1 λkik −Ci1i2...im )

.

(5)

Plugging Eq. (5) into Eq. (4) yields that the dual form is:

P

max

λ1,...,λm 


η log



−

X1≤i1,...,im≤n



eη−1(Pm

k=1 λkik −Ci1i2...im )

+



m

Xk=1

.

λ⊤
k rk




In order to streamline our subsequent presentation, we perform a change of variables, βk =
η−1λk, and reformulate the above problem as





ePm

k=1 βkik −

Ci1i2...im
η

m

β⊤
k rk.



−

min
β1,...,βm

ϕ(β1, . . . , βm) := log



X1≤i1,i2,...,im≤n

Xk=1

To further simplify the notation, we deﬁne B(β) := (Bi1...im)i1,i2,...,im∈[n] ∈
β = (β1, . . . , βm) by
Bi1...im = ePm
To this end, we obtain the dual entropic regularized MOT problem deﬁned by

Ci1i2...im
η

k=1 βkik −



.

Rn×...×n where

min
β1,...,βm

B(β1, . . . , βm)
ϕ(β1, . . . , βm) := log(
k

k1)

−

β⊤
k rk.

m

Xk=1

(6)

7

Remark 2.1. The ﬁrst part of the objective function ϕ is in the form of the logarithm of sum
of exponents while the second part is a linear function. This is diﬀerent from the objective
function used in previous dual entropic regularized OT problem [Cuturi, 2013, Altschuler et al.,
2017, Dvurechensky et al., 2018b, Lin et al., 2019a]. We also note that Eq. (6) is a special
instance of a softmax minimization problem, and the objective function ϕ is known to be
smooth [Nesterov, 2005]. Finally, we point out that the same problem was derived in the later
work by Tupitsa et al. [2020] and used for analyzing the accelerated alternating minimization
algorithm.

In the remainder of the paper, we also denote β⋆ = (β⋆
solution of the dual entropic regularized MOT problem in Eq. (6).

1 , . . . , β⋆

m)

Rmn as an optimal

∈

2.3 Properties of dual entropic regularized multimarginal OT

In this section, we present several useful properties of the dual entropic regularized MOT in
Eq. (6). In particular, we show that there exists an optimal solution β⋆ such that it has an
upper bound in terms of the ℓ∞-norm.

Lemma 2.2. For the dual entropic regularized MOT problem in Eq. (6), there exists an
optimal solution β⋆ = (β⋆

1 , . . . , β⋆

m) such that

β⋆
k

where R > 0 is deﬁned as

k∞ := max

1≤i≤m k

β∗
i k∞ ≤

R,

R := k

C

k∞
η −

log

(cid:18)

min
1≤i≤m,1≤j≤n

rij

.

(cid:19)

Proof. First, we claim that there exists an optimal solution β⋆ = (β⋆

1 , . . . , β⋆

m) such that

min
1≤j≤n

β⋆
ij ≤

0

≤

max
1≤j≤n

β⋆
ij for any i

[m].

∈

β⋆
1 , . . . ,
Indeed, letting
satisﬁes Eq. (8). Otherwise, we let m shift terms be

β⋆ = (

β⋆
m) be an optimal solution to Eq. (6), the claim holds true if

b

b

∆

βi =

b
max1≤j≤n

1 , . . . , β⋆
and deﬁne β⋆ = (β⋆
b

m) by

β⋆
ij + min1≤j≤n

β⋆
ij

2

b

∈

b

R for any i

[m].

∈

β⋆
i =

β⋆
i −

∆

βi1n for any i

[m].

∈

(7)

(8)

β⋆

b

By the deﬁnition of β⋆, it is clear that β⋆ satisﬁes Eq. (8). Since 1⊤
b
have (β⋆
βi for all i
log(
B(
k
fore, β⋆ is an optimal solution that satisﬁes Eq. (8).
P

n ri = 1 for all i
B(β⋆
[m]. In addition, we have log(
k
βi. Putting these pieces together yields ϕ(β⋆) = ϕ(

i )⊤ri −
i )⊤ri = (
β⋆
β⋆
β⋆
k1)+
m)
1 , . . . ,
b
Then, we show that

∆
m
i=1 ∆
b

∈

b

b

b

b

b

[m], we
∈
1 , . . . , β⋆
k1) =
m)
β⋆). There-

max
1≤j≤n

β⋆
ij −

min
1≤j≤n

β⋆
ij ≤

C
k

k∞
η −

log

min
1≤i≤m,1≤j≤n

(cid:18)

rij

for all i

(cid:19)

[m].

∈

(9)

8

(10)

(11)

(12)

(13)

Indeed, for any (j, l)

[m]

[n], we derive from the optimality condition of β⋆ that

∈
×
1≤ik≤n,∀k6=j ePk6=j β⋆
B(β⋆
1 , . . . , β⋆
m)
k

kik

k1

eβ⋆

jl

P

−η−1Ci1···l···im

Since C is a nonnegative cost tensor, we have

β⋆
jl ≥

log

(cid:18)

min
1≤i≤m,1≤j≤n

rij

log

−

(cid:19)





X1≤ik≤n,∀k6=j

Since rjl ∈

[0, 1] and Ci1...im ≤ k
C

k∞, we have

= rjl ≥

min
1≤i≤m,1≤j≤n

rij.

ePk6=j β⋆

kik





B(β⋆
+ log(
k

1 , . . . , β⋆
m)

k1).

β⋆
jl ≤

C
k

k∞
η −

log



ePk6=j β⋆
kik

B(β⋆
+ log(
k



1 , . . . , β⋆
m)

k1).

X1≤ik≤n,∀k6=j
Combining the bounds in Eq. (10) and Eq. (11) implies the desired Eq. (9).





Finally, we prove that Eq. (7) holds true. Indeed, Eq. (8) and Eq. (9) imply that

C
k∞
k
η

−

+ log

min
1≤i≤m,1≤j≤n

rij

(cid:18)

(cid:19)

min
1≤j≤n

β⋆
ij ≤

≤

0 for any i

[m],

∈

and

0

max
1≤j≤n

β⋆
ij ≤

≤

C
k

k∞
η −

log

min
1≤i≤m,1≤j≤n

rij

for any i

[m].

∈

(cid:18)

(cid:19)

Combining Eq. (12) and Eq. (13) with the deﬁnition of R implies that max1≤i≤m k
and hence the desired Eq. (7).

β⋆
i k∞ ≤

R
(cid:3)

The upper bound for the ℓ∞-norm of an optimal solution of dual entropic-regularized
multimarginal OT in Lemma 2.2 directly leads to the following direct bound for the ℓ2-norm.

Corollary 2.3. For the dual entropic regularized MOT problem in Eq. (6), there exists an
optimal solution β⋆ = (β⋆

1 , . . . , β⋆

m) such that

where R > 0 is deﬁned in Lemma 2.2.

β∗
k

k ≤

√mnR,

Since the function

H(X) is strongly convex with respect to the ℓ1-norm on the probability
, the entropic regularized MOT problem in Eq. (3) is a special case of the

−

Rnm

simplex Q
following linearly constrained convex optimization problem:

⊆

min
x∈Q

f (x),

s.t. Ax = b,

where f is strongly convex with respect to the ℓ1-norm on the set Q:

f (x′)

f (x)

(x′

x)⊤

∇

−

−

f (x)

≥

−

η
2 k

x′

x

1 for any x′, x
2
k

∈

−

Q.

We use the ℓ2-norm for the dual space of the Lagrange multipliers. By Nesterov [2005, Theo-
rem 1], the dual objective function ˜ϕ satisﬁes the following inequality:

ϕ(λ′)

ϕ(λ)

(λ′

−

−

λ)⊤

∇

ϕ(λ)

≤

−

2
A
1→2
k
k
2η

λ′
k

λ

2 for any λ′, λ
k

−

∈

Rmn.

e

e

e

9

Recall that the function ˜ϕ is given by

ϕ(λ) =

η log

−



e



eη−1(Pm

k=1 λkik −Ci1i2...im )

X1≤i1,...,im≤n

We notice that the function ϕ in Eq. (6) is deﬁned by

m

+

λ⊤
k rk.

Xk=1





ϕ(β) =

−

η−1

ϕ(η(β + (1/m)1mn)).

After some calculations, we have

e

ϕ(β′)

ϕ(β)

(β′

β)⊤

∇

−

−

ϕ(β)

−

2
A
1→2
k
k
2

(cid:19)

≤

(cid:18)

β′
k

β

2.
k

−

(14)

By deﬁnition, each column of the matrix A contains no more than m nonzero elements which
k1→2 is equal to maximum ℓ2-norm of the column of this matrix,
are equal to one. Since
A
k
k1→2 = √m. Thus, the dual objective function ϕ is m-gradient Lipschitz with
A
we have
k
respect to the ℓ2-norm. This implies that the squared norm of the gradient is bounded by
the dual objective gap [Nesterov, 2018]. We present this result in the following lemma and
provide the proof for the sake of completeness.

Lemma 2.4. For any given vector β

Rnm, we have

∈

m

ϕ(β)

Xi=1  

argmin
γ∈Rn

−

ϕ(β1, . . . , βi−1, γ, βi+1, . . . , βm)

Proof. We derive from Eq. (14) with ¯βi = βi −

ϕ(β)

argmin
γ∈Rn

−

ϕ(β1, . . . , βi−1, γ, βi+1, . . . , βm)

1
2m

1

(cid:18)

(cid:19)

k∇

! ≥
m ∇βiϕ(β) and ¯βk = βk for k
1
ϕ( ¯β)
2m

ϕ(β)

ϕ(β)

−

−

≤

≤

(cid:18)

2.
ϕ(β)
k

= i that

2.
k∇iϕ(β)
k

(cid:19)

(cid:3)

Summing up the above inequality over i

[m] yields the desired inequality.

∈

3 Computational Hardness

In this section, we show that the multimarginal optimal transport (MOT) problem in the
form of Eq. (2) is not a minimum-cost ﬂow problem when m
3. The proof idea is based on
a simple reduction with m-dimensional matching problem.

≥

3.1 Unimodularity, minimum-cost ﬂow and matching

We present some deﬁnitions and classical results in combinatorial optimization and graph
theory, including unimodularity, minimum-cost ﬂow and matching.

Deﬁnition 2. A totally unimodular (TU) matrix is one for which every square submatrix has
determinant

1, 0 or 1.

−

A direct way to determine whether a matrix is totally unimodular or not is by computing
the determinants of every square submatrix of this matrix. However, it is clearly intractable
in general. The following proposition provides an alternative way to check whether a matrix
is TU or not.

10

6
Proposition 3.1. Let A be a
-valued matrix. A is TU if each column contains at
1, 0, 1
}
most two nonzero entries and all rows are partitioned into two sets I1 and I2 such that: If
two nonzero entries of a column have the same sign, they are in diﬀerent sets. If these two
entries have diﬀerent signs, they are in the same set.

{−

In what follows, we present the deﬁnition of minimum-cost ﬂow problem and prove that
the constraint matrix of LP representation of a minimum-cost ﬂow problem is TU. Such result
is well known and can be derived from Berge [2001, Theorem 1, Chapter 15] which shows that
the incidence matrices of every directed graphs are TU. For the sake of completeness, we
provide the detailed proof based on Proposition 3.1.

Deﬁnition 3. The minimum-cost ﬂow problem ﬁnds the cheapest possible way of sending a
certain amount of ﬂow through a ﬂow network. Formally,

min
s.t.

f (u, v)
P
≥
f (u, v)
≤
f (u, v) =

(u,v)∈E f (u, v)

·

a(u, v)
0,
E,
for all (u, v)
c(u, v) for all (u, v)

∈

∈
f (v, u) for all (u, v)
(u,w)∈E or (w,u)∈E f (u, w) = 0,
w∈V f (s, w) = d and

−

E,

E,

∈

w∈V f (w, t) = d.

P
P
V , where each edge (u, v)

∈

V and
The ﬂow network G = (V, E) is a directed graph G = (V, E) with a source vertex s
a sink vertex t
0 and
E has capacity c(u, v) > 0, ﬂow f (u, v)
cost a(u, v), with most minimum-cost ﬂow algorithms supporting edges with negative costs.
a(u, v). The problem requires
The cost of sending this ﬂow along an edge (u, v) is f (u, v)
an amount of ﬂow d to be sent from source s to sink t. The deﬁnition of the problem is to
minimize the total cost of the ﬂow over all edges.

∈
≥

P

∈

·

Proposition 3.2. The constraint matrix arising from a minimum-cost ﬂow problem is TU.

Proof. The standard LP representation of the minimum-cost ﬂow problem is

min
x∈R|E|

c⊤x,

s.t. Ax = b, l

x

≤

≤

u.

R|E| with xj being the ﬂow through arc j, b

R|V | with bi being external supply
where x
at node i and 1⊤b = 0, cj is unit cost of ﬂow through arc j, lj and uj are lower and upper
R|V |×|E| is the arc-node incidence matrix with entries
bounds on ﬂow through arc j and A

∈

∈

∈

Aij =




−

1 if arc j starts at node i
1 if arc j ends at node i
0 otherwise

.

Since each arc has two endpoints, the constraint matrix A is a
which each column contains two nonzero entries 1 and
that A is TU and the rows of A are categorized into a single set.

-valued matrix in
1, 0, 1
}
1. Using Proposition 3.1, we obtain
(cid:3)

{−



−

We proceed to the deﬁnition of m-dimensional matching which generalizes 2-dimensional

matching. We present it in graph-theoretic sense as follows.
Deﬁnition 4. Let S1, S2, . . . , Sm be ﬁnite and disjoint sets, and let T be a subset of S1 ×
[m]. Now
· · · ×
M
for any two distinct vectors
⊆
(z1, . . . , zm)

Sm. That is, T consists of vectors (z1, . . . , zm) such that zi ∈
T is a m-dimensional matching if the following holds:

M and (z′

Si for all i

= z′

[m].

∈

1, . . . , z′

m)

i for all i

M , we have zi 6

∈

∈

∈

11

In computational complexity theory, m-dimensional matching refers to the following deci-
sion problem: given a set T and an integer k, decide whether there exists a m-dimensional
matching M
k. This problem is NP-complete even when m = 3 and
[Karp, 1972, Garey and Johnson, 2002]. A m-dimensional matching is
k =
S1|
|
also an exact cover since the set M covers each element of S1, S2, . . . , Sm exactly once.

T with
S3|
|

⊆
S2|
|

M
|

| ≥

=

=

3.2 Main result

The problem of computing the MOT distance between m
with at most n supports is equivalent to solving the following LP (cf. Eq. (2)):

2 discrete probability distributions

≥

min
X∈Rn×···×nh

C, X

,
i

s.t. rk(X) = rk for any k

[m], X

0.

≥

∈

In other words, the MOT problem is a LP with mn equality constraints and nm variables.
When m = 2, the MOT problem is the classical OT problem [Villani, 2003] which is known to
be a minimum-cost ﬂow problem. Such problem structure is computationally favorable and
permits the development of provably eﬃcient algorithms, including the network simplex algo-
rithms [Orlin, 1997, Tarjan, 1997] and specialized interior-point algorithms [Lee and Sidford,
2014]. However, it remains unknown if the MOT problem in the above LP form admits such
a structural decomposition when m

3.

We present a negative answer to this question for m
≥
theorem, we provide a simple yet intuitive counterexample.

≥

3. Before proceeding to the main

Example 3. We consider arguably the simplest MOT problem, with m = 3 distributions
supported on n = 2 elements each. We consider the nm = 8 entries of a multimarginal
tensor transportation plan, and number them slice by slice. A naive enumeration of all the
marginal constraints results in nm linear equalities, but some of them are redundant since they
involve several times the constraints that the sum of the elements of that tensor sum to 1. The
number of required constraints is m(n
1) + 1, namely only 4 mass conservation constraints
are eﬀective in this case. We therefore obtain the following matrix,

−

1 1 0 0 1 1 0 0
0 0 1 1 0 0 1 1
1 0 1 0 1 0 1 0
1 1 1 1 0 0 0 0



.





A = 





We form the sub-matrix by only considering the ﬁrst, fourth, sixth, and seventh columns of A,
and can then check that the resulting matrix has determinant equal to 2, namely,

det(A1,4,6,7) = det 







= 2.

1 0 1 0
0 1 0 1
1 0 0 1
1 1 0 0








Therefore, the marginal constraint matrix is not totally unimodular, illustrating that the MOT
with (m, n) = (3, 2) is not a minimum-cost ﬂow problem. More generally, one can numerically
check that the constraint matrix corresponding to m marginals with n points each has size
(mn
m + 1)
columns (out of nm) that form a determinant that is neither
1, 0, 1. The constraint matrix
itself can be obtained recursively, by deﬁning ﬁrst Ln,1 = In, to apply next that for t

nm, and that it is not totally unimodular by selecting a subset of (mn

m + 1)









2,

−

−

−

×

≥

Ln,t =

1n ⊗
(cid:2)

Ln,t−1

In ⊗

1nt−1

∈

Rnt×nt,

(cid:3)

12

where
⊗
dual multimarginal OT problem, which involves constraints of the type (α1)i1 + (α2)i1 +
(αm)im ≤
speciﬁed over the entries of transportation tensors, is An,m = ˜L⊤
Ln,m stripped of m
at 2n, 3n, . . . , nm.

is Kronecker’s product. In that case, Ln,m corresponds to the matrix constraint of the
+
Ci1i2...im as mentioned in the next section. The constraint matrix in the primal,
n,m, where ˜Ln,m is equal to
1 columns (one for each marginal but for the ﬁrst), indexed for instance

· · ·

−

≥

Example 3 provides some intuitions why the MOT problem is not a minimum-cost ﬂow
3. However, it is not easy to extend this approach to the general setting.
problem when m
Indeed, the constraint matrix in Eq. (2) becomes complicated when m and n are considerably
large. Thus, it is challenging to compute the determinants of even a small fraction of sub-
matrices, which is necessary to determine whether the constraint matrix is totally unimodular
or not. While the direct calculation is intractable, some combinatorial optimization toolbox,
e.g., Ghouila-Houri’s theorem [Ghouila-Houri, 1962], might be helpful. However, we do not
have concrete idea now and leave this topic to the future work.

Despite the above discussion, we can prove that the MOT problem in Eq. (2) is not a
minimum-cost ﬂow problem when m
3 by using a simple reduction with m-dimensional
matching problem. Roughly speaking, if the MOT problem is a minimum-cost ﬂow problem
when m
3, its integer programming counterpart with speciﬁc choice of the cost tensor C
must not be NP-hard. However, due to such speciﬁc choice, we can prove
and marginals
that this integer programming counterpart is equivalent to m-dimensional matching problem
which is known as NP-complete when m

3. This leads to the contradiction.

rk}
{

≥

≥

We present our theorem with the proof details as follows.

≥

Theorem 3.3. The MOT problem in the form of Eq. (2) is not a minimum-cost ﬂow problem
when m

3.

≥

Proof. We prove the result by contradiction. Indeed, we assume that the MOT problem in
Eq. (2) is a minimum-cost ﬂow problem when m
[m] in Eq. (2),
the resulting LP is equivalent to the following problem

1n
n for all k

3. Let rk =

≥

∈

min
X h

C, X

,
i

s.t. rk(X) = 1n for any k

[m], X

0.

≥

∈

(15)

We see from Eq. (15) that this is a minimum-cost ﬂow problem where the constraint matrix and
the right-hand side vector are both integer-valued. Then we consider the integer programming
counterpart of Eq. (15) which is deﬁned by

min
X
s.t.

,
i

C, X
h
rk(X) = 1n, for any k
Xi1i2...im ∈ {

0, 1
}

[m],
for any (i1, . . . , im)

∈

(16)

[n]

×

∈

. . .

×

[n].

It is well known in the combinatorial optimization literature [Schrijver, 2003] that Eq. (16) is
not NP-hard when m

3.

On the other hand, we claim that Eq. (16) is NP-complete when m

to an m-dimensional matching problem.
[n]

. . .

[n]

[n] as well as the cost tensor C is deﬁned by

≥
Indeed, we let Si = [n] for all i

3 since it reduces
[m] and T =

∈

≥

×

×

×

Ci1i2...im =

1 (i1, i2, . . . , im)
∈
0 (i1, i2, . . . , im) /
∈

T
T

(cid:26)

.

13

Then the objective function of any feasible solution is n so any feasible solution is an optimal so-
lution. Furthermore, ﬁnding any optimal solution X is equivalent to ﬁnding an m-dimensional
matching M . Indeed, we can deﬁne an one-to-one mapping as follows,

Xi1i2...im =

1 (i1, i2, . . . , im)
∈
0 (i1, i2, . . . , im) /
∈

M
M

(cid:26)

.

It is clear that X is a feasible solution of Eq. (16) if and only if M is an m-dimensional matching
M . Thus, Eq. (16) is NP-complete when m
3, which leads to a clear contradiction. This
(cid:3)
completes the proof of Theorem 3.3.

≥

3.3 Discussion

We make a few comments on our main result for the MOT problem in Eq. (2), which help
strengthen the understanding of the MOT problem.

First, Theorem 3.3 only holds true for the general MOT problem in Eq. (2). To be more
speciﬁc, we show that there exists the cost tensor C and marginals
such that the MOT
problem in Eq. (2) is not a minimum-cost ﬂow problem. Nonetheless, the MOT problem is
commonly referred to as the LP in Eq. (2) and thus it is important to understand the structure
of this LP when m
3. Further, we remark that there is no other reformulation of the general
MOT problems which can be solved eﬃciently; in particular, Altschuler and Boix-Adsera
[2021a] showed that the MOT problems with repulsive costs are computationally intractable:
several such problems of interest are NP-hard to solve – even approximately.

rk}
{

≥

≥

Second, Theorem 3.3 does not rule out the possibility that a few instances of the MOT
problem are minimum-cost ﬂow problems when m
3. However, such examples need to
admit special structure and are thus rare in real applications given that Example 3 is one of
the simplest MOT problems. Many common MOT problems, e.g., Wasserstein barycenters,
are not minimum-cost ﬂow problems.
Indeed, Lin et al. [2020] proved this result for even
the simplest Wasserstein barycenter problems in the standard LP form — the ﬁxed-support
3. Despite such negative result, the discrete
Wasserstein barycenters when m
≥
Wasserstein barycenter problems have their own structure [Anderes et al., 2016] and can be
eﬃciently solved in practice [Cuturi and Doucet, 2014, Benamou et al., 2015, Carlier et al.,
2015, Staib et al., 2017, Claici et al., 2018, Dvurechensky et al., 2018a, Kroshnin et al., 2019,
3 is diﬀerent from the OT
Ge et al., 2019].
problem and we believe that the minimum-cost ﬂow is not suﬃcient for characterizing the
structure of the MOT problem.

In conclusion, the MOT problem with m

3 and n

≥

≥

≥

O(m2nm+2) [Orlin, 1997] or better bound of

Finally, Theorem 3.3 aﬀects the complexity bound of various algorithms for solving the
3. If we could write the MOT problem as a minimum-cost ﬂow
MOT problem when m
problem on the directed graph with nm edges and mn vertices, the network simplex method
achieves the complexity bound of
1997], while the specialized interior-point algorithm achieves the complexity bound of
2014]. However, due to Theorem 3.3, these bounds of network simplex method and special-
ized interior-point algorithms are not valid and only the standard interior-point algorithms
O(n3m) [Wright, 1997]. We are also aware of
can achieve much worse complexity bound of
a stochastic central path method [Cohen et al., 2019] which achieves better complexity bound
of
2.38. However, this algorithm
is seemingly not implementable in practice and not comparable with the deterministic algo-
rithms we propose in this paper.

O(nωm) with the coeﬃcient of matrix multiplication ω

≈

e

e

e

e

e

O(mnm+1) [Tarjan,

O(√mnnm) [Lee and Sidford,

14

Algorithm 1: MultiSinkhorn(C, η,

Initialization: t = 0 and β0
while Et > ε′ do

∈

rk}k∈[m], ε′)
{
Rmn with β0 = 0mn.

Step 1. Choose the greedy coordinate K = argmax1≤k≤m ρ(rk, rk(B(βt))).
Step 2. Compute βt+1

Rmn by

∈

βt+1
k =

(cid:26)

βt
k + log(rk)
βt
k,

−

log(rk(B(βt))), k = K

otherwise

.

Step 3. Increment by t = t + 1.

end while
Output: B(βt).

4 Multimarginal Sinkhorn Algorithm

In this section, we propose and analyze a multimarginal Sinkhorn algorithm for solving the
entropic regularized multimarginal optimal transport (MOT) problem. We also generalizes the
rounding scheme [Altschuler et al., 2017] to the MOT setting. Together with a new rounding
O(m3nmε−2) when applied to solve
scheme, our algorithm achieves a complexity bound of
the MOT problem. The proof techniques are heavily based on the smooth dual objective
function in Eq. (6) and thus not a straightforward generalization of the analysis in the OT
setting [Altschuler et al., 2017, Dvurechensky et al., 2018b, Lin et al., 2019a] where they use
diﬀerent form of dual objective function; see Remark 2.1 for the details.

e

4.1 Algorithmic procedure

We present the pseudocode of the multimarginal Sinkhorn algorithm in Algorithm 1. This
algorithm is a new generalization of the classical Sinkhorn algorithm [Cuturi, 2013] and
diﬀerent from the existing multimarginal Sinkhorn algorithms [Benamou et al., 2015, 2019,
Indeed, the main diﬀerence lies in the greedy choice of the next
Peyr´e and Cuturi, 2019].
marginal (cf. Step 2). This simple yet crucial modiﬁcation makes our complexity bound
analysis work while only the asymptotic convergence properties are proved for the existing
multimarginal Sinkhorn algorithms.

Comments on algorithmic scheme. Algorithm 1 can be interpreted as a greedy block
coordinate descent algorithm [Dhillon et al., 2011, Nutini et al., 2015] for solving the dual
entropic regularized MOT problem in Eq. (6); see Tupitsa et al. [2020, Lemma 1] for the
justiﬁcation. However, the corresponding known complexity bounds for greedy block coordi-
nate descent algorithms can not be applied to analyze Algorithm 1. Indeed, the per-iteration
progress is quantiﬁed using the ℓ2-norm in the existing algorithmic scheme and convergence
analysis. This will lead to the worse complexity bound than ours since it does not respect
the structure of MOT problem. In contrast, Algorithm 1 employs KL divergence to quan-
tify the per-iteration progress and link it to the ℓ1-norm via appeal to the Pinsker inequal-
ity [Cover and Thomas, 2012].

More speciﬁcally, an exact coordinate update for the K-th variable is performed at each
iteration while other variables are ﬁxed1. Here we choose K by using the greedy rule as

1 Meshi et al. [2012] showed that the greedy rule was implemented eﬃciently by using a max-heap structure

15

Algorithm 2: Round(X,

rk}k∈[m])
{
Initialization: X (0) = X.
for k = 1 to m do

Compute zk = min
{
for j = 1 to n do

1n, rk/rk(X (k−1))

} ∈

Rn.

i1i2...im = zkj X (k−1)

i1i2...im in which ik = j is ﬁxed.

X (k)
end for

end for
Compute errk ∈
Compute Y

∈

Rn such that errk = rk −
Rn×n×...×n by

rk(X (m)) for all k

[m].

∈

Yi1i2...im = X (m)

i1i2...im +

m
k=1 errkik
m−1
err1k
1
Q
k

for any (i1, . . . , im)

[n]

×

∈

. . .

×

[n].

Output: Y .

follows,

K = argmax
1≤k≤m

ρ(rk, rk(B(βt))),

where ρ : Rn

+ ×

Rn

+ →

R+ is deﬁned as

ρ(a, b) := 1⊤

n (b

−

n

a) +

ai log

Xi=1

ai
bi (cid:19)

.

(cid:18)

Following up the optimal transport literature [Cuturi, 2013, Altschuler et al., 2017], we set
the stopping criterion as Et ≤

ε′ for some tolerance ε′ > 0, where Et is deﬁned by

Et :=

m

Xk=1

rk(B(βt))
k

rkk1.

−

(17)

Comments on arithmetic operations per iteration. The most expensive step is to
determine which coordinate is the greedy one. While the naive way requires O(mnm) arith-
metic operations to compute all marginals rk(B(βt)), we can adopt some implementation
tricks based on the observation that one of ρ(rk, rk(B(βt))) = 0 after the ﬁrst step.

Without loss of generality, we assume that m

3 and r1(B(βt)) = r1. The key step
n
is to construct a small tensor A which has nm−1 entries: Ai1,...,im−1 =
j=1 Bj,i1,...,im−1 for
[n]. This requires O(nm) arithmetic operations. It is clear that
any (i1, . . . , im−1)
r2(B(βt)), . . . , rm(B(βt)) exactly corresponds to the marginals of A and the computation
only needs O(mnm−1) arithmetic operations. Putting these pieces together yields that the
arithmetic operations per iteration is O(nm) for the case of m = O(n).

× · ×

[n]

P

≥

∈

Rounding scheme. Algorithm 1 is developed for solving the entropic-regularized MOT
problem and the output is not necessarily a feasible solution of unregularized MOT problem.
To address this issue, we develop a new rounding scheme by extending Altschuler et al. [2017,
Algorithm 2] to the MOT setting; see Algorithm 2. We can see that the diﬀerence between the
input and output of Algorithm 2 is simply a rank-one tensor. Using the approach presented

for many structured problems.

16

Algorithm 3: Approximating MOT by Algorithm 1 and 2

ε

2m log(n) and ε′ = ε

.

Input: η =
Step 1: Let ˜rk ∈

∆n for

k

∀

∈

8kCk∞
[m] be deﬁned as

(˜r1, ˜r2, . . . , ˜rm) =

1
(cid:18)

−

ε′
4m

(cid:19)

(r1, r2, . . . , rm) +

ε′
4mn

(1n, 1n, . . . , 1n).

Step 2: Compute
Step 3: Round
Output:

X.

e

X = MultiSinkhorn(C, η,

X = Round(

X,

˜rk}k∈[m]).

{

˜rk}k∈[m], ε′/2).

{

b

e

b

by [Lacombe et al., 2018, Proposition 4], we can compute
Finally, the total arithmetic operations required by Algorithm 2 is O(mnm).

C, Y
h

i

eﬃciently using

C, X
h

.
i

Algorithm for the MOT problem. We present the pseudocode of our main algorithm in
Algorithm 3, where Algorithm 1 and 2 are the subroutines. We notice that the regularization
parameter η is scaled as a function of the desired accuracy ε > 0, and remark that Step 1
is necessary since the multimarginal Sinkhorn algorithm is not well behaved if the marginal
distributions do not have dense support.

4.2 Technical lemmas

In this section, we provide two technical lemmas which are important in the analysis of
Algorithm 1. The ﬁrst lemma shows that the dual objective gap at iteration t is bounded by
the product between the residue term Et and a constant depending on η, C and

ri}i∈[m].
{

Lemma 4.1. Let
solution which is speciﬁed by Lemma 2.2. Then, the following inequality holds true:

}t≥0 be the iterates generated by Algorithm 1 and β⋆ be an optimal

βt
{

where Et is deﬁned in Eq. (17) and R > 0 is deﬁned as

ϕ(βt)

−

ϕ(β⋆)

≤

REt,

for all t

1.

≥

R := k

C

k∞
η −

log

(cid:18)

min
1≤i≤m,1≤j≤n

rij

.

(cid:19)

Proof. We ﬁrst prove that the following inequality holds true,

max
1≤i≤m

max
1≤i≤m

max
1≤j≤n

(cid:26)

max
1≤j≤n

(cid:26)

βt
ij −
β⋆
ij −

min
1≤j≤n

βt
ij

min
1≤j≤n

β⋆
ij

≤

≤

(cid:27)

(cid:27)

R,

R.

(18)

Note that the second inequality is a straightforward deduction of Eq. (9) in the proof of
Lemma 2.2. Thus, it suﬃces to prove the ﬁrst inequality.

We establish this by an induction argument. Indeed, this inequality holds trivially when
T . By the update for β in Algorithm 1,

t = 0. Assume that this inequality holds true for t
βT +1
k = βT

k for all k

= K, where K = argmax1≤k≤m ρ(rk, rk(B(βt))). This implies that

≤

max
1≤j≤n

βT +1
kj −

min
1≤j≤n

βT +1
kj ≤

R for all k

= K.

17

6
6
Now it remains to show max1≤j≤n βT +1
from the update formula of βT +1

that

Kj −

Kl

min1≤j≤n βT +1

Kj ≤

R. For any l

∈

[n], we derive

eβT +1

Kl

X1≤ik≤n,∀k6=K

ePk6=K βT
kik

−η−1Ci1...l...im = rKl ≥

min
1≤i≤m,1≤j≤n

rij.

Since C is a nonnegative cost tensor, we derive from the above inequality that

(19)

(20)

βT +1
Kl ≥

log

min
1≤i≤m,1≤j≤n

rij

log

−

(cid:19)

(cid:18)

Since rKl ≤

1 and Ci1...im ≤ k
C

k∞, we have





X1≤ik≤n,∀k6=K

ePk6=K βT

kik

.





βT +1
Kl ≤

C
k

k∞
η −

log





X1≤ik≤n,∀k6=K

ePk6=K βT

kik

.





Combining the bounds (19) and (20) implies the desired result.

Then, we proceed to the proof of Lemma 4.1. Since the function ϕ is convex and β⋆ is an

optimal solution, Eq. (18) implies that

ϕ(βt)

ϕ(β⋆)

(βt

−

≤

β⋆)⊤

∇

−

ϕ(βt) =

m

(βt

k −

Xk=1

k)⊤
β⋆

rk(B(βt))
B(βt)
k

k1 −

(cid:18)

rk

.

(cid:19)

Note that the initialization and the main update for the variable β in Algorithm 1 imply that
B(βt)
k

k1 = 1 for all t

1. Thus, we have

≥

m

(21)

(22)

Furthermore, we have 1⊤

n rk(B(βt) = 1⊤

ϕ(βt)

−

ϕ(β⋆)

≤

(βt

k −

k)⊤(rk(B(βt))
β⋆

rk).

−

Xk=1
n rk = 1 for all k

[m]. This implies

∈

n (rk(B(βt)
1⊤

−

rk) = 0 for all k

[m].

∈

For all k

∈

[m], we deﬁne 2m shift terms as follows,

∆βt

k =

max1≤j≤n βt

kj + min1≤j≤n βt
kj

2

,

∆β⋆

i =

max1≤j≤n β⋆

kj + min1≤j≤n β⋆
kj

2

.

Using these shift terms, we derive that

(βt

k)⊤(rk(B(βt))
β⋆

k −
(22)
= (βt
k −
βt
(
k −
k
max
1≤j≤n

≤
=

−

∆βt
∆βt

rk)
k1n)⊤(rk(B(βt))
k1nk∞ +
βt
min
kj −
1≤j≤n

(cid:18)

∆β⋆
(β⋆
rk)
k −
−
−
rk(B(βt))
∆β⋆
k1nk∞)
k
β⋆
kj −

k1n)⊤(rk(B(βt))
rkk1
−
rk(B(βt))
k
2

min
1≤j≤n

β⋆
k −
k
βt
kj + max
1≤j≤n

β⋆
kj

rk)

−

rkk1

.

−

(cid:19)

Plugging Eq. (18) into the above inequality yields

(βt

k −

k)⊤(rk(B(βt))
β⋆

rk)

¯R

rk(B(βt))
k

rkk1.

−

≤

−

(23)

18

Combining Eq. (23) and Eq. (21) yields

ϕ(βt)

ϕ(β⋆)

¯R

≤

−

m

Xk=1

rk(B(βt))
k

rkk1

−

!

= REt.

As a consequence, we obtain the conclusion of the lemma.

(cid:3)

The second lemma gives a descent inequality for the iterates generated by Algorithm 1

with a lower bound on the progress at each iteration.

βt
Lemma 4.2. Let
{
inequality holds true:

}t≥0 be the iterates generated by Algorithm 1. Then, the following

ϕ(βt)

−

ϕ(βt+1)

2

,

1
2

≥

Et
m

(cid:18)

(cid:19)

for all t

1.

≥

Proof. We ﬁrst show that

ϕ(βt)

−

ϕ(βt+1)

1
m  

≥

m

Xk=1

ρ(rk, rk(B(βt)))

!

.

(24)

(25)

By the deﬁnition of ϕ, we have

ϕ(βt)

B(βt)
ϕ(βt+1) = log(
k

k1)

−

B(βt+1)
log(
k

k1)

−

−

From the update formula for βt+1, it is clear that
Therefore, we have

B(βt)
k

k1 =

m

(βt

k −

Xk=1
B(βt+1)
k

βt+1
k

)⊤rk.

(26)

k1 = 1 for all t

1.

≥

log(rK (B(βt))))⊤rK.

−

ϕ(βt)

ϕ(βt+1) =

(βt

−
n rK = 1⊤
n rK(B(βt)) = 1, we have ϕ(βt)

−

K −

βt+1
K )⊤rK = (log(rK )

Since 1⊤
this equality with the fact that the K-th coordinate is the greedy one yields Eq. (25).

ϕ(βt+1) = ρ(rK, rK (B(βt))). Combining

−

We proceed to prove Eq. (24). Indeed, by the Pinsker inequality, we have

ρ(rk, rk(B(βt)))

1
2 k

rk(B(βt))

≥

2
1 for any k

rkk

−

[m].

∈

Plugging this inequality into Eq. (25) and using the Cauchy-Schwarz inequality yields

ϕ(βt)

−

ϕ(βt+1)

1
2m2

≥

m

Xk=1

m

1
2m  

≥

Xk=1
rk(B(βt))
k

−

rk(B(βt))
k

2
1

rkk

−

!

2

2

.

=

1
2

Et
m

(cid:18)

(cid:19)

rkk1

!

This completes the proof.

(cid:3)

19

 
 
4.3 Main results

We present an upper bound for the number of iterations required by Algorithm 1.

βt
Theorem 4.3. Let
{
required to reach the stopping criterion Et ≤

ε′ satisﬁes

}t≥0 be the iterates generated by Algorithm 1. The number of iterations

2 +

t

≤

2m2R
ε′

,

(27)

where R is deﬁned in Lemma 4.1.

Proof. Let β⋆ be an optimal solution of the dual entropic regularized MOT problem considered
ϕ(β⋆), we derive
in Lemma 4.1. By letting the objective gap at each iteration be δt = ϕ(βt)
from Lemma 4.1 and Lemma 4.2 that

−

δt

≤

REt,

δt+1

δt

−

≥

1
2

Et
m

(cid:18)

2

.

(cid:19)

Putting these pieces together with the fact that Et ≥
not fulﬁlled yields

ε′ as long as the stopping criterion is

δt

δt+1

−

≥

1
2  

max

2

,

ε′
m

δt
mR

2

.

((cid:18)
We now apply the switching strategy to obtain the desired upper bound in Eq. (27). Indeed,
we have

)!

(cid:18)

(cid:19)

(cid:19)

(δt)2
4m4R
Fixing an integer t1 > 0 and considering t > t1, the ﬁrst inequality further implies that

δt
2m2R

δt+1
2m2R

ε′
m

δt+1

2 ≤

2 −

1
2

4 ,

δt

−

≤

(cid:18)

(cid:19)

.

2

2

2m2R

2

2m2R

δt+1 −

δt ≥

1 =

⇒

1 +

t1 ≤

2

,

2m2R
δt1

and the second inequality further implies that

t

t1 ≤

−

1 + 2(δt1

δt)

−

2

m
ε′

=

⇒

t

≤

1 + t1 + 2δt1

(cid:16)
(cid:17)
δ1, we obtain that the total number of iterations satisﬁes

(cid:16)

m
ε′

2

.

(cid:17)

Let s = δt1 ≤

t

≤

min
0≤s≤δ1 (

2 +

2

2m2R
s

+ 2s

m
ε′

2

(cid:17)

(cid:16)

) ≤

2 +

2m2R
ε′

.

This completes the proof.

(cid:3)

Before presenting the main result on the complexity bound of Algorithm 3, we provide

the complexity bound of Algorithm 2 in the following theorem.

Theorem 4.4. Let X
of probability vectors, Algorithm 2 returns a nonnegative tensor Y
rk(Y ) = rk for all k

Rn×...×n be a nonnegative tensor and

[m] and

∈

ri}i∈[m] ⊆
{
∈

∆n be a sequence
Rn×...×n satisfying that

∈

Y
k

−

X

k1 ≤

2

rk(X)
k

rkk1

−

.

!

m

Xk=1

20

 
Proof. By the deﬁnition of zk and the update formula for X (k) for all k
X (m) is nonnegative and

∈

[m], each entry of

errk = rk −
X (m)

rk(X (m))

≥
k1 for all k

0 for all k

[m].

∈

(28)

[m]. Thus, we derive from Eq. (28) and

This implies that
− k
the update formula for Y that each entry of Y is nonnegative.

errkk1 = 1
k
Furthermore, we deﬁne A by Ai1...im :=

∈

m
k=1 errkik for all (i1, . . . , im)

[n]

∈

× · · · ×

[n] and

ﬁnd that

Q

[rk(A)]j = errkj 


Therefore, we conclude that

X1≤il≤n,∀l6=k Yl6=k

errlil


= errkj

errlk1.
k

Yl6=k

(29)

rk(Y ) = rk(X (m)) +

rk(A)
err1k
k
It remains to estimate the ℓ1 bound between Y and X. Indeed, we have

(29)
= rk(X (m)) + errk = rk for all k

m−1
1

[m].

∈

X
k

X (m)

k1 − k

k1 =

X (0)
k

X (m)

k1 − k

k1 =

m

Xk=1

X (k−1)
(
k

X (k)

k1 − k

k1).

(30)

Since
subtensor when rkj(X (k−1))

X (k−1)
k

k1 − k

X (k)

k1 is the amount of mass removed from X (k−1) by rescaling the kth

X (k−1)
k

k1 − k

≥
X (k)

rkj, we have
k1 = 1⊤

n (max

−
A simple calculation using the fact that X (0) = X shows that

0, rk(X (k−1))
{

rk}

) for all k

[m].

∈

X (0)
k

X (1)

k1 − k

k1 =

1
2

r1(X)
(
k

r1k1 +

X
k

k1 −

−

1).

(31)

Moreover, rk(X (0)) is entrywise larger than rk(X (k−1)) for all k
rk(X (k−1))

rk(X (0)) = rk(X). This implies

rk(X (k−2))

. . .

[m]. That is to say,

∈

≤

≤
k1 − k
−
Plugging Eq. (31) and Eq. (32) into Eq. (30) yields

≤
X (k+1)

X (k)
k

rk+1(X)

k1 ≤ k

rk+1k1 for all k

∈

[m

−

1].

(32)

X
k

X (m)

k1 − k

1
2

k1 ≤

r1(X)
(
k

r1k1 +

X
k

k1 −

−

1) +

m

Xk=2

rk(X)
k

rkk1.

−

(33)

By the deﬁnition of Y , we have

A
k1
k1 + k
m−1
err1k
1
k
Since X is entrywise larger than X (m) and

k1 ≤ k

X (m)

X
k

X

−

−

Y

X
k

−

Y

X

k1 ≤ k

X (m)

k1 − k

k1 + 1

− k

Plugging Eq. (33) into Eq. (34) yields the desired result.

X (m)

err1k1 = 1
− k
k
X (m)
X
k1 = 2(
k

k1 − k

k1, we have
X (m)
k1) + 1

(29)
=

X (m)

X
k

−

k1 +

err1k1.
k

X

− k

k1.

(34)

(cid:3)

We are ready to present the complexity bound of Algorithm 3 for solving the MOT problem

in Eq. (2). Note that ε′ = ε/(8
C
k

k∞) is deﬁned using the desired accuracy ε > 0.

21

Theorem 4.5. Algorithm 3 returns an ε-approximate multimarginal transportation plan
Rn×...×n within

O

(cid:18)

arithmetic operations.

Proof. We ﬁrst claim that

m3nm

2
∞ log(n)
C
k
k
ε2

(cid:19)

∈

X

b

m

Xk=1

b

X

C,
h

C, X ⋆

i − h

i ≤

mη log(n) + 4

X)

rk(
k

−

rkk1

C

! k

k∞.

(35)

b

e

X is deﬁned in Step 2 of Algorithm 3 and

where
is an optimal multimarginal transportation plan. By the deﬁnition of

X is returned by Algorithm 3 and X ⋆
˜rk}k∈[m] and using
{

m
k=1 k

e
rk(

X)

˜rkk1 ≤

−

ε′/2, we have

P

m
e

Xk=1

m

Xk=1

X)

rk(
k

rkk1 ≤

−

rk(
(
k

X)

˜rkk1 +

˜rk −
k

rkk1)

−

≤

ε′
2

+

ε′
2m

m

Xk=1

= ε′.

e

e

Plugging the above inequality into Eq. (35) and using η = ε/(2m log(n)) and ε′ = ε/(8
C
k
we obtain that
It remains to bound the number of iterations required by Algorithm 1 to reach Et ≤

(cf. Step 2 of Algorithm 3). Using Theorem 4.3, we have

C, X ⋆

C,
h

i − h

i ≤

X

ε.

b

k∞),
ε′/2

2 +

t

≤

4m2R
ε′

.

By the deﬁnition of R (cf. Lemma 4.1), η = ε/(2m log(n)) and ε′ = ε/(8
C
k

k∞), we have

32m2
C
k
ε
32m2
C
k
ε
C
k

m3

t

2 +

2 +

≤

≤

= O

(cid:18)

(cid:18)

2
∞ log(n)
k
ε2

.

(cid:19)

k∞

k∞

(cid:18)

C
k

log

k∞
η −
C
2m log(n)
k

(cid:18)
k∞

ε

min
1≤i≤m,1≤j≤n

˜rij

(cid:19)(cid:19)
ε
32mn

C
k

log

−

(cid:18)

k∞ (cid:19)(cid:19)

Since each iteration of Algorithm 1 requires O(nm) arithmetic operations, the total arith-
∞ log(n)ε−2). In addition,
metic operations required by Step 2 of Algorithm 3 is O(m3nm
2
k
˜rk}k∈[m] requires O(mn) arithmetic operations and Algorithm 2
computing a set of vectors
{
requires O(mnm) arithmetic operations. Putting these pieces together yields that the com-
∞ log(n)ε−2).
2
plexity bound of Algorithm 3 is O(m3nm
k

C
k

C
k

Proof of Eq. (35): Using Theorem 4.4, we obtain that
problem in Eq. (2) and

X is a feasible solution to the MOT

X
k

−

X

k1 ≤

2

This implies that

b

e

m

Xk=1

X)

rk(
k

−

b
rkk1

.

!

e

m

X

C,
h

C,

i − h

b

C
2
k

k∞

i ≤

X

e

22

Xk=1

X)

rk(
k

rkk1

−

.

!

e

(36)

 
 
 
Letting X ⋆ be an optimal solution of the MOT problem and
Algorithm 2 with an input X ⋆ and

X)

rk(
{

}k∈[m], Theorem 4.4 implies

Y be the output returned by

e

m

= 2

!

Xk=1

m

Xk=1

Y
k

−

X ⋆

2

k1 ≤

e
rk(X ⋆)
k

−

rk(

X)

k1

X)

rk(
k

−

rkk1

.

!

(37)

e

Since
exists

Rmn such that

X is returned by Algorithm 1, we have
β
e
e

X = B(

β) and

∈

e

e
min
β1,...,βm∈Rn

e
B(β1, . . . , βm)
log(
k

k1)

−

β⊤
i ri(

X).

e

m

Xi=1

This implies that

e
X is an optimal solution of the following problem:

e
X
k

k1 = 1. By the optimality condition, there
β is an optimal solution of the following problem:

e

min
e

C, X
h

ηH(X),

s.t. rk(X) = rk(

X) for all k

[m].

i −
Y is feasible for the above problem, we have

Since
Y ).
Using the property of entropy regularization function [Cover and Thomas, 2012], we have
0

m log(n). Putting these pieces yields

e
X), H(
H(

C,
h

ηH(

ηH(

X
e

≤ h

i −

i −

Y )

C,

Y

e

e

e

e

∈
X)

≤

≤

e

e

X

C,
h

C,

i − h

Y

i ≤

mη log(n).

Combining Eq. (37) and Eq. (38) together with the H¨older inequality yields

e

e

X

C,
h

C, X ⋆

i − h

C
mη log(n) + 2
k

k∞

i ≤

e
Combining Eq. (36) and Eq. (39) yields

X

C,
h

C, X ⋆

i − h

C
mη log(n) + 4
k

k∞

i ≤

This completes the proof of Eq. (35).

b

m

Xk=1

m

Xk=1

X)

rk(
k

−

rkk1

!

e

rk −
k

rk(

X)

k1

!

(38)

(39)

.

.

e

(cid:3)

Remark 4.6. Theorem 4.5 demonstrates that the complexity bound of Algorithm 3 is near-
linear in nm, which is the number of unknown variable of the MOT problem in Eq. (2). This is
the best possible dependence on n that we can hope for an optimization algorithm when applied
to solve the general MOT problem. Further, the complexity bound has the dependence m3 which
seems unimprovable using the current techniques; indeed, the iteration number of Algorithm 1
is proportional to m2 and the regularization parameter η is necessarily proportional to 1/m
such that the output returned by Algorithm 1 can be rounded to an ε-approximate multimarginal
transport plan.

Remark 4.7. Even though Benamou et al. [2015] has shown that the Sinkhorn-type algorithm
can be more eﬃcient in practice than LP solvers, the full theoretical analysis is not given. In
contrast, our theoretical analysis provides the provably eﬃcient way to solve the MOT problem,
demonstrating the importance of the greedy update rule in the Sinkhorn-type algorithm. This
leads to an algorithmic framework in which each iteration might take O(nm) number of arith-
metic operations in the worst case. However, we can develop some eﬃcient subroutines by
exploiting the special structure of many MOT problems in practice and show that the required
number of arithmetic operations is only polynomial in m and n [Altschuler and Boix-Adsera,
2021b,a, Altschuler and Boix-Adser`a, 2022]. Some of their results are based on both the al-
gorithmic scheme and the theoretical analysis of multimarginal Sinkhorn, demonstrating the
fundamental role that our analysis play in understanding the MOT problem.

23

 
 
 
 
Algorithm 4: Accelerated MultiSinkhorn(C, η,
Input: t = 0, θ0 = 1, K = 1 and ˇβ0 = ˜β0 = 0n.
while Et > ε′ do

Step 1. Compute ¯βt = (1
Step 2. Compute ˜βt+1

−
Rmn by

θt) ˇβt + θt ˜βt.

∈

˜rk}k∈[m], ε′)
{

k = ˜βt
˜βt+1

k −

1
mθt (cid:18)

rk(B( ¯βt))
B( ¯βt)

k

k1 −

rk

for all k

(cid:19)

[m].

∈

Step 3. Compute `βt = ¯βt + θt( ˜βt+1
Step 4. Compute

Rmn by

βt

∈

˜βt).

−

b

βt
k =

`βt
k + log(rk)
`βt
k,

−

(cid:26)

log(rk(B( `βt))), k = K,

otherwise.

Step 5. Compute βt = argmin
|
{
Step 6. Choose the greedy coordinate K = argmax1≤k≤m ρ(rk, rk(B(βt))).
Step 7. Compute ˇβt+1

Rmn by

ϕ(β)

∈ {

βt

}}

β

b

.

ˇβt,

b

∈

ˇβt+1
k =

(cid:26)

βt
k + log(rk)
βt
k,

−

log(rk(B(βt))), k = K,

otherwise.

θ2
Step 8. Compute θt+1 = θt(
t + 4
Step 9. Increment by t = t + 1.
p

end while
Output: B(βt).

θt)/2.

−

5 Accelerating Multimarginal Sinkhorn Algorithm

In this section, we present an accelerated multimarginal Sinkhorn algorithm for solving the
entropic regularized MOT problem in Eq. (3). Together with a rounding scheme, our algorithm
can be used for solving the MOT problem in Eq. (2) and achieves a complexity bound of
O(m3nm+1/3ε−4/3), which improves that of the multimarginal Sinkhorn algorithm in terms
of 1/ε and accelerated alternating minimization algorithm [Tupitsa et al., 2020] in terms of
e
n. The proof idea comes from a novel combination of Nesterov’s estimated sequence and the
techniques for analyzing the multimarginal Sinkhorn algorithm.

5.1 Algorithmic procedure

We present the pseudocode of accelerated multimarginal Sinkhorn algorithm in Algorithm 4.
This algorithm achieves the acceleration by using Nesterov’s estimate sequences [Nesterov,
2018]. While our algorithm can be interpreted as an accelerated block coordinate descent algo-
rithm, it is worthy noting that our algorithm is purely deterministic and thus diﬀers from other
accelerated randomized algorithms [Nesterov, 2012, Lin et al., 2015, Fercoq and Richt´arik,
2015, Allen-Zhu et al., 2016, Lu et al., 2018, Diakonikolas and Orecchia, 2018] in the machine
learning and optimization literature.

Comments on algorithmic scheme. Algorithm 4 is a novel combination of Nesterov’s
estimate sequences, a monotone search step, the choice of greedy coordinate and two coor-

24

Algorithm 5: Approximating MOT by Algorithms 2 and 4

Input: η =
Step 1: Let ˜rk ∈

ε

2m log(n) and ε′ = ε

8kCk∞ .
[m] be deﬁned as

∆n for

k

∀

∈

(˜r1, ˜r2, . . . , ˜rm) =

1
(cid:18)

−

ε′
4m

(cid:19)

(r1, r2, . . . , rm) +

ε′
4mn

(1n, 1n, . . . , 1n).

X = Accelerated MultSinkhorn(C, η,
˜rk}k∈[m]).

X = Round(

X,

{

˜rk}k∈[m], ε′/2).

{

.
Step 2: Compute
Step 3: Round
Output:

X.

e

b

e

b

dinate updates. Nesterov’s estimate sequences (Step 1-3) are crucial for optimizing a dual
objective function ϕ faster than Algorithm 1. The coordinate update (Step 4) guarantees
k1 = 1. The monotone search step (Step 5) guarantees that
that ϕ(
≤
βt). The greedy coordinate update (Step 6-7) guarantees that ϕ( ˇβt+1)
ϕ(βt)
ϕ(βt)
≤
b
with suﬃciently large progress. Similar to Algorithm 1, the greedy rule is based on the
function ρ : Rn

ϕ( `βt) and

R+ given by:

βt)
ϕ(

B(
k

βt)

Rn

≤

b

b

+ ×

+ →

ρ(a, b) = 1⊤

n (b

−

n

a) +

ai log

Xi=1

.

ai
bi (cid:19)

(cid:18)

Furthermore, we also use the same quantity as that in the multimarginal Sinkhorn algorithm
to measure the per-iteration residue of Algorithm 4:

Et =

m

Xk=1

rk(B(βt)
k

rkk1.

−

(40)

Comments on arithmetic operations per iteration. The most expensive step is to
compute rk(B( ¯βt))/
[m]. Since B( ¯βt) does not have any special property,
B( ¯βt)
k
it seems diﬃcult to design some implementation trick to reduce the dependency on m. Thus,
the arithmetic operations per iteration is still O(mnm). Note that, the accelerated alternating
minimization algorithm in [Tupitsa et al., 2020] also requires O(mnm) arithmetic operations
per iteration.

k1 for all k

∈

Algorithm for the MOT problem. We present the pseudocode of our main algorithm
in Algorithm 5, where Algorithms 4 and 2 are the subroutines. The regularization parameter
η is set as before, and Step 1 is also necessary since the accelerated multimarginal Sinkhorn
algorithm is not well behaved if the marginal distributions do not have dense support.

5.2 Technical lemmas

We ﬁrst present two technical lemmas which are essential in the analysis of Algorithm 4. The
ﬁrst lemma provides an inductive relationship on the quantity

where β⋆ is an optimal solution of the dual entropic regularized MOT problem in Eq. (6). In
order to facilitate the discussion, we recall Eq. (14) with

δt = ϕ( ˇβt)

−

ϕ(β⋆),

(41)

k1→2 = √m as follows,
A
k
m
2,
2
k

β′
k

−

β

(42)

ϕ(β′)

ϕ(β)

(β′

β)⊤

∇

−

−

ϕ(β)

−

≤

(cid:16)

25

(cid:17)

which will be used in the proof of the ﬁrst lemma.

}t≥0 be the iterates generated by Algorithm 4 and β⋆ be an optimal
Lemma 5.1. Let
solution of the dual entropic regularized MOT problem. Then the quantity δt deﬁned by Eq. (41)
satisﬁes the following inequality,

ˇβt
{

δt+1 ≤

(1

−

θt)δt +

mθ2
t
2

β⋆
k
(cid:16)

˜βt

2
k

−

β⋆

− k

˜βt+1

2
k

−

.

(cid:17)

Proof. Using Eq. (42) with β′ = `βt and β = ¯βt, we have

ϕ( `βt)

≤

ϕ( ¯βt) + θt( ˜βt+1

˜βt)⊤

∇

−

ϕ( ¯βt) +

mθ2
t
2

(cid:18)

(cid:19)

˜βt+1
k

˜βt

2.
k

−

By simple calculations, we ﬁnd that

( ˜βt+1

˜βt)⊤

−

∇

ϕ( ¯βt) = (1
ϕ( ¯βt) =

−
( ˜βt

−

θt)ϕ( ¯βt) + θtϕ( ¯βt),

¯βt)⊤

∇

−

ϕ( ¯βt) + ( ˜βt+1

¯βt)⊤

∇

−

ϕ( ¯βt).

Putting these pieces together yields that

ϕ( `βt)

≤

θt 

ϕ( ¯βt) + ( ˜βt+1

¯βt)⊤

∇

−

ϕ( ¯βt) +




+ (1
|
−

θt)ϕ( ¯βt)

−

θt( ˜βt
II

−

I
¯βt)⊤
{z
∇

ϕ( ¯βt)

.

}

mθt
2

(cid:18)

(cid:19)

˜βt+1
k

˜βt

2
k

−

(43)







We ﬁrst estimate the term II. Indeed, it follows from the deﬁnition of ¯βt that

{z

}

|

θt( ˜βt

−

−

¯βt) = θt ¯βt + (1

θt) ˇβt

−

−

¯βt = (1

θt)( ˇβt

¯βt).

−

−

Using this equation and the convexity of ϕ, we have

II = (1

−

θt)(ϕ( ¯βt) + ( ˇβt

¯βt)⊤

∇

−

ϕ( ¯βt))

(1

−

≤

θt)ϕ( ˇβt).

(44)

Then we proceed to estimate the term I. Indeed, by the update formula for ˜βt+1 and the
deﬁnition of ϕ, we have

(β

−

˜βt+1)⊤(

∇

ϕ( ¯βt) + mθt( ˜βt+1

−

˜βt)) = 0 for all β

Rmn.

∈

Letting β = β⋆ and rearranging the resulting equation yields that

( ˜βt+1

¯βt)⊤

∇

−

ϕ( ¯βt) = (β⋆

¯βt)⊤

∇

−

ϕ( ¯βt) +

mθt
2

˜βt

2
k

−

β⋆

− k

−

˜βt+1

2
k

˜βt+1

˜βt

2
k

−

− k

.

Using the convexity of ϕ again, we have (β⋆
pieces together yields that

−

ϕ( ¯βt)

∇

≤

ϕ(β⋆)

−

(cid:17)
ϕ( ¯βt). Putting these

β⋆
k
(cid:16)
¯βt)⊤

ϕ(β⋆) +

I

≤

mθt
2

(cid:16)

β⋆
k

˜βt

2
k

−

β⋆

− k

˜βt+1

2
k

−

.

(cid:17)

26

(45)

Plugging Eq. (44) and Eq. (45) into Eq. (43) yields that

ϕ( `βt)

(1

−

≤

θt)ϕ( ˇβt) + θtϕ(β⋆) +

mθ2
t
2

β⋆
k
(cid:16)

˜βt

2
k

−

β⋆

− k

˜βt+1

2
k

−

.

(cid:17)

Since ˇβt+1 is obtained by an coordinate update from βt, we have ϕ(βt)
deﬁnition of βt, we have ϕ(
`βt, we have ϕ( `βt)

ϕ( ˇβt+1). By the
βt is obtained by an coordinate update from

ϕ(βt). Since

βt). Putting these pieces together with yields that
b

βt)

ϕ(

≥

≥

≥

b

ϕ( ˇβt+1)

ϕ(β⋆)

−

b
≤

(1

−

θt)(ϕ( ˇβt)

−

ϕ(β⋆)) +

This completes the proof.

mθ2
t
2

β⋆
k

˜βt

2
k

−

β⋆

− k

˜βt+1

2
k

−

(cid:16)

.

(cid:17)

(cid:3)

The second lemma provides an upper bound for δt deﬁned by Eq. (41) where

}t≥0 are
generated by Algorithm 4 and β⋆ is an optimal solution deﬁned by Corollary 2.3. Note that
our lemma is a direct corollary of the analysis provided in Tseng [2008] and we provide the
proof details for the sake of completeness.

ˇβt
{

ˇβt
{

}t≥0 be the iterates generated by Algorithm 4 and β⋆ be an optimal
Lemma 5.2. Let
√mnR where
solution of the dual entropic regularized MOT problem satisfying that
R is deﬁned in Corollary 2.3. Then the quantity δt deﬁned by Eq. (41) satisﬁes the following
inequality,

β⋆
k

k ≤

2m2nR2
(t + 1)2 .

δt ≤

Proof. By simple calculations, we derive from the deﬁnition of θt that (θt+1/θt)2 = 1
Therefore, we conclude from Lemma 5.1 that

−

θt+1.

1

θt+1

−
θ2
t+1 (cid:19)

(cid:18)

δt+1 −

1

θt

−
θ2
t (cid:19)

(cid:18)

δt ≤

m
2

β⋆
k
(cid:16)

˜βt

2
k

−

β⋆

− k

˜βt+1

2
k

−

.

(cid:17)

Equivalently, we have

1

θt

−
θ2
t (cid:19)

δt +

m
2

β⋆
k

˜βt

2
k

−

≤

(cid:18)

(cid:18)
Recall that θ0 = 1 and ˜β0 = 0mn, we have δt ≤
remaining step is to show that 0 < θt ≤
have θ0 = 1. Assume that the claim holds for t

(cid:17)

(cid:16)

m
2

(cid:17)
2
k

1

θ0

−
θ2
0 (cid:19)

δ0 +

(mθ2

(cid:16)
β⋆
t−1/2)
k
t0, i.e., θt0 ≤

≤

t−1. The
2/(t + 2). Indeed, the claim holds when t = 0 as we

(1/2)m2nR2θ2

≤

2/(t0 + 2), we have

β⋆
k

˜β0

2.
k

−

θt0+1 =

2

1 +

1 + 4/θ2
t0

q

2
t0 + 3

.

≤

Putting these pieces together yields the desired inequality for δt.

(cid:3)

5.3 Main results

We present an upper bound for the number of iterations required by Algorithm 4.

27

βt
Theorem 5.3. Let
{
required to reach the stopping criterion Et ≤

ε′ satisﬁes

}t≥0 be the iterates generated by Algorithm 4. The number of iterations

1 + 4

t

≤

(cid:18)

2/3

,

√nmR
ε′

(cid:19)

where R > 0 is deﬁned in Lemma 2.2.

Proof. We ﬁrst claim that

ϕ(βt)

−

ϕ( ˇβt+1)

1
2m  

≥

m

Xk=1

rk(B(βt))
k

2
1

rkk

−

.

!

(46)

By the deﬁnition of ϕ, we have

m

ϕ(βt)

ϕ( ˇβt+1) = log(
B(βt)
k

k1)

−

B( ˇβt+1)
log(
k

k1)

−

−

(βt

k −

ˇβt+1
k

)⊤rk.

(47)

Xk=1
k1 = 1 and

From the update formula for
for all t
for all t

B( ˇβt+1)
βt and ˇβt+1, it is clear that
k
B(βt)
0. Then we derive from the monotone search step (cf. Step 5) that
k
b
1. Therefore, we have

B(
k

βt)

b

k1 = 1
k1 = 1

≥
≥

ϕ(βt)

ϕ( ˇβt+1) =

(βt

ˇβt+1
K )⊤rK = (log(rK )

K −

−
n rK(B(βt)) = 1, we have ϕ(βt)
n rK = 1⊤

Since 1⊤
≥
Combining this inequality with the fact that the K-th coordinate is the greedy one yields

ϕ( ˇβt+1) = ρ(rK , rK(B(βt))) for all t

−

−

1.

log(rK (B(βt))))⊤rK.

−

ϕ(βt)

−

ϕ( ˇβt+1)

1
m  

≥

m

Xk=1

ρ(rk, rk(B(βt)))

!

.

Using the Pinsker inequality [Cover and Thomas, 2012], we derive Eq. (46) as desired.

By the deﬁnition of βt, we have ϕ( ˇβt)

ϕ(βt). Plugging this inequality into Eq. (46)

together with the Cauchy-Schwarz inequality yields

≥

ϕ( ˇβt)

−

ϕ( ˇβt+1)

1
2m  

≥

m

Xk=1

rk(B(βt))
k

−

2
1

rkk

! ≥

2

.

1
2

Et
m

(cid:18)

(cid:19)

Therefore, we conclude that

ϕ( ˇβj )

−

ϕ( ˇβt+1)

Since ϕ( ˇβt+1)

ϕ(β⋆) for all t

≥

≥

E2

i 

for any j

1, 2, . . . , t

∈ {

.
}

t

1

≥

2m2 

Xi=j



1, we have ϕ( ˇβj)

ϕ( ˇβt+1)

−
4m4nR2
(j + 1)2 .

t

Xi=j

E2

i ≤

δj. Then Lemma 5.2 implies

≤

Putting these pieces together with the fact that Et ≥
not fulﬁlled yields

4m4nR2

(j + 1)2(t

j + 1) ≥

−

28

ε′ as soon as the stopping criterion is

(ε′)2.

Since this inequality holds true for all j
that t is even and let j = t/2. Then, we obtain that

1, 2, . . . , t

∈ {

, we assume without loss of generality
}

This completes the proof.

1 + 4

t

≤

(cid:18)

√nm2R
ε′

2/3

.

(cid:19)

(cid:3)

We are ready to present the complexity bound of Algorithm 5 for solving the MOT problem

in Eq. (2). Note that ε′ = ε/(8
C
k

k∞) is deﬁned using the desired accuracy ε > 0.

Theorem 5.4. Algorithm 5 returns an ε-approximate multimarginal transportation plan
Rn×...×n within

∈

X

b

m3nm+1/3

O

arithmetic operations.

4/3
∞ (log(n))1/3
C
k
k
ε4/3

!

ε where

Proof. Applying the same argument which is used in Theorem 4.5, we obtain that
C, X ⋆
h
Et ≤

ε′/2 (cf. Step 2 in Algorithm 5). Using Theorem 5.3, we have

It remains to bound the number of iterations required by Algorithm 4 to reach the criterion

X is returned by Algorithm 5.

X

C,
h

i −

i ≤

b

b

1 + 4

t

≤

(cid:18)

2/3

.

√nmR
ε′

(cid:19)

By the deﬁnition of R (cf. Lemma 2.2), η = ε/(2m log(n)) and ε′ = ε/(8
C
k

k∞), we have

1 + 4

(cid:18)

1 + 4

(cid:20)

1 + 4

2/3

√nm2R
ε′
8√nm2
ε
8√nm2
ε

(cid:19)
C
k

k∞

C
k

k∞

t

≤

≤

≤

= O

(cid:20)
m2n1/3

C
k

(cid:18)
4/3
∞ (log(n))1/3
k
ε4/3

.

!

C
k∞
k
η

(cid:18)

+ (m

−

1) log(n)

2 log

−

(cid:18)

min
1≤i≤m,1≤j≤n

2m log(n)
ε

C

k

k∞

+ (m

1) log(n)

2 log

−

−

(cid:18)

(cid:19)(cid:19)(cid:21)
ε
32mn

C
k

2/3

˜rij

k∞ (cid:19)(cid:19)(cid:21)

2/3

Since each iteration of Algorithm 4 requires O(mnm) arithmetic operations, the total arith-
4/3
metic operations required by Step 2 of Algorithm 5 is O(m3nm+1/3
∞ (log(n))1/3ε−4/3).
k
˜rk}k∈[m] requires O(mn) arithmetic operations and
In addition, computing a set of vectors
{
Algorithm 2 requires O(mnm) arithmetic operations. Putting these pieces together yields that
4/3
(cid:3)
∞ (log(n))1/3ε−4/3).
the complexity bound of Algorithm 5 is O(m3nm+1/3
k

C
k

C
k

Remark 5.5. Theorem 5.4 demonstrates that the complexity bound of Algorithm 5 is better
than that of Algorithm 3 in terms of 1/ε but not near-linear in nm. To be more speciﬁc,
(0, 1/ε2). This occurs if the desired solution accuracy
Algorithm 5 is recommended when n
is relatively small, saying 10−4, and the examples include the application problems from eco-
nomics, physics and generalized Euler ﬂows. In contrast, Algorithm 3 is recommended when
). This occurs if the desired solution accuracy is relatively large, saying 10−2,
n
and the examples include the application problems from image processing.

(1/ε2, +

∞

∈

∈

29

 
 
2.5

2

1.5

1

0.5

e
p
o

l

t
y
o
p

t
r
o
p
s
n
a
r
t

o

t

e
c
n
a

t
s
D

i

0

0

3

2.5

2

1.5

1

e
p
o

l

t
y
o
p

t
r
o
p
s
n
a
r
t

o

t

e
c
n
a
t
s
D

i

0.5

0

Performance on Random Images

ACCELERATION
SINKHORN

2

4

6
Iteration count

8

10

Performance on Random Images

ACCELERATION
SINKHORN

1.2

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

0.6

0.4

0.2

0

-0.2

-0.4

)
r
o
r
r
e

r
o

f

o

i
t

a
r

t

e
p
m
o
c
(
n

l

)
r
o
r
r
e

r
o
f

o
i
t
a
r

t
e
p
m
o
c
(
n

l

Spead of ln(compet ratio for error)

Max
Median
Min

SINKHORN vs ACCELERATION for MOT

True optimum
SINKHORN, eta=1
SINKHORN, eta=0.2
SINKHORN, eta=0.1
ACCELERATION, eta=1
ACCELERATION, eta=0.2
ACCELERATION, eta=0.1

0.55

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

T
O
M

f
o

l

e
u
a
V

0

2

4

6
Iteration count

8

10

0.1

0

2

4

6
Iteration Counts

8

10

Spead of ln(compet ratio for error)

Max
Median
Min

SINKHORN vs ACCELERATION for MOT

True optimum
SINKHORN, eta=1
SINKHORN, eta=0.2
SINKHORN, eta=0.1
ACCELERATION, eta=1
ACCELERATION, eta=0.2
ACCELERATION, eta=0.1

1.1

1

0.9

0.8

0.7

0.6

0.5

0.4

T
O
M

f
o

l

e
u
a
V

2

4

6
Iteration count

8

10

-0.6

0

2

4

6
Iteration count

8

10

0.3

0

2

4

6
Iteration Counts

8

10

Figure 1: Performance of multimarginal Sinkhorn v.s. accelerated multimarginal Sinkhorn on
the randomly generated synthetic images. Number of pixel in each synthetic image is set as
n = 25 (top) and n = 100 (bottom).

Remark 5.6. The complexity bound has the same dependence m3 as that of Algorithm 3.
However, the improvement seems possible and can be achieved if we implement Step 2 of
Algorithm 4 in distributed parallel manner and choose the greedy coordinate in Step 6 us-
ing the implementation trick we have mentioned before. Each iteration of Algorithm 4 re-
quires O(nm) arithmetic operations and thus Algorithm 5 achieves the complexity bound of
4/3
O(m2nm+1/3
∞ (log(n))1/3ε−4/3). Further, it seems possible to improve the dependence
k
of n by extending other algorithmic frameworks to the MOT setting [Blanchet et al., 2018,
Lahn et al., 2019, Jambulapati et al., 2019]. However, such extension is challenging since we
are not clear whether these frameworks heavily depend on the minimum-cost ﬂow structure of
the OT problem or not. As such, we leave this topic to the future work.

C
k

6 Experiments

In this section, we evaluate our new algorithms on both synthetic data and real images. In
particular, we compute the free-support Wasserstein barycenter based on the OT distance
with the quadratic Euclidean distance ground cost function and compare our algorithms with
the commercial linear programming (LP) solver Gurobi. All the experiments are conducted
in MATLAB R2020a on a workstation with an Intel Core i5-9400F (6 cores and 6 threads)
and 32GB memory, equipped with Ubuntu 18.04.

6.1 Experiments on synthetic data

We follow the setup in Altschuler et al. [2017] in order to compare diﬀerent algorithms on the
synthetic images. More speciﬁcally, we generate a triple of random grayscale images, each
normalized to have unit total mass. The marginals r1, r2 and r3 represent three images, and

30

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
GUROBI vs SINKHORN vs ACCELERATION for MOT

GUROBI
SINKHORN
ACCELERATION

8

7

6

5

4

3

2

1

0

e
m
T

i

l

a
n
o
i
t
a
t
u
p
m
o
C

0

50

100

150

Number of Support Points

Figure 2: Computational eﬃciency of Gurobi v.s. our algorithms as n varies.

the cost tensor C is generated by

3

Ci1,i2,i3 =

1
2  

λkk

xik −

2
Ai1,i2,i3(x)
k

!

for all (i1, i2, i3)

[n]

[n]

×

×

[n],

∈

Xk=1
3
k=1 λkxik is the Euclidean barycenter and x =

Rd are
∆3 is a weight vector and set as

xi}i∈[n] ⊆
{

where Ai1,i2,i3(x) =
pixel locations in the images. Moreover, λ = (λ1, λ2, λ3)
P
(1/3, 1/3, 1/3) consistently in this subsection.

∈

Each of the images has n pixel locations in total and is generated based on randomly posi-
tioning a foreground square in otherwise black background. We utilize a uniform distribution
on [0, 1] for the intensities of the background pixels and a uniform distribution on [0, 50] for
the foreground pixels. We set the proportion of the size of the square is as 10% of the image
and implement all the algorithms on the synthetic images with diﬀerent size n.

We generalize two metrics proposed by Altschuler et al. [2017] and use them to quantita-
tively measure the performance of diﬀerent algorithms. The ﬁrst metric is the distance be-
tween the output of the algorithm, X, and the transportation polytope between the marginals
r1, r2 and r3. Formally, we have

d(X) =

r1(X)
k

r1k1 +

r2(X)
k

r2k

−

+

r3(X)
k

r3k1,

−

−

where r1(X), r2(X) and r3(X) are the marginal vectors of the output X while r1, r2 and r3
stand for the true marginal vectors. The second metric is the competitive ratio, deﬁned by
log(d(X1)/d(X2)) where d(X1) and d(X2) refer to the distance between the outputs of two
algorithms and the transportation polytope.

We perform a pairwise comparative experiment: multimarginal Sinkhorn versus acceler-
ated multimarginal Sinkhorn, by running both algorithms with ten randomly selected pairs
In order to have further evaluations
of synthetic images with varying size n
with these algorithms, we also compare their performance with diﬀerent choices of regulariza-
tion parameter η
while using the value of the MOT problem (without entropic
regularization term) as the baseline. The maximum number of iterations is set as 10.

1, 0.2, 0.1
}

.
25, 100
}

∈ {

∈ {

31

 
5

4

3

2

1

e
p
o

l

t
y
o
p

t
r
o
p
s
n
a
r
t

o

t

e
c
n
a

t
s
D

i

0

0

Performance on MNIST Images

ACCELERATION
SINKHORN

2

4

6
Iteration count

8

10

0.5

0.4

0.3

0.2

0.1

0

-0.1

-0.2

)
r
o
r
r
e

r
o

f

o

i
t

a
r

t

e
p
m
o
c
(
n

l

Spead of ln(compet ratio for error)

Max
Median
Min

0

2

4

6
Iteration count

8

10

SINKHORN vs ACCELERATION for MOT

SINKHORN, eta=1
SINKHORN, eta=0.05
SINKHORN, eta=0.02
ACCELERATION, eta=1
ACCELERATION, eta=0.05
ACCELERATION, eta=0.02

T
O
M

f
o

l

e
u
a
V

1.2

1.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0

2

4

6
Iteration Counts

8

10

Figure 3: Performance of multimarginal Sinkhorn v.s. accelerated multimarginal Sinkhorn on
MNIST images. Number of pixel in each MNIST image is set as n = 576.

×

Experimental results. Figure 1 summarizes the results on synthetic images. The images
in the ﬁrst row show the comparative performance of both algorithms in terms of the itera-
5 synthetic images. In the leftmost one, the comparison uses
tion counts on 10 triples of 5
distance to transportation polytope d(X) where X are returned by the algorithms. In the
middle one, the maximum/median/minimum values of the competitive ratios are utilized for
the comparison. In the rightmost one, we vary the regularization parameter η
1, 0.2, 0.1
}
for both algorithms together with the value of the unregularized MOT problem as the base-
line. It is clear that accelerated multimarginal Sinkhorn algorithm outperforms multimarginal
Sinkhorn algorithm in terms of iteration numbers, illustrating the improvement achieved by
using the estimated sequence and monotone search.

∈ {

∈ {

To further compare our algorithms with Gurobi in terms of computational eﬃciency,
we conduct one more experiment with varying number of support points (or pixel locations)
. Figure 2 shows the running time taken by three algorithms across a wide
n
25, 100, 144
}
range of n. As n increases, we ﬁnd that multimarginal Sinkhorn algorithm performs the
best, followed by accelerated multimarginal Sinkhorn algorithm, both outperforming Gurobi.
This demonstrates that classical LP algorithms might not be suitable for solving the MOT
problem, partially conﬁrming our results in Section 3. Moreover, despite fewer iterations, the
direct implementation of accelerated multimarginal Sinkhorn algorithm is indeed slower than
multimarginal Sinkhorn algorithm. This is mainly due to the heavy computation of gradient
and we believe some parallel computing toolbox can be helpful. However, this is beyond the
scope of this paper and we leave it to future research.

6.2 Experiments on real images

We conduct the experiment with the same setup and MNIST dataset2. The MNIST dataset
consists of 60,000 images of handwritten digits of size 28 by 28 pixels. We add a very small
noise term (10−6) to all the zero elements in the measures and then normalize them such
that their sum becomes one. We also vary the regularization parameter η
1, 0.05, 0.02
}
for both algorithms but cannot run Gurobi.
Indeed, the LP constructed from the MOT
problem using 3 MNIST images is so lagre that Gurobi is out of memory. Figure 3 presents
the comparative performance of our algorithms on the MNIST images, and we ﬁnd that it is
consistent with the performance on the randomly generated synthetic images.

∈ {

In order to better visualize the quality of approximate barycenters obtained by each al-
gorithm, we run our algorithms with η = 0.05 to compute the free-support Wasserstein

2Available in http://yann.lecun.com/exdb/mnist/

32

 
 
 
 
 
 
 
 
Figure 4: Approximate barycenters obtained by running the multimarginal Sinkhorn (top)
and accelerated multimarginal Sinkhorn (bottom) algorithms.

barycenter of two triple of real images with diﬀerent weight vectors.
MOT problem as before and form the barycenter as follows,

Indeed, we solve the

3

µλ =

Xk=1 X1≤ik≤n

γi1,i2,i3δAi1,i2,i3 (x),

where Ai1,i2,i3(x) =
locations in the images and γ
solves the MOT problem.

P

∈

3
k=1 λkxik is the Euclidean barycenter, and x =

Rd are pixel
Rn×n×n is an optimal multimarginal transportation plan that

xi}i∈[n] ⊆
{

Figure 4 presents the approximate barycenters obtained by running our algorithms. These
results demonstrate that our algorithms can successfully capture the free-support barycenters
of high quality by solving the MOT problem and are at least competitive with the existing
algorithms [Benamou et al., 2015, 2019, Peyr´e and Cuturi, 2019] in practice.

33

7 Conclusion

≥

In this paper, we have studied the multimarginal optimal transport (MOT) problem, provid-
ing new algorithms and complexity bounds for approximating this problem. We demonstrated
that the standard linear programming (LP) form of the MOT problem is not a minimum-cost
3. This encourages us to study the alternatives to combinatorial
ﬂow problem when m
algorithms and standard deterministic interior-point algorithms.
In particular, we consid-
ered an entropic regularized version of the MOT problem, developing two deterministic al-
gorithms — the multimarginal Sinkhorn and accelerated multimarginal Sinkhorn algorithms
— for solving it. Combined with a new rounding scheme, the multimarginal Sinkhorn al-
gorithm can solve the MOT problem and achieves a near-linear time complexity bound of
∞ log(n)ε−2). For the accelerated multimarginal Sinkhorn algorithm, the com-
2
O(m3nm
k
4/3
plexity bound is O(m3nm+1/3
∞ (log(n))1/3ε−4/3) which is not near-linear in the number
k
of variables nm but has better dependence on 1/ε than that of the multimarginal Sinkhorn
algorithm.

C
k

C
k

We now discuss a few directions that arise naturally from our work. First, the complexity
bounds of the proposed algorithms in this paper do not incorporate low-rank approximation
framework for the cost tensor C. Intuitively, these low-rank approaches will lead to an im-
provement of these complexity bounds in terms of the number of support points n. Therefore,
with the low-rank approaches, the implementation of these algorithms will be feasible under
the large-scale settings of the MOT problem. Second, as mentioned in the paper, one draw-
back of the entropic regularization is that the sparsity of the solution is lost. Even though
an ε-approximate transportation plan can be obtained eﬃciently, it is not clear how diﬀerent
the resulting sparsity pattern of the obtained solution is with respect to the solution of the
actual MOT problem. An important direction is to incorporate sparsity penalty functions
to the entropic regularized MOT problem such that an ε-approximate sparse transportation
plan is achieved. Third, the MOT problem suﬀers from curse of dimensionality, demonstrat-
ing the importance of eﬃcient dimension reduction frameworks in both theory and practice.
Finally, it is of interest to extend the current algorithms in the paper to the multimarginal op-
timal transport among general measures, which are not necessarily probability measures, such
as multimarginal unbalanced optimal transport [Pham et al., 2020] or multimarginal partial
optimal transport [Le et al., 2022].

8 Acknowledgments

This work was supported in part by the Mathematical Data Science program of the Oﬃce of
Naval Research under grant number N00014-18-1-2764 to MJ, and by the NSF IFML 2019844
award and research gifts by UT Austin ML grant to NH.

References

M. Agueh and G. Carlier. Barycenters in the Wasserstein space. SIAM Journal on Mathe-

matical Analysis, 43(2):904–924, 2011. (Cited on page 2.)

Z. Allen-Zhu, Z. Qu, P. Richt´arik, and Y. Yuan. Even faster accelerated coordinate descent

using non-uniform sampling. In ICML, pages 1110–1119, 2016. (Cited on page 24.)

34

J. Altschuler, J. Weed, and P. Rigollet. Near-linear time approximation algorithms for optimal
transport via Sinkhorn iteration. In NeurIPS, pages 1964–1974, 2017. (Cited on pages 3, 6, 8,
15, 16, 30, and 31.)

J. M. Altschuler and E. Boix-Adsera. Hardness results for multimarginal optimal transport

problems. Discrete Optimization, 42:100669, 2021a. (Cited on pages 14 and 23.)

J. M. Altschuler and E. Boix-Adsera. Wasserstein barycenters can be computed in polynomial
time in ﬁxed dimension. Journal of Machine Learning Research, 22:1–19, 2021b. (Cited on
page 23.)

J. M. Altschuler and E. Boix-Adser`a. Wasserstein barycenters are NP-hard to compute. SIAM

Journal on Mathematics of Data Science, 4(1):179–203, 2022. (Cited on page 23.)

E. Anderes, S. Borgwardt, and J. Miller. Discrete Wasserstein barycenters: Optimal transport
for discrete data. Mathematical Methods of Operations Research, 84(2):389–409, 2016. (Cited
on page 14.)

J-D. Benamou, G. Carlier, M. Cuturi, L. Nenna, and G. Peyr´e. Iterative Bregman projections
for regularized transportation problems. SIAM Journal on Scientiﬁc Computing, 37(2):
A1111–A1138, 2015. (Cited on pages 2, 4, 6, 14, 15, 23, and 33.)

J-D. Benamou, G. Carlier, and L. Nenna. Generalized incompressible ﬂows, multi-marginal
transport and Sinkhorn algorithm. Numerische Mathematik, 142(1):33–54, 2019. (Cited on
pages 4, 6, 15, and 33.)

C. Berge. The Theory of Graphs. Courier Corporation, 2001. (Cited on page 11.)

J. Blanchet, A. Jambulapati, C. Kent, and A. Sidford. Towards optimal running times for

optimal transport. ArXiv Preprint: 1810.07717, 2018. (Cited on pages 3 and 30.)

Y. Brenier. The least action principle and the related concept of generalized ﬂows for incom-
pressible perfect ﬂuids. Journal of the American Mathematical Society, 2(2):225–255, 1989.
(Cited on page 2.)

Y. Brenier. Minimal geodesics on groups of volume-preserving maps and generalized solutions
of the Euler equations. Communications on Pure and Applied Mathematics: A Journal
Issued by the Courant Institute of Mathematical Sciences, 52(4):411–452, 1999.
(Cited on
page 2.)

Y. Brenier. Generalized solutions and hydrostatic approximation of the Euler equations.

Physica D: Nonlinear Phenomena, 237(14-17):1982–1988, 2008. (Cited on page 2.)

G. Buttazzo, L. D. Pascale, and P. Gori-Giorgi. Optimal-transport formulation of electronic

density-functional theory. Physical Review A, 85(6), 2012. (Cited on page 2.)

J. Cao, L. Mo, Y. Zhang, K. Jia, C. Shen, and M. Tan. Multi-marginal Wasserstein GAN. In

NeurIPS, pages 1776–1786, 2019. (Cited on page 2.)

G. Carlier and I. Ekeland. Matching for teams. Economic Theory, 42(2):397–418, 2010a.

(Cited on page 2.)

35

G. Carlier and I. Ekeland. Hedonic price equilibria, stable matching and optimal transport:
Equivalence, topology and uniqueness. Economic Theory, 42(2):317–354, 2010b. (Cited on
page 2.)

G. Carlier, A. Oberman, and E. Oudet. Numerical methods for matching for teams and
Wasserstein barycenters. ESAIM: Mathematical Modelling and Numerical Analysis, 49(6):
1621–1642, 2015. (Cited on pages 2 and 14.)

Y. Choi, M. Choi, M. Kim, J-W. Ha, S. Kim, and J. Choo. Stargan: Uniﬁed generative
adversarial networks for multi-domain image-to-image translation. In CVPR, pages 8789–
8797, 2018. (Cited on page 2.)

S. Claici, E. Chien, and J. Solomon. Stochastic Wasserstein barycenters. In ICML, pages

999–1008. PMLR, 2018. (Cited on page 14.)

M. B. Cohen, Y. T. Lee, and Z. Song. Solving linear programs in the current matrix multi-

plication time. In STOC, pages 938–942, 2019. (Cited on page 14.)

C. Cotar, G. Friesecke, and C. Kl¨uppelberg. Density functional theory and optimal trans-
portation with Coulomb cost. Communications on Pure and Applied Mathematics, 66(4):
548–599, 2013. (Cited on page 2.)

T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, 2012.

(Cited on pages 15, 23, and 28.)

M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NeurIPS,

pages 2292–2300, 2013. (Cited on pages 3, 6, 8, 15, and 16.)

M. Cuturi and A. Doucet. Fast computation of Wasserstein barycenters. In ICML, pages

685–693, 2014. (Cited on pages 2 and 14.)

M. Cuturi and G. Peyr´e. Semidual regularized optimal transport. SIAM Review, 60(4):

941–965, 2018. (Cited on page 7.)

S. I. Daitch and D. A. Spielman. Faster approximate lossy generalized ﬂow via interior point
(Cited on

In Foundations of Computer Science, pages 451–460. ACM, 2008.

algorithms.
page 3.)

I. S. Dhillon, P. K. Ravikumar, and A. Tewari. Nearest neighbor based greedy coordinate

descent. In NeurIPS, pages 2160–2168, 2011. (Cited on page 15.)

J. Diakonikolas and L. Orecchia. Alternating randomized block coordinate descent. In ICML,

pages 1224–1232. PMLR, 2018. (Cited on page 24.)

Y. Dolinsky and M. H. Soner. Robust hedging and martingale optimal transport in continuous

time. Probability Theory and Related Fields, 160:391–427, 2014. (Cited on page 2.)

R. M. Dudley. The speed of mean Glivenko-Cantelli convergence. The Annals of Mathematical

Statistics, 40(1):40–50, 1969. (Cited on page 6.)

P. Dvurechensky, D. Dvinskikh, A. Gasnikov, C. A. Uribe, and A. Nedi´c. Decentralize and
randomize: Faster algorithm for Wasserstein barycenters. In NeurIPS, pages 10760–10770,
2018a. (Cited on page 14.)

36

P. Dvurechensky, A. Gasnikov, and A. Kroshnin. Computational optimal transport: Com-
plexity by accelerated gradient descent is better than by Sinkhorn’s algorithm. In ICML,
pages 1367–1376, 2018b. (Cited on pages 3, 8, and 15.)

J. Edmonds and R. M. Karp. Theoretical improvements in algorithmic eﬃciency for network

ﬂow problems. Journal of the ACM (JACM), 19(2):248–264, 1972. (Cited on page 3.)

I. Ekeland. An optimal matching problem. ESAIM: Control, Optimisation and Calculus of

Variations, 11(1):57–71, 2005. (Cited on page 2.)

T. R. Ervolina and S. T. McCormick. Canceling most helpful total cuts for minimum cost

network ﬂow. Networks, 23(1):41–52, 1993a. (Cited on page 3.)

T. R. Ervolina and S. T. McCormick. Two strongly polynomial cut cancelling algorithms for
minimum cost network ﬂow. Discrete Applied Mathematics, 46(2):133–165, 1993b. (Cited
on page 3.)

O. Fercoq and P. Richt´arik. Accelerated, parallel, and proximal coordinate descent. SIAM

Journal on Optimization, 25(4):1997–2023, 2015. (Cited on page 24.)

R. Flamary and N. Courty.

POT: Python optimal transport library, 2017.

URL

https://pythonot.github.io/. (Cited on page 3.)

N. Fournier and A. Guillin. On the rate of convergence in Wasserstein distance of the empirical
measure. Probability Theory and Related Fields, 162(3-4):707–738, 2015. (Cited on page 6.)

A. Galichon, P. Henry-Labordere, and N. Touz. A stochastic control approach to non-arbitrage
bounds given marginals, with an application to Lookback options. The Annals of Applied
Probability, 24:312–336, 2014. (Cited on page 2.)

Z. Galil and ´E. Tardos. An o(n2(m+nlogn)logn) min-cost ﬂow algorithm. Journal of the ACM

(JACM), 35(2):374–386, 1988. (Cited on page 3.)

W. Gangbo and A. Swiech. Optimal maps for the multidimensional Monge-Kantorovich
problem. Communications on Pure and Applied Mathematics, 51(1):23–45, 1998. (Cited on
page 1.)

M. R. Garey and D. S. Johnson. Computers and Intractability, volume 29. WH Freeman New

York, 2002. (Cited on page 12.)

D. Ge, H. Wang, Z. Xiong, and Y. Ye.

Interior-point methods strike back: Solving the

Wasserstein barycenter problem. In NeurIPS, pages 6894–6905, 2019. (Cited on page 14.)

A. Genevay, L. Chizat, F. Bach, M. Cuturi, and G. Peyr´e. Sample complexity of Sinkhorn

divergences. In AISTATS, 2019. (Cited on page 7.)

A. Ghouila-Houri. Caract´erisation des matrices totalement unimodulaires. Comptes Redus
Hebdomadaires des S´eances de l’Acad´emie des Sciences (Paris), 254:1192–1194, 1962. (Cited
on page 13.)

A. V. Goldberg and S. Rao. Beyond the ﬂow decomposition barrier. Journal of the ACM

(JACM), 45(5):783–797, 1998. (Cited on page 3.)

37

A. V. Goldberg and R. E. Tarjan. Finding minimum-cost circulations by successive approxi-

mation. Mathematics of Operations Research, 15(3):430–466, 1990. (Cited on page 3.)

S. Guminov, P. Dvurechensky, N. Tupitsa, and A. Gasnikov. Accelerated alternating mini-
mization, accelerated Sinkhorn’s algorithm and accelerated iterative Bregman projections.
ArXiv Preprint: 1906.03622, 2019. (Cited on page 3.)

R. Hassin. The minimum cost ﬂow problem: a unifying approach to dual algorithms and
(Cited on

a new tree-search algorithm. Mathematical Programming, 25(2):228–239, 1983.
page 3.)

R. Hassin. Algorithms for the minimum cost circulation problem based on maximizing the
mean improvement. Operations Research Letters, 12(4):227–233, 1992. (Cited on page 3.)

Z. He, W. Zuo, M. Kan, S. Shan, and X. Chen. Attgan: Facial attribute editing by only
changing what you want. IEEE Transactions on Image Processing, 28(11):5464–5478, 2019.
(Cited on page 2.)

L. Hui, X. Li, J. Chen, H. He, and J. Yang. Unsupervised multi-domain image translation
with domain-speciﬁc encoders/decoders. In ICPR, pages 2044–2049. IEEE, 2018. (Cited on
page 2.)

A. Jambulapati, A. Sidford, and K. Tian. A direct tilde

algorithm for optimal transport. In NeurIPS, pages 11355–11366, 2019.
and 30.)

O
{

(1/epsilon) iteration parallel
}
(Cited on pages 3

L. V. Kantorovich. On the translocation of masses.

In Dokl. Akad. Nauk. USSR (NS),

volume 37, pages 199–201, 1942. (Cited on pages 1 and 5.)

R. M. Karp. Reducibility among combinatorial problems. In Complexity of Computer Com-

putations, pages 85–103. Springer, 1972. (Cited on page 12.)

M. Klein. A primal method for minimal cost ﬂows with applications to the assignment and

transportation problems. Management Science, 14(3):205–220, 1967. (Cited on page 3.)

A. Kroshnin, N. Tupitsa, D. Dvinskikh, P. Dvurechensky, A. Gasnikov, and C. Uribe. On the
complexity of approximating Wasserstein barycenters. In ICML, pages 3530–3540, 2019.
(Cited on pages 3 and 14.)

T. Lacombe, M. Cuturi, and S. Oudot. Large scale computation of means and clusters for

persistence diagrams using optimal transport. In NeurIPS, 2018. (Cited on page 17.)

N. Lahn, D. Mulchandani, and S. Raghvendra. A graph theoretic additive approximation of

optimal transport. In NeurIPS, pages 13836–13846, 2019. (Cited on pages 3 and 30.)

K. Le, H. Nguyen, K. Nguyen, T. Pham, and N. Ho. On multimarginal partial optimal
transport: Equivalent forms and computational complexity. In AISTATS, 2022. (Cited on
page 34.)

Y. T. Lee and A. Sidford. Path ﬁnding methods for linear programming: Solving linear pro-
O(sqrt(rank)) iterations and faster algorithms for maximum ﬂow. In Foundations

grams in
of Computer Science, pages 424–433. IEEE, 2014. (Cited on pages 3, 12, and 14.)

e

38

J. Lei. Convergence and concentration of empirical measures under Wasserstein distance in

unbounded functional spaces. Bernoulli, 26(1):767–798, 2020. (Cited on page 6.)

Q. Lin, Z. Lu, and L. Xiao. An accelerated randomized proximal coordinate gradient method
and its application to regularized empirical risk minimization. SIAM Journal on Optimiza-
tion, 25(4):2244–2273, 2015. (Cited on page 24.)

T. Lin, N. Ho, and M. Jordan. On eﬃcient optimal transport: An analysis of greedy and
accelerated mirror descent algorithms. In ICML, pages 3982–3991, 2019a. (Cited on pages 3,
8, and 15.)

T. Lin, N. Ho, and M. I. Jordan. On the eﬃciency of the Sinkhorn and Greenkhorn algorithms
and their acceleration for optimal transport. ArXiv Preprint: 1906.01437, 2019b. (Cited on
page 3.)

T. Lin, N. Ho, X. Chen, M. Cuturi, and M. I. Jordan. Fixed-support Wasserstein barycenters:
Computational hardness and fast algorithm. In NeurIPS, pages 5368–5380, 2020. (Cited on
pages 3 and 14.)

H. Lu, R. Freund, and V. Mirrokni. Accelerating greedy coordinate descent methods.

In

ICML, pages 3257–3266, 2018. (Cited on page 24.)

G. Mena and J. Niles-Weed. Statistical bounds for entropic optimal transport: Sample com-
plexity and the central limit theorem. In NeurIPS, pages 4541–4551, 2019. (Cited on page 7.)

C. B. Mendl and L. Lin. Kantorovich dual solution for strictly correlated electrons in atoms

and molecules. Physical Review B, 87:125106, 2013. (Cited on page 2.)

O. Meshi, A. Globerson, and T. S. Jaakkola. Convergence rate analysis of MAP coordinate

minimization algorithms. In NeurIPS, pages 3014–3022, 2012. (Cited on page 15.)

L. Mi and J. Bento. Multi-marginal optimal transport deﬁnes a generalized metric. ArXiv

Preprint: 2001.11114, 2020. (Cited on page 2.)

Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103

(1):127–152, 2005. (Cited on pages 8 and 9.)

Y. Nesterov. Eﬃciency of coordinate descent methods on huge-scale optimization problems.

SIAM Journal on Optimization, 22(2):341–362, 2012. (Cited on page 24.)

Y. Nesterov. Lectures on Convex Optimization, volume 137. Springer, 2018. (Cited on pages 10

and 24.)

J. Nutini, M. Schmidt, I. Laradji, M. Friedlander, and H. Koepke. Coordinate descent con-
verges faster with the Gauss-Southwell rule than random selection. In ICML, pages 1632–
1641, 2015. (Cited on page 15.)

J. B. Orlin. A faster strongly polynomial minimum cost ﬂow algorithm. Operations Research,

41(2):338–350, 1993. (Cited on page 3.)

J. B. Orlin. A polynomial time primal network simplex algorithm for minimum cost ﬂows.

Mathematical Programming, 78(2):109–129, 1997. (Cited on pages 3, 12, and 14.)

39

B. Pass. Multi-marginal optimal transport: Theory and applications. ESAIM: Mathematical

Modelling and Numerical Analysis, 49(6):1771–1790, 2015. (Cited on pages 1 and 2.)

G. Peyr´e and M. Cuturi. Computational Optimal Transport: With Applications to Data
(Cited on pages 2, 4, 15,

Science. Foundations and Trends(r) in Machine Learning, 2019.
and 33.)

K. Pham, K. Le, N. Ho, T. Pham, and H. Bui. On unbalanced optimal transport: An analysis

of Sinkhorn algorithm. In ICML, pages 7673–7682. PMLR, 2020. (Cited on page 34.)

A. Schrijver. Combinatorial Optimization: Polyhedra and Eﬃciency, volume 24. Springer

Science & Business Media, 2003. (Cited on pages 3 and 13.)

M. Seidl, P. Gori-Giorgi, and A. Savi. Strictly correlated electrons in density-functional
theory: A general formulation with applications to spherical densities. Physical Review A,
75:75:042511, 2007. (Cited on page 2.)

S. Srivastava, C. Li, and D. Dunson. Scalable Bayes via barycenter in Wasserstein space.

Journal of Machine Learning Research, 19(8):1–35, 2018. (Cited on page 2.)

M. Staib, S. Claici, J. M. Solomon, and S. Jegelka. Parallel streaming Wasserstein barycenters.

In NeurIPS, pages 2647–2658, 2017. (Cited on page 14.)

´E. Tardos. A strongly polynomial minimum cost circulation algorithm. Combinatorica, 5(3):

247–255, 1985. (Cited on page 3.)

R. E. Tarjan. Dynamic trees as search trees via euler tours, applied to the network simplex

algorithm. Mathematical Programming, 78(2):169–177, 1997. (Cited on pages 12 and 14.)

P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. sub-

mitted to SIAM Journal on Optimization, 2(3), 2008. (Cited on page 27.)

N. Tupitsa, P. Dvurechensky, A. Gasnikov, and C. A. Uribe. Multimarginal optimal transport
by accelerated alternating minimization. In CDC, pages 6132–6137. IEEE, 2020. (Cited on
pages 1, 4, 8, 15, 24, and 25.)

C. Villani. Topics in Optimal Transportation. American Mathematical Society, Providence,

RI, 2003. (Cited on pages 1 and 12.)

J. Weed and F. Bach. Sharp asymptotic and ﬁnite-sample rates of convergence of empirical
measures in Wasserstein distance. Bernoulli, 25(4A):2620–2648, 2019. (Cited on page 6.)

S. J. Wright. Primal-Dual Interior-Point Methods, volume 54. SIAM, 1997. (Cited on page 14.)

Y. Xie, X. Wang, R. Wang, and H. Zha. A fast proximal point method for computing exact

Wasserstein distance. In UAI, pages 433–453. PMLR, 2020. (Cited on page 3.)

40

