Early Detection of Security-Relevant Bug Reports
using Machine Learning: How Far Are We?

Arthur D. Sawadogo*, Quentin Guimard*, Tegawend´e F. Bissyand´e†,
Abdoul Kader Kabore†, Jacques Klein†, Naouel Moha*
*Universit´e du Qu´ebec `a Montr´eal
†University of Luxembourg

1
2
0
2
c
e
D
9
1

]
E
S
.
s
c
[

1
v
3
2
1
0
1
.
2
1
1
2
:
v
i
X
r
a

Abstract—Bug reports are common artefacts in software devel-
opment. They serve as the main channel for users to communicate
to developers information about the issues that they encounter
when using released versions of software programs. In the
descriptions of issues, however, a user may, intentionally or not,
expose a vulnerability. In a typical maintenance scenario, such
security-relevant bug reports are prioritised by the development
team when preparing corrective patches. Nevertheless, when
security relevance is not immediately expressed (e.g., via a tag)
or rapidly identiﬁed by triaging teams, the open security-relevant
bug report can become a critical leak of sensitive information that
attackers can leverage to perform zero-day attacks. To support
practitioners in triaging bug reports, the research community
has proposed a number of approaches for the detection of
security-relevant bug reports. In recent years, approaches in
this respect based on machine learning have been reported with
promising performance. Our work focuses on such approaches,
and revisits their building blocks to provide a comprehensive
view on the current achievements. To that end, we built a large
experimental dataset and performed extensive experiments with
variations in feature sets and learning algorithms. Eventually,
our study highlights different approach conﬁgurations that yield
best performing classiﬁers.

Index Terms—Machine-learning, security, bug reports, repos-

itories mining, vulnerability, deep-learning.

I. INTRODUCTION

Bug tracking systems are commonplace in both open source
software development and large industrial software projects.
Their consistent use as part of software project management
is regarded as one of the hallmark of a good software de-
velopment team [1]. In industrial settings, development teams
can setup bug tracking systems to keep a centralized view of
all bug ﬁxes (and potentially improvements) brought to the
code base. In open source communities, they allow end-users
to report bugs directly to the development teams.

A bug tracking system mainly includes a database of bug
reports, which are natural language descriptions of erroneous
behavior cases that are encountered when running the soft-
ware. A given record (i.e., a bug report’s details in the
database) may include the identity of the person who reported
it, the time it was reported, its severity and other tags that
can be leveraged for facilitating triaging. When the severity
of the bug is major (e.g., a blocking bug [2]) or when the
bug is relevant to security, development teams urge to address
the bug report. Sometimes, however, the person who reported
a problem with respect
to end-user requirements, may be
unaware that the deviated behavior is actually also offering

a vulnerability window for attackers to misuse the software.
Thus, unintentionally, such security-relevant bug reports can
publicly disclose exploitable vulnerabilities. Until the triaging
team can notice the security risk (and therefore take actions to
hide [3], [4] the bug report until a ﬁx is found), any malicious
actor can leverage the divulged input and perform zero-day
attacks.

Security-relevant bug reports can be categorized as Hidden
Impact Bugs [5]: the bugs are often only identiﬁed as vul-
nerabilities long after they have been made public. Identifying
security-relevant bug reports is thus an important challenge for
bug triagers. Automating this task will consequently improve
bug report management where vulnerabilities are assigned
immediately to the appropriate developers and are patched
in priority. An early attempt to automating the detection of
security-relevant bugs was performed by Wijayasekara et al.
[5], [6] focusing on Linux kernel CVEs1 that are associ-
ated to bug reports. Their classiﬁer was based on a basic
“bag of words” approach in combination with Naive Bayes,
Naive Bayes Multinomial, and Decision Tree classiﬁers. Behl
et al. [7] then used the Term Frequency-Inverse Document
Frequency (TF-IDF) along with a “vector space model” and
showed that it provides better performance than the Naive
Bayes algorithm. More recently, Peters et al. [8] proposed
a framework called FARSEC, which integrated ﬁltering and
ranking for security bug report prediction: prior to creating
the prediction models, FARSEC identiﬁes and removes non-
security bug reports with security related keywords. This
ﬁltering step is mainly aimed at decreasing the false positive
predictions. Unlike previous related works, which all used
prediction models based on machine learning, Goseva and
Tyo [9] also proposed to investigate unsupervised learning
algorithms and assessed the importance of the size of the
dataset. More recently, Shu et al. [10] studied hyperparameter
tuning and data pre-processing to improve the performance of
security-relevant bug report classiﬁcation.

Overall, it appears that machine learning has become a
central technique for identifying security-relevant bug reports.
The state of the art in this respect remains however unclear
about the limitations and promises of the different learning
choices. Thus, we propose in this paper to present extensive
experiments on features and algorithms for establishing the

1Common Vulnerability Exposure

 
 
 
 
 
 
achievements that can be reached. Our paper makes the
following contributions to the ﬁeld:

• We discuss motivational cases for the research direction
on detecting security-relevant bug reports, and provide an
overview of the state of research.

• We prepare and offer to the community a large dataset
of security-relevant bug reports to enable comprehensive
investigations around this topic.

• We performed extensive experiments on learning to iden-
tify security bug reports, and enumerate insights about the
variety of algorithms, features and parameters.

Paper Organization. The remainder of this paper is or-
ganized as follows. Section II highlights the importance of
security-relevant bug report identiﬁcation through motivating
cases. Section III discusses the state of the art approaches for
identifying security-relevant bug reports. We then overview
the construction of a large experimental dataset of security-
relevant bug reports in Section IV. We present our exper-
imental results in Section V and discuss threats to validity
as well as similar works in Section VI before concluding the
paper in Section VII.

II. ON THE IMPORTANCE OF IDENTIFYING
SECURITY-RELEVANT BUG REPORTS

Although the detection of security bug reports is increas-
ingly studied, the literature does not include data or motiva-
tional cases to support the importance of the topic. In this
section, we attempt to ﬁll this gap through (1) an empirical
study to highlight how security bug reports, when identiﬁed,
are processed in priority, (2) a highlight of cases where bugs
are reported by users who are not aware of the security-
relevance and the potential for attacks.

A. Priority patching by development teams

Our ﬁrst investigation is related to the priority that is given
to security bug reports. We aim to answer the following
question: “Are security-relevant bug reports responded with
faster patches than other bug reports?” When a bug report
is submitted, developers generally attempt to address it and
eventually propose a patch to ﬁx the relevant bug. Such a
patch can however take more or less time to be prepared.
Nevertheless, we expect that the process will be relatively fast
if the bug report is about a vulnerability that could be exploited
by attackers.

To answer our research question, we consider bug report
samples from our ground truth dataset (cf. Section IV for more
details on dataset collection). Overall, we focused on RedHat
bug reports and randomly sampled 800 in each category (i.e.,
security bug reports and non-security bug reports). We then
compute the delays between the creation date of a bug report,
and its closing date. Identify the exact ﬁx date of a bug report
is not obvious, we then rely on opening and closing dates
in order to have an accurate view. While these dates do not
conﬁrm that bug reports were closed at the same time as the
reported bug was ﬁxed, they give us an idea of how important
it is to identify security-relevant bug reports early on.

Figure 1 presents the distribution of time-to-ﬁx delays for
the two categories of bug reports. The median values (2248.0
days and 49.5 days) as well as the ranges of the quartiles con-
ﬁrm that security bug reports are addressed signiﬁcantly faster
than other bug reports. Given that the dataset was constructed
based on the security tag used in the bug tracking system, this
ﬁnding suggests that it is important to appropriately tag bug
reports in order to accelerate their processing. An automatic
approach to identify bug reports that are related to security is
thus desirable to alleviate triaging effort.

Fig. 1: Delays between creation date and closing date of security
bug reports vs other bugs reports

If proper notice is given, bugs triagers are likely to
prioritize addressing security-relevant bug reports

B. Unintentional disclosure of vulnerabilities

In a typical bug reporting scenario, a user is affected by an
erroneous software behavior and writes a description of the
bug for the development team. When such a description refers
to information such as CVE details or exploitation cases, the
triaging team can immediately tag the bug report as being
security-relevant. Such a bug report is generally handled with
high priority by maintainers, and a patch is rapidly constructed.
Unfortunately, there are reporting scenarios where the bug
report, although being security-relevant, is not tagged as such
due to the lack of explicit details that would inform triagers
immediately. In such cases, a software vulnerability is poten-
tially exposed publicly. Security relevance of the bug report
can then be later pointed out by other contributors. In the mean
time, however, the software is exposed to attacks, because
the delay between the bug’s reporting and its prioritization as
security-relevant could be important. Attackers (like all other
parties interested in contributing to the software development
in open source settings), are handed critical information for
exploiting software vulnerabilities. These attackers even have
the opportunity to quickly propose a patch that satisﬁes the
bug reporter (in terms of user-expected plain functionality) but
better ensures that the vulnerability remains hidden for their
future exploitations.

We look for evidence about these conjectures by carefully
investigating 100 security bug reports randomly sampled in our
collected dataset. We mainly look for bug reports that were not
immediately ﬂagged as security-relevant but were identiﬁed as
such only later by other contributors. The existence of such
bug reports justify the need for automated techniques for early
detection of security relevance. In the following, we present
two out of several cases that we have found.

2

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllSecurity.bugsNon.security.bugs01000200030004000Delay (days)Figure 2 shows an excerpt of bug report No 319004 in the
Bug tracking system of Mozilla. Obviously, the bug reporter
did not seem to know that the bug was related to security
aspect: she only reports a bothersome development issue and
is looking for a solution. During discussions, four days later, a
seventh comment raises concerns about potential exploitations
by attackers. A CVE (CVE-2005-4134) will be ﬁled only the
following day.

XPath selector - make xml parser features configurable

Details
Type: Bug
Status:RESOLVED
Priority: Major
Resolution: Fixed
Affects Version/s:
1.7
Fix Version/s:
1.7.1, 1.8
Component/s: None
-Created: 26/Aug/14 13:05

User2 (:shaver -- probably not reading bugmail closely)

Description

We need to be able to set xml parser features
configurable using system properties.

Description
From the URL:

Basically firefox logs all kinda of URL data in it’s
history.dat file, this little script will set a really
large topic and Firefox will then save that topic into
it’s history.dat.. The next time that firefox is opened,
it will instantly crash due to a buffer overflow -- this
will happen everytime until you manually delete the
history.dat file -- which most users won’t figure out.

We’re probably blowing Mork’s little mind, and it’ll
be easier to defend in the history code than in Mork.

People are reporting varying results in #developers.

-----------------------------------------------

elfguy:
Comment 7
This is being reported as ’remote exploit’ in the news:

http://digg.com/security/Kill_Firefox_1.5_with_remote_exploit

While it’s not a security issue it should be fixed fast as
malicious users may start using it now that it’s known.

Fig. 2: Example of a security Bug report from Buzilla-mozilla

Figure 3 presents bug report APLO-366 in the bug tracking
system of JIRA. Similar to the previous case, the bug is
reported without any tag related to security. Instead, the linked
patch is commented as referring to a CVE tag (CVE-2014-
3579) being addressed. Nevertheless, the patch itself does not
mention any security matter in its description, leading to a
silent security patching, with all its consequences.

Security-relevant bug reports could expose the software to
more attacks if they are not tagged as such earlier.

The presented examples further motivate the necessity to
automate the detection of security-relevant bug reports to
enable appropriate management.

III. A REVIEW OF THE STATE OF THE ART
In this section, we present the state of the research related
to the detection of security relevant bug reports. Since security
relevant bug reports are referred to with different terminology
in the literature, we propose to rely on a light-weight snow-
balling approach to identify relevant papers. Thus, we start
with two recent papers in the literature, namely in 2019 by
Shu et al. [10] and Pereira et al. [11], and check their related
work sections. Then, we iteratively look into the related work
and references of the identiﬁed relevant papers. Eventually,
we found that most of the related work rely on machine
learning (ML) based approaches. In a ﬁrst sub-section, we

----------------------------------

Comments
User1 added a comment 26/Aug/14 13:29

Fixed with http://git-wip-us.apache.org/repos/asf/
activemq-apollo/ commit/e5647554 with some sensible
defaults introduced. Features can be set by using system
properties starting with "org.apache.activemq.apollo.
documentBuilderFactory.feature:" prefix, like
-Dorg.apache.activemq.apollo.documentBuilderFactory.feature:
http://xml.org/sax/features/external-general-entities=true

User1 added a comment - 06/Feb/15 10:44

This issue fixes CVE-2014-3579. Take a look at
http://activemq.apache.org/security-advisories.html
for more details.

Fig. 3: Example of a security Bug report from Jira-apache

summarize approaches grouped based on the type of learning
(i.e., supervised, semi-supervised, unsupervised). Then, we list
the main feature extraction methods used to feed the machine
learning algorithms. Finally, we quickly enumerate the various
validation approaches used to assess the performance of the
ML-based state of the art approaches. Overall, Table I provides
a summary of information collected during our review of the
state of the art.

A. Overview of ML-based Approaches

a) Supervised learning approaches: Supervised learning
is a classical prediction problem in which there are two classes
and labeled data from both classes (positive class as the
targeted class and negative class) are available. The chosen
machine learning algorithm is trained on one part of the
data and predict on the other part. The main problem of this
approach is that it is often difﬁcult for a given problem to
have labeled data from both classes. It could happen in some
cases that only one class is labeled. Regarding the case of
security bug report detection, the prediction problem resides in
differentiating security relevant bug reports from non-security
relevant bug reports.

Supervised learning is the most widespread approach lever-
aged in the literature for security bug report identiﬁcation.
Wijayasekara et al. [5] have presented one of the seminal
work on detecting security bug reports using machine learning.
They rely on text mining to extract syntactical information bug
reports and compress before generating feature vectors that
are fed to Naive Bayes classiﬁers. The proposed methodology

3

Approach

TABLE I: Summary details of state of the art approaches for security bug reports identiﬁcation.
#
SBR∗

Evaluation

Algorithm

Features

Dataset

Type of Ap-
proach
Supervised

Statistical
model

Automatic

Cisco
software
system

Gegick et al. [12]

Wijayasekara et al [5]

hypothetical
term-by-
document
matrix
Most frequent
words

Behl et al. [7]
Zou et al. [13]

TF-IDF
Meta features
textual
and
features
Goseva-Popstojanova et al. [9] TF,TF-
IDF,BF
TF-IDF,
descrimitive
weight
TF-IDF

Mostafa et al. [4]

Das et al. [14]

Pereira et al. [11]

TF-IDF

Supervised

Peters et al. [8]

TF-IDF, score

Supervised

Supervised

supervised
Supervised

Naives bayes,
Decision
Trees, etc.
Naives bayes
SVM

Supervised,
unsupervised
Supervised,
semi-
supervised
Supervised

Automatic

linux

Automatic
Automatic

Automatic

Automatic

Automatic

Automatic

Automatic

Bugzilla
Firefox,
Thunderbird,
Seamonkey
NASA
dataset
Rehdat,
Mozilla

Ambari,
Camel,
Derby,
Wicket
Microsoft

Chromium,
Wicket,
Ambari,
Camel, Derby

SVM, KNN,
etc.
SAIS,SVM,
etc.

Naive Bayes
Multinomial
Model

Adaboost,
Naives
bayes,etc.
logistic
regression,
Naives bayes,
etc.

# All
BR∗

6000

10000
23262

3346

664

3399

1368

16168

159

4000

552073

1073149

351

45940

∗: the ”# SBR” column indicates the number of security relevant bug reports used in an approach. The ”# All BR” column indicates the
number of security and non-security bug reports used in an approach.

was tested on Linux vulnerabilities that were discovered in
the time period from 2006 to 2011, where they managed
to correctly identify up to 88% of vulnerabilities. Such an
approach improves over the work of Gegick et al. [12] who
used a term-by-document frequency matrix from words in
the natural language descriptions of bug reports to train a
statistical model. Similarly, Behl et al. [7], later compared the
use of term frequency-inverse document frequency (TF-IDF)
against a probabilistic learning approach like Naives Bayes.

Zou et al. [13] proposed to use a combination of text-mining
features and meta-data (e.g., time, severity, and priority) for
improving identiﬁcation of security bugs reports. They trained
a supervised approach (SVM) with Radial Basis Function(
RBF) and have managed to improve previous work by over
20 percentage points.

More recently, Das et al. [14] proposed an approach based
on class imbalance sampling and TF-IDF vectors to im-
prove security-relevant bug report detection using Naive Bayes
Multinomial classiﬁcation. Pereira et al. [11] also presented
preliminary results, which show that basic TF-IDF on just the
title of bug reports may be enough to provide accurate results
in detecting security bug reports.

Following up on this state of the art investigations, Peters

et al. [8] proposed FARSEC, a framework for ﬁltering and
ranking bug reports for reducing the presence of security-
related keywords and improving text-based prediction models
for security bugs ﬁxes. Their framework is presented as
improving the use of text-mining approaches (TF-IDF, DTF,
IDF, etc.) when considering feature extraction for machine
learning.

learning

approaches:

b) Semi-supervised

Semi-
supervised learning represents the case where only one
class is labeled and correctly identiﬁed. Several algorithms
perform semi-supervised learning for identifying security bug
reports with good prediction results. However, This approach,
although very interesting, got bad performances when the
positive set is not enough representative for training.

Mostafa et al. [4] recently presented an evolutive and
realistic approach for the identiﬁcation of security bug reports
which considers the evolution of security vocabulary on NVD
database and realistic constraints like small training set for
security bugs reports prediction. They train a semi-supervised
model to predict security bugs reports that will be used to
augment the training set of a fully supervised learning model.
c) Unsupervised learning approaches: The main differ-
ence between the previous approaches (supervised and semi-
supervised) and the unsupervised approaches is the fact that

4

in the unsupervised case the data are not labeled. The algo-
rithm must then train and predict from unlabeled examples.
Clustering algorithms are frequently used to group data with
similar characteristics from a classiﬁcation point of view.

In a recent study Goseva-Popstojanova et al. [9] assess
the impact of algorithms and features in the detection of
security bug reports. They offer one the rare works that explore
unsupervised learning algorithms to regroup security bug re-
ports. Their preliminary results however shows that supervised
learning algorithms offer higher performance metrics.

d) Deep learning approaches: Since traditional machine
learning requires manual feature engineering, its performance
is highly dependent on the quality of the features identiﬁed by
researchers or practitioners. Instead, many recent works turn to
deep learning for learning the the best representation and then
applying on top the automatically-extracted feature vectors a
traditional algorithm.

In a recent work, Kukkar et al. [15] have proposed a deep
learning model for multiclass severity classiﬁcation called Bug
Severity classiﬁcation using a Convolutional Neural Network
and Random forest with Boosting. They show that the Convo-
lutional Neural Network is able to extract the important feature
patterns of respective severity classes. Although they did not
apply their work on the problem of security classiﬁcation,
to the best of our knowledge, it is one of the most related
approach that leverages deep learning.

B. Feature extraction

Since the large majority of the state of the art rely on tra-
ditional machine learning, we investigate the leaning pipeline
for reproducing different approaches. Key steps in a machine
learning pipeline are feature engineering (i.e. what are the
key ingredients that make machine learning algorithms work)
and feature extraction (the process to extract and compute
the features from the input samples). In the case of the
security bug reports detection task, data collected (i.e., the bug
reports) might not be directly readable by algorithms in their
default nature. The feature extraction step is then required. We
consider in this study many feature extraction approaches used
in the state of art for extracting representative information in
security bug reports detection. We can summarize the different
approaches to feature extraction by:

• Frequencies of words in the text used in [7], [9], [11],
[12], [14]. The idea is to use frequencies of words to
retrieve quantitative metrics such as TF/IDF, BF, DF, IDF,
etc.

• Keyword mining used in [8], [16]. The idea is to mine
a list of words related to a prediction class and to use
this list for pre-ﬁltering and/or for performing prediction
results.

C. Validation

In the literature, several validation approaches have been
used to assess the performance of the machine learning based
approaches for detecting security relevant bug reports. Overall,

related works perform manual validation or automatic valida-
tion using oracle (k-fold cross-validation, random sampling,
etc.).

• Manual Validation requires a security team to validate
the prediction results. As a result, this validation method
is the most reliable in the sense that it provides strong
evidence that the approach does not overﬁt to the training
dataset and can be leveraged in the wild (i.e., in a real-
world practitioner setting). Unfortunately, manual valida-
tion does not scale (i.e., cannot be applied for hundreds
of samples since human resources are hard and expensive
to get).

• Automatic Validation, due to its ability to scale, is the
most widespread in the literature. Traditional techniques
such as k-fold cross-validation or random split validation
are often used. They are often however criticized since
they may not offer a realistic view of the performance
(e.g., due to possible overﬁtting, dataset size limitations,
sample distributions problemes, etc.)

Automatic validation is by far the most popular technique
used to validate a newly proposed security bug report detection
approach. However, it is noteworthy that a strong prerequisite
is the availability of ground truth. In other words, the avail-
ability of a dataset of labeled bug reports (i.e., a dataset in
which bug reports are accurately tagged as security relevant
or not) is crucial.

Limitations identiﬁed thanks to this review: We pos-
tulate that performance results and conclusions presented
in the literature may be biased by some key experimental
aspects:

• size and availability of datasets: As detailed in Ta-
ble I, experimental datasets are often small or limited
in project scope. Large datasets (such as from Mi-
crosoft) are not openly available to the community.
• lack of rationale guiding algorithm selection: we
have found that most experimental approaches do not
motivate the choice of their algorithm, which may
lead to redundant approaches in the literature.
⇒ In this work, we propose to address these limita-
tions for the community by proposing a large dataset
(cf. Section IV) that can enable reproducible research
on the identiﬁcation of security-relevant bug reports. We
also consider typical features used in the literature and
typical learning algorithms to build and study experimental
baseline variations for ML-based detection of security-
relevant bug reports (cf. Section V)

D. Related work on “How far are we” studies

In recent years, the research community has investigated
many research topics to produce an estimate of the perfor-
mance that can be reached. For example, Liu et al. [17]
have investigated the case of neural-machine-translation-based
commit message generation, while Lin et al. [18] focused on
sentiment analysis for software engineering. More recently

5

Lou et al. [19] presented a ”how far are we” study on the
problem of history-driven build failure ﬁxing.
E. Related work on Bug reports triaging

Bug triaging represents an essential component of the bug
handling process. This explains the fact that through literature,
several existing works proposed differents approaches improve
the process of bugs triaging [20]–[26].

Wu et al. [27] proposed bugMiner, a tool

that able to
estimate bug report trend with trend analysis model based
on Weibull distribution. BugMiner also allows completing
redundancy check on a new or given bug report. Whang et
al. [28] built an automatic approach based on semi-supervised
learning to identify bugs components by using historical
ﬁxed bugs reports. Another work that performs automating
bug triaging in literature is done by Jain et al. [23]. They
proposed an approach using ﬁeld-based weighting scheme to
perform a machine learning tool for bugs assignment. Kevic
et al. [22] proposed a collaborative approach to do automated
bug triaging by reﬁning the list of relevant developers able to
ﬁx a given bug. They use the information retrieval method
to identify the bug reports that are similar to the existing
triaged bugs. They then analyze the changes sets associated
with each bug report and predict the bug nature. Wang et
[25] investigate unsupervised learning approach for bug
al.
triaging,
they proposed an approach based on developers’
activeness scores in component of product to build a prioritized
list of developers and then improve supervised bug triage
approaches ( [29]). Dedık et al. [24] did a comparative study
on automating bug triaging using SVM classiﬁer and TF/IDF
features vectors. They proposed an overview of requirements
and needs for automating bug reports in an industrial context
and they did a comparative study of their results in industry
vs an open-source software project usually used in research:
Firefox.

Most recent, Goyal et al. [30] propose an empirical study
of ensemble-based methods for the classiﬁcation of new bug
reports. They studied ﬁve ensemble-based classiﬁcation tech-
niques used in state of art with several machine learning algo-
rithms and show that using ensemble classiﬁcation techniques
outperform single classiﬁers.

Mani et al. [26] go further by investigating the effectiveness
of a deep learning approach for bug triaging. They propose a
new bug-report representation using a deep bidirectional re-
current neural network with attention (DBRNN-A). Nnamoko
et al. [31] propose also a deep learning approach to predict
automatically labels of a newly reported bug. They used a
combination of neural network classiﬁcation and a similar one
vs. all approach to involve training a single classiﬁer per class.

IV. DATASET

Machine Learning approaches rely on datasets. Besides the
choice of the features, their performance strongly depends on
the quality of the dataset used to train and test the obtained
model. After quickly presenting the datasets used in the
literature, we will present our own dataset that we named SBR.
We propose SBR to overcome the limitations of the existing

datasets. To the best of our knowledge, SBR is currently
the most comprehensive and largest available dataset for the
community.

A. Datasets in the Literature of Security Bug Report Identiﬁ-
cation

As revealed in summary details of Table I, datasets lever-
aged by the state of the art approaches are generally limited to
a small number of repositories, resulting in a limited number
(and under-diversiﬁed set) of collected security-relevant bug
reports. Most of the identiﬁed works in the literature use less
than six sources (projects) of bug reports for their experi-
ments. There are even a few works [5], [7], [11] that use
a single source to predict security-relevant bug reports. Most
literature approaches have been applied on datasets containing
less than 1 500 security-relevant bug reports. Zou et al. [13]
exceptionally included 3 346 security-relevant bug reports for
their work. Pereira et al. [11] have collected a huge title set
of 552 073 security-relevant bug reports. Unfortunately, the
released dataset is incomplete (and thus not reliable for most
approaches) since it does not include full description of the
bug reports.

With the aforementioned limitations (i.e., few repositories
the risk is that
and the limited number of bug reports),
the proposed approaches may overﬁt to a few repositories
and may not be able to yield comparable performance with
data from another repository. Since some authors have al-
ready demonstrated in literature [9] of security bug report
identiﬁcation that the quality of the dataset is closely tied
to the prediction performance, we propose to build a large
and representative dataset of bugs reports for providing the
community a playground to advance research directions.

B. SBR: a Security-relevant Bug Report Dataset

We built a large dataset of bug reports that we named
SBR. The SBR dataset contains 5 028 security-relevant bug
reports and 9 336 non-security relevant bugs reports. To the
best of our knowledge, SBR is the largest dataset
in the
literature. Morever, SBR has been built by collecting bug
reports from various sources and by various means: Mozilla
and RedHat bug reports that include tags for security relevance
or not, bug reports linked to commits that were collected
and manually assessed by previous works as being related to
security issues [32], and security bug reports from a literature
dataset [3].

Table II summarizes the statistics on the different collection

sources.

Concretely, a number of security-relevant bug reports have
been manually collected by relying on a publicly available
security-commits dataset release in 2018 [32]. The main difﬁ-
culty in this ﬁrst part was to link commits to bugs reports. To
perform this task, we ﬁrst parsed and used regular expressions
to extract bug ids from the security-relevant commits. Then,
we used the API rest call for JIRA-projects (or manual process
for other projects), in order to extract bug reports correspond-
ing to the identiﬁed bug ids. Note that the dataset released

6

TABLE II: Dataset Details.

Source

manually collected from 79 Github projects
(starting from commits collected by Ponta et
al [32])

Bug reports available in the dataset proposed
by from Mostafa et al [3]

Mozilla Bugzilla

Redhat

All

Security
bug
reports

124

Non-
security
bug reports

257

total

381

9079

9079

202

4702

5028

9336

202

4702

14364

in [32] allows us to also collect non-security commits. By
following a similar process, these non-security commits have
been used to retrieve non-security bug reports. At the end of
this ﬁrst step, we identiﬁed 124 security bug reports and 257
non-security bug reports within 79 Github-hosted projects.

By checking the bug tracking systems of Mozilla and
RedHat projects we were able to rely on the publicly-visible
security tags put by development teams to collect 202 and
4 702 security-relevant bug reports, respectively from Mozilla
and RedHat.

A large set of non-security bug reports are obtained from
a publicly available dataset published by [3]. This dataset
includes 9 079 labeled non-vulnerable bugs reports.

Finally, we standardized the data formats (after curating
unreadable bug reports due to encoding issues and dropping
incomplete records) and merged all the collected bug reports to
obtain labeled dataset that contains 5028 security bugs reports
and 9336 non-security bugs reports. Besides the number of
security and non-security bug reports, another asset of the SBR
dataset is that it covers over 80 repositories (Redhat, Mozilla
and 79 Github projects). Figure 4 shows an example of a bug
report record in our dataset, which we publicly released at:

https://github.com/AAA-create/dataset.

{

"id": "JENKINS-47593",
"title": "can not to be online without permission",
"description": "After upgrade to 2.86 plugin need extra
permission for dynamic script \u00a0to make slave online
via ssh connection.\u00a0\r\n\r\nI can not use\u00a0\
"In-process Script Approval\" because this script look
like\u00a0\r\n’ssh user@123.212.12.232 ....’ and it is
dynamic!\u00a0"

},

Fig. 4: Example of a Bug report on our json format dataset

V. EXPERIMENTAL ASSESSMENT OF ML-BASED
IDENTIFICATION OF SECURITY-RELEVANT BUG REPORTS

The goal of this work is to investigate the performance that
can be achieved by reproducing the essential components of
literature approaches. The objective is thus to evaluate the
performance of various combinations of features (BF, DF, IDF,
TF-IDF, etc.) and machine learning algorithms in order to
identify what are the key ingredients leading to an efﬁcient
security-relevant bug report detector. We select to perform
the reproduction of literature ideas (instead of replication of

approaches) since our objective is not to compare the state of
the art approaches among themselves.

Concretely, we perform a comprehensive study by leverag-
ing the newly created SBR dataset (with both security and non-
security bug reports), and the identiﬁed features and algorithms
from literature works. In the following, we ﬁrst detail the
methodology of our study and then present the results.

A. Methodology

We propose to perform feature extraction following the
approaches proposed in the literature of machine learning for
text content. Most of these features are the same as those
used by the state of the art approaches for security bug report
identiﬁcation. Then, we propose to focus our investigations on
supervised learning, which is the most common model learning
scenario in the literature. We ﬁnally attempt a deep learning
experiment as an initial result that the community can build
on.

1) Feature extraction: In this work, we build four types of
features vectors: Binary Bag-of-words (BF), Term Frequency
(TF), Term Frequency-Inverse Document Frequency (TF-IDF)
and Word Embedding vectors (WE). For each bug report, we
then compute these features according to their respectively
equations.

• Binary Bag-of-Words (BF) is deﬁned by the following

equation:

BF (term) =

(cid:26) 0, if f (term) = 0
1, if f (term) > 0

(1)

with BF(term) being the binary bag-of-word of a term in
a given document and f(term) the frequency of this term.
Simply, for each given bug report, Binary bag-of-word
determines the presence or not of words of the collected
vocabulary (from all bug reports).

• Term Frequency (TF) is deﬁned by the following equa-

tion:

T F (term) = f (term)

(2)

TF gives the frequency of a word apparition for each
document. Words are enumerated from a given vocabu-
lary (e.g., the set of all words appearing across all bug
reports).

• Term Frequency-Inverse Document Frequency (TF-

IDF) is deﬁned by the following equation:

T F − IDF (term) = f (term) ∗ log

|D|
N (term)

(3)

with N (term) being the number of documents where
term appears and |D| the total number of documents
in the corpus. In other words, Term Frequency-Inverse
Document Frequency (TF-IDF) is a measure of how
much information the word provides, that is, whether it
is common or rare across all bug reports.

• Word embedding is common in natural language pro-
cessing approaches. It allows to represent words as nu-
merical vectors so that similar words are close in the

7

TABLE III: Classiﬁcation performance results with different combinations of algorithms and feature sets: the full content
(title+description) of the bug report is considered as input

BF

TF

TF-IDF

Learners

Accuracy

Precision

Recall

F-score

Accuracy

Precision

Recall

F-score

Accuracy

Precision

Recall

F-score

XGB
SVC
Decision Tree (DT)
Random Forest (RF)
Gaussian Naive Bayes (GNB)
K-Nearest-Neighbors (KNN)
Bagged Decision Trees 1 (BDT1)
Bagged Decision Trees 2 (BDT2)
Extra Trees (ET)
AdaBoost 1 (ADA1)
AdaBoost 2 (ADA2)
Gradient Boosting (GB)

0.688308
0.418101
0.687938
0.688308
0.546475
0.401242
0.687716
0.687716
0.687938
0.688751
0.688751
0.688308

0.544481
0.384834
0.544249
0.544481
0.444359
0.378142
0.544144
0.544144
0.544249
0.544825
0.544825
0.544481

0.882427
0.998138
0.881078
0.882427
0.978562
0.998138
0.879497
0.879497
0.881078
0.884555
0.884555
0.882427

0.673172
0.555323
0.672588
0.673172
0.610997
0.548297
0.672038
0.672038
0.672588
0.674039
0.674039
0.673172

0.839412
0.350655
0.798199
0.847444
0.635025
0.759920
0.844929
0.844929
0.847444
0.820063
0.820063
0.839342

0.809743
0.350655
0.705701
0.845274
0.194023
0.618666
0.834244
0.834244
0.846640
0.759188
0.759188
0.811433

0.708201
1.000000
0.727345
0.691343
0.012520
0.821085
0.695179
0.695179
0.689651
0.712291
0.712291
0.705528

0.755371
0.519057
0.716188
0.760493
0.023493
0.705445
0.758347
0.758347
0.760045
0.734750
0.734750
0.754581

0.979434
0.881345
0.970233
0.982014
0.860290
0.919760
0.974136
0.974136
0.982502
0.978597
0.978597
0.976715

0.994105
0.990082
0.962746
0.987868
0.802827
0.879974
0.978457
0.978457
0.988701
0.982016
0.982016
0.991426

0.946844
0.668059
0.951859
0.960472
0.797215
0.892604
0.947065
0.947065
0.961064
0.956444
0.956444
0.941649

0.969864
0.797654
0.957213
0.973944
0.799780
0.886049
0.962426
0.962426
0.974663
0.969037
0.969037
0.965843

vector space. We relied on the default embedding imple-
mentor of Tensorﬂow2 to compute the word embeddings
of bug reports.

In our methodology, we focus only on the textual description
of bug reports. No other artefacts (e.g., which developer was
assigned to the bug report) to avoid leaking information in the
training. Indeed, we consider the identiﬁcation of security-
relevant bug reports as a zero-day problem: the only informa-
tion available is the report that was submitted before anyone
from the development team could add insightful comments or
data.

Since we are performing early detection of security bug
reports, we extract three contents that are available at the time
the bug report is submitted: Title content, Description
content, and the concatenation of these two contents as
Title+Descrition content. For each content, we compute
TF, TF-IDF, BF and WE vectors. Note that before building
the feature vectors, some preprocessing operators have been
applied: we removed stop words (e.g., “as”, “is”, “would”,
etc.) which are very frequently appearing in any English
document and will not hold any discriminative power; we also
do stemming (only consider the root of words) using Porter
Stemming [33] method.

2) Model learning: Our experiments are focused on su-
pervised learning with manual feature engineering or deep
representation learning for feature vector inference.

Supervised learning: We experiment with eleven (11)
different supervised algorithms on our features sets. These
are Decision Tree, Decision Trees (Bagged), Random Forest,
Extra Trees, Adaptive Boosting, Gradient Boosting, eXtreme
Gradient Boosting, Gaussian Naive Bayes, Support Vector
Classiﬁer, K-Nearest-Neighbors, and Stochastic Gradient De-
scent. All these classiﬁers are used with their default hyper-
parameter values from the scikit-learn library, and we com-
pute the performance metrics based on 5-fold cross-validation
(given to the datase size).

Deep learning: The use of deep learning in security
detection is not much studied in the state of the art. We
propose however to experiment a straightforward approach
by leveraging Long Short Term Memory (LSTM) [34] and
Convolutional Neural Network (CNN) [35]. Since we already
use word embedding for feature extraction, we can apply

2tensorﬂow.feature.text

LSTM to the classiﬁcation problem of security relevance.
Nevertheless, we resort to applying CNN on the best data
format that it has been shown to be successful on, namely
images. Thus, we use Text2Image3 method to convert bug
reports to images for training and testing the CNN model.
More speciﬁcally, after a pre-processing step (tokenization,
stop-word removal, etc.), Text2Image builds 7x7 arrays by
considering the 49 words with the highest TF-IDF. We note
here that the number of words chosen can be considered as a
hyper-parameter. Finally, the images are plotted as heatmaps
on log scale using Gaussian ﬁltering for smoothness. Figure
5 shows an example of a security bug report heatmap. The
hottest the color is, the highest the TF-IDF value is. Given
such heatmaps, we use CNN to train a deep learning classiﬁer
to discriminate security-relevant bug reports from other bug
reports.

Fig. 5: Heatmap representation of a bug report (with hight TF-
IDF values being represented with hot colors - towards yellow)

B. Overall Results

We present the empirical results for the different classiﬁers
that we have built. Besides the feature extraction methods and
the algorithms, we have also varied the feature vector sizes as
well as the target bug report content (e.g., title or description)
to assess the impact of the different variations.

Exploring features and algorithms

Table III shows the classiﬁcation performance results that are
obtained for each pairwise combination of learning algorithms
and feature sets. We note that the F-score performance has a
wide range between 0.67 and 0.97 depending on the combina-
tion of feature set and algorithm. TF-IDF based feature set of-

3https://towardsdatascience.com/text2image-a-new-way-to-nlp-

cbf63376aa0d

8

TABLE IV: Classiﬁcation performance results - considering only bug report title as input

BF

TF

TF-IDF

XGB
SVC
Decision Tree (DT)
Random Forest (RF)
Gaussian Naive Bayes (GNB)
K-Nearest-Neighbors (KNN)
Bagged Decision Trees 1 (BDT1)
Bagged Decision Trees 2 (BDT2)
Extra Trees (ET)
AdaBoost 1 (ADA1)
AdaBoost 2 (ADA2)
Gradient Boosting (GB)

Accuracy
646831
0.355277
0.646831
0.646831
0.374386
0.357595
0.646831
0.646831
0.646831
0.646831
0.646831
0.646831

Precision
0.000000
0.353896
0.000000
0.000000
0.360492
0.354690
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000

Recall
0.000000
0.999802
0.000000
0.000000
0.996714
0.999411
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000

F-score
0.000000
0.522596
0.000000
0.000000
0.529329
0.523402
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000

Accuracy
0.646760
0.353169
0.648868
0.648939
0.513488
0.403541
0.648728
0.648728
0.649712
0.650273
0.650273
0.646550

Precision
0.050000
0.353169
0.513572
0.513650
0.407471
0.368955
0.512229
0.512229
0.519581
0.574718
0.574718
0.490137

Recall
0.000202
1.000000
0.108945
0.109938
0.830950
0.969869
0.109938
0.109938
0.108945
0.088977
0.088977
0.037963

F-score
0.000402
0.521831
0.179563
0.180896
0.546570
0.534385
0.180803
0.180803
0.179926
0.149848
0.149848
0.064262

Accuracy
0.836797
0.824108
0.838888
0.838888
0.756344
0.487035
0.839028
0.839028
0.838958
0.835124
0.835124
0.838888

Precision
0.926826
0.921228
0.935031
0.933670
0.696359
0.403159
0.934505
0.934505
0.934527
0.918708
0.918708
0.931980

Recall
0.579682
0.544363
0.580317
0.581335
0.539417
0.963674
0.581139
0.581139
0.580919
0.580569
0.580569
0.582545

F-score
0.713186
0.684287
0.716079
0.716449
0.607770
0.568305
0.716550
0.716550
0.716388
0.711449
0.711449
0.716901

TABLE V: Classiﬁcation performance results - considering only bug report description as input

BF

TF

TF-IDF

Learners

Accuracy

Precision

Recall

F-score

Accuracy

Precision

Recall

F-score

Accuracy

Precision

Recall

F-score

XGB
SVC
Decision Tree (DT)
Random Forest (RF)
Gaussian Naive Bayes (GNB)
K-Nearest-Neighbors (KNN)
Bagged Decision Trees 1 (BDT1)
Bagged Decision Trees 2 (BDT2)
Extra Trees (ET)
AdaBoost 1 (ADA1)
AdaBoost 2 (ADA2)
Gradient Boosting (GB)

0.628535
0.480961
0.628535
0.628535
0.614021
0.431061
0.628535
0.628535
0.628535
0.628535
0.628535
0.628535

0.523802
0.435743
0.523802
0.523802
0.509875
0.413578
0.523802
0.523802
0.523802
0.523802
0.523802
0.523802

0.817072
0.995330
0.817072
0.817072
0.968319
0.996777
0.817072
0.817072
0.817072
0.817072
0.817072
0.817072

0.638091
0.605892
0.638091
0.638091
0.667838
0.584274
0.638091
0.638091
0.638091
0.638091
0.638091
0.638091

0.842338
0.388573
0.810247
0.853267
0.711738
0.810324
0.848384
0.848384
0.855360
0.824510
0.824510
0.844353

0.833315
0.388573
0.744537
0.851863
0.514806
0.707844
0.841239
0.841239
0.861863
0.810038
0.810038
0.836646

0.742255
1.000000
0.778833
0.753385
0.373145
0.871196
0.751776
0.751776
0.747174
0.716594
0.716594
0.744321

0.785039
0.559479
0.761145
0.799333
0.396211
0.780832
0.793717
0.793717
0.800260
0.759808
0.759808
0.787696

0.975530
0.923174
0.966118
0.979434
0.754181
0.872348
0.974415
0.980410
0.980410
0.976018
0.976018
0.972532

0.989473
0.976550
0.952127
0.981087
0.609551
0.769005
0.976273
0.985714
0.985714
0.979926
0.979926
0.984765

0.940135
0.799684
0.951117
0.959850
0.831535
0.929133
0.950151
0.958066
0.958066
0.950854
0.950854
0.936025

0.964129
0.879195
0.951570
0.970315
0.703242
0.838228
0.962969
0.971652
0.971652
0.965151
0.965151
0.959744

fers the best performance (between 0.79 and 0.97). Aside from
Naive Bayes (GNB), Nearest Neighbors (k-NN), and Support
Vector Classiﬁer (SVC), all other algorithms (generally tree-
based) perform similarly with high classiﬁcation F-scores.

The choice of feature set has the biggest impact on the
classiﬁcation performance of security-relevant bug reports.
TF-IDF offering the best results. Tree-based learners are
the best for security-relevant classiﬁcation, and yield sim-
ilar performance results.

Exploring variations of inputs from bug reports
Our review of the state of the art has revealed that some
approaches claim to yield the best results when focusing on
part of the bug report (such as the title). We propose to redo
all experiments combining different algorithms with different
feature engineering targets, however focusing on changing the
input content to be considered from bug reports. Table IV
shows the results yielded when using only the bug report
title to classify security-relevant bug reports. TF-IDF is still
revealed as the best method for engineering features for classi-
fying security bug reports, and can help to achieve over 0.71 F-
score with tree-based learners. These results conﬁrm the recent
ﬁndings made by Pereira et al. [11] on closed Microsoft data:
it is possible to achieve reasonably good prediction perfor-
mance by only leveraging bug report titles. The best F-score
performance (0.71), using titles, however, is 26 percentage
points lower than the best performance (0.97) obtained with
full bug report contents.

Although, in our experiments, classiﬁcation with full con-
tents of bug reports leads to the best results, bug report
titles appear to carry sufﬁcient information for yielding
reasonable classiﬁcation performance.

Similarly, we have undertaken to assess whether report titles

are redundant within a bug report. Thus, we perform the same
experiments as above considering only the bug report descrip-
tions as input. Table V summarizes the obtained performance
results. TF-IDF based feature set is further conﬁrmed as being
the most adapted. Note that in this case, however, the best
performance is only at less than 1 percentage point from the
best performance obtained with the full bug report content.
Experimental results show that bug report description
contains most of the information, which if captured with
TF-IDF based features, will lead to accurate classiﬁcation
of security relevance.

Exploring deep learned features

Table VI provides accuracy results of using LSTM approaches
with word embeddings to predict security bug reports. Training
loss and validation loss at the different epochs are sensibly
similar, allowing us to discard immediate reasons that suggest
overﬁtting or underﬁtting. The obtained accuracy is generally
high (although generally slightly lower than the ones obtained
based on engineered features in previous experiments). This
suggests that Word embedding and LSTM are a good combi-
nation to accurately detect security bug reports. Accuracy is
indeed between 87% and 90%.

TABLE VI: Classiﬁcation results with LSTM + word embeddings

Epoch

Training loss

Validation loss

Accuracy

0
6
10
16
20

0.2855
0.2905
0.2696
0.2387
0.1981

0.0000
0.2877
0.3102
0.2380
0.2040

0.8714
0.8739
0.8774
0.8853
0.9062

Results of our alternate deep learning approach that lever-
ages TF-IDF heatmaps with CNN to extract structural image
features for classiﬁcation are provided in Table VII. The
accuracy is slightly higher than the precedent LSTM-based
approach, ranging between 88% and 92%.

9

TABLE VII: Classiﬁcation results with CNN + TF-IDF heatmaps

Epoch

Training loss

Validation loss

Accuracy

0
1
2
3
4

0.219164
0.254183
0.259294
0.210394
0.157864

0.308599
0.346557
0.296789
0.300965
0.302180

0.904665
0.888438
0.926978
0.922921
0.922921

Deep representation learning supported approaches can
achieve high performance, irrespective of the type of inputs
(tokens or heatmaps) that are fed to the neural nets.

Exploring Feature vector sizes

We propose to investigate the importance of the vector-
size parameter in security bug reports identiﬁcation based on
engineered features. We experimentally focus on traditional
supervised learning algorithms studied earlier in this section
using the TF-IDF based feature set, which was shown to
provide the best results.

Figure 6 shows the evolution of F-score values as a function
of TF-IDF feature vector size for the top learners (all are tree-
based except Gaussian Naive Bayes). As inputs, we considered
the description content (the orange curve) and the full content
(“tile + description”) (blue curve).

The experimental data reveal that the feature vector size
has a direct impact on the F-score values: the performance
generally increases with the feature vector size, but appears
to cap at some point (e.g., beyond a size of 50-80,
the
performance is somewhat stabilized). We even observe that
there is a window of feature size for which the performance
can decrease again before going back to the plateau, suggesting
that the amount of noisy features must be controlled when
trying to reduce the feature vector size.

Interestingly, we note that the performance of classiﬁers that
are based on Gaussian Naive Bayes, can be severely impacted
by large feature vectors. This result may explain the limited
performance yielded in the comparative experiments by non-
tree based classiﬁers: these learners are less capable to model
security-relevance when too many features are considered.
feature-vector size: The prediction F-measure evolves
with the size of feature vectors.

VI. DISCUSSION

In this section, we ﬁrst discuss the threats to validity
involved in our work. We then summarize the take-home
messages. Finally, we discuss related studies as well as related
approaches in the real of bug report triaging.

A. Threats to Validity

Our work carries some threats to validity that we have tried

to mitigate.

In terms of threats to external validity, we note that
the dataset may be noisy. Although, we have assembled
datasets from the literature, some of them may be unreliable.
Indeed, for example, Ponta et al [32] have identiﬁed security-
relevant bug reports to build their dataset by following a
manual process. We reduced this threat by double-checking
their manual collection process. Similarly, our negative set

is collected from the literature. Unfortunately, the heuristics
of collection by authors are not always available for cross-
checking.

In terms of threats to internal validity, we note that our
selection of learning algorithms may be biased by the trends in
the literature. We have mitigated this threat by considering 11
supervised learning algorithms, different neural architectures
as well as different input formats.

We also have some threats to construct validity related
to the implementation of deep learning approaches. Hyper-
the size of most discriminant
parameters such as epochs,
words, etc. create biases in the model. To reduce this bias, we
performed extensive tests for hyper-parameter selection and
reported training and validation losses at different epochs.

B. Take-Home Messages Summary

Our empirical evaluation on our collected dataset shows

that:

• Features based on TF-IDF yield the best classiﬁcation

results.

• Tree-based classiﬁers perform generally well for the
classiﬁcation of security-relevant bug reports with TF-
IDF feature set.

• The performance of non-tree based classiﬁers can be

affected by a large and noisy feature set.

• As suggested by previous experiments on proprietary
data, we conﬁrm on large open source data that bug report
title includes information to enable prediction of security
relevance. However, most relevant information tokens are
in the description.

We also offer initial results on the use of straightforward
deep learning models for the classiﬁcation of security-relevant
bug reports. Our ambition is to allow researchers to build on
these insights to build approaches that can be integrated to
development environments.

Our experimental results suggest that, in the lab, machine
learning-based classiﬁers can accurately predict security-
relevant bug reports. The challenge now for the research
community is to experimentally demonstrate this achieve-
in the wild, by integrating and assessing such
ment
classiﬁers in a production bug triaging pipeline.

VII. CONCLUSION

Quickly addressing security bug reports is paramount. In-
deed, when users report bugs, these reports are addressed with
varying depending on the tags that developers give them. An
unnoticed security bug report may further cause damages if
they are exploited by attackers before the development teams
can handle them. In this work, we propose an overview of the
state of art in security bug reports identiﬁcation. We dissect
the most recurrent components and show the achievements
made in the literature on a large dataset. We underlined some
factors (mainly feature engineering) that can lead to the best
identiﬁcation of security bug reports. As future work, we plan
to integrate security-relevant bug reports classiﬁers in open

10

Fig. 6: Results when the feature-vector size increases

source repositories, and assess these classiﬁers in production
bug triaging pipeline.

REFERENCES

[1] J. Spolsky, “Painless bug tracking,” 2010.
[2] D. Li, L. Li, D. Kim, T. F. Bissyand´e, D. Lo, and Y. L. Traon, “Watch
out for this commit! a study of inﬂuential software changes,” Software:
Evolution and Process, 2019.

[3] S. Mostafa and X. Wang, “Automatic identiﬁcation of security bug

reports via semi-supervised learning and cve mining.”

[4] S. Mostafa, B. Findley, N. Meng, and X. Wang, “SAIS: Self-Adaptive
Identiﬁcation of Security Bug Reports,” IEEE Trans. Dependable Secur.
Comput., vol. PP, no. c, pp. 1–1, 2019.

[5] D. Wijayasekara, M. Manic, and M. McQueen, “Vulnerability identi-
ﬁcation and classiﬁcation via text mining bug databases,” in IECON
2014-40th Annual Conference of the IEEE Industrial Electronics Society.
IEEE, 2014, pp. 3612–3618.

[6] D. Wijayasekara, M. Manic, J. L. Wright, and M. McQueen, “Mining
bug databases for unidentiﬁed software vulnerabilities,” in 2012 5th
International Conference on Human System Interactions.
IEEE, 2012,
pp. 89–96.

[7] D. Behl, S. Handa, and A. Arora, “A bug Mining tool to identify and
analyze security bugs using Naive Bayes and TF-IDF: A Comparative
Analysis,” ICROIT 2014 - Proc. 2014 Int. Conf. Reliab. Optim. Inf.
Technol., pp. 294–299, 2014.

[8] F. Peters, T. T. Tun, Y. Yu, and B. Nuseibeh, “Text Filtering and Ranking
for Security Bug Report Prediction,” IEEE Trans. Softw. Eng., vol. 45,
no. 6, pp. 615–631, 2019.

[9] K. Goseva-Popstojanova and J. Tyo, “Identiﬁcation of security related
bug reports via text mining using supervised and unsupervised classiﬁ-
cation,” Proc. - 2018 IEEE 18th Int. Conf. Softw. Qual. Reliab. Secur.
QRS 2018, pp. 344–355, 2018.

[10] R. Shu, T. Xia, L. Williams, and T. Menzies, “Better security bug
report classiﬁcation via hyperparameter optimization,” arXiv preprint
arXiv:1905.06872, 2019.

[11] M. Pereira, A. Kumar, and S. Cristiansen, “Identifying security bug
reports based solely on report titles and noisy data,” in 2019 IEEE
International Conference on Smart Computing (SMARTCOMP), June
2019, pp. 39–44.

[12] M. Gegick, P. Rotella, and T. Xie, “Identifying security bug reports via
text mining: An industrial case study,” Proc. - Int. Conf. Softw. Eng.,
pp. 11–20, 2010.

[13] D. Zou, Z. Deng, Z. Li, and H. Jin, “Automatically identifying security
bug reports via multitype features analysis,” in Information Security and
Privacy, W. Susilo and G. Yang, Eds. Cham: Springer International
Publishing, 2018, pp. 619–633.

[14] D. C. Das and M. R. Rahman, “Security and performance bug reports
identiﬁcation with class-imbalance sampling and feature selection,” 2018
Jt. 7th Int. Conf. Informatics, Electron. Vis. 2nd Int. Conf. Imaging, Vis.
Pattern Recognition, ICIEV-IVPR 2018, pp. 316–321, 2019.

[15] A. Kukkar, R. Mohana, A. Nayyar, J. Kim, B.-G. Kang, and N. Chil-
amkurti, “A novel deep-learning-based bug severity classiﬁcation tech-
nique using convolutional neural networks and random forest with
boosting,” Sensors, vol. 19, no. 13, p. 2964, 2019.

[16] S. Mostafa and X. Wang, “Automatic Identiﬁcation of Security Bug
Reports via Discriminative Term Ranking and Classiﬁcation,” pp. 1–11.
[17] Z. Liu, X. Xia, A. E. Hassan, D. Lo, Z. Xing, and X. Wang, “Neural-
machine-translation-based commit message generation: how far are we?”
in Proceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering. ACM, 2018, pp. 373–384.

[18] B. Lin, F. Zampetti, G. Bavota, M. Di Penta, M. Lanza, and R. Oliveto,
“Sentiment analysis for software engineering: How far can we go?” in
2018 IEEE/ACM 40th International Conference on Software Engineer-
ing (ICSE).

IEEE, 2018, pp. 94–104.

[19] Y. Lou, J. Chen, L. Zhang, D. Hao, and L. Zhang, “History-driven
build failure ﬁxing: how far are we?” in Proceedings of the 28th ACM
SIGSOFT International Symposium on Software Testing and Analysis,
2019, pp. 43–54.

[20] T. S. Gadge and N. Mangrulkar, “Approaches for automated bug triaging:
A review,” IEEE International Conference on Innovative Mechanisms
for Industry Applications, ICIMIA 2017 - Proceedings, no. Icimia, pp.
158–161, 2017.

11

●●●●●●●●●●●●0.9000.9250.950255075100sizeF−scorecontent●●descriptiontitle_descriptionBagged_Decision Trees_2●●●●●●●●●●●●0.9000.9250.950255075100sizeF−scoreBagged_Decision_Trees_1●●●●●●●●●●●●0.900.920.940.96255075100sizeF−scorecontent●●descriptiontitle_descriptionExtra_Trees●●●●●●●●●●●●0.8750.9000.9250.950255075100sizeF−scorecontent●●descriptiontitle_descriptionAdaBoost_1●●●●●●●●●●●●0.8750.9000.9250.950255075100sizeF−scorecontent●●descriptiontitle_descriptionAdaBoost_2●●●●●●●●●●●●0.700.750.800.85255075100sizeF−scorecontent●●descriptiontitle_descriptionGaussian_Naive_Bayes●●●●●●●●●●●●0.9000.9250.950255075100sizeF−scoremachine learning algorithme: Bagged_Decision_Trees_1descriptionTitle+description●●●●●●●●●●●●0.9000.9250.950255075100sizeF−scoremachine learning algorithme: Bagged_Decision_Trees_1descriptionTitle+description●●●●●●●●●●●●0.9000.9250.950255075100sizeF−scoremachine learning algorithme: Bagged_Decision_Trees_1descriptionTitle+description●●●●●●●●●●●●0.9000.9250.950255075100sizeF−scoremachine learning algorithme: Bagged_Decision_Trees_1descriptionTitle+description●●●●●●●●●●●●0.9000.9250.950255075100sizeF−scoremachine learning algorithme: Bagged_Decision_Trees_1descriptionTitle+description●●●●●●●●●●●●0.9000.9250.950255075100sizeF−scoremachine learning algorithme: Bagged_Decision_Trees_1descriptionTitle+description[21] A. Tamrawi, T. T. Nguyen, J. Al-Kofahi, and T. N. Nguyen, “Fuzzy set-
based automatic bug triaging (NIER track),” Proceedings - International
Conference on Software Engineering, pp. 884–887, 2011.

[22] K. Kevic, S. C. Muller, T. Fritz, and H. C. Gall, “Collaborative bug
triaging using textual similarities and change set analysis,” 2013 6th
International Workshop on Cooperative and Human Aspects of Software
Engineering, CHASE 2013 - Proceedings, pp. 17–24, 2013.

[23] V. Jain, A. Rath, and S. Ramaswamy, “Field weighting for automatic bug
triaging systems,” in 2012 IEEE International Conference on Systems,
Man, and Cybernetics (SMC), Oct 2012, pp. 2845–2848.

[24] V. Dedik and B. Rossi, “Automated Bug Triaging in an Industrial
Context,” Proceedings - 42nd Euromicro Conference on Software Engi-
neering and Advanced Applications, SEAA 2016, pp. 363–367, 2016.

[25] S. Wang, W. Zhang, and Q. Wang, “FixerCache: Unsupervised caching
active developers for diverse bug triage,” International Symposium on
Empirical Software Engineering and Measurement, 2014.

[26] S. Mani, A. Sankaran, and R. Aralikatte, “Deeptriage: Exploring the
effectiveness of deep learning for bug triaging,” ACM International
Conference Proceeding Series, pp. 171–179, 2019.

[27] L. Wu, B. Xie, G. Kaiser, and R. Passonneau, “BugMiner: Software
reliability analysis via data mining of bug reports,” SEKE 2011 - Pro-
ceedings of the 23rd International Conference on Software Engineering
and Knowledge Engineering, pp. 95–100, 2011.

[28] D. Wang, H. Zhang, R. Liu, M. Lin, and W. Wu, “Predicting bugs’
components via mining bug reports,” Journal of Software, vol. 7, no. 5,
pp. 1149–1154, 2012.

[29] J. Anvik, L. Hiew, and G. C. Murphy, “Who should ﬁx this bug?” in
Proceedings of the 28th international conference on Software engineer-
ing. ACM, 2006, pp. 361–370.

[30] A. Goyal and N. Sardana, “Empirical analysis of ensemble machine
learning techniques for bug triaging,” in 2019 Twelfth International
Conference on Contemporary Computing (IC3), Aug 2019, pp. 1–6.

[31] N. Nnamoko, L. Adri, and D. Campbell, “a Hierarchical One-vs . -

Remainder Approach,” pp. 247–260, 2019.

[32] S. E. Ponta, H. Plate, A. Sabetta, M. Bezzi, and C. Dangremont,
“A manually-curated dataset of ﬁxes to vulnerabilities of open-source
the 16th International Conference on
software,” in Proceedings of
Mining Software Repositories.
IEEE Press, 2019, pp. 383–387.

[33] P. Willett, “The porter stemming algorithm: then and now,” Program,

vol. 40, no. 3, pp. 219–223, 2006.

[34] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

computation, vol. 9, no. 8, pp. 1735–1780, 1997.

[35] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convolutional neu-
ral network for modelling sentences,” arXiv preprint arXiv:1404.2188,
2014.

12

