Clustering in graphs and hypergraphs
with categorical edge labels

Ilya Amburg
Cornell University
ia244@cornell.edu

Nate Veldt
Cornell University
nveldt@cornell.edu

Austin R. Benson
Cornell University
arb@cs.cornell.edu

0
2
0
2

b
e
F
7
1

]
I
S
.
s
c
[

2
v
3
4
9
9
0
.
0
1
9
1
:
v
i
X
r
a

ABSTRACT
Modern graph or network datasets often contain rich structure that
goes beyond simple pairwise connections between nodes. This calls
for complex representations that can capture, for instance, edges of
different types as well as so-called “higher-order interactions” that
involve more than two nodes at a time. However, we have fewer rig-
orous methods that can provide insight from such representations.
Here, we develop a computational framework for the problem of
clustering hypergraphs with categorical edge labels — or different
interaction types — where clusters corresponds to groups of nodes
that frequently participate in the same type of interaction.

Our methodology is based on a combinatorial objective func-
tion that is related to correlation clustering on graphs but enables
the design of much more efficient algorithms that also seamlessly
generalize to hypergraphs. When there are only two label types,
our objective can be optimized in polynomial time, using an algo-
rithm based on minimum cuts. Minimizing our objective becomes
NP-hard with more than two label types, but we develop fast ap-
proximation algorithms based on linear programming relaxations
that have theoretical cluster quality guarantees. We demonstrate
the efficacy of our algorithms and the scope of the model through
problems in edge-label community detection, clustering with tem-
poral data, and exploratory data analysis.

1 INTRODUCTION
Representing data as a graph or network appears in numerous
application domains, including, for example, social network anal-
ysis, biological systems, the Web, and any discipline that focuses
on modeling interactions between entities [5, 25, 49]. The simple
model of nodes and edges provides a powerful and flexible abstrac-
tion, and over time, more expressive models have been developed
to incorporate richer structure in data. In one direction, models
now use more information about the nodes and edges: multilayer
networks capture nodes and edges of different types [36, 48], meta-
paths formalize heterogeneous relational structure [24, 60], and
graph convolutional networks use node features for prediction
tasks [35]. In another direction, group, higher-order, or multi-way
interactions between several nodes — as opposed to pairwise inter-
actions — are paramount to the model. In this space, interaction
data is modeled with hypergraphs [9, 66, 67], tensors [1, 7, 53],
affiliation networks [40], simplicial complexes [10, 51, 54, 56], and
motif representations [11, 55]. Designing methods that effectively
analyze the richer structure encoded by these expressive models is
an ongoing challenge in graph mining and machine learning.

In this work, we focus on the fundamental problem of clus-
tering, where the general idea is to group nodes based on some
similarity score. While graph clustering methods have a long his-
tory [26, 43, 47, 57], existing approaches for rich graph data do not

naturally handle networks with categorical edge labels. In these
settings, a categorical edge label encodes a type of discrete similar-
ity score — two nodes connected by an edge with category label c
are similar with respect to c. This structure arises in a variety of set-
tings: brain regions are connected by different types of connectivity
patterns [20]; edges in coauthorship networks are categorized by
publication venues, and copurchasing data can contain information
about the type of shopping trip. In the examples of coauthorship
and copurchasing, the interactions are also higher-order — publi-
cations can involve multiple authors and purchases can be made
up of several items. Thus, we would like a scalable approach to
clustering nodes using a similarity score based on categorical edge
labels that work well for higher-order interactions.

Here, we solve this problem with a novel clustering framework
for edge-labeled graphs. Given a network with k edge labels (cate-
gories or colors), we create k clusters of nodes, each corresponding
to one of the labels. As an objective function for cluster quality,
we seek to simultaneously minimize two quantities: (i) the num-
ber of edges that cross cluster boundaries, and (ii) the number of
intra-cluster “mistakes”, where an edge of one category is placed
inside the cluster corresponding to another category. This approach
results in a clustering of nodes that respects both the coloring in-
duced by the edge labels and the topology of the original network.
We develop this computational framework in a way that seam-
lessly generalizes to the case of hypergraphs to model higher-order
interactions, where hyperedges have categorical labels.

The style of our objective function is related to correlation clus-
tering in signed networks [8], as well as its generalization for dis-
crete labels (colors), chromatic correlation clustering [12, 13], which
are based on similar notions of mistake minimization. However, a
key difference is that our objective function does not penalize plac-
ing nodes not connected by an edge in the same cluster. This mod-
eling difference provides serious advantages in terms of tractability,
scalability, and the ability to generalize to higher-order interactions.
We first study the case of edge-labeled (edge-colored) graphs
with only two categories. We develop an algorithm that optimizes
our Categorical Edge Clustering objective function in polynomial
time by reducing the problem to a minimum s-t graph cut problem
on a related network. We then generalize this construction to facil-
itate quickly finding the optimal solution exactly for hypergraphs.
This is remarkable on two fronts. First, typical clustering objectives
such as minimum bisection, ratio cut, normalized cut, and modular-
ity are NP-hard to optimize even in the case of two clusters [17, 62].
And in correlation clustering, having two edge types is also NP-
hard [8]. In contrast, our setup admits a simple algorithm based on
minimum s-t cuts. Second, our approach seamlessly generalizes to
hypergraphs. Importantly, we do not approximate hyperedge cuts
with weighted graph cuts, which is a standard heuristic approach

 
 
 
 
 
 
in hypergraph clustering [2, 45, 67]. Instead, our objective exactly
models the number of hyperedges that cross cluster boundaries and
the number of intra-cluster “mistake” hyperedges.

k , 2 − 1
r +1

With more than two categories, we show that minimizing our
objective is NP-hard, and we proceed to construct several approxi-
mation algorithms. The first set of algorithms are based on practical
linear programming relaxations, achieving an approximation ratio
of min (cid:8)2 − 1
(cid:9), where k is the number of categories and
r is the maximum hyperedge size (r = 2 for the graph case). The
second approach uses a reduction to multiway cut, where practi-
cal algorithms have a r +1
2 approximation ratio and algorithms of
theoretical interest have a 2(1 − 1
) approximation ratio.
k
We test our methods on synthetic benchmarks as well as a variety
of real-world datasets coming from neuroscience, biomedicine, and
social and information networks; our methods work far better than
baseline approaches at minimizing our objective function. Surpris-
ingly, our linear programming relaxation often produces a rounded
solution that matches the lower bound, i.e., it exactly minimizes
our objective function. Furthermore, our algorithms are also fast in
practice, often taking under 30 seconds on large hypergraphs.

We examine an application to a variant of the community de-
tection problem where edge labels indicate that two nodes are in
the same cluster and find that our approach accurately recovers
ground truth clusters. We also show how our formulation can be
used for temporal community detection, in which one clusters the
graph based on topology and temporal consistency. In this case,
we treat binned edge timestamps as categories, and our approach
finds good clusters in terms of topological metrics and temporal
aggregation metrics. Finally, we provide a case study in exploratory
data analysis with our methods using cooking data, where a recipe’s
ingredients form a hyperedge and its edge label the cuisine type.

2 PRELIMINARIES AND RELATED WORK
Let G = (V , E, C, ℓ) be an edge-labeled (hyper)graph, where V is
a set of nodes, E is a set of (hyper)edges, C is a set of categories
(or colors), and ℓ : E → C is a function which labels every edge
with a category. Often, we just use C = {1, 2, . . . , k }, and we can
think of ℓ as a coloring of the edges. We use “category”, “color”, and
“label” interchangeably, as these terms appear in different types of
literature (e.g., “color” is common for discrete labeling in graph
theory and combinatorics). We use k = |C | to denote the number of
categories, Ec ⊆ E for the set of edges having label c, and r for the
maximum hyperedge size (i.e., order), where the size of a hyperedge
is the number of nodes it contains (in the case of graphs, r = 2).

2.1 Categorical edge clustering objective
Given G, we consider the task of assigning a category (color) to
each node in such a way that nodes in category c tend to participate
in edges with label c; in this setup, we partition the nodes into k
clusters with one category per cluster. We encode the objective
function as minimizing the number of “mistakes” in a clustering,
where a mistake is an edge that either (i) contains nodes assigned
to different clusters or (ii) is placed in a cluster corresponding to
a category which is not the same as its label. In other words, the
objective is to minimize the number of edges that are not completely
contained in the cluster corresponding to the edge’s label.

Let Y be a categorical clustering, or equivalently, a coloring of the
nodes, where Y [i] denotes the color of node i. Let mY : E → {0, 1}
be the category-mistake function, defined for an edge e ∈ E by

mY (e) =

(cid:40)1
0

if Y [i] (cid:44) ℓ(e) for any node i ∈ e,
otherwise.

(1)

Then, the Categorical Edge Label Clustering objective score for the
clustering Y is simply the number of mistakes:

CatEdgeClus(Y ) = (cid:205)

e ∈E mY (e).

(2)

This form applies equally to hypergraphs; a mistake is a hyperedge
with a node placed in a category different from the edge’s label.

Our objective can easily be modified for weighted (hyper)graphs.
If a hyperedge e has weight we , then the category mistake function
simply becomes mY (e) = we if Y [i] (cid:44) ℓ(e) for any node i in e and
is 0 otherwise. Our results easily generalize to this setting, but we
present results in the unweighted case for ease of notation.

2.2 Relation to Correlation Clustering
Our objective function is related to chromatic correlation cluster-
ing [12], in which one clusters an edge-colored graph into any
number of clusters, and a penalty is incurred for any one of three
types of mistakes: (i) an edge of color c is placed in a cluster of a
different color; (ii) an edge of any color has nodes of two different
colors; or (iii) a pair of nodes not connected by an edge is placed in-
side a cluster. This objective is a strict generalization of the classical
correlation clustering objective [8].

Our Categorical Edge Clustering objective is similar, except we
remove the penalty for placing non-adjacent nodes in the same
cluster (mistakes of type (iii)). The chromatic correlation clustering
objective treats the absence of an edge between nodes i and j as a
strong indication that these nodes should not share the same label.
We instead interpret a non-edge simply as missing information:
the absence of an edge may be an indication that i and j do not
belong together, but it may also be the case that they have a re-
lationship that simply has not been measured. This is a natural
assumption with large, sparse real-world graphs, where we rarely
have information on all pairs of entities. Another key difference
between chromatic correlation clustering and our objective is that
in the former, one may form several clusters for the same color. For
our objective, merging two separate clusters for the same color can
only improve the objective.

Our formulation also leads to several differences in computa-
tional tractability. Chromatic correlation clustering is NP-hard in
general, and there are several approximation algorithms [6, 12, 13].
The tightest of these is a 4-approximation, though the algorithm is
mostly of theoretical interest, as it involves solving an incredibly
large linear program. Moreover, the higher-order generalization
of simple correlation clustering (without colors) to hypergraphs is
more complicated to solve and approximate than standard corre-
lation clustering [28, 32, 44, 46]. We will show that our Categor-
ical Edge Clustering objective can be solved in polynomial time
for graphs and hypergraphs with two categories. The problem be-
comes NP-hard for more than two categories, but we are able to
obtain practical 2-approximation algorithms for both graphs and
hypergraphs. Our approaches are based on linear programming
relaxations that are small enough to be solved quickly in practice.
2

2.3 Additional related work
There are several methods for clustering general data points that
have categorical features [14, 29, 31], but these methods are not
designed for clustering graph data. There are also methods for clus-
tering in graphs with attributes [4, 15, 63, 68]; these focus on vertex
features and do not connect categorical features to cluster indica-
tors. Finally, there are several clustering approaches for multilayer
networks modeling edge types [23, 39, 48], but the edge types are
not meant to be indicative of a cluster type.

3 THE CASE OF TWO CATEGORIES
In this section we design algorithms to solve the Categorical Edge
Clustering problem when there are only two categories. In this
case, both the graph and hypergraph problem can be reduced to a
minimum s-t cut problem, which can be efficiently solved.

3.1 An algorithm for graphs
To solve the two-category problem on graphs, we first convert it
to an instance of a weighted minimum s-t cut problem on a graph
with no edge labels. Recall that Ec is the set of edges with category
label c. Given the edge-labeled graph G = (V , E, C, ℓ), we construct
a new graph G ′ = (V ′, E ′) as follows:

• Introduce a terminal node vc for each of the two labels c ∈ L,

so that V ′ = V ∪ Vt where Vt = {vc | c ∈ L}.

• For each label c and each (i, j) ∈ Ec , introduce edges (i, j),

(vc , i) and (vc , j), all of which have weight 1
2 .

Since there are only two categories c1 and c2, let s = vc1 be treated
as a source node and t = vc2 be treated as a sink node. The minimum
s-t cut problem in G ′ is defined by

minimize
S ⊆V

cut(S ∪ s),

(3)

where cut(T ) is the weight of edges crossing from nodes in T ⊂ V ′
to its complement set ¯T = V ′\T . This classical problem that can be
efficiently solved in polynomial time, and we have an equivalence
with the original two-category edge clustering objective.

Proposition 3.1. For any S ⊆ V , the value of cut(S ∪ s) in G ′
is equal to the value of CatEdgeClus({S, ¯S }), where S and ¯S are the
clusters for categories c1 and c2.

Proof. Let edge e = (i, j) be a “mistake” in the clustering (mY (e) =
1) and without loss of generality have color c1. If i and j are as-
signed to c2, then the half-weight edges (i, vc1 ) and (j, vc1 ) are cut.
Otherwise, exactly one of i and j is assigned to c2. Without loss of
□
generality, let it be i. Then (i, vc1 ) and (i, j) are cut.

Thus, a minimizer for the s-t cut in G ′ directly gives us a mini-
mizer for our Categorical Edge Clustering objective. We next pro-
vide a similar reduction for the case of hypergraphs.

3.2 An algorithm for hypergraphs
We now develop a method to exactly solve our objective in the
two-color case with arbitrary order-r hypergraphs, and we again
proceed by reducing to an s-t cut problem. Our approach is to
construct a subgraph for every hyperedge and paste these subgraphs
together to create a new graph G ′ = (V ′, E ′), where minimum
s-t cuts produce partitions that minimize the Categorical Edge

s

uα

v1

v2

vr −1

vr

. . . . .

. . . . .

v1

v2

vr −1

vr

uβ

t

Figure 1: Subgraphs used for the s-t cut reduction of two-
color Categorical Edge Clustering in hypergraphs. Here, α
and β are hyperedges in the original hypergraph with col-
ors c1 (orange, left) and c2 (blue, right).

Clustering objective. A similar construction has been used for a Pr
Potts model in computer vision [37], and our reduction is the first
direct application of this approach to network analysis.

We start by adding terminal nodes s = vc1 and t = vc2 (corre-
sponding to categories c1 and c2) as well as all nodes in V to V ′.
For each hyperedge e = (v1, . . . , vr ) of G, we add a node ue to V ′
and add the following directed edges to E ′ (see also Figure 1):
• If e has label c1, add (s, ue ), (ue , v1), . . . , (ue , vr ) to E ′.
• If e has label c2, add (ue , t), (v1, ue ), . . . , (vr , ue ) to E ′.
Again, the minimum s-t cut on G ′ produces a partition that also
minimizes the categorical edge clustering objective, as shown below.

Theorem 3.2. Let S∗ be the solution to the minimum cut problem.
Then the label assignment Y defined by Y [i] = c1 if i ∈ S∗ and Y [i] =
c2 if i ∈ ¯S∗ minimizes the Categorical Edge Clustering objective.

Proof. Consider a hyperedge e = (v1, . . . , vr ) with label c2. We
show that mY (e) precisely corresponds to an s-t cut on the subgraph
of G ′ induced by e (Figure 1, right). If Y [v1] = . . . = Y [vr ] = c2,
then v1, . . . , vr ∈ ¯S∗ and the cost of the minimum s-t-cut is 0 (via
placing s by itself). Now suppose at least one of Y [v1], . . . , Y [vr ]
equals c1. Without loss of generality, say that Y [v1] = c1, so v1 ∈ S∗.
If ue ∈ S∗, we cut (ue , t) and none of the edges (vi , ue ) contribute
to the cut. If ue ∈ ¯S∗, we cut (v1, ue ); and it cannot be the case that
(vi , ue ) is cut for i (cid:44) 1 (otherwise, we could have reduced the cost
of the minimum cut by placing ue ∈ S∗).

To summarize, if edge e with label c2 induces a mistake in the
clustering, then the cut contribution is 1; otherwise, it is 0. A sym-
metric argument holds if e has label c1, using the graph in Figure 1
(left). By additivity, minimizing the s-t cut in G ′ minimizes the num-
□
ber of mistakes in the Categorical Edge Clustering objective.

This procedure also works for the special case of graphs. How-
ever, G ′ has more nodes and directed edges in the more general
reduction, which can increase running time in practice.
Computational considerations. Both algorithms solve a single
minimum cut problem on a graph with O(T ) vertices and O(T )
edges, where T = (cid:205)
e ∈E |e | is the sum of hyperedge degrees (this
is bounded above by r |E|, where r is the order of the hypergraph).
2) time in the worst case [50].
In theory, this can be solved in O(T
However, practical performance is often much different than this
worst-case running time. That being said, we do find the maximum
flow formulations to often be slower than the linear programming
relaxations we develop in Section 4. We emphasize that being able to
solve the Categorical Edge Clustering objective in polynomial time
3

v

v

u

3−color gadget

u

Edge

Figure 2: Gadget used for reducing maxcut to 3-color Cat-
egorical Edge Clustering. Each gadget has new auxiliary
nodes, but u and v may be a part of many 3-color gadgets.

for two colors is itself interesting, and that the algorithms we use
for experiments in Section 5 are able to scale to large hypergraphs.
Considerations for unlabeled edges. Our formulation assumed
that all of the (hyper)edges carry a unique label. However, in some
datasets, there may be edges with no label or both labels. In these
cases, the edge’s existence still signals that its constituent nodes
should be colored the same — just not with a particular color. A nat-
ural augmentation to our objective is then to penalize this edge only
when it is not entirely contained in some cluster. Our reductions
above handle this case by simply connecting the corresponding
nodes in V ′ to both terminals instead of just one.

4 MORE THAN TWO CATEGORIES
We now move to the general formulation of Categorical Edge Clus-
tering when there can be more than two categories or labels. We
first show that optimizing the objective in this setting is NP-hard.
After, we develop approximation algorithms based on linear pro-
gramming relaxations and multiway cut problems with theoretical
guarantees on solution quality. Many of these algorithms are prac-
tical, and we use them in numerical experiments in Section 5.

4.1 NP-hardness of Categorical Edge Clustering
We now prove that the Categorical Edge Clustering objective is NP-
hard for the case of three categories. Our proof follows the structure
of the NP-hardness reduction for 3-terminal multiway cut [21], and
the reduction is from the NP-hard maximum cut (maxcut) problem.
Written as a decision problem, this problem seeks to answer if there
exists a partition of the nodes of a graph into two sets such that the
number of edges cut by the partition is at least K.

Consider an unweighted instance of maxcut on a graph G =
(V , E). To convert this into an instance of 3-color Categorical Edge
Clustering, we replace each edge (u, v) ∈ E with the 3-color gadget
in Figure 2. We will use the following lemma in our reduction.

Lemma 4.1. In any node coloring of the 3-color gadget (Figure 2),
the minimum number of edges whose color does not match both of its
nodes (i.e., number of mistakes in categorical edge clustering) is three.
This only occurs when one of {u, v} is red and the other is blue.

Proof. If v is blue and u is red, then we can achieve the min-
imum three mistakes by clustering each node in the gadget with
its horizontal neighbor in Figure 2 or alternatively by placing each
node with its vertical neighbor. If u and v are constrained to be in
the same cluster, then the optimal solution is to place all nodes in
the gadget together, which makes 4 mistakes. It is not hard to check
□
that all other color assignments yield a penalty of 4 or more.

Now let G ′ be the instance of 3-color Categorical Edge Clustering
obtained by replacing each edge (u, v) ∈ E with a 3-color gadget.

Theorem 4.2. There exists a partition of the nodes in G into two
sets with K or more cut edges if and only if there is a 3-coloring of the
nodes in G ′ that makes 4|E| − K or fewer mistakes.

Proof. Consider first a cut in G = (V , E) of size K ′ ≥ K. Let Sr
and Sb
denote the two clusters in the corresponding bipartition
of G, mapping to red and blue clusters. Consider each (u, v) ∈ E
in turn along with its 3-color gadget. If (u, v) ∈ E is cut, cluster
all nodes in its gadget with their vertical neighbor if u ∈ Sb
and
v ∈ Sr , and cluster them with their horizontal neighbor if u ∈ Sr
. Either way, this makes exactly 3 mistakes. If (u, v) is
and v ∈ Sb
not cut, then label all nodes in the gadget red if u, v ∈ Sr , or blue
if u, v ∈ Sb
, which makes exactly 4 mistakes. The total number of
mistakes in G ′ is then 3K ′ + 4(|E| − K ′) = 4|E| − K ′ ≤ 4|E| − K.

Now start with G ′ and consider a node coloring that makes
B′ ≤ B = 4|E| − K mistakes. There are |E| total 3-color gadgets in
G ′. We claim that there must be at least K of these gadgets at which
only three mistakes are made. If this were not the case, then assume
exactly H < K gadgets where 3 mistakes are made. By Lemma 4.1,
there are |E| − H gadgets where at least 4 mistakes are made, so
the total number of mistakes is B′ ≥ 3H + 4(|E| − H ) = 4|E| − H >
4|E| − K, contradicting our initial assumption. Thus, by Lemma 4.1,
there are at least K edges (u, v) ∈ E where one of {u, v} is red and
□
the other is blue, and the maximum cut in G is at least K.

Consequently, if we can minimize Categorical Edge Clustering in
polynomial time, we can solve the maximum cut decision problem
in polynomial time, and Categorical Edge Clustering is thus NP-
hard. As a natural next step, we turn to approximation algorithms.

4.2 Algorithms based on LP relaxations
We now develop approximation algorithms by relaxing an integer
linear programming (ILP) formulation of our problem. We design
the algorithms for hypergraphs, with graphs as a special case. Sup-
pose we have an edge-labeled hypergraph G = (V , E, C, ℓ) with
C = {1, . . . , k}, where Ec = {e ∈ E | ℓ[e] = c}. The Categorical
Edge Clustering objective can be written as the following ILP:

min (cid:205)

c ∈C

(cid:205)

e ∈Ec xe

s.t.

for all v ∈ V :
for all c ∈ C, e ∈ Ec :
v , xe ∈ {0, 1}
xc

= k − 1

(cid:205)k
c=1 xc
v
v ≤ xe for all v ∈ e
xc
for all c ∈ C, v ∈ V , e ∈ E.

(4)

= 1 if node v is not assigned to category c, and
In this ILP, xc
v
= 0
is zero otherwise. The first constraint in (4) ensures that xc
v
for exactly one category. The second constraint says that in any
minimizer, xe = 0 if and only if all nodes in e are colored the same
as e; otherwise, xe = 1. If we relax the binary constraints in (4):

0 ≤ xc

v ≤ 1,

0 ≤ xe ≤ 1,

then the ILP is just a linear program (LP) that can be solved in
polynomial time.

When k = 2, the constraint matrix of the LP relaxation is totally
unimodular as it corresponds to the incidence matrix of a balanced
signed graph [65]. Thus, all basic feasible solutions for the LP satisfy
4

Algorithm 1: A simple 2-approximation for Categorical Edge
Clustering based on an LP relaxation. Algorithm 2 details a
more sophisticated rounding scheme.

1 Input: Labeled hypergraph G = (V , E, C, ℓ).
2 Output: Label Y [i] for each node i ∈ V .
3 Solve the LP-relaxation of ILP (4).
4 for c ∈ C do
5

Sc ← {v ∈ V | xc
for i ∈ Sc do assign Y [i] ← c.

v < 1/2}.

6
7 end
8 Assign unlabeled nodes to an arbitrary c ∈ C.

the binary constraints of the original ILP (4), which is another proof
that the two-category problem can be solved in polynomial time.
With more than two categories, the LP solution can be fractional,
and we cannot directly determine a node assignment from the LP
solution. Nevertheless, solving the LP provides a lower bound on the
optimal solution, and we show how to round the result to produce a
clustering within a bounded factor of the lower bound. Algorithm 1
contains our rounding scheme, and the following theorem shows
that it provides a clustering within a factor of 2 from optimal.

Theorem 4.3. Algorithm 1 returns at worst a 2-approximation to

the Categorical Edge Clustering objective.

Algorithm 2: LP relaxation for Categorical Edge Clustering
with a randomized rounding scheme. Theorem 4.4 gives ap-
proximation guarantees based on t.
1 Input: Labeled hypergraph G = (V , E, C = {1, 2, . . . , k}, ℓ);

rounding parameter t ∈ [1/2, 2/3].

2 Output: Label Y [i] for each node i ∈ V .
3 Solve the LP-relaxation of ILP (4).
4 π ← uniform random permutation of {1, 2, . . . , k}.
5 for c = π1, . . . , πk do
Sc ← {v ∈ V | xc
for i ∈ Sc do Y [i] ← π (c).

7
8 end
9 Assign unlabeled nodes to an arbitrary c ∈ C.

v < t }.

6

than two colors, this would mean v ∈ Sa ∩ Sb ∩ Sc for three distinct
colors a, b, c. This leads to a violation of the first constraint in (4):

x a
v

+ xb
v

+ xc
v

+ (cid:213)

xi
v < 3t + (k − 3) ≤ 2 + (k − 3) = (k − 1).

i:i(cid:60){a,b,c }

Consider an arbitrary t ∈ (1/2, 2/3). We can bound the expected
number of mistakes made by Algorithm 2 and pay for them indi-
vidually in terms of the LP lower bound. To do this, we consider a
single hyperedge e ∈ Ec with color c and bound the probability of
making a mistake and the LP cost of this hyperedge.

(cid:205)k

+ (cid:205)

+ xb
v

c=1 xc
v

v < 1/2 and xb

Proof. First, for any v ∈ V , xc

v < 1/2 and
v < 1 + k − 2 = k − 1,

v < 1/2 for at most one category
c ∈ C in the solution. If this were not the case, there would exist
two colors a and b such that x a
= x a
v

c ′ ∈C\{a,b } xc ′
which violates the first constraint of the LP relaxation. Therefore,
each node will be assigned to at most one category. Consider any
e ∈ Ec for which all nodes are not assigned to c. This means that
there exists at least one node v ∈ e such that xc
v ≥ 1/2. Thus,
the Algorithm incurs a penalty of one for this edge, but the LP
v ≥ 1/2. Therefore, every edge
relaxation pays a penalty of xe ≥ xc
□
mistake will be accounted for within a factor of 2.

We can get better approximations in expectation with a more
sophisticated randomized rounding algorithm (Algorithm 2). In this
approach, we form sets St
based on a threshold parameter t so that
c
each node may be included in more than one set. To produce a
valid clustering, we first generate a random permutation of colors
to indicate an (arbitrary) priority of one color over another. For
any v ∈ V contained in more than one set St
, we assign v to the
c
cluster with highest priority. By carefully setting the parameter t,
this approach has better guarantees than Algorithm 1.

Theorem 4.4. If t = k/(2k − 1), Algorithm 2 returns an at worst
(2 − 1/k)-approximation for Categorical Edge Clustering in expec-
tation. And if t = (r + 1)/(2r + 1), Algorithm 2 returns an at worst
(2 − 1/(1 + r ))-approximation in expectation.

Proof. For the choices of t listed in the statement of the theorem,
t ∈ [1/2, 2/3] as long as r ≥ 2 and k ≥ 2, which is always true.
We say that color c wants node v if v ∈ Sc , but this does not
automatically mean that v will be colored as c. For any v ∈ V , there
exist at most two colors that want v. If v were wanted by more

Case 1: xe ≥ t. In this case, we are guaranteed to make a mistake
at edge e, since xe ≥ t implies there is some node v ∈ e such that
v ≥ t, and so v (cid:60) Sc . However, because the LP value at this edge
xc
is xe ≥ t, we pay for our mistake within a factor 1/t.

Case 2: xe < t. Now, color c wants every node in the hyperedge
e ∈ Ec . If no other colors want any node v ∈ e, then Algorithm 2
will not make a mistake at e, and we have no mistake to account for.
Assume then that there is some node v ∈ e and a color c ′ (cid:44) c such
that c ′ wants v. This implies that xc ′
v < t, from which we have that
v ≥ 1 − xc ′
xc

v > 1 − t (to satisfy the first inequality in (4)). Thus,
v > 1 − t .

xe ≥ xc ′

(5)

This gives a lower bound of 1 − t on the contribution of the LP
objective at edge e.

In the worst case, each v ∈ e may be wanted by a different c ′ (cid:44) c,
and the number of colors other than c that want some node in e is
bounded above by B1 = k − 1 and B2 = r . We avoid a mistake at e
if and only if c has higher priority than all of the alternative colors,
where priority is established by the random permutation π . Thus,

Pr[mistake at e | xe < t] ≤ Bi

Bi +1 = min (cid:110) r

r +1 , k −1

k

(cid:111)

.

(6)

Bi
(1−t )(Bi +1)

Recall from (5) that the LP pays xe > 1−t. Therefore, the expected
cost at a hyperedge e ∈ Ec satisfying xe < t is at most
in
expectation. Taking the worst approximation factor from Case 1 and
Case 2, Algorithm 2 will in expectation provide an approximation
(cid:111). This will be minimized when the
factor of max (cid:110) 1
t ,
approximation bounds from Cases 1 and 2 are equal, which occurs
when t = Bi +1
2Bi +1 . If Bi = k − 1, then t = k −1
2k −1 and the expected
approximation factor is 2 − 1/k. And if Bi = r , then t = r
2r +1 and
□
the expected approximation factor is 2 − 1/(r + 1).
5

Bi
(1−t )(Bi +1)

For the graph case (r = 2), this theorem implies a 5

3 -approximation

for Categorical Edge Clustering with any number of categories.
Computational considerations. The linear program has O(|E|)
variables and sparse constraints, which written as a matrix in-
equality would have O(T ) non-zeros, where T is again the sum of
hyperedge degrees. Improving the best theoretical running times
for solving linear programs is an active area of research [19, 41],
but practical performance of solving linear programs is often much
different than worst-case guarantees. In Section 5, we show that a
high-performance LP solver from Gurobi is extremely efficient in
practice, finding solutions in seconds on hypergraphs with several
categories and tens of thousands of hyperedges in tens of seconds.

4.3 Algorithms based on multiway cut
We now provide alternative approximations based on multiway cut,
similar to the reductions from Section 3. Again, we develop this
technique for general hypergraphs and graphs are a special case.
Suppose we have an edge-labeled hypergraph G = (V , E, C, ℓ).
We construct a new graph G ′ = (V ′, E ′) as follows. First, introduce a
terminal node vc for each category c ∈ C, so that V ′ = V ∪ {vc | c ∈
C}. Second, for each hyperedge e = {v1, . . . , vr } ∈ E, add a clique
on nodes v1, . . . , vr , vℓ[e] to E ′, where each edge in the clique has
weight 1/r . (Overlapping cliques are just additive on the weights.)
The multiway cut objective is the number of cut edges in any
partition of the nodes into k clusters such that each cluster con-
tains exactly one of the terminal nodes. We can associate each
cluster with a category, and any clustering Y of nodes in Categori-
cal Edge Clustering for G can be mapped to a candidate partition
for multiway cut in G ′. Let MultiwayCut(Y ) denote the value of
the multiway cut objective for the clustering Y . The next result
relates multiway cut to Categorical Edge Clustering.

Theorem 4.5. For any clustering Y ,

CatEdgeClus(Y ) ≤ MultiwayCut(Y ) ≤

r + 1

2 CatEdgeClus(Y ).

Proof. Let e = {v1, . . . , vr } with label c = ℓ[e] be a hyperedge
in G. We can show that the bounds hold when considering the
associated clique in G ′ and then apply additivity. First, if e is not a
mistake in the Categorical Edge Clustering, then no edges are cut
in the clique. If e is a mistake in the Categorical Edge Clustering,
then there are some edges cut in the associated clique. The smallest
possible contribution to the multiway cut objective occurs when all
but one node is assigned to c. Without loss of generality, consider
this to be v1, which is in r cut edges: (r − 1) corresponding to the
edges from v1 to other nodes in the hyperedge, plus one for the
edge from v1 to the terminal vc . Each of the r cut edges has weight
1/r , so the multiway cut contribution is 1.

The largest possible cut occurs when all nodes in e are colored
differently from e. In this case, the edges incident to each node in
the clique are all cut. For any one of these nodes, the sum of edge
weights incident to that node equals 1 by the same arguments as
above. This cost is incurred for each of the r nodes in the hyperedge
plus the terminal node vc , for a total weight of r + 1. Since each
□
edge is counted twice, the actual penalty is (r + 1)/2.

Computational considerations. Minimizing the multiway cut
objective is NP-hard [21], but there are many approximation algo-
rithms. Theorem 4.5 implies that any p-approximation for multiway
cut provides a p(r + 1)/2-approximation for Categorical Edge Clus-
tering. For example, the simple isolating cuts heuristic yields a
r +1
2 (2 − 2
)-approximation, and more sophisticated algorithms pro-
k
vide a r +1
2 − 1
2 ( 3
)-approximation [18]. For our experiments, we
k
use the isolating cut approach, which solves O(k) maximum flow
2|E|) edges. This
problems on a graph with O(r |E|) vertices and O(r
can be expensive in practice. We will find that the LP relaxation
performs better in terms of solution quality and running time.
A node-weighted multiway cut reduction. We also provide an
approximation based on a direct reduction to a node-weighted mul-
tiway cut (NWMC) problem that is of theoretical interest. As above,
suppose we have an edge-labeled hypergraph G = (V , E, C, ℓ). We
construct a new graph G ′ = (V ′, E ′) as follows. First, introduce a ter-
minal node vc for each category c ∈ C, so that V ′ = V ∪{vc | c ∈ C}.
Assign infinite weights to all nodes in V ′. Next, for each hyperedge
e = {v1, . . . , vr } ∈ E, add an auxiliary node ve with weight 1. Next,
append edges (ve , v1), . . . , (ve , vr ) as well as (vc , ve ) for ℓ(e) = c
to E ′. It straightforward to check that deleting ve corresponds to
making a mistake at hyperedge e. Thus an optimizer of NWMC on
G ′ is also an optimizer of Categorical Edge Clustering on G.

Solving NWMC is also NP-hard [30], and there are again well-
known approximation algorithms. The above discussion implies
any p-approximation to NWMC also provides a p-approximation for
Categorical Edge Clustering. For example, an LP-based algorithm
has a 2(1 − 1/k)-approximation [30]. This approximation is better
but the LPs are too large to be practical; however, the improvement
of a direct algorithm suggests room for better theoretical results.

4.4 Approximation through a linear objective
The Categorical Edge Clustering objective assigns a penalty of 1
regardless of the proportion of the nodes in a hyperedge which are
clustered away from hyperedge’s color. Although useful, we might
consider alternative penalties that value the extent to which each
hyperedge is satisfied in the final clustering. One natural penalty for
a hyperedge of color c is the number of nodes within that hyperedge
that are not clustered into that color. With such a “linear” mistake
function, we define the Categorical Node Clustering Objective as

CatNodeClus(Y ) = (cid:205)

e ∈E m′
Y

(e), where m′
Y

(e) = (cid:205)

i ∈e IY [i](cid:44)ℓ(e).

It turns out that this objective is optimized with a simple majority
vote algorithm that assigns a node to the majority color of all
hyperedges that conatin it.

Theorem 4.6. The majority vote algorithm yields an optimizer of

the Categorical Node Clustering (linear) objective.

Proof. Suppose node u is contained in Ji hyperedges of color
. The cost of
j(cid:44)c Jj , which is minimized for c = 1. □

i. Without loss of generality, assume J1 ≥ . . . ≥ Jk
assigning u to c is Cc = (cid:205)

In Section 5, we will see that the majority vote solution provides
a good approximation to the optimizer of the Categorical Edge
Clustering objective. The reason is that the cost of a hyperedge
6

under the linear objective is at most r while that cost under the Cat-
egorical Edge Clustering objective is just 1, which makes majority
vote an r -approximation algorithm.

Theorem 4.7. The majority vote algorithm provides an

r -approximation for Categorical Edge Clustering.

5 EXPERIMENTS
We now run four types of numerical experiments to demonstrate
our methodology. First, we show that our algorithms indeed work
well on a broad range of datasets at optimizing our objective func-
tion and discover that our LP relaxation tends be extremely effective
in practice, often finding an optimal solution (i.e., matching the
lower bound). After, we show that our approach is superior to
competing baselines in categorical community detection experi-
ments where edges are colored to signal same-community mem-
bership. Next, we show how to use timestamped edge informa-
tion as a categorical edge label, and demonstrate that our method
can find clusters that preserve temporal information better than
methods that only look at graph topology, without sacrificing per-
formance on topological metrics. Finally, we present a case study
on a network of cooking ingredients and recipes to show that
our methods can also be used for exploratory data analysis. Our
code and datasets are available at https://github.com/nveldt/
CategoricalEdgeClustering.

5.1 Analysis on Real Graphs and Hypergraphs
We first evaluate our methods on several real-world edge-labeled
graphs and hypergraphs in terms of Categorical Edge Clustering.
The purpose of these experiments is to show that our methods can
optimize the objective quickly and accurately and to demonstrate
that our methods find global categorical clustering structure better
than natural baseline algorithms. All experiments ran on a laptop
with a 2.2 GHz Intel Core i7 processor and 8 GB of RAM. We
implemented our algorithms in Julia, using Gurobi software to
solve the linear programs.
Datasets. Table 1 provides summary statistics of the datasets we
use, and we briefly describe them. Brain [20] is a graph where
nodes represent brain regions from an MRI. There are two edge
categories: one for connecting regions with high fMRI correlation
and one for connecting regions with similar activation patterns. In
the Drug Abuse Warning Network (DAWN ) [59], nodes are drugs,
hyperedges are combinations of drugs taken by a patient prior to
an emergency room visit, and edge categories indicate the patient
disposition (e.g., “sent home” or “surgery”). The MAG-10 network
is a subset of the Microsoft Academic Graph [58] where nodes are
authors, hyperedges correspond to a publication from those authors,
and there are 10 edge categories which denote the computer science
conference publication venue (e.g., “WWW” or “KDD”). If the same
set of authors published at more than one conference, we used the
most common venue as the category, discarding cases where there
is a tie. In the Cooking dataset [34], nodes are food ingredients,
hyperedges are recipes made from combining multiple ingredients,
and categories indicate cuisine (e.g., “Southern-US” or “Indian”).
Finally, the Walmart-Trips dataset is made up of products (nodes),

groups of products purchased in a single shopping trip (hyperedges),
and categories are 44 unique “trip types” classified by Walmart [33].
Algorithms. We use two algorithms that we developed in Section 4.
The first is the simple 2-approximation rounding scheme outlined
in Algorithm 1, which we refer to as LP-round (LP) (in practice, this
performs as well as the more sophisticated algorithm in Algorithm 2
and has the added benefit of being deterministic). The second is
Cat-IsoCut (IC), which runs the standard isolating cut heuristic [21]
on an instance of multiway cut derived from the Categorical Edge
Clustering problem, as outlined in Section 4.3.

The first baseline we compare against is Majority Vote (MV )
discussed in Section 4.4: node i is assigned to category c if c is the
most common edge type in which i participates. The MV result
is also the default cluster assignment for IC, since in practice this
method leaves some nodes unattached from all terminal nodes.

The other baselines are Chromatic Balls (CB) and Lazy Chromatic
Balls (LCB) — two algorithms for chromatic correlation cluster-
ing [12]. These methods repeatedly select an unclustered edge and
greedily grow a cluster around it by adding nodes that share edges
with the same label. Unlike our methods, CB and LCB distinguish
between category (color) assignment and cluster assignment: two
nodes may be colored the same but placed in different clusters. To
provide a uniform comparison among methods, we merge distinct
clusters of the same category into one larger cluster. These methods
are not designed for hypergraph clustering, but we still use them
for comparison by reducing a hypergraph to an edge-labeled graph,
where nodes i and j share an edge in category c if they appear
together in more hyperedges of category c than any other.
Results. Table 1 reports how well each algorithm solves the Cat-
egorical Edge Clustering objective. We report the approximation
guarantee (the ratio between each algorithm’s output and the LP
lower bound), as well as the edge satisfaction, which is the fraction
of hyperedges that end up inside a cluster with the correct label.
Maximizing edge satisfaction is equivalent to minimizing the num-
ber of edge label mistakes but provides an intuitive way to interpret
and analyze our results. High edge satisfaction scores imply that a
dataset is indeed characterized by large groups of objects that tend
to interact in a certain way with other members of the same group.
A low satisfaction score indicates that a single label for each node
may be insufficient to capture the intricacies of the data.

In all cases, the LP solution is integral or nearly integral, indicat-
ing that LP does an extremely good job solving the original NP-hard
objective, often finding an exactly-optimal solution. As a result, it
outperforms all other methods on all datasets. Furthermore, on
nearly all datasets, we can solve the LP within a few seconds or a
few minutes. Walmart is the exception–given the large number of
categories, the LP contains nearly 4 million variables, and far more
constraints. Other baseline algorithms can be faster, but they do
not perform as well in solving the objective.

The high edge satisfaction scores indicate that our method does
the best job identifying sets of nodes which as a group tend to
participate in one specific type of interaction. In contrast, the MV
algorithm identifies nodes that individually exhibit a certain be-
havior, but the method does not necessarily form clusters of nodes
that as a group interact in a similar way. Because our LP method
outperforms our IC approach on all datasets in terms of both speed

7

Table 1: Summary statistics of datasets — number of nodes |V |, number of (hyper)edges |E|, maximum hyperedge size r , and
number of categories k — along with Categorical Edge Clustering performance for the algorithms LP-round (LP), Majority Vote
(MV), Cat-IsoCut (IC), ChromaticBalls (CB) and LazyChromaticBalls (LCB). Performance is listed in terms of the approxima-
tion guarantee given by the LP lower bound (lower is better) and in terms of the edge satisfaction, which is the fraction of
edges that are not mistakes (higher is better; see Eq. (2)). Our LP method performs the best overall and can even find exactly (or
nearly) optimal solutions to the NP-hard objective by matching the lower bound. We also report the running times for rough
comparison, though our implementations are not optimized for efficiency. Due to its simplicity, MV is extremely fast.

Approx. Guarantee

Edge Satisfaction

Runtime (in seconds)

Dataset

Brain
MAG-10
Cooking
DAWN
Walmart-Trips

|V |
638
80198
6714
2109
88837

|E|
21180
51889
39774
87104
65898

r
2
25
65
22
25

k
2
10
20
10
44

LP MV IC

CB

LCB LP MV IC

CB

LCB LP

MV IC

1.0
1.0
1.0
1.0
1.0

1.01
1.18
1.21
1.09
1.2

1.27
1.37
1.21
1.0
1.19

1.56
1.44
1.23
1.31
1.26

1.41
1.35
1.24
1.15
1.26

0.64
0.62
0.2
0.53
0.24

0.64
0.55
0.03
0.48
0.09

0.55
0.48
0.03
0.53
0.09

0.44
0.45
0.01
0.38
0.04

0.5
0.49
0.01
0.46
0.05

1.8
51
72
13
7686

0.0
0.1
0.0
0.0
0.2

1.9
203
1223
190
68801

CB

0.4
333
4.6
0.3
493

LCB

0.8
699
6.7
0.4
1503

and accuracy, in the remaining experiments we focus only on com-
paring LP against other competing algorithms.

5.2 Categorical Edge Community Detection
Next we demonstrate the superiority of LP in detecting communi-
ties of nodes with the same node labels (i.e., categorical communi-
ties), based on labeled edges between nodes. We perform experi-
ments on synthetic edge-labeled graphs, as well as two real-world
datasets, where we reveal edge labels indicative of the ground truth
node labels and see how well we can recover the node labels.
Synthetic Model. We use the synthetic random graph model of
Bonchi et al. for chromatic correlation clustering [12]. A user speci-
fies the number of nodes n, colors L, and clusters K, as well as edge
parameters p, q, and w. The model first assigns nodes to clusters
uniformly at random, and then assigns clusters to colors uniformly
at random. (Due to the random assignment, some clusters and col-
ors may not be sampled. Thus, K and L are upper bounds on the
number of distinct clusters and unique colors.) For nodes i and j
in the same cluster, the model connects them with an edge with
probability p. With probability 1 − w, the edge is the same color
as i and j. Otherwise, it is a uniform random color. If i and j are in
different clusters, an edge is drawn with probability q and given
a uniform random color. We will also use a generalization of this
model to synthetic r -uniform hypergraphs. The difference is that
we assign colored hyperedges to r -tuples of the n nodes, rather
than just pairs, and we assign each cluster to a unique color.
Synthetic Graph Results. We set up two experiments, where
performance is measured by the fraction of nodes placed in the
correct cluster (node label accuracy). In the first, we form graphs
with n = 1000, p = 0.05, and q = 0.01, fixing L = K = 15 (which
in practice leads to graphs with 15 clusters and typically between
8 and 12 distinct edge and cluster colors). We then vary the noise
parameter w from 0 to 0.75 in increments of 0.05. Figure 3a reports
the median accuracy over 5 trials of each method for each value of
w. In the second, we fix w = 0.2, and vary the number of clusters
K from 5 to 50 in increments of 5 with L = K. Figure 3b reports the
median accuracy over 5 trials for each value of K.

For our first two experiments, we additionally found that our
LP algorithm similarly outperformed other methods in terms of

(a) Graphs: Varying Noise w

(b) Graphs: Varying # Clusters

(c) Hypergraphs: Varying Noise w
(d) Hypergraphs: Varying # Clusters
Figure 3: (a)–(b): Performance of algorithms on a syn-
thetic graph model for chromatic correlation clustering [12].
Across a range of parameters, our LP method outperforms
competing methods in predicting the ground truth label of
the nodes. (c)–(d): In experiments on synthetic 3-uniform
hypergraphs, LP performs well for most parameter regimes
but there is some sensitivity to the very noisy setting.

cluster identification scores such as Adjusted Rand Index and F-
score, followed in performance by MV. Cluster identification scores
for LCB and CB were particularly low (ARI scores always below
0.02), as these methods tended to form far too many clusters.

The CB and LCB algorithms, as well as the synthetic graph
model itself, explicitly distinguish between ground truth node labels
and ground truth clusters. Thus, our third experiment explores a
parameter regime tailored more towards the strengths of CB and
LCB. We fix L = 20, and vary the number of clusters from K = 50 to
K = 200 in increments of 25. Following the experiments of Bonchi
et al. [12] we set p = w = 0.5, and set q = 0.03. Even in this setting,

8

0.00.20.40.60.000.250.500.751.00w = Pr(in-cluster edge is wrong)Node Label AccuracyLPMVLCBCB10203040500.000.250.500.751.00K = Number of ClustersNode Label AccuracyLPMVLCBCB0.00.20.40.60.000.250.500.751.00w = Pr(in-cluster edge is wrong)Node Label AccuracyLPMVLCBCB10203040500.000.250.500.751.00K = Number of ClustersNode Label AccuracyLPMVLCBCB(a) Label Accuracy

(b) ARI Scores

Figure 4: LCB and CB are primarily designed for settings
where K is much larger than L. Despite this, our LP method
always obtains better label assignment scores, and often ob-
tains better ARI cluster identification scores, when we fix
L = 20 and let K vary from 50 to 500.

we find that our algorithms maintain an advantage. For all values
of K, our LP algorithm outperforms other methods in terms of node
label accuracy, and also obtains higher ARI scores when K is a small
multiple of L. We note that LCB and CB only obtain better cluster
identification scores in parameter regimes where all algorithms
obtain ARI scores below 0.1.
Synthetic Hypergraph Results. We ran similar experiments on
synthetic 3-uniform hypergraphs. We again set n = 1000 and used
p = 0.005 and q = 0.0001 for intra-cluster and inter-cluster hyper-
edge probabilities. In one experiment, we fixed L = 15 and varied w,
and in another we fixed w = 0.2 and varied the number of clusters
L. Figures 3c and 3d shows the accuracies. Again, LP tends to have
the best performance. When L = 15, our method achieves nearly
perfect accuracy for w ≤ 0.6. However, we observe performance
sensitivity when the noise is too large: when w increases from 0.6
to 0.65, the output of LP no longer tracks the ground truth cluster
assignment. This occurs despite the fact that the LP solution is
integral, and we are in fact optimally solving the Categorical Edge
Clustering objective. We conjecture this sharp change in accuracy
is due to an information theoretic detectability threshold, which
depends on parameters of the synthetic model.
Academic Department Labels in an Email Network. To test
the algorithms on real-world data, we use the Email-Eu-core net-
work [42, 64]. Nodes in the graph represent researchers at a Euro-
pean institution, edges indicate email correspondence (we consider
the edges as undirected), and nodes are labeled by the departmental
affiliation of each researcher. We wish to test how well each method
can identify node labels, if we assume we have access to a (perhaps
noisy and imperfect) mechanism for associating emails with la-
bels for inter-department and intra-department communication. To
model such a mechanism, we generate edge categories in a manner
similar to the synthetic above. An edge inside of a cluster (i.e., an
email within the same department) is given the correct department
label with probability 1 − w, and a random label with probability w.
An edge between two members of different departments is given
a uniform random label. Figure 5a reports each algorithm’s abil-
ity to detect department labels when w varies from 0 to 0.75. Our
LP method returns the best results in all cases, and is robust in
detecting department labels even in the high-noise regime.

(a) Email-Eu-core

(b) Walmart-Products

Figure 5: Accuracy in clustering nodes in real-world datasets
when edge labels are a noisy signal for ground truth node
cluster membership. For both an email graph (a) and a prod-
uct co-purchasing hypergraph (b), our LP-Round method
consistently outperforms other methods.

Product Categories. The Walmart-Trips dataset from Section 5.1
also has product information. We assigned products to one of ten
broad departments in which they appear on walmart.com (e.g.,
“Clothing, Shoes, and Accessories”) to construct a Walmart-Products
hypergraph with ground truth node labels. Recall that hyperedges
are sets of co-purchased products. We generate noisy hyperedge
labels as before, with 1 −w as the probability that a hyperedge with
nodes from a single department will have the correct label. Results
are reported in Figure 5b, and our LP-round method can detect true
departments at a much higher rate than the other methods.

5.3 Temporal Community Detection
In the next experiment, we show how our framework can be used
to identify communities of nodes in a temporal network, where
we use timestamps on edges as a type of categorical label that two
nodes should be clustered together. For data, we use the CollegeMsg
network [52], which records private messages (time-stamped edges)
between 1899 users (nodes) of a social media platform at UC-Irvine.
Removing timestamps and applying a standard graph clustering
algorithm would be a standard approach to identify communities
of users. However, this loses the explicit relationship with time. As
an alternative, we convert timestamps into discrete edge labels by
ordering edges with respect to time and separating them into k
equal-sized bins representing time windows. Optimizing Categori-
cal Edge Clustering then corresponds to clustering users into time
windows, in order to maximize the number of private messages
that occur between users in the same time window. In this way, our
framework can identify temporal communities in a social network,
i.e., groups of users that are highly active in sending each other
messages within a short period of time.

We construct edge-labeled graphs for different values of k, and
compare LP against clusterings obtained by discarding time stamps
and running Graclus [22], a standard graph clustering algorithm.
Graclus seeks to cluster the nodes into k disjoint clusters S1, . . . , Sk
to minimize the normalized cut objective:

Ncut(S1, S2, . . . , Sk ) = (cid:205)k

i=1

cut(Si )
vol(Si ) ,

where cut(S) is the number of edges leaving S, and vol(S) is the
volume of S, i.e., the number of edge end points in S. Figure 6a shows
that LP is in fact competitive with Graclus in finding clusterings
9

60901201501800.20.40.60.81.0K = Number of ClustersNode Label AccuracyLPMVLCBCB60901201501800.00.10.20.30.4K = Number of ClustersARILPMVLCBCB0.00.20.40.60.000.250.500.751.00w = Pr(edge label is wrong)Node Label AccuracyLPMVLCBCB0.00.10.20.30.40.50.150.200.250.300.350.40w = Pr(edge label is wrong)Node Label AccuracyLPMVLCBCB(a) Normalized Cut

(b) Inner edge time difference

(a) Edge Satisfaction

(b) Unused ingredients

Figure 6: Results for LP and Graclus in clustering a tempo-
ral network. Our LP method is competitive for Graclus’s ob-
jective (normalized cut; left), while preserving the temporal
structure of network much better (right).

Figure 7: As β increases, we discard fewer high-degree in-
gredients before clustering the rest. Our method always
“makes” more recipes (higher edge satisfaction) and “wastes”
fewer ingredients (smaller number of unused ingredients).

with small normalized cut scores, even though LP is designed for a
different objective. However, LP still avoids cutting edges, and it
finds clusterings that also have small normalized cut values. The
other goal of LP is to place few edges in a cluster with the wrong
label, which in this scenario corresponds to clustering messages
together if they were sent close in time. We therefore also measure
the average difference between timestamps of interior edges and
the average time stamp in each clustering, i.e.,

(cid:205)

(cid:205)k

i=1

e ∈Ei

|timestamp(e) − µi |,

AvgTimeDiff(S1, . . . , Sk ) = 1
|Eint |
where Eint is the set of interior edges completely contained in some
cluster, Ei is the set of interior edges of cluster Si , and µi is the
average time stamp in Ei . Not surprisingly, this value tends to be
large for Graclus, since this method ignores timestamps. However,
Figure 6b shows that this value tends to be small for LP, indicting
that it is indeed detecting clusters of users that are highly interactive
within a specific short period of time.

5.4 Analysis of the Cooking Hypergraph
Finally, we apply our framework and LP-round algorithm to gain
insights into the Cooking hypergraph dataset from Section 5.1,
demonstrating our methodology for exploratory data analysis. An
edge in this hypergraph is a set of ingredients for a recipe, and each
recipe is categorized according to cuisine. Categorical Edge Clus-
tering thus corresponds to separating ingredients among cuisines,
in a way that maximizes the number of recipes whose ingredients
are all in the same cluster (see Ahn et al. [3] for related analyses).
Table 1 shows that only 20% of the recipes can be made (i.e., a
0.2 edge satisfaction) after partitioning ingredients among cuisine
types. This is due to the large number of common ingredients such
as salt and olive oil that are shared across many cuisines (a problem
in other recipe network analyses [61]). We negate the negative
effect of high-degree nodes as follows. For an ingredient i, let dc
be
i
the number of recipes of cuisine c containing i. Let Mi = maxc dc
i
measure majority degree and Ti = (cid:205)
the total degree. Note that
Bi = Ti −Mi is a lower bound on the number of hyperedge mistakes
we will make at edges incident to node i. We can refine the original
dataset by removing all nodes with Bi greater than some β.
Making recipes or wasting ingredients. Figure 7a shows edge
satisfaction scores for LP and MV when we cluster for different β.
When β = 10, edge satisfaction is above 0.64 with LP. As β increases,

t dc
i

Table 2: Examples of ingredients and recipes from special
clusters identified by LP, but not Majority Vote.
French Fruit-Based Desserts (β = 70)
Ingredients: ruby red grapefruit, strawberry ice cream, dry hard
cider, icing, prunes, tangerine juice, sour cherries.
Recipes: 1. {almond extract, bittersweet chocolate, sugar, sour cher-
ries, brioche, heavy cream, unsalted butter, kirsch}, 2. {large egg
yolks, ruby red grapefruit, dessert wine, sugar}

Brazilian Caipirinha Recipes (β = 170)
Ingredients: simple syrup, light rum, ice, superfine sugar, key lime,
coco, kumquats, liquor, mango nectar, vanilla essence
Recipes: {cachaca, ice} + 1. { lime juice, kumquats, sugar}, 2. {lime,
fruit puree, simple syrup}, 3. { superfine sugar, lime juice, passion
fruit juice}, 4. { sugar, liquor, mango nectar, lime, mango}

edge satisfaction decreases, but LP outperforms MV in all cases. We
also consider a measure of “ingredient waste” for each method. An
ingredient is unused if we cannot make any recipes by combining
the ingredient with other ingredients in its cluster. A low number
of unused ingredients indicates that a method forms clusters where
ingredients combine together well. Figure 7b shows the number of
unused ingredients as β varies. Again, LP outperforms MV.
Specific ingredient and recipe clusters. We finally highlight
specific ingredient clusters that LP identifies but MV does not.
When β = 170, LP places 10 ingredients with the Brazilian cuisine
which MV does not, leading to 23 extra recipes that are unique to
LP. Of these, 21 correspond to variants of the Caipirinha, a popular
Brazilian cocktail. When β = 70, 24 ingredients and 24 recipes are
unique to the French cuisine cluster of LP. Of these, 18 correspond
to desserts, and 14 have a significant fruit component. Table 2 has
examples of ingredients and recipes from both these clusters.

6 DISCUSSION
We have developed a computational framework for clustering nodes
of hypergraphs when edges have categorical labels that signal node
similarity. With two categories, our clustering objective can be
solved in polynomial time. For general problems, our linear pro-
gramming relaxations provide 2-approximation or even better guar-
antees, which are far tighter than what is seen in the related lit-
erature on correlation clustering. This method is also extremely
10

0102030400.650.700.750.800.850.90k = number of clusters1/k *(Norm. Cut) LP-roundGraclus10203040100200300400500k = number of clustersAvg Time Difference (hrs)LP-roundGraclus501001502000.30.40.50.6b = Ingredient Degree ThresholdEdge SatisfactionLP-roundM.Vote5010015020010001500200025003000b = Ingredient Degree Threshold# Unused IngredientsLP-roundM. Voteeffective in practice. Amazingly, our LP-round algorithm often ac-
tually minimizes our NP-hard objective (certified through integral
solutions) on hypergraphs with tens of thousands of edges in just
tens of seconds. The approach also works well in problems when
performance is measured in terms of some sort of ground truth
labeling, outperforming baselines by a substantial margin.

For the special cases of two-category graphs and rank-3 hy-
pergraphs, the Categorical Edge Clustering objective is a “regular
energy function” within the energy minimization framework of
computer vision [38]. This provides alternative polynomial time
algorithms in these cases (see Appendix A). However, these ap-
proaches do not work for two important regimes: more than two
categories, or in general hypergraphs (in the latter, the penalties
are no longer a semi-metric, which is needed for approximation
algorithms [16]).

ACKNOWLEDGMENTS
This research was supported by NSF Award DMS-1830274, ARO
Award W911NF19-1-0057, and ARO MURI.

REFERENCES
[1] Evrim Acar, Daniel M. Dunlavy, and Tamara G. Kolda. Link prediction on evolving
data using matrix and tensor factorizations. In 2009 IEEE International Conference
on Data Mining Workshops, pages 262–269. IEEE, 2009.

[2] Sameer Agarwal, Kristin Branson, and Serge Belongie. Higher order learning
In Proceedings of the 23rd International Conference on Machine

with graphs.
Learning - ICML '06. ACM Press, 2006.

[3] Yong-Yeol Ahn, Sebastian E. Ahnert, James P. Bagrow, and Albert-László Barabási.
Flavor network and the principles of food pairing. Scientific Reports, 1(1), Decem-
ber 2011.

[4] Leman Akoglu, Hanghang Tong, Brendan Meeder, and Christos Faloutsos. PICS:
Parameter-free identification of cohesive subgroups in large attributed graphs.
In Proceedings of the 2012 SIAM International Conference on Data Mining. Society
for Industrial and Applied Mathematics, 2012.

[5] Réka Albert and Albert-László Barabási. Statistical mechanics of complex net-

works. Reviews of Modern physics, 74(1):47, 2002.

[6] Yael Anava, Noa Avigdor-Elgrabli, and Iftah Gamzu. Improved theoretical and
In Proceedings of
practical guarantees for chromatic correlation clustering.
the 24th International Conference on World Wide Web, WWW ’15, pages 55–
65, Republic and Canton of Geneva, Switzerland, 2015. International World Wide
Web Conferences Steering Committee.

[7] Francesca Arrigo, Desmond J Higham, and Francesco Tudisco. A framework for
second order eigenvector centralities and clustering coefficients. arXiv:1910.12711,
2019.

[8] Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine

Learning, 56:89–113, 2004.

[9] Austin R Benson. Three hypergraph eigenvector centralities. SIAM Journal on

Mathematics of Data Science, 1(2):293–312, 2019.

[10] Austin R. Benson, Rediet Abebe, Michael T. Schaub, Ali Jadbabaie, and Jon Klein-
berg. Simplicial closure and higher-order link prediction. Proceedings of the
National Academy of Sciences, 115(48):E11221–E11230, 2018.

[11] Austin R. Benson, David F. Gleich, and Jure Leskovec. Higher-order organization

of complex networks. Science, 353(6295):163–166, 2016.

[12] Francesco Bonchi, Aristides Gionis, Francesco Gullo, Charalampos E. Tsourakakis,
and Antti Ukkonen. Chromatic correlation clustering. ACM Trans. Knowl. Discov.
Data, 9(4):34:1–34:24, June 2015.

[13] Francesco Bonchi, Aristides Gionis, Francesco Gullo, and Antti Ukkonen. Chro-
matic correlation clustering. In Proceedings of the 18th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, KDD ’12, pages 1321–1329,
New York, NY, USA, 2012. ACM.

[14] Shyam Boriah, Varun Chandola, and Vipin Kumar. Similarity measures for
In Proceedings of the 2008 SIAM

categorical data: A comparative evaluation.
International Conference on Data Mining, pages 243–254. SIAM, 2008.

[15] Cecile Bothorel, Juan David Cruz, Mateo Magnani, and Barbora Micenková.
Clustering attributed graphs: Models, measures and methods. Network Science,
3(3):408–444, March 2015.

[16] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization
via graph cuts. IEEE Transactions on Pattern Analysis and Machine Intelligence,
23(11):1222–1239, Nov 2001.

[17] Ulrik Brandes, Daniel Delling, Marco Gaertler, Robert Gorke, Martin Hoefer, Zo-
ran Nikoloski, and Dorothea Wagner. On modularity clustering. IEEE Transactions
on Knowledge and Data Engineering, 20(2):172–188, 2007.

[18] Gruia Calinescu, Howard Karloff, and Yuval Rabani. An improved approximation
algorithm for multiway cut. Journal of Computer and System Sciences, 60(3):564 –
574, 2000.

[19] Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the
current matrix multiplication time. In Proceedings of the 51st Annual ACM SIGACT
Symposium on Theory of Computing, pages 938–942. ACM, 2019.

[20] Nicolas A. Crossley, Andrea Mechelli, Petra E. Vértes, Toby T. Winton-Brown,
Ameera X. Patel, Cedric E. Ginestet, Philip McGuire, and Edward T. Bullmore. Cog-
nitive relevance of the community structure of the human brain functional coac-
tivation network. Proceedings of the National Academy of Sciences, 110(28):11583–
11588, 2013.

[21] E. Dahlhaus, D. S. Johnson, C. H. Papadimitriou, P. D. Seymour, and M. Yannakakis.
The complexity of multiterminal cuts. SIAM Journal on Computing, 23:864–894,
1994.

[22] Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis. Weighted graph cuts without
eigenvectors a multilevel approach. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 29(11):1944–1957, 2007.

[23] Manlio De Domenico, Vincenzo Nicosia, Alexandre Arenas, and Vito Latora.
Structural reducibility of multilayer networks. Nature Communications, 6(1),
2015.

[24] Yuxiao Dong, Nitesh V. Chawla, and Ananthram Swami. metapath2vec: Scalable
representation learning for heterogeneous networks. In Proceedings of the 23rd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
ACM Press, 2017.

[25] David Easley, Jon Kleinberg, et al. Networks, Crowds, and Markets. Cambridge

Books, 2012.

[26] Santo Fortunato. Community detection in graphs. Physics Reports, 486(3-5):75–

174, 2010.

[27] Daniel Freedman and Petros Drineas. Energy minimization via graph cuts:
Settling what is possible. In 2005 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (CVPR’05), volume 2, pages 939–946. IEEE, 2005.
[28] Takuro Fukunaga. LP-based pivoting algorithm for higher-order correlation clus-
tering. In Lusheng Wang and Daming Zhu, editors, Computing and Combinatorics,
pages 51–62, Cham, 2018. Springer International Publishing.

[29] Venkatesh Ganti, Johannes Gehrke, and Raghu Ramakrishnan. Cactus-clustering
categorical data using summaries. In KDD, volume 99, pages 73–83, 1999.
[30] Naveen Garg, Vijay V Vazirani, and Mihalis Yannakakis. Multiway cuts in node

weighted graphs. Journal of Algorithms, 50(1):49–61, 2004.

[31] David Gibson, Jon Kleinberg, and Prabhakar Raghavan. Clustering categori-
cal data: An approach based on dynamical systems. The VLDB Journal—The
International Journal on Very Large Data Bases, 8(3-4):222–236, 2000.

[32] David F. Gleich, Nate Veldt, and Anthony Wirth. Correlation Clustering General-
ized. In 29th International Symposium on Algorithms and Computation, volume
123 of ISAAC 2018, pages 44:1–44:13, Dagstuhl, Germany, 2018. Schloss Dagstuhl–
Leibniz-Zentrum fuer Informatik.

[33] Kaggle. Walmart recruiting: Trip type classification, 2015. https://www.kaggle.

com/c/walmart-recruiting-trip-type-classification.

[34] Kaggle. What’s cooking?, 2015. https://www.kaggle.com/c/whats-cooking.
[35] Thomas N Kipf and Max Welling. Semi-supervised classification with graph
convolutional networks. In Proceedings of the International Conference on Learning
Representations, 2017.

[36] Mikko Kivelä, Alex Arenas, Marc Barthelemy, James P. Gleeson, Yamir Moreno,
and Mason A. Porter. Multilayer networks. Journal of Complex Networks, 2(3):203–
271, 2014.

[37] Pushmeet Kohli. Minimizing Dynamic and Higher Order Energy Functions using

Graph Cuts. PhD thesis, Oxford Brookes University, 2007.

[38] V. Kolmogorov and R. Zabih. What energy functions can be minimized via graph
cuts? IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):147–
159, Feb 2004.

[39] Andrea Lancichinetti and Santo Fortunato. Consensus clustering in complex

networks. Scientific Reports, 2(1), March 2012.

[40] Silvio Lattanzi and D Sivakumar. Affiliation networks. In Proceedings of the 41st
Annual ACM Symposium on Theory of Computing, pages 427–434. Citeseer, 2009.
[41] Yin Tat Lee and Aaron Sidford. Efficient inverse maintenance and faster al-
In 2015 IEEE 56th Annual Symposium on

gorithms for linear programming.
Foundations of Computer Science, pages 230–249. IEEE, 2015.

[42] Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graph evolution: Densifi-
cation and shrinking diameters. ACM Transactions on Knowledge Discovery from
Data (TKDD), 1(1):2, 2007.

[43] Jure Leskovec, Kevin J. Lang, and Michael Mahoney. Empirical comparison
In Proceedings of the 19th

of algorithms for network community detection.
International Conference on World Wide Web - WWW '10. ACM Press, 2010.
[44] P. Li, H. Dau, G. Puleo, and O. Milenkovic. Motif clustering and overlapping
clustering for social network analysis. In IEEE INFOCOM 2017 - IEEE Conference
on Computer Communications, pages 1–9, May 2017.

11

[45] Pan Li and Olgica Milenkovic.

Inhomogeneous hypergraph clustering with
applications. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett, editors, Advances in Neural Information Processing
Systems 30, pages 2308–2318. Curran Associates, Inc., 2017.

[46] Pan Li, Gregory J. Puleo, and Olgica Milenkovic. Motif and hypergraph correlation

clustering. CoRR, abs/1811.02089, 2018.

[47] Cristopher Moore. The computer science and physics of community detection:

Landscapes, phase transitions, and hardness. Bulletin of the EATCS, 121, 2017.

[48] Peter J. Mucha, Thomas Richardson, Kevin Macon, Mason A. Porter, and Jukka-
Pekka Onnela. Community structure in time-dependent, multiscale, and multiplex
networks. Science, 328(5980):876–878, 2010.

[49] Mark EJ Newman. The structure and function of complex networks. SIAM Review,

45(2):167–256, 2003.

[50] James B Orlin. Max flows in o(nm) time, or better. In Proceedings of the forty-fifth
annual ACM symposium on Theory of computing, pages 765–774. ACM, 2013.

[51] Braxton Osting, Sourabh Palande, and Bei Wang. Spectral sparsification of
simplicial complexes for clustering and label propagation. arXiv:1708.08436, 2017.
[52] Pietro Panzarasa, Tore Opsahl, and Kathleen M Carley. Patterns and dynamics
of users’ behavior and interaction: Network analysis of an online community.
Journal of the American Society for Information Science and Technology, 60(5):911–
932, 2009.

[53] Evangelos Papalexakis, Konstantinos Pelechrinis, and Christos Faloutsos. Spot-
ting misbehaviors in location-based social networks using tensors. In Proceedings
of the 23rd International Conference on World Wide Web, pages 551–552. ACM,
2014.

[54] Mason A Porter. Nonlinearity+ networks: A 2020 vision. arXiv:1911.03805, 2019.
[55] Ryan A Rossi, Nesreen K Ahmed, and Eunyee Koh. Higher-order network
representation learning. In Companion Proceedings of the The Web Conference
2018, pages 3–4, 2018.

[56] Vsevolod Salnikov, Daniele Cassese, and Renaud Lambiotte. Simplicial complexes

and complex systems. European Journal of Physics, 40(1):014001, 2018.

[57] Satu Elisa Schaeffer. Graph clustering. Computer Science Review, 1(1):27–64, 2007.
[58] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June (Paul)
Hsu, and Kuansan Wang. An overview of microsoft academic service (MAS) and
applications. In WWW '15 Companion. ACM, 2015.

[59] Substance Abuse and Mental Health Services Administration. Drug Abuse
https://www.samhsa.gov/data/

Warning Network (DAWN), 2011.
data-we-collect/dawn-drug-abuse-warning-network.

[60] Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and Tianyi Wu. Pathsim:
Meta path-based top-k similarity search in heterogeneous information networks.
Proceedings of the VLDB Endowment, 4(11):992–1003, 2011.

[61] Chun-Yuen Teng, Yu-Ru Lin, and Lada A. Adamic. Recipe recommendation
using ingredient networks. In Proceedings of the 3rd Annual ACM Web Science
Conference on - WebSci '12. ACM Press, 2012.

[62] Dorothea Wagner and Frank Wagner. Between min cut and graph bisection. In
International Symposium on Mathematical Foundations of Computer Science, pages
744–750. Springer, 1993.

[63] Zhiqiang Xu, Yiping Ke, Yi Wang, Hong Cheng, and James Cheng. A model-based
approach to attributed graph clustering. In Proceedings of the 2012 International
Conference on Management of Data. ACM Press, 2012.

[64] Hao Yin, Austin R. Benson, Jure Leskovec, and David F. Gleich. Local higher-
order graph clustering. In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pages 555–564. ACM, 2017.
[65] Thomas Zaslavsky. Signed graphs. Discrete Applied Mathematics, 4(1):47 – 74,

1982.

[66] Justine Zhang, Cristian Danescu-Niculescu-Mizil, Christina Sauper, and Sean J.
Taylor. Characterizing online public discussions through patterns of participant
interactions. Proceedings of the ACM on Human-Computer Interaction, 2(CSCW):1–
27, 2018.

[67] Dengyong Zhou, Jiayuan Huang, and Bernhard Schölkopf. Learning with hy-
In Advances in Neural

pergraphs: Clustering, classification, and embedding.
Information Processing Systems, pages 1601–1608, 2007.

[68] Yang Zhou, Hong Cheng, and Jeffrey Xu Yu. Graph clustering based on struc-
tural/attribute similarities. Proceedings of the VLDB Endowment, 2(1):718–729,
2009.

A CONNECTION TO ENERGY MINIMIZATION
Special cases of our Categorical Edge Clustering framework fit
within the paradigm of energy function minimization in computer
vision [16, 27, 38]. The energy minimization approach uses mini-
mum s-t cut algorithms for functions of binary and ternary vari-
ables which satisfy a certain regularity property. In this appendix
we show that our objective induces a regular energy function in
both the graph and rank-3 hypergraph case when there are two

categories. This connection implies that in addition to the algo-
rithms we developed above, we may use the tools developed for
energy minimization to facilitate solving these special instances of
our problem exactly in polynomial time.

A.1 Graphs with two categories
To show the connection to energy minimization, we cast our ob-
jective as a so-called energy function. With two categories, we can
encode our coloring C of the n nodes in the graph as a vector of
1s and 2s corresponding to which color each node takes. For this,
we write C = (x1, ...., xn ) where xi = 1 if node i is assigned color 1
and xi = 2 if node i is assigned a color 2. Now the Categorical Edge
Clustering objective can be written as an energy function as

CatEdgeClus(C) = (cid:213)

i <j s.t. (i, j)∈E

Ei, j (xi , xj ),

where

(cid:20)Ei, j (1, 1)
Ei, j (2, 1)

(cid:21)

Ei, j (1, 2)
Ei, j (2, 2)

=

(cid:20)0
1

(cid:21)

1
1

if (i, j) is of color 1 and

(cid:21)

(cid:20)Ei, j (1, 1)
Ei, j (2, 1)

Ei, j (1, 2)
Ei, j (2, 2)
if (i, j) is of color 2. We will show that this energy function satisfies
a regularity property, which enables a reduction of our objective to
a minimum s-t graph cut [38].

(cid:20)1
1

1
0

=

(cid:21)

Definition A.1. A function of two binary variables is regular if

each term satisfies the following inequality

Ei, j (0, 0) + Ei, j (1, 1) ≤ Ei, j (0, 1) + Ei, j (1, 0).

It is easy to see that our energy function is indeed regular. We

formalize this observation in the following theorem.

Theorem A.2. The Categorical Edge Clustering objective for graphs

with two categories induces a regular energy function.

Proof. Regardless of whether (i, j) is an edge of color 1 or of
color 2, the off-diagonal terms in the energy function sum to 2
while the diagonal terms sum to 1. This ensures that the regularity
□
property is satisfied.

Having established the regularity of our energy function, the
results of Kolmogorov and Zabih [38, Theorem 4.1] say that we
can cast the energy minimization problem as an s-t-cut problem
on a directed graph. In particular, following their construction, we
would create a directed graph G ′ = (V ′, E ′) from G = (V , E, C, ℓ)
as follows, which is similar to the reduction we used in Section 3.1.

• Append nodes s and t to E ′
• For every undirected edge (i, j) with i < j in G, if (i, j) has
color 1, create a directed edge (i, j) and a directed edge (j, t)
in E ′, while if (i, j) has color 2 in G, append the directed edge
(i, j) and the directed edge (s, i) to E ′

This construction guarantees that the energy function Ei, j of every
edge (i, j) in G is exactly represented by the corresponding cut on
the subgraph in G ′ which the edge induced. The following theorem
is then a result of the additivity theorem from Kolmogorov and
Zabih [38].

12

Theorem A.3. Let C∗ be a two-colored clustering that is the so-
lution of the s-t-mincut problem on the graph G ′ constructed using
the procedure above. Then C∗ also optimizes the Categorical Edge
Clustering objective for the original graph G.

A.2 Rank-3 hypergraphs with two categories
The energy minimization framework also allows us to handle the
case of rank-3 hypergraphs. Adopting the conventions of the previ-
ous subsection, we can write the clustering objective as follows.

CatEdgeClus(C) =

(cid:213)

Ei, j (xi , xj ) +

(cid:213)

Ei, j,k (xi , xj , xk ),

i <j : (i, j)∈E

i <j <k : (i, j,k)∈E

where Ei, j (xi , xj ) is the same as in the previous section and the
higher-order energy for hyperedges is

Ei, j,k (1, 1, 2)
Ei, j,k (1, 2, 2)
Ei, j,k (2, 1, 2)
Ei, j,k (2, 2, 2)










+ I(C[i] = C[j] = C[k] = 2)

Ei, j,k (1, 1, 1)


Ei, j,k (1, 2, 1)


Ei, j,k (2, 1, 1)


Ei, j,k (2, 2, 1)


1
1
1
0

0
1
1
1

=



















+ I(C[i] = C[j] = C[k] = 1)

0
0
0
0










.

1
0
0
0









0


0


0


1



0
0
0
0










The energy function defined this way is regular, in the sense that
all projections into two variables are regular. We formalize this
observation in the theorem below.

Theorem A.4. The Categorical Edge Clustering objective for rank-
3 hypergraphs with two categories induces a regular energy function.

We proceed to construct a graph G ′ in a manner similar to that
described in the preceding subsection which will allow us to opti-
mize the Categorical Edge Clustering objective through a minimum
s-t-cut on G ′. After appending the source and sink nodes s and
t to G ′, we perform the procedure of the previous section for all
edges e ∈ E. For the remaining hyperedges of rank 3, we follow
the procedure outlined by Kolmogorov and Zabih [38, Section 4.1].
This is a special case of the more general approach we present in
Section 3. In particular, depending on the hyperedge color, we use
one of the two directed tree structures in Figure 1. The fact that
the minimum s-t cut on G ′ thus constructed induces a partition of
the nodes in E which minimizes the Categorical Edge Clustering
objective follows from a proof similar to that presented in the graph
case. The actual proof is the special r = 3 case of the main proof in
Section 3. Finally, we can establish the following theorem.

Theorem A.5. Let C∗ be a two-colored clustering that is the so-
lution of the s-t-mincut problem on the graph G ′ constructed using
the procedure above. Then C∗ also optimizes the Categorical Edge
Clustering objective for the original hypergraph G.

13

