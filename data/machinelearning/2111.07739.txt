1
2
0
2

v
o
N
5
1

]
E
S
.
s
c
[

1
v
9
3
7
7
0
.
1
1
1
2
:
v
i
X
r
a

Beep: Fine-grained Fix Localization by Learning to Predict
Buggy Code Elements

SHANGWEN WANG, National University of Defense Technology, China
KUI LIU‚àó, Nanjing University of Aeronautics and Astronautics, China
BO LIN, National University of Defense Technology, China
LI LI, Monash University, Australia
JACQUES KLEIN, University of Luxembourg, Luxembourg
XIAOGUANG MAO, National University of Defense Technology, China
TEGAWEND√â F. BISSYAND√â, University of Luxembourg, Luxembourg

Software Fault Localization refers to the activity of finding code elements (e.g., statements) that are related to
a software failure. The state-of-the-art fault localization techniques, however, produce coarse-grained results
that can deter manual debugging or mislead automated repair tools. In this work, we focus specifically on the
fine-grained identification of code elements (i.e., tokens) that must be changed to fix a buggy program: we
refer to it as fix localization. This paper introduces a neural network architecture (named Beep) that builds
on AST paths to predict the buggy code element as well as the change action that must be applied to repair
a program. Leveraging massive data of bugs and patches within the CoCoNut dataset, we trained a model
that was (1) effective in localizing the buggy tokens with the Mean First Rank significantly higher than a
statistics based baseline and a machine learning-based baseline, and (2) effective in predicting the repair
operators (with the associated buggy code elements) with a Recall@1‚âà 30-45% and the Mean First Rank ‚âà7-12
(evaluated by CoCoNut, ManySStuBs4J, and Defects4J datasets). To showcase how fine-grained fix localization
can help program repair, we employ it in two repair pipelines where we use either a code completion engine
to predict the correct token or a set of heuristics to search for the suitable donor code. A key strength of
accurate fix localization for program repair is that it reduces the chance of patch overfitting, a challenge in
generate-and-validate automated program repair: both two repair pipelines achieve a correctness ratio of
100%, i.e., all generated patches are found to be correct. Moreover, accurate fix localization helps enhance the
efficiency of program repair.

CCS Concepts: ‚Ä¢ Software and its engineering ‚Üí Software verification and validation; Software defect
analysis; Software testing and debugging.

Additional Key Words and Phrases: Fault Localization, Software Debugging, Program Repair.

‚àóCorresponding author.

Authors‚Äô addresses: Shangwen Wang, wangshangwen13@nudt.edu.cn, National University of Defense Technology, Chang-
sha, China; Kui Liu, kui.liu@nuaa.edu.cn, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Bo Lin,
linbo19@nudt.edu.cn, National University of Defense Technology, Changsha, China; Li Li, li.li@monash.edu, Monash
University, Melbourne, Australia; Jacques Klein, jacques.klein@uni.lu, University of Luxembourg, Luxembourg; Xiaoguang
Mao, xgmao@nudt.edu.cn, National University of Defense Technology, Changsha, China; Tegawend√© F. Bissyand√©,
tegawende.bissyande@uni.lu, University of Luxembourg, Luxembourg.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
¬© 2021 Association for Computing Machinery.
0004-5411/2021/11-ART $15.00
https://doi.org/10.1145/1122445.1122456

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

 
 
 
 
 
 
2

Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, and Tegawend√© F. Bissyand√©

ACM Reference Format:
Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, and Tegawend√© F. Bissyand√©. 2021.
Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements. J. ACM 1, 1 (November 2021),
25 pages. https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION
The complexity of modern software is a source of bugs (leading to program failures), which makes
debugging a resource-intensive activity of software development. Automating debugging has thus
become a core research field of computer science. Mainly, debugging activities are twofold: fault
localization and software repair [59]. While there is currently excitement in the research around
the automation of software repair, fault localization has always been regarded as one of the most
tedious and time-consuming activities in debugging [29]. There is a large amount of literature [91]
on advanced fault localization techniques that are developed to aid software engineers to locate
program bugs. The granularity and accuracy of such techniques remain however a key bottleneck
for their adoption by developers in the industry [68]. Research has indeed shown that literature
approaches to fault localization are limited by a set of strong assumptions on how developers behave
when debugging [68]. The researchers revealed that examining a faulty statement is not enough
for a developer to understand and fix the corresponding bug. We foresee two different approaches
to cope with this limitation: (1) provide a precise characterization of bug alongside coarse-grained
localization information (e.g., buffer overflow bug in line ùë•); or (2) provide fine-grained localization
of code elements to change alongside the required change operator (e.g., UPDATE boolean value
‚Äútrue" in line ùë•, column ùë¶). Our work focuses on the latter.

With the momentum of automated program repair (APR), in particular with the promising
generate-and-validate approaches, fault localization techniques are widely leveraged to automate
the APR pipeline in terms of identifying code locations that must be involved with code changes to
generate patches. Unfortunately, on the one hand, the granularity (e.g., method or line) of fault
localization outputs is such that APR tools contribute to exploding the search space in patch
generation, leading to an inefficient repair process [55]. On the other hand, the inaccuracy of fault
localization tools leads APR tools to generate plausible patches (i.e., patches that pass the available
test suites without necessarily being correct) that are applied to the non-buggy locations. This is
commonly known as the overfitting problem [57, 76, 82, 86, 95] (i.e., a plausible patch may still
be incorrect), which is now regarded as a key bottleneck for APR adoption in the industry [81].
Finally, a recent study [52] has highlighted that the poor performance of state-of-the-art APR tools
on benchmark defects can be largely attributed to the under-performance of fault localization:
about 1 out of 3 real-world bugs in the Defects4J benchmark [30] cannot be localized accurately
by commonly-used spectrum-based fault localization [52]. Overall, fault localization is a critical
concern towards facilitating both manual and automatic bug fixing.

Consider the example of Defects4J bug Closure-62 as well as its patch shown in Figure 1. Although
a fault localization tool may localize the buggy if statement, developers still need to investigate a
large number of change trials for addressing the bug. Similarly, a typical search-based generate-and-
validate APR tool needs to make various change trials on 12 single code elements (i.e., 12 code tokens:
‚Äúexcerpt‚Äù, ‚Äúequals‚Äù, ‚ÄúLINE‚Äù, ‚Äú&&‚Äù, ‚Äú0‚Äù, ‚Äú<=‚Äù, ‚Äúcharno‚Äù, ‚Äú&&‚Äù, ‚Äúcharno‚Äù, ‚Äú<‚Äù, ‚ÄúsourceExcerpt‚Äù and
‚Äúlength‚Äù). On the one hand, conducting trials on non-buggy code elements will generate and validate
a number of nonsensical patches, hence impacting efficiency due to expensive test campaigns [55].
On the other hand, modifying the non-buggy code elements increases the likelihood of yielding
overfitting patches. For this bug, jKali [62] removes the whole conditional expression as the faulty
element, which is sufficient to pass the weak test suite.

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements

3

1
2
3
4
5
6
7

// Ground-truth patch for Closure-62
-
+

if (excerpt.equals(LINE) && 0 <= charno && charno < sourceExcerpt.length()) {
if (excerpt.equals(LINE) && 0 <= charno && charno <= sourceExcerpt.length()) {

// An overfitting patch generated for Closure-62 by jKali
-
+

if (excerpt.equals(LINE) && 0 <= charno && charno < sourceExcerpt.length()) {
if (true) {
Fig. 1. The ground-truth patch and an overfitting patch from jKali for the bug Closure-62.

Our work aims to improve fault localization for debugging, with a focus on refining the granularity
of code elements to localize, without sacrificing accuracy. We start with the common assumption
that underlies most machine learning techniques to defect prediction [48]: buggy programs having
similar program structures tend to involve the same buggy elements. We, therefore, propose to
leverage the concept of abstract syntax tree (AST) path [7], which was demonstrated amenable for
learning program structure information [5, 6, 8]. Each code element can thus be accurately identified
in the program structure through its AST path. Eventually, we propose a learning approach for
fine-grained identification of which code elements must be changed in a buggy program: we refer
to this as fix localization. Our ambition with fix localization, therefore, goes beyond current fault
localization approaches circumscribing the code locations (i.e., at best, the statements) suspected
of causing a failure. We develop a new fix localization approach, Beep (Buggy codE Elements
Predictor), which takes as inputs a faulty method and yields a ranked list of code elements that are
likely to be the buggy ones.

We performed extensive experiments to assess the performance of Beep. Specifically, we trained
Beep on the dataset used by CoCoNut [60] repair tool, and evaluated the prediction results on three
defect benchmarks in total: CoCoNut (by using 10-fold cross validation), ManySStuBs4J [31], and
Defects4J [30]. Since previous studies [52] have highlighted that spectrum-based fault localization
yields reasonable performance at coarse granularity (e.g., file or method), we consider a fine-grained
localization scenario where the buggy method is known. We also perform experiments in a scenario
where the faulty line is provided (e.g., by another tool). Overall, the experimental results reveal
that the buggy code elements identified by Beep can significantly reduce the debugging effort. For
instance, the median token number for the buggy method in the ManySStuBs4J dataset is 72 while
Beep can predict the buggy token in top-7 on average. Moreover, Beep is also capable of accurately
predicting the code change operator associated to the buggy element.

To assess the performance on fixing bugs with the predicted buggy code elements, we design
two repair pipelines: in the first (1) we leverage an off-the-shelf code completion tool to predict the
correct contents for tokens identified as buggy by Beep; and in the second (2) we use straightforward
heuristics to search for replacements for the localized buggy tokens. Experimental evaluation of
the repair pipelines on existing defect benchmarks (Bears, Defects4J, Bugs.jar, and QuixBugs)
reveals that these pipelines can fix 27 and 32 bugs respectively, both with a 100% correctness
ratio. Moreover, the efficiency of the pipeline (measured by the Number of Patch Candidates, NPC)
significantly outperforms those of the state-of-the-art techniques in the literature (e.g., SimFix [27]
and TBar [54]).

To sum up, this paper makes the following contributions:

‚ë† [Fine-grained fix localization with Beep]: We develop a new fine-grained fix localization
approach, Beep, which takes advantage of a specialized deep learning architecture that considers
abstract syntax tree paths to precisely predict buggy AST nodes. Experimental results with
extensive patch datasets (ManySStuBs4J, CoCoNut, and Defects4J) demonstrate the effectiveness
of our prediction model which precisely predicts at Top-1 the buggy code elements for up to 45%
bugs. An ablation study further demonstrates the importance of the various components and
design decisions in our model.

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

4

Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, and Tegawend√© F. Bissyand√©

‚ë° [Prediction of code change operators for identified buggy code elements]: In the imple-
mentation of the Beep fix localization approach, we also learn to predict which operators (i.e.,
DELETE, UPDATE and INSERT) should be applied to the localized buggy code elements, in order to
fix the program. Evaluation results reveal that Beep can also accurately predict the associated
code change operator.

‚ë¢ [Demonstration of Beep added-value in repair processes]: Given the output of Beep (i.e., an
identified buggy code element and its associated predicted change operator), we design two repair
schemes that leverage either an off-the-shelf code completion tool or a set of straightforward
heuristics. We then assess the repair performance of the implemented repair pipelines on four
widely-used repair benchmarks. Results reveal that with fine-grained accurate localization, the
patch generation performance is characterized by high precision (100% generated valid patches
are also correct) and high efficiency (with an average of only 2 patch candidates to test before
finding a valid patch). The pipeline is also complementary to the state-of-the-art since it fixes
new bugs that were not yet fixed in the literature.

2 BACKGROUND AND DEFINITIONS

2.1 Automated Program Repair
APR is a software engineering research field that seeks to automate patch generation towards
releasing developers from the heavy burden of manual debugging. APR pipelines start with a
fault localization step that enumerates a list of code locations that are suspected as buggy. In
widespread generate-and-validate APR tools, spectrum-based fault localization techniques [69]
are adopted to produce bug positions at the line [55] or method [89] granularity. Because the
performance of fault localization can severely impact repair performance [52], researchers act on
this step to improve the pipeline: e.g., ACS [96] uses predicate switching [103] while SimFix [27]
applies a test case purification approach [98] to refine the fault localization results, respectively. In
recent works [28, 60], the assessment of the actual patch generation step of APR has been done
by assuming that the fault localization at the line is perfect. Nevertheless, even in such cases, the
patch generation is challenged since it often partially touches the relevant code elements [51]. In
summary, state-of-the-art APR approaches are still impacted by the presence of non-buggy code
elements within the suspected buggy code lines. Our work explores fix localization to achieve
finer-grained localization of buggy code elements for program repair.

2.2 Automated Fault localization
Automated fault localization [91] aims to precisely diagnose bug positions to facilitate program
debugging. The most widely studied spectrum-based fault localization (SBFL) techniques [69] usually
utilize the execution traces of passing and failing tests to identify the suspicious code elements
(e.g., lines/methods). The intuition behind this is that if a code element is covered by more failing
tests but fewer passing tests, then it is more likely to be the buggy one. Hence, researchers usually
apply statistical analysis (e.g., Ochiai [1] and Tarantula [29]) to calculate the suspiciousness of code
elements and rank them in descending order to represent the exposed bug positions. The inherent
limitation of SBFL approaches is that a code element executed by a failing test does not necessarily
indicate that the element is related to the test failure. To solve this problem, researchers also propose
mutation-based fault localization (MBFL) techniques that mutate code elements and then check
their impact on the test outcomes [67]. Besides SBFL and MBFL, researchers have explored various
fault localization techniques, e.g., slice-based [104], statistics based [49], program state-based [101],
learning-based [90], data mining based [36], and model-based techniques [63]. Nevertheless, the
state-of-the-art fault localization techniques provide the identified bug positions at best at the

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements

5

granularity of code lines. Our work is the first to target the identification of buggy code elements
nested within buggy lines, thus providing a more fine-grained localization for program debugging.

2.3 AST Paths for Code Representation
Recently, a number of works consider the Abstract Syntax Tree (AST) path [7] for code represen-
tation. Code2vec [8] and Code2seq [5] use all the AST paths within a method with an attention
mechanism to represent this method and predict its name as well as generate its natural language
description. Brody et al. [17] use this technique to encode the changed code within a file and then
predict the following code changes. Alon et al. [6] adopt it to address the any-code completion task,
which is generating a missing piece of source code in a given program. Compton et al. [20] extend
the application scenario of this technique to the whole Java class via variable obfuscation. All the
above works demonstrate that the AST path technique is amenable for learning program structure
information. We thus build on the concepts of AST path and operation path associated to the buggy
code elements (see example in Fig. 2). Formal definitions for each term in our study are provided as
below.

Fig. 2. AST & change operator for the correct patch in Fig. 1.

Definition 1 - [AST] : The AST of a code snippet is defined as a tuple: ‚ü®ùëÅ , ùêø,ùëá , ùëü, Œî, Œ¶‚ü©, where ùëÅ is
a set of non-leaf nodes, ùêø is a set of leaf nodes, and ùëá is a set of code tokens for ùêø. ùëü ‚àà ùëÅ represents
the root node, ùõø ‚àà Œî : ùëõ ‚Üí ùëõ‚Ä≤, ùëõ ‚àà ùëÅ , ùëõ‚Ä≤ ‚àà (ùëÅ ‚à™ ùëá ) is a function that reflects the parent-child
relationship between two AST nodes ùëõ and ùëõ‚Ä≤. ùúô ‚àà Œ¶ : ùëô ‚Üí ùë°, ùëô ‚àà ùêø, ùë° ‚àà ùëá maps a leaf node with a
corresponding code token.
Definition 2 - [AST path] : An AST path is a path starting from the root node ùëü to a leaf node ùëô,
that is defined as a quadruple: ùëù = ‚ü®ùëü, ùëô, ùëÅ ‚Ä≤, Œî‚Ä≤‚ü©, ùëô ‚àà ùêø, ùëÅ ‚Ä≤ ‚äÇ ùëÅ , Œî‚Ä≤ ‚äÇ Œî.
Definition 3 - [Operation Path] : An operation path is an AST path associated with code change
operator that works on a leaf node ùëô, which is defined as a triple: ùëúùëù = ‚ü®ùë°, ùëù, ùëú‚ü©, where ùë° is the code
token of the leaf node ùëô in the AST path ùëù, and ùëú ‚àà {UPDATE, DELETE, INSERT} is an atomic code
change operator that works on the leaf node.

Fig. 2 provides an illustration of the AST path (with the change operator) for the ground-truth
patch illustrated in Fig 1. MethodDeclaration is the root node of the AST, and other AST nodes
with grey backgrounds are non-leaf nodes, while all leaf nodes are presented with transparent
backgrounds (oval shapes). Each leaf node is attached with its associated code token (in rectangles).
The AST path for the buggy operator ‚Äú<‚Äù is ùëù: ‚ÄúMethodDeclaration ‚Üí Block ‚Üí IfStatement ‚Üí

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

MethodDeclarationModifierNameTypeBlockprivatelogStingIfStatementInfixExpressionBlock<=ParameterDeclarationNamevalueOperator‚Ä¶OthersInfixExpression<charnoOperatorInfixExpressionUPDATEIfStatementInfixExpressionBlockInfixExpressionOperatorVariableMethodInvocation6

Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, and Tegawend√© F. Bissyand√©

IfStatement ‚Üí InfixExpression ‚Üí InfixExpression ‚Üí InfixExpression ‚Üí Operator
‚Äù, highlighted with a red arrow in Fig. 2. For simplification, the other AST nodes irrelevant to
the buggy code element are not presented in this figure. The operation path for fixing the buggy
operator is ùëúùëù = ‚ü®‚Äò<‚Äô, ùëù, UPDATE‚ü©. As shown in Figure 2, the buggy operator ‚Äò<‚Äô is replaced (i.e.,
UPDATE) with the operator ‚Äò<=‚Äô.

2.4 Declarations
In this section, we provide the declarations of code elements and fix localization to clarify their
differences between this work and existing studies explored in the literature.
[Code Element]: Generally speaking, all code entities (such as code fragments, statements, ex-
pressions, and single code tokens) in the programs can be referred to as code elements for concrete
targets. In this work, Beep is to perform the fine-grained fault localization at code token level for
program repair, thus ‚Äúcode elements‚Äù studied in this paper denote code tokens in the program.
[Fix Localization]: In the basic pipeline of APR, fault localization aims to pinpoint the statement(s)
that can be selected as bug locations for mutation [40]. The accuracy of fault localization can directly
impact the bug-fixing performance of APR tools [52]. In the literature, researchers have taken
different efforts to improve the fault localization for automated program repair. For example, Tan
et al. [80] utilized the anti-patterns to improve fault localization by pinpointing the buggy location
at line/function level with higher accuracy. Shariffdeen et al. [75] leveraged the transformation of
existing patches to identify the patch insertion points of bugs at line level. Xin and Reiss [94] utilized
the stack trace of crashed programs after executing test cases to enhance the fault localization for
program repair. Nevertheless, as stated by Le Goues et al. [40], the challenge of fault localization
for program repair is ‚Äústatements that are executed exclusively by the bug-inducing input may not
be the best locations for a repair.‚Äù Our work explores to address this challenge by predicting the
fine-grained fault localization (referred to as fix localization in this work) at code element (i.e.,
code token that need to be fixed within the buggy program) level for program repair, which is
different from the existing efforts achieved in the literature on improving fault localization accuracy
at line/function level.

3 PROPOSED APPROACH
Beep is built based on a neural network model trained offline with patches collected in the wild. The
model is then used to predict the buggy code element and the associated code change operator. As
illustrated in Figure 3, the first step in Beep is a pre-processing step that produces AST differences
as representations of patches.

Fig. 3. Overview of our proposed Beep.

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

Patch CorpusASTDifferencingAST path with changeoperatorTrainingDataPre-processingOff-lineModelTrainingBuggy MethodASTAll AST pathsPredictionPredicted Resultsimport java.math.BigInteger;import java.util.Random; public class PrimeEx{ { printTest(1, 4); printTest(1882341361, 2); printTest(36, 9); System.out..out.println(isPrime(1882341361) + " expect true"); System.out.println(isPrime(2) + " expect true"); int numPrimes= 0; Stopwatc}h s. = new Stopwatch(); s.start(); for(int i= 2; i< 10000000; i++) { if(isPrim(numPrimes+ " " + s); s.start(); boolean[] primes = getPrimes(10000000); int np = 0; for(booleanb : primes) if(b) np++; s.stop(); System.out.stem.out.println(new BigInteger(1024, 10, new Random())); } public static boolean[] getPrimes(int max) { boolean[] result = new boolean[max + 1]; for(int i= 2; i< result.length; i++) result[i] = true; final do); for(int i= 2; i<= LIMIT; i++) { if(result[i]) { // c* i; while(index < result.length){ result[index] = false; index += i; } }MIT = Math.sqrt(num); booleanisPrime= (num == 2) ? true : num % 2 != 0; int div = 3; while(div <= LIMIT && isPrime) { isPrime=num % div != 0import java.math.BigInteger;import java.util.Random; public class PrimeEx{ { printTest(1, 4); printTest(1882341361, 2); printTest(36, 9); System.out..out.println(isPrime(1882341361) + " expect true"); System.out.println(isPrime(2) + " expect true"); int numPrimes= 0; Stopwatc}h s. = new Stopwatch(); s.start(); for(int i= 2; i< 10000000; i++) { if(isPrim(numPrimes+ " " + s); s.start(); boolean[] primes = getPrimes(10000000); int np = 0; for(booleanb : primes) if(b) np++; s.stop(); System.out.stem.out.println(new BigInteger(1024, 10, new Random())); } public static boolean[] getPrimes(int max) { boolean[] result = new boolean[max + 1]; for(int i= 2; i< result.length; i++) result[i] = true; final do); for(int i= 2; i<= LIMIT; i++) { if(result[i]) { // c* i; while(index < result.length){ result[index] = false; index += i; } }MIT = Math.sqrt(num); booleanisPrime= (num == 2) ? true : num % 2 != 0; int div = 3; while(div <= LIMIT && isPrime) { isPrime=num % div != 0import java.math.BigInteger;import java.util.Random; public class PrimeEx{ { printTest(1, 4); printTest(1882341361, 2); printTest(36, 9); System.out..out.println(isPrime(1882341361) + " expect true"); System.out.println(isPrime(2) + " expect true"); int numPrimes= 0; Stopwatc}h s. = new Stopwatch(); s.start(); for(int i= 2; i< 10000000; i++) { if(isPrim(numPrimes+ " " + s); s.start(); boolean[] primes = getPrimes(10000000); int np = 0; for(booleanb : primes) if(b) np++; s.stop(); System.out.stem.out.println(new BigInteger(1024, 10, new Random())); } public static boolean[] getPrimes(int max) { boolean[] result = new boolean[max + 1]; for(int i= 2; i< result.length; i++) result[i] = true; final do); for(int i= 2; i<= LIMIT; i++) { if(result[i]) { // c* i; while(index < result.length){ result[index] = false; index += i; } }MIT = Math.sqrt(num); booleanisPrime= (num == 2) ? true : num % 2 != 0; int div = 3; while(div <= LIMIT && isPrime) { isPrime=num % div != 0import java.math.BigInteger;import java.util.Random; public class PrimeEx{ { printTest(1, 4); printTest(1882341361, 2); printTest(36, 9); System.out..out.println(isPrime(1882341361) + " expect true"); System.out.println(isPrime(2) + " expect true"); int numPrimes= 0; Stopwatc}h s. = new Stopwatch(); s.start(); for(int i= 2; i< 10000000; i++) { if(isPrim(numPrimes+ " " + s); s.start(); boolean[] primes = getPrimes(10000000); int np = 0; for(booleanb : primes) if(b) np++; s.stop(); System.out.stem.out.println(new BigInteger(1024, 10, new Random())); } public static boolean[] getPrimes(int max) { boolean[] result = new boolean[max + 1]; for(int i= 2; i< result.length; i++) result[i] = true; final do); for(int i= 2; i<= LIMIT; i++) { if(result[i]) { // c* i; while(index < result.length){ result[index] = false; index += i; } }MIT = Math.sqrt(num); booleanisPrime= (num == 2) ? true : num % 2 != 0; int div = 3; while(div <= LIMIT && isPrime) { isPrime=num % div != 0Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements

7

3.1 Data pre-processing
We consider a collection of historical patches as a training dataset. Then, for each, we use the
GumTree [21] AST differencing algorithm to compute AST diffs, which are the actual input repre-
sentations for Beep. These diffs thus allow to readily identify the AST path (from the root towards
the modified leaf node) as well as the operation path (i.e., code token + AST path + change operator).
This set, which so far includes only true positive samples, is augmented by considering, for every
buggy method, all AST paths towards non-buggy leaf nodes and assigning alternatively each of the
three change operators. These new operation paths are therefore tagged as true negatives for the
training since they represent paths that should not be predicted as buggy.

Additionally, during pre-processing, we collect the code token of the leaf node in each AST
path to be included in the feature set for learning. Following the insights of a recent study by
Lutellier et al. [60], we split code tokens into sub-token sequences in order to significantly reduce
the size of the vocabulary. Concretely, as per previous studies [2, 5], code tokens are broken into
sub-token sequences ‚Äú{ùë°0, ùë°ùëñ, . . . , ùë°ùëõ}‚Äù based on camel case and underscore naming conventions, and
the obtained sub-tokens are converted to their lowercase form.

3.2 Training the Beep model
Beep is mainly composed of an encoder-decoder network and a pointer network to generate the
output. Our prediction approach is based on the intuition of software naturalness [26]: the buggy
part should be detectable via learning as encoder-decoders have already done for natural language
typographical/grammatical mistakes. A pointer network [85] is a simple modification of the attention
model which learns the conditional probability of an output sequence with elements that are discrete
tokens corresponding to positions in an input sequence. As several recent studies [17, 43] have
further demonstrated, pointer networks are indeed particularly effective when the output is simply
picked among elements from inputs. Since in our problem case, the predicted results (i.e., buggy
code elements and change operators) are discrete elements, Beep leverages the pointer network to
predict the buggy code elements along with the associated change operators. Figure 4 illustrates
the overview of the model architecture.

Fig. 4. Architecture of the Prediction Model.

Given an operation path, Beep first respectively encodes code sub-tokens, AST path, and operator
with three models1, and concatenates the vectors for path representation. Then, the path vectors

1Encoding each of the sub-tokens, AST path, and operations is followed with the encoding method of code2seq [4].

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

encodedsub-tokensLSTM-basedpathencoderIfStmtPrefixExpMdInvocUpdate/Delete/InsertoperationsencodedoperationsConcatenated vectorsFull-connectedlayerencoderhidden statesdecoderW1W2Predictionresults‚Ä¶embeddinghidden statessoftmaxpointer networksub-tokensLSTMLSTM++8

Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, and Tegawend√© F. Bissyand√©

are passed through a fully-connected layer and the Long Short-Term Memory (LSTM) based
encoder-decoder consecutively. Finally, the pointer network learns to predict the most probable
path.

Embedding Operation Paths. Given a set of ùëò operation paths {ùëúùëù1,. . . ,ùëúùëùùëò }, Beep embeds a vector
representation ùë£ùëñ for each path ùëúùëùùëñ = ‚ü® ùë°ùëñ , ùëùùëñ , ùëúùëñ ‚ü© where ùëùùëñ = {ùëõùëñ
ùëôùëñ } is the corresponding AST
1
path, ùë°ùëñ is the code token and ùëúùëñ is the change operator. To that end, Beep first leverages a learned
embedding matrix to embed each sub-token (after splitting ùë°ùëñ ) and sum the sub-token vectors to
represent the full token. The change operator of each operation path is embedded with another
embedding matrix. The embedding process is formulated as below:

2,. . . ,ùëõùëñ

, ùëõùëñ

Vùë° =

‚àëÔ∏Å

ùë°ùë† ‚ààùëáùë†

ùê∏ùë° (ùë°ùë† )

Vùëú = ùê∏ùëú (ùëú)

(1)

where ùê∏ùë° (‚àó) and ùê∏ùëú (‚àó) are learned embedding matrix. ùëáùë† is the sub-token sequence of code token ùë°
and ùëâùë° represents the vector representation for token ùë°, while ùëâùëú is the vector representation for
the change operator ùëú.

The AST path of each operation path is composed of several AST nodes. Beep also represents
each node ùëõùëñ using a learned embedding matrix ùê∏ùëù (‚àó) and encode the entire sequence with the
final state of the bi-directional LSTM neural networks:

Vùëù = ùêøùëÜùëá ùëÄ (ùê∏ùëù (ùëõ1), ùê∏ùëù (ùëõ2), . . . , ùê∏ùëù (ùëõùëô ))
where ùëâùëù represents the vector representation of an AST path. ùêøùëÜùëá ùëÄ denotes the bi-directional
LSTM neural networks.

(2)

Extracting Features with Encoder-Decoder Networks. To represent the operation path, we jointly
concatenate the vector representations of code token, the AST path, and the change operator, and
then pass them through a fully-connected layer, of which results are further fed into the LSTM-based
encoder-decoder networks to better capture the features, which is formulated as below:
ùëßùëñ = ùë°ùëéùëõ‚Ñé(ùëäùëñùëõ [ùëâùë° ; ùëâùëù ; ùëâùëú ])
(ùëí1, . . . , ùëíùëò ) = ùêøùëÜùëá ùëÄùëíùëõùëêùëúùëëùëíùëü (ùëß1, . . . , ùëßùëò )
(ùëè1, . . . , ùëèùëò ) = ùêøùëÜùëá ùëÄùëëùëíùëêùëúùëëùëíùëü (ùëí1, . . . , ùëíùëò )
where ùëäùëñùëõ is the weight matrix with the size of (ùëëùë° + 2ùëëùëù + ùëëùëú ) √ó ùëë‚Ñéùëñùëëùëëùëíùëõ, (ùëí1, . . . , ùëíùëò ) and (ùëè1, . . . , ùëèùëò )
are hidden states of the encoder and decoder, respectively.

(3)

Generating Results with Pointer Networks. Given the encoder and decoder hidden states (ùëí1, . . . , ùëíùëò )

and (ùëè1, . . . , ùëèùëò ), we calculate the attention vector as follows:
ùë¢ ùëó = ùë£ùëá ùë°ùëéùëõ‚Ñé(ùëä1ùëí ùëó + ùëä2ùëè ùëó )
(4)
where ùë£, ùëä1, and ùëä2 are learnable parameters of the model and j ‚àà (1,. . . ,k). The value of ùë¢ ùëó is used
as attention weight to the ùëóùë°‚Ñé input:

ùëù (ùëúùëù ùëó |ùëúùëù1, . . . , ùëúùëùùëò ) = ùë†ùëú ùëì ùë°ùëöùëéùë• (ùë¢ ùëó )
(5)
where softmax normalizes the vector ùë¢ = [ùë¢1, . . . , ùë¢ùëò ] to be an output distribution over the inputs.
At last, the output is a list of operation paths ranked by their distribution weights.

It should be noted that our pointer network is a variant of the original model: we only point to a

single path from the inputs rather than generating a sequence of outputs.

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements

Parameter training. We use cross-entropy loss [71] to train the parameters in our model:

ùêøùëúùë†ùë† =

‚àëÔ∏Å

‚àí

‚àëÔ∏Å

ùë¶ùëñ ‚ààùëå

ùëúùëù ‚ààùë¶ùëñ

[ùëåùëúùëù ¬∑ ùëôùëúùëî(ùëÉùëúùëù ) + (1 ‚àí ùëåùëúùëù ) ¬∑ ùëôùëúùëî(1 ‚àí ùëÉùëúùëù )]

9

(6)

ùëåùëúùëù ={1, 0} indicates whether the operation path is an oracle path, ùëÉùëúùëù is the outputted weight of the
operation path. We use the Adam approach [33] in the learning process to reduce the ùêøùëúùë†ùë†.

3.3 Buggy position prediction
Once the Beep model is trained, we can use it to predict the operation path (i.e, buggy token value
+ bug position + change operator) for an unseen buggy method. We first extract AST paths towards
all leaf nodes within the buggy method and assign alternatively each of the three change operators,
which generates all operation paths for this method. We then embed all these operation paths and
send them into the trained model. Beep will then return the prediction result (i.e., a list where the
operation paths are ranked by their output weights).

4 STUDY DESIGN

4.1 Research Questions
‚Ä¢ [RQ-1] Is Beep effective for identifying buggy code elements in real-world programs? We attempt
the fix localization on thousands of bugs to assess the performance of Beep. Thus, we focus on
the predicted results about the buggy token without considering the associated change operator.
‚Ä¢ [RQ-2] Does Beep accurately predict the change operator that must be applied to fix a bug? Given
that predicting the operator alone is irrelevant, we investigate the prediction performance of
Beep for the pair of ingredients constituted by the buggy code element and the change operator.
The prediction of these ingredients is essential for accelerating both manual and automated
program repair [53, 54].

‚Ä¢ [RQ-3] To what extent have our design choices influenced the performance of the neural network?
We assess the impact of splitting the tokens, encoding nodes of the AST paths, and using a fully
connected layer in improving the predictive power of Beep.

‚Ä¢ [RQ-4] Can fine-grained fix localization help improve the precision and efficiency of patch genera-
tion? In particular, we investigate a case study of program repair, how token-level fault localization
impacts (1) patch overfitting in APR, as well as (2) the number of patch candidates that are tried
before a plausible patch can be identified.

4.2 Subject Selection
Patch datasets. For training the proposed Beep neural network-based model, we collect patches
from the training dataset used by the CoCoNut [60] repair tool. To evaluate the performance of
Beep, we collect bugs from ManySStuBs4J [31] and Defects4J [30] datasets.

We choose these datasets because (1) CoCoNut and ManySStuBs4J are large-scale patch bench-
marks which are suitable for assessing the generalization ability of our approach and (2) Defects4J
is the most widely used benchmark in the software testing literature. We deduplicate the samples
and remove samples related to test code, since we focus on fix localization in source code. Overall,
the final datasets include 436 676, 26 406, and 393 bugs from CoCoNut Dataset, ManySStuBs4J, and
Defects4J, respectively.

Repair benchmarks. To answer RQ4 (on the added-value of Beep for APR patch generation per-
formance), we consider bugs within four widely-used Java defect benchmarks: Defects4J (V1.4) [30],
Bears [61], QuixBugs [50] and Bugs.jar [72]. We choose these benchmarks because they have been
widely used for evaluating the state of the art APR tools whose assessment reports are available for
comparison.

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

10

Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, and Tegawend√© F. Bissyand√©

4.3 Experiment Settings

Implementation. We implemented Beep by integrating the Pytorch implementation of
4.3.1
PointerNet2 with the framework provided for code2seq [5]. The Beep model is trained on two
servers with NVIDIA TITAN V, Xp, and 1080 Ti GPUs.

4.3.2 Parameter configuration. Following the insights of previous studies [5, 8], we set a limit
for the maximum length of the AST path, as well as for the maximum number of operation paths
that we feed to the neural network. These limits are respectively noted as ùëöùëéùë•ùëô and ùëöùëéùë•ùëò and are
determined empirically as hyper-parameters of our model.

For tuning, we randomly sample 43 000 patches among the considered CoCoNut patches as our
validation dataset, while the remaining are used for training and testing. In the tuning process,
we mainly focus on the key hyper-parameters: the vector length of encoded tokens (64, 128, 256),
learning rate (0.001, 0.002, 0.005), epoch size (20, 40, 50), batch size (64, 128, 256), ùëöùëéùë•ùëô (10, 15,
20), and ùëöùëéùë•ùëò (100, 120, 150, 180, 200). Default values of other parameters are taken from the
implementation of code2seq. The final values of hyper-parameters of our model are displayed in
Table 1.

Table 1. Hyper-parameter values inferred for Beep.

vector length of token learning rate

128

0.001

epoch size
40

batch size ùëöùëéùë•ùëô ùëöùëéùë•ùëò
120

256

15

4.3.3 Evaluation setting. To answer the first three research questions (c.f., Section 4.1) on the
performance of the Beep neural network-based model, we also perform 10-fold cross validation [35,
86] on CoCoNut dataset to avoid bias and ensure the generalization of the model. Each training
epoch takes about 8 minutes on our computing power, summing up to a total of about 5-hour
execution time for each validation fold.

1

4.3.4 Metrics. To answer research questions RQ-1 to RQ-3, we use the following widely-used
metrics [44, 59, 108]:

, ùëíùë†ùë¢ùë†
2

, ..., ùëíùë†ùë¢ùë†

Recall@Top-n: Following the existing works [44, 102, 108] for the fault localization, we selected
the Recall at Top-N as the metric. Specifically, given a descending-order ranked list of predicted
ùëõ }, if the buggy code element ùëíùëñ is within ùê∏ùë†ùë¢ùë† ,
suspicious code elements ùê∏ùë†ùë¢ùë† = {ùëíùë†ùë¢ùë†
it is localized. A smaller ùëõ value indicates the more accuracy of fault localization for automated
program repair. It is equivalent to the spectrum-based fault localization methods [44, 59, 68] that
locate the bug position at the line level with a ranked list of suspicious statements. In this work, we
select ùëõ as 1, 3, 5, 10, and 20. A higher Recall@Top-n value indicates the better precision of fault
localization. Note that in our evaluation datasets, a bug may possess more than one buggy code
element (e.g., a patch changes two or more code tokens). Following the validation procedure of
spectrum-based fault localization [59, 68], we consider that a bug is localized at top-ùëõ when one of
its buggy code elements is ranked among the top-ùëõ predictions of Beep.

Mean First Rank (MFR): It computes the mean rank value for the first localized buggy code
element in the ranked list of predicted suspicious code elements. For example, given a bug, the
list predicted by Beep contains ùëõ suspicious code elements, and the buggy code element is ranked
at the ùëò (ùëò ‚àà [1, ùëõ]) position in the list. A lower MFR value indicates the better precision of fault
localization.

The last research question RQ-4 is assessed with the following two metrics [55, 96]:

2https://github.com/shirgur/PointerNet

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements

11

Correctness Ratio (CR): In the literature, to assess the bug fixing performance of APR tools,
researchers mainly focus on the number of bugs that can be fixed by APR tools with generated
plausible patches (i.e., the patches can make the patched program pass all test suites) [32, 41].
Due to the overfitting challenge [70, 80, 86], the capability of generating correct patches (i.e., the
patches can really fix the buggy program but not just make the patched program pass all test suites)
[55, 89, 96] is proposed to evaluate the bug fixing performance of APR tools in the community.
Suppose that, an APR tool can generate plausible patches for ùë• bugs, and the generated patches of
ùë¶ bugs are correct, then ùê∂ùëÖ = ùë¶/ùë• ‚àó 100%. To assess the correctness of APR-generated patches, we
adopt the open patch correctness assessment rules provided by Liu et al. [55], where 15 rules are
explicitly defined to illustrate how to identify an APR-generated patch as correct when comparing
it with the ground-truth develop-written patch provided in benchmark datasets. Such rules are
publicly available at https://github.com/TruX-DTF/APR-Efficiency and are widely used in the patch
correctness validation of APR works [28, 60, 82, 86, 99].

Number of Patch Candidates (NPC): Repair efficiency is the other metric that is used to
assess the bug fixing performance of APR tools. The time cost of generating patches has been
proposed in the literature [23, 39] to assess the repair efficiency. However, time cost could be biased
by differences among buggy programs, experimental setup and platforms [55]. In this study, we
leverage the NPC score, the number of patch candidates generated by an APR tool when the first
plausible patch is produced [55], to assess the repair efficiency of APR tools.

5 STUDY RESULTS
We now report on the experimental results and conclude on the research questions.

5.1 Fix Localization Performance
To the best of our knowledge, Beep is the first approach that targets the fix localization of buggy
code elements (cf. Section 2.4) that must be changed. Nevertheless, we propose to build two baseline
localizers for comparison with Beep.

Baseline#1 ‚Äì statistics based: in the study on patch granularity by Liu et al. [51], a reported
finding suggests that some specific code elements are more prone to be buggy than others. We
leverage their data to build a simple predictor as a baseline: ranking the elements based on their
probabilities of being buggy. When two or more tokens present the same probability, their rankings
are further determined by their appearance orders in the token sequence of the buggy method.

Baseline#2 ‚Äì machine learning-based: we also propose to train a Random Forest model [9]
for predicting buggy tokens. For a specific token, we consider four kinds of features: ‚ü®token rank,
statement type, token length, and number of sub-tokens‚ü©, where token rank denotes the ranking of this
token in the token sequence of the buggy method, statement type denotes the type of the statement
to which this token belongs (e.g., ReturnStatement or IfStatement), token length denotes its
number of characters. The number of sub-tokens of each code token is the sum of sub-tokens that
each code token has, after each code token is split into sub-tokens based on the camel case and
underscore naming conventions.

Our experiments consider two scenarios where Beep is provided with the full buggy method or
the specific line: both inputs can indeed be provided by current spectrum-based fault localization
methods. Performance results are provided in Tables 2 and 3 in terms of the percentages of bug
cases for which the tools managed to place the buggy code element among the Top-ùëò of its ranked
suspicion list. The Mean First Rank (MFR) computes the mean rank value for the correct buggy
code element: the closer to 1, the better.

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

12

Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, and Tegawend√© F. Bissyand√©

Table 2. Fix localization performance - buggy line as input.

Dataset

CoCoNut

ManySStuBs4J

Defects4J

Tool
Baseline#1
Baseline#2
Beep
Baseline#1
Baseline#2
Beep
Baseline#1
Baseline#2
Beep

Top-1
26.1%
61.7%
63.8%
36.7%
36.9%
85.2%
18.2%
23.9%
86.7%

Top-3
65.9%
76.5%
83.9%
84.1%
86.3%
96.3%
49.4%
61.8%
91.4%

Top-5
86.7%
89.9%
91.2%
94.0%
97.2%
100%
83.1%
86.7%
96.8%

MFR
3.2
2.7
2.3
2.5
1.7
1.3
2.8
1.9
1.4

CoCoNut Dataset: 436 676 bugs, ManySStuBs4J: 26 406 bugs, and Defects4J: 393 bugs, the same as Table 3.
Table 3. Fix localization performance - buggy method as input.

Dataset

CoCoNut

ManySStuBs4J

Defects4J

Tool
Baseline#1
Baseline#2
Beep
Baseline#1
Baseline#2
Beep
Baseline#1
Baseline#2
Beep

Top-1
3.7%
14.0%
46.9%
0.7%
8.1%
30.7%
1.3%
3.0%
34.9%

Top-5
25.3%
31.9%
74.2%
10.4%
20.9%
56.4%
13.3%
23.9%
57.1%

Top-10
40.9%
41.9%
85.5%
22.3%
33.6%
72.6%
25.3%
29.8%
68.2%

Top-20 MFR
38.7
57.9%
32.1
48.3%
3.9
95.2%
42.0
40.5%
28.9
46.2%
6.7
90.1%
65.1
45.3%
32.1
31.3%
6.5
87.3%

The performance of Baseline#1 is consistent with the observations in the study of Liu et al. [51].
Baseline#2 also appears to provide a bit better performance than Baseline#1. Overall, Beep out-
performs these baselines on all datasets and with respect to all metrics. When the input for fix
localization is a buggy method, Beep can precisely rank the buggy code elements at the top-1
position for 46.9% of bugs in the CoCoNut dataset, 30.7% of bugs in the ManySStuBs4J dataset, and
34.9% of bugs in the Defects4J dataset. In contrast, both baselines perform poorly: Baseline#1 can
localize at Top-1 only 3.7%, 0.7%, and 1.3% of bugs, respectively in CoCoNut, ManySStuBs4J, and
Defects4J, while Baseline#2 localizes respectively 14.0%, 8.1%, 3.0% of bugs in these datasets. Finally,
we note that Beep can achieve a recall of around 90% in localizing the right buggy code element
within its Top 20 suggestions for each bug.

[RQ-1]‚û≤ ‚ë† Beep is largely more effective than both a statistical baseline approach and a machine
learning-based baseline approach in performing fine-grained localization of buggy code elements
(i.e., fix localization).

We further investigate how much manual debugging effort can be saved when using Beep
to localize the tokens that must be changed in a method. To that end, we count the number of
code tokens in each buggy method and buggy line identified from the three defects benchmarks
used in this study. We exclude the Java keywords (e.g., if, int, etc.) when calculating the results
since it is unlikely that these tokens are buggy. This configuration, however, may under-estimate
the effectiveness of Beep since there indeed exists bugs whose fixes require the changes of such
keywords (e.g., Math-57 from Defects4J benchmark is patched by changing int into float). Results
are shown in Fig. 5.

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements

13

(a) Buggy method

(b) Buggy line
Fig. 5. Number of tokens in each buggy method and buggy line.

We note that the medium values of the number of tokens in each buggy method are 38, 72, and
67 for the CoCoNut, ManySStuBs4J, and Defects4J benchmarks, respectively. Recall that the Mean
First Rank (MFR) values for Beep on these three benchmarks are 3.9, 6.7, and 6.5 respectively (cf.
Table 3). Such results indicate that Beep can rank the buggy elements at a rather high position
(i.e., usually in the top 10% among all the method code elements). When it comes to the number of
tokens in each buggy line, the medium values become 5, 3, and 5 for the three benchmarks. From
Table 2, the MFR values for Beep are 2.3, 1.3, and 1.4 respectively, which means Beep can generally
rank the buggy elements within the first half.
[RQ-1]‚û≤ ‚ë° When provided with a buggy method, Beep can drastically reduce debugging effort
needed by developers to pinpoint the buggy tokens: on average, Beep filters out about 90% of non-
buggy code tokens. Even in such a case where the buggy line is known (e.g., with spectrum-based
fault localization), Beep can still halve the number of tokens to manually check.

5.2 Joint Prediction of Bug and Change Operator
Aside from the question of accurate fine-grained bug localization, manual repair as well as APR are
challenged in the selection of adequate repair operators. Classically, several approaches in the APR
literature rely on heuristics to try-and-err with different operators [27, 54, 89].

Beep performs the selection of the change operators for the identified buggy code elements
by learning a model from the operation paths (i.e., AST path + operator) of existing patches. The
prediction performance of Beep (given a buggy method as input) is detailed in Table 4.
Table 4. Joint prediction performance: buggy code element + code change operator with the buggy
method as input.

Dataset

Top-1 Top-5 Top-10 Top-20 MFR

CoCoNut
ManySStuBs4J
Defects4J

44.6% 67.8%
29.7% 50.8%
29.2% 51.8%

77.9%
63.7%
57.7%

87.5%
79.3%
65.5%

7.5
11.5
12.3

We note that the performance of joint prediction is decreased only slightly when compared
against the prediction of buggy code elements alone (cf. Table 3). This suggests that the Beep model
is able to predict the change operator when it precisely locates the buggy element.
[RQ-2]‚û≤ We confirm that Beep can not only localize the buggy code element (RQ1) but also accurately
predict the code change operator associated to it.

5.3 Ablation Study
We assess the contributions of some components and design choices in the performance of our core
prediction model, Beep. Our analysis focuses on three key aspects: (1) the impact of our sub-token

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

14

Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, and Tegawend√© F. Bissyand√©

Table 5. Comparison of Model Designs.

Prediction
objective

Buggy code
element

Element +
Operator

Model Design

no token splitting
no AST nodes
no fully-connected layer
Beep
no token splitting
no AST nodes
no fully-connected layer
Beep

MFR
CoCoNut ManySStuBs4J Defects4J
7.2
7.2
7.0
6.7
16.4
12.3
12.6
11.5

7.3
7.2
6.7
6.5
16.4
14.6
13.7
12.3

4.7
3.9
4.0
3.9
13.5
7.6
8.3
7.5

splitting; (2) the impact of our node-based AST path encoder; and (3) the impact of fully-connected
layer for feature extraction.

In the first experiment assessing the impact of sub-token splitting, we directly feed each token
in our dataset to ùê∏ùë° (‚àó). In the second experiment assessing the design choice of considering each
node, we do not represent each node in the AST path ùëù. Instead, we learn an embedding matrix
ùê∏ùëùùëéùë°‚Ñéùë† for representing the whole AST path. In the third experiment, we drop the fully-connected
layer: we directly use the concatenated operation path representation as the input of the LSTM
encoder layer.

The results are summarized in Table 5 w.r.t. the performance of the bug prediction model.
Due to space constraints, we only provide MFR metric values which are indicative of the overall
performance. We note that under each experimental setting, the MFR value when no token splitting
is applied is always the highest. On CoCoNut dataset, while the performance of model variants
can be on par with that of the full Beep, the MFR value is always lowest with Beep, indicating that
our design choices globally lead to the best predictive model. On ManySStuBs4J and Defects4J
datasets, the results of Beep are systematically better than those yielded by its variants. Note that
the different models are trained on the CoCoNut dataset, which, for applied on ManySStuBs4J and
Defects4J bugs, leave room to improve the neural network design choices for Beep.

[RQ-3]‚û≤ Various design choices have together contributed to the performance of Beep. Splitting the
tokens before embedding appear to be the most rewarding design choice.

5.4 Repair Performance on Real Bugs
In this RQ, we seek to investigate if narrowing the mutation space of APR tools by leading them to
change the predicted buggy tokens can enhance the precision (i.e., avoiding generating overfitting
patches) and the efficiency (i.e., reducing the number of patch candidates that are tried before
the first plausible patch). After a review of literature artifacts, we found that state-of-the-art APR
tools are implemented such that the patch generation process is entangled with current fault
localization settings. We thus propose to experiment with implementations of two straightforward
repair pipelines based on code completion and on heuristics. The basic workflow for these two
pipelines is as follows:
Input: We assume that the faulty method is known. This is a reasonable assumption since, based
on data provided in recent studies [12, 59], existing techniques are effective in localizing bugs at
the method level granularity. Some APR tools [38, 52] in the literature are even assessed based on
the assumption.
Bug prediction: We use Beep to predict the buggy tokens. After this step, we obtain a ranked list
of operation paths.

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements

15

Operation path selection: We consider the first 20 operation paths since our evaluation results
(cf. Table 4) have shown that Beep has a very high recall within its top-20 results (i.e., the searched
operation path is in among those). We then use a simple try-and-err heuristic: we iterate over each
path following the ranking order and input it to the patch generation process. If all paths have
been tried but no test-adequate patch is generated, we re-iterate over pairwise combinations of
operation paths, and so on. This process comes to an end when a patch that passes all the tests is
generated.
Patch generation: In this step, patches are generated by using either a code completion technique
or straightforward heuristics. We will introduce the details of these two methods later in this
section (after the general workflow).
Patch validation: Each generated patch is then applied to the program, on which the test suite is
executed. A valid patch must make the program pass all the test cases. When such a patch is found,
the search stops. We then manually (but systematically) check whether the generated plausible
patch is indeed correct. Correctness is assessed in comparison with the ground truth patch. We
adopt the rules that were presented by Liu et al. [55] to ensure that our assessment is replicable.

5.4.1 Code completion for patch generation. Given the naturalness of software [26], we postulate
that it should be possible to statically predict missing elements just like one predicts missing
words in natural language text. Fortunately, such an idea has been very recently explored by Alon
et al. with their approach for code generation with structural language modeling (SLM) [6]. While
other state of the art such as seq2seq [78], code2seq [5] and structured prediction [16] are valid
alternatives for our code completion task, we build in this work on SLM: its AnyCodeGen trained
model was experimentally shown to significantly outperform a variety of strong baselines on Java
and C# languages for the task of code completion [6].

Once Beep yields the predicted operation path, we parse the change operator and the token
(buggy element). If the operator is DELETE, then we simply generate a patch that removes this token
from the method at the position identified by the predicted AST path. If the operator is UPDATE,
however, we create a hole (represented by the ‚Äú??‚Äù notation3) within the buggy method at the
position of the identified token for the predicted AST path. The holed code thus constitutes an
input for querying the AnyCodeGen structural language model-based engine. Finally, if the change
operator is INSERT, we simply add a placeholder (still represented by the ‚Äú??‚Äù notation) to the
method, again forming a query to the code completion engine.

5.4.2 Heuristics-based repair pipeline. In this implementation, we generate patches via using the
following heuristics:

‚Ä¢ If the predicted operation is DELETE, we directly remove this token.
‚Ä¢ If the predicted operation is INSERT or UPDATE, we transform the program using the following

rules:
‚Äì if the token is an operator, we change it to another one in the operator set (e.g., == ‚Üí != and

>= ‚Üí >);

‚Äì if the token is a boolean value, we change it to its reverse (e.g., false ‚Üí true);
‚Äì if the token is a data type, we change it to another type (e.g., int ‚Üí float);
‚Äì if the token is an identifier, we replace it or pad the location with its 5-nearest4 identifiers with
the same type. We consider generating only 5 candidates to remain comparable to AnyCodeGen
code completion engine which only returns 5 code completion results.

3This notation is recognized by AnyCodeGen as a program hole to be completed.
4Distance is computed within the token sequence of the buggy method.

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

16

Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, and Tegawend√© F. Bissyand√©

Table 6. Comparison with state-of-the-art APR tools on fixing real-world bugs.

APR Tool
JAID [18]
CapGen [89]
ACS [96]
FixMiner [37]
SimFix [27]
DLFix [46]
TBar [54]
CURE [28]
Beep + Code completion
Beep + Heuristics

Defects4J
25/31
21/25
18/23
25/31
34/56
30/65
43/81
57/104
16/16 (2)
21/21 (2)

Bears
-
-
-
-
-
-
-
-
2/2 (2)
0/0 (0)

QuixBugs
-
-
-
-
-
-
-
26/35
4/4 (0)
5/5 (0)

Bugs.jar
-
-
-
-
-
-
-
-
5/5 (4)
6/6 (4)

CR(%)
80.6%
84,0%
78.3%
80.6%
60.7%
46.2%
53.1%
59.7%
100%
100%

Data of other tools are extracted from the original papers. ‚Äò-‚Äô denotes no relevant data. x/y represents the tool generates y
plausible patches among which x are correct on this benchmark. Numbers in the parentheses denote the numbers of bugs

which are not fixed by the state-of-the-art before.
Analysis of repairability. We provide in Table 6 the performance of our repair pipelines on
the four defect benchmarks. We also compare their performance against the results of 26 APR
techniques reported in recent literature [28]. Due to space limitation, we only include in this table
the most representative tools w.r.t. yielding highest precision (i.e., high correctness ratio - CR -
above part of the table) and w.r.t. repairability (i.e., the highest number of bugs fixed - below part
of the table).

Overall, we find that our pipelines can both achieve 100% precision: all generated patches that
passed the test suite were found to be correct. Actually, during the manual check, we found that
they were all identical to the ground-truth patches. In comparison, the APR tool with the highest
precision in the literature, i.e., CapGen, has a correctness ratio of 84%. We also note that our
pipeline provides comparable performance with respect to the number of correctly fixed bugs when
compared with the first four tools. For instance, ACS can only correctly fix 18 bugs while Beep
(with heuristics-based patch generation) can correctly fix 21 bugs. On the other hand, although
tools with high repairability metrics outperform our pipelines in terms of the number of correctly
fixed bugs, their overall precision is low. For example, DLFix plausible patches even include more
incorrect ones than correct ones. Since in practice developers should check that the correctness of
generated patches before integrating them, it may be detrimental to use APR if the correctness
ratio is low.

We also note that when using simple heuristics for patch generation, we can fix more bugs (32 vs.
27) than when relying on a carefully-designed and fully-trained sophisticated Deep Learning model
for code completion (i.e., AnyCodeGen). Our results suggest that, for program repair, finding donor
code within the same method may be more effective than predicting missing tokens using big data.
Moreover, we found that our pipelines are able to fix bugs which were never yet reported to be
fixed by the considered 26 APR tools. 2 of such bugs can be found in the Defects4J dataset. Beep +
code completion also led to fixing 2 bugs from Bears as well as 4 bugs from Bugs.jar which were not
fixed by the state-of-the-art tools (such as ARJA [100]) which were applied on these benchmark.
These results further demonstrate token-level-based program repair is a sweet spot for developing
further directions in generate-and-validate APR.

[RQ-4]‚û≤ ‚ë† Automated program repair built on the top of Beep localization results can produce
patches with high correctness ratio, hence providing a novel perspective to address patch overfitting
in program repair.

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements

17

Analysis of patch generation efficiency. In this section, we consider only the Beep plus heuris-
tics repair pipeline since it is more effective. Repair efficiency assessment considers the number of
patch candidates (NPC) that are generated and tested before a plausible patch is hit. NPC score was
recently proposed by Liu et al. [55] to enable fair comparison across computing environments. They
provide the NPC score of several state-of-the-art tools for the Defects4J bugs with the assumption
that each APR is provided with the buggy line. We also place the proposed pipeline under this
setting. Fig. 6 compares the distribution of NPC of the pipeline vs. 16 APR tools considered by
Liu et al. [55] on Defects4J bugs.

Fig. 6. Comparison of efficiency.

With a median NPC score of 2, our pipeline is more efficient than pattern-based APR techniques
(e.g., TBar and kPAR) and even more recent state of the art heuristic-based APR techniques (e.g.,
SimFix). Nevertheless, six tools significantly outperform this pipeline in terms of efficiency: three
(i.e., DynaMoth, Nopol, ACS) actually use constraint-solving and synthesis strategies, which indeed
make them efficient (see [55]); the other three (i.e., jKali, Kali-A, and jMutRepair) apply naive repair
strategies that limit their search spaces ‚Äì jKali and Kali-A change conditional statements into true
or false and jMutRepair only mutate operators.
[RQ-4]‚û≤ ‚ë° Successful prediction of the buggy code element and adequate change operator by Beep
limits the number of patch candidates, hence improving the efficiency of patch generation.

6 DISCUSSION
Influence of bug occurrence: Beep, as similar data-driven approaches [46], performs poorly on
infrequent bugs that are not common in programs or their similar bugs are very rarely occurred.
Indeed, features of such bugs are generally not well captured in model training.
Declaration vs body of methods: Buggy method declarations are challenging for Beep. For
example, in Defects4J bug Lang-29, the return type of the method should be float instead of int.

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

RSRepair‚àíA0306090120150180‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óèjGenProgGenProg‚àíAjMutRepairkPARjKali Kali‚àíA DynaMoth Nopol ACSCardumenARJA SimFixFixMinerAVATAR TBar BEEP+Heuristics051015202530# of patch candidates18

Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, and Tegawend√© F. Bissyand√©

Given that such bugs have short AST paths (2 nodes), the model fails to learn any relevant features
for predicting the buggy elements.
The ability to fix multi-location bugs: Despite that, the majority of bugs which can be fixed
by Beep with the heuristics are about the modification of a single code token, we did note that
this pipeline can fix multi-location bugs. Totally, our pipeline fix two multi-location bugs from
Defects4J dataset. A case is illustrated in Figure 7. Fixing this bug requires to change the data type
int into double in two locations. After Beep accurately predicts the two buggy elements (and the
operators) within its Top 2, our heuristics successfully generate the correct patch since it allows to
change the data type. Another instance is Time-4 where our pipeline precisely predicts the deletion
of a token with an insertion in another location.

1
2
3
4
5
6
7
8
9
10
11
12

// Ground-truth patch for Math-79

public static double distance(int[] p1, int[] p2) {

-
+

-
+

int sum = 0;
double sum = 0;
for (int i = 0; i < p1.length; i++) {
final int dp = p1[i] - p2[i];
final double dp = p1[i] - p2[i];
sum += dp * dp;

}
return Math.sqrt(sum);

}

Fig. 7. The ground-truth patch for the bug Math-79.

Threats to Validity: Our experimental results carry some threats to validity, notably ‚ù∂ its gener-
alization beyond the Java programming language. Although the theoretical design of Beep is valid
for any language whose AST can be readily generated (e.g., C/C++), we selected Java due to the
availability of large-scale off-the-shelf datasets and benchmarks, and the possibility to compare
against some open released state of the art tools. As a second threat, we note that ‚ù∑ Beep was trained
and evaluated with two recent large-scale datasets with a significant proportion of single-statement
patches. Such a dataset may be biased or non-representative. We plan to extend the evaluations to
more kinds of bugs by collecting more diverse datasets. As a third threat, ‚ù∏ Defects4J contains bug
cases where the fix consists in adding or deleting whole statements. The localization of such bugs
is a good target for spectrum-based fault localization, but not of Beep because we did not consider
such cases in our training set (borrowed from CoCoNut). As a forth threat, ‚ùπ Beep is built on an
assumption of the available information of fault methods/statements, that is not practical. Beep is
a token-level fault localization built on top of the statement-level/method-level fault localization
tools. The assumption is mainly used to evaluate the possibility of predicting the exact buggy code
elements for program repair within a limited search space of suspicious bug locations. As a fifth
threat, ‚ù∫ the AST/Operation paths created by Beep do not have long length, which arises a threat
for Beep that the impact from the long path on the learning model cannot be assessed. We list it
as a part of future work for improving Beep. Finally, ‚ùª different embeddings of the sub-tokens,
AST path, and operations can impact the performance of the deep learning models, which could
further influence the performance of Beep, nonetheless, this work mainly focuses on exploring
the possibility of fine-grained fix localization for program repair. The influence of the different
embeddings will be investigated in future work.

7 RELATED WORK
Developers‚Äô Opinion on Fault Localization. A number of studies investigate developers‚Äô per-
spectives on current FL techniques. Parnin and Orso [68] find that several assumptions made by
automated debugging techniques do not hold in practice for developers (e.g., ‚Äúexamining a faulty

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements

19

statement in isolation is enough for a developer‚Äù). Xie et al. [93] find that simply providing the
ranked suspicious code lines for developers actually reduces their debugging efficiency. Kochhar
et al. [34] find that the most popular FL granularity so far (i.e., method level) only gains prefer-
ences from around half of the investigated practitioners. Our work provides developers with a
finer-grained FL results by targeting the buggy code tokens as well as the required change operator.
We expect Beep research direction to contribute to reducing the burden of manual debugging in
practice.
Learning-Based Fault Localization. With the power of advanced neural networks, a number
of works have been proposed for performing fault localization based on machine/deep learning.
Wong and Qi [92] propose a fault localization approach based on the back-propagation (BP)
neural network. MULTRIC [97] is the first learning-to-rank fault localization technique that can
integrate the suspiciousness values calculated with spectrum-based fault localization techniques
(SBFL) to improve the accuracy of SBFL [108]. Since then, program invariant [10] and source
code complexity information [77] have been explored to combine the SBFL suspiciousness values
for more effective learning-to-rank fault localization. These learning-to-rank fault localization
techniques aim to rank faulty statements higher than correct ones, while other learning based
fault localization techniques focus on the test coverage information [15, 92, 106, 107] that cannot
distinguish elements accidentally executed by failed tests and the actual faulty elements [45]. Zhang
et al. [105] construct a convolutional neural network customized for fault localization. Li et al. [44]
propose to leverage the traditional MLP and RNN networks to learn the effective existing/latent
features (collected from the various suspiciousness-value-based, fault-proneness-based and textual-
similarity-based features from the fault localization, defect prediction and information retrieval
areas) for precise fault localization. Li et al. [47] treat FL as an image pattern recognition problem
and achieve so via novel code coverage representation learning (RL) and data dependencies RL for
program statements. These works all locate the buggy code at the statement and method levels
which is still coarse-grained, our work explores to leverage the deep learning techniques to predict
the buggy code elements in buggy statements for program repair.
Fault Localization for APR. An essential step in APR is identifying the locations of buggy code
within the program to be repaired [14]. These locations are the target for selecting code entities that
must be transformed to generate patches. As reported in recent work [52], the commonly-adopted
FL configuration (with GZoltar and Ochiai) still provides too inaccurate bug localization information
for automated program repair. Therefore, researchers try to enhance FL for APR with predicate
switching [96, 103], test case purification [27], deep learning [44] and other information [88, 102].
Most APR tools leverage localization information at the granularity of code lines. To the best of our
knowledge, our work is the first to target the identification of buggy code elements nested within
buggy lines, thus providing a more fine-grained localization for APR. As reported by previous
works [52, 55], the accuracy of FL results can cause significant differences in repair performances. In
detail, accurate FL can enhance the efficiency as well as the precision of APR tools. Our evaluation
with a fine-grained token-level FL also demonstrates this point.
APR for Fault Localization. Previously, the connection between the two key points in software
debugging (i.e., fault localization and program repair) is that program repair techniques usually
use off-the-shelf fault localization techniques to identify potential buggy locations for patching
[27, 89]. Recently, however, Lou et al. [59] propose to utilize the patch execution information
during program repair for providing fault localization with useful feedback. A follow-up work [12]
further demonstrated that the effectiveness of this approach can be enhanced when integrating
patch generation information from diverse APR systems. Nevertheless, these works target accurate

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

20

Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, and Tegawend√© F. Bissyand√©

localization at the method level. They can therefore be complemented by Beep to achieve fine-
grained FL (i.e., fix localization).
State-of-the-art APR. The APR community has explored various APR techniques to address
the bugs in different language-written programs. One of the commonly studied approaches is
generate-and-validate program repair. Since GenProg [87] created the milestone of program repair,
which was proposed to solve C program bugs with a generic method, various APR techniques have
been proposed in the literature that can be categorized into four main categories: heuristic-based,
constraint-based, and template-based and learning based repair approaches [42]. Heuristic-based
repair approaches construct and iterate over a search space of syntactic program modifications,
e.g., SimFix [27]. Constraint-based repair approaches generally leverage the specific technique (e.g.,
constraint solving) to infer the constraints of the given buggy program, which are further used to
guide the generation and validation of patches, e.g., SemFix [66]. Template-based repair approaches
utilize the fix patterns identified from the real-world patches, that can provide the dedicated code
change behaviors for bug fixing, e.g., TBar [54]. Learning-based repair approaches rely on machine
learning or deep learning techniques to learn correct fix ingredients for patch generation, e.g.,
Prophet [58]. To date, the state-of-the-art APR research work mainly focuses on the bugs in C/C++
programs [22, 56, 58, 64, 66, 79] and Java programs [32]. This work is to provide the fine-grained
fault localization for APR to reduce the possibility of generating plausible but incorrect patches
because of the coarse fault localization.
Deep Learning for APR. A considerable number of studies have utilized deep learning techniques
for program repair.
‚û§ SequenceR [19], DLFix [46], CoCoNut [60], and CURE [28] work at the line level, thus requiring
as input the exact buggy statement, an information granularity that current fault localization
inaccurately provides. In contrast, our repair pipeline only targets buggy methods, a granularity
that is more accessible to off-the-shelf fault localization tools [12, 59].
‚û§ Although the approach of Tufano et al. [83] works with methods, they have to disregard methods
with more than 50 tokens due to the performance limitation of their seq2seq translation. In contrast,
we accept any-size methods.
‚û§ Allamanis et al. [3] use graph-based deep learning while Vasic et al. [84] adopt multi-headed
pointer networks for repairing bugs. Hellendoorn et al. [25] further propose a hybrid model by
analyzing the weakness of the previous two works. The limitation of these works is that they only
focus on variable-misuse bugs. While they propose to localize variable tokens, our work targets
any buggy token (and its associated change operator) within a method.
‚û§ Some other approaches [13, 24, 65, 73, 74] focus on learning to fix defects related to the pro-
gramming language syntax, some of which only apply to small-scale student programs [13, 73]. In
contrast, our work targets semantic defects in large-scale real-world programs.
‚û§ Phoenix [11] also synthesizes repairs for bugs. However, it uses static analysis as an oracle while
we rely on test cases.

8 CONCLUSION
We tackle the ambition of implementing fix localization as the fine-grained identification of buggy
code elements. Our Beep neural network-based architecture can accurately predict buggy tokens
and the required change operator. It can thus save great efforts for manual debugging. We also
assessed two intuitive repair pipelines with Beep, where the identified buggy code elements are
considered as missing and, thus, must be replaced via using either a code completion engine or a set
of code search heuristics. We explore the bug fixing performance of the proposed pipelines on bugs
across several benchmarks. Results reveal that all patches generated by the two pipelines are found

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements

21

to be correct. Hence, program repair built on top of fine-grained fix localization offers a promising
perspective to address the long-standing challenge of overfitting in automated program repair.
Artefacts: All data and source code in the study are publicly available at:

http://doi.org/10.5281/zenodo.4717352.

REFERENCES

[1] Rui Abreu, Peter Zoeteweij, and Arjan JC Van Gemund. 2007. On the accuracy of spectrum-based fault localization.

In Testing: Academic and Industrial Conference Practice and Research Techniques-MUTATION. IEEE, 89‚Äì98.

[2] Miltiadis Allamanis, Earl T Barr, Christian Bird, and Charles Sutton. 2015. Suggesting accurate method and class

names. In Proceedings of the 10th Joint Meeting on Foundations of Software Engineering. ACM, 38‚Äì49.

[3] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning to Represent Programs with Graphs.

In Proceedings of the 6th International Conference on Learning Representations. OpenReview.net.

[4] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Generating sequences from structured

representations of code. arXiv preprint arXiv:1808.01400 (2018).

[5] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating Sequences from Structured Repre-
sentations of Code. In Proceedings of the 7th International Conference on Learning Representations. OpenReview.net.
[6] Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. 2020. Structural language models of code. In Proceedings of 37th

International Conference on Machine Learning. PMLR, 245‚Äì256.

[7] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. A general path-based representation for predicting
program properties. In Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and
Implementation. ACM, 404‚Äì419. https://doi.org/10.1145/3192366.3192412

[8] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: learning distributed representations of
code. Proceedings of the ACM on Programming Languages 3, POPL (2019), 40:1‚Äì40:29. https://doi.org/10.1145/3290353
[9] Andrea Arcuri and Lionel C. Briand. 2011. A practical guide for using statistical tests to assess randomized algorithms
in software engineering. In Proceedings of the 33rd International Conference on Software Engineering. ACM, 1‚Äì10.
[10] Tien-Duy B. Le, David Lo, Claire Le Goues, and Lars Grunske. 2016. A learning-to-rank based fault localization
approach using likely invariants. In Proceedings of the 25th International Symposium on Software Testing and Analysis.
177‚Äì188.

[11] Rohan Bavishi, Hiroaki Yoshida, and Mukul R. Prasad. 2019. Phoenix: Automated Data-Driven Synthesis of Repairs
for Static Analysis Violations. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering. ACM, 613‚Äì624.

[12] Samuel Benton, Xia Li, Yiling Lou, and Lingming Zhang. 2020. On the Effectiveness of Unified Debugging: An
Extensive Study on 16 Program Repair Systems. In Proceedings of the 35th IEEE/ACM International Conference on
Automated Software Engineering. IEEE, 907‚Äì918.

[13] Sahil Bhatia, Pushmeet Kohli, and Rishabh Singh. 2018. Neuro-Symbolic Program Corrector for Introductory
Programming Assignments. In Proceedings of the IEEE/ACM 40th International Conference on Software Engineering.
60‚Äì70.

[14] Marcel B√∂hme, Ezekiel O. Soremekun, Sudipta Chattopadhyay, Emamurho Ugherughe, and Andreas Zeller. 2017.
Where is the Bug and How is It Fixed? An Experiment with Practitioners. In Proceedings of the 11th Joint Meeting on
Foundations of Software Engineering. ACM, 117‚Äì128.

[15] Lionel C Briand, Yvan Labiche, and Xuetao Liu. 2007. Using machine learning to support debugging with tarantula.

In Proceedings of the 18th IEEE International Symposium on Software Reliability. IEEE, 137‚Äì146.

[16] Marc Brockschmidt, Miltiadis Allamanis, Alexander L Gaunt, and Oleksandr Polozov. 2018. Generative code modeling

with graphs. arXiv preprint arXiv:1805.08490 (2018).

[17] Shaked Brody, Uri Alon, and Eran Yahav. 2020. A structural model for contextual code changes. Proceedings of the

ACM on Programming Languages 4, OOPSLA (2020), 1‚Äì28.

[18] Liushan Chen, Yu Pei, and Carlo A Furia. 2017. Contract-based program repair without the contracts. In Proceedings

of the 32nd IEEE/ACM International Conference on Automated Software Engineering. 637‚Äì647.

[19] Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-No√´l Pouchet, Denys Poshyvanyk, and Martin Mon-
perrus. 2019. Sequencer: Sequence-to-sequence learning for end-to-end program repair. IEEE Trans. on Software
Engineering (2019).

[20] Rhys Compton, Eibe Frank, Panos Patros, and Abigail Koay. 2020. Embedding Java Classes with code2vec: Improve-

ments from Variable Obfuscation. In Proceedings of the 17th Mining Software Repositories. ACM, 243‚Äì253.

[21] Jean-R√©my Falleri, Flor√©al Morandat, Xavier Blanc, Matias Martinez, and Martin Monperrus. 2014. Fine-grained
and accurate source code differencing. In Proceedings of the 29th ACM/IEEE International Conference on Automated
Software Engineering. ACM, 313‚Äì324. https://doi.org/10.1145/2642937.2642982

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

22

Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, and Tegawend√© F. Bissyand√©

[22] Xiang Gao, Sergey Mechtaev, and Abhik Roychoudhury. 2019. Crash-avoiding program repair. In Proceedings of the

28th ACM SIGSOFT International Symposium on Software Testing and Analysis. 8‚Äì18.

[23] Ali Ghanbari, Samuel Benton, and Lingming Zhang. 2019. Practical program repair via bytecode mutation. In
Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis. ACM, 19‚Äì30.
[24] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. DeepFix: Fixing common C language errors by

deep learning. In Proceedings of the 31st AAAI Conference on Artificial Intelligence. AAAI, 1345‚Äì1351.

[25] Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. 2020. Global Relational
Models of Source Code. In Proceedings of the 8th International Conference on Learning Representations. OpenReview.net.
[26] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar T. Devanbu. 2012. On the naturalness
of software. In Proceedings of the 34th International Conference on Software Engineering. IEEE, 837‚Äì847. https:
//doi.org/10.1109/ICSE.2012.6227135

[27] Jiajun Jiang, Yingfei Xiong, Hongyu Zhang, Qing Gao, and Xiangqun Chen. 2018. Shaping program repair space
with existing patches and similar code. In Proceedings of the 27th ACM SIGSOFT International Symposium on Software
Testing and Analysis. ACM, 298‚Äì309. https://doi.org/10.1145/3213846.3213871

[28] Nan Jiang, Thibaud Lutellier, and Lin Tan. 2021. CURE: Code-Aware Neural Machine Translation for Automatic
Program Repair. In Proceedings of the 43rd International Conference on Software Engineering. IEEE, 1161‚Äì1173. https:
//doi.org/10.1109/ICSE43902.2021.00107

[29] James A Jones, Mary Jean Harrold, and John Stasko. 2002. Visualization of test information to assist fault localization.

In Proceedings of the 24th International Conference on Software Engineering. 467‚Äì477.

[30] Ren√© Just, Darioush Jalali, and Michael D Ernst. 2014. Defects4J: A database of existing faults to enable controlled
testing studies for Java programs. In Proceedings of the 23rd International Symposium on Software Testing and Analysis.
ACM, 437‚Äì440. https://doi.org/10.1145/2610384.2628055

[31] Rafael-Michael Karampatsis and Charles A. Sutton. 2020. How Often Do Single-Statement Bugs Occur? The
ManySStuBs4J Dataset. In Proceedings of the 17th Mining Software Repositories. ACM, 573‚Äì577. https://doi.org/
10.1145/3379597.3387491

[32] Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim. 2013. Automatic patch generation learned from
human-written patches. In Proceedings of the 35th International Conference on Software Engineering. IEEE, 802‚Äì811.
https://doi.org/10.1109/ICSE.2013.6606626

[33] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980

(2014).

[34] Pavneet Singh Kochhar, Xin Xia, David Lo, and Shanping Li. 2016. Practitioners‚Äô expectations on automated fault
localization. In Proceedings of the 25th International Symposium on Software Testing and Analysis. ACM, 165‚Äì176.
[35] Ron Kohavi. 1995. A study of cross-validation and bootstrap for accuracy estimation and model selection. In
Proceedings of the 40th International Joint Conference on Artificial Intelligence. Montreal, Canada, 1137‚Äì1145.
[36] Anil Koyuncu, Tegawend√© F Bissyand√©, Dongsun Kim, Kui Liu, Jacques Klein, Martin Monperrus, and Yves Le Traon.
2019. D&C: A Divide-and-Conquer Approach to IR-based Bug Localization. arXiv preprint arXiv:1902.02703 (2019).
[37] Anil Koyuncu, Kui Liu, Tegawend√© F. Bissyand√©, Dongsun Kim, Jacques Klein, Martin Monperrus, and Yves Le Traon.
2020. FixMiner: Mining relevant fix patterns for automated program repair. Empirical Software Engineering 25, 3
(2020), 1980‚Äì2024. https://doi.org/10.1007/s10664-019-09780-z

[38] Xuan Bach D Le, David Lo, and Claire Le Goues. 2016. History driven program repair. In Proceedings of the 23rd IEEE
International Conference on Software Analysis, Evolution, and Reengineering. 213‚Äì224. https://doi.org/10.1109/SANER.
2016.76

[39] Claire Le Goues, Michael Dewey-Vogt, Stephanie Forrest, and Westley Weimer. 2012. A systematic study of automated
program repair: Fixing 55 out of 105 bugs for $8 each. In Proceedings of the 34th International Conference on Software
Engineering. IEEE, 3‚Äì13. https://doi.org/10.1109/ICSE.2012.6227211

[40] Claire Le Goues, Stephanie Forrest, and Westley Weimer. 2013. Current challenges in automatic software repair.

Software Quality Journal 21, 3 (2013), 421‚Äì443. https://doi.org/10.1007/s11219-013-9208-0

[41] Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer. 2012. GenProg: A generic method for
automatic software repair. IEEE Transactions on Software Engineering 38, 1 (2012), 54‚Äì72. https://doi.org/10.1109/TSE.
2011.104

[42] Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. 2019. Automated Program Repair. Commun. ACM 62, 12

(2019), 56‚Äì65. https://doi.org/10.1145/3318162

[43] Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li. 2020. A survey on deep learning for named entity recognition.

IEEE Transactions on Knowledge and Data Engineering (2020). https://doi.org/10.1109/TKDE.2020.2981314

[44] Xia Li, Wei Li, Yuqun Zhang, and Lingming Zhang. 2019. Deepfl: Integrating multiple fault diagnosis dimensions for
deep fault localization. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and
Analysis. 169‚Äì180.

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements

23

[45] Xia Li and Lingming Zhang. 2017. Transforming programs and tests in tandem for fault localization. Proceedings of

the ACM on Programming Languages 1, OOPSLA (2017), 1‚Äì30.

[46] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2020. DLFix: context-based code transformation learning for automated
program repair. In Proceedings of the 42nd International Conference on Software Engineering. 602‚Äì614. https://doi.org/
10.1145/3377811.3380345

[47] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2021. Fault Localization with Code Coverage Representation Learning. In

Proceedings of the 43rd International Conference on Software Engineering. IEEE, 661‚Äì673.

[48] Yi Li, Shaohua Wang, Tien N Nguyen, and Son Van Nguyen. 2019. Improving bug detection via context-based code
representation learning and attention-based neural networks. Proceedings of the ACM on Programming Languages 3,
OOPSLA (2019), 162:1‚Äì162:30. https://doi.org/10.1145/3360588

[49] Ben Liblit, Mayur Naik, Alice X Zheng, Alex Aiken, and Michael I Jordan. 2005. Scalable statistical bug isolation.

ACM Sigplan Notices 40, 6 (2005), 15‚Äì26.

[50] Derrick Lin, James Koppel, Angela Chen, and Armando Solar-Lezama. 2017. QuixBugs: A multi-lingual program repair
benchmark set based on the Quixey Challenge. In Proceedings Companion of the 2017 ACM SIGPLAN International
Conference on Systems, Programming, Languages, and Applications: Software for Humanity. ACM, 55‚Äì56. https:
//doi.org/10.1145/3135932.3135941

[51] Kui Liu, Dongsun Kim, Anil Koyuncu, Li Li, Tegawend√© F Bissyand√©, and Yves Le Traon. 2018. A closer look at
real-world patches. In Proceedings of the 34th International Conference on Software Maintenance and Evolution. IEEE,
275‚Äì286. https://doi.org/10.1109/ICSME.2018.00037

[52] Kui Liu, Anil Koyuncu, Tegawend√© F Bissyand√©, Dongsun Kim, Jacques Klein, and Yves Le Traon. 2019. You cannot fix
what you cannot find! an investigation of fault localization bias in benchmarking automated program repair systems.
In Proceedings of the 12th IEEE International Conference on Software Testing, Verification and Validation. IEEE, 102‚Äì113.
https://doi.org/10.1109/ICST.2019.00020

[53] Kui Liu, Anil Koyuncu, Dongsun Kim, and Tegawend√© F Bissyand√©. 2019. AVATAR: Fixing semantic bugs with fix
patterns of static analysis violations. In Proceedings of the 26th IEEE International Conference on Software Analysis,
Evolution and Reengineering. IEEE, 456‚Äì467. https://doi.org/10.1109/SANER.2019.8667970

[54] Kui Liu, Anil Koyuncu, Dongsun Kim, and Tegawend√© F. Bissyand√©. 2019. TBar: Revisiting Template-based Automated
Program Repair. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis.
ACM, 31‚Äì42. https://doi.org/10.1145/3293882.3330577

[55] Kui Liu, Shangwen Wang, Anil Koyuncu, Kisub Kim, Tegawend√© F. Bissyand√©, Dongsun Kim, Peng Wu, Jacques
Klein, Xiaoguang Mao, and Yves Le Traon. 2020. On the Efficiency of Test Suite based Program Repair: A Systematic
Assessment of 16 Automated Repair Systems for Java Programs. In Proceedings of the 42nd International Conference on
Software Engineering. ACM, 615‚Äì627. https://doi.org/10.1145/3377811.3380338

[56] Fan Long and Martin Rinard. 2015. Staged program repair with condition synthesis. In Proceedings of the 10th Joint

Meeting on Foundations of Software Engineering. ACM, 166‚Äì178. https://doi.org/10.1145/2786805.2786811

[57] Fan Long and Martin Rinard. 2016. An analysis of the search spaces for generate and validate patch generation
systems. In Proceedings of the 38th International Conference on Software Engineering. IEEE, 702‚Äì713. https://doi.org/
10.1145/2884781.2884872

[58] Fan Long and Martin Rinard. 2016. Automatic patch generation by learning correct code. In Proceedings of the
43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, Vol. 51. ACM, 298‚Äì312.
https://doi.org/10.1145/2837614.2837617

[59] Yiling Lou, Ali Ghanbari, Xia Li, Lingming Zhang, Haotian Zhang, Dan Hao, and Lu Zhang. 2020. Can automated
program repair refine fault localization? a unified debugging approach. In Proceedings of the 29th ACM SIGSOFT
International Symposium on Software Testing and Analysis. ACM, 75‚Äì87. https://doi.org/10.1145/3395363.3397351
[60] Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and Lin Tan. 2020. CoCoNuT: combining
context-aware neural translation models using ensemble for program repair. In Proceedings of the 29th ACM SIGSOFT
International Symposium on Software Testing and Analysis. ACM, 101‚Äì114. https://doi.org/10.1145/3395363.3397369
[61] Fernanda Madeiral, Simon Urli, Marcelo Maia, and Martin Monperrus. 2019. BEARS: An Extensible Java Bug
Benchmark for Automatic Program Repair Studies. In Proceedings of the 26th International Conference on Software
Analysis, Evolution and Reengineering. IEEE, 468‚Äì478. https://doi.org/10.1109/SANER.2019.8667991

[62] Matias Martinez and Martin Monperrus. 2016. ASTOR: a program repair library for Java (demo). In Proceedings of
the 25th International Symposium on Software Testing and Analysis. ACM, 441‚Äì444. https://doi.org/10.1145/2931037.
2948705

[63] Wolfgang Mayer and Markus Stumptner. 2008. Evaluating models for model-based debugging. In Proceedings of the

23rd IEEE/ACM International Conference on Automated Software Engineering. IEEE, 128‚Äì137.

[64] Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2016. Angelix: Scalable multiline program patch synthesis
via symbolic analysis. In Proceedings of the 38th International Conference on Software Engineering. ACM, 691‚Äì701.

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

24

Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, and Tegawend√© F. Bissyand√©

https://doi.org/10.1145/2884781.2884807

[65] Ali Mesbah, Andrew Rice, Emily Johnston, Nick Glorioso, and Edward Aftandilian. 2019. DeepDelta: learning to repair
compilation errors. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering. 925‚Äì936.

[66] Hoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chandra. 2013. SemFix: Program repair
via semantic analysis. In Proceedings of the 35th International Conference on Software Engineering. IEEE, 772‚Äì781.
https://doi.org/10.1109/ICSE.2013.6606623

[67] Mike Papadakis and Yves Le Traon. 2015. Metallaxis-FL: mutation-based fault localization. Software Testing, Verification

and Reliability 25, 5-7 (2015), 605‚Äì628.

[68] Chris Parnin and Alessandro Orso. 2011. Are automated debugging techniques actually helping programmers?. In

Proceedings of the 2011 International Symposium on Software Testing and Analysis. 199‚Äì209.

[69] Spencer Pearson, Jos√© Campos, Ren√© Just, Gordon Fraser, Rui Abreu, Michael D. Ernst, Deric Pang, and Benjamin
Keller. 2017. Evaluating and improving fault localization. In Proceedings of the 39th International Conference on
Software Engineering. ACM, 609‚Äì620. https://doi.org/10.1109/ICSE.2017.62

[70] Zichao Qi, Fan Long, Sara Achour, and Martin Rinard. 2015. An analysis of patch plausibility and correctness for
generate-and-validate patch generation systems. In Proceedings of the 24th International Symposium on Software
Testing and Analysis. ACM, 24‚Äì36. https://doi.org/10.1145/2771783.2771791

[71] Reuven Y. Rubinstein. 1999. The Cross-Entropy Method for Combinatorial and Continuous Optimization. Methodology

And Computing In Applied Probability 1 (1999), 127‚Äì190.

[72] Ripon Saha, Yingjun Lyu, Wing Lam, Hiroaki Yoshida, and Mukul R. Prasad. 2018. Bugs.jar: A large-scale, diverse
dataset of real-world java bugs. In Proceedings of the 15th IEEE/ACM International Conference on Mining Software
Repositories. ACM, 10‚Äì13. https://doi.org/10.1145/3196398.3196473

[73] Eddie Antonio Santos, Hazel Victoria Campbell, Dhvani Patel, Abram Hindle, and Jos√© Nelson Amaral. 2018. Syntax
and sensibility: Using language models to detect and correct syntax errors. In Proceedings of the IEEE 25th International
Conference on Software Analysis, Evolution and Reengineering. 311‚Äì322.

[74] Eddie A Santos, Joshua C Campbell, Abram Hindle, and Jos√© Nelson Amaral. 2017. Finding and correcting syntax errors
using recurrent neural networks. PeerJ Preprints 5 (2017), e3123v1. https://doi.org/10.7287/peerj.preprints.3123v1
[75] Ridwan Salihin Shariffdeen, Shin Hwei Tan, Mingyuan Gao, and Abhik Roychoudhury. 2021. Automated Patch

Transplantation. ACM Transactions on Software Engineering and Methodology 30, 1 (2021), 6:1‚Äì6:36.

[76] Edward K Smith, Earl T Barr, Claire Le Goues, and Yuriy Brun. 2015. Is the cure worse than the disease? overfitting
in automated program repair. In Proceedings of the 10th Joint Meeting on Foundations of Software Engineering. ACM,
532‚Äì543. https://doi.org/10.1145/2786805.2786825

[77] Jeongju Sohn and Shin Yoo. 2017. Fluccs: Using code and change metrics to improve fault localization. In Proceedings

of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis. 273‚Äì283.

[78] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances

in Neural Information Processing Systems. 3104‚Äì3112.

[79] Shin Hwei Tan and Abhik Roychoudhury. 2015. relifix: Automated repair of software regressions. In Proceedings of

the IEEE/ACM 37th IEEE International Conference on Software Engineering, Vol. 1. IEEE, 471‚Äì482.

[80] Shin Hwei Tan, Hiroaki Yoshida, Mukul R Prasad, and Abhik Roychoudhury. 2016. Anti-patterns in search-based
program repair. In Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software
Engineering. ACM, 727‚Äì738.

[81] Yida Tao, Jindae Kim, Sunghun Kim, and Chang Xu. 2014. Automatically generated patches as debugging aids: a
human study. In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering.
ACM, 64‚Äì74.

[82] Haoye Tian, Kui Liu, Abdoul Kader Kabor√©, Anil Koyuncu, Li Li, Jacques Klein, and Tegawend√© F. Bissyand√©.
2020. Evaluating Representation Learning of Code Changes for Predicting Patch Correctness in Program Repair. In
Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering. IEEE, 981‚Äì992.
[83] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. 2019.
An empirical study on learning bug-fixing patches in the wild via neural machine translation. ACM Transactions on
Software Engineering and Methodology 28, 4 (2019), 19:1‚Äì19:29. https://doi.org/10.1145/3340544

[84] Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh. 2019. Neural program repair by

jointly learning to localize and repair. arXiv preprint arXiv:1904.01720 (2019).

[85] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Proceedings of the 29th Advances in

Neural Information Processing Systems. 2692‚Äì2700.

[86] Shangwen Wang, Ming Wen, Bo Lin, Hongjun Wu, Yihao Qin, Deqing Zou, Xiaoguang Mao, and Hai Jin. 2020.
Automated Patch Correctness Assessment: How Far are We?. In Proceedings of the 35th IEEE/ACM International
Conference on Automated Software Engineering. IEEE, 968‚Äì980.

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements

25

[87] Westley Weimer, ThanhVu Nguyen, Claire Le Goues, and Stephanie Forrest. 2009. Automatically finding patches
using genetic programming. In Proceedings of the 31st International Conference on Software Engineering. IEEE, 364‚Äì374.
https://doi.org/10.1109/ICSE.2009.5070536

[88] Ming Wen, Junjie Chen, Yongqiang Tian, Rongxin Wu, and Shing-Chi Cheung. 2019. Historical Spectrum based Fault

Localization. IEEE Transactions on Software Engineering PP, 99 (2019), 1‚Äì1.

[89] Ming Wen, Junjie Chen, Rongxin Wu, Dan Hao, and Shing-Chi Cheung. 2018. Context-aware patch generation for
better automated program repair. In Proceedings of the 40th International Conference on Software Engineering. ACM,
1‚Äì11. https://doi.org/10.1145/3180155.3180233

[90] W. Eric Wong, Vidroha Debroy, Richard Golden, Xiaofeng Xu, and Bhavani Thuraisingham. 2011. Effective software

fault localization using an RBF neural network. IEEE Transactions on Reliability 61, 1 (2011), 149‚Äì169.

[91] W. Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016. A Survey on Software Fault Localization.

IEEE Transactions on Software Engineering 42, 8 (2016), 707‚Äì740. https://doi.org/10.1109/TSE.2016.2521368

[92] W. Eric Wong and Yu Qi. 2009. BP neural network-based effective fault localization. International Journal of Software

Engineering and Knowledge Engineering 19, 04 (2009), 573‚Äì597.

[93] Xiaoyuan Xie, Zicong Liu, Shuo Song, Zhenyu Chen, Jifeng Xuan, and Baowen Xu. 2016. Revisit of automatic debugging
via human focus-tracking analysis. In Proceedings of the 38th International Conference on Software Engineering. ACM,
808‚Äì819.

[94] Qi Xin and Steven P Reiss. 2017. Leveraging syntax-related code for automated program repair. In Proceedings of the
32nd IEEE/ACM International Conference on Automated Software Engineering. 660‚Äì670. https://doi.org/10.1109/ASE.
2017.8115676

[95] Yingfei Xiong, Xinyuan Liu, Muhan Zeng, Lu Zhang, and Gang Huang. 2018.

Identifying patch correctness in
test-based program repair. In Proceedings of the 40th International Conference on Software Engineering. ACM, 789‚Äì799.
https://doi.org/10.1145/3183519.3183540

[96] Yingfei Xiong, Jie Wang, Runfa Yan, Jiachen Zhang, Shi Han, Gang Huang, and Lu Zhang. 2017. Precise condition
synthesis for program repair. In Proceedings of the 39th IEEE/ACM International Conference on Software Engineering.
IEEE, 416‚Äì426. https://doi.org/10.1109/ICSE.2017.45

[97] Jifeng Xuan and Martin Monperrus. 2014. Learning to combine multiple ranking metrics for fault localization. In

Proceedings of the 2014 IEEE International Conference on Software Maintenance and Evolution. 191‚Äì200.

[98] Jifeng Xuan and Martin Monperrus. 2014. Test case purification for improving fault localization. In Proceedings of the
22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. 52‚Äì63. https://doi.org/10.1145/
2635868.2635906

[99] He Ye, Jian Gu, Matias Martinez, Thomas Durieux, and Martin Monperrus. 2021. Automated classification of overfitting

patches with statically extracted code features. IEEE Transactions on Software Engineering (2021).

[100] Yuan Yuan and Wolfgang Banzhaf. 2018. ARJA: Automated Repair of Java Programs via Multi-Objective Genetic
Programming. IEEE Transactions on Software Engineering (2018). https://doi.org/10.1109/TSE.2018.2874648
[101] Andreas Zeller and Ralf Hildebrandt. 2002. Simplifying and isolating failure-inducing input. IEEE Transactions on

Software Engineering 28, 2 (2002), 183‚Äì200.

[102] Mengshi Zhang, Xia Li, Lingming Zhang, and Sarfraz Khurshid. 2017. Boosting spectrum-based fault localization
using PageRank. In Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis.
261‚Äì272.

[103] Xiangyu Zhang, Neelam Gupta, and Rajiv Gupta. 2006. Locating faults through automated predicate switching. In

Proceedings of the 28th International Conference on Software Engineering. 272‚Äì281.

[104] Xiangyu Zhang, Neelam Gupta, and Rajiv Gupta. 2007. A study of effectiveness of dynamic slicing in locating real

faults. Empirical Software Engineering 12, 2 (2007), 143‚Äì160.

[105] Zhuo Zhang, Yan Lei, Xiaoguang Mao, and Panpan Li. 2019. CNN-FL: An effective approach for localizing faults
using convolutional neural networks. In Proceedings of the IEEE 26th International Conference on Software Analysis,
Evolution and Reengineering. IEEE, 445‚Äì455.

[106] Zhuo Zhang, Yan Lei, Qingping Tan, Xiaoguang Mao, Ping Zeng, and Xi Chang. 2017. Deep learning-based fault
localization with contextual information. IEICE Transactions on Information and Systems 100, 12 (2017), 3027‚Äì3031.
[107] Wei Zheng, Desheng Hu, and Jing Wang. 2016. Fault localization analysis based on deep neural network. Mathematical

Problems in Engineering 2016 (2016).

[108] Daming Zou, Jingjing Liang, Yingfei Xiong, Michael D Ernst, and Lu Zhang. 2019. An empirical study of fault

localization families and their combinations. IEEE Transactions on Software Engineering (2019).

J. ACM, Vol. 1, No. 1, Article . Publication date: November 2021.

