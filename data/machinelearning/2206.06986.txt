Exploring Representation of Horn Clauses using
GNNs (Extended Technical Report)

Chencheng Liang1, Philipp Rümmer1,2 and Marc Brockschmidt3

1Uppsala University, Department of Information Technology, Uppsala, Sweden
2University of Regensburg, Regensburg, Germany
3Microsoft Research

Abstract
In recent years, the application of machine learning in program verification, and the embedding of
programs to capture semantic information, has been recognised as an important tool by many research
groups. Learning program semantics from raw source code is challenging due to the complexity of
real-world programming language syntax and due to the difficulty of reconstructing long-distance
relational information implicitly represented in programs using identifiers. Addressing the first point,
we consider Constrained Horn Clauses (CHCs) as a standard representation of program verification
problems, providing a simple and programming language-independent syntax. For the second challenge,
we explore graph representations of CHCs, and propose a new Relational Hypergraph Neural Network
(R-HyGNN) architecture to learn program features.

We introduce two different graph representations of CHCs. One is called constraint graph (CG), and
emphasizes syntactic information of CHCs by translating the symbols and their relations in CHCs as
typed nodes and binary edges, respectively, and constructing the constraints as abstract syntax trees. The
second one is called control- and data-flow hypergraph (CDHG), and emphasizes semantic information
of CHCs by representing the control and data flow through ternary hyperedges.

We then propose a new GNN architecture, R-HyGNN, extending Relational Graph Convolutional
Networks, to handle hypergraphs. To evaluate the ability of R-HyGNN to extract semantic information
from programs, we use R-HyGNNs to train models on the two graph representations, and on five proxy
tasks with increasing difficulty, using benchmarks from CHC-COMP 2021 as training data. The most
difficult proxy task requires the model to predict the occurrence of clauses in counter-examples, which
subsumes satisfiability of CHCs. CDHG achieves 90.59% accuracy in this task. Furthermore, R-HyGNN
has perfect predictions on one of the graphs consisting of more than 290 clauses. Overall, our experiments
indicate that R-HyGNN can capture intricate program features for guiding verification problems.

Keywords
Constraint Horn clauses, Graph Neural Networks, Automatic program verification

1. Introduction

Automatic program verification is challenging because of the complexity of industrially relevant
programs. In practice, constructing domain-specific heuristics from program features (e.g.,

8th Workshop on Practical Aspects of Automated Reasoning
" chencheng.liang@it.uu.se (C. Liang); philipp.ruemmer@it.uu.se (P. Rümmer); marc@marcbrockschmidt.de
(M. Brockschmidt)
~ https://github.com/ChenchengLiang/ (C. Liang); https://github.com/pruemmer/ (P. Rümmer);
https://github.com/mmjb/ (M. Brockschmidt)
(cid:18) 0000-0002-4926-8089 (C. Liang); 0000-0002-2733-7098 (P. Rümmer); 0000-0001-6277-2768 (M. Brockschmidt)
© 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
CEUR Workshop Proceedings (CEUR-WS.org)

2
2
0
2

l
u
J

6
2

]
I

A
.
s
c
[

4
v
6
8
9
6
0
.
6
0
2
2
:
v
i
X
r
a

CEURWorkshopProceedingshttp://ceur-ws.orgISSN 1613-0073 
 
 
 
 
 
information from loops, control flow, or data flow) is essential for solving verification problems.
For instance, [1] and [2] extract semantic information by performing systematical static analysis
to refine abstractions for the counterexample-guided abstraction refinement (CEGAR) [3] based
system. However, manually designed heuristics usually aim at a specific domain and are hard
to transfer to other problems. Along with the rapid development of deep learning in recent
years, learning-based methods have evolved quickly and attracted more attention. For example,
the program features are explicitly given in [4, 5] to decide which algorithm is potentially the
best for verifying the programs. Later in [6, 7], program features are learned in the end-to-end
pipeline. Moreover, some generative models [8, 9] are also introduced to produce essential
information for solving verification problems. For instance, Code2inv [10] embeds the programs
by graph neural networks (GNNs) [11] and learns to construct loop invariants by a deep neural
reinforcement framework.

For deep learning-based methods, no matter how the learning pipeline is designed and the
neural network structure is constructed, learning to represent semantic program features is
essential and challenging (a) because the syntax of the source code varies depending on the pro-
gramming languages, conventions, regulations, and even syntax sugar and (b) because it requires
capturing intricate semantics from long-distance relational information based on re-occurring
identifiers. For the first challenge, since the source code is not the only way to represent a
program, learning from other formats is a promising direction. For example, inst2vec [12]
learns control and data flow from LLVM intermediate representation [13] by recursive neural
networks (RNNs) [14]. Constrained Horn Clauses (CHCs) [15], as an intermediate verification
language, consist of logic implications and constraints and can alleviate the difficulty since
they can naturally encode program logic with simple syntax. For the second challenge, we use
graphs to represent CHCs and learn the program features by GNNs since they can learn from
the structural information within the node’s N-hop neighbourhood by recursive neighbourhood
aggregation (i.e., neural message passing) procedure.

In this work, we explore how to learn program features from CHCs by answering two
questions: (1) What kind of graph representation is suitable for CHCs? (2) Which kind of GNN
is suitable to learn from the graph representation?

For the first point, we introduce two graph representations for CHCs: the constraint graph
(CG) and control- and data-flow hypergraph (CDHG). The constraint graph encodes the CHCs
into three abstract layers (predicate, clause, and constraint layers) to preserve as much structural
information as possible (i.e., it emphasizes program syntax). On the other hand, the Control-
and data-flow hypergraph uses ternary hyperedges to capture the flow of control and data
in CHCs to emphasize program semantics. To better express control and data flow in CDHG,
we construct it from normalized CHCs. The normalization changes the format of the original
CHC but retains logical meaning. We assume that different graph representations of CHCs
capture different aspects of semantics. The two graph representations can be used as a baseline
to construct new graph representations of CHC to represent different semantics. In addition,
similar to the idea in [16], our graph representations are invariant to the concrete symbol names
in the CHCs since we map them to typed nodes.

For the second point, we propose a Relational Hypergraph Neural Network (R-HyGNN), an
extension of Relational Graph Convolutional Networks (R-GCN) [17]. Similar to the GNNs
used in LambdaNet [18], R-HyGNN can handle hypergraphs by concatenating the node rep-

Table 1
Proxy tasks used to evaluate suitability of different graph representations.

Task
1. Argument identification

2. Count occurrence of rela-
tion symbols in all clauses
3. Relation symbol occur-
rence in SCCs

Task type
Node binary
classification
Regression
task on node
Node binary
classification

4. Existence of argument
bounds
5. Clause occurrence in
counter-examples

Node binary
classification
Node binary
classification

Description
For each element in CHCs, predict if it is an argu-
ment of relation symbols.
For each relation symbol, predict how many times
it occurs in all clauses.
For each relation symbol, predict if a cycle exists
from the node to itself (membership in strongly
connected component, SCC).
For each argument of a relation symbol, predict
if it has a lower or upper bound.
For each CHC, predict if it appears in counter-
examples.

resentations involved in a hyperedge and passing the messages to all nodes connected by the
hyperedge.

Finally, we evaluate our framework (two graph representations of CHCs and R-HyGNN)
by five proxy tasks (see details in Table 1) with increasing difficulties. Task 1 requires the
framework to learn to classify syntactic information of CHCs, which is explicitly encoded in the
two graph representations. Task 2 requires the R-HyGNN to predict a syntactic counting task.
Task 3 needs the R-HyGNN to approximate the Tarjan’s algorithm [19], which solves a general
graph theoretical problem. Task 4 is much harder than the last three tasks since the existence of
argument bounds is undecidable. Task 5 is harder than solving CHCs since it predicts the trace
of counter-examples (CEs). Note that Task 1 to 3 can be easily solved by specific, dedicated
standard algorithms. We include them to systematically study the representational power of
graph neural networks applied to different graph construction methods. However, we speculate
that using these tasks as pre-training objectives for neural networks that are later fine-tuned to
specific (data-poor) tasks may be a successful strategy which we plan to study in future work.
Our benchmark data is extracted from the 8705 linear and 8425 non-linear Linear Integer
Arithmetic (LIA) problems in the CHC-COMP repository1 (see Table 1 in the competition
report [20]). The verification problems come from various sources (e.g., higher-order program
verification benchmark2 and benchmarks generated with JayHorn3), therefore cover programs
with different size and complexity. We collect and form the train, valid, and test data using the
predicate abstraction-based model checker Eldarica [21]. We implement R-HyGNNs4 based
on the framework tf2_gnn5. Our code is available in a Github repository6. For both graph
representations, even if the predicted accuracy decreases along with the increasing difficulty
of tasks, for undecidable problems in Task 4, R-HyGNN still achieves high accuracy, i.e., 91%

1https://chc-comp.github.io/
2https://github.com/chc-comp/hopv
3https://github.com/chc-comp/jayhorn-benchmarks
4https://github.com/ChenchengLiang/tf2-gnn
5https://github.com/microsoft/tf2-gnn
6https://github.com/ChenchengLiang/Systematic-Predicate-Abstraction-using-Machine-Learning

and 94% for constraint graph and CDHG, respectively. Moreover, in Task 5, despite the high
accuracy (96%) achieved by CDHG, R-HyGNN has a perfect prediction on one of the graphs
consisting of more than 290 clauses, which is impossible to achieve by learning simple patterns
(e.g., predict the clause including false as positive). Overall, our experiments show that our
framework learns not only the explicit syntax but also intricate semantics.

(i) We encode CHCs into two graph representations, emphasis-
Contributions of the paper.
ing abstract program syntactic and semantic information, respectively. (ii) We extend a message
passing-based GNN, R-GCN, to R-HyGNN to handle hypergraphs. (iii) We introduce five proxy
supervised learning tasks to explore the capacity of R-HyGNN to learn semantic information
from the two graph representations. (iv) We evaluate our framework on the CHC-COMP bench-
mark and show that this framework can learn intricate semantic information from CHCs and
has the potential to produce good heuristics for program verification.

2. Background

2.1. From Program Verification to Horn clauses

Constrained Horn Clauses are logical implications involving unknown predicates. They can be
used to encode many formalisms, such as transition systems, concurrent systems, and interactive
systems. The connections between program logic and CHCs can be bridged by Floyd-Hoare
logic [22, 23], allowing to encode program verification problems into the CHC satisfiability
problems [24]. In this setting, a program is guaranteed to satisfy a specification if the encoded
CHCs are satisfiable, and vice versa.

We write CHCs in the form 𝐻 ← 𝐵1 ∧ · · · ∧ 𝐵𝑛 ∧ 𝜙, where (i) 𝐵𝑖 is an application 𝑞𝑖(𝑡𝑖¯)
of the relation symbol 𝑞𝑖 to a list of first-order terms 𝑡𝑖¯; (ii) 𝐻 is either an application 𝑞(𝑡¯), or
false; (iii) 𝜙 is a first-order constraint. Here, 𝐻 and 𝐵1 ∧ · · · ∧ 𝐵𝑛 ∧ 𝜙 in the left and right hand
side of implication ← are called “head" and “body", respectively.

An example in Figure 1 explains how to encode a verification problem into CHCs. In Figure 1a,
we have a verification problem, i.e., a C program with specifications. It has an external input
𝑛, and we can initially assume that 𝑥 == 𝑛, 𝑦 == 𝑛, and, 𝑛 >= 0. While 𝑥 is not equal to
0, 𝑥 and 𝑦 are decreased by 1. The assertion is that finally, 𝑦 == 0. This program can be
encoded to the CHC shown in Figure 1b. The variables 𝑥 and 𝑦 are quantified universally. We
can further simplify the CHCs in Figure 1b to the CHCs shown in Figure 1c without changing
the satisfiability by some preprocessing steps (e.g., inlining and slicing) [25]. For example, the
first CHC encodes line 3, i.e., the assume statement, the second clause encodes lines 4-7, i.e., the
while loop, and the third clause encodes line 8, i.e., the assert statement in Figure 1a. Solving the
CHCs is equivalent to answering the verification problem. In this example, with a given 𝑛, if the
CHCs are satisfiable for all 𝑥 and 𝑦, then the program is guaranteed to satisfy the specifications.

2.2. Graph Neural Networks

Let 𝐺 = (𝑉, 𝑅, 𝐸, 𝑋, ℓ) denote a graph in which 𝑣 ∈ 𝑉 is a set of nodes, 𝑟 ∈ 𝑅 is a set of edge
types, 𝐸 ∈ 𝑉 × 𝑉 × 𝑅 is a set of typed edges, 𝑥 ∈ 𝑋 is a set of node types, and ℓ : 𝑣 → 𝑥 is a

0
1
2
3
4
5
6
7
8
9

extern i n t n ;
void main ( ) {
i n t x , y ;
assume ( x ==n && y==n && n > = 0 ) ;
while ( x ! = 0 ) {
x − − ;
y − − ;

}
a s s e r t ( y = = 0 ) ;

}

(a) An verification problem written in C.

𝐿0(𝑛) ← 𝑡𝑟𝑢𝑒
𝐿1(𝑛) ← 𝐿0(𝑛)
𝐿2(𝑥, 𝑦, 𝑛) ← 𝐿1(𝑛)
𝐿3(𝑥, 𝑦, 𝑛) ← 𝐿2(𝑥, 𝑦, 𝑛) ∧ 𝑛 ≥ 0

∧ 𝑥 = 𝑛 ∧ 𝑦 = 𝑛

𝐿8(𝑥, 𝑦, 𝑛) ← 𝐿3(𝑥, 𝑦, 𝑛) ∧ 𝑥 = 0
𝐿4(𝑥, 𝑦, 𝑛) ← 𝐿3(𝑥, 𝑦, 𝑛) ∧ 𝑥 ̸= 0
𝐿5(𝑥, 𝑦, 𝑛) ← 𝐿4(𝑥′, 𝑦, 𝑛) ∧ 𝑥 = 𝑥′ − 1
𝐿6(𝑥, 𝑦, 𝑛) ← 𝐿5(𝑥, 𝑦′, 𝑛) ∧ 𝑦 = 𝑦′ − 1
𝐿3(𝑥, 𝑦, 𝑛) ← 𝐿6(𝑥, 𝑦, 𝑛)

false ← 𝐿8(𝑥, 𝑦, 𝑛) ∧ 𝑦 ̸= 0

line 0

line 1

line 2

line 3

line 4

line 4

line 5

line 6

line 6

line 8

(b) CHCs encoded from C program in Figure 1a.

𝐿(𝑥, 𝑦, 𝑛) ← 𝑛 ≥ 0 ∧ 𝑥 = 𝑛 ∧ 𝑦 = 𝑛
𝐿(𝑥, 𝑦, 𝑛) ← 𝐿(𝑥′, 𝑦′, 𝑛′) ∧ 𝑥′ ̸= 0 ∧ 𝑥 = 𝑥′ − 1 ∧ 𝑦 = 𝑦′ − 1 ∧ 𝑛 = 𝑛′

false ← 𝐿(𝑥, 𝑦, 𝑛) ∧ 𝑥 = 0 ∧ 𝑦 ̸= 0

line 3

line 4-7

line 8

(c) Simplified CHCs from Figure 1b.

Figure 1: An example to show how to encode a verification problem written in C to CHCs. For the C
program, the left-hand side numbers indicate the line number. The line numbers in Figure 1b and 1c
correspond to the line in Figure 1a. For example, the line 𝐿0(𝑛) ← 𝑡𝑟𝑢𝑒 in Figure 1b is transformed
from line 1 “extern int n ;" in Figure 1a.

labelling map from nodes to their type. A tuple 𝑒 = (𝑢, 𝑣, 𝑟) ∈ 𝐸 denotes an edge from node 𝑢
to 𝑣 with edge type 𝑟.

Message passing-based GNNs use a neighbourhood aggregation strategy, where at timestep
𝑡, each node updates its representation ℎ𝑡
by aggregating representations of its neighbours and
𝑣
is usually derived
then combining its own representation. The initial node representation ℎ0
𝑣
from its type or label ℓ(𝑣). The common assumption of this architecture is that after 𝑇 iterations,
the node representation ℎ𝑇
captures local information within 𝑡-hop neighbourhoods. Most
𝑣
GNN architectures [26, 27] can be characterized by their used “aggregation” function 𝜌 and
“update” function 𝜑. The node representation of the 𝑡-th layer of such a GNN is then computed
by ℎ𝑡
is the set
| 𝑢 ∈ 𝑁 𝑟
of nodes that are the neighbors of 𝑣 in edge type 𝑟.

), where 𝑅 is a set of edge type and 𝑁 𝑟
𝑣

𝑣 , 𝑟 ∈ 𝑅}), ℎ𝑡−1

𝑣 = 𝜑(𝜌({ℎ𝑡−1

𝑢

𝑣

A closed GNN architecture to the R-HyGNN is R-GCN [17]. They update the node represen-

tation by

ℎ𝑡
𝑣 = 𝜎(

∑︁

∑︁

𝑟∈𝑅

𝑢∈𝑁 𝑟
𝑣

1
𝑐𝑣,𝑟

𝑊 𝑡

𝑟 ℎ𝑡−1

𝑢 + 𝑊0ℎ𝑡−1

𝑣

),

(1)

where 𝑊𝑟 and 𝑊0 are edge-type-dependent trainable weights, 𝑐𝑣,𝑟 is a learnable or fixed
normalisation factor, and 𝜎 is a activation function.

3. Graph Representations for CHCs

Graphs as a representation format support arbitrary relational structure and thus can naturally
encode information with rich structures like CHCs. We define two graph representations for
CHCs that emphasize the program syntax and semantics, respectively. We map all symbols in
CHCs to typed nodes and use typed edges to represent their relations. In this section, we give
concrete examples to illustrate how to construct the two graph representations from a single
CHC modified from Figure 1c. In the examples, we first give the intuition of the graph design
and then describe how to construct the graph step-wise. To better visualize how to construct
the two graph representations in Figures 2 and 3, the concrete symbol names for the typed
nodes are shown in the blue boxes. R-HyGNN is not using these names (which, as a result of
repeated transformations, usually do not carry any meaning anyway) and only consumes the
node types. We include abstract examples, the formal definitions of the graph representations,
and the algorithms to construct them from multiple CHCs in this section as well. Note that
the two graph representations in this study are designed empirically. They can be used as a
baseline to create variations of the graphs to fit different purposes.

3.1. Constraint Graph (CG)

Our Constraint graph is a directed graph with binary edges designed to emphasize syntactic
information in CHCs. One concrete example of constructing the constraint graph for a single
CHC 𝐿(𝑥, 𝑦, 𝑛) ← 𝐿(𝑥′, 𝑦′, 𝑛′) ∧ 𝑥 ̸= 0 ∧ 𝑥 = 𝑥′ − 1 ∧ 𝑦 = 𝑦′ − 1 modified from Figure 1c is
shown in Figure 2. The corresponding node and edge types are described in Tables 2 and 3.

We construct the constraint graph by parsing the CHCs in three different aspects (relation
symbol, clause structure, and constraint) and building relations for them. In other words,
a constraint graph consists of three layers: the predicate layer depicts the relation between
relation symbols and their arguments; the clause layer describes the abstract syntax of head
and body items in the CHC; the constraint layer represents the constraint by abstract syntax
trees (ASTs).

Constructing a constraint graph. Now we give a concrete example to describe how to
construct a constraint graph for a single CHC 𝐿(𝑥, 𝑦, 𝑛) ← 𝐿(𝑥′, 𝑦′, 𝑛′) ∧ 𝑥 ̸= 0 ∧ 𝑥 =
𝑥′ − 1 ∧ 𝑦 = 𝑦′ − 1 step-wise. All steps correspond to the steps in Figure 2. In the first step, we
draw relation symbols and their arguments as typed nodes and build the connection between
them. In the second step, we construct the clause layer by drawing clauses, the relation symbols
in the head and body, and their arguments as typed nodes and build the relation between them.
In the third step, we construct the constraint layer by drawing ASTs for the sub-expressions of
the constraint. In the fourth step, we add connections between three layers. The predicate and
clause layer are connected by the relation symbol instance (RSI) and argument instance (AI)
edges, which means the elements in the predicate layer are instances of the clause layer. The
clause and constraint layers are connected by the GUARD and DATA edges since the constraint
is the guard of the clause implication, and the constraint and clause layer share some elements.

Figure 2: Construct constraint graph from the CHC 𝐿(𝑥, 𝑦, 𝑛) ← 𝐿(𝑥′, 𝑦′, 𝑛′)∧𝑥 ̸= 0∧𝑥 = 𝑥′−1∧𝑦 =
𝑦′ − 1. Note that some nodes have multiple concrete symbol names (e.g., node 𝑟𝑠𝑎1 has two concrete
names, 𝑥 and 𝑥′) since one relation symbol may bind with different arguments.

Table 2
Node types for constraint graph. The shape corresponds to the shape of nodes in Figure 2 and is only
used for visualizing the example.

Node types 𝑋 𝐶𝐺
relation
symbol
(𝑟𝑠)
false
relation
argument (𝑟𝑠𝑎)
clause (𝑐𝑙𝑎)

symbol

clause head (𝑐ℎ)
clause body (𝑐𝑏)
clause argument
(𝑐𝑎)
variable (𝑣𝑎𝑟)
operator (𝑜𝑝)
constant (𝑐)

Layer
Predicate layer

Predicate layer
Predicate layer

Clause layer

Clause layer
Clause layer
Clause layer

Explanation
Relation symbols in head or
body
false state
Arguments of the relation
symbols
Represent clause as a ab-
stract node
Relation symbol in head
Relation symbol in body
Arguments of relation sym-
bol in head and body
Free variables

Constraint layer
Constraint layer Operators over a theory
Constraint layer Constants over a theory

Elements in CHCs
𝐿

Shape

false
x, y

∅

𝐿
𝐿
𝑥, 𝑦

n
=, -
0, 1, true

Formal definition of constraint graph . A constraint graph 𝐶𝐺 = (𝑉, BE , 𝑅𝐶𝐺, 𝑋 𝐶𝐺, ℓ)
consists of a set of nodes 𝑣 ∈ 𝑉 , a set of typed binary edges BE ∈ 𝑉 × 𝑉 × 𝑅𝐶𝐺, a set of edge
types 𝑟 ∈ 𝑅𝐶𝐺 (Table 3), a set of node types 𝑥 ∈ 𝑋 𝐶𝐺 (Table 2), and a map ℓ : 𝑣 → 𝑥. Here,
(𝑣1, 𝑣2, 𝑟) ∈ BE denotes a binary edge with edge type 𝑟. The node types are used to generate
the initial feature 𝑥𝑣, a real-valued vector in R-HyGNN.

Table 3
Edge types for constraint graph. Here, 𝑟𝑠, 𝑟𝑠𝑎, 𝑐𝑙𝑎, etc. are node types described in Table 2.Here, “|"
means “or". For example, 𝑐 | 𝑜𝑝 |𝑣𝑎𝑟 means this node could be node with type 𝑐, 𝑜𝑝, or 𝑣𝑎𝑟.

Layer
Predicate layer

Definition
(𝑟𝑠, 𝑟𝑠𝑎, RSA)

Explanation
Connects relation symbols
and their arguments

(𝑟𝑠, 𝑐ℎ, RSA) | (𝑐𝑏, 𝑟𝑠, RSA)Connects relation symbols

Between predicate
and clause layer
Between predicate
and clause layer
Clause layer

(𝑝𝑎, 𝑐𝑎, AI)

(𝑐𝑙𝑎, 𝑐ℎ, CH)

Clause Body (CB)

Clause layer

(𝑐𝑏, 𝑐𝑙𝑎, CB)

Clause layer

(𝑐𝑎, 𝑐𝑏, CA) | (𝑐ℎ, 𝑐𝑎, CA) Connect 𝑐ℎ or 𝑐𝑏 nodes with

Edge type 𝑅𝐶𝐺
Relation Symbol Ar-
gument (RSA)
Relation Symbol In-
stance (RSI)
Argument Instance
(AI)
Clause Head (CH)

Clause Argument
(CA)
Guard (GUARD)

with their head and body
Connects relation symbols
and their arguments
Connect 𝑐𝑙𝑎𝑢𝑠𝑒 node to its
head
Connect the 𝑐𝑙𝑎𝑢𝑠𝑒 node to
its body

corresponding 𝑐𝑎 nodes
Connect the root node of
the AST to corresponding
𝑐𝑙𝑎𝑢𝑠𝑒 node
Connect 𝑐𝑎 nodes to corre-
sponding 𝑣𝑎𝑟 nodes AST
Connect nodes within AST

Between clause and
constraint layer

(𝑐 | 𝑜𝑝, 𝑐𝑙𝑎, GUARD)

Data (DATA)

Between clause and
constraint layer
AST sub-term (𝐴𝑠𝑡) Constraint layer

(𝑣𝑎𝑟, 𝑐𝑎, DATA)

(𝑐 | 𝑣𝑎𝑟 | 𝑜𝑝, 𝑜𝑝, 𝐴𝑠𝑡)

An abstract example of constraint graph . Except for the concrete example, we give a
abstract example to describe how to construct the constraint graph. First, the CHC 𝐻 ←
𝐵1 ∧ · · · ∧ 𝐵𝑛 ∧ 𝜙 in Section 2 can be re-written to

𝑞1(𝑡1¯ ) ← 𝑞2(𝑡2¯ ) ∧ · · · ∧ 𝑞𝑘(𝑡𝑘¯ ) ∧ 𝜙1 ∧ · · · 𝜙𝑛, (𝑛, 𝑘 ∈ N),

(2)

where 𝜙1 · · · 𝜙𝑘 are sub-formulas for constraint 𝜙. Notice that since there is no normalization
for the original CHCs, the same relation symbols can appear in both head and body (i.e.,
𝑞1, 𝑞2, · · · , 𝑞𝑘 may equal to each other).

Then, we can construct a constraint graph using Algorithm 1, in which the input is a set
of CHC and the output is a constraint graph 𝐶𝐺 = (𝑉, BE , 𝑅𝐶𝐺, 𝑋 𝐶𝐺, ℓ). The step-wise
constructing process for the CHC in Eq. (2) is visualized in Figure 6.

3.2. Control- and Data-flow Hypergraph (CDHG)

In contrast to the constraint graph, the CDHG representation emphasizes the semantic informa-
tion (control and data flow) in the CHCs by hyperedges which can join any number of vertices.
To represent control and data flow in CDHG, first, we preprocess every CHC and then split the
constraint into control and data flow sub-formulas.

Normalization. We normalize every CHC by applying the following rewriting steps: (i) We
ensure that every relation symbol occurs at most once in every clause. For instance, the CHC
𝑞(𝑎) ← 𝑞(𝑏) ∧ 𝑞(𝑐) has multiple occurrences of the relation symbol 𝑞, and we normalize it to

Table 4
Control- and data-flow sub-formula in constraints for the normalized CHCs from Figure 1c

Normalized CHCs
𝐿(𝑥, 𝑦, 𝑛) ← 𝑛 ≥ 0 ∧ 𝑥 = 𝑛 ∧ 𝑦 = 𝑛
𝐿(𝑥, 𝑦, 𝑛) ← 𝐿′(𝑥′, 𝑦′, 𝑛′) ∧ 𝑥 ̸= 0 ∧
𝑥 = 𝑥′ − 1 ∧ 𝑦 = 𝑦′ − 1
𝐿′(𝑥′, 𝑦′, 𝑛′) ← 𝐿(𝑥, 𝑦, 𝑛)∧𝑥′ = 𝑥∧
𝑦′ = 𝑦 ∧ 𝑛′ = 𝑛
false ← 𝐿(𝑥, 𝑦, 𝑛) ∧ 𝑥 = 0 ∧ 𝑦 ̸= 0

Control-flow sub-formula
𝑛 ≥ 0
𝑥 ̸= 0

Data-flow sub-formula
𝑥 = 𝑛, 𝑦 = 𝑛
𝑥 = 𝑥′ − 1, 𝑦 = 𝑦′ − 1

empty

𝑦 ̸= 0

𝑥′ = 𝑥, 𝑦′ = 𝑦, 𝑛′ = 𝑛

𝑥 = 0

equi-satisfiable CHCs 𝑞(𝑎) ← 𝑞′(𝑏) ∧ 𝑞′′(𝑐), 𝑞′(𝑏) ← 𝑞(𝑏′) ∧ 𝑏 = 𝑏′ and 𝑞′′(𝑐) ← 𝑞(𝑐′) ∧ 𝑐 = 𝑐′.
(ii) We associate each relation symbol 𝑞 with a unique vector of pair-wise distinct argument
variables 𝑥¯𝑞, and rewrite every occurrence of 𝑞 to the form 𝑞(𝑥¯𝑞). In addition, all the argument
vectors 𝑥¯𝑞 are kept disjoint. The normalized CHCs from Figure 1c are shown in Table 4.

Splitting constraints into control- and data-flow formulas. We can rewrite the con-
straint 𝜙 to a conjunction 𝜙 = 𝜙1 ∧ · · · ∧ 𝜙𝑘, 𝑘 ∈ N . The sub-formula 𝜙𝑖 is called a “data-flow
sub-formula” if and only if it can be rewritten to the form 𝑥 = 𝑡(𝑦¯) such that (i) 𝑥 is one of
the arguments in head 𝑞(𝑥¯𝑞); (ii) 𝑡(𝑦¯) is a term over variables 𝑦¯, where each element of 𝑦¯ is
an argument of some body literal 𝑞′(𝑥¯𝑞′). We call all other 𝜙𝑗 “control-flow sub-formulas”. A
constraint 𝜙 can then be represented by 𝜙 = 𝑔1 ∧ · · · ∧ 𝑔𝑚 ∧ 𝑑1 ∧ · · · ∧ 𝑑𝑛, where 𝑚, 𝑛 ∈ N
and 𝑔𝑖 and 𝑑𝑗 are the control- and data-flow sub-formulas, respectively. The control and data
flow sub-formulas for the normalized CHCs of our running example are shown in Table 4.

Constructing a CDHG. The CDHG represents program control- and data-flow by guarded
control-flow hyperedges CFHE and data-flow hyperedges DFHE. A CFHE edge denotes the flow
of control from the body to head literals of the CHC. A DFHE edge denotes how data flows from
the body to the head. Both control- and data-flow are guarded by the control flow sub-formula.
Constructing the CDHG for a normalized CHC 𝐿(𝑥, 𝑦, 𝑛) ← 𝐿′(𝑥′, 𝑦′, 𝑛′) ∧ 𝑥 ̸= 0 ∧ 𝑥 =
𝑥′ − 1 ∧ 𝑦 = 𝑦′ − 1 is shown in Figure 3. The corresponding node and edge types are described
in Tables 5 and 6.

In the first step, we draw relation symbols and their arguments and build the relation between
them. In the second step, we add a 𝑔𝑢𝑎𝑟𝑑 node and draw ASTs for the control flow sub-formulas.
In the third step, we construct guarded control-flow edges by connecting the relation symbols in
the head and body and the 𝑔𝑢𝑎𝑟𝑑 node, which connects the root of control flow sub-formulas. In
the fourth step, we construct the ASTs for the right-hand side of every data flow sub-formula. In
the fifth step, we construct the guarded data-flow edges by connecting the left- and right-hand
sides of the data flow sub-formulas and the 𝑔𝑢𝑎𝑟𝑑 node. Note that the diamond shapes in
Figure 3 are not nodes in the graph but are used to visualize our (ternary) hyperedges of types
CFHE and DFHE. We show it in this way to visualize CDHG better.

Formal definition of CDHG. A CDHG 𝐻𝐺 = (𝑉, HE , 𝑅𝐶𝐷𝐻𝐺, 𝑋 𝐶𝐷𝐻𝐺, ℓ) consists of a
set of nodes 𝑣 ∈ 𝑉 , a set of typed hyperedge HE ∈ 𝑉 * × 𝑅𝐶𝐷𝐻𝐺 where 𝑉 * is a list of node

Figure 3: Construct the CDHG from the CHC 𝐿(𝑥, 𝑦, 𝑛) ← 𝐿′(𝑥′, 𝑦′, 𝑛′) ∧ 𝑥 ̸= 0 ∧ 𝑥 = 𝑥′ − 1 ∧ 𝑦 =
𝑦′ − 1.

from 𝑉 , a set of node types 𝑥 ∈ 𝑋 𝐶𝐷𝐻𝐺 (Table 5),a set of edge types 𝑟 ∈ 𝑅𝐶𝐷𝐻𝐺 (Table 6),
and a map ℓ : 𝑣 → 𝑥. Here, (𝑣1, 𝑣2, · · · , 𝑣𝑛, 𝑟) ∈ 𝐻𝐸 denotes a hyperedge for a list of nodes
(𝑣1, · · · , 𝑣𝑛) with edge type 𝑟. The node types are used to generate the initial feature 𝑥𝑣, a
real-valued vector, in R-HyGNN.

The guarded CFHE is a typed hyperedge (𝑣1, · · · , 𝑣𝑛, 𝑔, CFHE) ∈ HE, where the type of
𝑣1, · · · , 𝑣𝑛 is 𝑟𝑠. Since R-HyGNN has more stable performance with fixed number of node in one
edge type, we transform the hyperedge (𝑣1, · · · , 𝑣𝑛, 𝑔, CFHE ) ∈ 𝐻𝐸 with variable number of
nodes to a set of ternary hyperedges (i.e., {(𝑣1, 𝑣2, 𝑔, CFHE ), (𝑣1, 𝑣3, 𝑔, CFHE ), · · · , (𝑣1, 𝑣𝑛, 𝑔, CFHE )}).

The guarded DFHE is a typed ternary hyperedge (𝑣𝑖, 𝑣𝑗, 𝑔, DFHE ) ∈ HE , where 𝑣𝑖’s type is

𝑟𝑠𝑎 and 𝑣𝑗’s type could be one of {𝑜𝑝, 𝑐, 𝑣}.

An abstract example of CDHG. Except for the concrete example, we give a abstract example
to describe how to construct the CDHG. After the preprocessings (normalization and splitting
the constraint to guard and data flow sub-formulas), the CHC 𝐻 ← 𝐵1 ∧ · · · ∧ 𝐵𝑛 ∧ 𝜙 in
Section 2 can be re-written to

𝑞1(𝑡1¯ ) ← 𝑞2(𝑡2¯ ) ∧ · · · ∧ 𝑞𝑘(𝑡𝑘¯ ) ∧ 𝑔1 ∧ · · · 𝑔𝑚 ∧ 𝑑1 ∧ · · · 𝑑𝑛, (𝑚, 𝑛, 𝑘 ∈ N).

(3)

We can construct the corresponding CDHG using Algorithm 2, in which the input is a
set of CHC and the output is a CDHG 𝐻𝐺 = (𝑉, HE , 𝑅𝐶𝐷𝐻𝐺, 𝑋 𝐶𝐷𝐻𝐺, ℓ). The step-wise

Table 5
Node types for the CDHG. Note that Tables 2 and 5 use some same node types because they represent
the same elements in the CHC. Some abstract nodes, such as initial and guard, do not have concrete
symbol names since they do not directly associate with any element in the CHCs.

Explanation
Relation symbols in head or body
Initial state
false state

Node types 𝑋 𝐶𝐷𝐻𝐺
relation symbol (𝑟𝑠)
initial
false
relation symbol argument (𝑟𝑠𝑎) Arguments of the relation symbols
variables (𝑣𝑎𝑟)
operator (𝑜𝑝)
constant (𝑐)
guard (𝑔)

Free variables
Operators over a theory
Constant over a theory
Guard for CFHE and DFHE

Shape

Elements in CHCs
𝐿
∅
false
𝑥, 𝑦
𝑛
=, +
0, 1, 𝑡𝑟𝑢𝑒
∅

Table 6
Edge types for the CDHG. 𝑟𝑠, 𝑟𝑠𝑎, 𝑣𝑎𝑟, 𝑜𝑝, 𝑐, 𝑒𝑡𝑐. are node types from Table 5.
Explanation
Connects the 𝑟𝑠 node in body and
head, and abstract guard node

Edge arity Definition
Ternary

false, 𝑔,

|

Edge type 𝑅𝐶𝐷𝐻𝐺
Control Flow Hy-
peredge (CFHE)
Data Flow Hyper-
edge (DFHE)

Ternary

(𝑜𝑝 | 𝑐, 𝑔, Guard)

(𝑟𝑠1, 𝑟𝑠2
CFHE)
(𝑎, 𝑜𝑝 | 𝑐 | 𝑣𝑎𝑟, 𝑔, DFHE) Connects the root node of right-
hand and left-hand side of data flow
sub-formulas, and a abstract guard
node
Connects all roots of ASTs of con-
trol flow sub-formulas and the
𝑔𝑢𝑎𝑟𝑑 node
Connects 𝑟𝑠𝑎 nodes and their 𝑟𝑠
node
Connects left-hand side element of
a binary operator or an element
from a unary operator
It connects right-hand side element
of a binary operator

(𝑜𝑝, 𝑜𝑝 | 𝑣𝑎𝑟 | 𝑐, 𝐴𝑟)

(𝑜𝑝, 𝑜𝑝 | 𝑣𝑎𝑟 | 𝑐, 𝐴𝑙)

(𝑎, 𝑟𝑠, RSA)

Guard (Guard)

Binary

Relation Symbol Ar-
gument (RSA)
AST_left (𝐴𝑙)

Binary

Binary

AST_right (𝐴𝑟)

Binary

constructing process for the CHC in (3) is visualized in Figure 7.

4. Relational Hypergraph Neural Network

Different from regular graphs, which connect nodes by binary edges, CDHG includes hyperedges
which connect arbitrary number of nodes. Therefore, we extend R-GCN to R-HyGNN to handle
hypergraphs. Concretely, to compute a new representation of a node 𝑣 at timestep 𝑡, we consider
all hyperedges 𝑒 that involve 𝑣. For each such hyperedge we create a “message” by concatenating
the representations of all nodes involved in that hyperedge and multiplying the result with a
, where 𝑟 is the type of the relation and 𝑝 the position that 𝑣 takes in the
learnable matrix 𝑊 𝑡
𝑟,𝑝

hyperedge. Intuitively, this means that we have one learnable matrix for each distinguishable
way a node can be involved in a relation. Hence, the updating rule for node representation at
time step 𝑡 in R-HyGNN is

ℎ𝑡
𝑣 = ReLU(

∑︁

∑︁

∑︁

𝑟∈𝑅

𝑝∈𝑃𝑟

𝑒∈𝐸𝑟,𝑝
𝑣

𝑊 𝑡

𝑟,𝑝 · ‖[ℎ𝑡−1

𝑢

| 𝑢 ∈ 𝑒]),

(4)

where ‖{·} denotes concatenation of all elements in a set, 𝑟 ∈ 𝑅 = {𝑟𝑖 | 𝑖 ∈ N} is the set of
edge types (relations), 𝑝 ∈ 𝑃𝑟 = {𝑝𝑗 | 𝑗 ∈ N} is the set of node positions under edge type 𝑟,
denotes learnable parameters when the node is in the 𝑝th position with edge type 𝑟, and
𝑊 𝑡
𝑟,𝑝
𝑒 ∈ 𝐸𝑟,𝑝
is the set of hyperedges of type 𝑟 in the graph in which node 𝑣 appears in position 𝑝,
𝑣
where 𝑒 is a list of nodes. The updating process for the representation of node 𝑣 at time step 1
is illustrated in Figure 4.

Note that different edge types may have the same number of connected nodes. For instance,

in CDHG, CFHE and DFHE are both ternary edges.

Overall, our definition of R-HyGNN is a generalization of R-GCN. Concretely, it can directly
be applied to the special-case of binary graphs, and in that setting is slightly more powerful as
each message between nodes is computed using the representations of both source and target
nodes, whereas R-GCN only uses the source node representation.

4.1. Training Model

The end-to-end model consists of three components: the first component is an embedding layer,
which learns to map the node’s type (encoded by integers) to the initial feature vectors; the
second component is R-HyGNN, which learns program features; the third component is a set
of fully connected neural networks, which learns to solve a specific task by using gathered
node representations from R-HyGNNs. All parameters in these three components are trained
together. Note that different tasks may gather different node representations. For example,
Task 1 gathers all node representations, while Task 2 only gathers node representations with
type 𝑟𝑠.

We set all vector lengths to 64, i.e., the embedding layer output size, the middle layers’ neuron
size in R-HyGNNs, and the layer sizes in fully connected neural networks are all 64. The
maximum training epoch is 500, and the patient is 100. The number of message passing steps is
8 (i.e., (4) is applied eight times). For the rest of the parameters (e.g., learning rate, optimizer,
dropout rate, etc.), we use the default setting in the tf2_gnn framework. We set these parameters
empirically according to the graph size and the structure. We apply these fixed parameter
settings for all tasks and two graph representations without fine-tuning.

5. Proxy Tasks

We propose five proxy tasks with increasing difficulty to systematically evaluate the R-HyGNN
on the two graph representations. Tasks 1 to 3 evaluate if R-HyGNN can solve general problems
in graphs. In contrast, Tasks 4 and 5 evaluate if combining our graph representations and
R-HyGNN can learn program features to solve the encoded program verification problems. We

first describe the learning target for every task and then explain how to produce training labels
and discuss the learning difficulty.

For both graph representations, the R-HyGNN model
Task 1: Argument identification.
performs binary classification on all nodes to predict if the node type is a relation symbol
argument (𝑟𝑠𝑎) and the metric is accuracy. The binary training label is obtained by reading
explicit node types. This task evaluates if R-HyGNN can differentiate explicit node types. This
task is easy because the graph explicitly includes the node type information in both typed nodes
and edges.

For both graph representa-
Task 2: Count occurrence of relation symbols in all clauses.
tions, the R-HyGNN model performs regression on nodes with type 𝑟𝑠 to predict how many
times the relation symbols occur in all clauses. The metric is mean square error. The training
label is obtained by counting the occurrence of every relation symbol in all clauses. This task is
designed to see if R-HyGNN can correctly perform a counting task. For example, the relation
symbol 𝐿 occurs four times in all CHCs in Figure 4, so the training label for node 𝐿 is value 4.
This task is harder than Task 1 since it needs to count the connected binary edges or hyperedges
for a particular node.

For both graph representations, the R-
Task 3: Relation symbol occurrence in SCCs.
HyGNN model performs binary classification on nodes with type 𝑟𝑠 to predict if this node is
an SCC (i.e., in a cycle) and the metric is accuracy. The binary training label is obtained using
Tarjan’s algorithm [19]. For example, in Figure 4, 𝐿 is an SCC because 𝐿 and 𝐿′ construct a cycle
by 𝐿 ← 𝐿′ and 𝐿′ ← 𝐿. This task is designed to evaluate if R-HyGNN can recognize general
graph structures such as cycles. This task requires the model to classify a graph-theoretic object
(SCC), which is harder than the previous two tasks since it needs to approximate a concrete
algorithm rather than classifying or counting explicit graphical elements.

For both graph representations, we train two
Task 4: Existence of argument bounds.
independent R-HyGNN models which perform binary classification on nodes with type 𝑟𝑠𝑎
to predict if individual arguments have (a) lower and (b) upper bounds in the least solution
of a set of CHCs, and the metric is accuracy. To obtain the training label, we apply the Horn
solver Eldarica to check the correctness of guessed (and successively increased) lower and upper
arguments bounds; arguments for which no bounds can be shown are assumed to be unbounded.
We use a timeout of 3 s for the lower and upper bound of a single argument, respectively. The
overall timeout for extracting labels from one program is 3 hours. For example, consider the
CHCs in Fig. 1c. The CHCs contain a single relation symbol 𝐿; all three arguments of 𝐿 are
bounded from below but not from above. This task is (significantly) harder than the previous
three tasks, as boundedness of arguments is an undecidable property that can, in practice, be
approximated using static analysis methods.

Task 5: Clause occurrence in counter-examples This task consists of two binary clas-
sification tasks on nodes with type guard (for CDHG), and with type clause (for constraint

graph) to predict if a clause occurs in the counter-examples. Those kinds of nodes are unique
representatives of the individual clauses of a problem. The task focuses on unsatisfiable sets of
CHCs. Every unsatisfiable clause set gives rise to a set of minimal unsatisfiable subsets (MUSes);
MUSes correspond to the minimal CEs of the clause set. Two models are trained independently
to predict whether a clause belongs to (a) the intersection or (b) the union of the MUSes of a
clause set. The metric for this task is accuracy. We obtain training data by applying the Horn
solver Eldarica [25], in combination with an optimization library that provides an algorithm to
compute MUSes7. This task is hard, as it attempts the prediction of an uncomputable binary
labelling of the graph.

6. Evaluation

We first describe the dataset we use for the training and evaluation and then analyse the
experiment results for the five proxy tasks.

6.1. Benchmarks and Dataset

Table 7 shows the number of labelled graph representations from a collection of CHC-COMP
benchmarks [20]. All graphs were constructed by first running the preprocessor of Eldarica [25]
on the clauses, then building the graphs as described in Section 3, and computing training data.
For instance, in the first four tasks we constructed 2337 constraint graphs with labels from
8705 benchmarks in the CHC-COMP LIA-Lin track (linear Horn clauses over linear integer
arithmetic). The remaining 6368 benchmarks are not included in the learning dataset because
when we construct the graphs, (1) the data generation process timed out, or (2) the graphs
were too big (more than 10,000 nodes), or (3) there was no clause left after simplification. In
Task 5, since the label is mined from CEs, we first need to identify unsat benchmarks using a
Horn solver (1-hour timeout), and then construct graph representations. We obtain 881 and 857
constraint graphs when we form the labels for Task 5 (a) and (b), respectively, in LIA-Lin.

Finally, to compare the performance of the two graph representations, we align the dataset
for both two graph representations to have 5602 labelled graphs for the first four tasks. For
Task 5 (a) and (b), we have 1927 and 1914 labelled graphs, respectively. We divide them to train,
valid, and test sets with ratios of 60%, 20%, and 20%. We include all corresponding files for the
dataset in a Github repository 8.

6.2. Experimental Results for Five Proxy Tasks

From Table 8, we can see that for all binary classification tasks, the accuracy for both graph
representations is higher than the ratios of the dominant labels. For the regression task, the
scattered points are close to the diagonal line. These results show that R-HyGNN can learn the
syntactic and semantic information for the tasks rather than performing simple strategies (e.g.,
fill all likelihood by 0 or 1). Next, we analyse the experimental results for every task.

7https://github.com/uuverifiers/lattice-optimiser/
8https://github.com/ChenchengLiang/Horn-graph-dataset

Table 7
The number of labeled graph representations extracted from a collection of CHC-COMP benchmark
[20]. For each SMT-LIB file, the graph representations for Task 1,2,3,4 are extracted together using the
timeout of 3 hours, and for task 5 is extracted using 20 minutes timeout. Here, T. denotes Task.

SMT-LIB files
Unsat
Total
1659
Linear LIA
8705
3601
Non-linear LIA 8425
5260
17130
Aligned

Constraint graphs
T. 5 (a)
881
1141
1927

T. 5 (b)
857
1138
1914

T. 1-4
2337
3376
5602

CDHGs
T. 5 (a)
880
1497
1927

T. 5 (b)
883
1500
1914

T. 1-4
3029
4343
5602

Task 1: Argument identification. When the task is performed in the constraint graph, the
accuracy of prediction is 100%, which means R-HyGNN can perfectly differentiate if a node is
a relation symbol argument 𝑟𝑠𝑎 node. When the task is performed in CDHG, the accuracy is
close to 100% because, unlike in the constraint graph, the number of incoming and outgoing
edges are fixed (i.e., 𝑅𝑆𝐴 and 𝐴𝐼), the 𝑟𝑠𝑎 nodes in CDHG may connect with a various number
of edges (including 𝑅𝑆𝐴, 𝐴𝑆𝑇 _1, 𝐴𝑆𝑇 _2, and DFHE) which makes R-HyGNN hard to predict
the label precisely.

Besides, the data distribution looks very different between the two graph representations
because the normalization of CHCs introduces new clauses and arguments. For example, in
the simplified CHCs in Figure 1c, there are three arguments for the relation symbol 𝐿, while in
the normalized clauses in Figure 4, there are six arguments for two relation symbols 𝐿 and 𝐿′.
If the relation symbols have a large number of arguments, the difference in data distribution
between the two graph representations becomes larger. Even though the predicted label in this
task cannot directly help solve the CHC-encoded problem, it is important to study the message
flows in the R-HyGNNs.

In the scattered plots in
Task 2: Count occurrence of relation symbols in all clauses.
Figure 5, the x- and y-axis denote true and the predicted values in the logarithm scale, respec-
tively. The closer scattered points are to the diagonal line, the better performance of predicting
the number of relation symbol occurrences in CHCs. Both CDHG and constraint graph show
good performance (i.e., most of the scattered points are near the diagonal lines). This syntactic
information can be obtained by counting the CFHE and RSI edges for CDHG and constraint
graph, respectively. When the number of nodes is large, the predicted values are less accurate.
We believe this is because graphs with a large number of nodes have a more complex structure,
and there is less training data. Moreover, the mean square error for the CDHG is larger than
the constraint graph because normalization increases the number of nodes and the maximum
counting of relation symbols for CDHG, and the larger range of the value is, the more difficult
for regression task. Notice that the number of test data (1115) for this task is less than the data
in the test set (1121) shown in Table 7 because the remaining six graphs do not have a 𝑟𝑠 node.

Task 3: Relation symbol occurrence in SCCs. The predicted high accuracy for both graph
representations shows that our framework can approximate Tarjan’s algorithm [19]. In contrast
to Task 2, even if the CDHG has more nodes than the constraint graph on average, the CDHG

has better performance than the constraint graph , which means the control and data flow in
CDHG can help R-HyGNN to learn graph structures better. For the same reason as task 2, the
number of test data (1115) for this task is less than the data in the test set (1121).

For both graph representations, the accuracy is
Task 4: Existence of argument bounds.
much higher than the ratio of the dominant label. Our framework can predict the answer
for undecidable problems with high accuracy, which shows the potential for guiding CHC
solvers. The CDHG has better performance than the constraint graph, which might be because
predicting argument bounds relies on semantic information. The number of test data (1028) for
this task is less than the data in the test set (1121) because the remaining 93 graphs do not have
a 𝑟𝑠𝑎 node.

For Task (a) and (b), the overall accuracy
Task 5: Clause occurrence in counter-examples.
for two graph representations is high. We manually analysed some predicted results by visual-
izing the (small) graphs9. We identify some simple patterns that are learned by R-HyGNNs. For
instance, the predicted likelihoods are always high for the 𝑟𝑠 nodes connected to the false nodes.
One promising result is that the model can predict all labels perfectly for some big graphs10 that
contain more than 290 clauses, which confirms that the R-HyGNN is learning certain intricate
patterns rather than simple patterns. In addition, the CDHG has better performance than the
constraint graph , possibly because semantic information is more important for solving this
task.

7. Related Work

Learning to represent programs. Contextual embedding methods (e.g. transformer [28],
BERT [29], GPT [30], etc.) achieved impressive results in understanding natural languages. Some
methods are adapted to explore source code understanding in text format (e.g. CodeBERT [31],
cuBERT [32], etc.). But, the programming language usually contains rich, explicit, and compli-
cated structural information, and the problem sets (learning targets) of it [33, 34] are different
from natural languages. Therefore, the way of representing the programs and learning models
should adapt to the programs’ characteristics. Recently, the frameworks consisting of structural
program representations (graph or AST) and graph or tree-based deep learning models made
good progress in solving program language-related problems. For example, [35] represents the
program by a sequence of code subtokens and predicts source code snippets summarization by
a novel convolutional attention network. Code2vec [36] learns the program from the paths in
its AST and predicts semantic properties for the program using a path-based attention model.
[37] use AST to represent the program and classify programs according to functionality using
the tree-based convolutional neural network (TBCNN). Some studies focus on defining efficient
program representations, others focus on introducing novel learning structures, while we do
both of them (i.e. represent the CHC-encoded programs by two graph representations and
propose a novel GNN structure to learn the graph representations).

9https://github.com/ChenchengLiang/Horn-graph-dataset/tree/main/example-analysis/task5-small-graphs
10https://github.com/ChenchengLiang/Horn-graph-dataset/tree/main/example-analysis/task5-big-graphs

Table 8
Experiment results for Tasks 1,3,4,5. Both the fourth and fifth tasks consist of two independent binary
classification tasks. Here, + and − stands for the positive and negative label. The T and P represent the
true and predicted labels. The Acc. is the accuracy of binary classifications. The Dom. is dominant label
ratio. Notice that even if the two graph representations originate from the same original CHCs, the
label distributions are different since the CDHG is constructed from normalized CHCs.
CG

CDHG

Task

Files

1121

1115

1028

1

3

4 (a)

4 (b)

5 (a)

386

5 (b)

383

T
+
-
+
-
+
-
+
-
+
-
+
-

P

+

-

Acc.

Dom.

+

-

Acc.

Dom.

93863
0
3204
301
13685
2928
18888
3291
1048
154
3030
622

0
1835971
133
7493
5264
71986
4792
66892
281
7163
558
3428

100%

95.1%

96.1% 70.1%

91.2% 79.7%

91.4% 74.8%

95.0% 84.7%

84.6% 53.1%

142598
10
8262
15
30845
3566
41539
3715
1230
121
3383
323

0
381445
58
8523
4557
103630
4360
92984
206
9036
481
4361

99.9% 72.8%

99.6% 50.7%

94.3% 75.2%

94.3% 67.8%

96.9% 86.4%

90.6% 54.8%

Since deep learning is introduced to learn the features
Deep learning for logic formulas.
from logic formulas, an increasing number of studies have begun to explore graph representa-
tions for logic formulas and corresponding learning frameworks because logic formulas are also
highly structured like program languages. For instance, DeepMath [38] had an early attempt
to use text-level learning on logic formulas to guide the formal method’s search process, in
which neural sequence models are used to select premises for automated theorem prover (ATP).
Later on, FormulaNet [39] used an edge-order-preserving embedding method to capture the
structural information of higher-order logic (HOL) formulas represented in a graph format. As
an extension of FormulaNet, [40] construct syntax trees of HOL formulas as structural inputs
and use message-passing GNNs to learn features of HOL to guide theorem proving by predicting
tactics and tactic arguments at every step of the proof. LERNA [41] uses convolutional neural
networks (CNNs) [42] to learn previous proof search attempts (logic formulas) represented by
graphs to guide the current proof search for ATP. NeuroSAT [43, 44] reads SAT queries (logic
formulas) as graphs and learns the features using different graph embedding strategies (e.g.
message passing GNNs) [45, 46, 26]) to directly predict the satisfiability or guide the SAT solver.
Following this trend, we introduce R-HyGNN to learn the program features from the graph
representation of CHCs.

Graph neural networks. Message passing GNNs [26], such as graph convolutional network
(GCN) [17], graph attention network (GAT) [47], and gated graph neural network (GGNN) [46]
have been applied in several domains ranging from predicting molecule properties to learning
logic formula representations. However, these frameworks only apply to graphs with binary
edges. Some spectral methods have been proposed to deal with the hypergraph [48, 49]. For

instance, the hypergraph neural network (HGNN) [50] extends GCN proposed by [51] to handle
hyperedges. The authors in [52] integrate graph attention mechanism [47] to hypergraph
convolution [51] to further improve the performance. But, they cannot be directly applied to
the spatial domain. Similar to the fixed arity predicates strategy described in LambdaNet [18],
R-HyGNN concatenates node representations connected by the hyperedge and updates the
representation depending on the node’s position in the hyperedge.

8. Conclusion and Future Work

In this work, we systematically explore learning program features from CHCs using R-HyGNN,
using two tailor-made graph representations of CHCs. We use five proxy tasks to evaluate
the framework. The experimental results indicate that our framework has the potential to
guide CHC solvers analysing Horn clauses. In future work, among others we plan to use this
framework to filter predicates in Horn solvers applying the CEGAR model checking algorithm.

References

[1] J. Leroux, P. Rümmer, P. Subotić, Guiding Craig interpolation with domain-specific abstrac-
tions, Acta Informatica 53 (2016) 387–424. URL: https://doi.org/10.1007/s00236-015-0236-z.
doi:10.1007/s00236-015-0236-z.

[2] Y. Demyanova, P. Rümmer, F. Zuleger, Systematic predicate abstraction using variable roles,
in: C. Barrett, M. Davies, T. Kahsai (Eds.), NASA Formal Methods, Springer International
Publishing, Cham, 2017, pp. 265–281.

[3] E. Clarke, O. Grumberg, S. Jha, Y. Lu, H. Veith, Counterexample-guided abstraction
refinement, in: E. A. Emerson, A. P. Sistla (Eds.), Computer Aided Verification, Springer
Berlin Heidelberg, Berlin, Heidelberg, 2000, pp. 154–169.

[4] V. Tulsian, A. Kanade, R. Kumar, A. Lal, A. V. Nori, MUX: Algorithm selection for software
model checkers, in: Proceedings of the 11th Working Conference on Mining Software
Repositories, MSR 2014, Association for Computing Machinery, New York, NY, USA,
2014, p. 132–141. URL: https://doi.org/10.1145/2597073.2597080. doi:10.1145/2597073.
2597080.

[5] Y. Demyanova, T. Pani, H. Veith, F. Zuleger, Empirical software metrics for benchmarking
of verification tools, in: J. Knoop, U. Zdun (Eds.), Software Engineering 2016, Gesellschaft
für Informatik e.V., Bonn, 2016, pp. 67–68.

[6] C. Richter, E. Hüllermeier, M.-C. Jakobs, H. Wehrheim, Algorithm selection for software
validation based on graph kernels, Automated Software Engineering 27 (2020) 153–186.
[7] C. Richter, H. Wehrheim, Attend and represent: A novel view on algorithm selection for
software verification, in: 2020 35th IEEE/ACM International Conference on Automated
Software Engineering (ASE), 2020, pp. 1016–1028.

[8] D. P. Kingma, M. Welling, Auto-encoding variational Bayes, 2013. URL: https://arxiv.org/

abs/1312.6114. doi:10.48550/ARXIV.1312.6114.

[9] H. Dai, Y. Tian, B. Dai, S. Skiena, L. Song, Syntax-directed variational autoencoder for

structured data, 2018. URL: https://arxiv.org/abs/1802.08786. doi:10.48550/ARXIV.1802.
08786.

[10] X. Si, A. Naik, H. Dai, M. Naik, L. Song, Code2inv: A deep learning framework for
program verification, in: Computer Aided Verification: 32nd International Conference,
CAV 2020, Los Angeles, CA, USA, July 21–24, 2020, Proceedings, Part II, Springer-Verlag,
Berlin, Heidelberg, 2020, p. 151–164. URL: https://doi.org/10.1007/978-3-030-53291-8_9.
doi:10.1007/978-3-030-53291-8_9.

[11] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. F. Zambaldi, M. Malinowski,
A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, Ç. Gülçehre, H. F. Song, A. J. Ballard,
J. Gilmer, G. E. Dahl, A. Vaswani, K. R. Allen, C. Nash, V. Langston, C. Dyer, N. Heess,
D. Wierstra, P. Kohli, M. Botvinick, O. Vinyals, Y. Li, R. Pascanu, Relational inductive
biases, deep learning, and graph networks, CoRR abs/1806.01261 (2018). URL: http://arxiv.
org/abs/1806.01261. arXiv:1806.01261.

[12] T. Ben-Nun, A. S. Jakobovits, T. Hoefler, Neural code comprehension: A learnable represen-
tation of code semantics, CoRR abs/1806.07336 (2018). URL: http://arxiv.org/abs/1806.07336.
arXiv:1806.07336.

[13] C. Lattner, V. Adve, LLVM: a compilation framework for lifelong program analysis amp;
transformation, in: International Symposium on Code Generation and Optimization, 2004.
CGO 2004., 2004, pp. 75–86. doi:10.1109/CGO.2004.1281665.

[14] T. Mikolov, M. Karafiát, L. Burget, J. H. Cernocký, S. Khudanpur, Recurrent neural network

based language model, in: INTERSPEECH, 2010.

[15] A. Horn, On sentences which are true of direct unions of algebras, The Journal of Symbolic

Logic 16 (1951) 14–21. URL: http://www.jstor.org/stable/2268661.

[16] M. Olsák, C. Kaliszyk, J. Urban, Property invariant embedding for automated reasoning,
CoRR abs/1911.12073 (2019). URL: http://arxiv.org/abs/1911.12073. arXiv:1911.12073.
[17] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. v. d. Berg, I. Titov, M. Welling, Modeling relational
data with graph convolutional networks, 2017. URL: https://arxiv.org/abs/1703.06103.
doi:10.48550/ARXIV.1703.06103.

[18] J. Wei, M. Goyal, G. Durrett, I. Dillig, LambdaNet: Probabilistic type inference using graph

neural networks, 2020. arXiv:2005.02161.

[19] R. E. Tarjan, Depth-first search and linear graph algorithms, SIAM J. Comput. 1 (1972)

146–160.

[20] G. Fedyukovich, P. Rümmer, Competition report: CHC-COMP-21, 2021. URL: https://

chc-comp.github.io/2021/report.pdf.

[21] P. Rümmer, H. Hojjat, V. Kuncak, Disjunctive interpolants for Horn-clause verification

(extended technical report), 2013. arXiv:1301.4973.

[22] R. W. Floyd, Assigning meanings to programs, Proceedings of Symposium on Applied
Mathematics 19 (1967) 19–32. URL: http://laser.cs.umass.edu/courses/cs521-621.Spr06/
papers/Floyd.pdf.

[23] C. A. R. Hoare, An axiomatic basis for computer programming, Commun. ACM 12 (1969)
576–580. URL: https://doi.org/10.1145/363235.363259. doi:10.1145/363235.363259.
[24] N. Bjørner, A. Gurfinkel, K. McMillan, A. Rybalchenko, Horn clause solvers for program

verification, 2015, pp. 24–51. doi:10.1007/978-3-319-23534-9_2.

[25] H. Hojjat, P. Ruemmer, The ELDARICA Horn solver, in: 2018 Formal Methods in Computer

Aided Design (FMCAD), 2018, pp. 1–7. doi:10.23919/FMCAD.2018.8603013.

[26] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, G. E. Dahl, Neural message passing for
quantum chemistry, CoRR abs/1704.01212 (2017). URL: http://arxiv.org/abs/1704.01212.
arXiv:1704.01212.

[27] K. Xu, W. Hu, J. Leskovec, S. Jegelka, How powerful are graph neural networks?, CoRR
abs/1810.00826 (2018). URL: http://arxiv.org/abs/1810.00826. arXiv:1810.00826.
[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polo-

sukhin, Attention is all you need, 2017. arXiv:1706.03762.

[29] J. Devlin, M. Chang, K. Lee, K. Toutanova, BERT: pre-training of deep bidirectional
transformers for language understanding, CoRR abs/1810.04805 (2018). URL: http://arxiv.
org/abs/1810.04805. arXiv:1810.04805.

[30] A. Radford, K. Narasimhan, Improving language understanding by generative pre-training,

2018.

[31] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang,
M. Zhou, CodeBERT: A pre-trained model for programming and natural languages, CoRR
abs/2002.08155 (2020). URL: https://arxiv.org/abs/2002.08155. arXiv:2002.08155.
[32] A. Kanade, P. Maniatis, G. Balakrishnan, K. Shi, Pre-trained contextual embedding
of source code, CoRR abs/2001.00059 (2020). URL: http://arxiv.org/abs/2001.00059.
arXiv:2001.00059.

[33] H. Husain, H. Wu, T. Gazit, M. Allamanis, M. Brockschmidt, CodeSearchNet challenge:
Evaluating the state of semantic code search, CoRR abs/1909.09436 (2019). URL: http:
//arxiv.org/abs/1909.09436. arXiv:1909.09436.

[34] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. B. Clement, D. Drain,
D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. Gong, M. Zhou, N. Duan,
N. Sundaresan, S. K. Deng, S. Fu, S. Liu, CodeXGLUE: A machine learning benchmark
dataset for code understanding and generation, CoRR abs/2102.04664 (2021). URL: https:
//arxiv.org/abs/2102.04664. arXiv:2102.04664.

[35] M. Allamanis, H. Peng, C. Sutton, A convolutional attention network for extreme summa-
rization of source code, CoRR abs/1602.03001 (2016). URL: http://arxiv.org/abs/1602.03001.
arXiv:1602.03001.

[36] U. Alon, M. Zilberstein, O. Levy, E. Yahav, Code2vec: Learning distributed representations
of code, Proc. ACM Program. Lang. 3 (2019) 40:1–40:29. URL: http://doi.acm.org/10.1145/
3290353. doi:10.1145/3290353.

[37] L. Mou, G. Li, Z. Jin, L. Zhang, T. Wang, TBCNN: A tree-based convolutional neural
network for programming language processing, CoRR abs/1409.5718 (2014). URL: http:
//arxiv.org/abs/1409.5718. arXiv:1409.5718.

[38] G. Irving, C. Szegedy, A. A. Alemi, N. Een, F. Chollet, J. Urban, DeepMath - deep
sequence models for premise selection,
in: D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, R. Garnett (Eds.), Advances in Neural Information Processing Systems
29, Curran Associates, Inc., 2016, pp. 2235–2243. URL: http://papers.nips.cc/paper/
6280-deepmath-deep-sequence-models-for-premise-selection.pdf.

[39] M. Wang, Y. Tang, J. Wang, J. Deng, Premise selection for theorem proving by deep
graph embedding,
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, R. Garnett (Eds.), Advances in Neural Information Processing Sys-

in:

tems 30, Curran Associates, Inc., 2017, pp. 2786–2796. URL: http://papers.nips.cc/paper/
6871-premise-selection-for-theorem-proving-by-deep-graph-embedding.pdf.

[40] A. Paliwal, S. M. Loos, M. N. Rabe, K. Bansal, C. Szegedy, Graph representations for
higher-order logic and theorem proving, CoRR abs/1905.10006 (2019). URL: http://arxiv.
org/abs/1905.10006. arXiv:1905.10006.

[41] M. Rawson, G. Reger, A neurally-guided, parallel theorem prover, in: A. Herzig, A. Popescu
(Eds.), Frontiers of Combining Systems, Springer International Publishing, Cham, 2019, pp.
40–56.

[42] A. Krizhevsky, I. Sutskever, G. E. Hinton,

ImageNet classification with deep con-
volutional neural networks,
in: F. Pereira, C. J. C. Burges, L. Bottou, K. Q.
Weinberger (Eds.), Advances in Neural Information Processing Systems, volume 25,
Curran Associates,
Inc., 2012. URL: https://proceedings.neurips.cc/paper/2012/file/
c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.

[43] D. Selsam, N. Bjørner, Neurocore: Guiding high-performance SAT solvers with unsat-
core predictions, CoRR abs/1903.04671 (2019). URL: http://arxiv.org/abs/1903.04671.
arXiv:1903.04671.

[44] D. Selsam, M. Lamm, B. Bünz, P. Liang, L. de Moura, D. L. Dill, Learning a SAT solver from
single-bit supervision, CoRR abs/1802.03685 (2018). URL: http://arxiv.org/abs/1802.03685.
arXiv:1802.03685.

[45] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, G. Monfardini, The graph neural
network model, IEEE Transactions on Neural Networks 20 (2009) 61–80. doi:10.1109/
TNN.2008.2005605.

[46] Y. Li, D. Tarlow, M. Brockschmidt, R. Zemel, Gated graph sequence neural networks, 2015.

URL: https://arxiv.org/abs/1511.05493. doi:10.48550/ARXIV.1511.05493.

[47] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò, Y. Bengio, Graph attention net-
works, 2017. URL: https://arxiv.org/abs/1710.10903. doi:10.48550/ARXIV.1710.10903.
[48] H. Gui, J. Liu, F. Tao, M. Jiang, B. Norick, J. Han, Large-scale embedding learning in

heterogeneous event data, 2016, pp. 907–912. doi:10.1109/ICDM.2016.0111.

[49] K. Tu, P. Cui, X. Wang, F. Wang, W. Zhu, Structural deep embedding for hyper-networks,
CoRR abs/1711.10146 (2017). URL: http://arxiv.org/abs/1711.10146. arXiv:1711.10146.
[50] Y. Feng, H. You, Z. Zhang, R. Ji, Y. Gao, Hypergraph neural networks, CoRR abs/1809.09401

(2018). URL: http://arxiv.org/abs/1809.09401. arXiv:1809.09401.

[51] T. N. Kipf, M. Welling, Semi-supervised classification with graph convolutional networks,
CoRR abs/1609.02907 (2016). URL: http://arxiv.org/abs/1609.02907. arXiv:1609.02907.
[52] S. Bai, F. Zhang, P. H. S. Torr, Hypergraph convolution and hypergraph attention, 2020.

arXiv:1901.08150.

Figure 4: An example to illustrate how to update node representation for R-HyGNN using (4). At
the right-hand side, there are three types of edges connected with node 1. We compute the updated
representation ℎ1
for node 1 at the time step 1. ‖ means concatenation. 𝑥𝑖 is the initial feature vector of
1
node 𝑖. The red blocks are the trace of the updating for node 1. The edge type 1 is a unary edge and is a
self-loop. It has one set of learnable parameters as the update function i.e., 𝑊𝑟0,𝑝1. The edge type 2 is
binary edge, it has two update functions i.e., 𝑊𝑟1,𝑝1 and 𝑊𝑟1,𝑝2. Node 1 is in the first position in edge
[1,2], [1,3], [1,4], and [1,5], so the concatenated node representation will be updated by 𝑊𝑟1,𝑝1 . On the
other hand, for the other two edges [6,1] and [7,1], node 1 is in the second position, so the concatenated
node representation will be updated by 𝑊𝑟1,𝑝2. For edge type 3, the same rule applies, i.e., depending on
node 1’s position in the edge, the concatenated node representation will go through different parameter
sets. Since there is no edge that node 1 is in the second position, we use a dashed box and arrow to
represent it. The aggregation is to add all updated representations from different edge types.

(a) Scatter plot for CDHG. The total 𝑟𝑠 node num-
ber is 16858. The mean square error is 4.22.

(b) Scatter plot for constraint graph. The total 𝑟𝑠
node number is 11131. The mean square error
is 1.04.

Figure 5: Scatter plot for Task 2. The x- and y-axis are in logarithmic scales.

Figure 6: An example to illustrate how to construct the Constraint graph from the CHC in (3). Here,
𝜙𝑟
𝑖

is the AST root node 𝜙𝑖.

Input: Simplified CHCs Π
Output: A constraint graph CG = (𝑉, BE, 𝑅𝐶𝐺, 𝑋 𝐶𝐺, ℓ)
Initialise graph: 𝑉, BE, ℓ = ∅, ∅, ∅;
for each clause 𝐶 in Π do

//step 1: construct the predicate layer;
for each relation symbol 𝑞(𝑡¯) in 𝐶 do

if 𝑣𝑞 /∈ 𝑉 then

//construct a 𝑟𝑠 node;
add a node 𝑣𝑞 with type 𝑟𝑠 to 𝑉 . Update map ℓ; //construct 𝑟𝑠𝑎 nodes and connect
them to the 𝑟𝑠 node;
for each argument 𝑡 in 𝑡¯ do

add a node 𝑣𝑡
𝑞
add a edge from 𝑣𝑞 to 𝑣𝑡
𝑞

with type 𝑟𝑠𝑎 to 𝑉 . Update map ℓ;
to with type RSA to BE ;

end

end

end
//step 2: construct the clause layer;
add a node 𝑣𝑐𝑙𝑎𝑢𝑠𝑒 with type 𝑐𝑙𝑎𝑢𝑠𝑒 to 𝑉 . Update map ℓ; add a clause head node 𝑣ℎ
𝑞
type 𝑐ℎ to 𝑉 for the relation symbol in 𝐶’ head. Update map ℓ;
add a edge from 𝑣𝑐𝑙𝑎𝑢𝑠𝑒 to 𝑣ℎ
𝑞
for each argument 𝑡 in 𝑡¯ do

with type CH to BE ;

add a node 𝑣ℎ,𝑡
add a edge from 𝑣ℎ
𝑞

𝑞

with type 𝑐𝑎 to 𝑉 . Update map ℓ;

to 𝑣ℎ,𝑡
𝑞

with type CA to BE ;

with

end
for each relation symbol 𝑞(𝑡¯) in 𝐶’ body do

add a clause body node 𝑣𝑏
𝑞
add a edge from 𝑣𝑏
𝑞
for each argument 𝑡 in 𝑡¯ do

with type 𝑐𝑏 to 𝑉 . Update map ℓ ;

to 𝑣𝑐𝑙𝑎𝑢𝑠𝑒 to with type CB to BE ;

add a node 𝑣𝑏,𝑡
𝑞
add a edge from 𝑣𝑏,𝑡
𝑞

with type 𝑐𝑎 to 𝑉 . Update map ℓ;
to with type CA to BE ;

to 𝑣𝑡
𝑞

end

end
//step 3: construct the constraint layer;
for each sub-expression 𝜑𝑖 in the constraint do
for each sub-expressions 𝑠𝑒 in 𝜑𝑖 do

if 𝑣𝑠𝑒 /∈ 𝑉 then

add a node 𝑣𝑠𝑒 with type 𝑟𝑠𝑎, 𝑣𝑎𝑟, 𝑐 or 𝑜𝑝 to 𝑉 . Update map ℓ;

end
add edge from 𝑣𝑠𝑒 to the left and right child of 𝑠𝑒 to BE with type 𝐴𝑠𝑡;

end

end
//step 4: add connection between three layers;
for each relation symbol node 𝑣𝑞 in predicate layer with type 𝑟𝑠 do

add edge (𝑣𝑞, 𝑣ℎ

𝑞 ) or (𝑣𝑏

𝑞, 𝑣𝑞) to BE with type 𝑅𝑆𝐼;

end
for each argument node 𝑣𝑡

add edge (𝑣𝑡

𝑞, 𝑣ℎ,𝑡

𝑞 ) or (𝑣𝑏,𝑡

𝑞 in predicate layer with type 𝑟𝑠𝑎 do
𝑞) to BE with type 𝐴𝐼;

𝑞 , 𝑣𝑡

end
for each root node of sub-expression 𝑣𝜑𝑟

𝑖 do

add edge (𝑣𝜑𝑟

, 𝑣𝑐𝑙𝑎𝑢𝑠𝑒) to BE with type Guard;

𝑖

end
for each node 𝑣𝑠𝑒 in AST tree with type 𝑣; if 𝑣𝑠𝑒 is an argument node do

add edge (𝑣𝑠𝑒, 𝑣ℎ,𝑡

𝑞 ) or (𝑣𝑏,𝑡

𝑞 , 𝑣𝑠𝑒) to BE with type Data;

end

end
return 𝑉, HE, ℓ

Algorithm 1: Construct a constraint graph from CHCs

Figure 7: An example to illustrate how to construct the CDHG from the CHC in (3). Here, 𝑔𝑟
are
𝑖
the AST root node of control flow sub-formula 𝑔𝑖 and the right-hand side of the data flow sub-formula,
respectively. The “|" connected edge type 𝐴𝑙 | 𝐴𝑟 means the edge could be type 𝐴𝑙 or 𝐴𝑟.

and 𝑑𝑟
𝑗

Input: Normalized CHCs Π
Output: A CDHG HG = (𝑉, HE, 𝑅𝐶𝐷𝐻𝐺, 𝑋 𝐶𝐷𝐻𝐺, ℓ)
Initialise graph: 𝑉, HE, ℓ = ∅, ∅, ∅;
for each clause 𝐶 in Π do

Add node 𝑣𝑖𝑛𝑖𝑡𝑖𝑎𝑙 and 𝑣false with type initial and false to 𝑉 . Update map ℓ;
//step 1: construct relation symbols and their arguments as typed nodes and add the 𝑅𝑆𝐴
edge between them;
for each relation symbol 𝑞(𝑡¯) in 𝐶 do

if 𝑣𝑞 /∈ 𝑉 then

add a node 𝑣𝑞 with type 𝑟𝑠 to 𝑉 . Update map ℓ;
for each argument 𝑡 in 𝑡¯ do

add a node 𝑣𝑡
𝑞
add a edge from 𝑣𝑞 to 𝑣𝑡
𝑞

with type 𝑟𝑠𝑎 to 𝑉 . Update map ℓ;
to with type RSA to HE ;

end

end

end
//step 2: construct ASTs for control flow sub-formulas in the constraints and connects the

roots of ASTs to an abstract node with type Guard;
add a node 𝑣𝑔 with type 𝑔𝑢𝑎𝑟𝑑 to 𝑉 . Update map ℓ; for each 𝑔𝑖 do

for each sub-expression 𝑠𝑒 in 𝑔𝑖 do

if 𝑣𝑠𝑒 /∈ 𝑉 then

add a node 𝑣𝑠𝑒 with type 𝑟𝑠𝑎, 𝑣𝑎𝑟, 𝑐, or 𝑜𝑝 to 𝑉 . Update map ℓ;

end
add edge from 𝑣𝑠𝑒 to left and right child of 𝑠𝑒 to HE with type 𝐴𝑙 and 𝐴𝑟,
respectively;

end
add edge from 𝑔𝑟
𝑖

to 𝑣𝑔 with type Guard to HE , where 𝑔𝑟
𝑖

is the root node of 𝑔𝑖’s AST;

end
//step 3: construct CFHEs;
if 𝐶’s body ̸= ∅ then

for each relation symbol 𝑞(𝑡¯) in 𝐶’s body do

add a ternary hyperedge (𝑣𝑞(ℎ𝑒𝑎𝑑), 𝑣𝑞(𝑏𝑜𝑑𝑦𝑖), 𝑣𝑔) with type CFHE , where 𝑣𝑞(ℎ𝑒𝑎𝑑) is
the node with type 𝑟𝑠 or false in the head, and 𝑣𝑞(𝑏𝑜𝑑𝑦𝑖) is the node with type 𝑟𝑠 in
the body;

end

else

add a ternary hyperedge (𝑣𝑞(ℎ𝑒𝑎𝑑), 𝑣𝑖𝑛𝑖𝑡𝑖𝑎𝑙, 𝑣𝑔) with type CFHE to HE ;

end
for each 𝑑𝑗 do

//step 4: construct AST for right-hand side of data flow sub-formula 𝑑𝑗 ;
for each sub-expression 𝑠𝑒 in right-hand side of 𝑑𝑗 do

if 𝑣𝑠𝑒 /∈ 𝑉 then

add a node 𝑣𝑠𝑒 with type 𝑟𝑠𝑎, 𝑣𝑎𝑟, 𝑐, or 𝑜𝑝 to 𝑉 . Update map ℓ;

end
add edge from 𝑣𝑠𝑒 to left and right child of 𝑠𝑒 to HE with type 𝐴𝑙 or 𝐴𝑟 ;

end
//step 5: construct DFHEs;
is the left-hand
add a ternary hyperedge (𝑣𝑡
𝑗 , 𝑣𝑔) with type DFHE to HE, where 𝑣𝑡
𝑞
side element of 𝑑𝑗 (a node with type 𝑟𝑠𝑎) and 𝑑𝑟
is the root node of 𝑑𝑖’s AST (a node
𝑖
with type 𝑟𝑠𝑎, 𝑣𝑎𝑟, 𝑐 or 𝑜𝑝 );

𝑞, 𝑑𝑟

end

end
return 𝑉, HE, ℓ

Algorithm 2: Construct a CDHG from CHCs

