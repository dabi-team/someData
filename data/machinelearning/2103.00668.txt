1
2
0
2

n
u
J

7
1

]
L
M

.
t
a
t
s
[

3
v
8
6
6
0
0
.
3
0
1
2
:
v
i
X
r
a

Learning Proposals for Probabilistic Programs with Inference Combinators

Sam Stites*1

Heiko Zimmermann*1

Hao Wu1

Eli Sennesh1

Jan-Willem van de Meent1

1Khoury College of Computer Sciences, Northeastern University, Boston, Massachusetts, USA
1{stites.s, zimmermann.h, wu.hao10, e.sennesh, j.vandemeent}@northeastern.edu

Abstract

We develop operators for construction of propos-
als in probabilistic programs, which we refer to
as inference combinators. Inference combinators
deﬁne a grammar over importance samplers that
compose primitive operations such as application
of a transition kernel and importance resampling.
Proposals in these samplers can be parameterized
using neural networks, which in turn can be trained
by optimizing variational objectives. The result is
a framework for user-programmable variational
methods that are correct by construction and can
be tailored to speciﬁc models. We demonstrate the
ﬂexibility of this framework by implementing ad-
vanced variational methods based on amortized
Gibbs sampling and annealing.

1 INTRODUCTION

One of the major ongoing developments in probabilistic
programming is the integration of deep learning with ap-
proaches for inference. Libraries such as Edward (Tran et
al., 2016), Pyro (Bingham et al., 2018), and Probabilistic
Torch (Siddharth et al., 2017), extend deep learning frame-
works with functionality for the design and training of deep
probabilistic models. Inference in these models is typically
performed with amortized variational methods, which learn
an approximation to the posterior distribution of the model.

Amortized variational inference is widely used in the train-
ing of variational autoencoders (VAEs). While standard
VAEs employ an unstructured prior over latent variables,
such as a spherical Gaussian, in probabilistic programming,
we are often interested in training a neural approximation
to a simulation-based model (Baydin et al., 2019), or more
generally to perform inference in models that employ pro-
grammatic priors as inductive biases. This provides a path

∗ Equal contribution

to incorporating domain knowledge into methods for learn-
ing structured representations in a range of applications,
such as identifying objects in an image or a video (Greff
et al., 2019), representing users and items in reviews (Es-
maeili et al., 2019), and characterizing the properties of
small molecules (Kusner et al., 2017).

While amortized variational methods are very general, they
often remain difﬁcult to apply to structured domains, where
gradient estimates based on a small number of (reparameter-
ized) samples are often not sufﬁcient. A large body of work
improves upon standard methods by combining variational
inference with more sophisticated sampling schemes based
on Markov chain Monte Carlo (MCMC) and importance
sampling (e.g. Naesseth et al. (2018); Le et al. (2019); Wu et
al. (2020); see Appendix A for a comprehensive discussion).
However to date, few of these methods have been imple-
mented in probabilistic programming systems. One reason
for this is that many methods rely on a degree of knowledge
about the structure of the underlying model, which makes it
difﬁcult to develop generic implementations.

In this paper, we address this difﬁculty by considering a de-
sign that goes one step beyond that of traditional probabilis-
tic programming systems. Instead of providing a language
for the deﬁnition of models in the form of programs, we
introduce a language for the deﬁnition of sampling strate-
gies that are applicable to programs. This is an instance of a
general idea that is sometimes referred to as inference pro-
gramming (Mansinghka et al., 2014). Instantiations of this
idea include inference implementations as monad transform-
ers ( ´Scibior et al., 2018), inference implementations using
low-level primitives for integration (Obermeyer et al., 2019),
and programming interfaces based on primitive operations
for simulation, generation, and updating of program traces
(Cusumano-Towner et al., 2019).

Here we develop a new approach to inference programming
based on constructs that we refer to as inference combi-
nators, which act on probabilistic programs that perform
importance sampling during evaluation. A combinator ac-

Accepted for the 37th Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2021).

 
 
 
 
 
 
cepts one or more programs as inputs and returns a new
program. Each combinator denotes an elementary operation
in importance sampling, such as associating a proposal with
a target, composition of a program and a transition kernel,
and importance resampling. The result is a set of constructs
that can be composed to deﬁne user-programmable impor-
tance samplers for probabilistic programs that are valid by
construction, in the sense that they generate samples that are
properly weighted (Liu, 2008; Naesseth et al., 2015) under
the associated target density of the program. These samplers
in turn form the basis for nested variational methods (Zim-
mermann et al., 2021) that minimize a KL divergence to
learn neural proposals.

We summarize the contributions of this paper as follows:

et al., 2018), Gen (Cusumano-Towner et al., 2019), Pyro
(Bingham et al., 2018), and Edward2 (Tran et al., 2016)2.

To deﬁne semantics for the inference language, we assume
the existence of semantics for the modeling language. Con-
cretely, we postulate that there exist denotational measure
semantics for the density of a program. We will also postu-
late that programs have corresponding operational sampler
semantics equivalent to likelihood weighting. In the next
section, we will use these axiomatic semantics to deﬁne
operational semantics for inference combinators such that
samplers deﬁned by these combinators are valid by construc-
tion, in the sense that evaluation yields properly weighted
samples for the density denoted by the program. Below, we
discuss the preliminaries that we need for this exposition.

• We develop a language for inference programming in
probabilistic programs, in which combinators deﬁne a
grammar over properly-weighted importance samplers
that can be tailored to speciﬁc models.

• We formalize semantics for inference and prove that
samplers in our language are properly weighted for the
density of a program.

• We demonstrate how variational methods can be used
to learn proposals for these importance samplers,
and thereby deﬁne a language for user-programmable
stochastic variational methods.

• We provide an implementation of inference combi-
nators for the Probabilistic Torch library1. We evalu-
ate this implementation by using it to deﬁne existing
state-of-the-art methods for probabilistic programs that
improve over standard methods.

2 PRELIMINARIES

The combinators in this paper deﬁne a language for samplers
that operate on probabilistic programs. We will refer to this
language as the inference language. The probabilistic pro-
grams that inference operates upon are themselves deﬁned
in a modeling language. Both languages require semantics.
For the inference language, semantics formally deﬁne the
sampling strategy, whereas the semantics of the modeling
language deﬁne the target density for the inference problem.

We deliberately opt not to deﬁne semantics for a speciﬁc
modeling language. Our implementation is based on Prob-
abilistic Torch, but combinators are applicable to a broad
class of modeling languages that can incorporate control
ﬂow, recursion, and higher-order functions. Our main tech-
nical requirement is that all sampled and observed vari-
ables must have tractable conditional densities. This re-
quirement is satisﬁed in many existing languages, including
Church (Goodman et al., 2008), Anglican (Wood et al.,
2014), WebPPL (Goodman, Stuhlmüller, 2014), Turing (Ge

2.1 LIKELIHOOD WEIGHTING

Probabilistic programs deﬁne a distribution over variables
in a programmatic manner. As a simple running example,
we consider the following program in Probabilistic Torch

ηv , ηx = ... # ( initialize generator networks )
def f(s , x ):

# select mixture component
z = s. sample ( Multinomial (1 , 0.2* ones (5)) , " z")
# sample image embedding
v = s. sample ( Normal (ηv
# condition on input image
s. observe ( Normal (ηx (v), 1) , x , "x")
return s , x , v , z

σ (z )) , "v")

µ (z), ηv

Program 1: A deep generative mixture model

This program corresponds to a density p(x, v, z), in which z
and v are unobserved variables and x is an observed variable.
We ﬁrst deﬁne a multinomial prior p(z), and then deﬁne
conditional distributions p(v | z) and p(x | v) using a set
of generator networks η. The goal of inference is to reason
about the posterior p(v, z | x). We refer to the object s as
the inference state. This data structure stores variables that
need to be tracked as side effects of the computation, which
we discuss in more detail below.

In this paper, the base case for evaluation performs like-
lihood weighting. This is a form of importance sampling
in which unobserved variables are sampled from the prior,
and are assigned an (unnormalized) importance weight ac-
cording to the likelihood. In the example above, we would
typically execute the program in a vectorized manner; we
would input a tensor of B samples xb ∼ ˆp(x) from an
empirical distribution, generate tensors of B × L samples
vb,l, zb,l ∼ p(v, z) from the prior, and compute

wb,l =

p(xb, vb,l, zb,l)
p(vb,l, zb,l)

= p(xb | vb,l, zb,l).

1Code is available at github.com/probtorch/combinators.

2See Appendix A for an extensive discussion of related work.

These weights serve to compute a self-normalized approxi-
mation of a posterior expectation of some function h(x, v, z)

E ˆp(x) p(v,z|x) [h(x, v, z)] (cid:39)

1
B

(cid:88)

b,l

wb,l
l(cid:48) wb,l(cid:48) h(xb, vb,l, zb,l).

(cid:80)

(1)

Self-normalized estimators are consistent, but not unbiased.
However, it is the case that each weight wb,l is a unbiased
estimate of the marginal likelihood p(xb).

2.2 TRACED EVALUATION

Probabilistic programs can equivalently denote densities
and samplers. In the context of speciﬁc languages, these two
views can be formalized in terms of denotational measure
semantics and operational sampler semantics ( ´Scibior et
al., 2017). To deﬁne operational semantics for inference
combinators, we begin by postulating sampler semantics
for programs that perform likelihood weighting. We assume
these semantics deﬁne an evaluation relation

c, τ, ρ, w (cid:59) f(c(cid:48)).

In this relation, f is a program, c(cid:48) are its input arguments,
and c are its return values. Evaluation additionally outputs a
weight w, a density map ρ, and a trace τ .3

A trace stores values for all sampled variables in an execu-
tion. We mathematically represent a trace as a mapping,

τ = [α1 (cid:55)→ c1, . . . , αn (cid:55)→ cn].

Here each αi is an address for a random variable and ci is its
corresponding value. Addresses uniquely identify a random
variable. In Probabilistic Torch, as well as in languages
like Pyro and Gen, each sample or observe call accepts an
address as the identiﬁer. In the example above, evaluation
returns a trace ["v" (cid:55)→ v, "z" (cid:55)→ z], where z, v ∼ p(z, v).

The density map stores the value of the conditional density
for all variables in the program,

ρ = [α1 (cid:55)→ r1, . . . , αn (cid:55)→ rn],

ri ∈ [0, ∞).

Whereas the trace only stores values for unobserved vari-
ables, the density map stores probability densities for both
observed and unobserved variables. In the example above,
evaluation of a program would output a map for the variables
with addresses "x", "v" and "z",

["x" (cid:55)→ p(x | v), "v" (cid:55)→ p(v|z), "z" (cid:55)→ p(z)],

in which conditional densities are computed using the traced
values v = τ ("v"), z = τ ("z"), and the observed value x,
which is an input to the program.

3In Probabilistic Torch, we return τ , ρ, and w by storing them

We deﬁne the weight in the evaluation as the joint proba-
bility of all observed variables (i.e. the likelihood). Since
the density map ρ contains entries for all variables, and the
trace only contains unobserved variables, the likelihood is

w =

(cid:89)

ρ(α).

α∈dom(ρ)\dom(τ )

To perform importance sampling, we will use a trace from
one program as a proposal for another program. For this
purpose, we deﬁne an evaluation under substitution

c, τ, ρ, w (cid:59) f(c(cid:48))[τ (cid:48)].

In this relation, values in τ (cid:48) are substituted for values of
unobserved random variables in f. This is to say that eval-
uation reuses τ (α) = τ (cid:48)(α) when a value exists at address
α, and samples from the program prior when it is not. This
is a common operation in probabilistic program inference,
which also forms the basis for traced Metropolis-Hastings
methods (Wingate et al., 2011).

When performing evaluation under substitution, the set of
reused variables is the intersection dom(τ ) ∩ dom(τ (cid:48)). Con-
versely, the newly sampled variables are dom(τ ) \ dom(τ (cid:48)).
We deﬁne the weight of an evaluation under substitution as
the joint probability of all observed and reused variables

w =

(cid:89)

ρ(α).

(2)

α∈dom(ρ)\(cid:0)

dom(τ )\dom(τ (cid:48))(cid:1)

2.3 DENOTATIONAL SEMANTICS

To reason about the validity of inference approaches, we
need to formalize what density a program denotes. For this
purpose, we assume the existence of denotational semantics
that deﬁne a prior and unnormalized density

f(c(cid:48))
(cid:75)
(cid:74)

γ(τ ) = γf (τ ; c(cid:48)),

f(c(cid:48))

p(τ ) = pf (τ ; c(cid:48)).
(cid:75)

(cid:74)

Given an unnormalized density, the goal of inference is to
approximate the corresponding normalized density

πf (τ ; c(cid:48)) =

γf (τ ; c(cid:48))
Zf (c(cid:48))

,

(cid:90)

Zf (c(cid:48)) =

dτ γf (τ ; c(cid:48)).

The reason that we specify a program as a density over
traces, rather than as a density over speciﬁc variables, is
that programs in higher-order languages with control ﬂow
and/or recursion may not always instantiate the same set of
variables. A program could, for example, perform a random
search that instantiates different variables in every evalua-
tion (van de Meent et al., 2016). We refer to van de Meent
et al. (2018) for a more pedagogical discussion of this point.

in the inference state s during evaluation.

Formal speciﬁcation of denotational semantics gives rise

to substantial technical questions4, but in practice the un-
normalized density is computable for programs in most
probabilistic languages. In the languages that we consider
here, in which conditional densities for all random variables
are tractable, the unnormalized density is simply the joint
probability of all variables in the program. Concretely, we
postulate the following relationship between the sampler
and the measure semantics of a program

c, τ, ρ, w (cid:59) f(c(cid:48))

f(c(cid:48))

(cid:74)

γ(τ ) =
(cid:75)

(cid:89)

ρ(α)

α∈dom(ρ)

f(c(cid:48))

(cid:74)

(cid:75)

(cid:89)

p(τ ) =

ρ(α)

α∈dom(τ )

In this notation, the top of the rule lists conditions, and
the bottom states their implications. This rule states that,
for any trace τ and density map ρ that can be generated
by evaluating f(c(cid:48)), the unnormalized density
f(c(cid:48))
γ(τ )
(cid:74)
evaluates to the product of the conditional probabilities in
p(τ ) corresponds to the
ρ, whereas the prior density
(cid:74)
product for all unobserved variables. This implies that the
trace is distributed according to the prior, and that the weight
is the ratio between the unnormalized density and the prior

f(c(cid:48))

(cid:75)

(cid:75)

w = γf (τ ; c(cid:48))/pf (τ ; c(cid:48)),

τ ∼ pf (τ ; c(cid:48)).

The above rule implicitly deﬁnes the support Ωf of the
density, in that it only deﬁnes the density for traces τ that
can be generated by evaluating f(c(cid:48)). In the languages that
we are interested in here, Ωf may not be statically deter-
minable through program analysis, but our exposition does
not require its explicit characterization.

Finally, we deﬁne the density that a program denotes under
substitution. We will deﬁne the unnormalized density to be
invariant under substitution, which is to say that

f(c(cid:48))[τ (cid:48)]
(cid:75)

(cid:74)

γ(τ ) = γf [τ (cid:48)](τ ; c(cid:48)) = γf (τ ; c(cid:48)).

For the prior under substitution we use the notation

f(c(cid:48))[τ (cid:48)]
(cid:75)

(cid:74)

p(τ ) = pf [τ (cid:48)](τ ; c(cid:48)),

to denote a density over newly sampled variables, rather
than over all unobserved variables,

c, τ, ρ, w (cid:59) f(c(cid:48))[τ (cid:48)]

f(c(cid:48))[τ (cid:48)]
(cid:75)

(cid:74)

p(τ ) =

(cid:89)

ρ(α)

α∈dom(τ )\dom(τ (cid:48))

This
construction
γf [τ (cid:48)](τ ; c(cid:48))/pf [τ (cid:48)](τ ; c(cid:48)), as
substitution is performed.

ensures

that

=
in the case where no

w

4Traditional measure theory based on Borel sets does not sup-
port higher-order functions. Recent work addresses this problem
using a synthetic measure theory for quasi-Borel spaces, which in
turn serves to formalize denotational semantics for a simply-typed
lambda calculus ( ´Scibior et al., 2017). In practice, these technical
issues do not give rise to problems, since existing languages cannot
construct objects that give rise to issues with measurability.

Figure 1: Inference combinators denote fundamental oper-
ations in importance sampling, which can be composed to
deﬁne a sampling strategy. See main text for details.

As previously, the support Ωf [τ (cid:48)] is deﬁned implicitly as
the set of traces that can be generated via evaluation under
substitution, which is a subset of the original support

Ωf [τ (cid:48)] = {τ ∈ Ωf : τ (α) = τ (cid:48)(α) ∀ α ∈ dom(τ ) ∩ dom(τ (cid:48))} .

3

INFERENCE COMBINATORS

We develop a domain-speciﬁc language (DSL) for impor-
tance samplers in terms of four combinators, with a grammar
that deﬁnes their possible compositions

f ::= A primitive program
p ::= f | extend(p, f)
q ::= p | resample(q) | compose(q(cid:48), q) | propose(p, q)

We distinguish between three expression types. We use f
to refer to a primitive program in the modeling language,
with sampler semantics that perform likelihood weighting
as described in the preceding section. We use p to refer to
a target program, which is either a primitive program, or a
composition of primitive programs that deﬁnes a density on
an extended space. Finally, we use q to refer to an inference
program that composes inference combinators.

Figure 1 shows a simple program that illustrates the use
of each combinator, for which we will formally specify
semantics below. The ﬁrst step deﬁnes a program q1 that
generates samples from a primitive program f1, which are
equally weighted (i.e. f1 does not have observed variables).
The second step deﬁnes a program q2 = propose(f2, q1)
that uses samples from q1 as proposals for the primitive
program f2, which results in weights that are proportional
to the ratio of densities. The third step q3 = resample(q2)
performs importance resampling, which replicates samples
with probability proportional to their weights. In the fourth
step, we use compose(f3, q3) to apply a program f3 that
denotes a transition kernel, and use the resulting samples
as proposals for a program extend(f2, f4), which deﬁnes
a density on an extended space. To illustrate this extended
space construction, we will consider a more representative
example of an inference program.

3.1 EXAMPLE: AMORTIZED GIBBS SAMPLING

Figure 2 shows a program that makes use of a combinator
DSL that is embedded in Python. This code implements
amortized population Gibbs (APG) samplers (Wu et al.,
2020), a recently developed method that combines stochas-
tic variational inference with sequential Monte Carlo (SMC)
samplers to learn a set of conditional proposals that approxi-
mate Gibbs updates. This is an example where combinators
are able to concisely express an algorithm that would be
non-trivial to implement from scratch, even for experts.

We brieﬂy discuss each operation in this algorithm. The
function pop_gibbs (Figure 2, left) accepts programs that
denote a target density, an initial proposal, and a set of
kernels. It returns a program q that performs APG sampling.
This program is evaluated in the right column to generate
samples, compute an objective, and perform gradient de-
scent to train the initial proposal and the kernels.

APG samplers perform a series of sweeps, where each sweep
iterates over proposals that approximate Gibbs conditionals.
To understand the weight computation in this sampler, we
consider Program 1 as a (non-representative) example. Here
updates could take the form of block proposals q(v | x, z)
and q(z | x, v). At each step, we construct an outgoing
sample by either updating v or z given an incoming sample
(w, v, z). Here we consider an update of the variable v,

w(cid:48) =

p(x, v(cid:48), z) q(v | x, z)
q(v(cid:48) | x, z) p(x, v, z)

w,

v(cid:48) ∼ q(· | x, z).

This weight update makes use of an auxiliary variable con-
struction, in which we compute the ratio between a target
density and proposal on an extended space

˜p(x, v(cid:48), z, v) = p(x, v(cid:48), z) q(v | x, z),
˜q(v(cid:48), z, v | x) = q(v(cid:48) | x, z) p(x, v, z).

In the inner loop for each sweep, we use the extend com-
binator to deﬁne the extended target ˜p(x, v(cid:48), z, v). To gen-
erate the proposal, we use the compose combinator to com-
bine the incoming sampler with the kernel, which deﬁnes
the extended proposal ˜q(v(cid:48), z, v | x). Since the marginal
˜p(x, v(cid:48), z) = p(x, v(cid:48), z), the outgoing sample (w(cid:48), v(cid:48), z) is
properly weighted for the target density as long as the in-
coming sample is properly weighted.

3.2 PROGRAMS AS PROPOSALS

λu , λz = ... # ( initialize encoder networks )
def g(s , x ):

u = s . sample ( Normal (λu
σ (x )) , "u")
z = s. sample ( Multinomial (1 , λz (u), "z")
return s , x , u , z

µ (x), λu

Suppose that we use propose(f, g) to associate this pro-
posal with the target in Program 1, which deﬁnes a density
p(x, v, z). We then have a superﬂuous variable with address
"u", as well as a missing variable with address "v". To deal
with this problem, we implicitly extend the target using the
conditional density q(u | x) from the inference program,
and conversely extend the proposal using the conditional
density p(v | z) from the target program,

˜p(x, v, z, u) = p(x, v, z) q(u | x),

˜q(v, z, u | x) = q(u, z | x) p(v | z).

Since, by construction, the conditional densities for u and v
are identical in the target and proposal, these terms cancel
when computing the importance weight

w =

˜p(x, v, z, u)
˜q(v, z, u | x)

=

p(x, v, z) q(u | x)
q(u, z | x) p(v | z)

=

p(x | v) p(z)
q(z | u)

.

This motivates a general importance sampling computation
for primitive programs of the following form

c1, τ1, ρ1, w1 (cid:59) g(c0),

c2, τ2, ρ2, w2 (cid:59) f(c0)[τ1].

We generate τ1 from the proposal, and then use substitution
to generate a trace τ2 that reuses as many variables from τ1
as possible, and samples any remaining variables from the
prior. Here the set of missing variables is dom(τ2)\dom(τ1)
and the set of superﬂuous variables is dom(τ1) \ dom(τ2).
Hence, we can deﬁne the importance weight

w =

γf (τ2; c0) pg[τ2](τ1; c0)
γg(τ1; c0) pf [τ1](τ2; c0)

w1,

=

w2
α∈dom(ρ1)\(dom(τ1)\dom(τ2)) ρ1(α)

(cid:81)

w1.

(3)

In the numerator, we take the product over all terms in the
target density, exclusive of missing variables. Note that this
expression is identical to w2 (Equation 2). In the denomina-
tor, we take the product over all terms in the proposal density
ρ1, exclusive of superﬂuous variables dom(τ1) \ dom(τ2).

When we use a program as a proposal for another program,
it may not be the case that the proposal and target instantiate
the same set of variables. Here two edge cases can arise:
(1) the proposal contains superﬂuous variables that are not
referenced in the target, or (2) there are variables in the
target that are missing from the proposal.

To account for both cases, we deﬁne an implicit auxiliary
variable construction. To illustrate this construction, we
consider a proposal that deﬁnes a density q(u, z | x)

3.3 PROPERLY WEIGHTED PROGRAMS

The expression in Equation 3 is not just valid for a composi-
tion propose(f, g) in which f and g are primitive programs.
As we will show, this expression also applies to any compo-
sition propose(p, q), in which p is a target program, and
q is itself an inference program. While this distinction may
seem subtle, it is fundamental; it allows us to use any sam-
pler q as a proposal, which yields a new sampler that can
once again be used as a proposal.

def pop_gibbs ( target , proposal , kernels , sweeps ):

q = propose ( partial ( target , suffix =0) ,

partial ( proposal , suffix =0))

for s in range ( sweeps ):

for k in kernels :
q = propose (

extend ( partial ( target , suffix =s +1) ,

partial (k , suffix =s )) ,

compose ( partial (k , suffix =s +1) ,

resample (q , dim =0)))

return q

data , opt = ...
target , proposal , kernels = ...
q = pop_gibbs ( target , proposal , kernels )
for _ in range (10000):

s0 = State ( sample_size =[40 , 20] ,

objective = inc_kl )
s , * outputs = q(s0 , data . next_batch (20))
s. loss . backward ()
opt . step ()
opt . zero_grad ()

Figure 2: A combinator-based implementation of amortized population Gibbs sampling (Wu et al., 2020) in python, along
with a procedure for training the target model, initial proposal, and each of the kernels that approximate Gibbs conditionals.
In this example partial is the partial application function from Python 3’s functools library.

To demonstrate the validity of combinator composition, we
make use of the framework of nested importance samplers
and proper weighting (Liu, 2008; Naesseth et al., 2015),
which we extend to evaluation of probabilistic programs.

Deﬁnition 1 (Properly Weighted Evaluation). Let q(c(cid:48))
= γq(· ; c(cid:48)). Let
denote an unnormalized density
πq(·; c(cid:48)) denote the corresponding probability density and
let Zq(c(cid:48)) denote the normalizing constant such that

q(c(cid:48))

(cid:74)

(cid:75)

πq(τ ; c(cid:48)) :=

γq(τ ; c(cid:48))
Zq(c(cid:48))

,

(cid:90)

Zq(c(cid:48)) :=

dτ γq(τ ; c(cid:48)).

We refer to the evaluation of c, τ, ρ, w (cid:59) q(c(cid:48)) as properly
weighted for its unnormalized density γq(· ; c(cid:48)) when, for all
measurable functions h

E
q(c(cid:48))[w h(τ )] = Cq(c(cid:48))

(cid:90)

dτ γq(τ ; c(cid:48)) h(τ ),

= Cq(c(cid:48)) Zq(c(cid:48)) Eπq(·;c(cid:48))[h(τ )],

for some constant Cq(c(cid:48)) > 0. When Cq(c(cid:48)) = 1, we refer to
the evaluation as strictly properly weighted.

A properly weighted evaluation can be used to deﬁne self-
normalized estimators of the form in Equation 1. Such es-
timators are strongly consistent, which is to say that they
converge almost surely in the limit of inﬁnite samples L

1
L

(cid:80)L

l=1 wl h(τ l)
(cid:80)L
l=1 wl

1
L

a.s.−→ Eπq(·;c(cid:48)) [h(τ )] .

prior density

f(c(cid:48))
(cid:75)

(cid:74)

p = pf (· ; c(cid:48)),

f(c(cid:48)) [w h(τ )] = Epf (·;c(cid:48))
E

(cid:20) γf (τ ; c(cid:48))
pf (τ ; c(cid:48))

(cid:21)

h(τ )

,

= Zf (c(cid:48)) Epf (·;c(cid:48))

(cid:20) πf (τ ; c(cid:48))
pf (τ ; c(cid:48))

(cid:21)

h(τ )

,

= Zf (c(cid:48)) Eπf (·;c(cid:48)) [h(τ )] .

3.4 OPERATIONAL SEMANTICS

Given a modeling language for primitive programs whose
evaluation is strictly properly weighted, our goal is to show
that the inference language preserves strict proper weighting.
To do so, we begin by formalizing rules for evaluation of
each combinator, which together deﬁne big-step operational
semantics for the inference language.

Compose. We begin with the rule for compose(q2,q1),
which performs program composition.

c1, τ1, ρ1, w1 (cid:59) q1(c0)

c2, τ2, ρ2, w2 (cid:59) q2(c1)

dom(ρ1) ∩ dom(ρ2) = ∅
c2, τ2 ⊕ τ1, ρ2 ⊕ ρ1, w2 · w1 (cid:59) compose(q2, q1)(c0)
This rule states that we can evaluate compose(q2, q1)(c0)
by ﬁrst evaluating q1(c0) and using the returned values c1 as
the inputs when evaluating q2(c1). We return the resulting
value c2 with weight w2 · w1. We combine traces τ2 ⊕ τ1
and density maps ρ2 ⊕ ρ1 using the operator ⊕, which we
deﬁne for maps µ1 and µ2 with disjoint domains as
(cid:40)

(µ1 ⊕ µ2)(α) =

µ1(α) α ∈ dom(µ1),
µ2(α) α ∈ dom(µ2).

Proposition 1 (Proper Weighting of Primitive Programs).
Evaluation of a primitive program f(c(cid:48)) is strictly properly
weighted for its unnormalized density

f(c(cid:48))
(cid:75)

(cid:74)

γ.

Proof. This holds by deﬁnition, since in a primitive program
w is uniquely determined by τ , which is a sample from the

Extend. The combinator extend(p, f) performs a com-
position between a target p and a primitive f which deﬁnes
a density on an extended space.
c2, τ2, ρ2, w2 (cid:59) f(c1)
c1, τ1, ρ1, w1 (cid:59) p(c0)
dom(ρ1) ∩ dom(ρ2) = ∅
dom(ρ2) = dom(τ2)
c2, τ1 ⊕ τ2, ρ1 ⊕ ρ2, w1 · w2 (cid:59) extend(p, f)(c0)

The program f deﬁnes a “kernel”, which may not con-
tain observed variables. We enforce this by requiring that
dom(ρ2) = dom(τ2).

Propose. The extend operator serves to incorporate aux-
iliary variables into a target density. When evaluating
propose(p, q), we discard auxiliary variables to continue
the inference computation. For this purpose, we deﬁne
a transformation marginal(p) to recover the original un-
extended program. We deﬁne this transformation recursively

f = marginal(f)

f(cid:48) = marginal(p)
f(cid:48) = marginal(extend(p, f))

We now deﬁne the operational semantics for propose as
c2, τ2, ρ2, w2 (cid:59) p(c0)[τ1]

c1, τ1, ρ1, w1 (cid:59) q(c0)

c3, τ3, ρ3, w3 (cid:59) marginal(p)(c0)[τ2]
(cid:89)

ρ1(α)

u1 =

α∈dom(ρ1)\(dom(τ1)\dom(τ2))
c3, τ3, ρ3, w2 · w1/u1 (cid:59) propose(p, q)(c0)

In this rule, the outgoing weight w2 · w1/u1 corresponds
precisely to Equation 3, since w2 is equal to the numerator,
whereas u1 is equal to the denominator in this expression.

Evaluation of p(c0)[τ1] applies substitution recursively to
sub-expressions (see Appendix C). Note that, by construc-
tion, evaluation of marginal(p)(c0)[τ2] is deterministic,
and that τ3 and ρ3 correspond to the entries in τ2 and ρ2 that
are associated with the unextended target.

Resample. This combinator performs importance resam-
pling on the return values, the trace, and the density map.
Since resampling is an operation that applies to a collection
of samples, we use a notational convention in which cl, τ l,
ρl, wl refer to elements in vectorized objects, and (cid:126)c, (cid:126)τ , (cid:126)ρ, (cid:126)w
refer to vectorized objects in their entirety5. Resampling as
applied to a vectorized program has the semantics

(cid:126)c1, (cid:126)τ1, (cid:126)ρ1, (cid:126)w1 (cid:59) q((cid:126)c0) (cid:126)a1 ∼ RESAMPLE( (cid:126)w1)

(cid:126)c2, (cid:126)τ2, (cid:126)ρ2 = REINDEX((cid:126)a1, (cid:126)c1, (cid:126)τ1, (cid:126)ρ1)

(cid:126)w2 = MEAN( (cid:126)w1)

(cid:126)c2, (cid:126)τ2, (cid:126)ρ2, (cid:126)w2 (cid:59) resample(q)((cid:126)c0)
In this rule, we make use of three operations. The ﬁrst sam-
ples indices with probability proportional to their weight
using a random procedure (cid:126)a1 ∼ RESAMPLE( (cid:126)w1)6. We then
use a function (cid:126)c2, (cid:126)τ2, (cid:126)ρ2 = REINDEX((cid:126)a1, (cid:126)c1, (cid:126)τ1, (cid:126)ρ1) to repli-
cate objects according to the selected indices,

2 = cal
cl
1 ,

1

2 = τ al
τ l
1 ,

1

2 = ρal
ρl
1 .

1

(4)

5For simplicity we describe resampling with a single indexing
dimension. The Probabilistic Torch implementation supports ten-
sorized evaluation. For this reason, we specify a dimension along
which resampling is to be performed in Figure 2.

6We use systematic resampling in our implementation. For a

comparison of methods see (Murray et al., 2016)

Finally, we use (cid:126)w2 = MEAN( (cid:126)w1) to set outgoing weights to
the average of the incoming weights, wl

2 = (cid:80)

1 /L.

l(cid:48) wl(cid:48)

Denotational Semantics. To demonstrate that program
q are (strictly) properly weighted, we need to deﬁne the
density that programs p and q denote. Owing to space limita-
tions, we relegate discussion of these denotational semantics
to Appendix B. Deﬁnitions follow in a straightforward man-
ner from the denotational semantics of primitive programs.

Proper Weighting. With this formalism in place, we now
state our main claim of correctness for samplers that are
deﬁned in the inference language.

Theorem 1 (Strict Proper Weighting of Inference Programs).
Evaluation of an inference program q(c) is strictly properly
weighted for its unnormalized density

q(c)
(cid:75)

(cid:74)

γ.

We provide a proof in Appendix E, which is by induction
from lemmas for each combinator.

4 LEARNING NEURAL PROPOSALS

To learn a proposal q, we use properly-weighted samples
to compute variational objectives. We consider three objec-
tives for this purpose. The ﬁrst two minimize a top-level
reverse or forward KL divergence, which corresponds to per-
forming stochastic variational inference (Wingate, Weber,
2013) or reweighted wake-sleep style inference (Le et al.,
2019). The third implements nested variational inference
(Zimmermann et al., 2021) by deﬁning an objective at each
level of recursion. We describe the loss and gradient com-
putations at a high level, and provide details in Appendix D
and Appendix F respectively.

Objective computation. To optimize the parameters of
proposal programs, we slightly modify the operational se-
mantics (for details see Appendix D) such that a user-deﬁned
objective function (cid:96) : (ρq, ρp, w, v) → R can be evaluated
in the context of the propose combinator. Objective func-
tions are deﬁned in terms of the density maps of the proposal
and target program ρq, ρp and the incoming and incremental
importance weights w, v at the current level of nesting. The
local losses computed at the individual levels of nesting are
accumulated to a global loss in the inference state which
consecutively can be used to compute gradients w.r.t. pa-
rameters of the target and proposal and programs.

Stochastic Variational Inference (SVI). Suppose that
we have a program q2 = propose(p, q1) in which the tar-
get and proposal have parameters θ and φ,

(cid:74)

p(c0)
(cid:75)

γ = γp(· ; c0, θ),

q1(c0)
(cid:74)
The target program p and the inference program q2
denote the same density
γ and
(cid:75)
(cid:74)
hence, as a result of Theorem 2, the evaluation of q2 is

γ = γq(· ; c0, φ). (5)

q2(c0)

p(c0)

γ =

(cid:75)

(cid:75)

(cid:74)

strictly properly weighted for γp. Hence, we can evaluate
c2, τ2, ρ2, w2 (cid:59)q2(c0) to compute a stochastic lower bound
(Burda et al., 2016),

L(θ, φ) := Eq2(c0) [log w2]
(cid:16)
Eq2(c0) [w2]

≤ log

(cid:17)

= log Zp(c0, θ),

(6)

where the penultimate equality holds by Deﬁnition 1.
The gradient ∇θL of this bound is a biased estimate of
∇θ log Zp. The gradient ∇φL can be approximated using
likelihood-ratio estimators (Wingate, Weber, 2013; Ran-
ganath et al., 2014), reparameterized samples (Kingma,
Welling, 2013; Rezende et al., 2014), or a combination of
the two (Ritchie et al., 2016). In the special case where
q1 = f is a primitive program, the gradient of the reverse
KL-divergence between p and q1 is
(cid:20) ∂ log w2
∂ ˜τ2

∇φL(θ, φ) = Eq2(c0)

∂ ˜τ2
∂φ

log w2

∂
∂φ

+

(cid:21)

,

= −∇φKL(πf ||πp).

Reweighted Wake-sleep (RWS) Style Inference. To im-
plement variational methods inspired by reweighted wake-
(cid:59) q2(c0) to compute a
sleep, we use samples cl
2, ρl
self-normalized estimate of the gradient

2, wl
2

2, τ l

Figure 3: Samples from the initial proposal q1, learned in-
termediate proposal q4, ﬁnal proposal q8, and ﬁnal target.
Deﬁnitions can be found in Figure 5, Appendix G.1.

Figure 4: Qualitative results for the tracking task: Top row
shows inferred positions of objects, bottom row shows re-
construction of the video frames.

(cid:75)

(cid:74)

(cid:74)

q3

p3

p2

γ = γ3,

γ =
(cid:75)

= γ2, and
We denote
(cid:75)
= γ1. For simplicity, we consider densities with con-
f1
(cid:74)
stant support Ωp = Ω2 = Ω1. The top-level evaluation
c3, τ3, ρ3, w3 (cid:59) q3(c0) then yields a trace τ3 = τ1 that
= p1(· ; c0) with
contains samples from the prior
weight

f1(c0)

(cid:75)

(cid:74)

(cid:74)

(cid:75)

w3 =

γ3(τ1; c0, θ)
(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)
γ2(τ1; c0, φ2)

(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)
γ2(τ1; c0, φ2)
(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)
γ1(τ1; c0, φ1)

(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)
γ1(τ1; c0, φ1)
p1(τ1; c0, φ1)

.

(9)

(7)

∇θ log Zp(c0, θ) = Eπp(τ ;c0,θ) [∇θ log γp(τ ; c0, θ)] ,
wl
2
l(cid:48) wl(cid:48)
2

∇θ log γp(τ l

2; c0, θ).

(cid:88)

(cid:80)

(cid:39)

l

Notice that here we compute the gradient w.r.t.
the non-
extended density γp, which does not include auxiliary vari-
ables and hence density terms which would integrate to one.
When approximating gradient this allows us to compute
lower variance estimates.

Similarly, we approximate the gradient of the forward KL
divergence with a self-normalized estimator that is deﬁned
in terms of the proposals cl
1, τ l

(cid:59) q1(c0),

1, wl
1

1, ρl

−∇φKL(πp||πq) = Eπp(τ ;c0,θ)[∇φ log πq(τ ; c0, φ)] ,

(cid:88)

(cid:39)

l

(cid:18) wl
2
(cid:80)
l(cid:48) wl(cid:48)
2

−

wl
1
l(cid:48) wl(cid:48)
1

(cid:80)

(cid:19)

∇φ log γq(τ l

1; c0, φ).

(8)

In the special case where the proposal q1 = f is a primitive
program without observations (i.e. wl
1 = 1), we can drop
the second term to recovers the standard RWS estimator.

Nested Variational Inference. A limitation of both SVI
and RWS is that they are not well-suited to learning param-
eters in samplers at multiple levels of nesting. To see this,
let us consider a program

q3 = propose (p3 , propose (p2 , f1 ))

For this program, the lower bound from Equation 6 does not
depend on φ2. Conversely, the RWS-style estimator from
Equation 8 does not depend on φ1. Analogous problems
arise in non-trivial inference programs that incorporate tran-
sition kernels and resampling. One such example are SMC
samplers, where we would like to learn a sequence of inter-
mediate densities, which impacts the variance of the ﬁnal
sampler. We consider this scenario Section 5.

Nested Variational Inference (NVI) (Zimmermann et al.,
2021) replaces the top-level objectives in SVI and RWS
with an objective that contains one term at each level of
nesting. In the example above, this objective has the form

D = D3(π2 || π3) + D2(π1 || π2) + D1(p1 || π1),

where each Di is a forward or a reverse KL divergence
or a corresponding stochastic upper or lower bound. The
individual terms can be optimized as described above, but
additional care has to be taken when optimizing the interme-
diate target densities, as their normalizing constants might
not be tractable. For details on the computation of the nested
variational objective and corresponding gradients we refer
to Appendix F. When we apply an NVI objective using
an upper bound based on the forward KL divergence to the
program in Figure 2, we recover the gradient estimators of
APG samplers.

Inference on TrackingReconstructionTable 1: AVO and NVI variants trained for different numbers
of annealing steps K and samples per step L for a ﬁxed
sampling budget of K · L = 288 samples.

L

(cid:80)

log ˆZ = log (cid:0) 1

l wl(cid:1)
K=2 K=4 K=6 K=8
1.88 1.99 2.05 2.06
AVO
NVI
1.88 1.99 2.06 2.07
NVIR 1.88 2.05 2.07 2.08
1.88 2.03 2.08 2.08
NVI*
NVIR* 1.88 1.99 2.08 2.08

ESS

K=2 K=4 K=6 K=8
295
426
319
427
961
418
481
427
965
418

291
341
828
304
981

285
308
934
414
978

Table 2: log pθ(x, z) on test sets that contain D objects and
T time steps. L is the number of particles and K is the num-
ber of sweeps. We run APG and RWS with computational
budget L·K = 200, and run HMC-RWS with L·K = 4000.

Model

Budget

T=10 T=20 T=10 T=20

D=3

D=4

L=200, K=1

RWS
-5247 -9396 -8275 -16070
HMC-RWS L=200, K=20 -5137 -9281 -8124 -15087
-8966
APG
-6879
APG
-6827
APG

-2849 -5008 -4411
L=100, K=2
L=40, K=5
-2300 -4646 -3529
L=20, K=10 -2267 -4606 -3516

5 EXPERIMENTS

We evaluate combinators in two experiments. First, we learn
proposals in an annealed importance sampler that addition-
ally learns intermediate densities. Second, we use an APG
sampler to learn proposals and a generative model for an
unsupervised multi-object tracking task.

Annealed Variational Inference. We consider the task
of generating samples from a 2-dimensional unnormalized
density consisting of 8 modes, equidistantly placed along a
circle. For purposes of evaluation we treat this density as a
blackbox, which we are only able to evaluate pointwise.

We implement an annealed importance sampler (Neal, 2001)
(Figure 5 in Appendix G.1) for a sequence of unnormalized
densities γk(τk; c0) = γK(τk; c0)βk γ1(τk; c0)1−βk that in-
terpolate between an initial proposal γ1 and the ﬁnal target
γK. The sampler employs forward kernels qk(τk; ck−1, φk)
and reverse kernels rk(τk−1; ck, θk) to deﬁne densities on
an extended space at each level of nesting. We train the
sampler using NVI by optimizing the kernel parameters θk
and φk, and parameters for the annealing schedule βk.

We compare NVI and NVI*, which additionally learns the
annealing schedule for the intermediate targets, and cor-
responding versions, NVIR and NVIR*, which additional
employ resampling at every level of nesting, to annealed
variational objectives (AVO) (Huang et al., 2018). Learning
the annealing schedule (NVI(R)*) results in improved sam-
ple quality, in terms of the log expected weight (log ˆZ) and
the effective sample size (ESS). We report results in Table 1
and refer to Appendix G.1 for additional details.

Amortized Population Gibbs.
In this task, the data is
a corpus of simulated videos that each contain multiple
moving objects. Our goal is to learn both the target program
(i.e. the generative model) and the inference program using
the APG sampler that we introduced in Section 3.1. See
Appendix G.2 for implementation details.

Figure 4 shows that the APG sampler can (fully unsuper-
vised) identify, track, and reconstruct individual object in
each frame. In Table 2 we compare the APG sampler against
an RWS baseline and a hand-coded HMC-RWS method,

which improves upon RWS proposals using Hamiltonian
Monte Carlo. Table 2 shows that APG outperforms both
baselines. Moreover, for a ﬁxed computational budget, in-
creasing the number of sweeps improves sample quality.

6 RELATED WORK

This paper builds directly on several lines of work, which
we discuss in detail in Appendix A.

Traced evaluation (Section 2) has a long history in sys-
tems that extend general-purpose programming languages
with functionality for probabilistic modeling and inference
(Wingate et al., 2011; Mansinghka et al., 2014; Goodman,
Stuhlmüller, 2014; Tolpin et al., 2016), including recent
work that combines probabilistic programming and deep
learning (Tran et al., 2016; Ritchie et al., 2016; Siddharth
et al., 2017; Bingham et al., 2018; Baydin et al., 2018).

The idea of developing abstractions for inference program-
ming has been around for some time (Mansinghka et al.,
2014), and several instantiations of such abstractions have
been proposed in recent years (Cusumano-Towner et al.,
2019; ´Scibior et al., 2017; Obermeyer et al., 2019). The
combinator-based language that we propose here is inspired
directly by the work of Naesseth et al. (2019) on nested
importance sampling, as well as on a body of work that
connects importance sampling and variational inference.

7 DISCUSSION

We have developed a combinator-based language for im-
portance samplers that are valid by construction, in the
sense that samples are properly weighted for the density
that a program denotes. We deﬁne semantics for these
combinators and provide a reference implementation in
Probabilistic Torch. Our experiments demonstrate that user-
programmable samplers can be used as a basis for sophisti-
cated variational methods that learn neural proposals and/or
deep generative models. Inference combinators, which can
be implemented as a DSL that extends a range of existing
systems, hereby open up opportunities to develop novel
nested variational methods for probabilistic programs.

ACKNOWLEDGEMENTS

This work was supported by the Intel Corporation, the
3M Corporation, NSF award 1835309, startup funds from
Northeastern University, the Air Force Research Laboratory
(AFRL), and DARPA. We would like to thank Adam ´Scibior
for helpful discussions regarding the combinator semantics.

REFERENCES

Baydin, Atılım Güne¸s, Le, Tuan Anh, Heinrich, Lukas,
Gram-Hansen, Bradley, Schroeder de Witt, Christian,
Bhimji, Wahid, Cranmer, Kyle, Wood, Frank. Pyprob/Ppx.
pyprob. 2018.

Baydin, Atılım Güne¸s, Shao, Lei, Bhimji, Wahid, Hein-
rich, Lukas, Meadows, Lawrence F., Liu, Jialin, Munk,
Andreas, Naderiparizi, Saeid, Gram-Hansen, Bradley,
Louppe, Gilles, Ma, Mingfei, Zhao, Xiaohui, Torr, Philip,
Lee, Victor, Cranmer, Kyle, Prabhat, Wood, Frank. “Eta-
lumis: Bringing Probabilistic Programming to Scientiﬁc
Simulators at Scale.” Proceedings of the International
Conference for High Performance Computing, Network-
ing, Storage, and Analysis (SC19), November 17–22,
2019. 2019.

Bingham, Eli, Chen, Jonathan P., Jankowiak, Martin, Ober-
meyer, Fritz, Pradhan, Neeraj, Karaletsos, Theofanis,
Singh, Rohit, Szerlip, Paul, Horsfall, Paul, Goodman,
Noah D. “Pyro: Deep Universal Probabilistic Program-
ming.” arXiv:1810.09538 [cs, stat] (Oct. 2018). arXiv:
1810.09538 [cs, stat].

Burda, Yuri, Grosse, Roger, Salakhutdinov, Ruslan. “Impor-
tance Weighted Autoencoders.” International Conference
on Representations. 2016. arXiv: 1509.00519.

Cusumano-Towner, Marco F., Saad, Feras A., Lew, Alexan-
der K., Mansinghka, Vikash K. “Gen: A General-Purpose
Probabilistic Programming System with Programmable
Inference.” Proceedings of the 40th ACM SIGPLAN Con-
ference on Programming Language Design and Imple-
mentation. PLDI 2019. New York, NY, USA: Association
for Computing Machinery, June 2019, pp. 221–236. ISBN:
978-1-4503-6712-7. DOI: 10.1145/3314221.3314642.

Esmaeili, Babak, Huang, Hongyi, Wallace, Byron, van de
Meent, Jan-Willem. “Structured neural topic models for
reviews.” The 22nd International Conference on Artiﬁcial
Intelligence and Statistics. PMLR. 2019, pp. 3429–3439.

Ge, Hong, Xu, Kai, Ghahramani, Zoubin. “Turing: A Lan-
guage for Flexible Probabilistic Inference.” Proceedings
of the Twenty-First International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS), Amos Storkey

and Fernando Perez-Cruz (Eds.) Vol. 84. 2018, pp. 1682–
1690.

Goodman, Noah, Mansinghka, Vikash, Roy, Daniel M,
Bonawitz, Keith, Tenenbaum, Joshua B. “Church: A Lan-
guage for Generative Models.” Proc. 24th Conf. Uncer-
tainty in Artiﬁcial Intelligence (UAI). 2008, pp. 220–229.

Goodman, Noah D, Stuhlmüller, Andreas. The Design
and Implementation of Probabilistic Programming Lan-
guages. 2014.

Greff, Klaus, Kaufman, Raphaël Lopez, Kabra, Rishabh,
Watters, Nick, Burgess, Christopher, Zoran, Daniel,
Matthey, Loic, Botvinick, Matthew, Lerchner, Alexan-
der. “Multi-object representation learning with iterative
variational inference.” International Conference on Ma-
chine Learning. PMLR. 2019, pp. 2424–2433.

Huang, Chin-Wei, Tan, Shawn, Lacoste, Alexandre,
Courville, Aaron C. “Improving Explorability in Vari-
ational Inference with Annealed Variational Objectives.”
Advances in Neural Information Processing Systems 31.
Ed. by S. Bengio, H. Wallach, H. Larochelle, K. Grau-
man, N. Cesa-Bianchi, R. Garnett. Curran Associates,
Inc., 2018, pp. 9701–9711.

Kingma, Diederik P., Welling, Max. “Auto-Encoding Vari-
ational Bayes.” International Conference on Learning
Representations (2013).

Kusner, Matt J, Paige, Brooks, Hernández-Lobato, José
Miguel. “Grammar variational autoencoder.” Interna-
tional Conference on Machine Learning. PMLR. 2017,
pp. 1945–1954.

Le, Tuan Anh, Kosiorek, Adam R., Siddharth, N., Teh, Yee
Whye, Wood, Frank. “Revisiting Reweighted Wake-Sleep
for Models with Stochastic Control Flow.” Uncertainty
in Artiﬁcial Intelligence. Le and Kosiorek contributed
equally. 2019.

Liu, Jun S. Monte Carlo strategies in scientiﬁc computing.

Springer Science & Business Media, 2008.

Mansinghka, Vikash, Selsam, Daniel, Perov, Yura. “Venture:
A Higher-Order Probabilistic Programming Platform with
Programmable Inference.” arXiv (Mar. 2014), pp. 78–78.

Murray, Lawrence M, Lee, Anthony, Jacob, Pierre E. “Par-
allel resampling in the particle ﬁlter.” Journal of Compu-
tational and Graphical Statistics 25.3 (2016), pp. 789–
805.

Naesseth, Christian, Linderman, Scott, Ranganath, Rajesh,
Blei, David. “Variational Sequential Monte Carlo.” en.
International Conference on Artiﬁcial Intelligence and
Statistics. PMLR, Mar. 2018, pp. 968–977.

Naesseth, Christian, Lindsten, Fredrik, Schon, Thomas.
“Nested Sequential Monte Carlo Methods.” International
Conference on Machine Learning. 2015, pp. 1292–1301.

Naesseth, Christian A., Lindsten, Fredrik, Schön, Thomas B.
“Elements of Sequential Monte Carlo.” arXiv:1903.04797
[cs, stat] (Mar. 2019). arXiv: 1903.04797. (Visited on
12/16/2019).

Neal, Radford M. “Annealed Importance Sampling.” en.
Statistics and Computing 11.2 (Apr. 2001), pp. 125–139.
ISSN: 1573-1375. DOI: 10.1023/A:1008923215028.

Obermeyer, Fritz, Bingham, Eli, Jankowiak, Martin, Phan,
Du, Chen, Jonathan P. “Functional Tensors for Proba-
bilistic Programming.” arXiv preprint arXiv:1910.10775
(2019).

Ranganath, Rajesh, Gerrish, Sean, Blei, David. “Black
Box Variational Inference.” en. Artiﬁcial Intelligence and
Statistics. Apr. 2014, pp. 814–822.

Rezende, Danilo Jimenez, Mohamed, Shakir, Wierstra,
Daan. “Stochastic Backpropagation and Approximate In-
ference in Deep Generative Models.” Proceedings of the
31st International Conference on Machine Learning. Ed.
by Eric P. Xing, Tony Jebara. Vol. 32. Proceedings of Ma-
chine Learning Research 2. Bejing, China: PMLR, June
2014, pp. 1278–1286.

Ritchie, Daniel, Horsfall, Paul, Goodman, Noah D. “Deep
Amortized Inference for Probabilistic Programs.” en.
arXiv:1610.05735 [cs, stat] (Oct. 2016). arXiv: 1610 .
05735 [cs, stat].

´Scibior, Adam, Kammar, Ohad, Ghahramani, Zoubin.
“Functional Programming for Modular Bayesian Infer-
ence.” Proc. ACM Program. Lang. 2.ICFP (July 2018),
83:1–83:29. ISSN: 2475-1421. DOI: 10 . 1145 / 3236778.
URL: http://doi.acm.org/10.1145/3236778.

´Scibior, Adam, Kammar, Ohad, Vákár, Matthijs, Staton,
Sam, Yang, Hongseok, Cai, Yufei, Ostermann, Klaus,
Moss, Sean K., Heunen, Chris, Ghahramani, Zoubin.
“Denotational Validation of Higher-Order Bayesian Infer-
ence.” Proc. ACM Program. Lang. 2.POPL (Dec. 2017),
60:1–60:29. ISSN: 2475-1421. DOI: 10.1145/3158148.

Siddharth, N., Paige, Brooks, van de Meent, Jan-Willem,
Desmaison, Alban, Goodman, Noah D., Kohli, Pushmeet,

Wood, Frank, Torr, Philip. “Learning Disentangled Repre-
sentations with Semi-Supervised Deep Generative Mod-
els.” Advances in Neural Information Processing Sys-
tems 30. Ed. by I. Guyon, U. V. Luxburg, S. Bengio, H.
Wallach, R. Fergus, S. Vishwanathan, R. Garnett. 2017,
pp. 5927–5937.

Tolpin, David, van de Meent, Jan-Willem, Yang, Hongseok,
Wood, Frank. “Design and Implementation of Probabilis-
tic Programming Language Anglican.” Proceedings of
the 28th Symposium on the Implementation and Applica-
tion of Functional Programming Languages. IFL 2016.
Leuven, Belgium: ACM, 2016, 6:1–6:12. ISBN: 978-1-
4503-4767-9. DOI: 10/ghxhzn.

Tran, Dustin, Kucukelbir, Alp, Dieng, Adji B., Rudolph,
Maja, Liang, Dawen, Blei, David M. “Edward: A Li-
brary for Probabilistic Modeling, Inference, and Criti-
cism.” arXiv:1610.09787 [cs, stat] (Oct. 2016). arXiv:
1610.09787 [cs, stat].

van de Meent, Jan-Willem, Paige, Brooks, Tolpin, David,
Wood, Frank. “Black-Box Policy Search with Probabilis-
tic Programs.” 2016, 1195–1204.

van de Meent, Jan-Willem, Paige, Brooks, Yang, Hongseok,
Wood, Frank. “An Introduction to Probabilistic Program-
ming.” arXiv:1809.10756 [cs, stat] (Sept. 2018). arXiv:
1809.10756 [cs, stat].

Wingate, David, Stuhlmueller, Andreas, Goodman, Noah D.
“Lightweight Implementations of Probabilistic Program-
ming Languages via Transformational Compilation.” Pro-
ceedings of the 14th International Conference on Artiﬁ-
cial Intelligence and Statistics. 2011, 770_778–770_778.

Wingate, David, Weber, Theo. “Automated Variational In-
ference in Probabilistic Programming.” arXiv preprint
arXiv:1301.1299 (2013), pp. 1–7. arXiv: 1301.1299.

Wood, Frank, van de Meent, Jan-Willem, Mansinghka,
Vikash. “A New Approach to Probabilistic Programming
Inference.” Artiﬁcial Intelligence and Statistics. 2014,
pp. 1024–1032.

Wu, Hao, Zimmermann, Heiko, Sennesh, Eli, Le, Tuan
Anh, van de Meent, Jan-Willem. “Amortized Population
Gibbs Samplers with Neural Sufﬁcient Statistics.” Inter-
national Conference on Machine Learning. PMLR. 2020,
pp. 10421–10431.

Zimmermann, Heiko, Wu, Hao, Esmaeili, Babak, Stites,
Sam, van de Meent, Jan-Willem. “Nested Variational In-
ference.” 3rd Symposium on Advances in Approximate
Bayesian Inference (2021).

A RELATED WORK

A.1 IMPORTANCE SAMPLING AND MCMC IN VARIATIONAL INFERENCE

This work ﬁts into a line of recent methods for deep generative modeling that seek to improve inference quality in stochastic
variational methods. We brieﬂy review related literature on standard methods before discussing advanced methods that
combine variational inference with importance sampling and MCMC, which inspire this work.

Variants of SVI maximize a lower bound, which equates to minimizing a reverse KL divergence, whereas methods that
derive from RWS minimize an upper bound, which equates to minimizing a forward KL divergence. In the case of SVI,
gradients can be approximated using reparameterization, which is widely used in variational autoencoders (Kingma, Welling,
2013; Rezende et al., 2014) or by using likelihood-ratio estimators (Wingate, Weber, 2013; Ranganath et al., 2014). In the
more general case, where the model contains a combination of reparameterized and non-reparameterized variables, the
gradient computation can be formalized in terms of stochastic computation graphs (Schulman et al., 2015). Work by Ritchie
et al. (2016) explains how to operationalize this computation in probabilistic programming systems. In the case of RWS,
gradients can be computed using simple self-normalized estimators, which do not require reparameterization (Bornschein,
Bengio, 2015). This idea was recently revisited in the context of probabilistic programming systems by (Le et al., 2019),
who demonstrate that RWS-based methods are often competitive with SVI.

Importance-weighted Variational Inference. There is a large body of work that improves upon standard SVI by deﬁning
tighter bounds, which results in better gradient estimates for the generative model. Many of these approaches derive from
importance-weighted autoencoders (IWAEs) (Burda et al., 2016), which use importance sampling to deﬁne a stochastic lower
bound E[log ˆZ] ≤ log Z based on an unbiased estimate E[ ˆZ] = Z (see Section G.1). Since any strictly properly weighted
importance sampler can be used to deﬁne an unbiased estimator ˆZ = 1
l wl, this gives rise to many possible extensions,
L
including methods based on SMC (Le et al., 2018; Naesseth et al., 2018; Maddison et al., 2017) and thermodynamic
integration (Masrani et al., 2019). Salimans et al. (2015) derive a stochastic lower bound for variational inference which
uses an importance weight deﬁned in terms of forward and reverse kernels in MCMC steps. Caterini et al. (2018) extend this
work by optimally selecting reverse kernels (rather than learning them) using inhomogeneous Hamiltonian dynamics. The
work on AVO (Huang et al., 2018) learn a sequence of transition kernels that performs annealing from the initial encoder to
the posterior, which we use as a baseline in our annealing experiments.

(cid:80)

A somewhat counter-intuitive property of these estimators is tightening the bound typically improves the quality of gradient
estimates for the generative model, but can adversely affect the signal-to-noise ratio of gradient estimates for the inference
model (Rainforth et al., 2018). These issues can be circumvented by using RWS-style estimators7, or by implementing
doubly-reparameterized estimators (Tucker et al., 2019).

SMC Samplers and MCMC.
In addition to enabling implementation of variational methods based on SMC, this work
enables the interleaving of resampling and move operations to deﬁne so-called SMC samplers (Chopin, 2002). One recent
example of are the APG samplers that we consider in our experiments (Wu et al., 2020). It is also possible to interleave
importance sampling with any MCMC operator, which preserves proper weighting as long as the stationary distribution of
this operator is the target density. In this space, there exists a large body of relevant work.

Hoffman (2017) apply Hamiltonian Monte Carlo to samples that are generated from the encoder, which serves to improve
the gradient estimate w.r.t. the generative model, while learning the inference network using a standard reparameterized
lower bound objective. Li et al. (2017) also use MCMC to improve the quality of samples from an encoder, but additionally
use these samples to train the encoder by minimizing the forward KL divergence relative to the ﬁltering distribution of the
Markov chain. Since the ﬁltering distribution after multiple MCMC steps is intractable, Li et al. (2017) use an adversarial
objective to minimize the forward KL. Wang et al. (2018) develop a meta-learning approach to learn Gibbs block conditionals.
This work assumes a setup in which it is possible to sample data and latent variables from the true generative model. This
approach minimizes the forward KL, but uses the learned conditionals to deﬁne an (approximate) MCMC sampler, rather
than using them as proposals in a SMC sampler.

Proper weighting and nested variational inference. This work directly builds on seminal work by Liu (2008); Naes-
seth et al. (2015); Naesseth et al. (2019) that formalizes proper weighting. This formalism makes it possible to reason
compositionally about validity of importance samplers and corresponding variational objectives (Zimmermann et al., 2021).

7Note that the gradient estimate for the generative model in Equation 7 is identical to the gradient estimate of the corresponding IWAE

bound, so these two approaches only differ in the gradient estimate that they compute for the inference model.

A.2 PROBABILISTIC PROGRAMMING

Probabilistic programming systems implement methods for inference in programmatically speciﬁed models. A wide variety
of systems exist, which differ in the base languages they employ, the types of inference methods they provide, and their
intended use cases. One widely used approach to the design of probabilistic programming systems is to deﬁne a language in
which all programs are amenable to a particular style of inference. Exemplars of this approach include Stan (Carpenter et al.,
2017), which emphasizes inference using Hamiltonian Monte Carlo methods in differentiable models with statically-typed
support, Infer.NET (Minka et al., 2010) which emphasizes message passing in programs that denote factor graphs, Problog
(De Raedt et al., 2007) and Dice (Holtzen et al., 2020), in which programs denote binary decision diagrams, and LibBi
(Murray, 2013), which emphasizes particle-based methods for state space models. More generally, systems that emphasize
MCMC methods in programs with statically typed support ﬁt this mold, including early systems like BUGS (Spiegelhalter
et al., 1995) and JAGS (Plummer, 2003), as well as more recent systems like PyMC3 (Salvatier et al., 2016).

A second widely used approach to probabilistic programming is to extend a general-purpose language with functionality
for probabilistic modeling, and implement inference methods that are generally applicable to programs in this language.
The advantage of this design is that it becomes more straightforward to develop simulation-based models that incorporate
complex deterministic functions, or to design programs that incorporate recursion and control ﬂow. A well-known early
exemplar of this style of probabilistic programming is Church (Goodman et al., 2008), whose modeling language is based
on Scheme. Since then, many existing languages have been adapted to probabilistic programming, including Lisp variants
(Mansinghka et al., 2014; Wood et al., 2014), Javascript (Goodman, Stuhlmüller, 2014), C (Paige, Wood, 2014), Scala
(Pfeffer, 2009), Go (Tolpin, 2018), and Julia (Ge et al., 2018; Cusumano-Towner et al., 2019).

This second class of probabilistic programming systems is the most directly relevant to the work that we present here. A key
technical consideration in the design of probabilistic programming systems is whether the modeling language is ﬁrst-order
or higher-order (sometimes also referred to as “universal”) (van de Meent et al., 2018). In ﬁrst-order languages, a program
denotes a density in which the support is statically determinable at compile time. Since most general-purpose languages
support higher-order functions and recursion, the support of probabilistic programs in these languages is generally not
statically determinable. However, the traced evaluation model that we describe in Section 2.2 can be implemented in almost
any language, and inference combinators can therefore be implemented as a DSL for inference in most of these systems.

In the context of our work, we are particularly interested in systems that extend or interoperate with deep learning
systems, including Pyro (Bingham et al., 2018), Edward2 (Tran et al., 2018), Probabilistic Torch (Siddharth et al., 2017),
Scruff (Pfeffer, Lynn, 2018), Gen (Cusumano-Towner et al., 2019) and PyProb (Baydin et al., 2019). These systems provide
ﬁrst-class support for inference with stochastic gradient methods. While support for stochastic variational inference in
probabilistic programming has been around for some time (Wingate, Weber, 2013; van de Meent et al., 2016b; Ritchie et al.,
2016), this style of inference has become much more viable in systems where variational distributions can be parameterized
using neural networks. However, to date, the methods that are implemented in these systems are typically limited to standard
SVI, RWS, and IWAE objectives. We are not aware of systems that currently support stochastic variational methods that
incorporate importance resampling, such as autoencoding SMC or APG samplers.

A.3 INFERENCE PROGRAMMING

The combinator-based language for inference programming that we develop in this paper builds on ideas that have been
under development in the probabilistic programming community for some time.

The Venture paper described “inference programming” as one of the desiderata for functionality of future systems (Mans-
inghka et al., 2014). Venture also implemented a stochastic procedure interface for manipulating traces and trace fragments,
which can be understood as a low-level programming interface for inference. WebPPL (Goodman, Stuhlmüller, 2014) and
Anglican (Tolpin et al., 2016) deﬁne an interface for inference implementations that is based on continuation-passing-style
(CPS) transformations, in which a black-box deterministic computation returns continuations to an inference backend, which
implements inference computations and continues execution (van de Meent et al., 2016a). A number of more recent systems
have implemented similar interfaces, albeit by different mechanisms than CPS transformations. Turing (Ge et al., 2018) uses
co-routines in Julia as a mechanism for interruptible computations. Pyro (Bingham et al., 2018) and Edward2 (Tran et al.,
2018) implement an interface in which requests to the inference backend are dispatched using composable functions that are
known as “messengers” or “tracers”. The arguably most general instantiation of this idea is found in PyProb (Baydin et al.,
2019), which employs a cross-platform Probabilistic Programming eXecution (PPX) protocol based on ﬂatbuffers to enable
computation between a program and inference backend that can be implemented in different languages (Baydin et al., 2018).
For a pedagogical discussion, see the introduction by van de Meent et al. (2018).

All of the above programming interfaces for inference are low-level, which is to say that it is the responsibility of the
developer to unsure that quantities like importance weights and acceptance probabilities are computed in a manner that
results in a correct inference algorithm. This means that users of probabilistic programming systems can in principle create
their own inference algorithms, but that doing so may require considerable expertise and debugging. For this reason, recent
systems have sought to develop higher-level interfaces for inference programming. Edward (Tran et al., 2016) provides
a degree of support for interleaving inference operations targeting different conditionals. Birch (Murray, Schön, 2018)
provides constructs for structural motifs, such as state space models, which are amenable to inference optimizations.

Exemplars of systems that more explicitly seek to enable inference programming include Gen (Cusumano-Towner et al.,
2019) and Functional Tensors (Obermeyer et al., 2019). Gen provides support for writing user-level code to interleave
operations such as MCMC updates or MAP estimation, which can be applied to subsets of variables. For this purpose, Gen
provides a generative function interface consisting of primitives generate, propose, assess, update, and choice_gradients.
Recursive calls to these primitives at generative function call sites serve to compositionally implement Gen’s built-in
modeling language, along with hierarchical traces. Lew et al. (2020) considered a type system for execution traces in
probabilistic programs, which allowed them to verify the correctness of certain inference algorithms at compile time.

Functional Tensors (funsors) provide an abstraction for integration that aims to unify exact and approximate inference.
Funsors providing a grammar and type system which take inspiration from modern autodifferentiation libraries. Expres-
sions in funsors denote discrete factors, Gaussian factors, point mass, delayed values, function application, substitution,
marginalization, and plated products. Types encapsulate tensor dimensions, which permit funsors to support broadcasting.
This deﬁnes an intermediate representation that can be used for a variety of probabilistic programs and inference methods.

The inference language that we develop here differs from these above approaches in that is designed to ensure that any
composition of combinators yields an importance sampler that is valid, in the sense that evaluation is properly-weighted
for the unnormalized density that a program denotes. In doing so, our work takes inspiration from Hakaru (Narayanan
et al., 2016), which frames inference as program transformations that be composed so as to preserve a measure-theoretic
denotation (Zinkov, Shan, 2017), as well as the work on validity of inference in higher-order probabilistic programs by
´Scibior et al. (2017), which we discuss in the next Section.

A.4 SAMPLING AND MEASURE SEMANTICS

Programming languages have been studied in terms of two kinds of semantics: what the programs do, operational semantics,
and what they mean, denotational semantics. For probabilistic programming languages, this has usually led to a denotational
measure semantics that interpret generative model programs as measures, and an operational sampler semantics that
interpret programs as procedures for using randomness to sample from a speciﬁed distribution. In the terms we use for our
combinators library, model composition changes the target density of a program, and thus its measure semantics, while
inference programming should alter the sampling semantics in a way that preserves the measure semantics.

Early work by Borgström et al. (2011); Toronto et al. (2015) characterized measure semantics for ﬁrst-order probabilistic
programs, with the ﬁrst enabling inference by compilation to a factor graph, and the second via semi-computable preimage
functions. This demonstrates one of the simplest, but most analytically difﬁcult, ways to perform inference in a probabilistic
program: get rid of the sampling semantics and use the measure semantics to directly evaluate the relevant posterior
expectation. When made possible by program analysis, this can even take the form of symbolic disintegration of the joint
distribution entailed by a generative program into observations and a posterior distribution, as in Shan, Ramsey (2017).

Fong (2013) gave categorical semantics to causal Bayesian networks, and via the usual compilation to a graphical model
construction, to ﬁrst-order probabilistic programs as well. Further work by Clerc et al. (2017); Dahlqvist et al. (2018) has
extended the consideration of categorical semantics for ﬁrst-order PPLs.

Quasi-Borel spaces were discovered by Heunen et al. (2017) and quickly found to provide a good model for higher-order
probabilistic programming. ´Scibior et al. (2017); ´Scibior et al. (2018) applied these categorical semantics to prove that
inference algorithms could be speciﬁed by monad transformers on sampling strategies that would preserve the categorical
measure semantics over the underlying generative model. The work of ´Scibior et al. (2018) included an explicit consideration
of importance weighting, which we have extended to cover the proper weighting of importance samples from arbitrarily
nested and extended inference programs.

B DENOTATIONAL SEMANTICS OF TARGET AND INFERENCE PROGRAMS

To reason about validity of inference, we need to specify what density a target program p or inference program q denotes. We
begin with target programs, which have the grammar p ::= f | extend(p, f). For a primitive program f, the denotational
semantics inherit trivially from the axiomatic denotational semantics of the modeling language. For a program extend(p, f)
we deﬁne the density as the composition of densities of the inputs

c1, τ1, ρ1, w1 (cid:59) p(c0)

c2, τ2, ρ2, w2 (cid:59) f(c1)

dom(ρ1) ∩ dom(ρ2) = ∅

dom(ρ2) = dom(τ2)

τ3 = τ2 ⊕ τ1

(cid:75)
In this rule, we omit subscripts
p, since this rule applies to both prior and the unnormalized density. As in the case
of primitive programs, we here adopt a convention in which the support is implicitly deﬁned as the set of traces τ3 = τ2 ⊕ τ1
by combining disjoint traces τ1 and τ2 that can be generated by evaluating the composition of p and f.

(cid:74)
γ and

·
(cid:75)

·
(cid:75)

(cid:74)

(cid:74)

(cid:74)

extend(p, f)(c0)
(cid:75)

(τ3) =

f(c1)
(cid:75)
(cid:74)

(τ2)

p(c0)

(τ1)

Inference programs have a grammar q ::= p | compose(q(cid:48), q) | resample(q) | propose(p, q). For each expression form,
we deﬁne the density that a program denotes in terms of the density of its corresponding target program. To do so, we deﬁne
a program tranformation target(q). This transformation replaces all sub-expressions of the form propose(p, q) with their
targets p and all sub-expressions resample(q) with q. We deﬁne the transformation recursively

p = target(p)

p = target(propose(p,q))

q(cid:48) = target(q)
q(cid:48) = target(resample(q))

q(cid:48)
1 = target(q1)
2,q(cid:48)

compose(q(cid:48)

q(cid:48)
2 = target(q2)

1) = target(compose(q2,q1))

Since this transformation removes all instances of propose and resample forms, transformed programs q(cid:48) =target(q) deﬁne
a simpliﬁed grammar q(cid:48) ::= p | compose(q(cid:48)

2). We now deﬁne the denotational semantics for inference programs as

1, q(cid:48)

The denotational semantics for a transformed program are trivially inherited from those for target programs when q(cid:48) =p,
whereas the denotational semantics for a composition compose(q(cid:48)
1) are analogous to those of an extension extend(p, f)

q(cid:48) = target(q)
q(cid:48)(c)
(cid:75)

q(c)

=

(cid:75)

(cid:74)

(cid:74)

2, q(cid:48)
c2, τ2, ρ2, w2 (cid:59) q(cid:48)
τ3 = τ1 ⊕ τ2

2(c1)

c1, τ1, ρ1, w1 (cid:59) q(cid:48)

1(c0)

dom(ρ1) ∩ dom(ρ2) = ∅
q(cid:48)
2(c1)
(cid:74)

1)(c0)

(τ3) =

2, q(cid:48)

(cid:75)

compose(q(cid:48)

(cid:74)

(τ2)
(cid:75)

q(cid:48)
1(c0)
(cid:74)

(τ1)
(cid:75)

C EVALUATION UNDER SUBSTITUTION

To use an extended program as a target for a proposal, we need to deﬁne its evaluation under substitution. We deﬁne the
operational semantics of this evaluation in a manner that is analogous to the unconditioned case

c1, τ1, ρ1, w1 (cid:59) p(c0)[τ0]

dom(ρ1) ∩ dom(ρ2) = ∅

c2, τ2, ρ2, w2 (cid:59) f(c1)[τ0]
dom(ρ2) = dom(τ2)

c1, τ1 ⊕ τ2, ρ1 ⊕ ρ2, w1 · w2 (cid:59) extend(p, f)(c0)[τ0]
This rule, like other rules, deﬁnes a recursion. In this case, the recursion ensures that we can perform conditioned evaluation
for any target program p.

We deﬁne evaluation under substitution of an inference program q by performing an evaluation under substitution for the
corresponding target program

c, τ, ρ, w (cid:59) target(q)(c0)[τ0]
c, τ, ρ, w (cid:59) q(c0)[τ0]

As in the previous section, the transformed programs deﬁne a grammar q(cid:48) ::= p | compose(q(cid:48)
evaluation under substitution is deﬁned as above. Evaluation under substitution of a program compose(q(cid:48)
deﬁned by recursively evaluating inputs under substitution, as with the extend combinator

1, q(cid:48)

2). In the base case q(cid:48) = p,
2) is once again

1, q(cid:48)

c1, τ1, ρ1, w1 (cid:59) q(cid:48)

1(c0)[τ0]

2(c1)[τ0]
c1, τ1 ⊕ τ2, ρ1 ⊕ ρ2, w1 · w2 (cid:59) compose(q(cid:48)

c2, τ2, ρ2, w2 (cid:59) q(cid:48)

2, q(cid:48)

1)(c0)[τ0]

dom(ρ1) ∩ dom(ρ2) = ∅

(cid:104)L1, (c1, τ1, ρ1, w1)(cid:105)

(cid:104)L0, q1(c0)(cid:105)

(cid:104)L2, (c2, τ2, ρ2, w2)(cid:105)

(cid:104)L1, q2(c1)(cid:105)

dom(ρ1) ∩ dom(ρ2) = ∅

(cid:104)L1, (c1, τ1, ρ1, w1)(cid:105)

(cid:104)L0, p(c0)(cid:105)
dom(ρ1) ∩ dom(ρ2) = ∅

(cid:104)L2, (c2, τ2, ρ2, w2)(cid:105)

(cid:104)L1, f(c1)(cid:105)

dom(ρ2) = dom(τ2)

(cid:104)L2, (c2, τ2 ⊕ τ1, ρ2 ⊕ ρ1, w2 · w1)(cid:105)

(cid:104)L0, compose(q2, q1)(c0)(cid:105)

(cid:104)L2, (c2, τ1 ⊕ τ2, ρ1 ⊕ ρ2, w1 · w2)(cid:105)

(cid:104)L0, extend(p, f)(c0)(cid:105)

(cid:104)L1, ((cid:126)c1, (cid:126)τ1, (cid:126)ρ1, (cid:126)w1)(cid:105)

(cid:104)L0, q((cid:126)c0)(cid:105) (cid:126)a1 ∼ RESAMPLE( (cid:126)w1)

(cid:126)c2, (cid:126)τ2, (cid:126)ρ2 = REINDEX((cid:126)a1, (cid:126)c1, (cid:126)τ1, (cid:126)ρ1)

(cid:126)w2 = MEAN( (cid:126)w1)

(cid:104)L1, ((cid:126)c2, (cid:126)τ2, (cid:126)ρ2, (cid:126)w2)(cid:105)

(cid:104)L0, resample(q)((cid:126)c0)(cid:105)

Table 3: Operational semantics for evaluating compose, extend, and resample combinators in the context of a loss function (cid:96)
and accumulated loss L.

Note that the operational semantics do not rely on evaluation under substitution of inference programs q, since conditional
evaluation is only every performed for target programs. However, the proofs in Section E do make use of the deﬁnition of
the prior under substitution, which is identical to the deﬁnition for primitive programs

c1, τ1, ρ1, w1 (cid:59) q(c0)[τ0]

p(c0)[τ0]
(cid:74)

p(τ1) = pq[τ0](τ1; c0) =
(cid:75)

(cid:89)

ρ1(α)

α∈dom(τ1)\dom(τ0)

D EVALUATION IN CONTEXT

To accumulating loss in the combinators framework, we reframe our operational semantics so that they are evaluated in a
context of a user-deﬁned objective function (cid:96) : (ρq, ρp, w, v) → R and an accumulated loss L which is initialized to 0 ∈ R.
To denote an expression · evaluated in the context of L and (cid:96), we describe the notation

(cid:104)L, (cid:96), ·(cid:105).

To examine traced evaluation c, τ, ρ, w (cid:59) f(c(cid:48)), in context we explicitly denotes both the input and output context,

(cid:104)L, (cid:96), (c, τ, ρ, w)(cid:105) (cid:59) (cid:104)L, (cid:96), f(c(cid:48))(cid:105).

Note that (cid:96) is initialized by the user and does not change during program execution. To account for this and lighten the
syntax, we deﬁne syntactic sugar of:

(cid:104)L, (c, τ, ρ, w)(cid:105)

(cid:104)L(cid:48), f(c(cid:48))(cid:105)

When reframing combinators to accumulate loss, the operational semantics of compose, extend, and resample only thread
their input L through program execution. For these combinators, we simply exchange the traced evaluation of (cid:59) with our

syntactic sugar of

as seen in Figure 3.

propose is the only combinator in which loss is accumulated

L2 = (cid:96)(ρ1, ρ2, w1, w2/u1) + L1

(cid:104)L1, (c1, τ1, ρ1, w1)(cid:105)

(cid:104)L0, q(c0)(cid:105)

c2, τ2, ρ2, w2 (cid:59) p(c0)[τ1]

c3, τ3, ρ3, w3 (cid:59) marginal(p)(c0)[τ2]
(cid:89)
ρ1(α)
u1 =

.

α∈dom(ρ1)\(dom(τ1)\dom(τ2))
(cid:104)L2, (cid:96), (c3, τ3, ρ3, w2 · w1/u1)(cid:105) (cid:59) (cid:104)L0, (cid:96), propose(p, q)(c0)(cid:105)

E PROPER WEIGHTING OF PROGRAMS

Lemma 1 (Strict proper weighting of the extend combinator). Evaluation of a target program p2 = extend(p1, f) is
γ.
strictly properly weighted for its unnormalized density
(cid:75)

γ when evaluation of p1 is strictly properly weighted for

p1
(cid:74)

p2

(cid:74)

(cid:75)

Proof. Recall from Section 3.4, that the program p2 denotes a composition

c1, τ1, ρ1, w1 (cid:59) p1(c0)

c1, τ2, ρ2, w1 (cid:59) p2(c0)

cf , τf , ρf , wf (cid:59) f(c1)
γ(τ2; c0) =

p2

τ2 = τ1 ⊕ τf

ρ2 = ρ1 ⊕ ρf

p1

γ(τ1; c0) ·

f

γ(τf ; c1)

(10)

(cid:74)
γ := γ1. Since the primitive program
Our induction hypothesis is that p1 is strictly properly weighted for its density
f may only include unobserved variables, wf = 1 and its evaluation is properly weighted relative to the prior density
p2
f
(cid:75)

(cid:75)
γ = γ2 follows directly from deﬁnitions
(cid:75)

p := pf . Strict proper weighting with respect to
(cid:104)

γ =

p1

f

(cid:74)

(cid:75)

(cid:74)

(cid:74)

(cid:75)

(cid:75)

(cid:74)

(cid:75)

(cid:74)

Ep2(c0)

(cid:105)
w1 h(τ2)

= Ep1(c0)

(cid:74)
(cid:104)
w1 Ef(c1)

(cid:2)h(τ1 ⊕ τf )(cid:3)(cid:105)

,

(cid:90)

(cid:90)

(cid:90)

=

=

=

(cid:90)

dτ1 γ1(τ1; c0)

dτf pf (τf ; c1) h(τ1 ⊕ τf ),

dτ1dτf γ2(τ1 ⊕ τf ; c0) h(τ1 ⊕ τf )

dτ2 γ2(τ2; c0) h(τ2).

(11)

Note in particular that Z1(c0) = Z2(c0), since the normalizing constant Zf (c1) = 1 for the program f.

Theorem 2 (Strict proper weighting of target programs). Evaluation of a target program p is (strictly) properly weighted
for the unnormalized density

γ that it denotes.

p
(cid:74)

(cid:75)

Proof. By induction on the grammar p ::= f | extend(p, f).

• Base case: p = f. This follows from Proposition 1.
• Inductive case: p2 = extend(p1, f). This follows from Lemma 1.

Lemma 2 (Strict proper weighting of the resample combinator). Evaluation of a program q2 = resample(q1) is strictly
properly weighted for its unnormalized density

γ when evaluation of q1 is strictly properly weighted for

γ.

q2

(cid:74)

(cid:75)

q1
(cid:74)

(cid:75)

Proof. In Section 3.4, we deﬁned the operational semantics for the resample combinator as

(cid:126)c1, (cid:126)τ1, (cid:126)ρ1, (cid:126)w1 (cid:59) q1((cid:126)c0)

(cid:126)a1 ∼ RESAMPLE( (cid:126)w1)

(cid:126)c2, (cid:126)τ2, (cid:126)ρ2 = REINDEX((cid:126)a1, (cid:126)c1, (cid:126)τ1, (cid:126)ρ1)

(cid:126)w2 = MEAN( (cid:126)w1)

(cid:126)c2, (cid:126)τ2, (cid:126)ρ2, (cid:126)w2 (cid:59) resample(q1)((cid:126)c0)

.

Whereas other combinators act on samples individually, the resample combinator accepts and returns a collection of samples.
Bold notation signiﬁes tensorized objects. For notational simplicity, we assume in this proof that all objects contain a
single dimension, e.g. (cid:126)τ = [τ 1, . . . , τ L], which is also the dimension along which resampling is performed, but this is not a
requirement in the underlying implementation.

q2(c0)

Let
outgoing samples are individually strictly properly weighted for the unnormalized density γ(· ; c0),

γ = γ(· ; c0) denote the unnormalized density of the program. Our goal is to demonstrate that

q1(c0)
(cid:75)
(cid:74)

γ =
(cid:75)

(cid:74)

Eq2(c0)

(cid:2)wl

2 h(τ l

2)(cid:3) =

(cid:90)

dτ (cid:48) γ(τ (cid:48); c0) h(τ (cid:48)),

under the inductive hypothesis that incoming samples are strictly properly weighted,

Eq1(c0)

(cid:2)wl

1 h(τ l

1)(cid:3) =

(cid:90)

dτ (cid:48) γ(τ (cid:48); c0) h(τ (cid:48)).

(12)

(13)

The resample combinator randomly selects ancestor indices (cid:126)a1 ∼ RESAMPLE( (cid:126)w1). Informally, this procedure selects al
with probability proportional to wk

1 . More formally, this procedure must satisfy

1 = k

E

RESAMPLE( (cid:126)w1)

(cid:2)I[al = k](cid:3) =

wk
1
l wl
1

(cid:80)

.

(14)

The outgoing return value, trace, and weight, are then reindexed according to (cid:126)a1, whereas the outgoing weights are set to the
average of the incoming weights

2 = cal
cl
1 ,

1

2 = τ al
τ l
1 ,

1

2 = ρal
ρl
1 ,

1

wl

2 =

1
L

L
(cid:88)

l(cid:48)=1

wl(cid:48)
1 .

(15)

Strict proper weighting now follows directly from deﬁnitions

Eq2(c0)

(cid:2)wl

2 h(τ l

2)(cid:3) = Eq2(c0)

= Eq1(c0)

= Eq1(c0)

= Eq1(c0)

= Eq1(c0)

(cid:34)

(cid:16) 1
L

(cid:34)

(cid:16) 1
L

(cid:34)

(cid:16) 1
L

(cid:34)

(cid:16) 1
L

(cid:88)

(cid:17)

wl(cid:48)
1

(cid:35)
h(τ al
1 )

1

l(cid:48)

(cid:88)

(cid:17)

E

wl(cid:48)
1

l(cid:48)

(cid:88)

(cid:17)

E

wl(cid:48)
1

l(cid:48)

(cid:35)

(cid:104)

(cid:105)
h(τ al
1 )

1

RESAMPLE( (cid:126)w1)

RESAMPLE( (cid:126)w1)

(cid:35)(cid:35)

I[al

1 = k] h(τ k
1 )

(cid:34)

(cid:88)

k

(cid:88)

wl(cid:48)
1

(cid:17) (cid:88)

E

l(cid:48)

k

(cid:35)
1 = k](cid:3) h(τ k
1 )

(cid:2)I[al

RESAMPLE( (cid:126)w1)

(16)

(cid:34)

(cid:16) 1
L

(cid:17) (cid:88)

(cid:0)(cid:0)(cid:88)
wl(cid:48)
1
(cid:0)
l(cid:48)
(cid:0)
(cid:88)

wk
1
(cid:24)(cid:24)(cid:24)(cid:24)(cid:80)
l(cid:48)(cid:48) wl(cid:48)(cid:48)
(cid:35)
1 h(τ k
1 )

wk

=

(cid:88)

k

1

1
L

k

k

(cid:35)
h(τ k
1 )

Eq1(c0)

(cid:2)wk

1 h(τ k

1 )(cid:3) .

= Eq1(c0)

(cid:34)

1
L

Lemma 3 (Strict proper weighting of the compose combinator). Evaluation of the program q3 = compose(q1, q2) is
strictly properly weighted for its unnormalized density
γ when evaluation of q1 and q2 is strictly properly
(cid:74)
weighted for the unnormalized densities
q2
(cid:75)
(cid:74)

compose(q1, q2)
γ.

γ and
(cid:75)

q1
(cid:74)

(cid:75)

Proof. In Section 3.4, we deﬁned the operational semantics for the compose combinator as
c1, τ1, ρ1, w1 (cid:59) q1(c0)

c2, τ2, ρ2, w2 (cid:59) q2(c1) dom(ρ1) ∩ dom(ρ2) = ∅

τ3 = τ2 ⊕ τ1

and its denotation as the product of conditional densities

c2, τ3, ρ3, w3 (cid:59) compose(q2, q1)(c0)

ρ3 = ρ2 ⊕ ρ1 w3 = w2 · w1

,

c1, τ1, ρ1, w1 (cid:59) q1(c0)

c2, τ2, ρ2, w2 (cid:59) q2(c1)

dom(ρ1) ∩ dom(ρ2) = ∅

.

compose(q2, q1)(c0)
(cid:74)

(τ1 ⊕ τ2) =

q2(c1)
(cid:75)

(τ2)

q1(c0)
(cid:75)

(τ1)

(cid:74)
γ = γ3 denote the unnormalized density of the composition. We will show that, for any measurable function h(τ3),
(cid:90)

(cid:75)

(cid:74)

Let

q3

(cid:74)

(cid:75)

Eq3(c0) [w3 h(τ3)] = Z3(c0)

dτ3 γ3(τ3; c0) h(τ3).

We can express the expectation with respect to the program q3 as a nested expectation with respect to q1 and q2

Eq3(c0) [w3 h(τ3)] = Eq1(c0)

(cid:105)
(cid:104)
w1 Eq2(c1) [w2 h(τ2 ⊕ τ1)]

,

(cid:74)

q1

γ = γ1,

Let
expectations as integrals with respect to γ1 and γ2. Strict proper weighting follows from deﬁnitions
(cid:90)

γ = γ2 of the inputs and their composition. By the induction hypothesis, we can express rewrite both
(cid:75)

q2
(cid:74)

(cid:90)

(cid:75)

Eq3(c0) [w3 h(τ3)] =

dτ1 γ1(τ1; c0)

dτ2 γ2(τ2; c1)h(τ1 ⊕ τ2; c0)

(cid:90)

(cid:90)

=

=

(cid:90)

dτ1

dτ2 γ1(τ1; c0) γ2(τ2; c1) h(τ1 ⊕ τ2; c0)

dτ3 γ3(τ3; c0) h(τ3; c0).

Lemma 4 (Strict proper weighting of the propose combinator). Evaluation of a program propose(p, q) is strictly prop-
γ when evaluation of q is strictly properly weighted for the
erly weighted for the unnormalized density
unnormalized density

propose(p, q)
(cid:75)

(cid:74)

γ.

q
(cid:75)

(cid:74)

Proof. Recall from Section 3.4 that that the operational semantics for propose are

c1, τ1, ρ1, w1 (cid:59) q(c0)

c2, τ2, ρ2, w2 (cid:59) p(c0)[τ1]

c3, τ3, ρ3, w3 (cid:59) marginal(p)(c0)[τ2]

u1 =

(cid:89)

ρ1(α)

α∈dom(ρ1)\(dom(τ1)\dom(τ2))
c3, τ3, ρ3, w2 · w1/u1 (cid:59) propose(p, q)(c0)

Our aim is to demonstrate that evaluation of propose(p, q) is strictly properly weighted for

This is to say that, for any measurable h(τ3)

propose(p, q)(c0)

γ =

(cid:75)

marginal(p)(c0)
(cid:75)

(cid:74)

γ = γp(·; c0).

(cid:74)

Eq(c0)

(cid:20) w2w1
u1

(cid:21)

h(τ3)

=

(cid:90)

dτ3 γp(·; c0) h(τ3).

We start by expressing the expectation with respect to q2(c0) as an expectation with respect to q1(c0) and p(c0)[τ1], and
use the inductive hypothesis to express the ﬁrst expectation as an integral with respect to

q(c0)

γ = γq(·; c0),
(cid:75)

Eq2(c0)

(cid:20) w2w1
u1

(cid:21)

h(τ3)

= Eq1(c0)

(cid:20)
w1 Ep(c0)[τ1]

(cid:20) w2
u1

(cid:90)

=

dτ1 γq(τ1; c0) Ep(c0)[τ1]

(cid:74)
(cid:21)(cid:21)

h(τ3)
(cid:20) w2
u1

(cid:21)
h(τ3)

.

p(c0)

= ˜γp(· ; c0) to refer to the density that p denotes, which possibly extends the density γp(· ; c0) using one or
We use
more primitive programs. We then use Equation 3 to replace w2/u1 with the relevant extended-space densities, and simplify
the resulting expressions

(cid:74)

(cid:75)

(cid:90)

dτ1 γ1(·; c0) Ep(c0)[τ1]

(cid:21)

h(τ3)

=

(cid:20) w2
u1

=

=

=

=

=

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

dτ1 γq(τ1; c0) Ep(c0)[τ1]

dτ1

dτ1

dτ1

(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
γq(τ1; c0)
(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
γq(τ1; c0)
(cid:90)

Ep(c0)[τ1]

(cid:90)

dτ2 pp[τ1](τ2; c0)
(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
pp[τ1](τ2; c0)
(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
pp[τ1](τ2; c0)

dτ2

dτ2 ˜γp(τ2; c0) h(τ3)

dτ3 γp(τ3; c0) h(τ3).

(cid:20) ˜γp(τ2; c0)pq[τ2](τ1; c0)
γq(τ1; c0)pp[τ1](τ2; c0)
(cid:20) ˜γp(τ2; c0) pq[τ2](τ1; c0)
pp[τ1](τ2; c0)

(cid:21)

h(τ3)

(cid:21)

h(τ3)

˜γp(τ2; c0) pq[τ2](τ1; c0)
pp[τ1](τ2; c0)

h(τ3)

˜γp(τ2; c0) pq[τ2](τ1; c0) h(τ3)

(cid:90)

(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)

dτ1pq[τ2](τ1; c0),

In the ﬁnal equality, we rely on the fact that γp(τ3; c0) is the marginal of ˜γp(τ2; c0) with respect to the set of auxiliary
variables dom(τ2) \ dom(τ3).

Theorem 3 (Strict proper weighting of compound inference programs). Compound inference programs q are strictly
properly weighted for their unnormalized densities γq.

Proof. By induction on the grammar for q.

• Base case: q = p. Theorem 2 above provides a proof.

• Inductive case: q2 = resample(q1). This follows from Lemma 2.
• Inductive case: q3 = compose(q1, q2). This follows from Lemma 3.
• Inductive case: q2 = propose(p, q1). This follows from Lemma 4.

F GRADIENT COMPUTATIONS

F.1 STOCHASTIC VARIATIONAL INFERENCE (SVI).

Let q2 = propose(p, q1) be a program in which the initial inference program q1, target program p, and inference program
q2 denote the densities

q1(c0)
(cid:75)

(cid:74)

γ = γq(· ; c0, φ)

p(c0)

γ = ˜γp(· ; c0, θ),

(cid:74)

(cid:75)

q2(c0)
(cid:74)

(cid:75)

γ = γp(· ; c0, θ), ,

with parameters θ and φ respectively. Notice that the target program p and the inference program q2 denote the same
γ and hence, as a result of Theorem 2, the evaluation of q2 is strictly properly weighted for γp.
density
q2(c0)
(cid:74)
Applying deﬁnition 1 we can now write

p(c0)

γ =

(cid:75)

(cid:75)

(cid:74)

Zp(c0; θ)Eπp(·; c0)[h(τ )] = Eq2(c0)[w2h(τ2)],
for any measurable function h. Given evaluations c1, τ1, ρ1, w1 ∼ q1(c0) and c(cid:48)
compute a stochastic lower bound (Burda et al., 2016),

c2, τ2, ρ2, w2 (cid:59) q2(c0),
2, τ (cid:48)

2, w(cid:48)

2, ρ(cid:48)

2 ∼ p[τ1](c0) we can similarly

L = Eq2(c0) [log w2] ≤ log

(cid:16)
Eq2(c0) [w2]

(cid:17)

= log (cid:0)Zp(c0, θ)Eπ(·;c0) [1](cid:1) = log Zp(c0, θ),

using the constant function h(τ ) = 1. The gradient of this bound

∇θEq2(c0) [log w2] = Eq1(c0)

= Eq1(c0)

(cid:20)
∇θ log w1 + ∇θEp(c0)[τ1]
(cid:20)
(cid:20)
∇θEp(c0)[τ1]

log

(cid:20)

log

˜γp(˜τ2; c0, θ)pq[˜τ2](τ1; c0, φ)
γq(τ1; c0, φ)pp[τ1](˜τ2; c0, θ)

(cid:21)(cid:21)

˜γp(˜τ2; c0, θ)pq[˜τ2](τ1; c0, φ)
pp[τ1](˜τ2; c0, θ)

(cid:21)(cid:21)

is a biased estimate of ∇θ log Zp, where we use Equation 3 to replace w2 with the incoming importance weight w1 and the
relevant extended-space densities.

If the target program does not introduce additional random variables, i.e. dom(˜τ2) \ dom(τ1) = ∅, the traces produced by
the conditioned evaluation ˜c2, ˜τ2, ˜ρ2, ˜w2 ∼ p[τ1](c0) do not depend on θ, as all variables are generated from the inference
program, and the prior term pp[τ1](˜τ2; c0) = 1. As a result we can move the gradient operator inside the inner expectation,

Eq1(c0)

(cid:104)
∇θEp(c0)[τ1]

(cid:2)log ˜γp(˜τ2; c0, θ)pq[˜τ2](τ1; c0)(cid:3)(cid:105)

= Eq1(c0)
= Eq2(c0)

(cid:104)

(cid:2)∇θ log ˜γp(˜τ2; c0, θ)pq[˜τ2](τ1; c0)(cid:3)(cid:105)

Ep(c0)[τ1]
(cid:2)∇θ log ˜γp(˜τ2; c0, θ)pq[˜τ2](τ1; c0)(cid:3) .

If, additionally, all random variables in the inference program are reused in the target program, i.e. dom(τ1) \ dom(˜τ2) = ∅,
the prior term pq[˜τ2](τ1; c0) = 1. Hence, in the case where the set of random variables in the proposal and target program is
the same, i.e. dom(˜τ2) = dom(τ1), we recover the standard variational inference gradient w.r.t. the model parameters θ,
Eq2(c0) [∇θ log γp(˜τ2; c0, θ)] .
The gradient with respect to the proposal parameters ∇φL can be approximated using likelihood-ratio estimators (Wingate,
Weber, 2013; Ranganath et al., 2014), reparameterized samples (Kingma, Welling, 2013; Rezende et al., 2014), or a
combination of the two (Ritchie et al., 2016). Here we only consider the fully reparameterized case, which allows us to
move the gradient operator inside the expectation

∇φEq2(c0) [log w2] = Eq2(c0) [∇φ log w2]
(cid:20) ∂ log w2
∂ ˜τ2
(cid:20) ∂ log w2
∂ ˜τ2

= Eq2(c0)

= Eq2(c0)

∂ ˜τ2
∂φ
∂ ˜τ2
∂φ

+

+

(cid:21)

∂ log w2
∂φ

∂
∂φ

log

pq[˜τ2](τ1; c0, φ)
γq(τ1; c0, φ)

(cid:21)

In the case where the set of random variables in the proposal and target program is the same, i.e. dom(˜τ2) = dom(τ1)
and q1 = f is a primitive program, we can write w1 = γf (τ1; c0, φ)/pf (τ1; c0, φ), and we recover the standard variational
inference gradient w.r.t φ,

Eq2(c0)

(cid:20) ∂
∂ ˜τ2

log

= Eq2(c0)

(cid:20) ∂
∂ ˜τ2

(cid:18) ˜γp(˜τ2; c0, θ)
pf (τ1; c0, φ)
(cid:18) ˜γp(˜τ2; c0, θ)
pf (τ1; c0, φ)

(cid:19) ∂ ˜τ2
∂φ
(cid:19) ∂ ˜τ2
∂φ

−

log

∂
∂φ

−

log pf (τ1; c0, φ)

(cid:21)

(cid:21)
log pf (τ1; c0, φ)

∂
∂φ

= −∇φKL(pf ||πp),

effectively minimizing a reverse KL-divergence. Noticing that the second term is zero in expectation, due to the reinforce
trick, we can derive the lower variance gradient

Eq2(c0)

(cid:20) ∂
∂ ˜τ2

log

γp(˜τ2; c0, θ)
pf (τ1; c0, φ)

∂ ˜τ2
∂φ

−

∂
∂φ

(cid:21)
log pf (τ1; c0, φ)

= Eq2(c0)

(cid:20) ∂
∂ ˜τ2

log

γp(˜τ2; c0, θ)
pf (τ1; c0, φ)

∂ ˜τ2
∂φ

(cid:21)

,

which can be approximated using reparameterized weights obtained by evaluating q2.

F.2 REWEIGHTED WAKE-SLEEP (RWS) STYLE INFERENCE.

To implement variational methods inspired by reweighted wake-sleep (Hinton et al., 1995; Bornschein, Bengio, 2015; Le
et al., 2019), we compute a self-normalized estimate of the gradient

∇θ log Zp(c0, θ) =

=

=

1
Zp(c0, θ)
1
Zp(c0, θ)
(cid:90)

∇θ

(cid:90)

(cid:90)

dτ γp(τ ; c0, θ)

dτ γp(τ ; c0, θ)∇θ log γp(τ ; c0, θ)

dτ πp(τ ; c0, θ)∇θ log γp(τ ; c0, θ)

= Eπp(· ;c0,θ) [∇θ log γp(τ ; c0, θ)] .

(17)

(18)

(19)

(20)

Notice that here we compute the gradient w.r.t. the non-extended density γp, which does not include auxiliary variables
and hence density terms which would integrate to one. In practice this allows us to compute lower variance approximation.
(cid:59) q2(c0) we can derive a
Using deﬁnition 1 and traces obtained from the evaluation of our inference program cl
costistent self-normalized estimator

2, wl
2

2, τ l

2, ρl

Eπp(· ;c0,θ) [∇θ log γp(τ ; c0, θ)] = Z −1

p (c0, θ)Eq2 [w2∇θ log γp(τ2; c0, θ)] (cid:39)

(cid:88)

l

wl
2
l(cid:48) wl(cid:48)
2

(cid:80)

∇θ log γp(τ l

2; c0, θ).

(21)

We can similarly approximate the gradient of the forward KL divergence with a self-normalized estimator,

−∇φKL(πp||πq) = Eπp(· ;c0,θ) [∇φ log πq(τ ; c0, φ)]

= Eπp(· ;c0,θ) [∇φ log γq(τ ; c0, φ)] − ∇φZq(c0, φ)
= Eπp(· ;c0,θ) [∇φ log γq(τ ; c0, φ)] − Eπq(· ;c0,θ) [∇φ log γq(τ ; c0, φ)]
= Z −1

p (c0, θ)Eq2 [w2∇φ log γq(τ1; c0, φ)] − Z −1
(cid:88)

(cid:19)

q

∇φ log γq(τ l

1; c0, φ).

(cid:18) wl
2
(cid:80)
l(cid:48) wl(cid:48)
2

−

(cid:80)

wl
1
l(cid:48) wl(cid:48)
1

(cid:39)

l

(c0, φ)Eq1 [w1∇φ log γq(τ1; c0, φ)]

In the special case where the proposal q1 = f is a primitive program without observations (i.e. wl
Eπq(· ;c0,θ) [∇φ log γq(τ ; c0, φ)] = Eπq(· ;c0,θ) [∇φ log πq(τ ; c0, φ)] = 0,

1 = 1) we have that

(22)

(23)

(24)

(25)

(26)

(27)

by application of the reinforce trick. In this case we drop the second term, which is introducing additional bias, through
self-normalization, and variance to the estimator, and recover the standard RWS estimator

(cid:88)

l

wl
2
l(cid:48) wl(cid:48)
2

(cid:80)

∇φ log γq(τ l

1; c0, φ).

(28)

G IMPLEMENTATION DETAILS FOR EXPERIMENTS

G.1 ANNEALED VARIATIONAL INFERENCE

In the annealing task we implement Annealed Variational Inference (Zimmermann et al., 2021) and learn to sample from a
multimodal Gaussian distribution γK, composed of eight equidistantly spaced modes with covariance matrix of I · 0.5 on a
circle of radius 10. We deﬁne our initial inference program and annealing path to be:

= Normal(µ = 0, σ = 5)

q0(c
(cid:75)
(cid:74)

qk(c)
(cid:75)

(cid:74)

γ = q1(c)1−βk γ(c)βk
K ,

βk =

k − 1
K − 1

,

for k = 1 . . . K

Implementations of these programs can be found in Figure 5. Additionally, each forward- and reverse- kernel program qk(c)
is deﬁned by a neural network:

x = Linear 50. ReLU(c)

µk = Linear 2(x) + c

covk = DiagEmbed . Softplus 2(x)

Learned intermediate densities of βk are embedded by a logit function and is extracted by sigmoid function.

A combinators implementation of nested variational inference is deﬁned:

def step (q , intermediate , do_resample ):

( fwd , rev ), p = intermediate
q(cid:48) = resample (q ) if do_resample else q
return propose ( extend (p , rev ), compose (q(cid:48) , fwd ))

path , kernels = ...
ixs = list ( range ( len ( path )))
do_resamples = map (λ i → i == ixs [ -1] , ixs )
nvir = reduce ( step , zip ( kernels , path [1:] , do_resamples [1:] , path [0]))

We implement nested variational inference (NVI), nested variational inference with resampling (NVIR), as well as nested
variational inference with learned intermediate densities (NVI*), and nested variational inference with resampling and
learned intermediate densities (NVI*). When implementing any NVI algorithm with resampling, we additionally implement
nested variational inference with resampling (NVIR*) by applying the resample combinator after all but the ﬁnal proposal
combinator.

We evaluate our model by training each model for 20,000 iterations with a sampling budget of 288 samples, distributed
across K intermediate densities. Metrics of log ˆZ and effective sample size average over 100 batches of 1,000 samples and
results are calculated using the mean of 10 training runs using unique, ﬁxed seeds. In the evaluation of NVIR* we do not
resample at test time.

G.2 AMORTIZED POPULATION GIBBS SAMPLERS

Many inference task requires learning a deep generative model. For this purpose, we we evaluate combinators in an
unsupervised tracking task. In this task, the data is a corpus of simulated videos that each contain multiple moving objects.
Out goal is to learn both the target program (i.e. the generative model) and the inference program using the APG sampler.

Consider a sequence of video frames x1:T , which contains T time steps and D different objects. We assume that the kth
object in the tth frame xt can be represented by some object feature zwhat
and a time-dependent position variable zwhere
.
The deep generative model takes the form

d,t

d

zwhat
d

∼ Normal(0, I),

xt ∼ Bernoulli

(cid:16)

(cid:16) (cid:88)

σ

d

ST(cid:0)gθ(zwhat

d

), zwhere
d,t

(cid:1)(cid:17)(cid:17)

,

d = 1, 2, ..., D,

t = 1, 2, .., T.

zwhere
d,1 ∼ Normal(0, I),

zwhere
d,t

∼ Normal(zwhere

d,t−1 , σ2

0I),

def target (s , x ):

zs , xs = s. sample ( Categorical (K), " zs "), []
for k in K:

count = sum ( zs == k )
dist = Normal (µ(k) , σ(k ))
xs . append (s . sample ( dist , str (k), count ))

return s , shuffle ( cat ( xs ))

def q_0 (s ):

c = s. sample ( Normal (µ(k ), σ(k )) , " q_0 " )
return s , c

η = ... # initialize neural network for kernel k
def q_k (s , c ):

c(cid:48) = s. sample ( Normal (ηµ (k), ησ (k )) , " q_k " )
return s , c(cid:48)

def gamma_k (s , log_gamma , q_0 , beta =1.0):

# sample from the initial proposal
x = s . sample ( q_0 , "x ")
# add a heuristic factor
s. factor ( beta * ( log_gamma (x) - q0 . log_prob (x )))
return x

Figure 5: models deﬁned for Annealed Variational Inference

∈ R2. To perform inference for this model, APG sampler learns neural proposals
where σ0 = 0.1 and zwhat
to iterate conditional updates to blocks of variables, which consists of one block of object features and T blocks of each
time-dependent object position as

∈ R10, zwhere

d,t

d

{zwhat

1:D },

{zwhere

1:D, 1},

{zwhere

1:D, 2},

· · · ,

{zwhere

1:D, T }

(29)

We train the model on 10000 video instances, each containing 10 timesteps and 3 different objects. We train with batch size
5, sample size 20, Adam optimizer with β1 = 0.9, β2 = 0.99 and lr=2e − 4.

Architecture for Generative Model. We learn a deep generative model of the form

pθ(x1:T | zwhat

1:D , zwhere

1:T

) =

T
(cid:89)

t=1

(cid:16)

Bernoulli

xt

(cid:12)
(cid:12)
(cid:12) σ

(cid:16) (cid:88)

d

ST(cid:0)gθ(zwhat

d

), zwhere
d,t

(cid:1)(cid:17)(cid:17)

(30)

Given each object feature zwhat
of which is

d

, the APG sampler reconstruct a 28 × 28 object image using a MLP decoder, the architecture

Decoder

gθ(·)
∈ R10

Input zwhat
FC 200. ReLU. FC 400. ReLU. FC 784. Sigmoid.

d

Then we put each reconstructed image gθ(zwhat
variable zwhere

d

d,t

as input. To ensure a pixel-wise Bernoulli likelihood, we clip on the composition as

) onto a 96 × 96 canvas using a spatial transformer ST which takes position

For each pixel pi ∈

(cid:16) (cid:88)

ST(cid:0)gθ(zwhat

d

), zwhere
d,t

(cid:1)(cid:17)

, σ(pi) =

d






pi = 0
pi = pi
pi = 1

if pi < 0
if 0 ≤ pi ≤ 1
if pi > 1

(31)

Architecture for Gibbs Neural Proposals. The APG sampler in the bouncing object employs neural proposals of the form

qφ(zwhere

1:D, t | xt) =

qφ(zwhere

1:D, t | xt, zwhat

1:D ) =

qφ(zwhat

1:D | x1:T , zwhere

1:T

) =

(cid:16)

(cid:16)

(cid:16)

Normal

Normal

Normal

D
(cid:89)

d=1

D
(cid:89)

d=1

D
(cid:89)

d=1

zwhere
d,t

(cid:12)
(cid:12) ˜µwhere
(cid:12)

d,t

, ˜σwhere 2
d,t

I

zwhere
d,t

(cid:12)
(cid:12) ˜µwhere
(cid:12)

d,t

, ˜σwhere 2
d,t

I

(cid:17)

,

(cid:17)

,

zwhat
d

(cid:12)
(cid:12) ˜µwhat
(cid:12)

d

, ˜σwhat 2
d

I

(cid:17)

.

for t = 1, 2, . . . , T,

(32)

for t = 1, 2, . . . , T,

(33)

(34)

We train the proposals with instances containing D = 3 objects and T = 10 time steps and test them with instances
containing up to D = 5 objects and T = 100 time steps. We use the tilde symbol ˜ to denote the parameters of the
conditional neural proposals (i.e. approximate Gibbs proposals).

Encoder

f L
φ(·)
d,t ∈ R4761

Input xconv
FC 200. ReLU. FC 2 × 100. ReLU. FC 2 × 2.

The APG sampler uses these proposals to iterate over the T + 1 blocks

{zwhat

1:D },

{zwhere

1:D, 1},

{zwhere

1:D, 2},

. . . ,

{zwhere

1:D, T }.

For the position features, the proposal qφ(zwhere
contain different pre-steps where we compute the input of that network. The initial proposal qφ(zwhere
the frame xt with the mean image of the object dataset; The conditional proposal qφ(zwhere
1:D, t | xt, zwhat
frame xt with each reconstructed object image gθ(zwhat
d
d = 1, 2, ..., D. Here is pseudocode of both pre-steps:

1:D ) share the same network, but
1:D, t | xt) will convolve
1:D ) will convolve the
). We perform convolution sequentially by looping over all objects

1:D, t | xt) and proposal qφ(zwhere

1:D, t | xt, zwhat

1:D, t | xt)

Algorithm 1 Convolution Processing for qφ(zwhere
1: Input frame xt ∈ R9216, mean image of object dataset mm ∈ R784
2: for d = 1 to D do
xconv
d,t ←− Conv2d(xt) with kernel mm, stride = 1, no padding.
3:
4: end for
5: Output Convolved features {xconv

d,t ∈ R4761}D

d=1

1:D, t | xt, zwhat
1:D )

Algorithm 2 Convolution Processing for qφ(zwhere
1: Input frame xt ∈ R9216, reconstructed object objects {gθ(zwhat
2: for d = 1 to D do
d,t ←− Conv2d(xt) with kernel gθ(zwhat
xconv
3:
4: end for
5: Output Convolved features {xconv

d,t ∈ R4761}D

d=1

d

d

), stride = 1, no padding.

) ∈ R784}D

d=1

We employ a MLP encoder f L
positions {zwhere

d,t

d=1 at step t, i.e. vector-valued mean ˜µwhere
}D

d,t

φ(·) that takes the convolved features as input and predict the variational parameters for

and logarithm of the diagonal covariance log ˜σwhere 2

d,t

as

d = 1, 2, . . . , D.

(35)

˜µwhere
d,t

, log ˜σwhere 2

d,t ←− f L

φ(xconv

d,t ),

The architecture of the MLP encoder f L

φ(·) is

For the object features, the APG sampler performs conditional updates in the sense that we crop each frame xt into a 28 × 28
subframe according to zwhere

using the spatial transformer ST as

d,t

d,t ←− ST(cid:0)xt, zwhere
xcrop

d,t

(cid:1),

d = 1, 2, . . . , D,

t = 1, 2, . . . , T.

(36)

we employ a MLP encoder T G
statistics, which we will sum up over all the time steps.

φ (·) that takes the cropped subframes as input, and predicts frame-wise neural sufﬁcient

Then we employ another network f G
{zwhat
d
architecture of this network is

d=1, i.e. the vector-valued means {˜µwhat
}D

φ (·) that takes the sums as input, and predict the variational parameters for object features
}D
d=1. The

d=1 and the logarithms of the diagonal covariances {log ˜σwhat 2
}D

d

d

REFERENCES

Baydin, Atılım Güne¸s, Le, Tuan Anh, Heinrich, Lukas, Gram-Hansen, Bradley, Schroeder de Witt, Christian, Bhimji, Wahid,

Cranmer, Kyle, Wood, Frank. Pyprob/Ppx. pyprob. 2018.

φ (·)
f G
d,t ∈ R784

Encoder
Input xcrop
FC 400. ReLU. FC 200. ReLU. −→ Neural Sufﬁcient Statistics T G
Intermediate Input (cid:80)T

d,t ) −→ FC 2 × 10.

φ (xcrop

t=1 T G

φ (xcrop

d,t ) ∈ R200

Baydin, Atılım Güne¸s, Shao, Lei, Bhimji, Wahid, Heinrich, Lukas, Meadows, Lawrence F., Liu, Jialin, Munk, Andreas,
Naderiparizi, Saeid, Gram-Hansen, Bradley, Louppe, Gilles, Ma, Mingfei, Zhao, Xiaohui, Torr, Philip, Lee, Victor,
Cranmer, Kyle, Prabhat, Wood, Frank. “Etalumis: Bringing Probabilistic Programming to Scientiﬁc Simulators at Scale.”
Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis (SC19),
November 17–22, 2019. 2019.

Bingham, Eli, Chen, Jonathan P., Jankowiak, Martin, Obermeyer, Fritz, Pradhan, Neeraj, Karaletsos, Theofanis, Singh, Rohit,
Szerlip, Paul, Horsfall, Paul, Goodman, Noah D. “Pyro: Deep Universal Probabilistic Programming.” arXiv:1810.09538
[cs, stat] (Oct. 2018). arXiv: 1810.09538 [cs, stat].

Borgström, Johannes, Gordon, Andrew D., Greenberg, Michael, Margetson, James, Van Gael, Jurgen. “Measure Transformer
Semantics for Bayesian Machine Learning.” Programming Languages and Systems. Ed. by Gilles Barthe. Berlin,
Heidelberg: Springer Berlin Heidelberg, 2011, pp. 77–96. ISBN: 978-3-642-19718-5.

Bornschein, Jörg, Bengio, Yoshua. “Reweighted Wake-Sleep.” International Conference on Learning Representations

(2015). arXiv: 1406.2751.

Burda, Yuri, Grosse, Roger, Salakhutdinov, Ruslan. “Importance Weighted Autoencoders.” International Conference on

Representations. 2016. arXiv: 1509.00519.

Carpenter, Bob, Gelman, Andrew, Hoffman, Matthew D., Lee, Daniel, Goodrich, Ben, Betancourt, Michael, Brubaker,
Marcus, Guo, Jiqiang, Li, Peter, Riddell, Allen. “Stan : A Probabilistic Programming Language.” English. Journal of
Statistical Software 76.1 (Jan. 2017). ISSN: 1548-7660. DOI: 10.18637/jss.v076.i01.

Caterini, Anthony L., Doucet, Arnaud, Sejdinovic, Dino. “Hamiltonian Variational Auto-Encoder.” arXiv:1805.11328 [cs,

stat] (May 2018). arXiv: 1805.11328 [cs, stat].

Chopin, N. “A Sequential Particle Filter Method for Static Models.” Biometrika 89.3 (Aug. 2002), pp. 539–552. DOI:

10.1093/biomet/89.3.539.

Clerc, Florence, Danos, Vincent, Dahlqvist, Fredrik. “Pointless Learning (long version).” International Conference on

Foundations of Software Science and Computation Structures (FoSSaCS). 679127. 2017, pp. 1–19.

Cusumano-Towner, Marco F., Saad, Feras A., Lew, Alexander K., Mansinghka, Vikash K. “Gen: A General-Purpose
Probabilistic Programming System with Programmable Inference.” Proceedings of the 40th ACM SIGPLAN Conference
on Programming Language Design and Implementation. PLDI 2019. New York, NY, USA: Association for Computing
Machinery, June 2019, pp. 221–236. ISBN: 978-1-4503-6712-7. DOI: 10.1145/3314221.3314642.

Dahlqvist, Fredrik, Silva, Alexandra, Danos, Vincent, Garnier, Ilias. “Borel Kernels and their Approximation, Categorically.”
Electronic Notes in Theoretical Computer Science 341 (2018), pp. 91–119. ISSN: 15710661. DOI: 10.1016/j.entcs.
2018.11.006. arXiv: 1803.02651. URL: https://doi.org/10.1016/j.entcs.2018.11.006.

De Raedt, Luc, Kimmig, Angelika, Toivonen, Hannu. “ProbLog: A Probabilistic Prolog and Its Application in Link

Discovery.” IJCAI International Joint Conference on Artiﬁcial Intelligence (2007), pp. 2468–2473.

Fong, Brendan. “Causal Theories: A Categorical Perspective on Bayesian Networks.” Master of Science in Mathematics
and the Foundations of Computer Science. University of Oxford, 2013. arXiv: 1301.6201. URL: http://arxiv.org/abs/
1301.6201.

Ge, Hong, Xu, Kai, Ghahramani, Zoubin. “Turing: A Language for Flexible Probabilistic Inference.” Proceedings of the
Twenty-First International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), Amos Storkey and Fernando
Perez-Cruz (Eds.) Vol. 84. 2018, pp. 1682–1690.

Goodman, Noah, Mansinghka, Vikash, Roy, Daniel M, Bonawitz, Keith, Tenenbaum, Joshua B. “Church: A Language for

Generative Models.” Proc. 24th Conf. Uncertainty in Artiﬁcial Intelligence (UAI). 2008, pp. 220–229.

Goodman, Noah D, Stuhlmüller, Andreas. The Design and Implementation of Probabilistic Programming Languages. 2014.

Heunen, Chris, Kammar, Ohad, Staton, Sam, Yang, Hongseok. “A convenient category for higher-order probability theory.”
Proceedings - Symposium on Logic in Computer Science. 2017. ISBN: 9781509030187. DOI: 10.1109/LICS.2017.8005137.
arXiv: 1701.02547.

Hinton, G. E., Dayan, P., Frey, B. J., Neal, R. M. “The "Wake-Sleep" Algorithm for Unsupervised Neural Networks.” en.

Science 268.5214 (May 1995), pp. 1158–1161. ISSN: 0036-8075, 1095-9203. DOI: 10.1126/science.7761831.

Hoffman, Matthew D. “Learning deep latent Gaussian models with Markov chain Monte Carlo.” International conference

on machine learning. PMLR. 2017, pp. 1510–1519.

Holtzen, Steven, Van den Broeck, Guy, Millstein, Todd. “Scaling Exact Inference for Discrete Probabilistic Programs.”
Proceedings of the ACM on Programming Languages 4.OOPSLA (Nov. 2020), 140:1–140:31. DOI: 10.1145/3428208.

Huang, Chin-Wei, Tan, Shawn, Lacoste, Alexandre, Courville, Aaron C. “Improving Explorability in Variational Inference
with Annealed Variational Objectives.” Advances in Neural Information Processing Systems 31. Ed. by S. Bengio, H.
Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, R. Garnett. Curran Associates, Inc., 2018, pp. 9701–9711.

Kingma, Diederik P., Welling, Max. “Auto-Encoding Variational Bayes.” International Conference on Learning Representa-

tions (2013).

Le, Tuan Anh, Igl, Maximilian, Rainforth, Tom, Jin, Tom, Wood, Frank. “Auto-Encoding Sequential Monte Carlo.”

International Conference on Learning Representations. 2018. arXiv: 1705.10306.

Le, Tuan Anh, Kosiorek, Adam R., Siddharth, N., Teh, Yee Whye, Wood, Frank. “Revisiting Reweighted Wake-Sleep for
Models with Stochastic Control Flow.” Uncertainty in Artiﬁcial Intelligence. Le and Kosiorek contributed equally. 2019.

Lew, Alexander K., Cusumano-Towner, Marco F., Sherman, Benjamin, Carbin, Michael, Mansinghka, Vikash K. “Trace
Types and Denotational Semantics for Sound Programmable Inference in Probabilistic Languages.” ACM Principles of
Programming Languages. Vol. 4. January. 2020, pp. 1–31. DOI: 10.1145/3371087.

Li, Yingzhen, Turner, Richard E, Liu, Qiang. “Approximate inference with amortised mcmc.” arXiv preprint

arXiv:1702.08343 (2017).

Liu, Jun S. Monte Carlo strategies in scientiﬁc computing. Springer Science & Business Media, 2008.

Maddison, Chris J, Lawson, John, Tucker, George, Heess, Nicolas, Norouzi, Mohammad, Mnih, Andriy, Doucet, Arnaud,
Teh, Yee. “Filtering Variational Objectives.” Advances in Neural Information Processing Systems 30. Ed. by I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, R. Garnett. Curran Associates, Inc., 2017, pp. 6573–6583.

Mansinghka, Vikash, Selsam, Daniel, Perov, Yura. “Venture: A Higher-Order Probabilistic Programming Platform with

Programmable Inference.” arXiv (Mar. 2014), pp. 78–78.

Masrani, Vaden, Le, Tuan Anh, Wood, Frank. “The Thermodynamic Variational Objective.” Advances in Neural Information
Processing Systems. Ed. by H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, R. Garnett. Vol. 32.
Curran Associates, Inc., 2019.

Minka, T, Winn, J, Guiver, J, Knowles, D. Infer.NET 2.4, Microsoft Research Cambridge. 2010.

Murray, Lawrence M. “Bayesian State-Space Modelling on High-Performance Hardware Using LibBi.” arXiv:1306.3277

[stat] (June 2013). arXiv: 1306.3277 [stat].

Murray, Lawrence M., Schön, Thomas B. “Automated Learning with a Probabilistic Programming Language: Birch.” Annual

Reviews in Control 46 (Jan. 2018), pp. 29–43. ISSN: 1367-5788. DOI: 10.1016/j.arcontrol.2018.10.013.

Naesseth, Christian, Linderman, Scott, Ranganath, Rajesh, Blei, David. “Variational Sequential Monte Carlo.” en. Interna-

tional Conference on Artiﬁcial Intelligence and Statistics. PMLR, Mar. 2018, pp. 968–977.

Naesseth, Christian, Lindsten, Fredrik, Schon, Thomas. “Nested Sequential Monte Carlo Methods.” International Conference

on Machine Learning. 2015, pp. 1292–1301.

Naesseth, Christian A., Lindsten, Fredrik, Schön, Thomas B. “Elements of Sequential Monte Carlo.” arXiv:1903.04797 [cs,

stat] (Mar. 2019). arXiv: 1903.04797. (Visited on 12/16/2019).

Narayanan, Praveen, Carette, Jacques, Romano, Wren, Shan, Chung-chieh, Zinkov, Robert. “Probabilistic Inference
by Program Transformation in Hakaru (System Description).” International Symposium on Functional and Logic
Programming. Springer, 2016, pp. 62–79.

Obermeyer, Fritz, Bingham, Eli, Jankowiak, Martin, Phan, Du, Chen, Jonathan P. “Functional Tensors for Probabilistic

Programming.” arXiv preprint arXiv:1910.10775 (2019).

Paige, Brooks, Wood, Frank. “A Compilation Target for Probabilistic Programming Languages.” International Conference

on Machine Learning (ICML) 32 (Mar. 2014).

Pfeffer, Avi. Figaro: An Object-Oriented Probabilistic Programming Language. Tech. rep. 9781577354260. 2009, pp. 1–9.

Pfeffer, Avi, Lynn, Spencer K. “Scruff: A Deep Probabilistic Cognitive Architecture for Predictive Processing.” Biologically

Inspired Cognitive Architectures Meeting. Springer, 2018, pp. 245–259.

Plummer, Martyn. “JAGS: A Program for Analysis of Bayesian Graphical Models Using Gibbs Sampling.” Proceedings of

the 3rd International Workshop on Distributed Statistical Computing. Vol. 124. Vienna, Austria., 2003.

Rainforth, Tom, Kosiorek, Adam, Le, Tuan Anh, Maddison, Chris, Igl, Maximilian, Wood, Frank, Teh, Yee Whye. “Tighter
Variational Bounds Are Not Necessarily Better.” en. International Conference on Machine Learning. July 2018, pp. 4277–
4285.

Ranganath, Rajesh, Gerrish, Sean, Blei, David. “Black Box Variational Inference.” en. Artiﬁcial Intelligence and Statistics.

Apr. 2014, pp. 814–822.

Rezende, Danilo Jimenez, Mohamed, Shakir, Wierstra, Daan. “Stochastic Backpropagation and Approximate Inference in
Deep Generative Models.” Proceedings of the 31st International Conference on Machine Learning. Ed. by Eric P. Xing,
Tony Jebara. Vol. 32. Proceedings of Machine Learning Research 2. Bejing, China: PMLR, June 2014, pp. 1278–1286.

Ritchie, Daniel, Horsfall, Paul, Goodman, Noah D. “Deep Amortized Inference for Probabilistic Programs.” en.

arXiv:1610.05735 [cs, stat] (Oct. 2016). arXiv: 1610.05735 [cs, stat].

Salimans, Tim, Kingma, Diederik, Welling, Max. “Markov chain monte carlo and variational inference: Bridging the gap.”

International Conference on Machine Learning. 2015, pp. 1218–1226.

Salvatier, John, Wiecki, Thomas V, Fonnesbeck, Christopher. “Probabilistic programming in Python using PyMC3.” PeerJ

Computer Science 2 (2016), e55.

Schulman, John, Heess, Nicolas, Weber, Theophane, Abbeel, Pieter. “Gradient Estimation Using Stochastic Computation
Graphs.” Advances in Neural Information Processing Systems 28. Ed. by C. Cortes, N. D. Lawrence, D. D. Lee, M.
Sugiyama, R. Garnett. Curran Associates, Inc., 2015, pp. 3528–3536.

´Scibior, Adam, Kammar, Ohad, Ghahramani, Zoubin. “Functional Programming for Modular Bayesian Inference.” Proc.
ACM Program. Lang. 2.ICFP (July 2018), 83:1–83:29. ISSN: 2475-1421. DOI: 10.1145/3236778. URL: http://doi.acm.
org/10.1145/3236778.

´Scibior, Adam, Kammar, Ohad, Vákár, Matthijs, Staton, Sam, Yang, Hongseok, Cai, Yufei, Ostermann, Klaus, Moss,
Sean K., Heunen, Chris, Ghahramani, Zoubin. “Denotational Validation of Higher-Order Bayesian Inference.” Proc. ACM
Program. Lang. 2.POPL (Dec. 2017), 60:1–60:29. ISSN: 2475-1421. DOI: 10.1145/3158148.

Shan, Chung-chieh, Ramsey, Norman. “Exact Bayesian inference by symbolic disintegration.” Proceedings of the 44th ACM

SIGPLAN Symposium on Principles of Programming Languages. 2017, pp. 130–144.

Siddharth, N., Paige, Brooks, van de Meent, Jan-Willem, Desmaison, Alban, Goodman, Noah D., Kohli, Pushmeet, Wood,
Frank, Torr, Philip. “Learning Disentangled Representations with Semi-Supervised Deep Generative Models.” Advances
in Neural Information Processing Systems 30. Ed. by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S.
Vishwanathan, R. Garnett. 2017, pp. 5927–5937.

Spiegelhalter, David J, Thomas, Andrew, Best, Nicky G, Gilks, Wally R. BUGS: Bayesian Inference Using Gibbs Sampling,

Version 0.50. MRC Biostatistics Unit, Cambridge. 1995.

Tolpin, David. Infergo: Go Programs That Learn. en. http://infergo.org/. 2018.

Tolpin, David, van de Meent, Jan-Willem, Yang, Hongseok, Wood, Frank. “Design and Implementation of Probabilistic
Programming Language Anglican.” Proceedings of the 28th Symposium on the Implementation and Application of
Functional Programming Languages. IFL 2016. Leuven, Belgium: ACM, 2016, 6:1–6:12. ISBN: 978-1-4503-4767-9.
DOI: 10/ghxhzn.

Toronto, Neil, McCarthy, Jay, Van Horn, David. “Running Probabilistic Programs Backwards.” European Symposium on
Programming Languages and Systems. Vol. 3. 1. 2015, pp. 53–79. arXiv: 1412.4053. URL: http://arxiv.org/abs/1412.
4053.

Tran, Dustin, Hoffman, Matthew D, Moore, Dave, Suter, Christopher, Vasudevan, Srinivas, Radul, Alexey, Johnson, Matthew,
Saurous, Rif A. “Simple, distributed, and accelerated probabilistic programming.” Proceedings of the 32nd International
Conference on Neural Information Processing Systems. 2018, pp. 7609–7620.

Tran, Dustin, Kucukelbir, Alp, Dieng, Adji B., Rudolph, Maja, Liang, Dawen, Blei, David M. “Edward: A Library for
Probabilistic Modeling, Inference, and Criticism.” arXiv:1610.09787 [cs, stat] (Oct. 2016). arXiv: 1610.09787 [cs,
stat].

Tucker, George, Lawson, Dieterich, Gu, Shixiang, Maddison, Chris J. “Doubly Reparameterized Gradient Estimators for

Monte Carlo Objectives.” arXiv:1810.04152 [cs, stat] (Oct. 2019). arXiv: 1810.04152 [cs, stat].

van de Meent, Jan-Willem, Paige, Brooks, Tolpin, David, Wood, Frank. “An Interface for Black Box Learning in Probabilistic

Programs.” POPL Workshop on Probabilistic Programming Semantics. 2016.

– “Black-Box Policy Search with Probabilistic Programs.” 2016, 1195–1204.

van de Meent, Jan-Willem, Paige, Brooks, Yang, Hongseok, Wood, Frank. “An Introduction to Probabilistic Programming.”

arXiv:1809.10756 [cs, stat] (Sept. 2018). arXiv: 1809.10756 [cs, stat].

Wang, Tongzhou, Wu, Yi, Moore, Dave, Russell, Stuart J. “Meta-learning MCMC proposals.” Advances in Neural Informa-

tion Processing Systems. 2018, pp. 4146–4156.

Wingate, David, Weber, Theo. “Automated Variational Inference in Probabilistic Programming.” arXiv preprint

arXiv:1301.1299 (2013), pp. 1–7. arXiv: 1301.1299.

Wood, Frank, van de Meent, Jan-Willem, Mansinghka, Vikash. “A New Approach to Probabilistic Programming Inference.”

Artiﬁcial Intelligence and Statistics. 2014, pp. 1024–1032.

Wu, Hao, Zimmermann, Heiko, Sennesh, Eli, Le, Tuan Anh, van de Meent, Jan-Willem. “Amortized Population Gibbs
Samplers with Neural Sufﬁcient Statistics.” International Conference on Machine Learning. PMLR. 2020, pp. 10421–
10431.

Zimmermann, Heiko, Wu, Hao, Esmaeili, Babak, Stites, Sam, van de Meent, Jan-Willem. “Nested Variational Inference.”

3rd Symposium on Advances in Approximate Bayesian Inference (2021).

Zinkov, Robert, Shan, Chung-chieh. “Composing Inference Algorithms as Program Transformations.” Uncertainty in

Artiﬁcial Intelligence (2017). arXiv: 1603.01882.

