Accelerating Primal Solution Findings for Mixed Integer Programs
Based on Solution Prediction

Jian-Ya Ding1, Chao Zhang1, Lei Shen1, Shengyin Li1, Bing Wang1, Yinghui Xu1, Le Song2
1: Artiﬁcial Intelligence Department, Zhejiang Cainiao Supply Chain Management Co., Ltd
2: Computational Science and Engineering College of Computing, Georgia Institute of Technology

9
1
0
2

p
e
S
9

]
I

A
.
s
c
[

2
v
5
7
5
9
0
.
6
0
9
1
:
v
i
X
r
a

Abstract

Mixed Integer Programming (MIP) is one of the most
widely used modeling techniques for combinatorial opti-
mization problems. In many applications, a similar MIP
model is solved on a regular basis, maintaining remark-
able similarities in model structures and solution ap-
pearances but differing in formulation coefﬁcients. This
offers the opportunity for machine learning methods to
explore the correlations between model structures and
the resulting solution values. To address this issue, we
propose to represent an MIP instance using a tripartite
graph, based on which a Graph Convolutional Network
(GCN) is constructed to predict solution values for binary
variables. The predicted solutions are used to generate a
local branching type cut which can be either treated as a
global (invalid) inequality in the formulation resulting
in a heuristic approach to solve the MIP, or as a root
branching rule resulting in an exact approach. Computa-
tional evaluations on 8 distinct types of MIP problems
show that the proposed framework improves the primal
solution ﬁnding performance signiﬁcantly on a state-of-
the-art open-source MIP solver.

Introduction
Mixed Integer Programming (MIP) is widely used to solve
combinatorial optimization (CO) problems in the ﬁeld of
Operations Research (OR). The existence of integer vari-
ables endows MIP formulations with the ability to capture
the discrete nature of many real-world decisions. Applica-
tions of MIP include production scheduling [Chen, 2010],
vehicle routing [Laporte, 2009], facility location [Farahani
and Hekmatfar, 2009], airline crew scheduling [Gopalakr-
ishnan and Johnson, 2005], to mention only a few. In many
real-world settings, homogeneous MIP instances with simi-
lar scales and combinatorial structures are optimized repeat-
edly but treated as completely new tasks. These MIP models
share remarkable similarities in model structures and solu-
tion appearances, which motivates us to integrate Machine
Learning (ML) methods to explore correlations between an
MIP model’s structure and its solution values to improve the
solver’s performance.

Identifying correlations between problem structures and
solution values is not new, and is widely used as guidelines

Copyright c(cid:13) 2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

for heuristics design for CO problems. These heuristic meth-
ods are usually human-designed priority rules to guide the
search directions to more promising regions in solution space.
For example, the nearest neighbor algorithm [Cook, 2011] for
the traveling salesman problem (TSP) constructs a heuristic
solution by choosing the nearest unvisited node as the sales-
man’s next move, based on the observation that two distantly
distributed nodes are unlikely to appear consecutively in the
optimal route. Similar examples include the shortest pro-
cessing time ﬁrst heuristic for ﬂow shop scheduling [Pinedo,
2012], the saving heuristic for vehicle routing [Clarke and
Wright, 1964], the ﬁrst ﬁt algorithm for bin packing [Dósa
and Sgall, 2013], among many others. A major drawback of
heuristics design using problem-speciﬁc knowledge is the
lack of generalization to other problems, where new domain
knowledge has to be re-identiﬁed.

MIP models can describe CO problems of various types us-
ing a standard formulation strategy z = minAx≤b,x∈X cT x,
differing only in model coefﬁcients A, b and c and integrality
constraints X . This makes it possible to explore connections
between problem structures and the resulting solution values
without prior domain knowledge. Predicting solution values
of general integer variables in MIP is a difﬁcult task. Notice
that most MIP models are binary variables intensive1, a natu-
ral way to explore hidden information in the model is to treat
solution value prediction of binary variables as binary clas-
siﬁcation tasks. Major challenges in solution prediction lie
in the implicit correlations among decision variables, since
a feasible solution x is restricted by constraints in MIP, i.e.,
Ax ≤ b. Rather than predicting each decision variable value
isolatedly, we propose a tripartite graph representation of
MIP and use graph embedding to capture connections among
the variables. Note that none of the two variable nodes are di-
rectly linked in the trigraph, but can be neighbors of distance
2 if they appear in the same constraint in the MIP formulation.
Correlations among variables are reﬂected in embeddings of
the trigraph where each vertex maintains aggregate feature
information from its neighbors.

Incorporating solution prediction results in MIP solving

1Take the benchmark set of MIPLIB 2017 [miplib2017, 2018]
as an example, among all 240 MIP benchmark instances, 164 of
them are Binary Integer Linear Programming (BILP) problems, and
44 out of the 76 remainings are imbalanced in the sense that binary
variables account for more than 90% of all integer variables.

 
 
 
 
 
 
process is not trivial. Fixing a single false-predicted deci-
sion variable can sometimes lead to the infeasibility of the
entire problem. Instead of utilizing the predicted solutions
directly, we identify predictable decision variables and use
this information to guide the Branch and Bound (B&B) tree
search to focus on unpredictable ones to accelerate conver-
gence. This is achieved by a novel labeling mechanism on the
training instances, where a sequence of feasible solutions is
generated by an iterated proximity search method. Stable de-
cision variables, of which the value remain unchanged across
these solutions, are recorded. It is noticeable that although
obtaining optimal solutions can be a difﬁcult task, the sta-
ble variables can be viewed as an easy-to-predict part that
reﬂects the MIP’s local optimality structure. This labeling
mechanism is inspiring especially for difﬁcult MIP instances
when solving them to optimality is almost impossible.

The overall framework of solution prediction based MIP

solving can be summarized as follows:
Training data generation: For a certain type of CO
problem, generate a set of p MIP instances I = {I1, . . . , Ip}
of similar scale from the same distribution D. For each
Ii ∈ I, collect variable features, constraint features, and
edge features, and use the iterated proximity search method
to generate solution labels for each binary variable in Ii.
GCN model training: For each Ii ∈ I, generate a tripartite
graph from its MIP formulation. Train a Graph Convolutional
Network (GCN) for binary variable solution prediction based
on the collected features, labels and trigraphs.
Application of solution prediction: For a new MIP instance
I from D, collect features, build the trigraph and use the GCN
model to make solution value predictions, based on which an
initial local branching cut heuristic (or a root branching exact
approach) is applied to solve I.

Related work
With similar motivation, there are some recent attempts that
consider the integration of ML and OR for solving CO prob-
lems. [Dai et al., 2017] combined reinforcement learning
and graph embedding to learn greedy construction heuristics
for several optimization problems over graphs. [Li, Chen,
and Koltun, 2018] trained a graph convolutional network to
estimate the likelihood that a vertex in a graph appears in
the optimal solution. [Selsam et al., 2018] proposed a mes-
sage passing neural network named NeuroSAT to solve SAT
problems via a supervised learning framework. [Vinyals, For-
tunato, and Jaitly, 2015], [Kool, van Hoof, and Welling, 2018;
Kool and Welling, 2018] and [Nazari et al., 2018] trained
Pointer Networks (or its variants) with recurrent neural net-
work (RNN) decoder to solve permutation related optimiza-
tion problems over graphs, such as Traveling Salesman Prob-
lem (TSP) and Vehicle Routing Problem (VRP). Different
from their settings, our solution prediction framework does
not restrict to certain graph-based problems but can adapt to
a variety of CO problems using a standard MIP formulation.
Quite related to our work, there is an increasing concern in
using ML techniques to enhance MIP solving performance.
[Alvarez, Louveaux, and Wehenkel, 2017], [Alvarez, We-
henkel, and Louveaux, 2016], [Khalil et al., 2016] tried to

use learning-based approaches to imitate the behavior of the
so-called strong branching method, a node-efﬁcient but time-
consuming branching variable selection method in the B&B
search tree. In a very recent work by [Gasse et al., 2019], a
GCN model is trained to imitate the strong branching rule.
Our model is different from theirs in terms of both the graph
and network structure as well as the application scenario of
the prediction results. [Tang, Agrawal, and Faenza, 2019]
designed a deep reinforcement learning framework for intel-
ligent selection of cutting planes. [He, Daume III, and Eisner,
2014] used imitation learning to train a node selection and a
node pruning policy to speed up the tree search in the B&B
process. [Khalil et al., 2017] used binary classiﬁcation to
predict whether a primal heuristic will succeed at a given
node and then decide whether to run a heuristic at that node.
[Kruber, Lübbecke, and Parmentier, 2017] proposed a su-
pervised learning method to decide whether a Danzig-Wolfe
reformulation should be applied and which decomposition
to choose among all possibles. Interested readers can refer
to [Bengio, Lodi, and Prouvost, 2018] for a comprehensive
survey on the use of machine learning methods in CO.

The proposed MIP solving framework is different from

previous work in two aspects:
Generalization: Previous solution generation method for CO
usually focus on problems with certain solution structures.
For example, applications of Pointer Networks [Vinyals, For-
tunato, and Jaitly, 2015; Kool and Welling, 2018] are only
suited for sequence-based solution encoding, and reinforce-
ment learning [Dai et al., 2017; Li, Chen, and Koltun, 2018]
type decision making is based on the assumption that a fea-
sible solution can be obtained by sequential decisions. In
contrast, the proposed framework does not limit to problems
of certain types but applies to most CO problems that can be
modeled as MIPs. This greatly enlarges the applicable area
of the proposed framework.
Representation: Previous applications of ML techniques to
enhance MIP solving performance mainly use hand-crafted
features, and make predictions on each variable indepen-
dently. Notice that the solution value of a variable is strongly
correlated to the objective function and the constraints it
participates in, we build a tripartite graph representation for
MIP, based on which graph embedding technique is applied
to extract correlations among variables, constraints and the
objective function without human intervention.

The Solution Framework
Consider an MIP problem instance I of the general form:

min cT x

s.t. Ax ≤ b,

xj ∈ {0, 1}, ∀j ∈ B,
xj ∈ Z, ∀j ∈ Q, xj ≥ 0, ∀j ∈ P,

(1)
(2)
(3)
(4)

where the index set of decision variables U := {1, . . . , n} is
partitioned into (B, Q, P), and B, Q, P are the index set of
binary, general integer and continuous variables, respectively.
The main task here is to predict the probability that a binary
variable xj, j ∈ B to take value 1 (or zero) in the optimal

solution. Next, we describe in detail the tripartite graph repre-
sentation of MIP, the GCN model structure, and how solution
prediction results are incorporated to accelerate MIP solving.

Graph Representation for MIP
Our main idea is to use a tripartite graph G = {V, E} to repre-
sent an input MIP instance I. In particular, objective function
coefﬁcients c, constraint right-hand-side (RHS) coefﬁcients
b and coefﬁcient matrix A information is extracted from I to
build the graph. Vertices and edges in the graph are detailed
as follows and graphically illustrated in Fig 1.

two fully-connected layers between variable nodes and the
output layer (line 13). The sigmoid activation function is
used for output so that the output value can be regarded as
the probability that the corresponding binary variable takes
value 1 in the MIP solution. The overall GCN is trained by
minimizing the binary cross-entropy loss.

Algorithm 1 Graph Convolutional Network (forward propa-
gation)

Input: Graph G = {V, E}; Input features {xj, ∀j ∈ V}; Num-
ber of transition iterations T ; Weight matrices Wt, ∀t ∈
{1, . . . , T } for graph embedding; Output layer weight matrix
Wout; Non-linearity σ (the relu function); Non-linearity σs
(the sigmoid function); Neighborhood function N ; Attention
coefﬁcients α.

Figure 1: Transforming an MIP instance to a tripartiete graph

Vertices:

1) the set of decision variable vertices VV , each of which

corresponds to a binary variable in I.

2) the set of constraint vertices VC, each of which

corresponds to a constraint in I.

3) an objective function vertex o.

Edges:

1) v-c edge: there exists an edge between v ∈ VV and
c ∈ VC if the corresponding variable of v has a non-zero
coefﬁcient in the corresponding constraint of c in the MIP
formulation.

2) v-o edge: for each v ∈ VV , there exists an edge between

v and o.

3) c-o edge: for each c ∈ VC, there exists an edge between

c and o.

Remark. The presented trigraph representation not only cap-
tures connections among the variables, constraints and objec-
tive functions but maintains the detailed coefﬁcients numerics
in its structure as well. In particular, non-zero entries in coef-
ﬁcient matrix A are included as features of v-c edges, entries
in objective coefﬁcients c as features of v-o edges, and en-
tries in b as features of c-o edges. Note that the constraint
RHS coefﬁcients b are correlated to the objective function by
viewing LP relaxation of I from a dual perspective.

Solution Prediction for MIP
We describe in Algorithm 1 the overall forward propagation
prediction procedure based on the trigraph. The procedure
consists of three stages: 1) a fully-connected “EMBEDDING”
layer with 64 dimension output for each node so that the
node representations are of the same dimension (lines 1-3
in the algorithm). 2) a graph attention network to transform
node information among connected nodes (lines 4-12). 3)

(cid:1)(cid:17)

(cid:1)(cid:17)

, (cid:80)
v∈VV ∩N (o)
(cid:1) (cid:17)

v ← EMBEDDING(xv), ∀v ∈ VV
c ← EMBEDDING(xc), ∀c ∈ VC
o ← EMBEDDING(xo)

Output: Predicted value of binary variables: zv, ∀v ∈ VV .
1: h0
2: h0
3: h0
4: for t = 1, . . . , T do
o ← σ
5:

V o·CONCAT(cid:0)ht−1

Wt

ht

(cid:16)

o

αvoht−1

v

6:
7:

8:

9:

10:
11:

12:

for c in C do :
o ← σ

ht

(cid:16)

Wt

(cid:16)

ht

c ← σ

Wt

oC · CONCAT (cid:0)ht
V c ·CONCAT(cid:0)ht

o, ht−1
c
o, (cid:80)

αvcht−1

v
(cid:1)(cid:17)

v∈VV ∩N (c)

(cid:16)

ht

o ← σ

Wt

Co · CONCAT(cid:0)ht

o, (cid:80)

αcoht
c

for v in V do :
o ← σ

ht

(cid:16)

Wt

(cid:16)

ht

v ← σ

Wt

oV · CONCAT (cid:0)ht
Cv · CONCAT(cid:0)ht

c∈VC ∩N (o)
(cid:1) (cid:17)

o, ht−1
v
o, (cid:80)

(cid:1)(cid:17)

αcvht
c

13: zv ← σs

(cid:0)Wout · CONCAT (cid:0)h0

v, hT
v

c∈VC ∩N (v)

(cid:1)(cid:1) , ∀v ∈ VV

Nodes’ representations in the tripartite graph are updated
via a 4-step procedure. In the ﬁrst step (line 5 in Algorithm
1), the objective node o aggregates the representations of all
variable nodes {hv, v ∈ VV } to update its representation ho.
The “CONCAT” operation represents the CONCATENATE
function that joins two arrays. In the second step (lines 6-8),
{hv, v ∈ VV } and ho are used to update the representations
of their neighboring constraint node c ∈ VC. In the third
step (line 9), representations of constraints {hc, c ∈ VC} are
aggregated to update ho, while in the fourth step (lines 10-12),
{hc, c ∈ VC} and ho are combined to update {hv, v ∈ VV }.
See Fig. 2 for an illustration of information transition ﬂow in
the trigraph. After T transitions, two fully-connected layers
coupled with a sigmoid activation function σs is used for
solution value prediction of each v ∈ VV (line 13).

The intuition behind Algorithm 1 is that at each iteration,
a variable node incrementally gathers more aggregation in-
formation from its neighboring nodes, which correspond to
the related constraints and variables in the MIP formulation.
It is worth mentioning that these transitions only extract
connection relations among the nodes, ignoring the detailed
coefﬁcients numerics A, b and c. To enhance the representa-
tion ability of our model, we include an attention mechanism
to import information from the coefﬁcients’ values.

ConsConsConsVarObjVarVarConsminc1x1+c2x2+...+cnxna11x1+a12x2+...+a1nxn≤b1am1x1+am2x2+...+amnxn≤bm...VarVarVarObjVarVarVarObjConsConsEmbeddingConsVarObjVarVarConsConsVarObjVarVarConsConsVarObjVarVarConsConsVarObjVarVarConsGraph Convolution LayerEmbeddingEmbeddingVarVarVarDense LayerOutputsConsVarObjVarVarConsConsVarObjVarVarConsConsVarObjVarVarConsConsVarObjVarVarConsStep 1Step 2Step 3Step 4ConsConsConsVarObjVarVarConsminc1x1+c2x2+...+cnxna11x1+a12x2+...+a1nxn≤b1am1x1+am2x2+...+amnxn≤bm...VarVarVarObjFigure 2: Information transition ﬂow in the trigraph convolu-
tional layer

The information transitions run consecutively as follows: Step 1, transform variable nodes informa-
tion to the objective node; Step 2, transform the objective and variable nodes information to con-
straint nodes; Step 3, transform constraint nodes information to the objective node; Step.4, transform
the objective node and constraint nodes information to variable nodes;

Attention Mechanism: A distinct feature in the tripartite
graph structure is the heterogeneities in nodes and arcs.
Rather than using a shared linear transformation (i.e., weight
matrix) for all nodes [Veliˇckovi´c et al., 2017], we consider
different transformations in each step of graph embedding
updates, reﬂecting the importance of feature of one type of
nodes on the other. In particular, given node i of type Ti and
node j of type Tj, the attention coefﬁcient which indicates
the importance of node i ∈ V from its neighbor j ∈ N (i) is
computed as:

αij = σs

(cid:16)

Watt

Ti,Tj

· CONCAT (cid:0)hi, heij , hj

(cid:1)(cid:17)

,

(5)

where hi, hj, heij are embeddings of node i, j ∈ V and edge
(i, j) ∈ E respectively, σs is the sigmoid activation function,
and Watt
Ti,Tj is the attention weight matrix between type Ti
and Tj nodes. For each i ∈ V, the attention coefﬁcient is
normalized cross over all neighbor nodes j ∈ N (i) using a
softmax function. With this mechanism, coefﬁcients infor-
mation in A, b and c (all of which contained in the feature
vector of the edges) are incorporated to reﬂect edge connec-
tion importance in the graph.

Prediction-Based MIP Solving
Next, we introduce how the solution value prediction results
are utilized to improve MIP solving performance. One ap-
proach is to add a local branching [Fischetti and Lodi, 2003]
type (invalid) global cut to the MIP model to reduce the
search space of feasible solutions. This method aims to iden-
tify decision variables that are predictable and stable, and
restrict the B&B tree search on unpredictable variables to
accelerate primal solution-ﬁnding. An alternative approach is
to perform an actual branching at the root node that maintains
global optimality. These two methods are detailed as follows.
a) Approximate approach. Let ˆxj denote the predicted
solution value of binary variable xj, j ∈ B, and let S ⊆
B denote a subset of indices of binary variables. A local
branching initial cut to the model is deﬁned as:

∆(x, ˆx, S) =

(cid:88)

xj +

(cid:88)

(1 − xj) ≤ φ,

(6)

j∈S:ˆxk

j =0

j∈S:ˆxk

j =1

where φ is a problem parameter that controls the maximum
distance from a new solution x to the predicted solution ˆx.
Adding cuts with respect to subset S rather than B is due to

the unpredictable nature of unstable variables in MIP solu-
tions. Therefore, only those variables with high probability to
take value 0 or 1 are included in S. For the extreme case that
φ equals 0, the initial cut is equivalent to ﬁxing variables with
indices in S at their predicted values. It is worth mentioning
that the inclusion of global constraint (6) shrinks the feasible
solution region of the original model, trading optimality for
speed as an approximate approach.

b) Exact approach. The proposed local branching type cut
can also be incorporated in an exact solver by branching on
the root node. To do this, we create two child nodes from the
root as follows:

Left: ∆(x, ˆx, S) ≤ φ,

Right: ∆(x, ˆx, S) ≥ φ + 1,

which preserves all feasible solution regions in the tree search.
After the root node branching, we switch back to the solver’s
default setting and perform an exact B&B tree search process.

Data Collection
Features: An ideal feature collection procedure should cap-
ture sufﬁcient information to describe the solution process,
and being of low computational complexity as well. A good
trade-off between these two concerns is to collect features at
the root node of the B&B tree, where the problem has been
presolved to eliminate redundant variables and constraints
and the LP relaxation is solved. In particular, we collect for
each instance 3 types of features: variable features, constraint
features, and edge features. Features descriptions are summa-
rized in Table 1 in Appendix A.

As presented in the feature table, features of vari-
ables and constraints can be divided into three cat-
egories: basic features, LP features,
and structure
features. The structure features (most of which can
be found in [Alvarez, Louveaux, and Wehenkel, 2017;
Khalil et al., 2016]) are usually hand-crafted statistics to
reﬂect correlations between variables and constraints. It is
noticeable that our tripartite graph neural network model can
naturally capture these correlations without human expertise
and can generate more advanced structure information to
improve prediction accuracy. This will be veriﬁed in the
computational evaluations section.

Labels: To make predictions on solution values of binary
variables, an intuitive labeling scheme for the variables is to
label them with the optimal solution values. Note, however,
obtaining optimal solutions for medium or large scale MIP
instances can be very time-consuming or even an impossible
task. This implies that labeling with optimal solutions can
only be applied to solvable MIP instances, which limits the
applications of the proposed framework.

In the situation when optimal solutions are difﬁcult to
obtain, we propose to identify stable and unstable variables
in solutions. This is motivated by the observation that solution
values of the majority of binary decision variables remain
unchanged across a series of different feasible solutions. To
be speciﬁc, given a set of K solutions {¯x1, . . . , ¯xK} to an
MIP instance I, a binary variable xj is deﬁned as unstable
if there exists some k1, k2 ∈ {1, . . . , K} such that xk1
(cid:54)=
j

ConsConsConsVarObjVarVarConsminc1x1+c2x2+...+cnxna11x1+a12x2+...+a1nxn≤b1am1x1+am2x2+...+amnxn≤bm...VarVarVarObjVarVarVarObjConsConsEmbeddingConsVarObjVarVarConsConsVarObjVarVarConsConsVarObjVarVarConsConsVarObjVarVarConsGraph Convolution LayerEmbeddingEmbeddingVarVarVarDense LayerOutputsConsVarObjVarVarConsConsVarObjVarVarConsConsVarObjVarVarConsConsVarObjVarVarConsStep 1Step 2Step 3Step 4ConsConsConsVarObjVarVarConsminc1x1+c2x2+...+cnxna11x1+a12x2+...+a1nxn≤b1am1x1+am2x2+...+amnxn≤bm...VarVarVarObjxk2
j , and as stable otherwise. Although obtaining optimal
solutions might be a difﬁcult task, the stable variables can
be viewed as an easy-to-predict part in the (near) optimal
solutions. To generate a sequence of solutions to an instance,
we use the proximity search method [Fischetti and Monaci,
2014]. Starting from some initial solution ¯xk with objective
value cT ¯xk, a neighborhood solution with the objective value
improvement being at least δ can be generated by solving the
following optimization:

min

(cid:88)

xj +

(cid:88)

(1 − xj)

(7)

j∈B:¯xk

j =0
s.t. cT x ≤ cT ¯xk − δ,

j∈B:¯xk

j =1

Ax ≤ b,
xj ∈ {0, 1}, ∀j ∈ B,
xj ∈ Z, ∀j ∈ G; xj ≥ 0, ∀j ∈ C,

(8)
(9)
(10)
(11)

where the objective function represents the distance between
¯xk and a new solution x. Note that the above optimization is
computationally tractable since solving process can terminate
as soon as a feasible solution is found. By iteratively apply-
ing this method, we obtain a series of improving feasible
solutions to the original problem. Stable binary variables are
labeled with their solution values while the unstable variables
are marked as unstable and discarded from the training set. A
limitation of this labeling method is the inability of handling
the case when the initial feasible solution ¯x0 is hard to obtain.
Remark. The logic behind the stable variable labeling
scheme is to explore local optimality patterns when global
optimality is not accessible. In each iteration of proximity
search, a neighboring better solution is found, with a few
ﬂips on solution values of the binary variables. Performing
this local search step for many rounds can identify local min-
imum patterns which reﬂect domain knowledge of the CO
problem. Take the Traveling Salesman Problem (TSP) as an
example. Let zjl deﬁne whether node l is visited immediately
after node j. If j and l are geometrically far away from each
other, zjl is likely to be zero in all the solutions generated by
proximity search and being recorded by our labeling scheme,
reﬂecting the underlying local optimality knowledge for TSP.

Experimental Evaluations
Setup. To evaluate the proposed framework, we modify
the state-of-the-art open-source MIP solver SCIP (version
6.0.1) for data collection and solution quality comparison.
The GCN model is built using the Tensorﬂow API. All
experiments were conducted on a cluster of three 4-core
machines with Intel 2.2 GHz processors and 16 GB RAM.
Instances. To test the effectiveness and generality of the
prediction-based solution framework, we generate MIP
instances of 8 distinct types: Fixed Charge Network Flow
(FCNF), Capacitated Facility Location (CFL), Generalized
Assignment (GA), Maximal Independent Set (MIS), Multidi-
mensional Knapsack (MK), Set Covering (SC), Traveling
Salesman Problem (TSP) and Vehicle Routing Problem
(VRP). These problems are the most commonly visited
NP-hard combinatorial optimizations in OR and are quite

general because they differ signiﬁcantly in MIP structures
and solution structures. For each problem type, 200 MIP
instances of similar scales are generated. The number of
instances used for training, validation, and testing is 140, 20,
40 respectively. Parameter calibrations are performed on the
validation set, while prediction accuracy and solution quality
comparisons are evaluated on the test instances. Detailed
MIP formulation and instance parameters of each type are
included in Appendix B.

Data collection. In terms of feature collection, we im-
plemented a feature extraction plugin embedded in the
branching procedure of SCIP. In particular, variable features,
constraint features, and edge features are collected right
before the ﬁrst branching decision is made at the root node,
where the presolving process, root LP relaxation, and root
cutting plane have completed. No further exploration of the
B&B search tree is needed and thus the feature collection
process terminates at the root node. Construction of the
tripartite graph is also completed at the root node where
SCIP is working with a transformed MIP model such that
redundant variables and constraints have been removed.
In terms of label collection, since optimal solutions to all
problem types can not be obtained within a 10000 seconds
time limit, we applied the proximity search method to label
stable binary variables. The initial solution ¯x0 for proximity
search is obtained under SCIP’s default setting with a 300
seconds execution time limit. If SCIP fails to obtain an initial
feasible solution within the time limit, the time limit doubles
until a feasible initial solution can be found. Parameter δ for
the proximity search is set as 0.01 · (cT ¯x0 − LB) where LB
is the lower bound objective value given by the solver. Each
proximity search iteration terminates as soon as a feasible
solution is found. This process generally converges within
20 to 40 iterations.

Parameter calibration. The performance of the proposed
framework beneﬁts from a proper selection of hyper-
parameters φ and S. Let zj denote the prediction probability
that binary variable xj, j ∈ B takes value 1 in the MIP solu-
tion. We sort xj in non-decreasing order of min(zj, 1 − zj)
and choose the ﬁrst η · |B| variables as S. The strategy of tun-
ing φ ∈ {0, 5, 10, 15, 20} and η ∈ {0.8, 0.9, 0.95, 0.99, 1}
is grid search, where the combination of φ and η that results
in best solution quality on the validation instances is selected.
Table 1 summarizes φ and η selections for each problem type.

Table 1: Hyper-parameter selections for each problem type

FCNF CFL

GA MIS MK

SC

TSP VRP

φ
η

0
0.80

0
0.95

5
0.99

10
0.90

10
0.80

0
0.90

0
0.90

5
0.95

Results of Solution Prediction
We demonstrate the effectiveness of the proposed GCN model
on prediction accuracy against the XGBoost (XGB) classiﬁer
[Chen and Guestrin, 2016]. For XGB, only variable features
are used for prediction since it can not process the trigraph
information. Noting that solution values of binary variables

are usually highly imbalanced, we use the average precision
(AP) metric [Zhu, 2004] to evaluate the performance of the
classiﬁers. In particular, the AP value is deﬁned as:

AP =

n
(cid:88)

k=1

P (k)∆r(k),

(12)

where k is the rank in the sequence of predicted variables,
P (k) is the precision at cut-off k in the list, and ∆r(k) is the
difference in recall from k − 1 to k.

Table 2: Comparisons on the average precision metric

Basic

Basic&structure

All

Instances XGB

GCN

XGB

FCNF
CFL
GA
MIS
MK
SC
TSP
VRP

0.099
0.449
0.499
0.282
0.524
0.747
0.327
0.391

0.261
0.590
0.744
0.355
0.840
0.748
0.358
0.403

0.275
0.567
0.750
0.289
0.808
0.748
0.349
0.420

Average

0.415

0.537

0.526

GCN

0.317
0.629
0.797
0.337
0.843
0.753
0.353
0.424

0.556

XGB

GCN

0.787
0.846
0.936
0.297
0.924
0.959
0.401
0.437

0.788
0.850
0.937
0.325
0.927
0.959
0.413
0.459

0.698

0.707

Table 2 describes the AP value comparison results for
the two classiﬁers under three settings: using only basic fea-
tures, using basic&structure features, and using all features
respectively. It is observed that the proposed GCN model
outperforms the baseline classiﬁer in all settings. The per-
formance advantage is particularly signiﬁcant in the basic
feature columns (0.537 by GCN against 0.415 by XGB),
where only raw coefﬁcient numerics in MIP are used for
prediction. The other notable statistic in the table is that the
GCN model with only basic features is on average superior to
the XGB classiﬁer with basic&structure features, indicating
that the proposed embedding framework can extract more
information compared to hand-crafted structure features used
in the literature [Alvarez, Louveaux, and Wehenkel, 2017;
Khalil et al., 2016]. For comparisons in the all features col-
umn, the advantage of GCN is less signiﬁcant due to the
reason that high-level MIP structure information is also cap-
tured in its LP relaxations.

Figure 3: Accuracy under different prediction percentage

To help illustrate the predictability in solution values for
problems of different types, we present in Fig.3 the detailed

accuracy curve for each of the 8 problem types using the
GCN model with all features. The ﬁgure depicts the pre-
diction accuracy if we only predict a certain percentage of
the most predictable binary variables2. It can be observed
from the ﬁgure that solution values of most considered MIP
problems (such as FCNF, GA, MK, SC,TSP) are fairly pre-
dictable, with an almost 100% accuracy if we make solution
value prediction on the top 0-80% most predictable variables.
Among the tested problem types, solution values to MIS and
VRP problem instances are fairly unpredictable, which is con-
sistent with the hyper-parameter calibration outcomes that φ
takes value greater than 0.

Comparisons of solution quality
a) Approximate approach: To evaluate the value of incor-
porating the (invalid) global cut in MIP solving, we compare
the performance of prediction-based approximate approach
against that of the solver’s aggressive heuristics setting3. To
be speciﬁc, we modiﬁed SCIP’s setting to “set heuristics
emphasis aggressive” to make SCIP focus on solution ﬁnd-
ing rather than proving optimality. For each problem type,
10 MIP instances are randomly selected from the 40 testing
instances for solution quality comparisons.

Notice that the proposed approximate approach does not
guarantee global optimality (i.e., does not provide a valid
lower bound), we use the primal gap metric [Khalil et al.,
2017] to capture solver’s performance on primal solution
ﬁnding. In particular, the primal gap metric γ(˜x) reports the
relative gap in the objective value of a feasible solution ˜x to
that of the optimal (or best-known) solution ˜x∗:

γ(˜x) =

|cT ˜x − cT ˜x∗|
max{|cT ˜x|, |cT ˜x∗|} + (cid:15)

× 100%,

(13)

where (cid:15) = 10−10 is a small constant to avoid numerical error
when max{|cT ˜x|, |cT ˜x∗|} = 0. Since all problem types are
not solvable within a 10000 seconds time limit (under the
SCIP’s default setting without the global cut), ˜x∗ is selected
as the best-known solution found by all tested methods.

Table 3 is a collection of solution quality results by the pro-
posed method with a 1000 seconds execution time limit. To
demonstrate the signiﬁcance of performance improvement,
we allow the solver to run for 2-10 times the execution time
(i.e., 2000-10000 seconds) in aggressive heuristics setting.
It is revealed from the table that the proposed approximate
method (the GCN-A column) gains remarkable advantages to
the SCIP’s aggressive heuristics setting under the same time
limit (the AGG1 column) on all testing problems. Compared
to the setting with 10000 seconds (the AGG10 column), the
proposed framework still maintains an average better perfor-
mance, indicating a 10 times acceleration in solution-ﬁnding,
with the trade of optimality guarantees.

2The predictability of a binary variable xj is measured by

max(zj, 1 − zj).

3As far as we know, there are hardly any stable approximate
solvers for MIP and “set heuristics emphasis aggressive” is the most
relevant setting we ﬁnd to accelerate primal solution-ﬁnding in the
SCIP’s documentation. Therefore we use this setting as a benchmark
for comparison.

Table 3: Primal gap comparisons for the approximate ap-
proach (%)

optimal solution, and ﬁnally prove optimality by exploring
the remaining nodes on both sides.

AGG1* AGG2 AGG5 AGG10 GCN-A

FCNF
CFL
GA
MIS
MK
SC
TSP
VRP

1.733
2.334
0.451
10.292
0.003
1.509
10.387
3.096

1.733
2.112
0.430
6.125
0.003
0.529
6.286
3.096

1.678
0.868
0.430
5.083
0.000
0.383
2.752
1.177

0.380
0.206
0.430
5.083
0.000
0.000
1.981
1.131

0.000
0.092
0.000
1.042
0.003
0.164
0.332
0.896

3.726

Average
* AGG1 represents SCIP’s aggressive heuristics setting with 1 × 1000 seconds execution time
limit. Similarly, AGG2, AGG5 and AGG10 correspond to aggressive heuristics setting with
2 × 1000, 5 × 1000 and 10 × 1000 seconds time limit respectively.

1.151

1.546

2.539

0.316

b) Exact approach To evaluate the value of performing an
actual branching at the root, we compare the performance
of the prediction-based exact approach against that of the
solver’s default setting with a 1000 seconds time limit. Be-
cause the exact approach provides a valid lower bound to the
original problem, we use the well-known optimality gap met-
ric ζ(˜x, LB) to measure the overall MIP solving performance:

ζ(˜x, LB) =

|cT ˜x − LB|
|cT ˜x| + (cid:15)

× 100%,

(14)

where ˜x and LB denote respectively the primal solution and
best lower bound obtained by a speciﬁc method.

Table 4: Optimality gap comparisons for the exact approach
(%)

FCNF CFL GA MIS MK SC TSP

VRP

DEF
GCN-E

7.06
4.78

3.96
3.64

5.30 62.92 2.01 12.09 12.13 68.69
4.31 61.75 2.00 11.19 12.84 68.13

We conclude the experimental results in Table 4. The DEF
row corresponds to SCIP’s default setting and GCN-E corre-
sponds to the exact approach using the new root branching
rule. It is revealed from the table that GCN-E outperforms
DEF in terms of the optimality gap within the time limit.
This provides empirical evidence that the proposed method is
potentially useful for accelerating MIP to global optimality.

Figure 4: Visualization of the B&B tree after performing a
root branching based on GCN prediction.

To help understand how the new branching rule acceler-
ates the MIP solving process, we plot the B&B tree of a
small-scale “CFL” instance in Fig. 4. It is observed from
the tree search process that the solver ﬁnds a good solution
quickly on the left tree, and goes to the right tree to get the

Generalization to larger instances
The graph embedding framework endows the model to train
and test on MIP instances of different scales. This is impor-
tant for MIP solving since there is hardly any good strategy
to handle large-scale NP-hard MIP problems. To investigate
this, we generate 200 small-scale MIP instances for each
problem type and train our GCN model on these instances
and test its applicability in large-scale ones. Detailed statis-
tics of small and large instances are reported in Appendix B.
It is revealed from table 5 that the GCN model maintains an
acceptable prediction accuracy degradation when the prob-
lem scale differs in the training and testing phase. Besides,
the prediction result is still useful to improve solver’s primal
solution ﬁnding performance.

Table 5: Generalization ability of the proposed framework

Average precision

Primal gap (%)

GCN

GCNG*

AGG1 GCN-A GCNG-A

FCNF
CFL
GA
MIS
MK
SC
TSP
VRP
Average
* GCNG is the GCN model trained on small-scale MIP instances. GCNG-A is the approximate

1.733
2.341
0.661
2.223
0.000
1.349
10.061
4.408
2.847

0.653
0.837
0.963
0.091
0.789
0.878
0.396
0.358
0.621

0.675
0.801
0.873
0.104
0.786
0.843
0.343
0.321
0.593

2.119
0.410
0.016
1.087
0.000
0.215
4.802
1.239
1.236

0.000
0.100
0.211
1.136
0.000
0.000
0.000
1.254
0.338

solving approach based on the GCNG prediction model.

Conclusions
We presented a supervised solution prediction framework
to explore the correlations between the MIP formulation
structure and its local optimality patterns. The key feature of
the model is a tripartite graph representation for MIP, based
on which graph embedding is used to extract connection
information among variables, constraints and the objective
function. Through extensive experimental evaluations on 8
types of distinct MIP problems, we demonstrate the effective-
ness and generality of the GCN model in prediction accuracy.
Incorporated in a global cut to the MIP model, the predic-
tion results help to accelerate SCIP’s solution-ﬁnding process
by 10 times on similar problems with a sacriﬁce in proving
global optimality. This result is inspiring to practitioners who
are facing routinely large-scale MIPs on which the solver’s
execution time is unacceptably long and tedious, while global
optimality is not a major concern.

Limitations of the proposed framework are two-fold. First,
this method is better being applied to binary variable intensive
MIP problems due to the difﬁculties in solution value pre-
diction for general integer variables. Second, the prediction
performance degrades for problems without local optimality
structure where correlations among variables from the global
view can not be obtained from the neighborhood information
reﬂected in the MIP’s trigraph representation.

[Khalil et al., 2017] Khalil, E. B.; Dilkina, B.; Nemhauser,
G. L.; Ahmed, S.; and Shao, Y. 2017. Learning to run heuris-
tics in tree search. In 26th International Joint Conference on
Artiﬁcial Intelligence (IJCAI).
[Kool and Welling, 2018] Kool, W., and Welling, M. 2018.
Attention solves your tsp. arXiv preprint arXiv:1803.08475.
[Kool, van Hoof, and Welling, 2018] Kool, W.; van Hoof, H.;
and Welling, M. 2018. Attention, learn to solve routing
problems! arXiv preprint arXiv:1803.08475.
[Kruber, Lübbecke, and Parmentier, 2017] Kruber,
M.;
Lübbecke, M. E.; and Parmentier, A. 2017. Learning
when to use a decomposition. In International Conference
on AI and OR Techniques in Constraint Programming for
Combinatorial Optimization Problems, 202–210. Springer.
[Laporte, 2009] Laporte, G. 2009. Fifty years of vehicle
routing. Transportation Science 43(4):408–416.
[Li, Chen, and Koltun, 2018] Li, Z.; Chen, Q.; and Koltun, V.
2018. Combinatorial optimization with graph convolutional
In Advances in Neural
networks and guided tree search.
Information Processing Systems, 537–546.
[miplib2017, 2018] 2018. MIPLIB 2017. http://miplib.zib.de.
[Nazari et al., 2018] Nazari, M.; Oroojlooy, A.; Snyder, L.;
and Takác, M. 2018. Reinforcement learning for solving the
vehicle routing problem. In Advances in Neural Information
Processing Systems, 9839–9849.
[Pinedo, 2012] Pinedo, M. 2012. Scheduling, volume 29.
Springer.
[Selsam et al., 2018] Selsam, D.; Lamm, M.; Bünz, B.; Liang,
2018. Learning a
P.; de Moura, L.; and Dill, D. L.
arXiv preprint
sat solver from single-bit supervision.
arXiv:1802.03685.
[Tang, Agrawal, and Faenza, 2019] Tang, Y.; Agrawal, S.;
2019. Reinforcement learning for in-
and Faenza, Y.
arXiv preprint
teger programming: Learning to cut.
arXiv:1906.04859.
[Veliˇckovi´c et al., 2017] Veliˇckovi´c,
P.; Cucurull, G.;
Casanova, A.; Romero, A.; Lio, P.; and Bengio, Y. 2017.
Graph attention networks. arXiv preprint arXiv:1710.10903.
[Vinyals, Fortunato, and Jaitly, 2015] Vinyals, O.; Fortunato,
M.; and Jaitly, N. 2015. Pointer networks. In Advances in
Neural Information Processing Systems, 2692–2700.
[Zhu, 2004] Zhu, M. 2004. Recall, precision and average
precision. Department of Statistics and Actuarial Science,
University of Waterloo, Waterloo 2:30.

References
[Alvarez, Louveaux, and Wehenkel, 2017] Alvarez, A. M.;
Louveaux, Q.; and Wehenkel, L. 2017. A machine learning-
based approximation of strong branching. INFORMS Journal
on Computing 29(1):185–195.
[Alvarez, Wehenkel, and Louveaux, 2016] Alvarez, A. M.;
Wehenkel, L.; and Louveaux, Q. 2016. Online learning
for strong branching approximation in branch-and-bound.
[Bengio, Lodi, and Prouvost, 2018] Bengio, Y.; Lodi, A.; and
Prouvost, A. 2018. Machine learning for combinatorial
optimization: a methodological tour d’horizon. arXiv preprint
arXiv:1811.06128.
[Chen and Guestrin, 2016] Chen, T., and Guestrin, C. 2016.
Xgboost: A scalable tree boosting system. In Proceedings of
the 22nd acm sigkdd international conference on knowledge
discovery and data mining, 785–794. ACM.
[Chen, 2010] Chen, Z.-L. 2010. Integrated production and
outbound distribution scheduling: review and extensions. Op-
erations research 58(1):130–148.
[Clarke and Wright, 1964] Clarke, G., and Wright, J. W.
1964. Scheduling of vehicles from a central depot to a number
of delivery points. Operations research 12(4):568–581.
[Cook, 2011] Cook, W. J. 2011. In pursuit of the traveling
salesman: mathematics at the limits of computation. Prince-
ton University Press.
[Dai et al., 2017] Dai, H.; Khalil, E.; Zhang, Y.; Dilkina, B.;
and Song, L. 2017. Learning combinatorial optimization
algorithms over graphs. In Advances in Neural Information
Processing Systems, 6348–6358.
[Dósa and Sgall, 2013] Dósa, G., and Sgall, J. 2013. First ﬁt
bin packing: A tight analysis. In 30th International Sympo-
sium on Theoretical Aspects of Computer Science (STACS
2013). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.
[Farahani and Hekmatfar, 2009] Farahani, R. Z., and Hekmat-
far, M. 2009. Facility location: concepts, models, algorithms
and case studies. Springer.
[Fischetti and Lodi, 2003] Fischetti, M., and Lodi, A. 2003.
Local branching. Mathematical programming 98(1-3):23–47.
[Fischetti and Monaci, 2014] Fischetti, M., and Monaci, M.
2014. Proximity search for 0-1 mixed-integer convex pro-
gramming. Journal of Heuristics 20(6):709–731.
[Gasse et al., 2019] Gasse, M.; Chételat, D.; Ferroni, N.;
Charlin, L.; and Lodi, A. 2019. Exact combinatorial op-
timization with graph convolutional neural networks. arXiv
preprint arXiv:1906.01629.
[Gopalakrishnan and Johnson, 2005] Gopalakrishnan,
B.,
and Johnson, E. L. 2005. Airline crew scheduling: state-of-
the-art. Annals of Operations Research 140(1):305–337.
[He, Daume III, and Eisner, 2014] He, H.; Daume III, H.; and
Eisner, J. M. 2014. Learning to search in branch and bound
algorithms. In Advances in neural information processing
systems, 3293–3301.
[Khalil et al., 2016] Khalil, E. B.; Le Bodic, P.; Song, L.;
Nemhauser, G. L.; and Dilkina, B. N. 2016. Learning to
branch in mixed integer programming. In AAAI, 724–731.

Appendix A
In this appendix, we describe in table 6 the variable node fea-
tures, constraint node features, and edge features in detail. All
the features are collected at the root node of the B&B search
tree where presolving and root LP relaxation has completed.

Appendix B
In this appendix, we describe the MIP formulation for each
of the 8 types of CO problems in the paper. Table 7 and 8
summarize the number of variables, constraints, percentage
of nonzeros in the coefﬁcient matrix A, and the percentage
of binary variables that takes value 1 in the optimal solution.

1) Fixed Charge Network Flow:
Consider a directed graph G = (V, E), where each node
v ∈ V has demand dv and the demand is balanced in the
graph: (cid:80)
v∈V = 0. The capacity of an arc e ∈ E is ue > 0
and the cost of an xe > 0 quantity ﬂow on this arc has a cost
fe + cexe.

Decision variables:

• ye: binary variable. ye = 1, if arc e ∈ E is used and

ye = 0 otherwise.

• xe: continuous variable, ﬂow quantity on arc e ∈ E.

Formulation:

min

(cid:88)

e∈E

(feye + cexe)

(15)

yij ≥ 0, ∀i ∈ {1, . . . , m}, j ∈ {1, . . . , n}
xj ∈ {0, 1}, ∀j ∈ {1, . . . , n}.

(22)
(23)

3) Generalized Assignment:

Suppose there are n tasks and m agents and we wish to assign
the tasks to agents to maximize total revenue. Let pij denote
the revenue of assigning task j to agent i and wij denote the
resource consumed, i ∈ {1, . . . , m}, j ∈ {1, . . . , n}. The
total resource of agent i is assumed to be ti.

Decision variables:

• xij: binary variable. xij = 1 if task j is assigned to

agent i, and xij = 0 otherwise.

Formulation:

max

m
(cid:88)

n
(cid:88)

pijxij

s.t.

i=1
n
(cid:88)

j=1
m
(cid:88)

j=1

wijxij ≤ ti, ∀i ∈ {1, . . . , m},

xij = 1, ∀j ∈ {1, . . . , n},

(24)

(25)

(26)

i=1
xij ∈ {0, 1}, ∀i ∈ {1, . . . , m}, j ∈ {1, . . . , n}.

(27)

s.t.

(cid:88)

−

(cid:88)

= dv, ∀v ∈ V,

(16)

4) Maximal Independent Set:

e∈E(v,V)

e∈E(V,v)
0 ≤ xe ≤ ueye, ∀e ∈ E,
ye ∈ {0, 1}, ∀e ∈ E.

(17)
(18)

2) Capacitated Facility Location:
Suppose there are m facilities and n customers and we wish
to satisfy the customers demand at minimum cost. Let fi
denote the ﬁxed cost of building facility i ∈ {1, . . . , m} and
cij the shipping cost of products from facility i to customer
j ∈ {1, . . . , n}. The demand of customer j is assumed to be
dj > 0 and the capacity of facility i is assumed to be ui > 0.

Decision variables:

• xj: binary variable. xj = 1 if facility j is built, and

xj = 0 otherwise.

• yij: continuous variable, the fraction of the demand dj

fulﬁlled by facility i.

Formulation:

m
(cid:88)

n
(cid:88)

cijyij +

j=1

m
(cid:88)

i=1

fixi

yij = 1, ∀j ∈ {1, . . . , n},

diyij ≤ uixi, ∀i ∈ {1, . . . , m}

min

s.t.

i=1
m
(cid:88)

i=1
n
(cid:88)

j=1

(19)

(20)

(21)

Consider an undirected graph G = (V, E), a subset of nodes
S ∈ V is called an independent set iff there is no edge
between any pair of nodes in S. The maximal independent
set problem is to ﬁnd an independent set in G of maximum
cardinality.

Decision variables:

• xv: binary variable. xv = 1 if node v ∈ V is is chosen

in the independent set, and 0 otherwise.

Formulation:

max

(cid:88)

v∈V

xv

s.t. xu + xv ≤ 1, ∀(u, v) ∈ E,
xv ∈ {0, 1}, ∀v ∈ V.

(28)

(29)
(30)

5) Multidimensional Knapsack:

Consider a knapsack problem of n items with m dimensional
capacity constraints. Let Wi denotes the capacity of the
i-th dimension in the knapsack, pj the proﬁt of packing
item j, and wij the size of item j in the i-th dimension,
i ∈ {1, . . . , m}, j ∈ {1, . . . , n}.

Decision variables:

• xj: binary variable. xj = 1 if item j is chosen in the

knapsack, and 0 otherwise.

Variable Features

Feature Description

count

Basic

LP

Structure

Constraint Features

Basic

LP

Structure

Edge Features
Basic

variable type (is binary, general integer)
objective function coefﬁcents (original, positive, negative)
number of non-zero coefﬁcient in the constraint.
number of up (down) locks.
LP value (xj, xj − (cid:98)xj(cid:99), (cid:100)xj(cid:101) − xj)
LP value is fractional
pseudocosts (upwards, downwards, ratio, sum, product)
global lower (upper) bound
reduced cost
degree statistics (mean, stdev, min, max) of constraints that the
variable has nonzero coefﬁcients.
maximum (minimum) ratio of positive (negative) LHS (RHS) value.
positive (negative) coefﬁcient statistics (count, mean, stdev,
min, max) of variables in the constraints.
coefﬁcient statistics of variables in the constraints (sum, mean, stdev,
max, min) with respect to three weighting schemes: unit weight, dual
cost, inverse of the coefﬁcients sum in the constraint.

constraint type (is singleton, aggregation, precedence, knapsack, logicor,
general linear, AND, OR, XOR, linking, cardinality, variable bound)
Left-hand-side (LHS) and right-hand-side (RHS)
number of nonzero (positive, negative) entries in the constraint
dual solution of the constraint
basis status of the constraint in the LP solution
sum norm of absolute (positive, negative) values of coefﬁcients
variable coefﬁcient statistics in the constraint (mean, stdev, min, max)

original edge coefﬁcient
normalized edge coefﬁcient

2
3
1
2
3
1
5
2
1

4

8

10

15

12

2
3
1
1
3
4

1
1

Table 6: Description of variable, constraint and edge features.

Table 7: Problem scale statistics for large-scale instances

Num. of
variables

Num. of
constraints

Fraction of nonzeros in
the coefﬁcient matrix

Fraction of nonzeros in
a feasible solution

FCNF
CFL
GA
MIS
MK
SC
TSP
VRP

[2398, 3568]
[28956, 28956]
[22400, 22400]
[400, 400]
[765, 842]
[4500, 4500]
[17689, 19600]
[1764, 1764]

[1402, 2078]
[29336, 29336]
[600, 600]
[19153, 19713]
[46, 51]
[3500, 3500]
[17954, 19879]
[1802, 1802]

0.00118
0.00014
0.00333
0.00502
1.00000
0.03000
0.00031
0.00314

0.02913
0.01358
0.02500
0.05815
0.02684
0.02602
0.00737
0.02963

Table 8: Problem scale statistics for small-scale instances

Num. of
variables

Num. of
constraints

Fraction of nonzeros in
the coefﬁcient matrix

Fraction of nonzeros in
a feasible solution

FCNF
CFL
GA
MIS
MK
SC
TSP
VRP

[1702, 2640]
[1212, 1212]
[1152, 1152]
[125, 125]
[315, 350]
[750, 750]
[1296, 1600]
[196, 196]

[996, 1533]
[1312, 1312]
[108, 108]
[1734, 1929]
[19, 21]
[550, 550]
[1367, 1679]
[206, 206]

0.00163
0.00303
0.01852
0.01620
1.00000
0.05000
0.00387
0.02422

0.03392
0.08624
0.08333
0.14572
0.07113
0.06533
0.02697
0.08069

Formulation:

n
(cid:88)

j=1
n
(cid:88)

max

s.t.

pjxj

wijxj ≤ Wi, ∀i ∈ {1, . . . , m},

j=1
xj ∈ {0, 1}, ∀j ∈ {1, . . . , n}.

(31)

(32)

(33)

6) Set Covering:

Given a ﬁnite set U and a collection of n subsets S1, . . . , Sn
of U, the set covering problem is to identify the fewest sets
of which the union is U.

Decision variables:

• xj: binary variable. xj = 1 if set j is chosen, and 0

8) Vehicle Routing Problem
Given a set of n customers, the vehicle routing prob-
lem is to ﬁnd the optimal set of routes to traverse in
order to fulﬁll the demand of customers. To serve the
customers, a ﬂeet of K vehicles, each of which has a maxi-
mum capacity Q are provided. Let cij denotes the distance
from customer i to customer j (i (cid:54)= j, i, j ∈ {0, . . . , n + 1}).

Decision variables:

• xij: binary variable. xij = 1 city j is visited immedi-
ately after city i by some vehicle, and 0 otherwise.
• yj: continuous variable, the cumulated demand on the

route that visits node j up to this visit.

Formulation:

min

s.t.

n+1
(cid:88)

n+1
(cid:88)

i=0

j=0

cijxij

n+1
(cid:88)

j=1,j(cid:54)=i

n
(cid:88)

xij = 1, ∀i ∈ {1, . . . , n},

(34)

n+1
(cid:88)

xih −

xhj = 0, ∀h ∈ {1, . . . , n},

otherwise.

Formulation:

n
(cid:88)

j=1

xj

(cid:88)

min

s.t.

xj ≥ 1, ∀v ∈ U,

(35)

i=0,i(cid:54)=h

j=1,j(cid:54)=h

j∈{1,...,n}|v∈Sj
xj ∈ {0, 1}, ∀j ∈ {1, . . . , n}.

(36)

n
(cid:88)

x0j ≤ K,

j=1
yj ≥ yi + qjxij − Q(1 − xij), ∀i, j ∈ {0, . . . , n + 1}

0 ≤ yi ≤ Q, ∀i ∈ {0, . . . , n + 1},
xij ∈ {0, 1}, ∀i, j ∈ {0, . . . , n + 1}.

(47)

(48)
(49)

7) Traveling Salesman Problem

Given a list of n cities, the traveling salesman problem
is to ﬁnd a shortest route to visit each city and returns to
the origin city. Let cij denotes the distance from city i to
city j (i (cid:54)= j, i, j ∈ {1, . . . , n}). We use the well-known
Miller-Tucker-Zemlin (MTZ) formulation to model the TSP.

Decision variables:

• xij: binary variable. xij = 1 city j is visited immedi-

ately after city i, and 0 otherwise.

• uj: continuous variable, indicating the that city j is the

uj-th visited city.

Formulation:

min

s.t.

n
(cid:88)

n
(cid:88)

i=1

j(cid:54)=i,j=1

cijxij

n
(cid:88)

i=1,i(cid:54)=j
n
(cid:88)

xij = 1, ∀j ∈ {1, . . . , n},

xij = 1, ∀i ∈ {1, . . . , n},

j=1,j(cid:54)=i
ui − uj + nxij ≤ n − 1, ∀2 ≤ i (cid:54)= j ≤ n,
0 ≤ ui ≤ n − 1, ∀i ∈ {2, . . . , n},
xij ∈ {0, 1}, ∀i, j ∈ {1, . . . , n}.

(37)

(38)

(39)

(40)
(41)
(42)

(43)

(44)

(45)

(46)

