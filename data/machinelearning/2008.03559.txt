Convex Q-Learning

Part 1: Deterministic Optimal Control

Prashant G. Mehta and Sean P. Meyn∗

Abstract

It is well known that the extension of Watkins’ algorithm to general function approximation settings
is challenging: does the “projected Bellman equation” have a solution? If so, is the solution useful in
the sense of generating a good policy? And, if the preceding questions are answered in the aﬃrmative,
is the algorithm consistent? These questions are unanswered even in the special case of Q-function
approximations that are linear in the parameter. The challenge seems paradoxical, given the long history
of convex analytic approaches to dynamic programming.

The paper begins with a brief survey of linear programming approaches to optimal control, leading to
a particular ‘over parameterization that lends itself to applications in reinforcement learning. The main
conclusions are summarized as follows:

(i) The new class of convex Q-learning algorithms is introduced based on the convex relaxation of the
Bellman equation. Convergence is established under general conditions, including a linear function
approximation for the Q-function.

(ii) A batch implementation appears similar to the famed DQN algorithm (one engine behind Alp-
haZero). It is shown that in fact the algorithms are very diﬀerent: while convex Q-learning solves
a convex program that approximates the Bellman equation, theory for DQN is no stronger than
for Watkins algorithm with function approximation: (a) it is shown that both seek solutions to the
same ﬁxed point equation, and (b) the “ODE approximations” for the two algorithms coincide,
and little is known about the stability of this ODE.

These results are obtained for deterministic nonlinear systems with total cost criterion. Many extensions
are proposed, including kernel implementation, and extension to MDP models.

Note: This pre-print is written in a tutorial style so it is accessible to new-comers. It will be a part of
a handout for upcoming short courses on RL. A more compact version suitable for journal submission is
in preparation.

0
2
0
2

g
u
A
8

]

C
O
.
h
t
a
m

[

1
v
9
5
5
3
0
.
8
0
0
2
:
v
i
X
r
a

∗PGM is with the Coordinated Science Laboratory and the Department of Mechanical Science and Engineering at the
University of Illinois at Urbana-Champaign (UIUC); SPM is with the Department of Electrical and Computer Engineering,
University of Florida, Gainesville, FL 32611, and Inria International Chair. Financial support from ARO award W911NF1810334
and National Science Foundation award EPCN 1935389 is gratefully acknowledged.

 
 
 
 
 
 
1

Introduction

This paper concerns design of reinforcement learning algorithms for nonlinear, deterministic state space
models. The setting is primarily deterministic systems in discrete time, where the main ideas are most easily
described.

Speciﬁcally, we consider a state space model with state space X, and input (or action) space U (the sets
X and U may be Euclidean space, ﬁnite sets, or something more exotic). The input and state are related
through the dynamical system

x(k + 1) = F(x(k), u(k)) ,

k ≥ 0 , x(0) ∈ X ,

(1)

where F : X × U → X. It is assumed that there is (xe, ue) ∈ X × U that achieves equilibrium:

xe = F(xe, ue)

The paper concerns inﬁnite-horizon optimal control, whose deﬁnition requires a cost function c : X → R+.
The cost function is non-negative, and vanishes at (xe, ue).

The (optimal) value function is denoted

J (cid:63)(x) = min
u

∞
(cid:88)

k=0

c(x(k), u(k)) ,

x(0) = x ∈ X ,

(2)

where the minimum is over all input sequences u := {u(k) : k ≥ 0}. The goal of optimal control is to ﬁnd an
optimizing input sequence, and in the process we often need to compute the value function J (cid:63). We settle for
an approximation in the majority of cases. In this paper the approximations will be based on Q-learning; it
is hoped that the main ideas will be useful in other formulations of reinforcement learning.

Background The dynamic programming equation associated with (2) is

J (cid:63)(x) = min
u

(cid:8)c(x, u) + J (cid:63)(F(x, u))(cid:9)

(3)

There may be state-dependent constraints, so the minimum is over a set U(x) ⊂ U. The function of two
variables within the minimum in (3) is the “Q-function” of reinforcement learning:

so that the Bellman equation is equivalent to

Q(cid:63)(x, u) := c(x, u) + J (cid:63)(F(x, u))

J (cid:63)(x) = min
u

Q(cid:63)(x, u)

From eqs. (4) and (5) we obtain a ﬁxed-point equation for the Q-function:

Q(cid:63)(x, u) = c(x, u) + Q(cid:63)(F(x, u))

(4)

(5)

(6)

with Q(x) = minu Q(x, u) for any function Q. The optimal input is state feedback u∗(k) = φ(cid:63)(x(cid:63)(k)), with

φ(cid:63)(x) ∈ arg min

u

Q(cid:63)(x, u) ,

x ∈ X

(7)

Temporal diﬀerence (TD) and Q-learning are two large families of reinforcement learning algorithms
based on approximating the value function or Q-function as a means to approximate φ(cid:63) [61, 9, 7]. Consider
a parameterized family {Qθ : θ ∈ Rd}; each a real-valued function on X × U. For example, the vector θ might
represent weights in a neural network. Q-learning algorithms are designed to approximate Q(cid:63) within this
parameterized family. Given the parameter estimate θ ∈ Rd, the “Qθ-greedy policy” is obtained:

φθ(x) = arg min

u

Qθ(x, u)

2

(8)

Ideally, we would like to ﬁnd an algorithm that ﬁnds a value of θ so that this best approximates the optimal
policy.

Most algorithms are based on the sample path interpretation of (6):

Q(cid:63)(x(k), u(k)) = c(x(k), u(k)) + Q(cid:63)(x(k + 1))

(9)

valid for any input-state sequence {u(k), x(k) : k ≥ 0}. Two general approaches to deﬁne θ∗ are each posed
in terms of the temporal diﬀerence:

Dk+1(θ) := −Qθ(x(k), u(k)) + c(x(k), u(k)) + Qθ(x(k + 1))

(10)

assumed to be observed on the ﬁnite time-horizon 0 ≤ k ≤ N .

(i) A gold standard loss function is the mean-square Bellman error associated with (10):

E ε(θ) =

1
N

N −1
(cid:88)

(cid:2)Dk+1(θ)(cid:3)2

k=0

(11)

and θ∗ is then deﬁned to be its global minimum. Computation of a global minimum is a challenge
since this function is not convex, even with {Qθ : θ ∈ Rd} linear in θ.

(ii) Watkins’ Q-learning algorithm, as well as the earlier TD methods of Sutton (see [62] for the early
origins), can be cast as a Galerkin relaxation of the Bellman equation: A sequence of d-dimensional
eligibility vectors {ζk} is constructed, and the goal then is to solve

0 =

1
N

N −1
(cid:88)

k=0

Dk+1(θ)ζk

(12)

The details of Watkins’ algorithm can be found in the aforementioned references, along with [25] (which
follows the notation and point of view of the present paper). We note here only that in the original algorithm
of Watkins,

(cid:78) The algorithm is deﬁned for ﬁnite state and action MDPs (Markov Decision Processes), and Q(cid:63) ∈

{Qθ : θ ∈ Rd} (the goal is to compute the Q-function exactly).

(cid:78) The approximation family is linear Qθ = θ(cid:124)ψ, with ψ : X × U → Rd.

(cid:78) ζk = ψ(x(k), u(k)).

Equation (12) is not how Q-learning algorithms are typically presented, but does at least approximate the
goal in many formulations. A limit point of Watkins’ algorithm, and generalizations such as [45, 40], solves
the “projected Bellman equation”:

f (θ∗) = 0 ,

f (θ) = E(cid:2)Dk+1(θ)ζk

(cid:3)

(13)

where the expectation is in steady-state (one assumption is the existence of a steady-state). The basic
extension of Watkins’ algorithm is deﬁned by the recursion

θn+1 = θn + αn+1Dn+1ζn

(14)

with {αn} the non-negative step-size sequence, and Dn+1 is short-hand for Dn+1(θn). The recursion (14) is
called Q(0)-learning for the special case ζn = ∇θQθ(x(n), u(n))(cid:12)
, in analogy with TD(0)-learning [25].
Criteria for convergence is typically cast within the theory of stochastic approximation, which is based on
the ODE (ordinary diﬀerential equation),

(cid:12)θ=θn

d
dt ϑt = f (ϑt) ,

ϑ0 ∈ Rd

(15)

Conditions for convergence of (14) or its ODE approximation (15) are very restrictive [45, 40].

3

While not obvious from its description, the DQN algorithm (a signiﬁcant component of famous applica-
tions such as AlphaGo) will converge to the same projected Bellman equation, provided it is convergent—see
Prop. 2.6 below.

This opens an obvious question: does (13) have a solution? Does the solution lead to a good policy?
An answer to the second question is wide open, despite the success in applications. The answer to the ﬁrst
question is, in general, no. The conditions imposed in [45] for a solution are very strong, and not easily
veriﬁed in any applications; the more recent work [40] oﬀers improvements, but nothing approaching a full
understanding of the algorithm.

The question of existence led to an entirely new approach in [42], based on the non-convex optimization

problem:

min
θ

J(θ) = min

θ

(cid:124)
1
2 f (θ)

M f (θ),

with M > 0

with f deﬁned in (13). Consider the continuous-time gradient descent algorithm associated with (16):

(cid:124)
d
dt ϑt = −[∂θf (ϑt)]

M f (ϑt)

(16)

(17)

The GQ-learning algorithm is a stochastic approximation (SA) translation of this ODE, using M = E[ζnζ (cid:124)
n]−1.
Convergence holds under conditions, such as a coercive condition on J (which is not easily veriﬁed a-priori).
Most important: does this algorithm lead to a good approximation of the Q-function, or the policy?

This brings us back to (ii): why approximate the solution of (13)? We do not have an answer. In this

paper we return to (11), for which we seek convex re-formulations.

Contributions. The apparent challenge with approximating the Q-function is that it solves a nonlinear
ﬁxed point equation (6) or (9), for which root ﬁnding problems for approximation may not be successful
(as counter examples show). This challenge seems paradoxical, given the long history of convex analytic
approaches to dynamic programming in both the MDP literature [43, 23, 11] and the linear optimal control
literature [63].

The starting point of this paper is to clarify this paradox, and from this create a new family of RL
algorithms designed to minimize a convex variant of the empirical mean-square error (11). This step was
anticipated in [44], for which a convex formulation of Q-learning was proposed for deterministic systems in
continuous time; in this paper we call this H-learning, motivated by commentary in the conclusions of [44].
The most important ingredient in H-learning is the creation of a convex program suitable for RL based on
an over-parameterization, in which the value function and Q-function are treated as separate variables.

A signiﬁcant failing of H-learning was the complexity of implementation in its ODE form, because of
implicit constraints on θ: complexity persists when using a standard Euler approximation to obtain a
recursive algorithm. In 2009, the authors believed that recursive algorithms were essential for the sake of
data eﬃciency. Motivation for the present work came from breakthroughs in empirical risk minimization
(ERM) appearing in the decade since [44] appeared, and the success of DQN algorithms that are based in
part on ERM.

The main contributions are summarized as follows:

(i) The linear programming (LP) approach to dynamic programming has an ancient history [43, 23, 11],
with applications to approximate dynamic programming beginning with de Farias’ thesis [20, 22]. The
“DPLP” (dynamic programming linear program) is introduced in Prop. 2.1 for deterministic control
systems, with generalizations to MDPs in Section 3.5. It is the over-parameterization that lends itself to
data driven RL algorithms for nonlinear control systems. The relationship with semi-deﬁnite programs
for LQR is made precise in Prop. 2.3.

(ii) Section 2.4 introduces new Q-learning algorithms inspired by the DPLP. Our current favorite is Batch
Convex Q-Learning (46): it appears to oﬀer the most ﬂexibility in terms of computational complexity
in the optimization stage of the algorithm. A casual reader might mistake (46) for the DQN algorithm.
The algorithms are very diﬀerent, as made clear in Propositions 2.4 and 2.6: the DQN algorithm cannot
solve the minimal mean-square Bellman error optimization problem. Rather, any limit of the algorithm
must solve the relaxation (13) (recall that there is little theory providing suﬃcient conditions for a
solution to this ﬁxed point equation).

The algorithms proposed here converge to entirely diﬀerent values:

4

(iii) As in [44], it is argued that there is no reason to introduce random noise for exploration for RL in
deterministic control applications. Rather, we opt for the quasi-stochastic approximation (QSA) ap-
proach of [44, 60, 6, 19]. Under mild conditions, including a linear function approximation architecture,
parameter estimates from the Batch Convex Q algorithm will converge to the solution to the quadratic
program

min
θ
s.t. E(cid:2)D◦

− (cid:104)µ, J θ(cid:105) + κεE(cid:2)(cid:8)D◦
k+1(θ)ζk(i)(cid:3) ≥ 0 ,

k+1(θ)(cid:9)2(cid:3)

i ≥ 1

(18)

where D◦

k+1(θ) denotes the observed Bellman error at time k + 1:

D◦

k+1(θ) := −Qθ(x(k), u(k)) + c(x(k), u(k)) + J θ(x(k + 1))

The expectations in (18) are in steady-state. The remaining variables are part of the algorithm design:
{ζk} is a non-negative vector-valued sequence, µ is a positive measure, and κε is non-negative.
The quadratic program (18) is a relaxation of the DPLP, which is tight under ideal conditions (including
the assumption that (J (cid:63), Q(cid:63)) is contained in the function class). See Corollary 2.5 for details.

Literature review This paper began as a resurrection of the conference paper [44], which deals with
Q-learning for deterministic systems in continuous time. A primitive version of convex Q-learning was
introduced; the challenge at the time was to ﬁnd ways to create a reliable online algorithm. This is resolved
in the present paper through a combination of Galerkin relaxation techniques and ERM. Complementary to
the present research is the empirical value iteration algorithms developed in [31, 58].

The ideas in [44] and the present paper were inspired in part by the LP approach to dynamic programming
introduced by Manne in the 60’s [43, 23, 4, 11]. A signiﬁcant program on linear programming approaches
to approximate dynamic programming is presented in [20, 21, 22], but we do not see much overlap with
the present work. There is a also an on-going research program on LP approaches to optimal control for
deterministic systems [64, 32, 34, 39, 35, 13, 29, 30, 14], and semi-deﬁnite programs (SDPs) in linear optimal
control [16, 65].

Major success stories in Deep Q-Learning (DQN) practice are described in [50, 49, 48], and recent surveys
[3, 59]. We do not know if the research community is aware that these algorithms, if convergent, will converge
to the projected Bellman equation (13), and that an ODE approximation of DQN is identical to that of (14)
(for which stability theory is currently very weak). A formal deﬁnition of “ODE approximation” is provided
at the end of Section 2.3.

Kernel methods in RL have a signiﬁcant history, with most of the algorithms designed to approximate the
value function for a ﬁxed policy, so that the function approximation problem can be cast in a least-squares
setting [52, 28] (the latter proposes extensions to Q-learning).

The remainder of the paper is organized as follows. Section 2 begins with a review of linear programs
for optimal control, and new formulations designed for application in RL. Application to LQR is brieﬂy
described in Section 2.2, which concludes with an example to show the challenges expected when using
standard Q-learning algorithms even in the simplest scalar LQR model. Section 2.4 presents a menu of
“convex Q-learning” algorithms. Their relationship with DQN (as well as signiﬁcant advantages) is presented
in Section 2.5. Various extensions and reﬁnements are collected together in Section 3, including application
to MDPs. The theory is illustrated with a single example in Section 4. Section 5 contains conclusions and
topics for future research. Proofs of technical results are contained in the appendices.

2 Convex Q-Learning

The RL algorithms introduced in Section 2.4 are designed to approximate the value function J (cid:63) within a
ﬁnite-dimensional function class {J θ : θ ∈ Rd}. We obtain a convex program when this parameterization is
linear. For readers interested in discounted cost, or ﬁnite-horizon problems, Section 3.4 contains hints on
how the theory and algorithms can be modiﬁed to your favorite optimization criterion.

5

As surveyed in the introduction, a favored approach in Q-learning is to consider for each θ ∈ Rd the

associated Bellman error

Bθ(x) = −J θ(x) + min
u

[c(x, u) + J θ(F(x, u))] ,

x ∈ X

(19)

A generalization of (11) is to introduce a weighting measure µ (typically a probability mass function (pmf)
on X), and consider the mean-square Bellman error

(cid:90)

E(θ) =

[Bθ(x)]2 µ(dx)

(20)

A signiﬁcant challenge is that the loss function E is not convex. We obtain a convex optimization problem
that is suitable for application in RL by applying a common trick in optimization: over-parameterize the
search space. Rather than approximate J (cid:63), we simultaneously approximate J (cid:63) and Q(cid:63), where the latter is
the Q-function deﬁned in (4).

Proofs of all technical results in this section can be found in the appendices.

2.1 Bellman Equation is a Linear Program
For any function J : X → R, and any scalar r, let SJ (r) denote the sub-level set:

SJ (r) = {x ∈ X : J(x) ≤ r}

(21)

The function J is called inf-compact if the set SJ (r) is either pre-compact, empty, or SJ (r) = X (the three
possibilities depend on the value of r). In most cases we ﬁnd that SJ (r) = X is impossible, so that we arrive
at the stronger coercive condition:

lim
(cid:107)x(cid:107)→∞

J(x) = ∞

(22)

Proposition 2.1.
ishes only at xe. Then, the pair (J (cid:63), Q(cid:63)) solve the following convex program in the “variables” (J, Q):

Suppose that the value function J (cid:63) deﬁned in (2) is continuous, inf-compact, and van-

max
J,Q

(cid:104)µ, J(cid:105)

s.t. Q(x, u) ≤ c(x, u) + J(F(x, u))

Q(x, u) ≥ J(x) ,

x ∈ X , u ∈ U(x)

J is continuous, and J(xe) = 0.

(23a)

(23b)

(23c)

(23d)

We can without loss of generality strengthen (23b) to equality: Q(x, u) = c(x, u) + J(F(x, u)). Based on

this substitution, the variable Q is eliminated:

(cid:104)µ, J(cid:105)

max
J
s.t. c(x, u) + J(F(x, u)) ≥ J(x) ,

x ∈ X , u ∈ U(x)

(24a)

(24b)

This more closely resembles what you ﬁnd in the MDP literature (see [4] for a survey).

The more complex LP (23) is introduced because it is easily adapted to RL applications. To see why,

deﬁne for any (J, Q) the Bellman error:

D◦(J, Q)(x,u) := −Q(x, u) + c(x, u) + J(F(x, u)) ,

x ∈ X u ∈ U

(25)

Similar to the term Dn+1 appearing in (14), we have for any input-state sequence,

D◦(J, Q)(x(k),u(k)) = −Q(x(k), u(k)) + cx(k), u(k)) + J(x(k + 1))

6

Hence we can observe the Bellman error along the sample path. The right hand side will be called the
temporal diﬀerence, generalizing the standard terminology.

We present next an important corollary that will motivate RL algorithms to come. Adopting the notation
of eqs. (11) and (18), denote E ε(J, Q) = D◦(J, Q)2: this is a non-negative function on X × U, for any pair of
functions J, Q.

Corollary 2.2.
(cid:37)ε ≤ 1, and pmfs µ, ν, the pair (J (cid:63), Q(cid:63)) solve the following quadratic program:

Suppose that the assumptions of Prop. 2.1 hold. Then, for any constants κε > 0, 0 ≤

max
J,Q

(cid:104)µ, J(cid:105) − κε(cid:104)ν, E ε(J, Q)(cid:105)

s.t. Constraints (23b)–(23d)

Q(x, u) ≥ (1 − (cid:37)ε)c(x, u) + J(F(x, u))

(26a)

(26b)

(26c)

The extra constraint (26c) is introduced so we arrive at something closer to (24). The choice (cid:37)ε = 0 is

not excluded, but we will need the extra ﬂexibility when we seek approximate solutions.

2.2 Semi-deﬁnite program for LQR

Consider the LTI model:

x(k + 1) = F x(k) + Gu(k),

x(0) = x0

y(k) = Hx(k)

(27a)

(27b)

where (F, G, H) are matrices of suitable dimension (in particular, F is n × n for an n-dimensional state
space), and assume that the cost is quadratic:

Ru
with y = Hx, and R > 0. We henceforth denote S = H (cid:124)H ≥ 0.

(cid:124)
c(x, u) = (cid:107)y(cid:107)2 + u

(28)

To analyze the LP (23) for this special case, it is most convenient to express all three functions appearing

in (23b) in terms of the variable z(cid:124) = (x(cid:124), u(cid:124)):

J (cid:63)(x, u) = z

(cid:124)

M J (cid:63)

z

Q(cid:63)(x, u) = z

(cid:124)

M Q(cid:63)

z

c(x, u) = z

M J (cid:63)

=

(cid:20)M (cid:63)
0

(cid:21)

0
0

M c =

M Q(cid:63)

= M c +

(cid:20)F T M (cid:63)F F T M (cid:63)G
GT M (cid:63)F GT M (cid:63)G

(cid:21)

(cid:124)

M cz
(cid:20)S 0
0 R

(cid:21)

(29a)

(29b)

(29c)

Justiﬁcation of the formula for M Q(cid:63)

is contained in the proof of Prop. 2.3 that follows.

Suppose that (F, G) is stabilizable and (F, H) is detectable, so that J (cid:63) is everywhere
Proposition 2.3.
ﬁnite. Then, the value function and Q-function are each quadratic: J (cid:63)(x) = x(cid:124)M (cid:63)x for each x, where
M (cid:63) ≥ 0 is a solution to the algebraic Riccati equation, and the quadratic Q-function is given in (29). The
matrix M (cid:63) is also the solution to the following convex program:

M (cid:63) ∈ arg max trace (M )
(cid:21)

s.t.

(cid:20)S 0
0 R

(cid:20)F T M F F T M G
GT M F GT M G

(cid:21)

(cid:21)
(cid:20)M 0
0
0

≥

(30a)

(30b)

+

where the maximum is over symmetric matrices M , and the inequality constraint (30b) is in the sense of
symmetric matrices.

Despite its linear programming origins, (30) is not a linear program: it is an example of a semi-deﬁnite

program (SDP), for which there is a rich literature with many applications to control [63].

7

LQR and DQN The class of LQR optimal control problems is a great vehicle for illustrating the potential
challenge with popular RL algorithms, and how these challenges might be resolved using the preceding
optimal control theory.

Recall that both DQN and Q(0)-learning, when convergent, solves the root ﬁnding problem (14). We
proceed by identifying the vector ﬁeld f deﬁned in (13) (and appearing in (14)) for the LQR problem, subject
to the most natural assumptions on the function approximation architecture.

For the linear system (27) with quadratic cost (28), the Q-function is a quadratic (obtained from the
algebraic Riccati equation (ARE) or the SDP (30)). To apply Q-learning we might formulate a linear
parameterization:

Qθ(x, u) = x

(cid:124)

(cid:124)
MF x + 2u

(cid:124)
N x + u

MGu

in which the three matrices depend linearly on θ. The Q-learning algorithm (14) and the DQN algorithm
require the minimum of the Q-function, which is easily obtained in this case:
G N (cid:9)x

(cid:124)(cid:8)MF − N

Qθ(x, u) = x

M −1

(cid:124)

Qθ(x) = min
u

Consequently, the function Qθ is typically a highly nonlinear function of θ, and is unlikely to be Lipschitz
continuous. It follows that the same is true for the vector ﬁeld f deﬁned in (13).

The challenges are clear even for the scalar state space model, for which we write

Qθ(x, u) = θ1x2 + 2θ2xu + θ3u2 ,

Qθ(x) = (cid:8)θ1 − θ2

2/θ3

(cid:9)x2

The vector ﬁeld (13) becomes,

f (θ) = −Σζθ + bc + (cid:8)θ1 − θ2

2/θ3

(cid:9)bx

bc = E(cid:36)[c(x(k), u(k))ζk] ,

bx = E(cid:36)[x(k + 1)2ζk] ,

Σζ = E(cid:36)[ζkζk

(cid:124)

]

where the expectations are in “steady-state” (this is made precise in (33) below). The vector ﬁeld is far
from Lipschitz continuous, which rules out stability analysis through stochastic approximation techniques
without projection onto a compact convex region that excludes θ3 = 0.

Projection is not a problem for this one-dimensional special case. It is not known how to stabilize these
Q-learning algorithms or their ODE approximation (15) if the state space has dimension greater than one.
We also do not know if the roots of f (θ) provide good approximations of the ARE, especially if Q(cid:63) does not
lie in the function approximation class.

2.3 Exploration and Quasi-Stochastic Approximation

The success of an RL algorithm based on temporal diﬀerence methods depends on the choice of input u
during training. The purpose of this section is to make this precise, and present our main assumption on
the input designed for generating data to train the algorithm (also known as exploration).

Throughout the remainder of the paper it is assumed that the input used for training is state-feedback

with perturbation, of the form

u(k) = φ(x(k), ξ(k))
(31)
where the exploration signal ξ is a bounded sequence evolving on Rp for some p ≥ 1. We adopt the quasi-
stochastic approximation (QSA) setting of [44, 60, 6, 19]. For example, ξ(k) may be a mixture of sinusoids
of irrational frequencies. It is argued in [60, 6] that this can lead to substantially faster convergence in RL
algorithms, as compared to the use of an i.i.d. signal.

It will be convenient to assume that the exploration is Markovian, which for a deterministic sequence

means it can be expressed

ξ(k + 1) = H(ξ(k))
(32)
It is assumed that H : Rp → Rp is continuous. Subject to the policy (31), it follows that the triple Φ(k) =
(x(k), u(k), ξ(k))(cid:124) is also Markovian, with state space Z = X × U × Rp. The continuity assumption imposed
below ensures that Φ has the Feller property, and boundedness of Φ from just one initial condition implies the
existence of an invariant probability measure that deﬁnes the “steady state” behavior [47, Theorem 12.0.1].

8

For any continuous function g : Z → R and N ≥ 1, denote

gN =

1
N

N
(cid:88)

k=1

g(Φ(k))

This is a deterministic function of the initial condition Φ(0). For any L > 0 denote

GL = {g : (cid:107)g(z(cid:48)) − g(z)(cid:107) ≤ L(cid:107)z − z(cid:48)(cid:107), for all z, z(cid:48) ∈ Z}

The following is assumed throughout this section.

(Aξ) The state and action spaces X and U are Polish spaces; F deﬁned in (1), φ deﬁned in (31),
and H in (32) are each continuous on their domains, and the larger state process Φ is bounded and
ergodic in the following sense: There is a unique probability measure (cid:36), with compact support,
such that for any continuous function g : Z → R, the following ergodic average exists for each
initial condition Φ(0)

E(cid:36)[g(Φ)] := lim
N→∞
Moreover, the limit is uniform on GL, for each L < ∞,

gN

(33)

lim
N→∞

sup
g∈GL

|gN − E(cid:36)[g(Φ)]| = 0

For analysis of Q-learning with function approximation, the vector ﬁeld f introduced in (13) is deﬁned
similarly:

f (θ) = lim
N→∞

1
N

N
(cid:88)

(cid:104)

k=1

−Qθ(x(k), u(k)) + c(x(k), u(k)) + Qθ(x(k + 1))

(cid:105)

ζk

Assumption (Aξ) is surely far stronger than required. If Φ is a Feller Markov chain (albeit deterministic),
it is possible to consider sub-sequential limits deﬁned by one of many possible invariant measures. Uniqueness
of (cid:36) is assumed so we can simplify the description of limits of the algorithm. The uniformity assumption is
used in Appendix B to get a simple proof of ODE approximations, starting with a proof that the algorithm
is stable in the sense that the iterates are bounded. In Borkar’s second edition [12] he argues that such
strong assumptions are not required to obtain algorithm stability, or ODE approximations.

The KroneckerWeyl Equidistribution Theorem provides ample examples of signals satisfying the ergodic
limit (33) when g depends only on the exploration signal [5, 37]. Once this is veriﬁed, then the full Assump-
tion (Aξ) will hold is Φ is an e-chain (an equicontinuity assumption introduced by Jamison) [47].

The choice of “exploration policy” (31) is imposed mainly to simplify analysis. We might speed conver-

gence signiﬁcantly with an “epsilon-greedy policy”:

u(k) = φθk (x(k), ξ(k))

in which the right hand side is a perturbation of the exact Qθ-greedy policy (8), using current estimate θk.
There is a growing literature on much better exploration schemes, motivated by techniques in the bandits
literature [53, 54]. The marriage of these techniques with the algorithms introduced in this paper is a subject
for future research.

ODE approximations The technical results that follow require that we make precise what we mean by
ODE approximation for a recursive algorithm. Consider a recursion of the form

θn+1 = θn + αn+1fn+1(θn) ,

n ≥ 0

(34)

in which {fn} is a sequence of functions that admits an ergodic limit:

fk(θ) ,

θ ∈ Rd

f (θ) := lim
N→∞

1
N

N
(cid:88)

k=1

9

The associated ODE is deﬁned using this vector ﬁeld:

d
dt ϑt = f (ϑt)

(35)

An ODE approximation is deﬁned by mimicking the usual Euler construction: the time-scale for the
ODE is deﬁned by the non-decreasing time points t0 = 0 and tn = (cid:80)n
0 αk for n ≥ 1. Deﬁne a continuous
time process Θ by Θtn = θn for each n, and extend to all t through piecewise linear interpolation. The
next step is to ﬁx a time horizon for analysis of length T > 0, where the choice of T is determined based on
properties of the ODE. Denote T0 = 0, and

Tn+1 = min{tn : tn − Tn ≥ T } ,

n ≥ 0

(36)

t : t ≥ Tn} denote the solution to the ODE (35) with initial condition ϑn

Let {ϑn
Tn = θk(n), with index deﬁned
so that tk(n) = Tn. We then say that the algorithm (34) admits an ODE approximation if for each initial θ0,

lim
n→∞

sup
Tn≤t≤Tn+1

(cid:107)Θt − ϑn

t (cid:107) = 0

(37)

2.4 Convex Q-learning

The RL algorithms introduced in this paper are all motivated by the “DPLP” (23). We search for an
approximate solution among a ﬁnite-dimensional family {J θ , Qθ : θ ∈ Rd}. The value θi might represent
the ith weight in a neural network function approximation architecture, but to justify the adjective convex
we require a linearly parameterized family:

J θ(x) = θ

(cid:124)

ψJ (x) ,

Qθ(x, u) = θ

(cid:124)

ψ(x, u)

(38)

The function class is normalized with J θ(xe) = 0 for each θ. For the linear approximation architecture this
requires ψJ
i (xe) = 0 for each 1 ≤ i ≤ d; for a neural network architecture, this normalization is imposed
through deﬁnition of the output of the network. Convex Q-learning based on a reproducing kernel Hilbert
space (RKHS) are contained in Section 3.3.

Recall the MSE loss (11) presents challenges because it is not convex for linear function approximation.

The quadratic program (26) obtained from the DPLP motivates the variation of (11):

E ε(θ) =

1
N

N −1
(cid:88)

(cid:2)D◦

k+1(θ)(cid:3)2

k=0

with temporal diﬀerence deﬁned by a modiﬁcation of (10):

D◦

k+1(θ) := −Qθ(x(k), u(k)) + c(x(k), u(k)) + J θ(x(k + 1))

(39)

(40)

The algorithms are designed so that J θ approximates Qθ, and hence D◦
(6), Q(x) = minu Q(x, u) for any function Q).

k+1(θ) ≈ Dk+1(θ) (recall from below

The ﬁrst of several versions of “CQL” involves a Galerkin relaxation of the constraints in the DPLP (23).

This requires speciﬁcation of two vector valued sequences {ζk, ζ +

k } based on the data, and denote

zε(θ) =

z+(θ) =

1
N

1
N

N −1
(cid:88)

k=0

D◦

k+1(θ)ζk

N −1
(cid:88)

k=0

(cid:2)J θ(x(k)) − Qθ(x(k), u(k))(cid:3)ζ +

k

(41a)

(41b)

with temporal diﬀerence sequence {D◦
that the entries of the vector ζ +

k are non-negative for each k.

k+1(θ)} deﬁned in (40). It is assumed that ζk takes values in Rd, and

10

LP Convex Q-Learning

θ∗ = arg max

(cid:104)µ, J θ(cid:105)

θ

s.t. zε(θ) = 0
z+(θ) ≤ 0

(42a)

(42b)

(42c)

This algorithm is introduced mainly because it is the most obvious translation of the general DPLP (23).
A preferred algorithm described next is motivated by the quadratic program (26), which directly penalizes
Bellman error. The objective function is modiﬁed to include the empirical mean-square error (39) and a
second loss function:

E +(θ) =

or E +(θ) =

1
N

1
N

N −1
(cid:88)

(cid:2){J θ(x(k)) − Qθ(x(k), u(k))}+

(cid:3)2

k=0

N −1
(cid:88)

(cid:2){J θ(x(k)) − Qθ(x(k))}+

(cid:3)2

k=0

(43a)

(43b)

where {z}+ = max(z, 0). The second option (43b) more strongly penalizes deviation from the constraint
Qθ ≥ J θ. The choice of deﬁnition (43a) or (43b) will depend on the relative complexity, which is application-
speciﬁc.

Convex Q-Learning

For positive scalars κε and κ+, and a tolerance Tol ≥ 0,

θ∗ = arg min

(cid:8)−(cid:104)µ, J θ(cid:105) + κεE ε(θ) + κ+E +(θ)(cid:9)

θ
s.t. zε(θ) = 0

z+(θ) ≤ Tol

(44a)

(44b)

(44c)

To understand why the optimization problem (42) or (44) may present challenges, consider their imple-
mentation based on a kernel. Either of these optimization problems is a convex program. However, due to
the Representer Theorem [10], the dimension of θ is equal to the number of observations N . Even in simple
examples, the value of N for a reliable estimate may be larger than one million.

The following batch RL algorithm is designed to reduce complexity, and there are many other potential
beneﬁts [38]. The time-horizon N is broken into B batches of more reasonable size, deﬁned by the sequence of
intermediate times T0 = 0 < T1 < T2 < · · · < TB−1 < TB = N . Also required are a sequence of regularizers:
Rn(J, Q, θ) is a convex functional of J, Q, θ, that may depend on θn. Examples are provided below.

Batch Convex Q-Learning

With θ0 ∈ Rd given, along with a sequence of positive scalars {κε

n, κ+

n }, deﬁne recursively,

θn+1 = arg min

θ

(cid:110)

−(cid:104)µ, J θ(cid:105) + κε

nE ε

n(θ) + κ+

n E +

n (θ) + Rn(J θ, Qθ, θ)

(cid:111)

(45a)

11

where for 0 ≤ n ≤ B − 1,

E ε
n(θ) =

1
rn

Tn+1−1
(cid:88)

k=Tn

(cid:2)−Qθ(x(k), u(k)) + c(x(k), u(k)) + J θ(x(k + 1))(cid:3)2

E +
n (θ) =

or E +

n (θ) =

1
rn

1
rn

Tn+1−1
(cid:88)

(cid:2){J θ(x(k)) − Qθ(x(k), u(k))}+

(cid:3)2

k=Tn

Tn+1−1
(cid:88)

(cid:2){J θ(x(k)) − Qθ(x(k))}+

(cid:3)2

k=Tn

(45b)

(45c)

(45d)

with rn = 1/(Tn+1 − Tn).
Output of the algorithm: θB, to deﬁne the ﬁnal approximation QθB .

The constraints (44b, 44c) are relaxed in BCQL only to streamline the discussion that follows.
How to choose a regularizer? It is expected that design of Rn will be inspired by proximal algorithms, so
that it will include a term of the form (cid:107)θ − θn(cid:107)2 (most likely a weighted norm—see discussion in Section 3.2).
With a simple scaled norm, the recursion becomes

θn+1 = arg min

θ

(cid:110)

−(cid:104)µ, J θ(cid:105) + κε

nE ε

n(θ) + κ+

n E +

n (θ) +

1
αn+1

1

2 (cid:107)θ − θn(cid:107)2(cid:111)

(46)

where {αn} plays a role similar to the step-size in stochastic approximation.

Convergence in this special case is established in the the following result, based on the steady-state

expectations (recall (33)):

(cid:2){−Qθ(x(k), u(k)) + c(x(k), u(k)) + J θ(x(k + 1))(cid:9)2(cid:3)

¯E ε(θ) = E(cid:36)
¯E +(θ) = E(cid:36)[{J θ(x(k)) − Qθ(x(k), u(k))}2
+]

(47a)

(47b)

Proposition 2.4. Consider the BCQL algorithm (46) subject to the following assumptions:

(i) The parameterization (J θ, Qθ) is linear, and ¯E ε is strongly convex.

(ii) The non-negative step-size sequence is of the form αn = α1/n, with α1 > 0.

(iii) The parameters reach steady-state limits:

r := lim
n→∞

rn ,

κε := lim
n→∞

κε
n ,

κ+ := lim
n→∞

κ+
n

Then, the algorithm is consistent: θn → θ∗ as n → ∞, where the limit is the unique optimizer:

θ∗ = arg min

θ

(cid:110)

−(cid:104)µ, J θ(cid:105) + κε ¯E ε(θ) + κ+ ¯E +(θ)

(cid:111)

(48)

The proof, contained in Appendix B, is based on recent results from SA theory [12]. Strong convexity of

¯E ε is obtained by design, which is not diﬃcult since it is a quadratic function of θ:

with b ∈ Rd, k ≥ 0, and

¯E ε(θ) = θ

(cid:124)

M θ + 2θ

(cid:124)

b + k

M = E(cid:36)

(cid:2)Υk+1Υ

(cid:124)
k+1

(cid:3) ,

with Υk+1 = ψ(x(k), u(k)) − ψJ (x(k + 1))

12

Justiﬁcation of (18) requires that we consider a special case of either CQL or BCQL. Consider the

following version of BCQL in which we apply a special parameterization:

Qθ(x, u) = θ

(cid:124)

ψ(x, u) = J θ(x) + Aθ(x, u) ,

(49)

with θ constrained to a convex set Θ, chosen so that Aθ(x, u) ≥ 0 for all θ ∈ Θ, x ∈ X, u ∈ U (further
discussion on this approximation architecture is contained in Section 3.1). We maintain the deﬁnition of E ε
n
from (45b). We also bring back inequality constraints on the temporal diﬀerence, consistent with the DPLP
constraint (23b). The batch version of (41a) is denoted

zε
n(θ) =

1
rn

Tn+1−1
(cid:88)

k=Tn

D◦

k+1(θ)ζk

(50)

We require ζk(i) ≥ 0 for all k, i to ensure that the inequality constraint zε

n(θ) ≥ 0 is consistent with (23b)

With θ0 ∈ Rd given, along with a positive scalar κε, consider the primal dual variant of BCQL (pd-

BCQL):

θn+1 = arg min

θ∈Θ

(cid:110)

−(cid:104)µ, J θ(cid:105) + κεE ε

(cid:124)
nzε
n(θ) − λ

n(θ) +

λn+1 = (cid:2)λn − αn+1zε

n(θ)(cid:3)

+

1
αn+1

1

2 (cid:107)θ − θn(cid:107)2(cid:111)

(51a)

(51b)

where the subscript “+” in the second recursion is a component-wise projection: for each i, the component
λn+1(i) is constrained to an interval [0, λmax(i)].

Suppose that assumptions (i) and (ii) of Prop. 2.4 hold. In addition, assume that Aθ(x, u) =
Corollary 2.5.
Qθ(x, u) − J θ(x) is non-negative valued for θ ∈ Θ, where Θ is a polyhedral cone with non-empty interior.
Then, the sequence {θn, λn} obtained from the pd-BCQL algorithm (51) is convergent to a pair (θ∗, λ∗), and
the following hold:

(i) The limit θ∗ is the solution of the convex program (18), and λ∗ is the Lagrange multiplier for the
linear inequality constraint in (18).

(ii) Consider the special case: the state space and action space are ﬁnite, µ has full support, and the
dimension of ζk is equal to dζ = |X| × |U|, with

ζk(i) = 1{(x(k), u(k)) = (xi, ui)}

where {(xi, ui)} is an enumeration of all state action pairs. Suppose moreover that (J (cid:63), Q(cid:63)) is contained
in the function class. Then,

(J θ, Qθ)

(cid:12)
(cid:12)
(cid:12)θ=θ∗

= (J (cid:63), Q(cid:63))

(cid:117)(cid:116)

Corollary 2.5 is a corollary to Prop. 2.4, in the sense that it follows the same proof for the joint sequence
{θn, λn}. Both the proposition and corollary start with a proof that {θn} is a bounded sequence, using the
“Borkar-Meyn” Theorem [15, 12, 56, 57] (based on a scaled ODE). The cone assumption on Θ is imposed
to simplify the proof that the algorithm is stable. Once boundedness is established, the next step is to show
that the algorithm (51) can be approximated by a primal-dual ODE for the saddle point problem

max
λ≥0

min
θ∈Θ

¯L(θ, λ) ,

(cid:124)
¯L(θ, λ) := −(cid:104)µ, J θ(cid:105) + κε ¯E ε(θ) − λ

¯zε
n(θ)

(52)

The function ¯L is quadratic and strictly convex in θ, and linear in λ. A suitable choice for the upper bound
λmax can be found through inspection of this saddle-point problem.

This approximation suggests improvements to the algorithm. For example, the use of an augmented
Lagrangian to ensure strict convexity in λ. We might also make better use of the solution to the quadratic
program (51a) to improve estimation of λ∗.

The choice of regularizer is more subtle when we consider kernel methods, so that the “parameterization”

is inﬁnite dimensional. Discussion on this topic is postponed to Section 3.3.

13

2.5 Comparisons with Deep Q-Learning

The Deep Q Network (DQN) algorithm was designed for neural network function approximation; the term
“deep” refers to a large number of hidden layers. The basic algorithm is summarized below, without imposing
any particular form for Qθ. The deﬁnition of {Tn} is exactly as in the BCQL algorithm.

DQN

With θ0 ∈ Rd given, along with a sequence of positive scalars {αn}, deﬁne recursively,

where for each n:

θn+1 = arg min

θ

(cid:110)

κεE ε

n(θ) +

1
αn+1

(cid:107)θ − θn(cid:107)2(cid:111)

E ε
n(θ) =

1
rn

Tn+1−1
(cid:88)

(cid:2)−Qθ(x(k), u(k)) + c(x(k), u(k)) + Qθn (x(k + 1))(cid:3)2

k=Tn

with rn = 1/(Tn+1 − Tn).
Output of the algorithm: θB, to deﬁne the ﬁnal approximation QθB .

(53a)

(53b)

The elegance and simplicity of DQN is clear. Most signiﬁcant:

if Qθ is deﬁned via linear function

approximation, then the minimization (53a) is the unconstrained minimum of a quadratic.

DQN appears to be nearly identical to (46), except that the variable J θ does not appear in the DQN loss
function. Prop. 2.6 (along with Prop. 2.4 and its corollary) show that this resemblance is superﬁcial—the
potential limits of the algorithms are entirely diﬀerent.

Proposition 2.6. Consider the DQN algorithm with possibly nonlinear function approximation, and with
(cid:12)
(cid:12)
. Assume that Qθ is continuously diﬀerentiable, and its gradient ∇Qθ(x, u) is
ζk = ∇θQθ(x(k), u(k))
(cid:12)θ=θk
globally Lipschitz continuous, with Lipschitz constant independent of (x, u). Suppose that B = ∞, the non-
negative step-size sequence satisﬁes αn = α1/n, with α1 > 0, and suppose that the sequence {θn} deﬁned by
the DQN algorithm is convergent to some θ∞ ∈ Rd.

Then, this limit is a solution to (13), and moreover the algorithm admits the ODE approximation d

f (ϑt) using the vector ﬁeld f deﬁned in (13).

dt ϑt =
(cid:117)(cid:116)

The assumption on the step-size is to facilitate a simple proof. This can be replaced by the standard

assumptions:

(cid:88)

αn = ∞ ,

(cid:88)

α2

n < ∞

The proof of Prop. 2.6 can be found in Appendix B. Its conclusion should raise a warning, since we do not
know if (13) has a solution, or if a solution has desirable properties. The conclusions are very diﬀerent for
convex Q-learning.

The interpretation of Prop. 2.6 is a bit diﬀerent with the use of an ε-greedy policy for exploration. In this
case, (13) is close to the ﬁxed point equation for TD(0) learning, for which there is substantial theory subject
to linear function approximation — see [61] for history. This theory implies existence of a solution for small
ε, but isn’t satisfying with regards to interpreting the solution. Moreover, stability of parameter-dependent
exploration remains a research frontier [36].

3

Implementation Guides and Extensions

3.1 A few words on constraints

The pd-BCQL algorithm deﬁned in (51) is motivated by a relaxation of the inequality constraint (23b)
required in the DPLP:

(54)

zε(θ) ≥ −Tol

14

Recall that this is a valid relaxation only if we impose positivity on the entries of ζk. In experiments it is
found that imposing hard constraints in the convex program is more eﬀective than the pure penalty approach
used in BCQL.

With the exception of pd-BCQL, the CQL algorithms introduce data-driven relaxations of the constraint
(23c) in the DPLP: consider the constraint (42c) or (44c), or the penalty E +
n (θ) in BCQL. A data driven
approach may be convenient, but it is unlikely that this is the best option. The purpose of these constraints
is to enforce non-negativity of the diﬀerence, Aθ(x, u) = Qθ(x, u) − J θ(x) (known as the advantage function
in RL).

There are at least two options to enforce or approximate the inequality Aθ ≥ 0:

1. Choose a parameterization, along with constraints on θ, so that non-negativity of Aθ is automatic.
Consider for example a linear parameterization in which d = dJ + dQ, and with basis functions {ψJ , ψA}
satisfying the following constraints:

i (x) = 0 for all x, and all i > dJ
ψJ
i (x, u) = 0 for all x, u, and all i ≤ dJ
ψA

subject to the further constraint that ψA

i (x, u) ≥ 0 for all i, x, u.

We then deﬁne ψ = ψJ + ψA, J θ(x) = θ(cid:124)ψJ (x, u), Qθ(x, u) = θ(cid:124)ψ(x, u), and Aθ(x, u) = θ(cid:124)ψA(x, u), so

that for any θ ∈ Rd,

Qθ(x, u) = J θ(x) + Aθ(x, u)
It follows that Qθ(x, u) ≥ J θ(x) for all x, u, provided we impose the constraint θi ≥ 0 for i > dJ . Using this
approach, the penalty term κ+E +(θ) may be eliminated from (44), as well as the constraint z+(θ) ≤ Tol. It
was found that this approach was most reliable in experiments conducted so far, along with the relaxation
(54), since the algorithm reduces to a convex program (much like DQN).
2. Choose a grid of points G ⊂ X × U, and replace (44c) with the simple inequality constraint

Aθ(xi, ui) ≥ −Tol for (xi, ui) ∈ G.

For example, this approach is reasonable for the LQR problem and similar control problems that involve
linear matrix inequalities (such as eq. (30b)). In particular, the constraints in (30) cannot be captured by
a linear parameterization of the matrices M , so application of approach 1 can only approximate a subset of
the constraint set.

3.2 Gain selection and SA approximations

Consider the introduction of a weighted norm in (46):

θn+1 = arg min

(cid:8)En(θ) +

θ
En(θ) = −(cid:104)µ, J θ(cid:105) + κε

nE ε

1

1
αn+1
n(θ) + κ+

n E +

n (θ)

2 (cid:107)θ − θn(cid:107)2

Wn

(cid:9)

(55)

where (cid:107)ϑ(cid:107)2

Wn = 1

2 ϑ(cid:124)Wnϑ for ϑ ∈ Rd, with Wn > 0.

The recursion can be represented in a form similar to stochastic approximation (SA):

Lemma 3.1.
BCQL is the solution to the ﬁxed point equation:

Suppose that {En(θ)} are continuously diﬀerentiable in θ. Then, the parameter update in

Suppose in addition that ∇En is Lipschitz continuous (uniformly in n), and the sequences {θn} and {trace (W −1
are uniformly bounded. Then,

n )}

θn+1 = θn − αn+1W −1

n ∇En(θn+1)

(56)

θn+1 = θn − αn+1{W −1

n ∇En(θn) + εn+1}

(57)

where (cid:107)εn+1(cid:107) = O(αn+1).

15

Proof. The ﬁxed point equation (56) follows from the ﬁrst-order condition for optimality:

0 = ∇(cid:8)En(θ) +

1
αn+1

1

2 (cid:107)θ − θn(cid:107)2

Wn

(cid:9)(cid:12)
(cid:12)θ=θn+1

= ∇En(θn+1) +

1
αn+1

1
2 Wn[θn+1 − θn]

To obtain the second conclusion, note that (56) implies (cid:107)θn+1 − θn(cid:107) = O(αn+1) under the boundedness
(cid:117)(cid:116)

assumptions, and then Lipschitz continuity of {∇En} in θ then gives the desired representation (57).

The Zap SA algorithm of [25] is designed for recursions of the form (57) so that

Wn ≈ ∇2 ¯E(θn)

in which the bar designates a steady-state expectation (recall (33)).

For the problem at hand, this is achieved in the following steps. First, deﬁne for each n,

An+1 = ∇2En (θn)

For the linear parametrization using (45c) we obtain simple expressions. To compress notation, denote

ψ(k) = ψ(x(k), u(k)) ,

(k) = ψJ (x(k)) .
ψJ

so that (55) gives

∇En (θ) = 2

1
rn

Tn+1−1
(cid:88)

(cid:110)

k=Tn

− (cid:104)µ, ψJ (cid:105) + κε

nD◦

k+1(θ)(cid:2)ψJ

(k+1) − ψ(k)

(cid:3)

+ κ+

n {J θ(x(k)) − Qθ(x(k))}+

(cid:2)ψJ

(k) − ψ(k)

(cid:3)(cid:111)

where (cid:104)µ, ψJ (cid:105) is the column vector whose ith entry is (cid:104)µ, ψJ

i (cid:105). Taking derivatives once more gives

An+1 = 2

1
rn

Tn+1−1
(cid:88)

k=Tn

(cid:110)(cid:2)ψ(k) − ψJ

(k+1)

(cid:3)(cid:2)ψ(k) − ψJ

(k+1)

(cid:3)(cid:124)

+ κ+
n

(cid:2)ψ(k) − ψJ

(k)

(cid:3)(cid:2)ψ(k) − ψJ

(k)

(cid:111)
(cid:3)(cid:124)1{J θ(x(k)) > Qθ(x(k))}

We then take W0 > 0 arbitrary, and for n ≥ 0,

Wn+1 = Wn + βn+1[An+1 − Wn]

(58)

in which the step-size for this matrix recursion is relatively large:

lim
n→∞

αn
βn

= 0

In [25] the choice βn = αη

n is proposed, with 1

2 < η < 1.

The original motivation in [26, 27, 24] was to minimize algorithm variance It is now known that the
“Zap gain” (58) often leads to a stable algorithm, even for nonlinear function approximation (such as neural
networks) [18].

It may be advisable to simply use the recursive form of the batch algorithm: make the change of notation

(cid:98)An = Wn (to highlight the similarity with the Zap algorithms in [25]), and deﬁne recursively

θn+1 = θn − αn+1 (cid:98)A−1
n ∇En(θn)
(cid:98)An+1 = (cid:98)An + βn+1[An+1 − (cid:98)An]

In preliminary experiments it is found that this leads to much higher variance, but this may be oﬀset by the
reduced complexity.

16

3.3 BCQL and kernel methods

The reader is referred to other sources, such as [10], for the deﬁnition of a reproducing kernel Hilbert space
(RKHS) and surrounding theory. In this subsection, the Hilbert space H deﬁnes a two dimensional function
class that deﬁnes approximations (J, Q). One formulation of this method is to choose a kernel k on (X×U)2,
in which k((x, u), (x(cid:48), u(cid:48))) is a symmetric and positive deﬁnite matrix for each x, x(cid:48) ∈ X and u, u(cid:48) ∈ U. The
function J does not depend on u, so for (f, g) ∈ H we associate g with Q, but take

J(x) = f (x, u◦)

for some distinguished u◦ ∈ U.

A candidate regularizer in this case is

Rn(J, Q) =

1
αn+1

1

2 (cid:107)(J, Q) − (J n, Qn)(cid:107)2

H

where (J n, Qn) ∈ H is the estimate at stage n based on the kernel BCQL method.

The notation must be modiﬁed in this setting:

Kernel Batch Convex Q-Learning

With (J 0, Q0) ∈ H given, along with a sequence of positive scalars {κε

n, κ+

n }, deﬁne recursively,

(J n+1, Qn+1) = arg min

J,Q

where for 0 ≤ n ≤ B − 1,

(cid:110)

−(cid:104)µ, J(cid:105) + κε

nE ε

n(J, Q) + κ+

n E +

n (J, Q) + Rn(J, Q)

(cid:111)

E ε
n(J, Q) =

1
rn

Tn+1−1
(cid:88)

(cid:2)−Q(x(k), u(k)) + c(x(k), u(k)) + J(x(k + 1))(cid:3)2

k=Tn

E +
n (J, Q) =

or E +

n (J, Q) =

1
rn

1
rn

Tn+1−1
(cid:88)

(cid:2){J(x(k)) − Q(x(k), u(k))}+

(cid:3)2

k=Tn

Tn+1−1
(cid:88)

(cid:2){J(x(k)) − Q(x(k))}+

(cid:3)2

k=Tn

(59)

(60a)

(60b)

(60c)

(60d)

The regularizer (59) is chosen so that we can apply the Representer Theorem to solve this inﬁnite-
dimensional optimization problem (60). The theorem states that computation of (J n, Qn) reduces to a ﬁnite
dimensional setting, with a linearly parameterized family similar to (38). However, the “basis functions” ψJ
and ψ depend upon n: for some {θn∗

i } ⊂ R2,

J n(x) =

(cid:88)

{θn(cid:63)
i

(cid:124)k((xi, ui), (x, u◦))}1

Qn(x, u) =

i
(cid:88)

{θn(cid:63)
i

i

(cid:124)k((xi, ui), (x, u))}2

where {xi, ui} are the state-input pairs observed on the time interval {Tn−1 ≤ k < Tn}.

3.4 Variations

The total cost problem (2) is our favorite because the control solution comes with stability guarantees under
mild assumptions. To help the reader we discuss here alternatives, and how the methods can be adapted to
other performance objectives.

17

Discounted cost Discounting in often preferred in operations research and computer sicence, since “in
the long run we are all dead”. It is also convenient because it is easier to be sure that the value function is
ﬁnite valued: with γ ∈ (0, 1) the discount factor,

J (cid:63)(x) = min
u

∞
(cid:88)

k=0

γkc(x(k), u(k)) ,

x(0) = x ∈ X .

(61)

The Q-function becomes Q(cid:63)(x, u) := c(x, u) + γJ (cid:63)(F(x, u)), and the Bellman equation has the same form (5).

Shortest path problem Given a subset A ⊂ X, deﬁne

τA = min{k ≥ 1 : x(k) ∈ A}

The discounted shortest path problem (SPP) is deﬁned to be the minimal discounted cost incurred before
reaching the set A:

J (cid:63)(x) = min
u

(cid:110) τA−1
(cid:88)

k=0

γkc(x(k), u(k)) + J0(x(τA))

(cid:111)

(62)

where J0 : X → R+ is the terminal cost. For the purposes of unifying the control techniques that follow, it is
useful to recast this as an instance of the total cost problem (2). This requires the deﬁnition of a new state
process xA with dynamics FA, and a new cost function cA deﬁned as follows:

(i) The modiﬁed state dynamics: append a graveyard state to (cid:78) to X, and denote

FA(x, u) =

(cid:40)

F(x, u) x ∈ Ac
(cid:78)

x ∈ A ∪ (cid:78)

so that xA(k + i) = (cid:78) for all i ≥ 1 if xA(k) ∈ A.

(ii) Modiﬁed cost function:

cA(x, u) =






c(x, u) x ∈ Ac
x ∈ A
J0(x)
x = (cid:78)
0

From these deﬁnitions it follows that the value function (62) can be expressed

J (cid:63)(x) = min
u

∞
(cid:88)

k=0

γkcA(xA(k), u(k))

Alternatively, we can obtain a dynamic programming equation by writing

J (cid:63)(x) = min
u

(cid:110)

c(x, u(0)) +

τA−1
(cid:88)

k=1

γkc(x(k), u(k)) + J0(x(τA))

(cid:111)

with the understanding that (cid:80)0
x(1) ∈ A. Hence,

1 = 0. The upper limit in the sum is equal to 0 when τA = 1; equivalently,

J (cid:63)(x) = min
u(0)

(cid:110)

(cid:104)
c(x, u(0)) + γ1{x(1) ∈ Ac}

min
u[1,∞]

τA−1
(cid:88)

γk−1c(x(k), u(k)) + J0(x(τA))

(cid:105)(cid:111)

c(x, u(0)) + γ1{x(1) ∈ Ac}J (cid:63)(x(1))

,

x(1) = F(x, u(0))

(63)

k=1
(cid:111)

(cid:110)

= min
u(0)

= min

u

(cid:110)

c(x, u) + γ1{F(x, u) ∈ Ac}J (cid:63)(F(x, u))

(cid:111)

18

Finite horizon Your choice of discount factor is based on how concerned you are with the distant future.
Motivation is similar for the ﬁnite horizon formulation: ﬁx a horizon T ≥ 1, and denote

J (cid:63)(x) = min
u[0,T ]

T
(cid:88)

k=0

c(x(k), u(k)) ,

x(0) = x ∈ X .

(64)

This can be interpreted as the total cost problem (2), following two modiﬁcations of the state description
and the cost function, similar to the SPP:

(i) Enlarge the state process to xT (k) = (x(k), τ(k)), where the second component is “time” plus an

oﬀset:

τ(k) = τ(0) + k ,

k ≥ 0

(ii) Extend the deﬁnition of the cost function as follows:

cT ((x, τ), u) =

(cid:40)

c(x, u) τ ≤ T
τ > T
0

That is, cT ((x, τ), u) = c(x, u)1{τ ≤ T } for all x, τ, u.

If these deﬁnitions are clear to you, then you understand that we have succeeded in the transformation:

J (cid:63)(x) = min
u

∞
(cid:88)

k=0

cT (xT (k), u(k)) ,

xT (0) = (x, τ) , τ = 0

(65)

However, to write down the Bellman equation it is necessary to consider all values of τ (at least values τ ≤ T ),
and not just the desired value τ = 0. Letting J (cid:63)(x, τ) denote the right hand side of (65) for arbitrary values
of τ ≥ 0, the Bellman equation (3) becomes

J (cid:63)(x, τ) = min
u

(cid:8)c(x, u)1{τ ≤ T } + J (cid:63)(F(x, u), τ + 1)(cid:9)

(66)

Based on (65) and the deﬁnition of cT , we know that J (cid:63)(x, τ) ≡ 0 for τ > T . This is considered a

boundary condition for the recursion (66), which is put to work as follows: ﬁrst, since J (cid:63)(x, T + 1) ≡ 0,

Applying (66) once more gives,

J (cid:63)(x, T ) = c(x) := min
u

c(x, u)

J (cid:63)(x, T − 1) = min
u

(cid:8)c(x, u) + c(F(x, u))(cid:9)

If the state space X is ﬁnite then these steps can be repeated until we obtain the value function J (cid:63)( · , 0).

What about the policy? It is again obtained via (66), but the optimal input depends on the extended

state, based on the policy

φ(cid:63)(x, τ) = arg min

u

(cid:8)c(x, u) + J (cid:63)(F(x, u), τ + 1)(cid:9) ,

τ ≤ T

This means that the feedback is no longer time-homogeneous:1

1Substituting τ(cid:63)(k) = k is justiﬁed because we cannot control time!

u(cid:63)(k) = φ(cid:63)(x(cid:63)(k), τ(cid:63)(k)) = φ(cid:63)(x(cid:63)(k), k)

(67)

19

3.5 Extensions to MDPs

The full extension of convex Q-learning will be the topic of a sequel. We present here a few ideas on how to
extend these algorithms to MDP models.

The state space model (1) is replaced by the controlled Markov model,

X(k + 1) = F(X(k), U (k), N (k + 1)) ,

k ≥ 0

(68)

where we have opted for upper-case to denote random variables, and N is an i.i.d. sequence. For simplicity
we assume here that the state space and action space are ﬁnite, and that the disturbance N also evolves on
a ﬁnite set (without loss of generality, given the assumptions on X and U). The controlled transition matrix
is denoted

Pu(x, x(cid:48)) = P{X(k + 1) = x(cid:48) | X(k) = x , U (k) = u}

The following operator-theoretic notation is useful: for any function h : X → R,

= P{F(x, u, N (1)) = x(cid:48)} ,

x, x(cid:48) ∈ X , u ∈ U

Puh (x) =

(cid:88)

x(cid:48)

Pu(x, x(cid:48))h(x(cid:48)) = E[h(X(k + 1)) | X(k) = x , U (k) = u]

An admissible policy is a sequence of mappings to deﬁne the input:

U (k) = φk(X(0), . . . , X(k))

This includes randomized policies or “quasi-randomized polices” of the form (31), since we can always extend
the state process X to include an exploration sequence.

As usual, a stationary policy is state feedback: U (k) = φ(X(k)). We impose the following controllability

condition: for each x0, x1 ∈ X, there is an admissible policy such that

where τx1 is the ﬁrst time to reach x1.

Denote for any admissible input sequence the average cost:

E[τx1 | X(0) = x0] < ∞

ηU (x) = lim sup
n→∞

1
n

n−1
(cid:88)

k=0

Ex[c(X(k), U (k))] ,

x ∈ X ,

(69)

and let η(cid:63) denote the minimum over all admissible inputs (independent of x under the conditions imposed
here [33, 55, 8, 46]). The average cost problem is a natural analog of the total cost optimal control problem,
since the dynamic programming equation for the relative value function h(cid:63) nearly coincides with (3):

h(cid:63)(x) = min
u

(cid:8)c(x, u) + Puh(cid:63) (x)(cid:9) − η(cid:63) ,

x ∈ X

The Q-learning formulation of [1] is adopted here:

Q(cid:63)(x, u) = c(x, u) + PuQ(cid:63)(x) − δ(cid:104)ν , Q(cid:63)(cid:105)

(70)

where ν is any pmf on X × U (a dirac delta function is most convenient), and δ > 0. The normalization is
imposed so that there is a unique solution to (70), and this solution results in

δ(cid:104)ν , Q(cid:63)(cid:105) = η(cid:63)

The relative value function is also not unique, but one solution is given by h(cid:63)(x) = minu Q(cid:63)(x, u), and the
optimal policy is any minimizer (recall (7)).

There is then an obvious modiﬁcation of the BCQL algorithm. First, in terms of notation we replace J θ

by hθ, and then modify the loss function as follows:

E ε
n(θ) =

1
rn

Tn+1−1
(cid:88)

k=Tn

(cid:8)−Qθ(x(k), u(k)) − δ(cid:104)ν , Qθ(cid:105) + c(x(k), u(k)) + hθ

(cid:9)2

(71)

k+1|k

where hθ
the conditional expectation. We propose three options:

k+1|k = E[hθ(X(k + 1)) | X(k), U (k)]. A challenge in the Markovian setting is the approximation of

20

1. Direct computation If we have access to a model, then this is obtained as a simple sum:

hθ
k+1|k =

(cid:88)

k(cid:48)

Pu(x, x(cid:48))hθ(x(cid:48)) ,

on observing (x, u) = (X(k), U (k)),

2. Monte-Carlo If we do not have a model, or if the state space is large, then we might resort to
computing the empirical pmf to approximate the conditional expectation (this is one role of the experience
replay buﬀer encountered in the RL literature [41, 38, 53]).

It may be preferable to opt for a Galerkin relaxation, similar to what was used in several of the algorithms
introduced for the deterministic model: choose dG functions {hk : 1 ≤ k ≤ dG}, and consider the ﬁnite-
dimensional function class: (cid:98)H = {(cid:80)

i αihi : α ∈ RdG} to deﬁne an estimate of the random variable Z:

(cid:98)E[Z | σ(Y )] = arg min

E[(Z − h(Y ))2]

h∈ (cid:98)H

(72)

As is well known, the solution is characterized by orthogonality:

i hi solves the minimum in (72) if and only if the following holds for

Lemma 3.2. A function h◦ = (cid:80)
each 1 ≤ i ≤ dG:

i α◦

Any minimizer satisﬁes

0 = E[(Z − h◦(Y ))hi(Y )]

Aα◦ = b

where A is a dG × dG matrix and b is a dG-dimensional vector, with entries

Consequently, if A is full rank, then α(cid:63) = A−1b is the unique minimizer.

Ai,j = E[hi(Y )hj(Y )] ,

b = E[Zhi(Y )]

(73)

(74)

(75)

(cid:117)(cid:116)

3. Pretend the world is deterministic. This means we abandon the conditional expectation, and
instead deﬁne a loss function similar to the deterministic setting:

E εvar
n

(θ) =

1
rn

Tn+1−1
(cid:88)

(cid:2)−Qθ(x(k), u(k)) − δ(cid:104)ν , Qθ(cid:105) + c(x(k), u(k)) + hθ(X(k + 1))(cid:3)2

(76)

k=Tn

where the inclusion of “var” in this new notation is explained in Prop. 3.3 that follows. Denote the steady
state loss functions

¯E ε(θ) = E(cid:36)
¯E εvar(θ) = E(cid:36)

(cid:2)−Qθ(x(k), u(k)) − δ(cid:104)ν , Qθ(cid:105) + c(x(k), u(k)) + hθ
(cid:2)−Qθ(x(k), u(k)) − δ(cid:104)ν , Qθ(cid:105) + c(x(k), u(k)) + hθ(X(k + 1))(cid:9)2(cid:3)

k+1|k

(cid:9)2(cid:3)

(77)

Proposition 3.3. Given any stationary policy, and any steady-state distribution (cid:36) for (X(k), U (k)), the
respective means of the loss function are related as follows:

¯E εvar(θ) = ¯E ε(θ) + σ2

k+1|k(θ)
(cid:2)(cid:0)hθ(X(k + 1)) − hθ

k+1|k

where σ2

k+1|k(θ) = E(cid:36)

(cid:1)2(cid:3)

(cid:117)(cid:116)

Consequently, if the variance of X(k + 1) − X(k) is not signiﬁcant, then the two objective functions E εvar
n are not very diﬀerent. In applications to robotics it is surely best to use the simpler loss function
(θ), while this may not be advisable in ﬁnancial applications.

n

and E ε
E εvar
n

21

4 Example

A simple example is illustrated in Fig. 1, in which the two dimensional state space is position and velocity:

x(t) ∈ X = [zmin, zgoal] × [−v, v]

Where zmin is a lower limit for x(t), and the target state is zgoal. The velocity is bounded in magnitude by
v > 0. The input u is the throttle position (which is negative when the car is in reverse). This example was
introduced in the dissertation [51], and has since become a favorite basic example in the RL literature [61].

Due to state and input constraints, a feasible policy will sometimes put the
car in reverse, and travel at maximal speed away from the goal to reach a higher
elevation to the left. Several cycles back and forth may be required to reach
the goal. It is a good example to test the theory because the value function is
not very smooth.

A continuous-time model can be constructed based on the two forces on the
on the car shown in Fig. 2. With zgoal − z(t) the distance along the road to the
goal we obtain

where a = d2

dt2 x, and θ > 0 for the special case shown in Fig. 2. This can be written in state space form

ma = −mg sin(θ) + ku

Figure 1: Mountain Car

d
dt x1 = x2
k
m

d
dt x2 =

u − g sin(θ(x1))

where θ(x1) is the road grade when at position x1.

A discrete time model is adopted in [61, Ch. 10] of the form:

x1(k + 1) = [[x1(t) + x2(t)]]1
x2(k + 1) = [[x2(k) + 10−3u(k) − 2.5 × 10−3 cos(3x1(k))]]2

(78a)

which corresponds to θ(x1) = π +3x1. The brackets are projecting the values of
x1(k + 1) to the interval [zmin, zgoal] = [−1.2, 0.5], and x1(k + 1) to the interval
[−v, v]. We adopt the values used in [61]:

zmin = −1.2, zgoal = 0.5, and v = 7 × 10−2.

(78b)

Figure 2: Two forces on the
Mountain Car

The control objective is to reach the goal in minimal time, but this can also
be cast as a total cost optimal control problem. Let xe = (zgoal, 0)(cid:124), and reduce the state space so that xe
is the only state x = (z, v)(cid:124) ∈ X satisfying z = zgoal. This is justiﬁed because the car parks on reaching the
goal. Let c(x, u) = 1 for all x, u with x (cid:54)= xe, and c(xe, u) ≡ 0.

The optimal total cost (2) is ﬁnite for each initial condition, and the Bellman equation (3) becomes

J (cid:63)(x) = 1 + min
u

(cid:8)J (cid:63)(F(x, u))(cid:9) ,

x1 < zgoal

with the usual boundary constraint J (cid:63)(xe) = 0.

Experiments with this example are a work in progress. Fig. 3 shows results from one experiment using

a parameterization of the form (49), in which each ψJ

i (x) was obtained via binning:

ψJ
i (x) = 1{x ∈ Bi}
where the ﬁnite collection of sets is disjoint, with ∪Bi = {x ∈ R2 : x1 < 0.5}. In this experiment they
were chosen to be rectangular, with 800 in total. The union was chosen to equal a half space because of the
following choice for the advantage basis. A vector x∆ ∈ R2
+ was chosen, equal to the center of the unique
bin in the positive quadrant containing the origin as one vertex. We then deﬁned

i (x, u) = 1{u = 1 , x < 0.5}ψJ
ψA
i (x, u) = 1{u = −1 , x < 0.5}ψj
ψA

i (x + x∆)
i (x − x∆)

for all x, u, and all dJ + 1 ≤ i ≤ 2dJ
for all x, u, and all 2dJ + 1 ≤ i ≤ 3dJ

22

mgmgcos(θ)θkumgsin(θ)Figure 3: Value function and its approximation using a version of CQL

Every algorithm failed with the basis with x∆ = 0 (no shift). The explanation comes from the fact that
the state moves very slowly, which means that x(k) and x(k +1) often lie in the same bin. For such k we have
J θ(x(k + 1)) = J θ(x(k)), which is bad news: it is easy to ﬁnd a vector θ0 satisfying Qθ0
(x)
when z < 0.5 (for example, take J θ ≡ const. and Qθ0

≡ 1 + const.). Hence for these bad values of k,

(x, u) = 1 + J θ0

Qθ0

(x(k), u(k)) = 1 + J θ0

(x(k)) = c(x(k), u(k)) + J θ0

(x(k + 1))

That is, the observed Bellman error is precisely zero! Both convex Q and DQN will likely return θ∗ ≈ θ0
without an enormous number of bins. The introduction of the shift resolved this problem.

Better results were obtained with an extension of this basis to include quadratics, deﬁned so that the
span of ψJ
includes all quadratics satisfying q(z, v) = 0 whenever z = 0.5. To achieve this, we simply merged
i
four bins adjacent to z ≡ 0.5, and nearest to v = 0.07, and replaced three basis vectors with quadratics
(chosen to be non-negative on the state space).

The plot on the left hand side of Fig. 3 was obtained using value iteration for an approximate model, in
which the state space was discretized with z-values equally spaced at values k × sz; v-values equally spaced
at values k × sv, with sz = 0.041 and sv = 0.001.2 The approximation shown on the right was obtained using
Convex Q-Learning (44) with slight modiﬁcations: First, the positivity penalties and constraints were relaxed
since Qθ ≥ J θ was imposed via constraints on θ. Second, (44b) was relaxed to the inequality constraints
(54) with Tol = 0, resulting in

θ∗ = arg min

(cid:8)−(cid:104)µ, J θ(cid:105) + κεE ε(θ)(cid:9)

θ
s.t. zε(θ) ≥ 0
θi ≥ 0 ,

i ≥ dJ

Finally, the Galerkin relaxation was essentially abandoned via

ζk(i) = 1{x(k) = xi, u(k) = ui} ,

1 ≤ i ≤ N ,

where N = 104 is the run length, and {xi, ui : 1 ≤ i ≤ N } represents all values observed! This means we are
enforcing D◦
k+1(θ∗) ≥ 0 for all k over the run. This is a very complex approach: with 104 observations we
have the same number of constraints. Regardless, the quadprog command in Matlab returned the answer in
one hour on a 2018 MacBook Pro with 16GB of ram.

5 Conclusions

The LP and QP characterization of dynamic programming equations gives rise to RL algorithms that provably
convergent, and for which we know what problem we are actually solving. Much more work is required to

2Many thanks to Fan Lu at UF for conducting this experiment

23

0-1.2-0.0720-0.86-0.04240-0.52-0.01460-0.180.014800.160.0420.07-0.070-1.2-0.04220-0.86-0.014-0.52400.014-0.18600.0420.160.070.580Value function obtained from VIAValue function approximation from convex Qdevelop these algorithms for particular applications, and to improve eﬃciency through a combination of
algorithm design and techniques from optimization theory.

An intriguing open question regards algorithms for MDP models. In this setting, a minor variant of the

linear program (24) is the dual of Manne’s LP [4]. The primal is expressed:

min E(cid:36)[c(X, U )]
s.t. (cid:36) ∈ P

(79)

where P is a convex subset of probability measures on X × U. Characterization of P is most easily described
when X × U is discrete. In this case, for any pmf on the product space we can write via Baye’s rule

(cid:36)(x, u) = π(x)φ(u | x)

where π is the ﬁrst marginal, and φ is interpreted as a randomized policy. We say that (cid:36) ∈ P if π is a
steady-state distribution for X when controlled using φ.

Actor-critic algorithms are designed to optimize average cost over a family of randomized policies {φθ}.
The mapping from a randomized policy φθ to a bivariate pmf (cid:36)θ ∈ P is highly nonlinear, which means the
valuable convexity of the primal (79) is abandoned. We hope to devise techniques to construct a convex
family of pmfs {(cid:36)θ} ⊂ P that generate policies which approximate the randomized polices of interest. We
then arrive at a convex program that approximates (79), and from its optimizer obtain a policy that achieves
the optimal average cost:

φθ∗

(u | x) =

1
πθ∗ (x)

(cid:36)θ∗

(x, u) ,

where πθ∗

(x) =

(cid:36)θ∗

(x, u(cid:48))

(cid:88)

u(cid:48)

Existing actor-critic theory might be extended to obtain RL algorithms to estimate θ∗.

References

[1] J. Abounadi, D. Bertsekas, and V. S. Borkar. Learning algorithms for Markov decision processes with

average cost. SIAM Journal on Control and Optimization, 40(3):681–698, 2001.

[2] B. D. O. Anderson and J. B. Moore. Optimal Control: Linear Quadratic Methods. Prentice-Hall,

Englewood Cliﬀs, NJ, 1990.

[3] O. Anschel, N. Baram, and N. Shimkin. Averaged-DQN: Variance reduction and stabilization for deep
reinforcement learning. In Proc. of the 34th International Conference on Machine Learning - Volume
70, ICML’17, pages 176–185. JMLR.org, 2017.

[4] A. Arapostathis, V. S. Borkar, E. Fernandez-Gaucherand, M. K. Ghosh, and S. I. Marcus. Discrete-time
controlled Markov processes with average cost criterion: a survey. SIAM J. Control Optim., 31:282–344,
1993.

[5] J. Beck. Strong Uniformity and Large Dynamical Systems. World Scientiﬁc, 2017.

[6] A. Bernstein, Y. Chen, M. Colombino, E. Dall’Anese, P. Mehta, and S. Meyn. Quasi-stochastic approx-
imation and oﬀ-policy reinforcement learning. In Proc. of the IEEE Conf. on Dec. and Control, pages
5244–5251, Mar 2019.

[7] D. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Atena Scientiﬁc, Cambridge, Mass,

1996.

[8] D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena Scientiﬁc, 4th edition,

2012.

[9] D. P. Bertsekas. Reinforcement learning and optimal control. Athena Scientiﬁc, 2019.

[10] C. M. Bishop. Pattern recognition and machine learning. Springer, 2006.

24

[11] V. S. Borkar. Convex analytic methods in Markov decision processes. In Handbook of Markov decision
processes, volume 40 of Internat. Ser. Oper. Res. Management Sci., pages 347–375. Kluwer Acad. Publ.,
Boston, MA, 2002.

[12] V. S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint (2nd ed., to appear). Hin-

dustan Book Agency, Delhi, India and Cambridge, UK, 2020.

[13] V. S. Borkar and V. Gaitsgory. Linear programming formulation of long-run average optimal control

problem. Journal of Optimization Theory and Applications, 181(1):101–125, 2019.

[14] V. S. Borkar, V. Gaitsgory, and I. Shvartsman. LP formulations of discrete time long-run average optimal
control problems: The non ergodic case. SIAM Journal on Control and Optimization, 57(3):1783–1817,
2019.

[15] V. S. Borkar and S. P. Meyn. The ODE method for convergence of stochastic approximation and
reinforcement learning. SIAM J. Control Optim., 38(2):447–469, 2000. (see also IEEE CDC, 1998).

[16] S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan. Linear Matrix Inequalities in System and Control

Theory, volume 15. SIAM, 1994.

[17] W. L. Brogan. Modern control theory. Pearson, 3rd edition, 1990.

[18] S. Chen, A. M. Devraj, A. Buˇsi´c, and S. Meyn. Zap Q Learning with nonlinear function approximation.

Submitted for publication and arXiv e-prints 1910.05405, 2019.

[19] Y. Chen, A. Bernstein, A. Devraj, and S. Meyn. Model-Free Primal-Dual Methods for Network Op-
timization with Application to Real-Time Optimal Power Flow. In American Control Conference and
arXiv, page arXiv:1909.13132, Sept. 2019.

[20] D. P. de Farias and B. Van Roy. The linear programming approach to approximate dynamic program-

ming. Operations Res., 51(6):850–865, 2003.

[21] D. P. De Farias and B. Van Roy. On constraint sampling in the linear programming approach to

approximate dynamic programming. Mathematics of operations research, 29(3):462–478, 2004.

[22] D. P. de Farias and B. Van Roy. A cost-shaping linear program for average-cost approximate dynamic

programming with performance guarantees. Math. Oper. Res., 31(3):597–620, 2006.

[23] C. Derman. Finite State Markovian Decision Processes, volume 67 of Mathematics in Science and

Engineering. Academic Press, Inc., 1970.

[24] A. M. Devraj. Reinforcement Learning Design with Optimal Learning Rate. PhD thesis, University of

Florida, 2019.

[25] A. M. Devraj, A. Buˇsi´c, and S. Meyn. Fundamental design principles for reinforcement learning algo-

rithms. In Handbook on Reinforcement Learning and Control. Springer, 2020.

[26] A. M. Devraj and S. P. Meyn. Fastest convergence for Q-learning. ArXiv e-prints, July 2017.

[27] A. M. Devraj and S. P. Meyn. Zap Q-learning. In Proceedings of the 31st International Conference on

Neural Information Processing Systems, 2017.

[28] Y. Feng, L. Li, and Q. Liu. A kernel loss for solving the Bellman equation. In Advances in Neural

Information Processing Systems, pages 15456–15467, 2019.

[29] V. Gaitsgory, A. Parkinson, and I. Shvartsman. Linear programming formulations of deterministic
inﬁnite horizon optimal control problems in discrete time. Discrete and Continuous Dynamical Systems
- Series B, 22(10):3821 – 3838, 2017.

25

[30] V. Gaitsgory and M. Quincampoix. On sets of occupational measures generated by a deterministic
control system on an inﬁnite time horizon. Nonlinear Analysis: Theory, Methods and Applications,
88:27 – 41, 2013.

[31] A. Gupta, R. Jain, and P. W. Glynn. An empirical algorithm for relative value iteration for average-cost

MDPs. In IEEE Conference on Decision and Control, pages 5079–5084, 2015.

[32] D. Hern´andez-Hern´andez, O. Hern´andez-Lerma, and M. Taksar. The linear programming approach to

deterministic optimal control problems. Applicationes Mathematicae, 24(1):17–33, 1996.

[33] O. Hern´andez-Lerma and J. B. Lasserre. Discrete-time Markov control processes, volume 30 of Appli-
cations of Mathematics (New York). Springer-Verlag, New York, 1996. Basic optimality criteria.

[34] O. Hern´andez-Lerma and J. B. Lasserre. The linear programming approach. In Handbook of Markov
decision processes, volume 40 of Internat. Ser. Oper. Res. Management Sci., pages 377–407. Kluwer
Acad. Publ., Boston, MA, 2002.

[35] A. Kamoutsi, T. Sutter, P. Mohajerin Esfahani, and J. Lygeros. On inﬁnite linear programming and the
moment approach to deterministic inﬁnite horizon discounted optimal control problems. IEEE Control
Systems Letters, 1(1):134–139, July 2017.

[36] P. Karmakar and S. Bhatnagar. Dynamics of stochastic approximation with iterate-dependent Markov
noise under veriﬁable conditions in compact state space with the stability of iterates not ensured. arXiv
e-prints, page arXiv:1601.02217, Jan 2016.

[37] L. Kuipers and H. Niederreiter. Uniform distribution of sequences. Courier Corporation, 2012.

[38] S. Lange, T. Gabel, and M. Riedmiller. Batch reinforcement learning. In Reinforcement learning, pages

45–73. Springer, 2012.

[39] J.-B. Lasserre. Moments, positive polynomials and their applications, volume 1. World Scientiﬁc, 2010.

[40] D. Lee and N. He. A uniﬁed switching system perspective and ODE analysis of Q-learning algorithms.

arXiv, page arXiv:1912.02270, 2019.

[41] L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Ma-

chine learning, 8(3-4):293–321, 1992.

[42] H. R. Maei, C. Szepesv´ari, S. Bhatnagar, and R. S. Sutton. Toward oﬀ-policy learning control with func-
tion approximation. In Proceedings of the 27th International Conference on International Conference
on Machine Learning, ICML’10, pages 719–726, USA, 2010. Omnipress.

[43] A. S. Manne. Linear programming and sequential decisions. Management Sci., 6(3):259–267, 1960.

[44] P. G. Mehta and S. P. Meyn. Q-learning and Pontryagin’s minimum principle. In Proc. of the IEEE

Conf. on Dec. and Control, pages 3598–3605, Dec. 2009.

[45] F. S. Melo, S. P. Meyn, and M. I. Ribeiro. An analysis of reinforcement learning with function approx-
imation. In ICML ’08: Proceedings of the 25th international conference on Machine learning, pages
664–671, New York, NY, USA, 2008. ACM.

[46] S. P. Meyn. Control Techniques for Complex Networks. Cambridge University Press, 2007. Pre-

publication edition available online.

[47] S. P. Meyn and R. L. Tweedie. Markov chains and stochastic stability. Cambridge University Press,
Cambridge, second edition, 2009. Published in the Cambridge Mathematical Library. 1993 edition
online.

[48] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.

Asynchronous methods for deep reinforcement learning. CoRR, abs/1602.01783, 2016.

26

[49] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. A. Riedmiller.

Playing Atari with deep reinforcement learning. ArXiv, abs/1312.5602, 2013.

[50] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. A.
Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,
D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement
learning. Nature, 518:529–533, 2015.

[51] A. W. Moore. Eﬃcient memory-based learning for robot control. PhD thesis, University of Cambridge,

Computer Laboratory, 1990.

[52] D. Ormoneit and P. Glynn. Kernel-based reinforcement learning in average-cost problems.

IEEE

Transactions on Automatic Control, 47(10):1624–1636, Oct 2002.

[53] I. Osband, B. Van Roy, D. Russo, and Z. Wen. Deep exploration via randomized value functions. arXiv

preprint arXiv:1703.07608, 2017.

[54] I. Osband, B. Van Roy, and Z. Wen. Generalization and exploration via randomized value functions.

In International Conference on Machine Learning, pages 2377–2386, 2016.

[55] M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley &

Sons, 2014.

[56] A. Ramaswamy and S. Bhatnagar. A generalization of the Borkar-Meyn Theorem for stochastic recursive

inclusions. Mathematics of Operations Research, 42(3):648–661, 2017.

[57] A. Ramaswamy and S. Bhatnagar. Stability of stochastic approximations with ‘controlled Markov’ noise

and temporal diﬀerence learning. IEEE Transactions on Automatic Control, pages 1–1, 2018.

[58] H. Sharma, R. Jain, and A. Gupta. An empirical relative value learning algorithm for non-parametric
MDPs with continuous state space. In European Control Conference, pages 1368–1373. IEEE, 2019.

[59] S. D.-C. Shashua and S. Mannor. Kalman meets Bellman: Improving policy evaluation through value

tracking. arXiv preprint arXiv:2002.07171, 2020.

[60] S. Shirodkar and S. Meyn. Quasi stochastic approximation. In Proc. of the 2011 American Control

Conference (ACC), pages 2429–2435, July 2011.

[61] R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press. On-line edition at
http://www.cs.ualberta.ca/~sutton/book/the-book.html, Cambridge, MA, 2nd edition, 2018.

[62] R. S. Sutton. Temporal Credit Assignment in Reinforcement Learning. PhD thesis, University of

Massachusetts, Amherst, 1984.

[63] L. Vandenberghe and S. Boyd. Applications of semideﬁnite programming. Applied Numerical Mathe-
matics, 29(3):283 – 299, 1999. Proceedings of the Stieltjes Workshop on High Performance Optimization
Techniques.

[64] R. Vinter. Convex duality and nonlinear optimal control. SIAM Journal on Control and Optimization,

31(2):518–21, 03 1993.

[65] Y. Wang and S. Boyd. Performance bounds for linear stochastic control. Systems Control Lett.,

58(3):178–182, 2009.

27

Appendices

A Convex programs for value functions

Suppose that the assumptions of Prop. 2.1 hold: the value function J (cid:63) deﬁned in (2) is
Lemma A.1.
ﬁnite-valued, inf-compact, and vanishes only at xe. Then, for each x ∈ X and stationary policy φ for which
J φ(x) < ∞,

J φ(x(k)) = 0

lim
k→∞

x(k) = xe

lim
k→∞

Proof. With u(k) = φ(x(k)) for each k, we have the simple dynamic programming equation:

J φ(x) =

N −1
(cid:88)

k=0

c(x(k), u(k)) + J φ(x(N ))

On taking the limit as N → ∞ we obtain

J φ(x) =

∞
(cid:88)

k=0

c(x(k), φ(x(k))) + lim
N→∞

J φ(x(N ))

It follows that limN→∞ J φ(x(N )) = 0.

Now, using J φ ≥ J (cid:63), it follows that limN→∞ J (cid:63)(x(N )) = 0 as well. It is here that we apply the inf-
compact assumption, along with continuity of J (cid:63), with together imply the desired limit limN→∞ x(N ) = xe.
(cid:117)(cid:116)

Proof of Prop. 2.1. Since µ is non-negative but otherwise arbitrary, to prove the proposition it is both
necessary and suﬃcient to establish the bound J ≤ J (cid:63) for each feasible (J, Q).

The constraints eqs. (23b) and (23c) then give, for any input-state sequence, and any feasible (J, Q),

J(x(k)) ≤ Q(x(k), u(k)) ≤ c(x(k), u(k)) + J(F(x(k), u(k))) = c(x(k), u(k)) + J(x(k + 1))

This bound can be iterated to obtain, for any N ≥ 1,

J(x) ≤

N −1
(cid:88)

k=0

c(x(k), u(k)) + J(x(N )) ,

x = x(0) ∈ X

We now apply Lemma A.1: Fix x ∈ X, and a stationary policy φ for which J φ(x) < ∞. With u(k) = φ(x(k))
for each k in the preceding bound we obtain

J(x) ≤ J φ(x) + lim
N→∞

J(x(N )) ,

x = x(0) ∈ X

Continuity of J combined with Lemma A.1 then implies that the limit on the right hand side is J(xe) = 0,
giving J(x) ≤ J φ(x). It follows that J(x) ≤ J (cid:63)(x) for all x as claimed.
(cid:117)(cid:116)

Proof of Prop. 2.3. The reader is referred to standard texts for the derivation of the ARE [2, 17]. The
following is a worthwhile exercise: postulate that J (cid:63) is a quadratic function of x, and you will ﬁnd that the
Bellman equation implies the ARE.

Now, on to the derivation of (30). The variables in the linear program introduced in Prop. 2.1 consist of

functions J and Q. For the LQR problem we restrict to quadratic functions:

and treat the symmetric matrices (M, M Q) as variables.

J(x) = x

(cid:124)

M x ,

Q(x, u) = z

(cid:124)

M Qz

28

To establish (30) we are left to show 1) the objective functions (23a) and (30a) coincide for some µ, and
2) the functional constraints (23b, 23c) are equivalent to the matrix inequality (30b). The ﬁrst task is the
simplest:

trace (M ) =

J(ei) = (cid:104)µ, J(cid:105)

n
(cid:88)

with {ei} the standard basis elements in Rn, and µ(ei) = 1 for each i.

The equivalence of (30b) and (23b, 23c) is established next, and through this we also obtain (29c). In

view of the discussion preceding (24), the inequality constraint (23b) can be strengthened to equality:

i=1

Q(cid:63)(x, u) = c(x, u) + J (cid:63)(F x + Gu)

(80)

It remains to establish the equivalence of (30b) and (24).

Applying (80), we obtain a mapping from M to M Q. Denote

M J =

(cid:21)

(cid:20)M 0
0
0

, Ξ =

(cid:20)F G
(cid:21)
F G

giving for all x and z(cid:124) = (x(cid:124), u(cid:124)),

J(x) = x

(cid:124)

M x = z

(cid:124)

M J z ,

J(F x + Gu) = z

(cid:124)

(cid:124)
Ξ

M J Ξz

This and (80) gives, for any z,

(cid:124)

z

M Qz = Q(x, u) = c(x, u) + J(F x + Gu)
(cid:124)
M J Ξz

M cz + z

(cid:124)
Ξ

= z

(cid:124)

The desired mapping from M to M Q then follows, under the standing assumption that M Q is a symmetric
matrix:

M Q = M c + Ξ

(cid:124)

M J Ξ =

(cid:21)

(cid:20)S 0
0 R

+

(cid:20)F T M F F T M G
GT M F GT M G

(cid:21)

The constraint (24) is thus equivalent to

(cid:124)

z

M J z = J(x) ≤ Q(x, u) = z

(cid:124)

M Qz ,

for all z

This is equivalent to the constraint M J ≤ M Q, which is (30b).

(cid:117)(cid:116)

B Limit theory for Convex Q learning and DQN

The proof of Prop. 2.4 and surrounding results is based on an ODE approximation for the parameter
estimates, which is made possible by (Aξ).

The step-size assumption in Prop. 2.4 is introduced to simplify the following ODE approximation.

Lemma B.1. The following conclusions hold under the assumptions of Prop. 2.4:

(i) For any n, n0 ≥ 1, and any function g : Z → R,

n+n0−1
(cid:88)

k=n

g(Φ(k))αk =

n+n0−1
(cid:88)

k=n

gkαk + α1[gn+n0 − gn]

(ii) Suppose that we have a sequence of functions satisfying, for some B < ∞,

g(k, · ) ∈ GL ,

|g(k, z) − g(k − 1, z)| ≤ Bαk ,

z ∈ Z , k ≥ 1

29

Then, (i) admits the extension

n+n0−1
(cid:88)

k=n

g(k, Φ(k))αk = g(n)

n+n0−1
(cid:88)

k=n

αk + ε(n, n0; B, L)

(81)

with g(n) = E(cid:36)[g(n, Φ)], as deﬁned in (33). The error sequence depends only on B, L, and Φ(0), and
with these variables ﬁxed satisﬁes

lim
n→∞

ε(n, n0; B, L) = 0

(cid:117)(cid:116)

These approximations easily lead to ODE approximations for many of the algorithms introduced in this

paper. However, there is a slight mismatch: for the batch algorithms, we obtain recursions of the form

θn+1 = θn + αn+1{f (θn, Φ(r)(n)) + ε(θn)}

(82)

where (cid:107)ε(θn)(cid:107) ≤ o(1)(cid:107)θn(cid:107), with o(1) → 0, and for n ≥ 1, r ≥ 1,

Φ(r)(n) = (Φ(n), . . . Φ(n + r − 1))

Assumption (Aξ) implies the same ergodic theorems for this larger state process:

Lemma B.2. Under (Aξ), the following limit exists for any continuous function g : Zr → R:

g := lim
N→∞

1
N

N
(cid:88)

k=1

g(Φ(k), . . . , Φ(k + r − 1))

(cid:117)(cid:116)

The proof is straightforward since g(Φ(k), . . . , Φ(k + r − 1)) = g(r)(Φ(k)) for some continuous function

g(r) : Z → R, giving g = E(cid:36)[g(r)(Φ)].

Applying (56) of Lemma 3.1 we arrive at the approximate QSA recursion (82), in which

f (θn, Φ(r)(n)) = − 1

2 ∇E ∞

n (θn)

The superscript indicates that rn, κε

n, κ+

n are replaced with their limits, giving

∇E ∞

n (θn) = 2

1
r

Tn+r−1
(cid:88)

(cid:110)

k=Tn

− (cid:104)µ, ψJ (cid:105) + D◦

k+1(θ)(cid:2)ψJ

(k+1) − ψ(k)

(cid:3)

The function f is Lipschitz continuous on Rd × Z. We let f denote its limit:

+ κ+{J θ(x(k)) − Qθ(x(k))}+

(cid:2)ψJ

(k) − ψ(k)

(cid:3)(cid:111)

f (θ) = lim
N→∞

1
N

N
(cid:88)

k=1

f (θk, Φ(r)(k)) = − 1

2 ∇E(cid:36)[E ∞

n (θ)] ,

θ ∈ Rd

The ODE of interest is then deﬁned by (35) with this f :

d
dt ϑt = f (ϑt)

Recall the deﬁnition of the “sampling times” {Tn} provided in (36), and the deﬁnition of the ODE approxi-
mation in (37).

Lemma B.3. Under the assumptions of Prop. 2.4, there is a ﬁxed σθ < ∞ such that for each initial
condition Φ(0),

lim sup
n→∞

(cid:107)θn(cid:107) ≤ σθ

30

Proof. This is established via the scaled ODE technique of [15, 12, 56, 57]. For each n ≥ 0 denote sn =
max(1, (cid:107)ΘTn (cid:107)), and

Θn

t =

1
sn

Θt,

t ≥ Tn ,

f n(θ, z) =

1
sn

f (snθ, z) ,

n

f

(θ) =

1
sn

f (snθ)

Lemmas B.1 and B.2 justify the approximation

Θn

Tn+t = Θn

Tn +

(cid:90) Tn+t

Tn

n

f

(Θn

r ) dr + εn
t ,

0 ≤ t ≤ T

(83)

where sup{(cid:107)εn

t (cid:107) : t ≤ T } converges to zero as n → ∞.

For the proof here we require a modiﬁcation of the deﬁnition of the ODE approximation, since we are
t : t ≥ Tn} the solution to the ODE (35)

approximating the scaled process Θn rather than Θ. Denote by {ϑn
with initial condition

Tn = Θn
ϑn

Tn = s−1

n ΘTn ,

sn = max(1, (cid:107)ΘTn (cid:107))

An application of the Bellman-Gronwall Lemma is used to obtain the ODE approximation:

lim
n→∞

sup
Tn≤t≤Tn+T

(cid:107)ϑn

t − Θn

t (cid:107) = 0

Uniformity in this step depends on the fact that (cid:107)ϑn
Tn
ODE are approximated by gradient descent with cost function equal to zero:

(cid:107) ≤ 1. Next, recognize that the dynamics of the scaled

s

lim
s→∞

f

(θ) = − 1

2 ∇ ¯E 0(θ)

in which

¯E 0(θ) = E(cid:36)[E ε,0

n (θ) + κ+E +

n (θ)]

(84)

with E +

n deﬁned in (47b), but with the cost function removed in (47a):

n (θ) = {−Qθ(x(k), u(k)) + J θ(x(k + 1))(cid:9)2
E ε,0

The function ¯E 0 is strongly convex, with unique minimum at θ = 0. From this we obtain a drift condition:
for T chosen suﬃciently large we have for some constant bθ,

We then obtain a similar contraction for the unscaled recursion: for all n ≥ 1 suﬃciently large,

(cid:107)ϑn

Tn+1

(cid:107) ≤ 1

4 (cid:107)ϑn
Tn

(cid:107)

whenever (cid:107)ϑn
Tn

(cid:107) ≥ bθ

(cid:107)ΘTn+1(cid:107) ≤ 1

2 (cid:107)ΘTn (cid:107)

whenever (cid:107)ΘTn (cid:107) ≥ bθ

This implies that {ΘTn } is a bounded sequence, and Lipschitz continuity then implies boundedness of {Θt :
(cid:117)(cid:116)
t ≥ 0} and hence also {θk : k ≥ 0}.

Proof of Prop. 2.4. The boundedness result Lemma B.3 was the hard part. Convergence of the algorithm
follows from boundedness, combined with the ODE approximation techniques of [12] (which are based on
(cid:117)(cid:116)
arguments similar to those leading to (83)).

Proof of Corollary 2.5. We require a representation similar to (82) in order to establish boundedness of {θn},
and then apply standard stochastic approximation arguments. For this we write Θ = {θ ∈ R : Y θ ≤ 0} for a
matrix Y of suitable dimension. This is possible under the assumption that the constraint set is a polyhedral
cone. We then obtain via a Lagrangian relaxation of (51a),

0 = ∇θ

(cid:110)

−(cid:104)µ, J θ(cid:105) + κεE ε

(cid:124)
nzε
n(θ) − λ

n(θ) + γ

(cid:124)
n+1Y θ +

1
αn+1

1

2 (cid:107)θ − θn(cid:107)2(cid:111)(cid:12)

(cid:12)
(cid:12)θ=θn+1

31

where γn+1 ≥ 0 satisﬁes the KKT condition γ
following the argument in Lemma 3.1,

(cid:124)
n+1Y θn+1 = 03. We thus obtain the desired approximation:

θn+1 = θn − αn+1

(cid:8)−(cid:104)µ, ψJ (cid:105) + κε∇E ε

n(θn) − λ

(cid:124)
n∇zε

n(θn) + Y

(cid:124)

γn+1 + εn+1

(cid:9)

(85)

where (cid:107)εn+1(cid:107) = O(αn+1(cid:107)θn(cid:107)).

Using the deﬁnition of the interpolated process Θ deﬁned above (37), we obtain the approximation

ΘTn+t = ΘTn −

λTn+t = λTn −

(cid:90) Tn+t

Tn
(cid:90) Tn+t

Tn

(cid:110)

−(cid:104)µ, ψJ (cid:105) + κε∇ ¯E ε(Θr) − [∇¯zε(Θr)]

(cid:124)λr

(cid:90) Tn+t

(cid:111)

dr −

Tn

(cid:124)

Y

dγr + eθ(t, n)

¯zε(Θr) dr +

(cid:90) Tn+t

Tn

dγ+

r −

(cid:90) Tn+t

Tn

dγ−

r + ez(t, n)

(86)

where {γr} is a vector valued processes with non-decreasing components, and {γ±
decreasing processes (arising from the projection in (51b)). The error processes satisfy

r } are scalar-valued non-

sup{(cid:107)eθ(t, n)(cid:107) + (cid:107)et(t, n)(cid:107) : 0 ≤ t ≤ T } = o(1)(cid:107)ΘTn (cid:107)

for any ﬁxed T

The proof of boundedness of {θn} is then obtained exactly exactly as in Lemma B.3: the “large state”

scaling results in the approximation by a linear system with projection:

Tn+t = ϑn
ϑn
Tn

(cid:90) Tn+t

−

M ϑn

r dr −

(cid:90) Tn+t

(cid:124)

Y

dΓr

Tn
where M θ = ∇ ¯E 0(θ), and the mean quadratic loss is deﬁned in (84) with κ+ = 0. Note that ϑn
and as in the proof of Lemma B.3, we have replaced the vector ﬁeld f
The process Γ has non-decreasing components, satisfying for all t,

used there with its limit f

Tn

n

Tn = s−1
n ΘTn ,
∞
.

(cid:90) Tn+t

0 =

Tn

{Y ϑn

r }(cid:124)

dΓr

Stability of the ODE follows using the Lyapunov function V (θ) = 1

dV (ϑn

r ) = ϑn
r

(cid:124)

dϑn

r = −ϑn
r

(cid:124)

M ϑn

r dr − ϑn
r

(cid:124)

(cid:124)

Y

2 (cid:107)θ(cid:107)2: for r ≥ Tn,
dΓr = −ϑn
r

M ϑn

r dr

(cid:124)

We have M > 0, so it follows that ϑn

r → 0 exponentially fast as r → ∞.

The arguments in Lemma B.3 then establish boundedness of {θn}, and then standard arguments imply
an ODE approximation without scaling, where the ODE approximation of (86) is the same set of equations,
with the error processes removed:

ϑTn+t = ΘTn −

ΛTn+t = λTn −

(cid:90) Tn+t

Tn
(cid:90) Tn+t

Tn

∇θ ¯L(ϑr, Λr) −

(cid:90) Tn+t

(cid:124)

dΓr

Y

Tn
(cid:90) Tn+t

∇λ ¯L(ϑr, Λr) dr +

Tn

dΓ +

r −

(cid:90) Tn+t

Tn

dΓ −
r

where ¯L is deﬁned in (52). For this we adapt analysis in [12, Section 1.2], where a similar ODE arises.

The stability proof of [12, Section 1.2] amounts to showing V (t) = 1

2 {(cid:107)ϑt − θ∗(cid:107)2 + (cid:107)Λt − λ∗(cid:107)2} is a

Lyapunov function:

(cid:90) t1

t0

V (t) dt < 0 , whenever (ϑt, Λt) (cid:54)= (θ∗, λ∗),

(87)

and any t1 > t0 ≥ Tn. The arguments there are for an unreﬂected ODE, but the proof carries over to the
more complex setting:

dV (t) = (cid:104)ϑt − θ∗, dϑt(cid:105) + (cid:104)Λt − λ∗, dΛt(cid:105)
t (cid:105) + (cid:104)Λt − λ∗, dΛ0
t (cid:105)

≤ (cid:104)ϑt − θ∗, dϑ0

3We are taking the gradient of a quadratic function of θ, so there is a simple formula for the Lagrange multiplier γn+1.

32

where the super-script “0” refers to diﬀerentials with the reﬂections removed:

dϑ0

t = −∇θ ¯L(ϑt, Λt) dt ,

dΛ0

t = ∇λ ¯L(ϑt, Λt) dt

The inequality above is justiﬁed by the reﬂection process characterizations:

(cid:124)
ϑ
t Y
θ∗(cid:124)
Y

(cid:124)

(cid:124)

From this we obtain

dΓt = 0 , Λ
dΓt ≤ 0 ,

λ∗(cid:124)

(cid:124)
t dΓ +
dΓ +

(cid:124)
(cid:124)
t dΓ −
t = (λmax)
t = 0 , Λ
λ∗(cid:124)
dΓ −
t ≤ (λmax)
t ≥ 0 ,

dΓ −
t
(cid:124)
dΓ −
t

dV (t) ≤ −(cid:104)ϑt − θ∗, ∇θ ¯L(ϑt, Λt)(cid:105) dt + (cid:104)Λt − λ∗, ∇λ ¯L(ϑt, Λt)(cid:105) dt
≤ −[ ¯L(ϑt , λ∗) − ¯L(θ∗ , λ∗)] dt − [ ¯L(θ∗ , λ∗) − ¯L(θ∗ , Λt)] dt

where the second inequality follows from convexity of ¯L in θ, and linearity in λ. This establishes the desired
(cid:117)(cid:116)
negative drift (87).

Proof of Prop. 2.6. Let LQ > 0 denote the Lipschitz constant for ∇Qθ. It follows that by increasing the
constant we obtain two bounds: for all θ, θ(cid:48) ∈ Rd,

(cid:107)∇Qθ(x, u) − ∇Qθ(cid:48)

(x, u)(cid:107) ≤ LQ(cid:107)θ − θ(cid:48)(cid:107) ,

(cid:107)Qθ(x, u) − Qθ(cid:48)

(x, u)(cid:107) ≤ LQ(cid:107)θ − θ(cid:48)(cid:107)(1 + (cid:107)θ(cid:107) + (cid:107)θ(cid:48)(cid:107))

(88)

The second bound follows from the identity Qrθ(x, u) = Qθ(x, u) + (cid:82) r
useful to prove that DQN is convergent – recall that convergence is assumed in the proposition).

1 θ(cid:124)∇Qsθ(x, u) ds, r ≥ 1 (and is only

The proof begins with an extension of Lemma 3.1, to express the DQN recursion as something resembling

stochastic approximation. This will follow from the ﬁrst-order condition for optimality:

0 = ∇

(cid:110)

E ε
n(θ) +

1
αn+1

(cid:107)θ − θn(cid:107)2(cid:111)(cid:12)
(cid:12)
(cid:12)θ=θn+1

= −2

1
r

+

Tn+1−1
(cid:88)

(cid:2)−Qθ(x(k), u(k)) + c(x(k), u(k)) + Qθn (x(k + 1))(cid:3)∇Qθ(x(k), u(k))

k=Tn
1
αn+1

(cid:0)θn+1 − θn

(cid:1)

(cid:12)
(cid:12)
(cid:12)θ=θn+1

It is here that we apply the Lipschitz continuity bounds in (88). This allows us to write

2

1
αn+1

(cid:0)θn+1 − θn

(cid:1) = 2

1
r

Tn+1−1
(cid:88)

(cid:2)−Qθk (x(k), u(k)) + c(x(k), u(k)) + Qθk (x(k + 1))(cid:3)ζk + εn

k=Tn

where (cid:107)εn(cid:107) = O(αn) under the assumption that {θn} is bounded. This brings us to a representation similar
to (14):

θn+1 = θn + αn+1

(cid:110) 1
r

Tn+1−1
(cid:88)

(cid:2)−Qθk (x(k), u(k)) + c(x(k), u(k)) + Qθk (x(k + 1))(cid:3)ζk + εn

(cid:111)

k=Tn

Stochastic approximation/Euler approximation arguments then imply the ODE approximation, and the
(cid:117)(cid:116)
representation of the limit.

33

