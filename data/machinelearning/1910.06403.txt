0
2
0
2

c
e
D
8

]

G
L
.
s
c
[

3
v
3
0
4
6
0
.
0
1
9
1
:
v
i
X
r
a

BOTORCH: A Framework for Efﬁcient Monte-Carlo
Bayesian Optimization

Maximilian Balandat
Facebook
balandat@fb.com

Brian Karrer
Facebook
briankarrer@fb.com

Daniel R. Jiang
Facebook
drjiang@fb.com

Samuel Daulton
Facebook
sdaulton@fb.com

Benjamin Letham
Facebook
bletham@fb.com

Andrew Gordon Wilson
New York University
andrewgw@cims.nyu.edu

Eytan Bakshy
Facebook
ebakshy@fb.com

Abstract

Bayesian optimization provides sample-efﬁcient global optimization for a broad
range of applications, including automatic machine learning, engineering, physics,
and experimental design. We introduce BOTORCH, a modern programming frame-
work for Bayesian optimization that combines Monte-Carlo (MC) acquisition
functions, a novel sample average approximation optimization approach, auto-
differentiation, and variance reduction techniques. BOTORCH’s modular design
facilitates ﬂexible speciﬁcation and optimization of probabilistic models written
in PyTorch, simplifying implementation of new acquisition functions. Our ap-
proach is backed by novel theoretical convergence results and made practical by
a distinctive algorithmic foundation that leverages fast predictive distributions,
hardware acceleration, and deterministic optimization. We also propose a novel
“one-shot” formulation of the Knowledge Gradient, enabled by a combination of
our theoretical and software contributions. In experiments, we demonstrate the
improved sample efﬁciency of BOTORCH relative to other popular libraries.

1

Introduction

Computational modeling and machine learning (ML) have led to an acceleration of scientiﬁc in-
novation in diverse areas, ranging from drug design to robotics to material science. These tasks
often involve solving time- and resource-intensive global optimization problems to achieve optimal
performance. Bayesian optimization (BO) [75, 46, 76], an established methodology for sample-
efﬁcient sequential optimization, has been proposed as an effective solution to such problems, and has
been applied successfully to tasks ranging from hyperparameter optimization [24, 92, 110], robotic
control [15, 5], chemical design [36, 60, 111], and tuning and policy search for internet-scale software
systems [4, 58, 57, 23]. Meanwhile, ML research has been undergoing a revolution driven largely
by new programming frameworks and hardware that reduce the time from ideation to execution
[43, 16, 1, 81]. While BO has become rich with new methodologies, today there is no coherent
framework that leverages these computational advances to simplify and accelerate BO research in
the same way that modern frameworks have for deep learning. In this paper, we address this gap by
introducing BOTORCH, a modular and scalable Monte Carlo (MC) framework for BO that is built
around modern paradigms of computation, and theoretically grounded in novel convergence results.
Our contributions include:

• A novel approach to optimizing MC acquisition functions that effectively combines with determin-

istic higher-order optimization algorithms and variance reduction techniques.

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
• The ﬁrst convergence results for sample average approximation (SAA) of MC acquisition functions,

including novel general convergence results for SAA via randomized quasi-MC.

• A new, SAA-based “one-shot” formulation of the Knowledge Gradient, a look-ahead acquisition

function, with improved performance over the state-of-the-art.

• Composable model-agnostic abstractions for MC BO that leverage modern computational tech-
nologies, including auto-differentiation and scalable parallel computation on CPUs and GPUs.

We discuss related work in Section 2 and then present the methodology underlying BOTORCH in
Sections 3 and 4. Details of the BOTORCH framework, including its modular abstractions and
implementation examples, are given in Section 5. Numerical results are provided in Section 6.

2 Background and Related Work

In BO, we aim to solve maxx∈X ftrue(x), where ftrue is an expensive-to-evaluate function and
X ⊂ Rd is a feasible set. BO consists of two main components: a probabilistic surrogate model
of the observed function—most commonly, a Gaussian process (GP)—and an acquisition function
that encodes a strategy for navigating the exploration vs. exploitation trade-off [92]. Taking a
model-agnostic view, our focus in this paper is on MC acquisition functions.

Popular libraries for BO include Spearmint [94], GPyOpt [98], Cornell-MOE [106], RoBO [52],
Emukit [97], and Dragonﬂy [49]. We provide further discussion of these packages in Appendix A.
Two other libraries, ProBO [72] and GPFlowOpt [55], are of particular relevance. ProBO is a
recently suggested framework1 for using general probabilistic programming in BO. While its model-
agnostic approach is similar to ours, ProBO, unlike BOTORCH, does not beneﬁt from gradient-based
optimization provided by differentiable programming, or algebraic methods designed to exploit
GPU acceleration. GPFlowOpt inherits support for auto-differentiation and hardware acceleration
from TensorFlow [via GPFlow, 64], but unlike BOTORCH, it does not use algorithms designed to
speciﬁcally exploit this potential. Neither ProBO nor GPFlowOpt naturally support MC acquisition
functions. In contrast to all existing libraries, BOTORCH is a modular programming framework and
employs novel algorithmic approaches that achieve a high degree of ﬂexibility and performance.

The MC approach to optimizing acquisition functions has been considered in the BO literature to an
extent, typically using stochastic methods for optimization [100, 106, 109, 104]. Our work takes the
distinctive view of sample average approximation (SAA), an approach that combines sampling with
deterministic optimization and variance reduction techniques. To our knowledge, we provide the ﬁrst
theoretical analysis and systematic implementation of this approach in the BO setting.

3 Monte-Carlo Acquisition Functions

We begin by describing a general formulation of BO in the context of MC acquisition functions.
i=1, where xi ∈ X and yi = ftrue(xi) + vi(xi) with
Suppose we have collected data D = {(xi, yi)}n
vi some noise corrupting the true function value ftrue(xi). We allow ftrue to be multi-output, in
which case yi, vi ∈ Rm. In some applications we may also have access to distributional information
of the noise vi, such as its (possibly heteroskedastic) variance. Suppose further that we have
a probabilistic surrogate model f that for any x := {x1, . . . , xq} provides a distribution over
f (x) := (f (x1), . . . , f (xq)) and y(x) := (y(x1), . . . , y(xq)). We denote by fD(x) and yD(x) the
respective posterior distributions conditioned on data D. In BO, the model f traditionally is a GP, and
the vi are assumed i.i.d. normal, in which case both fD(x) and yD(x) are multivariate normal. The
MC framework we consider here makes no particular assumptions about the form of these posteriors.

The next step in BO is to optimize an acquisition function evaluated on fD(x) over the candidate
set x. Following [105, 7], many acquisition functions can be written as

α(x; Φ, D) = E(cid:2)a(g(f (x)), Φ) | D(cid:3),
(1)
where g : Rq×m → Rq is a (composite) objective function, Φ ∈ Φ are parameters independent of x in
some set Φ, and a : Rq × Φ → R is a utility function that deﬁnes the acquisition function.

In some situations, the expectation over fD(x) in (1) and its gradient ∇xα(x; Φ, D) can be computed
analytically, e.g. if one considers a single-output (m = 1) model, a single candidate (q = 1) point x, a

1No implementation of ProBO is available at the time of this writing.

2

Figure 1: MC acquisition functions. Samples ξi
evaluated in parallel and averaged as in (2). All operations are fully differentiable.

D from the posterior fD(x) provided by the model f at x are

Gaussian posterior fD(x) = N (µx, σ2
x), and the identity objective g(f ) ≡ f . Expected Improvement
(EI) is a popular acquisition function that maximizes the expected difference between the currently
observed best value f ∗ (assuming noiseless observations) and f at the next query point, through the
utility a(f, f ∗) = max(f − f ∗, 0). EI and its gradient have a well-known analytic form [46].

In general, analytic expressions are not available for arbitrary objective functions g(·), utility functions
a(· , ·), non-Gaussian model posteriors, or collections of points x which are to be evaluated in a
parallel or asynchronous fashion [32, 94, 106, 100, 104]. Instead, MC integration can be used to
approximate the expectation (1) using samples from the posterior. An MC approximation ˆαN (x; Φ, D)
of (1) using N samples ξi

D(x) ∼ fD(x) is straightforward:

ˆαN (x; Φ, D) =

1
N

N
(cid:88)

i=1

a(g(ξi

D(x)), Φ).

(2)

The obvious way to evaluate (2) is to draw i.i.d. samples ξi
D(x). Alternatively, randomized quasi-
Monte Carlo (RQMC) techniques [14] can be used to signiﬁcantly reduce the variance of the estimate
and its gradient (see Appendix E for additional details).

4 MC Bayesian Optimization via Sample Average Approximation

To generate a new candidate set x, one must optimize the acquisition function α. Doing this effectively,
especially in higher dimensions, typically requires using gradient information. For differentiable
analytic acquisition functions (e.g. EI, UCB), one can either manually implement gradients, or use
auto-differentiation to compute ∇xα(x; Φ, D), provided one can differentiate through the posterior
parameters (as is the case for Gaussian posteriors).

4.1 Optimizing General MC Acquisition Functions

An unbiased estimate of the MC acquisition function gradient ∇xα(x; Φ, D) can often be obtained
from (2) via the reparameterization trick [50, 85]. The basic idea is that ξ ∼ fD(x) can be expressed as
a suitable (differentiable) deterministic transformation ξ = hD(x, (cid:15)) of an auxiliary random variable (cid:15)
independent of x. For instance, if fD(x) ∼ N (µx, Σx), then hD(x, (cid:15)) = µx +Lx(cid:15), with (cid:15) ∼ N (0, I)
and LxLT
x = Σx. If a(·, Φ) and g(·) are differentiable, then ∇xa(g(ξ), Φ) = ∇ga∇ξg∇xhD(x, (cid:15)).
Our primary methodological contribution is to take a sample average approximation [53] approach
to BO. The conventional way of optimizing MC acquisition functions of the form (2) is to re-draw
samples from (cid:15) for each evaluation and apply stochastic ﬁrst-order methods such as Stochastic
Gradient Descent (SGD) [105]. In our SAA approach, rather than re-drawing samples from (cid:15) for
each evaluation of the acquisition function, we draw a set of base samples E := {(cid:15)i}N
i=1 once,
and hold it ﬁxed between evaluations throughout the course of optimization (this can be seen as a
speciﬁc incarnation of the method of common random numbers). Conditioned on E, the resulting
MC estimate ˆαN (x; Φ, D) is deterministic. We then obtain the candidate set ˆx∗

N as

ˆx∗

N ∈ arg max

x∈Xq

ˆαN (x; Φ, D).

(3)

The gradient ∇x ˆαN (x; Φ, D) can be computed as the average of the sample-level gradients, exploiting
auto-differentiation. We emphasize that whether this average is a “proper” (i.e., unbiased, consistent)
estimator of ∇xα(x; Φ, D) is irrelevant for the convergence results we will derive below.

3

ModelSamplerPosteriorOBSERVATIONSCANDIDATE SETEXPECTEDUTILITY OF THE-CANDIDATE SETMONTE-CARLO ACQUISITION FUNCTIONObjectiveUtilityWhile the convergence properties of MC integration are well-studied [14], the respective literature on
SAA, i.e., convergence of the optimizer (3), is far less comprehensive. Here, we derive what, to the
best of our knowledge, are the ﬁrst SAA convergence results for (RQ)MC acquisition functions in the
context of BO. To simplify our exposition, we limit ourselves to GP surrogates and i.i.d. base samples;
more general results and proofs are presented in Appendix D. For notational simplicity, we will drop
the dependence of α and ˆαN on Φ and D for the remainder of this section. Let α∗ := maxx∈Xq α(x),
and denote by X ∗ the associated set of maximizers. Similarly, let ˆα∗
N := maxx∈Xq ˆαN (x). With this
we have the following key result:
Theorem 1. Suppose (i) X is compact, (ii) f has a GP prior with continuously differentiable mean
and covariance functions, and (iii) g(·) and a(·, Φ) are Lipschitz continuous. If the base samples
N , X ∗) → 0 a.s.. Under additional
{(cid:15)i}N
N → α∗ a.s., and (2) dist(ˆx∗
N , X ∗) > δ(cid:1) ≤ Ke−βN, ∀ N ≥ 1.
regularity conditions, (3) ∀ δ > 0, ∃ K < ∞, β > 0 s.t. P(cid:0)dist(ˆx∗

i=1 are i.i.d. N (0, 1), then (1) ˆα∗

Under relatively weak conditions,2 Theorem 1 ensures not only that the optimizer ˆx∗
N of ˆαN converges
to an optimizer of the true α with probability one, but also that the convergence (in probability)
happens at an exponential rate. We stated Theorem 1 informally and for i.i.d. base samples for
simplicity. In Appendix D.3 we give a formal statement, and extend it to base samples generated by a
family of RQMC methods, leveraging recent theoretical advances [79]. While at this point we do not
characterize improvements in theoretical convergence rates of RQMC over MC for SAA, we observe
empirically that RQMC methods work remarkably well in practice (see Figures 2 and 3).

Figure 2: MC and RQMC acquisition functions, with and
without (“ﬁxed”) re-drawing base samples between evalua-
tions. The model is a GP ﬁt on 15 points randomly sampled
from X = [0, 1]6 and evaluated on the Hartmann6 function
along the slice x(λ) = λ1.

Figure 3: Empirical convergence rates of the
optimizer for EI using MC / RQMC sam-
pling under SAA / stochastic optimization
(“re-sample”). Appendix E provides addi-
tional detail and discussion.

The primary beneﬁt from SAA comes from the fact that in order to optimize ˆαN (x; Φ, D) for ﬁxed
base samples E, one can now employ the full toolbox of deterministic optimization, including
quasi-Newton methods that provide faster convergence speeds and are generally less sensitive to
optimization hyperparameters than stochastic ﬁrst-order methods. By default, we use multi-start
optimization via L-BFGS-B in conjunction with an initialization heuristic that exploits fast batch
evaluation of acquisition functions (see Appendix F.1). We ﬁnd that in practice the bias from using
SAA only has a minor effect on the performance relative to using the analytic ground truth, and often
improves performance relative to stochastic approaches (see Appendix E), while avoiding tedious
tuning of optimization hyperparameters such as learning rates.

4.2 One-Shot Formulation of the Knowledge Gradient using SAA

The acquisition functions mentioned above, such as EI and UCB, are myopic, that is, they do not take
into account the effect of an observation on the model in future iterations. In contrast, look-ahead
methods do. Our SAA approach enables a novel formulation of a class of look-ahead acquisition
functions. For the purpose of this paper we focus on the Knowledge Gradient (KG) [27], but our
methods extend to other look-ahead acquisition functions such as two-step EI [107].

KG quantiﬁes the expected increase in the maximum of f from obtaining the additional (random)
observation data {x, yD(x)}. KG often shows improved BO performance relative to simpler, myopic
acquisition functions such as EI [90], but in its traditional form it is computationally expensive and

2Many utility functions a are Lipschitz, including those representing (parallel) EI and UCB [104]. Lipschitzness
is a sufﬁcient condition, and convergence can also be shown in less restrictive settings (see Appendix D).

4

0.00.10.20.3EIMC, n=32analyticqMC, n=32analytic0.00.20.40.60.81.00.00.10.20.3EIMC, n=32 (fixed)analytic0.00.20.40.60.81.0qMC, n=32 (fixed)analytic166425610244096N3530252015105log2E[||x*Nx*||22]MC (SAA)MC (re­sample)qMC (SAA)qMC (re­sample)hard to implement, two challenges that we address in this work. Writing Dx := D ∪ {x, yD(x)}, we
introduce a generalized variant of parallel KG (qKG) [106]:

(cid:104)
αKG(x; D) = E

E(cid:2)g(f (x(cid:48))) | Dx
(4)
D := maxx∈X E[g(f (x)) | D]. Equation (4) quantiﬁes the expected increase in the maximum
with µ∗
posterior mean of g ◦ f after gathering samples at x. For simplicity, we only consider standard BO
here, but extensions for multi-ﬁdelity optimization [83, 110] are also available in BOTORCH.

− µ∗
D,

max
x(cid:48)∈X

(cid:3) | D

(cid:105)

Maximizing KG requires solving a nested optimization problem. The standard approach is to optimize
the inner and outer problems separately, in an iterative fashion. The outer problem is handled using
stochastic gradient ascent, with each gradient observation potentially being an average over multiple
(cid:3)
D(x) ∼ yD(x), the inner problem maxxi∈X E (cid:2)f (xi) | Di
samples [106, 109]. For each sample yi
x
is solved numerically, either via another stochastic gradient ascent [109] or multi-start L-BFGS-B
[26]. An unbiased stochastic gradient can then be computed by leveraging the envelope theorem.
Alternatively, the inner problem can be discretized [106]. The computational expense of this nested
optimization can be quite large; our main insight is that it may also be unnecessary.

We treat optimizing αKG(x, D) in (4) as an entirely deterministic problem using SAA. Using the
reparameterization trick, we express yD(x) = hy
D(x, (cid:15)) for some deterministic hD,3 and draw N
ﬁxed base samples {(cid:15)i}N

i=1 for the outer expectation. The resulting MC approximation of KG is:

ˆαKG,N (x; D) =

1
N

N
(cid:88)

i=1

max
xi∈X

E(cid:2)g(f (xi)) | Di

x

(cid:3) − µ∗.

(5)

Theorem 2. Suppose conditions (i) and (ii) of Theorem 1 hold, and that (iii) g(·) is afﬁne. If the base
samples {(cid:15)i}i≥1 are drawn i.i.d from N (0, 1), then (1) ˆα∗
KG) →
KG) > δ(cid:1) ≤ Ke−βN for all N ≥ 1.
0 a.s., and (3) ∀ δ > 0, ∃ K < ∞, β > 0 s.t. P(cid:0)dist(ˆx∗
Theorem 2 also applies when using RQMC (Appendix D.3), in which case we again observe improved
empirical convergence rates. In Appendix D.4, we prove that if ftrue is drawn from the same GP prior
as f and g(f ) ≡ f , then the MC-approximated KG policy (i.e., when (5) is maximized in each period
to select measurements) is asymptotically optimal [27, 25, 83, 7], meaning that as the number of
measurements tends to inﬁnity, an optimal point x∗ ∈ X ∗

KG,N → α∗
KG,N , X ∗

KG a.s., (2) dist(ˆx∗

f := arg maxx∈X f (x) is identiﬁed.

KG,N , X ∗

Conditional on the ﬁxed base samples, (5) does not exhibit the nested structure used in the
conventional formulation (which requires solving an optimization problem to get a noisy gra-
dient estimate). Moving the maximization outside of the sample average yields the equivalent problem

max
x∈X

ˆαKG,N (x, D) ≡ max
x, x(cid:48)

1
N

N
(cid:88)

i=1

E(cid:2)g(f (xi)) | Di

x

(cid:3),

(6)

i=1 ∈ XN represent “next stage” solutions, or “fantasy points.” If g is afﬁne, the
where x(cid:48) := {xi}N
expectation in (6) admits an analytical expression. If not, we use another MC approximation of the
form (2) with NI ﬁxed inner based samples EI .4 The key difference from the envelope theorem
approach [109] is that we do not solve the inner problem to completion for every fantasy point for
every gradient step w.r.t. x. Instead, we solve (6) jointly over x and the fantasy points x(cid:48). The
resulting optimization problem is of higher dimension, namely (q + N )d instead of qd, but unlike the
envelope theorem formulation it can be solved as a single problem, using methods for deterministic
optimization. Consequently, we dub this KG variant the “One-Shot Knowledge Gradient” (OKG).
The ability to auto-differentiate the involved quantities (including the samples yi
(x)
through the posterior updates) w.r.t. x and x(cid:48) allows BOTORCH to solve this problem effectively.
The main limitation of OKG is the linear growth of the dimension of the optimization problem in N ,
which can be challenging to solve - however, in practical settings, we observe good performance for
moderate N . We provide a simpliﬁed implementation of OKG in the following section.

D(x) and ξi

Dx

5 Programmable Bayesian Optimization with BOTORCH

SAA provides an efﬁcient and robust approach to optimizing MC acquisition functions through
the use of deterministic gradient-based optimization. In this section, we introduce BOTORCH, a

3For a GP, hy
4Convergence results can be established in the same way, and will require that min{N, NI } → ∞.

D(x) a root decomposition of Σσ

D(x, (cid:15)) = µD(x) + Lσ

D(x)(cid:15), with Lσ

D(x) := ΣD(x, x) + Σv(x).

5

complementary differentiable programming framework for Bayesian optimization research. Follow-
ing the conceptual framework outlined in Figure 1, BOTORCH provides modular abstractions for
representing and implementing sophisticated BO procedures. Operations are implemented as PyTorch
modules that are highly parallelizable on modern hardware and end-to-end differentiable, which
allows for efﬁcient optimization of acquisition functions. Since the chain of evaluations on the sample
level does not make any assumptions about the form of the posterior, BOTORCH’s primitives can be
directly used with any model from which re-parameterized posterior samples can be drawn, including
probabilistic programs [99, 8], Bayesian neural networks [71, 87, 61, 41], and more general types of
GPs [19]. In this paper, we focus on an efﬁcient and scalable implementation of GPs, GPyTorch [29].

To illustrate the core components of BOTORCH, we demonstrate how both known and novel ac-
quisition functions can readily be implemented. For the purposes of exposition, we show a set of
simpliﬁed implementations here; details and additional examples are given in Appendices G and H.

5.1 Composing BOTORCH Modules for Multi-Objective Optimization

In our ﬁrst example, we consider qParEGO [20], a variant of ParEGO [54], a method for
multi-objective optimziation.

1
2
3
4
5
6

weights = torch . distributions . Dirichlet ( torch . ones ( num_objectives ) ) . sample ()
s c a l a r i z e d _ o b j e c t i v e = Ge ne r ic M CO b je c ti v e (

lambda Y : 0.05 * ( weights * Y ) . sum ( dim = -1) + ( weights * Y ) . min ( dim = -1) . values

)
qParEGO = q E x p e c t e d I m p r o v e m e n t ( model = model , objective = s c a l a r i z e d _ o b j e c t i v e )
candidates , values = optimize_acqf ( qParEGO , bounds = box_bounds , q =1)

Code Example 1: Multi-objective optimization via augmented Chebyshev scalarizations.

Code Example 1 implements the inner loop of qParEGO. We begin by instantiating a
GenericMCObjective module that deﬁnes an augmented Chebyshev scalarization. This is an in-
stance of BOTORCH’s abstract MCObjective, which applies a transformation g(·) to samples ξ from
a posterior in its forward(ξ) pass. In line 5, we instantiate an MCAcquisitionFunction module, in
this case, qExpectedImprovement, parallel EI. Acquisition functions combine a model and the objec-
tive into a single module that assigns a utility α(x) to a candidate set x in its forward pass. Models
can be any PyTorch module implementing a probabilistic model conforming to BOTORCH’s basic
Model API. Finally, candidate points are selected by optimizing the acquisition function, through
the use of the optimize_acqf() utility function, which ﬁnds the candidates x∗ ∈ arg maxx α(x).
Auto-differentiation makes it straightforward to use gradient-based optimization even for complex
acquisition functions and objectives. Our SAA approach permits the use of deterministic higher-order
optimization to efﬁciently and reliably ﬁnd x∗.

In [6] it is shown how performing operations on independently modeled objectives yields better
optimization performance when compared to modeling combined outcomes directly (e.g., for the
case of calibrating the outputs of a simulator). MCObjective is a powerful abstraction that makes
this straightforward. It can also be used to implement unknown (i.e. modeled) outcome constraints:
BOTORCH implements a ConstrainedMCObjective to compute a feasibility-weighted objective
using a sample-level differentiable relaxation of the feasibility [89, 28, 31, 58].

5.2

Implementing Parallel, Asynchronous Noisy Expected Improvement

Noisy EI (NEI) [58] is an extension of EI that is well-suited to highly noisy settings, such as A/B
tests. Here, we describe a novel full MC formulation of NEI that extends the original one from [58]
to joint parallel optimization and generic objectives. Letting (ξ, ξobs) ∼ fD((x, xobs)), our imple-
mentation avoids the need to characterize the (uncertain) best observed function value explicitly by
averaging improvements on samples from the joint posterior over new and previously evaluated points:

qNEI(x; D) = E(cid:2)(cid:0)max g(ξ) − max g(ξobs)(cid:1)

+ | D(cid:3).

(7)

Code Example 2 provides an implementation of qNEI as formulated in (7). New MC acquisition
functions are deﬁned by extending an MCAcquisitionFunction base class and deﬁning a forward

6

@ c o n c a t e n a t e _ p e n d i n g _ p o i n t s
def forward ( self , X : Tensor ) -> Tensor :

1 class q N o i s y E x p e c t e d I m p r o v e m e n t ( M C A c q u i s i t i o n F u n c t i o n ) :
2
3
4
5
6
7
8
9
10
11
12

q = X . shape [ -2]
X_full = torch . cat ([ X , match_shape ( self . X_baseline , X ) ] , dim = -2)
posterior = self . model . posterior ( X_full )
samples = self . sampler ( posterior )
obj = self . objective ( samples )
obj_new = obj [... ,: q ]. max ( dim = -1) . value
obj_prev = obj [... , q :]. max ( dim = -1) . value
improvement = ( obj_new - obj_prev ) . clamp_min (0)
return improvement . mean ( dim =0) . value

Code Example 2: Parallel Noisy EI

pass that compute the utility of a candidate x. In the constructor (not shown), the programmer sets
X_baseline to an appropriate subset of the points at which the function was observed.

Like all MC acquisition functions, qNEI can be extended to support asynchronous candidate genera-
tion, in which a set ˜x of pending points have been submitted for evaluation, but have not yet completed.
This is done by concatenating pending points into x with the @concatenate_pending_points deco-
rator. This allows us to compute the joint utility α(x ∪ ˜x; Φ, D) of all points, pending and new, but
optimize only with respect to the new x. This strategy also provides a natural way of generating
parallel BO candidates using sequential greedy optimization [94]: We generate a single candidate, add
it to the set of pending points, and proceed to the next. Due to submodularity of many common classes
of acquisition functions (e.g., EI, UCB) [105], this approach can often yield better optimization
performance compared to optimizing all candidate locations simultaneously (see Appendix F.2).

With the observed, pending, and candidate points (X_full) in hand, we use the Model’s posterior()
method to generate an object that represents the joint posterior across all points. The Posterior
returned by posterior(x) represents fD(x) (or yD(x), if the observation_noise keyword argu-
ment is set to True), and may be be explicit (e.g. a multivariate normal in the case of GPs), or
implicit (e.g. a container for a warmed-up MCMC chain). Next, samples are drawn from the posterior
distribution p via a MCSampler, which employs the reparameterization trick [50, 85]. Given base
samples E ∈ RNs×qm, a Posterior object produces Ns samples ξD ∈ RNs×q×m from the joint
posterior. Its forward(p) pass draws samples ξi
D from p by automatically constructing base samples
E. By default, BOTORCH uses RQMC via scrambled Sobol sequences [78]. Finally, these samples
are mapped through the objective, and the expected improvement between the candidate point x and
observed/pending points is computed by marginalizing the improvements on the sample level.

5.3 Look-ahead Bayesian Optimization with One-Shot KG

Code Example 3 shows a simpliﬁed OKG implementation, as discussed in Section 4.2.

def forward ( self , X : Tensor ) -> Tensor :

1 class qK n ow l ed g eG r ad i en t ( O n e S h o t A c q u i s i t i o n F u n c t i o n ) :
2
3
4
5
6
7
8
9

)
with settings . propagate_grads ( True ) :

return inner_acqf ( X_f ) . mean ( dim =0) . value

X , X_f = torch . split (X , [ X . size ( -2) - self .N , self . N ] , dim = -2)
fant_model = self . model . fantasize ( X =X , sampler = self . sampler , o bs erv ati on_ noi se = True )
inner_acqf = SimpleRegret (

fant_model , sampler = self . inner_sampler , objective = self . objective ,

Code Example 3: Implementation of One-Shot KG

Here, the input X to forward is a concatenation of x and N fantasy points x(cid:48) (this setup ensures that
OKG can be optimized using the same APIs as all other acquisition functions). After X is split into its
components, we utilize the Model’s fantasize(x, sampler) method that, given x and a MCSampler,
(x), ∀ x ∈ Xq, where
constructs a batched set of N fantasy models {f i}N
i=1 such that f i
Di
D(x)} is the original dataset augmented by a fantasy observation at x. The fantasy
models provide a distribution over functions conditioned on future observations at x, which is used
(cid:3) from (6) for
here to implement one-step look-ahead. SimpleRegret computes E(cid:2)g(f (xi)) | Di
each i in batch mode. The propagate_grads context enables auto-differentiation through both the
generation of the fantasy models and the evaluation of their respective posteriors at the points x(cid:48).

x := D ∪ {x, yi

D(x) d= fDi

x

x

7

6 Experiments

6.1 Exploiting Parallelism and Hardware Acceleration

BOTORCH utilizes inference and optimization methods designed to exploit parallelization via batched
computation, and integrates closely with GPyTorch [29]. These model have fast test-time (predictive)
distributions and sampling. This is crucial for BO, where the same models are evaluated many
times in order to optimize the acquisition function. GPyTorch makes use of structure-exploiting
algebra and local interpolation for O(1) computations in querying the predictive distribution, and
O(T ) for drawing a posterior sample at T points, compared to the standard O(n2) and O(T 3n3)
computations [82].

Figure 4 reports wall times for batch evaluation of qExpectedImprovement at multiple candidate
sets {xi}b
i=1 for different MC samples sizes N , on both CPU and GPU for a GPyTorch GP. We
observe signiﬁcant speedups from running on the GPU, with scaling essentially linear in the batch
size b, except for very large b and N . Figure 5 shows between 10–40X speedups when using fast
predictive covariance estimates over standard posterior inference in the same setting. The speedups
grow slower on the GPU, whose cores do not saturate as quickly as on the CPU when doing standard
posterior inference (for additional details see Appendix B). Together, batch evaluation and fast
predictive distributions enable efﬁcient, parallelized acquisition function evaluation for a very large
number (tens of thousands) of points. This scalability allows us to implement and exploit novel highly
parallelized initialization and optimization techniques.

Figure 4: Wall times for batched evaluation of qEI

Figure 5: Fast predictive distributions speedups

6.2 Bayesian Optimization Performance Comparisons

We compare (i) the empirical performance of standard algorithms implemented in BOTORCH with
those from other popular BO libraries, and (ii) our novel acquisition function, OKG, against other ac-
quisition functions, both within BOTORCH and in other packages. We isolate three key frameworks—
GPyOpt, Cornell MOE (MOE EI, MOE KG), and Dragonﬂy—because they are the most popular
libraries with ongoing support5 and are most closely related to BOTORCH in terms of state-of-the-art
acquisition functions. GPyOpt uses an extension of EI with a local penalization heuristic (henceforth
GPyOpt LP-EI) for parallel optimization [34]. For Dragonﬂy, we consider its default ensemble
heuristic (henceforth Dragonﬂy GP Bandit) [49].

Figure 6: Hartmann (d = 6), noisy, best suggested

Figure 7: DQN tuning benchmark (Cartpole)

5We were unable to install GPFlowOpt due to its incompatibility with current versions of GPFlow/TensorFlow.

8

102425651216464batch size (number of evaluations)2.03.97.815.631.262.5125.0250.0500.0wall time [ms]N=128, CPUN=1024, CPUN=128, GPUN=1024, GPU102425651216464batch size (number of evaluations)0510152025303540relative speedupN=128, CPUN=512, CPUN=1024, CPUN=128, GPUN=512, GPUN=1024, GPU255075100125150175200number of observations (including initial points)10−1100regret of suggested point (log scale)RNDBoTorch EIBoTorch NEIBoTorch OKGMOE KGMOE EIGPyOpt LP­EIDragonfly GP Bandit10203040506070number of observations (including initial points)100120140160180best observed function valueRNDBoTorch EIBoTorch NEIBoTorch OKGMOE KGMOE EIGPyOpt LP­EIDragonfly GP BanditOur results provide three main takeaways. First, we ﬁnd that BOTORCH’s algorithms tend to achieve
greater sample efﬁciency compared to those of other packages (all packages use their default models
and settings). Second, we ﬁnd that OKG often outperforms all other acquisition functions. Finally,
OKG is more computationally scalable than MOE KG (the gold-standard implementation of KG),
showing signiﬁcant reductions in wall time (up to 6X, see Appendix C.2) while simultaneously
achieving improved optimization performance (Figure 6).

Synthetic Test Functions: We consider BO for parallel optimization of q = 4 design points, on four
noisy synthetic functions used in Wang et al. [100]: Branin, Rosenbrock, Ackley, and Hartmann.
Figure 6 reports means and 95% conﬁdence intervals over 100 trials for Hartmann; results for the
other functions are qualitatively similar and are provided in Appendix C.1, together with details on
the evaluation. Results for constrained BO using a differentiable relaxation of the feasibility indicator
on the sample level are provided in Appendix C.3.

Hyperparameter Optimization: We illustrate the performance of BOTORCH on real-world applica-
tions, represented by three hyperparameter optimization (HPO) experiments: (1) Tuning 5 parameters
of a deep Q-network (DQN) learning algorithm [66, 67] on the Cartpole task from OpenAI gym [12]
and the default DQN agent implemented in Horizon [30], Figure 7; (2) Tuning 6 parameters of a
neural network surrogate model for the UCI Adult data set [56] introduced by Falkner et al. [22],
available as part of HPOlib2 [21], Figure 17 in Appendix C.4; (3) Tuning 3 parameters of the recently
proposed Stochastic Weight Averaging (SWA) procedure of Izmailov et al. [40] on the VGG-16 [93]
architecture for CIFAR-10, which achieves superior accuracy compared to previously reported results.
A more detailed description of these experiments is given in Appendix C.4.

7 Discussion and Outlook

We presented a novel strategy for effectively optimizing MC acquisition functions using SAA, and
established strong theoretical convergence guarantees (in fact, our RQMC convergence results are
novel more generally, and of independent interest). Our proposed OKG method, an extension of
this approach to “one-shot” optimization of look-ahead acquisition functions, constitutes a signiﬁ-
cant development of KG, improving scalability and allowing for generic composite objectives and
outcome constraints. This approach can naturally be extended to multi-step and other look-ahead
approaches [44].

We make these methodological and theoretical contributions available in our open-source library
BOTORCH (https://botorch.org), a modern programming framework for BO that features a
modular design and ﬂexible API, our distinct SAA approach, and algorithms speciﬁcally designed to
exploit modern computing paradigms such as parallelization and auto-differentiation. BOTORCH is
particularly valuable in helping researchers to rapidly assemble novel BO techniques. Speciﬁcally,
the basic MC acquisition function abstraction provides generic support for batch optimization, asyn-
chronous evaluation, RQMC integration, and composite objectives (including outcome constraints).

Our empirical results show that besides increased ﬂexibility, our advancements in both methodol-
ogy and computational efﬁciency translate into signiﬁcantly faster and more accurate closed-loop
optimization performance on a range of standard problems. While other settings such as high-
dimensional [47, 102, 59], multi-ﬁdelity [83, 110], or multi-objective [54, 80, 20] BO, and non-MC
acquisition functions such as Max-Value Entropy Search [101], are outside the scope of this paper,
these approaches can readily be realized in BOTORCH and are included in the open-source software
package. One can also naturally generalize BO procedures to incorporate neural architectures in
BOTORCH using standard PyTorch models. In particular, deep kernel architectures [103], deep
Gaussian processes [19, 88], and variational auto-encoders [33, 68] can easily be incorporated into
BOTORCH’s primitives, and can be used for more expressive kernels in high-dimensions.

In summary, BOTORCH provides the research community with a robust and extensible basis for
implementing new ideas and algorithms in a modern computational paradigm, theoretically backed
by our novel SAA convergence results.

9

Broader Impact

Bayesian optimization is a generic methodology for optimizing black-box functions, and therefore,
by its very nature, not tied to any particular application domain. As mentioned earlier in the paper,
Bayesian optimization has been used for various arguably good causes, including drug discovery
or reducing the energy footprint of ML applications by reducing the computational cost of tuning
hyperparameters. In the Appendix, we give an speciﬁc example for how our work can be applied
in a public health context, namely to efﬁciently distribute survey locations for estimating malaria
prevalence. BOTORCH as a tool speciﬁcally has been used in various applications, including transfer
learning for neural networks [62], high-dimensional Bayesian optimization [59], drug discovery [10],
sim-to-real transfer [69], trajectory optimization [42], and nano-material design [73]. However, there
is nothing inherent to this work and Bayesian optimization as a ﬁeld more broadly that would preclude
it from being abused in some way, as is the case with any general methodology.

Acknowledgments and Disclosure of Funding

We wish to thank Art Owen for insightful conversations on quasi-Monte-Carlo methods. We also
express our appreciation to Peter Frazier and Javier Gonzalez for their helpful feedback on earlier
versions of this paper.

Andrew Gordon Wilson is supported by NSF I-DISRE 193471, NIH R01 DA048764-01A1, NSF
IIS-1910266, and NSF 1922658 NRT-HDR: FUTURE Foundations, Translation, and Responsibility
for Data Science.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for large-scale machine
learning. In OSDI, volume 16, pages 265–283, 2016.

[2] Robert J Adler. An introduction to continuity, extrema, and related topics for general Gaussian processes.

IMS, 1990.

[3] Robert J. Adler. The Geometry of Random Fields. Society for Industrial and Applied Mathematics, 2010.

[4] Deepak Agarwal, Kinjal Basu, Souvik Ghosh, Ying Xuan, Yang Yang, and Liang Zhang. Online parameter
selection for web-based ranking problems. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, pages 23–32, 2018.

[5] Rika Antonova, Akshara Rai, and Christopher G. Atkeson. Deep kernels for optimizing locomotion

controllers. In Proceedings of the 1st Conference on Robot Learning, CoRL, 2017.

[6] Raul Astudillo and Peter Frazier. Bayesian optimization of composite functions. In Proceedings of the

36th International Conference on Machine Learning, volume 97, pages 354–363. PMLR, 2019.

[7] Julien Bect, François Bachoc, and David Ginsbourger. A supermartingale approach to gaussian process

based sequential design of experiments. Bernoulli, 25(4A):2883–2919, 11 2019.

[8] Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karalet-
sos, Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D. Goodman. Pyro: Deep Universal Probabilistic
Programming. Journal of Machine Learning Research, 2018.

[9] Nikolay Bliznyuk, David Ruppert, Christine Shoemaker, Rommel Regis, Stefan Wild, and Pradeep
Mugunthan. Bayesian calibration and uncertainty analysis for computationally expensive models using
optimization and radial basis function approximation. Journal of Computational and Graphical Statistics,
17(2):270–294, 2008.

[10] Jacques Boitreaud, Vincent Mallet, Carlos Oliver, and Jerome Waldispühl. Optimol: Optimization of

binding afﬁnities in chemical space for drug discovery. bioRxiv, 2020.

[11] Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic

theory of independence. Oxford university press, 2013.

[12] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and

Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.

10

[13] Alexander Buchholz, Florian Wenzel, and Stephan Mandt. Quasi-Monte Carlo variational inference. In

Proceedings of the 35th International Conference on Machine Learning. PMLR, 2018.

[14] Russel E Caﬂisch. Monte carlo and quasi-monte carlo methods. Acta numerica, 7:1–49, 1998.

[15] Roberto Calandra, André Seyfarth, Jan Peters, and Marc Peter Deisenroth. Bayesian optimization for

learning gaits under uncertainty. Annals of Mathematics and Artiﬁcial Intelligence, 2016.

[16] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan
Zhang, and Zheng Zhang. MXNet: A ﬂexible and efﬁcient machine learning library for heterogeneous
distributed systems. arXiv preprint arXiv:1512.01274, 2015.

[17] Xi Chen and Qiang Zhou. Sequential experimental designs for stochastic kriging. In Proceedings of the

2014 Winter Simulation Conference, WSC ’14, pages 3821–3832. IEEE Press, 2014.

[18] Kurt Cutajar, Mark Pullin, Andreas Damianou, Neil Lawrence, and Javier González. Deep Gaussian

Processes for Multi-ﬁdelity Modeling. arXiv preprint arXiv:1903.07320, 2019.

[19] Andreas Damianou and Neil Lawrence. Deep Gaussian Processes. In Artiﬁcial Intelligence and Statistics,

pages 207–215, 2013.

[20] Samuel Daulton, Maximilian Balandat, and Eytan Bakshy. Differentiable Expected Hypervolume
Improvement for Parallel Multi-Objective Bayesian Optimization. In Advances in Neural Information
Processing Systems 33, 2020.

[21] Katharina Eggensperger, Matthias Feurer, Aaron Klein, and Stefan Falkner. Hpolib2 (development

branch), 2019. URL https://github.com/automl/HPOlib2.

[22] Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: robust and efﬁcient hyperparameter optimization

at scale. CoRR, abs/1807.01774, 2018.

[23] Qing Feng, Benjamin Letham, Hongzi Mao, and Eytan Bakshy. High-dimensional contextual policy
search with unknown context rewards using Bayesian optimization. In Advances in Neural Information
Processing Systems 33, NeurIPS, 2020.

[24] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and Frank
Hutter. Efﬁcient and robust automated machine learning. In Advances in Neural Information Processing
Systems 28, pages 2962–2970. 2015.

[25] Peter Frazier, Warren Powell, and Savas Dayanik. The knowledge-gradient policy for correlated normal

beliefs. INFORMS journal on Computing, 21(4):599–613, 2009.

[26] Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.

[27] Peter I Frazier, Warren B Powell, and Savas Dayanik. A knowledge-gradient policy for sequential

information collection. SIAM Journal on Control and Optimization, 47(5):2410–2439, 2008.

[28] Jacob Gardner, Matt Kusner, Zhixiang, Kilian Weinberger, and John Cunningham. Bayesian optimization
with inequality constraints. In Proceedings of the 31st International Conference on Machine Learning,
volume 32, pages 937–945, 2014.

[29] Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, and Andrew G Wilson. GPytorch:
Blackbox matrix-matrix Gaussian process inference with GPU acceleration. In Advances in Neural
Information Processing Systems, pages 7576–7586, 2018.

[30] Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Yuchen He, Zachary Kaden, Vivek
Narayanan, and Xiaohui Ye. Horizon: Facebook’s open source applied reinforcement learning platform.
arXiv preprint arXiv:1811.00260, 2018.

[31] Michael A. Gelbart, Jasper Snoek, and Ryan P. Adams. Bayesian optimization with unknown constraints.

In Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence, UAI, 2014.

[32] David Ginsbourger, Janis Janusevskis, and Rodolphe Le Riche. Dealing with asynchronicity in
parallel Gaussian process based global optimization. Technical report, 2011. URL https://hal.
archives-ouvertes.fr/hal-00507632.

[33] Rafael Gómez-Bombarelli, Jennifer N. Wei, David Duvenaud, JoséMiguel Hernández-Lobato, Benjamín
Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams,
and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of
molecules. ACS Central Science, 4(2):268–276, 02 2018.

11

[34] Javier González, Zhenwen Dai, Philipp Hennig, and Neil D. Lawrence. Batch Bayesian optimization via
local penalization. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence and
Statistics, AISTATS, pages 648–657, 2016.

[35] GPy. GPy: A gaussian process framework in python. http://github.com/SheffieldML/GPy, since

2012.

[36] Ryan-Rhys Grifﬁths and José Miguel Hernández-Lobato. Constrained Bayesian optimization for auto-

matic chemical design. arXiv preprint arXiv:1709.05501, 2017.

[37] Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution

strategies. Evol. Comput., 9(2):159–195, June 2001.

[38] José Miguel Hernández-Lobato, Michael A. Gelbart, Matthew W. Hoffman, Ryan P. Adams, and Zoubin
Ghahramani. Predictive entropy search for Bayesian optimization with unknown constraints. In Proceed-
ings of the 32nd International Conference on Machine Learning, ICML, 2015.

[39] Tito Homem-de-Mello. On rates of convergence for stochastic optimization problems under non-
independent and identically distributed sampling. SIAM Journal on Optimization, 19(2):524–551,
2008.

[40] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson.
Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407,
2018.

[41] Pavel Izmailov, Wesley Maddox, Timur Garipov, Polina Kirichenko, Dmitry Vetrov, and Andrew Gordon
Wilson. Subspace inference for Bayesian deep learning. In Uncertainty in Artiﬁcial Intelligence, 2019.

[42] Achin Jain and Manfred Morari. Computing the racing line using Bayesian optimization. arXiv e-prints,

page arXiv:2002.04794, February 2020.

[43] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio
Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv
preprint arXiv:1408.5093, 2014.

[44] Shali Jiang, Daniel R. Jiang, Maximilian Balandat, Brian Karrer, Jacob Gardner, and Roman Garnett. Efﬁ-
cient nonmyopic Bayesian optimization via one-shot multi-step trees. In Advances in Neural Information
Processing Systems, 2020.

[45] D. R. Jones, C. D. Perttunen, and B. E. Stuckman. Lipschitzian optimization without the Lipschitz

constant. Journal of Optimization Theory and Applications, 79(1):157–181, Oct 1993.

[46] Donald R. Jones, Matthias Schonlau, and William J. Welch. Efﬁcient global optimization of expensive

black-box functions. Journal of Global Optimization, 13:455–492, 1998.

[47] Kirthevasan Kandasamy, Jeff Schneider, and Barnabás Póczos. High dimensional Bayesian optimisation
and bandits via additive models. In Proceedings of the 32nd International Conference on Machine
Learning, ICML, 2015.

[48] Kirthevasan Kandasamy, Gautam Dasarathy, Junier Oliva, Jeff Schneider, and Barnab’as P’oczos. Gaus-
sian process bandit optimisation with multi-ﬁdelity evaluations. In Advances in Neural Information
Processing Systems, NIPS, 2016.

[49] Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R.
Collins, Jeff Schneider, Barnabas Póczos, and Eric P. Xing. Tuning hyperparameters without grad students:
Scalable and robust Bayesian optimisation with Dragonﬂy. arXiv e-prints, art. arXiv:1903.06694, Mar
2019.

[50] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv e-prints, page

arXiv:1312.6114, Dec 2013.

[51] A. Klein, S. Falkner, S. Bartels, P. Hennig, and F. Hutter. Fast Bayesian optimization of machine learning

hyperparameters on large datasets. CoRR, 2016.

[52] A. Klein, S. Falkner, N. Mansur, and F. Hutter. Robo: A ﬂexible and robust Bayesian optimization

framework in Python. In NIPS 2017 Bayesian Optimization Workshop, December 2017.

[53] Anton J Kleywegt, Alexander Shapiro, and Tito Homem-de Mello. The sample average approximation
method for stochastic discrete optimization. SIAM Journal on Optimization, 12(2):479–502, 2002.

12

[54] J. Knowles. ParEGO: A hybrid algorithm with on-line landscape approximation for expensive mul-
IEEE Transactions on Evolutionary Computation, 10(1):50–66,

tiobjective optimization problems.
2006.

[55] Nicolas Knudde, Joachim van der Herten, Tom Dhaene, and Ivo Couckuyt. GPﬂowOpt: A Bayesian

Optimization Library using TensorFlow. arXiv preprint – arXiv:1711.03845, 2017.

[56] R. Kohavi and B. Becker. UCI machine learning repository, 1996. URL http://archive.ics.uci.

edu/ml.

[57] Benjamin Letham and Eytan Bakshy. Bayesian optimization for policy search via online-ofﬂine experi-

mentation. Journal of Machine Learning Research, 20(145):1–30, 2019.

[58] Benjamin Letham, Brian Karrer, Guilherme Ottoni, and Eytan Bakshy. Constrained Bayesian optimization

with noisy experiments. Bayesian Analysis, 14(2):495–519, 2019.

[59] Benjamin Letham, Roberto Calandra, Akshara Rai, and Eytan Bakshy. Re-examining linear embeddings
for high-dimensional Bayesian optimization. In Advances in Neural Information Processing Systems 33,
NeurIPS, 2020.

[60] Cheng Li, David Rubín de Celis Leal, Santu Rana, Sunil Gupta, Alessandra Sutti, Stewart Greenhill,
Teo Slezak, Murray Height, and Svetha Venkatesh. Rapid Bayesian optimisation for synthesis of short
polymer ﬁber materials. Scientiﬁc reports, 7(1):5683, 2017.

[61] Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew Gordon Wilson. A simple
In Advances in Neural Information Processing

baseline for Bayesian uncertainty in deep learning.
Systems, 2019.

[62] Wesley J Maddox, Shuai Tang, Pablo Garcia Moreno, Andrew Gordon Wilson, and Andreas Dami-
anou. On Transfer Learning via Linearized Neural Networks. In NeurIPS Workshop on Meta-Learning
(MetaLearn 2019), 2019.

[63] Malaria Atlas Project.

Malaria atlas project, 2019.

URL https://map.ox.ac.uk/

malaria-burden-data-download.

[64] Alexander G. de G. Matthews, Mark van der Wilk, Tom Nickson, Keisuke. Fujii, Alexis Boukouvalas,
Pablo León-Villagrá, Zoubin Ghahramani, and James Hensman. GPﬂow: A Gaussian process library
using TensorFlow. Journal of Machine Learning Research, 18(40):1–6, apr 2017.

[65] Richard M Meyer. Essential mathematics for applied ﬁelds. Springer Science & Business Media, 2012.

[66] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602,
2013.

[67] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529, 2015.

[68] Riccardo Moriconi, K. S. Sesh Kumar, and Marc Peter Deisenroth. High-Dimensional Bayesian Opti-

mization with Manifold Gaussian Processes. arXiv e-prints, page arXiv:1902.10675, Feb 2019.

[69] Fabio Muratore, Christian Eilers, Michael Gienger, and Jan Peters. Bayesian Domain Randomization for

Sim-to-Real Transfer. arXiv e-prints, page arXiv:2003.02471, March 2020.

[70] Iain Murray. Differentiation of the Cholesky decomposition. arXiv e-prints, page arXiv:1602.07527, Feb

2016.

[71] Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business

Media, 1996.

[72] Willie Neiswanger, Kirthevasan Kandasamy, Barnabas Póczos, Jeff Schneider, and Eric Xing. ProBO:
a Framework for Using Probabilistic Programming in Bayesian Optimization. arXiv e-prints, page
arXiv:1901.11515, January 2019.

[73] Thanh V. Nguyen, Youssef Mroueh, Samuel Hoffman, Payel Das, Pierre Dognin, Giuseppe Romano, and
Chinmay Hegde. Nano-material conﬁguration design with deep surrogate langevin dynamics. In ICLR
2020 Workshop on Integration of Deep Neural Models and Differential Equations, 2019.

13

[74] Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, and Svetha Venkatesh. Predictive variance reduction

search. In NIPS 2017 Workshop on Bayesian Optimization, Dec 2017.

[75] A O’Hagan. On curve ﬁtting and optimal design for regression. J. Royal Stat. Soc. B, 40:1–32, 1978.

[76] Michael A Osborne. Bayesian Gaussian processes for sequential prediction, optimisation and quadrature.

PhD thesis, Oxford University, UK, 2010.

[77] Art B. Owen. Randomly permuted (t,m,s)-nets and (t, s)-sequences. In Harald Niederreiter and Peter
Jau-Shyong Shiue, editors, Monte Carlo and Quasi-Monte Carlo Methods in Scientiﬁc Computing, pages
299–317, New York, NY, 1995. Springer New York.

[78] Art B Owen. Quasi-monte carlo sampling. Monte Carlo Ray Tracing: Siggraph, 1:69–88, 2003.

[79] Art B. Owen and Daniel Rudolf. A strong law of large numbers for scrambled net integration. arXiv

e-prints, page arXiv:2002.07859, February 2020.

[80] B. Paria, K. Kandasamy, and B. Póczos. A Flexible Multi-Objective Bayesian Optimization Approach

using Random Scalarizations. ArXiv e-prints, May 2018.

[81] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming
Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. 2017.

[82] Geoff Pleiss, Jacob R Gardner, Kilian Q Weinberger, and Andrew Gordon Wilson. Constant-time
predictive distributions for gaussian processes. In International Conference on Machine Learning, 2018.

[83] Matthias Poloczek, Jialei Wang, and Peter Frazier. Multi-information source optimization. In Advances

in Neural Information Processing Systems, pages 4288–4298, 2017.

[84] Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning. 2006.

The MIT Press, Cambridge, MA, USA, 38:715–719, 2006.

[85] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In Proceedings of the 31st International Conference
on International Conference on Machine Learning - Volume 32, ICML’14, pages II–1278–II–1286.
JMLR.org, 2014.

[86] Mark Rowland, Krzysztof M Choromanski, François Chalus, Aldo Pacchiano, Tamas Sarlos, Richard E
In Advances in Neural

Turner, and Adrian Weller. Geometrically coupled monte carlo sampling.
Information Processing Systems 31, pages 195–206. 2018.

[87] Yunus Saatci and Andrew G Wilson. Bayesian GAN. In Advances in neural information processing

systems, pages 3622–3631, 2017.

[88] Hugh Salimbeni and Marc Peter Deisenroth. Doubly stochastic variational inference for deep gaussian
processes. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
editors, Advances in Neural Information Processing Systems 30, pages 4588–4599. Curran Associates,
Inc., 2017.

[89] Matthias Schonlau, William J. Welch, and Donald R. Jones. Global versus local search in constrained

optimization of computer models. Lecture Notes-Monograph Series, 34:11–25, 1998.

[90] Warren Scott, Peter Frazier, and Warren Powell. The correlated knowledge gradient for simulation
optimization of continuous parameters using Gaussian process regression. SIAM Journal of Optimization,
21:996–1026, 2011.

[91] S. Seo, M. Wallat, T. Graepel, and K. Obermayer. Gaussian process regression: active data selection and
test point rejection. In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural
Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,
volume 3, pages 241–246 vol.3, July 2000.

[92] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. Taking the human

out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104:1–28, 2016.

[93] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image

recognition. arXiv preprint arXiv:1409.1556, 2014.

[94] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine learning

algorithms. In Advances in neural information processing systems, pages 2951–2959, 2012.

14

[95] Jasper Snoek, Kevin Swersky, Richard Zemel, and Ryan P Adams. Input warping for Bayesian opti-
mization of non-stationary functions. In Proceedings of the 31st International Conference on Machine
Learning, ICML’14, 2014.

[96] J. T. Springenberg, A. Klein, S.Falkner, and F. Hutter. Bayesian optimization with robust Bayesian neural

networks. In Advances in Neural Information Processing Systems 29, December 2016.

[97] The Emukit authors. Emukit: Emulation and uncertainty quantiﬁcation for decision making. https:

//github.com/amzn/emukit, 2018.

[98] The GPyOpt authors. GPyOpt: A Bayesian optimization framework in Python. http://github.com/

SheffieldML/GPyOpt, 2016.

[99] Dustin Tran, Matthew D Hoffman, Rif A Saurous, Eugene Brevdo, Kevin Murphy, and David M Blei. Deep
probabilistic programming. Proceedings of the International Conference on Learning Representations
(ICLR), 2017.

[100] Jialei Wang, Scott C Clark, Eric Liu, and Peter I Frazier. Parallel Bayesian global optimization of

expensive functions. arXiv preprint arXiv:1602.05149, 2016.

[101] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization. volume 70
of Proceedings of Machine Learning Research, pages 3627–3635, International Convention Centre,
Sydney, Australia, 06–11 Aug 2017. PMLR.

[102] Ziyu Wang, Frank Hutter, Masrour Zoghi, David Matheson, and Nando De Freitas. Bayesian optimization
in a billion dimensions via random embeddings. J. Artif. Int. Res., 55(1):361–387, January 2016.

[103] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In

Artiﬁcial Intelligence and Statistics, pages 370–378, 2016.

[104] J. T. Wilson, R. Moriconi, F. Hutter, and Marc Peter Deisenroth. The reparameterization trick for

acquisition functions. ArXiv e-prints, December 2017.

[105] James Wilson, Frank Hutter, and Marc Peter Deisenroth. Maximizing acquisition functions for Bayesian
optimization. In Advances in Neural Information Processing Systems 31, pages 9905–9916. 2018.

[106] Jian Wu and Peter Frazier. The parallel knowledge gradient method for batch Bayesian optimization. In

Advances in Neural Information Processing Systems, pages 3126–3134, 2016.

[107] Jian Wu and Peter Frazier. Practical two-step lookahead Bayesian optimization. In Advances in Neural

Information Processing Systems 32, 2019.

[108] Jian Wu and Peter I. Frazier. Discretization-free Knowledge Gradient Methods for Bayesian Optimization.

arXiv e-prints, page arXiv:1707.06541, Jul 2017.

[109] Jian Wu, Matthias Poloczek, Andrew Gordon Wilson, and Peter I Frazier. Bayesian optimization with

gradients. In Advances in Neural Information Processing Systems, pages 5267–5278, 2017.

[110] Jian Wu, Saul Toscano-Palmerin, Peter I. Frazier, and Andrew Gordon Wilson. Practical multi-ﬁdelity

Bayesian optimization for hyperparameter tuning. CoRR, abs/1903.04703, 2019.

[111] Yichi Zhang, Daniel W Apley, and Wei Chen. Bayesian optimization for materials design with mixed

quantitative and qualitative variables. Scientiﬁc Reports, 10(1):1–13, 2020.

15

Appendix to:

BOTORCH: A Framework for Efﬁcient Monte-Carlo
Bayesian Optimization

A Brief Overview of Other Software Packages for BO

One of the earliest commonly-used packages is Spearmint [94], which implements a variety of
modeling techniques such as MCMC hyperparameter sampling and input warping [95]. Spearmint
also supports parallel optimization via fantasies, and constrained optimization with the expected
improvement and predictive entropy search acquisition functions [31, 38]. Spearmint was among the
ﬁrst libraries to make BO easily accessible to the end user.

GPyOpt [98] builds on the popular GP regression framework GPy [35]. It supports a similar set of
features as Spearmint, along with a local penalization-based approach for parallel optimization [34].
It also provides the ability to customize different components through an alternative, more modular
API.

Cornell-MOE [106] implements the Knowledge Gradient (KG) acquisition function, which allows
for parallel optimization, and includes recent advances such as large-scale models incorporating
gradient evaluations [109] and multi-ﬁdelity optimization [110]. Its core is implemented in C++,
which provides performance beneﬁts but renders it hard to modify and extend.

RoBO [52] implements a collection of models and acquisition functions, including Bayesian neural
nets [96] and multi-ﬁdelity optimization [51].

Emukit [97] is a Bayesian optimization and active learning toolkit with a collection of acquisition
functions, including for parallel and multi-ﬁdelity optimization. It does not provide speciﬁc abstrac-
tions for implementing new algorithms, but rather speciﬁes a model API that allows it to be used with
the other toolkit components.

The recent Dragonﬂy [49] library supports parallel optimization, multi-ﬁdelity optimization [48],
and high-dimensional optimization with additive kernels [47]. It takes an ensemble approach and
aims to work out-of-the-box across a wide range of problems, a design choice that makes it relatively
hard to extend.

B Parallelism and Hardware Acceleration

B.1 Batch Evaluation

Batch evaluation, an important element of modern computing, enables automatic dispatch of indepen-
dent operations across multiple computational resources (e.g. CPU and GPU cores) for parallelization
and memory sharing. All BOTORCH components support batch evaluation, which makes it easy
to write concise and highly efﬁcient code in a platform-agnostic fashion. Batch evaluation enables
fast queries of acquisition functions at a large number of candidate sets in parallel, facilitating novel
initialization heuristics and optimization techniques.

Speciﬁcally, instead of sequentially evaluating an acquisition function at a number of candidate
sets x1, . . . , xb, where xk ∈ Rq×d for each k, BOTORCH evaluates a batched tensor x ∈ Rb×q×d.
Computation is automatically distributed so that, depending on the hardware used, speedups can be
close to linear in the batch size b. Batch evaluation is also heavily used in computing MC acquisition
functions, with the effect that signiﬁcantly increasing the number of MC samples often has little
impact on wall time. In Figure 4 we observe signiﬁcant speedups from running on the GPU, with

16

Figure 8: Stochastic/deterministic opt. of EI on Hart-
mann6

Figure 9: Branin (d = 2)

Figure 10: Rosenbrock (d = 3)

Figure 11: Ackley (d = 5)

scaling essentially linear in the batch size, except for very large b and N . The ﬁxed cost due to
communication overhead renders CPU evaluation faster for small batch and sample sizes.

C Additional Empirical Results

This section describes a number of empirical results that were omitted from the main paper due to
space constraints.

C.1 Synthetic Functions

Algorithms start from the same set of 2d + 2 QMC sampled initial points for each trial, with d
the dimension of the design space. We evaluate based on the true noiseless function value at the
“suggested point” (i.e., the point to be chosen if BO were to end at this batch). OKG, MOE KG,
and NEI use “out-of-sample” suggestions (introduced as χn in Section D.4), while the others use
“in-sample” suggestions [26].

All functions are evaluated with noise generated from a N (0, .25) distribution. Figures 9-11 give the
results for all synthetic functions from Section 6. The results show that BOTORCH’s NEI and OKG
acquisition functions provide highly competitive performance in all cases.

C.2 One-Shot KG Computational Scaling

Figure 12 shows the wall time for generating a set of q = 8 candidates as a function of the number
of total data points n for both standard (Cholesky-based) as well as scalable (Linear CG) posterior
inference methods, on both CPU and GPU. While the GPU variants have a signiﬁcant overhead
for small models, they are signiﬁcantly faster for larger models. Notably, our SAA based OKG
is signiﬁcantly faster than MOE KG, while at the same time achieving much better optimization
performance (Figure 13).

17

255075100125150175200number of observations (including initial points)1006×10−12×100regret of suggested point (log scale)LBFGS­B, 1024 fixed samplesADAM, LR=0.01, 128 samples/iterationSGD, LR=0.01, 128 samples/iterationADAM, LR=0.03, 128 samples/iterationSGD, LR=0.03, 128 samples/iterationADAM, LR=0.05, 128 samples/iterationSGD, LR=0.05, 128 samples/iteration020406080100120140160number of observations (including initial points)10−210−1100101regret of suggested point (log scale)RNDBoTorch EIBoTorch NEIBoTorch OKGMOE KGMOE EIGPyOpt LP­EIDragonfly GP Bandit050100150200250number of observations (including initial points)10−1100101102103regret of suggested point (log scale)RNDBoTorch EIBoTorch NEIBoTorch OKGMOE KGMOE EIGPyOpt LP­EIDragonfly GP Bandit255075100125150175200number of observations (including initial points)10−1100regret of suggested point (log scale)RNDBoTorch EIBoTorch NEIBoTorch OKGMOE KGMOE EIGPyOpt LP­EIDragonfly GP BanditFigure 12: KG wall times

Figure 13: Hartmann (d = 6), noisy, best suggested

Figure 14: Constrained Hartmann6, f2(x) = (cid:107)x(cid:107)1 − 3 Figure 15: Constrained Hartmann6, f1(x) = (cid:107)x(cid:107)2 − 1

C.3 Constrained Bayesian Optimization

We present results for constrained BO on a synthetic function. We consider a multi-output function
f = (f1, f2) and the optimization problem:

max
x∈X

f1(x)

s.t.

f2(x) ≤ 0.

(8)

Both f1 and f2 are observed with N (0, 0.52) noise and we model the two components using
independent GP models. A constraint-weighted composite objective is used in each of the BOTORCH
acquisition functions EI, NEI, and OKG.

Results for the case of a Hartmann6 objective and two types of constraints are given in Figures 14-15
(we only show results for BOTORCH’s algorithms, since the other packages do not natively support
optimization subject to unknown constraints).

The regret values are computed using a feasibility-weighted objective, where “infeasible” is assigned
an objective value of zero. For random search and EI, the suggested point is taken to be the
best feasible noisily observed point, and for NEI and OKG, we use out-of-sample suggestions by
optimizing the feasibility-weighted version of the posterior mean. The results displayed in Figure
15 are for the constrained Hartmann6 benchmark from [58]. Note, however, that the results here
are not directly comparable to the ﬁgures in [58] because (1) we use feasibility-weighted objectives
to compute regret and (2) they follow a different convention for suggested points. We emphasize
that our contribution of outcome constraints for the case of KG has not been shown before in the
literature.

C.4 Hyperparameter Optimization Details

This section gives further detail on the experimental settings used in each of the hyperparameter
optimization problems. As HPO typically involves long and resource intensive training jobs, it is
standard to select the conﬁguration with the best observed performance, rather than to evaluate a
“suggested” conﬁguration (we cannot perform noiseless function evaluations).

DQN and Cartpole: We consider the case of tuning a deep Q-network (DQN) learning algorithm
[66, 67] on the Cartpole task from OpenAI gym [12] and the default DQN agent implemented in

18

050100150200250300350400number of observations (including initial points)0100200300400500600wall time (s)Cholesky, CPULinear CG, CPUCholesky, GPULinear CG, GPUMOE, CPU255075100125150175200number of observations (including initial points)10−1100regret of suggested point (log scale)RNDBoTorch EIBoTorch NEIBoTorch OKGMOE KGMOE EIGPyOpt LP­EIDragonfly GP Bandit255075100125150175200number of observations (including initial points)10−1100regret of suggested point (log scale)RNDBoTorch EIBoTorch NEIBoTorch OKG255075100125150175200number of observations (including initial points)1006×10−12×1003×100regret of suggested point (log scale)RNDBoTorch EIBoTorch NEIBoTorch OKGFigure 16: DQN tuning benchmark (Cartpole)

Figure 17: NN surrogate model, best observed accuracy

Horizon [30]. Figure 16 shows the results of tuning ﬁve hyperparameters, exploration parameter
(“epsilon”), the target update rate, the discount factor, the learning rate, and the learning rate decay.
We allow for a maximum of 60 training episodes or 2000 training steps, whichever occurs ﬁrst. To
reduce noise, each “function evaluation” is taken to be an average of 10 independent training runs
of DQN. Figure 16 presents the optimization performance of various acquisition functions from the
different packages, using 15 rounds of parallel evaluations of size q = 4, over 100 trials. While
in later iterations all algorithms achieve reasonable performance, BOTORCH OKG, EI, NEI, and
GPyOpt LP-EI show faster learning early on.

Neural Network Surrogate: We consider the neural network surrogate model for the UCI Adult
data set introduced by Falkner et al. [22], which is available as part of HPOlib2 [21]. We use a
surrogate model to achieve a high level of precision in comparing the performance of the algorithms
without incurring excessive computational training costs. This is a six-dimensional problem over
network parameters (number of layers, units per layer) and training parameters (initial learning
rate, batch size, dropout, exponential decay factor for learning rate). Figure 17 shows optimization
performance in terms of best observed classiﬁcation accuracy. Results are means and 95% conﬁdence
intervals computed from 200 trials with 75 iterations of size q = 1. All BOTORCH algorithms
perform quite similarly here, with OKG doing slightly better in earlier iterations. Notably, they all
achieve signiﬁcantly better accuracy than all other algorithms.

Stochastic Weight Averaging on CIFAR-10: Our ﬁnal example is for the recently proposed Stochas-
tic Weight Averaging (SWA) procedure of Izmailov et al. [40], for which good hyperparameter settings
are not fully understood. The setting is 300 epochs of training on the VGG-16 [93] architecture
for CIFAR-10. We tune three SWA hyperparameters: learning rate, update frequency, and starting
iteration using OKG. Izmailov et al. [40] report the mean and standard deviation of the test accuracy
over three runs to be 93.64 and 0.18, respectively, which corresponds to a 95% conﬁdence interval of
93.64 ± 0.20. We tune the problem to an average accuracy of 93.84 ± 0.03.

D Additional Theoretical Results and Omitted Proofs

D.1 General SAA Results

Recall that we assume that f (x) ∼ h(x, (cid:15)) for some h : X × Rs → Rq×m and base random variable
(cid:15) ∈ Rs (c.f. Section 4 for an explicit expression for h in case of a GP model). We write

A(x, (cid:15)) := a(g(h(x, (cid:15)))).

(9)

Theorem 3 (Homem-de-Mello [39]). Suppose that (i) X is a compact metric space, (ii) ˆαN (x) a.s.−−→
α(x) for all x ∈ Xq, and (iii) there exists an integrable function (cid:96) : Rs (cid:55)→ R such that for almost
every (cid:15) and all x, y ∈ X,

|A(x, (cid:15)) − A(y, (cid:15))| ≤ (cid:96)((cid:15))(cid:107)x − y(cid:107).
f ) a.s.−−→ 0.

a.s.−−→ α∗ and dist(ˆx∗

Then ˆα∗
N
Proposition 1. Suppose that (i) X is a compact metric space, (ii) f is a GP with continuously
differentiable prior mean and covariance functions, and (iii) g(·) and a(·, Φ) are Lipschitz continuous.
Then, condition (10) in Theorem 3 holds.

N , X ∗

(10)

19

10203040506070number of observations (including initial points)100120140160180best observed function valueRNDBoTorch EIBoTorch NEIBoTorch OKGMOE KGMOE EIGPyOpt LP­EIDragonfly GP Bandit1020304050607080number of observations (including initial points)84.9585.0085.0585.1085.1585.2085.25best observed accuracyRNDBoTorch EIBoTorch NEIBoTorch OKGMOE KGMOE EIGPyOpt EIDragonfly GP BanditThe following proposition follows directly from Proposition 2.1, Theorem 2.3, and remarks on page
528 of [39].
Proposition 2 (Homem-de-Mello [39]). Suppose that, in addition to the conditions in Theorem 3,
i=1 are i.i.d., (ii) for all x ∈ Xq the moment generating function
(i) the base samples E = {(cid:15)i}N
x (t) := E[etA(x,(cid:15))] of A(x, (cid:15)) is ﬁnite in an open neighborhood of t = 0 and (iii) the moment
MA
generating function M (cid:96)(t) := E[et(cid:96)((cid:15))] is ﬁnite in an open neighborhood of t = 0. Then, there exist
K < ∞ and β > 0 such that P(dist(ˆxN , X ∗

f )) ≤ Ke−βN for all N ≥ 1.

D.2 Formal Statement of Theorem 1

Theorem 1 (Formal Version). Suppose (i) X is compact, (ii) f has a GP prior with continuously
differentiable mean and covariance functions, and (iii) g(·) and a(·, Φ) are Lipschitz continuous. If
the base samples {(cid:15)i}N

i=1 are drawn i.i.d. from N (0, 1), then

(1) ˆα∗

N → α∗ a.s., and

(2) dist(ˆx∗

N , X ∗) → 0 a.s.

x (t) := E[etA(x,(cid:15))] of A(x, (cid:15)) is
If, in addition, (iii) for all x ∈ Xq the moment generating function MA
ﬁnite in an open neighborhood of t = 0 and (iv) the moment generating function M (cid:96)(t) := E[et(cid:96)((cid:15))]
is ﬁnite in an open neighborhood of t = 0, then

(3) ∀ δ > 0, ∃ K < ∞, β > 0 s.t. P(cid:0)dist(ˆx∗

N , X ∗) > δ(cid:1) ≤ Ke−βN for all N ≥ 1.

D.3 Randomized Quasi-Monte Carlo Sampling for Sample Average Approximation

In order to use randomized QMC methods with SAA for MC acquisition function, the base samples
E = {(cid:15)i} will need to be generated via RQMC. For the case of Normal base samples, this can be
achieved in various ways, e.g. by using inverse CDF methods or a suitable Box-Muller transform of
samples (cid:15)i ∈ [0, 1]s (both approaches are implemented in BOTORCH). In the language of Section 4,
such a transform will become part of the base sample transform (cid:15) (cid:55)→ h(x, (cid:15)) for any ﬁxed x.

For the purpose of this paper, we consider scrambled (t, d)-sequences as discussed by Owen [77],
which are a particular class of RQMC method (BOTORCH uses PyTorch’s implementation of scram-
bled Sobol sequences, which are (t, d)-nets in base 2). Using recent theoretical advances from Owen
and Rudolf [79], it is possible to generalize the convergence results from Theorems 1 and 2 to the
RQMC setting (to our knowledge, this is the ﬁrst practical application of these theoretical results).
Let (Ni)i≥1 be a sequence with Ni ∈ N s.t. Ni → ∞ as i → ∞. Then we have the following (see
Appendix D.5 for the proofs):
Theorem 1(q). In the setting of Theorem 1, let {(cid:15)i} be samples from a (t, d)-sequence in base b with
gain coefﬁcients no larger than Γ < ∞, randomized using a nested uniform scramble as in [77].
Then, the conclusions of Theorem 1 still hold. In particular,

(1) ˆα∗
Ni

→ α∗ a.s. as i → ∞,

, X ∗) → 0 a.s. as i → ∞,

(2) dist(ˆx∗
Ni
(3) ∀ δ > 0, ∃ K < ∞, β > 0 s.t. P(cid:0)dist(ˆx∗
Ni
Theorem 2(q). In the setting of Theorem 2, let {(cid:15)i} be samples from (t, d)-sequence in base b, with
gain coefﬁcients no larger than Γ < ∞, randomized using a nested uniform scramble as in [77].
Then,

, X ∗) > δ(cid:1) ≤ Ke−βNi for all i ≥ 1.

(1) ˆα∗

KG,Ni

a.s.−−→ α∗

(2) dist(ˆx∗

KG,Ni

, X ∗

KG as i → ∞,
KG) a.s.−−→ 0 as i → ∞.

Theorem 2(q) as stated does not provide a rate on the convergence of the optimizer. We believe that
such result is achievable, but leave it to future work.

20

Note that while the above results hold for any sequence (Ni)i with Ni → ∞, in practice the RQMC
integration error can be minimized by using sample sizes that exploit intrinsic symmetry of the
(t, d)-sequences. Speciﬁcally, for integers b ≥ 2 and M ≥ 1, let

N := {mbk | m ∈ {1, . . . , M }, k ∈ N+}.

(11)

In practice, we chose the MC sample size N from the unique elements of N .

D.4 Asymptotic Optimality of OKG

Consider the case where ftrue is drawn from a GP prior with f d= ftrue, and that g(f ) ≡ f . The
KG policy (i.e., when used to select sequential measurements in a dynamic setting) is known to be
asymptotically optimal [27, 25, 83, 7], meaning that as the number of measurements tends to inﬁnity,
an optimal point x∗ ∈ X ∗
f := arg maxx∈X f (x) is identiﬁed. Although it does not necessarily signify
good ﬁnite sample performance, this is considered a useful property for acquisition functions [25]. In
this section, we state two results showing that OKG also possesses this property, providing further
theoretical justiﬁcation for the MC approach taken by BOTORCH.

Let D0 be the initial data and Dn for n ≥ 1 be the data generated by taking measurements according
to OKG using Nn MC samples in iteration n, i.e., xn+1 ∈ arg maxx∈Xq ˆαKG,Nn (x; Dn) for all n,
and let χn ∈ arg maxx∈X E[f (x) | Dn]. Then we can show the following:
Theorem 4. Suppose conditions (i) and (ii) of Theorem 1 and (iii) of Theorem 2 are satisﬁed. In
addition, suppose that lim supn Nn = ∞. Then, f (χn) → f (x∗) a.s. and in L1.

Theorem 4 shows that OKG is asymptotically optimal if the number of fantasies Nn grows asymptot-
ically with n (this assumes we have an analytic expression for the inner expectation. If not, a similar
condition must be imposed on the number of inner MC samples). In the special case of ﬁnite X, we
can quantify the sample sizes {Nn} that ensure asymptotic optimality of OKG:
Theorem 5. Along with conditions (i) and (ii) of Theorem 1, suppose that |X| < ∞ and q = 1. Then,
if for some δ > 0, Nn ≥ A−1
n log(Kn/δ) a.s., where An and Kn are a.s. ﬁnite and depend on Dn
(these quantities can be computed), we have f (χn) → maxx∈X f (x) a.s..

D.5 Proofs

In the following, we will denote by µD(x) := E[f (x) | D] and KD(x, y) := E[(f (x) −
E[f (x)])(f (y) − E[f (y)])T | D] the posterior mean and covariance functions of f conditioned
on data D, respectively. Under some abuse of notation, we will use µD(x) and KD(x, y) to denote
multi point (vector / matrix)-valued variants of µD and KD, respectively. If f has a GP prior, then
the posterior mean and covariance µD(x) and KD(x, y) have well-known explicit expressions [84].

For notational simplicity and without loss of generality, we will focus on single-output GP case
(m = 1) in this section. Indeed, in the multi-output case (m > 1), we have a GP f on X × M
with M = {1, . . . , m}, and covariance function (x1, i1), (x2, i2) (cid:55)→ ˜K((x1, i1), (x2, i2)). For
q = 1 we then deﬁne x (cid:55)→ ˜f (x) := [f (x, 0), ..., f (x, m)], and then stack these for q > 1: x (cid:55)→
[ ˜f (x1)T , ..., ˜f (xq)T ]T . Then the analysis in the proofs below can be done on mq-dimensional and
mq × mq-dimensional posterior mean and covariance matrices (instead of q and q × q dimensional
ones for m = 1). Differentiability assumptions are needed only to establish certain boundedness
results (e.g. in the proof of Proposition 1), but M is ﬁnite, so we will require differentiability of
K((·, i1), (·, i2)) for each i1 and i2. Assumptions on other quantities can be naturally extended (e.g.
for Theorem 2 g will need to be Lipschitz on Rq×m rather than on Rq, etc.).

Proof of Proposition 1. Without loss of generality, we may assume m = 1 (the multi-output GP
case follows immediately from applying the result below to q(cid:48) = qm and re-arranging the output).
For a GP, we have hD(x, (cid:15)) = µD(x) + LD(x)(cid:15) with (cid:15) ∼ N (0, Iq), where µD(x) is the posterior
mean and LD(x) is the Cholesky decomposition of the posterior covariance MD(x). It is easy
to verify from the classic GP inference equations [84] that if prior mean and covariance function
are continuously differentiable, then so are posterior mean µD(·) and covariance KD(·). Since the
Cholesky decomposition is also continuously differentiable [70], so is LD(·). As X is compact
and µD(·) and LD(·) are continuously differentiable, their derivatives are bounded. It follows from

21

the mean value theorem that there exist Cµ, CL < ∞ s.t. (cid:107)µD(x) − µD(y)(cid:107) ≤ Cµ(cid:107)x − y(cid:107) and
(cid:107)(LD(x) − LD(y))(cid:15)(cid:107) ≤ CL(cid:107)(cid:15)(cid:107)(cid:107)x − y(cid:107). Thus,

(cid:107)hD(x, (cid:15)) − hD(y, (cid:15))(cid:107) = (cid:107)µD(x) − µ(y) + (LD(x) − LD(y))(cid:15)(cid:107)

≤ (cid:107)µD(x) − µD(y)(cid:107) + (cid:107)(LD(x) − LD(y))(cid:15)(cid:107)
≤ (cid:96)h((cid:15))(cid:107)x − y(cid:107)

where (cid:96)h((cid:15)) := Cµ + CL(cid:107)(cid:15)(cid:107). Since, by assumption, g(·) and a(·; Φ) are Lipschitz (say with constants
La and Lg, respectively), it follows that (cid:107)A(x, (cid:15)) − A(y, (cid:15))(cid:107) ≤ LaLg(cid:96)h((cid:15))(cid:107)x − y(cid:107). It thus sufﬁces
to show that (cid:96)h((cid:15)) is integrable. To see this, note that |(cid:96)h((cid:15))| ≤ Cµ + CLC (cid:80)
i |(cid:15)i| for some C < ∞
(equivalence of norms), and that (cid:15)i ∼ N (0, 1) is integrable.

Lemma 1. Suppose that (i) f is a GP with continuously differentiable prior mean and covariance
function, and (ii) that a(·, Φ) and g(·) are Lipschitz. Then, for all x ∈ Xq the moment generating
x (t) := E[etA(x,(cid:15))] of A(x, (cid:15)) and M (cid:96)(t) := E[et(cid:96)((cid:15))] are ﬁnite for all t ∈ R.
functions MA

Proof of Lemma 1. Recall that hD(x, (cid:15)) = µD(x) + LD(x)(cid:15) for the case of f being a GP, where
µD(x) is the posterior mean and LD(x) is the Cholesky decomposition of the posterior covariance
KD(x). Mirroring the argument from the proof of Proposition 1, it is clear that A(x, (cid:15)) is Lipschitz
in (cid:15) for each x ∈ Xq, say with constant ˜CL. Note that this implies that E[|A(x, (cid:15))|] < ∞ for all
x. We can now appeal to results pertaining to the concentration of Lipschitz functions of Gaussian
random variables: the Tsirelson-Ibragimov-Sudakov inequality [11, Theorem 5.5] implies that

t2 ˜C 2
L
2
for any t ∈ R, which is clearly ﬁnite for all t since E[A(x, (cid:15))] ≤ E[|A(x, (cid:15))|]. From the proof of
Proposition 1, we know that A(x, (cid:15)) is (cid:96)((cid:15))-Lipschitz in x, where (cid:96)((cid:15)) is itself Lipschitz in (cid:15). Hence,
the concentration result in Theorem 5.5 of [11] applies again, and we are done.

+ t E[A(x, (cid:15))]

log M A

x (t) ≤

Proof of Theorem 1. Under the stated assumptions, Lemma 1 ensures that condition (10) in The-
orem 3 holds. Further, note that the argument about Lipschitzness of A(x, (cid:15)) in (cid:15) in the proof of
Lemma 1 implies that E[|A(x, (cid:15))|] < ∞ for all x ∈ Xq. Since the {(cid:15)i}N
i=1 are i.i.d, the strong law
of large numbers implies that ˆαN (x) → α(x) a.s. for all x ∈ X. Claims (1) and (2) then follow by
applying Theorem 3, and claim (3) follows by applying Proposition 2.

Proof of Theorem 1(q). Mirroring the proof of Theorem 1, we need to show that ˆαNi(x) → α(x)
a.s. as i → ∞ for all x ∈ Xq. For any x ∈ Xq and any (cid:15)0 ∈ Rq, we have (by convexity and
monotonicity of |x| (cid:55)→ |x|2 and the Lipschitz assumption on a and g) that

|A(x, (cid:15))|2 = |A(x, (cid:15)0) + A(x, (cid:15)) − A(x, (cid:15)0)|2

≤ |A(x, (cid:15)0)|2 + |A(x, (cid:15)) − A(x, (cid:15)0)|2
aL2
≤ |A(x, (cid:15)0)|2 + L2

g(cid:107)hD(x, (cid:15)) − hD(x, (cid:15)0)(cid:107)2

where hD(x, (cid:15)) = µD(x) + LD(x)Φ−1((cid:15)) with Φ−1 the inverse CDF of N (0, 1), applied element-
wise to the vector (cid:15) of qMC samples. Now choose (cid:15)0 = (0.5, . . . , 0.5), then

|A(x, (cid:15))|2 ≤ |a(g(0))|2 + L2

aL2

g(cid:107)hD(x, (cid:15))(cid:107)2

Since the {(cid:15)i} are generated by a nested uniform scramble, we know from Owen [77] that (cid:15) ∼
U [0, 1]q, and therefore Φ−1((cid:15)) ∼ N (0, Iq). Since afﬁne transformations of Gaussians remain
Gaussian, we have that E (cid:2)(cid:107)hD(x, (cid:15))(cid:107)2(cid:3) < ∞. This shows that A(x, (cid:15)) ∈ L2([0, 1]q). That
ˆαNi(x) → 0 a.s. as i → ∞ for all x ∈ Xq now follows from Owen and Rudolf [79, Theorem 3].

Lemma 2. If f is a GP, then fDx (x(cid:48)) = h(x(cid:48), x, (cid:15), (cid:15)I ), where (cid:15) ∼ N (0, Iq) and (cid:15)I ∼ N (0, 1) are
independent and h is linear in both (cid:15) and (cid:15)I .

22

Proof of Lemma 2. This essentially follows from the property of a GP that the covariance con-
ditioned on a new observation (x, y) is independent of y.6 We can write fDx(x(cid:48)) = µDx (x(cid:48)) +
Lσ
Dx

(x(cid:48))(cid:15)I , where

D(x) is the Cholesky decomposition of K σ
Lσ
(x(cid:48)) is the Cholesky decomposition of
Lσ
Dx

µDx (x(cid:48)) := µD(x(cid:48)) + KD(x(cid:48), x)K σ

D(x)−1Lσ

D(x)(cid:15),
D(x) := KD(x, x) + diag(σ2(x1), . . . , σ2(xq)), and

KDx(x(cid:48), x(cid:48)) := K(x(cid:48), x(cid:48)) − KD(x(cid:48), x)K σ

D(x)−1KD(x, x(cid:48)).

Hence, we see that fDx (x(cid:48)) = h(x(cid:48), x, (cid:15), (cid:15)I ), with

h(x(cid:48), x, (cid:15), (cid:15)I ) = µD(x(cid:48)) + KD(x(cid:48), x)K σ

D(x)−1Lσ

D(x)(cid:15) + Lσ
Dx

(x(cid:48))(cid:15)I ,

(12)

which completes the argument.

Theorem 6. Let (an)n≥1 be a sequence of non-negative real numbers such that an → 0. Suppose
that (i) X is a compact metric space, (ii) f is a GP with continuous sample paths and continuous
variance function x (cid:55)→ σ2(x), and (iii) (xn)n≥1 is such that αn
KG(x) − an
inﬁnitely often almost surely. Then αn

KG(xn) > supx∈Xq αn

KG(x) → 0 a.s. for all x ∈ Xq.

Proof of Theorem 6. Bect et al. [7] provide a proof for the case q = 1. Following their exposition,
one ﬁnds that the only thing that needs to be veriﬁed in order to generalize their results to q > 1 is that
condition (c) in their Deﬁnition 3.18 holds also for the case q > 1. What follows is the multi-point
analogue of step (f) in the proof of their Theorem 4.8, which establishes this.
Let µ : X → R and K : X × X → R+ denote mean and covariance function of f . Let Zx :=
f (x) + diag(σ(x)), where σ(x) := (σ(x1), . . . , σ(xq)), with (cid:15) ∼ N (0, Id) independent of f .
Moreover, let x∗ ∈ arg max µ(x). Following the same argument as Bect et al. [7], we arrive at the
intermediate conclusion that E[max{0, Wx,y}] = 0, where Wx,y := E[f (y) | Zx] − E[f (x∗) | Zx].
We need to show that this implies that maxx∈X f (x) = m(x∗).
Under some abuse of notation we will use µ and K also as the vector / matrix-valued mean / kernel
function. Let K σ(x) := K(x, x) + diag(σ(x)) and observe that

Wx,y = µ(y) − µ(x∗) + 1{C(x)(cid:31)0}(K(y, x) − K(x∗, x))K σ(x)−1(Zx − µ(x)),

i.e., Wx,y is Gaussian with Var(Wx,y) = V (x, y, x∗)V (x, y, x∗)T , where V (x, y, x∗)
:=
(K(y, x) − K(x∗, x))K σ(x)−1. Since E[max{0, Wx,y}] = 0, we must have that Var(Wx,y) = 0.
If K σ(x) (cid:31) 0, this means that (K(y, x) − K(x∗, x)) = 0q. But if K σ(x) (cid:54)(cid:31) 0, then K(x, x) (cid:54)(cid:31) 0,
which in turn implies that K(y, x) = K(x∗, x) = 0q. This shows that K(y, x) = K(x∗, x)
for all y ∈ X and all x ∈ X.
In particular, K(x, y) = K(x, x∗) for all y ∈ X. Thus,
K(x, x) − K(x, y) = K(x, x∗) − K(x, x∗) for all y, x ∈ X, and therefore Var(f (x) − f (y)) =
K(x, x) − K(x, y) − K(y, x) + K(y, y) = 0. As in [7] we can conclude that this means that the
sample paths of f − µ are constant over X, and therefore maxx∈X f (x) = m(x∗).

Proof of Theorem 2. From Lemma 2 we have that fDx(x(cid:48)) = h(x(cid:48), x, (cid:15), (cid:15)I ) with h as in (12). With-
out loss of generality, we can absorb (cid:15)I into (cid:15) for the purposes of showing that condition (10) holds
(cid:3). Since the afﬁne (and thus, continuously
for the mapping AKG(x, (cid:15)) := maxx(cid:48)∈X E(cid:2)g(f (x(cid:48))) | Dx
differentiable) transformation g preserves the necessary continuity and differentiability properties, we
can follow the same argument as in the proof of Theorem 1 of [108]. In particular, using continuous
differentiability of GP mean and covariance function, compactness of X, and continuous differentia-
bility of g, we can apply the envelope theorem in the same fashion. From this, it follows that for any
(cid:15) ∈ Rq and for each 1 ≤ l ≤ q, 1 ≤ k ≤ d, the restriction of x (cid:55)→ AKG(x, (cid:15)) to the k, l-th coordinate
is absolutely continuous for all x, thus the partial derivative ∂xlk AKG(x, (cid:15)) exists a.e. Further, for
each l there exist Λl ∈ Rq with (cid:107)Λl(cid:107) < ∞ s.t. |∂xkl AKG(x, (cid:15))| ≤ ΛT
l |ε| a.e. on Xq (here | · | denotes
the element-wise absolute value of a vector). This uniform bound on the partial derivatives can be
used to show that AKG is (cid:96)((cid:15))-Lipschitz. Indeed, writing the difference AKG(y, (cid:15)) − AKG(x, (cid:15)) as a

6In some cases we may consider constructing a heteroskedastic noise model that results in the function σ2(x)
changing depending on observations y, in which case this argument does not hold true anymore. We will not
consider this case further here.

23

sum of differences in each of the qd components of x and y, respectively, using the triangle inequality,
absolute continuity of the element-wise restrictions, and uniform bound on the partial derivatives, we
have that

|AKG(y, (cid:15)) − AKG(x, (cid:15))| ≤

q
(cid:88)

d
(cid:88)

k=1

l=1

ΛT

l |(cid:15)||ykl − xkl| ≤ max
1≤l≤d

(cid:8)ΛT

l |(cid:15)|(cid:9) (cid:107)y − x(cid:107)1

and so (cid:96)((cid:15)) = maxl{ΛT
verify that (cid:96)((cid:15)) is integrable. Indeed,

l |(cid:15)|}. Going back to viewing (cid:15) as a random variable, it is straightforward to

E[|(cid:96)((cid:15))|] ≤ max

l

{(cid:80)q

k=1 ΛlkE[|(cid:15)k|]} = (cid:112)2/π max

l

{(cid:107)Λl(cid:107)1} .

Since g is assumed to be afﬁne in (iii), we can apply Lemma 2 to see that E[g(f (x(cid:48))) | Dx] is a
GP. Therefore, AKG(x, (cid:15)) represents the maximum of a GP and its moment generating function
E[ etAKG(x,(cid:15))] is ﬁnite for all t by Lemma 4. This implies ﬁniteness of its absolute moments [65,
Exercise 9.15] and we have that E[|AKG(x, (cid:15))|] < ∞ for all x ∈ X. Since the {(cid:15)i} are i.i.d, the
strong law of large numbers ensures that ˆαKG,N (x) → αKG(x) a.s. Theorem 3 now applies to obtain
(1) and (2).

Moreover, by the analysis above, it holds that

(cid:96)((cid:15)) = max

{ΛT

l |(cid:15)|} ≤ q max

(cid:107)ΛT

l (cid:107)∞ (cid:107)(cid:15)(cid:107)∞ =: (cid:96)(cid:48)((cid:15)),

l

l

so (cid:96)(cid:48)((cid:15)) is also a Lipschitz constant for AKG(·, (cid:15)). Here, the absolute value version (the second result)
of Lemma 4 applies, so we have that E[ et(cid:96)(cid:48)((cid:15))] is ﬁnite for all t. The conditions of Proposition 2 are
now satisﬁed and we have the desired conclusion.

Proof of Theorem 2(q). In the RQMC setting, we have by Owen [77] that (cid:15) ∼ U [0, 1]q. Therefore,
we are now interested in examining ˜AKG(x, (cid:15)) := AKG(x, Φ−1((cid:15))), since Φ−1((cid:15)) ∼ N (0, Iq).
Following the same analysis as in the proof of Theorem 2, we have Lipschitzness of ˜AKG(·, (cid:15)):

| ˜AKG(y, (cid:15)) − ˜AKG(x, (cid:15))| ≤ (cid:96)(Φ−1((cid:15)))(cid:107)y − x(cid:107)1,

where (cid:96)(·) is as deﬁned in the proof of Theorem 2. As before, (cid:96)(Φ−1((cid:15))) is integrable. Like in
the proof of Theorem 2, ˜AKG(x, (cid:15)) is the maximum of a GP and its moment generating func-
tion E[ et ˜AKG(x,(cid:15))] is ﬁnite for all t by Lemma 4, implying ﬁniteness of its second moment:
E[ ˜AKG(x, (cid:15))2] < ∞ for all x ∈ X. Thus, that ˜AKG(x, (cid:15)) ∈ L2([0, 1]q) and ˆαKG,Ni(x) → αKG(x)
a.s. as i → ∞ for all x ∈ Xq follows from Owen and Rudolf [79, Theorem 3]. Theorem 3 now
allows us to conclude (1) and (2).

The following Lemma will be used to prove Theorem 4:
Lemma 3. Consider a Gaussian Process f on X ⊂ Rd with covariance function K(·, ·) : X×X → R.
Suppose that (i) X is compact, and (ii) K is continuously differentiable. Then f has continuous
sample paths.

Proof of Lemma 3. Since K is continuously differentiable and X is compact, K is Lipschitz on X×X,
i.e., ∃ L < ∞ such that |K(x, y)−K(x(cid:48), y(cid:48))| ≤ L(cid:0)(cid:107)x−x(cid:48)(cid:107)+(cid:107)y−y(cid:48)(cid:107)(cid:1) for all (x, y), (x(cid:48), y(cid:48)) ∈ X×X.
Thus

E|f (x) − f (x)|2 = K(x, x) − 2K(x, y) + K(y, y)

≤ |K(x, x) − K(x, y)| + |K(y, y) − K(x, y)|
≤ 2L(cid:107)x − y(cid:107)

Since X is compact, there exists C := maxx,y∈X (cid:107)x − y(cid:107) < ∞. With this it is easy to verify that
there exist C (cid:48) < ∞ and η > 0 such that 2L(cid:107)x − y(cid:107) < C (cid:48)| log (cid:107)x − y(cid:107)|−(1+η) for all x, y ∈ X.
Continuity of the sample paths then follows from Theorem 3.4.1 in [3].

24

KG,Nn

(x) for all n, the almost sure convergence of ˆxn

KG from Theorem 2 together with continuity of αn

Proof of Theorem 4. From Lemma 3 we know that the GP has continuous sample paths. If xn+1 ∈
arg maxx∈Xq ˆαn
to the set of optimizers
of αn
KG (established in the proof of Theorem 2)
implies that for all δ > 0 and each n ≥ 1, ∃ Nn < ∞ such that αn
KG(xn+1) > supx∈Xq αn
KG(x) − δ.
As lim supn Nn = ∞, ∃ (an)n≥1 with an → 0 such that αn
KG(x) − an
KG(x) → 0 a.s. for all x ∈ Xq then follows from Theorem 6. The convergence
inﬁnitely often. That αn
result for f (χn) then follows directly from Proposition 4.9 in [7].
Lemma 4. Let f be a mean zero GP deﬁned on X such that |f (x)| < ∞ almost surely for each
x ∈ X. It holds that the moment generating functions of supx∈X f (x) and supx∈X |f (x)| are both
ﬁnite, i.e.,

KG(xn+1) > supx∈Xq αn

KG,Nn

E(cid:2)et supx∈X f (x)(cid:3) < ∞ and E(cid:2)et supx∈X |f (x)|(cid:3) < ∞

for any t ∈ R.

Proof of Lemma 4. Let (cid:107)f (cid:107) := supx∈X f . Since the sample paths of f are almost surely ﬁnite, the
Borell-TIS inequality [2, Theorem 2.1] states that E (cid:107)f (cid:107) < ∞. We ﬁrst consider t > 0 and begin by
re-writing the expectation as
(cid:90) ∞

E(cid:2)et (cid:107)f (cid:107)(cid:3) =

P(cid:0)et (cid:107)f (cid:107) > u(cid:1) du

0

≤ 1 +

= 1 +

(cid:90) ∞

1
(cid:90) ∞

P(cid:0)et (cid:107)f (cid:107) > u(cid:1) du

P(cid:0)(cid:107)f (cid:107) − E (cid:107)f (cid:107) > t−1 log u − E (cid:107)f (cid:107)(cid:1) du

1
= 1 + tet E(cid:107)f (cid:107)

(cid:90) ∞

P(cid:0)(cid:107)f (cid:107) − E (cid:107)f (cid:107) > u(cid:1) etu du

≤ 1 + tet E(cid:107)f (cid:107)

−E (cid:107)f (cid:107)

(cid:20)(cid:90) 0

min{−E(cid:107)f (cid:107), 0}

≤ 1 + (cid:12)

(cid:12)E (cid:107)f (cid:107)(cid:12)

(cid:12) tet E(cid:107)f (cid:107) + tet E(cid:107)f (cid:107)

(cid:90) ∞

(cid:21)
P(cid:0)(cid:107)f (cid:107) − E (cid:107)f (cid:107) > u(cid:1) etu du

+

0
(cid:90) ∞

0

P(cid:0)(cid:107)f (cid:107) − E (cid:107)f (cid:107) > u(cid:1) etu du,

(13)

where a change of variables is performed in the third equality. Let σ2
now use the Borell-TIS inequality to bound the tail probability in (13) by 2e−u2/(2σ2

X := supx∈X E[f (x)2]. We can

X), obtaining:

E(cid:2)et (cid:107)f (cid:107)(cid:3) ≤ 1 + (cid:12)

(cid:12)E (cid:107)f (cid:107)(cid:12)

(cid:12) tet E(cid:107)f (cid:107) + tet E(cid:107)f (cid:107)

(cid:90) ∞

0

2e−u2/(2σ2

X)+tu du < ∞.

Similarly, for t < 0, we have:

E(cid:2)et (cid:107)|f |(cid:107)(cid:3) =

(cid:90) ∞

P(cid:0)et (cid:107)|f |(cid:107) > u(cid:1) du

0

≤ 1 +

= 1 +

(cid:90) ∞

1
(cid:90) ∞

1

P(cid:0)et (cid:107)|f |(cid:107) > u(cid:1) du

P(cid:0)(cid:107)|f |(cid:107) − E (cid:107)f (cid:107) < t−1 log u − E (cid:107)f (cid:107)(cid:1) du

= 1 − tet E(cid:107)f (cid:107)

(cid:90) −E (cid:107)f (cid:107)

P(cid:0)(cid:107)|f |(cid:107) − E (cid:107)f (cid:107) < u(cid:1) etu du

−∞

(cid:20)(cid:90) max{−E(cid:107)f (cid:107), 0}

≤ 1 − tet E(cid:107)f (cid:107)

(cid:90) 0

+

(cid:21)
P(cid:0)(cid:107)|f |(cid:107) − E (cid:107)f (cid:107) < u(cid:1) etu du

0

−∞

≤ 1 − (cid:12)

(cid:12)E (cid:107)f (cid:107)(cid:12)

(cid:12) tet E(cid:107)f (cid:107) − tet E(cid:107)f (cid:107)

(cid:90) 0

−∞

P(cid:0)(cid:107)|f |(cid:107) − E (cid:107)f (cid:107) < u(cid:1) etu du,

(14)

The same can be done for (14) to conclude that E(cid:2)et (cid:107)f (cid:107)(cid:3) < ∞ for all t. For the case of E(cid:2)et (cid:107)|f |(cid:107)(cid:3)
and t > 0, we use a similar line of analysis as (13) along with the observation that

P(cid:0)(cid:107)|f |(cid:107) − E (cid:107)f (cid:107) > u(cid:1) ≤ 2 P(cid:0)(cid:107)f (cid:107) − E (cid:107)f (cid:107) > u(cid:1).

25

For t < 0, the result is clear because (cid:107)|f |(cid:107) ≥ 0.

Proof of Theorem 5. Since we are in the case of ﬁnite X, let µn and Σn denote the posterior mean
vector and covariance matrix of our GP after conditioning on Dn. First, we give a brief outline of
the argument. We know from previous work (Lemma A.6 of [25] or Lemma 3 of [83]) that given a
posterior distribution parameterized by µ and Σ, if αKG(x; µ, Σ) = 0 for all x ∈ X, then an optimal
design is identiﬁed:

arg max
x∈X

µ(x) = arg max

x∈X

f (x)

almost surely. Thus, we can use the true KG values as a “potential function” to quantify how the
OKG policy performs asymptotically, even though we are never using the KG acquisition function
for selecting points. We emphasize that the data that induce {µn}n≥0 and {Σn}n≥0 are collected
using the OKG policy.

By a martingale convergence argument, there exists a limiting posterior distribution described by
random variables (µ∞, Σ∞), i.e., µn → µ∞ and Σn → Σ∞ almost surely [25, Lemma A.5]. Let
A ⊆ X be a subset of the feasible space. As was done in the proof of Theorem 4 of [25], we deﬁne
the event:

HA = (cid:8)αKG(x; µ∞, Σ∞) > 0, x ∈ A(cid:9) ∩ (cid:8)αKG(x; µ∞, Σ∞) = 0, x (cid:54)∈ A(cid:9).
Note that HA, for all possible subsets A, partition the sample space. Consider some A (cid:54)= ∅. By
Lemma A.7 of [25], if αKG(x; µ∞, Σ∞) > 0, then x is measured a ﬁnite number of times, meaning
that there exists an almost surely ﬁnite random variable M0 such that on iterations after N0, OKG
stops sampling from A. By the deﬁnition of HA in (15), there must exist another random iteration
index M1 ≥ M0 such that when n ≥ N1,

(15)

min
x∈A

αKG(x; µn, Σn) > max
x(cid:54)∈A

αKG(x; µn, Σn),

implying that the exact KG policy must prefer points in A over all others after iteration M1. This
implies that

HA ⊆

(cid:110)

arg max
x∈X

ˆαKG,Nn (x, µn, Σn) (cid:54)⊆ arg max

x∈X

αKG(x, µn, Σn), ∀ n ≥ M1 − 1

(cid:111)

=: E,

because if not, then there exists an iteration after M0 where an element from A is selected, which is a
contradiction. As shown in the proof of Lemma 2, the next period posterior mean E(cid:2)g(f (x(cid:48))) | Dx
(cid:3)
is a GP. Therefore, by Lemma 4, the moment generating function of maxx(cid:48)∈X E(cid:2)g(f (x(cid:48))) | Dx
(cid:3) is
ﬁnite. Theorem 2.6 of [39] establishes that our choice of Nn guarantees

(cid:104)
P

arg max
x∈X

ˆαKG,Nn (x, µn, Σn) (cid:54)⊆ arg max

x∈X

αKG(x, µn, Σn) | Fn

(cid:105)

≤ δ,

from which it follows that

(cid:104)
log P

∞
(cid:88)

n=0

arg max
x∈X

ˆαKG,Nn (x, µn, Σn) (cid:54)⊆ arg max

x∈X

αKG(x, µn, Σn) | Fn

(cid:105)

= −∞.

After writing the probability of E as an inﬁnite product and performing some manipulation, we
see that the above condition implies that the probability of event E is zero, and we conclude that
P(HA) = 0 for any nonempty A. Therefore, P(H∅) = 1 and αKG(x; µ∞, Σ∞) = 0 for all x almost
surely.

E Illustration of Sample Average Approximation

QMC methods have been used in other applications in machine learning, including variational
inference [13] and evolutionary strategies [86], but rarely in BO. Letham et al. [58] use QMC in the
context of a speciﬁc acquisition function. BOTORCH’s abstractions make it straightforward (and
mostly automatic) to use QMC integration with any acquisition function.
Using SAA, i.e., ﬁxing the base samples E = {(cid:15)i}, introduces a consistent bias in the function
approximation. While i.i.d. re-sampling in each evaluation ensures that ˆαN (x, Φ, D) and ˆαN (y, Φ, D)
are conditionally independent given (x, y), this no longer holds when ﬁxing the base samples.

26

Figure 18: MC and QMC acquisition functions, with and without re-drawing the base samples between
evaluations. The model is a GP ﬁt on 15 points randomly sampled from X = [0, 1]6 and evaluated on the
(negative) Hartmann6 test function. The acquisition functions are evaluated along the slice x(λ) = λ1.

Figure 18 illustrates this behavior for EI (we consider the simple case of q = 1 for which we have an
analytic ground truth available). The top row shows the MC and QMC version, respectively, when
re-drawing base samples for every evaluation. The solid lines correspond to a single realization, and
the shaded region covers four standard deviations around the mean, estimated across 50 evaluations.
It is evident that QMC sampling signiﬁcantly reduces the variance of the estimate. The bottom
row shows the same functions for 10 different realizations of ﬁxed base samples. Each of these
realizations is differentiable w.r.t. x (and hence λ in the slice parameterization). In expectation (over
the base samples), this function coincides with the true function (the dashed black line). Conditional
on the base sample draw, however, the estimate displays a consistent bias. The variance of this bias
(across re-drawing the base samples) is much smaller for the QMC versions.

Figure 19: Performance for optimizing QMC-based EI. Solid lines: ﬁxed base samples, optimized via L-BFGS-B.
Dashed lines: re-sampling base samples, optimized via Adam (lr=0.025).

Even thought the function values may show noticeable bias, the bias of the maximizer (in X) is
typically very small. Figure 19 illustrates this behavior, showing empirical cdfs of the relative gap
1 − α(ˆx∗
N (cid:107)2 over 250 optimization runs for different numbers
of samples, where x∗ is the optimizer of the analytic function EI, and ˆx∗
N is the optimizer of the
QMC approximation. The quality of the solution of the deterministic problem is excellent even for
relatively small sample sizes, and generally better than of the stochastic optimizer.

N )/α(x∗) and the distance (cid:107)x∗ − ˆx∗

Figure 20 shows empirical mean and variance of the metrics from Figure 19 as a function of the
number of MC samples N on a log-log scale. The stochastic optimizer used is Adam with a learning
rate of 0.025. Both for the SAA and the stochastic version we use the same number of random restart
initial conditions generated from the same initialization heuristic.

Empirical asymptotic convergence rates can be obtained as the slopes of the OLS ﬁt (dashed lines),
and are given in Table 1. It is quite remarkable that in order to achieve the same error as the MC
approximation with 4096 samples, the QMC approximation only requires 64 samples. This holds true
for the bias and variance of the (relativized) optimal value as well as for the distance from the true
optimizer. That said, as we are in a BO setting, we are not necessarily interested in the estimation
error ˆα∗

N − α∗ of the optimum, but primarily in how far x∗

N is from the true optimizer x∗.

27

0.00.10.20.3EIMC, n=32analyticqMC, n=32analytic0.00.20.40.60.81.00.00.10.20.3EIMC, n=32 (fixed)analytic0.00.20.40.60.81.0qMC, n=32 (fixed)analytic0.000.020.040.060.080.100.120.141(x*N)/(x*)0.000.250.500.751.00empirical CDFrelative gap at optimizerN=128N=64N=32N=160.000.020.040.060.080.10||x*x*N||2distance of optimizersN=128N=64N=32N=16Figure 20: Bias and variance of optimizer x∗
of the number of (Q)MC samples for both SAA and stochastic optimzation (“re-sample”).

N and true EI value EI(x∗

N ) evaluated at the optimizer as a function

MC

QMC

MC† QMC†

N /α∗]

E[1 − ˆα∗
Var(1 − ˆα∗
E[(cid:107)x∗
Var((cid:107)x∗

−0.52 −0.95 −0.10 −0.26
N /α∗) −1.16 −2.11 −0.19 −0.35
−1.04 −1.94 −0.16 −0.47
N − x∗(cid:107)2) −2.24 −4.14 −0.30 −0.63

N − x∗(cid:107)2]

Table 1: Empirical asymptotic convergence rates for the setting in Figure 20 (†denotes re-sampling + optimization
with Adam).

A somewhat subtle point is that whether better optimization of the acquisition function results in
improved closed-loop BO performance depends on the acquisition function as well as the underlying
problem. More exploitative acquisition functions, such as EI, tend to show worse performance for
problems with high noise levels. In these settings, not solving the EI maximization exactly adds
randomness and thus induces additional exploration, which can improve closed-loop performance.
While a general discussion of this point is outside the scope of this paper, BOTORCH does provide
a framework for optimizing acquisition functions well, so that these questions can be compartmen-
talized and acquisition function performance can be investigated independently from the quality of
optimization.

Perhaps the most signiﬁcant advantage of using deterministic optimization algorithms is that, unlike
for algorithms such as SGD that require tuning the learning rate, the optimization procedure is
essentially hyperparameter-free. Figure 8 shows the closed-loop optimization performance of qEI
for both deterministic and stochastic optimization for different optimizers and learning rates. While
some of the stochastic variants (e.g. ADAM with learning rate 0.01) achieve performance similar
to the deterministic optimization, the type of optimizer and learning rate matters. In fact, the rank
order of SGD and ADAM w.r.t. to the learning rate is reversed, illustrating that selecting the right
hyperparameters for the optimizer is itself a non-trivial problem.

F Additional Implementation Details

F.1 Batch Initialization for Multi-Start Optimization

For most acquisition functions, the optimization surface is highly non-convex, multi-modal, and
(especially for “improvement-based” ones such as EI or KG) often ﬂat (i.e. has zero gradient) in
much of the domain X. Therefore, optimizing the acquisition function is itself a challenging problem.

28

3025201510log2E[1(x*N)/*]optimal value, biasMC (SAA)MC (re­sample)qMC (SAA)qMC (re­sample)15.012.510.07.55.0log2E[||x*Nx*||2]optimizer, bias (2­norm)MC (SAA)MC (re­sample)qMC (SAA)qMC (re­sample)166425610244096N6050403020log2Var(1(x*N)/*)optimal value, varianceMC (SAA)MC (re­sample)qMC (SAA)qMC (re­sample)166425610244096N353025201510log2Var(||x*Nx*||2)optimizer, variance (2­norm)MC (SAA)MC (re­sample)qMC (SAA)qMC (re­sample)The simplest approach is to use zeroth-order optimizers that do not require gradient information, such
as DIRECT or CMA-ES [45, 37]. These approaches are feasible for lower-dimensional problems,
but do not scale to higher dimensions. Note that performing parallel optimization over q candidates
in a d-dimensional feature space means solving a qd-dimensional optimization problem.

A more scalable approach incorporates gradient information into the optimization. As described in
Section 4, BOTORCH by default uses quasi-second order methods, such as L-BFGS-B. Because of
the complex structure of the objective, the initial conditions for the algorithm are extremely important
so as to avoid getting stuck in a potentially highly sub-optimal local optimum. To reduce this risk,
one typically employs multi-start optimization (i.e. start the solver from multiple initial conditions
and pick the best of the ﬁnal solutions). To generate a good set of initial conditions, BOTORCH
heavily exploits the fast batch evaluation discussed in the previous section. Speciﬁcally, BOTORCH
by default uses Nopt initialization candidates generated using the following heuristic:
1. Sample ˜N0 quasi-random q-tuples of points ˜x0 ∈ R ˜N0×q×d from Xq using quasi-random Sobol

sequences.

2. Batch-evaluate the acquisition function at these candidate sets: ˜v = α(˜x0; Φ, D).
3. Sample N0 candidate sets x ∈ RN0×q×d according to the weight vector p ∝ exp(ηv), where
v = (˜v − ˆµ(˜v))/ˆσ(˜v) with ˆµ and ˆσ the empirical mean and standard deviation, respectively, and
η > 0 is a temperature parameter. Acquisition functions that are known to be ﬂat in large parts
of Xq are handled with additional care in order to avoid starting in locations with zero gradients.

Sampling initial conditions this way achieves an exploration/exploitation trade-off controlled by
the magnitude of η. As η → 0 we perform Sobol sampling, while η → ∞ means the initialization
is chosen in a purely greedy fashion. The latter is generally not advisable, since for large ˜N0 the
highest-valued points are likely to all be clustered together, which would run counter to the goal of
multi-start optimization. Fast batch evaluation allows evaluating a large number of samples ( ˜N0 in
the tens of thousands is feasible even for moderately sized models).

F.2 Sequential Greedy Batch Optimization

The pending points approach discussed in Section 5 provides a natural way of generating parallel BO
candidates using sequential greedy optimization, where candidates are chosen sequentially, while
in each step conditioning on selected points and integrating over the uncertainty in their outcome
(using MC integration). By using a full MC formulation, in which we jointly sample at new and
pending points, we avoid constructing an individual “fantasy” model for each sampled outcome, a
common (and costly) approach in the literature [94]. In practice, the sequential greedy approach often
performs well, and may even outperform the joint optimization approach, since it involves a sequence
of small, simpler optimization problems, rather than a larger and complex one that is harder to solve.

[105] provide a theoretical justiﬁcation for why the sequential greedy approach works well with a
class of acquisition functions that are submodular.

G Active Learning Example

Recall from Section 5 the negative integrated posterior variance (NIPV) [91, 17] of the model:

NIPV(x) = −

(cid:90)

X

E(cid:2)Var(f (x) | Dx) | D(cid:3) dx.

(16)

We can implement (16) using standard BOTORCH components, as shown in Code Example 4. Here
mc_points is the set of points used for MC-approximating the integral. In the most basic case,
one can use QMC samples drawn uniformly in X. By allowing for arbitrary mc_points, we permit
weighting regions of X using non-uniform sampling. Using mc_points as samples of the maximizer
of the posterior, we recover the recently proposed Posterior Variance Reduction Search [74] for BO.

This acquisition function supports both parallel selection of points and asynchronous evaluation.
Since MC integration requires evaluating the posterior variance at a large number of points, this
acquisition function beneﬁts signiﬁcantly from the fast predictive variance computations in GPyTorch
[82, 29].

29

fant_model = self . model . fantasize (

@ c o n c a t e n a t e _ p e n d i n g _ p o i n t s
@ t _ b a t c h _ m o d e _ t r a n s f o r m ()
def forward ( self , X : Tensor ) -> Tensor :

1 class q N e g a t i v e I n t e g r a t e d P o s t e r i o r V a r i a n c e ( A n a l y t i c A c q u i s i t i o n F u n c t i o n ) :
2
3
4
5
6
7
8
9
10
11
12
13
14
15

)
sz = [1] * len ( X . shape [: -2]) + [ -1 , X . size ( -1) ]
mc_points = self . mc_points . view (* sz )
with settings . propagate_grads ( True ) :

ivar = posterior . variance . mean ( dim = -2)
return - ivar . view ( X . shape [: -2])

X =X , sampler = self . _dummy_sampler ,
ob ser vat ion _n ois e = True

posterior = fant_model . posterior ( mc_points )

Code Example 4: Active Learning (NIPV)

To illustrate how NIPV may be used in combination with scalable probabilistic modeling, we examine
the problem of efﬁcient allocation of surveys across a geographic region. Inspired by Cutajar et al.
[18], we utilize publicly-available data from The Malaria Atlas Project (2019) dataset, which includes
the yearly mean parasite rate (along with standard errors) of Plasmodium falciparum at a 4.5km2
grid spatial resolution across Africa. In particular, we consider the following active learning problem:
given a spatio-temporal probabilistic model ﬁt to data from 2011-2016, which geographic locations
in and around Nigeria should one sample in 2017 in order to minimize the model’s error for 2017
across all of Nigeria?

We ﬁt a heteroskedastic GP model to 2500 training points prior to 2017 (using a noise model that
is itself a GP ﬁt to the provided standard errors). We then select q = 10 sample locations for 2017
using the NIPV acquisition function, and make predictions across the entirety of Nigeria using this
new data. Compared to using no 2017 data, we ﬁnd that our new dataset reduces MSE by 16.7% on
average (SEM = 0.96%) across 60 subsampled datasets. By contrast, sampling the new 2017 points at
a regularly spaced grid results only in a 12.4% reduction in MSE (SEM = 0.99%). The mean relative
improvement in MSE reduction from NIPV optimization is 21.8% (SEM = 6.64%). Figure 21 shows
the NIPV-selected locations on top of the base model’s estimated parasite rate and standard deviation.

Figure 21: Locations for 2017 samples from IPV minimization and the base grid. Observe how the NIPV
samples cluster in higher variance areas.

30

Figure 22: Composite function optimization for q = 1 Figure 23: Composite function optimization for q = 3

H Additional Implementation Examples

Comparing Implementation Complexity

Many of BOTORCH’s beneﬁts are qualitative, including the simpliﬁcation and acceleration of im-
plementing new acquisition functions. Quantifying this in a meaningful way is very challenging.
Comparisons are often made in terms of Lines of Code (LoC) - while this metric is problematic when
comparing across different design philosophies, non-congruent feature sets, or even programming
languages, it does provides a general idea of the effort required for developing and implementing new
methods.
MOE’s KG involves thousands of LoC in C++ and python spread across a large number of ﬁles,7
while our more efﬁcient implementation is <30 LoC. Astudillo and Frazier [6] is a full paper in
last year’s installment of this conference,8 whose composite function method we implement and
signiﬁcantly extend (e.g to support KG) in 7 LoC using BoTorch’s abstractions. The original NEI
implementation is >250 LoC, while the one from Code Example 2 is 14 LoC.

H.1 Composite Objectives

We consider the Bayesian model calibration of a simulator with multiple outputs from Section
5.3 of Astudillo and Frazier [6]. In this case, the simulator from Bliznyuk et al. [9] models the
concentrations of chemicals at 12 positions in a one-dimensional channel. Instead of modeling the
overall loss function (which measures the deviation of the simulator outputs with a set of observations)
directly, we follow Astudillo and Frazier [6] and model the underlying concentrations while utilizing
a composite objective approach. A powerful aspect of BOTORCH’s modular design is the ability to
easily combine different approaches into one. For the composite function problem in this section this
means that we can easily extend the work by Astudillo and Frazier [6] not only to use the Knowledge
Gradient, but also to the “parallel BO” setting of jointly selecting q > 1 points. Figures 22 and 22
show results for this with q = 1 and q = 3, repspectively. The plots show log regret evaluated at the
maximizer of the posterior mean averaged over 250 trials. While the performance of EI-CF is similar
for q = 1 and q = 3, KG-CF reaches lower regret signiﬁcantly faster for q = 1 compared to q = 3,
suggesting that “looking ahead“ is beneﬁcial in this context.

H.2 Generalized UCB

Code Example 5 presents a generalized version of parallel UCB from Wilson et al. [104] supporting
pending candidates, generic objectives, and QMC sampling. If no sampler is speciﬁed, a default
QMC sampler is used. Similarly, if no objective is speciﬁed, the identity objective is assumed.

H.3 Full Code Examples

In this section we provide full implementations for the code examples. Speciﬁcally, we include
parallel Noisy EI (Code Example 6), OKG (Code Example 7), and (negative) Integrated Posterior
Variance (Code Example 8).

7https://github.com/wujian16/Cornell-MOE
8Code available at https://github.com/RaulAstudillo06/BOCF

31

5101520253035Number of function evaluations543210log10 regretRNDRND CFEIEI CFOKGOKG CF51015202530Number of function evaluations543210log10 regretRNDRND CFEIEI CFOKGOKG CF) -> None :

def __init__ (
self ,
model : Model ,
beta : float ,
sampler : Optional [ MCSampler ] = None ,
objective : Optional [ MC Acq ui sit io nOb jec ti ve ] = None ,
X_pending : Optional [ Tensor ] = None ,

1 class q U ppe rC o nf id e nc eBo und ( MC Acqui sition Funct ion ) :
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

@ c o n c a t e n a t e _ p e n d i n g _ p o i n t s
@ t_ b a tc h _ m od e _ t ra n s f or m ()
def forward ( self , X : Tensor ) -> Tensor :
posterior = self . model . posterior ( X )
samples = self . sampler ( posterior )
obj = self . objective ( samples )
mean = obj . mean ( dim =0)
z = mean + self . beta_prime * ( obj - mean ) . abs ()
return z . max ( dim = -1) . values . mean ( dim =0)

super () . __init__ ( model , sampler , objective , X_pending )
self . beta_prime = math . sqrt ( beta * math . pi / 2)

Code Example 5: Generalized Parallel UCB

) -> None :

super () . __init__ ( model , sampler , objective , X_pending )
self . register_buffer ( " X_baseline " , X_baseline )

def __init__ (
self ,
model : Model ,
X_baseline : Tensor ,
sampler : Optional [ MCSampler ] = None ,
objective : Optional [ MC Acq ui sit io nOb jec ti ve ] = None ,
X_pending : Optional [ Tensor ] = None ,

1 class q N o i s y E x p e c t e d I m p ro v e m e n t ( MCA cquisi tionF unctio n ) :
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25

q = X . shape [ -2]
X_bl = match_shape ( self . X_baseline , X )
X_full = torch . cat ([ X , X_bl ] , dim = -2)
posterior = self . model . posterior ( X_full )
samples = self . sampler ( posterior )
obj = self . objective ( samples )
obj_n = obj [... ,: q ]. max ( dim = -1) . values
obj_p = obj [... , q :]. max ( dim = -1) . values
return ( obj_n - obj_p ) . clamp_min (0) . mean ( dim =0)

@ c o n c a t e n a t e _ p e n d i n g _ p o i n t s
@ t_ b a tc h _ m od e _ t ra n s f or m ()
def forward ( self , X : Tensor ) -> Tensor :

Code Example 6: Parallel Noisy EI (full)

32

super () . __init__ ( model , sampler , objective , X_pending )
self . inner_sampler = inner_sampler

splits = [ X . size ( -2) - self . Nf , self . N_f ]
X , X_fantasies = torch . split (X , splits , dim = -2)
# [...] some re - shaping for batch evaluation purposes
if self . X_pending is not None :

) -> None :

def forward ( self , X : Tensor ) -> Tensor :

def __init__ (
self ,
model : Model ,
sampler : MCSampler ,
objective : Optional [ Objective ] = None ,
inner_sampler : Optional [ MCSampler ] = None ,
X_pending : Optional [ Tensor ] = None ,

1 class qKnowledgeGr ad ient ( M CAcqu isitio nFunc tion ) :
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35

)
obj = self . objective
if isinstance ( obj , M C Ac qu isi ti onO bje ct ive ) :

X_p = match_shape ( self . X_pending , X )
X = torch . cat ([ X , X_p ] , dim = -2)

with settings . propagate_grads ( True ) :
values = inner_acqf ( X_fantasies )

X =X ,
sampler = self . sampler ,
obser vation_noise = True ,

fmodel = self . model . fantasize (

inner_acqf = SimpleRegret (

return values . mean ( dim =0)

)
else :

inner_acqf = PosteriorMean ( fmodel , objective = obj )

fmodel , sample = self . inner_sampler , objective = obj ,

Code Example 7: One-Shot Knowledge Gradient (full)

33

) -> None :

super () . __init__ ( model = model )
self . _dummy_sampler = IIDNormalSampler (1)
self . X_pending = X_pending
self . register_buffer ( " mc_points " , mc_points )

def __init__ (
self ,
model : Model ,
mc_points : Tensor ,
X_pending : Optional [ Tensor ] = None ,

1 class q N e g I n t e g r a t e d P o s t e r i o r V a r i a n c e ( A n a l y t i c A c q u i s i t i o n F u n c t i o n ) :
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26

)
batch_ones = [1] * len ( X . shape [: -2])
mc_points = self . mc_points . view (* batch_ones , -1 , X . size ( -1) )
with settings . propagate_grads ( True ) :

@ c o n c a t e n a t e _ p e n d i n g _ p o i n t s
@ t _ ba t c h _m o d e _t r a ns f o r m ()
def forward ( self , X : Tensor ) -> Tensor :

X =X ,
sampler = self . _dummy_sampler ,
obser vation_noise = True ,

ivar = posterior . variance . mean ( dim = -2)
return - ivar . view ( X . shape [: -2])

posterior = fant_model . posterior ( mc_points )

fant_model = self . model . fantasize (

Code Example 8: Active Learning (full)

34

