2
2
0
2

p
e
S
1
2

]

C
O
.
h
t
a
m

[

4
v
0
1
0
1
1
.
8
0
2
2
:
v
i
X
r
a

Convex integer optimization with Frank-Wolfe methods

Deborah Hendrych
Freie UniversitÃ¤t Berlin, Germany
Zuse Institute Berlin, Germany

Hannah Troppens
Freie UniversitÃ¤t Berlin, Germany
Zuse Institute Berlin, Germany

Mathieu BesanÃ§on
Zuse Institute Berlin, Germany

Sebastian Pokutta
Technische UniversitÃ¤t Berlin, Germany
Zuse Institute Berlin, Germany

hendrych@zib.de

troppens@zib.de

besancon@zib.de

pokutta@zib.de

Abstract
Mixed-integer nonlinear optimization is a broad class of problems that feature combinatorial structures and
nonlinearities. Typical exact methods combine a branch-and-bound scheme with relaxation and separation
subroutines. We investigate the properties and advantages of error-adaptive ï¬rst-order methods based on the
Frank-Wolfe algorithm for this setting, requiring only a gradient oracle for the objective function and linear
optimization over the feasible set. In particular, we will study the algorithmic consequences of optimizing
with a branch-and-bound approach where the subproblem is solved over the convex hull of the mixed-integer
feasible set thanks to linear oracle calls, compared to solving the subproblems over the continuous relaxation
of the same set. This novel approach computes feasible solutions while working on a single representation of
the polyhedral constraints, leveraging the full extent of Mixed-Integer Programming (MIP) solvers without an
outer approximation scheme.

1. Introduction
Mixed-integer nonlinear optimization problems (MINLP) is a challenging class of problems combining
both combinatorial structures and nonlinearities which can model a broad range of problems arising in
In Machine
engineering, transportation, and more generally operations and other application contexts.
Learning, combinatorial constraints can capture rich properties demanded of a solution, e.g. solutions that
must be a path, cycle, or tour in a graph, or solutions with maximum support or guaranteed sparsity. The
dominant algorithmic frameworks for solving such problems are combinations of BnB with spatial and integer
branching and cutting planes tightening local linear relaxations.

We focus in this paper on mixed-integer convex problems in which the nonlinear constraints and objectives
are convex and present a new algorithmic framework for solving these problems that exploit recent advances
in so-called Frank-Wolfe (FW) methods (also called: conditional gradient (CG) methods). The problem class
we consider is of the type:

ğ‘“ (ğ‘¥, y)

min
ğ‘¥,y
s.t. (ğ‘¥, y) âˆˆ X Ã— Y
ğ‘¥ ğ‘— âˆˆ â„¤ âˆ€ ğ‘— âˆˆ ğ½,

(1a)

(1b)

(1c)

where X is a compact subset formed by polyhedral constraints and possibly contains combinatorial structures,
Y is a (potentially non-polyhedral) compact convex set over which optimizing a linear function is computa-
tionally tractable and less demanding than the original problem, ğ‘“ : X Ã— Y â†’ â„ is a diï¬€erentiable convex
function with a Lipschitz-smooth gradient accessed as an oracle, we do not require an expression graph.

1

 
 
 
 
 
 
Several families of methods have been developed over the last decades, rooted in both nonlinear and discrete
optimization approaches; see Kronqvist et al. (2019) for a recent review of algorithms and solvers for convex
MINLPs. Some solvers implement a branch-and-bound algorithm that solves a continuous relaxation including
the nonlinearities at each node of the branch-and-bound tree; see e.g., Knitro (Byrd et al., 2006) or Couenne
(Belotti, 2009). Solving nonlinear problems at each node is however expensive: the typical algorithms used
are based on interior-point methods (almost always relying on expensive second-order information), which
yield high-precision results at the cost of more expensive iterations. Another approach, introduced by Quesada
& Grossmann (1992), solves Linear Programs (LPs) at each node and nonlinear problems only to compute
feasible solutions and additional cuts when integer solutions are found. This method is also implemented in
SCIP (Bestuzheva et al., 2021) for convex or nonconvex nonlinearities. The solvers SHOT (Lundell et al.,
2022) and Pavito.jl (JuMP-dev, 2018) use single-tree polyhedral outer approximation approaches, relying on
the Mixed-Integer Programming (MIP) solver to build the main relaxation of the problem. Bonmin (Bonami
et al., 2008) uses a hybrid approach with both outer polyhedral approximations and branch-and-bound. In
Chen & Goulart (2022), mixed-integer conic problems are tackled with ADMM using inexact termination
and a safe dual bound recovery in a BnB framework, similarly to our approach for the inexact subproblem
exploitation. However, in our case, the FW gap computed at each iteration directly produces a dual bound that
can be used to terminate node processing and obtain a global tree bound. Furthermore, our approach avoids
projections onto cones which can be expensive (requiring eigendecompositions) or numerically unstable (for
exponential and positive semideï¬nite cones for instance). It is important to note that in many MINLP solution
approaches, nonlinearities are handled through separation (via gradient cuts or supporting hyperplane cuts).
Separation is not possible if the nonlinearity is in the objective and most models therefore only allow for
nonlinearities in the constraints requiring an epigraph formulation for the objective function. By doing so,
the initial structure of Problem (1) is lost. Some lines of work focused on branch-and-bound approaches
for specialized sparse regression problems, also referred to as best subset selection (Bertsimas et al., 2016;
Hazimeh & Mazumder, 2020; GÃ³mez & Prokopyev, 2021; Moreira Costa et al., 2022). These approaches
however rely on the properties of the quadratic loss and/or of the constraint set and would not extend to other
loss functions (e.g. logistic or Poisson regression) or additional constraints (arbitrary linear constraints on the
features, grouped subset selection).
Our approach and contribution
Our approach for Problem (1) is markedly diï¬€erent from these previous approaches: the branch-and-bound
process is used to enforce integrality constraints (as expected). In each branch-and-bound node, we have to
solve a nonlinear program (NLP) based on the original problem, the current variable bounds arising from the
BnB process, and relaxed integrality constraints. The NLP itself is solved using a particularly well-suited
variant of the Frank-Wolfe algorithm, the so-called Blended Pairwise Conditional Gradient (BPCG) algorithm
(Tsuji et al., 2021) that requires a linear minimization oracle (here given by the MIP solver) which is called
repeatedly in the solution process and access to gradients from the objective by means of a ï¬rst-order oracle;
see also Braun et al. (2022) for a general overview of conditional gradients and Frank-Wolfe methods. No other
access to the feasible region (e.g., complete description) or the objective function (e.g., Hessian information)
is required and in particular we do not require epipgraph formulations and we retain the original polyhedral
structure (except for bound updates); see also Figure 1 for a schematic overview. In terms of key diï¬€erences
this results in:

1. Instead of solving weaker but expensive convex relaxations at each node of the branch-and-bound tree,
we optimize over the convex hull of integer-feasible solutions. This lets us compute much stronger
relaxations and automatically derive several feasible solutions at each node,

2. We replace exact convex solvers with a Frank-Wolfe based error-adaptive solution process, i.e. a solution
process for which the amount of computations performed is gradually increasing with the speciï¬ed
precision,

3. We leverage the active set representation of the solution at each node to warm-start the children iterates

and reduce the number of calls to the Mixed-Integer Programming (MIP) solver.

2

4. We develop new laziï¬cation techniques and strong branching strategies for our framework.

We will now discuss the above in more detail. The main motivation for outer-approximation frameworks
is the ability to leverage MIP solvers as the core of their procedure through single-tree approaches i.e. the
MIP solver is called only once with separation callbacks. This class of approaches however carries several
issues: only near-feasible and near-optimal solutions are obtained in the limit towards the end of the solution
process and rely on MIP heuristics for primal feasible solutions. Furthermore, the separation constraints for
the nonlinear feasible set, often linear cutting planes, are in general dense in the variables involved in the
nonlinear structures. These high-density constraints slow down the MIP solving process and yield numerically
ill-conditioned LP relaxations. We face none of these issues due to not relying on outer approximations at
all but rather relying on nonlinear relaxations. While this may seem ineï¬ƒcient at ï¬rst and naive realizations
would require a very large number of MIP subproblems to be solved, our framework allows utilizing recent
advances in Frank-Wolfe methods and MIP methodology to reduce the number MIP subproblems dramatically.
Within the context of FW, we utilize (a) blending (Braun et al., 2019) and laziï¬cation (Braun et al.,
2017) to aggressively reuse previously discovered vertices and in fact we extend the laziï¬cation approach
from a single subproblem to the whole tree for huge additional gains in performance, (b) adaptive error
guarantees and dual bounds to allow for inexact and incomplete resolution of nodes in the tree, interrupting
the solving process as soon as the dual bound is satisfying for the current node, (c) warm-starting BPCG,
using the vertices and weights of the parent nodeâ€™s active set, creating high-quality warm-start solutions and
signiï¬cantly reducing the number of required FW iterations. These are combined with modern MIP solution
techniques (see Koch et al. (2022) for an overview) that we can fully leverage as the subproblems solved by
BPCG are standard MIPs: (a) primal heuristics provide feasible solutions (also valid for Problem (1) and not
just the subproblem) along the way that can be reused across all other MIP calls via active set and shadow
set representations (b) cutting planes over standard MIPs can be fully leveraged for the resolution of the
MIP subproblems occuring in BPCG. These techniques are combined in one overarching framework and we
develop techniques such as strong branching and hybrid branching for our setup here, further improving the
solution process.

Within our BnB tree, the MIP solver serves multiple functions:

(a) the Linear Minimization Oracle
(LMO) calls a MIP solver, leveraging it to its full extent with cutting planes, conï¬‚ict analysis, restarts, (b) the
MIP model can integrate combinatorial constraints that are not directly representable with linear inequalities
(SOS1, indicator constraints), (c) some structures like cutting planes, conï¬‚icts, variable bounds computed for
a MIP call can be reused by following subproblems within a node and its children, (d) primal heuristics called
by the MIP solver collect multiple solutions that can all be evaluated for our original problem.

Our framework is in particular the ï¬rst one, to the best of our knowledge, that can exploit the ï¬‚exibility
and advances of modern MIP solvers outside of a polyhedral outer approximation scheme, staying feasible
throughout the process. Ideas of warm-starting Frank-Wolfe variants based on active sets have been leveraged
in the past. In Moondra et al. (2021), an away-step Frank-Wolfe variant leverages active sets from previous
iterations in an online optimization setting. In Joulin et al. (2014), Frank-Wolfe or variants thereof are called
over the integer hull of combinatorial problems, calling a MIP solver as LMO and additional roundings to
compute feasible solutions of good quality. The hardness of optimization over the integer hull motivated
laziï¬ed conditional gradient approaches in Braun et al. (2017), opening a stream of literature. We however
generalize laziï¬cation techniques to consider the whole tree. Branch-and-bound with Away-step Frank-Wolfe
(AFW) subproblems has been explored in Buchheim et al. (2018) for a portfolio problem. Each subproblem
is partly solved to compute lower bounds from the Frank-Wolfe dual gap and each branching performs ï¬xing
to all possible values in the case of integer variables. We leverage BPCG (Tsuji et al., 2021) which enables us
to aggressively reuse information and whose convergence speed is typically higher than that of AFW, while
maintaining a higher sparsity which greatly beneï¬ts our branching as the fractionality of solutions is lower in
most cases.

We complement the above with extensive computational results to demonstrate the eï¬€ectiveness of our

approach. The code will be made available as a Julia package Boscia.jl under the MIT license.

3

(a) Our approach

(b) Branch-and-bound
with NLP nodes

(c) LP-based branch-and-bound and outer ap-
proximation

Figure 1: Three main algorithmic frameworks for MINLPs. Diamond blocks represent the nodal relaxations
in the given framework. Figure 1b corresponds to a classic BnB framework on top of NLP
relaxations, Figure 1c represents the mechanism behind LP-based MINLP frameworks and outer
approximations, Figure 1a is our proposed approach with the linearized models solved as MIPs
within the Frank-Wolfe algorithm on top of which we branch.

2. Nonlinear branch-and-bound over integer hulls with linear oracles
In this section, we present the main paradigm for our solver framework. Our approach is primarily based on
branch-and-bound, the feasible region handled at each node is identical except for updated ï¬xings and bounds
on the variables that must take integer values. At each node, we solve a convex relaxation with updated
local bounds for the integer variables using a FW-based algorithm. This family of algorithms only requires
a gradient oracle for the objective function ğ‘“ , i.e. given a point ğ‘¥ return âˆ‡ ğ‘“ (ğ‘¥), and not a full analytical
expression nor Hessian information. It accesses the feasible region ğ‘ƒ though a Linear Minimization Oracle
(LMO), which, given a direction ğ‘‘, solves ğ‘£ â† argminğ‘¥ âˆˆğ‘ƒ (cid:104)ğ‘‘, ğ‘¥(cid:105), where ğ‘£ is an extreme point or vertex of
the feasible set. This means in particular that the feasible region does not have to be given in closed form,
e.g., with constraint matrix and right-hand side, as long as the LMO is well-deï¬ned and tractable. Instead of
optimizing over the continuous relaxation of Problem (1) with updated bounds at each node, we optimize over
the convex hull of integer feasible solutions at each node. For that purpose, the LMO called within the FW
procedure is the MIP solver with bounds updated for the node and objective given by the gradient information
at the current solution. This design choice is illustrated in Figure 2. Optimizing over the convex hull at each
node means that all vertices computed across FW iterations are feasible for the original problem, the feasible
region itself is tightened and the tree height is greatly reduced for problems that have a loose continuous
relaxation.

Blended Pairwise Conditional Gradient (BPCG). The classic FW algorithm calls the LMO once per
iteration. Even though this is acceptable in contexts where the LMO is inexpensive compared to gradient
evaluations or in large dimensions in which storage of vertices is an issue, the plain FW algorithm is not the best
ï¬t when optimizing over polytopes, not only because the descent direction can be signiï¬cantly misaligned with
the negative of the gradient, but also and especially because the LMO is particularly expensive (consisting
of solving a MIP) in our context. We need methods that exploit already-computed extreme points of the
feasible set before making new calls to the LMO, which requires a representation of the iterate as an active
:= {v1, . . . vğ‘˜ } with associated weights ğœ† â‰¥ 0, (cid:205) ğœ† = 1 such that the
set, an ordered set of vertices Sğ‘¡
iterate is a convex combination ğ‘¥ğ‘¡ := (cid:205)ğ‘˜ ğœ†ğ‘˜ vğ‘˜ . Typical algorithms leveraging this representation are the

4

BnBNLPnodeboundgradientdirectionMIPBnBNLPnodeboundBnBLPNLPnodeboundï¬xingssolution(a) Baseline problem

(b) Branching over continuous relaxation

(c) Optimizing over the convex hull

(d) Branching over the convex hull

Figure 2: Branching over the convex hull. Figure 2a shows the baseline problem with the level curves of the
objective, polyhedron, the optimum over the relaxation Ëœğ‘¥ and example of potentially active vertices
for a near-optimal solution (the top vertex is dropped in an optimal solution). Branching over the
continuous relaxation is shown in Figure 2b. Our approach is shown in Figure 2c and Figure 2d,
optimizing over the convex hull with the help of the MIP solver. Branching only once results in an
optimal solution in the left part in Figure 2d, with the right part being pruned once Ëœğ‘¥ğ‘™ is found.

Away Frank-Wolfe (AFW) (GuÃ©lat & Marcotte, 1986), Pairwise Frank-Wolfe (PFW) (Lacoste-Julien & Jaggi,
2015) and Blended Conditional Gradient (BCG) (Braun et al., 2019). The main algorithm we focus on is
the recently-proposed Blended Pairwise Conditional Gradient (BPCG) algorithm (Tsuji et al., 2021) which
blends Frank-Wolfe steps in which a vertex is added to the active set with weight transfer steps which do not
require any LMO call and only perform inner product between vertices and the gradient. Experimentally,
BPCG produces iterates with a smaller active set across all iterations than other FW algorithms. We use more
speciï¬cally a slight modiï¬cation of the laziï¬ed version of BPCG from (Tsuji et al., 2021) as implemented in
FrankWolfe.jl. We present the pseudo-code for the variant that we are using in the Technical Appendix.

Active and shadow set branching. We accelerate the solving process of individual nodes through warm
starts. At the end of each BPCG call, the optimal solution is given as a convex combination of vertices in an
active set. All these vertices are obtained by calls to the MIP LMO and are thus extreme points of the convex
hull of the integer feasible set. When branching at a given node, the active set vertices can be partitioned to
form active sets for the left and right children. Furthermore, as the relaxed solution is fractional in the variable
branched on, it must contain at least a vertex for each of the children. The weights are also transferred to
each child for the corresponding vertex and renormalized. This ensures a high-quality starting point at each
child node in the tree. When weights are adjusted via pairwise steps of BPCG these steps do not require an
LMO call and are thus computationally rather cheap. The LMO is only called when performing FW steps
that add a new vertex to the active set. BPCG also drops vertices from the active set when the pairwise step
is performed completely (i.e. ğ›¾ğ‘¡ = 1). In a single run of the BPCG algorithm, a dropped vertex is usually
not part of the optimal active set. However, we perform many runs for many subproblems arising across
diï¬€erent nodes. As such a dropped vertex might very well be relevant for another subproblem further down
the tree when the bounds have been updated, adding computational burden by redundant calls to the LMO.
We therefore maintain a shadow set, a set of discarded vertices collected within each node when they are

5

ËœxËœxËœxrËœxlFigure 3: Comparison of branching strategies on an integer sparse regression example with dimension 40.
Hybrid branching performs partial strong branching up to the switching point corresponding to
a certain depth in the tree. Partial strong branching uses a FW gap of 10âˆ’3 and at most 10 FW
iterations. Hybrid branching operates partial strong branching up to depth 5, the â€œswitchingâ€ limit
corresponds to the last call to strong branching by the hybrid strategy.

removed from the active set. The shadow set can be partitioned in a fashion similar to the active set. At all
non-root nodes, an extra-laziï¬cation layer is added to BPCG: if no suitable pair of vertices can be found in the
pairwise step, the algorithm searches the shadow set for a vertex with a suitable alignment with the gradient,
using identical criteria as in the lazy BPCG algorithm. Only if this additional step fails to ï¬nd a good pair
does the algorithm perform an LMO call computing a new vertex. By propagating both the active and shadow
set, we signiï¬cantly reduce the number of calls to the LMO, ensuring we never recompute a given vertex
twice within a run for our algorithm.

3. Branch-and-bound techniques for error-adaptive convex relaxations
In this section, we present the techniques derived from branch-and-bound techniques that can be used in our
framework. In particular, we highlight the advantages from the error-adaptivity of BPCG, i.e. the property
of the computational cost decreasing gradually when the error tolerance increases, and how the various
components of modern MIP solvers can be used to enhance the higher-level branch-and-bound process.

6

0100020003000400050006000Timeinms162016401660168017001720LowerboundHybridMostinfeasibleStrongSwitch0102030405060Numberofnodes162016401660168017001720LowerboundHybridMostinfeasibleStrongSwitchMIP solvers are implementations of complex algorithms with several key components including presolving,
heuristics, bound tightening, and conï¬‚ict analysis. This is usually also a main motivation for single-tree
MINLP solvers handling nonlinearities as a special case of separation within a MIP solving process. We can
however also leverage this additional information about the feasible set within our framework by transferring
it across nodes, and potentially exploiting a reoptimization emphasis within a given node that solvers like
SCIP support (Gamrath et al., 2015). This avoids the recomputation of already ï¬xed terms across successive
LMO calls.

FW gap-based termination. Frank-Wolfe methods produce at each iteration a primal feasible iterate and a
dual gap upper-bounding the optimality gap. As a ï¬rst-order class of methods, they typically feature many
inexpensive iterations (unlike e.g. interior point methods which require few expensive iterations), with a dual
bound gradually increasing with iterations. We leverage this fact to adapt the solving process at each node.
Whenever the dual bound reaches the objective value of the best incumbent (or becomes close to the gap
tolerance), the node can be terminated early. A signiï¬cant part of the solving process can be avoided in this
manner which is not necessarily the case when nodes are processed with other non-convex or convex solvers.
Pushing this further, the solution process of a node can be stopped at any point to produce a dual bound, even
if the solution is not an exact but approximate optimum. The only strict requirement is that the dual bound
after solving the current node becomes high enough to yield progress in the overall tree dual bound.

Tree state-dependent termination and evolving error. We implement additional termination criteria for
node processing which do not guarantee the node should be pruned but that save on the total number of
iterations. One of them is the number of open nodes with a lower bound (obtained from the parent) that is
lower than the dual bound of the node being processed during the BPCG run. This set of lower nodes will be
processed before the children of the current node are. If the number of these more promising nodes is high
enough, there is a higher probability for the children of the current node not to be processed at all, for instance
if a better incumbent is found elsewhere. We also add an adaptive Frank-Wolfe gap criterion, increasing the
precision with the depth in the BnB tree:

ğœ€ğ‘› = ğœ€0 ğœŒğ‘›
(2)
with ğœŒ âˆˆ ]0, 1] where ğ‘› is the depth of the current node, starting from the root at depth 0. With some convex
solvers, using a reduced precision could slow down the search, as approximate solutions might exhibit a much
higher fractionality when a weaker stopping criterion is used, requiring more branching. In the case of BPCG
however, the iterates are a convex combination of few integer extreme points. Furthermore, if the optimal
solution at a node is an extreme point itself, the process converges fast by dropping the other extreme points
from the active set once the optimal one has been added. As such solutions obtained from low precision runs
do not necessarily exhibit a higher fractionality which in turn might require more branching.

Strong branching. When tackling large discrete problems, the choice of variables to branch on can yield
drastic diï¬€erences in how fast the lower and upper bounds evolve and the overall size of the branch-and-bound
tree. A powerful technique to estimate the lower bound increase when branching on a given variable is the
so-called strong branching which solves the children subproblems of a given node for all candidate variables,
selecting the variable to branch on that improves the lowest lower bound across children. Other techniques
like pseudo-cost branching or recent machine learning approaches try to construct surrogate models to avoid
solving the multiple convex subproblems induced by strong branching (Nair et al., 2020).

We propose a new family of branching techniques that leverage the properties of FW algorithms to obtain
a partial estimate of the lower bound improvement while greatly reducing the cost. In our context, strong
branching with nonlinear subproblems over the integer hull would be too costly to perform variable selection.
However, we can (a) relax the strong branching over the integer hull to the continuous relaxation (solving LPs
for the LMO instead of MIPs), (b) run few iterations of the subproblem (and/or setting a very high FW gap
tolerance). In the limit case, on the one extreme end this corresponds to performing a single FW iteration
for the strong branching estimation and the optimal value of a single linear problem is used to select the
branching variable. On the other extreme end the complete high-accuracy solve would correspond to using

7

the continuous relaxation of the node. By carefully limiting the precision we can interpolate between these
two regimes.

4. Computational experiments
We leverage the FrankWolfe.jl framework (BesanÃ§on et al., 2022) as a subsolver. All features speciï¬c to
our branch-and-bound framework are implemented in the Julia package Boscia.jl which will be available
in open-source under MIT license for the community to test, use, and build upon. The BnB core structures
are implemented in Bonobo.jl, the underlying MIP solver is SCIP (Bestuzheva et al., 2021). All experiments
were carried out on an 8-core compute node equipped with an Intel Xeon E3-1245v5 3.50GHz CPU and
32GB RAM. We use Julia 1.7.0. The package versions used are FrankWolfe.jl v.0.2.11, Bonobo.jl v0.1.2,
SCIP.jl v0.11.3 with SCIP v8.0.0.
4.0.1 Problem classes.
We considered the following problem classes.

Sparse regression problems. Sparsity is a desirable property for prediction models for reasons of ro-
bustness, explainability, computational eï¬ƒciency or other underlying motivation. Our framework allows for
solving all cardinality-constrained regression models including linear regression, sparse Poisson regression,
and logistic regression, as long as the loss function is convex and loss gradients are available. We also
experiment with regression models where the predictor coeï¬ƒcients themselves are constrained to take integer
values.

Grouped sparse regression.

In addition to a simple cardinality constraint, our framework can repre-
sent richer generic constraints in regression models, for example adding group sparsity, where variables are
partitioned into subsets and only a few of those subsets can be in the support of the predictor. This is a
cardinality-constrained version of the grouped lasso model (Lounici et al., 2011; Friedman et al., 2010). Sim-
ilarly to simple sparse regression, our framework can accommodate all these formulations for all diï¬€erentiable
convex losses including Poisson, logistic, and linear regression with or without â„“2 regularization.

Sparse coding with permutation matrices. We consider the problem of ï¬nding a convex combination of

ğ¾ permutation matrices that approximate am ğ‘› Ã— ğ‘› doubly-stochastic matrix Ë†ğ‘‹:

min
ğ‘‹ âˆˆğ‘ƒğ‘›, ğœƒ âˆˆÎ”ğ‘˜

(cid:107)

âˆ‘ï¸

ğ‘– âˆˆ [ğ‘˜ ]

ğœƒğ‘– ğ‘‹ğ‘– âˆ’ Ë†ğ‘‹ (cid:107)2,

where Î”ğ‘˜ is the ğ‘˜-dimensional probability simplex, [ğ‘˜] represents the set of integers from 1 to ğ‘˜, and ğ‘ƒğ‘› is
the set of ğ‘› Ã— ğ‘› permutation matrices. which we can convexify with auxiliary variables using the bounds on
ğœƒ and ğ‘‹, and the fact that one of the variables of the bilinear terms is always binary:

min
ğ‘‹ âˆˆğ‘ƒğ‘›, ğœƒ âˆˆÎ”ğ‘˜ ,ğ‘Œ âˆˆâ„ğ‘›Ã—ğ‘›

(cid:107)

ğ‘Œğ‘– âˆ’ Ë†ğ‘‹ (cid:107)2

âˆ‘ï¸

ğ‘– âˆˆ [ğ‘˜ ]

s.t. 0 â‰¤ ğ‘Œğ‘– â‰¤ ğ‘‹ğ‘–, 0 â‰¤ ğœƒğ‘– âˆ’ ğ‘Œğ‘– â‰¤ 1 âˆ’ ğ‘‹ğ‘–.

This problem or variants have been considered in Valls et al. (2021); DufossÃ© & UÃ§ar (2016).
It can be
viewed as a cardinality-constrained version of sparse coding over the Birkhoï¬€ polytope, the convex hull of
permutation matrices.

Portfolio optimization. We revisit the example of Buchheim et al. (2018), selecting a portfolio with budget
ğ‘, integrality requirements on shares for some assets, and with a generic convex diï¬€erentiable risk penalty
term â„(Â·):

â„(ğ‘¥) ğ‘¥ğ‘‡ ğ‘€ğ‘¥ âˆ’ (cid:104)ğ‘Ÿ, x(cid:105)

min
ğ‘¥

s.t.

(cid:104)ğ‘, ğ‘¥(cid:105) â‰¤ ğ‘, ğ‘¥ ğ‘— âˆˆ â„¤ âˆ€ ğ‘— âˆˆ ğ½.

MIPLIB instance. Our framework reads standard instance formats automatically thanks to the MathOptInterface

backend (Legat et al., in press) in Julia. As an arbitrary example, we use the (bounded) instance 22433.mps

8

Figure 4: Convergence plot for a linear sparse regression example with a starting root node FW tolerance of

10âˆ’3.

from the MIPLIB 2017(Gleixner et al., 2021) and as a simple experiment, we compute vertices for random
directions and then minimize the sum of squared distances to these vertices, therefore promoting interior
integral solutions.
4.0.2 Results.
In Figure 5, we test the eï¬€ect of the laziï¬cation and warm-starting techniques added to BPCG, the root
node requires the most LMO calls of all layers and populates the active and shadow sets. Depending on the
instances, the cardinality of the active set and shadow sets evolve diï¬€erently, depending on whether the last
layers of nodes densify or sparsify the solution.

An example of primal-dual convergence of the branch-and-bound tree for a sparse regression example is
shown in Figure 4. As typical for our framework, the optimal primal is found early in the search process thanks
to the MIP heuristics and vertices, the process could be stopped much earlier with a non-zero dual gap but a
high-quality (most of the time optimal) primal solution. Figure 6 shows the same primal-dual evolution with
the LMO calls per node. Figure 8 shows the primal-dual convergence for an integer regression problem with
cardinality constraints. On all instances, the optimal primal solution is found early in the process, typically at
the root node thanks to the optimization performed over the integer hull, the following nodes are used only to
prove (near-)optimality.

Figure 7 shows the eï¬€ect of tuning the FW gap tolerance at each node with the layer-dependent error

described in Equation (2).

A comparison of branching strategies on a sparse regression problem is presented in Figure 3. On some
problem types, strong and hybrid strong branching provide a signiï¬cant speed-up, especially early on in the
branching process since superior variable branching strategies impact the rest of the process. We also observed
that the FW gap used for strong branching does not have a signiï¬cant impact in terms of progress per node or
time. This is likely due to the highly laziï¬ed nature of BPCG which performs few LMO calls mostly at the
beginning of the solving process. Higher precision for the continuous relaxation used for strong branching
typically does not require more LMO calls but also provides little gain in terms of prediction for the variable
to branch on. This limited eï¬€ect can be explained by the fact that the individual solutions or bounds are not
used directly but only rank the diï¬€erent variables, high precision is therefore useless as long as the ranking is
not changing when accuracy increases. On the 22433.mps MIPLIB instance, our algorithm requires between
one and three nodes to solve the problem depending on the quadratic objective, despite about 231 integer
variables. This is due to the number of fractional variables remaining small when optimizing over the integer
hull.

9

050100150200Numberofnodes0.750.800.850.900.95Objectivevalue020040060080010001200LMOcallsIncumbentLowerboundTotalLMOcallsFigure 5: Average cardinality of the active & shadow sets and number of LMO calls over nodes at each depth
level of the tree for the permutation matrices example with ğ‘›, ğ‘˜ = 3. The number of vertices in the
active and shadow sets increases moderately while the number of LMO calls drops quickly after
the root node.

Figure 6: Primal-dual progress on the permutation matrices example with ğ‘› = 3, ğ‘˜ = 2. Unlike many
examples, improving primal solutions are found late in the search process. The non-cumulative
number of LMO calls per node is displayed to highlight the decrease throughout the tree.

10

161116Nodedepth345678Avgsetsize10152025LMOcallsActivesetDiscardedsetLMOcalls020406080100120Numberofnodes0.00.10.20.30.40.5Objectivevalue510152025LMOcallsIncumbentLowerboundLMOcallsFigure 7: Eï¬€ect of the FW gap tolerance parameters on the total solving time on integer sparse regression.
Solving the subproblems with a too high accuracy is much costlier for the runtime. Furthermore,
lower decay rates, that imply nodes lower in the tree are solved with an accuracy that does not
increase a lot seem to accelerate the solving process.

Figure 8: Convergence plot for an integer regression example in dimension 40 with a cardinality constraint
on the support of the predictor. On most instances, the primal optimum is found at the root node,
the remaining nodes are spent proving near-optimality with the speciï¬ed relative gap tolerance of
1%.

11

0.650.70.750.80.850.91.0Dualgapdecayfactor123TimeinsFWgaptolerance0.0010.0050.0001246810121416Numberofnodes750760770780Objectivevalue50100150200250LMOcallsIncumbentLowerboundTotalLMOcalls5. Conclusion
In this paper, we proposed a novel algorithm for mixed-integer convex optimization relying only on gradient
and function evaluations of the objective. By embedding a FW-based subsolver within a BnB framework,
our method does not rely on outer approximation & separation nor on projection subproblems. Since FW
algorithms rely on LMO calls to handle the constraint set, we can signiï¬cantly strengthen convex relaxations
by optimizing at each node over the convex hull of mixed-integer feasible solutions, therefore leveraging the
capabilities of modern MIP solvers. Laziï¬cation techniques within and across nodes also avoid expensive
MIP solves by exploiting all vertices that have been discovered and further MIP information. Future work will
include exploiting MIP reoptimization techniques and transferring additional MIP information across LMO
calls within and across nodes, and designing adaptive rules for early termination criteria.

Acknowledgments
Research reported in this paper was partially supported through the Research Campus Modal funded by
the German Federal Ministry of Education and Research (fund numbers 05M14ZAM,05M20ZBM) and the
Deutsche Forschungsgemeinschaft (DFG) through the DFG Cluster of Excellence MATH+.

References
Belotti, P. Couenne: a userâ€™s manual. 2009.

Bertsimas, D., King, A., and Mazumder, R. Best subset selection via a modern optimization lens. The annals

of statistics, 44(2):813â€“852, 2016.

BesanÃ§on, M., Carderera, A., and Pokutta, S. Frankwolfe. jl: A high-performance and ï¬‚exible toolbox for

frankâ€“wolfe algorithms and conditional gradients. INFORMS Journal on Computing, 2022.

Bestuzheva, K., BesanÃ§on, M., Chen, W.-K., Chmiela, A., Donkiewicz, T., van Doornmalen, J., Eiï¬‚er, L.,
Gaul, O., Gamrath, G., Gleixner, A., et al. The scip optimization suite 8.0. arXiv preprint arXiv:2112.08872,
2021.

Bonami, P., Biegler, L. T., Conn, A. R., CornuÃ©jols, G., Grossmann, I. E., Laird, C. D., Lee, J., Lodi, A.,
Margot, F., Sawaya, N., and WÃ¤chter, A. An algorithmic framework for convex mixed integer nonlinear pro-
grams. Discrete Optimization, 5(2):186â€“204, 2008. ISSN 1572-5286. doi: https://doi.org/10.1016/j.disopt.
In
2006.10.011. URL https://www.sciencedirect.com/science/article/pii/S1572528607000448.
Memory of George B. Dantzig.

Braun, G., Pokutta, S., and Zink, D. Lazifying conditional gradient algorithms. In International conference

on machine learning, pp. 566â€“575. PMLR, 2017.

Braun, G., Pokutta, S., Tu, D., and Wright, S. Blended conditonal gradients. In International Conference on

Machine Learning, pp. 735â€“743. PMLR, 2019.

Braun, G., Carderera, A., Combettes, C. W., Hassani, H., Karbasi, A., Mokthari, A., and Pokutta, S.

Conditional gradient methods. preprint, 8 2022.

Buchheim, C., De Santis, M., Rinaldi, F., and Trieu, L. A frankâ€“wolfe based branch-and-bound algorithm for

mean-risk optimization. Journal of Global Optimization, 70(3):625â€“644, 2018.

Byrd, R. H., Nocedal, J., and Waltz, R. A. Knitro: An integrated package for nonlinear optimization. In

Large-scale nonlinear optimization, pp. 35â€“59. Springer, 2006.

Chen, Y. and Goulart, P. An early termination technique for admm in mixed integer conic programming. In

2022 European Control Conference (ECC), pp. 60â€“65. IEEE, 2022.

12

DufossÃ©, F. and UÃ§ar, B. Notes on birkhoï¬€â€“von neumann decomposition of doubly stochastic matrices. Linear

Algebra and its Applications, 497:108â€“115, 2016.

Friedman, J., Hastie, T., and Tibshirani, R. A note on the group lasso and a sparse group lasso. arXiv preprint

arXiv:1001.0736, 2010.

Gamrath, G., Hiller, B., and Witzig, J. Reoptimization techniques for mip solvers. In International Symposium

on Experimental Algorithms, pp. 181â€“192. Springer, 2015.

Gleixner, A., Hendel, G., Gamrath, G., Achterberg, T., Bastubbe, M., Berthold, T., Christophel, P., Jarck, K.,
Koch, T., Linderoth, J., et al. Miplib 2017: data-driven compilation of the 6th mixed-integer programming
library. Mathematical Programming Computation, pp. 1â€“48, 2021.

GÃ³mez, A. and Prokopyev, O. A. A mixed-integer fractional optimization approach to best subset selection.

INFORMS Journal on Computing, 33(2):551â€“565, 2021.

GuÃ©lat, J. and Marcotte, P. Some comments on wolfeâ€™s â€˜away stepâ€™. Mathematical Programming, 35(1):

110â€“119, 1986.

Hazimeh, H. and Mazumder, R. Fast best subset selection: Coordinate descent and local combinatorial

optimization algorithms. Operations Research, 68(5):1517â€“1537, 2020.

HunkenschrÃ¶der, C., Pokutta, S., and Weismantel, R. Optimizing a low-dimensional convex function over a

high-dimensional cube. arXiv preprint arXiv:2204.05266, 2022.

Joulin, A., Tang, K., and Fei-Fei, L. Eï¬ƒcient image and video co-localization with Frank-Wolfe algorithm.

In European Conference on Computer Vision, pp. 253â€“268. Springer, 2014.

JuMP-dev. Pavito, a gradient-based outer approximation solver for convex mixed-integer nonlinear program-

ming, 2018. https://github.com/jump-dev/Pavito.jl.

Koch, T., Berthold, T., Pedersen, J., and Vanaret, C. Progress in mathematical programming solvers from

2001 to 2020. EURO Journal on Computational Optimization, pp. 100031, 2022.

Kronqvist, J., Bernal, D. E., Lundell, A., and Grossmann, I. E. A review and comparison of solvers for convex

minlp. Optimization and Engineering, 20(2):397â€“455, 2019.

Lacoste-Julien, S. and Jaggi, M. On the global linear convergence of frank-wolfe optimization variants.

Advances in neural information processing systems, 28, 2015.

Legat, B., Dowson, O., Garcia, J., and Lubin, M. MathOptInterface: a data structure for mathematical

optimization problems. INFORMS Journal on Computing, in press.

Lounici, K., Pontil, M., Van De Geer, S., and Tsybakov, A. B. Oracle inequalities and optimal inference under

group sparsity. The annals of statistics, 39(4):2164â€“2204, 2011.

Lundell, A., Kronqvist, J., and Westerlund, T. The supporting hyperplane optimization toolkit for convex

minlp. Journal of Global Optimization, pp. 1â€“41, 2022.

Moondra, J., Mortagy, H., and Gupta, S. Reusing combinatorial structure: Faster iterative projections over
submodular base polytopes. Advances in Neural Information Processing Systems, 34:25386â€“25399, 2021.

Moreira Costa, C., Kreber, D., and Schmidt, M. An alternating method for cardinality-constrained optimiza-
tion: A computational study for the best subset selection and sparse portfolio problems. INFORMS Journal
on Computing, 2022.

13

Nair, V., Bartunov, S., Gimeno, F., von Glehn, I., Lichocki, P., Lobov, I., Oâ€™Donoghue, B., Sonnerat, N.,
Tjandraatmadja, C., Wang, P., et al. Solving mixed integer programs using neural networks. arXiv preprint
arXiv:2012.13349, 2020.

Quesada, I. and Grossmann, I. E. An LP/NLP based branch and bound algorithm for convex MINLP

optimization problems. Computers & chemical engineering, 16(10-11):937â€“947, 1992.

Tsuji, K., Tanaka, K., and Pokutta, S. Sparser kernel herding with pairwise conditional gradients without

swap steps. arXiv preprint arXiv:2110.12650, 2021.

Valls, V., Iosiï¬dis, G., and Tassiulas, L. Birkhoï¬€â€™s Decomposition Revisited: Sparse Scheduling for High-

Speed Circuit Switches. IEEE/ACM Transactions on Networking, 29(6):2399â€“2412, 2021.

Appendix A. Blended Pairwise Conditional Gradient

We present below the modiï¬ed Lazy Blended Pairwise Conditional Gradient (L-BPCG) from Tsuji et al.
(2021) to solve relaxations at each node. The procedure populates the shadow set with dropped vertices
and starts from the warm-started active set. The convergence follows from that of BPCG, we use the same
progress measure on whether a shadow vertex oï¬€ers suï¬ƒcient decrease as FW direction (vertex the iterates
moves towards) as we do for a classic pairwise step. The additional shadow vertex selection can be viewed as
a special case of a pairwise step where the initial weight in the active set is zero.

Appendix B. Branching strategies

We present additional results comparing the strong and hybrid branching strategies to most infeasible (most
fractional) branching. Strong branching is not beneï¬cial on all problems. In particular, strong and hybrid
branching provides a strong advantage for the dual bound progress when some branching decisions signiï¬cantly
tighten the problem. In some cases such as the mixed-binary quadratic problem shown in Figure 9, the most
infeasible branching strategy is less eï¬ƒcient both against the number of nodes and time despite being a much
cheaper variable selection heuristic.

Figure 11 shows the evolution of the dual bound when using diï¬€erent maximum depth levels for hybrid
strong branching. The most expensive strategies using strong branching further down the tree are are favourable
even against time. This opens a range of algorithmic questions on adaptive transitions from strong branching
to less expensive heuristics.

Figure 12 shows the evolution of the number of LMO calls and cardinality of the average number of active
and shadow set for each node depth. We systematically observe a decrease in the number of LMO calls across
layers thanks to warm starts and laziï¬cation. For some instances, the active set cardinality drops in the last
layer, i.e. ï¬nal solutions are sparser than the ones from previous layers. In some other instances, the leaf nodes
produce denser solutions once all integers are ï¬xed.

Figure 14 shows the primal dual convergence on the sparse poisson regression example in dimension 70.
Unlike most other examples we covered, the gap is closed by a sharp drop of the incumbent value and not by
an increase of the dual bound. This suggests a potential for improvement from designing and adding more
advanced objective-aware heuristics (e.g. a nonlinear feasibility pump) or adjusting the heuristic collection
from the MIP solver.

14

Algorithm A.1 Lazy Blended Pairwise Conditional Gradient with Shadow Set
Require: Starting active set A0 and weights ğœ†, shadow set S, function ğ‘“ , feasible set of current node ğ‘ƒ, ğœ€ğ‘¡ğ‘œğ‘™,

âŠ² away vertex
âŠ² local forward vertex

âŠ² descent step

âŠ² drop step

âŠ² forward vertex from dropped vertices

âŠ² Global LMO

accuracy ğ¾ â‰¥ 1

Ensure: ï¬nal iterate xğ‘‡ such that ğ‘“ (xğ‘‡ ) â‰¤ ğ‘“ âˆ— + ğœ€tol
1: x0 â† (cid:205)| A0 |
ğœ†ğ‘˜ vğ‘˜
ğ‘˜=1
2: Î¦0 â† maxvâˆˆğ‘ƒ (cid:104)âˆ‡ ğ‘“ (x0), v(cid:105) /2
3: ğ‘¡ = 0
4: while dual_gap > ğœ€tol do
5:
6:
7:
8:
9:

ğ‘ğ‘¡ â† argmaxvâˆˆAğ‘¡ (cid:104)âˆ‡ ğ‘“ (xğ‘¡ ), v(cid:105)
ğ‘ ğ‘¡ â† argminvâˆˆAğ‘¡ (cid:104)âˆ‡ ğ‘“ (xğ‘¡ ), v(cid:105)
if (cid:104)âˆ‡ ğ‘“ (xğ‘¡ ), ğ‘ğ‘¡ âˆ’ ğ‘ ğ‘¡ (cid:105) â‰¥ Î¦ğ‘¡ then

ğ‘‘ğ‘¡ â† ğ‘ğ‘¡ âˆ’ ğ‘ ğ‘¡
ğ›¾max â† weightOf(Ağ‘¡ , ğ‘ğ‘¡ )
ğ›¾ğ‘¡ â† argmin ğ›¾ âˆˆ [0, ğœ†max] ğ‘“ (ğ‘¥ âˆ’ ğ›¾ğ‘‘ğ‘¡ )
xğ‘¡+1 â† xğ‘¡ âˆ’ ğ›¾ğ‘¡ ğ‘‘ğ‘¡
Î¦ğ‘¡+1 â† Î¦ğ‘¡
if ğ›¾ğ‘¡ < ğ›¾max then
Ağ‘¡+1 â† Ağ‘¡

else

Ağ‘¡+1 â† Ağ‘¡ \{ğ‘ğ‘¡ }
Sğ‘¡+1 â† Sğ‘¡ âˆª {ğ‘ğ‘¡ }

end if

else

ğ‘ ğ‘¡ â† argminvâˆˆSğ‘¡ (cid:104)âˆ‡ ğ‘“ (xğ‘¡ ), v(cid:105)
if (cid:104)âˆ‡ ğ‘“ (xğ‘¡ ), ğ‘ğ‘¡ âˆ’ ğ‘ ğ‘¡ (cid:105) â‰¥ Î¦ğ‘¡ then

Ağ‘¡+1 â† Ağ‘¡ âˆª {ğ‘ ğ‘¡ }
Sğ‘¡+1 â† Sğ‘¡ \{ğ‘ ğ‘¡ }
descent or drop step

else

ğ‘¤ğ‘¡ â† argminvâˆˆğ‘ƒ (cid:104)âˆ‡ ğ‘“ (xğ‘¡ ), v(cid:105)
if (cid:104)âˆ‡ ğ‘“ (xğ‘¡ ), xğ‘¡ âˆ’ ğ‘¤ğ‘¡ (cid:105) â‰¥ Î¦ğ‘¡ /ğ¾ then

ğ‘‘ğ‘¡ = xğ‘¡ âˆ’ ğ‘¤ğ‘¡
ğ›¾ğ‘¡ â† argmin ğ›¾ âˆˆ [0, ğœ†max] ğ‘“ (ğ‘¥ âˆ’ ğ›¾ğ‘‘ğ‘¡ )
xğ‘¡+1 â† xğ‘¡ âˆ’ ğ›¾ğ‘¡ ğ‘‘ğ‘¡
Î¦ğ‘¡+1 â† Î¦ğ‘¡
Ağ‘¡+1 â† Ağ‘¡ âˆª {ğ‘¤ğ‘¡ }

else

xğ‘¡+1 â† xğ‘¡
Î¦ğ‘¡+1 â† Î¦ğ‘¡ /2
Ağ‘¡+1 â† Ağ‘¡

end if

10:
11:
12:
13:
14:

15:
16:
17:
18:
19:
20:

21:
22:
23:
24:
25:

26:
27:
28:
29:
30:
31:

32:
33:
34:
35:
36:

end if

37:
38:
39:
40:
41: end while

end if
ğ‘¡ â† ğ‘¡ + 1

15

Figure 9: Primal-dual bound convergence for a low-dimensional quadratic mixed-binary problem (12) with a

high inner dimension (500) taken from HunkenschrÃ¶der et al. (2022).

16

02000400060008000Timeinmsâˆ’5020âˆ’5000âˆ’4980âˆ’4960ObjectivevalueMostinfeasibleStrongHybridSwitch0102030405060Numberofnodesâˆ’5020âˆ’5000âˆ’4980âˆ’4960ObjectivevalueMostinfeasibleStrongHybridSwitchFigure 10: Dual bound convergence for grouped sparse regression against time and number of nodes. The
switching limit corresponds to the last node where the hybrid branching applies strong branching.

17

0200040006000800010000Timeinms210220230240250260LowerboundHybridMostinfeasibleStrongSwitch0510152025303540Numberofnodes210220230240250260LowerboundHybridMostinfeasibleStrongSwitchFigure 11: Depth adjustment for hybrid branching i.e. maximum depth up to which partial strong branching
is applied before switching to most fractional branching on an integer sparse regression example.
Num_int corresponds to a maximum depth equal to the number of integer variables. Since this
instance has general integer and not only binary variables, this is not the maximum depth of the
tree. The lowest numbers correspond to fewer strong branching calls. The maximum number of
FW iterations was 10 and the gap tolerance 10âˆ’3.

18

0100002000030000400005000060000Timeinms16201640166016801700LowerboundNumintNumint/2Numint/4Numint/100102030405060Numberofnodes16201640166016801700LowerboundNumintNumint/5Numint/10Numint/20Figure 12: Evolution of the LMO call, active and shadow set cardinalities for the permutation matrices

example.

19

161116Nodedepth234567Avgsetsize1015202530LMOcallsActivesetDiscardedsetLMOcalls161116Nodedepth3456789Avgsetsize10.012.515.017.520.022.525.0LMOcallsActivesetDiscardedsetLMOcallsFigure 13: Primal-dual convergence on the permutation matrices example. The number of LMO calls is

displayed for each node and decreases with the layers.

20

020406080100120Numberofnodes0.00.10.20.30.40.5Objectivevalue510152025LMOcallsIncumbentLowerboundLMOcalls020406080100120140Numberofnodes0.00.10.20.30.40.5Objectivevalue510152025LMOcallsIncumbentLowerboundLMOcallsFigure 14: Primal-dual convergence on the sparse Poisson regression example.

21

050000100000150000200000Timeinmsâˆ’20âˆ’100Dualbound05001000150020002500LmocallsLowerboundUpperboundTotallmocalls020406080100Numberofnodesâˆ’20âˆ’100Dualbound05001000150020002500LmocallsLowerboundUpperboundTotallmocalls