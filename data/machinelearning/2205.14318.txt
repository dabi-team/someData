2
2
0
2

y
a
M
8
2

]

G
L
.
s
c
[

1
v
8
1
3
4
1
.
5
0
2
2
:
v
i
X
r
a

Learning from Self-Sampled Correct and
Partially-Correct Programs

Ansong Ni∗
Yale University
ansong.ni@yale.edu

Jeevana Priya Inala
Microsoft Research
jinala@microsoft.com

Chenglong Wang
Microsoft Research
chenwang@microsoft.com

Oleksandr Polozov†
X, the moonshot factory
polozov@x.team

Christopher Meek‡
University of Washington
cameek@uw.edu

Dragomir Radev
Yale University
dragomir.radev@yale.edu

Jianfeng Gao
Microsoft Research
jfgao@microsoft.com

Abstract

Program synthesis aims to generate executable programs that are consistent with
the user speciﬁcation. While there are often multiple programs that satisfy the
same user speciﬁcation, existing neural program synthesis models are often only
learned from one reference program by maximizing its log-likelihood. This causes
the model to be overly conﬁdent in its predictions as it sees the single solution
repeatedly during training. This leads to poor generalization on unseen examples,
even when multiple attempts are allowed. To mitigate this issue, we propose to let
the model perform sampling during training and learn from both self-sampled fully-
correct programs, which yield the gold execution results, as well as partially-correct
programs, whose intermediate execution state matches another correct program. We
show that our use of self-sampled correct and partially-correct programs can beneﬁt
learning and help guide the sampling process, leading to more efﬁcient exploration
of the program space. Additionally, we explore various training objectives to
support learning from multiple programs per example and ﬁnd they greatly affects
the performance. Experiments on the MathQA and GSM8K datasets show that our
proposed method improves the PASS@k performance by 3.1% to 12.3% compared
to learning from a single reference program with MLE.

1

Introduction

Developing models that can automatically generate programs from user speciﬁcation (e.g., natural
language, input/output pairs, etc) has been a longstanding challenge [14, 21]. Recent progress
in neural program synthesis, especially using large language models pretrained on code [2, 6, 9],
has shown promise in generating general-purpose programming languages. These models have
been experimented on many different programming tasks, including math-word-problems [2, 10],
jupyter notebook cell generation [5], common programming tasks [2, 25] or even competition level
programming puzzles [17]. The performance of these models are often measured by the PASS@k

∗Majority of the work done during an internship at Microsoft Research.
†Work started while at Microsoft Research. Paper contribution limited to proof-reading.
‡Work initiated while at Microsoft.

Preprint. Under review.

 
 
 
 
 
 
metric [2, 6, 9, 16, 22], where k program candidates are generated and the task is consider solved if
any of those k candidates yields the expected execution results.

In practice, because of the expressiveness of the programming languages, there are often multiple
program solutions for each user speciﬁcation. However, existing datasets often include only one
reference program for each example, due to the cost of annotation [2, 10, 25]. As a result, existing
models are often trained with the MLE objective that aims to maximize the log-likelihood of the one
reference program as opposed to different correct programs in the program space. This results in
model overﬁtting as it becomes overly conﬁdent in its predictions since it only sees the same solution
over multiple epochs of training [2, 4, 10]. This leads to poor generalization on unseen speciﬁcations
and is reﬂected as low PASS@k performance, where even multiple attempts are allowed.

To mitigate the above issue, we propose learning from self-sampled programs. Concretely, during
the training time, the model samples programs from the program space, keep track of all programs
that are semantically correct with respect to the user speciﬁcation, and learn from all of these correct
programs as oppose to only from the reference program. To further improve the effectiveness of
learning from self-sampled programs, we allow the model to learn from partially-correct programs,
whose intermediate execution states are consistent with known correct programs. This new technique
allows the model to maximally utilize the self-sampled programs and more efﬁciently explore the
program space. We also study various common loss functions for learning from multiple programs
targets for a single speciﬁcation, including augmented-MLE, Maximize Marginal Likelihood (MML)
and β-smoothed MML [15].

We perform experiments on two math programming tasks, namely MathQA-Python [2] and Grade-
School-Math (GSM) [10], where the task is to generate python programs to solve math problems
described in natural language descriptions. Results show that learning from self-sampled programs
can improve the PASS@100 from 22.7% to 35.0% for GSM, and 25.1% to 28.2% for PASS@80 on a
ﬁltered version of MathQA-Python. Moreover, we ﬁnd that adding partially-correct programs can
outperform using only fully-correct programs by 2.7% on the GSM and it also shows improvement
with MathQA-Python. Such performance boosts from our proposed methods are also consistent for
different model sizes for the pretrained code language models. Ablation on different loss functions
show that MLE-Aug loss is the most effective in learning from multiple program targets and yields
the most improvements over MLE loss.

2 Overview

Problem formulation. We consider the task of program synthesis from speciﬁcations written in
natural language (NL). Given an NL description x ∈ X and a program executor E : Y → Z, the goal
is to generate a program y ∈ Y that executes to the expected results z∗ ∈ Z, i.e., E(y) = z∗.4

Standard approach and its limitation. Given a program synthesis dataset, typically only one gold
program y∗ is provided for each NL x, thus a parameterized model Pθ(y|x) is typically learned with
the Maximum Likelihood Estimation (MLE) objective from the NL-Program pair (x, y∗) as:

LMLE(x, y∗, Pθ) = − log Pθ(y∗|x)
The builtin assumption of using Eq. 1 for learning is that only the gold program y∗ is correct. However,
this assumption is clearly untrue for the program synthesis problem as typically multiple programs
can satisfy the same speciﬁcation. With only one gold program, Eq. 1 would encourage the model to
put all probability mass on y∗, which could easily lead to overﬁtting [2, 4, 10].

(1)

Overview of our approach. Because manually collecting multiple reference programs for each
speciﬁcation is a laborious process, in our work, we explore an alternate approach: where the model
self-samples additional correct (or partially-correct) programs and learns from them during training
time. Fig. 1 shows an example: for the speciﬁcation x, our model was able to self-sample a correct
program ˆy that is different from the gold program y∗ provided in the dataset. Looking at the program
execution states shown on the right, we can see that both these programs execute to produce the
sample desired output, i.e., ˆz = z∗, as noted with dashed red boxes in Fig. 1. Taking one step
further, our approach can also identify partially-correct programs from its self-sampled programs.

4For simplicity, we consider programs with no explicit inputs, i.e., inputs embedded as variable initialization.

2

Algorithm 1 Training Update

Input: Parameterized model Pθ(y|x)

Parallel training data (x, y∗)
Executor E : Y → Z
Initialize: Program buffer B = {y∗}

Gold exec. result z∗ = E(y∗)

ˆY ← SamplePrograms(x, Pθ, B)
for ˆy in ˆY do
ˆz ← E(ˆy)
if isCorrect(ˆz, z∗) then

if not isDuplicate(ˆy, B) then

1: repeat
2:
3:
4:
5:
6:
7:
8:
9: until convergence

θ

B ← B + ˆy
update
←−−− ∇θL(x, B, Pθ)

Figure 1: Examples of self-sampled correct and partially-
correct programs from MathQA (more in Appendix D). The
program statements and states marked in red are incorrect.

For example, on the bottom left, we show a sampled program ˆy(cid:48) that is incorrect only because of an
error in its last two lines. But we identify a subprogram ˆy(cid:48)
≤5 of it as partially-correct because the
execution state ˆs(cid:48)
5 of a known correct program
y∗, which is noted as the solid red boxes in Fig. 1. Based on these observations and intuitions, we
introduce our approach in the following sections.

5 for this subprogram matches the intermediate state s∗

3 Learning from Self-Sampled Programs

We now formally present our approach. There are three main steps: 1) sampling; 2) ﬁltering; and
3) learning as shown in Alg. 1. Here we mainly introduce the self-sampling framework using only
fully-correct programs and the extensions with partially-correct programs will be introduced in § 4.

3.1 Online Sampling and Filtering

i=1 ∼ Pθ(ˆy|x);

For each speciﬁcation x, we maintain a buffer B to save the different programs that are correct, i.e.,
execute to the expected results. Note that the buffers are persistent and cumulative across training
epochs. To add more programs in B, we perform online sampling and ﬁltering as follows.
Online sampling (line 2 in Alg. 1): For each example (x, y∗), the model samples a set candidate
programs, i.e., ˆY = {ˆyi}n
Filtering incorrect programs (line 5 in Alg. 1): As not all sampled programs in ˆY are correct (thus
not suitable for learning), we ﬁlter out all incorrect programs in ˆY , i.e., ˆY ∗ = {ˆy|ˆy ∈ ˆY ; E(ˆy) = z∗};
Filtering duplicate programs (line 6 in Alg. 1): Because the model can sample programs that are
correct but are "trivial variants" of other correct programs (e.g., the program differs from another
program only in whitespaces, comments or trivial statements like "x = x * 1.0"), we further ﬁlter
the buffer to remove them. This process is essential to control the buffer quality for better learning.
Concretely, we ﬁrst perform ﬁltering based on the linearized abstract syntax trees (ASTs) to eliminate
the differences in whitespaces, etc; then we set a constraint on maximum number of lines using y∗ as
the reference to prevent saving trivial variants.

3

n0 = 72n1 = 250n2 = 26t0 = n0 * 0.2778t1 = n1 / t0t2 = n2 -t1answer = t0 * t2n0 = 72n1 = 250n2 = 26t0 = n0 * 0.2778t1 = n2 * t0answer = t1 -n1A goods train runs at a speed of 72kmph and crosses a 250M long platform in 26 seconds. What is the length of the goods train?Gold ProgramCorrect ProgramNL Specs.{72}{72, 250}{72, 250, 26}{72, 250, 26, 20.0}{72, 250, 26, 20.0, 12.5}{72, 250, 26, 20.0, 12.5, 13.5}output = 270{72}{72, 250}{72, 250, 26}{72, 250, 26, 20.0}{72, 250, 26, 20.0, 520}output = 270n0 = 72n1 = 250n2 = 26t0 = n0 / 3.6t1 = n1 / t0t2 = n0 -t1answer = t0 * t1Partially-Correct Program{72}{72, 250}{72, 250, 26}{72, 250, 26, 20.0}{72, 250, 26, 20.0, 12.5}{72, 250, 26, 20.0, 12.5, 59.5}output = 1547CorrectProgram Execution TracingProgramsProgram Execution States𝑦∗𝑥#𝑦#𝑦′!𝑦′!"𝑠"∗̂𝑠"$𝑧∗̂𝑧Partially-CorrectName

MLE
MLE-Aug
MML

β-MML

Loss Functions L(x, B, Pθ)
− log Pθ(y∗|x)

− (cid:80)
− log (cid:80)
β log (cid:80)

ˆy∈B log Pθ(ˆy|x)
ˆy∈B Pθ(ˆy|x)
y Pθ(ˆy|x)β

− 1

Gradients ∇θ(x, B, Pθ)
−∇θ log Pθ(y∗|x)

− (cid:80)

ˆy∈B

(cid:80)

ˆy∈B

(cid:80)

− (cid:80)
− (cid:80)

ˆy∈B ∇θ log Pθ(ˆy|x)
Pθ (ˆy|x)
˜y∈B Pθ (˜y|x) ∇θ log Pθ(ˆy|x)
Pθ (ˆy|x)β
˜y∈B Pθ (˜y|x)β ∇θ log Pθ(ˆy|x)

Table 1: Comparison of loss functions and their gradients over multiple reference B. Note that they
all degenerates to MLE when only the gold program is used as reference, i.e., B = {y∗}

3.2 Learning from Multiple Targets

In this section, we discuss some common loss functions to learn from multiple targets for each
speciﬁcation. Empirically, we ﬁnd that the distinction between those loss functions greatly affects the
model performance (§ 5.3). The loss functions and their gradients are shown in Tab. 1.

Augmented MLE (MLE-Aug): This objective augments MLE with multiple targets by summing
the loss from multiple programs in B, as if {(x, ˆy)}ˆy∈B is a set of different examples in the dataset;
Maximum Marginal Likelihood (MML): MML attempts to approximate Pθ(z∗|x) by marginal-
izing over the correct programs in B. However, for each program ˆy ∈ B, the gradient of it is in
proportion to the likelihood Pθ(ˆy|x) given by the model. This typically results in the model still
putting a majority of the weight on one of the programs in B as noted in [15].
β-smoothed MML (β-MML): Proposed in [15], the β-MML objective is an extension of MML
with β ∈ [0, 1], where it recovers MML when β = 1 and is equivalent to MLE-Aug when β = 0.

4 Partially-Correct Programs

In § 3, we only show how to train the model with self-sampled fully-correct programs (FCPs).
However, during the sampling process, the model can often encounter programs that are very close
to being correct as they only have mistakes in the last few lines, as the examples shown in Fig. 1.
Learning from FCPs only makes the model miss the opportunity to learn useful information from
these programs. Moreover, while sampling a fully-correct program from scratch may not be hard
for simpler domain-speciﬁc languages, it can be challenging for larger sparse program space. And
if no FCP can be sampled, the method described in § 3 degenerates to MLE. Here we show how to
mitigate this issue and more efﬁciently explore the program space by identifying and learning from
partially-correct programs (PCPs).

4.1 Partial Correctness Deﬁnition

In the previous section, we deﬁne correctness of a program based on its ﬁnal execution result
z = E(y). Here we deﬁne partial correctness based on intermediate execution states of programs.
The distinction is also shown in Fig. 1 (solid red boxes vs. dashed red boxes).

Program state. Given a program y = (u1, ..., ut) where ui is the i-th statement, we deﬁne the
execution state si of a subprogram y≤i = (u1, ..., ui) as the set of values of all variables in the
current scope of subprogram y≤i. For example, given a subprogram "a=1;b=3;c=a+b", its program
state is {1,3,4}. Note that the state representation is name-agnostic as variable names are not used,
because there could be programs that are equivalent but use different intermediate variable names.
We use T (y≤i) = si to denote the evaluation of y≤i to obtain its state si, where T : Y → S is
called a tracing function. For simplicity, we only consider multi-line sequential programs (i.e., no
if-conditions or loops), and we leave the study of more complex programs to future work.

State-based equivalence and partial correctness. Given this representation of the program state,
we say two (sub)programs y1 and y2 are equivalent if and only if T (y1) = T (y2), i.e., those two
programs produces the exact same set of variable values. Even though this is a conservative way
for deﬁning program equivalence, it is sufﬁcient for the programs we study in this work. Based on
program equivalence, we recursively deﬁne partial correctness as follows: (1) A fully-correct program

4

Algorithm 2 SamplePrograms(x, Pθ, B) with partial correctness

Input: Model Pθ(y|x); the speciﬁcation x and a set of partially correct programs B
Output: Program samples ˆY .
1: Select ˆy≤i ∈ B \ {ˆy|E(ˆy) = z∗} uniformly at random
2: Sample a set of completions Yp ∼ Pθ(ˆy>i|ˆy≤i, x)
3: ˆY ← {[ˆy≤i||ˆy>i]}ˆy>i∈Yp /* concatenation */
4: return ˆY

is also partially correct; (2) Given a set of known partially-correct programs B, a (sub)program y
is partially-correct if and only if there exists a subprogram y∗
≤i from a program y∗ in B that is
semantically equivalent to y:

∃y∗ ∈ B. ∃j ≤ |y∗| s.t. T (y∗

≤j) = T (y)

From the deﬁnitions, it is easy to see that all subprograms of a partially-correct program are themselves
partially correct, including the empty program. An intuition for this deﬁnition is that for any partially
correct program y≤i, there must be some completion of it y>i such that y = [y≤i||y>i] is fully correct.
Note that we initialize B with the empty program y0 and the gold program y∗, and with self-sampling,
more PCPs and FCPs will be added in B.

4.2 Learning with Partially Correct Programs

Here we introduce how the saved partially correct programs can be used to guide and accelerate the
sampling process, as well as the changes in the two ﬁltering process. Due to space limit, we show the
full training update algorithm with partial correctness in Appendix C.

Guided-Sampling (line 2 in Alg. 1):
In § 3.1, we mentioned that full programs are sampled for
each speciﬁcation x as ˆy ∼ Pθ(ˆy|x). Instead of sampling from scratch, we sample from partially-
correct programs to improve efﬁciency. This partial-correctness-based sampling process is described
in more detail in Alg. 2. Because the sampling process is guided by the partially correct programs
and the generation length is reduced, the model can explore the program space more efﬁciently. Note
that since the empty program y0 is in the buffer B since initialization, the model can still generate
and explore the space from scratch.

Identify partially correct programs (line 5 in Alg. 1):
In § 3.1, we described the simple
correctness-based method to identify which programs to save in B. However, as mentioned in
§ 4, if a program ˆy does not produce the expected output z∗ but its subprogram ˆy≤i is partially-
correct, the model can still learn from ˆy≤i. Since any subprogram of a partially correct program is
itself partially correct, the task here is to ﬁnd the longest subprogram that is partially correct. We
can achieve this simply by traversing the subprograms in decreasing order of the length to ﬁnd the
longest subprogram ˆy≤i with state si that is identical to any of the states from a saved program. 5

Filtering subprograms (line 6 in Alg. 1): With the inclusion of partially correct programs, we
need to slightly change the two ﬁltering criteria in § 3.1. For deduplication, while we still use AST to
rule out changes to the programming style, we also check if the partially correct program ˆy≤i is a
subprogram of another known partially correct program in B. For the same reason, when saving a new
partially correct program ˆy, we need to prune out any existing programs in B that is a subprogram of
ˆy. As for the length constraint, the same principle still applies, but now it is compared against other
partially correct programs that executes to the same state.

Learning from partially correct programs: As partially-correct programs are subprograms y≤i
missing the later part y>i, with an auto-regressive left-to-right generation model, the learning of
Pθ(y≤i|x) is independent of y>i. Thus the learning objectives in § 3.2 do not need to change with the
inclusion of partially correct programs for learning. The only difference is that the end-of-sequence
"<eos>" token is not appended to the PCPs as those programs are not yet ﬁnished.

5In practice, we use a state → subprogram dictionary and the lookup takes a negligible amount of time.

5

5 Experiments

5.1 Experimental Setup

Datasets. We evaluate on two math programming datasets, in which Python code is generated to
solve math problems described in natural language. We chose this domain as our main evaluation
benchmarks as it involves moderately difﬁcult numerical reasoning tasks that requires program
execution, which is ideal to studying our technique. We leave the extension to general programs to
future work and more discussion is in § 6.
(cid:46) MathQA-Python: MathQA-Python consists of 19.2K training examples of NL and Python program
pairs [2]. However, we ﬁnd the raw dataset to contain many programs that share the same NL/program
templates and only differ in concrete number across the train/dev/test sets;
(cid:46) MathQA-Python-Filtered: To better understand the generalization of the trained models, we
derive a deduplicated version of the dataset by ﬁrst merging the train and dev data and then perform
the template-based deduplication. Partly inspired by [12], we re-split the train and dev set based on
the question templates, and 6.8K training data is left after such ﬁltering process. We will release the
processing scripts for replication and comparison.
(cid:46) GSM5.5K-Python: The grade-school-math (GSM8K) dataset [10] contains 7.5K training data.
Since it only provides natural language solutions with math formulas and does not have a dev set, we
ﬁrst reserved 20% of the training data as dev set, then automatically converted the formulas to Python
code in the same style as MathQA-Python. As the result, we run our experiments with the 5.5K
successfully converted training data with Python solutions. Note that the natural language solutions
are not used as input to the models in our experiments.

Evaluation metrics: Following recent work in neural program synthesis [2, 6, 10], we use PASS@k
as our main evaluation metric. It allows the model to sample k programs for each speciﬁcation and
the task is considered solved if any one of the k programs is correct, then the average PASS@k is
calculated on the test/dev set. More details (e.g., temperature) can be found in Appendix A.

Model training: We use GPT-Neo [3] as our program synthesis model and mainly study two model
sizes, 125M and 2.7B. Following previous work [2], we evaluate on the same model checkpoint with
the best PASS@1 score, but note that it might not be the best checkpoint for other k values (more
discussion in Appendix E). Detailed hyperparameter settings can also be found in Appendix A.

5.2 Main Results

Learning from self-sampled programs improves PASS@k. Fig. 2 shows the performance on two
datasets by learning from self-sampled FCPs and PCPs using MLE-Aug, compared with MLE on
single gold program. We can see that our proposed method can greatly improve PASS@k, especially
for higher k values. By inspecting the k generated programs for each task, we ﬁnd that there is more
diversity in the programs that the model generates using our method. More speciﬁcally, we calculate
the ratio of unique programs from the 100 samples for the comparison in Fig. 2a, and ﬁnd that while
30.5% of them are unique for our approach but only 20.8% for the model trained with MLE. By
comparing different base model sizes, we can see that learning from self-sampled programs can help
with both small and large program synthesis models, with a 12.3% and 9.0% PASS@100 improvement
on GSM5.5K-Python for GPT-Neo-125M and GPT-Neo-2.7B, respectively. We also compare our
method with previous work on the original test sets of Grade-school-math and MathQA-Python.
The results are shown in Tab. 2 and Tab. 3. We can see that our method improves over all previous
methods on MathQA-Python. On Grade-school-math, some of the prior works are evaluated on a
different format of NL inputs than ours, so they are not directly comparable. We test Codex using the
same input in a few-shot setting, and we ﬁnd that our method achieves better PASS@1 while being
signiﬁcantly worse in PASS@100 compared with Codex.6

Partially correct programs improve model performance. We next show our results on whether
partially-correct programs help with learning in Tab. 4. First, we observe that using using partial
correctness not only results in PCPs being saved and learned from, it also boosts the number of FCPs
being found with the guided-sampling process. As a result, most PASS@k performances drop if we
do not include partially correct programs in the buffer, as the model learns from a smaller number of

6Due to the little information we have about Codex (e.g., model size, training data), it is hard to derive any

conclusion. However, we found that the programs Codex generates are much more diverse.

6

(a) GSM5.5K-Python with GPT-Neo 125M

(b) GSM5.5K-Python with GPT-Neo 2.7B

(c) MathQA-Python-Filtered with GPT-Neo 125M

(d) MathQA-Python-Filtered with GPT-Neo 2.7B

Figure 2: Main results on the dev set of GSM5.5K-Python and MathQA-Python-Filtered datasets
comparing our proposed learning approach and the common MLE objective. All our methods include
partially-correct programs and use the MLE-Aug loss for learning.

PASS@80

Models

Previous work:

PASS@1

PASS@100

Models

Previous work:

Codex Davinci† [6]
LaMDA 8B∗ [2]
LaMDA 68B∗ [2]
LaMDA 137B∗ [2]

GPT-Neo 125M

w/ MLE
w/ self-sampling FCP only
w/ self-sampling FCP + PCP

42.0
74.7
79.5
81.2

83.2
84.4
84.7

Table 2: Compare with previous meth-
ods on the original test set of MathQA-
Python dataset. ∗: model not pretrained
on code. †: few-shot learning results

OpenAI 6B∗♣ [10]
PaLM-Coder 540B†♣ [9]
LaMDA 137B∗†♣ [2, 9]
Codex Cushman† [6]
Codex Davinci† [6]

21.8
50.9
7.6
5.0
17.0

70.9
-
-
58.0
71.0

GPT-Neo 2.7B
w/ MLE
w/ self-sampling FCP only
w/ self-sampling FCP + PCP
Table 3: Compare with previous methods on the
original test set of GSM8K dataset. ∗: model not
pretrained on code. †: few-shot learning results.
♣: different setting from ours7.

34.0
39.2
41.4

18.8
16.7
19.5

FCPs and PCPs. Interestingly, when comparing different base model sizes, we can ﬁnd that while
the sum of FCP and PCP programs sampled and saved in the buffer are about the same (i.e., 2.36
and 2.31), around 42% of them are FCP for the small model while it is 65% for the larger model.
This is because those PCPs are slowly converted to FCPs with the guided-sampling process. The
difference percentage of PCPs left in the buffer also reﬂects the model’s ability in program completion.
We perform the same experiments on MathQA-Python-Filtered and observe similar performance
improvement with GPT-Neo-2.7B but no clear advantage of adding PCPs for the 125M model. The
corresponding results are shown in Appendix B due to space limit.

7Natural language explanations of the programs are used as input and the few-shot programs are not in the

same format as ours.

7

Models

# Progs. in B
PCP
FCP

k=1

k=5

PASS@k
k=20

k=10

k=50

k=100

GPT-Neo 125M

w/ MLE
w/ self-sampling FCP + PCP
w/ self-sampling FCP only

GPT-Neo 2.7B
w/ MLE
w/ self-sampling FCP + PCP
w/ self-sampling FCP only

-
1.00
0.76

-
1.50
1.26

-
1.36

-
0.81
-

7.4
7.5
7.6

20.6
20.6
20.7

10.6
13.6
13.1

25.1
28.5
28.0

12.7
17.5
16.5

27.2
32.0
31.2

15.3
22.1
20.5

29.5
35.5
34.4

19.2
29.2
26.8

32.9
40.4
38.6

22.7
35.0
32.3

35.5
44.5
41.5

Table 4: Comparing using self-sampled partially-correct programs (PCPs) and only using fully-correct
programs (FCPs). Results are on the dev set of GSM5.5K-Python and MLE-Aug loss is used to learn
from self-sampled programs. The number of FCPs does not include the original reference program.

Self-Sampling

Loss Func.

-

MLE

FCP only

MML
MLE-Aug
β-MML

# Progs. in B
PCP
FCP

-

0.48
0.76
0.57

-

-
-
-

k=1

7.4

6.9
7.6
7.5

k=5

10.6

11.0
13.1
11.7

PASS@k
k=20

k=10

12.7

13.3
16.5
14.5

15.3

16.0
20.5
17.9

k=50

k=100

19.2

20.1
26.8
23.1

22.7

23.7
32.3
27.3

FCP + PCP

MML
MLE-Aug
β-MML

5.5
7.5
7.2
Table 5: Comparison of various loss functions (§ 3.2) with different self-sampling strategies. Results
are on the dev set of GSM5.5K-Python with GPT-Neo 125M as the base model. Best performance
within the same category is in bold and ones worse than MLE is underlined. β = 0.25 for β-MML.

9.0
13.6
12.0

16.2
29.2
23.6

1.10
1.36
1.14

0.40
1.00
0.62

13.1
22.1
18.4

18.7
35.0
27.9

11.0
17.5
14.9

5.3 Ablation on Learning Objectives

We next study the effects of different objective functions described in § 3.2 for learning from multiple
targets, including partially correct programs. Our experiment results on GSM5.5K-Python and
GPT-Neo-125M with different learning objectives are shown in Tab. 5.

The choice of learning objectives matters for PASS@k. First, we observed that all three loss
functions improves the model performance over MLE objective when learning from fully-correct
self-sampled programs, which shows the beneﬁts of learning from multiple targets. Second, we
observed that the three learning objectives (MML, MLE-Aug, β-MML) results in very different
model performance, especially in the PCP+FCP setup: the gap between MML and MLE-Aug is
16.3%. More speciﬁcally, we ﬁnd the MML objective only marginally improves over MLE with only
FCP and performs worse than MLE when also learning from PCPs. As discussed in § 3.2 and Tab. 1,
MML would cause the model to put all weight on one program in the buffer, as the gradient is in
proportional to the likelihood given by the model. As MLE already learns from the gold reference
program, it is hard for MML to make improvements with self-sampled programs, and the performance
may even decrease when MML puts all weight on a partially-correct program.

MLE-Aug loss can most effectively learn from multiple targets. We also note that MLE-Aug loss
results in best performances in all setups. From Tab. 1, we can see that the gradients of MLE-Aug
objective are equally distributed among the programs in the buffer, which leads to more diversity
in its generation due to a more balanced source of learning signals. We can also observe a positive
correlation between the size of the buffer and the PASS@k performance, note that this is a result of a
virtuous circle during training: when a model has better performance, it can sample more FCP or PCP
programs and more programs saved in the buffer leads to better model performance. Thus MLE-Aug
objective leads to the most FCP and PCP being saved, as well as the best PASS@k performance.
β-MML loss is proposed to alleviate the aforementioned issue for MML loss, but we do not observe
an advantage of using it instead of the MLE-Aug loss in our experiments.

8

6 Limitations and Future Work

More general deﬁnition of partial correctness.
In this work, we deﬁne partial correctness based
on state-based program equivalence. As mentioned in § 4.2, this is a conservative way for deﬁning
program equivalence as it requires exact match of the sets of variable values. We can relax this by,
for example, only match the variables that will be referenced later and ignore the others. Besides,
our correctness deﬁnition requires the existence of at least one fully-correct program and we use the
gold program from the dataset. When only the gold execution results are provided for learning (i.e.,
no reference programs), we would need to sample an FCP that matches the gold execution result
to begin with. We may also encounter spurious programs (i.e., incorrect programs yielding correct
result by coincidence) but they can be addressed by adding more test cases.

More complex programs. The programs that we work with are straight-line programs, which
do not contain conditions (e.g., if-else) or loops (e.g., while-do). Since most neural program
synthesis models perform left-to-right auto-regressive generation, the changes to the control ﬂow
break the alignment between program generation and program execution [7, 8, 23]. There are two
potential ways to extend our technique to address the problem. First, we can treat a branch or a loop
as an atomic unit (i.e., a block whose state is the state after executing all statements within it), then
we can apply state-based equivalence in the same way. Second, because our technique only requires
execution after the full programs are generate, we can still evaluate and compare program states based
on intermediate states.

7 Related Work

Code language models. Recently, there are several large language models that pre-trained on
code. Most of these models such as CodeGPT [20], Codex [6], GPT-Neo [3], PolyCoder [28] and
CodeGen [22] use a GPT-style decoder-only architecture and are trained using the causal language
modelling objective. Other models such as CodeT5 [27] use encoder-decoder architecture and models
such as [13] perform inﬁlling of code rather than left-to-right completion. In our work, we use
GPT-Neo as our base model, but our proposed method can be applied any of the above models.

Improving program synthesis with execution. There has been works on using execution to improve
neural program synthesis by either using an executor to prune the search space [17–19] or conditioning
the generation on a representation of the program states [7, 8, 11, 23, 24]. However, these methods
require doing decoding and execution in tandem, which greatly slows down inference. In contrast,
our work only uses execution during training.

Reinforcement learning for generating programs. As pointed out by [15], there are parallels
between our work and the reinforcement learning setting with sparse rewards for generating programs
[1, 4, 18, 19, 26]. Similarly, our approach of identifying partial correctness of programs is similar
to partial rewards. But instead of discounting an entire trajectory with a low reward as in RL, with
programs, we are able to truncate the program to a partially-correct program and assign it the “full
reward”, which is a main contribution of this work. Moreover, the application domain is different
as we apply our approach to general-purpose languages such as Python with large code-LMs, while
most other works test on domain-speciﬁc languages (DSLs).

8 Conclusion

We propose to let a program synthesis model sample additional program solutions for each speciﬁca-
tion and learn from the self-sampled programs that are correct or partially-correct. We deﬁne partial
correctness by tracing and matching intermediate execution states. We experiment on different math
programming tasks and show that such partially-correct programs can help more efﬁcient exploration
of the program space and provide useful learning signal, which improves the PASS@k performance.
Overall, our proposed method can improve PASS@k from 3.1% to 12.3% compared to learning from
a single program with MLE, and such improvement also generalizes to different model sizes.

9

References

[1] Rishabh Agarwal, Chen Liang, Dale Schuurmans, and Mohammad Norouzi. Learning to
generalize from sparse and underspeciﬁed rewards. In International Conference on Machine
Learning, pages 130–140. PMLR, 2019.

[2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David
Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large
language models. arXiv preprint arXiv:2108.07732, 2021.

[3] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale

Autoregressive Language Modeling with Mesh-Tensorﬂow, March 2021.

[4] Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Lever-
aging grammar and reinforcement learning for neural program synthesis. In International
Conference on Learning Representations, 2018.

[5] Shubham Chandel, Colin B Clement, Guillermo Serrato, and Neel Sundaresan. Training and
evaluating a jupyter notebook data science assistant. arXiv preprint arXiv:2201.12901, 2022.

[6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

[7] Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In

International Conference on Learning Representations, 2018.

[8] Xinyun Chen, Dawn Song, and Yuandong Tian. Latent execution for neural program synthesis
beyond domain-speciﬁc languages. Advances in Neural Information Processing Systems, 34,
2021.

[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christo-
pher Hesse, and John Schulman. Training veriﬁers to solve math word problems. arXiv preprint
arXiv:2110.14168, 2021.

[11] Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-
Lezama. Write, execute, assess: Program synthesis with a repl. Advances in Neural Information
Processing Systems, 32, 2019.

[12] Catherine Finegan-Dollak, Jonathan K Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh
Sadasivam, Rui Zhang, and Dragomir R Radev. Improving text-to-sql evaluation methodology.
In ACL, 2018.

[13] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,
Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code inﬁlling
and synthesis. arXiv preprint arXiv:2204.05999, 2022.

[14] Sumit Gulwani. Automating string processing in spreadsheets using input-output examples.

ACM Sigplan Notices, 46(1):317–330, 2011.

[15] Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, and Percy Liang. From language to programs:

Bridging reinforcement learning and maximum marginal likelihood. In ACL, 2017.

[16] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and
Percy S Liang. Spoc: Search-based pseudocode to code. Advances in Neural Information
Processing Systems, 32, 2019.

[17] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond,
Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code
generation with alphacode. arXiv preprint arXiv:2203.07814, 2022.

10

[18] Chen Liang, Jonathan Berant, Quoc Le, Kenneth Forbus, and Ni Lao. Neural symbolic machines:
Learning semantic parsers on freebase with weak supervision. In ACL, pages 23–33, 2017.

[19] Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc V Le, and Ni Lao. Memory aug-
mented policy optimization for program synthesis and semantic parsing. Advances in Neural
Information Processing Systems, 31, 2018.

[20] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. Codexglue: A machine learning
benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664,
2021.

[21] Zohar Manna and Richard J Waldinger. Toward automatic program synthesis. Communications

of the ACM, 14(3):151–165, 1971.

[22] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,
and Caiming Xiong. A conversational paradigm for program synthesis. arXiv preprint
arXiv:2203.13474, 2022.

[23] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,
David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show
your work: Scratchpads for intermediate computation with language models. arXiv preprint
arXiv:2112.00114, 2021.

[24] Maxwell Nye, Yewen Pu, Matthew Bowers, Jacob Andreas, Joshua B Tenenbaum, and Armando
Solar-Lezama. Representing partial programs with blended abstract semantics. In International
Conference on Learning Representations, 2020.

[25] Tal Schuster, Ashwin Kalyan, Alex Polozov, and Adam Tauman Kalai. Programming puzzles.
In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks
Track (Round 1), 2021.

[26] Riley Simmons-Edler, Anders Miltner, and Sebastian Seung. Program synthesis through

reinforcement learning guided tree search. arXiv preprint arXiv:1806.02932, 2018.

[27] Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven CH Hoi. Codet5: Identiﬁer-aware uniﬁed
pre-trained encoder-decoder models for code understanding and generation. arXiv preprint
arXiv:2109.00859, 2021.

[28] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evaluation

of large language models of code. In Deep Learning for Code Workshop, 2022.

Appendix

A Experiment Setting Details

Name

MathQA Grade-school-math

# Training Steps
Learning Rate (LR)
Optimizer
Adam Betas
Adam Eps
Weight Decay
LR Scheduler
# LR Warm-up Steps
Effective Batch Size
FP Precision
Gradient Clipping

50K

25K

1.0e-4
AdamW
(0.9, 0.999)
1.0e-8
0.1
Linear w/ Warmup
100
32
FP 32 for 125M, FP16 for 2.7B
1.0

Table 6: The hyperparameters used for model training on two different types of datasets.

11

Models

GPT-Neo 125M

w/ MLE
w/ self-sampling FCP + PCP
w/ self-sampling FCP only

GPT-Neo 2.7B
w/ MLE
w/ self-sampling FCP + PCP
w/ self-sampling FCP only

# Progs. in B
PCP
FCP

-
0.71
0.43

-
1.03
1.00

-
0.16

-
0.15
-

k=1

k=5

k=10

k=20

k=50

k=80

PASS@k

12.0
11.7
13.5

16.4
20.7
20.3

15.4
16.3
17.4

20.0
26.0
25.2

17.5
18.8
19.5

21.7
28.4
27.1

19.8
21.6
21.9

23.5
30.8
29.0

23.2
25.8
25.8

26.1
34.2
31.9

25.1
28.2
28.0

27.6
36.2
33.6

Table 7: Comparing using self-sampled partially-correct programs (PCPs) and only using fully-correct
programs (FCPs). Results are on the dev set of MathQA-Python-Filtered and MLE-Aug loss is used.
The number of FCPs does not include the original reference program.

Hyperparameters. All hyperparameters for training is shown in Tab. 6. We use β = 0.25 in the
experiments with β-MML, as a result of enumeration search among the values of {0.1, 0.25, 0.5, 0.9}.
We use the default AdamW optimizer settings and slightly tuned the learning rate by trying out several
values between 1.0e-3 and 1.0e-5. The difference in ﬂoating point precision is to ﬁt the GPT-Neo
2.7B model into the memory of the GPUs. All experiments are conducted on V100-32GB GPUs.

PASS@k evaluation. We use temperature sampling and sample n programs with T = 0.8,
where n = 80 for MathQA and n = 100 for GSM to evaluate PASS@n. We also report
PASS@{5, 10, 20, 50} using the n samples and the unbiased estimator proposed in [6]. We use
T = 0.2 to sample 1 program per speciﬁcation and evaluate PASS@1.

Codex few-shot settings. We estimate the Codex [6] performance under the few-shot settings.
More speciﬁcally, the prompt consists of a natural language task description "# Generate Python
code to solve the following math word problems:" and four examples, following previ-
ous work [9]. Each example consists of the NL speciﬁcation as a one-line comment and the gold
program solutions. We evaluate PASS@k for Codex using the same sampling methods as above.

Details for self-sampling. For self-sampling at training time, we sample one program per task in
each mini-batch, i.e., | ˆY | = 1 in Alg. 1 and Alg. 2. However, this is also scaled by the number of
epochs, which makes the total number of programs we sample for each task throughout training to be
around 235 for MathQA-Python-Filtered, 83 for MathQA-Python and 145 for GSM5.5K-Python. For
sampling temperature, we use the same setting as inference time, with T = 0.8.

B Partial Correctness Results on MathQA

Here we perform the same ablation study as § 5.2 to study the effect of using partially-correct
programs on the MathQA-Python-Filtered dataset. The results are shown in Tab. 7. From these
results, we can see a similar trend with Tab. 4, as adding PCPs typically leads to more FCPs being
found. However, we do not observe any advantage of using PCPs for the GPT-Neo 125M model
on this dataset. We think this is because with a small model, the number of PCPs can be found on
MathQA (i.e., 0.16) is much smaller than that of GSM (i.e., 1.36).

C Full Learning Algorithm with Partial Correctness

Our general learning framework in shown as Alg. 1 and it is further extended in § 4. Here we show a
complete version of the algorithm with using partially-correct programs in Alg. 3. Additionally, here
are the detailed explanation of the data structure and functions used in it:
(cid:46) Mapping M: This is a data structure that maps program state to a set of (sub)programs that execute
to that state, i.e., M : S → Y n. In this mapping, we save all PCPs and their execution states,
including all subprograms of any PCP. We use this to signiﬁcantly speed up the lookup process as
mentioned in § 4.2;
(cid:46) Function PartialCorrectnessCriteria(si, M): Since all states for all known PCPs are saved in M,

12

Algorithm 3 Training Update with Partially Correctness

Input: Parameterized model Pθ(y|x)

Parallel training data (x, y∗)
Tracing function T : Y → S

Initialize: Program buffer B = {y0, y∗} with an empty and a fully-correct program
2, ..., s∗
Gold program states (s∗
State-programs mapping M = {s∗

t ) where s∗
i → {y≤i}}t

i = T (y≤i)

1, s∗

i=1

1: repeat
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

ˆY ← SamplePrograms(x, Pθ, B) /* call Alg. 2 */
for ˆy in ˆY do

for i ← |ˆy|; i (cid:54)= 0; i ← i − 1 do

si ← T (ˆy≤i) /* get execution state for each subprogram ˆy≤i */
if PartialCorrectnessCriteria(si, M) then

YS ← M(si) /* get existing subprograms that executes to state si */
if not isDuplicate(ˆy≤i, YS) then
B ← updateBuffer(ˆy≤i, B)
M ← updateMapping(ˆy≤i, M)

continue /* we only need the longest matching subprogram */

update
←−−− ∇θL(x, B, Pθ)

12:
13: until convergence

θ

to know whether a subprogram ˆy≤i is partially-correct, we only need to check if its state matches any
of the known states for a PCP, i.e., if si ∈ M;
(cid:46) Function isDuplicate(ˆy≤i, YS): As mentioned in § 4.2, we use AST and length constraint to rule
out "trivial variants" and identify new PCPs to save in the buffer B. Here the programs to compare
are the set of programs YS that reaches the same state, i.e., being state-based equivalent;
(cid:46) Function updateBuffer(ˆy≤i, B): Here we not only need to add the new PCP into the buffer B, but
also need to prune out the saved programs that are subprograms of ˆy≤i;
(cid:46) Function updateMapping(ˆy≤i, M): Here we need to save the states of all subprograms of an
identiﬁed partially-correct program, thus we will loop through all subprograms of ˆy≤i and obtain its
execution state (similar to line 4 and 5 of Alg. 3), then update M accordingly. As mentioned above,
existing PCPs may be a subprogram of the new PCP, so we also need to prune out such existing PCPs
from the mapping M.

D Qualitative Analysis

In Tab. 8, we show more examples of the fully-correct and partially-correct programs that the models
found during self-sampling, from both the MathQA and GSM datasets. First, we can see that for
some NL speciﬁcation, it is possible that no FCP or PCP can be found with self-sampling, as in
MathQA-Example-1 and MathQA-Example-1. Take MathQA-Example-2 as an example, the question
is quite straightforward thus it leaves very little room for the existence of other correct programs, as
the gold program is already very short. Moreover, we can also observe that the ways self-sampled
FCP and PCP differ from the gold program vary a lot. In MathQA-Example-2, GSM-Example-1 and
GSM-Example-2 the sampled FCPs complete the task with very different paths compared with the
gold programs, and actually result in using fewer lines of code. Another way of getting FCP or PCP
is to perform small and local perturbations, e.g., switch the two sides of a addition or re-order the two
non-dependent statements, as shown in other examples. We ﬁnd that these local perturbations are
more common in general in both datasets, as such patterns are easier for the model to learn.

E Tracking Training Progress

Here we shown the PASS@k performance curve with respect to the training process in Fig. 3.

Learning from self-sampled programs mitigates overﬁtting. From the curves, we can observe
that for MLE, while PASS@1 and PASS@5 generally improves during training, other PASS@k
for higher k actually decreases after reaching the peak performance in early epochs, which is
consistent with previous ﬁndings [10]. This is due to overﬁtting: in the early stage of training,

13

NL Specs.

Gold Program

Self-Sampled FCP

Self-Sampled PCP

(MathQA-Example-1):
The charge for a single room at hotel P
is 70 percent less than the charge for a
single room at hotel R and 10 percent
less than the charge for a single room at
hotel G. The charge for a single room
at hotel R is what percent greater than
the charge for a single room at hotel G?

n0=70.0
n1=10.0
t0=100.0-n0
t1=100.0-n1
t2=t0/t1
t3=t2*100.0
t4=100.0-t3
t5=t4/t3
answer=t5*100.0

(MathQA-Example-2):
If john runs in the speed of 9 km/hr
from his house, in what time will he
reach the park which is 300m long from
his house?

n0=9.0
n1=300.0
t0=n0*1000.0
t1=n1/t0
answer=t1*60.0

n0=70.0
n1=10.0
t0=100.0-n1
t1=100.0-n0
t2=t0/t1
t3=t2*100.0
answer=t3-100.0

-

(MathQA-Example-3):
A class consists of 15 biology students
and 10 chemistry students. If you pick
two students at the same time, what’s
the probability that one is maths and
one is chemistry?

(GSM-Example-1):
Ellie has found an old bicycle in a ﬁeld
and thinks it just needs some oil to work
well again. She needs 10ml of oil to ﬁx
each wheel and will need another 5ml
of oil to ﬁx the rest of the bike. How
much oil does she need in total to ﬁx
the bike?

(GSM-Example-2):
There is very little car trafﬁc on Happy
Street. During the week, most cars pass
it on Tuesday - 25. On Monday, 20%
less than on Tuesday, and on Wednes-
day, 2 more cars than on Monday. On
Thursday and Friday, it is about 10
cars each day. On the weekend, trafﬁc
drops to 5 cars per day. How many cars
travel down Happy Street from Monday
through Sunday?

n0=15.0
n1=10.0
t0=n0+n1
t1=n0/t0
t2=n1/t0
t3=t0-1.0
t4=n1/t3
t5=n0/t3
t6=t1*t4
t7=t5*t2
answer=t6+t7

n0=2
n1=10
n2=5
t0=n0*n1
answer=t0+n2

n0=20
n1=100
n2=25
n3=2
n4=10
t0=n0/n1*n2
t1=n2-t0
t2=t1+n3
t3=n4*n3
t4=t0*n3
answer=t3+n2 \
+t2+t3+t4

n0=15.0
n1=10.0
t0=n0+n1
t1=n0/t0
t2=n1/t0
t3=t0-1.0
t4=n1/t3
t5=n0/t3
t6=t1*t4
t7=t5*t2
answer=t7+t6

n0=10
n1=5
t0=n0+n1
answer=n0+t0

n0=25
n1=2
n2=20
n3=100
n4=10
t0=n0-n1
t1=n2/n3*n0
t2=t0-t1
t3=t2+n4
t4=n0-t3
answer=t4+n3

-

-

n0=15.0
n1=10.0
t0=n0+n1
t1=n0/t0
t2=n1/t0
t3=t0-1.0
t4=n0/t3
t5=n1/t3

n0=10
n1=5
n2=2

n0=2
n1=25
n2=20
n3=100
n4=10

Table 8: More examples of self-sampled fully-correct (FCP) and partially-correct programs (PCPs).
"MathQA" denotes the MathQA-Python-Filtered dataset and "GSM" denotes the GSM5.5K-Python
dataset. All programs are from the ﬁnal buffer after training a GPT-Neo 2.7B model with learning
from self-sampled FCP+PCP using the MLE-Aug loss.

the model is less conﬁdent about its predictions thus the sampled k programs are very diverse,
and while training continues, it overﬁts to the one gold program provided for learning thus leads
to poor generalization when evaluated by PASS@k with high k values. Fig. 3 also shows how
our proposed self-sampling method can mitigate the overﬁtting problem, as it keeps improving or
maintaining PASS@{5, 10, 20} while such performances start decreasing for MLE. Though it also
shows improvements for PASS@{50, 100}, but the performance still decreases in later training stages.

14

Figure 3: How PASS@k on the dev set evolve during training. Results shown on GSM5.5K-Python
dataset with GPT-Neo 125M model. Exponential moving average smoothing is applied for more
clarity, but original curve is shown in shade.

Here we can also see the importance of suitable learning objective, as MML has almost no effect in
mitigating such overﬁtting issue.

Early stopping is needed when prioritizing high k value for PASS@k.
In our experiments, we
select the model checkpoint with the best PASS@1 performance to evaluate all PASS@k. This setup
aims to choose the best model that can solve the task with a small number of attempts (which
corresponds to smaller k value), as studied in [2]. We can also observe that with our methods, the
best PASS@1 checkpoint also yields the best or close to the best PASS@{5, 10, 20} performances.
However, in certain applications where large number of attempts are allowed, PASS@k with high k
values should be prioritized. An example is to generate candidate programs before reranking [10].
In this case, an earlier checkpoint (e.g., one with best PASS@100) should be used instead, which
is not the best checkpoint for PASS@k where k is small. Also note that our proposed method are
not suitable for these applications, as we observe no improvement on the peak PASS@{50, 100}
performances. We think this because when such peak performance is reached, it is still in the early
stage of training thus not many FCPs or PCPs have been saved in the buffer yet.

15

