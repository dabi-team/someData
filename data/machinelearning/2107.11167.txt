1
2
0
2

l
u
J

3
2

]

R
C
.
s
c
[

1
v
7
6
1
1
1
.
7
0
1
2
:
v
i
X
r
a

DYNAMIC DETECTION OF MOBILE MALWARE USING
SMARTPHONE DATA AND MACHINE LEARNING

J.S. Panman de Wit
University of Twente

J. van der Ham
University of Twente

D. Bucur
University of Twente

July 26, 2021

ABSTRACT

Mobile malware are malicious programs that target mobile devices. They are an increasing problem,
as seen in the rise of detected mobile malware samples per year. The number of active smartphone
users is expected to grow, stressing the importance of research on the detection of mobile malware.
Detection methods for mobile malware exist but are still limited.
In this paper, we provide an overview of the performance of machine learning (ML) techniques to
detect malware on Android, without using privileged access. The ML-classiﬁers use device infor-
mation such as the CPU usage, battery usage, and memory usage for the detection of 10 subtypes of
Mobile Trojans on the Android Operating System (OS).
We use a real-life dataset containing device and malware data from 47 users for a year (2016). We
examine which features, i.e. aspects, of a device, are most important to monitor to detect (subtypes
of) Mobile Trojans. The focus of this paper is on dynamic hardware features. Using these dynamic
features we apply state-of-the-art machine learning classiﬁers: Random Forest, K-Nearest Neigh-
bour, and AdaBoost. We show classiﬁcation results on different feature sets, making a distinction
between global device features, and speciﬁc app features. None of the measured feature sets require
privileged access.
Our results show that the Random Forest classiﬁer performs best as a general malware classiﬁer:
across 10 subtypes of Mobile Trojans, it achieves an F1 score of 0.73 with a False Positive Rate
(FPR) of 0.009 and a False Negative Rate (FNR) of 0.380. The Random Forest, K-Nearest Neigh-
bours, and AdaBoost classiﬁers achieve F1 scores above 0.72, an FPR below 0.02 and, an FNR
below 0.33, when trained separately to detect each subtype of Mobile Trojans.

1 Introduction

Nowadays smartphones have become an integral part of life, with people using their phones in both their private and
professional life. The number of active smartphone users globally is expected to be 7.3 billion by 2025 [1]. The rise
in smartphone users has also led to an increase in malicious programs targeting mobile devices, i.e. mobile malware.
Criminals try to exploit vulnerabilities on smartphones of other people for their own purposes. Additionally, over the
past years, malware authors have become less recreational-driven and more proﬁt-driven as they are actively searching
for sensitive, personal, and enterprise information [2].

Detection of mobile malware is becoming increasingly relevant, with machine learning showing the most promise. In
this paper we present an overview of the performance of state-of-the-art machine learning classiﬁers using sensor data
on the Android mobile operating system. We show different training approaches for each of the classiﬁers, showing
their performance on individual and general cases. These machine learning classiﬁers can be trained on a cluster on a
large dataset, resulting in a classiﬁer. This classiﬁer can then be deployed on mobile phones to detect future malware
based on the dynamic sensor data similar to the dataset, i.e. dynamic analysis.

Academic work is mainly divided into dynamic analysis and static analysis of mobile malware. Dynamic analysis
refers to the analysis of malware during run-time, i.e. while the application is running. Static analysis refers to the

 
 
 
 
 
 
A PREPRINT - JULY 26, 2021

analysis of malware outside run-time, e.g. by analysing the installation package of a malware app. Dynamic analysis
has advantages over static analysis but methods are still imperfect, ineffective, and incomprehensive [3].

An important limitation is that in most studies of malware detection, virtual environments are used, e.g. analysis on a
PC, instead of real mobile devices. An increasing trend is seen in malware that use techniques to avoid detection in
virtual environments, thereby making methods based on analysis in virtual environments less effective than methods
based on analysis on real devices [2]. Moreover, we found that most methods are assessed with i) malware running
isolated in an emulator, and ii) malware running for a brief period. This kind of assessment does not reﬂect the
circumstances of a real device with for example different applications running at the same time. Therefore, most
research does not provide a realistic assessment of detection performances of their detection methods due to their
unrealistic circumstances.

Using a dataset [4] containing data of 47 users throughout the year 2016, this research provides valuable knowledge
on detecting mobile malware on real devices and a realistic assessment of the research’s detection methods. This paper
focuses on dynamic hardware features as our initial literature exploration showed relatively few research compared to
other types of dynamic features (e.g. software and ﬁrmware). In this work, we compare the detection performance
of the Random Forest, K-Nearest Neighbour, and AdaBoost classiﬁers. The classiﬁer performance is assessed on 10
subtypes of Mobile Trojans, i.e. malware showing benign behaviour but performing malicious behaviour, as these are
the most prevalent mobile malware type [5, 6]. Furthermore, we examine which features are important in detecting
different subtypes of Mobile Trojans. We ﬁnd that the Random Forest and AdaBoost classiﬁers have similar and
better performances than the K-Nearest Neighbour. Additionally, our initial results show that AdaBoost may be more
suitable than the other classiﬁers, for use cases where detection models are built with data from one subset of users and
subsequently used for protecting other users. Additionally, we show that application-speciﬁc features are important in
detecting mobile malware. In contrast to other literature, which often assessed detection models on devices running
one or few applications in virtual environments, we ﬁnd low predictability using only global device features.

In summary, we make the following contributions:

• We use a recent real-life sensor dataset to create multiple mobile malware detection models. Most literature
is based on environments with models based on emulator data, simulated user input, and/or short testing
durations. Our research takes a realistic approach, contributing to the body of scientiﬁc literature on malware
detection that is based on real-life data rather than data derived from emulators and virtual environments.

• Our results are based on a dataset containing data from 47 real users over a time span of 1 year. To the authors’
best knowledge, this is one of the ﬁrst research on data from this many users over a long time period. The
large user pool and time span increases the conﬁdence in the generalisability of the results.

• We evaluated our detection methods on a Random Forest, K-nearest neighbour and AdaBoost classiﬁers with
several performance metrics. AdaBoost has been researched little in literature. We provide the performance
of the classiﬁers with several training and testing methods.

• Performance of the classiﬁers is shown for seperate mobile malware types. The results show a large difference
in performance for several mobile malware types, indicating the difﬁculty of detecting some mobile malware
types. Earlier research, to the best of our knowledge, did not include a comparison of this many mobile
malware types.

• We brieﬂy describe a testing method where models are trained on data of one set of users and tested on data
from another set of users. This testing method may be useful when detection models are built with one user
pool and are subsequently used to protect another, perhaps larger, user pool.

• All features included in this research do not require any privileged access. This increases the usability of
mobile applications based on our detection methods. Additionally, we show which feature categories and
which speciﬁc features are most important in detecting different malware types.

2 Related works

In an early, highly cited study (Shabtai [7] from 2011), the authors designed a behavioural malware detection frame-
work for Android devices called Andromaly. This used device data: touch screen, keyboard, scheduler, CPU load,
messaging, power, memory, applications, calls, processes, network, hardware, binder, and LED. 40 benign applica-
tions and 4 self-developed malware applications (a DOS Trojan, SMS Trojan, Spyware Trojan, and Spyware malware)
were executed. When training the detection model on data from the same mobile device, the J48 decision tree classiﬁer
performed best; on the other hand, a Naive Bayes classiﬁer detected malware best in experiments in which the testing
was done on data from a new device (see the overview in Table 2). Despite relatively high False Positive Rates with

2

A PREPRINT - JULY 26, 2021

all their models, Andromaly showed the potential of detecting malware based on dynamic features using machine
learning, compared different classiﬁers, trained and tested on data collected from real devices. However, the study is
relatively old, and the malware ecosystem has since changed.

Table 1: Related work. When multiple malware classiﬁers are studied, the best one is emphasized.

Table 2: *

Legend:

Acc accuracy, AZ Androzoo [8], bat battery, BN Bayesian Network, Cr Crowdroid [9],

cust Custom, Dr Drebin [10], DS Decision

Stump, DT Decision Tree, GBT Gradient Boosted Tree Ge Malware Genome Project [11], GM Gaussian Mixture, histo Histogram,

IR Isotonic Re-

gression Kmeans K-means clustering, KNN K-Nearest Neighbour, LDA Linear Discriminant Analysis, LDC Local Density Cluster-Based Outlier Fac-

tor,

loc Location, LR Logistic Regression, mem memory, md metadata, MLP Multi-Layer Perceptron, mult Multiple datasets, NB Naive Bayes, netw

network, NN Neural Network, Om Open Malware [12], perm permissions, PC Parzen classiﬁer proc process PS Play Store, QDA Quadratic Dis-

criminant Analysis, RBF Radial Basis Function RF Random Forest,

sens.

act Sensitive activities, ShD Sherlock dataset stdev standard deviation,

sto storage, SVM Support Vector Machine, SC system calls, UP user presence, VE Virtual Environment, VS VirusShare [13], VT VirusTotal

[14]

Article

Features

Training and testing

Performance

Ref Year Dynamic

Static Benign Malware Platform

Classiﬁers

Acc TPR FPR

BN, histo, J48,
Kmeans, LR, NB
BN,
LR,
J48,
MLP, NB, RF
RF, BN, NB,
MLP,
J48, DS,
LR
NB, RF,
SVM

LR,

stdev
1-KNN,
LDA,
QDC, MLP, PC,
RBF
NN, DT

[7]

2012 various (14)

-

40

4cust

2 devices

[15] 2013

bat, Binder, CPU,
mem, netw

perm 408PS

1330Ge,VT VE, Monkey

[16] 2013 Binder, CPU, mem -

408PS

1130Ge,VT VE, Monkey

[17] 2013

[18] 2014

CPU, net, mem,
SMS
bat, CPU, mem,
netw

[19] 2014 bat, time, loc

-

-

-

30PS

>2PS

-

5cust

3Cr
2cust

1 device

12 devices

GM-LDC

11 devices

[20] 2016 SC, SMS, UP

md

9804PS

2800

3 devices

[21] 2016 bat
[22] 2016 CPU, mem
[23] 2016 CPU, mem, SC
[24] 2016 CPU, mem, net, sto
[25] 2017 CPU, mem, net
[26] 2018 CPU, net

[27] 2018

[28] 2018

Method calls,
Intents
Network, SMS,
Files, more

[29] 2019 CPU, mem, api

[30] 2019 Bat, CPU, net, proc

-
-
-
-
-
-

-

-

-

-

-
940PS
1709PS
1059PS
-
>10

1 device
VE, Monkey LR
VE, Monkey Kmeans-RF
VE, Monkey RF

7cust
1120Ge
1523Dr
1047Dr
<5560Dr VE, Monkey
3ShD

SVM
DT

47 devices

17kPS, AZ 13.9kmult VE, Monkey RF

-

-

3ShD

VE, Monkey

4442Dr
27000Om VE
3ShD

47 devices

[31] 2020 API calls

perm 11505

19620

devices,

8
DynaLog

this

CPU, bat, mem,
net, sto

-

10cust

10cust

47 devices

RF, DT, KNN,
SVM, NN
SVM, RF, NN
IR, RF, DT, GBT,
MLP, SVM, LR
NN, NB, SVM,
DT

RF, NB, KNN,
MLP, AdaBoost

0.81

0.79 0.48

0.81

0.97 0.31

1.00

-

few

-

0.99 0.00

≈1 ≈1 ≈0
≈0
≈1

-

-

0.97 0.00

-
-
0.67
1.00
0.82
1

>0.85 -
0.86 0.17
0.61 0.28
0.82 0.01
-
1

-
<1%

0.98

0.98 -

0.813 0.813 -

0.97

1

-

1

-

<1%

0.95

0.98 0.09

0.96

0.65 0.01

The STREAM framework [15], aims to enable rapid large-scale validation of machine-learning classiﬁers for mobile
malware. STREAM used 41 features (listed in Table 2) collected every 5 seconds from emulators running in a so-called
ATAACK cloud. The emulator used the Android Monkey application to simulate pseudo-random user behaviour such
as input on the touchscreen. To evaluate their detection model, the authors used six classiﬁers, with good accuracy and
TPR for the Bayesian Network. The training set had 408 popular applications from the Google Play Store and 1330
malware applications from the Malware Genome Project database [11] and the VirusTotal database [14]. The testing
set had 24 benign and 23 malware applications from the same databases. This showed the potential of using dynamic
features, although the FPR for all classiﬁers was relatively high. Additionally, this research ran applications separately
for 90 seconds and made use of a virtual environment in the form of an emulator with user-like behaviour created by
the Android Monkey tool. This lowers the conﬁdence that the model would perform the same in real life. The same

3

A PREPRINT - JULY 26, 2021

dataset from Amos [15] was used in Alam [16] for anomaly detection with application behaviour features: only the
Binder, CPU, and memory features. The dataset was then balanced. The Random Forest classiﬁer had an excellent
accuracy of 99.9857% and a root MSE of 0.0183%. Only 2 false positives were measured during this experiment.
However, it is sensitive to the same limitations as Amos [15].

Saracino [20] describes an improvement of an earlier anomaly detection model (Dini [32]). This version includes
system calls, user presence, and SMS features, plus a static analysis of application packages. Many classiﬁers were
trained and tested. Their performance is not reported, but the authors state that 1-Nearest Neighbour achieved the best
classiﬁcation results. To evaluate the TPR, the detection framework has been tested against 2800 malware applications
from the Malware Genome Project, Contagio [33], and VirusShare [13], representing the Botnet, Installer, Trojan,
Ransomware, Rootkit, SMS Trojan, and Spyware malware families. How long the different malware applications
were run is not mentioned. Also, it is unknown whether the same device was used for the training and testing of the
classiﬁers. The framework achieved very good TPR and FPR, and also detected some zero-day attacks, undetected by
multiple Antivirus Software at that time. The performance was assessed on real devices, varying the usage intensity
over a period of one week, to reﬂect real-life circumstances. A limitation is a requirement for root permissions, as
system calls were used as features.

In Milosevic [22], memory and CPU usage are predictive features, tracked by running every application separately
for 10 minutes in an Android emulator. The emulator was fed with user-like input by the Monkey application. Their
optimized feature set only contained 7 features. The classiﬁcation algorithm used was linear Logistic Regression with
the use of a sliding window technique. This was validated on a set of 94 benign and 89 malware applications.

The classiﬁer with the highest TPR of 95.7% of malware had a relatively high FPR of 25%; the classiﬁer with the
highest F-measure had a TPR 85.5% and an FPR of 17.2%. This study showed the potential of using the memory and
CPU usage as features for the dynamic detection of mobile malware. However, the detection had relatively high FPR,
besides the disadvantage of using emulator data.

More studies were based on emulation. In Ferrante [23], the features were related to system calls, memory, and CPU
usage. Benign apps were downloaded from the Google Play store and malicious applications were collected from the
Drebin dataset [10]. The applications were run for 10 minutes in an Android emulator and features were collected
every 2 seconds. The emulator was fed with user-input from the Monkey application. As detection models, they ﬁrst
used a K-means clustering algorithm to cluster apps based on the similarity of memory and CPU usage, then a Random
Forest classiﬁer on every cluster that classiﬁed the applications based on their system calls. The best performing model
was the classiﬁer with 7-means clustering and a Random Forest classiﬁer of 50 trees. This showed the potential of
using system calls, memory, and CPU features for malware detection. The performance is relatively low compared to
other detection methods, and the method needs root permission as it uses system calls as features. In Canfora [24], the
features are related to CPU, memory, storage and network. The applications were run for 60 seconds in an Android
emulator. The features were used both raw and after a Discrete Cosine Transformation. Furthermore, the authors
varied the granularity of the detection method by either taking all the features, only the global features, or only the
features of the application under analysis. The Random-Forest classiﬁer using global features had very good accuracy
and FPR.

In Massarelli [25] the authors used the Drebin dataset for malicious applications (but it is unknown how many malware
applications were included from the 5560 in the dataset). The features were both system-wide and application-speciﬁc.
An emulator was used for training the model, fed with simulated user input events from the Monkey application. The
classiﬁer was an SVM with a Radial Basis Function kernel, which had good accuracy. The exact FPR is unknown, but
the precision of the model ranges from 10% to 90% depending on the malware family, which is low in comparison
with other methods. This also used an emulator, leading to limitations in its applicability.

In Martin [28], multiple classiﬁers, including Random Forest, K-nearest neighbour, and SVMs are assessed. They
achieve an F1 score of 0.803 on a dataset containing 4442 malicious malware samples. They use hardware features
related to network, SMS, ﬁle reads/writes. Other features included are: services started, classes loaded, information
leaks via the network, ﬁle and SMS services, circumvented permissions, cryptographic operations, SMSs sent, and
phone calls. Their detection models are also assessed using a virtual environment rather than devices. This is also the
case for Dai [29]. They use features related to CPU, Memory and API calls (but also software features such as API
calls, which this paper does not). Their best classiﬁer is well-performing with an F1-score of 0.98. Additionally, they
assess their detection model on a large dataset of more than 27.000 malicious applications.

During the writing of this paper, other authors used the SherLock dataset, but to a much more limited extent than this
research. In Memon [30], 7 classiﬁers were trained for malware detection: Isotonic Regression, Random Forest, Deci-
sion Trees, Gradient-Boosted Trees, MultiLayer Perceptron, SVM, and Logistic Regression. The data was balanced so
that 50% of the labels were benign and 50% were malicious. Only data from Q3 2016 was studied, meaning that only

4

A PREPRINT - JULY 26, 2021

three malware types (Madware, Ransomware and Click-jacking) were included for training and testing, with CPU,
network trafﬁc, battery, and process features. The Gradient-Boosted Trees had the best results with an F1 score of
0.91% and an FPR of 0.09%. Their Random Forest classiﬁer also worked almost equally well. These make for better
results than prior work, which may be due to the limited number of malware types included.

In Wassermann [26], Decision Trees were trained as classiﬁers, only on SherLock data from Q2 2016, so only three
malware types (Spyware, Phishing, Adware). The features used were network trafﬁc and CPU-related. The label
to predict was whether the malicious app (Moriarty) was running. Their model results in a recall of almost 100%
with less than 1% FPR. This research showed good app-detection results, although on a limited set of malware; also,
the model cannot distinguish between individual (benign or malicious) actions; it only predicts the presence of the
malware on the system.

Recent literature on dynamic malware detection (Droidcat [27] and Dl-Droid [31]) have high performance on a large
dataset containing more than 10000 malware applications. However, DroidCat focuses on software features such as
Method Calls and trained and tested their method within a virtual environment with simulated pseudo-random user
behaviour. Dl-Droid achieves a TPR of 0.95 and an FPR of 0.09, but also only including software features such as API
calls. This paper focuses on hardware features only.

Recent work by (Cai et. al. [34]) analysed 17.664 Android applications developed throughout 2010–2017. The paper
describes differences in method calls, ICC calls and source/sink calls during static-code analysis and dynamic analysis.
One of the ﬁnding of the paper is that access to sensitive data was almost 10x more frequent during dynamic analysis
than during static analysis (Figure 17 and Finding 8). This ﬁnding indicates that dynamic analysis may support in
capturing all sensitive data calls and could therefore help in detecting mobile malware.

Other research assessed have also used machine learning for the dynamic detection of mobile malware, but this liter-
ature is unclear about the feature collection method (Ham and Choi [17]), include few malware samples (Attar [18],
Dixon [19]), or experiment on an idle phone (Caviglione [21]).

While we have seen a body of prior work using dynamic hardware features to detect malware on mobile devices, im-
portant issues remain unaddressed. Most recent papers have trained their detection model on emulators with simulated
input; this makes it difﬁcult to extrapolate their performances with real users and to have high conﬁdence in how
realistic the results are. Additionally, most studies have only executed the benign and malicious applications for a few
minutes, meaning that any delayed malicious behaviour would not be detected. Also, a limited amount of research
has shown excellent results, particularly in what regards the FPR metric. In this paper, we address these issues and
provide a more realistic picture by learning from data gathered with real users, from a study which executed malware
from different categories, over a large time span.

3 Method

3.1 Malware data

We use the SherLock dataset [4] provided by Ben-Gurion University. This dataset contains 10 billion data records in
system logs collected from 47 Samsung Galaxy S5 devices used by 47 different volunteer users, over a period of over
one year (2016). During this year, the volunteers executed both benign and malicious applications on their devices.
The data collection agent on the phone is based on the Funf Open Sensing framework, developed by MIT Media Lab
[35]. This framework allows for the collection of sensor data (e.g. memory consumption or CPU usage). The authors
adjusted the source code to improve stability and reliability. Additionally they added features such as the collection of
statistics on all running applications of a smartphone. According to the SherLock paper[4] all features can be tracked
without any root permissions. The source code of the data collection agent can be found on GitHub [36].

The malicious applications were written by the researchers based on wild malware. Every month a different subtype
of Mobile Trojan was installed on the devices. Each malware version resembled a subtype of Mobile Trojan with both
benign and malicious actions. In total, 11 malware versions were included, listed in Table 3 with their benign behaviour,
malicious behaviour (as described by [4]), malware type, a description of their actions, and the wild malware on which
the implementation was based. Important to note is that each time data was transmitted by the malware application, it
was scrambled prior to sending to protect the privacy of the users.

The dataset is divided into 13 probes, namely groups of multiple sensors that shared the same sample interval. This
research uses the following probes:

• the Moriarty (Malware) probe for malware data,

• the T4 (System) probe for global device data, and

5

A PREPRINT - JULY 26, 2021

Table 3: Malware types

Benign
behaviour

Malicious
behaviour

Type

Description

1

2

3

4

5

6

7

8

9

Game

Contact theft

Spyware

Web browser

Spyware

Spyware

Utiliz.
Widget

Photo theft

Spyware

Sports app

SMS bank thief

SMS Fraud

Game

Game

Game

Phishing

Phishing

Adware

Adware

Madware

Hostile
downloader

Lock-screen

Ransomware

Ransomware

File Manager

Click-jacking

10

-

Device theft

11 Music Player

Botnet

Privilege
escalation
-

DOS

12

Web media
player

Recon. Inﬁltr.

Other

Steals, encrypts, and transmits all contact stored on
device.
i) Spies on location and audio, or ii) spies on web
trafﬁc and web history.
Steals photos that are taken and in storage, and
takes candid photos of the user.
Captures and reports immediately on SMSs that
contain codes and various keywords.
Makes fake shortcuts and notiﬁcations to login to
Facebook, Gmail, and Skype.
Gathers information and places ads, popups and
banners.
Gathers private information, places shortcuts, noti-
ﬁcations, and attempts to install new applications.
Performs either: 1) lock screen ransomware, or 2)
crypto ransomware.
Tricks the user to activate accessibility services to
then hijack the user interface.
-
Either performs: 1) DDoS attacks on command, or
2) SMS botnet activities
Maps the connected local network and searches for
ﬁles and vulnerabilities.

Wild
malware
SaveMe,
SocialPath
Code4hk,
xRAT
Photsy,
Phopsy

Spy.Agent.SI

Xbot

-

-

Simplocker.A,
SLocker
Shedun
(GhostPush)

Tascudap.A,
Nitmo.A. . .

-

• the Applications (Apps) probe for app-speciﬁc device data.

The Malware probe sensed data from the malicious application installed on the devices. Each malware probe’s record
is a log of the action taken by the user’s malware application. Each action is part of a malware session: the malware ap-
plication started in either a benign or malicious session. Within a benign session, only benign actions were performed;
in a malicious session, the malware performed both benign and malicious actions. An overview of Malware probe’s
columns, their data type, and a description is shown in Table 4. For this research, we excluded version 10 and 12 as
no malicious data was available for these versions.

Table 4: Malware probe data overview

Column

Datatype Description

UserId
UUID
Details
Action
ActionType
SessionType
Version
SessionID
Behavior

String
Numeric
String
String
String
String
Decimal
Decimal
String

User ID
Timestamp of action
Details of action
Action performed
Whether action is benign or malicious
Whether session is benign or malicious
Version number
Session ID
Behaviour of current session

The System probe tracked global device data every 5 seconds. Each record of the system probe is a log of the user’s
global device data at a given time, taken from the /proc/ folder. The feature categories that were tracked are battery,
CPU, network, memory, I/O interrupts, and storage. 41 features are used from this probe.

The App probe recorded app data every 5 seconds for each application installed on the device. For this research, the
only relevant data is the app data of the malware application. Each record is a log of the malware app data at a given
time, taken from the /proc/ folder. 35 features are used from this probe. For a complete overview of the features
used, refer to Table 13.

The data from the three probes is integrated by joining on the userid and timestamp ﬁelds. Not all probes had the
same tracking frequency so joining on the timestamp column can not be done with a precise join. Therefore, probe
data with a maximum of 5 seconds after the timestamp of the Malware data is used in the joining of the data. This

6

A PREPRINT - JULY 26, 2021

way we can match 83% of the Malware data with probe data. Additionally, we apply random undersampling to
rebalance the original dataset. The original dataset contains 90% malicious records and only 10% benign records. The
dataset is balanced by downsizing the number of malicious data points per malware type until 90% of the rows are
benign actions and 10% are malicious. The malicious data points that are removed are chosen uniformly at random.
The undersampling method is applied to both the training and testing data. We apply the resampling method on the
training data to prevent the classiﬁers from a bias towards the (original) majority class (i.e. malware data). We apply
the resampling method on the testing data to to better reﬂect real-life circumstances and to provide more realistic
classiﬁcation results. In the real world, the number of malicious actions on a mobile phone would be relatively low
compared to the number of benign records. Lastly, we normalize any features in the dataset by scaling it between zero
and one for the KNN classiﬁer. No data normalization is used for AdaBoost and RF as these are tree-based classiﬁers.
The dataset now consists of 28821 rows, 102 columns, and is 21.4 MB.

3.2 Supervised learning algorithms

We train a statistical classiﬁer able to recognize malware signatures in any log data collected on a smartphone. The
classiﬁer is trained, cross-validated, and tested using the dataset described above. Three training algorithms yielded
well-performing classiﬁers: Random Forest and K-Nearest Neighbour classiﬁers had a good performance in our related
work (see Section 2), so we include them in our study. We add the AdaBoost classiﬁer, which is designed speciﬁcally
to improve the performance of a Random Forest. All are nonlinear classiﬁers, able to capture complex relationships
between the variables in the dataset. Table 5 (based on [37]) lists advantages and disadvantages for these algorithms.

Table 5: Pros and cons of classiﬁers

Classiﬁer
Random Forest (RF) Can overcome overﬁtting
AdaBoost

Pros

K-Nearest
Neighbours (KNN)

Nonlinear classiﬁcation
Efﬁcient with high-dimensional data
High precision and accuracy
Nonlinear classiﬁcation
No assumption of features

Cons
A greedy algorithm
Sensitive to imbalanced dataset

Computationally expensive
Sensitive to imbalanced dataset

3.3 Training, cross-validating, and testing malware classiﬁers

All classiﬁers are trained to identify a malicious action (the ActionType column of the Malware dataset). The classiﬁers
that are trained differ in their input data (feature set), classiﬁcation target, and test mode. Three feature sets are assessed:
Global, Apps and a combination of both. The global feature set includes global device features from the System probe.
The apps feature set include features from the Apps probe.

Two classiﬁcation targets are tried. The ﬁrst classiﬁcation target (all malware) trains one general malware classiﬁer
to identify malicious actions by any of the 10 malware types in the dataset. The second classiﬁcation target (single
malware) trains one single-purpose classiﬁer per malware type, resulting in 10 different trained models.

Two test modes are run. Both use a percentage of the data for training, and the remaining for testing. In the normal
holdout test mode, the training and testing data is sampled at random. In the unknown device test mode, the testing data
contain data of devices that are not in the training data. The unknown device test mode is used to simulate a situation
in which no data is yet available for a certain device. In that case, a model trained on data from other devices is a
solution. Testing the models on other devices also helps to detect the potential bias of the model on a speciﬁc device.
Note that only the testing on the test set is adjusted; the cross-validation remains the same.

Table 6: Different conﬁgurations for the classiﬁers trained

Feature set
(complete listing in Table 13) Apps

Type
Global

Classiﬁcation target

Test mode

Combined
All malware
Single malware
Normal holdout
Unknown device

Description
Device features (the System probe)
App-speciﬁc features (the Apps probe)
Both Global and Apps features
Generic classiﬁer across malware types
One classiﬁer for each malware type
Training data includes all devices
Test set includes data from new devices

7

A PREPRINT - JULY 26, 2021

To train the dataset, we use 4-fold cross-validation with holdout. The hyperparameters of each classiﬁer are tuned
using a grid search. The search space for the hyperparameter settings are as follows. For AdaBoost, the number of
estimators is in the interval [5, 400]; for Random Forest, the number of trees in the interval [5, 320], the maximum tree
depth in [3, 320], the maximum number of features in [3, 102]; for K-Nearest Neighbours, the number of neighbours in
[1, 61]. We also use Recursive Feature Elimination together with cross-validation (RFCV) to ﬁnd the optimal number
of features for all classiﬁers. RFCV requires a feature ranking, as the method recursively eliminates the least important
feature. The feature rankings are based on the Random Forest classiﬁer and are calculated per different experiment
setup (i.e. per classiﬁcation target, per test mode, per feature set).

To evaluate a classiﬁer, we use the standard F1 score [3], namely the harmonic mean of the Precision and Recall scores.
These two metrics are normally a trade-off of each other, and a high F1 score indicates a good balance between these
two metrics. Additionally, the training and testing times of the classiﬁers are assessed.

The data is processed on a computing cluster with a Hadoop File System (HDFS) suitable for big data. The cluster
consists of 55 nodes with each 32 GB RAM and 8-16 cores running Apache PySpark 2.2.0. The training and testing
is done with the package Spark Sklearn [38], a distributed implementation of the machine-learning classiﬁers in
the popular package machine-learning library Scikit-learn [39]. Spark Sklearn is adjusted to provide and log more
information during the execution of experiments.

4 Results

4.1 Test mode normal holdout

An overview of the performances of the classiﬁers for classiﬁcation target all malware and test mode normal holdout
is shown in Table 7. This table shows the feature categories present in the best model, i.e. with the best hyperparameter
settings. From all models with similar performance as the best model of that classiﬁer (performance results that do not
differ statistically signiﬁcantly (McNemar test a < 0.05 [40]), the classiﬁer with the least number of features is chosen,
because this shows which features are relevant for detecting speciﬁc malware types. The number of features is based
on the RFCV method described in Section 3. Table 7 shows the following:

1. RF has the highest F1 score of 0.73 with feature set Apps and 0.72 with feature set Combined.
2. All classiﬁers have an F1 score below 0.42 with feature set Global. This suggests that global features are less

discriminative when detecting malicious actions.

3. RF with feature set Combined uses 10 app features (related to: App CPU, App Memory, App Process) to
detect malicious actions of malware, suggesting that 10 app features are sufﬁcient in detecting malicious
actions of mobile malware. These 10 features are shown in Table 14. All features and their corresponding
feature category are shown in Tables 17 and 18.

4. All classiﬁers have a higher FNR than FPR. This indicates that most models incorrectly classify a malicious

action as benign more often than vice versa.

Table 8 shows an overview of the performance per classiﬁer per malware type and feature set. Following the same
reason as for Table 7, the performances are shown for the least number of features and the best hyperparameter settings
per classiﬁer. Table 8 shows the following:

1. RF has the highest F1 score for 5 out of 10 malware types.
2. KNN has the highest F1 score for 5 out of 10 malware types.
3. AdaBoost has the highest F1 score for 4 out of 10 malware types 1.
4. AdaBoost has a high F1 score (>0.7) for 8 out of 10 malware types.
5. RF has a high F1 score for 7 out of 10 malware types.
6. KNN has a high F1 score for 7 out of 10 malware types.
7. All classiﬁers have a low F1 score for malware type 4 (Spyware SMS) and 6 (Adware).

The feature categories included in the feature sets of the best models of classiﬁers per malware type are shown in Table
9. Following the same reason as for Table 7, the performances are shown for the least number of features and the best
hyperparameter settings per classiﬁer. This shows the following:

1. 6 out of 10 malware versions are best detected using only app features.
2. Malware version 4 (Spyware SMS) is best detected using only global features. However, note that the F1

score for Spyware SMS is the lowest of all malware versions.

1Some performance scores are the same, leading to a draw, hence to more than expected highest F1 scores.

8

Table 7: Comparison performance per classiﬁer for classiﬁcation target all malware

A PREPRINT - JULY 26, 2021

Classiﬁer
Feature set
Best trained model

Global

AdaBoost
Apps

Comb.

Random Forest
Apps

Global

Comb.

Nr. features
Accuracy
F1 score
FPR
FNR
Feature categories used

26
0.922
0.226
0.003
0.868

25
0.943
0.595
0.012
0.522

43
0.945
0.610
0.013
0.502

24
0.922
0.422
0.022
0.670

29
0.958
0.730
0.013
0.346

10
0.959
0.722
0.009
0.380

X
X

X

X

Battery
CPU
IO Interrupts
Memory
Metadata
Network
Storage
Wiﬁ
App CPU
App Info
App Memory
App Network trafﬁc
App Process

X
X

X

X

X
X

X

X

X

X
X
X

X

X
X
X

KNN
Apps

13
0.944
0.674
0.029
0.336

Comb.

9
0.946
0.688
0.028
0.320

Global

26
0.897
0.356
0.048
0.674

X
X

X

X

✓

✓
✓
✓

X

X

X

X

X
X
X

X

X

X

Table 8: Comparison performance per classiﬁer for classiﬁcation target single malware and test mode normal holdout
(The numerical malware types in the ﬁrst column are those listed in Table 3.)

Malware

F1

AdaBoost
FPR

1
2
3
4
5
6
7
8
9
11

avg.
stdev

0.704
0.944
0.764
0.400
0.927
0.194
0.813
1.000
0.913
0.909

0.757
0.263

0.025
0.003
0.000
0.029
0.005
0.011
0.015
0.000
0.000
0.009

0.010
0.010

FNR

0.321
0.085
0.382
0.667
0.089
0.880
0.216
0.000
0.160
0.118

0.292
0.283

F1

0.571
0.956
0.793
0.500
0.927
0.271
0.857
0.800
0.936
0.938

0.755
0.232

Random Forest
FPR

0.025
0.001
0.003
0.000
0.005
0.039
0.003
0.000
0.000
0.000

0.008
0.013

FNR

0.500
0.077
0.324
0.667
0.089
0.782
0.227
0.333
0.120
0.118

0.324
0.251

F1

0.536
0.945
0.727
0.400
0.946
0.352
0.793
0.857
0.936
0.938

0.743
0.233

KNN
FPR

0.047
0.003
0.003
0.029
0.005
0.057
0.012
0.032
0.000
0.000

0.019
0.021

FNR

0.464
0.077
0.412
0.667
0.054
0.662
0.268
0.000
0.120
0.118

0.284
0.252

3. Malware version 11 (DOS) is best detected using only one global feature regarding network trafﬁc.
4. Malware version 1 and 9 (Spyware contacts theft and Privilege Escalation / Hostile downloader) is best

detected using both the Apps and Global features.

5. None of the classiﬁer includes I/O interrupts, Storage, or Wiﬁ features for any of the malware versions.

4.2 Test mode unknown device

An overview of the performances of the classiﬁers for classiﬁcation target all malware and test mode unknown device
is shown in Table 10. This table shows high False Negative Rates (FNR) for all classiﬁers. Therefore, we do not
include the feature categories as in the previous sections, as little insight can be drawn from these poorly performing
models. Table 11 shows an overview of the performance per classiﬁer per malware type and feature set. Comparing
this Table with Table 8, we see the following:

9

Table 9: Best performing classiﬁers per malware type
(The numerical malware types in the ﬁrst column are those listed in Table 3.)

Malware
Best trained model

1

2

3

4

5

6

7

8

9

11

A PREPRINT - JULY 26, 2021

AdaBoost KNN

RF
Apps Apps Global Apps Apps Apps

KNN KNN

RF

RF

8
0.94

5
0.73

28
0.50

7
0.95

11
0.35

9
0.86

X
X

X
X

Classiﬁer
Feature set
Nr. features
F1 score
Feature categories used

Comb.
49
0.70

Battery
CPU
I/O Interrupts
Memory
Network trafﬁc
Storage
Wiﬁ
App CPU
App memory
App network trafﬁc
App process

X
X

X
X

X
X
X
X

X
X

X

X
X

X

X
X

X
X
X

X
X

X
X

X

AdaBoost
Comb.
14
1.00

RF

KNN
Comb. Global

10
0.94

1
0.94

X
X

X

X
X

X

1. All classiﬁers show lower performance for 7 out of 10 malware versions (versions 1-4 and 7-9) for test mode

unknown device, compared to test mode normal holdout.

2. AdaBoost and RF show better performance for malware version 5 (Phishing) for test mode unknown device,

compared to test mode normal holdout.

3. AdaBoost shows a perfect score (f1 score of 1) for detecting malware version 8 (Ransomware) for test mode

unknown device. For test mode normal holdout, AdaBoost showed a perfect score as well.

4. AdaBoost and KNN shows a better performance for malware version 11 (DOS) for test mode unknown device,

compared to test mode normal holdout.

Table 10: Comparison performance per classiﬁer for classiﬁcation target All malware and test mode unknown device

Classiﬁer
Feature set
Best model

Nr. features
Accuracy
F1 score
FPR
FNR

Global

AdaBoost
Apps

Comb.

Random Forest
Apps

Comb.

Global

29
0.923
0.207
0.005
0.878

28
0.944
0.583
0.013
0.528

44
0.926
0.563
0.042
0.423

28
0.888
0.249
0.052
0.776

6
0.918
0.562
0.057
0.362

8
0.925
0.561
0.043
0.423

Global

30
0.824
0.202
0.126
0.732

KNN
Apps

4
0.917
0.537
0.053
0.415

Comb.

11
0.910
0.522
0.061
0.407

4.3 Training and testing times

Table 12 shows the training and testing times per classiﬁer. This shows that training times of both AdaBoost and RF
are signiﬁcantly higher than for KNN (an algorithm which does no training). In contrast, testing times for KNN are
signiﬁcantly higher in comparison.

5 Discussion

Below a discussion on the main ﬁndings of this research is presented. Most related papers use a different evaluation
method making direct comparison difﬁcult. Nevertheless, we tried ordering the ﬁndings from good to bad, compared
to existing detection methods.

10

A PREPRINT - JULY 26, 2021

Table 11: Comparison performance per classiﬁer for classiﬁcation target single malware and test mode unknown device
(The numerical malware types in the ﬁrst column are those listed in Table 3.)

Malware

F1

AdaBoost
FPR

1
2
3
4
5
6
7
8
9
11

avg.
stdev

0.444
0.817
0.618
0.000
0.967
0.200
0.480
1.000
0.901
0.977

0.640
0.352

0.028
0.015
0.013
0.028
0.000
0.056
0.027
0.000
0.037
0.000

0.020
0.018

FNR

0.600
0.230
0.514
1.000
0.064
0.817
0.500
0.000
0.089
0.045

0.386
0.353

F1

0.364
0.861
0.712
0.000
0.978
0.245
0.583
0.000
0.889
0.930

0.556
0.380

Random Forest
FPR

0.056
0.014
0.013
0.000
0.000
0.037
0.019
0.000
0.037
0.010

0.018
0.019

FNR

0.600
0.164
0.400
1.000
0.043
0.802
0.417
1.000
0.111
0.091

0.463
0.372

F1

0.316
0.847
0.597
0.000
0.932
0.207
0.476
0.667
0.871
0.952

0.586
0.330

KNN
FPR

0.153
0.027
0.080
0.000
0.000
0.134
0.015
0.032
0.022
0.000

0.046
0.057

FNR

0.400
0.115
0.343
1.000
0.128
0.706
0.583
0.000
0.178
0.091

0.354
0.322

Table 12: Training and testing times per classiﬁer for classiﬁcation target all malware and test mode normal holdout

Classiﬁer
Feature set

Train (s)
Test (s)

Global

17
<1

AdaBoost
Apps

18
<1

Comb.

31
<1

Random Forest
Apps

Comb.

Global

<1
<1

30
<1

10.5
<1

Global

<1
2

KNN
Apps

<1
3

Comb.

<1
8

High performance RF, KNN, AdaBoost for classiﬁcation target single malware
In this research, RF and KNN
showed good results (F1 scores on average above 0.7) when separate models were trained per malware type. These
ﬁndings are in line with earlier studies that found good performances for RF [16, 17, 24] and KNN [20] on the detection
of mobile malware. None of the literature we found examined the AdaBoost classiﬁer, hence the good performance of
AdaBoost is a new ﬁnding.

AdaBoost is consistent with different test modes All classiﬁers, except AdaBoost, show worse performances when
tested on new devices (test mode unknown device), than if tested on the same devices (test mode normal holdout). Most
studies do not provide information on the test mode, thus test modes of other research cannot be compared to our study.
Our results suggest that AdaBoost may be more suitable when its purpose is to ﬁrst build a detection method with a
subset of users and subsequently use this detection method to protect other users. This approach is scalable: new users
can be protected with an already existing detection method. Otherwise, the model needs to be retrained constantly for
every new user. AdaBoost with the Apps feature set and test mode unknown device had an F1 score of 0.583, an FPR
of 0.013, and an FNR of 0.528. The high FNR calls for improvements in detecting malicious actions of new devices,
given the better scalability of this method. Some suggestions are given in Section 8.

Perfect score on Ransomware, but little data AdaBoost showed a perfect score for malware version 8 (Ran-
somware). The dataset used for training and testing contained only 170 records for malware version 8. Given the
low number of testing instances, it is hard to estimate whether this performance is the same on larger test sets.

High performance RF for classiﬁcation target all malware The best RF model with classiﬁcation target all mal-
ware used the Apps feature set, containing 29 app features, and had an F1 score of 0.730, an FPR of 0.013, and an
FNR of 0.346 (i.e., TPR of 0.654). Similar results (with no statistically signiﬁcant difference to the best RF model),
are achieved with the Combined feature set (F1 score of 0.722, an FPR of 0.009, and TPR of 0.620). A TPR of 0.62 is
relatively low compared to other studies on the dynamic detecting of mobile malware, which showed TPRs between
0.61 and 1 (see Section 2 for a complete comparison). However, in contrast to [15, 22, 24], with TPRs between 0.82
and 0.97, our study used data from real-life users instead of virtual environments for the training and testing of de-
tection methods. The use of real-life data for the training of models may have led to lower performance due to the
additional noise included in our dataset (see paragraph Overall high false-negative rate below).

Other studies using real devices for training and testing of models, based their performance on apps running isolated
for 10 minutes [15], devices in an idle state [21] or unknown circumstances [17, 18]. One study that used real devices

11

A PREPRINT - JULY 26, 2021

under real-life circumstances for the assessment of their detection method is [20]. In that study, the researchers created
a multi-level framework (called MADAM) that showed high performance (TPR 0.97, FPR 0.005). MADAM requires
root permissions as it used System Calls for the detection of malware. In contrast to MADAM, we do not require any
root permissions. Furthermore, MADAM used both dynamic and static features, while we used only dynamic features.
Lastly, MADAM is a detection method consisting of multiple architectural blocks that monitor different aspects of the
device, in contrast to this present study that does not consist of a complex architecture.

False-negative rate higher than false-positive rate
In this research, the FNR (undetected malicious actions) was
overall higher than the FPR (benign actions labelled as malicious). In other studies that we examined during our
literature research, the FPR is overall higher than the FNR. This may be due to the difference in the distribution of
malicious and benign data points. In our research, the dataset contained 90% benign data points and 10% malicious
data points. As most models are biased towards the majority class, the FNR is expected to be higher than the FPR in
our case. In other studies the distribution of malicious and benign data points used are often 50/50 or a majority of
malicious data points, resulting in equal FPRs and FNRs, or higher FPRs than FNRs. This stresses the importance of
presenting both FPR and FNR values in research, which was not done in all examined studies.

Testing times long for KNN The testing times for KNN were more than 3 seconds for both the Apps and Combined
feature set. This is based on our experimental setup which contained more computing power than the average smart-
phone. Given that in real-life, malicious or suspicious data should be tracked and noticed near real-time, long testing
times may not be practical or feasible. Additionally long testing times stress the battery more as long computations
are needed. Therefore, based on testing times, AdaBoost and RF may be preferable when models are running on
smartphones.

Low predictability on Spyware SMS and Adware Performance scores on the detection of malware versions 4
(Spyware SMS) and 6 (Adware) were low (F1 score below 0.5). The dataset used for training and testing contained
190 records for malware version 4. The low performance on the detection of Spyware SMS may, therefore, be due to
the small number of training instances. The training and testing dataset contained 7940 records for malware version
6. Therefore, the low performance is most likely not a result of insufﬁcient training instances, and more research is
needed to ﬁnd the cause for the low performance on detecting Adware.

Lower F1-scores compared to other state-of-the-art dynamic malware detectors Our best classiﬁer (RF) shows
an F1-score of 0.73. This is lower compared to state-of-the-art dynamic malware detectors such as Cai [27] and
Saracino [20]. In [27], the performance is high, but the results are taken from malware running in virtual environments
for only 10 minutes with pseudo-random user input from Monkey application. This may lower the conﬁdence that
the models would perform the same when evaluated on a real device with a real user. Additionally, Cai [27] uses
non-hardware features such as Method Calls for their detection method. Our paper focuses on hardware features only.
Our lower performance may indicate that using hardware features only result in lower overall performance. Saracino
[20] uses a combination of hardware features and non-hardware features (System Calls) for their detection methods.
As mentioned in Section 2, System Calls require root permissions which our detection method does require. However,
still the detection method by Saracino shows a higher performance, indicating again that a combination of hardware
and non-hardware features may increase the performance of dynamic malware detectors.

Overall high false-negative rate
In this research, all detection methods showed relatively high FNR (F1 score
above 0.3). This implies that a high number of malicious actions are undetected, and may have different causes. Many
features in the feature set are inﬂuenced by many factors, as the features describe devices that are running multiple
applications simultaneously. For instance, the priority, the CPU allocation, and the memory allocation of the malware
app depend on other applications running parallel to it. Therefore, the features in the dataset do not solely reﬂect the
(type of) action of the malware app, but also the state of the device at a given moment. This may result in excessive
noise in the sensed data. Another possible cause may the fact that the inﬂuences of malicious and benign actions on
the features sensed are similar, and indistinguishable to the classiﬁer.

6 Limitations

All detection methods in this research are created using the same dataset. The limitations caused by using this particular
dataset are listed below:

1. All devices in the dataset are Samsung Galaxy S5. Therefore, it is unknown how the models perform when

used on other devices.

12

A PREPRINT - JULY 26, 2021

2. All malware types in the dataset were written for the purpose of this research. This limits the ﬁndings of the
research due to two reasons. First: before the malware probe sent any data to a server, the data was scrambled
to ensure the privacy of the volunteers. It is possible that this scrambling inﬂuenced the features analysed in
the research. As a result, our detection models may be biased towards detecting scrambling actions, limiting
its efﬁcacy on wild malware. Second, it is unknown whether real malware executes the same way as custom
malware. Although the behaviour of custom malware is based on wild malware, its implementation may
differ. It is therefore unknown whether similar detection performance is to be expected on wild malware.
3. The dataset contained a low number of data points for malware versions 4 and 6. This limits the conclusions

drawn from their results.

4. The dataset did not contain any information on the creation times of the malware apps. Therefore, the sus-
tainability metric, as mentioned in recent works [41][42], could not be included in the analysis. The papers
argue argue that detection methods should be evaluated by taking creation time of apps into considerations.
They argue that detection methods should be trained on older apps (e.g. 2018) and tested on newer apps (e.g.
2019). This way detection methods are evaluated on their ability to detect unseen behaviour from newer apps.

The detection methods of this research share characteristics that may limit their performance or applicability. The
limitations imposed by this are listed below:

1. All detection methods in this research are signature-based, i.e., identify malware based on a pattern of be-
haviour. In this research, the signature of a malicious action is a record of feature values from a some given
feature set. This signature may be different for other (wild) malware types. Therefore, it is unknown whether
similar performances are achieved on other (wild) malware types or other versions of the same malware type.

2. All detection methods in this research use RFCV as a feature-selection method. This method allows for

feature reduction but is a greedy-search strategy, so may be sub-optimal.

3. All detection methods using Apps features require data collection of all applications running. In our research,
we discarded data from other applications, as we knew which application was malicious. However, in real-life
circumstances, the malware application is unknown. Therefore, although Apps features show better perfor-
mance than Global features, they are more resource-intensive to monitor than Global features. A suggestion
for this issue is described in 8.

7 Conclusion

This research showed i) what machine-learning classiﬁer is most suitable for detecting Mobile Trojans, and ii) which
aspects of a smartphone (features), related to hardware, are most important in detecting Mobile Trojans. The Random
Forest classiﬁer showed the highest performance in F1-score, compared with AdaBoost and KNN, when one model is
used that is trained on many types of Mobile Trojans. This classiﬁer achieves an F1-score of 0.73 with an FPR of 0.009
and an FNR of 0.380. Random Forest, AdaBoost and K-Nearest Neighbour show high performances when separate
models are trained on each type of Mobile Trojan with an average F1-score above 0.72, FPR below 0.02 and FNR
below 0.33. Additionally, on 5 malware types (Spyware, Phishing, Ransomware, Privilege Escalation, DOS) we show
high performances (F1-score above 0.963 and FNR below 0.120) making these models relevant for future detection
methods that need to detect these malware types.

Multiple dynamic hardware feature sets are examined in this research, making a distinction between global device
features and features related to an application. When using one model for the detection of multiple types of Mobile
Trojans, 10 app features related to the memory usage, process information, and CPU usage of the app, are sufﬁcient
for detecting malicious actions. Additionally, app features, in general, showed the best performance for detecting 7
out of 10 malware versions, compared to global device features.

Recent related work performed their research in lab-like environments with models based on emulators, simulated
user input, and/or short testing durations. To create models that can detect mobile malware threats in real life, we need
models assessed on data of real devices rather than data of lab-like environments. This research tried to ﬁll this gap by
using data of real devices gathered over multiple years with malware based on real malware.

8 Future work

Some suggestion for future research are listed below:

13

A PREPRINT - JULY 26, 2021

1. Focus on improving False Negative Rates as this research’s detection methods show relatively high FNR. As
Random Forest and AdaBoost showed relative high performance, other (boosted) ensemble classiﬁers can be
examined such as GradientBoosting [43].

2. Improve the performance of detection methods on new devices and device models if new data becomes

available.

3. Improve dynamic detection methods by analysing real wild malware samples to improve the efﬁcacy of the
methods on the detection of real malware to overcome the limitation imposed by the use of self-written
malware.

4. Increase the sample size to improve the efﬁcacy and applicability of the methods. This research analysed 10

different samples of Mobile Trojans.

5. Examine whether time series analysis may improve the detection methods. This research’s detection methods
analyse dynamic features by individually assessing these values, without considering these value prior in time.
Therefore, the absolute values of features are analysed. With time series analysis, relative values can be used
which may improve the detection methods, as is suggested by the results of [22].

6. Optimize the resource requirements of detection methods using App features. As described in Section 5,
monitoring App features of all applications installed on a device is resource-intensive. Research to decrease
these resource requirements is needed. A suggestion may be to only monitor applications that require sensitive
permissions, e.g. access to SD card or access to contacts, as was implemented in the framework of [20].
7. Improve proposed detection methods by taking sustainability as an additional metric for detectors. Recent
works [41][42] have argued that detection methods should be evaluated by taking creation time of apps into
considerations. They argue that detection methods should be trained on older apps (e.g. 2018) and tested on
newer apps (e.g. 2019). This way detection methods are evaluated on their ability to detect unseen behaviour
from newer apps. The malware applications of our research did not contain information on the creation time,
hence this could not be evaluated.

9 Acknowledgements

The authors would like to thank the Ben-Gurion University, for providing this research’s dataset.

References

[1] Dea. Smartphone subscriptions worldwide 2016-2026. 2021. Accessed at: 2021-07.
[2] Kimberly Tam, Ali Feizollah, Nor Badrul Anuar, Rosli Salleh, and Lorenzo Cavallaro. The evolution of Android

malware and Android analysis techniques. ACM Computing Surveys (CSUR), 49(4):76, 2017.

[3] Ping Yan and Zheng Yan. A survey on dynamic mobile malware detection. Software Quality Journal, pages

1–29, 2017.

[4] Yisroel Mirsky, Asaf Shabtai, Lior Rokach, Bracha Shapira, and Yuval Elovici. Sherlock vs moriarty: A smart-
phone dataset for cybersecurity research. In Proceedings of the 2016 ACM workshop on Artiﬁcial intelligence
and security, pages 1–12. ACM, 2016.

[5] Google. Android security 2016 year in review. Technical report. Accessed at: 2017-11.
[6] Kaspersky. It threat evolution q1 2017. Technical report. Accessed at: 2017-11.
[7] Asaf Shabtai, Uri Kanonov, Yuval Elovici, Chanan Glezer, and Yael Weiss. Andromaly: a behavioral malware
detection framework for Android devices. Journal of Intelligent Information Systems, 38(1):161–190, 2012.
[8] Kevin Allix, Tegawendé F Bissyandé, Jacques Klein, and Yves Le Traon. Androzoo: Collecting millions of
Android apps for the research community. In 2016 IEEE/ACM 13th Working Conference on Mining Software
Repositories (MSR), pages 468–471. IEEE, 2016.

[9] Iker Burguera, Urko Zurutuza, and Simin Nadjm-Tehrani. Crowdroid: behavior-based malware detection system
In Proceedings of the 1st ACM workshop on Security and privacy in smartphones and mobile

for Android.
devices, pages 15–26. ACM, 2011.

[10] Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo Gascon, Konrad Rieck, and CERT Siemens. DREBIN:

Effective and explainable detection of Android malware in your pocket. In NDSS, 2014.

[11] Malware genome project. Accessed at: 2017-10.
[12] Open malware dataset. Technical report.
[13] Virusshare. Accessed at: 2017-10.

14

A PREPRINT - JULY 26, 2021

[14] Virustotal database. Unaccessible at: 2017-10.
[15] B. Amos, H. Turner, and J. White. Applying machine learning classiﬁers to dynamic Android malware detection
In 2013 9th International Wireless Communications and Mobile Computing Conference (IWCMC),

at scale.
pages 1666–1671, July 2013.

[16] M. S. Alam and S. T. Vuong. Random Forest Classiﬁcation for Detecting Android Malware. In 2013 IEEE
International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE
Cyber, Physical and Social Computing, pages 663–669, August 2013.

[17] Hyo-Sik Ham and Mi-Jung Choi. Analysis of Android malware detection performance using machine learning
classiﬁers. In ICT Convergence (ICTC), 2013 International Conference on, pages 490–495. IEEE, 2013.
[18] A. E. Attar, R. Khatoun, and M. Lemercier. A Gaussian mixture model for dynamic detection of abnormal
behavior in smartphone applications. In 2014 Global Information Infrastructure and Networking Symposium
(GIIS), pages 1–6, September 2014.

[19] Bryan Dixon, Shivakant Mishra, and Jeannette Pepin. Time and location power based malicious code detection
In Network Computing and Applications (NCA), 2014 IEEE 13th International

techniques for smartphones.
Symposium on, pages 261–268. IEEE, 2014.

[20] Andrea Saracino, Daniele Sgandurra, Gianluca Dini, and Fabio Martinelli. MADAM: Effective and efﬁcient
behavior-based Android malware detection and prevention. IEEE Transactions on Dependable and Secure Com-
puting, 2016.

[21] L. Caviglione, M. Gaggero, J. F. Lalande, W. Mazurczyk, and M. Urbanski. Seeing the Unseen: Revealing Mo-
bile Malware Hidden Communications via Energy Consumption and Artiﬁcial Intelligence. IEEE Transactions
on Information Forensics and Security, 11(4):799–810, April 2016.

[22] Jelena Milosevic, Alberto Ferrante, and Miroslaw Malek. Malaware: Effective and efﬁcient run-time mobile
malware detector. In Dependable, Autonomic and Secure Computing, 14th Intl Conf on Pervasive Intelligence
and Computing, 2nd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology
Congress (DASC/PiCom/DataCom/CyberSciTech), 2016 IEEE 14th Intl C, pages 270–277. IEEE, 2016.

[23] Alberto Ferrante, Eric Medvet, Francesco Mercaldo, Jelena Milosevic, and Corrado Aaron Visaggio. Spotting
the malicious moment: Characterizing malware behavior using dynamic features. In Availability, Reliability and
Security (ARES), 2016 11th International Conference on, pages 372–381. IEEE, 2016.

[24] Gerardo Canfora, Eric Medvet, Francesco Mercaldo, and Corrado Aaron Visaggio. Acquiring and analyzing app
metrics for effective mobile malware detection. In Proceedings of the 2016 ACM on International Workshop on
Security And Privacy Analytics, pages 50–57. ACM, 2016.

[25] L. Massarelli, L. Aniello, C. Ciccotelli, L. Querzoni, D. Ucci, and R. Baldoni. Android malware family clas-
siﬁcation based on resource consumption over time. In 2017 12th International Conference on Malicious and
Unwanted Software (MALWARE), pages 31–38, 2017.

[26] Sarah Wassermann and Pedro Casas. Bigmomal: Big data analytics for mobile malware detection. In Proceedings

of the 2018 Workshop on Trafﬁc Measurements for Cybersecurity, pages 33–39. ACM, 2018.

[27] Haipeng Cai, Na Meng, Barbara Ryder, and Daphne Yao. Droidcat: Effective Android malware detection and
categorization via app-level proﬁling. IEEE Transactions on Information Forensics and Security, 14(6):1455–
1470, 2018.

[28] Alejandro Martín, Víctor Rodríguez-Fernández, and David Camacho. CANDYMAN: Classifying Android mal-
ware families by modelling dynamic traces with markov chains. Engineering Applications of Artiﬁcial Intelli-
gence, 74:121–133, 2018.

[29] Yusheng Dai, Hui Li, Yekui Qian, Ruipeng Yang, and Min Zheng. Smash: a malware detection method based on

multi-feature ensemble learning. IEEE Access, 7:112588–112597, 2019.

[30] L U Memon, N Z Bawany, and J A Shamsi. A comparison of machine learning techniques for Android malware

detection using apache spark. Journal of Engineering Science and Technology, 14(3):1572–1586, 2019.

[31] Mohammed K Alzaylaee, Suleiman Y Yerima, and Sakir Sezer. DL-Droid: Deep learning based Android mal-

ware detection using real devices. Computers & Security, 89:101663, 2020.

[32] Gianluca Dini, Fabio Martinelli, Andrea Saracino, and Daniele Sgandurra. MADAM: a multi-level anomaly

detector for Android malware. Computer network security, pages 240–253, 2012.

[33] Contagio. Accessed at: 2017-10.
[34] Haipeng Cai and Barbara G Ryder. A longitudinal study of application structure and behaviors in android. IEEE

Transactions on Software Engineering, 2020.

15

A PREPRINT - JULY 26, 2021

[35] Google funf framework. Accessed at: 2020-05.
[36] Github sherlock. Accessed at: 2020-05.
[37] Sumeet Dua and Xian Du. Data mining and machine learning in cybersecurity. CRC press, 2016.
[38] Auto-scaling sci-kit learn with apache spark. Technical report. Accessed at: 2018-07.
[39] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.

[40] Thomas G Dietterich. Approximate statistical tests for comparing supervised classiﬁcation learning algorithms.

Neural computation, 10(7):1895–1923, 1998.

[41] HAIPENG CAI. Assessing and improving malware detection sustainability through app evolution studies. 2019.

[42] Lucky Onwuzurike, Enrico Mariconti, Panagiotis Andriotis, Emiliano De Cristofaro, Gordon Ross, and Gianluca
Stringhini. MaMaDroid: Detecting Android malware by building Markov chains of behavioral models (extended
version). ACM Transactions on Privacy and Security (TOPS), 22(2):1–34, 2019.

[43] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages

1189–1232, 2001.

16

A PREPRINT - JULY 26, 2021

10 Appendix

Table 13: All features in different feature sets (for the meaning of each feature, see the documentation of the original
dataset [4])

Global

Apps

Combined

Global plus Apps

userid
uuid
version
trafﬁc_mobilerxbytes
trafﬁc_mobilerxpackets
trafﬁc_mobiletxbytes
trafﬁc_mobiletxpackets
trafﬁc_totalrxbytes
trafﬁc_totalrxpackets
trafﬁc_totaltxbytes
trafﬁc_totaltxpackets
trafﬁc_totalwiﬁrxbytes
trafﬁc_totalwiﬁrxpackets
trafﬁc_totalwiﬁtxbytes
trafﬁc_totalwiﬁtxpackets
trafﬁc_timestamp
battery_charge_type
battery_current_avg
battery_health
battery_icon_small
battery_invalid_charger
battery_level
battery_online
battery_plugged
battery_present
battery_scale
battery_status
battery_technology
battery_temperature
battery_timestamp
battery_voltage
cpuhertz
cpu_0
cpu_1
cpu_2
cpu_3
total_cpu
totalmemory_freesize
totalmemory_max_size
totalmemory_total_size
totalmemory_used_size

userid
uuid
applicationname
cpu_usage
packagename
packageuid
uidrxbytes
uidrxpackets
uidtxbytes
uidtxpackets
cguest_time
cmaj_ﬂt
cstime
cutime
dalvikprivatedirty
dalvikpss
dalvikshareddirty
guest_time
importance
importancereasoncode
importancereasonpid
lru
nativeprivatedirty
nativepss
nativeshareddirty
num_threads
otherprivatedirty
otherpss
othershareddirty
pgid
pid
ppid
priority
rss
rsslim
sid
start_time
state
stime
tcomm
utime

17

A PREPRINT - JULY 26, 2021

Table 14: Features included in best performing Random Forest models (classiﬁcation target: all malware), ordered by
decreasing feature importance (for the meaning of each feature, see the documentation of the original dataset [4])

Global

Apps

Combined

dalvikprivatedirty_mor_app
dalvikpss_mor_app
importance_mor_app
num_threads_mor_app
otherprivatedirty_mor_app
otherpss_mor_app
rss_mor_app
stime_mor_app
utime_mor_app
vsize_mor_app

trafﬁc_mobilerxbytes
trafﬁc_mobiletxbytes
trafﬁc_mobiletxpackets
trafﬁc_totalrxbytes
trafﬁc_totalrxpackets
trafﬁc_totaltxbytes
trafﬁc_totaltxpackets
trafﬁc_totalwiﬁrxbytes
trafﬁc_totalwiﬁtxbytes
trafﬁc_totalwiﬁtxpackets
battery_current_avg
battery_icon_small
battery_level
battery_temperature
battery_voltage
cpu_0
cpu_1
cpu_2
cpu_3
total_cpu
totalmemory_freesize
totalmemory_max_size
totalmemory_total_size
totalmemory_used_size

cpu_usage_mor_app
uidrxbytes_mor_app
uidrxpackets_mor_app
uidtxbytes_mor_app
uidtxpackets_mor_app
cmaj_ﬂt_mor_app
cstime_mor_app
dalvikprivatedirty_mor_app
dalvikpss_mor_app
dalvikshareddirty_mor_app
importance_mor_app
importancereasonpid_mor_app
lru_mor_app
nativeprivatedirty_mor_app
nativepss_mor_app
nativeshareddirty_mor_app
num_threads_mor_app
otherprivatedirty_mor_app
otherpss_mor_app
othershareddirty_mor_app
pgid_mor_app
pid_mor_app
ppid_mor_app
priority_mor_app
rss_mor_app
start_time_mor_app
stime_mor_app
utime_mor_app
vsize_mor_app

18

A PREPRINT - JULY 26, 2021

Table 15: Features included in best performing AdaBoost models (classiﬁcation target = all malware) ordered per
feature importance (for the meaning of each feature, see the documentation of the original dataset [4])

Global

Apps

Combined

totalmemory_total_size
totalmemory_used_size
battery_voltage
battery_temperature
trafﬁc_totaltxbytes
total_cpu
totalmemory_freesize
trafﬁc_totalrxbytes
battery_level
cpu_0
trafﬁc_totaltxpackets
cpu_2
cpu_3
trafﬁc_totalwiﬁtxbytes
battery_current_avg
trafﬁc_totalrxpackets
cpu_1
trafﬁc_mobilerxbytes
trafﬁc_totalwiﬁrxbytes
trafﬁc_mobiletxbytes
totalmemory_max_size
trafﬁc_totalwiﬁtxpackets
battery_icon_small
trafﬁc_mobiletxpackets
trafﬁc_totalwiﬁrxpackets
trafﬁc_mobilerxpackets

rss_mor_app
utime_mor_app
otherprivatedirty_mor_app
dalvikprivatedirty_mor_app
importance_mor_app
otherpss_mor_app
dalvikpss_mor_app
num_threads_mor_app
vsize_mor_app
stime_mor_app
dalvikshareddirty_mor_app
cpu_usage_mor_app
uidtxbytes_mor_app
othershareddirty_mor_app
start_time_mor_app
pid_mor_app
uidrxbytes_mor_app
pgid_mor_app
lru_mor_app
uidtxpackets_mor_app
ppid_mor_app
nativeprivatedirty_mor_app
priority_mor_app
uidrxpackets_mor_app
cstime_mor_app

rss_mor_app
utime_mor_app
dalvikpss_mor_app
otherprivatedirty_mor_app
importance_mor_app
otherpss_mor_app
dalvikprivatedirty_mor_app
num_threads_mor_app
vsize_mor_app
stime_mor_app
trafﬁc_totalrxbytes
dalvikshareddirty_mor_app
totalmemory_used_size
cpu_usage_mor_app
othershareddirty_mor_app
total_cpu
uidtxbytes_mor_app
totalmemory_freesize
trafﬁc_totaltxbytes
start_time_mor_app
battery_voltage
totalmemory_total_size
battery_temperature
pid_mor_app
uidrxbytes_mor_app
ppid_mor_app
battery_level
cpu_0
totalmemory_max_size
trafﬁc_totalwiﬁtxbytes
trafﬁc_totaltxpackets
cpu_2
cpu_3
lru_mor_app
uidtxpackets_mor_app
nativeprivatedirty_mor_app
priority_mor_app
cpu_1
trafﬁc_totalrxpackets
pgid_mor_app
trafﬁc_mobiletxbytes
cstime_mor_app
trafﬁc_totalwiﬁrxbytes

19

A PREPRINT - JULY 26, 2021

Table 16: Features included in best performing KNN models (classiﬁcation target = all malware) ordered per feature
importance (for the meaning of each feature, see the documentation of the original dataset [4])

Global

Apps

Combined

rss_mor_app
utime_mor_app
dalvikpss_mor_app
otherprivatedirty_mor_app
importance_mor_app
otherpss_mor_app
dalvikprivatedirty_mor_app
num_threads_mor_app
vsize_mor_app

rss_mor_app
utime_mor_app
otherprivatedirty_mor_app
dalvikprivatedirty_mor_app
importance_mor_app
otherpss_mor_app
dalvikpss_mor_app
num_threads_mor_app
vsize_mor_app
stime_mor_app
dalvikshareddirty_mor_app
cpu_usage_mor_app
uidtxbytes_mor_app

totalmemory_total_size
totalmemory_used_size
battery_voltage
battery_temperature
trafﬁc_totaltxbytes
total_cpu
totalmemory_freesize
trafﬁc_totalrxbytes
battery_level
cpu_0
trafﬁc_totaltxpackets
cpu_2
cpu_3
trafﬁc_totalwiﬁtxbytes
battery_current_avg
trafﬁc_totalrxpackets
cpu_1
trafﬁc_mobilerxbytes
trafﬁc_totalwiﬁrxbytes
trafﬁc_mobiletxbytes
totalmemory_max_size
trafﬁc_totalwiﬁtxpackets
battery_icon_small
trafﬁc_mobiletxpackets
trafﬁc_totalwiﬁrxpackets
trafﬁc_mobilerxpackets

20

A PREPRINT - JULY 26, 2021

Table 17: All features and their feature category (part 1 of 2; for the meaning of each feature, see the documentation
of the original dataset [4])

Feature

Category

Feature

App_CPU
cguest_time
App_CPU
cpu_usage
App_CPU
cstime
App_CPU
cutime
App_CPU
guest_time
App_CPU
Itrealvalue
App_CPU
nice
App_CPU
num_threads
App_CPU
priority
App_CPU
Processor
App_CPU
rt_priority
App_CPU
stime
App_CPU
utime
App_Info
packagename
App_Info
packageuid
App_Info
version_code
App_Info
version_name
App_Memory
cmaj_ﬂt
App_Memory
cminﬂt
App_Memory
dalvikprivatedirty
App_Memory
dalvikpss
App_Memory
dalvikshareddirty
App_Memory
lru
App_Memory
majﬂt
App_Memory
minﬂt
App_Memory
nativeprivatedirty
App_Memory
nativepss
App_Memory
nativeshareddirty
App_Memory
otherprivatedirty
App_Memory
otherpss
App_Memory
othershareddirty
App_Memory
rss
App_Memory
rsslim
App_Memory
vsize
App_Network
uidrxbytes
App_Network
uidrxpackets
App_Network
uidtxbytes
App_Network
uidtxpackets
App_Process
endcode
App_Process
exit_signal
App_Process
Flags
importance
App_Process
importancereasoncode App_Process
App_Process
importancereasonpid
App_Process
pgid
App_Process
pid

ppid
sid
start_time
startcode
state
tcomm
tgpid
Wchan
battery_charge_type
battery_current_avg
battery_health
battery_icon_small
battery_invalid_charger
battery_level
battery_online
battery_plugged
battery_present
battery_scale
battery_status
battery_technology
battery_temperature
battery_timestamp
battery_voltage
btime
cpu_0
cpu_1
cpu_2
cpu_3
cpuhertz
ctxt
processes
procs_blocked
procs_running
tot_idle
tot_iowait
tot_irq
tot_nice
tot_softirq
tot_system
tot_user
total_cpu
companion_cpu0
companion_sum_cpu123
cpu123_intr_prs
cypress_touchkey_cpu0
cypress_touchkey_sum_cpu123

21

Category

App_Process
App_Process
App_Process
App_Process
App_Process
App_Process
App_Process
App_Process
Battery
Battery
Battery
Battery
Battery
Battery
Battery
Battery
Battery
Battery
Battery
Battery
Battery
Battery
Battery
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts

A PREPRINT - JULY 26, 2021

Table 18: All features and their feature category (part 2 of 2, for the meaning of each feature, see the documentation
of the original dataset [4])

Feature

Category

Feature

Category

ﬂip_cover_cpu0
ﬂip_cover_sum_cpu123
function_call_interrupts_cpu0
function_call_interrupts_sum_cpu123
home_key_cpu0
home_key_sum_cpu123
msmgpio_cpu0
msmgpio_sum_cpu123
pn547_cpu0
pn547_sum_cpu123
sec_headset_detect_cpu0
sec_headset_detect_sum_cpu123
slimbus_cpu0
slimbus_sum_cpu123
synaptics_rmi4_i2c_cpu0
synaptics_rmi4_i2c_sum_cpu123
volume_down_cpu0
volume_down_sum_cpu123
volume_up_cpu0
volume_up_sum_cpu123
wcd9xxx_cpu0
wcd9xxx_sum_cpu123
active
active_anon
active_ﬁle
anonpages
buffers
cached
commitlimit
committed_as
dirty
highfree
hightotal
inactive
inactive_anon
inactive_ﬁle
kernelstack
lowfree
lowtotal
mapped
Memfree
memtotal
mlocked
pagetables
shmem
slab

IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
IO Interrupts
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory
Memory

Memory
sreclaimable
Memory
sunreclaim
Memory
swapcached
Memory
swapfree
Memory
swaptotal
Memory
totalmemory_freesize
totalmemory_max_size Memory
totalmemory_total_size Memory
totalmemory_used_size Memory
Memory
unevictable
Memory
vmallocchunk
Memory
vmalloctotal
Memory
vmallocused
Memory
writeback
Metadata
applicationname
Metadata
sherlock_version
Metadata
userid
Metadata
uuid
Network
trafﬁc_mobilerxpackets
Network
trafﬁc_mobiletxbytes
Network
trafﬁc_mobiletxpackets
Network
trafﬁc_timestamp
Network
trafﬁc_totalrxbytes
Network
trafﬁc_totalrxpackets
Network
trafﬁc_totaltxbytes
Network
trafﬁc_totaltxpackets
trafﬁc_totalwiﬁrxbytes
Network
trafﬁc_totalwiﬁrxpackets Network
Network
trafﬁc_totalwiﬁtxbytes
trafﬁc_totalwiﬁtxpackets Network
Storage
external_availableblocks
Storage
external_availablebytes
Storage
external_blockcount
Storage
external_blocksize
Storage
external_freeblocks
Storage
external_freebytes
Storage
external_totalbytes
Storage
internal_availableblocks
Storage
internal_availablebytes
Storage
internal_blockcount
Storage
internal_blocksize
Storage
internal_freeblocks
Storage
internal_freebytes
Storage
internal_totalbytes
Wiﬁ
connectedwiﬁ_level
Wiﬁ
connectedwiﬁ_ssid

22

