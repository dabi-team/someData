1

Temporal Ordered Clustering in Dynamic
Networks: Unsupervised and Semi-supervised
Learning Algorithms

Krzysztof Turowski†, Jithin K. Sreedharan†, and Wojciech Szpankowski, Fellow, IEEE

0
2
0
2

g
u
A
6

]
I
S
.
s
c
[

4
v
2
7
6
0
0
.
5
0
9
1
:
v
i
X
r
a

Abstract—In temporal ordered clustering, given a single snap-
shot of a dynamic network in which nodes arrive at distinct time
instants, we aim at partitioning its nodes into K ordered clusters
C1 ≺ · · · ≺ CK such that for i < j, nodes in cluster Ci arrived
before nodes in cluster Cj , with K being a data-driven parameter
and not known upfront. Such a problem is of considerable
signiﬁcance in many applications ranging from tracking the
expansion of fake news to mapping the spread of information.
We ﬁrst formulate our problem for a general dynamic graph,
and propose an integer programming framework that ﬁnds the
optimal clustering, represented as a strict partial order set,
achieving the best precision (i.e., fraction of successfully ordered
node pairs) for a ﬁxed density (i.e., fraction of comparable
node pairs). We then develop a sequential importance procedure
and design unsupervised and semi-supervised algorithms to
ﬁnd temporal ordered clusters that efﬁciently approximate the
optimal solution. To illustrate the techniques, we apply our
methods to the vertex copying (duplication-divergence) model
which exhibits some edge-case challenges in inferring the clusters
as compared to other network models. Finally, we validate the
performance of the proposed algorithms on synthetic and real-
world networks.

Index Terms—Clustering, dynamic networks, unsupervised

learning, semi-supervised learning, temporal order

I. INTRODUCTION

The clustering of nodes is a classic problem in networks.
In its typical form in static networks, it ﬁnds communities
where methods like spectral clustering, modularity maximiza-
tion, minimum-cut method, and hierarchical clustering are
commonly used [1].

However, in dynamic networks that grow over time with
nodes or edges getting added or deleted, a criteria of clustering
based on its temporal characteristics ﬁnds signiﬁcant relevance
in practice since it helps us to study the existence of certain
network structures and their future behavior. One approach to
reason about the history of dynamic networks via clustering
is guided by the problem of node labeling according to their
arrival order when only the structure of the ﬁnal snapshot of

K. Turowski is with the Theoretical Computer Science Department, Jagiel-

lonian University, Krakow, Poland.
E-mail: krzysztof.szymon.turowski@gmail.com.

J. K. Sreedharan and W. Szpankowski are with the Dept. of Computer
Science and the NSF Center for Science and Information, Purdue University,
West Lafayette, IN 47907, U.S.A.
E-mail: {jithinks, szpan}@purdue.edu.

† Both the authors contributed equally to this research.
This work was supported by NSF Center for Science of Information
(CSoI) Grant CCF-0939370, and in addition by NSF Grants CCF-1524312,
CCF-2006440, and CCF-2007238, National Science Center Grant UMO-
2016/21/B/ST6/03146 and Google Research Award.

the network is provided. The availability of merely structure
means that either we are given an unlabeled graph or the
current node labels do not present any historical information.
As it turns out, in many real-world networks and graph models,
it is impossible to ﬁnd a complete order of arrival of nodes
due to a large number of symmetries inherent in the graph [2],
[3]. Figure 1 shows an example. In such cases, it is essential to
classify nodes that are indistinguishable themselves in terms
of arrival order into clusters {Ci }. Furthermore, the formed
clusters also will be ordered as C1 ≺ C2 ≺ · · · so that for any
i < j, all the nodes in the cluster Ci are estimated to be arrived
earlier than all the nodes in the cluster Cj, and all the nodes
inside each cluster are considered to be identical in arrival
order. We call such a clustering scheme as temporal ordered
clustering.

Fig. 1: Example showing how temporal clustering arises: a) the input
graph without labels. b) and c) arbitrary labellings of arrival order
with 1 representing the earliest arrival and 8 for the latest arrival. In
b) and c), the last two arrived nodes 7 and 8 have the same set of
neighbors. If we simulate the process of evolution starting from node
1, we observe that graphs in b) and c) at time 7 (i.e., with nodes 1−7)
are identical. Thus, nodes 7 and 8 in b) and c) are indistinguishable
as to which arrived early between them (this observation holds for
any labeling on the input graph), and the nodes behind these two
labels are part of a temporal cluster.

Temporal ordered clustering is related to many applications
in practice. For example in online social networks, it can be
useful to disseminate speciﬁc information or advertisements
targeted at nodes that arrived around the same time. In
biological networks, it identiﬁes the evolution of biomolecules
in the network and helps in predicting early proteins that are
known to be preferentially implicated in cancers and other
diseases [4]. In rumor or epidemic networks, temporal ordered
clustering can assist in identifying the sources and carriers of
false information.

Our contributions.

• We provide a general framework and derive an optimization
problem for ﬁnding temporal ordered clusters in dynamic
networks when only the ﬁnal snapshot of its evolution is
provided. Due to high computational complexity involved

3546218735462178abc 
 
 
 
 
 
in solving it, we reformulate the problem in terms of partial
orders – for any node pairs (u, v), a partial order σ deﬁnes
an order u <σ v in which node u is speciﬁed to be
arrived earlier than node v. Such a partial order naturally
translates into clusters of nodes and introduces an order
among them. Both the optimization problems depend on
the knowledge of the probabilistic evolution of the graph
model and the probability that any node u is older than
any other node v, denoted as pu,v. We then design a
sequential importance sampling algorithm to estimate pu,v
for any general graph model, and prove its convergence. The
solution to a linear programming relaxation of the original
optimization problem, with coefﬁcients as estimated pu,v,
presents an upper bound on the clustering quality.

• In the case of large networks, when the complexity for
solving the original optimization is higher, approximate
solutions, which directly make use of the estimated pu,vs,
are developed. Moreover, when some information about the
node-pair orders are available, we develop semi-supervised
the graph structure to improve
techniques that exploit
estimation precision. We observe that
the use of semi-
supervised learning enhances the estimated values of pu,v
quickly even with a small percentage of labeled data.

• In the second part of the paper, as an application of
the proposed general technique, we focus on duplication-
divergence or vertex copying dynamic network model (DD-
model) in which, informally, a new node copies the edges
of a randomly selected existing node and retains them with
a certain probability, and also makes random connections
to the remaining nodes (see Section V-A for details). The
DD-model poses unique challenges for temporal ordered
clustering in comparison with other graph models because
of the features listed below:
– Non-equiprobable large number of permutations: In many
of the graph models including the preferential attachment
and Erd˝os-Rényi graph models, all the feasible permu-
tations of the same structure representing node arrival
orders are equally likely [2]. Later in the paper, we
show with a counter example that this is not the case
in the DD-model. In other words, unlike in our previous
work [5], we do not assume the isomorphic graphs that
have positive probability under the graph model have
the same probability. Moreover, in the DD-model, all
the permutations of node labels with n letters are valid
unlike some models like preferential attachment model
and hence the effective space of total orderings is n!.
Thus the DD-model stands as corner case in the problem
of node arrival order inference.

– Large number of symmetry: We provide evidence of
a large number of automorphisms in a duplication-
divergence graph, whereas it is known that Erd˝os-Rényi
and preferential attachment graphs are asymmetric (when
the automorphism group contains only the identity per-
mutation) with high probability [6], [2].

– Ineffectiveness of degree-based techniques: In some mod-
els (including preferential attachment model), the oldest
nodes have larger expected degrees than the youngest

2

nodes over time, with high probability. But it is known
that in the DD-model the average degree does not exhibit
such a consistent trend [7], [8]. Thus any method based
on degrees is bound to fail in the DD-model.

Prior related work. Graph clustering is a well-studied prob-
lem which, in general, follows two main approaches: 1) deﬁne
a similarity metric between node pairs, and choose clusters so
as to maximize similarity among the nodes inside a cluster
and minimize similarity between nodes in different clusters; 2)
identify subgraphs within the input graph that reach a certain
value of ﬁtness measure, usually based on subgraph density,
conductance, normalized cut or sparse cut [1]. Many of the
clustering techniques on static graphs have been extended to
dynamic graphs, where primarily the aim was to study the
evolution of ﬁtness or similarity based clusters [9], [10], [11],
[12].

The temporal ordered clustering or partial order inference
considered in this paper poses a very different problem in
contrast to the classical formulation. The optimization criterion
for temporal ordered clustering introduces a fresh look taking
into account the graph model and its temporal behavior (see
Section III). The main aim of our clustering formulation is
to characterize the inherent limits and to develop estimation
algorithms for recovering the history of a dynamic network.
The nodes inside our clusters are indistinguishable in terms
of their arrival order due to symmetries in the input graph
and there exists a hierarchy or order among the clusters with
respect to graph evolution.

Previous works on semi-supervised clustering methods for
data represented as vectors [13], [14] and their extensions to
graphs [15] focus mainly on using the labeled nodes to deﬁne
clusters and their centroids. However, in temporal ordered
clustering, the labeled nodes need not fully represent all the
clusters, and they are used to reduce the complexity of estima-
tion of coefﬁcients of the associated linear programming (by
restricting the sampling distribution of importance sampling,
see Section IV-B)

Node arrival order in the DD-model has been studied in
[16] and [17], and the references therein. Most of the prior
works focus on getting the complete arrival order of nodes
(total order), but it turns out that it becomes nearly impossible
due to their symmetries [2], [3]. Instead of total order, in this
work we focus on deriving an optimal partial order of nodes
of nodes (see Section II). Our methods are general and are
applicable to a wide class of graph models, unlike our recent
work [5] where the methods were speciﬁc to the preferential
attachment model and not extendable.

A preliminary version of this paper is appeared in [18].

II. PROBLEM FORMULATION
Let Hn be the observed undirected and unweighted graph
of n nodes with V(Hn) being the set of vertices and E(Hn)
being the set of edges. The graph Hn is a result of evolution
over time, starting from a seed graph Hn0 with n0 nodes. At a
time instant k, when a new node appears, a set of new edges
adjacent to the new node is added, and the graph Hk will
evolve into Hk+1. Since the change in graph structure occurs

only when a new node is added, assuming the addition of a
node as a time epoch, Hn also represents graph at time epoch
n. The time epoch n0 denotes the creation of the seed graph
Gn0

1.
Given only the snapshot of the dynamic graph Hn at time n,
we usually do not know the time or order of arrivals of nodes.
Essentially, our goal is to label each node with a number i, 1 ≤
i ≤ K, such that all the nodes labeled by i arrived before nodes
with labels j where j > i. The number of labels (clusters)
K is unknown before and is a part of the optimal clustering
formulation. The arrival of a new node and the strategy it
uses to choose the existing nodes to make connections depend
on the graph generation model. We thus express the above
problem in the following way. Let Gn be a graph drawn from a
dynamic random graph model Gn on n vertices in which nodes
are labeled as [n] = {1, 2, . . . , n} according to their arrival,
i.e., node j was the jth node to arrive. Let Gn evolve from
the seed graph Gn0. To model the lack of knowledge of the
original labels, we subject the nodes to a permutation π drawn
uniformly at random from the symmetric group on n letters
Sn, and we are given the graph Hn := π(Gn); that is, the nodes
of Gn are randomly relabeled. We also use the notation Hn
to denote the random graph behind Hn. Our original goal is
to infer the arrival order in Gn after observing Hn, i.e., to ﬁnd
π−1. The permutation π−1 gives the true arrival order of the
nodes of the given graph.

Instead of putting a constraint on recovering the whole
permutation π−1 or equivalently K = n labels, we resort
to strict (irreﬂexive) partial orders. For a partial order σ, a
relation u <σ v means that node u is older than node v
according to the ordering σ..

A. Relation between temporal ordered clusters and partial
order set

Every partially ordered set can be represented by a clus-
tering {Ci } as follows. A strict partially ordered set can be
represented initially by a directed acyclic graph (DAG) with
nodes as the nodes in the graph Hn and directed edges as given
by the partial order σ: an edge from v to u exists when u <σ v.
Then taking the transitive closure of this DAG will result in
the DAG of the partial order set σ. Now, all the nodes with
in-degree 0 in the DAG will be part of cluster CK and the set
of nodes with all the in-edges coming from nodes in CK will
form cluster CK−1. This process repeats until we get C1. The
number of clusters K is not deﬁned before but found from the
DAG structure. Unlike the classical clustering, these clusters
are ordered such that C1 ≺ C2 . . . ≺ CK , where the relation
Ci ≺ Cj, i < j is deﬁned as all the nodes inside the cluster
Ci are estimated to be arrived earlier than all the nodes in the
cluster Cj, and all the nodes inside each cluster are considered
to be identical in arrival order. We note here that not all partial
orders result in a DAG that is weakly connected. If there are
multiple components in the DAG corresponding to a partial
order, each of them will give independent clustering. It might
be due to the nodes in these separate components of the DAG

3

are developed independently during evolution. Moreover, if
there are nodes that are not part of any comparison in the
partial order, we label them as unclassiﬁed.

In the following Section III-A, we formulate an optimization
problem for the clusters and ﬁnd that the time complexity of
its solution is n5-times larger than that of the solution of the
optimization problem of partial orders in Section III-B. Hence
in this paper, we focus only on the temporal-ordered clusters
derived from the partial order.

We deﬁne an estimator φ of the temporal ordered clustering2
as a function φ from the set of all labeled graphs on n vertices
to the set of all partial orders on nodes 1, . . . , n.

We consider estimators based on unsupervised and semi-

supervised learning paradigms:
• Unsupervised: In this case, the estimator does not have
access to any information of the node arrival orders. Its
results will be based only on the assumption that the graph
model ﬁts well the real-world network under consideration.
In Section III we formulate an optimization problem for
unsupervised learning and in Section IV we provide ap-
proximate solutions of the optimization.

• Semi-supervised: In some of the real-world networks, partial
information of the order of nodes is available - for some of
the node pairs u, v, it is revealed to the estimator that node
u is arrived earlier than node v. Such node pairs are termed
as perfect pairs. Taking this information into account would
help the estimator that is initially based on ﬁxed graph model
to adapt to the real-data. The semi-supervised estimators
introduced in Section IV learn the partial orders in the data
without violating the perfect pairs.

B. Measures for evaluating partial order

For a partial order σ,

let K(σ) denote the number of
i.e., K(σ) =

(cid:1).

that are comparable under σ:

(cid:1). That is, δ(σ) = K(σ)/(cid:0)n
2

pairs (u, v)
|{(u, v) : u <σ v}|, where |K(σ)|≤ (cid:0)n
2
Density: the density of a partial order σ is simply the number
of comparable pairs, normalized by the total possible number,
(cid:1). Note that δ(σ) ∈ [0, 1]. Then
(cid:0)n
2
the density of a partial order estimator φ is simply its minimum
possible density δ(φ) = minHn [δ(φ(Hn))].
Precision: it measures the expected fraction of correct pairs
out of all pairs that are guessed by the partial order. That is

θ(σ) = E

(cid:20) 1
K(σ)

|{u, v ∈ [n]: u <σ v, π−1(u) < π−1(v)}|

(cid:21)

.

For an estimator φ, we also denote by θ(φ) the quantity
E[θ(φ(π(Gn)))]. We note here that the typical graph cluster-
ing performance measures like Silhouette index and Davies-
Bouldin index do not ﬁnd useful in our set up since the dis-
tance measure in our case is difﬁcult to capture quantitatively
and is purely based on indistinguishability due to symmetries
and arrival order of nodes.

1In the rest of the paper, we omit conditioning on the given Gn0 in all the

expressions for the sake of brevity, if it is clear from the context.

2From now on, we use the terms node arrival order inferencing and temporal

ordered clustering interchangeably in the paper

III. SOLVING THE OPTIMIZATION PROBLEM

The precision of a given estimator φ can be written in the

form of a sum over all graphs Hn:

(cid:105)

1
K(φ(Hn))

Pr[π(Gn) = Hn]

θ(φ) = (cid:88)
Hn
|{u, v ∈ [n]: u <φ(Hn) v, π−1(u) < π−1(v)}|

(cid:12)
× E (cid:104)
(cid:12)π(Gn) = Hn
(cid:12)
Here π and Gn are the random quantities in the conditional
expectation. We formulate the optimal estimator as the one
that gives maximum precision for a given minimum density.
For an estimator to be optimal, it is then sufﬁcient to choose,
for each Hn, a partial order φ(Hn) that maximizes
Jε(φ) := K(φ(Hn))−1
× E (cid:104)

(cid:12)
(cid:12)π(Gn) = Hn
(cid:12)
subject to the density constraint δ(φ(Hn)) = K(φ(Hn))/(cid:0)n
(cid:1) ≥
2
ε, which says that we must have a certain minimum density
of comparable pairs (here, ε ∈ [0, 1] is a parameter of the
problem).

|{u, v ∈ [n]: u <φ(Hn) v, π−1(u) < π−1(v)}|

(cid:105)

In the the following ﬁrst two subsections, we formulate the
above optimization problem for two cases: when the estimator
outputs the clusters and when it outputs the partial order. Each
of these optimizations add a set of extra constraints to the
original problem.

Let

pu,v(Hn) := Pr[π−1(u) < π−1(v)|π(Gn) = Hn]

(1)

be the probability that u is arrived before v given the relabeled
graph Hn. The probability pu,v(Hn) turns out to be a critical
quantity that serves as the coefﬁcient in the linear program-
ming approximations of the optimization problems and its
estimation is explained in the last subsection of this section3.

A. Integer programming formulation for clusters

In this subsection, we restrict our optimization to linear
cluster estimators, where the clusters are arranged in a total
(linear) order.

To accomplish this optimization, we introduce, for each
vertex v, a vector (cid:174)xv = (xv,1, . . . , xv,n), where xv,i = 1 encodes
the fact that node v is placed in cluster i.

Then Jε can be written in terms of integer programming

(IP) formulation as

(cid:88)

(cid:88)

1≤u(cid:54)=v ≤n

1≤i< j ≤n

pu,v(Hn)

(cid:88)

xu,i xv, j
(cid:88)

,

(2)

xw,k xw(cid:48),l

1≤k<l ≤n

1≤w(cid:54)=w(cid:48) ≤n

subject to the basic constraints4

n
(cid:88)

j=1

xv, j = 1, ∀j ∈ [n] & xv, j ∈ {0, 1}, ∀v ∈ [n], ∀j ∈ [n].

3We drop the dependence of Hn in pu, v (Hn) and P if it is clear from

the context.

4Let the nodes in Hn take unique labels from the set [n] = {1, 2, . . . , n}
(the original random graph Gn is assumed to be labeled from [n], with label
i indicating ith arrival node).

4

We additionally have the following density constraint for a
given ε:

(cid:88)

(cid:88)

1≤k<l ≤n

1≤w(cid:54)=w(cid:48) ≤n

xw,k xw(cid:48),l ≥ ε

(cid:19)

(cid:18)n
2

.

.

.

Each term of the form xu,i xv, j becomes one only when the
node u is classiﬁed into a cluster i that has lower precedence
than node v’s cluster j (i < j). This corresponds to the event
u <φ(Hn) v with φ as given by the clusters. The probability
pu,v appears because of the event π−1(u) < π−1(v) inside
the expectation in Jε. The denominator in (2) corresponds to
K(φ(Hn)).

That is, we have a quadratic rational integer program with
linear basic constraints and a quadratic constraint introduced
by the minimum density. We show now how to convert
our program to a linear rational integer program with linear
constraints.

We deﬁne new variables zu,i,v, j = xu,i xv, j , for u, v, i, j ∈ [n].
We can then eliminate the rational part of the integer program
using the substitution

(cid:88)

s = (cid:169)
(cid:173)
(cid:173)
1≤k<l ≤n
1≤w(cid:54)=w(cid:48) ≤n
(cid:171)

zw,k,w(cid:48),l

−1

(cid:170)
(cid:174)
(cid:174)
(cid:172)

and z(cid:48)

u,i,v, j = s zu,i,v, j .

With the above change of variables,
restricted to {0, s}. The density constraint

the domain of z(cid:48)

is

zu,i,v, j ≥ ε

(cid:19)

(cid:18)n
2

=⇒ s ≤

1
ε (cid:0)n
2

(cid:1) .

(cid:88)

1≤u(cid:54)=v ≤n
1≤i< j ≤n

Now we transform the integer program to a linear program by
(cid:1)].
assuming z(cid:48) takes continuous values with domain [0, 1/ε (cid:0)n
2
We call the resulting optimization as LP-clusters.

Original integer program

LP approximation

(cid:88)

pu,v(Hn) zu,i,v, j

zw,k,w(cid:48),l

max
z(cid:48)

(cid:88)

1≤u(cid:54)=v ≤n
1≤i< j ≤n

pu,v(Hn)z(cid:48)

u,i,v, j

1≤u(cid:54)=v ≤n
1≤i< j ≤n

(cid:88)

max
z

1≤k<l ≤n
1≤w(cid:54)=w(cid:48) ≤n

subject to
• zu,i,v, j ∈ {0, 1}

∀u, i, v, j ∈ [n]
(cid:18)n
2

(cid:19)

zu,i,v, j ≥ ε

• (cid:88)

1≤u(cid:54)=v ≤n
1≤i< j ≤n

• (cid:88)
i ∈[n]

zu,i,u,i = 1, ∀u ∈ [n]

• zu,i,v, j = zv, j,u,i

• (cid:88)
i ∈[n]

∀u, i, v, j ∈ [n]

zu,i,v, j = zv, j,v, j ,

∀u, v, j ∈ [n]

subject to
• z(cid:48)

u,i,v, j ∈ [0, 1/ε (cid:0)n

(cid:1)]
2
∀u, i, v, j ∈ [n]

• (cid:88)

z(cid:48)
u,i,v, j = 1

1≤u(cid:54)=v ≤n
1≤i< j ≤n
z(cid:48)
u,i,u,i ≤ 1/ε

• (cid:88)
i ∈[n]
u,i,v, j = z(cid:48)

• z(cid:48)

v, j,u,i

• (cid:88)
i ∈[n]

u,i,v, j = z(cid:48)
z(cid:48)

v, j,v, j ,

(cid:19)

(cid:18)n
2

, ∀u ∈ [n]

∀u, i, v, j ∈ [n]

∀u, v, j ∈ [n]

The ﬁrst

three constraints are direct

translation of the
constraints in (2), and the last two comes from the substitution
zu,i,v, j = xu,i xv, j.

Complexity analysis. The LP approximation presented above
has Θ(n4) decision variables and Θ(n4) constraints. Therefore

the complexity of solving this optimization, without taking
into account the complexity of estimating pu,v, will be of the
order of n12 (d2c if d is the number of decision variables and
c is the number of constraints [19, Section 1.2.2]).

The numerical experiments of the optimization in terms of
clusters are presented later in Section VI-A. We also provide
comparisons showing the formulation in terms of partial orders
given in the next subsection computes much faster, yet outputs
estimates with precision closer to that of cluster optimization.

B. Integer programming formulation for partial orders

In this subsection, we derive the optimal partial order among
the nodes for the arrival order inference problem, extending
some results from our recent work in [5].

We now represent the optimization problem with Jε(φ) as
an integer program of partial order. For an estimator φ, we
deﬁne a binary variable yu,v for each ordered pair (u, v) as
yu,v = 1 when u <φ(Hn) v. Note that yu,v = 0 means either
u >φ(Hn) v or the pair (u, v) is incomparable in the partial
order φ(Hn).

In the following, we write the optimization in two forms:
the original integer program (left) and the linear program-
ming approximation (right). The objective functions of both
the formulations are equivalent to Jε(φ). The constraints of
the optimizations correspond to domain restriction, minimum
density, and partial order constraints – antisymmetry and
transitivity respectively. To use a linear programming approx-
imation, we ﬁrst convert the rational integer program into
truly integer program. With the substitution
an equivalent
s = 1/(cid:80)1≤u(cid:54)=v ≤n yu,v, and y(cid:48)
u,v = syu,v, the objective function
is rewritten as a linear function of the normalized variables.
u,v ∈ {0, s}, s ≤ 1/ε (cid:0)n
(cid:1).
These programs are equivalent if y(cid:48)
2
(cid:1) (cid:3). We call
u,v as (cid:2)0, 1/ε (cid:0)n
For the LP relaxation, we assume y(cid:48)
2
the LP in this subsection as the LP-partial-order.

Original integer program

LP approximation

(cid:80)

1≤u(cid:54)=v ≤n pu,v(Hn)yu,v
1≤u(cid:54)=v ≤n yu,v

(cid:80)

max
y

max
y(cid:48)

(cid:88)

1≤u(cid:54)=v ≤n

pu,v(Hn)y(cid:48)

u,v

subject to
• yu,v ∈ {0, 1}, ∀u, v ∈ [n]
• (cid:88)

yu,v ≥ ε

(cid:19)

(cid:18)n
2

1≤u(cid:54)=v ≤n

• yu,v + yv,u ≤ 1, ∀u, v ∈ [n]

• yu,v + yv,w − yu,w ≤ 1,

∀u, v, w ∈ [n]

subject to
• y(cid:48)
• (cid:88)

(cid:1)], ∀u, v ∈ [n]

u,v ∈ [0, 1/ε (cid:0)n
2
y(cid:48)
u,v = 1
(cid:1),
v,u ≤ 1/ε (cid:0)n
2
∀u, v ∈ [n]
u,w ≤ 1/ε (cid:0)n
(cid:1),
2
∀u, v, w ∈ [n]

v,w − y(cid:48)

1≤u(cid:54)=v ≤n
u,v + y(cid:48)

u,v + y(cid:48)

• y(cid:48)

• y(cid:48)

The above integer program and LP-partial-order formulation
is different from the LP-clusters in many ways. The idea of
LP-partial-order is to relax the formulation of LP-clusters by
focusing on the underlying partial order of clusters, rather
than clusters itself. This simpliﬁes the objective function,
though it brings additional partial order constraints into the
optimization. After ﬁnding the optimal partial order, we can
derive the ordered clusters from it using the peeling technique
in Section II-A. We note here that this may not need result
in unique clusters. Many partial orders can have the same the

5

cluster structure, especially when the DAG corresponding to
the partial order contains multiple components.

The next lemma bounds the effect of approximating the
coefﬁcients pu,v on the optimal value of the integer program.

Lemma 1. Consider the integer program whose objective
function is given by

ˆJε,λ(φ) =

(cid:80)1≤u<v ≤n ˆpu,v(Hn)yu,v
(cid:80)1≤u(cid:54)=v ≤n yu,v

,

integer pro-
with the same constraints as in the original
gram. Assume pu,v(Hn) can be approximated with | ˆpu,v(Hn) −
pu,v(Hn)|≤ λ uniformly for all u, v. Let φ∗ and ˆφ∗ denote
optimal points for the original and modiﬁed integer programs,
respectively. Then | ˆJε,λ( ˆφ∗) − Jε(φ∗)|≤ 3λ, for arbitrary λ > 0.

The proof of the above lemma is an extension of [5, Lemma
5.1, Supplementary Material] – we require a weaker assump-
tion | ˆpu,v(Hn) − pu,v(Hn)|≤ λ instead of | ˆpu,v(Hn)/pu,v(Hn) −
1|≤ λ in [5].

Complexity analysis and advantage over the cluster opti-
mization. The LP approximation has Θ(n2) decision variables
and Θ(n3) constraints (in the order of their appearance in the
formulation). Thus computational complexity of the LP will
of the order of n7 (without taking into account the estimation
complexity of pu,v), which is much less than n12 complexity
of cluster optimization in the previous subsection. Later in
Section VI-A, we provide numerical comparisons showing the
formulation in terms of partial orders computes much faster,
yet outputs estimates with precision closer to that of cluster
optimization.

C. Estimating coefﬁcients using importance sampling

We now discuss the importance sampling approach to
estimate the coefﬁcient pu,v that
is needed to solve the
optimization problem. The following approach to estimate pu,v
is applicable to any general graph model with Markovian
evolution (conditioned on the present state of the graph, the
new state is independent of the past state).

To estimate pu,v, we classify dynamic graph models into
two categories. Let Γ(Hn) be the set of all feasible permuta-
tions σ which generates a positive probability graph σ(Hn)
according to the distribution of the graph generation model.

i). Graph models with equiprobable isomorphic graphs.
Here, two isomorphic graphs have same probability under
the graph model. Formally, consider a graph G(1)
n with
n = σ(G(1)
n ] > 0 and another graph G(2)
P[Gn = G(1)
n )
with σ ∈ Γ(G(1)
n ), then the equiprobable condition can be
n ] = P[Gn = G(2)
stated as P[Gn = G(1)
n ]. Our previous work
in [5] focus on such a case and derives the following result.
Lemma 2 ([5, Lemma 4.1 in Supplementary Information]).
For all v, w ∈ [n] and graphs Hn,

n , G(2)

P[π−1(v) < π−1(w)|π(Gn) = Hn]

=

|σ : σ−1 ∈ (Hn), σ−1(v) < σ−1(w)|
|Γ(Hn)|

.

(3)

Though the graph models with such a property are not
common, it include preferential attachment and Erd˝os-Renyi
models. For preferential attachment model, we show in [5]
that the estimation of right-hand side of (3) deduces to
ﬁnding the proportion of linear extensions σ of a partial
order (set of node pair orderings that hold with probability
1) satisfying σ−1(v) < σ−1(w).

ii). Graph models with non-equiprobable

isomorphic
graphs. Many of the graph models do not possess equiprob-
able ismorphic graphs property. In this work, we propose a
new estimation scheme based on importance sampling that
is applicable to such a case for any general graph model
with Markovian evolution.
We have, for pu,v := P(π−1(u) < π−1(v)|π(Gn) = Hn),
pu,v = (cid:88)

P(π = σ|π(Gn) = Hn)

σ : σ−1 ∈Γ(Hn)
σ−1(u)<σ−1(v)

= (cid:88)

σ : σ−1 ∈Γ(Hn)
σ−1(u)<σ−1(v)

= (cid:88)

σ : σ−1 ∈Γ(Hn)
σ−1(u)<σ−1(v)
(cid:80)

σ : σ−1 ∈Γ(Hn)
σ−1(u)<σ−1(v)
(cid:80)

=

P[π = σ, π(Gn) = Hn]
P[π(Gn) = Hn]

P[Gn = σ−1(Hn)]P[π = σ]

(cid:80)

σ−1 ∈Γ(Hn)

P[Gn = σ−1(Hn)]P[π = σ]

P[Gn = σ−1(Hn)]

,

(4)

σ−1 ∈Γ(Hn)

P[Gn = σ−1(Hn)]
where we used the fact that P[π = σ] = 1/n! since it is
independent of Hn.

We now derive an estimator for pu,v by approximating the
numerator and denominator of right-hand side in (4). The pu,v
expression involves summing over permutations from the fea-
sible set, i.e., σ−1 ∈ Γ(Hn) (σ−1(Hn) gives a positive probable
graph by the deﬁnition of Γ(Hn)). Since there are at most
n! permutations to check for feasibility, direct sampling from
Γ(Hn) is impossible in many cases. However, we remark that
each permutation σ−1 ∈ Γ(Hn) invokes a chain structure when
the graph has a Markovian evolution, as follows. Applying σ−1
to Hn is essentially relabeling of nodes in Hn from [n]. Then
starting from labeling a guess of the youngest node with n,
by reverse engineering the Markovian evolution of the graph,
to ﬁnd the node with label s < n we need to know only the
node s + 1 and the graph Hs+1. Based on this observation, to
estimate the denominator in right-hand side of (4) we propose
a sequential importance sampling strategy in Theorem 1 that
generalizes to any localized sampling distribution (probability
to choose node s after selecting node s + 1) which meets a
certain criteria. This is directly extendable to estimating the
numerator in (4) too by putting an extra restriction to the
sampled permutation. Later Lemma 3 presents an estimator
of pu,v using the technique derived in Theorem 1.

Let RHn ⊆ V(Hn) denote the set of candidates for youngest
nodes at time n. The set RHn depends on the graph model.
For example, in case of preferential attachment model, in
which a new node attaches m edges to the existing nodes
with a probability distribution proportional to the degree of
the existing node, RHn is the set of m-degree nodes. We

6

labels. For instance,

consider only permutations that do not change the initial graph
Gn0
if Gn0 has three nodes and Gn
has 6 nodes, we consider the following permutations (rep-
resented in cyclic notation): (1)(2)(3)(456), (1)(2)(3)(45)(6),
(1)(2)(3)(46)(5), (1)(2)(3)(4)(56), (1)(2)(3)(4)(5)(6). Thus we
deﬁne Hn0 as Gn0 itself. Since we assume Hn0 is known, pu,v
expression in (4) has an additional conditioning of Hn0.

Let δ(Hn, zn) represent the graph in which the node zn ∈
RHn is deleted from Hn. Then the graph sequence Hn =
Hn, Hn−1 = δ(Hn, zn), . . . , Hn0 = Hn0 forms a nonhomoge-
neous Markov chain – nonhomogeneous because the state
space {Hs }s ≤n changes with s and thus the transition proba-
bilities too. Similarly Gn, Gn−1, . . . , Gn0 also make a Markov
chain, and for a ﬁxed permutation σ, σ(Gn) = Hn, both the
above Markov chains have same transition probabilities. Let
us also deﬁne the posterior probability of producing Hn from
δ(Hn, zn) as

w(δ(Hn, zn), Hn) := P[Hn = Hn|Hn−1 = δ(Hn, zn)].

(5)

The following theorem characterizes our estimator. For a
Markov chain, let Ex denote the expectation with starting state
x. Let Gs be the set of all labeled graphs on s vertices.

Theorem 1 (Sequential importance sampling). Consider a
time-nonhomogeneous Markov chain Hn = Hn, Hn−1 =
δ(Hn, zn), . . ., where zn ∈ RHn, zn−1 ∈ RHn−1, . . . etc be the
nodes removed randomly by the Markov chain and let its
transition probability matrices be {Qs = [qs(F (cid:48), F (cid:48)(cid:48))]}s ≤n for
any two graphs F (cid:48) ∈ Gs and F (cid:48)(cid:48) ∈ Gs−1. Then we have

(cid:88)

P[Gn = σ−1(Hn)|Hn0] = EHn=Hn

σ−1 ∈Γ(Hn)

(cid:34)n0+1
(cid:89)

s ≤n

w(δ(Hs, zs), Hs)
qs(Hs, δ(Hs, zs))

(cid:35)

.

Proof. Now we have the following iterative expression for the
denominator of pu,v.

pdenom
u,v

(Hn, Hn0) := (cid:88)

P[Gn = σ−1(Hn)|Hn0]

(6)

= (cid:88)
zn ∈R Hn

(cid:88)

σ−1 ∈Γ(Hn)

σ−1 ∈Γ(Hn)
P[Gn = σ−1(Hn), Gn−1 = σ−1

1 (δ(Hn, zn))|Hn0],

where σ1 ∈ Sn−1 is the permutation σ with “zn maps to n"
removed. Now we can rewrite the above expression as

(cid:88)

(cid:88)

zn ∈R Hn

σ−1 ∈Γ(Hn)

P[Gn = σ−1(Hn)|Gn−1 = σ−1

1 (δ(Hn, zn))]

× P[Gn−1 = σ−1

1 (δ(Hn, zn))|Hn0].

Note that P[Gn = σ−1(Hn)|Gn−1 = σ−1
1 (δ(Hn, zn))] for a ﬁxed
σ (thus σ1) is equivalent to w(δ(Hn, zn), Hn). Now introducing
a transition probability {Qs = [qs(i, j)]}s ≤n for the Markov
chain {Hs }s ≤n, and using importance sampling,

pdenom
u,v

(Hn, Hn0) = (cid:88)
zn ∈R Hn
× (cid:88)
σ−1 ∈Γ(δ(Hn,zn))
= (cid:88)
zn ∈R Hn

w(δ(Hn, zn), Hn)
qn(Hn, δ(Hn, zn))
P[Gn−1 = σ−1(δ(Hn, zn))|Hn0].

qn(Hn, δ(Hn, zn))

w(δ(Hn, zn), Hn)
qn(Hn, δ(Hn, zn))

qn(Hn, δ(Hn, zn))

× pdenom
u,v

(δ(Hn, zn), Hn0),

(7)

with pdenom
(Hn0, Hn0) = 1. Here qn(Hn, δ(Hn, zn)) is the tran-
sition probability to jump from Hn = Hn to Hn−1 = δ(Hn, zn).

u,v

Now let µ(Hn, Hn0) = EHn=Hn

(cid:34)n0+1
(cid:89)

s ≤n

(cid:35)

w(δ(Hs, zs), Hs)
qs(Hs, δ(Hs, zs))

by invoking continuous mapping theorem, we can prove that
(cid:3)
their ratio also converges to pu,v almost surely.

Theorem 1 and Lemma 3 provide us the ﬂexibility and
convenience to sample the permutations and estimate pu,v via
a wide-range of sampling distributions. In the next section, we
consider two such candidate distributions.

7

Then we have,
µ(Hn, Hn0) = (cid:88)
zn ∈R Hn

w(δ(Hn, zn), Hn)
qn(Hn, δ(Hn, zn))
(cid:34) n0+1
(cid:89)

× EHn−1=δ(Hb,zn)

s ≤n−1

qn(Hn, δ(Hn, zn))

(cid:35)

w(δ(Hs, zs), Hs)
qs(Hs, δ(Hs, zs))

= (cid:88)
zn ∈R Hn

w(δ(Hn, zn), Hn)
qn(Hn, δ(Hn, zn))

× µ(δ(Hn, zn), Hn0),

qn(Hn, δ(Hn, zn))

(8)

(9)

where (8) follows from the Markov property.

Deﬁning the function at n0 as
w(δ(Hn0, zn0), Hn0+1)
qn0(Hn0+1, δ(Hn0, zn0))

= 1, for any zn0

we note here that the iteration (9) of µ(Hn, Hn0) is identical
(cid:3)
to that of pdenom

in (7). This completes the proof.

u,v

Remark 1. Note that unlike qs(Hs, δ(Hs, zs)), which is under
our control to design a Markov chain, w(δ(Hs, zs), Hs) is a
well-deﬁned ﬁxed quantity (see (14)). The only constraint for
the transition probability matrices {Qs }s ≤n is that it should be
chosen to be in agreement with the graph evolution such that
the choices of jumps from Hs to Hs−1 restricts to removing
nodes from RHs , and it depends on the graph model.

pu,v estimator. Now we can form the estimator for pu,v for a
node pair (u, v) as follows. Let (cid:174)z(k) be the vector denoting
the sampled node sequence of the kth run of the Markov
chain. It can either represent a vector notation as (cid:174)z(k) =
(z(k)
n0+1) or take a function form (cid:174)z(k)(s) denoting
the new label of a vertex s in Hn. We propose the following
estimator and show that it has asymptotic consistency.

n−1, . . . , z(k)

n , z(k)

Lemma 3 (Estimator and its consistency). Let the estimator of
pu,v, for all u, v ∈ Hn, formed from k samples of the sequential
importance sampling (see Theorem 1) be

(cid:98)p(k)
u,v =

(cid:80)k

i=1 1{(cid:174)z(i)(u)<(cid:174)z(i)(v)}

(cid:81)n0+1
s ≤n

w(δ(Hs, (cid:174)z(i)
s ), Hs)
qs(Hs, δ(Hs, (cid:174)z(i)
s ))

(cid:80)k

i=1

(cid:81)n0+1
s ≤n

w(δ(Hs, (cid:174)z(i)
s ), Hs)
qs(Hs, δ(Hs, (cid:174)z(i)
s ))

.

(10)

u,v → pu,v a.s. as k → ∞.

Then (cid:98)p(k)
Proof. Using Theorem 1 and based on the observation that
the Markov sample paths in different runs are independent
and identically distributed, the numerator and denominator in
the right-hand side of (10) converge separately to that of (4)
by strong law of large numbers (in almost surely sense). Then

IV. APPROXIMATING OPTIMAL SOLUTION
In this section, we describe our main algorithms for node

arrival order recovery of a general graph model.

Algorithms for sampling the Markov chain. Finding the
whole set of permutations and calculating the exact pu,v
according to (4) is of exponential complexity. With Theorem 1
and eq. (10), we can approximate pu,v as the empirical average
of Markov chain based sample paths. We try two different
importance sampling distributions {Qs }s ≤n:
• local-unif-sampling with transition probabilities

qs(Hs, δ(Hs, zs)) =

1
|RHs |

.

(11)

• high-prob-sampling forms the Markov chain with

qs(Hs, δ(Hs, zs)) =

w(δ(Hs, zs), Hs)
(cid:80)u ∈R Hs w(δ(Hs, u), Hs)

.

(12)

The above transition probability corresponds to choosing the
high probability paths.

Though the high-prob-sampling looks like the right
approach to follow, as we show later in Section VI-A, it
has much slower rate of convergence than local-unif-
sampling. Moreover at each step s, without taking into ac-
count the speciﬁc graph model characteristics and using naive
implementations, high-prob-sampling requires O(n2)
computations – O(n) possibilities exist for immediate ancestor
of zs in δ(Hs, zs) which is needed for calculating the posterior
probability w and there are O(n) possibilities for the sum in the
denominator, while local-unif-sampling requires only
O(n) – counting |RHs | by checking all the nodes. In some
graph models (like the DD-model in Section V-A), all the
nodes in Hs can be part of RHs with a positive probability, and
local-unif-sampling will essentially become uniform
sampling.

The local-unif-sampling can be further improved
with the acceptance-rejection sampling technique: at a step t,
randomly sample a node u from V(Ht ) (instead of sampling
from RHt ). Then calculate the probability that the node u be
the youngest node in the graph. If this probability is positive,
we accept u as Vt and if it is zero, we randomly sample again
from V(Ht ).

Now we assume that pu,v are estimated for all u and v to
propose algorithms for temporal clustering. In fact, according
to Lemma 1, we only need to have maxu,v | ˆpu,v − pu,v |≤ λ
for a small λ > 0. Thus for small pu,v, ˆpu,v can be assumed
to be zero. We can then use LP-partial-order in Section III-B
with the estimated pu,v as the coefﬁcients. Due to the huge
computational complexity associated with the LP solution, we
now propose the following unsupervised and semi-supervised
approximation algorithms based on the estimates of pu,v.

Algorithm 1 Temporal Ordered Clustering: Semi-supervised

Input: graph Hn, graph model Gn description, training set of partial order σtrain, number of sample paths k
Output: Clusters C1 ≺ C2 . . . ≺ CK

8

for (cid:96) from 1 to k do

1: procedure TEMPORALORDEREDCLUSTERING
2:
3:
4:
5:

Find RHs and N R Hs by (13)
RHs ← RHs \N R Hs
Sample z((cid:96))

for s from n down to n0 do

end for

6:
7:
8:
9:
10:
11:
12: end procedure

s using a sampling method – local-unif-sampling (11) or high-prob-sampling (12)

end for
Estimate ˆp(k)
Use algorithm sort-by-pu,v-sum or pu,v-threshold to estimate clusters of nodes C1, C2, . . . , CK
return C1, C2, . . . , CK

u,v, ∀u, v ∈ V(Hn) using (4)

A. Unsupervised solution

sort-by-pu,v-sum algorithm. For this algorithm, we con-
struct a new complete graph with the node set same as that
of Hn and edge weights as pu,v. Let us now deﬁne a metric
pu := (cid:80)v ∈V pu,v for every node u of Hn. Since pu,v denotes
the probability that node u is older than node v, pu would
give a high score when a node u becomes the oldest node.
Our ranking is then sorted order of the pu values.

Instead of total order, a partial order can be found by a
simple binning over pu values: ﬁx the bin size |C| and group
|C| nodes in the sorted pu values into a cluster, and the process
repeats for other clusters. If |C|= 1, the algorithm will yield a
total order.

pu,v-threshold algorithm. Here, each of the estimated
pu,v’s is compared against a threshold τ. Only the node pairs
that are strictly greater than this condition are put into the
estimator output partial order. Note that if τ = 0.5, we get a
total order in virtually all relevant cases.

B. Semi-supervised solution

Suppose we have partial true data available. Let it be ordered
in partial order as σorig = {(u, v)}, in which for the pair (u, v),
u is the older than v. Let σtrain ⊂ σorig be the training set and
the let the test set be σtest := σorig\σtrain. Let |σtrain|= α|σorig|
for some 0 < α < 1. With the knowledge of σtrain, we modify
the estimation of pu,v as follows. The set of removable nodes
RHs at each instant s is modiﬁed to RHs \N R Hs , where N R Hs
is the set of nodes that can not be included in the removable
nodes as it would violate the partial order of σtrain. It is deﬁned
as follows:

N R Hs := {u : (u, v) ∈ σtrain, u, v ∈ V(Hs)}, ∀n ≥ s ≥ n0. (13)

After estimating pu,v with the redeﬁned RHs , we employ
sort-by-pu,v-sum algorithm or pu,v-threshold algo-
rithms to ﬁnd partial order. An example of RHs construction
is shown in Figure 2.

Algorithm 1 summarizes our semi-supervised algorithm.
The algorithm will become unsupervised when there is no
σtrain and step-5 is removed.

Fig. 2: Semi-supervised learning example DAG for σtrain =
{(u, v), (v, w), (w, x), (y, w): N R Hs = {v, w, x} and RHs =
{u, y, z}.

V. TEMPORAL ORDERED CLUSTERING FOR
DUPLICATION-DIVERGENCE MODEL

A. Duplication-divergence model (DD-model)

We consider Solé et al. deﬁnition of the DD-model [20]. It
proceeds as follows. Given an undirected, simple seed graph
Gn0 on n0 nodes and target number of nodes n, the graph Gk+1
with k + 1 nodes5 evolves from the Gk as follows: ﬁrst, a new
vertex v is added to Gk. Then the following steps are carried
out:
• Duplication: Select a node u from Gk uniformly at random.
The node v then makes connections to N (u), the neighbor
set of u.

• Divergence: Each of the newly made connections from v to
N (u) are deleted with probability 1 − p. Furthermore, for all
the nodes in Gk to which v is not connected, create an edge
from it to v independently with probability r
k .

The above process is repeated until the number of nodes in
the graph is equal to n. We denote the graph Gn generated
from the DD-model with parameters p and r, starting from
seed graph Gn0, by Gn ∼ DD-model(n, p, r, Gn0).

The posterior probability w(δ(Hs, zs), Hs), which is deﬁned
in (5) and used in Theorem 1 and high-prob-sampling,
can be calculated for the DD-model as follows. For a node
zs ∈ RHs , we say a node u is its parent if u can be selected
from the graph δ(Hs, zs) for the duplication step when zs is

5The subscript k with Gk can also be interpreted as time instant k

𝑢𝑣𝑦𝑧𝑤𝑥1
s − 1
(cid:16)
r
s − 1

added into δ(Hs, zs). The probability of having the node u as
the parent of zs ∈ RHs in the DD-model is

w(δ(Hs, zs), u, Hs)

=

p |N(zs )∩N(u)|(1 − p) |N(u)\N(zs )|

(cid:17) |N(zs )\N(u)| (cid:16)

1 −

r
s − 1

(cid:17) (s−1)−|N(zs )∪N(u)|

(14)

The above expression can be inferred directly from the def-
inition of the DD-model as follows. We ﬁrst pick u as a
1
parent node of zs with probability
s−1 . Then to calculate
the probabilities of edge addition events retrospectively, we
observe that edges from zs to the nodes in the N (zs) ∩ N (u)
stayed with probability p, but edges to N (u)\N (zs) were
dismissed with probability 1 − p. We also have to take into
account the edges between zs and vertices outside of N (u) –
each were chosen independently with probability r
s−1 and they
are exactly the edges from zs to N (zs)\N (u).

Now w(δ(Hs, zs), Hs)

= (cid:80)u ∈ PHs (zs ) w(δ(Hs, zs), u, Hs),

where PHs (zs) represents possible parents of zs in Hs.

9

that for the Erd˝os-Renyi model, any graph in Adm(Gn) is
generated equally likely with a given seed graph Gn0. Such
property was also proved for the preferential attachment model
in [2]. However, this does not hold for DD-model graphs as
shown in the following example.

Fig. 3: Example of asymmetric graph

For the graph G(1)

n presented in Figure 3, let Gn0 consists
of vertices 1, 2, 3, and let the parameters of the DD-model
be p = 0.2 and r = 0. The P[Gn = G(1
n ] can be calculated
iteratively using (14) as 0.068. Now, consider the permutation

Since all permutations have positive probability in this
version of the model, we have RHs = V(Hs) and Γ(Hs) = s!.

σ =

(cid:18)1
1

2
2

3
3

4
5

(cid:19)

5
4

.

B. Greedy algorithms for clustering

To form a comparison with algorithms proposed in Sec-
tion IV, we propose the following greedy unsupervised algo-
rithms for the DD-model.

sort-by-degree. The nodes are sorted by the degree and
arranged into clusters {Ci }i ≥1. Cluster C1 contains nodes with
the largest degree.

peel-by-degree. The nodes with the lowest degree are
ﬁrst collected and put in the highest cluster. Then they are
removed from the graph, and the nodes with the lowest degree
in the remaining graph are found and the process repeats.

sort-by-neighborhood. This algorithm will output a
partial order with all ordered pairs (u ≺ v) such that N (u)
contains N (v). This condition holds when r = 0. When r > 0,
we consider |N (v)\N(u)|≤ r as r is the average number
of extra connections a node makes apart from duplication
process. In most real-world data, we estimate r as smaller
than 1, and hence the original check is sufﬁcient.

peel-by-neighborhood. Here, we ﬁnd the set {u :
(cid:154) v|N (v)\N (u)|≤ r } (as mentioned before, it is sufﬁcient to
check N (v) ⊂ N (u) in many practical cases) and mark it as
the youngest cluster. These nodes are removed from the graph,
and the process is repeated until it hits Gn0. This algorithm
makes use of the DAG of the neighborhood relationship and
includes isolated nodes into the bins.

C. Comparison with other graph models

The node arrival order recovery problem in the DD-model
is different from that in other graph models like Erd˝os-Renyi
graphs and preferential attachment graphs.

First, for a ﬁxed graph Gn on n vertices, let us consider a
set of graphs Adm(Gn) = {σ(Gn): σ ∈ Γ(Gn)}. It is obvious

n). Let G(2)

n = σ(G(1)
Then σ ∈ Γ(G1
n ] is
0.051, and conditioned on the same structure probabilities of
G(1)
n are 0.5744 and 0.4256 respectively.

n ). The P[Gn = G(2

n and G(2)

Fig. 4: E log|Aut(Gn)|, Gn ∼ DD-model(2000, p, r, K20),
where |Aut(Gn)| is the number of automorphisms in graph
Gn.

Second, it is well known that both the Erd˝os-Renyi graphs
and preferential attachment graphs are asymmetric6 with high
probability [6], [2]. On the other hand, the graphs generated
from the DD-model for a certain range of parameters show a
signiﬁcant amount of symmetry, as shown in Fig. 4. This is in
accordance with many real-world networks (see Table III for
examples).

6An automorphism or symmetry of a graph G is an isomorphism from a
graph G to itself. We say that G is symmetric if it has at least one nontrivial
symmetry and that G is asymmetric if the only symmetry of G is the identity
permutation.

132450.00.10.20.30.40.50.60.70.80.91.0p5.04.03.02.01.00.0r10−310−210−1100101102103104105(cid:17)

t/s

(cid:16)√

Last, the behavior of the degree at time t of the node arrived
at an earlier time s (denoted by degt (s)) is different for all
three models. For Erd˝os-Renyi graph with edge probability
p, it is known that E[degt (s)] = p(t − 1). For the preferential
attachment graphs, E[degt (s)] = Θ
([21], Theorem 8.2).
However, for the DD-model, E[degt (s)] = Θ (cid:0)(t/s)p s2p−1(cid:1) for
any t ≥ s [7]. Note that when s = O(1) – the case of very
old nodes – the average degree is of order t p. For s = t, we
have E[degt (t)] = O(t2p−1) which is growing only for p >
1/2. For example, when p = 1 degrees of all the nodes on
average are of order O(t). Thus oldest nodes in the graph
need not have large average degrees as the graph evolves, and
algorithms based on such a heuristic are not applicable for the
DD-model. Moreover, Frieze et al. [8] has shown that degt (s)
is concentrated around the mean for s = O(1) in the sense that
for any A > 1 we observe polynomial tail:

Pr[degt (s) < C E[degt (s) log−k t]

= Pr[degt (s) > C E[degt (s) logk t] = O(t−A),
for certain ﬁxed constant k and a constant C dependent on
A. However, this is not the case for the last vertices, since
they are copied from already existing nodes in the network,
which would explain the ineffectiveness of greedy degree-
based heuristics for p ≤ 1/2.

VI. EXPERIMENTS
In this section, we evaluate our methods on synthetic
and real-world data sets. We made publicly available all
the code and data of
this project at https://github.com/
krzysztof-turowski/duplication-divergence.

We present the following results in the coming sections.

• Synthetic networks:

– How well the LP-partial-order performs in comparison

with the LP-clusters? (Figures 5 and 6)

– Fixing the LP-partial-order, how is the convergence of
pu,v’s that are estimated via sequential importance sam-
pling schemes local-unif-sampling and high-
prob-sampling as to the exact pu,v,
in terms of
resulting precision? (Figure 7)

– Fixing LP-partial-order

the LP formulation and
for
local-unif-sampling for the importance sampling
strategy, we study the performance of unsupervised al-
gorithms in comparison with greedy strategies speciﬁc to
the DD-model. (Figure 8)

– For the semi-supervised algorithms, we show results (pre-
cision and density) for various parameter conﬁgurations
and study their inﬂuence on the performance. (Tables I
and II)

• Real-world networks: For the semi-supervised algorithms,
how the precision improves with a small change in the train-
ing size, and how does the results compare against greedy
algorithms of the DD-model? (Figure 9 and Table IV)

Maximum likelihood estimation: For deriving total order,
a natural solution will be the maximum likelihood estimator
(MLE).

P[Gn = π−1(Hn)|π−1 = σ]

arg max
σ ∈Sn

10

But we do not consider MLE explicitly here because it is
known that many networks exhibit large number of symmetries
(see Table III for some examples), and thus there will be large
number of total orders that achieve the MLE criterion with
low value of precision. In fact, our optimal formulation in
Section III already captures the MLE solutions and outputs
them if they have high precision. Moreover for general graph
models, the MLE computation would require checking all σ ∈
Sn which incurs Θ(n! ) computational complexity.

A. Synthetic networks

In the following results on synthetic networks, σtries denote
the number of Markov chain sample paths (for sequential
importance sampling) used for estimating pu,v for all u, v ∈
V(Hn). All the studies are performed on multiple graph real-
izations from the DD-model with speciﬁed parameters, and the
results are averaged over them. When we make a comparison
based on LP formulation, we plot precision (θ) vs minimum
density ε (δ ≥ ε) in accordance with the formulations in
Sections III-A and III-B.

Fig. 5: Comparison between LP-clusters and LP-partial-order
formulation: Gn ∼ DD-model(n = 30, p = 0.6, r = 1.0, Gn0 =
K10) and σtries = 100, 000. Results are averaged over 100 graph
generations. Sampling method: local-unif-sampling.

Fig. 6: Time plot for LP-partial-order vs. LP-clusters. All the
experiments were performed on 48-CPU cluster, with Intel(R)
Xeon(R) CPU E7-8857 v2 @ 3.00GHz and 256GB RAM.

0.00.20.40.60.81.0ε0.50.60.70.80.91.0θLP-partial-orderLP-clusters15202530n102103104105106time[s]LP-partial-orderLP-clustersIn Figure 5, we compare the performance of the linear
programming approximations LP-cluster (Section III-A) and
LP-partial-order (Section III-B). Since clustering output from
the LP-cluster scheme induces a partial order, we use the
same measures of precision and density that are deﬁned for
partial order for comparing performances of LP-cluster and
LP-partial-order schemes. Our experiments conﬁrm that for
the same graph, with the same set of {pu,v, ∀u, v ∈ V(Hn)}, the
performance of them are nearly identical. However, Figure 6
shows that the difference between the running time of both
the formulations is huge – the LP-clusters which ﬁnds clus-
ters becomes barely feasible, whereas LP-partial-order which
outputs partial order runs in a reasonable time.

Figure 7 examines the precision of LP-partial-order obtained
with approximated {pu,v, ∀u, v ∈ V(Hn)} via sequential im-
portance strategies (local-unif-sampling and high-
prob-sampling) and that obtained with the {pu,v, ∀u, v ∈
V(Hn)} that is calculated exactly by considering all the pos-
sible n! orderings. We consider a small size (n = 13) example
here since it becomes infeasible to compute the exact curve
for larger values of n. We observe that the convergence of
the estimated curve is highly dependent on the method of
estimation: local-unif-sampling method requires only
100 samples, but high-prob-sampling is still visibly far
away from LP optimal curve even for 1000 samples. Thus,
along with the computational reasons stated in Section IV,
we use local-unif-sampling in the subsequent experi-
ments.

In Figure 8, we compare results of the unsupervised algo-
rithms with the estimated optimal curve via local-unif-
sampling. It turns out that greedy algorithms (sort-by-
degree, sort-by-neighborhood, peel-by-degree
and peel-by-neighborhood) perform reasonably well
for small p, but
their performance deteriorates for higher
values of p. On the other hand, pu,v-based algorithms (sort-
by-pu,v-sum and pu,v-threshold) offer consistent, close
to the theoretical bound, behavior different values of p (ﬁgure
shows only two ps due to space limitations). Moreover, the
in sort-by-pu,v-sum and threshold τ in
bin size |C|
pu,v-threshold algorithm offer a trade-off between higher
precision and higher density. The larger the bin size or the
higher the threshold, we observe a decrease in density, but
increase in precision as we stay close to the theoretical curve.

Table I contains the results of semi-supervised learning
extensions of the pu,v-based algorithms. A small
increase
in the percentage of the training set α yields a large in-
creases in precision for all sets of parameters. Moreover, for
larger bin size in sort-by-pu,v-sum algorithm we observe
mainly only an increase in precision with α, but for pu,v-
threshold algorithm both δ and θ grow visibly with α,
especially for large τ. In turn, when we ﬁx α and increase the
bin size in sort-by-pu,v-sum algorithm, precision remains
almost same, but density decreases signiﬁcantly. And if we
do the analogous procedure for pu,v-threshold algorithm
(ﬁx α and increase threshold), then precision grows, but in
expense of a visible fall of density. All the above conclusions
are summarized in Table II.

11

p = 0.3

p = 0.6

Algorithm

α

δ

θ

δ

θ

sort-by-pu, v -sum, |C |= 1
sort-by-pu, v -sum, |C |= 1
sort-by-pu, v -sum, |C |= 1
sort-by-pu, v -sum, |C |= 10
sort-by-pu, v -sum, |C |= 10
sort-by-pu, v -sum, |C |= 10
pu, v -threshold, τ = 0.5
pu, v -threshold, τ = 0.5
pu, v -threshold, τ = 0.5
pu, v -threshold, τ = 0.9
pu, v -threshold, τ = 0.9
pu, v -threshold, τ = 0.9

0.001
0.01
0.1
0.001
0.01
0.1
0.001
0.01
0.1
0.001
0.01
0.1

1.0
1.0
1.0
0.769
0.768
0.758
1.0
1.0
1.0
0.010
0.020
0.521

0.598
0.643
0.836
0.605
0.661
0.864
0.604
0.637
0.829
0.906
0.951
0.966

1.0
1.0
1.0
0.769
0.767
0.759
1.0
1.0
1.0
0.028
0.090
0.559

0.613
0.650
0.832
0.626
0.660
0.859
0.617
0.649
0.823
0.871
0.907
0.960

I:

Results

synthetic

on
learning
DD-model(50, p, 1.0, Gn0),

TABLE
semi-supervised
∼
Gn
graphs. pu,v-based algorithms use σtries = 100, 000. Gn0
Erd˝os-Renyi graph with n0 = 10 and p0 = 0.6.

networks with
algorithms:
averaged over 100
is

pu,v-based

Algorithm

Fixed

Free

Free

Free

sort-by-pu, v -sum
sort-by-pu, v -sum
pu, v -threshold
pu, v -threshold

α
|C |
α
τ

|C |(cid:37) δ (cid:38) θ ≈
α (cid:37) δ ≈
θ (cid:37)
τ (cid:37) δ (cid:38) θ (cid:37)
α (cid:37) δ (cid:37) θ (cid:37)

TABLE II: Conclusions from synthetic data: how the metrics
behave by ﬁxing one of the parameters and keeping other free.
The symbol ≈ indicates the changes are not signiﬁcant.

B. Real-world networks

We consider the following three real-world networks which
have the ground truth of node and edge age arrival order avail-
able. The directed networks are treated as undirected in our
studies. All the datasets are taken from SNAP repository [22].
• The ArXiv network: It is a directed network with 7,464
nodes and 116,268 edges. Here the nodes are the publica-
tions in arXiv online repository of theoretical high energy
physics, and the edges are formed when a publication
cite another. In this network, many nodes share the same
arrival time and date, and hence the true arrival order of
nodes is available only in bins of count 1,457.

• The Simple English Wikipedia dynamic network: A di-
rected network with 10,000 nodes and 169,894 edges.
Nodes represent articles and an edge indicates that a
hyperlink was added. It shows the evolution of hyperlinks
between articles of the Simple English Wikipedia.

• CollegeMsg network: In this dataset of private message
sent on an online social platform at University of Cali-
fornia, Irvine, nodes represent users and an edge from u
to v indicates user u sent a private message to user v at
time t. Number of nodes is 1,899 and number of edges
is 59,835.

Table III shows estimated parameters of the duplication-
divergence model for the above networks using the ﬁtting
technique in [23].

Figure 9 show the result of semi-supervised learning. Here
α represents the proportion of all pairs that is considered as
training set, i.e., size of the training set is α (cid:0)n
(cid:1). We randomly
2
(cid:1) pairs and the results presented are average over 100
pick α (cid:0)n
2
different such random sets. We observe that a small increase in

12

Fig. 7: Results on synthetic networks with exact curve: Gn ∼ DD-model(13, p, 1.0, Gn0) for p = 0.3 (left) and 0.6 (right),
averaged over 100 graphs. Gn0 is generated from Erd˝os-Renyi graph with n0 = 4 and p0 = 0.6.

∼
Fig. 8: Results on synthetic networks with greedy and unsupervised learning pu,v-based algorithms: Gn
DD-model(50, p, 1.0, Gn0) for p = 0.3 (left) and 0.6 (right), averaged over 100 graphs. pu,v-based algorithms use
σtries = 100,000. Gn0 is generated from Erd˝os-Renyi model with n0 = 10 and p0 = 0.6. The theoretical curve is estimated via
local-unif-sampling.

Network (Gobs)
ArXiv
Wikipedia
CollegeMsg

log|Aut(Gobs)|
12.59
1018.94
231.54

(cid:98)p
0.72
0.66
0.65

(cid:98)r
1.0
0.5
0.45

TABLE III: Parameters of the duplication-divergence model
estimated for the real-world networks considered in this paper.

of the labeled nodes.

Finally, the semi-supervised approach helps to obtain a sig-
niﬁcant improvement over greedy algorithms. As it is shown
in Table IV, greedy algorithms found orderings with precision
ranging from 0.47 to 0.63 for signiﬁcant values of the density
(it’s easy to achieve a precision of 0.78 like CollegeMsg data
set when the density of pairs outputted is as low 0.01) – which
is not much better than random guess.

α leads to a huge change in the precision. This also happens in
synthetic data and is caused by the large structural dependency
within networks, unlike in classical machine learning where
data is often assumed to be independent. This helps us to get
a near-perfect clustering (precision close to 1) with only 1%

VII. DISCUSSION AND FUTURE WORK

In this article we presented a framework for clustering of
nodes in dynamic networks based on latent temporal infor-
mation. We provided a way to ﬁnd an upper bound on the

0.00.20.40.60.81.0(cid:15)0.50.60.70.80.91.0θ0.00.20.40.60.81.0(cid:15)0.50.60.70.80.91.0θ0.20.40.60.81.0(cid:15)0.50.60.70.80.91.0θexactlocal-unif-sampling(σtries=10)local-unif-sampling(σtries=100)local-unif-sampling(σtries=1000)high-prob-sampling(σtries=10)high-prob-sampling(σtries=100)high-prob-sampling(σtries=1000)0.00.20.40.60.81.0ε0.50.60.70.80.91.0θ0.00.20.40.60.81.0ε0.50.60.70.80.91.0θ0.20.40.60.81.0(cid:15)0.50.60.70.80.91.0θsort-by-degreepeel-by-degreesort-by-neighborhoodpeel-by-neighborhoodpuv-threshold,τ=0.5puv-threshold,τ=0.6puv-threshold,τ=0.7puv-threshold,τ=0.8sort-by-puv-sum,|C|=1sort-by-puv-sum,|C|=5sort-by-puv-sum,|C|=1013

which will lead to a faster convergence of estimates of pu,v
(probability that node u is arrived earlier than node v), for all
the nodes u and v in the network, to the true values. From a
theoretical perspective, there remains an interesting question
of ﬁnding bounds on the convergence speed of estimates of
pu,v with various importance sampling distributions. One can
also look into clever bookkeeping techniques which will result
in reducing the computation time of a single path in our
sequential importance sampling algorithm. Another direction
is the application of the proposed framework and solution to
other types of random network models that not only involve
only the addition of vertices and edges, but also deletion of
them.

REFERENCES

[1] S. E. Schaeffer, “Graph clustering,” Computer Science Review, vol. 1,

no. 1, pp. 27–64, 2007.

[2] T. Łuczak, A. Magner, and W. Szpankowski, “Asymmetry and structural
information in preferential attachment graphs,” Random Structures and
Algorithms, pp. 1–23, 2019.

[3] K. Turowski, A. Magner, and W. Szpankowski, “Compression of Dy-
namic Graphs Generated by a Duplication Model,” in 56th Annual Aller-
ton Conference on Communication, Control, and Computing, Allerton
2018, Monticello, IL, USA, October 2-5, 2018, 2018, pp. 1089–1096.

[4] M. Srivastava, O. Simakov, J. Chapman, B. Fahey, M. E. Gauthier,
T. Mitros, G. S. Richards, C. Conaco, M. Dacre, U. Hellsten et al.,
“The amphimedon queenslandica genome and the evolution of animal
complexity,” Nature, vol. 466, no. 7307, p. 720, 2010.

[5] J. K. Sreedharan, A. Magner, A. Grama, and W. Szpankowski, “Inferring
temporal information from a snapshot of a dynamic network,” Scientiﬁc
Reports, vol. 9, no. 1, p. 3057, 2019.

[6] J. H. Kim, B. Sudakov, and V. Vu, “On the asymmetry of random regular
graphs and random graphs,” Random Structures & Algorithms, vol. 21,
no. 3-4, pp. 216–224, 2002.

[7] K. Turowski and W. Szpankowski, “Towards degree distribution of
duplication graph models,” 2019, https://www.cs.purdue.edu/homes/spa/
papers/random19.pdf.

[8] A. Frieze, K. Turowski, and W. Szpankowski, “Degree distribution
for duplication-divergence graphs: Large deviations,” 2020, to appear
in Proceedings of WG 2020: 46th International Workshop on Graph-
Theoretic Concepts in Computer Science.

[9] A. Loukas and P. Vandergheynst, “Spectrally approximating large graphs
with smaller graphs,” in International Conference on Machine Learning,
Stockholm, Sweden, 2018, pp. 3243–3252.

[10] F. Liu, D. Choi, L. Xie, and K. Roeder, “Global spectral clustering in
dynamic networks,” Proceedings of the National Academy of Sciences,
vol. 115, no. 5, pp. 927–932, 2018.

[11] R. Görke, P. Maillard, C. Staudt, and D. Wagner, “Modularity-driven
clustering of dynamic graphs,” in International Symposium on Experi-
mental Algorithms. Berlin, Heidelberg: Springer, 2010, pp. 436–448.
[12] D. Greene, D. Doyle, and P. Cunningham, “Tracking the evolution
of communities in dynamic social networks,” in 2010 International
Conference on Advances in Social Networks Analysis and Mining.
Washington, DC, USA: IEEE, 2010, pp. 176–183.

[13] E. Bair, “Semi-supervised clustering methods,” Wiley Interdisciplinary

Reviews: Computational Statistics, vol. 5, no. 5, pp. 349–361, 2013.

[14] S. Basu, A. Banerjee, and R. Mooney, “Semi-supervised clustering by
seeding,” in International Conference on Machine Learning.
San
Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2002, pp. 27–
34.

[15] B. Kulis, S. Basu, I. Dhillon, and R. Mooney, “Semi-supervised graph
clustering: a kernel approach,” Machine Learning, vol. 74, no. 1, pp.
1–22, 2009.

[16] S. Li, K. P. Choi, T. Wu, and L. Zhang, “Maximum likelihood inference
of the evolutionary history of a ppi network from the duplication history
of its proteins,” IEEE/ACM Transactions on Computational Biology and
Bioinformatics (TCBB), vol. 10, no. 6, pp. 1412–1421, 2013.

[17] S. Navlakha and C. Kingsford, “Network archaeology: uncovering
ancient networks from present-day interactions,” PLoS Computational
Biology, vol. 7, no. 4, p. e1001119, 2011.

(a) ArXiv network

(b) Simple English Wikipedia network

(c) CollegeMsg network

Fig. 9: Real-world networks: results of semi-supervised learn-
ing

Greedy algorithm

δ

θ

δ

θ

δ

θ

ArXiv

Wikipedia

CollegeMsg

sort-by-degree
0.47
peel-by-degree
0.46
sort-by-neighborhood 0.0001 0.51
peel-by-neighborhood
0.50

0.98
0.98

0.13

0.96 0.59
0.96 0.59
0.03 0.60
0.80 0.593

0.92 0.63
0.92 0.63
0.01 0.78
0.75 0.61

TABLE IV: Real-world networks: results of greedy algorithms.

optimum clustering quality, and proposed several algorithms
that perform well and capable of including some external
information about the precedence of vertices in their arrival
to the network.

Further work in our proposed framework can go in several
directions. For example, one can explore various ways to speed
up the algorithms presented in this work. This can be accom-
plished by ﬁnding a good importance sampling distribution,

10−410−310−210−1α0.50.60.70.80.91.0θsort-by-puv-sum,|C|=1puv-threshold,τ=0.510−410−310−210−1α0.50.60.70.80.91.0θsort-by-puv-sum,|C|=1puv-threshold,τ=0.510−410−310−210−1α0.50.60.70.80.91.0θsort-by-puv-sum,|C|=1puv-threshold,τ=0.514

[18] K. Turowski, J. K. Sreedharan, and W. Szpankowski, “Temporal ordered
clustering in dynamic networks,” 2020, to appear in Proceedings of IEEE
International Symposium on Information Theory.
[19] S. Boyd and L. Vandenberghe, Convex Optimization.

Cambridge

University Press, 2004.

[20] R. Pastor-Satorras, E. Smith, and R. V. Solé, “Evolving protein interac-
tion networks through gene duplication,” Journal of Theoretical Biology,
vol. 222, no. 2, pp. 199–210, 2003.

[21] R. Van Der Hofstad, Random graphs and complex networks. Cam-

bridge: Cambridge University Press, 2016, vol. 1.

[22] J. Leskovec and A. Krevl, “SNAP Datasets: Stanford large network

dataset collection,” http://snap.stanford.edu/data, Jun. 2014.

[23] J. K. Sreedharan, K. Turowski, and W. Szpankowski, “Revisiting pa-
rameter estimation in biological networks: Inﬂuence of symmetries,”
IEEE/ACM Transactions on Computational Biology and Bioinformatics,
2020, http://doi.org/10.1109/TCBB.2020.2980260.

Jithin K. Sreedharan is a Postdoctoral Research
Associate at the NSF Center for Science of Informa-
tion and Dept. of Computer Science in Purdue Uni-
versity. He received his Ph.D. in computer science
from INRIA, France, in 2017 with a fellowship from
INRIA-Bell Labs joint lab. Before that, he ﬁnished
M.S. from Indian Institute of Science (IISc), Ban-
galore, in 2013, and received the best thesis award.
His current works focus on data mining algorithms
for large networks with probabilistic guarantees,
statistical modeling and inference on networks, and

distributed techniques for analyzing big matrices.

Krzysztof Turowski is currently assistant professor
at
the Theoretical Computer Science Department
at the Jagiellonian University, Krakow, Poland. He
received his MS and PhD degrees from Gdansk
University of Technology, Poland in 2011 and 2015,
respectively, both in computer science. From 2010 to
2016 he was employed at the Department of Algo-
rithms and System Modelling at Gdansk University
of Technology and from 2016 to 2018 he worked at
Google as a software developer for Google Compute
Engine. From 2018 to 2019 he was a Postdoctoral
Research Scholar in the NSF Center for Science of Information at Purdue
University. His research interests include graph theory (especially various
models of graph coloring), analysis of algorithms and information theory.

Wojciech Szpankowski is Saul Rosen Distinguished
Professor of Computer Science at Purdue Univer-
sity where he teaches and conducts research in
analysis of algorithms, information theory, analytic
combinatorics, data science, random structures, and
stability problems of distributed systems. He held
several Visiting Professor/Scholar positions, includ-
ing McGill University, INRIA, France, Stanford,
Hewlett-Packard Labs, Universite de Versailles, Uni-
versity of Canterbury, New Zealand, Ecole Polytech-
nique, France, the Newton Institute, Cambridge, UK,
ETH, Zurich, and Gdansk University of Technology, Poland. He is a Fellow
of IEEE, and the Erskine Fellow. In 2010 he received the Humboldt Research
Award and in 2015 the Inaugural Arden L. Bement Jr. Award. He is also the
recipient of 2020 Flajolet Lecture Prize. He published two books: “Average
Case Analysis of Algorithms on Sequences”, John Wiley & Sons, 2001, and
“Analytic Pattern Matching: From DNA to Twitter”, Cambridge, 2015. In
2008 he launched the interdisciplinary Institute for Science of Information,
and in 2010 he became the Director of the newly established NSF Science
and Technology Center for Science of Information.

