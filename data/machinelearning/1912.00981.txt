Proving Data-Poisoning Robustness in Decision Trees

Samuel Drews
University of Wisconsin-Madison
Madison, WI, USA
sedrews@wisc.edu

Aws Albarghouthi
University of Wisconsin-Madison
Madison, WI, USA
aws@cs.wisc.edu

Loris D’Antoni
University of Wisconsin-Madison
Madison, WI, USA
loris@cs.wisc.edu

0
2
0
2

n
u
J

3
2

]
L
P
.
s
c
[

2
v
1
8
9
0
0
.
2
1
9
1
:
v
i
X
r
a

Abstract
Machine learning models are brittle, and small changes in
the training data can result in different predictions. We study
the problem of proving that a prediction is robust to data poi-
soning, where an attacker can inject a number of malicious
elements into the training set to influence the learned model.
We target decision-tree models, a popular and simple class of
machine learning models that underlies many complex learn-
ing techniques. We present a sound verification technique
based on abstract interpretation and implement it in a tool
called Antidote. Antidote abstractly trains decision trees for
an intractably large space of possible poisoned datasets. Due
to the soundness of our abstraction, Antidote can produce
proofs that, for a given input, the corresponding prediction
would not have changed had the training set been tampered
with or not. We demonstrate the effectiveness of Antidote
on a number of popular datasets.

CCS Concepts: • Software and its engineering → Auto-
mated static analysis; • Security and privacy → Formal
methods and theory of security; • Computing methodolo-
gies → Classification and regression trees.

Keywords: Abstract Interpretation, Adversarial Machine Learn-
ing, Decision Trees, Poisoning, Robustness

1 Introduction
Artificial intelligence, in the form of machine learning (ML),
is rapidly transforming the world as we know it. Today, ML
is responsible for an ever-growing spectrum of sensitive
decisions—from loan decisions, to diagnosing diseases, to
autonomous driving. Many recent works have shown how
ML models are brittle [3, 7, 28, 29, 32], and with ML spreading
across many industries, the issue of robustness in ML models
has taken center stage. The research field that deals with
studying robustness of ML models is referred to as adversarial
machine learning. In this field, researchers have proposed
many definitions that try to capture robustness to different
adversaries. The majority of these works have focused on
verifying or improving the model’s robustness to test-time
attacks [2, 14, 15, 27, 31], where an adversary can craft small
perturbations to input examples that fool the ML model
into changing its prediction, e.g., making the model think a
picture of a cat is that of a zebra [5].

Data-Poisoning Robustness. This paper focuses on veri-
fying data-poisoning robustness, which captures how robust

1

Figure 1. High-level overview of our approach

a training algorithm L is to variations in a given training set
T . Intuitively, applying L to the training set T results in a
classifier (model) M, and in this paper we are interested in
how the trained model varies when producing perturbations
of the input training set T .

The idea is that an adversary can produce slight modifica-
tions of the training set, e.g., by supplying a small amount of
malicious training points, to influence the produced model
and its predictions. This attack model is possible when data
is curated, for example, via crowdsourcing or from online
repositories, where attackers can try to add malicious ele-
ments to the training data. For instance, Xiao et al. [34] con-
sider adding malicious training points to affect a malware
detection model; similarly, Chen et al. [7] consider adding a
small number of images to bypass a facial recognition model.
A perturbed set ∆(T ) defines a set of neighboring datasets
the adversary could have attacked to yield the training set
T . To define what it means for the training algorithm L to
be robust, we need to measure how the model learned by L
varies when modifying the training set. Let us say we have an
input example x—e.g., a test example—and its classification
label is M(x) = y, where M = L(T ) is the model learned from
the training set T . We say that x is robust to poisoning if
and only if for all T ′ ∈ ∆(T ), we have L(T ′)(x) = y; that is,

Abstractlearning + inferenceTrainingdataset TLearninginput xPredicted label of xAbstract dataset Tinput xLabel of x&Proof of robustnessSet of training sets that attacker may have poisoned Our approach to proving data-poisoning robustness Standard machine-learning pipelineInferencemodelDon’t knowimpreciseabstraction 
 
 
 
 
 
no matter what dataset T ′ ∈ ∆(T ) we use to construct the
model M ′ = L(T ′), we want M ′ to always return the same
classification y on the input x. To be clear, in this paper we are
concerned with a local robustness property: we are proving
the invariance of individual test points’ classifications to
changes in the training set.

Verification Challenges. Data-poisoning robustness has
been studied extensively [3, 19, 21, 35, 36]. This body of work
has demonstrated data-poisoning attacks—i.e., modifications
to training sets—that can degrade classifier accuracy, some-
times dramatically, or force certain predictions on specific
inputs. While some defenses have been proposed against
specific attacks [16, 28], we are not aware of any technique
that can formally verify that a given learning algorithm is
robust to perturbations to a given training set. Verifying data-
poisoning robustness of a given learner requires solving a
number of challenges:

1. The datasets over which the learner operates are typically
large (thousands of elements). Even when considering
simple poisoning attacks, the number of modified train-
ing sets we need to consider can be intractably large to
represent and explore explicitly.

2. Because learners are complicated programs that employ
complex metrics (e.g., entropy and loss functions), their
verification requires new specialized techniques.

Our Approach. We focus on the problem of verifying data-
poisoning robustness for decision-tree learners. We choose
decision trees because (i) they are widely used interpretable
models; (ii) they are used in industrial models like random
forests and XGBoost [6]; (iii) decision-tree-learning has been
shown to be unstable to training-set perturbation [13, 18, 22,
30]; and (iv) decision-tree-learning algorithms are typically
deterministic—e.g., they do not employ stochastic optimiza-
tion techniques—making them amenable to verification.

We present Antidote, a tool for verifying data-poisoning
robustness of decision-tree learners. At a high level, Antidote
takes as input a training set T and an input x, symboli-
cally constructs every tree built by a particular decision-tree
learner L on every possible variation of T in ∆(T ), and ap-
plies all those trees to x. If all the trees agree on the label of
x, then we know that x is robust to poisoning T . (See Fig-
ure 1 for an overview.) Antidote addresses the two challenges
highlighted above as follows.

To address the first challenge of training on a combinato-
rially large number of datasets, Antidote employs a novel ab-
stract domain for concisely representing sets of datasets. An-
tidote is a sound abstract interpretation of standard decision-
tree learning algorithms: Instead of constructing a single
decision tree, it implicitly constructs an overapproximation
of all decision trees for every training set in ∆(T ).

Samuel Drews, Aws Albarghouthi, and Loris D’Antoni

To address the second challenge, Antidote has to soundly
approximate how decision-tree learning algorithms propa-
gate entropy computations across a model. Antidote takes
advantage of the following observation: every input x only
traverses a single root-to-leaf trace in the learned decision
tree—i.e., the sequence of predicates that affects the decision
for x. This observation allows Antidote to build a simpler
abstraction that only needs to track the predicates and train-
ing elements affecting the decision for x at a given node
in the tree, instead of across the entire tree. We call this a
trace-based view of decision-tree learning, where we are only
concerned with the tree trace(s) traversed by x.

Evaluation. We evaluated Antidote on a number of real
datasets from the literature. Antidote successfully proves
robustness for many test inputs, even in cases where the
learner is allowed to build a complex decision tree and the
attacker is allowed to contribute more than 1% of the points
in the training set. For instance, Antidote can, in around 1
minute, prove robustness for some test inputs of the MNIST-
1-7 [3, 28] datasets for cases where the attacker may have
contributed up to 192 malicious elements to the dataset. A
naïve enumeration approach would have to construct around
10432 models to prove the same property!

Contributions. We summarize our contributions as follows:

• Antidote: the first sound technique for verifying data-

poisoning robustness for decision-tree learners (§2).

• A trace-based view of decision-tree learning as a stand-
alone algorithm that allows us to sidestep the challenging
problem of reasoning about the set of all possible output
trees a learner can output on different datasets (§3).

• An abstract domain that concisely encodes sets of perturbed
datasets and the abstract transformers necessary to verify
robustness of a decision-tree learner (§4 and §5).

• An evaluation of Antidote on five representative datasets
from the literature. Antidote can prove poisoning robust-
ness for all datasets in cases where an enumeration ap-
proach would be doomed to fail (§6).

Proofs of theorems and figures for additional benchmarks

are available in Appendix A.

2 Overview
In this section, we give an overview of decision-tree learn-
ing, the poisoning-robustness problem, and motivate our
abstraction-based proof technique.

Decision-Tree Learning. Consider the dataset Tbw at the
top of Figure 2. It is comprised of 13 elements with a single
numerical feature. Each element is labeled as a white (empty)
or black (solid) circle. We use x to denote the feature value
of each element. Our goal is to construct a decision tree that
classifies a given number into white or black.

2

Proving Data-Poisoning Robustness in Decision Trees

Figure 3. Example MNIST-1-7 digit that is proven poisoning-
robust by Antidote.

Unfortunately, this approach is intractable. Even for our
tiny example, we have to train 92 trees ((cid:0)13
(cid:1) + 1). For a
2
dataset of 1000 elements and a poisoning of up to 10 elements,
we have ∼1023 possibilities.

(cid:1) + (cid:0)13
1

An Abstract Approach. Our approach to efficiently prov-
ing poisoning robustness exploits a number of insights. First,
we can perform decision-tree learning abstractly on a sym-
bolic set of training sets, without having to deal with a combi-
natorial explosion. The idea is that the operations in decision-
tree learning, e.g., selecting a predicate and splitting the
dataset, do not need to look at every concrete element of a
dataset, but at aggregate statistics (counts).

Recall our running example in Figure 2. Let us say that
up to two elements have been removed. No matter what
two elements you choose, the predicate x ⩽ 10 remains
one that gives a best split for the dataset. In cases of ties
between predicates, our algorithm abstractly represents all
possible splits. For each predicate, we can symbolically com-
pute best- and worst-case scores in the presence of poisoning
as an interval. Similarly, we can also compute an interval
that overapproximates the set of possible classification prob-
abilities. For instance, in the left branch of the decision-tree,
the probability will be [0.71, 1] instead of 0.78 (or 7/9). The
best case probability of 1 is when we drop the black points 0
and 4; the worst-case probability of 0.71 (or 5/7) is when we
drop any two white points.

The next insight that enables our approach is that we
do not need to explicitly build the tree. Since our goal is to
prove robustness of a single input point, which effectively
takes a single trace through the tree, we mainly need to keep
track of the abstract training sets as they propagate along
those traces. This insight drastically simplifies our approach;
otherwise, we would need to somehow abstractly represent
sets of elements of a tree data structure, a non-trivial problem
in program analysis.

Abstraction and Imprecision. We note that our approach
is sound but necessarily incomplete; that is, when our ap-
proach returns “robust” the answer is correct, but there are
robust instances for which our approach will not be able to
prove robustness. The are numerous sources of imprecision
due to overapproximation, for example, we use the intervals

Figure 2. Illustrative example

For simplicity, we assume that we can only build trees of
depth 1, like the one shown at the bottom Figure 2. At each
step of building a decision tree, the learning algorithm is
looking for a predicate φ with the best score, with the goal
of splitting the dataset into two pieces with least diversity,
i.e., most elements have the same class (formally defined
usually using a notion of entropy). This is what we see in
our example: using the predicate x ⩽ 10, we split the dataset
into two sets, one that is mostly white (left) and one that is
completely black (right). This is the best split we can have
for our data, assuming we can only pick predicates of the
form x ⩽ c, for an integer c.1

Given a new element for a classification, we check if it is
⩽ 10, in which case we say it is white with probability 7/9—
i.e., the fraction of white elements such that ⩽ 10. Otherwise,
if the element is > 10, we say it is black with probability 1.

Data-Poisoning Robustness. Imagine we want to classify
an input x but want to make sure the classification would not
have changed had the training data been slightly different.
For example, maybe some percentage of the data was mali-
ciously added by an attacker to sway the learning algorithm,
a problem known as data poisoning. Our goal is to check
whether the classification of x is robust to data poisoning.

A Naïve Approach. Consider our running example and
imagine we want to classify the number 5. Additionally, we
want to prove that removing up to two elements from the
training set would not change the classification of 5—i.e., we
assume that up to ∼15% (or 2/13) of the dataset is contributed
maliciously. The naïve way to do this is to consider every
possible training dataset with up to two elements removed
and retrain the decision tree. If all trees classify the input 5 as
white, the classification is robust to this level of poisoning.

1 Note that, while the set of predicates x ⩽ c is infinite, for this dataset
(and in general for any dataset), there exists only finitely many inequivalent
predicates—e.g., x ⩽ 4 and x ⩽ 5 split the dataset into the same two sets.

3

012347891011121314x ≤ 10x > 10Training setDecision treeWhite with probability 7/9Black with probability 1Samuel Drews, Aws Albarghouthi, and Loris D’Antoni

domain (or disjunctive intervals) to capture real-valued en-
tropy calculations of different training set splits, as well as
the final probability of classification.

T of size |T | − 10. If an input x is robust for this definition of
∆(T ), then no matter whether the attacker has contributed 10
training items or not, the classification of x does not change.

An Involved Example. To further illustrate our technique,
we preview one of our experiments. We applied our approach
to the MNIST-1-7 dataset, which has been used to study
data-poisoning for deep neural networks [28] and support
vector machines [3]. In our experiments, we checked whether
Antidote could prove data-poisoning for the inputs used in
the same dataset when training a decision tree. For example,
when applying Antidote to the image of the digit in Figure 3,
Antidote proves that it is poisoning robust (always classified
as a seven) for up to 192 poisoned elements in 90 seconds.
This is equivalent to training on ∼10432 datasets!

3 Poisoning and Decision Tree Learning
In this section, we begin by formally defining the data-
poisoning-robustness problem. Then, we present a trace-based
view of decision-tree learning, which will pave the way for
a poisoning-robustness proof technique.

3.1 The Poisoning Robustness Problem

In a typical supervised learning setting, we are given a learn-
ing algorithm L and a training set T ⊆ X × Y comprised
of elements of some set X, each with its classification label
from a finite set of classes Y. Applying L to T results in a
classifier (or model): M : X → Y. For now, we assume that
both the learning algorithm L and the models it learns are
deterministic functions.2

A perturbed set ∆(T ) ⊆ 2X×Y defines a set of possible
neighboring datasets of T . Our robustness definitions are
relative to some given perturbation ∆. (In Section 4.1, we
define a specific perturbed set that captures a particular form
of data poisoning.)

Definition 3.1 (Poisoning Robustness). Fix a learning al-
gorithm L, a training set T , and let ∆(T ) be a perturbed set.
Given an element x ∈ X, we say that x is robust to poisoning
T if and only if

∀T ′ ∈ ∆(T ). L(T ′)(x) = L(T )(x)

When T and ∆ are clear from context, we will simply say
that x is robust.

In other words, no matter what dataset T ′ ∈ ∆(T ) we use
to construct the model M = L(T ′), we want M to always
return the same classification for x. returned for x by the
model L(T ) learned on the original training set T .

Example 3.2. Imagine we suspect that an attacker has con-
tributed 10 training points to T , but we do not know which
ones. We can define ∆(T ) to be T as well as every subset of

2Our approach, however, needs to handle non-determinism in decision-tree
learning, which arises when breaking ties for choosing predicates with
equal scores and choosing labels for classes with equal probabilities.

4

3.2 Decision Trees: A Trace-Based View

We now formally define decision trees. We will formalize a
tree as the set of traces from the root to each of the leaves.
As we will see, this trace-based view will help enable our
proof technique. The idea of representing an already-learned
decision tree as a set of traces is not new and has often been
explored in the context of extracting interpretable rules from
decision trees [23].

A decision tree R is a finite set of traces, where each trace is
a tuple (σ , y) such that σ is a sequence of Boolean predicates
and y ∈ Y is the classification.

Semantically, a tree R is a function in X → Y. Given an
input x ∈ X, applying R(x) results in a classification y from
the trace (σ , y) ∈ R where x satisfies all the predicates in the
sequence σ = [φ1, . . . , φn], that is, (cid:211)n
i=1 x |= φi is true. We
say a tree R is well-formed if for every x ∈ X there exists
exactly one trace (σ , y) ∈ R such that x satisfies all predicates
in σ . In the following we assume all trees are well-formed.

Example 3.3 (Decision tree traces). Consider the decision-
tree in Figure 2. It contains two traces, each with a sequence
of predicates containing a single predicate: ([x ⩽ 10], white)
and ([x > 10], black).

3.3 Decision-Tree Learning: A Trace-Based View

We now present a simple decision-tree learning algorithm,
DTrace. Then, in Section 4, we abstractly interpret DTrace
with the goal of proving poisoning robustness.

One of our key insights is that we do not need to explicitly
represent the learned trees (i.e., the set of all traces), since
our goal is to prove robustness of a single input point, which
effectively takes a single trace through the tree. Therefore, in
this section, we will define a trace-based decision-tree learn-
ing algorithm. This is inspired by standard algorithms—like
CART [4], ID3 [24], and C4.5 [25]—but it is input-directed, in
the sense that it only builds the trace of the tree that a given
input x will actually traverse.

A Trace-Based Learner. Our trace-based learner DTrace is
shown in Figure 4. It takes a training set T and an input x
and computes the trace traversed by x in the tree learned on
T . Intuitively, if we compute the set of all traces DTrace(T , x)
for each x ∈ T , we get the full tree, the one that we would
have traditionally learned for T .

The learner DTrace repeats two core operations: (i) se-
lecting a predicate φ with which to split the dataset (using
bestSplit) and (ii) removing elements of the training set based
on whether they satisfy the predicate φ (depending on x, us-
ing filter).3 The number of times the loop is repeated (d) is the

3Note that (ii) is what distinguishes our trace-based learning from conven-
tional learning of a full tree. (i) selects predicates that would comprise the

Proving Data-Poisoning Robustness in Decision Trees

Input: training set T and input x ∈ X
Initialize: φ ← ⋄, σ ← empty trace
repeat d times

if ent(T ) = 0 then return
φ ← bestSplit(T )
if φ = ⋄ then return
T ← filter(T , φ, x)
if x |= φ then σ ← σφ else σ ← σ ¬φ

Output: argmaxi ∈[1,k ] pi , where cprob(T ) = ⟨p1, . . . , pk ⟩

Figure 4. Trace-based decision-tree learner DTrace

maximum depth of the trace that is constructed. Throughout,
we assume a fixed set of classes Y = {1, . . . , k}.

The mutable state of DTrace is the triple (T , φ, σ ):

• T is the training set, which will keep getting refined (by

dropping elements) as the trace is constructed.

• φ is the most recent predicate along the trace, which is

initially undefined (denoted by ⋄).

• σ is the sequence of predicates along the trace, which is

initially empty.

Predicate Selection. We assume that DTrace is equipped
with a finite set of predicates Φ with which it can construct
a decision-tree classifier; each predicate in Φ is a Boolean
function in X → B.

bestSplit(T ) computes a predicate φ⋆ ∈ Φ that splits the
current dataset T —usually minimizing a notion of entropy.
Ideally, the learning algorithm would consider every pos-
sible sequence of predicates to partition a dataset in order
to arrive at an optimal classifier. For efficiency, a decision-
tree-learning algorithms does this greedily: it selects the
best predicate it can find for a single split and moves on to
the next split. To perform this greedy choice, it measures
how diverse the two datasets resulting from the split are. We
formalize this below:

We use T ↓φ to denote the subset of T that satisfies φ, i.e.,
T ↓φ = {(x, y) ∈ T | x |= φ}
Let Φ′ be the set of all predicates that do not trivially split
the dataset: Φ′ = {φ ∈ Φ | T ↓φ (cid:44) ∅ ∧ T ↓φ (cid:44) T }. Finally,
bestSplit(T ) is defined as follows:
bestSplit(T ) = argmin

score(T , φ)

φ ∈Φ′
where score(T , φ) = |T ↓φ | · ent(T ↓φ ) + |T ↓¬φ | · ent(T ↓¬φ ).
Informally, bestSplit(T ) is the predicate that splits T into
two sets with the lowest entropy, as defined by the function
ent shown in Figure 5. Formally, ent computes Gini impu-
rity, which is used, for instance, in the CART algorithm [4].

tree, while (ii) directs us to recurse only along the path that the specific x
would take, as opposed to recursing down both (and without affecting how
predicates in the tree are selected).

5

k
(cid:213)

ent(T ) =

pi (1 − pi ), where cprob(T ) = ⟨p1, . . . , pk ⟩

cprob(T ) =

i=1
(cid:28) |{(x, y) ∈ T | y = i}|
|T |

(cid:29)

i ∈[1,k]

Figure 5. Auxiliary operator definitions. ent is Gini impurity;
cprob returns a vector of classification probabilities, one
element for each class i ∈ [1, k].

Note that if Φ′ = ∅, we assume bestSplit(T ) is undefined (re-
turns ⋄). Further, if multiple predicates are possible values of
bestSplit(T ), we assume one is returned nondeterministically.
Later, in Section 4, our abstract interpretation of DTrace will
actually capture all possible predicates in the case of a tie.

Example 3.4. Recall our example from Section 2 and Fig-
ure 2. For readability, we use T instead of Tbw for the name
of the dataset. Let us compute score(T , φ), where φ is x ⩽
10. We have |T ↓φ | = 9 and |T ↓¬φ | = 4. For the classifi-
cation probabilities, defined by cprob (Figure 5), we have
cprob(T ↓φ ) = ⟨7/9, 2/9⟩ and cprob(T ↓¬φ ) = ⟨0, 1⟩ assuming
the first element represents white classification; e.g., in T ↓φ ,
there’s a 7/9 chance of being classified as white. For ent, we
have ent(T ↓φ ) ≈ 0.35 and ent(T ↓¬φ ) = 0. Since T ↓φ is solely
composed of black points, its Gini impurity is 0.

The score of x ⩽ 10 is therefore ∼3.1. For the predicate x ⩽
11, we get the higher (worse) score of ∼3.2, as it generates a
more diverse split.

Filtering the Dataset. The operator filter removes elements
of T that evaluate differently than x on φ. Formally,

filter(T , φ, x) =

(cid:40)
T ↓φ
T ↓¬φ

if x |= φ
otherwise

Learner Result. When DTrace terminates in a state
(Tr , φr , σr ), we can read the classification of x as the class i
with the highest number of training elements in Tr .

Using cprob, in Figure 5, we compute the probability of
each class i for a training set T as a vector of probabilities.
Finally, DTrace returns the class with the highest probability:

argmax
i ∈[1,k ]

pi

where cprob(Tr ) = ⟨p1, . . . , pk ⟩

As before, in case of a tie in probabilities, we assume a non-
deterministic choice.

Example 3.5. Following the computation from Ex. 3.4,
DTrace(T , 18) terminates in state (T ↓x >10, x ⩽ 10, [x > 10]).
Point 18 is associated with the trace [x > 10] and is classified
as black because cprob(T ↓x >10) = ⟨0, 1⟩.

4 Abstractions of Poisoned Semantics
In this section, we begin by defining a data-poisoning model
in which an attacker contributes a number of malicious train-
ing items. Then, we demonstrate how to apply the trace-
based learner DTrace to abstract sets of training sets, allowing
us to efficiently prove poisoning-robustness.

4.1 The n-Poisoning Model
For our purposes, we will consider a poisoning model where
the attacker has contributed up to n elements of the training
set—we call it n-poisoning. Formally, given a training set
T and a natural number n ⩽ |T |, we define the following
perturbed set:

∆n(T ) = {T ′ ⊆ T :

|T \ T ′| ⩽ n}

In other words, ∆n(T ) captures every training set the attacker
could have possibly started from to arrive at T .

This definition of dataset poisoning matches many set-
tings studied in the literature [7, 28, 34]. The idea is that an
attacker has contributed a number of malicious data points
into the training set to influence the resulting classifier. For
example, Chen et al. [7] consider poisoning a facial recog-
nition model to enable bypassing authentication, and Xiao
et al. [34] consider poisoning a malware detector to allow
the attacker to install malware.

We do not know which n points inT are the malicious ones,
or if there are malicious points at all. Thus, the set ∆n(T )
captures every possible subset of T where we have removed
up to n (potentially malicious) elements. Our goal is to prove
that our classification is robust to up to n possible poisoned
points added by the attacker. So if we try every possible
dataset in ∆n(T ) and they all result in the same classification
on x, then x is robust regardless of the attacker’s potential
contribution.

(cid:1). So even for relatively
Observe that |∆n(T )| = (cid:205)n
i=1
small datasets and number n, the set of possibilities is mas-
sive, e.g., for MNIST-1-7 dataset (§6), for n = 50, we have
about 10141 possible training sets in ∆n(T ).

(cid:0) |T |
i

4.2 Abstract Domains for Verifying n-Poisoning
Our goal is to efficiently evaluate DTrace on an input x for
all possible training datasets in ∆n(T ). If all of them yield the
same classification y, then we know that x is a robust input.
Our insight is that we can abstractly interpret DTrace on a
symbolic set of training sets without having to fully expand
it into all of its possible concrete instantiations. This allows
us to train on an enormous number of datasets, which would
be impossible via enumeration.

Recall that the state of DTrace is (T , φ, σ ); for our purposes,
we do not have to consider the sequence of predicates σ , as
we are only interested in the final classification, which is a
function of T . In this section, we present the abstract domains
for each component of the learner’s state.

6

Samuel Drews, Aws Albarghouthi, and Loris D’Antoni

Abstract Training Sets. Abstracting training sets is the
main novelty of our technique. We use the abstract element
⟨T ′, n′⟩ to denote a set of training sets and it captures the
definition of ∆n′(T ′): For every training set T ′ and num-
ber n′, the concretization function is γ (⟨T ′, n′⟩) = ∆n′(T ′).
Therefore, we have that initially the abstraction function
α(∆n(T )) = ⟨T , n⟩ is precise. Note that an abstract element
⟨T ′, n′⟩ succinctly captures a large number of concrete sets,
∆n′(T ′). Further, all operations we perform on ⟨T ′, n′⟩ will
only modify T ′ and n′, without resorting to concretization.
We can define an efficient join operation on two elements

in the abstract domain4 as follows:

Definition 4.1 (Joins). Given two training sets T1,T2 and
n1, n2 ∈ N, ⟨T1, n1⟩ ⊔ ⟨T2, n2⟩ (cid:66) ⟨T ′, n′⟩ where T ′ = T1 ∪ T2
and n′ = max(|T1 \ T2| + n2, |T2 \ T1| + n1).

Notice that the join of two sets is an overapproximation
of the union of the two sets. The following proposition for-
malizes the soundness of this operation:

Proposition 4.2. For any T1, T2, n1, n2, the following holds:
γ (⟨T1, n1⟩) ∪ γ (⟨T2, n2⟩) ⊆ γ (⟨T1, n1⟩ ⊔ ⟨T2, n2⟩).

Example 4.3. For any training set T1, if we consider the
abstract sets ⟨T1, 2⟩ and ⟨T1, 3⟩, because the second set rep-
resents strictly more concrete training sets, we have

⟨T1, 2⟩ ⊔ ⟨T1, 3⟩ = ⟨T1, 3⟩
Now consider the training set T2 = {x1, x2}. We have
⟨T2, 2⟩ ⊔ ⟨T2 ∪ {x3}, 2⟩ = ⟨T2 ∪ {x3}, 3⟩

Notice how the join increased the poisoned elements from 2
to 3 to accommodate for the additional element x3.

Abstract Predicates and Numeric Values. When abstractly
interpreting what predicates the learner might choose for dif-
ferent training sets, we will need to abstractly represent sets
of possible predicates. Simply, a set of predicates is abstracted
precisely as the corresponding set of predicates Ψ—i.e., for
every set Ψ, we have α(Ψ) = Ψ and γ (Ψ) = Ψ. Moreover,
Ψ1 ⊔ Ψ2 = Ψ1 ∪ Ψ2. For certain operations, it will be handy
for Ψ to contain a special null predicate ⋄.

When abstractly interpreting numerical operations, like
cprob and ent, we will need to abstract sets of numerical val-
ues. We do so using the standard intervals abstract domain
(denoted [l, u]). For instance, α({0.2, 0.4, 0.6}) = [0.2, 0.6]

4Elements in the domain are ordered so that ⟨T1, n1 ⟩ ⊑ ⟨T2, n2 ⟩ if and only
if T1 ⊆ T2 ∧ n1 ⩽ n2 − |T2 \ T1 |. In the text, we define the concretization
function, a special case of the abstraction function, and the join operation;
note that we do not require an explicit meet operation for the purposes of
this paper—although one is well-defined:
⟨T1, n1 ⟩ ⊓ ⟨T2, n2 ⟩ (cid:66) if |T1 \ T2 | > n1 ∨ |T2 \ T1 | > n2 then ⊥

else ⟨T1 ∩ T2, min(n1 − |T1 \ T2 |, n2 − |T2 \ T1 |)⟩

Proving Data-Poisoning Robustness in Decision Trees

and γ ([0.2, 0.6]) = {x | 0.2 ⩽ x ⩽ 0.6}. The join of two inter-
vals is defined as [l1, u1] ⊔ [l2, u2] = [min(l1, l2), max(r1, r2)].
Interval arithmetic follows the standard definitions and we
thus elide it here.5

4.3 Abstract Learner DTrace#
We are now ready to define an abstract interpretation of the
semantics of our decision-tree learner, denoted DTrace#.
Abstract Domain. Recall that the state of DTrace is (T , φ, σ );
for our purposes, we do not have to consider the sequence of
predicates σ , as we are only interested in the final classifica-
tion, which is a function of T . Using the domains described
in Section 4.2, at each point in the learner, our abstract state
is a pair (⟨T ′, n′⟩, Ψ′) (i.e., in the product abstract domain)
that tracks the current set of training sets and the current
set of possible most recent predicates the algorithm has split
on (for all considered training sets).

When verifying n-poisoning for a training set T , the initial
abstract state of the learner will be the pair (⟨T , n⟩, {⋄}). In
the rest of the section, we define the abstract semantics (i.e.,
our abstract transformers) for all the operations performed
by DTrace#. For operations that only affect one element of the
state, we assume that the other component is left unchanged.

4.4 Abstract Semantics of Auxiliary Operators

We will begin by defining the abstract semantics of the aux-
iliary operations in the algorithm before proceeding to the
core operations, filter and bestSplit. This is because the aux-
iliary operators are simpler and highlight the nuances of our
abstraction.

Let us begin by considering ⟨T , n⟩↓#

φ , which is the abstract

analog of T ↓φ .

⟨T , n⟩↓#
φ

(cid:66) ⟨T ↓φ , min(n, |T ↓φ |)⟩

(1)

Simply, it removes elements not satisfying φ from T ; since
the size of T ↓φ can go below n, we take the minimum of the
two.

Proposition 4.4. Let T ′ ∈ γ (⟨T , n⟩). For any predicate φ, we
have T ′↓φ ∈ γ (⟨T , n⟩↓#

φ ).

Now consider cprob(T ), which returns a vector of proba-
bilities for different classes. Its abstract version returns an
interval for each probability, denoting the lower and upper

5While we choose intervals as our numerical abstract domain in this paper,
any numerical abstract domain could be used.

7

bounds based on the training sets in the abstract set:6

cprob#(⟨T , n⟩) (cid:66)

(cid:28) [max(0, ci − n), ci ]
[|T | − n, |T |]

(cid:29)

i ∈[1,k]

where ci = |{(x, i) ∈ T }|. In other words, for each class i, we
need to consider the best- and worst-case probability based
on removing n elements from the training set, as denoted by
the denominator and the numerator. Note that in the corner
case where n = |T |, we set cprob#(⟨T , n⟩) = ⟨[0, 1]⟩i ∈[1,k ].
Proposition 4.5. Let T ′ ∈ γ (⟨T , n⟩). Then,

cprob(T ′) ∈ γ (cid:0)cprob#(⟨T , n⟩)(cid:1)
where γ (cid:0)cprob#(⟨T , n⟩)(cid:1) is the set of all possible probability
vectors in the vector of intervals.

Example 4.6. Consider the training set on the left side of
the tree in Figure 2; call it Tℓ. It has 7 white elements and
2 black elements. cprob(Tℓ) = ⟨7/9, 2/9⟩, where the first
element is the white probability. cprob#(⟨Tℓ, 2⟩) produces
the vector ⟨[5/9, 1] , [0, 2/7]⟩. Notice the loss of precision in
the lower bound of the first element. If we remove two white
elements, we should get a probability of 5/7, but the interval
domain cannot capture the relation between the numerator
and denominator in the definition of cprob#.

The abstract version of the Gini impurity is identical to
the concrete one, except that it performs interval arithmetic:

ent#(T ) =

k
(cid:213)

i=1

ιi ([1, 1] − ιi ), where cprob#(T ) = ⟨ι1, . . . , ιk ⟩

Each term ιi denotes an interval.

4.5 Abstract Semantics of filter
We are now ready to define the abstract version of filter.
Since we are dealing with abstract training sets, as well as a
set of predicates Ψ, we need to consider for each φ ∈ Ψ all
cases where x |= φ or x |= ¬φ, and take the join of all the
resulting training sets (Definition 4.1). Let

Ψx = {φ ∈ Ψ | x |= φ} and Ψ¬x = {φ ∈ Ψ | x |= ¬φ}
6Note that this transformer can be more precise: for example, the interval
division as written is not guaranteed to be a subset of [0, 1], despite the
fact that all concrete values would be. Throughout this section, many of
the transformers are simply the “natural” lifting of numerical arithmetic
to interval arithmetic; while this may not be optimal, we do so to make
it easier to see the correctness of the approach (and to make proofs and
implementation straightforward).

In the case of cprob#, we can compute the optimal transformer inexpen-
sively: it is equivalent to write that cprob(T ) computes, for each class i ∈
[1, k], the average of the multiset Si = [if y = i then 1 else 0 | (x, y) ∈ T ].
We can then have cprob#(⟨T , n ⟩) perform a similar computation for each
component: let Li denote the m-many least elements of Si , and let Ui denote
the m-many greatest elements of Si , where m = |T | − n. These Li and Ui
exhibit extremal behavior of averaging, so we can directly compute the end-
points of the interval assigned to each class as [ 1
b ∈Ui b].
m
Note that our implementation used for the evaluation (Section 6) does
employ this optimal transformer for cprob#, while the other transformers
match what is presented.

b ∈Li b, 1
m

(cid:205)

(cid:205)

Then,

(cid:196)

(cid:196)

filter#(⟨T , n⟩, Ψ, x) (cid:66) (cid:169)
(cid:173)
φ ∈Ψx
(cid:171)
Proposition 4.7. Let T ′ ∈ γ (⟨T , n⟩) and φ ′ ∈ Ψ. Then,

⊔ (cid:169)
(cid:173)
φ ∈Ψ¬x
(cid:171)

⟨T , n⟩↓#
φ

(cid:170)
(cid:174)
(cid:172)

⟨T , n⟩↓#
¬φ

filter(T ′, φ ′, x) ∈ γ (cid:0)filter#(⟨T , n⟩, Ψ, x)(cid:1)

Samuel Drews, Aws Albarghouthi, and Loris D’Antoni

(cid:170)
(cid:174)
(cid:172)

φ3. This is because there is a chance that φ1, φ2 and φ3 are
indeed the predicates with the best score, but our abstraction
has lost too much precision for us to tell conclusively.

Let lb/ub be functions that return the lower/upper bound
of an interval. First, we define the lowest upper bound among
the abstract scores of the predicates in Φ as

Example 4.8. Consider the full dataset Tbw from Figure 2.
For readability, we write T instead of Tbw in the example.
Let x denote the input with numerical feature 4, and let
Ψ = {x ⩽ 10}. First, note that because Ψ¬x is the empty
set, the right-hand side of the result of applying the filter#
operator will be the bottom element ⟨∅, 0⟩ (i.e., the identity
element for ⊔). Then,
filter#(⟨T , 2⟩, Ψ, x) = ⟨T , 2⟩↓#

x ⩽10 ⊔ ⟨∅, 0⟩
= ⟨T ↓x ⩽10, 2⟩ ⊔ ⟨∅, 0⟩
= ⟨T ↓x ⩽10, 2⟩

(def. of filter#)
(def. of ⟨T , n⟩↓#
φ )
(def. of ⊔).

4.6 Abstract Semantics of bestSplit
We are now ready to define the abstract version of bestSplit.
We begin by defining bestSplit# without handling trivial
predicates, then we refine our definition.

Minimal Intervals. Recall that in the concrete case, bestSplit
returns a predicate that minimizes the function score(T , φ).
To lift bestSplit to the abstract semantics, we define score#,
which returns an interval, and what it means to be a mini-
mal interval—i.e., the interval corresponding to the abstract
minimal value of the objective function score#(T , φ).

Lifting score(T , φ) to score#(⟨T , n⟩, φ) can be done using
the sound transformers for the intermediary computations:

score#(⟨T , n⟩, φ) (cid:66) |⟨T , n⟩↓#

φ | · ent#(⟨T , n⟩↓#
φ )

+ |⟨T , n⟩↓#

¬φ | · ent#(⟨T , n⟩↓#

¬φ )

where |⟨T , n⟩| (cid:66) [|T | − n, |T |].

However, given a set of predicates Φ, bestSplit# must re-
turn the ones with the minimal scores. Before providing the
formal definition, we illustrate the idea with an example.
Example 4.9. Imagine a set of predicates Φ = {φ1, φ2, φ3, φ4}
with the following intervals for score#(⟨T , n⟩, φi ).

Notice that φ1 has the lowest upper bound for score (denoted
in red and named lubΦ). Therefore, we call score#(⟨T , n⟩, φ1)
the minimal interval with respect to Φ. bestSplit# returns
all the predicates whose scores overlap with the minimal
interval score#(⟨T , n⟩, φ1), which in this case are φ1, φ2, and

lubΦ = min
φ ∈Φ

ub(score#(⟨T , n⟩, φ))

We can now define the set of predicates whose score overlaps
with the minimal interval as:

{φ ∈ Φ | lb(score#(⟨T , n⟩, φ)) ⩽ lubΦ}

Dealing with Trivial Predicates. Our formulation above
considers the full set of predicates, Φ. To be more faithful to
the concrete semantics, bestSplit# needs to eliminate trivial
predicates from this set. In the concrete case, we only con-
sidered φ as a possible best split if φ performed a non-trivial
split on T , which we denoted φ ∈ Φ′. (Recall that a trivial
split of T is one that returns ∅ or T .)

This is a little tricky to lift to our abstract case, since a
predicate φ could non-trivially split some of the concrete
datasets but not others. We lift the set Φ′ in two ways:

• Universal predicates: the predicates that are non-trivial

splits for all concrete training sets in γ (⟨T , n⟩)7

Φ∀ = {φ ∈ Φ | ∅ (cid:60) γ (⟨T , n⟩↓#

φ ) ∧ ∅ (cid:60) γ (⟨T , n⟩↓#

¬φ )}

• Existential predicates: the predicates that are non-trivial
splits for at least one concrete training set in γ (⟨T , n⟩)

Φ∃ = {φ ∈ Φ | ⟨∅, ·⟩ (cid:44) ⟨T , n⟩↓#

¬φ }
Finally, the definition of bestSplit# considers two cases:

φ ∧ ⟨∅, ·⟩ (cid:44) ⟨T , n⟩↓#

bestSplit#(⟨T , n⟩) (cid:66) if Φ∀ = ∅ then Φ∃ ∪ {⋄} else

{φ ∈ Φ∃ : lb(score#(⟨T , n⟩, φ)) ⩽ lubΦ∀ }

The first case captures when no single predicate is non-
trivial for all sets: we then return all predicates that succeed
on at least one training set in ⟨T , n⟩, since we cannot be
sure one is strictly better than another. To be sound, we
also assume the cause of Φ∀ being empty is a particular
concrete training set for which every predicate forms a trivial
split, hence we include ⋄ as a possibility. The second case
corresponds to returning the predicates with minimal scores.

Lemma 4.10. Let T ′ ∈ γ (⟨T , n⟩). Then,

bestSplit(T ′) ∈ γ (bestSplit#(⟨T , n⟩))

7Note that checking ∅ (cid:60) γ (⟨T , n ⟩) is equivalent to checking n (cid:44) |T |.

8

lowest upper boundscore#(hT,ni,j1)<latexit sha1_base64="7dILqr3bj85cP1GWEtMkhp4XvGU=">AAADYHicbVLNjtMwEPa2/JTysy0cQHCxqCp1pVU3KQf2AlSCAxekItruSk22cpxpataxg+20dKM8BycehSs8A1ceAnHESbsS7TKSrU/zzYxnPk+QcKaN4/zcq1SvXb9xs3arfvvO3Xv7jeb9sZapojCikkt1GhANnAkYGWY4nCYKSBxwOAnOXxf8yQKUZlIMzSoBPyaRYDNGibGuacP1YmLmepZpKhXkZ16r43EiIg54eCiwp0p8iL0FUcmcTd2DaaPldJ3S8FXgbkDr1Z+L33369WwwbVYeeqGkaQzCUE60nrhOYvyMKMMoh7zupRoSQs9JBBMLBYlB+1k5W47b1hPimVT2CINL778ZGYm1XsWBjSwn2eUK5/+4SWpmx37GRJIaEHT90Czl2EhcCIVDpoAavrKAUMVsr5jOiSLUWDm3KmnqZ0X9hFzIvF73ir/QxS+EmZWjl+NLa+MB4VZ4IbEACDWOreaY20AmItzRtiDgAMwSQOCyysHWQ0PXzwoVin635kxY4c53uiIcwhdO99jxszfwkYzTD0Tod1LI7cBwwRK9Ef3zWvV623b1CR/hyN5LZuaYyyUoHBCFE6sB51AK9bLuKRCwpDKOiQgzz2bl5a3tEhUNbdNRSUeXtF0ld3dxroJxr+s+6/beO63+I7S2GnqCnqIOctFz1Edv0QCNEEVf0Df0Hf2o/KrWqvvV5jq0srfJeYC2rPr4L3UaGb0=</latexit>score#(hT,ni,j2)<latexit sha1_base64="GbYsBOKQWMPN83trW/oAiIIHog8=">AAADYHicbVLNjtMwEPa2/JTysy0cQHCxqCp1pVU3KQf2AlSCAxekItruSk22cpxpataxg+20dKM8BycehSs8A1ceAnHESbsS7TKSrU/zzYxnPk+QcKaN4/zcq1SvXb9xs3arfvvO3Xv7jeb9sZapojCikkt1GhANnAkYGWY4nCYKSBxwOAnOXxf8yQKUZlIMzSoBPyaRYDNGibGuacP1YmLmepZpKhXkZ16r43EiIg54eCiwp0p8iL0FUcmcTXsH00bL6Tql4avA3YDWqz8Xv/v069lg2qw89EJJ0xiEoZxoPXGdxPgZUYZRDnndSzUkhJ6TCCYWChKD9rNythy3rSfEM6nsEQaX3n8zMhJrvYoDG1lOsssVzv9xk9TMjv2MiSQ1IOj6oVnKsZG4EAqHTAE1fGUBoYrZXjGdE0WosXJuVdLUz4r6CbmQeb3uFX+hi18IMytHL8eX1sYDwq3wQmIBEGocW80xt4FMRLijbUHAAZglgMBllYOth4aunxUqFP1uzZmwwp3vdEU4hC+c7rHjZ2/gIxmnH4jQ76SQ24HhgiV6I/rnter1tu3qEz7Ckb2XzMwxl0tQOCAKJ1YDzqEU6mXdUyBgSWUcExFmns3Ky1vbJSoa2qajko4uabtK7u7iXAXjXtd91u29d1r9R2htNfQEPUUd5KLnqI/eogEaIYq+oG/oO/pR+VWtVferzXVoZW+T8wBtWfXxX3flGb4=</latexit>score#(hT,ni,j3)<latexit sha1_base64="dT9Qyggct4HRfcGZybj1KJFqiBc=">AAADYHicbVLNjtMwEPa2/JTysy0cQHCxqCp1pVVJugf2AlSCAxekItruSk22cpxpataxg+20dKM8BycehSs8A1ceAnHESbsS7TKSrU/zzYxnPk+QcKaN4/zcq1SvXb9xs3arfvvO3Xv7jeb9sZapojCikkt1GhANnAkYGWY4nCYKSBxwOAnOXxf8yQKUZlIMzSoBPyaRYDNGibGuacP1YmLmepZpKhXkZ16r43EiIg54eCiwp0p8iL0FUcmcTY8Opo2W03VKw1eBuwGtV38ufvfp17PBtFl56IWSpjEIQznReuI6ifEzogyjHPK6l2pICD0nEUwsFCQG7WflbDluW0+IZ1LZIwwuvf9mZCTWehUHNrKcZJcrnP/jJqmZHfsZE0lqQND1Q7OUYyNxIRQOmQJq+MoCQhWzvWI6J4pQY+XcqqSpnxX1E3Ih83rdK/5CF78QZlaOXo4vrY0HhFvhhcQCINQ4tppjbgOZiHBH24KAAzBLAIHLKgdbDw1dPytUKPrdmjNhhTvf6YpwCF843WPHz97ARzJOPxCh30khtwPDBUv0RvTPa9XrbdvVJ/wMR/ZeMjPHXC5B4YAonFgNOIdSqJd1T4GAJZVxTESYeTYrL29tl6hoaJuOSjq6pO0qubuLcxWMe133qNt777T6j9DaaugJeoo6yEXPUR+9RQM0QhR9Qd/Qd/Sj8qtaq+5Xm+vQyt4m5wHasurjv3qwGb8=</latexit>score#(hT,ni,j4)<latexit sha1_base64="DPhuQ6GXGNA9mLvU+PTTFZ7vBTw=">AAADYHicbVLNjtMwEPa2/JTysy0cQHCxqCp1pVVJChJ7ASrBgQtSEW13pSZbOc40NevYwXZaulGegxOPwhWegSsPgTjipF2JdhnJ1qf5ZsYznydIONPGcX7uVapXrl67XrtRv3nr9p39RvPuWMtUURhRyaU6CYgGzgSMDDMcThIFJA44HAdnrwv+eAFKMymGZpWAH5NIsBmjxFjXtOF6MTFzPcs0lQryU6/V8TgREQc8PBTYUyU+xN6CqGTOps8Opo2W03VKw5eBuwGtV3/Of/fp19PBtFm574WSpjEIQznReuI6ifEzogyjHPK6l2pICD0jEUwsFCQG7WflbDluW0+IZ1LZIwwuvf9mZCTWehUHNrKcZJcrnP/jJqmZHfkZE0lqQND1Q7OUYyNxIRQOmQJq+MoCQhWzvWI6J4pQY+XcqqSpnxX1E3Iu83rdK/5CF78QZlaOXo4vrI0HhFvhhcQCINQ4tppjbgOZiHBH24KAAzBLAIHLKgdbDw1dPytUKPrdmjNhhTvf6YpwCF843SPHz97ARzJOPxCh30khtwPDBUv0RvTPa9XrbdvVJ/wER/ZeMjPHXC5B4YAonFgNOIdSqJd1T4GAJZVxTESYeTYrL29tl6hoaJuOSjq6oO0qubuLcxmMe133abf33mn1H6C11dAj9Bh1kIueoz56iwZohCj6gr6h7+hH5Ve1Vt2vNtehlb1Nzj20ZdWHfwF9exnA</latexit>lubF<latexit sha1_base64="otxfuKn/ZMuwPGf8bJjjzBfgUzM=">AAADPnicbZLNjtMwEMe94WsJH9uFA0hcLKpK5VKScmAvwEpw4IJUxLa7UhNVjjNNzDp2sJ0tJcoz8ABc4T0QPAUvwG3FCYkjTtqVSJeRYo3mPzP+5ydHOWfaeN6PLefCxUuXr2xfda9dv3Fzp7N7a6JloSiMqeRSHUVEA2cCxoYZDke5ApJFHA6j4+e1fngCSjMpDswyhzAjiWBzRomxpVlnJ4AsT0teRNUsGKVs1ul6A68JfD7x10n32em3j0H/9/fRbNe5E8SSFhkIQznReup7uQlLogyjHCo3KDTkhB6TBKY2FSQDHZaN8wr3bCXGc6nsJwxuqv9OlCTTeplFtjMjJtWbWl38nzYtzHwvLJnICwOCri6aFxwbiWsMOGYKqOFLmxCqmPWKaUoUocbCam3SNCzr/Tn5ICvXDWrSumYclxbHsMJn0cMjwi1WIbEAiDXOpALMbSMTCe5ruxBwBGYBIHCz5UHrogM/LGsKtd/Wf+asLlcbrgiH+Ik32PPC8gW8JZPiDRH6lRSy3RifsFyvob9fUXd71tU7/BAn9lwwk2IuF6BwRBTOLQPOoQH11A0UCFhQmWVExGVgp6rm1Jw0htpy0sjJmWyfkr/5cM4nk+HAfzQYvva6+3fRKrbRPXQf9ZGPHqN99BKN0BhRVKBP6DP64nx1fjqnzq9Vq7O1nrmNWuH8+QsGbRFB</latexit>Proving Data-Poisoning Robustness in Decision Trees

4.7 Abstracting Conditionals

We abstractly interpret conditionals in DTrace, as is standard,
by taking the join of all abstract states from the feasible then
and else paths. In DTrace, there are two branching statements
of interest for our purposes, one with the condition ent(T ) =
0 and one with φ = ⋄.

Let us consider the condition φ = ⋄. Given an abstract
state (⟨T , n⟩, Ψ), we simply set Ψ = {⋄} and propagate the
state to the then branch (unless, of course, ⋄ (cid:60) Ψ, in which
case we omit this branch). For φ (cid:44) ⋄, we remove ⋄ from Ψ
and propagate the resulting state through the else branch.
Next, consider the conditional ent(T ) = 0. For the then
branch, we need to restrict an abstract state (⟨T , n⟩, Ψ) to
training sets with 0 entropy: intuitively, this occurs when
all elements have the same classification. We ask: are there
any concretizations composed of elements of the same class?,
and we proceed through the then branch with the following
training set abstraction:

(cid:196)

i ∈[1,k ]

pure(⟨T , n⟩, i)

where

pure(⟨T , n⟩, i) (cid:66) Let T ′ = {(x, y) ∈ T | y = i} in

if |T \ T ′| ⩽ n then ⟨T ′, n − |T \ T ′|⟩
else ⊥

The idea is as follows: the set T ′ defines a subset of T contain-
ing only elements of class i. But if we have to remove more
than n elements from T to arrive at T ′, then the conditional
is not realizable by a concrete training set of class i, and so
we return the empty abstract state.

In the case of ent(T ) (cid:44) 0 (the else branch), we soundly
(imprecisely) propagate the original state without restriction.

4.8 Soundness of Abstract Learner
Finally, DTrace# soundly overapproximates the results of
DTrace and can therefore be used to prove robustness to
n-poisoning.

Theorem 4.11. Let T ′ ∈ γ (⟨T , n⟩), let (T ′
state of DTrace(T ′, x), and let (⟨T ′′
state of DTrace#(⟨T , n⟩, x). Then T ′
f

f , ·, ·) be the final
f , nf ⟩, ·) be the final abstract
f , nf ⟩).

∈ γ (⟨T ′′

It follows from the soundness of DTrace# that we can use it
to prove n-poisoning robustness. Let I = ([l1, u2], . . . , [lk , uk ])
be a set of intervals. We say that interval [li , ui ] dominates I
if and only if li > uj for every j (cid:44) i.

5 Extensions
In this section, we present two extensions that make our
abstract interpretation framework more practical. First, we
show how our abstract domain can be modified to accom-
modate real-valued features (§ 5.1). Second, we present a
disjunctive abstract domain that is more precise than the one
we discussed, but more computationally inefficient (§ 5.2).

5.1 Real-Valued Features
Thus far, we have assumed that DTrace and DTrace# operate
on a finite set of predicates Φ. In real-world decision-tree
implementations, this is not quite accurate: for real-valued
features, there are infinitely many possible predicates of the
form λxi . xi ⩽ τ (where τ ∈ R), and the learner chooses
a finite set of possible τ values dynamically, based on the
training set T . We will use the subscript R to denote the
real-valued versions of existing operations.

From DTrace to DTraceR. The new learner DTraceR is
almost identical to DTrace. However, each invocation of
bestSplitR first computes a finite set of predicates ΦR. Con-
sider all of the values appearing in T for the ith feature in X,
sorted in ascending order. For each pair of adjacent values
(a, b) (i.e., such that there exists no c inT such that a < c < b),
we include in ΦR the predicate φ = λxi . xi ⩽ a+b
2 .

Example 5.1. In our running example from Figure 2, we
have training set elements in Tbw whose features take the nu-
meric values {0, 1, 2, 3, 4, 7, . . . , 14}. bestSplitR(Tbw) would
pick a predicate from the set ΦR = {λx . x ⩽ τ | τ ∈
2 , 3
{ 1

2 , . . . , 27

2 , 15

2 , 11

2 }}.

2 , 7

2 , 5

From DTrace# to DTrace#
R. To apply the abstract learner
in the real-valued setting, we can follow the idea above and
construct a finite set ΦR. Because our poisoning model as-
sumes dropping up to n elements of the training set, this
results in roughly (n+1)· |T | predicates in the worst case—i.e.,
we need to account for every pair (a, b) of adjacent feature
values or that are adjacent after removing up to n elements
between them.

Example 5.2. Continuing Example 5.1. Say we want to com-
pute ΦR for ⟨Tbw, 1⟩. Then, for every pair of values that are 1
apart we will need to add a predicate to accommodate the
possibility that we drop the value between them. E.g., in
Tbw = {. . . , 3, 4, 7, . . .}, we will additionally need the predi-
cate λx . x ⩽ (3+7)/2, for the case where we drop the element
with value 4 from the dataset.

Corollary 4.12. Let ⟨T ′, n′⟩ be the final abstract state of
DTrace#(⟨T , n⟩, x). If I = cprob#(⟨T ′, n′⟩)) and there exists
an interval in I that dominates I (i.e., same class is selected for
every T ∈ γ ⟨T , n⟩), then x is robust to n-poisoning of T .

To avoid a potential explosion in the size of the predicate
set and maintain efficiency, we compactly represent sets of
similar predicates symbolically. We describe this detail in
Appendix B.

9

5.2 Disjunctive Abstraction
The state abstraction used by DTrace# can be imprecise, mainly
due to the join operations that take place, e.g., during filter#.
The primary concern is that we are forced to perform a very
imprecise join between possibly quite dissimilar training set
fragments. Consider the following example:

Example 5.3. Let us return to Tbw from Figure 2, but imag-
ine we have continued the computation after filtering using
x ⩽ 10 and have selected some best predicates. Specifically,
consider a case in which we have x = 4 and
• ⟨T , 1⟩, where T = {0, 1, 2, 3, 4, 7, 8, 9, 10}
• Ψ = {x ⩽ 3, x ⩽ 4} (ignoring whether this is correct)
Let us evaluate filter#(⟨T , 1⟩, Ψ, x). Following the definition
of filter#, we will compute

⟨T ′, n′⟩ = ⟨T⩽4, 1⟩ ⊔ ⟨T>3, 1⟩

where T⩽4 = {(4, b), (3, w), (2, w), (1, w), (0, b)} and T>3 =
{(4, b), (7, w), (8, w), (9, w), (10, w)}, thus giving us T ′ = T
(the set we began with) and n′ = 5 (much larger than what
we began with). This is a large loss in precision.

To address this imprecision, we will consider a disjunctive
version of our abstract domain, consisting of unboundedly
many disjuncts of this previous domain, which we represent
as a set {(⟨T , n⟩i , Ψi )}i . Our join operation becomes very
simple: it is the union of the two sets of disjuncts.

Definition 5.4 (Joins). Given two disjunctive abstractions
DI = {(⟨T , n⟩i , Ψi )}i ∈I and D J = {(⟨T , n⟩j , Ψj )}j ∈J , we define

DI ⊔ D J (cid:66) DI ∪ D J

Adapting DTrace# to operate on this domain is immediate:
each of the transformers described in the previous section is
applied to each disjunct.

Because our disjunctive domain eschews memory- and
time-efficiency for precision, we are able to prove more
things, but at a cost (we explore this in our evaluation, § 6).
Note that, by construction, the disjunctive abstract domain
is at least as precise as our standard abstract domain.

6 Implementation and Evaluation
We implemented our algorithms DTrace and DTrace# in C++
in a (single-threaded) prototype we call Antidote. Our evalu-
ation8 aims to answer the following research questions:

RQ1 Can Antidote prove data-poisoning robustness for real-

world datasets? (§6.2)

RQ2 How does the performance of Antidote vary with re-
spect to the scale of the problem and the choice of
abstract domain? (§6.3)

8We use a machine with a 2.3GHz processor and 160GB of RAM throughout.

10

Samuel Drews, Aws Albarghouthi, and Loris D’Antoni

6.1 Benchmarks and Experimental Setup

We experiment on 5 datasets (Table 1). We obtained the
first three datasets from the UCI Machine Learning Reposi-
tory [12]. Iris is a small dataset that categorizes three related
flower species; Mammographic Masses and Wisconsin Diag-
nostic Breast Cancer are two datasets of differing complexi-
ties related to classifying whether tumors are cancerous. We
also evaluate on the widely-studied MNIST dataset of hand-
written digits [17], which consists of 70,000 grayscale images
(60,000 training, 10,000 test) of the digits zero through nine.
We consider a form of MNIST that has been used in the poi-
soning literature and create another variant for evaluation:

• We make the same simplification as in other work on data
poisoning [3, 28] and restrict ourselves to the classifica-
tion of ones versus sevens (13,007 training instances and
2,163 test instances), which we denote MNIST-1-7-Real.
Steinhardt et al. [28], for example, recently used this to
study poisoning in support vector machines.

• Each MNIST-1-7-Real image’s pixels are 8-bit integers
(which we treat as real-valued); to create a variant of the
problem with reduced scale, we also consider MNIST-1-7-
Binary, a black-and-white version that uses each pixel’s
most significant bit (i.e. our predicates are Boolean).

For each dataset, we consider a decision-tree learner with
a maximum tree depth (i.e. number of calls to bestSplit) rang-
ing from 1 to 4. Table 1 shows that test set9 accuracies of
the decision trees learned by DTrace are reasonably high—
affirmation that when we prove the robustness of its results,
we are proving something worthwhile.

Experimental Setup. For each test element, we explore the
amount of poisoning (i.e. how large of a n from our ∆n model)
for which we can prove the robustness property as follows.

1. For each combination of dataset T and tree depth d, we
begin with a poisoning amount n = 1, i.e. a single element
could be missing from the training set.

2. For each test set element x, we attempt to prove that x
is robust to poisoning T using any set in ∆n(T ). Let Sn
be the test subset for which we do prove robustness for
poisoning amount n. If Sn is non-empty, we double n and
again attempt to verify the property on elements in Sn.
3. If at a depth n all instances fail, we binary search between
n and n/2 to find an n/2 < n′ < n at which some instances
terminate. This approach allows us to better illustrate the
experiment trends in our plots.

Failure occurs due to any of three cases: (i) the computed
over-approximation does not conclusively prove robustness,

9The UCI datasets come as a single training set. We selected a random 80%-
20% split of the data, saving the 20% as the test set to use in our experiments.
The scale of the MNIST dataset is large; for pragmatic reasons, we fix a
random subset of 100 of the original 2,163 test set elements for robustness
proving, and we run our DTrace# experiments only on this subset.

Proving Data-Poisoning Robustness in Decision Trees

Table 1. Detailed metrics for the benchmark datasets considered in our evaluation. * Test set accuracy for MNIST is computed
on the full 2,163 instances; robustness experiments are performed on 100 randomly chosen test set elements.

Data Set

Size

Training Test

Features X Classes Y

Iris
Mammographic Masses
Wisconsin Diagnostic Breast Cancer
MNIST-1-7-Binary
MNIST-1-7-Real

120
664
456
13,007
13,007

30 R4
166 R5
113 R30
100*
100* R784

{0, 1}784

{Setosa, Versicolour, Virginica}
{benign, malignant}
{benign, malignant}
{one, seven}
{one, seven}

DT Test-Set Accuracy (%)

Depth 1

2

3

4

20.0
80.7
91.2
95.7
95.6

90.0
83.1
92.0
97.4
97.6

90.0
81.9
92.9
97.8
98.3

90.0
80.7
94.7
98.3
98.7

Iris

Mammographic
Masses

Wisconsin Diagnostic
Breast Cancer

MNIST-1-7-Binary

MNIST-1-7-Real

d
e
fi
i
r
e
V
n
o
i
t
c
a
r
F

1
0.8
0.6
0.4
0.2
0

1

2

4

8

1

2

4

8 16 32 64

1

2

4

8 16 32 64

1

Poisoning n

Poisoning n

Poisoning n

8
512
64
Poisoning n

1

8
512
64
Poisoning n

depth 1
depth 2
depth 3
depth 4
n = 1%

Figure 6. Fraction of test instances proven robust versus poisoning parameter n (log scale). The dotted line is a visual aid,
indicating n is 1% of the training set size.

(ii) the computation runs out of memory, or (iii) the computa-
tion exceeds a one-hour timeout. We run the entire procedure
for the non-disjunctive and disjunctive abstract domains.

6.2 Effectiveness of Antidote

We evaluate how effective Antidote is at proving data-poisoning
robustness. In this experiment, we consider a run of the
algorithm on a single test element successful if either the
non-disjunctive or disjunctive abstract domain succeeds at
proving robustness (mimicking a setting in which two in-
stances of DTrace#, one for each abstract domain, are run
in parallel)—we will contrast the results for the different
domains in §6.3. Figure 6 shows these results.

To exemplify the power of Antidote, draw your attention
to the depth-2 instance of DTrace# invoked on MNIST-1-7-
Real. For 38 of the 100 test instances, we are able to verify
that even if the training set had been poisoned by an attacker
who contributed up to 64 poisoned elements (≈ 1
2 %), the at-
tacker would not have had any power to change the resulting
classification. Conventional machine learning wisdom says
that, in decision tree learning, small changes to the train-
ing set can cause the model to behave quite differently. Our
results verify nuance—sometimes, there is some stability.10

10 The Iris dataset has an interesting quirk—we’re unable to prove much
at depth 1 because in the concrete case, one of the leaves is a 50/50 split
between two classes, thus changing one element could make the difference
for any of the test set instances taking that path. At depth 2, a predicate is
allowed to split that leaf further, making decision-tree learning more stable.

11

These 38 verified instances average ∼800s run time. ∆64(T )
consists of over 10174 concrete training sets; This is stagger-
ingly efficient compared to a naïve enumeration baseline,
which would be unable to verify robustness at this scale.

To answer RQ1, Antidote can verify robustness for real-
world datasets with extremely large perturbed sets and decision-
tree learners with high accuracies.

6.3 Performance of Antidote

We evaluate how the performance of Antidote is affected by
the complexity of the problem, e.g., the size of the training
set and its number of features, the number of poisoned ele-
ments, and the depth of the learned decision tree. Due to the
large number of parameters involved in our evaluation, this
section only provides a number of representative statistics.
In particular, although the reader can find plots describing all
the metrics evaluated on each dataset in Appendix C, most of
our analysis will focus on MNIST-1-7-Binary (see Figure 7),
since it exhibits the most illustrative behavior.

Box vs Disjuncts. In this section we use Disjuncts to re-
fer to the disjunctive abstract domain and Box to refer to
the non-disjunctive one. Disjuncts is more precise than Box
and, as expected, it can verify more instances. However, Dis-
juncts is slower and more memory-intensive. Consider the
MNIST-1-7-Binary dataset (see Figure 7). For depth 3 and
n = 64 (approximately 0.5% of the dataset), Disjuncts can
verify 52 instances while Box can only verify 15. However,

Disjuncts takes on average 32s to terminate (0 timeouts) and
uses 1,650MB of memory, while Box takes on average 0.7s
to terminate (0 timeouts) and uses 150MB of memory. It is
worth noting that Box can verify certain instances that Dis-
junct cannot verify due to timeouts. For example, at depth
4 and n = 128, Box is able to verify 1 problem instance,11
while Disjuncts always times out. An interesting direction
for future research would be to consider strategies that cap-
italize on the precision of tracking many disjuncts while
incorporating the efficiency of allowing some to be joined.

Number of Poisoned Elements. It is clear from the plots
that the number of poisoned elements greatly affects the
performance and efficacy of Antidote. We do not focus on
particular numbers, since the trends are clear from the plots
(including the ones in Appendix C): The memory consump-
tion and running times of Disjuncts grow exponentially with
n, but are still practical and Disjuncts is effective up to high
depths. The memory consumption and running times of Box
grow more slowly: 95% of all experiments we ran using Box
finished within 20 seconds, and none timed out (the longest
took 232 seconds).12 However, Box is less effective than Dis-
juncts as the depths increase; this is expected, as the loss of
precision with more operations is more severe for Box.

Size of Dataset and Number of Features. We measure
whether the size of the dataset (which in our benchmarks
is quite correlated with the number of features) affects the
performance. Consider the case of verifying a decision-tree
learner of depth 3 using the disjunctive domain and a per-
turbed set where 0.5% of the points13 are removed from the
dataset (similar trends are observed when varying these pa-
rameters). The average running time of Antidote is 0.1s for
Iris, 0.2s for Mammographic Masses, 26s for Wisconsin Di-
agnostic Breast Cancer, and 32s for MNIST-1-7-Binary. For
MNIST-1-7-Real, 100% of the benchmarks TO at 0.05% poi-
soning. As expected, the size of the dataset and the number
of features have an effect on the verification time. However,
it is hard to exactly quantify this effect, given how differently
each dataset behaves; an obvious comparison we can make
is the difference between MNIST-1-7-Binary and MNIST-1-7-
Real. These two datasets have identical sizes, but the former
uses binary features and the latter uses real features. As we
can see, handling real features results in a massive slowdown
and in proving fewer instances robust. This is not surpris-
ing since real features can result in more predicates, which
affect both running time and the discrimination power of
individual nodes in the decision tree.

11The 14 other instances that succeeded at n = 64 similarly terminated after
0.7s on average, but their final state did not prove robustness.
12This data must be taken with a grain of salt: Box is generally less effective
than Disjuncts; due to the incremental nature of our experiments, it did not
attempt as many of the “harder” problems as Disjuncts did.
13We round to the closest n for which the tool can verify at least one instance

12

Samuel Drews, Aws Albarghouthi, and Loris D’Antoni

Depth of the Tree. Consider the case of verifying a decision-
tree learner for MNIST-1-7-Binary using the disjunctive do-
main, and a perturbed set where up to 64 of the points
have been added maliciously to the dataset (similar trends
are observed when varying these parameters and for other
datasets). The average running time of Antidote is 0.3s at
depth 1, 0.5s at depth 2, 32s at depth 3, and 933s at depth 4.
As expected, the depth of the tree is an important factor in
the performance of the disjunctive domain, as each abstract
operation expands the set of disjuncts.

We summarize the results presented in this section and
answer RQ2: in general, the disjunctive domain is more precise
but slower than the non-disjunctive domain, and the depth of
the learned trees and the number of poisoned elements in the
dataset are the greatest factors affecting performance.

7 Related Work

Instability in Decision Trees. Decision-tree learning has
a long and storied history. A particular thread of work that
is relevant to ours is the analysis of decision-tree instabil-
ity [13, 18, 22, 30]. These works show that decision-tree
learning algorithms are in general susceptible to small data-
poisoning attacks—although they do not phrase it in those
terms. For the most part, the works are motivated from the
perspective that a decision tree represents a set of “rules,”
and they are concerned with conditions under which those
rules will not change (either by quantifying forms of invari-
ance or providing novel learning algorithms). Our work is
different in that it proves that no poisoning attack exists on
a formalization of very basic decision-tree learning, and we
can often precisely allow for the “rules” to change so long
as the ultimate classification does not.

Data Poisoning. Data-poisoning robustness has been stud-
ied extensively from an attacker perspective [3, 19, 21, 35, 36].
This body of work has demonstrated attacks that can degrade
classifier accuracy, sometimes dramatically. These works
phrase the problem of identifying a poisoned set as a con-
straint optimization problem. To make the problem tractable,
they typically focus on support vector machines (SVMs) and
forms of regression for which existing optimization tech-
niques are readily available. Our approach differs from these
works in multiple ways: (i) Our work focuses on decision
trees. The greedy, recursive nature of decision-tree learning
is fundamentally different from the optimization problem
solved in learning SVMs. (ii) While our technique is general,
in this paper we consider a poisoning model in which train-
ing elements have been added [7, 34]. Some works instead
focuses on a model in which elements of the training set
can be modified [1]. (iii) Final and most important, our work
proves that no poisoning attack exists using abstract inter-
pretation, while existing techniques largely provide search
techniques for finding poisoned training sets.

Proving Data-Poisoning Robustness in Decision Trees

Depth 1

Depth 2

Depth 3

Depth 4

d
e
fi
i
r
e
V
#

100
80
60
40
20
0
10,000
1,000
100
10
1
0.1

100,000

10,000

1,000

100

1

)
s
(

i

e
m
T
e
g
a
r
e
v
A

x
a
M
e
g
a
r
e
v
A

)
B
M

(
y
r
o
m
e
M

Box
Disjuncts

Timeout

OOM

64

8
Poisoning n

512

1

64

8
Poisoning n

512

1

64

8
Poisoning n

512

1

64

8
Poisoning n

512

Figure 7. Efficacy, performance, and memory usage for MNIST-1-7-Binary

Recently, techniques have been proposed to modify the
training processes of machine learning models to make them
robust to various data-poisoning attacks (while remaining
computationally efficient). These techniques [9, 10, 16, 28]
are often based on robust estimation, e.g. outlier removal;
see [11] for a survey. In general, these approaches provide
limited probabilistic guarantees about certain kinds of at-
tacks; the works are orthogonal to ours, though they raise an
interesting question for future work: Can one verify that, on
a given training set, these models actually make the training
process resistant to data poisoning?

Abstract Interpretation for Robustness. Abstract inter-
pretation [8] is one of the most popular models for static
program analysis. Our work is inspired by that of Gehr
et al. [14], where abstract interpretation is used to prove
input-robustness for neural networks. (Recently, Ranzato
and Zanella have done similar work for decision tree ensem-
bles [26].) Many papers have followed improving on this
problem [2, 27]. The main difference between these works
and ours is that we tackle the problem of verifying training-
time robustness, while existing works focus on test-time
robustness. The former problem requires abstracting sets
of training sets, while the latter only requires abstracting
sets of individual inputs. In particular, Gehr et al. rely on
well-known abstract domains—e.g., intervals and zonotopes—
to represent sets of real vectors, while our work presents
entirely new abstract domains for reasoning about sets of
training sets. To our knowledge, our work is the first that
even tries to tackle the problem of verifying data-poisoning
robustness.

Other works have focused on provable training of neural
networks to exhibit test-time robustness by construction [20,
33]: this is done by using abstract interpretation to over-
approximate the worst-case loss formed by any adversarial
perturbation to any element in the training set. One can
think of these techniques as performing a form of symbolic
training, which is conceptually similar to our core idea. Note,
however, two important distinctions: (i) These works address
the problem of adversarial changes to test inputs, while we
address adversarial changes to the training set; (ii) These
works construct a different, robust model, while we verify a
property of an unchanged model (or rather, the learner).

8 Conclusion
We presented Antidote, the first tool that can verify data-
poisoning robustness for decision tree learners, where an
attacker may have contributed malicious training items. Anti-
dote is based on abstract interpretation and introduces a new
abstract domain for representing sets of training sets. We
showed that Antidote can verify robustness for real-world
datasets in cases where an enumeration approach would be
intractable. To our knowledge, this paper is the first to verify
data-poisoning robustness for any kind of machine learning
model. A natural future direction is to extend our ideas to
neural networks, where the learning algorithm is stochastic.

Acknowledgements. We would like to thank Jerry Zhu, Il-
ias Diakonikolas, and Paris Koutris for their feedback, as well
as our shepherd, Martin Vechev. This material is based upon
work supported by the National Science Foundation under
grant numbers 1652140, 1704117, 1750965, and 1918211.

13

References
[1] Scott Alfeld, Xiaojin Zhu, and Paul Barford. 2016. Data poisoning
attacks against autoregressive models. In Thirtieth AAAI Conference
on Artificial Intelligence.

[2] Greg Anderson, Shankara Pailoor, Isil Dillig, and Swarat Chaudhuri.
2019. Optimization and abstraction: a synergistic approach for an-
alyzing neural network robustness. In Proceedings of the 40th ACM
SIGPLAN Conference on Programming Language Design and Implemen-
tation. ACM, 731–744.

[3] Battista Biggio, Blaine Nelson, and Pavel Laskov. 2012. Poisoning
Attacks Against Support Vector Machines. In Proceedings of the 29th
International Coference on International Conference on Machine Learn-
ing (Edinburgh, Scotland) (ICML’12). Omnipress, USA, 1467–1474.
http://dl.acm.org/citation.cfm?id=3042573.3042761

[4] Leo Breiman. 2017. Classification and regression trees. Routledge.
[5] Nicholas Carlini and David Wagner. 2017. Towards evaluating the
robustness of neural networks. In 2017 IEEE Symposium on Security
and Privacy (SP). IEEE, 39–57.

[6] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree
boosting system. In Proceedings of the 22nd acm sigkdd international
conference on knowledge discovery and data mining. ACM, 785–794.
[7] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017.
Targeted backdoor attacks on deep learning systems using data poi-
soning. arXiv preprint arXiv:1712.05526 (2017).

[8] P. Cousot and R. Cousot. 1977. Abstract Interpretation: A Unified
Lattice Model for Static Analysis of Programs by Construction or
Approximation of Fixpoints.

[9] Ilias. Diakonikolas, Gautam. Kamath, Daniel. Kane, Jerry. Li, Ankur.
Moitra, and Alistair. Stewart. 2019. Robust Estimators in High-
SIAM J.
Dimensions Without the Computational Intractability.
Comput. 48, 2 (2019), 742–864. https://doi.org/10.1137/17M1126680
arXiv:https://doi.org/10.1137/17M1126680

[10] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Stein-
hardt, and Alistair Stewart. 2019. Sever: A Robust Meta-Algorithm for
Stochastic Optimization. In Proceedings of the 36th International Con-
ference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA (Proceedings of Machine Learning Research), Kamalika
Chaudhuri and Ruslan Salakhutdinov (Eds.), Vol. 97. PMLR, 1596–1606.
http://proceedings.mlr.press/v97/diakonikolas19a.html

[11] Ilias Diakonikolas and Daniel M. Kane. 2019. Recent Advances in
Algorithmic High-Dimensional Robust Statistics. CoRR abs/1911.05911
(2019). arXiv:1911.05911 http://arxiv.org/abs/1911.05911

[12] Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository.

http://archive.ics.uci.edu/ml

[13] Kenneth Dwyer and Robert Holte. 2007. Decision Tree Instability
and Active Learning. In Machine Learning: ECML 2007, 18th European
Conference on Machine Learning, Warsaw, Poland, September 17-21, 2007,
Proceedings. 128–139. https://doi.org/10.1007/978-3-540-74958-5_15
[14] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri,
and M. Vechev. 2018. AI2: Safety and Robustness Certification of
Neural Networks with Abstract Interpretation. In 2018 IEEE Symposium
on Security and Privacy (SP). 3–18. https://doi.org/10.1109/SP.2018.
00058

[15] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochen-
derfer. 2017. Reluplex: An efficient SMT solver for verifying deep
neural networks. In International Conference on Computer Aided Verifi-
cation. Springer, 97–117.

[16] Ricky Laishram and Vir Virander Phoha. 2016. Curie: A method for
protecting SVM Classifier from Poisoning Attack. CoRR abs/1606.01584
(2016). arXiv:1606.01584 http://arxiv.org/abs/1606.01584

[17] Yann LeCun, Corinna Cortes, and Christopher J. C. Burges. [n.d.]. The
MNIST Database of handwritten digits. http://yann.lecun.com/exdb/
mnist

Samuel Drews, Aws Albarghouthi, and Loris D’Antoni

[18] Ruey-Hsia Li and Geneva G. Belford. 2002. Instability of decision tree
classification algorithms. In Proceedings of the Eighth ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, July
23-26, 2002, Edmonton, Alberta, Canada. 570–575. https://doi.org/10.
1145/775047.775131

[19] Shike Mei and Xiaojin Zhu. 2015. Using Machine Teaching to Identify
Optimal Training-set Attacks on Machine Learners. In Proceedings of
the Twenty-Ninth AAAI Conference on Artificial Intelligence (Austin,
Texas) (AAAI’15). AAAI Press, 2871–2877. http://dl.acm.org/citation.
cfm?id=2886521.2886721

[20] Matthew Mirman, Timon Gehr, and Martin T. Vechev. 2018. Dif-
ferentiable Abstract Interpretation for Provably Robust Neural Net-
works. In Proceedings of the 35th International Conference on Ma-
chine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden,
July 10-15, 2018 (Proceedings of Machine Learning Research), Jen-
nifer G. Dy and Andreas Krause (Eds.), Vol. 80. PMLR, 3575–3583.
http://proceedings.mlr.press/v80/mirman18b.html

[21] Andrew Newell, Rahul Potharaju, Luojie Xiang, and Cristina Nita-
Rotaru. 2014. On the Practicality of Integrity Attacks on Document-
Level Sentiment Analysis. In Proceedings of the 2014 Workshop on
Artificial Intelligent and Security Workshop (Scottsdale, Arizona, USA)
(AISec ’14). ACM, New York, NY, USA, 83–93. https://doi.org/10.1145/
2666652.2666661

[22] Jesús M. Pérez, Javier Muguerza, Olatz Arbelaitz, Ibai Gurrutxaga, and
José Ignacio Martín. 2005. Consolidated Trees: Classifiers with Stable
Explanation. A Model to Achieve the Desired Stability in Explanation.
In Pattern Recognition and Data Mining, Third International Conference
on Advances in Pattern Recognition, ICAPR 2005, Bath, UK, August 22-25,
2005, Proceedings, Part I. 99–107. https://doi.org/10.1007/11551188_11
[23] J.R. Quinlan. 1987. Simplifying decision trees. International Journal of
Man-Machine Studies 27, 3 (1987), 221 – 234. https://doi.org/10.1016/
S0020-7373(87)80053-6

[24] J. Ross Quinlan. 1986. Induction of decision trees. Machine learning 1,

1 (1986), 81–106.

[25] J Ross Quinlan. 1993. C4.5: Programs for machine learning. The
Morgan Kaufmann Series in Machine Learning, San Mateo, CA: Morgan
Kaufmann,| c1993 (1993).

[26] Francesco Ranzato and Marco Zanella. 2020. Abstract Interpretation of
Decision Tree Ensemble Classifiers. In Proceedings of the Thirty-Fourth
AAAI Conference on Artificial Intelligence (AAAI’20), V. Conitzer and
F. Sha (Eds.).

[27] Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev.
2019. An abstract domain for certifying neural networks. Proceedings
of the ACM on Programming Languages 3, POPL (2019), 41.

[28] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. 2017. Certified
defenses for data poisoning attacks. In Advances in neural information
processing systems. 3517–3529.

[29] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014.
Intrigu-
ing properties of neural networks. In 2nd International Conference on
Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16,
2014, Conference Track Proceedings.

[30] Peter D. Turney. 1995. Technical Note: Bias and the Quantification of
Stability. Machine Learning 20, 1-2 (1995), 23–33. https://doi.org/10.
1007/BF00993473

[31] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman
Jana. 2018. Formal security analysis of neural networks using symbolic
intervals. In 27th {USENIX} Security Symposium ({USENIX} Security
18). 1599–1614.

[32] Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri. 2018. Analyzing
the Robustness of Nearest Neighbors to Adversarial Examples. In
Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018.
5120–5129.

14

Proving Data-Poisoning Robustness in Decision Trees

[33] Eric Wong and J. Zico Kolter. 2018. Provable Defenses against Ad-
versarial Examples via the Convex Outer Adversarial Polytope. In
Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018
(Proceedings of Machine Learning Research), Jennifer G. Dy and An-
dreas Krause (Eds.), Vol. 80. PMLR, 5283–5292. http://proceedings.mlr.
press/v80/wong18a.html

[34] Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia
Eckert, and Fabio Roli. 2015. Is feature selection secure against training
data poisoning?. In International Conference on Machine Learning. 1689–
1698.

[35] Huang Xiao, Battista Biggio, Blaine Nelson, Han Xiao, Claudia Eckert,
and Fabio Roli. 2015. Support Vector Machines Under Adversarial
Label Contamination. Neurocomput. 160, C (July 2015), 53–62. https:
//doi.org/10.1016/j.neucom.2014.08.081

[36] Han Xiao, Huang Xiao, and Claudia Eckert. 2012. Adversarial Label
Flips Attack on Support Vector Machines. In Proceedings of the 20th
European Conference on Artificial Intelligence (Montpellier, France)
(ECAI’12). IOS Press, Amsterdam, The Netherlands, The Netherlands,
870–875. https://doi.org/10.3233/978-1-61499-098-7-870

A Proofs of Soundness for DTrace#
Proof of Proposition 4.2. SupposeT ∈ γ (⟨T1, n1⟩)∪γ (⟨T2, n2⟩).
Without loss of generality, assume T ∈ γ (⟨T1, n1⟩). Certainly
T ⊆ T1 ∪ T2; furthermore, observe that T can be formed by
first removing |T2 \T1|-many elements from T1 ∪T2 to recover
T1 and then removing ⩽ n1 remaining elements: this gives us
|(T1 ∪ T2) \ T | = |(T2 \ T1) ∪ (T1 \ T )| ⩽ |T2 \ T1| + |T1 \ T | ⩽
|T2 \ T1| + n1. This matches the definition of ⊔.
Proof of Proposition 4.4. Let ⟨S, m⟩ = γ (⟨T , n⟩↓#
φ ) (so we will
show T ′↓φ ∈ γ (⟨S, m⟩)). Since T ′ ⊆ T , we have that T ′↓φ ⊆
{(x, y) ∈ T : x |= φ} = S, thus T ′↓φ ⊆ S. Additionally,
S \T ′↓φ = S \T ′ ⊆ T \T ′; certainly, then, |S \T ′↓φ | ⩽ |T \T ′|
and thus ⩽ n. Recall there are two cases, m = n or m = |S |:
for the latter, we have trivially that |S \ T ′↓φ | ⩽ |S | ⩽ m.
Therefore |S \ T ′↓φ | ⩽ min(n, m).

Proof of Proposition 4.5. When n < |T |, this follows from
the soundness of interval arithmetic. In our corner case for
n = |T |, the concrete cprob is undefined behavior, so we
encompass every well-formed categorical probability distri-
bution by allowing each component to take any [0, 1] value.

Proof of Proposition 4.7. This is an immediate consequence
of the soundness of ⟨T , n⟩↓#
φ ′ and the soundness of the join.

Proof of Lemma 4.10. First, observe that score# is sound since
it involves composing sound interval arithmetic with other
sound operations. Let φ ′ = bestSplit(T ′).

• Case φ ′ = ⋄: By definition of bestSplit, this occurs
when T ′ is such that every predicate φ ∈ Φ results in
trivial splits. Soundness of ↓# then gives us that for
each φ ∈ Φ, either ∅ ∈ γ (⟨T , n⟩↓#
¬φ ),
thus Φ∀ = ∅. We return using the then-branch, which
explicitly includes ⋄.

φ ) or ∅ ∈ γ (⟨T , n⟩↓#

• Case φ ′ (cid:44) ⋄: By definition of bestSplit, (i) φ ′ non-
trivially splits T ′, and furthermore, (ii) for all other ψ

that non-trivially split T ′, we have that score(T ′, φ ′) ⩽
score(T ′,ψ ). (i) gives us that φ ′ ∈ Φ∃, therefore if Φ∀ =
∅, then φ ′ is included in the return value.
Otherwise, when Φ∀ (cid:44) ∅, we return using the then-
branch. Let ψ ∗ minimize lubΦ∀ : since ψ ∗ ∈ Φ∀, (ii) gives
us that score(T ′, φ ′) ⩽ score(T ′,ψ ∗), and thus we have
lb(score#(⟨T , n⟩, φ ′)) ⩽ ub(score#(⟨T , n⟩,ψ ∗)) = lubΦ∀ .
Therefore φ ′ is included in the return.

Proof of Theorem 4.11. DTrace# applies a sequence of oper-
ations; throughout the section, we state the soundness of
each of these operations. The soundness of DTrace# follows
from taking their composition.

B Real-Valued Features
In this appendix, we provide a complete exposition of the
technique used to handle real-valued features. We repeat
some of the arguments given in the main paper to make this
appendix self-contained. For real-valued features, there are
infinitely many possible predicates of the form λxi . xi ⩽ τ
(where τ ∈ R), and the learner DTraceR chooses a finite set
of possible τ values dynamically, based on the values that
occur in T . Throughout this section, we use the subscript R
to denote the real-valued versions of existing operations.

B.1 From DTrace to DTraceR
The new learner DTraceR is almost identical to DTrace. How-
ever, to formalize the aforementioned operation in the con-
crete semantics of DTraceR, we make a single modification
in bestSplitR, which maintains the original definition of
bestSplit, but it first computes a finite set of predicates ΦR
used in the remainder of its computation: consider all of
the values appearing in T for the ith feature in X, sorted in
ascending order. For each pair of adjacent values (a, b) (i.e.,
such that there exists no c in T such that a < c < b), we
include in ΦR the predicate φ = λxi . xi ⩽ a+b
2 .

Example B.1. In our running example from Figure 2, we
have training set elements in Tbw whose features take the nu-
meric values {0, 1, 2, 3, 4, 7, . . . , 14}. bestSplitR(Tbw) would
thus enumerate over the predicates ΦR = {λx . x ⩽ τ | τ ∈
2 , 3
{ 1

2 , . . . , 27

2 , 15

2 , 11

2 }}.

2 , 7

2 , 5

B.2 From DTrace# to DTrace#
R
The formalization for the abstract case is more involved than
the concrete case: (i) we will similarly modify bestSplit#, but
also (ii) we will change our abstract domain over predicates.
This change to the predicate domain means we will have
to make largely superficial adjustments to the many of the
other operations, as well.

bestSplit#

R, the real-valued version of bestSplit#, is respon-
sible for selecting a finite set of predicates that it will consider
in its computation. This motivates the second point, which
we will discuss first: because we don’t know which values

15

could be missing from ⟨T , n⟩, we might naively consider a
value of τ for every such possible combination of missing
values. If we did so, and if bestSplitRT considers m pred-
icates, then bestSplit#
R(⟨T , n⟩) might consider up to ≈ mn
predicates. For efficiency, we will instead create a finite set
of m-many symbolic predicates that overapproximates these
possibilities.

Definition B.2 (Symbolic Real-Valued Predicate). For a real-
valued feature at xi , a symbolic predicate ρ over xi takes the
form λxi . xi ⩽ [a, b) for real values a and b. The semantics
of a symbolic predicate is three-valued, which we denote as
follows:

ρ(x) (cid:66)

true
maybe
false

xi ⩽ a
a < xi < b
b ⩽ xi





Each symbolic predicate ρ = λxi . xi ⩽ [a, b) has the con-
cretization γ (ρ) (cid:66) {λxi . xi ⩽ τ | τ ∈ [a, b)}.

(cid:66) Ψ#

1 ∪ Ψ#

1 ⊔ Ψ#
2

ρ ∈Ψ# γ (ρ).

Without loss of generality, we focus on the single disjunct
case. We previously stated DTrace# operates over a state
(⟨T , n⟩, Ψ) where Ψ is some finite set of predicates; now we
let the state of DTrace#
R be (⟨T , n⟩, Ψ#) where Ψ# is repre-
sented as a finite set of symbolic predicates. The join remains
a simple set union Ψ#
2 . The concretiza-
tion captures an infinite set of (non-symbolic) predicates:
γ (Ψ#) (cid:66) (cid:208)
Symbolic Predicates in Auxiliary Operators. The defini-
tion of ⟨T , n⟩↓#
ρ changes slightly, since we must include the
maybe case to be sound. This complicates the computation
of the poisoning amount n, since we have two sources of
uncertainty: we must account for elements that could be
missing because either (i) they are missing from some partic-
ular T ′ ∈ γ (⟨T , n⟩), or (ii) ρ evaluates to maybe. Fortunately,
we will be able to succinctly encompass these possibilities
using our existing lower-level operations.

We define ⟨T , n⟩↓#

ρ as follows: suppose ρ is of the form
λxi . xi ⩽ [a, b), let φa = λxi . xi ⩽ a and let φb = λxi . xi <
b. Because φa and φb are concrete predicates, we can use
Equation 1 (Section 4.4) to compute their abstract semantics.
Then

⟨T , n⟩↓#

ρ := ⟨T , n⟩↓#
φa

⊔ ⟨T , n⟩↓#
φb

.

Proposition B.3. Let T ′ ∈ γ (⟨T , n⟩) and φ ′ ∈ γ (ρ). Then,
T ′↓φ ′ ∈ γ (⟨T , n⟩↓#

ρ ).

Proof. Denote ⟨Sa, ma⟩ = ⟨T , n⟩↓#
φa and ⟨Sb , mb ⟩ = ⟨T , n⟩↓#
φb .
We will show that T ′↓φ ′ ∈ γ (⟨Sa, ma⟩ ⊔ ⟨Sb , mb ⟩). This join
has nice structure since a ⩽ b and thus Sa ⊆ Sb .

We break into two cases, since mb = min(n, |Sb |). First,
suppose mb = |Sb |. Then ⟨Sa, ma⟩ ⊔ ⟨Sb , mb ⟩ = ⟨Sb , |Sb |⟩,
where γ (⟨Sb , |Sb |⟩) = P(T ↓φb ). Because φ ′ ∈ γ (ρ), we then
have T ′↓φ ′ ⊆ T ′↓φb ∈ P(T ↓φb ), and we are done.

16

Samuel Drews, Aws Albarghouthi, and Loris D’Antoni

Otherwise, we have mb = n. Here, ⟨Sa, ma⟩ ⊔ ⟨Sb , mb ⟩ =
⟨Sb , |Sb \Sa | +n⟩ (unless ma = |Sa |, in which case we immedi-
ately collapse to the previous case). Again, because φ ′ ∈ γ (ρ),
we immediately have that T ′↓φ ′ ⊆ Sb ; it remains to show that
|Sb \T ′↓φ ′ | ⩽ |Sb \Sa |+n. Observe that (i)T ↓φ ′ \T ′↓φ ′ ⊆ T \T ′
and thus |T ↓φ ′ \ T ′↓φ ′ | ⩽ n, and (ii) since Sa ⊆ Sb and φ ′ ∈
γ (ρ), we have that |Sb \T ↓φ ′ | ⩽ |Sb \Sa |. Combined, we know
that |Sb \ T ′↓φ ′ | = |Sb \ T ↓φ ′ | + |T ↓φ ′ \ T ′↓φ ′ | ⩽ |Sb \ Sa | + n.

Symbolic Predicates in filter#
R. In the original definition
of filter# we separated Ψ into two sets Ψx and Ψ¬x because
exclusively either φ |= x or ¬φ |= x. In this new three-valued
symbolic predicate case, we appropriately over-approximate
the ρ(x) = maybe possibility.
Let us denote the following:

Ψ#
x
Ψ#
¬x

= {ρ ∈ Ψ# | ρ(x) ∈ {true, maybe}}
= {ρ ∈ Ψ# | ρ(x) ∈ {maybe, false}}

and define

(cid:196)

(cid:196)

filter#

⟨T , n⟩↓#
ρ

R(⟨T , n⟩, Ψ#, x) (cid:66) (cid:169)
(cid:173)
(cid:171)

(cid:170)
(cid:174)
(cid:172)
Proposition B.4. Let T ′ ∈ γ (⟨T , n⟩) and let φ ′ ∈ γ (Ψ#).
Then,

⟨T , n⟩↓#
¬ρ

⊔ (cid:169)
(cid:173)
(cid:171)

ρ ∈Ψ#
¬x

ρ ∈Ψ#
x

(cid:170)
(cid:174)
(cid:172)

filterR(T ′, φ ′, x) ∈ γ (cid:0)filter#

R(⟨T , n⟩, Ψ#, x)(cid:1)

Proof. Because φ ′ ∈ γ (Ψ#), we know there is some ρ ∈ Ψ#
such that φ ′ ∈ γ (ρ). Once again, soundness now follows
from the soundness of the join and the soundness of ↓#
ρ .

Symbolic Predicates in bestSplit#
R. Finally, we discuss the
treatment of our favorite complicated instruction. Perhaps
surprisingly, very little has to change. Indeed, bestSplit#
begins by creating a finite set of symbolic predicates Φ# and
then proceeds exactly as its original definition.

R(⟨T , n⟩)

The construction of Φ# is very simple. In the concrete case
we considered λxi . xi ⩽ a+b
for all adjacent pairs (a, b) of
2
the feature values that occur in T ; here, we will consider
λxi . xi ⩽ [a, b) for all such pairs.

It should be surprising that this computation is sound.
After all, it involves (effectively) computing the argmin over
a set of predicates; here, we’re over-approximating that set,
and we ought to be concerned that our over-approximation
could include extraneous predicates whose valuation is so
small that they occlude the feasible minimizing predicates.
Serendipitously, the choice of Φ# prevents this from happen-
ing.
Lemma B.5. Let T ′ ∈ γ (⟨T , n⟩). Then,
bestSplitR(T ′) ∈ γ (cid:0)bestSplit#

R(⟨T , n⟩)(cid:1)

Proof. We begin by observing two facts about the Φ# con-
structed in bestSplit#
R: (i) For every φ ∈ Φ built during the
non-symbolic bestSplitR(T ′), there exists some ρ ∈ Φ# such

Proving Data-Poisoning Robustness in Decision Trees

that φ ∈ γ (ρ). (ii) For every ρ ∈ Φ#, there exists some
T ′′ ∈ γ (⟨T , n⟩) and φ ′′ ∈ Φ from bestSplitR(T ′′) (φ ′′ is not
necessarily returned as optimal, just constructed for consid-
eration) such that φ ′′ ∈ γ (ρ).

Let φ ′ = bestSplitR(T ′). The rest of the proof is exactly the
same as the soundness proof for bestSplit# (the previous two
observations ensure that Φ#
∀ and Φ#
∃ preserve the properties
necessary for those arguments), except for establishing that
our target φ ′ is in the concretization of some ρ ′ returned
in the then-branch of the definition. Take any φ∗ ∈ γ (ρ∗)
where ρ∗ is the minimizer in lubΦ# . Observe the following

for any ρ ′ ∈ γ (Φ#) such that φ ′ ∈ γ (ρ ′):

lb(score#(⟨T , n⟩, ρ ′)) ⩽ lb(score#(⟨T , n⟩, φ ′))

⩽ score(T ′, φ ′)
⩽ score(T ′, φ∗)
⩽ ub(score#(⟨T , n⟩, φ∗))
⩽ ub(score#(⟨T , n⟩, ρ∗))
= lubΦ#

and thus φ ′ would be included.

C Full Benchmarks
Figures 8, 9, 10, and 11 show the detailed performance metrics
of the remaining datasets.

17

Depth 1

Depth 2

Depth 3

Depth 4

Samuel Drews, Aws Albarghouthi, and Loris D’Antoni

Box
Disjuncts

2
4
Poisoning n

6

0

2
4
Poisoning n

6

0

2
4
Poisoning n

6

0

2
4
Poisoning n

6

d
e
fi
i
r
e
V
#

)
s
(

i

e
m
T
e
g
a
r
e
v
A

x
a
M
e
g
a
r
e
v
A

)
B
M

(
y
r
o
m
e
M

30

20

10

0
1
0.8
0.6
0.4
0.2
0
20

15

10

5

0

0

Figure 8. Iris (Note: this is the only benchmark for which we do not use log-log plots, since the numbers are generally small.

Depth 1

Depth 2

Depth 3

Depth 4

Box
Disjuncts

150

100

50

d
e
fi
i
r
e
V
#

0
10
1
10−1
10−2
10−3
10−4
10−5
1,000

100

10

)
s
(

i

e
m
T
e
g
a
r
e
v
A

x
a
M
e
g
a
r
e
v
A

)
B
M

(
y
r
o
m
e
M

1

2

16 32 64

4
8
Poisoning n

1

2

16 32 64

8
4
Poisoning n

1

2

16 32 64

4
8
Poisoning n

1

2

16 32 64

4
8
Poisoning n

Figure 9. Mammographic Masses

18

Proving Data-Poisoning Robustness in Decision Trees

Depth 1

Depth 2

Depth 3

Depth 4

d
e
fi
i
r
e
V
#

100

50

0

)
s
(

100

Box
Disjuncts

x
a
M
e
g
a
r
e
v
A

)
B
M

(
y
r
o
m
e
M

i

e
m
T
e
g
a
r
e
v
A

10

1

0.1
100,000
10,000
1,000

100
10

1

2

16 32 64

4
8
Poisoning n

1

2

16 32 64

4
8
Poisoning n

1

2

16 32 64

4
8
Poisoning n

1

2

16 32 64

4
8
Poisoning n

Figure 10. Wisconsin Diagnostic Breast Cancer

Depth 1

Depth 2

Depth 3

Depth 4

d
e
fi
i
r
e
V
#

100
80
60
40
20
0
10,000

1,000

100

10

1

100,000

10,000

1,000

100

1

)
s
(

i

e
m
T
e
g
a
r
e
v
A

x
a
M
e
g
a
r
e
v
A

)
B
M

(
y
r
o
m
e
M

Box
Disjuncts

Timeout

OOM

64

8
Poisoning n

512

1

64

8
Poisoning n

512

1

64

8
Poisoning n

512

1

64

8
Poisoning n

512

Figure 11. MNIST-1-7-Real

19

