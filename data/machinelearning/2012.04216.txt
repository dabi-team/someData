Fairness Preferences, Actual and Hypothetical: A Study of Crowdworker
Incentives

Angie Peng 1 Jeff Naecker 1 Ben Hutchinson 1 Andrew Smart 1 Nyalleng Moorosi 1

0
2
0
2
c
e
D
8

]
I

A
.
s
c
[

1
v
6
1
2
4
0
.
2
1
0
2
:
v
i
X
r
a

Abstract

How should we decide which fairness criteria or
deﬁnitions to adopt in machine learning systems?
To answer this question, we must study the fair-
ness preferences of actual users of machine learn-
ing systems. Stringent parity constraints on treat-
ment or impact can come with trade-offs, and
may not even be preferred by the social groups
in question (Zafar et al., 2017). Thus it might
be beneﬁcial to elicit what the group’s prefer-
ences are, rather than rely on a priori deﬁned
mathematical fairness constraints. Simply asking
for self-reported rankings of users is challenging
because research has shown that there are often
gaps between people’s stated and actual prefer-
ences(Bernheim et al., 2013).

This paper outlines a research program and ex-
perimental designs for investigating these ques-
tions. Participants in the experiments are invited
to perform a set of tasks in exchange for a base
payment—they are told upfront that they may
receive a bonus later on, and the bonus could de-
pend on some combination of output quantity and
quality. The same group of workers then votes on
a bonus payment structure, to elicit preferences.
The voting is hypothetical (not tied to an outcome)
for half the group and actual (tied to the actual
payment outcome) for the other half, so that we
can understand the relation between a group’s
actual preferences and hypothetical (stated) pref-
erences. Connections and lessons from fairness
in machine learning are explored.

1. Introduction

The increasingly large-scale deployments of AI systems on
global populations require large quantities of resources and

*Equal contribution 1Google AI, Mountain View, CA, USA.

Correspondence to: Angie Peng <apeng@google.com>.

Presented at the 4th Workshop on Mechanism Design for Social
Good. Copyright 2020 by the author(s).

coordination (Crawford & Joler, 2018; Amodei & Hernan-
dez, 2018), which are often achieved through centralized
and top-down decision-making processes. In doing so, the
diversity of stakeholders and their perspectives are often
given short shrift. Indeed, even when deployment objectives
include moral values such as fairness, we often see ﬁrst
principles-based approaches, which aim to proceed from
abstract ideals. However, stringent top-down constraints
on treatment or impact can come with trade-offs, and these
constraints may not even be preferred by the groups most
affected by the system. (Zafar et al., 2017).

What if instead, the fairness properties of a machine learning
system were engineered to better incorporate the preferences
of its users? Answering this question is important because
qualitative work has shown that perceptions of algorithmic
unfairness among technology users negatively affects their
trust in a company or product (Woodruff et al., 2018). We
argue that to design ML for stakeholders, we must 1) un-
derstand dimensions of variation of their preferences and
2) design social choice mechanisms for aggregating diverse
individual preferences to reach a collective decision (Ar-
row, 1951). While there have been some studies seeking
to understand people’s preferences regarding ML models
(Grgic-Hlaca et al., 2018; Harrison et al., 2020; Binns et al.,
2018; Jahanbakhsh et al., 2017; Saxena et al., 2019; Srivas-
tava et al., 2019), most previous work on eliciting judgments
around fairness have typically focused on qualitative percep-
tions (Woodruff et al., 2018), hypothetical scenarios (Lee,
2018), or actual scenarios in a hypothetical setting (Harrison
et al., 2020). In contrast, our goal is to measure fairness
preferences for the same situation in both a hypothetical and
actual setting in order to measure the gap between stated
and actual preferences. Questions of preferences are chal-
lenging because research has shown that there are often gaps
between people’s stated and actual preferences(Samuelson,
1948). Quantifying the relationship between stated and real
preferences then allows us to quickly and more efﬁciently
make predictions about actual preferences from survey data.

Studying preferences in this area is particularly challenging
for another reason: research has showing that people can
reverse their preferences depending on the context, violating
some common modeling assumptions, such as time consis-
tency of preferences. In the speciﬁc context of considering

 
 
 
 
 
 
Fairness Preferences, Actual and Hypothetical: A Study of Crowdworker Incentives

“fairness”, Andreoni et al. (2018) have shown that whether
the preference between equality of opportunity (ax ante
fairness) and equality of outcomes (ex post fairness) may
not even be stable within individuals. Thus when eliciting
preferences in this area, we must be careful to consider how
luck and randomness play a role.

In this paper, we present an experimental design which aims
to elicit preferences from an often-overlooked stakeholder
group: the crowdworkers who annotate ML data sets (Gray
& Suri, 2019; Ross et al., 2010). Given the structural corre-
spondences between pay incentive structures and ML sys-
tems (which we’ll describe in Section 2), these experiments
also serve as a useful proxy for discussing perspectives on
fairness in ML more generally.

This paper is structured as follows. We ﬁrst provide a brief
overview of the structural relationships between fairness
in the context of crowdworker incentives and fairness the-
orems in machine learning. These relationships motivate
the design of the experiment which we proceed to outline
in Section 3, including ethical considerations in the design.
We then describe how the experimental results will be used
to shed new light on relationships between stated and actual
preferences around ML model deployments, and conclude
with a discussion of a larger research agenda that could build
from these results.

2. Fair Pay/Work Criteria

The crowdworkers who label the datasets powering ML
systems are a critical part of the ML economy, and as such
their employment conditions are deserving of consideration
(Gray & Suri, 2019; Ross et al., 2010). Concerns about
crowdworkers are furthermore situated within a much bigger
societal conversation about how fair and equitable pay might
be determined for different types of work. One pertinent
question is how the aggregate value paid to a workforce
should be divided between the individual workers.1 In this
paper we focus on the relationship of pay to the measurable
durations and outputs of work. That is, for the sake of
this paper, we suppose that the fairness of pay could be
linked to the inputs of the labor process, such as time and
intensity spent on work, as well as the quality and quantity
of the outputs. Notions of fair pay under consideration may
appeal to any or all of these quantities. In some cases, input
and output might be highly correlated, but in other cases
difference in technology – or simply luck – may disrupt this
relationship.

A direct parallel can be made between fairness in pay and
machine learning fairness. In particular, Table 1 outlines
some structural relationships between different possible cri-

1This paper does not consider the question of how much of the

value of the collective work should be returned to the workers.

Pay criterion

ML criterion

ML terminology

Group ⊥ (cid:98)Y

Group ⊥ P ay
Group ⊥ P ay| (cid:91)labor Group ⊥ (cid:98)Y |Y
Group ⊥ (cid:91)labor|P ay Group ⊥ Y | (cid:98)Y

Demographic parity

Equal opportunity

Sufﬁciency

Table 1. Comparison of a selection of possible fairness criteria in
the ML and pay domains. “Hat”-notation (ˆ·) denotes the estimate
of an actual value. Labor can refer to a combination of input and
output variables (e.g. complexity of problems given and quantity
of tasks completed).

teria of fairness in the domains of pay and machine learning.
We claim that these deﬁnitions model several important di-
mensions of fairness of pay/work, such as “is it unfair if I
am paid less than someone who does the same work as me?”
(second line of the table), and “is it unfair if I do more work
than someone who is paid the same as me?” (third line of
the table).

One complicating factor is that, in many scenarios, labor
can be hard or impossible to measure accurately. We assert
that this actually directly parallels the situation in many
machine learning tasks, in which the appropriate sociotech-
nical framing of fairness is not just complex and contextual
(Selbst et al., 2019), but for which the question of construct
measurement is critical (Jacobs & Wallach, 2019; Jacobs
et al., 2020).

With these considerations in mind, it is instructive to recon-
sider the so-called “group fairness impossibility” theorems
of recent years (Pleiss et al., 2017; Kleinberg et al., 2016;
Chouldechova, 2017). In the fair pay scenario, we have the
following directly analogous result.

Theorem 2.1 (Fair Pay Impossibility Result). Suppose that
pay is calculated as a function of the measured outputs of
work, but that these outputs are not a perfect proxy for labor.
Furthermore suppose that different groups have different
distributions of pay and/or labor. Then it is impossible
for the following to both hold: Group ⊥ P ay|labor and
Group ⊥ labor|P ay. Similarly, it is impossible for the
following to both hold: P ay ⊥ labor|Outputs and P ay ⊥
Outputs|labor.

Proof. Follows from direct consideration of the fairness in
ML impossibility results in Kleinberg et al. (2016).

Where does this leave us? Darlington (1971) made two
relevant observations when considering a similar scenario
regarding university admissions. First, the more accurate
that our measurement of outputs is, the “closer” we can get
to achieving the incompatible conditions listed in above.
(And conversely, if measurements are extremely noisy then

Fairness Preferences, Actual and Hypothetical: A Study of Crowdworker Incentives

one independence condition or the other—or both!—must
be violated to a large degree.) Second, one possible res-
olution that Darlington suggests is to poll people in order
to determine socially acceptable trade offs. In the follow-
ing section, we describe an experimental design aimed at
precisely this.

3. Proposed Experiment

3.1. Overview

Participants are invited to perform a set of tasks in exchange
for a base payment - they are told upfront that they may re-
ceive a bonus later on, and the bonus could depend on some
combination of output quantity and quality. In our particular
experiment, we have set up a standard transcription task for
participants to perform; the task itself could theoretically be
anything, so long as there is something that results in work
that varies by quality and quantity. The goal of having par-
ticipants complete a task is to then give the same group the
ability to vote on a bonus payment structure. It is important
that the group voting has actually done some work, so that
we can understand the group’s actual preferences instead of
hypothetical (stated) preferences

3.2. Speciﬁc Task

Our speciﬁc task involves transcription short images of hand-
written lines through Amazon Mechanical Turk. Partici-
pants are given one hour to complete the following:

• Answer questions about their demographic background

• Read through instructions and do a few practice tran-

scription tasks

• Transcribe as many handwritten lines to the best of
their ability. See an example transcription task below
(Figure 1).

broader populations in the US, we want to use the demo-
graphics collected to provide a comparison point against
broader population distributions.

3.4. Voting on a Bonus Payment

Of the 1,000 participants, we propose a 4-cell treatment
(Table 2) in asking for bonus payment preferences. We
will have a set of bonus payment options and will give
participants a series of pairwise choices (example in Figure
2) to choose between.

• Group A will be asked for their stated preferences, but
the actual bonus payment will be based on their votes.

• Group B will be asked for their preferences, and they
will be told that one of their votes will be implemented
at random to ensure incentive compatibility (random
serial dictatorship model).

• Within Group A and Group B, half of each group will
receive some information on how each bonus structure
would actually map to the distribution of work done
by all participants (e.g. min, max, and mean payment
amounts)

• The other half of Group A and Group B will receive
only the bonus structure to vote on. (See Figure 2 for
an example.)

Hypothetical voting
Actual voting

No distribution info
Group A1 (n = 250)
Group B1 (n = 250)

Some distribution info
Group A2 (n = 250)
Group B2 (n = 250)

Table 2. An overview of treatment groups.

Figure 2. An example pairwise choice presented to participants

Figure 1. An example transcription task (from IAM handwriting
database)

4. Planned Analysis

3.3. Data Collection

Initially, our experiment will run in the United States, though
one of our goals for expansion is to do the same experiment
in India to get a sense of differences by country. Our plan
is to collect 1,000 complete tasks and ask participants to
self-report information on the following demographics: age
group, geographic region (within the United States), and
race/ethnicity. Given that mTurk is not representative of

Ideally, we would like to know people’s preferences re-
garding model deployment impacts before the model gets
deployed. However, prior to the effects of deployment being
felt, the best we can do is gather hypothetical preferences
regarding if the model were to be deployed in a certain man-
ner. Although such hypothetical responses are easy to col-
lect, they are only noisy estimates of real (post-deployment)
preferences (see, e.g., Tan et al., 2018, regarding privacy
preferences).

Fairness Preferences, Actual and Hypothetical: A Study of Crowdworker Incentives

Our approach is therefore to use estimation techniques to
predict preferences concerning new policies before they are
actually implemented. In the case of our experiment, we
will have collected hypothetical and actual preferences so
can also compare the estimates with actual data.

Predicting actual preferences from hypothetical preferences
will involve two main steps (Bernheim et al., 2013). First,
we train a regression model which predicts real bonus allo-
cation choices—under a “no information” condition—from
hypothetical bonus allocation choices (under the same con-
dition). Then, we use this model to predict real choices
under the “some information” condition, using hypothetical
choices under the same condition

5. Discussion and Future Work

As discussed in Section 2, there are strong structural corre-
spondences between pay/work and ML. Thus the experiment
proposed in Section 3 enables us to indirectly investigate
preferences concerning the latter using concrete and easily
comprehensible questions about preferences regarding the
former, using the analyses in Section 4. However, fairness
is highly contextual, and we are careful not to claim that
fairness concerns from one domain can simply be “ported”
to another. Further domain-speciﬁc work, both pre- and
post-deployment, will always be required when extrapolat-
ing stated preferences to actual preferences in different ML
application domains.

Our approach aims to aggregate divergent interpretations
of constructs like “fairness”, but more work is required to
bridge this line of preference-based work with approaches to
fairness estimation based on techniques from measurement
modeling (Jacobs & Wallach, 2019). Beyond measurement,
there is also the question of how to best combine stakeholder
preferences in a broad ecosystem. Consider the stakeholders
in a typical ecosystem in which ML is deployed, such as a
social media platform: content creators, content consumers,
advertisers, platform owners, etc. Each of these groups has
different objectives and considerations, including different
lenses on what is relevant to fairness. We might consider,
for example, fairness for different groups of creators, for
different groups of consumers, for different groups of adver-
tisers, etc. And although much has been written about the
incompatibilities of different technical deﬁnitions of “fair-
ness” (Kleinberg et al., 2016; Chouldechova, 2017; Verma &
Rubin, 2018), little has been written about the incompatibili-
ties of perspectives of the multitude of stakeholder roles. We
propose examining the incentives for one particular group
of stakeholders in this experiment, but in the future would
seek to connect this to the broader system of stakeholders
in a typical ML ecosystem.

porate different stakeholders into the design and creation
of ML systems by speciﬁc entities with different value sys-
tems, which is another area of future work that our proposed
experiment can help inform.

References

Amodei, D. and Hernandez, D. AI and compute. URL
https://blog. openai. com/ai-and-compute, 31, 2018.

Andreoni, J., Aydin, D., Barton, B., Bernheim, B. D., and
Naecker, J. When fair isn’t fair: Understanding choice
reversals involving social preferences. Technical report,
National Bureau of Economic Research, 2018.

Arrow, K. Social choice and individual values, Cowles
Foundation Res. Econ. Yale Univ. In Cowles Commission
Monograph, volume 12. Wiley, NY and Chapman and
Hall Los Alamitos, CA, 1951.

Bernheim, B. D., Bjorkegren, D., Naecker, J., and Rangel,
A. Non-choice evaluations predict behavioral responses
to changes in economic conditions. Technical report,
National Bureau of Economic Research, 2013.

Binns, R., Van Kleek, M., Veale, M., Lyngs, U., Zhao, J.,
and Shadbolt, N. ‘It’s reducing a human being to a per-
centage’ perceptions of justice in algorithmic decisions.
In Proceedings of the 2018 CHI Conference on Human
Factors in Computing Systems, pp. 1–14, 2018.

Chouldechova, A. Fair prediction with disparate impact: A
study of bias in recidivism prediction instruments. Big
data, 5(2):153–163, 2017.

Crawford, K. and Joler, V. Anatomy of ai, 2018.

Darlington, R. B. Another look at “cultural fairness” 1.
Journal of Educational Measurement, 8(2):71–82, 1971.

Gray, M. L. and Suri, S. Ghost Work: How to Stop Silicon
Valley from Building a New Global Underclass. Eamon
Dolan Books, 2019.

Grgic-Hlaca, N., Redmiles, E. M., Gummadi, K. P., and
Weller, A. Human perceptions of fairness in algorithmic
decision making: A case study of criminal risk prediction.
In Proceedings of the 2018 World Wide Web Conference,
pp. 903–912, 2018.

Harrison, G., Hanson, J., Jacinto, C., Ramirez, J., and Ur, B.
An empirical study on the perceived fairness of realistic,
imperfect machine learning models. In Proceedings of
the 2020 Conference on Fairness, Accountability, and
Transparency, pp. 392–402, 2020.

Lastly, there is also a value-laden question of how to incor-

Jacobs, A. Z. and Wallach, H. Measurement and fairness.

arXiv preprint arXiv:1912.05511, 2019.

Fairness Preferences, Actual and Hypothetical: A Study of Crowdworker Incentives

Tan, J., Sharif, M., Bhagavatula, S., Beckerle, M., Mazurek,
M. L., and Bauer, L. Comparing hypothetical and realistic
privacy valuations. In Proceedings of the 2018 Workshop
on Privacy in the Electronic Society, pp. 168–182, 2018.

Verma, S. and Rubin, J. Fairness deﬁnitions explained. In
Proceedings of the International Workshop on Software
Fairness, pp. 1–7, 2018.

Woodruff, A., Fox, S. E., Rousso-Schindler, S., and War-
shaw, J. A qualitative exploration of perceptions of algo-
rithmic fairness. In Proceedings of the 2018 CHI Confer-
ence on Human Factors in Computing Systems, pp. 1–14,
2018.

Zafar, M. B., Valera, I., Rodriguez, M., Gummadi, K., and
Weller, A. From parity to preference-based notions of fair-
ness in classiﬁcation. In Advances in Neural Information
Processing Systems, pp. 229–239, 2017.

Jacobs, A. Z., Blodgett, S. L., Barocas, S., Daum´e III, H.,
and Wallach, H. The meaning and measurement of bias:
lessons from natural language processing. In Proceedings
of the 2020 Conference on Fairness, Accountability, and
Transparency, pp. 706–706, 2020.

Jahanbakhsh, F., Fu, W.-T., Karahalios, K., Marinov, D., and
Bailey, B. You want me to work with who? stakeholder
perceptions of automated team formation in project-based
courses. In Proceedings of the 2017 CHI Conference on
Human Factors in Computing Systems, pp. 3201–3212,
2017.

Kleinberg, J., Mullainathan, S., and Raghavan, M. Inherent
trade-offs in the fair determination of risk scores. arXiv
preprint arXiv:1609.05807, 2016.

Lee, M. K. Understanding perception of algorithmic de-
cisions: Fairness, trust, and emotion in response to
algorithmic management. Big Data & Society, 5(1):
2053951718756684, 2018.

Pleiss, G., Raghavan, M., Wu, F., Kleinberg, J., and Wein-
berger, K. Q. On fairness and calibration. In Advances in
Neural Information Processing Systems, pp. 5680–5689,
2017.

Ross, J., Irani, L., Silberman, M. S., Zaldivar, A., and
Tomlinson, B. Who are the crowdworkers? shifting
demographics in Mechanical Turk. In CHI’10 extended
abstracts on Human factors in computing systems, pp.
2863–2872, 2010.

Samuelson, P. A. Consumption theory in terms of revealed

preference. Economica, 15(60):243–253, 1948.

Saxena, N. A., Huang, K., DFlippis, E., Radanovic, G.,
Parkes, D. C., and Liu, Y. How do fairness deﬁni-
tions fare?: Examining public attitudes towards algorith-
mic deﬁnitions of fairness. In Proceedings of the 2019
AAAI/ACM Conference on AI, Ethics, and Society, pp.
99–106, 2019.

Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubrama-
nian, S., and Vertesi, J. Fairness and abstraction in so-
ciotechnical systems. In Proceedings of the Conference
on Fairness, Accountability, and Transparency, pp. 59–
68, 2019.

Srivastava, M., Heidari, H., and Krause, A. Mathematical
notions vs. human perception of fairness: A descriptive
approach to fairness for machine learning. In Proceedings
of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pp. 2459–2468,
2019.

