9
1
0
2

r
p
A
6
2

]
S
D
.
s
c
[

1
v
2
8
6
2
1
.
4
0
9
1
:
v
i
X
r
a

An eﬃcient branch-and-cut algorithm
for approximately submodular function maximization

Naoya Uematsu ∗1,2, Shunji Umetani †1,2, and Yoshinobu Kawahara ‡1,3

1RIKEN Center for Advanced Intelligence Project, Quantative Biology Center, 6-2-4
Fruedai, Suita, Osaka, 565-0874, Japan.
2Graduate School of Information Science and Technology, Osaka University, 1-5
Yamadaoka, Suita, Osaka, 565-0871, Japan.
3Institute of Mathematics for Industry, Kyushu University, 744 Motooka, Fukuoka,
Fukuoka, 819-0395, Japan.

Abstract

When approaching to problems in computer science, we often encounter situations where
a subset of a ﬁnite set maximizing some utility function needs to be selected. Some of such
utility functions are known to be approximately submodular. For the problem of maximizing
an approximately submodular function (ASFM problem), a greedy algorithm quickly ﬁnds
good feasible solutions for many instances while guaranteeing (1 − e−γ)-approximation ratio
for a given submodular ratio γ. However, we still encounter its applications that ask more
accurate or exactly optimal solutions within a reasonable computation time. In this paper, we
present an eﬃcient branch-and-cut algorithm for the non-decreasing ASFM problem based on
its binary integer programming (BIP) formulation with an exponential number of constraints.
To this end, we ﬁrst derive a BIP formulation of the ASFM problem and then, develop an
improved constraint generation algorithm that starts from a reduced BIP problem with a
small subset of constraints and repeats solving the reduced BIP problem while adding a
promising set of constraints at each iteration. Moreover, we incorporate it into a branch-
and-cut algorithm to attain good upper bounds while solving a smaller number of nodes of
a search tree. The computational results for three types of well-known benchmark instances
shows that our algorithm performs better than the conventional exact algorithms.

1 INTRODUCTION

When approaching to problems in computer science, we often encounter situations where a
subset of a ﬁnite set maximizing some utility function needs to be selected. Some of such
utility functions are known to be submodular (e.g., sensor placement (Golovin and Krause,
2011; Kawahara et al., 2009; Kratica et al., 2001), document summarization (Lin and Bilmes,
2011), and inﬂuence spread problems (Kempe et al., 2003; Sakaue and Ishihata, 2018)). A set
function f : 2N → R is called submodular if it satisﬁes f (S ∪ {i}) − f (S) ≥ f (T ∪ {i}) − f (T ) for
all S ⊆ T ⊆ N and i /∈ T , where N := {1, . . . , n} is a ﬁnite set. Submodular functions can be
considered as discrete counterparts of convex functions through the continuous relaxation called
the Lov´asz extension (Lov´asz, 1983).

∗naoya.uematsu@riken.jp
†umetani@ist.osaka-u.ac.jp
‡kawahara@imi.kyushu-u.ac.jp

1

 
 
 
 
 
 
Meanwhile, in many practical situations, utility functions may not necessarily be submod-
ular. However even in those cases, submodularity can be approximately satisﬁed in various
problems such as feature selection (Das and Kempe, 2011; Yu and Liu, 2004), boosting inﬂu-
ence spread (Lin et al., 2018), data summarization (Balkanski et al., 2016) and combinatorial
auction (Conitzer et al., 2005). For this reason, the optimization of an approximately sub-
modular function has been attracted an increasing attention recently (Das and Kempe, 2018;
Horel and Singer, 2016; Krause and Golovin, 2014). This type of function is deﬁned with a
submodular ratio γ, which deﬁned for a set function f as the maximum value 0 < γ ≤ 1 such
that f (S ∪ {i}) − f (S) ≥ γ (f (T ∪ {i}) − f (T )), for all S ⊆ T ⊆ N and i /∈ T . That is, a
submodular ratio γ measures how close the function is to submodular (Das and Kempe, 2011;
Johnson et al., 2016).

In this paper, we address the problem of maximizing a non-decreasing approximately sub-
modular function f under a cardinality constraint (hereafter, referred to as approximately sub-
modular function maximization (ASFM) problem):

maximize
f (S)
subject to |S| ≤ k, S ⊆ N,

(1)

where k ≤ n is a positive integer comprising the cardinality constraint. A set function is non-
decreasing if f (S) ≤ f (T ) for all S ⊆ T and f (∅) = 0. Das and Kempe (2011) presented a
greedy algorithm for the ASFM problem that guarantees (1 − e−γ)-approximation ratio for a
given submodular ratio γ . Chen et al. (2015) proposed an A∗ search algorithm to obtain an
exactly optimal solution for the ASFM problem. Their algorithm computes an upper bound by
a variant of variable ﬁxing techniques with O(n) oracle queries. Their algorithm quickly ﬁnds
upper bounds; however the attained upper bounds are not often tight enough to prune nodes of
the search tree eﬀectively. Therefore, their algorithm often processes a huge number of nodes of
the search tree until obtaining an optimal solution.

Here, we present an eﬃcient branch-and-cut algorithm for the ASFM problem based on
its binary integer programming (BIP) formulation with an exponential number of constraints.
To this end, we ﬁrst derive a BIP formulation of the ASFM problem and then, develop a
modiﬁed constraint generation algorithm based on the BIP formulation. Unfortunately, the
modiﬁed constraint generation algorithm is not eﬃcient because of a large number of reduced
BIP problems to be solved. To overcome this, we propose an improved constraint generation
algorithm, where a promising set of constraints is added at each iteration. We further incorporate
it into a branch-and-cut algorithm to attain good upper bounds while solving a smaller number
of reduced BIP problems. Finally, we evaluate our algorithms under comparisons with the
existing ones using three types of well-known benchmark instances and the combinatorial auction
problem.

The remainder of this paper is organized as follows. First, in Section 2, we give a brief review
of the existing algorithms. In Section 3, we derive the IP formulation of the ASFM problem.
Then, in Section 4, we propose three algorithms for solving the ASFM problem. We illustrate
the eﬀectivity of the proposed algorithm with the combinatorial auction problem in Section 5
and show some computational results using three types of well-known benchmark instances in
Section 6. Finally, the paper is concluded in Section 7.

2 EXISTING ALGORITHMS

Here, we ﬁrst review the constraint generation algorithm by Nemhauser and Wolsey (1981) for
the problem (1) when f is not approximately but exactly submodular (referred to as submod-

2

ular function maximization (SFM) problem) in Subsection 2.1 and then, A∗ search algorithm
proposed by Chen et al. (2015) for the ASFM problem in Subsection 2.2.

2.1 Constraint Generation Algorithm for the SFM Problem

Nemhauser and Wolsey (1981) have proposed an exact algorithm for the SFM problem, called
the constraint generation algorithm. The algorithm starts from a reduced BIP problem with a
small subset of constraints and then, repeats solving the reduced BIP problem while adding a
new constraint at each iteration.

Given a set of feasible solutions Q ⊆ F , we deﬁne BIP(Q) as the following reduced BIP

problem of the SFM problem:

maximize
z
subject to z ≤ f (S) +

f ({i} | S) yi, S ∈ Q,

Xi∈N \S

yi ≤ k,

i∈N
X
yi ∈ {0, 1}, i ∈ N.

(2)

[0] , . . . , S(0)

[0] , . . . , S(0)

[k−1], S(0), . . . , S(t−1)} to obtain an optimal solution y(t) = (y(t)

The initial solution S(0) is obtained by applying the greedy algorithm (Minoux, 1978; Nemhauser et al.,
1978). Their algorithm starts with a set Q = {S(0)
[k] }, where S[i] denotes the ﬁrst i ele-
ments of a feasible solution S(0) with the order obtained by the greedy algorithm. We now con-
sider the t-th iteration of the constraint generation algorithm. The algorithm ﬁrst solves BIP(Q)
1 , . . . , y(t)
with Q = {S(0)
n )
and the optimal value z(t) that gives an upper bound of that of the problem (2). Let S(t) denote
the optimal solution of BIP(Q) corresponding to y(t), and S∗ denote the incumbent solution
of the problem (2) obtained so far. If f (S(t)) > f (S∗) holds, then the algorithm replaces the
incumbent solution S∗ with S(t). If z(t) > f (S(t)) holds, the algorithm concludes S(t) /∈ Q and
adds S(t) to Q, because S(t) does not satisfy any constraints of BIP(Q). That is, the algorithm
adds the following constraint to BIP(Q) for improving the upper bound z(t) of the optimal value
of the problem (2).

z ≤ f (S(t)) +

f ({i} | S(t)) yi.

(3)

These procedures are repeated until z(t) and f (S∗) meet.

Xi∈N \S(t)

The pseudo code of this algorithm is shown below. We note that the value of z(t) is non-
n
k

increasing with the number of iterations and the algorithm must terminate after at most
iterations.

(cid:0)

(cid:1)

Algorithm CG(S(0))

Input: The initial feasible solution S(0).

Output: The incumbent solution S∗.

Step1:

Set Q ← {S(0)

[0] , . . . , S(0)

[k] }, S∗ ← S(0) and t ← 1.

Step2:

Solve BIP(Q). Let S(t) and z(t) be an optimal solution and the optimal value of

BIP(Q), respectively.

Step3:

If f (S(t)) > f (S∗) holds, then set S∗ ← S(t).

Step4:

If z(t) = f (S∗) holds, then output the incumbnet solution S∗ and exit. Otherwise;

(i.e., z(t) > f (S∗) ≥ f (S(t))), set Q ← Q ∪ {S(t)}, t ← t + 1 and return to Step2.

3

2.2 A∗ Search Algorithm for the ASFM Problem

Chen et al. (2015) have proposed an A∗ search algosithm for the ASFM problem. We ﬁrst
deﬁne the search tree of the A∗ search algorithm. Each node S of the search tree represents a
feasible solution, where the root node is set to S ← ∅. The parent of a node T is deﬁned as
S = T \ {Tmax}, where Tmax is an element i ∈ T with the largest number. For example, node
S = {3} is the parent of node T = {3, 5}, since T \ {Tmax} = {3, 5} \ {5} = {3} = S. The
A∗ search algorithm employs a list L to manage nodes of the search tree. The value of a node
S is deﬁned as ¯f (S) = f (S) + h(S), where h(·) is a heuristic function. We note that ¯f (·) give
an upper bound of the optimal value of the SFM problem at the node S.

The initial feasible solution is obtained by the greedy algorithm (Minoux, 1978; Nemhauser et al.,

1978). The algorithm repeats to extract a node S with the largest value ¯f (·) from the list L
and insert its children T ∈ F into the list L at each iteration. Let S ∈ F be a node extracted
from the list L, and S∗ be the incumbent solution (i.e., best feasible solution obtained so far).
The algorithm obtains a feasible solution S′ ∈ F from the node S, e.g. a variety of greedy
algorithms. If f (S′) > f (S∗) holds, then the algorithm replaces the incumbent solution S∗ with
S′. Then, all children T ∈ F of the node S satisfying ¯f (T ) > f (S∗) are inserted into the list L.
The algorithm repeats these procedures until the list L becomes empty.

The pseudo code of this algorithm is shown below.

Algorithm A∗(S)

Input: The initial feasible solution S.

Output: The incumbent solution S∗.

Step1:

Set L ← {∅} and S∗ ← S.

Step2:

If L = ∅ holds, then output the incumbent solution S∗ and exit.

Step3: Extract a node S with the largest value ¯f (·) from the list L. If ¯f (S) ≤ f (S∗) holds,

then return to Step 2.

Step4: Obtain a feasible solution S′ ∈ F from the node S. If f (S′) > f (S∗) holds, then set

S∗ ← S′.

Step5:

Set L ← L ∪ {T } for all children T of the node S satisfying T ∈ F and ¯f (T ) > f (S∗).

Return to Step2.

We then illustrate a heuristic function h(·) applied to the A∗ search algorithm. Let S be
the current node of the A∗ search algorithm. We consider the following reduced problem of the
SFM problem for obtaining h(·).

maximize
subject to T ⊆ N \ S+, |T | ≤ k − |S|,

fS(T )

(4)

where S+ = {i ∈ N | i ≤ Smax} and fS(·) = f (· | S). Let T ∗ be an optimal solution of the
reduced problem (4). By approximately submodularity, we obtain
i∈T (1/γ)fS ({i}) ≥ fS(T )
for any T ⊆ N and the following inequality.

P

max
T ⊆N \S+,|T |≤k−|S|

fS({i}) ≥

1
γ

Xi∈T

1
γ

Xi∈T ∗

fS({i}) ≥ fS(T ∗).

(5)

Since the reduced problem (4) is still NP-hard, we consider obtaining an upper bound of fS(T ∗).
Let ¯S+ be the non-increasing ordered set with respect to fS({i}) for i ∈ N \ S+. We assume

4

that |S ∪ ¯S+| > k, because we can obtain the upper bound by computing f (S ∪ ¯S+) in otherwise.
[p] denote the set of the ﬁrst p = k − |S| elements of the sorted set ¯S+.
Let [p] = {1, . . . , p} and ¯S+
We then deﬁne a heuristic function h(·) by

h(S) =

1
γ

fS({i}).

Xi∈ ¯S+

[p]

(6)

We note that we let ¯S+
holds for some i ∈ ¯S+
S, we compute an upper bound ¯f (S) = f (S) + h(S).

[p] ∪ S be a feasible solution S′ ∈ F for the node S (Step 4). If fS({i}) = 0
[p], then we conclude fS( ¯S+
[p]) = fS(T ∗) by submodularity. For a given node

3 IP FORMULATION

In this section, we formulate the ASFM problem into a BIP problem. First, the submodular
ratio γ is obtained as follows:

γ = min
S,T ⊆N

f (S) − f (S ∩ T )
f (S ∪ T ) − f (T )

,

(7)

where we regard 0/0 = 1. According to Johnson et al. (2016), we now deﬁne an upper bound ¯γ
of the submodular ratio γ as follows:

¯γ = min
S⊆N

f ({i} | S)
f ({i} | S ∪ {j})

,

(8)

where i /∈ S ∪ {j}.

Proposition 1 A function f (·) is approximately submodular if there exists constants 1 ≥ ¯γ ≥
γ > 0 satisfying any of the following hold:

(i) f (A) − f (A ∩ B) ≥ γ(f (A ∪ B) − f (B)), ∀A, B ⊆ N.

(ii) f ({i} | S) ≥ γ f ({i} | T ), ∀S ⊆ T ⊆ N , i /∈ T.

(iii) f ({i} | S) ≥ ¯γ f ({i} | S ∪ {e}), ∀S ⊆ N, i /∈ S ∪ {e}.

(iv) f (T ) ≤ f (S) + f ({j1} | S) + 1

¯γ f ({j2} | S) +

j∈T \(S∪{j1,j2})

1
γ f ({j} | S) −

i∈S\T γf ({i} |

T ∪ S \ {i}), ∀S, T ⊆ N , j1, j2 ∈ T \ S.

(v) f (T ) ≤ f (S) + f ({j1} | S) + 1

N, j1, j2 ∈ T \ S.

P
¯γ f ({j2} | S) +

P
1
γ f ({j} | S),

∀S ⊆ T ⊆

j∈T \(S∪{j1,j2})

P

The proof of Proposition 1 is in the Appendix.

Proposition 2 A function f (·) is non-decreasing approximately submodular if there exists con-
stants 1 ≥ ¯γ ≥ γ > 0 satisfying any of the following hold:

(i∗) f (A) − f (A ∩ B) ≥ γ (f (A ∪ B) − f (B)), ∀A ⊆ B ⊆ N, f (A) ≤ f (B).

(ii∗) f ({i} | S) ≥ γ f ({i} | T ) ≥ 0, ∀S ⊆ T ⊆ N , i /∈ T.

(iv∗) f (T ) ≤ f (S) + f ({j1} | S) + 1

¯γ f ({j2} | S) +

j∈T \(S∪{j1,j2})

1
γ f ({j} | S), ∀S, T ⊆

N, j1, j2 ∈ T \ S.

P

5

The proof of Proposition 2 is in the Appendix. We next consider a set X of (η, y) satisfying the
following condition.

η ≤ f (S) + f ({j1} | S)yj1 + 1
∀S ⊆ N, |S| ≤ k, j1, j2 ∈ N \ S,

¯γ f ({j2} | S)yj2 +

P

j∈N \(S∪{j1,j2})

1
γ f ({j} | S)yj,

(9)

where y = {y1, . . . , yn} ∈ {0, 1}n.

Proposition 3 Suppose f (·) is a non-decreasing approximately submodular function, (η, y) ∈ X
if and only if η ≤ f (T ), T = {j ∈ N | yj = 1} ⊆ N .

The proof of Proposition 3 is in the Appendix. We now replace ¯γ with γ due to ¯γ ≥ γ. We
formulate the ASFM problem into the following BIP problem (10).

maximize

z

subject to z ≤ f (S) + f ({j} | S)yj +

j ∈ N \ S, S ∈ F,

yi ≤ k,

Xi∈N
yi ∈ {0, 1}, i ∈ N,

1
γ

f ({i} | S)yi,

Xi∈N \(S∪{j})

(10)

where F denotes the set of all feasible solutions satisfying the cardinality constraint |S| ≤ k.

4 PROPOSED ALGORITHMS

We ﬁrst present a modiﬁed constraint generation algorithm for the ASFM problem based on the
algorithm (Nemhauser and Wolsey, 1981) in Subsection 2.1. The modiﬁed constraint generation
algorithm often needs to solve a large number of reduced BIP problems because of generating
only one constraint at each iteration. We accordingly propose an improved constraint generation
algorithm to generate a promising set of constraints for attaining good upper bounds while
solving a smaller number of reduced BIP problems in Subsection 4.2. Moreover, we develop a
branch-and-cut algorithm by using the above algorithm in Subsection 4.3.

4.1 Modiﬁed Constraint Generation Algorithm

We ﬁrst deﬁne BIP(Q) as the following reduced BIP problem of the problem (10).

maximize

z

subject to z ≤ f (S) + f ({j} | S)yj +

j ∈ N \ S, S ∈ Q,

yi ≤ k,

Xi∈N
yi ∈ {0, 1}, i ∈ N,

1
γ

f ({i} | S)yi,

Xi∈N \(S∪{j})

(11)

where j = argmaxi∈N \Sf ({i} | S). We propose a modiﬁed constraint generation algorithm
for the ASFM problem based on the constraint generation algorithm for the SFM problem
(Subsection 2.1), where the proposed algorithm solves the above problem (11) instead of (2).

6

4.2

Improved Constraint Generation Algorithm

1 , . . . , y(t)

Let y(t) = (y(t)
n ) and z(t) be an optimal solution and the optimal value of BIP(Q) at
the t-th iteration of the constraint generation algorithm, respectively. We note that z(t) gives
an upper bound of the optimal value of the problem (10). To improve the upper bound z(t), it
is necessary to add a new feasible solution S′ ∈ F to Q satisfying the following inequality.

z(t) > f (S′) + f ({j} | S′)y(t)

j +

1
γ

f ({i} | S′) y(t)
i

.

(12)

Xi∈N \(S′∪{j})

For this purpose, we now consider the following problem to generate a new feasible solution
S′ ∈ F adding to Q called the separation problem.

minimize

f (S) + f ({j} | S)y(t)

j +

subject to |S| ≤ k, S ⊆ N, j ∈ N \ S.

Xi∈N \(S∪{j})

1
γ

f ({i} | S) y(t)
i

(13)

If the optimal value of the separation problem (13) is less than z(t), then we add an optimal
solution S′ of the separation problem (13) to Q; otherwise, we conclude z(t) is the optimal
value of the problem (10). We repeat adding a new feasible solution S′ obtained from the sep-
aration problem (13) to Q and solving the updated BIP(Q) until z(t) and f (S′) meet. This
procedure is often called the cutting-plane algorithm which is used for the mixed integer pro-
grams (Marchand et al., 2002). However, the computational cost to solve a separation problem
(13) is very expensive, almost the same as solving the SFM problem. To overcome this, we
propose an improved constraint generation algorithm to quickly generate a promising set of
constraints.

After solving BIP(Q), we obtain at least one feasible solution S♮ ∈ Q attaining the optimal

value z(t) of BIP(Q), i.e.,

z(t) = f (S♮) + f ({j}) | S♮) y(t)

j +

1
γ

f ({i} | S♮) y(t)
i

.

(14)

Xi∈N \(S♮∪{j})

Let S(t) be the optimal solution of BIP(Q) corresponding to y(t), where we assume S(t) 6∈ Q.

We then consider adding an element j ∈ S(t) \ S♮ to S♮. In the case with satisfying j =

argmaxi∈N \Sf ({i} | S), we obtain the following inequality by approximately submodularity:

z(t) = f (S♮) + f ({j}) | S♮) y(t)

j +

= f (S♮ ∪ {j}) +

≥ f (S♮ ∪ {j}) +

Xi∈N \(S♮∪{j})

Xi∈N \(S♮∪{j})

1
γ

f ({i} | S♮) y(t)

i

f ({i} | S♮) y(t)

Xi∈N \(S♮∪{j})
1
γ
f ({i} | S♮ ∪ {j}) y(t)

i

i

(15)

,

where y(t)
j = 1 due to j ∈ S(t). In the other case when j 6= argmaxi∈N \S f ({i} | S), we obtain
Ideally, we should have 1/γ in the sum of right hand side of the
a similar inequality above.
inequality. By the inequality (15) with γ ≃ 1, we observe that it is preferable to add the element
j ∈ S(t) \ S♮ to S♮ for improving the upper bound z(t). Here, we note that it is necessary to
remove another element i ∈ S♮ if |S♮| = k holds.

Based on this observation, we develop a heuristic algorithm to generate a set of new feasible
solutions S′ ∈ F for improving the upper bound z(t). Given a set of feasible solutions Q ⊆ F ,

7

let qi be the number of feasible solutions S ∈ Q including an element i ∈ N . We deﬁne the
occurrence rate pi of each element i with respect to Q as

pi =

qi
j∈N qj

.

(16)

P
For each element i ∈ S♮ ∪ S(t), we set a random value ri satisfying 0 ≤ ri ≤ pi. If there are
multiple feasible solutions S♮ ∈ Q satisfying the equation (14), then we select one of them at
random. We take the k largest elements i ∈ S♮ ∪ S(t) with respect to the value ri to generate a
feasible solution S′ ∈ F .

Algorithm SUB-ICG(Q, S(t), λ)

Input: A set of feasible solutions Q ⊆ F . A feasible solution S(t) 6∈ Q. The number of feasible

solutions to be generated λ.

Output: A set of feasible solutions Q′ ⊆ F .

Step1:

Set Q′ ← ∅ and h ← 1.

Step2: Select a feasible solution S♮ ∈ Q satisfying the equation (14) at random. Set a random

value ri (0 ≤ ri ≤ pi) for i ∈ S♮ ∪ S(t).

Step3:

If |S♮| = k holds, then take the k largest elements i ∈ S♮ ∪ S(t) with respect to ri to
generate a feasible solution S′ ∈ F . Otherwise, take the largest element i ∈ S(t) \ S♮ with
respect to ri to generate a feasible solution S′ = S♮ ∪ {i} ∈ F .

Step4:

If S′ 6∈ Q′ holds, then set Q′ ← Q′ ∪ {S′} and h ← h + 1.

Step5:

If h = λ holds, then output Q′ and exit. Otherwise, return to Step2.

We summarize the improved constraint generation algorithm as follows, in which we deﬁne
Q as the set of feasible solutions S(0), S(1), . . . , S(t−1) obtained by solving reduced BIP problems
and Q+ as the set of feasible solutions generated by SUB-ICG(Q, S(t), λ).

Algorithm ICG(S(0), λ)

Input: The initial feasible solution S(0). The number of feasible solutions to be generated at

each iteration λ.

Output: The incumbent solution S∗.

Step1:

Set Q ← {S(0)}, Q+ ← {S(0)

[0] , . . . , S(0)

[k] }, S∗ ← S(0) and t ← 1.

Step2:

Solve BIP(Q+). Let S(t) and z(t) be an optimal solution and the optimal value of

BIP(Q+), respectively.

Step3:

If f (S(t)) > f (S∗) holds, then set S∗ ← S(t).

Step4:

If z(t) = f (S∗) holds, then output the incumbent solution S∗ and exit.

Step5:

Set Q ← Q ∪ {S(t)}, Q+ ← Q+ ∪ {S(t)} ∪ SUB-ICG(Q, S(t), λ) and t ← t + 1.

Step6: For each feasible solution S ∈ SUB-ICG(Q, S(t), λ), if f (S) > f (S∗) holds, then set

S∗ ← S. Return to Step2.

We note that the improved constraint generation algorithm often attains good lower bounds

as well as the upper bounds because SUB-ICG gives good feasible solutions at each iteration.

8

4.3 Branch-and-Cut Algorithm

We propose a branch-and-cut algorithm incorporating the improved constraint generation al-
gorithm. We ﬁrst deﬁne the search tree of the branch-and-cut algorithm. Each node (S0, S1)
of the search tree consists of a pair of sets S0 and S1, where elements i ∈ S0 (resp., i ∈ S1)
correspond to variables ﬁxed to yi = 0 (resp., yi = 1) of the problem (10). The root node is set
to (S0, S1) ← (∅, ∅). Each node (S0, S1) has two children (S0 ∪ {i∗}, S1) and (S0, S1 ∪ {i∗}),
where i∗ = argmaxi∈N \(S0∪S1)f (S1 ∪ {i}).

The branch-and-cut algorithm employs a stack list L to manage nodes of the search tree.
The value of a node (S0, S1) is deﬁned as the optimal value z(S0,S1) of the following reduced
BIP problem BIP(Q+, S0, S1):

maximize

z

subject to z ≤ f (S) + f ({j} | S)yj +

S ∈ Q+, j ∈ N \ S,

yi ≤ k − |S1|,

1
γ

f ({i} | S)yi,

Xi∈N \(S∪{j})

(17)

Xi∈N \(S0∪S1)
yi ∈ {0, 1},
yi = 0,
yi = 1,

i ∈ N \ (S0 ∪ S1),
i ∈ S0,
i ∈ S1,

where j = argmaxi∈N \Sf ({i} | S), and Q+ is the set of feasible solution generated by the
improved constraint generation algorithm so far. We note that z(S0,S1) gives an upper bound of
the optimal value of the problem (10) at the node (S0, S1); i.e., under the condition that yi = 0
(i ∈ S0) and yi = 1 (i ∈ S1).

We start with a pair of sets Q = {S} and Q+ = {S[0], . . . , S[k]}, where S is the initial
feasible solutions obtained by the greedy algorithm (Minoux, 1978; Nemhauser et al., 1978). To
obtain good upper and lower bounds quickly, we ﬁrst apply the ﬁrst k iterations of the improved
constraint generation algorithm. We then repeat to extract a node (S0, S1) from the top of the
stack list L and insert its children into the top of the stack list L at each iteration. Thus, we
employ a depth-ﬁrst-search for the tree search of the branch-and-cut algorithm.

Let (S0, S1) be a node extracted from the stack list L, and S∗ be the incumbent solution of
the problem (10) (i.e., the best feasible solution obtained so far). We ﬁrst solve BIP(Q+, S0, S1)
to obtain an optimal solution S(S0,S1) and the optimal value z(S0,S1). We then generate a
set of feasible solutions by SUB-ICG(Q, S(S0,S1), λ). For each feasible solution S′ ∈ {S(S0,S1)} ∪
SUB-ICG(Q, S(S0,S1), λ), if f (S′) > f (S∗) holds, then we replace the incumbent solution S∗ with
S′. If z(S0,S1) > f (S∗) holds, then we insert the two children (S0 ∪ {i∗}, S1) and (S0, S1 ∪ {i∗})
into the top of the stack list L in this order.

To decrease the number of reduced BIP problems to be solved in the branch-and-cut algo-
rithm, we keep the optimal value z(S0,S1) of BIP(Q+, S0, S1) as an upper bound ¯z(S0∪{i∗},S1)
(resp., ¯z(S0,S1∪{i∗})) of the child (S0 ∪ {i∗}, S1) (resp., (S0, S1 ∪ {i∗})) when inserted to the stack
list L. If ¯z(S0,S1) ≤ f (S∗) holds when we extract a node (S0, S1) from the stack list L, then we
can prune the node (S0, S1) without solving BIP(Q+, S0, S1). We set the upper bound ¯z(∅,∅) of
the root node (∅, ∅) to ∞. We repeat these procedures until the stack list L becomes empty.

Algorithm BC-ICG(S, λ)

Input: The initial feasible solution S. The number of feasible solutions to be generated at

each node λ.

9

Output: The incumbent solution S∗.

Step1:

Set L ← {(∅, ∅)}, ¯z(∅,∅) ← ∞, Q ← {S}, Q+ ← {S[0], . . . , S[k]} and S∗ ← S.
Step2: Apply the ﬁrst k iterations of ICG(S, λ) to update the sets Q and Q+ and the incumbent

solution S∗.

Step3:

If L = ∅ holds, then output the incumbent solution S∗ and exit.

Step4: Extract a node (S0, S1) from the top of the stack list L. If ¯z(S0,S1) ≤ f (S∗) holds,

then return to Step3.

Step5: Solve BIP(Q+, S0, S1). Let S(S0,S1) and z(S0,S1) be an optimal solution and the optimal

value of BIP(Q+, S0, S1), respectively.

Step6:

Set Q ← Q ∪ {S(S0,S1)}, Q+ ← Q+ ∪ {S(S0,S1)} ∪ SUB-ICG(Q, S(S0,S1), λ).

Step7: For each feasible solution S′ ∈ {S(S0,S1)} ∪ SUB-ICG(Q, S(S0,S1), λ), if f (S′) > f (S∗)

holds, then set S∗ ← S.

Step8:

If z(S0,S1) ≤ f (S∗), then return to Step3.

Step9:

If |S0 ∪ S1| ≤ n − 1 and |S1| ≤ k − 1 hold, then set L ← L ∪ {(S0 ∪ {i∗}, S1), (S0, S1 ∪
{i∗})}, ¯z(S0∪{i∗},S1) ← z(S0,S1) and ¯z(S0,S1∪{i∗}) ← z(S0,S1), where i∗ = argmaxi∈N \(S0∪S1)
f (S1 ∪ {i}). Return to Step3.

We note that the branch-and-cut algorithm is similar to that for the traveling salesman prob-
lem based on a BIP formulation with an exponential number of subtour elimination constraints
(Crowder and Padberg, 1980; Gr¨otschel and Holland, 1991).

5 EXAMPLE

We consider the combinatorial auction (CA) problem that asks a bidder to select a package of
items to maximize its utility, which is formulated as the ASFM problem. A greedy algorithm
obtains a feasible solution quickly, however it often fails to obtain important items for the
problem. To do so, we need an optimal solution as well as other good solutions whose objective
value are close to the optimal value. For this purpose, we modify BC-ICG to hold all incumbent
solutions obtained so far as well as the ﬁnal incumbent solution.
Combinatorial auction (CA) We are given a set of n items N = {1, . . . , n}. We select a set
of items S ⊆ N to make a package of items. We deﬁne wi as the individual utility of an item
i ∈ N and rij as the mutual utility for a pair of items i, j ∈ N . The utility of a package of items
is deﬁned as

f (S) =

wi +

rij.

(18)

Xi∈S

Xi,j∈S

We have tested BC-ICG for an instance arising from a supermarket transaction data containing
170 items and 9835 transactions.1 We set the size of the package k = 2, 3. We also set the indi-
vidual utility randomly wi ∈ [1, 2], while setting the mutual utility rij ∈ [−0.09, 0.01] according
to the number of times that both items i, j are selected in the same transaction. We obtain a
lower bound γ of the submodular ratio γ by the following formula:

γ = min

i∈N (

wi +

wi +

j∈L[q1]

P

j∈U[q2]

rij
rij )

.

(19)

P
1http://www.sci.csueastbay.edu/~esuess/classes/Statistics_6620/Presentations/ml13/groceries.csv

10

Table 1: Frequency of items in a series of solutions obtained by BC-ICG.
“organic products”
1
2

“frozen vegetables”
2
3

“yogurt”
3
4

“softner”
1
0

“sugar”
2
2

“tea”
1
1

k = 2
k = 3

where the sets U and L are the non-increasing positive and non-decreasing negative ordered set
with respect to rij for an item i ∈ N , respectively, and q1 = min{k −1, |L|}, q2 = min{k −1, |U |}.
We note that L[q1] (resp., U[q2]) represents the ﬁrst q1 (resp., q2) elements of a sorted set L (resp.,
U ). For the instance, we obtained γ = 0.906, 0.736 with k = 2, 3, respectively.

Table 1 shows the frequency of items in the series of solutions obtained by BC-ICG. The
optimal solutions obtained by BC-ICG are [“yogurt”, “frozen vegetables”], [“yogurt”, “sugar”,
“organic products”] for k = 2, 3, respectively. On the other hand, the feasible solutions obtained
the greedy algorithm are [“tea”, “yogurt”], [“tea”, “yogurt”, “frozen vegetables”] for k = 2, 3,
respectively. We note that “tea” is not selected in the optimal solutions while it has the largest
value of the 170 items. That is, the greedy algorithm sometimes fails to attain important items
constitute the optimal solution.

6 COMPUTATIONAL RESULTS

We tested two existing algorithms: (i) the A∗ search algorithm with the heuristic function hmod
(A∗-MOD), (ii) the modiﬁed constraint generation algorithm (MCG) and two proposed algo-
rithms: (iii) the improved constraint generation algorithm (ICG), (iv) the branch-and-cut al-
gorithm (BC-ICG). All algorithms were tested on a personal computer with a 4.0 GHz Intel
Core i7 processor and 32 GB memory. For MCG, ICG, and BC-ICG, we use an mixed inte-
ger programming (MIP) solver called CPLEX 12.8 (2019) for solving reduced BIP problems,
and the number of feasible solutions to be generated at each iteration λ is set to 10k based on
computational results of preliminary experiments.

We report computational results for three types of well-known benchmark instances called fa-

cility location (LOC), weighted coverage (COV), and bipartite inﬂuence (INF) according to Kawahara et al.
(2009) and Sakaue and Ishihata (2018). We note that those instances were originally generated
for the SFM problem. For generating instances for the ASFM problem, we replace the original
utility function f (S) with f (S) + rS for a number of feasible solutions, where rS is a reward
value for a feasible solution S ⊆ F . We randomly selected 1000k feasible solutions S ⊆ F and
modiﬁed their utility functions while satisfying non-decreasing and a given lower bound γ = 0.8
of the submodular ratio γ.
Facility location (LOC) We are given a set of n locations N = {1, . . . , n} and a set of m
clients M = {1, . . . , m}. We consider to select a set of k locations to build facilities. We deﬁne
gij ≥ 0 as the beneﬁt of a client i ∈ M attaining from a facility of location j ∈ N . We select
a set of locations S ⊆ N to built the facilities. Each client i ∈ M attains the beneﬁt from the
most beneﬁcial facility. The total beneﬁt for the clients is deﬁned as

f (S) =

gij.

(20)

max
j∈S

Xi∈M
Weighted coverage (COV) We are given a set of m items M = {1, . . . , m} and a set of n
sensors N = {1, . . . , n}. Let Mj ⊆ M be the subset of items covered by a sensor j ∈ N , and
wi ≥ 0 be a weight of an item i ∈ M . We select a set of sensors S ⊆ N to cover items. The
total weighted coverage for the items is deﬁned as

f (S) =

wi max
j∈S

aij,

i∈M
X

11

(21)

where aij = 1 if i ∈ Mj holds and aij = 0 otherwise.
Bipartite inﬂuence (INF) We are given a set of m targets M = {1, . . . , m} and a set of items
N = {1, . . . , n}. Given a bipartite graph G = (M, N ; A), where A ⊆ M × N is a set of directed
edges, we consider an inﬂuence maximization problem on G. Let pj ∈ [0, 1] be the activation
probability of an item j ∈ N . The probability that a target i ∈ M gets activated by a set of
items S ⊆ N is 1 −
j∈S(1 − qij), where qij = pj if (i, j) ∈ A holds and qij = 0 otherwise. We
select a set of items S ⊆ N to activate targets. The expected number of targets activated by a
set of items S ⊆ N is deﬁned as

Q

f (S) =

1 −

Yj∈S

Xi∈M





(1 − qij)



.



(22)

We tested all algorithms for 18 classes of randomly generated instances that are characterized
by several parameters. We set m = n+1 and k = 5, 8 for LOC, COV and INF instances according
to (Kawahara et al., 2009). We set n = 20, 30, 40 for LOC instances and n = 20, 40, 60 for COV
and INF instances. For LOC instances, gij is a random value taken from interval [0, 1]. For
COV instances, a sensor j ∈ N randomly covers an item i ∈ M with probability 0.15, and wi
is a random value taken from interval [0, 1]. For INF instances, pj is a random value taken
from interval [0, 1], and the bipartite graph G is a random graph in which an edge (i, j) ∈ A
is generated randomly with probability 0.1. We set these parameters to diﬀerent values from
those in (Sakaue and Ishihata, 2018) considering the diﬀerence between the cardinality and
knapsack constraints. For each class of instances, ﬁve instances were generated and tested. For
all instances, we set the time limit to 7200 seconds.

Tables 2 and 3 show the average computation time (in seconds) and the average number of
processed nodes of the algorithms for each class of instances, respectively. If an algorithm could
not solve an instance optimally within the time limit, then we set the computation time to 7200
seconds. The best computation time among the compared algorithms is highlighted in bold.
The numbers in parentheses show the number of instances optimally solved within the time
limit. According to Table 2, ICG performed better than MCG in most of instances. Figure 1
shows trends of the upper and lower bounds obtained of MCG and ICG with respect to the
computation time for an LOC instance (n = 30, k = 8). We can see that ICG attained better
upper and lower bounds than those of MCG by generating good feasible solutions and adding
them as constraints at each iteration. The number of iterations that MCG and ICG solved a
reduced BIP problem were 275 and 35, respectively. We succeeded in improving the eﬃciency
of MCG by reducing the number of the iterations. According to Table 3, BC-ICG processed
much smaller number of nodes than A∗-MOD due to ICG attaining good upper bounds. We
note that A∗-MOD performed well for INF instances, because the utility function f (S) became
close to linear and A∗-MOD gave tight upper bound ¯f (S) = f (S) + h(S) for the instances.

Figure 2 shows performance proﬁles (Dolan and More, 2002) of the algorithms for a param-
eter 1 ≤ β ≤ 10. For given sets of algorithms A and instances I, the performance proﬁle is
deﬁned in terms of computation time T (A, I) of an algorithm A ∈ A to solve an instance I ∈ I
optimally. For a pair of algorithm A ∈ A and instance I ∈ I, the performance ratio R(A, I)
(i.e., the ratio of computation time over the best) is deﬁned as

R(A, I) =

T (A, I)

T (A′, I)

min
A′∈A

,

(23)

where we set R(A, I) = ∞ if none of the algorithms solved the instance I optimally. We note
that R(A, I) ≥ 1 holds by deﬁnition. The performance proﬁle of an algorithm A ∈ A illustrates

12

Table 2: Computation time (in seconds) of the proposed and the existing algorithms.

A∗-MOD

Type

BC-ICG

n k
20
LOC 30
40
20
LOC 30
40
20
COV 40
60
20
COV 40
60
20
INF 40
60
20
INF 40
60

0.97 (5)
8.69 (5)
51.00 (5)
13.22 (5)
409.50 (5)

MCG
1.39 (5)
27.98 (5)
2369.62 (5)
30.01 (5)
99.74 (5)

ICG
γ
0.52 (5)
5 0.8
0.70 (5)
11.57 (5)
5 0.8
10.98 (5)
973.04 (5)
5 0.8
183.09 (5)
0.32 (5)
0.35 (5)
8 0.8
17.29 (5)
8 0.8
21.77 (5)
667.62 (5)
8 0.8 > 5703.85 (4) > 5367.68 (3) > 3433.23 (3)
0.25 (5)
0.36 (5)
0.40 (5)
5 0.8
0.29 (5)
9.98 (5)
52.49 (5)
16.36 (5)
5 0.8
14.15 (5)
145.73 (5)
262.39 (5)
1739.64 (5)
163.36 (5)
5 0.8
0.07 (5)
0.10 (5)
8 0.8
8.71 (5)
0.09 (5)
2.31 (5)
8 0.8 > 3468.47 (4) > 1441.96 (4)
3.18 (5)
32.49 (5)
35.81 (5)
108.86 (5)
8 0.8 > 7200.00 (0)
0.36 (5)
0.61 (5)
5 0.8
2.46 (5)
1.07 (5)
8.61 (5) > 1992.35 (4)
15.98 (5)
5 0.8
22.14 (5)
16.68 (5)
61.20 (5) > 4467.06 (2)
5 0.8
25.17 (5)
1.75 (5)
8 0.8
15.98 (5)
3.46 (5)
438.83 (5) > 3775.80 (3) > 1670.35 (4)
912.62 (5)
8 0.8
8 0.8 > 4980.66 (4) > 7200.00 (0) > 7200.00 (0) > 7200.00 (0)

3.32 (5)

Type

Table 3: Number of processed nodes by the proposed and the existing algorithms.
A∗-MOD
BC-ICG
γ
8.60 × 100 (5)
5.62 × 103 (5)
5 0.8
8.30 × 101 (5)
2.74 × 104 (5)
5 0.8
3.77 × 102 (5)
1.03 × 105 (5)
5 0.8
1.00 × 100 (5)
4.68 × 104 (5)
8 0.8
4.78 × 101 (5)
6.55 × 105 (5)
8 0.8
3.09 × 102 (5)
8 0.8 > 3.71 × 106 (4)
1.00 × 100 (5)
2.03 × 103 (5)
5 0.8
6.14 × 101 (5)
2.63 × 104 (5)
5 0.8
1.96 × 102 (5)
1.07 × 105 (5)
5 0.8
1.00 × 100 (5)
3.54 × 104 (5)
8 0.8
1.00 × 100 (5)
8 0.8 > 2.71 × 106 (4)
9.80 × 100 (5)
8 0.8 > 3.00 × 106 (0)
2.62 × 101 (5)
1.16 × 103 (5)
5 0.8
1.86 × 102 (5)
9.04 × 103 (5)
5 0.8
1.71 × 102 (5)
3.02 × 104 (5)
5 0.8
1.70 × 101 (5)
9.51 × 103 (5)
8 0.8
5.20 × 102 (5)
3.90 × 105 (5)
8 0.8
8 0.8 > 2.01 × 106 (4) > 1.94 × 103 (0)

n k
20
LOC 30
40
20
LOC 30
40
20
COV 40
60
20
COV 40
60
20
INF 40
60
20
INF 40
60

the function ρA(β) that represents the number of instances I ∈ I satisfying R(A, I) ≤ β. We
observed that BC-ICG solved 78 instances optimally out of all 90 instances while MCG solved
only 41 instances with β = 3. These computational results show that BC-ICG improved the
eﬃciency of the conventional MCG.

7 CONCLUSIONS

In this paper, we formulate the non-decreasing ASFM problem into a BIP formulation with an
exponential number of constraints. For the ASFM problem, we propose an improved constraint
generation algorithm that starts from a small subset of constraints and repeats solving a reduced
BIP problem while adding a promising set of constraints at each iteration. We then incorporate
it into a branch-and-cut algorithm to attain good upper bounds. According to computational
results for three types of well-known benchmark instances, our algorithm performs better than
the conventional modiﬁed constraint generation algorithm.

13

Upper bound of ICG
Lower bound of ICG
Upper bound of MCG
Lower bound of MCG
Optimal value

30.4

30.2

30.0

29.8

29.6

29.4

e
u
a
v

l

e
v
i
t
c
e
j
b
O

0

30

60

90

120

150

Computation time (s)

Figure 1: Trends of the upper and lower bounds obtained by MCG and ICG with respect to the
elapsed computation time.

References

E. Balkanski, B. Mirzasoleiman, A. Krause, and Y. Singer. Learning sparse combinatorial repre-
sentations via two-stage submodular maximization. In Proceedings of the 33rd International
Conference on Machine Learning, pages 2207–2216, 2016.

W. Chen, Y. Chen, and K. Weinberger. Filtered search for submodular maximization with
controllable approximation bounds. In Proceedings of the 18th International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS’15), volume 38, pages 156–164, 2015.

V. Conitzer, T. Sandholm, and P. Santi. In Proceedings of American Association for Artiﬁcial

Intelligence 2005, pages 248–254, 2005.

H. Crowder and W. M. Padberg. Solcing large-scale symmetric traveling salesman problems to

optimality. Management Science, 26:495–509, 1980.

A. Das and D. Kempe. Submodular meets spectral: Greedy algorithms for subset selection,
sparse approximation and dictionary selection. In Proceedings of the 28th International Con-
ference on Machine Learning (ICML’11), pages 1057–1064, 2011.

A. Das and D. Kempe. Approximate submodularity and its applications: Subset selection,
sparse approximation and dictionary selection. Journal of Machine Learning Research, 19:
1–34, 2018.

E. D. Dolan and J.J. More. Benchmarking optimization software with performance proﬁles.

Mathematical Programming, 91:201–203, 2002.

D. Golovin and A. Krause. Adaptive submodularity: Theory and applications in active learning
and stochastic optimization. Journal of Artiﬁcial Intelligence Research, 42:427–486, 2011.

14

90

80

70

60

50

40

30

20

10

s
e
c
n
a
t
s
n

i

d
e
v
l
o
s

y
l
l

a
m

i
t
p
o

f
o

r
e
b
m
u
N

0

1

A*-MOD
MCG
ICG
BC-ICG

3

2
9
Ratio of computation time over the best

8

4

7

6

5

10

Figure 2: Performance proﬁle for the algorithms.

M. Gr¨otschel and O. Holland. Solution of large-scale symmetric travelling salesman problems.

Mathematical Programming, 51:141–202, 1991.

T. Horel and Y. Singer. Maximization of approximately submodular functions. In Proceedings

of the 30th Conference on Neural Information Proceeding Systems (NIPS2016), 2016.

K Johnson, A. R. Stine, and P. D. Foster. Submodularity in statistics: Comparing the success

of model selection methods. arXiv:1510.06301v2, 2016.

Y. Kawahara, K. Nagano, K. Tsuda, and J. A. Bilmes. Submodularity cuts and applications.

In Advances in Neural Information Processing Systems 26, pages 916–924, 2009.

D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of inﬂuence through a social
network. In Proceedings of the 9th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD’03), pages 137–146, 2003.

J. Kratica, D. Tosic, V. Filipovic, I. Ljubic, and P. Tolla. Solving the simple plant location

problem by genetic algorithm. RAIRO-Operations Research, 35(1):127–142, 2001.

A. Krause and D. Golovin. Submodular function maximization.
Approaches to Hard Problems. Cambridge University Press, 2014.

In Tractability: Practical

H. Lin and J. Bilmes. A class of submodular functions for document summarization. In Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages
510–520, 2011.

Y. Lin, W. Chen, and S. C. J Lui. Boosting information spread: An algorithmic approach.

IEEE Transactions on Computational Social Systems, 5:344–357, 2018.

L. Lov´asz. Submodular functions and convexity. In A. Bachem, M. Grotschel, and B. Korte,
editors, Mathematical Programming — The State of the Art, pages 235–257. Springer, Berlin,
Heidelberg, 1983.

15

H. Marchand, A. Martin, R. Weismantel, and L. Wolsey. Cutting planes in integer and mixed

integer programming. Discrete Applied Mathematics, 123:397–446, 2002.

M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. Lecture

Notes in Control and Information Sciences, 7:234–243, 1978.

G. L. Nemhauser and L. Wolsey. Maximizing submodular set functions: Formulations and

analysis of algorithms. Studies on Graphs and Discrete Programming, 11:279–301, 1981.

G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing

submodular set functions I. Mathematical Programming, 14(1):265–294, 1978.

S. Sakaue and M. Ishihata. Accelerated best-ﬁrst search with upper-bound computation for
submodular function maximization. In Proceedings of the 32nd AAAI Conference on Artiﬁcial
Intelligence (AAAI’18), pages 1413–1421, 2018.

IBM ILOG CPLEX Optimization studio.

https://www.ibm.com/products/ilog-cplex-

optimization-studio, 2019.

L. Yu and H. Liu. Eﬃcient feature selection via analysis of relevance and redundancy. Journal

of machine learning research, 5:1205–1224, 2004.

Appendix

The following proof is for Proposition 1. We prove with the following steps, (i) ⇔ (ii) ⇔ (iii),
(iii) ⇒ (iv) ⇒ (v) ⇒ (iii).

Proof:

(i) ⇒ (ii). Let S ⊆ T ⊆ N, i /∈ T, A = S ∪ {i}, B = T in (i). We obtain

f ({i} | S) ≥ γf ({i} | T ).

(24)

(ii) ⇒ (i). Let {j1, . . . , jl} = A \ B. We put that into (ii), and we obtain the following

inequality for i = 1, . . . , l.

f ({ji} | A ∩ B ∪ {j1, . . . , ji−1}) ≥ γf ({ji} | B ∪ {j1, . . . , ji−1}).

(25)

Sum the following l equations,

f ({j1} | A ∩ B) ≥ γf ({j1} | B)
f ({j2} | A ∩ B ∪ {j1}) ≥ γf ({j2} | B ∪ {j1})
...
f ({jl} | A ∩ B ∪ {j1, . . . , jl−1}) ≥ γ{f ({jl} | B ∪ {j1, . . . , jl−1})}.

We then obtain

f (A) − f (A ∩ B) ≥ γ(f (A ∪ B) − f (B)).

(26)

(27)

(ii) ⇒ (iii). It is clear when we let T = S ∪ {e}. Since (ii) considers larger sets than (iii),

γ = min

S⊆T ⊆N

f ({i} | S)
f ({i} | T )

≤ min
S⊆N

f ({i} | S)
f ({i} | S ∪ {e})

= ¯γ.

16

(iii) ⇒ (ii). If ∀S ⊆ T ⊆ N, i /∈ T, T \ S = {k1, . . . , kq}, we obtain the following inequalities

by (iii). We let Sj = S ∪ {k1, . . . , kj}.

f ({i} | S) ≥ ¯γf ({i} | S ∪ {k1})

f ({i} | S ∪ {k1}) ≥ ¯γf ({i} | S ∪ {k1, k2})

...

f ({i} | S ∪ {k1, . . . , kq−1}) ≥ ¯γf ({i} | T ).

(28)

By adding these inequalities, we obtain

f ({i} | S) ≥ ¯γf ({i} | T ) + (¯γ − 1)f ({i} | S ∪ {k1}) + · · · + (¯γ − 1)f ({i} | S ∪ {k1, . . . , kq−1}). (29)

By multiplying these inequalities from the bottom, we can obtain the following relation for
f ({i} | S ∪ {k1, . . . , kt}), for t = 1, . . . , q − 1,

f ({i} | S ∪ {k1, . . . , kt}) ≥ ¯γq−tf ({i} | T ).

By using (30), we rewrite the inequality (29) as follows.

ρi(S) ≥ ¯γf ({i} | T ) + (¯γ − 1){¯γq−1f ({i} | T ) + ¯γq−2f ({i} | T ) + · · · + ¯γf ({i} | T )}

= ¯γf ({i} | T ) + ¯γf ({i} | T )(¯γ − 1)
= ¯γqf ({i} | T ).

(cid:16)

1−¯γq−1
1−¯γ

(cid:17)

(30)

(31)

We note that ¯γq represents the lower bound of γ.
(iii) ⇒ (iv). For arbitrary S and T with T \ S = {j1, . . . , jl} and S \ T = {k1, . . . , kq}, we

obtain

f (S ∪ T ) − f (S)

l

=

=

[f (S ∪ {j1, . . . , jt}) − f (S ∪ {j1, . . . , jt−1})]

t=1
X
l

f ({jt} | S ∪ {j1, . . . , jt−1})

t=1
X

= f ({j1} | S) + f ({j2} | S ∪ {j1}) + f ({j3} | S ∪ {j1, j2}) + · · · + f ({jl} | S ∪ {j1, . . . , jl−1})
≤ f ({j1} | S) + 1
= f ({j1} | S) + 1

¯γ f ({j2} | S) + 1
¯γ f ({j2} | S) +

γ f ({j3} | S) + · · · + 1

γ f ({jl} | S)

f ({j} | S).

1
γ

Similarly, we obtain the following inequality.

Xj∈T \(S∪{j1,j2})

=

=

≥

f (S ∪ T ) − f (T )

q

[f (T ∪ {k1, . . . , kt}) − f (T ∪ {k1, . . . , kt−1})]

t=1
X
q

t=1
X
q

t=1
X

f ({kt} | T ∪ {k1, . . . , kt} \ {kt})

γf ({kt} | T ∪ S \ {kt}) =

γf ({i} | T ∪ S \ {i}).

Xi∈S\T

We obtain (iv) by adding these two inequalities.

(iv) ⇒ (v). If ∀S ⊆ T ⊆ N , then S \ T = ∅. We obtain (v).

17

(32)

(33)

(v) ⇒ (iii). Let ∀S ⊆ N, T = S ∪ {j1, j2}, j1 ∈ N \ (S ∪ {j2}) in (v), then we obtain

f (S ∪ {j1, j2}) ≤ f (S) + f ({j1} | S) +

1
¯γ

f ({j2} | S).

f ({j2} | S ∪ {j1}) = f (S ∪ {j1, j2}) − f (S ∪ {j1})

= f (S ∪ {j1, j2}) − f ({j1} | S) − f (S)
≤ 1

¯γ f ({j2} | S).

(34)

(35)

(cid:3)

The following proof is for Proposition 2.

(i∗) ⇔ (ii∗). We skip the proof of (i∗) ⇒ (ii∗) since it is the similar manner as (i) ⇒

Proof:
(ii).

(ii∗) ⇒ (i∗). γf ({i} | T ) ≥ 0. Since γ > 0, we obtain f ({i} | T ) ≥ 0 and f (A) ≤ f (B). The

rest is the same manner as (ii) ⇒ (i).

(ii∗) ⇒ (iv∗). It is clear that (ii∗) ⇒ (ii) ⇒ (iv). When f ({i} | T ) ≥ 0, the last term of (iv)

is nonpositive, and that brings us (iv∗).

(iv∗) ⇒ (ii∗). Suppose that S = T ∪{i} in (iv∗), we obtain f (T ) ≤ f (T ∪{i}) or f ({i} | T ) ≥ 0
(cid:3)

with γ > 0.

The following proof for Proposition 3.

Proof:

(⇐). Suppose Φ ≤ f (U ), then for all S ⊆ N , we obtain the following inequality.

f (S) + f ({j1} | S)yU

j1 + 1

¯γ f ({j2} | S)yU

j2 +

= f (S) + f ({j1} | S)yU

j1 + 1

¯γ f ({j2} | S)yU

j2 +

Xj∈N −(S∪{j1,j2})

1
γ

f ({j} | S)yU
j

1
γ

f ({j} | S)yU
j

(36)

≥ f (U ) ≥ Φ,

Xj∈U −(S∪{j1,j2})

where the ﬁrst inequality comes from Proposition 2 (iv∗).

(⇒). If (Φ, yU ) ∈ X, we obtain the following inequality,

1
γ

ρj(U )yU
j

Xj∈N \(U ∪{j1,j2})

(37)

(cid:3)

Φ ≤ f (U ) + f ({j1} | U )yU

j1 + 1

¯γ ρj2(U )yU

j2 +

= f (U ).

18

