Summarize and Generate to Back-translate:
Unsupervised Translation of Programming Languages

Wasi Uddin Ahmad†, Saikat Chakraborty‡, Baishakhi Ray‡, Kai-Wei Chang†
†University of California, Los Angeles, ‡Columbia University
†{wasiahmad, kwchang}@cs.ucla.edu, ‡{saikatc, rayb}@cs.columbia.edu

Abstract

Back-translation is widely known for its effec-
tiveness for neural machine translation when
In this
little to no parallel data is available.
is cou-
approach, a source-to-target model
pled with a target-to-source model trained in
parallel. The target-to-source model gener-
ates noisy sources, while the source-to-target
model is trained to reconstruct the targets and
vice versa. Recent developments of multi-
lingual pre-trained sequence-to-sequence mod-
els for programming languages have been
very effective for a broad spectrum of down-
stream software engineering tasks. Hence, it
is compelling to train them to build program-
ming language translation systems via back-
translation. However, these models cannot be
further trained via back-translation since they
learn to output sequences in the same language
as the inputs during pre-training. As an alterna-
tive, we propose performing back-translation
In
via code summarization and generation.
code summarization, a model learns to gen-
erate natural language (NL) summaries given
code snippets. In code generation, the model
learns to do the opposite. Therefore, target-to-
source generation in back-translation can be
viewed as target-to-NL-to-source generation.
We show that our proposed approach performs
competitively with state-of-the-art methods.

1

Introduction

2
2
0
2

y
a
M
3
2

]
L
C
.
s
c
[

1
v
6
1
1
1
1
.
5
0
2
2
:
v
i
X
r
a

Input in Java

1 p u b l i c s t a t i c boolean g e n e r a t o r (

l ) {

P s i B u i l d e r b ,
i n t
boolean r = f a l s e ;
i f ( ! r e c u r s i o n _ g u a r d ( b ,
r = g e ne r a t o r_ 0 ( b ,
i f ( ! r )

l ) )
l + 1 ) ;

r e t u r n g e n e r at o r_ 1 ( b ,

l + 1 ) ;

r e t u r n r ;

(1) Java to Python Generation

1 p u b l i c s t a t i c boolean g e n e r a t o r (

l ) {

i n t
P s i B u i l d e r b ,
boolean r = f a l s e ;
i f ( ! r e c u r s i o n _ g u a r d ( b ,
l ) )
i f ( ! g e ne r a t o r_ 0 ( b ,
r = g e ne r a t o r_ 0 ( b ,
l + 1 ) ;
i f ( ! r )

r e t u r n g e n e r at o r_ 1 ( b ,

r e t u r n r ;

l ) )
r e t u r n r ;

l + 1 ) ;

2

3

4

5

6

}

2

3

4

5

6

7

}

(2) Java Code to NL Summary

guard is used to determine if a generator is already
deﬁned .

(3) NL Summary to Python Code

1 d e f
2

i s _ g e n e r a t o r ( s e l f , name ) :
i f name i n s e l f . _ g e n e r a t o r s :

3

4

5

6

r e t u r n True

i f name i n s e l f . _ g e n e r a t o r s [ name ] :

r e t u r n True

r e t u r n False

Figure 1: Although PLBART is asked to generate in
Python given input in Java (1), it generates in Java (due
to its pre-training objective). In contrast, PLBART ﬁne-
tuned on code summarization and generation, generates
“noisy” translations (as in (2, 3)).

Choice of programming language (PL) in software
development depends on the requirement of the
software and the available features of a particular
PL. In modern API-driven software development,
the choice of language often depends on the avail-
ability of libraries and APIs. The advent of newer
and richer programming languages often requires
legacy software to be translated into modernized
PLs. In theory, modern programming languages’
“Turing Completeness” allows rule-based transla-
tion of programs from one PL to another. The rule-
based translation may require an extensive number

of handwritten transformation rules and could end
up producing very unreadable source code. In ad-
dition, such translation could entail translating the
entire library, even if a library implementing simi-
lar functionality is available in the target PL.

Aligning libraries and APIs across different PLs
is a non-trivial task. Recent progress in Neural Ma-
chine Translation (NMT) (Bahdanau et al., 2015;
Vaswani et al., 2017) leveraging pre-trained mod-
els (Feng et al., 2020a; Guo et al., 2021; Roziere
et al., 2021; Ding et al., 2021; Ahmad et al., 2021a;
Wang et al., 2021) could be a possible way to learn

 
 
 
 
 
 
(a) PLBART

(b) PLBART + S&G

Figure 2: T-SNE plot of function embeddings of Java and Python functions. Figure 2a shows the embedding
generated by PLBART model. Figure 2b are the generated embedding when the PLBART is ﬁnetuned to jointly
summarize code to NL and generate code from NL (PLBART + S&G). While PLBART clusters programs from
each individual PLs, same program in different PLs are brought closer to each other by PLBART + S&G.

the alignment between PLs and translate source
code across languages.

A signiﬁcant challenge in supervised learning
for NMT is the need for large-scale parallel corpora.
For instance, if we are planning to train a translator
for Java to Python translation, we need a consid-
erable number of the same program (i.e., exhibiting
the same semantic behavior) in both the languages.
Availability of such parallel datasets is a vital chal-
lenge in programming language translation (Chen
et al., 2018). Back-Translation (BT) (Edunov et al.,
2018; Lachaux et al., 2020) is a clever way to learn
alignments across different languages. While BT
demonstrates success in NMT, those require either
(i.) small (perhaps noisy) parallel datasets or (ii.) a
model with some capacity of cross-lingual genera-
tion - to kickstart the BT-based learning process.

In this research, we investigate the suitability
of multilingual Pre-trained Sequence-to-Sequence
Model (PSM) (e.g., PLBART (Ahmad et al.,
2021a)) for unsupervised programming language
translation via BT. In particular, we assume a use
case scenario, where there is no parallel data avail-
able. Without much of a surprise, we empirically
found that, while these PSMs are good at generat-
ing code in each language, they exhibit very little
to no knowledge about the cross-lingual generation
since such PSMs are typically trained to reconstruct
code sequences from noisy inputs. For example,
when we provide the input code in Figure 1 to
PLBART and ask to generate Python code without
any training, it generates a slight variation of the
input Java code, showing its lack of knowledge
about cross-lingual generation.

To endow such PSMs with knowledge about
cross-lingual generation, we propose the usage of a

third language (i.e., English). Since a large quantity
of monolingual code corpora comes with documen-
tation, which supposedly describes what the source
code is doing, we train a Summarize-and-Generate
(S&G) model that can generate pseudo-parallel
code sequences. Figure 1 shows PLBART’s be-
havior when it is further trained via S&G. First,
given the Java code, it generates a NL summary (ﬁg-
ure 1-2), and subsequently generates Python Code
(ﬁgure 1-3). We empirically show that, even if such
S&G model generates noisy parallel sequences, it
allows us to employ PSMs in the BT-based training
to learn programming language translation.

In summary, we present a Summarize-and-
Generate (S&G) based approach to enable unsuper-
vised program translation training of PLBART via
Back-Translation (BT). Experiment results show
that our proposed approach makes PLBART train-
able via BT and performs competitively with state-
of-the-art program translation models.1

2 Motivation

Recent years saw several Pre-trained Sequence-
to-Sequence models (PSM) (Ahmad et al., 2021a;
Wang et al., 2021). These models are pre-trained
on hundreds of Gigabytes of source code. Thus,
we are motivated to investigate their adoption in
learning program translation via back-translation in
this work. To understand such feasibility, we inves-
tigate the program representations generated by the
PSM. As a case study, we chose PLBART (Ahmad
et al., 2021a) and evaluated its multilingual embed-
dings as suggested in Artetxe and Schwenk (2019).
We ﬁnd the parallel Java function for each of the

1We have made our code publicly available at https:

//github.com/wasiahmad/SumGenToBT.

948 Python functions using the parallel dataset pro-
posed in Lachaux et al. (2020). We ﬁnd the nearest
neighbor using cosine similarity between function
embeddings and calculate the error rate. Unsur-
prisingly, PLBART performs poorly in function
retrieval with an 87.5% error rate.

In comparison, we ﬁne-tune PLBART jointly on
code summarization and generation in Java and
Python. Repeating the experiment of function re-
trieval, we ﬁnd ﬁne-tuned PLBART’s error rate
drops to 23.7%. To visually illustrate the embed-
dings produced by PLBART and its ﬁne-tuned vari-
ant, we provide a T-SNE plot of 8 sample functions’
embedding in Figure 2. We see the functions that
belong to the same language are clustered together
while the same functions in two different languages
are far apart from each other (see Figure 2a).

In contrast, the ﬁne-tuned PLBART breaks up
the intra-language clusters and brings functions
in different languages close to each other in the
embedding space (see Figure 2b). These results
motivate us to initialize the translation models with
ﬁne-tuned PLBART on code summarization and
generation for back-translation as it learned some
alignment across programming languages.

3 Approach

Sequence-to-sequence models, such as PLBART
(Ahmad et al., 2021a), CodeT5 (Wang et al., 2021),
map source code sequences into a shared multilin-
gual space by pre-training on multiple program-
ming languages jointly using unlabeled data (e.g.,
source code from Github). The pre-training ob-
jective of these models is either denoising autoen-
coding (DAE) or ﬁll-in-the-blank, where the mod-
els reconstruct the original code snippet or predict
the missing code tokens given a corrupted code
snippet. Although pre-trained jointly on many lan-
guages, these models only learn to generate in the
same language as input. As a result, these mod-
els are not trainable via back-translation (BT) to
learn programming language translation in an un-
supervised fashion. As an alternative, we propose
translating to and from natural language to perform
back-translation between two programming lan-
guages. We refer to translating to and from natural
language as code summarization and code genera-
tion, respectively. Our proposal is motivated based
on the availability of bimodal data, source code,
and their summaries that are used to train code
summarization and generation models.

3.1 Code Summarization and Generation

Source code documentation (e.g., docstring or com-
ment) written by software developers are available
along with source code on a large scale. Such doc-
umentation has been the key source to form source
code summarization datasets (Wan et al., 2018;
Hu et al., 2018; LeClair and McMillan, 2019; Hu-
sain et al., 2019). These datasets are also utilized
in natural language (NL) to code generation stud-
ies (Parvez et al., 2021). It is tangible that we can
use a code summarization and generation model to
translate programming languages. Such a model
would ﬁrst generate an NL summary from code
in the source language and then generate code in
the target language from the previously generated
NL summary. As we show in the evaluation, such
an approach does not work well in practice (see
table 2); however, code summarization and gener-
ation models are viable proxies to generate noisy
translations. This enables us to train PLBART, to
begin with generating noisy translations and further
learn to improve in a self-supervised fashion when
trained via back-translation. Formally, we jointly
train PLBART in a supervised setting to learn code
summarization (S) and generation (G):

(1)

S = T RAIN Code→Summary (Pc,s)
G = T RAIN Summary→Code (Pc,s)
where Pc,s is estimated using the code-to-text
benchmark from CodeXGlue (Lu et al., 2021). We
follow Tang et al. (2021) to perform multilingual
ﬁne-tuning of PLBART (in Java and Python) to
learn S and G.

3.2 Back-translation

Back-translation (BT) is one of the most popular
ways for unsupervised machine translation (Artetxe
et al., 2018b; Lample et al., 2018a,b). In this ap-
proach, we leverage monolingual data in an un-
supervised fashion. BT jointly trains a source-to-
target model coupled with a backward target-to-
source model. The target-to-source model trans-
lates target sequences into the source language, pro-
ducing noisy sources corresponding to the ground
truth target sequences. The source-to-target model
is then trained to generate the targets from the noisy
sources and vice versa. The two models are trained
in parallel until convergence. This training proce-
dure is widely known as online back-translation
and the focus of this work.

Back-translation uses a target-to-source model to

Figure 3: Overview of our proposed back-translation framework to train PLBART. In the ﬁrst k = m steps (out
of total N training steps), we use a multilingual code summarization and generation model (S, G) to perform
back-translation. In the remaining steps (N − m), PLBART is trained via standard back-translation method.

generate noisy sources and trains a source-to-target
model to reconstruct the targets. Speciﬁcally, in
each step k (a mini-batch update), back-translation
performs the following:

P (f )

k = {(x, fk−1(x))|x ∈ Dsource}
bk = T RAIN target→source (cid:16)
(cid:17)
P (f )
k
k = (cid:8)(bk(y), y) |y ∈ Dtarget
(cid:9)
P (b)
fk = T RAIN source→target (cid:16)

(cid:17)

P (b)
k

(2)

.

Here, Dsource, Dtarget represents unlabeled data in
source and target languages and T RAIN indicates
standard sequence-to-sequence training.

Generally, the training via back-translation starts
from a forward (f0) and a backward (b0) model that
is trained using parallel data (small gold-standard
or large-scale but noisy). Then an extensive collec-
tion of unlabeled data is used to train the translation
models. In this work, we assume there is no paral-
lel data available across programming languages.
We initialize the forward and backward model with
the pre-trained language model, PLBART. As men-
tioned before, PLBART cannot generate code in a
language different from the input (not even a noisy
code) (for example, ﬁgure 1-1). Therefore, we pro-
pose jointly ﬁne-tuning PLBART on code summa-
rization and generation on multiple programming
languages in a supervised setting. Then use the
resulting model to initialize the forward and back-
ward model (f0, b0) for back-translation.

3.3 Summarize–Generate to Back-translate

enables us to use them in initializing the source-to-
target (f ) and target-to-source (b) models for back-
translation. Presumably, such pre-trained models
should facilitate the learning process during train-
ing. Yet, their pre-training objective – i.e., recon-
struction of original input from a noisy source lim-
its their ability to generate code snippets across
languages (as shown in Figure 1). For example,
PLBART as f (·) and b(·) would reconstruct the
input, resulting in fk−1(x) ≈ x and bk(y) ≈ y. As
a result, the models will learn to merely copy the
input sequences rather than translate them.

To this end, we propose to make use of available
parallel data between programming and natural lan-
guages to ﬁne-tune PLBART and then use its pa-
rameters to initialize source-to-target (f ) and target-
to-source (b) models for back-translation. Con-
sequently, we revise the back-translation training
method outlined in Eq. (2) to follow a two-step
generation process to perform back-translation:
code-to-summary generation in natural language
followed by summary-to-code generation in the
source language. Formally, the ﬁrst m steps (k ≤
m) of back-translation is performed as:

P (f )
k = {(x, G (S (x))) |x ∈ Dsource}
(cid:9) .
k = (cid:8)(G (S (y)), y) |y ∈ Dtarget
P (b)
We ﬁnd the noisy parallel sequences2 generated
via summarization and generation commences the
learning process. The overall idea of our proposed
framework is illustrated in Figure 3 and the Al-

(3)

The recent advancements of pre-trained sequence-
to-sequence models on programming languages

2The output sequences are still noisy since the code sum-
marization and generation models are not highly accurate
although trained in a supervised fashion.

Pythondef circlearea(a) :   if a < 0 :     return -1   A = 3.14 * 3 * pow(a,2)   return A / 4SummaryCompute the area of thecircle inscribed in ahexagon with side length"a" Train STrain GJavaint nextPowerOf2(int n){  int count = 0;  if (n>0 && (n & (n-1)) == 0)    return n;  while(n != 0){    n >>= 1; count += 1;  }  return 1 << count;}Pythondef nextPowerOf2(n):  count = 0  if (n and not(n & (n - 1))):    return n  while(n != 0):    n >>= 1    count += 1  return 1 << countTrain fkApply (S, G) / bkTrain bkApply (S, G) / fkAlgorithm 1 Training Procedure
Input: Monolingual (unlabeled) data Dsource and
Dtarget; number of initial steps m; number of total
steps I; code summarizer S(·, ·); code generator
G(·, ·); parameters θ to initialize the forward and
backward translation models f (·, ·) and b(·, ·).
Output: Final model parameters θ.

1: for k = 0, · · · , I do
2:

y ← (ys ∼ Dsource) ∪ (yt ∼ Dtarget)
if k ≤ m then

xnl ∼ S(·|y)
ˆx ∼ G(·|xnl)

(cid:46) code-to-summary
(cid:46) summary-to-code

else

ˆx ← (xs ∼ b(·|yt)) ∪ (xt ∼ f (·|ys))
Update θ by maximizing log-likelihood of

f (ˆxs, yt) and b(ˆxt, ys)

3:

4:

5:

6:

7:

8:

gorithm 1 describes the training procedure. Note
that we ﬁnd it is sufﬁcient to apply our proposed
summarization-generation based back-translation
only for the ﬁrst m steps as the source-to-target and
target-to-source models gradually learn to translate,
the standard back-translation training reinstated.

4 Experiment Setup

4.1 Models and Baselines

Our model Our proposed approach can be ap-
plied to pre-trained sequence-to-sequence mod-
els, e.g., PLBART (Ahmad et al., 2021a) and
CodeT5 (Wang et al., 2021).
In this work, we
chose PLBART3 to perform experiments and show
the effectiveness of our proposed framework.

Baseline Models

j2py is a framework that translates Java source
code to Python.4 It follows handwritten rules man-
ually built using expert knowledge.

(S&G) performs
Summarize-and-Generate
code-to-code translation via two steps, code-to-
summary and summary-to-code generation. We
evaluate the S&G model (as in Eq. (1)) that is used
to perform code summarization and generation in
our proposed framework to train PLBART via BT.

TransCoder
is a neural translation model for
programming languages (Lachaux et al., 2020).

3Since its pretraining implementation is publicly available

at https://github.com/wasiahmad/PLBART.

4https://github.com/natural/

java2python

Java

Python

7.2 M
8.3 M
752 M 665 M

Github - unimodal data
Nb of functions
Nb of tokens
CodeNet - unimodal data
Nb of functions
Nb of tokens
CodeXGlue - bimodal data
164,923
Nb of functions
251,818
21.2 M 44.3 M
Nb of tokens

0.42 M 0.15 M
47.3 M 17.0 M

Table 1: Statistics of the data used to train PLBART
at different stages in this work. Bimodal data refers to
parallel function-summary pairs, while unimodal data
refers to monolingual (and unparallel) functions.

TransCoder is developed by pretraining Trans-
former (Vaswani et al., 2017) via masked language
modeling (MLM) objective (Devlin et al., 2019)
on monolingual source code datasets. In a second
step, TransCoder is trained via denoising autoen-
coding (DAE) and BT. In this work, we consider
TransCoder as the primary baseline.5

DOBF Roziere et al. (2021) proposed a pretrain-
ing objective, DOBF, that leverages the structural
aspects of programming languages. According
to this pretraining paradigm, the identiﬁers (class,
function, and variable names) in code snippets are
obfuscated, and a model is trained to recover the
original names. DOBF shares the exact same neu-
ral architecture as TransCoder.

The evaluation performances of TransCoder and
DOBF are reported from the ofﬁcial code release
by Lachaux et al. (2020).6

4.2 Evaluation Dataset and Metrics

Evaluation Dataset Lachaux et al. (2020) pro-
posed an evaluation dataset composed of parallel
functions in Java, Python, and C++ languages. The
dataset consists of 464 Java to Python and 482
Python to Java test examples, where each example
is accompanied by 10 unit test cases.

Evaluation Metrics

BLEU measures n-gram overlap between a gen-
erated translation and a collection of reference
translations (Papineni et al., 2002).

5We compare TransCoder and PLBART in terms of model

architecture and training setup in the Appendix D.

6https://github.com/facebookresearch/

CodeGen/blob/main/docs/transcoder.md#
results).

Exact Match (EM)
represents the percentage of
generated translations exactly match with the col-
lection of reference translations.

CodeBLEU measures grammatical and logical
correctness in addition to n-gram overlap between
generated and reference translations (Ren et al.,
2020). CodeBLEU is deﬁned as a weighted sum
of n-gram match, weighted n-gram match,7 syntax
match (based on AST), and data-ﬂow match.

Computational Accuracy (CA)
evaluates if a
generated function outputs the same as the refer-
ence when given the same set of inputs. Several re-
cent works (Kulal et al., 2019; Lachaux et al., 2020;
Chen et al., 2021) showed that match-based metrics
(e.g., BLEU, CodeBLEU) are unable to account for
the large and complex space of programs function-
ally equivalent to a reference solution. Lachaux
et al. (2020) introduced this evaluation metric to
assess functional correctness; a translated code is
considered correct if it passes a set of unit tests.

4.3 Training Datasets and Preprocessing

Code Summarization and Generation Lu et al.
(2021) curated a code summarization dataset con-
sisting of code and summary pairs based on the
CodeSearchNet dataset (Husain et al., 2019). We
use this dataset in Java and Python program-
ming languages to train the code-to-summary and
summary-to-code generation models.

Back-translation (BT) For BT training (as dis-
cussed in § 3.3), we use the GitHub public dataset
available on Google BigQuery (Hoffa, 2016).8 We
ﬁrst deduplicate9 the GitHub dataset at the pro-
gram level, extract the functions, and ﬁnally per-
form another deduplication at the function level.
Note that the Github dataset is composed of source
code that covers a wide variety of programming
topics (as they come from various projects). In
contrast, the evaluation dataset is composed of pro-
gramming problems covering basic data structure
and algorithmic concepts. Therefore, to investigate
the impact of data on BT training, we alternatively
chose unparallel code samples in Java and Python
from CodeNet (Puri et al., 2021). The CodeNet
dataset is collected from two online judge websites,
AIZU Online Judge and AtCoder, and composed

7Different weights are assigned to n-grams such that the

keywords (e.g., for, while) have higher weights

8https://console.cloud.google.com/
marketplace/product/github/github-repos
9We used a hash-based data deduplication method.

of submissions for 4053 problems. We use the
deduplicated accepted solutions of the problems
for BT training. Presumably, CodeNet and the eval-
uation dataset (Lachaux et al., 2020) have a similar
nature that should positively impact downstream
translation performance.

Preprocessing We use tree_sitter10 for to-
kenizing Java functions and the tokenizer of the
standard library for Python.11 We extract stan-
dalone functions12 from the BT training datasets
following the function extraction technique from
Lachaux et al. (2020). Considering our computa-
tional budget, we ﬁlter the standalone functions
exceeding a maximum length of 256 to cope with
our computational resources. The statistics of the
preprocessed datasets are presented in Table 1.

4.4

Implementation Details

We jointly train PLBART on code summarization
and generation in Java and Python using the au-
thors’ provided code.13 Subsequently, we further
train PLBART via back-translation as described
in Algorithm 1. We set I = 10, 000 and tuned
m = 200.14 We train PLBART using 8 Nvidia
GeForce RTX 2080 Ti GPUs, and the effective
batch size is maintained at 1024 instances at both
training stages. We optimize PLBART with the
Adam optimizer (Kingma and Ba, 2015), a learn-
ing rate of 10e-4, and use a polynomial learning
rate decay scheduling. The best models are selected
based on the validation BLEU scores. We imple-
ment our approach in Fairseq (Ott et al., 2019) and
use ﬂoat16 operations to speed up training.

Decoding During inference, we use beam search
decoding (Koen, 2004) to generate multiple trans-
lations using PLBART. We chose greedy search
(Beam 1) as the default decoding scheme for valida-
tion and evaluation. However, following Lachaux
et al. (2020), we report two sets of results for the
computational accuracy (CA) metric: CA@n B=n,
the percentage of functions with at least one correct
translation in the beam (of size n), and CA@1 B=n

10https://github.com/tree-sitter/

py-tree-sitter

11https://docs.python.org/3/library/

tokenize.html

12Standalone functions can be used without instantiating
a class. In Java, this corresponds to static methods, and in
Python, it corresponds to functions outside classes.

13https://github.com/wasiahmad/PLBART/

tree/main/multilingual

14We tuned m in the range [100, 1000] with 100 steps.

Models

Java → Python

Python → Java

BLEU EM CodeBLEU CA BLEU EM CodeBLEU CA

j2py*
TransCoder∗
TransCoder w/ DOBF∗
S&G (1)
PLBART (this work)
trained via BT
trained via BT (via S&G)

-
68.1
-
7.6

31.2
64.2

-
3.7
-
0.0

0.0
2.8

-
-
-
15.8

36.6
63.4

38.3
46.9
49.2
0.2

0.0
40.4

-
64.6
-
12.4

31.7
64.1

-
0.8
-
0

0.0
2.1

-
-
-
16.3

32.1
65.9

-
32.6
40.4
0.2

0.0
31.9

Table 2: Evaluation results of the baselines models and our proposed framework using greedy decoding. ∗ indicates
the updated scores reported in the ofﬁcial code repository of Lachaux et al. (2020).. Note that, TransCoder and
PLBART models have 312M and 140M parameters, respectively.

the percentage of functions where the hypothesis
in the beam with the highest log-probability is a
correct translation.

5 Results and Analysis

5.1 Main Result

Table 2 shows the performance of our proposed
approach and the baseline models on both Java
to Python and Python to Java translation. We be-
gin by comparing PLBART directly used in back-
translation (BT) training with our proposed ap-
proach (the last block in Table 2). Since, PLBART
does not know to generate across languages, so
when the model is trained via BT, it only learns
to copy the input sources. As a result, PLBART
scores 0% EM and 0% CA, while 30+ BLEU and
CodeBLEU scores. In contrast, following our pro-
posed approach of summarizing and generating to
back-translate, PLBART trained via BT (via S&G)
achieves 40.4% and 31.9% CA scores. This perfor-
mance is competitive to state-of-the-art translation
system, TransCoder.15 We further compare them
using beam search decoding in Table 3.

Overall, the experimental results conﬁrm our
conjecture that pre-trained sequence-to-sequence
models cannot be effectively used in BT training;
however, training via S&G empowers them to gen-
erate across languages and be further trained via
BT to learn programming language translation.

5.2 Analysis

Summarize and generate to create parallel data
Our proposed approach generates parallel code se-
quences on the ﬂy (online) for training. An alter-
native to our approach is to use a code summariza-

15Note that, while comparing PLBART with TransCoder
on the translation performance, their differences (shown in
Table 9) should be taken into account.

Models
Java → Python
CA@1 B=1
CA@1 B=10
CA@5 B=5
CA@10 B=10
Python → Java
CA@1 B=1
CA@1 B=10
CA@5 B=5
CA@10 B=10

TransCoder PLBART

46.9
48.8
60.0
64.4

32.6
36.0
44.3
51.1

40.4
41.8
47.7
50.3

31.9
34.5
45.1
50.0

Table 3: Computational accuracy (CA@m) with beam
search decoding and comparison between TransCoder
and PLBART. TransCoder’s performances are reported
from Lachaux et al. (2020). The value B indicates the
beam size. CA@m B=n means that we use beam de-
coding to generate n translations, and select the top m
translations based on their log-probability scores.

tion and generation model to create parallel code
sequences (ofﬂine) and warm-start PLBART for
back-translation-based training. We compare these
two approaches in Table 4, and the results show that
both approaches perform comparably. However, it
is essential to note that the online setting gives us
ﬂexibility as we can tune the number of initial steps
(m in Algorithm 1). In contrast, the ofﬂine setting
requires generating a sufﬁciently large number of
parallel code sequences for effective training.

Impact of in-domain training data The evalua-
tion dataset comprises solutions to programming
problems involving data structures and algorithm
concepts. While Github offers large-scale unla-
beled data, most of its code belongs to software
projects that use APIs and advanced functionali-
ties. Therefore, we utilize an alternative dataset
called CodeNet collected from two online judge
websites. We refer to this dataset as in-domain

Approach

Warm-start w/ PD
Proposed approach

Java to Python
BLEU EM CodeBLEU CA
41.9
60.5
40.4
64.2

61.1
63.4

2.8
2.8

Python to Java
BLEU EM CodeBLEU CA
32.0
62.6
31.9
64.1

65.9
65.9

2.4
2.1

Table 4: Comparison between PLBART warm-started using parallel data (PD) and our approach to summarize and
generate to back-translate on the ﬂy during the initial steps of back-translation training.

Data Source

Github
CodeNet

Java to Python

Python to Java

BLEU EM CodeBLEU CA BLEU EM CodeBLEU CA
31.9
64.2
46.5
65.6

64.1
65.1

65.9
68.5

63.4
64.7

40.4
50.9

2.8
3.1

2.1
2.5

Table 5: PLBART evaluation results when our proposed framework uses data from Github (available via BigQuery
(Hoffa, 2016)) and competitive programming sites (available via CodeNet (Puri et al., 2021)).

data, and we compare its usage with Github data
on BT-based training. The results in Table 5 show
that the use of in-domain data signiﬁcantly boosts
the performance in both translation directions. A
detailed error analysis reveals that such a perfor-
mance boost is due to a reduction in TypeError.
We speculate that in-domain data have similarities
in the data type usage that helps the model.

Due to the page limit, we present more ﬁndings
of the error analysis in the Appendix. In addition,
we discuss the limitations and risks of using our
proposed model in the Appendix.

6 Related Work

Programming Language Translation Translat-
ing programs or source code across different pro-
gramming languages (PL) requires a profound un-
derstanding of the PLs. Having strictly deﬁned
syntax and semantics, PLs are suitable for phrase-
based statistical machine translation (Nguyen et al.,
2013; Karaivanov et al., 2014; Aggarwal et al.,
2015). Chen et al. (2018) introduced a tree to tree
machine translation to translate programs and to
learn the syntactic alignment between source and
target PL. Recently proposed pre-trained program-
ming language models showed promising results
in translating programs across PLs (Feng et al.,
2020b; Guo et al., 2021; Ahmad et al., 2021a,b).
However, these approaches require a set of parallel
programs to train the encoder-decoder model.

Recently proposed Transcoder (Lachaux et al.,
2020) shows initial success results in unsupervised
program translation, eliminating the requirement
of bi-modal data. They achieve such jointly train-
ing a model using XLM (Conneau and Lample,
2019), Denoising Auto Encoding (DAE) (Vincent
et al., 2008), and Back-Translation(BT) (Lample

et al., 2018a). This work empirically investigated
the suitability of adopting BT to train existing pre-
trained encoder-decoder models and proposed an
alternative via summarization and generation.

Unsupervised Machine Translation via Back-
translation Gathering sufﬁciently large parallel
corpora has been a major challenge for Machine
Translation (MT) (Guzmán et al., 2019). Several
research efforts are invested in learning MT using
monolingual data (Artetxe et al., 2018a,b; Lachaux
et al., 2020) to solve this problem. For example,
Gulcehre et al. (2015) proposed integration of a
Language model integrated into the decoder. He
et al. (2016) proposed Neural MT (NMT) as a bidi-
rectional and dual learning task. More recent ad-
vancements in unsupervised MT leverages back-
translation (BT) (Sennrich et al., 2016; Lample
et al., 2018a,b). In back-translation, the target-to-
source model generates noisy sources given tar-
get sequences and then trains the source-to-target
model to reconstruct the targets and vice versa.
While BT has been widely adopted for unsuper-
vised NMT, it is used in other applications (Zhu
et al., 2017; Hoffman et al., 2018; Shen et al., 2017;
Yang et al., 2018; Zhang et al., 2019).

7 Conclusion

In this research, we show that pre-trained sequence-
to-sequence models (e.g., PLBART) are not suit-
able for direct adaptation via back-translation to
learn to translate. To address the issue, we pro-
pose to use code summarization and generation as
an alternative to performing back-translation. We
show that our proposed approach turns PLBART
into a translation model that performs comparably
to existing unsupervised translation models.

References

Karan Aggarwal, Mohammad Salameh, and Abram
Hindle. 2015. Using machine translation for con-
verting python 2 to python 3 code. Technical report,
PeerJ PrePrints.

Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and
Kai-Wei Chang. 2021a. Uniﬁed pre-training for pro-
gram understanding and generation. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 2655–2668,
Online. Association for Computational Linguistics.

Wasi Uddin Ahmad, Md Golam Rahman Tushar,
Saikat Chakraborty, and Kai-Wei Chang. 2021b.
Avatar: A parallel corpus for java-python program
translation. arXiv preprint arXiv:2108.11590.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
2018a. Unsupervised statistical machine transla-
In Proceedings of the 2018 Conference on
tion.
Empirical Methods in Natural Language Processing,
pages 3632–3642, Brussels, Belgium. Association
for Computational Linguistics.

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2018b. Unsupervised neural ma-
In International Conference on
chine translation.
Learning Representations.

Mikel Artetxe and Holger Schwenk. 2019. Mas-
sively multilingual sentence embeddings for zero-
shot cross-lingual transfer and beyond. Transac-
tions of the Association for Computational Linguis-
tics, 7:597–610.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde, Jared Kaplan, Harri Edwards, Yura
Burda, Nicholas Joseph, Greg Brockman, et al. 2021.
Evaluating large language models trained on code.
arXiv preprint arXiv:2107.03374.

Xinyun Chen, Chang Liu, and Dawn Song. 2018. Tree-
to-tree neural networks for program translation. In
Advances in Neural Information Processing Systems
31, pages 2547–2557. Curran Associates, Inc.

Alexis Conneau and Guillaume Lample. 2019. Cross-
lingual language model pretraining.
In H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,
E. Fox, and R. Garnett, editors, Advances in Neu-
ral Information Processing Systems 32, pages 7059–
7069. Curran Associates, Inc.

for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessan-
dro Morari, Baishakhi Ray, and Saikat Chakraborty.
2021. Contrastive learning for source code with
structural and functional properties. arXiv preprint
arXiv:2110.03868.

Sergey Edunov, Myle Ott, Michael Auli, and David
Grangier. 2018. Understanding back-translation at
In Proceedings of the 2018 Conference on
scale.
Empirical Methods in Natural Language Processing,
pages 489–500, Brussels, Belgium. Association for
Computational Linguistics.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,
Xiaocheng Feng, Ming Gong, Linjun Shou, Bing
Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020a.
CodeBERT: A pre-trained model for programming
and natural languages. In Findings of the Associa-
tion for Computational Linguistics: EMNLP 2020,
pages 1536–1547, Online. Association for Compu-
tational Linguistics.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,
Xiaocheng Feng, Ming Gong, Linjun Shou, Bing
Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020b.
CodeBERT: A pre-trained model for programming
and natural languages. In Findings of the Associa-
tion for Computational Linguistics: EMNLP 2020,
pages 1536–1547, Online. Association for Compu-
tational Linguistics.

Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2015. On us-
ing monolingual corpora in neural machine transla-
tion. arXiv preprint arXiv:1503.03535.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu
Tang, Shujie Liu, Long Zhou, Nan Duan, Jian Yin,
Daxin Jiang, et al. 2021. Graphcodebert: Pre-
training code representations with data ﬂow.
In
International Conference on Learning Representa-
tions.

Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan
Pino, Guillaume Lample, Philipp Koehn, Vishrav
Chaudhary, and Marc’Aurelio Ranzato. 2019. The
FLORES evaluation datasets for low-resource ma-
chine translation: Nepali–English and Sinhala–
English. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
6098–6111, Hong Kong, China. Association for
Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association

Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,
Tie-Yan Liu, and Wei-Ying Ma. 2016. Dual learn-
ing for machine translation. In Advances in Neural
Information Processing Systems, volume 29. Curran
Associates, Inc.

Felipe Hoffa. 2016. Github on bigquery: Analyze all

the open source code.

Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei Efros, and Trevor
Darrell. 2018. CyCADA: Cycle-consistent adversar-
In Proceedings of the 35th
ial domain adaptation.
International Conference on Machine Learning, vol-
ume 80 of Proceedings of Machine Learning Re-
search, pages 1989–1998. PMLR.

Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and
Zhi Jin. 2018. Summarizing source code with trans-
ferred api knowledge. In Proceedings of the Twenty-
Seventh International Joint Conference on Artiﬁcial
Intelligence, IJCAI-18, pages 2269–2275. Interna-
tional Joint Conferences on Artiﬁcial Intelligence
Organization.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. Code-
searchnet challenge: Evaluating the state of seman-
tic code search. arXiv preprint arXiv:1909.09436.

Svetoslav Karaivanov, Veselin Raychev, and Martin
Vechev. 2014. Phrase-based statistical translation
In Proceedings of the
of programming languages.
2014 ACM International Symposium on New Ideas,
New Paradigms, and Reﬂections on Programming &
Software, pages 173–184.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
In 3rd Inter-
method for stochastic optimization.
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Philipp Koen. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
In Proceedings of the 6th Conference of the
els.
Association for Machine Translation in the Ameri-
cas: Technical Papers, pages 115–124, Washington,
USA. Springer.

Sumith Kulal, Panupong Pasupat, Kartik Chandra,
Mina Lee, Oded Padon, Alex Aiken, and Percy S
Liang. 2019. Spoc: Search-based pseudocode to
code. Advances in Neural Information Processing
Systems, 32.

Marie-Anne Lachaux, Baptiste Roziere, Lowik
Chanussot, and Guillaume Lample. 2020. Unsu-
pervised translation of programming languages.
In Advances in Neural
Information Processing
Systems, volume 33, pages 20601–20611. Curran
Associates, Inc.

Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
and Marc’Aurelio Ranzato. 2018a. Unsupervised
machine translation using monolingual corpora only.
In International Conference on Learning Represen-
tations.

Guillaume Lample, Myle Ott, Alexis Conneau, Lu-
dovic Denoyer, and Marc’Aurelio Ranzato. 2018b.

Phrase-based & neural unsupervised machine trans-
In Proceedings of the 2018 Conference on
lation.
Empirical Methods in Natural Language Processing,
pages 5039–5049, Brussels, Belgium. Association
for Computational Linguistics.

Alexander LeClair and Collin McMillan. 2019. Rec-
ommendations for datasets for source code summa-
In Proceedings of the 2019 Conference
rization.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 3931–3937, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021.
Codexglue: A machine learning benchmark dataset
arXiv
for code understanding and generation.
preprint arXiv:2102.04664.

Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N
Nguyen. 2013. Lexical statistical machine transla-
tion for language migration. In Proceedings of the
2013 9th Joint Meeting on Foundations of Software
Engineering, pages 651–654.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
fairseq: A fast, extensible
Michael Auli. 2019.
In Proceedings of
toolkit for sequence modeling.
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics
(Demonstrations), pages 48–53, Minneapolis, Min-
nesota. Association for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.

Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty,
Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval
augmented code generation and summarization. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021, pages 2719–2734, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.

Ruchir Puri, David S Kung, Geert Janssen, Wei
Zhang, Giacomo Domeniconi, Vladmir Zolotov, Ju-
lian Dolby, Jie Chen, Mihir Choudhury, Lindsey
Decker, et al. 2021. Project codenet: A large-scale
ai for code dataset for learning a diversity of coding
tasks. arXiv preprint arXiv:2105.12655.

Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie
Liu, Duyu Tang, Ming Zhou, Ambrosio Blanco, and
Shuai Ma. 2020. Codebleu: a method for auto-
matic evaluation of code synthesis. arXiv preprint
arXiv:2009.10297.

Baptiste Roziere, Marie-Anne Lachaux, Marc
Szafraniec, and Guillaume Lample. 2021. Dobf: A
deobfuscation pre-training objective for program-
ming languages. In Advances in Neural Information
Processing Systems.

Zhirui Zhang, Shuo Ren, Shujie Liu, Jianyong Wang,
Peng Chen, Mu Li, Ming Zhou, and Enhong Chen.
2019. Style transfer as unsupervised machine trans-
In Thirty-Third AAAI Conference on Artiﬁ-
lation.
cial Intelligence.

Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. 2017. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Pro-
ceedings of the IEEE international conference on
computer vision, pages 2223–2232.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
Improving neural machine translation mod-
2016.
In Proceedings of the
els with monolingual data.
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
86–96, Berlin, Germany. Association for Computa-
tional Linguistics.

Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2017. Style transfer from non-parallel text
by cross-alignment. In Advances in Neural Informa-
tion Processing Systems 30.

Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-
man Goyal, Vishrav Chaudhary, Jiatao Gu, and An-
gela Fan. 2021. Multilingual translation from de-
noising pre-training. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021,
pages 3450–3466, Online. Association for Computa-
tional Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30, pages 5998–6008. Curran Asso-
ciates, Inc.

Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of the 25th international con-
ference on Machine learning, pages 1096–1103.

Yao Wan, Zhou Zhao, Min Yang, Guandong Xu,
Haochao Ying, Jian Wu, and Philip S. Yu. 2018. Im-
proving automatic source code summarization via
In Proceedings of
deep reinforcement learning.
the 33rd ACM/IEEE International Conference on
Automated Software Engineering, ASE 2018, page
397–407, New York, NY, USA. Association for
Computing Machinery.

Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven C.H.
Hoi. 2021. CodeT5: Identiﬁer-aware uniﬁed pre-
trained encoder-decoder models for code under-
standing and generation. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 8696–8708, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.

Zichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing, and
Taylor Berg-Kirkpatrick. 2018. Unsupervised text
style transfer using language models as discrimina-
tors. In Proceedings of the 32nd International Con-
ference on Neural Information Processing Systems,
pages 7298–7309.

Supplementary Material: Appendices

TransCoder PLBART

Error Category

TransCoder PLBART

Java → Python
#Tests
Error
Failure
Success
EM
Timeout
Python → Java
#Tests
Error
Failure
Success
EM
Timeout

464
149
93
218
17
4

482
201
118
157
6
6

464
146
123
188
24
7

482
212
108
154
2
8

Table 6: Detailed results of computational accuracy us-
ing greedy decoding for Java ↔ Python translation.

#Errors (Java → Python)

Compilation

Runtime

TypeError
IndexError
NameError
ValueError
UnboundLocalError
Others
SyntaxError

#Errors (Python → Java)

Compilation
TypeError
CantFindSymbol
SyntaxError
BadOperand
Others

A Analysis of Computational Accuracy

Runtime

149

-

149
47
18
17
11
13
17
26

201

151
89
23
14
15
10

50
40
5
2
3

146

-

146
61
20
16
15
11
14
9

212

180
108
30
25
12
5

27
15
6
3
3

Table 6 shows the breakdown of computational
accuracies for Java-to-Python and Python-to-Java
translation for TransCoder and our proposed ap-
proach using PLBART. We execute the generated
function and match the output w.r.t. the expected
output. TransCoder results in 149 error cases, 93
failure cases, and 218 success cases in Java-to-
Python translation, with 17 solutions matching the
ground truth. In contrast, PLBART results in 146
error cases, 123 failure cases, 188 success cases.
Out of these 188 successes in PLBART, 24 solu-
tions exactly match the target solution.

For Python-Java translation, TransCoder results
in 201 errors, 118 failures, and 157 successes, out
of which 6 are an exact match. On the other hand,
in the case of PLBART, there are 212 error cases,
108 failure cases, and 154 success cases, out of
which 2 exactly match the target solution.

B Error Analysis

We further analyze the error cases for TransCoder
and our proposed approach using PLBART. Since
Python is an interpreted language, syntactic and
semantic errors are caught at runtime. Thus, we
categorize all errors for Java-to-Python translation
as runtime errors. Table 7 shows the errors in
both Java-to-Python and Python-to-Java translation.
While PLBART is susceptible to TypeError,

IndexOutOfBoundsE.
NumberFormatE.
NullPointerE.
Others

Table 7: Category of errors made by the TransCoder
and PLBART translation models. The error categories
are sorted based on the PLBART’s error count on the
respective category. In Python → Java runtime error
categories, “E.” stands for “Exception”.

TransCoder is disproportionately susceptible to
SyntaxError.
In the case of Python-to-Java
translation, PLBART exhibits more Compilation
errors, but TransCoder exhibits more Runtime er-
rors. The most common type of compilation errors
in both TransCoder and PLBART is TypeError.
The most common runtime error in Python-to-Java
translation is InderOutOfBoundException
for both models, where TransCoder exhibits more
than twice the number of such errors in PLBART.
Finally, we identiﬁed the top ﬁve error categories
(which accounts for 123 errors out of 146) ex-
hibited by PLBART in Java-to-Python translation
and analyzed the error messages. In most cases,
TypeError and ValueError are due to mis-
match in underlying data type of variable. Table 8
shows the detailed statistics of different error types,
sub-types, and their frequencies.

Error Category

Count

Type Error
list indices must be integers or slices, not A
A object does not support item assignment
A object cannot be interpreted as an integer
unsupported/bad operand type(s)
int object is not iterable/callable/subscriptable
Others

Index Error
B index out of range
others

Name Error
name C is not deﬁned

Value Error
not enough values to unpack
too many values to unpack
the truth value of an array with more than one element is ambiguous
others

Unbound Local Error
local variable D referenced before assignment

61
18
13
8
10
6
6

20
19
1

16
16

15
7
3
3
2

11
11

Table 8: Analyzing the ﬁve most frequent error cases (123 out of 146) encountered in PLBART generated Java to
Python translation. A and B indicate {bool, int, tuple, str, range} and {string, list}, respectively. C and D indicate
identiﬁer (class, function, variable) names.

C Qualitative Examples

Figure 4 shows an example of Java-to-Python trans-
lation by PLBART. The translated code is both
syntactically and semantically correct i.e., our com-
piler could successfully parse and build the trans-
lated code. It passed 2 test cases out of 10 when exe-
cuted. The translated code is slightly different from
the input Java code. In particular, line 13 in the
input Java code is a loop that iterates backward (de-
creasing order). However, line 12 in the generated
python code iterates forward (increasing order). If
the generated python code was range(c-1, 0,
-1) instead of range(c-1), it would pass all
the test cases. We attribute such behavior to the
fact that range(*) is much more frequent pattern
than range(*, 0, -1) in python code.

D TransCoder vs. PLBART

As we consider TransCoder as the primary base-
line of our proposed approach using PLBART, for
the sake of fairness, we compare them in terms
of model structure and training setting. Table 9
presents the comparison. TransCoder and PLBART
both use the Transformer (Vaswani et al., 2017) ar-

chitecture, but TransCoder is a twice as large model
as PLBART. Both the models have gone through a
two-stage training process. In Stage-1, TransCoder
is pre-trained via MLM using 920B tokens, while
PLBART is pre-trained via DAE using 87B tokens.
In Stage-2, TransCoder leverages 625M tokens and
jointly trained via DAE and BT. In comparison,
PLBART is trained via BT using 430M tokens.

Why TransCoder does not suffer from the
same language generation issue? In Stage-1
pre-training, TransCoder only trains the Trans-
former Encoder and then initializes a decoder with
Encoders’ parameters, and the cross attention sub-
layers are randomly initialized. We speculate that
such random initialization leaves TransCoder un-
biased towards generating in the same language
as input. Moreover, PLBART uses language ID
token as a preﬁx to generate in the target lan-
guage. We noticed that PLBART’s decoder dis-
regards the preﬁx token if not ﬁne-tuned to gen-
erate in the target language. On the other hand,
TransCoder uses language embeddings with each
token in the input. Intuitively, this does not allow
the TransCoder’s decoder to ignore the language

Input function in Java

Translated function in Python

1 s t a t i c i n t sortedCount ( i n t mat [ ] [ ] ,
i n t c ) {
2

1 d e f s o r t e d _ c o u n t ( mat ,
2

r , c ) :

i n t
r ,
r e s u l t = 0 ;

i n t
f o r ( i n t
j ;
i n t
f o r ( j = 0 ;

i = 0 ;

i < r ;

i ++) {

j < c − 1 ;

j ++)

i f ( mat [ i ] [ j + 1 ] <= mat [ i ] [ j ] )

break ;
i f ( j == c − 1 )

r e s u l t ++;

}
f o r ( i n t
i n t
f o r ( j = c − 1 ;

i = 0 ;

j ;

i < r ;

i ++) {

j > 0 ;

j − −)

i f ( mat [ i ] [ j − 1 ] <= mat [ i ] [ j ] )

break ;

i f ( c > 1 && j == 0 )

r e s u l t ++;

}
r e t u r n r e s u l t ;

}

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

r e s u l t = 0
i
f o r
j = 0
j
f o r

i n range ( r ) :

i n range ( c − 1 ) :

i f mat [ i ] [ j + 1 ] <= mat [ i ] [ j ] :

break
j == c − 1 :

i f

r e s u l t += 1
i

i n range ( r ) :

f o r

j = c − 1
j
f o r

i n range ( c − 1 ) :

i f mat [ i ] [ j − 1 ] <= mat [ i ] [ j ] :

break

i f c > 1 and j == 0 :

r e s u l t += 1

r e t u r n r e s u l t

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

Figure 4: An example of Java to Python translation by PLBART that passes 2 out of 10 unit test cases. Line no. 13
(marked in green) in the Java function is incorrectly translated in python (line no. 12, marked in red). Replacing
the range function parameter “(c-1)” by “(c - 1, 0, -1)” would make the translated function pass all the test cases.

#layers (encoder)
#layers (decoder)
#heads
Model dim
Vocab size
Total parameters
Stage1: Pre-training
Objective
Total tokens
Token types
Languages
Stage2: Training
Objective
Total tokens
Token types
Languages

TransCoder
6
6
8
1024
64,000
312 M

MLM
920 B
BPE

PLBART
6
6
12
768
50,000
140 M

DAE
87 B
Sentencepiece

Java, Python, C++ Java, Python, English

DAE+BT
625 M
BPE
Java, Python, C++

BT
430 M
Sentencepiece
Java, Python

Table 9: TransCoder vs. PLBART.

information. For example, with position index “0”
and language ID “Python”, TransCoder is more
likely to generate “def” token and less likely to
generate “static” or “int” since they do not ap-
pear in the Python language. In essence, unlike
PLBART, TransCoder does not suffer from the is-
sue of sequence-to-sequence models being unable
to generate across languages.

E Limitations and Risks

One of the risks of using our developed translation
model is we used the Github dataset for training

that may contain information that uniquely identi-
ﬁes an individual or offensive content. Since we
are developing the translation model for research
purposes only, we believe our usage of the Github
data does not violate their licensing terms and con-
ditions. While we do not present it as a justiﬁca-
tion, the PLBART model was pre-trained on the
Github data that may include sensitive informa-
tion. As our intention is to develop a programming
language translation model, it is unlikely to gener-
ate sensitive information unless it is provided such
information as input.

