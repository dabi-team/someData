K-CONFIG: Using Failing Test Cases to Generate
Test Cases in GCC Compilers

Md Raﬁqul Islam Rabin
University of Houston
mdraﬁqulrabin@gmail.com

Mohammad Amin Alipour
University of Houston
amin.alipour@gmail.com

9
1
0
2

g
u
A
7
2

]
E
S
.
s
c
[

1
v
1
8
4
0
1
.
8
0
9
1
:
v
i
X
r
a

Abstract—The correctness of compilers is instrumental in the
safety and reliability of other software systems, as bugs in
compilers can produce programs that do not reﬂect the intents
of programmers. Compilers are complex software systems due to
the complexity of optimization. GCC is an optimizing C compiler
that has been used in building operating systems and many other
system software.

In this paper, we describe K-CONFIG, an approach that uses
the bugs reported in the GCC repository to generate new test
inputs. Our main insight is that the features appearing in the bug
reports are likely to reappear in the future bugs, as the bugﬁxes
can be incomplete or those features may be inherently challenging
to implement hence more prone to errors. Our approach ﬁrst
clusters the failing test input extracted from the bug reports into
clusters of similar test inputs. It then uses these clusters to create
conﬁgurations for Csmith, the most popular test generator for
C compilers. In our experiments on two versions of GCC, our
approach could trigger up to 36 miscompilation failures, and
179 crashes, while Csmith with the default conﬁguration did not
trigger any failures. This work signiﬁes the beneﬁts of analyzing
and using the reported bugs in the generation of new test inputs.

I. INTRODUCTION

Compilers translate programs understandable by developers
to programs that machines can understand and execute. Com-
pilers are the key part of software development infrastructure
that makes all software systems depend on them. Developers
rely on compilers to build and debug their programs, libraries,
and operating systems. Optimization passes in the compilers
search the input programs for the opportunities to improve
various aspects of the output programs such as execution time,
memory consumption, and the code size.

Optimizing compilers are complex software systems that
constitute several passes from various syntax and semantic
analyses to code generation. As programming languages grow
and add new features, the compilers that implement these
features also grow in size and complexity. Moreover, com-
pilers attempt to accommodate the translation of several pro-
gramming languages which further complicates the compiler
system. Today’s GCC compiler is over 10 million lines of
code [10]. Testing such massive, sophisticated systems is a
non-trivial task and researchers and developers still can ﬁnd
many bugs in modern compilers.

Several approaches for testing compilers have been pro-
posed; for example, [14], [5], [6], [8], [15], [16], [23], [13], to
name few. These approaches either generate test inputs from
scratch by grammar [24] and learning [8], or they create new

test input by manipulating [14] or transforming the existing
test input, e.g., [15].

In this paper, we evaluate the use of failure-inducing test
inputs to generate new test inputs. Our insight is that these
test inputs can provide hints into places in the code that are
more prone to be buggy. In fact, this idea is not that novel.
LangFuzz [14] transplants fragments of failing test
inputs
to other programs to generate new test input. However, our
work takes a signiﬁcantly different approach. In this approach
that we call K-CONFIG, instead of embedding fragments for
existing failing test inputs into new test input to create a new
test input, we analyze features of failing test inputs to create
new conﬁgurations for a test generator. The test generator uses
these conﬁgurations for creating new test input that exhibits
similar features to the original failing test input. This approach
is also different from (deep) learning-based approaches such
as DeepSmith [8], whereas they try to build a generative model
for the test inputs in two ways. First, while learning approaches
requires many test inputs with millions of tokens to train a
model, this approach can work with a couple of thousands of
failing test inputs. Second, learning based approaches tend to
converge to a limited language model of test input that overly
restricts the type of test inputs that can be produced [12].
K-CONFIG instead uses the conﬁguration of test generators
to guide testing which is less constrained than the generation
of test inputs in learning-based approaches. In particular, K-
CONFIG only speciﬁes the programming constructs that should
be present in the generated test inputs, and the order or number
of those constructs are determined by the test generator.

Figure 1 depicts the overall workﬂow of the approach. It
constitutes following main phases: (1) collecting failing test
inputs, (2) extracting conﬁgurable test features from failing
test inputs, (3) clustering test inputs into a similar cluster,
(4) generating conﬁgurations based on clusters, and ﬁnally (5)
using conﬁgurations to generate new test input.

Of course, there are limitations to the application of this
approach. First, it assumes that a stable test generator exists.
Second, it requires a set of failing test inputs. GCC compiler
easily satisﬁes both requirements, as it has been under develop-
ment for decades and the bug reports are available. Moreover,
it has a mature well-engineered test generator, Csmith [24]
[7]. It allows us to evaluate the effectiveness of this approach
in testing GCC compilers.

We have implemented the proposed approach for GCC C
compiler testing. We collected 3661 failing test inputs from

 
 
 
 
 
 
Fig. 1. The overall workﬂow of K-CONFIG approach.

inputs and collect

GCC codebase. We parse the test
the
features that can be used in the conﬁguration of Csmith.
We used the K-Means algorithm to cluster the test inputs.
K-Means returns centroids of the clusters of similar test
inputs. We use these centroids to synthesize conﬁgurations for
Csmith.

We performed a large scale experiment

to evaluate the
effectiveness of conﬁgurations generated by this approach with
the default conﬁguration of Csmith on two versions of GCC.
In total, we experimented the new and default conﬁguration for
over 900 hours (almost 40 days). The result of our experiment
shows that the new conﬁgurations could ﬁnd up to 36 test input
for miscompilation, and 179 test input for crashes per 13-hour
test sessions, while Csmith with the default conﬁguration could
not ﬁnd any failures at the same time. It reinforces the previous
studies that many of bugﬁxes are incomplete [25] and GCC
is not an exception. This also indicates that processing failing
test input can provide insights into the regions of code that
are susceptible to bugs.

Contributions This paper makes the following main con-

tributions:

• We propose a novel approach for testing compilers with

mature test generators.

• We perform a large-scale study to evaluate the efﬁciency

of the proposed approach.

• We make code and test input available for further use. 1
Paper Organization Section II demonstrates the proposed
approach. Section III describes the experimental setup for the
evaluation of the approach. Section IV provides an analysis
of the results and answer to the research questions. Section V
surveys the related works, Section VI discusses some of the
threats to validity. Finally, Section VII concludes the paper.

II. PROPOSED APPROACH

Main programming languages such as C and JavaScript have
test generators that produce test inputs for those languages.
The test inputs can be used to test compilers and interpreters
of the languages. For example, Csmith has been able to ﬁnd
hundreds of bugs in mainstream C compilers. Another good
example is jsFunFuzz [21] for JavaScript that has found thou-
sands of bugs in JavaScript interpreters. Newer programming

1We add the URL to the data and code at the time of publication.

languages are also developing such tools for testing compilers,
for example, Go-Fuzz [11] for Go.

Conﬁgurable test generators, such as Csmith, allow devel-
opers to specify some of the characteristics of the test input
to be generated. This way developers can control the test
generation and direct the test generation process. It is fair to
say that the current test generation techniques under-utilize the
conﬁguration of the test generators. We only could ﬁnd two
studies that use the conﬁgurations: swarm testing [13] and fo-
cused random testing [1]. Swarm testing randomly enables or
disables options in the test generators. Focused random testing
attempts to establish a causation relation between conﬁgurable
options and test coverage in order to ﬁnd conﬁgurations that
can target parts of the code.

In this section, we describe the proposed approach in detail.
Our approach is based on analysis of previous failing test
inputs to generate the conﬁguration for the test generators that
we call it K-CONFIG. K-CONFIG takes T S, a set of existing
test inputs that exhibit some interesting property P , and a
conﬁgurable test generator TESTGEN for a compiler. The goal
of K-CONFIG is to analyze T S to extract k conﬁgurations for
TESTGEN that are likely to generate test cases that exhibit P .
Figure 2 depicts an overview of the workﬂow for realizing
K-CONFIG for testing the GCC compiler. It can broadly be
divided into two phases: (1) extracting feature centroids from
test suite of failing test inputs, and (2) using centroids to
generate test cases. We describe these phases in the following
subsections.

Fig. 2. Overview of K-CONFIG approach for testing GCC.

A. Phase 1: Extracting Feature Centroids from Test Suite

Conﬁgurable Test Generator Csmith [24] is a conﬁgurable
random test generator for C compilers. The common practice

testing. That

for testing compilers is differential
is a test
input is compiled and executed by two or more versions of
compilers, or two or more optimization levels, and the results
are compared. The metamorphic test oracle speciﬁes the result
of the output of the compiled programs by all compilers and
optimization levels must be the same. An obnoxious feature of
testing compilers, especially C compilers is that the language
allows undeﬁned behavior. Undeﬁned behaviors are those the
standard of the language does specify standard behavior of the
program for certain conditions. For example, the C standard
does not specify default values for the uninitialized variables
in the programs; it, therefore, is up to the developers of the
compilers to decide on the actual behavior. Csmith does the
best effort to avoid the undeﬁned behaviors in C.

Controllable Features Csmith allows developers to choose
the C programming constructs that they want in the test inputs
generated by Csmith. The order and number of the constructs
however are chosen randomly and developers cannot control
them—mainly because Csmith is a random generator that
uses grammar to generate test cases. We use Csmith 2.3.0
in our experiments that provides 28 conﬁguration options.
These options are offered in the form of “feature” for
including the feature in the test input and “no-feature”
to exclude the feature in the generation of the test inputs.
For example, Csmith includes a volatile variable in test
input by using “--volatiles” or excludes it by using
“–no-volatiles”. The 28 features list: “argc, arrays, bit-
ﬁelds, comma-operators, compound-assignment, consts, divs,
pre-incr-operator, pre-decr-operator, post-incr-operator, post-
decr-operator, unary-plus-operator,
int8,
uint8, ﬂoat,
inline-function, muls, packed-struct, pointers,
structs, unions, volatiles, volatile-pointers, const-pointers,
global-variables, and builtins”

longlong,

jumps,

Test Suite Corpus At ﬁrst, to extract the properties of the
failing test input, we need a corpus of failing test suite. We
extracted 7131 test input from bug reports in GCC; these test
inputs caused some older versions of GCC to fail. We use
these test inputs as the basis of our analysis.

Extracting Test Features We use pycparser v2.19 [20],
a parser for the C programming language written in Python to
extract C programming constructs used in the test suite corpus.
We use the abstract syntax tree (AST) to ﬁnd out features
present in the test inputs in the test suite. An unanticipated
ﬁnding was that the pycparser failed on C programs having
comments. It is therefore likely that pycparser failed on
GCC regression test C programs as those C programs con-
tain comments. To resolve this issue and make pycparser
working on those C programs, we removed the comments from
C programs. Finally, pycparser was able to parse 3,661 of
7,131 test C programs. We investigated the rest of the test input
and found that they are indeed not parsable, but they caused
the earlier versions of GCC to crash. We used 3,661 parsable
test input and their corresponding AST in our experiments.

Extracting Feature Vectors In this step, we focus on count-
ing the number of occurrences of each of the 28 features
in our test suite. To do this, we use a combination pattern

matching in regular expressions in the text of test
inputs
and simple visiting of the abstract syntax tree. We extract
all occurrences of features in test inputs. We next count the
number of occurrences for each feature in each C program
or corresponding AST ﬁle. Figure 3 shows the number of
test input that contains each feature. We observe that the
distribution of features in failing test input is not uniform.
Features like global variables and compound assignments have
occurred more frequently than features like int 8.

Compute Centroids Given a set of feature vectors that
represents the presence or absence of each feature in the test
inputs, we use K-Means clustering on the feature vectors.
The K-Means clustering is an unsupervised machine learning
algorithm that performs clustering on unlabeled vector data.
Given a set of vector data, this algorithm observes the under-
lying patterns and cluster similar data together. The number
of clusters we want to see has to be predeﬁned. Each cluster
results in a centroid that has a minimum distance to the data
points of the cluster. Suppose, V is the vector data of n
observations and k is the number of disjoint clusters C. The
K-Means algorithm groups the n observations into k clusters
and each cluster has a centroid c, the mean of the samples
Vc in the cluster. The centroid c is set based on the minimum
distance dm of the inertia criterion. For K-Means, the distance
metric is the sum of squared distances within-cluster which is
deﬁned as:

dm =

n
(cid:88)

i=0

min
ci∈C,xj ∈Vci

(|ci − xj|2)

K-Means computes k centroids for a given k. At the end of
K-Means clustering, we have k clusters and k centroids where
are located at the center of each cluster. Since the feature
vectors contain only 0 and 1 values, the values in the centroids
would be a real value from 0 to 1 (inclusive).

B. Phase 2: Generating test input using K-CONFIG

Generating conﬁgurations in K-CONFIG Our realization of
K-CONFIG uses K-Means that results in a vector of real
values from 0 to 1. A closer value to 1 in a centroid, it means
that the corresponding feature was more prevalent in the test
inputs in that cluster. Therefore, we use those values as the
probability of including a feature in a test input. Algorithm 1
describes the algorithm for generating new test inputs. Given
a testing time budget timeBudget, a set of centroids CS,
the algorithm calls Conf igGen in round-robin fashion until
the test time budget expires. Procedure Conf igGen takes
a centroid C ∈ CS and generates a new conﬁguration.
In generating a new conﬁguration, Conf igGen chooses to
include feature fi with a random probability ci where fi is
represented by the element ci in the centroid vector.

Differential testing to evaluate test input We use a meta-
morphic relation between optimization levels of compilers.
In particular, we compile a test input with two optimization
levels: O0 and O3 and we compare the result of the execution
of the programs generated by those optimization levels.

Fig. 3. Number of test input for each feature.

Algorithm 1: K-CONFIG
timeBudget ← Testing time budget;
CS ← Set of centroids;
T S ← ∅;

while spentT ime ≤ timeBudget do

forall centroid C ∈ CS do

conf ig ← Conf igGen(C)
testInput ← Csmith(conf ig)
if doesFail(testInput,GCC) then
T S ← T S ∪ testInput

end

end

end

Function ConﬁgGen(C):

f eatures ← ∅
forall value v ∈ C do

randN um ← [0 : 1]
if randNum ≤ v then
f eatures.put(1)

else

f eatures.put(0)

end

end
return

III. EXPERIMENTAL SETUP

This section discusses the experimental parameters used to

evaluate the K-CONFIG approach.

Number of Clusters Choosing the number of cluster k is

Fig. 4. Visualization of feature vectors of failing test inputs in GCC.

key and hard. The best k ensures the similarity within the
clusters and dissimilarity between the clusters. But there are
no well-deﬁned methods to choose such a value of k. We
visualize our feature vectors in the projector [9] to see the
underlying clustering patterns. Figure 4 shows the projection
of our vector data. Here, the number of points is 3661 and the
dimension of each point is 28. That means each point is the
representation of a test input having a vector of 28 features.

We can see different cluster patterns in various aspects, that’s
why we come up with a decision to choose different k values,
where k = 1, 2, 4, 8, 16.

A. Initial Setting for Clusters

We consider the K-Means algorithm implementation of
python scikit-learn [22] machine learning library in this
paper. We use the KMeans API where we need to ﬁt the
vector data (the observations to the cluster) and have to pass
the n_clusters parameters (the number of centroids to
generate after forming the number of clusters). We also use
the default K-Means++ initialization method which selects
initial cluster centers for K-Means clustering in a smart way
to speed up the convergence. The inertia criterion for distance
metric is used is the sum of squared distances between centroid
and data points. After each iteration, the K-Means algorithm
minimizes the within-cluster sum of squared distances. We run
our algorithms with default n_int and max_iter option. As
a result, the K-Means algorithm runs 10 times with different
centroid seeds and continue for 300 iterations for each run.
The centroids are found at the last iteration of K-Means that
dumps the best output as the ﬁnal result.

Test Subject Compilers We use two mature versions of
GCC to evaluate the effectiveness of this approach: GCC 4.8.2
and GCC 5.4.0. GCC 4.8.2 was released in October of 2013
and GCC 5.4.0 was released three years after, in September
2016. Both releases are mature and have been widely used in
building various software systems.

System Hardware Our evaluation has been conducted on a
high performance computing cluster. The HPC Server is the
shared campus resource pool hosting a total of 5704 CPU cores
in 169 computes. The CPU type is Intel Xeon E5-2680v4 with
128GB shared main memory.

Test Generation Tool Csmith [24] is an open source auto-
matic test generation tool. Given a set of C language features
as options (by default enable), Csmith can generate random C
programs. We use Csmith 2.3.0 [7] in our approach.

Initial Test Suite We use the GCC regression bug test suite
that has more than 3000 parsable test C programs. This test
suite contains failure-inducing test inputs. We are interested in
mine the patterns of those failure-inducing test input to guide
the Csmith test input generation.

Test budget for testing campaign We run each conﬁguration
of Csmith for 13 hours to create test input and execute the test
input. We also experiment with two compilation time setup.
First, we use a 10 seconds timeout to compile a test input.
Then, we use a 30 seconds timeout to compile a test input. To
check the robustness of randomness, we run each experiment
three times.

B. Research Questions

In this study, we seek to answer the following research

questions.

• Research Question 1: Can the K-CONFIG ﬁnd more
failure-inducing test inputs compared to the state-of-the-
art approach?

TABLE I
POSSIBLE FAILURES IN THE EXPERIMENT

No Optimization (-O0)
Compiler crashes
Compiler crashes
Compiler doesn’t crash

High Optimization (-O3)
Compiler crashes
Compiler doesn’t crash
Compiler crashes

Output for -O0 and -O3 are identical
Output for -O0 and -O3 are different

Failure?
False
True
True
False
True

TABLE II
FAILURE TYPES

Failure Types
Miscompilation
Crash
Timeout

Deﬁnition
Compiler produces different output for no (-O0) and high (-O3) optimization.
Compiler crashes when compiling program.
Compiler takes longer time than the speciﬁed time to compile program.

• Research Question 2: What impact of choosing the dif-

ferent k have on K-CONFIG?

• Research Question 3: What are the common features in

the failure-inducing test inputs for GCC?

For RQ1, we seek to compare the Csmith with our setting
to the Csmith with the default setting in terms of the number
of failure-inducing test input for the crash, timeout, and mis-
compilation. RQ2 evaluates the impact of choosing different k
on the effectiveness of K-CONFIG. Finally, for RQ3, we aim
to ﬁnd the features that are culprits for the failures.

The remaining of this section discusses our evaluation of the
proposed K-CONFIG approach. We have been experimenting
on GCC.

C. Ground Truth

To ﬁnd the failure-inducing test inputs, we choose the result
of without optimization as ground truth. For example, for a
speciﬁc compiler version, we ﬁrst compiled a test input with
the lowest optimization (-O0). Then, we compiled the same
test input with the highest optimization (-O3) on the same
compiler version. Test oracles state that the behavior should be
the same in both trials. Any mismatch between the behaviors
represents a failure. There are other failures that are described
in Table I.

D. Failure Types

We have classiﬁed the failures into three classes: (1) mis-
compilation, (2) crash failure, and (3) timeout. Table
II
summarizes these failures. The miscompilation failures happen
wherein a compiler produces programs that output wrong out-
puts for different optimization. In a crash failure, the compiler
terminates the compilation abruptly with a crash report on
screen. The timeout failure happens when the compilation time
exceeds the predeﬁned threshold for compilation—we need to
set timeout due to avoid potential inﬁnite loop errors in the
compiler under test.

TABLE III
EXPERIMENT WITH GCC FOR DIFFERENT FEATURE SELECTION

Experiment ID
E1
E2
E3
E4
E5
E6

Feature Selection
Csmith default conﬁguration
Select k=1 centroid as features
Select k=2 centroids as features
Select k=4 centroids as features
Select k=8 centroids as features
Select k=16 centroids as features

Testing Window
13 hours
13 hours
13 hours
13 hours
13 hours
13 hours

E. Experiment parameters

Table III contains information about the experiments. We
have conducted six experiments to evaluate the effectiveness
of the K-CONFIG approach. In E1, we used Csmith with
the default conﬁguration. In E2 through E6 experiments,
instead of using the default setting of Csmith, we used the
different featured centroids as parameters for Csmith. We
chose different k values, where k = 1, 2, 4, 8, 16, respectively.
k = 1 is essentially setting the probability of including a
feature proportional to the number of times that is it has seen
in test inputs in the original test suite. In each experiment, for
13 hours, we generated, compiled test input, and executed the
output programs. We also ran each experiment three times to
avoid potential effects of randomness in the experiments.

IV. ANALYSIS OF RESULTS

Here,

This section presents the results of experiments to evaluate
the effectiveness of the K-CONFIG approach. Tables IV and VI
show the experiment results with GCC 4.8.2 for the compila-
tion timeout of 10 seconds and 30 seconds respectively. Table
V and VII show the experiment results with GCC 5.4.0 for the
compilation time of 10 seconds and 30 seconds respectively.
the ﬁrst column shows the type of experiments.
Column “Test input” shows the total number of test inputs
generated and executed in 13 hours period for a speciﬁc
experiment. “Crash(0)” shows the total number of crash fail-
ures that have been found in an experiment while compiling
a test input with the lowest optimization level (i.e., -O0),
“Crash(3)” demonstrates the number of crashes encountered
with the highest level of optimization (i.e., -O3). “Crash(both)”
column contains the number of test input that causes a crash
in both lowest (-O0) and highest (-O3) level of optimiza-
tion. “Total Crash” is the sum of “Crash(0)”, “Crash(3)”,
and “Crash(both)” in an experiment. Similarly, “Timeout(0)”,
“Timeout(3)”, and “Timeout(both)” represent the number of
timeouts encountered with lowest (-O0) optimization, highest
(-O3) optimization, and both the lowest (-O0) & highest (-
O3) optimization, respectively. “Total Timeout” is the sum of
“Timeout(0)”, “Timeout(3)”, and “Timeout(both)”. Note that
we run each experiment three times; (r1), (r2), and (r3) present
the result of individual experiments.

RQ1: Comparison of the conﬁguration of K-CONFIG with the
default conﬁguration of the test generator

In GCC 4.8.2, the conﬁguration of the K-CONFIG approach
could ﬁnd up to 179 test input for crashes and 36 test input for

miscompilation failures. But Csmith with the default conﬁg-
uration could not ﬁnd any failure-inducing test input. On the
other hand, in GCC 5.4.0, K-CONFIG suggests conﬁgurations
that could ﬁnd up to 53 test input with miscompilations and
no test input for crashes. Again Csmith with the default setting
could not ﬁnd any failure-inducing test input.
(cid:19)

(cid:16)

Observation 1: Csmith with K-CONFIG conﬁgurations
could ﬁnd more failure-inducing test inputs than Csmith
with the default conﬁguration.

(cid:18)

(cid:17)

RQ2: What impact of different k effectiveness of K-CONFIG

We experiment with a different selection of k in our
approach. Table IV-VII summarizes the results of those selec-
tions. When Csmith with default selection could not ﬁnd any
failure-inducing test input, Csmith with k = 1 setting even
could ﬁnd many failure-inducing test input. k = 1 is similar
to set the probability of conﬁgurations based on the ratio of
the number of test inputs per feature. Whatever value k takes
we could see the robustness of ﬁnding failure-inducing test
input in each experiment. Therefore, choosing the k value has
no major impact on K-CONFIG.
(cid:11)

(cid:8)

Observation 2: Different values for k did not impact the
effectiveness of K-CONFIG.

(cid:10)

(cid:9)

RQ3: Common features in the failure-inducing test inputs for
GCC

To get rid of bias for centroids in the experiment, we run
round-robin selection of centroids in test input generation.
Figure 3 shows the quantitative count of each feature in the
test input. We could observe that there is no missing feature
in the test suite, the feature exists in at least 5 to 2504 test
input, which supports the goodness of our test suite. Similarly,
Figure 5 shows the quantitative count of each feature in cen-
troids. We could observe that “global-variables, compound-
assignment, jumps, comma-operators, pointers, builtins” fea-
tures appear exceedingly in the failure-inducing test inputs,
and “arrays, post-incr-operator, structs, pre-incr-operator,
unary-plus-operator, consts, const-pointers, muls” features ap-
pear occasionally in the failure-inducing test inputs. On the
other hand, “argc, bitﬁelds, divs, pre-decr-operator, post-decr-
operator, longlong, int8, uint8, ﬂoat, inline-function, packed-
struct, unions, volatiles, volatile-pointers” features appear
rarely in the failure-inducing test inputs.
(cid:19)

(cid:16)

Observation 3: Table VIII shows the number of occur-
rences of the common features in the failure-inducing test
inputs.

(cid:18)

(cid:17)

V. RELATED WORK

Finding bugs in compilers is an active area of research.
Several approaches have been proposed to generate test in-
inputs, se-
puts, reduce test size, rank and prioritize test
lect/omit/mutate features, diverse test inputs, accelerate testing,
categorize similar bugs, etc. The goals of those approaches
are to reveal bugs in compiler and facilitate debugging. This

TABLE IV
GCC 4.8.2 (TIMEOUT=10S)

Experiment ID
E1 (r1)
E1 (r2)
E1 (r3)
E2 (r1)
E2 (r2)
E2 (r3)
E3 (r1)
E3 (r2)
E3 (r3)
E4 (r1)
E4 (r2)
E4 (r3)
E5 (r1)
E5 (r2)
E5 (r3)
E6 (r1)
E6 (r2)
E6 (r3)

Test input
9225
10037
9075
10351
11524
10122
10577
10952
10262
11325
11835
10897
10411
10777
9717
11024
11449
10754

Crash(0)
0
0
0
54
63
66
52
49
56
48
62
49
49
48
38
60
61
50

Crash(3)
0
0
0
83
102
95
71
76
70
62
79
80
73
83
69
115
103
94

Crash(both)
0
0
0
7
13
6
7
5
5
3
3
7
10
4
4
4
6
10

Total Crash
0
0
0
144
178
167
130
130
131
113
144
136
132
135
111
179
170
154

Timeout(0)
8
10
13
21
29
28
20
17
24
19
22
24
24
24
16
32
28
28

Timeout(3)
0
0
0
12
18
18
17
8
12
11
16
11
13
10
12
12
21
12

Timeout(both)
1187
1226
1164
1545
1609
1509
1542
1669
1512
1531
1634
1499
1596
1714
1586
1535
1649
1496

Total Timeout Miscompilation

1195
1236
1177
1578
1656
1555
1579
1694
1548
1561
1672
1534
1633
1748
1614
1579
1698
1536

0
0
0
26
25
24
31
22
20
24
28
26
23
26
21
30
29
36

TABLE V
GCC 5.4.0 (TIMEOUT=10S)

Experiment ID
E1 (r1)
E1 (r2)
E1 (r3)
E2 (r1)
E2 (r2)
E2 (r3)
E3 (r1)
E3 (r2)
E3 (r3)
E4 (r1)
E4 (r2)
E4 (r3)
E5 (r1)
E5 (r2)
E5 (r3)
E6 (r1)
E6 (r2)
E6 (r3)

Test input
8285
8722
8225
9901
10637
9565
10044
10407
9768
10224
10895
9633
9541
10020
9413
10306
10588
9915

Crash(0)
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Crash(3)
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Crash(both)
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Total Crash
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Timeout(0)
8
9
11
5
5
3
5
8
4
7
4
3
6
5
5
4
5
6

Timeout(3)
0
0
0
1
1
0
1
1
3
0
2
1
0
0
1
2
3
1

Timeout(both)
1059
1137
1010
1471
1560
1438
1481
1591
1443
1449
1525
1430
1514
1624
1478
1436
1544
1399

Total Timeout Miscompilation

1067
1146
1021
1477
1566
1441
1487
1600
1450
1456
1531
1434
1520
1629
1484
1442
1552
1406

0
0
0
41
51
42
38
39
32
42
28
30
45
35
30
53
47
37

TABLE VI
GCC 4.8.2 (TIMEOUT=30S)

Experiment ID
E1 (r1)
E1 (r2)
E1 (r3)
E2 (r1)
E2 (r2)
E2 (r3)
E3 (r1)
E3 (r2)
E3 (r3)
E4 (r1)
E4 (r2)
E4 (r3)
E5 (r1)
E5 (r2)
E5 (r3)
E6 (r1)
E6 (r2)
E6 (r3)

Test input
4400
4832
4316
4516
4637
4361
4252
4550
4221
4786
4944
4406
4456
4370
4232
4372
5140
4219

Crash(0)
0
0
0
19
25
32
29
23
19
19
26
19
19
16
25
26
19
21

Crash(3)
0
0
0
38
49
45
26
37
26
30
26
34
37
27
33
39
49
34

Crash(both)
0
0
0
3
2
1
4
1
3
3
1
4
2
2
2
3
0
3

Total Crash
0
0
0
60
76
78
59
61
48
52
53
57
58
45
60
68
68
58

Timeout(0)
3
4
6
9
11
17
4
6
8
7
12
10
15
5
5
16
15
13

Timeout(3)
0
0
0
4
2
6
4
4
3
5
6
4
4
5
4
3
4
7

Timeout(both)
552
609
540
620
685
606
633
688
618
616
677
607
627
695
623
618
674
611

Total Timeout Miscompilation

555
613
546
633
698
629
641
698
629
628
695
621
646
705
632
637
693
631

0
0
0
7
12
12
12
12
12
13
18
13
10
11
15
12
21
15

TABLE VII
GCC 5.4.0 (TIMEOUT=30S)

Experiment ID
E1 (r1)
E1 (r2)
E1 (r3)
E2 (r1)
E2 (r2)
E2 (r3)
E3 (r1)
E3 (r2)
E3 (r3)
E4 (r1)
E4 (r2)
E4 (r3)
E5 (r1)
E5 (r2)
E5 (r3)
E6 (r1)
E6 (r2)
E6 (r3)

Test input
4110
4587
4270
4189
4648
4062
4132
4385
4273
4333
4603
4420
3949
4090
4114
4198
4707
4003

Crash(0)
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Crash(3)
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Crash(both)
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Total Crash
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Timeout(0)
5
2
5
1
4
1
0
2
0
2
1
0
1
1
4
0
2
1

Timeout(3)
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Timeout(both)
515
573
505
614
667
609
615
676
597
605
669
594
616
683
602
615
663
599

Total Timeout Miscompilation

520
575
510
615
671
610
615
678
597
607
670
594
617
684
606
615
665
600

0
0
0
15
15
5
23
14
10
16
16
26
17
11
19
25
19
21

Fig. 5. Frequency of features in centroids.

TABLE VIII
RANKING OF FEATURES BASED ON THEIR FREQUENCY IN FAILURE
INDUCING TEST INPUTS

Frequency of features
Very frequent

Occasionally

Rarely

Feature list
“global-variables, compound-assignment, jumps, comma-operators,
pointers, builtins”
“arrays, post-incr-operator, structs, pre-incr-operator, unary-plus-operator,
consts, const-pointers, muls”
“argc, bitﬁelds, divs, pre-decr-operator, post-decr-operator, longlong, int8,
uint8, ﬂoat, inline-function, packed-struct, unions, volatiles, volatile-pointers”

section highlights the literature of compiler testing related
to our proposed approach. The summary of related works is
shown in Table IX.

A. Clustering-based approaches

Cluster ﬁltering approaches have been used in the compiler
testing for classifying test inputs, triggered bugs, and failure
reports. This classiﬁcation can help researchers to pick test
programs from the different cluster in order to increase the
diversity of program as well the testing acceleration as testing
from the same cluster is likely to observe the same facts.
Several approaches for cluster ﬁltering have been proposed; for
example, [18], [19], [17], [2]. These approaches either cluster
based on the proﬁle execution ([18], [19]) or ﬁlter based on
the information ﬂow ([17], [2]). Filtering and Prioritizing [18]
partitions a set of test cases into separate groups according
to the proﬁle similarity on execution space. The authors then
use the one-per-cluster sampling to select one test program
randomly from each cluster. If any bug found, the authors

TABLE IX
SUMMARY OF RELATED WORKS

Author and Reference
Leon et. al. [18]
Podgurski et. al. [19]
Leon et. al. [17]
Holler et. al. [14]
Groce et. al. [13]
Chen et. al. [5]
Zhang et. al. [26]
Chen et. al. [4]
Chen et. al. [2]

Publication Year
ISSRE 2003
ICSE 2003
ICSE 2005
USENIX 2012
ISSTA 2012
PLDI 2013
PLDI 2017
ICSE 2017
IEEE TSE 2018

Approach(s)
Filtering and prioritizing
Classiﬁcation and visualization
Filtering with cif
Fuzzing code fragments
Feature omission
Ranking test inputs
Skeletal program enumeration
Learning and scheduling
Coverage prediction and clustering

Test Compiler/Engine
GCC, Jikes and javac
GCC and Jacks
javac, Xerces and JTidy
Mozilla TraceMonkey, Google V8 and PHP
LLVM/Clang and GCC
SpiderMonkey and GCC
GCC/Clang, CompCert, Dotty and Scala
GCC and LLVM
GCC and LLVM

use the failure-pursuit sampling to select k nearest neighbors
of the failure-inducing program. This process is continued
until no more bugs are found. Another work in this area
is classiﬁcation and multivariate visualization [19] where the
failure-inducing inputs have been grouped together based on
the proﬁle execution space. The classiﬁcation approach has
four phases: (1) capturing the execution proﬁle in the ﬁrst
phase, (2) extracting proﬁle features, (3) grouping similar fail-
ures using cluster analysis and multivariate visualization, and
(4) explore the results in order to conﬁrm or reﬁne. Filtering
with complex information ﬂows [17] is another proﬁle-based
test case ﬁltering approach where both coverage-based and
proﬁle-distribution-based ﬁltering approaches are considered.
Another example is COP [2] where authors prioritize test
inputs by clustering them according to the predicted coverage
information. The evaluation result of all
those approaches
demonstrates that the cluster ﬁlter approach is effective and
ﬁnds many defects along with maximizing the coverage.

B. Feature-based Testing

Feature selection leads a program generator to generate
diverse test programs that explore the various area of space.
These diverse test inputs can increase code coverage and ﬁnd
hidden bugs. Several feature-based analysis have been pro-
posed; for example, [13], [26], [14], [12]. Swarm testing [13]
randomly chooses a subset of features available to generate
new test cases. The generated test cases are very diverse and
the evaluation result shows that this approach outperforms
Csmith’s default conﬁguration in both code coverage and crash
bug ﬁnding. Another notable work is SPE [26] where authors
enumerate a set of programs with different variable usage
patterns. The generated diverse test cases exploit different
optimization and the evaluation result shows that the skeletal
program enumeration has conﬁrmed bugs in all tested com-
pilers. Two more related studies in this area are LangFuzz
[14] and Learn&Fuzz [12]. LangFuzz approach extracts code
fragments from a given code sample that triggered past bugs
and then apply random mutation within a pool of fragments
to generate test inputs. On the other hand, the Learn&Fuzz
approach uses the generative learned char-RNN model
to
generate new test objects for the experiment.

C. Accelerate Testing

Another important fact behind the diverse test input is to
accelerate the testing. Running a larger set of test
inputs
will take a long period of time to ﬁnd compiler bugs, and
repeatedly testing similar test inputs will result in the same
compiler bugs. Several approaches for testing acceleration
have been performed; for example, [5], [4], [3], [2]. One
novel approach is taming [5] where authors order the test
inputs in such a way that diverse test inputs are highly ranked.
They ﬁrst deﬁne distance metrics between test cases and then
rank test cases in the furthest point ﬁrst order. The evaluation
result shows that the ranking approach speeds up the bug
ﬁnding in both the number of test inputs and testing time.
Another interesting approach is LET [4] where authors use
a learning model to schedule the test inputs. This learning-
to-test approach has two steps: learning and scheduling. In
learning steps, LET extracts a set of features from the past
bug triggering test cases and then trains a capability model
to predict the bug triggering probability of the test programs,
and trains another time model to predict the execution time of
the test programs. In scheduling steps, LET ranks the target
test programs based on the probability of bug triggering in unit
time. The evaluation result shows that the scheduled test inputs
signiﬁcantly accelerate compiler testing. Another example in
this area is COP [2] where authors predict
the coverage
information of compilers for test inputs and prioritize test
inputs by clustering them according to the predicted coverage
information. The result shows that COP signiﬁcantly speeds
up the test acceleration and outperforms the state-of-the-art
acceleration approaches.

VI. THREATS TO VALIDITY

There are various factors that may impact the validity of our
results. First, our initial test suite of failing test inputs is not
representative of all bugs in the GCC compiler. It has been
extracted from the bug reports available online. Some bugs
might have been reported using different mechanisms such as
email to developers. There are many more dormant bugs that
yet to be found. Therefore, it would not be a representative of
all bugs. However, using it in the K-CONFIG approach could
guide Csmith to trigger several failures.

Second, we only looked at the absolute number of failures.
We did not check to see if they represent distinct bugs because
determining the number of distinct faults is a non-trivial task.
Third, K-CONFIG approach divides observations (initial
failing test inputs) into k clusters that are similar within-cluster
but dissimilar between-cluster. In data with high dimensions,
the value of k is important. The wrong choice of k can push
clustering to include dissimilar observations or to exclude sim-
ilar ones. Unfortunately, there is no well-established method
to reach to the right value for k. Developers mostly suggest
to a fail and trial approach with multiple k.

Another concern in this paper is that we only did ex-
periments with only two versions of GCC. As a result, the
observed failing test input may be ﬁxed or passed by a newer
version of GCC. We run each script for 13 hours and evaluated
the test inputs in this time window. The number of test inputs
and the allocated time may also impact on the evaluation
result. Also, the different compilation timeout may affect the
number in the result.

VII. CONCLUSION

Compilers are key software tools for developers to build
software. Compiler testing is necessary to ensure the correct-
ness of compiler. In this paper, we have proposed K-CONFIG
to create a conﬁguration for test generators by processing
existing test inputs. We experimented this approach on two
versions of GCC compilers and found that the conﬁguration
suggested by K-CONFIG could trigger several crashes and
miscompilation failures in two stable versions of GCC.

REFERENCES

[1] M. A. Alipour, A. Groce, R. Gopinath, and A. Christi, “Generating
focused random tests using directed swarm testing,” in Proceedings of
the 25th International Symposium on Software Testing and Analysis,
ser. ISSTA 2016. New York, NY, USA: ACM, 2016, pp. 70–81.
[Online]. Available: http://doi.acm.org/10.1145/2931037.2931056
[2] J. Chen, G. Wang, D. Hao, Y. Xiong, H. Zhang, L. Zhang, and
B. XIE, “Coverage prediction for accelerating compiler testing,” IEEE
Transactions on Software Engineering, pp. 1–1, 2018.

[3] J. Chen, “Learning to accelerate compiler testing,” in Proceedings of the
40th International Conference on Software Engineering: Companion
Proceeedings, ser. ICSE ’18. New York, NY, USA: ACM, 2018,
pp. 472–475. [Online]. Available: http://doi.acm.org/10.1145/3183440.
3183456

[4] J. Chen, Y. Bai, D. Hao, Y. Xiong, H. Zhang, and B. Xie, “Learning
to prioritize test programs for compiler testing,” in Proceedings of
the 39th International Conference on Software Engineering, ser. ICSE
’17. Piscataway, NJ, USA: IEEE Press, 2017, pp. 700–711. [Online].
Available: https://doi.org/10.1109/ICSE.2017.70

[5] Y. Chen, A. Groce, C. Zhang, W.-K. Wong, X. Fern, E. Eide, and
J. Regehr, “Taming compiler fuzzers,” in Proceedings of
the 34th
ACM SIGPLAN Conference on Programming Language Design and
Implementation, ser. PLDI ’13. New York, NY, USA: ACM, 2013,
pp. 197–208. [Online]. Available: http://doi.acm.org/10.1145/2491956.
2462173

[6] Y. Chen, T. Su, C. Sun, Z. Su, and J. Zhao, “Coverage-directed
differential testing of jvm implementations,” in Proceedings of the 37th
ACM SIGPLAN Conference on Programming Language Design and
Implementation, ser. PLDI ’16. New York, NY, USA: ACM, 2016,
pp. 85–99.
[Online]. Available: http://doi.acm.org/10.1145/2908080.
2908095

[7] Csmith, https://embed.cs.utah.edu/csmith/.

[8] C. Cummins, P. Petoumenos, A. Murray, and H. Leather, “Compiler
fuzzing through deep learning,” in Proceedings of
the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis,
ser. ISSTA 2018. New York, NY, USA: ACM, 2018, pp. 95–105.
[Online]. Available: http://doi.acm.org/10.1145/3213846.3213848

[9] Embedding-Projector, https://projector.tensorﬂow.org/.
[10] GCC-LOC,
summary/.

https://www.openhub.net/p/gcc/analyses/latest/languages

[11] Go-Fuzz, https://github.com/dvyukov/go-fuzz/.
[12] P. Godefroid, H. Peleg, and R. Singh, “Learn&fuzz: Machine learning
for input fuzzing,” in Proceedings of the 32Nd IEEE/ACM International
Conference on Automated Software Engineering, ser. ASE 2017.
Piscataway, NJ, USA:
[Online].
IEEE Press, 2017, pp. 50–59.
Available: http://dl.acm.org/citation.cfm?id=3155562.3155573

[13] A. Groce, C. Zhang, E. Eide, Y. Chen, and J. Regehr, “Swarm
the 2012 International Symposium on
ISSTA 2012. New York,
[Online]. Available: http:

testing,” in Proceedings of
Software Testing and Analysis,
NY, USA: ACM, 2012, pp. 78–88.
//doi.acm.org/10.1145/2338965.2336763

ser.

[14] C. Holler, K. Herzig, and A. Zeller, “Fuzzing with code fragments,” in
Proceedings of the 21st USENIX Conference on Security Symposium,
ser. Security’12. Berkeley, CA, USA: USENIX Association, 2012, pp.
38–38. [Online]. Available: http://dl.acm.org/citation.cfm?id=2362793.
2362831

[15] V. Le, M. Afshari, and Z. Su, “Compiler validation via equivalence
modulo inputs,” in Proceedings of the 35th ACM SIGPLAN Conference
on Programming Language Design and Implementation, ser. PLDI ’14.
New York, NY, USA: ACM, 2014, pp. 216–226. [Online]. Available:
http://doi.acm.org/10.1145/2594291.2594334

[16] V. Le, C. Sun, and Z. Su, “Finding deep compiler bugs via guided
stochastic program mutation,” in Proceedings of
the 2015 ACM
SIGPLAN International Conference on Object-Oriented Programming,
Systems, Languages, and Applications, ser. OOPSLA 2015. New
York, NY, USA: ACM, 2015, pp. 386–399.
[Online]. Available:
http://doi.acm.org/10.1145/2814270.2814319

[17] D. Leon, W. Masri, and A. Podgurski, “An empirical evaluation of
test case ﬁltering techniques based on exercising complex information
ﬂows,” in Proceedings. 27th International Conference on Software
Engineering, 2005. ICSE 2005., May 2005, pp. 412–421.

[18] D. Leon and A. Podgurski, “A comparison of coverage-based
and distribution-based techniques for ﬁltering and prioritizing test
cases,” in Proceedings of
the 14th International Symposium on
Software Reliability Engineering, ser. ISSRE ’03. Washington, DC,
USA: IEEE Computer Society, 2003, pp. 442–. [Online]. Available:
http://dl.acm.org/citation.cfm?id=951952.952367

[19] A. Podgurski, D. Leon, P. Francis, W. Masri, M. Minch, and and,
“Automated support for classifying software failure reports,” in 25th
International Conference on Software Engineering, 2003. Proceedings.,
May 2003, pp. 465–475.

[20] pycparser, https://github.com/eliben/pycparser/.
[21] J. Ruderman, “Introducing jsfunfuzz,” https://www.squarefree.com/

2007/08/02/introducing-jsfunfuzz/, 2007.

[22] Scikit-Learn, https://github.com/scikit-learn/scikit-learn/.
[23] C. Sun, V. Le, and Z. Su, “Finding compiler bugs via live code mutation,”
in Proceedings of the 2016 ACM SIGPLAN International Conference on
Object-Oriented Programming, Systems, Languages, and Applications,
ser. OOPSLA 2016. New York, NY, USA: ACM, 2016, pp. 849–863.
[Online]. Available: http://doi.acm.org/10.1145/2983990.2984038
[24] X. Yang, Y. Chen, E. Eide, and J. Regehr, “Finding and understanding
the 32Nd ACM SIGPLAN
bugs in c compilers,” in Proceedings of
Conference on Programming Language Design and Implementation,
ser. PLDI ’11. New York, NY, USA: ACM, 2011, pp. 283–294.
[Online]. Available: http://doi.acm.org/10.1145/1993498.1993532
[25] Z. Yin, D. Yuan, Y. Zhou, S. Pasupathy, and L. Bairavasundaram, “How
do ﬁxes become bugs?” in Proceedings of the 19th ACM SIGSOFT
Symposium and the 13th European Conference on Foundations of
Software Engineering, ser. ESEC/FSE ’11. New York, NY, USA:
ACM, 2011, pp. 26–36. [Online]. Available: http://doi.acm.org/10.1145/
2025113.2025121

[26] Q. Zhang, C. Sun, and Z. Su, “Skeletal program enumeration for
rigorous compiler testing,” in Proceedings of the 38th ACM SIGPLAN
Conference on Programming Language Design and Implementation,
ser. PLDI 2017. New York, NY, USA: ACM, 2017, pp. 347–361.
[Online]. Available: http://doi.acm.org/10.1145/3062341.3062379

