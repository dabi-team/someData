Combining Rules and Embeddings via Neuro-Symbolic AI for Knowledge Base
Completion

Prithviraj Sen, Breno W. S. R. de Carvalho, Ibrahim Abdelaziz, Pavan Kapanipathi,
Francois Luus, Salim Roukos, Alexander Gray

IBM Research

1
2
0
2

p
e
S
6
1

]
I

A
.
s
c
[

1
v
6
6
5
9
0
.
9
0
1
2
:
v
i
X
r
a

Abstract

R(U, V ) ← ?(U, W ) ∧ ?(W, V )

Recent interest in Knowledge Base Completion (KBC) has
led to a plethora of approaches based on reinforcement learn-
ing, inductive logic programming and graph embeddings. In
particular, rule-based KBC has led to interpretable rules while
being comparable in performance with graph embeddings.
Even within rule-based KBC, there exist different approaches
that lead to rules of varying quality and previous work has not
always been precise in highlighting these differences. Another
issue that plagues most rule-based KBC is the non-uniformity
of relation paths: some relation sequences occur in very few
paths while others appear very frequently. In this paper, we
show that not all rule-based KBC models are the same and
propose two distinct approaches that learn in one case: 1) a
mixture of relations and the other 2) a mixture of paths. When
implemented on top of neuro-symbolic AI, which learns rules
by extending Boolean logic to real-valued logic, the latter
model leads to superior KBC accuracy outperforming state-of-
the-art rule-based KBC by 2-10% in terms of mean reciprocal
rank. Furthermore, to address the non-uniformity of relation
paths, we combine rule-based KBC with graph embeddings
thus improving our results even further and achieving the best
of both worlds.

1

Introduction

A number of approaches have been proposed for knowledge
base completion (KBC), a popular task that addresses the
inherent incompleteness of Knowledge Graphs (KG) (Bol-
lacker et al. 2008; Rebele et al. 2016). Compared to
embeddings-based techniques (Sun et al. 2019a; Lacroix,
Usunier, and Obozinski 2018), rule learning techniques for
KBC can handle cold-start challenges for new entities whose
embeddings are not available (inductive setting) (Yang, Yang,
and Cohen 2017a; Sadeghian et al. 2019a; Das et al. 2018),
and learn multi-hop, human-interpretable, ﬁrst order logic
rules that enables complex reasoning over KGs. For example,
the following rule (from Freebase) infers a person’s national-
ity given her/his place of birth and its corresponding country:

∀P, N ∃L: nationality(P, N ) ← bornIn(P, L)∧ partOf(L, N )

There exist two kinds of prevalent rule learning approaches
based on their mechanism to select relations. The ﬁrst ap-
proach, denoted Chain of Mixtures (CM), represents each
relation hop in the body of the rule, towards the right of
the implication ← symbol, as a mixture of relations. With

R1 R2 . . . Rn R1 R2 . . . Rn

(a) Chain of Mixtures

R(U, V ) ← ??(U, V )
. . .
Ri(U, W ) ∧ Rj (W, V )
. . .
(b) Mixture of Paths

Figure 1: Rule-based KBC approaches: (a) CM, (b) MP.

close ties to NeuralLP (Yang, Yang, and Cohen 2017a),
DRUM (Sadeghian et al. 2019a), CM is depicted in Fig-
ure 1 (a) where we learn a rule with two relations in the body.
The second approach, denoted Mixture of Paths (MP), learns
relation paths with close ties to MINERVA (Das et al. 2018),
RNNLogic (Qu et al. 2020). Figure 1 (b) depicts MP where
the body of the rule is deﬁned as a mixture of all possible
length 2 relation paths in the KG. Crucially, both approaches,
which rely on recurrent neural networks (RNN), use increas-
ingly complex training algorithms. More precisely, NeuralLP
and DRUM generate mixture probabilities using a long short-
term memory (LSTM) (Hochreiter and Schmidhuber 1997)
and bi-directional LSTM, respectively. RNNLogic, the latest
in the line of MP-based works, samples sets of rules from a
multinomial distribution deﬁned by its RNN’s latent vectors
and learning this RNN requires sampling from intractable
posteriors. While RNN training has improved signiﬁcantly
(Pascanu, Mikolov, and Bengio 2013), we are still faced with
issues when these are used to learn long-range dependencies
(Trinh et al. 2018). Sufﬁces to say that it is unclear whether
the current RNN-based rule-based KBC models are in their
simplest form.

Instead of following the current trend of devising increas-
ingly complex rule-based KBC approaches, we ask whether it
is possible to devise an approach without resorting to RNNs?
To this end, we propose to utilize Logical Neural Networks
(LNN) (Riegel et al. 2020), a member of Neuro-Symbolic AI
family (NeSy), to devise both a CM-based (LNN-CM) and
a MP-based approach (LNN-MP). LNN is an extension of
Boolean logic to the real-valued domain and is differentiable
thus enabling us to learn rules end-to-end via gradient-based
optimization. Another advantage of LNN is that while other
members of NeSy harbor tenuous connections to Boolean
logic’s semantics, e.g., Dong et al. (2019), LNN shares strong
ties thus making the learned rules fully interpretable. More

 
 
 
 
 
 
importantly, our RNN-less approach is conceptually simpler
to understand and reason about than recent rule-based KBC
approaches while still performing at par, if not better.

One shortcoming of MP (path-based rule learning) is that
it suffers from sparsity. Since not all relation paths are equally
prevalent in the KG, our goal is to guide the learning process
towards more prevalent, effective relation paths as opposed
to paths that appear less frequently. We show how to use pre-
trained knowledge graph embeddings (KGE) to overcome
such issues and learn more effective rules. While RNNLogic
(Qu et al. 2021) scores paths using RotatE (Sun et al. 2019a),
a speciﬁc KGE, we show how to use any of the vast array of
KGEs available in the literature. Our contributions are:

• We propose to implement with logical neural networks (LNN)
two simple approaches for rule-based KBC. LNN is a dif-
ferentiable framework for real-valued logic that has strong
connections to Boolean logic semantics.

• To address the non-uniform distribution of relation paths that
is likely to exist in any KG, we propose a simple approach
that combines rule-based KBC with any KGE in order to reap
the complementary beneﬁts of both worlds.

• Our experiments on 4 different KBC benchmarks show that
we: (a) outperform other rule-based baselines by 2%-10%
(mean reciprocal rank); (b) are comparable to state-of-the-art
rule with embedding-based technique RNNLogic.

2 Preliminaries: Logical Neural Networks
We begin with a brief overview of Logical Neural Networks
(LNN) (Riegel et al. 2020), a differentiable extension of
Boolean logic that can learn rules end-to-end while still main-
taining strong connections to Boolean logic semantics. In par-
ticular, LNN extends propositional Boolean operators with
learnable parameters that allows a better ﬁt to data. We next
review operators such as LNN-∧, useful for modeling ∧ in
the example rule (Section 1), and LNN-pred, useful for ex-
pressing mixtures used in both CM and MP (Figure 1).

2.1 Propositional LNN Operators
To address the non-differentiability of classical Boolean logic,
previous work has resorted to t-norms from fuzzy logic which
are differentiable but lack parameters and thus cannot adapt to
data. For instance, NeuralLP (Yang, Yang, and Cohen 2017a)
uses product t-norm deﬁned as x∧y ≡ xy instead of Boolean
conjunction. In contrast, LNN conjunction includes param-
eters that can not only express conjunctive semantics over
real-valued logic but also ﬁt the data better. The following
LNN-∧ operator extends the Łukasiewicz t-norm, deﬁned as
x ∧ y ≡ max(0, x + y − 1), with parameters:

LNN-∧(x; β, w) ≡ max[0, min{1, β − w(cid:62)(1 − x)}]

subject to:

β1 − αw ≤ (1 − α)1
β − (1 − α)1(cid:62)w ≥ α
w ≥ 0

(1)
(2)

where w denotes weights and β denotes bias, both learnable
parameters, x denotes a vector of inputs (in the case of binary
conjunction |x| = 2), 0 (1) denote a vector of 0s (1s), and

(a) LNN conjunction

(b) LNN disjunction

Figure 2: (a) LNN-∧ and (b) LNN-∨ learned with α = 0.7.

α denotes a hyperparameter. Recall that, a (binary) Boolean
conjunction operator returns true (1) if both inputs are
true (1) and false (0) otherwise. Intuitively, output is
“high” if both inputs are “high”, otherwise the output is “low”.
There are 4 steps to understanding how LNN extends this
behavior to real-valued logic. 1) Given variable x whose truth
value lies in the range [0, 1], LNN utilizes α > 1
2 to deﬁne
“low” as x ∈ [0, 1 − α] and “high” as x ∈ [α, 1]. 2) Note
that, the non-negativity constraint enforced on w in LNN-∧
ensures that it is a monotonically increasing function with
respect to input x. 3) Given its monotonicity, we can ensure
that LNN-∧ returns a “high” value when all entries in x are
“high” by simply constraining the output to be “high” when
x = α1 (Equation 2). 4) Similarly, to ensure that the output
is “low” if any input entry is “low”, i.e. 1 − α, Equation 1
constrains the output to be low when all but one entries
in x is 1. This constrained formulation of LNN-∧ ensures
that conjunctive semantics is never lost during the learning
process while still providing a better ﬁt.

Figure 2 (a) shows a learned (binary) LNN-∧ (α = 0.7).
Notice how, the output (z-axis) is close to 0 when either input
x or y is “low” (∈ [0, 0.3]) and jumps to 1 when both are
“high” (∈ [0.7, 1]), thus precisely capturing the semantics of
logical conjunction. Note that, max and min have subgra-
dients available thus making LNN-∧ amenable to training
via gradient-based optimization. Just to contrast with LNN
disjunction (∨) that is deﬁned as 1 − LNN- ∧ (1 − x; β, w),
Figure 2 (b) shows a learned LNN-∨ (α = 0.7). In con-
trast to LNN-∧, in this case, output is “low” when both
x, y ∈ [0, 0.3] and jumps to 1 when either of them are “high”
(∈ [0.7, 1]) thus capturing semantics of Boolean disjunction.
Another operator we will ﬁnd useful is LNN-pred. In the
next section, we show how to use this to express mixtures,
either over relations (CM) or relation paths (MP). LNN-pred
is a simple operator with one non-negative weight per input:

LNN-pred(x; w) = w(cid:62)x subject to w ≥ 0, w(cid:62)1 = 1

where x denotes (a vector of) inputs and w denotes non-
negative, learnable parameters constrained to sum to 1.

2.2 Training LNNs
While LNN operators are amenable to gradient-based learn-
ing, one complication we are yet to address is implementing
constrained optimization to learn LNN parameters. Fortu-
nately, there exist approaches that can convert any system
of linear constraints (including equalities and inequalities)
into a sequence of differentiable operations such that we
can sample parameters directly from the feasible set (Frerix,

 0 0.4 0.7 1 0 0.4 0.7 1 0 0.4 0.7 1xy 0 0.4 0.7 1 0 0.4 0.7 1 0 0.4 0.7 1 0 0.4 0.7 1xy 0 0.4 0.7 1Nießner, and Cremers 2020) which is what we use to train all
our LNNs. We also refer the interested reader to Riegel et al.
(2020) that describes additional LNN training algorithms.

3 Learning LNN Chain Rules for KBC

3.1 Notation and Problem Deﬁnition
Let G = (cid:104)V, R, E(cid:105) denote a knowledge graph (KG) such that
edge e ∈ E comprises a triple (cid:104)h, r, t(cid:105) where h, t ∈ V denote
source and destination vertices, and r ∈ R denotes a relation.
In this work, we focus on predicting destinations, i.e., given
query (cid:104)h, r, ?(cid:105) predict the answer t. Following previous work
(Yang, Yang, and Cohen 2017a), we learn chain rules in ﬁrst-
order logic (also called open path rules) that chains together
multiple relations from R to model the given relation:

∀X0, Xm ∃X1, . . . Xm−1 :
r0(X0, Xm) ← r1(X0, X1) ∧ . . . ∧ rm(Xm−1, Xm)

ri ∈ R ∀i = 0, . . . m (3)

where the head r0 denotes the relation being modeled,
r1, . . . rm form the relations in the body, and X0, . . . Xm
denote logical constants that can take values from V. Note
that, relations can repeat within the body, i.e., ri = rj, i (cid:54)= j.
Furthermore, we allow r0 to appear within the body which
leads to a recursive rule. Chain rules are closely connected
to multi-hop paths. Essentially, the above rule claims that
(cid:104)u, r0, v(cid:105) exists if there exists an m-length path p = u r1−→
ri−→ . . . rm−→ v connecting X0 = u and Xm = v. Given
. . .
such a multi-hop path p, we refer to the sequence of rela-
tions r1, . . . rm as its relation path. Furthermore, given an
m-length relation path r = r1, . . . rm, Pr(u, v) denotes the
set of all paths connecting u to v in G via relation path r.

Our goal is to learn to predict destinations for each r ∈ R,
however, in interest of keeping the approach simple we pose
each relation-speciﬁc learning task in isolation. Given that
standard KBC benchmarks (such as WN18RR) consist of
sparse graphs, following previous work (Yang, Yang, and
Cohen 2017a), we too introduce inverse relations. In other
words, for each r ∈ R we introduce a new relation r−1 by
adding for each (cid:104)h, r, t(cid:105) ∈ E a new triple (cid:104)t, r−1, h(cid:105) to G.
We refer to the augmented set of relations, including the in-
verse relations, as R+. Learning to predict destinations for
a given relation r ∈ R+ is essentially a binary classiﬁca-
tion task where (cid:104)h, r, t(cid:105) ∈ E and (cid:104)h, r, t(cid:105) /∈ E, ∀h, t ∈ V,
denote positive and negative examples, respectively. The ap-
proaches we present next differ in how they score a triple
(cid:104)h, r, t(cid:105) which in turn depends on paths Pr(u, v). Since we
are interested in using such models to score unseen triples,
we need to remove the triple from G during training because,
as mentioned earlier, we use triples from G as positive exam-
ples. More precisely, when scoring a triple (cid:104)h, r, t(cid:105) during
training we always compute Pr(u, v) not from E but from
E \ {(cid:104)h, r, t(cid:105), (cid:104)t, r−1, h(cid:105)}, i.e., we always remove 2 edges:
the triple itself and its corresponding inverse triple.

3.2 Chains of LNN-pred Mixtures
Given G, a relation to model r ∈ R, and a user-deﬁned rule-
length m, our ﬁrst approach for learning chain rules uses m

Figure 3: Kinship path counts (in thousands).

LNN-pred operators and one m-ary LNN-∧ operator. Score
for triple (cid:104)u, r, v(cid:105) is deﬁned as:

(cid:88)

(cid:88)

r=
r1,...,rm

p∈
Pr(u,v)

LNN-∧{LNN-pred(er1), . . . LNN-pred(erm )}

where er ∈ {0, 1}|R| is a one-hot encoding whose rth entry
is 1 with 0s everywhere else. To be clear, the above model
consists of m + 1 + m|R| parameters: m + 1 for LNN-∧
(w, β) and m LNN-pred operators each consisting of |R|-
sized w parameters. Due to one-hot encodings and the fact
that the inner summation term does not depend on path p, the
above expression can be simpliﬁed to:

(cid:88)

|Pr(u, v)|LNN- ∧ (w1
r1

, . . . wm
rm

)

(4)

where wi

r=r1,...,rm
r denotes the rth weight of the ith LNN-pred.
This model is closely related to NeuralLP (Yang, Yang, and
Cohen 2017a) that also chains together mixtures of relations
(see Equation 5 in Yang, Yang, and Cohen). Differences
between the above model and NeuralLP include the use of
1) LNN operators instead of product t-norm and, 2) a rule
generator. NeuralLP uses an RNN-based rule generator to
generate the relations present in the body of the chain rule,
we instead sum over all relation paths. NeuralLP was among
the ﬁrst NeSy-based KBC approaches on top of which later
approaches are built, e.g. DRUM (Sadeghian et al. 2019b).

3.3 Mixture of Relation Paths
One of the drawbacks of the previous model is that it treats
each LNN-pred independently. To get around this, we pro-
pose our second approach which consists of one LNN-pred
across all relation paths. Given G, r, m, score for (cid:104)u, r, v(cid:105) is:

(cid:88)

r=r1,...,rm

|Pr(u, v)|LNN-pred(er)

the score of path p given such a pre-trained KGE. Score of
(cid:104)u, r, v(cid:105) under the modiﬁed MP model is now given by:

(cid:88)

r=r1,...rm

LNN-pred(er)

(cid:88)

σ(p)

p∈Pr(u,v)

Essentially, the goal is to bias the learning process so that re-
lation paths corresponding to larger σ(p) are assigned larger
weights. Figure 4 shows the (sum of) path scores for all
length 2 relation paths in Kinship measured via CP-N3 em-
beddings (Lacroix, Usunier, and Obozinski 2018) where the
cell values are much larger, e.g, while there are only 1200
term16(U, W ) ∧ term16(W, V ) paths its total path score
is 7200. Scoring paths using pre-trained KGE was only in-
troduced recently in RNNLogic (Qu et al. 2020) which used
RotatE (Sun et al. 2019a) speciﬁcally. We next show how to
utilize a much larger class of KGEs for the same purpose.

There are at least 2 kinds of KGEs available that rely on
a: 1) similarity measure of the triple (cid:104)h, r, t(cid:105), e.g., CP-N3
(Lacroix, Usunier, and Obozinski 2018), and 2) distance
measure used to contrast t’s embedding with some function
of h and r’s embeddings, e.g., TransE (Bordes et al. 2013a),
RotatE (Sun et al. 2019a). We describe σ(p) for both cases:

similarity-based: σ(p) =

distance-based: σ(p) =

(cid:88)

(cid:104)h,r,t(cid:105)∈p

(cid:88)

(cid:104)h,r,t(cid:105)∈p

1
1 + exp{−sim(h, r, t)}

exp{2(δ − d(h, r, t))} − 1
exp{2(δ − d(h, r, t))} + 1

where δ denotes the margin parameter used by the underlying
distance-based KGE to convert distances into similarities. For
both of these, we break the path into a series of edges, use the
underlying KGE to compute similarity sim() or distance d()
for each triple (as the case may be) and aggregate across all
triples in the path. Based on extensive experimentation, we
recommend sigmoid and tanh as the non-linear activation
for similarity-based and distance-based KGE, respectively.

3.5 Training Algorithm
Among various possible training algorithms, based on exten-
sive experimentation, we have found the following scheme
to perform reliably. In each iteration, we sample uniformly at
random a mini-batch of positive triples B+ from (cid:104)h, r, t(cid:105) ∈ E
and negative triples B− from (cid:104)h, r, t(cid:105) /∈ E, ∀h, t ∈ V, such
that |B+| = |B−| to minimize the following loss:

(cid:88)

(cid:88)

(cid:104)h,r,t(cid:105)∈B+

(cid:104)h(cid:48),r,t(cid:48)(cid:105)∈B−

max{0, score(h(cid:48), t(cid:48))−score(h, t)+γ}

where γ denotes the margin hyperparameter.

4 Experimental Setup
In this section, we describe our experimental setup. We intro-
duce the datasets used and compare against state-of-the-art
baselines. We primarily validate our LNN-based rule-learning
approaches and their combination with KGE.
Datasets: To evaluate our approach, we experiment on stan-
dard KBC benchmarks, viz. Uniﬁed Medical Language Sys-
tem (UMLS) (Kok and Domingos 2007), Kinship (Kok and

Figure 4: Kinship CP-N3 path scores (×1000).

where er ∈ {0, 1}|R|m
is a one-hot encoding with 1 in its rth
position and 0 everywhere else. One way to deﬁne a unique
index for relation path r = r1, . . . rm is (cid:80)

i=1,...m ri|R|i.

While recent approaches (Das et al. 2018; Qu et al. 2021)
have followed a similar model, the idea was proposed in the
path ranking algorithm (PRA) (Lao, Mitchell, and Cohen
2011). Instead of path counts |Pr(u, v)|, PRA computes the
random walk probability of arriving from u to v. Another
difference lies in the parameterization. While LNN-pred con-
strains its parameters to sum to 1, PRA uses elastic net regu-
larization instead . Note that, PRA has been shown to perform
quite poorly on KBC benchmarks, e.g. see Qu et al. (2021).

3.4 Handling Sparsity with Graph Embeddings

One of the drawbacks of Mixture of Relation Paths (MP) is
that only paths with relation path r contribute to the esti-
mation of the weight parameter for r (wr). In contrast, any
path that contains ri in the ith edge of its relation path (a
much larger set of paths) may contribute to the estimation
of wi
ri in Equation 4. This is problematic because in most
knowledge graphs, there is a stark difference in path counts
for various relation paths. For example, Figure 3 shows the
counts for all length 2 relation paths in Kinship and bar-
ring 4 (term16(U, W ) ∧ term7(W, V ), term16(U, W )
∧ term8(W, V ), term16(U, W ) ∧ term16(W, V ),
term7(U, W ) ∧ term16(W, V )) all relation path counts
lie below 1000. In fact, a vast majority lie in single digits.
This implies that for a rare relation path r, estimated wr may
lack in statistical strength and thus may be unreliable.

To address sparsity of paths, we utilize knowledge graph
embeddings (KGE). The literature is rife with approaches
that embed V and R into a low-dimensional hyperspace by
learning distributed representations or embeddings. Our as-
sumption is that such techniques score more prevalent paths
higher than paths that are less frequent. Let σ(p) denote

Domingos 2007), WN18RR (Dettmers et al. 2018), and
FB15K-237 (Toutanova and Chen 2015). Table 1 provides
dataset statistics. We use standard train/validation/test splits
for all datasets. Note that, RNNLogic deﬁned its own splits
for Kinship and UMLS, we report results on these too for a
fair comparison (numbers in parenthesis in Table 1).
Baselines: We compare our approaches against state-of-the-
art KBC approaches categorized as follows:

• Rule-learning approaches: Neural-LP (Yang, Yang, and Co-
hen 2017b), DRUM (Sadeghian et al. 2019b), RNNLogic
(rules only) (Qu et al. 2021) and CTP (Minervini et al. 2020).
• Rule-learning approaches that utilize KGE: We compare
against RNNLogic (w/ embd.) (Qu et al. 2021) which uses
RotatE (Sun et al. 2019b), MINERVA (Das et al. 2018) and
MultiHopKG (Lin, Socher, and Xiong 2018).

• Embedding-based approaches: We report results from
ComplEx-N3, the best KGE we know of, and CP-N3, the
KGE used by our methods, (both from Lacroix, Usunier,
and Obozinski (2018)), RotatE (Sun et al. 2019b), used in
RNNLogic, and Complex (Trouillon et al. 2016).
Metrics: Given an unseen query (cid:104)h, r, ?(cid:105) we compute ﬁl-
tered ranks (Bordes et al. 2013a) for destination vertices
after removing the destinations that form edges with h and
r present in the train, test and validation sets. Based on Sun
et al. (2020)’s suggestions, our deﬁnitions of mean recipro-
cal rank (MRR) and Hits@K satisfy two properties: 1) They
assign a larger value to destinations ranked lower, and 2) If
destinations t1, . . . , tm share the same rank n + 1 then each
of them is assigned an average of ranks n + 1, . . . , n + m:

MRR(ti) =

1
m

n+m
(cid:88)

r=n+1

1
r

, Hits@K(ti) =

1
m

n+m
(cid:88)

r=n+1

δ(r ≤ K)

where δ() denotes the Dirac delta function. We include in-
verse triples and report averages across the test set.
Implementation: We evaluate rules learned with Chain of
Mixtures (LNN-CM), Mixture of Paths (LNN-MP) and their
combinations with KGE. We use Adagrad (Duchi, Hazan,
and Singer 2011) with step size ∈ {0.1, 1.0}, margin γ ∈
{0.1, 0.5, 1.0, 2.0} and batch size |B+| = |B−| = 8. We use
the validation set to perform hyperparameter tuning and learn
rules of length up to 4 for FB15K-237, 5 for WN18RR, and 3
for Kinship, UMLS. We combine our rule-learning approach
with pre-trained CP-N3 embeddings of dimension 4K for
FB15K-237, 3K for WN18RR, and 8K for Kinship, UMLS.

5 Results and Discussion
Table 2 shows all our results comparing LNN-CM and LNN-
MP on standard splits of all datasets. We compare against
all the baselines including KGE-based approaches (EMBD.),
approaches that only learn rules (RULES), and approaches
that learn rules with KGE (W/ EMBD.). For the last category,
we indicate the KGE used in the name of the approach.
LNN-CM vs. NeuralLP, DRUM: Recall that LNN-CM is
closely related to NeuralLP and DRUM. Table 2 shows that,
in terms of MRR, LNN-CM outperforms DRUM and Neu-
ralLP on UMLS and FB15K-237 while being comparable to
DRUM on Kinship and WN18RR. LNN-CM achieves this

Dataset

Train

Valid

Test

Relations

Entities

Kinship

UMLS

8544
(3206)

5216
(1959)

1068
(2137)

652
(1306)

1074
(5343)

661
(3264)

WN18RR

86835

3034

3134

50

92

11

104

135

40943

FB15K-237

272155

17535

20466

237

14541

Table 1: Dataset Statistics. Numbers in parenthesis reﬂects
splits from RNNLogic (Qu et al. 2021).

without using an RNN-based rule generator and is thus con-
ceptually much simpler. This is clear evidence that an RNN
is not necessary for KBC. DRUM and NeuralLP’s results
in Table 2 are signiﬁcantly lower than what was reported in
Yang, Yang, and Cohen (2017a); Sadeghian et al. (2019a). As
explained in Sun et al. (2020), this is likely due to the care-
fully deﬁned MRR and Hits@K used in our evaluation, not
using which can lead to overly-optimistic and unfair results1.
LNN-MP vs. LNN-CM: Table 2 shows that LNN-MP
(RULES) consistently outperforms LNN-CP. While previous
comparisons against NeuralLP and/or DRUM (Qu et al. 2021;
Lin, Socher, and Xiong 2018; Das et al. 2018) also hinted at
this result, a surprising ﬁnding is that the margin of difference
depends on the dataset. In particular, on FB15K-237 LNN-
CM’s MRR comes within 1% of state-of-the-art RNNLogic
(RULES)’s. Across all standard splits, LNN-MP (RULES) out-
performs all rule-learning methods. On the smaller Kinship
and UMLS datasets, LNN-MP outperforms CTP, and on the
larger WN18RR and FB15K-237 it outperforms RNNLogic.
In Table 3, we also report LNN-MP’s results on RNNLogic’s
splits which include a much smaller training set for Kinship
and UMLS (as shown in Table 1). Given the substantial in-
creases resulting from the switch to standard splits (+14.5%
and +8.5% in MRR on Kinship and UMLS, respectively),
it seems that the simpler LNN-MP (RULES) exploits the ad-
ditional training data much more effectively. On the other
hand, RNNLogic (RULES) hardly shows any improvement
on Kinship and in fact, deteriorates on UMLS. A possible rea-
son could be that the inexact training algorithm employed by
RNNLogic (based on expectation-maximization and ELBO
bound) fails to leverage the additional training data.
Learning Rules with Embeddings: Since LNN-MP outper-
forms LNN-CM (rules only), we combine it with CP-N3, one
of the best KGE available, to learn rules with embeddings.
In comparison to LNN-MP (RULES), LNN-MP w/ CP-N3
shows consistent improvements ranging from +1.2% MRR
(on WN18RR, Table 2) to +9.2% MRR (on Kinship standard
splits, Table 2). In comparison to RNNLogic w/ RotatE, LNN-

1Investigating more, Table 4 in Sadeghian et al. reports DRUM’s
Hits@10 on WN18RR as 0.568 with rule length up to 2. However,
56% of WN18RR’s validation set triples require paths longer than 2
to connect the source with the destination (test set fraction is similar).
This implies that one can only hope to achieve a maximum Hits@10
of 0.44 when restricted to rules of length 2 learned via any rule-
based KBC technique which is < 0.568 and a clear contradiction.

Kinship

UMLS

WN18RR

FB15K-237

MRR Hits@10 Hits@3 MRR Hits@10 Hits@3 MRR Hits@10 Hits@3 MRR Hits@10 Hits@3

.

D
B
M
E

S
E
L
U
R

CP-N3
Complex-N3
RotatE
Complex

NeuralLP
DRUM
CTP
RNNLogic
LNN-CM (Ours)
LNN-MP (Ours)

88.9 98.7
89.2 98.6
75.5 97.3
83.7 98.4

48.8 89.1
40.0 86.1
70.3 93.9
64.5 91.1
39.1 68.7
81.9 98.4

94.8
94.7
86.5
91.6

63.0
48.2
79.7
72.9
43.6
89.3

93.3 99.7
95.9 99.7
86.1 99.5
94.5 99.7

55.3 93.0
61.8 97.9
80.1 97.0
71.0 91.1
78.7 95.1
90.0 99.4

98.9
98.9
97.0
98.0

75.4
91.2
91.0
82.1
88.9
98.3

47.0∗ 54.0∗
48.0∗ 57.0∗
47.6∗ 57.1∗
44.0 49.6

−

33.7 50.2
34.8 52.1
−
45.5∗ 53.1∗
36.6 49.2
47.3 55.5

−
−
49.2∗
44.9

43.3
44.6
−
47.5∗
39.2
49.7

36.0∗ 54.0∗
−
37.0∗ 56.0∗ −
33.8∗ 53.3∗
31.8 49.1

37.5∗
34.7

−

25.8 48.5
25.8 49.1
−
28.8∗ 44.5∗
28.0 43.5
30.7 47.0

31.4
31.5
−
31.5∗
30.4
34.2

. RNNLogic
D
B
w/ RotatE
M
E
LNN-MP (Ours)
w/ CP-N3

W

/

−

−

−

−

−

−

48.3∗ 55.8∗

49.7∗

34.4∗ 53.0∗

38.0∗

91.1 99.2

96.5

94.5 100

99.2

48.5 56.1

50.2

35.1 53.0

39.1

Table 2: Results of LNN-CM and LNN-MP in comparison to other state-of-the-art approaches on standard splits of Kinship,
UMLS, WN18RR, and FB15K-237. Bold font denotes best within each category of KBC approaches. ∗ denotes results copied
from original papers. CTP did not scale to larger datasets. ComplEx-N3 does not report Hits@3 for WN18RR and FB15K-237.
RNNLogic does not report Kinship and UMLS results on standard splits w/ RotatE.

Kinship

UMLS

MRR Hits@10 Hits@3 MRR Hits@10 Hits@3

WN18RR

FB15K-237

MRR Hits@10 Hits@3 MRR Hits@10 Hits@3

CP-N3
ComplEx-N3
RotatE
Complex

60.2 92.2
60.5∗ 92.1∗
65.1∗ 93.2∗
54.4 89.0

RNNLogic
LNN-MP (Ours)

63.9 92.4
67.4 95.0

RNNLogic w/ RotatE
LNN-MP w/ CP-N3 (Ours)

72.2∗ 94.9∗
72.2 96.9

70.0
71.0∗
75.5∗
63.8

73.1
77.0

81.4∗
81.6

76.6 95.0
79.1∗ 95.7∗
74.4∗ 93.9∗
74.0 92.5

74.5 92.4
81.5 96.6

84.2∗ 96.5∗
87.6 98.0

85.6
87.3∗
82.2∗
81.1

83.3
91.8

89.1∗
94.7

MINERVA∗
MultiHopKG∗
RNNLogic∗
RNNLogic∗ w/ RotatE

44.8 51.3
47.2 54.2
47.8 55.3
50.6 59.2

45.6
−
50.3
52.3

29.3 45.6
40.7 56.4
37.7 54.9
44.3 64.0

LNN-MP (Ours)
LNN-MP w/ CP-N3 (Ours)

51.6 58.4
51.9 59.0

53.5
53.9

40.0 57.4
44.8 63.6

32.9
−
41.2
48.9

44.4
49.5

Table 4: Evaluating on direct triples (excluding inverses).

Table 3: Results on RNNLogic’s splits for Kinship and
UMLS. ∗ denotes results copied from Qu et al. (2021).

MP w/ CP-N3 shows small but consistent improvements on
UMLS (RNNLogic’s splits, Table 3), WN18RR, FB15K-237
(Table 2), and comparable results on Kinship (RNNLogic’s
splits, Table 3). This is encouraging due to the simplicity
of LNN-MP which adds to its appeal. Table 4 shows that
LNN-MP w/ CP-N3 also outperforms MINERVA and Multi-
HopKG, two more MP-based approaches that utilize embed-
dings. Note that, following MINERVA and MultiHopKG, for
this comparison we leave out inverse triples from the test set.

5.1 Qualitative Analysis and Discusssion

Learned rules can be extracted from LNN-MP by sorting the
relation paths in descending order of wr. Table 5 presents
some of the learned rules appearing in the top-10.
Rules for FB15K-237: Rule 1 in Table 5 describes a learned
rule that infers the language a person speaks by exploit-
ing knowledge of the language spoken in her/his country
of nationality. In terms of relation paths, this looks like:

P (person)

nationality

−→ N (nation)

spoken in

−→ L(language)

Similarly, Rule 2 uses the ﬁlm country relation instead of
nationality to infer the language used in a ﬁlm. Besides
spoken in, FB15K-237 contains other relations that can be
utilized to infer language such as the ofﬁcial language spo-
ken in a country. Rule 3 uses this relation to infer the lan-
guage spoken in a TV program by ﬁrst exploiting knowl-
edge of its country of origin. Rules 5, 6 and 7 are longer
rules containing 3 relations each in their body. Rule 5 in-
fers a TV program’s country by ﬁrst exploiting knowledge
of one of its actor’s birth place and then determining which
country the birth place belongs to. In terms of relation path:
A(actor) born in−→ L(birth place)
P (program)
located in−→ N (nation). Rule 6 uses a ﬁlm crew member’s mar-
riage location instead to infer the region where the ﬁlm was
released. Rule 7 infers the marriage location of a celebrity by
exploiting knowledge of where their friend got married.
Recursive Rules for WN18RR: Unlike FB15K-237,
WN18RR is a hierarchical KG and thus sparser which calls
for longer rules to be learned. A majority of WN18RR’s edges
are concentrated in a few relations, e.g. hypernym, and most
learned rules rely on such relations. Hypernym(X, Y ) is true
if Y , e.g. dog, is a kind of X, e.g. animal. Table 5 presents 3

tv program actor
−→

FB15K-237

1) person language(P, L) ← nationality(P, N ) ∧ spoken in(L, N )
2) ﬁlm language(F, L) ← ﬁlm country(F, C) ∧ spoken in(L, C)
3) tv program language(P, L) ← country of tv program(P, N ) ∧ ofﬁcial language(N, L)
4) burial place(P, L) ← nationality(P, N ) ∧ located in(L, N )
5) country of tv program(P, N ) ← tv program actor(P, A) ∧ born in(A, L) ∧ located in(L, N )
6) ﬁlm release region(F, R) ← ﬁlm crew(F, P ) ∧ marriage location(P, L) ∧ located in(L, R)
7) marriage location(P, L) ← celebrity friends(P, F ) ∧ marriage location(F, L(cid:48)) ∧ location adjoins(L(cid:48), L)

WN18RR

8) domain topic(X, Y ) ← hypernym(X, U ) ∧ hypernym(U, V ) ∧ hypernym(W, V ) ∧ hypernym(Z, W ) ∧ domain topic(Z, Y )
9) derivation(X, Y ) ← hypernym(X, U ) ∧ hypernym(V, U ) ∧ derivation(W, V ) ∧ hypernym(W, Z) ∧ hypernym(Y, Z)
10) hypernym(X, Y ) ← member meronym(U, X) ∧ hypernym(U, V ) ∧ hypernym(W, V ) ∧ member meronym(W, Z) ∧ hypernym(Z, Y )

Table 5: Examples of LNN-MP’s learned rules.

learned rules for WN18RR. Domain topic(X, Y ) is true if X
is the category, e.g., computer science, of scientiﬁc concept
Y , e.g., CPU. Rule 8 infers the domain topic of Y by ﬁrst
climbing down the hypernym is-a hierarchy 2 levels and then
climbing up: X is-a→ U is-a→ V is-a← W is-a← Z domain−→ Y . Rules 9
and 10 are recursive, where the relation in the head of the rule
also appears in the body. Here, derivation indicates related
form of concept, e.g., yearly is the derivation of year, and
member meronym denotes a concept which is a member of
another concept such as player and team.

6 Related Work

Knowledge Base Completion (KB) approaches have gained
a lot of interest recently, due to their ability to handle the
incompleteness of knowledge bases. Embedding-based tech-
niques maps entities and relations to low-dimensional vector
space to infer new facts. These techniques use neighborhood
structure of an entity or relation to learn their corresponding
embedding. Starting off with translational embeddings (Bor-
des et al. 2013b), embedding-based techniques have evolved
to use complex vector spaces such as ComplEx (Trouillon
et al. 2016), RotatE (Sun et al. 2019a), and QuatE (Zhang
et al. 2019). While the performance of embedding-based tech-
niques has improved over time, rule-learning techniques have
gained attention due to its inherent ability for learning inter-
pretable rules (Yang, Yang, and Cohen 2017b; Sadeghian
et al. 2019b; Rockt¨aschel and Riedel 2017; Qu et al. 2020).
The core ideas in rule learning can be categorized into
two groups based on their mechanism to select relations for
rules. While Chain of Mixtures (CM) represents each rela-
tion in the body as a mixture, e.g. NeuralLP (Yang, Yang,
and Cohen 2017a), DRUM (Sadeghian et al. 2019a), Mixture
of Paths (MP) learns relation paths; e.g., MINERVA (Das
et al. 2018), RNNLogic (Qu et al. 2021). Recent trends in
both these types of rule learning approaches has shown sig-
niﬁcant increase in complexity for performance gains over
their simpler precursors. Among the ﬁrst to learn rules for
KBC, NeuralLP (Yang, Yang, and Cohen 2017a) uses a long
short-term memory (LSTM) (Hochreiter and Schmidhuber
1997) as its rule generator. DRUM (Sadeghian et al. 2019a)
improves upon NeuralLP by learning multiple such rules
obtained by running a bi-directional LSTM for more steps.
MINERVA (Das et al. 2018) steps away from representing
each body relation as a mixture, proposing instead to learn

the relation sequences appearing in paths connecting source
to destination vertices using neural reinforcement learning.

RNNLogic (Qu et al. 2021) is the latest to adopt a path-
based approach for KBC that consists of two modules, a rule-
generator for suggesting high quality paths and a reasoning
predictor that uses said paths to predict missing information.
RNNLogic employs expectation-maximization for training
where the E-step identiﬁes useful paths per data instance
(edge in the KG) by sampling from an intractable posterior
while the M-step uses the per-instance useful paths to update
the overall set of paths. Both DRUM and RNNLogic repre-
sent a signiﬁcant increase in complexity of their respective
approaches compared to NeuralLP and MINERVA.

Unlike these approaches, we propose to utilize Logical
Neural Networks (LNN) (Riegel et al. 2020); a simple yet
powerful neuro-symbolic approach which extends Boolean
logic to the real-valued domain. On top of LNN, we propose
two simple approaches for rule learning based on CP and
MP. Furthermore, our approach allows for combining rule-
based KBC with any KGE, which results in state-of-the-art
performance across multiple datasets.

7 Conclusion

In this paper, we proposed an approach for rule-based KBC
based on Logic Neural Networks (LNN); a neuro-symbolic
differentiable framework for real-valued logic. In particular,
we present two approaches for rule learning, one that repre-
sent rules using chains of predicate mixtures (LNN-CM), and
another that uses mixtures of paths (LNN-MP). We show that
both approaches can be implemented using LNN and neuro-
symbolic AI, and result in better or comparable performance
to state-of-the-art. Furthermore, our framework facilitates
combining rule learning with knowledge graph embedding
techniques to harness the best of both worlds.

Our experimental results across four benchmarks show that
such a combination provides better results and establishes
new state-of-the-art performance across multiple datasets.
We also showed how easy it is to extract and interpret learned
rules. With both LNN-CM and LNN-MP implemented within
the same LNN framework, one avenue of future work would
be to explore combinations of the two approaches given their
good performance on a number of KBC benchmarks.

Rebele, T.; Suchanek, F.; Hoffart, J.; Biega, J.; Kuzey, E.; and
Weikum, G. 2016. YAGO: A multilingual knowledge base
from wikipedia, wordnet, and geonames. In International
semantic web conference, 177–185. Springer.
Riegel, R.; Gray, A.; Luus, F.; Khan, N.; Makondo, N.; Akhal-
waya, I. Y.; Qian, H.; Fagin, R.; Barahona, F.; Sharma, U.;
Ikbal, S.; Karanam, H.; Neelam, S.; Likhyani, A.; and Srivas-
tava, S. 2020. Logical Neural Networks. CoRR.
Rockt¨aschel, T.; and Riedel, S. 2017. End-to-end differen-
tiable proving. arXiv preprint arXiv:1705.11040.
Sadeghian, A.; Armandpour, M.; Ding, P.; and Wang, D. Z.
2019a. DRUM: End-To-End Differentiable Rule Mining On
Knowledge Graphs. In NeurIPS.
Sadeghian, A.; Armandpour, M.; Ding, P.; and Wang, D. Z.
2019b. Drum: End-to-end differentiable rule mining on
knowledge graphs. arXiv preprint arXiv:1911.00055.
Sun, Z.; Deng, Z.-H.; Nie, J.-Y.; and Tang, J. 2019a. Ro-
tate: Knowledge graph embedding by relational rotation in
complex space. arXiv preprint arXiv:1902.10197.
Sun, Z.; Deng, Z.-H.; Nie, J.-Y.; and Tang, J. 2019b. Ro-
tatE: Knowledge Graph Embedding by Relational Rotation
in Complex Space. In ICLR.
Sun, Z.; Vashishth, S.; Sanyal, S.; Talukdar, P.; and Yang,
Y. 2020. A Re-evaluation of Knowledge Graph Completion
Methods. In ACL.
Toutanova, K.; and Chen, D. 2015. Observed versus latent
features for knowledge base and text inference. In Proceed-
ings of the 3rd workshop on continuous vector space models
and their compositionality, 57–66.
Trinh, T. H.; Dai, A. M.; Luong, M.-T.; and Le, Q. V. 2018.
Learning Longer-term Dependencies in RNNs with Auxiliary
Losses. In ICLR Workshops.
´E.; and
Trouillon, T.; Welbl, J.; Riedel, S.; Gaussier,
Bouchard, G. 2016. Complex embeddings for simple link
prediction. In International conference on machine learning,
2071–2080. PMLR.
Yang, F.; Yang, Z.; and Cohen, W. W. 2017a. Differentiable
Learning of Logical Rules for Knowledge Base Reasoning.
In NeurIPS.
Yang, F.; Yang, Z.; and Cohen, W. W. 2017b. Differentiable
learning of logical rules for knowledge base reasoning. arXiv
preprint arXiv:1702.08367.
Zhang, S.; Tay, Y.; Yao, L.; and Liu, Q. 2019. Quater-
arXiv preprint
nion knowledge graph embeddings.
arXiv:1904.10281.

References
Bollacker, K.; Evans, C.; Paritosh, P.; Sturge, T.; and Taylor,
J. 2008. Freebase: a collaboratively created graph database
for structuring human knowledge. In Proceedings of the 2008
ACM SIGMOD international conference on Management of
data, 1247–1250.
Bordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and
Yakhnenko, O. 2013a. Translating embeddings for modeling
multi-relational data. In NeurIPS.
Bordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and
Yakhnenko, O. 2013b. Translating embeddings for model-
ing multi-relational data. Advances in neural information
processing systems, 26.
Das, R.; Dhuliawala, S.; Zaheer, M.; Vilnis, L.; Durugkar,
I.; Krishnamurthy, A.; Smola, A.; and McCallum, A. 2018.
Go for a Walk and Arrive at the Answer: Reasoning Over
Paths in Knowledge Bases using Reinforcement Learning. In
ICLR.
Dettmers, T.; Minervini, P.; Stenetorp, P.; and Riedel, S. 2018.
Convolutional 2d knowledge graph embeddings. In Thirty-
second AAAI conference on artiﬁcial intelligence.
Dong, H.; Mao, J.; Lin, T.; Wang, C.; Li, L.; and Zhou, D.
2019. Neural Logic Machines. In ICLR.
Duchi, J.; Hazan, E.; and Singer, Y. 2011. Adaptive Subgra-
dient Methods for Online Learning and Stochastic Optimiza-
tion. JMLR.
Frerix, T.; Nießner, M.; and Cremers, D. 2020. Homogeneous
Linear Inequality Constraints for Neural Network Activations.
In CVPR Workshops.
Hochreiter, S.; and Schmidhuber, J. 1997. Long short-term
memory. Neural computation.
Kok, S.; and Domingos, P. 2007. Statistical predicate inven-
tion. In Proceedings of the 24th international conference on
Machine learning, 433–440.
Lacroix, T.; Usunier, N.; and Obozinski, G. 2018. Canonical
tensor decomposition for knowledge base completion. In
International Conference on Machine Learning, 2863–2872.
PMLR.
Lao, N.; Mitchell, T.; and Cohen, W. W. 2011. Random Walk
Inference and Learning in A Large Scale Knowledge Base.
In EMNLP.
Lin, X. V.; Socher, R.; and Xiong, C. 2018. Multi-hop knowl-
edge graph reasoning with reward shaping. In EMNLP.
Minervini, P.; Riedel, S.; Stenetorp, P.; Grefenstette, E.; and
Rockt¨aschel, T. 2020. Learning reasoning strategies in end-
to-end differentiable proving. In International Conference
on Machine Learning, 6938–6949. PMLR.
Pascanu, R.; Mikolov, T.; and Bengio, Y. 2013. On the Difﬁ-
culty of Training Recurrent Neural Networks. In ICML.
Qu, M.; Chen, J.; Xhonneux, L.-P.; Bengio, Y.; and Tang,
J. 2020. Rnnlogic: Learning logic rules for reasoning on
knowledge graphs. arXiv preprint arXiv:2010.04029.
Qu, M.; Chen, J.; Xhonneux, L.-P.; Bengio, Y.; and Tang, J.
2021. {RNNL}ogic: Learning Logic Rules for Reasoning on
Knowledge Graphs. In ICLR.

