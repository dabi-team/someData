2
2
0
2

t
c
O
6
1

]

C
O
.
h
t
a
m

[

3
v
9
2
8
2
0
.
7
0
2
2
:
v
i
X
r
a

Online Bilevel Optimization: Regret Analysis of
Online Alternating Gradient Methods

Davoud Ataee Tarzanagh and Laura Balzano
Department of Electrical Engineering and Computer Science
University of Michigan, Ann Arbor
tarzanaq,girasole@umich.edu

Abstract

Online optimization is a well-established optimization paradigm that aims to make a sequence
of correct decisions given knowledge of the correct answer to previous decision tasks. Bilevel
programming involves a hierarchical optimization problem where the feasible region of the so-
called outer problem is restricted by the graph of the solution set mapping of the inner problem.
This paper brings these two ideas together and studies an online bilevel optimization setting in
which a sequence of time-varying bilevel problems are revealed one after the other. We extend
the known regret bounds for single-level online algorithms to the bilevel setting. Speciﬁcally,
we introduce new notions of bilevel regret, develop an online alternating time-averaged gradient
method that is capable of leveraging smoothness, and provide regret bounds in terms of the
path-length of the inner and outer minimizer sequences.

1

Introduction

Bilevel optimization is rapidly evolving due to its wide array of applications in modern machine
learning (ML) problems including meta-learning [25, 10], hyperparameter optimization [28, 24],
neural network architecture search [57, 8], and reinforcement learning [46, 84]. A fundamental
assumption in bilevel optimization which has been adopted by almost all of the relevant literature [3,
60, 29, 46, 27, 30, 85, 17, 51, 16, 79], is that the inner and outer cost functions do not change
throughout the horizon over which we seek to optimize. This oﬄine setting may not be suitable
to model temporal changes in today’s ML problems such as online actor-critic [95, 80], online
meta-learning [26], strategic dynamic regression [35], and sequential decision makings for which
the objective functions are time-varying and are not available to the decision-maker a priori. To
address these challenges, this paper considers an online bilevel optimization (OBO) setting in which
a sequence of bilevel problems are revealed one after the other, and studies computationally tractable
notions of bilevel regret minimization.

We begin by introducing the setting of online (single-level) optimization, which is modeled as
a game between a learner and an adversary [36, 97]. During each round t ∈ [T ] := {1, . . . , T } of a
repeated decision process, the learner is tasked with predicting xt ∈ X ⊂ Rd1, a convex decision set.
Concurrently, the adversary chooses a loss function ft : X → R; the learner then observes ft(x) and
suﬀers a loss of ft(xt). This process repeats across T rounds. In the non-static setting [11, 48], the
performance of the learner is measured through its (single-level) dynamic regret

D-Reg

T :=

T
(cid:88)

t=1

ft(xt) −

T
(cid:88)

t=1

ft(x∗

t ),

1

(1)

 
 
 
 
 
 
where x∗

t ∈ arg minx∈X ft(x).

In the case of static regret [36, 97], x∗
t

is replaced by x∗ ∈ arg minx∈X

(cid:80)T

t=1 ft(x), i.e.,

S-Reg

T :=

T
(cid:88)

t=1

ft(xt) − min
x∈X

T
(cid:88)

t=1

ft(x).

(2)

The static regret (2) essentially assumes that the comparators do not change over time. This can be
an unrealistic constraint in many practical streaming data problems, ranging from motion imagery
formation to network analysis, for which the underlying environment is dynamic [32, 11, 48]. The
parameters {x∗
could correspond to frames in a video or the weights of edges in a social network
t }T
t=1
and by nature are highly variable.

1.1 Online Bilevel Optimization

Bilevel optimization, also known as the Stackelberg leader-follower game [81], represents a situation
involving two players. The choice of each player inﬂuences the other’s outcome, in addition to her
own. One player, the leader, knows the objective function of the other, the follower. The leader
therefore can perfectly predict the follower’s choice, and she will consider that when optimizing her
own objective. The follower, on the other hand, only has knowledge of her own objective, and must
consider that objective as restricted by the leader’s choice. Bilevel optimization is in this sense
equivalent to a non-cooperative two-person game [9].

To formulate an OBO problem, let xt ∈ X ⊂ Rd1 and ft : X × Rd2 denote the decision
variable vector and the objective function for the leader, respectively; similarly deﬁne yt ∈ Rd2 and
gt ∈ X × Rd2 for the follower. In each round t ∈ [T ], knowing the decision xt−1 of the leader, the
follower has to select yt in an attempt to minimize gt(xt, y) using the information from rounds
t − 1, t − 2, . . . , 0. Being aware of the follower’s selection, the leader then moves by selecting xt to
minimize her own objective function ft(x, yt). This procedure of play is repeated across T rounds.
We characterize our approach in terms of the bilevel dynamic regret:

BD-Reg

T :=

T
(cid:88)

t=1

ft(xt, yt) −

T
(cid:88)

t=1

ft(x∗

t , y∗

t (x∗

t )),

where

y∗

t (x) ∈ arg min
y∈Rd2

gt(x, y)

and x∗

t ∈ arg min

x∈X

ft (x, y∗

t (x)) .

(3a)

(3b)

Our objective is to design online algorithms with a sublinear bilevel regret in the number of rounds.
A sublinear regret implies that, on average, the algorithm plays the optimal decision [11, 36, 97].
We also study the framework of regret minimization setting where x∗
in (3) is replaced by x∗ ∈
t
arg minx∈X

t (x)). In this case, we deﬁne

t=1 ft(x, y∗

(cid:80)T

BS-Reg

T :=

T
(cid:88)

t=1

ft(xt, yt) − min
x∈X

T
(cid:88)

t=1

ft(x, y∗

t (x)).

(4)

are changing over the
Note that the above regret is not fully static and the inner optima {y∗
time horizon T . Examples 1 and 3 provide two ML applications of the above regret minimization
framework.

t }T

t=1

2

Example 1. [Online Meta-Learning] Meta-learning aims to bootstrap from a set of given tasks
to learn faster on future tasks [69, 25]. A popular formulation is an online meta-learning (OML)
setting where agents are faced with tasks one after another [26]. OML can be formulated as the
follower-leader OBO. Speciﬁcally, for each task Tt, the follower receives some training data Dtr
for
t
t ):
adapting the leader’s model wt ∈ W ⊂ Rd1 to the current task following the strategy u(wt; Dtr

u(wt; Dtr

t ) ∈ arg min

u

(cid:10)u, ∇f (wt; Dtr

t )(cid:11) +

1
2β

(cid:107)u − wt(cid:107)2 ,

for some β > 0. Then, the test data Dts
t
of the follower’s model u(wt; Dtr
into leader’s algorithm to update wt. Let w∗
and static regret for OML is deﬁned, respectively, as:

t ). The loss observed at this round f (u(wt; Dtr
t ); Dts

will be revealed to the leader for evaluating the performance
t ) can then be fed
t ); Dts
t ). The bilevel dynamic

t = arg minw∈W f (u(w; Dtr

BD-Reg

T =

BS-Reg

T =

T
(cid:88)

t=1
T
(cid:88)

t=1

f (cid:0)u(wt; Dtr

t ); Dts
t

(cid:1) −

T
(cid:88)

t=1

f (cid:0)u(w∗

t ; Dtr

t ); Dts
t

(cid:1) ,

f (cid:0)u(wt; Dtr

t ); Dts
t

(cid:1) − min
w∈W

T
(cid:88)

t=1

f (cid:0)u(w; Dtr

t ); Dts
t

(cid:1) .

It should be mentioned that BS-Reg
T

is similar to the static regret notion introduced in [26] for OML.

In general, it is impossible to achieve a sublinear dynamic regret bound, due to the arbitrary
ﬂuctuation in the time-varying functions [97, 11]. Existing single-level analysis show that it is indeed
possible to bound the dynamic regret in terms of certain regularity of the comparator sequence
[97, 62, 86, 90], functional variation [11], or the gradient variation [19, 48]. Hence, in order to achieve
a sublinear regret, one has to impose some regularity constraints on the sequence of cost functions. In
this work, we deﬁne the outer and inner path-length (of order p) quantities to capture the regularity
of the sequences:

Pp,T :=

T
(cid:88)

t=2

(cid:107)x∗

t−1 − x∗

t (cid:107)p and Yp,T :=

T
(cid:88)

t=2

(cid:13)
(cid:13)y∗

t−1(x∗

t−1) − y∗

t (x∗

t )(cid:13)
p .
(cid:13)

(6)

Note that Pp,T is the path-length of the outer minimizers and is widely used for analyzing the
dynamic regret of single-level non-stationary optimization [97, 62, 86, 90]. Yp,T is a new regularity for
OBO that measures how fast the minimizers of inner cost functions change. The following example
shows that Pp,T and Yp,T are not comparable in general and both measures play a key role in OBO.

t )2+ 1

Example 2. Let X = [−1, 1] and consider a sequence of quadratic cost functions ft(x, y) =
2 (x+2a(1)
2 y2−(x−a(2)
1
are some time-varying constants. It follows from (3b) that y∗
t (x∗
y∗

, for t = 1, . . . , T , where {a(i)
t + a(2)

t (xt) = xt − a(2)

t for t = 1, . . . , T .

and gt(x, y) = 1

t }4
i=1
, and

t )y+a(4)

t )2+a(3)

t = (−1)t/

t = −a(1)

2 (y−a(2)

. Let a(2)

, x∗

√

t

t

t

t

t

t ) = a(1)
• If a(1)

t = a(2)

t

, then P1,T = P2,T = 0, Y1,T = O(

√

T ), and Y2,T = O(log T ).

• If a(1)

t = 0, then P1,T = O(

√

T ), P2,T = O(log T ), and Y1,T = Y2,T = 0.

This shows that Pp,T and Yp,T are not comparable (in general).

3

Single-Level Regret Minimization

Regret

Setting

Strongly
Convex

Convex

Non-Convex

Notion

Dynamic (Eq. (1))

Static (Eq. (2))

Dynamic (Eq. (1))

Static (Eq. (2))
Local (Eq. (19))

[62]
[90]a
[15]
[38]
[11]
[48]
[86]b
[97]
[40]c

aRequires O(1/(cid:96)f,1) gradients queries at t ∈ [T ].
bRequires vanishing gradient assumption.
cRequires O(w2) gradients queries at t ∈ [T ].

Bound
O(1 + P1,T )
O (1 + min(P1,T , P2,T ))
O (1 + P2,T )
O (log T )
1/3)
O(1 + T 2/3VT
O(1 + GT P1,T )
O (1 + P1,T )
√

O(

T )

O(T /W 2)

Setting
Strongly
Convex

Convex

Thm 5(I)
Thm 5(II)
Thm 8(I)
Thm 8(II)

Non-Convex Thm 9

Bilevel Regret Minimization

(Leader’s) Regret

Kt

O(κg)

Notion
Dynamic (Eq. (3))
Static (Eq. (4))

Bound
O (1 + P2,T + Y2,T )
O (cid:0)log T + ¯Y2,T
√

(cid:1)

O(κg log t)

O(κg log W )

Dynamic (Eq. (3)) O (1 + P1,T + Y1,T + Y2,T )
O(

T + ¯Y1,T + ¯Y2,T )

Static (Eq. (4))
Local (Eq. (18))

O((T + HT )/W )

Table 1: Comparison with the prior works on regret minimization. Kt is the number of inner it-
erations at round t; κg := (cid:96)g,1/µg is the condition number of gt(see, Assumption A); W = (cid:80)w−1
i=0 ui
for {ui}w
t=2 supx∈X |ft−1(x) − ft(x)|;
GT := (cid:80)T
t (x)(cid:107)2; and ¯Yp,T :=
(cid:13)
(cid:80)T
(cid:13)y∗

i=1 with 1 = u0 ≥ u1 . . . uw−1 > 0 (see, Deﬁnition 2); VT := (cid:80)T

t=2 supx∈X (cid:107)∇ft−1(x) − ∇ft(x)(cid:107)2; HT := (cid:80)T

t=2 supx∈X (cid:107)y∗

t−1(x) − y∗

t−1(x∗) − y∗

t (x∗)(cid:13)
p
(cid:13)

.

t=2

1.2 Our Results

Our main contributions lie in developing several new results for OBO, including the ﬁrst-known
bilevel regret bound. More speciﬁcally, we

• Deﬁne new notions of bilevel regret, given above in (3) and (4) (for dynamic and static
settings, respectively), applicable to a wide class of online bilevel convex optimization problems.
To minimize the proposed regret, we introduce an online alternating gradient descent (OAGD)
method that is capable of leveraging smoothness and provide its regret bounds in terms of the
path-length of the inner and/or outer minimizer sequences.

• Present a problem-dependent regret bound on the proposed dynamic regret that depends
solely on the inner and outer square path-lengths, i.e., P2,T and Y2,T , in Theorem 5. We then
establish a lower bound for OAGD in Theorem 7 that matches the upper-bound we obtain
for smooth, strongly convex functions under true inner gradient feedback and an approximate
outer gradient feedback.

• Introduce a novel notion of bilevel local regret, which permits eﬃcient online bilevel learning
in the nonconvex setting and generalizes oﬄine guarantees for convergence to an approximate
local stationary point. We give an alternating time-averaged gradient method, and prove in
Theorem 9 that it achieves sublinear regret according to our proposed deﬁnition of bilevel local
regret.

Notation. Any notation is deﬁned when it is used, but for reference the reader may also ﬁnd it
summarized in Table 3.

2 Related Work

We present a brief review of single-level regret minimization for online learning as well as oﬄine
bilevel optimization. A comparison with the related work is provided in Table 1.

• Static Regret Minimization: Single-level static regret (Eq.(2)) is well-studied in the
literature of online learning [37, 71, 38]. [97] shows that online gradient descent (OGD) provides

4

√

T ) regret bound for convex (possibly nonsmooth) functions.

a O(
O(d1 log T ), and O(log T ) for exponentially concave and strongly-convex functions, respectively.

[38] improves this bound to

• Dynamic Regret Minimization: Single-level dynamic regret forces the player to compete
with time-varying comparators, and thus is particularly favored in non-stationary environments [78,
11]. There are two kinds of dynamic regret in previous studies: The universal dynamic regret aims
to compare with any feasible comparator sequence [97, 89], while the worst-case dynamic regret
(deﬁned in (1)) speciﬁes the comparator sequence to be the sequence of minimizers of online functions
[11, 48, 62, 86, 90, 6, 93]. We present related works for the latter case, as it is the setting studied in
this paper. For strongly convex and smooth functions, [62] shows that an O(1 + P1,T ) dynamic regret
bound is achievable. [86] shows that OGD enjoys an O(1 + P1,T ) dynamic regret bound for convex
and smooth functions with an additional vanishing gradient condition. [15] proves that OGD can
achieve an O(1 + P2,T ) regret bound without the bounded gradient assumption. [90] proposes the
online multiple gradient descent algorithm and proves that it enjoys an O(1 + min(P1,T , P2,T )) regret
bound, which has been recently enhanced to O(1 + min(P1,T , P2,T , VT )) by an improved analysis [92],
where VT = (cid:80)T
[64, 65] provide dynamic regret bounds in terms of
(cid:80)T
t−1 − x∗

t=2 (cid:107)x∗
• Local Regret Minimization: There are several approach to treat the online (single-level)
non-convex optimization including adversarial multi-armed bandit with a continuum of arms [14, 53,
55, 42, 41] and classical Follow-the-Perturbed-Leader algorithm with access to an oﬄine non-convex
optimization oracle [2, 77, 54]. Complementing this literature, [40] considers a local regret that
averages a sliding window of gradients at the current model xt and quantiﬁes the objective of
predicting points with small gradients on average.

t=2 supx∈X |ft−1(x) − ft(x)|.
t (cid:107)1. [11] shows that OGD attains an O(T 2/3VT

1/3) regret for convex functions.

• (Oﬄine) Bilevel Optimization: Since its ﬁrst formulation by Stackelberg [76] and the ﬁrst
mathematical model by Bracken and McGill [13] there has been a steady growth in investigations
and applications of bilevel programming [74, 58]. Some of the initial work [3, 23, 4, 34, 73, 60, 63]
reduce the problem into a single-level optimization problem by replacing the lower-level problem by
its optimality conditions. Recently, (alternating) gradient-based approaches have gained popularity
due to their simplicity and eﬀectiveness [29, 46, 27, 30, 85, 17, 51, 16, 52, 20]. This type of approach
estimates the hypergradient ∇f (x, y∗(x)) for iterative updates, and can generally be divided into
approximate implicit diﬀerentiation (AID) and iterative diﬀerentiation (ITD) categories. ITD-based
approaches [61, 27, 25, 30] estimate the hypergradient ∇f (x, y∗(x)) in either a reverse (automatic
diﬀerentiation) or forward manner. AID-based approaches [22, 68, 30, 51] estimate the hypergradient
via implicit diﬀerentiation. [28] characterizes the asymptotic convergence of a backpropagation-based
approach as one of ITD-based algorithms by assuming the inner-level problem is strongly convex. [70]
provides a similar analysis for a truncated backpropagation scheme. [59, 56] analyze the asymptotic
performance of ITD-based approaches when the inner-level problem is convex. Finite-time complexity
analysis for bilevel optimization has also been explored [29, 51, 46]. However, all these works assume
that the cost function does not change throughout the horizon over which we seek to optimize.
We address this limitation of bilevel optimization by studying a new class of bilevel optimization
algorithms in the online setting.

3 Algorithm and Regret Bounds

In this section, we provide regret bounds dependent on the inner and outer regularities Pp,T and Yp,T .
We present regret bounds in two settings: (i) the full information of the loss function is revealed at
each step; (ii) only the true gradient of inner function and an approximate gradient of outer function
at the decision vectors (xt, yt) ∈ X × Rd2 are revealed.

5

3.1 OBO with Full Information

We ﬁrst list assumptions for OBO.

Assumption A. For all t ∈ [T ]:
A1. ft is (cid:96)f,0-Lipschitz continuous.
A2. gt(x, y) is µg-strongly convex in y for any x ∈ X .
A3. ∇ft, ∇gt, ∇2gt are respectively (cid:96)f,1, (cid:96)g,1, (cid:96)g,2-Lipschitz continuous.

Assumption B. The convex decision set X ⊆ Rd1 is bounded, i.e., (cid:107)x − x(cid:48)(cid:107) ≤ D for any x, x(cid:48) ∈ X .

Throughout, we use κg := (cid:96)g,1/µg to denote the the condition number of online inner functions.
Assumption A requires that the inner and outer functions {(ft, gt)}T
are well-behaved. These
assumptions are common in oﬄine bilevel optimization [29, 46, 51, 85, 17]. We ﬁrst show that an
O(1 + P1,T + Y1,T ) upper bound is achievable. Given (x1, y1) ∈ X × Rd2, we assume at each step
the full information of the cost functions (ft, gt) is revealed after the decisions (yt, xt) is submitted.
Then, we can update yt+1 and xt+1, respectively, by

t=1

yt+1 = arg min

y∈Rd2

gt(x, y) and xt+1 = arg min

x∈X

ft(x, yt+1),

(7)

for all t = 1, . . . , T .

Theorem 1. Under Assumptions A1. and B, for the sequence {(xt+1, yt+1)}T
we have

t=1

generated by (7),

BD-Reg

T ≤ O

1 + P1,T + Y1,T

.

(8)

(cid:16)

(cid:17)

Theorem 8 shows that by simply playing (at each round) the minimum of the previous round, i.e.
(7), one can achieve the problem-dependent bound O (1 + P1,T + Y1,T ). It is notable that a similar
upper bound of O(1 + P1,T ) with the full information can be achieved in the single-level setting [86].

3.2 OBO with Partial Information

Perhaps the simplest algorithm that applies to the most general setting of online (single-level)
optimization is OGD [97]: For each t ∈ [T ], play xt ∈ X , observe the cost function ft, and set

xt+1 = ΠX

(cid:0)xt − αt∇ft(xt)(cid:1), αt > 0,

where ΠX is the projection onto X .

We consider a natural extension of OGD to the bilevel setting and show that it enjoys regret
bounds in terms of the path-length of the inner and/or outer minimizer sequences. To do so, we
t (x) is deﬁned in (3b). The computation
need to compute the gradient ∇ft(x, y∗
t (x)) where y∗
of ∇ft(x, y∗
ygt(x, y). More concretely, since
∇ygt(x, y∗

t (x)) = 0, it follows from Assumption A and the implicit function theorem that

t (x)) involves Jacobian ∇2

xygt(x, y) and Hessian ∇2

∇y∗

t (x)∇2

yygt (x, y∗

t (x)) + ∇2

xygt (x, y∗

t (x)) = 0,

which together with the chain rule gives

∇ft(x, y∗

t (x)) = ∇xft (x, y∗
+ ∇y∗

t (x))

t (x)∇yft (x, y∗

t (x)) .

6

(9a)

(9b)

Algorithm 1 : OAGDOAGDOAGD for bilevel regret minimization
Require: (x1, y1) ∈ X × Rd2; w, T, {Kt}T

t=1 ∈ N; stepsizes {(αt, βt) ∈ R2

++}T

t=1

; and {ui}w−1
i=0

with

1 = u0 ≥ u1 . . . uw−1 > 0.

1: for t = 1 to T do
z1
t ← yt
2:
for k = 1 to Kt do

3:

4:

5:

6:

zk+1
t ← zk
end for
yt+1 ← zKt+1
t
xt+1 ← ΠX

7:
8: end for

t − βt∇zgt(xt, zk
t )

(cid:2)xt − αt ˜∇Ft,u(xt, yt+1)(cid:3)

The exact gradient ∇ft(x, y∗

t (x) that is not available in
general. Hence, it is not possible to utilize gradient-type algorithms to minimize bilevel regret. In
this work, inspired by oﬄine bilevel [29] and online single-level [40, 6] optimization, we deﬁne a new
time-averaged hypergradient as a surrogate of ∇ft(x, y∗
t (xt) by yt ∈ Rd2 and
using the history of the gradients as follows.

t (x)) requires information about y∗

t (x)) by replacing y∗

Deﬁnition 2 (Time-Averaged Hypergradient). Given a window size w ∈ [T ], let {ui}w−1
i=0
be a positive decreasing sequence with u0 = 1. Let Ft,u(x, y) := 1/W (cid:80)w−1
i=0 uift−i(x, y) with
W = (cid:80)w−1
i=0 ui and the convention ft ≡ 0 for t ≤ 0. Let Mt(x, y) be a solution to the following
equation:

yygt (x, y) = 0.
Then, the time-averaged hypergradient with window size w is deﬁned as

xygt (x, y) + Mt(x, y)∇2

∇2

where

˜∇Ft,u(x, y) :=

1
W

w−1
(cid:88)

i=0

ui ˜∇ft−i(x, y),

˜∇ft(x, y) := ∇xft(x, y) + Mt(x, y)∇yft(x, y).

(10a)

(10b)

(10c)

Remark 3. One can notice that the gradients of the loss functions from the w most recent rounds
of play are used to deﬁne the hypergradient. Speciﬁcally, by setting ui = 1, it averages a sliding
window of the online hypergradients at their corresponding parameters over a window at each update
iteration. If we set ui = γi for some γ ∈ (0, 1), it assigns more weights to the most recent values,
and it gives the exponential averaged of the hypergradients.

The pseudo-code of online alternating gradient descent (OAGD) method is presented in Algo-
rithm 1. This algorithm is very simple to implement. At each timestep t ∈ [T ], OAGD alternates
between the gradient update on yt and the time-averaged projected hyper-gradient on xt. One can
notice that the alternating update in Algorithm 1 serves as a template of running OGD on online
bilevel problems. In OAGD, w and ui are tunable parameters. Remark 3 and Theorem 9 provide a
suggested value for them. Intuitively, the value of w captures the level of averaging (smoothness)
of the outer gradient at round t. We note that OAGD is similar to single-level time-smoothing
OGD-type methods for the outer variable update [40, 6] and OGD with multiple local updates
[90]. Also, without the inner variable and by setting the window size w = 1, our OAGD reduces to
OGD-type methods such as [86]. It should be mentioned that w > 1 is not required for our bilevel

7

dynamic and static regret minimization. However, evaluations in Section 4 reveal that smoothing
hyper-gradient provides a performance boost over the case w = 1. Finally, we note that for w = 1,
Algorithm 1 is similar to the alternating gradient methods [29, 46, 17, 51] for solving oﬄine bilevel
problems.

Lemma 4. Under Assumption A, for all t ∈ [T ], x, ˙x, ¨x ∈ X , and y ∈ Rd2, we have

(cid:107) ˜∇ft(x, y) − ∇ft(x, y∗(x))(cid:107) ≤ Mf (cid:107)y − y∗(x)(cid:107) ,

(cid:107)∇ft( ˙x, y∗( ˙x)) − ∇ft(¨x, y∗(¨x))(cid:107) ≤ Lf (cid:107) ˙x − ¨x(cid:107) ,
t ( ˙x) − y∗
t (¨x)(cid:107) ≤ Ly (cid:107) ˙x − ¨x(cid:107) .

(cid:107)y∗

Here, Ly = O(κg), Mf = O(κ2

g), and Lf = O(κ3

g).

We are now ready to state the main results of this section. For ease of presentation, in Theorems 5

and 8, we set w = 1.

Theorem 5 (Strongly-Convex). Suppose Assumptions A and B hold. Further, assume ft is
strongly convex with parameter µf and βt = β = 2/((cid:96)g,1 + µg) for all t ∈ [T ]. Then, Algorithm 1
guarantees the following.
(I) If αt = α = 4c
µf
and all t ∈ [T ], then

0.5(κg+1) log (cid:0)12M 2

and Kt = K ≥

, for some c ≤ µ2

c )+2(cid:1)(cid:109)

f +2L2
y)

f /(2L2

f (1+ 1

(cid:108)

BD-Reg

T ≤ O

(cid:16)

1 +

1
α

P2,T

+ α

T
(cid:88)

t=1

(cid:107)∇ft(x∗

t , y∗

t (x∗

t ))(cid:107) + αY2,T

(cid:17)

.

(II) If αt = 2
µf t

and Kt = K ≥

(cid:108)

0.5(κg + 1) log (cid:0) 72L2

yM 2
f
µf

+ µf
2

(cid:1)(cid:109)

, for all t ∈ [T ], then

BS-Reg

T ≤ O

(cid:16)

log T + ¯Y2,T

(cid:17)

,

(12)

(13)

where ¯Y2,T = (cid:80)T

t=2

(cid:13)
(cid:13)y∗

t−1(x∗) − y∗

t (x∗)(cid:13)
2 is the static variant of Y2,T .
(cid:13)

Theorem 5 states that with ﬁxed stepsizes (α, β) and Kt = O(κg), Algorithm 1 can achieve a
problem-dependent bilevel regret bound. We note that the number of inner loops Kt is set to be a
constant depending on the condition number of the function. One may ask whether we can obtain a
tighter bound by using a larger Kt for all t ∈ [T ]. Unfortunately, according to our analysis, even
for suﬃcently large Kt → ∞, which means gt is minimized exactly, the regret bound can only be
improved by a constant factor and the order remains the same. It should be mentioned that in (12),
the ﬁrst two terms are outer optimization errors that are common to OGD methods [90, 15]. The
third term is the inner optimization error.

Corollary 6. Under the same setting as Theorem 5(I),
(I) If ft is non-negative for each t ∈ [T ], then

BD-Reg

T ≤ O

(cid:16)

1 + P2,T /α + α(FT + Y2,T )

(cid:17)

,

(14)

where FT := (cid:80)T

t=1 ft(x∗

t , y∗

t (x∗

t )).

8

(II) If (cid:80)T

t=1 (cid:107)∇ft(x∗

t , y∗

t (x∗

t ))(cid:107) = O(1 + P2,T /α + αY2,T ), then
(cid:16)

BD-Reg

T ≤ O

1 + P2,T /α + αY2,T

(cid:17)

.

(15)

t , y∗

Corollary 6 naturally interpolates between the single level and bilevel regret. In the case when
Y2,T = 0, Eq. (14) recovers the single-level regret for the strongly convex, smooth, and nonegative
losses similar to [93, 75]. We note that if the minimizers {x∗
lie in the interior of the domain
t }T
X , we have (cid:107)∇ft(x∗
t ))(cid:107) = 0 for all t ∈ [T ], which implies the O (1 + P2,T /α + αY2,T ) regret
bound. Some interesting conclusions can be derived if we consider speciﬁc rates of variability in (15).
• Oﬀ-line functions: If ft = f and gt = g, we have P2,T = Y2,T = 0 and it follows that the regret
grows at a rate O(1). This gives convergence rates for oﬄine bilevel gradient methods [51, 46, 17].
• Logarithmic regret: If the diﬀerence between consecutive inner and outer arguments decreases
as 1/t, we have that P2,T = O(log T ) and Y2,T = O(log T ) and the regret grows at a logarithmic
rate.

t (x∗

t=1

The following theorem provides the lower bound Ω(P2,T + Y2,T ) for OBO.

Theorem 7 (Lower Bound). For any OBO algorithm, there always exists a sequence of smooth
and strongly convex functions {(ft, gt)}T

such that

t=1

BD-Reg

T = Ω(1 + P2,T + Y2,T ).

Theorems 5 and 7 state that with ﬁxed stepsizes (α, β) and Kt = O(κg), Algorithm 1 can achieve

an optimal bilevel dynamic regret bound.

The following theorem provides the regret bounds for online convex functions {ft}T

t=1

.

Theorem 8 (Convex). Suppose Assumptions A and B hold. Further, assume ft is convex, βt =
β = 2/((cid:96)g,1 + µg), and Kt ≥ (cid:6)0.5(κg + 1) log 4t2(cid:7) for all t ∈ [T ]. Then, Algorithm 1 satisﬁes the
following.
(I) If ∃ (x∗

t )) ∈ X × Rd2 such that ∇ft(x∗

t )) = 0 and αt = α ≤ 1/(2L2

t , y∗

t , y∗

t (x∗

t (x∗

f ), then

(II) If αt = D
√
(cid:96)f,0

t

, then

BD-Reg

T ≤ O

(cid:16)

1 + P1,T + Y1,T + Y2,T

(cid:17)

.

BS-Reg

T ≤ O

(cid:16)√

T + ¯Y1,T + ¯Y2,T

(cid:17)

,

(16)

(17)

where ¯Yp,T = (cid:80)T

t=2

(cid:13)
(cid:13)y∗

t−1(x∗) − y∗

t (x∗)(cid:13)
p is the static variant of Yp,T .
(cid:13)

From Theorem 8, we see that Algorithm 1 achieves an O(1 + P1,T + Y1,T + Y2,T ) dynamic regret
for a sequence of loss functions that satisfy Assumption A with only gradient feedback. This is
comparable to that achieved in the full information setting provided in Theorem 1, albeit with
additional conditions (vanishing gradients and Assumptions A2.–A3.). Note that the condition
t )) = 0 is referred to as the vanishing gradient condition, which is widely used in the
∇ft(x∗
analysis of OGD methods in the single-level convex setting [86, Assumption 2]. We also emphasize
that in general Y1,T and Y2,T are not comparable; see Example 2.

t , y∗

t (x∗

9

3.3 Local Regret Minimization

In this section, we consider the problem of online bilevel learning with non-convex outer losses. While
minimizing the regret (3) makes sense for online convex functions {ft}T
, it is not appropriate for
general nonconvex online costs since the global minimization of a non-convex objective is intractable
in general. We address this issue with a combined approach, leveraging optimality criteria and
measures from (oﬄine) non-convex bilevel analysis, together with smoothing of the online part of
the outer objective function similar to [40]. Throughout this section, we set X = Rd1.

t=1

For the sequence {ui}w−1
i=1

local regret:

given in Deﬁnition 2 and for all w ∈ [T ], we deﬁne the following bilevel

BL-Reg

T,u :=

T
(cid:88)

t=1

(cid:13)
(cid:13)∇Ft,u(xt, y∗

t (xt))(cid:13)
2.
(cid:13)

(18)

t (x) ∈ arg miny∈Rd2 gt(x, y), and Ft,u(xt, y∗

t (xt)) = (cid:80)w−1

i=0 uift−i(xt, y∗

t (xt)) with the con-

Note that in the single-level setting and for ui = 1, (18) reduces to the single-level local regert

Here, y∗
vention ft ≡ 0 for t ≤ 0.

[40]:

L-Reg

T,u :=

T
(cid:88)

t=1

(cid:107)∇Ft,u(xt)(cid:107)2 .

We also introduce a new measure of variation of the function y∗

t (x) as follows:

HT :=

T
(cid:88)

t=2

sup
x∈X

(cid:107)y∗

t−1(x) − y∗

t (x)(cid:107)2.

(19)

(20)

t (x) are bounded, we have HT = O(T ); as such,
An immediate observation is that if the functions y∗
any regret guarantee stated in terms of HT automatically translates to O(T ). The main reason that
we introduce HT instead of working with a more uniform constant is to account for cases where HT
is naturally small. For example, in the online hyper-parameter problem mentioned in Section 4, HT
corresponds to the variability of the labels over T which can be signiﬁcantly small for a good range
of hyperparameters x ∈ X .
Assumption C. For any t ∈ [T ], |ft(x, y)| ≤ M for some ﬁnite constant M > 0.

Assumption C is commonly used in the literature [40]. The following theorem shows that

Algorithm 1 achieves a sublinear local regret.

Theorem 9 (Non-convex). Let {(ft, gt)}T
t=1
satisfying Assumptions A-C. If αt = α ≤ 1
3Lf

1) log max(6c, W )

(cid:109)
, for c = 3(1 + L2

yM 2

, βt = β =

2
(cid:96)g,1+µg
f α2) and all t ∈ [T ]. Then, Algorithm 1 guarantees

and Kt = K ≥

,

be the sequence of functions presented to Algorithm 1,

(cid:108)

0.5(κg +

(21)

BL-Reg

T,u ≤ O

(cid:16) T
αW

+

αHT
W

(cid:17)

.

The regret can be made sublinear in T if HT = O(T ) and w is selected accordingly.

Theorem 9 is tightly aligned with the state-of-the-art bounds in many nonconvex optimization
settings. For example, in the single-level setting (i.e., HT = 0), the results in Theorem 9 recover
the results in [40], albeit for a general weight sequence. In the oﬄine case ft = f , we recover a
convergence guarantee for gradient methods for nonconvex bilevel optimization.

10

4 Experimental Results

In this section, we conduct preliminary experiments to evaluate the proposed algorithm.

Hyper-parameter optimization (HO) is the process of ﬁnding a best set of hyper-parameter values
that cannot be learned using the training data alone [28]. An HO problem can be formulated as
a bilevel optimization problem; the outer objective f (y∗(x); Dval) aims to minimize the validation
loss with respect to the hyper-parameters x, and the inner objective g(x, y; Dtr) gives a learning
algorithm by minimizing the training loss with respect to the model parameters y.

Example 3. [Online HO for Dynamic Regression] Consider online hyper-parameter learning for
dynamic regression as follows: the players (follower and leader) sequentially receive data feature vectors
and then predict each label and hyper-parameters. We focus on the problem of online (regularized)
t } for t = 1, . . . T are received
regression, where at each round t, new samples (at, bt) ∈ Dt := {Dval
with at ∈ Rd2 being the feature vector and bt ∈ R being the corresponding label, and the potential
correct decision can change abruptly. Speciﬁcally, we consider an S-stage scenario where (x∗
s(x∗
s))
are potentially best decisions for s-th stage, i.e., for all s ∈ [S]:

s, y∗

, Dtr

t

(cid:80)Ts

s ∈ argmin
x∗
s.t. y∗

x∈X
s(x) ∈ argmin
y∈Rd2

(cid:1)

t=1 f (cid:0)y∗
(cid:80)Ts

t

s(x); Dval
t=1 g (cid:0)x, y; Dtr

t

(cid:1) .

(22)

Following the OBO setting introduced in Section 1.1, at each round t, given a sample (at, bt) ∈ Dtr
,
t
t yt based on the learned inner and outer
the follower is required to make the prediction by a(cid:62)
models (xt−1, yt−1) ∈ X × Rd2; then, as a consequence the follower suﬀers a loss g(xt−1, yt; Dtr
t ) =
. The leader then receives the
1/2(a(cid:62)
feedback of the inner model, i.e., yt, predicts the new hyper-parameter xt using a validation sample
t yt(xt) − bt)2. This process repeats across
(at, bt) ∈ Dval
t
T = {T1, . . . , TS} rounds.

t C(xt−1)yt, where C(x) := diag(exp(xi))d1
i=1

, and suﬀers the loss f (yt(xt); Dval

t yt − bt)2 + y(cid:62)

t ) = 1/2(a(cid:62)

Figure 1: Performance of OAGD on online hyper-parameter learning. OBO with (left) three
comparators and (right) ﬁxed comparator.

4.1 Synthetic data

The synthetic data is generated as follows: To simulate the distribution changes, we generate the
s)) ∈ Rd1 × Rd2 is the underlying model for
output according to bt = a(cid:62)

s) + (cid:15)t, where (x∗

s, y∗

s(x∗

s(x∗

t y∗

11

0.10.20.30.40.51041001051010P2,T+Y2,Tw=Tw=100w=10.10.20.30.40.5104100101102103104w=Tw=100w=1s-th stage, and (cid:15)t ∈ [0, 0.1] is the random noise. We consider two setups for the underlying model:
(i) there are three changes (S = 3) in the minimizers (x∗
s)), and (ii) the underlying model is
ﬁxed (S = 1), i.e., (x∗, y∗(x∗)) = (x∗
s)) for all t ∈ [T ]. The time horizon, the outer and inner
dimensions are set to T = 5000, d1 = 1 and d2 = 5, respectively.

s, y∗

s, y∗

s(x∗

s(x∗

We used a grid-search of parameters in our experiments. For the grid-search setting, we
select the best performing parameters Kt, α, and β from a grid {5, 10} × {0.001, 0.01, 0.1, 0.5} ×
{0.001, 0.01, 0.1, 0.5}. In our implementation, we consider LBFGS [83] to solve (10). Following [6],
the smoothing (averaging) parameter γ is set to 0.9.

All algorithms have been run on a Mac machine equipped with a 1.8 GHz Intel Core i5 processor

and 8 GB RAM.

Figure 1 demonstrates the variable variation P2,T + Y2,T and regret bound of OAGD with three
diﬀerent window size w ∈ {1, 100, T } on the synthetic data sets. We observe that OAGD with
w = T performs the best, and there is a gradual decrease in performance to w = 100 and w = 1. In
addition, by increasing the number of optimal points (left ﬁgure), the algorithm restarts more often
and yields larger regret. Finally, Figure 2 shows (on a 1-dimensional hyperparamter setting) that
the performance of OAGD is comparable to the performance of the oﬄine HO algorithm [28].

Figure 2: Performance of OAGD and HO for learning hyper-parameter x1.

4.2 Real data

The real data sets are taken from UCI machine learning repository [5]: Facebook Comment Volume
(m = 40949, d2 = 53), Insurance Company Benchmark (m = 9000, d2 = 85), Student Performance
for a math exam (m = 395, d2 = 32), and BodyFat (m = 336, d2 = 14). The m samples are divided
into 3 groups (training, validation and test samples) with the same sample size (cid:100)m/3(cid:101). Hence,
T{val,tr,te} = (cid:100)m/3(cid:101).

We consider an smoothing variant of the elastic net problem [67], i.e., the setting where the inner

objective in (22) is replaced with

g (cid:0)x, y; Dtr
t

(cid:1) =

1
2

(a(cid:62)

t y − bt)2 + y(cid:62)C(¯x)y +

d2(cid:88)

i=1

exp(xi)(y2

i + µ2)

1
2 ,

where ¯x = (xd2+1, · · · , xd1) and µ > 0 is a smoothing parameter.

Following [67], the parameter µ is initialized as µ1 = 1 and updated at each time step by
(cid:1). For the sake of comparison, we also implement a hyper-parameter

µt+1 = min (cid:0)0.99µt, 10µ1.3

t

12

OAGDOAGDOAGD (w = T ) OAGDOAGDOAGD (w = 10)
Errte

time

time

HOHOHO

Errte

Data [5]

name

Facebook
Insurance
Student
BodyFat

d1
54
86
273
15

Errte
10.14
6.085
2.29 89.14
1.08
1.051
0.594
0.072

4.103
88.340
1.039
0.121

–
8.15
4.15 87.312
1.142
1.89
0.130
0.051

bayesopt
bayesopt
bayesopt

Errte
7.093
98.130
20.134
45.814

time

500.000
500.000
500.000
455.290

time

–
4.473
71.775
0.695

Table 2: Comparison of OAGD, HO [28], and bayesopt in terms of squared test error (Errte) and
CPU times (sec).

selection method that uses Bayesian optimization. We use bayesopt in MATLAB with “MaxOb-
jectiveEvaluations=30” for Bayesian optimization. At each iteration of bayesopt, we make use of
MATLAB built-in solver fmincon.

Table 2 summarizes the obtained results of applying the two types of Algorithms 1, HO [28]
and bayesopt to (22). The algorithms with “500” seconds stopped at the time-limit. Moreover,
OAGD (w = ˆT ) stands for Algorithm 1 using the time averaged gradient approach with the window
size w = ˆT and HO does the one using fmincon. The hyphens “–” in the line of Facebook for HO
indicate that it terminates with an infeasible solution of the averaged problem (22). One can see
that OAGD (w = T ) is stable against new data compared to OAGD (w = 10), and computationally
eﬃcient compared to HO [28] and bayesopt.

5 Conclusion

This paper studies online bilevel optimization and provides regret guarantees under diﬀerent convexity
assumptions on the time-varying objective functions. In particular, we propose a new class of online
bilevel algorithms that are capable of leveraging smoothness and provide regret bound in terms of
problem-dependent quantities such as the path-length of the comparator sequence. A limitation of
our work is that we consider the local optimal follower, i.e. y∗
t (x) ∈ arg miny∈Rd2 gt(x, y), which can
be pessimistic in some applications. One direction for future work is to generalize our analysis to
t=1 gt(x, y).
the setting where y∗(x) ∈ arg miny∈Rd2

(cid:80)T

Acknowledgment

This work was supported by ARO YIP award W911NF1910027 and NSF CAREER award CCF-
1845076.

References

[1] Jacob Abernethy, Peter L Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal strategies

and minimax lower bounds for online convex games. 2008.

[2] Naman Agarwal, Alon Gonen, and Elad Hazan. Learning in non-convex games with an

optimization oracle. In Conference on Learning Theory, pages 18–29. PMLR, 2019.

13

[3] Eitaro Aiyoshi and Kiyotaka Shimizu. A solution method for the static constrained stackelberg
problem via penalty method. IEEE Transactions on Automatic Control, 29(12):1111–1114,
1984.

[4] Faiz A Al-Khayyal, Reiner Horst, and Panos M Pardalos. Global optimization of concave
functions subject to quadratic constraints: an application in nonlinear bilevel programming.
Annals of Operations Research, 34(1):125–147, 1992.

[5] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.

[6] Sergul Aydore, Tianhao Zhu, and Dean P Foster. Dynamic local regret for non-convex online

forecasting. Advances in Neural Information Processing Systems, 32, 2019.

[7] Dheeraj Baby and Yu-Xiang Wang. Online forecasting of total-variation-bounded sequences.

arXiv preprint arXiv:1906.03364, 2019.

[8] Juhan Bae and Roger B Grosse. Delta-stn: Eﬃcient bilevel optimization for neural networks
using structured response jacobians. Advances in Neural Information Processing Systems,
33:21725–21737, 2020.

[9] Jonathan F Bard. Practical bilevel optimization: algorithms and applications, volume 30.

Springer Science & Business Media, 2013.

[10] Luca Bertinetto, Joao F Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with
diﬀerentiable closed-form solvers. In International Conference on Learning Representations,
2018.

[11] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations

research, 63(5):1227–1244, 2015.

[12] Olivier Bousquet and Manfred K Warmuth. Tracking a small set of experts by mixing past

posteriors. Journal of Machine Learning Research, 3(Nov):363–396, 2002.

[13] Jerome Bracken and James T McGill. Mathematical programs with optimization problems in

the constraints. Operations Research, 21(1):37–44, 1973.

[14] Sébastien Bubeck, Gilles Stoltz, Csaba Szepesvári, and Rémi Munos. Online optimization in

x-armed bandits. Advances in Neural Information Processing Systems, 21, 2008.

[15] Ting-Jui Chang and Shahin Shahrampour. On online optimization: Dynamic regret analysis of
strongly convex and smooth problems. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 35, pages 6966–6973, 2021.

[16] Tianyi Chen, Yuejiao Sun, Quan Xiao, and Wotao Yin. A single-timescale method for stochastic
bilevel optimization. In International Conference on Artiﬁcial Intelligence and Statistics, pages
2466–2488. PMLR, 2022.

[17] Tianyi Chen, Yuejiao Sun, and Wotao Yin. Closing the gap: Tighter analysis of alternating
stochastic gradient methods for bilevel problems. Advances in Neural Information Processing
Systems, 34, 2021.

[18] Tianyi Chen, Yuejiao Sun, and Wotao Yin. A single-timescale stochastic bilevel optimization

method. arXiv preprint arXiv:2102.04671, 2021.

14

[19] Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin,
and Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning
Theory, pages 6–1. JMLR Workshop and Conference Proceedings, 2012.

[20] Mathieu Dagréou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. A framework for bilevel
optimization that enables stochastic and global variance reduction algorithms. arXiv preprint
arXiv:2201.13409, 2022.

[21] Amit Daniely, Alon Gonen, and Shai Shalev-Shwartz. Strongly adaptive online learning. In

International Conference on Machine Learning, pages 1405–1411, 2015.

[22] Justin Domke. Generic methods for optimization-based modeling. In Artiﬁcial Intelligence and

Statistics, pages 318–326. PMLR, 2012.

[23] Thomas Arthur Edmunds and Jonathan F Bard. Algorithms for nonlinear bilevel mathematical

programs. IEEE transactions on Systems, Man, and Cybernetics, 21(1):83–89, 1991.

[24] Matthias Feurer and Frank Hutter. Hyperparameter optimization. In Automated machine

learning, pages 3–33. Springer, Cham, 2019.

[25] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-
tation of deep networks. In International Conference on Machine Learning, pages 1126–1135.
PMLR, 2017.

[26] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning.

In International Conference on Machine Learning, pages 1920–1930. PMLR, 2019.

[27] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse
gradient-based hyperparameter optimization. In International Conference on Machine Learning,
pages 1165–1173. PMLR, 2017.

[28] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil.
Bilevel programming for hyperparameter optimization and meta-learning. In International
Conference on Machine Learning, pages 1568–1577. PMLR, 2018.

[29] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv

preprint arXiv:1802.02246, 2018.

[30] Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration
complexity of hypergradient computation. In International Conference on Machine Learning,
pages 3748–3758. PMLR, 2020.

[31] Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. On stochastic moving-average

estimators for non-convex optimization. arXiv preprint arXiv:2104.14840, 2021.

[32] Eric C Hall and Rebecca M Willett. Online convex optimization in dynamic environments.

IEEE Journal of Selected Topics in Signal Processing, 9(4):647–662, 2015.

[33] Nadav Hallak, Panayotis Mertikopoulos, and Volkan Cevher. Regret minimization in stochastic
non-convex learning via a proximal-gradient approach. In International Conference on Machine
Learning, pages 4008–4017. PMLR, 2021.

15

[34] Pierre Hansen, Brigitte Jaumard, and Gilles Savard. New branch-and-bound rules for linear
bilevel programming. SIAM Journal on scientiﬁc and Statistical Computing, 13(5):1194–1217,
1992.

[35] Keegan Harris, Hoda Heidari, and Steven Z Wu. Stateful strategic regression. Advances in

Neural Information Processing Systems, 34:28728–28741, 2021.

[36] Elad Hazan.

Introduction to online convex optimization. Foundations and Trends® in

Optimization, 2(3-4):157–325, 2016.

[37] Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimiza-

tion, 2(3-4):157–325, 2016.

[38] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex

optimization. Machine Learning, 69(2):169–192, 2007.

[39] Elad Hazan and Comandur Seshadhri. Adaptive algorithms for online decision problems. In

Electronic colloquium on computational complexity (ECCC), volume 14, 2007.

[40] Elad Hazan, Karan Singh, and Cyril Zhang. Eﬃcient regret minimization in non-convex games.

In International Conference on Machine Learning, pages 1433–1441. PMLR, 2017.

[41] Amélie Héliou, Matthieu Martin, Panayotis Mertikopoulos, and Thibaud Rahier. Online non-
convex optimization with imperfect feedback. Advances in Neural Information Processing
Systems, 33:17224–17235, 2020.

[42] Amélie Héliou, Matthieu Martin, Panayotis Mertikopoulos, and Thibaud Rahier. Zeroth-order
non-convex learning via hierarchical dual averaging. In International Conference on Machine
Learning, pages 4192–4202. PMLR, 2021.

[43] Mark Herbster and Manfred K Warmuth. Tracking the best expert. Machine learning, 32(2):151–

178, 1998.

[44] Mark Herbster and Manfred K Warmuth. Tracking the best linear predictor. Journal of Machine

Learning Research, 1(281-309):10–1162, 2001.

[45] Steven CH Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. Online learning: A comprehensive

survey. Neurocomputing, 459:249–289, 2021.

[46] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale framework
for bilevel optimization: Complexity analysis and application to actor-critic. arXiv preprint
arXiv:2007.05170, 2020.

[47] Feihu Huang and Heng Huang. Biadam: Fast adaptive bilevel optimization methods. arXiv

preprint arXiv:2106.11396, 2021.

[48] Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Online
optimization: Competing with dynamic comparators. In Artiﬁcial Intelligence and Statistics,
pages 398–406, 2015.

[49] Kaiyi Ji and Yingbin Liang. Lower bounds and accelerated algorithms for bilevel optimization.

ArXiv, abs/2102.03926, 2021.

16

[50] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Provably faster algorithms for bilevel optimization

and applications to meta-learning. ArXiv, abs/2010.07962, 2020.

[51] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and
enhanced design. In International Conference on Machine Learning, pages 4882–4892. PMLR,
2021.

[52] Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang.
A near-optimal algorithm for stochastic bilevel optimization via double-momentum. Advances
in Neural Information Processing Systems, 34, 2021.

[53] Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. Advances in

Neural Information Processing Systems, 17, 2004.

[54] Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-armed bandits in metric spaces. In
Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 681–690,
2008.

[55] Walid Krichene, Maximilian Balandat, Claire Tomlin, and Alexandre Bayen. The hedge
algorithm on a continuum. In International Conference on Machine Learning, pages 824–832.
PMLR, 2015.

[56] Junyi Li, Bin Gu, and Heng Huang. Improved bilevel model: Fast and optimal algorithm with

theoretical guarantee. arXiv preprint arXiv:2009.00690, 2020.

[57] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Diﬀerentiable architecture search.

arXiv preprint arXiv:1806.09055, 2018.

[58] Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level
optimization for learning and vision from a uniﬁed perspective: A survey and beyond. arXiv
preprint arXiv:2101.11517, 2021.

[59] Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A generic ﬁrst-order
algorithmic framework for bi-level programming beyond lower-level singleton. In International
Conference on Machine Learning, pages 6305–6315. PMLR, 2020.

[60] Yibing Lv, Tiesong Hu, Guangmin Wang, and Zhongping Wan. A penalty function method
based on kuhn–tucker condition for solving linear bilevel programming. Applied Mathematics
and Computation, 188(1):808–813, 2007.

[61] Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter opti-
mization through reversible learning. In International conference on machine learning, pages
2113–2122. PMLR, 2015.

[62] Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. Online optimiza-
tion in dynamic environments: Improved regret rates for strongly convex problems. In 2016
IEEE 55th Conference on Decision and Control (CDC), pages 7195–7201. IEEE, 2016.

[63] Gregory M Moore. Bilevel programming algorithms for machine learning model selection.

Rensselaer Polytechnic Institute, 2010.

[64] Parvin Nazari, Esmaeil Khorram, and Davoud Ataee Tarzanagh. Adaptive online distributed
optimization in dynamic environments. Optimization Methods and Software, pages 1–25, 2019.

17

[65] Parvin Nazari, Davoud Ataee Tarzanagh, and George Michailidis. Dadam: A consensus-based
distributed adaptive gradient method for online optimization. arXiv preprint arXiv:1901.09109,
2019.

[66] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer

Science & Business Media, 2003.

[67] Takayuki Okuno, Akiko Takeda, Akihiro Kawana, and Motokazu Watanabe. On lp-
hyperparameter learning via bilevel nonsmooth optimization. Journal of Machine Learning
Research, 22(245):1–47, 2021.

[68] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International

conference on machine learning, pages 737–746. PMLR, 2016.

[69] Juergen Schmidhuber, Jieyu Zhao, and MA Wiering. Simple principles of metalearning. Technical

report IDSIA, 69:1–23, 1996.

[70] Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-
In The 22nd International Conference on Artiﬁcial

propagation for bilevel optimization.
Intelligence and Statistics, pages 1723–1732. PMLR, 2019.

[71] Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and

trends in Machine Learning, 4(2):107–194, 2011.

[72] Shai Shalev-Shwartz and Yoram Singer. Online learning: Theory, algorithms, and applications.

2007.

[73] Chenggen Shi, Jie Lu, and Guangquan Zhang. An extended kuhn–tucker approach for linear

bilevel programming. Applied Mathematics and Computation, 162(1):51–63, 2005.

[74] Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. A review on bilevel optimization: from classical
to evolutionary approaches and applications. IEEE Transactions on Evolutionary Computation,
22(2):276–295, 2017.

[75] Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates.

Advances in neural information processing systems, 23, 2010.

[76] Heinrich von Stackelberg et al. Theory of the market economy. 1952.

[77] Arun Sai Suggala and Praneeth Netrapalli. Online non-convex learning: Following the perturbed

leader is optimal. In Algorithmic Learning Theory, pages 845–861. PMLR, 2020.

[78] Masashi Sugiyama and Motoaki Kawanabe. Machine learning in non-stationary environments:

Introduction to covariate shift adaptation. MIT press, 2012.

[79] Davoud Ataee Tarzanagh, Mingchen Li, Christos Thrampoulidis, and Samet Oymak. Fednest:
Federated bilevel, minimax, and compositional optimization. arXiv preprint arXiv:2205.02215,
2022.

[80] Kyriakos G Vamvoudakis and Frank L Lewis. Online actor–critic algorithm to solve the

continuous-time inﬁnite horizon optimal control problem. Automatica, 46(5):878–888, 2010.

[81] Heinrich Von Stackelberg and Stackelberg Heinrich Von. The theory of the market economy.

Oxford University Press, 1952.

18

[82] Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Tracking the best expert in non-stationary stochastic

environments. Advances in neural information processing systems, 29:3972–3980, 2016.

[83] Stephen Wright, Jorge Nocedal, et al. Numerical optimization. Springer Science, 35(67-68):7,

1999.

[84] Yue Frank Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A ﬁnite-time analysis of two time-
scale actor-critic methods. Advances in Neural Information Processing Systems, 33:17617–17628,
2020.

[85] Junjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimization.

Advances in Neural Information Processing Systems, 34, 2021.

[86] Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi. Tracking slowly moving clairvoyant:
In International

Optimal dynamic regret of online learning with true and noisy gradient.
Conference on Machine Learning, pages 449–457. PMLR, 2016.

[87] Lijun Zhang, Tie-Yan Liu, and Zhi-Hua Zhou. Adaptive regret of convex and smooth functions.

In International Conference on Machine Learning, pages 7414–7423, 2019.

[88] Lijun Zhang, Shiyin Lu, and Tianbao Yang. Minimizing dynamic regret and adaptive regret

simultaneously. arXiv preprint arXiv:2002.02085, 2020.

[89] Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic environments.

In Advances in neural information processing systems, pages 1323–1333, 2018.

[90] Lijun Zhang, Tianbao Yang, Jinfeng Yi, Jing Rong, and Zhi-Hua Zhou. Improved dynamic

regret for non-degenerate functions. In NIPS, 2017.

[91] Lijun Zhang, Tianbao Yang, Zhi-Hua Zhou, et al. Dynamic regret of strongly adaptive methods.

In International Conference on Machine Learning, pages 5882–5891, 2018.

[92] Peng Zhao and Lijun Zhang. Improved analysis for dynamic regret of strongly convex and

smooth functions. In Learning for Dynamics and Control, pages 48–59. PMLR, 2021.

[93] Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou. Dynamic regret of convex and
smooth functions. Advances in Neural Information Processing Systems, 33:12510–12520, 2020.

[94] Kai Zheng, Haipeng Luo, Ilias Diakonikolas, and Liwei Wang. Equipping experts/bandits with

long-term memory. Advances in neural information processing systems, 2019.

[95] Wei Zhou, Yiying Li, Yongxin Yang, Huaimin Wang, and Timothy Hospedales. Online meta-
critic learning for oﬀ-policy actor-critic methods. Advances in Neural Information Processing
Systems, 33:17662–17673, 2020.

[96] Yihan Zhou, Victor Sanches Portella, Mark Schmidt, and Nicholas Harvey. Regret bounds
without lipschitz continuity: online learning with relative-lipschitz losses. Advances in Neural
Information Processing Systems, 33:15823–15833, 2020.

[97] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
Proceedings of the 20th international conference on machine learning (icml-03), pages 928–936,
2003.

19

Supplementary Materials for “Online Bilevel Optimization: Regret Analysis of
Online Alternating Gradient Methods ”

Table of Contents

A Related Work

B Proof for OBO with Full Information

20

23

B.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

C Lower Bound

24

C.1 Proof of Theorem 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

D Proof for Convex OBO with Partial Information

24

D.1 Auxiliary Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

D.2 Proof of Theorem 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

D.3 Proof of Corollary 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

D.4 Proof of Theorem 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
D.5 Discussion on the number of inner iterations Kt and the window size w:

. . . . . . 35

E Proof for Nonconvex OBO with Partial Information

35

E.1 Auxiliary Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

E.2 Proof of Theorem 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

F Useful Results and Facts

40

A Related Work

Online learning and stochastic optimization are closely related. The key diﬀerence between them is
that at each round t of the online optimization the loss function can be arbitrarily chosen by the
adversary. Given the vastness of the online and stochastic optimization, we do not strive to provide
an exhaustive review. Instead, we mainly focus on a few representative works on online static and
worst-case dynamic regret minimization, as well as bilevel optimization; see [45, 36] and [74, 58] for
survey of online and bilevel optimization, respectively.

t=1

In single-level online optimization, the goal of the player
• Static Regret Minimization:
(learner) is to choose a sequence {xt}T
such that her regret is minimized. There are diﬀerent
notions of regret in the literature including static, dynamic (deﬁned in (1)), and adaptive [37, 71, 72].
In the case of static regret, x∗
t=1 ft(x). This type of regret is
t
well-studied in the literature of online learning [37, 71, 72]. [97] shows that online gradient descent
T ) regret bound for convex (possibly nonsmooth) functions. [38] improves
(OGD) provides a O(
this bound to O(d1 log T ), and O(log T ) for exponentially concave and strongly-convex functions,
respectively. These results were also shown to be minimax optimal [1]. [96] provides regret bounds for
online learning algorithms under relative Lipschitz and/or relative strongly-convexity assumptions.

is replaced by x∗ ∈ arg minx∈X

(cid:80)T

√

20

In addition to exploiting convexity of the online functions, there are recent studies improving
static regret by incorporating smoothness [75, 19]. These problem-dependent bounds can safeguard
the worst-case minimax rate yet be much better in easy cases of online learning problems (e.g., loss
functions with a small deviation). For instance, [75] shows that for convex smooth non-negative
t=1 ft(x(cid:63)) and
functions, OGD can achieve an O(
(cid:80)T
GT ) bound, where
x∗ ∈ arg minx∈X
GT = (cid:80)T
t=2 supx∈X (cid:107)∇ft−1(x) − ∇ft(x)(cid:107)2 is the gradient variation. These bounds are particularly
favored in slowly changing environments in which the online functions evolve gradually [93].

t=1 ft(x). For convex smooth functions, [19] establishes an O(

FT ) small-loss regret bound, where FT = (cid:80)T

√

√

• Dynamic Regret Minimization: Single-level dynamic regret forces the player to compete
with time-varying comparators, and thus is particularly favored in non-stationary environments [78].
The notion of dynamic regret is also referred to as tracking regret or shifting regret in the prediction
with expert advice setting [43, 44, 12, 82, 94]. There are two kinds of dynamic regret in previous
studies: The universal dynamic regret aims to compare with any feasible comparator sequence
[97, 89], while the worst-case dynamic regret (deﬁned in (1)) speciﬁes the comparator sequence to be
the sequence of minimizers of online functions [11, 48, 62, 86, 90, 6, 93]. We present related works
for the latter case as it is the setting studied in this paper.

It is known that in the worst case, sublinear dynamic regret is not attainable unless one imposes
regularity of some form on the comparator sequence or the function sequence [11, 32, 48].
[86]
shows that OGD enjoys an O((cid:112)T P1,T ) worst-case dynamic regret bound for convex functions when
the path-length P1,T is known. For strongly convex and smooth functions, [62] shows that an
O(P1,T ) dynamic regret bound is achievable. [15] proves that OGD can achieve an O(P2,T ) regret
bound without the bounded gradient assumption. [90] further proposes the online multiple gradient
descent algorithm and proves that the algorithm enjoys an O(min{P1,T , P2,T }) regret bound; this
bound has been recently enhanced to O(min{P1,T , P2,T , VT }) by an improved analysis [92], where
VT = (cid:80)T
t=2 supx∈X |ft−1(x) − ft(x)|. [86] further shows that O(P2,T ) rate is attainable for convex
and smooth functions, provided that all the minimizers x(cid:63)
lie in the interior of the domain X . The
t
above results use the path-length (or squared path-length) as the regularity, which is in terms of the
trajectory of the comparator sequence. [64, 65] extend the above results to the distributed settings
and provide dynamic regret bounds in terms of the (cid:96)1 path-length. [11] shows that OGD with a
1/3) regret for convex functions when VT is available, which
restarting strategy attains an O(T 2/3VT
2/3) for the square loss [7].
has been recently improved to O(T 1/3VT

• Adaptive Regret: Adaptive regret [39, 21, 91, 87, 88] is also used to capture the dynamics

in the environment. Speciﬁcally, it characterizes a local version of static regret, where

Regret

T ([r, s]) (cid:44)

s
(cid:88)

t=r

ft(xt) − min
x∈X

s
(cid:88)

t=r

ft(x),

[91] provides a connection between strongly adaptive regret and
for each interval [r, s] ⊆ [T ].
dynamic regret and proposes an adaptive algorithm which can bound the dynamic regret without
prior knowledge of the functional variation. [88] develops a new algorithm which can minimize the
dynamic regret and the adaptive regret simultaneously.

• Local Regret Minimization: Nonconvex online optimization is a more challenging setting
than the convex case. Some notable works in the nonconvex literature include adversarial multi-armed
bandit with a continuum of arms [14, 53, 55, 42, 41] and classical Follow-the-Perturbed-Leader
algorithm with access to an oﬄine non-convex optimization oracle [2, 77, 54]. [40] introduces a local
regret measure based on gradients of the loss to address intractable non-convex online models. Their
regret is local in the sense that it averages a sliding window of gradients and quantiﬁes the objective
of predicting points with small gradients on average. They are motivated by a game-theoretic

21

perspective, where an adversary reveals observations from an unknown static loss. The gradients
of the loss functions from the w most recent rounds of play are evaluated at the current model
parameters xt, and these gradients are then averaged. The motivation behind averaging is two-fold:
(i) a randomly selected update has a small time-averaged gradient in expectation if an algorithm
incurs local regret sublinear in T , and (ii) for any online algorithm, an adversarial sequence of loss
functions can force the local regret incurred to scale with T as O(T /w2).
[33] extends the local
regret minimization to online, non-smooth, non-convex problems. These arguments presented in
[40, 6, 65, 33] inspire our use of local regret for bilevel online optimization.

• (Oﬄine) Bilevel Optimization: Since its ﬁrst formulation by Stackelberg [76] and the ﬁrst
mathematical model by Bracken and McGill [13] there has been a signiﬁcant growth in applications
and developments of bilevel programming. Existing works either reduce the problem into a single-level
optimization problem [3, 23, 4, 34, 73, 60, 63, 74] or apply an (alternating) optimization method to
solve the original problem. The single-level formulations (using the Karush-Kuhn-Tucker (KKT)
conditions or penalty approaches) are generally diﬃcult to solve [74].

Gradient-based approaches are more attractive for bilevel programming due to their simplicity
and eﬀectiveness. This type of approach estimates the hypergradient ∇φ(x) for iterative updates, and
can generally be divided into two categories: approximate implicit diﬀerentiation (AID) and iterative
diﬀerentiation (ITD) classes. ITD-based approaches [61, 27, 25, 30] estimate the hypergradient
∇φ(x) in either a reverse (automatic diﬀerentiation) or forward manner. AID-based approaches [22,
68, 30, 51] estimate the hypergradient via implicit diﬀerentiation. [28] characterized the asymptotic
convergence of a backpropagation-based approach as one of ITD-based algorithms by assuming the
inner-level problem is strongly convex. [70] provided a similar analysis for a truncated backpropagation
scheme. [59, 56] analyzed the asymptotic performance of ITD-based approaches when the inner-level
problem is convex.

Finite-time complexity analysis for bilevel optimization has also been explored. [29] provided a
ﬁnite-time convergence analysis for an AID-based algorithm under diﬀerent loss geometries: φ(·)
is strongly convex, convex or nonconvex, and g(x, ·) is strongly convex. [51] provided an improved
ﬁnite-time analysis for both AID- and ITD-based algorithms under the nonconvex-strongly-convex
geometry. [49] provided the lower bounds on complexity as well as upper bounds under these two
geometries. When the objective functions can be expressed in an expected or ﬁnite-time form,
[29, 51, 46] developed stochastic bilevel algorithms and provided the ﬁnite-time analysis. There
have been subsequent studies on accelerating SGD-type bilevel optimization via momentum and
variance reduction techniques [18, 31, 50, 47] as well. However, a fundamental assumption in all the
aforementioned works is that the cost function does not change throughout the horizon over which
we seek to optimize it.

22

Table 3: Summary of the Notations

Description
Time (round) index
The number of inner iterations at each round t
The total number of rounds
Window size for the averaged-hypergradient
Outer stepsize
Inner stepsize
Leader’s decision at round t
Leader’s objective at round t
Follower’s decision at round t
Follower’s objective at round t
Leader’s optimal decision at round t
Follower’s optimal decision at round t for a given x

xyht, ∇2

yht Gradient, Jacobian, and Hessian of ht

t (x)) w.r.t. (cid:107)y(cid:63)

t (x) − yt(cid:107)

The (2-norm) diameter of X : D = maxx,x(cid:48)∈X (cid:107)x − x(cid:48)(cid:107)
Upper bound on the outer function: |ft| ≤ M
Diﬀerence between each ˜∇ft(x, yt) and ∇ft(x, y∗
Lipschitz constant of y(cid:63)
t (x)
Lipschitz constant of ∇ft(x)
Outer function value at the optimum: (cid:80)T
Path-length of the outer minimizers: (cid:80)T
Path-length of the inner minimizers: (cid:80)T
Inner minimizer funcation variation: (cid:80)T
Online functions variation: (cid:80)T
Online gradients variation: (cid:80)T
(single-level) dynamic regret: (cid:80)T
(single-level) static regret: (cid:80)T
(single-level) local regret: (cid:80)T
Bilevel dynamic regret: (cid:80)T
Bilevel (outer) static regret: (cid:80)T
Bilevel local regret: (cid:80)T

t=1 ft(xt) − (cid:80)T
t=1 ft(xt) − minx∈X
t=1 (cid:107)∇Ft,u(xt)(cid:107)2
t=1 ft(xt, yt) − (cid:80)T

t=1 ft(x(cid:63)
t=1 ft(xt, yt) − minx∈X
t (xt))(cid:107)2

t=1 ft(x(cid:63)
t=2 (cid:107)x(cid:63)
t=2 (cid:107)y(cid:63)
t=2 supx∈X (cid:107)y(cid:63)

t , y(cid:63)
t−1 − x(cid:63)
t−1(x(cid:63)

t=1 (cid:107)∇Ft,u(xt, y∗

t=2 supx∈X |ft−1(x) − ft(x)|
t=2 supx∈X (cid:107)∇ft−1(x) − ∇ft(x)(cid:107)2

t=1 ft(x(cid:63)
t )
(cid:80)T
t=1 ft(x)

t ))

t (x(cid:63)
t (cid:107)p
t−1) − y(cid:63)

t (x(cid:63)
t−1(x) − y(cid:63)

t )(cid:107)p
t (x)(cid:107)2

t , y(cid:63)

t (x(cid:63)
(cid:80)T

t ))

t=1 ft(x, y(cid:63)

t (x))

Notation
t
Kt
T
w
α
β
xt
ft
yt
gt
x∗
t
y∗
t (x)
∇ht, ∇2
D
M
Mf
Ly
Lf
FT
Pp,T
Yp,T
HT
VT
GT
D-RegT
S-RegT
L-RegT
BD-RegT
BS-RegT
BL-RegT

B Proof for OBO with Full Information

B.1 Proof of Theorem 1

Proof. Denote by x∗

0 = x1 and y∗

0(x∗

0) = y1. Then, it follows from Assumption A1. that

T
(cid:88)

t=1

ft(xt, yt) −

T
(cid:88)

t=1

ft(x∗

t , y∗

t (x∗

t )) =

T
(cid:88)

t=1

ft(x∗

t−1, y∗

t−1(x∗

t−1)) −

T
(cid:88)

t=1

ft(x∗

t , y∗

t (x∗

t ))

≤ (cid:96)f,0

T
(cid:88)

t=1

(cid:0)(cid:107)x∗

t−1 − x∗

t (cid:107) + (cid:107)y∗

t−1(x∗

t−1) − y∗

t (x∗

t )(cid:107)(cid:1)

= (cid:96)f,0(cid:107)x1 − x∗

1(cid:107) + (cid:96)f,0

T −1
(cid:88)

t=1

(cid:107)x∗

t − x∗

t+1(cid:107)

+ (cid:96)f,0(cid:107)y∗

1(x1) − y∗

1(x∗

1)(cid:107) + (cid:96)f,0

T −1
(cid:88)

t=1

(cid:107)y∗

t (x∗

t ) − y∗

t+1(x∗

t+1)(cid:107)

where the last inequality uses Lemma 4 and Assumption B. This completes the proof.

(cid:4)

≤ (cid:96)f,0D + (cid:96)f,0LyD + (cid:96)f,0(P1,T + Y1,T ),

23

C Lower Bound

C.1 Proof of Theorem 7

Proof. We randomly generate a sequence of functions {(ft, gt)}T
distribution of online functions such that for any bilevel algorithm A, we have E [BD-Reg
E[Pp,T + Yp,T ]. Speciﬁcally, for any bilevel algorithm A that generates a sequence of {(xt, yt)}T
we consider the expected regret as follows:

and show that there exists a
T ] ≥
,
t=1

t=1

E [BD-Reg

T ] = E

(cid:34) T

(cid:88)

t=1

ft(xt, yt) −

T
(cid:88)

t=1

ft(x(cid:63)

t , y(cid:63)

t (x(cid:63)

(cid:35)
t ))

.

Let d1 = d2 = 1. For each round t, we randomly sample vectors a(1)
distribution N (0, 1√
t

t
) and N (0, 1), respectively. For t = 1, . . . , T , let

and a(2)

t

from the Gaussian

It follows from (3b) that y∗

, x∗

t = a(1)

t

, and y∗

t (x∗

t + a(2)

t

. Hence,

(cid:17)(cid:17)2

(cid:16)

+ 4

x − a(1)

t

(cid:17)2

, and

t + a(2)
a(1)
t
(cid:17)

x + a(2)

t

y.

ft(x, y) = 4

(cid:16)

y −

(cid:16)

gt(x, y) =

(cid:16)

y2 −

1
2
t (xt) = xt + a(2)
t
(cid:20)(cid:16)

T
(cid:88)

E

E [BD-Reg

T ] = 4

(cid:16)

t + a(2)
a(1)

t

(cid:17)(cid:17)2(cid:21)

yt −

t ) = a(1)
(cid:20)(cid:16)

+ E

xt − a(1)

t

(cid:17)2(cid:21)

(A.1)

t=1
T
(cid:88)

≥ 4

(cid:20)(cid:16)

(cid:18)

2E

a(1)
t

(cid:17)2(cid:21)

+ E

(cid:20)(cid:16)

a(2)
t

(cid:17)2(cid:21)(cid:19)

t=1
1
T

≥ 8(

+ log T ) + 4T.

1 + a(2)
(cid:20)(cid:16)

E

Here, the ﬁst inequality follows from the independence of a(1)
Lemma 16(I).

t

and a(2)

t

; and the last inequality uses

If T = 1, then E[Pp,T + Yp,T ] = E[(a(1)

1 )2] + E[(a(1)

1 )2] = 3. For T ≥ 2, we obtain

E [Pp,T + Yp,T ] =

=

T
(cid:88)

t=2
T
(cid:88)

t=2

(cid:20)(cid:16)

E

t − a(1)
a(1)

t−1

(cid:17)2(cid:21)

+

T
(cid:88)

t=2

t + a(2)
a(1)

t − (a(1)

t−1 + a(2)
t−1)

(cid:17)2(cid:21)

(cid:18)

E

(cid:20)(cid:16)

2

a(1)
t

(cid:17)2(cid:21)

+ E

(cid:20)(cid:16)

a(1)
t−1

(cid:17)2(cid:21)(cid:19)

+ E

(cid:17)2(cid:21)

(cid:20)(cid:16)

a(2)
t

+ E

(cid:20)(cid:16)

a(2)
t−1

(cid:17)2(cid:21)

(A.2)

≤ 2(1 + log(T − 1) + log T ) + 2(T − 1).

Here, the second equality follows from the independence of a(1)
inequality uses Lemma 16(I).

t

and a(2)

t

for all t ∈ [T ]; and the

Now, it follows from (A.1) and (A.2) that for any given algorithm A, there exists a sequence of
(cid:4)

such that E [BD-Reg

T ] ≥ E[Pp,T + Yp,T ].

functions {(ft, gt)}T

t=1

D Proof for Convex OBO with Partial Information

In this section, we provide dynamic regret bound for OBO with partial information. Speciﬁcally,
we derive a problem-dependent regret bound for Algorithm 1. We ﬁrst provide some supporting
lemmas.

24

D.1 Auxiliary Lemma

The following lemma characterizes the inner estimation error (cid:107)yt+1 − y∗
t (xt)(cid:107), where yt+1 is the
inner variable update via Algorithm 1. It shows that by applying inner gradient descent multiple
times at each round t, we are able to extract more information from each inner function and therefore
are more likely to obtain a tight bound for the inner error in terms of the path-length Yp,T . We
should emphasize that Algorithm 1, though presented with an outer loop and an inner loop, reduces
to a single-loop method when the parameter Kt is set to 1. Allowing Kt to be greater than 1 leads
to better generality and better performance in both convex and nonconvex settings.

Lemma 10. Suppose Assumption A holds. In Algorithm 1, choose

β =

2
(cid:96)g,1 + µg

and Kt =

(cid:24) (κg + 1) log ρ−2
2

t

(cid:25)

(A.3)

for some positive deceasing sequence {ρt}T
rithm 1:

t=1

L1. If ρ1 < (cid:112)1/2, we have

. Then, for the sequence {yt}T +1
t=1

generated by Algo-

T
(cid:88)

t=1

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 ≤

+

6
1 − 2ρ2
1

L2. If ρ1 < 1, we get

ρ2
1
1 − 2ρ2
1
(cid:32)

L2
y

(cid:107)y1 − y∗

1(x1)(cid:107)2

T
(cid:88)

t=1

t (cid:107)xt − x∗
ρ2

t (cid:107)2 +

T
(cid:88)

t=2

t (cid:107)y∗
ρ2

t−1(x∗

t−1) − y∗

t (x∗

t )(cid:107)2

(cid:33)

.

T
(cid:88)

t=1

(cid:107)yt+1 − y∗

t (xt)(cid:107) ≤

ρ1
1 − ρ1

(cid:107)y1 − y∗

1(x1)(cid:107) +

1
1 − ρ1

(cid:0)Ly

T
(cid:88)

t=1

ρt(cid:107)xt − x∗
t (cid:107)

+

T
(cid:88)

t=2

ρt(cid:107)y∗

t−1(x∗

t−1) − y∗

t (x∗

t )(cid:107)(cid:1).

Proof. We show L1.. The proof of L2. follows similarly. Since β =
2.1.11], we have

2
(cid:96)g,1+µg

, from [66, Theorem

(cid:107)zK+1 − y∗

t (xt)(cid:107)2 ≤

(cid:18)

1 −

1
κg + 1

(cid:19)2Kt

(cid:107)z1 − y∗

t (xt)(cid:107)2.

By our assumption Kt = (cid:100)0.5(κg + 1) log ρ−2

(cid:18)

1 −

1
κg + 1

Then, using (A.4) and (A.5), we have

t (cid:101) which implies that
(cid:19)
(cid:18)

(cid:19)2Kt

≤ exp

−

2Kt
κg + 1

≤ ρ2
t .

(cid:107)zK+1 − y∗

t (xt)(cid:107)2 = (cid:107)yt+1 − y∗

t (xt)(cid:107)2 ≤ ρ2

t (cid:107)yt − y∗

t (xt)(cid:107)2.

Hence,

T
(cid:88)

t=1

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 ≤ ρ2

1(cid:107)y1 − y∗

1(x1)(cid:107)2 +

T
(cid:88)

t=2

t (cid:107)yt − y∗
ρ2

t (xt)(cid:107)2,

25

(A.4)

(A.5)

(A.6)

(A.7)

which implies that

T
(cid:88)

t=2

t (cid:107)yt − y∗
ρ2

t (xt)(cid:107)2 ≤ 2

≤ 2

T
(cid:88)

t=2
T
(cid:88)

t=1

It follows from Lemma 15 that

ρ2
t

(cid:0)(cid:107)yt − y∗

t−1(xt−1)(cid:107)2 + (cid:107)y∗

t−1(xt−1) − y∗

t (xt)(cid:107)2(cid:1)

t (cid:107)yt+1 − y∗
ρ2

t (xt)(cid:107)2 + 2

T
(cid:88)

t=2

t (cid:107)y∗
ρ2

t−1(xt−1) − y∗

t (xt)(cid:107)2.

t (cid:107)y∗
ρ2

t−1(xt−1) − y∗

t (xt)(cid:107)2 ≤ 3ρ2
+ 3ρ2
+ 3ρ2
≤ 3L2
+ 3L2
+ 3ρ2

t (cid:107)y∗
t (cid:107)y∗
t (cid:107)y∗
yρ2
yρ2
t (cid:107)y∗

t (xt) − y∗
t (x∗
t−1(xt−1) − y∗
t−1) − y∗
t−1(x∗
t−1(cid:107)xt−1 − x∗
t (cid:107)xt − x∗
t−1(x∗

t (cid:107)2
t−1) − y∗

t )(cid:107)2
t−1(x∗
t )(cid:107)2
t (x∗
t−1(cid:107)2

t−1)(cid:107)2

t )(cid:107)2,
where the second inequality uses the assumption that ρt ≤ ρt−1 for all t ∈ [T ].

t (x∗

Now, combining (A.8), (A.7), and (A.9), we obtain

T
(cid:88)

t=1

(cid:0)1 − 2ρ2
t

(cid:1) (cid:107)yt+1 − y∗

t (xt)(cid:107)2 ≤ ρ2

1(cid:107)y1 − y∗

1(x1)(cid:107)2 + 6L2
y

T
(cid:88)

t=1

t (cid:107)xt − x∗
ρ2

t (cid:107)2

+ 6

T
(cid:88)

t=2

t (cid:107)y∗
ρ2

t−1(x∗

t−1) − y∗

t (x∗

t )(cid:107)2,

which together with our assumption that ρt ≤ ρt−1 completes the proof.

D.1.1 Proof of Lemma 4

Proof. The proof follows from [29, Lemma 2.2] by setting

Ly :=

(cid:96)g,1
µg

= O(κg),

Mf := (cid:96)f,1 +

Lf := (cid:96)f,1 +

(cid:96)g,1(cid:96)f,1
µg

+

(cid:16)

(cid:96)f,0
µg

(cid:96)g,2 +

(cid:96)g,1((cid:96)f,1 + Mf )
µg

+

(cid:96)f,0
µg

(cid:17)

(cid:96)g,1(cid:96)g,2
µg

(cid:18)

(cid:96)g,2 +

where the other constants are deﬁned in Assumption A.

D.2 Proof of Theorem 5

D.2.1 Proof of Theorem 5 (I)

= O(κ2

g),
(cid:19)

(cid:96)g,1(cid:96)g,2
µg

= O(κ3

g),

(A.8)

(A.9)

(A.10)

(cid:4)

(cid:4)

Proof. Recall the update rule of Algorithm 1 (with w = 1): xt+1 = ΠX
the Pythagorean theorem, we get

(cid:0)xt − α ˜∇ft(xt, yt+1)(cid:1). From

(cid:107)xt+1 − x∗

t (cid:107)2 ≤

(cid:13)
(cid:13)
2
(cid:13)ΠX (xt − α ˜∇ft(xt, yt+1)) − x∗
(cid:13)
(cid:13)
t )
(cid:13)
t ) + α2 (cid:13)
(cid:13)
2
˜∇ft(xt, yt+1)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t (cid:107)2 − 2α ˜∇ft(xt, yt+1)(cid:62)(xt − x∗

= (cid:107)xt − x∗

(A.11)

,

26

which implies that

∇ft(xt, y∗

t (xt))(cid:62)(xt − x∗

t ) ≤

+

+

+

t (xt))(cid:107)2
1
2α

t (cid:107)2 −

(cid:107)∇ft(xt, y∗

α
2
1
(cid:107)xt − x∗
2α
(cid:13)
α
(cid:13)∇ft(xt, y∗
(cid:13)
2
(cid:68)
∇ft(xt, y∗

(cid:107)xt+1 − x∗

t (cid:107)2

t (xt)) − ˜∇ft(xt, yt+1)

(cid:13)
2
(cid:13)
(cid:13)
t (xt)) − ˜∇ft(xt, yt+1), x∗

(cid:69)

.

t − xt

Next, we upper bound each term of (A.12)–(A.15).
• Bounding (A.12): Observe that

(cid:107)∇ft(xt, y∗

t (xt))(cid:107)2 = (cid:107)∇ft(xt, y∗
≤ (cid:107)∇ft(xt, y∗
+ 2(cid:0)∇ft(xt, y∗
f (cid:107)xt − x∗
≤ L2

t , y∗
t (xt)) − ∇ft(x∗
t , y∗
t (xt)) − ∇ft(x∗
t , y∗
t (xt)) − ∇ft(x∗
t (cid:107)2 + 2DLf (cid:107)∇ft(x∗

t (x∗
t (x∗
t (x∗
t , y∗

t )) + ∇ft(x∗
t ))(cid:107)2
t ))(cid:1)(cid:62)∇ft(x∗
t (x∗
t ))(cid:107) ,

t , y∗

t (x∗

t ))(cid:107)2

t , y∗

t (x∗

t ))

where the last inequlaity is obtained from Assumption B and Lemma 11.
• Bounding (A.13): It follows from Lemma 15 that

(A.13) ≤

−

1
2α
1
2α

t (cid:107)2

(cid:107)xt − x∗
(cid:18)

(1 − c) (cid:13)

(cid:13)xt+1 − x∗

t+1

(cid:13)
2 +
(cid:13)

(cid:18)

1 −

(cid:19)

1
c

(cid:13)
(cid:13)x∗

t+1 − x∗
t

(cid:13)
2
(cid:13)

(cid:19)

.

for any constant c > 0.
• Bounding (A.14) and (A.15): From Lemma 4 and Lemma 15, we have

(A.12)

(A.13)

(A.14)

(A.15)

(A.16)

(A.17)

(A.14) + (A.15) ≤

+

≤

c
t (cid:107)2
(cid:107)xt − x∗
2α
(cid:19) (cid:13)
(cid:18)
α
(cid:13)∇ft(xt, y∗
(cid:13)
2
(cid:18)
αM 2
f
2

(cid:107)xt − x∗

t (cid:107)2 +

c
2α

1 +

1
c

t (xt)) − ˜∇ft(xt, yt+1)

(cid:13)
2
(cid:13)
(cid:13)

1 +

(cid:19)

1
c

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 .

(A.18)

• Bounding (cid:80)T
t (xt))(cid:62)(xt − x∗
t=1 ∇ft(xt, y∗
(A.18), and summing over t ∈ [T ], we obtain

t ): Combining (A.12)–(A.15) with (A.16), (A.17) and

T
(cid:88)

t=1

∇ft(xt, y∗

t (xt))(cid:62)(xt − x∗

t ) ≤ αDLf

T
(cid:88)

(cid:107)∇ft(x∗

t , y∗

t (x∗

t ))(cid:107)

t=1
αL2
f
2

(cid:33) T

(cid:88)

t=1

(cid:107)xt − x∗

t (cid:107)2

(cid:19) T

(cid:88)

− 1

(cid:13)
(cid:13)x∗

t+1 − x∗
t

(cid:13)
2
(cid:13)

(cid:32)

c
α

+

1
2α

(cid:18) 1
c

+

+

+

+

αM 2
f
2

(cid:18)

1 +

1
c

(cid:16)

(1 − c)

1
2α

27

t=1
(cid:19) T

(cid:88)

(cid:107)yt+1 − y∗

t (xt)(cid:107)2

t=1
(cid:107)x1 − x∗

1(cid:107)2 − (cid:13)

(cid:13)x∗

T +1 − x∗
T

(A.19)

2(cid:17)

(cid:13)
(cid:13)

.

By our assumptions, the conditions of Lemma 10 hold. Hence, substituting Lemma 10 in (A.19) and
rearranging it, we get

T
(cid:88)

t=1

∇ft(xt, y∗

t (xt))(cid:62)(xt − x∗

t ) ≤

(cid:32)

c
α

+

αL2
f
2

(cid:33) T

(cid:88)

t=1

(cid:107)xt − x∗

t (cid:107)2

1
2α

(cid:18) 1
c

+

(cid:19) T

(cid:88)

− 1

t=1

(cid:13)
(cid:13)x∗

t+1 − x∗
t

(cid:13)
2 +
(cid:13)

(1 − c)
2α

(cid:107)x1 − x∗

1(cid:107)2

T
(cid:88)

+αDLf

(cid:107)∇ft(x∗

t , y∗

t (x∗

t ))(cid:107) −

(1 − c)
2α

(cid:13)
(cid:13)x∗

T +1 − x∗
T

(cid:13)
2
(cid:13)

(A.20)

t=1
(cid:18)

1 +

αM 2
f
2

+

(cid:19) 6ρ2

(cid:16)

1 − 2ρ2

1
c

(cid:107)y1 − y∗

1(x1)(cid:107)2 + 2L2
y

T
(cid:88)

(cid:107)xt − x∗

t (cid:107)2

+2L2

y(cid:107)xT +1 − x∗

T +1(cid:107)2 + Y2,T + (cid:107)y∗

T (x∗

T ) − y∗

t=1
T +1(x∗

(cid:17)
T +1)(cid:107)

.

• Completing the proof of Theorem 5: Since {ft}T
we have

t=1

are strongly convex with parameter µf ,

BD-Reg

T =

≤

T
(cid:88)

t=1
T
(cid:88)

t=1

Observe that

ft(xt, y∗

t (xt)) −

T
(cid:88)

t=1

ft(x∗

t , y∗

t (x∗

t ))

∇ft(xt, y∗

t (xt))(cid:62)(xt − x∗

t ) −

µf
2

T
(cid:88)

t=1

(cid:107)xt − x∗

t (cid:107)2 .

c ≤

µ2
f
f + L2
y)

2(L2

and α =

4c
µf

implies

c
α

(cid:32)

ρ ≤

1
(cid:0)1 + 1
c

(cid:1) + 2

12M 2
f

(cid:33) 1
2

implies 2M 2
f

α
2

+

(cid:18)

1 +

(L2

f + L2

y) ≤ µf .

(cid:19) 6ρ2

1 − 2ρ2 ≤ 1.

1
c

(A.21a)

(A.21b)

We note that the above choice of c and ρ satisﬁes the constraint c ∈ (0, 1) and the condition of
Lemma 10, respectively. Therefore, from (A.20) and (A.21), we obtain

BD-Reg

T ≤ αDLf

T
(cid:88)

t=1

(cid:107)∇ft(x∗

t , y∗

t (x∗

t ))(cid:107)

(cid:18)

+

+

1
2α
(cid:16)
α
4

(1 − c) (cid:107)x1 − x∗

1(cid:107)2 + (

− 1)P2,T + (

1
c
yD2 + (cid:107)y∗

1
c
T ) − y∗

(cid:107)y1 − y∗

1(x1)(cid:107)2 + 2L2

T (x∗

T +1(x∗

T +1)(cid:107)2 + Y2,T

− 1) (cid:13)

(cid:13)x∗

T +1 − x∗
T

(cid:19)

(cid:13)
2
(cid:13)

(cid:17)

.

(A.22)

28

It follows from (A.21a) that c ≤

µ2
f
f +L2
y)

2(L2

for α = 4c/µf . Now, let

(cid:18)

c0(α) := 2

2 +

(cid:19)

,

2
c

c2(α) := αDLf ,
(cid:18) 1 − c
2α
2L2

c4(α) :=

+

(cid:16)

α
4

+

1
2α

(cid:18) 1
c

− 1

(cid:19)

− 1

,

(cid:18) 1
c

1
2α
α
4

,

c1(α) :=

c3(α) :=
(cid:19)(cid:19)

D2

yD2 + (cid:107)y1 − y∗

1(x1)(cid:107)2 + (cid:107)y∗

T (x∗

T ) − y∗

T +1(x∗

T +1)(cid:107)2(cid:17)

.

Then,

BD-Reg

T ≤ c1(α)P2,T + c2(α)

T
(cid:88)

t=1

This completes the proof.

D.2.2 Proof of Theorem 5(II)

Proof. From (A.11), we get

(cid:107)∇ft(x∗

t , y∗

t (x∗

t ))(cid:107) + c3(α)Y2,T + c4(α).

(A.23)

(A.24)

(cid:4)

(cid:107)xt+1 − x∗(cid:107)2 = (cid:107)xt − x∗(cid:107)2 − 2αt ˜∇ft(xt, yt+1)(cid:62)(xt − x∗) + α2
t

(cid:13)
˜∇ft(xt, yt+1)
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

(A.25)

which implies that

∇ft(xt, y∗

t (xt))(cid:62)(xt − x∗) ≤

+

+

(cid:107)∇ft(xt, y∗

αt
2
(cid:13)
αt
(cid:13)∇ft(xt, y∗
(cid:13)
2
(cid:68)
∇ft(xt, y∗

t (xt))(cid:107)2 +

1
2αt

(cid:107)xt − x∗(cid:107)2 −

1
2αt

(cid:107)xt+1 − x∗(cid:107)2

t (xt)) − ˜∇ft(xt, yt+1)

(cid:13)
2
(cid:13)
(cid:13)

(A.26)

t (xt)) − ˜∇ft(xt, yt+1), x∗ − xt

(cid:69)

.

From Lemma 4, for any c > 0, we have

(cid:68)
∇ft(xt, y∗

t (xt)) − ˜∇ft(xt, yt+1), x∗ − xt

(cid:69)

≤

c
2

(cid:107)xt − x∗(cid:107)2 +

M 2
f
2c

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 .

(A.27)

Combining (A.26) and (A.27), we get

∇ft(xt, y∗

t (xt))(cid:62)(xt − x∗) ≤

αt
2

(cid:107)∇ft(xt, y∗

t (xt))(cid:107)2 +

−

1
2αt

(cid:107)xt+1 − x∗(cid:107)2 +

M 2
f
2

1
2
(cid:18)

(cid:19)

+ c

(cid:18) 1
αt

(cid:107)xt − x∗(cid:107)2

αt +

(cid:19)

1
c

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 .

Applying the deﬁnition of µf -strong convexity to the pair of points {xt,x∗}, we have

2 (ft(xt, y∗

t (xt)) − ft(x∗, y∗

t (x∗))) ≤ 2∇ft(xt, y∗

t (xt))(cid:62)(xt − x∗) − µf (cid:107)xt − x∗(cid:107)2 .

29

(A.28)

(A.29)

From (A.28) and (A.29), we get

2 (ft(xt, y∗

t (xt)) − ft(x∗, y∗

t (x∗))) ≤ 2∇ft(xt, y∗

≤ αt (cid:107)∇ft(xt, y∗

t (xt))(cid:62)(xt − x∗) − µf (cid:107)xt − x∗(cid:107)2
(cid:18) 1
αt

+ c − µf

(cid:19)

t (xt))(cid:107)2 +
(cid:18)

(cid:19)

−

1
αt

(cid:107)xt+1 − x∗(cid:107)2 + M 2
f

αt +

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 .

1
c

(cid:107)xt − x∗(cid:107)2

(A.30)

Note that

Then,

(cid:32)

ρ ≤

1
f (α1 + 1

c ) + 2c

12L2

yM 2

(cid:33) 1
2

implies 2L2

yM 2

f (α1 +

1
c

)

6ρ2

1 − 2ρ2 ≤ c.

(A.31)

(cid:18)

αt +

M 2
f

1
c

(cid:19) T

(cid:88)

t=1

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 ≤M 2
f

(cid:18)

α1 +

1
c

(cid:19) 6ρ2

(cid:16)

1 − 2ρ2

(cid:107)y1 − y∗

1(x1)(cid:107)2 + 2L2
y

T
(cid:88)

(cid:107)xt − x∗

t (cid:107)2

+2L2

y(cid:107)xT +1 − x∗

T +1(cid:107)2 + Y2,T + (cid:107)y∗

T (x∗

T ) − y∗

t=1
T +1(x∗

T +1)(cid:107)

T
(cid:88)

≤ c

(cid:107)xt − x∗

t (cid:107)2 +

t=1
+ (cid:107)xT +1 − x∗

T +1(cid:107)2 +

Y2,T + +

(cid:107)y1 − y∗

c
2L2
y
T +1(x∗)(cid:107).
T (x∗) − y∗

(cid:107)y∗

c
2L2
y
c
L2
y

1(x1)(cid:107)2

(cid:17)

(A.32)

Summing from t = 1 to T , setting c = µf /4, and αt = 2
µf t

(deﬁne 1
α0

:= 0), we have

T
(cid:88)

2

ft(xt, y∗

t (xt)) − ft(x∗, y∗

t (x∗))

t=1
T
(cid:88)

≤

t=1
T
(cid:88)

t=1

+

(cid:107)xt − x∗(cid:107)2

(cid:18) 1
αt

−

1
αt−1

+ c −

(cid:19)

µf
2

αt (cid:107)∇ft(xt, y∗

t (xt))(cid:107)2 + M 2
f

T
(cid:88)

t=1

(cid:107)yt+1 − y∗

t (xt)(cid:107)2

(cid:18)

αt +

(cid:19)

1
c

(A.33)

T
(cid:88)

αt +

T
(cid:88)

≤ (cid:96)2

f,0

(cid:107)xt − x∗(cid:107)2

(cid:18) 1
αt

−

1
αt−1

+ 2c −

(cid:19)

µf
2

t=1

+

Y2,T +

c
2L2
y
+ (cid:107)xT +1 − x∗

t=1
c
2L2
y
T +1(cid:107)2 +

(cid:107)y1 − y∗

1(x1)(cid:107)2

c
L2
y

(cid:107)y∗

T (x∗) − y∗

T +1(x∗)(cid:107).

Let

e1 :=

e2 :=

µf
8L2
y
µf
8L2
y

(cid:0)(cid:107)y1 − y∗

1(x1)(cid:107)2 + (cid:107)y∗

T (x∗) − y∗

T +1(x∗)(cid:107)(cid:1) +

,

e3 :=

(cid:96)2
f,0
µf

.

30

D2 +

1
2

(cid:96)2
f,0
µf

,

(A.34)

Note that by our deﬁnition 1
α0
we obtain

:= 0 and (cid:107)xT +1 − x∗(cid:107)2 ≥ 0. Combining Lemma 16(II) with (A.33),

BS-Reg

T ≤ e3 log T + e2Y2,T + e1.

(A.35)

(cid:4)

D.3 Proof of Corollary 6

The following lemma provides the self-bounding property of smooth functions [75, Lemma 3.1].

Lemma 11. For a non-negative and L–smooth function f : X → R, we have

(cid:107)∇f (x)(cid:107) ≤ (cid:112)4Lf (x), ∀x ∈ X .

It follows from [75, Lemma 2.1 and Lemma 3.1] that the non-negativity is required outside the
domain X , and this is why we require the function f (·) to be non-negative outside the domain X .

Proof.

(cid:112)4(cid:96)f,1ft(x∗

(i) If ft ≥ 0 for t ∈ [T ], then it follows from Lemma 11 that (cid:107)∇ft(x∗

t , y∗

t (x∗

t ))(cid:107) ≤

t (x∗

t , y∗

t )). This together with (12) gives the desired result.
t=1

t ∈ arg minx∈X ft(x) for all t ∈ [T ] and the minimizers {x∗
t , y∗

lie in the interior of
t }T
t ))(cid:107) = 0 which together with (12) gives the desired

t (x∗

(ii) If x∗

the domain X , we have (cid:107)∇ft(x∗
result. If (cid:80)T
t=1 (cid:107)∇ft(x∗

t , y∗

t (x∗

t ))(cid:107) = O(P2,T /α + αY2,T ), then (15) follows from (12).

(cid:4)

D.4 Proof of Theorem 8

D.4.1 Proof of Theorem 8 (I)

Proof. From the update rule of Algorithm 1, we have xt+1 = ΠX
the Pythagorean theorem, we get

(cid:0)xt − α ˜∇ft(xt, yt+1)(cid:1). Now, from

1
2

(cid:107)xt+1 − x∗

t (cid:107)2 ≤

=

1
2
1
2

(cid:107)xt − α ˜∇ft(xt, yt+1)) − x∗

t (cid:107)2

(cid:107)xt − x∗

t (cid:107)2 − α ˜∇ft(xt, yt+1))(cid:62)(xt − x∗

t ) +

1
2

α2(cid:107) ˜∇ft(xt, yt+1))(cid:107)2

(A.36)

Rearranging the above inequality gives

∇ft(xt, y∗

t (xt))(cid:62)(xt − x∗

t ) ≤

+

t (cid:107)2 −

1
(cid:107)xt − x∗
2α
(cid:13)
α
(cid:13)∇ft(xt, y∗
(cid:13)
2

1
2α

(cid:107)xt+1 − x∗

t (cid:107)2

(cid:13)
2
t (xt)) − ˜∇ft(xt, yt+1)
(cid:13)
(cid:13)
t (xt)) − ˜∇ft(xt, yt+1), x∗
t − xt(cid:105)

(A.37a)

(A.37b)

(A.37c)

(A.37d)

+ (cid:104)∇ft(xt, y∗

+

α
2

(cid:107)∇ft(xt, y∗

t (xt))(cid:107)2.

31

Next, we upper bound each term of (A.37).
• Bounding (A.37a): Observe that

1
2α

1
2α
t+1 − x∗

−

(cid:107)x∗

t (cid:107)2 −
(cid:107)xt − x∗
1
2α
1
2α
1
2α
1
2α

(cid:107)x∗

≤

−

≤

(cid:107)xt − x∗

t+1 − x∗

(cid:107)xt − x∗

1
2α

(x∗

t (cid:107)2 −

t (cid:107)2 =
(cid:107)xt+1 − x∗
1
α
(cid:107)xt+1 − x∗
D
α
(cid:107)xt+1 − x∗

t (cid:107)2 +
1
2α
t (cid:107)2 +
1
2α

t − x∗

t (cid:107)2 −

(cid:107)x∗

(cid:107)xt − x∗

t (cid:107)2 −

1
2α

(cid:107)xt+1 − x∗

t+1(cid:107)2

t+1 − xt+1)(cid:62)(x∗

t − x∗

t+1)

t+1(cid:107)2

t+1(cid:107)

t+1(cid:107)2 +

(A.38)

D
α

(cid:107)x∗

t − x∗

t+1(cid:107).

• Bounding (A.37b) and (A.37c): It follows from Lemma 4 and Assumption B that

(cid:104)∇ft(xt, y∗

t (xt)) − ˜∇ft(xt, yt+1), x∗

t − xt(cid:105) ≤ DMf (cid:107)yt+1 − y∗

≤ M 2

f (cid:107)yt+1 − y∗

t (xt)(cid:107) ,
t (xt)(cid:107)2 .

(cid:13)
(cid:13)∇ft(xt, y∗
(cid:13)

(cid:13)
2
t (xt)) − ˜∇ft(xt, yt+1)
(cid:13)
(cid:13)

Hence,

(A.37b) + (A.37c) ≤ DMf (cid:107)yt+1 − y∗

t (xt)(cid:107) + M 2

f (cid:107)yt+1 − y∗

t (xt)(cid:107)2 .

• Bounding (A.37d): By the smoothness of φt(x) = ft(x, y∗

t (x)), for any x ∈ Rd1, we have

φt(x) − φt(xt) ≤ (cid:104)∇φt(xt), x − xt(cid:105) +

Lf
2

(cid:107)x − xt(cid:107)2.

Let x = x(cid:48)
t = xt − 1
Lf
convexity of ft(x, ·),

∇φt(xt) in the above inequality, we have φt(x(cid:48)

t) − φt(xt) ≤ − (cid:107)∇φt(xt)(cid:107)2

2Lf

. By

t ) = φt(x∗
t) ≥ φt(x∗
t ),
where the equality follows from the vanishing gradient condition. Hence,

t ) + ∇φt(x∗

t )(cid:62)(x(cid:48)

t − x∗

φt(x(cid:48)

φt(x∗

t ) − φt(xt) ≤ φt(x(cid:48)

t) − φt(xt) ≤ −

(cid:107)∇φt(xt)(cid:107)2
2Lf

which implies that

(A.37d) ≤ αLf (ft(xt, y∗

t (xt)) − ft(x∗

t , y∗

t (x∗

t ))).

(A.39)

32

• Bounding (cid:80)T

t=1 ∇ft(xt, y∗

t (xt))(cid:62)(xt − x∗

t ): Substituting (A.38) and (A.39) into (A.37), we get

T
(cid:88)

t=1

∇ft(xt, y∗

t (xt))(cid:62)(xt − x∗

t ) ≤

α
2

(cid:107)∇ft(xt, y∗

t (xt))(cid:107)2 +

D(cid:107)x∗

t+1(cid:107)

t − x∗
α

(cid:107)xt − x∗

t (cid:107)2 − (cid:107)xt+1 − x∗

t+1(cid:107)2

2α

αM 2
f
2

+

+

≤

+

+

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 + DMf (cid:107)yt+1 − y∗

t (xt)(cid:107)

(cid:107)∇ft(xt, y∗

α
2
(cid:107)xt − x∗

t (xt))(cid:107)2 +
t (cid:107)2 − (cid:107)xt+1 − x∗

t+1(cid:107)2

D(cid:107)x∗

t+1(cid:107)

t − x∗
α

2α

αM 2
f
2

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 + DMf (cid:107)yt+1 − y∗

t (xt)(cid:107) .

• Completing the proof of Theorem 8: By the convexity of ft, we obtain

ft(xt, y∗

t (xt)) − ft(x∗

t (x∗
t (cid:107)2 − (cid:107)xt+1 − x∗

t , y∗

(cid:107)xt − x∗

t )) ≤ αLf (ft(xt, y∗
t+1(cid:107)2
t − x∗
D(cid:107)x∗
α

+

t (xt)) − ft(x∗
t+1(cid:107)

t , y∗

t (x∗

t )))

2α

αM 2
f
2

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 + DMf (cid:107)yt+1 − y∗

t (xt)(cid:107)

+

+

Using Lemma 16(III)

ρt =

1
2t2

implies

T
(cid:88)

t=1

ρt =

π2
12

and

T
(cid:88)

t=1

ρ2
t =

π4
360

.

This together with Lemma 10 gives

T
(cid:88)

t=1

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 ≤

ρ2
1
1 − 2ρ2
1

(cid:107)y1 − y∗

1(x1)(cid:107)2

(A.40)

(A.41)

(A.42)

+

6
1 − 2ρ2
1

(cid:32)

L2
y

T
(cid:88)

t=1

t (cid:107)xt − x∗
ρ2

t (cid:107)2 +

T
(cid:88)

t=2

t (cid:107)y∗
ρ2

t−1(x∗

t−1) − y∗

t (x∗

t )(cid:107)2

(cid:33)

(A.43a)

≤ (cid:107)y1 − y∗

1(x1)(cid:107)2 +

π4
60

yD2 +
L2

π4
60

L2

yY2,T .

Similarly, we obtain

T
(cid:88)

t=1

(cid:107)yt+1 − y∗

t (xt)(cid:107) ≤

ρ1
1 − ρ1

(cid:107)y1 − y∗

1(x1)(cid:107)

+

1
1 − ρ1

(cid:32)

Ly

T
(cid:88)

t=1

ρt(cid:107)xt − x∗

t (cid:107) +

T
(cid:88)

t=2

ρt(cid:107)y∗

t−1(x∗

t−1) − y∗

t (x∗

t )(cid:107)

(cid:33)

(A.43b)

≤ (cid:107)y1 − y∗

1(x1)(cid:107) +

π2
2

LyD +

π2
2

LyY1,T .

33

Now, let

˙E1(α) :=

(cid:18)

αM 2
f
2

(cid:107)y1 − y∗

1(x1)(cid:107)2 +

π4
60

+ DMf

(cid:18)

(cid:107)y1 − y∗

1(x1)(cid:107) +

π2
2

(cid:19)

yD2
L2
(cid:19)

LyD

.

Substituting (A.43)– (A.43b) into (A.41), we have

(1 − αLf )

T
(cid:88)

t=1

ft(xt, y∗

t (xt)) − ft(x∗

t , y∗

t (x∗

t ))

≤ ˙e1 +

D2
2α

+

D
α

P1,T +

αM 2
f
2

π4
60

L2

yY2,T + DMf

π2
2

LyY1,T .

(A.44)

Let α ≤ 1/(2Lf ) and

˙c1(α) :=

˙c2(α) :=

˙c3(α) :=

˙c4(α) :=

˙E1(α),

αM 2
f
2

1
1 − αLf
1
1 − αLf
1
1 − αLf
D2
2(1 − αLf )α

DMf

π4
60
π2
2

L2
y,

Ly,

+

αL2
f
4Mf

(cid:107)y1 − y∗

1(x1)(cid:107)2 + DMf (cid:107)y1 − y∗

1(x1)(cid:107).

The above deﬁnitions together with (A.44) implies

BD-Reg

T ≤ ˙c1(α)P1,T + ˙c2(α)Y1,T + ˙c3(α)Y2,T + ˙c4(α).

This gives the desired result in (16).

D.4.2 Proof of Theorem 8 (II)

Proof. From the convexity of ft and following Steps (A.36)–(A.37), we have

(A.45)

(A.46)

(cid:4)

ft(xt, y∗

t (xt)) − ft(x∗, y∗

t (x∗)) ≤

+

αt
2
αtM 2
f
2

(cid:107)∇ft(xt, y∗

t (xt))(cid:107)2 +

(cid:107)xt − x∗(cid:107)2 − (cid:107)xt+1 − x∗(cid:107)2
2αt

(A.47)

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 + DMf (cid:107)yt+1 − y∗

t (xt)(cid:107) .

34

Summing from t = 1 to T , setting αt = D
√
(cid:96)f,0

t

(deﬁne 1
α0

:= 0), we have

T
(cid:88)

t=1

αt(cid:107)∇ft(xt, y∗

t (xt))(cid:107)2

(cid:33)

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 + DMf (cid:107)yt+1 − y∗

t (xt)(cid:107)

2ft(xt, y∗

t (xt)) − ft(x∗, y∗
(cid:18) 1
αt

−

1
αt−1

t (x∗))
(cid:19)

(cid:107)xt − x∗(cid:107)2

+

T
(cid:88)

≤

t=1
T
(cid:88)

t=1
D2
αT

+

≤

(cid:32) αtM 2
2

f

T
(cid:88)

+

(cid:32)

(cid:96)2
f,0αt +

αtM 2
f
2

t=1
√

≤ 3D(cid:96)f,0

T + ˙E1(α) +

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 + DMf (cid:107)yt+1 − y∗

t (xt)(cid:107)

α1M 2
f
2

π4
60

L2

yY2,T + DMf

π2
2

LyY1,T ,

(A.48)

(cid:33)

where the last inequality follows from (A.42)–(A.43b).

Let

˙e1 := ˙E1(α),

˙e2 := DMf

π2
2

Ly,

Then, from (A.48) and using the fact that (cid:80)T

˙e3 :=
√

αM 2
f
2
√

˙e4 :=

L2
y,

π4
60
T , we get

t=1 1/

t ≤ 2

BS-Reg

T ≤ ˙e1

√

T + + ˙e2Y1,T + ˙e3Y2,T + ˙e4.

This gives (17).

3
2

D(cid:96)f,0.

(A.49)

(A.50)

(cid:4)

D.5 Discussion on the number of inner iterations Kt and the window size w:
As mentioned before, by using inner gradient descent multiple times, we are able to get more
information from each inner function and obtain a tight bound for the dynamic regret in terms of
Yp,T . However, according to our analysis in Theorem 5 and Theorem 8, even for suﬃciently large
Kt and w > 1, the dynamic regret bound can only be improved by a constant factor. In fact, by
comparing (A.21) and (A.42), we see that Kt = (cid:6)0.5(κg + 1) log ρ−2
(cid:7) with ρt = O(1/t2) and O(1)
give similar regret bound in terms of O(Y2,T ); albeit with diﬀerent constants. A related question is
whether we can reduce the value of Kt by using for example smoothness of ∇yt(x) similar to the
oﬄine bilevel optimization [17] or adopting more advanced optimization techniques, such as the
acceleration or momentum-type gradient methods for both inner and outer updates [66]. These are
open problems to us, and will be investigated as a future work.

t

E Proof for Nonconvex OBO with Partial Information

This section gives regret bounds for online bilevel learning in the non-convex setting.

E.1 Auxiliary Lemma

Lemma 12. Under Assumption A, for all t ∈ [T ], we have

(cid:13)
˜∇Ft,u(x, yt+1) − ∇Ft,u(x, y∗
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
t (x))
(cid:13)

≤ M 2

f (cid:107)yt+1 − y∗

t (x)(cid:107)2 ,

(A.51)

where ˜∇Ft,u is deﬁned in (10b) and Mf is given in (??).

35

Proof. From (10) we get

(cid:13)
˜∇Ft,u(x, yt+1) − ∇Ft,u(x, y∗
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

w−1
(cid:88)

1
W

ui

=

(cid:13)
2
(cid:13)
t (x))
(cid:13)

(cid:16) ˜∇ft−i(xt, yt+1) − ∇ft−i(xt, y∗

(cid:17)
t (xt))

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

1
2W 2

+

1
2W 2

i=0
w−1
(cid:88)

w−1
(cid:88)

i=0

j=0

w−1
(cid:88)

w−1
(cid:88)

i=0

j=0

uiuj

(cid:13)
˜∇ft−i(xt, yt+1) − ∇ft−i(xt, y∗
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
t (xt))
(cid:13)

uiuj

(cid:13)
˜∇ft−j(xt, yt+1) − ∇ft−j(xt, y∗
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
t (xt))
(cid:13)

=

1
W 2

≤

M 2
f
W 2

w−1
(cid:88)

j=0

w−1
(cid:88)

uj

uj

w−1
(cid:88)

i=0

w−1
(cid:88)

(cid:13)
˜∇ft−i(xt, yt+1) − ∇ft−i(xt, y∗
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
t (xt))
(cid:13)

ui

ui (cid:107)yt+1 − y∗

t (xt)(cid:107)2

j=0

i=0
f (cid:107)yt+1 − y∗

= M 2

t (xt)(cid:107)2 .

2 ((cid:107)a(cid:107)2 + (cid:107)b(cid:107)2); the second inequality uses (??);
(cid:4)

Here, the ﬁrst inequality uses the fact that (cid:104)a, b(cid:105) ≤ 1
and the last equality follows since 1/W (cid:80)w−1
i=0 ui = 1.
Similar to Lemma 10, the following lemma characterizes the inner estimation error (cid:107)yt+1−y∗
t (xt)(cid:107),
where yt+1 is the inner variable update via Algorithm 1. In particular, it shows that by applying inner
gradient descent Kt times at each round t, we are able to obtain an error bound in terms of the local
t (xt))(cid:107)2 and the inner solution variation HT = (cid:80)T
t (x)(cid:107)2.
regret (cid:107)∇Ft,u(xt, y∗
Lemma 13. Suppose Assumption A holds. Let Kt = K for all t, and

t=2 supx∈X (cid:107)y∗

t−1(x) − y∗

K =

(cid:24) (κg + 1) log ρ−2
2

(cid:25)

and β =

2
(cid:96)g,1 + µg

,

where ρ < 1/
we have

√

c with c = 3(1 + L2

yM 2

f α2). Then, for the sequence {yt}T +1
t=1

(A.52)

generated by Algorithm 1,

T
(cid:88)

t=1

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 ≤

(cid:18)

1 +

(cid:19)

cρ2
1 − cρ2

ρ2 (cid:107)y1 − y∗

1(x1)(cid:107)2

(cid:13)
˜∇Ft,u(xt, y∗
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
t (xt))
(cid:13)

(A.53)

+

+

6L2
yα2ρ2
1 − cρ2

T
(cid:88)

t=1

3ρ2
1 − cρ2 HT ,

where HT = (cid:80)T
Proof. Following the proof of Lemma 10 and using Assumption A, we have

t=2 supx∈X (cid:107)y∗

t−1(x) − y∗

t (x)(cid:107)2.

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 = (cid:107)zK+1 − y∗

t (xt)(cid:107)2 ≤

(cid:19)2K

(cid:107)z1 − y∗

t (xt)(cid:107)2

(cid:18)

1 −

1
κg + 1
= ρ2 (cid:107)yt − y∗

t (xt)(cid:107)2 ,

(A.54)

36

which implies that

T
(cid:88)

t=1

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 ≤ ρ2(cid:107)y1 − y∗

1(x1)(cid:107)2 + ρ2

T
(cid:88)

t=2

(cid:107)yt − y∗

t (xt)(cid:107)2 .

(A.55)

From Lemme 15 we get

(cid:107)yt − y∗

t (xt)(cid:107)2 ≤3 (cid:0)(cid:107)yt − y∗

t−1(xt−1)(cid:107)2 + (cid:107)y∗

t (xt−1) − y∗

t (xt)(cid:107)2(cid:1)

By a similar argument as in (A.6) and for t ∈ {2, 3, . . . , T }, we have

+ 3(cid:107)y∗

t−1(xt−1) − y∗

t (xt−1)(cid:107)2.

(cid:107)y∗

t (xt−1) − y∗

t (xt)(cid:107)2 ≤ L2

= L2

y(cid:107)xt−1 − xt(cid:107)2
yα2 (cid:13)
(cid:13)
2
˜∇Ft−1,u(xt−1, yt)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
t−1(xt−1))(cid:13)
yα2 (cid:13)
2
(cid:13)∇Ft−1,u(xt−1, y∗
(cid:13)
yα2 (cid:13)
(cid:13)
2
˜∇Ft−1,u(xt−1, yt) − ∇Ft−1,u(xt−1, y∗
(cid:13)
(cid:13)
t−1(xt−1))
(cid:13)
(cid:13)
yα2 (cid:16)(cid:13)
(cid:13)
(cid:13)yt − y∗
yα2 (cid:16)(cid:13)
f ρ2 (cid:13)

t−1(xt−1))(cid:13)
2 + M 2
(cid:13)
f
t−1(xt−1))(cid:13)
2 + M 2
(cid:13)

(cid:13)∇Ft−1,u(xt−1, y∗

(cid:13)∇Ft−1,u(xt−1, y∗

(cid:13)yt−1 − y∗

≤ 2L2

+ 2L2

≤ 2L2

≤ 2L2

2(cid:17)

t−1(xt−1)(cid:13)
(cid:13)
t−1(xt−1)(cid:13)
(cid:13)

2(cid:17)

.

(A.56)

(A.57)

Here, the third and last inequalities follows from Lemma 12 and (A.54), respectively.

Further, from (A.54) and (A.56), we have

(cid:107)yt − y∗

t−1(xt−1)(cid:107)2 ≤ ρ2(cid:107)yt−1 − y∗

t−1(xt−1)(cid:107)2.

Combining (A.56), (A.57), and (A.58), we get

(cid:107)yt − y∗

t (xt)(cid:107)2 ≤ 6L2

yα2(cid:107)∇Ft−1,u(xt−1, y∗

t−1(xt−1))(cid:107)2
t−1(xt)(cid:13)
2
(cid:13)

(cid:13)yt−1 − y∗

+ 3(1 + 2L2
+ 3(cid:107)y∗

yM 2
t−1(xt−1) − y∗

f α2)ρ2 (cid:13)

t (xt−1)(cid:107)2.

Now, from (A.55), we obtain

T
(cid:88)

t=2

(cid:107)yt − y∗

t (xt)(cid:107)2 ≤ 3(1 + 2L2

yM 2

f α2)ρ2

T
(cid:88)

t=2

(cid:13)
(cid:13)yt−1 − y∗

t−1(xt)(cid:13)
2 + 3HT
(cid:13)

T
(cid:88)

+ 6L2

yα2

(cid:107)∇Ft−1,u(xt−1, y∗

t−1(xt−1))(cid:107)2

t=2
≤ 3(1 + 2L2

yM 2

f α2)ρ2(cid:107)y1 − y∗
T
(cid:88)

1(x1)(cid:107)2

+ 3(1 + 2L2

yM 2

f α2)ρ2

(cid:107)yt − y∗

t (xt)(cid:107)2

t=2

+ 6L2

yα2

T −1
(cid:88)

t=1

(cid:107)∇Ft,u(xt, y∗

t (xt))(cid:107)2 + 3HT .

37

(A.58)

(A.59)

(A.60)

Let c = 3(1 + 2L2

yM 2

f α2). From the above inequality, we get Hence,

(cid:0)1 − cρ2(cid:1)

T
(cid:88)

t=2

(cid:107)yt − y∗

t (xt)(cid:107)2 ≤ cρ2(cid:107)y1 − y∗

1(x1)(cid:107)2

+ 6L2

yα2

T
(cid:88)

t=2

(cid:107)∇Ft,u(xt, y∗

t (xt))(cid:107)2 + 3HT .

(A.61)

By our assumption 1 − cρ2 > 0. Now, dividing both side of the above inequality by 1 − cρ2 and
(cid:4)
combining with (A.55) completes the proof.

The following lemma shows that the diﬀerence between the time-averaged function Ft,u computed
t+1(xt+1)) is bounded. This is an extension of the single-level setting to the
and the proof uses the ideas of [6, Lemmas 3.2, 3.3] and [40,

at (xt, y∗
t (xt)) and (y∗
generic weight sequence {ui}w−1
i=0
Theorem 3].

Lemma 14. Let {(ft, gt)}T
t=1
Assumption C. Then, we have

be the sequence of functions presented to Algorithm 1, satisfying

T
(cid:88)

t=1

Proof. Observe that

Ft,u(xt, y∗

t (xt)) − Ft,u(xt+1, y∗

t+1(xt+1)) ≤

M (2T + 1)
W

+ M.

Ft+1,u(xt+1, y∗

t+1(xt+1)) − Ft,u(xt+1, y∗

t+1(xt+1))

=

=

+

1
W
u0
W

1
W

w−1
(cid:88)

ui

(cid:0)ft+1−i(xt+1, y∗

t+1(xt+1)) − ft−i(xt+1, y∗

t+1(xt+1))(cid:1)

i=0
ft+1(xt+1, y∗

t+1(xt+1)) −

uw−1
W

ft−w+1(xt+1, y∗

t+1(xt+1))

(A.62)

w−1
(cid:88)

i=1

[ui − ui−1] ft−i+1(xt+1, y∗

t+1(xt+1)),

where {ui}w−1
i=0

is the weight sequence with 1 = u0 ≥ u1 . . . uw−1 > 0, given in Deﬁnition 2.

Combine Assumption C with (A.62) to get

Ft+1,u(xt+1, y∗

t+1(xt+1)) − Ft,u(xt+1, y∗

t+1(xt+1)) ≤

M (u0 + uw−1)
W

+

M (cid:80)w−1

i=1 [ui−1 − ui]

W

Similarly, using Assumption C, we obtain

≤

2M
W

.

T
(cid:88)

t=1

Ft,u(xt, y∗

t (xt)) − Ft+1,u(xt+1, y∗

t+1(xt+1)) ≤

M
W

+ M.

Summing (A.64) over t ∈ [T ] and combining with (A.63) completes the proof.

38

(A.63)

(A.64)

(cid:4)

E.2 Proof of Theorem 9

Proof. Due to the Lf -smoothness of ft functions, Ft is Lf -smooth as well. Hence,

Ft,u(xt+1, y∗

t+1(xt+1)) − Ft,u(xt, y∗

≤ (cid:104)∇Ft,u(xt, y∗

t (xt)), xt+1 − xt(cid:105) +

t (xt))
Lf
2

≤ −α

(cid:68)
∇Ft,u(xt, y∗

(cid:69)
t (xt)), ˜∇Ft,u(xt, yt+1)

(cid:107)xt+1 − xt(cid:107)2
Lf α2
2

+

(cid:107) ˜∇Ft,u(xt, yt+1)(cid:107)2.

From Lemma 13, we get

(cid:68)
∇Ft,u(xt, y∗

t (xt)), ˜∇Ft,u(xt, yt+1)
(cid:68)

(cid:69)

= − (cid:104)∇Ft,u(xt, y∗

t (xt)), ∇Ft,u(xt, y∗

t (xt))(cid:105)

−

≤ −

≤ −

(cid:107)∇Ft,u(xt, y∗

∇Ft,u(xt, y∗
1
2
1
2

t (xt))

t (xt))(cid:107)2 +

t (xt)), ˜∇Ft,u(xt, yt+1) − ∇Ft,u(xt, y∗
(cid:13)
1
(cid:13)∇Ft,u(xt, yt+1) − ˜∇Ft,u(xt, y∗
(cid:13)
2
M 2
f
2

t (xt))(cid:107)2 +

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 ,

(cid:107)∇Ft,u(xt, y∗

(cid:69)

(cid:13)
(cid:13)
t (xt))
(cid:13)

−

and

(cid:107) ˜∇Ft,u(xt, yt+1)(cid:107)2 ≤ 2(cid:107)∇Ft,u(xt, y∗

t (xt))(cid:107)2 + 2M 2

f (cid:107)yt+1 − y∗

t (xt)(cid:107)2 .

Substituting (A.67) and (A.66) into (A.65) gives

− Lf α2(cid:17)

(cid:16) α
2

(cid:107)∇Ft,u(xt, y∗

t (xt))(cid:107)2 ≤ Ft,u(xt, y∗

t (xt)) − Ft,u(xt+1, y∗
+ Lf α2(cid:17)

(cid:107)yt+1 − y∗

t (xt)(cid:107)2 .

t (xt+1))

+ M 2
f

(cid:16) α
2

From Lemmas 13 and 14, we get

T
(cid:88)

t=1

A(α)(cid:107)∇Ft,u(xt, y∗

t (xt))(cid:107)2 ≤

M (2T + 1)
W

+ M

(A.65)

(A.66)

(A.67)

(A.68)

+ M 2
f

3M 2

+ Lf α2(cid:17) (cid:18)
(cid:16) α
2
f ( α
2 + Lf α2)ρ2
1 − cρ2

HT ,

1 +

(cid:19)

cρ2
1 − cρ2

ρ2 (cid:107)y1 − y∗

1(x1)(cid:107)2

(A.69)

+

where c = 3(1 + L2

yM 2

f α2) and

A(α) :=

α
2

− Lf α2 −

6M 2

yα2ρ2

f L2
1 − cρ2

+ Lf α2(cid:17)

.

(cid:16) α
2

Since 0 < α ≤ 1/(3Lf ), we get α/2 + Lf α2 ≤ α. Further, our choice of ρ = min(1/
together with Lemma 13 implies that

(A.70)

√

√

W )

6c, 1/

A(α) ≥

α
2

−

6M 2

yρ2α3
f L2
1 − cρ2 =

≥

α
2

α
2

−

−

39

yα3
f L2
M 2
3(1 − 1
6 )(1 + L2
1
2
10
5

α ≥

α,

yM 2

f α2)

which gives 1/A(α) ≤ 10/α.

Dividing both side of (A.69) by 10/α and setting

¨c1

:= 10M 2
f (

+ Lf α)

1
2
f ( 1
2 + Lf α)
1 − cρ2

30M 2

¨c2

:=

gives

(cid:18)

1 +

(cid:19)

cρ2
1 − cρ2

ρ2 (cid:107)y2 − y∗

1(x1)(cid:107)2 +

M
α

,

BL-Reg

T,u ≤

10M (2T + 1)
αW

+ ¨c2

HT
W

+ ¨c1.

This completes the proof.

F Useful Results and Facts

(A.71)

(cid:4)

In this part we present several technical lemmas used in the proofs. We start by assembling some
well-known facts about convex and smooth functions.

(F1) (Smoothness): Suppose f (x) is L-smooth. Then, by deﬁnition, the following inequalities

hold for any two points x, y ∈ Rd:

(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107),

f (y) − f (x) ≤ (cid:104)y − x, ∇f (x)(cid:105) +

L
2

(cid:107)y − x(cid:107)2.

Further, if x∗ ∈ arg minx∈Rd f (x), then

(cid:107)∇f (y)(cid:107)2 ≤ 2L(f (y) − f (x∗)).

(A.72a)

(A.72b)

(A.72c)

(F2) (Smoothness and Convexity): Suppose f (x) is L-smooth and convex. Then, the

following holds for any two points x, y ∈ Rd:

(cid:104)∇f (y) − ∇f (x), y − x(cid:105) ≥

1
L

(cid:107)∇f (y) − ∇f (x)(cid:107)2.

(A.73)

(F3) (Strong-Convexity): Suppose f (x) is µ-strongly convex. Then, by deﬁnition, the following

inequality holds for any two points x, y ∈ Rd:

f (y) − f (x) ≥ (cid:104)y − x, ∇f (x)(cid:105) +

(cid:107)y − x(cid:107)2.

µ
2

(A.74a)

Using the above inequality, one can conclude that

(cid:104)∇f (y) − ∇f (x), y − x(cid:105) ≥ µ(cid:107)y − x(cid:107)2.

(A.74b)

Lemma 15. For any set of vectors {xi}m
i=1

with xi ∈ Rd, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

m
(cid:88)

i=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

xi

≤ m

m
(cid:88)

i=1

(cid:107)xi(cid:107)2.

40

(A.75)

Lemma 16. For all T ∈ N,
T ≤ (cid:80)T
(I) log(T ) + 1
√
(II) (cid:80)T
T ;
≤ 2
(III) If 1 < s < ∞, then ξ(s) = (cid:80)T

1√
t

t=1

t=1

1

t ≤ log(T ) + 1;

t=1 1/ts is called the Riemann ξ-function and we have

ξ(2n) = (−1)n+1 (2π)2nB2n
2(2n)!

,

n = 1, 2, 3, . . . ,

where the coeﬃcients B2n are the Bernoulli numbers.

Lemma 17. For any x, y ∈ Rd, the following holds for any c > 0:

(A.76)

(A.77)

(cid:107)x + y(cid:107)2 ≤ (1 + c)(cid:107)x(cid:107)2 +

(cid:18)

1 +

(cid:19)

(cid:107)y(cid:107)2, and

1
c
(cid:18)

1 −

(cid:19)

1
c

(cid:107)z − y(cid:107)2 .

(cid:107)x − y(cid:107)2 ≥ (1 − c) (cid:107)x − z(cid:107)2 +

41

