Deep Reinforcement Learning for Joint Spectrum
and Power Allocation in Cellular Networks

Yasar Sinan Nasir and Dongning Guo
Department of Electrical and Computer Engineering
Northwestern University, Evanston, IL 60208.

0
2
0
2

c
e
D
9
1

]
P
S
.
s
s
e
e
[

1
v
2
8
6
0
1
.
2
1
0
2
:
v
i
X
r
a

Abstract—A wireless network operator typically divides the
radio spectrum it possesses into a number of subbands. In a
cellular network those subbands are then reused in many cells.
To mitigate co-channel interference, a joint spectrum and power
allocation problem is often formulated to maximize a sum-rate
objective. The best known algorithms for solving such problems
generally require instantaneous global channel state information
and a centralized optimizer. In fact those algorithms have not
been implemented in practice in large networks with time-varying
subbands. Deep reinforcement learning algorithms are promising
tools for solving complex resource management problems. A
major challenge here is that spectrum allocation involves discrete
subband selection, whereas power allocation involves continuous
variables. In this paper, a learning framework is proposed to
optimize both discrete and continuous decision variables. Specif-
ically, two separate deep reinforcement learning algorithms are
designed to be executed and trained simultaneously to maximize a
joint objective. Simulation results show that the proposed scheme
outperforms both the state-of-the-art fractional programming
algorithm and a previous solution based on deep reinforcement
learning.

I. INTRODUCTION

In today’s cellular networks, the spectrum is divided into
many subbands. Each cellular device suffers from the co-
channel interference caused by nearby access points which use
the same subbands. The interference can be particularly severe
with dense, irregularly placed access points. Joint subband
selection and transmit power control
tool for
interference mitigation.

is a crucial

For the single band scenario, state-of-the-art optimization
methods such as fractional programming (FP) [1] have been
applied to the power control problem to reach a near-optimal
allocation. We assume that the number of subbands is much
less than the number of cellular devices and that each link
can occupy at most one subband at a time. Therefore, the
joint subband selection and power allocation problem involves
mixed integer programming [2].

Conventional optimization-based schemes such as fractional
programming are model-driven and require a mathematically
tractable and accurate model [3]. Furthermore, such a scheme
is in general centralized and requires instantaneous global
channel state information (CSI). In addition,
it reaches a
solution after several iterations, and its computational com-

This material

is based upon work supported by the National Science
Foundation under Grants No. CCF-1910168 and No. CNS-2003098 as well
as a gift from Intel Incorporation.

plexity does not scale well for a large number of cellular
devices. Therefore, its implementation is quite challenging in
a practical scenario where channel conditions vary rapidly.

Recently, there has been extensive research on model-free
reinforcement learning based transmit power control which is
purely data-driven [3]. For the single band scenario, deep Q-
learning has been considered on a “centralized training and
distributed execution” framework in [4]–[6]. Since deep Q-
learning applies only to discrete power control, the continuous
transmit power domain had to be quantized in [4]–[6] which
may introduce a quantization error as discussed in [7], [8].
Reference [7] ﬁrst showed the performance in [5] can be im-
proved by quantizing the transmit power using logarithmic step
size instead of linear step size, and propose replacing deep Q-
learning algorithm by an actor-critic learning algorithm called
deep deterministic policy gradient that applies to continuous
power control.

the deep Q-network output

For the multiple band scenario, Tan et al. [2] have proposed
to train a single deep Q-network that jointly handles both
subband selection and transmit power control. One major
drawback of this approach is that the action space is the
Cartesian product of available subbands and quantized transmit
power levels. Therefore,
layer
size and the number of state action pairs to be visited for
convergence during training do not scale well with increasing
number of subbands. Moreover,
the joint deep Q-learning
approach is not directly applicable to a problem that includes
both discrete and continuous variables. To overcome these
challenges, we propose a novel approach that consists of two
layers, where the bottom layer is responsible for continuous
power allocation at the physical layer by adapting deep Q-
learning, and the top layer does discrete subband scheduling
using deep deterministic policy gradient. Using simulations,
we evaluate the proposed learning scheme by comparing it
with the joint deep Q-learning approach and the fractional
programming algorithm in terms of convergence rate and
achieved sum-rate performance.

II. SYSTEM MODEL

In this paper, we consider a cellular network with N links
that are placed in K cells and share M subbands. We denote
the set of link and subband indexes by N = {1, . . . , N }
and M = {1, . . . , M }, respectively. Link n is composed of
receiver n and its transmitter n. Transmitter n is placed at the

 
 
 
 
 
 
corresponding cell center that includes receiver n within its
cell boundaries. We consider a fully synchronized time slotted
system with a ﬁxed slot duration of T . We assume that all
transmitters and receivers are equipped with a single antenna.
Due to relative scarcity of available spectrum, K tends to be
much larger than M , i.e., K (cid:29) M . We let each link pick one
subband at the beginning of each time slot.

Similar to [9], our channel model is composed of two parts:
large and small scale fading. For simplicity, we assume that
the large-scale fading is same across all subbands, whereas the
small-scale fading is frequency selective, i.e., different across
all subbands [2]. Within each subband, small-scale fading is
assumed to be block-fading and ﬂat. Let g(t)
n→l,m denote the
downlink channel gain from transmitter n to receiver l on
subband m in time slot t:
(cid:12)
(cid:12)h(t)
(cid:12)

g(t)
n→l,m = βn→l

t = 1, 2, . . . ,

n→l,m

(1)

(cid:12)
(cid:12)
(cid:12)

2

,

where βn→l is the large-scale fading that includes path loss
and log-normal shadowing, and h(t)
n→l,m is the small-scale
Rayleigh fading. We assume that the large-scale fading re-
mains the same through many time slots. Note that in case of
mobile receivers, a time index can be associated with βn→l.
n→l,m [9].
Accordingly, the small-scale fading for each channel follows
a ﬁrst-order complex Gauss-Markov process:
1 − ρ2e(t)
n→l,m = ρh(t−1)
h(t)

We adopt Jake’s fading model

to describe h(t)

n→l,m +

n→l,m,

(2)

(cid:112)

where the correlation between two successive fading blocks
ρ = J0(2πfdT ) with J0(.) being the zeroth-order Bessel
function of the ﬁrst kind depending on the maximum Doppler
frequency fd. Besides, h(0)
n→l,m and the channel innovation
process e(1)
n→l,m, e(2)
n→l,m, . . . are independent and identically
distributed circularly symmetric complex Gaussian random
variables with unit variance. The cells are agnostic to the
speciﬁc fading statistics a priori.
We use binary variables α(t)

n,m to indicate the subband
selection of link n in time slot t. If link n selects subband
m , we have α(t)
n,j = 0, ∀j (cid:54)= m. We denote
the transmit power of transmitter n in time slot t as p(t)
n . The
signal-to-interference-plus-noise at receiver n on subband m
in time slot t is given by

n,m = 1 and α(t)

(cid:80)

γ(t)
n,m =

n→n,mp(t)
n
l→n,mp(t)
l + σ2

n,mg(t)
α(t)
l,mg(t)
l(cid:54)=n α(t)
where σ2 is the additive white Gaussian noise power spectral
density at receiver n. Assuming normalized bandwidth, the
downlink spectral efﬁciency achieved by link n on subband
m during time slot t is

(3)

,

C (t)

n,m = log

(cid:16)

1 + γ(t)
n,m

(cid:17)

.

(4)

III. PROBLEM FORMULATION

Denoting subband and power vectors in time slot t as
(cid:105)(cid:124)
(cid:105)(cid:124)
,

(cid:104)
1 , . . . , p(t)
p(t)

1,2, . . . , α(t)

1,1, α(t)
α(t)

and p(t) =

α(t) =

N,M

(cid:104)

N

respectively, we formulate the sum-rate maximization problem
as [2], [10]:

maximize
p(t),α(t)

subject to

N
(cid:88)

C (t)
n

n ≤ Pmax, ∀n ∈ N ,

n=1
0 ≤ p(t)
α(t)
n,m ∈ {0, 1}, ∀n ∈ N , ∀m ∈ M,
(cid:88)
n,m = 1, ∀n ∈ N ,

α(t)

m∈M

(P1a)

(P1b)

(P1c)

(P1d)

n = (cid:80)M

where C (t)
n,m is link n’s achieved spectral
efﬁciency, and (P1b) restricts the transmit power to be non-
negative and no larger than Pmax.

m=1 C (t)

Unfortunately, (P1) is in general non-convex and requires
mixed integer programming to be carried out for each time slot
as channel varies. Even for a given subband selection α(t), this
problem has been proven to be NP-hard [10]. Conventional
algorithms such as fractional programming are centralized
these algorithms still require many
solutions to (P1), but
iterations to converge and their computational complexity does
not scale well with increasing number of links. Besides that,
obtaining instantaneous global CSI in a centralized controller
and sending the allocation decisions back to the transmitters
is quite challenging in practice.

IV. A DEEP REINFORCEMENT LEARNING FRAMEWORK

A. Overview of Reinforcement Learning

Model-free reinforcement learning [11] is a trial-and-error
process where an agent
interacts with an unknown envi-
ronment in a sequence of discrete time steps to achieve a
task. At time t, agent ﬁrst observes the current state of the
environment which is a tuple of relevant environment features
and is denoted as s(t) ∈ S, where S is the set of possible
states. It then takes an action a(t) ∈ A from an allowed
set of actions A according to a policy which can be either
stochastic, i.e., π with a(t) ∼ π(·|s(t)) or deterministic, i.e.,
µ with a(t) = µ(s(t)) [12]. Since the interactions are often
modeled as a Markov decision process, the environment moves
to a next state s(t+1) following an unknown transition matrix
that maps state-action pairs onto a distribution of next states,
and the agent receives a reward s(t+1). Overall, the above
process is described as an experience at t + 1 denoted as
e(t+1) = (cid:0)s(t), a(t), r(t+1), s(t+1)(cid:1). The goal is to learn a
policy that maximizes the cumulative discounted reward at
time t, deﬁned as

R(t) =

∞
(cid:88)

τ =0

γτ r(t+τ +1),

(5)

where γ ∈ (0, 1] is the discount factor.

Next, we introduce two reinforcement learning methods that

are used in the proposed design.

Q-learning [11] is a popular reinforcement learning method
that learns an action value function Q(s, a). Let π(a|s) be the
probability of taking action a conditioned on the current state
being s. Assuming a stationary setting, the Q-function under

a π is the expected cumulative discounted reward when action
a is taken in state s:

Qπ(s, a) = Eπ

R(t)(cid:12)
(cid:105)
(cid:104)
(cid:12)s(t) = s, a(t) = a
(cid:12)

.

(6)

Assuming the optimal policy π∗(a|s) be equal to 1 for the most
favorable action a∗ that maximizes Qπ∗
(s, a) for a given state
s, the optimal Q-function satisﬁes the Bellman equation:

Qπ∗

(s, a) = R(s, a) + γ

(cid:88)

s(cid:48)∈S

P a

ss(cid:48) max

a(cid:48)

Qπ∗

(s(cid:48), a(cid:48)),

(7)

where R(s, a) = E (cid:2)r(t+1)(cid:12)
(cid:12)s(t) = s, a(t) = a(cid:3) is the ex-
pected reward of taking action a at state s, and P a
ss(cid:48) =
Pr (cid:0)s(t+1) = s(cid:48)(cid:12)
(cid:12)s(t) = s, a(t) = a(cid:1) is the transition probability
from state s to next state s(cid:48) with action a. The classical Q-
learning algorithm uses a lookup table to represent the Q-
function values and employs the ﬁxed-point relation in (7) to
iteratively update these values. However, the classical lookup
table approach is not practical for continuous or large discrete
state spaces.

To overcome this drawback, deep Q-learning replaces the
lookup table with a deep neural network which is called
deep Q-network and expressed as q(s, a; ψ) with ψ being its
parameters [13]. As described in [13, Fig. 1], its input layer
is fed by a given state s, and each port of its output layer
gives the Q-function value for input s and corresponding action
output. Deep Q-learning is an off-policy learning method that
stores the past experiences in an experience replay memory
denoted as D in the form of e = (s, a, r(cid:48), s(cid:48)). A small value
for the maximum size of this memory, |D|, will result with
over-ﬁtting, while a large value will slow down learning.
Additionally, deep Q-learning adopts “quasi-static target net-
work” technique that implies creating a target network with
parameters ψtarget to predict the target values in the following
mean-squared Bellman error:
(cid:104)
(y(r(cid:48), s(cid:48)) − q (s, a; ψ))2(cid:105)

L (ψ, D) = E(s,a,r(cid:48),s(cid:48))∼D

(8)

,

where the target y(r(cid:48), s(cid:48)) = r(cid:48) + γ maxa(cid:48) q (s(cid:48), a(cid:48); ψtarget). To
minimize (8), ψ is updated by sampling a random mini-batch
B from D and running gradient descent by

∇ψ

1
|B|

(cid:88)

(y(r(cid:48), s(cid:48)) − q (s, a; ψ))2 .

(9)

(s,a,r(cid:48),s(cid:48))∈B

Each iteration is followed by updating ψtrain by ψ. During
the training, instead of fully exploiting the updated policy,
the learning agent applies the (cid:15)-greedy strategy which takes a
random action with a probability of (cid:15) for exploration.

On the other hand, to overcome the challenge of applying
deep Q-learning to continuous action spaces, Reference [14]
had proposed an actor-critic learning scheme called deep
deterministic policy gradient. It iteratively trains a critic net-
work, deﬁned by φ, to represent an action-value function, and
uses the critic network to train an actor network, deﬁned by
θ, that parameterizes a deterministic policy. We deﬁne the
deterministic policy as µ : S → A, and for a given state s, the
action is determined by a = µ(s; θ). Hence, the target policy

µ∗ satisﬁes the Bellman property:
(cid:88)

(s, a) = R(s, a) + γ

Qµ∗

ss(cid:48)Qµ∗
P a

(s(cid:48), µ∗(s(cid:48))),

(10)

s(cid:48)∈S

Similar to deep Q-learning, the critic network is trained by
minimizing the mean-squared Bellman error deﬁned in (8).
However, compared to the deep Q-network, the critic network
has only one output that gives a Q-function value estimate for
a given state and action input. In addition, the target in (8)
becomes ycritic(r(cid:48), s(cid:48)) = r(cid:48) + γq (s(cid:48), µ(s(cid:48); θ); φtarget).
Since q(s, a; φ) is differentiable with respect

to action,
caused by action space being continuous, the policy parameters
are simply updated by the following gradient:

∇θ

1
|B|

(cid:88)

(s,... )∈B

q (s, µ(s; θ); φ) .

(11)

Note that a noise term is added to the deterministic policy
output for exploration during training.

B. Local Information and Neighborhood Sets

the beginning of time slot t. At

We next describe the extent of the local information at
transmitter n at
time t,
transmitter n has two types of neighborhood sets for each
subband. The ﬁrst set is called “interferers” that consists of c
indexes and is denoted as I (t)
n,m. For subband m, transmitter
n ﬁrst divides nearby transmitters into two groups whether
they used subband m during time slot t − 1 or not in order
to prioritize the transmitters that occupy subband m. Then, it
sorts each group according to the interfering channel strength
at receiver n from their transmitters during time slot t − 1 by
descending order, i.e., g(t−1)
i→n,m. Lastly, the ﬁrst c sorted nearby
transmitters forms I (t)
The second set

n,m.
is the set of “interfered receivers” that
consists of c indexes and is deﬁned as O(t)
n,m. Again, each
nearby receiver j is ﬁrst divided into two groups based
on α(t−1)
j,m . The sorting criteria within each group becomes
the potential signiﬁcance of the interference strength at re-
ceiver j from transmitter n during time slot t − 1,
i.e.,
l,m g(t−1)
g(t−1)
n→j,m

l→j,mp(t−1)
Compared to [5], we follow simpler practical constraints
on the available local information to be used in the state
set design, as our main goal is to show the usefulness of
the beginning of time slot t,
the proposed approach. At
transmitter n has access to the most recent local information
gathered at receiver n for each subband m such as g(t)
n→n,m,
i→n,m ∀i ∈ I (t)
g(t)
n,m, and sum interference power at receiver n,
i.e., (cid:80)
l→n,mp(t−1)
. Conversely, the channel
l
measurements gathered at nearby receivers are delayed by one
time slots, e.g., g(t−1)
n,m. Apart from the channel
measurements, we assume that each interfered and interferer
neighbor also sends crucial key performance indicators de-
layed by one time slot due to network latency, e.g., its achieved
spectral efﬁciency during last slot.

n→j,m ∀j ∈ O(t)

l∈N ,l(cid:54)=n α(t−1)

l∈N ,l(cid:54)=j α(t−1)

+ σ2(cid:17)−1

l,m g(t)

(cid:16)(cid:80)

.

l

Fig. 1: Diagram of the proposed power control algorithm.

C. Proposed Multi-Agent Learning Scheme

In order to allow distributed execution, each link, specif-
ically, each transmitter, operates as an independent learning
agent by treating other agents as part of its local environment.
Hence, our approach is based on multiple learning agents,
rather than a single learning agent that controls the entire ac-
tion space whose dimensions will grow exponentially with the
total number of links. The single learning agent approach has
similar drawbacks as the conventional centralized optimization
algorithms in terms of complexity and cost of communica-
tion. In contrast, the proposed multi-agent approach is easily
scalable to larger networks and can operate with just local
information after training.

At the beginning of each time slot, each agent successively
executes two policies to determine its associated subband and
transmit power level. The reinforcement learning component
at the top layer is a deep Q-network that is responsible for the
subband selection. The bottom layer uses deep deterministic
policy gradient algorithm to train the actor network responsible

for agent’s transmit power level decisions. As described in Fig.
1, the actor network at the bottom layer requires the subband
decision of the top layer to determine its state input before
setting agent’s transmit power.

We next describe key components of the proposed design:
1) Action Set Design: All agents have the same pair of
action spaces. The top layer uses a discrete action space
that consists of subband indexes, i.e, a(t)
n ∈ Asubband =
{1, . . . , M } = M. Hence, we denote the subband selection
of agent n for time slot t as a(t)
n . The bottom layer has a
continuous action space deﬁned as Apower = [0, 1]. Since
the bottom layer is executed after the top layer, we denote
its action as a(t)
. We later multiply it by Pmax to get
n = Pmaxa(t)
p(t)

n,a(t)
n
.

2) State Set Design: To be used in the state, all agents rank
the subbands at the beginning of each time slot according
to their direct channel gain to the total interference power
ratio. We denote the rank as z(t)
n,m. Now we describe the
state of agent n on subband m at time t as:

n,a(t)
n

(cid:40)

s(t)
n,m =

n,m p(t−1)
α(t−1)
n

, C (t−1)
n

, z(t)

n,m, g(t)

n→n,m,

(cid:88)

α(t−1)
l,m g(t)

l→n,mp(t−1)

l

,

(cid:110)

i→n,m, α(t−1)
g(t)

i,m p(t−1)

i

,

l(cid:54)=n
C (t−1)
i

C (t−1)
j

, z(t−1)
i,m

(cid:110)

(cid:111)
,

(cid:12)
(cid:12)∀i ∈ I (t)
(cid:12)
(cid:88)

n,m

, z(t−1)
j,m ,

l,m g(t−1)
α(t−1)

l→j,mp(t−1)

j→j,m,

n→j,m, g(t−1)
g(t−1)
(cid:12)
(cid:12)∀j ∈ O(t)
(cid:12)

l

n,m

(cid:111)

(cid:41)
.

l(cid:54)=j

(12)

Since the top layer does the subband decisions that requires
information from all subbands, it should have a broader
environment view than the bottom layer. Thus, for the top
(cid:111)
layer, we deﬁne agent n’s state as s(t)
.
Then, the bottom layer uses s(t)

n,1, . . . , s(t)
s(t)

as its input.

n =

n,M

(cid:110)

n,a(t)
n

3) Reward Function Design: Both learning layers collab-
oratively aim to maximize the objective in (P1a). Conse-
quently, they share the same reward function that describes
the overall contribution of agent’s combined subband and
power decisions on the sum-rate objective. This includes
agent’s own spectral efﬁciency and a penalty term de-
pending on its externalities to its interfered neighbors on
subband a(t)
n [5]. For the reward function, we ﬁrst compute
the externality of agent n to interfered j ∈ O(t+1)
during
n,a(t)
n
time slot t as

n→j = C (t)
π(t)

j\n,a(t)
n

− C (t)

j,a(t)
n

,

(13)

where C (t)
interference from agent n on subband a(t)

j\n,a(t)
n

is the spectral efﬁciency of j without the



C (t)

j\n,a(t)
n

= log

1 +

n during slot t:


α(t)
j,a(t)
n
l(cid:54)=n,j α(t)

l,a(t)
n

g(t)
j→j,a(t)
n
g(t)
l→j,a(t)
n

p(t)
j
p(t)
l + σ2

(cid:80)

Next, we deﬁne the reward of agent n as

r(t+1)
n

= C (t)

n,a(t)
n

−

(cid:88)

π(t)
n→j.

j∈O(t+1)
(t)
n,a
n

4) Centralized Training: Since multi-agent setting violates
the environment stationary assumption of the underlying
Markov decision process discussed in Section IV-A, there
is an extensive research to develop multi-agent learning
frameworks with good empirical performance, but rarely
with theoretical guarantees [15]. In this work, we ensure
the stability by training global policy parameters shared
across the network and trained by a centralized trainer
that gathers experiences of all agents. As shown in Fig. 1,
centralized training stores two experience-replay memories
for each layer: Dsubband and Dpower. At time t, the most
recent experience at Dsubband and Dpower from agent n is

 .

(14)

(15)

Fig. 2: A network conﬁguration example.

(cid:16)

(cid:17)

n,a(t−2)
n

n,a(t−2)
n

n,power =

and e(t−1)

, s(t−1)
n
(cid:17)

, a(t−2)
n
, r(t−1)
n

s(t−2)
n
, a(t−2)

e(t−1)
, r(t−1)
n,subband =
n
(cid:16)
s(t−2)
, s(t−1)
, respectively, due to
n,a(t−2)
n
the backhaul delay of 1 time slot. Note that the next state in
e(t−1)
n,power is with respect to the old subband selection a(t−2)
.
During time slot t, the centralized training runs one gradi-
ent step for each policy. As described in Fig 1, it broadcasts
most recent versions of ψ and θ once per Tu time slots.
The broadcasting takes Td time slots to ﬁnish, again due
to the backhaul delay.

n

V. SIMULATION RESULTS

In this section, our main goal is to compare the performance
of the proposed learning approach with the conventional
optimization methods and joint learning as the number of
subbands increases.

Throughout the simulations, we choose two network sizes
of (K, N ) = (5 cells, 20 links) and (10 cells, 50 links), re-
spectively. As described in Fig. 2, we consider homogeneous
hexagonal cells of 400 meters radius with each cell having
equal number of uniformly randomly placed receivers. We
vary the number of subbands M from 1 to 10. Following
the LTE standard, we set the distance dependent path loss
to 128.1 + 37.6 log10(d) (in dB), where d is transmitter-to-
receiver distance in km. The log-normal shadowing standard
deviation is 10 dB. We set fd = 10 Hz, T = 20 ms,
Pmax = 38 dBm, and σ2 = −114 dBm. Similar to [5], the
signal-to-interference-plus-noise ratio is capped at 30 dB in
the calculation of the spectral efﬁciency in (4) due to practical
constraints on front end’s dynamic range.

We compare the proposed approach with four benchmarks.
The ﬁrst is the joint learning approach as proposed in [2].
We discretize the transmit power into 10 levels. The second
is called the ‘ideal FP’. It runs the fractional programming
algorithm with an assumption of full instant CSI. The ﬁrst
scenario ignores any delay during the execution of centralized
optimization or passing the optimization outcomes to the
transmitters. On the other hand, the third benchmark is called
the ‘delayed FP’ and assumes one time slot delay to run the

10005000500100015002000x axis position (meters)15001000500050010001500y axis position (meters)TransmitterReceiverTABLE I: Testing results.

(K, N )
(cells, links)

(5, 20)

(10, 50)

M
subbands
1
2
4
1
2
4
5
10

reinforcement learning

average sum-rate performance in bps/Hz per link
other schemes
delayed FP
1.46
2.46
3.57
1.21
1.92
2.68
2.94
4.08

ideal FP
1.58
2.66
3.81
1.31
2.08
2.90
3.18
4.44

joint
1.50
2.64
4.38
1.26
2.10
3.34
3.76
4.41

output layer size
reinforcement learning

random
0.41
0.99
2.12
0.25
0.59
1.31
1.64
2.99

proposed
1 + 1
2 + 1
4 + 1
1 + 1
2 + 1
4 + 1
5 + 1
10 + 1

joint
10
20
40
10
20
40
50
100

average
iterations
FP
70.30
102.08
122.15
72.83
96.32
185.93
206.38
287.70

proposed
1.51
2.63
4.57
1.26
2.08
3.34
3.79
5.71

(a) M = 2 subbands, (K, N ) = (5, 20).

(b) M = 4 subbands, (K, N ) = (5, 20).

(c) M = 5 subbands, (K, N ) = (10, 50).

(d) M = 10 subbands, (K, N ) = (10, 50).

Fig. 3: Training convergence.

fractional programming algorithm. In the ﬁnal benchmark,
each transmitter just picks a random subband and transmit
power at the beginning of every time slot.

We divide training into four episodes with each running
for 5,000 time slots. At the beginning of each episode, we
randomly sample a new deployment, and we reset the explo-
ration and learning rate parameters. For faster convergence, we
replace the noise term added to the deterministic policy output
with Q-learning’s e-greedy algorithm. The implementation and
hyper-parameters are included in the source code which is

available at [16]. For better stability, we ensure that the bottom
layer has higher learning rate than the top layer, and it uses
a higher initial value of (cid:15), but with a higher decay rate. The
ﬁne-tuning of the (cid:15) value is important to avoid converging to
undesired situations in which all agents want to transmit with
Pmax or with zero power.

In Fig. 3, we show the training convergence of the proposed
and joint reinforcement learning scheme. For M = 2 subbands,
as shown in Fig. 3a, their convergence rates are quite close.
However, when we increase the number of subbands, the joint

02500500075001000012500150001750020000training iterations0.51.01.52.02.53.0average spectral efficiency (bps/Hz) per linkproposedjoint learningideal FPdelayed FPrandom02500500075001000012500150001750020000training iterations12345average spectral efficiency (bps/Hz) per linkproposedjoint learningideal FPdelayed FPrandom02500500075001000012500150001750020000training iterations0.51.01.52.02.53.03.54.04.5average spectral efficiency (bps/Hz) per linkproposedjoint learningideal FPdelayed FPrandom02500500075001000012500150001750020000training iterations123456average spectral efficiency (bps/Hz) per linkproposedjoint learningideal FPdelayed FPrandom[15] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement
learning for multiagent systems: A review of challenges, solutions, and
applications,” IEEE Transactions on Cybernetics, pp. 1–14, 2020.
[16] Y. S. Nasir and D. Guo, “TensorFlow code for deep reinforcement
learning for joint spectrum and power allocation in cellular networks,”
https://github.com/sinannasir/Spectrum-Power-Allocation, 2020.

learning approach is not able to keep up with the proposed
approach in terms of training convergence. This is mainly
caused by the increased size of the joint learning’s action
space and increased deep Q-network output layer complexity.
Next, we test the performance of the trained policies on several
randomly generated deployments in Table I. Testing shows that
a pretrained policy is still usable on new deployments and the
proposed approach is better scalable than the benchmarks.

VI. CONCLUSION AND FUTURE WORK

We have demonstrated a novel multi-agent reinforcement
learning framework for the joint subband selection and power
control problem. With centralized training and distributed
execution only local information is needed by the agent under
practicality constraints. In addition, as the number of subbands
increases, the proposed learning approach has better training
convergence and higher sum-rate performance than the joint
learning. For future work, we are looking into better and easily
tunable training and exploration schemes to better adapt to the
environment non-stationarity of the multi-agent setting.

REFERENCES

[1] K. Shen and W. Yu, “Fractional programming for communication
systems—part i: Power control and beamforming,” IEEE Transactions
on Signal Processing, vol. 66, no. 10, pp. 2616–2630, May 2018.
[2] J. Tan, Y. C. Liang, L. Zhang, and G. Feng, “Deep reinforcement
learning for joint channel selection and power control in D2D networks,”
2020, pp. 1–1.

[3] Z. Qin, H. Ye, G. Y. Li, and B. F. Juang, “Deep learning in physical
layer communications,” IEEE Wireless Communications, vol. 26, no. 2,
pp. 93–99, 2019.

[4] E. Ghadimi, F. D. Calabrese, G. Peters, and P. Soldati, “A reinforcement
learning approach to power control and rate adaptation in cellular
networks,” in 2017 IEEE International Conference on Communications
(ICC), May 2017, pp. 1–7.

[5] Y. S. Nasir and D. Guo, “Multi-agent deep reinforcement

learning
for dynamic power allocation in wireless networks,” IEEE Journal on
Selected Areas in Communications, vol. 37, no. 10, pp. 2239–2250,
2019.

[6] F. Meng, P. Chen, and L. Wu, “Power allocation in multi-user cellular
networks with deep Q learning approach,” in ICC 2019 - 2019 IEEE
International Conference on Communications (ICC), 2019, pp. 1–6.
[7] F. Meng, P. Chen, L. Wu, and J. Cheng, “Power allocation in multi-
user cellular networks: Deep reinforcement learning approaches,” IEEE
Transactions on Wireless Communications, vol. 19, no. 10, pp. 6255–
6267, 2020.

[8] Y. S. Nasir and D. Guo, “Deep Actor-Critic Learning for Distributed
in Wireless Mobile Networks,” arXiv e-prints, p.

Power Control
arXiv:2009.06681, Sep. 2020.

[9] L. Liang, J. Kim, S. C. Jha, K. Sivanesan, and G. Y. Li, “Spectrum
and power allocation for vehicular communications with delayed csi
feedback,” IEEE Wireless Communications Letters, vol. 6, no. 4, pp.
458–461, Aug 2017.

[10] Z. Q. Luo and S. Zhang, “Dynamic spectrum management: Complexity
and duality,” IEEE Journal of Selected Topics in Signal Processing,
vol. 2, no. 1, pp. 57–73, Feb 2008.

[11] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

Cambridge, MA, USA: MIT press, 2018.

[12] J. Achiam, “Spinning up in deep reinforcement

learning,” https://

spinningup.openai.com, 2018.

[13] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
et al., “Human-level control
learning,”
Nature, vol. 518, no. 7540, pp. 529–533, 2015.

through deep reinforcement

[14] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” arXiv e-prints, p. arXiv:1509.02971, Sep. 2015.

