pymoo: Multi-objective Optimization in Python

Julian Blank, Kalyanmoy Deb

Michigan State University, East Lansing, MI 48824, USA

0
2
0
2

n
a
J

2
2

]
E
N
.
s
c
[

1
v
4
0
5
4
0
.
2
0
0
2
:
v
i
X
r
a

A R T I C L E I N F O

Keywords:
Multi-objective Optimization
Python
Customization
Genetic Algorithm

A B S T R A C T

Python has become the programming language of choice for research and industry projects related
to data science, machine learning, and deep learning. Since optimization is an inherent part of these
research ï¬elds, more optimization related frameworks have arisen in the past few years. Only a few
of them support optimization of multiple conï¬‚icting objectives at a time, but do not provide com-
prehensive tools for a complete multi-objective optimization task. To address this issue, we have
developed pymoo, a multi-objective optimization framework in Python. We provide a guide to getting
started with our framework by demonstrating the implementation of an exemplary constrained multi-
objective optimization scenario. Moreover, we give a high-level overview of the architecture of pymoo
to show its capabilities followed by an explanation of each module and its corresponding sub-modules.
The implementations in our framework are customizable and algorithms can be modiï¬ed/extended by
supplying custom operators. Moreover, a variety of single, multi and many-objective test problems
are provided and gradients can be retrieved by automatic diï¬€erentiation out of the box. Also, pymoo
addresses practical needs, such as the parallelization of function evaluations, methods to visualize
low and high-dimensional spaces, and tools for multi-criteria decision making. For more information
about pymoo, readers are encouraged to visit: https://pymoo.org

1. Introduction

Optimization plays an essential role in many scientiï¬c
areas, such as engineering, data analytics, and deep learn-
ing. These ï¬elds are fast-growing and their concepts are
employed for various purposes, for instance gaining insights
from a large data sets or ï¬tting accurate prediction mod-
els. Whenever an algorithm has to handle a signiï¬cantly
large amount of data, an eï¬ƒcient implementation in a suit-
able programming language is important. Python [41] has
become the programming language of choice for the above
mentioned research areas over the last few years, not only be-
cause it is easy to use but also good community support ex-
ists. Python is a high-level, cross-platform, and interpreted
programming language that focuses on code readability. A
large number of high-quality libraries are available and sup-
port for any kind of scientiï¬c computation is ensured. These
characteristics make Python an appropriate tool for many re-
search and industry projects where the investigations can be
rather complex. A fundamental principle of research is to
ensure reproducibility of studies and to provide access to
materials used in the research, whenever possible. In com-
puter science, this translates to a sketch of an algorithm and
the implementation itself. However, the implementation of
optimization algorithms can be challenging and speciï¬cally
benchmarking is time-consuming. Having access to either a
good collection of diï¬€erent source codes or a comprehensive
library is time-saving and avoids an error-prone implemen-
tation from scratch.

To address this need for multi-objective optimization in
Python, we introduce pymoo. The goal of our framework is
not only to provide state of the art optimization algorithms,
but also to cover diï¬€erent aspects related to the optimization

blankjul@msu.edu (J. Blank); kdeb@msu.edu (K. Deb)

ORCID(s): 0000-0002-2227-6476 (J. Blank); 0000-0001-7402-9939 (K.

Deb)

process itself. We have implemented single, multi and many-
objective test problems which can be used as a testbed for
algorithms. In addition to the objective and constraint val-
ues of test problems, gradient information can be retrieved
through automatic diï¬€erentiation [5]. Moreover, a paral-
lelized evaluation of solutions can be implemented through
vectorized computations, multi-threaded execution, and dis-
tributed computing. Further, pymoo provides implementa-
tions of performance indicators to measure the quality of re-
sults obtained by a multi-objective optimization algorithm.
Tools for an explorative analysis through visualization of
lower and higher-dimensional data are available and multi-
criteria decision making methods guide the selection of a
single solution from a solution set based on preferences.

Our framework is designed to be extendable through of
its modular implementation. For instance, a genetic algo-
rithm is assembled in a plug-and-play manner by making
use of speciï¬c sub-modules, such as initial sampling, mating
selection, crossover, mutation and survival selection. Each
sub-module takes care of an aspect independently and, there-
fore, variants of algorithms can be initiated by passing dif-
ferent combinations of sub-modules. This concept allows
end-users to incorporate domain knowledge through custom
implementations. For example, in an evolutionary algorithm
a biased initial sampling module created with the knowledge
of domain experts can guide the initial search.

Furthermore, we like to mention that our framework is
well-documented with a large number of available code-snippets.
We created a starterâ€™s guide for users to become familiar with
our framework and to demonstrate its capabilities. As an ex-
ample, it shows the optimization results of a bi-objective op-
timization problem with two constraints. An extract from the
guide will be presented in this paper. Moreover, we provide
an explanation of each algorithm and source code to run it
on a suitable optimization problem in our software documen-
tation. Additionally, we show a deï¬nition of test problems

Julian Blank, Kalyanmoy Deb

Page 1 of 12

 
 
 
 
 
 
pymoo: Multi-objective Optimization in Python

and provide a plot of their ï¬tness landscapes. The frame-
work documentation is built using Sphinx [30] and correct-
ness of modules is ensured by automatic unit testing [36].
Most algorithms have been developed in collaboration with
the second author and have been benchmarked extensively
against the original implementations.

In the remainder of this paper, we ï¬rst present related ex-
isting optimization frameworks in Python and in other pro-
gramming languages. Then, we provide a guide to getting
started with pymoo in Section 3 which covers the most im-
portant steps of our proposed framework. In Section 4 we
illustrate the framework architecture and the corresponding
modules, such as problems, algorithms and related analyt-
ics. Each of the modules is then discussed separately in Sec-
tions 5 to 7. Finally, concluding remarks are presented in
Section 8.

2. Related Works

In the last decades, various optimization frameworks in
diverse programming languages were developed. However,
some of them only partially cover multi-objective optimiza-
tion. In general, the choice of a suitable framework for an
optimization task is a multi-objective problem itself. More-
over, some criteria are rather subjective, for instance, the us-
ability and extendibility of a framework and, therefore, the
assessment regarding criteria as well as the decision making
process diï¬€er from user to user. For example, one might have
decided on a programming language ï¬rst, either because of
personal preference or a project constraint, and then search
for a suitable framework. One might give more importance
to the overall features of a framework, for example paral-
lelization or visualization, over the programming language
itself. An overview of some existing multi-objective opti-
mization frameworks in Python is listed in Table 1, each of
which is described in the following.

Recently, the well-known multi-objective optimization
framework jMetal [15] developed in Java [19] has been ported
to a Python version, namely jMetalPy [2]. The authors aim
to further extend it and to make use of the full feature set
of Python, for instance, data analysis and data visualization.
In addition to traditional optimization algorithms, jMetalPy
also oï¬€ers methods for dynamic optimization. Moreover,
the post analysis of performance metrics of an experiment
with several independent runs is automated.

Parallel Global Multiobjective Optimizer, PyGMO [25],
is an optimization library for the easy distribution of mas-
sive optimization tasks over multiple CPUs. It uses the gen-
eralized island-model paradigm for the coarse grained paral-
lelization of optimization algorithms and, therefore, allows
users to develop asynchronous and distributed algorithms.

Platypus [21] is a multi-objective optimization frame-
work that oï¬€ers implementations of state-of-the art algo-
rithms. It enables users to create an experiment with var-
ious algorithms and provides post-analysis methods based
on metrics and visualization.

Table 1
Multi-objective Optimization Frameworks in Python

Name

License

Focus
on
multi-
objective

Pure
Python

Visu-
aliza-
tion

Decision
Making

jMetalPy

MIT

PyGMO

GPL-3.0

Platypus

GPL-3.0

DEAP

LGPL-3.0

Inspyred

MIT

pymoo

Apache 2.0

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

prototyping and testing of ideas. Even though, DEAP does
not focus on multi-objective optimization, however, due to
the modularity and extendibility of the framework multi-objective
algorithms can be developed. Moreover, parallelization and
load-balancing tasks are supported out of the box.

Inspyred [18] is a framework for creating bio-inspired
computational intelligence algorithms in Python which is
not focused on multi-objective algorithms directly, but on
evolutionary computation in general. However, an example
for NSGA-II [12] is provided and other multi-objective algo-
rithms can be implemented through the modular implemen-
tation of the framework.

If the search for frameworks is not limited to Python,
other popular frameworks should be considered: PlatEMO [45]
in Matlab, MOEA [20] and jMetal [15] in Java, jMetalCpp [31]
and PaGMO [3] in C++. Of course this is not an exhaustive
list and readers may search for other available options.

3. Getting Started 1

In the following, we provide a starterâ€™s guide for pymoo. It
covers the most important steps in an optimization scenario
starting with the installation of the framework, deï¬ning an
optimization problem, and the optimization procedure itself.

3.1. Installation

Our framework pymoo is available on PyPI [17] which is
a central repository to make Python software package eas-
ily accessible. The framework can be installed by using the
package manager:

$ pip install -U pymoo

Some components are available in Python and addition-
ally in Cython [1]. Cython allows developers to annotate ex-
isting Python code which is translated to C or C++ program-
ming languages. The translated ï¬les are compiled to a bi-
nary executable and can be used to speed up computations.

A Distributed Evolutionary Algorithms in Python (DEAP)

[16] is novel evolutionary computation framework for rapid

1All source codes in this paper are related to pymoo version 0.3.2. A

getting started guide for upcoming versions can be found at pymoo.org.

Julian Blank, Kalyanmoy Deb

Page 2 of 12

pymoo: Multi-objective Optimization in Python

During the installation of pymoo, attempts are made for com-
pilation, however, if unsuccessful due to the lack of a suit-
able compiler or other reasons, the pure Python version is
installed. We would like to emphasize that the compilation
is optional and all features are available without it. More de-
tail about the compilation and troubleshooting can be found
in our installation guide online.

3.2. Problem Deï¬nition

In general, multi-objective optimization has several ob-
jective functions with subject to inequality and equality con-
straints to optimize[11]. The goal is to ï¬nd a set of solutions
(variable vectors) that satisfy all constraints and are as good
as possible regarding all its objectives values. The problem
deï¬nition in its general form is given by:

min ğ‘“ğ‘š(ğ±)
s.t.

ğ‘”ğ‘—(ğ±) â‰¤ 0,
â„ğ‘˜(ğ±) = 0,
â‰¤ ğ‘¥ğ‘–

ğ‘¥ğ¿
ğ‘–

â‰¤ ğ‘¥ğ‘ˆ
ğ‘– ,

ğ‘š = 1, .., ğ‘€,

ğ‘— = 1, .., ğ½ ,

ğ‘˜ = 1, .., ğ¾,

ğ‘– = 1, .., ğ‘.

(1)

The formulation above deï¬nes a multi-objective optimiza-
tion problem with ğ‘ variables, ğ‘€ objectives, ğ½ inequality,
and ğ¾ equality constraints. Moreover, for each variable ğ‘¥ğ‘–
,
lower and upper variable boundaries (ğ‘¥ğ¿
) are also
ğ‘–
deï¬ned.

and ğ‘¥ğ‘ˆ
ğ‘–

In the following, we illustrate a bi-objective optimization

problem with two constraints.

1 + ğ‘¥2
2),

min ğ‘“1(ğ‘¥) = (ğ‘¥2
max ğ‘“2(ğ‘¥) = âˆ’(ğ‘¥1 âˆ’ 1)2 âˆ’ ğ‘¥2
2,
s.t. ğ‘”1(ğ‘¥) = 2 (ğ‘¥1 âˆ’ 0.1) (ğ‘¥1 âˆ’ 0.9) â‰¤ 0,
ğ‘”2(ğ‘¥) = 20 (ğ‘¥1 âˆ’ 0.4) (ğ‘¥1 âˆ’ 0.6) â‰¥ 0,
âˆ’ 2 â‰¤ ğ‘¥1
âˆ’ 2 â‰¤ ğ‘¥2

â‰¤ 2,
â‰¤ 2.

(2)

and ğ‘¥2

It consists of two objectives (ğ‘€ = 2) where ğ‘“1(ğ‘¥) is
minimized and ğ‘“2(ğ‘¥) maximized. The optimization is with
subject to two inequality constraints (ğ½ = 2) where ğ‘”1(ğ‘¥)
is formulated as a less-than-equal-to and ğ‘”2(ğ‘¥) as a greater-
than-equal-to constraint. The problem is deï¬ned with re-
spect to two variables (ğ‘ = 2), ğ‘¥1
, which both are in
the range [âˆ’2, 2]. The problem does not contain any equality
constraints (ğ¾ = 0). Contour plots of the objective functions
are shown in Figure 1. The contours of the objective func-
tion ğ‘“1(ğ‘¥) are represented by solid lines and ğ‘“2(ğ‘¥) by dashed
lines. Constraints ğ‘”1(ğ‘¥) and ğ‘”2(ğ‘¥) are parabolas which in-
tersect the ğ‘¥1
-axis at (0.1, 0.9) and (0.4, 0.6). The Pareto-
optimal set is marked by a thick orange line. Through the
combination of both constraints the Pareto-set is split into
two parts. Analytically, the Pareto-optimal set is given by
â‰¤ 0.9) âˆ§ ğ‘¥2 =
ğ‘ƒ ğ‘† = {(ğ‘¥1, ğ‘¥2) | (0.1 â‰¤ ğ‘¥1
â‰¤ 0.4)âˆ¨(0.6 â‰¤ ğ‘¥1
âˆš
is
0} and the eï¬ƒcient-front by ğ‘“2 = (
deï¬ned in [0.01, 0.16] and [0.36, 0.81].

ğ‘“1 âˆ’ 1)2 where ğ‘“1

Figure 1: Contour plot of the test problem 2.

In the following, we provide an example implementa-
tion of the problem formulation above using pymoo. We as-
sume the reader is familiar with Python and has a fundamen-
tal knowledge of NumPy [35] which is utilized to deal with
vector and matrix computations.

In pymoo, we consider pure minimization problems for
optimization in all our modules. However, without loss of
generality an objective which is supposed to be maximized,
can be multiplied by âˆ’1 and be minimized [8]. Therefore,
we minimize âˆ’ğ‘“2(ğ‘¥) instead of maximizing ğ‘“2(ğ‘¥) in our op-
timization problem. Furthermore, all constraint functions
need to be formulated as a less-than-equal-to constraint. For
this reason, ğ‘”2(ğ‘¥) needs to be multiplied by âˆ’1 to ï¬‚ip the â‰¥
to a â‰¤ relation. We recommend the normalization of con-
straints to give equal importance to each of them. For ğ‘”1(ğ‘¥),
the constant â€˜resourceâ€™ value of the constraint is 2 â‹… (âˆ’0.1) â‹…
(âˆ’0.9) = 0.18 and for ğ‘”2(ğ‘¥) it is 20 â‹… (âˆ’0.4) â‹… (âˆ’0.6) = 4.8,
respectively. We achieve normalization of constraints by di-
viding ğ‘”1(ğ‘¥) and ğ‘”2(ğ‘¥) by the corresponding constant [9].

Finally, the optimization problem to be optimized using

pymoo is deï¬ned by:

1 + ğ‘¥2
2),

min ğ‘“1(ğ‘¥) = (ğ‘¥2
min ğ‘“2(ğ‘¥) = (ğ‘¥1 âˆ’ 1)2 + ğ‘¥2
2,
s.t. ğ‘”1(ğ‘¥) = 2 (ğ‘¥1 âˆ’ 0.1) (ğ‘¥1 âˆ’ 0.9) âˆ• 0.18 â‰¤ 0,
ğ‘”2(ğ‘¥) = âˆ’20 (ğ‘¥1 âˆ’ 0.4) (ğ‘¥1 âˆ’ 0.6) âˆ• 4.8 â‰¤ 0,
âˆ’ 2 â‰¤ ğ‘¥1
âˆ’ 2 â‰¤ ğ‘¥2

â‰¤ 2,
â‰¤ 2.

(3)

Next, the derived problem formulation is implemented in
Python. Each optimization problem in pymoo has to inherit
from the Problem class. First, by calling the super() func-
tion the problem properties such as the number of variables
(n_var), objectives (n_obj) and constraints (n_constr) are ini-
tialized. Furthermore, lower (xl) and upper variables bound-
aries (xu) are supplied as a NumPy array. Additionally, the
evaluation function _evaluate needs to be overwritten from

Julian Blank, Kalyanmoy Deb

Page 3 of 12

0.50.00.51.01.5x10.500.250.000.250.500.751.00x2g1(x)g2(x)f1(x)f2(x)pymoo: Multi-objective Optimization in Python

the superclass. The method takes a two-dimensional NumPy
array x with ğ‘› rows and ğ‘š columns as an input. Each row
represents an individual and each column an optimization
variable. After doing the necessary calculations, the objec-
tive values are added to the dictionary out with the key F and
the constraints with key G.

As mentioned above, pymoo utilizes NumPy [35] for most
of its computations. To be able to retrieve gradients through
automatic diï¬€erentiation we are using a wrapper around NumPy
called Autograd [32]. Note that this is not obligatory for a
problem deï¬nition.

import a u t o g r a d . numpy a s anp
from pymoo . model . p r o b l e m import P r o b l e m

c l a s s MyProblem ( P r o b l e m ) :

d e f _ _ i n i t _ _ ( s e l f ) :

s u p e r ( ) . _ _ i n i t _ _ ( n _ v a r =2 ,
n _ o b j =2 ,
n _ c o n s t r =2 ,
x l=anp . a r r a y ( [ âˆ’ 2 , âˆ’ 2 ] ) ,
xu=anp . a r r a y ( [ 2 , 2 ] ) )

d e f _ e v a l u a t e ( s e l f , x , o u t , âˆ— a r g s , âˆ—âˆ— k w a r g s ) :

f 1 = x [ : , 0 ] âˆ— âˆ— 2 + x [ : , 1 ] âˆ— âˆ— 2
f 2 = ( x [ : , 0 ] âˆ’ 1 ) âˆ—âˆ—2 + x [ : , 1 ] âˆ— âˆ— 2

g1 = 2 âˆ— ( x [ : , 0 ] âˆ’ 0 . 1 ) âˆ— ( x [ : , 0 ] âˆ’ 0 . 9 )

0 . 1 8

/

g2 = âˆ’ 2 0 âˆ— ( x [ : , 0 ] âˆ’ 0 . 4 ) âˆ— ( x [ : , 0 ] âˆ’ 0 . 6 )

4 . 8

/

o u t [ " F " ] = anp . c o l u m n _ s t a c k ( [ f1 ,
f 2 ] )
o u t [ "G" ] = anp . c o l u m n _ s t a c k ( [ g1 , g2 ] )

and returns an initialized algorithm object.

from pymoo . a l g o r i t h m s . n s g a 2 import NSGA2
from pymoo . f a c t o r y import g e t _ s a m p l i n g ,

g e t _ c r o s s o v e r , g e t _ m u t a t i o n

a l g o r i t h m = NSGA2(

p o p _ s i z e =40 ,
n _ o f f s p r i n g s =10 ,
s a m p l i n g=g e t _ s a m p l i n g ( " r e a l _ r a n d o m " ) ,
c r o s s o v e r = g e t _ c r o s s o v e r ( " r e a l _ s b x " , p r o b = 0 . 9 ,

e t a =15) ,

m u t a t i o n= g e t _ m u t a t i o n ( " r e a l _ p m " , e t a =20) ,
e l i m i n a t e _ d u p l i c a t e s =T r u e

)

3.4. Optimization

Next, we use the initialized algorithm object to optimize
the deï¬ned problem. Therefore, the minimize function with
both instances problem and algorithm as parameters is called.
Moreover, we supply the termination criterion of running the
algorithm for 40 generations which will result in 40 + 40 Ã—
10 = 440 function evaluations. In addition, we deï¬ne a ran-
dom seed to ensure reproducibility and enable the verbose
ï¬‚ag to see printouts for each generation. The method re-
turns a Result object which contains the non-dominated set
of solutions found by the algorithm.

from pymoo . o p t i m i z e import m i n i m i z e

r e s = m i n i m i z e ( MyProblem ( ) ,

a l g o r i t h m ,
( ' n_gen ' , 4 0 ) ,
s e e d =1 ,
v e r b o s e=T r u e )

3.3. Algorithm Initialization

Next, we need to initialize a method to optimize the prob-
lem. In pymoo, an algorithm object needs to be created for
optimization. For each of the algorithms an API documenta-
tion is available and through supplying diï¬€erent parameters,
algorithms can be customized in a plug-and-play manner. In
general, the choice of a suitable algorithm for optimization
problems is a challenge itself. Whenever problem character-
istics are known beforehand we recommended using those
through customized operators. However, in our case the op-
timization problem is rather simple, but the aspect of having
two objectives and two constraints should be considered. For
this reason, we decided to use NSGA-II [12] with its default
conï¬guration with minor modiï¬cations. We chose a popu-
lation size of 40, but instead of generating the same num-
ber of oï¬€springs, we create only 10 each generation. This
is a steady-state variant of NSGA-II and it is likely to im-
prove the convergence property for rather simple optimiza-
tion problems without much diï¬ƒculties, such as the exis-
tence of local Pareto-fronts. Moreover, we enable a duplicate
check which makes sure that the mating produces oï¬€springs
which are diï¬€erent with respect to themselves and also from
the existing population regarding their variable vectors. To
illustrate the customization aspect, we listed the other un-
modiï¬ed default operators in the code-snippet below. The
constructor of NSGA2 is called with the supplied parameters

The optimization results are illustrated in Figure 2 where
the design space is shown in Figure 2a and in the objective
space in Figure 2b. The solid line represents the analyti-
cally derived Pareto set and front in the corresponding space
and the circles solutions found by the algorithm. It can be
observed that the algorithm was able to converge and a set
of nearly-optimal solutions was obtained. Some additional
post-processing steps and more details about other aspects
of the optimization procedure can be found in the remainder
of this paper and in our software documentation.

The starters guide showed the steps starting from the in-
stallation up to solving an optimization problem. The inves-
tigation of a constrained bi-objective problem demonstrated
the basic procedure in an optimization scenario.

4. Architecture

Software architecture is fundamentally important to keep
source code organized. On the one hand, it helps devel-
opers and users to get an overview of existing classes, and
on the other hand, it allows ï¬‚exibility and extendibility by
adding new modules. Figure 3 visualizes the architecture
of pymoo. The ï¬rst level of abstraction consists of the op-
timization problems, algorithms and analytics. Each of the
modules can be categorized into more detail and consists of
multiple sub-modules.

Julian Blank, Kalyanmoy Deb

Page 4 of 12

pymoo: Multi-objective Optimization in Python

modules mentioned in more detail.

(a) Design Space

5. Problems

It is common practice for researchers to evaluate the per-
formance of algorithms on a variety of test problems. Since
we know no single-best algorithm for all arbitrary optimiza-
tion problems exist [51], this helps to identify problem classes
where the algorithm is suitable. Therefore, a collection of
test problems with diï¬€erent numbers of variables, objectives
or constraints and alternating complexity becomes handy for
algorithm development. Moreover, in a multi-objective con-
text, test problems with diï¬€erent Pareto-front shapes or vary-
ing variable density close to the optimal region are of inter-
est.

(b) Objective Space

Figure 2: Result of the Getting Started Optimization

(i) Problems: Optimization problems in our framework
are categorized into single, multi, and many-objective
test problems. Gradients are available through auto-
matic diï¬€erentiation and parallelization can be imple-
mented by using a variety of techniques.

(ii) Optimization: Since most of the algorithms are based
on evolutionary computations, operators such as sam-
pling, mating selection, crossover and mutation have
to be chosen or implemented. Furthermore, because
many problems in practice have one or more constraints,
a methodology for handling those must be incorpo-
rated. Some algorithms are based on decomposition
which splits the multi-objective problem into many
single-objective problems. Moreover, when the algo-
rithm is used to solve the problem, a termination crite-
rion must be deï¬ned either explicitly or implicitly by
the implementation of the algorithm.

(iii) Analytics: During and after an optimization run an-
alytics support the understanding of data. First, in-
tuitively the design space, objective space, or other
metrics can be explored through visualization. More-
over, to measure the convergence and/or diversity of a
Pareto-optimal set performance indicators can be used.
To support the decision making process either through
ï¬nding points close to the area of interest in the ob-
jective space or high trade-oï¬€ solutions. This can be
applied either during an optimization run to mimic in-
teractive optimization or as a post analysis.

In the remainder of the paper, we will discuss each of the

5.1. Implementations

In our framework, we categorize test problems regard-
ing the number of objectives: single-objective (1 objective),
multi-objective (2 or 3 objectives) and many-objective (more
than 3 objectives). Test problems implemented in pymoo are
listed in Table 2. For each problem the number of variables,
objectives, and constraints are indicated. If the test problem
is scalable to any of the paramaters, we label the problem
with (s). If the problem is scalable, but a default number was
original proposed we indicate that with surrounding brack-
ets. In case the category does not apply, for example because
we refer to a test problem family with several functions, we
use (â‹…).

The implementations in pymoo let end-users deï¬ne what
values of the corresponding problem should be returned. On
an implementation level, the evaluate function of a Problem
instance takes a list return_value_of which contains the type
of values being returned. By default the objective values "F"
and if the problem has constraints the constraint violation
"CV" are included. The constraint function values can be re-
turned independently by adding "G". This gives developers
the ï¬‚exibility to receive the values that are needed for their
methods.

5.2. Gradients

All our test problems are implemented using Autograd [32].

Therefore, automatic diï¬€erentiation is supported out of the
box. We have shown in Section 3 how a new optimization
problem is deï¬ned.

If gradients are desired to be calculated the preï¬x "d"
needs to be added to the corresponding value of the return_value_of
list. For instance to ask for the objective values and its gra-
dients return_value_of = ["F", "dF"].

Let us consider the problem we have implemented shown
in Equation 3. The derivation of the objective functions ğ¹
with respect to each variable is given by:

âˆ‡ğ¹ =

[

2ğ‘¥1
2(ğ‘¥1 âˆ’ 1)

]

.

2ğ‘¥2
2ğ‘¥2

(4)

The gradients at the point [0.1, 0.2] are calculated by:

Julian Blank, Kalyanmoy Deb

Page 5 of 12

0.500.250.000.250.500.751.001.251.50x121012x20.00.10.20.30.40.50.60.70.8f10.00.20.40.60.8f2pymoo: Multi-objective Optimization in Python

Figure 3: Architecture of pymoo.

F , dF = p r o b l e m . e v a l u a t e ( np . a r r a y ( [ 0 . 1 , 0 . 2 ] ) ,

r e t u r n _ v a l u e s _ o f =[ " F " , "

dF " ] )

returns the following output

F <âˆ’ [ 0 . 0 5 , 0 . 8 5 ]
dF <âˆ’ [ [ 0 . 2 ,
[ âˆ’ 1 . 8 ,

0 . 4 ] ,
0 . 4 ] ]

It can easily be veriï¬ed that the values are matching with
the analytic gradient derivation. The gradients for the con-
straint functions can be calculated accordingly by adding
"dG" to the return_value_of list.

5.3. Parallelization

If evaluation functions are computationally expensive, a
serialized evaluation of a set of solutions can become the bot-
tleneck of the overall optimization procedure. For this rea-
son, parallelization is desired for an use of existing computa-
tional resources more eï¬ƒciently and distribute long-running
calculations. In pymoo, the evaluation function receives a set
of solutions if the algorithm is utilizing a population. This
empowers the user to implement any kind of parallelization
as long as the objective values for all solutions are written
as an output when the evaluation function terminates.
In
our framework, a couple of possibilities to implement par-
allelization exist:

(i) Vectorized Evaluation: A common technique to par-
allelize evaluations is to use matrices where each row
represents a solution. Therefore, a vectorized evalu-
ation refers to a column which includes the variables
of all solutions. By using vectors the objective val-
ues of all solutions are calculated at once. The code-
snippet of the example problem in Section 3 shows
such an implementation using NumPy [35]. To run
calculations on a GPU, implementing support for Py-
Torch [37] tensors can be done with little overhead
given suitable hardware and correctly installed drivers.

(ii) Threaded Loop-wise Evaluation: If the function eval-
uation should occur independently, a for loop can be
used to set the values. By default the evaluation is
serialized and no calculations occur in parallel. By
providing a keyword to the evaluation function, pymoo
spawns a thread for each evaluation and manages those
by using the default thread pool implementation in
Python. This behaviour can be implemented out of the
box and the number of parallel threads can be modi-
ï¬ed.

(iii) Distributed Evaluation: If the evaluation should not
be limited to a single machine, the evaluation itself
can be distributed to several workers or a whole clus-
ter. We recommend using Dask [7] which enables
distributed computations on diï¬€erent levels. For in-
stance, the matrix operation itself can be distributed
or a whole function can be outsourced. Similar to the
loop wise evaluation each individual can be evaluate
element-wise by sending it to a worker.

6. Optimization Module

The optimization module provides diï¬€erent kinds of sub-
modules to be used in algorithms. Some of them are more
of a generic nature, such as decomposition and termination
criterion, and others are more related to evolutionary com-
puting. By assembling those modules together algorithms
are built.

6.1. Algorithms

Available algorithm implementations in pymoo are listed
in Table 3. Compared to other optimization frameworks the
list of algorithms may look rather short, however, each al-
gorithm is customizable and variants can be initialized with
diï¬€erent parameters. For instance, a Steady-State NSGA-
II [34] can be initialized by setting the number of oï¬€spring

Julian Blank, Kalyanmoy Deb

Page 6 of 12

pymooOptimizationProblemsAnalyticsMating SelectionCrossoverMutationRepairSurvivalDecompositionsingle-objectivemulti-objectivemany-objectiveVisualizationPerformance IndicatorDecision MakingSamplingTermination CriterionConstraint HandlingParallelizationArchitectureGradientspymoo: Multi-objective Optimization in Python

Table 2
Multi-objective Optimization Test problems.

Problem

Variables Objectives

Constraints

Single-Objective

-

2

-

-

1

4

-

-

-

-

-

(â‹…)

2

10

-

6

2

1

4

(s)

-

-

-

-

-

-

-

-

Ackley

Cantilevered Beams

Griewank

Himmelblau

Knapsack

Pressure Vessel

Rastrigin

Rosenbrock

Schwefel

Sphere

Zakharov

G1-9

BNH

Carside

Kursawe

OSY

TNK

Truss2D

Welded Beam

CTP1-8

ZDT1-3

ZDT4

ZDT5

ZDT6

DTLZ 1-7

CDTLZ
DTLZ1âˆ’1
SDTLZ

(s)

4

(s)

2

(s)

4

(s)

(s)

(s)

(s)

(s)

(â‹…)

Multi-Objective

2

7

3

6

2

3

4

(s)

(30)

(10)

(80)

(10)

Many-Objective

(s)

(s)

(s)

(s)

1

1

1

1

1

1

1

1

1

1

1

(â‹…)

2

3

2

2

2

2

2

2

2

2

2

2

(s)

(s)

(s)

(s)

Table 3
Multi-objective Optimization Algorithms.

Algorithm

Reference

GA

DE

NSGA-II

RNSGA-II

[38]

[12]

[14]

NSGA-III

[10, 26, 4]

UNSGA-III

RNSGA-III

MOEAD

[43]

[47]

[52]

to 1. This can be achieved by supplying this as a parameter
in the initialization method as shown in Section 3.

6.2. Operators

The following evolutionary operators are available:

(i) Sampling: The initial population is mostly based on
sampling. In some cases it is created through domain
knowledge and/or some solutions are already evalu-
ated, they can directly be used as an initial population.
Otherwise, it can be sampled randomly for real, inte-
ger, or binary variables. Additionally, Latin-Hypercube
Sampling [33] can be used for real variables.

(ii) Crossover: A variety of crossover operators for dif-
ferent type of variables are implemented. In Figure 4
some of them are presented. Figures 4a- 4d help to vi-
sualize the information exchange in a crossover with
two parents being involved. Each row represents an
oï¬€spring and each column a variable. The correspond-
ing boxes indicate whether the values of the oï¬€spring
are inherited from the ï¬rst or from the second par-
ent. For one and two point crossovers it can be ob-
served that either one or two cuts in the variable se-
quence exist. Contrarily, the Uniform Crossover (UX)
does not have any clear pattern, because each vari-
able is chosen randomly either from the ï¬rst or from
the second parent. For the Half Uniform Crossover
(HUX) half of the variables, which are diï¬€erent, are
exchanged. For the purpose of illustration, we have
created two parents that have diï¬€erent values in 10 dif-
ferent positions. For real variables, Simulated Binary
Crossover [13] is known to be an eï¬ƒcient crossover. It
mimics the crossover of binary encoded variables. In
Figure 4e the probability distribution when the parents
ğ‘¥1 = 0.2 and ğ‘¥2 = 0.8 where ğ‘¥ğ‘– âˆˆ [0, 1] with ğœ‚ = 0.8
are recombined is shown. Analogously, in case of
integer variables we subtract 0.5 from the lower and
add 0.5 âˆ’ ğœ– to the upper bound before applying the
crossover and round to the nearest integer afterwards
(see Figure 4f).

(iii) Mutation: For real and integer variables Polynomial
Mutation [13] and for binary variables Bitï¬‚ip muta-
tion is provided.

Diï¬€erent problems require diï¬€erent type of operators. In
practice, if a problem is supposed to be solved repeatedly,
it makes sense to customize the evolutionary operators to
improve the convergence of the algorithm. Moreover, for
custom variable types, for instance trees or mixed variables,
custom operators can be implemented easily and called by
algorithm class. Our software documentation contains ex-
amples for custom modules, operators and variable types.

6.3. Termination Criterion

For every algorithm it must be determined when it should
terminate a run. This can be simply based on a predeï¬ned
number of function evaluations, iterations, or more advanced
criteria such as the change of a performance metric over
time. For example, we have implemented a termination cri-
terion based on the design space diï¬€erence between genera-
tions. To make the termination criterion more robust the last
ğ‘˜ generations are considered. The largest movement from a

Julian Blank, Kalyanmoy Deb

Page 7 of 12

pymoo: Multi-objective Optimization in Python

(a) One Point

(b) Two Point

(c) UX

(d) HUX

(e) SBX (real, eta=0.8)

(f) SBX (int, eta=3)

Figure 4: Illustration of some crossover operators for diï¬€erent
variables types.

solution to its closest neighbour is tracked across generation
and whenever it is below a certain threshold the algorithm is
considered to have converged. Analogously, the movement
in the objective space can be chosen for termination in pymoo.

6.4. Decomposition

Decomposition transforms multi-objective problems into
many single-objective optimization problems [42]. Such a
technique can be either embedded in a multi-objective al-
gorithm and solved simultaneously or independently using
a single-objective optimizer. Some decomposition methods
are based on the lp-metrics with diï¬€erent ğ‘ values. For in-
stance, a naive but frequently used decomposition approach
is the Weighted-Sum Method (ğ‘ = 1), which is known to
be not able to converge to the non-convex part of a Pareto-
front [11]. Moreover, instead of summing values, Tchebysh-
eï¬€ Method (ğ‘ = âˆ) considers only the maximum value of
the diï¬€erence between the ideal point and a solution. Sim-
ilarly, the Achievement Scalarization Function (ASF) [49]
and a modiï¬ed version Augmented Achievement Scalariza-
tion Function (AASF) [50] use the maximum of all diï¬€er-
ences. Furthermore, Penalty Boundary Intersection (PBI) [52]
is calculated by a weighted sum of the norm of the projection
of a point onto the reference direction and the perpendicu-
lar distance. Also it is worth to note that normalization is
essential for any kind of decomposition. All decomposition
techniques mentioned above are implemented in pymoo.

7. Analytics
7.1. Performance Indicators

For single-objective optimization algorithms the com-
parison regarding performance is rather simple because each
optimization run results in a single best solution. In multi-
objective optimization, however, each run returns a non-dominated
set of solutions. To compare sets of solutions various per-
formance indicators have been proposed in the past [29]. In
pymoo most commonly used performance indicators are de-
scribed:

(i) GD/IGD: Given the Pareto-front PF the deviation be-
tween the non-dominated set S found by the algorithm
and the optimum can be measured. Following this
principle, Generational Distance (GD) Indicator [46]
calculates the average euclidean distance in the ob-
jective space from each solution in S to the closest
solution in PF. This measures the convergence of S,
but does not indicate whether a good diversity on the
Pareto-front has been reached. Similarly, Inverted Gen-
erational Distance (IGD) Indicator [46] measures the
average Euclidean distance in the objective space from
each solution in PF to the closest solution in S. The
Pareto-front as a whole needs to be covered by so-
lutions from S to minimize the performance metric.
However, IGD is known to be not Pareto compliant [24].

(ii) GD+/IGD+: A variation of GD and IGD has been
proposed in [24]. The Euclidean distance is replaced
by a distance measure that takes the dominance re-
lation into account. The authors show that IGD+ is
weakly Pareto compliant.

(iii) Hypervolume: Moreover, the dominated portion of
the objective space can be used to measure the qual-
Instead of the
ity of non-dominated solutions [54].
Pareto-front a reference point needs to be provided.
It has been shown that Hypervolume is Pareto com-
pliant [53]. Because the performance metric becomes
computationally expensive in higher dimensional spaces
the exact measure becomes intractable. However, we
plan to include some proposed approximation meth-
ods in the near future.

Performance indicators are used to compare existing al-
gorithms. Moreover, the development of new algorithms can
be driven by the goodness of diï¬€erent metrics itself.

7.2. Visualization

The visualization of intermediate steps or the ï¬nal re-
sult is inevitable. In multi and many-objective optimization
visualization of the objective space is of interest especially,
and focuses on visualizing trade-oï¬€s between solutions. De-
pending on the dimensionality diï¬€erent types of plots are
suitable to represent a single or a set of solutions. In pymoo
the implemented visualizations wrap around the well-known
plotting library in Python Matplotlib [23]. Keyword argu-
ments provided by Matplotlib itself are still available which

Julian Blank, Kalyanmoy Deb

Page 8 of 12

0100Variables050Individuals0100Variables050Individuals0100Variables050Individuals0100Variables050Individuals0.20.8xp(x)1010xp(x)pymoo: Multi-objective Optimization in Python

allows to modify for instance the color, thickness, opacity
of lines, points or other shapes. Therefore, all visualization
techniques are customizable and extendable.

For 2 or 3 objectives, scatter plots (see Figure 5a and
5b) can give a good intuition about the solution set. Trade-
oï¬€s can be observed by considering the distance between
two points. It might be desired to normalize each objective
to make sure a comparison between values is based on rel-
ative and not absolute values. Pairwise Scatter Plots (see
Figure 5c) visualize more than 3 objectives by showing each
pair of axes independently. The diagonal is used to label the
corresponding objectives.

Also, high-dimensional data can be illustrated by Paral-
lel Coordinate Plots (PCP) as shown in Figure 5d. All axes
are plotted vertically and represent an objective. Each so-
lution is illustrated by a line from the left to the right. The
intersection of a line and an axis indicate the value of the
solution regarding the corresponding objective. For the pur-
pose of comparison solution(s) can be highlighted by vary-
ing color and opacity.

Moreover, a common practice is to project the higher di-
mensional objective values onto the 2D plane using a trans-
formation function. Radviz (Figure 5e) visualizes all points
in a circle and the objective axes are uniformly positioned
around on the perimeter. Considering a minimization prob-
lem and a non-dominated set of solutions, an extreme point
very close to an axis represents the worst solution for that
corresponding objective, but is comparably "good" in one or
many other objectives. Similarly, Star Coordinate Plots (Fig-
ure 5f) illustrate the objective space, except that the transfor-
mation function allows solutions outside of the circle.

Heatmaps (Figure 5g) are used to represent the goodness
of solutions through colors. Each row represents a solution
and each column a variable. We leave the choice to the end-
user of what color map to use and whether light or dark col-
ors illustrate better or worse solutions. Also, solutions can
be sorted lexicographically by their corresponding objective
values.

Instead of visualizing a set of solutions, one solution can
be illustrated at a time. The Petal Diagram (Figure 5h) is
a pie diagram where the objective value is represented by
each pieceâ€™s diameter. Colors are used to further distinguish
the pieces. Finally, the Spider-Web or Radar Diagram (Fig-
ure 5i) shows the objectives values as a point on an axis.
The ideal and nadir point [11] is represented by the inner
and outer polygon. By deï¬nition the solution lies in between
those two extremes. If the objective space ranges are scaled
diï¬€erently, normalization for the purpose of plotting can be
enabled and the diagram becomes symmetric.

7.3. Decision Making

In practice, after obtaining a set of non-dominated solu-
tions a single solution has to be chosen for implementation.

(i) Compromise Programming: One way of making a
decision is to compute value of a scalarized and aggre-
gated function and select one solution based on min-
imum or maximum value of the function. In pymoo a

number of scalarization functions described in Sec-
tion 6.4 can be used to come to a decision regarding
desired weights of objectives.

(ii) Pseudo-Weights: However, a more intuitive way to
chose a solution out of a Pareto-front is the pseudo-
weight vector approach proposed in [11]. The pseudo
weight ğ‘¤ğ‘–
for the ğ‘–-th objective function is calculated
by:

ğ‘¤ğ‘– =

(ğ‘“ max
ğ‘–
ğ‘š=1(ğ‘“ max

âˆ’ ğ‘“ğ‘–(ğ‘¥)) âˆ• (ğ‘“ max
ğ‘š âˆ’ ğ‘“ğ‘š(ğ‘¥)) âˆ• (ğ‘“ max

âˆ’ ğ‘“ min
ğ‘–
ğ‘š âˆ’ ğ‘“ min
ğ‘š )

)

ğ‘–

âˆ‘ğ‘€

. (5)

The normalized distance to the worst solution regard-
ing each objective ğ‘– is calculated. It is interesting to
note that for non-convex Pareto-fronts, the pseudo weight
does not correspond to the result of an optimization
using the weighted sum method.

(iii) High Trade-Oï¬€ Solutions: Furthermore, high trade-
oï¬€ solutions are usually of interest, but not straightfor-
ward to detect in higher-dimensional objective spaces.
We have implemented the procedure proposed in [40].
It was described to be embedded in an algorithm to
guide the search; we, however, use it for post-processing.
The metric for each solution pair ğ‘¥ğ‘–
dominated set is given by:

in a non-

and ğ‘¥ğ‘—

ğ‘‡ (ğ‘¥ğ‘–, ğ‘¥ğ‘—) =

âˆ‘ğ‘€

ğ‘–=1 max[0, ğ‘“ğ‘š(ğ‘¥ğ‘—) âˆ’ ğ‘“ğ‘š(ğ‘¥ğ‘–)]
ğ‘–=1 max[0, ğ‘“ğ‘š(ğ‘¥ğ‘–) âˆ’ ğ‘“ğ‘š(ğ‘¥ğ‘—)]

âˆ‘ğ‘€

, (6)

where the numerator represents the aggregated sacri-
ï¬ce and the denominator the aggregated gain. The
trade-oï¬€ measure ğœ‡(ğ‘¥ğ‘–, ğ‘†) for each solution ğ‘¥ğ‘–
with
respect to a set of solutions ğ‘† is obtained by:

ğœ‡(ğ‘¥ğ‘–, ğ‘†) = min
ğ‘¥ğ‘— âˆˆğ‘†

ğ‘‡ (ğ‘¥ğ‘–, ğ‘¥ğ‘—)

(7)

to all other so-
It ï¬nds the minimum ğ‘‡ (ğ‘¥ğ‘–, ğ‘¥ğ‘—) from ğ‘¥ğ‘–
lutions ğ‘¥ğ‘— âˆˆ ğ‘†. Instead of calculating the metric with
respect to all others, we provide the option to only con-
sider the ğ‘˜ closest neighbors in the objective space to
reduce the computational complexity.

Multi-objective frameworks should include methods for
multi-criteria decision making and support end-user further
in choosing a solution out of a trade-oï¬€ solution set.

8. Concluding Remarks

This paper has introduced pymoo, a multi-objective op-
timization framework in Python. We have walked through

Julian Blank, Kalyanmoy Deb

Page 9 of 12

pymoo: Multi-objective Optimization in Python

(a)

Scatter 2D

(b) Scatter 3D

(c) Scatter ND [6]

(d) PCP [48]

(e) Radviz [22]

(f) Star Coordinate Graph [27]

(g) Heatmap [39]

(h) Petal Diagram [44]

(i) Spider-Web/Radar [28]

Figure 5: Diï¬€erent visualization methods coded in pymoo.

our framework beginning with the installation up to the opti-
mization of a constrained bi-objective optimization problem.
Moreover, we have presented the overall architecture of the
framework consisting of three core modules: Problems, Op-
timization, and Analytics. Each module has been described
in depth and illustrative examples have been provided. We
have shown that our framework covers various aspects of
multi-objective optimization including the visualization of
high-dimensional spaces and multi-criteria decision making
to ï¬nally select a solution out of the obtained solution set.
One distinguishing feature of our framework with other ex-
isting ones is that we have provided a few options for various
key aspects of a multi-objective optimization task, providing
standard evolutionary operators for optimization, standard
performance metrics for evaluating a run, standard visualiza-
tion techniques for showcasing obtained trade-oï¬€ solutions,
and a few approaches for decision-making. Most such im-
plementations were originally suggested and developed by
the second author and his collaborators for more than 25

years. Hence, we consider that the implementations of all
such ideas are authentic and error-free. Thus, the results
from the proposed framework should stay as benchmark re-
sults of implemented procedures.

However, the framework can be extended to make it more
extensive. In the future, we plan to implement a more opti-
mization algorithms and test problems to provide more choices
to end-users. Also, we aim to implement some methods
from the classical literature on single-objective optimization
which can also be used for multi-objective optimization through
decomposition or embedded as a local search. So far, we
have provided a few basic performance metrics. We plan to
extend this by creating a module that runs a list of algorithms
on test problems automatically and provides a statistics of
diï¬€erent performance indicators.

Furthermore, we like to mention that any kind of contri-
bution is more than welcome. We see our framework as a
collaborative collection from and to the multi-objective op-
timization community. By adding a method or algorithm

Julian Blank, Kalyanmoy Deb

Page 10 of 12

0.000.250.500.75f10.50.00.51.0f2f10.00.10.20.30.40.5f20.00.10.20.30.40.5f30.00.10.20.30.40.5f1f10.00.5f10.00.5f20.00.5f10.00.5f30.00.5f10.00.5f40.00.5f20.00.5f1f2f20.00.5f20.00.5f30.00.5f20.00.5f40.00.5f30.00.5f10.00.5f30.00.5f2f3f30.00.5f30.00.5f40.00.5f40.00.5f10.00.5f40.00.5f20.00.5f40.00.5f3f4f4f1f2f3f4f5f6f7f8f9f10f1f2f3f4f5f6f7f8f9f10f1f2f3f4f5f6f7f8f9f10f1f2f3f4f5f6f1f2f3f4f5f6f1f2f3f4f5pymoo: Multi-objective Optimization in Python

to pymoo the community can beneï¬t from a growing com-
prehensive framework and it can help researchers to adver-
tise their methods. In general, diï¬€erent kinds of contribu-
tions are possible and more information can be found online.
Moreover, we would like to mention that even though we try
to keep our framework as bug-free as possible, in case of ex-
ceptions during the execution or doubt of correctness, please
contact us directly or use our issue tracker.

References
[1] Behnel, S., Bradshaw, R., Citro, C., Dalcin, L., Seljebotn, D., Smith,
K., 2011. Cython: The best of both worlds. Computing in Science
Engineering 13, 31 â€“39. doi:10.1109/MCSE.2010.118.

[2] BenÃ­tez-Hidalgo, A., Nebro, A.J., GarcÃ­a-Nieto, J., Oregi, I., Ser, J.D.,
2019. jmetalpy: a python framework for multi-objective optimization
with metaheuristics. CoRR abs/1903.02915. URL: http://arxiv.org/
abs/1903.02915, arXiv:1903.02915.

[3] Biscani, F.,

Izzo, D., Yam, C.H., 2010.

A global op-
timisation toolbox for massively parallel engineering optimisa-
tion. CoRR abs/1004.3824. URL: http://arxiv.org/abs/1004.3824,
arXiv:1004.3824.

[4] Blank, J., Deb, K., Roy, P.C., 2019. Investigating the normalization
procedure of NSGA-III, in: Deb, K., Goodman, E., Coello Coello,
C.A., Klamroth, K., Miettinen, K., Mostaghim, S., Reed, P. (Eds.),
Evolutionary Multi-Criterion Optimization, Springer International
Publishing, Cham. pp. 229â€“240.

[5] BÃ¼cker, M., Corliss, G., Hovland, P., Naumann, U., Norris, B., 2006.
Automatic Diï¬€erentiation: Applications, Theory, and Implementa-
tions (Lecture Notes in Computational Science and Engineering).
Springer-Verlag, Berlin, Heidelberg.

[6] Chambers, J.M., Kleiner, B., 1982. 10 graphical techniques for mul-

tivariate data and for clustering.

[7] Dask Development Team, 2016. Dask: Library for dynamic task

scheduling. URL: https://dask.org.

[8] Deb, K., 1995. Optimization for Engineering Design: Algorithms

and Examples. New Delhi: Prentice-Hall.

[9] Deb, K., Datta, R., 2012. A bi-objective constrained optimization
algorithm using a hybrid evolutionary and penalty function approach.
Engineering Optimization 45, 503â€“527.

[10] Deb, K., Jain, H., 2014. An evolutionary many-objective optimiza-
tion algorithm using reference-point-based nondominated sorting ap-
proach, part I: Solving problems with box constraints. IEEE Transac-
tions on Evolutionary Computation 18, 577â€“601. doi:10.1109/TEVC.
2013.2281535.

[11] Deb, K., Kalyanmoy, D., 2001. Multi-Objective Optimization Using
Evolutionary Algorithms. John Wiley & Sons, Inc., New York, NY,
USA.

[12] Deb, K., Pratap, A., Agarwal, S., Meyarivan, T., 2002. A fast and
elitist multiobjective genetic algorithm: NSGA-II. Trans. Evol. Comp
6, 182â€“197. URL: http://dx.doi.org/10.1109/4235.996017, doi:10.
1109/4235.996017.

[13] Deb, K., Sindhya, K., Okabe, T., 2007. Self-adaptive simulated binary
crossover for real-parameter optimization, in: Proceedings of the 9th
Annual Conference on Genetic and Evolutionary Computation, ACM,
New York, NY, USA. pp. 1187â€“1194. URL: http://doi.acm.org/10.
1145/1276958.1277190, doi:10.1145/1276958.1277190.

[14] Deb, K., Sundar, J., 2006. Reference point based multi-objective op-
timization using evolutionary algorithms, in: Proceedings of the 8th
Annual Conference on Genetic and Evolutionary Computation, ACM,
New York, NY, USA. pp. 635â€“642. URL: http://doi.acm.org/10.
1145/1143997.1144112, doi:10.1145/1143997.1144112.

[15] Durillo, J.J., Nebro, A.J., 2011.

jMetal: a java framework for
multi-objective optimization. Advances in Engineering Software 42,
760â€“771. URL: http://www.sciencedirect.com/science/article/pii/
S0965997811001219, doi:DOI:10.1016/j.advengsoft.2011.05.014.
[16] Fortin, F.A., De Rainville, F.M., Gardner, M.A., Parizeau, M., GagnÃ©,

C., 2012. DEAP: Evolutionary algorithms made easy.
Machine Learning Research 13, 2171â€“2175.

Journal of

[17] Foundation, P.S., . PyPI: the python package index. https://pypi.org.

Accessed: 2019-05-20.

[18] Garrett, A., . inspyred: python library for bio-inspired computational
intelligence. https://github.com/aarongarrett/inspyred. Accessed:
2019-05-16.

[19] Gosling, J., Joy, B., Steele, G.L., Bracha, G., Buckley, A., 2014. The
Java Language Speciï¬cation, Java SE 8 Edition. 1st ed., Addison-
Wesley Professional.

[20] Hadka, D., a. MOEA Framework: a free and open source Java frame-
work for multiobjective optimization. http://moeaframework.org. Ac-
cessed: 2019-05-16.

[21] Hadka, D., b. Platypus: multiobjective optimization in python. https:

//platypus.readthedocs.io. Accessed: 2019-05-16.

[22] Hoï¬€man, P., Grinstein, G., Pinkney, D., 1999. Dimensional an-
chors: a graphic primitive for multidimensional multivariate infor-
mation visualizations, in: Proc. Workshop on new Paradigms in Infor-
mation Visualization and Manipulation in conjunction with the ACM
International Conference on Information and Knowledge Manage-
ment (NPIVM99), ACM, New York, NY, USA. pp. 9â€“16. doi:http:
//doi.acm.org/10.1145/331770.331775.

[23] Hunter, J.D., 2007. Matplotlib: A 2d graphics environment. Comput-

ing in Science & Engineering 9, 90â€“95. doi:10.1109/MCSE.2007.55.

[24] Ishibuchi, H., Masuda, H., Tanigaki, Y., Nojima, Y., 2015. Modiï¬ed
distance calculation in generational distance and inverted generational
distance, in: Gaspar-Cunha, A., Henggeler Antunes, C., Coello, C.C.
(Eds.), Evolutionary Multi-Criterion Optimization, Springer Interna-
tional Publishing, Cham. pp. 110â€“125.

[25] Izzo, D., 2012. PyGMO and PyKEP: open source tools for massively
parallel optimization in astrodynamics (the case of interplane-
tary trajectory optimization),
5th International Conference
in:
on Astrodynamics Tools and Techniques (ICATT 2012). URL:

http://www.esa.int/gsp/ACT/doc/MAD/pub/ACT-RPR-MAD-2012-(ICATT)
PyKEP-PaGMO-SOCIS.pdf.

[26] Jain, H., Deb, K., 2014. An evolutionary many-objective optimiza-
tion algorithm using reference-point based nondominated sorting ap-
proach, part II: Handling constraints and extending to an adaptive ap-
proach. IEEE Transactions on Evolutionary Computation 18, 602â€“
622.

[27] Kandogan, E., 2000. Star coordinates: A multi-dimensional visual-
ization technique with uniform treatment of dimensions, in: In Pro-
ceedings of the IEEE Information Visualization Symposium, Late
Breaking Hot Topics, pp. 9â€“12.

[28] Kasanen, E., Ã–stermark, R., Zeleny, M., 1991. Gestalt system
of holistic graphics: New management support view of MCDM.
Computers & OR 18, 233â€“239. URL: https://doi.org/10.1016/
0305-0548(91)90093-7, doi:10.1016/0305-0548(91)90093-7.

[29] Knowles, J., Corne, D., 2002. On metrics for comparing non-
dominated sets, in: Proceedings of the 2002 Congress on Evolu-
tionary Computation Conference (CEC02), Institute of Electrical and
Electronics Engineers, United States. pp. 711â€“716.

[30] Lehmann, R., 2019. Sphinx documentation.
[31] LÃ³pez-Camacho, E., GarcÃ­a-Godoy, M.J., Nebro, A.J., Montes,
J.F.A., 2014.
jmetalcpp: optimizing molecular docking problems
with a C++ metaheuristic framework. Bioinformatics 30, 437â€“438.
URL: https://doi.org/10.1093/bioinformatics/btt679, doi:10.1093/
bioinformatics/btt679.

[32] Maclaurin, D., Duvenaud, D., Adams, R.P., 2015. Autograd:
Eï¬€ortless gradients in numpy, in: ICML 2015 AutoML Workshop.
URL:

/bib/maclaurin/maclaurinautograd/automl-short.pdf,https:

//indico.lal.in2p3.fr/event/2914/session/1/contribution/6/3/
material/paper/0.pdf,https://github.com/HIPS/autograd.

[33] McKay, M.D., Beckman, R.J., Conover, W.J., 2000. A comparison of
three methods for selecting values of input variables in the analysis
of output from a computer code. Technometrics 42, 55â€“61. URL:
http://dx.doi.org/10.2307/1271432, doi:10.2307/1271432.

[34] Mishra, S., Mondal, S., Saha, S., 2016. Fast implementation of

Julian Blank, Kalyanmoy Deb

Page 11 of 12

pymoo: Multi-objective Optimization in Python

ence on Evolutionary Multi-criterion Optimization, Springer-Verlag,
Berlin, Heidelberg. pp. 862â€“876. URL: http://dl.acm.org/citation.
cfm?id=1762545.1762618.

[54] Zitzler, E., Thiele, L., 1998. Multiobjective optimization using evo-
lutionary algorithms - a comparative case study, in: Proceedings of
the 5th International Conference on Parallel Problem Solving from
Nature, Springer-Verlag, London, UK, UK. pp. 292â€“304. URL:
http://dl.acm.org/citation.cfm?id=645824.668610.

steady-state nsga-ii, in: 2016 IEEE Congress on Evolutionary Com-
putation (CEC), pp. 3777â€“3784. doi:10.1109/CEC.2016.7744268.
[35] Oliphant, T., 2006â€“. NumPy: A guide to NumPy. USA: Trelgol
Publishing. URL: http://www.numpy.org/. [Online; accessed May 16,
2019].

[36] Pajankar, A., 2017. Python Unit Test Automation: Practical Tech-
niques for Python Developers and Testers. 1st ed., Apress, Berkely,
CA, USA.

[37] Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito,
Z., Lin, Z., Desmaison, A., Antiga, L., Lerer, A., 2017. Automatic
diï¬€erentiation in pytorch .

[38] Price, K., Storn, R.M., Lampinen, J.A., 2005. Diï¬€erential Evolution:
A Practical Approach to Global Optimization (Natural Computing Se-
ries). Springer-Verlag, Berlin, Heidelberg.

[39] Pryke, A., Mostaghim, S., Nazemi, A., 2007. Heatmap visualization
of population based multi objective algorithms, in: Obayashi, S., Deb,
K., Poloni, C., Hiroyasu, T., Murata, T. (Eds.), Evolutionary Multi-
Criterion Optimization, Springer Berlin Heidelberg, Berlin, Heidel-
berg. pp. 361â€“375.

[40] Rachmawati, L., Srinivasan, D., 2009. Multiobjective evolutionary
algorithm with controllable focus on the knees of the pareto front.
Evolutionary Computation, IEEE Transactions on 13, 810 â€“ 824.
doi:10.1109/TEVC.2009.2017515.

[41] Rossum, G., 1995. Python Reference Manual. Technical Report.

Amsterdam, The Netherlands, The Netherlands.

[42] Santiago, A., Huacuja, H.J.F., Dorronsoro, B., Pecero, J.E., Santillan,
C.G., Barbosa, J.J.G., Monterrubio, J.C.S., 2014. A Survey of De-
composition Methods for Multi-objective Optimization. Springer In-
ternational Publishing, Cham. pp. 453â€“465. URL: https://doi.org/
10.1007/978-3-319-05170-3_31, doi:10.1007/978-3-319-05170-3_31.

[43] Seada, H., Deb, K., 2016. A uniï¬ed evolutionary optimization pro-
cedure for single, multiple, and many objectives. IEEE Transactions
on Evolutionary Computation 20, 358â€“369. doi:10.1109/TEVC.2015.
2459718.

[44] Tan, Y.S., Fraser, N.M., 1998. The modiï¬ed star graph and the
petal diagram: two new visual aids for discrete alternative multicrite-
ria decision making. Journal of Multi-Criteria Decision Analysis 7,
20â€“33. doi:10.1002/(SICI)1099-1360(199801)7:1<20::AID-MCDA159>3.
0.CO;2-R.

[45] Tian, Y., Cheng, R., Zhang, X., Jin, Y., 2017. PlatEMO: A MAT-
LAB platform for evolutionary multi-objective optimization.
IEEE
Computational Intelligence Magazine 12, 73â€“87.

[46] Veldhuizen, D.A.V., Veldhuizen, D.A.V., 1999. Multiobjective Evo-
lutionary Algorithms: Classiï¬cations, Analyses, and New Innova-
tions. Technical Report. Evolutionary Computation.

[47] Vesikar, Y., Deb, K., Blank, J., 2018. Reference point based NSGA-III
for preferred solutions, in: 2018 IEEE Symposium Series on Compu-
tational Intelligence (SSCI), pp. 1587â€“1594. doi:10.1109/SSCI.2018.
8628819.

[48] Wegman, E., 1990. Hyperdimensional data analysis using parallel
coordinates. Journal of the American Statistical Association 85, 664â€“
675. doi:10.1080/01621459.1990.10474926.

[49] Wierzbicki, A.P., 1980. The use of reference objectives in multiob-
jective optimization, in: Multiple criteria decision making theory and
application. Springer, pp. 468â€“486.

[50] Wierzbicki, A.P., 1982. A mathematical basis for satisï¬cing decision
making. Mathematical Modelling 3, 391 â€“ 405. URL: http://www.
sciencedirect.com/science/article/pii/0270025582900380, doi:https:
//doi.org/10.1016/0270-0255(82)90038-0. special IIASA Issue.
[51] Wolpert, D.H., Macready, W.G., 1997. No free lunch theorems for
optimization. Trans. Evol. Comp 1, 67â€“82. URL: https://doi.org/
10.1109/4235.585893, doi:10.1109/4235.585893.

[52] Zhang, Q., Li, H., 2007. A multi-objective evolutionary algorithm
based on decomposition. IEEE Transactions on Evolutionary Com-
putation, Accepted 2007.

[53] Zitzler, E., Brockhoï¬€, D., Thiele, L., 2007. The hypervolume in-
dicator revisited: On the design of pareto-compliant indicators via
weighted integration, in: Proceedings of the 4th International Confer-

Julian Blank, Kalyanmoy Deb

Page 12 of 12

