1
2
0
2

n
a
J

1
3

]
L
P
.
s
c
[

3
v
7
1
6
4
0
.
3
0
0
2
:
v
i
X
r
a

Diﬀerentiate Everything with a Reversible Embeded
Domain-Speciﬁc Language

Jin-Guo Liu
Institute of Physics, Chinese Academy of Sciences,
Beijing 100190, China
cacate0129@iphy.ac.cn

Taine Zhao
Department of Computer Science, University of Tsukuba
thaut@logic.cs.tsukuba.ac.jp

Abstract

Reverse-mode automatic diﬀerentiation (AD) suﬀers from the issue of having too
much space overhead to trace back intermediate computational states for
back-propagation.
The traditional method to trace back states is called
checkpointing that stores intermediate states into a global stack and restore state
through either stack pop or re-computing. The overhead of stack manipulations
and re-computing makes the general purposed (not tensor-based) AD engines
unable to meet many industrial needs. Instead of checkpointing, we propose to
use reverse computing to trace back states by designing and implementing a
reversible programming eDSL, where a program can be executed bi-directionally
without implicit stack operations. The absence of implicit stack operations makes
the program compatible with existing compiler features,
including utilizing
existing optimization passes and compiling the code as GPU kernels. We
implement AD for sparse matrix operations and some machine learning
applications to show that our framework has the state-of-the-art performance.

1

Introduction

Most of the popular automatic diﬀerentiation (AD) tools in the market, such as TensorFlow (Abadi
et al., 2015), Pytorch (Paszke et al., 2017), and Flux (Innes et al., 2018) implements reverse mode
AD at the tensor level to meet the need in machine learning. Later, People in the scientiﬁc computing
domain also realized the power of these AD tools, they use these tools to solve scientiﬁc problems
such as seismic inversion (Zhu et al., 2020), variational quantum circuits simulation (Bergholm et al.,
2018; Luo et al., 2019) and variational tensor network simulation (Liao et al., 2019; Roberts et al.,
2019). To meet the diverse need in these applications, one sometimes has to deﬁne backward rules
manually, for example

1. To diﬀerentiate sparse matrix operations used in Hamiltonian engineering (Hao Xie &
Wang), people deﬁned backward rules for sparse matrix multiplication and dominant
eigensolvers (Golub & Van Loan, 2012),

2. In tensor network algorithms to study the phase transition problem (Liao et al., 2019; Seeger
et al., 2017; Wan & Zhang, 2019; Hubig, 2019), people deﬁned backward rules for singular
value decomposition (SVD) function and QR decomposition (Golub & Van Loan, 2012).

Instead of deﬁning backward rules manually, one can also use a general purposed AD (GP-AD)
framework like Tapenade (Hascoet & Pascual, 2013), OpenAD (Utke et al., 2008) and

Preprint. Under review.

 
 
 
 
 
 
Zygote (Innes, 2018; Innes et al., 2019). Researchers have used these tools in practical applications
such as bundle adjustment (Shen & Dai, 2018) and earth system simulation (Forget et al., 2015),
where diﬀerentiating scalar operations is important. However, the power of these tools are often
limited by their relatively poor performance. In many practical applications, a program might do
billions of computations. In each computational step, the AD engine might cache some data for
backpropagation. (Griewank & Walther, 2008) Frequent caching of data slows down the program
signiﬁcantly, while the memory usage will become a bottleneck as well. Caching implicitly also
make these frameworks incompatible with kernel functions. To avoid such issues, we need a new
GP-AD framework that does not cache automatically for users.

In this paper, we propose to implement the reverse mode AD on a reversible (domain-speciﬁc)
programming language (Perumalla, 2013; Frank, 2017), where intermediate states can be traced
backward without accessing an implicit stack. Reversible programming allows people to utilize the
reversibility to reverse a program.
In machine learning, reversibility is proven to substantially
decrease the memory usage in unitary recurrent neural networks (MacKay et al., 2018),
normalizing ﬂow (Dinh et al., 2014), hyper-parameter learning (Maclaurin et al., 2015) and
residual neural networks (Gomez et al., 2017; Behrmann et al., 2018). Reversible programming
will make these happen naturally. The power of reversible programming is not limited to handling
these reversible applications, any program can be written in a reversible style. Converting an
irreversible program to the reversible form would cost overheads in time and space. Reversible
diﬀerent with
programming
checkpointing (Griewank, 1992; Griewank & Walther, 2008; Chen et al., 2016), reverse
computing (Bennett, 1989; Levine & Sherman, 1990), to let user handle these overheads explicitly.

trade-oﬀ scheme

time-space

provides

ﬂexible

that

a

as

Jr,

such

1999)

devices

adiabatic

(Frank & Knight

There have been many prototypes of reversible languages like Janus (Lutz, 1986), R (not the
popular one) (Frank, 1997), Erlang (Lanese et al., 2018) and object-oriented ROOPL (Haulund,
2017). In the past, the primary motivation to study reversible programming is to support reversible
computing
complementary
metal-oxide-semiconductor (CMOS) (Koller & Athas, 1992), molecular mechanical computing
system (Merkle et al., 2018) and superconducting system (Likharev, 1977; Semenov et al., 2003;
Takeuchi et al., 2014, 2017), and these reversible computing devices are orders more
energy-eﬃcient. Landauer proves that only when a device does not erase information (i.e.
reversible), its energy eﬃciency can go beyond the thermal dynamic limit. (Landauer, 1961; Reeb
& Wolf, 2014) However, these reversible programming languages can not be used directly in real
scientiﬁc computing, since most of them do not have basic elements like ﬂoating point numbers,
arrays, and complex numbers. This motivates us to build a new embedded domain-speciﬁc
language (eDSL) in Julia (Bezanson et al., 2012, 2017) as a new playground of GP-AD.
In this paper, we ﬁrst compare the time-space trade-oﬀ in the optimal checkpointing and the optimal
reverse computing in Sec. 2. Then we introduce the language design of NiLang in Sec. 3. In Sec. 4,
we explain the implementation of automatic diﬀerentiation in NiLang. In Sec. 5, we benchmark the
performance of NiLang’s AD with other AD software and explain why it is fast.

2 Reverse computing as an Alternative to Checkpointing

One can use either checkpointing or reverse computing to trace back intermediate states of a T -
step computational process s1 = f1(s0), s2 = f2(s1), . . . , sT = fT (sT −1) with a run-time memory
S . In the checkpointing scheme, the program ﬁrst takes snapshots of states at certain time steps
S = {sa, sb, . . .}, 1 ≤ a < b < ... ≤ T by running a forward pass. When retrieving a state sk, if sk ∈ S ,
In the reverse
just return this state, otherwise, return max

s j<k ∈ S and re-compute sk from s j.

j

computing scheme, one ﬁrst writes the program in a reversible style. Without prior knowledge, a
regular program can be transpiled to the reversible style is by doing the transformation in Listing. 1.

2

Listing 1: Transpiling a regular code to the
reversible code without prior knowledge.

s1 += f1(s0)
s2 += f2(s1)
. . .
sT += fT (sT −1)

Listing 2: The reverse of Listing. 1

sT −= fT (sT −1)
. . .
s2 −= f2(s1)
s1 −= f1(s0)

Then one can visit states in the reversed order by running the reversed program in Listing. 2, which
erases the computed results from the tail. One may argue that easing through uncomputing is not
necessary here. This is not true for a general reversible program, because the intermediate states
might be mutable and used in other parts of the program.
It is easy to see, both checkpointing
and reverse computing can trace back states without time overhead, but both suﬀer from a space
overhead that linear to time (Table 1). The checkpointing scheme snapshots the output in every
step, and the reverse computing scheme allocates extra storage for storing outputs in every step. On
the other side, only checkpointing can achieve a zero space overhead by recomputing everything
from the beginning s0, with a time complexity O(T 2). The minimum space complexity in reverse
computing is O(S log(T/S )) (Bennett, 1989; Levine & Sherman, 1990; Perumalla, 2013), with time
complexity O(T 1.585).

Method

most time eﬃcient
(Time/Space)

most space eﬃcient
(Time/Space)

Checkpointing
Reverse computing

O(T )/O(T + S )
O(T )/O(T + S )

O(T 2)/O(S )
O(T ( T

S )0.585)/O(S log( T

S ))

Table 1: T and S are the time and space of the original irreversible program.
computing” case, the reversibility of the original program is not utilized.

In the “Reverse

The diﬀerence in space overheads can be explained by the diﬀerence of the optimal checkpointing
and optimal reverse computing algorithms. The optimal checkpointing algorithm that widely used
in AD is the treeverse algorithm in Fig. 1(a). This algorithm partitions the computational process
binomially into d sectors. At the beginning of each sector, the program snapshots the state and push
it into a global stack, hence the memory for checkpointing is dS . The states in the last sector are
retrieved by the above space-eﬃcient O(T 2) algorithm. After that, the last snapshot can be freed and
the program has one more quota in memory. With the freed memory, the second last sector can be
further partition into two sectors. Likewise, the lth sectors is partitioned into l sub-sectors, where l
is the sector index counting from the tail. Recursively apply this treeverse algorithm t times until the
sector size is 1. The approximated overhead in time and space are

Tc = tT, S c = dS ,

(1)

where T = η(t, d) holds. By carefully choosing either a t or d, the overhead in time and space can
be both logarithmic.
On the other side, the optimal time-space trade-oﬀ scheme in reverse computing is the Bennett’s
algorithm illustrated in Fig. 1 (b). It evenly evenly partition the program into k sectors. The program
marches forward (P process) for k steps to obtain the ﬁnal state sk+1, then backward (Q process) from
the k − 1th step to erase the states in between s1<i<k. This process is also called the compute-copy-
uncompute process. Recursively apply the compute-copy-uncompute process for each sector until
each P/Q process contains only one unit computation. the time and space complexities are

(cid:19) ln(2−(1/k))

ln k

Tr = T

(cid:18) T
S

, S r = k − 1
ln k

S log

T
S

.

(2)

Here, the overhead in time is polynomial, which is worse than the treeverse algorithm. The
treeverse like partition does not apply here because the ﬁrst sweep to create initial checkpoints
without introducing any space overheads is not possible in reversible computing. The pseudo-code
of Bennett’s time-space trade-oﬀ algorithm is shown in Listing. 3.

3

Figure 1: (a) Treeverse algorithm for optimal checkpointing. (Griewank, 1992) η(τ, δ) ≡

(cid:33)

(cid:32)

τ + δ
δ

=

is the binomial function.

(τ+δ)!
(b) Bennett’s time space trade-oﬀ scheme for reverse com-
τ!δ!
puting. (Bennett, 1973; Levine & Sherman, 1990) P and Q are computing and uncomputing
respectively. The pseudo-code is deﬁned in Listing. 3.

Listing 3: The Bennett’s time-space trade-oﬀ scheme. The ﬁrst argument {s1,...} is
the collection of states, k is the number of partitions, i and len are the starting point
and length of the working sector. A function call changes variables inplace. “∼” is the
symbol of uncomputing, which means undoing a function call. Statement si+1 ← 0
allocates a zero state and add it to the state collection. Its inverse si+1 → 0 discards a
zero cleared state from the collection. Its NiLang implementation is in Appendix A.

bennett({s1, ...}, k, i, len)

if len == 1
si+1 ← 0
fi(si+1, si)

else

# P process that calls the forward program k steps
bennett({s1, ...}, k, i+len÷k*(j-1), len÷k) for j=1,2,...,k
# Q process that calls the backward program k-1 steps
~bennett({s1, ...}, k, i+len÷k*(j-1), len÷k) for j=k-1,k-2,...,1

The reverse computing does not show advantage from the above complexity analysis. But we argue
the this analysis is from the worst case, which are very diﬀerent to the practical using cases. First,
reverse computing can make use of the reversibility to save memory. In Appendix B.2, we show
how to implement a unitary matrix multiplication without introducing overheads in space and time.
Second, reverse computing does not allocate automatically for users, user can optimize the memory
access patterns for their own devices like GPU. Third, reverse computing is compatible with eﬀective
codes, so that it ﬁts better with modern languages. In Appendix B.1, we show how to manipulate
inplace functions on arrays with NiLang. Fourth, reverse computing can utilize the existing compiler
to optimize the code because it does not introduce global stack operations that harm the purity of
functions. Fifth, reverse computing encourages users to think reversibly. In Appendix B.3, we show
reversible thinking can lead the user to a constant memory, constant time implementation of chained
multiplication algorithms.

3 Language design

NiLang is an embedded domain-speciﬁc language (eDSL) NiLang built on top of the host language
Julia (Bezanson et al., 2012, 2017). Julia is a popular language for scientiﬁc programming and
machine learning. We choose Julia mainly for speed. Julia is a language with high abstraction,
however, its clever design of type inference and just in time compiling make it has a C like speed.
Meanwhile, it has rich features for meta-programming. Its package for pattern matching MLStyle

4

...η(t, 3)η(t, 2)η(t, 1)η(t-1, 2)η(t-1, 1)...P2P2P2Q2Q2P1P1P1Q1Q1(b)(a)allows us to deﬁne an eDSL in less than 2000 lines. Comparing with a regular reversible
programming language, NiLang features array operations,
rich number systems including
ﬂoating-point numbers, complex numbers, ﬁxed-point numbers, and logarithmic numbers. It also
implements the compute-copy-uncompute (Bennett, 1973) macro to increase code reusability.
Besides the above reversible hardware compatible features, it also has some reversible hardware
incompatible features to meet the practical needs. For example, it views the ﬂoating-point + and −
operations as reversible.
It also allows users to extend instruction sets and sometimes inserting
external statements. These features are not compatible with future reversible hardware. NiLang’s
compiling process, grammar and operational semantics are described in Appendix G. The source
https://github.com/GiggleLiu/NiLang.jl,
code
https://github.com/GiggleLiu/NiLangCore.jl. By the time of writing, the version of NiLang is
v0.7.3.

available

online:

also

is

3.1 Reversible functions and instructions

Mathematically, any irreversible mapping y = f(args...) can be trivially transformed to its
reversible form y += f(args...) or y (cid:89)= f(args...) ((cid:89) is the bit-wise XOR), where y is a
pre-emptied variable. But in numeric computing with ﬁnite precision, this is not always true. The
reversibility of arithmetic instruction is closely related to the number system. For integer and ﬁxed
point number system, y += f(args...) and y -= f(args...) are rigorously reversible. For
logarithmic number system and tropical number system (Speyer & Sturmfels, 2009), y *=
f(args...) and y /= f(args...) as reversible (not introducing the zero element). While for
ﬂoating point numbers, none of the above operations are rigorously reversible. However, for
convenience, we ignore the round-oﬀ errors in ﬂoating-point + and - operations and treat them on
equal footing with ﬁxed-point numbers in the following discussion. In Appendix F, we will show
doing this is safe in most cases provided careful implementation. Other reversible operations
includes SWAP, ROT, NEG et. al., and this instruction set is extensible. One can deﬁne a reversible
multiplier in NiLang as in Listing. 4.

Listing 4: A reversible multiplier

julia> using NiLang

julia> @i function multiplier(y!::Real, a::Real, b::Real)

y! += a * b

end

julia> multiplier(2, 3, 5)
(17, 3, 5)

julia> (~multiplier)(17, 3, 5)
(2, 3, 5)

Macro @i generates two functions that are reversible to each other, multiplier and ∼multiplier,
each deﬁnes a mapping R3 → R3. The ! after a symbol is a part of the name, as a conversion to
indicate the mutated variables.

3.2 Reversible memory management

A distinct feature of reversible memory management is that the content of a variable must be known
when it is deallocated. We denote the allocation of a pre-emptied memory as x ← 0, and its inverse,
deallocating a zero emptied variable, as x → 0. An unknown variable can not be deallocate,
but can be pushed to a stack pop out later in the uncomputing stage.
If a variable is allocated
and deallocated in the local scope, we call it an ancilla. Listing. 5 deﬁnes the complex valued
accumulative log function.

5

Listing 5: Reversible complex valued log
function y += log(|x|) + iArg(x).

@i @inline function (:+=)(log)(y!::Complex{T

}, x::Complex{T}) where T

n ← zero(T)
n += abs(x)

y!.re += log(n)
y!.im += angle(x)

n -= abs(x)
n → zero(T)

end

Listing 6: Compute-copy-uncompute ver-
sion of Listing. 5

@i @inline function (:+=)(log)(y!::Complex{T

}, x::Complex{T}) where T

@routine begin

n ← zero(T)
n += abs(x)

end
y!.re += log(n)
y!.im += angle(x)
~@routine

end

Here, the macro @inline tells the compiler that this function can be inlined. One can input “←”
and “→” by typing “\leftarrow[TAB KEY]” and “\rightarrow[TAB KEY]” respectively in a Julia
editor or REPL. NiLang does not have immutable structs, so that the real part y!.re and imaginary
y!.im of a complex number can be changed directly. It is easy to verify that the bottom two lines
i.e., the bottom two lines uncomputes
in the function body are the inverse of the top two lines.
the top two lines. The motivation of uncomputing is to zero clear the contents in ancilla n so that
it can be deallocated correctly. Compute-copy-uncompute is a useful design pattern in reversible
programming so that we created a pair of macros @routine and ∼@routine for it. One can rewrite
the above function as in Listing. 6.

3.3 Reversible control ﬂows

One can deﬁne reversible if, for and while statements in a reversible program. Fig. 2 (a) shows
the ﬂow chart of executing the reversible if statement. There are two condition expressions in
this chart, a precondition and a postcondition. The precondition decides which branch to enter in the
forward execution, while the postcondition decides which branch to enter in the backward execution.
The pseudo-code for the forward and backward passes are shown in Listing. 7 and Listing. 8.

Listing 7: Translating a reversible if state-
ment (forward)

Listing 8: Translating a reversible if state-
ment (backward)

branchkeeper = precondition
if precondition
branch A

else

branch B

branchkeeper = postcondition
if postcondition

else

~(branch A)

~(branch B)

end
assert branchkeeper == postcondition

end
assert branchkeeper == precondition

Fig. 2 (b) shows the ﬂow chart of the reversible while statement.
It also has two condition
expressions. Before executing the condition expressions, the program presumes the postcondition
is false. After each iteration, the program asserts the postcondition to be true. To reverse this
statement, one can exchange the precondition and postcondition, and reverse the body statements.
The pseudo-code for the forward and backward passes are shown in Listing. 9 and Listing. 10.

Listing 9: Translating a reversible while
statement (forward)

Listing 10: Translating a reversible while
statement (backward)

assert postcondition == false
while precondition
loop body
assert postcondition == true

end

assert precondition == false
while postcondition
~(loop body)
assert precondition == true

end

6

Figure 2: The ﬂow chart for reversible (a) if statement and (b) while statement. “pre” and “post”
represents precondition and postcondition respectively. The assersion errors are thrown to the host
language instead of handling them in NiLang.

The reversible for statement is similar to the irreversible one except that after execution, the
program will assert the iterator to be unchanged. To reverse this statement, one can exchange
start and stop and inverse the sign of step. Listing. 11 computes the Fibonacci number
recursively and reversibly.

Listing 11: Computing Fibonacci number recursively and reversibly.

@i function rrfib(out!, n)

@invcheckoff if (n >= 1, ~)

counter ← 0
counter += n
while (counter > 1, counter!=n)
rrfib(out!, counter-1)
counter -= 2

end
counter -= n % 2
counter → 0

end
out! += 1

end

Here, out! is an integer initialized to 0 for storing outputs. The precondition and postcondition are
wrapped into a tuple. In the if statement, the postcondition is the same as the precondition, hence
we omit the postcondition by inserting a "∼" in the second ﬁeld for “copying the precondition in
this ﬁeld as the postcondition”. In the while statement, the postcondition is true only for the initial
loop. Once code is proven correct, one can turn oﬀ the reversibility check by adding @invcheckoff
before a statement. This will remove the reversibility check and make the code faster and compatible
with GPU kernels (kernel functions can not handle exceptions).

4 Reversible automatic diﬀerentiation

4.1 Back propagation

We decompose the problem of reverse mode AD into two sub-problems, reversing the code and
computing
∂[multiple outputs] . Reversing the code is trivial in reversible programming. Computing the
gradient here is similar to forward mode automatic diﬀerentiation that computes ∂[multiple outputs]
.

∂[single input]

∂[single input]

7

prestmts1stmts2FTTFerrorpostpoststmtsFTTFerrorpreTerrorFpre equals to post(a)(b)original program Oreverse program Rgradient program for Rcompute reversereversegradient program for OInspired by the Julia package ForwardDiﬀ (Revels et al., 2016), we use the operator overloading
technique to diﬀerentiate the program eﬃciently.
In the backward pass, we wrap each output
variable with a composite type GVar that containing an extra gradient ﬁeld, and feed it into the
reversed generic program.
Instructions are multiple dispatched to corresponding gradient
instructions that update the gradient ﬁeld of GVar at the meantime of uncomputing. By reversing
this gradient program, we can obtain the gradient program for the reversed program too. One can
deﬁne the adjoint (“adjoint” here means the program for back-propagating gradients) of a primitive
instruction as a reversible function on either the function itself or its reverse since the adjoint of a
function’s reverse is equivalent to the reverse of the function’s adjoint.

f :((cid:126)x, (cid:126)gx) → ((cid:126)y, (cid:126)gT
x

f −1:((cid:126)y, (cid:126)gy) → ((cid:126)x, (cid:126)gT
y

∂(cid:126)x
∂(cid:126)y
∂(cid:126)y
∂(cid:126)x

)

)

(3)

(4)

It can be easily veriﬁed by applying the above two mappings consecutively, which turns out to be an
identity mapping considering ∂(cid:126)y
∂(cid:126)x

= 1.

∂(cid:126)x
∂(cid:126)y

The implementation details are described in Appendix C. In most languages, operator overloading
is accompanied with signiﬁcant overheads of function calls and object allocation and deallocation.
But in a language with type inference and just in time compiling like Julia, the boundary between
two approaches are vague. The compiler inlines small functions, packs an array of constant sized
immutable objects into a continuous memory, and truncates unnecessary branches automatically.

4.2 Hessians

Combining forward mode AD and reverse mode AD is a simple yet eﬃcient way to obtain Hessians.
By wrapping the elementary type with Dual deﬁned in package ForwardDiﬀ and throwing it into
the gradient program deﬁned in NiLang, one obtains one row/column of the Hessian matrix. We
will use this approach to compute Hessians in the graph embedding benchmark in Sec. D.2.

4.3 CUDA kernels

CUDA programming is playing a signiﬁcant role in high-performance computing. In Julia, one can
write GPU compatible functions in native Julia language with KernelAbstractions. (Besard et al.,
2017) Since NiLang does not push variables into stack automatically for users, it is safe to write
diﬀerentiable GPU kernels with NiLang. We will diﬀerentiate CUDA kernels with no more than
extra 10 lines in the bundle adjustment benchmark in Sec. 5.

5 Benchmarks

We benchmark our framework with the state-of-the-art GP-AD frameworks, including source code
transformation based Tapenade and Zygote and operator overloading based ForwardDiﬀ and
ReverseDiﬀ. Since most tensor based AD software like famous TensorFlow and PyTorch are not
designed for the using cases used in our benchmarks, we do not include those package to avoid an
unfair comparison. In the following benchmarks, the CPU device is Intel(R) Xeon(R) Gold 6230
CPU @ 2.10GHz, and the GPU device is NVIDIA Titan V. For NiLang benchmarks, we have
turned the reversibility check oﬀ to achieve a better performance.

We reproduced the benchmarks for Gaussian mixture model (GMM) and bundle adjustment in
Srajer et al. (2018) by re-writing the programs in a reversible style. We show the results in Fig. 3.
The Tapenade data is obtained by executing the docker ﬁle provided by the original benchmark,
which provides a baseline for comparison.

NiLang’s objective function is ∼2× slower than normal code due to the uncomputing overhead. In
this case, NiLang does not show advantage to Tapenade in obtaining gradients, the ratio between
computing the gradients and the objective function are close. This is because the bottleneck of this
model is the matrix vector multiplication, traditional AD can already handle this function well. The
extra memory used to reverse the program is negligible comparing to the original program as shown
in Fig. 4. The backward pass is not shown here, it is just two times the reversible program in order

8

Figure 3: Absolute runtimes in seconds for computing the objective (-O) and Jacobians (-J). (a)
GMM with 10k data points, the loss function has a single output, hence computing Jacobian is the
same as computing gradient. ForwardDiﬀ data is missing due to not ﬁnishing in limited time. The
NiLang GPU data is missing because we do not write kernel here. (b) Bundle adjustment.

Figure 4: Peak memory of running the original and the reversible GMM program. The labels are
(d, k) pairs.

to store gradients. The data is obtained by counting the main memory allocations in the program
manually. The analytical expression of memory usage in unit of ﬂoating point number is

S = (2 + d2)k + 2d + P,
S r = (3 + d2 + d)k + 2log2k + P,

(5)

(6)

where d and k are the size and number of covariance matrices. P = d(d+1)
k + k + dk is the size of
parameter space. The memory of the dataset (d×N) is not included because it will scale as N. Due to
the hardness of estimating peak memory usage, the Tapenade data is missing here. The ForwardDiﬀ
memory usage is approximately the original size times the batch size, where the batch size is 12 by
default.

2

In the bundle adjustment benchmark, NiLang performs the best on CPU. We also compiled our
adjoint program to GPU with no more than 10 lines of code with KernelAbstractions, which provides
another ∼200× speed up. Parallelizing the adjoint code requires the forward code not reading the
same variable simultaneously in diﬀerent threads, and this requirement is satisﬁed here. The peak
memory of the original program and the reversible program are both equal to the size of parameter
space because all “allocation”s happen on registers in this application.
One can ﬁnd more benchmarks in Appendix D, including diﬀerentiating sparse matrix dot product
and obtaining Hessians in the graph embedding application.

9

105106107# of parameters103101101(b) Bundle AdjustmentJulia-ONiLang-OTapenade-OForwardDiff-JNiLang-JTapenade-JNiLang-J (GPU)103105# of parameters102100102seconds(a) Gaussian Mixture Model102103104105# of parameters102103104105106peak memory(2, 5)(10, 5)(2, 200)(10, 50)(64, 5)(64, 10)(64, 25)(64, 200)regularreversible6 Discussion

In this work, we demonstrate a new approach to back propagates a program called reverse computing
AD by designing a reversible eDSL NiLang. NiLang is a powerful tool to diﬀerentiate code from
the source code level so that can be directly useful to machine learning researches. It can generate
eﬃcient backward rules, which is exempliﬁed in Appendix E. It can also be used to diﬀerentiate
reversible neural networks like normalizing ﬂows (Kobyzev et al., 2019) to save memory, e.g. back-
propagating NICE network (Dinh et al., 2014) with only constant space overheads. NiLang is most
useful in solving large scale scientiﬁc problems memory eﬃciently. In Liu et al. (2020), people
solve the ground state problem of a 28 × 28 square lattice spin-glass by re-writing the quantum
simulator with NiLang. There are some challenges in reverse computing AD too.

• The native BLAS and convolution operations in NiLang are not optimized for the memory
layout, and are too slow comparing with state of the art machine learning libraries. We
need a better implementation of these functions in the reversible programming context so
that it can be more useful in training traditional deep neural networks.

• Although we show some examples of training neural networks on GPU, the shared-reading

of a variable is not allowed.

• NiLang’s IR does not have variable analysis. The uncomputing pass is not always necessary
for the irreversible host language to deallocate memory. In many cases, the host language’s
variable analysis can ﬁgure this out, but it is not guarantted.

Another interesting issue is how to make use of reversible computing devices to save energy in
machine learning. Reversible computing is not always more energy eﬃcient than irreversible
computing. In the time-space trade-oﬀ scheme in Sec. 2, we show the time to uncompute a unit of
memory is exponential to n as Qn = (2k − 1)n, and the computing energy also increases
exponentially. On the other side, the amount of energy to erase a unit of memory is a constant.
When (2k − 1)n > 1/ξ, erasing the memory irreversibly is more energy-eﬃcient, where ξ is the
energy ratio between a reversible operation (an instruction or a gate) and its irreversible
counterpart.

Acknowledgments

The authors are grateful to the people who help improve this work and fundings that sponsored the
research. To meet the anonymous criteria, we will add the acknowledgments after the open review
session.

References

Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoﬀrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,
Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin
Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine
learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software
available from tensorﬂow.org.

Jens Behrmann, David Duvenaud, and Jörn-Henrik Jacobsen. Invertible residual networks. CoRR,

abs/1811.00995, 2018. URL http://arxiv.org/abs/1811.00995.

Charles H. Bennett. Logical reversibility of computation. 1973. URL https://ieeexplore.

ieee.org/abstract/document/5391327.

Charles H. Bennett. Time/space trade-oﬀs for reversible computation. SIAM Journal on Computing,
18(4):766–776, 1989. doi: 10.1137/0218053. URL https://doi.org/10.1137/0218053.

Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, M. Sohaib Alam, Shahnawaz
Ahmed, Juan Miguel Arrazola, Carsten Blank, Alain Delgado, Soran Jahangiri, Keri McKiernan,

10

Johannes Jakob Meyer, Zeyue Niu, Antal Száva, and Nathan Killoran. Pennylane: Automatic
diﬀerentiation of hybrid quantum-classical computations, 2018. URL https://arxiv.org/
abs/1811.04968.

Tim Besard, Christophe Foket, and Bjorn De Sutter. Eﬀective extensible programming: Unleashing
julia on gpus. CoRR, abs/1712.03112, 2017. URL http://arxiv.org/abs/1712.03112.

Jeﬀ Bezanson, Stefan Karpinski, Viral B. Shah, and Alan Edelman. Julia: A fast dynamic language

for technical computing, 2012. URL https://arxiv.org/abs/1209.5145.

Jeﬀ Bezanson, Alan Edelman, Stefan Karpinski, and Viral B. Shah. Julia: A fresh approach to
numerical computing. SIAM Review, 59(1):65–98, Jan 2017. ISSN 1095-7200. doi: 10.1137/
141000671. URL http://dx.doi.org/10.1137/141000671.

Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear
memory cost. CoRR, abs/1604.06174, 2016. URL http://arxiv.org/abs/1604.06174.

Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components

estimation, 2014. URL https://arxiv.org/abs/1410.8516.

GAEL Forget, J-M Campin, Patrick Heimbach, Christopher N Hill, Rui M Ponte, and Carl Wunsch.
Ecco version 4: An integrated framework for non-linear inverse modeling and global ocean state
estimation. 2015. URL https://dspace.mit.edu/handle/1721.1/99660.

Michael P Frank. The r programming language and compiler. Technical report, MIT Reversible

Computing Project Memo, 1997.

Michael P. Frank. Throwing computing into reverse. IEEE Spectrum, 54(9):32–37, Sep 2017. ISSN
0018-9235. doi: 10.1109/mspec.2017.8012237. URL http://dx.doi.org/10.1109/MSPEC.
2017.8012237.

Michael Patrick Frank and Thomas F Knight Jr. Reversibility for eﬃcient computing. PhD
thesis, 1999. URL http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.
428.4962&rep=rep1&type=pdf.

Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU press, 2012.

residual

Aidan N Gomez, Mengye Ren, Raquel Urtasun,
network:

The
reversible
In
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
Information Processing Systems 30, pp.
and R. Garnett
URL http://papers.nips.cc/paper/
2214–2224. Curran Associates,
6816-the-reversible-residual-network-backpropagation-without-storing-activations.
pdf.

and Roger B Grosse.
activations.

(eds.), Advances in Neural

Backpropagation without

Inc., 2017.

storing

Andreas Griewank. Achieving logarithmic growth of temporal and spatial complexity in reverse
automatic diﬀerentiation. Optimization Methods and software, 1(1):35–54, 1992. URL https:
//www.tandfonline.com/doi/abs/10.1080/10556789208805505.

Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques of

algorithmic diﬀerentiation. SIAM, 2008.

Jin-Guo Liu Hao Xie and Lei Wang. Automatic diﬀerentiation of dominant eigensolver and its

applications in quantum physics. URL https://arxiv.org/abs/2001.04121.

Laurent Hascoet and Valérie Pascual. The tapenade automatic diﬀerentiation tool: Principles, model,
and speciﬁcation. ACM Transactions on Mathematical Software (TOMS), 39(3):20, 2013. URL
https://dl.acm.org/citation.cfm?id=2450158.

Tue Haulund. Design and implementation of a reversible object-oriented programming language,

2017. URL https://arxiv.org/abs/1707.07845.

Claudius Hubig. Use and implementation of autodiﬀerentiation in tensor network methods with

complex scalars, 2019. URL https://arxiv.org/abs/1907.13422.

11

Michael Innes. Don’t unroll adjoint: Diﬀerentiating ssa-form programs, 2018. URL https://

arxiv.org/abs/1810.07951.

Michael

Innes, Elliot Saba, Keno Fischer, Dhairya Gandhi, Marco Concetto Rudilosso,
Neethu Mariya Joy, Tejan Karmali, Avik Pal, and Viral Shah. Fashionable modelling with ﬂux,
2018. URL https://arxiv.org/abs/1811.01457.

Mike Innes, Alan Edelman, Keno Fischer, Christopher Rackauckas, Elliot Saba, Viral B. Shah, and
Will Tebbutt. A diﬀerentiable programming system to bridge machine learning and scientiﬁc
computing. CoRR, abs/1907.07587, 2019. URL http://arxiv.org/abs/1907.07587.

Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and
Marin Soljaci´c. Tunable eﬃcient unitary neural networks (eunn) and their application to rnns,
2016. URL https://arxiv.org/abs/1612.05231.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. URL https:

//arxiv.org/abs/1412.6980.

Ivan Kobyzev, Simon Prince, and Marcus A. Brubaker. Normalizing ﬂows: An introduction and

review of current methods, 2019.

J. G. Koller and W. C. Athas. Adiabatic switching, low energy computing, and the physics of storing
and erasing information. In Workshop on Physics and Computation, pp. 267–270, Oct 1992. doi:
10.1109/PHYCMP.1992.615554. URL https://ieeexplore.ieee.org/document/615554.

Rolf Landauer. Irreversibility and heat generation in the computing process. IBM journal of research
and development, 5(3):183–191, 1961. URL https://ieeexplore.ieee.org/document/
5392446.

Ivan Lanese, Naoki Nishida, Adrián Palacios, and Germán Vidal. A theory of reversibility for
Journal of Logical and Algebraic Methods in Programming, 100:71–97, Nov 2018.
erlang.
ISSN 2352-2208. doi: 10.1016/j.jlamp.2018.06.004. URL http://dx.doi.org/10.1016/
j.jlamp.2018.06.004.

Robert Y Levine and Alan T Sherman. A note on bennett’s time-space tradeoﬀ for reversible
computation. SIAM Journal on Computing, 19(4):673–677, 1990. URL https://epubs.siam.
org/doi/abs/10.1137/0219046.

Hai-Jun Liao, Jin-Guo Liu, Lei Wang, and Tao Xiang. Diﬀerentiable programming tensor networks.
Physical Review X, 9(3), Sep 2019. ISSN 2160-3308. doi: 10.1103/physrevx.9.031041. URL
http://dx.doi.org/10.1103/PhysRevX.9.031041.

K. Likharev. Dynamics of some single ﬂux quantum devices:
Transactions on Magnetics, 13(1):242–244, January 1977.
TMAG.1977.1059351. URL https://ieeexplore.ieee.org/document/1059351.

IEEE
ISSN 1941-0069. doi: 10.1109/

I. parametric quantron.

Jin-Guo Liu, Lei Wang, and Pan Zhang. Tropical tensor network for ground states of spin glasses,

2020. URL https://arxiv.org/abs/2008.06888.

Xiu-Zhe Luo, Jin-Guo Liu, Pan Zhang, and Lei Wang. Yao.jl: Extensible, eﬃcient framework for

quantum algorithm design, 2019. URL https://arxiv.org/abs/1912.10877.

Christopher Lutz. Janus: a time-reversible language. Letter to R. Landauer., 1986.

Matthew MacKay, Paul Vicol,

Reversible recurrent
neural networks.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 9029–9040. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
8117-reversible-recurrent-neural-networks.pdf.

Jimmy Ba, and Roger B Grosse.

Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter opti-
In Francis Bach and David Blei (eds.), Proceedings
mization through reversible learning.
of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of
Machine Learning Research, pp. 2113–2122, Lille, France, 07–09 Jul 2015. PMLR. URL
http://proceedings.mlr.press/v37/maclaurin15.html.

12

Ralph C. Merkle, Robert A. Freitas, Tad Hogg, Thomas E. Moore, Matthew S. Moses, and James
Ryley. Mechanical computing systems using only links and rotary joints. Journal of Mechanisms
and Robotics, 10(6), Sep 2018. ISSN 1942-4310. doi: 10.1115/1.4041209. URL http://dx.
doi.org/10.1115/1.4041209.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic diﬀerentiation in
In NIPS Autodiﬀ Workshop, 2017. URL https://openreview.net/forum?id=
PyTorch.
BJJsrmfCZ.

Kalyan S Perumalla. Introduction to reversible computing. Chapman and Hall/CRC, 2013.

David Reeb and Michael M Wolf. An improved landauer principle with ﬁnite-size corrections. New
Journal of Physics, 16(10):103011, 2014. URL https://iopscience.iop.org/article/
10.1088/1367-2630/16/10/103011/meta.

Jarrett Revels, Miles Lubin, and Theodore Papamarkou. Forward-mode automatic diﬀerentiation in

julia, 2016. URL https://arxiv.org/abs/1607.07892.

Chase Roberts, Ashley Milsted, Martin Ganahl, Adam Zalcman, Bruce Fontaine, Yijian Zou, Jack
Hidary, Guifre Vidal, and Stefan Leichenauer. Tensornetwork: A library for physics and machine
learning, 2019. URL https://arxiv.org/abs/1905.01330.

Matthias Seeger, Asmus Hetzel, Zhenwen Dai, Eric Meissner, and Neil D. Lawrence. Auto-

diﬀerentiating linear algebra, 2017. URL https://arxiv.org/abs/1710.08717.

V. K. Semenov, G. V. Danilov, and D. V. Averin. Negative-inductance squid as the basic element
of reversible josephson-junction circuits. IEEE Transactions on Applied Superconductivity, 13
ISSN 2378-7074. doi: 10.1109/TASC.2003.814155. URL https:
(2):938–943, June 2003.
//ieeexplore.ieee.org/document/1211760.

Yan Shen and Yuxing Dai. Fast automatic diﬀerentiation for large scale bundle adjustment. IEEE
Access, 6:11146–11153, 2018. URL https://ieeexplore.ieee.org/abstract/document/
8307241/.

David Speyer and Bernd Sturmfels. Tropical mathematics. Mathematics Magazine, 82(3):163–
173, 2009. URL https://www.tandfonline.com/doi/pdf/10.1080/0025570X.2009.
11953615.

Filip Srajer, Zuzana Kukelova, and Andrew Fitzgibbon. A benchmark of selected algorithmic
diﬀerentiation tools on some problems in computer vision and machine learning. Optimization
Methods and Software, 33(4-6):889–906, 2018. URL https://openreview.net/forum?id=
SMCWZLzTGDN.

Jun Takahashi and Anders W. Sandvik. Valence-bond solids, vestigial order, and emergent so(5)
symmetry in a two-dimensional quantum magnet, 2020. URL https://arxiv.org/abs/2001.
10045.

N Takeuchi, Y Yamanashi, and N Yoshikawa. Reversible logic gate using adiabatic superconducting
Scientiﬁc reports, 4:6354, 2014. URL https://www.nature.com/articles/

devices.
srep06354.

Naoki Takeuchi, Yuki Yamanashi, and Nobuyuki Yoshikawa. Reversibility and energy dissipation
in adiabatic superconductor logic. Scientiﬁc reports, 7(1):1–12, 2017. URL https://www.
nature.com/articles/s41598-017-00089-9.

Clay S Turner. A fast binary logarithm algorithm [dsp tips & tricks]. IEEE Signal Processing Mag-
azine, 27(5):124–140, 2010. URL http://www.claysturner.com/dsp/binarylogarithm.
pdf.

Jean Utke, Uwe Naumann, Mike Fagan, Nathan Tallent, Michelle Strout, Patrick Heimbach, Chris
Hill, and Carl Wunsch. Openad/f: A modular open-source tool for automatic diﬀerentiation of
ISSN 0098-3500. doi: 10.1145/
fortran codes. ACM Trans. Math. Softw., 34(4), July 2008.
1377596.1377598. URL https://doi.org/10.1145/1377596.1377598.

13

Zhou-Quan Wan and Shi-Xin Zhang. Automatic diﬀerentiation for complex valued svd, 2019. URL

https://arxiv.org/abs/1909.02659.

Weiqiang Zhu, Kailai Xu, Eric Darve, and Gregory C. Beroza. A general approach to seismic
inversion with automatic diﬀerentiation, 2020. URL https://arxiv.org/abs/2003.06027.

A NiLang implementation of Bennett’s time-space trade-oﬀ algorithm

Listing 12: NiLang implementation of the Bennett’s time-space trade-oﬀ scheme.
using NiLang, Test

PROG_COUNTER = Ref(0)
PEAK_MEM = Ref(0)

# (2k-1)^n
# n*(k-1)+2

@i function bennett(f::AbstractVector, state::Dict{Int,T}, k::Int, base, len) where T

if (len == 1, ~)

state[base+1] ← zero(T)
f[base](state[base+1], state[base])
@safe PROG_COUNTER[] += 1
@safe (length(state) > PEAK_MEM[] && (PEAK_MEM[] = length(state)))

else

n ← 0
n += len÷k
# the P process
for j=1:k

bennett(f, state, k, base+n*(j-1), n)

end
# the Q process
for j=k-1:-1:1

~bennett(f, state, k, base+n*(j-1), n)

end
n -= len÷k
n → 0

end

end

k = 4
n = 4
N = k ^ n
state = Dict(1=>1.0)
f(x) = x * 2.0
instructions = fill(PlusEq(f), N)

# run the program
@instr bennett(instructions, state, k, 1, N)

@test state[N+1] ≈ 2.0^N && length(state) == 2
@test PEAK_MEM[] == n*(k-1) + 2
@test PROG_COUNTER[] == (2*k-1)^n

The input f is a vector of functions and state is a dictionary. We also added some irreversible
external statements (those marked with @safe) to help analyse to program.

B Cases where reverse computing shows advantage

B.1 Handling eﬀective codes

Reverse computing can handling eﬀective codes with mutable structures and arrays. For example,
the aﬃne transformation can be implemented without any overhead.

14

Listing 13: Inplace aﬃne transformation.

@i function i_affine!(y!::AbstractVector{T}, W::AbstractMatrix{T}, b::AbstractVector{T}, x:

:AbstractVector{T}) where T

@safe @assert size(W) == (length(y!), length(x)) && length(b) == length(y!)
@invcheckoff for j=1:size(W, 2)

for i=1:size(W, 1)

@inbounds y![i] += W[i,j]*x[j]

end

end
@invcheckoff for i=1:size(W, 1)
@inbounds y![i] += b[i]

end

end

Here, the expression following the @safe macro is an external irreversible statement.

B.2 Utilizing reversibility

Reverse computing can utilize reversibility to trace back states without extra memory cost. For
example, we can deﬁne the unitary matrix multiplication that can be used in a type of memory-
eﬃcient recurrent neural network. (Jing et al., 2016)

Listing 14: Two level decomposition of a unitary matrix.

@i function i_umm!(x!::AbstractArray, θ)

M ← size(x!, 1)
N ← size(x!, 2)
k ← 0
@safe @assert length(θ) == M*(M-1)/2
for l = 1:N

for j=1:M

for i=M-1:-1:j
INC(k)
ROT(x![i,l], x![i+1,l], θ[k])

end

end

end
k → length(θ)

end

B.3 Encourages reversible thinking

Last but not least, reversible programming encourages users to code in a memory friendly style.
Since allocations in reversible programming are explicit, programmers have the ﬂexibility to control
how to allocate memory and which number system to use. For example, to compute the power of a
positive ﬁxed-point number and an integer, one can easily write irreversible code as in Listing. 15

Listing 15: A regular power function.
function mypower(x::T, n::Int) where T

y = one(T)
for i=1:n

y *= x

end
return y

end

Listing 16: A reversible power function.

@i function mypower(out,x::T,n::Int) where T

if (x != 0, ~)

@routine begin

ly ← one(ULogarithmic{T})
lx ← one(ULogarithmic{T})
lx *= convert(x)
for i=1:n

ly *= x

end

end
out += convert(ly)
~@routine

end

end

15

Since the ﬁxed-point number is not reversible under *=, naive checkpointing would require stack
operations inside a loop. With reversible thinking, we can convert the ﬁxed-point number to
logarithmic numbers to utilize the reversibility of *= as shown in Listing. 16. Here, the algorithm
to convert a regular ﬁxed-point number to a logarithmic number can be eﬃcient. (Turner, 2010)

C Implementation of AD in NiLang

To backpropagate the program, we ﬁrst reverse the code through source code transformation and then
insert the gradient code through operator overloading. If we inline all the functions in Listing. 6,
the function body would be like Listing. 17. The automatically generated inverse program (i.e.
(y, x) → (y − log(x), x)) would be like Listing. 18.

Listing 17: The inlined function body of
Listing. 6.

(cid:7)
@routine begin

nsq ← zero(T)
n ← zero(T)
nsq += x[i].re ^ 2
nsq += x[i].im ^ 2
n += sqrt(nsq)

end
y![i].re += log(n)
y![i].im += atan(x[i].im, x[i].re)
~@routine
(cid:6)

(cid:4)

(cid:5)

Listing 18: The inverse of Listing. 17.
(cid:7)
@routine begin

(cid:4)

nsq ← zero(T)
n ← zero(T)
nsq += x[i].re ^ 2
nsq += x[i].im ^ 2
n += sqrt(nsq)

end
y![i].re -= log(n)
y![i].im -= atan(x[i].im, x[i].re)
~@routine
(cid:6)

(cid:5)

To compute the adjoint of the computational process in Listing. 17, one simply insert the gradient
code into its inverse in Listing. 18. The resulting inlined code is show in Listing. 19.

Listing 19: Insert the gradient code into Listing. 18, the original computational

processes are highlighted in yellow background.

@routine begin

nsq ← zero(GVar{T,T})
n ← zero(GVar{T,T})

gsqa ← zero(T)
gsqa += x[i].re.x * 2
x[i].re.g -= gsqa * nsq.g
gsqa -= nsq.x * 2
gsqa -= x[i].re.x * 2
gsqa → zero(T)
nsq.x += x[i].re.x ^2

gsqb ← zero(T)
gsqb += x[i].im.x * 2
x[i].im.g -= gsqb * nsq.g
gsqb -= x[i].im.x * 2
gsqb → zero(T)
nsq.x += x[i].im.x ^2

@zeros T ra rb
rta += sqrt(nsq.x)
rb += 2 * ra
nsq.g -= n.g / rb
rb -= 2 * ra

ra -= sqrt(nsq.x)
~@zeros T ra rb
n.x += sqrt(nsq.x)

end

y![i].re.x -= log(n.x)
n.g += y![i].re.g / n.x

y![i].im.x-=atan(x[i].im.x,x[i].re.x)
@zeros T xy2 jac_x jac_y
xy2 += abs2(x[i].re.x)
xy2 += abs2(x[i].im.x)
jac_y += x[i].re.x / xy2
jac_x += (-x[i].im.x) / xy2
x[i].im.g += y![i].im.g * jac_y
x[i].re.g += y![i].im.g * jac_x
jac_x -= (-x[i].im.x) / xy2
jac_y -= x[i].re.x / xy2
xy2 -= abs2(x[i].im.x)
xy2 -= abs2(x[i].re.x)
~@zeros T xy2 jac_x jac_y

~@routine

Here, @zeros TYPE var1 var2... is the macro to allocate multiple variables of the same type.
Its inverse operations starts with ∼@zeros deallocates zero emptied variables.
In practice,
“inserting gradients” is not achieved by source code transformation, but by operator overloading.
We change the element type to a composite type GVar with two ﬁelds, value x and gradient g. With
multiple dispatching primitive instructions on this new type, values and gradients can be updated
simultaneously. Although the code looks much longer, the computing time (with reversibility check
closed) is not.

16

Listing 20: Time and allocation to diﬀerentiate complex valued log.

(cid:7)
julia> using NiLang, NiLang.AD, BenchmarkTools

julia> @inline function (ir_log)(x::Complex{T}) where T

log(abs(x)) + im*angle(x)

end

julia> @btime ir_log(x) setup=(x=1.0+1.2im); # native code

30.097 ns (0 allocations: 0 bytes)

julia> @btime (@instr y += log(x)) setup=(x=1.0+1.2im; y=0.0+0.0im); # reversible code

17.542 ns (0 allocations: 0 bytes)

julia> @btime (@instr ~(y += log(x))) setup=(x=GVar(1.0+1.2im, 0.0+0.0im); y=GVar(0.1+0.2im

, 1.0+0.0im)); # adjoint code
25.932 ns (0 allocations: 0 bytes)

(cid:6)

(cid:4)

(cid:5)

The performance is unreasonably good because the generated Julia code is further compiled to
LLVM so that it can enjoy existing optimization passes. For example, the optimization passes can
ﬁnd out that for an irreversible device, uncomputing local variables n and nsq to zeros does not aﬀect
return values, so that it will ignore the uncomputing process automatically. Unlike checkpointing
based approaches that focus a lot in the optimization of data caching on a global stack, NiLang does
not have any optimization pass in itself. Instead, it throws itself to existing optimization passes in
Julia. Without accessing the global stack, NiLang’s code is quite friendly to optimization passes. In
this case, we also see the boundary between source code transformation and operator overloading
can be vague in a Julia, in that the generated code can be very diﬀerent from how it looks.
The joint functions for primitive instructions (:+=)(sqrt) and (:-=)(sqrt) used above can be
deﬁned as in Listing. 21.

(cid:4)

(cid:5)

Listing 21: Adjoints for primitives (:+=)(sqrt) and (:-=)(sqrt).

(cid:7)
@i @inline function (:-=)(sqrt)(out!::GVar, x::GVar{T}) where T

@routine @invcheckoff begin

@zeros T a b
a += sqrt(x.x)
b += 2 * a

end
out!.x -= a
x.g += out!.g / b
~@routine

end(cid:6)

D More Benchmarks

D.1 Sparse matrices

We compare the call, uncall and backward propagation time used for sparse matrix dot product and
matrix multiplication in Table 2. Their reversible implementations are shown in Listing. 22 and
Listing. 23. The computing time for backward propagation is approximately 1.5-3 times the Julia’s
native forward pass, which is close to the theoretical optimal performance.

17

(cid:7)
using SparseArrays

Listing 22: Reversible sparse matrix multiplication.

(cid:4)

@i function i_dot(r::T, A::SparseMatrixCSC{T},B::SparseMatrixCSC{T}) where {T}

m ← size(A, 1)
n ← size(A, 2)
@invcheckoff branch_keeper ← zeros(Bool, 2*m)
@safe size(B) == (m,n) || throw(DimensionMismatch("matrices must have the same

dimensions"))

@invcheckoff @inbounds for j = 1:n

ia1 ← A.colptr[j]
ib1 ← B.colptr[j]
ia2 ← A.colptr[j+1]
ib2 ← B.colptr[j+1]
ia ← ia1
ib ← ib1
@inbounds for i=1:ia2-ia1+ib2-ib1-1

ra ← A.rowval[ia]
rb ← B.rowval[ib]
if (ra == rb, ~)

r += A.nzval[ia]' * B.nzval[ib]

end
## b move -> true, a move -> false
branch_keeper[i] (cid:89)= ia == ia2-1 || (ib != ib2-1 && ra > rb)
ra → A.rowval[ia]
rb → B.rowval[ib]
if (branch_keeper[i], ~)

INC(ib)

INC(ia)

else

end

end
~@inbounds for i=1:ia2-ia1+ib2-ib1-1

## b move -> true, a move -> false
branch_keeper[i] (cid:89)= ia == ia2-1 || (ib != ib2-1 && A.rowval[ia] > B.rowval[ib])
if (branch_keeper[i], ~)

INC(ib)

INC(ia)

else

end

end

end
@invcheckoff branch_keeper → zeros(Bool, 2*m)

end(cid:6)

(cid:7)
@i function i_mul!(C::StridedVecOrMat, A::AbstractSparseMatrix, B::StridedVector{T}, α::

Listing 23: Reversible sparse matrix dot-product.

Number, β::Number) where T

@safe size(A, 2) == size(B, 1) || throw(DimensionMismatch())
@safe size(A, 1) == size(C, 1) || throw(DimensionMismatch())
@safe size(B, 2) == size(C, 2) || throw(DimensionMismatch())
nzv ← nonzeros(A)
rv ← rowvals(A)
if (β != 1, ~)

@safe error("only β = 1 is supported, got β = $(β).")

end
# Here, we close the reversibility check inside the loop to increase performance
@invcheckoff for k = 1:size(C, 2)

@inbounds for col = 1:size(A, 2)

αxj ← zero(T)
αxj += B[col,k] * α
for j = SparseArrays.getcolptr(A)[col]:(SparseArrays.getcolptr(A)[col + 1] - 1)

C[rv[j], k] += nzv[j]*αxj

end
αxj -= B[col,k] * α

end

end

end(cid:6)

(cid:5)

(cid:4)

(cid:5)

18

Julia-O
NiLang-O
NiLang-B

dot

3.493e-04
4.675e-04
5.821e-04

mul! (complex valued)
8.005e-05
9.332e-05
2.214e-04

Table 2: Absolute runtimes in seconds for computing the objectives (O) and the
backward pass (B) of sparse matrix operations. The matrix size is 1000 × 1000,
and the element density is 0.05. The total time for computing gradients can be
estimated by summing “O” and “B”.

Figure 5: The Petersen graph has 10 vertices and 15 edges. We want to ﬁnd a minimum embedding
dimension for it.

D.2 Graph embedding problem

Graph embedding can be used to ﬁnd a proper representation for an order parameter (Takahashi
& Sandvik, 2020) in condensed matter physics. People want to ﬁnd a minimum Euclidean space
dimension k that a Petersen graph can embed into, that the distances between pairs of connected
vertices are l1, and the distance between pairs of disconnected vertices are l2, where l2 > l1. The
Petersen graph is shown in Fig. 5. Let us denote the set of connected and disconnected vertex pairs
as L1 and L2, respectively. This problem can be variationally solved with the following loss.

L = Var(dist(L1)) + Var(dist(L2))

+ exp(relu(dist(L1) − dist(L2) + 0.1))) − 1

(7)

The ﬁrst line is a summation of distance variances in two sets of vertex pairs, where Var(X) is the
variance of samples in X. The second line is used to guarantee l2 > l1, where X means taking the
average of samples in X. Its reversible implementation could be found in our benchmark repository.

√

We repeat the training for dimension k from 1 to 10. In each training, we ﬁx two of the vertices
and optimize the positions of the rest. Otherwise, the program will ﬁnd the trivial solution with
overlapped vertices. For k < 5, the loss is always much higher than 0, while for k ≥ 5, we can
get a loss close to machine precision with high probability. From the k = 5 solution, it is easy to
see l2/l1 =
2. An Adam optimizer with a learning rate 0.01 (Kingma & Ba) requires ∼2000 steps
training. The trust region Newton’s method converges much faster, which requires ∼20 computations
of Hessians to reach convergence. Although training time is comparable, the converged precision of
the later is much better.
Since one can combine ForwardDiﬀ and NiLang to obtain Hessians, it is interesting to see how much
performance we can get in diﬀerentiating the graph embedding program.
In Table 3, we show the the performance of diﬀerent implementations by varying the dimension k.
The number of parameters is 10k. As the baseline, (a) shows the time for computing the function
call. We have reversible and irreversible implementations, where the reversible program is slower
than the irreversible native Julia program by a factor of ∼2 due to the uncomputing overhead. The

19

61728394105k

Julia-O
NiLang-O
NiLang-U
NiLang-G
ReverseDiﬀ-G
ForwardDiﬀ-G
Zygote-G
(NiLang+F)-H
ForwardDiﬀ-H
(ReverseDiﬀ+F)-H

2
4.477e-06
7.173e-06
7.453e-06
1.509e-05
2.823e-05
1.518e-05
5.315e-04
4.528e-04
2.378e-04
1.966e-03

4
4.729e-06
7.783e-06
7.839e-06
1.690e-05
4.582e-05
4.053e-05
5.570e-04
1.025e-03
2.380e-03
6.058e-03

6
4.959e-06
8.558e-06
8.464e-06
1.872e-05
6.045e-05
6.732e-05
5.811e-04
1.740e-03
6.903e-03
1.225e-02

8
5.196e-06
9.212e-06
9.298e-06
2.076e-05
7.651e-05
1.184e-04
6.096e-04
2.577e-03
1.967e-02
2.035e-02

10
5.567e-06
1.002e-05
1.054e-05
2.266e-05
9.666e-05
1.701e-04
6.396e-04
3.558e-03
3.978e-02
3.140e-02

Table 3: Absolute times in seconds for computing the objectives (O), uncall objective (U), gradients
(G) and Hessians (H) of the graph embedding program. k is the embedding dimension, the number
of parameters is 10k.

reversible program shows the advantage of obtaining gradients when the dimension k ≥ 3. The
larger the number of inputs, the more advantage it shows due to the overhead proportional to input
size in forward mode AD. The same reason applies to computing Hessians, where the combo of
NiLang and ForwardDiﬀ gives the best performance for k ≥ 3.

E Porting NiLang to Zygote

Zygote is a popular machine learning package in Julia. We can port NiLang’s automatically gener-
ated backward rules to Zygote to accelerate some performance-critical functions. The following ex-
ample shows how to speed up the backward propagation of norm by ∼50 times.

20

(cid:7)
julia> using Zygote, NiLang, NiLang.AD, BenchmarkTools, LinearAlgebra

Listing 24: Porting NiLang to Zygote.

(cid:4)

julia> x = randn(1000);

julia> @benchmark norm'(x)
BenchmarkTools.Trial:

memory estimate: 339.02 KiB
allocs estimate: 8083
--------------
minimum time:
median time:
mean time:
maximum time:
--------------
samples:
evals/sample:

10000
1

228.967 µs (0.00% GC)
237.579 µs (0.00% GC)
277.602 µs (12.06% GC)
5.552 ms (94.00% GC)

julia> @i function r_norm(out::T, out2::T, x::AbstractArray{T}) where T

for i=1:length(x)

@inbounds out2 += x[i]^2

end
out += sqrt(out2)

end

julia> Zygote.@adjoint function norm(x::AbstractArray{T}) where T

# compute the forward with regular norm (might be faster)
out = norm(x)
# compute the backward with NiLang's norm, element type is GVar
out, δy -> (grad((~r_norm)(GVar(out, δy), GVar(out^2), GVar(x))[3]),)

end

julia> @benchmark norm'(x)
BenchmarkTools.Trial:

memory estimate: 23.69 KiB
allocs estimate: 2
--------------
minimum time:
median time:
mean time:
maximum time:
--------------
samples:
evals/sample:

10000
7

4.015 µs (0.00% GC)
5.171 µs (0.00% GC)
6.872 µs (13.00% GC)
380.953 µs (93.90% GC)

(cid:6)

(cid:5)

We ﬁrst import the norm function from Julia standard library LinearAlgebra. Zygote’s builtin AD
engine will generate a slow code and memory allocation of 339KB. Then we write a reversible norm
function r_norm with NiLang and port the backward function to Zygote by specifying the backward
rule (the function marked with macro Zygote.@adjoint). Except for the speed up in computing
time, the memory allocation also decreases to 23KB, which is equal to the sum of the original x and
the array used in backpropagation.

The later one has a doubled size because GVar has an extra gradient ﬁeld.

(1000 × 8 + 1000 × 8 × 2)/1024 ≈ 23

F A benchmark of round-oﬀ error in leapfrog

Running reversible programming with the ﬂoating pointing number system can introduce round-oﬀ
errors and make the program not reversible. The quantify the eﬀects, we use the leapfrog integrator
to compute the orbitals of planets in our solar system as a benchmark. The leapfrog interations can
be represented as

(cid:126)ai = G

m j((cid:126)x j − (cid:126)xi)
(cid:107)(cid:126)xi − (cid:126)x j(cid:107)3
(cid:126)vi+1/2 = (cid:126)vi−1/2 + (cid:126)ai∆t
(cid:126)xi+1 = (cid:126)xi + (cid:126)vi+1/2∆t

21

(8)

(9)
(10)

Figure 6: Round-oﬀ errors in the ﬁnal axes of planets as a function of the number of time steps.
“(regular)” means an irreversible program, “(reversible)” means a reversible program (Listing. 25),
and “(ccumulative)” means a reversible program with the acceleration computed with ccumulative
errors (Listing. 26).

where G is the gravitational constant and m j is the mass of jth planet, (cid:126)x, (cid:126)v and (cid:126)a are location,
velocity and acceleration respectively. The ﬁrst value of velocity is v1/2 = a0∆t/2. Since the
dynamics of our solar system are symplectic and the leapfrog integrator is time-reversible, the
reversible program does not have overheads and the evolution time can go arbitrarily long with
constant memory. We compare the mean error in the ﬁnal axes of the planets and show the results
in Fig. 6. Errors are computed by comparing with the results computed with high precision
ﬂoating-point numbers. One of the key steps that introduce round-oﬀ error is the computation of
acceleration. If it is implemented as in Listing. 25, the round-oﬀ error does not bring additional
eﬀect in the reversible context, hence we see overlapping lines “(regular)” and “(reversible)” in the
ﬁgure. This is because, when returning a dirty (not exactly zero cleared due to the ﬂoating-point
round-oﬀ error) ancilla to the ancilla pool,
the small remaining value will be zero-cleared
automatically in NiLang. The acceleration function can also be implemented as in Listing. 26,
where the same variable rb is repeatedly used for compute and uncompute,
the error will
In both Float64 (double precision ﬂoating point) and Float32
accumulate on this variable.
(single precision ﬂoating point) benchmarks, the results show a much lower precision. Hence,
simulating reversible programming with ﬂoating-point numbers does not necessarily make the
results less reliable if one can avoid cumulative errors in the implementation.

Listing 25: Compute the acceleration. Com-
pute and uncompute on ancilla rc

(cid:7)
@i function :(+=)(acceleration)(y!::V3{T},

(cid:4)

Listing 26: Compute the acceleration. Com-
pute and uncompute on the input variable rb.

(cid:7)
@i function :(+=)(acceleration)(y!::V3{T},

(cid:4)

ra::V3{T}, rb::V3{T}, mb::Real, G)
where T

@routine @invcheckoff begin

@zeros T d anc1 anc2 anc3 anc4
rc ← zero(V3{T})
d += sqdistance(ra, rb)
anc1 += sqrt(d)
anc2 += anc1 ^ 3
anc3 += G * mb
anc4 += anc3 / anc2
rc += rb - ra

end
y! += anc4 * rc
~@routine

end(cid:6)

ra::V3{T}, rb::V3{T}, mb::Real, G)
where T

@routine @invcheckoff begin

@zeros T d anc1 anc2 anc3 anc4
d += sqdistance(ra, rb)
anc1 += sqrt(d)
anc2 += anc1 ^ 3
anc3 += G * mb
anc4 += anc3 / anc2
rb -= ra

end
y! += anc4 * rb
~@routine
# rb is not recovered rigorously!

(cid:5)

end(cid:6)

(cid:5)

22

102104106leapfrog steps1012107102103108round off errorFloat32 (regular)Float32 (reversible)Float32 (cummulative)Float64 (regular)Float64 (reversible)Float64 (cummulative)G Language Description

G.1 Grammar

A minimum deﬁnition of NiLang’s grammar is

s
s
c
x, y, z
i, n
e
σP[x (cid:55)→ v]
σP[x (cid:55)→ nothing]
σP, e ⇓e v
σP, s ⇓p σ(cid:48)
P
p σ(cid:48)
σP, s ⇓−1
P

a statement
multiple statements
a constant
symbols
integers
julia expression
environemnt with x’s value equal to v
environemnt with x undeﬁned
a Julia expression e under environment σP is interpreted as value v
the evaluation of a statement s under environment σP generates environment σ(cid:48)
P
the reverse evaluation of a statement s under environment σP generates environment σ(cid:48)
P

Statements

s

:: =

Data views d

:: =

| x ← e | x → e

| e ( d∗ )

∼s
| @routine s ; s∗ ; ∼@routine
| if ( e , e ) s∗ else s∗ end
| while ( e , e ) s∗ end
| for x = e : e : e s∗ end
| begin s∗ end
d . x | d [ e ]
| c | x

| d (cid:46) e

Reversible functions p

:: = @i function x ( x∗ ) s* end

(cid:46) is the pipe operator in Julia. Here, e is a reversible function and d (cid:46) e represents a bijection of d.
Function arguments are data views, where a data view is a modiﬁable memory. It can be a variable,
a ﬁeld of a data view, an array element of a data view, or a bijection of a data view.

G.2 Operational Semantics

The following operational semantics for the forward and backward evaluation shows how a
statement is evaluated and reversed.

ANCILLA

σP, e ⇓e v
σP[x (cid:55)→ nothing], x → e ⇓p σP[x (cid:55)→ v]

σP, e ⇓e v
σP[x (cid:55)→ v], x ← e ⇓p σP[x (cid:55)→ nothing]

ANCILLA−1

σP, x ← e ⇓p σ(cid:48)
P
p σ(cid:48)
σP, x → e⇓−1
P

σP, x → e ⇓p σ(cid:48)
P
p σ(cid:48)
σP, x ← e⇓−1
P

BLOCK

BLOCK−1

σP, s1 ⇓p σ(cid:48)

P σ(cid:48)

P, begin s2 · · · sn end ⇓p σ(cid:48)(cid:48)

P

σP, begin s1 · · · sn end ⇓p σ(cid:48)(cid:48)
P

σP, begin end ⇓p σP

σP, sn⇓−1

P σ(cid:48)

P, begin s1 · · · sn−1 end ⇓−1

p σ(cid:48)
σP, begin s1 · · · sn end ⇓−1

p σ(cid:48)(cid:48)
P

p σ(cid:48)(cid:48)
P

σP, begin end ⇓−1

p σP

FOR

σP, e1 ⇓e n1 σP, e2 ⇓e n2 σP, e3 ⇓e n3

(n1 <= n3) == (n2 > 0)

σP[x (cid:55)→ n1], s ⇓p σ(cid:48)

P σ(cid:48)

P, for x = e1 + e2:e2:e3 s end ⇓p σ(cid:48)(cid:48)

P

σP, for x = e1:e2:e3 s end ⇓p σ(cid:48)(cid:48)
P

23

FOR-EXIT

σP, e1 ⇓e n1 σP, e2 ⇓e n2 σP, e3 ⇓e n3

(n1 <= n3) != (n2 > 0)

σP, for x = e1:e2:e3 s end ⇓p σP

FOR−1

σP, for x = e3: −e2:e1 ∼begin s end end ⇓p σ(cid:48)
P
σP, for x = e1:e2:e3 s end ⇓−1

p σ(cid:48)
P

IF-T

IF-F

σP, e1 ⇓e true σP, s1 ⇓p σ(cid:48)

P σ(cid:48)

P, e2 ⇓e true

σP, if (e1, e2) s1 else s2 end ⇓p σ(cid:48)
P

σP, e1 ⇓e f alse σP, s2 ⇓p σ(cid:48)

P σ(cid:48)

P, e2 ⇓e f alse

σP, if (e1, e2) s1 else s2 end ⇓p σ(cid:48)
P

IF−1

σP, if (e2, e1) ∼begin s1 end else ∼begin s2 end end ⇓p σ(cid:48)
P
σP, if (e1, e2) s1 else s2 end ⇓−1

p σ(cid:48)
P

WHILE

σP, e1 ⇓e true σP, e2 ⇓e f alse σP, s ⇓p σ(cid:48)

P σ(cid:48)
σP, while (e1, e2) s end ⇓p σ(cid:48)
P

P, (e1, e2, s) ⇓loop σ(cid:48)(cid:48)

P

WHILE-REC

σP, e2 ⇓e true σP, e1 ⇓e true σP, s ⇓p σ(cid:48)
σP, (e1, e2, s) ⇓loop σ(cid:48)(cid:48)
P

P σ(cid:48)

P, (e1, e2, s) ⇓loop σ(cid:48)(cid:48)

P

WHILE-EXIT

σP, e2 ⇓e true σP, e1 ⇓e f alse
σP, (e1, e2, s) ⇓loop σP

WHILE−1

σP, while (e2, e1) ∼begin s end end ⇓p σ(cid:48)
P
σP, while (e1, e2) s end ⇓−1

p σ(cid:48)
P

UNCOMPUTE

σP, s⇓−1
p σ(cid:48)
P
σP, ∼s ⇓p σ(cid:48)
P

COMPUTE-COPY-UNCOMPUTE

σP, s1 ⇓p σ(cid:48)

P σ(cid:48)

P, begin s end ⇓p σ(cid:48)(cid:48)
σP, @routine s1; s; ∼@routine ⇓p σ(cid:48)(cid:48)(cid:48)
P

P σ(cid:48)(cid:48)

P, s1⇓−1

p σ(cid:48)(cid:48)(cid:48)
P

COMPUTE-COPY-UNCOMPUTE−1

σP, @routine s1; ∼begin s end ∼@routine ⇓p σ(cid:48)
P
σP, @routine s1; s; ∼@routine⇓−1

p σ(cid:48)
P

CALL

∅[x1 (cid:55)→ v1 · · · xn (cid:55)→ vn], (x1 · · · xn) = x f (x1 · · · xn) ⇓e σP0 [x1 (cid:55)→ v(cid:48)
σPi−1 , v(cid:48)

σP, di ⇓get vi
1 · · · xn (cid:55)→ v(cid:48)
n]
i , di ⇓set σPi

σP, x f (d1 · · · dn) ⇓p σPn

CALL−1

σP, (∼x f )(d1 · · · dn) ⇓p σ(cid:48)
P
σP, x f (d1 · · · dn)⇓−1

p σ(cid:48)
P

GET-VIEW

σP, d ⇓e v
σP, d ⇓get v

SET-VIEW-SYM

σP, v, x ⇓set σP[x (cid:55)→ v]

24

SET-VIEW-ARRAY

z f resh σP[z (cid:55)→ v], setindex!(d, z, x) ⇓e v(cid:48) σP, v(cid:48), d ⇓set σ(cid:48)
P
σP, v, d[x] ⇓set σ(cid:48)
P

SET-VIEW-FIELD

z f resh σP[z (cid:55)→ v], chﬁeld(d, x, z) ⇓e v(cid:48) σP, v(cid:48), d ⇓set σ(cid:48)
P
σP, v, d.x ⇓set σ(cid:48)
P

SET-VIEW-BIJECTOR

z f resh σP[z (cid:55)→ v], z (cid:46) (∼e) ⇓e v(cid:48) σP, v(cid:48), d ⇓set σ(cid:48)
P
σP, v, d (cid:46) e ⇓set σ(cid:48)
P

a

as

are

regular

evaluated

Here, chﬁeld(x, y, z) is a Julia function that returns an object similar to x, but with ﬁeld y modiﬁed
to value z, setindex!(x, z, y) is a Julia function that sets the yth element of an array x to the value of
z. We do not deﬁne primitive instructions like d1 (cid:12)= x f (d2 · · · dn), (cid:12) ∈ {+, −, ∗, /, (cid:89)} because these
expression
instructions
prime((cid:12), x f )(d1, d2 · · · dn) = (d1 (cid:12) x f (d2 · · · dn), d2 · · · dn) using the above CALL rule, where
prime((cid:12), x f ) is a predeﬁned Julia function. To reverse the call,
the reverse julia function
∼prime((cid:12), x f ) should also be properly deﬁned. On the other side, any non-primitive NiLang
function deﬁntion will generate two Julia functions x f and ∼x f so that it can be used in a recursive
deﬁnition or other reversible functions. When calling a function, NiLang does not allow the input
data views mappings to the same memory, i.e. shared read and write is not allowed. If the same
variable is used for shared writing, the result might incorrect. If the same variable is used for both
reading and writing, the program will become irreversible. For example, one can not use x -= x to
erase a variable, while coding x.y -= x.z is safe. Even if a variables is used for shared reading, it
can be dangerous in automatic diﬀerentiating. The share read of a variable induces a shared write
of its gradient in the adjoint program. For example, y += x * x will not give the correct gradient,
but its equivalent forms z ← 0; z += x; y += x * z; z -= x; z → 0 and y += x ^2 will.

julia

G.3 Compilation

The compilation of a reversible function to native Julia functions is consisted of three stages:
preprocessing, reversing and translation as shown in Fig. 7.

Figure 7: Compiling the body of the complex valued log function deﬁned in Listing. 5.

In the preprocessing stage, the compiler pre-processes human inputs to the reversible NiLang IR.
The preprocessor removes eye candies and expands shortcuts to symmetrize the code. In the left
most code box in Fig. 7, one uses @routine <stmt> statement to record a statement, and
∼@routine to insert the corresponding inverse statement for uncomputing. This macro is
expanded in the preprocessing stage. In the reversing stage, based on this symmetrized reversible
IR, the compiler generates reversed statements. In the translation stage, the compiler translates this
reversible IR as well as its inverse to native Julia code. It adds @assignback before each function
call, inserts codes for reversibility check, and handle control ﬂows. The @assignback macro
assigns the outputs of a function to its input data views. As a ﬁnal step, the compiler attaches a
return statement that returns all updated input arguments. Now, the function is ready to execute on
the host language.

25

PreprocessTranslateTranslateReverseJuliaCompileLLVMSource CodeNiLang IRJulia LanguageJuliaCompile