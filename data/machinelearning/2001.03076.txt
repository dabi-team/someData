Sampling Prediction-Matching Examples in Neural Networks:
A Probabilistic Programming Approach

Serena Booth∗, Ankit Shah∗, Yilun Zhou∗, Julie Shah
Computer Science and Artiﬁcial Intelligence Laboratory
Massachusetts Institute of Technology
{serenabooth, ajshah, yilun, julie a shah}@csail.mit.edu

0
2
0
2

n
a
J

9

]

G
L
.
s
c
[

1
v
6
7
0
3
0
.
1
0
0
2
:
v
i
X
r
a

Abstract

Though neural network models demonstrate impressive per-
formance, we do not understand exactly how these black-box
models make individual predictions. This drawback has led
to substantial research devoted to understand these models
in areas such as robustness, interpretability, and generaliza-
tion ability. In this paper, we consider the problem of explor-
ing the prediction level sets of a classiﬁer using probabilistic
programming. We deﬁne a prediction level set to be the set
of examples for which the predictor has the same speciﬁed
prediction conﬁdence with respect to some arbitrary data dis-
tribution. Notably, our sampling-based method does not re-
quire the classiﬁer to be differentiable, making it compatible
with arbitrary classiﬁers. As a speciﬁc instantiation, if we take
the classiﬁer to be a neural network and the data distribution
to be that of the training data, we can obtain examples that
will result in speciﬁed predictions by the neural network. We
demonstrate this technique with experiments on a synthetic
dataset and MNIST. Such level sets in classiﬁcation may fa-
cilitate human understanding of classiﬁcation behaviors.1

Introduction

Are we comfortable with neural networks—with their high-
dimensional, uninterpretable intermediary representations—
making high stakes decisions such as who is released
from jail or who receives a mortgage? As neural networks
gain traction in commercial products, enabling people to
understand these models becomes essential. To this end,
many research communities have analyzed neural network
properties. Research on fairness of neural networks aims
to ensure models do not discriminate against protected
groups (Corbett-Davies et al., 2017). Research on robustness
analyzes whether a model will perform drastically differ-
ently if the input is only slightly different from test data (Car-
lini et al., 2019). Research on interpretability tries to un-
derstand how decisions are made (Doshi-Velez and Kim,
2017). Finally, research on transfer learning explores if a
neural network trained on one data distribution can work
well or be adapted to work well on another distribution at
test time (Torrey and Shavlik, 2010).

∗Equal contribution
1Code: https://github.com/serenabooth/level-set-inference

To complement these research approaches, we propose a
technique for analyzing sets of examples of a given pre-
diction conﬁdence to evaluate a network’s global perfor-
mance. Our technique empirically evaluates prediction level
sets, or sets of a given conﬁdence, through applying proba-
bilistic programming (Cusumano-Towner et al., 2019). We
ﬁrst create a generative model and train a neural network to
classify generated images. We then sample examples of a
given conﬁdence by applying Metropolis Hastings to infer
a latent representation of an image which meets the speci-
ﬁed classiﬁcation conﬁdence. We demonstrate our technique
on two domains: a procedurally-generated synthetic domain
and MNIST digits produced with a VAE or a GAN. Our re-
sults show that our inference successfully samples a set of
examples for the given classiﬁcation conﬁdence in each do-
main. We further demonstrate how our technique can sample
“ambiguous” images from network decision boundaries.

Related Work

Adversarial Attacks on Neural Networks

Many approaches demonstrate that neural network predic-
tions are locally unstable; i.e. an input can be slightly per-
turbed to make the network produce vastly different predic-
tions. Szegedy et al. (2013) ﬁrst demonstrated that images
of seemingly random noise can receive conﬁdent predictions
and a correctly classiﬁed image can be slightly perturbed to
be conﬁdently predicted as an incorrect class. Subsequent
work has focused on more effective attack methods (e.g.
Goodfellow, Shlens, and Szegedy, 2014; Nguyen, Yosinski,
and Clune, 2015; Carlini and Wagner, 2017; Athalye et al.,
2017; Li et al., 2018; Athalye, Carlini, and Wagner, 2018)
and better defenses against them (e.g. Madry et al., 2017;
Cohen, Rosenfeld, and Kolter, 2019; Lecuyer et al., 2019).
Remarkably, all these adversarial attacks produce examples
that are out of distribution, and it is hard to characterize
the distributional properties of such adversarial examples.
Our work instead aims to produce “adversarial” examples in
some known distribution implicitly speciﬁed through a gen-
erative process. Our method is able to generate examples
with arbitrary predictions (e.g. equally conﬁdent predictions
across classes), while most adversarial attacks generate ex-

 
 
 
 
 
 
amples with conﬁdent predictions but incorrect labels. If the
data generation models the distribution of the whole dataset
except for a class l, and we generate examples according
to that distribution but also enforce conﬁdent prediction in
class l, we can uncover in-distribution adversarial examples.

Density Estimation

Estimating the data distribution is an important and chal-
lenging task. Traditionally, parametric (e.g. Gaussian mix-
ture model) and non-parametric (e.g. kernel density esti-
mation (Rosenblatt, 1956)) methods work well on low di-
mensional data, in which different features represent distinc-
tively different meanings (such as age, salary, height, etc.).
However, recent deep learning applications work on very
high-dimensional datasets in which features are also highly
correlated with each other. For example, an image can easily
have tens of thousands of features (i.e. pixels), and nearby
pixels are correlated. It is thus much more difﬁcult, if even
possible, for these traditional models to capture the data dis-
tribution and generate realistic-looking data.
A commonly used model for estimating such high dimen-
sional data distributions is an autoencoder, in which two
neural networks respectively encode (compress) and decode
(reconstruct) the image into and from a low-dimensional la-
tent vector. Variational autoencoders (VAEs) (Kingma and
Welling, 2013) additionally regularize the latent vectors to
conform to a unit Gaussian distribution, which reduces the
task of sampling from the data distribution to reconstruc-
tion from unit Gaussian samples. However, due to the train-
ing objective of minimizing l2 loss, the images reconstructed
for vision domain problems through a VAE are typically less
sharp than the original images.
Compared to autoencoders, generative adversarial networks
(GANs) (Goodfellow et al., 2014) employ a generator-
discriminator architecture designed to speciﬁcally regular-
ize the generated image to stay within the data distribution.
Consequently, mode collapse, in which the generated data
lacks the variety found in the original data, is a common
problem, and several methods (e.g. Srivastava et al., 2017;
Lin et al., 2018) have been proposed to solve the problem.
In our experiments on a synthetic dataset with known low-
dimensional features, we use the groundtruth distribution.
On the MNIST image dataset, we use both a variational au-
toencoder and a GAN to generate images from the respective
learned data distributions.

Conﬁdence in Neural Networks

Our technique explores conﬁdence level sets in neural net-
works. However, prior work has shown many modern neu-
ral network architectures result in overconﬁdent networks,
with many incorrect predictions having undue high conﬁ-
dence (Guo et al., 2017). To address the problem of over-
conﬁdence in neural networks, Lee et al. (2017) used a
GAN to generate out-of-distribution data and regularize the
classiﬁer to have uniform conﬁdence (i.e. complete ambiva-
lence) on these examples. Several Bayesian formulations
have also been formulated with different approximate in-

ference methods (e.g. Blundell et al., 2015; Graves, 2011;
Balan et al., 2015). Gal and Ghahramani (2016) drew a con-
nection between the dropout layer (Srivastava et al., 2014)
and Bayesian formulation and used this to calculate uncer-
tainty. Lakshminarayanan, Pritzel, and Blundell (2017) pro-
posed an ensemble approach to capture model uncertainty.
Our work is complementary to these models in that we do
not attempt to alter the conﬁdence of a trained neural net-
work model, but instead expose examples with particular
conﬁdence values. As a result, if the model is over-conﬁdent,
our sampling approach may return few or even no valid ex-
amples, within some speciﬁed distribution. Moreover, since
we do not require access to anything beyond a prediction
conﬁdence (e.g. gradient information), our model can be
used to study all of these conﬁdence-calibrated models.

Probabilistic Programming

Probabilistic programming consists of the emerging set of
tools in which programming languages enable the deﬁnition
of models and automate parts of inference. With popular-
ity of deep learning, several frameworks (e.g. Bingham et
al., 2018; Salvatier, Wiecki, and Fonnesbeck, 2016; Tran et
al., 2016) also have deep learning integration, in which a
neural network model is used to deﬁne and/or learn a dis-
tribution. In our work, we use the state-of-the-art language
Gen (Cusumano-Towner et al., 2019). One prior applica-
tion of probabilistic programming has parallels to our ap-
proach: Fremont et al. (2018) designed a constrained prob-
abilistic programming language, SCENIC, which generates
distributions over scenes produced from a generative model.
While our work is inspired by SCENIC, their system was de-
signed to explore clear classiﬁcation failures and assist in
dataset augmentation, whereas we apply probabilistic pro-
gramming to explore conﬁdence level sets—including clas-
siﬁcation successes.

Methodology

Given a classiﬁer f : X → ∆L that maps a data point to the
probability simplex of L classes, the overall goal is to ﬁnd
an input x ∈ X such that f (x) = p for some particular pre-
diction conﬁdence p ∈ ∆L. A common approach to achieve
this is to start with some initial guess x0, and iteratively op-
timize the function L(x) = d(f (x), p) using gradient-based
method, for some distance metric d(·, ·). One such example
distance metric is the total variation distance. This method
is reminiscent of popular adversarial attacks. However, there
is no guarantee that such the resulting x∗ will still remain in
the original data distribution.
Thus, we introduce a data distribution p(x) and consider the
inference problem of sampling from the posterior

p(x|f (x) = p) ∝ p(x)p(f (x) = p|x).

Note that the likelihood is actually a delta function; this
makes sampling infeasible because in general the set {x :
f (x) = p} has measure 0. To solve this problem, we relax
the formulation by introducing ˜px, which is sampled from a

Dirichlet distribution with parameters proportional to f (x):
˜px ∼ Dir(αf (x)),
where α is a hyper-parameter to be set to determine the tol-
erance of an inaccurate ˜px: the lower the value of this hy-
perparameter, the more tolerant the sampling procedure.
In addition, rather than sampling directly from the data dis-
tribution, we sample in the latent factor space Z, which can
be mapped to X via a deterministic reconstruction function
.
g : Z → X. We use the notation ˜pz
= ˜pg(x). We assume the
latent space distribution p(z) is much easier to sample from;
for example, if our generative model is a GAN or VAE, this
distribution is a unit Gaussian.
To summarize, given

z ∼ p(z),
x = g(z),
˜pz ∼ Dir(αf (x)),

(1)
(2)
(3)
p(z | ˜pz = p) ∝ p(z)p( ˜pz = p | z),
(4)
where α is the hyper-parameter and p is the target predic-
tion, we wish to sample from posterior p(z | ˜pz = p), and
reconstruct x from z using g(·). We see that the poste-
rior distribution is large only if the prior is large as well,
limiting our sampled z (and therefore the reconstructed x)
to the generative distribution p(x). In addition, we have
limα→∞ ||f (g(z)) − p|| = 0 with probability 1, meaning
that prediction of the samples will get closer to the target
prediction as we increase α.
To sample from the posterior, we use the Metropolis-
Hastings algorithm. We start from z(0) ∼ p(z) and then use
a Gaussian proposal function centered on the current z with
symmetric covariance matrix (i.e. kI for k > 0). To exploit
parallelism of computer architectures, we start with N par-
ticles, and we sample for T time steps. In this process, we
conservatively choose a relatively large T so that we sample
beyond the requisite “burn-in” time. For each particle, the
last sample z(T ) of the trajectory is selected. Finally, we se-
lect n ≤ N such unweighted samples using resampling with
replacement.

Experiments

We conduct experiments on two datasets: a synthetic im-
age dataset in which images look either like a “house” or
a “rocket” (see ﬁgure 2), and the MNIST dataset. For both
datasets, we either have access to the true data distribution or
learn a data distribution p(z) over the latent space. We then
sample n latent vectors {zi}n
i=1 from the posterior distribu-
tion (Eqn 4) with certain known or learned data distribution
(i.e. prior) and speciﬁed target prediction p, and reconstruct
{xi}n
To evaluate the quality of our sampled images, we compute
the predicted probabilities: { ˜pi = f (xi)}n
i=1. We then cal-
culate average deviation from target over class according to

i=1 using the reconstruction function g(·).

.
=

∆

1
n

1
|L|

n
(cid:88)

(cid:88)

|pl − ˜pi,l|,

(5)

i=1
where ˜pi,l is the predicted probability of xi for class l.

l∈L

Rockets or Houses: A Synthetic Geometric World

We construct a synthetic world where simple rules govern
how an image of a triangle on top of a rectangle is drawn.
Some of the images look like “houses” when a short triangle
is put on top of a wide rectangle. Others look like “rockets”
when a tall triangle is put on top of a slim rectangle. See ﬁg-
ure 2 for prototypical images. Each image is rendered from
three parameters: rectangle width w, rectangle height h, and
triangle height t, which are generated from a mixture model
conditioned on a latent variable c representing whether the
image should be a house or a rocket. Speciﬁcally, to draw an
image we follow the generative process:
c ∼ Ber(0.5),

w|(c = 0) ∼ No+(10, 52),
h|(c = 0) ∼ No+(30, 102),
t|(c = 0) ∼ No+(8, 22),
w|(c = 1) ∼ No+(30, 102),
h|(c = 1) ∼ No+(30, 102),
t|(c = 1) ∼ No+(10, 22),

.
= [w, h, t]

z

where No+(µ, σ2) is the normal distribution truncated to the
positive support. After rendering these images, we subse-
quently applied a Gaussian blur for for easier classiﬁcation.
Using the synthetic images and the known groundtruth la-
bels (c), we train a CNN-based classiﬁer on 16,000 such im-
ages. Our classiﬁer achieves 97.3% accuracy (94.7% preci-
sion, 97.4% recall). We then use our method to sample ex-
amples with target prediction p = [0.5, 0.5], with α = 10.
Note that in this binary case, the Dirichlet distribution re-
duces to the Beta distribution.
Figure 1 plots some of the images sampled from the poste-
rior distribution with β constrained to produce images which
conﬁdently classify as houses (β = 0.999), images which
conﬁdently classify as rockets (β = 0.001), and images
which classify as ambiguous (β = 0.5). Empirically, these
images indeed appear to be houses, rockets, or ambiguous
images. For generated houses, the mean classiﬁer conﬁdence
was 94.9% (minimum 84.1%); for generated rockets, the
mean classiﬁer conﬁdence was 94.9% (minimum 86.4%).
The average generated image resulted in a classiﬁcation con-
ﬁdence of 51.0%. This result is also conﬁrmed in Figure 3,
which visualizes the parameters of these ambiguous images
rendered among the mixtures of two Gaussians. We com-
pare our inference approach for selecting images from the
three level sets to a set of randomly generated latents. We
ﬁnd many more ambiguous images through our inference
approach than when directly sampling latents from the re-
spective distributions.
Our method is not limited to test data from the distribution.
We also demonstrate its potential for understanding behav-
iors of a trained classiﬁer on different data distribution. To
this end, we ﬁx the classiﬁer, trained on these “house” or
“rocket” images, but we introduce a circle to the geome-
try rendering. A circle is randomly overlaid on an image as

(a) Predicted houses, β = 0.999

(b) Predicted rockets, β = 0.001

(c) Ambiguous images, β = 0.5

Figure 1: Given our trained classiﬁer and generative world, we constrain β to infer images of a given conﬁdence level. Quali-
tatively, the generated ambiguous images appear to be primarily either extremes drawn from the distribution (e.g., small values
for rectangle height h) or images with aspect ratios in between those of prototypical houses and rockets as shown in Figure 2.

shown in Figure 2. We introduce the parameters circle radius
r and circle center (x, y) into our generative process:

r ∼ No+(20, 102),
x ∼ U(20, 40)
y ∼ U(20, 40),

z

.
= [w, h, t, r, x, y]
where w, h, and t are deﬁned as before, and U(a, b) is a
uniform distribution.
Using this new generative function, we again infer images
from our three classes: those for which the classiﬁer con-
ﬁdently predicts houses, those for which the classiﬁer con-
ﬁdently predicts rockets, and those for which the classiﬁer
has low prediction conﬁdence. We then evaluate how the
presence of this out-of-distribution geometry affects classi-
ﬁcation predictions, and we ﬁnd it has minor effects. For
generated rocket images, average conﬁdence in the rocket
prediction falls 1.6% when circles are rendered; accuracy is
unchanged. For generated house images, average conﬁdence
in the house prediction increases 1.1%; accuracy is likewise
unchanged. For generated ambiguous images, average con-
ﬁdence in predictions increases 4.9% when circles are ren-
dered. Curiously, when we infer ambiguous images with the

VAE

GAN

ambiguous
1vs7
8vs9

1.83% 5.93%
6.22% 7.92%
7.66% 5.31%

Table 1: Average deviation for different generative model
and different prediction targets.

circles rendered, the accuracy of the classiﬁer is 15% higher
than when the circles are not rendered.

MNIST

On the MNIST dataset, we learn to approximate the data
distribution using both a variational autoencoder (VAE)
and a generative adversarial network (GAN) as the source
data distribution is not available. Both models are trained
with a 5-dimensional latent space. Hence, we have two in-
duced data distribution pV AE(x) and pGAN (x), derived
from pV AE(z) = pGAN (z) = MVN(0, I) ∈ R5. For each
VAE and GAN prior, we studied three target predictions:
1. ambiguous, with pl = 0.1 ∀l ∈ {0, 1, ..., 9};
2. 1vs7, with p1 = p7 = 0.46 and pl = 0.01 ∀l (cid:54)= 1, 7;
3. 8vs9, with p8 = p9 = 0.46 and pl = 0.01 ∀l (cid:54)= 8, 9.
Table 1 shows the average prediction deviation ∆ as deﬁned
in Eqn 5. We can see that in general these models are indeed
sampling data points around the speciﬁed target prediction.
While they seem to have comparable performance, we note
in the qualitative evaluations that sampling with a GAN prior
will reduce much less valid samples than with a VAE prior.

Figure 2: Left: the prototypical house image; middle: the
prototypical rocket image; right: introduction of out of dis-
tribution circle geometry. Images have a Gaussian blur ap-
plied to them for ease of classiﬁcation.

VAE Visualization For qualitative evaluation, we visual-
ize both the generated images and the latent space (reduced
to 2 dimensions using t-SNE (Maaten and Hinton, 2008)).
The images sampled from the VAE prior for each class are

4b and 4c, the gray, blue and red dots represent latent vec-
tors sampled from the prior distribution, and are color-coded
according to classiﬁer predictions. The dark green dots rep-
resent latent vectors sampled from the posterior.
We can see that the sampled latent vectors for the ambigu-
ous case indeed lie at the intersections of several colored
regions, though probably not between all regions, because
it is hard for an image to look like all the 10 digits at the
same time. In addition, they are also quite centrally located,
indicating a high likelihood of drawing this digit according
to VAE. For the latent vectors of 1vs7 and 8vs9, they are
generally “in-distribution”, although they do tend to occupy
a smaller probability mass. This smaller mass is expected, as
most of the highly likely images (including those seen in the
training set) are indeed unambiguously a single digit. How-
ever, the sampled latent vectors are also not completely out
of distribution, which is commonly believed to be the case
for adversarial examples (Li et al., 2019).

GAN Visualization For posterior sampling with a GAN
as the learned generative model, the ﬁnal resampling step
(which is done with replacement) produces less than 10 dis-
tinct latent vector samples. Thus, we show the reconstructed
images from all of them in Figure 5.
The small number of distinct samples in the GAN model re-
sults from the classiﬁer being overly conﬁdent on most sam-
ples, an issue discussed in the Related Work section. This
overconﬁdence makes it increasingly challenging to sample
an image which produces ambiguous predictions from the
generative model.
We see that in comparison with the VAE, the images recon-
structed with the GAN model are much sharper. The GAN
generator explicitly penalizes the production of blurry im-
ages that do not look like real images. Therefore, we be-
lieve that the sampler is using a different strategy: rather
than sampling blurred images (which would have high like-
lihood under VAE but low likelihood under GAN), it sam-
ples images that look like hand-drawn digits but are not ac-
tual digits (e.g. last image of Figure 5a and second image
of Figure 5c). One direct extension of this work is to try to
use GAN model on some other handwritten datasets such as
Omniglot (Lake, Salakhutdinov, and Tenenbaum, 2013) and
Kuzushiji-MNIST (Clanuwat et al., 2018).

Conclusion and Future Work

In this paper, we propose a method to sample data that have
known classiﬁcation prediction values. Compared to directly
performing unconstrained gradient descent from an initial
data point, as is the case in most adversarial attack work,
our method also regularizes the resulting examples such that
the sampled data has high likelihood under some speciﬁed
data distribution. To achieve this, we formulate the infer-
ence problem as a Bayesian one, and we use probabilis-
tic programming to sample from the posterior. In addition
to selecting “in-distribution” samples, our method operates
globally: the initial starting point does not affect the ﬁnal re-
sult after sufﬁciently many burn-in samples have been drawn

latents discovered
three-dimensional
Figure 3: Above:
through sampling the posterior as predicted conﬁdently
houses, conﬁdently rockets, or ambiguous. Below: 3000 la-
tents generated directly by sampling the prior distributions.
Ambiguous images have conﬁdences ranging between 45%
and 55% for each class. Indeed, we discover many more am-
biguous examples through our targeted inference approach.

shown in Figure 4. For the ambiguous target, we see that the
depicted images are often the result of several digits overlaid
on each other and lacking in intensity. For the 1vs7 target,
we see that most samples are indeed “between” digit 1 and
digit 7, in that the horizontal stroke is short but visible in
most cases. For the 8vs9 target, the lower circle is in general
smaller than the upper circle, which is also visually between
digit 8 (which should have equally-sized circles) and digit 9
(in which the lower circle degenerates to a vertical stroke).
With the same VAE model, the sampled latent vectors for
the three targets are plotted in Figure 4 after reduction to 2
dimensions using t-SNE. For Figure 4a, the lightly colored
dots represent latent vectors sampled from the prior distri-
bution (i.e. 5D unit Gaussian), color-coded according to the
classiﬁer prediction, and the dark green dots represent la-
tent vectors sampled from the posterior (Eqn 4). For Figure

(a) Ambiguous target

(b) 1vs7 target

(c) 8vs9 target

Figure 4: Images inferred for speciﬁed prediction conﬁdences sampled from the VAE’s latent space, and visualizations of their
presence in the latent space. Dark green dots represent selected ambigious samples. Left: images showing equal conﬁdence
in predictions for all digits 0, 1, . . . , 9. Middle: images showing equal conﬁdence for 1 and 7 classiﬁcations. Right: images
showing equal conﬁdence for 8 and 9 classiﬁcations.

(a) Ambiguous target

(b) 1vs7 target

(c) 8vs9 target

Figure 5: Sampled images for three targets with GAN-
learned data distribution.

and discarded. On both a synthetic dataset and the MNIST
dataset, we demonstrate that our method can indeed sam-
ple data points close to the speciﬁed target while remain-
ing likely under the given distribution. In addition, our tech-
nique may be useful for identifying overconﬁdent networks:
the lack of diversity of ambiguous samples with the MNIST
GAN distribution indicates that the MNIST classiﬁer may
be overly conﬁdent on GAN-generated images.
There are several future directions for this work. First,
MNIST is a relatively easy task with visually distinct

classes. We will evaluate our technique on harder datasets
such as CIFAR-10 or CIFAR-100 for which ambiguous la-
bels (e.g. car vs truck for CIFAR-10 or different classes
within one super-class in CIFAR-100) are more likely.
Moreover, we can verify the effectiveness of conﬁdence cal-
ibration (Guo et al., 2017) of the classiﬁer using our model.
Our method could be also extended for in-distribution ad-
versarial example discovery. For such discovery, we would
require or learn a generative model for the entire dataset with
examples for one class removed. Given this distribution, we
would then sample examples from the posterior given the in-
ference target of being highly conﬁdent as the missing class.
Finally, we can use this method to create neural network
models with notions of ambiguity similar to those of a hu-
man. Speciﬁcally, we plan to iteratively train a classiﬁer and
augment the dataset to incorporate labeled examples drawn
from the ambiguous sets. In the end, the classiﬁer will imi-
tate human prediction both in certain and in ambiguous cases
and will hopefully make only human-like mistakes, which
could be valuable for use in sensitive domains.

Acknowledgements

We thank Alex Lew, Marco Cusumano-Towner, Christian
Muise, and Hendrik Strobelt for fruitful discussions.

References
Athalye, A.; Engstrom, L.; Ilyas, A.; and Kwok, K. 2017.
Synthesizing robust adversarial examples. arXiv preprint
arXiv:1707.07397.

Athalye, A.; Carlini, N.; and Wagner, D. 2018. Obfus-
cated gradients give a false sense of security: Circum-
venting defenses to adversarial examples. arXiv preprint
arXiv:1802.00420.

Balan, A. K.; Rathod, V.; Murphy, K. P.; and Welling, M.
2015. Bayesian dark knowledge. In Advances in Neural
Information Processing Systems, 3438–3446.

Bingham, E.; Chen, J. P.; Jankowiak, M.; Obermeyer, F.;
Pradhan, N.; Karaletsos, T.; Singh, R.; Szerlip, P.; Hors-
fall, P.; and Goodman, N. D. 2018. Pyro: Deep Universal
Probabilistic Programming. Journal of Machine Learning
Research.

Blundell, C.; Cornebise, J.; Kavukcuoglu, K.; and Wierstra,
D. 2015. Weight uncertainty in neural networks.
In
Proceedings of the 32nd International Conference on In-
ternational Conference on Machine Learning-Volume 37,
1613–1622. JMLR. org.

Carlini, N., and Wagner, D. 2017. Towards evaluating the
robustness of neural networks. In 2017 IEEE Symposium
on Security and Privacy (SP), 39–57. IEEE.

Carlini, N.; Athalye, A.; Papernot, N.; Brendel, W.; Rauber,
2019.
arXiv preprint

J.; Tsipras, D.; Goodfellow, I.; and Madry, A.
On evaluating adversarial robustness.
arXiv:1902.06705.

Clanuwat, T.; Bober-Irizar, M.; Kitamoto, A.; Lamb, A.; Ya-
mamoto, K.; and Ha, D. 2018. Deep learning for classical
japanese literature. arXiv preprint arXiv:1812.01718.

Cohen, J. M.; Rosenfeld, E.; and Kolter, J. Z. 2019. Certiﬁed
adversarial robustness via randomized smoothing. arXiv
preprint arXiv:1902.02918.

Corbett-Davies, S.; Pierson, E.; Feller, A.; Goel, S.; and
Huq, A. 2017. Algorithmic decision making and the cost
In Proceedings of the 23rd ACM SIGKDD
of fairness.
International Conference on Knowledge Discovery and
Data Mining, 797–806. ACM.

Cusumano-Towner, M. F.; Saad, F. A.; Lew, A. K.; and
Mansinghka, V. K. 2019. Gen: A general-purpose prob-
abilistic programming system with programmable infer-
ence. In Proceedings of the 40th ACM SIGPLAN Confer-
ence on Programming Language Design and Implementa-
tion, PLDI 2019, 221–236. New York, NY, USA: ACM.

Doshi-Velez, F., and Kim, B. 2017. Towards a rigorous
science of interpretable machine learning. arXiv preprint
arXiv:1702.08608.

Fremont, D. J.; Yue, X.; Dreossi, T.; Ghosh, S.; Sangiovanni-
2018. Scenic:
arXiv preprint

Vincentelli, A. L.; and Seshia, S. A.
Language-based scene generation.
arXiv:1809.09310.

Gal, Y., and Ghahramani, Z. 2016. Dropout as a bayesian
approximation: Representing model uncertainty in deep

learning. In international conference on machine learn-
ing, 1050–1059.

Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.
2014. Generative adversarial nets. In Advances in neural
information processing systems, 2672–2680.

Goodfellow, I. J.; Shlens, J.; and Szegedy, C. 2014. Explain-
ing and harnessing adversarial examples. arXiv preprint
arXiv:1412.6572.

Graves, A. 2011. Practical variational inference for neural
networks. In Advances in neural information processing
systems, 2348–2356.

Guo, C.; Pleiss, G.; Sun, Y.; and Weinberger, K. Q. 2017. On
calibration of modern neural networks. In Proceedings of
the 34th International Conference on Machine Learning-
Volume 70, 1321–1330. JMLR. org.

Kingma, D. P., and Welling, M. 2013. Auto-encoding vari-

ational bayes. arXiv preprint arXiv:1312.6114.

Lake, B. M.; Salakhutdinov, R. R.; and Tenenbaum, J. 2013.
One-shot learning by inverting a compositional causal
In Advances in neural information processing
process.
systems, 2526–2534.

Lakshminarayanan, B.; Pritzel, A.; and Blundell, C. 2017.
Simple and scalable predictive uncertainty estimation us-
ing deep ensembles. In Advances in Neural Information
Processing Systems, 6402–6413.

Lecuyer, M.; Atlidakis, V.; Geambasu, R.; Hsu, D.; and Jana,
S. 2019. Certiﬁed robustness to adversarial examples with
differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), 656–672. IEEE.

Lee, K.; Lee, H.; Lee, K.; and Shin, J.

2017. Train-
ing conﬁdence-calibrated classiﬁers for detecting out-of-
distribution samples.

Li, B.; Chen, C.; Wang, W.; and Carin, L. 2018. Second-
order adversarial attack and certiﬁable robustness. arXiv
preprint arXiv:1809.03113.

Li, Y.; Li, L.; Wang, L.; Zhang, T.; and Gong, B. 2019.
Nattack: Learning the distributions of adversarial exam-
ples for an improved black-box attack on deep neural net-
works. arXiv preprint arXiv:1905.00441.

Lin, Z.; Khetan, A.; Fanti, G.; and Oh, S. 2018. Pacgan: The
power of two samples in generative adversarial networks.
In Advances in Neural Information Processing Systems,
1498–1507.

Maaten, L. v. d., and Hinton, G.

2008. Visualizing
data using t-sne. Journal of machine learning research
9(Nov):2579–2605.

Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and
Vladu, A. 2017. Towards deep learning models resistant
to adversarial attacks. arXiv preprint arXiv:1706.06083.

Nguyen, A.; Yosinski, J.; and Clune, J. 2015. Deep neural
networks are easily fooled: High conﬁdence predictions
for unrecognizable images. In Proceedings of the IEEE

conference on computer vision and pattern recognition,
427–436.

Rosenblatt, M. 1956. Remarks on some nonparametric esti-
mates of a density function. The Annals of Mathematical
Statistics 832–837.

Salvatier, J.; Wiecki, T. V.; and Fonnesbeck, C. 2016. Proba-
bilistic programming in python using pymc3. PeerJ Com-
puter Science 2:e55.

Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and
Salakhutdinov, R. 2014. Dropout: a simple way to prevent
neural networks from overﬁtting. The journal of machine
learning research 15(1):1929–1958.

Srivastava, A.; Valkov, L.; Russell, C.; Gutmann, M. U.; and
Sutton, C. 2017. Veegan: Reducing mode collapse in gans
using implicit variational learning. In Advances in Neural
Information Processing Systems, 3308–3318.

Szegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan,
D.; Goodfellow, I.; and Fergus, R. 2013. Intriguing prop-
erties of neural networks.

Torrey, L., and Shavlik, J. 2010. Transfer learning. In Hand-
book of research on machine learning applications and
trends: algorithms, methods, and techniques. IGI Global.
242–264.

Tran, D.; Kucukelbir, A.; Dieng, A. B.; Rudolph, M.; Liang,
D.; and Blei, D. M. 2016. Edward: A library for proba-
bilistic modeling, inference, and criticism. arXiv preprint
arXiv:1610.09787.

