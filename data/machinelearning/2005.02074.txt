Explainable AI for Classiﬁcation using Probabilistic Logic Inference

Xiuyi Fan1 , Siyuan Liu1 , Thomas C. Henderson2
1Department of Computer Science, Swansea University, UK
2School of Computing, University of Utah, USA
{xiuyi.fan,siyuan.liu}@swansea.ac.uk, tch@cs.utah.edu

0
2
0
2

y
a
M
5

]
I

A
.
s
c
[

1
v
4
7
0
2
0
.
5
0
0
2
:
v
i
X
r
a

Abstract

The overarching goal of Explainable AI is to develop systems
that not only exhibit intelligent behaviours, but also are able
to explain their rationale and reveal insights. In explainable
machine learning, methods that produce a high level of pre-
diction accuracy as well as transparent explanations are valu-
able. In this work, we present an explainable classiﬁcation
method. Our method works by ﬁrst constructing a symbolic
Knowledge Base from the training data, and then performing
probabilistic inferences on such Knowledge Base with lin-
ear programming. Our approach achieves a level of learning
performance comparable to that of traditional classiﬁers such
as random forests, support vector machines and neural net-
works. It identiﬁes decisive features that are responsible for
a classiﬁcation as explanations and produces results similar
to the ones found by SHAP, a state of the art Shapley Value
based method. Our algorithms perform well on a range of
synthetic and non-synthetic data sets.

Introduction
The need for building AI systems that are explainable has
been raised, see e.g., (Doran et al. 2017). The ability to
make machine-led decision making transparent, explainable,
and therefore accountable is critical in building trustwor-
thy systems. Producing explanations is at the core of re-
alising explainable AI. Two main approaches for explain-
able machine learning have been explored in the litera-
ture: (1) intrinsically interpretable methods (Rudin 2019),
in which prediction and explanation are both produced by
the same underlying mechanism, and (2) model-agnostic
methods (Molnar 2019), in which explanations are treated
as a post hoc exercise and are separated from the prediction
In the case for methods (1), while many intrinsi-
model.
cally interpretable models, such as short decision trees, lin-
ear regression, Naive Bayes, k-nearest neighbours and de-
cision rules (Yang et al. 2017) are easy to understand, they
can be weak for prediction and suffer from performance
loss in complex tasks. As for methods (2), model agnos-
tic approaches such as local surrogate (Ribeiro et al. 2016),
global surrogate (Alonso et al. 2018), feature importance
(Fisher et al. 2018) and symbolic Bayesian network trans-
formation (Shih et al. 2018) leave the prediction model in-
tact and use interpretable but presumably weak models
to “approximate” the more sophisticated prediction model.

However, it has been argued that since model agnostic ap-
proaches separate explanation from prediction, explanation
modules cannot be faithful representations of their predic-
tion counterpart (Rudin 2019). In this context, we present
a classiﬁcation approach that produces accurate predictions
and explanations as well as supports domain knowledge in-
corporation.

Given a set of data instances, whose class membership is
known, classiﬁcation is the problem of identifying to which
of a set of classes a new instance belongs. Each instance
is characterised by a set of features F . For some data D,
there exists a labelling function L : D 7→ {POS, ¬POS}.1
Let D ⊆ D be the training set s.t. for each d ∈ D, L(d) is
known. For x ∈ D, we would like to know:

Q1: whether L(x) = POS;
Q2:

if so, which features f ⊆ F make L(x) = POS.

Standard supervised learning techniques answer Q1 but
not Q2, which asks for decisive features. Understanding
“what causes a query instance x to be classiﬁed as in some
class C?” is as important as “does x belong to C?” For
instance, for a diagnostic system taking patients’ medical
records as the input and producing disease classiﬁcations as
the output, pinpointing symptoms that lead to the diagnosis
is as important as the diagnosis itself. In this paper, we pro-
pose algorithms answering both questions. In a nutshell, we
solve classiﬁcation as inference on probabilistic Knowledge
Bases (KBs) learned from data. Speciﬁcally, given training
data D with features F , we deﬁne a function M that maps D
to a probabilistic KB. Then, for a query x, we check whether
M(D) and x together entail POS. Very roughly, we take
classiﬁcation as evaluating

M(D), x |= POS.
In this way, computing explanations for L(x) = POS in our
setting can be formulated as:

(1)

Given M(D), x |= POS, identify some x′ ⊆ x s.t.
M(D), x′ |= POS.
We present two algorithms for probabilistic KB construc-
tion. The ﬁrst one constructs KBs from decision trees and

1POS stands for positive. For presentation simplicity, we only
consider binary classiﬁcation problems in this paper. Our approach
generalises to multi-category classiﬁcation by replacing POS with
class labels for each candidate class accordingly.

 
 
 
 
 
 
the second constructs KBs directly from data. Query clas-
siﬁcation is modelled with probabilistic logic inference car-
ried out with linear programming. The main contributions
are: (i) a method of performing classiﬁcation with proba-
bilistic logic inference; (ii) a polynomial time inference al-
gorithm on KBs; and (iii) algorithms for identifying decisive
features as explanations and incorporating domain knowl-
edge in classiﬁcation and explanation.

Training as Knowledge Base Construction

KB construction is at the core of our approach. Speciﬁcally,
a KB contains a set of disjunction clauses and each clause
has a probability, deﬁned formally as follows.

Deﬁnition 1. A Knowledge Base (KB) {hp1, c1i, . . . ,
hpm, cmi} is a set of pairs of clauses ci and probability of
clauses pi = P (ci), 1 ≤ i ≤ m. Each clause is a disjunc-
tion of literals and each literal is a propositional variable or
its negation.

Example 1. With two propositional variables α and β,
{h0.6, ¬α ∨ βi, h0.8, αi} is a simple KB containing two
clauses with probabilities 0.6 and 0.8, respectively.

Generating logic clauses from data has been studied in
the literature, see e.g., (Chiang et al. 2001; Quinlan 1987)
for extracting rules from decision trees, and more recently,
(Mashayekhi and Gras 2017) for extracting rules from ran-
dom forests. Unlike these approaches where, due to their use
of strict inference methods, non-probabilistic rules are gen-
erated, our KBs consist of probabilistic rules. Speciﬁcally,
from a decision tree constructed from the training data, we
create a clause c from each path from the root to the leaf of
the tree. The probability of c is the ratio between the positive
samples and all samples at the leaf. Formally, we deﬁne the
KB KT drawn from a decision tree T as follows.

Deﬁnition 2. Let T be a decision tree, each non-root node in
T labelled by a feature-value pair a v, read as feature a hav-
ing value v. Let {ρ1, . . . , ρk} be the set of root-to-leaf paths
in T, where each ρi is of the form hroot, a1 v1, . . . , an vmi
and an vm labels a leaf node in T. Then, the KB drawn
from T is KT = {hp1, c1i, . . . , hpk, cki} s.t.
for each ρi,
hpi, cii ∈ KT, where ci = POS ∨ ¬a1 v1 ∨ . . . ∨ ¬an vm,
and pi is the ratio between positive and the total samples in
the node labelled by an vm.

Algorithms 1 and 2 construct KT from data D. Speciﬁ-
cally, Algorithm 1 takes a root-to-leaf path from a decision
tree to generate a clause. The path with features a1, . . . , an,
s.t. each feature has a value in {v1, . . . , vm}, is interpreted
as a1 v1 ∧ . . . ∧ an vm → POS, and read as, a sample is
positive if its feature a1 has value v1, . . . , feature an has
value vm. As a disjunction, the clause is then written as
POS ∨ ¬a1 v1 ∨ . . . ∨ ¬an vm. Algorithm 2 builds a tree and
then constructs clauses from paths in the tree. Example 2
illustrates how to build a KB from a decision tree.

Algorithm 1 Clause from Tree Path

1: procedure CLAUSEFROMPATH(path)
2:
3:
4:
5:
6:
7:

clause ← POS
for each edge e in path do
a ← feature of e
v ← value of e
clause ← clause ∨ ¬a v

return clause

Algorithm 2 Construct KB with Decision Tree
1: procedure DECSIONTREEKB(D)
2:
3:
4:
5:
6:
7:
8:

KT ← {}; Use ID3 to compute a tree T from D
allPaths ← all paths from the root to leaves in T
for each path in allPaths do
n ← end node in path
r ← ratio between positive and total samples in n
add [r] CLAUSEFROMPATH(path) to KT

return KT

Example 2. Given a data set with four strings, 0000, 1111,
1010, 1100, labelled positive, and four strings, 0010, 0100,
1110, 1000, labelled negative. There are four features, bits
1-4, each feature takes its value from {0, 1}. The decision
tree constructed is shown in Figure 1. There are eight leaves,
thus eight root-to-leaf paths and clauses. E.g., root → a4 0
→ a1 0 → a2 0 → a3 0 gives the clause POS ∨ ¬a4 0 ∨
¬a1 0 ∨ ¬a2 0 ∨ ¬a3 0. The probability of the clause is
the number of positive samples over the total samples at the
leaf. There is only one sample, 0000, at this leaf, since it is
positive, the clause probability is 1. The KB KT is shown in
Table 1.2

xqqq
&▼▼▼

xqqq

a4 0

xqqq

a3 0

root

&▼▼▼

a4 1

a1 1

&▼▼▼

a2 0

a2 1

&▼▼▼

a3 1

a3 0

a3 1

xqqq

a1 0

a2 1

xqqq

a2 0

a3 1

a3 0

Figure 1: Decision tree learned from data in Example 2. A node
aX Y is read as “bit X has value Y ”.

Table 1: KT from the tree in Figure 1.

[0.0]
[1.0]
[0.0]
[1.0]
[0.0]
[0.0]
[1.0]
[1.0]

POS ∨¬a1 0 ∨ ¬a2 0 ∨ ¬a3 1 ∨ ¬a4 0
POS ∨¬a1 0 ∨ ¬a2 0 ∨ ¬a3 0 ∨ ¬a4 0
POS ∨¬a1 0 ∨ ¬a2 1 ∨ ¬a4 0
POS ∨¬a1 1 ∨ ¬a2 0 ∨ ¬a3 1 ∨ ¬a4 0
POS ∨¬a1 1 ∨ ¬a2 0 ∨ ¬a3 0 ∨ ¬a4 0
POS ∨¬a1 1 ∨ ¬a2 1 ∨ ¬a3 1 ∨ ¬a4 0
POS ∨¬a1 1 ∨ ¬a2 1 ∨ ¬a3 0 ∨ ¬a4 0
POS ∨¬a4 1

Algorithm 2 constructs clauses from root-to-leaf paths in
a decision tree. We can also use paths from the root to all
nodes, not just the leaves, to construct clauses, i.e., replacing

2Henceforth, [p] z1 ∨ . . . ∨ zl denotes an l-literal clause in a KB

with probability p.

x
&
x
&
x
(cid:15)
(cid:15)
(cid:15)
(cid:15)
&
x
(cid:15)
(cid:15)
x
(cid:15)
(cid:15)
(cid:15)
(cid:15)
&
line 3 in Algorithm 2 with

allPaths ← all paths from the root to all nodes in T .
As random forests have been introduced to improve the
stability of decision trees, we can apply the same idea to
obtain more clauses from a forest, i.e., repeatedly gener-
ated different decision trees, and for each tree, we construct
clauses for each path originated at its root, in the spirit of
(Mashayekhi and Gras 2017). If we further take the above
idea of “generating as many clauses as possible” to its limit,
we realise that constructing KBs from trees is a special case
of selecting clauses constructed from all k-combinations of
feature-value pairs, for k = 1 . . . n, where n is the total num-
ber of features in the data. Formally, we deﬁne the KB KD
drawn directly from data D as follows.
Deﬁnition 3. Given data D with features F = {a1, . . . , an}
taking values from V = {v1, . . . , vm}, for each Fk =
{a′
k =
{a′
k . For each c =
{a′′
k} ∈ Ck, Si ⊆ D is the set of samples
i′ for all i ∈ {1, . . . , k}. If
s.t.
|Si| 6= 0, then let pi be the ratio between positive samples in
Si and |Si|, hpi, POS ∨ ¬a′′
ki is in the KB
KD drawn directly from data. There is no other clause in KD
except those constructed as above.

k} ∈ 2F \ {}, let C1
1, . . . , a′
k v|v ∈ V }. Ck = C1
k v′
1 v′
i having value v′

k = {a′
k × . . . × Ck

1, . . . , a′′
feature a′′

1 v|v ∈ V }, . . . , Ck

1 ∨ . . . ∨ ¬a′′

k v′

1 v′

Deﬁnition 3 can be illustrated with the following example.
Example 3. Let F = {a1, a2} and V = {0, 1}. Then
2F \ {} = {{a1}, {a2}, {a1, a2}}. For illustration, let
us choose Fk = {a1, a2}. Then C1
k = {a1 0, a1 1},
C2
k = {a2 0, a2 1}, and Ck = {{a1 0, a2 0}, {a1 0, a2 1},
{a1 1, a2 0}, {a1 1, a2 1}}. Then, suppose we choose c =
{a1 0, a2 0} and add hpi, POS∨¬a1 0∨¬a2 0i to KD, where
pi is the ratio between positive samples with both features
a1, a2 having value 0 and total samples with these feature-
values. KD can be constructed by choosing different Fk and
c iteratively.

Algorithm 3 Construct KB Directly

feaVals ← {a v| feature a has value v in entry}
label ←binary label of entry as integer
S ← POWERSET(feaVals) \{}
for each key as an element of S do

if key is in counts then

counts[key] ← counts[key] + [1, label]

counts ← {}, KD ← {}
for each entry in data do

1: procedure DIRECTKB(data)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

for each key in counts do

return KD

else

counts[key] ← [1, label]

r ← counts[key][1]/counts[key][0]
Insert “[r] POS ∨ ¬key” to KD

KB construction approaches. Proposition 1 and 2 sanction
that all clauses extracted from decision trees can be con-
structed directly in KD and all clauses built in KD can be
extracted from some trees, respectively.
Proposition 1. Given a data set D, KT ⊆ KD.

Proof. (Sketch.) S constructed in Line 6, Algorithm 3 is
the powerset of all possible feature-value pairs in D and a
path in a decision tree represents some feature-value pairs
in D. Thus, any clause produced by a tree is produced by
Algorithm 3.

Proposition 2. Given a data set D, for each clause c ∈ KD,
there exists a decision tree T constructed from D s.t. there is
a path p in T and the clause drawn from p is c.

Proof. (Sketch.) All clauses in KD are of the form POS ∨
¬a1 v1 ∨ . . . ∨ ¬an vm where ai vj are feature-value pairs
and for any ai, aj ∈ {a1, . . . , an}, if i 6= j, then ai 6= aj.
Thus, one can construct a tree T containing the path root →
a1 v1 → . . . → an vm.

Querying as Probabilistic Inference
Our KB construction methods produce clauses with proba-
bilities. Intuitively, for a query that asserting some feature-
value pairs, we want to compute the probability of POS un-
der these feature-value pairs and predicting the query be-
ing positive when the probability is greater than 0.5. To
introduce our inference method for computing such proba-
bilities, we ﬁrst review a few concepts in probabilistic logic
(Nilsson 1986), which pave the way for discussion.

Given a KB K4 with clauses c1, . . . , cm composed from
n propositional variables, the complete conjunction set, as
W, over K is the set of 2n conjunctions s.t. each conjunc-
tion contains n distinct propositional variables. A proba-
bility distribution π (wrt. K) is the set of 2n probabilities
π(w) ≥ 0, (w ∈ W) s.t. Pw∈W π(w) = 1. π satisﬁes K iff
for each i = 1, . . . , m, the sum of π(w) equals P (ci) for all
w s.t. the truth assignment satisfying w satisﬁes ci. A KB K
is consistent iff there exists a π satisfying K.

With a consistent KB, Nilsson suggested that one can de-
rive literal probabilities from π, i.e., for all literals z in
the KB, P (z) is the sum of π(w) for all w ∈ W contain-
ing z, e.g., for a consistent KB with two literals α and β,
P (α) = P (α ∧ β) + P (α ∧ ¬β) (Nilsson 1986). In short,
to compute literal probabilities, one ﬁrst computes probabil-
ity assignments over the complete conjunction set, and then
adds up all relevant probabilities for the literal.

At ﬁrst glance, since POS is an literal in our knowl-
edge base, it might be possible to perform our inference
with the above approach for computing P (POS): all clauses
in a KB are of the form POS ∨ ¬a1 v1 ∨ . . . ∨ ¬an vm,
each with an associated probability; a query is a set of

Algorithm 3 gives a procedural construction for KD.3 The
following propositions describe the relation between the two

3In Line 14, ¬{s1, . . . , sn} is ¬s1 ∨ . . . ∨ ¬sn, e.g.
for
key = {a1 v1, a2 v2}, insert “[p] POS ∨ ¬a1 v1 ∨ ¬a2 v2” to KD.
counts is a dictionary with keys being sets of feature-value pairs

and values being two-element arrays. label is either 0 or 1. Line 9
is an element-wise addition, e.g., [1,0]+[1,1]=[2,1]. At the end of
the ﬁrst loop, counts[key][0] is the number of samples containing
key and counts[key][1] is the number of positive ones.

4From this point on, we use K to denote a KB constructed using

either of the two approaches (KT or KD).

n v′

1, . . . , a′

feature-value pairs, e.g., a′
1 v′
m, each with an
assigned probability 1; P (POS) computed as the sum of
P (POS∧a1 v1∧. . .∧an vm), P (POS∧a1 v1∧. . .∧¬an vm),
. . . , P (POS∧¬a1 v1∧. . .∧¬an vm) estimates the likelihood
of POS. However, this idea fails for the following two rea-
sons. Firstly, this approach requires solving the probability
distribution π, which has been shown to be NP-hard wrt. the
number of literals in the KB(Georgakopoulos et al. 1988),
thus the state-of-the-art approaches only work for KB with
a few hundred of variables (Finger and Bona 2011).

Secondly, putting a KB and a query together introduces
inconsistency, so there is no solution for π. For instance, for
the KB in Example 2, let the query be 0000, which trans-
lates to four clauses, a1 0, a2 0, a3 0 and a4 0, each with
P (ai 0) = 1. Consequently, P (¬ai 0) = 0. Together with
P (POS ∨ ¬a1 0 ∨ ¬a2 0 ∨ ¬a3 0 ∨ ¬a4 0) = 1, we infer
P (POS) = 1. However, P (POS) = 1 is inconsistent with
P (POS ∨ ¬a1 0 ∨ ¬a2 0 ∨ ¬a3 1 ∨ ¬a4 0) = 0, as for any
α, β, we must have P (α) ≤ P (α ∨ β). In this case, K is
inconsistent with the query thus there is no solution for π.

One might suspect the inconsistency illustrate above is an
artefact of our KB construction, i.e., there could exist ways
to construct KB s.t. consistency can be ensured. Although
this might be the case, there is no such existing method as
far as we know and when we incorporate domain knowledge
later in this paper, it becomes clear that being able to tolerate
inconsistency is useful.

Since the source of the complexity is in the computation
of the probability distribution over the complete conjunction
set, we avoid computing it explicitly and introduce an ef-
ﬁcient algorithm for estimating literal probabilities without
computing π. We formulate the computation as an optimiza-
tion problem so that inconsistency is tolerated. This is the
core of our inference method.
Deﬁnition 4. Given a KB K = {hp1, c1i, . . . , hpm, cmi}
with clauses C = {c1, . . . , cm} over literals Z, a linear pro-
gram LK of K with unknowns ω(σ), σ ∈ C ∪ Z, is the fol-
lowing.
minimise:

m

|ω(ci) − pi|

X
i=1

subject to: for each clause ci = z1 ∨ . . . ∨ zl,

ω(ci) ≤ ω(z1) + . . . + ω(zl);

for zj = z1 . . . zl in clause ci:

ω(ci) ≥ ω(zj);

1 = ω(zj) + ω(¬zj);
0 ≤ ω(zj) ≤ 1.

(2)

(3)

(4)
(5)
(6)

Deﬁnition 4 estimates literal probabilities from clause
probabilities without computing the distribution over the
complete conjunction set, i.e., for any literal z in the KB,
ω(z) approximates P (z). The intuition is as follows.
• Constraints
(3)
ity laws,
(Casella and Berger 2002),
and (6) deﬁne the bound.

probabil-
are
is the Boole’s inequality
(5)
is monotonicity;
(4)

given
i.e., Eqn.

by Eqn.

(3-6)

• The optimisation function Eqn. (2) is used to tolerate in-
consistency, i.e., for a KB containing inconsistent clauses,
s.t. some of the constraints cannot be met, we allow clause
probabilities to be relaxed by not forcing ω(ci) = P (ci)
as constraints. We still want the estimated clause prob-
abilities (ω(ci)) to be as close to their speciﬁed values
(P (ci)) as possible, so Eqn. (2) minimises their differ-
ence. A linear difference is chosen to ensure a low com-
putational complexity.

Note that, for all literals in a clause, their estimated prob-
abilities are constrained by inequalities local to the clause
(e.g., ω(ci) ≤ ω(z1) + . . . + ω(zl)). We avoid the expo-
nential growth of constraints, which causes the NP compu-
tational difﬁculties, by forgoing not only explicit probability
computation for the complete conjunction set but also global
constraints on estimated clause probabilities, e.g., for two
clauses c1 = α ∨ β and c2 = α ∨ β ∨ γ, we do not enforce
ω(c1) ≤ ω(c2). We illustrate probability computation with
the following example.
Example 4. (Example 1 cont.) Given these two clauses,
c1 = ¬α ∨ β; c2 = α, and their probabilities, P (c1) =
0.6, P (c2) = 0.8, the complete conjunction set W = {¬α ∧
¬β, ¬α ∧ β, α ∧ ¬β, α ∧ β}. Truth assignments satisfying
α ∧ β, ¬α ∧ β, and ¬α ∧ ¬β satisfy c1 and truth assignments
satisfying α ∧ β and α ∧ ¬β satisfy c2. K is consistent iff
π1 = π(α ∧ β), π2 = π(α ∧ ¬β), π3 = π(¬α ∧ β), and
4
π4 = π(¬α ∧ ¬β) s.t. P
j=1 πj = 1, π1 + π3 + π4 = 0.6
and π1 + π2 = 0.8. LK is:

minimise:

subject to:

|ω(c1) − 0.6| + |ω(c2) − 0.8|

ω(c1) ≤ ω(¬α) + ω(β); ω(c2) ≤ ω(α);
ω(c1) ≥ ω(¬α);
1 = ω(α) + ω(¬α);
0 ≤ ω(α) ≤ 1;

1 = ω(β) + ω(¬β);
0 ≤ ω(β) ≤ 1.

ω(c1) ≥ ω(β);

ω(c2) ≥ ω(α);

A solution to LK is: ω(¬α ∨ β) = 0.6; ω(α) = 0.8;
ω(¬α) = 0.2; ω(β) = 0.6; ω(¬β) = 0.4.

It is easy to see that K is consistent, and for all literals z in
K, ω(z) is a probability assignment for z. Deﬁnition 4 gives
a means of performing probabilistic inference, as this Exam-
ple can be seen as modus ponens, i.e., from (α → β, α) ⊢ β
where P (α → β) = 0.6, P (α) = 0.8, we infer ω(β) = 0.6.
In general, for a literal z in a KB K, it may be the case that
no π exists such that ω(z) equals the probability computed
from π. E.g., consider:
Example 5. Let K = {c1 . . . c4}, in which

c1 is [1.0] α ∨ β;
c3 is [1.0] β ∨ γ;

c2 is [1.0] α ∨ γ;
c4 is [1.0] α ∨ β ∨ γ.

Then, ω(ci) = 1, ω(α) = ω(β) = ω(γ) = 0.5 is a solu-
tion to LK where the objective function attains 0. However,
ω(z) 6= P (z), for z = α, β, γ.5

5This shows that LK has a feasible region larger than the
solution space of π. However, since linear programming algo-
rithms look for solutions at the boundary of variables, we do not
Indeed, the Gurobi solver ﬁnds
see such solutions in practice.
ω(z) = P (z) = 1, for z = α, β, γ, which are in the solutions
computed with Nilsson’s method.

Relations between literal probability found via comput-
ing exact solutions from the distribution over the complete
conjunction set and solutions found in LK are as follows.

Lemma 1. If K is consistent, then solutions for all ω(z)
z ∈ Z exist s.t. Eqn. (2) minimises to 0.

Proof. (Sketch.) Eqn.
(2) minimises to 0 only when
ω(ci) = pi for all ci. If K is consistent, then there is an
assignment of values to the literal probabilities that satis-
ﬁes the constraints, i.e., for all literals z and all clauses c,
ω(z)=P (z) and ω(c)=P (c) minimise (2) to 0.

Corollary 1. Given a KB K, if LK does not minimise to 0,
then K is not consistent.

Proof. (Sketch.) By the contrapositive of Lemma 1, if there
is no assignment that solves the linear programming prob-
lem, then there is no exact solution for π.

Proposition 3. Given a KB K with n propositional variables
x1, . . . , xn, each ω(xi) in LK can be computed in polyno-
mial time wrt. n.

Proof. (Sketch.) Let u be the number of unknowns in LK,
m the number of clauses in K, u = 2n + m. Linear Pro-
gramming is polynomial time solvable.

It is theoretically interesting to ask, for consistent KBs,
what the error bound between literal probability computed
with Nilsson’s method and our linear programming method
is, subject to a chosen linear programming solver. However,
in the context of this work, answering such question is less
important as KBs generated by our approach are not neces-
sarily consistent. For such KBs, Nilsson’s approach gives no
solution thus these is no “error bound” exists.

j) = 0, where v′

With a means to reason with KBs, we are ready to answer
queries. Algorithm 4 deﬁnes the query process. Let LK
be the linear system constructed from K. Given a query Q
with feature-value pairs a1 v1, . . . , an vm, we amend LK
by inserting ω(ai vj) = 1 and ω(ai v′
j is
a possible value of ai, v′
j 6= vj, for all ai, vj in Q. ω(POS)
computed in LK answers whether Q is positive. Since the
solution of ω(POS) can be a range, we compute the upper
and lower bounds of ω(POS) by maximising and minimising
ω(POS) subject to minimising Eqn.(2), respectively, and use
the average of the two. It returns positive when the average
is greater than 0.5. The intuition of our approach is that, for
a query x, to evaluate whether K, x |= POS, we compute
ω(POS) in LK, in which K is treated “defeasibly” s.t.
the
probabilities of a clauses in K can be relaxed whereas the
query x is treated “strictly” as constraints in LK. Example 6
illustrates the query process.

Example 6. (Example 2 cont.) For query 0101, we add the
following equations as constraints to LK:
ω(a1 0) = 1, ω(a1 1) = 0, ω(a2 0) = 0, ω(a2 1) = 1,
ω(a3 0) = 1, ω(a3 1) = 0, ω(a4 0) = 0, ω(a4 1) = 1.
The computed ω(POS) is no greater than 0.5, representing a
negative classiﬁcation.

Algorithm 4 Query Knowledge Base

for each feature a in query do

1: procedure QUERYKB(query, LK)
2:
3:
4:
5:
6:
7:
8:

for each possible value v of a do
if a has value v in query then
Add ω(a v) = 1 to LK

return ω(POS) computed in LK

Add ω(a v) = 0 to LK

else

The proposed querying mechanism differs fundamentally
from that of decision trees. A decision tree query can be
viewed as ﬁnding the longest clause in the KB that matches
with the query in and checking whether its probability is
greater than 0.5. For instance, for query 0101, a decision
tree query returns positive as the longest matching clause in
“POS ∨¬a4 1” has probability 1. However, our approach
considers probabilities from other clauses in the K and pro-
duces a different answer.

Since KB constructed with Algorithm 3 contains far more
clauses than Algorithm 2, to improve query efﬁciency, for
a given query Q, we can construct a KB that only contains
clauses directly relevant to Q, as shown in Example 7, and
perform query on this subset of clauses, as shown in Algo-
rithm 5.6 Query performed on the relevant KB gives the
same result as in the full KB, KD, as irrelvant clauses give
no additional constraint to ω(POS).

Algorithm 5 Construct Relevant Knowledge Base

feaVals ← {a v| feature a has value v in Q}
S ← POWERSET(feaVals) \{}, relevantKB ← {}
for each key an element of S do
for each clause in KD do

1: procedure QUERYRELEVANT(Q,KD)
2:
3:
4:
5:
6:
7:
8:

Insert clause to relevantKB

if clause contains key then

return relevantKB

Example 7. (Example 6 cont.) relevantKB for query 0101
is follows:
[0.33]
[0.5]
[0.0]
[0.5]
[0.0]

POS ∨¬a1 0
[0.5]
POS ∨¬a3 0
[1.0]
POS ∨¬a1 0 ∨ ¬a2 1
[0.5]
POS ∨¬a2 1 ∨ ¬a3 0
[1.0]
POS ∨¬a1 0 ∨ ¬a2 1 ∨ ¬a3 0

POS ∨¬a2 1
POS ∨¬a4 1
POS ∨¬a1 0 ∨ ¬a3 0
POS ∨¬a2 1 ∨ ¬a4 1

Overall, our method is non-parametric so no tuning is re-
quired. Query generalization is the result of restricting the
solution space of ω(POS) through clauses describing subsets
of the query. In Example 7, 0101 is not in the training set.
However, the relations between its substrings and POS are
described by clauses in the KB. Jointly, these clauses decide
ω(POS), which approximates P (POS) for this query.

6In line 6, a clause containing a key is deﬁned syntactically, e.g.,

“POS ∨ ¬a1 0 ∨ ¬a2 0” contains {¬a1 0, ¬a2 0}.

Explanation and Knowledge Incorporation
Several methods for comparing feature importance as a
form of explanation have been introduced in the literature.
Some of these methods, e.g.
(Zhao and Hastie 2019) and
(Apley and Zhu 2016), study the relation between features
and the overall classiﬁcation for all training cases. They are
“global” methods in the sense that they answer the question:
“Which feature has the strongest correlation with the class
label in a dataset?” Whereas other methods, notably Shaply
Value based approaches ( ˇStrumbelj and Kononenko 2014;
Lundberg and Lee 2017; Lundberg et al. 2020), study fea-
ture value contribution for individual instances. They are
“local” and answer: “For a given query instance, how much
contribution does each of its feature value make?” In this
sense, ours is a local approach that explains query instances.
One advantage of the presented classiﬁcation method is
that it supports partial queries, which are queries with miss-
ing values, as the probability of POS can be computed with-
out values assigned to all features. Explanation computation
can be supported with partial queries in our approach. Al-
gorithm 6 outlines one approach. Given a query Q with n
features, to ﬁnd the k most decisive features, we construct
sub-queries s.t. each sub-query contains exactly k feature-
value pairs in Q. If Q yields a positive classiﬁcation, then
the sub-query that maximises ω(POS) is an explanation; oth-
erwise, the sub-query that minimises ω(POS) is. Since we
n
know that there are (cid:0)
k(cid:1) different sub-queries in total, the
order of sub-query evaluation can be strategised with meth-
ods such as hill climbing for more efﬁcient calculation. Al-
though in principle, Algorithm 6 could work with any clas-
siﬁcation technique supporting partial queries, our proposed
method does not require reconstructing the trained model for
testing each of the sub-queries, making the explanation gen-
eration convenient. The explanation approach is illustrated
in Example 8.

Algorithm 6 Explanation Computation

1: procedure COMPUTEEXPLANATION(Q,LK, k)
S ← {sQ|sQ ∈ 2Q, SIZEOF(sQ) = k}
2:
if QUERYKB(Q, LK) > 0.5 then
3:
4:
5:
6:

return arg maxsQ∈S QUERYKB(sQ, LK)

return arg minsQ∈S QUERYKB(sQ, LK)

else

n
k-feature explanation, our approach considers (cid:0)
k(cid:1) k-feature
coalitions and select the “most decisive” coalition. Where-
ase Shaply Value approaches consider each feature individ-
ually and returns the set of k most decisive fetures.

Incorporating domain knowledge to complement data-
driven machine learning is supported by our approach. Since
a KB consists of probabilistic clauses, any knowledge K′,
about either a speciﬁc query or the overall model, can be
used alongside K, as long as it is represented in clausal form.
In other words, Equation 1 can be revised to

(7)

M(D), K′, x |= POS.
Two advantages of our approaches are (1) incorporated
knowledge is used in the same way as clauses learned from
data; and (2) since the inference process tolerates inconsis-
tency, incomplete or imperfect knowledge can be incorpo-
rated. For instance, suppose we somehow know it is “mostly
true” that a string is positive if either its 3rd or 4th digit is 0.
If we liberally take “mostly true” as, saying, probability 0.9,
this can be represented as a3 0 ∨ a4 0 → POS, so we insert
POS ∨ ¬a4 0

POS ∨ ¬a3 0

[0.9]

[0.9]

into K to complement clauses learned from data. Although
similar clauses or even the same clause with different prob-
abilities may already exist in the KB, our ability of toler-
ating inconsistencies could accommondate such knowledge,
as shown in the next section.

Performance Analysis
Deﬁnition 4 gives an efﬁcient system construction. As
shown in Figure 2, we can solve KBs containing up to
10,000 variables and 10,000 clauses within a few seconds
on a single CPU workstation with an Xeon 2660v2 proces-
sor and 32GB RAM. The ability of approaching KBs of such
large sizes enables solving practical classiﬁcation tasks.

30

20

10

)
s
(

i

e
m
T
n
u
R

0
1000

#clause=5000, length=10
#clause=5000, length=20
#clause=5000, length=30
#clause=10000, length=10
#clause=10000, length=20
#clause=10000, length=30

2000

3000

4000

5000

6000

7000

8000

9000

10000

Number of Variables

Example 8. (Example 6 cont.) To compute the single most
decisive feature, we let k = 1. S contains four feature-
value pairs: q1 = {a1 0}, q2 = {a2 1}, q3 = {a3 0},
q4 = {a4 1}. Let ωi, i = 1 . . . 4 be ω(POS) computed
with q1 . . . q4, respectively. We have ω1 = 0.33, ω2 =
0.5, ω3 = 0.5, and ω4 = 1. Thus, the computed ex-
planation for the classiﬁcation is a1 0. We read this as:
0 - - - is responsible for 0101 being negative.
This matches with our intuition well as for each of the other
choices, there are at least as many positive samples as neg-
ative ones.

Note that there is a subtle difference between our ap-
proach and Shaply Value based methods. Upon computing a

Figure 2: Experiment results from KB with different sizes.

To evaluate the proposed classiﬁers, we ﬁrst conduct ex-
periments on six real data sets, with results shown in Ta-
ble 2. For each data set, we measure the performance
with the F1 score, taken as the average of 50 runs for each
data set. Our approaches are Tree (Algorithm 2) and Di-
rect (Algorithm 3). We use CART (a decision tree algo-
rithm), multi-layer perceptron (MLP) neural networks (with
two hidden layers with 12 and 10 nodes, respectively), ran-
dom forest (with 100 trees) and support vector machine as
our comparison baselines. The six real data sets include the
Titanic 7, Mushroom, Nursery and HIV-1 protease cleav-

7https://www.kaggle.com/c/titanic

 
 
Table 2: Experiment results (F1 scores) with multiple data sets and
several baseline algorithms.

Titanic Mushroom

Tree
Direct
CART
MLP
Forest
SVM

0.79
0.79
0.82
0.81
0.82
0.78

0.99
0.99
0.99
0.99
0.99
0.99

Nursery
0.99
0.99
0.99
0.99
1
0.99

HIV-1
0.87
0.97
0.94
0.73
0.98
0.99

Bill
0.98
0.99
0.99
0.98
0.99
0.99

Vehicle
0.95
0.96
0.98
0.96
0.98
0.97

age data sets from the UCI Machine Learning Repository
(Dua and Karra Taniskidou 2017), the UK parliament bill
data set reported in ( ˇCyras et al. 2019) as well as an image
data set for vehicle classiﬁcation. For the Titanic data set,
we used seven discrete features – ticket class, sex, age (dis-
cretized to 4 categories), number of siblings, number of par-
ents, passenger fare (discretized to 3 categories), and port
of embarkation. For the Mushroom data set, we used the
ﬁrst 11 features. For the multi-class data set Nursery, we
randomly selected two classes and discarded others. For the
Parliament bill, we used ﬁve features – House of Commons
or House of Lords, type of bill, number of sponsors, bill sub-
ject, and ﬁnal stages of the bill. The vehicle image data set
contains 1635 images with 767 of them being cars and the
rest busses and trucks. Feature extraction has been applied
with 12 features created for each image. They are: num-
ber of pixels of the object, shape coefﬁcient 1-5, mean and
standard deviation of RGB channels. Each data set has been
pre-processed such that the positive and the negative sam-
ples are balanced by randomly replicating samples in the
smaller class. For all data set, the ratio between training
and testing is 70% to 30%. Overall, we see that Direct gives
satisfactory performance.

To evaluate our explanation approach, we ﬁrst compare
Direct with the state of the art Shapley Value based approach
SHAP (Lundberg et al. 2020), using the Titantic and Mush-
room data sets. The results are shown in Figure 3, with Fig-
ure 3(a)(b) showing the results from the Titantic data set and
Figure 3(c)(d) from the Mushroom. Figure 3(a) shows the
percentage of the same features suggested as explanations
for different explanation lengths (i.e., k = 1, 2, 3, 4, 5). For
example, when k = 1 (computing one-feature explanations),
75% of all instances have the same feature chosen as the
explanation by both approaches. When k = 2 (comput-
ing two-feature explanations), there are 72% and 25% in-
stances found with the same 1 and 2 features, respectively.
Figure 3(b) shows the percentage of each feature being se-
lected as an explanation across all instances. We see that
when k = 1, ours and SHAP both suggest that feature 2 ex-
plains the classiﬁcation result for over 70% instances. When
k = 2, the two approaches agree that feature 2 is an expla-
nation while differing on the choice for the other feature.

Results presented in Figure 3 shows that our approach
gives similar results to SHAP. As there is no explanation
ground truth in these data sets, it is impossible to decide who
gives “correct” explanations. To address this, we performed
further experiments with synthetic data sets with known ex-
planation ground truth. Speciﬁcally, we created four syn-
thetic data sets of integer strings, Syn 10/4, Syn 10/8, Syn

12/4, and Syn 12/8, with the following rules. For each
data set, we set a (random) seed string of the same length
as strings in the data set from the same alphabet. For in-
stance, for the “Syn 10/4” data set with 10 bits strings where
each bit can take 4 possible values, 3232411132 is the seed.
(Here, the size of the alphabet is 4. Each 10-bit string de-
notes a data instance with 10 features s.t. each feature takes
its value from {1,2,3,4}.) A string s in the data set is labelled
positive iff s match bits in the seed for exactly ﬁve places.
E.g., 31334212428 is positive and 3133421232 is negative
(it shares 6 bits as the seed rather than 5). For each string
classiﬁed as positive, we compute a k-bit explanation. An
explanation is correct iff the seed string has the same values
for the bits identiﬁed as the explanation. The accuracy of an
explanation is deﬁned as the number of correct bits over the
length of explanation. For instance, for k = 5, we have

Query
3233112143
3244341112

Explanation
323–1-1–
-2—411-2

Seed
3232411132
3232411132

Accuracy
1.0
0.8

The 2nd query contains an incorrect explanation 4. On our
synthetic data sets with a 70% to 30% split on training and
testing, the classiﬁcation result is shown in Table 3 and the
explanation accuracy for the Direct and SHAP approaches is
shown in Table 4. This is an informative experiment as: (1)
there is no “useless” feature in the data set as every feature
(bit) could be decisive thus functions as part of an explana-
tion as long as its value is the same as the feature in the seed;
(2) the seed is the known ground truth for explanation com-
parison; (3) moreover, as shown in Table 3, these datasets
represent non-trivial classiﬁcation problems.

Table 3: Experiment results (F1 scores) with synthetic data sets and
several baseline algorithms.

Syn 10/4
0.71
0.92
0.79
0.77
0.90
0.85

Syn 10/8
0.78
0.95
0.87
0.83
0.96
0.86

Syn 12/4
0.62
0.89
0.70
0.73
0.85
0.81

Syn 12/8
0.70
0.94
0.84
0.80
0.93
0.81

Tree
Direct
CART
MLP
Forest
SVM

Table 4: Explanation accuracy on four syntactic data sets and vari-
ous explanation lengths k.

10/4

10/8

12/4

12/4

Direct
SHAP
Direct
SHAP
Direct
SHAP
Direct
SHAP

k = 1
1
1
1
0.996
1
0.993
1
1

k = 2
1
1
1
0.995
0.982
0.980
1
0.990

k = 3
1
0.996
0.997
0.972
1
0.973
0.998
0.977

k = 4
0.995
0.993
0.980
0.967
0.997
0.942
0.975
0.929

k = 5
0.972
0/962
0.976
0.951
0.901
0.856
0.964
0.918

Table 3 shows that, similar to Table 2, the classiﬁcation
accuracy of our approach is competitive comparing to the
baseline approaches. This further validates our approach for

8The underlined bits are identical to the seed.

k=1

25%

k=2
3%

25%

k=3

9%

15%

k=4

13%

14%

k=5

10%

24%

75%

72%

77%

73%

66%

1 same feature

2 same features

3 same features

4 same features

5 same features

No same features

e
g
a
t
n
e
c
r
e
P

0.8

0.6

0.4

0.2

0

(a) The percentages of the same explanations suggested by Direct and Shapley over the Titantic data
k=5

k=2

k=1

k=4

k=3

0.8

0.8

1

1

e
g
a
t
n
e
c
r
e
P

0.6

0.4

0.2

e
g
a
t
n
e
c
r
e
P

0.6

0.4

0.2

1

2

3
5
4
Features

6

7

0

1

2

3
5
4
Features

6

7

0

1

2

3
5
4
Features

6

7

Direct

SHAP

e
g
a
t
n
e
c
r
e
P

0.8

0.6

0.4

0.2

0

1

2

3
5
4
Features

6

7

e
g
a
t
n
e
c
r
e
P

0.8

0.6

0.4

0.2

0

1

2

3
5
4
Features

6

7

(b) The percentages of features serving as explanations suggested by Direct and Shapley over the Titantic data set

k=1

11%

k=2

5%

k=3

k=4

12%

k=5
1%

5%

52%

18%

35%

29%

89%

95%

48%

53%

47%

1 same feature

2 same features

3 same features

4 same featreus

5 same features

No same features

(c) The percentages of the same explanations suggested by Direct and Shapley over the Mushroom data set

e
g
a
t
n
e
c
r
e
P

1

0.8

0.6

0.4

0.2

0

k=1

1 2 3 4 5 6 7 8 9 10 11
Features

e
g
a
t
n
e
c
r
e
P

1

0.8

0.6

0.4

0.2

0

k=2

1 2 3 4 5 6 7 8 9 10 11
Features

e
g
a
t
n
e
c
r
e
P

1

0.8

0.6

0.4

0.2

0

k=3

1 2 3 4 5 6 7 8 9 10 11
Features

Direct

SHAP

e
g
a
t
n
e
c
r
e
P

1

0.8

0.6

0.4

0.2

0

k=4

1 2 3 4 5 6 7 8 9 10 11
Features

e
g
a
t
n
e
c
r
e
P

1

0.8

0.6

0.4

0.2

0

k=5

1 2 3 4 5 6 7 8 9 10 11
Features

(d) The percentages of the same explanations suggested by Direct and Shapley over the Mushroom data set

Figure 3: Explanation results comparison.

classiﬁcation. Table 4 shows that although our approach (Di-
rect) and SHAP both can identify part of the seed string from
each query instances, hence computing correct explanations,
ours gives higher accuracy across the board.

To demonstrate the effect of knowledge incorporation, we
gradually add clauses drawn from sub-strings derived from
the seed to the KB. The result is shown in Figure 4(a). Tested
on the data set with string length 10, size of alphabet 4
with the Tree algorithm, we see that the classiﬁcation per-
formance improves as the number of true clauses inserted
grows. To show that the knowledge incorporation is resilient
to pollution, we insert clauses of a random length between
1 and 10 with a random probability to pollute the KB. As

shown in Figure 4(b), for the same data set, the classiﬁcation
performance deteriorates gradually as the number of random
clauses grows.

Related Work
programming,

or

logic

Probabilistic
ProbLog,
(Fierens et al. 2015) provides a means to do logic pro-
gramming with probabilities.
Our work differs from
ProbLog in several ways. (1) ProbLog develops Logic Pro-
gramming and uses grounded predicates with closed world
assumption to allow negations whereas we use propositional
clauses with classical negations; (2) ProbLog uses Sato’s
distribution semantics and assumes all atomic variables,

0.76

0.74

0.72

0.7

e
c
n
a
m
r
o
f
r
e
P

0.68

20

40

60
Number of True Clauses
(a)
F1

80

100

0.7

0.6

0.5

e
c
n
a
m
r
o
f
r
e
P

0.4

20

40

60
Number of Random Clauses

80

100

Precision

Recall

(b)

Figure 4: The plot on the left / right side shows classiﬁcation results
from KBs with true / random clauses inserted.

the variables not derived with Logic Programming, being
independent whereas we use Nilsson’s probabilistic logic
semantics and make no independence assumption.
(3)
ProbLog performs inference with weighted model counting,
which is then solved with MAX-SAT, an NP-hard problem,
whereas we use linear programming, which is polynomial.

Performing probabilistic logic inference with math-
ematical programming has been studied recently in
(Henderson et al. 2020) with its NonlInear Probabilistic
Logic Solver (NILS) approach. Although in both works
clauses with associated probabilities are turned into systems
of equations, the two approaches differ signiﬁcantly. NILS
either assumes independence amongst its variables or ex-
pand probability of conjunctions as the product of the prob-
ability of a literal and some conditionals. Thus NILS pro-
duces non-linear systems and rely on gradient descent meth-
ods for ﬁnding solutions. Consequently, NILS is unsuitable
for classiﬁcation as the independence assumption does not
hold between the class labels and feature values or, in gen-
eral, values across different features. When independence
cannot be assumed, systems constructed with NILS con-
tains kth order equations with 2k − 1 unknowns for each
k literal clauses. Such high order equations with high num-
ber unknowns are difﬁcult to solve numerically. Comparing
with NILS, the construction given in Deﬁnition 4 “hides”
the complexity introduced by conditionals in NILS with in-
equalities and ensures polynomial complexity. Moreover,
NILS does not tolerate inconsistency whereas our approach
does.

More generally, developing intelligent system based on
reasoning with KB has been explored in the past, see
e.g., (McCarthy 1968; Nilsson 1991). Some of the early
works on learning KB from data use classical logic, e.g.,
(Khardon and Roth 1994) or default logic (Roth 1996). A
comprehensive review on combining logic and probability
is beyond the scope of this section. For broader discussions
on this topic, see e.g., (Bacchus 1990) for probabilistic logic,
(Chavira and Darwiche 2008) for weighted model counting,
and (Gogate and Domingos 2016) for probabilistic graphi-
cal models with logical structures. The problem of testing
a KB’s consistency is known as the probabilistic satisﬁa-
bility (PSAT) problem. Works dedicated to solving PSAT
include (Cozman and di Ianni 2015; Finger and Bona 2011;
Georgakopoulos et al. 1988). Since most of these compute
exact solutions over consistent KBs by solving an NP prob-
lem, they are not suitable for classiﬁcation.

In explainable machine learning,

there has been sig-
in providing explanations for classi-

niﬁcant

interest

ﬁers; see e.g., (Biran and Cotton 2017) for an overview.
Works have been proposed to use simpler thus weaker
classiﬁers to explain results from stronger ones, e.g.,
(F´eraud and Cl´erot 2002). Recent works on model-agnostic
explainers (Ribeiro et al. 2016; Ribeiro et al. 2018) focus
on adding explanations to existing (black-box) classiﬁers.
(Alonso et al. 2018) use KB based classiﬁers to explain
results obtained from MLP and random forests. LIME
(Ribeiro et al. 2016) augment the data with randomly gen-
erated samples close to the instance to be explained and
then construct a simple thus explainable classiﬁer to gen-
erate explanations. (Robnik- ˇSikonja and Kononenko 2008)
works by decomposing a model’s predictions based on in-
(Shih et al. 2018)
dividual contributions of each feature.
explains Bayesian network classiﬁers by compiling naive
Bayes and latent-tree classiﬁers into Ordered Decision Dia-
grams. (Lundberg et al. 2020) provides explanations for de-
cision trees based on the game-theoretic Shapley values.

(Berrar et al. 2019),

(Sachan et al. 2018)

and
(Vo et al. 2017) are some recent work on incorporat-
ing knowledge into machine learning. (Yu 2007) contains
a survey, categorising methods into four groups based on
use of knowledge: (1) to prepare training samples, (2) to
initialise the hypothesis or hypothesis space, (3) to alter the
search objective and (4) to augment the search process. Our
approach fundamentally differs from those as we represent
knowledge in the same format as the model learned from
data and reason with both uniformly.

Conclusion

We present a non-parametric classiﬁcation technique that
gives explanations to its predictions and supports knowledge
incorporation. Our approach is based on approximating lit-
eral probabilities in probabilistic logic by solving linear sys-
tems corresponding to KBs, which are either directly learned
from data or augmented with additional knowledge. Our
linear program construction is efﬁcient and our approaches
tolerate inconsistency in a KB. As a stand-alone classiﬁer,
our approach matches or exceeds the performance of exist-
ing algorithms on both synthetic and non-synthetic data sets.
At the same time, our approaches generate explanations in
the form of “most decisive” features. Upon comparing with
a state of the art Shapley Value based explanation method,
SHAP, our approach ﬁnds similar explanation as SHAP on
real data sets. On four synthetic data sets with known ex-
planation ground truth, our approach is shown to be supe-
rior as it achieves higher accuracy. Overall, we envisage our
approaches to be most useful for classiﬁcation tasks where
there exists knowledge to complement data and explanations
are required to ensure usability.

There are four research directions that we plan to explore.
Firstly, this work focuses on developing the underlying ex-
plainable classiﬁcation techniques. We will apply tech-
niques developed practical applications and perform user
studies in the future. Secondly, we will study semantics for
inconsistent KBs. Thirdly, we will study richer explanation
generation with with (probabilistic) logic inference. Lastly,
we would like to develop other suitable representations for

knowledge incorporation.

References

Jose Alonso, Alejandro Ramos Soto, Ciro Castiello, and
Corrado Mencar. Hybrid data-expert explainable beer style
classiﬁer. In Proc. of IJCAI-17 Workshop on Explainable AI,
2018.
Daniel W. Apley and Jingyu Zhu. Visualizing the effects of
predictor variables in black box supervised learning models,
2016.
F. Bacchus. Representing and Reasoning with Probabilistic
Knowledge. MIT Press, Cambridge, MA, 1990.
Daniel Berrar, Philippe Lopes, and Werner Dubitzky. Incor-
porating domain knowledge in machine learning for soccer
outcome prediction. Machine Learning, 108(1):97–126, Jan
2019.
Or Biran and Courtenay V. Cotton. Explanation and justiﬁ-
cation in machine learning : A survey. In Proc. of IJCAI-17
Workshop on Explainable AI, 2017.
G. Casella and R.L. Berger. Statistical Inference. Duxbury
advanced series in statistics and decision sciences. Thomson
Learning, 2002.
Mark Chavira and Adnan Darwiche. On probabilistic in-
ference by weighted model counting. Artif. Intell., 172(6-
7):772–799, 2008.
Ding-An Chiang, Wei Chen, Yi-Fan Wang, and Lain-Jinn
Hwang. Rules generation from the decision tree. J. Inf. Sci.
Eng., 17(2):325–339, 2001.
Fabio G. Cozman and Lucas Fargoni di Ianni. Probabilistic
satisﬁability and coherence checking through integer pro-
gramming. International Journal of Approximate Reason-
ing, 58:57 – 70, 2015. Special Issue of the Twelfth Euro-
pean Conference on Symbolic and Quantitative Approaches
to Reasoning with Uncertainty (ECSQARU 2013).
D. Doran, S. Schulz, and T. R. Besold. What does explain-
able AI really mean? A new conceptualization of perspec-
tives. CoRR, abs/1710.00794, 2017.
Dheeru Dua and Eﬁ Karra Taniskidou. UCI machine learn-
ing repository, 2017.
Daan Fierens, Guy Van den Broeck, Joris Renkens, Dim-
itar Sht. Shterionov, Bernd Gutmann, Ingo Thon, Gerda
Janssens, and Luc De Raedt. Inference and learning in prob-
abilistic logic programs using weighted boolean formulas.
TPLP, 15(3):358–401, 2015.
Marcelo Finger and Glauber De Bona. Probabilistic satis-
ﬁability: Logic-based algorithms and phase transition.
In
IJCAI 2011, Proceedings of the 22nd International Joint
Conference on Artiﬁcial Intelligence, Barcelona, Catalonia,
Spain, July 16-22, 2011, pages 528–533, 2011.
A. Fisher, C. Rudin, and F Dominici. All Models are Wrong
but many are Useful: Variable Importance for Black-Box,
Proprietary, or Misspeciﬁed Prediction Models, using Model
Class Reliance. arXiv e-prints, page arXiv:1801.01489, Jan
2018.

Raphael F´eraud and Fabrice Cl´erot. A methodology to
explain neural network classiﬁcation. Neural Networks,
15(2):237 – 246, 2002.
George Georgakopoulos, Dimitris Kavvadias, and Chris-
tos H Papadimitriou. Probabilistic satisﬁability. Journal of
Complexity, 4(1):1 – 11, 1988.
Vibhav Gogate and Pedro M. Domingos. Probabilistic theo-
rem proving. Commun. ACM, 59(7):107–115, 2016.
T.C. Henderson, R. Simmons, B. Serbinowski, M. Cline,
D. Sacharny, X. Fan, and A. Mitiche. Probabilistic sentence
satisﬁability: An approach to psat. Artiﬁcial Intelligence,
278:103199, 2020.
Roni Khardon and Dan Roth. Learning to reason. In Pro-
ceedings of the 12th National Conference on Artiﬁcial Intel-
ligence, Seattle, WA, USA, July 31 - August 4, 1994, Volume
1., pages 682–687, 1994.
Scott M. Lundberg and Su-In Lee. A uniﬁed approach to in-
terpreting model predictions. In Proc. of NIPS, pages 4768–
4777, 2017.
Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex De-
Grave, Jordan M. Prukin, Bala Nair, Bonit Katz, Jonathan
Himmelfarb, Nisha Bansal, and Su-In Lee. From local ex-
planations to golbal understanding with explainable ai for
trees. Nature machine intelligence, 2(1):56–67, 2020.
Morteza Mashayekhi and Robin Gras. Rule extraction from
decision trees ensembles: New algorithms based on heuris-
International
tic search and sparse group lasso methods.
Journal of Information Technology & Decision Making,
16(06):1707–1727, 2017.
John McCarthy. Programs with common sense. In Semantic
Information Processing, pages 403–418. MIT Press, 1968.
Interpretable Machine Learning, A
Christoph Molnar.
Guide for Making Black Box Models Explainable. 2019.
https://christophm.github.io/interpretable-ml-book/.
Nils J. Nilsson. Probabilistic logic. Artiﬁcial Intelligence,
28(1):71–87, 1986.
N. Nilsson. Logic and artiﬁcial intelligence. Artif. Intell.,
47(1-3):31–56, 1991.
J. R. Quinlan. Generating production rules from decision
trees. In Proc of IJCAI, pages 304–307, 1987.
Marco T´ulio Ribeiro, Sameer Singh, and Carlos Guestrin.
”why should I trust you?”: Explaining the predictions of any
classiﬁer. In Proc. of SIGKDD, pages 1135–1144, 2016.
Marco T´ulio Ribeiro, Sameer Singh, and Carlos Guestrin.
Anchors: High-precision model-agnostic explanations.
In
Proc of AAAI-18, pages 1527–1535, 2018.
M. Robnik- ˇSikonja and I. Kononenko. Explaining classiﬁca-
tions for individual instances. IEEE Transactions on Knowl-
edge and Data Engineering, 20(5):589–600, May 2008.
Dan Roth. Learning in order to reason: The approach. In
SOFSEM ’96: Theory and Practice of Informatics, 23rd
Seminar on Current Trends in Theory and Practice of In-
formatics, Milovy, Czech Republic, November 23-30, 1996,
Proceedings, pages 113–124, 1996.

Cynthia Rudin. Stop explaining black box machine learning
models for high stakes decisions and use interpretable mod-
els instead. Nature Machine Intelligence, 1:206–215, May
2019.
Mrinmaya Sachan, Kumar Avinava Dubey, Tom M.
Mitchell, Dan Roth, and Eric P. Xing. Learning pipelines
with limited data and domain knowledge: A study in pars-
In Proc. of NIPS, pages 140–151,
ing physics problems.
2018.
Andy Shih, Arthur Choi, and Adnan Darwiche. A sym-
bolic approach to explaining bayesian network classiﬁers.
In Proc. of IJCAI, pages 5103–5111, 2018.
Khuong Vo, Dang Pham, Mao Nguyen, Trung Mai, and Tho
Quan. Combination of domain knowledge and deep learning
for sentiment analysis. In Somnuk Phon-Amnuaisuk, Swee-
Peng Ang, and Soo-Young Lee, editors, Multi-disciplinary
Trends in Artiﬁcial Intelligence, pages 162–173, Cham,
2017. Springer International Publishing.
Erik ˇStrumbelj and Igor Kononenko. Explaining predic-
tion models and individual predictions with feature contribu-
tions. Knowledge and Information System, 41(3):647–665,
December 2014.
H. Yang, C. Rudin, and M. Seltzer. Scalable bayesian rule
lists. In Proc. of ICML, pages 3921–3930, 2017.
Kristijonas ˇCyras, David Birch, Yike Guo, Francesca Toni,
Rajvinder Dulay, Sally Turvey, Daniel Greenberg, and
Tharindi Hapuarachchi.
Explanations by arbitrated ar-
gumentative dispute. Expert Systems with Applications,
127:141 – 156, 2019.
Ting Yu. Incorporating Prior Domain Knowledge into In-
ductive Machine Learning. PhD thesis, University of Tech-
nology Sydney, Sydney, Australia, 2007.
Qingyuan Zhao and Trevor Hastie. Causal interpretations of
black-box models. Journal of Business & Economic Statis-
tics, 0(0):1–10, 2019.

