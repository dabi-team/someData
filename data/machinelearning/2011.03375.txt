0
2
0
2

v
o
N
6

]

G
L
.
s
c
[

1
v
5
7
3
3
0
.
1
1
0
2
:
v
i
X
r
a

A Scalable MIP-based Method for Learning
Optimal Multivariate Decision Trees

Haoran Zhu, Pavankumar Murali, Dzung T. Phan, Lam M. Nguyen, Jayant R. Kalagnanam
IBM Research, Thomas J. Watson Research Center, Yorktown Heights, NY 10598, USA
haoran@ibm.com, pavanm@us.ibm.com, phandu@us.ibm.com,
LamNguyen.MLTD@ibm.com, jayant@us.ibm.com

Abstract

Several recent publications report advances in training optimal decision trees
(ODT) using mixed-integer programs (MIP), due to algorithmic advances in integer
programming and a growing interest in addressing the inherent suboptimality of
heuristic approaches such as CART. In this paper, we propose a novel MIP formu-
lation, based on a 1-norm support vector machine model, to train a multivariate
ODT for classiﬁcation problems. We provide cutting plane techniques that tighten
the linear relaxation of the MIP formulation, in order to improve run times to reach
optimality. Using 36 data-sets from the University of California Irvine Machine
Learning Repository, we demonstrate that our formulation outperforms its counter-
parts in the literature by an average of about 10% in terms of mean out-of-sample
testing accuracy across the data-sets. We provide a scalable framework to train
multivariate ODT on large data-sets by introducing a novel linear programming
(LP) based data selection method to choose a subset of the data for training. Our
method is able to routinely handle large data-sets with more than 7,000 sample
points and outperform heuristics methods and other MIP based techniques. We
present results on data-sets containing up to 245,000 samples. Existing MIP-based
methods do not scale well on training data-sets beyond 5,500 samples.

1

Introduction

Decision tree models have been used extensively in machine learning (ML), mainly due to their
transparency which allows users to derive interpretation on the results. Standard heuristics, such as
CART [Breiman et al., 1984], ID3 [Quinlan, 1986] and C4.5 [Quinlan, 1996], help balance gains in
accuracy with training times for large-scale problems. However, these greedy top-down approaches
determine the split at each node one-at-a-time without considering future splits at subsequent nodes.
This means that splits at nodes further down in the tree might affect generalizability due to weak
performance. Pruning is typically employed to address this issue, but this means that the training
happens in two steps – ﬁrst, top-down training and then pruning to identify better (stronger) splits. A
better approach would be a one-shot training of the entire tree that determines splits at each node
with full knowledge of all future splits, while optimizing, say, the misclassiﬁcation rate [Bertsimas
and Dunn, 2017]. The obtained decision tree is usually referred to as an optimal decision tree (ODT)
in the literature.

The discrete nature of decisions involved in training a decision tree has inspired researchers in the ﬁeld
of Operations Research to encode the process using a mixed-integer programming (MIP) framework
[Bessiere et al., 2009, Günlük et al., 2018, Rhuggenaath et al., 2018, Verwer and Zhang, 2017, 2019].
This has been further motivated by algorithmic advances in integer optimization [Bertsimas and
Dunn, 2017]. An MIP-based ODT training method is able to learn the entire decision tree in a single
step, allowing each branching rule to be determined with full knowledge of all the remaining rules.

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
Papers on ODT have used optimality criteria such as the average testing length [Laurent and Rivest,
1976], training accuracy, the combination of training accuracy and model interpretability [Bertsimas
and Dunn, 2017, Hu et al., 2019], and the combination of training accuracy and fairness [Aghaei
et al., 2019]. In a recent paper, Aghaei et al. [Aghaei et al., 2020] propose a strong max-ﬂow based
MIP formulation to train univariate ODT for binary classiﬁcation. The ﬂexibility given by the choice
of different objective functions, as well as linear constraints in the MIP model, also allows us to train
optimal decision trees under various optimality criteria.

Yet another drawback of heuristic approaches is the difﬁculty in training multivariate (or oblique)
decision trees, wherein splits at a node use multiple variables, or hyperplanes. While multivariate
splits are much stronger than univariate (or axis-parallel) splits, they are more complicated because
the splitting decision at each node cannot be enumerated. Many approaches have been proposed to
train multivariate decision trees including the use of support vector machine (SVM) [Bennett and
Blue, 1998], logistic regression [Truong, 2009] and Householder transformation [Wickramarachchi
et al., 2016]. As noted in [Bertsimas and Dunn, 2017], these approaches do not perform well on
large-scale data-sets as they also rely on top-down induction for the training process.

The ﬁrst contribution of this paper is a new MIP formulation, SVM1-ODT, for training multivariate
decision trees. Our formulation differs from others in the literature – we use a 1-norm SVM to
maximize the number of correctly classiﬁed instances and to maximize the margin between clusters at
the leaf nodes. We show that this formulation produces ODTs with a higher out-of-sample accuracy
compared to the ODTs trained from state-of-the-art MIP models and heuristic methods on 20 data-sets
selected from the UCI ML repository. In Sect. 4, we report the testing performance of the ODT trained
using our formulation and show that is has an average improvement of 6-10% in mean out-of-sample
accuracy for a decision tree of depth 2, and an average improvement of 17-20% for a decision tree of
depth 3.

The second contribution of this paper is towards increasing tractability of the MIP-based ODT training
for very large data-sets that are typical in real-world applications. It is imperative to bear in mind
that the tractability of MIPs limits the size of the training data that can be used. Prior MIP-based
ODT training formulations [Bertsimas and Dunn, 2017, Verwer and Zhang, 2019] are intractable for
large-sized data-sets (more than 5000 samples) since the number of variables and constraints increase
linearly with the size of the training data. We address tractability in two steps. First, we tighten the
LP-relaxation of SVM1-ODT by providing new cutting planes and getting rid of the big-M constant.
Second, we propose an efﬁcient linear programming (LP) -based data-selection method to be used
prior to training the tree. This step is comprised of selecting a subset of data points that maximizes
the information captured from the entire data-set.

Our method is able to routinely handle large data-sets with more than 7,000 sample points. We
present results on data-sets containing up to 245,000 samples. Existing MIP-based methods do not
scale well on training data beyond 5,000 samples, and do not provide a signiﬁcant improvement over
a heuristic approach. For large-scale data-sets, when SVM1-ODT is used along with the LP-based
data selection method, our results indicate that the resulting decision trees offer higher training and
testing accuracy, compared to CART (see Sect. 4, Figure 2). However, solely using any MIP-based
formulation (including ours) without data selection can rarely outperform CART, due to the model
becoming intractable resulting in the MIP solver failing to ﬁnd any better feasible solution than the
initially provided warm-start solution. This indicates that any loss of information from data-selection
is more than adequately compensated by the use of optimal decision trees (using the SVM1-ODT
formulation).

2 MIP formulation for training multivariate ODT for classiﬁcation

In this section, we present our formulation to train an optimal multivariate classiﬁcation tree using a
data-set comprising numerical features, and for general data-sets containing categorical features, we
propose an extension of such formulation in the supplementary material. For any n ∈ Z+, let [n] :=
{1, 2, . . . , n} denote a ﬁnite set of data points, [Y ] = {1, 2, . . . , Y } be a set of class labels, and
[d] = {1, 2, . . . , d} be the index set of all features. Our formulation is established for the balanced
binary tree with depth D. Let the set of branch nodes of the tree be denoted by B := {1, . . . , 2D − 1},
and the set of leaf nodes be denoted by L := {2D, . . . , 2D+1 − 1}. Similar to [Bertsimas and Dunn,

2

2017], let AR(l) and AL(l) denote the sets of ancestors of leaf node l whose right and left branches,
respectively, are on the path from the root node to leaf node l.

Next, we deﬁne the variables to be used. Each data point i ∈ [n] is denoted by (xi, yi), where xi is a
d-dimensional vector, and yi ∈ [Y ]. Since we train multivariate trees, we use a branching hyperplane
at branch node b, denoted by (cid:104)hb, xi(cid:105) = gb, where gb is the bias term in the hyperplane. Indicator
binary variable ci = 1 when i is misclassiﬁed and 0 otherwise. Indicator binary variable eil = 1
when i enters leaf node l. Variable ˆyi ∈ [Y ] denotes the predicted label for i, and the decision
variable ul ∈ [Y ] is the label assigned to leaf node l. We let mib denote the slack variable for the
soft margin for each point i corresponding to a hyperplane (cid:104)hb, xi(cid:105) = gb used in the SVM-type
model (3). The objective for the learning problem shown in (1) attempts to minimize the total
misclassiﬁcation ((cid:80)
b (cid:107)hb(cid:107)1) and the sum of slack variables for
classiﬁcation ambiguity subtracted from the soft margin ((cid:80)
b (cid:107)hb(cid:107)1 helps
promote sparsity in the decision hyperplanes constructed at the branch nodes of a decision tree during
the training process.

i ci), the 1-norm SVM margin ((cid:80)

i,b mib). Additionally, (cid:80)

Then, SVM1-ODT can be expressed as follows:

min

(cid:88)

i∈[n]

ci + α1

(cid:88)

mib + α2

i∈[n],b∈B

(cid:88)

b∈B

(cid:107)hb(cid:107)1

s.t. (yi − Y )ci ≤ yi − ˆyi ≤ (yi − 1)ci, ∀i ∈ [n]

ˆyi =

(cid:88)

l∈L

wil, ∀i ∈ [n]

wil ≥ eil, ul − wil + eil ≥ 1, ∀i ∈ [n], l ∈ L
Y eil + ul − wil ≤ Y, wil ≤ Y eil, ∀i ∈ [n], l ∈ L
ib − p−

ib, ∀i ∈ [n], b ∈ B

hbjxij = p+

gb −

(cid:88)

j∈[d]

p+
ib ≤ M (1 − eil), ∀i ∈ [n], l ∈ L, b ∈ AR(l)
p−
ib + mib ≥ (cid:15)eil, ∀i ∈ [n], l ∈ L, b ∈ AR(l)
p−
ib ≤ M (1 − eil), ∀i ∈ [n], l ∈ L, b ∈ AL(l)
p+
ib + mib ≥ (cid:15)eil, ∀i ∈ [n], l ∈ L, b ∈ AL(l)
(cid:88)

eil = 1, ∀i ∈ [n]

l∈L
ˆyi, wil, hbj, gb ∈ R, p+

ib, p−

ib, mib ∈ R+, eil, ci ∈ {0, 1}, ul ∈ {1, . . . , Y }.

(1a)

(1b)

(1c)

(1d)
(1e)

(1f)

(1g)

(1h)

(1i)

(1j)

(1k)

(1l)

ib, mib, eil and ci are derived variables for the MIP model.

We notice that hb, gb and ul are main decision variables to characterize a decision tree, while ˆyi, wil,
ib, p−
p+
Constraint (1b) expresses the relationship between ci and ˆyi. If ˆyi = yi, then ci = 0 since ci is
minimized. If ˆyi (cid:54)= yi and ˆyi < yi, then (yi − 1)ci ≥ yi − ˆyi ≥ 1, thus ci = 1, Similarly, if ˆyi > yi,
then (yi − Y )ci ≤ −1, thus ci = 1. In a balanced binary tree with a ﬁxed depth, the branching rule is
given by: i goes to the left branch of b if (cid:104)hb, xi(cid:105) ≤ gb, and goes to the right side otherwise. Predicted
class ˆyi = (cid:80)
l∈L ul · eil. Since ul · eil is bilinear, we perform McCormick relaxation [McCormick,
1976] of this term using an additional variable wil such that wil = ul · eil. Since eil is binary, this
McCormick relaxation is exact. That is to say, ˆyi = (cid:80)
l∈L ul ·eil if and only if (1c)-(1e) hold for some
extra variables w. Since ul and eil are integer variables, it follows that ˆyi also integral. Constraints
(1f), (1g) and (1i) formulate the branching rule at each node b ∈ B: if i goes to the left branch at
node b, then gb − (cid:80)
j∈[d] hbjxij ≤ 0.
As per MIP convention, we formulate this relationship by separating gb − (cid:80)
j∈[d] hbjxij into a pair
of complementary variables p+
ib cannot both be strictly positive at the
same time), and forcing one of these two variables to be 0 through the big-M method [Wolsey, 1998].
We should remark that this is not exactly the same as our branching rule: when i goes to the right
branch, it should satisfy gb − (cid:80)
ib = 0.
However, due to the two constraints (1h), (1j), and the penalizing term mib ≥ 0, this phenomenon
cannot occur. Constraint (1k) enforces that each i should be assigned to exactly one leaf node. For a

j∈[d] hbjxij ≥ 0, and if it goes to the right side, then gb − (cid:80)

j∈[d] hbjxij < 0 strictly. The only special case is when p+

ib (meaning p+

ib and p−

ib and p−

ib = p−

3

given dataset, model parameters (cid:15), α1 and α2 are tuned via cross-validation. In the following section,
we provide explanations for constraints (1h), (1j) and objective function (1a), and some strategies to
tighten SVM1-ODT.

2.1 Multi-hyperplane SVM model for ODT

When constructing a branching hyperplane, we normally want to maximize the shortest distance
from this hyperplane to its closest data points. For any branching node associated with a hyperplane
(cid:104)hb, xi(cid:105) = gb by ﬁxing other parts of the tree, we can view the process of learning the hyperplane as
constructing a binary classiﬁer over data points {(xi, yi)} that reach the node. The artiﬁcial label
yi ∈ {left, right} (≡ {−1, +1}) is derived from the child of the node: xi goes to the left or the right;
that is determined by eil. This problem is reduced to an SVM problem for each branching node.
Applying the 1-norm SVM model with soft margin [Olvi and Fung, 2000, Zhu et al., 2004] to the
node b gives us

min
s.t.

1
2 (cid:107)hb(cid:107)1 + α1
|gb − (cid:80)

(cid:80)

i∈[n] mib

j∈[d] hbjxij| + mib ≥ (cid:15)eil, ∀i ∈ [n], l ∈ Lb.

(2)

Here Lb is the set of leaf nodes that have node b as an ancestor and mib denotes the slack variable for
soft margin. We slightly modify the original 1-norm SVM model by using a small constant (cid:15)eil in (2)
instead of eil to prevent variables from getting too big. The constraint is only active when eil = 1,
namely data i enters the branching node b, and eil = 1 implicitly encodes yi.
We use 1-norm, instead of the Euclidean norm, primarily because it can be linearized. The sparsity
for hb targeted heuristically by including the 1-norm term [Zhu et al., 2004] in the objective allows
for feature selection at each branching node. As we noted previously, we can express the term
gb − (cid:80)
j∈[d] hbjxij as the difference of two positive complementary variables, and the absolute
value |gb − (cid:80)
j∈[d] hbjxij| just equals one of these two complementary variables, depending on
which branch such a data point enters. When taking the sum over all branching nodes, we have the
following multi-hyperplane SVM problem to force data points close to the center of the corresponding
sub-region at each leaf node l ∈ L:

min (cid:80)
s.t.

b∈AL(l)∪AR(l)

|gb − (cid:80)
∀i ∈ [n], b ∈ AL(l) ∪ AR(l).

j∈[d] hbjxij| + mib ≥ (cid:15)eil,

1
2 (cid:107)hb(cid:107)1 + α1

(cid:80)

i∈[n],b∈AL(l)∪AR(l) mib

Note that (cid:83)

l∈L(AL(l) ∪ AR(l)) = B. By combining (3) over all leaf nodes l ∈ L we obtain:

min (cid:80)
s.t.

b∈B
(1f) − (1j).

1
2 (cid:107)hb(cid:107)1 + α1

(cid:80)

i∈[n],b∈B mib

(3)

(4)

bj, h+

bj, h−

bj − h−

bj + h−

bj = h+

i ci back into the objective function, and assigning some regular-

Adding the misclassiﬁcation term (cid:80)
ization parameters, we end up getting the desired objective function in (1a).
In LP, the minimized absolute value term |hbj| can be easily formulated as an extra variable h(cid:48)
bj and
bj, hbj = h+
two linear constraints: h(cid:48)
bj ≥ 0. We highlight the major
difference here from another recent work [Bertsimas and Dunn, 2017] in using MIP to train ODT.
First, in their formulation (OCT-H), they consider penalizing the number of variables used across all
branch nodes in the tree, in order to encourage model simplicity. Namely, their minimized objective
is: (cid:80)
b (cid:107)hb(cid:107)0. However, this requires some additional binary variables for each hbj,
which makes the MIP model even harder to solve. In fact, there is empirical evidence that using
the 1-norm helps with model sparsity (e.g., LASSO [Tibshirani, 1996], 1-norm SVM [Zhu et al.,
2004]. For that reason, we do not bother adding another 0-norm regularization term into our objective.
Secondly, unlike in [Bertsimas and Dunn, 2017], we use variables ul to denote the assigned label
on each leaf, where ul ∈ [Y ]. The next theorem shows, in SVM1-ODT (1), integer u variables can
be equivalently relaxed to be continuous between [1, Y ]. This relaxation is important in terms of
optimization tractability.
Theorem 1. Every integer variable ul, l ∈ L, in (1) can be relaxed to be continuous in [1, Y ].

i ci + α1

(cid:80)

We note that all proofs are delegated to the supplementary material.

4

2.2 Strategies to tighten MIP formulation

For MIPs, it is well-known that avoiding the use of a big-M constant can enable us to obtain a tighter
linear relaxation of the formulation, which helps an optimization algorithm such as branch-and-bound
converge faster to a global solution. The next theorem shows that the big-M constant M can be
ﬁxed to be 1 in (1) for SVM1-ODT, with the idea being to disperse the numerical issues between
parameters by re-scaling. The numerical instability for a very small (cid:15), as in the theorem below, should
be easier to handle by an MIP solver than a very big M .
(h(cid:48)
b, g(cid:48)
solution

Theorem 2. Let
be
also an optimal
(h(cid:48)
is an optimal solution to (1) with parameters (M α1, M α2, 1, (cid:15)/M ).

l)l∈L, (ˆy(cid:48)
i)i∈[n], (w(cid:48)
parameters
(1) with
solution to (1) with parameters
l)l∈L, (ˆy(cid:48)

ib)i∈[n],b∈B
is
it
Then,
if and only
if
ib/M )i∈[n],b∈B

il)i∈[n],l∈L, (p+(cid:48)
(α1, α2, M, (cid:15)).
(α1, α2, M, (cid:15)),
ib /M, p−(cid:48)

il)i∈[n],l∈L, (p+(cid:48)

b/M )b∈B, (u(cid:48)

i)i∈[n], (w(cid:48)

ib /M, m(cid:48)

b)b∈B, (u(cid:48)

b/M, g(cid:48)

ib , p−(cid:48)

ib , m(cid:48)

feasible

il, e(cid:48)

il, e(cid:48)

i, c(cid:48)

i, c(cid:48)

to

a

Note that {(h(cid:48)
since (cid:104)h(cid:48)

b, g(cid:48)
b, h(cid:105) ≤ g(cid:48)

l)l∈L} and {(h(cid:48)

b)b∈B, (u(cid:48)
b is equivalent to (cid:104)h(cid:48)

b/M, g(cid:48)
b/M, x(cid:105) ≤ g(cid:48)

b/M )b∈B, (u(cid:48)

l)l∈L} represent the same decision tree,

b/M , at each branch node b ∈ B.

In MIP parlance, a cutting-plane (also called a cut) is a linear constraint that is not part of the original
model and does not eliminate any feasible integer solutions. Pioneered by Gomory [Gomory, 1960,
2010], cutting-plane methods, as well as branch-and-cut methods, are among the most successful
techniques for solving MIP problems in practice. Numerous types of cutting-planes have been
studied in integer programming literature and several of them are incorporated in commercial solvers
(see, e.g., [Cornuéjols, 2008, Wolsey, 1998]). Even though the state-of-the-art MIP solvers can
automatically generate cutting-planes during the solving process, these cuts usually do not take the
speciﬁc structure of the model into consideration. Therefore, most commercial solvers allow the
inclusion of user cuts, which are added externally by users in order to further tighten the MIP model.
In this section, we propose a series of cuts for SVM1-ODT, and they are added once at the beginning
before invoking a MIP solver. For the ease of notation, we denote by Nk the set of data points with
the same dependent variable value k, i.e., Nk := {i ∈ [n] : yi = k}, for any k ∈ [Y ].
Theorem 3. Given a set I ⊆ [n] with |I ∩ Nk| ≤ 1 for any k ∈ [Y ]. Then for any L ⊆ L, the
inequality

(cid:88)

i∈I

ci ≥

(cid:88)

i∈I,l∈L

eil − |L|

(5)

is a valid cutting-plane for SVM1-ODT (1).

Here the index set I is composed by arbitrarily picking at most one data point from each class Nk.
Theorem 3 admits Ω(nY · 22D
) number of cuts. Next, we list cuts that will be added later as user cuts
for our numerical experiments. They all help provide lower bounds for the term (cid:80)
i∈[n] ci, which
also appear in the objective function of our SVM1-ODT formulation. Our cutting planes are added
once at the beginning before invoking a MIP solver.

Proposition 1. Let {|Nk| | k ∈ [Y ]} = {s1, . . . , sk} with s1 ≤ s2 ≤ . . . ≤ sY . Then the following
inequalities

1. ∀l ∈ L, (cid:80)

i∈[n] ci ≥ (cid:80)

i∈[n] eil − sY ;

2. ∀l ∈ L, (cid:80)

i∈[n](ci + eil) ≥ s1 · (Y − 2D + 1);

3. (cid:80)

i∈[n] ci ≥ s1 + . . . + sY −2D if Y > 2D.

are all valid cutting-planes for SVM1-ODT (1).

Note that the second inequality is only added when Y ≥ 2D, and the last lower bound inequality is
only added when Y > 2D. As trivial as the last lower bound inequality might seem, in some cases it
can be quite helpful. During the MIP solving process, when the current best objective value meets this
lower bound value, optimality can be guaranteed and the solver will terminate the branch-and-bound
process. Therefore, a tightness of the lower bound has a signiﬁcant impact on run time.

5

3 LP-based data-selection procedure

As mentioned previously, our main focus is to be able to train ODT over very large training data-sets.
For the purpose of scalability, we rely on a data-selection method prior to the actual training process
using SVM1-ODT.

The outline for our procedure is as follows: ﬁrst, we use a decision tree trained using a heuristic
(e.g., CART) as an initial solution. Next, data-selection is performed on clusters represented by the
data points with the same dependent values at each leaf node. Finally, we merge all the data subsets
selected from each cluster as the new training data, and use SVM1-ODT to train a classiﬁcation tree
on this data-set. In each cluster, our data-selection is motivated by the following simple heuristic:
suppose for a data subset I0 all the points in conv{xi | i ∈ I0} are correctly classiﬁed as label
y. Then, we can drop out all the data points that lie in the interior of conv{xi | i ∈ I0} from our
training set, since by assigning {xi | i ∈ I0} to the same leaf node and labeling it with y, we will
also correctly classify all the remaining data points inside their convex combination. With that in
mind, a data subset I0 is selected as per the following two criteria: (1) the points within the convex
hull conv (cid:0){xi | i ∈ I0}(cid:1) are as many as possible; and (2) |I0| is as small as possible. In each cluster
N ⊆ [n], the following 0-1 LP can be deﬁned to do data-selection:

min
s.t.

i(cid:54)=j λjixi ≤ (cid:15)(cid:48) · 1, ∀j ∈ N

fT a − gT b
−(cid:15)(cid:48) · 1 ≤ bjxj − (cid:80)
(cid:80)

i(cid:54)=j λji = bj, ∀j ∈ N
0 ≤ λji ≤ ai, ∀i (cid:54)= j ∈ N
aj + bj ≤ 1, ∀j ∈ N
aj, bj ∈ {0, 1}, ∀j ∈ N .

(6)

Here f, g are two parameter vectors with non-negative components. Data point xi is selected if ai = 1.
Data point xj is contained in the convex combination of selected data points if bj = 1. When (cid:15)(cid:48) = 0,
for any j ∈ N with bj = 1, the ﬁrst two constraints express xj as the convex combination of points
in {xi | λji > 0}. Here we introduce a small constant (cid:15)(cid:48) to allow some perturbation. The third
inequality 0 ≤ λji ≤ ai means we can only use selected data points, which are those with ai = 1,
to express other data points. The last constraint aj + bj ≤ 1 ensures that any selected data point
cannot be expressed as a convex combination of other selected data points. Depending on the choice
of f, g and (cid:15)(cid:48), we have many different variants of (6). In the next section, we describe one variant of
data-selection. We discuss balanced data-selection in the supplementary material.

3.1 Selecting approximal extreme points

We notice that the original 0-1 LP can be formulated to maximize the number of data points inside
the convex hull of selected data points by selecting f = 0 and g = 1. This special case of (6) is used
because choosing these values allows us to decompose it into N smaller LPs while maximizing the
points inside the convex hull. By projecting out variable a, the resulting 0-1 LP is equivalent to the
following LP, as shown by the next result:

max (cid:80)
s.t.

i∈N bi

−(cid:15)(cid:48) · 1 ≤ bjxj − (cid:80)
(cid:80)

i(cid:54)=j λji = bj, ∀j ∈ N

0 ≤ bj ≤ 1, ∀j ∈ N .

i(cid:54)=j λjixi ≤ (cid:15)(cid:48) · 1, ∀j ∈ N

(7)

We note that such an LP is decomposable: it can be decomposed into |N | many small LPs, each with
d + 2 constraints and |N | variables, and each can be solved in parallel.

Theorem 4. The following hold.

1) If (cid:15)(cid:48) = 0, then for any optimal solution (b, ¯λ) of (7), there exists λ s.t. (b, λ) is optimal

solution of (6) with f = 0, g = 1, and vice versa;

2) If (cid:15)(cid:48) > 0, then for any optimal solution (b, ¯λ) of (7), there exists λ s.t. ((cid:98)b(cid:99), λ) is an optimal
solution of (6) with f = 0, g = 1. Here, (cid:98)b(cid:99) is a vector with every component being (cid:98)bi(cid:99).

6

3.2 Data-selection algorithm

For each cluster N , let IN := {j ∈ N : bj = 1}. Note that among those extreme points in
{i ∈ N : ∃j ∈ IN , s.t. λji > 0}, some of them may be outliers, which will result in λji being a
small number for all j ∈ N . From the classic Carathéodory’s theorem in convex analysis, for any
point xj with j ∈ IN , it can be written as the convex combination of at most d + 1 extreme points.
Then in that case, there must exist i with λji ≥ 1
d+1 . Denote JN := {i ∈ N : ∃j ∈ IN , s.t. λji ≥
1
d+1 }, KN := N \ (IN ∪ JN ). One can show that the size of set JN is upper bounded by (d + 1)|IN |,
so if |IN | is small, |JN | is also relatively small. For those data points in KN , we use a simple
heuristic for data-selection inspired by support vector machines, wherein the most critical points for
training are those closest to the classiﬁcation hyperplane. That is, for some data-selection threshold
number N and the boundary hyperplanes h ∈ H (hyperplanes for each leaf node), we sort data point
i ∈ KN by minh∈H dist(xi, h), in an increasing order, and select the ﬁrst N points. This heuristic is
called hyperplane based data-selection and is presented in Algorithm 1 below. The LP in (7) above
is solved in the second step of Algorithm 1. When |IN | is relatively large, we can simply select all
the extreme points as the new training data. When |IN | is relatively small, we can select those points
with index in JN as the new training data. The remaining data points in KN are selected according
to the above hyperplane based data-selection heuristic.

Algorithm 1 LP-based data-selection in each cluster N

Given β1, β2 ∈ (0, 1), β2 < (d + 1)(1 − β1);
Solve LP (7) and obtain the optimal solution (b, ¯λ),
denote IN := {i ∈ N : bi = 1}, λ = T (¯λ),
JN := {i ∈ N : ∃j ∈ IN , s.t. λji ≥ 1
d+1 },
KN := N \ (IN ∪ JN );
if |IN |/|N | ≥ 1 − β1 then

Select N \ IN as training set;

else if |JN | > β2|N | then
Select JN as training set;

else

For KN , do Hyperplane Data-selection and pick the ﬁrst β2|N | − |JN | points, together with
JN , as the selected new training set.

end if

In Algorithm 1, T (¯λ) is the transformation we used in the proof of Theorem 4 to construct a feasible
solution for (6), and β1, β2 are some pre-given threshold parameters. For MIP solvers such as CPLEX,
the obtained optimal solution (b, ¯λ) is a vertex in the associated polyhedron. In that case, T (¯λ) = ¯λ.

For large data-sets, for (7), we observed that it took a considerable amount of time to import the
constraints into the LP solver and solve it. However, since LP (7) can be decomposed into |N |
number of much smaller LPs, the computational process can be accelerated dramatically.

4 Numerical experiments

We present results mainly from two types of numerical experiments to evaluate the performance of
our ODT training procedure: (1) benchmark the mean out-of-sample performance of the ODT trained
using SVM1-ODT on medium-sized data-sets (n ≤ 7000), w.r.t. its counterparts in literature; and
(2) benchmark the mean out-of-sample performance of the ODT trained using SVM1-ODT together
with our data-selection procedure on large-scale data-sets (7, 000 ≤ n ≤ 245, 000), w.r.t. CART and
OCT-H [Bertsimas and Dunn, 2017]. For benchmarking, we use data-sets from the UCI Machine
Learning Repository [Dheeru and Taniskidou, 2017].

Accuracy of multivariate ODT: We tested the accuracy of the ODT trained using our SVM1-ODT
against baseline methods CART and OCT-H [Bertsimas and Dunn, 2017].

We used the same training-testing split as in [Bertsimas and Dunn, 2017], which is 75% of the entire
data-set as training set, and the rest 25% as testing set, with 5-fold cross-validation. The time limit
was set to be 15 or 30 minutes for medium-sized data-sets, and for larger data-sets we increased

7

Table 1: Performance on data-sets with more than 4 classes using D = 3. The numbers inside the bracket ‘()’
for CART and OCT-H are the numerical results reported in [Bertsimas and Dunn, 2017].

tree depth D = 3

data-set
n
d
Y

Dermatology
358
34
6

Heart-disease
297
13
5

Image
210
19
7

testing accuracy (%)

S1O
CART
OCT-H
Fair

98.9
76.1(78.7)
82.6 (83.4)
86.4

training accuracy (%)

S1O
CART
OCT-H
Fair

100
80.7
89.4
100

65.3
55.6 (54.9)
56.2 (54.7)
47.2

85.7
57.1 (52.5)
59.4 (57.4)
63.3

90.2
68.0
81.9
92.3

100
57.1
82.7
100

it up to 4 hours to ensure that the solver could make sufﬁcient progress. Due to intractability of
MIP models and the loss in interpretability for deep decision trees we only train ODT for small tree
depths, similar to [Bertsimas and Dunn, 2017, Günlük et al., 2018, Verwer and Zhang, 2019]. With
the exception of Table 1, all results shown in this section and in the supplementary material are for
D = 2.

For the following numerical results, our SVM1-ODT formulation is abbreviated as “S1O”, “OCT-H”
is the MIP formulation to train multivariate ODT in [Bertsimas and Dunn, 2017], “Fair” is the MIP
formulation from [Aghaei et al., 2019] without the fairness term in objective, and “BinOCT” is from
[Verwer and Zhang, 2019]. We implemented all these MIP approaches in Python 3.6 and solved them
using CPLEX 12.9.0 [Cplex, 2009]. We invoked the DecisionTreeClassiﬁer implementation from
scikit-learn to train a decision tree using CART, using default parameter settings. For all methods, the
maximum tree depth was set to be the same as our SVM1-ODT.

In Figure (1a), we compare the mean out-of-sample accuracy of the ODT trained from several
different MIP formulations. Here the labels on the x-axis represent the names of the data-sets,
followed by the data size. In Figure (1b), we have more comparison over the counterparts in literature,
along with the BinOCT from [Verwer and Zhang, 2019]. Table 2 shows detailed results about the
ODT training, including the running time and training accuracy of different methods. Moreover,
we also tested a few data-sets with more than 4 classes using tree depth 3, with time limit being
30 minutes, the results are more exciting: For our tested 3 data-sets, we obtained 20.4 percentage
points average improvement over CART, 17.2 percentage points improvement over OCT-H, and 17.7
percentage points improvement over Fair. We list the detailed results in Table 1.

(a)

(b)

Figure 1: Accuracy comparison for multivariate ODT trained on medium-sized data-sets w.r.t (1a): OCT-H and
Fair ; (1b): OCT-H, Fair and BinOCT [Verwer and Zhang, 2019], D = 2. In-depth results in Tables 3 and 5 in
supplementary material.

8

Table 2: Accuracy and running time on medium-sized data-sets, for D=2. The numbers inside the bracket ‘()’
for CART and OCT-H are the numerical results reported in [Bertsimas and Dunn, 2017].
Congress Spectf-

data-set

Image

Iris

n
d
Y

150
4
3

testing accuracy (%)

S1O
CART

OCT-H

Fair

98.6
92.4
(92.4)
94.4
(95.1)
90.0

training accuracy (%)

S1O
CART
OCT-H
Fair

98.9
96.4
99.5
100

running time (s)

S1O
OCT-H
Fair

8.17
1.56
11.14

232
16
2

98.0
93.5
(98.6)
94.8
(98.6)
91.4

98.5
96.3
96.2
100

1.48
727.3
1.82

heart
80
44
2

83.3
72.0
(69.0)
75.0
(67.0)
57.0

85
88.0
92.5
100

0.42
900
0.23

Breast-
cancer
683
9
2

Heart-
disease
297
13
5

97.6
91.1
(92.3)
96.1
(97.0)
95.4

97.3
93.4
95.3
99.7

517.4
900
803.5

69.9
57.5
(54.1)
56.7
(54.7)
56.7

75.3
58.8
60.5
68.4

900
900
900

210
19
7

55.0
42.9
(38.9)
49.8
(49.1)
46.9

56.7
42.9
48.0
56.6

900
900
713.7

Hayes-
roth
132
4
3

71.9
55.8
(52.7)
61.2
(61.2)
72.2

76.8
60.0
77.4
82.8

900
900
900

Figure 2: Comparison for large data-sets, D=2. In-depth results in Table 7 in supplementary material.

Training multivariate ODT on large data-sets: We use SVM1-ODT together with the LP-based
data-selection (denoted by “S1O-DS") to train multivariate ODT on large data-sets, which has never
been attempted by prior MIP-based training methods in the literature. The time limit for these
experiments is 4 hours. Figure 2 depicts the performance of our method w.r.t. CART and OCT-H.
For solving S1O and OCT-H, we use the decision tree trained using CART as a warm start solution
for CPLEX, as in [Bertsimas and Dunn, 2017, Verwer et al., 2017]. For OCT-H, we observe that
within the time limit, the solver is either not able to ﬁnd any new feasible solution other than the one
provided as warm start, implying the decision tree trained using OCT-H has the same performance as
the one from CART, or the solver simply fails to construct the MIP formulation, which is depicted
by the missing bars in the Figure 2. This ﬁgure basically means that solely relying on any MIP
formulation to train ODT using large data-sets will result in no feasible solution being found within
the time limit.

5 Conclusions

We propose a novel MIP-based method to train an optimal multivariate classiﬁcation tree, which
has better generalization behavior compared to state-of-the-art MIP-based methods. Additionally, in

9

order to train ODT on very large data-sets, we devise an LP-based data-selection method. Numerical
experiments suggest that the combination of these two can enable us to obtain a decision tree with
better out-of-sample accuracy than CART and other comparable MIP-based methods, while solely
using any MIP-based training method will fail to do that almost certainly. In the current setup,
data selection occurs prior to training using SVM1-ODT. So, once a data subset has been selected,
it is used at every branch node to determine optimal branching rules. A natural extension of this
methodology could be a combined model for ODT training and data selection, wherein the branching
rules learned at the each layer, is applied to the entire data-set and data selection is redone at every
node in the subsequent layer prior to branching.

10

Broader impact

Ensemble methods such as random forests and gradient boosting methods such as XGBoost, and
LightGBM typically perform well, in terms of scalability and out-of-sample accuracy, for large-scale
classiﬁcation problems. However, these methods suffer from low interpretability and are incapable
of modeling fairness. We hope the scalable MIP-based framework proposed in this paper proves to
be seminal in addressing applicability of ODTs to large-scale real-world problems, while relying on
the decision tree structure to preserve interpretability. The MIP framework might especially come in
handy for sequential or joint prediction-optimization models, wherein the problem structure could be
utilized to devise decomposition-based solution procedures. Alternatively, the proposed approach
could be used to train a classiﬁcation tree to provide insights on the behavior of a black-box model.

Funding statement

The ﬁrst author of this paper was funded by IBM Research during his internship there.

References

Sina Aghaei, Mohammad Javad Azizi, and Phebe Vayanos. Learning optimal and fair decision trees

for non-discriminative decision-making. In AAAI, 2019.

Sina Aghaei, Andres Gomez, and Phebe Vayanos. Learning optimal classiﬁcation trees: Strong

max-ﬂow formulations. arXiv preprint arXiv:2002.09142, 2020.

Kristin P Bennett and JA Blue. A support vector machine approach to decision trees. In 1998
IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on
Computational Intelligence (Cat. No. 98CH36227), volume 3, pages 2396–2401. IEEE, 1998.

Dimitris Bertsimas and Jack Dunn. Optimal classiﬁcation trees. Machine Learning, 106(7):1039–

1082, 2017.

Christian Bessiere, Emmanuel Hebrard, and Barry O’Sullivan. Minimising decision tree size as
combinatorial optimisation. In International Conference on Principles and Practice of Constraint
Programming, pages 173–187. Springer, 2009.

Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone. Classiﬁcation and regression

trees. wadsworth int. Group, 37(15):237–251, 1984.

Gérard Cornuéjols. Valid inequalities for mixed integer linear programs. Mathematical Programming,

112(1):3–44, 2008.

IBM ILOG Cplex. V12. 1: User’s manual for cplex. International Business Machines Corporation,

46(53):157, 2009.

Dua Dheeru and E Karra Taniskidou. Uci machine learning repository. 2017.

Ralph E Gomory. Solving linear programming problems in integers. Combinatorial Analysis, 10:

211–215, 1960.

Ralph E Gomory. Outline of an algorithm for integer solutions to linear programs and an algorithm
for the mixed integer problem. In 50 Years of Integer Programming 1958-2008, pages 77–103.
Springer, 2010.

Oktay Günlük, Jayant Kalagnanam, Matt Menickelly, and Katya Scheinberg. Optimal decision trees

for categorical data via integer programming. arXiv preprint arXiv:1612.03225, 2018.

Xiyang Hu, Cynthia Rudin, and Margo Seltzer. Optimal sparse decision trees. In Advances in Neural

Information Processing Systems, pages 7265–7273, 2019.

Hyaﬁl Laurent and Ronald L Rivest. Constructing optimal binary decision trees is np-complete.

Information processing letters, 5(1):15–17, 1976.

11

Garth P McCormick. Computability of global solutions to factorable nonconvex programs: Part

I—convex underestimating problems. Mathematical Programming, 10(1):147–175, 1976.

Mangasarian Olvi and Glenn Fung. Data selection for support vector machine classiﬁers.

In
Proceedings of the sixth ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (KDD), 2000.

J. Ross Quinlan. Induction of decision trees. Machine learning, 1(1):81–106, 1986.

J Ross Quinlan. Improved use of continuous attributes in c4. 5. Journal of artiﬁcial intelligence

research, 4:77–90, 1996.

Jason Rhuggenaath, Yingqian Zhang, Alp Akcay, Uzay Kaymak, and Sicco Verwer. Learning fuzzy
decision trees using integer programming. In 2018 IEEE International Conference on Fuzzy
Systems (FUZZ-IEEE), pages 1–8. IEEE, 2018.

Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society: Series B (Methodological), 58(1):267–288, 1996.

Alfred Kar Yin Truong. Fast growing and interpretable oblique trees via logistic regression models.

PhD thesis, Oxford University, UK, 2009.

Sicco Verwer and Yingqian Zhang. Learning decision trees with ﬂexible constraints and objectives
using integer optimization. In International Conference on AI and OR Techniques in Constraint
Programming for Combinatorial Optimization Problems, pages 94–103. Springer, 2017.

Sicco Verwer and Yingqian Zhang. Learning optimal classiﬁcation trees using a binary linear program

formulation. In 33rd AAAI Conference on Artiﬁcial Intelligence, 2019.

Sicco Verwer, Yingqian Zhang, and Qing Chuan Ye. Auction optimization using regression trees and

linear models as integer programs. Artiﬁcial Intelligence, 244:368–395, 2017.

DC Wickramarachchi, BL Robertson, Marco Reale, CJ Price, and J Brown. Hhcart: An oblique

decision tree. Computational Statistics & Data Analysis, 96:12–23, 2016.

Laurence A Wolsey. Integer programming, volume 52. John Wiley & Sons, 1998.

Ji Zhu, Saharon Rosset, Robert Tibshirani, and Trevor J Hastie. 1-norm support vector machines. In

Advances in neural information processing systems, pages 49–56, 2004.

12

A Scalable Mixed-Integer Programming Based
Framework for Optimal Decision Trees
Supplementary Material, NeurIPS 2020

A Supplementary material for Section 2

Theorem 1. Every integer variable ul, l ∈ L in (1) can be relaxed to be continuous in [1, Y ].

Proof of Theorem 1. It sufﬁces to show, for any extreme point V of conv( ¯P ), where ¯P is the feasible
region in (1) while all variables ul are relaxed to [1, Y ], the u components of V are still integer. We
denote V (ui) to be the ui component in extreme point V , analogously we have V (ci), V (hb,j) and
so on.

Arbitrarily pick l ∈ L. If for all i ∈ [n], V (eil) = 0, meaning there is no data point entering leaf
node l, then we can see that V (ul) can be any value in [1, Y ] and this would maintain the feasibility
of point V . Since V is an extreme point, V (ul) has to be either 1 or Y in this case, which are both
integer. Now we assume there exists some i ∈ [n] such that V (eil) = 1, then from (1c)-(1e), we
see that V (ul) = V (wil) = V (ˆyi). From (1b): (yi − Y )V (ci) ≤ yi − V (ˆyi) ≤ (yi − 1)V (ci), here
V (ci) ∈ {0, 1}, we get: when V (ci) = 0, then V (ˆyi) = yi, which implies V (ul) = yi, it is an
integer. If V (ci) = 1, then (1b) degenerates to V (ˆyi) ∈ [1, Y ]. From the extreme point assumption
on V , we know in this case V (ˆyi) should be either 1 or Y , then from V (ul) = V (ˆyi), we know V (ul)
is still integral.

b, g(cid:48)

b)b∈B, (u(cid:48)

Let (h(cid:48)
solution to (1) with parameters

Theorem 2.
be a feasible
ther an optimal
(h(cid:48)
is an optimal solution to (1) with parameters (M α1, M α2, 1, (cid:15)/M ).

solution to (1) with parameters
l)l∈L, (ˆy(cid:48)

il)i∈[n],l∈L, (p+(cid:48)

b/M )b∈B, (u(cid:48)

i)i∈[n], (w(cid:48)

i)i∈[n], (w(cid:48)

l)l∈L, (ˆy(cid:48)

b/M, g(cid:48)

il, e(cid:48)

i, c(cid:48)

i, c(cid:48)

il)i∈[n],l∈L, (p+(cid:48)

il, e(cid:48)
(α1, α2, M, (cid:15)).

(α1, α2, M, (cid:15)),
ib /M, p−(cid:48)

ib /M, m(cid:48)

ib , p−(cid:48)

ib)i∈[n],b∈B
fur-
is
if

ib , m(cid:48)
Then it
if and only
ib/M )i∈[n],b∈B

Proof of Theorem 2. Let (h(cid:48)
i, c(cid:48)
be an arbitrary feasible solution to (1) with parameters (α1, α2, M, (cid:15)). So (1b)-(1e), (1k) hold, and

il)i∈[n],l∈L, (p+(cid:48)

i)i∈[n], (w(cid:48)

b)b∈B, (u(cid:48)

l)l∈L, (ˆy(cid:48)

ib , p−(cid:48)

ib , m(cid:48)

ib)i∈[n],b∈B

il, e(cid:48)

b, g(cid:48)

g(cid:48)
b −

(cid:88)

j∈[d]

bj · xij = p+(cid:48)
h(cid:48)

ib − p−(cid:48)

ib , ∀i ∈ [n], b ∈ B

p+(cid:48)
ib ≤ M (1 − e(cid:48)
p−(cid:48)
ib ≥ (cid:15)e(cid:48)
ib + m(cid:48)
p−(cid:48)
ib ≤ M (1 − e(cid:48)
p+(cid:48)
ib ≥ (cid:15)e(cid:48)
ib + m(cid:48)

il), ∀i ∈ [n], l ∈ L, b ∈ AR(l)
il, ∀i ∈ [n], l ∈ L, b ∈ AR(l)
il), ∀i ∈ [n], l ∈ L, b ∈ AL(l)
il, ∀i ∈ [n], l ∈ L, b ∈ AL(l),

which is equivalent to:

g(cid:48)
b/M −

(cid:88)

j∈[d]

bj/M · xij = p+(cid:48)
h(cid:48)

ib /M − p−(cid:48)

ib /M, ∀i ∈ [n], b ∈ B

p+(cid:48)
ib /M ≤ 1 − e(cid:48)
p−(cid:48)
ib /M + m(cid:48)
p−(cid:48)
ib /M ≤ 1 − e(cid:48)
p+(cid:48)
ib /M + m(cid:48)

il, ∀i ∈ [n], l ∈ L, b ∈ AR(l)

ib/M ≥ (cid:15)/M e(cid:48)

il, ∀i ∈ [n], l ∈ L, b ∈ AR(l)

il, ∀i ∈ [n], l ∈ L, b ∈ AL(l)

ib/M ≥ (cid:15)/M e(cid:48)

il, ∀i ∈ [n], l ∈ L, b ∈ AL(l),

13

ib , m(cid:48)

solution (h(cid:48)

ib)i∈[n],b∈B is

ib , p−(cid:48)
b/M, g(cid:48)

Note that constraints (1b)-(1e), (1k) do not involve variables (hb, gb)b∈B, (p+
i, c(cid:48)
i)i∈[n],
Therefore, we know that
(p+(cid:48)
(M, (cid:15)),
(h(cid:48)
i)i∈[n], (w(cid:48)
ib /M, m(cid:48)
is feasible to (1) with parameters (1, (cid:15)/M ).
Furthermore, if (h(cid:48)
b, g(cid:48)
for any feasible solution (h(cid:48)(cid:48)

to (1) with parameters
il)i∈[n],l∈L, (p+(cid:48)

b )b∈B, . . . to (1) with parameters (M, (cid:15)), there is

feasible
i, c(cid:48)

b/M )b∈B, (u(cid:48)

ib /M, p−(cid:48)

l)l∈L, (ˆy(cid:48)

b)b∈B,

l)l∈L,

b , g(cid:48)(cid:48)

il, e(cid:48)

b, g(cid:48)

(u(cid:48)

(ˆy(cid:48)

ib, p−
ib, mib)i∈[n],b∈B.
il, e(cid:48)
(w(cid:48)
il)i∈[n],l∈L,
if
and only if,
ib/M )i∈[n],b∈B

b)b∈B, . . . is the optimal solution to (1) with parameters (α1, α2, M, (cid:15)), then

c(cid:48)
i + α1

(cid:88)

i

(cid:88)

i,b

m(cid:48)

ib + α2

|h(cid:48)

bj| ≤

(cid:88)

b,j

(cid:88)

i

c(cid:48)(cid:48)
i + α1

(cid:88)

i,b

m(cid:48)(cid:48)

ib + α2

|h(cid:48)(cid:48)

bj|.

(cid:88)

b,j

b/M, g(cid:48)
b /M, g(cid:48)(cid:48)

b/M )b∈B, (u(cid:48)
b /M )b∈B, (u(cid:48)(cid:48)

Since (h(cid:48)
and (h(cid:48)(cid:48)
are both feasible to (1) with parameters (1, (cid:15)/M ), and the objective function of (1) with parameters
(cid:80)
(M α1, M α2) is (cid:80)

il)i∈[n],l∈L, (p+(cid:48)
il)i∈[n],l∈L, (p+(cid:48)(cid:48)

ib /M, p−(cid:48)
ib /M, p−(cid:48)(cid:48)

ib /M, m(cid:48)
ib /M, m(cid:48)(cid:48)

i)i∈[n], (w(cid:48)
i )i∈[n], (w(cid:48)(cid:48)

ib/M )i∈[n],b∈B
ib/M )i∈[n],b∈B

l)l∈L, (ˆy(cid:48)
l )l∈L, (ˆy(cid:48)(cid:48)

il, e(cid:48)
il, e(cid:48)(cid:48)

i, c(cid:48)
i , c(cid:48)(cid:48)

b,j |hbj|, therefore:

i,b mib + M α2

(cid:80)

i ci + M α1
(cid:88)

i
(cid:88)

i
(cid:88)

i
(cid:88)

i

=

≤

=

|h(cid:48)

bj|/M

(cid:88)

b,j

c(cid:48)
i + M α1

(cid:88)

m(cid:48)

ib/M + M α2

c(cid:48)
i + α1

c(cid:48)(cid:48)
i + α1

(cid:88)

i,b
m(cid:48)

ib + α2

i,b
(cid:88)

i,b

m(cid:48)(cid:48)

ib + α2

(cid:88)

|h(cid:48)

bj|

b,j
(cid:88)

b,j

|h(cid:48)(cid:48)
bj|

c(cid:48)(cid:48)
i + M α1

(cid:88)

i,b

m(cid:48)(cid:48)

ib/M + M α2

|h(cid:48)(cid:48)

bj|/M.

(cid:88)

b,j

b/M, g(cid:48)

Hence (h(cid:48)
is an optimal solution to (1) with parameters (M α1, M α2, 1, (cid:15)/M ). The other direction is exactly
the same.

b/M )b∈B, (u(cid:48)

il)i∈[n],l∈L, (p+(cid:48)

ib /M, p−(cid:48)

i)i∈[n], (w(cid:48)

ib /M, m(cid:48)

l)l∈L, (ˆy(cid:48)

ib/M )i∈[n],b∈B

il, e(cid:48)

i, c(cid:48)

A.1 Cutting-planes for SVM1-ODT

In this subsection, we provide proofs for Theorem 3 and Proposition 1. Note that we denote by Nk
the set of data points with the same dependent value k, i.e., Nk := {i ∈ [n] : yi = k}, for any
k ∈ [Y ].

Theorem 3. Given a set I ⊆ [n] with |I ∩ Nk| ≤ 1 for any k ∈ [Y ]. Then for any L ⊆ L, the
inequality

(cid:88)

ci ≥

(cid:88)

eil − |L|

is a valid cutting-plane to SVM1-ODT (1).

i∈I

i∈I,l∈L

Here the index set I is composed by arbitrarily picking at most one data point from each class Nk.

Proof of Theorem 3. First, we consider the case that (cid:80)
l∈L eil = 1 for all i ∈ I. This means
for any data point i ∈ I, it is classiﬁed into leaf node among L. Then inequality (5) reduces to:
(cid:80)
i∈I ci ≥ |I| − |L|. It trivially holds when |L| ≥ |I|. When |I| > |L|, according to the property
that |I ∩ Nk| ≤ 1 for any k ∈ [Y ], we know the data points in I have exactly |I| many different
classes. Since all of those data points are entering leaf nodes in L, according to the Pigeon Hole
Principle, we know at least |I| − |L| of them are misclassiﬁed.

Now we consider general case. We can divide I into two parts: I = I1 ∪ I2, where I1 := {i ∈
I : (cid:80)
l∈L eil = 0}. Then from our above discussion, we have
(cid:80)

l∈L eil = 1}, I2 := {i ∈ I : (cid:80)
ci ≥ (cid:80)

i∈I1,l∈L eil − |L|. From the deﬁnition of I2, there is (cid:80)

i∈I2,l∈L eil = 0. Hence:

i∈I1

(cid:88)

i∈I

ci ≥

(cid:88)

i∈I1

ci ≥

(cid:88)

eil − |L|

i∈I1,l∈L

14

(cid:88)

=

eil − |L| +

(cid:88)

(cid:88)

eil =

eil − |L|.

i∈I1,l∈L

i∈I2,l∈L

i∈I,l∈L

We complete the proof.

Proposition 1. Let {|Nk| | k ∈ [Y ]} = {s1, . . . , sk} with s1 ≤ s2 ≤ . . . ≤ sY . Then the following
inequalities

1. ∀l ∈ L, (cid:80)

2. ∀l ∈ L, (cid:80)

i∈[n] ci ≥ (cid:80)
i∈[n] eil − sY ;
i∈[n](ci + eil) ≥ s1 · (Y − 2D + 1);

3. (cid:80)

i∈[n] ci ≥ s1 + . . . + sY −2D if Y > 2D.

are all valid cutting-planes to SVM1-ODT (1).

Proof of Proposition 1. We prove the validity of these inequalities individually.

1. Since s1 ≤ . . . ≤ sY , we can partition the set [n] into sY different disjoint Is that all have
property |I ∩ Nk| ≤ 1 for any k ∈ [Y ] as in Theorem 3. Select L = {l} for an arbitrary
l ∈ L. Then, for each of the above set I we obtain a cut from (5). Combining all these SY
cuts together, we would obtain the desired inequality (cid:80)

i∈[n] ci ≥ (cid:80)

i∈[n] eil − sY .

2. For any l ∈ L, denote L := L \ {l}. Then for index set I with |I ∩ Nk| ≤ 1 for any
i∈I,l(cid:48)∈L eil(cid:48) − |L|. From (1k) of

k ∈ [Y ], Theorem 3 gives us the inequality (cid:80)
SVM1-ODT, there is (cid:80)

l(cid:48)∈L eil(cid:48) = 1 − eil. Hence we obtain the inequality

i∈I ci ≥ (cid:80)

(ci + eil) ≥ |I| − 2D + 1.

(cid:88)

i∈I

Then we construct s1 number of disjoint Is each with cardinality |Y |, by arbitrarily picking
one element from each class Nk without replacement. This process can be done for exactly
s1 many times. Therefore, we obtain s1 many inequalities for different index set I:

(ci + eil) ≥ |Y | − 2D + 1.

(cid:88)

i∈I

For those i ∈ [n] which are not contained in any of these s1 many Is, we use the trivial
lower bound inequality ci + eil ≥ 0. Lastly, we combine all those inequalities together,
which leads to:

(cid:88)

(ci + eil) ≥ s1 · (Y − 2D + 1).

i∈[n]

3. When Y > 2D, since there are more classes than the number of leaf nodes, and all data
points have to be categorized into exactly one of those leaf nodes, by the Pigeonhole
Principle, we know the number of misclassiﬁcation is at least s1 + . . . + sY −2D .

A.2 SVM1-ODT for data-set with Categorical Features

Usually in practice, when dealing with data-set with both numerical and categorical features, people
will do some feature transformation in pre-processing, e.g., one-hot encoding etc. It is doing ﬁne
for most heuristic-based training methods, but since here our training method is MIP-based, the
tractability performance can be very dimension-sensitive. Most feature transformation increases
In this subsection we are going to talk about the modiﬁcations of
the data dimension greatly.

15

the formulation SVM1-ODT, in order to train the initial data-set directly without doing feature
transformation. First we add the following new constraints:

(cid:88)

hbj ≤ 1, ∀b ∈ B

j∈Fc
sbjv ≤ hbj, ∀b ∈ B, j ∈ Fc, v ∈ Vj
(cid:88)
sbjvI(xi,j = v) ≥ eil +

(cid:88)

v∈Vj

j∈Fc
∀i ∈ [n], l ∈ L, b ∈ AL(l)
(cid:88)

(cid:88)

sbjvI(xi,j = v) ≤ 2 − eil −

v∈Vj

j∈Fc
∀i ∈ [n], l ∈ L, b ∈ AR(l)
(cid:88)

hbj − 1 ≤ hbj ≤ 1 −

(cid:88)

hbj,

(cid:88)

j∈Fc

hbj − 1,

(cid:88)

j∈Fc

hbj,

j∈Fc
∀b ∈ B, j ∈ Fq
sbjv, hbj ∈ {0, 1}, ∀b ∈ B, j ∈ Fc, v ∈ Vj

j∈Fc

(8a)

(8b)

(8c)

(8d)

(8e)

(8f)

Here Fc ⊆ [d] denotes the index set of categorical features, and Fq = [d] \ Fc denotes the index
set of numerical features. I(xi,j = v) is the indicator function about whether xi,j = v or not. For
j ∈ Fc, Vj denotes the set of all possible values at categorical feature j.

For our above formulation, we are assuming: At each branch node of the decision tree, the branching
rule cannot be based on both categorical features and numerical features, and if it is based on
categorical feature, then the branching rule is only provided by one single categorical feature. These
constraints are formulated by (8a) and (8e) individually, where hbj ∈ {0, 1} for j ∈ Fc characterizes
whether the branching rule at node b is based on categorical feature j or not. Furthermore, we
use another binary variable sbjv to denote the conditions that data points must satisfy in order to
go left at that node b: If the j-th component of data point xi has value v such that sbjv = 1, then
this data point goes to the left branch. Here we allow multiple different values v ∈ Vj such that
sbjv = 1. Clearly sbjv can be positive only when categorical feature j is chosen as the branching
feature. So we have constraint (8b). Note that this branching rule for categorical features have also
been considered in [Aghaei et al., 2019], through different formulation. Now we explain how to
formulate this categorical branching rule into linear constraints. Still, we start from the leaf node.
When eil = 1, we know the branching rule at each b ∈ AL(l) ∪ AR(l) is applied to data point xi. To
be speciﬁc, when b ∈ AL(l) and (cid:80)
hbj = 1, there must exists some feature j ∈ Fc and v ∈ Vj
j∈Fc
such that xi,j = v and sbjv = 1. Analogously, when b ∈ AR(l) and (cid:80)
hbj = 1, the previous
situation cannot happen. Then one can easily see that these two implications are formulated by (8c)
and (8d). Cases of (cid:80)
hbj = 0 are not discussed here since in that case the branching rule is
given by the branching hyperplane on numerical features, which was discusses in Sect. 2.

j∈Fc

j∈Fc

Furthermore we should mention that, to train data-set with both numerical and categorical features,
we will also modify some constraints in the original formulation (1): (1h) and (1j) should be changed
to p−
hbj). This is because those two
constraints are only considered when the branching rule is not given by categorical features, which
means (cid:80)

ib + mib ≥ (cid:15)(eil − (cid:80)

ib + mib ≥ (cid:15)(eil − (cid:80)

hbj) and p+

j∈Fc

j∈Fc

hbj = 0.

j∈Fc

B Supplementary material for Section 3

Theorem 4.

1) If (cid:15)(cid:48) = 0, then for any optimal solution (b, ¯λ) of (7), there exists λ s.t. (b, λ) is optimal

solution of (6) with f = 0, g = 1, and vice versa;

2) If (cid:15)(cid:48) > 0, then for any optimal solution (b, ¯λ) of (7), there exists λ s.t. ((cid:98)b(cid:99), λ) is an optimal
solution of (6) with f = 0, g = 1. Here, (cid:98)b(cid:99) is a vector with every component being (cid:98)bi(cid:99).

16

i

i(cid:48)(cid:54)=i

i(cid:48)(cid:54)=j,i

i(cid:48)(cid:54)=j,i

¯λijxj + (cid:80)

¯λii(cid:48)xi(cid:48)) + (cid:80)

¯λji(cid:48)xi(cid:48) = ¯λji

Proof of Theorem 4. We consider two cases for (cid:15)(cid:48).
Case 1 ((cid:15)(cid:48) = 0): First we show: If (b, ¯λ) is an optimal solution in (7), then there exists λ such
that (b, λ) is optimal in (6). Notice that when (cid:15)(cid:48) = 0, b ∈ {0, 1}|N |. This is because if bj ∈ (0, 1),
we can multiply bj and all λji by 1
, the new solution is still feasible, with higher objective value,
bj
which contradicts to the optimal assumption of b. Assuming there exists some i ∈ N such that
bi = 1 and exists some j (cid:54)= i ∈ N such that ¯λji > 0. Since bj = (cid:80)
¯λji, we know bj = 1. Now
¯λji ¯λii(cid:48) +¯λji(cid:48)
we transform the original j-th row of ¯λ in this way: replace ¯λji by 0, replace ¯λji(cid:48) by
.
1−¯λji ¯λij
Here i(cid:48) (cid:54)= i, j. As long as there exists i (cid:54)= j ∈ N such that bi = 1, λj,i > 0, we can do the above
transformation to get rid of it. Therefore after doing this transformation for ﬁnitely many times, we
obtain a new λ with 0 ≤ λji ≤ 1 − bi, ∀i (cid:54)= j ∈ N . It remains to show, after each transformation, the
ﬁrst two equations in (7) still hold. The ﬁrst equation holds because xj = ¯λjixi + (cid:80)
¯λji(cid:48)xi(cid:48) =
¯λji((cid:80)
¯λii(cid:48) + ¯λji(cid:48))xi(cid:48). Hence xj =
¯λji ¯λii(cid:48) +¯λji(cid:48)
(cid:80)
. This
1−¯λji ¯λij
is true because (cid:80)
¯λij) =
(¯λji((cid:80)
¯λij) = 1.
Therefore, we have shown for any optimal solution (b, ¯λ) of (7), there exists λ s.t. (b, λ) is feasible
in (6). The optimality of such (b, λ) in (6) automatically follows from the fact that (7) is a relaxation
over (6). Now, we show the other direction. It sufﬁces to show: If (b, λ) is an optimal solution of (6),
then (b, λ) is also an optimal solution of (7). Clearly (b, λ) is feasible in (7). If it is not the optimal
solution, meaning there exists (b(cid:48), ¯λ) in (7), such that 1T b(cid:48) > 1T b. From what we just showed, there
exists λ(cid:48) such that (b(cid:48), λ(cid:48)) is feasible in (6), with 1T b(cid:48) > 1T b, which gives the contradiction.
Case 2 ((cid:15)(cid:48) > 0): Since (cid:98)b∗(cid:99) is a binary vector, follow the same argument as in above, we can
construct λ such that ((cid:98)b∗(cid:99), λ) is a feasible solution to (6). To show it is also optimal, it sufﬁces to
show, any feasible solution in (6), has objective value no larger than (cid:80)
i (cid:99). Assume (bi)i∈N
is part of one feasible solution in (6). Then apparently it is also part of one feasible solution in (7),
since (7) is simply the relaxation of (6). Now we show bi ≤ b∗
i for all i: when bi = 0 it is trivially
true, when bi = 1 then b∗
i to 1 while remain
feasible, and this contradicts to the optimality assumption of b∗. Hence: (cid:80)
i(cid:98)bi(cid:99) ≤ (cid:80)
i (cid:99). Since b
is binary, then (cid:80)

xi(cid:48). For the second equation we just need to check 1 = (cid:80)
i(cid:48)(cid:54)=j,i (¯λji

i(cid:48)(cid:54)=j,i(¯λji
¯λij) = (¯λji(1 − ¯λij) + (1 − ¯λji))(1 − ¯λji

i should also be 1, since otherwise we can also increase b∗

¯λji ¯λii(cid:48) +¯λji(cid:48)
1−¯λji ¯λij
¯λii(cid:48) + ¯λji(cid:48)))(1 − ¯λji

¯λii(cid:48) + ¯λji(cid:48))/(1 − ¯λji
¯λji(cid:48))(1 − ¯λji

i bi, and this concludes the proof.

i(cid:98)bi(cid:99) = (cid:80)

¯λij) = ((cid:80)

¯λii(cid:48)) + (cid:80)

i(cid:48)(cid:54)=j,i(¯λji

i∈N (cid:98)b∗

i(cid:98)b∗

i(cid:48)(cid:54)=j,i

i(cid:48)(cid:54)=j,i

i(cid:48)(cid:54)=j,i

i(cid:48)(cid:54)=j,i

B.1 Balanced Data-selection

In this section we talk about another variant of (7), under the same data-selection framework as we
mentioned in Sect. 3, called balanced data-selection. Sometimes when doing data-selection, it might
not be a good idea to pick all the extreme points or discard as many points inside the convex hull as
possible. One example is Figure 3: In this data-set, most of data points lie inside the rectangle, some
of them lie on the circle, and a few of them lie inside the circle while outside the rectangle. If we
only care about maximizing the number of points inside the convex hull of entire data points, then we
would end up picking all points on the circle, however in this case it is obvious that picking those 4
vertices of the rectangle is a better option since they capture the data distribution better, even though
these 4 points are not extreme points of the entire data-set.

Motivated by the above example, we realize that in many
cases the balance between the number of selected points
(those with ai = 1 in (6)) and the number of points inside
the convex hull of selected points (points with bi = 1) is
important. This can be represented by choosing nonzero
objective f and g. One obvious choice here is f = g = 1.
However, once we pick nonzero objective f, we cannot
simply project out a variables by replacing ai = 1 − bi
any more, and 0-1 LP (6) might also seem intractable to
solve optimally in large data-set case. But since this 0-1
LP was proposed by heuristic, we do not have to solve
it to optimality. Here we can approximately solve it by
looking at the optimal solution of its corresponding linear

17

Figure 3: In this case, the balanced data-
selection is more preferable.

relaxation:

max (cid:80)
s.t.

i∈N (bi − ai)

bjxj = (cid:80)
(cid:80)

i∈N ,i(cid:54)=j λjixi, ∀j ∈ N

i∈N ,i(cid:54)=j λji = bj, ∀j ∈ N

0 ≤ λji ≤ ai, ∀i (cid:54)= j ∈ N
aj + bj ≤ 1, ∀j ∈ N
aj, bj ∈ [0, 1].

(9)

(9) is the natural linear relaxation for (6) with (cid:15)(cid:48) = 0. Then we have the following observations:
Remark 1. Assume (a(cid:48), b(cid:48), λ(cid:48)) is the optimal solution of (9). Then:

• If b(cid:48)

i = 0 for some i ∈ N , then xi cannot be written as the convex combination of some

other points;

• If a(cid:48)

• If a(cid:48)

i = 0 for some i ∈ N , then b(cid:48)
i > 0, b(cid:48)

i > 0 for some i ∈ N , then a(cid:48)

i ∈ {0, 1};

i + b(cid:48)

i = 1.

Hence we know that, by enforcing bi = 0 in (6) for those i with b(cid:48)
i = 0, it will not change the optimal
solution; For those points xi with b(cid:48)
j ∈ (0, 1), we expect that it is easier to express
xi as the convex combination of other points than xj, so we greedily assign bi = 1, ai = 0 in (6). The
tricky part is about those point with a(cid:48)
i > 0, which is used to express other points and can
also be expressed by other points in the optimal solution of (9). So we implement the original 0-1 LP
(6) to handle those “ambiguous” points. In other words, we will do the following steps:

i = 1 and xj with b(cid:48)

i > 0, b(cid:48)

Algorithm 2 Approximately get a balanced data-selection solution

Solve the LP relaxation (9), and denote (a(cid:48), b(cid:48), λ) to be the optimal solution, I0 := {i ∈ N : b(cid:48)
0}, I1 := {i ∈ N : b(cid:48)
Assign bi = 0 for i ∈ I0, bi = 1 for i ∈ I1 in (6) with f = g = 1, and solve it;
Pick the index support of the a components in the optimal solution as the selected data subset.

i = 1};

i =

B.2 Iterative ODT training algorithm

Provided with the framework of using MIP to train ODT and from the nature of our data-selection
method, in this subsection we want to propose a generic iterative method to continuously obtain
more accurate ODTs from subsets of data-set. We are introducing this iterative training method
corresponding to (7), where f = 0, g = 1. Note that the balanced data-selection can also be applied
to this iterative method. We should also mention that this iterative method shares the same spirit as
the classic E/M algorithm in statistics.

Algorithm 3 Iterative ODT training method based on data-selection

Initialize data-selection parameter (cid:15)(cid:48), SVM1-ODT training parameter α1, α2, (cid:15), M , and initial
heuristically generated decision tree T ;
for t = 0, 1, 2, . . . do

Cluster N is picked as: the data points of [n] assigned into each leaf node of T , which are also
been correctly classiﬁed. Denote the collection of incorrectly classiﬁed data to be I;
Solve the data-selection sub-problem (7) for each cluster N . Denote ¯IN := {j ∈ N : bj =
1}, JN := {i /∈ ¯IN : ∃j ∈ ¯IN , s.t. λji > 0}, KN := N \ ( ¯IN ∪ JN ). Then denote J to be the
collection of all JN , and K to be the collection of all KN ;
Input the data subset I ∪ J ∪ K into MIP (1), and replacing the term (cid:80)
function by: (cid:80)
Assign T to be decision tree corresponding to the optimal (best current feasible) solution of the
previous SVM1-ODT, and iterate the loop.

i∈I∪K ci, and solve the SVM1-ODT;

j∈J (|I| + 1)cj + (cid:80)

i∈I∪J∪K ci in objective

end for

For this iterative method, we have the following statement.

18

Proposition 2. When α1 = α2 = 0, and the SVM1-ODT in each iteration of Algorithm 3 is solved
to optimality, then the decision tree obtained at each iteration would have higher accuracy over the
entire training data than the previous decision trees.

Proof. For decision tree T0, denote I to be the collection of incorrectly classiﬁed data in T0. After
solving the data-selection sub-problem (7) for each cluster, we denote ¯I to be the collection of
data point i in each cluster which has bi = 1, and J, K as deﬁned in the Algorithm 3. Clearly
[n] = I ∪ ¯I ∪ J ∪ K, and the accuracy of T0 is 1 − |I|
n . Now we assume T1 to be the tree obtained
at the next iteration, which minimizes the objective (cid:80)
i∈I∪K ci. Since T0 is a
feasible solution, with objective value |I|, then we must have, for decision tree T1, cj = 0 for all
j ∈ J, and (cid:80)
i∈I∪K ci ≤ |I|. In other words, tree T1 correctly classiﬁes all data points in J, and the
misclassiﬁcation number over I ∪ K is at most |I| many. According to the construction of J and
¯I, we know that once all data points in J are correctly classiﬁed, then all data points in ¯I are also
correctly classiﬁed, since data point in ¯I is contained in the convex hull of points in J. Also because
[n] = I ∪ ¯I ∪ J ∪ K, we know the training accuracy of T1 over [n] is just 1 −
, which is at
least 1 − |I|

j∈J (|I| + 1)cj + (cid:80)

i∈I∪K ci
n

n , the accuracy of T0.

(cid:80)

C Additional numerical results and detailed records

Table 3: Accuracy and running time on medium-sized data-sets, for tree depth D=2. The numbers after ‘/’ for
CART and OCT-H are the numerical results reported in [Bertsimas and Dunn, 2017].

data-set

n
d
Y

Banknote-
authen
1372
4
2

Echocar-
diogram
61
10
2

Seeds

210
7
3

Dermat-
ology
358
34
6

Indian-
liver
579
10
2

Parkinsons

195
21
2

testing accuracy (%)

S1O
CART
OCT-H
Fair

99.7
89.9 /89.0
88.9 /91.5
100

100
91.1 /74.7
91.1 /77.3
86.7

98.7
88.9 /87.2
88.2 /90.6
90.2

80.7
65.2 /65.4
74.6 /74.2
73.9

75.2
71.6 /71.7
72.3 /72.6
71.9

91.0
79.9 /84.1
86.8 /84.9
81.3

training accuracy (%)

S1O
CART
OCT-H
Fair

100
91.7
87.2
100

running time (s)

S1O
OCT-H
Fair

900
900
94.8

100
100
100
100

5.1
0.09
0.28

81.1
67.0
75.9
81.1

900
900
231

79.6
71.4
75.4
81.3

900
900
600

100
88.2
92.0
100

900
900
19.6

100
92.5
94.2
100

39.3
519
55

19

Table 4: Accuracy and running time on medium-sized data-sets, for tree depth D=2. The numbers after ‘/’ for
CART and OCT-H are the numerical results reported in [Bertsimas and Dunn, 2017].

data-set
n
d
Y

sonar
208
60
2

testing accuracy (%)

survival
306
3
2

S1O
CART
OCT-H
Fair

82.4
69.3 /70.4
74.6 /70.0
70.6

75.1
73.4 /73.2
73.3 /73.0
74.1

training accuracy (%)

S1O
CART
OCT-H
Fair

running time (s)

S1O
OCT-H
Fair

100
79.4
89.0
100

900
900
3.14

74.2
77.1
74.7
80.0

900
900
900

Hepatitis
80
19
2

89.5
80.1 /83.0
84.2 /81.0
84.2

100
94.5
99.1
100

44
107
0.3

Relax
182
12
2

73.6
68.2 /71.1
66.0 /70.7
73.3

74.9
74.7
77.5
90.5

900
900
900

Table 5: Testing results for multivariate ODT on medium-sized data-sets, D= 2. The numbers after ‘/’ for CART
and OCT-H are the numerical results reported in [Bertsimas and Dunn, 2017].

data-set

n
d
Y

Balance-
scale
625
4
3

testing accuracy (%)

Ionosphere Monks-1

Monks-2

Monks-3 Wine

351
34
2

124
6
2

169
6
2

122
6
2

178
13
3

89.1
67.5 /64.5

S1O
CART
BinOCT 69.3
OCT-H
Fair

85.3 /87.6
89.2

90.3
87.7 /87.8
88.6
86.4 /86.2
85.4

94.7
68.4 /57.4
80.0
90.6 /93.5
92.3

71.4
54.8 /60.9
58.1
72.2 /75.8
62.3

96.7
94.2 /94.2
93.5
91.8 /92.3
83.5

95.3
83.7 /81.3
90.7
88.4 /91.1
93.2

training accuracy (%)

86.5
S1O
71.7
CART
BinOCT 73.3
82.9
OCT-H
89.8
Fair

100
91.0
91.1
94.2
100

87.7
65.2
69.8
79.2
96.2

92.4
93.5
93.8
90.0
97.4

100
94.1
97.3
97.0
100

98.1
76.8
83.5
94.9
100

20

Table 6: More testing results for medium-sized data-sets, D=2. Here “S1O-DS” refers to the combination of
SVM1-ODT and LP-based data-selection method.

data-set

statlog

spambase

n
d
Y

4435
36
6

4601
57
2

testing accuracy (%)

Thyroid-
disease-ann-
thyroid
3772
21
3

S1O-DS
CART
OCT-H
Fair

74.2
63.7 /63.2
63.7 /63.2
63.7

89.1
84.3 /84.2
87.2 /85.7
86.1

96.3
97.5 /95.6
92.8 /92.5
93.0

training accuracy (%)

S1O-DS
CART
OCT-H
Fair

running time (s)

S1O-DS

74.1
63.5
63.5
63.5

900

88.9
86.7
90.3
91.4

900

percentage of selected data points (%)

DS

Parameters set

(cid:15)

α1
α2
(cid:15)(cid:48)

β1
β2

7.0

0.1

1000000

0.1

0.0

0.3

0.07

10.0

0.02

1000

0.1

0.0

0.3

0.1

96.7
98.4
94.2
95.8

900

12

0.01

1000

0.1

0.0

0.1

0.12

Wall-following-
robot-2

seismic

5456
2
4

93.7
93.7/94.0
93.7/94.0
93.7

93.7
93.7
93.7
93.7

2.08

1.0

0.0008

30

0.01

0.0

0.4

1.0

2584
20
2

93.3
93.3 /93.3
93.3 /93.3
93.3

93.3
93.3
93.3
93.3

0.16

10.0

0.01

1000

0.1

0.0

0.4

0.1

21

Table 7: Testing results for large-scale data-sets, with tree depth D = 2, time limit is set to be 4h.

data-set

Avila

EEG

HTRU

pendigits

n
d
Y

10430
10
12

14980
14
2

17898
8
2

testing accuracy (%)

S1O-DS
CART
OCT-H

52.6
50.3
N/A

training accuracy (%)

S1O-DS
CART
OCT-H

55.0
50.7
N/A

66.5
58.6
58.6

67.1
60.3
60.3

percentage of selected data points (%)

97.8
97.3
97.3

97.5
97.8
97.8

7494
16
10

38.9
36.2
36.2

38.9
36.5
36.5

skin-
segmentation
245057
3
2

shuttle

43500
9
7

86.3
80.6
N/A

87.1
81.0
N/A

94.0
93.8
N/A

94.1
94.0
N/A

DS

4.5

2.0

3

7.1

0.42

2.0

Parameters set

(cid:15)

α1
α2
(cid:15)(cid:48)

β1
β2

0.01

1000

0.1

0.1

0.2

0.05

0.04

1000

0.1

0.0

0.1

0.02

0.5

1000

0.1

0.0

0.1

0.03

0.01

1000

0.1

0.3

0.1

0.1

0.03

1000

0.1

0.0

0.2

0.05

0.007

1000

0.1

0.0

0.1

0.02

22

Table 8: Testing results for large-scale data-sets, with tree depth D = 3, time limit is set to be 4h.

data-set

Avila

EEG

HTRU

pendigits

n
d
Y

10430
10
12

14980
14
2

17898
8
2

testing accuracy (%)

S1O-DS
CART
OCT-H

55.8
53.5
N/A

training accuracy (%)

S1O-DS
CART
OCT-H

58.1
56.2
N/A

66.5
64.2
N/A

70.6
65.3
N/A

percentage of selected data points (%)

97.9
98.1
N/A

97.4
97.8
N/A

7494
16
10

62.5
57.9
57.9

62.2
58.4
58.4

skin-
segmentation
245057
3
2

shuttle

43500
9
7

94.9
87.1
N/A

94.7
87.6
N/A

99.5
99.7
N/A

99.5
99.7
N/A

DS

2.0

2.0

1.8

2.8

0.39

1.5

Parameters set

(cid:15)

α1
α2
(cid:15)(cid:48)

β1
β2

0.02

1000

0.1

0.0

0.2

0.02

0.02

1000

0.1

0.0

0.1

0.02

0.02

100000

0.01

0.1

0.2

0.04

0.01

1000

0.1

0.3

0.1

0.08

0.03

1000

0.1

0.0

0.2

0.03

0.008

1000

0.1

0.0

0.1

0.015

23

