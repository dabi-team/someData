1
2
0
2

b
e
F
2
2

]

R
C
.
s
c
[

2
v
1
2
7
5
0
.
2
1
9
1
:
v
i
X
r
a

Using Deep Learning to Solve Computer Security
Challenges: A Survey*

Yoon-Ho Choi2,1, Peng Liu†1, Zitong Shang1, Haizhou Wang1, Zhilong
Wang1, Lan Zhang1, Junwei Zhou3,1, and Qingtian Zou1

1The Pennsylvania State University, United States
2Pusan National University, Republic of Korea
3Wuhan University of Technology, China

Abstract

Although using machine learning techniques to solve computer security challenges is
not a new idea, the rapidly emerging Deep Learning technology has recently triggered
a substantial amount of interests in the computer security community. This paper seeks
to provide a dedicated review of the very recent research works on using Deep Learning
techniques to solve computer security challenges. In particular, the review covers eight
computer security problems being solved by applications of Deep Learning: security-
oriented program analysis, defending return-oriented programming (ROP) attacks, achiev-
ing control-ﬂow integrity (CFI), defending network attacks, malware classiﬁcation, system-
event-based anomaly detection, memory forensics, and fuzzing for software security.

1

Introduction

Using machine learning techniques to solve computer security challenges is not a new idea. For
example, in the year of 1998, Ghosh and others in [1] proposed to train a (traditional) neural
network based anomaly detection scheme(i.e., detecting anomalous and unknown intrusions
against programs); in the year of 2003, Hu and others in [2] and Heller and others in [3] ap-
plied Support Vector Machines to based anomaly detection scheme (e.g., detecting anomalous
Windows registry accesses).

The machine-learning-based computer security research investigations during 1990-2010,
however, have not been very impactful. For example, to the best of our knowledge, none of the
machine learning applications proposed in [1–3] has been incorporated into a widely deployed
intrusion-detection commercial product.

Regarding why not very impactful, although researchers in the computer security commu-
nity seem to have different opinions, the following remarks by Sommer and Paxson [4] (in the
context of intrusion detection) have resonated with many researchers:

• Remark A: “It is crucial to have a clear picture of what problem a system targets: what
speciﬁcally are the attacks to be detected? The more narrowly one can deﬁne the target
activity, the better one can tailor a detector to its speciﬁcs and reduce the potential for
misclassiﬁcations.” [4]

*The authors of this paper are listed in alphabetic order.
†Corresponding author: pxl20@psu.edu

1

 
 
 
 
 
 
• Remark B: “If one cannot make a solid argument for the relation of the features to the

attacks of interest, the resulting study risks foundering on serious ﬂaws.” [4]

These insightful remarks, though well aligned with the machine learning techniques used
by security researchers during 1990-2010, could become a less signiﬁcant concern with Deep
Learning (DL), a rapidly emerging machine learning technology, due to the following obser-
vations. First, Remark A implies that even if the same machine learning method is used, one
algorithm employing a cost function that is based on a more speciﬁcally deﬁned target attack
activity could perform substantially better than another algorithm deploying a less speciﬁcally
deﬁned cost function. This could be a less signiﬁcant concern with DL, since a few recent
studies have shown that even if the target attack activity is not narrowly deﬁned, a DL model
could still achieve very high classiﬁcation accuracy. Second, Remark B implies that if feature
engineering is not done properly, the trained machine learning models could be plagued by se-
rious ﬂaws. This could be a less signiﬁcant concern with DL, since many deep learning neural
networks require less feature engineering than conventional machine learning techniques.

As stated in [5], “DL is a statistical technique that exploits large quantities of data as training
sets for a network with multiple hidden layers, called a deep neural network (DNN). A DNN
is trained on a dataset, generating outputs, calculating errors, and adjusting its internal param-
eters. Then the process is repeated hundreds of thousands of times until the network achieves
an acceptable level of performance. It has proven to be an effective technique for image classi-
ﬁcation, object detection, speech recognition, and natural language processing––problems that
challenged researchers for decades. By learning from data, DNNs can solve some problems
much more effectively, and also solve problems that were never solvable before.”

Now let’s take a high-level look at how DL could make it substantially easier to overcome
the challenges identiﬁed by Sommer and Paxson [4]. First, one major advantage of DL is
that it makes learning algorithms less dependent on feature engineering. This characteristic of
DL makes it easier to overcome the challenge indicated by Remark B. Second, another major
advantage of DL is that it could achieve high classiﬁcation accuracy with minimum domain
knowledge. This characteristic of DL makes it easier to overcome the challenge indicated by
Remark A.

Key observation. The above discussion indicates that DL could be a game changer in

applying machine learning techniques to solving computer security challenges.

Motivated by this observation, this paper seeks to provide a dedicated review of the very re-
cent research works on using Deep Learning techniques to solve computer security challenges.
It should be noticed that since this paper aims to provide a dedicated review, non-deep-learning
techniques and their security applications are out of the scope of this paper.

The remaining of the paper is organized as follows. In Section 2, we present a four-phase
workﬂow framework which we use to summarize the existing works in a uniﬁed manner. In
Section 3-10, we provide a review of eight computer security problems being solved by appli-
cations of Deep Learning, respectively. In Section 11, we will discuss certain similarity and
certain dissimilarity among the existing works. In Section 12, we mention four further areas of
investigation. In Section 13, we conclude the paper.

2

2 A four-phase workﬂow framework can summarize the ex-

isting works in a uniﬁed manner

We found that a four-phase workﬂow framework can provide a uniﬁed way to summarize all
the research works surveyed by us. In particular, we found that each work surveyed by us
employs a particular workﬂow when using machine learning techniques to solve a computer
security challenge, and we found that each workﬂow consists of two or more phases. By “a
uniﬁed way”, we mean that every workﬂow surveyed by us is essentially an instantiation of a
common workﬂow pattern which is shown in Figure 1.

2.1 Deﬁnitions of the four phases

The four phases, shown in Figure 1, are deﬁned as follows. To make the deﬁnitions of the four
phases more tangible, we use a running example to illustrate each of the four phases.

Figure 1: Overview of the four-phase workﬂow

Phase I.(Obtaining the Raw Data)
In this phase, certain raw data are collected.

Running Example: When Deep Learning is used to detect suspicious events in a Hadoop
distributed ﬁle system (HDFS), the raw data are usually the events (e.g., a block is allocated,
read, written, replicated, or deleted) that have happened to each block. Since these events
are recorded in Hadoop logs, the log ﬁles hold the raw data. Since each event is uniquely
identiﬁed by a particular (block ID, timestamp) tuple, we could simply view the raw data as n
event sequences. Here n is the total number of blocks in the HDFS. For example, the raw data
collected in [6] in total consists of 11,197,954 events. Since 575,139 blocks were in the HDFS,
there were 575,139 event sequences in the raw data, and on average each event sequence had
19 events. One such event sequence is shown as follows:

081110 112428 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock:

/user/root/rand/_temporary/_task_200811101024_0001_m_001649_0/
part-01649.blk_-1033546237298158256

081110 112428 9602 INFO dfs.DataNode$DataXceiver:

Receiving block blk_-1033546237298158256 src: /10.250.13.240:54015
dest:/10.250.13.240:50010

081110 112428 9982 INFO dfs.DataNode$DataXceiver:

Receiving block blk_-1033546237298158256 src: /10.250.13.240:52837
dest:/10.250.13.240:50010

081110 112432 9982 INFO dfs.DataNode$DataXceiver:

writeBlock blk_-1033546237298158256 received exception
java.io.IOException:Could not read from stream

3

Data PreprocessingRepresentation LearningClassifierLearningRaw DataPhase IPhase IIPhase IIIPhase IVDefinition of four Phases•Phase I, Raw Data: The raw data.•Phase II, Data Preprocessing : Convert the raw data into a clean data set, whenever the raw data format is not feasible for the analysis.•Phase III, Representation Learning: Asdefined in [1] “learning representations of the data that make it easier to extract useful information when building classifiers or other predictors”.•Phase IV, Classifier: Classify or predict according to the input data.[1] Y. Bengio, A. Courville and P. Vincent, "Representation Learning: A Review and New Perspectives," inIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798-1828, Aug. 2013.Phase II.(Data Preprocessing)

Both Phase II and Phase III aim to properly extract and represent the useful information held
in the raw data collected in Phase I. Both Phase II and Phase III are closely related to feature
engineering. A key difference between Phase II and Phase III is that Phase III is completely
dedicated to representation learning, while Phase II is focused on all the information extraction
and data processing operations that are not based on representation learning.

Running Example: Let’s revisit the aforementioned HDFS. Each recorded event is de-
scribed by unstructured text. In Phase II, the unstructured text is parsed to a data structure
that shows the event type and a list of event variables in (name, value) pairs. Since there are 29
types of events in the HDFS, each event is represented by an integer from 1 to 29 according to
its type. In this way, the aforementioned example event sequence can be transformed to:

22, 5, 5, 7

Phase III.(Representation Learning)

As stated in [7], “Learning representations of the data that make it easier to extract useful

information when building classiﬁers or other predictors.”

Running Example: Let’s revisit the same HDFS. Although DeepLog [8] directly employed
one-hot vectors to represent the event types without representation learning, if we view an event
type as a word in a structured language, one may actually use the word embedding technique
to represent each event type.
It should be noticed that the word embedding technique is a
representation learning technique.

Phase IV.(Classifier Learning)

This phase aims to build speciﬁc classiﬁers or other predictors through Deep Learning.

Running Example: Let’s revisit the same HDFS. DeepLog [8] used Deep Learning to build
a stacked LSTM neural network for anomaly detection. For example, let’s consider event
sequence {22,5,5,5,11,9,11,9,11,9,26,26,26} in which each integer represents the event type of
the corresponding event in the event sequence. Given a window size h = 4, the input sample and
the output label pairs to train DeepLog will be: {22,5,5,5 → 11 }, {5,5,5,11 → 9 }, {5,5,11,9
→ 11 }, and so forth. In the detection stage, DeepLog examines each individual event. It
determines if an event is treated as normal or abnormal according to whether the event’s type
is predicted by the LSTM neural network, given the history of event types. If the event’s type
is among the top g predicted types, the event is treated as normal; otherwise, it is treated as
abnormal.

2.2 Using the four-phase workﬂow framework to summarize some repre-

sentative research works

In this subsection, we use the four-phase workﬂow framework to summarize two representative
works for each security problem. System security includes many sub research topics. However,
not every research topics are suitable to adopt deep learning-based methods due to their intrinsic
characteristics. For these security research subjects that can combine with deep-learning, some
of them has undergone intensive research in recent years, others just emerging. We notice that
there are 5 mainstream research directions in system security. This paper mainly focuses on
system security, so the other mainstream research directions (e.g., deepfake) are out-of-scope.

4

Therefore, we choose these 5 widely noticed research directions, and 3 emerging research
direction in our survey:

1. In security-oriented program analysis, malware classiﬁcation (MC), system-event-based
anomaly detection (SEAD), memory forensics (MF), and defending network attacks,
deep learning based methods have already undergone intensive research.

2. In defending return-oriented programming (ROP) attacks, Control-ﬂow integrity (CFI),

and fuzzing, deep learning based methods are emerging research topics.

We select two representative works for each research topic in our survey. Our criteria to select
papers mainly include: 1) Pioneer (one of the ﬁrst papers in this ﬁeld); 2) Top (published
on top conference or journal); 3) Novelty; 4) Citation (The citation of this paper is high);
5) Effectiveness (the result of this paper is pretty good); 6) Representative (the paper is a
representative work for a branch of the research direction). Table 1 lists the reasons why we
choose each paper, which is ordered according to their importance.

Table 1: List of criteria we used to choose representative work for each research topic.

Order

Paper

RFBNN [9]
EKLAVYA [10]
ROPNN [11]
HeNet [12]
Barnum [13]
CFG-CNN [14]
50b(yte)-CNN [15]
PCNN [16]
Resenberg [17]
DeLaRosa [18]
DeepLog [8]
DeepMem [19]
NeuZZ [20]
Learn & Fuzz [21]

1

2

3

4

Pioneer
Top
Pioneer
Effectiveness
Pioneer
Representative
Novelty
Novelty
Novelty
Novelty
Pioneer
Pioneer
Novelty
Pioneer

Top
Novelty
Novelty
Novelty
Novelty
N/A
Effectiveness
Effectiveness
Effectiveness
Representative
Top
Top
Top
Novelty

Novelty
Citation
Effectiveness
Citation
N/A
N/A
N/A
N/A
Top
N/A
Citations
N/A
Effectiveness
Top

Citations
N/A
N/A
N/A
N/A
N/A
N/A
N/A
Representative
N/A
N/A
N/A
N/A
N/A

The summary for each paper we selected is shown in Table 2. There are three columns
in the table. In the ﬁrst column, we listed eight security problems, including security-oriented
program analysis, defending return-oriented programming (ROP) attacks, control-ﬂow integrity
(CFI), defending network attacks (NA), malware classiﬁcation (MC), system-event-based anomaly
detection (SEAD), memory forensics (MF), and fuzzing for software security. In the second
column, we list the very recent two representative works for each security problem. From the
3-th to 6-th columns, we sequentially describe how the four phases are deployed at each work.
In the “Summary” column, we sequentially describe how the four phases are deployed at each
work, then, we list the evaluation results for each work in terms of accuracy (ACC), precision
(PRC), recall (REC), F1 score (F1), false-positive rate (FPR), and false-negative rate (FNR),
respectively.

5

Table 2: Solutions using Deep Learning for eight security problems. The metrics in the Evaluation column
include accuracy (ACC), precision (PRC), recall (REC), F1 score (F1), false positive rate (FPR), and false
negative rate (FNR).

Works

RFBNN [9]

Security
Problem

Security
Oriented
Program
Analysis [9,
10, 22, 23]

Summary

Phase I

Phase II

Dataset comes from previous paper [24],
consisting of 2200 separate binaries. 2064 of the
binaries were for Linux, obtained from the
coreutils, binutils, and ﬁndutils packages. The
remaining 136 for Windows consist of binaries
from popular open-source projects. Half of the
binaries were for x86, and the other half for
x86-64.

They extract ﬁxed-length subsequences
(1000-byte chunks) from code section of binaries,
Then, use “one-hot encoding”, which converts a
byte into a Z256 vector.

Phase III

N/A

Phase IV

Evaluation

Bi-directional RNN

ACC:98.4% PRE:N/A
F1: 0.98
REC:0.97
FNR:N/A
FPR: N/A

Phase II

Tokenizing the hexadecimal value of each
instruction.

EKLAVYA [10]

Phase I

They adopted source code from previous work [9]
as their rawdata, then obtained two datasets by
using two commonly used compilers: gcc and
clang, with different optimization levels ranging
from O0 to O3 for both x86 and x64. They
obtained the ground truth for the function
arguments by parsing the DWARF debug
information. Next, they extract functions from the
binaries and remove functions which are
duplicates of other functions in the dataset.
Finally, they match caller snipper and callee body.

Phase III

Word2vec technique to compute word
embeddings.

Phase IV

RNN

Evaluation

ACC:81.0% PRE:N/A
F1: N/A
REC:N/A
FNR:N/A
FPR: N/A

Phase II

Form one-hot vector for bytes.

Defending
Return
Oriented
Program-
ming
Attacks
[11, 12, 25]

ROPNN [11]

Phase I

The data is a set of gadget chains obtained from
existing programs. A gadget searching tool,
ROPGadget is used to ﬁnd available gadgets.
Gadgets are chained based on whether the
produced gadget chain is executable on a CPU
emulator. The raw data is represented in
hexadecimal form of instruction sequences.

Phase III

N/A

Phase I

HeNet [12]

Data is acquired from Intel PT, which is a
processor trace tool that can log control ﬂow data.
Taken Not-Taken (TNT) packet and Target IP
(TIP) packet are the two packets of interested.
Logged as binary numbers, information of
executed branches can be obtained from TNT, and
binary executed can be obtained from TIP. Then
the binary sequences are transferred into
sequences of values between 0-255, called pixels,
byte by byte.

Phase IV

1-D CNN

Evaluation

ACC:99.9% PRE:0.99
F1: 0.01
REC:N/A
FNR:N/A
FPR: N/A

Phase II

Given the pixel sequences, slice the whole
sequence and reshape to form sequences of
images for neural network training.

Phase III

Phase IV

Evaluation

Continued on Next Page. . .

6

Barnum [13]

Achieving
Control Flow
Integrity
[13, 14, 25]

Word2vec technique to compute word
embeddings.

DNN

ACC:98.1% PRE:0.99
F1: 0.97
REC:0.96
FNR:0.04
FPR: 0.01

Phase I

Phase II

The raw data, which is the exact sequence of
instructions executed, was generated by
combining the program binary, get immediately
before the program opens a document, and Intel®
PT trace. While Intel® PT built-in ﬁltering options
are set to CR3 and current privilege level (CPL),
which only traces the program activity in the user
space.

The raw instruction sequences are summarized
into Basic Blocks with IDs assigned and are then
sliced into manageable subsequences with a ﬁx
window size 32, founded experimentally. Only
sequences ending on indirect calls, jumps and
returns are analyzed, since control-ﬂow hijacking
attacks always occur there. The label is the next
BBID in the sequence.

Phase III

N/A

Phase IV

LSTM

Evaluation

ACC:N/A% PRE:0.98
F1: 0.98
REC:1.00
FNR:0.02
FPR: 0.98

CFG-CNN [14]

Phase I

Phase II

The raw data is instruction level control-ﬂow
graph constructed from program assembly code by
an algorithm proposed by the authors. While in
the CFG, one vertex corresponds to one
instruction and one directed edge corresponds to
an execution path from one instruction to another.
The program sets for experiments are obtained
from popular programming contest CodeChief.

Phase III

N/A

Since each vertex of the CFG represents an
instruction with complex information that could
be viewed from different aspects, including
instruction name, type, operands etc., a vertex is
represented as the sum of a set of real valued
vectors, corresponding to the number of views
(e.g. addq 32,%rsp is converted to linear
combination of randomly assigned vectors of addq
value, reg). The CFG is then sliced by a set of
ﬁxed size windows sliding through the entire
graph to extract local features on different levels.

Phase IV

Evaluation

DGCNN with different numbers
of views and with or without
operands

ACC:84.1% PRE:N/A
F1: N/A
REC:N/A
FNR:N/A
FPR: N/A

Defending
Network
Attacks [15,
16, 26–30]

50b(yte)-CNN [15]

Phase I

Phase II

Open dataset UNSW-NB15 is used. First,
tcpdump tool is utilised to capture 100 GB of the
raw trafﬁc (i.e. PCAP ﬁles) containing benign
activities and 9 types of attacks. The Argus,
Bro-IDS (now called Zeek) analysis tools are then
used and twelve algorithms are developed to
generate totally 49 features with the class label. In
the end, the total number of data samples is
2,540,044 which are stored in CSV ﬁles.

The ﬁrst 50 bytes of each network trafﬁc ﬂow are
picked out and each is directly used as one feature
input to the neural network.

Phase III

N/A

Phase I

PCCN [16]

Open dataset CICIDS2017, which contains benign
and 14 types of attacks, is used. Background
benign network trafﬁcs are generated by proﬁling
the abstract behavior of human interactions. Raw
data are provided as PCAP ﬁles, and the results of
the network trafﬁc analysis using CICFlowMeter
are pvodided as CSV ﬁles. In the end the dataset
contains 3,119,345 data samples and 83 features
categorized into 15 classes (1 normal + 14
attacks).

Phase IV

Evaluation

CNN with 2 hidden fully
connected layers

ACC:N/A% PRE:N/A
F1: 0.93
REC:N/A
FNR:N/A
FPR: N/A

Phase II

Extract a total of 1,168,671 ﬂow data, including
12 types of attack activities, from original dataset.
Those ﬂow data are then processed and visualized
into grey-scale 2D graphs. The visualization
method is not speciﬁed.

Phase III

N/A

Phase IV

Evaluation

Parallel cross CNN.

ACC:N/A% PRE:0.99
F1: 0.99
REC:N/A
FNR:N/A
FPR: N/A

Continued on Next Page. . .

7

Malware
Classiﬁca-
tion [17, 18,
31–40]

Rosenberg [17]

Phase I

Phase II

The android dataset has the latest malware
families and their variants, each with the same
number of samples. The samples are labeled by
VirusTotal. Then Cuckoo Sandbox is used to
extract dynamic features (API calls) and static
features (string). To avoid some anti-forensic
sample, they applied YARA rule and removed
sequences with less than 15 API calls. After
preprocessing and balance the benign samples
number, the dataset has 400,000 valid samples.

Long sequences cause out of memory during
training LSTM model. So they use sliding window
with ﬁxed size and pad shorter sequences with
zeros. One-hot encoding is applied to API calls.
For static features strings, they deﬁned a vector of
20,000 Boolean values indicating the most
frequent Strings in the entire dataset. If the sample
contain one string, the corresponding value in the
vector will be assigned as 1, otherwise, 0.

Phase III

N/A

Phase IV

Evaluation

They used RNN, BRNN, LSTM,
Deep LSTM, BLSTM, Deep
BLSTM, GRU, bi-directional
GRU, Fully-connected DNN, 1D
CNN in their experiments

ACC:98.3% PRE:N/A
F1: N/A
REC:N/A
FNR:N/A
FPR: N/A

DeLaRosa [18]

Phase I

Phase II

The windows dataset is from Reversing Labs
including XP, 7, 8, and 10 for both 32-bit and
64-bit architectures and gathered over a span of
twelve years (2006-2018). They selected nine
malware families in their dataset and extracted
static features in terms of bytes, basic, and
assembly features.

Phase III

N/A

Phase I

More than 24 million raw log entries with the size
of 2412 MB are recorded from the 203-node
HDFS. Over 11 million log entries with 29 types
are parsed, which are further grouped to 575,061
sessions according to block identiﬁer. These
sessions are manually labeled as normal and
abnormal by HDFS experts. Finally, the
constructed dataset HDFS 575,061 sessions of
logs in the dataset, among which 16,838 sessions
were labeled as anomalous

For bytes-level features, they used a sliding
window to get the histogram of the bytes and
compute the associated entropy in a window; for
basic features, they created a ﬁxed-sized feature
vector given either a list of ASCII strings, or
extracted import and metadata information from
the PE Header(Strings are hashed and calculate a
histogram of these hashes by counting the
occurrences of each value); for assembly features,
the disassembled code generated by Radare2 can
be parsed and transformed into graph-like data
structures such as call graphs, control ﬂow graph,
and instruction ﬂow graph.

Phase IV

N/A

Evaluation

ACC:90.1% PRE:N/A
F1: N/A
REC:N/A
FNR:N/A
FPR: N/A

Phase II

The raw log entries are parsed to different log type
using Spell [46] which is based a longest common
subsequence. There are total 29 log types in
HDFS dataset

DeepLog [8]

System
Event Based
Anomaly
Detection
[8, 41–45]

Phase III

Phase IV

Evaluation

DeepLog directly utilized one-hot
vector to represent 29 log key without
represent learning

A stacked LSTM with two
hidden LSTM layers.

ACC:N/A% PRE:0.95
F1: 0.96
REC:0.96
FNR:N/A
FPR: N/A

LogAnom [41]

Phase I

Phase II

LogAnom also used HDFS dataset, which is same
as DeepLog.

The raw log entries are parsed to different log
templates using FT-Tree [47] according the
frequent combinations of log words. There are
total 29 log templates in HDFS dataset

Phase III

Phase IV

Evaluation

LogAnom employed Word2Vec to
represent the extracted log templates
with more semantic information

Two LSTM layers with 128
neurons

ACC:N/A% PRE:0.97
F1: 0.96
REC:0.94
FNR:N/A
FPR: N/A

Phase I

Continued on Next Page. . .

Phase II

DeepMem [19]

Memory
Forensics
[19, 48–50]

8

400 memory dumps are collected on Windows 7
x86 SP1 virtual machine with simulating various
random user actions and forcing the OS to
randomly allocate objects. The size of each dump
is 1GB.

Construct memory graph from memory dumps,
where each node represents a segment between
two pointers and an edge is created if two nodes
are neighbor

Phase III

Phase IV

Evaluation

Each node is represented by a latent
numeric vector from the embedding
network.

Fully Connected Network (FCN)
with ReLU layer.

ACC:N/A% PRE:0.99
F1: 0.99
REC:0.99
FNR:0.01
FPR: 0.01

MDMF [48]

Phase I

Phase II

Create a dataset of benign host memory snapshots
running normal, non-compromised software,
including software that executes in many of the
malicious snapshots. The benign snapshot is
extracted from memory after ample time has
passed for the chosen programs to open. By
generating samples in parallel to the separate
malicious environment, the benign memory
snapshot dataset created.

Various representation for the memory snapshots
including byte sequence and image, without
relying on domain-knowledge of the OS.

Phase III

N/A

Phase IV

Evaluation

Recurrent Neural Network with
LSTM cells and Convolutional
Neural Network composed of
multiple layers, including
pooling and fully connected
layers. for image data

ACC:98.0% PRE:N/A
F1: N/A
REC:N/A
FNR:N/A
FPR: N/A

Fuzzing [20,
21, 51–53]

L-Fuzz [21]

Phase I

The raw data are about 63,000 non-binary PDF
objects, sliced in ﬁx size, extracted from 534 PDF
ﬁles that are provided by Windows fuzzing team
and are previously used for prior extended fuzzing
of Edge PDF parser.

Phase III

N/A

Phase I

NEUZZ [20]

Phase IV

Char-RNN

For each program tested, the raw data is collected
by running AFL-2.52b on a single core machine
for one hour. The training data are byte level input
ﬁles generated by AFL, and the labels are bitmaps
corresponding to input ﬁles. For experiments,
NEUZZ is implemented on 10 real-world
programs, the LAVA-M bug dataset, and the CGC
dataset.

Phase II

N/A

Phase II

N/A

Evaluation

ACC:N/A% PRE:N/A
F1: 0.93
REC:N/A
FNR:N/A
FPR: N/A

Phase III

N/A

Phase IV

NN

Evaluation

ACC:N/A% PRE:N/A
F1: 0.93
REC:N/A
FNR:N/A
FPR: N/A

1 Deep Learning metrics are often not available in fuzzing papers. Typical fuzzing metrics used for evaluations are: code coverage, pass rate

and bugs.

2.3 Methodology for reviewing the existing works

Data representation (or feature engineering) plays an important role in solving security prob-
lems with Deep Learning. This is because data representation is a way to take advantage of
human ingenuity and prior knowledge to extract and organize the discriminative information
from the data. Many efforts in deploying machine learning algorithms in security domain ac-
tually goes into the design of preprocessing pipelines and data transformations that result in a

9

representation of the data to support effective machine learning.

In order to expand the scope and ease of applicability of machine learning in security do-
main, it would be highly desirable to ﬁnd a proper way to represent the data in security domain,
which can entangle and hide more or less the different explanatory factors of variation behind
the data. To let this survey adequately reﬂect the important role played by data representation,
our review will focus on how the following three questions are answered by the existing works:

• Question 1: Is Phase II pervasively done in the literature? When Phase II is skipped in a

work, are there any particular reasons?

• Question 2: Is Phase III employed in the literature? When Phase III is skipped in a work,

are there any particular reasons?

• Question 3: When solving different security problems, is there any commonality in
terms of the (types of) classiﬁers learned in Phase IV? Among the works solving the
same security problem, is there dissimilarity in terms of classiﬁers learned in Phase IV?

Phase III

c o n s i d e r a t i o n
no consideration

n o a d o p t i o n

adoption

class 1

class 2

n o c o m p a r i s o n

class 3

comparison

class 4

Figure 2: Classiﬁcation tree for different Phase III methods. Here, consideration,
adoption, and comparison indicate that a work considers Phase III, adopts Phase III and
makes comparison with other methods, respectively.

To group the Phase III methods at different applications of Deep Learning in solving the
same security problem, we introduce a classiﬁcation tree as shown in Figure 2. The classiﬁ-
cation tree categorizes the Phase III methods in our selected survey works into four classes.
First, class 1 includes the Phase III methods which do not consider representation learning.
Second, class 2 includes the Phase III methods which consider representation learning but, do
not adopt it. Third, class 3 includes the Phase III methods which consider and adopt repre-
sentation learning but, do not compare the performance with other methods. Finally, class 4
includes the Phase III methods which consider and adopt representation learning and, compare
the performance with other methods.

In the remaining of this paper, we take a closer look at how each of the eight security

problems is being solved by applications of Deep Learning in the literature.

3 A closer look at applications of Deep Learning in solving

security-oriented program analysis challenges

3.1 Introduction

Recent years, security-oriented program analysis is widely used in software security. For ex-
ample, symbolic execution and taint analysis are used to discover, detect and analyze vulner-
abilities in programs. Control ﬂow analysis, data ﬂow analysis and pointer/alias analysis are

10

important components when enforcing many secure strategies, such as control ﬂow integrity,
data ﬂow integrity and doling dangling pointer elimination. Reverse engineering was used by
defenders and attackers to understand the logic of a program without source code.

In the security-oriented program analysis, there are many open problems, such as precise
pointer/alias analysis, accurate and complete reversing engineer, complex constraint solving,
program de-obfuscation, and so on. Some problems have theoretically proven to be NP-hard,
and others still need lots of human effort to solve. Either of them needs a lot of domain knowl-
edge and experience from expert to develop better solutions. Essentially speaking, the main
challenges when solving them through traditional approaches are due to the sophisticated rules
between the features and labels, which may change in different contexts. Therefore, on the one
hand, it will take a large quantity of human effort to develop rules to solve the problems, on the
other hand, even the most experienced expert cannot guarantee completeness. Fortunately, the
deep learning method is skillful to ﬁnd relations between features and labels if given a large
amount of training data. It can quickly and comprehensively ﬁnd all the relations if the training
samples are representative and effectively encoded.

In this section, we will review the very recent four representative works that use Deep
Learning for security-oriented program analysis. We observed that they focused on different
goals. Shin, et al. designed a model [9] to identify the function boundary. EKLAVYA [10]
was developed to learn the function type. Gemini [23] was proposed to detect similarity among
functions. DEEPVSA [22] was designed to learn memory region of an indirect addressing from
the code sequence. Among these works, we select two representative works [9, 10] and then,
summarize the analysis results in Table 2 in detail.

Our review will be centered around three questions described in Section 2.3. In the re-
maining of this section, we will ﬁrst provide a set of observations, and then we provide the
indications. Finally, we provide some general remarks.

3.2 Key ﬁndings from a closer look

From a close look at the very recent applications using Deep Learning for solving security-
oriented program analysis challenges, we observed the followings:

Observation 3.1: All of the works in our survey used binary ﬁles as their raw data.

Phase II in our survey had one similar and straightforward goal – extracting code se-
quences from the binary. Difference among them was that the code sequence was ex-
tracted directly from the binary ﬁle when solving problems in static program analysis,
while it was extracted from the program execution when solving problems in dynamic
program analysis.

∗Observation 3.2: Most data representation methods generally took into account the
domain knowledge.

Most data representation methods generally took into the domain knowledge, i.e., what
kind of information they wanted to reserve when processing their data. Note that the
feature selection has a wide inﬂuence on Phase II and Phase III, for example, embedding
granularities, representation learning methods. Gemini [23] selected function level fea-
ture and other works in our survey selected instruction level feature. To be speciﬁcally,
all the works except Gemini [23] vectorized code sequence on instruction level.

11

Observation 3.3: To better support data representation for high performance, some
works adopted representation learning.

For instance, DEEPVSA [22] employed a representation learning method, i.e., bi-directional
LSTM, to learn data dependency within instructions. EKLAVYA [10] adopted represen-
tation learning method, i.e., word2vec technique, to extract inter-instruciton information.
It is worth noting that Gemini [23] adopts the Structure2vec embedding network in its
siamese architecture in Phase IV (see details in Observation 3.7). The Structure2vec
embedding network learned information from an attributed control ﬂow graph.

Observation 3.4: According to our taxonomy, most works in our survey were classiﬁed
into class 4.

To compare the Phase III, we introduced a classiﬁcation tree with three layers as shown
in Figure 2 to group different works into four categories. The decision tree grouped our
surveyed works into four classes according to whether they considered representation
learning or not, whether they adopted representation learning or not, and whether they
compared their methods with others’, respectively, when designing their framework. Ac-
cording to our taxonomy, EKLAVYA [10], DEEPVSA [22] were grouped into class 4
shown in Figure 2. Also, Gemini’s work [23] and Shin, et al.’s work [9] belonged to class
1 and class 2 shown in Figure 2, respectively.

Observation 3.5: All the works in our survey explain why they adopted or did not adopt
one of representation learning algorithms.

Two works in our survey adopted representation learning for different reasons: to en-
hance model’s ability of generalization [10]; and to learn the dependency within instruc-
tions [22].
It is worth noting that Shin, et al. did not adopt representation learning
because they wanted to preserve the “attractive” features of neural networks over other
machine learning methods – simplicity. As they stated, “ﬁrst, neural networks can learn
directly from the original representation with minimal preprocessing (or “feature engi-
neering”) needed.” and “second, neural networks can learn end-to-end, where each of
its constituent stages are trained simultaneously in order to best solve the end goal.”
Although Gemini [23] did not adopt representation learning when processing their raw
data, the Deep Learning models in siamese structure consisted of two graph embedding
networks and one cosine function.

∗Observation 3.6: The analysis results showed that a suitable representation learning
method could improve accuracy of Deep Learning models.

DEEPVSA [22] designed a series of experiments to evaluate the effectiveness of its repre-
sentative method. By combining with the domain knowledge, EKLAVYA [10] employed
t-SNE plots and analogical reasoning to explain the effectiveness of their representation
learning method in an intuitive way.

∗Observation 3.7: Various Phase IV methods were used.

In Phase IV, Gemini [23] adopted siamese architecture model which consisted of two
Structure2vec embedding networks and one cosine function. The siamese architecture
took two functions as its input, and produced the similarity score as the output. The other
three works [9, 10, 22] adopted bi-directional RNN, RNN, bi-directional LSTM respec-
tively. Shin, et al. adopted bi-directional RNN because they wanted to combine both

12

the past and the future information in making a prediction for the present instruction [9].
DEEPVSA [22] adopted bi-directional RNN to enable their model to infer memory re-
gions in both forward and backward ways.

The above observations seem to indicate the following indications:

Indication 3.1: Phase III is not always necessary.

Not all authors regard representation learning as a good choice even though some case
experiments show that representation learning can improve the ﬁnal results. They value
more the simplicity of Deep Learning methods and suppose that the adoption of repre-
sentation learning weakens the simplicity of Deep Learning methods.

Indication 3.2: Even though the ultimate objective of Phase III in the four surveyed
works is to train a model with better accuracy, they have different speciﬁc motivations as
described in Observation 3.5.

When authors choose representation learning, they usually try to convince people the
effectiveness of their choice by empirical or theoretical analysis.

∗Indication 3.3: 3.7 indicates that authors usually refer to the domain knowledge when
designing the architecture of Deep Learning model.

For instance, the works we reviewed commonly adopt bi-directional RNN when their
prediction partly based on future information in data sequence.

3.3 Discussion

Despite the effectiveness and agility of deep learning-based methods, there are still some chal-
lenges in developing a scheme with high accuracy due to the hierarchical data structure, lots
of noisy, and unbalanced data composition in program analysis. For instance, an instruction
sequence, a typical data sample in program analysis, contains three-level hierarchy: sequence–
instruction–opcode/operand. To make things worse, each level may contain many different
structures, e.g., one-operand instructions, multi-operand instructions, which makes it harder to
encode the training data.

4 A closer look at applications of Deep Learning in defend-

ing ROP attacks

4.1 Introduction

Return-oriented programming (ROP) attack is one of the most dangerous code reuse attacks,
which allows the attackers to launch control-ﬂow hijacking attack without injecting any ma-
licious code. Rather, It leverages particular instruction sequences (called “gadgets”) widely
existing in the program space to achieve Turing-complete attacks [54]. Gadgets are instruction
sequences that end with a RET instruction. Therefore, they can be chained together by spec-
ifying the return addresses on program stack. Many traditional techniques could be used to
detect ROP attacks, such as control-ﬂow integrity (CFI [55]), but many of them either have low

13

detection rate or have high runtime overhead. ROP payloads do not contain any codes. In other
words, analyzing ROP payload without the context of the program’s memory dump is mean-
ingless. Thus, the most popular way of detecting and preventing ROP attacks is control-ﬂow
integrity. The challenge after acquiring the instruction sequences is that it is hard to recognize
whether the control ﬂow is normal. Traditional methods use the control ﬂow graph (CFG) to
identify whether the control ﬂow is normal, but attackers can design the instruction sequences
which follow the normal control ﬂow deﬁned by the CFG. In essence, it is very hard to design
a CFG to exclude every single possible combination of instructions that can be used to launch
ROP attacks. Therefore, using data-driven methods could help eliminate such problems.

In this section, we will review the very recent three representative works that use Deep
Learning for defending ROP attacks: ROPNN [11], HeNet [12] and DeepCheck [25]. ROPNN
[11] aims to detect ROP attacks, HeNet [12] aims to detect malware using CFI, and DeepCheck
[25] aims at detecting all kinds of code reuse attacks.

Speciﬁcally, ROPNN is to protect one single program at a time, and its training data are
generated from real-world programs along with their execution. Firstly, it generates its be-
nign and malicious data by “chaining-up” the normally executed instruction sequences and
“chaining-up” gadgets with the help of gadgets generation tool, respectively, after the memory
dumps of programs are created. Each data sample is byte-level instruction sequence labeled as
“benign” or “malicious”. Secondly, ROPNN will be trained using both malicious and benign
data. Thirdly, the trained model is deployed to a target machine. After the protected program
started, the executed instruction sequences will be traced and fed into the trained model, the
protected program will be terminated once the model found the instruction sequences are likely
to be malicious.

HeNet is also proposed to protect a single program. Its malicious data and benign data
are generated by collecting trace data through Intel PT from malware and normal software,
respectively. Besides, HeNet preprocesses its dataset and shape each data sample in the format
of image, so that they could implement transfer learning from a model pre-trained on ImageNet.
Then, HeNet is trained and deployed on machines with features of Intel PT to collect and
classify the program’s execution trace online.

The training data for DeepCheck are acquired from CFGs, which are constructed by dissem-
bling the programs and using the information from Intel PT. After the CFG for a protected pro-
gram is constructed, authors sample benign instruction sequences by chaining up basic blocks
that are connected by edges, and sample malicious instruction sequences by chaining up those
that are not connected by edges. Although a CFG is needed during training, there is no need
to construct CFG after the training phase. After deployed, instruction sequences will be con-
structed by leveraging Intel PT on the protected program. Then the trained model will classify
whether the instruction sequences are malicious or benign.

We observed that none of the works considered Phase III, so all of them belong to class
1 according to our taxonomy as shown in Figure 2. The analysis results of ROPNN [11] and
HeNet [12] are shown in Table 2. Also, we observed that three works had different goals.

Our review will be centered around three questions described in Section 2.3. In the re-
maining of this section, we will ﬁrst provide a set of observations, and then we provide the
indications. Finally, we provide some general remarks.

4.2 Key ﬁndings from a closer look

From a close look at the very recent applications using Deep Learning for defending return-
oriented programming attacks, we observed the followings:

14

Observation 4.1: All the works [11, 12, 25] in this survey focused on data generation
and acquisition.

In ROPNN [11], both malicious samples (gadget chains) were generated using an auto-
mated gadget generator (i.e. ROPGadget [56]) and a CPU emulator (i.e. Unicorn [57]).
ROPGadget was used to extract instruction sequences that could be used as gadgets from
a program, and Unicorn was used to validate the instruction sequences. Correspond-
ing benign sample (gadget-chain-like instruction sequences) were generated by disas-
sembling a set of programs. In DeepCheck [25] refers to the key idea of control-ﬂow
integrity [55]. It generates program’s run-time control ﬂow through new feature of In-
tel CPU (Intel Processor Tracing), then compares the run-time control ﬂow with the
program’s control-ﬂow graph (CFG) that generates through static analysis. Benign in-
struction sequences are that with in the program’s CFG, and vice versa. In HeNet [12],
program’s execution trace was extracted using the similar way as DeepCheck. Then,
each byte was transformed into a pixel with an intensity between 0-255. Known malware
samples and benign software samples were used to generate malicious data benign data,
respectively.

Observation 4.2: None of the ROP works in this survey deployed Phase III.

Both ROPNN [11] and DeepCheck [25] used binary instruction sequences for training.
In ROPNN [11], one byte was used as the very basic element for data pre-processing.
Bytes were formed into one-hot matrices and ﬂattened for 1-dimensional convolutional
layer. In DeepCheck [25], half-byte was used as the basic unit. Each half-byte (4 bits)
was transformed to decimal form ranging from 0-15 as the basic element of the input
vector, then was fed into a fully-connected input layer. On the other hand, HeNet [12]
used different kinds of data. By the time this survey has been drafted, the source code of
HeNet was not available to public and thus, the details of the data pre-processing was not
be investigated. However, it is still clear that HeNet used binary branch information col-
lected from Intel PT rather than binary instructions. In HeNet, each byte was converted
to one decimal number ranging from 0 to 255. Byte sequences was sliced and formed
into image sequences (each pixel represented one byte) for a fully-connected input layer.

Observation 4.3: Fully-connected neural network was widely used.

Only ROPNN [11] used 1-dimensional convolutional neural network (CNN) when ex-
tracting features. Both HeNet [12] and DeepCheck [25] used fully-connected neural
network (FCN). None of the works used recurrent neural network (RNN) and the vari-
ants.

The above observations seem to indicate the following indications:

• Indication 4.1: It seems like that one of the most important factors in ROP problem is

feature selection and data generation.

All three works use very different methods to collect/generate data, and all the authors
provide very strong evidences and/or arguments to justify their approaches. ROPNN [11]
was trained by the malicious and benign instruction sequences. However, there is no
clear boundary between benign instruction sequences and malicious gadget chains. This
weakness may impair the performance when applying ROPNN to real world ROP attacks.

15

As oppose to ROPNN, DeepCheck [25] utilizes CFG to generate training basic-block se-
quences. However, since the malicious basic-block sequences are generated by randomly
connecting nodes without edges, it is not guaranteed that all the malicious basic-blocks
are executable. HeNet [12] generates their training data from malware. Technically,
HeNet could be used to detect any binary exploits, but their experiment focuses on ROP
attack and achieves 100% accuracy. This shows that the source of data in ROP problem
does not need to be related to ROP attacks to produce very impressive results.

• Indication 4.2: Representation learning seems not critical when solving ROP problems

using Deep Learning.

Minimal process on data in binary form seems to be enough to transform the data into
a representation that is suitable for neural networks. Certainly, it is also possible to
represent the binary instructions at a higher level, such as opcodes, or use embedding
learning. However, as stated in [11], it appears that the performance will not change
much by doing so. The only beneﬁt of representing input data to a higher level is to
reduce irrelevant information, but it seems like neural network by itself is good enough
at extracting features.

• Indication 4.3: Different Neural network architecture does not have much inﬂuence on

the effectiveness of defending ROP attacks.

Both HeNet [12] and DeepCheck [25] utilizes standard DNN and achieved comparable
results on ROP problems. One can infer that the input data can be easily processed by
neural networks, and the features can be easily detected after proper pre-process.

It is not surprising that researchers are not very interested in representation learning for
ROP problems as stated in Observation 4.1. Since ROP attack is focus on the gadget chains,
it is straightforward for the researcher to choose the gadgets as their training data directly. It
is easy to map the data into numerical representation with minimal processing. An example
is that one can map binary executable to hexadecimal ASCII representation, which could be a
good representation for neural network.

Instead, researchers focus more in data acquisition and generation. In ROP problems, the
amount of data is very limited. Unlike malware and logs, ROP payloads normally only con-
tain addresses rather than codes, which do not contain any information without providing the
instructions in corresponding addresses. It is thus meaningless to collect all the payloads. At
the best of our knowledge, all the previous works use pick instruction sequences rather than
payloads as their training data, even though they are hard to collect.

4.3 Discussion

Even though, Deep Learning based method does not face the challenge to design a very com-
plex ﬁne-grained CFG anymore, it suffers from a limited number of data sources. Generally,
Deep Learning based method requires lots of training data. However, real-world malicious
data for the ROP attack is very hard to ﬁnd, because comparing with benign data, malicious
data need to be carefully crafted and there is no existing database to collect all the ROP at-
tacks. Without enough representative training set, the accuracy of the trained model cannot be
guaranteed.

16

5 A closer look at applications of Deep Learning in achieving

CFI

5.1 Introduction

The basic ideas of control-ﬂow integrity (CFI) techniques, proposed by Abadi in 2005 [55],
could be dated back to 2002, when Vladimir and his fellow researchers proposed an idea called
program shepherding [58], a method of monitoring the execution ﬂow of a program when it is
running by enforcing some security policies. The goal of CFI is to detect and prevent control-
ﬂow hijacking attacks, by restricting every critical control ﬂow transfers to a set that can only
appear in correct program executions, according to a pre-built CFG. Traditional CFI techniques
typically leverage some knowledge, gained from either dynamic or static analysis of the target
program, combined with some code instrumentation methods, to ensure the program runs on a
correct track.

However, the problems of traditional CFI are: (1) Existing CFI implementations are not
compatible with some of important code features [59]; (2) CFGs generated by static, dynamic
or combined analysis cannot always be precisely completed due to some open problems [60];
(3) There always exist certain level of compromises between accuracy and performance over-
head and other important properties [61, 62]. Recent research has proposed to apply Deep
Learning on detecting control ﬂow violation. Their result shows that, compared with tra-
ditional CFI implementation, the security coverage and scalability were enhanced in such a
fashion [13]. Therefore, we argue that Deep Learning could be another approach which re-
quires more attention from CFI researchers who aim at achieving control-ﬂow integrity more
efﬁciently and accurately.

In this section, we will review the very recent three representative papers that use Deep
Learning for achieving CFI. Among the three, two representative papers [13, 14] are already
summarized phase-by-phase in Table 2. We refer to interested readers the Table 2 for a concise
overview of those two papers.

Our review will be centered around three questions described in Section 2.3. In the re-
maining of this section, we will ﬁrst provide a set of observations, and then we provide the
indications. Finally, we provide some general remarks.

5.2 Key ﬁndings from a closer look

From a close look at the very recent applications using Deep Learning for achieving control-
ﬂow integrity, we observed the followings:

Observation 5.1: None of the related works realize preventive1 prevention of control
ﬂow violation.

After doing a thorough literature search, we observed that security researchers are quite
behind the trend of applying Deep Learning techniques to solve security problems. Only
one paper has been founded by us, using Deep Learning techniques to directly enhance
the performance of CFI [13]. This paper leveraged Deep Learning to detect docu-
ment malware through checking program’s execution traces that generated by hardware.
Speciﬁcally, the CFI violations were checked in an ofﬂine mode. So far, no works have
realized Just-In-Time checking for program’s control ﬂow.

1We refer readers to [62] which systemizes the knowledge of protections by CFI schemes.

17

In order to provide more insightful results, in this section, we try not to narrow down
our focus on CFI detecting attacks at run-time, but to extend our scope to papers that
take good use of control ﬂow related data, combined with Deep Learning techniques
[14, 63]. In one work, researchers used self-constructed instruction-level CFG to detect
program defection [14]. In another work, researchers used lazy-binding CFG to detect
sophisticated malware [63].

Observation 5.2: Diverse raw data were used for evaluating CFI solutions.

In all surveyed papers, there are two kinds of control ﬂow related data being used: pro-
gram instruction sequences and CFGs. Barnum et al. [13] employed statically and dy-
namically generated instruction sequences acquired by program disassembling and Intel®
Processor Trace. CNNoverCFG [14] used self-designed algorithm to construct instruc-
tion level control-ﬂow graph. Minh Hai Nguyen et al. [63] used proposed lazy-binding
CFG to reﬂect the behavior of malware DEC.

Observation 5.3: All the papers in our survey adopted Phase II.

All the related papers in our survey employed Phase II to process their raw data before
sending them into Phase III. In Barnum [13], the instruction sequences from program run-
time tracing were sliced into basic-blocks. Then, they assigned each basic-blocks with
an unique basic-block ID (BBID). Finally, due to the nature of control-ﬂow hijacking
attack, they selected the sequences ending with indirect branch instruction (e.g., indirect
call/jump, return and so on) as the training data. In CNNoverCFG [14], each of instruc-
tions in CFG were labeled with its attributes in multiple perspectives, such as opcode,
operands, and the function it belongs to. The training data is generated are sequences
generated by traversing the attributed control-ﬂow graph. Nguyen and others [63] con-
verted the lazy-binding CFG to corresponding adjacent matrix and treated the matrix as
a image as their training data.

Observation 5.4: All the papers in our survey did not adopt Phase III.

We observed all the papers we surveyed did not adopted Phase III. Instead, they adopted
the form of numerical representation directly as their training data. Speciﬁcally, Barnum
[13] grouped the the instructions into basic-blocks, then represented basic-blocks with
uniquely assigning IDs.
In CNNoverCFG [14], each of instructions in the CFG was
represented by a vector that associated with its attributes. Nguyen and others directly
used the hashed value of bit string representation.

Observation 5.5: Various Phase IV models were used.

Barnum [13] utilized BBID sequence to monitor the execution ﬂow of the target program,
which is sequence-type data. Therefore, they chose LSTM architecture to better learn
the relationship between instructions. While in the other two papers [14,63], they trained
CNN and directed graph-based CNN to extract information from control-ﬂow graph and
image, respectively.

The above observations seem to indicate the following indications:

Indication 5.1: All the existing works did not achieve Just-In-Time CFI violation detec-
tion.

18

It is still a challenge to tightly embed Deep Learning model in program execution. All
existing work adopted lazy-checking – checking the program’s execution trace following
its execution.

Indication 5.2: There is no uniﬁed opinion on how to generate malicious sample.

Data are hard to collect in control-ﬂow hijacking attacks. The researchers must carefully
craft malicious sample. It is not clear whether the “handcrafted” sample can reﬂect the
nature the control-ﬂow hijacking attack.

∗Indication 5.3: The choice of methods in Phase II are based on researchers’ security
domain knowledge.

5.3 Discussion

The strength of using deep learning to solve CFI problems is that it can avoid the complicated
processes of developing algorithms to build acceptable CFGs for the protected programs. Com-
pared with the traditional approaches, the DL based method could prevent CFI designer from
studying the language features of the targeted program and could also avoid the open problem
(pointer analysis) in control ﬂow analysis. Therefore, DL based CFI provides us a more gen-
eralized, scalable, and secure solution. However, since using DL in CFI problem is still at an
early age, which kinds of control-ﬂow related data are more effective is still unclear yet in this
research area. Additionally, applying DL in real-time control-ﬂow violation detection remains
an untouched area and needs further research.

6 A closer look at applications of Deep Learning in defend-

ing network attacks

6.1 Introduction

Network security is becoming more and more important as we depend more and more on net-
works for our daily lives, works and researches. Some common network attack types include
probe, denial of service (DoS), Remote-to-local (R2L), etc. Traditionally, people try to detect
those attacks using signatures, rules, and unsupervised anomaly detection algorithms. How-
ever, signature based methods can be easily fooled by slightly changing the attack payload;
rule based methods need experts to regularly update rules; and unsupervised anomaly detec-
tion algorithms tend to raise lots of false positives. Recently, people are trying to apply Deep
Learning methods for network attack detection.

In this section, we will review the very recent seven representative works that use Deep
Learning for defending network attacks. [15,27,29] build neural networks for multi-class classi-
ﬁcation, whose class labels include one benign label and multiple malicious labels for different
attack types. [16] ignores normal network activities and proposes parallel cross convolutional
neural network (PCCN) to classify the type of malicious network activities. [26] applies Deep
Learning to detecting a speciﬁc attack type, distributed denial of service (DDoS) attack. [28,30]
explores both binary classiﬁcation and multi-class classiﬁcation for benign and malicious ac-
tivities. Among these seven works, we select two representative works [15, 16] and summarize
the main aspects of their approaches regarding whether the four phases exist in their works, and

19

what exactly do they do in the Phase if it exists. We direct interested readers to Table 2 for a
concise overview of these two works.

Our review will be centered around three questions described in Section 2.3. In the re-
maining of this section, we will ﬁrst provide a set of observations, and then we provide the
indications. Finally, we provide some general remarks.

6.2 Key ﬁndings from a closer look

From a close look at the very recent applications using Deep Learning for solving network
attack challenges, we observed the followings:

Observation 6.1: All the seven works in our survey used public datasets, such as UNSW-
NB15 [64] and CICIDS2017 [65].

The public datasets were all generated in test-bed environments, with unbalanced sim-
ulated benign and attack activities. For attack activities, the dataset providers launched
multiple types of attacks, and the numbers of malicious data for those attack activities
were also unbalanced.

Observation 6.2: The public datasets were given into one of two data formats, i.e.,
PCAP and CSV.

One was raw PCAP or parsed CSV format, containing network packet level features, and
the other was also CSV format, containing network ﬂow level features, which showed
the statistic information of many network packets. Out of all the seven works, [26, 27]
used packet information as raw inputs, [16, 28–30] used ﬂow information as raw inputs,
and [15] explored both cases.

In order to parse the raw inputs, preprocessing methods, including
Observation 6.3:
one-hot vectors for categorical texts, normalization on numeric data, and removal of
unused features/data samples, were commonly used.

Commonly removed features include IP addresses and timestamps. [30] also removed
port numbers from used features. By doing this, they claimed that they could “avoid
over-ﬁtting and let the neural network learn characteristics of packets themselves”. One
outlier was that, when using packet level features in one experiment, [15] blindly chose
the ﬁrst 50 bytes of each network packet without any feature extracting processes and fed
them into neural network.

Observation 6.4: Using image representation improved the performance of security
solutions using Deep Learning.

After preprocessing the raw data, while [16] transformed the data into image representa-
tion, [26–30] directly used the original vectors as an input data. Also, [15] explored both
cases and reported better performance using image representation.

Observation 6.5: None of all the seven surveyed works considered representation learn-
ing.

All the seven surveyed works belonged to class 1 shown in Figure 2. They either directly
used the processed vectors to feed into the neural networks, or changed the representation

20

without explanation. One research work [15] provided a comparison on two different
representations (vectors and images) for the same type of raw input. However, the other
works applied different preprocessing methods in Phase II. That is, since the different
preprocessing methods generated different feature spaces, it was difﬁcult to compare the
experimental results.

Observation 6.6: Binary classiﬁcation model showed better results from most experi-
ments.

Among all the seven surveyed works, [26] focused on one speciﬁc attack type and only
did binary classiﬁcation to classify whether the network trafﬁc was benign or malicious.
Also, [15, 16, 27, 29] included more attack types and did multi-class classiﬁcation to
classify the type of malicious activities, and [28, 30] explored both cases. As for multi-
class classiﬁcation, the accuracy for selective classes was good, while accuracy for other
classes, usually classes with much fewer data samples, suffered by up to 20% degrada-
tion.

Observation 6.7: Data representation inﬂuenced on choosing a neural network model.

The above observations seem to indicate the following indications:

Indication 6.1: All works in our survey adopt a kind of preprocessing methods in
Phase II, because raw data provided in the public datasets are either not ready for neural
networks, or that the quality of data is too low to be directly used as data samples.

Preprocessing methods can help increase the neural network performance by improving
the data samples’ qualities. Furthermore, by reducing the feature space, pre-processing
can also improve the efﬁciency of neural network training and testing. Thus, Phase II
should not be skipped.
If Phase II is skipped, the performance of neural network is
expected to go down considerably.

Indication 6.2: Although Phase III is not employed in any of the seven surveyed works,
none of them explains a reason for it. Also, they all do not take representation learning
into consideration.

Indication 6.3: Because no work uses representation learning, the effectiveness are not
well-studied.

Out of other factors, it seems that the choice of pre-processing methods has the largest
impact, because it directly affects the data samples fed to the neural network.

Indication 6.4: There is no guarantee that CNN also works well on images converted
from network features.

Some works that use image data representation use CNN in Phase IV. Although CNN
has been proven to work well on image classiﬁcation problem in the recent years, there
is no guarantee that CNN also works well on images converted from network features.

From the observations and indications above, we hereby present two recommendations: (1)
Researchers can try to generate their own datasets for the speciﬁc network attack they want
to detect. As stated, the public datasets have highly unbalanced number of data for different

21

classes. Doubtlessly, such unbalance is the nature of real world network environment, in which
normal activities are the majority, but it is not good for Deep Learning. [27] tries to solve this
problem by oversampling the malicious data, but it is better to start with a balanced data set.
(2) Representation learning should be taken into consideration. Some possible ways to apply
representation learning include: (a) apply word2vec method to packet binaries, and categori-
cal numbers and texts; (b) use K-means as one-hot vector representation instead of randomly
encoding texts. We suggest that any change of data representation may be better justiﬁed by
explanations or comparison experiments.

6.3 Discussion

One critical challenge in this ﬁeld is the lack of high-quality data set suitable for applying
deep learning. Also, there is no agreement on how to apply domain knowledge into training
deep learning models for network security problems. Researchers have been using different
pre-processing methods, data representations and model types, but few of them have enough
explanation on why such methods/representations/models are chosen, especially for data rep-
resentation.

7 A closer look at applications of Deep Learning in malware

classiﬁcation

7.1 Introduction

The goal of malware classiﬁcation is to identify malicious behaviors in software with static and
dynamic features like control-ﬂow graph and system API calls. Malware and benign programs
can be collected from open datasets and online websites. Both the industry and the academic
communities have provided approaches to detect malware with static and dynamic analyses.
Traditional methods such as behavior-based signatures, dynamic taint tracking, and static data
ﬂow analysis require experts to manually investigate unknown ﬁles. However, those hand-
crafted signatures are not sufﬁciently effective because attackers can rewrite and reorder the
malware. Fortunately, neural networks can automatically detect large-scale malware variants
with superior classiﬁcation accuracy.

In this section, we will review the very recent twelve representative works that use Deep
Learning for malware classiﬁcation [17, 18, 31–40]. [18] selects three different kinds of static
features to classify malware. [31–33] also use static features from the PE ﬁles to classify pro-
grams. [34] extracts behavioral feature images using RNN to represent the behaviors of origi-
nal programs. [35] transforms malicious behaviors using representative learning without neural
network. [36] explores RNN model with the API calls sequences as programs’ features. [37,38]
skip Phase II by directly transforming the binary ﬁle to image to classify the ﬁle. [17, 39] ap-
plies dynamic features to analyze malicious features. [40] combines static features and dynamic
features to represent programs’ features. Among these works, we select two representative
works [17, 18] and identify four phases in their works shown as Table 2.

Our review will be centered around three questions described in Section 2.3. In the re-
maining of this section, we will ﬁrst provide a set of observations, and then we provide the
indications. Finally, we provide some general remarks.

22

7.2 Key ﬁndings from a closer look

From a close look at the very recent applications using Deep Learning for solving malware
classiﬁcation challenges, we observed the followings:

Observation 7.1: Features selected in malware classiﬁcation were grouped into three
categories: static features, dynamic features, and hybrid features.

Typical static features include metadata, PE import Features, Byte/Entorpy, String, and
Assembly Opcode Features derived from the PE ﬁles [31–33]. De LaRosa, Kilgallon,
et al. [18] took three kinds of static features: byte-level, basic-level ( strings in the ﬁle,
the metadata table, and the import table of the PE header), and assembly features-level.
Some works directly considered binary code as static features [37, 38].

Different from static features, dynamic features were extracted by executing the ﬁles to
retrieve their behaviors during execution. The behaviors of programs, including the API
function calls, their parameters, ﬁles created or deleted, websites and ports accessed, etc,
were recorded by a sandbox as dynamic features [39]. The process behaviors includ-
ing operation name and their result codes were extracted [34]. The process memory,
tri-grams of system API calls and one corresponding input parameter were chosen as dy-
namic features [35]. An API calls sequence for an APK ﬁle was another representation
of dynamic features [17, 36].

Static features and dynamic features were combined as hybrid features [40]. For static
features, Xu and others in [40] used permissions, networks, calls, and providers, etc. For
dynamic features, they used system call sequences.

Observation 7.2:
needed to be vertorized for Deep Learning models.

In most works, Phase II was inevitable because extracted features

One-hot encoding approach was frequently used to vectorize features [17, 32–34, 36].
Bag-of-words (BoW) and n-gram were also considered to represent features [36]. Some
works brought the concepts of word frequency in NLP to convert the sandbox ﬁle to
ﬁxed-size inputs [39]. Hashing features into a ﬁxed vector was used as an effective
method to represent features [31]. Bytes histogram using the bytes analysis and bytes-
entropy histogram with a sliding window method were considered [18]. In [18], De La
Rosa and others embeded strings by hashing the ASCII strings to a ﬁxed-size feature
vector. For assembly features, they extracted four different levels of granularity: opera-
tion level (instruction-ﬂow-graph), block level (control-ﬂow-graph), function level (call-
graph), and global level (graphs summarized). bigram, trigram and four-gram vectors
and n-gram graph were used for the hybrid features [40].

Observation 7.3: Most Phase III methods were classiﬁed into class 1.

Following the classiﬁcation tree shown in Figure 2, most works were classiﬁed into class
1 shown in Figure 2 except two works [34, 35], which belonged to class 3 shown in
Figure 2. To reduce the input dimension, Dahl et al. [35] performed feature selection
using mutual information and random projection. Tobiyama et al. generated behavioral
feature images using RNN [34].

Observation 4: After extracting features, two kinds of neural network architectures, i.e.,
one single neural network and multiple neural networks with a combined loss function,
were used.

23

Hierarchical structures, like convolutional layers, fully connected layers and classiﬁ-
cation layers, were used to classify programs [31, 33–38]. A deep stack of denoising
autoencoders was also introduced to learn programs’ behaviors [39]. De La Rosa and
others [18] trained three different models with different features to compare which static
features are relevant for the classiﬁcation model. Some works investigated LSTM models
for sequential features [17, 36].

Two networks with different features as inputs were used for malware classiﬁcation by
combining their outputs with a dropout layer and an output layer [32]. In [32], one net-
work transformed PE Metadata and import features using feedforward neurons, another
one leveraged convolutional network layers with opcode sequences. Lifan Xu et al. [40]
constructed a few networks and combined them using a two-level multiple kernel learn-
ing algorithm.

The above observations seem to indicate the following indications:

Indication 7.1: Except two works transform binary into images [37, 38], most works
surveyed need to adapt methods to vectorize extracted features.

The vectorization methods should not only keep syntactic and semantic information in
features, but also consider the deﬁnition of the Deep Learning model.

Indication 7.2: Only limited works have shown how to transform features using repre-
sentation learning.

Because some works assume the dynamic and static sequences, like API calls and in-
struction, and have similar syntactic and semantic structure as natural language, some
representation learning techniques like word2vec may be useful in malware detection.
In addition, for the control-ﬂow graph, call graph and other graph representations, graph
embedding is a potential method to transform those features.

7.3 Discussion

Though several pieces of research have been done in malware detection using Deep Learning,
it’s hard to compare their methods and performances because of two uncertainties in their ap-
proaches. First, the Deep Learning model is a black-box, researchers cannot detail which kind
of features the model learned and explain why their model works. Second, feature selection
and representation affect the model’s performance. Because they do not use the same datasets,
researchers cannot prove their approaches – including selected features and Deep Learning
model – are better than others. The reason why few researchers use open datasets is that exist-
ing open malware datasets are out of data and limited. Also, researchers need to crawl benign
programs from app stores, so their raw programs will be diverse.

8 A closer look at applications of Deep Learning in system-

event-based anomaly detection

8.1 Introduction

System logs recorded signiﬁcant events at various critical points, which can be used to debug
the system’s performance issues and failures. Moreover, log data are available in almost all

24

computer systems and are a valuable resource for understanding system status. There are a few
challenges in anomaly detection based on system logs. Firstly, the raw log data are unstruc-
tured, while their formats and semantics can vary signiﬁcantly. Secondly, logs are produced by
concurrently running tasks. Such concurrency makes it hard to apply workﬂow-based anomaly
detection methods. Thirdly, logs contain rich information and complexity types, including text,
real value, IP address, timestamp, and so on. The contained information of each log is also
varied. Finally, there are massive logs in every system. Moreover, each anomaly event usually
incorporates a large number of logs generated in a long period.

Recently, a large number of scholars employed deep learning techniques [8, 41–45] to de-
tect anomaly events in the system logs and diagnosis system failures. The raw log data are
unstructured, while their formats and semantics can vary signiﬁcantly. To detect the anomaly
event, the raw log usually should be parsed to structure data, the parsed data can be transformed
into a representation that supports an effective deep learning model. Finally, the anomaly event
can be detected by deep learning based classiﬁer or predictor.

In this section, we will review the very recent six representative papers that use deep learn-
ing for system-event-based anomaly detection [8,41–45]. DeepLog [8] utilizes LSTM to model
the system log as a natural language sequence, which automatically learns log patterns from
the normal event, and detects anomalies when log patterns deviate from the trained model.
LogAnom [41] employs Word2vec to extract the semantic and syntax information from log
templates. Moreover, it uses sequential and quantitative features simultaneously. Desh [42]
uses LSTM to predict node failures that occur in super computing systems from HPC logs.
Andy Brown et al. [43] presented RNN language models augmented with attention for anomaly
detection in system logs. LogRobust [44] uses FastText to represent semantic information of
log events, which can identify and handle unstable log events and sequences. Christophe Bert-
ero et al. [45] map log word to a high dimensional metric space using Google’s word2vec
algorithm and take it as features to classify. Among these six papers, we select two represen-
tative works [8, 41] and summarize the four phases of their approaches. We direct interested
readers to Table 2 for a concise overview of these two works.

Our review will be centered around three questions described in Section 2.3. In the re-
maining of this section, we will ﬁrst provide a set of observations, and then we provide the
indications. Finally, we provide some general remarks.

8.2 Key ﬁndings from a closer look

From a close look at the very recent applications using deep learning for solving security-event-
based anomaly detection challenges, we observed the followings:

Observation 8.1: Most works of our surveyed papers evaluated their performance using
public datasets.

By the time we surveyed this paper, only two works in [42,45] used their private datasets.

Observation 8.2: Most works in this survey adopted Phase II when parsing the raw log
data.

After reviewing the six works proposed recently, we found that ﬁve works [8, 41–44]
employed parsing technique, while only one work [45] did not.

DeepLog [8] parsed the raw log to different log type using Spell [46] which is based a
longest common subsequence. Desh [42] parsed the raw log to constant message and

25

variable component. Loganom [41] parsed the raw log to different log templates using
FT-Tree [47] according to the frequent combinations of log words. Andy Brown et al.
[43] parsed the raw log into word and character tokenization. LogRobust [44] extracted
its log event by abstracting away the parameters in the message. Christophe Bertero et
al. [45] considered logs as regular text without parsing.

Observation 8.3: Most works have considered and adopted Phase III.

Among these six works, only DeepLog represented the parsed data using the one-hot
vector without learning. Moreover, Loganom [41] compared their results with DeepLog.
That is, DeepLog belongs to class 1 and Loganom belongs to class 4 in Figure 2, while
the other four works follow in class 3.

The four works [41,42,44,45] used word embedding techniques to represent the log data.
Andy Brown et al. [43] employed attention vectors to represent the log messages.

DeepLog [8] employed the one-hot vector to represent the log type without learning. We
have engaged an experiment replacing the one-hot vector with trained word embeddings.

Observation 8.4: Evaluation results were not compared using the same dataset.

DeepLog [8] employed the one-hot vector to represent the log type without learning,
which employed Phase II without Phase III. However, Christophe Bertero et al. [45]
considered logs as regular text without parsing, and used Phase III without Phase II. The
precision of the two methods is very high, which is greater than 95%. Unfortunately, the
evaluations of the two methods used different datasets.

Observation 8.5: Most works empolyed LSTM in Phase IV.

Five works including [8, 41–44] employed LSTM in the Phase IV, while Christophe
Bertero et al. [45] tried different classiﬁers including naive Bayes, neural networks and
random forest.

The above observations seem to indicate the following indications:

Indication 8.1: Phase II has a positive effect on accuracy if being well-designed.

Since Christophe Bertero et al. [45] considers logs as regular text without parsing, we can
say that Phase II is not required. However, we can ﬁnd that most of the scholars employed
parsing techniques to extract structure information and remove the useless noise.

Indication 8.2: Most of the recent works use trained representation to represent parsed
data.

As shown in Table 3, we can ﬁnd Phase III is very useful, which can improve detection
accuracy.

Indication 8.3: Phase II and Phase III cannot be skipped simultaneously.

Both Phase II and Phase III are not required. However, all methods have employed
Phase II or Phase III.

Indication 8.4: Observation 8.3 indicates that the trained word embedding format can
improve the anomaly detection accuracy as shown in Table 3.

26

Table 3: Comparison between word embedding and one-hot representation.

Method
Word Embedding 3
One-hot Vector 4
DeepLog 5

FP 1
680
711
833

FN 2
219
705
619

Precision
96.069%
95.779%
95%

Recall
98.699%
95.813%
96%

F1-measure
97.366%
95.796%
96%

1FP: false positive; 2FN: False negative;3Word Embedding: Log keys are embedded
by Continuous Bag of words;4 One-hot Vector: We reproduced the results according
to DeepLog;5 DeepLog: Orignial results presented in the paper [8].

Indication 8.5: Observation 8.5 indicates that most of the works adopt LSTM to detect
anomaly events.

We can ﬁnd that most of the works adopt LSTM to detect anomaly event, since log
data can be considered as sequence and there can be lags of unknown duration between
important events in a time series. LSTM has feedback connections, which can not only
process single data points, but also entire sequences of data.

As our consideration, neither Phase II nor Phase III is required in system event-based
anomaly detection. However, Phase II can remove noise in raw data, and Phase III can learn a
proper representation of the data. Both Phase II and Phase III have a positive effect on anomaly
detection accuracy. Since the event log is text data that we can’t feed the raw log data into deep
learning model directly, Phase II and Phase III can’t be skipped simultaneously.

8.3 Discussion

Deep learning can capture the potentially nonlinear and high dimensional dependencies among
log entries from the training data that correspond to abnormal events. In that way, it can release
the challenges mentioned above. However, it still suffers from several challenges. For example,
how to represent the unstructured data accurately and automatically without human knowledge.

9 A closer look at applications of Deep Learning in solving

memory forensics challenges

9.1 Introduction

In the ﬁeld of computer security, memory forensics is security-oriented forensic analysis of a
computer’s memory dump. Memory forensics can be conducted against OS kernels, user-level
applications, as well as mobile devices. Memory forensics outperforms traditional disk-based
forensics because although secrecy attacks can erase their footprints on disk, they would have
to appear in memory [19]. The memory dump can be considered as a sequence of bytes, thus
memory forensics usually needs to extract security semantic information from raw memory
dump to ﬁnd attack traces.

27

The traditional memory forensic tools fall into two categories: signature scanning and data
structure traversal. These traditional methods usually have some limitations. Firstly, it needs
expert knowledge on the related data structures to create signatures or traversing rules. Sec-
ondly, attackers may directly manipulate data and pointer values in kernel objects to evade
detection, and then it becomes even more challenging to create signatures and traversing rules
that cannot be easily violated by malicious manipulations, system updates, and random noise.
Finally, the high-efﬁciency requirement often sacriﬁces high robustness. For example, an ef-
ﬁcient signature scan tool usually skips large memory regions that are unlikely to have the
relevant objects and relies on simple but easily tamperable string constants. An important clue
may hide in this ignored region.

In this section, we will review the very recent four representative works that use Deep
Learning for memory forensics [19,48–50]. DeepMem [19] recognized the kernel objects from
raw memory dumps by generating abstract representations of kernel objects with a graph-based
Deep Learning approach. MDMF [48] detected OS and architecture-independent malware from
memory snapshots with several pre-processing techniques, domain unaware feature selection,
and a suite of machine learning algorithms. MemTri [49] predicts the likelihood of criminal ac-
tivity in a memory image using a Bayesian network, based on evidence data artefacts generated
by several applications. Dai et al. [50] monitor the malware process memory and classify mal-
ware according to memory dumps, by transforming the memory dump into grayscale images
and adopting a multi-layer perception as the classiﬁer.

Among these four works [19, 48–50], two representative works (i.e., [19, 48]) are already
summarized phase-by-phase in Table 1. We direct interested readers to Table 2 for a concise
overview of these two works.

Our review will be centered around the three questions raised in Section 2.3. In the re-
maining of this section, we will ﬁrst provide a set of observations, and then we provide the
indications. Finally, we provide some general remarks.

9.2 Key ﬁndings from a closer look

From a close look at the very recent applications using Deep Learning for solving memory
forensics challenges, we observed the followings:

Observation 9.1: Most methods used their own datasets for performance evaluation,
while none of them used a public dataset.

DeepMem was evaluated on self-generated dataset by the authors, who collected a large
number of diverse memory dumps, and labeled the kernel objects in them using existing
memory forensics tools like Volatility. MDMF employed the MalRec dataset by Geor-
gia Tech to generate malicious snapshots, while it created a dataset of benign memory
snapshots running normal software. MemTri ran several Windows 7 virtual machine in-
stances with self-designed suspect activity scenarios to gather memory images. Dai et al.
built the Procdump program in Cuckoo sandbox to extract malware memory dumps. We
found that each of the four works in our survey generated their own datasets, while none
was evaluated on a public dataset.

Observation 9.2: Among the four works [19, 48–50], two works [19, 49] employed
Phase II while the other two works [48, 50] did not employ.

DeepMem [19] devised a graph representation for a sequence of bytes, taking into ac-
count both adjacency and points-to relations, to better model the contextual information

28

in memory dumps. MemTri [49] ﬁrstly identiﬁed the running processes within the mem-
ory image that match the target applications, then employed regular expressions to locate
evidence artefacts in a memory image. MDMF [48] and Dai et al. [50] transformed the
memory dump into image directly.

Observation 9.3: Among four works [19,48–50], only DeepMem [19] employed Phase III
for which it used an embedding method to represent a memory graph.

MDMF [48] directly fed the generated memory images into the training of a CNN model.
Dai et al. [50] used HOG feature descriptor for detecting objects, while MemTri [49]
extracted evidence artefacts as the input of Bayesian Network. In summary, DeepMem
belonged to class 3 shown in Figure 2, while the other three works belonged to class 1
shown in Figure 2.

Observation 9.4: All the four works [19,48–50] have employed different classiﬁers even
when the types of input data are the same.

DeepMem chose fully connected network (FCN) model that has multi-layered hidden
neurons with ReLU activation functions, following by a softmax layer as the last layer.
MDMF [48] evaluated their performance both on traditional machine learning algorithms
and Deep Learning approach including CNN and LSTM. Their results showed the accu-
racy of different classiﬁers did not have a signiﬁcant difference. MemTri employed a
Bayesian network model that is designed with three layers, i.e., a hypothesis layer, a sub-
hypothesis layer, and an evidence layer. Dai et al. used a multi-layer perception model
including an input layer, a hidden layer and an output layer as the classiﬁer.

The above observations seem to indicate the following indications:

Indication 9.1: There lacks public datasets for evaluating the performance of different
Deep Learning methods in memory forensics.

From Observation 9.1, we ﬁnd that none of the four works surveyed was evaluated on
public datasets.

Indication 9.2: From Observation 9.2, we ﬁnd that it is disputable whether one should
employ Phase II when solving memory forensics problems.

Since both [48] and [50] directly transformed a memory dump into an image, Phase II
is not required in these two works. However, since there is a large amount of useless
information in a memory dump, we argue that appropriate prepossessing could improve
the accuracy of the trained models.

Indication 9.3: From Observation 9.3, we ﬁnd that Phase III is paid not much attention
in memory forensics.

Most works did not employ Phase III. Among the four works, only DeepMem [19] em-
ployed Phase III during which it used embeddings to represent a memory graph. The
other three works [48–50] did not learn any representations before training a Deep Learn-
ing model.

29

Indication 9.4: For Phase IV in memory forensics, different classiﬁers can be employed.

Which kind of classiﬁer to use seems to be determined by the features used and their data
structures. From Observation 9.4, we ﬁnd that the four works have actually employed
different kinds of classiﬁers even the types of input data are the same. It is very inter-
esting that MDMF obtained similar results with different classiﬁers including traditional
machine learning and Deep Learning models. However, the other three works did not
discuss why they chose a particular kind of classiﬁer.

Since a memory dump can be considered as a sequence of bytes, the data structure of a
training data example is straightforward. If the memory dump is transformed into a simple
form in Phase II, it can be directly fed into the training process of a Deep Learning model,
and as a result Phase III can be ignored. However, if the memory dump is transformed into a
complicated form in Phase II, Phase III could be quite useful in memory forensics.

Regarding the answer for Question 3 at Section 2.3, it is very interesting that during Phase IV
different classiﬁers can be employed in memory forensics. Moreover, MDMF [48] has shown
that they can obtain similar results with different kinds of classiﬁers. Nevertheless, they also
admit that with a larger amount of training data, the performance could be improved by Deep
Learning.

9.3 Discussion

An end-to-end manner deep learning model can learn the precise representation of memory
dump automatically to release the requirement for expert knowledge. However, it still needs
expert knowledge to represent data and attacker behavior. Attackers may also directly manipu-
late data and pointer values in kernel objects to evade detection.

10 A closer look at applications of Deep Learning in security-

oriented fuzzing

10.1 Introduction

Fuzzing of software security is one of the state of art techniques that people use to detect
software vulnerabilities. The goal of fuzzing is to ﬁnd all the vulnerabilities exist in the program
by testing as much program code as possible. Due to the nature of fuzzing, this technique works
best on ﬁnding vulnerabilities in programs that take in input ﬁles, like PDF viewers [21] or web
browsers. A typical workﬂow of fuzzing can be concluded as: given several seed input ﬁles,
the fuzzer will mutate or fuzz the seed inputs to get more input ﬁles, with the aim of expanding
the overall code coverage of the target program as it executes the mutated ﬁles. Although
there have already been various popular fuzzers [66], fuzzing still cannot bypass its problem of
sometimes redundantly testing input ﬁles which cannot improve the code coverage rate [20,53].
Some input ﬁles mutated by the fuzzer even cannot pass the well-formed ﬁle structure test [21].
Recent research has come up with ideas of applying Deep Learning in the process of fuzzing
to solve these problems.

In this section, we will review the very recent four representative works that use Deep
Learning for fuzzing for software security. Among the three, two representative works [20, 21]
are already summarized phase-by-phase in Table 2. We direct interested readers to Table 2 for
a concise overview of those two works.

30

Our review will be centered around three questions described in Section 2.3. In the re-
maining of this section, we will ﬁrst provide a set of observations, and then we provide the
indications. Finally, we provide some general remarks.

10.2 Key ﬁndings from a closer look

From a close look at the very recent applications using Deep Learning for solving security-
oriented program analysis challenges, we observed the followings:

Observation 10.1: Deep Learning has only been applied in mutation-based fuzzing.

Even though various of different fuzzing techniques, including symbolic execution based
fuzzing [67], tainted analysis based fuzzing [68] and hybrid fuzzing [69] have been
proposed so far, we observed that all the works we surveyed employed Deep Learn-
ing method to assist the primitive fuzzing – mutation-based fuzzing. Speciﬁcally, they
adopted Deep Learning to assist fuzzing tool’s input mutation. We found that they com-
monly did it in two ways: 1) training Deep Learning models to tell how to efﬁciently
mutate the input to trigger more execution path [20, 53]; 2) training Deep Learning mod-
els to tell how to keep the mutated ﬁles compliant with the program’s basic semantic
requirement [21]. Besides, all three works trained different Deep Learning models for
different programs, which means that knowledge learned from one programs cannot be
applied to other programs.

Observation 10.2: Similarity among all the works in our survey existed when choosing
the training samples in Phase I.

The works in this survey had a common practice, i.e., using the input ﬁles directly as
training samples of the Deep Learning model. Learn&Fuzz [21] used character-level
PDF objects sequence as training samples. Neuzz [20] regarded input ﬁles directly as
byte sequences and fed them into the neural network model. Mohit Rajpal et al. [53] also
used byte level representations of input ﬁles as training samples.

Observation 10.3: Difference between all the works in our survey existed when assign-
ing the training labels in Phase I.

Despite the similarity of training samples researchers decide to use, there was a huge
difference in the training labels that each work chose to use. Learn&Fuzz [21] directly
used the character sequences of PDF objects as labels, same as training samples, but
shifted by one position, which is a common generative model technique already broadly
used in speech and handwriting recognition. Unlike Learn&Fuzz, Neuzz [20] and Raj-
pal’s work [53] used bitmap and heatmap respectively as training labels, with the bitmap
demonstrating the code coverage status of a certain input, and the heatmap demonstrat-
ing the efﬁcacy of ﬂipping one or more bytes of the input ﬁle. Whereas, as a common
terminology well-known among fuzzing researchers, bitmap was gathered directly from
the results of AFL. Heatmap used by Rajpal et al. was generated by comparing the code
coverage supported by the bitmap of one seed ﬁle and the code coverage supported by
bitmaps of the mutated seed ﬁles. It was noted that if there is acceptable level of code
coverage expansion when executing the mutated seed ﬁles, demonstrated by more “1”s,
instead of “0”s in the corresponding bitmaps, the byte level differences among the orig-
inal seed ﬁle and the mutated seed ﬁles will be highlighted. Since those bytes should be
the focus of later on mutation, heatmap was used to denote the location of those bytes.

31

Different labels usage in each work was actually due to the different kinds of knowledge
each work wants to learn. For a better understanding, let us note that we can simply
regard a Deep Learning model as a simulation of a “function”. Learn&Fuzz [21] wanted
to learn valid mutation of a PDF ﬁle that was compliant with the syntax and semantic
requirements of PDF objects. Their model could be seen as a simulation of f (x, θ) = y,
where x denotes sequence of characters in PDF objects and y represents a sequence
that are obtained by shifting the input sequences by one position. They generated new
PDF object character sequences given a starting preﬁx once the model was trained. In
Neuzz [20], an NN(Neural Network) model was used to do program smoothing, which
simultated a smooth surrogate function that approximated the discrete branching behav-
iors of the target program. f (x, θ) = y, where x denoted program’s byte level input and
y represented the corresponding edge coverage bitmap. In this way, the gradient of the
surrogate function was easily computed, due to NN’s support of efﬁcient computation of
gradients and higher order derivatives. Gradients could then be used to guide the direc-
tion of mutation, in order to get greater code coverage. In Rajpal and others’ work [53],
they designed a model to predict good (and bad) locations to mutate in input ﬁles based
on the past mutations and corresponding code coverage information. Here, the x variable
also denoted program’s byte level input, but the y variable represented the corresponding
heatmap.

Observation 10.4: Various lengths of input ﬁles were handled in Phase II.

Deep Learning models typically accepted ﬁxed length input, whereas the input ﬁles for
fuzzers often held different lengths. Two different approaches were used among the three
works we surveyed: splitting and padding. Learn&Fuzz [21] dealt with this mismatch
by concatenating all the PDF objects character sequences together, and then splited the
large character sequence into multiple training samples with a ﬁxed size. Neuzz [20]
solved this problem by setting a maximize input ﬁle threshold and then, padding the
smaller-sized input ﬁles with null bytes. From additional experiments, they also found
that a modest threshold gived them the best result, and enlarging the input ﬁle size did
not grant them additional accuracy. Aside from preprocessing training samples, Neuzz
also preprocessed training labels and reduced labels dimension by merging the edges
that always appeared together into one edge, in order to prevent the multicollinearity
problem, that could prevent the model from converging to a small loss value. Rajpal
and others [53] used the similar splitting mechanism as Learn&Fuzz to split their input
ﬁles into either 64-bit or 128-bit chunks. Their chunk size was determined empirically
and was considered as a trainable parameter for their Deep Learning model, and their
approach did not require sequence concatenating at the beginning.

Observation 10.5: All the works in our survey skipped Phase III.

According to our deﬁnition of Phase III, all the works in our survey did not consider
representation learning. Therefore, all the three works [20, 21, 53] fell into class 1 shown
in Figure 2.While as in Rajpal and others’ work, they considered the numerical represen-
tation of byte sequences. They claimed that since one byte binary data did not always
represent the magnitude but also state, representing one byte in values ranging from 0 to
255 could be suboptimal. They used lower level 8-bit representation.

The above observations seem to indicate the following indications:

32

Indication 10.1: No alteration to the input ﬁles seems to be a correct approach.

As far as we concerned, it is due to the nature of fuzzing. That is, since every bit of
the input ﬁles matters, any slight alteration to the input ﬁles could either lose important
information or add redundant information for the neural network model to learn.

Indication 10.2: Evaluation criteria should be chosen carefully when judging mutation.

Input ﬁles are always used as training samples regarding using Deep Learning technique
in fuzzing problems. Through this similar action, researchers have a common desire to
let the neural network mode learn how the mutated input ﬁles should look like. But the
criterion of judging a input ﬁle actually has two levels: on the one hand, a good input ﬁle
should be correct in syntax and semantics; on the other hand, a good input ﬁle should be
the product of a useful mutation, which triggers the program to behave differently from
previous execution path. This idea of a fuzzer that can generate semantically correct
input ﬁle could still be a bad fuzzer at triggering new execution path was ﬁrst brought up
in Learn&Fuzz [21]. We could see later on works trying to solve this problem by using
either different training labels [53] or use neural network to do program smoothing [20].
We encouraged fuzzing researchers, when using Deep Learning techniques, to keep this
problem in mind, in order to get better fuzzing results.

Indication 10.3: Works in our survey only focus on local knowledge.

In brief, some of the existing works [20, 53] leveraged the Deep Learning model to
learn the relation between program’s input and its behavior and used the knowledge that
learned from history to guide future mutation. For better demonstration, we deﬁned the
knowledge that only applied in one program as local knowledge. In other words, this
indicates that the local knowledge cannot direct fuzzing on other programs.

10.3 Discussion

Corresponding to the problems conventional fuzzing has, the advantages of applying DL in
fuzzing are that DL’s learning ability can ensure mutated input ﬁles follow the designated
grammar rules better. The ways in which input ﬁles are generated are more directed, and
will, therefore, guarantee the fuzzer to increase its code coverage by each mutation. However,
even if the advantages can be clearly demonstrated by the two papers we discuss above, some
challenges still exist, including mutation judgment challenges that are faced both by traditional
fuzzing techniques and fuzzing with DL, and the scalability of fuzzing approaches.

We would like to raise several interesting questions for the future researchers: 1) Can the
knowledge learned from the fuzzing history of one program be applied to direct testing on other
programs? 2) If the answer to question one is positive, we can suppose that global knowledge
across different programs exists? Then, can we train a model to extract the global knowledge?
3) Whether it is possible to combine global knowledge and local knowledge when fuzzing
programs?

11 Discussion

Using high-quality data in Deep Learning is important as much as using well-structured deep
neural network architectures. That is, obtaining quality data must be an important step, which

33

should not be skipped, even in resolving security problems using Deep Learning. So far, this
study demonstrated how the recent security papers using Deep Learning have adopted data
conversion (Phase II) and data representation (Phase III) on different security problems. Our
observations and indications showed a clear understanding of how security experts generate
quality data when using Deep Learning.

Since we did not review all the existing security papers using Deep Learning, the generality
of observations and indications is somewhat limited. Note that our selected papers for review
have been published recently at one of prestigious security and reliability conferences such
as USENIX SECURITY, ACM CCS and so on [9]- [42], [43, 44], [19, 48], [51]- [53]. Thus,
our observations and indications help to understand how most security experts have used Deep
Learning to solve the well-known eight security problems from program analysis to fuzzing.

Our observations show that we should transfer raw data to synthetic formats of data ready
for resolving security problems using Deep Learning through data cleaning and data augmen-
tation and so on. Speciﬁcally, we observe that Phases II and III methods have mainly been used
for the following purposes:

• To clean the raw data to make the neural network (NN) models easier to interpret

• To reduce the dimensionality of data (e.g., principle component analysis (PCA), t-distributed

stochastic neighbor embedding (t-SNE))

• To scale input data (e.g., normalization)

• To make NN models understand more complex relationships depending on security prob-

lems (e.g. memory graphs)

• To simply change various raw data formats into a vector format for NN models (e.g.

one-hot encoding and word2vec embedding)

In this following, we do further discuss the question, “What if Phase II is skipped?”, rather
than the question, “Is Phase III always necessary?”. This is because most of the selected papers
do not consider Phase III methods (76%), or adopt with no concrete reasoning (19%). Specif-
ically, we demonstrate how Phase II has been adopted according to eight security problems,
different types of data, various models of NN and various outputs of NN models, in depth. Our
key ﬁndings are summarized as follows:

• How to ﬁt security domain knowledge into raw data has not been well-studied yet.

• While raw text data are commonly parsed after embedding, raw binary data are converted

using various Phase II methods.

• Raw data are commonly converted into a vector format to ﬁt well to a speciﬁc NN model

using various Phase II methods.

• Various Phase II methods are used according to the relationship between output of secu-

rity problem and output of NN models.

11.1 What if Phase II is skipped?

From the analysis results of our selected papers for review, we roughly classify Phase II meth-
ods into the following four categories.

34

PA

ROP

CFI

NA

33.4%

33.4%

66.7%

66.7%

100%

SEAD

MF

FUZZING

100%

MC

16.7%

16.7%

41.7%

41.7%

83.4%

100%

33.4%

66.7%

Embedding

Parsing & Embedding

One-hot

None

Other

Figure 3: Statistics of Phase II methods for eight security problems

• Embedding: The data conversion methods that intend to convert high-dimensional dis-

crete variables into low-dimensional continuous vectors [70].

• Parsing combined with embedding: The data conversion methods that constitute an input

data into syntactic components in order to test conformability after embedding.

• One-hot encoding: A simple embedding where each data belonging to a speciﬁc category
is mapped to a vector of 0s and a single 1. Here, the low-dimension transformed vector
is not managed.

• Domain-speciﬁc data structures: A set of data conversion methods which generate data
structures capturing domain-speciﬁc knowledge for different security problems, e.g.,
memory graphs [19].

11.1.1 Findings on eight security problems

We observe that over 93% of the papers use one of the above-classiﬁed Phase II methods. 7% of
the papers do not use any of the above-classiﬁed methods, and these papers are mostly solving
a software fuzzing problem. Speciﬁcally, we observe that 35% of the papers use a Category
1 (i.e. embedding) method; 30% of the papers use a Category 2 (i.e. parsing combined with
embedding) method; 15% of the papers use a Category 3 (i.e. one-hot encoding) method; and
13% of the papers use a Category 4 (i.e. domain-speciﬁc data structures) method. Regarding
why one-hot encoding is not widely used, we found that most security data include categorical
input values, which are not directly analyzed by Deep Learning models.

From Figure 5a, we also observe that according to security problems, different Phase II
methods are used. First, PA, ROP and CFI should convert raw data into a vector format using
embedding because they commonly collect instruction sequence from binary data. Second, NA
and SEAD use parsing combined with embedding because raw data such as the network trafﬁc
and system logs consist of the complex attributes with the different formats such as categorical
and numerical input values. Third, we observe that MF uses various data structures because
memory dumps from memory layout are unstructured. Fourth, fuzzing generally uses no data
conversion since Deep Learning models are used to generate the new input data with the same

35

Binary
Text

)

%

(
E
G
A
T
N
E
C
R
E
P

100

80

60

40

20

0

92.4

51.9

0

0

22.3

0

14.9

7.6

Embedding Parsing & Embdding

One-hot

None

11.2

0

Others

Figure 4: Statistics of Phase II methods on type of data.

data format as the original raw data. Finally, we observe that MC commonly uses one-hot
encoding and embedding because malware binary and well-structured security log ﬁles include
categorical, numerical and unstructured data in general. These observations indicate that type
of data strongly inﬂuences on use of Phase II methods. We also observe that only MF among
eight security problems commonly transform raw data into well-structured data embedding
a specialized security domain knowledge. This observation indicates that various conversion
methods of raw data into well-structure data which embed various security domain knowledge
are not yet studied in depth.

11.1.2 Findings on different data types

Note that according to types of data, a NN model works better than the others. For example,
CNN works well with images but does not work with text. From Figure 4 for raw binary data,
we observe that 51.9%, 22.3% and 11.2% of security papers use embedding, one-hot encoding
and Others, respectively. Only 14.9% of security papers, especially related to fuzzing, do not
use one of Phase II methods. This observation indicates that binary input data which have
various binary formats should be converted into an input data type which works well with a
speciﬁc NN model. From Figure 4 for raw text data, we also observe that 92.4% of papers
use parsing with embedding as the Phase II method. Note that compared with raw binary data
whose formats are unstructured, raw text data generally have the well-structured format. Raw
text data collected from network trafﬁcs may also have various types of attribute values. Thus,
raw text data are commonly parsed after embedding to reduce redundancy and dimensionality
of data.

11.1.3 Findings on various models of NN

According to types of the converted data, a speciﬁc NN model works better than the others. For
example, CNN works well with images but does not work with raw text. From Figure ??, we
observe that use of embedding for DNN (42.9%), RNN (28.6%) and LSTM (14.3%) models
approximates to 85%. This observation indicates that embedding methods are commonly used
to generate sequential input data for DNN, RNN and LSTM models. Also, we observe that one-
hot encoded data are commonly used as input data for DNN (33.4%), CNN (33.4%) and LSTM
(16.7%) models. This observation indicates that one-hot encoding is one of common Phase II
methods to generate numerical values for image and sequential input data because many raw
input data for security problems commonly have the categorical features. We observe that

36

Embdeeing

Parsing & Embddding

One-hot

None

Others

7.2%

7.2%

42.9%

14.3%

28.6%

8.4%

16.7%

16.7%

8.4%

50%

16.7%

33.4%

16.7%

20%

20%

33.4%

20%

66.7%

33.4%

40%

DNN

CNN

RNN

LSTM

GNN

SNN

Combination

DBN

(a) Statistics of Phase II methods for eight security problems.

DNN

9.1%

54.6%

18.2%

18.2%

CNN

RNN

LSTM

25%

25%

25% 25%

66.7%

16.7%

16.7%

11.2%

22.3%

66.7%

GNN

SNN

Combination

DBN

50%

50%

50%

50%

100%

100%

Embedding

One-hot encoding

Others

Parsing & Embddding

None

(b) Phase II methods over type of NN.

Figure 5: Statistics of Phase II methods for various types of NNs.

the CNN (66.7%) model uses the converted input data using the Others methods to express
the speciﬁc domain knowledge into the input data structure of NN networks. This is because
general vector formats including graph, matrix and so on can also be used as an input value of
the CNN model.

From Figure 5b, we observe that DNN, RNN and LSTM models commonly use embed-
ding, one-hot encoding and parsing combined with embedding. For example, we observe secu-
rity papers of 54.6%, 18.2% and 18.2% models use embedding, one-hot encoding and parsing
combined with embedding, respectively. We also observe that the CNN model is used with
various Phase II methods because any vector formats such as image can generally be used as
an input data of the CNN model.

11.1.4 Findings on output of NN models

According to the relationship between output of security problem and output of NN, we may
use a speciﬁc Phase II method. For example, if output of security problem is given into a class
(e.g., normal or abnormal), output of NN should also be given into classiﬁcation.

From Figure 6a, we observe that embedding is commonly used to support a security prob-
lem for classiﬁcation (100%). Parsing combined with embedding is used to support a security
problem for object detection (41.7%) and classiﬁcation (58.3%). One-hot encoding is used
only for classiﬁcation (100%). These observations indicate that classiﬁcation of a given input

37

Embedding

Parsing & Embddding One-hot encoding

None

Others

41.7%

58.3%

100%

100%

20%

33.4%

60%

20%

66.7%

Data Generation

Object Detection

Classiﬁcation

(a) Output of NN over Phase II methods.

Classiﬁcation

Object Detection

Data Generation

6.3%

9.4%

43.8%

18.8%

14.3%

14.3%

21.9%

71.5%

100%

Embedding

One-hot encoding

Others

Parsing & Embddding

None

(b) Phase II methods over output of NN.

Figure 6: Statistics of Phase II methods for various output of NN.

data is the most common output which is obtained using Deep Learning under various Phase II
methods.

From Figure 6b, we observe that security problems, whose outputs are classiﬁcation, com-
monly use embedding (43.8%) and parsing combined with embedding (21.9%) as the Phase II
method. We also observe that security problems, whose outputs are object detection, commonly
use parsing combined with embedding (71.5%). However, security problems, whose outputs
are data generation, commonly do not use the Phase III methods. These observations indicate
that a speciﬁc Phase II method has been used according to the relationship between output of
security problem and use of NN models.

12 Further areas of investigation

Since any Deep Learning models are stochastic, each time the same Deep Learning model is ﬁt
even on the same data, it might give different outcomes. This is because deep neural networks
use random values such as random initial weights. However, if we have all possible data for
every security problem, we may not make random predictions. Since we have the limited
sample data in practice, we need to get the best-effort prediction results using the given Deep
Learning model, which ﬁts to the given security problem.

How can we get the best-effort prediction results of Deep Learning models for different
security problems? Let us begin to discuss about the stability of evaluation results for our
selected papers for review. Next, we will show the inﬂuence of security domain knowledge on
prediction results of Deep Learning models. Finally, we will discuss some common issues in
those ﬁelds.

38

12.1 How stable are evaluation results?

When evaluating neural network models, Deep Learning models commonly use three methods:
train-test split; train-validation-test split; and k-fold cross validation. A train-test split method
splits the data into two parts, i.e., training and test data. Even though a train-test split method
makes the stable prediction with a large amount of data, predictions vary with a small amount of
data. A train-validation-test split method splits the data into three parts, i.e., training, validation
and test data. Validation data are used to estimate predictions over the unknown data. k-fold
cross validation has k different set of predictions from k different evaluation data. Since k-fold
cross validation takes the average expected performance of the NN model over k-fold validation
data, the evaluation result is closer to the actual performance of the NN model.

From the analysis results of our selected papers for review, we observe that 40.0% and
32.5% of the selected papers are measured using a train-test split method and a train-validation-
test split method, respectively. Only 17.5% of the selected papers are measured using k-fold
cross validation. This observation implies that even though the selected papers show almost
more than 99% of accuracy or 0.99 of F1 score, most solutions using Deep Learning might not
show the same performance for the noisy data with randomness.

To get stable prediction results of Deep Learning models for different security problems,
we might reduce the inﬂuence of the randomness of data on Deep Learning models. At least, it
is recommended to consider the following methods:

• Do experiments using the same data many time: To get a stable prediction with a
small amount of sample data, we might control the randomness of data using the same
data many times.

• Use cross validation methods, e.g. k-fold cross validation: The expected average and

variance from k-fold cross validation estimates how stable the proposed model is.

12.2 How does security domain knowledge inﬂuence the performance of

security solutions using Deep Learning?

When selecting a NN model that analyzes an application dataset, e.g., MNIST dataset [71], we
should understand that the problem is to classify a handwritten digit using a 28 × 28 black.
Also, to solve the problem with the high classiﬁcation accuracy, it is important to know which
part of each handwritten digit mainly inﬂuences the outcome of the problem, i.e., a domain
knowledge.

While solving a security problem, knowing and using security domain knowledge for each
security problem is also important due to the following reasons (we label the observations
and indications that realted to domain knowledge with ‘∗’): Firstly, the dataset generation,
preprocess and feature selection highly depend on domain knowledge. Different from the image
classiﬁcation and natural language processing, raw data in the security domain cannot be sent
into the NN model directly. Researchers need to adopt strong domain knowledge to generate,
extract, or clean the training set. Also, in some works, domain knowledge is adopted in data
labeling because labels for data samples are not straightforward.

Secondly, domain knowledge helps with the selection of DL models and its hierarchical
structure. For example, the neural network architecture (hierarchical and bi-directional LSTM)
designed in DEEPVSA [22] is based on the domain knowledge in the instruction analysis.

Thirdly, domain knowledge helps to speed up the training process. For instance, by adopt-
ing strong domain knowledge to clean the training set, domain knowledge helps to spend up

39

the training process while keeping the same performance. However, due to the inﬂuence of the
randomness of data on Deep Learning models, domain knowledge should be carefully adopted
to avoid potential decreased accuracy.

Finally, domain knowledge helps with the interpretability of models’ prediction. Recently,
researchers try to explore the interpretability of the deep learning model in security areas, For
instance, LEMNA [72] and EKLAVYA [10] explain how the prediction was made by models
from different perspectives. By enhancing the trained models‘ interpretability, they can improve
their approaches’ accuracy and security. The explanation for the relation between input, hidden
state, and the ﬁnal output is based on domain knowledge.

12.3 Common challenges

In this section, we will discuss the common challenges when applying DL to solving security
problems. These challenges as least shared by the majority of works, if not by all the works.
Generally, we observe 7 common challenges in our survey:

1. The raw data collected from the software or system usually contains lots of noise.

2. The collected raw is untidy. For instance, the instruction trace, the Untidy data: variable

length sequences,

3. Hierarchical data syntactic/structure. As discussed in Section 3.3, the information may
not simply be encoded in a single layer, rather, it is encoded hierarchically, and the syn-
tactic is complex.

4. Dataset generation is challenging in some scenarios. Therefore, the generated training

data might be less representative or unbalanced.

5. Different for the application of DL in image classiﬁcation, and natural language process,
which is visible or understandable, the relation between data sample and its label is not
intuitive, and hard to explain.

12.4 Availability of trained model and quality of dataset.

Finally, we investigate the availability of the trained model and the quality of the dataset. Gen-
erally, the availability of the trained models affects its adoption in practice, and the quality of
the training set and the testing set will affect the credibility of testing results and comparison
between different works. Therefore, we collect relevant information to answer the following
four questions and shows the statistic in Table 4:

1. Whether a paper’s source code is publicly available?

2. Whether raw data, which is used to generate the dataset, is publicly available?

3. Whether its dataset is publicly available?

4. How are the quality of the dataset?

We observe that both the percentage of open source of code and dataset in our surveyed
ﬁelds is low, which makes it a challenge to reproduce proposed schemes, make comparisons
between different works, and adopt them in practice. Speciﬁcally, the statistic shows that 1) the

40

Table 4: Analysis of the datasets and trained model.

Topic

Paper

Source
Available

Raw Data
Available1

Dataset
Available2

Quality of Dataset

Sample Num Balance

PA

ROP

CFI

Network

Malware

LogEvent

MemoryFoensic

FUZZING

RFBNN [9]
EKLAVYA [10]
ROPNN [11]
HeNet [12]
Barnum [13]
CFG-CNN [14]
50b(yte)-CNN [15]
PCCN [16]
Rosenber [17]
DeLaRosa [18]
DeepLog [8]
LogAnom [41]
DeepMem [19]
MDMF [48]
NeuZZ [20]
Learn & Fuzz [21]

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
P3
(cid:55)
(cid:51)
(cid:55)
(cid:51)
(cid:55)

(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
P
P
(cid:51)
(cid:55)
(cid:55)
(cid:55)

N/A
N/A
N/A
N/A
N/A
N/A
115835
1168671
500000
100000
N/A
N/A
N/A
N/A
N/A
N/A

N/A
N/A
N/A
N/A
N/A
N/A
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
N/A
N/A

1 “Raw data” refers to the data that used to generate training set but cannot be feed into the model directly. For instance, a collection of

binary ﬁles is raw ﬁle.

2 “Dataset” is the collection of data sample that can be feed in to the DL model directly. For instance, a collection of image, sequence.
3 “P” denotes that its source code or dataset is partially available to public.

percentage of open source of code in our surveyed ﬁelds is low, only 6 out of 16 paper published
their model’s source code. 2) the percentage of public data sets is low. Even though, the raw
data in half of the works are publicly available, only 4 out of 16 fully or partially published
their dataset. 3) the quality of datasets is not guaranteed, for instance, most of the dataset is
unbalanced.

The performance of security solutions even using Deep Learning might vary according to
datasets. Traditionally, when evaluating different NN models in image classiﬁcation, standard
datasets such as MNIST for recognizing handwritten 10 digits and CIFAR10 [73] for recogniz-
ing 10 object classes are used for performance comparison of different NN models. However,
there are no known standard datasets for evaluating NN models on different security problems.
Due to such a limitation, we observe that most security papers using Deep Learning do not
compare the performance of different security solutions even when they consider the same se-
curity problem. Thus, it is recommended to generate and use a standard dataset for a speciﬁc
security problem for comparison. In conclusion, we think that there are three aspects that need
to be improved in future research:

1. Developing standard dataset.

2. Publishing their source code and dataset.

3. Improving the interpretability of their model.

13 Conclusion

This paper seeks to provide a dedicated review of the very recent research works on using Deep
Learning techniques to solve computer security challenges. In particular, the review covers

41

eight computer security problems being solved by applications of Deep Learning: security-
oriented program analysis, defending ROP attacks, achieving CFI, defending network attacks,
malware classiﬁcation, system-event-based anomaly detection, memory forensics, and fuzzing
for software security. Our observations of the reviewed works indicate that the literature of
using Deep Learning techniques to solve computer security challenges is still at an earlier stage
of development.

14 Availability of data and materials

Not applicable.

15 Funding

This work was supported by ARO W911NF-13-1-0421 (MURI), NSF CNS-1814679, and ARO
W911NF-15-1-0576.

16 Acknowledgements

We are grateful to the anonymous reviewers for their useful comments and suggestions.

References

[1] A. K. Ghosh, J. Wanken, and F. Charron. Detecting Anomalous and Unknown Intrusions
against Programs. In Proceeding of Annual Computer Security Applications Conference
(ACSAC), 1998.

[2] W. Hu, Y. Liao, and V. R. Vemuri. Robust Anomaly Detection using Support Vector

Machines. In International Conference on Machine Learning (ICML), 2003.

[3] K. A. Heller, K. M. Svore, A. D. Keromytis, and S. J. Stolfo. One Class Support Vector
Machines for Detecting Anomalous Windows Registry Accesses. In Proceedings of the
Workshop on Data Mining for Computer Security, 2003.

[4] R. Sommer and V. Paxson. Outside the Closed World: On Using Machine Learning For
Network Intrusion Detection. In 2010 IEEE Symposium on Security and Privacy (S&P),
2010.

[5] NSCAI Intern Report for Congress. Technical report, 2019. https://drive.
google.com/file/d/153OrxnuGEjsUvlxWsFYauslwNeCEkvUb/view.

[6] Wei Xu, Ling Huang, Armando Fox, David Patterson, and Michael I. Jordan. Detecting
In Proceedings of the ACM
Large-Scale System Problems by Mining Console Logs.
SIGOPS 22Nd Symposium on Operating Systems Principles, SOSP ’09, pages 117–132,
New York, NY, USA, 2009. ACM.

[7] Y. Bengio, A. Courville, and P. Vincent. Representation Learning: A Review and
IEEE Transactions on Pattern Analysis and Machine Intelligence,

New Perspectives.
35(8):1798–1828, Aug 2013.

42

[8] Min Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. DeepLog: Anomaly Detection
and Diagnosis from System Logs Through Deep Learning. In Proceedings of the 2017
ACM SIGSAC Conference on Computer and Communications Security, CCS ’17, pages
1285–1298, New York, NY, USA, 2017. ACM.

[9] Eui Chul Richard Shin, Dawn Song, and Reza Moazzezi. Recognizing Functions in Bi-
naries with Neural Networks. In 24th USENIX Security Symposium (USENIX Security
15), pages 611–626, 2015.

[10] Zheng Leong Chua, Shiqi Shen, Prateek Saxena, and Zhenkai Liang. Neural Nets Can
Learn Function Type Signatures from Binaries. In 26th USENIX Security Symposium
(USENIX Security 17), pages 99–116, 2017.

[11] Xusheng Li, Zhisheng Hu, Yiwei Fu, Ping Chen, Minghui Zhu, and Peng Liu.
arXiv preprint

ROPNN: Detection of ROP Payloads Using Deep Neural Networks.
arXiv:1807.11110, 2018.

[12] Li Chen, Salmin Sultana, and Ravi Sahita. Henet: A Deep Learning Approach on In-
tel® Processor Trace for Effective Exploit Detection. In 2018 IEEE Security and Privacy
Workshops (SPW), pages 109–115. IEEE, 2018.

[13] Carter Yagemann, Salmin Sultana, Li Chen, and Wenke Lee. Barnum: Detecting Doc-
In International

ument Malware via Control Flow Anomalies in Hardware Traces.
Conference on Information Security, pages 341–359. Springer, 2019.

[14] Anh Viet Phan, Minh Le Nguyen, and Lam Thu Bui. Convolutional Neural Networks over
In 2017 IEEE 29th International

Control Flow Graphs for Software defect prediction.
Conference on Tools with Artiﬁcial Intelligence (ICTAI), pages 45–52. IEEE, 2017.

[15] K Millar, A Cheng, HG Chew, and C-C Lim. Deep Learning for Classifying Malicious
Network Trafﬁc. In Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining,
pages 156–161. Springer, 2018.

[16] Yong Zhang, Xu Chen, Da Guo, Mei Song, Yinglei Teng, and Xiaojuan Wang. PCCN:
Parallel Cross Convolutional Neural Network for Abnormal Network Trafﬁc Flows Detec-
tion in Multi-Class Imbalanced Network Trafﬁc Flows. IEEE Access, 7:119904–119916,
2019.

[17] Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval Elovici. Generic Black-box End-
to-End Attack against State of the Art API Call based Malware Classiﬁers. In International
Symposium on Research in Attacks, Intrusions, and Defenses, 2018.

[18] Leonardo De La Rosa, Sean Kilgallon, Tristan Vanderbruggen, and John Cavazos.
Efﬁcient Characterization and Classiﬁcation of Malware Using Deep Learning.
In
Proceedings - Resilience Week 2018, RWS 2018, pages 77–83, 2018.

[19] Wei Song, Heng Yin, Chang Liu, and Dawn Song. DeepMem: Learning Graph Neural
Network Models for Fast and Robust Memory Forensic Analysis. In Proceedings of the
2018 ACM SIGSAC Conference on Computer and Communications Security, CCS ’18,
pages 606–618, New York, NY, USA, 2018. ACM.

[20] Dongdong Shi and Kexin Pei. NEUZZ: Efﬁcient Fuzzing with Neural Program Smooth-

ing. IEEE security & privacy, 2019.

43

[21] Patrice Godefroid, Hila Peleg, and Rishabh Singh. Learn&Fuzz: Machine Learning
for Input Fuzzing. In Proceedings of the 32nd IEEE/ACM International Conference on
Automated Software Engineering, pages 50–59. IEEE Press, 2017.

[22] Wenbo Guo, Dongliang Mu, Xinyu Xing, Min Du, and Dawn Song. {DEEPVSA}: Fa-
cilitating Value-set Analysis with Deep Learning for Postmortem Program Analysis. In
28th USENIX Security Symposium (USENIX Security 19), pages 1787–1804, 2019.

[23] Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn Song. Neural
Network-Based Graph Embedding for Cross-Platform Binary Code Similarity Detection.
In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications
Security, pages 363–376. ACM, 2017.

[24] Tiffany Bao, Jonathan Burket, Maverick Woo, Rafael Turner, and David Brumley.
In 23rd USENIX
BYTEWEIGHT: Learning to Recognize Functions in Binary Code.
Security Symposium (USENIX Security 14), pages 845–860, San Diego, CA, August
2014. USENIX Association.

[25] Jiliang Zhang, Wuqiao Chen, and Yuqi Niu. DeepCheck: A Non-intrusive Control-ﬂow
Integrity Checking based on Deep Learning. arXiv preprint arXiv:1905.01858, 2019.

[26] Xiaoyong Yuan, Chuanhuang Li, and Xiaolin Li. DeepDefense: Identifying DDoS At-
In 2017 IEEE International Conference on Smart Computing

tack via Deep Learning.
(SMARTCOMP), pages 1–8. IEEE, 2017.

[27] Remi Varenne, Jean Michel Delorme, Emanuele Plebani, Danilo Pau, and Valeria
Tomaselli. Intelligent Recognition of TCP Intrusions for Embedded Micro-controllers.
In International Conference on Image Analysis and Processing, pages 361–373. Springer,
2019.

[28] Chuanlong Yin, Yuefei Zhu, Jinlong Fei, and Xinzheng He. A Deep Learning Approach
for Intrusion Detection using Recurrent Neural Networks. IEEE Access, 5:21954–21961,
2017.

[29] Serpil Ustebay, Zeynep Turgut, and M Ali Aydin. Cyber Attack Detection by Using
Neural Network Approaches: Shallow Neural Network, Deep Neural Network and Au-
toEncoder. In International Conference on Computer Networks, pages 144–155. Springer,
2019.

[30] Osama Faker and Erdogan Dogdu. Intrusion Detection Using Big Data and Deep Learning
Techniques. In Proceedings of the 2019 ACM Southeast Conference, pages 86–93. ACM,
2019.

[31] Joshua Saxe and Konstantin Berlin. Deep Neural Network based Malware Detection
using Two Dimensional Binary Program Features. In 2015 10th International Conference
on Malicious and Unwanted Software (MALWARE), pages 11–20. IEEE, 2015.

[32] Bojan Kolosnjaji, Ghadir Eraisha, George Webster, Apostolis Zarras, and Claudia Eck-
ert. Empowering Convolutional Networks for Malware Classiﬁcation and Analysis.
Proceedings of the International Joint Conference on Neural Networks, 2017-May:3838–
3845, 2017.

44

[33] Niall McLaughlin, Jesus Martinez Del Rincon, Boo Joong Kang, Suleiman Yerima,
Paul Miller, Sakir Sezer, Yeganeh Safaei, Erik Trickel, Ziming Zhao, Adam Doupe,
and Gail Joon Ahn. Deep Android Malware Detection. Proceedings of the 7th ACM
Conference on Data and Application Security and Privacy, pages 301–308, 2017.

[34] Shun Tobiyama, Yukiko Yamaguchi, Hajime Shimada, Tomonori Ikuse, and Takeshi Yagi.
Malware Detection with Deep Neural Network Using Process Behavior. In Proceedings
- International Computer Software and Applications Conference, 2:577–582, 2016.

[35] George E. Dahl, Jack W. Stokes, Li Deng, and Dong Yu. Large-scale Malware Classiﬁca-
tion using Random Projections and Neural Networks. IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pages 3422–3426, 2013.

[36] Robin Nix and Jian Zhang. Classiﬁcation of Android Apps and Malware using Deep
Neural Networks. Proceedings of the International Joint Conference on Neural Networks,
2017-May:1871–1878, 2017.

[37] Mahmoud Kalash, Mrigank Rochan, Noman Mohammed, Neil D.B. Bruce, Yang Wang,
and Farkhund Iqbal. Malware Classiﬁcation with Deep Convolutional Neural Networks.
9th IFIP International Conference on New Technologies, Mobility and Security, NTMS
2018 - Proceedings, 2018-Janua:1–5, 2018.

[38] Zhihua Cui, Fei Xue, Xingjuan Cai, Yang Cao, Gai Ge Wang, and Jinjun Chen. Detection
of Malicious Code Variants Based on Deep Learning. IEEE Transactions on Industrial
Informatics, 14(7):3187–3196, 2018.

[39] Omid E. David and Nathan S. Netanyahu. DeepSign: Deep Learning for Automatic
Malware Signature Generation and Classiﬁcation. Proceedings of the International Joint
Conference on Neural Networks, 2015-Septe, 2015.

[40] Lifan Xu, Dongping Zhang, Nuwan Jayasena, and John Cavazos. HADM: Hybrid Anal-

ysis for Detection of Malware. 16:702–724, 2018.

[41] Weibin Meng, Ying Liu, Yichen Zhu, Shenglin Zhang, Dan Pei, Yuqing Liu, Yihao Chen,
Ruizhi Zhang, Shimin Tao, Pei Sun, and Rong Zhou. Loganomaly: Unsupervised Detec-
tion of Sequential and Quantitative Anomalies in Unstructured Logs. In Proceedings of
the 28th International Joint Conference on Artiﬁcial Intelligence, IJCAI’19, pages 4739–
4745. AAAI Press, 2019.

[42] Anwesha Das, Frank Mueller, Charles Siegel, and Abhinav Vishnu. Desh: Deep Learning
for System Health Prediction of Lead Times to Failure in HPC. In Proceedings of the
27th International Symposium on High-Performance Parallel and Distributed Computing,
HPDC ’18, pages 40–51, New York, NY, USA, 2018. ACM.

[43] Andy Brown, Aaron Tuor, Brian Hutchinson, and Nicole Nichols. Recurrent Neu-
ral Network Attention Mechanisms for Interpretable System Log Anomaly Detection.
In Proceedings of the First Workshop on Machine Learning for Computing Systems,
MLCS’18, pages 1:1–1:8, New York, NY, USA, 2018. ACM.

[44] Xu Zhang, Yong Xu, Qingwei Lin, Bo Qiao, Hongyu Zhang, Yingnong Dang, Chunyu
Xie, Xinsheng Yang, Qian Cheng, Ze Li, Junjie Chen, Xiaoting He, Randolph Yao, Jian-
Guang Lou, Murali Chintalapati, Furao Shen, and Dongmei Zhang. Robust Log-based

45

Anomaly Detection on Unstable Log Data. In Proceedings of the 27th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the Foundations of
Software Engineering, ESEC/FSE 2019, pages 807–817, New York, NY, USA, 2019.
ACM.

[45] C. Bertero, M. Roy, C. Sauvanaud, and G. Tredan. Experience Report: Log Mining Using
Natural Language Processing and Application to Anomaly Detection. In 2017 IEEE 28th
International Symposium on Software Reliability Engineering (ISSRE), pages 351–360,
Oct 2017.

[46] M. Du and F. Li. Spell: Streaming Parsing of System Event Logs. In 2016 IEEE 16th

International Conference on Data Mining (ICDM), pages 859–864, Dec 2016.

[47] Shenglin Zhang, Weibin Meng, Jiahao Bu, Sen Yang, Ying Liu, D. Pei, J. Xu, Yu Chen,
Hui Dong, Xianping Qu, and Lei Song. Syslog Processing for Switch Failure Diag-
In 2017 IEEE/ACM 25th International
nosis and Prediction in Datacenter Networks.
Symposium on Quality of Service (IWQoS), pages 1–10, June 2017.

[48] Rachel Petrik, Berat Arik, and Jared M. Smith. Towards Architecture and OS-Independent
Malware Detection via Memory Forensics. In Proceedings of the 2018 ACM SIGSAC
Conference on Computer and Communications Security, CCS ’18, pages 2267–2269,
New York, NY, USA, 2018. ACM.

[49] Antonis Michalas and Rohan Murray. MemTri: A Memory Forensics Triage Tool Using
Bayesian Network and Volatility. In Proceedings of the 2017 International Workshop on
Managing Insider Security Threats, MIST ’17, pages 57–66, New York, NY, USA, 2017.
ACM.

[50] Yusheng Dai, Hui Li, Yekui Qian, and Xidong Lu. A Malware Classiﬁcation Method
Based on Memory Dump Grayscale Image. Digital Investigation, 27:30 – 37, 2018.

[51] Yunchao Wang, Zehui Wu, Qiang Wei, and Qingxian Wang. NeuFuzz: Efﬁcient Fuzzing

with Deep Neural Network. IEEE Access, 7:36340–36352, 2019.

[52] Konstantin B¨ottinger, Patrice Godefroid, and Rishabh Singh. Deep Reinforcement
Fuzzing. In 2018 IEEE Security and Privacy Workshops (SPW), pages 116–122. IEEE,
2018.

[53] Mohit Rajpal, William Blum, and Rishabh Singh. Not All Bytes are Equal: Neural Byte

Sieve for Fuzzing. arXiv preprint arXiv:1711.04596, 2017.

[54] Hovav Shacham et al. The Geometry of Innocent Flesh on the Bone: Return-into-
In ACM conference on Computer and

libc without Function Calls (on the x86).
communications security, pages 552–561. New York,, 2007.

[55] Mart´ın Abadi, Mihai Budiu, ´Ulfar Erlingsson, and Jay Ligatti. Control-Flow Integrity
Principles, Implementations, and Applications. ACM Transactions on Information and
System Security (TISSEC), 13(1):4, 2009.

[56] Jonathan Salwant. ROPGadget.

https://github.com/JonathanSalwan/

ROPgadget.

[57] Unicorn-The ultimate CPU emulator. https://www.unicorn-engine.org/.

46

[58] Vladimir Kiriansky, Derek Bruening, Saman P Amarasinghe, et al. Secure Execution via
Program Shepherding. In USENIX Security Symposium, volume 92, page 84, 2002.

[59] Xiaoyang Xu, Masoud Ghaffarinia, Wenhao Wang, Kevin W. Hamlen, and Zhiqiang Lin.
CONFIRM: Evaluating Compatibility and Relevance of Control-ﬂow Integrity Protec-
tions for Modern Software. In 28th USENIX Security Symposium (USENIX Security
19), pages 1805–1821, Santa Clara, CA, August 2019. USENIX Association.

[60] Susan Horwitz. Precise Flow-insensitive May-alias Analysis is NP-hard. ACM Trans.

Program. Lang. Syst., 19(1):1–6, January 1997.

[61] Gang Tan and Trent Jaeger. CFG Construction Soundness in Control-Flow Integrity.
In Proceedings of the 2017 Workshop on Programming Languages and Analysis for
Security, pages 3–13. ACM, 2017.

[62] Zhilong Wang and Peng Liu. GPT Conjecture: Understanding the Trade-offs between

Granularity, Performance and Timeliness in Control-Flow Integrity, 2019.

[63] Minh Hai Nguyen, Dung Le Nguyen, Xuan Mao Nguyen, and Tho Thanh Quan. Auto-
Detection of Sophisticated Malware using Lazy-Binding Control Flow Graph and Deep
Learning. Computers & Security, 76:128–155, 2018.

[64] Nour Moustafa and Jill Slay. UNSW-NB15: A Comprehensive Data Set for Net-
work Intrusion Detection Systems (UNSW-NB15 Network Data Set). In 2015 military
communications and information systems conference (MilCIS), pages 1–6. IEEE, 2015.

[65] IDS 2017 Datasets, 2019. https://www.unb.ca/cic/datasets/ids-2017.

html.

[66] Jun Li, Bodong Zhao, and Chao Zhang. Fuzzing: A Survey. Cybersecurity, 1(1):6, 2018.

[67] Nick Stephens, John Grosen, Christopher Salls, Andrew Dutcher, Ruoyu Wang, Ja-
copo Corbetta, Yan Shoshitaishvili, Christopher Kruegel, and Giovanni Vigna. Driller:
In In the Network and
Augmenting Fuzzing Through Selective Symbolic Execution.
Distributed System Security Symposium (NDSS), volume 16, pages 1–16, 2016.

[68] S. Bekrar, C. Bekrar, R. Groz, and L. Mounier. A Taint Based Approach for Smart
Fuzzing. In 2012 IEEE Fifth International Conference on Software Testing, Veriﬁcation
and Validation, pages 818–825, April 2012.

[69] Insu Yun, Sangho Lee, Meng Xu, Yeongjin Jang, and Taesoo Kim. QSYM : A Practi-
cal Concolic Execution Engine Tailored for Hybrid Fuzzing. In 27th USENIX Security
Symposium (USENIX Security 18), pages 745–761, Baltimore, MD, August 2018.
USENIX Association.

[70] Google Developers. Embeddings, 2016. https://developers.google.com/
machine-learning/crash-course/embeddings/video-lecture.

[71] Yann LeCun and Corinna Cortes. MNIST Handwritten Digit Database. 2010. http:

//yann.lecun.com/exdb/mnist/.

47

[72] Wenbo Guo, Dongliang Mu, Jun Xu, Purui Su, Gang Wang, and Xinyu Xing. Lemna:
Explaining deep learning based security applications. In Proceedings of the 2018 ACM
SIGSAC Conference on Computer and Communications Security, pages 364–379, 2018.

[73] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (Canadian Institute for
Advanced Research). 2010. https://www.cs.toronto.edu/˜kriz/cifar.
html.

48

