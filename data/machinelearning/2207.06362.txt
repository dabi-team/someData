2
2
0
2

l
u
J

3
1

]

C
O
.
h
t
a
m

[

1
v
2
6
3
6
0
.
7
0
2
2
:
v
i
X
r
a

Iterative Linear Quadratic Optimization for Nonlinear Control:
Differentiable Programming Algorithmic Templates

Vincent Roulet†, Siddhartha Srinivasa‡, Maryam Fazel(cid:5), Zaid Harchaoui†
† Department of Statistics University of Washington, Seattle, USA
‡ Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle
(cid:5) Department of Electrical and Computer Engineering, University of Washington, Seattle

Abstract

We present the implementation of nonlinear control algorithms based on linear and quadratic approximations
of the objective from a functional viewpoint. We present a gradient descent, a Gauss-Newton method, a Newton
method, differential dynamic programming approaches with linear quadratic or quadratic approximations, vari-
ous line-search strategies, and regularized variants of these algorithms. We derive the computational complexities
of all algorithms in a differentiable programming framework and present sufﬁcient optimality conditions. We
compare the algorithms on several benchmarks, such as autonomous car racing using a bicycle model of a car.
The algorithms are coded in a differentiable programming language in a publicly available package.

1 Introduction

We consider nonlinear control problems in discrete time with ﬁnite horizon, i.e., problems of the form

min
x0,...,xτ ∈Rnx
u0...,uτ −1∈Rnu

τ −1
(cid:88)

t=0

ht(xt, ut) + hτ (xτ )

(1)

subject to xt+1 = ft(xt, ut),

for t

0, . . . , τ

,
1
}
Rnu is the control applied to the system, ft : Rnx

x0 = ¯x0,

∈ {

−

where at time t, xt
∈
Rnx is the discrete dynamic, ht : Rnx
ﬁxed initial state. Problem (1) is entirely determined by the initial state and the controls as illustrated in Fig. 1.

R is the cost on the state and control variables and ¯x0

Rnx is the state of the system, ut

×
→
Rnx is a given

Rnu

→

∈

∈

Problems of the form (1) have been tackled in various ways, from direct approaches using nonlinear optimiza-
tion (Betts, 2010; Wright, 1990, 1991a; Pantoja, 1988; Dunn and Bertsekas, 1989; Rao et al., 1998) to convex
relaxations using semideﬁnite optimization (Boyd and Vandenberghe, 1997). A popular approach of the former
category proceeds by computing at each iteration the linear quadratic regulator associated with a linear quadratic
approximation of the problem around the current candidate solutions (Jacobson and Mayne, 1970; Li and Todorov,
2007; Sideris and Bobrow, 2005; Tassa et al., 2012). The computed feedback policies are then applied either along
the linearized dynamics or along the original dynamics to output a new candidate solution.

We present the algorithmic implementation of such approaches, from the computational complexities of the
optimization oracles to various implementations of line-search procedures. By considering these algorithms from
a functional viewpoint, we delineate the discrepancies between the different algorithms and identify the common
subroutines. We review the implementation of (i) a Gauss-Newton method (Sideris and Bobrow, 2005), (ii) a New-
ton method (Pantoja, 1988; Liao and Shoemaker, 1991; Dunn and Bertsekas, 1989), (iii) a Differential Dynamic
Programming (DDP) approach based on linear approximations of the dynamics and quadratic approximations
of the costs (Tassa et al., 2012), (iv) a DDP approach based on quadratic approximations of both dynamics and
costs (Jacobson and Mayne, 1970). We also consider regularized variants of the aforementioned algorithms with
their corresponding line-searches. In addition, we present simple formulations of the gradient and the Hessian
of the overall objective w.r.t.
the control variables that can be used to estimate the smoothness properties of
the objective. We also recall necessary optimality conditions for problem (1), present a counterexample of why
Pontryagin’s maximum principle (Pontryagin et al., 1963) does not apply in discrete time, and present sufﬁcient
optimality conditions derived from the continuous counterpart of the problem. Finally, we present numerical
comparisons of the algorithms and their variants on several control tasks such as autonomous car racing.

1

 
 
 
 
 
 
Figure 1: Computational scheme of the discrete time control problem (1).

Related work. The idea of tackling nonlinear control problems of the form (1) by minimizing linear quadratic
approximations of the problem is at least 50 years old (Jacobson and Mayne, 1970). One of the ﬁrst approaches
consisted of a Differential Dynamic Programming (DDP) approach using quadratic approximations as presented
by Jacobson and Mayne (1970) and further explored by Mayne and Polak (1975); Murray and Yakowitz (1984);
Liao and Shoemaker (1991). An implementation of a Newton method for nonlinear control problems of the
form (1) was developed after the DDP approach by Pantoja (1988); Dunn and Bertsekas (1989). A parallel
implementation of a Newton step and sequential quadratic programming methods were developed by Wright
(1990, 1991a), which led to efﬁcient implementations of interior point methods for linear quadratic control prob-
lems under constraints by using the block band diagonal structure of the system of KKT equations solved at
each step (Wright, 1991b). A detailed comparison of the DDP approach and the Newton method was conducted
by Liao and Shoemaker (1992), who observed that the original DDP approach generally outperforms its Newton
counterpart. We extend this analysis by comparing regularized variants of the algorithms. Finally, the storage of
second order information for DDP and Newton can be alleviated with a careful implementation in a differentibale
programming framework as done in our implementation and noted earlier by Nganga and Wensing (2021).

Simpler approaches consisting in taking linear approximations of the dynamics and quadratic approximations
of the costs were implemented as part of public software (Todorov et al., 2012). The resulting Iterative Linear
Quadratic Regulator algorithm as formulated by Li and Todorov (2007) amounts naturally to a Gauss-Newton
method (Sideris and Bobrow, 2005). A variant that mixes linear quadratic approximations of the problem with
a DDP approach was further analyzed empirically by Tassa et al. (2012). Here, we detail the line-searches for
both approaches and present their regularized variants. We provide detailed computational complexities of all
aforementioned algorithms that illustrate the trade-offs between the approaches.

Our derivations are based on the decomposition of the ﬁrst and second derivatives of the problem in a compact
formulation that can be used to, e.g., estimate the smoothness properties of the problem in a straightforward way.
We also present sufﬁcient optimality conditions of a candidate solution for problem (1) by translating sufﬁcient
conditions developed in continuous time by Arrow (1968); Mangasarian (1966); Kamien and Schwartz (1971).

For our experiments, we adapted the bicycle model of a miniature car developed by Liniger et al. (2015)
in Python. We provide an implementation in Python, available at https://github.com/vroulet/ilqc for
further exploration of the algorithms. This work also serves as a companion reference for the convergence analysis
of iterative linear quadratic optimization algorithms for nonlinear control by the same authors (Roulet et al., 2022).

Outline.
In Sec. 2 we recall how linear quadratic control problems are solved by dynamic programming and
how the linear quadratic case serves as a building block for nonlinear control algorithms. Sec. 3 presents how ﬁrst
and second order information of the objective can be expressed in terms of the ﬁrst and second order information
of the dynamics. The implementation of classical optimization oracles such as a gradient step, a Gauss-Newton
step or a Newton step is presented in Sec. 4. Sec 5 details the rationale and the implementation of differential
dynamic programming approaches. Sec. 6 details the line-search procedures. Sec. 7 presents the computational
complexities of each oracle in terms of space and time complexities in a differentiable programming framework.
We recall necessary optimality conditions for problem (1) and present sufﬁcient optimality conditions in Sec. 8. A
summary of all algorithms with detailed pseudocode and computational schemes is given in Sec. 9. All algorithms
are then tested on several synthetic problems in Sec. 10: swinging-up a ﬁxed pendulum, or a pendulum on a cart,
and autonomous car racing with simple dynamics or with a bicycle model.

2

......=Objective∈

∈

∈

∇

∇

→

→

Rp,

Rd, y

, we denote

→
A tensor

∈
Rn, we denote by

Rτ nx . For a function f : Rd

= (ai,j,k)1≤i≤d,1≤j≤p,1≤k≤n

xf (x, y) = (∂xifj(x, y))1≤i≤d,1≤j≤n

f (x) = (∂xifj(x))1≤i≤d,1≤j≤n
Rp

A
Rn×n(cid:48)
, R
∈
Rd0×p0×n0, P
Rd1×d2 , T

f (x)
−
Rd×p×n is represented as a list of matrices

Notations. For a sequence of vectors x1, . . . , xτ
x = (x1; . . . ; xτ )
the gradient of f , i.e., the transpose of the Jacobian of f on x. For a function f : Rd
x
∈
f : Rd

Rnx, we denote by semi-colons their concatenation s.t.
Rd×n
Rn, we denote for
×
Rd×n the partial gradient of f w.r.t. x on (x, y). For
∈
Rn, we denote the Lipschitz continuity constant of f as lf = supx,y∈Rd,x(cid:54)=y (cid:107)
∈

x
2/
−
(cid:107)
= (A1, . . . , An)
A
Rd×d(cid:48)
Rd×p×n and P
1, . . . n
. Given
∈
∈
A ∈
}
k=1 Rk,n(cid:48)P (cid:62)AkQ(cid:1)
k=1 Rk,1P (cid:62)AkQ, . . . , (cid:80)n
Rd(cid:48)×p(cid:48)×n(cid:48)
. For
Rd1×p1×n1 . Then,
Rn0×n1 denote
0[P, Q, R]
1 =
∈
Rd2×p2×n2 . If P, Q
0[P S, QT, RU ]
1[S, T, U ] =
∈
” in place of the identity matrix. For example, we denote
] = (cid:0)P (cid:62)A1Q, . . . , P (cid:62)AnQ(cid:1). If P, Q or R are vectors we consider the ﬂatten object.
·
Rn, rather than having
Rd, y
R1×1×n. Similarly, for z
a
2 the
(cid:107)
(cid:107)
Rd,

where Ak = (ai,j,k)1≤i≤d,1≤j≤p
Rp×p(cid:48)
0
∈
A
for S
or R are identity matrices, we use the symbol “

Rd×p for k
[P, Q, R] = (cid:0)(cid:80)n
Rp0×p1, R
∈
Rn1×n2, we have

·
A
, z] = (cid:80)n
Rn, we denote
∈
·
Rd×p and we deﬁne the norm of a tensor
2,2 the spectral norm of a matrix A
∈
(cid:107)
2).
z
y
x
[x, y, z]/(
2,2,2 = supx(cid:54)=0,y(cid:54)=0,z(cid:54)=0 A
2
(cid:107)
(cid:107)
(cid:107)
(cid:107)
(cid:107)
Rn composed of coordinates fj : Rd
R for j
→
→

A
∈
Euclidean norm for a
induced by the Euclidean norm as

[P, Q, In] =
A
In particular, for x
]
·

] = (cid:0)x(cid:62)A1y, . . . , x(cid:62)Any(cid:1)(cid:62)

For a multivariate function f : Rd

A
Rd0×d1 , Q
∈
Rp1×p2 , U

∈
Rd×p. We denote

Rp, we denote

k=1 zkAk

f (y)
(cid:107)

1, . . . , n

[P, Q,

A
(cid:107)

[x, y,

[x, y,

[
A

(cid:107)A(cid:107)

∈ {

, Q

,
·

A

A

A

A

A

A

2.

∈

∈

∈

∈

∈

∈

∈

∈

∈

∈

(cid:107)

(cid:107)

y

2

·

its Hessian x
f : Rd
Rp
×
Rd, y
on x
∈
2
yyf (x, y)

2f (x) = (

Rd as a tensor
∇
Rn composed of coordinates fj : Rd
Rp by deﬁning, e.g.,
2
xxf (x, y) = (
∇
Rd×p×n,
2
xyf (x, y)

2fn(x))
2f1(x), . . . ,
Rp
R for j
×
2
xxf1(x, y), . . . ,
∇
2
yxf (x, y)

∈
→
∈
Rp×p×n,

∇
→

∇

∇

∈
∇
For a function f : Rd

∈

Rn, and x

∈
expansion of f around x and the quadratic expansion of f around x as, respectively,

→

Rp×d×n are deﬁned similarly.
Rd, we deﬁne the ﬁnite difference expansion of f around x, the linear

∇

∈

∈ {

, we denote
}
Rd×d×n. For a multivariate function
∈
1, . . . , n
, we decompose its Hessian
}
∈ {
Rd×d×n. The quantities
2
xxfn(x, y))
∇

∈

δx
f (y) = f (x + y)

f (x),

−

(cid:96)x
f (y) =

∇

f (x)(cid:62)y,

qx
f (y) =

∇

f (x)(cid:62)y +

2f (x)[y, y,

].

·

The linear and quadratic approximations of f around x are then f (x + y)
f (x) + qx

f (y) respectively.

≈

f (y) and f (x + y)

1
2 ∇
f (x) + (cid:96)x

(2)

≈

2 From Linear Control Problems to Nonlinear Control Algorithms

Algorithms for nonlinear control problems revolve around the resolution of linear quadratic control problems by
dynamic programming. Therefore, we start by recalling the rationale of dynamic programming and how discrete
time control problems with linear dynamics and quadratic costs can be solved by dynamic programming.

2.1 Dynamic Programming

The idea of dynamic programming is to decompose dynamical problems such as (1) into a sequence of nested
subproblems deﬁned by the cost-to-go from xt at time t

0, . . . , τ

∈ {

1
}

−

ct(xt) =

min
ut,...,uτ −1∈Rnu
yt,...,yτ ∈Rnx

τ −1
(cid:88)

s=t

hs(ys, us) + hτ (yτ )

subject to ys+1 = fs(ys, us)

for s

t, . . . , τ

∈ {

,

1
}

−

yt = xt.

The cost-to-go from xτ at time τ is simply the last cost, namely, cτ (xτ ) = hτ (xτ ), and the original problem (1)
amounts to compute c0(¯x0). The cost-to-go functions deﬁne nested subproblems that are linked for t
1
}

by Bellman’s equation (Bellman, 1971)

0, . . . , τ

∈ {

−

ct(xt) = min
ut∈Rnu

ht(xt, ut) +

min
ut+1,...,uτ −1∈Rnu
yt+1,...,yτ ∈Rnx

τ −1
(cid:88)

s=t+1

hs(ys, us) + hτ (yτ )

subject to

ys+1 = fs(ys, us) for s

= min
ut∈Rnu

ht(xt, ut) + ct+1(ft(xt, ut)).

t + 1, . . . , τ

∈ {

−

1
}

, yt+1 = ft(xt, ut)
(3)

3

0; . . . ; u∗

τ −1)(cid:3).

t=0, initial state ¯x0, procedure BP

→

(u∗

1, . . . , 0 do

t=0 , (ht)τ

t=0, ¯x0, BP
t=0 , costs (ht)τ

Algorithm 1 Dynamic programming procedure
(cid:2)DynProg : (ft)τ −1
1: Inputs: Dynamics (ft)τ −1
2: Initialize cτ = hτ
3: for t = τ
4:
5: end for
6: Initialize x∗
7: for t = 0, . . . , τ
Compute u∗
8:
9: end for
10: Output: Optimal command u = (u∗

1 do
t = πt(x∗

t+1 = ft(x∗

t ), x∗

0 = ¯x0

−

−

Compute ct, πt = BP(ft, ht, ct+1), store πt

t , u∗
t )

0; . . . ; u∗

τ −1) for problem (1)

(cid:46) BP = LQBP (Algo. 2) for linear quadratic control

The optimal control at time t from state xt is given by ut = πt(xt), where πt, called a policy, is given by

πt(xt) = arg min

ht(xt, ut) + ct+1(ft(xt, ut))

ut∈Rnu {

.

}

Deﬁne the procedure that back-propagates the cost-to-go functions as

BP : ft, ht, ct+1

→

(cid:18) ct : x
πt : x

minu∈Rnu
→
arg minu∈Rnu {
→

{

ht(x, u) + ct+1(ft(x, u))

,
ht(x, u) + ct+1(ft(x, u))

}

(cid:19)

.

}

A dynamic programming approach, formally described in Algo. 1, solves problems of the form (1) as follows.

1. Compute recursively the cost-to-go functions ct for t = τ, . . . , 0 using Bellman’s equation (3), i.e., compute

from cτ = hτ ,

ct, πt = BP(ft, ht, ct+1)

for t

and record at each step the policies πt.

τ
∈ {

−

1, . . . , 0

,
}

2. Unroll the optimal trajectory that starts from time 0 at ¯x0, follows the dynamics ft, and uses at each step the

optimal control given by the computed policies, i.e., starting from x∗

0 = ¯x0, compute

t = πt(x∗
u∗

t ),

t+1 = ft(x∗
x∗

t , u∗
t )

for t = 0, . . . , τ

1.

−

(4)

The resulting command u∗ = (u∗
In the following, we consider Algo. 1 as a procedure

0; . . . ; u∗

τ −1) and trajectory x∗ = (x∗

1; . . . ; x∗

τ ) are then optimal for problem (1).

DynProg : (ft)τ −1

t=0 , (ht)τ

t=0, ¯x0, BP

0, . . . , u∗
u∗

τ −1.

→

The bottleneck of the approach is the ability to solve Bellman’s equation (3), i.e., having access to the procedure
BP deﬁned above.

2.2 Linear Dynamics, Quadratic Costs

For linear dynamics and quadratic costs, problem (1) takes the form

min
x0,...,xτ ∈Rnx
u0...,uτ −1∈Rnu

τ −1
(cid:88)

t=0

(cid:18) 1
2

x(cid:62)
t Ptxt +

1
2

t Qtut + x(cid:62)
u(cid:62)

t Rtut + p(cid:62)

t xt + q(cid:62)

t ut

(cid:19)

+

1
2

τ Pτ xτ + p(cid:62)
x(cid:62)

τ xτ

subject to xt+1 = Atxt + Btut,

for t

0, . . . , τ

x0 = ¯x0.

∈ {
Namely, we have ht(xt, ut) = 1
t Qtut + x(cid:62)
t ut and ft(xt, ut) = Atxt +
Btut. In that case, under appropriate conditions on the quadratic functions, Bellman’s equation (3) can be solved
analytically as recalled in Lemma 2.1. Note that the operation LQBP deﬁned in (5) amounts to computing the
qt(x, u) + ct+1((cid:96)t(x, u)), namely, the block
Schur complement of a block of the Hessian of the quadratic x, u
corresponding to the Hessian w.r.t. the control variables.

−
t Rtut + p(cid:62)

t Ptxt + 1

t xt + q(cid:62)

2 u(cid:62)

2 x(cid:62)

→

1

,
}

4

Lemma 2.1. The back-propagation of cost-to-go functions for linear dynamics and quadratic costs is imple-
mented1 in Algo. 2 which computes

LQBP : ((cid:96)t, qt, ct+1)

→

(cid:18) ct : x
πt : x

qt(x, u) + ct+1((cid:96)t(x, u))
minu∈Rnu
{
→
arg minu∈Rnu {

}
qt(x, u) + ct+1((cid:96)t(x, u))

→

(cid:19)

,

}

(5)

for linear functions (cid:96)t and quadratic functions qt, ct+1, s.t. qt(x,

) + ct+1((cid:96)t(x,

Proof. Consider (cid:96)t, qt, ct+1 to be parameterized as (cid:96)t(x, u) = Ax+Bu, qt(x, u) = 1
p(cid:62)x + q(cid:62)u, ct+1(x) = 1

t+1. The cost-to-go function at time t is

2 x(cid:62)Jt+1x + j(cid:62)

t+1x + j0

·

)) is strongly convex for any x.
·

2 x(cid:62)P x+ 1

2 u(cid:62)Qu+x(cid:62)Ru+

ct(x) =

1
2

x(cid:62)P x + p(cid:62)x + j0

t+1

+ min
u∈R

(cid:26) 1
2

(Ax + Bu)(cid:62)Jt+1(Ax + Bu) + j(cid:62)

t+1(Ax + Bu) +

u(cid:62)Qu + x(cid:62)Ru + q(cid:62)u

(cid:27)

.

1
2

Since h(x,
is

) + ct+1((cid:96)(x,
·

)) is strongly convex, we have that Q + B(cid:62)Jt+1B
·

(cid:31)

0. Therefore, the policy at time t

πt(x) =

(Q + B(cid:62)Jt+1B)−1[(R(cid:62) + B(cid:62)Jt+1A)x + q + B(cid:62)jt+1].

−

Using that minu u(cid:62)M u/2 + m(cid:62)x =
−
B(cid:62)Jt+1A)x + q + B(cid:62)jt+1, we get that the cost-to-go function at time t is given by

m(cid:62)M −1m/2 where, here, M = Q + B(cid:62)Jt+1B, m = (R(cid:62) +

ct(x) =

x(cid:62) (cid:0)P + A(cid:62)Jt+1A

1
2
+ (cid:0)p + A(cid:62)jt+1

(R + A(cid:62)Jt+1B)(Q + B(cid:62)Jt+1B)−1(R(cid:62) + B(cid:62)Jt+1A)(cid:1) x

−

(R + A(cid:62)Jt+1B)(Q + B(cid:62)Jt+1B)−1(q + B(cid:62)jt+1)(cid:1)(cid:62)

x

−

(q + B(cid:62)jt+1)(cid:62)(Q + B(cid:62)Jt+1B)−1(q + B(cid:62)jt+1) + j0

t+1.

1
2

−

If problem (1) consists of linear dynamics and quadratic costs that are strongly convex w.r.t.

the control
variable, the procedure LQBP can be applied iteratively in a dynamic programming approach to give the solution
of the problem, as formally stated in Corollary 2.2.

Corollary 2.2. Consider problem (1) such that for all t
ht(x,

∈ {
) strongly convex for any x, and hτ is convex quadratic. Then, the solution of problem (1) is given by
·

, ft is linear, ht is convex quadratic with
}

0, . . . , τ

−

1

with DynProg implemented in Algo. 1 and LQBP implemented in Algo. 2

u∗ = DynProg((ft)τ −1

t=0 , (ht)τ

t=0, ¯x0, LQBP),

1

∈ {

0, . . . , τ

for a given x

Proof. Note that at time t
the composition of a convex function and a linear function and ct+1(ft(x,
the sum of a convex and a strongly convex function. Moreover, x, u
since x, u
assumption. Therefore ct : x
convex function.

)) is convex as
·
) is then strongly convex as
·
ct+1(ft(x, u)) + ht(x, u) is jointly convex
ct+1(ft(x, u)) is the composition of a convex function with a linear function and ht is convex by
minu∈Rnu ct+1(ft(x, u)) + ht(x, u) is convex as the partial inﬁmum of jointly

Rnx , if ct+1 is convex, then ct+1(ft(x,
)) + ht(x,

→

→

→

−

∈

}

·

In summary, at time t

) is strongly convex,
·
and (ii) ct is convex. This ensures that the assumptions of Lemma 2.1 are satisﬁed at each iteration of Algo. 1
(line 4) since cτ = hτ is convex.

, if ct+1 is convex, then (i) ct+1(ft(x,

)) + ht(x,
·

0, . . . , τ

1
}

∈ {

−

2.3 Nonlinear Control Algorithm Example

Nonlinear control algorithms based on nonlinear optimization use linear or quadratic approximations of the dy-
namics and the costs at a current candidate sequence of controllers to apply a dynamic programming procedure
to the resulting problem. For example, the Iterative Linear Quadratic Regulator (ILQR) algorithm uses linear
approximations of the dynamics and quadratic approximations of the costs (Li and Todorov, 2007). Each iteration
of the ILQR algorithm is composed of three steps illustrated in Fig. 4.

1For ease of reference and comparisons, we grouped all following procedures, algorithms, and computational schemes in Sec. 9.

5

Iterative Linear Quadratic Regulator Iteration.

1. Forward pass: Given a set of control variables u0, . . . , uτ −1, compute the trajectory x1, . . . , xτ as xt+1 =
. Record
}
1
, the gradients of the dynamics and the gradients and
}

ft(xt, ut) starting from x0 = ¯x0, and the associated costs ht(xt, ut), hτ (xτ ), for t
along the computations, i.e., for t
Hessians of the costs.

0, . . . , τ

0, . . . , τ

∈ {

∈ {

−

−

1

2. Backward pass: Compute the optimal policies associated with the linear quadratic control problem

min
y0,...yτ ∈Rnx
v0,...,vτ −1∈Rnu

τ −1
(cid:88)

t=0

(cid:18) 1
2

y(cid:62)
t Ptyt +

1
2

t Qtvt + y(cid:62)
v(cid:62)

t Rtvt + p(cid:62)

t yt + q(cid:62)

t vt

(cid:19)

+

1
2

τ Pτ yτ + p(cid:62)
y(cid:62)

τ yτ

subject to yt+1 = Atyt + Btvt,

for t

where Pt =
pt =

At =

2
ht(xt, ut) Qt =
xtxt
xtht(xt, ut)
qt =
xtft(xt, ut)(cid:62) Bt =

∇

∇

∇

which can be written compactly as

1

,
}

0, . . . , τ

−
ht(xt, ut) Rt =

∇

∈ {
2
utut
utht(xt, ut)
utft(xt, ut)(cid:62),

∇

y0 = 0,
2
xtut

∇

∇

ht(xt, ut)

τ −1
(cid:88)

qxt,ut
ht

(yt, vt) + qxτ
hτ

(yτ )

min
y0,...yτ ∈Rnx
v0,...,vτ −1∈Rnu

t=0
subject to yt+1 = (cid:96)xt,ut

ft

(6)

y0 = 0,

(yt, vt),

for t

0, . . . , τ

∈ {
(yt, vt) = 1

1

,
}
−
t Ptyt + 1

2 y(cid:62)

(yτ ) = 1

τ yτ and qxt,ut

where qxτ
τ Pτ yτ + p(cid:62)
t Qtvt + y(cid:62)
t vt
hτ
are the quadratic expansions of the costs and (cid:96)xt,ut
(yt, vt) = Atyt + Btvt is the linear expansion of the
dynamics, both expansions being deﬁned around the current sequence of controls and associated trajectory.
The optimal policies associated to this problem are obtained by computing recursively, starting from cτ =
qxτ
hτ

t Rtvt + p(cid:62)

t yt + q(cid:62)

2 y(cid:62)

2 v(cid:62)

ht

ft

,

ct, πt = LQBP((cid:96)xt,ut

ft

, qxt,ut
ht

, ct+1)

for t

τ
∈ {

1, . . . , 0
}

,

−

where LQBP presented in Algo. 2 outputs afﬁne policies of the form πt : yt

Ktyt + kt.

→

3. Roll-out pass: Deﬁne the set of candidate policies as

of controllers is then given as unext
along the linearized dynamics as

t = ut + vγ

πγ
t : y
t is given by rolling out the policies πγ
t , where vγ

γkt + Kty for γ

. The next sequence
}
t from y0 = 0

→

≥

0

{

for γ found by a line-search such that

t = πγ
vγ

t (yt),

yt+1 = (cid:96)xt,ut

ft

(yt, vγ

t ),

τ −1
(cid:88)

t=0

(ht(xt + yt, ut + vγ
t )

−

ht(xt, ut)) + hτ (xτ + yτ )

hτ (xτ )

−

≤

γc0(0),

with c0(0) the solution of the linear quadratic control problem (6).

The procedure is then repeated on the next sequence of control variables. Ignoring the line-search phase (namely,
taking γ = 1), each iteration can be summarized as computing unext = u + v where

v = DynProg(((cid:96)xt,ut

ft

t=0 , (qxt,ut
)τ −1

ht

)τ
t=0, y0, LQBP)

for y0 = 0, where DynProg is presented in Algo 1. Note that for convex costs ht such that ht(x,
convex, the subproblems (6) satisfy the assumptions of Cor. 2.2.

·

) is strongly

The iterations of the following nonlinear control algorithms can always be decomposed into the three passes
described above for the ILQR algorithm. The algorithms vary by (i) what approximations of the dynamics and
the costs are computed in the forward pass, (ii) how the policies are computed in the backward pass, (iii) how the
policies are rolled out.

6

3 Objective Decomposition

Problem (1) is entirely determined by the choice of the initial state and a sequence of control variables, such that
the objective in (1) can be written in terms of the control variables u = (u0; . . . ; uτ −1) as

J

τ −1
(cid:88)

(u) =

ht(xt, ut) + hτ (xτ )

t=0
s.t. xt+1 = ft(xt, ut)

for t

0, . . . , τ

∈ {

,
1
}

−

x0 = ¯x0.

The objective can be decomposed into the costs and the control of τ steps of a sequence of dynamics deﬁned as
follows.

Deﬁnition 3.1. We deﬁne the control of τ discrete time dynamics (ft : Rnx
Rτ nx , which, given an initial point x0
Rnx
Rτ nu , outputs the corresponding trajectory x1, . . . , xτ , i.e.,

Rτ nu

→

×

∈

Rnu

×

→

Rnx and a sequence of controls u = (u0; . . . ; uτ −1)

Rnx )τ −1

t=0 as the function f [τ ] :

∈

f [τ ](x0, u) = (x1; . . . ; xτ )
s.t. xt+1 = ft(xt, ut)

for t

0, . . . , τ

−
Overall, problem (1) can be written as the minimization of a composition

∈ {

.
1
}

(7)

min

u∈Rτ nu {J

(u) = h

g(u)

}

◦

, where h(x, u) =

τ −1
(cid:88)

t=0

ht(xt, ut) + hτ (xτ ),

g(u) = (f [τ ](¯x0, u), u),

(8)

for x = (x1; . . . ; xτ ) and u = (u0; . . . ; uτ

1).

−

The implementation of classical oracles for problem (8) relies on the dynamical structure of the problem
encapsulated in the control f [τ ] of the discrete time dynamics (ft)τ −1
t=0 . The following lemma presents a compact
formulation of the ﬁrst and second order information of f [τ ] with respect to the ﬁrst and second order information
of the dynamics (ft)τ −1
t=0 .
Lemma 3.2. Consider the control f [τ ] of τ dynamics (ft)τ −1
For x = (x1; . . . ; xτ ) and u = (u0; . . . ; uτ −1), deﬁne

t=0 as deﬁned in Def. 3.1 and an initial point x0

Rnx .

∈

F (x, u) = (f0(x0, u0); . . . ; fτ −1(xτ −1, uτ −1)).

The gradient of the control f [τ ] of the dynamics (ft)τ −1

Rτ nu can be written

uf [τ ](x0, u) =

∇

∇

t=0 on u

∈
uF (x, u)(I

xF (x, u))−1.

−∇
Rτ nu can be written

The Hessian of the control f [τ ] of the dynamics (ft)τ −1

t=0 on u

uuf [τ ](x0, u)=
2

∇
where M = (I

2
xxF (x, u)[N, N, M ]+

∇
xF (x, u))−1 and N =

−∇

2
uuF (x, u)[
·

∈
, M ]+
∇
·
uf [τ ](x0, u)(cid:62).

,

∇

2
xuF (x, u)[N,

, M ]+
·

∇

2
uxF (x, u)[
·

, N, M ].

∇

Proof. Denote simply, for u
φ can be decomposed, for u

Rτ nu, φ(u) = f [τ ](x0, u) with x0 a ﬁxed initial state. By deﬁnition, the function
Rτ nu , as φ(u) = (φ1(u); . . . ; φτ (u)), such that

t u)

for t

0, . . . , τ

∈ {
Inu is such that E(cid:62)

,
1
}

−

(9)

, Et = et
}

t u = ut, with et the t + 1th canonical
Rnu×nu the identity matrix. By derivating (9), we get, denoting

⊗

with φ0(u) = x0 and for t
vector in Rτ ,
⊗
xt = φt(u) for t

∈ {

0, . . . , τ
−
the Kronecker product and Inu ∈
and using that E(cid:62)
∈ {
φt+1(u) =

0, . . . , τ

φt(u)

}

1

∇

t u = ut,

So, for v = (v0; . . . ; vτ −1)
we have, with y0 = 0,

∈

∇
∇
Rτ nu , denoting

xtft(xt, ut) + Et

utft(xt, ut)

for t

∇

φ(u)(cid:62)v = (y1; . . . ; yτ ) s.t.

∇

∇

0, . . . , τ

∈ {
φt(u)(cid:62)v = yt for t

−

.
1
}

∈
∈
φt+1(u) = ft(φt(u), E(cid:62)

1, . . . , τ

,
}

∈ {

(10)

yt+1 =

∇

xtft(xt, ut)(cid:62)yt +

∇

utft(xt, ut)(cid:62)vt

for t

0, . . . , τ

∈ {

.
1
}

−

7

Denoting y = (y1; . . . ; yτ ), we have then

where A = (cid:80)τ −1
with Bt =

t=1 ete(cid:62)
ut ft(xt, ut)(cid:62) for t

t+1 ⊗

At with At =
0, . . . , τ

∇

−

∈ {

(I

A)y = Bv,

i.e.,

φ(u)(cid:62)v = (I

A)−1Bv,

∇
xtft(xt, ut)(cid:62) for t

−
1, . . . , τ

∈ {

1
}

−

and B = (cid:80)τ

t=1 ete(cid:62)

t ⊗

Bt−1

∇
−

1

, i.e.
}














A =

0 A1
...
. . .

0
. . .

...
0

. . .

. . .
0
...
. . .
. . .
0
. . . Aτ −1
0
. . .

, B =


















B0

0
...
0

0
. . .
. . .
. . .

0
...
0

. . .
. . .
. . .
0 Bτ −1









.

By deﬁnition of F in the claim, one easily check that A =
get

∇

xF (x, u)(cid:62) and B =

uF (x, u)(cid:62). Therefore we

uf [τ ](x0, u) =

φ(u) =

uF (x, u)(I

∇

∇
Rp, f : Rp

∇
R, x
Rn, we have

−∇
Rd, we have
2(f

∈

g(x)(cid:62)+
∇
2f (x)[
] +
∇
·
◦
gt for gt(u) = (φt(u), E(cid:62)
t u), we get from Eq. (9), us-

∇
g(x)(cid:62),

∇
g(x)(cid:62),

2f (x)

g(x)

∇

∇

∇

∇
xF (x, u))−1.
2(f
g)(x) =
◦
g)(x) =

∇

For the Hessian, note that for g : Rd
f (x)]
,
·
f (x)]
,
·
gt(u) = (

→
→
Rd×d. If f : Rp
Rd×d×n. Applying this on ft
φt(u), Et),

2g(x)[
,
·
∇
2g(x)[
,
·
∇
ing that
∇

∇
∇

→

∈
∈
∇
2φt+1(u) =

◦

∇

∇
+

+

∇

,
·

∇
ft(xt, ut)[

2φt(u)[
,
·
2
xtxt
2
xtut
∇
2φ0(u) = 0. Therefore for v = (v0; . . . ; vτ −1), w = (w0; . . . ; wτ −1)

xtft(xt, ut)]
φt(u)(cid:62),
φt(u)(cid:62),
∇
φt(u)(cid:62), E(cid:62)
] +
t ,
·

ft(xt, ut)[E(cid:62)
∇
ft(xt, ut)[E(cid:62)
t ,

t , E(cid:62)
t ,
φt(u)(cid:62),

] +
·
2
utxt

ft(xt, ut)[

2
utut

·
],
·

∇

∇

∇

∇

]

Rτ nu ,

∈

0, . . . , τ
for t
µ = (µ1; . . . ; µτ )

∈ {

−
∈

1
, with
}
Rτ nx , we get

∇

2φt+1(u)[v, w, µt+1]

2φ(u)[v, w, µ] =

∇

=

τ −1
(cid:88)
t=0 ∇
τ −1
(cid:16)
(cid:88)

∇

t=0

2
xtxt

ft(xt, ut)[yt, zt, λt+1] +

2
utut

∇

ft(xt, ut)[vt, wt, λt+1]

(11)

+

2
xtut

∇

ft(xt, ut)[yt, wt, λt+1] +

ft(xt, ut)[vt, zt, λt+1]

(cid:17)

,

2
utxt

∇

where y = (y1; . . . ; yτ ) =
Rτ nx is deﬁned by

∇

φ(u)(cid:62)v, z = (z1; . . . ; zτ ) =

∇

φ(u)(cid:62)w, with y0 = z0 = 0 and λ = (λ1; . . . ; λτ )

∈

λt =

xtft(xt, ut)λt+1 + µt

for t

1, . . . , τ

∇
On the other hand, denoting Ft(x, u) = ft(xt, ut) for t
variables u can be decomposed as

∈ {

∈ {
0, . . . , τ

−
1

1

,
}

λτ = µτ .

, the Hessian of F with respect to the
}

−

2
uuF (x, u)[v, w, λ] =

∇

τ −1
(cid:88)
t=0 ∇

2
uuFt(x, u)[v, w, λt+1] =

τ −1
(cid:88)
t=0 ∇

2
utut

ft(xt, ut)[vt, wt, λt+1].

The Hessian of F with respect to the variable x can be decomposed as

2
xxF (x, u)[y, z, λ] =

∇

τ −1
(cid:88)
t=0 ∇

2
xxFt(x, u)[y, z, λt+1] =

τ −1
(cid:88)
t=1 ∇

2
xtxt

ft(xt, ut)[yt, zt, λt+1].

A similar decomposition can be done for

∇
2
xxF (x, u)[y, z, λ]+

∇

2φ(u)[v, w, µ]=

∇
Finally, by noting that y = (
λ = (I

∇

−∇
∇
xF (x, u))−1µ, the claim is shown.

uF (x, u)(I

−∇

2
xuF (x, u). From Eq. (11), we then get
2
uuF (x, u)[v, w, λ]+

2
xuF (x, u)[y, w, λ]+

∇

xF (x, u))−1)(cid:62)v, z = (

∇

uF (x, u)(I

−∇

8

2
uxF (x, u)[v, z, λ].

∇
xF (x, u))−1)(cid:62)w, and

Lemma 3.2 can be used to get estimates on the smoothness properties of the control of τ dynamics given the

smoothness properties of each individual dynamics.
Lemma 3.3. If τ dynamics (ft)τ −1
u
Lipschitz continuous gradients with

→

f [τ ](x0, u), with f [τ ] the control of the τ dynamics (ft)τ −1

t=0 are Lipschitz continuous with Lipschitz continuous gradients, then the function
t=0 , is lf [τ ] -Lipschitz continuous and has Lf [τ ] -

lf [τ ]

lu
f S,

Lf [τ ]

S(Lxx

f l2

f [τ ] + 2Lxu

f lf [τ ] + Luu

f ),

(12)

≤
where S= (cid:80)τ −1
f )t, lu
t=0 (lx
= supx,u (cid:107)∇
ft
2,2,2, Lxu
2
Luu
uuf (x, u)
= supx,u (cid:107)∇
ft
ft
(cid:107)
mum over all dynamics such as lx

≤
2,2, lx
= supx,u (cid:107)∇
uf (x, u)
ft
(cid:107)
2
xuf (x, u)
= supx,u (cid:107)∇
f = maxt∈{0,...,τ −1} lx
.
ft

2,2, Lxx
xf (x, u)
ft
(cid:107)

= supx,u (cid:107)∇

2
xxf (x, u)
(cid:107)

2,2,2 and we drop the index t to denote the maxi-
(cid:107)

2,2,2,

∇

(cid:107)∇

xtft(xt, ut) and (A

xF (x, u) = (cid:80)τ −1

→
t=1 et+1e(cid:62)

Proof. The Lipschitz continuity constant of u
the norm of the gradients and the Hessians. With the notations of Lemma 3.2,
since it can be written
we have (I
by
the Hessian of u
uuf [τ ](x0, u)=
2

−∇
uf [τ ](x0, u)
2,2
(cid:107)
f [τ ](x0, u) can be decomposed as
→
2
xxF (x, u)[N, N, M ]+

f [τ ](x0, u) and its gradients can be estimated by upper bounding
xF (x, u) is nilpotent of degree τ
BD). Hence,
⊗
xF (x, u)t. The Lipschitz continuity constant of f [τ ] is then estimated
(cid:80)τ −1
f )t. As shown in Lemma 3.2,

xF (x, u))−1 = (cid:80)τ −1
t=0 ∇
uF (x, u)
2,2
(cid:107)

∇
where M = (I
2
abF (x, u)

∇
xF (x, u))−1 and N =
x, u
f for a, b
(cid:107)∇
(cid:107)A(cid:107)
as deﬁned in the notations. Note that for a given tensor

∇
uf [τ ](¯x0, u)(cid:62). Given the structure of F , bounds on the Hessians are
∇
w.r.t. the Euclidean norm
, where
}
Rd×p×n and P, Q, R of appropriate sizes, we have

2,2,2 is the norm of a tensor

2
uuF (x, u)[
·

2
uxF (x, u)[
·

2
xuF (x, u)[N,

xF (x, u))−1

, M ]+
·

, M ]+
·

−∇
2,2,2
(cid:107)

D) = (AC

t=0 (lx

∇
B)(C

t ⊗∇

≤ (cid:107)∇

(I
(cid:107)

2,2
(cid:107)

Lab

−∇

∈ {

lu
f

∇

∇

A

≤

≤

⊗

⊗

,

, N, M ],

[P, Q, R]
2,2,2
(cid:107)

(cid:107)A

P
2,2,2
≤ (cid:107)A(cid:107)
(cid:107)
uuf [τ ](x0, u)
2
||
||∇

2,2
(cid:107)

2,2,2

2,2

Q
(cid:107)
(cid:107)
Lxx
f (cid:107)

A ∈

R

2,2. We then get
(cid:107)
(cid:107)
N

M

2
2,2(cid:107)
(cid:107)

(cid:107)
f = Lux
where for twice differentiable functions we used that Lxu
f .

≤

2,2 + Luu
f (cid:107)

M

2,2 + 2Lxu
f (cid:107)
(cid:107)

M

2,2
(cid:107)

N
(cid:107)

2,2,
(cid:107)

4 Classical Optimization Oracles

4.1 Formulation

Classical optimization algorithms rely on the availability to some oracles on the objective. Here, we consider these
oracles to compute the minimizer of an approximation of the objective around the current point with an optional
regularization term. Formally, on a point u

Rτ nu , given a regularization ν

0, for an objective of the form

∈

≥

as in (8), we consider

(i) a gradient oracle to use a linear expansion of the objective, and to output, for ν > 0,

min
u∈Rτ nu

g(u),

h

◦

arg min
v∈Rτ nu

(cid:110)

(cid:96)u
h◦g(v) +

(cid:111)

ν
2 (cid:107)

v

2
2
(cid:107)

ν−1

=

−

(h

∇

◦

g)(u),

(13)

(ii) a Gauss-Newton oracle to use a linear quadratic expansion of the objective, and to output
(cid:111)

(cid:110)

arg min
v∈Rτ nu

qg(u)
h

((cid:96)u

g (v)) +

ν
2 (cid:107)

v

2
2
(cid:107)

=

(
∇

−

g(u)

2h(g(u))

g(u) + ν I)−1

(h

g)(u),

(14)

∇

∇

∇

◦

(iii) a Newton oracle to use a quadratic expansion of the objective, and to output
ν
2 (cid:107)

g)(u) + ν I)−1

qu
h◦g(v) +

arg min
v∈Rτ nu

(
∇

2
2
(cid:107)

2(h

−

=

(cid:110)

(cid:111)

v

◦

∇

g)(u),

(h

◦

(15)

where (cid:96)x

f , qx

f are the linear and quadratic expansions of a function f around x as deﬁned in the notations in Eq. (2).
Gauss-Newton and Newton oracles are generally deﬁned without a regularization, i.e., for ν = 0. However,
in practice, a regularization may be necessary to ensure that Gauss-Newton and Newton oracles provide a descent
direction. Moreover, the reciprocal of the regularization, 1/ν, can play the role of a stepsize as detailed in Sec. 6.
Lemma 4.1 presents how the computation of the above oracles can be decomposed into the dynamical structure of
the problem.

9

Lemma 4.1. Consider a nonlinear dynamical problem summarized as

min
u∈Rτ nu

h

◦

g(u), where h(x, u) =

τ −1
(cid:88)

t=0

ht(xt, ut) + hτ (xτ ),

g(u) = (f [τ ](¯x0, u), u),

with f [τ ] the control of τ dynamics (ft)τ −1

t=0 as deﬁned in Def. 3.1.

Let u = (u0; . . . ; uτ −1) and f [τ ](¯x0, u) = (x1; . . . ; xτ ). Gradient (13), Gauss-Newton (14) and Newton (15)

g amount to solving for v∗ = (v∗

0; . . . ; v∗

τ −1) linear quadratic control problems of the form

oracles for h

◦

τ −1
(cid:88)

qt(yt, vt) + qτ (yτ )

(16)

min
v0,...,vτ −1∈Rnu
y0,...,yτ ∈Rnx

t=0
subject to yt+1 = (cid:96)xt,ut

ft

(yt, vt)

for t

0, . . . , τ

∈ {

,

1
}

−

y0 = 0,

where for

(i) the gradient oracle (13), qτ (yτ ) = (cid:96)xτ
hτ

(yτ ) and, for 0

t

≤

≤

τ

qt(yt, vt) = (cid:96)xt,ut

ht

(yt, vt) +

(ii) the Gauss-Newton oracle (14), qτ (yτ ) = qxτ
hτ

(yτ ) and, for 0

≤

qt(yt, vt) = qxt,ut

ht

(yt, vt) +

(iii) for the Newton oracle (15), qτ (yτ ) = qxτ
hτ

(yτ ) and, deﬁning

1,

−
ν
2 (cid:107)

t

≤
ν
2 (cid:107)

vt

2
2,
(cid:107)

1,

τ

−

vt

2
2,
(cid:107)

λτ =

∇
t

hτ (xτ ),

λt =

xt ht(xt, ut) +

∇

∇

xtft(xt, ut)λt+1

for t

τ
∈ {

−

1, . . . , 1

,
}

(17)

τ

≤

−

1,

≤

we have, for 0

qt(yt, vt) = qxt,ut

ht

(yt, vt) +

1
2 ∇

2ft(xt, ut)[
,
·

·

, λt+1](yt, vt) +

ν
2 (cid:107)

vt

2
2,
(cid:107)

Rnu

Rnx , x

where for f : Rnx

2f (x, u)[
·

,

∇

→
×
, λ] : (y, v)
·

→∇

Rnx , u

Rnu , λ

∈

∈

∈
2
xxf (x, u)[y, y, λ] + 2

Rnx , we deﬁne

2
xuf (x, u)[y, v, λ]+

∇

2
uuf (x, u)[v, v, λ].

(18)

∇

Proof. In the following, we denote for simplicity φ(u) = f [τ ](¯x0, u). The optimization oracles can be rewritten
as follows.

1. The gradient oracle (13) is given by

v∗ = arg min
v∈Rτ nu

(cid:26)

∇

h(g(u))(cid:62)

∇

g(u)(cid:62)v +

(cid:27)

.

ν
2 (cid:107)

v

2
2
(cid:107)

2. The Gauss-Newton oracle (14) is given by

v∗ = arg min
v∈Rτ nu

(cid:26) 1
2

v(cid:62)

∇

g(u)

∇

2h(g(u))

∇

g(u)(cid:62)v +

∇

h(g(u))(cid:62)

∇

g(u)(cid:62)v +

(cid:27)

.

ν
2 (cid:107)

v

2
2

(cid:107)

3. The Newton oracle (15) is given by

v∗= arg min
v∈Rτ nu

(cid:26) 1
2

v(cid:62)

∇

g(u)

∇

2h(g(u))

∇

g(u)(cid:62)v+

1
2 ∇

2g(u)[v, v,

h(g(u))]+

∇

∇

h(g(u))(cid:62)

g(u)(cid:62)v+

∇

(19)

(20)

(cid:27)

.

2
2

v

ν
2 (cid:107)
(cid:107)
(21)

10

We have, denoting x = φ(u),

v(cid:62)

∇

g(u)

∇

h(g(u))(cid:62)
2h(g(u))
∇
2g(u)[v, v,

∇

g(u)(cid:62)v=
∇
g(u)(cid:62)v=v(cid:62)

∇
h(g(u))]=

φ(u)(cid:62)v +

xh(x, u)(cid:62)
∇
2
xxh(x, u)
φ(u)
∇
∇
2φ(u)[v, v,

xh(x, u)].

∇

uh(x, u)(cid:62)v
uuh(x, u)v+2v(cid:62)
2

∇
φ(u)(cid:62)v+v(cid:62)

∇

φ(u)

∇

∇

2
xuh(x, u)v

∇

∇

For v = (v0; . . . ; vτ −1)

∈

∇

∇
Rτ nu , denoting y =

φ(u)(cid:62)v = (y1; . . . ; yτ ), with y0 = 0, we have then

∇

h(g(u))(cid:62)

∇

∇

g(u)(cid:62)v=

τ −1
(cid:88)

t=0

(cid:2)
∇

xtht(xt, ut)(cid:62)yt+

∇

utht(xt, ut)(cid:62)vt

(cid:3) +

hτ (xτ )(cid:62)yτ =

∇

τ −1
(cid:88)

t=0

(cid:96)xt,ut
ht

(yt, vt)+(cid:96)xτ
hτ

(yτ ).

(22)

(23)

Following the proof of Lemma 3.2, we have that y =

φ(u)(cid:62)v = (y1; . . . ; yτ ) satisﬁes

yt+1 =

∇

xtft(xt, ut)(cid:62)yt +

∇

∇
ut ft(xt, ut)(cid:62)vt = (cid:96)xt,ut

ft

(yt, vt),

for t

0, . . . , τ

∈ {

1

,
}

−

with y0 = 0. Hence, plugging Eq. (22) and Eq. (23) into Eq. (19) we get the claim for the gradient oracle.

The Hessians of the total cost are block diagonal with, e.g.,

of the form

2
utut

∇

ht(xt, ut) for t

0, . . . , τ

∈ {

1

−

. Therefore, we have
}

∇

2
uuh(x, u) being composed of τ diagonal blocks

1
2

v(cid:62)

=

∇
τ −1
(cid:88)

t=0

g(u)

∇

2h(g(u))

g(u)(cid:62)v

∇

(cid:20) 1
2

y(cid:62)
t ∇

2
xtxt

ht(xt, ut)yt+

1
2

v(cid:62)
t ∇

2
utut

ht(xt, ut)vt+y(cid:62)

t ∇

2
xtut

(cid:21)

ht(xt, ut)vt

+

1
2

y(cid:62)
τ ∇

2hτ (xτ )yτ .

The linear quadratic approximation in (20) can then be written as

1
2

v(cid:62)

∇

g(u)

∇

2h(g(u))

∇

g(u)(cid:62)v +

h(g(u))(cid:62)

∇

∇

g(u)(cid:62)v =

τ −1
(cid:88)

t=0

qxt,ut
ht

(yt, vt) + qxτ
hτ

(yτ ).

(24)

Hence, plugging Eq. (24) and Eq. (23) into Eq. (20) we get the claim for the Gauss-Newton oracle.
xh(x, u)=(

For the Newton oracle, denoting µ=

x1h1(x1, u1); . . . ;

xτ −1hτ −1(xτ −1, uτ −1);

∇

∇

∇

hτ (xτ )),

∇

and deﬁning adjoint variables λt as

λτ =

hτ (xτ )

λt =

∇
∇
we have, as in the proof of Lemma 3.2,

xtht(xt, ut) +

∇

xtft(xt, ut)λt+1

for t

1, . . . , τ

∈ {

,
1
}

−

2φ(u)[v, v,

∇

xh(x, u)] =

∇

=

τ −1
(cid:88)
t=0 ∇
τ −1
(cid:16)
(cid:88)

∇

t=0

2φt+1(u)[v, v, µt+1]

2
xtxt

ft(xt, ut)[yt, yt, λt+1] +

2
utut

ft(xt, ut)[vt, vt, λt+1]

∇
(cid:17)

+ 2

2
xtut

∇

ft(xt, ut)[yt, vt, λt+1]

.

(25)

Hence, plugging Eq. (24), Eq. (25) and Eq. (23) into Eq. (21) we get the claim for the Newton oracle.

From an optimization viewpoint, gradient, Gauss-Newton or Newton oracles are considered as black-boxes.
Second order methods such as Gauss-Newton or Newton methods are generally considered to be too computation-
ally expensive for optimizing problems in high dimensions because they a priori require solving a linear system at
a cubic cost in the dimension of the problem. Here, the dimension of the problem in the control variables is τ nu,
with nu, the dimension of the control variables, usually small (see the numerical examples in Sec. 10), but τ , the
number of time steps, potentially large if, e.g., the discretization time step used to deﬁne (1) from a continuous
time control problem is small while the original time length of the continuous time control problem is large. A
cubic cost w.r.t. the number of time steps τ is then a priori prohibitive.

11

A closer look at the implementation of all the above oracles (13), (14), (15), shows that they all amount to
solving linear quadratic control problems as presented in Lemma 4.1. Hence they can be solved by a dynamic pro-
gramming approach detailed in Sec. 4.2 at a cost linear w.r.t. the number of time steps τ . As a consequence, if the
dimensions nu, nx of the control and state variables are negligible compared to the horizon τ , the computational
complexities of Gauss-Newton and Newton oracles, detailed in Sec. 7 are of the same order as the computational
complexity of a gradient oracle. This observation was done by Pantoja (1988); Dunn and Bertsekas (1989) for
a Newton step and Sideris and Bobrow (2005) for a Gauss-Newton step. Wright (1990) also presented how se-
quential quadratic programming methods can naturally be cast in a similar way. Lemma 4.1 casts all classical
optimization oracles in the same formulation, including a gradient oracle.

4.2

Implementation

Given Lemma 4.1, classical optimization oracles for objectives of the form

(u) = h

◦

J

g(u), where h(x, u) =

τ −1
(cid:88)

t=0

ht(xt, ut) + hτ (xτ ),

g(u) = (f [τ ](¯x0, u), u),

with f [τ ](¯x0, u) the control of τ dynamics (ft)τ −1
t=0 deﬁned in Def. 3.1, can be implemented by (i) instantiating
the linear quadratic control problem (16) with the chosen approximations, (ii) solving the linear quadratic control
problem (16) by dynamic programming as detailed in Sec. 2. Precisely, their implementation can be split into the
following three phases.

1. Forward pass: All oracles start by gathering the information necessary for the step in a forward pass that

takes the generic form of Algo. 5 and can be summarized as

(u), (mxt,ut

ft

t=0 , (mxt,ut
)τ −1

ht

t=0 , mxτ
)τ −1
hτ

= Forward(u, (ft)τ −1

t=0 , (ht)τ

t=0, ¯x0, of , oh)

J
that compute the objective
J
(mxt,ut
t=0 , mxτ
)τ −1
hτ
ft

t=0 , (mxt,ut
)τ −1

ht

(u) associated to the given sequence of controls u and record approximations
of the dynamics and the costs up to the orders of and oh, respectively.

2. Backward pass: Once approximations of the dynamics have been computed, a backward pass on the cor-
responding linear quadratic control problem (16) can be done as in the linear quadratic case presented in
Sec. 2. The backward passes of the gradient oracle in Algo. 6, the Gauss-Newton oracle in Algo. 7 and the
Newton oracle in Algo. 8 take generally the form

(πt)τ −1

t=0 , c0 = Backward((mxt,ut

ft

t=0 , (mxt,ut
)τ −1

ht

t=0 , mxτ
)τ −1
hτ

, ν).

Namely, they take as input a regularization ν
(mxt,ut
t=0 , mxτ
)τ −1
hτ
ft
to-go corresponding to the subproblem (16).

t=0 , (mxt,ut
)τ −1

ht

0 and some approximations of the dynamics and the costs
computed in a forward pass, and return a set of policies and the ﬁnal cost-

≥

3. Roll-out pass: Given the output of a backward pass deﬁned above, the oracle is computed by rolling out the
policies along the linear trajectories deﬁned in the subproblem (16). Formally, given a sequence of policies
(πt)τ −1

t=0 , the oracles are then given as v = (v0; . . . ; vτ −1) computed, for y0 = 0, by Algo. 11 as

v = Roll(y0, (πt)τ −1

t=0 , ((cid:96)xt,ut

ft

)τ −1
t=0 ),

with (πt)τ −1
t=0 output by one of the backward passes in Algo. 6, Algo. 7 or Algo. 8. For the Gauss-Newton
and Newton oracles, an additional procedure checks whether the subproblems are convex at each iteration
as explained in more detail in Sec. 9.

Gradient, Gauss-Newton, and Newton oracles are implemented by, respectively, Algo. 12, Algo. 13, Algo. 14.
Additional line-searches are presented in Sec. 6. The computational scheme of a gradient oracle, i.e., gradient
back-propagation, is illustrated in Fig. 3. The computational scheme of the Gauss-Newton oracle is presented in
Fig. 4. Finally, the computational scheme of a Newton oracle is illustrated in Fig. 6.

12

Gradient back-propagation. For a gradient oracle (13), the procedure LQBP normally used to solve linear
quadratic control problems simpliﬁes to the procedure LBP presented in Algo. 3 that implements
(cid:111)





(cid:110)



→

πt : x

ct : x

minu∈Rnu

→
arg minu∈Rnu

→

t (x, u) + ct+1((cid:96)f
(cid:96)h
(cid:110)
t (x, u) + ct+1((cid:96)f
(cid:96)h

t (x, u)) + ν
2
u
2
2 (cid:107)
(cid:107)
t (x, u)) + ν
u
2 (cid:107)
(cid:107)

(cid:111)

 ,

2
2

(26)

LBP : ((cid:96)f

t , (cid:96)h

t , ct+1, ν)

for linear functions (cid:96)f
t , ct+1. Plugging into the overall dynamic programming procedure, Algo. 3, the lineariza-
tions of the dynamics and the costs, we get that the gradient oracle, Algo. 6, computes afﬁne cost-to-go functions
of the form ct(yt) = j(cid:62)

t , (cid:96)h

t yt + j0

t with

jτ =

∇

hτ (xτ ),

jt =

∇

xtht(xt, ut) +

∇

xtft(xt, ut)jt+1

for t

0, . . . , τ

∈ {

.

1
}

−

Moreover, the policies are independent of the state variables, i.e., πt(yt) = kt, with

kt =

ν−1(

∇

−

utht(xt, ut) +

∇

utft(xt, ut)jt+1) =

ν−1

−

ut(h

∇

◦

g)(u).

The roll-out of these policies is independent of the dynamics and output directly the gradient up to a factor
Note that we naturally retrieve the gradient back-propagation algorithm (Griewank and Walther, 2008).

ν−1.

−

Simpliﬁcations. Some simpliﬁcations can be done in the implementations of the oracles. The gradient oracle
can directly return the values of the gradient without the need for a roll-out phase. For the Gauss-Newton oracle,
if there is no intermediate cost (ht = 0 for t
), the oracle can be computed by solving the dual
}
subproblem by making calls to an automatic differentiation procedure as done by, e.g., Roulet et al. (2019). For
the Newton oracle, the quadratic approximations of the dynamics do not need to be stored and can simply be
computed in the backward pass by computing the second derivative of f (cid:62)

t λt+1 on xt, ut as explained in Sec. 7.

0, . . . , τ

∈ {

−

1

5 Differential Dynamic Programming Oracles

The original differential dynamic programming algorithm was developed by Jacobson and Mayne (1970) and
revisited by, e.g., Mayne and Polak (1975); Murray and Yakowitz (1984); Liao and Shoemaker (1992); Tassa
et al. (2014). The reader can verify from the aforementioned citations that our presentation matches the original
formulation in, e.g., the quadratic case, while offering a larger perspective on the method that incorporates, e.g.,
linear quadratic approximations.

5.1 Derivation

5.1.1 Rationale

Denoting h the total cost as in (8) and f [τ ] the control in τ dynamics (ft)τ −1
(DDP) oracles consist in solving approximately

t=0 , Differential Dynamic Programming

min
v∈Rτ nu

h(f [τ ](¯x0, u + v), u + v),

by means of a dynamic programming procedure and using the resulting policies to update the current sequence of
controllers. For a consistent presentation with the classical optimization oracles presented in Sec. 4, we consider
a regularized formulation of the DDP oracles, that is,

min
v∈Rτ nu

h(f [τ ](¯x0, u + v), u + v) +

ν
2 (cid:107)

v

2
2,
(cid:107)

for some regularization ν

0.
The objective in problem (27) can be rewritten as

≥

h(f [τ ](¯x0, u + v), u + v) = h(f [τ ](¯x0, u)) + δf [τ ](¯x0,u)

h

(δ ¯x0,u

f [τ ] (0, v), v),

(27)

(28)

where for a function f , δx
In particular, δ ¯x0,u

f is the ﬁnite difference expression of f around x as deﬁned in the notations in Eq. (2).

f [τ ] (0, v) is the trajectory deﬁned by the ﬁnite differences of the dynamics given as

δxt,ut
ft

(yt, vt) = ft(xt + yt, ut + vt)

ft(xt, ut).

−

13

The dynamic programming approach is then applied on the above dynamics. Namely, the goal is to solve

τ −1
(cid:88)

δxt,ut
ht

min
v0,...,vτ −1∈Rnu
t=0
y0,...,yτ ∈Rnx
subject to yt+1 = δxt,ut

ft

(yt, vt) +

ν
2 (cid:107)

vt

2 + δxτ
2
hτ
(cid:107)

(yτ )

(yt, vt)

for t

0, . . . , τ

∈ {

,
1
}

−

y0 = 0,

by dynamic programming. Denote then c∗
These cost-to-go functions satisfy the recursive equation

t the cost-to-go functions associated to problem (29) for t

c∗
t (yt) = min
vt∈Rnu

(cid:110)

δxt,ut
ht

(yt, vt) +

ν
2 (cid:107)

vt

2 + c∗
2
(cid:107)

t+1(δxt,ut
ft

(yt, vt))

(cid:111)

,

(29)

0, . . . τ

∈ {

.
}

(30)

and such that our objective is to compute c∗

τ = δxτ
starting from c∗
are not linear
hτ
and the costs δxt,ut
are not quadratic, there is no analytical solution for the subproblem (30). To circumvent this
ht
issue, the cost-to-go functions are approximated as c∗
ct(yt), where ct is computed from approximations
t (yt)
of the dynamics and the costs. The approximation is done around the nominal value of the subproblem (29) which
is v = 0 and corresponds to y = 0 and no change of the original objective in (28).
Denoting mf an expansion of a function f around the origin such that f (x)

0(0). Since the dynamics δxt,ut

f (0) + mf (x), the cost-to-go

≈

ft

≈

functions are computed with a procedure

(cid:99)BP : δf

t , δh

t , ct+1





→

(cid:110)

ct : y

→

minv∈Rnu

πt : y

(δh

t +ct+1

δf
t )(0, 0) + mδh
◦
(cid:110)
arg minv∈Rnu

mδh

t

(y, v) + mct+1◦δf
(y, v) + ν

t

(y, v) + mct+1◦δf

t

t

(y, v) + ν

(cid:111)

,

v

2
2

(cid:107)



 ,

2 (cid:107)

(cid:111)

v
2 (cid:107)

2
2
(cid:107)

→
t and δxt,ut
δf

(31)
δh
t . A DDP oracle computes then a sequence of policies

applied to the ﬁnite differences δxt,ut
ht →
by iterating in a backward pass, starting from cτ = mδxτ
,
hτ

ft →

ct, πt = (cid:99)BP(δxt,ut

ft

, δxt,ut
ht

, ct+1)

for t

τ
∈ {

.
1, . . . , 0
}

−

(32)

Given a set of policies, an approximate solution is given by rolling out the policies along the dynamics deﬁning
problem (29), i.e., by computing v0, . . . , vτ −1 as

vt = πt(yt),

yt+1 = δxt,ut

ft

(yt, vt) = ft(xt + yt, ut + vt)

ft(xt, ut)

for t = 0, . . . , τ

1.

−

(33)

−

The main difference with the classical optimization oracles relies a priori in the computation of the policies in (32)
detailed below and in the roll-out pass that uses the ﬁnite differences of the dynamics. Note that, while only the
non-constant parts of the cost-to-go functions are useful to compute the policies, the overall procedure computes
also the constant part of the cost-to-go functions. The latter is used for line-searches as detailed in Sec. 6.

5.1.2 Detailed Derivations of the Backward Passes

Linear Approximation.
and the dynamics, we have

If we consider a linear approximation for the composition of the cost-to-go function

mct+1◦δx,u
ft

= (cid:96)ct+1◦δx,u

f

δx,u
f
ct+1

= (cid:96)

(0,0)

(cid:96)δx,u

f

= (cid:96)ct+1 ◦

◦

(cid:96)x,u
f

,

where we denote simply (cid:96)f = (cid:96)0

f the linear expansion of a function f around the origin.
Plugging this model into (31) and using linear approximations of the costs, the recursion (32) amounts to

computing, starting from cτ = (cid:96)δxτ ,uτ

hτ

= (cid:96)xτ ,uτ
hτ

,

ct(y) = min
v∈Rnu

δxt,ut
ht

(0, 0) + (cid:96)δxt,ut

ht

(y, v) + ct+1(δxt,ut

(0, 0)) + (cid:96)ct+1((cid:96)xt,ut

ft

(y, v)) +

ν
v
2 (cid:107)

(cid:107)

2
2,

= min
v∈Rnu

(cid:96)xt,ut
ht

(y, v) + ct+1((cid:96)xt,ut

ft

(y, v)) +

ft
ν
2 (cid:107)

2
2,

v

(cid:107)

where in the last line we used that the cost-to-go functions ct are necessarily afﬁne, s.t. ct+1(y) = ct+1(0) +
(cid:96)ct+1(y). We retrieve then the same recursion as the one used for a gradient oracle and the output policies are then
the same. Since the computed policies are constant, they are not affected by the dynamics along which a roll-out
phase is performed. In other words, the oracle returned by using linear approximations in a DDP approach is just
a gradient oracle.

14

Linear Quadratic Approximation.
cost-to-go function and the dynamics, we have

If we consider a linear quadratic approximation for the composition of the

mct+1◦δf x,u = q

δx,u
f
ct+1

(0,0)

(cid:96)δx,u

f

= qct+1 ◦

◦

(cid:96)x,u
f

,

where we denote simply qf = q0
f the quadratic expansion of a function f around the origin. Plugging this model
into (31) and using quadratic approximations of the costs, the recursion (32) amounts to computing, starting from
cτ = qδxτ ,uτ

,

= qxτ ,uτ
hτ

hτ

ct(y) = min
v∈Rnu

δxt,ut
ht

(0, 0) + qδxt,ut

ht

(y, v) + ct+1(δxt,ut

ft

(0, 0)) + q

δx,u
f
ct+1

(0,0)

= min
v∈Rnu

qxt,ut
ht

(y, v) + ct+1(0) + qct+1((cid:96)xt,ut

ft

(y, v)) +

ν
2 (cid:107)

v

2
2.
(cid:107)

(cid:96)(0,0)
δx,u
f

◦

(y, v) +

ν
2 (cid:107)

v

2
2
(cid:107)

(34)

If the costs ht are convex for all t and qxt,ut
2
2 is strongly convex for all t and all y, then the cost-to-go
functions ct are convex quadratics for all t, i.e., ct+1(y) = ct+1(0) + qct+1(y). In that case, the recursion (34)
simpliﬁes as

) + ν
·

2 (cid:107) · (cid:107)

(y,

ht

ct(y) = min
v∈Rnu

qxt,ut
ht

(y, v) + ct+1((cid:96)xt,ut

ft

(y, v)) +

ν
v
2 (cid:107)

2
2,
(cid:107)

(35)

and the policies are given by the minimizer of Eq. (35). The recursion (35) is then the same as the recursion done
when computing a Gauss-Newton oracle. Namely, the backward pass in this case is the backward pass of a Gauss-
Newton oracle. Though the output policies are the same, the output of the oracle will differ since the roll-out phase
does not follow the linearized trajectories in the DDP approach. The computational scheme of a DDP approach
with linear quadratic approximations presented in Fig. 5 is then almost the same as the one of a Gauss-Newton
oracle presented in Fig. 4, except that in the roll-out phase the linear approximations of the dynamics are replaced
by ﬁnite differences of the dynamics.

Quadratic Approximation.
function and the dynamics, we get

If we consider a quadratic approximation for the composition of the cost-to-go

mct+1◦δx,u

= qct+1◦δx,u

=

1
2 ∇

2f (x, u)[
·

,

,
·

ct+1(0)] + qct+1 ◦

(cid:96)x,u
f

,

f

∇
, λ] is deﬁned in (18). Plugging this model into (31) and using quadratic approximations of

f

where
the costs, the recursion (32) amounts to, starting from cτ = qδxτ ,uτ

2f (x, u)[
,
·

∇

·

= qxτ ,uτ
hτ

,

hτ

ct(y) = min
v∈Rnu

δxt,ut
ht

(0, 0) + qδxt,ut

ht

(y, v) + ct+1(δxt,ut

ft

(0, 0)) + qct+1◦δx,u
ft

(y, v) +

ν
v
2 (cid:107)

2
2
(cid:107)

(36)

qxt,ut
ht

= min
v∈Rnu

(y, v) + ct+1(0) + qct+1 ◦
Provided that the costs are convex and that qxt,ut
) + 1
2
2 is strongly
·
convex for all t and all y, the cost-to-go functions ct are convex quadratics for all t. In that case, the recursion (36)
simpliﬁes as

ct+1(0)](y, v) +

ct+1(0)](y,

(y, v) +

2 (cid:107) · (cid:107)

) + ν

2ft(xt, ut)[
·

2
2.
(cid:107)

1
2 ∇
2ft(xt, ut)[
,
·

ν
v
2 (cid:107)

(cid:96)xt,ut
ft

2 ∇

(y,

,
·

,
·

∇

∇

ht

·

,

ct(y) = min
v∈Rnu

qxt,ut
ht

(y, v) + ct+1((cid:96)xt,ut

ft

(y, v)) +

1
2 ∇

2ft(xt, ut)[
·

,

,

·

∇

ct+1(0)](y, v) +

ν
v
2 (cid:107)

2
2,
(cid:107)

(37)

and the policies are given by the minimizer of Eq. (37). The overall backward pass is detailed in Algo. 9.

Compared to the backward pass of the Newton oracle in Algo. 8, we note that the additional cost derived
from the curvatures of the dynamics is not computed the same way. Namely, the Newton oracle computes this
additional cost by using back-propagated adjoint variables in Eq. (17), while in the DDP approach the additional
cost is directly deﬁned through the previously computed cost-to-go function. Fig. 7 illustrates the computational
scheme of the implementation of DDP with quadratic approximations and can be compared to the computational
scheme of the Newton oracle in Fig. 6.

Note that, while we used second order Taylor expansions for the compositions and the costs, the approximate
cost-to-go-functions ct are not second order Taylor expansion of the true cost-to-go functions c∗
t , except for cτ .
Indeed, ct is computed as an approximate solution of the Bellman equation. The true Taylor expansion of the
cost-to-go function requires the gradient and the Hessian of the cost and the dynamic in Eq. (36) computed at the
minimizer of the subproblem. Here, since we only use an approximation of the minimizer, we do not have access
to the true gradient and Hessian of the cost-to-go function.

15

5.2

Implementation

The implementation of the DDP oracles follows the same steps as the ones given for classical optimization oracles
as detailed below. The implementation of a DDP oracle with linear quadratic approximations is given in Algo. 15
and illustrated in Fig. 5. The implementation of a DDP oracle with quadratic approximations is given in Algo. 16
and illustrated in Fig. 7.

1. Forward pass: As for the classical optimization methods, the forward pass is provided in Algo. 5 which
gathers the information necessary for the backward pass. Namely, the oracle starts with Algo. 5 and com-
putes

(u), (mxt,ut

ft

t=0 , (mxt,ut
)τ −1

ht

t=0 , mxτ
)τ −1
hτ

= Forward(u, (ft)τ −1

t=0 , (ht)τ

t=0, ¯x0, of , oh),

J

where of and oh deﬁne the order of approximations used for the dynamics and the costs respectively.

2. Backward pass: As for the classical optimization oracles, the backward pass can generally be written

(πt)τ −1

t=0 , c0 = Backward((mxt,ut

ft

t=0 , (mxt,ut
)τ −1

ht

t=0 , mxτ
)τ −1
hτ

, ν),

If linear approximations are used, the backward pass is given in Algo. 6, if linear quadratic approximations
are used, the backward pass is given in Algo. 7 and if quadratic approximations are used, the backward pass
is given in Algo. 9.

3. Roll-out pass: The roll-out phase differs by using ﬁnite differences of the original dynamics of problem (29)
t=0 , the oracles are then

rather than the linearized dynamics. Formally, given a sequence of policies (πt)τ −1
given as v = (v0; . . . ; vτ −1) computed, for y0 = 0, by Algo. 11 as

v = Roll(y0, (πt)τ −1

t=0 , (δxt,ut

ft

)τ −1
t=0 ),

where δxt,ut

ft

(yt, vt) = ft(xt + yt, ut + vt)

ft(xt, ut).

−

6 Line-searches

So far, we deﬁned procedures that, given a command and some regularization parameter, output a direction that
minimizes an approximation of the objective or approximately minimizes a shifted objective. Given access to such
procedures, the next command can be computed in several ways. The main criterion is to ensure that the value of
the objective decreases along the iterations, which is generally done by a line-search.

In the following, we only consider oracles based on linear quadratic or quadratic approximations of the ob-
jective such as Gauss-Newton and Newton, and refer the reader to Nocedal and Wright (2006) for classical line-
searches for gradient descent.

6.1 Rules

We start by considering the implementation of line-searches for classical optimization oracles which can again ex-
ploit the dynamical structure of the problem and are mimicked by differential dynamic programming approaches.
Rτ nu
We consider, as in Sec. 4, that we have access to an oracle for an objective
and any regularization ν

, that, given a command u

0, outputs

J

∈

≥

Oracleν(

)(u) = arg min
v∈Rτ nu

J

mu

J (v) +

ν
2 (cid:107)

v

2
2,
(cid:107)

(38)

J is a linear quadratic or quadratic expansion of the objective

(u) +
J (v). Given such an oracle, we can deﬁne a new candidate command that decreases the value of the objective

around u s.t.

(u + v)

≈ J

J

J

where mu
mu
in several ways.

16

6.1.1 Directional Steps

The next iterate can be deﬁned along the direction provided by the oracle, as long as this direction is a descent
direction. Namely, the next iterate can be computed as

unext = u + γv, with v = Oracleν(

)(u) for ν

0 s.t.

≥

∇J

J

(u)(cid:62)v < 0,

where the stepsize γ is chosen to satisfy, e.g., an Armijo condition, that is,

(u + γv)

(u) +

≤ J

J

γ
2 ∇J

(u)(cid:62)v.

(39)

(40)

In this case, the search is usually initialized at each step with γ = 1. If condition (40) is not satisﬁed for γ = 1, the
stepsize is decreased by a factor ρdec < 1 until condition (40) is satisﬁed. If a stepsize γ = 1 is accepted, then the
linear quadratic or quadratic algorithms may exhibit a quadratic local convergence (Nocedal and Wright, 2006).
Alternative line-search criterions such as Wolfe’s condition or trust-region methods can also be implemented (No-
cedal and Wright, 2006).

6.1.2 Regularized Steps

Given a current iterate u
by the oracle decreases the objective. Namely, the next command can be computed as

Rτ nu , we can ﬁnd a regularization such that the current iterate plus the direction output

∈

unext = u + vγ, where vγ = Oracle1/γ(

)(u) = arg min
v∈Rτ nu

J

mu

J (v) +

1
2γ (cid:107)

v

2
2,
(cid:107)

(41)

where the parameter γ > 0 acts as a stepsize that controls how large should be the step (the smaller the parameter
γ, the smaller the step vγ). The stepsize γ can then be chosen to satisfy

(u + vγ)

J

≤ J

(u) + mu

J (vγ) +

1
2γ (cid:107)

vγ

2
2,

(cid:107)

(42)

which ensures a sufﬁcient decrease of the objective to, e.g., prove convergence to stationary points (Roulet et al.,
2019). In practice, as for the line-search on the descent direction, given an initial stepsize for the iteration, the
stepsize is either selected or reduced by a factor ρdec until condition (42) is satisﬁed. However, here, we initialize
the stepsize at each iteration as ρincγprev where γprev is the stepsize selected at the previous iteration and ρinc > 1
is an increasing factor. By trying a larger stepsize at each iteration, we may beneﬁt from larger steps in some
regions of the optimization path. Note that such an approach is akin to trust region methods which increase the
radius of the trust region at each iteration depending on the success of each iteration (Nocedal and Wright, 2006).
In practice, we observed that, when using regularized steps, acceptable stepsizes for condition (42) tend to be
arbitrarily large as the iterations increase. Namely, we tried choosing ρinc = 10 and observed that the acceptable
stepsizes tended to plus inﬁnity with such a procedure. To better capture this tendency, we consider regularizations
that may depend on the current state and of the form ν(u)
2, i.e., stepsizes of the form γ(u) =
(cid:107)
∝ (cid:107)∇
¯γ/
2. The line-search is then performed on ¯γ only. Intuitively, as we are getting closer to a stationary
point, quadratic models are getting more accurate to describe the objective. By scaling the regularization with
2, which is a measure of stationarity, we may better capture such behavior. Note that for
respect to
(cid:107)
ν = 0, we retrieve the iteration with a descent direction of stepsize γ = 1 described above.

h(x, u)
(cid:107)

h(x, u)

h(x, u)

(cid:107)∇

(cid:107)∇

6.2

Implementation

6.2.1 Directional Steps

The Armijo condition (40) can be computed directly from the knowledge of a gradient oracle and the chosen
oracle (such as Gauss-Newton or Newton). We present here the implementation of the line-search in terms of the
dynamical structure of the problem. Denote

(πt)τ −1

t=0 , c0 = Backward((mxt,ut

ft

t=0 , (mxt,ut
)τ −1

ht

t=0 , mxτ
)τ −1
hτ

, ν)

the policies and the value of the cost-to-go function output by the backward pass of the considered oracle, i.e.,
Gauss-Newton or Newton.

17

By deﬁnition, c0(0) is the minimum of the corresponding linear quadratic control problem (16). Moreover, the
linear quadratic control problem can be summarized as a quadratic problem of the form minv mJ (v) + ν
2
2 =
(cid:107)
(u)(cid:62)v with Q a quadratic that is either the Hessian of
minv
for a Newton oracle or an
approximation of it for a Gauss-Newton oracle. Therefore, we have that, for a Newton or a Gauss-Newton oracle
v = Oracle1/γ(

2 v(cid:62)(Q + ν I)v +

v
2 (cid:107)

∇J

J

),

1

J

1
2 ∇J

(u)(cid:62)v =

1
2 ∇J

−

(u)(cid:62)(Q + ν I)−1

(u) = min

v∈Rτ nu

mJ (v) +

∇J

ν
2 (cid:107)

v

2
2 = c0(0).
(cid:107)

Therefore the right-hand part of condition (40) can be given by the value of the cost-to-go function c0(0). On
the other hand, sequences of controllers of the form γv can be deﬁned by modifying the policies output in the
backward pass as shown in the following lemma adapted from Liao and Shoemaker (1992, Theorem 1).

Lemma 6.1. Given a sequence of afﬁne policies (πt)τ −1
denote v∗ = Roll(y0, (πt)τ −1

t=0 ) and πγ

t=0 , ((cid:96)t)τ −1

t : y

t=0 , linear dynamics ((cid:96)t)τ −1
γπt(0) +

πt(0)(cid:62)y for t = 0, . . . , τ

t=0 and an initial state y0 = 0,

1. We have that

−

→

∇

γv∗ = vγ, where vγ = Roll(y0, (πγ

t )τ −1

t=0 , ((cid:96)t)τ −1

t=0 ).

t )τ −1

t=0 as yγ
Proof. Deﬁne (yγ
w.r.t. γ. Proceeding by induction, we have that yγ
t (yγ
linear. Therefore vγ

t+1 = (cid:96)t(yγ

t , πt(yγ

t = πγ

t ) is linear w.r.t. γ which gives the claim.

t )) for t

0, . . . , τ

1
}
−
t is linear w.r.t. γ using the form of πγ

0 = 0. We have that yγ

1 is linear
t and the fact that (cid:96)t is

with yγ

∈ {

Therefore, computing the next sequence of controllers by moving along a descent direction as in (39) according

to an Armijo condition (40) amounts to computing, with Algo. 17,

where Pol : γ

t=0, (ft)τ −1
unext = LineSearch(u, (ht)τ
(cid:18) (πγ
γπt(0) +
y
t :
cγ
γc0(y)
y
0 :
t=0 , c0 = Backward((mxt,ut

(πt)τ −1

→
→

→

ft

t=0 , ((cid:96)xt,ut

)τ −1
t=0 , Pol),
ft
(cid:19)
πt(0)(cid:62)y)τ −1
t=0

∇

t=0 , (mxt,ut
)τ −1

ht

t=0 , mxτ
)τ −1
hτ

, ν), for ν

≥

0 s.t. c0(0) < 0,

where Backward

BackwardGN, BackwardNE

is given in Algo. 7 or Algo. 8.

∈ {

}

In practice, in our implementation of the backward passes in Algo. 7, Algo. 8, the returned initial cost-to-go
function is either negative if the step is well deﬁned or inﬁnite if it is not. To ﬁnd a regularization that ensures
a descent direction, i.e., c0(0) < 0, it sufﬁces thus to ﬁnd a feasible step. In our implementation, we ﬁrst try to
compute a descent direction without regularization (ν = 0), then try a small regularization ν = 10−6, which we
increase by 10 until a ﬁnite negative cost-to-go function c0(0) is returned. See Algo. 18 for an instance of such
implementation.

From the above discussion, it is clear that one iteration of the Iterative Linear Quadratic Regulator algorithm
described in Sec. 2.3 uses a Gauss-Newton oracle without regularization to move along the direction of the oracle
by using an Armijo condition. The overall iteration is given in Algo. 18, where we added a procedure to ensure that
the output direction is a descent direction. All other algorithms, with or without regularization can be written in a
similar way using a forward, a backward pass, and multiple roll-out phases until the next sequence of controllers
is found.

6.2.2 Regularized Steps

For regularized steps, the line-search (42) requires computing mu
2
2. This is by deﬁnition the mini-
(cid:107)
mum of the sub-problem that is computed by dynamic programming. This minimum can therefore be accessed as
mu
2
2 = c0(0) for c0 output by the backward pass with a regularization ν = 1/γ. Overall, the next
(cid:107)
sequence of controls is then provided through the line-search procedure given in Algo. 17 as

J (vγ) + 1

J (vγ) + 1

2γ (cid:107)

2γ (cid:107)

vγ

vγ

unext = LineSearch(u, (ht)τ

Backward((mxt,ut

t=0 , ((cid:96)xt,ut
t=0, (ft)τ −1
ft
t=0 , (mxt,ut
)τ −1

)τ −1
t=0 , Pol),
t=0 , mxτ
)τ −1
hτ

ht

ft

, 1/γ),

where Pol : γ

→

where Backward

BackwardGN, BackwardNE

∈ {

}

is given in Algo. 7 or Algo. 8.

18

6.2.3 Line-searches for Differential Dynamic Programming Approaches

The line-search for DDP approaches as presented by, e.g., Liao and Shoemaker (1992, Sec. 2.2) based on Jacobson
and Mayne (1970), mimics the one done for the classical optimization oracles except that the policies are rolled
out on the original dynamics. Namely, the usual line-search consists in applying Algo. 17 as follows

where Pol : γ

t=0, (ft)τ −1
unext = LineSearch(u, (ht)τ
(cid:18) (πγ
γπt(0) +
y
t :
cγ
γc0(y)
y
0 :
t=0 , c0 = Backward((mxt,ut

(πt)τ −1

→
→

→

ft

t=0 , (δxt,ut

)τ −1
t=0 , Pol)
ft
πt(0)(cid:62)y)τ −1
t=0 ,

(cid:19)

∇

t=0 , (mxt,ut
)τ −1

ht

t=0 , mxτ
)τ −1
hτ

, ν) for ν

≥

0 s.t. c0(0) < 0,

where Backward
is given by Algo. 7 or Algo. 9. As for the classical opti-
mization oracles, a direction is ﬁrst computed without regularization and if the resulting direction is not a descent
direction a small regularization is added to ensure that c0(0) < 0.

BackwardGN, BackwardDDP

∈ {

}

We also consider line-searches based on selecting an appropriate regularization. Namely, we consider line-

searches of the form

unext = LineSearch(u, (ht)τ

Backward((mxt,ut

t=0 , (δxt,ut
t=0, (ft)τ −1
ft
t=0 , (mxt,ut
)τ −1

)τ −1
t=0 , Pol),
t=0 , mxτ
)τ −1
hτ

ht

ft

, 1/γ),

where Pol : γ

→

where Backward

BackwardGN, BackwardDDP

∈ {

}

is given by Algo. 7 or Algo. 9.

7 Computational Complexities

7.1 Formal Computational Complexities

We present in Table 1 the computational complexities of the algorithms following the implementations described
in Sec. 4 and Sec. 5 and detailed in Sec. 9. We ignore the additional cost of the line-searches which requires a
theoretical analysis of the admissible stepsizes depending on the smoothness properties of the dynamics and the
Rn is of the order of O(nd),
costs. We consider for simplicity that the cost of evaluating a function f : Rd
as it is the case if f is linear. For the computational complexities of the core operation of the backward pass, i.e,
LQBP in Algo. 2 or LBP in Algo. 3, we simply give the leading computational complexities, which, in the case
of LQBP, are the matrix multiplications and inversions.

→

The time complexities differ depending on whether linear or quadratic approximations of the costs are used.
nx need to be multiplied.

nu need to be inverted and matrices of size nx

In the latter case, matrices of size nu
However, all oracles have a linear time complexity with respect to the horizon τ .

×

×

We note that the space complexities of the gradient descent and the Gauss-Newton method or the DDP ap-
proach with linear quadratic approximations are essentially the same. On the other hand, the space complexity of
the Newton oracle is a priori larger.

7.2 Computational Complexities in a Differentiable Programming Framework

The decomposition of each oracle between forward, backward and roll-out passes has the advantage to clarify
the discrepancies between each approach. However, storing the linear or quadratic approximations of the costs
or the dynamics may come at a prohibitive cost in terms of memory. A careful implementation of these oracles
only requires storing in memory the function and the inputs given at each time-step. Namely, the forward pass
0, . . . , τ
can simply keep in memory ht, ft, xt, ut for t
. The backward pass computes then, on the ﬂy, the
}
information necessary to compute the policies.

∈ {

The previous time complexities of the forward pass, corresponding to the computations of the gradients of the
dynamics or the costs and Hessians of the costs, are then incurred during the backward pass. A major difference
lies in the computation of the quadratic information of the dynamic required in quadratic oracles such as a Newton
oracle or a DDP oracle with quadratic approximations. Indeed, a closer look at Algo. 8 and Algo. 9 show that
f (x, u)(cid:62)λ need to be computed, which comes at a cost
only the Hessians of scalar functions of the form x, u
→
(nx + nu)2. In comparison, the cost of computing the second order information of f is O((nx + nu)2nx). As an
example, Algo. 10 presents an implementation of a Newton step using stored functions and inputs.

19

Time complexities of the forward pass in Algo. 5

Function eval.
(of = oh = 0)

Linearization
(of = oh = 1)

Lin.-quad.
(of = 1, oh = 2)

Quad.
(of = oh = 2)

(cid:16)

(cid:16)

(cid:16)

(cid:16)

τ

τ

τ

τ

nx
(cid:124)

nx
(cid:124)

nx
(cid:124)

nx
(cid:124)

2+nxnu
(cid:123)(cid:122)
(cid:125)
ft
2+nxnu
(cid:123)(cid:122)
(cid:125)
ft,∇ft
2+nxnu
(cid:123)(cid:122)
(cid:125)
ft,∇ft
2+nxnu
(cid:123)(cid:122)
(cid:125)
ft,∇ft

+ nx+nu
(cid:124) (cid:123)(cid:122) (cid:125)
ht

+ nx+nu
(cid:124) (cid:123)(cid:122) (cid:125)
ht,∇ht

+ nx+nu
(cid:124) (cid:123)(cid:122) (cid:125)
ht,∇ht

2+nu

+ (nx
(cid:124)

(cid:17)

=O(τ (nx

2+nxnu))

(cid:17)

=O(τ (nx

2+nxnu))

+ nx
(cid:124)

2+nu
2+nxnu
(cid:123)(cid:122)
(cid:125)
∇2ht
2+nxnu)nx
(cid:123)(cid:122)
(cid:125)
∇2ft

+ nx+nu
(cid:124) (cid:123)(cid:122) (cid:125)
ht,∇ht

Space complexities of the forward pass in Algo. 5

Function eval.
(of = oh = 0)

Linearization
(of = oh = 1)

Lin.-quad.
(of = 1, oh = 2)

Quad.
(of = oh = 2)

0

τ

τ

τ

(cid:16)

(cid:16)

(cid:16)

nx
(cid:124)

nx
(cid:124)

nx
(cid:124)

2+nxnu
(cid:123)(cid:122)
(cid:125)
∇ft
2+nxnu
(cid:123)(cid:122)
(cid:125)
∇ft
2+nxnu
(cid:123)(cid:122)
(cid:125)
∇ft

+ nx+nu
(cid:124) (cid:123)(cid:122) (cid:125)
∇ht

+ nx+nu
(cid:124) (cid:123)(cid:122) (cid:125)
∇ht

2+nu

+ (nx
(cid:124)

(cid:17)

=O(τ (nx

2+nxnu))

+ nx
(cid:124)

2+nu
2+nxnu
(cid:123)(cid:122)
(cid:125)
∇2ht
2+nxnu)nx
(cid:123)(cid:122)
(cid:125)
∇2ft

+ nx+nu
(cid:124) (cid:123)(cid:122) (cid:125)
∇ht

(cid:17)

=O(τ (nx+nu)2)

+ nx
(cid:124)

2+nu
2+nxnu
(cid:123)(cid:122)
(cid:125)
∇2ht

(cid:17)

=O(τ nx(nx+nu)2)

(cid:17)

=O(τ (nx+nu)2)

+ nx
(cid:124)

2+nxnu
2+nu
(cid:123)(cid:122)
(cid:125)
∇2ht

(cid:17)

=O(τ nx(nx+nu)2)

Time complexities of the backward passes in Algo. 6, 7, 8, 9 and the roll-out in Algo. 11

GD

GN/DDP-LQ

NE/DDP-Q

(cid:16)

(cid:16)

(cid:16)

τ

τ

τ

nx
(cid:124)

nx
(cid:124)

nx
(cid:124)

2+nxnu
(cid:123)(cid:122)
(cid:125)
Roll
2+nxnu
(cid:123)(cid:122)
(cid:125)
Roll
2+nxnu
(cid:123)(cid:122)
(cid:125)
Roll

+ nx
(cid:124)

+ nx
(cid:124)

2+nxnu
(cid:123)(cid:122)
(cid:125)
LBP
3+nu

3+nu
(cid:123)(cid:122)
LQBP

(cid:17)

=O(τ (nx

2+nxnu))

(cid:17)

2nx
(cid:125)

=O(τ (nx+nu)3)

+ nx
(cid:124)

3+nu

3+nu
(cid:123)(cid:122)
LQBP

2nx
(cid:125)

+ (nx
(cid:124)

2+nu

2+nxnu)nx
(cid:123)(cid:122)
(cid:125)
t [·,·,λ]

∇f 2

(cid:17)

=O(τ (nx+nu)3)

Table 1: Space and time complexities of the oracles of Sec. 4 and 5. GD stands for gradient Descent, GN for
Gauss-Newton, NE for Newton, DDP-LQ and DDP-Q stand for DDP with linear quadratic or quadratic approx.

The computational complexities of the oracles when the dynamics and the costs functions are stored in memory
are presented in Table 2. We consider for simplicity that the memory cost of storing the information necessary to
evaluate a function f : Rd

Rn is nd as it is the case for a linear function f .

In summary, by considering an implementation that simply stores in memory the inputs and the programs that
implement the functions, a Newton oracle and an oracle based on a DDP approach with quadratic approximation
have the same time and space complexities as their linear quadratic counterparts up to constant factors. This remark
was done by Nganga and Wensing (2021) for implementing a DDP algorithm with quadratic approximations.

→

20

All cases

Time complexities of the forward pass

(cid:16)

τ

nx
(cid:124)

2+nxnu
(cid:123)(cid:122)
(cid:125)
ft

+ nx+nu
(cid:124) (cid:123)(cid:122) (cid:125)
ht

(cid:17)

=O(τ (nx

2+nxnu))

Space complexities of the forward pass

Function eval.

All other cases

0

τ

(cid:16)

nx
(cid:124)

2+nxnu
(cid:123)(cid:122)
(cid:125)
ft

+ nx+nu
(cid:124) (cid:123)(cid:122) (cid:125)
ht

(cid:17)

=O(τ (nx

2+nxnu))

Time complexities of the backward passes

(cid:17)

=O(τ (nx

2+nxnu))

GD

GN/DDP-LQ

(cid:16)

(cid:16)

τ

τ

NE/DDP-Q

(cid:16)

τ

nx
(cid:124)

nx
(cid:124)
(cid:16)

nx
(cid:124)
(cid:16)

nx
(cid:124)

2+nxnu
(cid:123)(cid:122)
(cid:125)
∇ft
2+nxnu
(cid:123)(cid:122)
(cid:125)
∇ft
2+nxnu
(cid:123)(cid:122)
(cid:125)
Roll
2+nxnu
(cid:123)(cid:122)
(cid:125)
∇ft
2+nxnu
(cid:123)(cid:122)
(cid:125)
Roll

nx
(cid:124)

+ τ

+ τ

2+nxnu
(cid:123)(cid:122)
(cid:125)
LBP
(cid:17)

+ nx+nu
(cid:124) (cid:123)(cid:122) (cid:125)
∇ht
+ nx+nu
(cid:124) (cid:123)(cid:122) (cid:125)
∇ht
+ nx
(cid:124)

+ nx
(cid:124)

+ nx
(cid:124)

+ nx
(cid:124)

2+nxnu
(cid:123)(cid:122)
(cid:125)
Roll
2+nxnu
2+nu
(cid:123)(cid:122)
(cid:125)
∇2ht
2nx
(cid:125)

(cid:17)

3+nu

3+nu
(cid:123)(cid:122)
LQBP

=O(τ (nx+nu)3)

+ nx+nu
(cid:124) (cid:123)(cid:122) (cid:125)
∇ht

+ nx
(cid:124)

+ nx
(cid:124)

3+nu

3+nu
(cid:123)(cid:122)
LQBP

2+nxnu
2+nu
(cid:123)(cid:122)
(cid:125)
∇2ht
2nx
(cid:125)

+ nx
(cid:124)

2+nxnu
2+nu
(cid:123)(cid:122)
(cid:125)
∇2(f (cid:62)
=O(τ (nx+nu)3)

t λ)

(cid:17)

(cid:17)

Table 2: Space and time complexities of the oracles when storing functions as in, e.g., Algo. 10.

21

8 Optimality Conditions

We recall the optimality conditions for nonlinear control problems in continuous and discrete time and their
discrepancies. The problem we consider in continuous time is

min
x∈C1([0,1],Rnx )
u∈C([0,1],Rnu )

(cid:90) 1

0

h(x(t), u(t), t)dt + h(x(1), 1)

(43)

subject to

˙x(t) = f(x(t), u(t), t),

for t

[0, 1] x(0) = ¯x0,

∈
1([0, 1], Rd) denote the set of continuous and continuously differentiable functions from
where
[0, 1] onto Rd respectively and we assume f and h to be continuously differentiable. By using an Euler discretiza-
tion scheme with discretization stepsize ∆ = 1/τ , we get the discrete time control problem

([0, 1], Rd) and

C

C

min
x0,...,xτ ∈Rnx
u0...,uτ −1∈Rnu

τ −1
(cid:88)

t=0

ht(xt, ut) + hτ (xτ )

subject to xt+1 = xt + ft(xt, ut),

for t

0, . . . , τ

∈ {

where xt = x(∆t), ut = u(∆t), ht = ∆h(
have xt + ft(xt, ut) = ft(xt, ut).

,
·

, ∆t), hτ = h(
·
·

, 1), ft = ∆f(
·

,

8.1 Necessary Optimality Conditions

(44)

1

,
}

x0 = ¯x0,

−
, ∆t). Compared to problem (1), we
·

Necessary optimality conditions for the continuous time control problem are known as Pontryagin’s maximum
principle, recalled below. See Arutyunov and Vinter (2004) for a recent proof and Lewis (2006) for a comprehen-
sive overview.

Theorem 8.1 (Pontryagin’s maximum principle (Pontryagin et al., 1963)). Deﬁne the Hamiltonian associated
with problem (43) as

H(x(t), u(t), λ(t), t) = λ(t)(cid:62)f(x(t), u(t), t)

h(x(t), u(t), t).

−

A trajectory x

1([0, 1], Rnx ) such that

∈ C

C

1([0, 1], Rnx ) and a control function u

([0, 1], Rnu ) are optimal if there exists λ

∈ C

∈

˙x(t) =
˙λ(t) =

for all t
∇λ(t)H(x(t), u(t), λ(t), t)
−∇x(t)H(x(t), u(t), λ(t), t) for all t
for all t

H(x(t), u, λ(t), t)

H(x(t), u(t), λ(t), t) = max
u∈Rnu

[0, 1], with x(0) = ¯x0

[0, 1], with λ(1) =
[0, 1].

−∇x(1)h(x(1), 1)

(C1)

(C2)

(C3)

∈

∈

∈

In comparison, necessary optimality conditions for the discretized problem (44) are given by considering the
Karush–Kuhn–Tucker conditions of the problem, or equivalently by considering a sequence of controls such that
the gradient of the objective is null (Bertsekas, 2016).

Fact 8.2. Deﬁne the Hamiltonian associated with problem (44) as

Ht(xt, ut, λt+1) = λ(cid:62)

t+1ft(xt, ut)

ht(xt, ut)

−

Rnx and a sequence of controls u0, . . . , uτ −1

Rnu are optimal if there exists

∈

A trajectory x0, . . . , xτ
λ1, . . . , λτ

∈
Rnx such that

xt+1
λt+1

−

−

∈
xt =
λt =
0 =

∇

λt+1Ht(xt, ut, λt+1)
xtHt(xt, ut, λt+1)

utHt(xt, ut, λt+1)

−∇

∇

for all t
for all t
for all t

0, . . . , τ

1, . . . , τ

0, . . . , τ

∈ {

∈ {

∈ {

1

, with x0 = ¯x0
}
, with λτ =
1
}
.
}

1

−∇

−

−

−

hτ (xτ )

(D1)

(D2)

(D3)

Proof. Necessary optimality conditions are given by considering stationary points of the Lagrangian (Bertsekas,
1976).

22

The Lagrangian of problem (44) is given for λ = (λ1; . . . ; λτ )(cid:62), x = (x1; . . . ; xτ ), u = (u0; . . . ; uτ −1) as,

for x0 = ¯x0 ﬁxed,

L(x, u, λ) =

=

τ −1
(cid:88)

t=0

τ −1
(cid:88)

t=0

ht(xt, ut) +

ht(xt, ut) +

τ −1
(cid:88)

t=0

τ −1
(cid:88)

t=1

λ(cid:62)
t+1(xt+1

xt

−

−

ft(xt, ut)) + hτ (xτ )

(cid:0)x(cid:62)

t (λt

λt+1)

−

−

t+1ft(xt, ut)(cid:1) + hτ (xτ ) + λ(cid:62)
λ(cid:62)

τ xτ

λ(cid:62)
1 (x0 + f0(x0, u0)).

−

We have then for t

0, . . . , τ

∈ {

1
}

−

∇

λt+1L(x, u, λ) = 0
utL(x, u, λ) = 0

⇐⇒

⇐⇒

xt+1
0 =

−

−∇

xt = ft(xt, ut) =
∇
utft(xt, ut)λt+1 +

λt+1Ht(xt, ut, λt+1),

utht(xt, ut) =

−∇

utHt(xt, ut, λt+1),

∇

for t

∇
1, . . . , τ

1
∈ {
}
xtL(x, u, λ) = 0

−

∇

and for t = τ ,

xτ L(x, u, λ) = 0

∇

⇐⇒ ∇

⇐⇒

λt+1

−

λt =

xtft(xt, ut)λt+1 +

−∇
hτ (xτ ) + λτ = 0.

xtht(xt, ut) =

∇

−∇

xtHt(xt, ut, λt+1),

The ﬁrst two necessary optimality conditions (D1) and (D2) for the discretized problem correspond to the
discretizations of the ﬁrst two necessary optimality conditions (C1) and (C2) for the continuous time problem.
The third condition differs since, in discrete time, the control variables only need to be stationary points of the
Hamiltonian. One may wonder whether condition (D3) could be replaced by a stronger necessary optimality
condition of the form

ut

arg max
u∈Rnu

∈

Ht(xt, ut, λt+1).

(D4)

, λt+1) is concave as, e.g., if the costs
If the Hamiltonian is convex w.r.t.
ht(xt,
) are convex and if the dynamics are afﬁne input of the form ft(xt, ut) = at(xt) + Bt(xt)ut, then con-
·
dition (D3) is equivalent to condition (D4). However, generally, condition (D4) is not a necessary optimality
condition for the discrete-time control problem as shown in the following counter-example.

to the control variable, i.e., Ht(xt,

·

Example 8.3. Consider the continuous time control problem

min
x(t),u(t)∈C([0,1],R)

(cid:90) 1

0

(ax(t)2

−

u(t)2)dt + ax(1)2

subject to

˙x(t) = u(t),

x(0) = 0,

for some a > 0 and the associated discrete time control problem, for an Euler scheme with discretization ∆ = 1/τ ,

τ −1
(cid:88)

∆(ax2

min
x0,...,xτ ∈R
u0,...,uτ −1∈R
subject to xt+1 = xt + ∆ut,

t −

t=0

t ) + ax2
u2
τ

x0 = 0.

The Hamiltonians in continuous time, H(x(t), u, λ(t)) = λ(t)(cid:62)u + u2

ax(t)2, and in discrete time,
t , are both strongly convex in u such that neither condition (C3) or (D4)

−

ax2

Ht(xt, ut, λt+1) = ∆λ(cid:62)
can be satisﬁed.

t+1u + u2

−

According to Theorem 8.1, this means that the continuous time control problem has no solution. This can be

veriﬁed by expressing the continuous time control problem uniquely in terms of the trajectory x(t) as

(cid:26)

min
x(t):x(0)=0

C(x) =

(cid:90) 1

0

(ax(t)2

−

˙x(t)2)dt + ax(1)2

(cid:27)

.

By considering functions of the form xk(t) = exp(tk)
below, namely, C(xk)
below and has no minimizer.

2a(exp(1)

k2/(2k

1)2

−

≤

−

−

1, we observe that the corresponding costs are unbounded
−
which shows that the problem is unbounded
1)

→k→+∞ −∞

23

On the other hand, the discrete time control problem can be expressed in terms of the control variables as

min
u∈Rτ nu

a∆2u(cid:62)D−(cid:62)JD−1u

∆

u
(cid:107)

(cid:107)

2
2,

−

t=1 et+1e(cid:62)
t . We have, using that ∆ < 1 for the ﬁrst inequality,
where J = diag(∆, . . . , ∆, 1), D = I
u(cid:62)D−(cid:62)JD−1u
2
2
2/4. Hence for any a such
u
2 = ∆
u
(cid:107)
(cid:107)
(cid:107)
(cid:107)
that a∆2/4 > 1, the above problem is strongly convex and has a unique solution. Yet, if condition (D4) was
necessary the discrete control problem should not have a solution since condition (D4) cannot be satisﬁed.

∆σmin(D−1)2

2
D
2/
(cid:107)
(cid:107)

2
2 ≥
(cid:107)

2
2 ≥

D−1u

u
(cid:107)

∆

∆

−

≥

(cid:107)

(cid:107)

(cid:80)τ −1

8.2 Sufﬁcient Optimality Conditions

Sufﬁcient optimality conditions for continuous time control problems were presented by Mangasarian (1966);
Arrow (1968); Kamien and Schwartz (1971). We present their translation in discrete time and refer the reader to,
e.g., Kamien and Schwartz (1971) for the details in the continuous case. We rewrite problem (44) as

min
x0,...,xτ ∈Rnx
δ0,...,δτ −1∈Rnx

τ −1
(cid:88)

t=0

mt(xt, δt) + hτ (xτ ), where mt(xt, δt) =

inf
u∈Rnu
δt=ft(xt,u)

ht(xt, u)

(45)

subject to δt = xt+1

xt, x0 = ¯x0.

−

Sufﬁcient conditions are related to the true Hamiltonian, presented by Clarke (1979), and deﬁned as the convex
conjugate of mt(xt,

Rnx,

), i.e., for xt, λt+1
·

∈

H (cid:63)

t (xt, λt+1) = sup
δ∈Rnx

λ(cid:62)
t+1δ

mt(xt, δ) = sup
u∈Rnu

−

λ(cid:62)
t+1ft(xt, u)

ht(xt, u) = sup
u∈Rnu

−

Ht(xt, u, λt+1).

Theorem 8.4. Assume that mt deﬁned in (45) is such that mt(xt,
exist x∗

, λ∗

τ and λ∗

1, . . . , λ∗

t+1) is concave and

0, . . . , x∗

) is convex for any xt and hτ is convex. If there
·

τ such that H (cid:63)
t (
·
t+1)
t , λ∗

∂xtH (cid:63)
t (x∗
∂λt+1H (cid:63)

t , λ∗
t (x∗

t+1)

λ∗
t −
x∗
t+1 −

λ∗
t+1 ∈
x∗
t ∈

for t
for t

1, . . . , τ

0, . . . , τ

∈ {

∈ {

1

1

,
}
,
}

−

−

hτ (x∗
τ )

λ∗
τ =
∇
x∗
0 = ¯x0.

(46)

(47)

Then x∗

0, . . . , x∗

τ is an optimal trajectory for (45).

Proof. Since mt(xt,

) is convex for any xt, problem (45) can be rewritten
·

min
x1,...,xτ ∈Rnx
x0=ˆx0

sup
λ1,...,λτ ∈Rnx

τ −1
(cid:88)

(cid:16)

t=0

λt+1

(cid:62)(xt+1

xt)

−

−

H (cid:63)

t (xt, λt+1)

(cid:17)

+ hτ (xτ ).

(48)

The above problem can be written as minx∈Rτ nx supλ∈Rτ nx f (x, λ) with f (x,
tions amount to consider x∗, λ∗ such that (i) 0
for any x

∂λ∗ f (x∗, λ∗), (ii) f (
·

Rτ nx ,

∈

) concave for any x. The assump-
·
∂x∗ f (x∗, λ∗). Then
, λ∗) convex and 0

∈

∈

sup
λ∈Rτ nx

f (x, λ)

≥

f (x, λ∗)

(ii)

≥

f (x∗, λ∗)

(i)
= sup

f (x∗, λ).

λ∈Rτ nx

Hence x∗

∈

arg minx∈Rτ nx supλ∈Rτ nx f (x, λ), that is, x∗

0, . . . , x∗

τ is an optimal trajectory.

Conditions (46) and (47) of Theorem 8.4 amount to the existence of u∗

ht(xt, u), v∗

t ∈

arg maxv∈Rnu λ(cid:62)

t+1ft(xt, v)

ht(xt, v) such that

−

t ∈

arg maxu∈Rnu λ(cid:62)

t+1ft(xt, u)

−

λ∗
t −
x∗
t+1 −

λ∗
t+1 =
∇
t = ft(x∗
x∗

xtft(x∗
t , v∗
t , u∗
t ).

t )λ∗

t+1 − ∇

xtht(x∗

t , v∗
t )

24

9 Detailed Computational Schemes

In Fig. 2, we present a summary of the different algorithms presented until now. Recall that our objective is

τ −1
(cid:88)

(u) =

ht(xt, ut) + hτ (xτ )

J

t=0
s.t. xt+1 = ft(xt, ut)

∈ {
(u) = h(g(u)), where, for u = (u0; . . . ; uτ −1), x = (x1; . . . ; xτ ),

−

for t

0, . . . , τ

,
1
}

x0 = ¯x0,

that can be summarized as

J

τ −1
(cid:88)

h(x, u) =

ht(xt, ut) + hτ (xτ ), g(u) = (f [τ ](¯x0, u), u), f [τ ](x0, u) = (x1; . . . ; xτ )

t=0

.
}
We present nonlinear control algorithms from a functional viewpoint by introducing ﬁnite difference, linear and
quadratic expansions of the dynamics and the costs presented in the notations in Eq. (2).

s.t. xt+1 = ft(xt, ut)

0, . . . , τ

for t

∈ {

−

1

Rnu

→

Rp, with p = 1 (for the costs) or p = nx (for the dynamics), these expansions

f (x + y, u + v)

−

f (x, u),
1
2 ∇

(cid:96)x,u
f

: y, v

xf (x, u)(cid:62)y +

∇
2
uuf (x, u)[v, v,

uf (x, u)(cid:62)v

(49)

] +
·

∇

2
xuf (x, u)[y, v,

]
·

→ ∇
1
] +
2 ∇
·

uf (x, u)(cid:62)v +

2
xxf (x, u)[y, y,

For a function f : Rnx

read for x, u

∈
δx,u
f

Rnx

×
: y, v

×
Rnu,

qx,u
f

: y, v

→ ∇

→
xf (x, u)(cid:62)y +

∇

For λ

Rp, we denote shortly

∈
1
2 ∇

2f (x, u)[
,
·

, λ] : (y, v)
·

→

1
2 ∇

2
xxf (x, u)[y, y, λ] +

1
2 ∇

2
uuf (x, u)[v, v, λ] +

2
xuf (x, u)[y, v, λ].

∇

∈

2f (x)

f of a function f : Rd

tors, matrices or tensors deﬁning the linear or quadratic functions. For example, to store the linear expansion (cid:96)x
the quadratic expansion qx

In the algorithms, we consider storing in memory linear or quadratic functions by storing the associated vec-
f or
Rd×p and
Rd×d×p. In the backward or roll-out passes, we consider that having access to the linear or quadratic
∇
functions, means having access to the associated matrices/tensors deﬁning the operations as presented in, e.g.,
Algo. 2. The functional viewpoint helps to isolate the main technical operations in the procedures LQBP in
Algo. 2 or LBP in Algo. 3 and to identify the discrepancies between, e.g., the Newton oracle in Algo. 14 and a
DDP oracle with quadratic approximations presented in Algo. 16. For a presentation of the algorithms in a purely
algebraic viewpoint, we refer the reader to, e.g., Wright (1990); Liao and Shoemaker (1992); Sideris and Bobrow
(2005).

Rp around a point x, we consider storing

f (x)

→

∇

∈

In Algo. 7, 8, 9, we a priori need to check whether the subproblems deﬁned by the Bellman recursion are
)) is strongly convex
strongly convex or not. Namely in Algo. 7, 8, 9, we need to check that qt(x,
·
for any x. With the notations of Algo. 2, this amounts checking that Q + B(cid:62)Jt+1B
0. This can be done by
checking the positivity of the minimum eigenvalue of Q + B(cid:62)Jt+1B. In our implementation, we simply check
that

) + ct+1((cid:96)t(x,

(cid:31)

·

j0
t −

j0
t+1 =

1
2

(q + B(cid:62)jt+1)(cid:62)(Q + B(cid:62)Jt+1B)−1(q + B(cid:62)jt+1) < 0.

(50)

−
If condition (50) is not satisﬁed then necessarily Q + B(cid:62)Jt+1B
0. We chose to use condition (50) since
this quantity is directly available and computing the eigenvalues of Q + B(cid:62)Jt+1B
0 can slow down the
computations. Moreover, if criterion (50) is satisﬁed for all t
, this means that, for the Gauss-
}
Newton and the Newton methods, the resulting direction is a descent direction for the objective. Algo. 4 details
the aforementioned veriﬁcation step.

0, . . . , τ

∈ {

(cid:31)

−

(cid:54)(cid:23)

1

25

Forward pass
Algo. 5

Backward pass

Dyn approx.

Cost approx.

1st order

1st order

BackwardGD
Algo. 6

Roll-out
Algo. 11

None

2nd order

BackwardGN
Algo. 7

Linearized dyn.

2nd order

2nd order

BackwardNE
Algo. 8

Original dyn.

Linearized dyn.

BackwardDDP
Algo. 9

Original dyn.

Oracle

GD
Algo. 12

GN/ILQR
Algo. 13

DDP-LQ
Algo. 15
NE
Algo. 14

DDP-Q
Algo. 16

Figure 2: Taxonomy of non-linear control oracles. GD stands for gradient Descent, GN for Gauss-Newton, NE
for Newton, DDP-LQ and DDP-Q stand for DDP with linear quadratic or quadratic approx. The iterations of the
algorithms use a line-search procedure presented in Algo. 17 as illustrated in Algo. 18.

Algorithm 2 Analytic solution of Bellman’s equation (5) for linear dynamics, quadratic costs
[LQBP : (cid:96)t, qt, ct+1

ct, πt]

1: Inputs:

→

1. Linear function (cid:96)t parameterized as (cid:96)t(x, u) = Atx + Btu
2. Quadratic function qt parameterized as qt(x, u) = 1
3. Quadratic function ct+1 parameterized as ct+1(x) = 1
t x + j0

2: Deﬁne the cost-to-go function ct : x

2 x(cid:62)Jtx + j(cid:62)

1

2 x(cid:62)Ptx + 1

2 u(cid:62)Qtu + x(cid:62)Rtu + p(cid:62)

t x + q(cid:62)
t u

2 x(cid:62)Jt+1x + j(cid:62)
t with

t+1x + j0

t+1

→
(Rt + A(cid:62)

−
(Rt + A(cid:62)

Jt = Pt + A(cid:62)
t Jt+1At
jt = pt + A(cid:62)
t jt+1
1
2

t = j0
j0

−
(qt + B(cid:62)

t+1 −

t Jt+1Bt)(Qt + B(cid:62)

t Jt+1Bt)(Qt + B(cid:62)

t Jt+1Bt)−1(R(cid:62)
t Jt+1Bt)−1(qt + B(cid:62)

t + B(cid:62)
t jt+1),

t Jt+1At)

t jt+1)(cid:62)(Qt + B(cid:62)

t Jt+1Bt)−1(qt + B(cid:62)

t jt+1)

3: Deﬁne the policy πt : x

Ktx + kt with

→

Kt =

−

(Qt + B(cid:62)

t Jt+1Bt)−1(R(cid:62)

t + B(cid:62)

t Jt+1At),

kt =

−

(Qt + B(cid:62)

t Jt+1Bt)−1(qt + B(cid:62)

t jt+1)

4: Output: Cost-to-go ct and policy πt at time t

26

Algorithm 3 Analytic solution of Bellman’s equation (26) for linear dynamics, linear regularized costs
(cid:104)
LBP : (cid:96)f

(cid:105)

t , (cid:96)h

t , ct+1, ν

ct, πt

→

1: Inputs:

1. Linear function (cid:96)f parameterized as (cid:96)f
t (x, u) = Atx + Btu
t x + q(cid:62)
2. Linear function (cid:96)h parameterized as (cid:96)h
t (x, u) = p(cid:62)
t u
3. Afﬁne function ct+1 parameterized as ct+1(x) = j(cid:62)
t+1x + j0
4. Regularization ν

t+1

2: Deﬁne ct : x
3: Deﬁne πt : x
4: Output: Cost-to-go ct and policy πt at time t

0
≥
t with jt = pt + A(cid:62)
t x + j0
j(cid:62)
kt with kt =

(qt + B(cid:62)

→
→

−

t jt+1)/ν.

t jt+1, j0

t = j0

t+1 − (cid:107)

qt + B(cid:62)

t jt+1

2
2/(2ν).
(cid:107)

Algorithm 4 Check if subproblems given by qt(y,
[CheckSubProblem : (cid:96)t, qt, ct+1

valid

→

True, False
}

]

∈ {

) + ct+1((cid:96)t(y,
·

)) are valid for solving Bellman’s equation (5)
·

1: Option: Check strong convexity of subproblems or check only if the result gives a descent direction
2: Inputs:

1. Linear function (cid:96)t parameterized as (cid:96)t(x, u) = Atx + Btu,
2. Quadratic function qt parameterized as qt(x, u) = 1
3. Quadratic function ct+1 parameterized as ct+1(x) = 1

2 x(cid:62)Ptx + 1

2 u(cid:62)Qtu + x(cid:62)Rtu + p(cid:62)

t x + q(cid:62)
t u

2 x(cid:62)Jt+1x + j(cid:62)

t+1x + j0

t+1.

Compute the eigenvalues λ1
if λ1 > 0 then valid = True else valid = False

. . .

≤

λnu of Qt + B(cid:62)

t Jt+1Bt

3: if check strong convexity then
4:
5:
6: else if check descent direction then
7:
8:
9: end if
10: Output: valid

Compute j0
if j0

j0
t+1 =

t −

≤

1

t jt+1)(cid:62)(Qt + B(cid:62)
2 (qt + B(cid:62)
j0
t+1 < 0 then valid = True else valid = False

t −

−

t Jt+1Bt)−1(qt + B(cid:62)

t jt+1)

Algorithm 5 Forward pass
(cid:104)
Forward : u, (ft)τ −1

t=0 , (ht)τ

t=0, ¯x0, of , oh

(u), (mxt,ut

ft

t=0 , (mxt,ut
)τ −1

ht

t=0 , mxτ
)τ −1
hτ

(cid:105)

→ J

1: Inputs: Command u = (u0; . . . ; uτ −1), dynamics (ft)τ −1

t=0 , costs (ht)τ

0, 1, 2
}

∈ {

and the costs oh

t=0, initial state ¯x0, order of the infor-
0, 1, 2
}

∈ {

mation to collect on the dynamics of

2: Initialize x0 = ¯x0,
3: for t = 0, . . . τ
4:

J
1 do

(u) = 0

J

≥

−
(u)
Compute ht(xt, ut), update
if oh
1 then Compute and store
if oh = 2 then Compute and store
Compute xt+1 = ft(xt, ut)
1 then Compute and store
if of
if of = 2 then Compute and store

≥

5:

6:
7:
8:

(u) + ht(xt, ut)
ht(xt, ut) deﬁning (cid:96)xt,ut
2ht(xt, ut) deﬁning, with

ht

← J
∇
∇

as in (49)

ht(xt, ut), qxt,ut

ht

as in (49)

∇

ft(xt, ut) deﬁning (cid:96)xt,ut
2ft(xt, ut) deﬁning, with

ft

∇
∇

as in (49)

ft(xt, ut), qxt,ut

ft

as in (49)

∇

9:
10: end for
11: Compute hτ (xτ ), update
1 then Compute and store
12: if oh
13: if oh = 2 then Compute and store
14: Output: Total cost
15: Stored: (if of and oh non-zeros) Approximations (mxt,ut

(u) + hτ (xτ )
hτ (xτ ) deﬁning (cid:96)xτ
hτ
2hτ (xτ ) deﬁning, with

← J
∇
∇

(u)

(u)

≥

J

J

ft

as in (49)

hτ (xτ ), qxτ
hτ

∇
t=0 , (mxt,ut
)τ −1

ht

as in (49)

t=0 , mxτ
)τ −1
hτ

deﬁned by

mxt,ut
ft

=

(cid:40)

(cid:96)xt,ut
ft
qxt,ut
ft

if of = 1
if of = 2

, mxt,ut

ht

=

(cid:40)

(cid:96)xt,ut
ht
qxt,ut
ht

if oh = 1
if oh = 2

, mxτ
hτ

=

(cid:40)

(cid:96)xτ
hτ
qxτ
hτ

if oh = 1
if oh = 2

27

Algorithm 6 Backward pass for gradient oracle
(cid:104)
BackwardGD : ((cid:96)xt,ut

t=0 , ((cid:96)xt,ut
)τ −1

t=0 , (cid:96)xτ
)τ −1
hτ

ht

ft

, ν)

→

1: Inputs: Linear expansions of the dynamics ((cid:96)xt,ut

(cid:105)

(πt)τ −1

t=0 , c0
t=0 , linear expansions of the costs ((cid:96)xt,ut
)τ −1

ht

ft

t=0 , (cid:96)xτ
)τ −1
hτ

, regu-

larization ν > 0
2: Initialize cτ = (cid:96)xτ
hτ
1, . . . 0 do
3: for t = τ
Deﬁne (cid:96)t = (cid:96)xt,ut
(yt, vt) + ν
2
vt
qt : yt, vt
,
4:
2
2 (cid:107)
(cid:107)
, (cid:96)xt,ut
Compute ct, πt = LQBP((cid:96)t, qt, ct+1) = LBP((cid:96)xt,ut
ht

(cid:96)xt,ut
ht

→

−

ft

ft

5:
6: end for
7: Outputs: Policies (πt)τ −1

t=0 , cost-to-go function at initial time c0

, ct+1, ν) where LBP is given in Algo. 3

(πt)τ −1

t=0 , c0
t=0 , quadratic expansions of the costs (qxt,ut
)τ −1

ht

→

ft

t=0 , qxτ
)τ −1
hτ

,

Algorithm 7 Backward pass for Gauss-Newton oracle
(cid:104)
BackwardGN : ((cid:96)xt,ut
t=0 , (qxt,ut
)τ −1

t=0 , qxτ
)τ −1
hτ

, ν)

ht

ft

(cid:105)

1: Inputs: Linear expansions of the dynamics ((cid:96)xt,ut

0

regularization ν
≥
2: Initialize cτ = qxτ
hτ
1, . . . 0 do
3: for t = τ
Deﬁne (cid:96)t = (cid:96)xt,ut
,
4:
if CheckSubProblem((cid:96)t, qt, ct+1) is True then

qt : yt, vt

qxt,ut
ht

→

−

ft

(yt, vt) + ν

vt

2
2,
(cid:107)

2 (cid:107)

else

5:
6:
7:
8:
9:
10: end for
11: Outputs: Policies (πt)τ −1

0 for s

πs : x

end if

→

≤

t, c0 : x

, break

→ −∞

t=0 , cost-to-go function at initial time c0

Compute ct, πt = LQBP((cid:96)t, qt, ct+1) with LQBP given in Algo. 2

)τ −1
t=0 , quadratic expansions of

the costs

ft

(yt, vt) + ν

vt

2 + 1
2

2 ∇

2ft(xt, ut)[
,
·

·

(cid:107)

, λt+1](yt, vt)

2 (cid:107)

Algorithm 8 Backward pass for Newton oracle
(cid:104)
BackwardNE : (qxt,ut
t=0 , qxτ
)τ −1
hτ
the dynamics (qxt,ut

t=0 , (qxt,ut
)τ −1

(πt)τ −1

t=0 , c0

, ν)

→

ht

ft

(cid:105)

(qxt,ut
ht

, regularization ν

1: Inputs: Quadratic expansions of
0

t=0 , qxτ
)τ −1
hτ
2: Initialize cτ = qxτ
hτ
1, . . . 0 do
3: for t = τ
Deﬁne (cid:96)t = (cid:96)xt,ut
qt : (yt, vt)
,
4:
ft
xtht(xt, ut) +
Compute λt =
5:
if CheckSubProblem((cid:96)t, qt, ct+1) is True then
6:

≥
hτ (xτ )

qxt,ut
ht

→
xtft(xt, ut)λt+1

, λτ =

∇

∇

∇

−

Compute ct, πt = LQBP((cid:96)t, qt, ct+1) with LQBP given in Algo. 2

else

7:
8:
9:
10:
11: end for
12: Outputs: Policies (πt)τ −1

0 for s

πs : x

end if

→

≤

t, c0 : x

, break

→ −∞

t=0 , cost-to-go function at initial time c0

28

Algorithm 9 Backward pass for a DDP approach with quadratic approximations
(cid:104)
BackwardDDP : (qxt,ut
t=0 , qxτ
)τ −1
hτ

ht
1: Inputs: Quadratic expansions on the dynamics (qxt,ut

t=0 , (qxt,ut
)τ −1

(πt)τ −1

t=0 , c0

, ν)

→

ft

(cid:105)

0

≥

(qxt,ut
ht

, regularization ν

t=0 , qxτ
)τ −1
hτ
2: Initialize cτ = qxτ
hτ
1, . . . 0 do
3: for t = τ
Deﬁne (cid:96)t = (cid:96)xt,ut
,
4:
if CheckSubProblem((cid:96)t, qt, ct+1) is True then
5:
6:
7:

qt : yt, vt

qxt,ut
ht

→

−

else

ft

t, c0 : x

, break

→ −∞

πs : x

0 for s

end if

8:
9:
10: end for
11: Outputs: Policies (πt)τ −1

→

≤

t=0 , cost-to-go function at initial time c0

Compute ct, πt = LQBP((cid:96)t, qt, ct+1) with LQBP given in Algo. 2

)τ −1
t=0 , quadratic expansions on the costs

ft

(yt, vt) + ν

yt

2 + 1
2
(cid:107)

2 ∇

2ft(xt, ut)[
,
·

,
·

∇

ct+1(0)](yt, vt)

2 (cid:107)

t=0 with associated trajectory (xt)τ

t=0

hτ (xτ ) of the ﬁnal cost on xτ

ft(xt, ut)(cid:62)λt+1 on xt, ut which gives 1

(yt, vt) + ν

vt

2 + 1
2

2 (cid:107)

(cid:107)

2ft(xt, ut)[
,
·

·

2 ∇

2 ∇

2ft(xt, ut)[
,
·
, λt+1](yt, vt)

, λt+1].

·

Algorithm 10 Backward pass for Newton oracle with function storage
1: Inputs: Stored functions (ft)τ −1
2: Compute the quadratic expansion qxτ
hτ
3: Set cτ = qxτ
, λτ =
hτ
∇
4: for t = τ
1, . . . 0 do
5:

t=0, inputs (ut)τ −1
of the ﬁnal cost and the derivative

t=0 , costs (ht)τ

hτ (xτ )

−

∇

of the dynamic around xt, ut
of the cost around xt, ut

ft

Compute the linear approximation (cid:96)xt,ut
Compute the quadratic approximation qxt,ut
Compute the Hessian of xt, ut
→
Deﬁne (cid:96)t = (cid:96)xt,ut
qt : (yt, vt)
,
ft
Compute λt =
xtht(xt, ut) +
if CheckSubProblem((cid:96)t, qt, ct+1) is True then

→
xtft(xt, ut)λt+1

qxt,ut
ht

∇

∇

ht

Compute ct, πt = LQBP((cid:96)t, qt, ct+1)

6:

7:
8:
9:
10:
11:

else

πs : x

12:
13:
14:
15: end for
16: Outputs: Policies (πt)τ −1

0 for s

end if

→

≤

t, c0 : x

, break

→ −∞

t=0 , cost-to-go function at initial time c0

t=1 , (φt)τ −1

Algorithm 11 Roll-out on dynamics
(cid:2)Roll : y0, (πt)τ −1
v(cid:3)
t=0 →
1: Inputs: Initial state y0, sequence of policies (πt)τ −1
2: for t = 0, . . . , τ
3:
4: end for
5: Output: Sequence of controllers v = (v0; . . . ; vτ −1)

Compute and store vt = πt(yt), yt+1 = φt(yt, vt).

1 do

−

t=0 , dynamics to roll-on (φt)τ −1
t=0

29

Algorithm 12 Gradient oracle
(cid:2)GD : u, (ft)τ −1
1: Inputs: Command u=(u0; . . . ; uτ −1), dynamics (ft)τ −1
2: Compute with Algo. 5

t=0 , (ht)τ

t=0, ¯x0, ν

v(cid:3)

→

t=0 , costs (ht)τ

t=0, initial state ¯x0, regularization ν>0

(u), ((cid:96)xt,ut

ft

J
3: Compute with Algo. 6

t=0 , ((cid:96)xt,ut
)τ −1

ht

t=0 , (cid:96)xτ
)τ −1
hτ

= Forward(u, (ft)τ −1

t=0 , (ht)τ

t=0, ¯x0, of = 1, oh = 1)

(πt)τ −1

t=0 , c0 = BackwardGD(((cid:96)xt,ut

ft

t=0 , ((cid:96)xt,ut
)τ −1

ht

t=0 , qxτ
)τ −1
hτ

, ν)

4: Compute with Algo. 11

v = Roll(0, (πt)τ −1

t=0 , ((cid:96)xt,ut

ft

5: Output: Gradient direction v = arg min˜v∈Rτ nu

)τ −1
t=0 )
(cid:111)

(cid:110)

h◦g(˜v) + ν
(cid:96)u
˜v
2 (cid:107)

2
2
(cid:107)

=

ν−1

−

(h

∇

◦

g)(u)

Algorithm 13 Gauss-Newton oracle
(cid:2)GN : u, (ft)τ −1
t=0 , (ht)τ
1: Inputs: Command u=(u0; . . . ; uτ −1), dynamics (ft)τ −1
2: Compute with Algo. 5

t=0, ¯x0, ν

v(cid:3)

→

t=0 , costs (ht)τ

t=0, initial state ¯x0, regularization ν

0
≥

(u), ((cid:96)xt,ut

ft

J
3: Compute with Algo. 7

t=0 , (qxt,ut
)τ −1

ht

t=0 , qxτ
)τ −1
hτ

= Forward(u, (ft)τ −1

t=0 , (ht)τ

t=0, ¯x0, of = 1, oh = 2)

(πt)τ −1

t=0 , c0 = BackwardGN(((cid:96)xt,ut

ft

t=0 , (qxt,ut
)τ −1

ht

t=0 , qxτ
)τ −1
hτ

, ν)

4: Compute with Algo. 11

5: Output: If c0(0) = +

∞
v = arg min˜v∈Rτ nu

v = Roll(0, (πt)τ −1

t=0 , ((cid:96)xt,ut
, returns infeasible, otherwise returns Gauss-Newton direction
(cid:110)
g(u) + ν I)−1
(
∇

g (˜v)) + ν
2 (cid:107)

2h(x, u)

)τ −1
t=0 )

qg(u)
h

g(u)

((cid:96)u

∇

∇

=

−

(cid:111)

˜v

2
2

ft

(cid:107)

(h

∇

◦

g)(u)

t=0 , (ht)τ

Algorithm 14 Newton oracle
(cid:2)NE : u, (ft)τ −1
1: Inputs: Command u=(u0; . . . ; uτ −1), dynamics (ft)τ −1
2: Compute with Algo. 5

t=0, ¯x0, ν

v(cid:3)

→

t=0 , costs (ht)τ

t=0, initial state ¯x0, regularization ν

0
≥

(u), (qxt,ut

ft

J
3: Compute with Algo. 8

t=0 , (qxt,ut
)τ −1

ht

t=0 , qxτ
)τ −1
hτ

= Forward(u, (ft)τ −1

t=0 , (ht)τ

t=0, ¯x0, of = 2, oh = 2)

(πt)τ −1

t=0 , c0 = BackwardNE((qxt,ut

ft

t=0 , (qxt,ut
)τ −1

ht

t=0 , qxτ
)τ −1
hτ

, ν)

4: Compute with Algo. 11

v = Roll(0, (πt)τ −1

t=0 , ((cid:96)xt,ut

ft

)τ −1
t=0 )

5: Output: If c0(0) = +

∞
v = arg min˜v∈Rτ nu

, returns infeasible, otherwise returns Newton direction
(cid:110)

(cid:111)

2(h

g)(u) + ν I)−1

(h

g)(u)

h◦g(˜v) + ν
qu
2 (cid:107)

˜v

(cid:107)

2
2

=

(
∇

−

◦

∇

◦

30

Algorithm 15 Differential dynamic programming oracle with linear quadratic approximations
(cid:2)DDP-LQ : u, (ft)τ −1
1: Inputs: Command u=(u0; . . . ; uτ −1), dynamics (ft)τ −1
2: Compute with Algo. 5

t=0 , costs (ht)τ

t=0 , (ht)τ

t=0, ¯x0, ν

v(cid:3)

→

t=0, initial state ¯x0, regularization ν

0
≥

(u), ((cid:96)xt,ut

ft

J
3: Compute with Algo. 7

t=0 , (qxt,ut
)τ −1

ht

t=0 , qxτ
)τ −1
hτ

= Forward(u, (ft)τ −1

t=0 , (ht)τ

t=0, ¯x0, of = 1, oh = 2)

(πt)τ −1

t=0 , c0 = BackwardGN(((cid:96)xt,ut

ft

t=0 , (qxt,ut
)τ −1

ht

t=0 , qxτ
)τ −1
hτ

, ν)

4: Compute with Algo. 11, for δxt,ut

ft

(yt, vt) = f (xt + yt, ut + vt)

f (xt, ut),

v = Roll(0, (πt)τ −1

−
t=0 , (δxt,ut
)τ −1
t=0 )

ft

5: Output: If c0(0) = +

imations v

, returns infeasible, otherwise returns DDP oracle with linear-quadratic approx-

∞

Algorithm 16 Differential dynamic programming oracle with quadratic approximations
(cid:2)DDP-Q : u, (ft)τ −1
v(cid:3)
t=0, ¯x0, ν
1: Inputs: Command u=(u0; . . . ; uτ −1), dynamics (ft)τ −1
2: Compute with Algo. 5

t=0 , costs (ht)τ

t=0 , (ht)τ

→

t=0, initial state ¯x0, regularization ν

0
≥

(u), (qxt,ut

ft

J
3: Compute with Algo. 9

t=0 , (qxt,ut
)τ −1

ht

t=0 , qxτ
)τ −1
hτ

= Forward(u, (ft)τ −1

t=0 , (ht)τ

t=0, ¯x0, of = 2, oh = 2)

(πt)τ −1

t=0 , c0 = BackwardDDP((qxt,ut

ft

t=0 , (qxt,ut
)τ −1

ht

t=0 , qxτ
)τ −1
hτ

, ν)

4: Compute with Algo. 11, for δxt,ut

ft

(yt, vt) = f (xt + yt, ut + vt)

f (xt, ut),

v = Roll(0, (πt)τ −1

−
t=0 , (δxt,ut
)τ −1
t=0 )

ft

5: Output: If c0(0) = +

∞

tions v

, returns infeasible, otherwise returns DDP oracle with quadratic approxima-

31

Algorithm 17 Line-search
(cid:2)LineSearch : u, (ht)τ
1: Option: directional step or regularized step
2: Inputs: Current controls u, costs (ht)τ

t=0 , (φt)τ −1

t=0, (ft)τ −1

t=0 , (Pol : γ

(πγ

t )τ −1

t=0 , cγ
0 )

unext(cid:3)

→

→

t=0 , family of policies and corresponding costs given by γ

t=0, initial state ¯x0, original dynamics (ft)τ −1
t=0 , cγ

t )τ −1

t=0 , dynamics to roll-
0 , decreasing factor

(πγ

→

t=0, ¯x0, of = 0, oh = 0)

∈

∇

t=0 , (ht)τ

h(x, u) for x = f [τ ](¯x0, u)

(u) = Forward(u, (ft)τ −1

Compute
Initialize γ = ρincγprev/

(0, 1), increasing factor ρinc > 1, previous stepsize γprev

out on (φt)τ −1
ρdec
3: Compute
J
4: if directional step then
Initialize γ = 1
5:
6: else if regularized step then
7:
8:
(cid:107)∇
9: end if
10: Initialize y0 = 0, accept = False, minimal stepsize γmin = 10−12
11: while not accept do
Get πγ
0 = Pol(γ)
12:
Compute vγ = Roll(y0, (πγ
13:
Set unext = u + vγ
14:
Compute
15:
if
16:
J
if γ

(unext) = Forward(unext, (ft)τ −1

cγ
0 (0) then set accept = True else set γ

≤
− J
γmin then break

t=1 , (φt)τ −1
t=0 )

h(x, u)
2
(cid:107)

t=0 , (ht)τ

J
(unext)

t )τ −1

t , cγ

(u)

17:
≤
18: end while
19: if regularized step then γ := γ
h(x, u)
(cid:107)
20: Output: Next sequence of controllers unext, store value of the stepsize selected γ

(cid:107)∇

2

t=0, ¯x0, of = 0, oh = 0)

ρdecγ

→

Algorithm 18 Iterative Linear Quadratic Regulator/Gauss-Newton step with line-search on descent directions
1: Inputs: Command u, dynamics (ft)τ −1
2: Compute with Algo. 5

t=0 , costs (ht)τ −1

t=0 , inital state ¯x0

(u), ((cid:96)xt,ut

ft

J
3: Compute with Algo. 7

t=0 , (qxt,ut
)τ −1

ht

t=0 , qxτ
)τ −1
hτ

= Forward(u, (ft)τ −1

t=0 , (ht)τ

t=0, ¯x0, of = 1, oh = 2)

(πt)τ −1

t=0 , c0 = BackwardGN(((cid:96)xt,ut

ft

t=0 , (qxt,ut
)τ −1

ht

t=0 , qxτ
)τ −1
hτ

, 0)

4: Set ν = νinit with, e.g., νinit = 10−6
5: while c0(0) = +
6:
7:
8: end while

do
Compute (πt)τ −1
Set ν

ρincν with, e.g., ρinc = 10

→

∞
t=0 , c0 = BackwardGN(((cid:96)xt,ut

ft

9: Deﬁne Pol : γ

(cid:18) (πγ
t :
cγ
0 :
10: Compute with Algo. 17

→

y
y

→
→

γπt(0) +
γc0(y)

t=0 , (qxt,ut
)τ −1

ht

t=0 , qxτ
)τ −1
hτ

, ν)

πt(0)(cid:62)y)τ −1
t=0 ,

(cid:19)

∇

unext = LineSearch(u, (ht)τ

t=0, (ft)τ −1

t=0 , ((cid:96)xt,ut

ft

)τ −1
t=0 , Pol)

11: Output: Next sequence of controllers unext

32

Figure 3: Computational scheme of a gradient oracle.

33

............Input function or procedureLinear functionStore in memoryFigure 4: Computational scheme of an ILQR/Gauss-Newton oracle.

34

...... ............Input function or procedureLinear functionQuadratic functionStore in memoryFigure 5: Computational scheme of a DDP oracle with linear quadratic approximations.

35

...... ............Input function or procedureLinear functionQuadratic functionStore in memoryFigure 6: Computational scheme of a Newton oracle.

36

...... ..................Input function or procedureLinear functionQuadratic functionStore in memoryFigure 7: Computational scheme of a DDP oracle with quadratic approximations.

37

...... ............Input function or procedureLinear functionQuadratic functionStore in memory10 Experiments

We ﬁrst describe in detail the continuous time systems studied in the experiments and then present the numer-
ical performances of the algorithms reviewed in this work. The code is available at https://github.com/
vroulet/ilqc. Numerical constants are provided in the Appendix.

10.1 Discretization

In the following, we denote by z(t) the state of a system at time t. Given a control u(t) at time t, we consider
time-invariant dynamical systems governed by a differential equation of the form

where f models the physics of the movement and is described below for each model.

˙z(t) = f(z(t), u(t)),

for t

[0, T ],

∈

Given a continuous time dynamic, the discrete time dynamics are given by a discretization method such that

the states follow dynamics of the form

for a sequence of controls u0, . . . , uτ −1. One discretization method is the Euler method, which, for a time-step
∆ = T /τ , is

zt+1 = f (zt, ut)

for t

0, . . . τ

∈ {

,

1
}

−

Alternatively, we can consider a Runge-Kutta method of order 4 that deﬁnes the discrete-time dynamics as

f (zt, ut) = zt + ∆f(zt, ut).

f (zt, ut) = zt +

∆
6
where k1 = f(zt, ut)

(k1 + k2 + k3 + k4)

k3 = f(zt + ∆k2/2, ut)

k2 = f(zt + ∆k1/2, ut)
k4 = f(zt + ∆k3, ut),

where we consider the controls to be piecewise constant, i.e., constant on time intervals of size ∆. We can also
consider a Runge-Kutta method with varying control inputs such that, for ut = (vt, vt+1/3, vt+2/3),

f (zt, ut) = zt +

∆
6
where k1 = f(zt, vt)

(k1 + k2 + k3 + k4)

k2 = f(zt + ∆k1/2, vt+1/3)
k4 = f(zt + ∆k3, vt+2/3).

k3 = f(zt + ∆k2/2, vt+1/3)

10.2 Swinging up a Pendulum

10.2.1 Fixed Pendulum

We consider the problem of controlling a ﬁxed pendulum such that it swings up as illustrated in Fig. 8. Namely,
the dynamics of a pendulum are given as

ml2 ¨θ(t) =

mlg sin θ(t)

µ ˙θ(t) + u(t),

−

−

with θ the angle of the rod, m the mass of the blob, l the length of the blob, µ a friction coefﬁcient, g the
gravitational constant, and u a torque applied to the pendulum (which deﬁnes the control we have on the system).
Denoting the angle speed ω = ˙θ and the state of the system x = (θ; ω), the continuous time dynamics are
(cid:18)

(cid:19)

f : (x = (θ; ω), u)

→

g
l sin θ

−

−

ω
µ
ml2 ω + 1

ml2 u

,

such that the continuous time system is deﬁned by ˙x(t) = f(x(t), u(t)). After discretization by an Euler method,
we get discrete time dynamics ft(xt, ut) = f (xt, ut) of the form, for xt = (θt; ωt) and ∆ the discretization step,

f (xt, ut) = xt + ∆f(xt, ut) =

ωt + ∆ (cid:0)
A classical task is to enforce the pendulum to swing up and stop without using too much torque at each time

ml2 ωt + 1

g
l sin θt

ml2 ut

−

−

(cid:1)

.

(cid:18)

θt + ∆ωt
µ

(cid:19)

step, i.e., for ¯x0 = (0; 0), the costs we consider are, for some non-negative parameters λ

0, ρ

0,

ht(xt, ut) = λ

ut
(cid:107)

2
2
(cid:107)

for t

0, . . . , τ

∈ {

1

,
}

−

hτ (xτ ) = (π

−

38

≥
θτ )2 + ρ

≥
2
2.
(cid:107)

ωτ
(cid:107)

Figure 8: Fixed pendulum.

Figure 9: Pendulum on a cart.

10.2.2 Pendulum on a Cart

We consider here controlling a pendulum on a cart as illustrated in Fig. 9. This system is described by the angle θ
of the pendulum with the vertical and the position zx of the cart on the horizontal axis. Contrary to the previous
example, here we do not control directly the angle of the pendulum we only control the system with a force u
that drives the acceleration of the cart. The dynamics of the system satisfy (see Magdy et al. (2019) for detailed
derivations)

(M + m)¨zx + ml cos θ ¨θ =
ml cos θ¨zx + (I + ml2)¨θ =

b ˙zx + ml ˙θ2 sin θ + u
mgl sin θ,

−

−

(51)

where M is the mass of the cart, m is the mass of the pendulum rod, I is the pendulum rod moment of inertia, l is
the length of the rod, and b is the viscous friction coefﬁcient of the cart. The system of equations can be written in
matrix form and solved to express the angle and position accelerations as

(cid:19)

(cid:18)¨zx
¨θ

=

=

(cid:18)M + m ml cos θ
I + ml2

ml cos θ

(cid:19)−1 (cid:18)

−

(cid:19)

b ˙zx + ml ˙θ2 sin θ + u
mgl sin θ
(cid:18) I + ml2

−

1
I(M + m) + ml2M + m2l2 sin2 θ

ml cos θ
ml cos θ M + m

−

−

(cid:19) (cid:18)

−

b ˙zx + ml ˙θ2 sin θ + u
mgl sin θ

(cid:19)

.

−

The discrete dynamical system follows using an Euler discretization scheme or a Runge Kutta method. We con-
sider the task of swinging up the pendulum and keeping it vertical for a few time steps while constraining the move-
ment of the cart on the horizontal line. Formally, we consider the following cost, deﬁned for xt = (zx, θ, ζx, ω),
where ζx, ω represent the discretizations of ˙zx and ˙θ respectively,

h(xt, ut) =

(cid:40)

ρ2(max(zx
ρ2(max(zx

−
−

¯z+
x , 0) + max(zx + ¯z−
x , 0) + max(zx + ¯z−
¯z+

x , 0)) + λu2
x , 0)) + λu2
t

t + (θ + π)2 + ρ1ω2

¯t
if t
≥
if t < ¯t,

where ρ1, ρ2, λ are some non-negative parameters, ¯t is a time step after which the pendulum needs to stay vertically
inverted and ¯z+

x are bounds that restrain the movement of the cart along the whole horizontal line.

x , ¯z−

10.3 Autonomous Car Racing

We consider the control of a car on a track through two different dynamical models: a simple one where the
orientation of the car is directly controlled by the steering angle, and a more realistic one that takes into account
the tire forces to control the orientation of the car. In the following, we present the dynamics, a simple tracking
cost, and a contouring cost enforcing the car to race the track at a reference speed or as fast as possible.

10.3.1 Dynamics

Simple model. A simple model of the car is described in Fig. 10. The state of the car is decomposed as z(t) =
(x(t), y(t), θ(t), v(t)), where (dropping the dependency w.r.t. time for simplicity)

1. x, y denote the position of the car on the plane,
2. θ denotes the angle between the orientation of the car and the horizontal axis, a.k.a. the yaw,
3. v denotes the longitudinal speed.

39

Figure 10: Simple model of a car.

Figure 11: Bicycle model of a car.

The car is controlled through u(t) = (a(t), δ(t)), where

1. a is the longitudinal acceleration of the car,
2. δ is the steering angle.

For a car of length L, the continuous time dynamics are then

˙x = v cos θ

˙y = v sin θ

˙θ = v tan(δ)/L

˙v = a.

(52)

Bicycle model. We consider the model presented by Liniger et al. (2015) recalled below and illustrated in
Fig. 11. In this model, the state of the car at time t is decomposed as z(t) = (x(t), y(t), θ(t), vx(t), vy(t), ω(t))
where

1. x, y denote the position of the car on the plane,
2. θ denotes the angle between the orientation of the car and the horizontal axis, a.k.a. the yaw,
3. vx denotes the longitudinal speed,
4. vy denotes the lateral speed,
5. ω denotes the derivative of the orientation of the car, a.k.a. the yaw rate.

The control variables are analogous to the simple model, i.e., u(t) = (a(t), δ(t)), where

1. a is the PWM duty cycle of the car, this duty cycle can be negative to take into account braking,
2. δ is the steering angle.

These controls act on the state through the following forces.

1. A longitudinal force on the rear wheels, denoted Fr,x modeled using a motor model for the DC electric

motor as well as a friction model for the rolling resistance and the drag

where Cm1, Cm2, Cr0, Crd are constants estimated from experiments, see Appendix A.

Fr,x = (Cm1

Cm2vx)a

−

Cr0

−

−

Crdv2
x

2. Lateral forces on the front and rear wheels, denoted Ff,y, Fy,r respectively, modeled using a simpliﬁed

Pacejka tire model

Ff,y = Df sin(Cf arctan(Bf αf )) where αf = δ

arctan2

−

Fr,y = Dr sin(Cr arctan(Brαr)) where αr = arctan2

(cid:19)

(cid:18) ωlf + vy
vx
(cid:19)

vy

(cid:18) ωlr

−
vx

where αf , αr are the slip angles on the front and rear wheels respectively, lf , lr are the distance from
the center of gravity to the front and the rear wheel respectively and the constants Br, Cr, Dr, Bf , Cf , Df
deﬁne the exact shape of the semi-empirical curve, presented in Fig. 12.

40

Figure 12: Pacejka model of the friction on the tires as a function of the slip angles

The continuous time dynamics are then

˙x = vx cos θ

vy sin θ

−

˙y = vx sin θ + vy cos θ

˙θ = ω

where m is the mass of the car and Iz is the inertia.

10.3.2 Costs

˙vx =

˙vy =

˙ω =

1
m
1
m
1
Iz

(Fr,x

−

Ff,y sin δ) + vyω

(Fr,y + Ff,y cos δ)

vxω

−

(Ff,ylf cos δ

Fr,ylr),

−

(53)

Tracks. We consider tracks that are given as a continuous curve, namely a cubic spline approximating a set of
points. As a result, for any time t, we have access to the corresponding point ˆx(t), ˆy(t) on the curve. The track
we consider is a simple track illustrated in Fig. 13.

Tracking cost. A simple cost on the states is

ct(zt) =

xt
(cid:107)

−

ˆx(∆vreft)

2
2 +
(cid:107)

yt
(cid:107)

2
ˆy(∆vreft)
2
(cid:107)

−

for t = 1, . . . , τ,

(54)

for zt = (xt, yt), where ∆ is some discretization step and vref is some reference speed. The cost above is the one
we choose for the simple model of a car. The disadvantage of such a cost is that it enforces the car to follow the
track at a constant speed which may not be physically possible. We consider in the following a contouring cost as
done by Liniger et al. (2015).

Ideal cost. Given a track parameterized in continuous time, an ideal cost is to enforce the car to be as close as
possible to the track, while moving along the track as fast as possible. Formally, deﬁne the distance from the car
at position (x, y) to the track deﬁned by the curve ˆx(t), ˆy(t) as

d(x, y) = min
t∈R

(cid:112)((x

−

ˆx(t))2 + (y

ˆy(t))2.

−

Denoting t∗ = t(x, y) = arg mint∈R
position (x, y), the distance d(x, y) can be expressed as

ˆx(t))2 + (y

(x

−

ˆy(t))2, the reference time on the track for a car at

−

d(x, y) = sin(θ(t∗)) (x

ˆx(t∗))

−

−

cos(θ(t∗)) (y

ˆy(t∗)) ,

−

where θ(t) = ∂ ˆy(t)
cost for the problem is then deﬁned as h(z) = h(x, y) = d(x, y)2
the track by minimizing d(x, y)2, and also encourages the car to go as far as possible by adding the term

∂ ˆx(t) is the angle of the track with the x-axis. The distance d(x, y) is illustrated in Fig. 14. An ideal
t(x, y), which enforces the car to be close to
t(x, y).

−

−

41

−202αforαr−0.2−0.10.00.10.2PacejkaTiremodelFf,yFr,yFigure 13: Simple and complex tracks used with a trajectory computed on the bicycle model (53).

Contouring and lagging costs. The computation of t∗ involves solving an optimization problem and is not
practical. As Liniger et al. (2015), we rather augment the states with a ﬂexible reference time. Namely, we
augment the state of the car by adding a variable s whose objective is to approximate the reference time t∗. The
cost is then decomposed into the contouring cost and the lagging cost illustrated in Fig. 15 and deﬁned as

ec(x, y, s) = sin(θ(s)) (x
el(x, y, s) =

−
cos(θ(s)) (x

−

−

ˆx(s))

cos(θ(s)) (y

−
ˆx(s))

−
sin(θ(s)) (y

−

ˆy(s))

ˆy(s)) .

−

speed. Namely we consider an additional penalty of the form
advance. For the reference time s not to go backward in time, we add a log-barrier term

Rather than encouraging the car to make the most progress on the track, we enforce them to keep a reference
2
2 where vref is a parameter chosen in
(cid:107)
ε log( ˙s) for ε = 10−6.
Finally, we let the system control the reference time through its second order derivative ¨s. Overall this means
that we augment the state variable by adding the variables s and ν := vs and that we augment the control variable
by adding the variable α := as such that the discretized problem is written for, e.g., the bicycle model, as

˙s
(cid:107)

vref

−

−

min
(a0,δ0,α0),...,(aτ −1,δτ −1,ατ −1)

τ −1
(cid:88)

t=0

ρcec(xt, yt, st)2 + ρlel(xt, yt, st)2 + ρv

vs,t
(cid:107)

−

vref

2
2 −

(cid:107)

ε log νt

s.t. xt+1, yt+1, θt+1, vx,t+1, vy,t+1, ωt+1 = f (xt, yt, θ,vx,t, vy,t, ωt, δt, at)

st+1 = st + ∆νt,

νt+1 = νt + ∆αt

z0 = ˆz0

s0 = 0 ν0 = vref,

where f is a discretization of the continuous time dynamics, ∆ is a discretization step and ˆz0 is a given initial state
where z0 regroups all state variables at time 0 (i.e. all variables except a0, δ0).

This cost is deﬁned by the parameters ρc, ρl, ρv, vref which are ﬁxed in advance. The larger the parameter ρc,
the closer the car to the track. The larger the parameter ρl, the closer the car to its reference time s. In practice, we
want the reference time to be a good approximation of the ideal projection of the car on the track so ρl should be
chosen large enough. On the other hand, varying ρc allows having a car that is either conservative and potentially
slow or a car that is fast but inaccurate, i.e., far from the track. The most important aspect of the trajectory is to
ensure that the car remains inside the borders of the track deﬁned in advance.

Border costs. To enforce the car to remain inside the track deﬁned by some borders, we penalize the approxi-
mated distance of the car to the border when it goes outside the border as eb(x, y, s) = ein
b (x, y, s)
with

b (x, y, s) + eout

b (x, y, s) = max((w + din(x, y, s))2, 0)
ein
b (x, y, s) = max((w + dout(x, y, s))2, 0)
eout

din(x, y, s) =
−
dout(x, y, s) = (z

(z

−

zin(s))(cid:62)nin(s)
−
zout(s))(cid:62)nout(s)

(55)

for z = (x, y), where nin(s) and nout(s) denote the normal at the borders at time s and w is the width of the car.
In practice, we use a smooth approximation of the max function in Eq. (55). The normals nin(s) and nout(s) can
easily be computed by derivating the curves deﬁning the inner and outer borders. These costs are illustrated in
Fig. 16.

42

1.001.251.501.752.002.252.502.753.00Speed1.001.251.501.752.002.252.502.753.00SpeedFigure 14: Distance to the track.

Figure 15: Approx. by contouring
and lagging costs.

Figure 16: Border costs.

Constrained controls. We constrain the steering angle to be between [
ing angle as

−

δ(˜δ) =

2
3

arctan(˜δ)

for ˜δ

R.

∈

π/3, π/3] by parameterizing the steer-

Similarly, we constrain the acceleration a to be between [c, d] (with c =

0.1, d = 1.), by parameterizing it as

a(˜a) = (d

−

c) sig(4˜a/(d

−

−
c)) + c

with sig : x

→

1/(1 + e−x) the sigmoid function. The ﬁnal set of control variables is then ˜a, ˜δ, α.

Control costs. For both trajectory costs, we add a square regularization on the control variables of the system,
i.e., the cost on the control variables is λ

0 where ut are the control variables at time t.

ut
(cid:107)

2
2 for some λ
(cid:107)

≥

Overall contouring cost. The whole problem with contouring cost is then

min
(˜a0,˜δ0,α0),...,(˜aτ −1,˜δτ −1, ˜ατ −1)

τ −1
(cid:88)

t=0

(cid:104)
ρcec(xt, yt, st)2 + ρlel(xt, yt, st)2 + ρv

vs,t

(cid:107)

vref

2
2 −
(cid:107)

−

ε log(νt)

+ ρbeb(xt, yt, st)2 + λ(˜a2

t + ˜δ2

t + α2
t )

(cid:105)

(56)

s.t. xt+1, yt+1, θt+1, vx,t+1, vy,t+1, ωt+1 = f (xt, yt, θt, vx,t, vy,t, ωt, δt(˜δt), at(˜at))

st+1 = st + ∆νt,

νt+1 = νt + ∆αt

z0 = ˆz0

s0 = 0 ν0 = vref,

with parameters ρc, ρl, ρv, vref, ρb, λ and f given in Eq. (53).

J

the objective, u(k) the set of controls at iteration k, and

10.4 Results
∗)(cid:1)
All the following plots are in log-scale where on the vertical axis we plot log (cid:0)(
J
(u) estimated from running
with
the algorithms for more iterations than presented. The acronyms (GD, GN, NE, DDP-LQ, DDP-Q) correspond
to the taxonomy of algorithms presented in Fig. 2. For the bicycle model of a car, gradient oracles appeared
numerically unstable for moderate horizons, probably due to the highly nonlinear modeling of the tire forces,
hence we do not plot GD for that example. Finally the algorithms are stopped if the stepsizes found by line-search
are smaller than 10−20 or if the relative difference in terms of costs is smaller than 10−12. The algorithms are run
with double precision.

∗ = minu∈Rτ nu

(u(k))

(u(0))

∗)/(

− J

− J

J

J

J

43

 10.4.1 Linear Quadratic Approximations

In Fig. 17, we compare a gradient descent and nonlinear control algorithms with linear quadratic approximations,
i.e., GN or DDP-LQ with directional or regularized steps.

1. We observe that GN and DDP-LQ always outperform GD.
2. Similarly, we observe that DDP generally outperforms GN, for the same steps (directional or regularized),
except for the simple model of a car where a GN method with directional steps appears better than its DDP
counterpart.

3. For GN, taking a directional step can be better than taking regularized steps for easy problems such as the
ﬁxed pendulum or the simple model of a car. The regularized steps can be advantageous for harder problems
as illustrated in the control of a bicycle model of a car or the pendulum on a cart.

4. For DDP, regularized steps generally outperform directional steps and all other algorithms. An exception is
the control of a pendulum on a cart where DDP with directional steps may suddenly obtain a good solution,
once close enough to the minimum, while DDP with regularized steps may stay stuck.

In Fig. 18, we plot the same algorithms but with respect to time.

1. We observe that in terms of time, the regularized steps may require fewer evaluations during the line-search

as they incorporate previous stepsizes and may provide faster convergence in time.

2. On the other hand, as previously mentioned, by initializing the line-search of the directional steps at 1, we

may observe sudden convergence as illustrated in the control of a pendulum on a cart.

Finally, in Fig. 19, we plot the stepsizes taken by the algorithms for the pendulum and the simple model of a car.
1. On the pendulum example, the stepsizes used by directional steps quickly tend to 1 which means that the
algorithms (GN or DDP-LQ) are then taking the largest possible stepsize for this strategy and may exhibit
quadratic convergence.

2. On the other hand, for the regularized steps, on the pendulum example, the regularization (i.e. the inverse of
the stepsizes) quickly converges to 0, which means that, as the number of iterations increases, the regularized
and directional steps coincide.

3. For the car example, the step sizes for the directional steps never converge exactly to one, which may explain
the slower convergence. For the regularized steps, on the horizons τ = 25 or 50 we observe again that DDP
uses increasingly larger stepsizes which corroborate its performance on this problem.

10.4.2 Quadratic Approximations

In Fig. 17, we compare nonlinear control algorithms with quadratic approximations, i.e., NE or DDP-Q with
different steps.

1. Here the regularized version of the classical oracle, i.e., Newton, rarely outperforms its counterpart with

descent direction.

2. Overall, DDP methods always outperform their NE counterparts.
3. As with the linear quadratic approximations, the DDP approach with directional steps outperforms the
regularized step version for the pendulum on a cart. On the other hand, DDP with regularized steps is better
for the bicycle model of a car and a short horizon of the simple model of a car.

In Fig 21, we plot the same algorithms but in time.

1. Here we observe no particular difference with the plots in iteration, i.e., the search with regularized steps

does not lead to much more favorable line-search time.

2. We observe that DDP-Q is comparable in computational time to DDP-LQ for the ﬁxed pendulum and the
simple model of a car. On the other hand, for the bicycle model of a car, DDP-Q may be much slower.

3. Generally NE does not compare favorably to its linear-quadratic counterpart.

In Fig. 22, we compare the stepsizes taken by the methods.

1. In terms of directional steps, DDP-Q appears to take relatively large steps while its NE counterpart may

have more variations.

2. We observe clearly in these plots that the regularized version of NE is not able to take small regularization
constants, which explains its poor performance. On the other hand, DDP-Q with regularized steps tends to
quickly take small regularizations (large stepsizes).

44

Figure 17: Convergence of algorithms with linear quadratic approximations in iterations.

45

050Iterations10−910−4Costτ=25050Iterations10−910−4τ=50050Iterations10−910−4τ=100SwingingupPendulum050Iterations10−410−1Costτ=25050Iterations10−410−1τ=50050Iterations10−410−1τ=100SwingingupPendulumonaCart050Iterations10−410−1Costτ=25050Iterations10−410−1τ=50050Iterations10−410−1τ=100SimpleModelofCarwithTrackingCost050Iterations10−510−1Costτ=25050Iterations10−610−2τ=50050Iterations10−710−2τ=100BicycleModelofCarwithContouringCostGDGNregGNdirDDP-LQregDDP-LQdirFigure 18: Convergence of algorithms with linear quadratic approximations in time.

46

0.00.51.0Time10−910−4Costτ=2502Time10−910−4τ=50010Time10−910−4τ=100SwingingupPendulum050Time10−410−1Costτ=250200Time10−1τ=500250500Time10−410−1τ=100SwingingupPendulumonaCart050Time10−410−1Costτ=250200400Time10−410−1τ=5005001000Time10−410−1τ=100SimpleModelofCarwithTrackingCost05001000Time10−510−1Costτ=2502000Time10−610−2τ=5005000Time10−710−2τ=100BicycleModelofCarwithContouringCostGDGNregGNdirDDP-LQregDDP-LQdirFigure 19: Stepsizes taken along the iterations for linear quadratic approximations.

47

0101006×10−1Stepsizeτ=250101004×10−16×10−1τ=500101004×10−16×10−1τ=100020Iterations102Stepsize020Iterations104025Iterations106SwingingupPendulum0204010−1Stepsizeτ=250204010−1τ=5005010−1τ=100050Iterations104Stepsize050Iterations103050Iterations103SimpleModelofCarwithTrackingCostGNdirDDP-LQdirGNregDDP-LQregFigure 20: Convergence of algorithms with quadratic approximations in iterations.

48

050Iterations10−910−4Costτ=25050Iterations10−910−4τ=50050Iterations10−910−4τ=100SwingingupPendulum050Iterations10−410−1Costτ=25050Iterations10−410−1τ=50050Iterations10−410−1τ=100SwingingupPendulumonaCart050Iterations10−410−1Costτ=25050Iterations10−410−1τ=50050Iterations10−410−1τ=100SimpleModelofCarwithTrackingCost050Iterations10−910−3Costτ=25050Iterations10−910−3τ=50050Iterations10−1010−4τ=100BicycleModelofCarwithContouringCostNEregNEdirDDP-QregDDP-QdirFigure 21: Convergence of algorithms with quadratic approximations in time.

49

0510Time10−910−4Costτ=2501020Time10−910−4τ=50050Time10−910−4τ=100SwingingupPendulum0100200Time10−1Costτ=250200Time10−410−1τ=5005001000Time10−410−1τ=100SwingingupPendulumonaCart0200Time10−410−1Costτ=250200Time10−410−1τ=5005001000Time10−410−1τ=100SimpleModelofCarwithTrackingCost025005000Time10−910−3Costτ=25010000Time10−910−3τ=5002000040000Time10−1010−4τ=100BicycleModelofCarwithContouringCostNEregNEdirDDP-QregDDP-QdirFigure 22: Stepsizes taken along the iterations for quadratic approximations.

50

05010−1Stepsizeτ=250501008×10−19×10−1τ=500501008×10−19×10−1τ=100050Iterations103Stepsize050Iterations102050Iterations103SwingingupPendulum05010−1Stepsizeτ=2505010−2τ=5005010−1τ=100050Iterations105Stepsize050Iterations104050Iterations105SimpleModelofCarwithTrackingCostNEdirDDP-QdirNEregDDP-QregAcknowledgments. This work was supported by NSF DMS-1839371, DMS-2134012, CCF-2019844, CIFAR-
LMB, NSF TRIPODS II DMS-2023166 and faculty research awards. The authors deeply thank Alexander Liniger
for his help on implementing the bicycle model of a car. The authors also thank Dmitriy Drusvyatskiy, Krishna
Pillutla and John Thickstun for fruitful discussions on the paper and the code.

References

Arrow, K. (1968). Applications of control theory to economic growth.

In Lectures on Applied Mathematics,

volume 12 (Mathematic of the Decision Science, Part 2).

Arutyunov, A. V. and Vinter, R. B. (2004). A simple ‘ﬁnite approximations’ proof of the Pontryagin maximum

principle under reduced differentiability hypotheses. Set-valued analysis, 12(1):5–24.

Bellman, R. (1971). Introduction to the mathematical theory of control processes, volume 2. Academic press.

Bertsekas, D. (1976). Dynamic Programming and Stochastic Control. Academic Press.

Bertsekas, D. (2016). Nonlinear Programming, volume 4. Athena Scientiﬁc.

Betts, J. (2010). Practical methods for optimal control and estimation using nonlinear programming. SIAM.

Boyd, S. and Vandenberghe, L. (1997). Semideﬁnite programming relaxations of non-convex problems in control
In Communications, Computation, Control, and Signal Processing, pages

and combinatorial optimization.
279–287. Springer.

Clarke, F. (1979). Optimal control and the true Hamiltonian. SIAM Review, 21(2):157–166.

Dunn, J. and Bertsekas, D. (1989). Efﬁcient dynamic programming implementations of Newton’s method for

unconstrained optimal control problems. Journal of Optimization Theory and Applications, 63(1):23–38.

Griewank, A. and Walther, A. (2008). Evaluating derivatives: principles and techniques of algorithmic differen-

tiation. SIAM.

Jacobson, D. and Mayne, D. (1970). Differential Dynamic Programming. Elsevier.

Kamien, M. and Schwartz, N. (1971). Sufﬁcient conditions in optimal control theory. Journal of Economic

Theory, 3(2):207–214.

Lewis, A. D. (2006). The maximum principle of Pontryagin in control and in optimal control. Handouts for the

course taught at the Universitat Politecnica de Catalunya.

Li, W. and Todorov, E. (2007). Iterative linearization methods for approximately optimal control and estimation

of non-linear stochastic system. International Journal of Control, 80(9):1439–1453.

Liao, L.-Z. and Shoemaker, C. (1991). Convergence in unconstrained discrete-time differential dynamic program-

ming. IEEE Transactions on Automatic Control, 36(6):692–706.

Liao, L.-Z. and Shoemaker, C. A. (1992). Advantages of differential dynamic programming over Newton’s method

for discrete-time optimal control problems. Technical report, Cornell University.

Liniger, A., Domahidi, A., and Morari, M. (2015). Optimization-based autonomous racing of 1: 43 scale RC cars.

Optimal Control Applications and Methods, 36(5):628–647.

Magdy, M., El Marhomy, A., and Attia, M. A. (2019). Modeling of inverted pendulum system with gravitational

search algorithm optimized controller. Ain Shams Engineering Journal, 10(1):129–149.

Mangasarian, O. (1966). Sufﬁcient conditions for the optimal control of nonlinear systems. SIAM Journal on

Control, 4(1):139–152.

Mayne, D. and Polak, E. (1975). First-order strong variation algorithms for optimal control. Journal of Optimiza-

tion Theory and Applications, 16(3):277–301.

51

Murray, D. and Yakowitz, S. (1984). Differential dynamic programming and Newton’s method for discrete optimal

control problems. Journal of Optimization Theory and Applications, 43(3):395–414.

Nganga, J. and Wensing, P. (2021). Accelerating second-order differential dynamic programming for rigid-body

systems. IEEE Robotics and Automation Letters, 6(4):7659–7666.

Nocedal, J. and Wright, S. (2006). Numerical optimization. Springer Science & Business Media.

Pantoja, J. (1988). Differential dynamic programming and Newton’s method. International Journal of Control,

47(5):1539–1553.

Pontryagin, L., Boltyansky, V., Gamkrelidze, R., and Mischenko, E. (1963). The mathematical theory of optimal

processes. Wiley-Interscience.

Rao, C., Wright, S., and Rawlings, J. (1998). Application of interior-point methods to model predictive control.

Journal of optimization theory and applications, 99(3):723–757.

Roulet, V., Srinivasa, S., Drusvyatskiy, D., and Harchaoui, Z. (2019). Iterative linearized control: stable algorithms
and complexity guarantees. In Proceedings of the 36th International Conference on Machine Learning, pages
5518–5527.

Roulet, V., Srinivasa, S., Fazel, M., and Harchaoui, Z. (2022). Complexity bounds of iterative linear quadratic

optimization algorithms for discrete time nonlinear control. arXiv preprint arXiv:2204.02322.

Sideris, A. and Bobrow, J. (2005). An efﬁcient sequential linear quadratic algorithm for solving nonlinear optimal

control problems. In Proceedings of the 2005 American Control Conference, pages 2275–2280.

Tassa, Y., Erez, T., and Todorov, E. (2012). Synthesis and stabilization of complex behaviors through online
trajectory optimization. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages
4906–4913.

Tassa, Y., Mansard, N., and Todorov, E. (2014). Control-limited differential dynamic programming. In 2014 IEEE

International Conference on Robotics and Automation (ICRA), pages 1168–1175.

Todorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based control. In International

Conference on Intelligent Robots and Systems (IROS), pages 5026–5033. IEEE.

Wright, S. (1990). Solution of discrete-time optimal control problems on parallel computers. Parallel Computing,

16(2-3):221–237.

Wright, S. (1991a). Partitioned dynamic programming for optimal control. SIAM Journal on optimization,

1(4):620–642.

Wright, S. (1991b). Structured interior point methods for optimal control.

In Proceedings of the 30th IEEE

Conference on Decision and Control, pages 1711–1716.

52

A Experimental Details

The code is available at https://github.com/vroulet/ilqc. We add for ease of reference, the hyper-
parameters used for each setting.

Pendulum.

1. mass m = 1,
2. gravitational constant g = 10,
3. length of the blob l = 1,
4. friction coefﬁcient µ = 0.01,
5. speed regularization λ = 0.1,
6. control regularization ρ = 10−6,
7. total time of the movement T = 2, discretization step ∆ = T /τ for varying τ
8. Euler discretization scheme.

Pendulum on a cart.

1. mass of the rod m = 0.2,
2. mass of the cart M = 0.5,
3. viscous coefﬁcient b = 0.1,
4. moment of inertia I = 0.006,
5. length of the rod 0.3,
6. speed regularization λ1 = 0.1,
7. barrier parameter ρ2 = 1.,
8. control regularization ρ = 10−6,
9. total time of the movement T = 2.5, discretization step ∆ = T /τ for varying τ ,
10. stay put time ¯t = τ
0.6/∆
− (cid:98)
11. barriers ¯z+ = 2, ¯z− =
2,
12. Euler discretization scheme.

−

(cid:99)

,

Simple car with tracking cost.
1. length of the car L = 1,
2. reference speed vref = 3,
3. initial speed vinit = 1,
4. control regularization λ = 10−6,
5. total time of the movement T = 2,
6. simple track,
7. Euler discretization scheme.

Bicycle model of a car with a contouring objective.

·

10−6

1. Cm1 = 0.287, Cm2 = 0.0545,
2. Cr0 = 0.0518, Crd = 0.00035,
3. Br = 3.3852, Cr = 1.2691, Dr = 0.1737, lr = 0.033
4. Bf = 2.579, Cf = 1.2, Df = 0.192, lf = 0.029
5. m = 0.041, Iz = 27.8
6. contouring error penalty ρc = 0.1,
7. lagging error penalty ρl = 10,
8. reference speed penalty ρv = 0.1,
9. barrier error penalty ρb = 100,
10. reference speed vref = 3,
11. initial speed vinit = 1,
12. control regularization λ = 10−6,
13. total time of the movement T = 1,
14. simple track,
15. Runge-Kutta discretization scheme.

53

