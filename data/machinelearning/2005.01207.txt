0
2
0
2

y
a
M
3

]
E
N
.
s
c
[

1
v
7
0
2
1
0
.
5
0
0
2
:
v
i
X
r
a

Obtaining Basic Algebra Formulas with Genetic Programming and Functional
Rewriting

EDWIN CAMILO CUBIDES, Universidad Nacional de Colombia
JONATAN GOM ´EZ, Universidad Nacional de Colombia

In this paper we develop a set of genetic programming operators and an initialization population process based on concepts of

functional programming rewriting for boosting inductive genetic programming. Such genetic operators are used with in a hybrid

adaptive evolutionary algorithm that evolves operator rates at the same time it evolves the solution. Solutions are represented

using recursive functions where genome is encoded as an ordered list of trees and phenotype is written in a simple functional

programming language that uses rewriting as operational semantic (computational model). The ﬁtness is the number of examples

successfully deduced over the cardinal of the set of examples. Parents are selected following a tournament selection mechanism

and the next population is obtained following a steady state strategy. The evolutionary process can use some previous functions

(programs) induced as background knowledge. We compare the performance of our technique in a set of hard problems (for classical

genetic programming). In particular, we take as test-bed the problem of obtaining equivalent algebraic expressions of some notable
products (such as square of a binomial, and cube of a binomial), and the recursive formulas of sum of the ﬁrst n and squares of the
ﬁrst n natural numbers.

CCS Concepts: •Theory of computation → Equational logic and rewriting; Inductive inference; •Software and its engineer-

ing → Genetic programming;

General Terms: Genetic Programming, Inductive Learning

Additional Key Words and Phrases: Genetic programming, Functional programming, Inductive functional programming, Adaptive

genetic operators

ACM Reference format:

Edwin Camilo Cubides and Jonatan Gom´ez. 2016. Obtaining Basic Algebra Formulas with Genetic Programming and Functional

Rewriting. 1, 1, Article 1 (January 2016), 15 pages.

DOI: 10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION

Nowadays the Machine Learning area has been deeply and extensively studied [14]. The Machine Learning is the

Artiﬁcial Intelligence area that use inductive learning and deals with the problem of generating a model or hypothesis

(learn a task) to approximate a function well over a suﬃciently large set of training examples, and automatically to

improve this model to explain other unobserved examples if it is enough robust [13, 14]. The majority of problem that

Machine Learning tries to solve are diﬃcult, mainly due to their complex nature and their representation.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or
to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions from permissions@acm.org.

© 2016 ACM. Manuscript submitted to ACM

Manuscript submitted to ACM

1

 
 
 
 
 
 
2

E. C. Cubides & J. Gom´ez

In particular, a problem of this kind is the automatic programming problem, which can be deﬁned as the automatic

process to obtain a computer program with some mechanism and where humans intervention is low [6]. It is very

complicated because its representation depends on the target programming language. Mainly, there are two types of

automatic programming: Deductive where the program is generated from a high-level description, and Inductive where

the program is generated from a set of examples (pairs input/output).

Genetic Programming (GP) [8–11] is an inductive programming technique and a type of soft computing technique.

In GP, programs are represented using some data structure (usually trees). Basically, programs are randomly generated

and evolved by applying some genetic operators

Inductive Programming (IP) is an inductive technique [15–19] and a type of hard computing technique. In IP pro-

grams are represented using some logic language. The inductive method is very interesting since the theoretical point

of view, but is unacceptable since the computational point of view, because this method grows exponentially in time

and space [5]. Genetic Inductive Programming (GIP) [20–22] is an inductive Machine Learning technique that uses the

formal representation and non deterministic search, and its goal is to combine GP and IP to reduce the search space

and the required time to run the algorithm.

Given dataset of examples in form of pair input/output, it is interesting describes those examples as a function

(expression) that represent the set in a compact format, as application of techniques described previously we present

some examples of concrete cases of arithmetic pairs and use an evolutionary algorithm proposed for ﬁnding solution as

abstract algebraic expressions that describe all examples as an unify theory, the solution found for the dataset presented

could be associated to notable product of elemental algebra.

This paper is organized as follows. Section 2 presents the basic concepts and theoretic fundaments about term

rewriting system and functional programming. Section 3 describes the hybrid adaptive evolutionary algorithm that

adapts the operator rates while it is solving the optimization problem. Section 4 the evolutionary algorithm proposed

to evolve functional programs is described. Section 5 explain the conﬁguration of the experiments that was done and

the result are analyzed. Finally, we presented the conclusions obtained from the experiments done.

2 TERM REWRITING SYSTEM AND

FUNCTIONAL PROGRAMMING

Concepts as Equational Logic and Term Rewriting Systems are necessary to understand the theory behind of the

learning induction of programs written on a declarative language. A short introduction to these concepts is done in

this section (a full explanation can be found in [2]).

Let V be a countable set of variables. Let Σ denotes a non-empty, ﬁnite set of function symbols or signature, each
of which has a ﬁxed associated arity, it is deﬁned as a mapping from Σ into N such that arity(f ) = n, where n is the
number of parameters of f , we often write f /n to denote that arity(f ) = n. If arity(f ) = 0 then the function symbol
f is called a constant (it is supposed that Σ contains at least one constant). The expression T (Σ, V) denotes the set of
terms built from Σ and V. The set of variables occurring in a term t is denoted Var (t). A term t is a ground term if
V ar (t) = œ. A fresh variable is a variable that appears nowhere else.

An occurrence u in a term t is represented by a sequence of natural numbers. These sequences belong to the language
given by the regular expression {Λ} ∪ {n(.m)∗ : n,m ∈ N+}. O(t) denotes the set of occurrences and O(t) denotes the
set of non-variables occurrences of t. t |u denotes the subterm of t in the occurrence u. t[u] denotes the leftmost symbol
Manuscript submitted to ACM

Obtaining Basic Algebra Formulas with Genetic Programming and Functional Rewriting

3

of the subterm of t in the occurrence u. t[s]u denotes the replacement of the subterm of t in the occurrence u by the

term s.

A substitution σ is deﬁned as a mapping from the set of variables V into the set of terms T (Σ, V). If it is necessary, a
substitution can be extended as an homomorphism from T (Σ, V) into T (Σ, V), and it is applied only on the variables
of the term. A substitution is usually represented by σ = {x1/t1, . . . , xn /tn } where σ (xi ) = ti , xi , i = 1, . . . , n. The
identity substitution id is the empty substitution { }. A ground substitution is a substitution where all ti are ground
terms. A term s is an instance of a term t if there exist a substitution σ such that s = σ (t). The composition of two
substitutions δ and σ is denoted by (δ ◦σ )(t) and it is done as the composition between functions, i.e. (δ ◦σ )(t) = δ (σ (t)).
The expression σ ≤ θ means that there exist a substitution δ such that θ = δ ◦ σ . A substitution σ is an uniﬁer of two
term t and s if σ (t) = σ (s). σ is a most general uniﬁer (mgu) if it is an uniﬁer and σ ≤ θ for any another uniﬁer θ.

A equation or rewriting rule is an expression l = r , where l and r are terms, l is called the left-hand side (lhs), of the
equation, and r is the right-hand side (rhs). A Term Rewriting System (TRS) is a ﬁnite set of rewriting rules. A functional
program P is a TRS in which every rule satisﬁes Var (r ) ⊆ Var (l). Given an equation, an orphan variable x in the
equation is a variable such that x ∈ Var (r ) and x < Var (l).

For TRS R, r ≪ R denotes that r is a new variant of a rule in R such that r contains only fresh variables, i.e. contains
no variable previously met during computation. Given a TRS R, we assume that the signature Σ is partitioned into two
disjoint sets Σ = C ⊎ F , where F = { f : f (t1, . . . , tn ) = r ∈ R} and C = Σ \ F . Symbols in C are called constructors
and symbols in F are called deﬁned functions. The elements of T (C, V) are called constructor terms. A constructor
substitution σ = {x1/t1, . . . , xn /tn } is a substitution such that each ti , i = 1, . . . , n is a constructor term. A term is
linear if it does not contains multiple occurrences of the same variable. A TRS is left-linear if the left-hand sides of all
rules are linear terms. A pattern is a term of the form f (c1, . . . , cn ) where f ∈ F and c1, . . . , cn are constructor terms.
We say that a TRS is construct-based (CB) is the left-hand sides of R are patterns.

Given a TRS R, t

u,l = r, σ
−−−−−−−−→R s is a rewrite step if there exist an occurrence u of t, a rule l = r ∈ R and a substitution
σ such that t |u = σ (l) and s = t[σ (r )]u . A term t is said to be in normal form with respect to (w.r.t) R if there exists

u,l = r, σ
−−−−−−−−→R s. An equation t = s is normalized w.r.t R if t and s are in normal form. R is said
not a term s such that t
to be terminating or is noetheriano if there exists not an inﬁnite rewrite steps chain t1 →R t2 →R t3 →R · · · . R is
R t2 and t1 →∗
said to be conﬂuent if for all t1, t2, t3 ∈ T (F , V) with t1 →∗
R t3, then there exists t ∈ T (F , V) such
that t2 →∗
R t. R is said to be canonical if the binary one-step rewriting relation →R is terminating and
conﬂuent.

R t and t3 →∗

We have deﬁned the lexical, the syntax and the semantic of a simple functional language with evaluation strategy

eager without sorts called. The syntax is similar to Maude [12] for specifying arithmetic operations and Prolog [7]
for the notation of list. The language has as constructors the value 0 (cardinal of the empty set), the unary function s
successor (s : N → N+
: n 7→ s(n) = n + 1), the notation for list is [H |T ] where H is the head of the list and T is the
tail of the list, the empty list is denotes by the string [], into the system a list is denoted as the binary function •(H ,T )
(dot operator), the name of the functions are non-empty sequences of small letters and the variables are non-empty

sequences of capital letters.

3 GENETIC PROGRAMMING (GP)

Genetic Programming is a branch of Genetic Algorithms, in which individuals (chromosomes) of population are com-

puter programs [8–11]. The main diﬀerence between GP and traditional genetic algorithms is that the representation of

Manuscript submitted to ACM

4

E. C. Cubides & J. Gom´ez

the solution. GP creates computer programs in some particular programming language to represent a solution, whereas

genetic algorithms create a string of numbers that represent the solution. The goal of the GP is to ﬁnd a program that

solves a problem such that its analytic solution is very complicated to ﬁnd. The ﬁtness of a valid individual is generally

obtained by the performance and the behavior of it over a training data sets. As any program can be represented as a

tree or a set of trees (genotype), the programs (chromosomes) usually are represented as data structures (trees), these

trees are obtained doing parsing over sentences (strings) of a program (phenotype), from where GP is applied over a

particular domain (programming language). The ﬁtness of a valid individual is generally obtained by the performance

and the behavior of it over a training data sets.

3.1 Functions and Terminals in GP

The set of terminals and functions is the most important component of the GP. The set of terminals and functions is the

alphabet of the programs that are build over the language programming. The variables and constants of the programs

belong to set of terminals.

3.2 Representation of programs

As any program can be represented as a tree or a set of trees (genotype), the programs (chromosomes) usually are

represented as data structures (trees), these trees are obtained doing parsing over sentences (strings) of a program

(phenotype), from where GP is applied over a particular domain (programming language).

3.3 Generation of initial populations

As the mutation is almost always lethal, it is very restricted, then, the diversity in the generations is obtained only into

the initial population, the parameter to build individuals is the maximum depth of the branches of the trees, represented

by l. The main methods are:

Full: Length of all branches equal to l > 0.
Grow: Length of all branches of depth less than or equal to l > 0.
Ramped (half-and-half): The individuals are created with the full or grow methods where trees are of iterative

depth 1, 2, 3, . . . , l.

3.4 Genetic Programming Operators

Crossover: given a couple of programs, a subtree of each program is randomly selected and these subtrees are

swapped.

Mutation: given a program, a subtree of this program is randomly selected and it is replaced by a new randomly

generated tree.

4 HYBRID ADAPTIVE EVOLUTIONARY

ALGORITHM (HAEA)

Parameter adaptation techniques tried to eliminate the parameter setting process by adapting parameters through the

algorithm’s execution. Parameter adaptation techniques can be roughly divided into centralized control techniques

(central learning rule), decentralized control techniques, and hybrid control techniques. In the centralized learning rule

approach, genetic operator rates (such as mutation rate, crossover rate, etc.) are adapted according to a global learning

Manuscript submitted to ACM

Obtaining Basic Algebra Formulas with Genetic Programming and Functional Rewriting

5

rule that takes into account the operator productivities through generations (iterations). Generally, only one operator

is applied per generation, and it is selected based on its productivity. The productivity of an operator is measured in

terms of good individuals produced by the operator. A good individual is one that improves the ﬁtness measure of

the current population. If an operator generates a higher number of good individuals than other operators then its

probability is rewarded in another case this one is punished.

In decentralized control strategies, genetic operator rates are encoded in the individual and are subject to the evolu-

tionary process. Accordingly, genetic operator rates can be encoded as an array of real values in the semi-open interval

[0.0,1.0), with the constraint that the sum of these values must be equal to one. Since the operator rates are encoded

as real numbers, special genetic operators, meta-operators, are applied to adapt or evolve them.

In [3, 4] was proposed an evolutionary algorithm that adapts the operator rates while it is solving the optimization
problem. The Hybrid Adaptive Evolutionary Algorithm (HaEa) is a mixture of ideas borrowed from Evolutionary
Strategies, decentralized control adaptation, and central control adaptation.

In HaEa, each individual is “independently” evolved from the other individuals of the population, as in evolutionary
strategies. In each generation, every individual selects only one operator from the set of possible operators. Such

operator is selected according to the operator rates encoded into the individual. When a non-unary operator is applied,

additional parents (the individual being evolved is considered a parent) are chosen according to any selection strategy.
As can be noticed, HaEa does not generate a parent population from which the next generation is totally produced.
Among the oﬀspring produced by the genetic operator, only one individual is chosen as child, and will take the place of
its parent in the next population. In order to be able to preserve good individuals through evolution, HaEa compares
the parent individual against the oﬀspring generated by the operator. The best selection mechanism will determine the

individual (parent or oﬀspring) that has the highest ﬁtness. Therefore, an individual is preserved through evolution if

it is better than all the possible individuals generated by applying the genetic operators.

The genetic operator rates are encoded into the individual in the same way as decentralized control adaptation.
These probabilities are initialized with values following a standard uniform distribution U (0, 1). A roulette selection
scheme is used to select the operator to be applied. To do this, the operator rates are normalized in such a way that

their summation is equal to one.

The performance of the child is compared against its parent performance in order to determine the productivity of

the operator. The operator is rewarded if the child is better than the parent and punished if it is worst. The magnitude

of reward/punishment is deﬁned by a learning rate that is randomly generated. Finally, operator rates are recomputed,

normalized, and assigned to the individual that will be copied to the next population. The learning rate is generated

in a random fashion instead of setting it to a speciﬁc value.

5 GENETIC INDUCTIVE FUNCTIONAL

PROGRAMMING

The evolutionary algorithm proposed takes as entry a ﬁnite set of concrete examples (pairs input/output) of the function

(functional program) to induce, the dataset is divided into two sets, the ﬁrst is the positive basic examples E

+

(or the

training set), second is the positive extra examples E

++

(or the validation set) and optionality a program as background

knowledge, to continuation we present the evolutionary algorithm proposed.

Manuscript submitted to ACM

6

E. C. Cubides & J. Gom´ez

5.1 Representation of Individuals

In our evolutionary algorithm individuals or genomes are ordered lists of trees, these lists are in genotype space and

the list of equations represented by these trees are in the phenotype space.

Example 5.1. Following list of equations represents an individual of the evolutionary algorithm.

sum(N,0) = N

sum(N,s(M)) = s(sum(N,M))

prod(N,0) = 0

prod(N,s(M)) = sum(prod(N,M),N)

double(0) = 0

double(s(N)) = s(s(double(N)))

triple(0) = 0

triple(s(N)) = s(s(s(triple(N))))

square(0) = 0

square(s(N)) = sum(square(N),sum(s(N),N))

cube(0) = 0

cube(s(N)) = s(sum(cube(N),triple(sum(square(N),N))))

5.2 Generation of the Initial Population

From set of concrete examples is possible to obtain an initial population using a process of generalization to each

example. Generalization and restricted generalization concepts are deﬁned to continuation

Deﬁnition 5.2. (Generalization) Given a ground term t, s is a generalization of t, if there exists a ground replacement

σ such that σ (s) ≡ t.

Deﬁnition 5.3. (Restricted Generalization) Given a ground term t, s is a restricted generalization of t, if s is a gener-

alization of t, and s[Λ] ≡ =, i.e. s ≡ X = Y , s[1] is a function non-constant and ∀x ∈ Var (Y ), x ∈ Var (X ).

Initial population is obtained building the set of all restricted generalizations of each concrete example. The restricted

generalization in our algorithm are obtained replacement each subterm of an example by variables in all possible

forms. Individuals in our algorithm is composed by two disjoint set, the ﬁrst to obtain the basic equations (base cases)

and the second to obtain the recursive equations, those basic and recursive sets are initially composed by restricted

generalizations.

Example 5.4. Given the example square_bino(0,0) = 0, in Table 1 are presented all generalizations of this equa-
tion, after we delete the generalizations such that lhs is not a function or there exists variables in rhs that are not int

lhs, in Table 2 are presented all restricted generalizations of the equation, this set is used as initial population of the

evolutionary algorithm proposed.

As the number of restricted generalizations are limited, then the initial population can have duplicate individuals

to establish a minimum size of the population.

Manuscript submitted to ACM

Obtaining Basic Algebra Formulas with Genetic Programming and Functional Rewriting

7

No.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

Generalization

A = B
A = 0
square_bino(A,B) = C
square_bino(A,B) = B
square_bino(A,B) = A
square_bino(A,B) = 0
square_bino(A,A) = B
square_bino(A,A) = A
square_bino(A,A) = 0
square_bino(A,0) = B
square_bino(A,0) = A
square_bino(A,0) = 0
square_bino(0,A) = B
square_bino(0,A) = A
square_bino(0,A) = 0
square_bino(0,0) = A
square_bino(0,0) = 0

Table 1. Generalizations of the example
square bino(0,0) = 0.

No. Restricted Generalization

4
5
6
8
9
11
12
14
15
17

square_bino(A,B) = B
square_bino(A,B) = A
square_bino(A,B) = 0
square_bino(A,A) = A
square_bino(A,A) = 0
square_bino(A,0) = A
square_bino(A,0) = 0
square_bino(0,A) = A
square_bino(0,A) = 0
square_bino(0,0) = 0

Table 2. Restricted generalizations of the example square bino(0,0) = 0.

5.3 Genetic Operators

In this Section we present a set of nine operators, three binaries and six unaries, two of these operators are the classical

operators of genetic programming, another operators are built in such a way that these preserve the syntax of programs

induced.

5.3.1 Global XOver Operator. The global xover operator is a binary operator, that randomly selects an equation

from each individual, and exchanges them (at random positions).

Example 5.5. Given two individuals {p1, p2} deﬁned as follows

p1 = {sum n(N) = N, sum n(s(N)) = sum(N,sum n(N))}
p2 = {sum n(s(N)) = s(sum(N,sum n(N))), sum n(0) = 1}

Manuscript submitted to ACM

8

E. C. Cubides & J. Gom´ez

the global xover operator selects two equations, in this example, equations sum n(s(N)) = sum(N,sum n(N)) of p1 and
sum n(s(N)) = s(sum(N,sum n(N))) of p2, exchanges both of them and obtains the following new individuals

p ′
1
p ′
2

= {sum n(s(N)) = s(sum(N,sum n(N))), sum n(N) = N, }

= {sum n(s(N)) = sum(N,sum n(N)), sum n(0) = 1}

5.3.2 Global Swap Operator. The global swap operator is an unary operator, that takes an individual and swaps two

randomly selected equations if it is possible (if the individual has at least two equations). Otherwise, the individual

remains the same.

Example 5.6. Given an individual p deﬁned as follows

p

=

{sum n(N) = N, sum n(s(N)) = sum(s(N),sum n(N))}

the global swap operator selects two equations of p, in this example, equations sum n(s(N)) = sum(s(N),sum n(N))
and sum n(N) = N, and swaps both of them obtaining following new individual.

p ′

=

{sum n(s(N)) = sum(s(N),sum n(N)), sum n(N) = N}

5.3.3

Internal Swap Operator. The internal swap operator is an unary operator, that randomly selects an equation,

in it selects a random function, and then two diﬀerent parameters are also randomly selected if it is possible (if the

function has arity greater than or equals to two), and these parameters are swapped. Otherwise, the individual remains

the same.

Example 5.7. Given an individual p deﬁned as follows

p

=

{prod(N,0) = 0, prod(s(M),N) = sum(N,prod(N,M)) }

internal

the
equation
operator
prod(s(M),N) = sum(N,prod(N,M)) of p, and in it selects function prod at position 1, and swaps both parameters
(s(M) and N) obtaining following new individual

selects

swap

the

p ′

=

{prod(N,0) = 0, prod(N,s(M)) = sum(N,prod(N,M)) }

5.3.4 Equalization Operator. The equalization operator is a binary operator, that randomly selects an equation from

each individual, the ﬁrst one is called the receptor equation and the second one is called the emitter equation. Constants

on lhs of the receptor equation are replaced by variables of the receptor equation. Variables of the rhs of the receptor

equation are replaced by variables on lhs of the receptor equation, and constants follow the same process but including

a random element, i.e. replaced with a probability of 0.5. This full process is applied to the emitter equation as well.

if subterms of the rhs of each equation can unify, then one of this unify term of the rhs of one of those equations is

replaced by the lhs of the another equation, if the rhs of the new equation has variables that are not present in the lhs,

then these variables are replaced by randomly selected variables from the lhs.

Example 5.8. Given two individuals {p1, p2} deﬁned as follows

p1 = {sum n(s(A)) = sum(s(A),A)}
p2 = {sum n(A) = A}

Manuscript submitted to ACM

Obtaining Basic Algebra Formulas with Genetic Programming and Functional Rewriting

9

the equalization operator selects a random equation of each individual sum n(s(A)) = sum(s(A),A) of p1 and
sum n(A) = A of p2, the ﬁrst one is the receptor equation and the second one is the equation emitter, obtains equation
sum n(N) = N replacing in the emitter equation all variables by fresh variables. Subterms of the rhs of the recep-
tor equation (sum(s(A),A), s(A), A, A) are unifying with subterms of the rhs of the new emitter equation (N) and

replaces all subterms of the rhs of the receptor equation by the lhs of the new emitter equation obtaining following

new equations

sum n(s(A)) = sum n(N)

sum n(s(A)) = sum(sum n(N),A)

sum n(s(A)) = sum(s(sum n(N)),A)

sum n(s(A)) = sum(s(A),sum n(N))

the operator replaces variables of rhs of the new equations by random variables on lhs obtaining following new

equations

sum n(s(A)) = sum n(A)

sum n(s(A)) = sum(sum n(A),A)

sum n(s(A)) = sum(s(sum n(A)),A)

sum n(s(A)) = sum(s(A),sum n(A))

builds a new individual joins equations of initial individuals and randomly inserts last new equations in this new

individual obtaining following new individuals

p ′
1

p ′
2

p ′
3

p ′
4

= {sum n(s(A)) = sum(s(A),A), sum n(A) = A,

= {sum n(s(A)) = sum(s(A),A), sum n(A) = A,

= {sum n(s(A)) = sum(s(A),A), sum n(A) = A,

= {sum n(s(A)) = sum(s(A),A), sum n(A) = A,

sum n(s(A)) = sum n(A)}

sum n(s(A)) = sum(sum n(A),A)}

sum n(s(A)) = sum(s(sum n(A)),A)}

sum n(s(A)) = sum(s(A),sum n(A))}

This operator can generate recursive equations of new individuals.

5.3.5 Composition Operator. Composition operator is a binary operator. This operator works as equalization oper-

ator, the diﬀerence is that one of the programs must be the background knowledge.

5.3.6

Functional Swap Operator. The functional swap operator is an unary operator, that randomly selects an equa-
tion, in it selects a random function symbol (f ′) diﬀerent to function symbol at position 1 if it is possible (if the equation
Manuscript submitted to ACM

10

E. C. Cubides & J. Gom´ez

has at least two functional calling), and searches the set of function symbols diﬀerent to function symbol at position
1 with the same arity of the function symbol f ′, if the set is not empty, the operator selects a new random function
symbol (f ′′) of the set and swaps function symbols f ′ and f ′′. Otherwise, the individual remains the same.

Example 5.9. Given an individual p deﬁned as follows

p

=

{prod(N,0) = 0, prod(s(M),N) = prod(N,sum(N,M)) }

functional

the
equation
operator
prod(s(M),N) = prod(N,sum(N,M)) of p, in it selects function symbol sum at position 2.2, obtains the set {prod}
of function symbols of arity two that are diﬀerent of the function symbol at position 1, into this set selects the function
symbol prod, and swaps function symbols sum and prod obtaining following new individual

select

swap

can

p ′

=

{prod(N,0) = 0, prod(s(M),N) = sum(N,prod(N,M)) }

5.3.7

Functional Rename Operator. The functional rename operator is an unary operator, that randomly selects an

equation, in it selects a random function, if the arity of this one is greater than two and there is an unary function

as parameter of the function selected, the operator selects a parameter diﬀerent to the unary function and maps this

function to another parameter and replaces the unary function by its single parameter. Otherwise, the individual

remains the same.

Example 5.10. Given an individual p deﬁned as follows

p = {sum(N,0) = N, sum(s(N),M) = s(sum(N,M))}

functional

the
equation
sum(s(N),M) = s(sum(N,M)) of p, in it selects function symbol sum at position 1, since the successor function is
unary, maps to the another parameter the successor function and replaces this successor function by its parameter (N)
obtaining following new individual

operator

rename

select

can

p ′ = {sum(N,0) = N, sum(N,s(M)) = s(sum(N,M))}

5.3.8 XOver GP Operator.

5.3.9 Mutation GP Operator.

5.4 Selection of Parents

In our evolutionary algorithm we use the hybrid adaptive evolutionary algorithm (HaEa) that evolves the solutions,
this algorithm in each iteration evolves all individuals of the population, and each evolution selects one operator for

each individual; if this operator uses another individual to evolve, then randomly are selected four individuals from

the population and a tournament selection mechanism is performance with them and winner of this process is selected

as the additional individual to use with the operator.

5.5 Replacement Strategy

To obtain next population in our evolutionary algorithm we following a steady state strategy thus, if any child is better

than his/her parent, this child replacement the parent in next population and the operator that generates the child is

Manuscript submitted to ACM

Obtaining Basic Algebra Formulas with Genetic Programming and Functional Rewriting

11

rewarded, else if parent is better than or equal to child then parent is added to the next population but the operator

that generates children is punished.

5.6 Fitness Function

Fitness of individuals is calculated using the covering of these individuals, it is deﬁned as follows

Deﬁnition 5.11. (Covering) The covering (Cov) of a individual (program) p is deﬁned as the set of positive basic and

positive extra examples that p can deduce, in others words

and the covering factor (CovF

+

) is deﬁned as the proportion of positive basic and positive extra examples that p can

Cov(p) = (cid:8)e ∈ E

+

++

∪ E

: p |= e(cid:9),

deduce, i.e.

CovF

+

(p) = |Cov(p)|
|E+ ∪ E++|

Covering factor is a function from the set of programs to the interval [0, 1].
From previous deﬁnitions ﬁtness function of an individual p is calculated as the covering factor of p, and therefore

the goal of the evolutionary algorithm is to maximize that function.

6 EXPERIMENTS AND RESULTS

Using the evolutionary algorithm previously described, we try to solve seven problems that consist in to obtain alge-

braic expressions from arithmetic concrete examples given in form of pairs input/output, Our dataset is divided into

two sets, the ﬁrst is the positive basic examples (or the training set) and second is the positive extra examples (or the

validation set).

6.1 Global Configuration

In Table 4 are presented the parameters used to performance the evolutionary algorithm, as some individuals could be

non-terminant programs, it is necessary to limit the number of nodes of each equations, the number of rewrite steps

and the number of reducible expression that is searched in each rewrite steps. In Table 5 are presented all dataset and

the background knowledge of each problem proposed to solve.

Parameter
Minimum size of the population
Number of experiments
Maximum number of nodes of each equation
Maximum number of rewriting steps
Maximum number of searches of reducible expressions
Maximum depth of the branches of the individuals

Value
500
100
30
500
500
2

Table 3. Configuration proposed of the parameters of the evolutionary algorithm GP.

Manuscript submitted to ACM

12

E. C. Cubides & J. Gom´ez

Parameter
Minimum size of the population
Number of experiments
Maximum number of iterations of the HaEa algorithm
Maximum number of basic equations
Maximum number of recursive equations
Maximum number of nodes of each equation
Maximum number of rewriting steps
Maximum number of searches of reducible expressions

Value
500
100
100
3
3
30
500
500

Table 4. Configuration proposed of the parameters of the evolutionary algorithm HaEa.

Description
Problem

Positive Basic
Examples E+
cube_bino(0,0) = 0

Positive Extra
Examples E++

cube_bino(1,0) = 1, cube_bino(0,1) = 1,
cube_bino(1,1) = 8, cube_bino(2,0) = 8,
cube_bino(0,2) = 8
cube(2) = 8, cube(3) = 27

cube(0) = 0
cube(1) = 1
square_bino(0,0) = 0

square_bino(1,0) = 1, square_bino(0,1) = 1,
square_bino(1,1) = 4, square_bino(2,1) = 9,
square_bino(2,2) = 16, square_bino(3,1) = 16,
square_bino(2,3) = 25, square_bino(3,2) = 25
square(2) = 4, square(3) = 9,
square(4) = 16, square(5) = 25

square(0) = 0
square(1) = 1
square_trino(0,0,0) = 0 square_trino(0,1,1) = 4, square_trino(1,0,1) = 4,
square_trino(1,1,0) = 4, square_trino(2,0,0) = 4,
square_trino(0,2,0) = 4, square_trino(0,0,2) = 4,
square_trino(1,1,1) = 9, square_trino(2,1,1) = 16,
square_trino(1,2,1) = 16, square_trino(1,1,2) = 16
sum_n(2) = 3, sum_n(3) = 6, sum_n(4) = 10

sum_n(0) = 0
sum_n(1) = 1
sum_n_square(0) = 0
sum_n_square(1) = 1

sum_n_square(2) = 5, sum_n_square(3) = 14,
sum_n_square(4) = 30

sum, double,
square

Background
Knowledge

sum, prod,
triple, square,
cube
sum, triple,
square
sum, prod,
double, square

sum, prod,
triple
sum, prod,
double, square

sum

Cube of a binomial

Cube of the successor
of a natural number

Square of a binomial

Square of the successor
of a natural number

Square of a trinomial

Sum of the of the ﬁrst
n natural numbers
Sum of the squares
of the ﬁrst n natural
numbers

Table 5. Descriptions of the functions to induce with those dataset and background knowledge.

6.2 Global Results

Using dataset of each problem presented previously, and the conﬁguration of parameters deﬁned in Table ??, we

obtained following results, where each stacked bar chart represents the total of experiments, subarea with diagonal

lines represents the proportion of successful experiments and empty area represents the proportion of fail experiments.

In Table 6 are presented some of solutions found using the evolutionary algorithm proposed, each solution is inter-

preted as mathematical expression, thus those expressions will be equivalents using transitivity property of equality

of natural numbers, and so to establish some notable products.

Manuscript submitted to ACM

Obtaining Basic Algebra Formulas with Genetic Programming and Functional Rewriting

13

Description
Problem

Cube of a
binomial

Cube of the
successor
of a natural
number

Square of a
binomial

Square of the
successor
of a natural
number

Square of a
Trinomial

Sum of the
of the ﬁrst
n natural
numbers

Sum of the
squares
of the ﬁrst
n natural
numbers

Solution
(Program)

cube_bino(A,B) = cube(sum(A,B))
cube_bino(A,B) =
sum(prod(sum(sum(prod(A,A),prod(B,B)),

sum(prod(B,A),prod(A,B))),B),

prod(A,sum(sum(prod(B,A),prod(B,A)),

sum(prod(B,B),prod(A,A)))))

cube(0) = 0;
cube(s(A)) = sum(triple(sum(square(A),A)),s(cube(A)))
cube(s(A)) =
sum(sum(sum(triple(square(A)),s(A)),sum(A,cube(A))),A);
cube(A) = A
square_bino(A,B) = square(sum(B,A))
square_bino(A,B) =
sum(sum(prod(A,A),double(prod(A,B))),prod(B,B))
square(s(A)) = sum(square(A),s(double(A)));
square(0) = 0
square(0) = 0;
square(s(A)) = sum(sum(A,square(A)),s(A))
square_trino(A,B,C) = square(sum(B,sum(C,A)))
square_trino(A,B,C) =
sum(prod(sum(C,A),sum(B,sum(sum(B,C),A))),prod(B,B))

sum_n(0) = 0; sum_n(s(A)) = s(sum(sum_n(A),A))

sum_n(s(A)) = sum(s(A),sum_n(A)); sum_n(A) = A

sum_n_square(s(A)) =
sum(sum(square(A),A),sum(s(sum_n_square(A)),A));
sum_n_square(A) = A
sum_n_square(s(A)) =
sum(sum(sum_n_square(A),s(square(A))),sum(A,A));
sum_n_square(0) = 0

Equivalent
Mathematical Expression

cube bino(A, B) = (A + B)3
cube bino(A, B) =
((AA + BB) +
(BA + AB))B +
A((BA + BA) +
(BB + AA))
cube(0) = 0;
cube(A + 1) = 3(A2 + A) + (A3 + 1)
cube(A + 1) =
(cid:0) (cid:0)3A2 + (A + 1)(cid:1) + (cid:0)A + A3(cid:1)(cid:1) + A;
cube(A) = A
square bino(A, B) = (B + A)2

square bino(A, B) = (AA + 2AB) + BB

square(A + 1) = A2 + (2A + 1);
square(0) = 0
square(0) = 0;
square(A + 1) = (cid:0)A + A2(cid:1) + (A + 1)
square trino(A, B, C) = (B + (C + A))2
square trino(A, B, C) =
(C + A)(B + ((B + C) + A)) + BB

0
n=0 n = 0;
A+1
A
n=0 n = (cid:0) Í
n=0 n + A(cid:1) + 1
A+1
A
n=0 n = (cid:0)(A + 1) + Í
n=0 n(cid:1);
A
n=A n = 0
A+1
n=0 n2 = (cid:0)A2 + A(cid:1) + (cid:0)(cid:0) Í
A
n=A n2 = 0

Í
Í

Í
Í

Í
Í

A
n=0 n2 + 1(cid:1) + A(cid:1);

A+1
A
n=0 n2 = (cid:0) Í
n=0 n2 + (cid:0)A2 + 1(cid:1)(cid:1) + (A + A);
0
n=0 n2 = 0

Í
Í

Table 6. Examples of the programs solutions and those respective algebraic expressions found using the evolutionary algorithm from
dataset and background knowledge presented in Table 5.

7 CONCLUSIONS

In this paper we presented an evolutionary algorithm that permit to obtain equivalent algebraic expressions of some

notable products and summations, these expressions were obtained using an inductive process from sets of concrete

examples (pairs input/output), the algebraic expressions was obtained from arithmetic equalities and not as usually

is done from a deductive method from high-level description using Field Axioms for the real numbers [1] as is done

usually.

Manuscript submitted to ACM

14

E. C. Cubides & J. Gom´ez

Problem/Program ✓ ✗

Summary

cube-bino

cube

square-bino

square

square-trino

sum-n

96

4

0 100

81

19

0 100

28

1

72

99

sum-n-square

0 100

Table 7. GP = 7h-2m-35s

Problem/Program ✓ ✗

Summary

cube-bino

cube

square-bino

square

square-trino

sum-n

100

0

8 92

100

99

97

100

0

1

3

0

sum-n-square

69 31

Table 8. HaEa = 10h-56m-54s

96.0%

0.0%

81.0%

0.0%

28.0%

1.0%

0.0%

100.0%

8.0%

100.0%

99.0%

97.0%

100.0%

69.0%

%
100

90

80

70

60

50

40

30

20

10

0

⋆

⋆

Classical GP Algorithm

HaEa Algorithm

Fig. 1. Box-and-whisker diagram: .

It is important to mention that solutions found are correct with respect to each dataset, is to said, solution can

deduce all examples presented, but also could deduce values that do not belong to desired function to induce, so our

evolutionary algorithm is correct with respect to the set of fact presented.

The background knowledge is a factor very important in the induction of interesting programs, since if the back-

ground knowledge contains a lot information (functions) the algorithm prefers to obtain sort solutions, and in this

case those kind of solution are the trivial solutions.

The quality and quantity of the examples presented determine the expression obtained, since if to the algorithm is

presented few examples, it could ﬁnd solutions faster, but these solutions usually are not the solution that we desired.

Manuscript submitted to ACM

Obtaining Basic Algebra Formulas with Genetic Programming and Functional Rewriting

15

For example, if in the problem of the cube of a successor of a natural number is deleted the example cube_succe(4) = 64
the solution found is incorrect, on the contrary, if a lot examples are added then to ﬁnd a solution is very hard.

The dataset presented in Table 5 could be used as benchmark functions to prove another algorithms that are used

to obtain representation of theories as algebraic expressions, since we prove that with these dataset and background

knowledges it is possible to generate the function that explains the dataset with algebraic expressions.

REFERENCES

[1] T. Apostol. Calculus, Vol. 1: One-Variable Calculus, with an Introduction to Linear Algebra. Wiley India Pvt. Limited, 2nd edition, 2007.
[2] F. Baader and T. Nipkow. Term Rewriting and All That. Cambridge University Press, New York, NY, USA, 1998.
[3] J. Gomez. Self Adaptation of Operator Rates for Multimodal Optimization. In CEC2004 Congress on Evolutionary Computation, volume 2, pages

1720–1726, 2004.

[4] J. Gomez. Self Adaptation of Operator Rates in Evolutionary Algorithms. In Genetic and Evolutionary Computation - GECCO 2004, Part I, volume

3102 of Lecture Notes in Computer Science, pages 1162–1173. Springer, 2004.

[5] J. Hernandez-Orallo and M. J. Ramirez-Quintana. Inverse narrowing for the inductive inference of functional logic programs. 2011.
[6] K. Igwe and N. Pillay. Automatic Programming using Genetic Programming. In 2013 Third World Congress on Information and Communication

Technologies (WICT 2013), pages 337–342, Dec 2013.

[7] P. Iranzo and M. Frasnedo. Programaci´on l´ogica: teor´ıa y pr´actica. Pearson Educacin, 2007.
[8] J. R. Koza. Genetic Programming: On the Programming of Computers by Means of Natural Selection. Cambridge, MA: MIT Press, 1992.
[9] J. R. Koza. Genetic Programming II: Automatic Discovery of Reusable Programs. MIT Press, 1994.
[10] J. R. Koza, F. H. Bennett, D. Andre, and M. A. Keane. Genetic Programming III: Darwinian Invention and Problem Solving. Morgan Kaufmann, 1999.
[11] J. R. Koza, M. A. Keane, M. J. Streeter, W. Mydlowec, J. Yu, and G. Lanza. Genetic Programming IV: Routine Human-Competitive Machine Intelligence.

Kluwer Academic Publishers, 2003.

[12] P. L. M. Clavel, S. Eker and J. Meseguer. Principles of Maude. In J. Meseguer, editor, Electronic Notes in Theoretical Computer Science, volume 4.

Elsevier Science Publishers, 2000.

[13] D. Michie, D. J. Spiegelhalter, and C. C. Taylor. Machine Learning, Neural and Statistical Classiﬁcation, February 1994.
[14] T. M. Mitchell. Machine Learning. McGraw-Hill, Inc., New York, NY, USA, 1st edition, 1997.
[15] S. Muggleton, editor. Proceedings of the First International Workshop on Inductive Logic Programming, Viano de Castelo, Portugal, 1991.
[16] S. Muggleton. Inductive Logic Programming. Academic Press Limited, London, 1992.
[17] G. D. Plotkin. A Note on Inductive Generalization. Machine Intelligence, 6, 1970.
[18] G. D. Plotkin. Authomatic Methods of Inductive Inference. Ph. D., Science of Computer, Edimburgh University, 1971.
[19] E. Shapiro. Inductive Inference of Theories from Facts. Computational Logic: Essays in Honor of Alan Robinson. MIT Press, 1991.
[20] A. Tveit. Genetic inductive logic programming. M.Sc. Thesis, Norwegian University of Science and Technology, IDI/NTNU, N-7491 Trondheim,

Norway, 1997.

[21] A. Varˇsek. Inductive Logic Programming with Genetic Algorithms. PhD thesis, Faculty of Electrical Engineering and Computer Science, University

of Ljubljana, Ljubljana, Slovenia, 1993. (In Slovenian).

[22] M. L. Wong and K. S. Leung. Inductive logic programming using genetic algorithms. In J. W. Brahan and G. E. Lasker, editors, Advances in Artiﬁcial

Intelligence - Theory and Application II, pages 119–124. I.I.A.S., Ontario, Canada, 1994.

Manuscript submitted to ACM

