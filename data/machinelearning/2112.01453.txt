1
2
0
2

c
e
D
2

]

G
L
.
s
c
[

1
v
3
5
4
1
0
.
2
1
1
2
:
v
i
X
r
a

Target Propagation via Regularized Inversion

Vincent Roulet & Zaid Harchaoui
Department of Statistics
University of Washington

December 3, 2021

Abstract

Target Propagation (TP) algorithms compute targets instead of gradients along neural networks and propagate
them backward in a way that is similar yet different than gradient back-propagation (BP). The idea was ﬁrst presented
as a perturbative alternative to back-propagation that may improve gradient evaluation accuracy when training multi-
layer neural networks (Le Cun et al., 1988). However, TP may have remained more of a template algorithm with
many variations than a well-identiﬁed algorithm. Revisiting insights of Le Cun et al. (1988) and more recently of Lee
et al. (2015), we present a simple version of target propagation based on a regularized inversion of network layers,
easily implementable in a differentiable programming framework. We compare its computational complexity to the
one of BP and delineate the regimes in which TP can be attractive compared to BP. We show how our TP can be used
to train recurrent neural networks with long sequences on various sequence modeling problems. The experimental
results underscore the importance of regularization in TP in practice.

1 Introduction

Target propagation algorithms can be seen as perturbative learning alternatives to the gradient back-propagation algo-
rithm, where virtual targets are propagated backward instead of gradients (Le Cun, 1986; Le Cun et al., 1988; Rohwer,
1989; Mirowski and LeCun, 2009; Bengio, 2014; Goodfellow et al., 2016). A high-level summary is presented in
Fig. 1: while gradient back-propagation considers storing intermediate gradients in a forward pass, target propagation
algorithms proceed by computing and storing approximate inverses. The approximate inverses are then passed on
backward along the graph of computations to ﬁnally yield a weight update for stochastic learning.

Target propagation aims to take advantage of the availability of approximate inverses to compute better descent
directions for the objective at hand. Bengio et al. (2013); Bengio (2020) argued that the approach could be relevant
for problems involving multiple compositions such as the training of Recurrent Neural Networks (RNNs), which
generally suffer from the phenomenon of exploding or vanishing gradients (Hochreiter, 1998; Bengio et al., 1994;
Schmidhuber, 1992). Recently, empirical results indeed showed the potential advantages of target propagation over
classical gradient back-propagation for training RNNs on several tasks (Manchev and Spratling, 2020). However,
these recent investigations remain built on multiple approximations, which hinder the analysis of the core idea of TP,
i.e., using layer inverses.

On the theoretical side, difference target propagation, a modern variant of target propagation, was related to an ap-
proximate Gauss-Newton method, suggesting interesting venues to explain the beneﬁts of target propagation (Bengio,
2020; Meulemans et al., 2020). Previous works have considered approximating inverses by adding multiple reverse
layers (Manchev and Spratling, 2020; Meulemans et al., 2020; Bengio, 2020). However, it is unclear whether such
reverse layers actually learn layer inverses during the training process. Even if they were, the additional cost of
computational complexity of learning approximate inverses should be carefully accounted for.

In this work, we propose a simple target propagation approach, revisiting the original insights of Le Cun et al.
(1988) on the critical importance of the good conditioning of layer inverses. We deﬁne regularized inverses through a
variational formulation and we obtain approximate inverses via these regularized inverses. In this spirit, we can also
interpret the difference target propagation formula (Lee et al., 2015) as a ﬁnite difference approximation of a linearized

1

 
 
 
 
 
 
Fig. 1: Our implementation of target propagation uses linearization of gradient inverses instead of gradients in a backward pass
akin to gradient back-propagation.

regularized inverse. We propose a smoother formula that can directly be integrated into a differentiable programming
framework.

We detail the computational complexity of the proposed target propagation and compare it to the one of gradient
back-propagation, showing that the additional cost of computing inverses can be effectively amortized for very long se-
quences. Following the benchmark of Manchev and Spratling (2020), we observe that the proposed target propagation
can perform better than classical gradient-based methods on several tasks involving RNNs.

The code to reproduce the experiments is provided at https://github.com/vroulet/tpri. The appendix
details the implementations of the algorithms and discuss previous interpretations of target propagation as a Gauss-
Newton method.

Related work. Many variations of back-propagation algorithms have been explored; see Werbos (1994); Goodfel-
low et al. (2016) for an extensive bibliography. Closer to target propagation, penalized formulations of the training
problem have been considered to decouple the optimization of the weights in a distributed way or using an ADMM
approach (Carreira-Perpinan and Wang, 2014; Taylor et al., 2016; Gotmare et al., 2018). Rather than modifying the
backward operations in the layers, one can also modify the weight updates for deep forward networks by using a reg-
ularized inverse (Frerix et al., 2018). Wiseman et al. (2017) recast target propagation as an ADMM-like algorithm for
language modeling and reported disappointing experimental results. Recently, in a careful experimental benchmark
evaluation, Manchev and Spratling (2020) explored further target propagation to train RNNs, mapping a sequence to a
single ﬁnal output, in an attempt to understand the beneﬁts of target propagation to capture long-range dependencies,
and obtained promising experimental results. Another line of research has considered synthetic gradients that approxi-
mate gradients using an additional layer instead of using back-propagated gradients (Jaderberg et al., 2017; Czarnecki
et al., 2017) to speed up the training of deep neural networks. Recently, Ahmad et al. (2020); Dalm et al. (2021)
considered using analytical inverses to implement target propagation and blend it with what they called a gradient-
adjusted incremental formula. Yet, an additional orthogonality penalty is critical for their approach to work. Recently,
Meulemans et al. (2020) considered using as many reverse layers as forwarding operations. We focus here on the
optimization gains of using target propagation that cannot be obtained by adding a prohibitive number of reverse lay-
ers. Finally, we do not discuss the biological plausibility of TP since we are unable to comment on this. We refer the
interested reader to, e.g., (Bengio, 2020).

Notations. For f : Rp
→
(cid:1)
as ∂xf (x, y) = (cid:0)∂f j(x, y)/∂xi

Rq

×

Rd×p.

i,j ∈

Rd, we denote the partial derivative of f w.r.t. x

Rp on a point (x, y)

Rp

Rq

×

∈

∈

2 Target Propagation with Linearized Regularized Inverses

While target propagation was initially developed for multi-layer neural networks, we focus on its implementation for
recurrent neural networks, as we shall follow the benchmark of Manchev and Spratling (2020) in the experiments.
Recurrent Neural Networks (RNNs) are also a canonical family of neural networks in which interesting phenomena
arise in back-propagation algorithms.

2

Gradient Back-PropagationTarget PropagationForward PassBackward PassProblem setting. A simple RNN parameterized by θ = (Whh, Wxh, bh, Why, by) maps a sequence of inputs x1:τ =
(x1, . . . , xτ ) to an output ˆy = gθ(x1:τ ) by computing hidden states ht

Rp corresponding to the inputs xt.

Formally, the output ˆy and the hidden states ht are computed as an output operation following transition operations

∈

deﬁned as

ˆy = cθ(hτ ) := s(Whyhτ + by),
ht = fθ,t(ht−1) := a(Wxhxt + Whhht−1 + bh)

for t

1, . . . , τ

,

}

∈ {

where s is, e.g., the soft-max function for classiﬁcation tasks, a is a non-linear operation such as the hyperbolic tangent
function, and the initial hidden state is generally ﬁxed as h0 = 0. Given samples of sequence-output pairs (x1:τ , y),
the RNN is trained to minimize the error (cid:96)(y, gθ(x1:τ )) of predicting ˆy = gθ(x1:τ ) instead of y.

As one considers longer sequences, RNNs face the challenge of exploding/vanishing gradients ∂gθ(x1:τ )/∂ht (Ben-
gio and Frasconi, 1995); see Appendix A for more discussion. We acknowledge that speciﬁc parameterization-based
strategies have been proposed to address this issue of exploding/vanishing gradients, such as orthonormal parame-
terizations of the weights (Arjovsky et al., 2016; Helfrich et al., 2018; Lezcano-Casado and Martınez-Rubio, 2019).
The focus here is to simplify and understand target propagation as a backpropagation-type algorithm using RNNs as
a workbench. Indeed, training RNNs is an optimization problem involving multiple compositions for which approxi-
mate inverses can easily be available. The framework could also be potentially applied to, e.g., time-series or control
models (Roulet et al., 2019).

Given the parameters Whh, Wxh, bh of the transition operations, we can get approximate inverses of fθ,t(ht−1)
for all t
, that yield optimization surrogates that can be better performing than the ones corresponding to
}
regular gradients. We present below a simple version of target propagation based on regularized inverses and inverse
linearizations.

1, . . . , τ

∈ {

Back-propagating targets. The idea of target propagation is to compute virtual targets vt for each layer t = τ, . . . , 1
such that if the layers were able to match their corresponding target at time t, i.e., fθ,t(ht−1)
vt, the objective would
decrease. The ﬁnal target vτ is computed as a gradient step on the loss w.r.t. hτ . The targets are then back-propagated
using an approximate inverse1 f −1

θ,t of fθ,t at each time step.

≈

Formally, consider an RNN that computed τ states h1, . . . , hτ from a sequence x1, . . . , xτ with associated output

y. For a given stepsize γh > 0, we propose to back-propagate targets by computing

vτ = hτ

γh∂h(cid:96)(y, cθ(hτ )),

−
vt−1 = ht−1 + ∂hf −1

(cid:62)

θ,t (ht)

(vt

ht),

for t

τ, . . . , 1
}

∈ {

−

.

(1)

(2)

The update rule (2) blends two ideas: i) regularized inversion; ii) linear approximation. We shall describe below
that our update (2) allows us to interpret the “magic formula” of difference target propagation in Eq. 15 of Lee et al.
(2015) as 0th-order ﬁnite difference approximation, while ours is a 1st-order linear approximation. We shall also show
that (2) puts in practice an insight from Bengio (2020) suggesting to use the inverse of the gradients in the spirit of a
Gauss-Newton method.

Once all targets are computed, the parameters of the transition operations are updated such that the outputs of fθ,t
at each time step move closer to the given target. Formally, the update consists of a gradient step with stepsize γθ on
the squared error between the targets and the current outputs, i.e., for θh

Whh, Wxh, bh

,

∈ {

}

∂θh (cid:107)
As for the parameters θy = (Why, by) of the output operation, they are updated by a simple gradient step on the loss
with a stepsize γθ.

fθ,t(ht−1)

(3)

γθ

t=1

vt

−

−

(cid:107)

θnext
h = θh

2
2/2.

τ
(cid:88)

1In the following, to ease the presentation, we abuse notations and denote approximate inverses by f −1
θ,t .

3

2.1 Regularized Inversion

To explore further the original idea of Le Cun et al. (1988), we consider using the variational deﬁnition of the inverse,

f −1
θ,t (vt) = argmin

vt−1∈Rp (cid:107)

fθ,t(vt−1)

vt

2
2 = argmin
(cid:107)

vt−1∈Rp (cid:107)

−

a(Wxhxt + Whhvt−1 + bh)

vt

2
2.

(cid:107)

−

(4)

As long as vt belongs to the image fθ,t(Rp) of fθ,t, this deﬁnition recovers exactly the inverse of vt by fθ,t. More
fθ,t(Rp), Eq. (4) computes the best approximation of the inverse in the sense of the Euclidean
generally, if vt
projection. When one considers an activation function a and θh = (Whh, Wxh, bh), the solution of (4) can easily be
computed.

(cid:54)∈

Formally, for the sigmoid, the hyperbolic tangent or the ReLU, their inverse can be obtained analytically for any
a(Rp). So for vt

hh(a−1(vt)
a(Rp), the minimizer of (4) is obtained by ﬁrst projecting vt onto a(Rp), before inverting the linear
operation. To account for non-invertible matrices Whh, we also add a regularization in the computation of the inverse.
Overall we consider approximating the inverse of the layer by a regularized inverse of the form

hhWhh)−1W (cid:62)

Wxhxt

If vt

bh).

−

−

(cid:54)∈

∈

a(Rp) and Whh full rank, we get
f −1
θ,t (vt) = (W (cid:62)

vt

∈

f −1
θ,t (vt) = (W (cid:62)
with r > 0 and π a projection onto a(Rp).

hhWhh + r I)−1W (cid:62)

hh(a−1(π(vt))

Wxhxt

bh),

−

−

Regularized inversion vs. parameterized inversion. Bengio (2014); Manchev and Spratling (2020) parameterize
the inverse as a reverse layer such that
f −1
θ,t (vt) = ψθ(cid:48),t(vt) := a(Wxhxt + V vt + c),
and learn the parameters θ(cid:48) = (V, c) for this reverse layer to approximate the inverse of the forward computations.
The parameterized layer needs to be learned to get a good approximation which involves numerically solving an
optimization problem for each layer. These optimization problems come with a computational cost that can be better
controlled by using regularized inversions presented earlier.

However, the approach based on parameterized inverses may lack theoretical grounding, as pointed out by Bengio
(2020), as we do not know how close the learned inverse is to the actual inverse throughout the training process. In
contrast, the regularized inversion (4) is less ad hoc and clearly deﬁned and, as we shall show in the experiments, leads
to competitive performance on real datasets.

In any case, the analytic formulation of the inverse gives simple insights on an approach with parameterized
inverses. Namely, the analytical formula suggests parameterizing the reverse layer s.t. (i) the reverse activation is
deﬁned as the inverse of the activation and not any activation, (ii) the layer uses a non-linear operation followed by a
linear one instead of the usual scheme, i.e., a linear operation followed by a non-linear one.

2.2 Linearized Inversion

Earlier instances of target propagation used direct inverses of the network layers such that the target propagation
update formula would read vt−1 = f −1
θ,t (vt) in (2). Yet, we are unaware of a successful implementation of TP using
directly the inverses. To circumvent this issue, Lee et al. (2015) proposed the difference target propagation formula
that back-propagates the targets as

vt−1 = ht−1 + f −1
If the inverses were exact, the difference target propagation formula would reduce to vt−1 = f −1
θ,t (vt). Lee et al.
(2015) introduced the difference target propagation formula to mitigate the approximation error of the inverses by
parameterized layers. The difference target propagation formula can naturally be interpreted as an approximation of
the linearization used in (2), as

f −1
θ,t (ht).

θ,t (vt)

−

where ∂hf −1

θ,t (ht)

(cid:62)

f −1
θ,t (vt)

−

θ,t (ht) = ∂hf −1
f −1

θ,t (ht)

(cid:62)

(vt

vt
ht) + O(
(cid:107)

−

−

ht

2
2),
(cid:107)

denotes the Jacobian of the inverse of the layer at ht.

4

Fig. 2: The graph of computations of target propagation is the same as the one of gradient back-propagation except that f −1 needs
to be computed and Jacobian of the inverses, ∂hf −1 (cid:62) are used instead of gradients ∂hf in the transition operations.

We show in Appendix D that the ﬁrst-order approximation we propose (2) leads to slightly better training curves
than the ﬁnite-difference approximation. Moreover, our interpretation illuminates the “mystery” of this formula, which
appeared to be critical to the success of target propagation.

3 Gradient Back-propagation versus Target Propagation

Graph of computations. Gradient back-propagation and target propagation both compute a descent direction for
the objective at hand. The difference lies in the oracles computed and stored in the forward pass, while the graph of
computations remains the same. To clarify this view, we reformulate target propagation in terms of displacements
λt := vt

ht such that Eq. (1), (2) and (3) read

−

λτ =

dθh =

γh∂h(cid:96)(y, cθ(hτ )),
−
τ
(cid:88)

∂θh fθ,t(ht−1)λt,

t=1

λt−1 = ∂hf −1

θ,t (ht)

(cid:62)

λt,

for t

τ, . . . , 1
}

∈ {

,

θnext
h = θh + γhdθh.

Target propagation amounts then to computing a descent direction dθh for the parameters θh with a graph of compu-
tations, illustrated in Fig. 2, analogous to that of gradient-back-propagation illustrated in Appendix A. The difference
lies in the use of the Jacobian of the inverse
∂hf −1

instead of ∂hfθ,t(ht−1).

(cid:62)

θ,t (ht)

The implementation of TP with the formula (2) can be done in a differentiable programming framework, where, rather
than computing the gradient of the layer, one evaluates the inverse and keep the Jacobian of the inverse. With the
precise graph of computation of TP and BP, we can compare their computational complexity explicitly and bound the
difference of the directions they output.

Arithmetic complexity. Clearly, the space complexities of gradient back-propagation (BP) and our implementation
of target propagation (TP) are the same since the Jacobians of the inverse, and the original gradients have the same
size. In terms of time complexity, TP appears at ﬁrst glance to introduce an important overhead since it requires
the computation of some inverses. However, a close inspection of the formula of the regularized inverse reveals that
a matrix inversion needs to be computed only once for all time steps. Therefore the cost of the inversion may be
amortized if the length of the sequence is particularly long.

5

...... ............ Formally, the time complexity of the forward-backward pass of gradient back-propagation is essentially driven by

matrix-vector products, i.e.,




T
(cid:124)

τ
(cid:88)

t=1

BP =

T

(fθ,t) +

T

(∂hfθ,t) +
(cid:123)(cid:122)
Forward

T

(∂θh fθ,t)
(cid:125)

+

T
(cid:124)

(∂hfθ,t(ht−1)) +
T
(cid:123)(cid:122)
Backward

(∂θfθ,t(ht−1))

(cid:125)



τ (dp + p2 + pq) + τ (p2 + pq),

≈

where d is the dimension of the input xt, q is the dimension of the parameters θh, for a function f we denote by
the time complexity to evaluate f and we consider e.g. ∂θfθ,t(ht−1)) as the linear function λ

T
∂θfθ,t(ht−1))λ.

(f )

On the other hand, the time complexity of target propagation is




T
(cid:124)

τ
(cid:88)

t=1

TP =

T

(fθ,t)+
T

(f −1

θ,t )+
(∂θhfθ,t) +
T
T
(cid:123)(cid:122)
Forward

(∂hf −1
θ,t )
(cid:125)

+

T
(cid:124)



(∂hf −1

θ,t )(ht)(cid:62))+
T
(cid:123)(cid:122)
Backward


(∂θfθ,t(ht−1))

(cid:125)

→

(f −1

θ,t ),

+

P

(f −1

P

hhWhh + r I)−1W (cid:62)

θ,t ) is the cost of encoding the inverse, which, in our case, amounts to the cost of encoding gθ : z

where
→
(W (cid:62)
Wxhxt + bh).
Encoding g comes at the cost of inverting one matrix of size p. Therefore, the time-complexity of target propagation
can be estimated as

hh, such that our regularized inverse can be computed as f −1

θ,t (vt) = gθ(a−1(vt)

−

TP

T

≈

p3 + τ (dp + p2 + pq) + τ (p2 + pq)

BP

≈ T

if τ

p,

≥

i.e., for long sequences whose length is larger than the dimension of the hidden states, the cost of TP with regularized
inverses is approximately the same as the cost of BP. If a parameterized inverse was used rather than a regularized
inverse, the cost of encoding the inverse would correspond to the cost of updating the reverse layers by, e.g., a stochastic
gradient descent. This update has a cost similar to BP. However, it is unclear whether these updates get us close to the
actual inverses.

Bounding the difference between target propagation and gradient back-propagation. As the computational
graphs of BP and TP are the same, we can bound the difference between the oracles returned by both methods. First,
note that the updates of the parameters of the output functions are the same since, in TP, gradients steps of the loss are
used to update these parameters. The difference between TP and BP lies in the updates with respect to the parameters
of the transition operations. For BP, the updates are computed by chain rule as

∂θh (cid:96) (y, gθ(x1:τ )) =

τ
(cid:88)

t=1

∂θh fθ,t(ht−1)

∂hτ
∂ht

∂h(cid:96)(y, cθ(hτ )),

where the term ∂hτ /∂ht decomposes along the time steps as ∂hτ /∂ht = (cid:81)τ
computed by TP has the same structure, namely it can be decomposed for γh = 1 as

s=t+1 ∂hfθ,s(hs−1). The direction

dθ =

τ
(cid:88)

t=1

∂θhfθ,t(ht−1)

ˆ∂hτ
ˆ∂ht

∂h(cid:96)(y, cθ(hτ )),

where ˆ∂hτ / ˆ∂ht = (cid:81)τ
as, for any matrix norm

s=t+1 ∂hf −1

θ,s (hs)

(cid:107) · (cid:107)

as formally stated in the following lemma.

(cid:62)

. We can then bound the difference between the directions given by BP or TP

Lemma 3.1. The difference between the oracle returned by gradient back-propagation ∂θh(cid:96) (y, gθ(x1:τ )) and the
oracle returned by target propagation can be bounded as

∂θh (cid:96) (y, gθ(x1:τ ))
(cid:107)

−

dθ

(cid:107) ≤

c

sup
t=1,...,τ (cid:107)

∂hfθ,t(ht−1)

∂hf −1

θ,t (ht)

(cid:62)

,

(cid:107)

−

where c = (cid:80)τ

t=1

(cid:80)t−1

s=0 asbt−1−s with a = supt=1,...τ (cid:107)

∂hfθ,t(ht−1)

, b = supt=1,...τ (cid:107)
(cid:107)

∂hf −1

θ,t (ht)

(cid:62)

.

(cid:107)

6

Fig. 3: Temporal order problem T = 60, Temporal Problem T = 120, Adding problem T = 30.

For regularized inverses, we have, denoting ut = Wxhxt + Whhht−1 + bh,

(cid:16)
hh(cid:107)

(cid:62)

−

W (cid:62)

∂hf −1

(cid:107) ≤ (cid:107)

a(ut)

θ,t (ht)

∂hfθ,t(ht−1)
(cid:107)
For the two oracles to be close, we then need the preactivation ut = Wxhxt + Whhht−1 + bh to lie in the region
hhWhh + r I)−1 to be close
of the activation function that is close to being linear s.t.
∇
to the identity which can be the case if, e.g., r = 0 and the weight matrices Wh were orthonormal. By initializing the
weight matrices as orthonormal matrices, the differences between the two oracles can be closer. However, in the long
term, target propagation appears to give better oracles, as shown in the experiments below.

I. We also need (W (cid:62)

a(ut)

(cid:107)(cid:107)∇

− ∇

(cid:107)∇

hhWhh + r I)−1

a(ut)−1

a(ut)−1

(W (cid:62)

+

≈

−

(cid:107)

(cid:107)

(cid:107)

I

.

(cid:17)

Target propagation as a Gauss-Newton method? Recently target propagation has been interpreted as an approxi-
mate Gauss-Newton method, by considering that the difference target propagation formula approximates the lineariza-
tion of the inverse, which itself is a priori equal to the inverse of the gradients (Bengio, 2020; Meulemans et al., 2020;
2021). Namely, provided that f −1

I, we have

θ,t (fθ,t(ht−1))

ht−1 such that ∂hfθ,t(ht−1)∂hf −1
(∂hfθ,t(ht−1))−1 .

≈
∂hf −1
θ,t (ht)

θ,t (ht)

≈

≈

y

−

f1(x)
◦

2, a Gauss-Newton update would take the form x(k+1) = x(k)
2
(cid:107)

By composing the inverses of the gradients, we get an update similar to the one of Gauss-Newton (GN) method.
fn
Namely, recall that if n invertible functions f1, . . . , fn were composed to solve a least square problem of the form
◦
∂x0f1(x0)−(cid:62) . . . ∂xn−1f (xn−1)−(cid:62)(xn
. . .
−
y), where xt is deﬁned iteratively as x0 = x(k), xt+1 = ft(xt). In other words, GN and TP share the idea of compos-
ing the inverse of gradients. However, numerous differences remain as detailed in Appendix C. Note that even if TP
was approximating GN, it is unclear whether GN updates are adapted to stochastic problems. In any case, by using
an analytical formula for the inverse, we can test this interpretation by using non-regularized inverses, which would
amount to directly use the inverses as in a GN method. If the success of TP could be explained by its interpretation as
a GN method, we should observe efﬁcient training curves when no regularization is added.

−

(cid:107)

7

024Iterations×104020004000TrainLoss024Iterations×104020004000012Iterations×104050100024Iterations×1040255075100Accuracy024Iterations×1040255075100012Iterations×1040255075100TPBPFig. 4: Image classiﬁcation pixel by pixel. From left to right: MNIST, MNIST with permuted images, CIFAR10, FashionMNIST
with GRU.

4 Experiments

In the following, we compare our simple target propagation approach, which we shall refer to as TP, to gradient Back-
Propagation referred to as BP. We follow the experimental benchmark of Manchev and Spratling (2020) to which we
add results on RNNs on CIFAR and GRUs on FashionMNIST. Additional experimental, details on the initialization
and the hyper-parameter selection can be found in Appendix D.

Data. We consider two synthetic datasets generated to present training difﬁculties for RNNs and several real datasets
consisting of scanning images pixel by pixel to classify them (Hochreiter and Schmidhuber, 1997; Le et al., 2015;
Manchev and Spratling, 2020).

Temporal order problem. A sequence of length T is generated using a set of randomly chosen symbols

a, b, c, d
.
}
{
[4T /10, 5T /10]. The network
XX, XY, Y X, Y Y

.
}

∈
{

Two additional symbols X and Y are added at positions t1
[T /10, 2T /10] and t2
must predict the correct order of appearance of X and Y out of four possible choices

∈

Adding problem. The input consists of two sequences: one is made of randomly chosen numbers from [0, 1], and
[T /10, T /2]. The second
the other one is a binary sequence full of zeros except at positions t1
position acts as a marker for the time steps t1 and t2. The goal of the network is to output the mean of the two random
numbers of the ﬁrst sequence (Xt1 + Xt2)/2.

[1, T /10] and t2

∈

∈

Image classiﬁcation pixel by pixel. The inputs are images of (i) grayscale handwritten digits given in the database
MNIST (LeCun and Cortes, 1998), (ii) colored objects from the database CIFAR10 (Krizhevsky, 2009) or (iii)
grayscale images of clothes from the database FashionMNIST (Xiao et al., 2017). The images are scanned pixel
by pixel and channel by channel for CIFAR10, and fed to a sequential network such as a simple RNN or a GRU
network (Cho et al., 2014). The inputs are then sequences of 28
28 = 784 pixels for MNIST or FashionMNIST
×
3 = 3072 pixels for CIFAR with a very long-range dependency problem. We also consider permuting
and 32
the images of MNIST by a ﬁxed permutation before feeding them into the network, which gives potentially longer
dependencies in the sequential data.

32

×

×

Model.
In both synthetic settings, we consider randomly generated mini-batches of size 20, a simple RNN with
hidden states of dimension 100, and hyperbolic tangent activation. For the temporal order problem, the last layer uses
a soft-max function on top of a linear operation, and the loss is the cross-entropy. For the adding problem, the last
layer is linear, the loss is the mean-squared error, and a sample is considered to be accurately predicted if the mean
squared error is less than 0.04 as done by (Manchev and Spratling, 2020).

8

024Iterations×1042000400060008000TrainLoss024Iterations×10420004000600080000.02.55.07.5Iterations×1037000710072000.02.55.07.5Iterations×103600070008000024Iterations×1040255075100Accuracy024Iterations×10402550751000.02.55.07.5Iterations×103010200.02.55.07.5Iterations×103010203040TPBP(6a) Conv. w.r.t. stepsize & regularization

(6b) Perf. vs width & length.

For the classiﬁcation of images with sequential networks, we consider mini-batches of size 16 and a cross-entropy
loss. For MNIST and CIFAR, we consider a simple RNN with hidden states of dimension 100, hyperbolic tangent
activation, and a softmax output. For FashionMNIST, we consider a GRU network and adapted our implementation of
target propagation to that case while using hidden states of dimension 100 and a softmax output.

Target propagation can tackle long sequences better than gradient back-propagation.
In Fig. 3, we observe that
TP performs better than BP on the temporal ordering problem: it is able to reach 100% accuracy in fewer iterations
than BP for sequences of length 60 and, for sequences of length 120, it is still able to reach 100% accuracy in fewer
than 40 000 iterations while BP is not. On the other hand, for the adding problem, TP performs less well than BP. The
contrast in performance between the two synthetic tasks was also observed by (Manchev and Spratling, 2020) using
difference target propagation with parameterized inverses. The main difference between these tasks is the different
nature of the outputs, which are binary for the temporal problem and continuous for the adding problem.

In Fig. 4, we observe that TP generally performs better than BP for image classiﬁcation tasks. For the MNIST
104 iterations. This phenomenon is also observed with permuted
dataset, it reaches around 74% accuracy after 4
·
104 iterations and
images, where the optimization appears smoother, and TP obtains around 86% accuracy after 4
is still faster than BP. On the CIFAR dataset, no algorithms appear to reach a signiﬁcant accuracy, though TP is still
faster. On the FashionMNIST dataset, where a GRU network is used, our implementation of TP performs on par with
BP, which shows that our approach can be generalized to more complex networks than a simple RNN.

·

Target propagation requires a non-zero regularization term. As mentioned in Sec. 3, by using an analytical
formula to compute the inverse of the layers, we can question the interpretation of TP as a Gauss-Newton method,
which would amount to TP without regularization. To understand the effect of the regularization term, we computed
the area under the training loss curve of TP for 400 iterations on a log10 grid of varying step-sizes γθ and regularizations
r for a ﬁxed γh = 10−3. The results are presented in Fig. 6a, where the smaller the area, the brighter the point and the
absence of dots in the grid mean that the algorithm diverged. Fig. 6a shows that without regularization we were not
able to obtain convergence of the algorithm. Simply using the gradients of the inverse as in a Gauss-Newton method
may not directly work for RNNs. Additional modiﬁcations of the method could be added to make target propagation
closer to Gauss-Newton, such as inverting the layers with respect to their parameters as proposed by Bengio (2020).
For now, the regularization appears to successfully handle the rationale of target propagation.

Target propagation is adapted for long sequences.
In Fig. 6b, we compare the performance of BP and TP in terms
of accuracy after 400 iterations on the MNIST problem for various widths determined by the size of the hidden states
and various lengths determined by the size of the inputs (i.e., we feed the RNN with k pixels at a time, which gives
a length 784/k). Fig 6b shows that TP is generally appropriate for long sequences, while BP remains more efﬁcient
for short sequences. TP can then be seen as an interesting alternative for dynamical problems which involve many
discretization steps as in RNNs and related architectures.

9

Region of parameters  with convergence 41449196392784Length163264128256512WidthBPTPConclusion

We proposed a simple target propagation approach grounded in two important computational components, regularized
inversion, and linearized propagation. The proposed approach also sheds light on previous insights and successful
rules for target propagation. The code is available to facilitate the reproduction of the results. We have used target
propagation within a stochastic gradient outer loop to train neural networks for a fair comparison to stochastic gradient
using gradient backpropagation. Developing adaptive stochastic gradient algorithms in the spirit of Adam that lead
to boosts in performance when using target propagation instead of gradient backpropagation is an interesting avenue
for future work. Continuous counterparts of target propagation in a neural ODE spirit is also an interesting avenue for
future work.

Acknowledgments. This work was supported by NSF CCF-1740551, NSF DMS-1839371, the CIFAR program
“Learning in Machines and Brains”, and faculty research awards. We thank Nikolay Manchev for all the details he
provided on his code.

References

Nasir Ahmad, Marcel A van Gerven, and Luca Ambrogioni. Gait-prop: A biologically plausible learning rule derived

from backpropagation of error. Advances in Neural Information Processing Systems, 33, 2020.

Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In Proceedings of the

33rd International Conference on Machine Learning, 2016.

Yoshua Bengio. How auto-encoders could provide credit assignment in deep networks via target propagation. arXiv

preprint arXiv:1407.7906, 2014.

Yoshua Bengio. Deriving differential target propagation from iterating approximate inverses.

arXiv preprint

arXiv:2007.15139, 2020.

Yoshua Bengio and Paolo Frasconi. Diffusion of context and credit information in markovian models. Journal of

Artiﬁcial Intelligence Research, 3:249–270, 1995.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difﬁ-

cult. IEEE transactions on neural networks, 5(2):157–166, 1994.

Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic

neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

Miguel Carreira-Perpinan and Weiran Wang. Distributed optimization of deeply nested systems. In Proceedings of

the 17th International Conference on Artiﬁcial Intelligence and Statistics, 2014.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv
preprint arXiv:1406.1078, 2014.

Wojciech Marian Czarnecki, Grzegorz ´Swirszcz, Max Jaderberg, Simon Osindero, Oriol Vinyals, and Koray
In Proceedings of the 34th

Kavukcuoglu. Understanding synthetic gradients and decoupled neural interfaces.
International Conference on Machine Learning, 2017.

Sander Dalm, Nasir Ahmad, Luca Ambrogioni, and Marcel van Gerven. Scaling up learning with gait-prop. arXiv

preprint arXiv:2102.11598, 2021.

Olivier Devolder, François Glineur, and Yurii Nesterov. First-order methods of smooth convex optimization with

inexact oracle. Mathematical Programming, 146(1-2):37–75, 2014.

10

Thomas Frerix, Thomas Möllenhoff, Michael Moeller, and Daniel Cremers. Proximal backpropagation. In Proceed-

ings of the 6th International Conference on Learning Representations, 2018.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. The MIT Press, 2016.

Akhilesh Gotmare, Valentin Thomas, Johanni Brea, and Martin Jaggi. Decoupling backpropagation using constrained
optimization methods. In Credit Assignment in Deep Learning and Reinforcement Learning Workshop (ICML 2018
ECA), 2018.

Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled cayley transform. In

Proceedings of the 35th International Conference on Machine Learning, 2018.

Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem solutions. Inter-

national Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):107–116, 1998.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.

Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David Silver, and Koray
In Proceedings of the 34th International

Kavukcuoglu. Decoupled neural interfaces using synthetic gradients.
Conference on Machine Learning, 2017.

Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto,

2009.

Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks of rectiﬁed linear

units. arXiv preprint arXiv:1504.00941, 2015.

Yann Le Cun. Learning process in an asymmetric threshold network. In Disordered systems and biological organiza-

tion. Springer, 1986.

Yann Le Cun, Conrad C Galland, and Geoffrey E Hinton. GEMINI: gradient estimation through matrix inversion after

noise injection. In Advances in Neural Information Processing Systems 1, 1988.

Yann LeCun and Corinna Cortes. MNIST handwritten digit database. http://yann.lecun.com/exdb/mnist/, 1998.

Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation.

In Machine

Learning and Knowledge Discovery in Databases. Springer, 2015.

Mario Lezcano-Casado and David Martınez-Rubio. Cheap orthogonal constraints in neural networks: A simple
In Proceedings of the 36th International Conference on

parametrization of the orthogonal and unitary group.
Machine Learning, 2019.

Nikolay Manchev and Michael Spratling. Target propagation in recurrent neural networks. Journal of Machine

Learning Research, 21(7):1–33, 2020.

Alexander Meulemans, Francesco Carzaniga, Johan Suykens, João Sacramento, and Benjamin F. Grewe. A theoretical

framework for target propagation. In Advances in Neural Information Processing Systems 33, 2020.

Alexander Meulemans, Matilde Tristany Farinha, Javier García Ordóñez, Pau Vilimelis Aceituno, João Sacramento,
and Benjamin F Grewe. Credit assignment in neural networks through deep feedback control. arXiv preprint
arXiv:2106.07887, 2021.

Piotr Mirowski and Yann LeCun. Dynamic factor graphs for time series modeling. In Machine Learning and Knowl-

edge Discovery in Databases. Springer, 2009.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient problem. arXiv preprint

arXiv:1211.5063, 2012.

11

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin
Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch:
An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems
32, 2019.

Richard Rohwer. The “moving targets" training algorithm. In Advances in Neural Information Processing Systems 2,

1989.

Vincent Roulet, Siddhartha Srinivasa, Dmitriy Drusvyatskiy, and Zaid Harchaoui. Iterative linearized control: stable
algorithms and complexity guarantees. In Proceedings of the 36th International Conference on Machine Learning,
2019.

D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning Internal Representations by Error Propagation. MIT

Press, Cambridge, MA, USA, 1986.

Jürgen Schmidhuber. Learning complex, extended sequences using the principle of history compression. Neural

Computation, 4(2):234–242, 1992.

Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In Proceedings

of the 28th International Conference on Machine Learning, 2011.

Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum

in deep learning. In Proceedings of the 30th International Conference on Machine Learning, 2013.

Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training neural networks
without gradients: A scalable ADMM approach. In Proceedings of the 33rd International Conference on Machine
Learning, 2016.

Paul Werbos. The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting.

Wiley-Interscience, 1994.

Sam Wiseman, Sumit Chopra, Marc-Aurelio Ranzato, Arthur Szlam, Ruoyu Sun, Soumith Chintala, and Nicolas

Vasilache. Training language models using target-propagation. arXiv preprint arXiv:1702.04770, 2017.

Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine

learning algorithms, 2017. URL https://github.com/zalandoresearch/fashion-mnist.

12

Appendix Plan

The Appendix is organized as follows.

1. Sec. A recalls how gradient back-propagation works for RNNs.
2. Sec. B details the implementations of target propagation.
3. Sec. C details the differences between TP and gradient back-propagation or Gauss-Newton optimization.
4. Sec. D details the parameters used in our experiments and presents additional experiments.

A Gradient back-propagation in Recurrent Neural Networks

Given differentiable activation functions a, the training of recurrent neural networks is amenable to optimization by
gradient descent. The gradients can be computed by gradient back-propagation implemented in modern differentiable
programming software (Rumelhart et al., 1986; Werbos, 1994; Paszke et al., 2019). The gradient back-propagation
algorithm is illustrated in Fig. 6. Formally, the gradients are computed by the chain rule such that, for a sample
(y, x1:τ ) and θh = (Whh, Wxh, bh),

∂(cid:96) (y, gθ(x1:τ ))
∂θh

=

τ
(cid:88)

t=1

∂ht
∂θh

∂hτ
∂ht

∂ ˆy
∂hτ

∂(cid:96)
∂ ˆy

.

The term ∂hτ /∂ht decomposes along the time steps as

∂hτ
∂ht

=

τ
(cid:89)

s=t+1

∂hs
∂hs−1

.

As τ grows, the norm of the term ∂hτ /∂ht can then either increase to inﬁnity (exploding gradients) or exponentially
decrease to 0 (vanishing gradients). This phenomenon may prevent the RNN from learning from dependencies be-
tween temporally distant events (Hochreiter, 1998). Several solutions were proposed to tackle this issue, including
changing the network architecture (Hochreiter and Schmidhuber, 1997), Hessian-free optimization (Sutskever et al.,
2011), gradient clipping and regularization (Pascanu et al., 2012), or orthonormal parametrizations (Arjovsky et al.,
2016; Helfrich et al., 2018; Lezcano-Casado and Martınez-Rubio, 2019). We consider here propagating targets instead
of gradients as ﬁrst presented by LeCun and co-workers (Le Cun, 1986; Le Cun et al., 1988) and recently revisited by
Bengio and co-workers (Bengio, 2014; Lee et al., 2015).

B Detailed Implementation

B.1 Target Propagation for RNNs

As detailed in Sec. 3, target propagation with linearized regularized inverses amounts to move along a descent di-
rection computed by a forward-backward algorithm akin to gradient propagation. The iterations of linearized target
propagation are then summarized in Algo. 1. The iterations of Algo. 1 make calls to any algorithm providing a descent
direction which is computed by Algo. 2.

In the implementation of the regularized inverses, since the inverse of activation functions such as the sigmoid
or the tangent hyperbolic is numerically unstable, we consider projecting on a subset of a(Rp). For the hyperbolic
ε] for ε = 10−3. Concretely, for an hyperbolic tangent activation function,
tangent, we clip the target to [
Rd. To read Algo. 2, we recall our notations
the projection is then π(x) = (min(max(xi,
for θ = (Whh, Wxh, bh, Why, by):

i=1 for x

1 + ε), 1

1 + ε, 1

ε))d

−

−

−

−

∈

cθ(hτ ) = α(Whyhτ + by),

fθ,t(ht−1) = a(Wxhxt + Whhht−1 + bh),

f −1
θ,t (vt) = (W (cid:62)

hhWhh + r I)−1W (cid:62)

hh(a−1(π(vt))

bh).

(5)

(6)

(7)

Wxhxt

−

−

Note that Algo. 2 can also be used for mini-batches of sequence-output pairs since all operations are either element-
wise or linear with respect to the sample of sequence-output pair.

13

Algorithm 1 Stochastic learning with target propagation
1: Inputs: Initial parameters θ(0) = (Whh, Wxh, bh, Why, by) of an RNN deﬁned by Eq. (5) and (6), stepsize γθ,

Fig. 6: Gradient back-propagation for RNN.

total number of iterations K

2: for k = 1 . . . K do
3:
4:

Draw a sample or a mini-batch of sequences-output pairs (x1:τ , y).
Compute

dθ = (dθh, dθy ) = TP(θ(k−1), x1:τ , y),

where TP is Algo. 2
Update the parameters as θ(k) = θ(k−1) + γθdθ.

5:
6: end for

B.2 Target-propagation for GRU networks

B.2.1 Formulation

Starting from h0 = 0, given an input sequence x1, . . . , xτ , the GRU network (as implemented in Pytorch2 (Paszke
et al., 2019)), iterates for t = 1, . . . , τ ,

mt = fm,t(ht−1) := σ(Wimxt + Whmht−1 + bm)
zt = fz,t(ht−1) := σ(Wizxt + Whzht−1 + bz)
nt = fn,t(ht−1, mt) := tanh(Winxt + bin + mt
ht−1 + zt
ht = fh,t(ht−1, zt, nt) := (1

zt)

−

(cid:12)

(cid:12)

(Whnht−1 + bhn))
(cid:12)
nt,

(8)

(9)

(10)

(11)

where
parameters of the network with

(cid:12)

is the Hadamard product, σ is a sigmoid. In the following, we will denote simply θ = (θm, θz, θn) the

θm = (Wim, Whm, bm),

θz = (Wiz, Whz, bz),

θn = (Win, bin.Whn, bhn).

The output of the network is e.g. a soft-max operation on the hidden state computed at the last step (if applied to an
image scanned pixel by pixel for example). See the main paper for the expression of the output in that case.

2Compared to https://pytorch.org/docs/stable/generated/torch.nn.GRU.html, we used a single variable

bm = bim + bhm, same for bz.

14

...... ............ Algorithm 2 Proposed target propagation algorithm
1: Parameters: π a projection onto a susbet of a(Rp), stepsize γh, regularization r.
2: Inputs: Current parameters θ = (θh, θy) with θh = (Whh, Wxh, bh), θy = (Why, by) of the RNN, sample of

sequences-output pairs (x1:τ , y).

hhWhh + r I)−1W (cid:62)

hh giving access to f −1

θ,t (vt) deﬁned in Eq. (7).

Compute and store ht = fθ,t(ht−1),

3: Forward Pass:
4: Compute and store V = (W (cid:62)
5: Initialize h0 = 0.
6: for t = 1, . . . , τ do
7:
8: end for
9: Compute and store (cid:96)(y, cθ(hτ )),
10: Backward Pass:
11: Deﬁne λτ =
12: for t = τ, . . . , 1 do
13:
14: end for
15: Outputs: Descent directions for θh, θy:

Compute λt−1 = ∂htf −1

γh∂hτ (cid:96)(y, cθ(hτ )),

θ,t (ht)(cid:62)λt.

−

∂θhfθ,t(ht−1),

∂htf −1

θ,t (ht).

∂∂hτ (cid:96)(y, cθ(hτ )),

∂θy (cid:96)(y, cθ(hτ )).

dθy =

∂θy (cid:96)(y, cθ(hτ )).

−

dθh =

τ
(cid:88)

t=1

∂θh fθ,t(ht−1)λt,

dθy =

∂θy (cid:96)(y, cθ(hτ )).

−

B.2.2 Modifying the chain rule

The underlying idea of our implementation of target propagation in a differentiable programming framework is to
mix classical gradients and Jacobians of the inverse of the functions. Denote for a given output loss
computed on
a given mini-batch with the current parameters θ, ˆ∂
/ ˆ∂ht the direction back-propagated by our implementation of
target propagation until the step ht. The directions for the parameters of the network can be output as
ˆ∂
Lˆ∂θ
/ ˆ∂ht−1 given ˆ∂
L

/ ˆ∂ht and appropriate regularized inverses. For that, we start with
the chain rule for ∂ht/∂ht−1 and we will replace some of the gradients by Jacobians of regularized inverses at some
places.

The main task is to deﬁne ˆ∂

ˆ∂
Lˆ∂ht

∂ht
∂θ

τ
(cid:88)

t=1

=

L

L

L

.

Classical chain rule. We have
∂zt
∂ht
∂ht−1
∂ht−1

−

=

(cid:18)

(cid:19)

diag(ht−1) + I diag(1

= diag(1

zt) +

−

∂zt
∂ht−1

(diag(nt)

−

zt) +

−

∂zt
∂ht−1

diag(ht−1)) +

diag(nt) +

∂nt
∂ht−1

diag(zt)

∂nt
∂ht−1

diag(zt).

Now for ∂nt/∂ht−1, we further decompose the function fn,t(ht−1) as

with gt(u) = tanh(Winxt + bin + u) and at = (cid:96)(ht−1) := Whnht−1 + bhn. We then have, denoting u = mt

fn,t(ht−1) = gt(mt

at),

(cid:12)

(cid:18) ∂mt
∂ht−1
gt(u) = diag(tanh(cid:48)(Winxt + bin + u)).

∂nt
∂ht−1

=

with

∇

diag(at) +

∂at
∂ht−1

(cid:19)

diag(mt)

gt(u),

∇

15

(12)

(13)

at

(cid:12)

(14)

Inverses. Now, the variables zt, mt and at are functions of ht that incorporate a linear operation and that can be
inverted. Namely, we can deﬁne the following regularized inverses

hmWhm + r I)−1W (cid:62)
hzWhz + r I)−1W (cid:62)
hnWhn + r I)−1W (cid:62)
We can then do the following substitutions in Eq.(12) and (14)

f −1
m,t(vt) = (W (cid:62)
f −1
z,t (vt) = (W (cid:62)
(cid:96)−1(vt) = (W (cid:62)

hm(σ−1(vt)
hz(σ−1(vt)
hn(vt

−
bhn).

−

Wirxt

−
Wizxt

bm)

−
bz)

−

∂mt
∂ht−1 ←

∂zt

∂ht−1 ←

∂at
∂ht−1 ←

ˆ∂mt
ˆ∂ht−1
ˆ∂zt
ˆ∂ht−1
ˆ∂at
ˆ∂ht−1

=

=

=

∇

∇

∇

f −1
m,t(mt)(cid:62)

f −1
z,t (zt)(cid:62)

(cid:96)−1(at)(cid:62)

to deﬁne the quantity back-propagated by target propagation.

Note that by taking the gradient of the inverse we can ignore the biases and the inputs. Namely, we have for

example

hence

The expression for

∇

f −1
m,t(mt) = diag((σ−1)(cid:48)(mt))Whm(W (cid:62)

hmWhm + r I)−1,

∇

f −1
m,t(mt)(cid:62) = (W (cid:62)

hmWhm + r I)−1W (cid:62)

hm diag((σ−1)(cid:48)(mt))

∇

f −1
z,t (zt) is identical. Since (cid:96) is afﬁne, we have simply
(cid:96)−1(at)(cid:62) = (W (cid:62)

hnWhn + r I)−1W (cid:62)

hn.s

∇

Summary. Combined together, we get, denoting dt = ˆ∂L
ˆ∂ht

,

ˆ∂
Lˆ∂ht−1

= (1

zt)

dt +

f −1
z,t (zt)(cid:62)((nt

ht−1)

dt)

+

+

−

(cid:12)

(cid:12)

∇

∇
−
tanh(cid:48)(Winxt + bin + u)
f −1
m,t(mt)(cid:62)(at
tanh(cid:48)(Winxt + bin + u)
(cid:96)−1(at)(cid:62)(mt
(cid:12)
hzWhz + r I)−1W (cid:62)
dt + (W (cid:62)
zt)
hz
−
(cid:0)(σ−1)(cid:48)(mt)
+ (W (cid:62)
hmWhm + r I)−1W (cid:62)
hm
hnWhn + r I)−1W (cid:62)
+ (W (cid:62)
hn(mt

(cid:12)
(cid:12)
(cid:0)(σ−1)(cid:48)(zt)
at
tanh(cid:48)(Winxt + bin + u)
This provides a rule to propagate targets through linearized regularized inverses.

= (1

(nt

dt)

dt)

zt

zt

∇

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

ht−1)

−

(cid:1)

dt

(cid:12)

tanh(cid:48)(Winxt + bin + u)
zt

dt).

(cid:12)

(cid:12)

(cid:1)

dt

zt

(cid:12)

(cid:12)

C Target Propagation vs Gradient or Gauss-Newton descent

C.1 Gradient Back-Propagation vs Target Propagation

Lemma 3.1. The difference between the oracle returned by gradient back-propagation ∂θh(cid:96) (y, gθ(x1:τ )) and the
oracle returned by target propagation can be bounded as

∂θh (cid:96) (y, gθ(x1:τ ))
(cid:107)

−

dθ

(cid:107) ≤

c

sup
t=1,...,τ (cid:107)

∂hfθ,t(ht−1)

∂hf −1

θ,t (ht)

(cid:62)

,
(cid:107)

−

where c = (cid:80)τ

t=1

(cid:80)t−1

s=0 asbt−1−s with a = supt=1,...τ (cid:107)

∂hfθ,t(ht−1)

, b = supt=1,...τ (cid:107)
(cid:107)

∂hf −1

θ,t (ht)

(cid:62)

.

(cid:107)

16

For regularized inverses, we have, denoting ut = Wxhxt + Whhht−1 + bh,

∂hfθ,t(ht−1)
(cid:107)

∂hf −1

θ,t (ht)

(cid:62)

W (cid:62)

(cid:16)
hh(cid:107)

a(ut)

a(ut)−1

+

I

(W (cid:62)

hhWhh + r I)−1

a(ut)−1

(cid:17)

.

−

− ∇
Proof. The ﬁrst claim is a direct application of Lemma C.1 and the second claim follows from the formulation of the
regularized inverse, using that

a(a−1(ht))−1 =

a−1(ht) =

a(ut)−1.

(cid:107) ≤ (cid:107)

(cid:107)(cid:107)∇

(cid:107)∇

−

(cid:107)

(cid:107)

(cid:107)

∇
Lemma C.1. Given A1, . . . , An, B1, . . . , Bn

∇

∇

Rn×n, for any matrix norm

, and any 1

t

≤

≤

n,

(cid:107) · (cid:107)

where a = supi=1,...,n (cid:107)
Proof. Deﬁne for t

Ai

(cid:107)

(cid:81)t

, b = supi=1,...n (cid:107)
(cid:81)t
i=1 Ai
(cid:107)
t−1
(cid:89)

−
(cid:33)

1, δt =
(cid:32)t−1
(cid:89)

Bi

(cid:107)
i=1 Bi

Bi

+ (At

Ai

i=1

−

i=1

≥
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

δt

≤

At

∈

Ai

−

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t
(cid:89)

i=1

aibt−1−i

δ

i=1

Bi

t
(cid:89)

t−1
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) ≤
and δ = supi=1,...,n (cid:107)
, we have
(cid:107)

i=0

t−1
(cid:89)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) ≤

Bi

Bt)

−

Ai

Bi

.
(cid:107)

−

aδt−1 + δbt−1

t−1
(cid:88)

i=0

δ

≤

aibt−1−i.

A convergence to a stationary point for TP can be derived from classical results on an approximate gradient descent

detailed below (the proof is akin to the results of Devolder et al. (2014)).

Corollary C.2 (Corollary of Lemma C.3). Denote εk a bound on the difference between the oracle returned by
gradient back-propagation and by target-propagation both applied to the whole dataset. Provided that the objective
is L-smooth and the stepsizes of TP are chosen such that γ = γhγy < 1/2L, after k iterations, we get

min

i∈{0,...,k−1} (cid:107)∇

F (θi)

2
2 ≤
(cid:107)

c1

F (θ0)

−

minθ∈Rd F (θ)
γk

+

c2
k

k−1
(cid:88)

i=0

ε2
i .

(cid:80)n

where F (θi) = 1
n
Lemma C.3. Let f : Rd
1/(2L), i.e., xk+1 = xk
size γ
satisﬁes, for c1, c2 two universal constants,

γ (cid:98)
∇

→

≤

−

i=1 (cid:96)(φ(xi, θ), yi) with φ(xi, θ) the output of the RNN on a sample xi and (cid:96) the chosen loss.

R be a L-smooth function. Consider an ε-approximate gradient descent on f with step-
εk. After k iterations, this method
f (xk)

f (xk), where

f (xk)

(cid:98)
∇
(cid:107)

− ∇

2
(cid:107)

≤

min

i∈{0,...,k−1} (cid:107)∇

f (xi)
(cid:107)

2
2 ≤

c1

f (x0)

−

minx∈Rd f (x)
γk

+

c2
k

k−1
(cid:88)

i=0

ε2
i .

Proof. Denote gk = (cid:98)
− ∇
∇
approximate gradient descent satisfy

f (xk)

f (xk) for all k

0. By L-smoothness of the objective, the iterations of the

≥

f (xk+1)

f (xk) +

∇

≤

f (xk)(cid:62)(xk+1

xk) +

−

L
2 (cid:107)

xk+1

xk

2
2
(cid:107)

−
Lγ2

f (xk)(cid:62)gk +

γ

∇

f (xk) + gk

2
2

(cid:107)

2 (cid:107)∇

= f (xk)

= f (xk)

f (xk)

≤

γ

γ

γ

−

−

−

(cid:107)∇
(cid:18)
1

(cid:18)
1

2
f (xk)
2 −
(cid:107)
(cid:19)
Lγ
2
Lγ
2

−

(cid:19)

−

(cid:107)∇

(cid:107)∇

f (xk)

f (xk)

2
2 +
(cid:107)

2
2 +
(cid:107)

Lγ2

gk

2
2 + γ(Lγ
(cid:107)

−

1)

∇

f (xk)(cid:62)gk

2 (cid:107)

Lγ2

gk

2
2 + γ(1
(cid:107)

−

Lγ)

(cid:107)∇

2 (cid:107)

where in the last inequality we bounded the absolute value of the last term and used that γL
for any a, b
b = (cid:112)γ(1

θa2 + θ−1b2, which gives for θ > 0, a = (cid:112)γ(1

≤

R and θ > 0, 2ab
2,
(cid:107)

∈
gk
Lγ)/2
(cid:107)

−

f (xk+1)

f (xk)

γ

−

≤

(cid:18)
1

Lγ + θ(1
2

−

−

(cid:19)

Lγ)

2
2 +
f (xk)
(cid:107)

(cid:107)∇

Lγ2 + θ−1γ(1

2

Lγ)

−

gk
(cid:107)

2
2.
(cid:107)

17

2

2,
(cid:107)

gk
f (xk)
(cid:107)
(cid:107)
1. Now we use that
f (xk)
2 and
(cid:107)

≤
Lγ)/2

(cid:107)∇

−

Using 0
(cid:107)
the terms, summing from i = 0, . . . , k

1/2, θ = 1/4 and

Lγ

≤

≤

ε2
k, we get f (xk+1)

2
2 ≤
≤
(cid:107)
1, taking the minimum, dividing by k we get the result.

2 + 2γε2
2
f (xk)
(cid:107)

f (xk)

(cid:107)∇

11
16 γ

−

k. Rearranging

gk

−

C.2 Target Propagation vs Gauss-Newton updates

We discuss the interpretation of Target Propagation (TP) as a Gauss-Newton (GN) method which was proposed by
Bengio (2020); Meulemans et al. (2020). As already mentioned in Sec. 3, the main similarity between TP and GN is
the fact that both TP and GN use the inverse or approximations of inverses of the gradients. In this section, we shall
discuss this interpretation for feed-forward networks to follow the claims of Meulemans et al. (2020). Namely, we
consider here a network deﬁned by L weights W1, . . . , WL and L activation functions a1, . . . , aL which transform an
input x0 into an output xL by computing (no biases were considered by Meulemans et al. (2020)),

xt = ft(xt−1) = at(Wtxt−1)

for t

1, . . . , L

∈ {

}

Denoting φ(x; θ) the output of the network for an input x = x0, with θ = (W1, . . . , WL) being the parameters of the
network, the objective consists in minimizing the loss between the outputs of the network and the sample outputs, i.e.,
minimizing

(y, φ(x; θ)) for pairs of inputs outputs samples (x, y).

L

GN step. Recall ﬁrst the rationale of a GN step for such feed-forward networks with a squared-loss, which amount
to solving

min
θ∈Rp

1
n

n
(cid:88)

i=1

φ(xi; θ)
(cid:107)

−

yi

2
2,

(cid:107)

RK (for classiﬁcation in K classes) and φ(xi, θ)

RdL. A GN step amounts to linearize the non-linear
with yi
function φ around a current set of parameters θ(k) and solve the corresponding least-square problems to deﬁne the next
set of parameters, i.e,

∈

∈

θ(k+1) = argmin

θ

1
n

n
(cid:88)

(cid:107)

i=1

φ(xi; θ(k)) + ∂θφ(xi; θ(k))(cid:62)(θ

θ(k))

yi

2
2

(cid:107)

−

−

= θ(k)

(cid:32) n
(cid:88)

−

i=1

∂θφ(xi; θ(k))∂θφ(xi; θ(k))(cid:62)

(cid:33)−1 (cid:32) n
(cid:88)

i=1

∂θφ(xi; θ(k))

(cid:16)

φ(xi, θ(k))

(cid:33)

(cid:17)

.

yi

−

To consider TP as an approximate GN method we need the following considerations.

1. Consider the iteration on a mini-batch of size 1, s.t.

θ(k+1) = θ(k)

−

(cid:16)

∂θφ(xi; θ(k))∂θφ(xi; θ(k))(cid:62)(cid:17)−1 (cid:16)

∂θφ(xi; θ(k))

(cid:16)

φ(xi, θ(k))

(cid:17)(cid:17)

.

yi

−

2. Consider that the gradients of the networks are invertible, s.t.

θ(k+1) = θ(k)

(cid:16)

−

∂θφ(xi; θ(k))

(cid:17)−(cid:62) (cid:16)

φ(xi, θ(k))

3. Consider updating only one set of parameters θl = Wl , s.t.,

θ(k+1)
l

= θ(k)

l −

(cid:16)

∂θlφ(xi; θ(k))

(cid:17)−(cid:62) (cid:16)

φ(xi, θ(k))

(cid:17)

.

yi

−

(cid:17)

.

yi

−

with

so that, provided that all matrices inside the matrix multiplication are invertible, we get

∂θlφ(xi; θ(k)) = ∂θl fl(xl−1)∂xfl+1(xl) . . . ∂xfL(xL−1)

∂θl φ(xi; θ(k))−T = ∂θl fl(xl−1)−T ∂xfl+1(xl)−T . . . ∂xfL(xL−1)−T

4. Finally, ignore the last inversion and replace it by a gradient step on the parameters θl, then we get an iteration

similar to TP, with

θ(k+1)
l

= θ(k)

l −

∂θl fl(xl−1)∂xfl+1(xl)−T . . . ∂xfL(xL−1)−T ∂xLL

(y, xL)

18

for

L

a squared loss. Namely, we keep the inversion of the gradients of the intermediate functions.

Our objective here is to question whether viewing TP as a GN step with the approximations explained above is
meaningful or not.

Does the original TP formulation approximate GN? Meulemans et al. (2020) start by considering the original
TP formulation, i.e., targets computed as vt = ψt(vt+1) for ψt an approximate inverse of ft and with vL = xL
η∂xL(cid:96)(y, xL). Meulemans et al. (2020, Lemma 1) show then that, provided that we use the exact inverse, ψt = f −1

−
,

t

∆xt = vt

xt =

η

−

−

L−1
(cid:89)

s=t

∂xs fs+1(xs)−(cid:62)∂xL(cid:96)(y, xL) + O(η2).

(Meulemans et al., 2020, Theorem 2) conclude that (i) for mini-batches of size 1, (ii) for a squared loss, (iii) for
0, TP uses a Gauss-Newton optimization with block diagonal approximation to compute the
invertible ft, as η
targets in the sense that as η

→

0,

→

∆xt

η∂xt(ft+1

. . .

◦

◦

≈ −

fL)−(cid:62)(xt).

As the stepsize of any optimization algorithm tends to 0, they all are the same, since the update would be 0 in all
cases. An optimization algorithm aims not to have inﬁnitesimal stepsizes. To make the claim of Meulemans et al.
(2020) more precise, the constants hidden in O(η2) need to be detailed in order to understand in which regimes of the
stepsize the approximation is meaningful. Assuming the inverses ψt to be (cid:96) Lipschitz continuous and L-smooth (i.e.
with L-Lipschitz continuous gradients), a quick look at the proof of Lemma 1 of Meulemans et al. (2020) shows that

vt

−

xt =

η

−

L−1
(cid:89)

s=t

∂xsfs+1(xs)−(cid:62)∂xL(cid:96)(y, xL) + ξt

ξt
(cid:107)

2
(cid:107)
as

aL

≤

≤

≤

at
La2
L
2

s+1 + (cid:96)as+1 + L(cid:96)2η2

2
∂xL(cid:96)(y, xL)
2
(cid:107)

(cid:107)

for s

t, . . . , L

∈ {

1
}

−

η2

2
∂xL (cid:96)(y, xL)
2.
(cid:107)
(cid:107)
2 grows w.r.t. the stepsize η as a polynomial with leading term η2L−t
(cid:107)

The above bound shows that
. So unless
the stepsize is extremely small, it seems unclear whether the original TP formulation approximates GN in this case.
Though the above bound may be pessimistic, it captures correctly the dependency of the error w.r.t. η. True, as η
0,
the leading term is in O(η2), but generally stepsizes are computed for η not inﬁnitesimally small.

→

ξt

(cid:107)

Finally, if the similarity of TP with GN could explain its efﬁciency, then by the reasoning of Meulemans et al.
(2020), the original TP formulation should be efﬁcient. Yet, the original TP formulation has never been shown to
produce satisfying results.

Does TP with the difference target propagation approximate GN? Meulemans et al. (2020) make a similar claim
ψt(xt+1). Namely, Meulemans
for TP with the Difference Target Propagation formula, i.e., vt = xt + ψt(vt+1)
et al. (2020, Lemma 3) show that

−

∆xt = vt

xt =

η

−

−

L−1
(cid:89)

s=t

∂xs ψs(xs)(cid:62)∂xL(cid:96)(y, xL) + O(η2).

Once again, for the claim to be meaningful beyond inﬁnitesimal stepsizes, the terms in O(η2) need to be detailed. A
quick look at the proof of Meulemans et al. (2020, Lemma 3) shows that under appropriate smoothness assumptions
the error can be bounded as a polynomial in η with a leading term η2L−t
. So again, unless we consider inﬁnitesimal
stepsizes, it is unclear whether this approximation is useful.

19

Linearized target propagation and GN.
as presented in (2), namely vt

If we use a linearized version of the difference target propagation formula
xt+1) , then we have the equality

−

xt = ∂xt+1ψt(xt+1)(cid:62)(vt+1
L−1
(cid:89)

∆xt = vt

xt =

−

−

η

−

s=t

∂xsψs(xs)(cid:62)∂xL(cid:96)(y, xL)

and the idea that TP could be seen as an approximate GN method may be pursued in a meaningful way. However the
error of approximation of the inverse of the gradients must be taken into account in order to understand the validity of
the approach.

Propagating the approximation error of the gradient inverses. We compute the approximation error incurred by
composing gradients of the inverse instead of inverses of gradients. Formally, the approximation error for one layer
can be estimated under the assumption that

with e an ε-Lipschitz continuous function and the assumption that the minimal singular value σ of ∂xtft(xt−1) is
positive.

ψt(ft(xt−1)) = xt−1 + e(xt−1),

(15)

The function e a priori depends on θ; we ignore this dependency and simply consider e to be ε-Lipschitz continuous
2 =
(cid:107)

for all θ. For a function e, we deﬁne its Lipschitz continuity constant as ε = supx sup(cid:107)λ(cid:107)2≤1 (cid:107)
supx (cid:107)
denotes the spectral norm. By differentiating both sides of Eq. (15), we get

, where
(cid:107)

∂xe(x)(cid:62)λ

∂xe(x)

(cid:107) · (cid:107)

∂xft(xt−1)∂xψt(xt) = I +∂xe(xt−1).

By assuming the minimal singular value σ of ∂xft(xt−1) to be positive, we get that ∂xft(xt−1) is invertible and so
∂xψt(xt) = ∂xft(xt−1)−1(I +∂xe(xt−1)).

Hence

and ∂xψt(xt) is σ−1(1 + ε) Lipschitz-continuous.

(∂xft(xt−1))−1

(cid:107)

∂xψt(xt)

(cid:107) ≤

−

ε
σ

,

(16)

Now for multiple compositions, using Lemma C.1, we get

(∂hf1(x0))−1 . . . (∂xfL(xL−1))−1

∂xψ1(x1) . . . ∂xψL(xL)

(cid:107) ≤
Therefore the accumulation error diverges with the length L of the network as soon as ε

−

(cid:107)

(1 + ε)L
σL
σ

1.

.

≥

−

Testing the hypothesis that TP could be interpreted as using GN updates directions. Here we come back to the
setting of RNNs presented in the paper. In this case the length of the compositions of layers is τ and according to
the previous discussion, the error of approximation of the product of the inverse of the gradients by the product of
the gradients of the approximate inverses could easily diverge as τ grows (long sequences). Nevertheless, by using
analytical formulas for the inverses, we can ensure that the approximation error is zero, which would correspond then
to the ideal setting where TP uses GN update directions for the hidden states.

Formally, in the context of RNNs, a Gauss-Newton update direction for the hidden states is given as (ignoring the

inverse of the output function)

If no regularization is used in the deﬁnition of the regularized inverse, i.e., if we use

τ −1
(cid:89)

s=t+1

γh

−

(∂hft+1,θ(ht))−(cid:62) ∂h(cid:96)(y, cθ(hτ )),

which requires the inverse of Whh to be well deﬁned, we would get

f −1
θ,t (ht) = (W (cid:62)

hhWhh)−1W (cid:62)

hh(a−1(ht)

Wxhxt

bh),

−

−

∂f −1

θ,t (ht) = ∂hft+1,θ(ht)−1.

20

BP

γ

γh

TP

γθ

κ

Temporal order problem length 60

10−5

10−2

10−1

10

Temporal order problem length 120

10−5

10−2

10−2

Adding problem

10−3

10−1

10−1

MNIST pixel by pixel

10−6

10−4

10−1

MNIST pixel by pixel permuted

10−4

10−4

10−1

1

1

1

1

CIFAR

10−3

10−2

10−2

10

FashionMNIST with GRU

10−2

10−1

10−2

1

Table 1: Hyper-parameters chosen for Fig. 3 and 4.

The updates of TP using the formula (2) would then be exactly the ones of a GN update direction, i.e.,

vt

−

ht =

γh

−

τ −1
(cid:89)

s=t+1

(∂hft+1,θ(ht))−(cid:62) ∂h(cid:96)(y, cθ(hτ )).

So by considering our implementation without regularization, we can test whether the interpretation of TP as an
approximate GN method is meaningful in terms of optimization convergence. As shown in Fig. 6b, it appears that
regularizing the inverses is necessary to obtain convergence, hence the interpretation of TP as GN may not be sufﬁcient
to explain why TP can converge.

D Experimental Details

D.1

Initialization and hyper-parameters

Initialization and data generation.
In all experiments, the weights of the RNN are initialized as random orthogonal
matrices, and the biases are initialized as 0 as presented by Le et al. (2015) and Manchev and Spratling (2020). For
all experiments, the data was not normalized, as done by Manchev and Spratling (2020). We kept a setting as similar
as possible as the one of Manchev and Spratling (2020) to be able to compare target propagation with regularized or
parameterized inverses.

In the synthetic tasks, for BP we used a momentum of 0.9 with Nesterov accelerated gradient
Hyper-parameters.
scheme as done by Manchev and Spratling (2020). Otherwise, we did not use any momentum for the experiment on
MNIST pixel by pixel presented in the main paper. The learning rates of BP and the parameters of TP were found by
a grid-search on a log10 basis and are presented in Table 1. We did not add a regularization term in the training of the
RNNs.

For the Fig. 6b, we used batch sizes of size 512 and performed a grid search for the stepsizes of BP and for the
stepsizes γh of TP while keeping the same regularization r and stepsize γθ to the parameters found for the length 784.

21

Software. We used Python 3.8 and PyTorch 1.6. The RNN was coded using the cuDNN implementation available
in PyTorch that is highly optimized for computing forward passes on the network or gradient back-propagation.

Hardware. All experiments were performed on GPUs using Nvidia GeForce GTX 1080 Ti (12G memory). Each
experiment only used one gpu at a time (clock speed 1.5 Ghz).

Time evaluation. On our GPU, we observed that for the MNIST pixel by pixel experiment, 200 iterations (each
iteration considering 16 samples) were taking approximately 60s for BP and 800s for TP. Note that with larger batch-
sizes the cost of the regularized inversion would be amortized by the fact that more samples are treated simultaneously.
We kept the setting of Manchev and Spartling (Manchev and Spratling, 2020) for ease of comparison.

D.2 Additional experiments

The overhead of TP can be worth its performance. To account for the additional cost of inversion for each mini-
batch, we consider the convergence of the algorithms in time rather than in iterations. We found that, on average,
1 iteration of BP takes approximately 13 times less time than one iteration of TP in our implementation (note that
BP beneﬁts from highly optimized implementations for GPU machines, and TP could potentially also beneﬁt from
the same optimized implementations). Therefore we ran BP for 13 times more iterations than TP and multiplied the
number of iterations by the approximate time needed for each iteration for all algorithms. In the right panel of Fig. 7,
we observe that in time too, TP performs better than BP, which stays stuck at an accuracy of approximately 22

Regularized inverses outperform parameterized inverses. We evaluate the impact of using regularized inverses as
opposed to parameterized inverse and linearized propagation as opposed to ﬁnite-difference-based propagation. The
variant of target propagation with parameterized inverse and ﬁnite-difference propagation corresponds to the approach
of Lee et al. (2015) recently implemented by Manchev and Spratling (2020) and referred to in the ﬁgure above as
DTP-PI. The variant of target propagation with regularized inverse and ﬁnite-difference propagation is referred to
in the ﬁgure above as DTP-RI. Recall that our approach involves regularized inverses and linearized propagation,
referred as TP. In Fig. 7, we observe that both TP and DTP-RI outperform DTP-PI, demonstrating the beneﬁts of
using regularized inverses. On the other hand, both TP and DTP-RI perform on par overall, with the former being
slightly better for the given parameters.

Target propagation is robust to the choice of the target stepsize γh.
In Fig. 6a, we observed how the convergence
could be affected by the choice of the regularization and the step-size γθ. In the left panel of Fig. 8, we observe that
varying γh does not lead to signiﬁcant changes in the convergence behavior.

Target propagation does not beneﬁt from momentum techniques. Numerous methods have been proposed to
enhance the performance of a classical stochastic gradient descent by using, e.g., a momentum term akin to Nesterov’s
accelerated gradient formula (Sutskever et al., 2013). Since TP also produces a priori a descent direction, we can
wonder whether an additional momentum provides faster convergence. In the right panel of Fig. 8, we observe that
adding a momentum on TP is possible but does not seem to provide signiﬁcantly faster convergence while being less
stable. On the other hand, on this same ﬁgure, the momentum seems to help the gradient descent. Our preliminary
experiments using Adam with TP did not conclude; namely, we were not able to obtain a convergence similar to
the one illustrated in Fig. (4) that simply used Algo. 2. We leave for future work the implementation of appropriate
adaptive stepsizes strategies for TP.

22

Fig. 7: Left: MNIST in time. Right: Comparison of different implementations of TP.

Fig. 8: Left: MNIST for varying γh and ﬁxed γθ = 1, r = 1. Right: MNIST with momentum.

23

024Timeins×1040255075100AccuracyTPBP024Iterations×1040255075100AccuracyTPDTP-RIDTP-PI024Iterations×1030255075100Accuracyγh1e-061e-051e-041e-031e-02024Iterations×1040255075100AccuracyTPBP