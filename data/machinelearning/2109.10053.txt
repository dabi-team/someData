Toward a Fairness-Aware Scoring System for Algorithmic

Decision-Making

Yi Yang∗

Ying Wu †

Mei Li‡

Xiangyu Chang §

December, 2021

Abstract

Scoring systems, as a type of predictive model, have signiﬁcant advantages in interpretability

and transparency and facilitate quick decision-making. As such, scoring systems have been

extensively used in a wide variety of industries such as healthcare and criminal justice. However,

the fairness issues in these models have long been criticized, and the use of big data and machine

learning algorithms in the construction of scoring systems heightens this concern. In this paper,

we propose a general framework to create fairness-aware, data-driven scoring systems. First, we

develop a social welfare function that incorporates both eﬃciency and group fairness. Then, we

transform the social welfare maximization problem into the risk minimization task in machine

learning, and derive a fairness-aware scoring system with the help of mixed integer programming.

Lastly, several theoretical bounds are derived for providing parameter selection suggestions.

Our proposed framework provides a suitable solution to address group fairness concerns in the

development of scoring systems.

It enables policymakers to set and customize their desired

fairness requirements as well as other application-speciﬁc constraints. We test the proposed

algorithm with several empirical data sets. Experimental evidence supports the eﬀectiveness of

the proposed scoring system in achieving the optimal welfare of stakeholders and in balancing

the needs for interpretability, fairness, and eﬃciency.

key words: Fairness, Machine Learning, Scoring System, Mixed Integer Programming

2
2
0
2

n
a
J

9

]

G
L
.
s
c
[

3
v
3
5
0
0
1
.
9
0
1
2
:
v
i
X
r
a

∗Department of Information Management and E-Business, School of Management, Xi’an Jiaotong University
†Department of Information Management and E-Business, School of Management, Xi’an Jiaotong University
‡Department of Marketing and Supply Chain Management, Price College of Business, University of Oklahoma
§Center for Intelligent Decision-Making and Machine Learning, School of Management, Xi’an Jiaotong University;

email: xiangyuchang@xjtu.edu.cn.

1

 
 
 
 
 
 
1

Introduction

Predictive models play an essential role in everyday decision-making (Bjarnadottir et al. 2018). A

scoring system is a type of sparse linear predictive model whose coeﬃcients are small integers (Ustun

and Rudin 2016). The coeﬃcients derived from a scoring system can be directly transferred into

point scores, which enable quick and easy calculation, interpretation, and decision-making (Souder

1972). As such, scoring systems have been extensively adopted in a large variety of ﬁelds such as

medical diagnosis (Moreno et al. 2005, Wang et al. 2021), criminal justice (Hoﬀman 1994, Brayne

2014), consumer risk analysis (Capon 1982, Karlan and Zinman 2011), marketing (McNamara 1972,

Zhang et al. 2015), and humanitarian operations (Ndirangu et al. 2013, Skouﬁas et al. 2020). For

example, in the medical ﬁeld, scoring systems such as SAPS I, II, and III (Le Gall et al. 1984,

1993, Moreno et al. 2005) are used to predict ICU mortality risk. Table 4 of the Appendix provides

examples of the wide applications of scoring systems.

Notwithstanding their extensive applications, a crucial concern has been raised regarding fair-

ness in assessment scores derived from these scoring systems, especially along some important social

identities (hereafter referred to as sensitive attributes or sensitive features) such as membership in

a certain race, gender, or socio-economic group (Coﬀman et al. 2021). For example, in the health-

care industry, racial bias has been found in a widely adopted patient health risk scoring system,

where among patients receiving the same calculated risk scores, black patients are, in fact, found

to be considerably sicker than white patients (Obermeyer et al. 2019). Similarly, research shows

that scores derived from COMPAS, a judicial system adopted for making critical pretrial, parole,

probation, and sentencing decision in the U.S., are biased against African-American defendants in

that it is skewed toward labeling black defendants as having high risk of reoﬀending whereas white

defendants are labeled as having low risk (Chouldechova 2017). There are ample other examples of

biases found in scoring systems in the contexts of healthcare (Bierman 2007, Chen et al. 2008) and

criminal justice (Campbell et al. 2020), as well as in other contexts such as credit limits (Vigdor

2019) and loan applications (Alesina et al. 2013).

The concern over group fairness is heightened by the widespread adoption of big data in the

construction of scoring systems. Big data is frequently heterogeneous (Choi et al. 2018), generated

by subgroups with their own traits and behaviors. This heterogeneity can bias the data input into

a scoring system, which subsequently results in unfair predictions (Mehrabi et al. 2021).

As assessment scores are extensively used to make decisions and/or predictions, the lack of

group fairness in scoring systems negatively impacts the lives of disadvantaged groups.

In the

case of COMPAS, assessment scores are used, among other things, to make probation and even

sentencing decisions (Berk et al. 2021). The lack of fairness in this type of scoring systems can have

2

life-changing consequences for the disadvantaged population. In the healthcare industry, assessment

scores are widely adopted and drive important healthcare resource-allocation decisions for millions

of people in the U.S. (Obermeyer et al. 2019). The lack of fairness leads to disparity of access to

medical resources for many lives in the underrepresented minorities. Therefore, there is a dire need

to incorporate group fairness into the development of scoring systems.

In this paper, we propose a fairness-aware framework that aims to incorporate group fairness into

the construction of scoring systems. First, we quantify group fairness, integrating machine learn-

ing perspectives. Speciﬁcally, we investigate disparate impact (Lambrecht and Tucker 2019) and

disparate mistreatment (Zafar et al. 2017a), two types of outcome fairness recognized by machine

learning literature. Then, we develop a social welfare function that incorporates both prediction

eﬃciency and group fairness. We transform the social welfare maximization problem into the em-

pirical risk minimization task in machine learning. With the help of mixed integer programming

techniques, a fairness-aware scoring system is developed. Furthermore, we mathematically derive

theoretical bounds to provide insights regarding the model parameter selection in the proposed

fairness-aware scoring system. We validate the proposed algorithm with several empirical data

sets. Findings support the eﬀectiveness of the proposed scoring system in achieving the optimal

welfare of stakeholders and in balancing the needs of interpretability, group fairness, and eﬃciency.

Our research makes the following contributions. First, we propose a workable solution to social

group fairness issues by developing a new framework for the construction of scoring systems that is

fairness aware. Speciﬁcally, we incorporate learning from machine learning and develop an objective

function that appends fairness to prediction eﬃciency in an optimization problem for scoring system

development. This contrasts signiﬁcantly with past research that is operation-centric, i.e., seeking

to maximize or minimize some operational outcomes such as proﬁt, costs, or accuracy (Celis et al.

2019), and treats fairness only as a constraint to an optimization problem. As such, our framework

informs decision-makers on the overall welfare of those aﬀected, incorporating the level of group

fairness. This lays a foundation for the development of scoring systems that maximize total social

welfare.

Second, our framework provides a proven way of regulating the trade-oﬀ between eﬃciency

and group fairness, as it can accommodate the diﬀerent combinations of fairness and eﬃciency

levels. This ﬂexibility allows decision-makers the ability to set and customize their desired fairness

requirement for diﬀerent group fairness notions. In addition, our approach also provides policymak-

ers with a way to customize other requirements by directly imposing a variety of application-speciﬁc

constraints. Thus, our method provides decision-makers with a general way to construct scoring

systems that suit an organization’s context and objectives.

Third, when constructing the fairness-aware framework, we utilize 0-1 hard loss to encode

3

both the eﬃciency and fairness objectives, and produce a scoring system without the rounding

procedure. This contrasts with past research that applies surrogate loss to make optimization

problems convex, which leads to sub-optimization due to approximations. As a subsequent step, we

derive mathematical proofs that provide guidance for model parameter selection. These parameters

are instrumental to the implementation of the scoring systems.

Fourth, we apply our algorithm to several empirical data sets. We ﬁnd evidence of group biases

in all of the empirical data sets we investigated, which supports the salience of group fairness

issues in scoring systems and further motivates our research. More importantly, we validate the

eﬀectiveness of our new framework in enhancing group fairness in scoring systems. Lastly, our

validation with sepsis data set sheds light on some promising clinical insights. The scoring system

derived based on our framework has captured several risk factors with meaningful cut-oﬀ values

for sepsis mortality prediction. Some of these factors have not been explored before. As such, a

by-product of our research may help reveal new clinical insights in medical diagnosis.

2 Related Literature

In this section, we provide an expanded view of fairness issues addressed in works of literature.

We ﬁrst describe scoring systems and their construction, paying particular attention to the use of

data-driven methods to construct scoring systems. We then introduce the three common notions

of group fairness. Lastly, we provide an overview of how fairness is addressed in machine learning,

as well as in Operations Management and other ﬁelds.

2.1 Scoring Systems

The use of scoring systems for decision-making can be traced back to the seminal work by Burgess

(1928) on parole violation where a scoring system is applied to inmates for prediction of parole

success and failure. The popularity of scoring systems continues, and they are actively engaged in

decision support in today’s environment. Because the coeﬃcients in scoring systems are integer val-

ues (i.e., point scores), these tools allow users to make quick predictions by only adding/subtracting

or multiplying a few small numbers, without the need for a sophisticated computer or calculator, or

for extensive training (Zeng et al. 2017). The ease of calculation and interpretation makes scoring

systems excellent decision tools in situations requiring quick and accurate judgments. For exam-

ple, scoring systems are extensively used in medical diagnosis. Often times, medical professionals

need to make a quick assessment of a patient’s health state at the bedside of the sick patient, and

scoring systems are then applied to patients’ clinical data such as vital signs. Medical professionals

subsequently perform a few simple calculations (addition or subtraction), which enables them to

4

make a quick evaluation of the patient’s condition (Strand and Flaatten 2008).

Scoring systems can be constructed via multiple approaches.

In some cases, construction is

based on the experience and domain knowledge of a panel of subject matter experts (e.g., the

APACHE I by Knaus et al. (1981)). A prevalent method to construct a scoring system is data-

driven (Struck et al. 2017), where scoring systems are usually derived using regression models

followed by the rounding of coeﬃcients to obtain integer-valued point scores (e.g., the SAPS II by

Le Gall et al. (1993)). In addition to traditional statistical approaches, more and more machine

learning techniques are introduced to construct scoring systems based on big data (Hurley and

Adebayo 2016, Chen et al. 2021). For example, Dumitrescu et al. (2021) use information extracted

from various short-depth decision trees to improve the performance of logistic regression when

constructing a credit scoring system. A typical standard procedure to develop a data-driven scoring

system is illustrated in Figure 6 in the Appendix.

2.2 Fairness Issues in Predictive Models

In recent years, a critical issue has been raised concerning predictive models. Many predictive

models are criticized for the lack of incorporation of fairness dimensions (Zou and Schiebinger

2018). Scoring systems, as a type of predictive model, are no exception. The aforementioned

examples highlight people’s concerns over group fairness and give rise to growing public scrutiny

in the decision-making systems that impact resource access and allocation.

In response to these concerns, several laws and regulations have been established to ensure

group fairness for some high-stake domains such as credit, housing, education, healthcare, and

employment (i.e., the Equal Credit Opportunity Act, Equal Employment Opportunity Act, Fair

Housing Act, the Aﬀordable Care Act, and General Data Protection Regulation, etc.).

There are three common types of group (un)fairness discussed in literature (Zafar et al. 2019):

disparate treatment, disparate impact, and disparate mistreatment. Disparate treatment is concerned

with procedural discrimination, whereas the other two types (disparate impact and disparate mis-

treatment) are concerned with outcome discrimination.

We note that legal actions/legislation, in general, prohibit disparate treatment, i.e., treating

individuals diﬀerently based on membership in certain groups (e.g., race or gender), and intent to

discriminate (Kallus et al. 2021). As we mentioned earlier, disparate treatment is only concerned

with procedural discrimination (Fu et al. 2020), and there is no guarantee for the elimination of

outcome discrimination (Kleinberg et al. 2017). For example, although the legislation prohibits

any explicit use of certain sensitive features (race, gender, or religion, etc.) in the construction of

predictive models (Barocas and Selbst 2016), the outcome could still reﬂect discrimination along

these sensitive features. One of the reasons is that even if sensitive features are excluded from

5

inputs, there usually exist other attributes that are highly correlated to the precluded features.

Algorithms can be designed to incorporate these seemingly neutral attributes to serve as proxies of

sensitive features, thereby circumventing existing non-discriminatory legislation and allowing the

systematic denial of resource access to certain groups (Hurley and Adebayo 2016).

This paper is motivated by group fairness concerns in existing research and aims to address

outcome (un)fairness. We focus on two major types of issues related to outcome (un)fairness:

disparate impact and disparate mistreatment. Disparate impact refers to cases where a system

adversely aﬀects the members from one group more than another, even if it appears to be neutral

(Lambrecht and Tucker 2019). Disparate mistreatment refers to cases where the misclassiﬁcation

rates diﬀer for groups of individuals with diﬀerent memberships (Zafar et al. 2017a). Both of these

two outcome fairness types have received much attention in the literature (Harris et al. 2019, Wick

et al. 2019). Next, we review existing literature on how these two types of outcome fairness are

addressed in machine learning.

2.3 Fairness Research in Machine Learning

Fairness issues associated with machine learning algorithms have attracted increasing attention

(Mehrabi et al. 2021). Much of the focus is on classiﬁcation scenarios where a disadvantaged group

suﬀers from discrimination through a classiﬁer.

In this regard, past work has been conducted

to formalize the concept of algorithmic fairness, such as statistical parity (Dwork et al. 2012,

Corbett-Davies et al. 2017), conditional statistical parity (Corbett-Davies et al. 2017), equality

of opportunity, and equalized odds (Hardt et al. 2016), etc. Based on these concepts, various

algorithmic interventions are designed to implement the fairness requirements.

These algorithmic interventions can be mainly categorized into three groups, depending on

the stage when intervention is implemented: pre-processing, in-processing, and post-processing

(Barocas et al. 2017). Our proposed algorithm belongs to the in-processing group.

The key focus of existing research investigating in-processing algorithmic interventions is to

solve a constrained optimization problem, by imposing a constraint on the fairness level, while

optimizing the learning objective such as accuracy (Celis et al. 2019). However, because most

fairness metrics are non-convex due to the use of the indicator function, it is diﬃcult to solve the

master optimization problem. A widely used strategy to achieve convexity is to adopt surrogate

functions for both objectives and fairness constraints. Examples of this scheme include Woodworth

et al. (2017), Zafar et al. (2017a,b), Donini et al. (2018), Yona and Rothblum (2018), Zafar et al.

(2019), Hossain et al. (2020). Most of these studies are limited to a single notion of fairness

or support only a single sensitive attribute, which limits their generality (Kozodoi et al. 2021).

Although several attempts have been made to develop a uniﬁed framework that can handle more

6

than one fairness notion (Quadrianto and Sharmanska 2017, Zafar et al. 2017a, 2019), they still

utilize surrogate functions instead of hard loss to avoid non-convex optimization. This may lead to

sub-par fairness and the sub-optimality of the produced classiﬁer (Lohaus et al. 2020). In addition,

previous research usually assumes the classiﬁer coeﬃcients are continuous and is hard to control the

sparsity, which becomes an obstacle to creating scoring systems. Our proposed framework directly

applies 0-1 hard loss without approximation, thereby overcoming the above ﬂaws when constructing

scoring systems.

2.4 Fairness Research in Operations Management and Other Fields

In operation research, fairness is considered in the context of resource allocation (Rea et al. 2021,

Samorani et al. 2021), where the task is to distribute a set of goods or chores “fairly” among

individual agents. In recent decades, a number of works have been conducted to study the fair

division of both divisible (Robertson and Webb 1998, Procaccia 2013, Gal et al. 2017) and indivisible

items (Lang and Rothe 2016, Aziz et al. 2019). These research studies focus on the allocation of

resources so that the agents’ utilities satisfy certain concepts of fairness, such as envy-freeness

(Foley 1967), i.e., no agent should prefer another’s allocation to his/her own. Several studies have

also addressed the problem of fair allocation of scarce resources such as organ transplantation

(Bertsimas et al. 2013, Zou et al. 2020) and social services (Zardari et al. 2010, Azizi et al. 2018).

Fairness has also been studied in several branches of economics. Extant research supports that

people are concerned with fairness issues (Babcock et al. 1996, Charness and Rabin 2002, Bandiera

et al. 2005, Benjamin et al. 2010). Social preference research in behavioral economics captures

this phenomenon. It indicates that the self-interest assumption must sometimes be appended to

account for interdependent preferences such as fairness (Hamman et al. 2010). Further, several

research studies in this ﬁeld focus on developing economic models to better illustrate the deviation

from purely self-interested behaviors in diﬀerent contexts (Fehr and Schmidt 1999, Bolton and

Ockenfels 2000, Charness and Rabin 2002). Tricomi et al. (2010) also provide direct neurobiological

evidence in support of the existence of fairness considerations for social preferences in the human

brain. In welfare economics (Rabin 1993), much interest has been devoted to the study of social

welfare functions with fairness requirements, such as Rawlsian (Rawls 1999) fairness and α-fairness

(Atkinson et al. 1970), as well as fair allocation in economic models whose main focus is on allocation

rules (Fleurbaey and Maniquet 2008).

We build on social preference theory, and develop a utility function that incorporates both

fairness and eﬃciency considerations, and formulate a social welfare maximization problem to

derive a fairness-aware scoring system. Unlike operational and economic models, which mainly

focus on fairness among individuals, we are concerned with fairness issues rising from individuals

7

belonging to certain groups (such as gender, racial groups, socio-economic groups, etc.). Thus,

our research contributes to the solution of some salient societal problems such as racial or gender

discrimination, which has severe negative implications for underrepresented groups and the rights

and opportunities of their members (Dover et al. 2015). In addition, our problem setting diﬀers from

research in operations management and economics in that we focus on the task of classiﬁcation,

which has predictive value for future decision scenarios.

The remainder of this paper is organized as follows. In Section 3, we motivate our research

using a real-life example in medical diagnosis, and provide detailed illustrations regarding disparate

impact and disparate mistreatment, two common types of outcome unfairness. In Section 4, we

develop a general framework to construct a fairness-aware scoring system with the help of a social

welfare function and mixed integer programming. We present in Section 5 how diﬀerent fairness

measures are formulated and incorporated into our framework for diﬀerent application scenarios.

Section 6 derives and describes several theoretical bounds for the proposed method. In Section 7,

the experimental study of our approach is carried out on several empirical data sets. Section 8

concludes. All the technical proofs can be found in Appendix.

3 An Illustrative Example of Disparate Impact and Disparate

Mistreatment

In this section, we provide a detailed explanation of the two common types of outcome unfairness

(i.e., disparate impact and disparate mistreatment) in the context of sepsis diagnosis. We ﬁrst

explain these two concepts using simpliﬁed, made-up examples. We then use an empirical data set

to illustrate the existence of these disparities, which further motivates our research.

3.1 Sepsis and Scoring Systems in Sepsis Mortality Prediction

Sepsis is a severe and widespread syndrome, a leading cause of mortality and morbidity globally

(Sweeney et al. 2018). Due to the high costs associated with treatment, sepsis has posed a signiﬁcant

challenge to healthcare systems worldwide (Angus et al. 2001). The early prediction of clinical

outcomes, such as in-hospital mortality, can save lives, as medical professionals can then respond

quickly to sepsis patients at greater risk (Mukherjee and Evans 2017). Several scoring systems (e.g.,

SAPS II) have been adopted to assess the severity of sepsis patients’ sickness. These tools consider

patient vital signs, laboratory results, and demographic statistics as risk factors, and produce

severity assessments as outputs. If the outputs of these scoring systems are biased against certain

demographic groups, they can result in unfair allocation of medical resources among patients with

diﬀerent sensitive traits. Below we discuss and illustrate the two types of outcome (un)fairness we

8

study: disparate impact and disparate mistreatment, in the context of the scoring systems used to

assess the severity of sepsis.

3.2 Disparate Impact and Disparate Mistreatment

Figure 1 presents a simpliﬁed, made-up case, purely for the purpose of illustrating the concepts of

disparate impact and disparate mistreatment. The made-up example shows the results from three

medical scoring systems (denoted by D1, D2, and D3) on a data set containing six patients. This

data set includes both non-sensitive features such as body temperature, as well as sensitive features

such as gender. In addition, the data set includes a label indicating the in-hospital mortality status

of a patient (1 = mortality, 0 = otherwise). The goal of the scoring systems is to predict whether a

patient is at a high mortality risk and needs urgent treatment. In what follows, we present diﬀerent

fairness issues through the performance of these scoring systems.

Figure 1: Decisions of three medical scoring systems (i.e., D1, D2, and D3) for sepsis mortality

prediction.
Note: Statistical parity is denoted by SP, equality of opportunity by EO, and equal overall misclassiﬁcation rate by

OMR.

Disparate Impact: As we described earlier, the disparate impact problem arises if

a decision-making system produces results that beneﬁt (or hurt) a group of people

with certain sensitive features more frequently than is the case for other groups

(Barocas and Selbst 2016). The elimination of disparate impact reﬂects the ability

of a decision-making system to achieve statistical parity (SP) (Corbett-Davies et al.

2017), also known as demographic parity (Dwork et al. 2012).

A sepsis patient beneﬁts from a decision of urgent treatment since this indicates

that s/he will be allocated more medical resources.

In this case, we deem only

system D3 to be unfair due to disparate impact. As shown in Figure 1, the fraction

of males and females that were predicted to be at high risk by D3 are diﬀerent

9

(2/3 and 1, respectively). There exists a treatment rate gap of 1/3 between the
two gender groups and thus, D3 does not satisfy statistical parity.1

Disparate Mistreatment: Recall that disparate mistreatment exists if a decision-

making system achieves diﬀerent misclassiﬁcation error rates for groups of people

with diﬀerent values of sensitive features (Zafar et al. 2019). In addition to the

overall error rate, this has been extended to diﬀerent misclassiﬁcations such as

false negatives and false positives. Here, we consider two commonly used algorith-

mic fairness notions in this category, namely equal overall misclassiﬁcation rate

(OMR) (Zafar et al. 2017a) and equality of opportunity (EO) (Hardt et al. 2016).

The former notion eliminates disparate mistreatment by ensuring the same error

rate among diﬀerent groups, while the latter ensures the same false negative rate.

In Figure 1, only D1 is free from disparate mistreatment because it has the same

false negative and overall error rates for two groups. On contrast, both D2 and D3

are unfair due to disparate mistreatment since their rates of erroneous decisions for

males and females are diﬀerent. D2 and D3 both achieve diﬀerent false negative

rates (1/2 and 0) for males and females. D2 also has diﬀerent error rates (2/3 and

0) for males and females, whereas D3 has rates of 2/3 and 1/3.

3.3 Evidence of Disparate Impact and Disparate Mistreatment in Real-Life

Sepsis Mortality Prediction

Next, we illustrate the existence of disparate impact and disparate mistreatment with a real-life

example in the medical ﬁeld. We consider ﬁve commonly used scoring systems for sepsis mortality

prediction: SAPS II, LODS, SOFA, qSOFA, and SIRS (Sweeney et al. 2018). We evaluate their

performance on (un)fairness through an empirical data set extracted from the Medical Informa-

tion Mart for Intensive Care database (MIMIC-III) (Johnson et al. 2016). Detailed information

regarding the scoring systems and the data set can be found in Section 7.1.

Table 1 displays the results of (un)fairness checks with respect to diﬀerent notions of fairness.

Columns (3) through (5) of Table 1 give the absolute value of rate diﬀerence between the two

genders, which is calculated based on diﬀerent algorithmic fairness notions, including SP, EO, and

OMR. It demonstrates that there indeed exist disparities between males and females. In particular,

disparate mistreatment measured by EO is most evident. The gap in false negative rate between

the two groups can reach 11.35% (achieved by SAPS II). This indicates that over 10% more male

sepsis patients with a high mortality risk are ignored at the population level compared to female

1Note that these examples are purely used to illustrate the concepts of disparate impact and disparate mistreatment.

In real-life scenarios, decision-makers may have their own preferences regarding which fairness notion to adopt.

10

patients. In comparison, the disparate mistreatment by OMR and disparate impact by SP are less

severe. However, the absolute values of rate diﬀerence are still up to 4.09% (by SIRS) and 4.22%

(by qSOFA), respectively.

Table 1: Fairness checks of the existing medical scoring systems on Sepsis data set.

Scoring System Accuracy

Fairness Level

SAPS II

LODS

SOFA

qSOFA

SIRS

SP

0.0064

0.0323

0.0329

0.0422

0.0011

EO

0.1135

0.0772

0.0351

0.0530

0.1031

OMR

0.0235

0.0111

0.0081

0.0052

0.0409

0.7442

0.7363

0.7209

0.6304

0.5567

Note: Statistical parity is denoted by SP, equality of opportunity by

EO, and equal overall misclassiﬁcation rate by OMR.

Note that none of the scoring systems we checked above uses the sensitive feature (i.e., gender)

as an input. Nevertheless, the prediction results indicate unfairness against people with certain

sensitive traits. This provides empirical proof of the ﬂaws associated with procedural fairness.

A ﬂawed scoring system leads to the unfair allocation of medical resources among patients from

diﬀerent groups. Our empirical ﬁndings provide living proof of the importance of considering

outcome fairness, and the need to incorporate both fairness and eﬃciency in developing scoring

systems.

4 Model Formulation

To address the issues with disparate impact and disparate mistreatment, we develop a new fairness-

aware framework. In this section, we ﬁrst formalize a utilitarian social welfare function that captures

these group fairness dimensions. After that, we translate a social welfare maximization task into

an empirical risk minimization problem at the center of supervised learning, and then develop a

scoring system achieving the optimal social welfare.

4.1 Model Construction

i=1 denotes a data set with n i.i.d. observations, where xi ∈ X ⊆ Rd+1
Suppose that D = {(xi, yi)}n
is the ith individual’s feature vector with the form of xi = [1, xi,1, . . . , xi,d]T and yi ∈ Y = {−1, 1} is
the ith’s class label. Moreover, si represents the sensitive feature (e.g., race, gender) of individual
i with si ∈ S = {a1, a2, · · · , ac}. Thus, there are c subgroups in the population according to
the sensitive feature. In this paper, we focus on the group-based unfairness which considers the

11

disparities among members of diﬀerent social groups featured by si. We note that xi may or may
not contain the sensitive feature si in the actual applications. For simplicity, we assume here that
si is contained in the feature vector xi. Although a person might be coded with multiple sensitive
features in some cases, we will consider only a single sensitive feature in this work. However, we

will show that the proposed framework can be directly extended to the case where more than one

sensitive feature needs to be considered. In this work, we focus on the linear models of the form
ˆy = sign[wT x], where w = [w0, w1, . . . , wd]T ∈ W represents a vector of coeﬃcients and w0 is the
intercept term.

Traditional economics has a long history of using models of homo economicus (Henrich et al.

2001). That is, it assumes an individual is completely rational and only cares about his/her own

payoﬀs, but is indiﬀerent about the outcomes of others (Cox and Sadiraj 2012). However, there is a

large body of experimental and ﬁeld evidence that contradicts that assumption. Existing research

supports that the majority of people are not purely self-interested (Babcock et al. 1996, Charness

and Rabin 2002, Bandiera et al. 2005, Benjamin et al. 2010), they care about other people, and are

driven by fairness considerations (Fehr and Schmidt 1999, Dawes et al. 2007, Tricomi et al. 2010).

Thus, it is reasonable to assume that an individual’s utility function does not only depend on the
system outcome regulated by w, but also on its level of (un)fairness δ 2, which is represented as

Ui(w, δ). Then, a decision-maker wishes to maximize the following social welfare function given as
a weighted sum of individual utilities,

SW F (w, δ) =

n
(cid:88)

i=1

ζiUi(w, δ),

(1)

where ζi ∈ [0, 1] is the social weight that represents the value placed by society on the ith individual’s
welfare and is normalized so that (cid:80)n

i=1 ζi = 1.

Social preference research captures such departures from narrow self-interest. The distributional

preferences model in social preference usually assumes that people are not only self-interested but

also concerned over the inequity between their and others’ outcomes, and an individual’s utility

function is a linear combination of these two parts (Fehr and Schmidt 1999, Charness and Rabin

2002). Built on this, we consider a setting where the utility of an individual i is additively separable

into data utility and fairness utility:

Ui(w, δ) = ui(w) + vi(δ),

(2)

where ui is data utility that reﬂects an individual i’s own payoﬀ owing to system output, and vi is
fairness utility that represents the inﬂuence of (un)fairness on i. For simplicity, let us consider the

2Here, δ actually represents the maximal level of unfairness of the system outcomes. Thus, a smaller value of δ

implies a higher fairness level (i.e., smaller disparities among groups).

12

following case with linear utility functions:

ui(w) = ai − bi1 (cid:2)yiwT xi ≤ 0(cid:3) ,

vi(δ) = −ρiδ,

(3)

(4)

where both ai ≥ 0 and bi ≥ 0.

Let us discuss the Eqs. (3) and (4) for a thorough understanding of this framework.

Data Utility: We ﬁrst introduce data utility. For a person i, his/her data utility

depends on whether the system classiﬁes him/her correctly or not. Here, we assume

that misclassiﬁcation usually reduces a person’s data utility. Take the medical

diagnosis as an example. Misdiagnosing a sick patient may lead to premature

death due to a lack of appropriate treatment. On the other hand, incorrectly

diagnosing a healthy person will also cost him/her time and money for unnecessary

treatments, including possible invasive procedures, thus his/her utility will also

decrease. Speciﬁcally, as shown in (3), if i is correctly classiﬁed, s/he will receive

the positive data utility with ai. If i is misclassiﬁed, the data utility decreases to
ai − bi.

Fairness Utility: Next, we specify fairness utility. In equation (4), ρi is i’s preference
weight placed on fairness. It reﬂects a person’s attitude towards unfairness among

groups. When ρi = 0, it falls on the classical assumption of people as purely self-
interested individuals. Next, we discuss the case where ρi (cid:54)= 0. When i is a member
of the advantaged group, if ρi > 0, it is a weight that reﬂects his/her benevolence
towards the less advantaged. In this case, i is willing to scarify his/her own data

utility to help the disadvantaged group. On the other hand, ρi < 0 represents i’s
sense of “competition.” It means i prefers to maintain a bigger gap among groups.

In the case where i belongs to the less advantaged group, if ρi > 0, it is a weight
that reﬂects his/her “hostility” towards the advantaged. In this case, i is unwilling

to accept the inequity among outcomes of diﬀerent groups, and hopes to reduce

the gap among groups. If ρi < 0, it shows the “friendliness” of i. That means even
though i is in a disadvantaged group, s/he is willing to see more beneﬁts attributed

to the advantaged group.

13

Afterwards, combining (1)-(4) leads to

SW F (w, δ) =

=

=

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

ui(w) +

n
(cid:88)

i=1

vi(w)

(cid:104)

ζi

ai − bi1 (cid:2)yiwT xi ≤ 0(cid:3) − ρiδ

(cid:105)

ζiai −

n
(cid:88)

i=1

ζibi1 (cid:2)yiwT xi ≤ 0(cid:3) − δ

n
(cid:88)

i=1

ζiρi.

(5)

Note that in Eq (5), the ﬁrst term on the right-hand side is the sum of data utilities over

population, which reﬂects the eﬃciency of the system w. The second term is the sum of fairness

utilities. Further, we apply the utilitarian social welfare function in which all people are treated the

same and social weights are equal across all individuals: ζi =
to maximize social welfare in this case is equivalent to solving the following optimization problem:

for all i. Hence, ﬁnding a classiﬁer

1
n

min
w,δ

1
n

n
(cid:88)

i=1

bi1 (cid:2)yiwT xi ≤ 0(cid:3) +

1
n

δ

n
(cid:88)

i=1

ρi,

s.t.

g(w, D) ≤ δ,

w ∈ W,

(6)

(7)

where (7) is the fairness constraint with g(w, D) encoding a speciﬁc fairness measure, δ is the

maximal unfairness level (achieved by the system) that the users need to tolerate, and W encodes

hard qualities that must be satisﬁed by the coeﬃcients. Note that in problem (6), the objective

function consists of two parts. The ﬁrst part is the average value of the weighted 0-1 loss which

penalizes misclassiﬁcation, and could be regarded as weighted error rate over data set. The second

part reﬂects the “penalty” for unfairness. Thus, the above optimization problem could be adapted

into a regularized empirical risk minimization (ERM) framework in machine learning as follows:

min
w,δ

1
n

n
(cid:88)

i=1

bi1 (cid:2)yiwT xi ≤ 0(cid:3) + ¯ρδ + λ0(cid:107)w(cid:107)0 + (cid:15)(cid:107)w(cid:107)1,

s.t.

g(w, D) ≤ δ,

w ∈ W,

(8)

(9)

(cid:80)n

i=1 ρi
n

where ¯ρ =

is the average preference for fairness in the population, and λ0 ≥ 0, (cid:15) ≥ 0 are the
penalty parameters. Usually, the constraints (9) restrict coeﬃcients to a ﬁnite set of discrete values
such as W = {−10, . . . , 10}d+1 to output an integer score. In addition to the original objective in

problem (6), two more penalties are added into the problem (8). Speciﬁcally, (cid:96)0-penalty is applied
to control the sparsity of the model where (cid:107)w(cid:107)0 = (cid:80)d
1 [wj (cid:54)= 0] is the number of non-zero

j=1

14

coeﬃcients. The classiﬁer tends to include less coeﬃcients if its weight λ0 becomes bigger. The
(cid:96)1-penalty in the objective is used to obtain the coprime coeﬃcients to reduce redundancy, and the
(cid:96)1-penalty parameter (cid:15) should be set small enough to avoid (cid:96)1-regularization.

Now, we have cast the social welfare maximization problem prevalent in economics as a regu-

larized ERM task in machine learning to derive a fairness-aware scoring system. In the following,

we show that the proposed framework could degenerate to several commonly used classiﬁcation

approaches in the machine learning ﬁeld with some choices of model parameters.

4.2 Simpliﬁed Models

4.2.1 Degeneration to Classiﬁcation Model for Speciﬁed δ

Now let us consider a case where the value of δ is pre-speciﬁed as δs.

In this situation, the

optimization problem (8) is equivalent to

min
w

1
N

n
(cid:88)

i=1

bi1 (cid:2)yiwT xi ≤ 0(cid:3) + λ0(cid:107)w(cid:107)0 + (cid:15)(cid:107)w(cid:107)1

s.t.

g(w, D) ≤ δs,

w ∈ W.

(10)

(11)

This will train a fairness-guaranteed classiﬁer following most of the existing algorithmic or in-

processing approaches, which mainly aim at solving a constrained optimization problem by imposing

a constraint on a given level of fairness while optimizing the accuracy (Donini et al. 2018, Zafar

et al. 2019). Unlike the existing approaches, framework (10) directly optimizes the (weighted) error

rate as well as the fairness by 0-1 loss without the approximations that other methods utilize for

scalability. As a result, it avoids sub-optimization and will normally achieve better classiﬁcation

performance while guaranteeing the given fairness level.

4.2.2 Degeneration to Classical Classiﬁcation Model

Especially with a proper choice of the value of δs (e.g., δs = 1) such that the constraint (11) ceases

to bind, (10) could further degenerate to the ordinary classiﬁcation model which only focuses on

the (weighted) error rate (i.e., maximizing only the total data utility):

min
w

1
N

n
(cid:88)

i=1

bi1 (cid:2)yiwT xi ≤ 0(cid:3) + λ0(cid:107)w(cid:107)0 + (cid:15)(cid:107)w(cid:107)1

(12)

s.t. w ∈ W.

Note that if the heterogeneity of data utility preference bi is further ignored, then (12) degrades
to the classic ERM framework as in Ustun and Rudin (2016), which directly applies 0-1 loss instead

15

of convex surrogate functions. This will produce scoring systems that are robust to outliers and

attain the learning-theoretic guarantee on predictive accuracy (Brooks 2011).

4.3 Formulation of Mixed Integer Programming

Unfortunately, solving the problem given by (8) is challenging. The indicator functions in (8) are

non-continuous and non-convex functions of the classiﬁer coeﬃcient w, therefore leading to non-

convex formulations, which are diﬃcult to solve directly. To overcome this hurdle, we reformulate

the problem (8) into the following mixed integer programming (MIP) task to recover the optimal

fairness-aware scoring systems:

min
w,ψ,Φ,α,β,δ

1
n

n
(cid:88)

i=1

biψi +

d
(cid:88)

j=1

Φj + ¯ρδ,

(13a)

s.t.

Miψi ≥ γ −

d
(cid:88)

j=0

yiwjxi,j

i = 1, . . . , N

0-1 loss,

(13b)

G(ψ,Dp, Dq) ≤ δ

p, q = 1, . . . , c

outcome fairness,

(13c)

αs = 0

s ∈ {1, . . . , d}

procedural fairness,

(13d)

Φj = λ0αj + (cid:15)βj

−Ωjαj ≤ wj ≤ Ωjαj

−βj ≤ wj ≤ βj

wj ∈ Wj

ψi ∈ {0, 1}
Φj ∈ R+

αj ∈ {0, 1}
βj ∈ R+

δ ∈ [0, 1]

j = 1, . . . , d

j = 1, . . . , d

j = 1, . . . , d

j = 0, . . . , d

coef. penalty,

(cid:96)0-norm,

(cid:96)1-norm,

coeﬃcient set,

i = 1, . . . , N

loss variables,

j = 1, . . . , d

j = 1, . . . , d

j = 1, . . . , d

penalty variables,

(cid:96)0 variables,

(cid:96)1 variables,

(un)fairness level.

(13e)

(13f)

(13g)

(13h)

Here, Dp = {(xi, yi)}si=ap and Dq = {(xi, yi)}si=aq are individuals from any two diﬀerent groups p
and q, and Ωj = maxwj ∈Wj |wj| is deﬁned as the largest absolute value of wj.

In this formulation, constraint set (13b) uses Big-M constraints for 0-1 loss to set the loss
variables ψi = 1 (cid:2)yiwT xi ≤ 0(cid:3) to 1 if the ith example is misclassiﬁed by the classiﬁer w. The
Big-M constant (Wolsey 1998) Mi can be set as Mi = maxw∈W (γ − yiwT xi), and its computation
is simple since w is restricted to a ﬁnite set. The value of γ could be set to a small positive number
which is not greater than a lower bound on |yiwT xi| (i.e., 0 < γ ≤ mini |yiwT xi|). When the
features are binary, γ can be set to any value between 0 and 1 since the coeﬃcients are integers.

16

In other cases, γ might be set arbitrarily based on an implicit assumption on the values of features

(Zeng et al. 2017). With this setting, if example i is misclassiﬁed, the value of the right-hand side

of the inequality (13b) is positive. Thus, ψi has to be 1 to satisfy the inequality. On the contrary,
if i is classiﬁed correctly, we have γ − (cid:80)d
j=0 yiwjxi,j ≤ 0. In this case, the value of ψi could be
0 or 1. However, since the bigger value of ψi results in more penalty in the objective, ψi will be
forced to equal to 0 in this case. Therefore, ψi will work as an indicator to show whether the i is
misclassiﬁed or not.

To evaluate the unfairness level achieved by w, we focus on several generally known fairness

notions proposed via the machine learning community, which are evaluated over sub-population

groups. Constraint set (13c) encodes the fairness assessment as inequalities among any two diﬀerent

groups p and q in society. Its explicit expressions G(·) depend on the given fairness notion and will

be presented in detail in Section 5. In addition, constraint (13d) ensures the sensitive feature is

not incorporated into the ﬁnal classiﬁer, and thus guarantees procedural fairness at the same time.
Constraint set (13e) represents the total penalty assigned to each coeﬃcient, where αj = 1 [wj (cid:54)= 0]
deﬁned by (13f) encodes the (cid:96)0-penalty and βj = |wj| deﬁned by (13g) encodes the (cid:96)1-penalty.

Although the proposed framework mainly focuses on the considerations regarding fairness, it

also allows decision-makers to implement a variety of application-speciﬁc constraints into its MIP

formulation. Remark 1 shows some examples of application-speciﬁc constraints that can be en-

coded into this method. Note that our framework could also handle multiple application-speciﬁc

constraints at the same time. Thus, this framework provides decision-makers with great ﬂexibility

for their model customization in a simple way.

Remark 1 The MIP formulation ensures that several types of application-speciﬁc constraints could

be implemented. We specify here some common choices for diﬀerent applications.

1) Model Size Control: we could limit the number of input features with the help of the indicator
j=1 αj ≤ Au, where Al is the lower bound and

variables αj by adding the constraint: Al ≤ (cid:80)d
Au is the upper bound of the model size, respectively.

2) Logical Relationship: some logical structures such as “if-then” constraints can be implemented.

For example, to ensure that a classiﬁer will only contain features αj and αk if it also contains
the feature αl, we can encode this as αj + αk ≤ 2αl.

3) Domain Knowledge: some established relationships between input features and the outcome

could be pre-speciﬁed with sign constraints in this model. For example, if the feature j is a

well-known factor for speciﬁc outcomes (e.g., excess body weight usually causes a higher risk

of type 2 diabetes), this positive or negative relationship could be set by adding wj > 0 or
wj < 0, respectively.

17

4) Preference for Feature Selection: practitioners may have preferences between diﬀerent fea-

tures. For the hard preferences where practitioners insist on incorporating a feature j into

the ﬁnal scorecard (Reilly and Evans 2006), we can set αj = 1 to ensure the feature will be
included. The soft preferences between features could be also realized by adjusting the value of

λ0 for diﬀerent features. For example, if we prefer feature j over feature k to some extent, we
can express this requirement as λ0,k = λ0,j + Λ, where Λ > 0 shows the maximal additional
social welfare loss we could tolerate for using feature j instead of feature k. In this way, the

feature k will be used only if it brings additional welfare gain greater than Λ. This approach

can also be used to deal with the problem of missing values in data set (Ustun and Rudin

2016).

5 Fairness Constraints

In Section 3, we have provided intuitive illustrations of disparate impact and disparate mistreatment,

in the context of sepsis morality prediction. In this section, we will show how these two concepts

are modelled mathematically, and derive fairness metrics based on them. Then we present how to

incorporate these fairness metrics into the proposed framework for deriving various fairness-aware

scoring systems.

5.1 Elimination of Disparate Impact

As discussed previously, in the algorithmic decision-making context, even though laws strictly

prohibit procedural discrimination, algorithms can still produce biased outcomes across groups

of diﬀerent sensitive features, thus resulting in disparate impact (Fu et al. 2021).

In response,

statistical parity, one of the fairness notions suggested in the machine learning ﬁeld, is developed.

Statistical parity simply requires the independence of the sensitive feature s and the decision ˆy.

In other words, the system decisions should achieve the same distributions across all demographic

groups. Thinking of the event ˆy = 1 as “acceptance” in the binary classiﬁcation scenario, this

notion requires the acceptance rate to be identical for all groups, i.e.,

P (ˆy = 1 | s = ap) = P (ˆy = 1 | s = aq)

for any two diﬀerent groups p and q. Then, a property that a decision system satisﬁes statistical

parity between two groups p and q up to bias δ could be expressed as:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ δ
(cid:12)P (ˆy = 1 | s = ap) − P (ˆy = 1 | s = aq)

for any p, q = 1, . . . , c.

18

Representing this inequality empirically leads to






(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
Nap

i∈I −
ap

−

1
Naq






(cid:88)

1 (cid:2)yiwT xi ≤ 0(cid:3) + N +

ap −


1 (cid:2)yiwT xi ≤ 0(cid:3)



(cid:88)

i∈I +
ap

(14)

(cid:88)

1 (cid:2)yiwT xi ≤ 0(cid:3) + N +

aq −

1 (cid:2)yiwT xi ≤ 0(cid:3)

(cid:88)

i∈I +
aq

(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤ δ,

ap = (cid:8)i ∈ {1, 2, . . . , n} |si = ap, yi = −1 (cid:9), Nap =

i∈I −
aq
ap = (cid:8)i ∈ {1, 2, . . . , n} |si = ap, yi = 1 (cid:9), I −
ap| and N +

ap| for any p = 1, . . . , c.

ap = |I +

where I +
ap ∪ I −
|I +

For any two diﬀerent groups p, q = 1, . . . , c, the left-hand side of (14) could be re-expressed by

the indicator variables ψi in Section 4.3 as follows:

GSP =

(cid:12)
(cid:12)
(cid:32) N +
(cid:12)
ap
(cid:12)
(cid:12)
Nap
(cid:12)
(cid:12)

(cid:33)

−

N +
aq
Naq

+

1
Nap







(cid:88)

ψi −




i∈I −
ap

ψi


 −

1
Naq

(cid:88)

i∈I +
ap




(cid:88)

ψi −

(cid:88)

ψi

i∈I −
aq

i∈I +
aq

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)

(cid:12)
(cid:12)

.

Then, we can rewrite the fairness constraint (14) for statistical parity as GSP ≤ δ. Incorporating
this inequality into (13c), we can derive a fairness-aware scoring system based on this notion.

Note that statistical parity is well-suited to contexts such as employment or school admissions,

where it may be desirable or required by laws or regulations for diversity or aﬃrmative action

(Chouldechova 2017, Lohaus et al. 2020). In these situations, selecting individuals proportionally

across racial, gender, or geographical groups might be necessary. Moreover, because this notion is

independent of the target value y, it is also appealing in applications where there does not exist the

ground-truth information for decisions, or where the historical decisions used for training are biased

themselves and thus cannot be trusted (Zafar et al. 2019). Implementing statistical parity will aid

the prevention of discrimination based on redundant encoding (Dwork et al. 2012). It may also

help to level the playing ﬁeld and beneﬁt the disadvantaged group in the long run (Hu and Chen

2018). However, this fairness notion might be inadequate in some cases. When disproportionality

is truly present and independent from a sensitive feature, enforcing statistical parity requires us to

reject qualiﬁed candidates from one group and/or approve unqualiﬁed candidates from the other

group. This risk introduces reverse discrimination against qualiﬁed individuals (Zafar et al. 2017a).

In addition, since this notion ignores any possible correlation between y and s, it may reject the

optimal classiﬁer ˆy = y when base rates are diﬀerent (i.e., P (y = 1|s = ap) (cid:54)= P (y = 1|s = aq).

5.2 Elimination of Disparate Mistreatment

Disparate mistreatment has been extended to diﬀerent types of misclassiﬁcation such as false neg-

atives and false positives, in addition to the general misclassiﬁcation rate. Here we consider two

19

frequently used fairness notions in the machine learning community: equal overall misclassiﬁcation

rate (OMR) (Zafar et al. 2017a) and equality of opportunity (EO) (Hardt et al. 2016).

5.2.1 Equal Overall Misclassiﬁcation Rate

This notion is also known as accuracy parity (Zhao and Gordon 2019), which requires the error

rate to be same among all groups. It can be expressed as

P (ˆy (cid:54)= y | s = ap) = P (ˆy (cid:54)= y | s = aq)

for any p, q = 1, . . . , c.

Then, an algorithmic decision-making system that satisﬁes equal OMR between any two groups

p and q up to bias δ could be expressed as follows:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ δ.
(cid:12)P (ˆy (cid:54)= y | s = ap) − P (ˆy (cid:54)= y | s = aq)

Rewriting this inequality with the indicator variables ψi used in the MIP formulation gives the
fairness constraint for equal OMR as

GOM R =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
Nap

(cid:88)

i∈Iap

ψi −

1
Naq

(cid:88)

ψi

i∈Iaq

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ δ,

(15)

where Iap = {i ∈ {1, 2, . . . , n} |si = ap } for any p = 1, . . . , c.

5.2.2 Equality of Opportunity

The second fairness notion EO aims to ensure that the true positive rate of each group is identical,

i.e., TPp = P (ˆy = 1|s = ap, y = 1) is the same for ∀p ∈ {1, · · · , c}. Note that this is equivalent
to requiring that the false negative rate of each group is the same. We also relax this equality

requirement and instead require the maximal unfairness level to be tolerated (i.e., δ). We re-

express it with ψi in a way similar to the previous notions. Then, the fairness constraint for EO is
given by

GEO =

for any p, q = 1, . . . , c.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N +
ap

(cid:88)

i∈I +
ap

ψi −

1
N +
aq

(cid:88)

ψi

i∈I +
aq

≤ δ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Remark 2 Some other notions of fairness related to the elimination of disparate mistreatment,

such as predictive equality (Corbett-Davies et al. 2017) and equalized odds (Hardt et al. 2016),

could also be implemented directly by constructing corresponding fairness constraint function G(·)

20

with the help of ψi. In this paper, we mainly focus on the aforementioned fairness measurements
(equal OMR and EO). Other notions of fairness can be easily tailored for interested readers.

Since disparate mistreatment slightly relaxes the requirement that ˆy is independent of s, it

will not rule out the perfect predictor ˆy = y even when the base rates diﬀer across groups.

In

scenarios where ground truth information for decisions is accessible and reliable, it would be possible

to distinguish disproportionality in decision outcomes among groups that result from candidates’

qualiﬁcations as well as from discrimination against certain groups. Thus, disparate mistreatment

will eﬀectively avoid reverse discrimination and is widely discussed in healthcare (Rajkomar et al.

2018), criminal justice (Chouldechova 2017), and credit ﬁelds (Lohaus et al. 2020). However, it

may also be insuﬃcient under certain contexts. For example, Berk et al. (2021) argue that in

settings where a cost-weighted approach is required, equal overall misclassiﬁcation rate might be

inadequate. Zhang et al. (2019) also suggest that enforcing equality of opportunity can make the

outcomes seem fairer in the short term but can lead to undesirable results in the long run.

At this point, we have derived the speciﬁc expressions of fairness constraints for diﬀerent fairness

notions and discussed their proper application contexts. We would like to issue a caution that

decision-makers need to match the selection of fairness notion with decision contexts. When this is

properly done, one can then apply the expressions of the selected fairness notion in the constraint

set (13c) so as to develop the optimal scoring system that maximizes social welfare.

Remark 3 After incorporating the speciﬁc expressions of fairness constraints into the proposed

framework (13a), the corresponding MIP problems can be solved via commercial optimization solvers

such as CPLEX, Gurobi, or CBC. Note that some of these solvers utilize heuristics to speed up the

process. Thus, the solving process may be sensitive to speciﬁc parameter settings of the solvers,

which may aﬀect the convergence rate and the global optimality of the returned solutions.

It is also worth noting that even though the fairness constraint set (13c) is deﬁned on one speciﬁc

fairness notion, the proposed framework could be easily extended for satisfying multiple fairness

notions simultaneously. In certain application scenarios, it might be desirable to evaluate the level

of (un)fairness on more than one notion of fairness deﬁned above (e.g., measure the (un)fairness

on both disparate impact and disparate mistreatment). In this case, a desirable scoring system

could be developed by including the corresponding constraints simultaneously. Furthermore, the

proposed framework can also incorporate fairness with respect to multiple sensitive features (e.g.,

race, gender, religion, disability) simultaneously by including constraints for each sensitive feature.

These extensions showcase the high ﬂexibility of the proposed framework.

21

6 Theoretical Analysis

In this section, we present some theoretical analyses of the proposed scoring systems. First, we

show that although a ﬁnite discrete set W is used to construct the scoring system, the total social

welfare of the proposed method is not worse than a baseline classiﬁer with real-valued coeﬃcients
θ ∈ Rd+1. Then, we show the relationship between the maximum social welfare and the optimal

(un)fairness level. Thus, for a target social welfare level, we can roughly estimate the corresponding

fairness level. This enables a quick approach to roughly forecast a range of optimal fairness levels

for each scoring system. Proofs of the theorems are deferred to the Appendix.

Theorem 1 indicates that we can always generate a ﬁnite discrete coeﬃcients set such that the

social welfare of the proposed method with discrete coeﬃcients is even better than the social welfare

of a baseline linear classiﬁer with real-valued coeﬃcients. This conclusion has been validated by

the experimental study (see Section 7).

Theorem 1 Let θ = [θ0, θ1, . . . , θd]T ∈ Rd+1 denote the real-valued coeﬃcients of any linear
classiﬁer which is trained with a data set D = {(xi, yi)}n
i=1 and achieves an (un)fairness level
δ with respect to a given fairness requirement G(·). We also denote η(k) as the value of the
|θT xi|
(cid:107)θ(cid:107)2

kth smallest margin achieved by training examples, especially when k = 1, η(1) = mini
(cid:27)

(cid:26)

.

Let I(k) =

i ∈ {1, 2, . . . , n}

< η(k)

denote the set of training examples whose margin is

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|θT xi|
(cid:107)θ(cid:107)2

smaller than η(k), and X(k) = maxi /∈I(k)
xi ∈ D for i /∈ I(k).

(cid:107)xi(cid:107)2 denote the largest magnitude of training example

Fitting a linear classiﬁer via the proposed framework (8) and restricting coeﬃcients to W =
d]T are obtained. If the reso-

{−Ω, . . . , Ω}d+1, we suppose that the coeﬃcients w∗ = [w∗

1, . . . , w∗

0, w∗

lution parameter Ω satisﬁes

√

X(k)

d + 1

2η(k)

,

Ω >

then the diﬀerence of total social welfare between the two classiﬁers is bounded as

SW F (w∗) − SW F (θ) ≥ (1 − k) max
i∈I(k)

bi − N ¯ρ∆F (k),

(16)

where ∆F (k) is a function of k, whose concrete expression depends on the given fairness metric.

Note that when k = 1, ∆F (k) equals zero for all three fairness metrics. In this case, according to
the proof of Theorem 1, the coeﬃcient set W contains a classiﬁer with discrete coeﬃcients w that

achieves the same classiﬁcation results (and thus the same social welfare) as the baseline classiﬁer
with real coeﬃcients θ. Since our linear classiﬁer with w∗ is optimal over W, it may attain a better

social welfare than w, and thus is better than the real-valued classiﬁer θ.

22

Consequently, Theorem 1 shows that if we select an appropriate coeﬃcient set W, i.e., choose a

large enough resolution parameter Ω, the proposed scoring system could achieve even larger total

social welfare than real-valued classiﬁers. This may provide guidelines for practitioners on setting

model parameters in order to pursue large social welfare.

In Theorem 2, we establish the relationship between the optimal fairness level of the proposed

scoring system and the social welfare it can achieve.

Theorem 2 Let w∗ and δ∗ denote the optimal classiﬁer maximizing total social welfare and its

corresponding fairness level, respectively. Then, it holds that

SW F (w∗) > ∆∗(δ∗) − 1 [¯ρ > 0] ¯ρ,

(17)

where ∆∗ is a function of δ∗ and its detailed expression depends on the selected fairness metric.

Theorem 2 gives the lower bound of the social welfare for the proposed scoring system, as a
function of its optimal fairness level δ∗. The proof of Theorem 2 in the Appendix presents the
concrete expression of ∆∗ for all three fairness metrics discussed previously. It is noteworthy that
for these fairness metrics, ∆∗ is a linear function of δ∗. This enables rapid estimation of the optimal

(un)fairness level for the scoring system. For example, if a practitioner wants to roughly estimate

the maximal unfairness level corresponding to a certain value of social welfare (e.g., SWF1), s/he
can simply deduce the upper bound of the corresponding unfairness level δ∗ by setting the left-hand

side of inequality (17) equal to SWF1. This will provide a quick overview of the system’s fairness
performance, which may further allow the practitioner to make rapid adjustments to relevant

policies or information publishing.

7 Experimental Study

In this section, we conduct experiments to validate the proposed method, using several empirical

data sets. Results show that our fairness-aware scoring systems are eﬀective in maximizing total

welfare. In addition, we highlight two advantages associated with our framework: interpretability

and ﬂexibility.

7.1 Application of Fairness-Aware Scoring System to Sepsis Mortality Predic-

tion

In Section 3, we discussed the existence of unfairness in scoring systems used to predict sepsis

mortality. Below, we provide details on the empirical data set, the construction of fairness-aware

scoring systems, and comparison with existing scorecards.

23

7.1.1 Data and Processing

The Sepsis data set is extracted from the Medical Information Mart for Intensive Care database

(MIMIC-III) (Johnson et al. 2016). This data repository has been widely used for medical model

development and validation (Henry et al. 2015). The data set includes 2,021 patients with 19

variables. These variables track the worst value of patients’ health indicators within 24hr of ICU

admission, and include patients’ demographic characteristics such as age and gender. We con-

sider gender (Male/Female) as the sensitive feature. The outcome variable is in-hospital death: 1

indicates death and 0 otherwise.

We convert the raw variables in the sepsis data set into rule-based binary-coded data using the

Ruleﬁt method (Friedman et al. 2008), where each column represents whether the attributes satisfy

a speciﬁc rule. We directly refer to the 77 rules discovered in Wu et al. (2021) as ﬁnal model inputs

and use them to predict the outcome variable. More details regarding the Sepsis data set can be

found in the Appendix.

7.1.2 Model Setting and Baselines

In this experiment, we train fairness-aware scoring systems (FASSs) for diﬀerent fairness measures

as mentioned in Section 5. To further show the interpretability and ﬂexibility of our approach,

we also consider the FASSs model with an additional application-speciﬁc constraint that limits the

model size (see Remark 1). Usually, a person can only handle a few cognitive entities at once (7 ± 2

according to Miller (1956)). Thus, we set the model size to be at most 7 (denoted by FASS7) so it

can be explained and understood by medical professionals in a short period of time.

For all our methods, the coeﬃcient set is chosen as W = {−10, . . . , 10}d+1 and ai = bi = 1 for
∀i = 1, . . . , n for simplicity. Note that with this setting, the data utility is reduced to accuracy. In
addition, we set λ0 ∈ [7 × 10−5, 9 × 10−4] and (cid:15) = 0.01 so that the proposed method will sacriﬁce
little welfare for sparsity. The CPLEX 12.6.3 is employed to solve the ﬁnal MIP problem. The

experiments also compare our approaches to several commonly used medical scoring systems for in-

hospital mortality prediction of sepsis patients in ICUs, as discussed in Section 3. More speciﬁcally,

we consider the following baseline scoring systems: SAPS II, LODS, SOFA, qSOFA, and SIRS. The

details of the baselines can be found in the Appendix.

With this set-up, we randomly partition the Sepsis data into a training set (70%) and test

set (30%) and repeat the partition randomly ﬁve times to evaluate the average performance of all

models, unless otherwise stated. Then, FASS scorecards based on the whole data are produced to

show the interpretability.

24

7.1.3 Results

Below we discuss our experimental ﬁndings. We ﬁrst address the beneﬁts of our framework in social

welfare maximization. We then explain the advantage of our framework in terms of interpretability

and ﬂexibility.

• Social Welfare Maximization

We ﬁrst compare the performance of our methods in terms of social welfare maximization

with the baseline scoring systems mentioned above. In this experiment, the value of average

preference for fairness (i.e., ¯ρ) is set arbitrarily for simplicity. Table 2 provides an overview

of average values of social welfare for all scoring systems on the Sepsis data set. This chart

shows clearly that the proposed FASS model consistently achieves the optimal social welfare

regardless of which fairness metric is utilized. For example, when OMR measures fairness,

FASS can increase total social welfare gains by 10.12% compared to LODS (the best one

among baselines). We obtained similar ﬁndings when fairness is measured by EO or SP. Note

that the results show that compared to FASS (i.e., without model size constraint), FASS7

achieves slightly lower but similar performance. FASS7 still outperforms all baseline models

on all three fairness metrics, even though at times the model size of FASS7 is smaller than

some baseline models.

In summary, all these results indicate that the proposed scoring system performs eﬀectively

in achieving optimal social welfare with diﬀerent fairness metrics.

Table 2: The average values of total social welfare for all scoring systems on Sepsis data set.

Dataset

Sepsis

Fairness

Notions

SP

EO

OMR

¯ρ

5

0.2

5

Baselines

Ours

SAPS II

LODS

SOFA qSOFA

SIRS

FASS

FASS7

0.6761

0.7102

0.5768

0.5368

0.5638

0.7198

0.7096

0.6393

0.5787

0.5089

0.6278

0.5509

0.4520

0.6916

0.6869

0.5373

0.7550

0.7472

0.4022

0.7405

0.7223

Note: Statistical parity is denoted by SP, equality of opportunity by EO, and equal overall

misclassiﬁcation rate by OMR. The optimal values are highted in bold.

In addition, Figure 2 displays the accuracy and (un)fairness level of all scoring systems. As

can be seen from this ﬁgure, the proposed models obtain a lower unfairness level compared

to the baselines with all fairness metrics. Furthermore, both FASS and FASS7 achieve better

accuracy. This indicates that the derived scoring systems outperform the existing medical

scoring systems in sepsis mortality prediction.

25

Figure 2: Accuracy and (un)fairness level of scoring systems.
Note: SPII, LD, SF, qSF, and SR stand for SAPS II, LODS, SOFA, qSOFA, and SIRS, respectively. FS and FS7

stand for FASS and FASS7, respectively.

• Interpretability and Flexibility

Interpretability is often a critical factor when machine learning models are utilized in a product

or a decision process in practice (Molnar 2020). A good practical predictive model should be

interpretable and also reasonable. Especially, Than et al. (2014) emphasize the importance of

“sensibility” of produced models in medical applications, where sensibility refers to whether

a prediction rule is both clinically reasonable and easy to use.

To show interpretability as well as sensibility, we focus on a FASS7 model. The choice

of FASS7 model is to provide a scenario where we sacriﬁce some performance in order to

improve interpretability and ease of use by restricting the model size. Figure 3 shows the

ﬁnal scorecard produced by FASS7 on the whole Sepsis data set. For brevity of illustration,

we only represent the EO scorecard developed based on FASS7. We defer the SP and OMR

scorecards to the Appendix. As shown in Figure 3, the EO scorecard identiﬁes two risk-

increasing rules (rules with positive points) and ﬁve risk-decreasing rules (rules with negative

points). Each rule consists of two or three conditions, and each condition consists of a variable

and a related cut-oﬀ value (e.g., 7.2 for pH.art). If all the conditions are satisﬁed, a rule is

endorsed, and we add or subtract the corresponding score. A higher total score indicates a

greater risk of in-hospital death. We ﬁnd that the risk tendency of the results is in line with

the ﬁndings in Wu et al. (2021).

Note that there are 77 rules that can be used to predict sepsis mortality. Our EO scorecard

based on FASS7 is able to identify the seven most important predictors, providing clinical

sensibility. Speciﬁcally, the scorecard recognizes several indicators with cut-oﬀ values sup-

ported by medical research. For example, the risk factors most involved in the scorecard are

FiO2 (Fraction of inspired oxygen) and pH.art (arterial pH) with cut-oﬀ values at around

0.8 and 7.1, respectively. FiO2 is routinely measured in ICUs to assess patient pulmonary

26

0.011680.039920.03080.026060.021580.009880.006030.121940.083180.041040.056660.112760.036960.03490.031540.019420.027820.017660.031540.002230.00235SPII LDSFqSFSRFSFS70.000.020.040.060.080.10(Un)fairness Level Accuracy      (Un)fairness Level 0.00.20.40.60.81.0AccuracySPII LDSFqSFSRFSFS70.00.10.20.30.4(Un)fairness Level0.00.20.40.60.81.0AccuracySPII LDSFqSFSRFSFS70.000.020.040.060.080.10SPEOOMR(Un)fairness Level0.00.20.40.60.81.0AccuracyFigure 3: EO scorecard developed by FASS7 for sepsis mortality prediction.

function and the presence or severity degree of sepsis-related respiratory dysfunction (Santana

et al. 2013). The scorecard indicates that a FiO2 level higher than 0.8 may lead to greater

risk, which is consistent with a piece of recent evidence in Dahl et al. (2015). Similarly, the

identiﬁed 7.1 of arterial pH (below its normal range: 7.35-7.45) implies the potential presence

of acidosis that leads to unfavorable outcomes for ICU patients. It is the recommended treat-

ment threshold of acute metabolic acidosis in severe sepsis and septic shock from the Survival

Sepsis Campaign (Dellinger et al. 2013). Our scorecard also identiﬁes several other risk fac-

tors that have been recognized by medical research, such as GCS (Glasgow Coma Scale) at 9,

age at 80, and potassium at 4.25 (Solinger and Rothman 2013, Martin-Loeches et al. 2019).

Thus, the risk factors derived by our scorecard are consistent with those supported by medical

research. This indicates that our prediction rules are sensible from the clinical perspective.

An interesting observation is that the EO scorecard developed under our framework may po-

tentially beneﬁt clinical research. Besides factors that have been acknowledged, the proposed

scorecard also identiﬁed new cut-oﬀ values for some important diagnostic indicators such as

GCS at 5 and 7, bilirubin at 2.3 and 7.3, and pH.art at 7.2. This may help reveal possible

and complicated interactions between these sepsis-related variables. Furthermore, the com-

bination of rules developed under our framework is unique in the choice of indicators as well

as cut-oﬀ values. Given that sepsis is a rather complex syndrome with unclear pathology and

multiple comorbidities (Singer et al. 2016), a by-product of our research is that it reveals a

holistic pattern of rules that are eﬀective in medical diagnosis. We call for future examination

of our research results by medical professionals. The detailed justiﬁcation of the identiﬁed

27

risk factors can be found in the Appendix.

In addition, as shown in Figure 3 and Figure 7 in the Appendix, FASS7 scorecards have a

model size not greater than 7 for all fairness metrics. Therefore, the developed systems satisfy

the additional constraint related to sparsity. This indicates that the proposed method could

handle the application-speciﬁc constraints eﬀectively. It provides decision-makers a high level

of ﬂexibility to customize their own requirements and develop an application-speciﬁc scoring

system.

In conclusion, the proposed scoring systems can capture sensible risk-predictive rules. They

help practitioners understand how the model works and how each input aﬀects the ﬁnal

output. The transparency and interpretability facilitate the adoption of our proposed model

in the real-life decision process. Moreover, sparsity and small integer coeﬃcients in our

models enable practitioners to make quick predictions by hand. These are the advantages of

our models for practical applications.

7.2 Other Numerical Experiments

In this section, we conduct several numerical experiments to compare the performance of our

scoring systems to other popular classiﬁcation models in machine learning. We choose the contexts

of income and credit prediction to showcase the generalizability of our framework beyond the

healthcare industry.

7.2.1 Data Sets and Experimental Setup

We conduct numerical experiments with two empirical data sets from the UCI Machine Learning

Repository (Dua and Graﬀ 2017): the Adult income data set and the German credit data set. The

original Adult data set contains 48,842 observations with 14 features, including a binary class label

which indicates whether an individual earns more than 50,000 dollars a year (coded as 1) or not

(coded as 0). The original German data set contains 1,000 observations with 20 features, including

a binary class label indicating whether a customer’s credit is good or not. We removed all data

points with missing values, and processed each data set by binarizing all input features. Moreover,

several sampling methods are used to create ﬁnal balanced data set for model training. Detailed

information regarding UCI data sets and data processing can be found in Appendix.

We consider gender (Male/Female) as the sensitive feature. In each experiment, we randomly

partition the data into a training set (70%) and test set (30%) and repeat the partition randomly

5 times to evaluate the average performance of models, unless otherwise stated. As a comparison,

6 baseline scoring system and linear classiﬁers (Lasso, Ridge, Elastic Net, SVM, Huberized SVM,

28

and SLIM) are also conducted in all the experiments, and the hyperparameters are selected via

5-fold cross-validation. For the proposed fairness-aware scoring system, the coeﬃcient set is chosen
as W = {−10, . . . , 10}d+1 and ai = bi = 1 for all i = 1, . . . , n for simplicity.
data utility is reduced to the accuracy. In addition, we set λ0 < 1/nd and (cid:15) = 0.01 so that our
system will sacriﬁce little welfare for sparsity. The CPLEX 12.6.3 is employed to solve the ﬁnal

In this case, the

MIP problem.

7.2.2 Results

• Social Welfare Maximization

We ﬁrst verify the eﬀectiveness of the proposed methods in achieving the optimal social welfare

as deﬁned in Section 4. In other words, the goal is to develop a scoring system maximizing

the total social welfare that is deﬁned as the sum of data utility and fairness utility over

population.

In this experiment, the value of average preference for fairness (i.e., ¯ρ) is set

arbitrarily for simplicity.

Table 3: The average values of total social welfare for all methods on UCI data sets.

¯ρ

0.2

0.5

0.5

Dataset

Fairness

Notions

Adult

German

SP

EO

OMR

SP

EO

OMR

Ridge

Lasso

Elasticnet

SVM Huberized SVM SLIM

FASS

Baselines

Ours

0.7082

0.7070

0.6698

0.6643

0.7499

0.7532

0.2

0.7886

0.7938

5

5

0.5980

0.6004

0.6389

0.6565

0.7092

0.6682

0.7521

0.7965

0.6009

0.6318

0.7030

0.6791

0.7345

0.7872

0.6061

0.6534

0.6968

0.6479

0.7412

0.7876

0.5994

0.6350

0.7020

0.7468

0.6973

0.7821

0.7430

0.7707

0.7801

0.8089

0.4537

0.6659

0.6057

0.7240

Note: Statistical parity is denoted by SP, equality of opportunity by EO, and equal overall misclassiﬁcation

rate by OMR. The optimal values are highted in bold.

Table 3 summarizes the results we obtained from applying baseline models and our approaches

when incorporating diﬀerent fairness measures. These results clearly show that the proposed

method works well and yields the maximum total welfare in both data sets. Figure 4 further

provides more insights regarding the data utility and fairness utility for welfare maximization

on the Adult data set. It can be seen from this ﬁgure that our framework attains competitive

data utility compared to the baselines while gaining more fairness utility. Hence, our fairness-

aware scoring system improves the overall social welfare signiﬁcantly. Due to the lack of space,

we defer the graphs on German data set to Appendix, which also achieves similar results.

Finally, the results from both data sets provide empirical validation for our theoretical analysis

29

in Theorem 1.

Figure 4: Data utility and fairness utility with diﬀerent fairness measures on Adult data set.

• Impact of Average Preference for Fairness

In the previous study, we assume that the average value of fairness preference is set arbi-

trarily. In this experiment, we study the impact of the average preference for fairness on the

performance of the proposed system. We start by varying the value of ¯ρ associated with the

fairness level. For each value of ¯ρ, we report the accuracy, which reﬂects data utility, and the

(un)fairness level δ between two groups. The results are displayed in Figure 5. It is appar-

ent from this ﬁgure that both accuracy and (un)fairness level decrease as ¯ρ increases in all

fairness metrics. Note that ¯ρ controls the trade-oﬀ between prediction eﬃciency and fairness.

When ¯ρ becomes larger, the scoring system produced by (6) has the tendency to sacriﬁce

more classiﬁcation accuracy to attain a lower unfairness level, since the latter will bring more

beneﬁts for the objective function. Thus, as ¯ρ increases from 0 → +∞, we transition from

an unfair model with the best accuracy to a model with the best fairness level regardless of

accuracy. The complete results on the German data set, which show a tendency similar to

the results in Figure 5, are deferred to the Appendix.

In addition, we also investigate the simpliﬁed case where the fairness level δs is speciﬁed in

advance, as discussed in Section 4.2.1. Complete results are deferred to the Appendix.

8 Conclusions

Fairness has been a prevalent research topic that has garnered increasing interest in recent years

(McCoy and Lee 2014, Samorani et al. 2021). In order to reduce systematic biases and improve

transparency, operations management (OM) scholars have called for research work that integrates

fairness into operational models (Cohen 2018, Rea et al. 2021). Our study answers this call by

30

Figure 5: Trade-oﬀs between accuracy and fairness with diﬀerent fairness measures on Adult data

set in a randomly selected run.

considering group fairness in the development of scoring systems, a type of predictive model that

has been extensively adopted in a wide variety of industries. Past research attests that scores derived

from many prevalent scoring systems are biased along with some important sensitive features such

as gender, race, and socio-economic status (Chouldechova 2017, Obermeyer et al. 2019, Campbell

et al. 2020). This has serious negative consequences for members of disadvantaged groups.

In this research, we propose a general framework for developing data-driven scoring systems

that are fairness-centric. We ﬁrst develop a social welfare optimization model that incorporates

both fairness and eﬃciency components. Then, we cast the welfare maximization problem to

the empirical risk minimization task in machine learning. By utilizing outcome fairness notions

from machine learning, we develop metrics to quantify fairness so as to incorporate it into the

objective function. Mixed integer programming techniques are utilized to derive a fairness-aware

scoring system. In addition, several theoretical bounds are provided for model parameter selection.

Finally, experiments on empirical data sets verify the eﬀectiveness of our approach.

Our study contributes to existing fairness research in the OM ﬁeld. We examine group fairness,

i.e., fairness among members belonging to certain social groups (gender, race, etc.), which comple-

ments extant research with a key focus on resource allocation among individuals. As such, we add

to existing fairness research in an important way.

We develop a fairness-aware framework that incorporates both eﬃciency and fairness. It enables

the analysis of trade-oﬀ between these two important objectives. A beneﬁt of the proposed frame-

work is that it provides ﬂexibility for decision-makers to customize scoring systems with respect

to diﬀerent fairness measures.

It can also be easily modiﬁed to incorporate other requirements

by adding a variety of application-speciﬁc constraints. Experiments on several empirical data sets

show that our approach is capable of achieving both objectives of reducing unfairness among groups

and obtaining competitive eﬃciency compared to some state-of-the-art classiﬁcation methods.

31

0.00.10.20.30.40.50.700.760.82 Accuracy   (Un)fairness LevelAccuracy0.00.20.4SPFairness0.00.10.20.30.40.50.760.790.82Accuracy0.00.20.4EOFairness0.00.10.20.30.40.50.760.790.82Accuracy0.00.10.2OMRFairnessOur research can be extended to many meaningful areas. First, our research question inves-

tigates algorithmic fairness notions in machine learning. Fairness notions in machine learning are

largely inspired by the concepts of discrimination in social sciences and law (Grgic-Hlaca et al.

2016). As such, the key focus is the elimination of disparity in treatment, in impact, or in mis-

treatment. We take note that the meaning of fairness may diﬀer in other research areas. For

example, in economics and game theory, researchers also proposed several fairness notions based on

the agent’s preference, such as envy-freeness (Foley 1967). We call for future research to transfer

these preference-based notions into classiﬁcation tasks, formulate corresponding fairness metrics

that can be recognized by the classiﬁcation algorithm, and derive corresponding decision support

systems.

Second, we develop a fairness-aware scoring system with a ﬁxed-size data set. Future research

can take into consideration the speed of data accumulation, which presents a challenge to the

constant maintenance and update of the existing scoring system. In this regard, we call for the

exploration of methods such as online learning (McMahan 2017) (i.e., a machine learning method

in which data becomes available in a sequential order and is used to update the best predictor for

future data (Hoi et al. 2014)) to meet the challenge of eﬃciently updating scoring systems.

Third, the construction of fairness-aware scoring systems on big data requires intensive compu-

tational resources. If a large data set is used for model training, the computation time might be

of concern. Future research can investigate some data reduction algorithms or speed-up techniques

commonly used in the mixed integer programming ﬁeld that could potentially be incorporated into

our framework.

9 Appendix

A Examples of the wide applications of scoring systems.

Table 4 provides examples to show the wide applications of scoring systems in diﬀerent areas.

B Development of Data-Driven Scoring Systems

A typical standard procedure to develop a data-driven scoring system is illustrated in the ﬂowchart in Figure 6.

There are commonly six steps. The ﬁrst two are data construction steps, collecting and processing past data (e.g.,

data cleansing and feature engineering). The next two are scorecard development steps. The scorecard construction

relies on the chosen algorithm and customized scenarios. It conducts model ﬁtting with the training data set and

scales the model into a scorecard. Then, the constructed scorecard will be evaluated on the test data set to provide

an overview of its predicted performance. The last two are implementation and monitoring steps. Once the scorecard

32

m
o
o
r

y
c
n
e
g
r
e
m
e

e
h
t

n
i

s
t
n
e
i
t
a
p

n
i
a
p

t
s
e
h
c

n
i

e
m
o
c
t
u
o

e
h
t

t
c
i
d
e
r
P

s
t
n
e
i
t
a
p

r
o
f

t
n
e
v
e

c
a
i
d
r
a
c

e
s
r
e
v
d
a

r
o
j
a
m
a

f
o

k
s
i
r

e
h
t

s
s
e
s
s
A

r
e
d
r
o
s
i
d

s
s
e
r
t
s

c
i
t
a
m
u
a
r
t
-
t
s
o
p

r
o
f

n
e
e
r
c
S

e
m
o
r
d
n
y
s

e
s
n
o
p
s
e
r

y
r
o
t
a
m
m
a
ﬂ
n
i

m
e
t
s
y
s

t
c
e
t
e
D

s
t
n
e
v
e

c
i
m
e
h
c
s
i

d
n
a

h
t
a
e
d

f
o

k
s
i
r

e
h
t

s
s
e
s
s
A

k
s
i
r

y
t
i
l
a
t
r
o
m
U
C

I

d
n
a

e
s
a
e
s
i
d

f
o

y
t
i
r
e
v
e
s

e
h
t

s
s
e
s
s
A

I
E
H
C
A
P
A

I
I
E
H
C
A
P
A

I
I
I
E
H
C
A
P
A

S
W
E
M

I

S
P
A
S

I
I

S
P
A
S

I
I
I

S
P
A
S

T
R
A
E
H

S
C
A
D
E

L
C
P

S
R
I
S

I

M
T

I

)
1
8
9
1
(

.
l
a

t
e

s
u
a
n
K

)
5
8
9
1
(

.
l
a

t
e

s
u
a
n
K

)
1
9
9
1
(

.
l
a

t
e

s
u
a
n
K

)
1
0
0
2
(

.
l
a

t
e

e
b
b
u
S

)
4
8
9
1
(

.
l
a

t
e

l
l
a
G
e
L

)
3
9
9
1
(

.
l
a

t
e

l
l
a
G
e
L

)
5
0
0
2
(

.
l
a

t
e

o
n
e
r
o
M

)
0
0
0
2
(

.
l
a

t
e

n
a
m
t
n
A

)
2
9
9
1
(

.
l
a

t
e

e
n
o
B

)
4
1
0
2
(

.
l
a

t
e

n
a
h
T

)
6
1
0
2
(

.
l
a

t
e

n
e
e
v
S

)
8
0
0
2
(

.
l
a

t
e

x
i
S

e
r
a
c
h
t
l
a
e
H

s

m
e
t
s
y
s

g
n

i
r
o
c
s

f
o

s
n
o
i
t
a
c
i
l
p
p
a

e
d
i
w
e
h
t

f
o

s
e
l
p
m
a
x
E

:
4

e
l
b
a
T

n
o
i
t
a
c
i
l
p
p
A

e
m
a
N
m
e
t
s
y
S

g
n
i
r
o
c
S

s
r
e
p
a
P

s
a
e
r
A

t
l
u
a
f
e
d

f
o

k
s
i
r

e
h
t

d
n
a

s
t
n
a
c
i
l
p
p
a

f
o

s
s
e
n
i
h
t
r
o
w
t
i
d
e
r
c

e
h
t

s
s
e
s
s
A

e
m
a
n

t
u
o
h
t
i
w
m
e
t
s
y
s

g
n
i
r
o
c
S

r
o
i
v
a
h
e
b

e
r
u
t
u
f

r
i
e
h
t

t
c
i
d
e
r
p

d
n
a

s
r
e
m
o
t
s
u
c

f
o

e
u
l
a
v

e
h
t

e
r
u
s
a
e
M

e
r
o
c
S
M
F
R

)
2
1
0
2
(

u
s
H
d
n
a

i
h
C

)
2
8
9
1
(

n
o
p
a
C

)
2
1
0
2
(

i
q
i
d
d
i
S

)
9
0
0
2
(

.
l
a

t
e

h
e
Y

r
e
m
u
s
n
o
C

s
i
s
y
l
a
n
A
k
s
i
R

)
5
1
0
2
(

.
l
a

t
e

g
n
a
h
Z

g
n
i
t
e
k
r
a
M

d
e
t
c
e
ﬀ
a

s
e
i
t
i
n
u
m
m
o
c

f
o

s
t
c
a
p
m

i

c
i
m
o
n
o
c
e
-
o
i
c
o
s

f
o

y
t
i
r
e
v
e
s

e
h
t

s
s
e
s
s
A

e
c
n
a
n
d
r
o

d
e
d
o
l
p
x
e
n
u

r
o
/
d
n
a

s
e
n
i
m
d
n
a
l

y
b

e
r
o
c
S

t
c
a
p
m

I

e
n
i
M

)
3
0
0
2
(

.
l
a

t
e

n
e
k
i
v
p
r
a
H

)
3
0
0
2
(

.
l
a

t
e

i
n
i
n
e
B

d
e
t
n
e
m
e
l
p
m

i

s
i

t
p
e
c
n
o
c

g
n
i
t
e
k
r
a
m
e
h
t

h
c
i
h
w
o
t

t
n
e
t
x
e

e
h
t

y
f
i
t
n
a
u
Q

e
r
o
c
S

t
p
e
c
n
o
C
g
n
i
t
e
k
r
a
M

)
2
7
9
1
(

a
r
a
m
a
N
c
M

s
e
c
i
v
r
e
s

t
e
g
r
a
t

d
n
a

s
e
t
a
r

y
t
r
e
v
o
p

r
o
t
i
n
o
M

d
r
a
c
e
r
o
c
S

y
t
r
e
v
o
P
e
l
p
m
i
S

)
0
2
0
2
(

.
l
a

t
e

s
a
ﬁ
u
o
k
S

y
t
i
r
u
c
e
s
n
i

d
o
o
f

d
l
o
h
e
s
u
o
h

e
r
u
s
a
e
M

x
e
d
n
I

s
e
i
g
e
t
a
r
t
S

g
n
i
p
o
C

)
8
0
0
2
(

.
l
a

t
e

l
l
e
w
x
a
M

e
r
o
c
S

S
A
I
F
H

)
3
1
0
2
(

.
l
a

t
e

u
g
n
a
r
i
d
N

n
a
i
r
a
t
i
n
a
m
u
H

s
n
o
i
t
a
r
e
p
O

e
s
a
e
l
e
r

l
a
i
r
t
e
r
p

r
e
t
f
a

r
a
e
p
p
a

o
t

e
r
u
l
i
a
f

d
n
a

t
s
e
r
r
a
-
e
r

f
o

k
s
i
r

e
h
t

s
s
e
s
s
A

e
r
o
c
S

t
n
e
m

s
s
e
s
s
A
k
s
i
R

l
a
i
r
t
e
r
P

)
8
1
0
2
(

n
o
s
n
e
v
e
t
S

T
A
P
-
S
A
R

I

)
0
2
0
2
(

.
l
a

t
e

r
e
d
w
o
L

e
s
a
e
l
e
r

r
e
t
f
a
m

s
i
v
i
d
i
c
e
r

f
o

d
o
o
h
i
l
e
k
i
l

s
’
r
e
n
o
s
i
r
p

l
a
r
e
d
e
f

a

s
s
e
s
s
A

e
r
o
c
S

r
o
t
c
a
F

t
n
e
i
l
a
S

)
3
8
9
1
(

n
a
m
ﬀ
o
H

e
s
n
e
ﬀ
o

e
h
t

f
o

s
s
e
n
s
u
o
i
r
e
s

e
h
t

e
t
a
u
l
a
v
E

e
r
o
c
S

y
t
i
v
a
r
G
e
s
n
e
ﬀ
O

)
6
8
9
1
(

a
c
i
r
i
c
S

d
n
a

r
e
m
a
r
K

s
t
n
a
d
n
e
f
e
d

r
o
f

y
r
o
g
e
t
a
c

y
r
o
t
s
i
h

l
a
n
i
m

i
r
c

e
l
b
a
c
i
l
p
p
a

e
h
t

e
n
i
m
r
e
t
e
D

e
r
o
c
S

y
r
o
t
s
i
H

l
a
n
i
m

i
r
C

)
7
9
9
1
(

k
c
e
B
d
n
a

n
a
m
ﬀ
o
H

t
s
i
v
i
d
i
c
e
r

a

g
n
i
m
o
c
e
b

t
n
a
d
n
e
f
e
d

a

f
o

d
o
o
h
i
l
e
k
i
l

e
h
t

s
s
e
s
s
A

S
A
P
M
O
C

)
5
1
0
2
(

e
t
n
i
o
p
h
t
r
o
N

s
r
e
d
n
e
ﬀ
o

f
o

l
e
v
e
l

k
s
i
r

e
h
t

y
f
i
s
s
a
l
C

m
e
t
s
y
S

t
n
e
m

s
s
e
s
s
A
k
s
i
R
o
i
h
O

)
9
0
0
2
(

.
l
a

t
e

a
s
s
e
t
a
L

l

a
n
i
m
i
r
C

e
c
i
t
s
u
J

33

is validated, it is implemented in practice, and a monitoring and tracking procedure will be put in place to indicate

the need for updates or redevelopments (Thomas et al. 2017).

Figure 6: General steps in developing a data-driven scoring system.

As indicated in Figure 6, data plays a vital role in the development of scoring systems and is the foundation of

the whole process. The quality of the input data set primarily determines the quality of system output. A scoring

system built on biased data might produce biased predictions that lead to fairness issues.

C Omitted Proofs

C.1 Proof of Theorem 1

Proof 1 Applying the Theorem 1 of Ustun and Rudin (2016), it is easy to deduce that for a baseline classiﬁer with

real coeﬃcients θ, there exists the coeﬃcient set W that contains a classiﬁer with discrete coeﬃcients w that assigns

the exactly same label for any example i as the baseline classiﬁer with θ. I.e., for all i ∈ {1, 2, . . . , n}, there exist

w ∈ W where W = {−Ω, . . . , Ω} with Ω >

, such that 1 (cid:2)yiwT xi ≤ 0(cid:3) = 1 (cid:2)yiρT xi ≤ 0(cid:3).

√

X1

d + 1

2η(1)

We apply the above results to the reduced data set D\I(k), and it follows that

n
(cid:88)

bi1

(cid:105)
(cid:104)
yiwT xi ≤ 0

−

n
(cid:88)

(cid:104)

bi1

yiθT xi ≤ 0

(cid:105)

yiθT xi ≤ 0

(cid:105)(cid:111)

+

(cid:88)

(cid:104)

(cid:110)

1

bi

yiwT xi ≤ 0

(cid:105)

(cid:104)

− 1

yiθT xi ≤ 0

(cid:105)(cid:111)

i /∈I(k)

yiθT xi ≤ 0

(cid:105)(cid:111)

=

=

≤

i=1
(cid:88)

(cid:110)

1

bi

i=1
(cid:105)
(cid:104)
yiwT xi ≤ 0

− 1

i∈I(k)
(cid:88)

i∈I(k)
(cid:88)

i∈I(k)

(cid:110)

1

(cid:105)
(cid:104)
yiwT xi ≤ 0

bi

− 1

bi

(cid:104)

(cid:104)

≤(k − 1) max
i∈I(k)

bi.

(18)

(19)

(20)

The equation (18) is due to the fact that the classiﬁer with w assigns the exact same label as the classiﬁer with θ

for any example i ∈ D\I(k). The inequality in (19) results from the most extreme case where all examples in I(k)
are misclassiﬁed by w but correctly classiﬁed by θ. The inequality in (20) follows from the fact that there exist k − 1

elements in I(k).

Next, we discuss the fairness level of the discrete linear classiﬁer w. Note that for convenience, we abuse notation
somewhat and use ψi(w) = 1 (cid:2)yiwT xi ≤ 0(cid:3) as in Section 4.3 to indicate whether an example i is misclassiﬁed or not
by the classiﬁer with coeﬃcients w. Now, we consider the following three fairness deﬁnitions mentioned in Section 5.

34

I. Equal Overall Misclassiﬁcation Rate

We ﬁrst consider the case where the equal overall misclassiﬁcation rate is used to measure fairness level. Note

that c is the number of diﬀerent groups, and since the classiﬁer with θ satisﬁes a given fairness requirement

G(·) and δ, we have

for any p, q = 1, . . . , c.

GOM R(θ) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
Nap

(cid:88)

i∈Iap

ψi(θ) −

1
Naq

(cid:88)

i∈Iaq

ψi(θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ δ

(21)

Recall that the classiﬁer with w assigns the exact same label as the classiﬁer with θ for any example i ∈
(cid:0)ψi(w) − ψi(θ)(cid:1) for
D\I(k). Thus, ψi(θ) = ψi(w) for all i ∈ D\I(k). Besides, we denote zp = (cid:80)
p = 1, 2, . . . , c. Summing |zp| over p, it follows that

i∈I(k)∩Iap

c
(cid:88)

p=1

|zp| =

≤

c
(cid:88)

p=1

c
(cid:88)

p=1

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i∈I(k)∩Iap

ψi(w) − ψi(θ)

(cid:88)

(cid:12)
(cid:12)
(cid:12)ψi(w) − ψi(θ)

i∈I(k)∩Iap
(cid:12)
(cid:12) = k − 1.

≤ (cid:12)

(cid:12)I(k)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(22)

(23)

Since (cid:80)

i∈I(k)∩Iap

ψi(w) = zp + (cid:80)

i∈I(k)∩Iap

ψi(θ) , we have

1
Naq

(cid:88)

i∈Iaq

ψi(w)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ψi(w) −

(cid:88)

i∈Iap


(cid:88)

ψi(w) +

(cid:88)

i∈I(k)∩Iap

i∈Iap −I(k)



ψi(w)

 −

1
Naq





(cid:88)

ψi(w) +

(cid:88)

i∈I(k)∩Iaq

i∈Iaq −I(k)



ψi(w)



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

ψi(θ) + zp +

(cid:88)

i∈I(k)∩Iap

i∈Iap −I(k)



ψi(θ)

 −





1
Naq

(cid:88)

ψi(θ) + zq +

(cid:88)

i∈I(k)∩Iaq

i∈Iaq −I(k)



ψi(θ)



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(24)



ψi(θ) + zp

 −

1
Naq





(cid:88)

i∈Iaq

(cid:88)

i∈Iap

ψi(θ) + zq

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

1
Nap

1
Nap

1
Nap

1
Nap











GOM R(w) =

=

=

=

=

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
Nap

(cid:88)

i∈Iap

1
Nap

(cid:88)

i∈Iap

ψi(θ) −

ψi(θ) −

1
Naq

(cid:88)

i∈Iaq

1
Naq

(cid:88)

i∈Iaq

ψi(θ) +

zp
Nap

−

zq
Naq

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ψi(θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

zp
Nap

−

zq
Naq

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ δ +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

zp
Nap

−

zq
Naq

≤ δ + (k − 1) max

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:26) 1
Nap

(cid:27)

,

,

1
Naq

for any p, q = 1, . . . , c. The equation (24) follows from the fact that ψi(θ) = ψi(w) for all i ∈ DN \I(k), and the
last inequality is due to the inequality (23). Hence, the maximal increment of the tolerance level of unfairness

among all groups is ∆F (k) = (k − 1) max

(cid:27)

for w.

(cid:26) 1
Na1

,

1
Na2

, . . . ,

1
Nac

35

II. Equality of Opportunity

Recall that if the equality of opportunity is used, we have

GEO(θ) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N +
ap

(cid:88)

i∈I+
ap

ψi(θ) −

1
N +
aq

(cid:88)

i∈I+
aq

ψi(θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ δ

for any p, q = 1, . . . , c. We denote z+

p = (cid:80)

i∈I(k)∩I+
ap

(cid:0)ψi(w) − ψi(θ)(cid:1) for p = 1, 2, . . . , c. Similar in Case I, it

follows that

c
(cid:88)

p=1

(cid:12)
(cid:12)z+

p

(cid:12)
(cid:12) =

≤

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

c
(cid:88)

p=1

c
(cid:88)

c
(cid:88)

p=1

(cid:88)

i∈I(k)∩I+
ap

(cid:12)
(cid:12)
(cid:12)
(cid:12)
ψi(w) − ψi(θ)
(cid:12)
(cid:12)
(cid:12)

p=1

i∈I(k)∩I+
ap

(cid:88)

(cid:88)

(cid:12)
(cid:12)
(cid:12)ψi(w) − ψi(θ)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)ψi(w) − ψi(θ)

(cid:12)
(cid:12)
(cid:12)

i∈I(k)∩Iap
(cid:12)
(cid:12) = k − 1.

≤ (cid:12)

(cid:12)I(k)

Afterwards, for any two groups p, q = 1, . . . , c, we have

GEO(w) =

=

=

=

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N +
ap

1
N +
ap

1
N +
ap

1
N +
ap

ψi(w) −

1
N +
aq

(cid:88)

i∈I+
aq

ψi(w)

(cid:88)

i∈I+
ap


(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)


















(cid:88)

ψi(w) +

(cid:88)

i∈I(k)∩I+
ap

i∈I+

ap −I(k)

ψi(w)


 −

1
N +
aq

(cid:88)

ψi(θ) + z+

p +

(cid:88)

i∈I(k)∩I+
ap

i∈I+
ap −I(k)






ψi(θ)


 −

(cid:88)

i∈I+
ap

ψi(θ) + z+
p


 −

1
N +
aq




i∈I+
aq

(cid:88)

ψi(θ) + z+
q




1
N +
aq






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N +
ap

(cid:88)

i∈I+
ap

ψi(θ) −

1
N +
aq

(cid:88)

i∈I+
aq

(cid:12)
(cid:12)
(cid:12)
(cid:12)
ψi(θ)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

z+
p
N +
ap

−

z+
q
N +
aq

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

ψi(w) +

(cid:88)

ψi(w)

i∈I+

aq −I(k)

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)

(cid:12)
(cid:12)

i∈I(k)∩I+
aq





(cid:88)

ψi(θ) + z+

q +

(cid:88)

ψi(θ)

i∈I(k)∩I+
aq

i∈I+

aq −I(k)

(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤ δ +

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

z+
p
N +
ap

−

z+
q
N +
aq

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:40)

≤ δ + (k − 1) max

(cid:41)

.

1
N +
ap

,

1
N +
aq

Hence, for w the maximal increment of the tolerance level of unfairness among all groups is

∆F (k) = (k − 1) max

(cid:26) 1
N +
a1

,

1
N +
a2

, . . . ,

(cid:27)

.

1
N +
ac

III. Statistical Parity

36

Now, we consider a relatively complex case where the given fairness notion is statistical parity. Recall that for

any two groups p, q = 1, . . . , c, it follows that

GSP (θ) =

(cid:32)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

N +
ap
Nap

−

N +
aq
Naq

+

1
Nap






(cid:88)

i∈I−
ap





ψi(θ) −

ψi(θ)


 −

1
Naq

(cid:88)

i∈I+
ap




i∈I−
aq

(cid:88)

ψi(θ) −

ψi(θ)

(cid:88)

i∈I+
aq

(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤ δ.

Again, we denote z+

p = (cid:80)

i∈I(k)∩I+
ap

Then, it is easy to deduce that

(cid:0)ψi(w) − ψi(θ)(cid:1) and z−

p = (cid:80)

i∈I(k)∩I−
ap

(cid:0)ψi(w) − ψi(θ)(cid:1) for p = 1, 2, . . . , c.

(cid:88)

i∈I(k)∩I+
ap

(cid:12)
(cid:12)
(cid:12)
(cid:0)ψi(w) − ψi(θ)(cid:1)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i∈I(k)∩I−
(cid:12)
ap

(cid:12)
(cid:12)
(cid:12)
(cid:0)ψi(w) − ψi(θ)(cid:1)
(cid:12)
(cid:12)
(cid:12)
(cid:12)








(cid:88)

i∈I(k)∩I−
ap

|ψi(w) − ψi(θ)|




(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)










c
(cid:88)

p=1

(cid:12)
(cid:12)z+

p

(cid:12) + (cid:12)
(cid:12)

(cid:12)z−

p

(cid:12)
(cid:12) =

≤

≤

=

c
(cid:88)

p=1

c
(cid:88)

p=1

c
(cid:88)

p=1

c
(cid:88)

p=1

(cid:88)

1

i∈I(k)∩Iap
(cid:12)
(cid:12) = k − 1.

= (cid:12)

(cid:12)I(k)

(cid:88)

|ψi(w) − ψi(θ)| +

i∈I(k)∩I+
ap

(cid:88)

1 +

(cid:88)

i∈I(k)∩I+
ap

i∈I(k)∩I−
ap



1




Similar to the above two cases, the value of GSP (w) is upper bounded as

(25)

ψi(w)

(cid:88)

i∈I+
aq






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)





ψi(w) −

ψi(w)


 −

1
Naq

(cid:88)

i∈I+
ap




(cid:88)

ψi(w) −

i∈I−
aq


ψi(θ) + z−

p −

(cid:88)

ψi(w) − z+
q

i∈I+
ap
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)




ψi(θ) − z+
p








(cid:88)

ψi(θ) −

ψi(θ)


 −

1
Naq

(cid:88)

i∈I+
ap




i∈I−
aq

(cid:88)

ψi(θ) −






ψi(θ)

(cid:88)

i∈I+
aq

GSP (w) =

=

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:32)

(cid:32)

N +
ap
Nap

−

N +
aq
Naq

N +
ap
Nap

−

N +
aq
Naq



(cid:33)

(cid:33)

+

1
Nap

+

1
Nap











(cid:88)

i∈I−
ap

(cid:88)

i∈I−
ap

(cid:88)

ψi(θ) + z−

q −

−

1
Naq




i∈I−
aq

(cid:88)

i∈I+
aq

(cid:33)

(cid:32)

N +
ap
Nap

−

N +
aq
Naq

+

1
Nap






i∈I−
ap
(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:18) z−

p − z+
p
Nap
p − z+
z−
p
Nap

−

−

q − z+
z−
q
Naq
q − z+
z−
q
Naq

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ δ +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ δ + (k − 1) max

(cid:27)

(cid:26) 1
Nap

,

1
Naq

for any p, q = 1, . . . , c. The last inequality follows from the inequality (25). Therefore, the maximal increment

(cid:26) 1
Na1

,

1
Na2

, . . . ,

1
Nac

(cid:27)

for w.

of the tolerance level of unfairness among all groups is ∆F (k) = (k − 1) max

37

With ∆F (k) in hand, we can then calculate the bound of social welfare diﬀerence between two classiﬁers. For the

classiﬁer with w and the classiﬁer with θ, we have

SW F (w) =

SW F (θ) =

n
(cid:88)

i=1
n
(cid:88)

i=1

ai −

ai −

n
(cid:88)

i=1
n
(cid:88)

i=1

bi1

(cid:104)
yiwT xi ≤ 0

(cid:105)

− δw

n
(cid:88)

ρi

bi1

(cid:105)
(cid:104)
yiθT xi ≤ 0

− δθ

i=1
n
(cid:88)

ρi,

i=1

where δw (resp. δθ) is the tolerance level of unfairness that the classiﬁer with w (resp. θ) can achieve. Note that

δθ = δ according to the assumption. Then, we have

SW F (w) − SW F (θ) = −

(cid:34) n
(cid:88)

i=1

bi1

(cid:104)

(cid:105)
yiwT xi ≤ 0

−

n
(cid:88)

i=1

(cid:104)

bi1

yiθT xi ≤ 0

(cid:35)
(cid:105)

− N ¯ρ (δw − δθ)

≥ (1 − k) max
i∈I(k)

bi − N ¯ρ(δ + ∆F (k) − δ)

= (1 − k) max
i∈I(k)

bi − N ¯ρ∆F (k).

(26)

Recall that according to the deﬁnition of w∗, it is the discrete classiﬁer maximizing the social welfare. Based on the

inequality (26), we have

SW F (w∗) − SW F (θ) ≥ SW F (w) − SW F (θ)

= (1 − k) max
i∈I(k)

bi − N ¯ρ∆F (k).

(27)

C.2 Proof of Theorem 2

Proof 2 Let V(w) = (cid:80)n
that only focuses on the data utility. We ﬁrst prove

i=1 vi be the overall data utility of w, and wdata = argmaxw∈W V(w) denote the classiﬁer

V(wdata) ≥ max {∆∗, V(w∗)} ,

(28)

where ∆∗ is a parameter related to δ∗ and the corresponding fairness deﬁnition. According to the deﬁnition of wdata,
we have V(wdata) ≥ V(w∗) directly.

I. Equal Overall Misclassiﬁcation Rate

First, we consider a classiﬁer wuf ∈ W that does not satisfy the equal overall misclassiﬁcation rate requirement
with δ∗ for all groups. In other words,

GOM R(wuf ) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
Nap

(cid:88)

i∈Iap

ψi(wuf ) −

1
Naq

(cid:88)

i∈Iaq

ψi(wuf )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

> δ∗

for any p, q = 1, . . . , c and p (cid:54)= q. From the above equation, it follows that

or

1
Nap

(cid:88)

i∈Iap

ψi(wuf ) −

1
Naq

(cid:88)

i∈Iaq

ψi(wuf ) < −δ∗

1
Naq

(cid:88)

i∈Iaq

ψi(wuf ) −

1
Nap

(cid:88)

i∈Iap

ψi(wuf ) < −δ∗.

38

(29)

(30)

(31)

Multiplying (30) by Nap , we can get

(cid:88)

i∈Iap

ψi(wuf ) −

Nap
Naq

(cid:88)

i∈Iaq

ψi(wuf ) < −Nap δ∗.

Thus,

(cid:88)

i∈Iap

ψi(wuf ) +

(cid:88)

i∈Iaq

ψi(wuf ) =

(cid:88)

i∈Iap

ψi(wuf ) −

Nap
Naq

(cid:88)

i∈Iaq

ψi(wuf ) +

Nap + Naq
Naq

(cid:88)

i∈Iaq

ψi(wuf )

< Nap (1 − δ∗) + Naq .

(32)

(33)

The inequality in (33) results from (32) and the fact that

1
Naq

(cid:80)

i∈Iaq

ψi(wuf ) ≤ 1. Then we multiply (31) by

Naq . Using the similar method, we have

(cid:88)

i∈Iap

ψi(wuf ) +

(cid:88)

i∈Iaq

ψi(wuf ) < Naq (1 − δ∗) + Nap .

Combining this with (33) gives

(cid:88)

i∈Iap

ψi(wuf ) +

(cid:88)

i∈Iaq

ψi(wuf ) < max (cid:8)Nap (1 − δ∗) + Naq , Naq (1 − δ∗) + Nap

(cid:9)

for any p, q = 1, . . . , c and p (cid:54)= q. Summarizing this inequality over (cid:0)c
that

2

(cid:1) unique pairs of p, q, it is easy to deduce

n
(cid:88)

i=1

ψi(wuf ) =

c
(cid:88)

(cid:88)

p=1

i∈Iap

ψi(wuf ) <

1
c − 1

(cid:88)

p(cid:54)=q

max (cid:8)Nap (1 − δ∗) + Naq , Naq (1 − δ∗) + Nap

(cid:9) .

Hence, the lower bound of V(wuf ) is given by

V(wuf ) =

≥

>

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

ai −

n
(cid:88)

i=1

biψi(wuf )

ai − max

i

bi

n
(cid:88)

i=1

ψi(wuf )

ai −

maxi bi
c − 1

(cid:88)

p(cid:54)=q

max (cid:8)Nap (1 − δ∗) + Naq , Naq (1 − δ∗) + Nap

(cid:9) .

maxi bi
Deﬁne ∆∗ = (cid:80)n
c − 1
by deﬁnition, it follows that V(wdata) ≥ V(wuf ) > ∆∗.

i=1 ai−

(cid:80)

p(cid:54)=q max (cid:8)Nap (1 − δ∗) + Naq , Naq (1 − δ∗) + Nap

(cid:9). Since wdata = argmaxw∈W V(w)

II. Equality of Opportunity

Similar to Case I, we ﬁrst consider a classiﬁer wuf which does not satisfy the equality of opportunity require-
ment with δ∗ for all groups. That is,

GEO(wuf ) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N +
ap

(cid:88)

i∈I+
ap

ψi(wuf ) −

1
N +
aq

(cid:88)

i∈I+
aq

(cid:12)
(cid:12)
(cid:12)
(cid:12)
ψi(wuf )
(cid:12)
(cid:12)
(cid:12)

> δ∗

for any two groups p, q = 1, . . . , c and p (cid:54)= q. Thus, for wuf , we have

1
N +
ap

(cid:88)

i∈I+
ap

ψi(wuf ) −

1
N +
aq

(cid:88)

i∈I+
aq

ψi(wuf ) < −δ∗

(34)

39

or

1
N +
aq

(cid:88)

i∈I+
aq

ψi(wuf ) −

1
N +
ap

(cid:88)

i∈I+
ap

ψi(wuf ) < −δ∗.

Multiplying (34) by N +

ap results in

ψi(wuf ) −

(cid:88)

i∈I+
ap

N +
ap
N +
aq

(cid:88)

i∈I+
aq

ψi(wuf ) < −N +

ap δ∗.

Then, we have

(cid:88)

i∈I+
ap

ψi(wuf ) +

(cid:88)

i∈I+
aq

ψi(wuf ) =

ψi(wuf ) −

(cid:88)

i∈I+
ap

N +
ap
N +
aq

(cid:88)

i∈I+
aq

ψi(wuf ) +

N +

ap + N +
aq
N +
aq

(cid:88)

i∈I+
aq

ψi(wuf )

From the inequality (35), using similar methods leads to

< N +

ap (1 − δ∗) + N +
aq .

(cid:88)

i∈I+
ap

ψi(wuf ) +

(cid:88)

i∈I+
aq

ψi(wuf ) < N +

aq (1 − δ∗) + N +
ap .

Combining (36) and ((37), we obtain

(35)

(36)

(37)

(cid:88)

i∈I+
ap

ψi(wuf ) +

(cid:88)

i∈I+
aq

ψi(wuf ) < max (cid:8)N +

ap (1 − δ∗) + N +

aq , N +

aq (1 − δ∗) + N +
ap

(cid:9)

for any p, q = 1, . . . , c and p (cid:54)= q. Summarizing this inequality over (cid:0)c
derive that

2

(cid:1) unique pairs of p, q, we can easily

(cid:88)

i∈I+

ψi(wuf ) <

1
c − 1

(cid:88)

p(cid:54)=q

max (cid:8)N +

ap (1 − δ∗) + N +

aq , N +

aq (1 − δ∗) + N +
ap

(cid:9) ,

when I + = (cid:8)i ∈ {1, 2, . . . , n} |yi = 1 (cid:9) is the set of individuals from positive class. Hence,

n
(cid:88)

i=1

ψi(wuf ) =

(cid:88)

ψi(wuf ) +

(cid:88)

ψi(wuf )

i∈I+
1
c − 1

<

(cid:88)

p(cid:54)=q

i∈I−
max (cid:8)N +

ap (1 − δ∗) + N +

aq , N +

aq (1 − δ∗) + N +
ap

(cid:9) + N −,

when I − = (cid:8)i ∈ {1, 2, . . . , n} |yi = −1 (cid:9) is the set of individuals from negative class and N − = |I −| is the size
of I −. Then, the lower bound of V(wuf ) is

V(wuf ) =

≥

>

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

ai −

n
(cid:88)

i=1

biψi(wuf )

ai − max

i

bi

n
(cid:88)

i=1

ψi(wuf )

ai −

maxi bi
c − 1

(cid:88)

p(cid:54)=q

max (cid:8)Nap (1 − δ∗) + Naq , Naq (1 − δ∗) + Nap

(cid:9) − max

i

biN −.

Here, we deﬁne ∆∗ = (cid:80)n
Recalling the deﬁnition of wdata, it directly follows that V(wdata) ≥ V(wuf ) > ∆∗.

p(cid:54)=q max (cid:8)Nap (1 − δ∗) + Naq , Naq (1 − δ∗) + Nap

i=1 ai −

(cid:80)

maxi bi
c − 1

(cid:9) − maxi biN −.

40

III. Statistical Parity

When statistical parity is used for the fairness requirement, we ﬁrst consider a classiﬁer wuf which does not
satisfy this fairness requirement with δ∗ for all groups. This leads to

GSP (wuf ) =

(cid:32)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
> δ∗

(cid:33)

N +
ap
Nap

−

N +
aq
Naq

+

1
Nap






(cid:88)

i∈I−
ap

ψi(wuf ) −



ψi(wuf )


 −

1
Naq






(cid:88)

i∈I−
aq

(cid:88)

i∈I+
ap

ψi(wuf ) −

ψi(wuf )

(cid:88)

i∈I+
aq






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

for any two groups p, q = 1, . . . , c and p (cid:54)= q. For wuf , this implies that

(cid:33)

(cid:32)

N +
ap
Nap

−

N +
aq
Naq

+

1
Nap






(cid:88)

i∈I−
ap

ψi(wuf ) −



ψi(wuf )


 −

1
Naq






(cid:88)

i∈I−
aq

(cid:88)

i∈I+
ap

ψi(wuf ) −

ψi(wuf )

(cid:88)

i∈I+
aq


 < −δ∗


or

(cid:33)

(cid:32)

N +
aq
Naq

−

N +
ap
Nap

+

1
Naq







(cid:88)

ψi(wuf ) −




i∈I−
aq

ψi(wuf )


 −

1
Nap

(cid:88)

i∈I+
aq




i∈I−
ap

(cid:88)

ψi(wuf ) −

(38)


 < −δ∗.


ψi(wuf )

(cid:88)

i∈I+
ap

(39)

In the case where (38) holds, we multiply both sides of it by Nap Naq . Then, we have

Naq

(cid:88)

ψi(wuf ) −






i∈I−
ap





ψi(wuf )


 − Nap




(cid:88)

i∈I+
ap

(cid:88)

i∈I−
aq

ψi(wuf ) −

(cid:88)

i∈I+
aq


 < Nap N +


ψi(wuf )

aq − Naq N +

ap − Nap Naq δ∗.

Rearranging this inequality, it is easy to derive that

Naq

=Naq










(cid:88)

i∈Iap

(cid:88)

i∈I−
ap

ψi(wuf ) +

ψi(wuf ) +

(cid:88)

i∈Iaq

(cid:88)

i∈I+
ap





ψi(wuf )

ψi(wuf ) +

(cid:88)

i∈I−
aq

ψi(wuf ) +






ψi(wuf )

(cid:88)

i∈I+
aq

< − Nap Naq δ∗ + Nap N +

aq + Naq N +

ap + Nap N −

aq + Naq N −

aq + (Naq − Nap )

• If Naq > Nap , based on (40) we have

ψi(wuf ).

(40)

(cid:88)

i∈I+
aq





Naq

(cid:88)

i∈Iap

ψi(wuf ) +

(cid:88)

ψi(wuf )





i∈Iaq
aq + Naq N +

< − Nap Naq δ∗ + Nap N +

ap + Nap N −

aq + Naq N −

aq + (Naq − Nap )N +
aq .

The last inequality is due to the fact that (cid:80)

i∈I+
aq

ψi(wuf ) ≤ N +

aq . This ﬁnally leads to

(cid:88)

i∈Iap

ψi(wuf ) +

(cid:88)

i∈Iaq

ψi(wuf ) < Nap

(cid:32)

N −
aq
Naq

(cid:33)

− δ∗

+ N +

ap + Naq .

41

(41)

(42)

• If Naq ≤ Nap , from (40) we have

Naq





(cid:88)

i∈Iap

ψi(wuf ) +

(cid:88)

i∈Iaq

ψi(wuf )

since (cid:80)

i∈I+
aq

ψi(wuf ) ≥ 0. Then,


 < −Nap Naq δ∗ + Nap N +

aq + Naq N +

ap + Nap N −

aq + Naq N −
aq ,

(cid:88)

i∈Iap

ψi(wuf ) +

(cid:88)

i∈Iaq

ψi(wuf ) < (1 − δ∗)Nap + N +

ap + N −
aq .

(43)

Now, we consider the other case where (39) holds. We multiply both sides of (39) also by Nap Naq . Applying
similar steps as above, we can derive that





Nap

(cid:88)

i∈Iap

ψi(wuf ) +

(cid:88)

ψi(wuf )





i∈Iaq
ap + Nap N +

< − Nap Naq δ∗ + Naq N +

aq + Naq N −

ap + Nap N −

ap + (Nap − Naq )

ψi(wuf ).

(cid:88)

i∈I+
ap

• If Naq ≥ Nap , we can obtain

(cid:88)

i∈Iap

ψi(wuf ) +

(cid:88)

i∈Iaq

ψi(wuf ) < (1 − δ∗)Naq + N +

aq + N −
ap .

• If Naq < Nap , we have

(cid:88)

i∈Iap

ψi(wuf ) +

(cid:88)

i∈Iaq

ψi(wuf ) < Naq

(cid:32)

N −
ap
Nap

(cid:33)

− δ∗

+ N +

aq + Nap .

Combining this with (42), (43), and (44) gives

(44)

(45)

ψi(wuf ) +

(cid:88)

ψi(wuf )

(cid:88)

i∈Iap

<1 (cid:2)Naq > Nap

i∈Iaq
(cid:40)

(cid:3) max

(1 − δ∗)Naq + N +

aq + N −

ap , Nap

(cid:32)

+ 1 (cid:2)Naq < Nap

(cid:3) max

(1 − δ∗)Nap + N +

ap + N −

aq , Naq

(cid:40)

(cid:33)

(cid:41)

− δ∗

+ N +

ap + Naq

(cid:33)

(cid:41)

− δ∗

+ N +

aq + Nap

N −
aq
Naq
(cid:32)

N −
ap
Nap

+ 1 (cid:2)Naq = Nap

=M(p, q, δ∗).

(cid:3) max (cid:8)(1 − δ∗)Naq + N +

aq + N −

ap , (1 − δ∗)Nap + N +

ap + N −
aq

(cid:9)

(46)

We summarize (46) over (cid:0)c

2

(cid:1) unique pairs of p, q. Then, we can easily obtain

Thus,

n
(cid:88)

i=1

ψi(wuf ) <

1
c − 1

(cid:88)

M(p, q, δ∗).

p(cid:54)=q

V(wuf ) =

≥

>

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

ai −

n
(cid:88)

i=1

biψi(wuf )

ai − max

i

bi

n
(cid:88)

i=1

ψi(wuf )

ai −

maxi bi
c − 1

(cid:88)

M(p, q, δ∗).

p(cid:54)=q

42

We deﬁne ∆∗ = (cid:80)n
∆∗ with the statistical parity notion.

i=1 ai −

maxi bi
c − 1

(cid:80)

p(cid:54)=qM(p, q, δ∗). Afterwards, it directly follows that V(wdata) ≥ V(wuf ) >

Consequently, there all exists ∆∗ related to δ∗ for three diﬀerent fairness notions such that V(wdata) > ∆∗.

Because of the fact that V(wdata) ≥ V(w∗), we can conduct that V(wdata) ≥ max {∆∗, V(w∗)}.

Now, we have shown that in these three fairness notions, V(wdata) > ∆∗ and V(wdata) ≥ V(w∗). Due to the

deﬁnition of w∗, we have SW F (w∗) ≥ SW F (wdata) which leads to

V(w∗) − ¯ρδ∗ ≥ V(wdata) − ¯ρδdata

where δdata ∈ [0, 1] is the (un)fairness level of wdata. Combining (47) and V(wdata) > ∆∗, we can obtain

SW F (w∗) = V(w∗) − ¯ρδ∗ > ∆∗ − ¯ρδdata.

(47)

(48)

If ¯ρ > 0, the right-hand side of the inequality in (48) is greater than ∆∗ − ¯ρ. If ¯ρ ≤ 0, it is greater than ∆∗. This

yields the statement of the theorem.

D Supplementary Information for Experimental Study

D.1 Sepsis Mortality Prediction

D.1.1 Summary Statistics of Data Set

The MIMIC-III database contains information related to patients admitted to ICU at a large tertiary care hospital

between 2001 and 2012 in the U.S. This database is publicly accessible. It includes patients’ records and information

like their demographics, vital sign measurements, laboratory test results, procedures, medications, etc. The Sepsis

data set is extracted from MIMIC-III. It involves a subset of MIMIC-III patients who are diagnosed with sepsis,

severe sepsis, or septic shock based on the ICD-9 code suggested in Singer et al. (2016).

Table 5 displays the summary statistics of the ﬁnal Sepsis data set.

Table 5: Summary statistics of the sepsis data set

Data set

N

doriginal

d

Sensitive feature Positive% Male%

Sepsis

2021

19

77 Gender (binary)

33.40%

53.74%

D.1.2 Baseline Scoring Systems

In this experiment, we consider the following 5 baseline scoring systems that are widely applied for mortality prediction

in medical practice:

SAPS II: Simpliﬁed Acute Physiology Score (Le Gall et al. 1993) is developed based on medical

data from 137 ICUs of 12 countries in Europe/North America. The scoring system measures 17

variables of ICU patients and assigns diﬀerent points to diﬀerent variables. A regression-based

model is provided to convert a total score to a mortality probability.

43

LODS: Logistic Organ Dysfunction System (Le Gall et al. 1996) uses the same database as SAPS II

for model development but aims to assess the dysfunction levels of 6 human organ systems among

ICU patients. The total dysfunction score ranges from 0 to 22 and can also be converted to a

mortality probability by a logistic regression model.

SOFA: Developed through expert consensus, Sepsis-related Organ Failure Assessment (Vincent et al.

1996) measures the degree of organ failure and evaluates morbidity of septic patients. The total

score ranges from 0 to 24, with 0 to 4 for each of the six organ systems. Although the score is

not initially designed to predict patient survival, it has become a basic variable in many mortality

prediction models due to a high correlation between organ failure and survival.

qSOFA: The quick SOFA (Singer et al. 2016) is a simpliﬁed and quick version of SOFA, often used

at the bedside to identify patients with suspected infection who are at greater risk of bad clinical

outcomes outside the ICU. It only consists of three criteria (1 point for each) about blood pressure,

respiratory rate, and central nervous system status. A score ≥ 2 is usually considered a sepsis

case and is associated with at least a threefold increase in in-hospital mortality.

SIRS: Systemic Inﬂammatory Response Syndrome (Bone et al. 1992) is actually not a scoring system

speciﬁcally designed for sepsis mortality, but we include it in our experiments as it forms an

essential part of the initial deﬁnition of sepsis: a host’s systemic inﬂammatory response syndrome

to infection. The manifestation by two or more of the four conditions of SIRS is considered to be

a SIRS case. We manually assign 1 point to each condition and assume a score of > 3 to be a

positive case.

The above scoring systems have been widely used by medical centers and researchers worldwide (Arabi et al.

2003). All the systems can be used at ICU admission or 24 hrs after admission for evaluation of the severity of

sickness and for mortality prediction. In general, higher scores indicate more severe health conditions and hence a

higher risk of mortality. We assume a risk probability greater than 0.5 to be at a high mortality risk for prediction

models such as SAPS II and LODS. Similarly, we consider a cutoﬀ score greater than 12, 2, and 3 to be at a high

mortality risk for SOFA, qSOFA, and SIRS, respectively.

D.1.3 Social Welfare Maximization

Table 6 represents the complete results of scoring systems both on training and test sets. All results on these sets

suggest that our methods can achieve the optimal social welfare compared to baseline scoring systems.

D.1.4 Interpretability of Scorecard

In medical applications, for example, the following statement of Than et al. (2014) describes the importance of
sensibility3 of produced models:

“An important consideration during development is the clinical sensibility of the resulting prediction

rule [...] Evaluation of sensibility requires judgment rather than statistical methods. A sensible rule is

easy to use, and has content and face validities. Prediction rules are unlikely to be applied in practice

if they are not considered sensible by the end-user, even if they are accurate.”

3As mentioned in Than et al. (2014), “sensibility” refers to whether a prediction rule is both clinically reasonable

and easy to use.

44

Table 6: The average values of total social welfare for all scoring systems on Sepsis data set.

Dataset

Fairness

¯ρ

Baselines

Ours

Notions

SAPS II

LODS

SOFA qSOFA

SIRS

FASS

FASS7

Training set

SP

EO

Sepsis

OMR

Test set

SP

EO

OMR

5

0.2

5

5

0.2

5

0.7024

0.7269

0.6298

0.6761

0.7102

0.5768

0.5904

0.5540

0.3805

0.5015

0.7124

0.7060

0.7217

0.7158

0.6167

0.5355

0.7665

0.7507

0.6601

0.6400

0.5871

0.3281

0.7442

0.7339

0.5368

0.5638

0.5089

0.4520

0.6916

0.6869

0.7198

0.7096

0.6278

0.5373

0.7550

0.7472

0.6393

0.5787

0.5509

0.4022

0.7405

0.7223

Note: Statistical parity is denoted by SP, equality of opportunity by EO, and equal overall

misclassiﬁcation rate by OMR. The optimal values are highted in bold.

Thus, it is necessary for the scorecard to be interpretable and reasonable for medical applications. As shown in

the main manuscript, the developed EO scorecard identiﬁed several risk factors that have been recognized by medical

research, which provides clinical sensibility. For example, it identiﬁes FiO2 (Fraction of inspired oxygen) with cut-oﬀ

values at around 0.8. This is consistent with the ﬁndings in Dahl et al. (2015) that the relative risk of mortality is

2.1 in patients with an average FiO2 ≥ 0.80 as compared to patients with an average FiO2 ≤ 0.40. Moreover, it also

points out the risk factors, like GCS (Glasgow Coma Scale) at 9. GCS reﬂects a patient’s degree of disturbance of

consciousness. A GCS below 9 is well-acknowledged as severe disturbance and is associated with higher death risk

(Kurowski et al. 2016). Other risk factors like age at 80, and potassium at 4.25 also play roles in the scorecard, and

their eﬀect on the prediction of ICU mortality has been demonstrated in Gogos et al. (2003), Solinger and Rothman

(2013), and Martin-Loeches et al. (2019).

In addition, the EO scorecard identiﬁes multiple cut-oﬀ values associated with variables, which are not yet

recognized in the literature. Examples include GCS at 5 and 7, bilirubin at 2.3 and 7.3, and pH.art at 7.2. This may

help reveal possible and complicated interactions between these sepsis-related variables. The interaction between

GCS and bilirubin, as suggested in the ﬁfth rule in our EO scorecard, may aﬀect the risk thresholds of both variables.

Recent evidence like Wang et al. (2020) argues that the level of bilirubin correlates with mortality in patients with

traumatic brain injury who always have lower GCS. Further, Sedlak and Snyder (2004) and Marconi et al. (2018)

point out that a high bilirubin level sometimes confers various health beneﬁts due to its antioxidant activity. For

pH.art, the study of Kraut and Madias (2010) suggests that metabolic acidosis might be beneﬁcial for oxygen delivery

and metabolism, and thus a slight upward adjustment of pH cut-oﬀ from 7.1 to 7.2 may not always be harmful, which

is also shown in the fourth rule of the EO scorecard.

The complete scorecards for all fairness metrics are displayed in Figure 7. Similar to the EO scorecard, the

SP and OMR scorecards also identify several sensible risk factors. All of these results indicate that the developed

scorecards can capture sensible risk-predictive rules. Furthermore, they help in revealing the complexity of sepsis by

discovering promising interactions between these variables, which may give insights to further medical research and

decision-making.

45

(a) Statistical Parity

(b) Equality of Opportunity

(c) Equal Overall Misclassiﬁcation Rate

Figure 7: Scorecards developed by FASS7 for Sepsis prediction.
Note: The training welfare of the SP scorecard and EO scorecard is 0.6896 and 0.7466, respectively. The scorecard

derived for equal OMR achieves a training welfare of 0.7277.

46

D.2 UCI Numerical Experiments

This section shows the detailed information and complete experimental results on UCI data sets.

D.2.1 Data Processing and Summary Statistics

Because Adult and German data sets are imbalanced (the positive rate in the raw data is 24% for the Adult data

set and 30% for the German data set), some sampling methods in imbalanced learning are applied to eliminate an

impact on the results. We used random undersampling on the Adult data set and SMOTENC (Chawla et al. 2002)

on the German data set to create balanced data sets for the purpose of comparing the relative performance between

classiﬁers. The summary statistics of ﬁnal data sets are shown in Table 7, where gender (with feature values: male

and female) is used as a sensitive feature.

Table 7: Summary of real UCI data sets

Data set Noriginal

doriginal

N

d Male% Sensitive feature

Adult

German

48, 842

1, 000

14

20

2, 000

3, 000

36

65

72.8%

68.1%

Gender (binary)

D.2.2 Social Welfare Maximization

We provide the total social welfare of all methods on training and test sets in Table 8. The complete results of the

data utility and fairness utility on Adult and German data sets are displayed in Figures 8 and 9, respectively. These

results verify again the eﬀectiveness of the proposed method.

47

Table 8: The average values of total social welfare for all methods on UCI data sets.

Dataset

Fairness

¯ρ

Baselines

Ours

Notions

Ridge

Lasso

Elasticnet

SVM Huberized SVM SLIM

FASS

Adult

Training set

SP

EO

OMR

Test set

SP

EO

OMR

0.2

0.5

0.5

0.2

0.5

0.5

Training set

0.7296

0.7299

0.7036

0.7018

0.7747

0.7728

0.7290

0.7005

0.7754

0.7190

0.6944

0.7604

0.7082

0.7070

0.6698

0.6643

0.7499

0.7532

0.7092

0.6682

0.7521

0.7030

0.6791

0.7345

SP

EO

German

OMR

Test set

0.2

0.7961

0.7998

5

5

0.6818

0.6915

0.6721

0.6875

0.7996

0.6845

0.6825

0.7938

0.6835

0.6818

SP

EO

OMR

0.2

0.7886

0.7938

5

5

0.5980

0.6004

0.6389

0.6565

0.7965

0.6009

0.6318

0.7872

0.6061

0.6534

0.7127

0.6686

0.7594

0.6968

0.6479

0.7412

0.7938

0.6904

0.6705

0.7876

0.5994

0.6350

0.7384

0.7626

0.7357

0.7959

0.7684

0.7938

0.7020

0.7468

0.6973

0.7821

0.7430

0.7707

0.7902

0.8105

0.6824

0.7604

0.6680

0.7591

0.7801

0.8089

0.4537

0.6659

0.6057

0.7240

Note: statistical parity is denoted by SP, equality of opportunity by EO, and equal overall misclassiﬁcation

rate by OMR. The optimal values are highted in bold.

48

(a) Training Set

Figure 8: Data utility and fairness utility with diﬀerent fairness measures on Adult data set.

(b) Test Set

49

 FASSEOData UtilityOMRFairness Utility FASSEOData UtilityOMRFairness Utility(a) Training Set

Figure 9: Data utility and fairness utility with diﬀerent fairness measures on German data set.

(b) Test Set

D.2.3 Impact of Average Preference for Fairness

The trade-oﬀs between accuracy and fairness with diﬀerent average preference for fairness on Adult and German data

sets are represented in Figures 10 and 11, respectively.

D.2.4 Classiﬁcation Model for Speciﬁed δ

Finally, we consider a simpliﬁed case where the fairness level δs is speciﬁed in advance, as discussed previously in

Section 4.2.1. In this situation, the data utility maximizing scoring system which satisﬁes the given fairness level is

developed. Tables 9 and 10 provide the data utility and (un)fairness level on training and test sets when the maximal

unfairness level is pre-speciﬁed on two data sets.

50

 FASSEOSPData UtilityOMRFairness Utility FASSSPEOData UtilityOMRFairness Utility(a) Training Set

(b) Test Set

Figure 10: Trade-oﬀs between accuracy and fairness on Adult data set in a randomly selected run.

Table 9: The average values of data utility and fairness level for all methods on Adult data set with δs = 0.05.

Metric

Ridge

Lasso

Elasticnet

SVM Huberized SVM SLIM FASS-EO FASS-OMR FASS-SP

Baselines

Ours

Training set

Data utility

0.8149

0.8129

EO level

0.1976

0.2139

OMR level

0.0715

0.0684

SP level

0.4033

0.4098

Test set

Data utility

0.8067

0.8050

EO level

0.1897

0.1897

OMR level

0.0828

0.0817

SP level

0.4259

0.4258

0.8141

0.2042

0.0703

0.4065

0.8047

0.1941

0.0822

0.4278

0.8020

0.2106

0.0925

0.4548

0.7866

0.1956

0.1086

0.4771

0.8040

0.2458

0.0815

0.4615

0.7876

0.2180

0.1073

0.4872

0.7986

0.0389

0.7927

0.0495

0.8114

0.1444

0.0740

0.3724

0.7930

0.1210

0.0966

0.3959

0.7996

0.7681

0.0440

0.0414

0.7880

0.7593

0.0599

0.0783

Note: Statistical parity is denoted by SP, equality of opportunity by EO, and equal overall misclassiﬁcation rate by

OMR. The optimal values are highted in bold.

51

0.00.10.20.30.40.50.700.750.800.85 Accuracy     (Un)fairness LevelAccuracy0.00.20.4SPFairness0.00.10.20.30.40.50.750.800.85Accuracy0.00.20.4EOFairness0.00.10.20.30.40.50.750.800.85Accuracy0.00.20.4OMRFairness0.00.10.20.30.40.50.700.760.82 Accuracy   (Un)fairness LevelAccuracy0.00.20.4SPFairness0.00.10.20.30.40.50.760.790.82Accuracy0.00.20.4EOFairness0.00.10.20.30.40.50.760.790.82Accuracy0.00.10.2OMRFairness(a) Training Set

(b) Test Set

Figure 11: Trade-oﬀs between accuracy and fairness on German data set in a randomly selected

run.

Table 10: The average values of data utility and fairness level for all methods on German data set with δs = 0.01.

Metric

Ridge

Lasso

Elasticnet

SVM Huberized SVM SLIM FASS-EO FASS-OMR FASS-SP

Baselines

Ours

Training set

Data utility

0.8261

0.8251

EO level

0.0227

0.0249

OMR level

0.0202

0.0210

SP level

0.1494

0.1294

0.8257

0.0203

0.0193

0.1280

Test set

Data utility

0.8187

0.8196

0.8207

EO level

0.0325

0.0372

OMR level

0.0293

0.0346

SP level

0.1624

0.1369

0.0366

0.0347

0.1384

0.8232

0.0242

0.0208

0.1269

0.8144

0.0341

0.0321

0.1455

0.8178

0.0215

0.0221

0.1272

0.8114

0.0523

0.0297

0.1488

0.7694

0.0035

0.7522

0.0188

0.8161

0.0371

0.0171

0.1115

0.8076

0.0480

0.0427

0.1115

0.8035

0.7969

0.0096

0.0076

0.8002

0.7993

0.0282

0.0334

Note: Statistical parity is denoted by SP, equality of opportunity by EO, and equal overall misclassiﬁcation rate by

OMR. The optimal values are highted in bold.

52

0.00.10.20.30.760.810.86 Accuracy     (Un)fairness LevelAccuracy0.00.10.2SPFairness0123450.790.810.83Accuracy0.000.030.06EOFairness0123450.7600.8050.850Accuracy0.000.020.04OMRFairness0.00.10.20.30.760.810.86 Accuracy   (Un)fairness LevelAccuracy0.000.150.30SPFairness0123450.7500.7750.800Accuracy0.000.030.06EOFairness0123450.760.810.86Accuracy0.000.030.06OMRFairnessIn the Adult data set, we ﬁrst experiment with baseline classiﬁers. As shown in Table 9, all baselines lead to

similar performance on overall data utility (around 0.8) on test set. However, these classiﬁers result in highly disparate

impact and disparate mistreatment for male and female groups. Speciﬁcally, the disparate impact is exceptionally

high since the minimum SP level between the two groups, achieved by SLIM, is 0.3959 on test set. Moreover, the

disparate mistreatment based on EO is also signiﬁcant since the minimum EO fairness level, again by SLIM, is

0.1210 on test set. In comparison, the disparate mistreatment based on OMR is relatively milder, where the best
level is 0.0817 using Lasso. To harness these disparities, we set δs = 0.05 and develop FASS scoring systems with

diﬀerent fairness metrics. Results show that our models only sacriﬁce a small degree of data utility to remedy the

disparities. As shown in Table 9, our systems strictly limit the unfairness level to less than 0.05 on the training set

and signiﬁcantly reduce the disparity levels on the test set. For example, FASS-EO decreases disparate mistreatment

based on EO to 0.0495 on test set while maintaining high accuracy close to the baselines (it even outperforms two

SVMs on accuracy, besides decreasing unfairness level).

For the German data, the phenomenon of disparity is less severe compared to the Adult data, thus δs is set as

0.01 in this scenario. As shown in Table 10, the disparate impact, again, is the most signiﬁcant since the minimum SP

rate diﬀerence of baselines is 0.1115 by SLIM on test sets. However, our approach can limit this value to 0.0076 < 0.01

on the training set and reduce it to 0.0334 on the test set while guaranteeing a competitive accuracy. In contrast to

the disparate impact, the disparate mistreatment issue is less of a concern on German data, and our framework still

outperforms the baselines on fairness levels for both EO and OMR metrics.

The details regarding disparities between the two groups on Adult and German data sets are presented in Figures

12 and 13, respectively. As can be seen from these ﬁgures, the rate gap between the two groups is substantially

narrowed in our framework compared to baselines, and thus our scoring system achieves a better fairness level with

all fairness measures.

Overall, the experimental results on UCI data sets demonstrate the eﬀectiveness of the proposed methods when

the maximal fairness level is speciﬁed in advance. In this case, our approach can achieve a better fairness level for

all fairness metrics compared to baselines, with only a moderate loss in data utility (accuracy).

References

Alesina, A. F., Lotti, F., and Mistrulli, P. E. (2013). Do women pay more for credit? Evidence from Italy.

Journal of the European Economic Association, 11(1):45–66.

Angus, D. C., Linde-Zwirble, W. T., Lidicker, J., Clermont, G., Carcillo, J., and Pinsky, M. R. (2001).

Epidemiology of severe sepsis in the United States: Analysis of incidence, outcome, and associated

costs of care. Critical Care Medicine, 29(7):1303–1310.

Antman, E. M., Cohen, M., Bernink, P. J. L. M., McCabe, C. H., Horacek, T., Papuchis, G., Mautner, B.,

Corbalan, R., Radley, D., and Braunwald, E. (2000). The TIMI Risk Score for unstable angina/non–ST

elevation MI: A method for prognostication and therapeutic decision making. JAMA, 284(7):835–842.

Arabi, Y., Al Shirawi, N., Memish, Z., Venkatesh, S., and Al-Shimemeri, A. (2003). Assessment of six

mortality prediction models in patients admitted with severe sepsis and septic shock to the intensive

care unit: A prospective cohort study. Critical Care, 7(5):1–7.

Atkinson, A. B. et al. (1970). On the measurement of inequality. Journal of Economic Theory, 2(3):244–263.

53

(a) Training Set

(b) Test Set

Figure 12: Illustration of disparities for male and female subgroups on Adult data set.

(a) Training Set

(b) Test Set

Figure 13: Illustration of disparities for male and female subgroups on German data set.

54

RidgeLassoElasticnetSVMHuberized SVMSLIMFASS-SP0.00.20.40.60.81.0FASS-SPPredicted Positive RateRidgeLassoElasticnetSVMHuberized SVMSLIMFASS-EO0.00.20.40.60.81.0FASS-EOTrue Positive RateRidgeLassoElasticnetSVMHuberized SVMSLIMFASS-OMR0.00.10.20.30.4FASS-OMROverall Misclassification Rate Male      FemaleRidgeLassoElasticnetSVMHuberized SVMSLIMFASS-SP0.00.20.40.60.81.0Predicted Positive RateFASS-SPRidgeLassoElasticnetSVMHuberized SVMSLIMFASS-EO0.00.20.40.60.81.0True Positive RateFASS-EORidgeLassoElasticnetSVMHuberized SVMSLIMFASS-OMR0.00.10.20.30.4Overall Misclassification RateFASS-OMR Male      FemaleAziz, H., Caragiannis, I., Igarashi, A., and Walsh, T. (2019). Fair allocation of indivisible goods and chores.

In International Joint Conference on Artiﬁcial Intelligence, pages 53–59.

Azizi, M. J., Vayanos, P., Wilder, B., Rice, E., and Tambe, M. (2018). Designing fair, eﬃcient, and

interpretable policies for prioritizing homeless youth for housing resources. In International Conference

on the Integration of Constraint Programming, Artiﬁcial Intelligence, and Operations Research, pages

35–51.

Babcock, L., Wang, X., and Loewenstein, G. (1996). Choosing the wrong pond: Social comparisons in

negotiations that reﬂect a self-serving bias. The Quarterly Journal of Economics, 111(1):1–19.

Bandiera, O., Barankay, I., and Rasul, I. (2005). Social preferences and the response to incentives: Evidence

from personnel data. The Quarterly Journal of Economics, 120(3):917–962.

Barocas, S., Hardt, M., and Narayanan, A. (2017). Fairness in machine learning. Nips tutorial, 1:2017.

Barocas, S. and Selbst, A. D. (2016). Big data’s disparate impact. California Law Review, 104:671–732.

Benini, A. A., Conley, C. E., Shdeed, R., Spurway, K., and Yarmoshuk, M. (2003). Integration of diﬀerent

data bodies for humanitarian decision support: An example from mine action. Disasters, 27(4):288–304.

Benjamin, D. J., Choi, J. J., and Strickland, A. J. (2010). Social identity and preferences. American

Economic Review, 100(4):1913–28.

Berk, R., Heidari, H., Jabbari, S., Kearns, M., and Roth, A. (2021). Fairness in criminal justice risk

assessments: The state of the art. Sociological Methods & Research, 50(1):3–44.

Bertsimas, D., Farias, V. F., and Trichakis, N. (2013). Fairness, eﬃciency, and ﬂexibility in organ allocation

for kidney transplantation. Operations Research, 61(1):73–87.

Bierman, A. S. (2007). Sex matters: Gender disparities in quality and outcomes of care. Canadian Medical

Association Journal, 177(12):1520–1521.

Bjarnadottir, M., Anderson, D., Zia, L., and Rhoads, K. (2018). Predicting colorectal cancer mortality:

Models to facilitate patient-physician conversations and inform operational decision making. Production

and Operations Management, 27(12):2162–2183.

Bolton, G. E. and Ockenfels, A. (2000). ERC: A theory of equity, reciprocity, and competition. American

Economic Review, 90(1):166–193.

Bone, R. C., Balk, R. A., Cerra, F. B., Dellinger, R. P., Fein, A. M., Knaus, W. A., Schein, R. M., and

Sibbald, W. J. (1992). Deﬁnitions for sepsis and organ failure and guidelines for the use of innovative

therapies in sepsis. Chest, 101(6):1644–1655.

Brayne, S. (2014). Surveillance and system avoidance: Criminal justice contact and institutional attachment.

American Sociological Review, 79(3):367–391.

Brooks, J. P. (2011). Support vector machines with the ramp loss and the hard margin loss. Operations

Research, 59(2):467–479.

Burgess, E. W. (1928). Factors determining success or failure on parole. Technical report, Illinois Committee

on Indeterminate-Sentence Law and Parole.

55

Campbell, C. A., D’Amato, C., and Papp, J. (2020). Validation of the Ohio Youth Assessment System

Dispositional Tool (OYAS-DIS): An examination of race and gender diﬀerences. Youth Violence and

Juvenile Justice, 18(2):196–211.

Capon, N. (1982). Credit scoring systems: A critical analysis. Journal of Marketing, 46(2):82–91.

Celis, L. E., Huang, L., Keswani, V., and Vishnoi, N. K. (2019). Classiﬁcation with fairness constraints: A

meta-algorithm with provable guarantees. In Proceedings of the Conference on Fairness, Accountability,

and Transparency, page 319–328.

Charness, G. and Rabin, M. (2002). Understanding social preferences with simple tests. The Quarterly

Journal of Economics, 117(3):817–869.

Chawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer, W. P. (2002). SMOTE: Synthetic minority

over-sampling technique. Journal of Artiﬁcial Intelligence Research, 16:321–357.

Chen, E. H., Shofer, F. S., Dean, A. J., Hollander, J. E., Baxt, W. G., Robey, J. L., Sease, K. L., and Mills,

A. M. (2008). Gender disparity in analgesic treatment of emergency department patients with acute

abdominal pain. Academic Emergency Medicine, 15(5):414–418.

Chen, T., Huang, Y., Lin, C., and Sheng, Z. (2021). Finance and Firm Volatility: Evidence from Small

Business Lending in China. Management Science, page (forthcoming).

Chi, B.-W. and Hsu, C.-C. (2012). A hybrid approach to integrate genetic algorithm into dual scoring model

in enhancing the performance of credit scoring model. Expert Systems with Applications, 39(3):2650–

2661.

Choi, T.-M., Wallace, S. W., and Wang, Y. (2018). Big data analytics in operations management. Production

and Operations Management, 27(10):1868–1883.

Chouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction

instruments. Big Data, 5(2):153–163.

Coﬀman, K. B., Exley, C. L., and Niederle, M. (2021). The role of beliefs in driving gender discrimination.

Management Science, page (forthcoming).

Cohen, M. C. (2018). Big data and service operations. Production and Operations Management, 27(9):1709–

1723.

Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., and Huq, A. (2017). Algorithmic decision making and

the cost of fairness. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge

Discovery and Data Mining, pages 797–806.

Cox, J. C. and Sadiraj, V. (2012). Direct tests of individual preferences for eﬃciency and equity. Economic

Inquiry, 50(4):920–931.

Dahl, R. M., Grønlykke, L., Haase, N., Holst, L. B., Perner, A., Wetterslev, J., Rasmussen, B. S., Mey-

hoﬀ, C. S., and the 6S-Trial and TRISS Trial investigators (2015). Variability in targeted arterial

oxygenation levels in patients with severe sepsis or septic shock. Acta Anaesthesiologica Scandinavica,

59(7):859–869.

Dawes, C. T., Fowler, J. H., Johnson, T., Mcelreath, R., and Smirnov, O. (2007). Egalitarian motives in

humans. Nature, 446:794–796.

56

Dellinger, R. P., Levy, M. M., Rhodes, A., Annane, D., Gerlach, H., Opal, S. M., Sevransky, J. E., Sprung,

C. L., Douglas, I. S., Jaeschke, R., et al. (2013). Surviving Sepsis Campaign: International guidelines

for management of severe sepsis and septic shock, 2012. Intensive Care Medicine, 39(2):165–228.

Donini, M., Oneto, L., Ben-David, S., Shawe-Taylor, J., and Pontil, M. (2018). Empirical risk minimization

under fairness constraints. In Proceedings of the 32nd International Conference on Neural Information

Processing Systems, page 2796–2806.

Dover, T. L., Major, B., Kunstman, J. W., and Sawyer, P. J. (2015). Does unfairness feel diﬀerent if it

can be linked to group membership? Cognitive, aﬀective, behavioral and physiological implications of

discrimination and unfairness. Journal of Experimental Social Psychology, 56:96–103.

Dua, D. and Graﬀ, C. (2017). UCI machine learning repository.

Dumitrescu, E., Hue, S., Hurlin, C., and Tokpavi, S. (2021). Machine learning for credit scoring: Improving

logistic regression with non-linear decision-tree eﬀects. European Journal of Operational Research, page

(forthcoming).

Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel, R. (2012). Fairness through awareness.

In

Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, page 214–226.

Fehr, E. and Schmidt, K. M. (1999). A theory of fairness, competition, and cooperation. The Quarterly

Journal of Economics, 114(3):817–868.

Fleurbaey, M. and Maniquet, F. (2008). Utilitarianism versus fairness in welfare economics.

In Justice,

political liberalism, and utilitarianism: Themes from Harsanyi and Rawls, pages 263–280. Cambridge

University Press.

Foley, D. K. (1967). Resource Allocation and the Public Sector, volume 7. Yale Economics Essays.

Friedman, J. H., Popescu, B. E., et al. (2008). Predictive learning via rule ensembles. Annals of Applied

Statistics, 2(3):916–954.

Fu, R., Huang, Y., and Singh, P. V. (2020). Artiﬁcial intelligence and algorithmic bias: Source, detection,

mitigation, and implications. In Pushing the Boundaries: Frontiers in Impactful OR/OM Research,

pages 39–63. INFORMS.

Fu, R., Huang, Y., and Singh, P. V. (2021). Crowds, lending, machine, and bias. Information Systems

Research, 32(1):72–92.

Gal, Y., Mash, M., Procaccia, A. D., and Zick, Y. (2017). Which is the fairest (rent division) of them all?

Journal of the ACM (JACM), 64(6):1–22.

Gogos, C. A., Lekkou, A., Papageorgiou, O., Siagris, D., Skoutelis, A., and Bassaris, H. P. (2003). Clinical

prognostic markers in patients with severe sepsis: A prospective analysis of 139 consecutive cases.

Journal of Infection, 47(4):300–306.

Grgic-Hlaca, N., Zafar, M. B., Gummadi, K. P., and Weller, A. (2016). The case for process fairness in

learning: Feature selection for fair decision making. In NIPS symposium on machine learning and the

law, volume 1, pages 2–11.

Hamman, J. R., Loewenstein, G., and Weber, R. A. (2010). Self-interest through delegation: An additional

rationale for the principal-agent relationship. American Economic Review, 100(4):1826–46.

57

Hardt, M., Price, E., and Srebro, N. (2016). Equality of opportunity in supervised learning. In Proceedings

of the 30th International Conference on Neural Information Processing Systems, page 3323–3331.

Harpviken, K. B., Millard, A. S., Kjellman, K. E., and Skara, B. A. (2003). Measures for mines: approaches

to impact assessment in humanitarian mine action. Third World Quarterly, 24(5):889–908.

Harris, D. G., Li, S., Pensyl, T., Srinivasan, A., and Trinh, K. (2019). Approximation algorithms for

stochastic clustering. Journal of Machine Learning Research, 20:1–33.

Henrich, J., Boyd, R., Bowles, S., Camerer, C., Fehr, E., Gintis, H., and McElreath, R. (2001). In search

of homo economicus: Behavioral experiments in 15 small-scale societies. American Economic Review,

91(2):73–78.

Henry, K. E., Hager, D. N., Pronovost, P. J., and Saria, S. (2015). A targeted real-time early warning score

(TREWScore) for septic shock. Science Translational Medicine, 7(299):299ra122.

Hoﬀman, P. B. (1983). Screening for risk: A revised salient factor score (SFS 81). Journal of Criminal

Justice, 11(6):539–547.

Hoﬀman, P. B. (1994). Twenty years of operational use of a risk prediction instrument: The United States

Parole Commission’s Salient Factor Score. Journal of Criminal Justice, 22(6):477–494.

Hoﬀman, P. B. and Beck, J. L. (1997). The origin of the federal criminal history score. Federal Sentencing

Reporter, 9(4):192–197.

Hoi, S. C., Wang, J., and Zhao, P. (2014). Libol: A library for online learning algorithms. Journal of

Machine Learning Research, 15(1):495.

Hossain, S., Mladenovic, A., and Shah, N. (2020). Designing fairly fair classiﬁers via economic fairness

notions. In Proceedings of The Web Conference 2020, page 1559–1569.

Hu, L. and Chen, Y. (2018). A short-term intervention for long-term fairness in the labor market.

In

Proceedings of the 2018 World Wide Web Conference, page 1389–1398.

Hurley, M. and Adebayo, J. (2016). Credit scoring in the era of big data. Yale Journal of Law and Technology,

18:148–216.

Johnson, A. E., Pollard, T. J., Shen, L., Li-Wei, H. L., Feng, M., Ghassemi, M., Moody, B., Szolovits, P.,

Celi, L. A., and Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database. Scientiﬁc

Data, 3(1):1–9.

Kallus, N., Mao, X., and Zhou, A. (2021). Assessing algorithmic fairness with unobserved protected class

using data combination. Management Science, page (forthcoming).

Karlan, D. and Zinman, J. (2011). Microcredit in theory and practice: Using randomized credit scoring for

impact evaluation. Science, 332(6035):1278–84.

Kleinberg, J., Mullainathan, S., and Raghavan, M. (2017). Inherent trade-oﬀs in the fair determination of

risk scores. In The 8th Innovations in Theoretical Computer Science Conference, volume 67, pages

43:1–43:23.

Knaus, W. A., Draper, E. A., Wagner, D. P., and Zimmerman, J. E. (1985). APACHE II: A severity of

disease classiﬁcation system. Critical Care Medicine, 13(10):818–829.

58

Knaus, W. A., Wagner, D. P., Draper, E. A., Zimmerman, J. E., and Damiano, A. M. (1991). The APACHE

III prognostic system: Risk prediction of hospital mortality for critically ill hospitalized adults. Chest,

100(6):1619–1636.

Knaus, W. A., Zimmerman, J. E., Wagner, D. P., Draper, E. A., and Lawrence, D. E. (1981). APACHE-

acute physiology and chronic health evaluation: A physiologically based classiﬁcation system. Critical

Care Medicine, 9(8):591.

Kozodoi, N., Jacob, J., and Lessmann, S. (2021). Fairness in credit scoring: Assessment, implementation

and proﬁt implications. European Journal of Operational Research, page (forthcoming).

Kramer, J. H. and Scirica, A. J. (1986). Complex policy choices: The Pennsylvania commission on sentencing.

Federal Probation, 50:15.

Kraut, J. A. and Madias, N. E. (2010). Metabolic acidosis: Pathophysiology, diagnosis and management.

Nature Reviews Nephrology, 6(5):274.

Kurowski, A., Szarpak, (cid:32)L., Frass, M., Samarin, S., and Czyzewski, (cid:32)L. (2016). GCS scale used as a prognostic

factor in unconscious patients following cardiac arrest in prehospital situations: Preliminary data.

American Journal of Emergency Medicine, 34(6):1178–1179.

Lambrecht, A. and Tucker, C. (2019). Algorithmic bias? An empirical study of apparent gender-based

discrimination in the display of STEM career ads. Management Science, 65(7):2966–2981.

Lang, J. and Rothe, J. (2016). Fair division of indivisible goods. In Economics and Computation, pages

493–550. Springer.

Latessa, E., Smith, P., Lemke, R., Makarios, M., and Lowenkamp, C. (2009). Creation and validation of

the Ohio risk assessment system: Final report. Technical report, Center for Criminal Justice Research,

School of Criminal Justice, University of Cincinnati, Cincinnati, OH.

Le Gall, J.-R., Klar, J., Lemeshow, S., Saulnier, F., Alberti, C., Artigas, A., and Teres, D. (1996). The

Logistic Organ Dysfunction System: A new way to assess organ dysfunction in the intensive care unit.

JAMA, 276(10):802–810.

Le Gall, J.-R., Lemeshow, S., and Saulnier, F. (1993). A new simpliﬁed acute physiology score (SAPS II)

based on a European/North American multicenter study. JAMA, 270(24):2957–2963.

Le Gall, J.-R., Loirat, P., Alperovitch, A., Glaser, P., Granthil, C., Mathieu, D., Mercier, P., Thomas, R.,

and Villers, D. (1984). A simpliﬁed acute physiology score for ICU patients. Critical Care Medicine,

12(11):975–977.

Lohaus, M., Perrot, M., and Von Luxburg, U. (2020). Too relaxed to be fair. In International Conference

on Machine Learning, pages 6360–6369.

Lowder, E. M., Lawson, S. G., Grommon, E., and Ray, B. R. (2020). Five-county validation of the

Indiana Risk Assessment System–Pretrial Assessment Tool (IRAS-PAT) using a local validation ap-

proach. Justice Quarterly, 37(7):1241–1260.

Marconi, V. C., Duncan, M. S., So-Armah, K., Re 3rd, V. L., Lim, J. K., Butt, A. A., Goetz, M. B.,

Rodriguez-Barradas, M. C., Alcorn, C. W., Lennox, J., et al. (2018). Bilirubin is inversely associated

59

with cardiovascular disease among HIV-positive and HIV-negative individuals in VACS (Veterans Aging

Cohort Study). Journal of the American Heart Association, 7(10):e007792.

Martin-Loeches, I., Guia, M. C., Vallecoccia, M. S., Suarez, D., Ibarz, M., Irazabal, M., Ferrer, R., and

Artigas, A. (2019). Risk factors for mortality in elderly and very elderly critically ill patients with

sepsis: A prospective, observational, multicenter cohort study. Annals of Intensive Care, 9(1):1–9.

Maxwell, D., Caldwell, R., and Langworthy, M. (2008). Measuring food insecurity: Can an indicator based

on localized coping behaviors be used to compare across contexts? Food Policy, 33(6):533–540.

McCoy, J. H. and Lee, H. L. (2014). Using fairness models to improve equity in health delivery ﬂeet

management. Production and Operations Management, 23(6):965–977.

McMahan, H. B. (2017). A survey of algorithms and analysis for adaptive online learning. Journal of

Machine Learning Research, 18(1):3117–3166.

McNamara, C. P. (1972). The present status of the marketing concept. Journal of Marketing, 36(1):50–57.

Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., and Galstyan, A. (2021). A survey on bias and fairness

in machine learning. ACM Computing Surveys, 54(6):1–35.

Miller, G. A. (1956). The magical number seven, plus or minus two: Some limits on our capacity for

processing information. Psychological Review, 63(2):81.

Molnar, C. (2020). Interpretable machine learning. Lulu.com.

Moreno, R. P., Metnitz, P. G., Almeida, E., Jordan, B., Bauer, P., Campos, R. A., Iapichino, G., Edbrooke,

D., Capuzzo, M., and Le Gall, J.-R. (2005). SAPS 3-From evaluation of the patient to evaluation

of the intensive care unit. Part 2: Development of a prognostic model for hospital mortality at ICU

admission. Intensive Care Medicine, 31(10):1345–1355.

Mukherjee, V. and Evans, L. (2017). Implementation of the surviving sepsis campaign guidelines. Current

Opinion in Critical Care, 23(5):412–416.

Ndirangu, M., Sachs, S. E., Palm, C., and Deckelbaum, R. J. (2013). HIV aﬀected households in Western

Kenya experience greater food insecurity. Food Policy, 42:11–17.

Northpointe (2015). Practitioner’s Guide to COMPAS Core. Technical report, Northpointe Inc.

Obermeyer, Z., Powers, B., Vogeli, C., and Mullainathan, S. (2019). Dissecting racial bias in an algorithm

used to manage the health of populations. Science, 366(6464):447–453.

Procaccia, A. D. (2013). Cake cutting: Not just child’s play. Communications of the ACM, 56(7):78–87.

Quadrianto, N. and Sharmanska, V. (2017). Recycling privileged learning and distribution matching for

fairness. In Advances in Neural Information Processing Systems, volume 30, pages 677–688.

Rabin, M. (1993).

Incorporating fairness into game theory and economics. American Economic Review,

83(5):1281–1302.

Rajkomar, A., Hardt, M., Howell, M. D., Corrado, G., and Chin, M. H. (2018). Ensuring fairness in machine

learning to advance health equity. Annals of Internal Medicine, 169(12):866–872.

Rawls, J. (1999). A Theory of Justice: Revised Edition. Harvard University Press.

60

Rea, D., Froehle, C., Masterson, S., Stettler, B., Fermann, G., and Pancioli, A. (2021). Unequal but

fair: Incorporating distributive justice in operational allocation models. Production and Operations

Management, page (forthcoming).

Reilly, B. M. and Evans, A. T. (2006). Translating clinical research into clinical practice: Impact of using

prediction rules to make decisions. Annals of Internal Medicine, 144(3):201–209.

Robertson, J. and Webb, W. (1998). Cake-cutting Algorithms: Be Fair if You Can. CRC Press.

Samorani, M., Harris, S. L., Blount, L. G., Lu, H., and Santoro, M. A. (2021). Overbooked and over-

looked: Machine learning and racial bias in medical appointment scheduling. Manufacturing & Service

Operations Management, page (forthcoming).

Santana, A. R., de Sousa, J. L., Amorim, F. F., Menezes, B. M., Ara´ujo, F. V. B., Soares, F. B., de Car-

valho Santos, L. C., de Ara´ujo, M. P. B., Rocha, P. H. G., J´unior, P. N. F., et al. (2013). SaO2/FiO2

ratio as risk stratiﬁcation for patients with sepsis. Critical Care, 17(4):1–59.

Sedlak, T. W. and Snyder, S. H. (2004). Bilirubin beneﬁts: Cellular protection by a biliverdin reductase

antioxidant cycle. Pediatrics, 113(6):1776–1782.

Siddiqi, N. (2012). Credit Rsk Scorecards: Developing and Implementing Intelligent Credit Scoring, volume 3.

John Wiley & Sons.

Singer, M., Deutschman, C. S., Seymour, C. W., Shankar-Hari, M., Annane, D., Bauer, M., Bellomo, R.,

Bernard, G. R., Chiche, J.-D., Coopersmith, C. M., et al. (2016). The third international consensus

deﬁnitions for sepsis and septic shock (Sepsis-3). JAMA, 315(8):801–810.

Six, A., Backus, B., and Kelder, J. (2008). Chest pain in the emergency room: value of the HEART score.

Netherlands Heart Journal, 16(6):191–196.

Skouﬁas, E., Diamond, A., Vinha, K., Gill, M., and Dellepiane, M. R. (2020). Estimating poverty rates in

subnational populations of interest: An assessment of the Simple Poverty Scorecard. World Develop-

ment, 129:104887.

Solinger, A. B. and Rothman, S. I. (2013). Risks of mortality associated with common laboratory tests: A

novel, simple and meaningful way to set decision limits from data available in the electronic medical

record. Clinical Chemistry and Laboratory Medicine, 51(9):1803–1813.

Souder, W. E. (1972). A scoring methodology for assessing the suitability of management science models.

Management Science, 18(10):B526–B543.

Stevenson, M. (2018). Assessing risk assessment in action. Minnesota Law Review, 103:303.

Strand, K. and Flaatten, H. (2008). Severity scoring in the ICU: a review. Acta Anaesthesiologica Scandi-

navica, 52(4):467–478.

Struck, A. F., Ustun, B., Ruiz, A. R., Lee, J. W., LaRoche, S. M., Hirsch, L. J., Gilmore, E. J., Vlachy, J.,

Haider, H. A., Rudin, C., et al. (2017). Association of an electroencephalography-based risk score with

seizure probability in hospitalized patients. JAMA Neurology, 74(12):1419–1424.

Subbe, C., Kruger, M., Rutherford, P., and Gemmel, L. (2001). Validation of a modiﬁed Early Warning

Score in medical admissions. QJM: An International Journal of Medicine, 94(10):521–526.

61

Sveen, J., Bondjers, K., and Willebrand, M. (2016). Psychometric properties of the PTSD Checklist for

DSM-5: a pilot study. European Journal of Psychotraumatology, 7(1):30165.

Sweeney, T. E., Perumal, T. M., Henao, R., Nichols, M., Howrylak, J. A., Choi, A. M., Bermejo-Martin,

J. F., Almansa, R., Tamayo, E., and Davenport, E. E. (2018). A community approach to mortality

prediction in sepsis via gene expression analysis. Nature Communications, 9(1):694.

Than, M., Flaws, D., Sanders, S., Doust, J., Glasziou, P., Kline, J., Aldous, S., Troughton, R., Reid, C.,

Parsonage, W. A., Frampton, C., Greenslade, J. H., Deely, J. M., Hess, E., Sadiq, A. B., Singleton,

R., Shopland, R., Vercoe, L., Woolhouse-Williams, M., Ardagh, M., Bossuyt, P., Bannister, L., and

Cullen, L. (2014). Development and validation of the emergency department assessment of chest pain

score and 2h accelerated diagnostic protocol. Emergency Medicine Australasia, 26(1):34–44.

Thomas, L., Crook, J., and Edelman, D. (2017). Credit Scoring and Its Applications. SIAM.

Tricomi, E., Rangel, A., Camerer, C. F., and O’Doherty, J. P. (2010). Neural evidence for inequality-averse

social preferences. Nature, 463:1089–1091.

Ustun, B. and Rudin, C. (2016). Supersparse linear integer models for optimized medical scoring systems.

Machine Learning, 102(3):349–391.

Vigdor, N. (2019). Apple card investigated after gender discrimination complaints. The New York Times.

Vincent, J.-L., Moreno, R., Takala, J., Willatts, S., De Mendon¸ca, A., Bruining, H., Reinhart, C., Suter, P.,

and Thijs, L. G. (1996). The SOFA (Sepsis-related Organ Failure Assessment) score to describe organ

dysfunction/failure. Intensive Care Medicine, 22(7):707–710.

Wang, G., Li, J., and Hopp, W. J. (2021). An instrumental variable forest approach for detecting heteroge-

neous treatment eﬀects in observational studies. Management Science, page (forthcoming).

Wang, R., He, M., and Xu, J. (2020). Serum bilirubin level correlates with mortality in patients with

traumatic brain injury. Medicine, 99(27):e21020.

Wick, M., Panda, S., and Tristan, J.-B. (2019). Unlocking fairness: A trade-oﬀ revisited. In Proceedings of

the 33rd International Conference on Neural Information Processing Systems, pages 8783–8792.

Wolsey, L. A. (1998). Integer Programming, volume 52. John Wiley & Sons.

Woodworth, B., Gunasekar, S., Ohannessian, M. I., and Srebro, N. (2017). Learning non-discriminatory

predictors. In Proceedings of the 2017 Conference on Learning Theory, volume 65, pages 1920–1953.

Wu, Y., Huang, S., and Chang, X. (2021). Understanding the complexity of sepsis mortality prediction via

rule discovery and analysis: A pilot study. BMC Medical Informatics and Decision Making, 21(1):1–15.

Yeh, I.-C., Yang, K.-J., and Ting, T.-M. (2009). Knowledge discovery on RFM model using Bernoulli

sequence. Expert Systems with Applications, 36(3):5866–5871.

Yona, G. and Rothblum, G. (2018). Probably approximately metric-fair learning. In Proceedings of the 35th

International Conference on Machine Learning, volume 80, pages 5680–5688.

Zafar, M. B., Valera, I., Gomez Rodriguez, M., and Gummadi, K. P. (2017a). Fairness beyond disparate

treatment & disparate impact: Learning classiﬁcation without disparate mistreatment. In Proceedings

of the 26th International Conference on World Wide Web, page 1171–1180.

62

Zafar, M. B., Valera, I., Gomez-Rodriguez, M., and Gummadi, K. P. (2019). Fairness constraints: A ﬂexible

approach for fair classiﬁcation. Journal of Machine Learning Research, 20(75):1–42.

Zafar, M. B., Valera, I., Rogriguez, M. G., and Gummadi, K. P. (2017b). Fairness constraints: Mechanisms

for fair classiﬁcation. In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and

Statistics, volume 54, pages 962–970.

Zardari, N. u. H., Cordery, I., and Sharma, A. (2010). An objective multiattribute analysis approach for

allocation of scarce irrigation water resources. Journal of the American Water Resources Association,

46(2):412–428.

Zeng, J., Ustun, B., and Rudin, C. (2017).

Interpretable classiﬁcation models for recidivism prediction.

Journal of the Royal Statistical Society: Series A (Statistics in Society), 3(180):689–722.

Zhang, X., Khalili, M. M., Tekin, C., and Liu, M. (2019). Group retention when using machine learning in

sequential decision making: The interplay between user dynamics and fairness. In Proceedings of the

33rd International Conference on Neural Information Processing Systems.

Zhang, Y., Bradlow, E. T., and Small, D. S. (2015). Predicting customer value using clumpiness: From RFM

to RFMC. Marketing Science, 34(2):195–208.

Zhao, H. and Gordon, G. (2019). Inherent tradeoﬀs in learning fair representations. In Advances in Neural

Information Processing Systems, volume 32, pages 15675–15685.

Zou, J., Lederer, D. J., and Rabinowitz, D. (2020). Eﬃciency in lung transplant allocation strategies. Annals

of Applied Statistics, 14(3):1088–1121.

Zou, J. and Schiebinger, L. (2018). AI can be sexist and racist—it’s time to make it fair. Nature, 559:324–326.

63

