2
2
0
2

n
a
J

8

]

C
O
.
h
t
a
m

[

2
v
2
5
4
7
0
.
0
1
0
2
:
v
i
X
r
a

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

Near Optimality of Finite Memory Feedback Policies in Partially
Observed Markov Decision Processes

Ali Devran Kara
Department of Mathematics
University of Michigan
Ann Arbor, MI 48109-1043, USA

Serdar Y ¨uksel
Department of Mathematics and Statistics
Queen’s University
Kingston, ON, Canada,

Editor: -

ALIKARA@UMICH.EDU

YUKSEL@QUEENSU.CA

Abstract

In the theory of Partially Observed Markov Decision Processes (POMDPs), existence of optimal
policies have in general been established via converting the original partially observed stochastic
control problem to a fully observed one on the belief space, leading to a belief-MDP. However,
computing an optimal policy for this fully observed model, and so for the original POMDP, using
classical dynamic or linear programming methods is challenging even if the original system has
ﬁnite state and action spaces, since the state space of the fully observed belief-MDP model is
always uncountable. Furthermore, there exist very few rigorous value function approximation and
optimal policy approximation results, as regularity conditions needed often require a tedious study
involving the spaces of probability measures leading to properties such as Feller continuity. In this
paper, we study a planning problem for POMDPs where the system dynamics and measurement
channel model are assumed to be known. We construct an approximate belief model by discretizing
the belief space using only ﬁnite window information variables. We then ﬁnd optimal policies for
the approximate model and we rigorously establish near optimality of the constructed ﬁnite window
control policies in POMDPs under mild non-linear ﬁlter stability conditions and the assumption
that the measurement and action sets are ﬁnite (and the state space is real vector valued). We
also establish a rate of convergence result which relates the ﬁnite window memory size and the
approximation error bound, where the rate of convergence is exponential under explicit and testable
exponential ﬁlter stability conditions. While there exist many experimental results and few rigorous
asymptotic convergence results, an explicit rate of convergence result is new in the literature, to our
knowledge.

Keywords: POMDPs, nonlinear ﬁlters, stochastic control

1. Introduction

For Partially Observed Stochastic Control, also known as Partially Observed Markov Decision Prob-
lems (POMDPs), existence of optimal policies have in general been established via converting the
original partially observed stochastic control problem to a fully observed Markov Decision Problem
(MDP) one on the belief space, leading to a belief-MDP. However, computing an optimal policy
for this fully observed model, and so for the original POMDP, using classical methods (such as

1

 
 
 
 
 
 
KARA AND Y ¨UKSEL

dynamic programming, policy iteration, linear programming) is challenging even if the original
system has ﬁnite state and action spaces, since the state space of the fully observed model is always
uncountable.

In the MDP theory, various methods have been developed to compute approximately optimal
policies by reducing the original problem into a simpler one. A partial list of these techniques is
as follows: approximate dynamic programming, approximate value or policy iteration, simulation-
based techniques, neuro-dynamic programming (or reinforcement learning), state aggregation, etc.
(Dufour and Prieto-Rumeau, 2012; Bertsekas, 1975; Chow and Tsitsiklis, 1991). Saldi et al. (2017)
investigated ﬁnite action and state approximations of fully observed stochastic control problems
with general state and action spaces under the discounted cost and average cost optimality criteria
where weak continuity conditions were shown to be sufﬁcient for near optimality of ﬁnite state
approximations in the sense that optimal policies obtained from these models asymptotically achieve
the optimal cost for the original problem under the weak continuity assumption on the controlled
transition kernel.

On POMDPs, however, the problem of approximation is signiﬁcantly more challenging. Most
of the studies in the literature are algorithmic and computational contributions (Porta et al., 2006;
Zhou and Hansen, 2001). These studies develop computational algorithms, utilizing structural con-
vexity/concavity properties of the value function under the discounted cost criterion. Vlassis and
Spaan (2005) provide an insightful algorithm which may be regarded as a quantization of the belief
space; however, no rigorous convergence results are provided. Smith and Simmons (2012); Pineau
et al. (2006) also present quantization based algorithms for the belief state, where the state, mea-
surement, and the action sets are ﬁnite. Zhang et al. (2014) also provides a computationally efﬁcient
approximation scheme by quantizing the belief space uniformly under L1 distance.

For partially observed setups, Saldi et al. (2020, 2017) introduce a rigorous approximation anal-
ysis after establishing weak continuity conditions on the transition kernel deﬁning the (belief-MDP)
via the non-linear ﬁlter (Feinberg et al., 2012; Kara et al., 2019), and show that ﬁnite model approx-
imations obtained through quantization are asymptotically optimal and the control policies obtained
from the ﬁnite model can be applied to the actual system with asymptotically vanishing error as
the number of quantization bins increases. Another rigorous set of studies is by Zhou et al. (2008,
2010) where the authors provide an explicit quantization method for the set of probability mea-
sures containing the belief states, where the state space is parametrically representable under strong
density regularity conditions. The quantization is done through the approximations as measured
by Kullback-Leibler divergence (relative entropy) between probability density functions. Further
recent studies include Mao et al. (2020); Subramanian and Mahajan (2019). Subramanian and Ma-
hajan (2019) present a notion of approximate information variable and studies near optimality of
policies that satisﬁes the approximate information state property. Mao et al. (2020) analyzes a sim-
ilar problem under a decentralized setup. Our explicit approximation results in this paper will ﬁnd
applications in both of these studies.

We refer the reader to the survey papers by Lovejoy (1991); White (1991); Hansen (2013) and
the recent book by Krishnamurthy (2016) for further structural results as well as algorithmic and
computational methods for approximating POMDPs. Notably, for POMDPs Krishnamurthy (2016)
presents structural results on optimal policies under monotonicity conditions of the value function
in the belief variable.

For our work, we speciﬁcally focus on ﬁnite memory approximations. With regard to approxi-

mations based on ﬁnite memory, the following two papers are particularly relevant to our paper:

2

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

Yu and Bertsekas (2008) study near optimality of ﬁnite window policies for average cost prob-
lems where the state, action and observation spaces are ﬁnite; under the condition that the liminf
and limsup of the average cost are equal and independent of the initial state, the paper establishes
the near-optimality of (non-stationary) ﬁnite memory policies. Here, a concavity argument building
on a work of Feinberg (1982) (which becomes consequential by the equality assumption) and the
ﬁniteness of the state space is crucial. The paper shows that for any given (cid:15) > 0, there exists an
(cid:15)-optimal ﬁnite window policy. However, the authors do not provide a performance bound related
to the length of the window, and in fact the proof method builds on convex analysis. Nonetheless,
the constant property of the value functions over initial priors is related to unique ergodicity, and
thus the stability problem of non-linear ﬁlters, which is a topic of current investigation particularly
in the controlled setup.

In another related direction, White-III and Scherer (1994) study ﬁnite memory approximation
techniques for POMDPs with ﬁnite state, action and measurements. The POMDP is reduced to a
belief MDP and the worst and best case predictors prior to the N most recent information variables
are considered to build an approximate belief MDP. The original value function is bounded using
these approximate belief MDPs that use only ﬁnite memory, where the ﬁniteness of the state space
is critically used. Furthermore, a loss bound is provided for a suboptimally constructed policy
that only uses ﬁnite history, where the bound depends on a more speciﬁc ergodicity coefﬁcient
(which requires restrictive, sample pathwise, contraction properties).
In our paper, we consider
more general signal spaces and more relaxed ﬁlter stability conditions, and establish explicit rates
of convergence results. We also rigorously establish the relation of the loss bound to nonlinear ﬁlter
stability and state space reduction techniques for MDPs.

A recent work by the authors Kara and Y¨uksel (2021) introduces a different ﬁnite history ap-
proximation technique where the approximation is done via an alternative belief MDP-reduction
method rather than direct discretization of the space of probability measures. It is shown that the
approximation error can be related to the controlled ﬁlter stability in terms of the total variation
distance, whereas in this paper, the error bound is also shown to be related to more general, and in
particular weak convergence inducing, metrics. Although, the approximation technique introduced
in Kara and Y¨uksel (2021) provides an error upper bound in terms of the more stringent total vari-
ation distance, it proves to be numerically efﬁcient as it is shown that a ﬁnite history Q learning
algorithm converges to the optimality equation of an approximate model. Our analysis here re-
quires less stringent conditions on ﬁlter stability, however the use of the bounded-Lipschitz metric
on probability measures leads to a signiﬁcantly more tedious analysis.

Contributions. In this paper, we rigorously establish near optimality of ﬁnite memory feedback
control policies for the case where the actions and measurements are ﬁnite (with the state being real
vector valued), provided that the controlled non-linear ﬁlter is stable in a sense to be presented in
the paper. We also explicitly relate the approximation error with the window size. This is the ﬁrst
rigorous result, to our knowledge, where ﬁnite window policies are shown to be (cid:15)-optimal with an
explicit rate of convergence with respect to the window size.

1.1 Preliminaries and the Main Results

Let X ⊂ Rm denote a Borel set which is the state space of a partially observed controlled Markov
process. Here and throughout the paper Z+ denotes the set of non-negative integers and N denotes
the set of positive integers. Let Y be a ﬁnite set denoting the observation space of the model, and let

3

KARA AND Y ¨UKSEL

the state be observed through an observation channel Q. The observation channel, Q, is deﬁned as
a stochastic kernel (regular conditional probability) from X to Y, such that Q( · |x) is a probability
measure on the power set P (Y) of Y for every x ∈ X, and Q(A| · ) : X → [0, 1] is a Borel
measurable function for every A ∈ P (Y). A decision maker (DM) is located at the output of the
channel Q, and hence it only sees the observations {Yt, t ∈ Z+} and chooses its actions from U, the
action space which is a ﬁnite subset of some Euclidean space. An admissible policy γ is a sequence
of control functions {γt, t ∈ Z+} such that γt is measurable with respect to the σ-algebra generated
by the information variables It = {Y[0,t], U[0,t−1]},

I0 = {Y0}, where

t ∈ N,

Ut = γt(It),

t ∈ Z+,

(1)

are the U-valued control actions and Y[0,t] = {Ys, 0 ≤ s ≤ t}, U[0,t−1] = {Us, 0 ≤ s ≤ t − 1}.
We deﬁne Γ to be the set of all such admissible policies. The update rules of the system are deter-
mined by (1) and the following relationships:

Pr(cid:0)(X0, Y0) ∈ B(cid:1) =

(cid:90)

B

µ(dx0)Q(dy0|x0), B ∈ B(X × Y),

where µ is the (prior) distribution of the initial state X0, and

(cid:18)

Pr

(Xt, Yt) ∈ B

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(X, Y, U )[0,t−1] = (x, y, u)[0,t−1]

=

(cid:19)

(cid:90)

B

T (dxt|xt−1, ut−1)Q(dyt|xt),

B ∈ B(X × Y), t ∈ N, where T is the transition kernel of the model which is a stochastic kernel
from X × U to X. Note that, although Y is ﬁnite, we use integral sign instead of the summation
sign for notation convenience by letting the measure to be sum of dirac-delta measures. We let the
objective of the agent (decision maker) be the minimization of the inﬁnite horizon discounted cost,

Jβ(µ, T , γ) = ET ,γ

µ

(cid:35)

βtc(Xt, Ut)

(cid:34) ∞
(cid:88)

t=0

for some discount factor β ∈ (0, 1), over the set of admissible policies γ ∈ Γ, where c : X×U → R
is a Borel-measurable stage-wise cost function and ET ,γ
denotes the expectation with initial state
probability measure µ and transition kernel T under policy γ. Note that µ ∈ P(X), where we let
P(X) denote the set of probability measures on X. We deﬁne the optimal cost for the discounted
inﬁnite horizon setup as a function of the priors and the transition kernels as

µ

J ∗
β(µ, T ) = inf
γ∈Γ

Jβ(µ, T , γ).

For partially observed stochastic problems, the optimal policies use all the available information in
general. The question we ask is the following one: suppose we deﬁne an N -memory admissible
policy γN so that γN is a sequence of control functions {γt, t ∈ Z+} such that γt is measurable
with respect to the σ-algebra generated by the information variables

I N
t = {Y[t−N,t], U[t−N,t−1]}, if t ≥ N,
I N
t = {Y[0,t], U[0,t−1]}, if 0 < t < N,
I0 = {Y0},

4

(2)

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

that is the controller can only have access to the information variables through a window whose
length is N . We deﬁne ΓN to be the set of all such N -memory admissible policies. Similarly, we
deﬁne the optimal cost function under N -memory admissible policies as

J N
β (µ, T ) = inf

γN ∈ΓN

Jβ(µ, T , γN ).

Under this setup, we will study the following problem.

Problem: Under suitable conditions, can we ﬁnd explicit bounds on J N

β (µ, T ) − J ∗

β(µ, T ) in terms of

N and a constructive approximate solution achieving this bound?

Our goal is to ﬁnd the best possible control policy in ΓN that is, in the set of policies that use only
a ﬁnite history of information variables, in an ofﬂine setting by reducing the problem to a simpler
approximate setup where we assume that the system dynamics are known to the designer. A general
summary of the approach we will follow to answer this problem is as follows: We ﬁrst deﬁne the
belief MDP counterpart of the partially observed system. Then, we construct a ﬁnite subset of the
belief state space using the probability distributions that can be achieved using the ﬁnite window
information variables (I N
t ’s) from a ﬁxed probability distribution. This ﬁnite subset leads to an
approximate MDP model for which we ﬁnd optimal policies. The calculation of the policies is
greatly simpliﬁed compared to the calculation of optimal policies for the original POMDP model.
Finally, we show that the loss occurring from applying this approximate policy to the original model
can be upper bounded by the expected error of the dicretization of the belief space. The loss is
evaluated compared to the best possible admissible policy in the set Γ. The accumulating error then,
can be represented in relation to the ﬁlter stability problem, that is, how fast the controlled process
forgets its initial distribution as it observes the information variables from the system.

Note that although we take the inﬁmum over all N -memory admissible policies, we will explic-
itly construct ﬁnite window policies which will be time-invariant, or a ﬁnite-state probabilistic au-
tomaton (as is also referred to by Yu and Bertsekas, 2008) that accepts as inputs the ﬁnite window of
observations and actions, and produces as outputs the control actions in a time-invariant/stationary
fashion. Accordingly, the inﬁmum above for J N
β (µ, T ) can be replaced with the minimum over
such policies.

We answer the problem above afﬁrmatively in Theorems 12, 16, and 17 under complementary

conditions.

2. Regularity and Stability Properties of the Belief-MDP

In this section, we introduce the belief MDP reduction of POMDPs and provide regularity properties
of the belief MDPs.

2.1 Convergence Notions for Probability Measures

For the analysis of the technical results, we will use different notions of convergence for sequences
of probability measures.

Two important notions of convergences for sequences of probability measures are weak conver-
gence, and convergence under total variation. For some N ∈ N a sequence {µn, n ∈ N} in P(RN )
is said to converge to µ ∈ P(RN ) weakly if (cid:82)
RN c(x)µ(dx) for every contin-
uous and bounded c : RN → R. One important property of weak convergence is that the space of

RN c(x)µn(dx) → (cid:82)

5

KARA AND Y ¨UKSEL

probability measures on a complete, separable, and metric (Polish) space endowed with the topol-
ogy of weak convergence is itself complete, separable, and metric (Parthasarathy, 1967). One such
metric is the bounded Lipschitz metric (Villani, 2008, p.109), which is deﬁned for µ, ν ∈ P(X) as

ρBL(µ, ν) := sup

(cid:107)f (cid:107)BL≤1

(cid:90)

|

(cid:90)

f dµ −

f dν|

(3)

where

(cid:107)f (cid:107)BL := (cid:107)f (cid:107)∞ + sup
x(cid:54)=y

|f (x) − f (y)|
d(x, y)

and (cid:107)f (cid:107)∞ = supx∈X |f (x)|.

For probability measures µ, ν ∈ P(RN ), the total variation metric is given by

(cid:107)µ − ν(cid:107)T V = 2

sup
B∈B(RN )

|µ(B) − ν(B)| = sup

f :(cid:107)f (cid:107)∞≤1

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)

f (x)µ(dx) −

(cid:90)

(cid:12)
(cid:12)
f (x)ν(dx)
(cid:12)
(cid:12)

,

where the supremum is taken over all measurable real f such that (cid:107)f (cid:107)∞ = supx∈RN |f (x)| ≤ 1. A
sequence µn is said to converge in total variation to µ ∈ P(RN ) if (cid:107)µn − µ(cid:107)T V → 0.

2.2 Ergodicity and Filter Stability Properties of Partially Observed MDPs
Given a prior µ ∈ P(X) and a policy γ ∈ Γ, we deﬁne the ﬁlter and predictor for a POMDP in the
following.

Deﬁnition 1 The one step predictor process is deﬁned as the sequence of conditional probability
measures

πµ,γ
n− (·) = P µ,γ(Xn ∈ ·|Y[0,n−1], U[0,n−1] = γn(Y[0,n−1], U[0,n−2])) = P µ,γ(Xn ∈ ·|Y[0,n−1]), n ∈ N

where P µ,γ is the probability measure induced by the prior µ and the policy γ, when µ is the
probability measure on X0.

Deﬁnition 2 The ﬁlter process is deﬁned as the sequence of conditional probability measures

n (·) = P µ,γ(Xn ∈ ·|Y[0,n], U[0,n−1] = γn(Y[0,n−1], U[0,n−2])) = P µ,γ(Xn ∈ ·|Y[0,n]), n ∈ N
πµ,γ
(4)

where P µ,γ is the probability measure induced by the prior µ and the policy γ.

Deﬁnition 3 (Dobrushin, 1956, Equation 1.16) For a kernel operator K : S1 → P(S2) (that is
a regular conditional probability from S1 to S2) for standard Borel spaces S1, S2, we deﬁne the
Dobrushin coefﬁcient as:

δ(K) = inf

n
(cid:88)

i=1

min(K(x, Ai), K(y, Ai))

(5)

where the inﬁmum is over all x, y ∈ S1 and all partitions {Ai}n

i=1 of S2.

6

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

We note that this deﬁnition holds for continuous or ﬁnite/countable spaces S1 and S2 and 0 ≤
δ(K) ≤ 1 for any kernel operator.

Example 1 Assume for a ﬁnite setup, we have the following stochastic transition matrix

K =





1
3
0
3
4

1
3
1
2
0





1
3
1
2
1
4

The Dobrushin coefﬁcient is the minimum over any two rows where we sum the minimum elements
among those rows. For this example, the ﬁrst and the second rows give 2
3 , the ﬁrst and the third
rows give 7

12 and the second and the third rows give 1

4 . Then the Dobrushin coefﬁcient is 1
4 .

Let

˜δ(T ) := inf
u∈U

δ(T (·|·, u)).

Deﬁnition 4 For X ⊂ Rm for some m ∈ N, and for two probability measures µ, ν ∈ P(X), µ is
said to be absolutely continuous with respect to ν if µ(A) = 0 for every set A ∈ B(X) for which
ν(A) = 0. We denote the absolute continuity of µ with respect to ν by µ (cid:28) ν.

Theorem 5 (McDonald and Y¨uksel, 2020, Theorem 3.3) Assume that for µ, ν ∈ P(X), we have
µ (cid:28) ν. Then we have (exponential ﬁlter stability)

Eµ,γ (cid:2)(cid:107)πµ,γ

n+1 − πν,γ

n+1(cid:107)T V

(cid:3) ≤ (1 − ˜δ(T ))(2 − δ(Q))Eµ,γ [(cid:107)πµ,γ

n − πν,γ

n (cid:107)T V ] .

In particular, deﬁning α := (1 − ˜δ(T ))(2 − δ(Q)), we have

Eµ,γ [(cid:107)πµ,γ

n − πν,γ

n (cid:107)T V ] ≤ 2αn.

This result will be a key ingredient for our main results. It provides conditions on when the
belief-state processes for a given POMDP under different priors get closer when they are fed with
same observation processes, in expectation under the true probability space. In a vague sense, if the
state process is tracked only using a ﬁnite window of recent measurement and control variables (and
forgets the past observations and actions), then the amount of mismatch from the true ﬁlter can be
bounded with an error that is exponentially diminishing with the window size. The relationship is
(cid:0)P π(XN ∈ ·|Y[0,N ]), P ˆπ(XN ∈ ·|Y[0,N ])(cid:1)(cid:3) which appears
via the term supπ∈P(X) supγ∈Γ Eγ
π
crucially in (9). We note that if one does not wish to have an explicit rate of convergence result, one
could have more relaxed conditions for ﬁlter stability which will still lead to rigorous approximation
results on the performance of ﬁnite window policies via the controlled ﬁlter stability analysis in
McDonald and Y¨uksel (2022).

(cid:2)ρBL

2.3 Reduction to Fully Observed Models and Regularity Properties of Belief-MDPs

It is by now a standard result that, for optimality analysis, any POMDP can be reduced to a com-
pletely observable Markov decision process (Yushkevich, 1976; Rhenius, 1974), whose states are
the posterior state distributions or beliefs of the observer or the ﬁlter process as deﬁned in (4); that
is, the state at time n is

Pr{Xn ∈ · |Y0, . . . , Yn, U0, . . . , Un−1} ∈ P(X).

7

KARA AND Y ¨UKSEL

We call this equivalent process the ﬁlter process . The ﬁlter process has state space Z = P(X) and
action space U. Here, Z is equipped with the Borel σ-algebra generated by the topology of weak
convergence (Billingsley, 1999). As noted earlier, under this topology, Z is a standard Borel space
(Parthasarathy, 1967). Then, the transition probability η of the ﬁlter process can be constructed as
follows (see also Hern´andez-Lerma, 1989). If we deﬁne the measurable function

F (z, u, y) := F ( · |y, u, z) = Pr{Xn+1 ∈ · |Zn = z, Un = u, Yn+1 = y}

from P(X) × U × Y to P(X) and use the stochastic kernel P ( · |z, u) = Pr{Yn+1 ∈ · |Zn =
z, Un = u} from P(X) × U to Y, we can write η as

η( · |z, u) =

(cid:90)

Y

1{F (z,u,y)∈ · }P (dy|z, u).

The one-stage cost function ˜c : P(X) × U → [0, ∞) of the ﬁlter process is given by

˜c(z, u) :=

(cid:90)

X

c(x, u)z(dx),

(6)

(7)

which is a Borel measurable function. Hence, the ﬁlter process is a completely observable Markov
process with the components (Z, U, ˜c, η).

For the ﬁlter process, the information variables is deﬁned as

˜It = {Z[0,t], U[0,t−1]},

t ∈ N,

˜I0 = {Z0}.

It is well known that an optimal control policy of the original POMDP can use the belief Zt as a
sufﬁcient statistic for optimal policies (see Yushkevich, 1976; Rhenius, 1974), provided they exist.
More precisely, the ﬁlter process is equivalent to the original POMP in the sense that for any optimal
policy for the ﬁlter process, one can construct a policy for the original POMP which is optimal. On
existence, we note the following.

With the recent results by Feinberg et al. (2016); Kara et al. (2019) the transition model of
the belief-MDP can be shown to satisfy weak continuity conditions on the belief state and action
variables, and accordingly we have that the measurable selection conditions (Hernandez-Lerma and
Lasserre, 1996, Chapter 3) apply. Notably, we state the following.

Assumption 1

(i) The transition probability T (·|x, u) is weakly continuous in (x, u), i.e., for

any (xn, un) → (x, u), T (·|xn, un) → T (·|x, u) weakly.

(ii) The observation channel Q(·|x, u) is continuous in total variation, i.e., for any (xn, un) →

(x, u), Q(·|xn, un) → Q(·|x, u) in total variation.

Assumption 2

(i) The transition probability T (·|x, u) is continuous in total variation in (x, u),

i.e., for any (xn, un) → (x, u), T (·|xn, un) → T (·|x, u) in total variation.

(ii) The observation channel Q(·|x) is independent of the control variable.

Theorem 6

(i) (Feinberg et al., 2016) Under Assumption 1, the transition probability η(·|z, u)

of the ﬁlter process is weakly continuous in (z, u).

8

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

(ii) (Kara et al., 2019) Under Assumption 2, the transition probability η(·|z, u) of the ﬁlter pro-

cess is weakly continuous in (z, u).

Under the above weak continuity conditions, the measurable selection conditions (Hernandez-
Lerma and Lasserre, 1996, Chapter 3) apply and a solution to the discounted cost optimality equa-
tion exists, and accordingly an optimal control policy exists. This policy is stationary (in the belief
state). Thus there exists a function Φ : P(X) → U such that for any policy γ for a prior µ

γ(y[0,n]) = Φ (cid:0)P µ,γ(Xn ∈ ·|Y[0,n] = y[0,n])(cid:1) = Φ(πµ,γ
n )

In particular, we have that J ∗
the assumptions we will work with.

β(µ, T , Q) = J ∗

β(µ, η). This will be the case in our paper, under

For the rest of the paper, we will use γ for the belief process policy Φ for consistency of notation.
The following supporting result, to be used later in the paper, provides further regularity proper-
ties on the transition model for the belief model under mild conditions on the fully observed model.
This result may be useful for POMDP theory beyond the application considered in this paper. We
also note that the bounds in (i) and (iii) below are applicable when we only have ﬁlter stability, but
not exponential ﬁlter stability (McDonald and Y¨uksel, 2022), whereas items (ii)-(iv) will be used
under exponential ﬁlter stability in this paper.

Theorem 7

i. Assume that

(cid:107)T (·|x, u) − T (·|x(cid:48), u)(cid:107)T V ≤ αX|x − x(cid:48)|

for some αX < ∞ for all u ∈ U. We have

ρBL

(cid:0)η(·|z, u), η(·|z(cid:48), u)(cid:1) ≤ 3(1 + αX)ρBL(z, z(cid:48)).

ii. Assume that

(cid:107)T (·|x, u) − T (·|x(cid:48), u)(cid:107)T V ≤ αX|x − x(cid:48)|

for some αX < ∞ for all u ∈ U. Then, under the conditions of Theorem 5 we have

ρBL

(cid:0)η(·|z, u), η(·|z(cid:48), u)(cid:1) ≤ (3 − 2δ(Q))(1 + αX)ρBL(z, z(cid:48)).

iii. Without any assumption

ρBL

(cid:0)η(·|z, u), η(·|z(cid:48), u)(cid:1) ≤ 3(cid:107)z − z(cid:48)(cid:107)T V .

iv. Under the conditions of Theorem 5

ρBL

(cid:0)η(·|z, u), η(·|z(cid:48), u)(cid:1) ≤ (3 − 2δ(Q))(1 − ˜δ(T ))(cid:107)z − z(cid:48)(cid:107)T V .

Proof See Section 6.1.

9

KARA AND Y ¨UKSEL

3. Approximate Model Construction: Finite Belief-MDP through Finite Memory

In this section, we will construct a ﬁnite state space by quantizing the belief state space so that the
approximate ﬁnite model is obtained using only a ﬁnite memory.

Our construction builds on but signiﬁcantly differs from the approach by Saldi et al. (2018,
2017). As we will explain, we cannot afford to use uniform quantization in our setup, which was a
crucial tool used by Saldi et al. (2018, 2017).

As we discussed in the previous section, we can write the inﬁnite horizon cost as

Jβ(T , Q, γ, µ) =

∞
(cid:88)

βtEµ [˜c (πt, γ(πt))]

t=0
N −1
(cid:88)

t=0

=

βtEµ [˜c (πt, γ(πt))] +

∞
(cid:88)

t=N

βtEµ [˜c (πt, γ(πt))] .

Now we focus on the second term:

∞
(cid:88)

t=N

βtEµ [˜c (πt, γ(πt))]

=

∞
(cid:88)

t=N

βtEµ

(cid:2)˜c (cid:0)P µ,γ(Xt ∈ ·|Y[0,t], U[0,t−1]), γ(P µ,γ(Xt ∈ ·|Y[0,t], U[0,t−1]))(cid:1)(cid:3) .

Notice that for any time step t ≥ N and for a ﬁxed observation realization sequence y[0,t] and

for a ﬁxed control action sequence u[0,t−1], the state process can be viewed as

P µ(Xt ∈ ·|Y[0,t] = y[0,t], U[0,t−1] = u[0,t−1])
= P πt−N − (Xt ∈ ·|Y[t−N,t] = y[t−N,t], U[t−N,t−1] = u[t−N,t−1])

where

πt−N −(·) = P µ(Xt−N ∈ ·|Y[0,t−N −1] = y[0,t−N −1], U[0,t−N −1] = u[0,t−N −1]).

That is, we can view the state as the Bayesian update of πt−N−, the predictor at time t−N , using the
observations Yt−N , . . . , Yt. Notice that with this representation only the most recent N observation
realizations are used for the update and the past information of the observations is embedded in
πt−N−.

Hence, we can view the state space for time stages t ≥ N as

Z = (cid:8)P π(XN ∈ ·|Y[0,N ], U[0,N −1]); π ∈ P(X), Y[0,N ] ∈ YN +1, U[0,N −1] ∈ UN (cid:9)

Consider the following ﬁnite set Z N

ˆπ deﬁned by, for a ﬁxed probability measure ˆπ ∈ P(X),

Z N

ˆπ :=

(cid:110)
P ˆπ(XN ∈ ·|Y[0,N ], U[0,N −1]); Y[0,N ] ∈ YN +1, U[0,N −1] ∈ UN (cid:111)

.

Deﬁne the map F : Z → Z N

ˆπ , which will serve as the quantizer, with

ρBL(z, y)

(8)

F (z) := arg min

y∈Z N
ˆπ

10

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

This map separates the set Z into |Y|N +1 × |U|N sets, accordingly this quantizes the set of

probability measures.

To complete our approximate controlled Markov model, we now deﬁne the one-stage cost func-
ˆπ × U:

ˆπ × U → [0, ∞) and the transition probability ηN on Z N

ˆπ given realizations in Z N

tion cN : Z N
For a given zi = P ˆπ(XN ∈ ·|yi

[0,N ], ui

[0,N −1]) and control action u

cN (zi, u) = cN (P ˆπ(XN ∈ ·|yi

[0,N ], ui

[0,N −1]), u) := ˜c(P ˆπ(XN ∈ ·|yi

[0,N ], ui

[0,N −1]), u),

where ˜c is deﬁned in (7) and

ηN (·|zi, u) = ηN (·|P ˆπ(XN ∈ ·|yi

[0,N ], ui

[0,N −1], u))

:= F ∗ η(·|P ˆπ(XN ∈ ·|yi

[0,N ], ui

[0,N −1], u),

where

F ∗ η(z(cid:48)|z, u) = η({z ∈ P(X) : F (z) = z(cid:48)}|z, u),

for all z(cid:48) ∈ Z N
ˆπ .

We thus have deﬁned a ﬁnite state MDP with the state space Z N

ˆπ , action space U, cost function

cN and the transition probability ηN .

An optimal policy γ∗

N for this ﬁnite state model is a function taking values from the ﬁnite state
space and hence at some time step t ≥ N it only uses N most recent observations and control action
variables that is, γ∗
t deﬁned in (2) for all t ≥ N .

N is a measurable function of the information set I N

We list the steps to construct the approximate model informally as follows:

• Fix a probability distribution ˆπ ∈ P(X) as an estimator for the predictor of N step back

(π−

t−N ).

• Calculate Bayesian updates for all possible realizations y[0,N ], u[0,N −1] (|Y|N +1 ×|U|N many

realizations) starting from the prior ˆπ to form the ﬁnite subset Z N
ˆπ .

• Calculate ηN (approximate transition model) and cN (approximate cost function) using a
ˆπ there are only |Y| many possible transi-
nearest neighbor map. Note that from any zi ∈ Z N
tions under the true dynamics. To construct the ηN , these possible transitions are mapped to
the closest element in Z N
ˆπ .

• Calculate the value functions and the optimal policies for the ﬁnite model with state space

Z N

ˆπ , transition kernel ηN and the cost function cN .

As we have noted before, the complexity of POMDPs in general arises from the structure of the
belief state space Z which is a set of probability measures on X. This set is always uncountable
and needs to be associated with proper topologies to make the analysis feasible. Approximations
for POMDPs are usually done by choosing a ﬁnite subset, say ˆZ, of the belief state space Z (Smith
and Simmons, 2012; Pineau et al., 2006; Saldi et al., 2017; Zhou et al., 2008, 2010; Subramanian
and Mahajan, 2019; Zhang et al., 2014), and ﬁnding an approximate MDP model for this ﬁnite set.
To choose the ﬁnite set, the aforementioned works use a uniform quantization scheme, in various
topologies on Z. In other words, the quantization is made such that for any z ∈ Z, there exists an
element ˆz ∈ ˆZ with (cid:107)z − ˆz(cid:107) ≤ (cid:15) for a ﬁxed (cid:15) > 0. The metric to measure distances of the belief

11

KARA AND Y ¨UKSEL

states varies for different works, although for ﬁnite X, L1 distance of distributions is what is used
in general for the quantization of Z, which coincides with total variation and weak convergence
topology on Z when X is ﬁnite; for general X, a more appropriate and natural topology is the weak
convergence topology on Z which is what we work with in the paper since ρBL metrizes the weak
convergence.

In this paper, instead of quantizing Z directly and uniformly, we use ﬁnite window information
t ’s) to construct the ﬁnite subset of Z since our goal is to analyze the effect of the

variables (I N
window size on the approximation performance. That is, we use the ﬁnite set

Z N

ˆπ :=

P ˆπ(XN ∈ ·|Y[0,N ], U[0,N −1]); Y[0,N ] ∈ YN +1, U[0,N −1] ∈ UN (cid:111)
(cid:110)

constructed using Y[0,N ], U[0,N −1]. For this set, we cannot afford a uniform discretization scheme.
A uniform quantization would mean that for a ﬁxed (cid:15) > 0

(cid:16)

(cid:17)
P ˆπ(XN ∈ ·|y[0,N ], u[0,N −1]), P π(XN ∈ ·|y[0,N ], u[0,N −1])

ρBL

< (cid:15)

uniformly for any π ∈ Z and for any y[0,N ] ∈ YN +1, u[0,N −1] ∈ UN . However, this is in gen-
eral inapplicable for ﬁlter stability problems as it requires for the processes with different starting
points to converge uniformly for any realizations of information variables (even for highly unlikely
ones). That is why, we follow a different approach and show that we do not have to force uniform
quantization, and the error of approximate value functions can be related to the expected error of
the form

(cid:104)

Eγ
π

ρBL

(cid:16)

P π(XN ∈ ·|Y[0,N ], U[0,N −1]), P ˆπ(XN ∈ ·|Y[0,N ], U[0,N −1])

(cid:17)(cid:105)

which in turn can be bounded using Theorem 5. Our technical analysis, accordingly, is slightly
more tedious at the beneﬁt of arriving at a practical and intuitive ﬁnite-memory method whose near
optimality is rigorously established.

Remark 8 The ﬁnite subset Z N
ˆπ is crucial for the approximation of the belief MDP we construct
in this section. This set depends on the choice of ˆπ as the starting probability distribution and the
ﬁnite window information variables Y[0,N ], U[0,N −1]. Kara and Y¨uksel (2021) consider a similar
approach for construction of an approximate POMDP model by using the same ﬁnite set Z N
ˆπ . How-
ever, instead of using a nearest neighbor map for the correspondence between the original belief
space and the ﬁnite set Z N
ˆπ , there, the states with matching ﬁnite history are used for the correspon-
dence by putting a different topology on the belief space and the set Z N
ˆπ . The method used in this
paper naturally results in a smaller approximation error due to the nature of the nearest neighbor
map, and lets us work with the weak convergence topology and ρBL metric. On one hand, Kara and
Y¨uksel (2021) provide a greater error term in terms of the total variation distance, which always
upper bounds the ρBL metric, on the other hand, the method used there, is computationally more ef-
ﬁcient and the approximate model can actually be learned with a reinforcement learning algorithm
that uses ﬁnite window information variables.

On the choice of ˆπ, we note that Theorem 12 provides a bound that is independent of the choice
of ˆπ. However, since this is only an upper bound, it is apparent that the true value of the error
may depend on different choices of ˆπ. For the approximate model, we use ˆπ as an estimator for
the true predictor πµ,γ
(·) = P µ,γ(Xt ∈ ·|Y[0,t−1]) at any given time t under some policy γ. Since
t−

12

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

t−

ˆπ is a ﬁxed probability distribution and does not vary with time, we can argue that a reasonable
choice would be the time averages of πµ,γ∗
(·) under an optimal policy γ∗ and if the hidden state
process Xt admits an invariant measure under this optimal policy γ∗, the time averages of πµ,γ∗
(·)
t−
will converge to the invariant measure of Xt under the optimal policy. However, one issue with this
approach is that the designer does not have access to the optimal policy and thus, cannot compute
the invariant measure under γ∗. The designer, instead, can use the approximate optimal policy
γN , but the choice of ˆπ also affects the approximate policy γN , and hence, this approach may not
feasible for practical purposes. In any case, our result provides a bound uniform over the prior
selections.

We now discuss the the role of the realizations of Y[0,N ], U[0,N −1]. Notice that, to construct Z N
ˆπ ,
we consider all possible |Y|N +1 × |U|N realizations which reduces the uncountable state space
to a ﬁnite one but the size of this ﬁnite subset grows fast with the size of window, N , and with
the size of spaces Y and U. The bound we provide in Theorem 9, reveals that the approximation
error is related to the expectation over the realizations of Y[0,N ], U[0,N −1] under the true dynamics,
which suggests that if a realization y[0,N ], u[0,N −1], is highly unlikely under the true dynamics, these
realizations can be discarded when constructing Z N
ˆπ for computational ease by reducing the size of
Z N
ˆπ .

In fact this question motivates the following direction. A close look at Deﬁnition 3 reveals that
the Dobrushin coefﬁcient of a channel and a channel obtained by quantizing the measurements of
that same channel are so that the coefﬁcient of the former is a lower bound for the latter. Therefore,
one can always quantize the measurement channels further if the original channel satisﬁes the
contraction condition presented for ﬁlter stability, with the quantized channel also satisfying the
contraction property. This presents a recipe for dealing with both low probability channel outcomes
or even continuous space measurement channels; however one would additionally need to show
that the value function of the approximate model with quantized measurements would be close to
the value function of the original model. This is a possible future direction (using the weak Feller
property of the belief-MDP) (as shown e.g. by Saldi et al., 2017) and continuity properties of ﬁlter
updates in measurement realizations McDonald and Y¨uksel (2022).

4. Approximation Error Analysis and Rates of Convergence

An optimal policy for the constructed ﬁnite model, γ∗
for the original MDP.

N can be extended to P(X) and can be used

4.1 Analysis via Expected Filter Approximation Error

The next result, which is related to the construction given by Saldi et al. (2018, Theorem 4.38) (see
also Saldi et al., 2017), provides a mismatch error for using this policy. This result is going to be
the key supporting tool for the main theorem of the paper, which will be presented immediately
after. The proof requires multiple technical lemmas and are presented in Section 6.2 (with some
supporting but tedious technical steps moved to the Appendix).

Assumption 3

• ρBL(η(·|z, u), η(·|z(cid:48), u)) ≤ αZ ρBL(z, z(cid:48)) (see Theorem 7).

• |˜c(z, u) − ˜c(z(cid:48), u)| ≤ α˜cρBL(z, z(cid:48)) for some α˜c for all u ∈ U.

13

KARA AND Y ¨UKSEL

Theorem 9 Under Assumption 3, for all z ∈ P(X) of the form

z = P π(XN ∈ ·|y[0,N ], u[0,N −1]),

the following holds if β < 1

4αZ +1 :

(cid:20)
Jβ(η, γ∗

(cid:12)
(cid:12)
N , z) − Jβ(η, γ∗, z)
(cid:12)
(cid:12)

E

sup
γ∈Γ

Y[0,N ], γ(Y[0,N −1])

(cid:21)

≤ K sup

π∈P(X)

Eγ
π

sup
γ∈Γ

(cid:16)

(cid:104)

ρBL

P π(XN ∈ ·|Y[0,N ], U[0,N −1]), P ˆπ(XN ∈ ·|Y[0,N ], U[0,N −1])

(cid:17)(cid:105)

.

Where K is a constant that depends on β, αZ , α˜c and (cid:107)˜c(cid:107)∞. (The exact expression for the constant
can be found in the proof).

Remark 10 We note that the upper bound for β can be chosen as
(see Remark 21 in Appendix), where

1

(2+(cid:107)L(cid:107)∞)αZ +1 , instead of 4αZ +1

(cid:107)L(cid:107)∞ = sup

π∈P(X)

sup
γ∈Γ

sup
Y[0,N ],U[0,N −1]

ρBL

(cid:16)

P π(·|Y[0,N ], U[0,N −1]), P ˆπ(·|Y[0,N ], U[0,N −1])

(cid:17)

.

Hence, under a uniform ﬁlter stability bound, the upper bound can be chosen near

1
2αZ +1 .

The proof of this theorem is rather long, and accordingly, it is presented in Section 6.2.

Before presenting the main result of the paper, we provide further supporting results that will

let us work with a probability density function.

Lemma 11 Assume that the transition kernel T (dx1|x0, u0) admits a density function f with re-
spect to a reference measure φ such that T (dx1|x0, u0) = f (x1, x0, u0)φ(dx1). If |f (x1, x0, u0) −
f (x1, x0

(cid:48) ∈ X and u0 ∈ U then

(cid:48), u0)| ≤ αX|x0 − x0

(cid:48)| for all x1, x0, x0

(cid:107)T (·|x0, u0) − T (·|x0

(cid:48), u0)(cid:107)T V ≤ αX|x − x(cid:48)|.

We note that using this result, assumptions of Theorem 7 can be expressed with the Lipschitz
condition on the density function noted above. We now restate the assumptions that will be used for
the main result.

Assumption 4

1. The transition kernel T (dx1|x0, u0) admits a density function f with respect to a reference

measure φ such that T (dx1|x0, u0) = f (x1, x0, u0)φ(dx1).

2. There exists some αX < ∞ such that |f (x1, x0, u0) − f (x1, x0

(cid:48), u0)| ≤ αX|x0 − x0

(cid:48)|.

3. There exists some αc < ∞ such that for all u ∈ U, |c(x, u) − c(x(cid:48), u)| ≤ αc|x − x(cid:48)|.

4. α := (1 − ˜δ(T ))(2 − δ(Q)) < 1,

5. The transition kernel T is dominated, i.e. there exists a dominating measure ˆπ ∈ P(X) such
that for every x ∈ X and u ∈ U, T (·|x, u) (cid:28) ˆπ(·) that is T (·|x, u) is absolutely continuous
with respect to ˆπ for every x ∈ X and u ∈ U.

14

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

Before the result, we discuss the Lipschitz constants of interest and their relation to each
other. First, note that by Lemma 11, Assumption 4 (second item) implies that (cid:107)T (·|x0, u0) −
(cid:48), u0)(cid:107)T V ≤ αX|x − x(cid:48)|. Hence, by Theorem 7, we can have various bounds on αZ , where
T (·|x0
ρBL(η(·|z, u), η(·|z(cid:48), u)) ≤ αZ ρBL(z, z(cid:48)). In particular, we have αZ ≤ (3 − 2δ(Q))(1 + αX).

Theorem 12 Assume that we let the system start running for N time steps under a known policy γ
(which may not be optimal), and the ﬁnite window policy starts acting after observing N information
variables at time t = N . Under Assumption 4, if β < 1

4αZ +1 we have

Eγ
µ

(cid:2)Jβ(πN−, T , γ∗

N ) − Jβ(πN−, T , γ∗)|Y[0,N ], U[0,N −1]

(cid:3) ≤ K(β, αX, αc, (cid:107)c(cid:107)∞)αN .

where γ∗
N is the optimal ﬁnite window policy and where K(β, αX, αc, (cid:107)c(cid:107)∞) is a constant that
depends on β, αX, αc and (cid:107)c(cid:107)∞ (The exact formula for the constant can be found in the appendix
in (28)).

In the statement, Jβ(πN−, T , γ∗

N ) (respectively Jβ(πN−, T , γ∗)) denotes the cost under the
N (respectively γ∗) when the initial prior distribution is πN−, where πN− = P r(XN ∈

policy γ∗
·|Y[0,N −1], U[0,N −1]) and the expectation is with respect to the realizations of Y[0,N −1], U[0,N −1].

Remark 13 We note that Theorem 12 applies to all ﬁnite state, measurement, action models as
long as α = (1 − ˜δ(T ))(2 − δ(Q)) < 1 and β satisﬁes the condition noted. Example 1 shows how
to calculate the Dobrushin coefﬁcient for transition matrices in ﬁnite setups. All other conditions
apply since all probability measures on a ﬁnite/countable set are majorized by a probability measure
which places a positive mass on every single point. Notice that the third condition only requires that
the difference in the cost is bounded for every ﬁxed control action. Condition 1 and 5 of Assumption
4 coincide in the ﬁnite case.

Remark 14 We note that the error bound of the result is independent of the chosen ˆπ. As we will
see in the following proof, the ﬁrst upper bound is a result of Theorem 9 which indeed depends on
the ˆπ chosen by the user. However, thanks to Theorem 5, we can get a further upper bound on the
error which is uniform over any ˆπ as long as ˆπ is a dominating measure.

One can interpret the absolute continuity assumption, that is π (cid:28) ˆπ, as follows: assume that
the starting distribution of the process is π but we start the update from the ﬁxed prior ˆπ. The
information, y[0,t], u[0,t−1] can eventually ﬁx the starting error uniformly for all such ˆπ, as long
as, the ﬁxed starting distribution ˆπ, puts on a positive measure to every event that the real starting
distribution π puts on a positive measure. However, if it is not the case, that is, if the incorrect
starting distribution ˆπ puts 0 measure to some event, that π puts positive measure to, information
variables are not sufﬁcient to ﬁx the starting error occurring from that 0 measure event. Of course
this would not be feasible as the prior would not be compatible with the measured data. In any
case, in our setup, the ﬁxed prior ˆπ serves as an approximation and this can be made to satisfy the
absolute continuity condition by design.

Proof When we reduce a partially observed MDP to a fully observed process, the initial state of
the belief process becomes the Bayesian update of the prior distribution of the state process of the
POMDP. Hence, we can write that

Eγ
µ

(cid:2)Jβ(πN−, T , γ∗

N ) − Jβ(πN−, T , γ∗)|Y[0,N ], U[0,N −1]

(cid:3)

15

KARA AND Y ¨UKSEL

= Eγ
µ

(cid:2)Jβ(πN , η, γ∗

N ) − Jβ(πN , η, γ∗)|Y[0,N ], U[0,N −1]

(cid:3) .

Notice that, using Theorem 7, Condition 2 of Assumption 4 implies that

ρBL

(cid:0)η(·|z, u), η(·|z(cid:48), u)(cid:1) ≤ 3(1 + αX)ρBL(z, z(cid:48))

and we also have that

|˜c(z, u) − ˜c(z(cid:48), u)| =

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

c(x, u)z(dx) −

(cid:90)

(cid:12)
(cid:12)
c(x, u)z(cid:48)(dx)
(cid:12)
(cid:12)

≤ (αc + (cid:107)c(cid:107)∞)ρBL(z, z(cid:48)).

Thus, we can use Theorems 7 and 9 to write

Eγ
µ

(cid:2)Jβ(πN , η, γ∗

N ) − Jβ(πN , η, γ∗)|Y[0,N ], U[0,N −1]

(cid:3)

≤ K(β, αX, αc, (cid:107)c(cid:107)∞)

sup
π∈P(X)

sup
γ∈Γ

(cid:104)

Eγ
π

ρBL

(cid:16)

P π(XN ∈ ·|Y[0,N ]), P ˆπ(XN ∈ ·|Y[0,N ])

(cid:17)(cid:105)

.(9)

We set ˆπ in Assumption 4 as our representative probability measure for the quantization of the
belief space. Notice that by our choice of ˆπ is a dominating measure and therefore πt− (cid:28) ˆπ for any
time step t, where πt− is the predictor at time t . Thus, Theorem 5 yields that under Assumption 4

(cid:104)

Eγ
π

(cid:107)P π(·|Y[0,N ]) − P ˆπ(·|Y[0,N ])(cid:107)T V

(cid:105)

≤ αN ,

for any predictor π. We also have by deﬁnition that

(cid:16)

ρBL

P π,γ(·|Y[0,N ]), P ˆπ,γ(·|Y[0,N ])

(cid:17)

≤ (cid:107)P π,γ(·|Y[0,N ]) − P ˆπ,γ(·|Y[0,N ])(cid:107)T V .

Hence the result follows.

Remark 15 By studying (9) and utilizing Theorem 7(i)-(iii), we can arrive at complementary results
when we only have ﬁlter stability but not exponential ﬁlter stability.

We note that the initial N time steps of the control problem should be treated separately as our
approximation technique uses information variables with size N , but for the initial N steps, there
is not enough observation or control action variables to make use of the approximate ﬁnite window
policies. In the following result, for the ﬁrst N time steps, we use a control policy that is found
with a policy iteration type argument where the terminal cost estimated with βN E[Jβ(γ∗
N )] with
γ∗
N being the approximate ﬁnite window policy.

The following result is stated for the best possible policy in the set ΓN (see Equation 2), since
inf γN ∈ΓN Jβ(µ, T , γN ) = J N
β (µ, T ), however, in the proof, instead of working with the policy
achieving this inﬁmum, we construct a policy (possibly sub-optimal) which achieves the stated
upper bound, and this upper bound naturally becomes an upper bound for the best possible ﬁnite
window policy in ΓN .

Theorem 16 Under Assumption 4, if β < 1

4αZ +1 we have

β (µ, T ) − J ∗
J N

β(µ, T ) ≤ K(β, αX, αc, (cid:107)c(cid:107)∞)αN βN

where K(β, αX, αc, (cid:107)c(cid:107)∞) is a constant that depends on β, αX, αc and (cid:107)c(cid:107)∞. (The exact formula
for the constant can be found in the appendix).

16

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

Proof Recall the optimal policy for the ﬁnite model constructed in Section 3 is denoted by γ∗
N .
Notice that γ∗
N is a stationary policy and optimal for any initial point since it solves the discounted
cost optimality equation.

Now, we construct the following policy. Use the policy γ∗

N , after time N unaltered, but mod-
ify the ﬁrst N time-stage policies, as a batch update, which can be solved via a ﬁnite dynamic
programming algorithm.

˜γ0, . . . , ˜γN −1 = argminγ1,. . . ,γN −1

N −1
(cid:88)

E[

k=0

βkc(xk, uk)] + βkE[Jβ(γ∗

N )|IN ]

where IN is the history by time N . We denote this policy by γ++ := {˜γ0, . . . , ˜γN −1, γ∗

N , . . . }.
Note that we denote the true optimal policy for the original model by γ∗. We now deﬁne the
policy γ+ := {γ∗
N , . . . }: Apply γ∗ until time N , and then use our ﬁnite window
policy γ∗N . Note that this policy is not practical as it assumes that the controller already knows the
true optimal policy γ∗, however, we will make use this hypothetical policy for the analysis.

[0,N −1], γ∗

N , γ∗

N , γ∗

From the way γ+ is constructed, we clearly, Jβ(γ++) ≤ Jβ(γ+). And thus,

Jβ(γ++) − Jβ(γ∗) ≤ Jβ(γ+) − Jβ(γ∗)

Furthermore, because of the way we constructed them, we have γ++ ∈ ΓN . Hence, we can

write

β (µ, T ) − J ∗
J N

β(µ, T ) = inf
γ∈ΓN

Jβ(µ T , γ) − Jβ(µ, T , γ∗)

≤ Jβ(µ, T , γ++) − Jβ(µ, T , γ∗)
≤ Jβ(µ, T , γ+) − Jβ(µ, T , γ∗)

Then, we start analyzing the error term: because γ+ and γ∗ use the same policy before time N , we
have

Jβ(µ, T , γ+) − Jβ(µ, T , γ∗)

=

∞
(cid:88)

t=N

= βN

βtEµ

(cid:2)c(Xt, γ∗

N (Y[t−N,t], U[t−N,t−1])(cid:3) −

∞
(cid:88)

t=N

βtEµ

(cid:2)c(Xt, γ∗(Y[0,t], U[0,t−1])(cid:3)

(cid:20)

βt−N Eγ∗
µ

∞
(cid:88)

t=N

Eµ

(cid:2)c(Xt, γ∗

N (Y[t−N,t], U[t−N,t−1])(cid:3)

− Eµ

(cid:2)c(Xt, γ∗(Y[0,t], U[0,t−1])(cid:3) |Y[0,N ], U[0,N −1]

(cid:21)

= βN Eγ∗
µ

(cid:2)Jβ(πN−, T , γ∗

N ) − Jβ(πN−, T , γ∗)|Y[0,N ], U[0,N −1]

(cid:3)

(10)

The last step follows from the observation that conditioning on the observations and control actions
Y[0,N ], U[0,N −1], the state process can be thought as if it starts at time t = N whose prior measure
is πN− and from the fact that the probability measure of Y[0,N ], U[0,N −1] is determined by the initial
measure µ and the policy γ∗ since γN
t for t ≤ N .

t = γ∗

The result then follows from Theorem 12.

17

KARA AND Y ¨UKSEL

4.2 Analysis via Uniform Bounds on Filter Approximation Error

The following result provides an alternative setup where our analysis is applicable when the ﬁlter
stability error is presented in terms of a uniform bound on the ﬁlter approximation error under the
total variation distance. We deﬁne this bound, arising from ﬁlter stability error, as follows:

¯LT V := sup

π∈P(X)

sup
γ∈Γ

sup
y[0,N ],u[0,N −1]

(cid:13)
(cid:13)
(cid:13)P π(·|y[0,N ], u[0,N −1]) − P ˆπ(·|y[0,N ], u[0,N −1])
(cid:13)
(cid:13)
(cid:13)T V

.

(11)

Theorem 17 If β < 1
αZ
have that

, where αZ can be chosen as (3 − 2δ(Q))(1 − ˜δ(T )) by Theorem 7, we

(i)

(ii)

sup
z

(cid:12)
(cid:12)J N

β (z) − J ∗

β(z)(cid:12)

(cid:12) ≤

(αZ − 1)β + 1
(1 − β)2(1 − αZ β)

(cid:107)c(cid:107)∞ ¯LT V .

(cid:12)
(cid:12)Jβ(z, γN ) − J ∗

β(z)(cid:12)

(cid:12) ≤

sup
z

2(1 + (αZ − 1)β)
(1 − β)3(1 − αZ β)

(cid:107)c(cid:107)∞ ¯LT V .

Before the proof, we note that the bound applies for all β < 1, if (3 − 2δ(Q))(1 − ˜δ(T )) < 1.
Furthermore, since (3 − 2δ(Q))(1 − ˜δ(T )) = 2α − (1 − ˜δ(T )) where α = (2 − δ(Q))(1 − ˜δ(T ))
and, therefore, if α < 1 we have that for all β < 1/(1 + ˜δ(T )) the result applies independent of
δ(Q), though this is a conservative bound. If δ(Q) = 1, the bound applies for all β < 1 as long as
α < 1.
Proof (i) We start by writing the ﬁxed point equations

J ∗
β(z) = min

u

(cid:18)

(cid:90)

˜c(z, u) + β

J ∗
β(z1)η(dz1|z, u)

(cid:19)

(cid:18)

J N
β (z) = min

u

˜c(F (z), u) + β

(cid:90)

J N
β (z1)η(dz1|F (z), u)

(cid:19)

.

Hence we can write that

β (z)(cid:12)
(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)

u

(cid:12)
(cid:12)J ∗

β(z) − J N

sup
z

(cid:12) ≤ sup
u

|˜c(z, u) − ˜c(F (z), u)|

+ β sup

J ∗
β(z1)η(dz1|z, u) −

≤ (cid:107)c(cid:107)∞(cid:107)z − F (z)(cid:107)T V + β sup
u

+ β sup

u

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

J ∗
β(z1)η(dz1|z, u) −

≤ (cid:107)c(cid:107)∞(cid:107)z − F (z)(cid:107)T V + β sup

z

(cid:90)

(cid:12)
(cid:12)
J N
β (z1)η(dz1|F (z), u)
(cid:12)
(cid:12)

(cid:90)

(cid:12)
β(z1) − J N
(cid:12)J ∗
(cid:90)

β (z1)(cid:12)

J ∗
β(z1)η(dz1|F (z), u)
β (z)(cid:12)

(cid:12) + β(cid:107)J ∗

(cid:12)
(cid:12)J ∗

β(z) − J N

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12) η(dz1|F (z), u)

β(cid:107)BLαZ (cid:107)z − F (z)(cid:107)T V .

(12)

18

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

Note that by Theorem 7, αZ ≤ (3 − 2δ(Q))(1 − ˜δ(T )) when Z is associated with total variation
2−β
distance. We also have that (cid:107)J ∗
(1−β)(1−αZ β) (cid:107)c(cid:107)∞ when Z metrized by total variation
distance (see Lemma 24). Hence, by noting (cid:107)z − F (z)(cid:107)T V ≤ ¯LT V , we can conclude that

β(cid:107)BL ≤

(cid:12)
(cid:12)J ∗

β(z) − J N

β (z)(cid:12)

(cid:12) ≤

sup
z

(αZ − 1)β + 1
(1 − β)2(1 − αZ β)

(cid:107)c(cid:107)∞ ¯LT V .

(ii) We start by writing

sup
z

(cid:12)
(cid:12)Jβ(z, γN ) − J ∗

β(z)(cid:12)

(cid:12) ≤ sup

z

(cid:12)
(cid:12)Jβ(z, γN ) − J N

β (z)(cid:12)

(cid:12) + sup

z

(cid:12)
(cid:12)J N

β (z) − J ∗

β(z)(cid:12)
(cid:12)

where the second term is bounded by (i). We now focus on the ﬁrst term:

(cid:12)
(cid:12)Jβ(z, γN ) − J N

β (z)(cid:12)

(cid:12) ≤ sup
u

+ β

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)

|˜c(z, u) − ˜c(F (z), u)|

Jβ(z1, γN )η(dz1|z, γN (z)) −

(cid:90)

(cid:12)
(cid:12)
J N
β (z1)η(dz1|F (z), γN (z))
(cid:12)
(cid:12)

≤ (cid:107)c(cid:107)∞ (cid:107)z − F (z)(cid:107)T V + β

(cid:12)
(cid:12)Jβ(z1, γN ) − J N

β (z1)(cid:12)

(cid:12) η(dz1|z, γN (z))

(cid:90)

(cid:90)

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)
(cid:90)

+ β

+ β

+ β

(cid:12)
(cid:12)J N

β (z1) − J ∗

β(z1)(cid:12)

(cid:12) η(dz1|z, γN (z))
(cid:90)

J ∗
β(z1)η(dz1|z, γN (z)) −

(cid:12)
(cid:12)
J ∗
β(z1)η(dz1|F (z), γN (z))
(cid:12)
(cid:12)

(cid:12)
(cid:12)J ∗

β(z1) − J N

β (z1)(cid:12)

(cid:12) η(dz1|F (z), γN (z))

≤ (cid:107)c(cid:107)∞ (cid:107)z − F (z)(cid:107)T V + β sup
z
(cid:12) + β(cid:107)J ∗

β (z) − J ∗

+ 2β sup

β(z)(cid:12)

(cid:12)
(cid:12)J N

z

(cid:12)
(cid:12)Jβ(z, γN ) − J N

β (z)(cid:12)
(cid:12)

β(cid:107)BLαZ (cid:107)z − F (z)(cid:107)T V .

Thus, using (i), and (cid:107)J ∗

β(cid:107)BL ≤

2−β

(1−β)(1−αZ β) (cid:107)c(cid:107)∞, we can write

(cid:12)
(cid:12)Jβ(z, γN ) − J N

β (z)(cid:12)

(cid:12) ≤

sup
z

(1 + β)((αZ − 1)β + 1)
(1 − β)3(1 − αZ β)

(cid:107)c(cid:107)∞ ¯LT V .

We then conclude that

sup
z

(cid:12)
(cid:12)Jβ(z, γN ) − J ∗

β(z)(cid:12)

(cid:12) ≤ sup

z

(cid:12)
(cid:12)Jβ(z, γN ) − J N

β (z)(cid:12)

(cid:12) + sup

z

(cid:12)
(cid:12)J N

β (z) − J ∗

β(z)(cid:12)
(cid:12)
(1 + β)((αZ − 1)β + 1)
(1 − β)3(1 − αZ β)

(cid:107)c(cid:107)∞ ¯LT V

≤

=

(αZ − 1)β + 1
(1 − β)2(1 − αZ β)
2(1 + (αZ − 1)β)
(1 − β)3(1 − αZ β)

(cid:107)c(cid:107)∞ ¯LT V . +

(cid:107)c(cid:107)∞ ¯LT V .

19

KARA AND Y ¨UKSEL

4.3 A Discussion on the Controlled Filter Stability Problem

The above results suggest that the loss occurring from applying a ﬁnite window policy is mainly
controlled by the term

sup
π∈P(X)

sup
γ∈Γ

(cid:104)

Eγ
π

ρBL

(cid:16)

P π(·|Y[0,N ], U[0,N −1]), P ˆπ(·|Y[0,N ], U[0,N −1])

(cid:17)(cid:105)

,

(13)

or its variations where the BL metric is replaced with total variation, or the expectation is replaced
with a uniform bound (given in (11)). All these terms describe how fast two different belief-state
processes forget their initial priors when fed with the same observations/control actions under a true
distribution. Thus, any bound for this term directly applies to the main results we presented for the
loss caused by a ﬁnite window policy. This term is related to the ﬁlter stability problem and our
approximation results point out the close relation between ﬁlter stability and the performance of
ﬁnite window policies. In a way, the main result or message of this paper is perhaps to explicitly
relate ﬁnite window approximations for POMDPs to the ﬁlter stability problem.

To bound the term (13), we use Theorem 5, to achieve an exponential convergence rate in
the window size for a controlled setup. However, we should note that, Theorem 5 provides only a
sufﬁcient condition to bound the ﬁlter stability term geometrically fast in the total variation distance.
This result can be seen as too strong, if one is only interested in making this ρBL distance (13)
smaller with increasing window size. In fact, as we will see in Section 5, even when the assumptions
of Theorem 5 are not satisﬁed, the ﬁlter stability term still converges to 0. In the literature, there are
various set of assumptions to achieve ﬁlter stability. Two main approaches have been:

• The transition kernel is in some sense sufﬁciently ergodic, forgetting the initial measure and
therefore passing this insensitivity (to incorrect initializations) on to the ﬁlter process. This
condition is often tailored towards control-free models.

• The measurement channel provides sufﬁcient information about the underlying state, allow-
ing the ﬁlter to track the true state process. This approach is typically based on martingale
methods and accordingly does not often lead to rates of convergence for the ﬁlter stability
problem, but only asymptotic ﬁlter stability.

The result we use in this paper (Theorem 5) provides exponential ﬁlter stability, using a joint con-
traction property of the Bayesian ﬁlter update and measurement update steps through the Dobrushin
coefﬁcient. When these requirements are not satisﬁed, the ﬁlter stability can be checked via dif-
ferent assumptions from the literature. However, we also note that, for the controlled setup, ﬁlter
stability results are limited compared to the control free setup. A comprehensive review on ﬁlter
stability in the control-free case, is available by Chigansky et al. (2009). In the controlled case, re-
cent additional studies include McDonald and Y¨uksel (2019); McDonald and Y¨uksel (2022) where
martingale methods are used to arrive at controlled ﬁlter stability, which in turn can lead to weaker
conditions (though without rates of convergence) for near optimality of ﬁnite window policies.

With regard to Theorem 5, the following example studies an additive Gaussian model (not nec-
essarily linear) and provides different parameters for the condition (1 − δ(T )) × (2 − δ( ˆQ)) < 1 to
hold.
Example 2 Consider a system where X = Y = R and the transition and measurement kernels are
given by

xn+1 = f (xn, un) + N (0, σ2

t ),

yn = g(xn) + N (0, σ2
q )

20

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

where the functions f and g are measurable and bounded such that f (x, u) ∈ [−t, t] and g(x) ∈
[−q, q].

Note that, in our paper we assume and present our results from ﬁnite Y. Therefore, to make the
example compatible with our results, we discretize the observation space Y. We provide two dis-
cretization schemes one with ˆY1 = {−q, q} and the other with ˆY2 = {−q, 0, q}. For discretization,
we use a nearest neighbor mapping.

First, we study the observation space ˆY1 = {−q, q}. Using the nearest neighbor mapping, we

have that

ˆyn = −q
ˆyn = q

if yn = g(xn) + N (0, σ2

q ) ≤ 0,

if yn = g(xn) + N (0, σ2

q ) > 0

We then can write the following for the transition kernel T (·|x, u) ∈ P(X)

T (dxt+n|xn, un) ∼ N (f (xn, un), σ2
t )

and for the compound channel ˆQ(·|x) ∈ P( ˆY1):

ˆQ(q|xn) = P r(N (g(xn), σ2

q ) > 0),

ˆQ(−q|xn) = P r(N (g(xn), σ2

q ) ≤ 0).

For these kernels, the Dobrushin coefﬁcients can be calculated as

δ(T ) ≥ 2P r(N (t, σ2

t ) ≤ 0),

δ( ˆQ) = 2P r(N (q, σ2

q ) ≤ 0).

Notice that these probabilities are fully determined by the ratio of the mean and standard deviation
of the Gaussian in question, σt/t and σq/q . The higher the ratio, the higher the Dobrushin co-
efﬁcient. Below, we see a list of the ratio of the transition kernel and lowest possible ratio of the
measurement kernel such that (1 − δ(T )) × (2 − δ( ˆQ)) < 1. If the ratio of σq/q is higher than the
stated value, we will get exponential stability for the given transition kernel. Note that if σt/t > 1.5,
then δ(T ) > 0.5 which makes (1 − δ(T )) × (2 − δ( ˆQ)) < 1 regardless of the channel for which we
use ’any’ in table to indicate that any channel would lead to exponential ﬁlter stability.

σt
t
σq
q
δ(T )
δ( ˆQ)

1.5
any
0.50
any

1.4
0.6
0.48
0.1

1.3
0.8
0.44
0.21

1.2
1.01
0.40
0.32

1.1
1.3
0.36
0.44

1.0
1.65
0.32
0.54

0.9
2.13
0.27
0.64

0.8
3.25
0.21
0.76

0.7
5.5
0.15
0.86

0.6
8.0
0.10
0.90

0.5
20.0
0.05
0.96

0.4
70.0
0.01
0.99

0.3
1000.0
0.00
1.00

Table 1: Approximate minimum ratio of σq

q for exponential ﬁlter stability for ˆY1 = {−q, q}

We now analyze the problem for the observation space ˆY2 = {−q, 0, q}. For this set, the nearest

neighbor mapping yields that

ˆyn = −q

ˆyn = q

if yn = g(xn) + N (0, σ2

if yn = g(xn) + N (0, σ2

−q
2

,

,

q ) ≤
q
2

q ) >

ˆyn = 0

else.

21

KARA AND Y ¨UKSEL

For the compound channel, we then have

ˆQ(q|xn) = P r(N (g(xn), σ2

),

ˆQ(−q|xn) = P r(N (g(xn), σ2

q ) ≤

ˆQ(0|xn) = P r(

q
2

q ) >

q
2
≥ N (g(xn), σ2

q ) >

−q
2

).

−q
2

),

For these kernels, the Dobrushin coefﬁcients can be calculated as

δ(T ) = 2P r(N (t, σ2

δ( ˆQ) = 2P r(N (q, σ2

t ) ≤ 0),
−q
2

q ) ≤

) + P r(

−q
2

< N (q, σ2

q ) <

q
2

).

Below, we now see a list of the ratio of the transition kernel and lowest possible ratio of the mea-
surement kernel such that (1 − δ(T )) × (2 − δ( ˆQ)) < 1 for the observation space ˆY2 = {−q, 0, q}.

σt
t
σq
q
δ(T )
δ( ˆQ)

1.5
any
0.50
any

1.4
0.39
0.48
0.1

1.3
0.6
0.44
0.21

1.2
0.85
0.40
0.32

1.1
1.2
0.36
0.44

1.0
1.54
0.32
0.54

0.9
2.1
0.27
0.64

0.8
3.2
0.21
0.76

0.7
5.9
0.15
0.86

0.6
8.0
0.10
0.90

0.5
20.0
0.05
0.96

0.4
80.0
0.01
0.99

0.3
1000.0
0.00
1.00

Table 2: Approximate minimum ratio of σq

q for exponential ﬁlter stability for ˆY2 = {−q, 0, q}

5. Numerical Study

In this section, we give an outline of the algorithm used to determine the approximate belief MDP
and the ﬁnite window policy. We also present the performance of the ﬁnite window policies and the
value function error for the approximate belief MDP in relation to the window size.

A summary of the algorithm is as follows:

• We determine a ˆπ ∈ P(X) such that it puts positive measure over the state space X.

• Following Section 3, we construct the ﬁnite belief space Z N

ˆπ by taking the Bayesian update
of ˆπ for all possible realizations of the form {y0, . . . , yN , u0, . . . , uN −1}. Hence we get a
approximate ﬁnite belief space with size |Y|N +1 × |U|N .

• We calculate all transitions from each z ∈ Z N
and control action u and we map them to Z N
transition kernels ηN for the ﬁnite model.

ˆπ by considering every possible observation y
ˆπ using a nearest neighbor map and construct the

• For the ﬁnite models obtained, through value or policy iteration, we calculate the value func-

tions and optimal policies.

The example we use is a machine repair problem. In this model, we have X, Y, U = {0, 1}

with

xt =

(cid:40)

1 machine is working at time t
0 machine is not working at time t .

(cid:40)

1 machine is being repaired at time t
0 machine is not being repaired at time t .

ut =

22

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

The probability that the repair was successful given initially the machine was not working is given
by κ:

P r(xt+1 = 1|xt = 0, ut = 1) = κ

The probability that the machine breaks down while in a working state is given by θ:

P r(xt = 0|xt = 1, ut = 0) = θ

The probability that the channel gives an incorrect measurement is given by (cid:15):

P r(yt = 1|xt = 0) = P r(yt = 0|xt = 1) = (cid:15)

The one stage cost function is given by

c(x, u) =






R + E x = 0, u = 1
x = 0, u = 0
E
x = 1, u = 0
0
x = 1, u = 1
R

where R is the cost of repair and E is the cost incurred by a broken machine.

We study the example with discount factor β = 0.8, and R = 5, E = 1 and present three
different results by changing the other parameters. For all different cases, we choose ˆπ(·) =
0.1δ0(·) + 0.9δ1(·).

For the ﬁrst case, we take (cid:15) = 0.3, κ = 0.2, θ = 0.1. We analyze the performance for
N ∈ {0, . . . , 5}. To get a proper ﬁnite window policy for all N ’s we use, we let the system
run 5 time steps under the policy γ0(It) = 0 that is ut = 0 no matter what for the ﬁrst 5 time
steps. Then, we start applying the policy and start calculating the cost. In Figure 1, we plot the
approximate value functions and the cost incurred by the ﬁnite window policies, that is we plot the
terms Eγ0[|J N

β (Z5)|Y[0,5], U[0,4]] and Eγ0[Jβ(Z5, γN )|Y[0,5], U[0,4]].

Recall that our main result Theorem 12, emphasizes the relation between the ﬁlter stability term
and ’value error’ and ’robustness error’, both of which are guaranteed to converge to zero with
increasing window length, where

Filter stability term: sup

(cid:16)

(cid:104)

ρBL

P π(·|Y[0,N ]), P ˆπ(·|Y[0,N ])

(cid:17)(cid:105)

Eγ
π

π

sup
γ
β (Z5) − J ∗

Value error: Eγ0[|J N
Robustness error: Eγ0[|Jβ(Z5, γN ) − J ∗

β(Z5)|Y[0,5], U[0,4]]

β(Z5)|Y[0,5], U[0,4]]

To get the errors, we simply subtract the cost values from their minimum (largest window) which
serves as an approximation of the value function. Furthermore, we scale the errors according to
the ﬁlter stability term to get a better understanding of the error rate in relation to the ﬁlter stability
constant that is we make them start from the same value to see the decrease rates more clearly.
Figure 2 shows the relation.

23

KARA AND Y ¨UKSEL

Figure 1: Approximate value function and the performance of ﬁnite window policy for different

window sizes

Figure 2: Comparison of approximation error and ﬁlter stability term for different window sizes

It can be seen from Figure 2, that the error rate stays below the ﬁlter stability convergence rate

and goes to 0 as the window size increases.

For the second case, we take (cid:15) = 0.01, κ = 0.3, θ = 0.1. For this case, we directly plot the

scaled errors in relation to the ﬁlter stability term in Figure 3.

It can be seen that as the channel is more informative, the ﬁlter stability term gets smaller much
faster and the recent information becomes more informative. As a result, we get a faster decrease in
the error with the increasing window size.

One can notice that for the ﬁrst two cases, the Dobrushin coefﬁcient α, we deﬁned in Assump-
tion 4 is greater than 1, however, the error terms still converge to 0 with increasing window. This is
because the ﬁlter stability term gets smaller even though the Dobrushin constant α > 1. For the last
case, we select the parameters in a way to make α < 1.

We take (cid:15) = 0.3, κ = 0.4, θ = 0.3 so that α = 0.7. In Figure 4, it can be observed that the
error terms and the ﬁlter stability term converges to 0 at similar rates. αN , however, gets smaller at
a slower rate and upper bounds the convergence rate of error terms.

24

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

Figure 3: Comparison of approximation error and ﬁlter stability term for different window sizes for

an informative channel

Figure 4: Comparison of approximation error, ﬁlter stability term and Dobrushin term for different

window sizes

6. Proofs of Main Technical Results

In this section, we provide the technical proofs.

6.1 Proof of Theorem 7
We will build on the proof by Kara et al. (2019). Let X be a separable metric space. Another metric
that metrizes the weak topology on P(X) is the following:

ρ(µ, ν) =

∞
(cid:88)

m=1

2−(m+1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

X

fm(x)µ(dx) −

(cid:90)

X

(cid:12)
(cid:12)
fm(x)ν(dx)
(cid:12)
(cid:12)

,

(14)

where {fm}m≥1 is an appropriate sequence of continuous and bounded functions such that (cid:107)fm(cid:107)∞ ≤
1 for all m ≥ 1 (see Parthasarathy, 1967, Theorem 6.6, p. 47).

25

KARA AND Y ¨UKSEL

We equip P(X) with the metric ρ to deﬁne bounded-Lipschitz norm (cid:107)f (cid:107)BL of any Borel mea-

surable function f : P(X) → R. With this metric, we can start the proof:

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
(cid:107)f (cid:107)BL≤1

= sup

(cid:107)f (cid:107)BL≤1

≤ sup

(cid:107)f (cid:107)BL≤1

f (z1)η(dz1|z(cid:48)

0, u) −

(cid:90)

P(X)

f (z1)η(dz1|z0, u)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

f (z1(z(cid:48)

0, u, y1))P (dy1|z(cid:48)

0, u) −

f (z1(z(cid:48)

0, u, y1))P (dy1|z(cid:48)

0, u) −

(cid:90)

Y

(cid:90)

Y

f (z1(z0, u, y1))P (dy1|z0, u)

f (z1(z(cid:48)

0, u, y1))P (dy1|z0, u)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P(X)
(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

Y

Y

+ sup

(cid:107)f (cid:107)BL≤1

(cid:90)

Y

(cid:12)
(cid:12)f (z1(z(cid:48)

0, u, y1)) − f (z1(z0, u, y1))(cid:12)

(cid:12)P (dy1|z0, u)

≤ (cid:107)P (·|z(cid:48)

0, u) − P (·|z0, u)(cid:107)T V
(cid:12)
(cid:12)f (z1(z(cid:48)

(cid:90)

+ sup

(cid:107)f (cid:107)BL≤1

Y

0, u, y1)) − f (z1(z0, u, y1))(cid:12)

(cid:12)P (dy1|z0, u)

(15)

(16)

where, in the last inequality, we have used (cid:107)f (cid:107)∞ ≤ (cid:107)f (cid:107)BL ≤ 1.

We ﬁrst anayze the ﬁrst term:

(cid:107)P (·|z(cid:48)

0, u) − P (·|z0, u)(cid:107)T V = sup

(cid:107)f (cid:107)∞≤1

(cid:90)

|

f (y1)P (dy1|z(cid:48)

0, u) −

(cid:90)

f (y1)P (dy1|z0, u)|

= sup

(cid:107)f (cid:107)∞≤1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

(cid:90)

(cid:90)

X

X

Y
(cid:90)

−

f (y1)Q(dy1|x1)T (dx1|x0, u)z(cid:48)

0(dx0)

(cid:90)

(cid:90)

X

X

Y

(cid:12)
(cid:12)
f (y1)Q(dy1|x1)T (dx1|x0, u)z0(dx0)
(cid:12)
(cid:12)

.

For all x(cid:48)

0, x0 and for any (cid:107)f (cid:107)∞ ≤ 1, we have

f (y1)Q(dy1|x1)T (dx1|x(cid:48)

0, u) −

f (y1)Q(dy1|x1)T (dx1|x0, u)

(cid:90)

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Y
≤ (cid:107)T (·|x(cid:48)

X

0, u) − T (·|x0, u)(cid:107)T V ≤ αX|x(cid:48)

(cid:90)

(cid:90)

Y

X
0 − x0|.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Then, we can write

(cid:107)P (·|z(cid:48)

0, u) − P (·|z0, u)(cid:107)T V ≤ (1 + αX)ρBL(z(cid:48)

0, z0).

Recall the metric introduced in (14); using this metric, we now focus on the term (16):

(cid:90)

Y

(cid:12)
(cid:12)f (z1(z(cid:48)

0, u, y1)) − f (z1(z0, u, y1))(cid:12)

(cid:12)P (dy1|z0, u)

sup
(cid:107)f (cid:107)BL≤1
(cid:90)

≤

=

Y

(cid:90)

∞
(cid:88)

Y

m=1

ρ(z1(z(cid:48)

0, u, y1), z1(z0, u, y1))P (dy1|z0, u)

2−m+1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

X

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)

26

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

=

∞
(cid:88)

m=1

2−m+1

(cid:90)

Y

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

X

−

(cid:90)

X

(cid:12)
(cid:12)
P (dy1|z0, u)
fm(x1)z1(z0, u, y1)(dx1)
(cid:12)
(cid:12)

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)

−

(cid:90)

X

(cid:12)
(cid:12)
fm(x1)z1(z0, u, y1)(dx1)
P (dy1|z0, u),
(cid:12)
(cid:12)

where we have used Fubini’s theorem with the fact that supm (cid:107)fm(cid:107)∞ ≤ 1. For each m, let us deﬁne

(cid:26)

(cid:26)

I+ :=

I− :=

y1 ∈ Y :

y1 ∈ Y :

(cid:90)

X

(cid:90)

X

fm(x1)z1(z(cid:48)

0, u, y1)(dx1) >

fm(x1)z1(z(cid:48)

0, u, y1)(dx1) ≤

(cid:90)

X

(cid:90)

X

fm(x1)z1(z0, u, y1)(dx1)

(cid:27)

fm(x1)z1(z0, u, y1)(dx1)

.

(17)

(cid:27)

Then, we can write

(cid:90)

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:90)

Y

fm(x1)z1(z(cid:48)
X
(cid:90)

0, u, y1)(dx1) −

(cid:90)

X

(cid:12)
(cid:12)
fm(x1)z1(z0, u, y1)(dx1)
(cid:12)
(cid:12)

P (dy1|z0, u)

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)P (dy1|z0, u) −

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)P (dy1|z(cid:48)

0, u) −

fm(x1)z1(z0, u, y1)(dx1)P (dy1|z0, u) −

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)P (dy1|z(cid:48)

0, u) −

(cid:90)

(cid:90)

X

I+

(cid:90)

(cid:90)

I+

(cid:90)

X

(cid:90)

I−

(cid:90)

X

(cid:90)

I−

X

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)P (dy1|z(cid:48)

0, u)

fm(x1)z1(z0, u, y1)(dx1)P (dy1|z0, u)

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)P (dy1|z(cid:48)

0, u)

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)P (dy1|z0, u)

For the ﬁrst and the last term, we can write

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)P (dy1|z0, u) −

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)P (dy1|z(cid:48)

0, u)

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)P (dy1|z(cid:48)

0, u) −

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)P (dy1|z0, u)

(cid:90)

(cid:90)

I+
(cid:90)

X
(cid:90)

0, u, y1)(dx1) (cid:0)1I− − 1I+

X

I−
(cid:1) P (dy1|z(cid:48)

0, u)

fm(x1)z1(z(cid:48)
(cid:90)

−

fm(x1)z1(z(cid:48)

0, u, y1)(dx1) (cid:0)1I− − 1I+

(cid:1) P (dy1|z0, u)

Y
0, u) − P (·|z0, u)(cid:107)T V ≤ (1 + αX)ρBL(z(cid:48)
≤ (cid:107)P (·|z(cid:48)

0, z0).

(18)

For the second and the third term:
(cid:90)

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)P (dy1|z(cid:48)

0, u) −

fm(x1)z1(z0, u, y1)(dx1)P (dy1|z0, u)

fm(x1)z1(z0, u, y1)(dx1)P (dy1|z0, u) −

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)P (dy1|z(cid:48)

0, u)

(cid:90)

(cid:90)

I+
(cid:90)

X
(cid:90)

I−

X

27

=

+

+

+

X

I+

(cid:90)

(cid:90)

I+

(cid:90)

X

(cid:90)

I−

(cid:90)

X

(cid:90)

I−

X

X
(cid:90)

X

(cid:90)

(cid:90)

I+
(cid:90)

+

I−
(cid:90)

Y

=

(cid:90)

+

I+
(cid:90)

X
(cid:90)

I−

X

KARA AND Y ¨UKSEL

(cid:90)

(cid:90)

I+

X
(cid:90)

(cid:90)

I−

X

(cid:90)

(cid:90)

=

X
(cid:90)

I+

+

(cid:90)

I−

X

=

(cid:90)

X

fm(x1)Q(dy1|x1)T (dx1|z(cid:48)

0, u) −

fm(x1)Q(dy1|x1)T (dx1|z0, u)

fm(x1)Q(dy1|x1)T (dx1|z0, u) −

fm(x1) (Q(I+|x1) − Q(I−|x1)) T (dx1|z(cid:48)
(cid:90)

0, u)

−

fm(x1) (Q(I+|x1) − Q(I−|x1)) T (dx1|z0, u)

fm(x1)Q(dy1|x1)T (dx1|z(cid:48)

0, u)

(19)

X
≤ (1 + αX)ρBL(z(cid:48)

0, z0).

Hence, we can write that
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

(cid:90)

X

fm(x1)z1(z(cid:48)

0, u, y1)(dx1) −

Y
≤ 2(1 + αX)ρBL(z(cid:48)

0, z0)

Now we go back to the term (16):

(cid:90)

X

fm(x1)z1(z0, u, y1)(dx1)

(cid:12)
(cid:12)
P (dy1|z0, u)
(cid:12)
(cid:12)

(cid:90)

Y

(cid:12)
(cid:12)f (z1(z(cid:48)

0, u, y1)) − f (z1(z0, u, y1))(cid:12)

(cid:12)P (dy1|z0, u)

sup
(cid:107)f (cid:107)BL≤1
(cid:90)

≤

=

Y

(cid:90)

∞
(cid:88)

Y

m=1

ρ(z1(z(cid:48)

0, u, y1), z1(z0, u, y1))P (dy1|z0, u)

2−m+1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

X

fm(x1)z1(z(cid:48)

0, u, y1)(dx1) −

(cid:90)

(cid:90)

=

∞
(cid:88)

2−m+1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤ 2(1 + αX)ρBL(z(cid:48)

m=1

Y

X

0, z0).

fm(x1)z1(z(cid:48)

0, u, y1)(dx1) −

Thus, combining all the results:

(cid:90)

X

(cid:90)

X

(cid:12)
(cid:12)
fm(x1)z1(z0, u, y1)(dx1)
P (dy1|z0, u)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
P (dy1|z0, u)
fm(x1)z1(z0, u, y1)(dx1)
(cid:12)
(cid:12)

f (z1)η(dz1|z(cid:48)

0, u) −

(cid:90)

P(X)

(cid:12)
(cid:12)
f (z1)η(dz1|z0, u)
(cid:12)
(cid:12)

P(X)

(cid:90)

(cid:12)
(cid:12)
sup
(cid:12)
(cid:12)
(cid:107)f (cid:107)BL≤1
≤ (cid:107)P (·|z(cid:48)

0, u) − P (·|z0, u)(cid:107)T V
(cid:12)
(cid:12)f (z1(z(cid:48)

(cid:90)

+ sup

(cid:107)f (cid:107)BL≤1
≤ 3(1 + αX)ρBL(z(cid:48)

Y

0, z0).

0, u, y1)) − f (z1(z0, u, y1))(cid:12)

(cid:12)P (dy1|z0, u)

We now prove part (ii); we note ﬁrst that if (cid:107)P (·|z(cid:48)
the result follows. Hence, we write (by Dobrushin, 1956)

0, u)−P (·|z0, u)(cid:107)T V ≤ 2(1−δ(Q))ρBL(z(cid:48)

0, z0),

(cid:107)P (·|z(cid:48)

0, u) − P (·|z0, u)(cid:107)T V = sup

(cid:107)f (cid:107)∞≤1

(cid:90)

|

f (y1)P (dy1|z(cid:48)

0, u) −

(cid:90)

f (y1)P (dy1|z0, u)|

= sup

(cid:107)f (cid:107)∞≤1

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

f (y1)Q(dy1|x1)T (dx1|z(cid:48)

0, u) −

(cid:90)

(cid:12)
(cid:12)
f (y1)Q(dy1|x1)T (dx1|z0, u)
(cid:12)
(cid:12)

28

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

≤ (1 − δ(Q))(cid:107)T (dx1|z(cid:48)

0, u) − T (dx1|z0, u)(cid:107)T V ≤ (1 − δ(Q))(1 + αX)ρBL(z(cid:48)

0, z0).

To prove part (iii) of the theorem, we start from the terms (15) and (16). For (15), we have

(cid:107)P (·|z(cid:48)

0, u) − P (·|z0, u)(cid:107)T V

= sup

(cid:107)f (cid:107)∞≤1

≤ sup

(cid:107)f (cid:107)∞≤1

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)

f (y1)Q(dy1|x1)T (dx1|x0, u)z(cid:48)

0(dx0) −

(cid:90)

f (y1)Q(dy1|x1)T (dx1|x0, u)z0(dx0)

f (x0)z0(dx0) −

(cid:90)

f (x0)z(cid:48)

0(dx0)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (cid:107)z0 − z(cid:48)

0(cid:107)T V .

(cid:12)
(cid:12)
(cid:12)
(cid:12)

For (16), as before, we start by writing

sup
(cid:107)f (cid:107)BL≤1
∞
(cid:88)

≤

m=1

2−m+1

(cid:90)

Y

(cid:12)
(cid:12)f (z1(z(cid:48)

0, u, y1)) − f (z1(z0, u, y1))(cid:12)

(cid:12)P (dy1|z0, u)

(cid:90)

Y

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

X

fm(x1)z1(z(cid:48)

0, u, y1)(dx1)

−

(cid:90)

X

fm(x1)z1(z0, u, y1)(dx1)

(cid:12)
(cid:12)
P (dy1|z0, u).
(cid:12)
(cid:12)

For each m, using the bounding terms (18) and (19), we have

(cid:90)

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Y
≤ (cid:107)P (·|z(cid:48)

X

fm(x1)z1(z(cid:48)

0, u, y1)(dx1) −

(cid:90)

X

(cid:12)
(cid:12)
fm(x1)z1(z0, u, y1)(dx1)
(cid:12)
(cid:12)

P (dy1|z0, u)

0, u) − P (·|z0, u)(cid:107)T V + (cid:107)T (·|z(cid:48)

0, u) − T (·|z0, u)(cid:107)T V ≤ 2(cid:107)z(cid:48)

0 − z0(cid:107)T V

which completes the proof of part (iii).

For part (iv), for (15) we have (by Dobrushin, 1956)

(cid:107)P (·|z(cid:48)

0, u) − P (·|z0, u)(cid:107)T V

(cid:90)

(cid:90)

(cid:107)f (cid:107)∞≤1

= sup

f (y1)Q(dy1|x1)T (dx1|z(cid:48)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤ (1 − δ(Q))(cid:107)T (dx1|z(cid:48)
0, u) − T (dx1|z0, u)(cid:107)T V
(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)

f (x1)T (dx1|x0, u)z(cid:48)

= (1 − δ(Q))

0, u) −

sup
(cid:107)f (cid:107)∞≤1

(cid:12)
(cid:12)
f (y1)Q(dy1|x1)T (dx1|z0, u)
(cid:12)
(cid:12)

0(dx0) −

(cid:90)

f (x1)T (dx1|x0, u)z(cid:48)

(cid:12)
(cid:12)
0(dx0)
(cid:12)
(cid:12)

≤ (1 − δ(Q))(1 − ˜δ(T ))(cid:107)z(cid:48)

0 − z0(cid:107)T V .

For (16), (using Lemma 3.2 McDonald and Y¨uksel, 2020) and the analysis above,

fm(x1)z1(z(cid:48)

0, u, y1)(dx1) −

(cid:90)

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

X

Y
≤ (2 − δ(Q))(cid:107)T (·|z(cid:48)

(cid:90)

X

fm(x1)z1(z0, u, y1)(dx1)

(cid:12)
(cid:12)
P (dy1|z0, u)
(cid:12)
(cid:12)

0, u) − T (·|z0, u)(cid:107)T V ≤ (2 − δ(Q))(1 − ˜δ(T ))(cid:107)z(cid:48)

0 − z0(cid:107)T V

which completes the proof.

29

KARA AND Y ¨UKSEL

6.2 Proof of Theorem 9

Before presenting our proof program and the series of technical results needed, we introduce some
notation.

We denote the loss function due to the quantization by L(z), i.e.

L(z) = ρBL(z, F (z))

with F deﬁned in Equation 8.

In the following, to specify the probability measures according to which the expectations are
N and for the

taken, we use the following notation; for the kernel ηN and a policy γ, we use Eγ
kernel η and a policy γ, we will use Eγ.

β and the optimal policy by γ∗

The last notation we introduce is the following: Recall that we denote the optimal cost for the
ﬁnite model by J N
ˆπ , however, we
can always extend them over the original state space Z so that they are constant over the quantization
bins. We denote the extended versions by ˜J N
β and ˜γ∗
N .
Now, we introduce our value iteration approach for the original model and the ﬁnite model. We

N . These are deﬁned on a ﬁnite set Z N

write for any k < ∞

(cid:18)

vk+1(z) = min

u

˜c(z, u) + β

(cid:90)

vk(z1)η(dz1|z, u)

∀z ∈ Z,

(cid:19)

(cid:32)

vN
k+1(z) = min

u

cN (z, u) + β

(cid:88)

z1

(cid:33)

k (z1)ηN (z1|z, u)
vN

∀z ∈ Z N
ˆπ .

where v0, vN
0 ≡ 0. It is well known that the above operations deﬁne contractions under either model
and hence the value functions converge to the optimal expected discounted cost. In particular, we
have that

(cid:12)
(cid:12)J N

β (z) − vN

k (z)(cid:12)

(cid:12) ≤ (cid:107)˜c(cid:107)∞

βk
1 − β

,

(cid:12)
(cid:12)J ∗

β(z) − vk(z)(cid:12)

(cid:12) ≤ (cid:107)˜c(cid:107)∞

βk
1 − β

.

Notice that if we extend vN
k+1(z) and ˜cN (z, u)
becomes constant over the quantization bins. Then, the extended value functions for the ﬁnite model
can be also obtained with

k+1(z) and cN (z, u) over all of the state space then ˜vN

(cid:18)

˜vN
k+1(z) = min

u

˜c(F (z), u) + β

(cid:90)

˜vN
k (z1)η(dz1|F (z), u)

(cid:19)

∀z ∈ Z.

(20)

To see this, observe the following:

(cid:18)

˜vN
k+1(z) = min

u

c(F (z), u) + β

(cid:90)

˜vN
k (z1)η(dz1|F (z), u)

(cid:19)



= min

u

˜c(F (z), u) + β



= min

u

˜c(F (z), u) + β

ZN
i

|Z N
ˆπ |
(cid:88)

(cid:90)

i=1

|Z N
ˆπ |
(cid:88)

i=1



˜vN
k (z1)η(dz1|F (z), u)



k (zi
˜vN
1)

(cid:90)

Z N
i



η(dy|F (z), u)



30

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS



= min

u

˜c(F (z), u) + β

|Z N
ˆπ |
(cid:88)

i=1



k (zi
˜vN

1)η(Z N

i |F (z), u)



where Z n
bin and it is constant over the bin.

i denotes the ith quantization bin and ˜vN

k (zi

1) denotes the value of ˜vN

k at that quantization

For the proof the goal is to bound the term |Jβ(˜γ∗
N , z) − Jβ(γ∗, z)|. We will see in the following
that bounding this term is related to studying the term | ˜J N
N , z) − Jβ(γ∗, z)| (the value function
approximation error). Therefore, in what follows, we ﬁrst analyze | ˜J N
N , z) − Jβ(γ∗, z)| and
observe that it can be bounded using the expected loss terms E[L(Zt)]. Finally, we will observe
that upper bounds on these expressions can be written through the ﬁlter stability term

β (˜γ∗

β (˜γ∗

sup
π∈P(X)

sup
γ∈Γ

(cid:104)

Eγ
π

ρBL

(cid:16)

P π(XN ∈ ·|Y[0,N ], γ(Y[0,N −1])), P ˆπ(XN ∈ ·|Y[0,N ]), γ(Y[0,N −1])

(cid:17)(cid:105)

.

Next, we present some supporting technical results.

Lemma 18 Under Assumption 3, we have

| ˜J N

β (˜γ∗

N , z) − Jβ(γ∗, z)|
(cid:32)

(cid:32)

≤ lim
k→∞

sup
γ∈Γ

α˜c

k−1
(cid:88)

t=0

βt

Eγ

z [L(Zt)] + 3αZ

t−1
(cid:88)

(4αZ + 1)mEγ

z [L(Zt−m−1)]

(cid:33)

m=0

(cid:32)

βt+1(cid:107)vk−t−1(cid:107)BL

Eγ

z [L(Zt)] + 3αZ

+ αZ

k−1
(cid:88)

t=0

t−1
(cid:88)

(4αZ + 1)mEγ

z [L(Zt−m−1)]

(cid:33) (cid:33)
.

m=0

where vk denotes the value function at a time step k that is produced by the value iteration with
v0 ≡ 0.

Proof

We follow a value iteration approach to write for any k < ∞

(cid:18)

vk+1(z) = min

u

˜c(z, u) + β

(cid:90)

vk(y)η(dy|z, u)

∀z ∈ Z,

(cid:19)

˜vN
k+1(z) = min

u

(cid:18)

˜c(F (z), u) + β

(cid:90)

˜vN
k (y)η(dy|F (z), u)

(cid:19)

∀z ∈ Z.

where v0, ˜vN

0 ≡ 0.
We then write

| ˜J N

β (z) − J ∗

β(z)| ≤ | ˜J N

β (z) − ˜vN

k (z)| + |˜vN

k (z) − vk(z)| + |vk(z) − J ∗

β(z)|

(21)

For the ﬁrst and the last term, using the fact that the dynamic programming operator is a con-

traction, we have that

(cid:12)
(cid:12) ˜J N

β (z) − ˜vN

k (z)(cid:12)

(cid:12) ≤ (cid:107)˜c(cid:107)∞

(cid:12)
(cid:12)J ∗

β(z) − vk(z)(cid:12)

(cid:12) ≤ (cid:107)˜c(cid:107)∞

βk
1 − β

.

βk
1 − β

,

31

KARA AND Y ¨UKSEL

Now, we focus on the second term |˜vN
Step 1: We show in the Appendix, Section A.1, that for all k ≥ 1,

k (z) − vk(z)|.

|˜vN

k (z) − vk(z)|
(cid:32)
k−1
(cid:88)

≤ sup
γ∈Γ

α˜c

βtEγ

z,N [L(Zt)] +

k−1
(cid:88)

βt+1(cid:107)vk−t−1(cid:107)BLEγ

z,N [L(Zt)] αZ

(cid:33)

t=0
where L(z) is the loss function due to the quantization, i.e. L(z) = ρBL(z, ˆz) where ˆz is the
representative state z belongs to.

t=0

Step 2: We show in the Appendix, Section A.2, that

Eγ

z,N [L(Zt)] ≤ Eγ

z [L(Zt)] + 3αZ

t−1
(cid:88)

(4αZ + 1)mEγ

z [L(Zt−m−1)].

m=0

Step 3: Combining our results, we have that

|˜vN

k (z) − vk(z)|
(cid:32)
k−1
(cid:88)

≤ sup

α˜c

γ

(cid:32)

≤ sup

γ

α˜c

t=0
k−1
(cid:88)

t=0

+ αZ

k−1
(cid:88)

t=0

βtEγ

z,N [L(Zt)] +

k−1
(cid:88)

t=0

βt+1(cid:107)vk−t−1(cid:107)BLEγ

z,N [L(Zt)] αZ

(cid:32)

βt

Eγ

z [L(Zt)] + 3αZ

t−1
(cid:88)

(4αZ + 1)mEγ

z [L(Zt−m−1)]

(cid:33)

(cid:33)

m=0

(cid:32)

βt+1(cid:107)vk−t−1(cid:107)BL

Eγ

z [L(Zt)] + 3αZ

t−1
(cid:88)

(4αZ + 1)mEγ

z [L(Zt−m−1)]

(cid:33) (cid:33)
.

Hence, using (21) and (22), we write
| ˜J N

β (z) − J ∗

|˜vN

k (z) − vk(z)|

β(z)| ≤ lim
k→∞
(cid:32)

(cid:32)

≤ lim
k→∞

sup
γ

α˜c

βt

Eγ

z [L(Zt)] + 3αZ

k−1
(cid:88)

t=0

m=0

(22)

t−1
(cid:88)

(4αZ + 1)mEγ

z [L(Zt−m−1)]

(cid:33)

m=0

(cid:32)

βt+1(cid:107)vk−t−1(cid:107)BL

Eγ

z [L(Zt)] + 3αZ

+ αZ

k−1
(cid:88)

t=0

t−1
(cid:88)

(4αZ + 1)mEγ

z [L(Zt−m−1)]

(cid:33) (cid:33)
.

m=0

The next result, gives a bound on the loss function occurring from the quantization (on L(z))

and relates the bound to the ﬁlter stability problem.

Lemma 19 For Z0 = P π0(XN ∈ ·|Y[0,N ], γ(Y[0,N −1])), where π0 ∈ P(X) is the prior distribution
of X0, we have that for any N < t < ∞

Eγ
π0

(cid:104)

Eγ
Z0

sup
γ∈Γ

[L(Zt)] |Y[0,N ], γ (cid:0)Y[0,N −1]

(cid:1)(cid:105)

≤ sup

π∈P(X)

Eγ
π

sup
γ∈Γ

(cid:16)

(cid:104)
ρBL

P π(XN ∈ ·|Y[0,N ], γ(Y[0,N −1])), P ˆπ(XN ∈ ·|Y[0,N ]), γ(Y[0,N −1])

(cid:17)(cid:105)

.

32

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

Proof We ﬁrst note that, since the quantizer is a nearest-neighbor quantization, the quantization
error is almost surely upper bounded by
ρBL
law of total expectation:

(cid:0)P πt− (XN +t ∈ ·|Y[t,t+N ], U[t,t+N −1]), P ˆπ(XN +t ∈ ·|Y[t,t+N ], U[t,t+N −1])(cid:1). Using this and the

Eγ
π0

(cid:2)Eγ

≤ sup
γ∈Γ

z0 [L(Zt)] |Y[0,N ], γ(Y[0,N −1])(cid:3)
(cid:18)
Eγ
π0

(cid:20)
Eγ

ρBL

(cid:20)

πt−

P πt− (XN +t ∈ ·|Y[t,t+N ], U[t,t+N −1])

≤ sup

π∈P(X)

Eγ
π

sup
γ∈Γ

(cid:16)

(cid:104)

ρBL

P π(XN ∈ ·|Y[0,N ], U[0,N −1]), P ˆπ(XN ∈ ·|Y[0,N ], U[0,N −1])

, P ˆπ(XN +t ∈ ·|Y[t,t+N ], U[t,t+N −1])

(cid:21)

(cid:19)(cid:21)(cid:12)
(cid:12)
(U, Y )[0,t−1]
(cid:12)
(cid:12)
(cid:17)(cid:105)

where (U, Y )[0,t−1] = U[0,t−1], Y[0,t−1].

Lemma 20 Under Assumption 3 we have that

(cid:107)vk(cid:107)BL ≤ α˜c

k−1
(cid:88)

(βαZ )t + (cid:107)˜c(cid:107)∞

t=0

k−1
(cid:88)

t=0

(βαZ )t 1 − βk−t
1 − β

.

In particular, we have that for all k

(cid:107)vk(cid:107)BL ≤

1
1 − βαZ

(cid:18) (cid:107)˜c(cid:107)∞
1 − β

(cid:19)

+ α˜c

.

Proof It is easy to see that (cid:107)vk(cid:107)∞ ≤ (cid:107)˜c(cid:107)∞
approach and assume the claim holds for k and analyze the term k + 1

t=0 βt = (cid:107)˜c(cid:107)∞

(cid:80)k−1

1−βk
1−β . Then, we use an inductive

(cid:107)vk+1(cid:107)BL = (cid:107)vk+1(cid:107)∞ + sup
x(cid:54)=y

|vk+1(x) − vk+1(y)|
ρBL(x, y)

For the second term, we have

(cid:18)

|vk+1(x) − vk+1(y)| ≤ sup
u

|˜c(x, u) − ˜c(y, u)| + β

≤ α˜cρBL(x, y) + β(cid:107)vk(cid:107)BLαZ ρBL(x, y).

Hence, using the induction hypothesis, we have that

(cid:107)vk+1(cid:107)BL ≤ (cid:107)vk+1(cid:107)∞ + α˜c + βαZ (cid:107)vk(cid:107)BL

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)

vk(z)η(dz|x, u) −

(cid:90)

(cid:12)
(cid:12)
vk(z)η(dz|y, u)
(cid:12)
(cid:12)

(cid:19)

≤ (cid:107)˜c(cid:107)∞

1 − βk+1
1 − β

(cid:32)

+ α˜c + βαZ

α˜c

k−1
(cid:88)

(βαZ )t + (cid:107)˜c(cid:107)∞

t=0

k−1
(cid:88)

t=0

(βαZ )t 1 − βk−t
1 − β

(cid:33)

k
(cid:88)

= α˜c

(βαZ )t(cid:48)

+ (cid:107)˜c(cid:107)∞

k
(cid:88)

(βαZ )t(cid:48) 1 − βk−t(cid:48)

,

t(cid:48) = t + 1,

1 − β

t(cid:48)=0

t(cid:48)=0

33

KARA AND Y ¨UKSEL

which concludes the induction argument. Therefore, we can write

(cid:107)vk(cid:107)BL ≤ α˜c

k−1
(cid:88)

t=0

(βαZ )t + (cid:107)˜c(cid:107)∞

k−1
(cid:88)

t=0

(βαZ )t 1 − βk−t
1 − β

(cid:18)

=

α˜c +

(cid:107)˜c(cid:107)∞
1 − β

(cid:19) 1 − (βαZ )k
1 − βαZ

−

(cid:107)˜c(cid:107)∞βk(1 − αk
Z )
(1 − β)(1 − αZ )

.

Now, we can prove Theorem 9.

Proof of Theorem 9
Consider the following dynamic programming operators, ˆTn, Tn : Bb(Z) → Bb(Z) (Bb(Z)
denotes the set of measurable and bounded functions on Z) such that for any f ∈ Bb(Z) and for
any z ∈ Z

(Tn(f ))(z) := c(z, ˜γ∗

N (z)) + β

(cid:90)

f (y)η(dy|z, ˜γ∗

N (z))

( ˆTn(f ))(z) := c(F (z), ˜γ∗

N (z)) + β

(cid:90)

f (y)η(dy|F (z), ˜γ∗

N (z))

where ˜γ∗
space Z, i.e.
quantization map such that it maps z to the closest point from the ﬁnite state space Z N
ρBL is used to metrize the belief space).

N denotes the extension of the optimal policy for the ﬁnite state space model to the belief
it is constant over the quantization bins, furthermore F (z) is the nearest neighbor
ˆπ (recall that

The optimal cost for the ﬁnite model, J N

β the
extension of the optimal cost for the ﬁnite model to the belief space P(X) by assigning the same
values over the quantizaton bins , i.e. it is a piece-wise constant function over the quantization bins.
Note that, we have

β is only deﬁned on a ﬁnite set, we denote by ˜J N

Jβ(˜γ∗
˜J N
β (˜γ∗

N , z) = Tn(Jβ(˜γ∗
N , z) = ˆTn( ˜J N
β (˜γ∗

N , z)),
N , z)).

Using these equalities, we write

Jβ(˜γ∗

N , z) − Jβ(γ∗, z) ≤ |Tn(Jβ(˜γ∗

N , z)) − Tn(Jβ(γ∗, z))|
+ |Tn(Jβ(γ∗, z)) − ˆTn(Jβ(γ∗, z))|
+ | ˆTn(Jβ(γ∗, z)) − ˆTn( ˜J N
N , z))|
+ | ˜J N
N , z) − Jβ(γ∗, z)|

β (˜γ∗

β (˜γ∗

(cid:90)

≤ β

|Jβ(˜γ∗

N , z1) − Jβ(γ∗, z1)|η(dz1|z, ˜γ∗

N (z))

N (z)) − c(F (z), ˜γ∗

N (z))|

+ |c(z, ˜γ∗
(cid:90)

+ β|

Jβ(γ∗, z1)η(dz1|z, ˜γ∗

N (z)) −

(cid:90)

Jβ(γ∗, z1)η(dz1|F (z), ˜γ∗

N (z))|

(cid:90)

+ β

| ˜J N

β (˜γ∗

N , z1) − Jβ(γ∗, z1)|η(dz1|F (z), ˜γ∗

N (z))

34

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

+ | ˜J N
β (˜γ∗
(cid:18)

N , z) − Jβ(γ∗, z)|

(cid:19)

≤ β sup

γ

Eγ

z [|Jβ(˜γ∗

N , Z1) − Jβ(γ∗, Z1)|]

+ αcL(z) + βαZ L(z)(cid:107)J ∗

β(cid:107)BL

+ βEγ

z,N [| ˜J N

β (˜γ∗

N , Z1) − Jβ(γ∗, Z1)|] + | ˜J N

β (˜γ∗

N , z) − Jβ(γ∗, z)|

(23)

where EN
point is z.

z,γ denotes the expectation with respect the the kernel η(dy|F (z), γ(z)) when the initial

Now, using Lemma 18, we can write that:

βt

Eγ

z [L(Zt)] + 3αZ

t−1
(cid:88)

(4αZ + 1)mEγ

z [L(Zt−m−1)]

(cid:33)

| ˜J N

β (˜γ∗

N , z) − Jβ(γ∗, z)|
(cid:32)

(cid:32)

k−1
(cid:88)

≤ lim
k→∞

sup
γ

αc

k−1
(cid:88)

+ αZ

t=0
:= g(z).

t=0

m=0

(cid:32)

βt+1(cid:107)vk−t−1(cid:107)BL

Eγ

z [L(Zt)] + 3αZ

t−1
(cid:88)

(4αZ + 1)mEγ

z [L(Zt−m−1)]

(cid:33) (cid:33)

m=0

(24)

We ﬁrst introduce the following notation along with the above notation (24)

f (z) := |Jβ(˜γ∗

N , z) − Jβ(γ∗, z)|.

Notice that with the new notation, we can rewrite the bound on (23) as:

f (z) ≤ β sup

γ

≤ β sup

γ

Eγ

z [f (Z1)] + αcL(z) + βαZ L(z)(cid:107)J ∗

β(cid:107)BL + βEγ

z,N [g(Z1)] + g(z)

Eγ

z [f (Z1)] + αcL(z) + βαZ L(z)(cid:107)J ∗

β(cid:107)BL + βEγ

z [g(Z1)] + β(cid:107)g(cid:107)BLαZ L(z) + g(z)

where we used the fact that

Eγ

z,N [g(Z1)] − Eγ

z [g(Z1)] =

(cid:90)

g(z1)η(dz1|F (z), γ(z)) −

(cid:90)

g(z1)η(dz1|z, γ(z))

Using the same bound on f (Z1) one can also write that for any initial point z0:

≤ (cid:107)g(cid:107)BLαZ L(z).

f (z0) ≤ sup
γ∈Γ

β2Eγ

z0[f (Z2)] + αc

1
(cid:88)

t=0

+

1
(cid:88)

t=0

βt+1Eγ

z0[g(Zt)] + (cid:107)g(cid:107)BLαZ

βtEγ

z0[L(Zt)] + αZ (cid:107)J ∗

β(cid:107)BL

1
(cid:88)

βt+1Eγ

z0[L(Zt)]

1
(cid:88)

t=0

βt+1Eγ

z0[L(Zt)] +

t=0
1
(cid:88)

t=0

βtEγ

z0[g(Zt)].

In general, for any k < ∞, we can write

(cid:32)

f (z0) ≤ sup
γ∈Γ

βkEγ

z0[f (Zk)] + αc

k−1
(cid:88)

t=0

βtEγ

z0[L(Zt)] + αZ (cid:107)J ∗

β(cid:107)BL

k−1
(cid:88)

t=0

βt+1Eγ

z0[L(Zt)]

35

KARA AND Y ¨UKSEL

+

k−1
(cid:88)

t=0

βt+1Eγ

z0[g(Zt)] + (cid:107)g(cid:107)BLαZ

k−1
(cid:88)

t=0

βt+1Eγ

z0[L(Zt)] +

k−1
(cid:88)

t=0

βtEγ

z0[g(Zt)]

(cid:33)
.

(25)

Recall that our main goal is to bound Eπ0[f (Z0)|Y[0,N ], γ(Y[0,N −1])] where Z0 = Pπ0(XN ∈
·|Y[0,N ], γ(Y[0,N −1]). To this end, ﬁrst notice that using Lemma 19, we have for any t

Eπ0

(cid:104)

Eγ
Z0

(cid:105)
[L(Zt)]|Y[0,N ], γ(Y[0,N −1])

≤ sup

π∈P(X)

sup
γ∈Γ

Eπ,γ (cid:104)

ρBL

(cid:16)

P π(XN ∈ ·|Y[0,N ]), P ˆπ(XN ∈ ·|Y[0,N ])

(cid:17)(cid:105)

:= B.

(26)

Using this bound and Lemma 20 for (cid:107)vk−t−1(cid:107)BL, we can write that

(cid:104)
Eγ
Z0

E

[g(Zt)](cid:12)
(cid:32)

≤ lim
k→∞

αc

(cid:105)
(cid:12)Y[0,N ], γ(Y[0,N −1])

k−1
(cid:88)

t=0

(cid:32)

βt

B + 3αZ

t−1
(cid:88)

m=0
(cid:32)

+ αZ

k−1
(cid:88)

t=0

βt+1(cid:107)vk−t−1(cid:107)BL

B + 3αZ

(cid:33)

B(4αZ + 1)m

≤ B (cid:0)αc + βαZ (cid:107)J ∗

β(cid:107)BL

≤ B (cid:0)αc + βαZ (cid:107)J ∗

β(cid:107)BL

≤ B (cid:0)αc + βαZ (cid:107)J ∗

β(cid:107)BL

(cid:1) lim
k→∞

(cid:1) lim
k→∞

(cid:1) lim
k→∞

k−1
(cid:88)

t=0
k−1
(cid:88)

t=0
k−1
(cid:88)

t=0

= BK0(β, αZ , α˜c, (cid:107)˜c(cid:107)∞)

t−1
(cid:88)

m=0

B(4αZ + 1)m

(cid:33) (cid:33)

(cid:33)

(cid:32)

βt

1 + 3αZ

t−1
(cid:88)

(4αZ + 1)m

(cid:18)

1 + 3αZ

βt

m=0
(4αZ + 1)t − 1
4αZ

(cid:19)

βt(4αZ + 1)t

(27)

where

K0(β, αZ , α˜c, (cid:107)˜c(cid:107)∞) = (cid:0)αc + βαZ (cid:107)J ∗

β(cid:107)BL

(cid:1)

1
1 − β(4αZ + 1)

.

By Lemma 23 we also have that

(cid:107)g(cid:107)BL ≤(αc + βαZ (cid:107)Jβ(cid:107)BL)

(cid:18)

2
1 − β(4αZ + 1)

+

3αZ
1 − βαZ

+

9α2
Z
(1 − β(4αZ + 1))2

(cid:19)

:= ˆK0(β, αZ , α˜c, (cid:107)˜c(cid:107)∞)

Going back to (25), using (26) and (27) and taking the limit k → ∞:

E[f (Z0)|Y[0,N ], γ(Y[0,N −1])] ≤

Bαc
1 − β

+

βαZ (cid:107)J ∗

β(cid:107)BLB

1 − β

+

+

ˆK0(β, αZ , α˜c, (cid:107)˜c(cid:107)∞)αZ βB
1 − β

βK0(β, αZ , α˜c, (cid:107)˜c(cid:107)∞)B
1 − β
K0(β, αZ , α˜c, (cid:107)˜c(cid:107)∞)B
1 − β

+

.

36

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

Thus, we can write that

E[f (Z0)|Y[0,N ], γ(Y[0,N −1])] ≤
Eπ,γ (cid:104)

K sup

sup
γ∈Γ

π∈P(X)

(cid:16)

P π(XN ∈ ·|Y[0,N ]), P ˆπ(XN ∈ ·|Y[0,N ])

(cid:17)(cid:105)

,

ρBL

where

K =

αc + βαZ (cid:107)J ∗

β(cid:107)BL + (β + 1)K0(β, αZ , α˜c, (cid:107)˜c(cid:107)∞) + ˆK0(β, αZ , α˜c, (cid:107)˜c(cid:107)∞)βαZ

1 − β

(28)

and using Lemma 24

7. Conclusion

(cid:107)J ∗

β(cid:107)BL ≤

1
1 − βαZ

(cid:18) (cid:107)˜c(cid:107)∞
1 − β

(cid:19)

+ α˜c

.

In this paper, we studied performance bounds for policies that use only a ﬁnite-window of recent
observation and action variables rather than the entire history in partially observed stochastic control
problems. We have rigorously established approximation bounds that are easy to compute, and have
shown that this bound critically depends on the ergodicity and stability properties of the belief-state
process. We have provided the results for continuous-space valued state spaces and ﬁnite observa-
tion and action spaces, however our studies suggest that these results can also be generalized to real
valued observation and actions also. Application to decentralized POMDPs is another direction that
will beneﬁt from the analysis presented.

Appendix A. Technical Proofs of Supporting Results

In this section, we prove the proofs of some technical results.

A.1 Proof of Step 1 in the Proof of Lemma 18

We claim that

|˜vN

k (z) − vk(z)| ≤ sup
γ∈Γ

(cid:32)

α˜c

k−1
(cid:88)

t=0

βtEN

z,γ [L(Zt)] +

k−1
(cid:88)

t=0

βt+1(cid:107)vk−t−1(cid:107)BLEN

z,γ [L(Zt)] αZ

(cid:33)

where L(z) is the loss function due to the quantization, i.e. L(z) = ρBL(z, F (z)) with F deﬁned
in (8).

We prove the claim using an inductive approach: for k = 1 we have (noting v0, ˜vN
(cid:90)

(cid:18)

(cid:19)

0 ≡ 0)

˜vN
1 (z) = min
(cid:18)

u

˜c(F (z), u) + β

˜vN
0 (y)η(dy|F (z), u)
(cid:19)

(cid:90)

v1(z) = min

u

˜c(z, u) + β

v0(y)η(dy|z, u)

= min

u

˜c(z, u).

= min

˜c(F (z), u)

u

Note that under the stated assumptions the measurable selection conditions hold, and the minimum
can be achieved using a policy γ for the original model and a policy γN for the ﬁnite model which

37

KARA AND Y ¨UKSEL

deﬁned only on a ﬁnite set. By extending the ﬁnite model policy γN over all state space P(X), we
can write that

|˜vN

1 (z) − v1(z)| ≤ max

˜c(F (z), γ(z)) − ˜c(z, γ(z)), ˜c(z, γN (z)) − ˜c(F (z), γN (z))

(cid:18)

(cid:19)

≤ sup

γ

|˜c(F (z), γ(z)) − ˜c(z, γ(z))| ≤ α˜cL(z)

which completes the proof for k = 1. Now, we assume that the claim holds for k and analyze the
step k + 1. Similar to k = 1 case, we can write

|˜vN

k+1(z) − vk+1(z)|

≤ sup

γ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:32)

˜c(F (z), γ(z)) − ˜c(z, γ(z)) + β

≤ sup

|˜c(F (z), γ(z)) − ˜c(z, γ(z))| + β

γ

(cid:90)

˜vN
k (y)η(dy|F (z), γ(z)) − β

(cid:90)

(cid:12)
(cid:12)
vk(y)η(dy|z, γ(z))
(cid:12)
(cid:12)

(cid:90)

|˜vN

k (y) − vk(y)|η(dy|F (z), γ(z))

+ β

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

vk(y)η(dy|F (z), γ(z)) −

(cid:90)

(cid:12)
(cid:12)
vk(y)η(dy|z, γ(z))
(cid:12)
(cid:12)

(cid:33)

≤ α˜cL(z) + β(cid:107)vk(cid:107)BLαZ L(z)

+ β sup

γ

(cid:18) (cid:90) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

α˜c

k−1
(cid:88)

t=0

βtEN

y,γ [L(Zt)] +

k−1
(cid:88)

t=0

βt+1(cid:107)vk−t−1(cid:107)BLEN

(cid:12)
(cid:12)
(cid:12)
y,γ [L(Zt)] αZ ]
(cid:12)
(cid:12)

η(dy|F (z), γ(z))

(cid:19)

(cid:18)

≤ sup

γ

α˜cL(z) + α˜c

k−1
(cid:88)

t=0

βt+1EN

z,γ [L(Zt+1)] + β(cid:107)vk(cid:107)BLαZ L(z)

k−1
(cid:88)

+

(cid:32)

= sup

γ

α˜c

t=0
k
(cid:88)

t(cid:48)=0

βt+2(cid:107)vk−t−1(cid:107)BLEN

z,γ[L(Zt+1)]

(cid:19)

βt(cid:48)

EN

z,γ [L(Zt(cid:48))] +

k
(cid:88)

t(cid:48)=0

βt(cid:48)+1(cid:107)vk−t(cid:48)(cid:107)BLEN

z,γ [L(Zt(cid:48))]

(cid:33)

,

(t(cid:48) = t + 1).

For the last two steps, note that EN
state Z0 = y, thus using the iterative expectation we have

y,γ [L(Zt)] denotes the expected loss at time t when the initial

(cid:90)

EN

y,γ [L(Zt)] η(dy|F (z), γ(z)) = EN

z,γ [L(Zt+1)] .

Hence, we have proved that for all k ≥ 1

|˜vN

k (z) − vk(z)| ≤ sup
γ

(cid:18)

α˜c

k−1
(cid:88)

t=0

βtEN

z,γ [L(Zt)] +

k−1
(cid:88)

t=0

βt+1(cid:107)vk−t−1(cid:107)BLEN

z,γ [L(Zt)] αZ

(cid:19)
.

A.2 Proof of Step 2 in the Proof of Lemma 18

The claim is that

EN

z,γ [L(Zt)] ≤ Ez,γ [L(Zt)] + 3αZ

t−1
(cid:88)

(4αZ + 1)mEz,γ[L(Zt−m−1)].

m=0

38

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

We ﬁrst write

EN

z,γ [L(Zt)] ≤ Ez,γ [L(Zt)] +

(cid:18)

(cid:19)

(cid:107)L(cid:107)BL

ρBL

(cid:16)

z,t, P γ,N
P γ

z,t

(cid:17)

(29)

where

z,t, P γ,N
P γ
are the marginal distributions of the state zt for the true model and approximate model respectively,
with Z0 = z.

z,t

We focus on the term (cid:107)L(cid:107)BLρBL

. We ﬁrst claim that (cid:107)L(cid:107)BL ≤ 3, where L(z) =

(cid:16)

z,t, P γ,N
P γ

z,t

(cid:17)

ρ(z, F (z)). Recall that F determines quantization given in (8).

(cid:107)L(cid:107)BL ≤ (cid:107)L(cid:107)∞ + sup
z,z(cid:48)

|ρ(z, F (z)) − ρ(z(cid:48), F (z(cid:48)))|
ρ(z, z(cid:48))

≤ 2 + sup
z,z(cid:48)

|ρ(z, F (z)) − ρ(z(cid:48), F (z(cid:48)))|
ρ(z, z(cid:48))

where we used the fact that

(cid:107)L(cid:107)∞ = sup

z

ρBL(z, F (z)) ≤ 2

as for any two probability measures µ, ν,we have that ρBL(µ, ν) ≤ 2 (see (3)).

Note that F is a nearest neighbor quantizer as deﬁned in (8), thus we can write that
(cid:12)ρ(z, F (z)) − ρ(z(cid:48), F (z(cid:48)))(cid:12)
(cid:12)

(cid:12) ≤ max (cid:0)ρ(z, F (z(cid:48))) − ρ(z(cid:48), F (z(cid:48))), ρ(z(cid:48), F (z)) − ρ(z, F (z))(cid:1)
(cid:12) ≤ ρ(z, z(cid:48))
≤ sup

(cid:12)ρ(z, ˆz) − ρ(z(cid:48), ˆz)(cid:12)
(cid:12)

ˆz

where for the last step we used the triangle inequality. Hence we can conclude that

(cid:107)L(cid:107)BL ≤ 2 + sup
z,z(cid:48)

|ρ(z, F (z)) − ρ(z(cid:48), F (z(cid:48)))|
ρ(z, z(cid:48))

≤ 3.

Now, we show that

(cid:16)

ρBL

z,t, P γ,N
P γ

z,t

(cid:17)

≤ αZ

t−1
(cid:88)

(4αZ + 1)mEz,γ[L(Zt−m−1)].

m=0

We prove the claim by induction: for t = 1

(30)

(31)

(cid:16)

ρBL

z,1, P γ,N
P γ

z,1

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

f (z1)η(dz1|F (z), u1) −

(cid:90)

f (z1)η(z1)η(dz1|z, u1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= sup

(cid:107)f (cid:107)BL≤1
≤ αZ L(z).

Now assume the claim holds for t − 1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

z,t, P γ,N
P γ

= sup

ρBL

z,t

(cid:17)

(cid:16)

(cid:107)f (cid:107)BL≤1

(cid:90)

f (zt)η(dzt|zt−1, γ(zt−1)))P γ

z,t−1(dzt−1)

(cid:90)

−

f (zt)η(dzt|F (zt−1), γ(zt−1))P γ,N

z,t−1(dzt−1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

39

KARA AND Y ¨UKSEL

≤ sup

(cid:107)f (cid:107)BL≤1

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

f (zt)η(dzt|zt−1, γ(zt−1))P γ

z,t−1(dzt−1)

(cid:90)

−

f (zt)η(dzt|zt−1, γ(zt−1))P γ,N

(cid:12)
(cid:12)
z,t−1(dzt−1)
(cid:12)
(cid:12)

f (zt)η(dzt|zt−1, γ(zt−1))P γ,N

z,t−1(dzt−1)

+ sup

(cid:107)f (cid:107)BL≤1

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

−

≤ ((cid:107)f (cid:107)∞ + αZ )ρBL

(cid:12)
(cid:12)
z,t−1(dzt−1)
(cid:12)
(cid:12)

f (zt)η(dzt|F (zt−1), γ(zt−1))P γ,N
(cid:16)
(cid:17)

z,t−1, P γ,N
P γ

+ αZ EN

z [L(Zt−1)]

z,t−1
(cid:17)

+ αZ EN

z [L(Zt−1)].

≤ (1 + αZ )ρBL

(cid:16)

z,t−1, P γ,N
P γ

z,t−1

Using (29), we can write

(cid:16)

ρBL

z,t, P γ,N
P γ

z,t

(cid:17)

≤ (1 + αZ )ρBL

(cid:16)

z,t−1, P γ,N
P γ

z,t−1

(cid:17)

(cid:18)

+ αZ

Ez[L(Zt−1)] + 3ρBL

(cid:16)

z,t−1, P γ,N
P γ

z,t−1

(cid:17) (cid:19)

= (4αZ + 1)ρBL

(cid:16)

z,t−1, P γ,N
P γ

z,t−1

(cid:17)

(cid:32)

≤ (4αZ + 1)

αZ

t−2
(cid:88)

+ αZ Ez[L(Zt−1)]
(cid:33)

(4αZ + 1)mEz[L(Zt−m−2)]

+ αZ Ez[L(Zt−1)]

m=0

t−1
(cid:88)

= αZ

(4αZ + 1)mE[Lz(Zt−m−1)]

m=0

which completes the proof of (31).

Now, we go back to the main claim and start from (29) to write

EN

z,γ [L(Zt)] ≤ Ez,γ [L(Zt)] + (cid:107)L(cid:107)BLρBL

(cid:16)

z,t, P γ,N
P γ

z,t

(cid:17)

≤ Ez,γ [L(Zt)] + 3ρBL

(cid:16)

z,t, P γ,N
P γ

z,t

(cid:17)

≤ Ez,γ [L(Zt)] + 3αZ

t−1
(cid:88)

(4αZ + 1)mEz,γ[L(Zt−m−1)].

m=0

Remark 21 Note that for the previous proof, (4αZ + 1) can be replaced by 1 + αZ ((cid:107)L(cid:107)∞ + 2)
where

(cid:107)L(cid:107)∞ = sup

π∈P(X)

sup
γ∈Γ

sup
y[0,N ],u[0,N −1]

(cid:16)

(cid:17)
P π(·|y[0,N ], u[0,N −1]), P ˆπ(·|y[0,N ], u[0,N −1])

ρBL

which is upper bounded by 2, however, usually smaller than 2, provided there is a uniform ﬁlter
stability.

Lemma 22 We introduce the following notation

ˆEz[L(Zt)] = sup
u0

(cid:90)

. . . sup
ut−2

(cid:90)

(cid:90)

sup
ut−1

L(zt)η(dzt|zt−1, ut−1)η(dzt−1|zt−2, ut−2) . . . η(dz1|z, u0).

40

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

Under Assumption 3, we have that

Ez,γ[L(Zt)] = ˆEz[L(Zt)].

sup
γ∈Γ

Proof Recall that for a ﬁxed policy γ = {γt}t,

Ez,γ[L(Zt)] =

(cid:90)

It is easy to see that

L(zt)η(dzt|zt−1, γt−1(zt−1))η(dzt−1|zt−1, γt−2(zt−2)) . . . η(dz1|z, γ0(z0))

(cid:90)

≤

L(zt)η(dzt|zt−1, γt−1(zt−1))η(dzt−1|zt−1, γt−2(zt−2)) . . . η(dz1|z, γ0(z0))
(cid:90)

(cid:90)

L(zt)η(dzt|zt−1, ut−1)η(dzt−1|zt−1, γt−2(zt−2)) . . . η(dz1|z, γ0(z0))

sup
ut−1

By repeating this step we can have

(cid:90)

L(zt)η(dzt|zt−1, γt−1(zt−1))η(dzt−1|zt−1, γt−2(zt−2)) . . . η(dz1|z, γ0(z0))

(cid:90)

. . . sup
ut−2

(cid:90)

(cid:90)

sup
ut−1

≤ sup
u0

L(zt)η(dzt|zt−1, ut−1)η(dzt−1|zt−2, ut−2) . . . η(dz1|z, u0)

Hence by taking supremum over all policies we can write

Ez,γ[L(Zt)] ≤ ˆEz[L(Zt)].

sup
γ∈Γ

For the other direction, we ﬁrst focus on the term suput−1
tion 3, one can show that there exists a measurable map γt−1 such that

(cid:82) L(zt)η(dzt|zt−1, ut−1), using Assump-

(cid:90)

sup
ut−1

L(zt)η(dzt|zt−1, ut−1) =

(cid:90)

L(zt)η(dzt|zt−1, γt−1(zt−1)).

Using the same argument, we can see that there exists a sequence of measurable functions
{γ0, γ1, . . . , γt−1} such that

(cid:90)

sup
u0

(cid:90)

=

. . . sup
ut−2

(cid:90)

(cid:90)

sup
ut−1

L(zt)η(dzt|zt−1, ut−1)η(dzt−1|zt−2, ut−2) . . . η(dz1|z, u0)

L(zt)η(dzt|zt−1, γt−1(zt−1))η(dzt−1|zt−1, γt−2(zt−2)) . . . η(dz1|z, γ0(z0)).

Hence we can write that

which proves the main claim.

ˆEz[L(Zt)] ≤ sup
γ∈Γ

Ez,γ[L(Zt)]

41

KARA AND Y ¨UKSEL

Lemma 23 Under Assumption 3

(cid:107)g(cid:107)BL ≤ (αc + βαZ (cid:107)Jβ(cid:107)BL)

(cid:18)

2
1 − β(4αZ + 1)

+

3αZ
1 − βαZ

+

9α2
Z
(1 − β(4αZ + 1))2

(cid:19)

.

Proof Using Lemma 22, we can write that

(cid:32)

g(z) = lim
k→∞

sup
γ

αc

(cid:32)

k−1
(cid:88)

t=0

βt

Ez,γ [L(Zt)] + 3αZ

t−1
(cid:88)

(4αZ + 1)mEz,γ[L(Zt−m−1)]

(cid:33)

(cid:32)

m=0

βt+1(cid:107)vk−t−1(cid:107)BL

Ez,γ [L(Zt)] + 3αZ

t−1
(cid:88)

(4αZ + 1)mEz,γ[L(Zt−m−1)]

(cid:33) (cid:33)

+ αZ

k−1
(cid:88)

t=0
(cid:32)

= lim
k→∞

αc

+ αZ

k−1
(cid:88)

t=0

k−1
(cid:88)

t=0

(cid:32)

βt

ˆEz [L(Zt)] + 3αZ

t−1
(cid:88)

(4αZ + 1)m ˆEz[L(Zt−m−1)]

(cid:33)

m=0

m=0

(cid:32)

βt+1(cid:107)vk−t−1(cid:107)BL

ˆEz [L(Zt)] + 3αZ

t−1
(cid:88)

(4αZ + 1)m ˆEz[L(Zt−m−1)]

(cid:33) (cid:33)
.

m=0

Using the fact that (cid:107)L(cid:107)∞ ≤ 2 we can write

(cid:32)

(cid:107)g(cid:107)∞ ≤ lim
k→∞

αc

(cid:32)

βt

2 + 3αZ

k−1
(cid:88)

t=0

(cid:33)

2(4αZ + 1)m

t−1
(cid:88)

m=0
(cid:32)

+ αZ

k−1
(cid:88)

t=0

βt+1(cid:107)vk−t−1(cid:107)BL

2 + 3αZ

(cid:33) (cid:33)

2(4αZ + 1)m

t−1
(cid:88)

m=0

≤ 2 (cid:0)αc + βαZ (cid:107)J ∗

β(cid:107)BL

(cid:1)

1
1 − β(4αZ + 1)

.

Next, we show that when deﬁned as a function of z, (cid:107) ˆEz[L(Zt)](cid:107)BL ≤ 3 αt+1

Z −1
αZ −1 We follow an

inductive approach. For t = 1:

(cid:12)
(cid:12) ˆEz [L(Z1)] − ˆEˆz [L(Z1)] (cid:12)

(cid:12) ≤ sup
u

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)

L(z1)η(dz1|z, u) −

(cid:90)

L(z1)η(dz1|ˆz, u)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ (cid:107)L(cid:107)BLαZ ρBL(z, ˆz) ≤ 3αZ ρBL(z, ˆz)

where (cid:107)L(cid:107)BL ≤ 3 follows from (30). Hence we have that (cid:107) ˆEz[L(Z1)](cid:107)BL ≤ 3 + 3αZ . Now we
assume the claim holds for t − 1 and focus on the case for t:

(cid:12) ˆEz [L(Zt)] − ˆEˆz [L(Zt)] (cid:12)
(cid:12)
(cid:12)
(cid:90)

(cid:90)

(cid:90)

= sup
u0

. . . sup
ut−2

. . . sup
ut−2

sup
ut−1

(cid:90)

− sup
u0
(cid:18)(cid:90)

≤ sup
u0

sup
ut−1
(cid:90)

L(zt)η(dzt|zt−1, ut−1)η(dzt−1|zt−2, ut−2) . . . η(dz1|z, u0)

(cid:90)

L(zt)η(dzt|zt−1, ut−1)η(dzt−1|zt−2, ut−2) . . . η(dz1|ˆz, u0)

ˆEz1 [L(Zt)] η(dz1|z, u0) −

(cid:90)

ˆEz1 [L(Zt)] η(dz1|ˆz, u0)

(cid:19)

42

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

≤ αZ ρBL(z, ˆz)(cid:107) ˆEz[L(Zt−1)](cid:107)BL ≤ αZ ρBL(z, ˆz)3

αt
Z − 1
αZ − 1

.

We then have that

(cid:107) ˆEz[L(Zt)](cid:107)BL ≤ (cid:107)L(cid:107)∞ + 3αZ

αt
Z − 1
αZ − 1

≤ 3 + 3αZ

αt
Z − 1
αZ − 1

≤ 3

αt+1
Z − 1
αZ − 1

.

Thus, we can write that

(cid:12) ˆEz [L(Zt)] − ˆEˆz [L(Zt)] (cid:12)
(cid:12)

(cid:12) ≤ 3αZ

αt
Z − 1
αZ − 1

ρBL(z, ˆz).

Hence for any z, ˆz

|g(z) − g(ˆz)| ≤ lim
k→∞

(cid:18)

αc

k−1
(cid:88)

t=0

βt(cid:0)(cid:12)

(cid:12) ˆEz [L(Zt)] − ˆEˆz [L(Zt)] (cid:12)
(cid:12)

t−1
(cid:88)

+ 3αZ

(4αZ + 1)m (cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆEz [L(Zt−m−1)] − ˆEˆz [L(Zt−m−1)]
(cid:12)
(cid:12)

(cid:19)

m=0

+ αZ

k−1
(cid:88)

t=0

βt+1(cid:107)vk−t−1(cid:107)BL

(cid:18) (cid:12)
(cid:12)
ˆEz [L(Zt)] − ˆEˆz [L(Zt)]
(cid:12)
(cid:12)
(cid:12)
(cid:12)

t−1
(cid:88)

+ 3αZ

(4αZ + 1)m (cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆEz [L(Zt−m−1)] − ˆEˆz [L(Zt−m−1)]
(cid:12)
(cid:12)

(cid:19)(cid:19)

(cid:32)

≤ lim
k→∞

αc

(cid:32)

k−1
(cid:88)

t=0

βt

3αZ

m=0
αt
Z − 1
αZ − 1
(cid:32)

k−1
(cid:88)

+ αZ

βt+1(cid:107)vk−t−1(cid:107)BL

3αZ

t=0
3αZ
αZ − 1

≤

(αc + βαZ (cid:107)Jβ(cid:107)BL)

(cid:18)

Hence, we have that

ρBL(z, ˆz) + 3αZ

t−1
(cid:88)

(4αZ + 1)m3αZ

m=0

− 1

αt−m−1

Z
αZ − 1

(cid:33)

ρBL(z, ˆz)

αt
Z − 1
αZ − 1

ρBL(z, ˆz) + 3αZ

t−1
(cid:88)

(4αZ + 1)m3αZ

1
1 − βαZ

+

3αZ
(1 − β(4αZ + 1))2

(cid:19)

ρBL(z, ˆz).

m=0

− 1

αt−m−1

Z
αZ − 1

(cid:33) (cid:33)

ρBL(z, ˆz)

(cid:107)g(cid:107)BL ≤ (αc + βαZ (cid:107)Jβ(cid:107)BL)

(cid:18)

2
1 − β(4αZ + 1)

+

3αZ
1 − βαZ

+

9α2
Z
(1 − β(4αZ + 1))2

(cid:19)

.

Lemma 24

i. Under Assumption 3, if Z is metrized with ρBL, we have that

(cid:107)J ∗

β(cid:107)BL ≤

1
1 − βαZ

(cid:18) (cid:107)˜c(cid:107)∞
1 − β

(cid:19)

+ α˜c

.

ii. Without any assumption, if Z is metrized with total variation distance, we have that

(cid:107)J ∗

β(cid:107)BL ≤

2 − β
(1 − β)(1 − βαZ )

(cid:107)c(cid:107)∞

43

KARA AND Y ¨UKSEL

Proof We start with the ﬁrst part, for any x, y ∈ Z

|J ∗

β(x) − J ∗

β(y)| ≤ sup
u

(cid:18)

|˜c(x, u) − ˜c(y, u)| + β|

(cid:90)

J ∗
β(z)η(dz|x, u) −

(cid:90)

J ∗
β(z)η(dz|y, u)|

(cid:19)

≤ α˜cρBL(x, y) + β(cid:107)J ∗

β(cid:107)BLαZ ρBL(x, y).

Thus, we can write that

(cid:107)J ∗

β(cid:107)BL ≤ (cid:107)J ∗

β(cid:107)∞ + α˜c + βαZ (cid:107)J ∗

β(cid:107)BL ≤

(cid:107)˜c(cid:107)∞
1 − β

+ α˜c + βαZ (cid:107)J ∗

β(cid:107)BL.

Hence, rearranging the terms, we can write (provided βαZ < 1)

(cid:107)J ∗

β(cid:107)BL ≤

1
1 − βαZ

(cid:18) (cid:107)˜c(cid:107)∞
1 − β

(cid:19)

+ α˜c

.

For the second part, similar arguments lead to the following bound

(cid:107)J ∗

β(cid:107)BL ≤ (cid:107)J ∗

β(cid:107)∞ + (cid:107)c(cid:107)∞ + βαZ (cid:107)J ∗

β(cid:107)BL,

hence the result follows after rearranging the terms and noting that (cid:107)J ∗

β(cid:107)∞ ≤ (cid:107)c(cid:107)∞
1−β .

References

D.P. Bertsekas. Convergence of discretization procedures in dynamic programming. IEEE Trans.

Autom. Control, 20(3):415–419, Jun. 1975.

P. Billingsley. Convergence of probability measures. New York: Wiley, 2nd edition, 1999.

P. Chigansky, R. Liptser, and R. van Handel.

Intrinsic methods in ﬁlter stability. Handbook of

Nonlinear Filtering, 2009.

C.S. Chow and J. N. Tsitsiklis. An optimal one-way multigrid algorithm for discrete-time stochastic

control. IEEE transactions on automatic control, 36(8):898–914, 1991.

R.L. Dobrushin. Central limit theorem for nonstationary Markov chains. i. Theory of Probability &

Its Applications, 1(1):65–80, 1956.

F. Dufour and T. Prieto-Rumeau. Approximation of Markov decision processes with general state

space. J. Math. Anal. Appl., 388:1254–1267, 2012.

E. A. Feinberg. Controlled Markov processes with arbitrary numerical criteria. Th. Probability and

its Appl., pages 486–503, 1982.

E.A. Feinberg, P.O. Kasyanov, and N.V. Zadioanchuk. Average cost Markov decision processes
with weakly continuous transition probabilities. Math. Oper. Res., 37(4):591–607, Nov. 2012.

E.A. Feinberg, P.O. Kasyanov, and M.Z. Zgurovsky. Partially observable total-cost Markov decision
process with weakly continuous transition probabilities. Mathematics of Operations Research,
41(2):656–681, 2016.

44

NEAR OPTIMALITY OF FINITE MEMORY FEEDBACK POLICIES IN POMPDS

E. A. Hansen. Solving POMDPs by searching in policy space. arXiv preprint arXiv:1301.7380,

2013.

O. Hern´andez-Lerma. Adaptive Markov Control Processes. Springer-Verlag, 1989.

O. Hernandez-Lerma and J. B. Lasserre. Discrete-Time Markov Control Processes: Basic Optimal-

ity Criteria. Springer, 1996.

A. D. Kara and S. Y¨uksel. Convergence of ﬁnite memory q-learning for pomdps and near optimality

of learned policies under ﬁlter stability. arXiv preprint arXiv:2103.12158, 2021.

A. D. Kara, N. Saldi, and S. Y¨uksel. Weak feller property of non-linear ﬁlters. Systems & Control

Letters, 134:104–512, 2019.

V. Krishnamurthy. Partially observed Markov decision processes: from ﬁltering to controlled sens-

ing. Cambridge University Press, 2016.

W.S. Lovejoy. A survey of algorithmic methods for partially observed Markov decision processes.

Annals of Operations Research, 28:47–66, 1991.

W. Mao, K. Zhang, E. Miehling, and T. Bas¸ar. Information state embedding in partially observable

cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2004.01098, 2020.

C. McDonald and S. Y¨uksel. Observability and ﬁlter stability for partially observed markov pro-
cesses. In 2019 IEEE 58th Conference on Decision and Control (CDC), pages 1623–1628. IEEE,
2019.

C. McDonald and S. Y¨uksel. Exponential ﬁlter stability via Dobrushin’s coefﬁcient. Electronic

Communications in Probability, 25, 2020.

C. McDonald and S. Y¨uksel. Robustness to incorrect priors and controlled ﬁlter stability in par-
tially observed stochastic control. SIAM Journal on Control and Optimization, to appear (also
arXiv:2110.03720), 2022.

K.R. Parthasarathy. Probability Measures on Metric Spaces. AMS Bookstore, 1967.

J. Pineau, G. Gordon, and S. Thrun. Anytime point-based approximations for large pomdps. Journal

of Artiﬁcial Intelligence Research, 27:335–380, 2006.

J. M. Porta, N. Vlassis, M. T. J. Spaan, and P. Poupart. Point-based value iteration for continuous

pomdps. Journal of Machine Learning Research, 7(Nov):2329–2367, 2006.

D. Rhenius. Incomplete information in Markovian decision models. Ann. Statist., 2:1327–1334,

1974.

N. Saldi, S. Y¨uksel, and T. Linder. On the asymptotic optimality of ﬁnite approximations to markov
decision processes with borel spaces. Mathematics of Operations Research, 42(4):945–978,
2017.

N. Saldi, T. Linder, and S. Yuksel. Finite Approximations in discrete-time stochastic control.

Birkh¨auser,, 2018.

45

KARA AND Y ¨UKSEL

N. Saldi, S. Y¨uksel, and T. Linder. Finite model approximations for partially observed markov
decision processes with discounted cost. IEEE Transactions on Automatic Control, 65, 2020.

T. Smith and R. Simmons. Point-based pomdp algorithms: Improved analysis and implementation.

arXiv preprint arXiv:1207.1412, 2012.

J. Subramanian and A. Mahajan. Approximate information state for partially observed systems. In

2019 IEEE 58th Conference on Decision and Control (CDC), pages 1629–1636, 2019.

C. Villani. Optimal transport: old and new. Springer, 2008.

N. Vlassis and M. T. J. Spaan. Perseus: Randomized point-based value iteration for pomdps. Journal

of artiﬁcial intelligence research, 24:195–220, 2005.

C.C. White. A survey of solution techniques for the partially observed Markov decision process.

Annals of Operations Research, 32:215–230, 1991.

C. C. White-III and W. T. Scherer. Finite-memory suboptimal design for partially observed markov

decision processes. Operations Research, 42(3):439–455, 1994.

H. Yu and D.P. Bertsekas. On near optimality of the set of ﬁnite-state controllers for average cost

pomdp. Mathematics of Operations Research, 33(1):1–11, 2008.

A.A. Yushkevich. Reduction of a controlled Markov model with incomplete data to a problem
with complete information in the case of Borel state and control spaces. Theory Prob. Appl., 21:
153–158, 1976.

Z. Zhang, D. Hsu, and W. S. Lee. Covering number for efﬁcient heuristic-based pomdp planning.

In International conference on machine learning, pages 28–36. PMLR, 2014.

E. Zhou, M. C. Fu, and S. I. Marcus. A density projection approach to dimension reduction for
continuous-state POMDPs. In Decision and Control, 2008. CDC 2008. 47th IEEE Conference
on, pages 5576–5581, 2008.

E. Zhou, M. C. Fu, and S. I. Marcus. Solving continuous-state POMDPs via density projection.

IEEE Transactions on Automatic Control, 55(5):1101 – 1116, 2010.

R. Zhou and E.A. Hansen. An improved grid-based approximation algorithm for POMDPs. In Int.

J. Conf. Artiﬁcial Intelligence, pages 707–714, Aug. 2001.

46

