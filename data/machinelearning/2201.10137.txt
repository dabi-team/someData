Noname manuscript No.
(will be inserted by the editor)

Leveraging Structural Properties of Source Code Graphs for
Just-In-Time Bug Prediction

Md Nadim · Debajyoti Mondal · Chanchal K. Roy

2
2
0
2

n
a
J

5
2

]
E
S
.
s
c
[

1
v
7
3
1
0
1
.
1
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract The most common use of data visualization is to minimize the complexity for proper
understanding. A graph is one of the most commonly used representations for understanding re-
lational data. It produces a simpliﬁed representation of data that is challenging to comprehend if
kept in a textual format. In this study, we propose a methodology to utilize the relational properties
of source code in the form of a graph to identify Just-in-Time (JIT) bug prediction in software
systems during diﬀerent revisions of software evolution and maintenance. We presented a method
to convert the source codes of commit patches to equivalent graph representations and named it
Source Code Graph (SCG). To understand and compare multiple source code graphs, we extracted
several structural properties of these graphs, such as the density, number of cycles, nodes, edges,
etc. We then utilized the attribute values of those SCGs to visualize and detect buggy software com-
mits. We process more than 246K software commits from 12 subject systems in this investigation.
Our investigation on these 12 open-source software projects written in C++ and Java program-
ming languages shows that if we combine the features from SCG with conventional features used
in similar studies, we will get the increased performance of Machine Learning (ML) based buggy
commit detection models. We also ﬁnd the increase of F1 Scores in predicting buggy and non-buggy
commits statistically signiﬁcant using the Wilcoxon Signed Rank Test. Since SCG-based feature
values represent the style or structural properties of source code updates or changes in the software
system, it suggests the importance of careful maintenance of source code style or structure for
keeping a software system bug-free.

Keywords Source Code Visualization · Graph Representation · Graph Attribute · Machine
Learning Models · Feature Selection · Classiﬁcation

University of Saskatchewan
E-mail: mdn769@usask.ca
E-mail: d.mondal@usask.ca
E-mail: chanchal.roy@usask.ca

 
 
 
 
 
 
2

1 Introduction

Md Nadim et al.

Fixing software bugs or faults is one of the most common activities in the software development and
maintenance process, which is estimated to take about 50-80% of developers’ time [Collofello and
Woodﬁeld (1989)]. Fixing a detected bug may also be a reason for inducing new bugs in software
systems [Wen et al. (2019)]. Delay in catching a bug when it is ﬁrst induced in a software system
may also make the detection and ﬁxing process more complicated. As a result, it can reduce the
reliability and acceptance of the software system. On the other hand, if we can detect an induced
bug early, it could signiﬁcantly reduce the impacts because of the availability of resources such as
developers (who change over time).

Existing studies to detect and investigate the presence of a bug in software commits either use
statistical metadata [Borg et al. (2019); Rosen et al. (2015a)] of those commits extracted from its
subversion (SVN) maintenance system or repositories such as GitHub. Some studies also combine
those statistical data and natural language-based textual processing of source code [Shivaji et al.
(2013)] based features. However, to the best of our knowledge, the eﬀect of coding structure or
patterns on the presence of a bug in software systems is not adequately investigated in existing
studies. The M.Sc. thesis [Nadim (2020)] done by the ﬁrst author of this manuscript reported an
investigation of the patterns of source code in commit patch, which show improvement in detecting
bug inducing commits from six open-source software projects. It performed an inquiry into 642
manually labelled datasets. Although the ﬁndings of that study showed that the use of code pattern-
based feature values in ML-based BIC detection increases the bug detection performance measures,
the datasets are from six manually labelled software systems, which are not much larger considering
the count of data instances in a real word software environment. In this study, we performed an
investigation on the data instances of 12 open-source C++ and Java-based software projects, and
our total data instances are 246,279. We modelled patterns in the source code as graphs, referred
to as the Source Code Graphs (SCG). We investigated the possibility of using the attributed values
of SCG as feature values to visualize and detect buggy commits in software systems. We show the
summary of our datasets in Table 1 from the 12 subject systems. As the number of subject systems
in this investigation is quite extensive, the results of this study will provide robust evidence about
the generalizability of the ﬁndings.

The motivation for this investigation is to ﬁnd practical evidence about diﬀerent coding styles
that may considerably impact the overall quality of software systems. If we can ﬁnd speciﬁc styles
or patterns, we can produce a programming style guidance for a developer to make the software
development, evolution & maintenance activities more eﬀective. In this study, we want to investigate
the possibility of identifying bug presence in software commits using the feature values extracted
from the visual representation of source code. We collected two types of data labels and initial
datasets with 15 feature values of 12 open-source software projects from the implementation of
Commit Guru [Rosen et al. (2015a)], which can mine risky commits from any GIT SCM1 source code
manager repository. They initially clone project data from its GitHub repository and then process
the commit logs for extracting 13 change-level metric values. They also describe their approach
in another study [Kamei et al. (2013)]. They also utilize SCM blame/annotate2 functionality for
processing a repository and produce a dataset containing diﬀerent commit categories and bug
presence information. Several studies [Islam and Zibran (2021); Mockus and Votta (2000); Ray

1 https://git-scm.com/
2 https://docs.github.com/en/repositories/working-with-ﬁles/using-ﬁles/tracking-changes-in-a-ﬁle

Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction

3

et al. (2016)] also applied a similar approach for detecting a commit which is ﬁxing a bug and then
tracking the commit, which introduced the ﬁxed bug in the software project. The bug introducing
commits are generally considered as risky or buggy commits. Ray et al. (2016) also reported that the
technique used in these studies to identify bug-ﬁxing commits is 96% accurate. We are introducing
two new categories of Source Code Graphs (SCG)-based feature values in this study. As we wanted
to compare the performance of SCG-based features and the features commonly used in similar
studies in detecting buggy and non-buggy commits, we ﬁnd the initial dataset prepared by Commit
Guru appropriate to take as a starter. We can also argue that these starter datasets we use are
accurate and reasonably extensive enough for our investigation, considering the popularity of the
applied approach in this research area.

We added 24 new feature values we extracted from these 12 software projects to perform this
investigation. Our feature values represent the coding style or structure of the software developer,
and we named them Source Code Graph (SCG) based features. The 15 feature values used in the
study [Rosen et al. (2015a)] are most common in similar machine learning-based software quality
detection models. We can extract most of these feature values, such as the number of lines added/
deleted/ modiﬁed in a commit, density of change, developer age/ time from the GitHub repository of
any software project. In this study, we named these GitHub statistics-based Commit Guru features
and used the term ‘C’ features to refer to them in this manuscript. CommitGuru also provided two
types of data labelling, one as the presence of a bug that is either True or False. The other is the
classiﬁcation of commits, representing diﬀerent categories (e.g., merge, perfective, preventive, etc.).

The primary motivation behind this study is to see whether the network complexity of source
code graphs (SCGs) can detect the buggy commits and whether this technique outperforms the
usual feature-based ML approaches. We extracted 12 Source Code Graph (SCG) based features
from both the added (represented as A) and deleted (represented as D) code fragments from the
commits of all the 12 software projects. Therefore, we have 12 features from added commit lines and
12 from deleted commit lines, making 12 × 2 = 24 features representing the Source Code Graph
(SCG) of commit patches. We listed the names of SCG based features in Table 2. We selected
features inspired by the ‘node embedding’ in graph neural networks [Hamilton et al. (2017)] and
metrics for graph comparison [Gove (2019)]. We wanted to detect buggy software commits based
on the source code structures of added and deleted lines in software commits. So, we investigated
the graph-based features as they have been successfully used to represent and compare graphs
structure in graph neural networks.

We also show the mean and standard deviation of SCG-based feature values in buggy and non-
buggy commits in Table 2. This table calculates the mean and standard deviation from the feature
values of all the 12 subject systems combined. We can observe a considerable diﬀerence in values
between buggy and non-buggy commits for both mean and standard deviation in the table. We also
performed a t-Test [Student (1908)] on the data samples in each feature value individually from
Commit Guru and Source Code Graphs to see whether the diﬀerence in mean values of various
features in buggy commits and non-buggy commits are statistically signiﬁcant or not. We found the
diﬀerence of mean values in most of the buggy and non-buggy commits’ features in both Commit
Guru (our baseline ’C’) and our proposed SCG (A and D) are statistically signiﬁcant. However, the
means from a few features are not statistically signiﬁcant in some of the 12 subject systems; among
them, the majority have come from the Commit Guru (C) features. These scenarios motivate us
to investigate buggy and non-buggy commit detection performance combined with Commit Guru
and SCG-based features using machine learning detection models.

4

Md Nadim et al.

Table 1 The 12 subject systems Used for the Study

SL. No
1
2
3
4
5
6
7
8
9
10
11
12
Total Number of Commit Instances:

Subject Systems Prog. Lang. Commit Instances
Accumulo
Ambari
Bitcoin
Camel
Jackrabbit
Jenkins
Litecoin
Lucene
Mongo
Oozie
Tomcat
Zxing

11,045
24,588
16,759
26,911
8,909
28,923
17,836
31,957
50,303
2,280
23,153
3,615
246,279

Java
Java
CPP
Java
Java
Java
CPP
Java
CPP
Java
Java
Java

Table 2 Source Code Graph (SCG) based Feature Names & their Comparison of Mean and Standard
Deviation in Buggy and Non-buggy Commits. We combine Add (A) and Delete (D) types of SCG based
features while calculating the Mean and STDEV.

SL. No.

1
2
3
4
5
6
7
8
9
10
11
12

SCG-based
Feature Names

Number of Cycles
Graph Density
Number of Nodes
Number of Edges
Average In Degree
Average Out Degree
Maximum Degree
Minimum Degree
Sum of Degree
Average Degree
Median Degree
Number of Self Loops

Mean
(Buggy)
56.05
0.04
23.05
43.92
1.62
1.62
12.3
0.95
87.84
3.15
2.47
0.99

Mean
(Non-Buggy)
13.81
0.05
14.14
23.76
1.24
1.24
8.22
0.82
47.52
2.46
1.96
0.77

STDEV
(Buggy)
33.9
0.04
5.51
13.08
0.33
0.33
3.59
0.17
26.17
0.85
0.53
0.19

STDEV
(Non-Buggy)
9.76
0.05
5.97
11.23
0.48
0.48
3.9
0.28
22.47
1.16
0.81
0.31

Table 3 Number of Data Instances and Its Assigned Class Labels (0–6) in different commit categories

Subject
System
Accumulo
Ambari
Bitcoin
Camel
Jackrabbit
Jenkins
Litecoin
Lucene
Mongo
Oozie
Tomcat
Zxing

None
(0)
2,840
13,602
3,371
10,315
4,269
11,704
4,307
11,328
23,577
1,192
8,302
1,785

Merge
(1)
3,409
495
4,598
196
3
4,372
6,346
614
2,279
1
40
75

Corrective
(2)
2,192
5,184
3,127
8,492
1,611
6,874
2,843
8,474
9,034
482
8,897
813

Preventive
(3)
354
483
1,798
1,394
571
859
658
2,329
4,285
124
660
125

Feature
Addition (4)
1,534
4,015
3,197
4,645
1,797
3,756
3,083
6,292
8,238
350
3,537
653

Non
Functional (5)
450
209
348
964
446
873
330
2,024
842
95
574
37

Perfective
(6)
266
600
320
905
212
485
269
896
2,048
36
1,143
127

Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction

5

Table 4 Number of data instances and its assigned class labels (0–1) based on bug presence

Subject Systems
Accumulo
Ambari
Bitcoin
Camel
Jackrabbit
Jenkins
Litecoin
Lucene
Mongo
Oozie
Tomcat
Zxing

Total:

Buggy Commits (L = 1) Non Buggy Commits (L = 0)

1,763
5,502
3,318
9,834
1,771
4,640
2,867
8,011
12,588
453
5,677
847
57,271

9,282
19,086
13,441
17,077
7,138
24,283
14,969
23,946
37,715
1,827
17,476
2,768
189,008

We have computed some standardized [Jajuga and Walesiak (2000)] box plots of all Commmit
Guru and SCG features in Figures 1, 2, and 3 to see the distribution of the feature values in buggy
and non-buggy commits and compare them with each other. The standardization process made
all the feature values easier to plot on the same scale. To standardize, we ﬁrst subtract the mean
value from each feature value and then divide it by standard deviation. After the standardization,
we found all the SCG feature values on the scale of −2.50 to +2.50. After that, we draw the
box plots for all the subject systems. In the distribution of Commit Guru (C) in Figure 1, we
can see that a higher portion of the feature values from buggy and non-buggy commits overlap
each other. On the other hand, the distribution of SCG based Add (A) and Delete (D) feature
values in Figures 2 & 3, we see most of the non-overlapping distributions of feature values from the
commits when partitioned into the buggy and the non-buggy categories. For example, in all of these
distributions of SCG feature values, we found that graph density in Buggy (True) is lower than
the Buggy (False) commits. We also observed, most of the SCG feature values, such as number of
nodes, number of edges, incoming degree, etc., are higher in buggy commits than the non-buggy
commits. We can see an exception in the distribution of SCG feature values in the subject system
‘Ambari’. In this subject system, feature values in both the SCG A and D commit categories are
distributed too far from each other in non-buggy commits than buggy commits. Although this is an
exception in one subject system out of 12 in this study, it also provides evidence that SCG feature
values’ distribution diﬀers in buggy and non-buggy commits. These scenarios also motivate us to
investigate the capability of detecting bugs by utilizing the visual representation of source code.
Findings from this study could lead us to develop autonomous bug detection systems based on the
structural attributes of source code.

We organized this study in the following sections. Section 2 discuss some background terminol-
ogy used in this study, research hypothesis and research questions are discussed in section 3, overall
research methodology is in section 4, we discuss experimental results in section 5, some threats to
validity is discussed in section 6, related works are in section 7, and ﬁnally we concluded this paper
in section 8.

6

Md Nadim et al.

Fig. 1 Commit Guru (C) feature distribution in all the Subject Systems. We can see an almost overlapping distri-
bution of feature values from the buggy and non-buggy commit labels in all the 12 subject systems.

2 Background Terminologies

This section will discuss a few relevant terminologies that we have used in this paper.

Software Evolution: Software evolution is a continuous process to keep a software system
consistent within its life cycle. The changing requirements, technologies, and stakeholder knowledge
[Rajlich (2014)] are the main factors for software evolution to keep a software system up to date.

Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction

7

Fig. 2 SCG Add (A) feature distribution in all the Subject Systems. We can see a large portion of the distribution
of feature values from the buggy and non-buggy commit labels are not overlapping in all the 12 subject systems.

Commit Operation: The contribution of diﬀerent software developers in software repositories
is known as commit. Any of these software commit operations may happen for diﬀerent reasons,
such as ﬁxing a bug, adding new software features, or cleaning the code [Hindle et al. (2008); Mockus
and Votta (2000); Purushothaman and Perry (2005)]. Table 3 identiﬁes seven diﬀerent categories
of commits in our subject systems. A commit operation might be a reason for introducing a new
bug in the software systems [Gu et al. (2010); Kamei et al. (2013); Kim et al. (2008); ´Sliwerski

8

Md Nadim et al.

Fig. 3 SCG Delete (D) feature distribution in all the Subject Systems. Similar to the distribution of SCG Add (A),
we can see that a large portion of the distribution of feature values from the buggy and non-buggy commit labels
does not overlap in all the 12 subject systems.

et al. (2005b); Yin et al. (2011)], these commits are known as Bug Inducing Commit (BIC) [Nadim
(2020)].

Commit Attributes: These are the attributes, which could be extracted by analyzing the
commits from a software repository. These attributes explain the nature of a commit operation,
such as the number of changed lines (Added or Deleted), the number of changed ﬁles, changed line
density (ratio of changed lines vs. the total number of lines), etc.

Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction

9

t-SNE: t-Distributed Stochastic Neighbor Embedding (t-SNE) is a technique that visualizes
high dimensional datasets by providing each data point location in lower dimension (i.e. two or
three) [Kruiger et al. (2017); van der Maaten and Hinton (2008)]. This technique is a state-of-
the-art technique for dimensionality reduction and is well suited for data visualization. It is a
variation of Stochastic Neighbor Embedding [Hinton and Roweis (2003)] that provides a much
easier optimization during the reduction of dimensionality in datasets.

The t-SNE is obtained by deﬁning a probability distribution over pairs of high-dimensional
data points such that similar data points obtain higher probabilities and dissimilar data points
obtain lower probabilities. A similar probability distribution is also deﬁned in the low-dimensional
space. Then by minimizing the Kullback-Leibler divergence between these two distributions, one
can obtain a low dimensional plot (t-SNE plot) where the distances among data points in the plot
attain the relative distances in the high-dimensions.

Formally, t-SNE ﬁrst calculates the probabilities pi|j that is proportional to the similarity of

objects xi and xj, as follows.

pi|j =

exp(−||xi − xj||2/2σ2
i )
k(cid:54)=1 exp(−||xi − xk||2/2σ2
i )

(cid:80)

Where σi denotes the bandwidth of the Gaussian kernels, which is set based on data density. Let
yi and yj the low dimensional representations to be determined for xi and xj. Then the similarities
qi|j between yi and yj is deﬁned as follows.

qi|j =

(1 + ||yi − yj||2)−1
(cid:80)

l(cid:54)=k(1 + ||yk − yl||2)−1

(cid:80)
k

.

Finally, the low dimensional representations yi are set by minimizing the Kullback-Leibler di-

vergence between these two probability distributions.

The plots generated by t-SNE often reveal clusters. We have used t-SNE to visualize the diﬀerent
categories of commits in Figure 11 and the bug presence (buggy and non-buggy labels) in commit
in Figure 7.

3 Research Hypothesis/ Questions

In this study, our investigation evaluates the validity of the following hypothesis (h1).

– h1: The complexity of coding structures is positively related to the presence of bugs/defects in

software systems.

The rationale behind this hypothesis is that the source code having a complex coding structure
is likely to contain more bugs as it demands higher eﬀorts from the developer during the commit
operation. As we are encoding the source code of a commit-patch in a graph, its complexity is
captured by diﬀerent graph attributes shown in Table 2. The validation of the hypothesis is done by
analyzing the graph properties of the buggy and non-buggy commits leveraging feature visualization
and statistics. We ﬁrst converted the changed source code of each of the commits from all the
subject systems in a graph representation, where the code statements in the modiﬁed code are
converted to a network connecting nodes and edges. The nodes are formed using the tokens of
code statements, and how the control of diﬀerent statements is ﬂowing is represented by the edges.

10

Md Nadim et al.

Figure 6 demonstrates a few such code patterns as the sequence of tokens, which have been reported
in the M.Sc. thesis done by [Nadim (2020)]. The Master’s thesis also noted that the use of those
Token Patterns (TPs) as feature values could enhance the performance of Bug Inducing Commit
(BIC) detection using Machine Learning (ML) based detection models. Therefore, the validation
of h1 could answer the following two research questions:

– RQ1: Can we detect bugs/defects in software systems by utilizing the complexity of coding

structures?

– RQ2: Can we meaningfully visualize the presence of a bug in software commits using the visual

(graph) attribute values of the coding structure?

The positive answer to both the research question will validate our research hypothesis.

4 Methodology

The overall methodology of this study is divided into the following steps.

4.1 Dataset Collection

To verify the hypothesis (h1) for evaluating whether the coding style or structure co-relates with
the presence of bugs, we need some labelled datasets from software projects. The labels should
indicate which commits contain bugs and which are not. Our selection of software projects for this
study is based on the use of software projects in recent similar studies [Borg et al. (2019); Tabassum
et al. (2020); Vieira et al. (2019); Wen et al. (2019)], and the availability of a considerable amount
of labelled data from each subject system. We selected 12 open-source software systems written
in C++ and Java programming languages. We collected the data labels of buggy and non-buggy
commits of these 12 software projects using the web-based interface of Commit Guru3. Therefore,
the availability of candidate software projects on the commit guru server was also an important
criterion for selecting them for our investigation. Rosen et al. (2015b) published a study to predict
the risk of software commits based on the dataset they extracted from the GitHub repository of the
software systems. Their implemented tool is publicly available to mine any GIT SCM (Source Code
Manager) repository. We have utilized the Commit Guru tool to initially prepare the dataset and
then added 24 other Source Code Graph (SCG) based feature values to each commit ID from our
graph implementation of source codes to implement this study. The summary of prepared datasets
is given in Tables 1, 3, and 4. Table 1 lists 12 open-source software systems written in the C++
and Java programming languages of various sizes and application domains. In Table 3, the number
of commits in each of the ﬁve commit classiﬁcations is given, and the number of the buggy and
non-buggy commits in those software systems are given in Table 4.

Commit Guru provided us with the labelling of the target classes, and it also provided 15
feature values extracted from the GitHub statistics of those software systems. Table 5 shows a
demonstration of the dataset we obtained from Commit Guru and how we combine our SCG-based
features in 12 subject systems. Data value from each commit is identiﬁed by an ID (e.g., commit
hash). Each commit is represented using three diﬀerent types of feature values and two diﬀerent

3 http://commit.guru/

Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction

11

Table 5 Sample Dataset

Features from
Commit Guru (C)
C1 C2
4
3
4
1
2
2
1
1
1
1

C15
1.68
1.95
1
0
0

ID

1
2
3
4
5

Source Code
Added (A)

Source Code
Deleted (D)

A1 A2 A12 D1 D2 D12

Buggy = 1
Others = 0

4
4
7
2
4

0.12
0.17
0.1
0.08
0.13

13
10
18
18
13

4
4
7
2
4

0.13
0.17
0.1
0.08
0.13

13
10
18
18
13

1
0
0
1
0

types of feature labelling. The ﬁrst three columns (C1, C2, and C15) next to ID (e.g., commit
id) show a sample of three feature values (out of 15), which we obtained from the website of
Commit Guru. The other six columns show the sample of feature values, which we extracted from
the graph attributes. To extract the feature values from graph attributes, we ﬁrst converted the
source code lines, which are added in a commit and which are deleted in a commit, into separate
graphs and then extracted attribute values to be used as feature values to represent the commit.
Actual names of graph-based feature values are given in Table 2. We have two diﬀerent labels from
each of the commits in our datasets, i) Bug Presence: L=0 represents commits with no bug, and
ii) L=1 represents the buggy commits. We show the number of buggy and non-buggy commits
from each subject system in Table 4. The commits from various software projects are also divided
into seven diﬀerent commit categories, which are given a numeric value, such as 0-None, 1-Merge,
2-Corrective, etc. We present these commit categories and their number of occurrences in Table 3.

4.2 Converting Source Code Patterns to Graph Representation

Diﬀerent software developers perform their programming activities uniquely when using diﬀerent
functionalities of a particular programming language. For example, let us assume a Java program-
mer who wants to test a variable v1 is less than or equal to another variable v2 or not? To do so,
s/he will write a code such as if (v1 <= v2) or if (v1 < (v2 + 1)). The code is doing the same com-
putation in both cases, but they are written in two diﬀerent patterns or structures. The diﬀerence
in coding patterns done by diﬀerent software developers might make it diﬀerent for the software
evolution & maintenance activities. We investigate these diﬀerences in source code patterns in the
form of Source Code Graph (SCG) to identify buggy commits from software revisions.

We ﬁrst extracted all the source code lines (added and deleted lines) from the commit patches
to identify those patterns from the source code and saved them in separate ﬁles. We then converted
those added and deleted lines into two separate graph representations. A programmer performs
addition or deletion in source code lines during the commit operation of software evolution. Newly
added lines are indicated by a plus sign (+), and the deleted lines are indicated by a minus sign
(-) in the commit patch. We utilized these signs to identify which lines are added and which lines
are deleted while processing the commit patches of our subject systems. Figure 4 shows a sample
of a typical commit patch. Besides +/- signs, there are some lines without any sign. Those lines
are kept to maintain the context of the commit patch. We have converted such commit patches
in two diﬀerent graph representations, i) taking all the added and context lines, ii) taking all the
deleted and context lines. In this example, lines 5-7 are replaced by line 8, which changes the
source code structure. Previously, the code implemented the throw command exception under
the else-statement of the if-statement, but after applying the commit patch, the programmer

12

Md Nadim et al.

removed the else-statement and implemented the throw command exception independent of
the if-statement.

Fig. 4 Sample Added (+) and Deleted (-) Source Code Lines in a Commit Patch.

Figure 5 shows a few more steps in generating the Source Code Graph from the commit patch
shown in Figure 4. A segment from the XML representation of source code lines shows the hierarchy
of tokens converted into a graph. Two graphs in this ﬁgure show the change in code structure in that
commit operation. As a developer may perform multiple changes (addition/ deletion) in a single
commit, it could generate multiple graph representations in such commits. We took the union of
graphs to form a single graph of a single commit for extracting graph attribute-based feature values.
We use those feature values as additional features to represent commits in bug visualization and
its ML-based classiﬁcation to verify our research questions and hypothesis.

We then calculated its attribute values to be used as additional feature values to represent those

commits. The key steps in this section are as follows.

i. Extract the source code of commit patches for each of the commits. The added and deleted

lines are kept separate and processed separately.

ii. Apply the SrcML4 tool on the source code to generate an XML representation of the coding

pattern.

iii. Parse the XML ﬁles to extract a token structure for visualization. The hierarchy of these tokens
represents the hierarchy of their occurrences in the source code (demonstrated in Figure 6).
iv. Generate the graphs considering the tokens as nodes, and the sequence of tokens are connected

by edges.

v. Calculate diﬀerent attribute values from the generated graphs. We extracted 12 diﬀerent at-
tribute values (Table 2) calculated from both of the graphs representing source code added (A)
and source code deleted (D).

vi. The attribute values are added to the dataset to be used to represent the commits.

4 https://www.srcml.org/

Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction

13

Fig. 5 Demonstration of source code graph (SCG) Generation

4.3 Diﬀerent Combinations of the Feature Values (C, A, D)

Extracting graph representations from the source code of commit patches provided us with two
diﬀerent datasets for each subject system. The feature values we used to represent commits in those
datasets are listed in Table 2. We represented commits with those 12 graph attribute values from
both the commit lines, which are added and deleted in the commit patch. Therefore, we deﬁned each
of the commits using 12 graph attribute values by added lines and another similar representation
with the deleted lines. For simplicity in presentation, we will use the letter A and D to represent
results using graph-based features of added lines and deleted lines in the source code, respectively.
The results using the features encoded by Commit Guru are presented by the letter C. We present
our results using these representations of features and diﬀerent combinations of them. Our results in
Figures 8, 9, and 10 show the improvements of using SCG-based features (A, D) in the combination
of GithHub Statistics based features (C) from Commit Guru implementation. For example, feature
CA represents the result when we combine the Commit Guru and Graph (Added) datasets, and
CAD corresponds to the combination of all three types of datasets. We have three (C, A, D)
diﬀerent kinds of features. Therefore, we can obtain seven diﬀerent combinations of features (C,
A, D, CA, CD, AD, CAD). We apply each of these seven feature combinations to detect buggy
commits for all the 12 software projects. Then we ﬁnd which feature combination is performing
well in detecting buggy commits. We evaluated and compared those results of diﬀerent feature
combinations to ﬁnd those which improve the buggy commit detection F1 scores. We reported and
presented these results to verify our hypothesis of this study.

14

Md Nadim et al.

Fig. 6 This diagram demonstrates a few examples of extracted coding patterns, where some patterns are simple
(id 1, id 2, etc.) and some have nested structures (i.e. id 12 and id 29, etc.). The pattern denoted by id 1 represents
an expression statement in the source code that ends with a single operation with an operator such as +/-. Similarly,
id 29 represents a nested structure that begins with an expression statement, then a function call, which has some
arguments, and any of those arguments contains a nested expression. All these coding styles/patterns are extracted
from the XML representation of the source ﬁles.

4.4 Visualizing the Datasets Utilizing t-SNE

We ﬁrst evaluated the hypothesis h0: Diﬀerent commit labels (e.g., buggy and non-buggy) have
diﬀerent coding structures; by producing a visualization in Figure 7. We utilized the labelling
of the commits done by Commit Guru. Six visualizations from three subject systems (selected
alphabetically) are shown in this ﬁgure. It demonstrates the eﬀectiveness of adding source code
graph (SCG) based features in visualizing buggy and non-buggy commits in the source code of
software projects. Three of these visualizations (in the left) are computed using only the Commit
Guru or C based feature values, and the other three (in the right) have utilized the combination of
C and SCG-based feature values (CAD). These visualizations are done using the t-SNE [Kruiger
et al. (2017); van der Maaten and Hinton (2008)] graph visualization technique, which implements
dimensionality reduction technique to plot higher dimensional data into lower dimension (2D). The
red and green dots in the ﬁgure represent buggy and non-buggy commits, respectively. As we have
three separate feature sets (C, A, D), we can obtain seven diﬀerent feature combinations (C, A,
D, CA, CD, AD, CAD) of them. We visualized all the datasets from our 12 subject systems using
those seven feature combinations. Therefore, to visualize the buggy and non-buggy commits, we
implemented a total of 12×7=84 visualizations. Among them, six are shown in Figure 7. In this

Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction

15

ﬁgure, C represents the use of feature values obtained from the Commit Guru web page, and CAD
represents we combined Graph (Added and Deleted) features values with the Commit Guru feature
values. We can see some diﬀerences in the regions enclosed by red lines in these visualizations. For
example, the visualizations on the right-hand side of the ﬁgure can separately identify some regions
where red or green pixels are denser/ clustered than the other regions. We also observed a similar
visualization trend in our other visualizations. All of them are publicly available on our GitHub
repository5. The inclusion of SCG-based feature values also improves the clustering of the diﬀerent
commits categories shown in Table 3; however, more study is required to enhance these clusterings.

4.5 Applying the ML-based Classiﬁcation Models

We applied three ML-based classiﬁcation models to see whether the combinations of Graph-based
features and the default Commit Guru based feature can improve the detection performance of
buggy and non-buggy commits. We apply machine learning models using time-sensitive detection
techniques, where training data must be from past timestamps than the testing data. Tan et al.
(2015) show that cross-validation can provide false higher precision in predicting a data instance
sensitive to timestamp. A situation may happen that train the ML model with the data from the
future and test it using the data from the past. We took 70% of the data for training the models
and 30% for testing.

We use the conﬁgurations of Machine Learning (ML) classiﬁers listed in Table 6 to apply
the buggy, and non-buggy commit detection approaches. All the other parameters which are not
mentioned in the table are set to their default values. We use the same parameter values of each ML
classiﬁer to detect buggy and non-buggy commits from all the 12 subject systems. As our goal is
to compare the performance of diﬀerent feature combinations (C, A, and D), any other parameter
values of ML models might have provided a similar comparison scenario.

Table 6 Modified Parameters in Different Machine Learning Classifiers. We use the default values
for all the other parameters not listed in this table.

Random Forest Classiﬁer (RF)
max depth = 100
n estimators = 100

Logistic Regression Classiﬁer (LR)
solver = ‘liblinear’
max iter = 200

K-Neighbors Classiﬁer (KNN)

n neighbors = 5

4.6 Evaluation Metric

We calculate some evaluation metrics to compare the performance of buggy and non-buggy commit
detection using diﬀerent feature combinations. The evaluation metrics depend on the number of the
detected buggy and non-buggy commit instances using ML models. The formulas for calculating
the evaluation metrics are as follows.

To calculate the evaluation metrics, we need to calculate True positives (TP), True negatives
(TN), False positives (FP), and False negatives. TP and TN are the numbers of commit instances
correctly detected as buggy and non-buggy, respectively. On the other hand, FP and FN are the

5 https://github.com/mnadims/pubDemo/tree/main/BugInCommit

16

Md Nadim et al.

Fig. 7 Visualizing Commits Based on the Bug Presence using the t-SNE method. Here, red points represent the
buggy commits, and green points represent non-buggy commits. We can visualize improved clusters (enclosed by red
lines) of red or green points in the right where we use a combination of C, A, and D features.

numbers of commit instances incorrectly detected as buggy and non-buggy, respectively. Therefore,
a higher precision value indicates the lower FP value, which also shows higher reliability of the
detected results. Similarly, a higher recall value means the lower value of FN, which also indicates
the higher accuracy of the detected result. Trying to improve precision typically reduces recall
and vice versa [GoogleDevelopers (2020 (accessed August 26, 2021)]. Thus, a detection model with
higher precision with lower recall is not much accurate; on the other hand, a detection model with
lower precision and higher recall is unreliable. Therefore, we calculate the F1 score, which is the

Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction

17

Fig. 8 We are comparing the improvements of F1 Scores using diﬀerent Feature Combinations in the Logistic Re-
gression Classiﬁer. This diagram indicates the improvement of F1 scores if we use SCG-based features in combination
with GitHub statistics-based features. Here, we compare the improvements for the SCG-based feature combinations,
CA, CD, and CAD, to the results we obtain using only the C features. This diagram shows the improvements in
F1 scores up to 140% (e.g., Jackrabbit). We can also see a decrease in F1 Scores in Mongo and Oozie, but all the
other ten subject systems increase F1 Score if we use SCG-based features combined with the C feature.

harmonic mean of both the precision and recall, to compare the overall performance of our diﬀerent
feature combinations.

P recision =

T P
T P + F P

Recall =

T P
T P + F N

F 1 Score =

2 × P recision × Recall
P recision + Recall

5 Experimental Results

We generated visualization of commits from each of the 12 subject systems based on the identiﬁed
labels of bug presence in those commits. The visualization of buggy and non-buggy commits is
shown in Figure 7, where green dots represent the commits that do not have a bug and red dots

18

Md Nadim et al.

Fig. 9 We are comparing the improvements of F1 Scores using diﬀerent Feature Combinations in the Random
Forest Classiﬁer. Here, 1.0 represents 100%.

represent the buggy commits. We have 12 subject systems and seven diﬀerent feature combina-
tions, so we generated 12×7=84 diﬀerent visualizations. Among them, six are shown in this ﬁgure.
Investigating all the 84 visualizations (similar to the visualizations shown in this ﬁgure 7), we found
the following ﬁndings. First, the addition of SCG-based features (A, D) along with GitHub based
features obtained from Commit Guru implementation (C) provide better visualization compared to
the only SCG-based features or only C-based features. Second, the addition of SCG-based features
makes a better distinct representation of buggy commits. Finally, this visualization supports our
h1, as adding our graph-based features improves the separation of the commits in the buggy and
non-buggy categories.

We also investigated the results of detecting buggy and non-buggy commits using diﬀerent
feature combinations. The ﬁndings of detecting buggy commits using SCG-based features (A or D)
and Github based Commit guru feature (C) are presented in Figures 8, 9, and 10. These ﬁgures show
the improvements in buggy commit detection F1 scores when combining A or D features with C
features using diﬀerent machine learning classiﬁcation models such as Logistic Regression, Random
Forest, and K-Nearest Neighbours. The improvement of F1 Scores of a buggy and non-buggy
commit detection are represented using a horizontal bar chart from each of the three ML models.
Figures 8, 9, and 10 present the improvements in F1 scores using Logistic Regression, Random
Forest, and K-Nearest Neighbours classiﬁers, respectively. We investigated 12 subject systems and
seven diﬀerent feature combinations using three diﬀerent machine learning classiﬁers. Therefore,

Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction

19

Fig. 10 We are comparing the improvements of F1 Scores using diﬀerent Feature Combinations in the K-Nearest
Neighbours Classiﬁer. Here, 1.0 represents 100%.

we have 3×12×7=252 executions of ML models. Analyzing the results, we found CA, CD, and
CAD feature combinations provide improved results in detecting buggy commits in almost all the
subject systems using all the three ML models. The bar lengths in these three ﬁgures represent the
per cent of improvement in F1 scores of each of the CA, CD, and CAD features compared to the
C feature.

In Figure 8, we can see an improvement in F1 scores of more than 140% using CAD feature
combination in the Jackrabbit subject system. Improvements in other subject systems also remain
between 40% to 80% in most of the subject systems. We can also see exceptions in two subject
systems where the F1 scores declined when we added CA, CD, or CAD feature values, but in all the
other subject systems, the SCG-based feature combination with C features improved the F1 Scores.
The results using the other two machine learning classiﬁers in Figures 9 & 10 also provide similar
results with a couple of exceptions when using the Random Forest classiﬁer and the K-Nearest
Neighbour classiﬁer. In general, we can conclude, the inclusion of SCG based features with the
C feature improves buggy and non-buggy commit detection performance in almost all of the 12
subject systems used in this study.

SCG or Source Code Graph represents the changed code structure in a software commit patch.
Diﬀerent structures of a source code fragment could present diﬀerent levels of complexity to the
developer while changing the source code during software revision. This diﬀerence could also be
a reason for a software commit to being buggy or non-buggy. Therefore, using machine learning

20

Md Nadim et al.

models, the source code structure might contain strong feature values to identify buggy and non-
buggy commits. The results of this study also support this scenario. The inclusion of feature
values extracted from SCG helps the machine learning models to distinguish buggy and non-buggy
commits more eﬃciently, which increases the Precision, Recall, and F1 Score in these classiﬁers
results. However, we were also experiencing some test scenarios in Figures 8, 9, and 10, where the
F1 Score decreases using a combination of SCG-based features with Commit Guru features. We
can see such declining cases in two subject systems (e.g., Mongo and Oozie) in Figure 8 using the
Logistic Regression classiﬁer. Using the Random Forest classiﬁer in Figure 9, we can see such a
scenario resulting from Bitcoin and Oozie subject systems. Similarly, in Figure 10, we can notice a
similar decrease in F1 Score in the Accumulo and Oozie subject systems.

We can also notice that the Oozie subject system contains the smallest number of data instances
than all the other subject systems. It is also the common subject system in the results of all
the machine learning classiﬁers that show a decrease of F1 Scores using C, A, and D feature
combinations. Thus, we can say that there are fewer data instances in Oozie, which can not perform
enough training of the machine learning models to distinguish between buggy and non-buggy
commits. Although using the subject system Mongo, the most extensive subject system in this
investigation, also shows a decrease in F1 Scores in Figure 8 when using the Logistic Regression
classiﬁer. Still, in Figures 9 and 10, the other two machine learning classiﬁers (RF and KNN) show
increases in F1 Scores in the results of Mongo using SCG-based features. The internal classiﬁcation
mechanism of these ML classiﬁers might be responsible for such performance diﬀerences. We plan
to investigate these diﬀerences among ML classiﬁers in the future study. The other two subject
systems, which show a decrease in F1 Scores, are Accumulo and Bitcoin, but we can not see a
reduction in all three cases (CA, CD, and CAD) of feature combinations and classiﬁers. F1 Score
decreases only in the result of Accumulo and Bitcoin when we use CD features with KNN classiﬁer
and CAD features with Random Forest classiﬁer, respectively.

In summary, the number of subject systems when the use of all the SCG-based feature combi-
nations decreases the F1 Score is only 5.56% (2 out of 36) of the total number of tested scenarios
(12 × 3) in 12 subject systems using three machine learning classiﬁers. Therefore, 94.44 % of the
test cases of 12 subject systems in all the three machine learning models of this investigation show
increase in F1 Scores by combining SCG based features and Commit Guru features. In the future,
we plan to investigate whether the source code structure or the number of available data instances
to train and test machine learning models in these subject systems (e.g., Accumulo, Bitcoin, Mongo,
and Oozie) is responsible for reducing the F1 Score. We also plan to add additional subject systems
to continue this study in the future.

We also performed the Wilcoxon Signed-Rank Test [Rosner et al. (2006); Wilcoxon (1945)] to see
the statistical signiﬁcance of the improvements in F1 Scores using diﬀerent feature combinations. As
we have 12 subject systems, we obtained 12 F1 Scores for each of the seven feature combinations. We
wanted to verify whether the improvement of F1 Scores with the addition of SCG-based features is
statistically signiﬁcant. In Figures 8, 9, and 10, F1 Scores obtained by using CAD, CA, CD feature
combinations provide improved results than F1 Scores obtained by using only C. Therefore, we
did the Wilcoxon Signed-Rank Test using the F1 Scores of these four feature combinations. The
resulting p values are in Table 7. We ﬁnd all the p values less than 0.05, which indicates the
improvement of results is statistically signiﬁcant. To summarize our statistically signiﬁcant test
result, we can say that using all the three ML models, CA, CD, and CAD features provides a
better result than the C features in all the subject systems using all the three machine learning
models of this study.

Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction

21

Table 7 p-Values in Wilcoxon Signed Rank Test. As all the p-Values are below 0.05, we get improved
results in all the combinations of A and D features with C feature

ML Algorithm

Random Forest
Classiﬁer

Logistic Regression
Classiﬁer

K-Nearest Neighbour
Classiﬁer

Comparing Features
C vs CA
C vs CD
C vs CAD
C vs CA
C vs CD
C vs CAD
C vs CA
C vs CD
C vs CAD

p-Value
0.005
0.049
0.006
0.005
0.010
0.007
0.003
0.039
0.002

The visualization of commit categories in Figure 11 shows when we use only the feature values
C, then it is impossible to distinguish diﬀerent commit categories from the generated visualization
(images on the left). Using CAD (Combination of C, A, and D) feature values can produce a distinct
cluster of merge commits. Although CAD feature values cannot distinguish all the seven commit
categories, it indicates a separate region for the merge-commits (green dots). This visualization
means merge-commits show a more distinctive structural diﬀerence in source code than other
commits categories. We obtained similar visualization in all the diﬀerent subject systems. In general,
we can summarize that the addition of Source Code Graph (SCG) representation based on structural
feature values helps produce a better visual representation of commit categories. Findings from this
visualization support our hypothesis, which states that diﬀerent commit categories have diﬀerent
coding structures. Therefore, it also answers our research questions RQ1 and RQ2 and suggests
providing importance in the structural properly of source code while changing a codebase. We
believe more investigations of these structural properties of source code could build intelligent
software-driven autonomous systems and minimize the occurrences of bugs in the future.

6 Threats to Validity

We utilized the automatically labelled dataset and default feature values from the implementation
done by [Rosen et al. (2015b)] and compared the visualization and classiﬁcation performance using
a diﬀerent combination of feature values of C, A, and D. It is possible their implementation may
contain incorrectly labelled commits. As in this investigation, we compare the performance im-
provement of visualization and ML-based models classiﬁcation by using diﬀerent combinations of
C, A, D; we believe the impact of the incorrectly labelled dataset is insigniﬁcant in the comparison
scenario of our investigation. If we had used any other data labelling methods, our comparison
scenario might have provided a similar result as we got in this investigation. The analysis based
on visualization is subjective to human perception. Hence a deeper investigation using controlled
study can be an avenue for future research.

We have used 12 subject systems written in C++ and Java programming languages. The use
of any other programming language might have diﬀerent ﬁndings. The datasets from all the 12
subject systems contain more than 246 K data instances, and we also implemented three diﬀerent
ML-based algorithms and seven diﬀerent feature combinations. Thus, we believe our ﬁndings should
be generalizable toward other software systems written in C++ or Java programming languages
or other programming languages with similar architecture to C++ and Java. However, a practical

22

Md Nadim et al.

Fig. 11 Comparing the visualization of commit categories shows the enhanced clustering of diﬀerent commit cat-
egories when we use SCG-based features in combination with the C feature (e.g., CAD on the right) compared to
the visualization on the left when SCG features are not included (e.g., only C on the left). The visualization using
CAD features can distinctly identify the clusters of merge commits which are represented by green dots (enclosed
by a red boundary) in this image.

veriﬁcation of this assumption is required. We plan to do a similar investigation with a few other
popular programming languages and software systems in the future.

7 Related Work

Visualizing a software system can help to analyze the evolution of software architecture, identify
the developer network, ﬁnd stable software releases, and monitor software quality trends [Diehl
(2007a)]. A rich body of studies [Alexandru et al. (2019); Burch et al. (2011); Kim et al. (2020);
Sandoval Alcocer et al. (2013); Tomida et al. (2019)] explored diﬀerent ways to visualize evolving
software systems for making it easily understandable to keep the consistent evolution. Chevalier
et al. (2007) did a visualization of evolution patterns in C++ source code by rendering syntax
matched code blocks in consecutive versions to detect the code fragments which have been changed
during evolution. There are several tools for visualizing software systems [Mostafa and Krintz
(2009); Reniers et al. (2014); Voinea et al. (2005)]. Among those tools, many were mainly used
by software architects [Nam et al. (2018)], who are interested in the high-level abstraction of the
software system. They are not interested in coding details. Bulmer et al. (2018) examined code

Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction

23

patterns by visualizing the code written by novice programmers such as ﬁrst-year students. They
wanted to help either instructors or students identify poor programming practices during the coding
process. Jones et al. (2002) implemented source code visualization for locating defects or faults in
the source code. Studies were also done to help the maintenance activities by visualizing the software
system [Storey et al. (2008)]. Sandoval Alcocer et al. (2019) proposed an interactive visualization
technique that allows practitioners to analyze the performance of software components along with
multiple software versions at once. Diehl (2007b) identiﬁed three categories (structure, behaviour,
and evolution) for using visualization techniques to explore software systems.

There are a bunch of studies [Asaduzzaman et al. (2012); Bavota et al. (2012); Bernardi et al.
(2012); Canfora et al. (2011); Ell (2013); Eyolfson et al. (2011); Kim and Whitehead (2006);
´Sliwerski et al. (2005a)] to investigate software commit operations that can induce new bug (bug in-
ducing commit) in software systems, which is known as Just-In-Time (JIT) [Yang et al. (2015)] bug
prediction in literature. Most of the studies for JIT bug prediction utilized source code repository-
based statistics [Borg et al. (2019); Rosen et al. (2015a)] of commit operations such as the number
of lines added, deleted, or modiﬁed. Shivaji et al. (2013) treated the source code as natural lan-
guage text [Hindle et al. (2012); Ray et al. (2016)] and utilized Natural Language Processing (NLP)
Jones (1994) techniques to extract feature values to apply ML-based detection models. Deep learn-
ing techniques, such as Deep Belief Networks (DBN) [Wang et al. (2016)], have also been used
to learn semantic features from the abstract syntax trees of the program. In this study, we used
the structural properties of source code by representing code segment in commit patch as a graph
structure. Compared to all the existing studies in literature, our study emphasized the structural
attributes of the source code and reached the performance improvement in detecting bugs.

We did not ﬁnd any study that utilized the features obtained from source code visualization to
identify bug presence in software systems using machine learning-based detection models. However,
several studies [Allamanis et al. (2018); Ferrante et al. (1987); Mou et al. (2016); Nguyen and
Nguyen (2015); Pradel and Sen (2018); Tufano et al. (2018); Wang et al. (2016)] proposed diﬀerent
representations to preserve syntactic and semantic information in source code embedding, but
they require compiled intermediate model of byte-codes [LLVM Compiler Infrastructure (2019);
Myers (1981); Zhang et al. (2019)]. These techniques are not directly applicable for the source code
fragments in the commit patch as they are uncompilable and incomplete. Zhang et al. (2019) is the
only study that can process incomplete and uncompilable source code for converting them numeric
vector representation, which could use in ML-based models. They implemented Abstract Syntax
Tree (AST) [Baxter et al. (1998)] based Neural Network (ASTNN) to learn vector representation
of source code fragments and applied them to classify source code and ﬁnd code clones. Our work
diﬀers from their work in the ﬁnal embedding of source code-based feature values. They utilized
the Word2Vec [Mikolov et al. (2013)] model for making numeric vector-based representation from
the generalized tokens obtained from the AST of a source code fragment.

On the other hand, we converted the generalized tokens of the source code fragment into a graph
from which we extracted distinct feature values. In addition, during the processing of source code
fragments by Abstract Syntax Tree (AST) based Neural Network (ASTNN), they created a separate
numeric vector representation for each of the code fragments, making it challenging to adapt the
technique for the commits that have multiple code fragments changes. In our investigation, we
found most of the commit operations have multiple changed fragments, and we created separate
graph representation for each of the fragments and then took the union of all the graphs [Bondy
and Murty (1976)] in a commit to forming a single graph. The union of graphs makes it very easy
to represent a single commit by visualization using graph representation and to take the attributes

24

Md Nadim et al.

of the ﬁnal graph to form a feature set for the commit. Since a graph structure often encodes
rich information, we believe our graph-based feature will provide a more easily understandable
representation of source code complexity and developers’ coding style.

8 Conclusion & Future Work

Source code in software systems experiences several changes to adapt to diﬀerent requirements
in its evolution & maintenance process. Sometimes a change done to solve a problem (bug) in a
software system may also be a reason for introducing new issues (bug introducing) in that system,
requiring more subsequent changes and attention to eliminate the introduced bug. The key objective
of this investigation was to produce a graph representation based on the programming styles of
software developers. The attribute values from these representations are used in detecting and
visualizing commits during software revisions. We introduced Source Code Graph (SCG) based
feature values and validated their use to visualize and detect buggy and non-buggy commits from
12 C++ and Java-based software systems. All the software systems used in this study contain more
than 246 K commit instances. This study also investigated the use of SCG-based feature values
to visualize the buggy and non-buggy commits and seven commit categories. The visualizations
we implemented show that the addition of SCG-based features can distinctly identify the merge
commits and improve the detection and visualization of other commit categories. As SCG-based
features mainly represent the style of writing source code, this study summarizes the importance of
developers’ coding style or patterns. These ﬁndings motivate us to do more experiments on these
source code patterns in future. The ﬁndings of this research can help us to produce guidelines
for software developers to avoid coding structures that are more likely to induce bugs in software
systems. Avoiding an inconsistent/ risky coding structure on a Maintenance Phases of Software
Engineering might help the developer avoid unstable/ buggy software evolution.

Although all the visualizations of bug presence and commit classiﬁcation in this study have been
improved by adding Source Code Graph (SCG) based features, it still can not entirely separate
distinct regions among the commit categories. We plan to do more investigations in future on the
extraction and encoding of SCG-based features to identify diﬀerent commit categories eﬀectively
(e.g., automatically identifying 1-Merge, 2-Corrective, 3-Preventive, and other commits categories
as presented in Table 3). In the future, we also plan to extend this study to ﬁnd and represent
speciﬁc coding styles and patterns most common in buggy software revisions. The continuation of
this study in more detailed investigations can help to propose the correct coding style or guideline
for eﬀective and bug-free software evolution & maintenance.

In this study, we show a novel approach to encode source code structure into a graph repre-
sentation and then utilize the attributes of that graph to identify buggy and non-buggy software
commits. We can also investigate the techniques applied for encoding and using structural proper-
ties of source code in some related studies, such as detecting replicated source codes in the code
repositories of diﬀerent software systems [Baxter et al. (1998); Dhaya and Kanthavel (2020); Zhang
et al. (2019)], and implementing automatic identiﬁcation of software defects (e.g., faults or bugs)
[Shakya and Smys (2020)]. Therefore, we plan to investigate these new research directions in future
studies.

Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction

25

Acknowledgements This work is supported by the Natural Sciences and Engineering Research Council of Canada
(NSERC) and by two Canada First Research Excellence Fund (CFREF) grants coordinated by Global Institute for
Food Security (GIFS) and Global Institute for Water Security (GIWS).

References

Alexandru CV, Proksch S, Behnamghader P, Gall HC (2019) Evo-clocks: Software evolution at a

glance. In: 2019 Working Conference on Software Visualization (VISSOFT), pp 12–22

Allamanis M, Brockschmidt M, Khademi M (2018) Learning to represent programs with graphs. In:
International Conference on Learning Representations, URL https://openreview.net/forum?
id=BJOFETxR-

Asaduzzaman M, Bullock MC, Roy CK, Schneider KA (2012) Bug introducing changes: A case
study with android. In: 2012 9th IEEE Working Conference on Mining Software Repositories
(MSR), pp 116–119

Bavota G, De Carluccio B, De Lucia A, Di Penta M, Oliveto R, Strollo O (2012) When does a
refactoring induce bugs? an empirical study. In: Proceedings of the 2012 IEEE 12th Interna-
tional Working Conference on Source Code Analysis and Manipulation, IEEE Computer Society,
Washington, DC, USA, SCAM ’12, pp 104–113

Baxter I, Yahin A, Moura L, Sant’Anna M, Bier L (1998) Clone detection using abstract syntax
trees. In: Proceedings. International Conference on Software Maintenance (Cat. No. 98CB36272),
pp 368–377, DOI 10.1109/ICSM.1998.738528

Bernardi ML, Canfora G, Di Lucca GA, Di Penta M, Distante D (2012) Do developers introduce
bugs when they do not communicate? the case of eclipse and mozilla. In: 2012 16th European
Conference on Software Maintenance and Reengineering, pp 139–148

Bondy JA, Murty USR (1976) Graphs and Subgraphs, Elsevier, New York, NY, pp 9–10
Borg M, Svensson O, Berg K, Hansson D (2019) Szz unleashed: an open implementation of the
szz algorithm - featuring example usage in a study of just-in-time bug prediction for the jenkins
project. Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning
Techniques for Software Quality Evaluation - MaLTeSQuE 2019 DOI 10.1145/3340482.3342742,
URL http://dx.doi.org/10.1145/3340482.3342742

Bulmer J, Pinchbeck A, Hui B (2018) Visualizing code patterns in novice programmers. In: Pro-
ceedings of the 23rd Western Canadian Conference on Computing Education, Association for
Computing Machinery, New York, NY, USA, WCCCE ’18, DOI 10.1145/3209635.3209652, URL
https://doi.org/10.1145/3209635.3209652

Burch M, Vehlow C, Beck F, Diehl S, Weiskopf D (2011) Parallel edge splatting for scalable dynamic
graph visualization. IEEE Transactions on Visualization and Computer Graphics 17(12):2344–
2353

Canfora G, Ceccarelli M, Cerulo L, Di Penta M (2011) How long does a bug survive? an empirical

study. In: 2011 18th Working Conference on Reverse Engineering, pp 191–200

Chevalier F, Auber D, Telea A (2007) Structural analysis and visualization of c++ code evolution
using syntax trees. In: Ninth International Workshop on Principles of Software Evolution: In
Conjunction with the 6th ESEC/FSE Joint Meeting, Association for Computing Machinery,
New York, NY, USA, IWPSE ’07, p 90–97, DOI 10.1145/1294948.1294971, URL https://doi.
org/10.1145/1294948.1294971

26

Md Nadim et al.

Collofello JS, Woodﬁeld SN (1989) Evaluating the eﬀectiveness of reliability-assurance techniques.
Journal of Systems and Software 9(3):191 – 195, DOI https://doi.org/10.1016/0164-1212(89)
90039-3, URL http://www.sciencedirect.com/science/article/pii/0164121289900393
Dhaya R, Kanthavel R (2020) Comprehensively meld code clone identiﬁer for replicated source
code identiﬁcation in diverse web browsers. Journal of trends in Computer Science and Smart
technology (TCSST) pp 109 – 119

Diehl S (2007a) Software Visualization: Visualizing the Structure, Behaviour, and Evolution of

Software. Springer-Verlag, Berlin, Heidelberg

Diehl S (2007b) Software Visualization: Visualizing the Structure, Behaviour, and Evolution of

Software. Springer-Verlag, Berlin, Heidelberg

Ell J (2013) Identifying failure inducing developer pairs within developer networks. In: Proceedings

of the 35th International Conference on Software Engineering (ICSE’13), pp 1471 – 1473

Eyolfson J, Tan L, Lam P (2011) Do time of day and developer experience aﬀect commit bugginess?
In: Proceedings of the 8th Working Conference on Mining Software Repositories (MSR’11), pp
153 – 162

Ferrante J, Ottenstein KJ, Warren JD (1987) The program dependence graph and its use in
optimization. ACM Trans Program Lang Syst 9(3):319–349, DOI 10.1145/24039.24041, URL
https://doi-org.cyber.usask.ca/10.1145/24039.24041

GoogleDevelopers (2020 (accessed August 26, 2021)) Classiﬁcation: Precision and recall —
machine learning crash course. URL https://developers.google.com/machine-learning/
crash-course/classification/precision-and-recall

Gove R (2019) Gragnostics: Fast, interpretable features for comparing graphs. In: Banissi E, Ursyn
A, Bannatyne MWM, Datia N, Francese R, Sarfraz M, Wyeld TG, Bouali F, Venturini G, Azzag
H, Lebbah M, Trutschl M, Cvek U, M¨uller H, Nakayama M, Kernbach S, Caruccio L, Risi M,
Erra U, Vitiello A, Rossano V (eds) 23rd International Conference on Information Visualisation
(IV), IEEE, pp 201–209

Gu Z, Barr ET, Hamilton DJ, Su Z (2010) Has the bug really been ﬁxed? In: Proceedings of the
32Nd ACM/IEEE International Conference on Software Engineering - Volume 1, ACM, New
York, NY, USA, ICSE ’10, pp 55–64

Hamilton WL, Ying R, Leskovec J (2017) Representation learning on graphs: Methods and appli-

cations. IEEE Data Eng Bull 40(3):52–74

Hindle A, German DM, Holt R (2008) What do large commits tell us? a taxonomical study of large
commits. In: Proceedings of the 2008 International Working Conference on Mining Software
Repositories, Association for Computing Machinery, New York, NY, USA, MSR ’08, p 99–108,
DOI 10.1145/1370750.1370773, URL https://doi.org/10.1145/1370750.1370773

Hindle A, Barr ET, Su Z, Gabel M, Devanbu P (2012) On the naturalness of software. In: Pro-
ceedings of the 34th International Conference on Software Engineering, IEEE Press, ICSE ’12,
p 837–847

Hinton G, Roweis S (2003) Stochastic neighbor embedding. Advances in neural information pro-

cessing systems 15:833–840

Islam MR, Zibran MF (2021) What changes in where? an empirical study of bug-ﬁxing change
patterns. SIGAPP Appl Comput Rev 20(4):18–34, DOI 10.1145/3447332.3447334, URL https:
//doi.org/10.1145/3447332.3447334

Jajuga K, Walesiak M (2000) Standardisation of data set under diﬀerent measurement scales. In: R
D, W G (eds) Classiﬁcation and Information Processing at the Turn of the Millennium. Studies
in Classiﬁcation, Data Analysis, and Knowledge Organization, Springer, Berlin, Heidelberg, pp

Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction

27

105–111

Jones JA, Harrold MJ, Stasko J (2002) Visualization of test information to assist fault localization.
In: Proceedings of the 24th International Conference on Software Engineering, Association for
Computing Machinery, New York, NY, USA, ICSE ’02, p 467–477, DOI 10.1145/581339.581397,
URL https://doi.org/10.1145/581339.581397

Jones KS (1994) Natural Language Processing: A Historical Review, Springer Netherlands,
Dordrecht, pp 3–16. DOI 10.1007/978-0-585-35958-8\ 1, URL https://doi.org/10.1007/
978-0-585-35958-8_1

Kamei Y, Shihab E, Adams B, Hassan AE, Mockus A, Sinha A, Ubayashi N (2013) A large-scale

empirical study of just-in-time quality assurance. IEEE Trans Softw Eng 39(6):757–773

Kim S, Whitehead EJ Jr (2006) How long did it take to ﬁx bugs? In: Proceedings of the International

Workshop on Mining Software Repositories (MSR’06), pp 173 – 174

Kim S, Whitehead, Jr EJ, Zhang Y (2008) Classifying software changes: Clean or buggy? IEEE

Transactions on Software Engineering 34(2):181–196

Kim Y, Kim J, Jeon H, Kim YH, Song H, Kim B, Seo J (2020) Githru: Visual analytics for
understanding software development history through git metadata analysis. IEEE Transactions
on Visualization and Computer Graphics pp 1–1, DOI 10.1109/TVCG.2020.3030414

Kruiger JF, Rauber PE, Martins RM, Kerren A, Kobourov S, Telea AC (2017) Graph layouts by

t-sne. Comput Graph Forum 36(3):283–294

LLVM Compiler Infrastructure (2019) Llvm’s analysis and transform passes – llvm 13 documenta-

tion. URL https://llvm.org/docs/Passes.html, [Online; accessed 16-June-2021]

van der Maaten L, Hinton G (2008) Visualizing data using t-sne. Journal of Machine Learning

Research 9(86):2579–2605, URL http://jmlr.org/papers/v9/vandermaaten08a.html

Mikolov T, Sutskever I, Chen K, Corrado G, Dean J (2013) Distributed representations of words
and phrases and their compositionality. In: Proceedings of the 26th International Conference
on Neural Information Processing Systems - Volume 2, Curran Associates Inc., Red Hook, NY,
USA, NIPS’13, p 3111–3119

Mockus, Votta (2000) Identifying reasons for software changes using historic databases. In: Pro-
ceedings 2000 International Conference on Software Maintenance, pp 120–130, DOI 10.1109/
ICSM.2000.883028

Mostafa N, Krintz C (2009) Tracking performance across software revisions. In: Proceedings of the
7th International Conference on Principles and Practice of Programming in Java, Association
for Computing Machinery, New York, NY, USA, PPPJ ’09, p 162–171, DOI 10.1145/1596655.
1596682, URL https://doi.org/10.1145/1596655.1596682

Mou L, Li G, Zhang L, Wang T, Jin Z (2016) Convolutional neural networks over tree structures
for programming language processing. In: Proceedings of the Thirtieth AAAI Conference on
Artiﬁcial Intelligence, AAAI Press, AAAI’16, p 1287–1293

Myers EM (1981) A precise inter-procedural data ﬂow algorithm. In: Proceedings of the 8th ACM
SIGPLAN-SIGACT Symposium on Principles of Programming Languages, Association for Com-
puting Machinery, New York, NY, USA, POPL ’81, p 219–230, DOI 10.1145/567532.567556, URL
https://doi-org.cyber.usask.ca/10.1145/567532.567556

Nadim M (2020) Investigating the techniques to detect and reduce bug inducing commits during
change operations in software systems. Master’s thesis, University of Saskatchewan, Saskatoon,
Canada, URL https://harvest.usask.ca/handle/10388/13125

Nam D, Lee YK, Medvidovic N (2018) Eva: A tool for visualizing software architectural evolution.
In: 2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-

28

Companion), pp 53–56

Md Nadim et al.

Nguyen AT, Nguyen TN (2015) Graph-based statistical language model for code. In: Proceedings
of the 37th International Conference on Software Engineering - Volume 1, IEEE Press, ICSE ’15,
p 858–868

Pradel M, Sen K (2018) Deepbugs: A learning approach to name-based bug detection. Proc ACM
Program Lang 2(OOPSLA), DOI 10.1145/3276517, URL https://doi.org/10.1145/3276517
Purushothaman R, Perry D (2005) Toward understanding the rhetoric of small source code changes.

IEEE Transactions on Software Engineering 31(6):511–526, DOI 10.1109/TSE.2005.74

Rajlich V (2014) Software evolution and maintenance. In: Future of Software Engineering Pro-
ceedings, Association for Computing Machinery, New York, NY, USA, FOSE 2014, p 133–144,
DOI 10.1145/2593882.2593893, URL https://doi.org/10.1145/2593882.2593893

Ray B, Hellendoorn V, Godhane S, Tu Z, Bacchelli A, Devanbu P (2016) On the ”naturalness”
of buggy code. In: Proceedings of the 38th International Conference on Software Engineering,
Association for Computing Machinery, New York, NY, USA, ICSE ’16, p 428–439, DOI 10.1145/
2884781.2884848, URL https://doi-org.cyber.usask.ca/10.1145/2884781.2884848

Reniers D, Voinea L, Ersoy O, Telea A (2014) The solid* toolset for software visual analytics of
program structure and metrics comprehension: From research prototype to product. Sci Com-
put Program 79:224–240, DOI 10.1016/j.scico.2012.05.002, URL https://doi.org/10.1016/j.
scico.2012.05.002

Rosen C, Grawi B, Shihab E (2015a) Commit guru: Analytics and risk prediction of software
commits. In: Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,
Association for Computing Machinery, New York, NY, USA, ESEC/FSE 2015, p 966–969, DOI
10.1145/2786805.2803183, URL https://doi.org/10.1145/2786805.2803183

Rosen C, Grawi B, Shihab E (2015b) Commit guru: Analytics and risk prediction of software
commits. In: Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,
Association for Computing Machinery, New York, NY, USA, ESEC/FSE 2015, p 966–969, DOI
10.1145/2786805.2803183, URL https://doi.org/10.1145/2786805.2803183

Rosner B, Glynn RJ, Lee MLT (2006) The wilcoxon signed rank test for paired comparisons of

clustered data. Biometrics 62(1):185–192

Sandoval Alcocer JP, Bergel A, Ducasse S, Denker M (2013) Performance evolution blueprint:
Understanding the impact of software evolution on performance. In: 2013 First IEEE Working
Conference on Software Visualization (VISSOFT), pp 1–9

Sandoval Alcocer JP, Beck F, Bergel A (2019) Performance evolution matrix: Visualizing perfor-
mance variations along software versions. In: 2019 Working Conference on Software Visualization
(VISSOFT), pp 1–11

Shakya S, Smys S (2020) Reliable automated software testing through hybrid optimization algo-
rithm. Journal of Ubiquitous Computing and Communication Technologies (UCCT) pp 126 –
135

Shivaji S, James Whitehead E, Akella R, Kim S (2013) Reducing features to improve code change-

based bug prediction. IEEE Transactions on Software Engineering 39(4):552–569

´Sliwerski J, Zimmermann T, Zeller A (2005a) Hatari: Raising risk awareness. ACM SIGSOFT

Software Engineering Notes 30(5):107 – 110

´Sliwerski J, Zimmermann T, Zeller A (2005b) When do changes induce ﬁxes? ACM SIGSOFT

Software Engineering Notes 30(4):1 – 5

Storey M, Bennett C, Bull RI, German DM (2008) Remixing visualization to support collaboration

in software maintenance. In: 2008 Frontiers of Software Maintenance, pp 139–148

Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction

29

Student (1908) The probable error of a mean. Biometrika pp 1–25
Tabassum S, Minku LL, Feng D, Cabral GG, Song L (2020) An investigation of cross-project
learning in online just-in-time software defect prediction. In: 2020 IEEE/ACM 42nd International
Conference on Software Engineering (ICSE), pp 554–565

Tan M, Tan L, Dara S, Mayeux C (2015) Online defect prediction for imbalanced data. In: Proceed-
ings of the 37th International Conference on Software Engineering - Volume 2, IEEE Press, Pis-
cataway, NJ, USA, ICSE ’15, pp 99–108, URL http://dl.acm.org/citation.cfm?id=2819009.
2819026

Tomida Y, Higo Y, Matsumoto S, Kusumoto S (2019) Visualizing code genealogy: How code is
evolutionarily ﬁxed in program repair? In: 2019 Working Conference on Software Visualization
(VISSOFT), pp 23–27

Tufano M, Watson C, Bavota G, Di Penta M, White M, Poshyvanyk D (2018) Deep learning
similarities from diﬀerent representations of source code. In: Proceedings of the 15th International
Conference on Mining Software Repositories, Association for Computing Machinery, New York,
NY, USA, MSR ’18, p 542–553, DOI 10.1145/3196398.3196431, URL https://doi.org/10.
1145/3196398.3196431

Vieira R, da Silva A, Rocha L, Gomes JaP (2019) From reports to bug-ﬁx commits: A 10 years
dataset of bug-ﬁxing activity from 55 apache’s open source projects. In: Proceedings of the
Fifteenth International Conference on Predictive Models and Data Analytics in Software Engi-
neering, Association for Computing Machinery, New York, NY, USA, PROMISE’19, p 80–89,
DOI 10.1145/3345629.3345639, URL https://doi.org/10.1145/3345629.3345639

Voinea L, Telea A, van Wijk JJ (2005) Cvsscan: Visualization of code evolution. In: Proceedings
of the 2005 ACM Symposium on Software Visualization, Association for Computing Machinery,
New York, NY, USA, SoftVis ’05, p 47–56, DOI 10.1145/1056018.1056025, URL https://doi.
org/10.1145/1056018.1056025

Wang S, Liu T, Tan L (2016) Automatically learning semantic features for defect prediction. In:
Proceedings of the 38th International Conference on Software Engineering, Association for Com-
puting Machinery, New York, NY, USA, ICSE ’16, p 297–308, DOI 10.1145/2884781.2884804,
URL https://doi.org/10.1145/2884781.2884804

Wen M, Wu R, Liu Y, Tian Y, Xie X, Cheung SC, Su Z (2019) Exploring and exploiting the
correlations between bug-inducing and bug-ﬁxing commits. In: Proceedings of the 27th ACM
Joint Meeting on European Software Engineering Conference and Symposium on the Foundations
of Software Engineering (ESEC/FSE’19), pp 326 – 337

Wilcoxon F (1945) Individual comparisons by ranking methods. Biometrics Bulletin 1(6):80 – 83,

URL http://www.jstor.org/stable/3001968

Yang X, Lo D, Xia X, Zhang Y, Sun J (2015) Deep learning for just-in-time defect prediction. In:
Proceedings of the IEEE International Conference on Software Quality, Reliability and Security
(QRS’15), pp 17 – 26

Yin Z, Yuan D, Zhou Y, Pasupathy S, Bairavasundaram L (2011) How do ﬁxes become bugs?
In: Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on
Foundations of Software Engineering, ACM, New York, NY, USA, ESEC/FSE ’11, pp 26–36
Zhang J, Wang X, Zhang H, Sun H, Wang K, Liu X (2019) A novel neural source code representation
based on abstract syntax tree. In: 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE), pp 783–794, DOI 10.1109/ICSE.2019.00086

