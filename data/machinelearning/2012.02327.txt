0
2
0
2
c
e
D
3

]
L
C
.
s
c
[

1
v
7
2
3
2
0
.
2
1
0
2
:
v
i
X
r
a

Evolving Character-Level DenseNet
Architectures using Genetic Programming

Trevor Londt[0000−0002−9428−8089], Xiaoying Gao[0000−0002−6326−7947], and Peter
Andreae[0000−0002−2789−680X]

School of Engineering and Computer Science,
Victoria University of Wellington, Wellington, New Zealand
{trevor.londt, xgao, peter.andreae}@ecs.vuw.ac.nz

Abstract. DenseNet architectures have demonstrated impressive per-
formance in image classiﬁcation tasks, but limited research has been
conducted on using character-level DenseNet (char-DenseNet) architec-
tures for text classiﬁcation tasks. It is not clear what DenseNet archi-
tectures are optimal for text classiﬁcation tasks. The iterative task of
designing, training and testing of char-DenseNets is an NP-Hard prob-
lem that requires expert domain knowledge. Evolutionary deep learn-
ing (EDL) has been used to automatically design CNN architectures
for the image classiﬁcation domain, thereby mitigating the need for ex-
pert domain knowledge. This study demonstrates the ﬁrst work on using
EDL to evolve char-DenseNet architectures for text classiﬁcation tasks.
A novel genetic programming-based algorithm (GP-Dense) coupled with
an indirect-encoding scheme, facilitates the evolution of performant char-
DenseNet architectures. The algorithm is evaluated on two popular text
datasets, and the best-evolved models are benchmarked against four cur-
rent state-of-the-art character-level CNN and DenseNet models. Results
indicate that the algorithm evolves performant models for both datasets
that outperform two of the state-of-the-art models in terms of model
accuracy and three of the state-of-the-art models in terms of parameter
size.

Keywords: Character-level DenseNet · Evolutionary deep learning ·
Genetic programming · Text classiﬁcation.

1

Introduction

Natural language processing (NLP) has beneﬁted signiﬁcantly from the appli-
cation of deep learning [1] techniques in recent years [2][3][4][5]. In particular,
the task of text classiﬁcation has gained impressive performance increases. Long
short-term memory (LSTM) [6] models have traditionally been the architec-
ture of choice for text classiﬁcation tasks. However, transformer [7] models and
convolutional neural networks (CNN) [2][3][8] outperform LSTM’s for text classi-
ﬁcation tasks. While transformer models dominate the leaderboards, they have
the disadvantage of containing massive numbers of parameters and the need

 
 
 
 
 
 
2

T. Londt et al.

for pre-trained language models. Only a limited number of languages have pre-
trained language models available. CNN’s are generally not encumbered by these
limitations and have demonstrated excellent success for text classiﬁcation tasks.
There are two approaches when using CNNs for text classiﬁcation: word-level
(word-CNN) and character-level CNNs (char-CNN). Word-level approaches gen-
erally outperform character-level approaches [2]. However, word-level approaches
require a pre-trained word-model such as word2vec [9] or GloVe [10]. This re-
quirement is a similar limitation as seen with transformer models. Further, an
additional disadvantage of word-level CNNs is that they require the input text to
be pre-processed by removing stop words, stemming words, removing punctua-
tion and dealing with out of vocabulary words. Each of these operations increases
the likelihood of the input text being corrupted or important discriminating in-
formation being inadvertently removed. There is also the potential problem of
not having a pre-trained word-model available for a particular language. Char-
CNNs require no pre-trained word or language models. Also, char-CNNs require
no pre-processing of the input text data, mitigating any potential errors intro-
duced through incorrect pre-processing. The disadvantage of char-CNNs is that
they are, in general, less accurate than word-level CNNs. Research has shown
that adding depth to a char-CNN does not result in breakthrough performance
improvements, as shown in the image classiﬁcation domain. DenseNet [11] is a
network architecture that has shown good performance in the image domain
and has recently shown promising results in the text classiﬁcation domain [8].
However, the results have still not demonstrated breakthrough level improve-
ments. Research focused on using DenseNets for text classiﬁcation is limited,
with little to no further research conducted to determine what hyperparameters
or DenseNet topologies could further improve classiﬁcation performance.

Designing, training and testing a deep network architecture is not a trivial
task. A network topology has a learning bias, and its performance is depen-
dant on the quality of chosen hyperparameters. The selection of these proper-
ties is based on trial and error or the practitioners’ experience. Evolutionary
deep learning (EDL) is an evolutionary computation technique that aims to lo-
cate performant network architectures for a particular task automatically. An
evolutionary-inspired search algorithm coupled with the backpropagation [12] al-
gorithm is used to locate, train and evaluate a population of candidate network
architectures. Each candidate architectures is encoded in a genotype, contained
in the population. Each genotype is decoded to a phenotype which represents
a trainable candidate architecture. EDL removes the need for expert domain
knowledge and the need for a trial and error approach. To the best of our knowl-
edge, EDL has never been used to locate candidate char-DenseNets for text
classiﬁcation tasks.

Genetic programming (GP) [13] is an evolutionary-inspired algorithm that
evolves a population of programs represented by tree structures. The task of man-
ually constructing a neural network can be considered as a sequence of program
steps taken to construct a neural network from scratch up to a performant state.
These sequence of steps are the equivalent of a computer program (genotype)

Evolving char-DenseNets using Genetic Programming

3

that when executed, constructs a neural network (phenotype). This property
makes GP an appropriate algorithm to evolve neural network architectures.

1.1 Goal

The goal of this work is to use GP with the backpropagation algorithm to evolve
performant character-level DenseNet architectures for text classiﬁcation tasks
automatically. This goal is achieved through the following objectives:

1. Introduce an appropriate indirect-encoding for representing char-DenseNet

architectures that can be used in a GP-based algorithm.

2. Evaluate the proposed algorithm over two well-known text classiﬁcation
datasets, one being of small size and the other of large size, to determine the
ability of the algorithm to generalise over diﬀerent text classiﬁcation tasks.
3. Benchmark the performance of the best-evolved models against current state-

of-the-art char-CNN and char-DenseNet models.

2 Background

2.1 Character-level Convolutional Neural Networks

Zhang et al. [2] demonstrated that char-CNNs are an eﬀective approach for
text classiﬁcation. The approach implemented a modular design and used the
back-propagation [12] algorithm, via stochastic gradient descent [14], for network
training. A temporal convolutional module was used for convolutional operations
with their large model (Large Char-CNN) using 1024 feature maps and their
small model (Small Char-CNN) using 256 feature maps. Max-pooling allowed
networks deeper than six layers. ReLU [15] was used for non-linearity, and two
fully connected layers provided the classiﬁer component of their network. Each
character in the input text sequence was converted to a one-hot-vector. Each
vector was then stacked to produce a matrix of vectors representing the text
sequence. Text sequences were limited to a maximum of 1014 characters. Longer
text sequences were truncated, and shorter sequences were padded. The authors
created eight datasets to evaluate their model. The model was shallow at only
six layers. Their work showed that char-CNNs perform well over large datasets
but underperform on smaller datasets when compared to traditional machine
learning methods.

Motivated by the performance increases gained by adding depth to a net-
work as evidenced in image classiﬁcation tasks, Conneau et al. [3] introduced
their very deep convolutional neural network (VDCNN) model. Introducing the
concept of a convolutional block, consisting of a convolutional layer, a batch
normalisation layer and a ReLU activation function. Their model stacked these
convolutional blocks in sequential order one after the other. Further, their model
implemented ResNet [16] links to allow their model to be extended to a depth
of 29 layers. Their model outperformed all current state of the art char-CNNs
and demonstrated the importance of adding depth to char-CNNs. However, their

4

T. Londt et al.

model could not be extended beyond 29 layers without degrading the model’s
classiﬁcation accuracy.

Le et al. [8] conducted a study to understand the role of depth for both
char-CNNs and word-CNNs. Their model was inspired by the DenseNet [11]
model used for image classiﬁcation tasks. Using the same hyperparameters as
in [2] and [3] and introducing the concept of a Dense block, where each block
consisted of multiple convolutional blocks stacked in sequential order with all
convolutional blocks densely connected. Their model was able to outperform
VDCNN on some datasets. Their research showed that DenseNet architectures
have promising potential for text classiﬁcation tasks. An interesting ﬁnding is
that, again, adding depth to their model resulted in only minor improvements.
The authors conclude that char-CNNs must be deep to be eﬀective and that the
problem of improving char-CNNs is not yet well understood. It is not known
what other char-CNN architectures may perform better.

2.2 Related work

Evolutionary deep learning (EDL) is a challenging and popular research task,
particularly in the image classiﬁcation domain. However, there is little to no re-
search conducted on evolving CNNs for text classiﬁcation tasks. There is no
research directly related to evolving char-DenseNets. One important related
research work conducted was by Liang et al. [17]. Their algorithm, LEAF,
can evolve performant network architectures and hyperparameters concurrently
for image and text classiﬁcation tasks. The LEAF algorithm consists of an
algorithm-layer, systems-layer and problem-domain subsystem where each com-
ponent is responsible for diﬀerent aspects of the algorithm. The problem-domain
layer is supported by both the algorithm-layer and systems-layer. CoDeepNEAT
[18] is the underlying algorithm for the algorithm-layer, which is responsible for
evolving the topology and hyperparameters of the candidate networks. It should
be noted that networks were encoded as graphs structures. CoDeepNEAT makes
use of components such as LSTM, convolutional and fully connected layers,
where each selected component represents a node in the network topology. Con-
cerning text classiﬁcation, their algorithm was only evaluated on the Wikipedia
comment toxicity dataset [19]. It is noted that although the LEAF algorithm is
the most well-known algorithm for evolving architectures for text classiﬁcation
tasks, it is not speciﬁcally designed to evolve char-CNN architectures.

3 The Proposed Method

The proposed method, GP-Dense, is a genetic programming-based algorithm
that evolves GP trees representing executable programs that can construct train-
able char-DenseNet architectures. The genotypes (GP trees) consists of exe-
cutable program symbols, each representing an action to be performed in the
construction of a neural network. Decoding a genotype to a phenotype involves
the process of executing the program symbols in the genotype resulting in a

Evolving char-DenseNets using Genetic Programming

5

structurally valid and trainable neural network. The overall workﬂow of GP-
Dense is presented below in algorithm 1.

The algorithm begins by assigning a unique seed for the current experiment
run. Then a population of randomly generated genotypes of varying depth is
created. Each genotype is decoded to a phenotype and uploaded to the graphics
processing unit (GPU). Each phenotype is trained and evaluated on the vali-
dation set to determine its ﬁtness. The top ﬁttest corresponding genotypes are
added to the elite population. Tournament selection is used to generate a pop-
ulation of selected genotypes. Crossover and mutation operations are applied
to the selected individuals resulting in a new population of oﬀspring genotypes.
The elite genotypes are then added to the new population. This process is re-
peated until a maximum number of generations has been attained, after which
the ﬁttest genotype is retrained on the full training set for an extended period
of epochs and evaluated on the test set to determine its ﬁnal ﬁtness.

Algorithm 1: GP-DenseNet.

1 begin;
2

seed ← Assign next seed from list;
population ← genotypes with speciﬁed depth range;

3
4 while not maximum generations do
foreach genotype ∈ population do
5

GPU ← phenotype ← decode(genotype);
evaluate(genotype, reduced train. set, val. set);

6

7

8

9

10

11

12

end foreach
elite ← ﬁttest from population;
selected ← tournament(population);
of f spring population ← crossover(selected);
population ← mutate(of f spring population);
limit(population ∪ elite);

13
14 end while
15 f ittest ← population;
16 evaluate(f ittest, f ull train. set, test set);
17 end;

3.1 Encoding Network Architecture

An appropriate genotype encoding is required to represent neural network archi-
tectures in order for evolutionary algorithms to operate on them. There are two
encoding paradigms: direct and indirect encodings. Direct encoding approaches
explicitly state the components and structure of the network architecture. For
example, the genotype could contain convolutional layers, max-pooling and ac-
tivation functions, where the position of these components is directly related
to their position in the phenotype structure. A signiﬁcant drawback of this ap-

6

T. Londt et al.

proach is that evolutionary operators such as the crossover operation could result
in an oﬀspring architecture with a broken topology that is no longer a valid feed-
forward neural network. Indirect encodings contain the information on how to
build a neural network architecture starting from a valid base network architec-
ture. This information can take the form of executable program symbols. For
example, a program symbol can represent adding depth to the network. Evolu-
tionary operations, such as crossover operations, have little potential to create
invalid oﬀspring networks as the selected program symbols are designed to al-
ways return a valid network topology. Indirect encodings are a natural ﬁt for
genetic programming algorithms.

Gruau et el.[20] created the well known cellular encoding scheme for evolving
multilayer perceptrons. Cellular encoding is inspired by the division of biological
cells, as seen in nature. By using a set of operations on cells (nodes in the
network), the encoding can represent an extensive range of network topologies
from shallow to deep and narrow to wide. We propose the use of a reduced set
of cellular encoding operations to evolve char-DenseNet architecture where each
cell in the network represents a dense block [8].

Input and Classiﬁer Layer: Each character of the input text sequence is
encoded as a n-dimensional vector using a lookup table that contains embeddings
of a ﬁxed alphabet. Therefore, an input text sequence of length l will be encoded
into a matrix of dimension n x l. The classiﬁer layer of the ancestor network
is deﬁned as in [3], containing a k-max pooling layer followed by three fully
connected layers using ReLU [15] activation functions and softmax outputs in
the ﬁnal layer.

Dense Blocks (Cell): A dense block (cell), as represented in ﬁgure 1, consists
of multiple convolutional blocks as in [3]. Each convolutional block consists of
a batch normalisation layer, ReLU [15] activation function and convolutional
layer. The output channels from each convolutional block are transported to the
input channels of every following convolutional block in a feed-forward manner.
All preceding input channels to a convolutional block are concatenated together.
Densely connected blocks have been shown to increase classiﬁcation performance
and mitigate the vanishing gradient problem [11]. Each dense block is followed
by a transitional layer, as implemented in [8], containing a 1x3 convolution and
a 1x2 local max-pooling layer.

Fig. 1: Internal conﬁguration of a dense block (cell).

ConvolutionBatchNorm.ReLUCATCATCATConvolutionBatchNorm.ReLUConvolutionBatch Norm.ReLUEvolving char-DenseNets using Genetic Programming

7

GP Function Set: Two operations are used from Gruau et el’s.[20] cellular
encoding: SEQ and PAR. The SEQ program symbol is a sequential split oper-
ation. The operation is provided with a mother cell that is duplicated into a
new child cell. The mother and child cell are then connected to each other in a
feed-forward sequential manner from the mother to the child. The PAR program
symbol is also provided with a mother cell and produces a duplicate child cell.
However, the two cells are then connected in parallel, still in a feed-forward man-
ner. In this work, we treat a dense block as a cell. We introduce a third operation
called the END operation. This operation is a no operation symbol and is used
to terminate the execution of a branch of the GP tree currently being traversed.

Terminal Set and Decorator Set: Two sets are deﬁned: Dense block pa-
rameter terminals (DBT) and Training parameter decorators (TPD). The DBT
set consists of integers in the range from 1 to 10. This terminal set deﬁnes the
number of convolutional blocks contained in a dense block (cell). DBT is used
on all dense blocks (cells). The TPD consists of real values in the range 0.0 to
0.5 and represents the probability of a dense block being dropped during a batch
training cycle. This function serves as a dropout feature to prevent the neural
network from over-relying on a particular feature learned from the input data.
If a dense block has a dropout value of 0.5, then during every second batch
of training data, the dense block will be bypassed in the neural network. Each
symbol in a GP tree can be decorated with a value from the TPD terminal set
to control the dense block dropout operation during the training cycle of the
neural network. A probability value is used to determine if a dense block (cell)
is to be decorated with the drop out feature or not. To the best of the authors’
knowledge, this is the ﬁrst work to include a CNN training hyperparameter in
the genotype of an evolutionary algorithm.

3.2 Decoding Network Architecture

Fig. 2: Decoding genotype to phenotype. S=SEQ, P=PAR, E=END, DB=Dense
Block, D=Decorator

SPSEESEEE53320.2SPSEESEEE5332SPSEESEEE5332SPSEESEEE5332SPSEESEE5332I.II.III.IV.V.InputInital DBDepth=2ClassiﬁerInputInital DBDepth=2ClassiﬁerDBDepth=5InputInital DBDepth=2ClassiﬁerDBDepth=5DBDepth=3DBDepth=5InputInital DBDepth=2ClassiﬁerDBDepth=5DBDepth=3DBDepth=5Depth=3DBInputInital DBDepth=2ClassiﬁerDBDepth=5DBDepth=3DBDepth=5Depth=3DBDBDepth=20.20.20.20.2Drop=0.28

T. Londt et al.

The decoding of a genotype to its corresponding phenotype is demonstrated
in ﬁgure 2. GP-Dense uses a depth-ﬁrst traversal approach when decoding a
genotype. GP-Dense always start with a ancestor network containing one dense
block containing two convolutional blocks with an initial 32 input and output
channels. In step I, the ancestor network is represented by the symbol DB. The
root symbol, represented by the ﬁrst letter S (SEQ) of the GP tree, is executed.
Step II displays the outcome. The phenotype now has two dense blocks, the
original dense block and a child dense block that contains ﬁve convolutional
blocks. The ancestor dense block will now be operated on by the left branch of
the preceding symbol in the genotype and the child block by the right branch of
the preceding genotype symbol. Step III represents the neural network after the
P (PAR) symbol has been executed. Note that the PAR symbol operates on the
ancestor dense block that was created by the previous genotype symbol. This
ancestor dense block is now referred to as the mother dense block and the new
dense block as the child dense block. The mother and the child are now connected
in parallel. The next symbols is an S (SEQ) and the output is shown in step IV.
The following symbol is the END symbol which performs no operation but traces
back up the GP tree as indicated by the green arrows in step IV. Step V presents
the neural network after executing the S (SEQ) symbol connected to the right
branch of the root cell in the genotype. This operation results in the mother
dense block connected to a new child dense block, containing two convolutional
blocks, in sequential order. Note that the child dense block is decorated with the
real value of 0.2 , which represents a dropout function applied to the child dense
block.

3.3 Evolutionary Operators

Tournament selection is used to select k randomly selected genotypes from the
population to build a breeding pool. Single point crossover is used in this work. A
random point in the GP tree of each genotype is selected, and the subtrees of each
genotype are exchanged at that point. This operation results in two oﬀspring
genotypes. The chosen mutation operation is uniform. A selected genotype is
mutated in a random position in its GP tree structure by attaching a small
randomly generated subtree at that point. The chosen crossover and mutation
operations are well-researched variants and are chosen for their simplicity. Future
research will focus on novel crossover and mutation operations.

4 Experiment Design

4.1 Peer Competitors

There are no related EDL algorithms with which to benchmark the proposed
method. To mitigate this problem, the best evolved models from the proposed
algorithm are benchmarked against current state of the hand-crafted character-
level models: Le et al’s [8] char-DenseNet model, Zhang et al’s [2] Small-Char-
CNN and Large-Char-CNN models and Conneau et al’s [3] VDCNN model. All

Evolving char-DenseNets using Genetic Programming

9

the benchmark models are pure character-level models, meaning that there is no
data augmentation or text pre-processing conducted.

4.2 Benchmark Datasets

Two datasets from the work of Zhang et al.[2] are selected for this research work
as listed in table 1. The AG News dataset is the smallest of the datasets selected
and is regarded as a challenging dataset to classify. The second chosen dataset
is the Yelp Review Full dataset which is considered a large text dataset. This
dataset is particularly diﬃcult to model as the current best accuracies level are
typically still below 65%. Each dataset is split as shown in table 1. These values
are based on the split ratio as in [2] and [3].

Table 1: Datasets by training, validation and test splits.
No. classes No. train No. Validation No. Test
Dataset
7,600
AG News
50,000
Yelp Review Full

112,852
603,571

7,148
46,429

4
5

Table 2 lists the statistics of the instances in each dataset. The AG News dataset
consists of text sentences of 256 characters on average. Zhang et al.’s [2], Conneau
et al’s.[3] and Le et al’s. [8] models all use a temporal length of 1014 characters.
This implies redundant padding and wasted convolutional operations. Therefore,
this work uses a maximum sentence length of 256 characters when evolving
models for the AG news dataset and 512 characters for the Yelp Reviews Full
dataset.

Table 2: Sentence statistics.

Dataset
AG News
236±66
Yelp Review Full 732±664

Mean Minimum Maximum
1,012
100
5,849
10

4.3 Parameter Settings

Table 3: Parameter settings.

Parameter
Run count, seed
Epochs, batch size
Initial Learning Rate, momentum
Learning Schedule
Weight Initialisation
Training Data Usage
Alphabet
Max Sentence Length
Kernel size, stride, padding
Number of Generations
Population Size, elitism size
Crossover Prob and type, Mut. Prob. and type
Mutation Growth, size
Tournament Selection Size
Initial, max Tree Depth
Fitness Function
Probability Dropout applied to cell

Value
30, Unique per Run
10, 128 [2],[3]
0.01, 0.9 [3]
Halve every 3 epochs [2]
Kaiming[21][2],[3]
0.25
Same as in [2] and [3]
256 (AG News) 512 (Yelp)
3,1,1 [3]
20
20, 0.1
0.5 Single, 0.1, Unif.
Grow, [1,3]
3
[1,10], 17 [13]
max(val. acc.) [2] [3]
0.1

10

T. Londt et al.

Table 3 lists the parameter settings for the experiment. Thirty runs of the al-
gorithm are conducted for each dataset. Each run is assigned a single unique
seed. Candidate architectures are trained for 10 epochs to mitigate the problem
of long training times. The values for the learning rate, momentum and batch
size are the same as in [2] and [3]. Stochastic gradient descent is used for the
network optimiser as used in [2]. Each population run consists of 20 individuals
and is evolved over 20 generations. Elitism, crossover and mutation rate settings
are values found in the literature [13]. The mutation growth depth is set to a low
value to ensure that only small changes are made to the genotype so as not to
signiﬁcantly alter the phenotype. The maximum tree depth is set to a value of 17
and is considered best practice [13]. The objective is to maximise the candidate
architectures validation accuracy.

5 Results and Discussions

Fig. 3: Distribution of validation accuracy for evolved architectures.

The validation accuracies of all the models evolved by the GP-Dense algorithm
over the AG News and Yelp Reviews Full dataset are presented in ﬁgure 3. The
distribution in orange represents the validation accuracies attained over the Yelp
Reviews Full dataset, and the distribution in blue represents those attained over
the AG News dataset. It is noted that these validation accuracies are attained
using only 25% of each dataset. Both distributions are left-skewed, indicating
that GP-Dense has managed to maintain populations of models with similar
good validation accuracies and only a few of lower quality. It can be observed
that GP-Dense attained a maximum validation accuracy of 62.13% for the Yelp
Reviews Full dataset and 90.19% for the AG News dataset. These values are
achieved when retraining the best-evolved models for each dataset using 100%
of the training set. Evaluating each of the best fully trained models yields test
accuracies of 61.05% for Yelp Reviews Full and 89.59% for the AG News dataset.
Figure 4 presents the combined validation performance of all evolved models
over each generation for 30 runs. GP-Dense converges to a maximum validation

405060708090100Accuracy050100150200250Number of ModelsGP-Dense (Best) [Yelp Reviews Full size=100%][TestAcc.=61.05%]GP-Dense (Best) [AG News size=100%][TestAcc.=89.58%]GP-Dense (Best) [Yelp Reviews Full size=100%][Val Acc.=62.13%]GP-Dense (Best) [AG News size=100%][Val Acc.=90.19%]GP-Dense [AG News size=25%][All Val. Acc.]GP-Dense [Yelp Reviews Full size=25%][All Val. Acc.]Evolving char-DenseNets using Genetic Programming

11

accuracy early in the evolutionary process and maintains this position through
to the end of the evolutionary process. There are scatterings of low-quality mod-
els that appear in each generation. An analysis of these models indicates that
they are the result of crossover and mutation operations that resulted in low-
performance models.

(a) AG News.

(b) Yelp Reviews Full.

Fig. 4: GP-Dense performance over generations.

The distribution of the number of SEQ and PAR operations that are contained in
the genotypes of all evolved models is presented in ﬁgure 5. There is no evidence
that the evolutionary process favoured either operation over the other. Each of
the best-evolved models is highlighted by a lime green square. The best-evolved
model from AG News contained 4 PAR operations and 1 SEQ operation in its
genotype, indicating that the phenotype is wide and shallow. The best model
evolved for Yelp Reviews Full contained 7 PAR operations and 8 SEQ operations,
indicating that the phenotype is likely to be deep and relatively wide.

(a) AG News.

(b) Yelp Reviews Full.

Fig. 5: Density of number of SEQ vs PAR operations.

The ﬁttest genotype and corresponding phenotype evolved for the AG News
dataset is presented in ﬁgure 6. Note that the phenotype has eight dense blocks

Init.1234567891011121314151617181920Generation number.2030405060708090Validation accuracy. (%)Init.1234567891011121314151617181920Generation number.2030405060708090Validation accuracy. (%)05101520253035SEQ05101520253035PAR0.00.20.40.60.81.005101520253035SEQ05101520253035PAR0.00.20.40.60.81.012

T. Londt et al.

connected in parallel. It can also be observed that the deepest path in the network
is eight convolutional layers deep. The networks depth is comparable to Zhang
et al.’s [2] original char-CNN network. The ﬁttest genotype and corresponding
phenotype for the Yelp Reviews Full dataset set are presented in ﬁgure 7. This
network is signiﬁcantly deeper at 34 convolutional layers deep. The network is
comparable to the depth of VDCNN [3] at 29 convolutional layers.

Fig. 6: AG News: Best evolved genotype and corresponding phenotype.

Fig. 7: Yelp Reviews Full: Best evolved genotype and corresponding phenotype.

PARdepth=8drop=0.0PARdepth=3drop=0.0PARdepth=5drop=0.2ENDPARdepth=3drop=0.0ENDSEQdepth=2drop=0.0ENDENDENDENDINPUTk=3depth=2drop=0.0k=3depth=8drop=0.0k=3depth=3drop=0.0k=3depth=3drop=0.0k=3depth=5drop=0.2OUTPUTk=3depth=2drop=0.0SEQdepth=7drop=0.0PARdepth=7drop=0.0PARdepth=5drop=0.0SEQdepth=5drop=0.0SEQdepth=2drop=0.0SEQdepth=6drop=0.0PARdepth=4drop=0.0PARdepth=4drop=0.5PARdepth=5drop=0.0ENDENDSEQdepth=2drop=0.0ENDENDPARdepth=8drop=0.0PARdepth=5drop=0.0SEQdepth=5drop=0.0ENDSEQdepth=3drop=0.5ENDENDENDENDENDENDENDENDENDPARdepth=5drop=0.0ENDENDINPUTk=3depth=2drop=0.0k=3depth=7drop=0.0k=3depth=4drop=0.5k=3depth=5drop=0.0k=3depth=5drop=0.0k=3depth=2drop=0.0k=3depth=5drop=0.0k=3depth=7drop=0.0k=3depth=5drop=0.0k=3depth=4drop=0.0k=3depth=8drop=0.0k=3depth=5drop=0.0k=3depth=2drop=0.0OUTPUTk=3depth=3drop=0.5k=3depth=5drop=0.0k=3depth=6drop=0.0Evolving char-DenseNets using Genetic Programming

13

Fig. 8: Training response of best evolved model for AG News dataset.

The training response for the ﬁttest phenotype evolved over the AG News dataset
is presented in ﬁgure 8. The model converged within six epochs, highlighting
the evolutionary pressure applied by GP-Dense to evolve models that converge
within ten epochs. The validation accuracy degraded after the third epoch. How-
ever, after halving the learning rate, the validation accuracy increased again. The
same behaviour is seen when the learning rate halved again at the sixth epoch.
Further reducing the learning rate stabilised the validation accuracy.

Fig. 9: Training response of best evolved model for Yelp Reviews Full dataset.

Figure 9 presents the training response of the ﬁttest evolved phenotype over
the Yelp Reviews Full dataset. Halving the learning rate every three epochs did
not provide any beneﬁt in improving the accuracy of the model. The model be-
gan overﬁtting after the tenth epoch. This observation suggests that GP-Dense
may need to evolve a candidate model for longer than ten epochs when train-

0200040006000800010000120001400016000Iteration020406080100Validation Accuracy (%)Learning RateTraining Accuracy (Smoothed)Validation Accuracy01234567891011121314151617181920EpochMax Val. Acc.: 93.269%0.0000.0020.0040.0060.0080.010Learning Rate020000400006000080000100000120000140000Iteration020406080100Validation Accuracy (%)Learning RateTraining Accuracy (Smoothed)Validation Accuracy0123456789101112131415161718192021222324252627282930EpochMax Val. Acc.: 62.130%0.0000.0020.0040.0060.0080.010Learning Rate14

T. Londt et al.

ing on large datasets. This approach would decrease the probability of models
overﬁtting in later epochs.

Table 4: Test accuracies attained.

Model or Algorithm
Large Char-CNN (Zhang et al. [2])
Small Char-CNN (Zhang et al. [2])
VDCNN-29 (Conneau et al. [3])
char-DenseNet (Le et al.) [8]
GP-Dense Best (ours)

AG News Params. Yelp Reviews Params.
˜15
˜11
˜17
-
˜7

60.38
59.16
64.72
64.10
61.05

87.18
84.35
91.27
92.10
89.58

˜15
˜11
˜17
-
˜4

The comparison of test accuracies with the other four hand-crafted state-of-
the-art models are presented in table 4. GP-Dense outperformed all char-CNN
models by Zhang et al. [2] for both the AG News and Yelp Reviews Full dataset.
The evolved model underperformed VDCNN [3] by an absolute value of 1.69% on
AG News and 3.67% for Yelp Reviews Full. However, VDCNN has almost triple
the number of trainable parameters than the model evolved by GP-Dense on AG
News more than double on Yelp Full Reviews. char-DenseNet outperformed GP-
Dense by an absolute value of 2.52% on AG News and 3.05% on Yelp Reviews
Full. The parameter size for char-DenseNet is not reported in [8]. GP-Dense
evolved models for both AG News and Yelp Reviews Full that are smaller than
all other reported models.

6 Conclusions

This work proposed an evolutionary deep learning algorithm (GP-Dense) to
evolve character-level DenseNet architectures for text classiﬁcation tasks auto-
matically. This goal was achieved through the implementation of a GP-based
evolutionary algorithm using an indirect encoding to represent candidate neural
network architectures. GP-Dense was able to successfully evolve competitive net-
work architectures using only 25% of the datasets involved. It was observed that
GP-Dense performed better on the smaller dataset, AG News, than on the larger
dataset, Yelp Reviews Full. EDL has the potential to perform well on evolving
character-level DenseNet architectures for text classiﬁcation tasks as evidenced
by this work. However, it has demonstrated that further research such as param-
eters tuning and using a larger subset of a training dataset is required to ensure
that performant architectures can be evolved for larger datasets.

References

1. Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning. The MIT Press (2016)
2. Zhang, X., Zhao, J., LeCun, Y.: Character-level convolutional networks for text
classiﬁcation. In: Proceedings of the 28th International Conference on Neural In-
formation Processing Systems - Volume 1. p. 649–657. NIPS’15, MIT Press, Cam-
bridge, MA, USA (2015)

Evolving char-DenseNets using Genetic Programming

15

3. Conneau, A., Schwenk, H., Barrault, L., Lecun, Y.: Very deep convolutional net-

works for text classiﬁcation. pp. 1107–1116 (04 2017)

4. Collobert, R., Weston, J., Com, J., Karlen, M., Kavukcuoglu, K., Kuksa, P.: Nat-

ural Language Processing (Almost) from Scratch. Tech. rep. (2011)

5. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., Le, Q.V.: XLNet:

Generalized Autoregressive Pretraining for Language Understanding (2019)

6. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),

1735–1780 (Nov 1997)

7. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-

rectional transformers for language understanding (2018)

8. Le, H.T., Cerisara, C., Denis, A.: Do Convolutional Networks need to be Deep
for Text Classiﬁcation ? In: AAAI Workshop on Aﬀective Content Analysis. New
Orleans, United States (Feb 2018)

9. CHURCH, K.W.: Word2vec. Natural Language Engineering 23(1), 155–162 (2017)
10. Pennington, J., Socher, R., Manning, C.D.: GloVe: Global Vectors for Word Rep-

resentation. Tech. rep.

11. Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q.: Densely Connected

Convolutional Networks (aug 2016)

12. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning Representations by Back-

Propagating Errors, p. 696–699. MIT Press, Cambridge, MA, USA (1988)

13. Koza, J.R.: Genetic Programming: On the Programming of Computers by Means

of Natural Selection. MIT Press, Cambridge, MA, USA (1992)

14. De Sa, C., Feldman, M., R´e, C., Olukotun, K.: Understanding and optimizing
asynchronous low-precision stochastic gradient descent. In: 2017 ACM/IEEE 44th
Annual International Symposium on Computer Architecture (ISCA). pp. 561–574
(2017)

15. Hara, K., Saito, D., Shouno, H.: Analysis of function of rectiﬁed linear unit used
in deep learning. In: 2015 International Joint Conference on Neural Networks
(IJCNN). pp. 1–8 (2015)

16. He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning for Image Recognition.

CoRR abs/1512.0 (2015)

17. Liang, J., Meyerson, E., Hodjat, B., Fink, D., Mutch, K., Miikkulainen, R.: Evo-

lutionary Neural AutoML for Deep Learning (feb 2019)

18. Miikkulainen, R., Liang, J., Meyerson, E., Rawal, A., Fink, D., Francon, O., Raju,
B., Shahrzad, H., Navruzyan, A., Duﬀy, N., Hodjat, B.: Evolving deep neural
networks (2017)

19. Wulczyn, E., Thain, N., Dixon, L.: Wikipedia Talk Labels: Personal Attacks (2

2017)

20. Gruau, F., Gruau, F., I, L.C.B.l., De Doctorat, O.A.D., Demongeot, M.J., Cosnard,
E.M.M., Mazoyer, M.J., Peretto, M.P., Whitley, M.D.: Neural Network Synthesis
Using Cellular Encoding And The Genetic Algorithm. (1994)

21. Kaiming, H., Xiangyu, Z., Shaoqing, R., Jian, S.: Delving Deep into Rectiﬁers:
Surpassing Human-Level Performance on ImageNet Classiﬁcation Kaiming. Bio-
chemical and Biophysical Research Communications 498(1), 254–261 (2018)

